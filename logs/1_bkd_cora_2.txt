Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.3 der2:0 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 18.693439483642578 = 1.9457048177719116 + 2.0 * 8.37386703491211
Epoch 0, val loss: 1.9580012559890747
Epoch 10, training loss: 18.6829776763916 = 1.9365607500076294 + 2.0 * 8.373208045959473
Epoch 10, val loss: 1.9487006664276123
Epoch 20, training loss: 18.662967681884766 = 1.925233244895935 + 2.0 * 8.368866920471191
Epoch 20, val loss: 1.9369988441467285
Epoch 30, training loss: 18.581310272216797 = 1.910210132598877 + 2.0 * 8.335550308227539
Epoch 30, val loss: 1.9215553998947144
Epoch 40, training loss: 18.004161834716797 = 1.8923189640045166 + 2.0 * 8.05592155456543
Epoch 40, val loss: 1.9032893180847168
Epoch 50, training loss: 16.181598663330078 = 1.8742992877960205 + 2.0 * 7.15364933013916
Epoch 50, val loss: 1.8855829238891602
Epoch 60, training loss: 15.472198486328125 = 1.8621997833251953 + 2.0 * 6.804999351501465
Epoch 60, val loss: 1.8740075826644897
Epoch 70, training loss: 15.128530502319336 = 1.8519576787948608 + 2.0 * 6.638286590576172
Epoch 70, val loss: 1.8637069463729858
Epoch 80, training loss: 14.923785209655762 = 1.8423558473587036 + 2.0 * 6.540714740753174
Epoch 80, val loss: 1.853519082069397
Epoch 90, training loss: 14.744375228881836 = 1.8328267335891724 + 2.0 * 6.455774307250977
Epoch 90, val loss: 1.8434908390045166
Epoch 100, training loss: 14.59768295288086 = 1.8242740631103516 + 2.0 * 6.386704444885254
Epoch 100, val loss: 1.8344533443450928
Epoch 110, training loss: 14.493301391601562 = 1.8165507316589355 + 2.0 * 6.338375091552734
Epoch 110, val loss: 1.8261969089508057
Epoch 120, training loss: 14.402474403381348 = 1.8091933727264404 + 2.0 * 6.296640396118164
Epoch 120, val loss: 1.8183693885803223
Epoch 130, training loss: 14.32942008972168 = 1.8021563291549683 + 2.0 * 6.263631820678711
Epoch 130, val loss: 1.8109451532363892
Epoch 140, training loss: 14.271583557128906 = 1.7951315641403198 + 2.0 * 6.238225936889648
Epoch 140, val loss: 1.8038678169250488
Epoch 150, training loss: 14.218235969543457 = 1.7877753973007202 + 2.0 * 6.215230464935303
Epoch 150, val loss: 1.7967697381973267
Epoch 160, training loss: 14.173603057861328 = 1.7799073457717896 + 2.0 * 6.196847915649414
Epoch 160, val loss: 1.7894865274429321
Epoch 170, training loss: 14.131988525390625 = 1.7713732719421387 + 2.0 * 6.180307865142822
Epoch 170, val loss: 1.782028317451477
Epoch 180, training loss: 14.094143867492676 = 1.7619686126708984 + 2.0 * 6.166087627410889
Epoch 180, val loss: 1.7741039991378784
Epoch 190, training loss: 14.063896179199219 = 1.7513350248336792 + 2.0 * 6.156280517578125
Epoch 190, val loss: 1.7654014825820923
Epoch 200, training loss: 14.026140213012695 = 1.7390847206115723 + 2.0 * 6.143527507781982
Epoch 200, val loss: 1.7556498050689697
Epoch 210, training loss: 13.994396209716797 = 1.7249135971069336 + 2.0 * 6.134741306304932
Epoch 210, val loss: 1.7445629835128784
Epoch 220, training loss: 13.961708068847656 = 1.7085340023040771 + 2.0 * 6.1265869140625
Epoch 220, val loss: 1.7317456007003784
Epoch 230, training loss: 13.927136421203613 = 1.6893445253372192 + 2.0 * 6.118896007537842
Epoch 230, val loss: 1.7169114351272583
Epoch 240, training loss: 13.892463684082031 = 1.6667779684066772 + 2.0 * 6.112843036651611
Epoch 240, val loss: 1.699416995048523
Epoch 250, training loss: 13.855895042419434 = 1.6401689052581787 + 2.0 * 6.107862949371338
Epoch 250, val loss: 1.6786582469940186
Epoch 260, training loss: 13.820977210998535 = 1.608943223953247 + 2.0 * 6.106017112731934
Epoch 260, val loss: 1.6542162895202637
Epoch 270, training loss: 13.771713256835938 = 1.5732707977294922 + 2.0 * 6.099221229553223
Epoch 270, val loss: 1.6257938146591187
Epoch 280, training loss: 13.72230339050293 = 1.5326499938964844 + 2.0 * 6.094826698303223
Epoch 280, val loss: 1.5931882858276367
Epoch 290, training loss: 13.673531532287598 = 1.486931562423706 + 2.0 * 6.093299865722656
Epoch 290, val loss: 1.5560977458953857
Epoch 300, training loss: 13.616026878356934 = 1.4371166229248047 + 2.0 * 6.0894551277160645
Epoch 300, val loss: 1.5151937007904053
Epoch 310, training loss: 13.555536270141602 = 1.3839083909988403 + 2.0 * 6.085813999176025
Epoch 310, val loss: 1.4713386297225952
Epoch 320, training loss: 13.501070022583008 = 1.3283660411834717 + 2.0 * 6.0863518714904785
Epoch 320, val loss: 1.425419807434082
Epoch 330, training loss: 13.434343338012695 = 1.2724690437316895 + 2.0 * 6.080937385559082
Epoch 330, val loss: 1.3792318105697632
Epoch 340, training loss: 13.373065948486328 = 1.2168738842010498 + 2.0 * 6.07809591293335
Epoch 340, val loss: 1.333832859992981
Epoch 350, training loss: 13.326984405517578 = 1.1619980335235596 + 2.0 * 6.082493305206299
Epoch 350, val loss: 1.2895909547805786
Epoch 360, training loss: 13.258801460266113 = 1.1091388463974 + 2.0 * 6.074831485748291
Epoch 360, val loss: 1.2471495866775513
Epoch 370, training loss: 13.200718879699707 = 1.0577771663665771 + 2.0 * 6.071470737457275
Epoch 370, val loss: 1.2063554525375366
Epoch 380, training loss: 13.144674301147461 = 1.007514476776123 + 2.0 * 6.06857967376709
Epoch 380, val loss: 1.166850209236145
Epoch 390, training loss: 13.1116361618042 = 0.9583244919776917 + 2.0 * 6.076655864715576
Epoch 390, val loss: 1.1286234855651855
Epoch 400, training loss: 13.04218864440918 = 0.9115201830863953 + 2.0 * 6.065334320068359
Epoch 400, val loss: 1.0924636125564575
Epoch 410, training loss: 12.993120193481445 = 0.8663082122802734 + 2.0 * 6.063405990600586
Epoch 410, val loss: 1.0577727556228638
Epoch 420, training loss: 12.944677352905273 = 0.8224206566810608 + 2.0 * 6.06112813949585
Epoch 420, val loss: 1.0241799354553223
Epoch 430, training loss: 12.905817985534668 = 0.7799066305160522 + 2.0 * 6.062955856323242
Epoch 430, val loss: 0.9919784069061279
Epoch 440, training loss: 12.857535362243652 = 0.7398514151573181 + 2.0 * 6.058842182159424
Epoch 440, val loss: 0.9618226289749146
Epoch 450, training loss: 12.81644344329834 = 0.7022683620452881 + 2.0 * 6.057087421417236
Epoch 450, val loss: 0.9341329336166382
Epoch 460, training loss: 12.776951789855957 = 0.6666036248207092 + 2.0 * 6.055173873901367
Epoch 460, val loss: 0.908048152923584
Epoch 470, training loss: 12.738428115844727 = 0.6326099038124084 + 2.0 * 6.052908897399902
Epoch 470, val loss: 0.8836314678192139
Epoch 480, training loss: 12.70427131652832 = 0.6003129482269287 + 2.0 * 6.051979064941406
Epoch 480, val loss: 0.8609914779663086
Epoch 490, training loss: 12.681241989135742 = 0.5698093175888062 + 2.0 * 6.055716514587402
Epoch 490, val loss: 0.8402193188667297
Epoch 500, training loss: 12.64293384552002 = 0.541319727897644 + 2.0 * 6.050806999206543
Epoch 500, val loss: 0.8214722275733948
Epoch 510, training loss: 12.609419822692871 = 0.5143042206764221 + 2.0 * 6.047557830810547
Epoch 510, val loss: 0.8043022155761719
Epoch 520, training loss: 12.596750259399414 = 0.48857805132865906 + 2.0 * 6.054086208343506
Epoch 520, val loss: 0.7885575294494629
Epoch 530, training loss: 12.553645133972168 = 0.4643096625804901 + 2.0 * 6.044667720794678
Epoch 530, val loss: 0.7745346426963806
Epoch 540, training loss: 12.529012680053711 = 0.4411616027355194 + 2.0 * 6.043925762176514
Epoch 540, val loss: 0.761907696723938
Epoch 550, training loss: 12.517107009887695 = 0.4188901484012604 + 2.0 * 6.049108505249023
Epoch 550, val loss: 0.7501979470252991
Epoch 560, training loss: 12.480134963989258 = 0.397745281457901 + 2.0 * 6.041194915771484
Epoch 560, val loss: 0.7398698329925537
Epoch 570, training loss: 12.45839786529541 = 0.3774029612541199 + 2.0 * 6.040497303009033
Epoch 570, val loss: 0.7306097149848938
Epoch 580, training loss: 12.437834739685059 = 0.3579561710357666 + 2.0 * 6.0399394035339355
Epoch 580, val loss: 0.7221410870552063
Epoch 590, training loss: 12.419017791748047 = 0.33939871191978455 + 2.0 * 6.039809703826904
Epoch 590, val loss: 0.7145922183990479
Epoch 600, training loss: 12.398161888122559 = 0.3219251334667206 + 2.0 * 6.038118362426758
Epoch 600, val loss: 0.7079756855964661
Epoch 610, training loss: 12.376614570617676 = 0.3052332401275635 + 2.0 * 6.035690784454346
Epoch 610, val loss: 0.7021378874778748
Epoch 620, training loss: 12.369378089904785 = 0.2894337773323059 + 2.0 * 6.039972305297852
Epoch 620, val loss: 0.6968077421188354
Epoch 630, training loss: 12.345809936523438 = 0.27464234828948975 + 2.0 * 6.035583972930908
Epoch 630, val loss: 0.6923220157623291
Epoch 640, training loss: 12.325728416442871 = 0.26065337657928467 + 2.0 * 6.032537460327148
Epoch 640, val loss: 0.6884191632270813
Epoch 650, training loss: 12.310293197631836 = 0.24735763669013977 + 2.0 * 6.031467914581299
Epoch 650, val loss: 0.6849042177200317
Epoch 660, training loss: 12.295889854431152 = 0.23475605249404907 + 2.0 * 6.030566692352295
Epoch 660, val loss: 0.6819245219230652
Epoch 670, training loss: 12.288365364074707 = 0.22289378941059113 + 2.0 * 6.032735824584961
Epoch 670, val loss: 0.6793773770332336
Epoch 680, training loss: 12.273968696594238 = 0.21186158061027527 + 2.0 * 6.03105354309082
Epoch 680, val loss: 0.6774082779884338
Epoch 690, training loss: 12.259880065917969 = 0.2015167772769928 + 2.0 * 6.029181480407715
Epoch 690, val loss: 0.675906777381897
Epoch 700, training loss: 12.246491432189941 = 0.19176436960697174 + 2.0 * 6.027363300323486
Epoch 700, val loss: 0.6747443675994873
Epoch 710, training loss: 12.236387252807617 = 0.182575985789299 + 2.0 * 6.026905536651611
Epoch 710, val loss: 0.6739286780357361
Epoch 720, training loss: 12.236308097839355 = 0.17388881742954254 + 2.0 * 6.031209468841553
Epoch 720, val loss: 0.6734461188316345
Epoch 730, training loss: 12.218511581420898 = 0.1657545119524002 + 2.0 * 6.026378631591797
Epoch 730, val loss: 0.673316240310669
Epoch 740, training loss: 12.211048126220703 = 0.15814249217510223 + 2.0 * 6.026453018188477
Epoch 740, val loss: 0.6736152768135071
Epoch 750, training loss: 12.199588775634766 = 0.15095147490501404 + 2.0 * 6.024318695068359
Epoch 750, val loss: 0.6740750670433044
Epoch 760, training loss: 12.189956665039062 = 0.14417825639247894 + 2.0 * 6.022889137268066
Epoch 760, val loss: 0.6747815012931824
Epoch 770, training loss: 12.197237968444824 = 0.1377953439950943 + 2.0 * 6.029721260070801
Epoch 770, val loss: 0.6757656335830688
Epoch 780, training loss: 12.17398738861084 = 0.13184115290641785 + 2.0 * 6.021073341369629
Epoch 780, val loss: 0.6769005060195923
Epoch 790, training loss: 12.167777061462402 = 0.12620918452739716 + 2.0 * 6.0207839012146
Epoch 790, val loss: 0.6783463358879089
Epoch 800, training loss: 12.160357475280762 = 0.120869480073452 + 2.0 * 6.019743919372559
Epoch 800, val loss: 0.6799213290214539
Epoch 810, training loss: 12.16716480255127 = 0.11580687761306763 + 2.0 * 6.025679111480713
Epoch 810, val loss: 0.6815639138221741
Epoch 820, training loss: 12.1492338180542 = 0.11108790338039398 + 2.0 * 6.019073009490967
Epoch 820, val loss: 0.6834219098091125
Epoch 830, training loss: 12.14150619506836 = 0.10661528259515762 + 2.0 * 6.0174455642700195
Epoch 830, val loss: 0.6854367852210999
Epoch 840, training loss: 12.136213302612305 = 0.10235808044672012 + 2.0 * 6.016927719116211
Epoch 840, val loss: 0.6875483393669128
Epoch 850, training loss: 12.15096378326416 = 0.09830677509307861 + 2.0 * 6.0263285636901855
Epoch 850, val loss: 0.6898034811019897
Epoch 860, training loss: 12.132725715637207 = 0.09452005475759506 + 2.0 * 6.019103050231934
Epoch 860, val loss: 0.6920470595359802
Epoch 870, training loss: 12.12460994720459 = 0.09093180298805237 + 2.0 * 6.016839027404785
Epoch 870, val loss: 0.6945860981941223
Epoch 880, training loss: 12.115740776062012 = 0.08751191943883896 + 2.0 * 6.0141143798828125
Epoch 880, val loss: 0.6971539258956909
Epoch 890, training loss: 12.112480163574219 = 0.08424460887908936 + 2.0 * 6.01411771774292
Epoch 890, val loss: 0.6997442245483398
Epoch 900, training loss: 12.115090370178223 = 0.08113361895084381 + 2.0 * 6.0169782638549805
Epoch 900, val loss: 0.7024199962615967
Epoch 910, training loss: 12.116552352905273 = 0.07820428907871246 + 2.0 * 6.019174098968506
Epoch 910, val loss: 0.7051782011985779
Epoch 920, training loss: 12.105281829833984 = 0.07540278881788254 + 2.0 * 6.014939308166504
Epoch 920, val loss: 0.7079638242721558
Epoch 930, training loss: 12.095882415771484 = 0.07275380194187164 + 2.0 * 6.011564254760742
Epoch 930, val loss: 0.7108412384986877
Epoch 940, training loss: 12.091998100280762 = 0.07020411640405655 + 2.0 * 6.010897159576416
Epoch 940, val loss: 0.7137404084205627
Epoch 950, training loss: 12.092080116271973 = 0.06776485592126846 + 2.0 * 6.012157440185547
Epoch 950, val loss: 0.716736376285553
Epoch 960, training loss: 12.08552360534668 = 0.06545194238424301 + 2.0 * 6.010035991668701
Epoch 960, val loss: 0.7196703553199768
Epoch 970, training loss: 12.084537506103516 = 0.06324606388807297 + 2.0 * 6.010645866394043
Epoch 970, val loss: 0.7226818799972534
Epoch 980, training loss: 12.08066463470459 = 0.06114650145173073 + 2.0 * 6.009758949279785
Epoch 980, val loss: 0.7257362604141235
Epoch 990, training loss: 12.078850746154785 = 0.05913932994008064 + 2.0 * 6.0098557472229
Epoch 990, val loss: 0.7287160158157349
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.6753
Flip ASR: 0.6089/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.700185775756836 = 1.952774167060852 + 2.0 * 8.373705863952637
Epoch 0, val loss: 1.946869969367981
Epoch 10, training loss: 18.68427276611328 = 1.9419459104537964 + 2.0 * 8.371163368225098
Epoch 10, val loss: 1.9360570907592773
Epoch 20, training loss: 18.65237808227539 = 1.928631067276001 + 2.0 * 8.361873626708984
Epoch 20, val loss: 1.9220341444015503
Epoch 30, training loss: 18.560585021972656 = 1.9110769033432007 + 2.0 * 8.324753761291504
Epoch 30, val loss: 1.902674674987793
Epoch 40, training loss: 18.0220890045166 = 1.891025424003601 + 2.0 * 8.065531730651855
Epoch 40, val loss: 1.8802598714828491
Epoch 50, training loss: 16.696754455566406 = 1.8702927827835083 + 2.0 * 7.4132304191589355
Epoch 50, val loss: 1.856528639793396
Epoch 60, training loss: 16.284765243530273 = 1.847454309463501 + 2.0 * 7.218655109405518
Epoch 60, val loss: 1.8333714008331299
Epoch 70, training loss: 15.84352970123291 = 1.8313299417495728 + 2.0 * 7.006099700927734
Epoch 70, val loss: 1.8183447122573853
Epoch 80, training loss: 15.363718032836914 = 1.819469690322876 + 2.0 * 6.772124290466309
Epoch 80, val loss: 1.8068172931671143
Epoch 90, training loss: 14.988070487976074 = 1.810322880744934 + 2.0 * 6.588873863220215
Epoch 90, val loss: 1.797173261642456
Epoch 100, training loss: 14.833815574645996 = 1.8000304698944092 + 2.0 * 6.516892433166504
Epoch 100, val loss: 1.7858898639678955
Epoch 110, training loss: 14.71613597869873 = 1.7887400388717651 + 2.0 * 6.463697910308838
Epoch 110, val loss: 1.7743672132492065
Epoch 120, training loss: 14.583910942077637 = 1.7785528898239136 + 2.0 * 6.402678966522217
Epoch 120, val loss: 1.7647143602371216
Epoch 130, training loss: 14.470440864562988 = 1.7694648504257202 + 2.0 * 6.350488185882568
Epoch 130, val loss: 1.7556381225585938
Epoch 140, training loss: 14.380335807800293 = 1.7598981857299805 + 2.0 * 6.310218811035156
Epoch 140, val loss: 1.746361255645752
Epoch 150, training loss: 14.303422927856445 = 1.7497903108596802 + 2.0 * 6.276816368103027
Epoch 150, val loss: 1.7369567155838013
Epoch 160, training loss: 14.232725143432617 = 1.7387033700942993 + 2.0 * 6.247010707855225
Epoch 160, val loss: 1.7271281480789185
Epoch 170, training loss: 14.174212455749512 = 1.7259037494659424 + 2.0 * 6.224154472351074
Epoch 170, val loss: 1.7160979509353638
Epoch 180, training loss: 14.114959716796875 = 1.7110940217971802 + 2.0 * 6.201932907104492
Epoch 180, val loss: 1.7034004926681519
Epoch 190, training loss: 14.064557075500488 = 1.6937541961669922 + 2.0 * 6.185401439666748
Epoch 190, val loss: 1.6889840364456177
Epoch 200, training loss: 14.017430305480957 = 1.6735421419143677 + 2.0 * 6.1719441413879395
Epoch 200, val loss: 1.672184944152832
Epoch 210, training loss: 13.970836639404297 = 1.6495764255523682 + 2.0 * 6.160630226135254
Epoch 210, val loss: 1.6523184776306152
Epoch 220, training loss: 13.92406940460205 = 1.6210864782333374 + 2.0 * 6.151491641998291
Epoch 220, val loss: 1.6288821697235107
Epoch 230, training loss: 13.878365516662598 = 1.5882431268692017 + 2.0 * 6.145061016082764
Epoch 230, val loss: 1.6017791032791138
Epoch 240, training loss: 13.824329376220703 = 1.5511430501937866 + 2.0 * 6.136593341827393
Epoch 240, val loss: 1.5717451572418213
Epoch 250, training loss: 13.76975154876709 = 1.510207176208496 + 2.0 * 6.129772186279297
Epoch 250, val loss: 1.5386266708374023
Epoch 260, training loss: 13.71452522277832 = 1.4661996364593506 + 2.0 * 6.124162673950195
Epoch 260, val loss: 1.5034271478652954
Epoch 270, training loss: 13.665044784545898 = 1.4219517707824707 + 2.0 * 6.121546268463135
Epoch 270, val loss: 1.4682952165603638
Epoch 280, training loss: 13.606524467468262 = 1.3794944286346436 + 2.0 * 6.1135149002075195
Epoch 280, val loss: 1.4352192878723145
Epoch 290, training loss: 13.55689525604248 = 1.339486837387085 + 2.0 * 6.108704090118408
Epoch 290, val loss: 1.4042272567749023
Epoch 300, training loss: 13.509535789489746 = 1.3025742769241333 + 2.0 * 6.103480815887451
Epoch 300, val loss: 1.3761131763458252
Epoch 310, training loss: 13.474175453186035 = 1.2690280675888062 + 2.0 * 6.102573871612549
Epoch 310, val loss: 1.3513145446777344
Epoch 320, training loss: 13.431428909301758 = 1.2393304109573364 + 2.0 * 6.0960493087768555
Epoch 320, val loss: 1.3297679424285889
Epoch 330, training loss: 13.39455509185791 = 1.212483525276184 + 2.0 * 6.091035842895508
Epoch 330, val loss: 1.3105876445770264
Epoch 340, training loss: 13.361942291259766 = 1.1874849796295166 + 2.0 * 6.087228775024414
Epoch 340, val loss: 1.2931283712387085
Epoch 350, training loss: 13.334722518920898 = 1.163769006729126 + 2.0 * 6.085476875305176
Epoch 350, val loss: 1.276899814605713
Epoch 360, training loss: 13.304314613342285 = 1.1406728029251099 + 2.0 * 6.081820964813232
Epoch 360, val loss: 1.261393427848816
Epoch 370, training loss: 13.276407241821289 = 1.1179834604263306 + 2.0 * 6.079211711883545
Epoch 370, val loss: 1.246079683303833
Epoch 380, training loss: 13.243988990783691 = 1.0949846506118774 + 2.0 * 6.074501991271973
Epoch 380, val loss: 1.2308489084243774
Epoch 390, training loss: 13.215415954589844 = 1.0715391635894775 + 2.0 * 6.071938514709473
Epoch 390, val loss: 1.2152200937271118
Epoch 400, training loss: 13.187145233154297 = 1.0473225116729736 + 2.0 * 6.069911479949951
Epoch 400, val loss: 1.1991392374038696
Epoch 410, training loss: 13.167645454406738 = 1.0222299098968506 + 2.0 * 6.072707653045654
Epoch 410, val loss: 1.1826353073120117
Epoch 420, training loss: 13.130965232849121 = 0.996788740158081 + 2.0 * 6.0670881271362305
Epoch 420, val loss: 1.1656816005706787
Epoch 430, training loss: 13.098930358886719 = 0.9706946015357971 + 2.0 * 6.064117908477783
Epoch 430, val loss: 1.1484863758087158
Epoch 440, training loss: 13.067887306213379 = 0.9440550208091736 + 2.0 * 6.061916351318359
Epoch 440, val loss: 1.1310904026031494
Epoch 450, training loss: 13.054189682006836 = 0.9170240759849548 + 2.0 * 6.068583011627197
Epoch 450, val loss: 1.1136099100112915
Epoch 460, training loss: 13.007829666137695 = 0.8903785347938538 + 2.0 * 6.058725357055664
Epoch 460, val loss: 1.0963122844696045
Epoch 470, training loss: 12.97747802734375 = 0.8639289736747742 + 2.0 * 6.056774616241455
Epoch 470, val loss: 1.0793880224227905
Epoch 480, training loss: 12.948224067687988 = 0.8377047181129456 + 2.0 * 6.055259704589844
Epoch 480, val loss: 1.0628862380981445
Epoch 490, training loss: 12.939087867736816 = 0.811726450920105 + 2.0 * 6.063680648803711
Epoch 490, val loss: 1.0468111038208008
Epoch 500, training loss: 12.900686264038086 = 0.7867252230644226 + 2.0 * 6.056980609893799
Epoch 500, val loss: 1.0313817262649536
Epoch 510, training loss: 12.86376667022705 = 0.7624490857124329 + 2.0 * 6.050658702850342
Epoch 510, val loss: 1.016783356666565
Epoch 520, training loss: 12.83777141571045 = 0.738590657711029 + 2.0 * 6.049590587615967
Epoch 520, val loss: 1.0026484727859497
Epoch 530, training loss: 12.809447288513184 = 0.7150366902351379 + 2.0 * 6.047205448150635
Epoch 530, val loss: 0.988936722278595
Epoch 540, training loss: 12.794515609741211 = 0.6917684078216553 + 2.0 * 6.051373481750488
Epoch 540, val loss: 0.9756053686141968
Epoch 550, training loss: 12.760758399963379 = 0.669055163860321 + 2.0 * 6.045851707458496
Epoch 550, val loss: 0.9626963138580322
Epoch 560, training loss: 12.73645305633545 = 0.6469168066978455 + 2.0 * 6.044768333435059
Epoch 560, val loss: 0.9503147602081299
Epoch 570, training loss: 12.711357116699219 = 0.6251256465911865 + 2.0 * 6.043115615844727
Epoch 570, val loss: 0.9382989406585693
Epoch 580, training loss: 12.68614673614502 = 0.6038202047348022 + 2.0 * 6.041163444519043
Epoch 580, val loss: 0.9265758991241455
Epoch 590, training loss: 12.664092063903809 = 0.5831114053726196 + 2.0 * 6.04049015045166
Epoch 590, val loss: 0.9153848886489868
Epoch 600, training loss: 12.641080856323242 = 0.5629133582115173 + 2.0 * 6.039083957672119
Epoch 600, val loss: 0.9046058654785156
Epoch 610, training loss: 12.630841255187988 = 0.5431919693946838 + 2.0 * 6.043824672698975
Epoch 610, val loss: 0.8943518996238708
Epoch 620, training loss: 12.601241111755371 = 0.5241993069648743 + 2.0 * 6.038520812988281
Epoch 620, val loss: 0.8847923278808594
Epoch 630, training loss: 12.576203346252441 = 0.5058532357215881 + 2.0 * 6.03517484664917
Epoch 630, val loss: 0.8758944869041443
Epoch 640, training loss: 12.564140319824219 = 0.48801279067993164 + 2.0 * 6.0380635261535645
Epoch 640, val loss: 0.8675807118415833
Epoch 650, training loss: 12.539226531982422 = 0.47079530358314514 + 2.0 * 6.034215450286865
Epoch 650, val loss: 0.8599083423614502
Epoch 660, training loss: 12.519556999206543 = 0.4541029632091522 + 2.0 * 6.032727241516113
Epoch 660, val loss: 0.8528591394424438
Epoch 670, training loss: 12.502671241760254 = 0.43783995509147644 + 2.0 * 6.032415866851807
Epoch 670, val loss: 0.8462949395179749
Epoch 680, training loss: 12.481069564819336 = 0.42197588086128235 + 2.0 * 6.029546737670898
Epoch 680, val loss: 0.8402320146560669
Epoch 690, training loss: 12.464879035949707 = 0.40641334652900696 + 2.0 * 6.029232978820801
Epoch 690, val loss: 0.8346818089485168
Epoch 700, training loss: 12.45074462890625 = 0.3911311626434326 + 2.0 * 6.029806613922119
Epoch 700, val loss: 0.8294860124588013
Epoch 710, training loss: 12.434199333190918 = 0.3761484920978546 + 2.0 * 6.029025554656982
Epoch 710, val loss: 0.82461017370224
Epoch 720, training loss: 12.414066314697266 = 0.36157652735710144 + 2.0 * 6.0262451171875
Epoch 720, val loss: 0.8202430605888367
Epoch 730, training loss: 12.396333694458008 = 0.3471982181072235 + 2.0 * 6.024567604064941
Epoch 730, val loss: 0.8162879347801208
Epoch 740, training loss: 12.380387306213379 = 0.3329913020133972 + 2.0 * 6.023697853088379
Epoch 740, val loss: 0.8126624226570129
Epoch 750, training loss: 12.383624076843262 = 0.3190055191516876 + 2.0 * 6.032309055328369
Epoch 750, val loss: 0.809424638748169
Epoch 760, training loss: 12.353129386901855 = 0.30532106757164 + 2.0 * 6.023904323577881
Epoch 760, val loss: 0.8063848614692688
Epoch 770, training loss: 12.33666706085205 = 0.292056143283844 + 2.0 * 6.022305488586426
Epoch 770, val loss: 0.8038938045501709
Epoch 780, training loss: 12.32133674621582 = 0.27910515666007996 + 2.0 * 6.021115779876709
Epoch 780, val loss: 0.8017480373382568
Epoch 790, training loss: 12.306936264038086 = 0.2664899528026581 + 2.0 * 6.020223140716553
Epoch 790, val loss: 0.8000373244285583
Epoch 800, training loss: 12.310595512390137 = 0.2543133795261383 + 2.0 * 6.028141021728516
Epoch 800, val loss: 0.7986136674880981
Epoch 810, training loss: 12.285013198852539 = 0.24270649254322052 + 2.0 * 6.021153450012207
Epoch 810, val loss: 0.7975727319717407
Epoch 820, training loss: 12.26795768737793 = 0.23166336119174957 + 2.0 * 6.018146991729736
Epoch 820, val loss: 0.7970672845840454
Epoch 830, training loss: 12.257518768310547 = 0.22110220789909363 + 2.0 * 6.0182085037231445
Epoch 830, val loss: 0.7970065474510193
Epoch 840, training loss: 12.251233100891113 = 0.2110876888036728 + 2.0 * 6.020072937011719
Epoch 840, val loss: 0.7972292900085449
Epoch 850, training loss: 12.236323356628418 = 0.20172972977161407 + 2.0 * 6.01729679107666
Epoch 850, val loss: 0.7978736758232117
Epoch 860, training loss: 12.224928855895996 = 0.1929001659154892 + 2.0 * 6.016014575958252
Epoch 860, val loss: 0.7990121841430664
Epoch 870, training loss: 12.214354515075684 = 0.1845545470714569 + 2.0 * 6.014900207519531
Epoch 870, val loss: 0.8004660606384277
Epoch 880, training loss: 12.220813751220703 = 0.17670100927352905 + 2.0 * 6.022056579589844
Epoch 880, val loss: 0.8022472858428955
Epoch 890, training loss: 12.201963424682617 = 0.16939933598041534 + 2.0 * 6.016282081604004
Epoch 890, val loss: 0.8042665123939514
Epoch 900, training loss: 12.189489364624023 = 0.16251350939273834 + 2.0 * 6.013487815856934
Epoch 900, val loss: 0.8066325783729553
Epoch 910, training loss: 12.181201934814453 = 0.15603584051132202 + 2.0 * 6.012583255767822
Epoch 910, val loss: 0.8093295693397522
Epoch 920, training loss: 12.176910400390625 = 0.149928480386734 + 2.0 * 6.013491153717041
Epoch 920, val loss: 0.8122500777244568
Epoch 930, training loss: 12.16695785522461 = 0.1442064642906189 + 2.0 * 6.011375904083252
Epoch 930, val loss: 0.8152649998664856
Epoch 940, training loss: 12.164406776428223 = 0.13883629441261292 + 2.0 * 6.0127854347229
Epoch 940, val loss: 0.8185021877288818
Epoch 950, training loss: 12.154406547546387 = 0.13377296924591064 + 2.0 * 6.010316848754883
Epoch 950, val loss: 0.8220057487487793
Epoch 960, training loss: 12.172075271606445 = 0.1289968490600586 + 2.0 * 6.021539211273193
Epoch 960, val loss: 0.8256051540374756
Epoch 970, training loss: 12.146017074584961 = 0.1244598925113678 + 2.0 * 6.010778427124023
Epoch 970, val loss: 0.8291656970977783
Epoch 980, training loss: 12.138113021850586 = 0.12019874900579453 + 2.0 * 6.0089569091796875
Epoch 980, val loss: 0.8330482840538025
Epoch 990, training loss: 12.132579803466797 = 0.116140216588974 + 2.0 * 6.0082197189331055
Epoch 990, val loss: 0.8370615839958191
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8000
Overall ASR: 0.1624
Flip ASR: 0.1778/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.693214416503906 = 1.9455879926681519 + 2.0 * 8.37381362915039
Epoch 0, val loss: 1.9395679235458374
Epoch 10, training loss: 18.6812744140625 = 1.9350297451019287 + 2.0 * 8.373122215270996
Epoch 10, val loss: 1.9294377565383911
Epoch 20, training loss: 18.659082412719727 = 1.9218318462371826 + 2.0 * 8.36862564086914
Epoch 20, val loss: 1.916459560394287
Epoch 30, training loss: 18.58062744140625 = 1.9040976762771606 + 2.0 * 8.338264465332031
Epoch 30, val loss: 1.8988662958145142
Epoch 40, training loss: 18.16069221496582 = 1.8830510377883911 + 2.0 * 8.13882064819336
Epoch 40, val loss: 1.878700613975525
Epoch 50, training loss: 16.615942001342773 = 1.8609929084777832 + 2.0 * 7.377474308013916
Epoch 50, val loss: 1.857675552368164
Epoch 60, training loss: 15.779092788696289 = 1.8452694416046143 + 2.0 * 6.966911792755127
Epoch 60, val loss: 1.8435018062591553
Epoch 70, training loss: 15.206305503845215 = 1.8335272073745728 + 2.0 * 6.686388969421387
Epoch 70, val loss: 1.8319419622421265
Epoch 80, training loss: 14.930094718933105 = 1.8232154846191406 + 2.0 * 6.553439617156982
Epoch 80, val loss: 1.821523666381836
Epoch 90, training loss: 14.792530059814453 = 1.8132572174072266 + 2.0 * 6.489636421203613
Epoch 90, val loss: 1.8113620281219482
Epoch 100, training loss: 14.683371543884277 = 1.803559422492981 + 2.0 * 6.439906120300293
Epoch 100, val loss: 1.8017746210098267
Epoch 110, training loss: 14.587499618530273 = 1.7946557998657227 + 2.0 * 6.396421909332275
Epoch 110, val loss: 1.7931193113327026
Epoch 120, training loss: 14.507207870483398 = 1.7867399454116821 + 2.0 * 6.360233783721924
Epoch 120, val loss: 1.7855829000473022
Epoch 130, training loss: 14.424489974975586 = 1.7793543338775635 + 2.0 * 6.322567939758301
Epoch 130, val loss: 1.778753399848938
Epoch 140, training loss: 14.352367401123047 = 1.7722100019454956 + 2.0 * 6.290078639984131
Epoch 140, val loss: 1.772192120552063
Epoch 150, training loss: 14.290648460388184 = 1.7644041776657104 + 2.0 * 6.263122081756592
Epoch 150, val loss: 1.7652382850646973
Epoch 160, training loss: 14.241477012634277 = 1.7552587985992432 + 2.0 * 6.243109226226807
Epoch 160, val loss: 1.757339596748352
Epoch 170, training loss: 14.202313423156738 = 1.7443054914474487 + 2.0 * 6.22900390625
Epoch 170, val loss: 1.7482354640960693
Epoch 180, training loss: 14.161088943481445 = 1.7318382263183594 + 2.0 * 6.214625358581543
Epoch 180, val loss: 1.738054871559143
Epoch 190, training loss: 14.119362831115723 = 1.7175971269607544 + 2.0 * 6.200882911682129
Epoch 190, val loss: 1.7265782356262207
Epoch 200, training loss: 14.081314086914062 = 1.701088547706604 + 2.0 * 6.190112590789795
Epoch 200, val loss: 1.7134367227554321
Epoch 210, training loss: 14.031745910644531 = 1.6819932460784912 + 2.0 * 6.1748762130737305
Epoch 210, val loss: 1.6983544826507568
Epoch 220, training loss: 13.983888626098633 = 1.6595865488052368 + 2.0 * 6.162150859832764
Epoch 220, val loss: 1.6807293891906738
Epoch 230, training loss: 13.936870574951172 = 1.6330509185791016 + 2.0 * 6.151909828186035
Epoch 230, val loss: 1.659919023513794
Epoch 240, training loss: 13.88752269744873 = 1.601970911026001 + 2.0 * 6.142776012420654
Epoch 240, val loss: 1.6353729963302612
Epoch 250, training loss: 13.835573196411133 = 1.5655769109725952 + 2.0 * 6.134998321533203
Epoch 250, val loss: 1.606544852256775
Epoch 260, training loss: 13.779733657836914 = 1.5233800411224365 + 2.0 * 6.128176689147949
Epoch 260, val loss: 1.5730619430541992
Epoch 270, training loss: 13.733942031860352 = 1.475629448890686 + 2.0 * 6.129156112670898
Epoch 270, val loss: 1.5351430177688599
Epoch 280, training loss: 13.661060333251953 = 1.424267292022705 + 2.0 * 6.118396759033203
Epoch 280, val loss: 1.4944182634353638
Epoch 290, training loss: 13.598382949829102 = 1.3700696229934692 + 2.0 * 6.114156723022461
Epoch 290, val loss: 1.451521873474121
Epoch 300, training loss: 13.534934043884277 = 1.3139655590057373 + 2.0 * 6.1104841232299805
Epoch 300, val loss: 1.4072576761245728
Epoch 310, training loss: 13.471589088439941 = 1.2575676441192627 + 2.0 * 6.107010841369629
Epoch 310, val loss: 1.3630248308181763
Epoch 320, training loss: 13.408815383911133 = 1.2024779319763184 + 2.0 * 6.103168487548828
Epoch 320, val loss: 1.3200607299804688
Epoch 330, training loss: 13.346665382385254 = 1.1484183073043823 + 2.0 * 6.099123477935791
Epoch 330, val loss: 1.2781710624694824
Epoch 340, training loss: 13.291350364685059 = 1.095698595046997 + 2.0 * 6.09782600402832
Epoch 340, val loss: 1.2375872135162354
Epoch 350, training loss: 13.229853630065918 = 1.0451072454452515 + 2.0 * 6.092373371124268
Epoch 350, val loss: 1.198967456817627
Epoch 360, training loss: 13.174036979675293 = 0.9960591197013855 + 2.0 * 6.088988780975342
Epoch 360, val loss: 1.161596655845642
Epoch 370, training loss: 13.127544403076172 = 0.9482599496841431 + 2.0 * 6.08964204788208
Epoch 370, val loss: 1.1255128383636475
Epoch 380, training loss: 13.06832218170166 = 0.9025511145591736 + 2.0 * 6.0828857421875
Epoch 380, val loss: 1.0909804105758667
Epoch 390, training loss: 13.019669532775879 = 0.8587356209754944 + 2.0 * 6.0804667472839355
Epoch 390, val loss: 1.0581244230270386
Epoch 400, training loss: 12.978792190551758 = 0.8171349167823792 + 2.0 * 6.080828666687012
Epoch 400, val loss: 1.0272221565246582
Epoch 410, training loss: 12.929315567016602 = 0.7783209681510925 + 2.0 * 6.075497150421143
Epoch 410, val loss: 0.9985772967338562
Epoch 420, training loss: 12.886338233947754 = 0.7420684099197388 + 2.0 * 6.072134971618652
Epoch 420, val loss: 0.972284734249115
Epoch 430, training loss: 12.851574897766113 = 0.7080658078193665 + 2.0 * 6.071754455566406
Epoch 430, val loss: 0.9481065273284912
Epoch 440, training loss: 12.814176559448242 = 0.676572859287262 + 2.0 * 6.0688018798828125
Epoch 440, val loss: 0.9262893795967102
Epoch 450, training loss: 12.783600807189941 = 0.6474305987358093 + 2.0 * 6.068085193634033
Epoch 450, val loss: 0.9068729281425476
Epoch 460, training loss: 12.74923038482666 = 0.6204183101654053 + 2.0 * 6.064405918121338
Epoch 460, val loss: 0.8896324038505554
Epoch 470, training loss: 12.716886520385742 = 0.5952989459037781 + 2.0 * 6.060793876647949
Epoch 470, val loss: 0.8742659687995911
Epoch 480, training loss: 12.688249588012695 = 0.5715603232383728 + 2.0 * 6.058344841003418
Epoch 480, val loss: 0.8604500889778137
Epoch 490, training loss: 12.677730560302734 = 0.5491271018981934 + 2.0 * 6.06430196762085
Epoch 490, val loss: 0.8480933308601379
Epoch 500, training loss: 12.640600204467773 = 0.5280881524085999 + 2.0 * 6.05625581741333
Epoch 500, val loss: 0.8372628092765808
Epoch 510, training loss: 12.615018844604492 = 0.5082707405090332 + 2.0 * 6.05337381362915
Epoch 510, val loss: 0.8276782035827637
Epoch 520, training loss: 12.59847640991211 = 0.48933762311935425 + 2.0 * 6.054569244384766
Epoch 520, val loss: 0.8191601037979126
Epoch 530, training loss: 12.578137397766113 = 0.4714187681674957 + 2.0 * 6.053359508514404
Epoch 530, val loss: 0.8116377592086792
Epoch 540, training loss: 12.554024696350098 = 0.45426875352859497 + 2.0 * 6.049878120422363
Epoch 540, val loss: 0.805161714553833
Epoch 550, training loss: 12.536797523498535 = 0.4377928078174591 + 2.0 * 6.049502372741699
Epoch 550, val loss: 0.7994011640548706
Epoch 560, training loss: 12.517327308654785 = 0.42188844084739685 + 2.0 * 6.047719478607178
Epoch 560, val loss: 0.7943921089172363
Epoch 570, training loss: 12.497306823730469 = 0.4065639078617096 + 2.0 * 6.0453715324401855
Epoch 570, val loss: 0.7901163697242737
Epoch 580, training loss: 12.479242324829102 = 0.3915855884552002 + 2.0 * 6.04382848739624
Epoch 580, val loss: 0.7864346504211426
Epoch 590, training loss: 12.466547012329102 = 0.37696659564971924 + 2.0 * 6.044790267944336
Epoch 590, val loss: 0.7833161950111389
Epoch 600, training loss: 12.447617530822754 = 0.3627811372280121 + 2.0 * 6.042418003082275
Epoch 600, val loss: 0.7808451056480408
Epoch 610, training loss: 12.428842544555664 = 0.34887492656707764 + 2.0 * 6.039983749389648
Epoch 610, val loss: 0.7788944840431213
Epoch 620, training loss: 12.421436309814453 = 0.3352048397064209 + 2.0 * 6.043115615844727
Epoch 620, val loss: 0.777461588382721
Epoch 630, training loss: 12.407876968383789 = 0.32194095849990845 + 2.0 * 6.042967796325684
Epoch 630, val loss: 0.7764672636985779
Epoch 640, training loss: 12.391372680664062 = 0.3090498149394989 + 2.0 * 6.04116153717041
Epoch 640, val loss: 0.7759532332420349
Epoch 650, training loss: 12.370248794555664 = 0.2965623736381531 + 2.0 * 6.036843299865723
Epoch 650, val loss: 0.7758316397666931
Epoch 660, training loss: 12.353760719299316 = 0.2843044102191925 + 2.0 * 6.034728050231934
Epoch 660, val loss: 0.7761058211326599
Epoch 670, training loss: 12.34284782409668 = 0.27235209941864014 + 2.0 * 6.035247802734375
Epoch 670, val loss: 0.7767227292060852
Epoch 680, training loss: 12.329005241394043 = 0.26074180006980896 + 2.0 * 6.0341315269470215
Epoch 680, val loss: 0.777666449546814
Epoch 690, training loss: 12.326037406921387 = 0.24957694113254547 + 2.0 * 6.0382304191589355
Epoch 690, val loss: 0.7789292335510254
Epoch 700, training loss: 12.305191993713379 = 0.23881182074546814 + 2.0 * 6.0331902503967285
Epoch 700, val loss: 0.7804447412490845
Epoch 710, training loss: 12.288410186767578 = 0.22845181822776794 + 2.0 * 6.029979228973389
Epoch 710, val loss: 0.782249927520752
Epoch 720, training loss: 12.277107238769531 = 0.2184496521949768 + 2.0 * 6.0293288230896
Epoch 720, val loss: 0.7842552065849304
Epoch 730, training loss: 12.277390480041504 = 0.20877450704574585 + 2.0 * 6.034307956695557
Epoch 730, val loss: 0.7864881157875061
Epoch 740, training loss: 12.266301155090332 = 0.1996408849954605 + 2.0 * 6.033329963684082
Epoch 740, val loss: 0.7888527512550354
Epoch 750, training loss: 12.253838539123535 = 0.1909133344888687 + 2.0 * 6.031462669372559
Epoch 750, val loss: 0.7915487885475159
Epoch 760, training loss: 12.236218452453613 = 0.18258988857269287 + 2.0 * 6.0268144607543945
Epoch 760, val loss: 0.7943366765975952
Epoch 770, training loss: 12.226298332214355 = 0.17462974786758423 + 2.0 * 6.025834083557129
Epoch 770, val loss: 0.7973530888557434
Epoch 780, training loss: 12.216136932373047 = 0.16702811419963837 + 2.0 * 6.024554252624512
Epoch 780, val loss: 0.8005654811859131
Epoch 790, training loss: 12.219727516174316 = 0.15977594256401062 + 2.0 * 6.029975891113281
Epoch 790, val loss: 0.8039094805717468
Epoch 800, training loss: 12.206087112426758 = 0.15291827917099 + 2.0 * 6.026584625244141
Epoch 800, val loss: 0.8073251247406006
Epoch 810, training loss: 12.195882797241211 = 0.14640912413597107 + 2.0 * 6.0247368812561035
Epoch 810, val loss: 0.8108228445053101
Epoch 820, training loss: 12.192276000976562 = 0.14022095501422882 + 2.0 * 6.026027679443359
Epoch 820, val loss: 0.8144401907920837
Epoch 830, training loss: 12.178056716918945 = 0.13434584438800812 + 2.0 * 6.021855354309082
Epoch 830, val loss: 0.8181759715080261
Epoch 840, training loss: 12.169909477233887 = 0.12874513864517212 + 2.0 * 6.02058219909668
Epoch 840, val loss: 0.8220017552375793
Epoch 850, training loss: 12.168848991394043 = 0.12338529527187347 + 2.0 * 6.022731781005859
Epoch 850, val loss: 0.8259068727493286
Epoch 860, training loss: 12.165048599243164 = 0.11831390112638474 + 2.0 * 6.023367404937744
Epoch 860, val loss: 0.8297439813613892
Epoch 870, training loss: 12.153496742248535 = 0.11349362879991531 + 2.0 * 6.020001411437988
Epoch 870, val loss: 0.8337050676345825
Epoch 880, training loss: 12.144478797912598 = 0.10890265554189682 + 2.0 * 6.017787933349609
Epoch 880, val loss: 0.8377086520195007
Epoch 890, training loss: 12.142241477966309 = 0.10451165586709976 + 2.0 * 6.01886510848999
Epoch 890, val loss: 0.841713011264801
Epoch 900, training loss: 12.135955810546875 = 0.10031747817993164 + 2.0 * 6.017819404602051
Epoch 900, val loss: 0.8456012010574341
Epoch 910, training loss: 12.133851051330566 = 0.09634752571582794 + 2.0 * 6.018751621246338
Epoch 910, val loss: 0.8496054410934448
Epoch 920, training loss: 12.124341011047363 = 0.09255554527044296 + 2.0 * 6.015892505645752
Epoch 920, val loss: 0.8535617589950562
Epoch 930, training loss: 12.12353801727295 = 0.08892128616571426 + 2.0 * 6.017308235168457
Epoch 930, val loss: 0.8574831485748291
Epoch 940, training loss: 12.114862442016602 = 0.0854625254869461 + 2.0 * 6.014699935913086
Epoch 940, val loss: 0.8613646626472473
Epoch 950, training loss: 12.112842559814453 = 0.082156702876091 + 2.0 * 6.015342712402344
Epoch 950, val loss: 0.865303635597229
Epoch 960, training loss: 12.111836433410645 = 0.07899538427591324 + 2.0 * 6.016420364379883
Epoch 960, val loss: 0.869156002998352
Epoch 970, training loss: 12.10254192352295 = 0.07596856355667114 + 2.0 * 6.013286590576172
Epoch 970, val loss: 0.8730179667472839
Epoch 980, training loss: 12.109280586242676 = 0.07307147979736328 + 2.0 * 6.018104553222656
Epoch 980, val loss: 0.8768040537834167
Epoch 990, training loss: 12.095725059509277 = 0.07031489163637161 + 2.0 * 6.012704849243164
Epoch 990, val loss: 0.8805530667304993
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8000
Overall ASR: 0.8893
Flip ASR: 0.8667/225 nodes
The final ASR:0.57565, 0.30502, Accuracy:0.80741, 0.01048
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11604])
remove edge: torch.Size([2, 9510])
updated graph: torch.Size([2, 10558])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9889
Flip ASR: 0.9867/225 nodes
The final ASR:0.97909, 0.00696, Accuracy:0.82963, 0.00605
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.3 der2:0 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 18.695924758911133 = 1.9483777284622192 + 2.0 * 8.373773574829102
Epoch 0, val loss: 1.945313572883606
Epoch 10, training loss: 18.683734893798828 = 1.9381080865859985 + 2.0 * 8.37281322479248
Epoch 10, val loss: 1.9348641633987427
Epoch 20, training loss: 18.65790557861328 = 1.9252614974975586 + 2.0 * 8.366321563720703
Epoch 20, val loss: 1.921632170677185
Epoch 30, training loss: 18.550228118896484 = 1.9083280563354492 + 2.0 * 8.32094955444336
Epoch 30, val loss: 1.9044898748397827
Epoch 40, training loss: 17.915254592895508 = 1.889276146888733 + 2.0 * 8.012989044189453
Epoch 40, val loss: 1.886087417602539
Epoch 50, training loss: 16.33118438720703 = 1.8680206537246704 + 2.0 * 7.231582164764404
Epoch 50, val loss: 1.8659813404083252
Epoch 60, training loss: 15.646955490112305 = 1.853184700012207 + 2.0 * 6.896885395050049
Epoch 60, val loss: 1.8530086278915405
Epoch 70, training loss: 15.2130126953125 = 1.8435769081115723 + 2.0 * 6.684717655181885
Epoch 70, val loss: 1.8441230058670044
Epoch 80, training loss: 14.948516845703125 = 1.8324567079544067 + 2.0 * 6.558030128479004
Epoch 80, val loss: 1.833946704864502
Epoch 90, training loss: 14.786888122558594 = 1.822136640548706 + 2.0 * 6.482375621795654
Epoch 90, val loss: 1.8243907690048218
Epoch 100, training loss: 14.656994819641113 = 1.8130438327789307 + 2.0 * 6.421975612640381
Epoch 100, val loss: 1.8159655332565308
Epoch 110, training loss: 14.54517650604248 = 1.8054677248001099 + 2.0 * 6.36985445022583
Epoch 110, val loss: 1.808821439743042
Epoch 120, training loss: 14.441126823425293 = 1.7984530925750732 + 2.0 * 6.32133674621582
Epoch 120, val loss: 1.8021540641784668
Epoch 130, training loss: 14.347865104675293 = 1.7920591831207275 + 2.0 * 6.277903079986572
Epoch 130, val loss: 1.7961385250091553
Epoch 140, training loss: 14.280613899230957 = 1.785691499710083 + 2.0 * 6.247461318969727
Epoch 140, val loss: 1.7902213335037231
Epoch 150, training loss: 14.226838111877441 = 1.7785810232162476 + 2.0 * 6.224128723144531
Epoch 150, val loss: 1.7836570739746094
Epoch 160, training loss: 14.182891845703125 = 1.7704472541809082 + 2.0 * 6.206222057342529
Epoch 160, val loss: 1.7763195037841797
Epoch 170, training loss: 14.147882461547852 = 1.7613551616668701 + 2.0 * 6.193263530731201
Epoch 170, val loss: 1.7682634592056274
Epoch 180, training loss: 14.10820198059082 = 1.7513701915740967 + 2.0 * 6.178415775299072
Epoch 180, val loss: 1.7596689462661743
Epoch 190, training loss: 14.0717134475708 = 1.7403241395950317 + 2.0 * 6.165694713592529
Epoch 190, val loss: 1.75039541721344
Epoch 200, training loss: 14.037701606750488 = 1.7277518510818481 + 2.0 * 6.154974937438965
Epoch 200, val loss: 1.7400171756744385
Epoch 210, training loss: 14.005464553833008 = 1.7131489515304565 + 2.0 * 6.146157741546631
Epoch 210, val loss: 1.728114128112793
Epoch 220, training loss: 13.976700782775879 = 1.6962388753890991 + 2.0 * 6.140231132507324
Epoch 220, val loss: 1.7144557237625122
Epoch 230, training loss: 13.938776016235352 = 1.6767628192901611 + 2.0 * 6.131006717681885
Epoch 230, val loss: 1.698851466178894
Epoch 240, training loss: 13.903099060058594 = 1.6542332172393799 + 2.0 * 6.1244330406188965
Epoch 240, val loss: 1.6808255910873413
Epoch 250, training loss: 13.864006042480469 = 1.627945899963379 + 2.0 * 6.118030071258545
Epoch 250, val loss: 1.6597540378570557
Epoch 260, training loss: 13.82968521118164 = 1.5972084999084473 + 2.0 * 6.116238594055176
Epoch 260, val loss: 1.6351114511489868
Epoch 270, training loss: 13.780844688415527 = 1.5622138977050781 + 2.0 * 6.109315395355225
Epoch 270, val loss: 1.607024073600769
Epoch 280, training loss: 13.728680610656738 = 1.5226777791976929 + 2.0 * 6.103001594543457
Epoch 280, val loss: 1.5752202272415161
Epoch 290, training loss: 13.675520896911621 = 1.4784692525863647 + 2.0 * 6.0985260009765625
Epoch 290, val loss: 1.5396398305892944
Epoch 300, training loss: 13.623323440551758 = 1.429768681526184 + 2.0 * 6.096777439117432
Epoch 300, val loss: 1.5004512071609497
Epoch 310, training loss: 13.561873435974121 = 1.377794861793518 + 2.0 * 6.092039108276367
Epoch 310, val loss: 1.45889151096344
Epoch 320, training loss: 13.502668380737305 = 1.3235284090042114 + 2.0 * 6.089570045471191
Epoch 320, val loss: 1.4155935049057007
Epoch 330, training loss: 13.439432144165039 = 1.2676869630813599 + 2.0 * 6.085872650146484
Epoch 330, val loss: 1.3714650869369507
Epoch 340, training loss: 13.380987167358398 = 1.2108538150787354 + 2.0 * 6.085066795349121
Epoch 340, val loss: 1.326798915863037
Epoch 350, training loss: 13.317941665649414 = 1.1541348695755005 + 2.0 * 6.081903457641602
Epoch 350, val loss: 1.2825738191604614
Epoch 360, training loss: 13.25527286529541 = 1.0982528924942017 + 2.0 * 6.07850980758667
Epoch 360, val loss: 1.238863229751587
Epoch 370, training loss: 13.19375228881836 = 1.04288649559021 + 2.0 * 6.075432777404785
Epoch 370, val loss: 1.195608377456665
Epoch 380, training loss: 13.14023494720459 = 0.9881849884986877 + 2.0 * 6.076025009155273
Epoch 380, val loss: 1.1528410911560059
Epoch 390, training loss: 13.082439422607422 = 0.9358517527580261 + 2.0 * 6.073293685913086
Epoch 390, val loss: 1.11161208152771
Epoch 400, training loss: 13.02519416809082 = 0.8861138224601746 + 2.0 * 6.069540023803711
Epoch 400, val loss: 1.0725544691085815
Epoch 410, training loss: 12.972843170166016 = 0.8387977480888367 + 2.0 * 6.067022800445557
Epoch 410, val loss: 1.0352052450180054
Epoch 420, training loss: 12.92419719696045 = 0.7939016222953796 + 2.0 * 6.065147876739502
Epoch 420, val loss: 0.9997934699058533
Epoch 430, training loss: 12.880193710327148 = 0.7518786191940308 + 2.0 * 6.064157485961914
Epoch 430, val loss: 0.9670623540878296
Epoch 440, training loss: 12.840071678161621 = 0.713071882724762 + 2.0 * 6.063499927520752
Epoch 440, val loss: 0.9371644258499146
Epoch 450, training loss: 12.797168731689453 = 0.6770126819610596 + 2.0 * 6.060078144073486
Epoch 450, val loss: 0.9098811745643616
Epoch 460, training loss: 12.75808334350586 = 0.6432037353515625 + 2.0 * 6.057439804077148
Epoch 460, val loss: 0.8850221633911133
Epoch 470, training loss: 12.725117683410645 = 0.6113006472587585 + 2.0 * 6.05690860748291
Epoch 470, val loss: 0.8622972369194031
Epoch 480, training loss: 12.70777416229248 = 0.5812128186225891 + 2.0 * 6.0632805824279785
Epoch 480, val loss: 0.8416188955307007
Epoch 490, training loss: 12.664972305297852 = 0.5531144738197327 + 2.0 * 6.055928707122803
Epoch 490, val loss: 0.8230994939804077
Epoch 500, training loss: 12.627531051635742 = 0.5265395641326904 + 2.0 * 6.050495624542236
Epoch 500, val loss: 0.8063868284225464
Epoch 510, training loss: 12.599420547485352 = 0.5010674595832825 + 2.0 * 6.0491766929626465
Epoch 510, val loss: 0.790984034538269
Epoch 520, training loss: 12.579678535461426 = 0.4765739440917969 + 2.0 * 6.0515522956848145
Epoch 520, val loss: 0.776799201965332
Epoch 530, training loss: 12.550880432128906 = 0.4532874822616577 + 2.0 * 6.048796653747559
Epoch 530, val loss: 0.7638772130012512
Epoch 540, training loss: 12.520858764648438 = 0.4309382140636444 + 2.0 * 6.0449604988098145
Epoch 540, val loss: 0.7522130012512207
Epoch 550, training loss: 12.501572608947754 = 0.40952640771865845 + 2.0 * 6.046022891998291
Epoch 550, val loss: 0.7416245341300964
Epoch 560, training loss: 12.475179672241211 = 0.38911494612693787 + 2.0 * 6.043032169342041
Epoch 560, val loss: 0.732062041759491
Epoch 570, training loss: 12.470329284667969 = 0.3696029484272003 + 2.0 * 6.050363063812256
Epoch 570, val loss: 0.7234684824943542
Epoch 580, training loss: 12.433208465576172 = 0.3511193096637726 + 2.0 * 6.04104471206665
Epoch 580, val loss: 0.715945303440094
Epoch 590, training loss: 12.411824226379395 = 0.33354073762893677 + 2.0 * 6.039141654968262
Epoch 590, val loss: 0.7093793749809265
Epoch 600, training loss: 12.391407012939453 = 0.3167104125022888 + 2.0 * 6.03734827041626
Epoch 600, val loss: 0.7036345601081848
Epoch 610, training loss: 12.383562088012695 = 0.30057063698768616 + 2.0 * 6.0414958000183105
Epoch 610, val loss: 0.6987221240997314
Epoch 620, training loss: 12.364114761352539 = 0.28535568714141846 + 2.0 * 6.039379596710205
Epoch 620, val loss: 0.6946178674697876
Epoch 630, training loss: 12.352860450744629 = 0.27089011669158936 + 2.0 * 6.040985107421875
Epoch 630, val loss: 0.6913559436798096
Epoch 640, training loss: 12.325799942016602 = 0.2572169005870819 + 2.0 * 6.034291744232178
Epoch 640, val loss: 0.688782274723053
Epoch 650, training loss: 12.309028625488281 = 0.2441583126783371 + 2.0 * 6.032434940338135
Epoch 650, val loss: 0.6868489384651184
Epoch 660, training loss: 12.29352855682373 = 0.23167192935943604 + 2.0 * 6.030928134918213
Epoch 660, val loss: 0.685547411441803
Epoch 670, training loss: 12.302085876464844 = 0.219755619764328 + 2.0 * 6.041165351867676
Epoch 670, val loss: 0.6848316788673401
Epoch 680, training loss: 12.269488334655762 = 0.20855668187141418 + 2.0 * 6.030465602874756
Epoch 680, val loss: 0.6846632957458496
Epoch 690, training loss: 12.254755020141602 = 0.19795115292072296 + 2.0 * 6.028401851654053
Epoch 690, val loss: 0.685080349445343
Epoch 700, training loss: 12.244548797607422 = 0.1878516972064972 + 2.0 * 6.028348445892334
Epoch 700, val loss: 0.6859938502311707
Epoch 710, training loss: 12.234299659729004 = 0.1782941222190857 + 2.0 * 6.028002738952637
Epoch 710, val loss: 0.6873277425765991
Epoch 720, training loss: 12.222110748291016 = 0.16926942765712738 + 2.0 * 6.026420593261719
Epoch 720, val loss: 0.6891373991966248
Epoch 730, training loss: 12.212449073791504 = 0.160723477602005 + 2.0 * 6.025862693786621
Epoch 730, val loss: 0.6913859248161316
Epoch 740, training loss: 12.202957153320312 = 0.15266112983226776 + 2.0 * 6.025147914886475
Epoch 740, val loss: 0.6939838528633118
Epoch 750, training loss: 12.199045181274414 = 0.14506538212299347 + 2.0 * 6.026989936828613
Epoch 750, val loss: 0.6969030499458313
Epoch 760, training loss: 12.186304092407227 = 0.13793502748012543 + 2.0 * 6.024184703826904
Epoch 760, val loss: 0.7001181244850159
Epoch 770, training loss: 12.174762725830078 = 0.1312440037727356 + 2.0 * 6.021759510040283
Epoch 770, val loss: 0.7036663293838501
Epoch 780, training loss: 12.166370391845703 = 0.12491034716367722 + 2.0 * 6.020730018615723
Epoch 780, val loss: 0.7074970006942749
Epoch 790, training loss: 12.171735763549805 = 0.11895357817411423 + 2.0 * 6.02639102935791
Epoch 790, val loss: 0.7115143537521362
Epoch 800, training loss: 12.153520584106445 = 0.11342588067054749 + 2.0 * 6.020047187805176
Epoch 800, val loss: 0.715759813785553
Epoch 810, training loss: 12.145530700683594 = 0.10820623487234116 + 2.0 * 6.018662452697754
Epoch 810, val loss: 0.7202061414718628
Epoch 820, training loss: 12.142242431640625 = 0.10328252613544464 + 2.0 * 6.019479751586914
Epoch 820, val loss: 0.7248221039772034
Epoch 830, training loss: 12.132752418518066 = 0.09864696860313416 + 2.0 * 6.01705265045166
Epoch 830, val loss: 0.7295839786529541
Epoch 840, training loss: 12.128050804138184 = 0.09428124129772186 + 2.0 * 6.016884803771973
Epoch 840, val loss: 0.7344540357589722
Epoch 850, training loss: 12.129993438720703 = 0.09019212424755096 + 2.0 * 6.019900798797607
Epoch 850, val loss: 0.7393829226493835
Epoch 860, training loss: 12.115409851074219 = 0.08635328710079193 + 2.0 * 6.014528274536133
Epoch 860, val loss: 0.7443994283676147
Epoch 870, training loss: 12.110342979431152 = 0.08272075653076172 + 2.0 * 6.013811111450195
Epoch 870, val loss: 0.7495436668395996
Epoch 880, training loss: 12.110466003417969 = 0.07928241789340973 + 2.0 * 6.015591621398926
Epoch 880, val loss: 0.7547301054000854
Epoch 890, training loss: 12.103534698486328 = 0.07604437321424484 + 2.0 * 6.013745307922363
Epoch 890, val loss: 0.7598322033882141
Epoch 900, training loss: 12.099198341369629 = 0.07301050424575806 + 2.0 * 6.013093948364258
Epoch 900, val loss: 0.765105664730072
Epoch 910, training loss: 12.09207820892334 = 0.07011882215738297 + 2.0 * 6.010979652404785
Epoch 910, val loss: 0.7704177498817444
Epoch 920, training loss: 12.109882354736328 = 0.06737715005874634 + 2.0 * 6.021252632141113
Epoch 920, val loss: 0.7757319808006287
Epoch 930, training loss: 12.088580131530762 = 0.06481793522834778 + 2.0 * 6.011880874633789
Epoch 930, val loss: 0.7810047268867493
Epoch 940, training loss: 12.082626342773438 = 0.06238926202058792 + 2.0 * 6.01011848449707
Epoch 940, val loss: 0.7863955497741699
Epoch 950, training loss: 12.077204704284668 = 0.06007642298936844 + 2.0 * 6.008563995361328
Epoch 950, val loss: 0.7917875051498413
Epoch 960, training loss: 12.07833480834961 = 0.05787362903356552 + 2.0 * 6.010230541229248
Epoch 960, val loss: 0.7971988320350647
Epoch 970, training loss: 12.071097373962402 = 0.05579343065619469 + 2.0 * 6.0076518058776855
Epoch 970, val loss: 0.8025596141815186
Epoch 980, training loss: 12.069106101989746 = 0.05382402986288071 + 2.0 * 6.007640838623047
Epoch 980, val loss: 0.8079503774642944
Epoch 990, training loss: 12.070013046264648 = 0.05194742605090141 + 2.0 * 6.009032726287842
Epoch 990, val loss: 0.8132922649383545
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.7638
Flip ASR: 0.7156/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.679851531982422 = 1.9322972297668457 + 2.0 * 8.373777389526367
Epoch 0, val loss: 1.9322320222854614
Epoch 10, training loss: 18.668598175048828 = 1.9225244522094727 + 2.0 * 8.37303638458252
Epoch 10, val loss: 1.9231592416763306
Epoch 20, training loss: 18.646888732910156 = 1.9103538990020752 + 2.0 * 8.368267059326172
Epoch 20, val loss: 1.911417841911316
Epoch 30, training loss: 18.565738677978516 = 1.8939847946166992 + 2.0 * 8.33587646484375
Epoch 30, val loss: 1.8953644037246704
Epoch 40, training loss: 18.111879348754883 = 1.8746311664581299 + 2.0 * 8.118623733520508
Epoch 40, val loss: 1.8766756057739258
Epoch 50, training loss: 16.406675338745117 = 1.853935956954956 + 2.0 * 7.276370048522949
Epoch 50, val loss: 1.856905460357666
Epoch 60, training loss: 15.608054161071777 = 1.838751196861267 + 2.0 * 6.8846516609191895
Epoch 60, val loss: 1.8432122468948364
Epoch 70, training loss: 15.180880546569824 = 1.8291683197021484 + 2.0 * 6.675856113433838
Epoch 70, val loss: 1.8337088823318481
Epoch 80, training loss: 14.921430587768555 = 1.81865656375885 + 2.0 * 6.551386833190918
Epoch 80, val loss: 1.822830319404602
Epoch 90, training loss: 14.747918128967285 = 1.8091763257980347 + 2.0 * 6.4693708419799805
Epoch 90, val loss: 1.8124115467071533
Epoch 100, training loss: 14.6225004196167 = 1.8006285429000854 + 2.0 * 6.410935878753662
Epoch 100, val loss: 1.8028895854949951
Epoch 110, training loss: 14.50472354888916 = 1.7932367324829102 + 2.0 * 6.355743408203125
Epoch 110, val loss: 1.7946605682373047
Epoch 120, training loss: 14.41276741027832 = 1.7862639427185059 + 2.0 * 6.313251972198486
Epoch 120, val loss: 1.7872099876403809
Epoch 130, training loss: 14.341780662536621 = 1.7786279916763306 + 2.0 * 6.281576156616211
Epoch 130, val loss: 1.779463291168213
Epoch 140, training loss: 14.278426170349121 = 1.7698184251785278 + 2.0 * 6.254303932189941
Epoch 140, val loss: 1.7711607217788696
Epoch 150, training loss: 14.223028182983398 = 1.760231852531433 + 2.0 * 6.231398105621338
Epoch 150, val loss: 1.7624539136886597
Epoch 160, training loss: 14.175264358520508 = 1.7496285438537598 + 2.0 * 6.212818145751953
Epoch 160, val loss: 1.753175139427185
Epoch 170, training loss: 14.133682250976562 = 1.7374746799468994 + 2.0 * 6.198103904724121
Epoch 170, val loss: 1.7428929805755615
Epoch 180, training loss: 14.094717979431152 = 1.7232449054718018 + 2.0 * 6.185736656188965
Epoch 180, val loss: 1.7311911582946777
Epoch 190, training loss: 14.056717872619629 = 1.7066453695297241 + 2.0 * 6.175036430358887
Epoch 190, val loss: 1.7177759408950806
Epoch 200, training loss: 14.01576042175293 = 1.6874587535858154 + 2.0 * 6.164150714874268
Epoch 200, val loss: 1.702392339706421
Epoch 210, training loss: 13.974679946899414 = 1.6650370359420776 + 2.0 * 6.154821395874023
Epoch 210, val loss: 1.6844664812088013
Epoch 220, training loss: 13.933060646057129 = 1.6385470628738403 + 2.0 * 6.147256851196289
Epoch 220, val loss: 1.6634232997894287
Epoch 230, training loss: 13.88817024230957 = 1.6080055236816406 + 2.0 * 6.140082359313965
Epoch 230, val loss: 1.6390609741210938
Epoch 240, training loss: 13.838342666625977 = 1.572812557220459 + 2.0 * 6.132765293121338
Epoch 240, val loss: 1.6110827922821045
Epoch 250, training loss: 13.786237716674805 = 1.5324339866638184 + 2.0 * 6.126901626586914
Epoch 250, val loss: 1.5787606239318848
Epoch 260, training loss: 13.730005264282227 = 1.48640775680542 + 2.0 * 6.121798992156982
Epoch 260, val loss: 1.5416028499603271
Epoch 270, training loss: 13.677658081054688 = 1.4357091188430786 + 2.0 * 6.120974540710449
Epoch 270, val loss: 1.5005512237548828
Epoch 280, training loss: 13.610906600952148 = 1.3818398714065552 + 2.0 * 6.114533424377441
Epoch 280, val loss: 1.456884503364563
Epoch 290, training loss: 13.54455852508545 = 1.325275182723999 + 2.0 * 6.1096415519714355
Epoch 290, val loss: 1.4109448194503784
Epoch 300, training loss: 13.478511810302734 = 1.2668869495391846 + 2.0 * 6.1058125495910645
Epoch 300, val loss: 1.3634889125823975
Epoch 310, training loss: 13.414555549621582 = 1.2084205150604248 + 2.0 * 6.103067398071289
Epoch 310, val loss: 1.3161391019821167
Epoch 320, training loss: 13.351461410522461 = 1.1514060497283936 + 2.0 * 6.100027561187744
Epoch 320, val loss: 1.2703843116760254
Epoch 330, training loss: 13.287033081054688 = 1.096773624420166 + 2.0 * 6.09512996673584
Epoch 330, val loss: 1.2266288995742798
Epoch 340, training loss: 13.228302955627441 = 1.0445690155029297 + 2.0 * 6.091866970062256
Epoch 340, val loss: 1.1854212284088135
Epoch 350, training loss: 13.176427841186523 = 0.9953091144561768 + 2.0 * 6.090559482574463
Epoch 350, val loss: 1.1472219228744507
Epoch 360, training loss: 13.118494033813477 = 0.9496369361877441 + 2.0 * 6.084428787231445
Epoch 360, val loss: 1.1121370792388916
Epoch 370, training loss: 13.067972183227539 = 0.9070329666137695 + 2.0 * 6.080469608306885
Epoch 370, val loss: 1.0801775455474854
Epoch 380, training loss: 13.026284217834473 = 0.8672828674316406 + 2.0 * 6.079500675201416
Epoch 380, val loss: 1.0508742332458496
Epoch 390, training loss: 12.99135971069336 = 0.830605685710907 + 2.0 * 6.080377101898193
Epoch 390, val loss: 1.0243104696273804
Epoch 400, training loss: 12.942261695861816 = 0.796587347984314 + 2.0 * 6.0728373527526855
Epoch 400, val loss: 1.0004225969314575
Epoch 410, training loss: 12.90288257598877 = 0.7646773457527161 + 2.0 * 6.069102764129639
Epoch 410, val loss: 0.9784911274909973
Epoch 420, training loss: 12.869729995727539 = 0.734290599822998 + 2.0 * 6.067719459533691
Epoch 420, val loss: 0.9580138921737671
Epoch 430, training loss: 12.843551635742188 = 0.7055162787437439 + 2.0 * 6.0690178871154785
Epoch 430, val loss: 0.9390751719474792
Epoch 440, training loss: 12.804115295410156 = 0.6780796051025391 + 2.0 * 6.063017845153809
Epoch 440, val loss: 0.9214676022529602
Epoch 450, training loss: 12.771929740905762 = 0.6514192819595337 + 2.0 * 6.06025505065918
Epoch 450, val loss: 0.9047169089317322
Epoch 460, training loss: 12.751076698303223 = 0.6253576874732971 + 2.0 * 6.062859535217285
Epoch 460, val loss: 0.888664960861206
Epoch 470, training loss: 12.715890884399414 = 0.6001163721084595 + 2.0 * 6.057887077331543
Epoch 470, val loss: 0.873353123664856
Epoch 480, training loss: 12.688665390014648 = 0.5753088593482971 + 2.0 * 6.056678295135498
Epoch 480, val loss: 0.8586880564689636
Epoch 490, training loss: 12.66169548034668 = 0.5511435866355896 + 2.0 * 6.055275917053223
Epoch 490, val loss: 0.8445815443992615
Epoch 500, training loss: 12.629276275634766 = 0.527532696723938 + 2.0 * 6.050871849060059
Epoch 500, val loss: 0.8312664031982422
Epoch 510, training loss: 12.603943824768066 = 0.5044147372245789 + 2.0 * 6.049764633178711
Epoch 510, val loss: 0.8186139464378357
Epoch 520, training loss: 12.579246520996094 = 0.4819553792476654 + 2.0 * 6.048645496368408
Epoch 520, val loss: 0.8066849708557129
Epoch 530, training loss: 12.553939819335938 = 0.460359126329422 + 2.0 * 6.04679012298584
Epoch 530, val loss: 0.7958090901374817
Epoch 540, training loss: 12.530204772949219 = 0.4394681453704834 + 2.0 * 6.045368194580078
Epoch 540, val loss: 0.7858451008796692
Epoch 550, training loss: 12.507773399353027 = 0.41921716928482056 + 2.0 * 6.044278144836426
Epoch 550, val loss: 0.7767082452774048
Epoch 560, training loss: 12.490509986877441 = 0.3996981978416443 + 2.0 * 6.045405864715576
Epoch 560, val loss: 0.768454372882843
Epoch 570, training loss: 12.4680814743042 = 0.38100680708885193 + 2.0 * 6.043537139892578
Epoch 570, val loss: 0.7611836194992065
Epoch 580, training loss: 12.445943832397461 = 0.3630680739879608 + 2.0 * 6.041438102722168
Epoch 580, val loss: 0.7547969222068787
Epoch 590, training loss: 12.425424575805664 = 0.3458653688430786 + 2.0 * 6.0397796630859375
Epoch 590, val loss: 0.7491679191589355
Epoch 600, training loss: 12.408550262451172 = 0.32938891649246216 + 2.0 * 6.039580821990967
Epoch 600, val loss: 0.744326651096344
Epoch 610, training loss: 12.39880657196045 = 0.31367024779319763 + 2.0 * 6.042568206787109
Epoch 610, val loss: 0.7402108311653137
Epoch 620, training loss: 12.376619338989258 = 0.29872167110443115 + 2.0 * 6.038949012756348
Epoch 620, val loss: 0.7367353439331055
Epoch 630, training loss: 12.356273651123047 = 0.2845156192779541 + 2.0 * 6.035879135131836
Epoch 630, val loss: 0.7339335680007935
Epoch 640, training loss: 12.339646339416504 = 0.27096229791641235 + 2.0 * 6.034341812133789
Epoch 640, val loss: 0.731713593006134
Epoch 650, training loss: 12.335251808166504 = 0.2580169439315796 + 2.0 * 6.0386176109313965
Epoch 650, val loss: 0.7299217581748962
Epoch 660, training loss: 12.317995071411133 = 0.24578052759170532 + 2.0 * 6.036107063293457
Epoch 660, val loss: 0.7286207675933838
Epoch 670, training loss: 12.298635482788086 = 0.23421505093574524 + 2.0 * 6.032210350036621
Epoch 670, val loss: 0.7279723286628723
Epoch 680, training loss: 12.285202980041504 = 0.22319655120372772 + 2.0 * 6.031002998352051
Epoch 680, val loss: 0.7277210354804993
Epoch 690, training loss: 12.280169486999512 = 0.21267865598201752 + 2.0 * 6.033745288848877
Epoch 690, val loss: 0.727742612361908
Epoch 700, training loss: 12.270425796508789 = 0.20273685455322266 + 2.0 * 6.033844470977783
Epoch 700, val loss: 0.7281586527824402
Epoch 710, training loss: 12.25035285949707 = 0.19334374368190765 + 2.0 * 6.028504371643066
Epoch 710, val loss: 0.7290498614311218
Epoch 720, training loss: 12.239690780639648 = 0.18440096080303192 + 2.0 * 6.027645111083984
Epoch 720, val loss: 0.7303354740142822
Epoch 730, training loss: 12.23603630065918 = 0.17589737474918365 + 2.0 * 6.030069351196289
Epoch 730, val loss: 0.7318820357322693
Epoch 740, training loss: 12.22176742553711 = 0.16783227026462555 + 2.0 * 6.026967525482178
Epoch 740, val loss: 0.7336338758468628
Epoch 750, training loss: 12.223457336425781 = 0.16020797193050385 + 2.0 * 6.031624794006348
Epoch 750, val loss: 0.735753059387207
Epoch 760, training loss: 12.204041481018066 = 0.15305209159851074 + 2.0 * 6.025494575500488
Epoch 760, val loss: 0.7381801605224609
Epoch 770, training loss: 12.193679809570312 = 0.14624159038066864 + 2.0 * 6.023719310760498
Epoch 770, val loss: 0.7409345507621765
Epoch 780, training loss: 12.18455982208252 = 0.13975682854652405 + 2.0 * 6.022401332855225
Epoch 780, val loss: 0.7438752055168152
Epoch 790, training loss: 12.180530548095703 = 0.13358528912067413 + 2.0 * 6.023472785949707
Epoch 790, val loss: 0.7469912767410278
Epoch 800, training loss: 12.17387866973877 = 0.12775589525699615 + 2.0 * 6.023061275482178
Epoch 800, val loss: 0.7501969337463379
Epoch 810, training loss: 12.1666898727417 = 0.1222909465432167 + 2.0 * 6.022199630737305
Epoch 810, val loss: 0.7536752223968506
Epoch 820, training loss: 12.158201217651367 = 0.11711805313825607 + 2.0 * 6.020541667938232
Epoch 820, val loss: 0.757428765296936
Epoch 830, training loss: 12.160284996032715 = 0.11219297349452972 + 2.0 * 6.024045944213867
Epoch 830, val loss: 0.7612150311470032
Epoch 840, training loss: 12.148289680480957 = 0.10754977911710739 + 2.0 * 6.020370006561279
Epoch 840, val loss: 0.765090823173523
Epoch 850, training loss: 12.144350051879883 = 0.10313525050878525 + 2.0 * 6.0206074714660645
Epoch 850, val loss: 0.7691149115562439
Epoch 860, training loss: 12.134480476379395 = 0.09896017611026764 + 2.0 * 6.017760276794434
Epoch 860, val loss: 0.7732794880867004
Epoch 870, training loss: 12.134337425231934 = 0.09500104933977127 + 2.0 * 6.019668102264404
Epoch 870, val loss: 0.7775656580924988
Epoch 880, training loss: 12.125788688659668 = 0.09125030785799026 + 2.0 * 6.017269134521484
Epoch 880, val loss: 0.7817211747169495
Epoch 890, training loss: 12.119537353515625 = 0.08769369125366211 + 2.0 * 6.0159220695495605
Epoch 890, val loss: 0.7860777378082275
Epoch 900, training loss: 12.113673210144043 = 0.08430904150009155 + 2.0 * 6.014682292938232
Epoch 900, val loss: 0.790522575378418
Epoch 910, training loss: 12.11800765991211 = 0.08109045773744583 + 2.0 * 6.018458366394043
Epoch 910, val loss: 0.7948970198631287
Epoch 920, training loss: 12.114258766174316 = 0.07804491370916367 + 2.0 * 6.018106937408447
Epoch 920, val loss: 0.7992337942123413
Epoch 930, training loss: 12.106810569763184 = 0.07517126202583313 + 2.0 * 6.015819549560547
Epoch 930, val loss: 0.803618848323822
Epoch 940, training loss: 12.098740577697754 = 0.07244078069925308 + 2.0 * 6.013149738311768
Epoch 940, val loss: 0.8080501556396484
Epoch 950, training loss: 12.10097885131836 = 0.06984267383813858 + 2.0 * 6.015568256378174
Epoch 950, val loss: 0.8124818205833435
Epoch 960, training loss: 12.090082168579102 = 0.06737817823886871 + 2.0 * 6.011352062225342
Epoch 960, val loss: 0.8169112205505371
Epoch 970, training loss: 12.086666107177734 = 0.06502432376146317 + 2.0 * 6.0108208656311035
Epoch 970, val loss: 0.8213375210762024
Epoch 980, training loss: 12.09465217590332 = 0.06277720630168915 + 2.0 * 6.015937328338623
Epoch 980, val loss: 0.8257418870925903
Epoch 990, training loss: 12.084332466125488 = 0.060648027807474136 + 2.0 * 6.011842250823975
Epoch 990, val loss: 0.8299772143363953
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8074
Overall ASR: 0.9705
Flip ASR: 0.9644/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.683597564697266 = 1.936136245727539 + 2.0 * 8.373730659484863
Epoch 0, val loss: 1.9292685985565186
Epoch 10, training loss: 18.670368194580078 = 1.9262127876281738 + 2.0 * 8.372077941894531
Epoch 10, val loss: 1.9194496870040894
Epoch 20, training loss: 18.64301300048828 = 1.9142225980758667 + 2.0 * 8.364395141601562
Epoch 20, val loss: 1.9069645404815674
Epoch 30, training loss: 18.55666732788086 = 1.8986239433288574 + 2.0 * 8.329021453857422
Epoch 30, val loss: 1.8902143239974976
Epoch 40, training loss: 18.120445251464844 = 1.881424069404602 + 2.0 * 8.119510650634766
Epoch 40, val loss: 1.872071623802185
Epoch 50, training loss: 16.605350494384766 = 1.8640984296798706 + 2.0 * 7.370625972747803
Epoch 50, val loss: 1.8527776002883911
Epoch 60, training loss: 15.935149192810059 = 1.8455617427825928 + 2.0 * 7.044793605804443
Epoch 60, val loss: 1.8328006267547607
Epoch 70, training loss: 15.376485824584961 = 1.8293914794921875 + 2.0 * 6.773547172546387
Epoch 70, val loss: 1.8161267042160034
Epoch 80, training loss: 14.998973846435547 = 1.8161811828613281 + 2.0 * 6.591396331787109
Epoch 80, val loss: 1.8029457330703735
Epoch 90, training loss: 14.771241188049316 = 1.8059879541397095 + 2.0 * 6.482626438140869
Epoch 90, val loss: 1.7921323776245117
Epoch 100, training loss: 14.62199878692627 = 1.796181559562683 + 2.0 * 6.412908554077148
Epoch 100, val loss: 1.7820615768432617
Epoch 110, training loss: 14.490656852722168 = 1.7872087955474854 + 2.0 * 6.351724147796631
Epoch 110, val loss: 1.7731633186340332
Epoch 120, training loss: 14.386058807373047 = 1.7788487672805786 + 2.0 * 6.303605079650879
Epoch 120, val loss: 1.7648710012435913
Epoch 130, training loss: 14.317519187927246 = 1.7701199054718018 + 2.0 * 6.273699760437012
Epoch 130, val loss: 1.7562061548233032
Epoch 140, training loss: 14.26002025604248 = 1.7607308626174927 + 2.0 * 6.249644756317139
Epoch 140, val loss: 1.7474110126495361
Epoch 150, training loss: 14.21052360534668 = 1.7506897449493408 + 2.0 * 6.229917049407959
Epoch 150, val loss: 1.7384761571884155
Epoch 160, training loss: 14.164461135864258 = 1.7399444580078125 + 2.0 * 6.212258338928223
Epoch 160, val loss: 1.729314923286438
Epoch 170, training loss: 14.1198091506958 = 1.7280913591384888 + 2.0 * 6.195858955383301
Epoch 170, val loss: 1.7195717096328735
Epoch 180, training loss: 14.084365844726562 = 1.7146809101104736 + 2.0 * 6.184842586517334
Epoch 180, val loss: 1.709002137184143
Epoch 190, training loss: 14.036795616149902 = 1.6998698711395264 + 2.0 * 6.168462753295898
Epoch 190, val loss: 1.6973110437393188
Epoch 200, training loss: 13.99359130859375 = 1.6827638149261475 + 2.0 * 6.155413627624512
Epoch 200, val loss: 1.6842782497406006
Epoch 210, training loss: 13.951242446899414 = 1.662827730178833 + 2.0 * 6.14420747756958
Epoch 210, val loss: 1.6690300703048706
Epoch 220, training loss: 13.920087814331055 = 1.6392788887023926 + 2.0 * 6.14040470123291
Epoch 220, val loss: 1.651017427444458
Epoch 230, training loss: 13.867515563964844 = 1.611733317375183 + 2.0 * 6.1278910636901855
Epoch 230, val loss: 1.6300681829452515
Epoch 240, training loss: 13.821307182312012 = 1.5797007083892822 + 2.0 * 6.120803356170654
Epoch 240, val loss: 1.6056326627731323
Epoch 250, training loss: 13.770966529846191 = 1.5426359176635742 + 2.0 * 6.114165306091309
Epoch 250, val loss: 1.577307939529419
Epoch 260, training loss: 13.71845817565918 = 1.5000956058502197 + 2.0 * 6.1091814041137695
Epoch 260, val loss: 1.5448576211929321
Epoch 270, training loss: 13.660958290100098 = 1.4529703855514526 + 2.0 * 6.103993892669678
Epoch 270, val loss: 1.5089519023895264
Epoch 280, training loss: 13.600619316101074 = 1.402100682258606 + 2.0 * 6.099259376525879
Epoch 280, val loss: 1.4704432487487793
Epoch 290, training loss: 13.548490524291992 = 1.348662257194519 + 2.0 * 6.099914073944092
Epoch 290, val loss: 1.430593490600586
Epoch 300, training loss: 13.48045825958252 = 1.2952845096588135 + 2.0 * 6.092586994171143
Epoch 300, val loss: 1.3909049034118652
Epoch 310, training loss: 13.418187141418457 = 1.2421033382415771 + 2.0 * 6.08804178237915
Epoch 310, val loss: 1.3517769575119019
Epoch 320, training loss: 13.3578462600708 = 1.1894601583480835 + 2.0 * 6.084193229675293
Epoch 320, val loss: 1.3136117458343506
Epoch 330, training loss: 13.301568984985352 = 1.1378636360168457 + 2.0 * 6.081852912902832
Epoch 330, val loss: 1.2766926288604736
Epoch 340, training loss: 13.250178337097168 = 1.0887136459350586 + 2.0 * 6.080732345581055
Epoch 340, val loss: 1.2412806749343872
Epoch 350, training loss: 13.193621635437012 = 1.0420796871185303 + 2.0 * 6.075770854949951
Epoch 350, val loss: 1.20801842212677
Epoch 360, training loss: 13.140055656433105 = 0.9972090125083923 + 2.0 * 6.071423530578613
Epoch 360, val loss: 1.1761130094528198
Epoch 370, training loss: 13.090642929077148 = 0.9538357257843018 + 2.0 * 6.068403720855713
Epoch 370, val loss: 1.145266056060791
Epoch 380, training loss: 13.052826881408691 = 0.9119802713394165 + 2.0 * 6.070423126220703
Epoch 380, val loss: 1.1154468059539795
Epoch 390, training loss: 13.000374794006348 = 0.8722924590110779 + 2.0 * 6.0640411376953125
Epoch 390, val loss: 1.0877190828323364
Epoch 400, training loss: 12.957261085510254 = 0.8350024819374084 + 2.0 * 6.061129093170166
Epoch 400, val loss: 1.0618091821670532
Epoch 410, training loss: 12.916048049926758 = 0.7994416356086731 + 2.0 * 6.058303356170654
Epoch 410, val loss: 1.0374934673309326
Epoch 420, training loss: 12.895709037780762 = 0.7658284306526184 + 2.0 * 6.064940452575684
Epoch 420, val loss: 1.0146034955978394
Epoch 430, training loss: 12.845254898071289 = 0.7343916296958923 + 2.0 * 6.055431842803955
Epoch 430, val loss: 0.9938509464263916
Epoch 440, training loss: 12.808002471923828 = 0.7048807740211487 + 2.0 * 6.051560878753662
Epoch 440, val loss: 0.9747359752655029
Epoch 450, training loss: 12.776057243347168 = 0.6769403219223022 + 2.0 * 6.049558639526367
Epoch 450, val loss: 0.9570059180259705
Epoch 460, training loss: 12.746516227722168 = 0.650558352470398 + 2.0 * 6.04797887802124
Epoch 460, val loss: 0.9405633807182312
Epoch 470, training loss: 12.721977233886719 = 0.625860869884491 + 2.0 * 6.048058032989502
Epoch 470, val loss: 0.925674319267273
Epoch 480, training loss: 12.695374488830566 = 0.6024059653282166 + 2.0 * 6.046484470367432
Epoch 480, val loss: 0.9120997190475464
Epoch 490, training loss: 12.66695499420166 = 0.5799318552017212 + 2.0 * 6.043511390686035
Epoch 490, val loss: 0.8997712731361389
Epoch 500, training loss: 12.645487785339355 = 0.5583626627922058 + 2.0 * 6.043562412261963
Epoch 500, val loss: 0.8883594274520874
Epoch 510, training loss: 12.622540473937988 = 0.5377027988433838 + 2.0 * 6.042418956756592
Epoch 510, val loss: 0.8779222369194031
Epoch 520, training loss: 12.596927642822266 = 0.517668604850769 + 2.0 * 6.0396294593811035
Epoch 520, val loss: 0.8685148358345032
Epoch 530, training loss: 12.57652473449707 = 0.49835261702537537 + 2.0 * 6.039085865020752
Epoch 530, val loss: 0.8599961400032043
Epoch 540, training loss: 12.557316780090332 = 0.4797358810901642 + 2.0 * 6.038790225982666
Epoch 540, val loss: 0.8522083759307861
Epoch 550, training loss: 12.534770011901855 = 0.46186381578445435 + 2.0 * 6.0364532470703125
Epoch 550, val loss: 0.8453340530395508
Epoch 560, training loss: 12.512807846069336 = 0.4446266293525696 + 2.0 * 6.034090518951416
Epoch 560, val loss: 0.8394688963890076
Epoch 570, training loss: 12.49376392364502 = 0.427948534488678 + 2.0 * 6.032907485961914
Epoch 570, val loss: 0.8343991041183472
Epoch 580, training loss: 12.479413032531738 = 0.4118443727493286 + 2.0 * 6.03378438949585
Epoch 580, val loss: 0.829831600189209
Epoch 590, training loss: 12.45670223236084 = 0.3963892459869385 + 2.0 * 6.03015661239624
Epoch 590, val loss: 0.8260144591331482
Epoch 600, training loss: 12.439934730529785 = 0.38139382004737854 + 2.0 * 6.029270648956299
Epoch 600, val loss: 0.8228848576545715
Epoch 610, training loss: 12.431805610656738 = 0.36692968010902405 + 2.0 * 6.032437801361084
Epoch 610, val loss: 0.8201575875282288
Epoch 620, training loss: 12.407089233398438 = 0.3529520034790039 + 2.0 * 6.027068614959717
Epoch 620, val loss: 0.8181664943695068
Epoch 630, training loss: 12.39127254486084 = 0.33944764733314514 + 2.0 * 6.025912284851074
Epoch 630, val loss: 0.8166268467903137
Epoch 640, training loss: 12.396984100341797 = 0.3263067305088043 + 2.0 * 6.035338878631592
Epoch 640, val loss: 0.815351128578186
Epoch 650, training loss: 12.369096755981445 = 0.3138084411621094 + 2.0 * 6.027644157409668
Epoch 650, val loss: 0.8143538236618042
Epoch 660, training loss: 12.349292755126953 = 0.30168846249580383 + 2.0 * 6.023802280426025
Epoch 660, val loss: 0.8139995336532593
Epoch 670, training loss: 12.335250854492188 = 0.28993773460388184 + 2.0 * 6.022656440734863
Epoch 670, val loss: 0.813818633556366
Epoch 680, training loss: 12.336870193481445 = 0.2785318195819855 + 2.0 * 6.029169082641602
Epoch 680, val loss: 0.8138761520385742
Epoch 690, training loss: 12.315474510192871 = 0.2676564157009125 + 2.0 * 6.023909091949463
Epoch 690, val loss: 0.8140448927879333
Epoch 700, training loss: 12.297575950622559 = 0.25712576508522034 + 2.0 * 6.0202250480651855
Epoch 700, val loss: 0.8147209286689758
Epoch 710, training loss: 12.285573959350586 = 0.2469441294670105 + 2.0 * 6.019314765930176
Epoch 710, val loss: 0.8154041767120361
Epoch 720, training loss: 12.275068283081055 = 0.23711127042770386 + 2.0 * 6.018978595733643
Epoch 720, val loss: 0.8163433074951172
Epoch 730, training loss: 12.277118682861328 = 0.22768139839172363 + 2.0 * 6.024718761444092
Epoch 730, val loss: 0.8174009919166565
Epoch 740, training loss: 12.253782272338867 = 0.21870635449886322 + 2.0 * 6.017538070678711
Epoch 740, val loss: 0.8187941312789917
Epoch 750, training loss: 12.242558479309082 = 0.2101573795080185 + 2.0 * 6.016200542449951
Epoch 750, val loss: 0.8205809593200684
Epoch 760, training loss: 12.233242988586426 = 0.20189610123634338 + 2.0 * 6.015673637390137
Epoch 760, val loss: 0.8223077654838562
Epoch 770, training loss: 12.237196922302246 = 0.19397298991680145 + 2.0 * 6.021612167358398
Epoch 770, val loss: 0.8241322636604309
Epoch 780, training loss: 12.220721244812012 = 0.1863526999950409 + 2.0 * 6.017184257507324
Epoch 780, val loss: 0.8262348771095276
Epoch 790, training loss: 12.207682609558105 = 0.1791166216135025 + 2.0 * 6.014283180236816
Epoch 790, val loss: 0.8286123275756836
Epoch 800, training loss: 12.19852352142334 = 0.1721210479736328 + 2.0 * 6.0132012367248535
Epoch 800, val loss: 0.8310561180114746
Epoch 810, training loss: 12.197277069091797 = 0.1653933972120285 + 2.0 * 6.015941619873047
Epoch 810, val loss: 0.8335447907447815
Epoch 820, training loss: 12.183979988098145 = 0.1589384377002716 + 2.0 * 6.012520790100098
Epoch 820, val loss: 0.8361291289329529
Epoch 830, training loss: 12.181624412536621 = 0.15278969705104828 + 2.0 * 6.0144171714782715
Epoch 830, val loss: 0.839184582233429
Epoch 840, training loss: 12.170768737792969 = 0.14693821966648102 + 2.0 * 6.01191520690918
Epoch 840, val loss: 0.8423624634742737
Epoch 850, training loss: 12.161771774291992 = 0.14132414758205414 + 2.0 * 6.010223865509033
Epoch 850, val loss: 0.8459112644195557
Epoch 860, training loss: 12.168317794799805 = 0.1359664797782898 + 2.0 * 6.016175746917725
Epoch 860, val loss: 0.849609375
Epoch 870, training loss: 12.153903007507324 = 0.13090646266937256 + 2.0 * 6.01149845123291
Epoch 870, val loss: 0.8535679578781128
Epoch 880, training loss: 12.146954536437988 = 0.12609872221946716 + 2.0 * 6.010427951812744
Epoch 880, val loss: 0.8578911423683167
Epoch 890, training loss: 12.143383026123047 = 0.12155193090438843 + 2.0 * 6.010915756225586
Epoch 890, val loss: 0.8621464967727661
Epoch 900, training loss: 12.1371431350708 = 0.1172734797000885 + 2.0 * 6.009934902191162
Epoch 900, val loss: 0.8669402599334717
Epoch 910, training loss: 12.127750396728516 = 0.1131901815533638 + 2.0 * 6.007279872894287
Epoch 910, val loss: 0.871966540813446
Epoch 920, training loss: 12.121963500976562 = 0.10929694026708603 + 2.0 * 6.006333351135254
Epoch 920, val loss: 0.8768852949142456
Epoch 930, training loss: 12.117116928100586 = 0.10556960105895996 + 2.0 * 6.005773544311523
Epoch 930, val loss: 0.8820057511329651
Epoch 940, training loss: 12.128462791442871 = 0.10201059281826019 + 2.0 * 6.01322603225708
Epoch 940, val loss: 0.8872328996658325
Epoch 950, training loss: 12.112784385681152 = 0.0986294224858284 + 2.0 * 6.007077693939209
Epoch 950, val loss: 0.8923999071121216
Epoch 960, training loss: 12.10682487487793 = 0.09541263431310654 + 2.0 * 6.005706310272217
Epoch 960, val loss: 0.8978149890899658
Epoch 970, training loss: 12.107327461242676 = 0.09233587980270386 + 2.0 * 6.007495880126953
Epoch 970, val loss: 0.9030953049659729
Epoch 980, training loss: 12.100744247436523 = 0.08940570801496506 + 2.0 * 6.005669116973877
Epoch 980, val loss: 0.9083351492881775
Epoch 990, training loss: 12.096009254455566 = 0.08660611510276794 + 2.0 * 6.004701614379883
Epoch 990, val loss: 0.9138457775115967
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7926
Overall ASR: 0.9926
Flip ASR: 0.9911/225 nodes
The final ASR:0.90898, 0.10303, Accuracy:0.80864, 0.01364
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11602])
remove edge: torch.Size([2, 9494])
updated graph: torch.Size([2, 10540])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9926
Flip ASR: 0.9911/225 nodes
The final ASR:0.98278, 0.00758, Accuracy:0.82963, 0.00524
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.3 der2:0 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 18.68834686279297 = 1.9405527114868164 + 2.0 * 8.373896598815918
Epoch 0, val loss: 1.9359110593795776
Epoch 10, training loss: 18.67717170715332 = 1.9301201105117798 + 2.0 * 8.373525619506836
Epoch 10, val loss: 1.9264806509017944
Epoch 20, training loss: 18.659454345703125 = 1.9175654649734497 + 2.0 * 8.370944023132324
Epoch 20, val loss: 1.9148478507995605
Epoch 30, training loss: 18.60190200805664 = 1.9008527994155884 + 2.0 * 8.35052490234375
Epoch 30, val loss: 1.899397373199463
Epoch 40, training loss: 18.2684326171875 = 1.8807661533355713 + 2.0 * 8.193833351135254
Epoch 40, val loss: 1.8811554908752441
Epoch 50, training loss: 16.598878860473633 = 1.861592173576355 + 2.0 * 7.368643760681152
Epoch 50, val loss: 1.8642698526382446
Epoch 60, training loss: 15.778770446777344 = 1.8480706214904785 + 2.0 * 6.9653496742248535
Epoch 60, val loss: 1.852941632270813
Epoch 70, training loss: 15.280067443847656 = 1.8384175300598145 + 2.0 * 6.720824718475342
Epoch 70, val loss: 1.8445395231246948
Epoch 80, training loss: 14.9488525390625 = 1.8296468257904053 + 2.0 * 6.559602737426758
Epoch 80, val loss: 1.8369312286376953
Epoch 90, training loss: 14.744854927062988 = 1.822191596031189 + 2.0 * 6.461331844329834
Epoch 90, val loss: 1.830246090888977
Epoch 100, training loss: 14.591360092163086 = 1.8155083656311035 + 2.0 * 6.38792610168457
Epoch 100, val loss: 1.8241066932678223
Epoch 110, training loss: 14.483071327209473 = 1.8090171813964844 + 2.0 * 6.337027072906494
Epoch 110, val loss: 1.818087100982666
Epoch 120, training loss: 14.402481079101562 = 1.8029029369354248 + 2.0 * 6.299788951873779
Epoch 120, val loss: 1.8122303485870361
Epoch 130, training loss: 14.339166641235352 = 1.7966777086257935 + 2.0 * 6.271244525909424
Epoch 130, val loss: 1.8062275648117065
Epoch 140, training loss: 14.288143157958984 = 1.790149211883545 + 2.0 * 6.248997211456299
Epoch 140, val loss: 1.7999954223632812
Epoch 150, training loss: 14.23671817779541 = 1.783339500427246 + 2.0 * 6.226689338684082
Epoch 150, val loss: 1.7936773300170898
Epoch 160, training loss: 14.19507884979248 = 1.776066541671753 + 2.0 * 6.209506034851074
Epoch 160, val loss: 1.7872105836868286
Epoch 170, training loss: 14.158744812011719 = 1.768001675605774 + 2.0 * 6.195371627807617
Epoch 170, val loss: 1.7803022861480713
Epoch 180, training loss: 14.123444557189941 = 1.7588990926742554 + 2.0 * 6.182272911071777
Epoch 180, val loss: 1.7727642059326172
Epoch 190, training loss: 14.092930793762207 = 1.748494029045105 + 2.0 * 6.172218322753906
Epoch 190, val loss: 1.7644901275634766
Epoch 200, training loss: 14.057839393615723 = 1.7366533279418945 + 2.0 * 6.160593032836914
Epoch 200, val loss: 1.7551817893981934
Epoch 210, training loss: 14.02549934387207 = 1.722954273223877 + 2.0 * 6.151272773742676
Epoch 210, val loss: 1.744593858718872
Epoch 220, training loss: 13.99738597869873 = 1.7070618867874146 + 2.0 * 6.145162105560303
Epoch 220, val loss: 1.7323760986328125
Epoch 230, training loss: 13.960451126098633 = 1.6887128353118896 + 2.0 * 6.135869026184082
Epoch 230, val loss: 1.7184455394744873
Epoch 240, training loss: 13.926767349243164 = 1.6674758195877075 + 2.0 * 6.129645824432373
Epoch 240, val loss: 1.7022587060928345
Epoch 250, training loss: 13.888908386230469 = 1.642615556716919 + 2.0 * 6.1231465339660645
Epoch 250, val loss: 1.6832149028778076
Epoch 260, training loss: 13.858542442321777 = 1.6135072708129883 + 2.0 * 6.1225175857543945
Epoch 260, val loss: 1.660737156867981
Epoch 270, training loss: 13.809042930603027 = 1.5801125764846802 + 2.0 * 6.114465236663818
Epoch 270, val loss: 1.6346032619476318
Epoch 280, training loss: 13.75976276397705 = 1.5418490171432495 + 2.0 * 6.108956813812256
Epoch 280, val loss: 1.6044303178787231
Epoch 290, training loss: 13.721864700317383 = 1.4987457990646362 + 2.0 * 6.1115593910217285
Epoch 290, val loss: 1.5702110528945923
Epoch 300, training loss: 13.658487319946289 = 1.452476978302002 + 2.0 * 6.103005409240723
Epoch 300, val loss: 1.5331979990005493
Epoch 310, training loss: 13.598793029785156 = 1.4032349586486816 + 2.0 * 6.097779273986816
Epoch 310, val loss: 1.4935035705566406
Epoch 320, training loss: 13.54269027709961 = 1.3509162664413452 + 2.0 * 6.095887184143066
Epoch 320, val loss: 1.451076626777649
Epoch 330, training loss: 13.484198570251465 = 1.298598051071167 + 2.0 * 6.092800140380859
Epoch 330, val loss: 1.4091733694076538
Epoch 340, training loss: 13.424036026000977 = 1.2474669218063354 + 2.0 * 6.088284492492676
Epoch 340, val loss: 1.3683712482452393
Epoch 350, training loss: 13.369221687316895 = 1.1979458332061768 + 2.0 * 6.085638046264648
Epoch 350, val loss: 1.3292468786239624
Epoch 360, training loss: 13.314444541931152 = 1.150592565536499 + 2.0 * 6.081925868988037
Epoch 360, val loss: 1.2921550273895264
Epoch 370, training loss: 13.265162467956543 = 1.1052714586257935 + 2.0 * 6.0799455642700195
Epoch 370, val loss: 1.2570276260375977
Epoch 380, training loss: 13.222371101379395 = 1.061678171157837 + 2.0 * 6.080346584320068
Epoch 380, val loss: 1.2236754894256592
Epoch 390, training loss: 13.182424545288086 = 1.019946813583374 + 2.0 * 6.081238746643066
Epoch 390, val loss: 1.1921420097351074
Epoch 400, training loss: 13.128416061401367 = 0.9798827767372131 + 2.0 * 6.07426643371582
Epoch 400, val loss: 1.1621390581130981
Epoch 410, training loss: 13.081522941589355 = 0.940721333026886 + 2.0 * 6.070400714874268
Epoch 410, val loss: 1.1330490112304688
Epoch 420, training loss: 13.043777465820312 = 0.9020627737045288 + 2.0 * 6.070857524871826
Epoch 420, val loss: 1.1044201850891113
Epoch 430, training loss: 13.006265640258789 = 0.8644188046455383 + 2.0 * 6.070923328399658
Epoch 430, val loss: 1.0765529870986938
Epoch 440, training loss: 12.959696769714355 = 0.8276856541633606 + 2.0 * 6.066005706787109
Epoch 440, val loss: 1.049773931503296
Epoch 450, training loss: 12.91819953918457 = 0.7916561961174011 + 2.0 * 6.063271522521973
Epoch 450, val loss: 1.0236793756484985
Epoch 460, training loss: 12.900323867797852 = 0.7563387751579285 + 2.0 * 6.07199239730835
Epoch 460, val loss: 0.998448371887207
Epoch 470, training loss: 12.847943305969238 = 0.7225019335746765 + 2.0 * 6.062720775604248
Epoch 470, val loss: 0.9746271371841431
Epoch 480, training loss: 12.808752059936523 = 0.690130889415741 + 2.0 * 6.059310436248779
Epoch 480, val loss: 0.9522383809089661
Epoch 490, training loss: 12.774065971374512 = 0.6590092778205872 + 2.0 * 6.057528495788574
Epoch 490, val loss: 0.9312884211540222
Epoch 500, training loss: 12.755327224731445 = 0.6292089819908142 + 2.0 * 6.063059329986572
Epoch 500, val loss: 0.9119266271591187
Epoch 510, training loss: 12.70901107788086 = 0.6011340618133545 + 2.0 * 6.053938388824463
Epoch 510, val loss: 0.8942418098449707
Epoch 520, training loss: 12.678852081298828 = 0.5745498538017273 + 2.0 * 6.052151203155518
Epoch 520, val loss: 0.8783607482910156
Epoch 530, training loss: 12.657841682434082 = 0.5493233799934387 + 2.0 * 6.054259300231934
Epoch 530, val loss: 0.8640609383583069
Epoch 540, training loss: 12.63452434539795 = 0.5258039832115173 + 2.0 * 6.054360389709473
Epoch 540, val loss: 0.8514565229415894
Epoch 550, training loss: 12.601668357849121 = 0.5037487149238586 + 2.0 * 6.048959732055664
Epoch 550, val loss: 0.8407199382781982
Epoch 560, training loss: 12.577157974243164 = 0.48290273547172546 + 2.0 * 6.047127723693848
Epoch 560, val loss: 0.8313865065574646
Epoch 570, training loss: 12.558940887451172 = 0.46323782205581665 + 2.0 * 6.0478515625
Epoch 570, val loss: 0.8234358429908752
Epoch 580, training loss: 12.537496566772461 = 0.44476819038391113 + 2.0 * 6.0463643074035645
Epoch 580, val loss: 0.8169659376144409
Epoch 590, training loss: 12.513115882873535 = 0.42732831835746765 + 2.0 * 6.042893886566162
Epoch 590, val loss: 0.8118324279785156
Epoch 600, training loss: 12.493388175964355 = 0.4106864035129547 + 2.0 * 6.041350841522217
Epoch 600, val loss: 0.8077059984207153
Epoch 610, training loss: 12.492204666137695 = 0.39475852251052856 + 2.0 * 6.048723220825195
Epoch 610, val loss: 0.8045232892036438
Epoch 620, training loss: 12.465343475341797 = 0.3796845078468323 + 2.0 * 6.042829513549805
Epoch 620, val loss: 0.8023085594177246
Epoch 630, training loss: 12.441574096679688 = 0.3652494251728058 + 2.0 * 6.0381622314453125
Epoch 630, val loss: 0.8009915351867676
Epoch 640, training loss: 12.429411888122559 = 0.35125869512557983 + 2.0 * 6.039076805114746
Epoch 640, val loss: 0.8002814650535583
Epoch 650, training loss: 12.409966468811035 = 0.3376871943473816 + 2.0 * 6.036139488220215
Epoch 650, val loss: 0.8002740740776062
Epoch 660, training loss: 12.394637107849121 = 0.3244999051094055 + 2.0 * 6.035068511962891
Epoch 660, val loss: 0.8009812235832214
Epoch 670, training loss: 12.381974220275879 = 0.3115788400173187 + 2.0 * 6.035197734832764
Epoch 670, val loss: 0.8021703362464905
Epoch 680, training loss: 12.37729549407959 = 0.2989257276058197 + 2.0 * 6.039185047149658
Epoch 680, val loss: 0.8037848472595215
Epoch 690, training loss: 12.357295036315918 = 0.28665846586227417 + 2.0 * 6.035318374633789
Epoch 690, val loss: 0.8059088587760925
Epoch 700, training loss: 12.339126586914062 = 0.2747042179107666 + 2.0 * 6.0322113037109375
Epoch 700, val loss: 0.8085249662399292
Epoch 710, training loss: 12.327226638793945 = 0.2630150020122528 + 2.0 * 6.032105922698975
Epoch 710, val loss: 0.8115805983543396
Epoch 720, training loss: 12.315524101257324 = 0.2515661418437958 + 2.0 * 6.031979084014893
Epoch 720, val loss: 0.8149799704551697
Epoch 730, training loss: 12.303138732910156 = 0.240451380610466 + 2.0 * 6.031343460083008
Epoch 730, val loss: 0.8188531398773193
Epoch 740, training loss: 12.294353485107422 = 0.2297273427248001 + 2.0 * 6.032312870025635
Epoch 740, val loss: 0.823168158531189
Epoch 750, training loss: 12.277263641357422 = 0.21948543190956116 + 2.0 * 6.028889179229736
Epoch 750, val loss: 0.8279200792312622
Epoch 760, training loss: 12.264440536499023 = 0.20973245799541473 + 2.0 * 6.0273542404174805
Epoch 760, val loss: 0.8331530690193176
Epoch 770, training loss: 12.270421981811523 = 0.2004542350769043 + 2.0 * 6.034984111785889
Epoch 770, val loss: 0.8387207984924316
Epoch 780, training loss: 12.250934600830078 = 0.19174304604530334 + 2.0 * 6.029595851898193
Epoch 780, val loss: 0.8445844650268555
Epoch 790, training loss: 12.249799728393555 = 0.18351472914218903 + 2.0 * 6.033142566680908
Epoch 790, val loss: 0.8509446978569031
Epoch 800, training loss: 12.230412483215332 = 0.1758098304271698 + 2.0 * 6.02730131149292
Epoch 800, val loss: 0.8575018048286438
Epoch 810, training loss: 12.218245506286621 = 0.16855104267597198 + 2.0 * 6.024847030639648
Epoch 810, val loss: 0.8644587397575378
Epoch 820, training loss: 12.211572647094727 = 0.16166438162326813 + 2.0 * 6.024954319000244
Epoch 820, val loss: 0.8717231750488281
Epoch 830, training loss: 12.205740928649902 = 0.15517035126686096 + 2.0 * 6.025285243988037
Epoch 830, val loss: 0.8790390491485596
Epoch 840, training loss: 12.193623542785645 = 0.14906123280525208 + 2.0 * 6.022281169891357
Epoch 840, val loss: 0.8867183923721313
Epoch 850, training loss: 12.187942504882812 = 0.1432723104953766 + 2.0 * 6.022335052490234
Epoch 850, val loss: 0.8946311473846436
Epoch 860, training loss: 12.189170837402344 = 0.13779538869857788 + 2.0 * 6.0256876945495605
Epoch 860, val loss: 0.9025529026985168
Epoch 870, training loss: 12.175505638122559 = 0.13264216482639313 + 2.0 * 6.021431922912598
Epoch 870, val loss: 0.9106675386428833
Epoch 880, training loss: 12.17520809173584 = 0.1277613788843155 + 2.0 * 6.023723125457764
Epoch 880, val loss: 0.9188699722290039
Epoch 890, training loss: 12.165478706359863 = 0.12312375754117966 + 2.0 * 6.021177291870117
Epoch 890, val loss: 0.9271048903465271
Epoch 900, training loss: 12.160283088684082 = 0.11873520165681839 + 2.0 * 6.020773887634277
Epoch 900, val loss: 0.9354894757270813
Epoch 910, training loss: 12.15453815460205 = 0.11456213146448135 + 2.0 * 6.019988059997559
Epoch 910, val loss: 0.9439391493797302
Epoch 920, training loss: 12.153013229370117 = 0.11058373749256134 + 2.0 * 6.021214962005615
Epoch 920, val loss: 0.9524871706962585
Epoch 930, training loss: 12.148770332336426 = 0.10680051147937775 + 2.0 * 6.020985126495361
Epoch 930, val loss: 0.9610633254051208
Epoch 940, training loss: 12.138636589050293 = 0.10318803042173386 + 2.0 * 6.017724514007568
Epoch 940, val loss: 0.9696305990219116
Epoch 950, training loss: 12.138288497924805 = 0.0997387170791626 + 2.0 * 6.019274711608887
Epoch 950, val loss: 0.9783341884613037
Epoch 960, training loss: 12.139400482177734 = 0.09643787145614624 + 2.0 * 6.021481513977051
Epoch 960, val loss: 0.9869750142097473
Epoch 970, training loss: 12.127828598022461 = 0.09329790621995926 + 2.0 * 6.017265319824219
Epoch 970, val loss: 0.995690107345581
Epoch 980, training loss: 12.122243881225586 = 0.09028110653162003 + 2.0 * 6.015981197357178
Epoch 980, val loss: 1.0044605731964111
Epoch 990, training loss: 12.128777503967285 = 0.08738549053668976 + 2.0 * 6.02069616317749
Epoch 990, val loss: 1.0132274627685547
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7815
Overall ASR: 0.4908
Flip ASR: 0.4000/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.70960807800293 = 1.9619600772857666 + 2.0 * 8.373824119567871
Epoch 0, val loss: 1.970321774482727
Epoch 10, training loss: 18.696866989135742 = 1.9505776166915894 + 2.0 * 8.37314510345459
Epoch 10, val loss: 1.9582420587539673
Epoch 20, training loss: 18.673904418945312 = 1.9361597299575806 + 2.0 * 8.36887264251709
Epoch 20, val loss: 1.942774772644043
Epoch 30, training loss: 18.599576950073242 = 1.9166489839553833 + 2.0 * 8.341464042663574
Epoch 30, val loss: 1.9220936298370361
Epoch 40, training loss: 18.257030487060547 = 1.8939505815505981 + 2.0 * 8.181539535522461
Epoch 40, val loss: 1.8988349437713623
Epoch 50, training loss: 16.844955444335938 = 1.8691482543945312 + 2.0 * 7.487903594970703
Epoch 50, val loss: 1.8738049268722534
Epoch 60, training loss: 15.8953218460083 = 1.8519350290298462 + 2.0 * 7.021693229675293
Epoch 60, val loss: 1.8582284450531006
Epoch 70, training loss: 15.274388313293457 = 1.840392827987671 + 2.0 * 6.7169976234436035
Epoch 70, val loss: 1.8471940755844116
Epoch 80, training loss: 15.012239456176758 = 1.8281694650650024 + 2.0 * 6.592034816741943
Epoch 80, val loss: 1.8347234725952148
Epoch 90, training loss: 14.842292785644531 = 1.817109227180481 + 2.0 * 6.51259183883667
Epoch 90, val loss: 1.8233582973480225
Epoch 100, training loss: 14.710980415344238 = 1.8068735599517822 + 2.0 * 6.452053546905518
Epoch 100, val loss: 1.8126084804534912
Epoch 110, training loss: 14.622458457946777 = 1.7971833944320679 + 2.0 * 6.412637710571289
Epoch 110, val loss: 1.8023698329925537
Epoch 120, training loss: 14.549569129943848 = 1.7876375913619995 + 2.0 * 6.380965709686279
Epoch 120, val loss: 1.79246985912323
Epoch 130, training loss: 14.476516723632812 = 1.7787551879882812 + 2.0 * 6.348880767822266
Epoch 130, val loss: 1.7836374044418335
Epoch 140, training loss: 14.40949821472168 = 1.7702327966690063 + 2.0 * 6.319632530212402
Epoch 140, val loss: 1.775400996208191
Epoch 150, training loss: 14.350460052490234 = 1.761136770248413 + 2.0 * 6.294661521911621
Epoch 150, val loss: 1.7668565511703491
Epoch 160, training loss: 14.298803329467773 = 1.750971794128418 + 2.0 * 6.273915767669678
Epoch 160, val loss: 1.7577064037322998
Epoch 170, training loss: 14.244627952575684 = 1.7395306825637817 + 2.0 * 6.252548694610596
Epoch 170, val loss: 1.747779130935669
Epoch 180, training loss: 14.191252708435059 = 1.726504921913147 + 2.0 * 6.2323737144470215
Epoch 180, val loss: 1.7368109226226807
Epoch 190, training loss: 14.147890090942383 = 1.7114615440368652 + 2.0 * 6.218214511871338
Epoch 190, val loss: 1.7245320081710815
Epoch 200, training loss: 14.099950790405273 = 1.6941438913345337 + 2.0 * 6.2029032707214355
Epoch 200, val loss: 1.7104395627975464
Epoch 210, training loss: 14.054179191589355 = 1.673680067062378 + 2.0 * 6.190249443054199
Epoch 210, val loss: 1.6942297220230103
Epoch 220, training loss: 14.007479667663574 = 1.649566888809204 + 2.0 * 6.178956508636475
Epoch 220, val loss: 1.6750644445419312
Epoch 230, training loss: 13.962361335754395 = 1.6210652589797974 + 2.0 * 6.170648097991943
Epoch 230, val loss: 1.6523741483688354
Epoch 240, training loss: 13.91176986694336 = 1.5884149074554443 + 2.0 * 6.161677360534668
Epoch 240, val loss: 1.626702904701233
Epoch 250, training loss: 13.85573959350586 = 1.5513062477111816 + 2.0 * 6.15221643447876
Epoch 250, val loss: 1.5975797176361084
Epoch 260, training loss: 13.798110961914062 = 1.5092788934707642 + 2.0 * 6.144415855407715
Epoch 260, val loss: 1.564484715461731
Epoch 270, training loss: 13.737567901611328 = 1.462252140045166 + 2.0 * 6.137657642364502
Epoch 270, val loss: 1.527437448501587
Epoch 280, training loss: 13.678915977478027 = 1.411501169204712 + 2.0 * 6.133707523345947
Epoch 280, val loss: 1.4874407052993774
Epoch 290, training loss: 13.612131118774414 = 1.3585734367370605 + 2.0 * 6.126779079437256
Epoch 290, val loss: 1.4458943605422974
Epoch 300, training loss: 13.547310829162598 = 1.3042184114456177 + 2.0 * 6.121546268463135
Epoch 300, val loss: 1.4032217264175415
Epoch 310, training loss: 13.485780715942383 = 1.2501113414764404 + 2.0 * 6.117834568023682
Epoch 310, val loss: 1.3608183860778809
Epoch 320, training loss: 13.423770904541016 = 1.198121428489685 + 2.0 * 6.1128249168396
Epoch 320, val loss: 1.3201109170913696
Epoch 330, training loss: 13.363203048706055 = 1.148383378982544 + 2.0 * 6.107409954071045
Epoch 330, val loss: 1.2811174392700195
Epoch 340, training loss: 13.318258285522461 = 1.1010806560516357 + 2.0 * 6.108588695526123
Epoch 340, val loss: 1.244210958480835
Epoch 350, training loss: 13.255110740661621 = 1.05733323097229 + 2.0 * 6.098888874053955
Epoch 350, val loss: 1.2098774909973145
Epoch 360, training loss: 13.205693244934082 = 1.0165691375732422 + 2.0 * 6.09456205368042
Epoch 360, val loss: 1.1781998872756958
Epoch 370, training loss: 13.162353515625 = 0.9783484935760498 + 2.0 * 6.0920023918151855
Epoch 370, val loss: 1.1488604545593262
Epoch 380, training loss: 13.120012283325195 = 0.9429649710655212 + 2.0 * 6.088523864746094
Epoch 380, val loss: 1.1219203472137451
Epoch 390, training loss: 13.079466819763184 = 0.90998774766922 + 2.0 * 6.084739685058594
Epoch 390, val loss: 1.0975708961486816
Epoch 400, training loss: 13.040584564208984 = 0.8789878487586975 + 2.0 * 6.080798149108887
Epoch 400, val loss: 1.0750840902328491
Epoch 410, training loss: 13.006850242614746 = 0.8496353626251221 + 2.0 * 6.078607559204102
Epoch 410, val loss: 1.0542937517166138
Epoch 420, training loss: 12.974570274353027 = 0.8218536972999573 + 2.0 * 6.076358318328857
Epoch 420, val loss: 1.0349950790405273
Epoch 430, training loss: 12.941274642944336 = 0.7953747510910034 + 2.0 * 6.0729498863220215
Epoch 430, val loss: 1.017395257949829
Epoch 440, training loss: 12.91808032989502 = 0.7699415683746338 + 2.0 * 6.074069499969482
Epoch 440, val loss: 1.0009095668792725
Epoch 450, training loss: 12.883150100708008 = 0.7455071210861206 + 2.0 * 6.068821430206299
Epoch 450, val loss: 0.9854841232299805
Epoch 460, training loss: 12.853821754455566 = 0.7216346859931946 + 2.0 * 6.066093444824219
Epoch 460, val loss: 0.9707140922546387
Epoch 470, training loss: 12.83023452758789 = 0.6979552507400513 + 2.0 * 6.0661396980285645
Epoch 470, val loss: 0.9563996195793152
Epoch 480, training loss: 12.80559253692627 = 0.6748894453048706 + 2.0 * 6.065351486206055
Epoch 480, val loss: 0.9427496790885925
Epoch 490, training loss: 12.773344039916992 = 0.6522863507270813 + 2.0 * 6.060528755187988
Epoch 490, val loss: 0.9297424554824829
Epoch 500, training loss: 12.747011184692383 = 0.629844069480896 + 2.0 * 6.058583736419678
Epoch 500, val loss: 0.9169663786888123
Epoch 510, training loss: 12.741362571716309 = 0.6075996160507202 + 2.0 * 6.0668816566467285
Epoch 510, val loss: 0.9042949080467224
Epoch 520, training loss: 12.701834678649902 = 0.5859254598617554 + 2.0 * 6.057954788208008
Epoch 520, val loss: 0.8924540281295776
Epoch 530, training loss: 12.673238754272461 = 0.5645579695701599 + 2.0 * 6.054340362548828
Epoch 530, val loss: 0.881039559841156
Epoch 540, training loss: 12.653258323669434 = 0.5434340238571167 + 2.0 * 6.054912090301514
Epoch 540, val loss: 0.8699164390563965
Epoch 550, training loss: 12.630056381225586 = 0.5227563977241516 + 2.0 * 6.05364990234375
Epoch 550, val loss: 0.8594056367874146
Epoch 560, training loss: 12.602622985839844 = 0.5025448203086853 + 2.0 * 6.050039291381836
Epoch 560, val loss: 0.8496127128601074
Epoch 570, training loss: 12.580938339233398 = 0.48267999291419983 + 2.0 * 6.049129009246826
Epoch 570, val loss: 0.8402751684188843
Epoch 580, training loss: 12.562911987304688 = 0.4633018672466278 + 2.0 * 6.049805164337158
Epoch 580, val loss: 0.8316288590431213
Epoch 590, training loss: 12.53880786895752 = 0.44459739327430725 + 2.0 * 6.047105312347412
Epoch 590, val loss: 0.8238632678985596
Epoch 600, training loss: 12.516430854797363 = 0.4262772500514984 + 2.0 * 6.045076847076416
Epoch 600, val loss: 0.8168036937713623
Epoch 610, training loss: 12.497917175292969 = 0.4083077609539032 + 2.0 * 6.044804573059082
Epoch 610, val loss: 0.8103142380714417
Epoch 620, training loss: 12.486272811889648 = 0.3909207880496979 + 2.0 * 6.047676086425781
Epoch 620, val loss: 0.8044374585151672
Epoch 630, training loss: 12.460009574890137 = 0.3741385340690613 + 2.0 * 6.042935371398926
Epoch 630, val loss: 0.7995058298110962
Epoch 640, training loss: 12.441305160522461 = 0.35780656337738037 + 2.0 * 6.041749477386475
Epoch 640, val loss: 0.7950802445411682
Epoch 650, training loss: 12.430573463439941 = 0.34208425879478455 + 2.0 * 6.044244766235352
Epoch 650, val loss: 0.7909761667251587
Epoch 660, training loss: 12.410248756408691 = 0.32708102464675903 + 2.0 * 6.041584014892578
Epoch 660, val loss: 0.7877672910690308
Epoch 670, training loss: 12.389355659484863 = 0.31261900067329407 + 2.0 * 6.038368225097656
Epoch 670, val loss: 0.7851474285125732
Epoch 680, training loss: 12.37209415435791 = 0.2986287474632263 + 2.0 * 6.0367326736450195
Epoch 680, val loss: 0.7828066945075989
Epoch 690, training loss: 12.361051559448242 = 0.2851310968399048 + 2.0 * 6.037960052490234
Epoch 690, val loss: 0.7810163497924805
Epoch 700, training loss: 12.357027053833008 = 0.27229395508766174 + 2.0 * 6.0423665046691895
Epoch 700, val loss: 0.7792895436286926
Epoch 710, training loss: 12.328157424926758 = 0.2600851356983185 + 2.0 * 6.034036159515381
Epoch 710, val loss: 0.7785577178001404
Epoch 720, training loss: 12.315116882324219 = 0.24834905564785004 + 2.0 * 6.033383846282959
Epoch 720, val loss: 0.7781332731246948
Epoch 730, training loss: 12.301695823669434 = 0.23700490593910217 + 2.0 * 6.032345294952393
Epoch 730, val loss: 0.777849018573761
Epoch 740, training loss: 12.295025825500488 = 0.22603517770767212 + 2.0 * 6.0344953536987305
Epoch 740, val loss: 0.777887761592865
Epoch 750, training loss: 12.2928466796875 = 0.21559840440750122 + 2.0 * 6.038624286651611
Epoch 750, val loss: 0.7780247926712036
Epoch 760, training loss: 12.26945686340332 = 0.20562635362148285 + 2.0 * 6.031915187835693
Epoch 760, val loss: 0.7788124084472656
Epoch 770, training loss: 12.255245208740234 = 0.19602307677268982 + 2.0 * 6.029611110687256
Epoch 770, val loss: 0.77986079454422
Epoch 780, training loss: 12.243314743041992 = 0.1867465227842331 + 2.0 * 6.028284072875977
Epoch 780, val loss: 0.7809508442878723
Epoch 790, training loss: 12.234772682189941 = 0.17780961096286774 + 2.0 * 6.028481483459473
Epoch 790, val loss: 0.7824082374572754
Epoch 800, training loss: 12.225245475769043 = 0.1692441701889038 + 2.0 * 6.028000831604004
Epoch 800, val loss: 0.7838330268859863
Epoch 810, training loss: 12.220149993896484 = 0.16110143065452576 + 2.0 * 6.029524326324463
Epoch 810, val loss: 0.7857851386070251
Epoch 820, training loss: 12.20489501953125 = 0.15331922471523285 + 2.0 * 6.025787830352783
Epoch 820, val loss: 0.7880284190177917
Epoch 830, training loss: 12.194625854492188 = 0.1458660066127777 + 2.0 * 6.024379730224609
Epoch 830, val loss: 0.7903605699539185
Epoch 840, training loss: 12.20241641998291 = 0.13872458040714264 + 2.0 * 6.031846046447754
Epoch 840, val loss: 0.7928864359855652
Epoch 850, training loss: 12.180148124694824 = 0.1319274604320526 + 2.0 * 6.024110317230225
Epoch 850, val loss: 0.7954205870628357
Epoch 860, training loss: 12.172600746154785 = 0.12547577917575836 + 2.0 * 6.023562431335449
Epoch 860, val loss: 0.7985872030258179
Epoch 870, training loss: 12.16480827331543 = 0.1193091869354248 + 2.0 * 6.022749423980713
Epoch 870, val loss: 0.8017328381538391
Epoch 880, training loss: 12.165456771850586 = 0.1134575828909874 + 2.0 * 6.025999546051025
Epoch 880, val loss: 0.804719090461731
Epoch 890, training loss: 12.150415420532227 = 0.10796071588993073 + 2.0 * 6.0212273597717285
Epoch 890, val loss: 0.808256983757019
Epoch 900, training loss: 12.143145561218262 = 0.10280494391918182 + 2.0 * 6.020170211791992
Epoch 900, val loss: 0.8119630217552185
Epoch 910, training loss: 12.146848678588867 = 0.09793400019407272 + 2.0 * 6.0244574546813965
Epoch 910, val loss: 0.81563401222229
Epoch 920, training loss: 12.137903213500977 = 0.09337912499904633 + 2.0 * 6.022262096405029
Epoch 920, val loss: 0.8193389773368835
Epoch 930, training loss: 12.127570152282715 = 0.08912471681833267 + 2.0 * 6.019222736358643
Epoch 930, val loss: 0.8235568404197693
Epoch 940, training loss: 12.119610786437988 = 0.08512219041585922 + 2.0 * 6.017244338989258
Epoch 940, val loss: 0.8278618454933167
Epoch 950, training loss: 12.115817070007324 = 0.08134255558252335 + 2.0 * 6.017237186431885
Epoch 950, val loss: 0.832176148891449
Epoch 960, training loss: 12.12838363647461 = 0.07778279483318329 + 2.0 * 6.0253005027771
Epoch 960, val loss: 0.8362667560577393
Epoch 970, training loss: 12.111837387084961 = 0.07448890060186386 + 2.0 * 6.018674373626709
Epoch 970, val loss: 0.8405954241752625
Epoch 980, training loss: 12.10188102722168 = 0.07141393423080444 + 2.0 * 6.015233516693115
Epoch 980, val loss: 0.845390796661377
Epoch 990, training loss: 12.097634315490723 = 0.06849486380815506 + 2.0 * 6.0145697593688965
Epoch 990, val loss: 0.8499730825424194
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8074
Overall ASR: 0.7306
Flip ASR: 0.6889/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.71001625061035 = 1.9623795747756958 + 2.0 * 8.373818397521973
Epoch 0, val loss: 1.9607785940170288
Epoch 10, training loss: 18.697607040405273 = 1.9513684511184692 + 2.0 * 8.373119354248047
Epoch 10, val loss: 1.9492460489273071
Epoch 20, training loss: 18.674177169799805 = 1.9380738735198975 + 2.0 * 8.368051528930664
Epoch 20, val loss: 1.9350333213806152
Epoch 30, training loss: 18.58384132385254 = 1.9205228090286255 + 2.0 * 8.331659317016602
Epoch 30, val loss: 1.9164514541625977
Epoch 40, training loss: 18.121965408325195 = 1.8995693922042847 + 2.0 * 8.111198425292969
Epoch 40, val loss: 1.8951871395111084
Epoch 50, training loss: 16.812166213989258 = 1.8766629695892334 + 2.0 * 7.467751979827881
Epoch 50, val loss: 1.8726749420166016
Epoch 60, training loss: 15.965348243713379 = 1.860327124595642 + 2.0 * 7.052510738372803
Epoch 60, val loss: 1.8582745790481567
Epoch 70, training loss: 15.351688385009766 = 1.8470654487609863 + 2.0 * 6.7523112297058105
Epoch 70, val loss: 1.8459340333938599
Epoch 80, training loss: 15.060751914978027 = 1.8348416090011597 + 2.0 * 6.612955093383789
Epoch 80, val loss: 1.8341448307037354
Epoch 90, training loss: 14.895108222961426 = 1.82341468334198 + 2.0 * 6.535846710205078
Epoch 90, val loss: 1.8226511478424072
Epoch 100, training loss: 14.75245189666748 = 1.8131139278411865 + 2.0 * 6.469668865203857
Epoch 100, val loss: 1.8122878074645996
Epoch 110, training loss: 14.63044548034668 = 1.804787039756775 + 2.0 * 6.412829399108887
Epoch 110, val loss: 1.8038488626480103
Epoch 120, training loss: 14.540053367614746 = 1.7974637746810913 + 2.0 * 6.371294975280762
Epoch 120, val loss: 1.7963179349899292
Epoch 130, training loss: 14.46448802947998 = 1.790081262588501 + 2.0 * 6.337203502655029
Epoch 130, val loss: 1.7886561155319214
Epoch 140, training loss: 14.405611038208008 = 1.782386302947998 + 2.0 * 6.311612129211426
Epoch 140, val loss: 1.7806439399719238
Epoch 150, training loss: 14.353494644165039 = 1.7744126319885254 + 2.0 * 6.289540767669678
Epoch 150, val loss: 1.7724906206130981
Epoch 160, training loss: 14.309640884399414 = 1.7661491632461548 + 2.0 * 6.271745681762695
Epoch 160, val loss: 1.7642606496810913
Epoch 170, training loss: 14.25538444519043 = 1.7573938369750977 + 2.0 * 6.248995304107666
Epoch 170, val loss: 1.7558950185775757
Epoch 180, training loss: 14.206902503967285 = 1.747915267944336 + 2.0 * 6.229493618011475
Epoch 180, val loss: 1.7470982074737549
Epoch 190, training loss: 14.165382385253906 = 1.737156629562378 + 2.0 * 6.214112758636475
Epoch 190, val loss: 1.7373605966567993
Epoch 200, training loss: 14.126401901245117 = 1.7249717712402344 + 2.0 * 6.200715065002441
Epoch 200, val loss: 1.7265716791152954
Epoch 210, training loss: 14.087167739868164 = 1.7111605405807495 + 2.0 * 6.1880035400390625
Epoch 210, val loss: 1.7146183252334595
Epoch 220, training loss: 14.052249908447266 = 1.6954455375671387 + 2.0 * 6.178401947021484
Epoch 220, val loss: 1.701262354850769
Epoch 230, training loss: 14.015548706054688 = 1.6773765087127686 + 2.0 * 6.16908597946167
Epoch 230, val loss: 1.686341643333435
Epoch 240, training loss: 13.978511810302734 = 1.656746745109558 + 2.0 * 6.160882472991943
Epoch 240, val loss: 1.6694663763046265
Epoch 250, training loss: 13.941370010375977 = 1.6332550048828125 + 2.0 * 6.154057502746582
Epoch 250, val loss: 1.6505770683288574
Epoch 260, training loss: 13.89951229095459 = 1.6063101291656494 + 2.0 * 6.14660120010376
Epoch 260, val loss: 1.6292345523834229
Epoch 270, training loss: 13.855429649353027 = 1.5752792358398438 + 2.0 * 6.140075206756592
Epoch 270, val loss: 1.6048133373260498
Epoch 280, training loss: 13.808059692382812 = 1.5394549369812012 + 2.0 * 6.134302139282227
Epoch 280, val loss: 1.576688289642334
Epoch 290, training loss: 13.759359359741211 = 1.498915433883667 + 2.0 * 6.130221843719482
Epoch 290, val loss: 1.545003056526184
Epoch 300, training loss: 13.704583168029785 = 1.454005241394043 + 2.0 * 6.125288963317871
Epoch 300, val loss: 1.5098848342895508
Epoch 310, training loss: 13.644596099853516 = 1.4043859243392944 + 2.0 * 6.120105266571045
Epoch 310, val loss: 1.4713432788848877
Epoch 320, training loss: 13.590887069702148 = 1.3506879806518555 + 2.0 * 6.1200995445251465
Epoch 320, val loss: 1.4297115802764893
Epoch 330, training loss: 13.522029876708984 = 1.2943096160888672 + 2.0 * 6.113860130310059
Epoch 330, val loss: 1.38619065284729
Epoch 340, training loss: 13.456048965454102 = 1.236309289932251 + 2.0 * 6.109869956970215
Epoch 340, val loss: 1.3415942192077637
Epoch 350, training loss: 13.38936710357666 = 1.177813172340393 + 2.0 * 6.105776786804199
Epoch 350, val loss: 1.296675205230713
Epoch 360, training loss: 13.334502220153809 = 1.1194485425949097 + 2.0 * 6.107526779174805
Epoch 360, val loss: 1.2520400285720825
Epoch 370, training loss: 13.266242980957031 = 1.0631599426269531 + 2.0 * 6.101541519165039
Epoch 370, val loss: 1.2088123559951782
Epoch 380, training loss: 13.20151424407959 = 1.0090805292129517 + 2.0 * 6.096216678619385
Epoch 380, val loss: 1.167248010635376
Epoch 390, training loss: 13.142050743103027 = 0.9567392468452454 + 2.0 * 6.092655658721924
Epoch 390, val loss: 1.126988172531128
Epoch 400, training loss: 13.095593452453613 = 0.9062157273292542 + 2.0 * 6.094688892364502
Epoch 400, val loss: 1.0880684852600098
Epoch 410, training loss: 13.034287452697754 = 0.8581469655036926 + 2.0 * 6.088070392608643
Epoch 410, val loss: 1.051095962524414
Epoch 420, training loss: 12.981714248657227 = 0.8119310736656189 + 2.0 * 6.0848917961120605
Epoch 420, val loss: 1.0156747102737427
Epoch 430, training loss: 12.934328079223633 = 0.7674723863601685 + 2.0 * 6.083427906036377
Epoch 430, val loss: 0.9815359711647034
Epoch 440, training loss: 12.886566162109375 = 0.7249675989151001 + 2.0 * 6.080799102783203
Epoch 440, val loss: 0.9491241574287415
Epoch 450, training loss: 12.844198226928711 = 0.6841928362846375 + 2.0 * 6.080002784729004
Epoch 450, val loss: 0.9182320833206177
Epoch 460, training loss: 12.796318054199219 = 0.645315945148468 + 2.0 * 6.075500965118408
Epoch 460, val loss: 0.8889856338500977
Epoch 470, training loss: 12.760467529296875 = 0.6081345081329346 + 2.0 * 6.07616662979126
Epoch 470, val loss: 0.8615188598632812
Epoch 480, training loss: 12.719722747802734 = 0.5730196833610535 + 2.0 * 6.0733513832092285
Epoch 480, val loss: 0.8361213207244873
Epoch 490, training loss: 12.677886009216309 = 0.5399101376533508 + 2.0 * 6.068987846374512
Epoch 490, val loss: 0.813035786151886
Epoch 500, training loss: 12.649770736694336 = 0.5083187222480774 + 2.0 * 6.070725917816162
Epoch 500, val loss: 0.7917726039886475
Epoch 510, training loss: 12.61584186553955 = 0.47845253348350525 + 2.0 * 6.068694591522217
Epoch 510, val loss: 0.7722969651222229
Epoch 520, training loss: 12.577276229858398 = 0.4500957429409027 + 2.0 * 6.063590049743652
Epoch 520, val loss: 0.7548401355743408
Epoch 530, training loss: 12.562438011169434 = 0.4230280816555023 + 2.0 * 6.069705009460449
Epoch 530, val loss: 0.7388451099395752
Epoch 540, training loss: 12.520021438598633 = 0.39743897318840027 + 2.0 * 6.061291217803955
Epoch 540, val loss: 0.7243497371673584
Epoch 550, training loss: 12.491198539733887 = 0.3731699287891388 + 2.0 * 6.059014320373535
Epoch 550, val loss: 0.7113427519798279
Epoch 560, training loss: 12.47175121307373 = 0.3500903844833374 + 2.0 * 6.060830593109131
Epoch 560, val loss: 0.6994444727897644
Epoch 570, training loss: 12.441308975219727 = 0.3283127248287201 + 2.0 * 6.056498050689697
Epoch 570, val loss: 0.6889129281044006
Epoch 580, training loss: 12.429140090942383 = 0.30783867835998535 + 2.0 * 6.060650825500488
Epoch 580, val loss: 0.6794599294662476
Epoch 590, training loss: 12.398787498474121 = 0.2888881266117096 + 2.0 * 6.054949760437012
Epoch 590, val loss: 0.6712497472763062
Epoch 600, training loss: 12.374393463134766 = 0.271196573972702 + 2.0 * 6.05159854888916
Epoch 600, val loss: 0.6642590761184692
Epoch 610, training loss: 12.365974426269531 = 0.254616379737854 + 2.0 * 6.055678844451904
Epoch 610, val loss: 0.6581900119781494
Epoch 620, training loss: 12.339752197265625 = 0.23937778174877167 + 2.0 * 6.050187110900879
Epoch 620, val loss: 0.6529974341392517
Epoch 630, training loss: 12.31887149810791 = 0.22535660862922668 + 2.0 * 6.046757221221924
Epoch 630, val loss: 0.6490494608879089
Epoch 640, training loss: 12.303394317626953 = 0.2123178392648697 + 2.0 * 6.045538425445557
Epoch 640, val loss: 0.6458386182785034
Epoch 650, training loss: 12.297558784484863 = 0.20016945898532867 + 2.0 * 6.048694610595703
Epoch 650, val loss: 0.6433520913124084
Epoch 660, training loss: 12.285160064697266 = 0.18902868032455444 + 2.0 * 6.048065662384033
Epoch 660, val loss: 0.6413829922676086
Epoch 670, training loss: 12.265026092529297 = 0.17881450057029724 + 2.0 * 6.043105602264404
Epoch 670, val loss: 0.6402917504310608
Epoch 680, training loss: 12.251230239868164 = 0.169316828250885 + 2.0 * 6.040956497192383
Epoch 680, val loss: 0.6397502422332764
Epoch 690, training loss: 12.256854057312012 = 0.16045476496219635 + 2.0 * 6.048199653625488
Epoch 690, val loss: 0.6396424174308777
Epoch 700, training loss: 12.230592727661133 = 0.1522931009531021 + 2.0 * 6.039149761199951
Epoch 700, val loss: 0.6400609612464905
Epoch 710, training loss: 12.22281265258789 = 0.14471860229969025 + 2.0 * 6.0390472412109375
Epoch 710, val loss: 0.640925407409668
Epoch 720, training loss: 12.215563774108887 = 0.13765326142311096 + 2.0 * 6.038955211639404
Epoch 720, val loss: 0.6421082615852356
Epoch 730, training loss: 12.204312324523926 = 0.1310618668794632 + 2.0 * 6.036625385284424
Epoch 730, val loss: 0.643688440322876
Epoch 740, training loss: 12.203655242919922 = 0.12492205947637558 + 2.0 * 6.039366722106934
Epoch 740, val loss: 0.6455663442611694
Epoch 750, training loss: 12.190580368041992 = 0.11918852478265762 + 2.0 * 6.035696029663086
Epoch 750, val loss: 0.6477023959159851
Epoch 760, training loss: 12.179834365844727 = 0.11385524272918701 + 2.0 * 6.032989501953125
Epoch 760, val loss: 0.6501398682594299
Epoch 770, training loss: 12.174896240234375 = 0.10883811861276627 + 2.0 * 6.033029079437256
Epoch 770, val loss: 0.6527236700057983
Epoch 780, training loss: 12.168560028076172 = 0.10411521047353745 + 2.0 * 6.032222270965576
Epoch 780, val loss: 0.6555585861206055
Epoch 790, training loss: 12.16247272491455 = 0.09969475120306015 + 2.0 * 6.031388759613037
Epoch 790, val loss: 0.6585614085197449
Epoch 800, training loss: 12.153670310974121 = 0.09553305059671402 + 2.0 * 6.029068470001221
Epoch 800, val loss: 0.6617343425750732
Epoch 810, training loss: 12.162349700927734 = 0.09160693734884262 + 2.0 * 6.03537130355835
Epoch 810, val loss: 0.665047824382782
Epoch 820, training loss: 12.152728080749512 = 0.08793757855892181 + 2.0 * 6.032395362854004
Epoch 820, val loss: 0.6684303283691406
Epoch 830, training loss: 12.140629768371582 = 0.08449922502040863 + 2.0 * 6.028065204620361
Epoch 830, val loss: 0.6719471216201782
Epoch 840, training loss: 12.13414478302002 = 0.08123539388179779 + 2.0 * 6.026454925537109
Epoch 840, val loss: 0.6755916476249695
Epoch 850, training loss: 12.13817024230957 = 0.07812385261058807 + 2.0 * 6.030023097991943
Epoch 850, val loss: 0.6793022751808167
Epoch 860, training loss: 12.12702465057373 = 0.07518092542886734 + 2.0 * 6.025921821594238
Epoch 860, val loss: 0.6831881403923035
Epoch 870, training loss: 12.120890617370605 = 0.07239802926778793 + 2.0 * 6.0242462158203125
Epoch 870, val loss: 0.6870549917221069
Epoch 880, training loss: 12.136388778686523 = 0.069747194647789 + 2.0 * 6.033320903778076
Epoch 880, val loss: 0.6910336017608643
Epoch 890, training loss: 12.113978385925293 = 0.06725375354290009 + 2.0 * 6.023362159729004
Epoch 890, val loss: 0.6948727369308472
Epoch 900, training loss: 12.11101245880127 = 0.06490325182676315 + 2.0 * 6.023054599761963
Epoch 900, val loss: 0.6988961696624756
Epoch 910, training loss: 12.104527473449707 = 0.06265000998973846 + 2.0 * 6.020938873291016
Epoch 910, val loss: 0.7029456496238708
Epoch 920, training loss: 12.101677894592285 = 0.060486987233161926 + 2.0 * 6.020595550537109
Epoch 920, val loss: 0.7070274949073792
Epoch 930, training loss: 12.10777473449707 = 0.05842483788728714 + 2.0 * 6.024674892425537
Epoch 930, val loss: 0.7111809253692627
Epoch 940, training loss: 12.100849151611328 = 0.056463152170181274 + 2.0 * 6.02219295501709
Epoch 940, val loss: 0.7152857780456543
Epoch 950, training loss: 12.101541519165039 = 0.054613146930933 + 2.0 * 6.023464202880859
Epoch 950, val loss: 0.7194088101387024
Epoch 960, training loss: 12.090664863586426 = 0.05285472050309181 + 2.0 * 6.018905162811279
Epoch 960, val loss: 0.7234894633293152
Epoch 970, training loss: 12.087716102600098 = 0.051175087690353394 + 2.0 * 6.018270492553711
Epoch 970, val loss: 0.7276637554168701
Epoch 980, training loss: 12.084486961364746 = 0.049560122191905975 + 2.0 * 6.017463207244873
Epoch 980, val loss: 0.7318360209465027
Epoch 990, training loss: 12.092941284179688 = 0.0480111800134182 + 2.0 * 6.022465229034424
Epoch 990, val loss: 0.7359482645988464
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9004
Flip ASR: 0.8800/225 nodes
The final ASR:0.70726, 0.16803, Accuracy:0.80741, 0.02117
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11584])
remove edge: torch.Size([2, 9506])
updated graph: torch.Size([2, 10534])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9705
Flip ASR: 0.9644/225 nodes
The final ASR:0.97417, 0.00301, Accuracy:0.83086, 0.00349
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 18.704833984375 = 1.9569921493530273 + 2.0 * 8.373921394348145
Epoch 0, val loss: 1.9552675485610962
Epoch 10, training loss: 18.693748474121094 = 1.9466452598571777 + 2.0 * 8.373551368713379
Epoch 10, val loss: 1.9457207918167114
Epoch 20, training loss: 18.675434112548828 = 1.9338369369506836 + 2.0 * 8.370798110961914
Epoch 20, val loss: 1.933584451675415
Epoch 30, training loss: 18.615299224853516 = 1.9164410829544067 + 2.0 * 8.3494291305542
Epoch 30, val loss: 1.9169347286224365
Epoch 40, training loss: 18.290752410888672 = 1.8951735496520996 + 2.0 * 8.197789192199707
Epoch 40, val loss: 1.8975186347961426
Epoch 50, training loss: 16.582414627075195 = 1.874835729598999 + 2.0 * 7.353789329528809
Epoch 50, val loss: 1.879533290863037
Epoch 60, training loss: 15.918190956115723 = 1.8566524982452393 + 2.0 * 7.030769348144531
Epoch 60, val loss: 1.8625551462173462
Epoch 70, training loss: 15.4432373046875 = 1.8388750553131104 + 2.0 * 6.802181243896484
Epoch 70, val loss: 1.845926284790039
Epoch 80, training loss: 15.170493125915527 = 1.8228839635849 + 2.0 * 6.673804759979248
Epoch 80, val loss: 1.8310716152191162
Epoch 90, training loss: 15.006572723388672 = 1.8075023889541626 + 2.0 * 6.59953498840332
Epoch 90, val loss: 1.8168855905532837
Epoch 100, training loss: 14.870983123779297 = 1.7923178672790527 + 2.0 * 6.539332866668701
Epoch 100, val loss: 1.8029218912124634
Epoch 110, training loss: 14.765753746032715 = 1.7775115966796875 + 2.0 * 6.494121074676514
Epoch 110, val loss: 1.7894878387451172
Epoch 120, training loss: 14.662017822265625 = 1.763015866279602 + 2.0 * 6.449501037597656
Epoch 120, val loss: 1.7765822410583496
Epoch 130, training loss: 14.560774803161621 = 1.748417615890503 + 2.0 * 6.4061784744262695
Epoch 130, val loss: 1.7638636827468872
Epoch 140, training loss: 14.467767715454102 = 1.7330459356307983 + 2.0 * 6.367361068725586
Epoch 140, val loss: 1.7509866952896118
Epoch 150, training loss: 14.393376350402832 = 1.7160604000091553 + 2.0 * 6.338657855987549
Epoch 150, val loss: 1.7368758916854858
Epoch 160, training loss: 14.32099437713623 = 1.696582555770874 + 2.0 * 6.312205791473389
Epoch 160, val loss: 1.7211482524871826
Epoch 170, training loss: 14.250782012939453 = 1.6746774911880493 + 2.0 * 6.288052082061768
Epoch 170, val loss: 1.7035685777664185
Epoch 180, training loss: 14.181239128112793 = 1.6498711109161377 + 2.0 * 6.265684127807617
Epoch 180, val loss: 1.684009075164795
Epoch 190, training loss: 14.117255210876465 = 1.6215909719467163 + 2.0 * 6.247832298278809
Epoch 190, val loss: 1.6617203950881958
Epoch 200, training loss: 14.053452491760254 = 1.5892066955566406 + 2.0 * 6.232122898101807
Epoch 200, val loss: 1.636217713356018
Epoch 210, training loss: 13.98774242401123 = 1.5522631406784058 + 2.0 * 6.217739582061768
Epoch 210, val loss: 1.6071770191192627
Epoch 220, training loss: 13.924166679382324 = 1.5108543634414673 + 2.0 * 6.206655979156494
Epoch 220, val loss: 1.5745797157287598
Epoch 230, training loss: 13.85450553894043 = 1.4649343490600586 + 2.0 * 6.1947855949401855
Epoch 230, val loss: 1.5385795831680298
Epoch 240, training loss: 13.784544944763184 = 1.4146413803100586 + 2.0 * 6.1849517822265625
Epoch 240, val loss: 1.4991422891616821
Epoch 250, training loss: 13.717045783996582 = 1.3602526187896729 + 2.0 * 6.178396701812744
Epoch 250, val loss: 1.456703543663025
Epoch 260, training loss: 13.641650199890137 = 1.3035755157470703 + 2.0 * 6.169037342071533
Epoch 260, val loss: 1.4125192165374756
Epoch 270, training loss: 13.569319725036621 = 1.245854139328003 + 2.0 * 6.1617326736450195
Epoch 270, val loss: 1.3680779933929443
Epoch 280, training loss: 13.496675491333008 = 1.188178300857544 + 2.0 * 6.1542487144470215
Epoch 280, val loss: 1.324080228805542
Epoch 290, training loss: 13.427425384521484 = 1.1321231126785278 + 2.0 * 6.147651195526123
Epoch 290, val loss: 1.2817121744155884
Epoch 300, training loss: 13.361749649047852 = 1.0789225101470947 + 2.0 * 6.141413688659668
Epoch 300, val loss: 1.2422926425933838
Epoch 310, training loss: 13.300932884216309 = 1.0287891626358032 + 2.0 * 6.136071681976318
Epoch 310, val loss: 1.205769658088684
Epoch 320, training loss: 13.248754501342773 = 0.9820501804351807 + 2.0 * 6.133352279663086
Epoch 320, val loss: 1.1723928451538086
Epoch 330, training loss: 13.194339752197266 = 0.939080536365509 + 2.0 * 6.12762975692749
Epoch 330, val loss: 1.1424561738967896
Epoch 340, training loss: 13.143736839294434 = 0.8994709253311157 + 2.0 * 6.122132778167725
Epoch 340, val loss: 1.115695595741272
Epoch 350, training loss: 13.098699569702148 = 0.8625181913375854 + 2.0 * 6.118090629577637
Epoch 350, val loss: 1.0914442539215088
Epoch 360, training loss: 13.05787181854248 = 0.8277164697647095 + 2.0 * 6.115077495574951
Epoch 360, val loss: 1.0691707134246826
Epoch 370, training loss: 13.026126861572266 = 0.7949270606040955 + 2.0 * 6.115600109100342
Epoch 370, val loss: 1.0487085580825806
Epoch 380, training loss: 12.981252670288086 = 0.7637796998023987 + 2.0 * 6.108736515045166
Epoch 380, val loss: 1.0298460721969604
Epoch 390, training loss: 12.943161010742188 = 0.7338408827781677 + 2.0 * 6.1046600341796875
Epoch 390, val loss: 1.0120031833648682
Epoch 400, training loss: 12.91386604309082 = 0.7047144174575806 + 2.0 * 6.1045756340026855
Epoch 400, val loss: 0.9948740601539612
Epoch 410, training loss: 12.876481056213379 = 0.6761891841888428 + 2.0 * 6.1001458168029785
Epoch 410, val loss: 0.9785954356193542
Epoch 420, training loss: 12.85094165802002 = 0.6484418511390686 + 2.0 * 6.101249694824219
Epoch 420, val loss: 0.9628208875656128
Epoch 430, training loss: 12.809812545776367 = 0.6213399171829224 + 2.0 * 6.094236373901367
Epoch 430, val loss: 0.9476957321166992
Epoch 440, training loss: 12.778887748718262 = 0.5947327017784119 + 2.0 * 6.092077732086182
Epoch 440, val loss: 0.9330728054046631
Epoch 450, training loss: 12.749828338623047 = 0.5685588717460632 + 2.0 * 6.090634822845459
Epoch 450, val loss: 0.9189924597740173
Epoch 460, training loss: 12.72201156616211 = 0.5431197881698608 + 2.0 * 6.089446067810059
Epoch 460, val loss: 0.905517041683197
Epoch 470, training loss: 12.690284729003906 = 0.5183979272842407 + 2.0 * 6.085943222045898
Epoch 470, val loss: 0.8929175138473511
Epoch 480, training loss: 12.662405014038086 = 0.4943445324897766 + 2.0 * 6.0840301513671875
Epoch 480, val loss: 0.8810413479804993
Epoch 490, training loss: 12.645008087158203 = 0.4709319472312927 + 2.0 * 6.087038040161133
Epoch 490, val loss: 0.869770884513855
Epoch 500, training loss: 12.612451553344727 = 0.4482274055480957 + 2.0 * 6.0821123123168945
Epoch 500, val loss: 0.8592449426651001
Epoch 510, training loss: 12.585623741149902 = 0.42620202898979187 + 2.0 * 6.079710960388184
Epoch 510, val loss: 0.8494464755058289
Epoch 520, training loss: 12.560628890991211 = 0.4048287868499756 + 2.0 * 6.077899932861328
Epoch 520, val loss: 0.8402919173240662
Epoch 530, training loss: 12.537728309631348 = 0.3840491473674774 + 2.0 * 6.076839447021484
Epoch 530, val loss: 0.8317702412605286
Epoch 540, training loss: 12.51732349395752 = 0.36396247148513794 + 2.0 * 6.076680660247803
Epoch 540, val loss: 0.8238475918769836
Epoch 550, training loss: 12.489850044250488 = 0.34463030099868774 + 2.0 * 6.072609901428223
Epoch 550, val loss: 0.8165903687477112
Epoch 560, training loss: 12.467887878417969 = 0.32598915696144104 + 2.0 * 6.070949554443359
Epoch 560, val loss: 0.8100561499595642
Epoch 570, training loss: 12.44938850402832 = 0.3079451024532318 + 2.0 * 6.070721626281738
Epoch 570, val loss: 0.8040600419044495
Epoch 580, training loss: 12.44062614440918 = 0.29087892174720764 + 2.0 * 6.074873447418213
Epoch 580, val loss: 0.7986253499984741
Epoch 590, training loss: 12.407654762268066 = 0.2746724784374237 + 2.0 * 6.06649112701416
Epoch 590, val loss: 0.7940460443496704
Epoch 600, training loss: 12.390301704406738 = 0.25927937030792236 + 2.0 * 6.065511226654053
Epoch 600, val loss: 0.790270209312439
Epoch 610, training loss: 12.37165355682373 = 0.2446211874485016 + 2.0 * 6.063516139984131
Epoch 610, val loss: 0.7871081233024597
Epoch 620, training loss: 12.365522384643555 = 0.23064151406288147 + 2.0 * 6.067440509796143
Epoch 620, val loss: 0.7845567464828491
Epoch 630, training loss: 12.343646049499512 = 0.21749788522720337 + 2.0 * 6.063074111938477
Epoch 630, val loss: 0.782558262348175
Epoch 640, training loss: 12.33038330078125 = 0.20509715378284454 + 2.0 * 6.062643051147461
Epoch 640, val loss: 0.7813388109207153
Epoch 650, training loss: 12.313053131103516 = 0.19345954060554504 + 2.0 * 6.0597968101501465
Epoch 650, val loss: 0.7807791233062744
Epoch 660, training loss: 12.298770904541016 = 0.18253201246261597 + 2.0 * 6.058119297027588
Epoch 660, val loss: 0.7806658148765564
Epoch 670, training loss: 12.284536361694336 = 0.17223963141441345 + 2.0 * 6.056148529052734
Epoch 670, val loss: 0.7811458110809326
Epoch 680, training loss: 12.283956527709961 = 0.16259939968585968 + 2.0 * 6.060678482055664
Epoch 680, val loss: 0.7819557189941406
Epoch 690, training loss: 12.2677001953125 = 0.1536710411310196 + 2.0 * 6.057014465332031
Epoch 690, val loss: 0.7832745909690857
Epoch 700, training loss: 12.25146770477295 = 0.14532162249088287 + 2.0 * 6.053072929382324
Epoch 700, val loss: 0.7850663065910339
Epoch 710, training loss: 12.239082336425781 = 0.13748867809772491 + 2.0 * 6.050796985626221
Epoch 710, val loss: 0.7871808409690857
Epoch 720, training loss: 12.25018310546875 = 0.13012464344501495 + 2.0 * 6.060029029846191
Epoch 720, val loss: 0.7895010709762573
Epoch 730, training loss: 12.222393989562988 = 0.12333793938159943 + 2.0 * 6.049528121948242
Epoch 730, val loss: 0.7922499179840088
Epoch 740, training loss: 12.213136672973633 = 0.1169859915971756 + 2.0 * 6.048075199127197
Epoch 740, val loss: 0.7953602075576782
Epoch 750, training loss: 12.204214096069336 = 0.11104297637939453 + 2.0 * 6.046585559844971
Epoch 750, val loss: 0.798678457736969
Epoch 760, training loss: 12.199370384216309 = 0.1054663360118866 + 2.0 * 6.046952247619629
Epoch 760, val loss: 0.8022210597991943
Epoch 770, training loss: 12.193642616271973 = 0.10024659335613251 + 2.0 * 6.046698093414307
Epoch 770, val loss: 0.8058870434761047
Epoch 780, training loss: 12.184003829956055 = 0.0953952819108963 + 2.0 * 6.044304370880127
Epoch 780, val loss: 0.809809684753418
Epoch 790, training loss: 12.176983833312988 = 0.09082863479852676 + 2.0 * 6.04307746887207
Epoch 790, val loss: 0.8138891458511353
Epoch 800, training loss: 12.17237377166748 = 0.08654157817363739 + 2.0 * 6.042916297912598
Epoch 800, val loss: 0.8181324601173401
Epoch 810, training loss: 12.166069030761719 = 0.08251220732927322 + 2.0 * 6.041778564453125
Epoch 810, val loss: 0.8223733305931091
Epoch 820, training loss: 12.160317420959473 = 0.07873309403657913 + 2.0 * 6.040791988372803
Epoch 820, val loss: 0.8267756700515747
Epoch 830, training loss: 12.158700942993164 = 0.07518690079450607 + 2.0 * 6.041757106781006
Epoch 830, val loss: 0.8312736749649048
Epoch 840, training loss: 12.149763107299805 = 0.07186145335435867 + 2.0 * 6.0389509201049805
Epoch 840, val loss: 0.8358832001686096
Epoch 850, training loss: 12.142721176147461 = 0.06871388852596283 + 2.0 * 6.037003517150879
Epoch 850, val loss: 0.8405343294143677
Epoch 860, training loss: 12.151294708251953 = 0.06575687974691391 + 2.0 * 6.042768955230713
Epoch 860, val loss: 0.8452771306037903
Epoch 870, training loss: 12.139036178588867 = 0.06296255439519882 + 2.0 * 6.038036823272705
Epoch 870, val loss: 0.8499719500541687
Epoch 880, training loss: 12.129687309265137 = 0.06033971533179283 + 2.0 * 6.034673690795898
Epoch 880, val loss: 0.8547722697257996
Epoch 890, training loss: 12.125865936279297 = 0.057862453162670135 + 2.0 * 6.03400182723999
Epoch 890, val loss: 0.8595831990242004
Epoch 900, training loss: 12.143738746643066 = 0.055521417409181595 + 2.0 * 6.044108867645264
Epoch 900, val loss: 0.8643511533737183
Epoch 910, training loss: 12.121841430664062 = 0.053316157311201096 + 2.0 * 6.034262657165527
Epoch 910, val loss: 0.8692033886909485
Epoch 920, training loss: 12.119233131408691 = 0.05123422294855118 + 2.0 * 6.033999443054199
Epoch 920, val loss: 0.8740885853767395
Epoch 930, training loss: 12.115683555603027 = 0.04927029088139534 + 2.0 * 6.033206462860107
Epoch 930, val loss: 0.87897127866745
Epoch 940, training loss: 12.108416557312012 = 0.04741659015417099 + 2.0 * 6.0304999351501465
Epoch 940, val loss: 0.883888840675354
Epoch 950, training loss: 12.108386993408203 = 0.045653291046619415 + 2.0 * 6.03136682510376
Epoch 950, val loss: 0.8887711763381958
Epoch 960, training loss: 12.103018760681152 = 0.04397085681557655 + 2.0 * 6.029523849487305
Epoch 960, val loss: 0.8936378955841064
Epoch 970, training loss: 12.106671333312988 = 0.04237782955169678 + 2.0 * 6.03214693069458
Epoch 970, val loss: 0.89848393201828
Epoch 980, training loss: 12.100505828857422 = 0.04087378829717636 + 2.0 * 6.029816150665283
Epoch 980, val loss: 0.9033485054969788
Epoch 990, training loss: 12.095105171203613 = 0.039451342076063156 + 2.0 * 6.02782678604126
Epoch 990, val loss: 0.9082522392272949
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8074
Overall ASR: 0.6384
Flip ASR: 0.5689/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.696813583374023 = 1.949101209640503 + 2.0 * 8.373856544494629
Epoch 0, val loss: 1.94931960105896
Epoch 10, training loss: 18.68300437927246 = 1.9378935098648071 + 2.0 * 8.37255573272705
Epoch 10, val loss: 1.9366230964660645
Epoch 20, training loss: 18.657596588134766 = 1.9240179061889648 + 2.0 * 8.366788864135742
Epoch 20, val loss: 1.919789433479309
Epoch 30, training loss: 18.57097816467285 = 1.9054628610610962 + 2.0 * 8.332757949829102
Epoch 30, val loss: 1.8963598012924194
Epoch 40, training loss: 18.094186782836914 = 1.8848687410354614 + 2.0 * 8.104659080505371
Epoch 40, val loss: 1.870894193649292
Epoch 50, training loss: 16.937856674194336 = 1.8660959005355835 + 2.0 * 7.535880088806152
Epoch 50, val loss: 1.8479125499725342
Epoch 60, training loss: 16.139720916748047 = 1.844637155532837 + 2.0 * 7.1475419998168945
Epoch 60, val loss: 1.825006127357483
Epoch 70, training loss: 15.574875831604004 = 1.8247936964035034 + 2.0 * 6.8750410079956055
Epoch 70, val loss: 1.8055907487869263
Epoch 80, training loss: 15.224405288696289 = 1.809377908706665 + 2.0 * 6.707513809204102
Epoch 80, val loss: 1.7911484241485596
Epoch 90, training loss: 14.950335502624512 = 1.7961623668670654 + 2.0 * 6.577086448669434
Epoch 90, val loss: 1.778484582901001
Epoch 100, training loss: 14.777515411376953 = 1.7821524143218994 + 2.0 * 6.497681617736816
Epoch 100, val loss: 1.76510751247406
Epoch 110, training loss: 14.63199234008789 = 1.767112135887146 + 2.0 * 6.432440280914307
Epoch 110, val loss: 1.7510509490966797
Epoch 120, training loss: 14.52059268951416 = 1.7519809007644653 + 2.0 * 6.384305953979492
Epoch 120, val loss: 1.737078309059143
Epoch 130, training loss: 14.42611312866211 = 1.7360491752624512 + 2.0 * 6.345032215118408
Epoch 130, val loss: 1.722536563873291
Epoch 140, training loss: 14.337309837341309 = 1.7189805507659912 + 2.0 * 6.309164524078369
Epoch 140, val loss: 1.707077980041504
Epoch 150, training loss: 14.260351181030273 = 1.7001984119415283 + 2.0 * 6.280076503753662
Epoch 150, val loss: 1.6906033754348755
Epoch 160, training loss: 14.191914558410645 = 1.6792261600494385 + 2.0 * 6.256344318389893
Epoch 160, val loss: 1.6722232103347778
Epoch 170, training loss: 14.131134986877441 = 1.6556235551834106 + 2.0 * 6.23775577545166
Epoch 170, val loss: 1.6517020463943481
Epoch 180, training loss: 14.072160720825195 = 1.6289563179016113 + 2.0 * 6.221602439880371
Epoch 180, val loss: 1.6287293434143066
Epoch 190, training loss: 14.013810157775879 = 1.5985277891159058 + 2.0 * 6.207641124725342
Epoch 190, val loss: 1.602815866470337
Epoch 200, training loss: 13.959051132202148 = 1.5641224384307861 + 2.0 * 6.197464466094971
Epoch 200, val loss: 1.5737749338150024
Epoch 210, training loss: 13.896995544433594 = 1.5261074304580688 + 2.0 * 6.185443878173828
Epoch 210, val loss: 1.5421682596206665
Epoch 220, training loss: 13.835521697998047 = 1.484560251235962 + 2.0 * 6.175480842590332
Epoch 220, val loss: 1.5078972578048706
Epoch 230, training loss: 13.776594161987305 = 1.4401204586029053 + 2.0 * 6.16823673248291
Epoch 230, val loss: 1.4715328216552734
Epoch 240, training loss: 13.714930534362793 = 1.3941633701324463 + 2.0 * 6.160383701324463
Epoch 240, val loss: 1.435245394706726
Epoch 250, training loss: 13.654158592224121 = 1.348293423652649 + 2.0 * 6.152932643890381
Epoch 250, val loss: 1.3993867635726929
Epoch 260, training loss: 13.593852043151855 = 1.3024340867996216 + 2.0 * 6.145709037780762
Epoch 260, val loss: 1.364241361618042
Epoch 270, training loss: 13.538010597229004 = 1.2573044300079346 + 2.0 * 6.140353202819824
Epoch 270, val loss: 1.3301568031311035
Epoch 280, training loss: 13.486677169799805 = 1.2130900621414185 + 2.0 * 6.136793613433838
Epoch 280, val loss: 1.2975616455078125
Epoch 290, training loss: 13.43116283416748 = 1.1705114841461182 + 2.0 * 6.130325794219971
Epoch 290, val loss: 1.2661265134811401
Epoch 300, training loss: 13.378387451171875 = 1.1289376020431519 + 2.0 * 6.124724864959717
Epoch 300, val loss: 1.2356724739074707
Epoch 310, training loss: 13.330788612365723 = 1.0880061388015747 + 2.0 * 6.121391296386719
Epoch 310, val loss: 1.2056111097335815
Epoch 320, training loss: 13.28878402709961 = 1.0478650331497192 + 2.0 * 6.12045955657959
Epoch 320, val loss: 1.176107406616211
Epoch 330, training loss: 13.234567642211914 = 1.0089792013168335 + 2.0 * 6.112794399261475
Epoch 330, val loss: 1.1472930908203125
Epoch 340, training loss: 13.18853759765625 = 0.9710144400596619 + 2.0 * 6.108761787414551
Epoch 340, val loss: 1.11920166015625
Epoch 350, training loss: 13.147892951965332 = 0.9340370297431946 + 2.0 * 6.106927871704102
Epoch 350, val loss: 1.0919477939605713
Epoch 360, training loss: 13.112580299377441 = 0.8987268805503845 + 2.0 * 6.106926918029785
Epoch 360, val loss: 1.065700888633728
Epoch 370, training loss: 13.065116882324219 = 0.8650672435760498 + 2.0 * 6.100024700164795
Epoch 370, val loss: 1.0412975549697876
Epoch 380, training loss: 13.024862289428711 = 0.8331986665725708 + 2.0 * 6.095831871032715
Epoch 380, val loss: 1.0183213949203491
Epoch 390, training loss: 12.9884614944458 = 0.8030893206596375 + 2.0 * 6.092686176300049
Epoch 390, val loss: 0.9970059990882874
Epoch 400, training loss: 12.971545219421387 = 0.7748137712478638 + 2.0 * 6.098365783691406
Epoch 400, val loss: 0.9775367975234985
Epoch 410, training loss: 12.92697525024414 = 0.748410701751709 + 2.0 * 6.089282035827637
Epoch 410, val loss: 0.9599992632865906
Epoch 420, training loss: 12.893277168273926 = 0.7237187623977661 + 2.0 * 6.084779262542725
Epoch 420, val loss: 0.9443039298057556
Epoch 430, training loss: 12.86710262298584 = 0.7004519701004028 + 2.0 * 6.083325386047363
Epoch 430, val loss: 0.9302968978881836
Epoch 440, training loss: 12.848687171936035 = 0.6785802841186523 + 2.0 * 6.085053443908691
Epoch 440, val loss: 0.9176306128501892
Epoch 450, training loss: 12.815330505371094 = 0.6578007340431213 + 2.0 * 6.078764915466309
Epoch 450, val loss: 0.9064361453056335
Epoch 460, training loss: 12.794938087463379 = 0.6378976106643677 + 2.0 * 6.07852029800415
Epoch 460, val loss: 0.896293580532074
Epoch 470, training loss: 12.765841484069824 = 0.6186918616294861 + 2.0 * 6.073575019836426
Epoch 470, val loss: 0.8870105147361755
Epoch 480, training loss: 12.740315437316895 = 0.5998944044113159 + 2.0 * 6.0702104568481445
Epoch 480, val loss: 0.8784153461456299
Epoch 490, training loss: 12.71966552734375 = 0.5813812613487244 + 2.0 * 6.0691423416137695
Epoch 490, val loss: 0.8702790141105652
Epoch 500, training loss: 12.697386741638184 = 0.5630951523780823 + 2.0 * 6.067145824432373
Epoch 500, val loss: 0.8625774383544922
Epoch 510, training loss: 12.680288314819336 = 0.5449956059455872 + 2.0 * 6.067646503448486
Epoch 510, val loss: 0.8552221059799194
Epoch 520, training loss: 12.658244132995605 = 0.5269784927368164 + 2.0 * 6.0656328201293945
Epoch 520, val loss: 0.8481895327568054
Epoch 530, training loss: 12.633795738220215 = 0.5091219544410706 + 2.0 * 6.0623369216918945
Epoch 530, val loss: 0.8413755297660828
Epoch 540, training loss: 12.610881805419922 = 0.4912909269332886 + 2.0 * 6.059795379638672
Epoch 540, val loss: 0.8348545432090759
Epoch 550, training loss: 12.595868110656738 = 0.4735322594642639 + 2.0 * 6.0611677169799805
Epoch 550, val loss: 0.8283874988555908
Epoch 560, training loss: 12.57996654510498 = 0.45589974522590637 + 2.0 * 6.062033176422119
Epoch 560, val loss: 0.8219939470291138
Epoch 570, training loss: 12.550477981567383 = 0.4384293556213379 + 2.0 * 6.056024551391602
Epoch 570, val loss: 0.8157427906990051
Epoch 580, training loss: 12.530253410339355 = 0.4211258590221405 + 2.0 * 6.054563999176025
Epoch 580, val loss: 0.8096212148666382
Epoch 590, training loss: 12.508840560913086 = 0.4039798080921173 + 2.0 * 6.052430152893066
Epoch 590, val loss: 0.8036940693855286
Epoch 600, training loss: 12.490071296691895 = 0.3870091736316681 + 2.0 * 6.051530838012695
Epoch 600, val loss: 0.7979435324668884
Epoch 610, training loss: 12.484257698059082 = 0.3702493906021118 + 2.0 * 6.057003974914551
Epoch 610, val loss: 0.7922990322113037
Epoch 620, training loss: 12.462106704711914 = 0.3539700210094452 + 2.0 * 6.054068565368652
Epoch 620, val loss: 0.7869381904602051
Epoch 630, training loss: 12.436847686767578 = 0.33814510703086853 + 2.0 * 6.049351215362549
Epoch 630, val loss: 0.7819789052009583
Epoch 640, training loss: 12.416601181030273 = 0.3226882815361023 + 2.0 * 6.046956539154053
Epoch 640, val loss: 0.7774430513381958
Epoch 650, training loss: 12.399698257446289 = 0.3076344430446625 + 2.0 * 6.046031951904297
Epoch 650, val loss: 0.7733272910118103
Epoch 660, training loss: 12.410299301147461 = 0.29297569394111633 + 2.0 * 6.058661937713623
Epoch 660, val loss: 0.7695654630661011
Epoch 670, training loss: 12.367077827453613 = 0.27895259857177734 + 2.0 * 6.044062614440918
Epoch 670, val loss: 0.7662714123725891
Epoch 680, training loss: 12.353961944580078 = 0.2654113471508026 + 2.0 * 6.044275283813477
Epoch 680, val loss: 0.7635602355003357
Epoch 690, training loss: 12.336052894592285 = 0.25231438875198364 + 2.0 * 6.041869163513184
Epoch 690, val loss: 0.7613503336906433
Epoch 700, training loss: 12.32348346710205 = 0.23972678184509277 + 2.0 * 6.0418782234191895
Epoch 700, val loss: 0.7597300410270691
Epoch 710, training loss: 12.324949264526367 = 0.22772598266601562 + 2.0 * 6.048611640930176
Epoch 710, val loss: 0.758507251739502
Epoch 720, training loss: 12.298328399658203 = 0.2162306010723114 + 2.0 * 6.041049003601074
Epoch 720, val loss: 0.7578399777412415
Epoch 730, training loss: 12.2833890914917 = 0.20533563196659088 + 2.0 * 6.039026737213135
Epoch 730, val loss: 0.7577148079872131
Epoch 740, training loss: 12.270206451416016 = 0.19493719935417175 + 2.0 * 6.03763484954834
Epoch 740, val loss: 0.7581247091293335
Epoch 750, training loss: 12.258003234863281 = 0.185021311044693 + 2.0 * 6.0364909172058105
Epoch 750, val loss: 0.7589654326438904
Epoch 760, training loss: 12.248598098754883 = 0.17556779086589813 + 2.0 * 6.036515235900879
Epoch 760, val loss: 0.7602301239967346
Epoch 770, training loss: 12.253856658935547 = 0.1665949821472168 + 2.0 * 6.043630599975586
Epoch 770, val loss: 0.7618567943572998
Epoch 780, training loss: 12.230489730834961 = 0.15817388892173767 + 2.0 * 6.036158084869385
Epoch 780, val loss: 0.7637861371040344
Epoch 790, training loss: 12.21800708770752 = 0.15020333230495453 + 2.0 * 6.033901691436768
Epoch 790, val loss: 0.7661935687065125
Epoch 800, training loss: 12.207950592041016 = 0.14266543090343475 + 2.0 * 6.032642364501953
Epoch 800, val loss: 0.7690175175666809
Epoch 810, training loss: 12.198433876037598 = 0.1355285346508026 + 2.0 * 6.031452655792236
Epoch 810, val loss: 0.7721766829490662
Epoch 820, training loss: 12.211570739746094 = 0.1287609487771988 + 2.0 * 6.041404724121094
Epoch 820, val loss: 0.7756578922271729
Epoch 830, training loss: 12.192408561706543 = 0.12243776768445969 + 2.0 * 6.034985542297363
Epoch 830, val loss: 0.7793079614639282
Epoch 840, training loss: 12.1753568649292 = 0.11644443869590759 + 2.0 * 6.02945613861084
Epoch 840, val loss: 0.7832488417625427
Epoch 850, training loss: 12.168668746948242 = 0.11076522618532181 + 2.0 * 6.028951644897461
Epoch 850, val loss: 0.7874414324760437
Epoch 860, training loss: 12.16812801361084 = 0.1053847149014473 + 2.0 * 6.031371593475342
Epoch 860, val loss: 0.7918715476989746
Epoch 870, training loss: 12.155349731445312 = 0.1003328412771225 + 2.0 * 6.02750825881958
Epoch 870, val loss: 0.7963956594467163
Epoch 880, training loss: 12.151274681091309 = 0.09561486542224884 + 2.0 * 6.027830123901367
Epoch 880, val loss: 0.8010686039924622
Epoch 890, training loss: 12.152261734008789 = 0.09114743024110794 + 2.0 * 6.030557155609131
Epoch 890, val loss: 0.8059656023979187
Epoch 900, training loss: 12.139481544494629 = 0.08696626871824265 + 2.0 * 6.026257514953613
Epoch 900, val loss: 0.8109444975852966
Epoch 910, training loss: 12.131831169128418 = 0.08302485942840576 + 2.0 * 6.024403095245361
Epoch 910, val loss: 0.8160400390625
Epoch 920, training loss: 12.12690258026123 = 0.0793144479393959 + 2.0 * 6.023794174194336
Epoch 920, val loss: 0.8212378025054932
Epoch 930, training loss: 12.120819091796875 = 0.07578874379396439 + 2.0 * 6.022515296936035
Epoch 930, val loss: 0.826535165309906
Epoch 940, training loss: 12.117154121398926 = 0.07244411110877991 + 2.0 * 6.022355079650879
Epoch 940, val loss: 0.831897497177124
Epoch 950, training loss: 12.127145767211914 = 0.06928680092096329 + 2.0 * 6.028929710388184
Epoch 950, val loss: 0.8373075723648071
Epoch 960, training loss: 12.119999885559082 = 0.06631282716989517 + 2.0 * 6.026843547821045
Epoch 960, val loss: 0.8427087068557739
Epoch 970, training loss: 12.1185302734375 = 0.06350808590650558 + 2.0 * 6.027511119842529
Epoch 970, val loss: 0.8481659889221191
Epoch 980, training loss: 12.102478981018066 = 0.06089654937386513 + 2.0 * 6.020791053771973
Epoch 980, val loss: 0.8536306023597717
Epoch 990, training loss: 12.09756088256836 = 0.05842699855566025 + 2.0 * 6.019567012786865
Epoch 990, val loss: 0.859125018119812
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8148
Overall ASR: 0.1624
Flip ASR: 0.1822/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.70962142944336 = 1.961779236793518 + 2.0 * 8.373921394348145
Epoch 0, val loss: 1.961256742477417
Epoch 10, training loss: 18.698713302612305 = 1.9515327215194702 + 2.0 * 8.373590469360352
Epoch 10, val loss: 1.9511244297027588
Epoch 20, training loss: 18.68054962158203 = 1.9387426376342773 + 2.0 * 8.370903968811035
Epoch 20, val loss: 1.9380329847335815
Epoch 30, training loss: 18.621353149414062 = 1.9209630489349365 + 2.0 * 8.350194931030273
Epoch 30, val loss: 1.9196078777313232
Epoch 40, training loss: 18.345176696777344 = 1.8984216451644897 + 2.0 * 8.223377227783203
Epoch 40, val loss: 1.8971045017242432
Epoch 50, training loss: 17.346044540405273 = 1.875274896621704 + 2.0 * 7.735384464263916
Epoch 50, val loss: 1.8751158714294434
Epoch 60, training loss: 16.624252319335938 = 1.8549926280975342 + 2.0 * 7.384629726409912
Epoch 60, val loss: 1.8564317226409912
Epoch 70, training loss: 15.733277320861816 = 1.8389135599136353 + 2.0 * 6.947181701660156
Epoch 70, val loss: 1.8420120477676392
Epoch 80, training loss: 15.213993072509766 = 1.8255585432052612 + 2.0 * 6.694217205047607
Epoch 80, val loss: 1.8297390937805176
Epoch 90, training loss: 14.984020233154297 = 1.810978651046753 + 2.0 * 6.586520671844482
Epoch 90, val loss: 1.8159371614456177
Epoch 100, training loss: 14.856870651245117 = 1.7942681312561035 + 2.0 * 6.531301021575928
Epoch 100, val loss: 1.8008126020431519
Epoch 110, training loss: 14.749341011047363 = 1.7778888940811157 + 2.0 * 6.4857258796691895
Epoch 110, val loss: 1.7866652011871338
Epoch 120, training loss: 14.650544166564941 = 1.7623786926269531 + 2.0 * 6.444082736968994
Epoch 120, val loss: 1.7735589742660522
Epoch 130, training loss: 14.561904907226562 = 1.7463469505310059 + 2.0 * 6.407779216766357
Epoch 130, val loss: 1.7600030899047852
Epoch 140, training loss: 14.479954719543457 = 1.729203462600708 + 2.0 * 6.375375747680664
Epoch 140, val loss: 1.7457904815673828
Epoch 150, training loss: 14.394346237182617 = 1.7108540534973145 + 2.0 * 6.3417463302612305
Epoch 150, val loss: 1.7306842803955078
Epoch 160, training loss: 14.310924530029297 = 1.6906479597091675 + 2.0 * 6.31013822555542
Epoch 160, val loss: 1.7139273881912231
Epoch 170, training loss: 14.23486042022705 = 1.667986273765564 + 2.0 * 6.283437252044678
Epoch 170, val loss: 1.6952093839645386
Epoch 180, training loss: 14.165144920349121 = 1.6420812606811523 + 2.0 * 6.261531829833984
Epoch 180, val loss: 1.673730492591858
Epoch 190, training loss: 14.09730339050293 = 1.612821102142334 + 2.0 * 6.242241382598877
Epoch 190, val loss: 1.6494638919830322
Epoch 200, training loss: 14.03110408782959 = 1.5799204111099243 + 2.0 * 6.225591659545898
Epoch 200, val loss: 1.6222118139266968
Epoch 210, training loss: 13.968591690063477 = 1.5431383848190308 + 2.0 * 6.212726593017578
Epoch 210, val loss: 1.591858983039856
Epoch 220, training loss: 13.904319763183594 = 1.5027456283569336 + 2.0 * 6.20078706741333
Epoch 220, val loss: 1.558533787727356
Epoch 230, training loss: 13.838623046875 = 1.459086298942566 + 2.0 * 6.189768314361572
Epoch 230, val loss: 1.5225449800491333
Epoch 240, training loss: 13.775794982910156 = 1.4123051166534424 + 2.0 * 6.1817450523376465
Epoch 240, val loss: 1.4841279983520508
Epoch 250, training loss: 13.713601112365723 = 1.3632609844207764 + 2.0 * 6.175169944763184
Epoch 250, val loss: 1.4443457126617432
Epoch 260, training loss: 13.648201942443848 = 1.313111424446106 + 2.0 * 6.167545318603516
Epoch 260, val loss: 1.4036364555358887
Epoch 270, training loss: 13.58618450164795 = 1.2619045972824097 + 2.0 * 6.162139892578125
Epoch 270, val loss: 1.362428903579712
Epoch 280, training loss: 13.52591323852539 = 1.210134506225586 + 2.0 * 6.157889366149902
Epoch 280, val loss: 1.3211708068847656
Epoch 290, training loss: 13.462711334228516 = 1.1584101915359497 + 2.0 * 6.152150630950928
Epoch 290, val loss: 1.280226469039917
Epoch 300, training loss: 13.402132034301758 = 1.106844425201416 + 2.0 * 6.14764404296875
Epoch 300, val loss: 1.2396180629730225
Epoch 310, training loss: 13.340038299560547 = 1.054560899734497 + 2.0 * 6.1427388191223145
Epoch 310, val loss: 1.1987488269805908
Epoch 320, training loss: 13.286972045898438 = 1.002162218093872 + 2.0 * 6.142405033111572
Epoch 320, val loss: 1.1582729816436768
Epoch 330, training loss: 13.221354484558105 = 0.9508889317512512 + 2.0 * 6.135232925415039
Epoch 330, val loss: 1.1192117929458618
Epoch 340, training loss: 13.162693977355957 = 0.9010079503059387 + 2.0 * 6.130843162536621
Epoch 340, val loss: 1.0817421674728394
Epoch 350, training loss: 13.11069393157959 = 0.8527392148971558 + 2.0 * 6.128977298736572
Epoch 350, val loss: 1.045897364616394
Epoch 360, training loss: 13.056973457336426 = 0.8069364428520203 + 2.0 * 6.12501859664917
Epoch 360, val loss: 1.012447476387024
Epoch 370, training loss: 13.010174751281738 = 0.7637602090835571 + 2.0 * 6.123207092285156
Epoch 370, val loss: 0.9815011620521545
Epoch 380, training loss: 12.96008014678955 = 0.7238293290138245 + 2.0 * 6.1181254386901855
Epoch 380, val loss: 0.9532580375671387
Epoch 390, training loss: 12.912147521972656 = 0.6866979598999023 + 2.0 * 6.112724781036377
Epoch 390, val loss: 0.9276864528656006
Epoch 400, training loss: 12.873523712158203 = 0.6523838043212891 + 2.0 * 6.110569953918457
Epoch 400, val loss: 0.9046323895454407
Epoch 410, training loss: 12.83787727355957 = 0.6208030581474304 + 2.0 * 6.108537197113037
Epoch 410, val loss: 0.8841602802276611
Epoch 420, training loss: 12.79863452911377 = 0.5918483138084412 + 2.0 * 6.103393077850342
Epoch 420, val loss: 0.866111695766449
Epoch 430, training loss: 12.769968032836914 = 0.5651702880859375 + 2.0 * 6.102398872375488
Epoch 430, val loss: 0.8502376079559326
Epoch 440, training loss: 12.743491172790527 = 0.5407083630561829 + 2.0 * 6.101391315460205
Epoch 440, val loss: 0.83643639087677
Epoch 450, training loss: 12.710172653198242 = 0.5181663632392883 + 2.0 * 6.09600305557251
Epoch 450, val loss: 0.8245434165000916
Epoch 460, training loss: 12.683026313781738 = 0.49714940786361694 + 2.0 * 6.092938423156738
Epoch 460, val loss: 0.8142214417457581
Epoch 470, training loss: 12.657696723937988 = 0.47728854417800903 + 2.0 * 6.090204238891602
Epoch 470, val loss: 0.8051323294639587
Epoch 480, training loss: 12.64127254486084 = 0.45835646986961365 + 2.0 * 6.091457843780518
Epoch 480, val loss: 0.7970869541168213
Epoch 490, training loss: 12.62314224243164 = 0.4402988851070404 + 2.0 * 6.091421604156494
Epoch 490, val loss: 0.7899710536003113
Epoch 500, training loss: 12.592947959899902 = 0.4229280352592468 + 2.0 * 6.085010051727295
Epoch 500, val loss: 0.7836569547653198
Epoch 510, training loss: 12.56981086730957 = 0.4059690237045288 + 2.0 * 6.081921100616455
Epoch 510, val loss: 0.7780251502990723
Epoch 520, training loss: 12.565906524658203 = 0.3893003761768341 + 2.0 * 6.088303089141846
Epoch 520, val loss: 0.7729161381721497
Epoch 530, training loss: 12.528019905090332 = 0.37285104393959045 + 2.0 * 6.077584266662598
Epoch 530, val loss: 0.7682300209999084
Epoch 540, training loss: 12.508561134338379 = 0.3565327525138855 + 2.0 * 6.076014041900635
Epoch 540, val loss: 0.7640774250030518
Epoch 550, training loss: 12.494832038879395 = 0.3402387201786041 + 2.0 * 6.077296733856201
Epoch 550, val loss: 0.7603409290313721
Epoch 560, training loss: 12.47153377532959 = 0.3241182863712311 + 2.0 * 6.073707580566406
Epoch 560, val loss: 0.7571903467178345
Epoch 570, training loss: 12.450604438781738 = 0.3080434203147888 + 2.0 * 6.071280479431152
Epoch 570, val loss: 0.7544854879379272
Epoch 580, training loss: 12.432275772094727 = 0.29217177629470825 + 2.0 * 6.070052146911621
Epoch 580, val loss: 0.7525172233581543
Epoch 590, training loss: 12.41623306274414 = 0.27651628851890564 + 2.0 * 6.069858551025391
Epoch 590, val loss: 0.7510810494422913
Epoch 600, training loss: 12.397163391113281 = 0.2613569498062134 + 2.0 * 6.0679030418396
Epoch 600, val loss: 0.7503163814544678
Epoch 610, training loss: 12.375919342041016 = 0.2466706484556198 + 2.0 * 6.064624309539795
Epoch 610, val loss: 0.7502371072769165
Epoch 620, training loss: 12.358687400817871 = 0.23255501687526703 + 2.0 * 6.063066005706787
Epoch 620, val loss: 0.7507944703102112
Epoch 630, training loss: 12.342949867248535 = 0.21905890107154846 + 2.0 * 6.06194543838501
Epoch 630, val loss: 0.7519742250442505
Epoch 640, training loss: 12.339070320129395 = 0.20626962184906006 + 2.0 * 6.066400527954102
Epoch 640, val loss: 0.7537607550621033
Epoch 650, training loss: 12.314533233642578 = 0.19428929686546326 + 2.0 * 6.060122013092041
Epoch 650, val loss: 0.7560109496116638
Epoch 660, training loss: 12.299302101135254 = 0.18315984308719635 + 2.0 * 6.058071136474609
Epoch 660, val loss: 0.7589150667190552
Epoch 670, training loss: 12.287538528442383 = 0.17274968326091766 + 2.0 * 6.057394504547119
Epoch 670, val loss: 0.762446939945221
Epoch 680, training loss: 12.291455268859863 = 0.16307565569877625 + 2.0 * 6.064189910888672
Epoch 680, val loss: 0.7663139700889587
Epoch 690, training loss: 12.26500415802002 = 0.15420731902122498 + 2.0 * 6.055398464202881
Epoch 690, val loss: 0.7705444693565369
Epoch 700, training loss: 12.252695083618164 = 0.14598093926906586 + 2.0 * 6.053357124328613
Epoch 700, val loss: 0.7751486301422119
Epoch 710, training loss: 12.25871467590332 = 0.1383274346590042 + 2.0 * 6.0601935386657715
Epoch 710, val loss: 0.7800312638282776
Epoch 720, training loss: 12.237641334533691 = 0.1312633603811264 + 2.0 * 6.053188800811768
Epoch 720, val loss: 0.7849916219711304
Epoch 730, training loss: 12.225273132324219 = 0.12468213587999344 + 2.0 * 6.050295352935791
Epoch 730, val loss: 0.7902340888977051
Epoch 740, training loss: 12.217105865478516 = 0.11852622032165527 + 2.0 * 6.049289703369141
Epoch 740, val loss: 0.795699417591095
Epoch 750, training loss: 12.219369888305664 = 0.11277073621749878 + 2.0 * 6.053299427032471
Epoch 750, val loss: 0.8012556433677673
Epoch 760, training loss: 12.206223487854004 = 0.10745149105787277 + 2.0 * 6.049386024475098
Epoch 760, val loss: 0.8068581223487854
Epoch 770, training loss: 12.19568920135498 = 0.10246521979570389 + 2.0 * 6.046611785888672
Epoch 770, val loss: 0.8126052021980286
Epoch 780, training loss: 12.187444686889648 = 0.09776913374662399 + 2.0 * 6.044837951660156
Epoch 780, val loss: 0.8185340762138367
Epoch 790, training loss: 12.18275260925293 = 0.09333902597427368 + 2.0 * 6.04470682144165
Epoch 790, val loss: 0.8245716691017151
Epoch 800, training loss: 12.176569938659668 = 0.08917062729597092 + 2.0 * 6.043699741363525
Epoch 800, val loss: 0.8305869698524475
Epoch 810, training loss: 12.170960426330566 = 0.0852799117565155 + 2.0 * 6.042840480804443
Epoch 810, val loss: 0.8366333842277527
Epoch 820, training loss: 12.164752006530762 = 0.08161912858486176 + 2.0 * 6.041566371917725
Epoch 820, val loss: 0.842820942401886
Epoch 830, training loss: 12.161369323730469 = 0.07815402746200562 + 2.0 * 6.041607856750488
Epoch 830, val loss: 0.8491635918617249
Epoch 840, training loss: 12.153416633605957 = 0.07488136738538742 + 2.0 * 6.039267539978027
Epoch 840, val loss: 0.855411946773529
Epoch 850, training loss: 12.157025337219238 = 0.07180246710777283 + 2.0 * 6.042611598968506
Epoch 850, val loss: 0.8617275953292847
Epoch 860, training loss: 12.144050598144531 = 0.06888379901647568 + 2.0 * 6.037583351135254
Epoch 860, val loss: 0.8680437803268433
Epoch 870, training loss: 12.139283180236816 = 0.06612949073314667 + 2.0 * 6.036576747894287
Epoch 870, val loss: 0.8744827508926392
Epoch 880, training loss: 12.135045051574707 = 0.06351280212402344 + 2.0 * 6.035766124725342
Epoch 880, val loss: 0.8809772729873657
Epoch 890, training loss: 12.157926559448242 = 0.06102374568581581 + 2.0 * 6.0484514236450195
Epoch 890, val loss: 0.8874703049659729
Epoch 900, training loss: 12.128534317016602 = 0.05869191884994507 + 2.0 * 6.034921169281006
Epoch 900, val loss: 0.8938133716583252
Epoch 910, training loss: 12.122908592224121 = 0.05648116394877434 + 2.0 * 6.0332136154174805
Epoch 910, val loss: 0.900169849395752
Epoch 920, training loss: 12.1194486618042 = 0.054386723786592484 + 2.0 * 6.032530784606934
Epoch 920, val loss: 0.906677782535553
Epoch 930, training loss: 12.1150541305542 = 0.05238797143101692 + 2.0 * 6.031332969665527
Epoch 930, val loss: 0.913240373134613
Epoch 940, training loss: 12.119671821594238 = 0.050483983010053635 + 2.0 * 6.0345940589904785
Epoch 940, val loss: 0.9197733998298645
Epoch 950, training loss: 12.117668151855469 = 0.04866623878479004 + 2.0 * 6.034501075744629
Epoch 950, val loss: 0.9261804223060608
Epoch 960, training loss: 12.108227729797363 = 0.04695826768875122 + 2.0 * 6.030634880065918
Epoch 960, val loss: 0.9325422048568726
Epoch 970, training loss: 12.10519027709961 = 0.04533837363123894 + 2.0 * 6.02992582321167
Epoch 970, val loss: 0.9389473795890808
Epoch 980, training loss: 12.098700523376465 = 0.04378407821059227 + 2.0 * 6.027458190917969
Epoch 980, val loss: 0.9454165697097778
Epoch 990, training loss: 12.102873802185059 = 0.04229717329144478 + 2.0 * 6.030288219451904
Epoch 990, val loss: 0.951883852481842
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7926
Overall ASR: 0.9594
Flip ASR: 0.9511/225 nodes
The final ASR:0.58672, 0.32744, Accuracy:0.80494, 0.00924
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11628])
remove edge: torch.Size([2, 9536])
updated graph: torch.Size([2, 10608])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8481
Overall ASR: 0.9668
Flip ASR: 0.9600/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9889
Flip ASR: 0.9867/225 nodes
The final ASR:0.97663, 0.00920, Accuracy:0.83580, 0.01062
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 18.684179306030273 = 1.9363998174667358 + 2.0 * 8.373889923095703
Epoch 0, val loss: 1.9331674575805664
Epoch 10, training loss: 18.673084259033203 = 1.9263122081756592 + 2.0 * 8.37338638305664
Epoch 10, val loss: 1.9228805303573608
Epoch 20, training loss: 18.65257453918457 = 1.9135024547576904 + 2.0 * 8.369536399841309
Epoch 20, val loss: 1.909693717956543
Epoch 30, training loss: 18.575611114501953 = 1.8958663940429688 + 2.0 * 8.339872360229492
Epoch 30, val loss: 1.8917664289474487
Epoch 40, training loss: 18.133222579956055 = 1.8742105960845947 + 2.0 * 8.12950611114502
Epoch 40, val loss: 1.870915412902832
Epoch 50, training loss: 17.017528533935547 = 1.8507355451583862 + 2.0 * 7.5833964347839355
Epoch 50, val loss: 1.848824143409729
Epoch 60, training loss: 16.427326202392578 = 1.8321959972381592 + 2.0 * 7.29756498336792
Epoch 60, val loss: 1.8324469327926636
Epoch 70, training loss: 15.590225219726562 = 1.8205755949020386 + 2.0 * 6.884824752807617
Epoch 70, val loss: 1.8229045867919922
Epoch 80, training loss: 15.073909759521484 = 1.812533974647522 + 2.0 * 6.630687713623047
Epoch 80, val loss: 1.816334843635559
Epoch 90, training loss: 14.834774017333984 = 1.8030805587768555 + 2.0 * 6.5158467292785645
Epoch 90, val loss: 1.8075637817382812
Epoch 100, training loss: 14.657402992248535 = 1.791683316230774 + 2.0 * 6.432859897613525
Epoch 100, val loss: 1.7972540855407715
Epoch 110, training loss: 14.539576530456543 = 1.7800147533416748 + 2.0 * 6.3797807693481445
Epoch 110, val loss: 1.7872951030731201
Epoch 120, training loss: 14.452834129333496 = 1.7680226564407349 + 2.0 * 6.342405796051025
Epoch 120, val loss: 1.7771755456924438
Epoch 130, training loss: 14.375906944274902 = 1.7553431987762451 + 2.0 * 6.310281753540039
Epoch 130, val loss: 1.7665702104568481
Epoch 140, training loss: 14.304943084716797 = 1.741533637046814 + 2.0 * 6.281704902648926
Epoch 140, val loss: 1.7550209760665894
Epoch 150, training loss: 14.239950180053711 = 1.725771188735962 + 2.0 * 6.257089614868164
Epoch 150, val loss: 1.7420777082443237
Epoch 160, training loss: 14.183841705322266 = 1.7077134847640991 + 2.0 * 6.238064289093018
Epoch 160, val loss: 1.7274326086044312
Epoch 170, training loss: 14.126468658447266 = 1.6870179176330566 + 2.0 * 6.219725131988525
Epoch 170, val loss: 1.7108615636825562
Epoch 180, training loss: 14.071880340576172 = 1.6631587743759155 + 2.0 * 6.2043609619140625
Epoch 180, val loss: 1.691650390625
Epoch 190, training loss: 14.019927024841309 = 1.6357831954956055 + 2.0 * 6.192071914672852
Epoch 190, val loss: 1.6695374250411987
Epoch 200, training loss: 13.965587615966797 = 1.6050926446914673 + 2.0 * 6.1802473068237305
Epoch 200, val loss: 1.644594430923462
Epoch 210, training loss: 13.906311988830566 = 1.5702009201049805 + 2.0 * 6.168055534362793
Epoch 210, val loss: 1.616346001625061
Epoch 220, training loss: 13.84860897064209 = 1.5313259363174438 + 2.0 * 6.158641338348389
Epoch 220, val loss: 1.5849599838256836
Epoch 230, training loss: 13.794869422912598 = 1.4894287586212158 + 2.0 * 6.1527204513549805
Epoch 230, val loss: 1.5513044595718384
Epoch 240, training loss: 13.729117393493652 = 1.445267915725708 + 2.0 * 6.141924858093262
Epoch 240, val loss: 1.5159225463867188
Epoch 250, training loss: 13.668059349060059 = 1.3990575075149536 + 2.0 * 6.134500980377197
Epoch 250, val loss: 1.4790633916854858
Epoch 260, training loss: 13.610015869140625 = 1.3514268398284912 + 2.0 * 6.129294395446777
Epoch 260, val loss: 1.4414784908294678
Epoch 270, training loss: 13.549750328063965 = 1.3040770292282104 + 2.0 * 6.122836589813232
Epoch 270, val loss: 1.404768943786621
Epoch 280, training loss: 13.4920654296875 = 1.257551670074463 + 2.0 * 6.1172566413879395
Epoch 280, val loss: 1.3690693378448486
Epoch 290, training loss: 13.438557624816895 = 1.2116645574569702 + 2.0 * 6.1134467124938965
Epoch 290, val loss: 1.3344285488128662
Epoch 300, training loss: 13.390069961547852 = 1.1672427654266357 + 2.0 * 6.111413478851318
Epoch 300, val loss: 1.301147222518921
Epoch 310, training loss: 13.33427906036377 = 1.1242326498031616 + 2.0 * 6.105023384094238
Epoch 310, val loss: 1.2693945169448853
Epoch 320, training loss: 13.283991813659668 = 1.0823179483413696 + 2.0 * 6.100836753845215
Epoch 320, val loss: 1.2387962341308594
Epoch 330, training loss: 13.235001564025879 = 1.0415552854537964 + 2.0 * 6.0967230796813965
Epoch 330, val loss: 1.209333896636963
Epoch 340, training loss: 13.189257621765137 = 1.001900553703308 + 2.0 * 6.0936784744262695
Epoch 340, val loss: 1.1808075904846191
Epoch 350, training loss: 13.14822006225586 = 0.9632751941680908 + 2.0 * 6.092472553253174
Epoch 350, val loss: 1.153212070465088
Epoch 360, training loss: 13.102582931518555 = 0.9259786605834961 + 2.0 * 6.088302135467529
Epoch 360, val loss: 1.1265897750854492
Epoch 370, training loss: 13.058966636657715 = 0.8895235061645508 + 2.0 * 6.084721565246582
Epoch 370, val loss: 1.1007722616195679
Epoch 380, training loss: 13.016209602355957 = 0.8536769151687622 + 2.0 * 6.081266403198242
Epoch 380, val loss: 1.0756407976150513
Epoch 390, training loss: 12.989928245544434 = 0.8185875415802002 + 2.0 * 6.085670471191406
Epoch 390, val loss: 1.0510883331298828
Epoch 400, training loss: 12.93768310546875 = 0.7848618626594543 + 2.0 * 6.07641077041626
Epoch 400, val loss: 1.0275288820266724
Epoch 410, training loss: 12.900979042053223 = 0.7521466016769409 + 2.0 * 6.074416160583496
Epoch 410, val loss: 1.0050029754638672
Epoch 420, training loss: 12.867612838745117 = 0.7203259468078613 + 2.0 * 6.073643684387207
Epoch 420, val loss: 0.9835314750671387
Epoch 430, training loss: 12.838763236999512 = 0.6899605393409729 + 2.0 * 6.074401378631592
Epoch 430, val loss: 0.9631261825561523
Epoch 440, training loss: 12.797759056091309 = 0.6609400510787964 + 2.0 * 6.068409442901611
Epoch 440, val loss: 0.9441361427307129
Epoch 450, training loss: 12.763507843017578 = 0.633138120174408 + 2.0 * 6.065185070037842
Epoch 450, val loss: 0.926537036895752
Epoch 460, training loss: 12.744976997375488 = 0.6065810322761536 + 2.0 * 6.069198131561279
Epoch 460, val loss: 0.9102141857147217
Epoch 470, training loss: 12.710868835449219 = 0.5813183784484863 + 2.0 * 6.064775466918945
Epoch 470, val loss: 0.8952955007553101
Epoch 480, training loss: 12.678182601928711 = 0.5573815703392029 + 2.0 * 6.060400485992432
Epoch 480, val loss: 0.8817179799079895
Epoch 490, training loss: 12.66572380065918 = 0.534476101398468 + 2.0 * 6.065623760223389
Epoch 490, val loss: 0.8694255948066711
Epoch 500, training loss: 12.627131462097168 = 0.512621283531189 + 2.0 * 6.057255268096924
Epoch 500, val loss: 0.8582643866539001
Epoch 510, training loss: 12.601555824279785 = 0.491611510515213 + 2.0 * 6.054972171783447
Epoch 510, val loss: 0.8482422232627869
Epoch 520, training loss: 12.583460807800293 = 0.4712558388710022 + 2.0 * 6.056102275848389
Epoch 520, val loss: 0.8391838669776917
Epoch 530, training loss: 12.558064460754395 = 0.4514806568622589 + 2.0 * 6.0532917976379395
Epoch 530, val loss: 0.8309685587882996
Epoch 540, training loss: 12.53533935546875 = 0.4322318434715271 + 2.0 * 6.051553726196289
Epoch 540, val loss: 0.8236042261123657
Epoch 550, training loss: 12.529491424560547 = 0.4132673442363739 + 2.0 * 6.058112144470215
Epoch 550, val loss: 0.8169950246810913
Epoch 560, training loss: 12.495940208435059 = 0.3948920667171478 + 2.0 * 6.0505242347717285
Epoch 560, val loss: 0.810899019241333
Epoch 570, training loss: 12.472733497619629 = 0.37680262327194214 + 2.0 * 6.0479655265808105
Epoch 570, val loss: 0.8055932521820068
Epoch 580, training loss: 12.451567649841309 = 0.3589770495891571 + 2.0 * 6.046295166015625
Epoch 580, val loss: 0.8009148836135864
Epoch 590, training loss: 12.438085556030273 = 0.3414140045642853 + 2.0 * 6.048335552215576
Epoch 590, val loss: 0.7968572974205017
Epoch 600, training loss: 12.414755821228027 = 0.32420846819877625 + 2.0 * 6.045273780822754
Epoch 600, val loss: 0.7934362292289734
Epoch 610, training loss: 12.408675193786621 = 0.30743682384490967 + 2.0 * 6.050619125366211
Epoch 610, val loss: 0.7907316088676453
Epoch 620, training loss: 12.381635665893555 = 0.2913180887699127 + 2.0 * 6.045158863067627
Epoch 620, val loss: 0.7886725068092346
Epoch 630, training loss: 12.35671329498291 = 0.2757750153541565 + 2.0 * 6.040469169616699
Epoch 630, val loss: 0.7874178290367126
Epoch 640, training loss: 12.341263771057129 = 0.26085516810417175 + 2.0 * 6.0402045249938965
Epoch 640, val loss: 0.7869473695755005
Epoch 650, training loss: 12.331389427185059 = 0.2466328889131546 + 2.0 * 6.0423784255981445
Epoch 650, val loss: 0.7871264815330505
Epoch 660, training loss: 12.310169219970703 = 0.2332392930984497 + 2.0 * 6.0384650230407715
Epoch 660, val loss: 0.7881003618240356
Epoch 670, training loss: 12.293631553649902 = 0.22051426768302917 + 2.0 * 6.036558628082275
Epoch 670, val loss: 0.7898498773574829
Epoch 680, training loss: 12.28543758392334 = 0.2085655927658081 + 2.0 * 6.038435935974121
Epoch 680, val loss: 0.7923334240913391
Epoch 690, training loss: 12.281732559204102 = 0.19743485748767853 + 2.0 * 6.042149066925049
Epoch 690, val loss: 0.7953813076019287
Epoch 700, training loss: 12.258013725280762 = 0.18709169328212738 + 2.0 * 6.035460948944092
Epoch 700, val loss: 0.7991112470626831
Epoch 710, training loss: 12.243042945861816 = 0.177422434091568 + 2.0 * 6.032810211181641
Epoch 710, val loss: 0.8034723401069641
Epoch 720, training loss: 12.240728378295898 = 0.16837434470653534 + 2.0 * 6.036177158355713
Epoch 720, val loss: 0.8084188103675842
Epoch 730, training loss: 12.235261917114258 = 0.1599547117948532 + 2.0 * 6.03765344619751
Epoch 730, val loss: 0.8136135935783386
Epoch 740, training loss: 12.213298797607422 = 0.152167409658432 + 2.0 * 6.0305657386779785
Epoch 740, val loss: 0.8193182945251465
Epoch 750, training loss: 12.203924179077148 = 0.1448599249124527 + 2.0 * 6.029531955718994
Epoch 750, val loss: 0.8254196643829346
Epoch 760, training loss: 12.195196151733398 = 0.13801215589046478 + 2.0 * 6.028592109680176
Epoch 760, val loss: 0.8318703174591064
Epoch 770, training loss: 12.193309783935547 = 0.13158661127090454 + 2.0 * 6.0308613777160645
Epoch 770, val loss: 0.8385829925537109
Epoch 780, training loss: 12.182401657104492 = 0.12554630637168884 + 2.0 * 6.028427600860596
Epoch 780, val loss: 0.8456305265426636
Epoch 790, training loss: 12.177495002746582 = 0.11988627910614014 + 2.0 * 6.028804302215576
Epoch 790, val loss: 0.8528655767440796
Epoch 800, training loss: 12.170373916625977 = 0.11454316228628159 + 2.0 * 6.0279154777526855
Epoch 800, val loss: 0.8603473901748657
Epoch 810, training loss: 12.15864372253418 = 0.10950813442468643 + 2.0 * 6.024567604064941
Epoch 810, val loss: 0.8679570555686951
Epoch 820, training loss: 12.153077125549316 = 0.10476063936948776 + 2.0 * 6.024158477783203
Epoch 820, val loss: 0.8757413625717163
Epoch 830, training loss: 12.151771545410156 = 0.10026098042726517 + 2.0 * 6.025755405426025
Epoch 830, val loss: 0.8837532997131348
Epoch 840, training loss: 12.142692565917969 = 0.09601391106843948 + 2.0 * 6.02333927154541
Epoch 840, val loss: 0.8918331861495972
Epoch 850, training loss: 12.143847465515137 = 0.09199411422014236 + 2.0 * 6.02592658996582
Epoch 850, val loss: 0.9000117182731628
Epoch 860, training loss: 12.13159465789795 = 0.08821289241313934 + 2.0 * 6.021690845489502
Epoch 860, val loss: 0.9082086086273193
Epoch 870, training loss: 12.125326156616211 = 0.08461453765630722 + 2.0 * 6.020355701446533
Epoch 870, val loss: 0.9165979027748108
Epoch 880, training loss: 12.120856285095215 = 0.08119810372591019 + 2.0 * 6.019829273223877
Epoch 880, val loss: 0.925017774105072
Epoch 890, training loss: 12.12720012664795 = 0.07797341048717499 + 2.0 * 6.024613380432129
Epoch 890, val loss: 0.9334481954574585
Epoch 900, training loss: 12.112346649169922 = 0.07489985227584839 + 2.0 * 6.018723487854004
Epoch 900, val loss: 0.9417547583580017
Epoch 910, training loss: 12.110962867736816 = 0.07200311124324799 + 2.0 * 6.019479751586914
Epoch 910, val loss: 0.950080931186676
Epoch 920, training loss: 12.102507591247559 = 0.0692688599228859 + 2.0 * 6.0166192054748535
Epoch 920, val loss: 0.9586050510406494
Epoch 930, training loss: 12.117733001708984 = 0.06666526943445206 + 2.0 * 6.025533676147461
Epoch 930, val loss: 0.9671182036399841
Epoch 940, training loss: 12.107815742492676 = 0.06420405954122543 + 2.0 * 6.021805763244629
Epoch 940, val loss: 0.9752948880195618
Epoch 950, training loss: 12.092134475708008 = 0.061879727989435196 + 2.0 * 6.015127182006836
Epoch 950, val loss: 0.9836820363998413
Epoch 960, training loss: 12.089802742004395 = 0.05966821685433388 + 2.0 * 6.015067100524902
Epoch 960, val loss: 0.9920644164085388
Epoch 970, training loss: 12.09843635559082 = 0.05756545066833496 + 2.0 * 6.020435333251953
Epoch 970, val loss: 1.0003926753997803
Epoch 980, training loss: 12.087173461914062 = 0.05557670444250107 + 2.0 * 6.015798568725586
Epoch 980, val loss: 1.00855553150177
Epoch 990, training loss: 12.083739280700684 = 0.053677476942539215 + 2.0 * 6.015030860900879
Epoch 990, val loss: 1.016860842704773
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7815
Overall ASR: 0.7565
Flip ASR: 0.7067/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.686046600341797 = 1.9381682872772217 + 2.0 * 8.373939514160156
Epoch 0, val loss: 1.9395338296890259
Epoch 10, training loss: 18.675434112548828 = 1.9280858039855957 + 2.0 * 8.373674392700195
Epoch 10, val loss: 1.928114414215088
Epoch 20, training loss: 18.65921401977539 = 1.915493130683899 + 2.0 * 8.37186050415039
Epoch 20, val loss: 1.9132710695266724
Epoch 30, training loss: 18.614601135253906 = 1.8980330228805542 + 2.0 * 8.358283996582031
Epoch 30, val loss: 1.8923730850219727
Epoch 40, training loss: 18.438270568847656 = 1.8751744031906128 + 2.0 * 8.281548500061035
Epoch 40, val loss: 1.865808129310608
Epoch 50, training loss: 17.652313232421875 = 1.851152777671814 + 2.0 * 7.900580406188965
Epoch 50, val loss: 1.838942050933838
Epoch 60, training loss: 16.633737564086914 = 1.826717734336853 + 2.0 * 7.403509616851807
Epoch 60, val loss: 1.8128643035888672
Epoch 70, training loss: 15.843488693237305 = 1.808700442314148 + 2.0 * 7.017394065856934
Epoch 70, val loss: 1.7940287590026855
Epoch 80, training loss: 15.385056495666504 = 1.7912445068359375 + 2.0 * 6.796905994415283
Epoch 80, val loss: 1.7758394479751587
Epoch 90, training loss: 15.1056547164917 = 1.7760013341903687 + 2.0 * 6.6648268699646
Epoch 90, val loss: 1.7607173919677734
Epoch 100, training loss: 14.91590404510498 = 1.7612475156784058 + 2.0 * 6.577328205108643
Epoch 100, val loss: 1.7462321519851685
Epoch 110, training loss: 14.756743431091309 = 1.7472025156021118 + 2.0 * 6.504770278930664
Epoch 110, val loss: 1.7329720258712769
Epoch 120, training loss: 14.632667541503906 = 1.7328927516937256 + 2.0 * 6.449887275695801
Epoch 120, val loss: 1.7200546264648438
Epoch 130, training loss: 14.53642749786377 = 1.7173326015472412 + 2.0 * 6.409547328948975
Epoch 130, val loss: 1.706405758857727
Epoch 140, training loss: 14.447060585021973 = 1.6998956203460693 + 2.0 * 6.373582363128662
Epoch 140, val loss: 1.6914145946502686
Epoch 150, training loss: 14.370248794555664 = 1.6804770231246948 + 2.0 * 6.34488582611084
Epoch 150, val loss: 1.6749839782714844
Epoch 160, training loss: 14.296602249145508 = 1.658762812614441 + 2.0 * 6.318919658660889
Epoch 160, val loss: 1.6572489738464355
Epoch 170, training loss: 14.229194641113281 = 1.6341183185577393 + 2.0 * 6.2975382804870605
Epoch 170, val loss: 1.637494444847107
Epoch 180, training loss: 14.16080093383789 = 1.6057511568069458 + 2.0 * 6.277524948120117
Epoch 180, val loss: 1.6150240898132324
Epoch 190, training loss: 14.092758178710938 = 1.5733704566955566 + 2.0 * 6.2596940994262695
Epoch 190, val loss: 1.589455246925354
Epoch 200, training loss: 14.02285099029541 = 1.5371085405349731 + 2.0 * 6.242871284484863
Epoch 200, val loss: 1.561123251914978
Epoch 210, training loss: 13.951780319213867 = 1.496376872062683 + 2.0 * 6.227701663970947
Epoch 210, val loss: 1.5293349027633667
Epoch 220, training loss: 13.8895902633667 = 1.450889229774475 + 2.0 * 6.219350337982178
Epoch 220, val loss: 1.4938230514526367
Epoch 230, training loss: 13.813284873962402 = 1.4016095399856567 + 2.0 * 6.205837726593018
Epoch 230, val loss: 1.4556373357772827
Epoch 240, training loss: 13.740323066711426 = 1.3492034673690796 + 2.0 * 6.195559978485107
Epoch 240, val loss: 1.415278434753418
Epoch 250, training loss: 13.668335914611816 = 1.2945510149002075 + 2.0 * 6.186892509460449
Epoch 250, val loss: 1.3733631372451782
Epoch 260, training loss: 13.6062593460083 = 1.2389498949050903 + 2.0 * 6.18365478515625
Epoch 260, val loss: 1.3307965993881226
Epoch 270, training loss: 13.532114028930664 = 1.1845604181289673 + 2.0 * 6.173776626586914
Epoch 270, val loss: 1.2894281148910522
Epoch 280, training loss: 13.462961196899414 = 1.1323953866958618 + 2.0 * 6.165282726287842
Epoch 280, val loss: 1.2496784925460815
Epoch 290, training loss: 13.410419464111328 = 1.0826009511947632 + 2.0 * 6.163909435272217
Epoch 290, val loss: 1.2118520736694336
Epoch 300, training loss: 13.3397798538208 = 1.035701036453247 + 2.0 * 6.152039527893066
Epoch 300, val loss: 1.1763883829116821
Epoch 310, training loss: 13.284795761108398 = 0.9912893176078796 + 2.0 * 6.146753311157227
Epoch 310, val loss: 1.1428147554397583
Epoch 320, training loss: 13.239777565002441 = 0.9489566087722778 + 2.0 * 6.145410537719727
Epoch 320, val loss: 1.1107655763626099
Epoch 330, training loss: 13.182720184326172 = 0.9087967872619629 + 2.0 * 6.136961936950684
Epoch 330, val loss: 1.0806347131729126
Epoch 340, training loss: 13.129817008972168 = 0.870239794254303 + 2.0 * 6.129788398742676
Epoch 340, val loss: 1.0519111156463623
Epoch 350, training loss: 13.090218544006348 = 0.8327146172523499 + 2.0 * 6.128751754760742
Epoch 350, val loss: 1.024208903312683
Epoch 360, training loss: 13.040141105651855 = 0.7966349720954895 + 2.0 * 6.121753215789795
Epoch 360, val loss: 0.9976513981819153
Epoch 370, training loss: 12.99569320678711 = 0.7617712616920471 + 2.0 * 6.1169610023498535
Epoch 370, val loss: 0.9724224209785461
Epoch 380, training loss: 12.95337200164795 = 0.7278023362159729 + 2.0 * 6.1127848625183105
Epoch 380, val loss: 0.9482327103614807
Epoch 390, training loss: 12.92385196685791 = 0.6946542859077454 + 2.0 * 6.114598751068115
Epoch 390, val loss: 0.9250551462173462
Epoch 400, training loss: 12.874442100524902 = 0.6628112196922302 + 2.0 * 6.105815410614014
Epoch 400, val loss: 0.9031052589416504
Epoch 410, training loss: 12.837728500366211 = 0.6321329474449158 + 2.0 * 6.102797985076904
Epoch 410, val loss: 0.8825058341026306
Epoch 420, training loss: 12.81053638458252 = 0.6026224493980408 + 2.0 * 6.103957176208496
Epoch 420, val loss: 0.8632670044898987
Epoch 430, training loss: 12.767888069152832 = 0.5746363997459412 + 2.0 * 6.096625804901123
Epoch 430, val loss: 0.845492422580719
Epoch 440, training loss: 12.73723030090332 = 0.5480701327323914 + 2.0 * 6.094580173492432
Epoch 440, val loss: 0.829264760017395
Epoch 450, training loss: 12.708606719970703 = 0.5228719711303711 + 2.0 * 6.092867374420166
Epoch 450, val loss: 0.8144699931144714
Epoch 460, training loss: 12.683411598205566 = 0.4989846348762512 + 2.0 * 6.0922136306762695
Epoch 460, val loss: 0.8011521100997925
Epoch 470, training loss: 12.653399467468262 = 0.476460725069046 + 2.0 * 6.088469505310059
Epoch 470, val loss: 0.7891340851783752
Epoch 480, training loss: 12.632414817810059 = 0.45516544580459595 + 2.0 * 6.088624477386475
Epoch 480, val loss: 0.7783938646316528
Epoch 490, training loss: 12.602706909179688 = 0.4352029860019684 + 2.0 * 6.083752155303955
Epoch 490, val loss: 0.7688398361206055
Epoch 500, training loss: 12.578625679016113 = 0.4163173735141754 + 2.0 * 6.0811543464660645
Epoch 500, val loss: 0.7605147361755371
Epoch 510, training loss: 12.557851791381836 = 0.3983733355998993 + 2.0 * 6.079739093780518
Epoch 510, val loss: 0.7530834674835205
Epoch 520, training loss: 12.540225982666016 = 0.3813478946685791 + 2.0 * 6.079439163208008
Epoch 520, val loss: 0.746491014957428
Epoch 530, training loss: 12.518895149230957 = 0.3652697205543518 + 2.0 * 6.076812744140625
Epoch 530, val loss: 0.7408458590507507
Epoch 540, training loss: 12.499058723449707 = 0.35010603070259094 + 2.0 * 6.07447624206543
Epoch 540, val loss: 0.7359429597854614
Epoch 550, training loss: 12.478986740112305 = 0.33560359477996826 + 2.0 * 6.071691513061523
Epoch 550, val loss: 0.7318173050880432
Epoch 560, training loss: 12.461443901062012 = 0.3217293322086334 + 2.0 * 6.069857120513916
Epoch 560, val loss: 0.7282918095588684
Epoch 570, training loss: 12.457972526550293 = 0.3084501028060913 + 2.0 * 6.074761390686035
Epoch 570, val loss: 0.7253855466842651
Epoch 580, training loss: 12.437320709228516 = 0.2958245575428009 + 2.0 * 6.0707478523254395
Epoch 580, val loss: 0.723028838634491
Epoch 590, training loss: 12.417978286743164 = 0.28381282091140747 + 2.0 * 6.06708288192749
Epoch 590, val loss: 0.72126305103302
Epoch 600, training loss: 12.40040111541748 = 0.27233514189720154 + 2.0 * 6.064033031463623
Epoch 600, val loss: 0.7200174331665039
Epoch 610, training loss: 12.391265869140625 = 0.26130035519599915 + 2.0 * 6.064982891082764
Epoch 610, val loss: 0.719268262386322
Epoch 620, training loss: 12.372903823852539 = 0.25076520442962646 + 2.0 * 6.061069488525391
Epoch 620, val loss: 0.7188650369644165
Epoch 630, training loss: 12.361481666564941 = 0.24065159261226654 + 2.0 * 6.060415267944336
Epoch 630, val loss: 0.7189869284629822
Epoch 640, training loss: 12.349692344665527 = 0.23095861077308655 + 2.0 * 6.059366703033447
Epoch 640, val loss: 0.7195303440093994
Epoch 650, training loss: 12.337143898010254 = 0.2216445505619049 + 2.0 * 6.0577497482299805
Epoch 650, val loss: 0.7204377055168152
Epoch 660, training loss: 12.327494621276855 = 0.21277181804180145 + 2.0 * 6.057361602783203
Epoch 660, val loss: 0.7217391133308411
Epoch 670, training loss: 12.312838554382324 = 0.2042446881532669 + 2.0 * 6.054296970367432
Epoch 670, val loss: 0.7234945893287659
Epoch 680, training loss: 12.317815780639648 = 0.19604849815368652 + 2.0 * 6.060883522033691
Epoch 680, val loss: 0.7254883646965027
Epoch 690, training loss: 12.29697036743164 = 0.1882103681564331 + 2.0 * 6.054379940032959
Epoch 690, val loss: 0.7277846336364746
Epoch 700, training loss: 12.2847318649292 = 0.1807323843240738 + 2.0 * 6.051999568939209
Epoch 700, val loss: 0.7304158210754395
Epoch 710, training loss: 12.272774696350098 = 0.17357569932937622 + 2.0 * 6.049599647521973
Epoch 710, val loss: 0.7334286570549011
Epoch 720, training loss: 12.269293785095215 = 0.16670766472816467 + 2.0 * 6.051292896270752
Epoch 720, val loss: 0.7367146015167236
Epoch 730, training loss: 12.255953788757324 = 0.16016307473182678 + 2.0 * 6.047895431518555
Epoch 730, val loss: 0.7401784062385559
Epoch 740, training loss: 12.250609397888184 = 0.1539091020822525 + 2.0 * 6.0483503341674805
Epoch 740, val loss: 0.7439400553703308
Epoch 750, training loss: 12.246000289916992 = 0.1479320526123047 + 2.0 * 6.049034118652344
Epoch 750, val loss: 0.7479261755943298
Epoch 760, training loss: 12.23340129852295 = 0.14221079647541046 + 2.0 * 6.045595169067383
Epoch 760, val loss: 0.7520884275436401
Epoch 770, training loss: 12.225248336791992 = 0.13675181567668915 + 2.0 * 6.044248104095459
Epoch 770, val loss: 0.7564801573753357
Epoch 780, training loss: 12.220196723937988 = 0.1315326988697052 + 2.0 * 6.044332027435303
Epoch 780, val loss: 0.7611154317855835
Epoch 790, training loss: 12.213438034057617 = 0.12654681503772736 + 2.0 * 6.043445587158203
Epoch 790, val loss: 0.7659438252449036
Epoch 800, training loss: 12.208321571350098 = 0.12178922444581985 + 2.0 * 6.043266296386719
Epoch 800, val loss: 0.7708868384361267
Epoch 810, training loss: 12.202999114990234 = 0.11723984032869339 + 2.0 * 6.042879581451416
Epoch 810, val loss: 0.7760069966316223
Epoch 820, training loss: 12.194462776184082 = 0.11287665367126465 + 2.0 * 6.040792942047119
Epoch 820, val loss: 0.7812741994857788
Epoch 830, training loss: 12.187005996704102 = 0.10867878049612045 + 2.0 * 6.039163589477539
Epoch 830, val loss: 0.7867125272750854
Epoch 840, training loss: 12.185271263122559 = 0.10465101152658463 + 2.0 * 6.040309906005859
Epoch 840, val loss: 0.792292594909668
Epoch 850, training loss: 12.180727005004883 = 0.10077599436044693 + 2.0 * 6.039975643157959
Epoch 850, val loss: 0.7980043888092041
Epoch 860, training loss: 12.186631202697754 = 0.0970504954457283 + 2.0 * 6.044790267944336
Epoch 860, val loss: 0.8036885857582092
Epoch 870, training loss: 12.168693542480469 = 0.09346102923154831 + 2.0 * 6.03761625289917
Epoch 870, val loss: 0.8094709515571594
Epoch 880, training loss: 12.161903381347656 = 0.09001775830984116 + 2.0 * 6.035943031311035
Epoch 880, val loss: 0.8152480721473694
Epoch 890, training loss: 12.155366897583008 = 0.086654894053936 + 2.0 * 6.034356117248535
Epoch 890, val loss: 0.8211833238601685
Epoch 900, training loss: 12.153916358947754 = 0.08335022628307343 + 2.0 * 6.035283088684082
Epoch 900, val loss: 0.8272751569747925
Epoch 910, training loss: 12.153594017028809 = 0.08012016862630844 + 2.0 * 6.036736965179443
Epoch 910, val loss: 0.8332778811454773
Epoch 920, training loss: 12.141399383544922 = 0.07698937505483627 + 2.0 * 6.032205104827881
Epoch 920, val loss: 0.8393712639808655
Epoch 930, training loss: 12.137552261352539 = 0.07391367107629776 + 2.0 * 6.0318193435668945
Epoch 930, val loss: 0.845456600189209
Epoch 940, training loss: 12.133049011230469 = 0.070920929312706 + 2.0 * 6.031064033508301
Epoch 940, val loss: 0.8516249656677246
Epoch 950, training loss: 12.141236305236816 = 0.06803210824728012 + 2.0 * 6.036602020263672
Epoch 950, val loss: 0.8578741550445557
Epoch 960, training loss: 12.134777069091797 = 0.0652402862906456 + 2.0 * 6.034768581390381
Epoch 960, val loss: 0.8641172051429749
Epoch 970, training loss: 12.120956420898438 = 0.0625859871506691 + 2.0 * 6.0291852951049805
Epoch 970, val loss: 0.8703256845474243
Epoch 980, training loss: 12.119104385375977 = 0.06006556749343872 + 2.0 * 6.029519557952881
Epoch 980, val loss: 0.8766134977340698
Epoch 990, training loss: 12.117609024047852 = 0.05766171216964722 + 2.0 * 6.02997350692749
Epoch 990, val loss: 0.883047878742218
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8037
Overall ASR: 0.8339
Flip ASR: 0.8222/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.689334869384766 = 1.9415993690490723 + 2.0 * 8.373867988586426
Epoch 0, val loss: 1.936370849609375
Epoch 10, training loss: 18.677574157714844 = 1.9309605360031128 + 2.0 * 8.373307228088379
Epoch 10, val loss: 1.925729751586914
Epoch 20, training loss: 18.656272888183594 = 1.9175180196762085 + 2.0 * 8.369377136230469
Epoch 20, val loss: 1.9117703437805176
Epoch 30, training loss: 18.582677841186523 = 1.899306058883667 + 2.0 * 8.341686248779297
Epoch 30, val loss: 1.8926459550857544
Epoch 40, training loss: 18.1868839263916 = 1.8776390552520752 + 2.0 * 8.154622077941895
Epoch 40, val loss: 1.8704928159713745
Epoch 50, training loss: 16.53457260131836 = 1.854887843132019 + 2.0 * 7.339842796325684
Epoch 50, val loss: 1.8474093675613403
Epoch 60, training loss: 15.916997909545898 = 1.8360211849212646 + 2.0 * 7.040488243103027
Epoch 60, val loss: 1.830311894416809
Epoch 70, training loss: 15.403732299804688 = 1.818689227104187 + 2.0 * 6.7925214767456055
Epoch 70, val loss: 1.8143800497055054
Epoch 80, training loss: 15.086898803710938 = 1.8018555641174316 + 2.0 * 6.642521381378174
Epoch 80, val loss: 1.7982604503631592
Epoch 90, training loss: 14.900920867919922 = 1.78659188747406 + 2.0 * 6.557164669036865
Epoch 90, val loss: 1.78409743309021
Epoch 100, training loss: 14.732189178466797 = 1.772286057472229 + 2.0 * 6.47995138168335
Epoch 100, val loss: 1.7705812454223633
Epoch 110, training loss: 14.614789009094238 = 1.7580373287200928 + 2.0 * 6.428375720977783
Epoch 110, val loss: 1.7571051120758057
Epoch 120, training loss: 14.524725914001465 = 1.742323637008667 + 2.0 * 6.391201019287109
Epoch 120, val loss: 1.7423806190490723
Epoch 130, training loss: 14.43955135345459 = 1.7252017259597778 + 2.0 * 6.357174873352051
Epoch 130, val loss: 1.726860761642456
Epoch 140, training loss: 14.364507675170898 = 1.7070974111557007 + 2.0 * 6.328705310821533
Epoch 140, val loss: 1.7109867334365845
Epoch 150, training loss: 14.298527717590332 = 1.6870509386062622 + 2.0 * 6.30573844909668
Epoch 150, val loss: 1.6936842203140259
Epoch 160, training loss: 14.23458194732666 = 1.664185643196106 + 2.0 * 6.285198211669922
Epoch 160, val loss: 1.6745251417160034
Epoch 170, training loss: 14.17043685913086 = 1.6380822658538818 + 2.0 * 6.266177177429199
Epoch 170, val loss: 1.6529628038406372
Epoch 180, training loss: 14.104073524475098 = 1.6085383892059326 + 2.0 * 6.247767448425293
Epoch 180, val loss: 1.62905752658844
Epoch 190, training loss: 14.036876678466797 = 1.57546865940094 + 2.0 * 6.230703830718994
Epoch 190, val loss: 1.6030007600784302
Epoch 200, training loss: 13.970537185668945 = 1.538832426071167 + 2.0 * 6.2158522605896
Epoch 200, val loss: 1.5744757652282715
Epoch 210, training loss: 13.901935577392578 = 1.4981807470321655 + 2.0 * 6.201877593994141
Epoch 210, val loss: 1.5431082248687744
Epoch 220, training loss: 13.837773323059082 = 1.4534437656402588 + 2.0 * 6.192164897918701
Epoch 220, val loss: 1.509480595588684
Epoch 230, training loss: 13.766697883605957 = 1.4058281183242798 + 2.0 * 6.180434703826904
Epoch 230, val loss: 1.473687767982483
Epoch 240, training loss: 13.696391105651855 = 1.3555524349212646 + 2.0 * 6.170419216156006
Epoch 240, val loss: 1.4363378286361694
Epoch 250, training loss: 13.627442359924316 = 1.3031654357910156 + 2.0 * 6.16213846206665
Epoch 250, val loss: 1.3979616165161133
Epoch 260, training loss: 13.558650970458984 = 1.2501170635223389 + 2.0 * 6.154266834259033
Epoch 260, val loss: 1.3595912456512451
Epoch 270, training loss: 13.490194320678711 = 1.1974406242370605 + 2.0 * 6.146376609802246
Epoch 270, val loss: 1.3218008279800415
Epoch 280, training loss: 13.429903030395508 = 1.1454790830612183 + 2.0 * 6.1422119140625
Epoch 280, val loss: 1.2849873304367065
Epoch 290, training loss: 13.366652488708496 = 1.0961874723434448 + 2.0 * 6.135232448577881
Epoch 290, val loss: 1.2500239610671997
Epoch 300, training loss: 13.308762550354004 = 1.0496009588241577 + 2.0 * 6.129580974578857
Epoch 300, val loss: 1.2168911695480347
Epoch 310, training loss: 13.252822875976562 = 1.0054165124893188 + 2.0 * 6.1237030029296875
Epoch 310, val loss: 1.1854740381240845
Epoch 320, training loss: 13.20083236694336 = 0.9630406498908997 + 2.0 * 6.118896007537842
Epoch 320, val loss: 1.155539631843567
Epoch 330, training loss: 13.150846481323242 = 0.9224283695220947 + 2.0 * 6.114209175109863
Epoch 330, val loss: 1.1267850399017334
Epoch 340, training loss: 13.104540824890137 = 0.8834089636802673 + 2.0 * 6.110566139221191
Epoch 340, val loss: 1.0992791652679443
Epoch 350, training loss: 13.066168785095215 = 0.8463122248649597 + 2.0 * 6.109928131103516
Epoch 350, val loss: 1.0731728076934814
Epoch 360, training loss: 13.019243240356445 = 0.8111729621887207 + 2.0 * 6.104035377502441
Epoch 360, val loss: 1.0484371185302734
Epoch 370, training loss: 12.977442741394043 = 0.7775974273681641 + 2.0 * 6.0999226570129395
Epoch 370, val loss: 1.024906039237976
Epoch 380, training loss: 12.94509220123291 = 0.7456268668174744 + 2.0 * 6.099732875823975
Epoch 380, val loss: 1.0025551319122314
Epoch 390, training loss: 12.904130935668945 = 0.7153245806694031 + 2.0 * 6.094403266906738
Epoch 390, val loss: 0.9816496968269348
Epoch 400, training loss: 12.869375228881836 = 0.6864753365516663 + 2.0 * 6.091449737548828
Epoch 400, val loss: 0.9620100259780884
Epoch 410, training loss: 12.835678100585938 = 0.6587539911270142 + 2.0 * 6.088461875915527
Epoch 410, val loss: 0.9433894157409668
Epoch 420, training loss: 12.82130241394043 = 0.6320302486419678 + 2.0 * 6.094635963439941
Epoch 420, val loss: 0.9258245825767517
Epoch 430, training loss: 12.777809143066406 = 0.606810986995697 + 2.0 * 6.085499286651611
Epoch 430, val loss: 0.9092788100242615
Epoch 440, training loss: 12.746699333190918 = 0.5825561881065369 + 2.0 * 6.082071781158447
Epoch 440, val loss: 0.894017219543457
Epoch 450, training loss: 12.717364311218262 = 0.5590376257896423 + 2.0 * 6.079163551330566
Epoch 450, val loss: 0.8797183632850647
Epoch 460, training loss: 12.689067840576172 = 0.5359783172607422 + 2.0 * 6.076544761657715
Epoch 460, val loss: 0.8661258220672607
Epoch 470, training loss: 12.675981521606445 = 0.5135328769683838 + 2.0 * 6.08122444152832
Epoch 470, val loss: 0.8534980416297913
Epoch 480, training loss: 12.643247604370117 = 0.4918346405029297 + 2.0 * 6.075706481933594
Epoch 480, val loss: 0.8418831825256348
Epoch 490, training loss: 12.612581253051758 = 0.47088074684143066 + 2.0 * 6.070850372314453
Epoch 490, val loss: 0.8313813805580139
Epoch 500, training loss: 12.595097541809082 = 0.4504539668560028 + 2.0 * 6.072321891784668
Epoch 500, val loss: 0.8217579126358032
Epoch 510, training loss: 12.566039085388184 = 0.43057936429977417 + 2.0 * 6.067729949951172
Epoch 510, val loss: 0.8132153153419495
Epoch 520, training loss: 12.542346954345703 = 0.4113387167453766 + 2.0 * 6.06550407409668
Epoch 520, val loss: 0.8055886030197144
Epoch 530, training loss: 12.524250984191895 = 0.39268290996551514 + 2.0 * 6.065783977508545
Epoch 530, val loss: 0.7988948822021484
Epoch 540, training loss: 12.4984769821167 = 0.374668687582016 + 2.0 * 6.061903953552246
Epoch 540, val loss: 0.7931159138679504
Epoch 550, training loss: 12.506621360778809 = 0.35716843605041504 + 2.0 * 6.074726581573486
Epoch 550, val loss: 0.7881245017051697
Epoch 560, training loss: 12.467013359069824 = 0.3405575454235077 + 2.0 * 6.063228130340576
Epoch 560, val loss: 0.7837815284729004
Epoch 570, training loss: 12.44034481048584 = 0.32449573278427124 + 2.0 * 6.057924747467041
Epoch 570, val loss: 0.7801790237426758
Epoch 580, training loss: 12.422139167785645 = 0.30889689922332764 + 2.0 * 6.056621074676514
Epoch 580, val loss: 0.7770600914955139
Epoch 590, training loss: 12.40345573425293 = 0.2937379479408264 + 2.0 * 6.054858684539795
Epoch 590, val loss: 0.7744036912918091
Epoch 600, training loss: 12.387434959411621 = 0.27899590134620667 + 2.0 * 6.054219722747803
Epoch 600, val loss: 0.7722166180610657
Epoch 610, training loss: 12.37393569946289 = 0.2647567391395569 + 2.0 * 6.05458927154541
Epoch 610, val loss: 0.770413875579834
Epoch 620, training loss: 12.362854957580566 = 0.2510991394519806 + 2.0 * 6.055877685546875
Epoch 620, val loss: 0.7691057920455933
Epoch 630, training loss: 12.338549613952637 = 0.237997367978096 + 2.0 * 6.050276279449463
Epoch 630, val loss: 0.7682183980941772
Epoch 640, training loss: 12.323761940002441 = 0.22541272640228271 + 2.0 * 6.049174785614014
Epoch 640, val loss: 0.7677162289619446
Epoch 650, training loss: 12.315467834472656 = 0.21336442232131958 + 2.0 * 6.051051616668701
Epoch 650, val loss: 0.7676023840904236
Epoch 660, training loss: 12.300150871276855 = 0.2019175887107849 + 2.0 * 6.049116611480713
Epoch 660, val loss: 0.7678573131561279
Epoch 670, training loss: 12.291421890258789 = 0.1910959929227829 + 2.0 * 6.0501627922058105
Epoch 670, val loss: 0.7685716152191162
Epoch 680, training loss: 12.275747299194336 = 0.18086202442646027 + 2.0 * 6.047442436218262
Epoch 680, val loss: 0.7697073221206665
Epoch 690, training loss: 12.259940147399902 = 0.17129595577716827 + 2.0 * 6.0443220138549805
Epoch 690, val loss: 0.7713056206703186
Epoch 700, training loss: 12.247593879699707 = 0.16221970319747925 + 2.0 * 6.042686939239502
Epoch 700, val loss: 0.7733398675918579
Epoch 710, training loss: 12.23644733428955 = 0.15367569029331207 + 2.0 * 6.041385650634766
Epoch 710, val loss: 0.7757657766342163
Epoch 720, training loss: 12.23580265045166 = 0.14562413096427917 + 2.0 * 6.045089244842529
Epoch 720, val loss: 0.7785504460334778
Epoch 730, training loss: 12.229635238647461 = 0.13811051845550537 + 2.0 * 6.045762538909912
Epoch 730, val loss: 0.7816293239593506
Epoch 740, training loss: 12.212100982666016 = 0.13113833963871002 + 2.0 * 6.040481090545654
Epoch 740, val loss: 0.7850098609924316
Epoch 750, training loss: 12.20072078704834 = 0.12458288669586182 + 2.0 * 6.038068771362305
Epoch 750, val loss: 0.7886773943901062
Epoch 760, training loss: 12.197556495666504 = 0.11840634793043137 + 2.0 * 6.039575099945068
Epoch 760, val loss: 0.7925620079040527
Epoch 770, training loss: 12.196165084838867 = 0.11267083138227463 + 2.0 * 6.041747093200684
Epoch 770, val loss: 0.7966092824935913
Epoch 780, training loss: 12.17864990234375 = 0.10730011761188507 + 2.0 * 6.035675048828125
Epoch 780, val loss: 0.8008028864860535
Epoch 790, training loss: 12.170841217041016 = 0.1022559329867363 + 2.0 * 6.034292697906494
Epoch 790, val loss: 0.8051992654800415
Epoch 800, training loss: 12.164634704589844 = 0.09750252217054367 + 2.0 * 6.033565998077393
Epoch 800, val loss: 0.8097562193870544
Epoch 810, training loss: 12.165966033935547 = 0.09301163256168365 + 2.0 * 6.036477088928223
Epoch 810, val loss: 0.8144649267196655
Epoch 820, training loss: 12.162668228149414 = 0.08884017914533615 + 2.0 * 6.036913871765137
Epoch 820, val loss: 0.8191991448402405
Epoch 830, training loss: 12.148428916931152 = 0.08490997552871704 + 2.0 * 6.031759262084961
Epoch 830, val loss: 0.8240587711334229
Epoch 840, training loss: 12.14299201965332 = 0.08122521638870239 + 2.0 * 6.030883312225342
Epoch 840, val loss: 0.8290401101112366
Epoch 850, training loss: 12.14413070678711 = 0.07774287462234497 + 2.0 * 6.033194065093994
Epoch 850, val loss: 0.8340820670127869
Epoch 860, training loss: 12.13413143157959 = 0.0744590163230896 + 2.0 * 6.029836177825928
Epoch 860, val loss: 0.839142382144928
Epoch 870, training loss: 12.130998611450195 = 0.07136322557926178 + 2.0 * 6.029817581176758
Epoch 870, val loss: 0.8442866802215576
Epoch 880, training loss: 12.125173568725586 = 0.06844907999038696 + 2.0 * 6.028362274169922
Epoch 880, val loss: 0.8494059443473816
Epoch 890, training loss: 12.121397972106934 = 0.06570569425821304 + 2.0 * 6.027846336364746
Epoch 890, val loss: 0.8545591831207275
Epoch 900, training loss: 12.115880012512207 = 0.06310532242059708 + 2.0 * 6.0263872146606445
Epoch 900, val loss: 0.8597592115402222
Epoch 910, training loss: 12.111475944519043 = 0.060634978115558624 + 2.0 * 6.025420665740967
Epoch 910, val loss: 0.8650094270706177
Epoch 920, training loss: 12.12017822265625 = 0.05829654633998871 + 2.0 * 6.030941009521484
Epoch 920, val loss: 0.8703048229217529
Epoch 930, training loss: 12.119915962219238 = 0.05608323588967323 + 2.0 * 6.03191614151001
Epoch 930, val loss: 0.8754627704620361
Epoch 940, training loss: 12.10198974609375 = 0.05401454120874405 + 2.0 * 6.023987770080566
Epoch 940, val loss: 0.88069087266922
Epoch 950, training loss: 12.09807014465332 = 0.05204774811863899 + 2.0 * 6.023011207580566
Epoch 950, val loss: 0.8859186172485352
Epoch 960, training loss: 12.094612121582031 = 0.0501706600189209 + 2.0 * 6.022220611572266
Epoch 960, val loss: 0.8911508917808533
Epoch 970, training loss: 12.091135025024414 = 0.04837704077363014 + 2.0 * 6.021378993988037
Epoch 970, val loss: 0.8963992595672607
Epoch 980, training loss: 12.092340469360352 = 0.04666716605424881 + 2.0 * 6.022836685180664
Epoch 980, val loss: 0.9016644954681396
Epoch 990, training loss: 12.093053817749023 = 0.04506061598658562 + 2.0 * 6.023996829986572
Epoch 990, val loss: 0.9068305492401123
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7963
Overall ASR: 0.8376
Flip ASR: 0.8044/225 nodes
The final ASR:0.80935, 0.03743, Accuracy:0.79383, 0.00924
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11560])
remove edge: torch.Size([2, 9472])
updated graph: torch.Size([2, 10476])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9963
Flip ASR: 0.9956/225 nodes
The final ASR:0.98155, 0.01044, Accuracy:0.83086, 0.00761
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 18.70425033569336 = 1.9563987255096436 + 2.0 * 8.373926162719727
Epoch 0, val loss: 1.9603296518325806
Epoch 10, training loss: 18.693361282348633 = 1.9463728666305542 + 2.0 * 8.373494148254395
Epoch 10, val loss: 1.9505811929702759
Epoch 20, training loss: 18.673357009887695 = 1.9339680671691895 + 2.0 * 8.369694709777832
Epoch 20, val loss: 1.9379078149795532
Epoch 30, training loss: 18.5991153717041 = 1.9170887470245361 + 2.0 * 8.341012954711914
Epoch 30, val loss: 1.9202094078063965
Epoch 40, training loss: 18.225059509277344 = 1.8963220119476318 + 2.0 * 8.164368629455566
Epoch 40, val loss: 1.8989583253860474
Epoch 50, training loss: 17.332080841064453 = 1.874563455581665 + 2.0 * 7.728758811950684
Epoch 50, val loss: 1.8773938417434692
Epoch 60, training loss: 16.798582077026367 = 1.8538538217544556 + 2.0 * 7.4723639488220215
Epoch 60, val loss: 1.8579665422439575
Epoch 70, training loss: 16.002925872802734 = 1.8375991582870483 + 2.0 * 7.082663059234619
Epoch 70, val loss: 1.8427625894546509
Epoch 80, training loss: 15.444681167602539 = 1.8247535228729248 + 2.0 * 6.809963703155518
Epoch 80, val loss: 1.8308742046356201
Epoch 90, training loss: 15.113999366760254 = 1.8123284578323364 + 2.0 * 6.6508355140686035
Epoch 90, val loss: 1.8187408447265625
Epoch 100, training loss: 14.839165687561035 = 1.7981761693954468 + 2.0 * 6.5204949378967285
Epoch 100, val loss: 1.8053598403930664
Epoch 110, training loss: 14.671836853027344 = 1.7833915948867798 + 2.0 * 6.444222450256348
Epoch 110, val loss: 1.7918118238449097
Epoch 120, training loss: 14.562751770019531 = 1.7680643796920776 + 2.0 * 6.397343635559082
Epoch 120, val loss: 1.7779213190078735
Epoch 130, training loss: 14.478241920471191 = 1.7517434358596802 + 2.0 * 6.3632493019104
Epoch 130, val loss: 1.7632429599761963
Epoch 140, training loss: 14.402562141418457 = 1.7343645095825195 + 2.0 * 6.334098815917969
Epoch 140, val loss: 1.7481513023376465
Epoch 150, training loss: 14.325475692749023 = 1.7158896923065186 + 2.0 * 6.304792881011963
Epoch 150, val loss: 1.732391119003296
Epoch 160, training loss: 14.258589744567871 = 1.6954597234725952 + 2.0 * 6.281565189361572
Epoch 160, val loss: 1.7152761220932007
Epoch 170, training loss: 14.198925018310547 = 1.6726239919662476 + 2.0 * 6.263150691986084
Epoch 170, val loss: 1.6964101791381836
Epoch 180, training loss: 14.141899108886719 = 1.6472482681274414 + 2.0 * 6.247325420379639
Epoch 180, val loss: 1.6754943132400513
Epoch 190, training loss: 14.086565017700195 = 1.618933916091919 + 2.0 * 6.233815670013428
Epoch 190, val loss: 1.6524261236190796
Epoch 200, training loss: 14.032007217407227 = 1.5875349044799805 + 2.0 * 6.222236156463623
Epoch 200, val loss: 1.6270203590393066
Epoch 210, training loss: 13.982772827148438 = 1.553066372871399 + 2.0 * 6.214853286743164
Epoch 210, val loss: 1.599518060684204
Epoch 220, training loss: 13.921316146850586 = 1.5161057710647583 + 2.0 * 6.202605247497559
Epoch 220, val loss: 1.5701491832733154
Epoch 230, training loss: 13.86514663696289 = 1.47654128074646 + 2.0 * 6.194302558898926
Epoch 230, val loss: 1.5391331911087036
Epoch 240, training loss: 13.808661460876465 = 1.4348505735397339 + 2.0 * 6.186905384063721
Epoch 240, val loss: 1.5068573951721191
Epoch 250, training loss: 13.751608848571777 = 1.3915194272994995 + 2.0 * 6.180044651031494
Epoch 250, val loss: 1.4740461111068726
Epoch 260, training loss: 13.693475723266602 = 1.3469538688659668 + 2.0 * 6.1732611656188965
Epoch 260, val loss: 1.4408828020095825
Epoch 270, training loss: 13.649744987487793 = 1.3015296459197998 + 2.0 * 6.174107551574707
Epoch 270, val loss: 1.407689094543457
Epoch 280, training loss: 13.586155891418457 = 1.2561748027801514 + 2.0 * 6.164990425109863
Epoch 280, val loss: 1.375492811203003
Epoch 290, training loss: 13.525495529174805 = 1.2113205194473267 + 2.0 * 6.157087326049805
Epoch 290, val loss: 1.3441145420074463
Epoch 300, training loss: 13.476119995117188 = 1.1670069694519043 + 2.0 * 6.1545562744140625
Epoch 300, val loss: 1.3135344982147217
Epoch 310, training loss: 13.42598819732666 = 1.1243332624435425 + 2.0 * 6.150827407836914
Epoch 310, val loss: 1.2839372158050537
Epoch 320, training loss: 13.371112823486328 = 1.0828781127929688 + 2.0 * 6.14411735534668
Epoch 320, val loss: 1.255616307258606
Epoch 330, training loss: 13.321269989013672 = 1.042747974395752 + 2.0 * 6.139261245727539
Epoch 330, val loss: 1.2282227277755737
Epoch 340, training loss: 13.276580810546875 = 1.0038084983825684 + 2.0 * 6.136386394500732
Epoch 340, val loss: 1.2014933824539185
Epoch 350, training loss: 13.234471321105957 = 0.9660508036613464 + 2.0 * 6.134210109710693
Epoch 350, val loss: 1.175290822982788
Epoch 360, training loss: 13.186223030090332 = 0.9290228486061096 + 2.0 * 6.128600120544434
Epoch 360, val loss: 1.1494204998016357
Epoch 370, training loss: 13.142850875854492 = 0.8925895690917969 + 2.0 * 6.125130653381348
Epoch 370, val loss: 1.1239715814590454
Epoch 380, training loss: 13.101297378540039 = 0.8563501238822937 + 2.0 * 6.12247371673584
Epoch 380, val loss: 1.0982316732406616
Epoch 390, training loss: 13.071425437927246 = 0.820381760597229 + 2.0 * 6.125521659851074
Epoch 390, val loss: 1.0723748207092285
Epoch 400, training loss: 13.01983642578125 = 0.7847993969917297 + 2.0 * 6.117518424987793
Epoch 400, val loss: 1.046757698059082
Epoch 410, training loss: 12.97574520111084 = 0.7494410872459412 + 2.0 * 6.113152027130127
Epoch 410, val loss: 1.0211350917816162
Epoch 420, training loss: 12.93449592590332 = 0.7140897512435913 + 2.0 * 6.110203266143799
Epoch 420, val loss: 0.9955698847770691
Epoch 430, training loss: 12.907194137573242 = 0.6788450479507446 + 2.0 * 6.1141743659973145
Epoch 430, val loss: 0.9703744649887085
Epoch 440, training loss: 12.859028816223145 = 0.6446021795272827 + 2.0 * 6.107213497161865
Epoch 440, val loss: 0.9457421898841858
Epoch 450, training loss: 12.818174362182617 = 0.6110960245132446 + 2.0 * 6.103538990020752
Epoch 450, val loss: 0.922429621219635
Epoch 460, training loss: 12.780292510986328 = 0.5786208510398865 + 2.0 * 6.100835800170898
Epoch 460, val loss: 0.9002456665039062
Epoch 470, training loss: 12.743600845336914 = 0.5472210049629211 + 2.0 * 6.098189830780029
Epoch 470, val loss: 0.8794355988502502
Epoch 480, training loss: 12.716286659240723 = 0.5170842409133911 + 2.0 * 6.0996012687683105
Epoch 480, val loss: 0.8601081967353821
Epoch 490, training loss: 12.677654266357422 = 0.4884943664073944 + 2.0 * 6.094580173492432
Epoch 490, val loss: 0.8424906730651855
Epoch 500, training loss: 12.645768165588379 = 0.46132946014404297 + 2.0 * 6.092219352722168
Epoch 500, val loss: 0.8265290856361389
Epoch 510, training loss: 12.621146202087402 = 0.4355575442314148 + 2.0 * 6.092794418334961
Epoch 510, val loss: 0.8119783401489258
Epoch 520, training loss: 12.58923053741455 = 0.4110570549964905 + 2.0 * 6.089086532592773
Epoch 520, val loss: 0.7988504767417908
Epoch 530, training loss: 12.574033737182617 = 0.38799622654914856 + 2.0 * 6.093018531799316
Epoch 530, val loss: 0.7870146036148071
Epoch 540, training loss: 12.535473823547363 = 0.36610978841781616 + 2.0 * 6.084681987762451
Epoch 540, val loss: 0.7764440774917603
Epoch 550, training loss: 12.510727882385254 = 0.3454616665840149 + 2.0 * 6.082633018493652
Epoch 550, val loss: 0.767074465751648
Epoch 560, training loss: 12.485931396484375 = 0.32583969831466675 + 2.0 * 6.080045700073242
Epoch 560, val loss: 0.7586840391159058
Epoch 570, training loss: 12.472190856933594 = 0.3072012662887573 + 2.0 * 6.082494735717773
Epoch 570, val loss: 0.7511482238769531
Epoch 580, training loss: 12.450400352478027 = 0.28952130675315857 + 2.0 * 6.080439567565918
Epoch 580, val loss: 0.7445241212844849
Epoch 590, training loss: 12.426477432250977 = 0.27285727858543396 + 2.0 * 6.076809883117676
Epoch 590, val loss: 0.7389301061630249
Epoch 600, training loss: 12.4108247756958 = 0.2570626437664032 + 2.0 * 6.076880931854248
Epoch 600, val loss: 0.7342040538787842
Epoch 610, training loss: 12.386643409729004 = 0.2420395165681839 + 2.0 * 6.072301864624023
Epoch 610, val loss: 0.7303066849708557
Epoch 620, training loss: 12.369876861572266 = 0.2278423309326172 + 2.0 * 6.071017265319824
Epoch 620, val loss: 0.7272204756736755
Epoch 630, training loss: 12.361038208007812 = 0.21442031860351562 + 2.0 * 6.073308944702148
Epoch 630, val loss: 0.724891722202301
Epoch 640, training loss: 12.348359107971191 = 0.2018377035856247 + 2.0 * 6.07326078414917
Epoch 640, val loss: 0.7232327461242676
Epoch 650, training loss: 12.325709342956543 = 0.19006191194057465 + 2.0 * 6.067823886871338
Epoch 650, val loss: 0.7223602533340454
Epoch 660, training loss: 12.309218406677246 = 0.17900821566581726 + 2.0 * 6.065104961395264
Epoch 660, val loss: 0.722202718257904
Epoch 670, training loss: 12.29653549194336 = 0.1686461716890335 + 2.0 * 6.0639448165893555
Epoch 670, val loss: 0.7226074934005737
Epoch 680, training loss: 12.291630744934082 = 0.15896447002887726 + 2.0 * 6.066333293914795
Epoch 680, val loss: 0.7234031558036804
Epoch 690, training loss: 12.276256561279297 = 0.14999054372310638 + 2.0 * 6.063133239746094
Epoch 690, val loss: 0.7249172925949097
Epoch 700, training loss: 12.262449264526367 = 0.14163699746131897 + 2.0 * 6.06040620803833
Epoch 700, val loss: 0.7268345355987549
Epoch 710, training loss: 12.261247634887695 = 0.13385556638240814 + 2.0 * 6.063695907592773
Epoch 710, val loss: 0.7291788458824158
Epoch 720, training loss: 12.245620727539062 = 0.1265714019536972 + 2.0 * 6.0595245361328125
Epoch 720, val loss: 0.7319570183753967
Epoch 730, training loss: 12.235495567321777 = 0.11981617659330368 + 2.0 * 6.057839870452881
Epoch 730, val loss: 0.7351706027984619
Epoch 740, training loss: 12.230753898620605 = 0.11352255940437317 + 2.0 * 6.058615684509277
Epoch 740, val loss: 0.7387068867683411
Epoch 750, training loss: 12.220154762268066 = 0.10763879120349884 + 2.0 * 6.056258201599121
Epoch 750, val loss: 0.742536723613739
Epoch 760, training loss: 12.209701538085938 = 0.10216934233903885 + 2.0 * 6.053766250610352
Epoch 760, val loss: 0.7466216087341309
Epoch 770, training loss: 12.204750061035156 = 0.09706372022628784 + 2.0 * 6.053843021392822
Epoch 770, val loss: 0.7509983777999878
Epoch 780, training loss: 12.201119422912598 = 0.09228698164224625 + 2.0 * 6.054416179656982
Epoch 780, val loss: 0.7555367350578308
Epoch 790, training loss: 12.193533897399902 = 0.08783984184265137 + 2.0 * 6.052846908569336
Epoch 790, val loss: 0.7603772282600403
Epoch 800, training loss: 12.186776161193848 = 0.08368656039237976 + 2.0 * 6.051544666290283
Epoch 800, val loss: 0.7652928233146667
Epoch 810, training loss: 12.179373741149902 = 0.07978541404008865 + 2.0 * 6.0497941970825195
Epoch 810, val loss: 0.7704038023948669
Epoch 820, training loss: 12.17466926574707 = 0.07613328844308853 + 2.0 * 6.049267768859863
Epoch 820, val loss: 0.7755938768386841
Epoch 830, training loss: 12.171615600585938 = 0.07271236181259155 + 2.0 * 6.04945182800293
Epoch 830, val loss: 0.7809170484542847
Epoch 840, training loss: 12.163138389587402 = 0.06950939446687698 + 2.0 * 6.046814441680908
Epoch 840, val loss: 0.786450207233429
Epoch 850, training loss: 12.158239364624023 = 0.06647930294275284 + 2.0 * 6.04587984085083
Epoch 850, val loss: 0.7919012904167175
Epoch 860, training loss: 12.160394668579102 = 0.06364167481660843 + 2.0 * 6.048376560211182
Epoch 860, val loss: 0.7973417639732361
Epoch 870, training loss: 12.148175239562988 = 0.06098872423171997 + 2.0 * 6.043593406677246
Epoch 870, val loss: 0.803004264831543
Epoch 880, training loss: 12.144440650939941 = 0.05847281590104103 + 2.0 * 6.0429840087890625
Epoch 880, val loss: 0.8086346387863159
Epoch 890, training loss: 12.1450834274292 = 0.05610489100217819 + 2.0 * 6.04448938369751
Epoch 890, val loss: 0.8142432570457458
Epoch 900, training loss: 12.137308120727539 = 0.05386795476078987 + 2.0 * 6.041719913482666
Epoch 900, val loss: 0.8198794722557068
Epoch 910, training loss: 12.132218360900879 = 0.051754072308540344 + 2.0 * 6.040232181549072
Epoch 910, val loss: 0.8255704641342163
Epoch 920, training loss: 12.127702713012695 = 0.049762774258852005 + 2.0 * 6.038969993591309
Epoch 920, val loss: 0.8312754034996033
Epoch 930, training loss: 12.127914428710938 = 0.04787128046154976 + 2.0 * 6.0400214195251465
Epoch 930, val loss: 0.8369405269622803
Epoch 940, training loss: 12.130366325378418 = 0.046084146946668625 + 2.0 * 6.042140960693359
Epoch 940, val loss: 0.8424986004829407
Epoch 950, training loss: 12.120421409606934 = 0.04440337419509888 + 2.0 * 6.038009166717529
Epoch 950, val loss: 0.848188579082489
Epoch 960, training loss: 12.115453720092773 = 0.04280390590429306 + 2.0 * 6.036324977874756
Epoch 960, val loss: 0.8537654876708984
Epoch 970, training loss: 12.111111640930176 = 0.04129175469279289 + 2.0 * 6.034909725189209
Epoch 970, val loss: 0.8593922257423401
Epoch 980, training loss: 12.110466957092285 = 0.039856236428022385 + 2.0 * 6.035305500030518
Epoch 980, val loss: 0.8649810552597046
Epoch 990, training loss: 12.112671852111816 = 0.03849165886640549 + 2.0 * 6.037090301513672
Epoch 990, val loss: 0.8702892065048218
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7704
Overall ASR: 0.7306
Flip ASR: 0.6800/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.697628021240234 = 1.9498156309127808 + 2.0 * 8.373906135559082
Epoch 0, val loss: 1.9513647556304932
Epoch 10, training loss: 18.685564041137695 = 1.9387446641921997 + 2.0 * 8.373409271240234
Epoch 10, val loss: 1.9395259618759155
Epoch 20, training loss: 18.663686752319336 = 1.9247832298278809 + 2.0 * 8.369451522827148
Epoch 20, val loss: 1.924377202987671
Epoch 30, training loss: 18.592496871948242 = 1.9056483507156372 + 2.0 * 8.343423843383789
Epoch 30, val loss: 1.9034305810928345
Epoch 40, training loss: 18.249940872192383 = 1.8815394639968872 + 2.0 * 8.184200286865234
Epoch 40, val loss: 1.877747893333435
Epoch 50, training loss: 16.871252059936523 = 1.8545987606048584 + 2.0 * 7.508326530456543
Epoch 50, val loss: 1.8495339155197144
Epoch 60, training loss: 16.3237247467041 = 1.8289605379104614 + 2.0 * 7.247382164001465
Epoch 60, val loss: 1.8243765830993652
Epoch 70, training loss: 15.980587005615234 = 1.8067593574523926 + 2.0 * 7.0869140625
Epoch 70, val loss: 1.8025095462799072
Epoch 80, training loss: 15.64133358001709 = 1.784629225730896 + 2.0 * 6.928352355957031
Epoch 80, val loss: 1.7806396484375
Epoch 90, training loss: 15.297228813171387 = 1.76490318775177 + 2.0 * 6.766162872314453
Epoch 90, val loss: 1.7619010210037231
Epoch 100, training loss: 15.031952857971191 = 1.747205138206482 + 2.0 * 6.642374038696289
Epoch 100, val loss: 1.7450628280639648
Epoch 110, training loss: 14.837502479553223 = 1.7290149927139282 + 2.0 * 6.554243564605713
Epoch 110, val loss: 1.7279516458511353
Epoch 120, training loss: 14.688924789428711 = 1.708737850189209 + 2.0 * 6.490093231201172
Epoch 120, val loss: 1.7092351913452148
Epoch 130, training loss: 14.561105728149414 = 1.68632173538208 + 2.0 * 6.437392234802246
Epoch 130, val loss: 1.6889854669570923
Epoch 140, training loss: 14.444133758544922 = 1.66209077835083 + 2.0 * 6.391021728515625
Epoch 140, val loss: 1.667801022529602
Epoch 150, training loss: 14.3495512008667 = 1.635594129562378 + 2.0 * 6.356978416442871
Epoch 150, val loss: 1.6451271772384644
Epoch 160, training loss: 14.262371063232422 = 1.6054755449295044 + 2.0 * 6.3284478187561035
Epoch 160, val loss: 1.6198769807815552
Epoch 170, training loss: 14.185115814208984 = 1.5720534324645996 + 2.0 * 6.3065314292907715
Epoch 170, val loss: 1.592323899269104
Epoch 180, training loss: 14.102283477783203 = 1.5359337329864502 + 2.0 * 6.283174991607666
Epoch 180, val loss: 1.563098669052124
Epoch 190, training loss: 14.027997016906738 = 1.4973350763320923 + 2.0 * 6.265330791473389
Epoch 190, val loss: 1.5322407484054565
Epoch 200, training loss: 13.966176986694336 = 1.4566622972488403 + 2.0 * 6.254757404327393
Epoch 200, val loss: 1.5001137256622314
Epoch 210, training loss: 13.892627716064453 = 1.4148805141448975 + 2.0 * 6.238873481750488
Epoch 210, val loss: 1.4678705930709839
Epoch 220, training loss: 13.82752799987793 = 1.372525691986084 + 2.0 * 6.227501392364502
Epoch 220, val loss: 1.4360401630401611
Epoch 230, training loss: 13.768168449401855 = 1.3302431106567383 + 2.0 * 6.218962669372559
Epoch 230, val loss: 1.4051263332366943
Epoch 240, training loss: 13.705400466918945 = 1.288824439048767 + 2.0 * 6.208288192749023
Epoch 240, val loss: 1.375321626663208
Epoch 250, training loss: 13.647297859191895 = 1.248042345046997 + 2.0 * 6.199627876281738
Epoch 250, val loss: 1.346356749534607
Epoch 260, training loss: 13.591835975646973 = 1.207947850227356 + 2.0 * 6.191944122314453
Epoch 260, val loss: 1.3182438611984253
Epoch 270, training loss: 13.534208297729492 = 1.168621301651001 + 2.0 * 6.182793617248535
Epoch 270, val loss: 1.2906965017318726
Epoch 280, training loss: 13.479080200195312 = 1.1296073198318481 + 2.0 * 6.174736499786377
Epoch 280, val loss: 1.2632708549499512
Epoch 290, training loss: 13.430204391479492 = 1.090918779373169 + 2.0 * 6.169642925262451
Epoch 290, val loss: 1.2358863353729248
Epoch 300, training loss: 13.37893295288086 = 1.0528202056884766 + 2.0 * 6.163056373596191
Epoch 300, val loss: 1.2086495161056519
Epoch 310, training loss: 13.325078964233398 = 1.0152161121368408 + 2.0 * 6.154931545257568
Epoch 310, val loss: 1.1813544034957886
Epoch 320, training loss: 13.276227951049805 = 0.9778333902359009 + 2.0 * 6.149197101593018
Epoch 320, val loss: 1.1541109085083008
Epoch 330, training loss: 13.240845680236816 = 0.9407498240470886 + 2.0 * 6.150047779083252
Epoch 330, val loss: 1.1270475387573242
Epoch 340, training loss: 13.187422752380371 = 0.9046071171760559 + 2.0 * 6.1414079666137695
Epoch 340, val loss: 1.1004287004470825
Epoch 350, training loss: 13.14040470123291 = 0.8692328333854675 + 2.0 * 6.135585784912109
Epoch 350, val loss: 1.0743991136550903
Epoch 360, training loss: 13.109396934509277 = 0.8347509503364563 + 2.0 * 6.137322902679443
Epoch 360, val loss: 1.049141526222229
Epoch 370, training loss: 13.059017181396484 = 0.8013589382171631 + 2.0 * 6.128829002380371
Epoch 370, val loss: 1.0249688625335693
Epoch 380, training loss: 13.017539978027344 = 0.769184947013855 + 2.0 * 6.1241774559021
Epoch 380, val loss: 1.0017890930175781
Epoch 390, training loss: 12.981064796447754 = 0.7382047176361084 + 2.0 * 6.121429920196533
Epoch 390, val loss: 0.97980797290802
Epoch 400, training loss: 12.948081970214844 = 0.7087175250053406 + 2.0 * 6.119682312011719
Epoch 400, val loss: 0.9592170119285583
Epoch 410, training loss: 12.910951614379883 = 0.6808382868766785 + 2.0 * 6.11505651473999
Epoch 410, val loss: 0.9401730895042419
Epoch 420, training loss: 12.8784761428833 = 0.6544268131256104 + 2.0 * 6.112024784088135
Epoch 420, val loss: 0.9225881695747375
Epoch 430, training loss: 12.869264602661133 = 0.6293339729309082 + 2.0 * 6.119965076446533
Epoch 430, val loss: 0.9063944220542908
Epoch 440, training loss: 12.825459480285645 = 0.6059380769729614 + 2.0 * 6.109760761260986
Epoch 440, val loss: 0.8917277455329895
Epoch 450, training loss: 12.795053482055664 = 0.5838155746459961 + 2.0 * 6.105618953704834
Epoch 450, val loss: 0.8784079551696777
Epoch 460, training loss: 12.76715087890625 = 0.562828540802002 + 2.0 * 6.102160930633545
Epoch 460, val loss: 0.8663789629936218
Epoch 470, training loss: 12.742147445678711 = 0.542729377746582 + 2.0 * 6.0997090339660645
Epoch 470, val loss: 0.8554771542549133
Epoch 480, training loss: 12.737222671508789 = 0.5233970284461975 + 2.0 * 6.106912612915039
Epoch 480, val loss: 0.8455318808555603
Epoch 490, training loss: 12.699121475219727 = 0.5050726532936096 + 2.0 * 6.097024440765381
Epoch 490, val loss: 0.8367155194282532
Epoch 500, training loss: 12.675174713134766 = 0.487507700920105 + 2.0 * 6.0938334465026855
Epoch 500, val loss: 0.8287384510040283
Epoch 510, training loss: 12.653358459472656 = 0.470543771982193 + 2.0 * 6.091407299041748
Epoch 510, val loss: 0.821487545967102
Epoch 520, training loss: 12.642190933227539 = 0.4541182518005371 + 2.0 * 6.09403657913208
Epoch 520, val loss: 0.8149883151054382
Epoch 530, training loss: 12.617023468017578 = 0.43812164664268494 + 2.0 * 6.089450836181641
Epoch 530, val loss: 0.8091013431549072
Epoch 540, training loss: 12.598309516906738 = 0.4226013720035553 + 2.0 * 6.087853908538818
Epoch 540, val loss: 0.8037947416305542
Epoch 550, training loss: 12.57899284362793 = 0.40739911794662476 + 2.0 * 6.08579683303833
Epoch 550, val loss: 0.7988249659538269
Epoch 560, training loss: 12.563331604003906 = 0.39263200759887695 + 2.0 * 6.085350036621094
Epoch 560, val loss: 0.7944749593734741
Epoch 570, training loss: 12.540304183959961 = 0.3781963884830475 + 2.0 * 6.081053733825684
Epoch 570, val loss: 0.7903476357460022
Epoch 580, training loss: 12.521268844604492 = 0.36401546001434326 + 2.0 * 6.07862663269043
Epoch 580, val loss: 0.786615252494812
Epoch 590, training loss: 12.50257682800293 = 0.3500589430332184 + 2.0 * 6.076259136199951
Epoch 590, val loss: 0.7833468914031982
Epoch 600, training loss: 12.513545989990234 = 0.33634617924690247 + 2.0 * 6.088599681854248
Epoch 600, val loss: 0.7804064154624939
Epoch 610, training loss: 12.472916603088379 = 0.32287245988845825 + 2.0 * 6.075022220611572
Epoch 610, val loss: 0.7775691747665405
Epoch 620, training loss: 12.455050468444824 = 0.3097016513347626 + 2.0 * 6.07267427444458
Epoch 620, val loss: 0.7749835252761841
Epoch 630, training loss: 12.437993049621582 = 0.2967136800289154 + 2.0 * 6.070639610290527
Epoch 630, val loss: 0.7727682590484619
Epoch 640, training loss: 12.421323776245117 = 0.283914178609848 + 2.0 * 6.068704605102539
Epoch 640, val loss: 0.7708138227462769
Epoch 650, training loss: 12.420872688293457 = 0.27131035923957825 + 2.0 * 6.0747809410095215
Epoch 650, val loss: 0.7691239714622498
Epoch 660, training loss: 12.396448135375977 = 0.2590145766735077 + 2.0 * 6.068717002868652
Epoch 660, val loss: 0.7677651643753052
Epoch 670, training loss: 12.37837028503418 = 0.247039794921875 + 2.0 * 6.065665245056152
Epoch 670, val loss: 0.7664780616760254
Epoch 680, training loss: 12.362690925598145 = 0.23529113829135895 + 2.0 * 6.063699722290039
Epoch 680, val loss: 0.7657619118690491
Epoch 690, training loss: 12.348273277282715 = 0.2237793356180191 + 2.0 * 6.062246799468994
Epoch 690, val loss: 0.7653792500495911
Epoch 700, training loss: 12.363800048828125 = 0.21249815821647644 + 2.0 * 6.075651168823242
Epoch 700, val loss: 0.7652279734611511
Epoch 710, training loss: 12.32338809967041 = 0.201543927192688 + 2.0 * 6.060922145843506
Epoch 710, val loss: 0.7655355930328369
Epoch 720, training loss: 12.308837890625 = 0.19069136679172516 + 2.0 * 6.059073448181152
Epoch 720, val loss: 0.7660771608352661
Epoch 730, training loss: 12.29554557800293 = 0.18019162118434906 + 2.0 * 6.057676792144775
Epoch 730, val loss: 0.7672080397605896
Epoch 740, training loss: 12.282998085021973 = 0.17009888589382172 + 2.0 * 6.0564494132995605
Epoch 740, val loss: 0.7688239216804504
Epoch 750, training loss: 12.279898643493652 = 0.16070590913295746 + 2.0 * 6.059596538543701
Epoch 750, val loss: 0.770599901676178
Epoch 760, training loss: 12.266763687133789 = 0.15212613344192505 + 2.0 * 6.057318687438965
Epoch 760, val loss: 0.7727968096733093
Epoch 770, training loss: 12.252472877502441 = 0.14431674778461456 + 2.0 * 6.054078102111816
Epoch 770, val loss: 0.7755361199378967
Epoch 780, training loss: 12.242289543151855 = 0.13705237209796906 + 2.0 * 6.052618503570557
Epoch 780, val loss: 0.778758704662323
Epoch 790, training loss: 12.246891021728516 = 0.13024669885635376 + 2.0 * 6.058321952819824
Epoch 790, val loss: 0.7823528051376343
Epoch 800, training loss: 12.22649097442627 = 0.12392296642065048 + 2.0 * 6.051283836364746
Epoch 800, val loss: 0.786159873008728
Epoch 810, training loss: 12.217167854309082 = 0.11795520782470703 + 2.0 * 6.0496063232421875
Epoch 810, val loss: 0.7901792526245117
Epoch 820, training loss: 12.208532333374023 = 0.11232497543096542 + 2.0 * 6.0481038093566895
Epoch 820, val loss: 0.7945354580879211
Epoch 830, training loss: 12.202542304992676 = 0.10701901465654373 + 2.0 * 6.0477614402771
Epoch 830, val loss: 0.7991692423820496
Epoch 840, training loss: 12.206475257873535 = 0.10203813761472702 + 2.0 * 6.052218437194824
Epoch 840, val loss: 0.804068386554718
Epoch 850, training loss: 12.190902709960938 = 0.09735886007547379 + 2.0 * 6.046772003173828
Epoch 850, val loss: 0.8087823987007141
Epoch 860, training loss: 12.182602882385254 = 0.09297478944063187 + 2.0 * 6.044814109802246
Epoch 860, val loss: 0.8135881423950195
Epoch 870, training loss: 12.176830291748047 = 0.08883277326822281 + 2.0 * 6.043998718261719
Epoch 870, val loss: 0.8187588453292847
Epoch 880, training loss: 12.17072868347168 = 0.0849197581410408 + 2.0 * 6.042904376983643
Epoch 880, val loss: 0.8240906596183777
Epoch 890, training loss: 12.175691604614258 = 0.08120929449796677 + 2.0 * 6.0472412109375
Epoch 890, val loss: 0.8294981122016907
Epoch 900, training loss: 12.170638084411621 = 0.07773832976818085 + 2.0 * 6.046449661254883
Epoch 900, val loss: 0.8349474668502808
Epoch 910, training loss: 12.157299995422363 = 0.07443337142467499 + 2.0 * 6.041433334350586
Epoch 910, val loss: 0.8401991128921509
Epoch 920, training loss: 12.161076545715332 = 0.07132402062416077 + 2.0 * 6.0448760986328125
Epoch 920, val loss: 0.845664381980896
Epoch 930, training loss: 12.148748397827148 = 0.06837306916713715 + 2.0 * 6.040187835693359
Epoch 930, val loss: 0.8511137962341309
Epoch 940, training loss: 12.141732215881348 = 0.06558659672737122 + 2.0 * 6.03807258605957
Epoch 940, val loss: 0.8566104173660278
Epoch 950, training loss: 12.13987922668457 = 0.06294272840023041 + 2.0 * 6.038468360900879
Epoch 950, val loss: 0.862154483795166
Epoch 960, training loss: 12.158370971679688 = 0.060440581291913986 + 2.0 * 6.048964977264404
Epoch 960, val loss: 0.8676815629005432
Epoch 970, training loss: 12.13348388671875 = 0.05806468427181244 + 2.0 * 6.037709712982178
Epoch 970, val loss: 0.8730210065841675
Epoch 980, training loss: 12.128052711486816 = 0.055832330137491226 + 2.0 * 6.0361104011535645
Epoch 980, val loss: 0.8782753348350525
Epoch 990, training loss: 12.124073028564453 = 0.05371234193444252 + 2.0 * 6.035180568695068
Epoch 990, val loss: 0.8836860656738281
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.6236
Flip ASR: 0.5689/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.683664321899414 = 1.9358221292495728 + 2.0 * 8.373921394348145
Epoch 0, val loss: 1.9367709159851074
Epoch 10, training loss: 18.6734619140625 = 1.9263077974319458 + 2.0 * 8.373577117919922
Epoch 10, val loss: 1.9278383255004883
Epoch 20, training loss: 18.656436920166016 = 1.9149936437606812 + 2.0 * 8.370721817016602
Epoch 20, val loss: 1.9169517755508423
Epoch 30, training loss: 18.59577751159668 = 1.90012788772583 + 2.0 * 8.347825050354004
Epoch 30, val loss: 1.902531385421753
Epoch 40, training loss: 18.269636154174805 = 1.881546139717102 + 2.0 * 8.194045066833496
Epoch 40, val loss: 1.8847521543502808
Epoch 50, training loss: 17.247177124023438 = 1.8608858585357666 + 2.0 * 7.693145751953125
Epoch 50, val loss: 1.8649123907089233
Epoch 60, training loss: 16.46282958984375 = 1.8425579071044922 + 2.0 * 7.310135364532471
Epoch 60, val loss: 1.8477568626403809
Epoch 70, training loss: 15.70045280456543 = 1.8287335634231567 + 2.0 * 6.935859680175781
Epoch 70, val loss: 1.8342936038970947
Epoch 80, training loss: 15.234769821166992 = 1.8158049583435059 + 2.0 * 6.709482669830322
Epoch 80, val loss: 1.8206148147583008
Epoch 90, training loss: 14.993866920471191 = 1.802006721496582 + 2.0 * 6.595930099487305
Epoch 90, val loss: 1.8066062927246094
Epoch 100, training loss: 14.83265495300293 = 1.7881407737731934 + 2.0 * 6.522256851196289
Epoch 100, val loss: 1.7931662797927856
Epoch 110, training loss: 14.689714431762695 = 1.7743430137634277 + 2.0 * 6.457685470581055
Epoch 110, val loss: 1.7795346975326538
Epoch 120, training loss: 14.57791805267334 = 1.7598955631256104 + 2.0 * 6.409011363983154
Epoch 120, val loss: 1.765255093574524
Epoch 130, training loss: 14.49081802368164 = 1.743901014328003 + 2.0 * 6.373458385467529
Epoch 130, val loss: 1.7498977184295654
Epoch 140, training loss: 14.41300106048584 = 1.726340889930725 + 2.0 * 6.343329906463623
Epoch 140, val loss: 1.7333980798721313
Epoch 150, training loss: 14.340267181396484 = 1.7069841623306274 + 2.0 * 6.316641330718994
Epoch 150, val loss: 1.715852975845337
Epoch 160, training loss: 14.27842903137207 = 1.6854238510131836 + 2.0 * 6.296502590179443
Epoch 160, val loss: 1.6969373226165771
Epoch 170, training loss: 14.209351539611816 = 1.661341905593872 + 2.0 * 6.274004936218262
Epoch 170, val loss: 1.6766091585159302
Epoch 180, training loss: 14.152458190917969 = 1.6344506740570068 + 2.0 * 6.259003639221191
Epoch 180, val loss: 1.654454231262207
Epoch 190, training loss: 14.084863662719727 = 1.6047067642211914 + 2.0 * 6.240078449249268
Epoch 190, val loss: 1.6303387880325317
Epoch 200, training loss: 14.024346351623535 = 1.5716644525527954 + 2.0 * 6.2263407707214355
Epoch 200, val loss: 1.6039470434188843
Epoch 210, training loss: 13.965460777282715 = 1.5348073244094849 + 2.0 * 6.21532678604126
Epoch 210, val loss: 1.574837565422058
Epoch 220, training loss: 13.904419898986816 = 1.494307518005371 + 2.0 * 6.205056190490723
Epoch 220, val loss: 1.5430736541748047
Epoch 230, training loss: 13.838699340820312 = 1.4504287242889404 + 2.0 * 6.1941351890563965
Epoch 230, val loss: 1.5089850425720215
Epoch 240, training loss: 13.774399757385254 = 1.4029802083969116 + 2.0 * 6.1857099533081055
Epoch 240, val loss: 1.472325325012207
Epoch 250, training loss: 13.712810516357422 = 1.3522443771362305 + 2.0 * 6.180283069610596
Epoch 250, val loss: 1.4334321022033691
Epoch 260, training loss: 13.640955924987793 = 1.299357295036316 + 2.0 * 6.170799255371094
Epoch 260, val loss: 1.3927690982818604
Epoch 270, training loss: 13.573028564453125 = 1.244508981704712 + 2.0 * 6.164259910583496
Epoch 270, val loss: 1.3509790897369385
Epoch 280, training loss: 13.508453369140625 = 1.18874990940094 + 2.0 * 6.159851551055908
Epoch 280, val loss: 1.308657169342041
Epoch 290, training loss: 13.440984725952148 = 1.1329724788665771 + 2.0 * 6.154006004333496
Epoch 290, val loss: 1.2668043375015259
Epoch 300, training loss: 13.376001358032227 = 1.078270435333252 + 2.0 * 6.148865222930908
Epoch 300, val loss: 1.2257682085037231
Epoch 310, training loss: 13.3126220703125 = 1.0249570608139038 + 2.0 * 6.143832683563232
Epoch 310, val loss: 1.185996413230896
Epoch 320, training loss: 13.250822067260742 = 0.9733486771583557 + 2.0 * 6.138736724853516
Epoch 320, val loss: 1.147441029548645
Epoch 330, training loss: 13.213089942932129 = 0.9234153032302856 + 2.0 * 6.144837379455566
Epoch 330, val loss: 1.1105186939239502
Epoch 340, training loss: 13.145851135253906 = 0.8765487670898438 + 2.0 * 6.134651184082031
Epoch 340, val loss: 1.075652003288269
Epoch 350, training loss: 13.08780288696289 = 0.8323212265968323 + 2.0 * 6.127740859985352
Epoch 350, val loss: 1.043251872062683
Epoch 360, training loss: 13.039201736450195 = 0.7903338670730591 + 2.0 * 6.124433994293213
Epoch 360, val loss: 1.012886643409729
Epoch 370, training loss: 12.991975784301758 = 0.7504220008850098 + 2.0 * 6.120777130126953
Epoch 370, val loss: 0.9842411875724792
Epoch 380, training loss: 12.948803901672363 = 0.7124899625778198 + 2.0 * 6.118156909942627
Epoch 380, val loss: 0.9573246836662292
Epoch 390, training loss: 12.918516159057617 = 0.6766400337219238 + 2.0 * 6.120937824249268
Epoch 390, val loss: 0.9322015643119812
Epoch 400, training loss: 12.869485855102539 = 0.6430622339248657 + 2.0 * 6.113211631774902
Epoch 400, val loss: 0.9091604351997375
Epoch 410, training loss: 12.832308769226074 = 0.6113548874855042 + 2.0 * 6.110476970672607
Epoch 410, val loss: 0.8878191113471985
Epoch 420, training loss: 12.796661376953125 = 0.5813304781913757 + 2.0 * 6.107665538787842
Epoch 420, val loss: 0.8680111169815063
Epoch 430, training loss: 12.763426780700684 = 0.5528956055641174 + 2.0 * 6.1052656173706055
Epoch 430, val loss: 0.8495389819145203
Epoch 440, training loss: 12.730695724487305 = 0.5258269309997559 + 2.0 * 6.1024346351623535
Epoch 440, val loss: 0.832344651222229
Epoch 450, training loss: 12.69993782043457 = 0.4998970031738281 + 2.0 * 6.100020408630371
Epoch 450, val loss: 0.8163281083106995
Epoch 460, training loss: 12.686063766479492 = 0.475182443857193 + 2.0 * 6.105440616607666
Epoch 460, val loss: 0.8013597130775452
Epoch 470, training loss: 12.64392375946045 = 0.4516809284687042 + 2.0 * 6.096121311187744
Epoch 470, val loss: 0.7876514792442322
Epoch 480, training loss: 12.616036415100098 = 0.4292677044868469 + 2.0 * 6.093384265899658
Epoch 480, val loss: 0.7749055027961731
Epoch 490, training loss: 12.590253829956055 = 0.40763577818870544 + 2.0 * 6.091309070587158
Epoch 490, val loss: 0.7629708051681519
Epoch 500, training loss: 12.575329780578613 = 0.38663971424102783 + 2.0 * 6.0943450927734375
Epoch 500, val loss: 0.7517805099487305
Epoch 510, training loss: 12.54996395111084 = 0.36655426025390625 + 2.0 * 6.091704845428467
Epoch 510, val loss: 0.7414372563362122
Epoch 520, training loss: 12.520700454711914 = 0.34711670875549316 + 2.0 * 6.0867919921875
Epoch 520, val loss: 0.7317963242530823
Epoch 530, training loss: 12.496186256408691 = 0.32830801606178284 + 2.0 * 6.083939075469971
Epoch 530, val loss: 0.7228617072105408
Epoch 540, training loss: 12.492009162902832 = 0.3101012110710144 + 2.0 * 6.090953826904297
Epoch 540, val loss: 0.7146389484405518
Epoch 550, training loss: 12.457939147949219 = 0.2927914261817932 + 2.0 * 6.082573890686035
Epoch 550, val loss: 0.7070125937461853
Epoch 560, training loss: 12.437223434448242 = 0.276133269071579 + 2.0 * 6.080544948577881
Epoch 560, val loss: 0.7001163959503174
Epoch 570, training loss: 12.41533374786377 = 0.26027628779411316 + 2.0 * 6.077528953552246
Epoch 570, val loss: 0.6939461827278137
Epoch 580, training loss: 12.398181915283203 = 0.2452225387096405 + 2.0 * 6.076479911804199
Epoch 580, val loss: 0.6884710192680359
Epoch 590, training loss: 12.381906509399414 = 0.23097831010818481 + 2.0 * 6.075464248657227
Epoch 590, val loss: 0.6836753487586975
Epoch 600, training loss: 12.368106842041016 = 0.2175857424736023 + 2.0 * 6.075260639190674
Epoch 600, val loss: 0.6795376539230347
Epoch 610, training loss: 12.357358932495117 = 0.20506121218204498 + 2.0 * 6.076148986816406
Epoch 610, val loss: 0.6760579347610474
Epoch 620, training loss: 12.339971542358398 = 0.19351498782634735 + 2.0 * 6.073228359222412
Epoch 620, val loss: 0.6732869744300842
Epoch 630, training loss: 12.322839736938477 = 0.1827104240655899 + 2.0 * 6.070064544677734
Epoch 630, val loss: 0.6711398363113403
Epoch 640, training loss: 12.307403564453125 = 0.1726766973733902 + 2.0 * 6.067363262176514
Epoch 640, val loss: 0.6696431636810303
Epoch 650, training loss: 12.296459197998047 = 0.16330848634243011 + 2.0 * 6.066575527191162
Epoch 650, val loss: 0.6687043309211731
Epoch 660, training loss: 12.293172836303711 = 0.15458017587661743 + 2.0 * 6.069296360015869
Epoch 660, val loss: 0.6682634353637695
Epoch 670, training loss: 12.279282569885254 = 0.14649556577205658 + 2.0 * 6.0663933753967285
Epoch 670, val loss: 0.6682354807853699
Epoch 680, training loss: 12.26469612121582 = 0.13901695609092712 + 2.0 * 6.062839508056641
Epoch 680, val loss: 0.6686854362487793
Epoch 690, training loss: 12.254447937011719 = 0.1320410668849945 + 2.0 * 6.061203479766846
Epoch 690, val loss: 0.6695565581321716
Epoch 700, training loss: 12.246149063110352 = 0.12549172341823578 + 2.0 * 6.060328483581543
Epoch 700, val loss: 0.6707592606544495
Epoch 710, training loss: 12.245878219604492 = 0.11939343065023422 + 2.0 * 6.063242435455322
Epoch 710, val loss: 0.6723703742027283
Epoch 720, training loss: 12.228879928588867 = 0.11377155780792236 + 2.0 * 6.057554244995117
Epoch 720, val loss: 0.6741054058074951
Epoch 730, training loss: 12.221441268920898 = 0.10852255672216415 + 2.0 * 6.056459426879883
Epoch 730, val loss: 0.6761759519577026
Epoch 740, training loss: 12.21382999420166 = 0.10357715934515 + 2.0 * 6.055126190185547
Epoch 740, val loss: 0.6786627173423767
Epoch 750, training loss: 12.206433296203613 = 0.0989159494638443 + 2.0 * 6.05375862121582
Epoch 750, val loss: 0.6812853217124939
Epoch 760, training loss: 12.206560134887695 = 0.09452392160892487 + 2.0 * 6.056017875671387
Epoch 760, val loss: 0.6841062903404236
Epoch 770, training loss: 12.210389137268066 = 0.09038512408733368 + 2.0 * 6.060001850128174
Epoch 770, val loss: 0.6871103644371033
Epoch 780, training loss: 12.192325592041016 = 0.08653289824724197 + 2.0 * 6.052896499633789
Epoch 780, val loss: 0.6901533603668213
Epoch 790, training loss: 12.183435440063477 = 0.08289886265993118 + 2.0 * 6.050268173217773
Epoch 790, val loss: 0.6934597492218018
Epoch 800, training loss: 12.19115924835205 = 0.07945283502340317 + 2.0 * 6.055853366851807
Epoch 800, val loss: 0.6969712376594543
Epoch 810, training loss: 12.182245254516602 = 0.07623452693223953 + 2.0 * 6.053005218505859
Epoch 810, val loss: 0.700570285320282
Epoch 820, training loss: 12.167570114135742 = 0.07315925508737564 + 2.0 * 6.047205448150635
Epoch 820, val loss: 0.7041636109352112
Epoch 830, training loss: 12.161785125732422 = 0.07026156783103943 + 2.0 * 6.045761585235596
Epoch 830, val loss: 0.7080028057098389
Epoch 840, training loss: 12.15880012512207 = 0.06750541180372238 + 2.0 * 6.045647144317627
Epoch 840, val loss: 0.7119858264923096
Epoch 850, training loss: 12.155306816101074 = 0.06488564610481262 + 2.0 * 6.045210361480713
Epoch 850, val loss: 0.7161325216293335
Epoch 860, training loss: 12.153006553649902 = 0.06241736188530922 + 2.0 * 6.045294761657715
Epoch 860, val loss: 0.720175564289093
Epoch 870, training loss: 12.148118019104004 = 0.060068290680646896 + 2.0 * 6.04402494430542
Epoch 870, val loss: 0.724388062953949
Epoch 880, training loss: 12.141477584838867 = 0.057835694402456284 + 2.0 * 6.041821002960205
Epoch 880, val loss: 0.7286922335624695
Epoch 890, training loss: 12.146828651428223 = 0.055707044899463654 + 2.0 * 6.045560836791992
Epoch 890, val loss: 0.7330418229103088
Epoch 900, training loss: 12.134817123413086 = 0.05370275676250458 + 2.0 * 6.040557384490967
Epoch 900, val loss: 0.737547755241394
Epoch 910, training loss: 12.12966251373291 = 0.05178222060203552 + 2.0 * 6.038939952850342
Epoch 910, val loss: 0.7419063448905945
Epoch 920, training loss: 12.126035690307617 = 0.049959294497966766 + 2.0 * 6.03803825378418
Epoch 920, val loss: 0.7464763522148132
Epoch 930, training loss: 12.138506889343262 = 0.04822360724210739 + 2.0 * 6.045141696929932
Epoch 930, val loss: 0.7511339783668518
Epoch 940, training loss: 12.126233100891113 = 0.046556148678064346 + 2.0 * 6.0398383140563965
Epoch 940, val loss: 0.7557647228240967
Epoch 950, training loss: 12.117820739746094 = 0.044980984181165695 + 2.0 * 6.036419868469238
Epoch 950, val loss: 0.7603042721748352
Epoch 960, training loss: 12.113449096679688 = 0.043472327291965485 + 2.0 * 6.0349884033203125
Epoch 960, val loss: 0.7649900913238525
Epoch 970, training loss: 12.127216339111328 = 0.042026303708553314 + 2.0 * 6.042594909667969
Epoch 970, val loss: 0.7696898579597473
Epoch 980, training loss: 12.114747047424316 = 0.040665168315172195 + 2.0 * 6.037040710449219
Epoch 980, val loss: 0.7745087146759033
Epoch 990, training loss: 12.120673179626465 = 0.03935440257191658 + 2.0 * 6.040659427642822
Epoch 990, val loss: 0.7791005373001099
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8148
Overall ASR: 0.7565
Flip ASR: 0.7244/225 nodes
The final ASR:0.70357, 0.05751, Accuracy:0.80370, 0.02400
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11588])
remove edge: torch.Size([2, 9464])
updated graph: torch.Size([2, 10496])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9889
Flip ASR: 0.9867/225 nodes
The final ASR:0.98155, 0.00603, Accuracy:0.82840, 0.00462
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.3 der2:0.2 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 18.691162109375 = 1.9435038566589355 + 2.0 * 8.373828887939453
Epoch 0, val loss: 1.938223123550415
Epoch 10, training loss: 18.679508209228516 = 1.9330381155014038 + 2.0 * 8.373234748840332
Epoch 10, val loss: 1.9283345937728882
Epoch 20, training loss: 18.65909194946289 = 1.9198567867279053 + 2.0 * 8.369617462158203
Epoch 20, val loss: 1.9154735803604126
Epoch 30, training loss: 18.59368324279785 = 1.9020217657089233 + 2.0 * 8.345830917358398
Epoch 30, val loss: 1.8978402614593506
Epoch 40, training loss: 18.260190963745117 = 1.8811137676239014 + 2.0 * 8.189538955688477
Epoch 40, val loss: 1.8781276941299438
Epoch 50, training loss: 16.688011169433594 = 1.8596538305282593 + 2.0 * 7.414178371429443
Epoch 50, val loss: 1.8579769134521484
Epoch 60, training loss: 15.893316268920898 = 1.8424503803253174 + 2.0 * 7.02543306350708
Epoch 60, val loss: 1.8435178995132446
Epoch 70, training loss: 15.328714370727539 = 1.8325679302215576 + 2.0 * 6.748073101043701
Epoch 70, val loss: 1.835113286972046
Epoch 80, training loss: 15.020797729492188 = 1.823082447052002 + 2.0 * 6.598857402801514
Epoch 80, val loss: 1.8264038562774658
Epoch 90, training loss: 14.835856437683105 = 1.814336895942688 + 2.0 * 6.5107598304748535
Epoch 90, val loss: 1.8178280591964722
Epoch 100, training loss: 14.678783416748047 = 1.8056401014328003 + 2.0 * 6.4365715980529785
Epoch 100, val loss: 1.809775471687317
Epoch 110, training loss: 14.557356834411621 = 1.7992812395095825 + 2.0 * 6.379037857055664
Epoch 110, val loss: 1.8037861585617065
Epoch 120, training loss: 14.470653533935547 = 1.793192744255066 + 2.0 * 6.338730335235596
Epoch 120, val loss: 1.7978163957595825
Epoch 130, training loss: 14.403665542602539 = 1.7861957550048828 + 2.0 * 6.308734893798828
Epoch 130, val loss: 1.79090416431427
Epoch 140, training loss: 14.344610214233398 = 1.7786716222763062 + 2.0 * 6.2829694747924805
Epoch 140, val loss: 1.7837096452713013
Epoch 150, training loss: 14.29347038269043 = 1.7709494829177856 + 2.0 * 6.261260509490967
Epoch 150, val loss: 1.7766294479370117
Epoch 160, training loss: 14.245119094848633 = 1.762580156326294 + 2.0 * 6.241269588470459
Epoch 160, val loss: 1.769200086593628
Epoch 170, training loss: 14.201864242553711 = 1.7529934644699097 + 2.0 * 6.224435329437256
Epoch 170, val loss: 1.7608742713928223
Epoch 180, training loss: 14.162969589233398 = 1.7418866157531738 + 2.0 * 6.210541248321533
Epoch 180, val loss: 1.751564383506775
Epoch 190, training loss: 14.12491512298584 = 1.7290459871292114 + 2.0 * 6.197934627532959
Epoch 190, val loss: 1.7411327362060547
Epoch 200, training loss: 14.086190223693848 = 1.7143827676773071 + 2.0 * 6.185903549194336
Epoch 200, val loss: 1.729160189628601
Epoch 210, training loss: 14.046492576599121 = 1.6972646713256836 + 2.0 * 6.174613952636719
Epoch 210, val loss: 1.715384840965271
Epoch 220, training loss: 14.006179809570312 = 1.6771459579467773 + 2.0 * 6.164516925811768
Epoch 220, val loss: 1.699317216873169
Epoch 230, training loss: 13.966765403747559 = 1.6535866260528564 + 2.0 * 6.156589508056641
Epoch 230, val loss: 1.6805493831634521
Epoch 240, training loss: 13.921667098999023 = 1.626434087753296 + 2.0 * 6.147616386413574
Epoch 240, val loss: 1.6587469577789307
Epoch 250, training loss: 13.874781608581543 = 1.5949057340621948 + 2.0 * 6.139937877655029
Epoch 250, val loss: 1.6335458755493164
Epoch 260, training loss: 13.825338363647461 = 1.5586774349212646 + 2.0 * 6.133330345153809
Epoch 260, val loss: 1.6044716835021973
Epoch 270, training loss: 13.777688026428223 = 1.5177572965621948 + 2.0 * 6.129965305328369
Epoch 270, val loss: 1.571546196937561
Epoch 280, training loss: 13.717568397521973 = 1.4730033874511719 + 2.0 * 6.1222825050354
Epoch 280, val loss: 1.5357170104980469
Epoch 290, training loss: 13.660627365112305 = 1.4253989458084106 + 2.0 * 6.117614269256592
Epoch 290, val loss: 1.4979602098464966
Epoch 300, training loss: 13.601313591003418 = 1.3757673501968384 + 2.0 * 6.1127729415893555
Epoch 300, val loss: 1.4589818716049194
Epoch 310, training loss: 13.541378021240234 = 1.325076699256897 + 2.0 * 6.108150482177734
Epoch 310, val loss: 1.4195502996444702
Epoch 320, training loss: 13.487308502197266 = 1.2743747234344482 + 2.0 * 6.106466770172119
Epoch 320, val loss: 1.3808680772781372
Epoch 330, training loss: 13.430057525634766 = 1.2257418632507324 + 2.0 * 6.1021575927734375
Epoch 330, val loss: 1.343963861465454
Epoch 340, training loss: 13.37487506866455 = 1.178318738937378 + 2.0 * 6.098278045654297
Epoch 340, val loss: 1.3086367845535278
Epoch 350, training loss: 13.320804595947266 = 1.1320216655731201 + 2.0 * 6.094391345977783
Epoch 350, val loss: 1.2744656801223755
Epoch 360, training loss: 13.27385425567627 = 1.0868017673492432 + 2.0 * 6.093526363372803
Epoch 360, val loss: 1.2413675785064697
Epoch 370, training loss: 13.225075721740723 = 1.0439083576202393 + 2.0 * 6.090583801269531
Epoch 370, val loss: 1.2099429368972778
Epoch 380, training loss: 13.174985885620117 = 1.0027965307235718 + 2.0 * 6.086094856262207
Epoch 380, val loss: 1.1805721521377563
Epoch 390, training loss: 13.130054473876953 = 0.9631467461585999 + 2.0 * 6.08345365524292
Epoch 390, val loss: 1.1525310277938843
Epoch 400, training loss: 13.087355613708496 = 0.9247953295707703 + 2.0 * 6.08128023147583
Epoch 400, val loss: 1.1257528066635132
Epoch 410, training loss: 13.049518585205078 = 0.8878016471862793 + 2.0 * 6.08085823059082
Epoch 410, val loss: 1.100393295288086
Epoch 420, training loss: 13.007080078125 = 0.852502167224884 + 2.0 * 6.07728910446167
Epoch 420, val loss: 1.07658052444458
Epoch 430, training loss: 12.972107887268066 = 0.8183456063270569 + 2.0 * 6.076880931854248
Epoch 430, val loss: 1.053794026374817
Epoch 440, training loss: 12.931623458862305 = 0.7850677967071533 + 2.0 * 6.073277950286865
Epoch 440, val loss: 1.0319551229476929
Epoch 450, training loss: 12.895692825317383 = 0.7525495886802673 + 2.0 * 6.0715718269348145
Epoch 450, val loss: 1.0107358694076538
Epoch 460, training loss: 12.86156940460205 = 0.7206705808639526 + 2.0 * 6.070449352264404
Epoch 460, val loss: 0.9901272058486938
Epoch 470, training loss: 12.836727142333984 = 0.6896177530288696 + 2.0 * 6.073554515838623
Epoch 470, val loss: 0.9703858494758606
Epoch 480, training loss: 12.795637130737305 = 0.6597136855125427 + 2.0 * 6.067961692810059
Epoch 480, val loss: 0.9515277147293091
Epoch 490, training loss: 12.761693000793457 = 0.6306091547012329 + 2.0 * 6.065541744232178
Epoch 490, val loss: 0.9334636926651001
Epoch 500, training loss: 12.73031234741211 = 0.602349579334259 + 2.0 * 6.063981533050537
Epoch 500, val loss: 0.9161664247512817
Epoch 510, training loss: 12.70751667022705 = 0.5751838088035583 + 2.0 * 6.066166400909424
Epoch 510, val loss: 0.8997494578361511
Epoch 520, training loss: 12.677435874938965 = 0.5495860576629639 + 2.0 * 6.063924789428711
Epoch 520, val loss: 0.8848310708999634
Epoch 530, training loss: 12.644979476928711 = 0.525367796421051 + 2.0 * 6.059805870056152
Epoch 530, val loss: 0.8712128400802612
Epoch 540, training loss: 12.618155479431152 = 0.5024095773696899 + 2.0 * 6.057872772216797
Epoch 540, val loss: 0.8587567210197449
Epoch 550, training loss: 12.603399276733398 = 0.4806530177593231 + 2.0 * 6.061373233795166
Epoch 550, val loss: 0.8474370837211609
Epoch 560, training loss: 12.577262878417969 = 0.46012991666793823 + 2.0 * 6.058566570281982
Epoch 560, val loss: 0.8374455571174622
Epoch 570, training loss: 12.551624298095703 = 0.4408513903617859 + 2.0 * 6.055386543273926
Epoch 570, val loss: 0.8287160992622375
Epoch 580, training loss: 12.528558731079102 = 0.4225262701511383 + 2.0 * 6.053016185760498
Epoch 580, val loss: 0.8209487795829773
Epoch 590, training loss: 12.50811767578125 = 0.4049670398235321 + 2.0 * 6.051575183868408
Epoch 590, val loss: 0.8139892816543579
Epoch 600, training loss: 12.494473457336426 = 0.38819238543510437 + 2.0 * 6.053140640258789
Epoch 600, val loss: 0.8077104091644287
Epoch 610, training loss: 12.470952987670898 = 0.37224331498146057 + 2.0 * 6.0493550300598145
Epoch 610, val loss: 0.8024353384971619
Epoch 620, training loss: 12.452505111694336 = 0.35691335797309875 + 2.0 * 6.04779577255249
Epoch 620, val loss: 0.797713041305542
Epoch 630, training loss: 12.439766883850098 = 0.34211063385009766 + 2.0 * 6.048828125
Epoch 630, val loss: 0.7935258746147156
Epoch 640, training loss: 12.422476768493652 = 0.3279992938041687 + 2.0 * 6.047238826751709
Epoch 640, val loss: 0.789873480796814
Epoch 650, training loss: 12.404718399047852 = 0.314426064491272 + 2.0 * 6.0451459884643555
Epoch 650, val loss: 0.7870010733604431
Epoch 660, training loss: 12.388248443603516 = 0.30140307545661926 + 2.0 * 6.043422698974609
Epoch 660, val loss: 0.7844668626785278
Epoch 670, training loss: 12.39119815826416 = 0.2888861894607544 + 2.0 * 6.051156044006348
Epoch 670, val loss: 0.7824302315711975
Epoch 680, training loss: 12.361368179321289 = 0.27695512771606445 + 2.0 * 6.042206764221191
Epoch 680, val loss: 0.7808969616889954
Epoch 690, training loss: 12.349117279052734 = 0.2655574083328247 + 2.0 * 6.0417799949646
Epoch 690, val loss: 0.7800345420837402
Epoch 700, training loss: 12.352240562438965 = 0.2546195387840271 + 2.0 * 6.0488104820251465
Epoch 700, val loss: 0.7792468070983887
Epoch 710, training loss: 12.32501220703125 = 0.2441391944885254 + 2.0 * 6.040436267852783
Epoch 710, val loss: 0.7790040969848633
Epoch 720, training loss: 12.310351371765137 = 0.23405656218528748 + 2.0 * 6.038147449493408
Epoch 720, val loss: 0.7792823314666748
Epoch 730, training loss: 12.298556327819824 = 0.22426456212997437 + 2.0 * 6.037146091461182
Epoch 730, val loss: 0.7796589732170105
Epoch 740, training loss: 12.29832649230957 = 0.21475432813167572 + 2.0 * 6.041786193847656
Epoch 740, val loss: 0.7803207635879517
Epoch 750, training loss: 12.284741401672363 = 0.20555846393108368 + 2.0 * 6.039591312408447
Epoch 750, val loss: 0.7810914516448975
Epoch 760, training loss: 12.269067764282227 = 0.19665302336215973 + 2.0 * 6.03620719909668
Epoch 760, val loss: 0.7823205590248108
Epoch 770, training loss: 12.25642204284668 = 0.18796303868293762 + 2.0 * 6.034229278564453
Epoch 770, val loss: 0.7836142778396606
Epoch 780, training loss: 12.250750541687012 = 0.17945820093154907 + 2.0 * 6.035645961761475
Epoch 780, val loss: 0.7851468920707703
Epoch 790, training loss: 12.245392799377441 = 0.17122256755828857 + 2.0 * 6.037085056304932
Epoch 790, val loss: 0.7865409851074219
Epoch 800, training loss: 12.22778606414795 = 0.16326932609081268 + 2.0 * 6.0322585105896
Epoch 800, val loss: 0.7884150743484497
Epoch 810, training loss: 12.218729019165039 = 0.15555566549301147 + 2.0 * 6.031586647033691
Epoch 810, val loss: 0.7902310490608215
Epoch 820, training loss: 12.211063385009766 = 0.14809304475784302 + 2.0 * 6.031485080718994
Epoch 820, val loss: 0.7921940684318542
Epoch 830, training loss: 12.201048851013184 = 0.1409323662519455 + 2.0 * 6.03005838394165
Epoch 830, val loss: 0.7942389845848083
Epoch 840, training loss: 12.194785118103027 = 0.13412195444107056 + 2.0 * 6.030331611633301
Epoch 840, val loss: 0.7968801259994507
Epoch 850, training loss: 12.184550285339355 = 0.1276293247938156 + 2.0 * 6.028460502624512
Epoch 850, val loss: 0.799481987953186
Epoch 860, training loss: 12.17768383026123 = 0.121473029255867 + 2.0 * 6.02810525894165
Epoch 860, val loss: 0.8024247288703918
Epoch 870, training loss: 12.182290077209473 = 0.11567147076129913 + 2.0 * 6.033309459686279
Epoch 870, val loss: 0.8054046630859375
Epoch 880, training loss: 12.167654037475586 = 0.11032197624444962 + 2.0 * 6.028666019439697
Epoch 880, val loss: 0.808853805065155
Epoch 890, training loss: 12.157426834106445 = 0.10529674589633942 + 2.0 * 6.026064872741699
Epoch 890, val loss: 0.812577486038208
Epoch 900, training loss: 12.150447845458984 = 0.10057486593723297 + 2.0 * 6.024936676025391
Epoch 900, val loss: 0.8162106275558472
Epoch 910, training loss: 12.144845962524414 = 0.09613581001758575 + 2.0 * 6.024354934692383
Epoch 910, val loss: 0.8202348947525024
Epoch 920, training loss: 12.143782615661621 = 0.09196822345256805 + 2.0 * 6.025907039642334
Epoch 920, val loss: 0.8243957757949829
Epoch 930, training loss: 12.136320114135742 = 0.08806098997592926 + 2.0 * 6.024129390716553
Epoch 930, val loss: 0.8286008834838867
Epoch 940, training loss: 12.137792587280273 = 0.0844268798828125 + 2.0 * 6.0266828536987305
Epoch 940, val loss: 0.8329896330833435
Epoch 950, training loss: 12.1262845993042 = 0.08101289719343185 + 2.0 * 6.0226359367370605
Epoch 950, val loss: 0.8375319242477417
Epoch 960, training loss: 12.120964050292969 = 0.07779748737812042 + 2.0 * 6.021583080291748
Epoch 960, val loss: 0.8420228958129883
Epoch 970, training loss: 12.130636215209961 = 0.07476098090410233 + 2.0 * 6.027937412261963
Epoch 970, val loss: 0.8465451598167419
Epoch 980, training loss: 12.115800857543945 = 0.07193615287542343 + 2.0 * 6.021932125091553
Epoch 980, val loss: 0.8512393236160278
Epoch 990, training loss: 12.109404563903809 = 0.06926511228084564 + 2.0 * 6.020069599151611
Epoch 990, val loss: 0.8560546040534973
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7741
Overall ASR: 0.7159
Flip ASR: 0.6578/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.700939178466797 = 1.9533004760742188 + 2.0 * 8.373819351196289
Epoch 0, val loss: 1.944960117340088
Epoch 10, training loss: 18.68880271911621 = 1.9428563117980957 + 2.0 * 8.372973442077637
Epoch 10, val loss: 1.9340333938598633
Epoch 20, training loss: 18.666107177734375 = 1.9302700757980347 + 2.0 * 8.367918968200684
Epoch 20, val loss: 1.9202169179916382
Epoch 30, training loss: 18.587186813354492 = 1.9136812686920166 + 2.0 * 8.336752891540527
Epoch 30, val loss: 1.901666522026062
Epoch 40, training loss: 18.08255386352539 = 1.8944941759109497 + 2.0 * 8.094029426574707
Epoch 40, val loss: 1.880749225616455
Epoch 50, training loss: 16.371849060058594 = 1.8752535581588745 + 2.0 * 7.248297691345215
Epoch 50, val loss: 1.8598530292510986
Epoch 60, training loss: 15.623452186584473 = 1.8586512804031372 + 2.0 * 6.8824005126953125
Epoch 60, val loss: 1.8421882390975952
Epoch 70, training loss: 15.183116912841797 = 1.8447766304016113 + 2.0 * 6.669169902801514
Epoch 70, val loss: 1.8278898000717163
Epoch 80, training loss: 14.920875549316406 = 1.8333301544189453 + 2.0 * 6.5437726974487305
Epoch 80, val loss: 1.816177248954773
Epoch 90, training loss: 14.743817329406738 = 1.8222980499267578 + 2.0 * 6.46075963973999
Epoch 90, val loss: 1.8046187162399292
Epoch 100, training loss: 14.620254516601562 = 1.811161756515503 + 2.0 * 6.40454626083374
Epoch 100, val loss: 1.7932772636413574
Epoch 110, training loss: 14.523601531982422 = 1.8004781007766724 + 2.0 * 6.3615617752075195
Epoch 110, val loss: 1.7826757431030273
Epoch 120, training loss: 14.438498497009277 = 1.7907648086547852 + 2.0 * 6.323866844177246
Epoch 120, val loss: 1.7732270956039429
Epoch 130, training loss: 14.369414329528809 = 1.7819271087646484 + 2.0 * 6.29374361038208
Epoch 130, val loss: 1.7646498680114746
Epoch 140, training loss: 14.302960395812988 = 1.773606300354004 + 2.0 * 6.264677047729492
Epoch 140, val loss: 1.7567203044891357
Epoch 150, training loss: 14.246906280517578 = 1.7654109001159668 + 2.0 * 6.240747928619385
Epoch 150, val loss: 1.7491672039031982
Epoch 160, training loss: 14.194740295410156 = 1.7568880319595337 + 2.0 * 6.218925952911377
Epoch 160, val loss: 1.7417020797729492
Epoch 170, training loss: 14.149593353271484 = 1.7476658821105957 + 2.0 * 6.200963973999023
Epoch 170, val loss: 1.7340519428253174
Epoch 180, training loss: 14.108800888061523 = 1.7374778985977173 + 2.0 * 6.185661315917969
Epoch 180, val loss: 1.725935697555542
Epoch 190, training loss: 14.0684814453125 = 1.726141333580017 + 2.0 * 6.171170234680176
Epoch 190, val loss: 1.7170530557632446
Epoch 200, training loss: 14.039423942565918 = 1.7130651473999023 + 2.0 * 6.163179397583008
Epoch 200, val loss: 1.707043170928955
Epoch 210, training loss: 13.999004364013672 = 1.6979873180389404 + 2.0 * 6.150508403778076
Epoch 210, val loss: 1.6956284046173096
Epoch 220, training loss: 13.961909294128418 = 1.6804202795028687 + 2.0 * 6.140744686126709
Epoch 220, val loss: 1.6824058294296265
Epoch 230, training loss: 13.925409317016602 = 1.6596509218215942 + 2.0 * 6.132879257202148
Epoch 230, val loss: 1.6668007373809814
Epoch 240, training loss: 13.892460823059082 = 1.6351747512817383 + 2.0 * 6.128643035888672
Epoch 240, val loss: 1.6484125852584839
Epoch 250, training loss: 13.848225593566895 = 1.607026219367981 + 2.0 * 6.120599746704102
Epoch 250, val loss: 1.6271201372146606
Epoch 260, training loss: 13.805709838867188 = 1.5746153593063354 + 2.0 * 6.115547180175781
Epoch 260, val loss: 1.6024901866912842
Epoch 270, training loss: 13.761474609375 = 1.5374703407287598 + 2.0 * 6.112002372741699
Epoch 270, val loss: 1.5740416049957275
Epoch 280, training loss: 13.711029052734375 = 1.4959872961044312 + 2.0 * 6.107521057128906
Epoch 280, val loss: 1.5426757335662842
Epoch 290, training loss: 13.657516479492188 = 1.4516308307647705 + 2.0 * 6.102942943572998
Epoch 290, val loss: 1.508851170539856
Epoch 300, training loss: 13.60843276977539 = 1.4052739143371582 + 2.0 * 6.101579666137695
Epoch 300, val loss: 1.4737011194229126
Epoch 310, training loss: 13.550955772399902 = 1.3588130474090576 + 2.0 * 6.096071243286133
Epoch 310, val loss: 1.4387834072113037
Epoch 320, training loss: 13.498292922973633 = 1.3132548332214355 + 2.0 * 6.092519283294678
Epoch 320, val loss: 1.4049465656280518
Epoch 330, training loss: 13.45316219329834 = 1.269659161567688 + 2.0 * 6.091751575469971
Epoch 330, val loss: 1.3729270696640015
Epoch 340, training loss: 13.401376724243164 = 1.2287179231643677 + 2.0 * 6.086329460144043
Epoch 340, val loss: 1.3435248136520386
Epoch 350, training loss: 13.35721206665039 = 1.1898486614227295 + 2.0 * 6.083681583404541
Epoch 350, val loss: 1.3160701990127563
Epoch 360, training loss: 13.315032958984375 = 1.1527217626571655 + 2.0 * 6.081155776977539
Epoch 360, val loss: 1.290083885192871
Epoch 370, training loss: 13.275158882141113 = 1.1169192790985107 + 2.0 * 6.079119682312012
Epoch 370, val loss: 1.2652140855789185
Epoch 380, training loss: 13.233786582946777 = 1.0816097259521484 + 2.0 * 6.0760884284973145
Epoch 380, val loss: 1.240693211555481
Epoch 390, training loss: 13.202054977416992 = 1.0461581945419312 + 2.0 * 6.077948570251465
Epoch 390, val loss: 1.2159526348114014
Epoch 400, training loss: 13.156216621398926 = 1.0106314420700073 + 2.0 * 6.0727925300598145
Epoch 400, val loss: 1.1907998323440552
Epoch 410, training loss: 13.116914749145508 = 0.9748172760009766 + 2.0 * 6.071048736572266
Epoch 410, val loss: 1.1652114391326904
Epoch 420, training loss: 13.074941635131836 = 0.938846230506897 + 2.0 * 6.068047523498535
Epoch 420, val loss: 1.139003872871399
Epoch 430, training loss: 13.036684036254883 = 0.9028810262680054 + 2.0 * 6.066901683807373
Epoch 430, val loss: 1.112494945526123
Epoch 440, training loss: 13.000589370727539 = 0.8670920133590698 + 2.0 * 6.06674861907959
Epoch 440, val loss: 1.085934042930603
Epoch 450, training loss: 12.961685180664062 = 0.8321218490600586 + 2.0 * 6.064781665802002
Epoch 450, val loss: 1.059600830078125
Epoch 460, training loss: 12.921043395996094 = 0.7979633212089539 + 2.0 * 6.061540126800537
Epoch 460, val loss: 1.0339068174362183
Epoch 470, training loss: 12.88505744934082 = 0.7648051977157593 + 2.0 * 6.060126304626465
Epoch 470, val loss: 1.0089035034179688
Epoch 480, training loss: 12.857845306396484 = 0.7327179312705994 + 2.0 * 6.062563896179199
Epoch 480, val loss: 0.9846885204315186
Epoch 490, training loss: 12.82079792022705 = 0.7020505666732788 + 2.0 * 6.05937385559082
Epoch 490, val loss: 0.9616312384605408
Epoch 500, training loss: 12.785473823547363 = 0.6726011633872986 + 2.0 * 6.056436538696289
Epoch 500, val loss: 0.9400429725646973
Epoch 510, training loss: 12.75132942199707 = 0.6441839933395386 + 2.0 * 6.053572654724121
Epoch 510, val loss: 0.9196122884750366
Epoch 520, training loss: 12.731114387512207 = 0.6166695356369019 + 2.0 * 6.057222366333008
Epoch 520, val loss: 0.9002537131309509
Epoch 530, training loss: 12.716353416442871 = 0.5902746915817261 + 2.0 * 6.063039302825928
Epoch 530, val loss: 0.8821462988853455
Epoch 540, training loss: 12.669971466064453 = 0.565078854560852 + 2.0 * 6.052446365356445
Epoch 540, val loss: 0.8655059933662415
Epoch 550, training loss: 12.639145851135254 = 0.5408573746681213 + 2.0 * 6.049144268035889
Epoch 550, val loss: 0.8501185774803162
Epoch 560, training loss: 12.610846519470215 = 0.5173580646514893 + 2.0 * 6.046744346618652
Epoch 560, val loss: 0.8357205390930176
Epoch 570, training loss: 12.597756385803223 = 0.49458593130111694 + 2.0 * 6.0515851974487305
Epoch 570, val loss: 0.8223925828933716
Epoch 580, training loss: 12.564421653747559 = 0.4727858603000641 + 2.0 * 6.045817852020264
Epoch 580, val loss: 0.8101373910903931
Epoch 590, training loss: 12.53745174407959 = 0.4518333971500397 + 2.0 * 6.042809009552002
Epoch 590, val loss: 0.7991026043891907
Epoch 600, training loss: 12.520339965820312 = 0.4316561222076416 + 2.0 * 6.044342041015625
Epoch 600, val loss: 0.789057731628418
Epoch 610, training loss: 12.510746955871582 = 0.4124564826488495 + 2.0 * 6.049145221710205
Epoch 610, val loss: 0.7801234722137451
Epoch 620, training loss: 12.476775169372559 = 0.39428970217704773 + 2.0 * 6.041242599487305
Epoch 620, val loss: 0.7722898125648499
Epoch 630, training loss: 12.45433521270752 = 0.37706243991851807 + 2.0 * 6.038636207580566
Epoch 630, val loss: 0.7654209733009338
Epoch 640, training loss: 12.434972763061523 = 0.36055949330329895 + 2.0 * 6.037206649780273
Epoch 640, val loss: 0.7595353722572327
Epoch 650, training loss: 12.44638729095459 = 0.3448914885520935 + 2.0 * 6.050747871398926
Epoch 650, val loss: 0.7544147968292236
Epoch 660, training loss: 12.410372734069824 = 0.33012112975120544 + 2.0 * 6.040125846862793
Epoch 660, val loss: 0.750195324420929
Epoch 670, training loss: 12.386276245117188 = 0.31620344519615173 + 2.0 * 6.035036563873291
Epoch 670, val loss: 0.7467633485794067
Epoch 680, training loss: 12.369597434997559 = 0.3029140830039978 + 2.0 * 6.033341884613037
Epoch 680, val loss: 0.7440793514251709
Epoch 690, training loss: 12.381060600280762 = 0.29028141498565674 + 2.0 * 6.045389652252197
Epoch 690, val loss: 0.7419600486755371
Epoch 700, training loss: 12.35148811340332 = 0.27844858169555664 + 2.0 * 6.036520004272461
Epoch 700, val loss: 0.7404823303222656
Epoch 710, training loss: 12.33159351348877 = 0.2672072947025299 + 2.0 * 6.032193183898926
Epoch 710, val loss: 0.7397364377975464
Epoch 720, training loss: 12.316614151000977 = 0.25645962357521057 + 2.0 * 6.0300774574279785
Epoch 720, val loss: 0.7395784258842468
Epoch 730, training loss: 12.328641891479492 = 0.24620257318019867 + 2.0 * 6.041219711303711
Epoch 730, val loss: 0.7397900223731995
Epoch 740, training loss: 12.292403221130371 = 0.23644882440567017 + 2.0 * 6.027976989746094
Epoch 740, val loss: 0.7405616044998169
Epoch 750, training loss: 12.283098220825195 = 0.22714805603027344 + 2.0 * 6.027975082397461
Epoch 750, val loss: 0.7418971657752991
Epoch 760, training loss: 12.276838302612305 = 0.21821832656860352 + 2.0 * 6.0293097496032715
Epoch 760, val loss: 0.7436606287956238
Epoch 770, training loss: 12.271739959716797 = 0.20979437232017517 + 2.0 * 6.030972957611084
Epoch 770, val loss: 0.7455945611000061
Epoch 780, training loss: 12.251591682434082 = 0.20171765983104706 + 2.0 * 6.024937152862549
Epoch 780, val loss: 0.7480676770210266
Epoch 790, training loss: 12.24290943145752 = 0.19398058950901031 + 2.0 * 6.0244646072387695
Epoch 790, val loss: 0.7509124875068665
Epoch 800, training loss: 12.23298168182373 = 0.18653994798660278 + 2.0 * 6.023221015930176
Epoch 800, val loss: 0.754054069519043
Epoch 810, training loss: 12.243366241455078 = 0.1793835610151291 + 2.0 * 6.031991481781006
Epoch 810, val loss: 0.7574540376663208
Epoch 820, training loss: 12.221613883972168 = 0.17257413268089294 + 2.0 * 6.024519920349121
Epoch 820, val loss: 0.7610108256340027
Epoch 830, training loss: 12.21144962310791 = 0.16608309745788574 + 2.0 * 6.022683143615723
Epoch 830, val loss: 0.7649165391921997
Epoch 840, training loss: 12.202150344848633 = 0.1598329395055771 + 2.0 * 6.021158695220947
Epoch 840, val loss: 0.7690969109535217
Epoch 850, training loss: 12.195452690124512 = 0.15379376709461212 + 2.0 * 6.020829677581787
Epoch 850, val loss: 0.7734352946281433
Epoch 860, training loss: 12.204270362854004 = 0.1479886770248413 + 2.0 * 6.028141021728516
Epoch 860, val loss: 0.7777947187423706
Epoch 870, training loss: 12.188676834106445 = 0.142490953207016 + 2.0 * 6.023092746734619
Epoch 870, val loss: 0.7823637127876282
Epoch 880, training loss: 12.176733016967773 = 0.1372271478176117 + 2.0 * 6.0197529792785645
Epoch 880, val loss: 0.787193775177002
Epoch 890, training loss: 12.168253898620605 = 0.13213498890399933 + 2.0 * 6.018059253692627
Epoch 890, val loss: 0.7920613288879395
Epoch 900, training loss: 12.160904884338379 = 0.12720590829849243 + 2.0 * 6.016849517822266
Epoch 900, val loss: 0.7971057891845703
Epoch 910, training loss: 12.157577514648438 = 0.12244560569524765 + 2.0 * 6.017565727233887
Epoch 910, val loss: 0.8023459911346436
Epoch 920, training loss: 12.152185440063477 = 0.11787992715835571 + 2.0 * 6.017152786254883
Epoch 920, val loss: 0.8075582981109619
Epoch 930, training loss: 12.157150268554688 = 0.11354508250951767 + 2.0 * 6.0218024253845215
Epoch 930, val loss: 0.8129989504814148
Epoch 940, training loss: 12.142040252685547 = 0.10946660488843918 + 2.0 * 6.016286849975586
Epoch 940, val loss: 0.818473219871521
Epoch 950, training loss: 12.136834144592285 = 0.1055675745010376 + 2.0 * 6.0156331062316895
Epoch 950, val loss: 0.8240863680839539
Epoch 960, training loss: 12.129985809326172 = 0.10185782611370087 + 2.0 * 6.014063835144043
Epoch 960, val loss: 0.8298335075378418
Epoch 970, training loss: 12.126431465148926 = 0.09829683601856232 + 2.0 * 6.01406717300415
Epoch 970, val loss: 0.8355963826179504
Epoch 980, training loss: 12.130931854248047 = 0.09489943087100983 + 2.0 * 6.018016338348389
Epoch 980, val loss: 0.8412957191467285
Epoch 990, training loss: 12.117279052734375 = 0.09166711568832397 + 2.0 * 6.012805938720703
Epoch 990, val loss: 0.8469757437705994
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8111
Overall ASR: 0.6605
Flip ASR: 0.6178/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.685184478759766 = 1.937542200088501 + 2.0 * 8.373821258544922
Epoch 0, val loss: 1.9389612674713135
Epoch 10, training loss: 18.673402786254883 = 1.9270381927490234 + 2.0 * 8.37318229675293
Epoch 10, val loss: 1.9284577369689941
Epoch 20, training loss: 18.652009963989258 = 1.9138849973678589 + 2.0 * 8.369062423706055
Epoch 20, val loss: 1.9150539636611938
Epoch 30, training loss: 18.575822830200195 = 1.8964170217514038 + 2.0 * 8.339702606201172
Epoch 30, val loss: 1.8971744775772095
Epoch 40, training loss: 18.086002349853516 = 1.8761109113693237 + 2.0 * 8.10494613647461
Epoch 40, val loss: 1.8769735097885132
Epoch 50, training loss: 16.487966537475586 = 1.8547523021697998 + 2.0 * 7.316607475280762
Epoch 50, val loss: 1.8559194803237915
Epoch 60, training loss: 15.76224136352539 = 1.840302586555481 + 2.0 * 6.9609694480896
Epoch 60, val loss: 1.8422515392303467
Epoch 70, training loss: 15.29811954498291 = 1.8310047388076782 + 2.0 * 6.733557224273682
Epoch 70, val loss: 1.8328063488006592
Epoch 80, training loss: 15.036820411682129 = 1.8220468759536743 + 2.0 * 6.607386589050293
Epoch 80, val loss: 1.8229366540908813
Epoch 90, training loss: 14.86351203918457 = 1.8134756088256836 + 2.0 * 6.525018215179443
Epoch 90, val loss: 1.813531517982483
Epoch 100, training loss: 14.706883430480957 = 1.8060671091079712 + 2.0 * 6.450407981872559
Epoch 100, val loss: 1.8056704998016357
Epoch 110, training loss: 14.562298774719238 = 1.8003188371658325 + 2.0 * 6.380990028381348
Epoch 110, val loss: 1.799475073814392
Epoch 120, training loss: 14.44735336303711 = 1.7947593927383423 + 2.0 * 6.326296806335449
Epoch 120, val loss: 1.7934904098510742
Epoch 130, training loss: 14.362947463989258 = 1.7885791063308716 + 2.0 * 6.287184238433838
Epoch 130, val loss: 1.787070631980896
Epoch 140, training loss: 14.297094345092773 = 1.7819163799285889 + 2.0 * 6.257588863372803
Epoch 140, val loss: 1.7801965475082397
Epoch 150, training loss: 14.242608070373535 = 1.7747374773025513 + 2.0 * 6.233935356140137
Epoch 150, val loss: 1.7731486558914185
Epoch 160, training loss: 14.195098876953125 = 1.7668694257736206 + 2.0 * 6.214114665985107
Epoch 160, val loss: 1.7657992839813232
Epoch 170, training loss: 14.15088939666748 = 1.7581359148025513 + 2.0 * 6.196376800537109
Epoch 170, val loss: 1.7579302787780762
Epoch 180, training loss: 14.111610412597656 = 1.7483439445495605 + 2.0 * 6.181633472442627
Epoch 180, val loss: 1.7494035959243774
Epoch 190, training loss: 14.075787544250488 = 1.7371559143066406 + 2.0 * 6.169315814971924
Epoch 190, val loss: 1.7400110960006714
Epoch 200, training loss: 14.040933609008789 = 1.7243952751159668 + 2.0 * 6.15826940536499
Epoch 200, val loss: 1.7294371128082275
Epoch 210, training loss: 14.008475303649902 = 1.7096271514892578 + 2.0 * 6.149424076080322
Epoch 210, val loss: 1.7173937559127808
Epoch 220, training loss: 13.975318908691406 = 1.6925606727600098 + 2.0 * 6.141379356384277
Epoch 220, val loss: 1.7035605907440186
Epoch 230, training loss: 13.940133094787598 = 1.67280113697052 + 2.0 * 6.133666038513184
Epoch 230, val loss: 1.6877176761627197
Epoch 240, training loss: 13.904010772705078 = 1.6498124599456787 + 2.0 * 6.12709903717041
Epoch 240, val loss: 1.6692602634429932
Epoch 250, training loss: 13.871827125549316 = 1.6229248046875 + 2.0 * 6.124451160430908
Epoch 250, val loss: 1.647796869277954
Epoch 260, training loss: 13.826683044433594 = 1.5919756889343262 + 2.0 * 6.117353916168213
Epoch 260, val loss: 1.6229047775268555
Epoch 270, training loss: 13.780572891235352 = 1.5563058853149414 + 2.0 * 6.112133502960205
Epoch 270, val loss: 1.5941598415374756
Epoch 280, training loss: 13.73263931274414 = 1.5155613422393799 + 2.0 * 6.10853910446167
Epoch 280, val loss: 1.5611858367919922
Epoch 290, training loss: 13.67924690246582 = 1.4700690507888794 + 2.0 * 6.104588985443115
Epoch 290, val loss: 1.5242587327957153
Epoch 300, training loss: 13.621428489685059 = 1.4200359582901 + 2.0 * 6.100696086883545
Epoch 300, val loss: 1.4834142923355103
Epoch 310, training loss: 13.561283111572266 = 1.3658523559570312 + 2.0 * 6.097715377807617
Epoch 310, val loss: 1.438926100730896
Epoch 320, training loss: 13.502917289733887 = 1.3086036443710327 + 2.0 * 6.097157001495361
Epoch 320, val loss: 1.3917781114578247
Epoch 330, training loss: 13.43693733215332 = 1.2501415014266968 + 2.0 * 6.093398094177246
Epoch 330, val loss: 1.3434712886810303
Epoch 340, training loss: 13.373019218444824 = 1.1915855407714844 + 2.0 * 6.09071683883667
Epoch 340, val loss: 1.2952063083648682
Epoch 350, training loss: 13.31023120880127 = 1.1345977783203125 + 2.0 * 6.0878167152404785
Epoch 350, val loss: 1.2481032609939575
Epoch 360, training loss: 13.249422073364258 = 1.0793884992599487 + 2.0 * 6.08501672744751
Epoch 360, val loss: 1.2026716470718384
Epoch 370, training loss: 13.192537307739258 = 1.0262385606765747 + 2.0 * 6.083149433135986
Epoch 370, val loss: 1.1591883897781372
Epoch 380, training loss: 13.141847610473633 = 0.9754040837287903 + 2.0 * 6.083221912384033
Epoch 380, val loss: 1.1179780960083008
Epoch 390, training loss: 13.082828521728516 = 0.927388608455658 + 2.0 * 6.0777201652526855
Epoch 390, val loss: 1.0792371034622192
Epoch 400, training loss: 13.031408309936523 = 0.8815411925315857 + 2.0 * 6.0749335289001465
Epoch 400, val loss: 1.0425724983215332
Epoch 410, training loss: 12.99104118347168 = 0.8375656604766846 + 2.0 * 6.076737880706787
Epoch 410, val loss: 1.0077893733978271
Epoch 420, training loss: 12.941207885742188 = 0.7960464358329773 + 2.0 * 6.072580814361572
Epoch 420, val loss: 0.975392758846283
Epoch 430, training loss: 12.896586418151855 = 0.7569445371627808 + 2.0 * 6.069820880889893
Epoch 430, val loss: 0.9452937245368958
Epoch 440, training loss: 12.853315353393555 = 0.7196236848831177 + 2.0 * 6.066845893859863
Epoch 440, val loss: 0.9168921113014221
Epoch 450, training loss: 12.813197135925293 = 0.6839527487754822 + 2.0 * 6.064622402191162
Epoch 450, val loss: 0.8901435136795044
Epoch 460, training loss: 12.784870147705078 = 0.6499999165534973 + 2.0 * 6.067435264587402
Epoch 460, val loss: 0.8651827573776245
Epoch 470, training loss: 12.742314338684082 = 0.6182109117507935 + 2.0 * 6.062051773071289
Epoch 470, val loss: 0.8422731757164001
Epoch 480, training loss: 12.707524299621582 = 0.5880639553070068 + 2.0 * 6.059730052947998
Epoch 480, val loss: 0.8210551142692566
Epoch 490, training loss: 12.692108154296875 = 0.5592326521873474 + 2.0 * 6.066437721252441
Epoch 490, val loss: 0.8012511134147644
Epoch 500, training loss: 12.6450777053833 = 0.531774640083313 + 2.0 * 6.056651592254639
Epoch 500, val loss: 0.7829662561416626
Epoch 510, training loss: 12.616716384887695 = 0.5053977966308594 + 2.0 * 6.055659294128418
Epoch 510, val loss: 0.765937328338623
Epoch 520, training loss: 12.5914888381958 = 0.47981375455856323 + 2.0 * 6.055837631225586
Epoch 520, val loss: 0.7498475313186646
Epoch 530, training loss: 12.566052436828613 = 0.4550829529762268 + 2.0 * 6.055484771728516
Epoch 530, val loss: 0.7348012924194336
Epoch 540, training loss: 12.533949851989746 = 0.431161105632782 + 2.0 * 6.051394462585449
Epoch 540, val loss: 0.7205948829650879
Epoch 550, training loss: 12.509899139404297 = 0.40781763195991516 + 2.0 * 6.0510406494140625
Epoch 550, val loss: 0.7071712613105774
Epoch 560, training loss: 12.485506057739258 = 0.3851483464241028 + 2.0 * 6.0501790046691895
Epoch 560, val loss: 0.6944745779037476
Epoch 570, training loss: 12.46987247467041 = 0.3632391691207886 + 2.0 * 6.053316593170166
Epoch 570, val loss: 0.6826403141021729
Epoch 580, training loss: 12.436100006103516 = 0.34226569533348083 + 2.0 * 6.046916961669922
Epoch 580, val loss: 0.6717060804367065
Epoch 590, training loss: 12.413677215576172 = 0.3220377266407013 + 2.0 * 6.0458197593688965
Epoch 590, val loss: 0.661697268486023
Epoch 600, training loss: 12.398836135864258 = 0.30263596773147583 + 2.0 * 6.048099994659424
Epoch 600, val loss: 0.652513861656189
Epoch 610, training loss: 12.375105857849121 = 0.2841455936431885 + 2.0 * 6.045480251312256
Epoch 610, val loss: 0.6442796587944031
Epoch 620, training loss: 12.351533889770508 = 0.2666226327419281 + 2.0 * 6.042455673217773
Epoch 620, val loss: 0.6369943022727966
Epoch 630, training loss: 12.356066703796387 = 0.2499949187040329 + 2.0 * 6.053035736083984
Epoch 630, val loss: 0.6305552124977112
Epoch 640, training loss: 12.316214561462402 = 0.23439434170722961 + 2.0 * 6.040910243988037
Epoch 640, val loss: 0.6251591444015503
Epoch 650, training loss: 12.298243522644043 = 0.21977411210536957 + 2.0 * 6.039234638214111
Epoch 650, val loss: 0.6205593347549438
Epoch 660, training loss: 12.285225868225098 = 0.2060069739818573 + 2.0 * 6.039609432220459
Epoch 660, val loss: 0.6167913675308228
Epoch 670, training loss: 12.270833969116211 = 0.1932060867547989 + 2.0 * 6.038814067840576
Epoch 670, val loss: 0.6137953400611877
Epoch 680, training loss: 12.254497528076172 = 0.18135367333889008 + 2.0 * 6.036571979522705
Epoch 680, val loss: 0.6115204691886902
Epoch 690, training loss: 12.24194622039795 = 0.17030830681324005 + 2.0 * 6.035819053649902
Epoch 690, val loss: 0.6099740862846375
Epoch 700, training loss: 12.228585243225098 = 0.16000445187091827 + 2.0 * 6.034290313720703
Epoch 700, val loss: 0.6090340614318848
Epoch 710, training loss: 12.224047660827637 = 0.15046373009681702 + 2.0 * 6.036791801452637
Epoch 710, val loss: 0.6086863279342651
Epoch 720, training loss: 12.2108793258667 = 0.14172597229480743 + 2.0 * 6.034576892852783
Epoch 720, val loss: 0.6088171005249023
Epoch 730, training loss: 12.204314231872559 = 0.13365525007247925 + 2.0 * 6.035329341888428
Epoch 730, val loss: 0.6094547510147095
Epoch 740, training loss: 12.189983367919922 = 0.1262107491493225 + 2.0 * 6.031886100769043
Epoch 740, val loss: 0.610663652420044
Epoch 750, training loss: 12.180798530578613 = 0.11933483928442001 + 2.0 * 6.030731678009033
Epoch 750, val loss: 0.6121863722801208
Epoch 760, training loss: 12.171900749206543 = 0.11294659972190857 + 2.0 * 6.029477119445801
Epoch 760, val loss: 0.6141306757926941
Epoch 770, training loss: 12.182027816772461 = 0.10703126341104507 + 2.0 * 6.037498474121094
Epoch 770, val loss: 0.6164306402206421
Epoch 780, training loss: 12.156977653503418 = 0.10162714123725891 + 2.0 * 6.027675151824951
Epoch 780, val loss: 0.6189677119255066
Epoch 790, training loss: 12.152457237243652 = 0.09660931676626205 + 2.0 * 6.027924060821533
Epoch 790, val loss: 0.6216399669647217
Epoch 800, training loss: 12.143143653869629 = 0.09191892296075821 + 2.0 * 6.0256123542785645
Epoch 800, val loss: 0.6246696710586548
Epoch 810, training loss: 12.16461181640625 = 0.08755786716938019 + 2.0 * 6.038527011871338
Epoch 810, val loss: 0.6279328465461731
Epoch 820, training loss: 12.132731437683105 = 0.08352597057819366 + 2.0 * 6.024602890014648
Epoch 820, val loss: 0.6313137412071228
Epoch 830, training loss: 12.126571655273438 = 0.07976515591144562 + 2.0 * 6.023403167724609
Epoch 830, val loss: 0.6347619295120239
Epoch 840, training loss: 12.1220064163208 = 0.07623303681612015 + 2.0 * 6.022886753082275
Epoch 840, val loss: 0.638433575630188
Epoch 850, training loss: 12.138762474060059 = 0.07291785627603531 + 2.0 * 6.032922267913818
Epoch 850, val loss: 0.642223060131073
Epoch 860, training loss: 12.117502212524414 = 0.06983675062656403 + 2.0 * 6.02383279800415
Epoch 860, val loss: 0.6461271643638611
Epoch 870, training loss: 12.110620498657227 = 0.06696193665266037 + 2.0 * 6.021829128265381
Epoch 870, val loss: 0.6499577164649963
Epoch 880, training loss: 12.104029655456543 = 0.06424282491207123 + 2.0 * 6.019893646240234
Epoch 880, val loss: 0.6539673209190369
Epoch 890, training loss: 12.101007461547852 = 0.06166621670126915 + 2.0 * 6.019670486450195
Epoch 890, val loss: 0.6580005884170532
Epoch 900, training loss: 12.10334587097168 = 0.05924521014094353 + 2.0 * 6.022050380706787
Epoch 900, val loss: 0.662200391292572
Epoch 910, training loss: 12.098630905151367 = 0.056990623474121094 + 2.0 * 6.020820140838623
Epoch 910, val loss: 0.6661810278892517
Epoch 920, training loss: 12.089727401733398 = 0.05486036837100983 + 2.0 * 6.0174336433410645
Epoch 920, val loss: 0.6702451109886169
Epoch 930, training loss: 12.086392402648926 = 0.052833978086709976 + 2.0 * 6.01677942276001
Epoch 930, val loss: 0.6744367480278015
Epoch 940, training loss: 12.111595153808594 = 0.050914619117975235 + 2.0 * 6.030340194702148
Epoch 940, val loss: 0.6786660552024841
Epoch 950, training loss: 12.083176612854004 = 0.04910993203520775 + 2.0 * 6.017033576965332
Epoch 950, val loss: 0.682823121547699
Epoch 960, training loss: 12.081002235412598 = 0.04741092398762703 + 2.0 * 6.016795635223389
Epoch 960, val loss: 0.6867607235908508
Epoch 970, training loss: 12.080214500427246 = 0.04579431191086769 + 2.0 * 6.017210006713867
Epoch 970, val loss: 0.6908784508705139
Epoch 980, training loss: 12.072614669799805 = 0.044255297631025314 + 2.0 * 6.014179706573486
Epoch 980, val loss: 0.695056140422821
Epoch 990, training loss: 12.071913719177246 = 0.04278767481446266 + 2.0 * 6.014563083648682
Epoch 990, val loss: 0.6991340517997742
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9705
Flip ASR: 0.9644/225 nodes
The final ASR:0.78229, 0.13498, Accuracy:0.80494, 0.02310
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11606])
remove edge: torch.Size([2, 9582])
updated graph: torch.Size([2, 10632])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9705
Flip ASR: 0.9644/225 nodes
The final ASR:0.97540, 0.00348, Accuracy:0.82963, 0.00605
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.3 der2:0.2 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 18.699338912963867 = 1.9517260789871216 + 2.0 * 8.37380599975586
Epoch 0, val loss: 1.9601573944091797
Epoch 10, training loss: 18.687097549438477 = 1.940855860710144 + 2.0 * 8.37312126159668
Epoch 10, val loss: 1.949126958847046
Epoch 20, training loss: 18.66485595703125 = 1.927037239074707 + 2.0 * 8.368908882141113
Epoch 20, val loss: 1.934860348701477
Epoch 30, training loss: 18.589590072631836 = 1.9081331491470337 + 2.0 * 8.340728759765625
Epoch 30, val loss: 1.9155348539352417
Epoch 40, training loss: 18.152877807617188 = 1.885624647140503 + 2.0 * 8.133626937866211
Epoch 40, val loss: 1.89337158203125
Epoch 50, training loss: 16.715742111206055 = 1.8601843118667603 + 2.0 * 7.427779197692871
Epoch 50, val loss: 1.8683266639709473
Epoch 60, training loss: 15.965238571166992 = 1.8429620265960693 + 2.0 * 7.061138153076172
Epoch 60, val loss: 1.8531004190444946
Epoch 70, training loss: 15.276656150817871 = 1.8349205255508423 + 2.0 * 6.72086763381958
Epoch 70, val loss: 1.845734715461731
Epoch 80, training loss: 15.016487121582031 = 1.8263120651245117 + 2.0 * 6.59508752822876
Epoch 80, val loss: 1.8370598554611206
Epoch 90, training loss: 14.858762741088867 = 1.8165034055709839 + 2.0 * 6.521129608154297
Epoch 90, val loss: 1.8268201351165771
Epoch 100, training loss: 14.731671333312988 = 1.8065894842147827 + 2.0 * 6.462541103363037
Epoch 100, val loss: 1.8165258169174194
Epoch 110, training loss: 14.594523429870605 = 1.7990963459014893 + 2.0 * 6.397713661193848
Epoch 110, val loss: 1.808413028717041
Epoch 120, training loss: 14.482004165649414 = 1.7934246063232422 + 2.0 * 6.344289779663086
Epoch 120, val loss: 1.8019390106201172
Epoch 130, training loss: 14.400972366333008 = 1.7873177528381348 + 2.0 * 6.306827068328857
Epoch 130, val loss: 1.7954154014587402
Epoch 140, training loss: 14.336564064025879 = 1.7801815271377563 + 2.0 * 6.278191089630127
Epoch 140, val loss: 1.7883660793304443
Epoch 150, training loss: 14.282519340515137 = 1.7721763849258423 + 2.0 * 6.255171298980713
Epoch 150, val loss: 1.7809087038040161
Epoch 160, training loss: 14.232451438903809 = 1.7633527517318726 + 2.0 * 6.234549522399902
Epoch 160, val loss: 1.7730717658996582
Epoch 170, training loss: 14.190620422363281 = 1.753495454788208 + 2.0 * 6.218562602996826
Epoch 170, val loss: 1.7646726369857788
Epoch 180, training loss: 14.147087097167969 = 1.7423826456069946 + 2.0 * 6.202352046966553
Epoch 180, val loss: 1.7554800510406494
Epoch 190, training loss: 14.106874465942383 = 1.7295842170715332 + 2.0 * 6.188645362854004
Epoch 190, val loss: 1.745045781135559
Epoch 200, training loss: 14.066604614257812 = 1.714614748954773 + 2.0 * 6.175994873046875
Epoch 200, val loss: 1.7329446077346802
Epoch 210, training loss: 14.034976959228516 = 1.6968812942504883 + 2.0 * 6.169047832489014
Epoch 210, val loss: 1.718880295753479
Epoch 220, training loss: 13.986836433410645 = 1.6763561964035034 + 2.0 * 6.155240058898926
Epoch 220, val loss: 1.702478051185608
Epoch 230, training loss: 13.94429874420166 = 1.6525990962982178 + 2.0 * 6.145849704742432
Epoch 230, val loss: 1.6834828853607178
Epoch 240, training loss: 13.900585174560547 = 1.6247371435165405 + 2.0 * 6.1379241943359375
Epoch 240, val loss: 1.6612638235092163
Epoch 250, training loss: 13.854705810546875 = 1.5918958187103271 + 2.0 * 6.131404876708984
Epoch 250, val loss: 1.6350352764129639
Epoch 260, training loss: 13.805620193481445 = 1.5539638996124268 + 2.0 * 6.125828266143799
Epoch 260, val loss: 1.604596495628357
Epoch 270, training loss: 13.749394416809082 = 1.5109330415725708 + 2.0 * 6.1192307472229
Epoch 270, val loss: 1.5700055360794067
Epoch 280, training loss: 13.691807746887207 = 1.4627494812011719 + 2.0 * 6.114529132843018
Epoch 280, val loss: 1.5311278104782104
Epoch 290, training loss: 13.643180847167969 = 1.4100465774536133 + 2.0 * 6.116567134857178
Epoch 290, val loss: 1.4887386560440063
Epoch 300, training loss: 13.569358825683594 = 1.3555312156677246 + 2.0 * 6.1069135665893555
Epoch 300, val loss: 1.4453366994857788
Epoch 310, training loss: 13.505754470825195 = 1.3002914190292358 + 2.0 * 6.102731704711914
Epoch 310, val loss: 1.401390552520752
Epoch 320, training loss: 13.443674087524414 = 1.2454025745391846 + 2.0 * 6.099135875701904
Epoch 320, val loss: 1.3580840826034546
Epoch 330, training loss: 13.385438919067383 = 1.1918905973434448 + 2.0 * 6.096774101257324
Epoch 330, val loss: 1.3163740634918213
Epoch 340, training loss: 13.329045295715332 = 1.1418901681900024 + 2.0 * 6.0935773849487305
Epoch 340, val loss: 1.2779877185821533
Epoch 350, training loss: 13.276276588439941 = 1.0960558652877808 + 2.0 * 6.0901103019714355
Epoch 350, val loss: 1.243051290512085
Epoch 360, training loss: 13.226529121398926 = 1.052926778793335 + 2.0 * 6.086801052093506
Epoch 360, val loss: 1.2106056213378906
Epoch 370, training loss: 13.179428100585938 = 1.011864423751831 + 2.0 * 6.083781719207764
Epoch 370, val loss: 1.179885745048523
Epoch 380, training loss: 13.141096115112305 = 0.9725114703178406 + 2.0 * 6.084292411804199
Epoch 380, val loss: 1.1509126424789429
Epoch 390, training loss: 13.09895133972168 = 0.9347671270370483 + 2.0 * 6.08209228515625
Epoch 390, val loss: 1.1233201026916504
Epoch 400, training loss: 13.052390098571777 = 0.8984677195549011 + 2.0 * 6.076961040496826
Epoch 400, val loss: 1.0968021154403687
Epoch 410, training loss: 13.011438369750977 = 0.862873375415802 + 2.0 * 6.074282646179199
Epoch 410, val loss: 1.0710340738296509
Epoch 420, training loss: 12.97080135345459 = 0.8275724053382874 + 2.0 * 6.0716142654418945
Epoch 420, val loss: 1.045441746711731
Epoch 430, training loss: 12.937565803527832 = 0.7923675775527954 + 2.0 * 6.072598934173584
Epoch 430, val loss: 1.020031213760376
Epoch 440, training loss: 12.893558502197266 = 0.7578685879707336 + 2.0 * 6.067844867706299
Epoch 440, val loss: 0.9951629638671875
Epoch 450, training loss: 12.8573637008667 = 0.7241579294204712 + 2.0 * 6.06660270690918
Epoch 450, val loss: 0.9710544347763062
Epoch 460, training loss: 12.819653511047363 = 0.69105464220047 + 2.0 * 6.064299583435059
Epoch 460, val loss: 0.947575032711029
Epoch 470, training loss: 12.783584594726562 = 0.6586839556694031 + 2.0 * 6.062450408935547
Epoch 470, val loss: 0.9247056245803833
Epoch 480, training loss: 12.766044616699219 = 0.6276249885559082 + 2.0 * 6.069210052490234
Epoch 480, val loss: 0.9032300114631653
Epoch 490, training loss: 12.718976020812988 = 0.5984213948249817 + 2.0 * 6.060277462005615
Epoch 490, val loss: 0.8834972977638245
Epoch 500, training loss: 12.686808586120605 = 0.5705909729003906 + 2.0 * 6.058108806610107
Epoch 500, val loss: 0.865257203578949
Epoch 510, training loss: 12.655400276184082 = 0.5440585017204285 + 2.0 * 6.055670738220215
Epoch 510, val loss: 0.8484171032905579
Epoch 520, training loss: 12.637365341186523 = 0.5189604759216309 + 2.0 * 6.059202671051025
Epoch 520, val loss: 0.8330702185630798
Epoch 530, training loss: 12.601973533630371 = 0.4956946074962616 + 2.0 * 6.053139686584473
Epoch 530, val loss: 0.8195788264274597
Epoch 540, training loss: 12.576130867004395 = 0.47383877635002136 + 2.0 * 6.051146030426025
Epoch 540, val loss: 0.8076654076576233
Epoch 550, training loss: 12.552475929260254 = 0.45320817828178406 + 2.0 * 6.049633979797363
Epoch 550, val loss: 0.797092616558075
Epoch 560, training loss: 12.54198169708252 = 0.43372440338134766 + 2.0 * 6.054128646850586
Epoch 560, val loss: 0.7877964377403259
Epoch 570, training loss: 12.51517105102539 = 0.41564515233039856 + 2.0 * 6.049762725830078
Epoch 570, val loss: 0.77973872423172
Epoch 580, training loss: 12.4945068359375 = 0.39884117245674133 + 2.0 * 6.04783296585083
Epoch 580, val loss: 0.7731505632400513
Epoch 590, training loss: 12.47311782836914 = 0.3829464316368103 + 2.0 * 6.045085906982422
Epoch 590, val loss: 0.7674399018287659
Epoch 600, training loss: 12.454737663269043 = 0.3678259551525116 + 2.0 * 6.043456077575684
Epoch 600, val loss: 0.7625351548194885
Epoch 610, training loss: 12.438344955444336 = 0.3533901870250702 + 2.0 * 6.042477607727051
Epoch 610, val loss: 0.7584070563316345
Epoch 620, training loss: 12.42813491821289 = 0.339650958776474 + 2.0 * 6.044241905212402
Epoch 620, val loss: 0.7549108862876892
Epoch 630, training loss: 12.408788681030273 = 0.326623797416687 + 2.0 * 6.041082382202148
Epoch 630, val loss: 0.752156138420105
Epoch 640, training loss: 12.393202781677246 = 0.31409814953804016 + 2.0 * 6.039552211761475
Epoch 640, val loss: 0.7499450445175171
Epoch 650, training loss: 12.379243850708008 = 0.3019696772098541 + 2.0 * 6.038637161254883
Epoch 650, val loss: 0.7480954527854919
Epoch 660, training loss: 12.376028060913086 = 0.29017889499664307 + 2.0 * 6.042924404144287
Epoch 660, val loss: 0.7465598583221436
Epoch 670, training loss: 12.350851058959961 = 0.2787806987762451 + 2.0 * 6.036035060882568
Epoch 670, val loss: 0.7455231547355652
Epoch 680, training loss: 12.33863639831543 = 0.26760628819465637 + 2.0 * 6.035514831542969
Epoch 680, val loss: 0.7448338866233826
Epoch 690, training loss: 12.344226837158203 = 0.2566141188144684 + 2.0 * 6.043806552886963
Epoch 690, val loss: 0.7443580031394958
Epoch 700, training loss: 12.317540168762207 = 0.2458864152431488 + 2.0 * 6.035826683044434
Epoch 700, val loss: 0.7440484166145325
Epoch 710, training loss: 12.302101135253906 = 0.23540502786636353 + 2.0 * 6.033348083496094
Epoch 710, val loss: 0.744204044342041
Epoch 720, training loss: 12.28963851928711 = 0.225086510181427 + 2.0 * 6.032276153564453
Epoch 720, val loss: 0.7444457411766052
Epoch 730, training loss: 12.283004760742188 = 0.21498876810073853 + 2.0 * 6.034008026123047
Epoch 730, val loss: 0.7449255585670471
Epoch 740, training loss: 12.268447875976562 = 0.205190971493721 + 2.0 * 6.031628608703613
Epoch 740, val loss: 0.745564341545105
Epoch 750, training loss: 12.258708953857422 = 0.1957405060529709 + 2.0 * 6.031484127044678
Epoch 750, val loss: 0.7465603947639465
Epoch 760, training loss: 12.248274803161621 = 0.1866588443517685 + 2.0 * 6.030807971954346
Epoch 760, val loss: 0.7476670742034912
Epoch 770, training loss: 12.235013961791992 = 0.17799539864063263 + 2.0 * 6.028509140014648
Epoch 770, val loss: 0.7491199374198914
Epoch 780, training loss: 12.229156494140625 = 0.16972841322422028 + 2.0 * 6.029714107513428
Epoch 780, val loss: 0.7508661150932312
Epoch 790, training loss: 12.221172332763672 = 0.16192005574703217 + 2.0 * 6.029626369476318
Epoch 790, val loss: 0.7524678707122803
Epoch 800, training loss: 12.206124305725098 = 0.15460243821144104 + 2.0 * 6.025761127471924
Epoch 800, val loss: 0.7547174692153931
Epoch 810, training loss: 12.198915481567383 = 0.14766688644886017 + 2.0 * 6.0256242752075195
Epoch 810, val loss: 0.7569776773452759
Epoch 820, training loss: 12.189129829406738 = 0.141093447804451 + 2.0 * 6.024018287658691
Epoch 820, val loss: 0.7593810558319092
Epoch 830, training loss: 12.205466270446777 = 0.13486993312835693 + 2.0 * 6.0352983474731445
Epoch 830, val loss: 0.7619985938072205
Epoch 840, training loss: 12.180389404296875 = 0.12909400463104248 + 2.0 * 6.0256476402282715
Epoch 840, val loss: 0.7645136117935181
Epoch 850, training loss: 12.169381141662598 = 0.12366995960474014 + 2.0 * 6.022855758666992
Epoch 850, val loss: 0.7675728797912598
Epoch 860, training loss: 12.161880493164062 = 0.11853678524494171 + 2.0 * 6.021671772003174
Epoch 860, val loss: 0.7704461216926575
Epoch 870, training loss: 12.157331466674805 = 0.11367180943489075 + 2.0 * 6.021829605102539
Epoch 870, val loss: 0.7735510468482971
Epoch 880, training loss: 12.15686321258545 = 0.10908193141222 + 2.0 * 6.023890495300293
Epoch 880, val loss: 0.7765997052192688
Epoch 890, training loss: 12.150107383728027 = 0.1047988161444664 + 2.0 * 6.022654056549072
Epoch 890, val loss: 0.7799869775772095
Epoch 900, training loss: 12.139337539672852 = 0.10074923187494278 + 2.0 * 6.019294261932373
Epoch 900, val loss: 0.7834696173667908
Epoch 910, training loss: 12.134320259094238 = 0.09689632803201675 + 2.0 * 6.018712043762207
Epoch 910, val loss: 0.7868351936340332
Epoch 920, training loss: 12.132277488708496 = 0.0932355746626854 + 2.0 * 6.0195207595825195
Epoch 920, val loss: 0.7903928756713867
Epoch 930, training loss: 12.132634162902832 = 0.08977586776018143 + 2.0 * 6.021429061889648
Epoch 930, val loss: 0.7939885854721069
Epoch 940, training loss: 12.121930122375488 = 0.0865243449807167 + 2.0 * 6.017703056335449
Epoch 940, val loss: 0.7977339029312134
Epoch 950, training loss: 12.118806838989258 = 0.08343522995710373 + 2.0 * 6.017685890197754
Epoch 950, val loss: 0.8015140891075134
Epoch 960, training loss: 12.119084358215332 = 0.08049970120191574 + 2.0 * 6.01929235458374
Epoch 960, val loss: 0.8051835298538208
Epoch 970, training loss: 12.108708381652832 = 0.07770738750696182 + 2.0 * 6.015500545501709
Epoch 970, val loss: 0.808927059173584
Epoch 980, training loss: 12.106269836425781 = 0.0750461295247078 + 2.0 * 6.01561164855957
Epoch 980, val loss: 0.8127908110618591
Epoch 990, training loss: 12.107674598693848 = 0.07251351326704025 + 2.0 * 6.017580509185791
Epoch 990, val loss: 0.8164690732955933
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8037
Overall ASR: 0.5535
Flip ASR: 0.4711/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.69672966003418 = 1.9492067098617554 + 2.0 * 8.373761177062988
Epoch 0, val loss: 1.9513287544250488
Epoch 10, training loss: 18.68474578857422 = 1.9392874240875244 + 2.0 * 8.372729301452637
Epoch 10, val loss: 1.9411439895629883
Epoch 20, training loss: 18.660945892333984 = 1.927382469177246 + 2.0 * 8.366782188415527
Epoch 20, val loss: 1.9281338453292847
Epoch 30, training loss: 18.572765350341797 = 1.9120675325393677 + 2.0 * 8.33034896850586
Epoch 30, val loss: 1.9110023975372314
Epoch 40, training loss: 18.042510986328125 = 1.895113468170166 + 2.0 * 8.073698997497559
Epoch 40, val loss: 1.8922321796417236
Epoch 50, training loss: 16.39836883544922 = 1.8763437271118164 + 2.0 * 7.261012077331543
Epoch 50, val loss: 1.8711659908294678
Epoch 60, training loss: 16.16073226928711 = 1.8578295707702637 + 2.0 * 7.151451110839844
Epoch 60, val loss: 1.8525680303573608
Epoch 70, training loss: 15.770295143127441 = 1.8447424173355103 + 2.0 * 6.962776184082031
Epoch 70, val loss: 1.8396552801132202
Epoch 80, training loss: 15.303138732910156 = 1.834099292755127 + 2.0 * 6.7345194816589355
Epoch 80, val loss: 1.8288838863372803
Epoch 90, training loss: 14.87443733215332 = 1.8262192010879517 + 2.0 * 6.52410888671875
Epoch 90, val loss: 1.820799469947815
Epoch 100, training loss: 14.690258026123047 = 1.8179380893707275 + 2.0 * 6.436160087585449
Epoch 100, val loss: 1.811667799949646
Epoch 110, training loss: 14.568391799926758 = 1.8078280687332153 + 2.0 * 6.380281925201416
Epoch 110, val loss: 1.8006712198257446
Epoch 120, training loss: 14.478544235229492 = 1.7977089881896973 + 2.0 * 6.340417861938477
Epoch 120, val loss: 1.7898045778274536
Epoch 130, training loss: 14.403560638427734 = 1.7884594202041626 + 2.0 * 6.307550430297852
Epoch 130, val loss: 1.7796283960342407
Epoch 140, training loss: 14.329062461853027 = 1.7797499895095825 + 2.0 * 6.274656295776367
Epoch 140, val loss: 1.770047664642334
Epoch 150, training loss: 14.264354705810547 = 1.7713711261749268 + 2.0 * 6.2464919090271
Epoch 150, val loss: 1.760909080505371
Epoch 160, training loss: 14.211503028869629 = 1.7625445127487183 + 2.0 * 6.2244791984558105
Epoch 160, val loss: 1.7518296241760254
Epoch 170, training loss: 14.164380073547363 = 1.752763032913208 + 2.0 * 6.205808639526367
Epoch 170, val loss: 1.742368221282959
Epoch 180, training loss: 14.125277519226074 = 1.7417984008789062 + 2.0 * 6.191739559173584
Epoch 180, val loss: 1.7322602272033691
Epoch 190, training loss: 14.088048934936523 = 1.7293553352355957 + 2.0 * 6.179346561431885
Epoch 190, val loss: 1.7213878631591797
Epoch 200, training loss: 14.051385879516602 = 1.7150973081588745 + 2.0 * 6.168144226074219
Epoch 200, val loss: 1.7093322277069092
Epoch 210, training loss: 14.0162992477417 = 1.6985647678375244 + 2.0 * 6.158867359161377
Epoch 210, val loss: 1.695690631866455
Epoch 220, training loss: 13.979698181152344 = 1.6794397830963135 + 2.0 * 6.150129318237305
Epoch 220, val loss: 1.680214285850525
Epoch 230, training loss: 13.941892623901367 = 1.656923770904541 + 2.0 * 6.142484664916992
Epoch 230, val loss: 1.6620664596557617
Epoch 240, training loss: 13.902166366577148 = 1.6302320957183838 + 2.0 * 6.135967254638672
Epoch 240, val loss: 1.6406153440475464
Epoch 250, training loss: 13.8599214553833 = 1.5989539623260498 + 2.0 * 6.130483627319336
Epoch 250, val loss: 1.6156777143478394
Epoch 260, training loss: 13.811903953552246 = 1.5624706745147705 + 2.0 * 6.124716758728027
Epoch 260, val loss: 1.5865801572799683
Epoch 270, training loss: 13.762552261352539 = 1.5207291841506958 + 2.0 * 6.120911598205566
Epoch 270, val loss: 1.553182601928711
Epoch 280, training loss: 13.70553207397461 = 1.474115252494812 + 2.0 * 6.115708351135254
Epoch 280, val loss: 1.5164494514465332
Epoch 290, training loss: 13.648981094360352 = 1.4238035678863525 + 2.0 * 6.112588882446289
Epoch 290, val loss: 1.4769879579544067
Epoch 300, training loss: 13.590191841125488 = 1.3723325729370117 + 2.0 * 6.108929634094238
Epoch 300, val loss: 1.4370205402374268
Epoch 310, training loss: 13.52966594696045 = 1.3209925889968872 + 2.0 * 6.104336738586426
Epoch 310, val loss: 1.3978809118270874
Epoch 320, training loss: 13.472253799438477 = 1.2713783979415894 + 2.0 * 6.100437641143799
Epoch 320, val loss: 1.3609893321990967
Epoch 330, training loss: 13.421430587768555 = 1.2248904705047607 + 2.0 * 6.098269939422607
Epoch 330, val loss: 1.326694130897522
Epoch 340, training loss: 13.368932723999023 = 1.181044101715088 + 2.0 * 6.093944549560547
Epoch 340, val loss: 1.2951090335845947
Epoch 350, training loss: 13.318628311157227 = 1.1391751766204834 + 2.0 * 6.089726448059082
Epoch 350, val loss: 1.2653496265411377
Epoch 360, training loss: 13.281200408935547 = 1.0984972715377808 + 2.0 * 6.091351509094238
Epoch 360, val loss: 1.2365024089813232
Epoch 370, training loss: 13.23061752319336 = 1.0586098432540894 + 2.0 * 6.08600378036499
Epoch 370, val loss: 1.2081936597824097
Epoch 380, training loss: 13.18214225769043 = 1.0186631679534912 + 2.0 * 6.08173942565918
Epoch 380, val loss: 1.1797314882278442
Epoch 390, training loss: 13.137748718261719 = 0.9784598350524902 + 2.0 * 6.079644203186035
Epoch 390, val loss: 1.150783896446228
Epoch 400, training loss: 13.09034538269043 = 0.9380754828453064 + 2.0 * 6.076135158538818
Epoch 400, val loss: 1.1213958263397217
Epoch 410, training loss: 13.055547714233398 = 0.8974048495292664 + 2.0 * 6.079071521759033
Epoch 410, val loss: 1.0914876461029053
Epoch 420, training loss: 13.000974655151367 = 0.8574567437171936 + 2.0 * 6.07175874710083
Epoch 420, val loss: 1.0616589784622192
Epoch 430, training loss: 12.957098960876465 = 0.8181201219558716 + 2.0 * 6.069489479064941
Epoch 430, val loss: 1.0322507619857788
Epoch 440, training loss: 12.914947509765625 = 0.7797362804412842 + 2.0 * 6.067605495452881
Epoch 440, val loss: 1.0036487579345703
Epoch 450, training loss: 12.87833023071289 = 0.7430288791656494 + 2.0 * 6.06765079498291
Epoch 450, val loss: 0.9765119552612305
Epoch 460, training loss: 12.839427947998047 = 0.7087116837501526 + 2.0 * 6.0653581619262695
Epoch 460, val loss: 0.9516861438751221
Epoch 470, training loss: 12.80130386352539 = 0.6763662695884705 + 2.0 * 6.062469005584717
Epoch 470, val loss: 0.9288208484649658
Epoch 480, training loss: 12.76646614074707 = 0.6457245349884033 + 2.0 * 6.060370922088623
Epoch 480, val loss: 0.9078438878059387
Epoch 490, training loss: 12.737212181091309 = 0.6168178915977478 + 2.0 * 6.060197353363037
Epoch 490, val loss: 0.8887940645217896
Epoch 500, training loss: 12.708155632019043 = 0.5896635055541992 + 2.0 * 6.059246063232422
Epoch 500, val loss: 0.871617317199707
Epoch 510, training loss: 12.675684928894043 = 0.5638936758041382 + 2.0 * 6.055895805358887
Epoch 510, val loss: 0.8561088442802429
Epoch 520, training loss: 12.64621353149414 = 0.539204478263855 + 2.0 * 6.053504467010498
Epoch 520, val loss: 0.8418611884117126
Epoch 530, training loss: 12.631279945373535 = 0.5154300332069397 + 2.0 * 6.057924747467041
Epoch 530, val loss: 0.828703761100769
Epoch 540, training loss: 12.598800659179688 = 0.4926431477069855 + 2.0 * 6.053078651428223
Epoch 540, val loss: 0.8166570663452148
Epoch 550, training loss: 12.570243835449219 = 0.4708191454410553 + 2.0 * 6.049712181091309
Epoch 550, val loss: 0.8057016134262085
Epoch 560, training loss: 12.546170234680176 = 0.44965967535972595 + 2.0 * 6.048255443572998
Epoch 560, val loss: 0.7956352829933167
Epoch 570, training loss: 12.529342651367188 = 0.42912524938583374 + 2.0 * 6.050108909606934
Epoch 570, val loss: 0.7863329648971558
Epoch 580, training loss: 12.514240264892578 = 0.40944933891296387 + 2.0 * 6.052395343780518
Epoch 580, val loss: 0.7776955366134644
Epoch 590, training loss: 12.484132766723633 = 0.3904910981655121 + 2.0 * 6.046820640563965
Epoch 590, val loss: 0.7698816657066345
Epoch 600, training loss: 12.459972381591797 = 0.3722340166568756 + 2.0 * 6.0438690185546875
Epoch 600, val loss: 0.7627841234207153
Epoch 610, training loss: 12.439541816711426 = 0.35451483726501465 + 2.0 * 6.042513370513916
Epoch 610, val loss: 0.7563063502311707
Epoch 620, training loss: 12.432106018066406 = 0.3373900353908539 + 2.0 * 6.04735803604126
Epoch 620, val loss: 0.750438928604126
Epoch 630, training loss: 12.40433120727539 = 0.32108449935913086 + 2.0 * 6.041623115539551
Epoch 630, val loss: 0.7452214956283569
Epoch 640, training loss: 12.383122444152832 = 0.3054386079311371 + 2.0 * 6.038841724395752
Epoch 640, val loss: 0.7408138513565063
Epoch 650, training loss: 12.36667537689209 = 0.2903703451156616 + 2.0 * 6.038152694702148
Epoch 650, val loss: 0.7370092868804932
Epoch 660, training loss: 12.35667610168457 = 0.2759169042110443 + 2.0 * 6.040379524230957
Epoch 660, val loss: 0.7338418960571289
Epoch 670, training loss: 12.344659805297852 = 0.26224231719970703 + 2.0 * 6.041208744049072
Epoch 670, val loss: 0.7312805652618408
Epoch 680, training loss: 12.322745323181152 = 0.24925005435943604 + 2.0 * 6.036747455596924
Epoch 680, val loss: 0.7293955087661743
Epoch 690, training loss: 12.30626392364502 = 0.2369592934846878 + 2.0 * 6.034652233123779
Epoch 690, val loss: 0.7281738519668579
Epoch 700, training loss: 12.29345417022705 = 0.22528348863124847 + 2.0 * 6.034085273742676
Epoch 700, val loss: 0.7275470495223999
Epoch 710, training loss: 12.280797004699707 = 0.21424002945423126 + 2.0 * 6.033278465270996
Epoch 710, val loss: 0.7274821996688843
Epoch 720, training loss: 12.277090072631836 = 0.2038324624300003 + 2.0 * 6.036628723144531
Epoch 720, val loss: 0.7279594540596008
Epoch 730, training loss: 12.259438514709473 = 0.19410289824008942 + 2.0 * 6.032667636871338
Epoch 730, val loss: 0.7289665937423706
Epoch 740, training loss: 12.244592666625977 = 0.18495051562786102 + 2.0 * 6.029820919036865
Epoch 740, val loss: 0.7305117249488831
Epoch 750, training loss: 12.232966423034668 = 0.1762915402650833 + 2.0 * 6.028337478637695
Epoch 750, val loss: 0.7325347065925598
Epoch 760, training loss: 12.239274978637695 = 0.1681261509656906 + 2.0 * 6.035574436187744
Epoch 760, val loss: 0.7349771857261658
Epoch 770, training loss: 12.219286918640137 = 0.160446435213089 + 2.0 * 6.029420375823975
Epoch 770, val loss: 0.7376523613929749
Epoch 780, training loss: 12.207609176635742 = 0.15328194200992584 + 2.0 * 6.027163505554199
Epoch 780, val loss: 0.7407898306846619
Epoch 790, training loss: 12.197843551635742 = 0.14650095999240875 + 2.0 * 6.025671482086182
Epoch 790, val loss: 0.7442675828933716
Epoch 800, training loss: 12.199461936950684 = 0.14007140696048737 + 2.0 * 6.0296950340271
Epoch 800, val loss: 0.7479923367500305
Epoch 810, training loss: 12.188395500183105 = 0.13404949009418488 + 2.0 * 6.027173042297363
Epoch 810, val loss: 0.7520108819007874
Epoch 820, training loss: 12.175029754638672 = 0.12836527824401855 + 2.0 * 6.023332118988037
Epoch 820, val loss: 0.7562857270240784
Epoch 830, training loss: 12.173202514648438 = 0.12301136553287506 + 2.0 * 6.025095462799072
Epoch 830, val loss: 0.7608160972595215
Epoch 840, training loss: 12.162225723266602 = 0.11798118054866791 + 2.0 * 6.022122383117676
Epoch 840, val loss: 0.7655981779098511
Epoch 850, training loss: 12.159317016601562 = 0.11324925720691681 + 2.0 * 6.02303409576416
Epoch 850, val loss: 0.7705783247947693
Epoch 860, training loss: 12.15035343170166 = 0.10878906399011612 + 2.0 * 6.020781993865967
Epoch 860, val loss: 0.7757338881492615
Epoch 870, training loss: 12.145673751831055 = 0.10459116101264954 + 2.0 * 6.020541191101074
Epoch 870, val loss: 0.7810834050178528
Epoch 880, training loss: 12.149299621582031 = 0.10062500089406967 + 2.0 * 6.024337291717529
Epoch 880, val loss: 0.7864755988121033
Epoch 890, training loss: 12.13381576538086 = 0.09690802544355392 + 2.0 * 6.018454074859619
Epoch 890, val loss: 0.7919431328773499
Epoch 900, training loss: 12.128242492675781 = 0.0933832973241806 + 2.0 * 6.017429828643799
Epoch 900, val loss: 0.7975078225135803
Epoch 910, training loss: 12.127614974975586 = 0.09003980457782745 + 2.0 * 6.018787384033203
Epoch 910, val loss: 0.8031433820724487
Epoch 920, training loss: 12.127389907836914 = 0.08687885850667953 + 2.0 * 6.0202555656433105
Epoch 920, val loss: 0.8087554574012756
Epoch 930, training loss: 12.11720085144043 = 0.08391718566417694 + 2.0 * 6.016641616821289
Epoch 930, val loss: 0.8144010901451111
Epoch 940, training loss: 12.112398147583008 = 0.08109958469867706 + 2.0 * 6.015649318695068
Epoch 940, val loss: 0.8201003074645996
Epoch 950, training loss: 12.108853340148926 = 0.07840930670499802 + 2.0 * 6.015222072601318
Epoch 950, val loss: 0.8258354067802429
Epoch 960, training loss: 12.110085487365723 = 0.07583644241094589 + 2.0 * 6.017124652862549
Epoch 960, val loss: 0.8316051363945007
Epoch 970, training loss: 12.100961685180664 = 0.07338295131921768 + 2.0 * 6.013789176940918
Epoch 970, val loss: 0.8373687863349915
Epoch 980, training loss: 12.10776424407959 = 0.07104520499706268 + 2.0 * 6.018359661102295
Epoch 980, val loss: 0.8431963324546814
Epoch 990, training loss: 12.098498344421387 = 0.06882338970899582 + 2.0 * 6.014837265014648
Epoch 990, val loss: 0.8489348888397217
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.2325
Flip ASR: 0.2133/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.683561325073242 = 1.9361364841461182 + 2.0 * 8.373712539672852
Epoch 0, val loss: 1.9325687885284424
Epoch 10, training loss: 18.67028045654297 = 1.9261767864227295 + 2.0 * 8.372052192687988
Epoch 10, val loss: 1.9228235483169556
Epoch 20, training loss: 18.640653610229492 = 1.9140182733535767 + 2.0 * 8.363317489624023
Epoch 20, val loss: 1.9101788997650146
Epoch 30, training loss: 18.528759002685547 = 1.8986691236495972 + 2.0 * 8.315045356750488
Epoch 30, val loss: 1.8938852548599243
Epoch 40, training loss: 17.971765518188477 = 1.8824994564056396 + 2.0 * 8.044632911682129
Epoch 40, val loss: 1.8766957521438599
Epoch 50, training loss: 16.615158081054688 = 1.8647809028625488 + 2.0 * 7.37518835067749
Epoch 50, val loss: 1.8571590185165405
Epoch 60, training loss: 15.847345352172852 = 1.8470897674560547 + 2.0 * 7.000127792358398
Epoch 60, val loss: 1.8391106128692627
Epoch 70, training loss: 15.373336791992188 = 1.8325401544570923 + 2.0 * 6.770398139953613
Epoch 70, val loss: 1.8248310089111328
Epoch 80, training loss: 15.066411972045898 = 1.8206909894943237 + 2.0 * 6.622860431671143
Epoch 80, val loss: 1.8127167224884033
Epoch 90, training loss: 14.775107383728027 = 1.8118869066238403 + 2.0 * 6.481610298156738
Epoch 90, val loss: 1.803309679031372
Epoch 100, training loss: 14.607455253601074 = 1.8044178485870361 + 2.0 * 6.401518821716309
Epoch 100, val loss: 1.7949800491333008
Epoch 110, training loss: 14.492791175842285 = 1.7964755296707153 + 2.0 * 6.34815788269043
Epoch 110, val loss: 1.7859402894973755
Epoch 120, training loss: 14.402048110961914 = 1.7881724834442139 + 2.0 * 6.3069376945495605
Epoch 120, val loss: 1.7769286632537842
Epoch 130, training loss: 14.331748962402344 = 1.7801287174224854 + 2.0 * 6.275810241699219
Epoch 130, val loss: 1.7684215307235718
Epoch 140, training loss: 14.27047061920166 = 1.7723066806793213 + 2.0 * 6.249082088470459
Epoch 140, val loss: 1.7601426839828491
Epoch 150, training loss: 14.218708992004395 = 1.7643667459487915 + 2.0 * 6.227170944213867
Epoch 150, val loss: 1.7519755363464355
Epoch 160, training loss: 14.171194076538086 = 1.7560675144195557 + 2.0 * 6.207563400268555
Epoch 160, val loss: 1.7438840866088867
Epoch 170, training loss: 14.134902000427246 = 1.7471742630004883 + 2.0 * 6.193863868713379
Epoch 170, val loss: 1.7355389595031738
Epoch 180, training loss: 14.093093872070312 = 1.7372874021530151 + 2.0 * 6.177903175354004
Epoch 180, val loss: 1.7269748449325562
Epoch 190, training loss: 14.057242393493652 = 1.726357102394104 + 2.0 * 6.16544246673584
Epoch 190, val loss: 1.717690348625183
Epoch 200, training loss: 14.026524543762207 = 1.7138608694076538 + 2.0 * 6.156332015991211
Epoch 200, val loss: 1.70751953125
Epoch 210, training loss: 13.993122100830078 = 1.699600338935852 + 2.0 * 6.146760940551758
Epoch 210, val loss: 1.696135401725769
Epoch 220, training loss: 13.959532737731934 = 1.6831731796264648 + 2.0 * 6.138179779052734
Epoch 220, val loss: 1.6831793785095215
Epoch 230, training loss: 13.928345680236816 = 1.6640636920928955 + 2.0 * 6.13214111328125
Epoch 230, val loss: 1.6682524681091309
Epoch 240, training loss: 13.889787673950195 = 1.6418445110321045 + 2.0 * 6.123971462249756
Epoch 240, val loss: 1.6510016918182373
Epoch 250, training loss: 13.854955673217773 = 1.6159253120422363 + 2.0 * 6.119515419006348
Epoch 250, val loss: 1.630857229232788
Epoch 260, training loss: 13.812190055847168 = 1.585493803024292 + 2.0 * 6.113348007202148
Epoch 260, val loss: 1.6073437929153442
Epoch 270, training loss: 13.766525268554688 = 1.55020272731781 + 2.0 * 6.108161449432373
Epoch 270, val loss: 1.579786777496338
Epoch 280, training loss: 13.723657608032227 = 1.5091232061386108 + 2.0 * 6.107267379760742
Epoch 280, val loss: 1.5478006601333618
Epoch 290, training loss: 13.664534568786621 = 1.4627923965454102 + 2.0 * 6.1008710861206055
Epoch 290, val loss: 1.5112882852554321
Epoch 300, training loss: 13.605316162109375 = 1.4110407829284668 + 2.0 * 6.097137928009033
Epoch 300, val loss: 1.4706840515136719
Epoch 310, training loss: 13.541776657104492 = 1.3544447422027588 + 2.0 * 6.093666076660156
Epoch 310, val loss: 1.4261658191680908
Epoch 320, training loss: 13.488175392150879 = 1.293964147567749 + 2.0 * 6.097105503082275
Epoch 320, val loss: 1.3786126375198364
Epoch 330, training loss: 13.414298057556152 = 1.2324992418289185 + 2.0 * 6.090899467468262
Epoch 330, val loss: 1.3306223154067993
Epoch 340, training loss: 13.34228515625 = 1.1714231967926025 + 2.0 * 6.085431098937988
Epoch 340, val loss: 1.2831083536148071
Epoch 350, training loss: 13.285379409790039 = 1.111397385597229 + 2.0 * 6.086990833282471
Epoch 350, val loss: 1.236424446105957
Epoch 360, training loss: 13.215887069702148 = 1.053968071937561 + 2.0 * 6.080959320068359
Epoch 360, val loss: 1.1924866437911987
Epoch 370, training loss: 13.156326293945312 = 0.9996857643127441 + 2.0 * 6.078320503234863
Epoch 370, val loss: 1.1509747505187988
Epoch 380, training loss: 13.099445343017578 = 0.9478375315666199 + 2.0 * 6.075803756713867
Epoch 380, val loss: 1.1116217374801636
Epoch 390, training loss: 13.049676895141602 = 0.8986116647720337 + 2.0 * 6.07553243637085
Epoch 390, val loss: 1.0743399858474731
Epoch 400, training loss: 12.994579315185547 = 0.8523929119110107 + 2.0 * 6.0710930824279785
Epoch 400, val loss: 1.0398063659667969
Epoch 410, training loss: 12.946449279785156 = 0.8086957335472107 + 2.0 * 6.06887674331665
Epoch 410, val loss: 1.0071356296539307
Epoch 420, training loss: 12.902769088745117 = 0.7668113708496094 + 2.0 * 6.067978858947754
Epoch 420, val loss: 0.9760525226593018
Epoch 430, training loss: 12.856694221496582 = 0.7267992496490479 + 2.0 * 6.064947605133057
Epoch 430, val loss: 0.9467123746871948
Epoch 440, training loss: 12.814910888671875 = 0.688759982585907 + 2.0 * 6.063075542449951
Epoch 440, val loss: 0.919100821018219
Epoch 450, training loss: 12.778120994567871 = 0.6527593731880188 + 2.0 * 6.062680721282959
Epoch 450, val loss: 0.8933982849121094
Epoch 460, training loss: 12.743565559387207 = 0.6186909079551697 + 2.0 * 6.062437534332275
Epoch 460, val loss: 0.8695022463798523
Epoch 470, training loss: 12.704895973205566 = 0.5870175957679749 + 2.0 * 6.058938980102539
Epoch 470, val loss: 0.8476217985153198
Epoch 480, training loss: 12.670125961303711 = 0.5572953224182129 + 2.0 * 6.05641508102417
Epoch 480, val loss: 0.8279657959938049
Epoch 490, training loss: 12.635955810546875 = 0.5292091369628906 + 2.0 * 6.053373336791992
Epoch 490, val loss: 0.8099773526191711
Epoch 500, training loss: 12.607954025268555 = 0.5025815367698669 + 2.0 * 6.0526862144470215
Epoch 500, val loss: 0.7937176823616028
Epoch 510, training loss: 12.587297439575195 = 0.47750043869018555 + 2.0 * 6.054898262023926
Epoch 510, val loss: 0.7792929410934448
Epoch 520, training loss: 12.554014205932617 = 0.454054594039917 + 2.0 * 6.0499796867370605
Epoch 520, val loss: 0.7667058110237122
Epoch 530, training loss: 12.526519775390625 = 0.4319149851799011 + 2.0 * 6.04730224609375
Epoch 530, val loss: 0.7556214332580566
Epoch 540, training loss: 12.51486873626709 = 0.4107685089111328 + 2.0 * 6.0520501136779785
Epoch 540, val loss: 0.7458165287971497
Epoch 550, training loss: 12.480390548706055 = 0.39098405838012695 + 2.0 * 6.044703483581543
Epoch 550, val loss: 0.7373120784759521
Epoch 560, training loss: 12.459199905395508 = 0.37232840061187744 + 2.0 * 6.043435573577881
Epoch 560, val loss: 0.7302176356315613
Epoch 570, training loss: 12.438040733337402 = 0.3545503318309784 + 2.0 * 6.041745185852051
Epoch 570, val loss: 0.7240588665008545
Epoch 580, training loss: 12.41909122467041 = 0.3375747501850128 + 2.0 * 6.04075813293457
Epoch 580, val loss: 0.7187397480010986
Epoch 590, training loss: 12.400745391845703 = 0.32143130898475647 + 2.0 * 6.039657115936279
Epoch 590, val loss: 0.7143030166625977
Epoch 600, training loss: 12.387340545654297 = 0.306259423494339 + 2.0 * 6.04054069519043
Epoch 600, val loss: 0.7107914686203003
Epoch 610, training loss: 12.365708351135254 = 0.2918524146080017 + 2.0 * 6.036928176879883
Epoch 610, val loss: 0.7081209421157837
Epoch 620, training loss: 12.351300239562988 = 0.2781439423561096 + 2.0 * 6.036578178405762
Epoch 620, val loss: 0.7059836387634277
Epoch 630, training loss: 12.343341827392578 = 0.26509109139442444 + 2.0 * 6.039125442504883
Epoch 630, val loss: 0.7044064402580261
Epoch 640, training loss: 12.3244047164917 = 0.25274524092674255 + 2.0 * 6.035829544067383
Epoch 640, val loss: 0.7035318613052368
Epoch 650, training loss: 12.311540603637695 = 0.24101416766643524 + 2.0 * 6.0352630615234375
Epoch 650, val loss: 0.7032107710838318
Epoch 660, training loss: 12.293258666992188 = 0.22987760603427887 + 2.0 * 6.03169059753418
Epoch 660, val loss: 0.7033743262290955
Epoch 670, training loss: 12.286455154418945 = 0.2192852944135666 + 2.0 * 6.033585071563721
Epoch 670, val loss: 0.7040750980377197
Epoch 680, training loss: 12.270833969116211 = 0.20928052067756653 + 2.0 * 6.030776500701904
Epoch 680, val loss: 0.7051463723182678
Epoch 690, training loss: 12.274523735046387 = 0.19980879127979279 + 2.0 * 6.037357330322266
Epoch 690, val loss: 0.7067033648490906
Epoch 700, training loss: 12.252620697021484 = 0.19088958203792572 + 2.0 * 6.030865669250488
Epoch 700, val loss: 0.7085752487182617
Epoch 710, training loss: 12.237422943115234 = 0.18248429894447327 + 2.0 * 6.027469158172607
Epoch 710, val loss: 0.7108184695243835
Epoch 720, training loss: 12.228358268737793 = 0.17448052763938904 + 2.0 * 6.0269389152526855
Epoch 720, val loss: 0.7133172154426575
Epoch 730, training loss: 12.233209609985352 = 0.16686636209487915 + 2.0 * 6.033171653747559
Epoch 730, val loss: 0.7160099744796753
Epoch 740, training loss: 12.209487915039062 = 0.1597280502319336 + 2.0 * 6.0248799324035645
Epoch 740, val loss: 0.7190590500831604
Epoch 750, training loss: 12.200790405273438 = 0.152939110994339 + 2.0 * 6.02392578125
Epoch 750, val loss: 0.7223910093307495
Epoch 760, training loss: 12.199182510375977 = 0.14646564424037933 + 2.0 * 6.026358604431152
Epoch 760, val loss: 0.7258502244949341
Epoch 770, training loss: 12.194160461425781 = 0.1403297632932663 + 2.0 * 6.026915550231934
Epoch 770, val loss: 0.7295042276382446
Epoch 780, training loss: 12.178983688354492 = 0.13457484543323517 + 2.0 * 6.022204399108887
Epoch 780, val loss: 0.7334119081497192
Epoch 790, training loss: 12.171119689941406 = 0.12908326089382172 + 2.0 * 6.021018028259277
Epoch 790, val loss: 0.7374365329742432
Epoch 800, training loss: 12.170912742614746 = 0.1238386258482933 + 2.0 * 6.0235371589660645
Epoch 800, val loss: 0.7415305972099304
Epoch 810, training loss: 12.170849800109863 = 0.11889000236988068 + 2.0 * 6.025979995727539
Epoch 810, val loss: 0.7457454800605774
Epoch 820, training loss: 12.15249252319336 = 0.11422543227672577 + 2.0 * 6.019133567810059
Epoch 820, val loss: 0.750225841999054
Epoch 830, training loss: 12.14649772644043 = 0.10978391021490097 + 2.0 * 6.018356800079346
Epoch 830, val loss: 0.7547163367271423
Epoch 840, training loss: 12.146105766296387 = 0.10553036630153656 + 2.0 * 6.02028751373291
Epoch 840, val loss: 0.7592474818229675
Epoch 850, training loss: 12.135859489440918 = 0.10150757431983948 + 2.0 * 6.017176151275635
Epoch 850, val loss: 0.7639104723930359
Epoch 860, training loss: 12.132980346679688 = 0.09768779575824738 + 2.0 * 6.017646312713623
Epoch 860, val loss: 0.7687050104141235
Epoch 870, training loss: 12.12922477722168 = 0.09405074268579483 + 2.0 * 6.017587184906006
Epoch 870, val loss: 0.7735840678215027
Epoch 880, training loss: 12.123025894165039 = 0.09057589620351791 + 2.0 * 6.0162248611450195
Epoch 880, val loss: 0.7783403992652893
Epoch 890, training loss: 12.117121696472168 = 0.08728165924549103 + 2.0 * 6.014920234680176
Epoch 890, val loss: 0.7833065390586853
Epoch 900, training loss: 12.11319351196289 = 0.0841345340013504 + 2.0 * 6.014529705047607
Epoch 900, val loss: 0.7883785367012024
Epoch 910, training loss: 12.122842788696289 = 0.08110888302326202 + 2.0 * 6.020866870880127
Epoch 910, val loss: 0.7934294939041138
Epoch 920, training loss: 12.112007141113281 = 0.07826464623212814 + 2.0 * 6.016871452331543
Epoch 920, val loss: 0.7984558343887329
Epoch 930, training loss: 12.100165367126465 = 0.07554366439580917 + 2.0 * 6.012310981750488
Epoch 930, val loss: 0.8037005662918091
Epoch 940, training loss: 12.097777366638184 = 0.07293997704982758 + 2.0 * 6.012418746948242
Epoch 940, val loss: 0.8089441061019897
Epoch 950, training loss: 12.110380172729492 = 0.07045089453458786 + 2.0 * 6.019964694976807
Epoch 950, val loss: 0.8141458034515381
Epoch 960, training loss: 12.095149993896484 = 0.06808006018400192 + 2.0 * 6.013535022735596
Epoch 960, val loss: 0.8194631934165955
Epoch 970, training loss: 12.086397171020508 = 0.06582440435886383 + 2.0 * 6.010286331176758
Epoch 970, val loss: 0.8248013257980347
Epoch 980, training loss: 12.085088729858398 = 0.06365739554166794 + 2.0 * 6.010715484619141
Epoch 980, val loss: 0.8301301598548889
Epoch 990, training loss: 12.09949779510498 = 0.06158774346113205 + 2.0 * 6.018955230712891
Epoch 990, val loss: 0.835463285446167
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.8376
Flip ASR: 0.8178/225 nodes
The final ASR:0.54121, 0.24721, Accuracy:0.81605, 0.00924
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11602])
remove edge: torch.Size([2, 9528])
updated graph: torch.Size([2, 10574])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8481
Overall ASR: 0.9926
Flip ASR: 0.9911/225 nodes
The final ASR:0.98155, 0.00797, Accuracy:0.83704, 0.01090
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.3 der2:0.2 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 18.68701171875 = 1.9393514394760132 + 2.0 * 8.37382984161377
Epoch 0, val loss: 1.9308810234069824
Epoch 10, training loss: 18.67552947998047 = 1.9291913509368896 + 2.0 * 8.3731689453125
Epoch 10, val loss: 1.9212936162948608
Epoch 20, training loss: 18.654224395751953 = 1.9160891771316528 + 2.0 * 8.369067192077637
Epoch 20, val loss: 1.9085006713867188
Epoch 30, training loss: 18.58156394958496 = 1.8980613946914673 + 2.0 * 8.341751098632812
Epoch 30, val loss: 1.8907196521759033
Epoch 40, training loss: 18.20909881591797 = 1.876430869102478 + 2.0 * 8.16633415222168
Epoch 40, val loss: 1.870072603225708
Epoch 50, training loss: 16.826356887817383 = 1.8532423973083496 + 2.0 * 7.4865570068359375
Epoch 50, val loss: 1.848284125328064
Epoch 60, training loss: 15.95882511138916 = 1.836695909500122 + 2.0 * 7.061064720153809
Epoch 60, val loss: 1.8343636989593506
Epoch 70, training loss: 15.325737953186035 = 1.8267728090286255 + 2.0 * 6.74948263168335
Epoch 70, val loss: 1.825675129890442
Epoch 80, training loss: 15.04822826385498 = 1.8176862001419067 + 2.0 * 6.615271091461182
Epoch 80, val loss: 1.8178668022155762
Epoch 90, training loss: 14.888675689697266 = 1.8091986179351807 + 2.0 * 6.539738655090332
Epoch 90, val loss: 1.8101394176483154
Epoch 100, training loss: 14.778464317321777 = 1.8002080917358398 + 2.0 * 6.489128112792969
Epoch 100, val loss: 1.8020656108856201
Epoch 110, training loss: 14.665443420410156 = 1.7922087907791138 + 2.0 * 6.436617374420166
Epoch 110, val loss: 1.7949273586273193
Epoch 120, training loss: 14.564204216003418 = 1.7856310606002808 + 2.0 * 6.389286518096924
Epoch 120, val loss: 1.7892249822616577
Epoch 130, training loss: 14.475683212280273 = 1.779471755027771 + 2.0 * 6.3481059074401855
Epoch 130, val loss: 1.784037709236145
Epoch 140, training loss: 14.402718544006348 = 1.7725774049758911 + 2.0 * 6.315070629119873
Epoch 140, val loss: 1.7783616781234741
Epoch 150, training loss: 14.34687614440918 = 1.7645207643508911 + 2.0 * 6.291177749633789
Epoch 150, val loss: 1.771831750869751
Epoch 160, training loss: 14.300616264343262 = 1.7552051544189453 + 2.0 * 6.272705554962158
Epoch 160, val loss: 1.7642792463302612
Epoch 170, training loss: 14.254257202148438 = 1.744755744934082 + 2.0 * 6.254750728607178
Epoch 170, val loss: 1.7559274435043335
Epoch 180, training loss: 14.210768699645996 = 1.7330753803253174 + 2.0 * 6.238846778869629
Epoch 180, val loss: 1.7466990947723389
Epoch 190, training loss: 14.168848037719727 = 1.7197481393814087 + 2.0 * 6.224549770355225
Epoch 190, val loss: 1.7362334728240967
Epoch 200, training loss: 14.12964916229248 = 1.7042195796966553 + 2.0 * 6.212714672088623
Epoch 200, val loss: 1.724149227142334
Epoch 210, training loss: 14.086472511291504 = 1.6861649751663208 + 2.0 * 6.200153827667236
Epoch 210, val loss: 1.7101821899414062
Epoch 220, training loss: 14.04422664642334 = 1.6651356220245361 + 2.0 * 6.189545631408691
Epoch 220, val loss: 1.6939246654510498
Epoch 230, training loss: 13.998656272888184 = 1.640461802482605 + 2.0 * 6.1790971755981445
Epoch 230, val loss: 1.6747058629989624
Epoch 240, training loss: 13.954263687133789 = 1.6114338636398315 + 2.0 * 6.171414852142334
Epoch 240, val loss: 1.6521799564361572
Epoch 250, training loss: 13.901846885681152 = 1.5777140855789185 + 2.0 * 6.162066459655762
Epoch 250, val loss: 1.625854730606079
Epoch 260, training loss: 13.847355842590332 = 1.5386320352554321 + 2.0 * 6.154361724853516
Epoch 260, val loss: 1.5950840711593628
Epoch 270, training loss: 13.791048049926758 = 1.4939335584640503 + 2.0 * 6.148557186126709
Epoch 270, val loss: 1.559618592262268
Epoch 280, training loss: 13.730433464050293 = 1.4443405866622925 + 2.0 * 6.1430463790893555
Epoch 280, val loss: 1.5200682878494263
Epoch 290, training loss: 13.66568660736084 = 1.3902181386947632 + 2.0 * 6.137734413146973
Epoch 290, val loss: 1.4766830205917358
Epoch 300, training loss: 13.606849670410156 = 1.3327934741973877 + 2.0 * 6.137028217315674
Epoch 300, val loss: 1.4305474758148193
Epoch 310, training loss: 13.53490161895752 = 1.2745236158370972 + 2.0 * 6.130188941955566
Epoch 310, val loss: 1.383834719657898
Epoch 320, training loss: 13.467432975769043 = 1.2169831991195679 + 2.0 * 6.125225067138672
Epoch 320, val loss: 1.3378031253814697
Epoch 330, training loss: 13.403057098388672 = 1.161252498626709 + 2.0 * 6.1209025382995605
Epoch 330, val loss: 1.2934099435806274
Epoch 340, training loss: 13.350530624389648 = 1.1089253425598145 + 2.0 * 6.120802402496338
Epoch 340, val loss: 1.251997470855713
Epoch 350, training loss: 13.2904052734375 = 1.0615781545639038 + 2.0 * 6.114413738250732
Epoch 350, val loss: 1.2149789333343506
Epoch 360, training loss: 13.238872528076172 = 1.0178180932998657 + 2.0 * 6.110527038574219
Epoch 360, val loss: 1.1810322999954224
Epoch 370, training loss: 13.19033145904541 = 0.9769220948219299 + 2.0 * 6.1067047119140625
Epoch 370, val loss: 1.1494276523590088
Epoch 380, training loss: 13.14755630493164 = 0.9382768273353577 + 2.0 * 6.104639530181885
Epoch 380, val loss: 1.1200345754623413
Epoch 390, training loss: 13.110342979431152 = 0.901650071144104 + 2.0 * 6.10434627532959
Epoch 390, val loss: 1.0927397012710571
Epoch 400, training loss: 13.068960189819336 = 0.8670387268066406 + 2.0 * 6.100960731506348
Epoch 400, val loss: 1.0672543048858643
Epoch 410, training loss: 13.0260648727417 = 0.83380126953125 + 2.0 * 6.096131801605225
Epoch 410, val loss: 1.0430629253387451
Epoch 420, training loss: 12.986367225646973 = 0.8011100888252258 + 2.0 * 6.092628479003906
Epoch 420, val loss: 1.019375205039978
Epoch 430, training loss: 12.948151588439941 = 0.7687823176383972 + 2.0 * 6.08968448638916
Epoch 430, val loss: 0.9960610866546631
Epoch 440, training loss: 12.911385536193848 = 0.7366820573806763 + 2.0 * 6.0873517990112305
Epoch 440, val loss: 0.9731042981147766
Epoch 450, training loss: 12.878745079040527 = 0.7049828767776489 + 2.0 * 6.086881160736084
Epoch 450, val loss: 0.9505018591880798
Epoch 460, training loss: 12.847546577453613 = 0.674424409866333 + 2.0 * 6.08656120300293
Epoch 460, val loss: 0.9287467002868652
Epoch 470, training loss: 12.80643367767334 = 0.6445847749710083 + 2.0 * 6.0809245109558105
Epoch 470, val loss: 0.9077239632606506
Epoch 480, training loss: 12.77355670928955 = 0.6153227090835571 + 2.0 * 6.0791168212890625
Epoch 480, val loss: 0.8871723413467407
Epoch 490, training loss: 12.743282318115234 = 0.5866811275482178 + 2.0 * 6.078300476074219
Epoch 490, val loss: 0.8672723174095154
Epoch 500, training loss: 12.719600677490234 = 0.5591055750846863 + 2.0 * 6.080247402191162
Epoch 500, val loss: 0.8483085632324219
Epoch 510, training loss: 12.681232452392578 = 0.532572329044342 + 2.0 * 6.074329853057861
Epoch 510, val loss: 0.8305952548980713
Epoch 520, training loss: 12.650053977966309 = 0.5068357586860657 + 2.0 * 6.071609020233154
Epoch 520, val loss: 0.8135915994644165
Epoch 530, training loss: 12.637103080749512 = 0.4818335473537445 + 2.0 * 6.077634811401367
Epoch 530, val loss: 0.7973938584327698
Epoch 540, training loss: 12.594476699829102 = 0.45790937542915344 + 2.0 * 6.068283557891846
Epoch 540, val loss: 0.7823521494865417
Epoch 550, training loss: 12.569839477539062 = 0.4346866011619568 + 2.0 * 6.0675764083862305
Epoch 550, val loss: 0.7682974338531494
Epoch 560, training loss: 12.546806335449219 = 0.4121686518192291 + 2.0 * 6.067318916320801
Epoch 560, val loss: 0.7549067735671997
Epoch 570, training loss: 12.520124435424805 = 0.3904392421245575 + 2.0 * 6.064842700958252
Epoch 570, val loss: 0.7426644563674927
Epoch 580, training loss: 12.495172500610352 = 0.36929941177368164 + 2.0 * 6.062936782836914
Epoch 580, val loss: 0.7312306761741638
Epoch 590, training loss: 12.492074012756348 = 0.34876349568367004 + 2.0 * 6.0716552734375
Epoch 590, val loss: 0.7205250859260559
Epoch 600, training loss: 12.454849243164062 = 0.3291625380516052 + 2.0 * 6.062843322753906
Epoch 600, val loss: 0.710686445236206
Epoch 610, training loss: 12.428607940673828 = 0.3103390038013458 + 2.0 * 6.059134483337402
Epoch 610, val loss: 0.7019056677818298
Epoch 620, training loss: 12.406682014465332 = 0.2921997010707855 + 2.0 * 6.057240962982178
Epoch 620, val loss: 0.6938112378120422
Epoch 630, training loss: 12.401862144470215 = 0.27487170696258545 + 2.0 * 6.06349515914917
Epoch 630, val loss: 0.6864955425262451
Epoch 640, training loss: 12.370796203613281 = 0.25867724418640137 + 2.0 * 6.05605936050415
Epoch 640, val loss: 0.6802517175674438
Epoch 650, training loss: 12.35069465637207 = 0.2433623969554901 + 2.0 * 6.053666114807129
Epoch 650, val loss: 0.6748700737953186
Epoch 660, training loss: 12.332916259765625 = 0.22887016832828522 + 2.0 * 6.052022933959961
Epoch 660, val loss: 0.6700674891471863
Epoch 670, training loss: 12.364992141723633 = 0.21529832482337952 + 2.0 * 6.0748467445373535
Epoch 670, val loss: 0.6659801006317139
Epoch 680, training loss: 12.303086280822754 = 0.20278765261173248 + 2.0 * 6.050149440765381
Epoch 680, val loss: 0.6627383232116699
Epoch 690, training loss: 12.291337013244629 = 0.19117936491966248 + 2.0 * 6.050078868865967
Epoch 690, val loss: 0.6605421900749207
Epoch 700, training loss: 12.275282859802246 = 0.1803313046693802 + 2.0 * 6.047475814819336
Epoch 700, val loss: 0.6585508584976196
Epoch 710, training loss: 12.26230525970459 = 0.17013990879058838 + 2.0 * 6.046082496643066
Epoch 710, val loss: 0.6572020053863525
Epoch 720, training loss: 12.251494407653809 = 0.16060231626033783 + 2.0 * 6.045445919036865
Epoch 720, val loss: 0.6564621329307556
Epoch 730, training loss: 12.242127418518066 = 0.1517619490623474 + 2.0 * 6.045182704925537
Epoch 730, val loss: 0.6561260223388672
Epoch 740, training loss: 12.230501174926758 = 0.14363345503807068 + 2.0 * 6.043433666229248
Epoch 740, val loss: 0.6564418077468872
Epoch 750, training loss: 12.220354080200195 = 0.13605913519859314 + 2.0 * 6.042147636413574
Epoch 750, val loss: 0.6571370959281921
Epoch 760, training loss: 12.210832595825195 = 0.12896451354026794 + 2.0 * 6.040934085845947
Epoch 760, val loss: 0.6580533385276794
Epoch 770, training loss: 12.227173805236816 = 0.12232241779565811 + 2.0 * 6.052425861358643
Epoch 770, val loss: 0.6593426465988159
Epoch 780, training loss: 12.196167945861816 = 0.11623029410839081 + 2.0 * 6.039968967437744
Epoch 780, val loss: 0.6609585285186768
Epoch 790, training loss: 12.18748664855957 = 0.11054594069719315 + 2.0 * 6.038470268249512
Epoch 790, val loss: 0.6630875468254089
Epoch 800, training loss: 12.180045127868652 = 0.10521353781223297 + 2.0 * 6.037415981292725
Epoch 800, val loss: 0.6652504801750183
Epoch 810, training loss: 12.202449798583984 = 0.10022072494029999 + 2.0 * 6.051114559173584
Epoch 810, val loss: 0.6675949096679688
Epoch 820, training loss: 12.17289924621582 = 0.09559670090675354 + 2.0 * 6.038651466369629
Epoch 820, val loss: 0.6702038049697876
Epoch 830, training loss: 12.162723541259766 = 0.09127483516931534 + 2.0 * 6.03572416305542
Epoch 830, val loss: 0.6731696128845215
Epoch 840, training loss: 12.157071113586426 = 0.08720225840806961 + 2.0 * 6.0349345207214355
Epoch 840, val loss: 0.6760799288749695
Epoch 850, training loss: 12.156932830810547 = 0.08337253332138062 + 2.0 * 6.03678035736084
Epoch 850, val loss: 0.6791352033615112
Epoch 860, training loss: 12.14660930633545 = 0.07978761196136475 + 2.0 * 6.033411026000977
Epoch 860, val loss: 0.6825048327445984
Epoch 870, training loss: 12.14047908782959 = 0.07639847695827484 + 2.0 * 6.032040119171143
Epoch 870, val loss: 0.6859195232391357
Epoch 880, training loss: 12.168780326843262 = 0.07320988923311234 + 2.0 * 6.04778528213501
Epoch 880, val loss: 0.6893253922462463
Epoch 890, training loss: 12.13394832611084 = 0.07023240625858307 + 2.0 * 6.031857967376709
Epoch 890, val loss: 0.6927367448806763
Epoch 900, training loss: 12.126805305480957 = 0.06744050234556198 + 2.0 * 6.029682636260986
Epoch 900, val loss: 0.6965559124946594
Epoch 910, training loss: 12.121826171875 = 0.06478627026081085 + 2.0 * 6.028520107269287
Epoch 910, val loss: 0.7001599073410034
Epoch 920, training loss: 12.118487358093262 = 0.062261100858449936 + 2.0 * 6.02811336517334
Epoch 920, val loss: 0.7038737535476685
Epoch 930, training loss: 12.135693550109863 = 0.059872645884752274 + 2.0 * 6.037910461425781
Epoch 930, val loss: 0.707563579082489
Epoch 940, training loss: 12.113996505737305 = 0.05763046443462372 + 2.0 * 6.0281829833984375
Epoch 940, val loss: 0.7114009857177734
Epoch 950, training loss: 12.10933780670166 = 0.05551150441169739 + 2.0 * 6.026913166046143
Epoch 950, val loss: 0.7153611183166504
Epoch 960, training loss: 12.118019104003906 = 0.05349672958254814 + 2.0 * 6.032261371612549
Epoch 960, val loss: 0.7190716862678528
Epoch 970, training loss: 12.103419303894043 = 0.051606785506010056 + 2.0 * 6.025906085968018
Epoch 970, val loss: 0.7229772806167603
Epoch 980, training loss: 12.098015785217285 = 0.049798499792814255 + 2.0 * 6.024108409881592
Epoch 980, val loss: 0.7269132137298584
Epoch 990, training loss: 12.09534740447998 = 0.04807116836309433 + 2.0 * 6.0236382484436035
Epoch 990, val loss: 0.7307945489883423
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 730, in <module>
    torch.cuda.empty_cache()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/cuda/memory.py", line 121, in empty_cache
    torch._C._cuda_emptyCache()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.3 der2:0.2 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 18.69180679321289 = 1.9439918994903564 + 2.0 * 8.373907089233398
Epoch 0, val loss: 1.9355990886688232
Epoch 10, training loss: 18.681325912475586 = 1.9342715740203857 + 2.0 * 8.373527526855469
Epoch 10, val loss: 1.9265103340148926
Epoch 20, training loss: 18.663331985473633 = 1.9224274158477783 + 2.0 * 8.370451927185059
Epoch 20, val loss: 1.9149376153945923
Epoch 30, training loss: 18.603525161743164 = 1.906526803970337 + 2.0 * 8.348499298095703
Epoch 30, val loss: 1.8991535902023315
Epoch 40, training loss: 18.314138412475586 = 1.8868954181671143 + 2.0 * 8.213621139526367
Epoch 40, val loss: 1.880477786064148
Epoch 50, training loss: 17.323020935058594 = 1.8659712076187134 + 2.0 * 7.728525161743164
Epoch 50, val loss: 1.861138105392456
Epoch 60, training loss: 16.47931671142578 = 1.8476139307022095 + 2.0 * 7.31585168838501
Epoch 60, val loss: 1.8442316055297852
Epoch 70, training loss: 15.693537712097168 = 1.8343439102172852 + 2.0 * 6.929596900939941
Epoch 70, val loss: 1.832581639289856
Epoch 80, training loss: 15.340160369873047 = 1.8222593069076538 + 2.0 * 6.758950710296631
Epoch 80, val loss: 1.8218157291412354
Epoch 90, training loss: 15.116700172424316 = 1.8090728521347046 + 2.0 * 6.65381383895874
Epoch 90, val loss: 1.809906005859375
Epoch 100, training loss: 14.930760383605957 = 1.7947922945022583 + 2.0 * 6.567984104156494
Epoch 100, val loss: 1.7976763248443604
Epoch 110, training loss: 14.768157005310059 = 1.7818118333816528 + 2.0 * 6.493172645568848
Epoch 110, val loss: 1.7867439985275269
Epoch 120, training loss: 14.642924308776855 = 1.7698516845703125 + 2.0 * 6.4365363121032715
Epoch 120, val loss: 1.7767043113708496
Epoch 130, training loss: 14.53923225402832 = 1.7573049068450928 + 2.0 * 6.390963554382324
Epoch 130, val loss: 1.7661184072494507
Epoch 140, training loss: 14.452571868896484 = 1.7432866096496582 + 2.0 * 6.354642391204834
Epoch 140, val loss: 1.7544530630111694
Epoch 150, training loss: 14.37542724609375 = 1.7278800010681152 + 2.0 * 6.3237738609313965
Epoch 150, val loss: 1.7412832975387573
Epoch 160, training loss: 14.311118125915527 = 1.710515022277832 + 2.0 * 6.300301551818848
Epoch 160, val loss: 1.7265678644180298
Epoch 170, training loss: 14.246904373168945 = 1.6907662153244019 + 2.0 * 6.278069019317627
Epoch 170, val loss: 1.710074782371521
Epoch 180, training loss: 14.187403678894043 = 1.6685162782669067 + 2.0 * 6.259443759918213
Epoch 180, val loss: 1.691678762435913
Epoch 190, training loss: 14.128290176391602 = 1.6432892084121704 + 2.0 * 6.242500305175781
Epoch 190, val loss: 1.6709171533584595
Epoch 200, training loss: 14.074090003967285 = 1.6146243810653687 + 2.0 * 6.229732990264893
Epoch 200, val loss: 1.6475001573562622
Epoch 210, training loss: 14.014388084411621 = 1.582474708557129 + 2.0 * 6.215956687927246
Epoch 210, val loss: 1.621543526649475
Epoch 220, training loss: 13.953855514526367 = 1.546930193901062 + 2.0 * 6.203462600708008
Epoch 220, val loss: 1.592680811882019
Epoch 230, training loss: 13.896931648254395 = 1.5077952146530151 + 2.0 * 6.194568157196045
Epoch 230, val loss: 1.561069369316101
Epoch 240, training loss: 13.8351411819458 = 1.4654430150985718 + 2.0 * 6.184849262237549
Epoch 240, val loss: 1.5274370908737183
Epoch 250, training loss: 13.773616790771484 = 1.4207051992416382 + 2.0 * 6.176455974578857
Epoch 250, val loss: 1.4919103384017944
Epoch 260, training loss: 13.710866928100586 = 1.3736350536346436 + 2.0 * 6.168615818023682
Epoch 260, val loss: 1.4550074338912964
Epoch 270, training loss: 13.649717330932617 = 1.3252626657485962 + 2.0 * 6.162227153778076
Epoch 270, val loss: 1.4173924922943115
Epoch 280, training loss: 13.594217300415039 = 1.2767369747161865 + 2.0 * 6.158740043640137
Epoch 280, val loss: 1.379951000213623
Epoch 290, training loss: 13.52956771850586 = 1.228822946548462 + 2.0 * 6.150372505187988
Epoch 290, val loss: 1.3436728715896606
Epoch 300, training loss: 13.47179889678955 = 1.1825207471847534 + 2.0 * 6.144639015197754
Epoch 300, val loss: 1.3084895610809326
Epoch 310, training loss: 13.418663024902344 = 1.1379597187042236 + 2.0 * 6.14035177230835
Epoch 310, val loss: 1.2749637365341187
Epoch 320, training loss: 13.373082160949707 = 1.095885992050171 + 2.0 * 6.1385979652404785
Epoch 320, val loss: 1.2436091899871826
Epoch 330, training loss: 13.320178985595703 = 1.0566091537475586 + 2.0 * 6.131784915924072
Epoch 330, val loss: 1.2145134210586548
Epoch 340, training loss: 13.273411750793457 = 1.019668698310852 + 2.0 * 6.126871585845947
Epoch 340, val loss: 1.1873855590820312
Epoch 350, training loss: 13.231176376342773 = 0.9846522212028503 + 2.0 * 6.12326192855835
Epoch 350, val loss: 1.1618595123291016
Epoch 360, training loss: 13.19406509399414 = 0.9513496160507202 + 2.0 * 6.1213579177856445
Epoch 360, val loss: 1.1378097534179688
Epoch 370, training loss: 13.154790878295898 = 0.9196968674659729 + 2.0 * 6.117547035217285
Epoch 370, val loss: 1.115212082862854
Epoch 380, training loss: 13.114683151245117 = 0.8891563415527344 + 2.0 * 6.112763404846191
Epoch 380, val loss: 1.0936473608016968
Epoch 390, training loss: 13.0803804397583 = 0.8593292832374573 + 2.0 * 6.110525608062744
Epoch 390, val loss: 1.0728662014007568
Epoch 400, training loss: 13.058006286621094 = 0.8301593661308289 + 2.0 * 6.1139235496521
Epoch 400, val loss: 1.052870512008667
Epoch 410, training loss: 13.015478134155273 = 0.8020188212394714 + 2.0 * 6.106729507446289
Epoch 410, val loss: 1.033568263053894
Epoch 420, training loss: 12.978231430053711 = 0.7743902206420898 + 2.0 * 6.1019206047058105
Epoch 420, val loss: 1.0150294303894043
Epoch 430, training loss: 12.946207046508789 = 0.7471976280212402 + 2.0 * 6.099504470825195
Epoch 430, val loss: 0.9969341158866882
Epoch 440, training loss: 12.914280891418457 = 0.7203692197799683 + 2.0 * 6.0969557762146
Epoch 440, val loss: 0.9792104363441467
Epoch 450, training loss: 12.883265495300293 = 0.6938679218292236 + 2.0 * 6.094698905944824
Epoch 450, val loss: 0.961891770362854
Epoch 460, training loss: 12.879678726196289 = 0.6677203178405762 + 2.0 * 6.1059794425964355
Epoch 460, val loss: 0.9449569582939148
Epoch 470, training loss: 12.8319673538208 = 0.6425250768661499 + 2.0 * 6.09472131729126
Epoch 470, val loss: 0.9286612272262573
Epoch 480, training loss: 12.797812461853027 = 0.6179816126823425 + 2.0 * 6.0899152755737305
Epoch 480, val loss: 0.9130504131317139
Epoch 490, training loss: 12.768057823181152 = 0.593998372554779 + 2.0 * 6.087029933929443
Epoch 490, val loss: 0.8980425596237183
Epoch 500, training loss: 12.740835189819336 = 0.5705797076225281 + 2.0 * 6.085127830505371
Epoch 500, val loss: 0.8835394978523254
Epoch 510, training loss: 12.717389106750488 = 0.5477050542831421 + 2.0 * 6.084842205047607
Epoch 510, val loss: 0.8695967197418213
Epoch 520, training loss: 12.700920104980469 = 0.5255316495895386 + 2.0 * 6.08769416809082
Epoch 520, val loss: 0.8562485575675964
Epoch 530, training loss: 12.665493965148926 = 0.504108726978302 + 2.0 * 6.080692768096924
Epoch 530, val loss: 0.8437535166740417
Epoch 540, training loss: 12.641707420349121 = 0.483181893825531 + 2.0 * 6.079262733459473
Epoch 540, val loss: 0.8318743705749512
Epoch 550, training loss: 12.617303848266602 = 0.4626433253288269 + 2.0 * 6.077330112457275
Epoch 550, val loss: 0.8204922676086426
Epoch 560, training loss: 12.608172416687012 = 0.4423637092113495 + 2.0 * 6.08290433883667
Epoch 560, val loss: 0.8096392750740051
Epoch 570, training loss: 12.571867942810059 = 0.42255276441574097 + 2.0 * 6.074657440185547
Epoch 570, val loss: 0.7992720603942871
Epoch 580, training loss: 12.549814224243164 = 0.402898371219635 + 2.0 * 6.073457717895508
Epoch 580, val loss: 0.7894757986068726
Epoch 590, training loss: 12.52866268157959 = 0.38343384861946106 + 2.0 * 6.0726141929626465
Epoch 590, val loss: 0.7801584005355835
Epoch 600, training loss: 12.508431434631348 = 0.36420953273773193 + 2.0 * 6.072111129760742
Epoch 600, val loss: 0.7712838053703308
Epoch 610, training loss: 12.483677864074707 = 0.3452443480491638 + 2.0 * 6.069216728210449
Epoch 610, val loss: 0.7630366683006287
Epoch 620, training loss: 12.467106819152832 = 0.3266465365886688 + 2.0 * 6.070230007171631
Epoch 620, val loss: 0.7553130984306335
Epoch 630, training loss: 12.444280624389648 = 0.30848148465156555 + 2.0 * 6.067899703979492
Epoch 630, val loss: 0.7481072545051575
Epoch 640, training loss: 12.426843643188477 = 0.2908842861652374 + 2.0 * 6.06797981262207
Epoch 640, val loss: 0.7416080832481384
Epoch 650, training loss: 12.40339183807373 = 0.27390792965888977 + 2.0 * 6.064742088317871
Epoch 650, val loss: 0.7357445359230042
Epoch 660, training loss: 12.385631561279297 = 0.25763246417045593 + 2.0 * 6.063999652862549
Epoch 660, val loss: 0.7306463122367859
Epoch 670, training loss: 12.369230270385742 = 0.2422032356262207 + 2.0 * 6.06351375579834
Epoch 670, val loss: 0.7262763977050781
Epoch 680, training loss: 12.350937843322754 = 0.22768260538578033 + 2.0 * 6.061627388000488
Epoch 680, val loss: 0.7227115035057068
Epoch 690, training loss: 12.340104103088379 = 0.2140774428844452 + 2.0 * 6.063013553619385
Epoch 690, val loss: 0.7197994589805603
Epoch 700, training loss: 12.323352813720703 = 0.20135964453220367 + 2.0 * 6.0609965324401855
Epoch 700, val loss: 0.717695415019989
Epoch 710, training loss: 12.306181907653809 = 0.1895860880613327 + 2.0 * 6.058298110961914
Epoch 710, val loss: 0.7163200378417969
Epoch 720, training loss: 12.29236888885498 = 0.1786188781261444 + 2.0 * 6.056875228881836
Epoch 720, val loss: 0.715482771396637
Epoch 730, training loss: 12.289482116699219 = 0.16843415796756744 + 2.0 * 6.060523986816406
Epoch 730, val loss: 0.7152529358863831
Epoch 740, training loss: 12.270402908325195 = 0.15892393887043 + 2.0 * 6.055739402770996
Epoch 740, val loss: 0.7155279517173767
Epoch 750, training loss: 12.257479667663574 = 0.1501455456018448 + 2.0 * 6.053667068481445
Epoch 750, val loss: 0.7163938879966736
Epoch 760, training loss: 12.253694534301758 = 0.14192785322666168 + 2.0 * 6.055883407592773
Epoch 760, val loss: 0.7175761461257935
Epoch 770, training loss: 12.244197845458984 = 0.13437798619270325 + 2.0 * 6.054909706115723
Epoch 770, val loss: 0.7192003726959229
Epoch 780, training loss: 12.228262901306152 = 0.12727893888950348 + 2.0 * 6.050491809844971
Epoch 780, val loss: 0.7212907075881958
Epoch 790, training loss: 12.219941139221191 = 0.12070776522159576 + 2.0 * 6.049616813659668
Epoch 790, val loss: 0.7235831618309021
Epoch 800, training loss: 12.213427543640137 = 0.11459099501371384 + 2.0 * 6.0494184494018555
Epoch 800, val loss: 0.7262837886810303
Epoch 810, training loss: 12.204398155212402 = 0.10886967182159424 + 2.0 * 6.047764301300049
Epoch 810, val loss: 0.7291202545166016
Epoch 820, training loss: 12.201150894165039 = 0.1035432443022728 + 2.0 * 6.048803806304932
Epoch 820, val loss: 0.7324955463409424
Epoch 830, training loss: 12.190393447875977 = 0.09858492761850357 + 2.0 * 6.045904159545898
Epoch 830, val loss: 0.7358561754226685
Epoch 840, training loss: 12.184510231018066 = 0.09393137693405151 + 2.0 * 6.045289516448975
Epoch 840, val loss: 0.7396109700202942
Epoch 850, training loss: 12.18725299835205 = 0.08958248049020767 + 2.0 * 6.048835277557373
Epoch 850, val loss: 0.7434662580490112
Epoch 860, training loss: 12.173141479492188 = 0.0854882076382637 + 2.0 * 6.043826580047607
Epoch 860, val loss: 0.7474668622016907
Epoch 870, training loss: 12.166179656982422 = 0.0816667228937149 + 2.0 * 6.0422563552856445
Epoch 870, val loss: 0.7516264319419861
Epoch 880, training loss: 12.18274211883545 = 0.07806447148323059 + 2.0 * 6.052338600158691
Epoch 880, val loss: 0.7557891011238098
Epoch 890, training loss: 12.158440589904785 = 0.07472795248031616 + 2.0 * 6.041856288909912
Epoch 890, val loss: 0.7602320909500122
Epoch 900, training loss: 12.149922370910645 = 0.07156583666801453 + 2.0 * 6.039178371429443
Epoch 900, val loss: 0.7647252678871155
Epoch 910, training loss: 12.145834922790527 = 0.06859052926301956 + 2.0 * 6.0386223793029785
Epoch 910, val loss: 0.769200325012207
Epoch 920, training loss: 12.149168968200684 = 0.06577868014574051 + 2.0 * 6.0416951179504395
Epoch 920, val loss: 0.773758053779602
Epoch 930, training loss: 12.142402648925781 = 0.06314209848642349 + 2.0 * 6.03963041305542
Epoch 930, val loss: 0.7783479690551758
Epoch 940, training loss: 12.138789176940918 = 0.06064613536000252 + 2.0 * 6.039071559906006
Epoch 940, val loss: 0.7831248044967651
Epoch 950, training loss: 12.129318237304688 = 0.05828319117426872 + 2.0 * 6.035517692565918
Epoch 950, val loss: 0.7877196669578552
Epoch 960, training loss: 12.128639221191406 = 0.05604983866214752 + 2.0 * 6.036294460296631
Epoch 960, val loss: 0.7924357652664185
Epoch 970, training loss: 12.124126434326172 = 0.05393874645233154 + 2.0 * 6.035093784332275
Epoch 970, val loss: 0.7970976829528809
Epoch 980, training loss: 12.119235038757324 = 0.05193992331624031 + 2.0 * 6.033647537231445
Epoch 980, val loss: 0.8018476366996765
Epoch 990, training loss: 12.128209114074707 = 0.050037726759910583 + 2.0 * 6.039085865020752
Epoch 990, val loss: 0.8064332604408264
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 730, in <module>
    torch.cuda.empty_cache()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/cuda/memory.py", line 121, in empty_cache
    torch._C._cuda_emptyCache()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.3 der2:0.2 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 18.694334030151367 = 1.9466049671173096 + 2.0 * 8.37386417388916
Epoch 0, val loss: 1.9503178596496582
Epoch 10, training loss: 18.68291664123535 = 1.9362220764160156 + 2.0 * 8.373347282409668
Epoch 10, val loss: 1.9405187368392944
Epoch 20, training loss: 18.662086486816406 = 1.9231263399124146 + 2.0 * 8.36948013305664
Epoch 20, val loss: 1.927557110786438
Epoch 30, training loss: 18.587108612060547 = 1.9047478437423706 + 2.0 * 8.341180801391602
Epoch 30, val loss: 1.9091206789016724
Epoch 40, training loss: 18.20825958251953 = 1.8820418119430542 + 2.0 * 8.163108825683594
Epoch 40, val loss: 1.8874238729476929
Epoch 50, training loss: 17.27750587463379 = 1.8557993173599243 + 2.0 * 7.710853576660156
Epoch 50, val loss: 1.862399697303772
Epoch 60, training loss: 16.89307403564453 = 1.8310551643371582 + 2.0 * 7.531009197235107
Epoch 60, val loss: 1.839600682258606
Epoch 70, training loss: 16.26948356628418 = 1.8126682043075562 + 2.0 * 7.228407382965088
Epoch 70, val loss: 1.8231922388076782
Epoch 80, training loss: 15.529679298400879 = 1.8024181127548218 + 2.0 * 6.863630771636963
Epoch 80, val loss: 1.8140780925750732
Epoch 90, training loss: 15.207174301147461 = 1.793487310409546 + 2.0 * 6.706843376159668
Epoch 90, val loss: 1.8045964241027832
Epoch 100, training loss: 14.97542667388916 = 1.7805746793746948 + 2.0 * 6.597425937652588
Epoch 100, val loss: 1.7919484376907349
Epoch 110, training loss: 14.807971000671387 = 1.7683321237564087 + 2.0 * 6.519819259643555
Epoch 110, val loss: 1.780174970626831
Epoch 120, training loss: 14.686602592468262 = 1.7566125392913818 + 2.0 * 6.46499490737915
Epoch 120, val loss: 1.768491268157959
Epoch 130, training loss: 14.58535099029541 = 1.743823766708374 + 2.0 * 6.4207634925842285
Epoch 130, val loss: 1.7561938762664795
Epoch 140, training loss: 14.50543212890625 = 1.7295745611190796 + 2.0 * 6.3879289627075195
Epoch 140, val loss: 1.7432366609573364
Epoch 150, training loss: 14.4325532913208 = 1.7136104106903076 + 2.0 * 6.359471321105957
Epoch 150, val loss: 1.7294467687606812
Epoch 160, training loss: 14.356135368347168 = 1.6961559057235718 + 2.0 * 6.329989910125732
Epoch 160, val loss: 1.7146141529083252
Epoch 170, training loss: 14.278614044189453 = 1.6768043041229248 + 2.0 * 6.300904750823975
Epoch 170, val loss: 1.6982847452163696
Epoch 180, training loss: 14.209228515625 = 1.654935359954834 + 2.0 * 6.277146816253662
Epoch 180, val loss: 1.6799908876419067
Epoch 190, training loss: 14.150397300720215 = 1.6299608945846558 + 2.0 * 6.260218143463135
Epoch 190, val loss: 1.6592620611190796
Epoch 200, training loss: 14.080391883850098 = 1.6018645763397217 + 2.0 * 6.239263534545898
Epoch 200, val loss: 1.6360299587249756
Epoch 210, training loss: 14.02042007446289 = 1.5704500675201416 + 2.0 * 6.224985122680664
Epoch 210, val loss: 1.6102415323257446
Epoch 220, training loss: 13.960308074951172 = 1.5353082418441772 + 2.0 * 6.212500095367432
Epoch 220, val loss: 1.5818272829055786
Epoch 230, training loss: 13.900525093078613 = 1.4966766834259033 + 2.0 * 6.2019243240356445
Epoch 230, val loss: 1.5509425401687622
Epoch 240, training loss: 13.842884063720703 = 1.4559632539749146 + 2.0 * 6.193460464477539
Epoch 240, val loss: 1.519242286682129
Epoch 250, training loss: 13.778564453125 = 1.4136109352111816 + 2.0 * 6.182476997375488
Epoch 250, val loss: 1.4871950149536133
Epoch 260, training loss: 13.71680736541748 = 1.370184302330017 + 2.0 * 6.173311710357666
Epoch 260, val loss: 1.4547804594039917
Epoch 270, training loss: 13.660560607910156 = 1.3259083032608032 + 2.0 * 6.167325973510742
Epoch 270, val loss: 1.4225971698760986
Epoch 280, training loss: 13.6061429977417 = 1.28180730342865 + 2.0 * 6.162168025970459
Epoch 280, val loss: 1.391000747680664
Epoch 290, training loss: 13.5460786819458 = 1.2381542921066284 + 2.0 * 6.153962135314941
Epoch 290, val loss: 1.3607157468795776
Epoch 300, training loss: 13.491448402404785 = 1.1953734159469604 + 2.0 * 6.148037433624268
Epoch 300, val loss: 1.3312674760818481
Epoch 310, training loss: 13.439981460571289 = 1.153478980064392 + 2.0 * 6.143251419067383
Epoch 310, val loss: 1.3027784824371338
Epoch 320, training loss: 13.391212463378906 = 1.1127928495407104 + 2.0 * 6.139209747314453
Epoch 320, val loss: 1.275502324104309
Epoch 330, training loss: 13.340622901916504 = 1.073729395866394 + 2.0 * 6.13344669342041
Epoch 330, val loss: 1.2491859197616577
Epoch 340, training loss: 13.295004844665527 = 1.0359164476394653 + 2.0 * 6.129544258117676
Epoch 340, val loss: 1.223746418952942
Epoch 350, training loss: 13.25323486328125 = 0.9991320967674255 + 2.0 * 6.12705135345459
Epoch 350, val loss: 1.198883056640625
Epoch 360, training loss: 13.209430694580078 = 0.9632835388183594 + 2.0 * 6.123073577880859
Epoch 360, val loss: 1.1746891736984253
Epoch 370, training loss: 13.167928695678711 = 0.9284653663635254 + 2.0 * 6.119731903076172
Epoch 370, val loss: 1.1510227918624878
Epoch 380, training loss: 13.126363754272461 = 0.8943936824798584 + 2.0 * 6.115984916687012
Epoch 380, val loss: 1.1279963254928589
Epoch 390, training loss: 13.085885047912598 = 0.8607805967330933 + 2.0 * 6.112552165985107
Epoch 390, val loss: 1.1051983833312988
Epoch 400, training loss: 13.048317909240723 = 0.8275574445724487 + 2.0 * 6.110380172729492
Epoch 400, val loss: 1.082558274269104
Epoch 410, training loss: 13.010391235351562 = 0.7946926951408386 + 2.0 * 6.10784912109375
Epoch 410, val loss: 1.060287594795227
Epoch 420, training loss: 12.970354080200195 = 0.7620881795883179 + 2.0 * 6.104133129119873
Epoch 420, val loss: 1.038178563117981
Epoch 430, training loss: 12.947741508483887 = 0.7296563386917114 + 2.0 * 6.109042644500732
Epoch 430, val loss: 1.0163140296936035
Epoch 440, training loss: 12.898996353149414 = 0.6977730989456177 + 2.0 * 6.100611686706543
Epoch 440, val loss: 0.9949679970741272
Epoch 450, training loss: 12.861543655395508 = 0.6663949489593506 + 2.0 * 6.097574234008789
Epoch 450, val loss: 0.974350094795227
Epoch 460, training loss: 12.825249671936035 = 0.6355300545692444 + 2.0 * 6.094859600067139
Epoch 460, val loss: 0.9543309211730957
Epoch 470, training loss: 12.80616283416748 = 0.6053528189659119 + 2.0 * 6.100405216217041
Epoch 470, val loss: 0.9350546598434448
Epoch 480, training loss: 12.761343955993652 = 0.5761420130729675 + 2.0 * 6.0926008224487305
Epoch 480, val loss: 0.9170008897781372
Epoch 490, training loss: 12.726069450378418 = 0.5478744506835938 + 2.0 * 6.089097499847412
Epoch 490, val loss: 0.9001749157905579
Epoch 500, training loss: 12.700089454650879 = 0.5204203724861145 + 2.0 * 6.089834690093994
Epoch 500, val loss: 0.8845204710960388
Epoch 510, training loss: 12.669958114624023 = 0.494096577167511 + 2.0 * 6.087930679321289
Epoch 510, val loss: 0.8699948191642761
Epoch 520, training loss: 12.634795188903809 = 0.4685976803302765 + 2.0 * 6.083098888397217
Epoch 520, val loss: 0.8568854928016663
Epoch 530, training loss: 12.618618965148926 = 0.44394242763519287 + 2.0 * 6.087338447570801
Epoch 530, val loss: 0.8450137376785278
Epoch 540, training loss: 12.582245826721191 = 0.42056116461753845 + 2.0 * 6.0808424949646
Epoch 540, val loss: 0.8344228267669678
Epoch 550, training loss: 12.553709030151367 = 0.39802852272987366 + 2.0 * 6.077840328216553
Epoch 550, val loss: 0.8251921534538269
Epoch 560, training loss: 12.528196334838867 = 0.3762950897216797 + 2.0 * 6.075950622558594
Epoch 560, val loss: 0.817155122756958
Epoch 570, training loss: 12.517647743225098 = 0.3553752899169922 + 2.0 * 6.081136226654053
Epoch 570, val loss: 0.8101735711097717
Epoch 580, training loss: 12.482345581054688 = 0.3353884220123291 + 2.0 * 6.073478698730469
Epoch 580, val loss: 0.8043962121009827
Epoch 590, training loss: 12.460524559020996 = 0.31627658009529114 + 2.0 * 6.072124004364014
Epoch 590, val loss: 0.7998388409614563
Epoch 600, training loss: 12.441892623901367 = 0.29802098870277405 + 2.0 * 6.071935653686523
Epoch 600, val loss: 0.7964090704917908
Epoch 610, training loss: 12.423370361328125 = 0.2806088924407959 + 2.0 * 6.071380615234375
Epoch 610, val loss: 0.7940722703933716
Epoch 620, training loss: 12.398560523986816 = 0.264170378446579 + 2.0 * 6.067194938659668
Epoch 620, val loss: 0.7928183674812317
Epoch 630, training loss: 12.380559921264648 = 0.24854719638824463 + 2.0 * 6.066006183624268
Epoch 630, val loss: 0.7924998998641968
Epoch 640, training loss: 12.36829948425293 = 0.2337191253900528 + 2.0 * 6.067290306091309
Epoch 640, val loss: 0.7930435538291931
Epoch 650, training loss: 12.354582786560059 = 0.21971815824508667 + 2.0 * 6.067432403564453
Epoch 650, val loss: 0.7943851947784424
Epoch 660, training loss: 12.33250617980957 = 0.2066541314125061 + 2.0 * 6.062925815582275
Epoch 660, val loss: 0.7965142130851746
Epoch 670, training loss: 12.316606521606445 = 0.19439364969730377 + 2.0 * 6.061106204986572
Epoch 670, val loss: 0.7993865609169006
Epoch 680, training loss: 12.301008224487305 = 0.1828693002462387 + 2.0 * 6.059069633483887
Epoch 680, val loss: 0.802889347076416
Epoch 690, training loss: 12.297218322753906 = 0.17209598422050476 + 2.0 * 6.06256103515625
Epoch 690, val loss: 0.8070017695426941
Epoch 700, training loss: 12.282611846923828 = 0.1620565801858902 + 2.0 * 6.060277462005615
Epoch 700, val loss: 0.81162428855896
Epoch 710, training loss: 12.268265724182129 = 0.15276707708835602 + 2.0 * 6.057749271392822
Epoch 710, val loss: 0.8168214559555054
Epoch 720, training loss: 12.256884574890137 = 0.1441374570131302 + 2.0 * 6.056373596191406
Epoch 720, val loss: 0.8222882747650146
Epoch 730, training loss: 12.245548248291016 = 0.13618145883083344 + 2.0 * 6.054683208465576
Epoch 730, val loss: 0.8282219767570496
Epoch 740, training loss: 12.233524322509766 = 0.1287337690591812 + 2.0 * 6.052395343780518
Epoch 740, val loss: 0.8344430327415466
Epoch 750, training loss: 12.22433853149414 = 0.12182188034057617 + 2.0 * 6.051258563995361
Epoch 750, val loss: 0.8409726619720459
Epoch 760, training loss: 12.225970268249512 = 0.11539439857006073 + 2.0 * 6.055287837982178
Epoch 760, val loss: 0.8476822376251221
Epoch 770, training loss: 12.208931922912598 = 0.10943875461816788 + 2.0 * 6.049746513366699
Epoch 770, val loss: 0.8545622825622559
Epoch 780, training loss: 12.204461097717285 = 0.10388419777154922 + 2.0 * 6.050288677215576
Epoch 780, val loss: 0.8616190552711487
Epoch 790, training loss: 12.198234558105469 = 0.09873917698860168 + 2.0 * 6.049747467041016
Epoch 790, val loss: 0.8688185811042786
Epoch 800, training loss: 12.192005157470703 = 0.09396544843912125 + 2.0 * 6.049019813537598
Epoch 800, val loss: 0.8760413527488708
Epoch 810, training loss: 12.180843353271484 = 0.08952375501394272 + 2.0 * 6.045660018920898
Epoch 810, val loss: 0.8834128379821777
Epoch 820, training loss: 12.17404842376709 = 0.08535081893205643 + 2.0 * 6.04434871673584
Epoch 820, val loss: 0.8908568620681763
Epoch 830, training loss: 12.17496395111084 = 0.08145592361688614 + 2.0 * 6.046753883361816
Epoch 830, val loss: 0.8982962369918823
Epoch 840, training loss: 12.164703369140625 = 0.07782205194234848 + 2.0 * 6.043440818786621
Epoch 840, val loss: 0.9057909846305847
Epoch 850, training loss: 12.170353889465332 = 0.07442580908536911 + 2.0 * 6.047964096069336
Epoch 850, val loss: 0.9133477210998535
Epoch 860, training loss: 12.159940719604492 = 0.07124298810958862 + 2.0 * 6.04434871673584
Epoch 860, val loss: 0.9206187725067139
Epoch 870, training loss: 12.149009704589844 = 0.06827640533447266 + 2.0 * 6.0403666496276855
Epoch 870, val loss: 0.9281172156333923
Epoch 880, training loss: 12.143035888671875 = 0.06548293679952621 + 2.0 * 6.038776397705078
Epoch 880, val loss: 0.9355248808860779
Epoch 890, training loss: 12.137977600097656 = 0.06284724920988083 + 2.0 * 6.037565231323242
Epoch 890, val loss: 0.9429143667221069
Epoch 900, training loss: 12.138684272766113 = 0.060355085879564285 + 2.0 * 6.0391645431518555
Epoch 900, val loss: 0.9502298831939697
Epoch 910, training loss: 12.138666152954102 = 0.0580420047044754 + 2.0 * 6.04031229019165
Epoch 910, val loss: 0.9575842022895813
Epoch 920, training loss: 12.127068519592285 = 0.055844131857156754 + 2.0 * 6.035612106323242
Epoch 920, val loss: 0.9646691083908081
Epoch 930, training loss: 12.123563766479492 = 0.053780291229486465 + 2.0 * 6.034891605377197
Epoch 930, val loss: 0.9718224406242371
Epoch 940, training loss: 12.120965003967285 = 0.0518265925347805 + 2.0 * 6.034569263458252
Epoch 940, val loss: 0.9789124727249146
Epoch 950, training loss: 12.124725341796875 = 0.04998738318681717 + 2.0 * 6.0373687744140625
Epoch 950, val loss: 0.9858949780464172
Epoch 960, training loss: 12.118518829345703 = 0.04824664816260338 + 2.0 * 6.0351362228393555
Epoch 960, val loss: 0.9928195476531982
Epoch 970, training loss: 12.109920501708984 = 0.0465959794819355 + 2.0 * 6.031662464141846
Epoch 970, val loss: 0.9996731281280518
Epoch 980, training loss: 12.10714340209961 = 0.04503616690635681 + 2.0 * 6.03105354309082
Epoch 980, val loss: 1.0064795017242432
Epoch 990, training loss: 12.11084270477295 = 0.04354780167341232 + 2.0 * 6.033647537231445
Epoch 990, val loss: 1.0131906270980835
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 730, in <module>
    torch.cuda.empty_cache()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/cuda/memory.py", line 121, in empty_cache
    torch._C._cuda_emptyCache()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.3 der2:0.2 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 18.69194984436035 = 1.94417405128479 + 2.0 * 8.37388801574707
Epoch 0, val loss: 1.9466211795806885
Epoch 10, training loss: 18.680896759033203 = 1.9342291355133057 + 2.0 * 8.373333930969238
Epoch 10, val loss: 1.9372659921646118
Epoch 20, training loss: 18.65987777709961 = 1.9221032857894897 + 2.0 * 8.368886947631836
Epoch 20, val loss: 1.9254190921783447
Epoch 30, training loss: 18.572322845458984 = 1.9063280820846558 + 2.0 * 8.33299732208252
Epoch 30, val loss: 1.909927248954773
Epoch 40, training loss: 18.06655502319336 = 1.8875242471694946 + 2.0 * 8.089515686035156
Epoch 40, val loss: 1.8921645879745483
Epoch 50, training loss: 16.954139709472656 = 1.8672164678573608 + 2.0 * 7.543461322784424
Epoch 50, val loss: 1.8739352226257324
Epoch 60, training loss: 16.24360466003418 = 1.8510464429855347 + 2.0 * 7.196279525756836
Epoch 60, val loss: 1.8597996234893799
Epoch 70, training loss: 15.732150077819824 = 1.8365346193313599 + 2.0 * 6.947807788848877
Epoch 70, val loss: 1.846336007118225
Epoch 80, training loss: 15.368515968322754 = 1.8217875957489014 + 2.0 * 6.773364067077637
Epoch 80, val loss: 1.8332470655441284
Epoch 90, training loss: 15.120138168334961 = 1.8069616556167603 + 2.0 * 6.656588077545166
Epoch 90, val loss: 1.8202224969863892
Epoch 100, training loss: 14.929627418518066 = 1.7909260988235474 + 2.0 * 6.569350719451904
Epoch 100, val loss: 1.8059793710708618
Epoch 110, training loss: 14.751545906066895 = 1.7750645875930786 + 2.0 * 6.488240718841553
Epoch 110, val loss: 1.7921077013015747
Epoch 120, training loss: 14.636526107788086 = 1.7590553760528564 + 2.0 * 6.438735485076904
Epoch 120, val loss: 1.777897596359253
Epoch 130, training loss: 14.54195499420166 = 1.7406682968139648 + 2.0 * 6.400643348693848
Epoch 130, val loss: 1.7618998289108276
Epoch 140, training loss: 14.460210800170898 = 1.7210536003112793 + 2.0 * 6.3695783615112305
Epoch 140, val loss: 1.7445077896118164
Epoch 150, training loss: 14.381165504455566 = 1.6998827457427979 + 2.0 * 6.340641498565674
Epoch 150, val loss: 1.7258222103118896
Epoch 160, training loss: 14.30605697631836 = 1.6765071153640747 + 2.0 * 6.314774990081787
Epoch 160, val loss: 1.7053899765014648
Epoch 170, training loss: 14.22903823852539 = 1.6505242586135864 + 2.0 * 6.289257049560547
Epoch 170, val loss: 1.6830307245254517
Epoch 180, training loss: 14.155558586120605 = 1.6215981245040894 + 2.0 * 6.266980171203613
Epoch 180, val loss: 1.6582361459732056
Epoch 190, training loss: 14.084602355957031 = 1.5887490510940552 + 2.0 * 6.247926712036133
Epoch 190, val loss: 1.6302176713943481
Epoch 200, training loss: 14.016960144042969 = 1.5521880388259888 + 2.0 * 6.232386112213135
Epoch 200, val loss: 1.5989841222763062
Epoch 210, training loss: 13.947484016418457 = 1.5121124982833862 + 2.0 * 6.217685699462891
Epoch 210, val loss: 1.5648856163024902
Epoch 220, training loss: 13.877578735351562 = 1.4683127403259277 + 2.0 * 6.204632759094238
Epoch 220, val loss: 1.5276455879211426
Epoch 230, training loss: 13.810901641845703 = 1.4211597442626953 + 2.0 * 6.194870948791504
Epoch 230, val loss: 1.4880849123001099
Epoch 240, training loss: 13.740880966186523 = 1.3714089393615723 + 2.0 * 6.1847357749938965
Epoch 240, val loss: 1.446800947189331
Epoch 250, training loss: 13.676692962646484 = 1.3197293281555176 + 2.0 * 6.178481578826904
Epoch 250, val loss: 1.4044054746627808
Epoch 260, training loss: 13.610177993774414 = 1.267584204673767 + 2.0 * 6.171297073364258
Epoch 260, val loss: 1.3620920181274414
Epoch 270, training loss: 13.541650772094727 = 1.2159720659255981 + 2.0 * 6.162839412689209
Epoch 270, val loss: 1.32082998752594
Epoch 280, training loss: 13.477916717529297 = 1.1655964851379395 + 2.0 * 6.1561598777771
Epoch 280, val loss: 1.281297206878662
Epoch 290, training loss: 13.417263984680176 = 1.1167123317718506 + 2.0 * 6.150275707244873
Epoch 290, val loss: 1.2436532974243164
Epoch 300, training loss: 13.364237785339355 = 1.0697641372680664 + 2.0 * 6.1472368240356445
Epoch 300, val loss: 1.208328366279602
Epoch 310, training loss: 13.305159568786621 = 1.025694489479065 + 2.0 * 6.139732360839844
Epoch 310, val loss: 1.175588846206665
Epoch 320, training loss: 13.25434684753418 = 0.9838311672210693 + 2.0 * 6.135257720947266
Epoch 320, val loss: 1.1451830863952637
Epoch 330, training loss: 13.20544147491455 = 0.9437204599380493 + 2.0 * 6.130860328674316
Epoch 330, val loss: 1.1166354417800903
Epoch 340, training loss: 13.165627479553223 = 0.9050635099411011 + 2.0 * 6.130281925201416
Epoch 340, val loss: 1.0897408723831177
Epoch 350, training loss: 13.11439323425293 = 0.8679813146591187 + 2.0 * 6.12320613861084
Epoch 350, val loss: 1.06385338306427
Epoch 360, training loss: 13.068846702575684 = 0.8318836092948914 + 2.0 * 6.118481636047363
Epoch 360, val loss: 1.0391184091567993
Epoch 370, training loss: 13.028627395629883 = 0.7966133952140808 + 2.0 * 6.116006851196289
Epoch 370, val loss: 1.0153424739837646
Epoch 380, training loss: 12.987944602966309 = 0.7622969150543213 + 2.0 * 6.112823963165283
Epoch 380, val loss: 0.9922476410865784
Epoch 390, training loss: 12.946187973022461 = 0.7287581562995911 + 2.0 * 6.108715057373047
Epoch 390, val loss: 0.9700763821601868
Epoch 400, training loss: 12.906740188598633 = 0.6960355043411255 + 2.0 * 6.105352401733398
Epoch 400, val loss: 0.9487375020980835
Epoch 410, training loss: 12.872007369995117 = 0.6642462015151978 + 2.0 * 6.103880405426025
Epoch 410, val loss: 0.9283563494682312
Epoch 420, training loss: 12.838924407958984 = 0.6336215734481812 + 2.0 * 6.102651596069336
Epoch 420, val loss: 0.9092711806297302
Epoch 430, training loss: 12.798604011535645 = 0.6042044162750244 + 2.0 * 6.0971999168396
Epoch 430, val loss: 0.891711413860321
Epoch 440, training loss: 12.766480445861816 = 0.57602858543396 + 2.0 * 6.095225811004639
Epoch 440, val loss: 0.8756794333457947
Epoch 450, training loss: 12.741972923278809 = 0.5490861535072327 + 2.0 * 6.096443176269531
Epoch 450, val loss: 0.8611205816268921
Epoch 460, training loss: 12.707240104675293 = 0.5236014127731323 + 2.0 * 6.0918192863464355
Epoch 460, val loss: 0.8479870557785034
Epoch 470, training loss: 12.67543888092041 = 0.4991447329521179 + 2.0 * 6.088147163391113
Epoch 470, val loss: 0.8363533020019531
Epoch 480, training loss: 12.646463394165039 = 0.47575512528419495 + 2.0 * 6.085354328155518
Epoch 480, val loss: 0.8260176777839661
Epoch 490, training loss: 12.632553100585938 = 0.4532278776168823 + 2.0 * 6.089662551879883
Epoch 490, val loss: 0.8168405890464783
Epoch 500, training loss: 12.597949028015137 = 0.43161746859550476 + 2.0 * 6.083165645599365
Epoch 500, val loss: 0.8087170124053955
Epoch 510, training loss: 12.569280624389648 = 0.41074952483177185 + 2.0 * 6.079265594482422
Epoch 510, val loss: 0.801629900932312
Epoch 520, training loss: 12.546448707580566 = 0.3906017243862152 + 2.0 * 6.07792329788208
Epoch 520, val loss: 0.795468270778656
Epoch 530, training loss: 12.530009269714355 = 0.37104859948158264 + 2.0 * 6.079480171203613
Epoch 530, val loss: 0.7901710867881775
Epoch 540, training loss: 12.501752853393555 = 0.3522584140300751 + 2.0 * 6.074747085571289
Epoch 540, val loss: 0.785609781742096
Epoch 550, training loss: 12.479087829589844 = 0.33394819498062134 + 2.0 * 6.072569847106934
Epoch 550, val loss: 0.781795084476471
Epoch 560, training loss: 12.463136672973633 = 0.31626418232917786 + 2.0 * 6.073436260223389
Epoch 560, val loss: 0.7786526083946228
Epoch 570, training loss: 12.445540428161621 = 0.2991833984851837 + 2.0 * 6.073178291320801
Epoch 570, val loss: 0.7761145234107971
Epoch 580, training loss: 12.42021656036377 = 0.28296440839767456 + 2.0 * 6.0686259269714355
Epoch 580, val loss: 0.7743565440177917
Epoch 590, training loss: 12.400605201721191 = 0.26747435331344604 + 2.0 * 6.06656551361084
Epoch 590, val loss: 0.7731539011001587
Epoch 600, training loss: 12.391016960144043 = 0.25267571210861206 + 2.0 * 6.0691704750061035
Epoch 600, val loss: 0.7725207209587097
Epoch 610, training loss: 12.369025230407715 = 0.2387634664773941 + 2.0 * 6.065130710601807
Epoch 610, val loss: 0.7724205851554871
Epoch 620, training loss: 12.35017204284668 = 0.2255304902791977 + 2.0 * 6.062320709228516
Epoch 620, val loss: 0.7729254364967346
Epoch 630, training loss: 12.343339920043945 = 0.2130931168794632 + 2.0 * 6.065123558044434
Epoch 630, val loss: 0.7739410400390625
Epoch 640, training loss: 12.323734283447266 = 0.20151188969612122 + 2.0 * 6.061110973358154
Epoch 640, val loss: 0.7756274342536926
Epoch 650, training loss: 12.306363105773926 = 0.1906381994485855 + 2.0 * 6.057862281799316
Epoch 650, val loss: 0.7778147459030151
Epoch 660, training loss: 12.292769432067871 = 0.18043719232082367 + 2.0 * 6.056166172027588
Epoch 660, val loss: 0.780432939529419
Epoch 670, training loss: 12.284204483032227 = 0.17083343863487244 + 2.0 * 6.056685447692871
Epoch 670, val loss: 0.7835246324539185
Epoch 680, training loss: 12.273860931396484 = 0.1618797332048416 + 2.0 * 6.055990695953369
Epoch 680, val loss: 0.7869665026664734
Epoch 690, training loss: 12.261001586914062 = 0.15351378917694092 + 2.0 * 6.053743839263916
Epoch 690, val loss: 0.7908923029899597
Epoch 700, training loss: 12.248490333557129 = 0.14571677148342133 + 2.0 * 6.051386833190918
Epoch 700, val loss: 0.7951135039329529
Epoch 710, training loss: 12.246772766113281 = 0.1384308636188507 + 2.0 * 6.054171085357666
Epoch 710, val loss: 0.7997563481330872
Epoch 720, training loss: 12.233973503112793 = 0.13153880834579468 + 2.0 * 6.051217555999756
Epoch 720, val loss: 0.8046126365661621
Epoch 730, training loss: 12.225984573364258 = 0.12517178058624268 + 2.0 * 6.050406455993652
Epoch 730, val loss: 0.8099752068519592
Epoch 740, training loss: 12.212316513061523 = 0.11915998160839081 + 2.0 * 6.046578407287598
Epoch 740, val loss: 0.8154938220977783
Epoch 750, training loss: 12.207732200622559 = 0.11353658884763718 + 2.0 * 6.047097682952881
Epoch 750, val loss: 0.8213310837745667
Epoch 760, training loss: 12.200233459472656 = 0.10823296755552292 + 2.0 * 6.0460004806518555
Epoch 760, val loss: 0.8274340033531189
Epoch 770, training loss: 12.190905570983887 = 0.1032419502735138 + 2.0 * 6.043831825256348
Epoch 770, val loss: 0.8337910771369934
Epoch 780, training loss: 12.187335014343262 = 0.09856606274843216 + 2.0 * 6.044384479522705
Epoch 780, val loss: 0.8403918743133545
Epoch 790, training loss: 12.18506908416748 = 0.09416567534208298 + 2.0 * 6.045451641082764
Epoch 790, val loss: 0.8470346331596375
Epoch 800, training loss: 12.174158096313477 = 0.09002035856246948 + 2.0 * 6.042068958282471
Epoch 800, val loss: 0.8541507720947266
Epoch 810, training loss: 12.166047096252441 = 0.08611050993204117 + 2.0 * 6.039968490600586
Epoch 810, val loss: 0.8611865043640137
Epoch 820, training loss: 12.161397933959961 = 0.08242269605398178 + 2.0 * 6.039487838745117
Epoch 820, val loss: 0.868452250957489
Epoch 830, training loss: 12.1627836227417 = 0.0789409875869751 + 2.0 * 6.041921138763428
Epoch 830, val loss: 0.8758072257041931
Epoch 840, training loss: 12.163864135742188 = 0.07566098868846893 + 2.0 * 6.044101715087891
Epoch 840, val loss: 0.8833200931549072
Epoch 850, training loss: 12.150456428527832 = 0.07253200560808182 + 2.0 * 6.038962364196777
Epoch 850, val loss: 0.8908495903015137
Epoch 860, training loss: 12.14098834991455 = 0.06961856037378311 + 2.0 * 6.035685062408447
Epoch 860, val loss: 0.898592472076416
Epoch 870, training loss: 12.13608169555664 = 0.06685011088848114 + 2.0 * 6.034615993499756
Epoch 870, val loss: 0.9063135385513306
Epoch 880, training loss: 12.132628440856934 = 0.06421945244073868 + 2.0 * 6.034204483032227
Epoch 880, val loss: 0.9140962362289429
Epoch 890, training loss: 12.140959739685059 = 0.06172851473093033 + 2.0 * 6.039615631103516
Epoch 890, val loss: 0.921897828578949
Epoch 900, training loss: 12.12444019317627 = 0.059374623000621796 + 2.0 * 6.032532691955566
Epoch 900, val loss: 0.9297791719436646
Epoch 910, training loss: 12.123427391052246 = 0.05714995786547661 + 2.0 * 6.033138751983643
Epoch 910, val loss: 0.9376898407936096
Epoch 920, training loss: 12.126572608947754 = 0.055039796978235245 + 2.0 * 6.0357666015625
Epoch 920, val loss: 0.9455462098121643
Epoch 930, training loss: 12.116143226623535 = 0.05300939828157425 + 2.0 * 6.031567096710205
Epoch 930, val loss: 0.9534351229667664
Epoch 940, training loss: 12.112164497375488 = 0.05110742151737213 + 2.0 * 6.030528545379639
Epoch 940, val loss: 0.961309015750885
Epoch 950, training loss: 12.108591079711914 = 0.04929087311029434 + 2.0 * 6.0296502113342285
Epoch 950, val loss: 0.9691699147224426
Epoch 960, training loss: 12.110323905944824 = 0.047557611018419266 + 2.0 * 6.031383037567139
Epoch 960, val loss: 0.9770857095718384
Epoch 970, training loss: 12.104130744934082 = 0.045912038534879684 + 2.0 * 6.029109477996826
Epoch 970, val loss: 0.9848978519439697
Epoch 980, training loss: 12.099091529846191 = 0.04434261471033096 + 2.0 * 6.027374267578125
Epoch 980, val loss: 0.992756724357605
Epoch 990, training loss: 12.095137596130371 = 0.04285336285829544 + 2.0 * 6.026142120361328
Epoch 990, val loss: 1.0005124807357788
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 730, in <module>
    torch.cuda.empty_cache()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/cuda/memory.py", line 121, in empty_cache
    torch._C._cuda_emptyCache()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.3 der2:0.3 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 18.68402862548828 = 1.9364831447601318 + 2.0 * 8.373772621154785
Epoch 0, val loss: 1.9397655725479126
Epoch 10, training loss: 18.672359466552734 = 1.926705241203308 + 2.0 * 8.372827529907227
Epoch 10, val loss: 1.9303995370864868
Epoch 20, training loss: 18.64788818359375 = 1.9147729873657227 + 2.0 * 8.366558074951172
Epoch 20, val loss: 1.9185664653778076
Epoch 30, training loss: 18.539554595947266 = 1.8992894887924194 + 2.0 * 8.3201322555542
Epoch 30, val loss: 1.9031819105148315
Epoch 40, training loss: 17.777860641479492 = 1.8819115161895752 + 2.0 * 7.947974681854248
Epoch 40, val loss: 1.8860787153244019
Epoch 50, training loss: 16.1700382232666 = 1.8639105558395386 + 2.0 * 7.153063774108887
Epoch 50, val loss: 1.868659496307373
Epoch 60, training loss: 15.55351448059082 = 1.8518571853637695 + 2.0 * 6.850828647613525
Epoch 60, val loss: 1.857322096824646
Epoch 70, training loss: 15.171905517578125 = 1.8415579795837402 + 2.0 * 6.665173530578613
Epoch 70, val loss: 1.8466664552688599
Epoch 80, training loss: 14.96241283416748 = 1.8320926427841187 + 2.0 * 6.565160274505615
Epoch 80, val loss: 1.836710810661316
Epoch 90, training loss: 14.814598083496094 = 1.823712706565857 + 2.0 * 6.495442867279053
Epoch 90, val loss: 1.8277815580368042
Epoch 100, training loss: 14.667097091674805 = 1.8157095909118652 + 2.0 * 6.425693988800049
Epoch 100, val loss: 1.8196724653244019
Epoch 110, training loss: 14.547195434570312 = 1.8089388608932495 + 2.0 * 6.369128227233887
Epoch 110, val loss: 1.8127977848052979
Epoch 120, training loss: 14.454244613647461 = 1.8029899597167969 + 2.0 * 6.325627326965332
Epoch 120, val loss: 1.8065707683563232
Epoch 130, training loss: 14.376443862915039 = 1.7968453168869019 + 2.0 * 6.289799213409424
Epoch 130, val loss: 1.8002649545669556
Epoch 140, training loss: 14.316961288452148 = 1.7903475761413574 + 2.0 * 6.263307094573975
Epoch 140, val loss: 1.793889045715332
Epoch 150, training loss: 14.261733055114746 = 1.7834409475326538 + 2.0 * 6.2391462326049805
Epoch 150, val loss: 1.787416934967041
Epoch 160, training loss: 14.215839385986328 = 1.775947093963623 + 2.0 * 6.219945907592773
Epoch 160, val loss: 1.7805640697479248
Epoch 170, training loss: 14.180704116821289 = 1.767535924911499 + 2.0 * 6.2065839767456055
Epoch 170, val loss: 1.7731167078018188
Epoch 180, training loss: 14.144861221313477 = 1.758068561553955 + 2.0 * 6.193396091461182
Epoch 180, val loss: 1.7649455070495605
Epoch 190, training loss: 14.109792709350586 = 1.7473160028457642 + 2.0 * 6.181238174438477
Epoch 190, val loss: 1.75589919090271
Epoch 200, training loss: 14.07708740234375 = 1.7349430322647095 + 2.0 * 6.171072006225586
Epoch 200, val loss: 1.7457209825515747
Epoch 210, training loss: 14.050335884094238 = 1.7205259799957275 + 2.0 * 6.164905071258545
Epoch 210, val loss: 1.7340244054794312
Epoch 220, training loss: 14.011850357055664 = 1.7038127183914185 + 2.0 * 6.154018878936768
Epoch 220, val loss: 1.7204976081848145
Epoch 230, training loss: 13.975977897644043 = 1.6840764284133911 + 2.0 * 6.145950794219971
Epoch 230, val loss: 1.7046880722045898
Epoch 240, training loss: 13.940773010253906 = 1.660916805267334 + 2.0 * 6.139928340911865
Epoch 240, val loss: 1.686249017715454
Epoch 250, training loss: 13.899280548095703 = 1.6338295936584473 + 2.0 * 6.132725715637207
Epoch 250, val loss: 1.664605975151062
Epoch 260, training loss: 13.856122016906738 = 1.6019792556762695 + 2.0 * 6.127071380615234
Epoch 260, val loss: 1.639186978340149
Epoch 270, training loss: 13.808441162109375 = 1.5645617246627808 + 2.0 * 6.121939659118652
Epoch 270, val loss: 1.6093770265579224
Epoch 280, training loss: 13.762811660766602 = 1.5210644006729126 + 2.0 * 6.12087345123291
Epoch 280, val loss: 1.574690580368042
Epoch 290, training loss: 13.699586868286133 = 1.4722764492034912 + 2.0 * 6.113655090332031
Epoch 290, val loss: 1.535499930381775
Epoch 300, training loss: 13.638745307922363 = 1.4180305004119873 + 2.0 * 6.110357284545898
Epoch 300, val loss: 1.4920153617858887
Epoch 310, training loss: 13.578064918518066 = 1.3585675954818726 + 2.0 * 6.109748840332031
Epoch 310, val loss: 1.4443752765655518
Epoch 320, training loss: 13.507885932922363 = 1.2959352731704712 + 2.0 * 6.105975151062012
Epoch 320, val loss: 1.394277811050415
Epoch 330, training loss: 13.436790466308594 = 1.2327767610549927 + 2.0 * 6.102006912231445
Epoch 330, val loss: 1.3438823223114014
Epoch 340, training loss: 13.368356704711914 = 1.1698933839797974 + 2.0 * 6.099231719970703
Epoch 340, val loss: 1.2940521240234375
Epoch 350, training loss: 13.300005912780762 = 1.1078678369522095 + 2.0 * 6.096068859100342
Epoch 350, val loss: 1.2449661493301392
Epoch 360, training loss: 13.23870849609375 = 1.048124074935913 + 2.0 * 6.095292091369629
Epoch 360, val loss: 1.1978216171264648
Epoch 370, training loss: 13.174421310424805 = 0.991811990737915 + 2.0 * 6.091304779052734
Epoch 370, val loss: 1.1535398960113525
Epoch 380, training loss: 13.117355346679688 = 0.9383156895637512 + 2.0 * 6.08951997756958
Epoch 380, val loss: 1.1115820407867432
Epoch 390, training loss: 13.062975883483887 = 0.8877937197685242 + 2.0 * 6.087591171264648
Epoch 390, val loss: 1.072116494178772
Epoch 400, training loss: 13.008423805236816 = 0.8404945731163025 + 2.0 * 6.083964824676514
Epoch 400, val loss: 1.0353020429611206
Epoch 410, training loss: 12.961859703063965 = 0.7959891557693481 + 2.0 * 6.082935333251953
Epoch 410, val loss: 1.0007847547531128
Epoch 420, training loss: 12.912698745727539 = 0.7544640302658081 + 2.0 * 6.079117298126221
Epoch 420, val loss: 0.9691566824913025
Epoch 430, training loss: 12.869026184082031 = 0.7155251502990723 + 2.0 * 6.0767502784729
Epoch 430, val loss: 0.939819872379303
Epoch 440, training loss: 12.836892127990723 = 0.6788893938064575 + 2.0 * 6.079001426696777
Epoch 440, val loss: 0.9125028848648071
Epoch 450, training loss: 12.79371452331543 = 0.6446439623832703 + 2.0 * 6.074535369873047
Epoch 450, val loss: 0.8877667188644409
Epoch 460, training loss: 12.753813743591309 = 0.6125254034996033 + 2.0 * 6.070644378662109
Epoch 460, val loss: 0.8651794195175171
Epoch 470, training loss: 12.726693153381348 = 0.5820900201797485 + 2.0 * 6.072301387786865
Epoch 470, val loss: 0.8444496393203735
Epoch 480, training loss: 12.697608947753906 = 0.5536362528800964 + 2.0 * 6.071986198425293
Epoch 480, val loss: 0.8256372213363647
Epoch 490, training loss: 12.6598482131958 = 0.5270721912384033 + 2.0 * 6.066388130187988
Epoch 490, val loss: 0.8088794946670532
Epoch 500, training loss: 12.628063201904297 = 0.5019776225090027 + 2.0 * 6.063042640686035
Epoch 500, val loss: 0.7938067317008972
Epoch 510, training loss: 12.604584693908691 = 0.4782538115978241 + 2.0 * 6.063165664672852
Epoch 510, val loss: 0.7802667021751404
Epoch 520, training loss: 12.581747055053711 = 0.45602527260780334 + 2.0 * 6.06286096572876
Epoch 520, val loss: 0.7681792378425598
Epoch 530, training loss: 12.553848266601562 = 0.43523699045181274 + 2.0 * 6.059305667877197
Epoch 530, val loss: 0.757768988609314
Epoch 540, training loss: 12.529898643493652 = 0.4156692922115326 + 2.0 * 6.057114601135254
Epoch 540, val loss: 0.7486361265182495
Epoch 550, training loss: 12.51013469696045 = 0.3972281217575073 + 2.0 * 6.056453227996826
Epoch 550, val loss: 0.7405012249946594
Epoch 560, training loss: 12.492425918579102 = 0.37992164492607117 + 2.0 * 6.0562520027160645
Epoch 560, val loss: 0.7337051630020142
Epoch 570, training loss: 12.470084190368652 = 0.36360955238342285 + 2.0 * 6.053237438201904
Epoch 570, val loss: 0.7278078198432922
Epoch 580, training loss: 12.457820892333984 = 0.348111093044281 + 2.0 * 6.054854869842529
Epoch 580, val loss: 0.7228577136993408
Epoch 590, training loss: 12.441143035888672 = 0.33344021439552307 + 2.0 * 6.05385160446167
Epoch 590, val loss: 0.7185795307159424
Epoch 600, training loss: 12.418322563171387 = 0.31956547498703003 + 2.0 * 6.049378395080566
Epoch 600, val loss: 0.7152240872383118
Epoch 610, training loss: 12.400120735168457 = 0.30619052052497864 + 2.0 * 6.0469651222229
Epoch 610, val loss: 0.7124249339103699
Epoch 620, training loss: 12.385414123535156 = 0.29320859909057617 + 2.0 * 6.046102523803711
Epoch 620, val loss: 0.7100469470024109
Epoch 630, training loss: 12.370231628417969 = 0.28063395619392395 + 2.0 * 6.044798851013184
Epoch 630, val loss: 0.7080774903297424
Epoch 640, training loss: 12.361604690551758 = 0.2684485614299774 + 2.0 * 6.0465779304504395
Epoch 640, val loss: 0.7067444324493408
Epoch 650, training loss: 12.344606399536133 = 0.25657424330711365 + 2.0 * 6.044015884399414
Epoch 650, val loss: 0.7056813836097717
Epoch 660, training loss: 12.32828426361084 = 0.24501071870326996 + 2.0 * 6.041636943817139
Epoch 660, val loss: 0.7049887180328369
Epoch 670, training loss: 12.315866470336914 = 0.23365211486816406 + 2.0 * 6.041107177734375
Epoch 670, val loss: 0.7046260833740234
Epoch 680, training loss: 12.314297676086426 = 0.22254733741283417 + 2.0 * 6.045875072479248
Epoch 680, val loss: 0.7044825553894043
Epoch 690, training loss: 12.290726661682129 = 0.21178123354911804 + 2.0 * 6.039472579956055
Epoch 690, val loss: 0.7047010660171509
Epoch 700, training loss: 12.286280632019043 = 0.2014397829771042 + 2.0 * 6.042420387268066
Epoch 700, val loss: 0.7051826119422913
Epoch 710, training loss: 12.269259452819824 = 0.19158750772476196 + 2.0 * 6.0388360023498535
Epoch 710, val loss: 0.705967903137207
Epoch 720, training loss: 12.253445625305176 = 0.182173952460289 + 2.0 * 6.035635948181152
Epoch 720, val loss: 0.7071271538734436
Epoch 730, training loss: 12.242411613464355 = 0.173166424036026 + 2.0 * 6.034622669219971
Epoch 730, val loss: 0.708452045917511
Epoch 740, training loss: 12.231550216674805 = 0.16458716988563538 + 2.0 * 6.033481597900391
Epoch 740, val loss: 0.7101433873176575
Epoch 750, training loss: 12.24118709564209 = 0.15648044645786285 + 2.0 * 6.04235315322876
Epoch 750, val loss: 0.7121360898017883
Epoch 760, training loss: 12.217751502990723 = 0.14890636503696442 + 2.0 * 6.034422397613525
Epoch 760, val loss: 0.7142957448959351
Epoch 770, training loss: 12.20790958404541 = 0.14185355603694916 + 2.0 * 6.0330281257629395
Epoch 770, val loss: 0.7168909311294556
Epoch 780, training loss: 12.204877853393555 = 0.13521307706832886 + 2.0 * 6.03483247756958
Epoch 780, val loss: 0.7195862531661987
Epoch 790, training loss: 12.202526092529297 = 0.12905238568782806 + 2.0 * 6.036736965179443
Epoch 790, val loss: 0.7223165035247803
Epoch 800, training loss: 12.181684494018555 = 0.12330126017332077 + 2.0 * 6.029191493988037
Epoch 800, val loss: 0.7254290580749512
Epoch 810, training loss: 12.174814224243164 = 0.11789470165967941 + 2.0 * 6.028459548950195
Epoch 810, val loss: 0.7286131978034973
Epoch 820, training loss: 12.171006202697754 = 0.11279942095279694 + 2.0 * 6.0291032791137695
Epoch 820, val loss: 0.7318556308746338
Epoch 830, training loss: 12.166224479675293 = 0.10802523791790009 + 2.0 * 6.029099464416504
Epoch 830, val loss: 0.7350782155990601
Epoch 840, training loss: 12.158515930175781 = 0.10354901105165482 + 2.0 * 6.0274834632873535
Epoch 840, val loss: 0.7386515736579895
Epoch 850, training loss: 12.149787902832031 = 0.09933662414550781 + 2.0 * 6.025225639343262
Epoch 850, val loss: 0.742206335067749
Epoch 860, training loss: 12.144330978393555 = 0.09534687548875809 + 2.0 * 6.024492263793945
Epoch 860, val loss: 0.7458129525184631
Epoch 870, training loss: 12.155378341674805 = 0.09156343340873718 + 2.0 * 6.031907558441162
Epoch 870, val loss: 0.7495105862617493
Epoch 880, training loss: 12.141928672790527 = 0.08802174031734467 + 2.0 * 6.02695369720459
Epoch 880, val loss: 0.7532346248626709
Epoch 890, training loss: 12.133699417114258 = 0.08468331396579742 + 2.0 * 6.024507999420166
Epoch 890, val loss: 0.7570838928222656
Epoch 900, training loss: 12.12868881225586 = 0.08153588324785233 + 2.0 * 6.023576259613037
Epoch 900, val loss: 0.7608733177185059
Epoch 910, training loss: 12.121296882629395 = 0.07853879779577255 + 2.0 * 6.021378993988037
Epoch 910, val loss: 0.7647568583488464
Epoch 920, training loss: 12.116767883300781 = 0.07568247616291046 + 2.0 * 6.020542621612549
Epoch 920, val loss: 0.7686976790428162
Epoch 930, training loss: 12.119051933288574 = 0.07296278327703476 + 2.0 * 6.023044586181641
Epoch 930, val loss: 0.7726085782051086
Epoch 940, training loss: 12.117794036865234 = 0.07039589434862137 + 2.0 * 6.0236992835998535
Epoch 940, val loss: 0.776546061038971
Epoch 950, training loss: 12.121261596679688 = 0.06796538829803467 + 2.0 * 6.026648044586182
Epoch 950, val loss: 0.7804021239280701
Epoch 960, training loss: 12.103028297424316 = 0.06567679345607758 + 2.0 * 6.018675804138184
Epoch 960, val loss: 0.7843782901763916
Epoch 970, training loss: 12.098806381225586 = 0.06349486112594604 + 2.0 * 6.017655849456787
Epoch 970, val loss: 0.7883790731430054
Epoch 980, training loss: 12.094656944274902 = 0.061397846788167953 + 2.0 * 6.016629695892334
Epoch 980, val loss: 0.792300283908844
Epoch 990, training loss: 12.106310844421387 = 0.0593913309276104 + 2.0 * 6.0234599113464355
Epoch 990, val loss: 0.7962546944618225
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 730, in <module>
    torch.cuda.empty_cache()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/cuda/memory.py", line 121, in empty_cache
    torch._C._cuda_emptyCache()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.3 der2:0.3 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 18.69603729248047 = 1.9485983848571777 + 2.0 * 8.373719215393066
Epoch 0, val loss: 1.947523593902588
Epoch 10, training loss: 18.683879852294922 = 1.9382400512695312 + 2.0 * 8.372819900512695
Epoch 10, val loss: 1.937133550643921
Epoch 20, training loss: 18.659908294677734 = 1.925196647644043 + 2.0 * 8.367356300354004
Epoch 20, val loss: 1.9238563776016235
Epoch 30, training loss: 18.57695960998535 = 1.9070988893508911 + 2.0 * 8.334930419921875
Epoch 30, val loss: 1.905741810798645
Epoch 40, training loss: 18.1468505859375 = 1.8859434127807617 + 2.0 * 8.130454063415527
Epoch 40, val loss: 1.885676383972168
Epoch 50, training loss: 16.804506301879883 = 1.8606809377670288 + 2.0 * 7.471912384033203
Epoch 50, val loss: 1.8615117073059082
Epoch 60, training loss: 15.955574989318848 = 1.840851902961731 + 2.0 * 7.057361602783203
Epoch 60, val loss: 1.8445284366607666
Epoch 70, training loss: 15.351900100708008 = 1.832180142402649 + 2.0 * 6.759860038757324
Epoch 70, val loss: 1.8373310565948486
Epoch 80, training loss: 15.15434741973877 = 1.8217359781265259 + 2.0 * 6.6663055419921875
Epoch 80, val loss: 1.8275505304336548
Epoch 90, training loss: 14.981301307678223 = 1.8104248046875 + 2.0 * 6.585438251495361
Epoch 90, val loss: 1.8175849914550781
Epoch 100, training loss: 14.803640365600586 = 1.8003454208374023 + 2.0 * 6.501647472381592
Epoch 100, val loss: 1.8088843822479248
Epoch 110, training loss: 14.668390274047852 = 1.7932466268539429 + 2.0 * 6.437572002410889
Epoch 110, val loss: 1.8025426864624023
Epoch 120, training loss: 14.56569766998291 = 1.7863969802856445 + 2.0 * 6.389650344848633
Epoch 120, val loss: 1.7961502075195312
Epoch 130, training loss: 14.46983528137207 = 1.7791283130645752 + 2.0 * 6.345353603363037
Epoch 130, val loss: 1.7893108129501343
Epoch 140, training loss: 14.381512641906738 = 1.7719879150390625 + 2.0 * 6.304762363433838
Epoch 140, val loss: 1.782586693763733
Epoch 150, training loss: 14.313758850097656 = 1.7643961906433105 + 2.0 * 6.274681091308594
Epoch 150, val loss: 1.7755653858184814
Epoch 160, training loss: 14.255054473876953 = 1.7552343606948853 + 2.0 * 6.2499098777771
Epoch 160, val loss: 1.7675621509552002
Epoch 170, training loss: 14.205856323242188 = 1.7443963289260864 + 2.0 * 6.230730056762695
Epoch 170, val loss: 1.7584348917007446
Epoch 180, training loss: 14.157761573791504 = 1.7322331666946411 + 2.0 * 6.212764263153076
Epoch 180, val loss: 1.7482316493988037
Epoch 190, training loss: 14.113716125488281 = 1.7183618545532227 + 2.0 * 6.197677135467529
Epoch 190, val loss: 1.7366911172866821
Epoch 200, training loss: 14.073084831237793 = 1.7020539045333862 + 2.0 * 6.185515403747559
Epoch 200, val loss: 1.7233800888061523
Epoch 210, training loss: 14.030938148498535 = 1.6830905675888062 + 2.0 * 6.173923969268799
Epoch 210, val loss: 1.707811713218689
Epoch 220, training loss: 13.98879623413086 = 1.6607800722122192 + 2.0 * 6.164008140563965
Epoch 220, val loss: 1.6896226406097412
Epoch 230, training loss: 13.948678016662598 = 1.6344146728515625 + 2.0 * 6.157131671905518
Epoch 230, val loss: 1.6681699752807617
Epoch 240, training loss: 13.899892807006836 = 1.6037863492965698 + 2.0 * 6.148053169250488
Epoch 240, val loss: 1.6431776285171509
Epoch 250, training loss: 13.851216316223145 = 1.5684815645217896 + 2.0 * 6.141367435455322
Epoch 250, val loss: 1.614313006401062
Epoch 260, training loss: 13.799429893493652 = 1.528298258781433 + 2.0 * 6.135565757751465
Epoch 260, val loss: 1.5813883543014526
Epoch 270, training loss: 13.744074821472168 = 1.483686089515686 + 2.0 * 6.130194187164307
Epoch 270, val loss: 1.5450923442840576
Epoch 280, training loss: 13.685227394104004 = 1.4354937076568604 + 2.0 * 6.124866962432861
Epoch 280, val loss: 1.505888819694519
Epoch 290, training loss: 13.624839782714844 = 1.3847252130508423 + 2.0 * 6.120057106018066
Epoch 290, val loss: 1.46474027633667
Epoch 300, training loss: 13.564626693725586 = 1.3327767848968506 + 2.0 * 6.115924835205078
Epoch 300, val loss: 1.4231200218200684
Epoch 310, training loss: 13.504632949829102 = 1.2814346551895142 + 2.0 * 6.111598968505859
Epoch 310, val loss: 1.3825032711029053
Epoch 320, training loss: 13.44894027709961 = 1.2311500310897827 + 2.0 * 6.108895301818848
Epoch 320, val loss: 1.3431594371795654
Epoch 330, training loss: 13.391180038452148 = 1.1821675300598145 + 2.0 * 6.104506015777588
Epoch 330, val loss: 1.3054275512695312
Epoch 340, training loss: 13.337121963500977 = 1.134538173675537 + 2.0 * 6.101292133331299
Epoch 340, val loss: 1.268863320350647
Epoch 350, training loss: 13.285554885864258 = 1.0880850553512573 + 2.0 * 6.0987348556518555
Epoch 350, val loss: 1.2336379289627075
Epoch 360, training loss: 13.23006534576416 = 1.0427547693252563 + 2.0 * 6.093655109405518
Epoch 360, val loss: 1.1992297172546387
Epoch 370, training loss: 13.182121276855469 = 0.9980006217956543 + 2.0 * 6.092060565948486
Epoch 370, val loss: 1.1652531623840332
Epoch 380, training loss: 13.134891510009766 = 0.9542962908744812 + 2.0 * 6.090297698974609
Epoch 380, val loss: 1.1320444345474243
Epoch 390, training loss: 13.084110260009766 = 0.9121936559677124 + 2.0 * 6.085958480834961
Epoch 390, val loss: 1.0997380018234253
Epoch 400, training loss: 13.036982536315918 = 0.871370255947113 + 2.0 * 6.08280611038208
Epoch 400, val loss: 1.068357229232788
Epoch 410, training loss: 12.993374824523926 = 0.8322274088859558 + 2.0 * 6.080573558807373
Epoch 410, val loss: 1.038311243057251
Epoch 420, training loss: 12.95181655883789 = 0.7951350808143616 + 2.0 * 6.078340530395508
Epoch 420, val loss: 1.010021448135376
Epoch 430, training loss: 12.9158296585083 = 0.7601013779640198 + 2.0 * 6.077864170074463
Epoch 430, val loss: 0.9834074974060059
Epoch 440, training loss: 12.874673843383789 = 0.7272807359695435 + 2.0 * 6.073696613311768
Epoch 440, val loss: 0.9587779641151428
Epoch 450, training loss: 12.838176727294922 = 0.6964526176452637 + 2.0 * 6.07086181640625
Epoch 450, val loss: 0.9360577464103699
Epoch 460, training loss: 12.807332992553711 = 0.6673552393913269 + 2.0 * 6.06998872756958
Epoch 460, val loss: 0.9149726629257202
Epoch 470, training loss: 12.781925201416016 = 0.6402825117111206 + 2.0 * 6.070821285247803
Epoch 470, val loss: 0.8957965970039368
Epoch 480, training loss: 12.743610382080078 = 0.614963948726654 + 2.0 * 6.064323425292969
Epoch 480, val loss: 0.8785209059715271
Epoch 490, training loss: 12.716734886169434 = 0.590985119342804 + 2.0 * 6.062874794006348
Epoch 490, val loss: 0.8625816702842712
Epoch 500, training loss: 12.691021919250488 = 0.5681477785110474 + 2.0 * 6.061437129974365
Epoch 500, val loss: 0.8478026986122131
Epoch 510, training loss: 12.680986404418945 = 0.5465644001960754 + 2.0 * 6.067211151123047
Epoch 510, val loss: 0.8343172073364258
Epoch 520, training loss: 12.642372131347656 = 0.5261664390563965 + 2.0 * 6.058102607727051
Epoch 520, val loss: 0.8220700621604919
Epoch 530, training loss: 12.619206428527832 = 0.5065949559211731 + 2.0 * 6.056305885314941
Epoch 530, val loss: 0.8106473684310913
Epoch 540, training loss: 12.598965644836426 = 0.48768532276153564 + 2.0 * 6.05564022064209
Epoch 540, val loss: 0.7999022603034973
Epoch 550, training loss: 12.578071594238281 = 0.46941015124320984 + 2.0 * 6.054330825805664
Epoch 550, val loss: 0.7899003624916077
Epoch 560, training loss: 12.55476188659668 = 0.4517733156681061 + 2.0 * 6.051494121551514
Epoch 560, val loss: 0.7805792093276978
Epoch 570, training loss: 12.536036491394043 = 0.43455812335014343 + 2.0 * 6.050739288330078
Epoch 570, val loss: 0.771725594997406
Epoch 580, training loss: 12.516133308410645 = 0.417786568403244 + 2.0 * 6.049173355102539
Epoch 580, val loss: 0.7633650898933411
Epoch 590, training loss: 12.502287864685059 = 0.4015066921710968 + 2.0 * 6.050390720367432
Epoch 590, val loss: 0.755643904209137
Epoch 600, training loss: 12.479653358459473 = 0.3856900930404663 + 2.0 * 6.0469818115234375
Epoch 600, val loss: 0.7483247518539429
Epoch 610, training loss: 12.46102523803711 = 0.37025928497314453 + 2.0 * 6.045382976531982
Epoch 610, val loss: 0.7414849996566772
Epoch 620, training loss: 12.447754859924316 = 0.35521188378334045 + 2.0 * 6.046271324157715
Epoch 620, val loss: 0.7351058125495911
Epoch 630, training loss: 12.434856414794922 = 0.3405939042568207 + 2.0 * 6.047131061553955
Epoch 630, val loss: 0.7292602062225342
Epoch 640, training loss: 12.415058135986328 = 0.32656243443489075 + 2.0 * 6.044247627258301
Epoch 640, val loss: 0.7239336371421814
Epoch 650, training loss: 12.394582748413086 = 0.3129640519618988 + 2.0 * 6.040809154510498
Epoch 650, val loss: 0.7191697359085083
Epoch 660, training loss: 12.381563186645508 = 0.29978665709495544 + 2.0 * 6.04088830947876
Epoch 660, val loss: 0.7148368954658508
Epoch 670, training loss: 12.367013931274414 = 0.2870854139328003 + 2.0 * 6.039964199066162
Epoch 670, val loss: 0.7111056447029114
Epoch 680, training loss: 12.3495454788208 = 0.27488332986831665 + 2.0 * 6.0373311042785645
Epoch 680, val loss: 0.707834005355835
Epoch 690, training loss: 12.337060928344727 = 0.2630389332771301 + 2.0 * 6.03701114654541
Epoch 690, val loss: 0.7049596905708313
Epoch 700, training loss: 12.336922645568848 = 0.25156813859939575 + 2.0 * 6.042677402496338
Epoch 700, val loss: 0.7025413513183594
Epoch 710, training loss: 12.313960075378418 = 0.24054954946041107 + 2.0 * 6.036705493927002
Epoch 710, val loss: 0.7005763053894043
Epoch 720, training loss: 12.29755973815918 = 0.2299027144908905 + 2.0 * 6.0338287353515625
Epoch 720, val loss: 0.6990043520927429
Epoch 730, training loss: 12.285225868225098 = 0.2195320874452591 + 2.0 * 6.032846927642822
Epoch 730, val loss: 0.6978312134742737
Epoch 740, training loss: 12.27653980255127 = 0.20950907468795776 + 2.0 * 6.033515453338623
Epoch 740, val loss: 0.6970513463020325
Epoch 750, training loss: 12.265116691589355 = 0.19991670548915863 + 2.0 * 6.032599925994873
Epoch 750, val loss: 0.6966465711593628
Epoch 760, training loss: 12.252182960510254 = 0.1906440258026123 + 2.0 * 6.030769348144531
Epoch 760, val loss: 0.6965349316596985
Epoch 770, training loss: 12.242840766906738 = 0.1816662847995758 + 2.0 * 6.030587196350098
Epoch 770, val loss: 0.6967926621437073
Epoch 780, training loss: 12.23351764678955 = 0.1730441451072693 + 2.0 * 6.030236721038818
Epoch 780, val loss: 0.6973879337310791
Epoch 790, training loss: 12.227447509765625 = 0.164852112531662 + 2.0 * 6.03129768371582
Epoch 790, val loss: 0.6982280611991882
Epoch 800, training loss: 12.2118501663208 = 0.15700644254684448 + 2.0 * 6.027421951293945
Epoch 800, val loss: 0.6993036866188049
Epoch 810, training loss: 12.203027725219727 = 0.14948169887065887 + 2.0 * 6.026772975921631
Epoch 810, val loss: 0.7006473541259766
Epoch 820, training loss: 12.200913429260254 = 0.1423068791627884 + 2.0 * 6.029303073883057
Epoch 820, val loss: 0.7022665143013
Epoch 830, training loss: 12.191941261291504 = 0.1354934573173523 + 2.0 * 6.028223991394043
Epoch 830, val loss: 0.7040969133377075
Epoch 840, training loss: 12.178206443786621 = 0.12906929850578308 + 2.0 * 6.024568557739258
Epoch 840, val loss: 0.7061173319816589
Epoch 850, training loss: 12.171632766723633 = 0.1229681521654129 + 2.0 * 6.024332523345947
Epoch 850, val loss: 0.7082791328430176
Epoch 860, training loss: 12.173345565795898 = 0.11717448383569717 + 2.0 * 6.028085708618164
Epoch 860, val loss: 0.7106532454490662
Epoch 870, training loss: 12.161117553710938 = 0.11173462867736816 + 2.0 * 6.024691581726074
Epoch 870, val loss: 0.7131461501121521
Epoch 880, training loss: 12.150784492492676 = 0.10658658295869827 + 2.0 * 6.022099018096924
Epoch 880, val loss: 0.7157530188560486
Epoch 890, training loss: 12.143434524536133 = 0.10171288251876831 + 2.0 * 6.02086067199707
Epoch 890, val loss: 0.7184915542602539
Epoch 900, training loss: 12.163220405578613 = 0.09710856527090073 + 2.0 * 6.033055782318115
Epoch 900, val loss: 0.721405565738678
Epoch 910, training loss: 12.13277530670166 = 0.09281083196401596 + 2.0 * 6.01998233795166
Epoch 910, val loss: 0.7243800163269043
Epoch 920, training loss: 12.127452850341797 = 0.08877500891685486 + 2.0 * 6.019339084625244
Epoch 920, val loss: 0.7273403406143188
Epoch 930, training loss: 12.121683120727539 = 0.08494389057159424 + 2.0 * 6.018369674682617
Epoch 930, val loss: 0.7304642200469971
Epoch 940, training loss: 12.118059158325195 = 0.08131144940853119 + 2.0 * 6.018373966217041
Epoch 940, val loss: 0.7336903214454651
Epoch 950, training loss: 12.121079444885254 = 0.07789985090494156 + 2.0 * 6.021589756011963
Epoch 950, val loss: 0.7369787096977234
Epoch 960, training loss: 12.115639686584473 = 0.07471677660942078 + 2.0 * 6.020461559295654
Epoch 960, val loss: 0.74021977186203
Epoch 970, training loss: 12.10545825958252 = 0.07171682268381119 + 2.0 * 6.016870498657227
Epoch 970, val loss: 0.743421196937561
Epoch 980, training loss: 12.101150512695312 = 0.06885939091444016 + 2.0 * 6.016145706176758
Epoch 980, val loss: 0.7467026114463806
Epoch 990, training loss: 12.102874755859375 = 0.066165030002594 + 2.0 * 6.018354892730713
Epoch 990, val loss: 0.7500848174095154
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8111
Overall ASR: 0.7269
Flip ASR: 0.6711/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.69795036315918 = 1.950404405593872 + 2.0 * 8.373772621154785
Epoch 0, val loss: 1.9423054456710815
Epoch 10, training loss: 18.685123443603516 = 1.939249038696289 + 2.0 * 8.372937202453613
Epoch 10, val loss: 1.9311273097991943
Epoch 20, training loss: 18.660459518432617 = 1.9254316091537476 + 2.0 * 8.367513656616211
Epoch 20, val loss: 1.916818380355835
Epoch 30, training loss: 18.580636978149414 = 1.9069006443023682 + 2.0 * 8.336868286132812
Epoch 30, val loss: 1.8974921703338623
Epoch 40, training loss: 18.212221145629883 = 1.8852462768554688 + 2.0 * 8.163487434387207
Epoch 40, val loss: 1.8754808902740479
Epoch 50, training loss: 16.98060417175293 = 1.8617637157440186 + 2.0 * 7.559420585632324
Epoch 50, val loss: 1.8512228727340698
Epoch 60, training loss: 16.288347244262695 = 1.8422666788101196 + 2.0 * 7.2230401039123535
Epoch 60, val loss: 1.8320659399032593
Epoch 70, training loss: 15.825418472290039 = 1.8288294076919556 + 2.0 * 6.998294353485107
Epoch 70, val loss: 1.81807541847229
Epoch 80, training loss: 15.40869140625 = 1.8158522844314575 + 2.0 * 6.796419620513916
Epoch 80, val loss: 1.8046232461929321
Epoch 90, training loss: 15.144743919372559 = 1.8027738332748413 + 2.0 * 6.670985221862793
Epoch 90, val loss: 1.791666865348816
Epoch 100, training loss: 14.919652938842773 = 1.7920708656311035 + 2.0 * 6.563791275024414
Epoch 100, val loss: 1.7809455394744873
Epoch 110, training loss: 14.738677978515625 = 1.7842434644699097 + 2.0 * 6.477217197418213
Epoch 110, val loss: 1.7727932929992676
Epoch 120, training loss: 14.616988182067871 = 1.7764376401901245 + 2.0 * 6.4202752113342285
Epoch 120, val loss: 1.764564871788025
Epoch 130, training loss: 14.52026653289795 = 1.767473816871643 + 2.0 * 6.376396179199219
Epoch 130, val loss: 1.7555181980133057
Epoch 140, training loss: 14.439460754394531 = 1.7581477165222168 + 2.0 * 6.340656757354736
Epoch 140, val loss: 1.7466148138046265
Epoch 150, training loss: 14.373139381408691 = 1.7483937740325928 + 2.0 * 6.31237268447876
Epoch 150, val loss: 1.7376114130020142
Epoch 160, training loss: 14.31784439086914 = 1.7377400398254395 + 2.0 * 6.2900519371032715
Epoch 160, val loss: 1.7279506921768188
Epoch 170, training loss: 14.261634826660156 = 1.725955605506897 + 2.0 * 6.267839431762695
Epoch 170, val loss: 1.7175583839416504
Epoch 180, training loss: 14.210086822509766 = 1.7127065658569336 + 2.0 * 6.248690128326416
Epoch 180, val loss: 1.706183910369873
Epoch 190, training loss: 14.160058975219727 = 1.6975501775741577 + 2.0 * 6.231254577636719
Epoch 190, val loss: 1.6934889554977417
Epoch 200, training loss: 14.113568305969238 = 1.6799356937408447 + 2.0 * 6.216816425323486
Epoch 200, val loss: 1.679062843322754
Epoch 210, training loss: 14.06574535369873 = 1.659527063369751 + 2.0 * 6.203109264373779
Epoch 210, val loss: 1.6623234748840332
Epoch 220, training loss: 14.017643928527832 = 1.6357442140579224 + 2.0 * 6.1909499168396
Epoch 220, val loss: 1.6429872512817383
Epoch 230, training loss: 13.968502044677734 = 1.607645869255066 + 2.0 * 6.1804280281066895
Epoch 230, val loss: 1.6201494932174683
Epoch 240, training loss: 13.917149543762207 = 1.5744365453720093 + 2.0 * 6.171356678009033
Epoch 240, val loss: 1.593095302581787
Epoch 250, training loss: 13.867692947387695 = 1.5360466241836548 + 2.0 * 6.165822982788086
Epoch 250, val loss: 1.5619808435440063
Epoch 260, training loss: 13.806621551513672 = 1.4936037063598633 + 2.0 * 6.156508922576904
Epoch 260, val loss: 1.5279526710510254
Epoch 270, training loss: 13.748510360717773 = 1.4475442171096802 + 2.0 * 6.150483131408691
Epoch 270, val loss: 1.4912059307098389
Epoch 280, training loss: 13.6919584274292 = 1.399232268333435 + 2.0 * 6.146363258361816
Epoch 280, val loss: 1.45310640335083
Epoch 290, training loss: 13.630181312561035 = 1.3506805896759033 + 2.0 * 6.1397504806518555
Epoch 290, val loss: 1.4154032468795776
Epoch 300, training loss: 13.571606636047363 = 1.3024578094482422 + 2.0 * 6.1345744132995605
Epoch 300, val loss: 1.3784197568893433
Epoch 310, training loss: 13.51947021484375 = 1.2550536394119263 + 2.0 * 6.132208347320557
Epoch 310, val loss: 1.3426584005355835
Epoch 320, training loss: 13.463257789611816 = 1.2099705934524536 + 2.0 * 6.126643657684326
Epoch 320, val loss: 1.3093544244766235
Epoch 330, training loss: 13.411287307739258 = 1.16693913936615 + 2.0 * 6.122174263000488
Epoch 330, val loss: 1.278025507926941
Epoch 340, training loss: 13.360856056213379 = 1.1249364614486694 + 2.0 * 6.117959976196289
Epoch 340, val loss: 1.2477020025253296
Epoch 350, training loss: 13.312193870544434 = 1.083881139755249 + 2.0 * 6.114156246185303
Epoch 350, val loss: 1.2183295488357544
Epoch 360, training loss: 13.264910697937012 = 1.0438575744628906 + 2.0 * 6.1105265617370605
Epoch 360, val loss: 1.1899011135101318
Epoch 370, training loss: 13.218474388122559 = 1.0040286779403687 + 2.0 * 6.107223033905029
Epoch 370, val loss: 1.1615155935287476
Epoch 380, training loss: 13.182450294494629 = 0.9647303819656372 + 2.0 * 6.108860015869141
Epoch 380, val loss: 1.1337502002716064
Epoch 390, training loss: 13.130258560180664 = 0.9266741871833801 + 2.0 * 6.101792335510254
Epoch 390, val loss: 1.1066352128982544
Epoch 400, training loss: 13.086121559143066 = 0.889348566532135 + 2.0 * 6.098386287689209
Epoch 400, val loss: 1.080191731452942
Epoch 410, training loss: 13.061028480529785 = 0.8525992035865784 + 2.0 * 6.104214668273926
Epoch 410, val loss: 1.0542876720428467
Epoch 420, training loss: 13.007493019104004 = 0.817121148109436 + 2.0 * 6.09518575668335
Epoch 420, val loss: 1.0290706157684326
Epoch 430, training loss: 12.963545799255371 = 0.7824429273605347 + 2.0 * 6.090551376342773
Epoch 430, val loss: 1.0045534372329712
Epoch 440, training loss: 12.938379287719727 = 0.7484651207923889 + 2.0 * 6.094956874847412
Epoch 440, val loss: 0.9805934429168701
Epoch 450, training loss: 12.890477180480957 = 0.7157750725746155 + 2.0 * 6.087350845336914
Epoch 450, val loss: 0.9576594829559326
Epoch 460, training loss: 12.852005004882812 = 0.6841222643852234 + 2.0 * 6.083941459655762
Epoch 460, val loss: 0.9356897473335266
Epoch 470, training loss: 12.824841499328613 = 0.6535508632659912 + 2.0 * 6.0856451988220215
Epoch 470, val loss: 0.9146575331687927
Epoch 480, training loss: 12.787763595581055 = 0.6245248913764954 + 2.0 * 6.0816192626953125
Epoch 480, val loss: 0.8951070308685303
Epoch 490, training loss: 12.75190544128418 = 0.5971397757530212 + 2.0 * 6.077383041381836
Epoch 490, val loss: 0.8771858811378479
Epoch 500, training loss: 12.723671913146973 = 0.5712507367134094 + 2.0 * 6.0762104988098145
Epoch 500, val loss: 0.8607616424560547
Epoch 510, training loss: 12.697856903076172 = 0.5468551516532898 + 2.0 * 6.075500965118408
Epoch 510, val loss: 0.8459067344665527
Epoch 520, training loss: 12.667757034301758 = 0.5240444540977478 + 2.0 * 6.071856498718262
Epoch 520, val loss: 0.8325843811035156
Epoch 530, training loss: 12.643359184265137 = 0.5024346709251404 + 2.0 * 6.070462226867676
Epoch 530, val loss: 0.8206600546836853
Epoch 540, training loss: 12.621824264526367 = 0.4820285141468048 + 2.0 * 6.069897651672363
Epoch 540, val loss: 0.809887707233429
Epoch 550, training loss: 12.59887409210205 = 0.4626944363117218 + 2.0 * 6.068089962005615
Epoch 550, val loss: 0.8003296852111816
Epoch 560, training loss: 12.57664680480957 = 0.4443298578262329 + 2.0 * 6.066158294677734
Epoch 560, val loss: 0.7917715907096863
Epoch 570, training loss: 12.557355880737305 = 0.42676210403442383 + 2.0 * 6.065296649932861
Epoch 570, val loss: 0.7840626239776611
Epoch 580, training loss: 12.533703804016113 = 0.40989989042282104 + 2.0 * 6.061902046203613
Epoch 580, val loss: 0.7771657109260559
Epoch 590, training loss: 12.516096115112305 = 0.39347729086875916 + 2.0 * 6.061309337615967
Epoch 590, val loss: 0.7709463238716125
Epoch 600, training loss: 12.497429847717285 = 0.37748804688453674 + 2.0 * 6.059970855712891
Epoch 600, val loss: 0.7651965022087097
Epoch 610, training loss: 12.483522415161133 = 0.3619604706764221 + 2.0 * 6.060781002044678
Epoch 610, val loss: 0.7600300312042236
Epoch 620, training loss: 12.462356567382812 = 0.34678056836128235 + 2.0 * 6.057787895202637
Epoch 620, val loss: 0.7553540468215942
Epoch 630, training loss: 12.441535949707031 = 0.33189550042152405 + 2.0 * 6.0548200607299805
Epoch 630, val loss: 0.7511047124862671
Epoch 640, training loss: 12.435978889465332 = 0.3172556757926941 + 2.0 * 6.059361457824707
Epoch 640, val loss: 0.7472431063652039
Epoch 650, training loss: 12.411757469177246 = 0.3030461370944977 + 2.0 * 6.054355621337891
Epoch 650, val loss: 0.7438279390335083
Epoch 660, training loss: 12.394600868225098 = 0.2891639769077301 + 2.0 * 6.052718639373779
Epoch 660, val loss: 0.7408447265625
Epoch 670, training loss: 12.386987686157227 = 0.27557095885276794 + 2.0 * 6.055708408355713
Epoch 670, val loss: 0.7382322549819946
Epoch 680, training loss: 12.36943244934082 = 0.26237955689430237 + 2.0 * 6.053526401519775
Epoch 680, val loss: 0.7358861565589905
Epoch 690, training loss: 12.348179817199707 = 0.24959436058998108 + 2.0 * 6.04929256439209
Epoch 690, val loss: 0.7340163588523865
Epoch 700, training loss: 12.331880569458008 = 0.23712515830993652 + 2.0 * 6.047377586364746
Epoch 700, val loss: 0.7326583862304688
Epoch 710, training loss: 12.34538745880127 = 0.2250349074602127 + 2.0 * 6.060176372528076
Epoch 710, val loss: 0.7317280769348145
Epoch 720, training loss: 12.305667877197266 = 0.213529571890831 + 2.0 * 6.046069145202637
Epoch 720, val loss: 0.7311744689941406
Epoch 730, training loss: 12.290620803833008 = 0.20251810550689697 + 2.0 * 6.044051170349121
Epoch 730, val loss: 0.7311151027679443
Epoch 740, training loss: 12.280263900756836 = 0.1919388771057129 + 2.0 * 6.044162273406982
Epoch 740, val loss: 0.73152756690979
Epoch 750, training loss: 12.26586627960205 = 0.18189065158367157 + 2.0 * 6.041987895965576
Epoch 750, val loss: 0.7323537468910217
Epoch 760, training loss: 12.257723808288574 = 0.1723913699388504 + 2.0 * 6.042666435241699
Epoch 760, val loss: 0.7336310148239136
Epoch 770, training loss: 12.247888565063477 = 0.16341528296470642 + 2.0 * 6.042236804962158
Epoch 770, val loss: 0.7353754043579102
Epoch 780, training loss: 12.244864463806152 = 0.15498676896095276 + 2.0 * 6.044939041137695
Epoch 780, val loss: 0.7373532652854919
Epoch 790, training loss: 12.225987434387207 = 0.1471574455499649 + 2.0 * 6.039414882659912
Epoch 790, val loss: 0.7397434115409851
Epoch 800, training loss: 12.216036796569824 = 0.1398083120584488 + 2.0 * 6.038114070892334
Epoch 800, val loss: 0.7425050735473633
Epoch 810, training loss: 12.213111877441406 = 0.13290944695472717 + 2.0 * 6.040101051330566
Epoch 810, val loss: 0.7455658912658691
Epoch 820, training loss: 12.203263282775879 = 0.12649142742156982 + 2.0 * 6.03838586807251
Epoch 820, val loss: 0.7488436102867126
Epoch 830, training loss: 12.19381332397461 = 0.12049942463636398 + 2.0 * 6.036656856536865
Epoch 830, val loss: 0.7523805499076843
Epoch 840, training loss: 12.191864013671875 = 0.11489380151033401 + 2.0 * 6.038485050201416
Epoch 840, val loss: 0.7561584115028381
Epoch 850, training loss: 12.179922103881836 = 0.10967401415109634 + 2.0 * 6.035123825073242
Epoch 850, val loss: 0.760097086429596
Epoch 860, training loss: 12.173261642456055 = 0.1047714501619339 + 2.0 * 6.034245014190674
Epoch 860, val loss: 0.764190673828125
Epoch 870, training loss: 12.175400733947754 = 0.10016322135925293 + 2.0 * 6.037618637084961
Epoch 870, val loss: 0.7684327960014343
Epoch 880, training loss: 12.164046287536621 = 0.09587115794420242 + 2.0 * 6.034087657928467
Epoch 880, val loss: 0.7727490663528442
Epoch 890, training loss: 12.155671119689941 = 0.09182613343000412 + 2.0 * 6.031922340393066
Epoch 890, val loss: 0.7771702408790588
Epoch 900, training loss: 12.158914566040039 = 0.08802369236946106 + 2.0 * 6.035445213317871
Epoch 900, val loss: 0.7817116975784302
Epoch 910, training loss: 12.145980834960938 = 0.08444266766309738 + 2.0 * 6.030768871307373
Epoch 910, val loss: 0.7862916588783264
Epoch 920, training loss: 12.13889217376709 = 0.08106754720211029 + 2.0 * 6.028912544250488
Epoch 920, val loss: 0.7910028100013733
Epoch 930, training loss: 12.140966415405273 = 0.07785652577877045 + 2.0 * 6.03155517578125
Epoch 930, val loss: 0.7958128452301025
Epoch 940, training loss: 12.135886192321777 = 0.07483939081430435 + 2.0 * 6.030523300170898
Epoch 940, val loss: 0.8006124496459961
Epoch 950, training loss: 12.127273559570312 = 0.07199515402317047 + 2.0 * 6.027639389038086
Epoch 950, val loss: 0.805465042591095
Epoch 960, training loss: 12.121013641357422 = 0.0692857950925827 + 2.0 * 6.025864124298096
Epoch 960, val loss: 0.8103816509246826
Epoch 970, training loss: 12.128340721130371 = 0.0667107030749321 + 2.0 * 6.030815124511719
Epoch 970, val loss: 0.8153461813926697
Epoch 980, training loss: 12.123278617858887 = 0.06426363438367844 + 2.0 * 6.029507637023926
Epoch 980, val loss: 0.8202329874038696
Epoch 990, training loss: 12.111845970153809 = 0.061946336179971695 + 2.0 * 6.02495002746582
Epoch 990, val loss: 0.8251838684082031
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.8155
Flip ASR: 0.7822/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.69451904296875 = 1.947020411491394 + 2.0 * 8.373749732971191
Epoch 0, val loss: 1.9389398097991943
Epoch 10, training loss: 18.682523727416992 = 1.9368799924850464 + 2.0 * 8.372821807861328
Epoch 10, val loss: 1.9292925596237183
Epoch 20, training loss: 18.657703399658203 = 1.924337387084961 + 2.0 * 8.366683006286621
Epoch 20, val loss: 1.9170030355453491
Epoch 30, training loss: 18.557308197021484 = 1.9074898958206177 + 2.0 * 8.324909210205078
Epoch 30, val loss: 1.9003040790557861
Epoch 40, training loss: 17.929723739624023 = 1.8879634141921997 + 2.0 * 8.020879745483398
Epoch 40, val loss: 1.8814127445220947
Epoch 50, training loss: 16.35880470275879 = 1.864789605140686 + 2.0 * 7.247007369995117
Epoch 50, val loss: 1.859176754951477
Epoch 60, training loss: 15.708191871643066 = 1.847832441329956 + 2.0 * 6.930179595947266
Epoch 60, val loss: 1.8435548543930054
Epoch 70, training loss: 15.34177303314209 = 1.8346011638641357 + 2.0 * 6.7535858154296875
Epoch 70, val loss: 1.8304373025894165
Epoch 80, training loss: 15.10987663269043 = 1.8211746215820312 + 2.0 * 6.644351005554199
Epoch 80, val loss: 1.81708562374115
Epoch 90, training loss: 14.93488883972168 = 1.8089932203292847 + 2.0 * 6.562947750091553
Epoch 90, val loss: 1.805418848991394
Epoch 100, training loss: 14.804388999938965 = 1.7985113859176636 + 2.0 * 6.502938747406006
Epoch 100, val loss: 1.7953194379806519
Epoch 110, training loss: 14.684381484985352 = 1.78927743434906 + 2.0 * 6.44755220413208
Epoch 110, val loss: 1.7863699197769165
Epoch 120, training loss: 14.575565338134766 = 1.7809584140777588 + 2.0 * 6.397303581237793
Epoch 120, val loss: 1.7782524824142456
Epoch 130, training loss: 14.476301193237305 = 1.7734454870224 + 2.0 * 6.351428031921387
Epoch 130, val loss: 1.7708375453948975
Epoch 140, training loss: 14.384783744812012 = 1.7661551237106323 + 2.0 * 6.309314250946045
Epoch 140, val loss: 1.7636470794677734
Epoch 150, training loss: 14.314846992492676 = 1.7580440044403076 + 2.0 * 6.2784013748168945
Epoch 150, val loss: 1.7558585405349731
Epoch 160, training loss: 14.256094932556152 = 1.748490333557129 + 2.0 * 6.253802299499512
Epoch 160, val loss: 1.7470567226409912
Epoch 170, training loss: 14.205101013183594 = 1.7375904321670532 + 2.0 * 6.233755111694336
Epoch 170, val loss: 1.737391471862793
Epoch 180, training loss: 14.1603422164917 = 1.7252718210220337 + 2.0 * 6.217535018920898
Epoch 180, val loss: 1.7269331216812134
Epoch 190, training loss: 14.114570617675781 = 1.7114033699035645 + 2.0 * 6.201583385467529
Epoch 190, val loss: 1.7153762578964233
Epoch 200, training loss: 14.071412086486816 = 1.695441484451294 + 2.0 * 6.187985420227051
Epoch 200, val loss: 1.7023848295211792
Epoch 210, training loss: 14.034934043884277 = 1.6768466234207153 + 2.0 * 6.179043769836426
Epoch 210, val loss: 1.687412977218628
Epoch 220, training loss: 13.987278938293457 = 1.6552869081497192 + 2.0 * 6.165996074676514
Epoch 220, val loss: 1.6701487302780151
Epoch 230, training loss: 13.943910598754883 = 1.6301952600479126 + 2.0 * 6.156857490539551
Epoch 230, val loss: 1.6502089500427246
Epoch 240, training loss: 13.900715827941895 = 1.6012400388717651 + 2.0 * 6.14973783493042
Epoch 240, val loss: 1.6272424459457397
Epoch 250, training loss: 13.85221004486084 = 1.5681507587432861 + 2.0 * 6.142029762268066
Epoch 250, val loss: 1.6011724472045898
Epoch 260, training loss: 13.801898002624512 = 1.530638337135315 + 2.0 * 6.135629653930664
Epoch 260, val loss: 1.5716990232467651
Epoch 270, training loss: 13.75501823425293 = 1.488579273223877 + 2.0 * 6.1332197189331055
Epoch 270, val loss: 1.5388729572296143
Epoch 280, training loss: 13.693723678588867 = 1.4430874586105347 + 2.0 * 6.1253180503845215
Epoch 280, val loss: 1.5034295320510864
Epoch 290, training loss: 13.635687828063965 = 1.3946326971054077 + 2.0 * 6.120527744293213
Epoch 290, val loss: 1.4659260511398315
Epoch 300, training loss: 13.583610534667969 = 1.344166874885559 + 2.0 * 6.11972188949585
Epoch 300, val loss: 1.4271365404129028
Epoch 310, training loss: 13.518814086914062 = 1.2931485176086426 + 2.0 * 6.112832546234131
Epoch 310, val loss: 1.3882708549499512
Epoch 320, training loss: 13.464113235473633 = 1.2422025203704834 + 2.0 * 6.110955238342285
Epoch 320, val loss: 1.3497780561447144
Epoch 330, training loss: 13.403966903686523 = 1.192143201828003 + 2.0 * 6.105911731719971
Epoch 330, val loss: 1.3122632503509521
Epoch 340, training loss: 13.347208023071289 = 1.1429798603057861 + 2.0 * 6.102114200592041
Epoch 340, val loss: 1.2756038904190063
Epoch 350, training loss: 13.297385215759277 = 1.0949490070343018 + 2.0 * 6.101218223571777
Epoch 350, val loss: 1.2402002811431885
Epoch 360, training loss: 13.24748706817627 = 1.0492485761642456 + 2.0 * 6.099119186401367
Epoch 360, val loss: 1.2067432403564453
Epoch 370, training loss: 13.190018653869629 = 1.005728006362915 + 2.0 * 6.0921454429626465
Epoch 370, val loss: 1.1753666400909424
Epoch 380, training loss: 13.142261505126953 = 0.9640685319900513 + 2.0 * 6.089096546173096
Epoch 380, val loss: 1.145578384399414
Epoch 390, training loss: 13.100975036621094 = 0.9242643117904663 + 2.0 * 6.088355541229248
Epoch 390, val loss: 1.1175012588500977
Epoch 400, training loss: 13.056818008422852 = 0.8871325254440308 + 2.0 * 6.084842681884766
Epoch 400, val loss: 1.0914230346679688
Epoch 410, training loss: 13.015722274780273 = 0.8525232672691345 + 2.0 * 6.081599712371826
Epoch 410, val loss: 1.0675907135009766
Epoch 420, training loss: 12.977161407470703 = 0.8196778297424316 + 2.0 * 6.078742027282715
Epoch 420, val loss: 1.045352816581726
Epoch 430, training loss: 12.949384689331055 = 0.7882299423217773 + 2.0 * 6.080577373504639
Epoch 430, val loss: 1.0246037244796753
Epoch 440, training loss: 12.906584739685059 = 0.7584519982337952 + 2.0 * 6.074066162109375
Epoch 440, val loss: 1.0052006244659424
Epoch 450, training loss: 12.873478889465332 = 0.729937732219696 + 2.0 * 6.071770668029785
Epoch 450, val loss: 0.9873521327972412
Epoch 460, training loss: 12.846673965454102 = 0.7022889256477356 + 2.0 * 6.072192668914795
Epoch 460, val loss: 0.9704777598381042
Epoch 470, training loss: 12.813016891479492 = 0.6753631830215454 + 2.0 * 6.068826675415039
Epoch 470, val loss: 0.9547366499900818
Epoch 480, training loss: 12.78564453125 = 0.6491356492042542 + 2.0 * 6.068254470825195
Epoch 480, val loss: 0.9398276209831238
Epoch 490, training loss: 12.754646301269531 = 0.623417317867279 + 2.0 * 6.065614700317383
Epoch 490, val loss: 0.9258677959442139
Epoch 500, training loss: 12.725202560424805 = 0.5982936024665833 + 2.0 * 6.063454627990723
Epoch 500, val loss: 0.9126341938972473
Epoch 510, training loss: 12.700160026550293 = 0.5735244750976562 + 2.0 * 6.063317775726318
Epoch 510, val loss: 0.9001374244689941
Epoch 520, training loss: 12.669760704040527 = 0.5494367480278015 + 2.0 * 6.06016206741333
Epoch 520, val loss: 0.8883736729621887
Epoch 530, training loss: 12.641377449035645 = 0.5257832407951355 + 2.0 * 6.057796955108643
Epoch 530, val loss: 0.8773952126502991
Epoch 540, training loss: 12.6236572265625 = 0.5025602579116821 + 2.0 * 6.060548305511475
Epoch 540, val loss: 0.867030918598175
Epoch 550, training loss: 12.592840194702148 = 0.48004674911499023 + 2.0 * 6.056396961212158
Epoch 550, val loss: 0.8575611710548401
Epoch 560, training loss: 12.574968338012695 = 0.4581378996372223 + 2.0 * 6.058415412902832
Epoch 560, val loss: 0.8491207361221313
Epoch 570, training loss: 12.542216300964355 = 0.43698588013648987 + 2.0 * 6.052615165710449
Epoch 570, val loss: 0.8415480852127075
Epoch 580, training loss: 12.526802062988281 = 0.41638585925102234 + 2.0 * 6.055208206176758
Epoch 580, val loss: 0.8348656296730042
Epoch 590, training loss: 12.501386642456055 = 0.3965757489204407 + 2.0 * 6.05240535736084
Epoch 590, val loss: 0.8289844393730164
Epoch 600, training loss: 12.475071907043457 = 0.3774126172065735 + 2.0 * 6.048829555511475
Epoch 600, val loss: 0.8240477442741394
Epoch 610, training loss: 12.452774047851562 = 0.3588871955871582 + 2.0 * 6.046943187713623
Epoch 610, val loss: 0.819866955280304
Epoch 620, training loss: 12.454108238220215 = 0.34095683693885803 + 2.0 * 6.056575775146484
Epoch 620, val loss: 0.8162679672241211
Epoch 630, training loss: 12.418662071228027 = 0.32395580410957336 + 2.0 * 6.047353267669678
Epoch 630, val loss: 0.81360924243927
Epoch 640, training loss: 12.398903846740723 = 0.307625949382782 + 2.0 * 6.0456390380859375
Epoch 640, val loss: 0.8117608428001404
Epoch 650, training loss: 12.377208709716797 = 0.291998028755188 + 2.0 * 6.042605400085449
Epoch 650, val loss: 0.8103474974632263
Epoch 660, training loss: 12.360434532165527 = 0.27706170082092285 + 2.0 * 6.041686534881592
Epoch 660, val loss: 0.809817910194397
Epoch 670, training loss: 12.3551025390625 = 0.262820303440094 + 2.0 * 6.046141147613525
Epoch 670, val loss: 0.8097179532051086
Epoch 680, training loss: 12.333650588989258 = 0.24940618872642517 + 2.0 * 6.0421223640441895
Epoch 680, val loss: 0.8103844523429871
Epoch 690, training loss: 12.314338684082031 = 0.2367025464773178 + 2.0 * 6.038817882537842
Epoch 690, val loss: 0.8116936087608337
Epoch 700, training loss: 12.306625366210938 = 0.22461576759815216 + 2.0 * 6.041004657745361
Epoch 700, val loss: 0.8134298324584961
Epoch 710, training loss: 12.28916072845459 = 0.21325217187404633 + 2.0 * 6.037954330444336
Epoch 710, val loss: 0.8156830668449402
Epoch 720, training loss: 12.280510902404785 = 0.20254473388195038 + 2.0 * 6.03898286819458
Epoch 720, val loss: 0.8185201287269592
Epoch 730, training loss: 12.269144058227539 = 0.19251564145088196 + 2.0 * 6.038314342498779
Epoch 730, val loss: 0.8216965198516846
Epoch 740, training loss: 12.25261116027832 = 0.18309026956558228 + 2.0 * 6.034760475158691
Epoch 740, val loss: 0.8253735303878784
Epoch 750, training loss: 12.24252700805664 = 0.17418338358402252 + 2.0 * 6.0341715812683105
Epoch 750, val loss: 0.829373836517334
Epoch 760, training loss: 12.241315841674805 = 0.16581518948078156 + 2.0 * 6.037750244140625
Epoch 760, val loss: 0.8336374759674072
Epoch 770, training loss: 12.230234146118164 = 0.15799033641815186 + 2.0 * 6.036121845245361
Epoch 770, val loss: 0.8382027745246887
Epoch 780, training loss: 12.215320587158203 = 0.150649756193161 + 2.0 * 6.03233528137207
Epoch 780, val loss: 0.8431500196456909
Epoch 790, training loss: 12.204601287841797 = 0.14373070001602173 + 2.0 * 6.030435085296631
Epoch 790, val loss: 0.8483448624610901
Epoch 800, training loss: 12.208131790161133 = 0.1371620148420334 + 2.0 * 6.035484790802002
Epoch 800, val loss: 0.8536068201065063
Epoch 810, training loss: 12.19773006439209 = 0.13103534281253815 + 2.0 * 6.033347129821777
Epoch 810, val loss: 0.8590518236160278
Epoch 820, training loss: 12.181473731994629 = 0.1252480298280716 + 2.0 * 6.028112888336182
Epoch 820, val loss: 0.8649157285690308
Epoch 830, training loss: 12.176386833190918 = 0.11975442618131638 + 2.0 * 6.028316020965576
Epoch 830, val loss: 0.8707700371742249
Epoch 840, training loss: 12.171670913696289 = 0.11457526683807373 + 2.0 * 6.028547763824463
Epoch 840, val loss: 0.8766006231307983
Epoch 850, training loss: 12.16388988494873 = 0.1097201332449913 + 2.0 * 6.027084827423096
Epoch 850, val loss: 0.8828455209732056
Epoch 860, training loss: 12.157519340515137 = 0.10511370748281479 + 2.0 * 6.02620267868042
Epoch 860, val loss: 0.889098584651947
Epoch 870, training loss: 12.159822463989258 = 0.1007409319281578 + 2.0 * 6.029540538787842
Epoch 870, val loss: 0.8952711224555969
Epoch 880, training loss: 12.145790100097656 = 0.09664551168680191 + 2.0 * 6.024572372436523
Epoch 880, val loss: 0.9017133712768555
Epoch 890, training loss: 12.140596389770508 = 0.0927390530705452 + 2.0 * 6.023928642272949
Epoch 890, val loss: 0.9082924127578735
Epoch 900, training loss: 12.14117431640625 = 0.08903051167726517 + 2.0 * 6.026072025299072
Epoch 900, val loss: 0.9147212505340576
Epoch 910, training loss: 12.132004737854004 = 0.0855170339345932 + 2.0 * 6.0232439041137695
Epoch 910, val loss: 0.9212879538536072
Epoch 920, training loss: 12.132843017578125 = 0.08219005912542343 + 2.0 * 6.025326251983643
Epoch 920, val loss: 0.9278907775878906
Epoch 930, training loss: 12.12360668182373 = 0.07903977483510971 + 2.0 * 6.022283554077148
Epoch 930, val loss: 0.9343435764312744
Epoch 940, training loss: 12.117121696472168 = 0.07604821026325226 + 2.0 * 6.02053689956665
Epoch 940, val loss: 0.9410722851753235
Epoch 950, training loss: 12.123193740844727 = 0.07319754362106323 + 2.0 * 6.024998188018799
Epoch 950, val loss: 0.9476498365402222
Epoch 960, training loss: 12.113771438598633 = 0.07052353024482727 + 2.0 * 6.0216240882873535
Epoch 960, val loss: 0.9540424346923828
Epoch 970, training loss: 12.106297492980957 = 0.06797786802053452 + 2.0 * 6.01915979385376
Epoch 970, val loss: 0.9607798457145691
Epoch 980, training loss: 12.102298736572266 = 0.06555303186178207 + 2.0 * 6.018373012542725
Epoch 980, val loss: 0.9673035740852356
Epoch 990, training loss: 12.106980323791504 = 0.0632438212633133 + 2.0 * 6.0218682289123535
Epoch 990, val loss: 0.9736900329589844
=== picking the best model according to the performance on validation ===
target class rate on Vs: 0.8000
accuracy on clean test nodes: 0.8037
Overall ASR: 0.8266
Flip ASR: 0.7956/225 nodes
The final ASR:0.78967, 0.04459, Accuracy:0.81481, 0.01090
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11590])
remove edge: torch.Size([2, 9496])
updated graph: torch.Size([2, 10530])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8444
Overall ASR: 0.9926
Flip ASR: 0.9911/225 nodes
The final ASR:0.98155, 0.00797, Accuracy:0.83333, 0.00907
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.3 der2:0.3 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 18.685691833496094 = 1.9379987716674805 + 2.0 * 8.373846054077148
Epoch 0, val loss: 1.9326626062393188
Epoch 10, training loss: 18.67466163635254 = 1.9281738996505737 + 2.0 * 8.373244285583496
Epoch 10, val loss: 1.9239590167999268
Epoch 20, training loss: 18.65363121032715 = 1.9160970449447632 + 2.0 * 8.368766784667969
Epoch 20, val loss: 1.9129645824432373
Epoch 30, training loss: 18.564586639404297 = 1.900086522102356 + 2.0 * 8.332249641418457
Epoch 30, val loss: 1.898197889328003
Epoch 40, training loss: 17.98029899597168 = 1.880677580833435 + 2.0 * 8.049810409545898
Epoch 40, val loss: 1.8800368309020996
Epoch 50, training loss: 16.737794876098633 = 1.8591676950454712 + 2.0 * 7.4393134117126465
Epoch 50, val loss: 1.8603320121765137
Epoch 60, training loss: 15.960060119628906 = 1.846524953842163 + 2.0 * 7.056767463684082
Epoch 60, val loss: 1.8491090536117554
Epoch 70, training loss: 15.440003395080566 = 1.838564157485962 + 2.0 * 6.800719738006592
Epoch 70, val loss: 1.8411229848861694
Epoch 80, training loss: 15.173712730407715 = 1.827947974205017 + 2.0 * 6.672882556915283
Epoch 80, val loss: 1.8313939571380615
Epoch 90, training loss: 14.973403930664062 = 1.816471815109253 + 2.0 * 6.578465938568115
Epoch 90, val loss: 1.8214890956878662
Epoch 100, training loss: 14.791739463806152 = 1.807970404624939 + 2.0 * 6.491884708404541
Epoch 100, val loss: 1.8142882585525513
Epoch 110, training loss: 14.674895286560059 = 1.8012908697128296 + 2.0 * 6.436802387237549
Epoch 110, val loss: 1.8082329034805298
Epoch 120, training loss: 14.582103729248047 = 1.794045329093933 + 2.0 * 6.394029140472412
Epoch 120, val loss: 1.8012832403182983
Epoch 130, training loss: 14.505890846252441 = 1.7866061925888062 + 2.0 * 6.359642505645752
Epoch 130, val loss: 1.7942044734954834
Epoch 140, training loss: 14.433917045593262 = 1.7791422605514526 + 2.0 * 6.32738733291626
Epoch 140, val loss: 1.7871017456054688
Epoch 150, training loss: 14.374149322509766 = 1.7714574337005615 + 2.0 * 6.3013458251953125
Epoch 150, val loss: 1.780074954032898
Epoch 160, training loss: 14.322717666625977 = 1.7630727291107178 + 2.0 * 6.27982234954834
Epoch 160, val loss: 1.7727878093719482
Epoch 170, training loss: 14.264392852783203 = 1.753722071647644 + 2.0 * 6.255335330963135
Epoch 170, val loss: 1.7649917602539062
Epoch 180, training loss: 14.21225643157959 = 1.7432271242141724 + 2.0 * 6.2345147132873535
Epoch 180, val loss: 1.7564008235931396
Epoch 190, training loss: 14.168746948242188 = 1.731145977973938 + 2.0 * 6.2188005447387695
Epoch 190, val loss: 1.7465656995773315
Epoch 200, training loss: 14.119681358337402 = 1.7171084880828857 + 2.0 * 6.201286315917969
Epoch 200, val loss: 1.7351969480514526
Epoch 210, training loss: 14.077723503112793 = 1.7007085084915161 + 2.0 * 6.188507556915283
Epoch 210, val loss: 1.7219746112823486
Epoch 220, training loss: 14.034510612487793 = 1.6814860105514526 + 2.0 * 6.176512241363525
Epoch 220, val loss: 1.7065205574035645
Epoch 230, training loss: 13.992290496826172 = 1.6590018272399902 + 2.0 * 6.16664457321167
Epoch 230, val loss: 1.6884124279022217
Epoch 240, training loss: 13.963900566101074 = 1.6326864957809448 + 2.0 * 6.16560697555542
Epoch 240, val loss: 1.6671531200408936
Epoch 250, training loss: 13.906900405883789 = 1.6024115085601807 + 2.0 * 6.152244567871094
Epoch 250, val loss: 1.642683506011963
Epoch 260, training loss: 13.85925006866455 = 1.5679513216018677 + 2.0 * 6.145649433135986
Epoch 260, val loss: 1.614700198173523
Epoch 270, training loss: 13.808298110961914 = 1.5288902521133423 + 2.0 * 6.139703750610352
Epoch 270, val loss: 1.582927942276001
Epoch 280, training loss: 13.758919715881348 = 1.4853826761245728 + 2.0 * 6.136768341064453
Epoch 280, val loss: 1.547489047050476
Epoch 290, training loss: 13.69969367980957 = 1.4387478828430176 + 2.0 * 6.130472660064697
Epoch 290, val loss: 1.5095343589782715
Epoch 300, training loss: 13.640860557556152 = 1.3901909589767456 + 2.0 * 6.125334739685059
Epoch 300, val loss: 1.4701623916625977
Epoch 310, training loss: 13.582463264465332 = 1.3404632806777954 + 2.0 * 6.120999813079834
Epoch 310, val loss: 1.4300355911254883
Epoch 320, training loss: 13.542980194091797 = 1.2908070087432861 + 2.0 * 6.126086711883545
Epoch 320, val loss: 1.3904753923416138
Epoch 330, training loss: 13.470662117004395 = 1.2435684204101562 + 2.0 * 6.113546848297119
Epoch 330, val loss: 1.353236198425293
Epoch 340, training loss: 13.42005729675293 = 1.1988177299499512 + 2.0 * 6.110620021820068
Epoch 340, val loss: 1.3183891773223877
Epoch 350, training loss: 13.367411613464355 = 1.156134009361267 + 2.0 * 6.1056389808654785
Epoch 350, val loss: 1.2856149673461914
Epoch 360, training loss: 13.32660961151123 = 1.1153748035430908 + 2.0 * 6.105617523193359
Epoch 360, val loss: 1.254919409751892
Epoch 370, training loss: 13.27740478515625 = 1.077170968055725 + 2.0 * 6.100116729736328
Epoch 370, val loss: 1.2265710830688477
Epoch 380, training loss: 13.234683990478516 = 1.041176199913025 + 2.0 * 6.09675407409668
Epoch 380, val loss: 1.20038640499115
Epoch 390, training loss: 13.19371223449707 = 1.0068084001541138 + 2.0 * 6.093451976776123
Epoch 390, val loss: 1.1756118535995483
Epoch 400, training loss: 13.169310569763184 = 0.9737335443496704 + 2.0 * 6.097788333892822
Epoch 400, val loss: 1.1519616842269897
Epoch 410, training loss: 13.120445251464844 = 0.9421265125274658 + 2.0 * 6.0891594886779785
Epoch 410, val loss: 1.1296968460083008
Epoch 420, training loss: 13.08210563659668 = 0.9116719961166382 + 2.0 * 6.085216999053955
Epoch 420, val loss: 1.108465313911438
Epoch 430, training loss: 13.048008918762207 = 0.8819349408149719 + 2.0 * 6.08303689956665
Epoch 430, val loss: 1.0877759456634521
Epoch 440, training loss: 13.016914367675781 = 0.8528613448143005 + 2.0 * 6.082026481628418
Epoch 440, val loss: 1.0677496194839478
Epoch 450, training loss: 12.983219146728516 = 0.8247327208518982 + 2.0 * 6.079243183135986
Epoch 450, val loss: 1.0487130880355835
Epoch 460, training loss: 12.949460983276367 = 0.7973921298980713 + 2.0 * 6.0760345458984375
Epoch 460, val loss: 1.0305862426757812
Epoch 470, training loss: 12.920501708984375 = 0.7706742286682129 + 2.0 * 6.074913501739502
Epoch 470, val loss: 1.013282299041748
Epoch 480, training loss: 12.897223472595215 = 0.7445580959320068 + 2.0 * 6.0763325691223145
Epoch 480, val loss: 0.9968917369842529
Epoch 490, training loss: 12.86116886138916 = 0.7193365693092346 + 2.0 * 6.070916175842285
Epoch 490, val loss: 0.9815376400947571
Epoch 500, training loss: 12.83368968963623 = 0.6948021650314331 + 2.0 * 6.069443702697754
Epoch 500, val loss: 0.9673075079917908
Epoch 510, training loss: 12.803741455078125 = 0.670751690864563 + 2.0 * 6.066494941711426
Epoch 510, val loss: 0.9538787007331848
Epoch 520, training loss: 12.793805122375488 = 0.6471536755561829 + 2.0 * 6.0733256340026855
Epoch 520, val loss: 0.9412792325019836
Epoch 530, training loss: 12.756564140319824 = 0.6242095232009888 + 2.0 * 6.0661773681640625
Epoch 530, val loss: 0.9296872615814209
Epoch 540, training loss: 12.726655006408691 = 0.6016934514045715 + 2.0 * 6.062480926513672
Epoch 540, val loss: 0.9189985990524292
Epoch 550, training loss: 12.711395263671875 = 0.5794976949691772 + 2.0 * 6.065948963165283
Epoch 550, val loss: 0.9087962508201599
Epoch 560, training loss: 12.683448791503906 = 0.5576990246772766 + 2.0 * 6.062874794006348
Epoch 560, val loss: 0.8993844985961914
Epoch 570, training loss: 12.654821395874023 = 0.5365355014801025 + 2.0 * 6.05914306640625
Epoch 570, val loss: 0.8908939361572266
Epoch 580, training loss: 12.630606651306152 = 0.5157142877578735 + 2.0 * 6.057446002960205
Epoch 580, val loss: 0.8828723430633545
Epoch 590, training loss: 12.605867385864258 = 0.49539893865585327 + 2.0 * 6.055234432220459
Epoch 590, val loss: 0.8755059242248535
Epoch 600, training loss: 12.586870193481445 = 0.47577714920043945 + 2.0 * 6.055546283721924
Epoch 600, val loss: 0.8690587878227234
Epoch 610, training loss: 12.567996978759766 = 0.45673036575317383 + 2.0 * 6.055633544921875
Epoch 610, val loss: 0.8632408976554871
Epoch 620, training loss: 12.543861389160156 = 0.43836110830307007 + 2.0 * 6.052750110626221
Epoch 620, val loss: 0.8579911589622498
Epoch 630, training loss: 12.526815414428711 = 0.4207373559474945 + 2.0 * 6.053039073944092
Epoch 630, val loss: 0.8535415530204773
Epoch 640, training loss: 12.505339622497559 = 0.403826504945755 + 2.0 * 6.050756454467773
Epoch 640, val loss: 0.849608838558197
Epoch 650, training loss: 12.485676765441895 = 0.3876732289791107 + 2.0 * 6.049001693725586
Epoch 650, val loss: 0.8462233543395996
Epoch 660, training loss: 12.469459533691406 = 0.37230247259140015 + 2.0 * 6.04857873916626
Epoch 660, val loss: 0.843593418598175
Epoch 670, training loss: 12.449990272521973 = 0.35758456587791443 + 2.0 * 6.046202659606934
Epoch 670, val loss: 0.841395914554596
Epoch 680, training loss: 12.434967994689941 = 0.34345540404319763 + 2.0 * 6.0457563400268555
Epoch 680, val loss: 0.8396399021148682
Epoch 690, training loss: 12.438096046447754 = 0.3299401104450226 + 2.0 * 6.054078102111816
Epoch 690, val loss: 0.8382132053375244
Epoch 700, training loss: 12.412901878356934 = 0.317147433757782 + 2.0 * 6.047877311706543
Epoch 700, val loss: 0.8371217250823975
Epoch 710, training loss: 12.394277572631836 = 0.3049619197845459 + 2.0 * 6.0446577072143555
Epoch 710, val loss: 0.836560070514679
Epoch 720, training loss: 12.404934883117676 = 0.293207585811615 + 2.0 * 6.055863857269287
Epoch 720, val loss: 0.8359737992286682
Epoch 730, training loss: 12.369417190551758 = 0.2820368707180023 + 2.0 * 6.043690204620361
Epoch 730, val loss: 0.835731029510498
Epoch 740, training loss: 12.351463317871094 = 0.2712838649749756 + 2.0 * 6.0400896072387695
Epoch 740, val loss: 0.8358247876167297
Epoch 750, training loss: 12.339498519897461 = 0.26084426045417786 + 2.0 * 6.039327144622803
Epoch 750, val loss: 0.835965633392334
Epoch 760, training loss: 12.32729721069336 = 0.25070804357528687 + 2.0 * 6.038294792175293
Epoch 760, val loss: 0.8363361954689026
Epoch 770, training loss: 12.334067344665527 = 0.24086369574069977 + 2.0 * 6.04660177230835
Epoch 770, val loss: 0.8367767333984375
Epoch 780, training loss: 12.30955696105957 = 0.23140713572502136 + 2.0 * 6.039074897766113
Epoch 780, val loss: 0.8371528387069702
Epoch 790, training loss: 12.295263290405273 = 0.2223162204027176 + 2.0 * 6.036473751068115
Epoch 790, val loss: 0.8379404544830322
Epoch 800, training loss: 12.285501480102539 = 0.21350526809692383 + 2.0 * 6.0359978675842285
Epoch 800, val loss: 0.8387366533279419
Epoch 810, training loss: 12.27859115600586 = 0.20495910942554474 + 2.0 * 6.036816120147705
Epoch 810, val loss: 0.8395972847938538
Epoch 820, training loss: 12.265816688537598 = 0.19668170809745789 + 2.0 * 6.034567356109619
Epoch 820, val loss: 0.8405494689941406
Epoch 830, training loss: 12.259910583496094 = 0.1886780560016632 + 2.0 * 6.035616397857666
Epoch 830, val loss: 0.8416682481765747
Epoch 840, training loss: 12.258343696594238 = 0.18095359206199646 + 2.0 * 6.038694858551025
Epoch 840, val loss: 0.8427311182022095
Epoch 850, training loss: 12.242213249206543 = 0.17350900173187256 + 2.0 * 6.0343523025512695
Epoch 850, val loss: 0.8439838886260986
Epoch 860, training loss: 12.231350898742676 = 0.1663770079612732 + 2.0 * 6.032486915588379
Epoch 860, val loss: 0.8454186916351318
Epoch 870, training loss: 12.228484153747559 = 0.15947669744491577 + 2.0 * 6.034503936767578
Epoch 870, val loss: 0.8468620777130127
Epoch 880, training loss: 12.222002029418945 = 0.1528836190700531 + 2.0 * 6.03455924987793
Epoch 880, val loss: 0.8482529520988464
Epoch 890, training loss: 12.206282615661621 = 0.146588534116745 + 2.0 * 6.029847145080566
Epoch 890, val loss: 0.8499959111213684
Epoch 900, training loss: 12.199511528015137 = 0.1405525654554367 + 2.0 * 6.029479503631592
Epoch 900, val loss: 0.8517646789550781
Epoch 910, training loss: 12.19471549987793 = 0.1347591131925583 + 2.0 * 6.029978275299072
Epoch 910, val loss: 0.853655219078064
Epoch 920, training loss: 12.187360763549805 = 0.12919774651527405 + 2.0 * 6.029081344604492
Epoch 920, val loss: 0.8554580211639404
Epoch 930, training loss: 12.185826301574707 = 0.12392114102840424 + 2.0 * 6.030952453613281
Epoch 930, val loss: 0.8574846982955933
Epoch 940, training loss: 12.173148155212402 = 0.11889322847127914 + 2.0 * 6.027127265930176
Epoch 940, val loss: 0.8596788048744202
Epoch 950, training loss: 12.169360160827637 = 0.114079549908638 + 2.0 * 6.027640342712402
Epoch 950, val loss: 0.8619318604469299
Epoch 960, training loss: 12.167665481567383 = 0.10947895795106888 + 2.0 * 6.029093265533447
Epoch 960, val loss: 0.8642259836196899
Epoch 970, training loss: 12.161645889282227 = 0.1050930917263031 + 2.0 * 6.028276443481445
Epoch 970, val loss: 0.8666077256202698
Epoch 980, training loss: 12.153398513793945 = 0.10092271864414215 + 2.0 * 6.026237964630127
Epoch 980, val loss: 0.8691073656082153
Epoch 990, training loss: 12.15159797668457 = 0.09695325046777725 + 2.0 * 6.027322292327881
Epoch 990, val loss: 0.8716157078742981
=== picking the best model according to the performance on validation ===
target class rate on Vs: 0.8000
accuracy on clean test nodes: 0.7815
Overall ASR: 0.5240
Flip ASR: 0.4444/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.69415855407715 = 1.9464586973190308 + 2.0 * 8.373849868774414
Epoch 0, val loss: 1.952992558479309
Epoch 10, training loss: 18.68350601196289 = 1.936646580696106 + 2.0 * 8.373429298400879
Epoch 10, val loss: 1.943115234375
Epoch 20, training loss: 18.666173934936523 = 1.924662709236145 + 2.0 * 8.370755195617676
Epoch 20, val loss: 1.930540919303894
Epoch 30, training loss: 18.61170196533203 = 1.9086236953735352 + 2.0 * 8.35153865814209
Epoch 30, val loss: 1.9135338068008423
Epoch 40, training loss: 18.278257369995117 = 1.8894704580307007 + 2.0 * 8.194393157958984
Epoch 40, val loss: 1.8940327167510986
Epoch 50, training loss: 16.55866813659668 = 1.8692361116409302 + 2.0 * 7.3447160720825195
Epoch 50, val loss: 1.8738682270050049
Epoch 60, training loss: 15.880452156066895 = 1.8557723760604858 + 2.0 * 7.012340068817139
Epoch 60, val loss: 1.8602410554885864
Epoch 70, training loss: 15.349220275878906 = 1.8444873094558716 + 2.0 * 6.752366542816162
Epoch 70, val loss: 1.8477318286895752
Epoch 80, training loss: 15.016529083251953 = 1.8343400955200195 + 2.0 * 6.591094493865967
Epoch 80, val loss: 1.8371236324310303
Epoch 90, training loss: 14.838188171386719 = 1.8247570991516113 + 2.0 * 6.506715297698975
Epoch 90, val loss: 1.8269842863082886
Epoch 100, training loss: 14.695060729980469 = 1.815563440322876 + 2.0 * 6.439748764038086
Epoch 100, val loss: 1.8168526887893677
Epoch 110, training loss: 14.578194618225098 = 1.806778073310852 + 2.0 * 6.385708332061768
Epoch 110, val loss: 1.8070083856582642
Epoch 120, training loss: 14.482704162597656 = 1.7985669374465942 + 2.0 * 6.342068672180176
Epoch 120, val loss: 1.7978755235671997
Epoch 130, training loss: 14.40623664855957 = 1.790673017501831 + 2.0 * 6.30778169631958
Epoch 130, val loss: 1.7893396615982056
Epoch 140, training loss: 14.346868515014648 = 1.7825734615325928 + 2.0 * 6.282147407531738
Epoch 140, val loss: 1.7807235717773438
Epoch 150, training loss: 14.296441078186035 = 1.773995041847229 + 2.0 * 6.261222839355469
Epoch 150, val loss: 1.771842360496521
Epoch 160, training loss: 14.2476224899292 = 1.7648919820785522 + 2.0 * 6.241365432739258
Epoch 160, val loss: 1.7627761363983154
Epoch 170, training loss: 14.208213806152344 = 1.7550650835037231 + 2.0 * 6.226574420928955
Epoch 170, val loss: 1.7534208297729492
Epoch 180, training loss: 14.167581558227539 = 1.7440911531448364 + 2.0 * 6.211745262145996
Epoch 180, val loss: 1.7435070276260376
Epoch 190, training loss: 14.1303071975708 = 1.7316375970840454 + 2.0 * 6.199334621429443
Epoch 190, val loss: 1.7325365543365479
Epoch 200, training loss: 14.098118782043457 = 1.7172698974609375 + 2.0 * 6.19042444229126
Epoch 200, val loss: 1.7201448678970337
Epoch 210, training loss: 14.06038761138916 = 1.7005990743637085 + 2.0 * 6.17989444732666
Epoch 210, val loss: 1.7062318325042725
Epoch 220, training loss: 14.022500038146973 = 1.681296944618225 + 2.0 * 6.1706013679504395
Epoch 220, val loss: 1.690262794494629
Epoch 230, training loss: 13.984870910644531 = 1.6585863828659058 + 2.0 * 6.163142204284668
Epoch 230, val loss: 1.671764612197876
Epoch 240, training loss: 13.942756652832031 = 1.6319522857666016 + 2.0 * 6.155402183532715
Epoch 240, val loss: 1.6500388383865356
Epoch 250, training loss: 13.897899627685547 = 1.6005558967590332 + 2.0 * 6.148672103881836
Epoch 250, val loss: 1.6247848272323608
Epoch 260, training loss: 13.85054874420166 = 1.5638377666473389 + 2.0 * 6.143355369567871
Epoch 260, val loss: 1.5953994989395142
Epoch 270, training loss: 13.79884147644043 = 1.5221617221832275 + 2.0 * 6.138339996337891
Epoch 270, val loss: 1.5616555213928223
Epoch 280, training loss: 13.738134384155273 = 1.4752062559127808 + 2.0 * 6.131464004516602
Epoch 280, val loss: 1.524034023284912
Epoch 290, training loss: 13.67697525024414 = 1.4230589866638184 + 2.0 * 6.12695837020874
Epoch 290, val loss: 1.4820778369903564
Epoch 300, training loss: 13.611678123474121 = 1.3662395477294922 + 2.0 * 6.1227192878723145
Epoch 300, val loss: 1.4363080263137817
Epoch 310, training loss: 13.548066139221191 = 1.3065966367721558 + 2.0 * 6.120734691619873
Epoch 310, val loss: 1.388255000114441
Epoch 320, training loss: 13.480685234069824 = 1.2465403079986572 + 2.0 * 6.117072582244873
Epoch 320, val loss: 1.3397998809814453
Epoch 330, training loss: 13.41193962097168 = 1.1874501705169678 + 2.0 * 6.112244606018066
Epoch 330, val loss: 1.2922779321670532
Epoch 340, training loss: 13.347343444824219 = 1.1298017501831055 + 2.0 * 6.108770847320557
Epoch 340, val loss: 1.2463369369506836
Epoch 350, training loss: 13.294458389282227 = 1.0740293264389038 + 2.0 * 6.110214710235596
Epoch 350, val loss: 1.2017942667007446
Epoch 360, training loss: 13.228605270385742 = 1.021253228187561 + 2.0 * 6.103675842285156
Epoch 360, val loss: 1.1604714393615723
Epoch 370, training loss: 13.173221588134766 = 0.9717199206352234 + 2.0 * 6.100750923156738
Epoch 370, val loss: 1.1222705841064453
Epoch 380, training loss: 13.117807388305664 = 0.9247199296951294 + 2.0 * 6.096543788909912
Epoch 380, val loss: 1.086346983909607
Epoch 390, training loss: 13.077073097229004 = 0.8803888559341431 + 2.0 * 6.098341941833496
Epoch 390, val loss: 1.052839994430542
Epoch 400, training loss: 13.023625373840332 = 0.8394424915313721 + 2.0 * 6.0920915603637695
Epoch 400, val loss: 1.0224231481552124
Epoch 410, training loss: 12.985862731933594 = 0.8017398715019226 + 2.0 * 6.092061519622803
Epoch 410, val loss: 0.9948192238807678
Epoch 420, training loss: 12.940871238708496 = 0.767109751701355 + 2.0 * 6.086880683898926
Epoch 420, val loss: 0.9700727462768555
Epoch 430, training loss: 12.911344528198242 = 0.7350303530693054 + 2.0 * 6.0881571769714355
Epoch 430, val loss: 0.9475969076156616
Epoch 440, training loss: 12.871487617492676 = 0.7056475281715393 + 2.0 * 6.082920074462891
Epoch 440, val loss: 0.9276351928710938
Epoch 450, training loss: 12.836626052856445 = 0.6784386038780212 + 2.0 * 6.079093933105469
Epoch 450, val loss: 0.9097492694854736
Epoch 460, training loss: 12.806641578674316 = 0.6528366804122925 + 2.0 * 6.076902389526367
Epoch 460, val loss: 0.8934394121170044
Epoch 470, training loss: 12.798263549804688 = 0.6285347938537598 + 2.0 * 6.084864616394043
Epoch 470, val loss: 0.8785619139671326
Epoch 480, training loss: 12.756983757019043 = 0.6058573126792908 + 2.0 * 6.075563430786133
Epoch 480, val loss: 0.8651297688484192
Epoch 490, training loss: 12.726422309875488 = 0.5841227173805237 + 2.0 * 6.071149826049805
Epoch 490, val loss: 0.8528938293457031
Epoch 500, training loss: 12.701215744018555 = 0.5630670785903931 + 2.0 * 6.0690741539001465
Epoch 500, val loss: 0.8413749933242798
Epoch 510, training loss: 12.681587219238281 = 0.5426039695739746 + 2.0 * 6.069491386413574
Epoch 510, val loss: 0.8307047486305237
Epoch 520, training loss: 12.67684555053711 = 0.522888720035553 + 2.0 * 6.0769782066345215
Epoch 520, val loss: 0.8209087252616882
Epoch 530, training loss: 12.635456085205078 = 0.5037586092948914 + 2.0 * 6.0658488273620605
Epoch 530, val loss: 0.812072217464447
Epoch 540, training loss: 12.61113166809082 = 0.4851734936237335 + 2.0 * 6.062979221343994
Epoch 540, val loss: 0.8040376901626587
Epoch 550, training loss: 12.587882041931152 = 0.46694955229759216 + 2.0 * 6.060466289520264
Epoch 550, val loss: 0.7966324687004089
Epoch 560, training loss: 12.58650016784668 = 0.44914013147354126 + 2.0 * 6.0686798095703125
Epoch 560, val loss: 0.7899789810180664
Epoch 570, training loss: 12.545867919921875 = 0.43183672428131104 + 2.0 * 6.057015419006348
Epoch 570, val loss: 0.7843167781829834
Epoch 580, training loss: 12.527103424072266 = 0.41508421301841736 + 2.0 * 6.056009769439697
Epoch 580, val loss: 0.7795352935791016
Epoch 590, training loss: 12.524991035461426 = 0.3988109827041626 + 2.0 * 6.063089847564697
Epoch 590, val loss: 0.7754778265953064
Epoch 600, training loss: 12.494612693786621 = 0.3832748234272003 + 2.0 * 6.055668830871582
Epoch 600, val loss: 0.7722903490066528
Epoch 610, training loss: 12.47393798828125 = 0.36838406324386597 + 2.0 * 6.05277681350708
Epoch 610, val loss: 0.7700231671333313
Epoch 620, training loss: 12.460748672485352 = 0.35405683517456055 + 2.0 * 6.053346157073975
Epoch 620, val loss: 0.7684001922607422
Epoch 630, training loss: 12.44715690612793 = 0.3403370976448059 + 2.0 * 6.053410053253174
Epoch 630, val loss: 0.7674447298049927
Epoch 640, training loss: 12.425982475280762 = 0.3273214101791382 + 2.0 * 6.049330711364746
Epoch 640, val loss: 0.7672674059867859
Epoch 650, training loss: 12.409262657165527 = 0.3148220181465149 + 2.0 * 6.047220230102539
Epoch 650, val loss: 0.7675523161888123
Epoch 660, training loss: 12.402518272399902 = 0.30283474922180176 + 2.0 * 6.04984188079834
Epoch 660, val loss: 0.7682533264160156
Epoch 670, training loss: 12.387066841125488 = 0.29142388701438904 + 2.0 * 6.047821521759033
Epoch 670, val loss: 0.7693969011306763
Epoch 680, training loss: 12.373930931091309 = 0.2805272340774536 + 2.0 * 6.046701908111572
Epoch 680, val loss: 0.7708960771560669
Epoch 690, training loss: 12.359903335571289 = 0.27010995149612427 + 2.0 * 6.044896602630615
Epoch 690, val loss: 0.7727862000465393
Epoch 700, training loss: 12.369738578796387 = 0.2601158916950226 + 2.0 * 6.054811477661133
Epoch 700, val loss: 0.7748181819915771
Epoch 710, training loss: 12.33336353302002 = 0.250723272562027 + 2.0 * 6.041320323944092
Epoch 710, val loss: 0.777155339717865
Epoch 720, training loss: 12.322551727294922 = 0.24171064794063568 + 2.0 * 6.0404205322265625
Epoch 720, val loss: 0.7798794507980347
Epoch 730, training loss: 12.311338424682617 = 0.2329980581998825 + 2.0 * 6.039170265197754
Epoch 730, val loss: 0.7826411724090576
Epoch 740, training loss: 12.304120063781738 = 0.22459229826927185 + 2.0 * 6.039763927459717
Epoch 740, val loss: 0.7856540083885193
Epoch 750, training loss: 12.293403625488281 = 0.21654252707958221 + 2.0 * 6.038430690765381
Epoch 750, val loss: 0.7887710332870483
Epoch 760, training loss: 12.288875579833984 = 0.20886409282684326 + 2.0 * 6.040005683898926
Epoch 760, val loss: 0.7921183705329895
Epoch 770, training loss: 12.27499008178711 = 0.20149821043014526 + 2.0 * 6.036746025085449
Epoch 770, val loss: 0.7956544160842896
Epoch 780, training loss: 12.267613410949707 = 0.19439521431922913 + 2.0 * 6.036609172821045
Epoch 780, val loss: 0.799285888671875
Epoch 790, training loss: 12.261969566345215 = 0.18756765127182007 + 2.0 * 6.037200927734375
Epoch 790, val loss: 0.8029621839523315
Epoch 800, training loss: 12.248855590820312 = 0.1810217946767807 + 2.0 * 6.03391695022583
Epoch 800, val loss: 0.8068500757217407
Epoch 810, training loss: 12.242201805114746 = 0.1747274547815323 + 2.0 * 6.0337371826171875
Epoch 810, val loss: 0.8109092712402344
Epoch 820, training loss: 12.244020462036133 = 0.16864408552646637 + 2.0 * 6.037688255310059
Epoch 820, val loss: 0.8149425387382507
Epoch 830, training loss: 12.23585319519043 = 0.16283932328224182 + 2.0 * 6.0365071296691895
Epoch 830, val loss: 0.8191879987716675
Epoch 840, training loss: 12.220999717712402 = 0.1572508066892624 + 2.0 * 6.031874656677246
Epoch 840, val loss: 0.82355135679245
Epoch 850, training loss: 12.215842247009277 = 0.1518518477678299 + 2.0 * 6.0319952964782715
Epoch 850, val loss: 0.8279045820236206
Epoch 860, training loss: 12.209924697875977 = 0.1466423124074936 + 2.0 * 6.031641006469727
Epoch 860, val loss: 0.83236163854599
Epoch 870, training loss: 12.20292854309082 = 0.14163652062416077 + 2.0 * 6.030645847320557
Epoch 870, val loss: 0.8370040655136108
Epoch 880, training loss: 12.198104858398438 = 0.13679596781730652 + 2.0 * 6.030654430389404
Epoch 880, val loss: 0.8415859937667847
Epoch 890, training loss: 12.189818382263184 = 0.13215206563472748 + 2.0 * 6.028833389282227
Epoch 890, val loss: 0.8462564945220947
Epoch 900, training loss: 12.184615135192871 = 0.12768514454364777 + 2.0 * 6.0284647941589355
Epoch 900, val loss: 0.8510333299636841
Epoch 910, training loss: 12.177422523498535 = 0.12336143851280212 + 2.0 * 6.0270304679870605
Epoch 910, val loss: 0.8558726906776428
Epoch 920, training loss: 12.176992416381836 = 0.11916013062000275 + 2.0 * 6.028916358947754
Epoch 920, val loss: 0.860734224319458
Epoch 930, training loss: 12.17092227935791 = 0.11508767306804657 + 2.0 * 6.027917385101318
Epoch 930, val loss: 0.865463137626648
Epoch 940, training loss: 12.163705825805664 = 0.11116231977939606 + 2.0 * 6.026271820068359
Epoch 940, val loss: 0.8704099059104919
Epoch 950, training loss: 12.157807350158691 = 0.10735972970724106 + 2.0 * 6.025223731994629
Epoch 950, val loss: 0.8753240704536438
Epoch 960, training loss: 12.15713119506836 = 0.10367181152105331 + 2.0 * 6.026729583740234
Epoch 960, val loss: 0.8802269101142883
Epoch 970, training loss: 12.147915840148926 = 0.10009392350912094 + 2.0 * 6.023910999298096
Epoch 970, val loss: 0.8851048350334167
Epoch 980, training loss: 12.145685195922852 = 0.09663473814725876 + 2.0 * 6.024525165557861
Epoch 980, val loss: 0.8900630474090576
Epoch 990, training loss: 12.139410018920898 = 0.09328761696815491 + 2.0 * 6.023061275482178
Epoch 990, val loss: 0.8949365615844727
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7852
Overall ASR: 0.7528
Flip ASR: 0.7067/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.714012145996094 = 1.9663358926773071 + 2.0 * 8.373838424682617
Epoch 0, val loss: 1.9762457609176636
Epoch 10, training loss: 18.701129913330078 = 1.9547032117843628 + 2.0 * 8.373213768005371
Epoch 10, val loss: 1.9635348320007324
Epoch 20, training loss: 18.679231643676758 = 1.9400478601455688 + 2.0 * 8.36959171295166
Epoch 20, val loss: 1.9469609260559082
Epoch 30, training loss: 18.61322021484375 = 1.9203498363494873 + 2.0 * 8.346435546875
Epoch 30, val loss: 1.924499273300171
Epoch 40, training loss: 18.264192581176758 = 1.897621512413025 + 2.0 * 8.1832857131958
Epoch 40, val loss: 1.8997838497161865
Epoch 50, training loss: 16.42738151550293 = 1.874062418937683 + 2.0 * 7.2766594886779785
Epoch 50, val loss: 1.8745530843734741
Epoch 60, training loss: 15.688400268554688 = 1.8585352897644043 + 2.0 * 6.914932727813721
Epoch 60, val loss: 1.8598248958587646
Epoch 70, training loss: 15.296988487243652 = 1.8458305597305298 + 2.0 * 6.725578784942627
Epoch 70, val loss: 1.8471142053604126
Epoch 80, training loss: 15.075019836425781 = 1.8335764408111572 + 2.0 * 6.620721817016602
Epoch 80, val loss: 1.8347539901733398
Epoch 90, training loss: 14.87346076965332 = 1.8211239576339722 + 2.0 * 6.526168346405029
Epoch 90, val loss: 1.8221174478530884
Epoch 100, training loss: 14.706682205200195 = 1.8100366592407227 + 2.0 * 6.448322772979736
Epoch 100, val loss: 1.8107848167419434
Epoch 110, training loss: 14.593624114990234 = 1.8003405332565308 + 2.0 * 6.396641731262207
Epoch 110, val loss: 1.800806999206543
Epoch 120, training loss: 14.508316993713379 = 1.7912490367889404 + 2.0 * 6.35853385925293
Epoch 120, val loss: 1.7914438247680664
Epoch 130, training loss: 14.437257766723633 = 1.7823554277420044 + 2.0 * 6.327451229095459
Epoch 130, val loss: 1.782660961151123
Epoch 140, training loss: 14.374069213867188 = 1.773484468460083 + 2.0 * 6.300292491912842
Epoch 140, val loss: 1.7744413614273071
Epoch 150, training loss: 14.324742317199707 = 1.7640763521194458 + 2.0 * 6.280333042144775
Epoch 150, val loss: 1.7660834789276123
Epoch 160, training loss: 14.276232719421387 = 1.7537585496902466 + 2.0 * 6.261237144470215
Epoch 160, val loss: 1.7572649717330933
Epoch 170, training loss: 14.23273754119873 = 1.7422078847885132 + 2.0 * 6.245265007019043
Epoch 170, val loss: 1.7476454973220825
Epoch 180, training loss: 14.194585800170898 = 1.7291436195373535 + 2.0 * 6.232721328735352
Epoch 180, val loss: 1.7369810342788696
Epoch 190, training loss: 14.155027389526367 = 1.7144101858139038 + 2.0 * 6.220308780670166
Epoch 190, val loss: 1.725056767463684
Epoch 200, training loss: 14.116231918334961 = 1.6976491212844849 + 2.0 * 6.209291458129883
Epoch 200, val loss: 1.7117522954940796
Epoch 210, training loss: 14.078734397888184 = 1.6783732175827026 + 2.0 * 6.200180530548096
Epoch 210, val loss: 1.6966503858566284
Epoch 220, training loss: 14.036447525024414 = 1.656221628189087 + 2.0 * 6.190113067626953
Epoch 220, val loss: 1.6793099641799927
Epoch 230, training loss: 13.993436813354492 = 1.6303175687789917 + 2.0 * 6.1815595626831055
Epoch 230, val loss: 1.6592028141021729
Epoch 240, training loss: 13.947723388671875 = 1.599970817565918 + 2.0 * 6.1738762855529785
Epoch 240, val loss: 1.6356115341186523
Epoch 250, training loss: 13.902427673339844 = 1.564698338508606 + 2.0 * 6.168864727020264
Epoch 250, val loss: 1.6081629991531372
Epoch 260, training loss: 13.848801612854004 = 1.5246421098709106 + 2.0 * 6.162079811096191
Epoch 260, val loss: 1.5769994258880615
Epoch 270, training loss: 13.79271125793457 = 1.479072093963623 + 2.0 * 6.1568193435668945
Epoch 270, val loss: 1.5413459539413452
Epoch 280, training loss: 13.737837791442871 = 1.4276134967803955 + 2.0 * 6.155112266540527
Epoch 280, val loss: 1.5010411739349365
Epoch 290, training loss: 13.66555118560791 = 1.3718045949935913 + 2.0 * 6.146873474121094
Epoch 290, val loss: 1.456893801689148
Epoch 300, training loss: 13.596217155456543 = 1.3119550943374634 + 2.0 * 6.1421308517456055
Epoch 300, val loss: 1.409287929534912
Epoch 310, training loss: 13.538147926330566 = 1.2490568161010742 + 2.0 * 6.144545555114746
Epoch 310, val loss: 1.3590147495269775
Epoch 320, training loss: 13.457562446594238 = 1.1861906051635742 + 2.0 * 6.135685920715332
Epoch 320, val loss: 1.3084981441497803
Epoch 330, training loss: 13.3832368850708 = 1.1242300271987915 + 2.0 * 6.12950325012207
Epoch 330, val loss: 1.2587345838546753
Epoch 340, training loss: 13.320240020751953 = 1.0640658140182495 + 2.0 * 6.128087043762207
Epoch 340, val loss: 1.2101378440856934
Epoch 350, training loss: 13.252366065979004 = 1.0076162815093994 + 2.0 * 6.122375011444092
Epoch 350, val loss: 1.1646819114685059
Epoch 360, training loss: 13.192837715148926 = 0.9554124474525452 + 2.0 * 6.118712425231934
Epoch 360, val loss: 1.1227973699569702
Epoch 370, training loss: 13.137526512145996 = 0.9072803258895874 + 2.0 * 6.115123271942139
Epoch 370, val loss: 1.0845028162002563
Epoch 380, training loss: 13.086087226867676 = 0.8637825846672058 + 2.0 * 6.111152172088623
Epoch 380, val loss: 1.0501291751861572
Epoch 390, training loss: 13.04066276550293 = 0.8241204619407654 + 2.0 * 6.10827112197876
Epoch 390, val loss: 1.019292950630188
Epoch 400, training loss: 13.007245063781738 = 0.7878400683403015 + 2.0 * 6.1097025871276855
Epoch 400, val loss: 0.9915505647659302
Epoch 410, training loss: 12.962925910949707 = 0.7553595304489136 + 2.0 * 6.103783130645752
Epoch 410, val loss: 0.9675350785255432
Epoch 420, training loss: 12.927390098571777 = 0.7259345650672913 + 2.0 * 6.100727558135986
Epoch 420, val loss: 0.9464642405509949
Epoch 430, training loss: 12.893193244934082 = 0.6987363696098328 + 2.0 * 6.097228527069092
Epoch 430, val loss: 0.927749514579773
Epoch 440, training loss: 12.86137866973877 = 0.6733909845352173 + 2.0 * 6.093993663787842
Epoch 440, val loss: 0.9110750555992126
Epoch 450, training loss: 12.841635704040527 = 0.6493948698043823 + 2.0 * 6.096120357513428
Epoch 450, val loss: 0.8960527181625366
Epoch 460, training loss: 12.808369636535645 = 0.6265667080879211 + 2.0 * 6.0909013748168945
Epoch 460, val loss: 0.8825125694274902
Epoch 470, training loss: 12.780735969543457 = 0.6047325134277344 + 2.0 * 6.088001728057861
Epoch 470, val loss: 0.8701940774917603
Epoch 480, training loss: 12.75282096862793 = 0.5834016799926758 + 2.0 * 6.084709644317627
Epoch 480, val loss: 0.8586559295654297
Epoch 490, training loss: 12.741347312927246 = 0.5624037981033325 + 2.0 * 6.089471817016602
Epoch 490, val loss: 0.847881019115448
Epoch 500, training loss: 12.709025382995605 = 0.5420419573783875 + 2.0 * 6.083491802215576
Epoch 500, val loss: 0.83788001537323
Epoch 510, training loss: 12.68360710144043 = 0.522116482257843 + 2.0 * 6.080745220184326
Epoch 510, val loss: 0.8287479281425476
Epoch 520, training loss: 12.657083511352539 = 0.502413272857666 + 2.0 * 6.077335357666016
Epoch 520, val loss: 0.820099413394928
Epoch 530, training loss: 12.63676643371582 = 0.48288750648498535 + 2.0 * 6.076939582824707
Epoch 530, val loss: 0.8120133280754089
Epoch 540, training loss: 12.625341415405273 = 0.46374887228012085 + 2.0 * 6.080796241760254
Epoch 540, val loss: 0.8046632409095764
Epoch 550, training loss: 12.592023849487305 = 0.44517436623573303 + 2.0 * 6.073424816131592
Epoch 550, val loss: 0.7979110479354858
Epoch 560, training loss: 12.57193660736084 = 0.4268675446510315 + 2.0 * 6.072534561157227
Epoch 560, val loss: 0.7917715907096863
Epoch 570, training loss: 12.549481391906738 = 0.4090103805065155 + 2.0 * 6.070235729217529
Epoch 570, val loss: 0.7861918807029724
Epoch 580, training loss: 12.531865119934082 = 0.3915922939777374 + 2.0 * 6.070136547088623
Epoch 580, val loss: 0.7813279032707214
Epoch 590, training loss: 12.509202003479004 = 0.37458688020706177 + 2.0 * 6.067307472229004
Epoch 590, val loss: 0.7769825458526611
Epoch 600, training loss: 12.494190216064453 = 0.3579455316066742 + 2.0 * 6.068122386932373
Epoch 600, val loss: 0.7731497883796692
Epoch 610, training loss: 12.487116813659668 = 0.341881662607193 + 2.0 * 6.072617530822754
Epoch 610, val loss: 0.7698964476585388
Epoch 620, training loss: 12.45391845703125 = 0.3264598250389099 + 2.0 * 6.063729286193848
Epoch 620, val loss: 0.7671830058097839
Epoch 630, training loss: 12.438363075256348 = 0.31153109669685364 + 2.0 * 6.063416004180908
Epoch 630, val loss: 0.7649767994880676
Epoch 640, training loss: 12.42200756072998 = 0.29704394936561584 + 2.0 * 6.062481880187988
Epoch 640, val loss: 0.7632523775100708
Epoch 650, training loss: 12.407418251037598 = 0.283171147108078 + 2.0 * 6.062123775482178
Epoch 650, val loss: 0.7619978189468384
Epoch 660, training loss: 12.389504432678223 = 0.2699578106403351 + 2.0 * 6.0597734451293945
Epoch 660, val loss: 0.7612342834472656
Epoch 670, training loss: 12.374221801757812 = 0.25729039311408997 + 2.0 * 6.058465480804443
Epoch 670, val loss: 0.7609495520591736
Epoch 680, training loss: 12.369070053100586 = 0.24514806270599365 + 2.0 * 6.0619611740112305
Epoch 680, val loss: 0.761107325553894
Epoch 690, training loss: 12.361062049865723 = 0.23373495042324066 + 2.0 * 6.063663482666016
Epoch 690, val loss: 0.7617496252059937
Epoch 700, training loss: 12.338146209716797 = 0.2228996604681015 + 2.0 * 6.057623386383057
Epoch 700, val loss: 0.7628526091575623
Epoch 710, training loss: 12.32292366027832 = 0.2125813364982605 + 2.0 * 6.055171012878418
Epoch 710, val loss: 0.7643818855285645
Epoch 720, training loss: 12.332331657409668 = 0.20277270674705505 + 2.0 * 6.064779281616211
Epoch 720, val loss: 0.7662673592567444
Epoch 730, training loss: 12.304892539978027 = 0.19363750517368317 + 2.0 * 6.055627346038818
Epoch 730, val loss: 0.7685027122497559
Epoch 740, training loss: 12.28912353515625 = 0.18498076498508453 + 2.0 * 6.052071571350098
Epoch 740, val loss: 0.7711707353591919
Epoch 750, training loss: 12.278925895690918 = 0.1767561286687851 + 2.0 * 6.051084995269775
Epoch 750, val loss: 0.7741866707801819
Epoch 760, training loss: 12.269075393676758 = 0.1689278483390808 + 2.0 * 6.050073623657227
Epoch 760, val loss: 0.7775553464889526
Epoch 770, training loss: 12.275259017944336 = 0.16151010990142822 + 2.0 * 6.0568742752075195
Epoch 770, val loss: 0.7812047004699707
Epoch 780, training loss: 12.25875186920166 = 0.1545247733592987 + 2.0 * 6.0521135330200195
Epoch 780, val loss: 0.7850625514984131
Epoch 790, training loss: 12.244982719421387 = 0.14795562624931335 + 2.0 * 6.048513412475586
Epoch 790, val loss: 0.7891908884048462
Epoch 800, training loss: 12.23534870147705 = 0.1417262703180313 + 2.0 * 6.046811103820801
Epoch 800, val loss: 0.7936179637908936
Epoch 810, training loss: 12.238118171691895 = 0.13580361008644104 + 2.0 * 6.051157474517822
Epoch 810, val loss: 0.7982578277587891
Epoch 820, training loss: 12.230125427246094 = 0.13023842871189117 + 2.0 * 6.049943447113037
Epoch 820, val loss: 0.8029645085334778
Epoch 830, training loss: 12.213850975036621 = 0.124965138733387 + 2.0 * 6.044443130493164
Epoch 830, val loss: 0.8079283237457275
Epoch 840, training loss: 12.209090232849121 = 0.1199377030134201 + 2.0 * 6.044576168060303
Epoch 840, val loss: 0.8130964636802673
Epoch 850, training loss: 12.217925071716309 = 0.11517723649740219 + 2.0 * 6.0513739585876465
Epoch 850, val loss: 0.8183308243751526
Epoch 860, training loss: 12.198746681213379 = 0.11067279428243637 + 2.0 * 6.044036865234375
Epoch 860, val loss: 0.8235769867897034
Epoch 870, training loss: 12.190237998962402 = 0.10640665888786316 + 2.0 * 6.0419158935546875
Epoch 870, val loss: 0.8291248679161072
Epoch 880, training loss: 12.184379577636719 = 0.10232505202293396 + 2.0 * 6.041027069091797
Epoch 880, val loss: 0.8347398042678833
Epoch 890, training loss: 12.196111679077148 = 0.09845702350139618 + 2.0 * 6.048827171325684
Epoch 890, val loss: 0.8403911590576172
Epoch 900, training loss: 12.174042701721191 = 0.09478563070297241 + 2.0 * 6.039628505706787
Epoch 900, val loss: 0.8459852337837219
Epoch 910, training loss: 12.16887378692627 = 0.09129991382360458 + 2.0 * 6.038786888122559
Epoch 910, val loss: 0.8517979979515076
Epoch 920, training loss: 12.163667678833008 = 0.08796282112598419 + 2.0 * 6.0378522872924805
Epoch 920, val loss: 0.8576744198799133
Epoch 930, training loss: 12.159445762634277 = 0.084761843085289 + 2.0 * 6.037342071533203
Epoch 930, val loss: 0.8636196851730347
Epoch 940, training loss: 12.169979095458984 = 0.08170349150896072 + 2.0 * 6.044137954711914
Epoch 940, val loss: 0.8695359826087952
Epoch 950, training loss: 12.152252197265625 = 0.07881466299295425 + 2.0 * 6.036718845367432
Epoch 950, val loss: 0.875407338142395
Epoch 960, training loss: 12.149307250976562 = 0.07606814801692963 + 2.0 * 6.036619663238525
Epoch 960, val loss: 0.8814179301261902
Epoch 970, training loss: 12.141883850097656 = 0.07342231273651123 + 2.0 * 6.034230709075928
Epoch 970, val loss: 0.8874370455741882
Epoch 980, training loss: 12.142975807189941 = 0.07088781148195267 + 2.0 * 6.036044120788574
Epoch 980, val loss: 0.8934838771820068
Epoch 990, training loss: 12.141258239746094 = 0.06846727430820465 + 2.0 * 6.03639554977417
Epoch 990, val loss: 0.8994033336639404
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8037
Overall ASR: 0.8192
Flip ASR: 0.7822/225 nodes
The final ASR:0.69865, 0.12645, Accuracy:0.79012, 0.00972
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11582])
remove edge: torch.Size([2, 9478])
updated graph: torch.Size([2, 10504])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9705
Flip ASR: 0.9644/225 nodes
The final ASR:0.97540, 0.00348, Accuracy:0.83210, 0.00698
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.3 der2:0.3 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 18.68075180053711 = 1.9330558776855469 + 2.0 * 8.373847961425781
Epoch 0, val loss: 1.9337085485458374
Epoch 10, training loss: 18.669706344604492 = 1.92313551902771 + 2.0 * 8.373285293579102
Epoch 10, val loss: 1.924407958984375
Epoch 20, training loss: 18.648441314697266 = 1.9108079671859741 + 2.0 * 8.368816375732422
Epoch 20, val loss: 1.9123399257659912
Epoch 30, training loss: 18.56346321105957 = 1.8941015005111694 + 2.0 * 8.334680557250977
Epoch 30, val loss: 1.8956549167633057
Epoch 40, training loss: 18.047086715698242 = 1.875304937362671 + 2.0 * 8.085890769958496
Epoch 40, val loss: 1.8773221969604492
Epoch 50, training loss: 16.767410278320312 = 1.8549410104751587 + 2.0 * 7.456234931945801
Epoch 50, val loss: 1.857283353805542
Epoch 60, training loss: 15.951959609985352 = 1.8401765823364258 + 2.0 * 7.055891513824463
Epoch 60, val loss: 1.8439847230911255
Epoch 70, training loss: 15.444933891296387 = 1.8302524089813232 + 2.0 * 6.807340621948242
Epoch 70, val loss: 1.834646463394165
Epoch 80, training loss: 15.114603042602539 = 1.8193106651306152 + 2.0 * 6.647645950317383
Epoch 80, val loss: 1.8244054317474365
Epoch 90, training loss: 14.898275375366211 = 1.8076368570327759 + 2.0 * 6.545319080352783
Epoch 90, val loss: 1.8137317895889282
Epoch 100, training loss: 14.742223739624023 = 1.7956385612487793 + 2.0 * 6.473292350769043
Epoch 100, val loss: 1.8029894828796387
Epoch 110, training loss: 14.613176345825195 = 1.7837598323822021 + 2.0 * 6.414708137512207
Epoch 110, val loss: 1.7925913333892822
Epoch 120, training loss: 14.513776779174805 = 1.7717311382293701 + 2.0 * 6.371022701263428
Epoch 120, val loss: 1.781803846359253
Epoch 130, training loss: 14.435320854187012 = 1.7583587169647217 + 2.0 * 6.3384809494018555
Epoch 130, val loss: 1.7697640657424927
Epoch 140, training loss: 14.359314918518066 = 1.743348479270935 + 2.0 * 6.3079833984375
Epoch 140, val loss: 1.7566231489181519
Epoch 150, training loss: 14.29022216796875 = 1.7265324592590332 + 2.0 * 6.281844615936279
Epoch 150, val loss: 1.742246150970459
Epoch 160, training loss: 14.227973937988281 = 1.7074556350708008 + 2.0 * 6.26025915145874
Epoch 160, val loss: 1.7261571884155273
Epoch 170, training loss: 14.163962364196777 = 1.6856788396835327 + 2.0 * 6.239141941070557
Epoch 170, val loss: 1.7079638242721558
Epoch 180, training loss: 14.110567092895508 = 1.6605881452560425 + 2.0 * 6.224989414215088
Epoch 180, val loss: 1.6870578527450562
Epoch 190, training loss: 14.051676750183105 = 1.6315416097640991 + 2.0 * 6.2100677490234375
Epoch 190, val loss: 1.6628762483596802
Epoch 200, training loss: 13.9920015335083 = 1.598496675491333 + 2.0 * 6.196752548217773
Epoch 200, val loss: 1.6355007886886597
Epoch 210, training loss: 13.93812084197998 = 1.5614945888519287 + 2.0 * 6.188313007354736
Epoch 210, val loss: 1.6049057245254517
Epoch 220, training loss: 13.875373840332031 = 1.5204449892044067 + 2.0 * 6.177464485168457
Epoch 220, val loss: 1.571291446685791
Epoch 230, training loss: 13.813615798950195 = 1.4753941297531128 + 2.0 * 6.1691107749938965
Epoch 230, val loss: 1.5347025394439697
Epoch 240, training loss: 13.751388549804688 = 1.427161693572998 + 2.0 * 6.162113666534424
Epoch 240, val loss: 1.4961367845535278
Epoch 250, training loss: 13.688191413879395 = 1.3769826889038086 + 2.0 * 6.155604362487793
Epoch 250, val loss: 1.4567543268203735
Epoch 260, training loss: 13.625249862670898 = 1.3252215385437012 + 2.0 * 6.1500139236450195
Epoch 260, val loss: 1.416956901550293
Epoch 270, training loss: 13.56753158569336 = 1.272702693939209 + 2.0 * 6.147414207458496
Epoch 270, val loss: 1.3774694204330444
Epoch 280, training loss: 13.502923011779785 = 1.2205184698104858 + 2.0 * 6.141202449798584
Epoch 280, val loss: 1.3391900062561035
Epoch 290, training loss: 13.441706657409668 = 1.169045329093933 + 2.0 * 6.136330604553223
Epoch 290, val loss: 1.302368402481079
Epoch 300, training loss: 13.382880210876465 = 1.1188417673110962 + 2.0 * 6.13201904296875
Epoch 300, val loss: 1.2672995328903198
Epoch 310, training loss: 13.327722549438477 = 1.0704395771026611 + 2.0 * 6.128641605377197
Epoch 310, val loss: 1.2341660261154175
Epoch 320, training loss: 13.272406578063965 = 1.0237547159194946 + 2.0 * 6.124325752258301
Epoch 320, val loss: 1.2029532194137573
Epoch 330, training loss: 13.235132217407227 = 0.9791882634162903 + 2.0 * 6.12797212600708
Epoch 330, val loss: 1.1737920045852661
Epoch 340, training loss: 13.175265312194824 = 0.9377451539039612 + 2.0 * 6.118760108947754
Epoch 340, val loss: 1.147053599357605
Epoch 350, training loss: 13.128555297851562 = 0.8987221121788025 + 2.0 * 6.114916801452637
Epoch 350, val loss: 1.1225260496139526
Epoch 360, training loss: 13.085631370544434 = 0.8617620468139648 + 2.0 * 6.111934661865234
Epoch 360, val loss: 1.099753499031067
Epoch 370, training loss: 13.044953346252441 = 0.8267232179641724 + 2.0 * 6.109115123748779
Epoch 370, val loss: 1.0785319805145264
Epoch 380, training loss: 13.010581970214844 = 0.79360431432724 + 2.0 * 6.108489036560059
Epoch 380, val loss: 1.0588129758834839
Epoch 390, training loss: 12.974970817565918 = 0.7623214721679688 + 2.0 * 6.106324672698975
Epoch 390, val loss: 1.0405395030975342
Epoch 400, training loss: 12.93578052520752 = 0.7325200438499451 + 2.0 * 6.101630210876465
Epoch 400, val loss: 1.023375391960144
Epoch 410, training loss: 12.901616096496582 = 0.7037392258644104 + 2.0 * 6.098938465118408
Epoch 410, val loss: 1.0071725845336914
Epoch 420, training loss: 12.871411323547363 = 0.6758219003677368 + 2.0 * 6.097794532775879
Epoch 420, val loss: 0.9916478991508484
Epoch 430, training loss: 12.839284896850586 = 0.6486403942108154 + 2.0 * 6.095322132110596
Epoch 430, val loss: 0.9767574667930603
Epoch 440, training loss: 12.805007934570312 = 0.6217944025993347 + 2.0 * 6.091606616973877
Epoch 440, val loss: 0.9623153209686279
Epoch 450, training loss: 12.784640312194824 = 0.5951202511787415 + 2.0 * 6.094759941101074
Epoch 450, val loss: 0.9481605887413025
Epoch 460, training loss: 12.747323036193848 = 0.5688402056694031 + 2.0 * 6.0892415046691895
Epoch 460, val loss: 0.9343374967575073
Epoch 470, training loss: 12.714343070983887 = 0.542643666267395 + 2.0 * 6.085849761962891
Epoch 470, val loss: 0.9208289384841919
Epoch 480, training loss: 12.683477401733398 = 0.5165249705314636 + 2.0 * 6.0834760665893555
Epoch 480, val loss: 0.907661497592926
Epoch 490, training loss: 12.660861015319824 = 0.4905776083469391 + 2.0 * 6.085141658782959
Epoch 490, val loss: 0.8948863744735718
Epoch 500, training loss: 12.628899574279785 = 0.4650026559829712 + 2.0 * 6.081948280334473
Epoch 500, val loss: 0.8827611207962036
Epoch 510, training loss: 12.599549293518066 = 0.43987971544265747 + 2.0 * 6.079834938049316
Epoch 510, val loss: 0.871326208114624
Epoch 520, training loss: 12.56854248046875 = 0.41531577706336975 + 2.0 * 6.076613426208496
Epoch 520, val loss: 0.860636830329895
Epoch 530, training loss: 12.547504425048828 = 0.39135709404945374 + 2.0 * 6.078073501586914
Epoch 530, val loss: 0.8508298993110657
Epoch 540, training loss: 12.526286125183105 = 0.36835503578186035 + 2.0 * 6.078965663909912
Epoch 540, val loss: 0.841938853263855
Epoch 550, training loss: 12.49339771270752 = 0.34637150168418884 + 2.0 * 6.073513031005859
Epoch 550, val loss: 0.834273099899292
Epoch 560, training loss: 12.467367172241211 = 0.32538896799087524 + 2.0 * 6.07098913192749
Epoch 560, val loss: 0.8276740312576294
Epoch 570, training loss: 12.45554256439209 = 0.30544885993003845 + 2.0 * 6.075047016143799
Epoch 570, val loss: 0.8221489191055298
Epoch 580, training loss: 12.423648834228516 = 0.28653621673583984 + 2.0 * 6.068556308746338
Epoch 580, val loss: 0.8176963925361633
Epoch 590, training loss: 12.401659965515137 = 0.26883092522621155 + 2.0 * 6.0664143562316895
Epoch 590, val loss: 0.8143417835235596
Epoch 600, training loss: 12.396562576293945 = 0.2521587610244751 + 2.0 * 6.072201728820801
Epoch 600, val loss: 0.8119684457778931
Epoch 610, training loss: 12.365407943725586 = 0.23662684857845306 + 2.0 * 6.064390659332275
Epoch 610, val loss: 0.8106372952461243
Epoch 620, training loss: 12.34620475769043 = 0.2220689356327057 + 2.0 * 6.062067985534668
Epoch 620, val loss: 0.8102526068687439
Epoch 630, training loss: 12.33320426940918 = 0.2084915190935135 + 2.0 * 6.062356472015381
Epoch 630, val loss: 0.8106999397277832
Epoch 640, training loss: 12.31428050994873 = 0.19589607417583466 + 2.0 * 6.059192180633545
Epoch 640, val loss: 0.8119085431098938
Epoch 650, training loss: 12.303138732910156 = 0.18421275913715363 + 2.0 * 6.059463024139404
Epoch 650, val loss: 0.8138883113861084
Epoch 660, training loss: 12.294418334960938 = 0.173387810587883 + 2.0 * 6.060515403747559
Epoch 660, val loss: 0.8166054487228394
Epoch 670, training loss: 12.276172637939453 = 0.16328929364681244 + 2.0 * 6.056441783905029
Epoch 670, val loss: 0.8199150562286377
Epoch 680, training loss: 12.26547622680664 = 0.15396712720394135 + 2.0 * 6.055754661560059
Epoch 680, val loss: 0.8239009976387024
Epoch 690, training loss: 12.253851890563965 = 0.14532433450222015 + 2.0 * 6.054263591766357
Epoch 690, val loss: 0.8282696604728699
Epoch 700, training loss: 12.247274398803711 = 0.1373133808374405 + 2.0 * 6.054980278015137
Epoch 700, val loss: 0.8331395983695984
Epoch 710, training loss: 12.231411933898926 = 0.12989653646945953 + 2.0 * 6.050757884979248
Epoch 710, val loss: 0.838374137878418
Epoch 720, training loss: 12.228747367858887 = 0.1229865550994873 + 2.0 * 6.05288028717041
Epoch 720, val loss: 0.8439839482307434
Epoch 730, training loss: 12.217546463012695 = 0.11657004803419113 + 2.0 * 6.050487995147705
Epoch 730, val loss: 0.8499072790145874
Epoch 740, training loss: 12.211464881896973 = 0.11060257256031036 + 2.0 * 6.050431251525879
Epoch 740, val loss: 0.8559830188751221
Epoch 750, training loss: 12.199871063232422 = 0.1050640419125557 + 2.0 * 6.047403335571289
Epoch 750, val loss: 0.8624383211135864
Epoch 760, training loss: 12.195981979370117 = 0.09988278895616531 + 2.0 * 6.048049449920654
Epoch 760, val loss: 0.8690317869186401
Epoch 770, training loss: 12.190376281738281 = 0.09503646939992905 + 2.0 * 6.047669887542725
Epoch 770, val loss: 0.8758297562599182
Epoch 780, training loss: 12.177722930908203 = 0.09054345637559891 + 2.0 * 6.0435895919799805
Epoch 780, val loss: 0.8827986717224121
Epoch 790, training loss: 12.171862602233887 = 0.08632618933916092 + 2.0 * 6.0427680015563965
Epoch 790, val loss: 0.8900173902511597
Epoch 800, training loss: 12.17218017578125 = 0.08237314224243164 + 2.0 * 6.044903755187988
Epoch 800, val loss: 0.8973118662834167
Epoch 810, training loss: 12.16482162475586 = 0.0786607638001442 + 2.0 * 6.0430803298950195
Epoch 810, val loss: 0.9046820402145386
Epoch 820, training loss: 12.158029556274414 = 0.07518931478261948 + 2.0 * 6.041419982910156
Epoch 820, val loss: 0.9121813774108887
Epoch 830, training loss: 12.150731086730957 = 0.07195081561803818 + 2.0 * 6.0393900871276855
Epoch 830, val loss: 0.9198145270347595
Epoch 840, training loss: 12.151222229003906 = 0.06890524923801422 + 2.0 * 6.041158676147461
Epoch 840, val loss: 0.9275180101394653
Epoch 850, training loss: 12.140091896057129 = 0.06601788103580475 + 2.0 * 6.037036895751953
Epoch 850, val loss: 0.9352041482925415
Epoch 860, training loss: 12.13670825958252 = 0.06331013888120651 + 2.0 * 6.036699295043945
Epoch 860, val loss: 0.9430541396141052
Epoch 870, training loss: 12.138089179992676 = 0.06076187267899513 + 2.0 * 6.038663864135742
Epoch 870, val loss: 0.9509024024009705
Epoch 880, training loss: 12.127922058105469 = 0.058368831872940063 + 2.0 * 6.03477668762207
Epoch 880, val loss: 0.9587807059288025
Epoch 890, training loss: 12.125916481018066 = 0.056103289127349854 + 2.0 * 6.034906387329102
Epoch 890, val loss: 0.966765820980072
Epoch 900, training loss: 12.122329711914062 = 0.05396721139550209 + 2.0 * 6.034181118011475
Epoch 900, val loss: 0.9746073484420776
Epoch 910, training loss: 12.115771293640137 = 0.05194440484046936 + 2.0 * 6.0319132804870605
Epoch 910, val loss: 0.9825378656387329
Epoch 920, training loss: 12.111701011657715 = 0.05003483220934868 + 2.0 * 6.0308332443237305
Epoch 920, val loss: 0.9905056357383728
Epoch 930, training loss: 12.109301567077637 = 0.04822015389800072 + 2.0 * 6.030540943145752
Epoch 930, val loss: 0.9985090494155884
Epoch 940, training loss: 12.115327835083008 = 0.04650269076228142 + 2.0 * 6.034412384033203
Epoch 940, val loss: 1.0062971115112305
Epoch 950, training loss: 12.109639167785645 = 0.04488055408000946 + 2.0 * 6.032379150390625
Epoch 950, val loss: 1.0140312910079956
Epoch 960, training loss: 12.101861000061035 = 0.0433415062725544 + 2.0 * 6.02925968170166
Epoch 960, val loss: 1.0217602252960205
Epoch 970, training loss: 12.097007751464844 = 0.04188373312354088 + 2.0 * 6.027562141418457
Epoch 970, val loss: 1.0295616388320923
Epoch 980, training loss: 12.1016263961792 = 0.04048779606819153 + 2.0 * 6.030569076538086
Epoch 980, val loss: 1.0371878147125244
Epoch 990, training loss: 12.094451904296875 = 0.03916613757610321 + 2.0 * 6.027642726898193
Epoch 990, val loss: 1.0446895360946655
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8111
Overall ASR: 0.7085
Flip ASR: 0.6489/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.702999114990234 = 1.9553186893463135 + 2.0 * 8.37384033203125
Epoch 0, val loss: 1.955534815788269
Epoch 10, training loss: 18.69109344482422 = 1.9450068473815918 + 2.0 * 8.373043060302734
Epoch 10, val loss: 1.9445197582244873
Epoch 20, training loss: 18.66727066040039 = 1.9321541786193848 + 2.0 * 8.367558479309082
Epoch 20, val loss: 1.9301520586013794
Epoch 30, training loss: 18.569997787475586 = 1.9150837659835815 + 2.0 * 8.327457427978516
Epoch 30, val loss: 1.9106402397155762
Epoch 40, training loss: 17.86492156982422 = 1.8950815200805664 + 2.0 * 7.984920024871826
Epoch 40, val loss: 1.8883517980575562
Epoch 50, training loss: 16.426891326904297 = 1.8747378587722778 + 2.0 * 7.276076316833496
Epoch 50, val loss: 1.8670223951339722
Epoch 60, training loss: 15.875346183776855 = 1.8605200052261353 + 2.0 * 7.007412910461426
Epoch 60, val loss: 1.8521510362625122
Epoch 70, training loss: 15.337182998657227 = 1.8476461172103882 + 2.0 * 6.7447686195373535
Epoch 70, val loss: 1.8387731313705444
Epoch 80, training loss: 14.986154556274414 = 1.8359452486038208 + 2.0 * 6.575104713439941
Epoch 80, val loss: 1.826735258102417
Epoch 90, training loss: 14.77060317993164 = 1.8235912322998047 + 2.0 * 6.473505973815918
Epoch 90, val loss: 1.8137866258621216
Epoch 100, training loss: 14.62794303894043 = 1.8101848363876343 + 2.0 * 6.408879280090332
Epoch 100, val loss: 1.7998321056365967
Epoch 110, training loss: 14.515467643737793 = 1.7967020273208618 + 2.0 * 6.359382629394531
Epoch 110, val loss: 1.7860456705093384
Epoch 120, training loss: 14.429625511169434 = 1.7836425304412842 + 2.0 * 6.322991371154785
Epoch 120, val loss: 1.772943377494812
Epoch 130, training loss: 14.360861778259277 = 1.7706897258758545 + 2.0 * 6.295085906982422
Epoch 130, val loss: 1.760127305984497
Epoch 140, training loss: 14.29692268371582 = 1.75746488571167 + 2.0 * 6.269729137420654
Epoch 140, val loss: 1.747344732284546
Epoch 150, training loss: 14.24124526977539 = 1.7435134649276733 + 2.0 * 6.248866081237793
Epoch 150, val loss: 1.7343590259552002
Epoch 160, training loss: 14.1949462890625 = 1.7284789085388184 + 2.0 * 6.23323392868042
Epoch 160, val loss: 1.7208490371704102
Epoch 170, training loss: 14.147638320922852 = 1.7120237350463867 + 2.0 * 6.217807292938232
Epoch 170, val loss: 1.7066060304641724
Epoch 180, training loss: 14.104452133178711 = 1.6935826539993286 + 2.0 * 6.205434799194336
Epoch 180, val loss: 1.691046953201294
Epoch 190, training loss: 14.06984806060791 = 1.6728479862213135 + 2.0 * 6.198500156402588
Epoch 190, val loss: 1.6739016771316528
Epoch 200, training loss: 14.019867897033691 = 1.649450421333313 + 2.0 * 6.185208797454834
Epoch 200, val loss: 1.6546690464019775
Epoch 210, training loss: 13.97325325012207 = 1.6228535175323486 + 2.0 * 6.17519998550415
Epoch 210, val loss: 1.6330734491348267
Epoch 220, training loss: 13.925122261047363 = 1.5927095413208008 + 2.0 * 6.166206359863281
Epoch 220, val loss: 1.6086827516555786
Epoch 230, training loss: 13.884861946105957 = 1.5583819150924683 + 2.0 * 6.1632399559021
Epoch 230, val loss: 1.5807989835739136
Epoch 240, training loss: 13.828217506408691 = 1.520341157913208 + 2.0 * 6.153938293457031
Epoch 240, val loss: 1.5502502918243408
Epoch 250, training loss: 13.77180004119873 = 1.47925865650177 + 2.0 * 6.146270751953125
Epoch 250, val loss: 1.5170754194259644
Epoch 260, training loss: 13.715225219726562 = 1.4348948001861572 + 2.0 * 6.140165328979492
Epoch 260, val loss: 1.481221079826355
Epoch 270, training loss: 13.659036636352539 = 1.3878235816955566 + 2.0 * 6.135606288909912
Epoch 270, val loss: 1.4429905414581299
Epoch 280, training loss: 13.600937843322754 = 1.3390992879867554 + 2.0 * 6.130919456481934
Epoch 280, val loss: 1.4034781455993652
Epoch 290, training loss: 13.543298721313477 = 1.2899645566940308 + 2.0 * 6.126667022705078
Epoch 290, val loss: 1.3632140159606934
Epoch 300, training loss: 13.483918190002441 = 1.2405186891555786 + 2.0 * 6.121699810028076
Epoch 300, val loss: 1.3227659463882446
Epoch 310, training loss: 13.430344581604004 = 1.1915644407272339 + 2.0 * 6.11939001083374
Epoch 310, val loss: 1.2827779054641724
Epoch 320, training loss: 13.374398231506348 = 1.143900990486145 + 2.0 * 6.115248680114746
Epoch 320, val loss: 1.2439336776733398
Epoch 330, training loss: 13.321374893188477 = 1.0971941947937012 + 2.0 * 6.112090110778809
Epoch 330, val loss: 1.2059431076049805
Epoch 340, training loss: 13.26877498626709 = 1.0512943267822266 + 2.0 * 6.108740329742432
Epoch 340, val loss: 1.168790340423584
Epoch 350, training loss: 13.218704223632812 = 1.006203532218933 + 2.0 * 6.106250286102295
Epoch 350, val loss: 1.132712721824646
Epoch 360, training loss: 13.17345905303955 = 0.9623147249221802 + 2.0 * 6.10557222366333
Epoch 360, val loss: 1.0978758335113525
Epoch 370, training loss: 13.119392395019531 = 0.9200165867805481 + 2.0 * 6.0996880531311035
Epoch 370, val loss: 1.064561367034912
Epoch 380, training loss: 13.072510719299316 = 0.8790830969810486 + 2.0 * 6.096714019775391
Epoch 380, val loss: 1.0326505899429321
Epoch 390, training loss: 13.037025451660156 = 0.83962082862854 + 2.0 * 6.098702430725098
Epoch 390, val loss: 1.002309799194336
Epoch 400, training loss: 12.987383842468262 = 0.8018994927406311 + 2.0 * 6.092741966247559
Epoch 400, val loss: 0.9740180373191833
Epoch 410, training loss: 12.944549560546875 = 0.7660413980484009 + 2.0 * 6.089253902435303
Epoch 410, val loss: 0.9473327398300171
Epoch 420, training loss: 12.91380500793457 = 0.7318650484085083 + 2.0 * 6.090970039367676
Epoch 420, val loss: 0.9223896265029907
Epoch 430, training loss: 12.871896743774414 = 0.6996109485626221 + 2.0 * 6.0861430168151855
Epoch 430, val loss: 0.8993744850158691
Epoch 440, training loss: 12.835675239562988 = 0.6691808700561523 + 2.0 * 6.083247184753418
Epoch 440, val loss: 0.8779979348182678
Epoch 450, training loss: 12.803993225097656 = 0.6402532458305359 + 2.0 * 6.081870079040527
Epoch 450, val loss: 0.8581962585449219
Epoch 460, training loss: 12.771790504455566 = 0.6128122806549072 + 2.0 * 6.079489231109619
Epoch 460, val loss: 0.8399405479431152
Epoch 470, training loss: 12.742239952087402 = 0.586837112903595 + 2.0 * 6.077701568603516
Epoch 470, val loss: 0.8230562806129456
Epoch 480, training loss: 12.712438583374023 = 0.562066912651062 + 2.0 * 6.075185775756836
Epoch 480, val loss: 0.8075864315032959
Epoch 490, training loss: 12.694876670837402 = 0.5384649634361267 + 2.0 * 6.0782060623168945
Epoch 490, val loss: 0.793325662612915
Epoch 500, training loss: 12.66209602355957 = 0.5160852670669556 + 2.0 * 6.073005199432373
Epoch 500, val loss: 0.7803432941436768
Epoch 510, training loss: 12.634517669677734 = 0.494727224111557 + 2.0 * 6.069895267486572
Epoch 510, val loss: 0.7685580849647522
Epoch 520, training loss: 12.619174003601074 = 0.47424960136413574 + 2.0 * 6.07246208190918
Epoch 520, val loss: 0.7579668760299683
Epoch 530, training loss: 12.59373664855957 = 0.45482972264289856 + 2.0 * 6.069453239440918
Epoch 530, val loss: 0.7483512759208679
Epoch 540, training loss: 12.568045616149902 = 0.436210960149765 + 2.0 * 6.065917491912842
Epoch 540, val loss: 0.7398995757102966
Epoch 550, training loss: 12.545761108398438 = 0.4183282256126404 + 2.0 * 6.063716411590576
Epoch 550, val loss: 0.7324581742286682
Epoch 560, training loss: 12.53618335723877 = 0.40111324191093445 + 2.0 * 6.067534923553467
Epoch 560, val loss: 0.7259088754653931
Epoch 570, training loss: 12.508753776550293 = 0.3845771849155426 + 2.0 * 6.062088489532471
Epoch 570, val loss: 0.7203046679496765
Epoch 580, training loss: 12.487457275390625 = 0.3686692416667938 + 2.0 * 6.059393882751465
Epoch 580, val loss: 0.7155723571777344
Epoch 590, training loss: 12.472025871276855 = 0.35328084230422974 + 2.0 * 6.059372425079346
Epoch 590, val loss: 0.7116537094116211
Epoch 600, training loss: 12.456993103027344 = 0.33840134739875793 + 2.0 * 6.059295654296875
Epoch 600, val loss: 0.7083876729011536
Epoch 610, training loss: 12.4368896484375 = 0.3241465091705322 + 2.0 * 6.056371688842773
Epoch 610, val loss: 0.7058328986167908
Epoch 620, training loss: 12.421290397644043 = 0.31036779284477234 + 2.0 * 6.055461406707764
Epoch 620, val loss: 0.7040429711341858
Epoch 630, training loss: 12.407649040222168 = 0.2970731854438782 + 2.0 * 6.055287837982178
Epoch 630, val loss: 0.7029003500938416
Epoch 640, training loss: 12.39057445526123 = 0.2842671871185303 + 2.0 * 6.0531535148620605
Epoch 640, val loss: 0.7022935748100281
Epoch 650, training loss: 12.380995750427246 = 0.27200081944465637 + 2.0 * 6.054497241973877
Epoch 650, val loss: 0.7021586298942566
Epoch 660, training loss: 12.36419677734375 = 0.2602309584617615 + 2.0 * 6.051982879638672
Epoch 660, val loss: 0.7027022242546082
Epoch 670, training loss: 12.348097801208496 = 0.24899046123027802 + 2.0 * 6.049553871154785
Epoch 670, val loss: 0.7037500739097595
Epoch 680, training loss: 12.340609550476074 = 0.2382086217403412 + 2.0 * 6.0512003898620605
Epoch 680, val loss: 0.7053321003913879
Epoch 690, training loss: 12.32510757446289 = 0.22792811691761017 + 2.0 * 6.048589706420898
Epoch 690, val loss: 0.7072928547859192
Epoch 700, training loss: 12.313542366027832 = 0.21815484762191772 + 2.0 * 6.047693729400635
Epoch 700, val loss: 0.7097211480140686
Epoch 710, training loss: 12.30968189239502 = 0.208871990442276 + 2.0 * 6.050405025482178
Epoch 710, val loss: 0.7125460505485535
Epoch 720, training loss: 12.292261123657227 = 0.20003391802310944 + 2.0 * 6.04611349105835
Epoch 720, val loss: 0.7156409025192261
Epoch 730, training loss: 12.280536651611328 = 0.1916704773902893 + 2.0 * 6.044433116912842
Epoch 730, val loss: 0.7191734910011292
Epoch 740, training loss: 12.27072525024414 = 0.18368352949619293 + 2.0 * 6.043520927429199
Epoch 740, val loss: 0.7230457663536072
Epoch 750, training loss: 12.278682708740234 = 0.17608562111854553 + 2.0 * 6.05129861831665
Epoch 750, val loss: 0.7271468043327332
Epoch 760, training loss: 12.251049041748047 = 0.16881266236305237 + 2.0 * 6.041118144989014
Epoch 760, val loss: 0.7315199375152588
Epoch 770, training loss: 12.244988441467285 = 0.16187413036823273 + 2.0 * 6.041557312011719
Epoch 770, val loss: 0.7362005710601807
Epoch 780, training loss: 12.236458778381348 = 0.15526120364665985 + 2.0 * 6.0405988693237305
Epoch 780, val loss: 0.7411300539970398
Epoch 790, training loss: 12.235701560974121 = 0.14893393218517303 + 2.0 * 6.043383598327637
Epoch 790, val loss: 0.7460782527923584
Epoch 800, training loss: 12.22250747680664 = 0.14289060235023499 + 2.0 * 6.03980827331543
Epoch 800, val loss: 0.7512000203132629
Epoch 810, training loss: 12.21478271484375 = 0.13715560734272003 + 2.0 * 6.038813591003418
Epoch 810, val loss: 0.7565150260925293
Epoch 820, training loss: 12.204850196838379 = 0.13167032599449158 + 2.0 * 6.036590099334717
Epoch 820, val loss: 0.7620484232902527
Epoch 830, training loss: 12.203495025634766 = 0.1263720840215683 + 2.0 * 6.0385613441467285
Epoch 830, val loss: 0.7676784992218018
Epoch 840, training loss: 12.198060989379883 = 0.12132689356803894 + 2.0 * 6.03836727142334
Epoch 840, val loss: 0.7734423875808716
Epoch 850, training loss: 12.186159133911133 = 0.11645752936601639 + 2.0 * 6.034850597381592
Epoch 850, val loss: 0.7792447805404663
Epoch 860, training loss: 12.180363655090332 = 0.11177629977464676 + 2.0 * 6.0342936515808105
Epoch 860, val loss: 0.7852635383605957
Epoch 870, training loss: 12.175171852111816 = 0.10729505866765976 + 2.0 * 6.033938407897949
Epoch 870, val loss: 0.7914347648620605
Epoch 880, training loss: 12.173036575317383 = 0.10300954431295395 + 2.0 * 6.035013675689697
Epoch 880, val loss: 0.7974694967269897
Epoch 890, training loss: 12.164135932922363 = 0.09891559183597565 + 2.0 * 6.032609939575195
Epoch 890, val loss: 0.803551971912384
Epoch 900, training loss: 12.156830787658691 = 0.09498386085033417 + 2.0 * 6.030923366546631
Epoch 900, val loss: 0.8099106550216675
Epoch 910, training loss: 12.151581764221191 = 0.09119316935539246 + 2.0 * 6.030194282531738
Epoch 910, val loss: 0.8163638710975647
Epoch 920, training loss: 12.148937225341797 = 0.08755151927471161 + 2.0 * 6.030693054199219
Epoch 920, val loss: 0.8229151964187622
Epoch 930, training loss: 12.142010688781738 = 0.08407393842935562 + 2.0 * 6.028968334197998
Epoch 930, val loss: 0.8293495178222656
Epoch 940, training loss: 12.143513679504395 = 0.08075255155563354 + 2.0 * 6.031380653381348
Epoch 940, val loss: 0.8359494805335999
Epoch 950, training loss: 12.13829517364502 = 0.07759471237659454 + 2.0 * 6.030350208282471
Epoch 950, val loss: 0.8426828980445862
Epoch 960, training loss: 12.130459785461426 = 0.0745978057384491 + 2.0 * 6.027931213378906
Epoch 960, val loss: 0.8492676019668579
Epoch 970, training loss: 12.127192497253418 = 0.07174559682607651 + 2.0 * 6.02772331237793
Epoch 970, val loss: 0.856001079082489
Epoch 980, training loss: 12.124627113342285 = 0.06903254985809326 + 2.0 * 6.027797222137451
Epoch 980, val loss: 0.8628157377243042
Epoch 990, training loss: 12.119632720947266 = 0.06644875556230545 + 2.0 * 6.026591777801514
Epoch 990, val loss: 0.8695304989814758
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8000
Overall ASR: 0.7380
Flip ASR: 0.6889/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.726659774780273 = 1.9789111614227295 + 2.0 * 8.37387466430664
Epoch 0, val loss: 1.979807734489441
Epoch 10, training loss: 18.71439552307129 = 1.9675610065460205 + 2.0 * 8.373416900634766
Epoch 10, val loss: 1.9690219163894653
Epoch 20, training loss: 18.693706512451172 = 1.9536763429641724 + 2.0 * 8.370015144348145
Epoch 20, val loss: 1.9553608894348145
Epoch 30, training loss: 18.627592086791992 = 1.9346556663513184 + 2.0 * 8.346467971801758
Epoch 30, val loss: 1.9363561868667603
Epoch 40, training loss: 18.288928985595703 = 1.9115781784057617 + 2.0 * 8.188674926757812
Epoch 40, val loss: 1.914129614830017
Epoch 50, training loss: 17.303668975830078 = 1.8877612352371216 + 2.0 * 7.707953929901123
Epoch 50, val loss: 1.89179265499115
Epoch 60, training loss: 16.384965896606445 = 1.8695858716964722 + 2.0 * 7.2576904296875
Epoch 60, val loss: 1.8756537437438965
Epoch 70, training loss: 15.55931568145752 = 1.8552398681640625 + 2.0 * 6.8520379066467285
Epoch 70, val loss: 1.862372636795044
Epoch 80, training loss: 15.112961769104004 = 1.8414347171783447 + 2.0 * 6.635763645172119
Epoch 80, val loss: 1.8492990732192993
Epoch 90, training loss: 14.849898338317871 = 1.8257695436477661 + 2.0 * 6.512064456939697
Epoch 90, val loss: 1.8345868587493896
Epoch 100, training loss: 14.689908981323242 = 1.8089261054992676 + 2.0 * 6.440491199493408
Epoch 100, val loss: 1.8192259073257446
Epoch 110, training loss: 14.577317237854004 = 1.7926310300827026 + 2.0 * 6.392343044281006
Epoch 110, val loss: 1.8046672344207764
Epoch 120, training loss: 14.483113288879395 = 1.776828646659851 + 2.0 * 6.353142261505127
Epoch 120, val loss: 1.7904292345046997
Epoch 130, training loss: 14.404644012451172 = 1.7609646320343018 + 2.0 * 6.321839809417725
Epoch 130, val loss: 1.7759954929351807
Epoch 140, training loss: 14.33251953125 = 1.744472622871399 + 2.0 * 6.294023513793945
Epoch 140, val loss: 1.7612524032592773
Epoch 150, training loss: 14.27193832397461 = 1.726826548576355 + 2.0 * 6.272555828094482
Epoch 150, val loss: 1.745866298675537
Epoch 160, training loss: 14.218746185302734 = 1.7073367834091187 + 2.0 * 6.255704879760742
Epoch 160, val loss: 1.7293862104415894
Epoch 170, training loss: 14.164200782775879 = 1.6856849193572998 + 2.0 * 6.2392578125
Epoch 170, val loss: 1.711205005645752
Epoch 180, training loss: 14.116497993469238 = 1.661400556564331 + 2.0 * 6.227548599243164
Epoch 180, val loss: 1.6912639141082764
Epoch 190, training loss: 14.063260078430176 = 1.634448766708374 + 2.0 * 6.214405536651611
Epoch 190, val loss: 1.6691207885742188
Epoch 200, training loss: 14.010629653930664 = 1.6043108701705933 + 2.0 * 6.203159332275391
Epoch 200, val loss: 1.6445462703704834
Epoch 210, training loss: 13.9646635055542 = 1.5706063508987427 + 2.0 * 6.197028636932373
Epoch 210, val loss: 1.617283582687378
Epoch 220, training loss: 13.903145790100098 = 1.5337040424346924 + 2.0 * 6.184720993041992
Epoch 220, val loss: 1.5873849391937256
Epoch 230, training loss: 13.846890449523926 = 1.4933346509933472 + 2.0 * 6.1767778396606445
Epoch 230, val loss: 1.5547449588775635
Epoch 240, training loss: 13.790756225585938 = 1.4493794441223145 + 2.0 * 6.170688629150391
Epoch 240, val loss: 1.519366979598999
Epoch 250, training loss: 13.728965759277344 = 1.4025349617004395 + 2.0 * 6.163215637207031
Epoch 250, val loss: 1.4817978143692017
Epoch 260, training loss: 13.667190551757812 = 1.353686809539795 + 2.0 * 6.15675163269043
Epoch 260, val loss: 1.442355990409851
Epoch 270, training loss: 13.605493545532227 = 1.3032196760177612 + 2.0 * 6.151136875152588
Epoch 270, val loss: 1.4018152952194214
Epoch 280, training loss: 13.543898582458496 = 1.2519845962524414 + 2.0 * 6.145956993103027
Epoch 280, val loss: 1.3608415126800537
Epoch 290, training loss: 13.486804962158203 = 1.2009081840515137 + 2.0 * 6.142948627471924
Epoch 290, val loss: 1.3201329708099365
Epoch 300, training loss: 13.42188549041748 = 1.1508140563964844 + 2.0 * 6.135535717010498
Epoch 300, val loss: 1.280073642730713
Epoch 310, training loss: 13.363885879516602 = 1.101623296737671 + 2.0 * 6.131131172180176
Epoch 310, val loss: 1.2409647703170776
Epoch 320, training loss: 13.315618515014648 = 1.0536506175994873 + 2.0 * 6.130983829498291
Epoch 320, val loss: 1.202889323234558
Epoch 330, training loss: 13.257977485656738 = 1.0072523355484009 + 2.0 * 6.125362396240234
Epoch 330, val loss: 1.1660921573638916
Epoch 340, training loss: 13.20244312286377 = 0.9622762203216553 + 2.0 * 6.120083332061768
Epoch 340, val loss: 1.1306207180023193
Epoch 350, training loss: 13.162367820739746 = 0.9188356995582581 + 2.0 * 6.121766090393066
Epoch 350, val loss: 1.0962834358215332
Epoch 360, training loss: 13.104254722595215 = 0.8769156336784363 + 2.0 * 6.113669395446777
Epoch 360, val loss: 1.0635710954666138
Epoch 370, training loss: 13.057273864746094 = 0.8366267085075378 + 2.0 * 6.110323429107666
Epoch 370, val loss: 1.0321506261825562
Epoch 380, training loss: 13.026010513305664 = 0.797699511051178 + 2.0 * 6.114155292510986
Epoch 380, val loss: 1.0020486116409302
Epoch 390, training loss: 12.969042778015137 = 0.7604367136955261 + 2.0 * 6.104302883148193
Epoch 390, val loss: 0.9734611511230469
Epoch 400, training loss: 12.928303718566895 = 0.7246564030647278 + 2.0 * 6.101823806762695
Epoch 400, val loss: 0.946344256401062
Epoch 410, training loss: 12.890856742858887 = 0.6902865171432495 + 2.0 * 6.100285053253174
Epoch 410, val loss: 0.920721173286438
Epoch 420, training loss: 12.853606224060059 = 0.6573840975761414 + 2.0 * 6.098111152648926
Epoch 420, val loss: 0.8966190218925476
Epoch 430, training loss: 12.819747924804688 = 0.6261032223701477 + 2.0 * 6.096822261810303
Epoch 430, val loss: 0.8742470145225525
Epoch 440, training loss: 12.782526016235352 = 0.5964146852493286 + 2.0 * 6.093055725097656
Epoch 440, val loss: 0.8535603880882263
Epoch 450, training loss: 12.749265670776367 = 0.5680489540100098 + 2.0 * 6.090608596801758
Epoch 450, val loss: 0.8344694972038269
Epoch 460, training loss: 12.727180480957031 = 0.5410288572311401 + 2.0 * 6.093075752258301
Epoch 460, val loss: 0.8168612718582153
Epoch 470, training loss: 12.6910400390625 = 0.5152621865272522 + 2.0 * 6.087888717651367
Epoch 470, val loss: 0.8008363842964172
Epoch 480, training loss: 12.661473274230957 = 0.4907287061214447 + 2.0 * 6.085372447967529
Epoch 480, val loss: 0.7863050699234009
Epoch 490, training loss: 12.636049270629883 = 0.46727418899536133 + 2.0 * 6.084387302398682
Epoch 490, val loss: 0.7730329632759094
Epoch 500, training loss: 12.615068435668945 = 0.4449180066585541 + 2.0 * 6.085075378417969
Epoch 500, val loss: 0.760983943939209
Epoch 510, training loss: 12.583918571472168 = 0.4235875904560089 + 2.0 * 6.080165386199951
Epoch 510, val loss: 0.7500730156898499
Epoch 520, training loss: 12.565326690673828 = 0.4031568467617035 + 2.0 * 6.081084728240967
Epoch 520, val loss: 0.7401387691497803
Epoch 530, training loss: 12.541106224060059 = 0.3835979402065277 + 2.0 * 6.07875394821167
Epoch 530, val loss: 0.7311387658119202
Epoch 540, training loss: 12.514223098754883 = 0.3648698925971985 + 2.0 * 6.074676513671875
Epoch 540, val loss: 0.7230060696601868
Epoch 550, training loss: 12.498383522033691 = 0.3469197750091553 + 2.0 * 6.0757317543029785
Epoch 550, val loss: 0.7157121300697327
Epoch 560, training loss: 12.480518341064453 = 0.32965025305747986 + 2.0 * 6.07543420791626
Epoch 560, val loss: 0.7091606855392456
Epoch 570, training loss: 12.454404830932617 = 0.31322479248046875 + 2.0 * 6.070590019226074
Epoch 570, val loss: 0.7033488750457764
Epoch 580, training loss: 12.436568260192871 = 0.2974289655685425 + 2.0 * 6.0695695877075195
Epoch 580, val loss: 0.6981887221336365
Epoch 590, training loss: 12.430745124816895 = 0.28224244713783264 + 2.0 * 6.074251174926758
Epoch 590, val loss: 0.6936687231063843
Epoch 600, training loss: 12.404251098632812 = 0.2677317261695862 + 2.0 * 6.0682597160339355
Epoch 600, val loss: 0.6897185444831848
Epoch 610, training loss: 12.384672164916992 = 0.25383368134498596 + 2.0 * 6.0654191970825195
Epoch 610, val loss: 0.6863800287246704
Epoch 620, training loss: 12.375170707702637 = 0.24050897359848022 + 2.0 * 6.067330837249756
Epoch 620, val loss: 0.6836398243904114
Epoch 630, training loss: 12.361247062683105 = 0.22780409455299377 + 2.0 * 6.066721439361572
Epoch 630, val loss: 0.681397557258606
Epoch 640, training loss: 12.339225769042969 = 0.21572047472000122 + 2.0 * 6.061752796173096
Epoch 640, val loss: 0.6797497868537903
Epoch 650, training loss: 12.329073905944824 = 0.2042069137096405 + 2.0 * 6.06243371963501
Epoch 650, val loss: 0.678618848323822
Epoch 660, training loss: 12.312440872192383 = 0.19329707324504852 + 2.0 * 6.059571743011475
Epoch 660, val loss: 0.6780757904052734
Epoch 670, training loss: 12.301451683044434 = 0.18293164670467377 + 2.0 * 6.05925989151001
Epoch 670, val loss: 0.6779242753982544
Epoch 680, training loss: 12.301878929138184 = 0.1731334924697876 + 2.0 * 6.064372539520264
Epoch 680, val loss: 0.6783409118652344
Epoch 690, training loss: 12.277510643005371 = 0.1638426035642624 + 2.0 * 6.0568342208862305
Epoch 690, val loss: 0.6792203783988953
Epoch 700, training loss: 12.265589714050293 = 0.15507137775421143 + 2.0 * 6.0552592277526855
Epoch 700, val loss: 0.6804725527763367
Epoch 710, training loss: 12.26057243347168 = 0.14678432047367096 + 2.0 * 6.056893825531006
Epoch 710, val loss: 0.6821911931037903
Epoch 720, training loss: 12.250985145568848 = 0.1390247493982315 + 2.0 * 6.055980205535889
Epoch 720, val loss: 0.6843238472938538
Epoch 730, training loss: 12.23762035369873 = 0.13170185685157776 + 2.0 * 6.052959442138672
Epoch 730, val loss: 0.6866869330406189
Epoch 740, training loss: 12.230192184448242 = 0.12484140694141388 + 2.0 * 6.052675247192383
Epoch 740, val loss: 0.689391553401947
Epoch 750, training loss: 12.218568801879883 = 0.11842010915279388 + 2.0 * 6.050074577331543
Epoch 750, val loss: 0.6924402117729187
Epoch 760, training loss: 12.21176528930664 = 0.11238710582256317 + 2.0 * 6.049689292907715
Epoch 760, val loss: 0.6957359910011292
Epoch 770, training loss: 12.207625389099121 = 0.10672964155673981 + 2.0 * 6.050447940826416
Epoch 770, val loss: 0.6992218494415283
Epoch 780, training loss: 12.198801040649414 = 0.10145024955272675 + 2.0 * 6.048675537109375
Epoch 780, val loss: 0.7029987573623657
Epoch 790, training loss: 12.191680908203125 = 0.09650084376335144 + 2.0 * 6.047590255737305
Epoch 790, val loss: 0.7068087458610535
Epoch 800, training loss: 12.182647705078125 = 0.09187775105237961 + 2.0 * 6.045384883880615
Epoch 800, val loss: 0.710886538028717
Epoch 810, training loss: 12.183238983154297 = 0.08753566443920135 + 2.0 * 6.0478515625
Epoch 810, val loss: 0.715162456035614
Epoch 820, training loss: 12.174861907958984 = 0.08344995230436325 + 2.0 * 6.045705795288086
Epoch 820, val loss: 0.7194048762321472
Epoch 830, training loss: 12.169594764709473 = 0.07964245975017548 + 2.0 * 6.044976234436035
Epoch 830, val loss: 0.7237644791603088
Epoch 840, training loss: 12.16512393951416 = 0.07606790959835052 + 2.0 * 6.044528007507324
Epoch 840, val loss: 0.7282984256744385
Epoch 850, training loss: 12.156744956970215 = 0.07270240038633347 + 2.0 * 6.04202127456665
Epoch 850, val loss: 0.7328677177429199
Epoch 860, training loss: 12.150619506835938 = 0.06954371184110641 + 2.0 * 6.0405378341674805
Epoch 860, val loss: 0.7374989986419678
Epoch 870, training loss: 12.151750564575195 = 0.06657332926988602 + 2.0 * 6.042588710784912
Epoch 870, val loss: 0.7421929836273193
Epoch 880, training loss: 12.14424991607666 = 0.06376377493143082 + 2.0 * 6.040243148803711
Epoch 880, val loss: 0.7468637228012085
Epoch 890, training loss: 12.144109725952148 = 0.061142079532146454 + 2.0 * 6.0414838790893555
Epoch 890, val loss: 0.7515794038772583
Epoch 900, training loss: 12.13753890991211 = 0.058668456971645355 + 2.0 * 6.039435386657715
Epoch 900, val loss: 0.7563068270683289
Epoch 910, training loss: 12.130163192749023 = 0.05633462220430374 + 2.0 * 6.036914348602295
Epoch 910, val loss: 0.7609984278678894
Epoch 920, training loss: 12.126577377319336 = 0.05413423478603363 + 2.0 * 6.036221504211426
Epoch 920, val loss: 0.7658426761627197
Epoch 930, training loss: 12.127869606018066 = 0.05204872414469719 + 2.0 * 6.037910461425781
Epoch 930, val loss: 0.7706255912780762
Epoch 940, training loss: 12.131841659545898 = 0.050081219524145126 + 2.0 * 6.04088020324707
Epoch 940, val loss: 0.7754589319229126
Epoch 950, training loss: 12.122512817382812 = 0.04821234941482544 + 2.0 * 6.0371503829956055
Epoch 950, val loss: 0.7802664041519165
Epoch 960, training loss: 12.112909317016602 = 0.04645627737045288 + 2.0 * 6.033226490020752
Epoch 960, val loss: 0.7850264310836792
Epoch 970, training loss: 12.114142417907715 = 0.0447908379137516 + 2.0 * 6.034675598144531
Epoch 970, val loss: 0.7899096012115479
Epoch 980, training loss: 12.108616828918457 = 0.04320478066802025 + 2.0 * 6.032706260681152
Epoch 980, val loss: 0.7946390509605408
Epoch 990, training loss: 12.107988357543945 = 0.04169470816850662 + 2.0 * 6.033146858215332
Epoch 990, val loss: 0.7993678450584412
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7963
Overall ASR: 0.8413
Flip ASR: 0.8089/225 nodes
The final ASR:0.76261, 0.05695, Accuracy:0.80247, 0.00630
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11668])
remove edge: torch.Size([2, 9534])
updated graph: torch.Size([2, 10646])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9963
Flip ASR: 0.9956/225 nodes
The final ASR:0.98401, 0.00870, Accuracy:0.82840, 0.00462
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.3 der2:0.3 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 18.684349060058594 = 1.936658501625061 + 2.0 * 8.373845100402832
Epoch 0, val loss: 1.9408937692642212
Epoch 10, training loss: 18.67368507385254 = 1.927230715751648 + 2.0 * 8.3732271194458
Epoch 10, val loss: 1.932134747505188
Epoch 20, training loss: 18.65365219116211 = 1.9157724380493164 + 2.0 * 8.368939399719238
Epoch 20, val loss: 1.9210058450698853
Epoch 30, training loss: 18.57931137084961 = 1.9005292654037476 + 2.0 * 8.339390754699707
Epoch 30, val loss: 1.9060477018356323
Epoch 40, training loss: 18.178024291992188 = 1.8826631307601929 + 2.0 * 8.147680282592773
Epoch 40, val loss: 1.888993740081787
Epoch 50, training loss: 17.139774322509766 = 1.8623143434524536 + 2.0 * 7.638730049133301
Epoch 50, val loss: 1.8692938089370728
Epoch 60, training loss: 16.297285079956055 = 1.8453176021575928 + 2.0 * 7.225983619689941
Epoch 60, val loss: 1.8537960052490234
Epoch 70, training loss: 15.55112361907959 = 1.832636833190918 + 2.0 * 6.859243392944336
Epoch 70, val loss: 1.842430591583252
Epoch 80, training loss: 15.190080642700195 = 1.8214244842529297 + 2.0 * 6.684328079223633
Epoch 80, val loss: 1.8317515850067139
Epoch 90, training loss: 14.963225364685059 = 1.8090204000473022 + 2.0 * 6.5771026611328125
Epoch 90, val loss: 1.819422960281372
Epoch 100, training loss: 14.80058479309082 = 1.7959961891174316 + 2.0 * 6.502294540405273
Epoch 100, val loss: 1.806688904762268
Epoch 110, training loss: 14.680816650390625 = 1.783176302909851 + 2.0 * 6.448820114135742
Epoch 110, val loss: 1.7944329977035522
Epoch 120, training loss: 14.583484649658203 = 1.7703324556350708 + 2.0 * 6.406576156616211
Epoch 120, val loss: 1.782235026359558
Epoch 130, training loss: 14.496454238891602 = 1.7570481300354004 + 2.0 * 6.36970329284668
Epoch 130, val loss: 1.7697116136550903
Epoch 140, training loss: 14.420400619506836 = 1.7427465915679932 + 2.0 * 6.338827133178711
Epoch 140, val loss: 1.7566806077957153
Epoch 150, training loss: 14.352025985717773 = 1.7267898321151733 + 2.0 * 6.312618255615234
Epoch 150, val loss: 1.742409586906433
Epoch 160, training loss: 14.285482406616211 = 1.7087122201919556 + 2.0 * 6.288384914398193
Epoch 160, val loss: 1.72677743434906
Epoch 170, training loss: 14.226317405700684 = 1.6882410049438477 + 2.0 * 6.269038200378418
Epoch 170, val loss: 1.7093325853347778
Epoch 180, training loss: 14.167085647583008 = 1.664994716644287 + 2.0 * 6.251045227050781
Epoch 180, val loss: 1.6898322105407715
Epoch 190, training loss: 14.111419677734375 = 1.638643741607666 + 2.0 * 6.236388206481934
Epoch 190, val loss: 1.6679807901382446
Epoch 200, training loss: 14.054373741149902 = 1.6092493534088135 + 2.0 * 6.222562313079834
Epoch 200, val loss: 1.6438852548599243
Epoch 210, training loss: 13.99582290649414 = 1.576715111732483 + 2.0 * 6.2095537185668945
Epoch 210, val loss: 1.6174381971359253
Epoch 220, training loss: 13.940269470214844 = 1.5409119129180908 + 2.0 * 6.199678897857666
Epoch 220, val loss: 1.588587760925293
Epoch 230, training loss: 13.881926536560059 = 1.5018941164016724 + 2.0 * 6.190016269683838
Epoch 230, val loss: 1.5576566457748413
Epoch 240, training loss: 13.820977210998535 = 1.4602304697036743 + 2.0 * 6.180373191833496
Epoch 240, val loss: 1.5247148275375366
Epoch 250, training loss: 13.760147094726562 = 1.4160765409469604 + 2.0 * 6.172035217285156
Epoch 250, val loss: 1.490222454071045
Epoch 260, training loss: 13.701569557189941 = 1.3698738813400269 + 2.0 * 6.1658477783203125
Epoch 260, val loss: 1.4546369314193726
Epoch 270, training loss: 13.640233993530273 = 1.3225481510162354 + 2.0 * 6.158843040466309
Epoch 270, val loss: 1.4187551736831665
Epoch 280, training loss: 13.598835945129395 = 1.2750662565231323 + 2.0 * 6.161884784698486
Epoch 280, val loss: 1.3830312490463257
Epoch 290, training loss: 13.524925231933594 = 1.2287501096725464 + 2.0 * 6.148087501525879
Epoch 290, val loss: 1.3489278554916382
Epoch 300, training loss: 13.467157363891602 = 1.1839938163757324 + 2.0 * 6.141582012176514
Epoch 300, val loss: 1.3162003755569458
Epoch 310, training loss: 13.414522171020508 = 1.1412161588668823 + 2.0 * 6.136652946472168
Epoch 310, val loss: 1.2852613925933838
Epoch 320, training loss: 13.364458084106445 = 1.1007964611053467 + 2.0 * 6.13183069229126
Epoch 320, val loss: 1.256392002105713
Epoch 330, training loss: 13.32679557800293 = 1.0629392862319946 + 2.0 * 6.131927967071533
Epoch 330, val loss: 1.2297303676605225
Epoch 340, training loss: 13.279123306274414 = 1.0282561779022217 + 2.0 * 6.125433444976807
Epoch 340, val loss: 1.2056057453155518
Epoch 350, training loss: 13.235795974731445 = 0.9964488744735718 + 2.0 * 6.119673728942871
Epoch 350, val loss: 1.1837631464004517
Epoch 360, training loss: 13.198817253112793 = 0.9667030572891235 + 2.0 * 6.1160569190979
Epoch 360, val loss: 1.1635156869888306
Epoch 370, training loss: 13.165763854980469 = 0.938652753829956 + 2.0 * 6.113555431365967
Epoch 370, val loss: 1.1445175409317017
Epoch 380, training loss: 13.130866050720215 = 0.9118315577507019 + 2.0 * 6.1095170974731445
Epoch 380, val loss: 1.1266489028930664
Epoch 390, training loss: 13.097665786743164 = 0.8858785033226013 + 2.0 * 6.105893611907959
Epoch 390, val loss: 1.1092946529388428
Epoch 400, training loss: 13.065051078796387 = 0.8601976037025452 + 2.0 * 6.102426528930664
Epoch 400, val loss: 1.0919958353042603
Epoch 410, training loss: 13.044692993164062 = 0.83451247215271 + 2.0 * 6.105090141296387
Epoch 410, val loss: 1.074387788772583
Epoch 420, training loss: 13.002601623535156 = 0.8086385726928711 + 2.0 * 6.096981525421143
Epoch 420, val loss: 1.0565704107284546
Epoch 430, training loss: 12.97425365447998 = 0.7826764583587646 + 2.0 * 6.095788478851318
Epoch 430, val loss: 1.0385019779205322
Epoch 440, training loss: 12.941351890563965 = 0.7562353014945984 + 2.0 * 6.09255838394165
Epoch 440, val loss: 1.0200196504592896
Epoch 450, training loss: 12.909292221069336 = 0.7292523384094238 + 2.0 * 6.090020179748535
Epoch 450, val loss: 1.0009465217590332
Epoch 460, training loss: 12.888772010803223 = 0.7017288208007812 + 2.0 * 6.093521595001221
Epoch 460, val loss: 0.9814062118530273
Epoch 470, training loss: 12.848621368408203 = 0.6741177439689636 + 2.0 * 6.087251663208008
Epoch 470, val loss: 0.961571455001831
Epoch 480, training loss: 12.81756591796875 = 0.6462119817733765 + 2.0 * 6.085677146911621
Epoch 480, val loss: 0.9417226910591125
Epoch 490, training loss: 12.78449821472168 = 0.6182541251182556 + 2.0 * 6.083122253417969
Epoch 490, val loss: 0.9219027757644653
Epoch 500, training loss: 12.751189231872559 = 0.5904077291488647 + 2.0 * 6.080390930175781
Epoch 500, val loss: 0.9022879600524902
Epoch 510, training loss: 12.722911834716797 = 0.5626308917999268 + 2.0 * 6.080140590667725
Epoch 510, val loss: 0.8829658031463623
Epoch 520, training loss: 12.695880889892578 = 0.5352529287338257 + 2.0 * 6.0803141593933105
Epoch 520, val loss: 0.8641977310180664
Epoch 530, training loss: 12.660313606262207 = 0.5084062218666077 + 2.0 * 6.075953483581543
Epoch 530, val loss: 0.8462874889373779
Epoch 540, training loss: 12.63087272644043 = 0.4820477366447449 + 2.0 * 6.0744123458862305
Epoch 540, val loss: 0.8292076587677002
Epoch 550, training loss: 12.61361026763916 = 0.4562074542045593 + 2.0 * 6.078701496124268
Epoch 550, val loss: 0.813028872013092
Epoch 560, training loss: 12.584187507629395 = 0.43113085627555847 + 2.0 * 6.076528549194336
Epoch 560, val loss: 0.7978515028953552
Epoch 570, training loss: 12.550533294677734 = 0.4069598615169525 + 2.0 * 6.071786880493164
Epoch 570, val loss: 0.7839402556419373
Epoch 580, training loss: 12.521777153015137 = 0.38351595401763916 + 2.0 * 6.0691304206848145
Epoch 580, val loss: 0.7710863351821899
Epoch 590, training loss: 12.495317459106445 = 0.36084628105163574 + 2.0 * 6.067235469818115
Epoch 590, val loss: 0.7592896819114685
Epoch 600, training loss: 12.475332260131836 = 0.3389686048030853 + 2.0 * 6.068181991577148
Epoch 600, val loss: 0.7485232949256897
Epoch 610, training loss: 12.45487117767334 = 0.3180692195892334 + 2.0 * 6.068400859832764
Epoch 610, val loss: 0.7387101054191589
Epoch 620, training loss: 12.43036937713623 = 0.29828739166259766 + 2.0 * 6.066040992736816
Epoch 620, val loss: 0.730144202709198
Epoch 630, training loss: 12.403696060180664 = 0.27957844734191895 + 2.0 * 6.062058925628662
Epoch 630, val loss: 0.7227712869644165
Epoch 640, training loss: 12.38394546508789 = 0.2619796693325043 + 2.0 * 6.060982704162598
Epoch 640, val loss: 0.7164508104324341
Epoch 650, training loss: 12.36666202545166 = 0.24541977047920227 + 2.0 * 6.06062126159668
Epoch 650, val loss: 0.7111488580703735
Epoch 660, training loss: 12.347290992736816 = 0.22997404634952545 + 2.0 * 6.058658599853516
Epoch 660, val loss: 0.7068162560462952
Epoch 670, training loss: 12.333619117736816 = 0.21559806168079376 + 2.0 * 6.0590105056762695
Epoch 670, val loss: 0.7035313248634338
Epoch 680, training loss: 12.322516441345215 = 0.2022787630558014 + 2.0 * 6.060118675231934
Epoch 680, val loss: 0.701155960559845
Epoch 690, training loss: 12.30375862121582 = 0.18993887305259705 + 2.0 * 6.056910037994385
Epoch 690, val loss: 0.6996227502822876
Epoch 700, training loss: 12.291413307189941 = 0.17854343354701996 + 2.0 * 6.0564351081848145
Epoch 700, val loss: 0.698941707611084
Epoch 710, training loss: 12.276474952697754 = 0.16800954937934875 + 2.0 * 6.054232597351074
Epoch 710, val loss: 0.6989952325820923
Epoch 720, training loss: 12.261282920837402 = 0.15827828645706177 + 2.0 * 6.051502227783203
Epoch 720, val loss: 0.6997720003128052
Epoch 730, training loss: 12.251895904541016 = 0.14927200973033905 + 2.0 * 6.05131196975708
Epoch 730, val loss: 0.7011972069740295
Epoch 740, training loss: 12.247574806213379 = 0.1409706026315689 + 2.0 * 6.05330228805542
Epoch 740, val loss: 0.7030743360519409
Epoch 750, training loss: 12.234977722167969 = 0.13330601155757904 + 2.0 * 6.050836086273193
Epoch 750, val loss: 0.7055385112762451
Epoch 760, training loss: 12.220465660095215 = 0.1262209713459015 + 2.0 * 6.047122478485107
Epoch 760, val loss: 0.7085241675376892
Epoch 770, training loss: 12.214920043945312 = 0.11964092403650284 + 2.0 * 6.0476393699646
Epoch 770, val loss: 0.7119047045707703
Epoch 780, training loss: 12.205533027648926 = 0.11354271322488785 + 2.0 * 6.045995235443115
Epoch 780, val loss: 0.7155465483665466
Epoch 790, training loss: 12.199642181396484 = 0.1078798770904541 + 2.0 * 6.045881271362305
Epoch 790, val loss: 0.7195467948913574
Epoch 800, training loss: 12.191633224487305 = 0.10260112583637238 + 2.0 * 6.044516086578369
Epoch 800, val loss: 0.723772406578064
Epoch 810, training loss: 12.189842224121094 = 0.09768500924110413 + 2.0 * 6.046078681945801
Epoch 810, val loss: 0.7281566858291626
Epoch 820, training loss: 12.184571266174316 = 0.09308528900146484 + 2.0 * 6.045742988586426
Epoch 820, val loss: 0.7326518297195435
Epoch 830, training loss: 12.172516822814941 = 0.08883010596036911 + 2.0 * 6.041843414306641
Epoch 830, val loss: 0.7373067140579224
Epoch 840, training loss: 12.171541213989258 = 0.0848301574587822 + 2.0 * 6.043355464935303
Epoch 840, val loss: 0.7420746684074402
Epoch 850, training loss: 12.160346031188965 = 0.08109123259782791 + 2.0 * 6.039627552032471
Epoch 850, val loss: 0.7469221949577332
Epoch 860, training loss: 12.155134201049805 = 0.07756710052490234 + 2.0 * 6.038783550262451
Epoch 860, val loss: 0.7518584132194519
Epoch 870, training loss: 12.151185035705566 = 0.07423943281173706 + 2.0 * 6.038472652435303
Epoch 870, val loss: 0.7568429708480835
Epoch 880, training loss: 12.153783798217773 = 0.07111016660928726 + 2.0 * 6.041337013244629
Epoch 880, val loss: 0.7617728114128113
Epoch 890, training loss: 12.145159721374512 = 0.06815694272518158 + 2.0 * 6.038501262664795
Epoch 890, val loss: 0.7666190266609192
Epoch 900, training loss: 12.137906074523926 = 0.06541632860898972 + 2.0 * 6.036244869232178
Epoch 900, val loss: 0.7714979648590088
Epoch 910, training loss: 12.13312816619873 = 0.06282002478837967 + 2.0 * 6.035153865814209
Epoch 910, val loss: 0.7764614820480347
Epoch 920, training loss: 12.127195358276367 = 0.0603555403649807 + 2.0 * 6.033420085906982
Epoch 920, val loss: 0.7814282178878784
Epoch 930, training loss: 12.130245208740234 = 0.05801805108785629 + 2.0 * 6.036113739013672
Epoch 930, val loss: 0.7863462567329407
Epoch 940, training loss: 12.128769874572754 = 0.05581642687320709 + 2.0 * 6.0364766120910645
Epoch 940, val loss: 0.791141927242279
Epoch 950, training loss: 12.11815071105957 = 0.05373711884021759 + 2.0 * 6.032207012176514
Epoch 950, val loss: 0.7959799766540527
Epoch 960, training loss: 12.11483383178711 = 0.0517718568444252 + 2.0 * 6.031530857086182
Epoch 960, val loss: 0.8008477687835693
Epoch 970, training loss: 12.109910011291504 = 0.04990328475832939 + 2.0 * 6.030003547668457
Epoch 970, val loss: 0.8056776523590088
Epoch 980, training loss: 12.115191459655762 = 0.048120394349098206 + 2.0 * 6.033535480499268
Epoch 980, val loss: 0.8104390501976013
Epoch 990, training loss: 12.121204376220703 = 0.0464404933154583 + 2.0 * 6.037382125854492
Epoch 990, val loss: 0.8149805068969727
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 730, in <module>
    torch.cuda.empty_cache()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/cuda/memory.py", line 121, in empty_cache
    torch._C._cuda_emptyCache()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.3 der2:0.3 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 18.675649642944336 = 1.9278043508529663 + 2.0 * 8.373922348022461
Epoch 0, val loss: 1.924447774887085
Epoch 10, training loss: 18.66501808166504 = 1.9180034399032593 + 2.0 * 8.373507499694824
Epoch 10, val loss: 1.9152370691299438
Epoch 20, training loss: 18.646181106567383 = 1.9057599306106567 + 2.0 * 8.370210647583008
Epoch 20, val loss: 1.903173804283142
Epoch 30, training loss: 18.58578872680664 = 1.8891313076019287 + 2.0 * 8.348328590393066
Epoch 30, val loss: 1.8866252899169922
Epoch 40, training loss: 18.319721221923828 = 1.868900179862976 + 2.0 * 8.225410461425781
Epoch 40, val loss: 1.8676600456237793
Epoch 50, training loss: 17.3882999420166 = 1.8466989994049072 + 2.0 * 7.7708001136779785
Epoch 50, val loss: 1.8474127054214478
Epoch 60, training loss: 16.572776794433594 = 1.8273674249649048 + 2.0 * 7.372704982757568
Epoch 60, val loss: 1.8308392763137817
Epoch 70, training loss: 15.886839866638184 = 1.8123703002929688 + 2.0 * 7.037234783172607
Epoch 70, val loss: 1.8173885345458984
Epoch 80, training loss: 15.351234436035156 = 1.79868745803833 + 2.0 * 6.776273250579834
Epoch 80, val loss: 1.805217981338501
Epoch 90, training loss: 15.073137283325195 = 1.785138726234436 + 2.0 * 6.643999099731445
Epoch 90, val loss: 1.7933605909347534
Epoch 100, training loss: 14.916814804077148 = 1.7698638439178467 + 2.0 * 6.573475360870361
Epoch 100, val loss: 1.7807563543319702
Epoch 110, training loss: 14.783573150634766 = 1.7538191080093384 + 2.0 * 6.514876842498779
Epoch 110, val loss: 1.767785906791687
Epoch 120, training loss: 14.672953605651855 = 1.737099528312683 + 2.0 * 6.467926979064941
Epoch 120, val loss: 1.754146933555603
Epoch 130, training loss: 14.570568084716797 = 1.719113826751709 + 2.0 * 6.425727367401123
Epoch 130, val loss: 1.7394055128097534
Epoch 140, training loss: 14.481971740722656 = 1.6991091966629028 + 2.0 * 6.3914313316345215
Epoch 140, val loss: 1.7231248617172241
Epoch 150, training loss: 14.400225639343262 = 1.6763759851455688 + 2.0 * 6.361924648284912
Epoch 150, val loss: 1.7048898935317993
Epoch 160, training loss: 14.328252792358398 = 1.6504579782485962 + 2.0 * 6.338897228240967
Epoch 160, val loss: 1.6842162609100342
Epoch 170, training loss: 14.25754165649414 = 1.6212881803512573 + 2.0 * 6.318126678466797
Epoch 170, val loss: 1.6610244512557983
Epoch 180, training loss: 14.188570022583008 = 1.5886685848236084 + 2.0 * 6.29995059967041
Epoch 180, val loss: 1.6351546049118042
Epoch 190, training loss: 14.118831634521484 = 1.5522422790527344 + 2.0 * 6.283294677734375
Epoch 190, val loss: 1.6064190864562988
Epoch 200, training loss: 14.048927307128906 = 1.5122601985931396 + 2.0 * 6.268333435058594
Epoch 200, val loss: 1.5749880075454712
Epoch 210, training loss: 13.983940124511719 = 1.4692816734313965 + 2.0 * 6.25732946395874
Epoch 210, val loss: 1.541197419166565
Epoch 220, training loss: 13.913593292236328 = 1.4239797592163086 + 2.0 * 6.24480676651001
Epoch 220, val loss: 1.505974531173706
Epoch 230, training loss: 13.84291934967041 = 1.3769142627716064 + 2.0 * 6.233002662658691
Epoch 230, val loss: 1.4696612358093262
Epoch 240, training loss: 13.773576736450195 = 1.32798171043396 + 2.0 * 6.222797393798828
Epoch 240, val loss: 1.4323680400848389
Epoch 250, training loss: 13.704829216003418 = 1.277580976486206 + 2.0 * 6.213624000549316
Epoch 250, val loss: 1.3941112756729126
Epoch 260, training loss: 13.651453971862793 = 1.22649085521698 + 2.0 * 6.212481498718262
Epoch 260, val loss: 1.3555716276168823
Epoch 270, training loss: 13.571822166442871 = 1.1760326623916626 + 2.0 * 6.19789457321167
Epoch 270, val loss: 1.317865014076233
Epoch 280, training loss: 13.505891799926758 = 1.1259483098983765 + 2.0 * 6.189971923828125
Epoch 280, val loss: 1.2807704210281372
Epoch 290, training loss: 13.443100929260254 = 1.0762978792190552 + 2.0 * 6.183401584625244
Epoch 290, val loss: 1.2442941665649414
Epoch 300, training loss: 13.382162094116211 = 1.027465581893921 + 2.0 * 6.1773481369018555
Epoch 300, val loss: 1.208683967590332
Epoch 310, training loss: 13.332659721374512 = 0.980291485786438 + 2.0 * 6.176184177398682
Epoch 310, val loss: 1.1745303869247437
Epoch 320, training loss: 13.269381523132324 = 0.9355878233909607 + 2.0 * 6.166896820068359
Epoch 320, val loss: 1.1431386470794678
Epoch 330, training loss: 13.215082168579102 = 0.893304169178009 + 2.0 * 6.160889148712158
Epoch 330, val loss: 1.1143293380737305
Epoch 340, training loss: 13.166065216064453 = 0.8533097505569458 + 2.0 * 6.156377792358398
Epoch 340, val loss: 1.0879530906677246
Epoch 350, training loss: 13.12389087677002 = 0.8156901001930237 + 2.0 * 6.15410041809082
Epoch 350, val loss: 1.0640660524368286
Epoch 360, training loss: 13.07695484161377 = 0.7805052995681763 + 2.0 * 6.148224830627441
Epoch 360, val loss: 1.0428709983825684
Epoch 370, training loss: 13.035017013549805 = 0.7474768161773682 + 2.0 * 6.143770217895508
Epoch 370, val loss: 1.0240728855133057
Epoch 380, training loss: 13.008186340332031 = 0.7164419889450073 + 2.0 * 6.145872116088867
Epoch 380, val loss: 1.0073046684265137
Epoch 390, training loss: 12.962512969970703 = 0.687471866607666 + 2.0 * 6.137520790100098
Epoch 390, val loss: 0.9926235675811768
Epoch 400, training loss: 12.92573070526123 = 0.6602076888084412 + 2.0 * 6.132761478424072
Epoch 400, val loss: 0.9797613024711609
Epoch 410, training loss: 12.906049728393555 = 0.6344484090805054 + 2.0 * 6.135800838470459
Epoch 410, val loss: 0.9683641195297241
Epoch 420, training loss: 12.863348007202148 = 0.6101278066635132 + 2.0 * 6.126610279083252
Epoch 420, val loss: 0.9584378600120544
Epoch 430, training loss: 12.834647178649902 = 0.58719801902771 + 2.0 * 6.123724460601807
Epoch 430, val loss: 0.9500206708908081
Epoch 440, training loss: 12.807519912719727 = 0.5653711557388306 + 2.0 * 6.121074199676514
Epoch 440, val loss: 0.942641019821167
Epoch 450, training loss: 12.797011375427246 = 0.5444700717926025 + 2.0 * 6.126270771026611
Epoch 450, val loss: 0.936209499835968
Epoch 460, training loss: 12.755546569824219 = 0.5243848562240601 + 2.0 * 6.115581035614014
Epoch 460, val loss: 0.9308133721351624
Epoch 470, training loss: 12.740423202514648 = 0.5049928426742554 + 2.0 * 6.117715358734131
Epoch 470, val loss: 0.9262505769729614
Epoch 480, training loss: 12.710195541381836 = 0.48617005348205566 + 2.0 * 6.11201286315918
Epoch 480, val loss: 0.922397255897522
Epoch 490, training loss: 12.683920860290527 = 0.4676852524280548 + 2.0 * 6.108117580413818
Epoch 490, val loss: 0.9193252325057983
Epoch 500, training loss: 12.662489891052246 = 0.44939979910850525 + 2.0 * 6.1065449714660645
Epoch 500, val loss: 0.9168045520782471
Epoch 510, training loss: 12.640843391418457 = 0.43129053711891174 + 2.0 * 6.104776382446289
Epoch 510, val loss: 0.9148495197296143
Epoch 520, training loss: 12.63236141204834 = 0.4134043753147125 + 2.0 * 6.10947847366333
Epoch 520, val loss: 0.9137022495269775
Epoch 530, training loss: 12.595746994018555 = 0.3957211971282959 + 2.0 * 6.10001277923584
Epoch 530, val loss: 0.9131468534469604
Epoch 540, training loss: 12.57524585723877 = 0.3782593011856079 + 2.0 * 6.0984930992126465
Epoch 540, val loss: 0.9132708311080933
Epoch 550, training loss: 12.568745613098145 = 0.36097007989883423 + 2.0 * 6.103887557983398
Epoch 550, val loss: 0.9138771295547485
Epoch 560, training loss: 12.537190437316895 = 0.3440881669521332 + 2.0 * 6.096550941467285
Epoch 560, val loss: 0.9152205586433411
Epoch 570, training loss: 12.510848045349121 = 0.3275558054447174 + 2.0 * 6.091646194458008
Epoch 570, val loss: 0.9172433614730835
Epoch 580, training loss: 12.495908737182617 = 0.31144243478775024 + 2.0 * 6.092233180999756
Epoch 580, val loss: 0.9197438359260559
Epoch 590, training loss: 12.482573509216309 = 0.2958275377750397 + 2.0 * 6.093372821807861
Epoch 590, val loss: 0.922780454158783
Epoch 600, training loss: 12.454587936401367 = 0.28079771995544434 + 2.0 * 6.086894989013672
Epoch 600, val loss: 0.9264925122261047
Epoch 610, training loss: 12.43880844116211 = 0.26638031005859375 + 2.0 * 6.086214065551758
Epoch 610, val loss: 0.9307789206504822
Epoch 620, training loss: 12.428143501281738 = 0.25257477164268494 + 2.0 * 6.087784290313721
Epoch 620, val loss: 0.9355159401893616
Epoch 630, training loss: 12.404510498046875 = 0.2394910305738449 + 2.0 * 6.082509517669678
Epoch 630, val loss: 0.9408838152885437
Epoch 640, training loss: 12.389442443847656 = 0.22706052660942078 + 2.0 * 6.081191062927246
Epoch 640, val loss: 0.9468012452125549
Epoch 650, training loss: 12.373705863952637 = 0.21523049473762512 + 2.0 * 6.079237461090088
Epoch 650, val loss: 0.9531069397926331
Epoch 660, training loss: 12.372659683227539 = 0.20400017499923706 + 2.0 * 6.084329605102539
Epoch 660, val loss: 0.9598166942596436
Epoch 670, training loss: 12.35000991821289 = 0.19344183802604675 + 2.0 * 6.07828426361084
Epoch 670, val loss: 0.9670675992965698
Epoch 680, training loss: 12.35010051727295 = 0.18343982100486755 + 2.0 * 6.083330154418945
Epoch 680, val loss: 0.9745787382125854
Epoch 690, training loss: 12.321197509765625 = 0.17402231693267822 + 2.0 * 6.073587417602539
Epoch 690, val loss: 0.9823285341262817
Epoch 700, training loss: 12.31045150756836 = 0.16514867544174194 + 2.0 * 6.072651386260986
Epoch 700, val loss: 0.9905984401702881
Epoch 710, training loss: 12.301153182983398 = 0.15675793588161469 + 2.0 * 6.072197437286377
Epoch 710, val loss: 0.9991111755371094
Epoch 720, training loss: 12.297958374023438 = 0.14885249733924866 + 2.0 * 6.0745530128479
Epoch 720, val loss: 1.0078288316726685
Epoch 730, training loss: 12.280725479125977 = 0.1414547711610794 + 2.0 * 6.069635391235352
Epoch 730, val loss: 1.0169713497161865
Epoch 740, training loss: 12.269135475158691 = 0.13446687161922455 + 2.0 * 6.067334175109863
Epoch 740, val loss: 1.0262720584869385
Epoch 750, training loss: 12.268939018249512 = 0.12786878645420074 + 2.0 * 6.070535182952881
Epoch 750, val loss: 1.035772681236267
Epoch 760, training loss: 12.26424503326416 = 0.12167130410671234 + 2.0 * 6.071286678314209
Epoch 760, val loss: 1.0454802513122559
Epoch 770, training loss: 12.244880676269531 = 0.11582685261964798 + 2.0 * 6.0645270347595215
Epoch 770, val loss: 1.055428385734558
Epoch 780, training loss: 12.236002922058105 = 0.11033084243535995 + 2.0 * 6.062836170196533
Epoch 780, val loss: 1.0655370950698853
Epoch 790, training loss: 12.23982048034668 = 0.10515075922012329 + 2.0 * 6.0673346519470215
Epoch 790, val loss: 1.075782299041748
Epoch 800, training loss: 12.228211402893066 = 0.10025400668382645 + 2.0 * 6.063978672027588
Epoch 800, val loss: 1.0859984159469604
Epoch 810, training loss: 12.216681480407715 = 0.09566792845726013 + 2.0 * 6.060506820678711
Epoch 810, val loss: 1.0965224504470825
Epoch 820, training loss: 12.207878112792969 = 0.09132882952690125 + 2.0 * 6.058274745941162
Epoch 820, val loss: 1.1070289611816406
Epoch 830, training loss: 12.211575508117676 = 0.08722643554210663 + 2.0 * 6.062174320220947
Epoch 830, val loss: 1.1175777912139893
Epoch 840, training loss: 12.198864936828613 = 0.08337297290563583 + 2.0 * 6.057745933532715
Epoch 840, val loss: 1.1281661987304688
Epoch 850, training loss: 12.197517395019531 = 0.07973451167345047 + 2.0 * 6.058891296386719
Epoch 850, val loss: 1.1388343572616577
Epoch 860, training loss: 12.187271118164062 = 0.07631228864192963 + 2.0 * 6.055479526519775
Epoch 860, val loss: 1.1494101285934448
Epoch 870, training loss: 12.181295394897461 = 0.07307455688714981 + 2.0 * 6.054110527038574
Epoch 870, val loss: 1.160186529159546
Epoch 880, training loss: 12.178900718688965 = 0.07001011818647385 + 2.0 * 6.054445266723633
Epoch 880, val loss: 1.1708210706710815
Epoch 890, training loss: 12.176583290100098 = 0.06711114943027496 + 2.0 * 6.054736137390137
Epoch 890, val loss: 1.1814630031585693
Epoch 900, training loss: 12.171304702758789 = 0.06437550485134125 + 2.0 * 6.053464412689209
Epoch 900, val loss: 1.1920889616012573
Epoch 910, training loss: 12.168766021728516 = 0.061799731105566025 + 2.0 * 6.053483009338379
Epoch 910, val loss: 1.2026317119598389
Epoch 920, training loss: 12.159616470336914 = 0.05935997888445854 + 2.0 * 6.05012845993042
Epoch 920, val loss: 1.2130601406097412
Epoch 930, training loss: 12.157211303710938 = 0.05705057829618454 + 2.0 * 6.050080299377441
Epoch 930, val loss: 1.223617672920227
Epoch 940, training loss: 12.152745246887207 = 0.054860468953847885 + 2.0 * 6.048942565917969
Epoch 940, val loss: 1.2339972257614136
Epoch 950, training loss: 12.153745651245117 = 0.052783600986003876 + 2.0 * 6.050480842590332
Epoch 950, val loss: 1.244370698928833
Epoch 960, training loss: 12.158820152282715 = 0.05082174763083458 + 2.0 * 6.053999423980713
Epoch 960, val loss: 1.2545455694198608
Epoch 970, training loss: 12.143533706665039 = 0.04895569011569023 + 2.0 * 6.04728889465332
Epoch 970, val loss: 1.2647596597671509
Epoch 980, training loss: 12.13593864440918 = 0.0471915602684021 + 2.0 * 6.044373512268066
Epoch 980, val loss: 1.2748486995697021
Epoch 990, training loss: 12.1347017288208 = 0.0455092154443264 + 2.0 * 6.044596195220947
Epoch 990, val loss: 1.2847821712493896
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7407
Overall ASR: 0.4797
Flip ASR: 0.3911/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.698909759521484 = 1.9511423110961914 + 2.0 * 8.373883247375488
Epoch 0, val loss: 1.960000991821289
Epoch 10, training loss: 18.68724822998047 = 1.9403538703918457 + 2.0 * 8.37344741821289
Epoch 10, val loss: 1.9483819007873535
Epoch 20, training loss: 18.667064666748047 = 1.9272024631500244 + 2.0 * 8.3699312210083
Epoch 20, val loss: 1.9338003396987915
Epoch 30, training loss: 18.60181999206543 = 1.9095433950424194 + 2.0 * 8.346138000488281
Epoch 30, val loss: 1.9137934446334839
Epoch 40, training loss: 18.32709503173828 = 1.8881956338882446 + 2.0 * 8.219449996948242
Epoch 40, val loss: 1.8901461362838745
Epoch 50, training loss: 17.37333106994629 = 1.8669461011886597 + 2.0 * 7.753192901611328
Epoch 50, val loss: 1.8670719861984253
Epoch 60, training loss: 16.540416717529297 = 1.8454947471618652 + 2.0 * 7.347461223602295
Epoch 60, val loss: 1.8444896936416626
Epoch 70, training loss: 16.11369514465332 = 1.8253188133239746 + 2.0 * 7.144188404083252
Epoch 70, val loss: 1.8233705759048462
Epoch 80, training loss: 15.741554260253906 = 1.8057526350021362 + 2.0 * 6.96790075302124
Epoch 80, val loss: 1.8033361434936523
Epoch 90, training loss: 15.453855514526367 = 1.7884889841079712 + 2.0 * 6.832683086395264
Epoch 90, val loss: 1.786425232887268
Epoch 100, training loss: 15.196063995361328 = 1.7721072435379028 + 2.0 * 6.711978435516357
Epoch 100, val loss: 1.7695281505584717
Epoch 110, training loss: 14.96338939666748 = 1.7569266557693481 + 2.0 * 6.603231430053711
Epoch 110, val loss: 1.7537745237350464
Epoch 120, training loss: 14.82105827331543 = 1.7393521070480347 + 2.0 * 6.540853023529053
Epoch 120, val loss: 1.7370785474777222
Epoch 130, training loss: 14.695863723754883 = 1.7187713384628296 + 2.0 * 6.488546371459961
Epoch 130, val loss: 1.718379259109497
Epoch 140, training loss: 14.590448379516602 = 1.6968923807144165 + 2.0 * 6.446777820587158
Epoch 140, val loss: 1.6986491680145264
Epoch 150, training loss: 14.492600440979004 = 1.673413634300232 + 2.0 * 6.40959358215332
Epoch 150, val loss: 1.677466630935669
Epoch 160, training loss: 14.399538040161133 = 1.6477878093719482 + 2.0 * 6.375874996185303
Epoch 160, val loss: 1.6547898054122925
Epoch 170, training loss: 14.312535285949707 = 1.619095802307129 + 2.0 * 6.346719741821289
Epoch 170, val loss: 1.6304316520690918
Epoch 180, training loss: 14.228821754455566 = 1.5870202779769897 + 2.0 * 6.320900917053223
Epoch 180, val loss: 1.6036350727081299
Epoch 190, training loss: 14.151286125183105 = 1.551023244857788 + 2.0 * 6.300131320953369
Epoch 190, val loss: 1.5739248991012573
Epoch 200, training loss: 14.07693099975586 = 1.5108410120010376 + 2.0 * 6.283044815063477
Epoch 200, val loss: 1.5412119626998901
Epoch 210, training loss: 14.008828163146973 = 1.4669746160507202 + 2.0 * 6.2709269523620605
Epoch 210, val loss: 1.5056650638580322
Epoch 220, training loss: 13.934956550598145 = 1.4200918674468994 + 2.0 * 6.257432460784912
Epoch 220, val loss: 1.4688729047775269
Epoch 230, training loss: 13.862242698669434 = 1.3711408376693726 + 2.0 * 6.245551109313965
Epoch 230, val loss: 1.4307371377944946
Epoch 240, training loss: 13.792241096496582 = 1.3208191394805908 + 2.0 * 6.235711097717285
Epoch 240, val loss: 1.3922463655471802
Epoch 250, training loss: 13.729608535766602 = 1.270105004310608 + 2.0 * 6.2297515869140625
Epoch 250, val loss: 1.3544658422470093
Epoch 260, training loss: 13.658233642578125 = 1.2208807468414307 + 2.0 * 6.218676567077637
Epoch 260, val loss: 1.3181647062301636
Epoch 270, training loss: 13.595536231994629 = 1.1736584901809692 + 2.0 * 6.210938930511475
Epoch 270, val loss: 1.2842906713485718
Epoch 280, training loss: 13.536392211914062 = 1.1288572549819946 + 2.0 * 6.2037672996521
Epoch 280, val loss: 1.2528635263442993
Epoch 290, training loss: 13.480307579040527 = 1.0867258310317993 + 2.0 * 6.19679069519043
Epoch 290, val loss: 1.2238683700561523
Epoch 300, training loss: 13.42834186553955 = 1.0474027395248413 + 2.0 * 6.190469741821289
Epoch 300, val loss: 1.1971760988235474
Epoch 310, training loss: 13.378181457519531 = 1.010548710823059 + 2.0 * 6.183816432952881
Epoch 310, val loss: 1.1724425554275513
Epoch 320, training loss: 13.333382606506348 = 0.9757066369056702 + 2.0 * 6.178837776184082
Epoch 320, val loss: 1.149306297302246
Epoch 330, training loss: 13.289517402648926 = 0.9428431391716003 + 2.0 * 6.173336982727051
Epoch 330, val loss: 1.1274105310440063
Epoch 340, training loss: 13.248153686523438 = 0.9113407135009766 + 2.0 * 6.1684064865112305
Epoch 340, val loss: 1.1063156127929688
Epoch 350, training loss: 13.207271575927734 = 0.8805617094039917 + 2.0 * 6.163354873657227
Epoch 350, val loss: 1.085677146911621
Epoch 360, training loss: 13.179658889770508 = 0.8501016497612 + 2.0 * 6.164778709411621
Epoch 360, val loss: 1.0651235580444336
Epoch 370, training loss: 13.134966850280762 = 0.8201176524162292 + 2.0 * 6.157424449920654
Epoch 370, val loss: 1.0443922281265259
Epoch 380, training loss: 13.095998764038086 = 0.7902026176452637 + 2.0 * 6.152897834777832
Epoch 380, val loss: 1.0237139463424683
Epoch 390, training loss: 13.05777645111084 = 0.7601713538169861 + 2.0 * 6.148802757263184
Epoch 390, val loss: 1.0027680397033691
Epoch 400, training loss: 13.02159309387207 = 0.7298628687858582 + 2.0 * 6.145864963531494
Epoch 400, val loss: 0.9816282987594604
Epoch 410, training loss: 12.98777961730957 = 0.6994097232818604 + 2.0 * 6.1441850662231445
Epoch 410, val loss: 0.960484504699707
Epoch 420, training loss: 12.950311660766602 = 0.6690292954444885 + 2.0 * 6.140641212463379
Epoch 420, val loss: 0.9394779205322266
Epoch 430, training loss: 12.913339614868164 = 0.6387603878974915 + 2.0 * 6.137289524078369
Epoch 430, val loss: 0.9187192320823669
Epoch 440, training loss: 12.878846168518066 = 0.6085563898086548 + 2.0 * 6.1351447105407715
Epoch 440, val loss: 0.8982776999473572
Epoch 450, training loss: 12.84567642211914 = 0.5785447359085083 + 2.0 * 6.133565902709961
Epoch 450, val loss: 0.878256618976593
Epoch 460, training loss: 12.807147979736328 = 0.5489640831947327 + 2.0 * 6.129091739654541
Epoch 460, val loss: 0.8590925335884094
Epoch 470, training loss: 12.77252197265625 = 0.5197609066963196 + 2.0 * 6.126380443572998
Epoch 470, val loss: 0.8406031131744385
Epoch 480, training loss: 12.742656707763672 = 0.49096250534057617 + 2.0 * 6.125846862792969
Epoch 480, val loss: 0.8229697346687317
Epoch 490, training loss: 12.711350440979004 = 0.46297818422317505 + 2.0 * 6.124186038970947
Epoch 490, val loss: 0.8061280846595764
Epoch 500, training loss: 12.673715591430664 = 0.4356834292411804 + 2.0 * 6.119016170501709
Epoch 500, val loss: 0.7906880378723145
Epoch 510, training loss: 12.649523735046387 = 0.4093342125415802 + 2.0 * 6.1200947761535645
Epoch 510, val loss: 0.7763944268226624
Epoch 520, training loss: 12.619169235229492 = 0.38399454951286316 + 2.0 * 6.117587566375732
Epoch 520, val loss: 0.7632991671562195
Epoch 530, training loss: 12.584197998046875 = 0.35988059639930725 + 2.0 * 6.11215877532959
Epoch 530, val loss: 0.7516409158706665
Epoch 540, training loss: 12.558908462524414 = 0.3369568884372711 + 2.0 * 6.110975742340088
Epoch 540, val loss: 0.7413567900657654
Epoch 550, training loss: 12.534855842590332 = 0.31532901525497437 + 2.0 * 6.1097636222839355
Epoch 550, val loss: 0.7323046922683716
Epoch 560, training loss: 12.5142822265625 = 0.29509449005126953 + 2.0 * 6.109593868255615
Epoch 560, val loss: 0.7245823740959167
Epoch 570, training loss: 12.488729476928711 = 0.27642396092414856 + 2.0 * 6.106152534484863
Epoch 570, val loss: 0.7182568907737732
Epoch 580, training loss: 12.463862419128418 = 0.2590164840221405 + 2.0 * 6.102423191070557
Epoch 580, val loss: 0.7130612730979919
Epoch 590, training loss: 12.442907333374023 = 0.24284867942333221 + 2.0 * 6.100029468536377
Epoch 590, val loss: 0.7088996171951294
Epoch 600, training loss: 12.42435359954834 = 0.22783704102039337 + 2.0 * 6.0982584953308105
Epoch 600, val loss: 0.7057026028633118
Epoch 610, training loss: 12.422811508178711 = 0.2140021175146103 + 2.0 * 6.104404926300049
Epoch 610, val loss: 0.7032856345176697
Epoch 620, training loss: 12.396135330200195 = 0.20124942064285278 + 2.0 * 6.097443103790283
Epoch 620, val loss: 0.7018097043037415
Epoch 630, training loss: 12.376115798950195 = 0.18956921994686127 + 2.0 * 6.093273162841797
Epoch 630, val loss: 0.7011346220970154
Epoch 640, training loss: 12.360799789428711 = 0.17878946661949158 + 2.0 * 6.091005325317383
Epoch 640, val loss: 0.7011200785636902
Epoch 650, training loss: 12.36446475982666 = 0.16885235905647278 + 2.0 * 6.097805976867676
Epoch 650, val loss: 0.7016231417655945
Epoch 660, training loss: 12.336526870727539 = 0.15965788066387177 + 2.0 * 6.08843469619751
Epoch 660, val loss: 0.7026088237762451
Epoch 670, training loss: 12.324076652526855 = 0.15121443569660187 + 2.0 * 6.08643102645874
Epoch 670, val loss: 0.7040765881538391
Epoch 680, training loss: 12.313089370727539 = 0.14338883757591248 + 2.0 * 6.084850311279297
Epoch 680, val loss: 0.7060493230819702
Epoch 690, training loss: 12.309840202331543 = 0.13610854744911194 + 2.0 * 6.0868659019470215
Epoch 690, val loss: 0.7083150744438171
Epoch 700, training loss: 12.298755645751953 = 0.12936000525951385 + 2.0 * 6.084697723388672
Epoch 700, val loss: 0.710885763168335
Epoch 710, training loss: 12.282527923583984 = 0.12310391664505005 + 2.0 * 6.0797119140625
Epoch 710, val loss: 0.7138016819953918
Epoch 720, training loss: 12.275141716003418 = 0.1172671914100647 + 2.0 * 6.07893705368042
Epoch 720, val loss: 0.7169694900512695
Epoch 730, training loss: 12.27304744720459 = 0.11180320382118225 + 2.0 * 6.08062219619751
Epoch 730, val loss: 0.7202423810958862
Epoch 740, training loss: 12.257752418518066 = 0.10665923357009888 + 2.0 * 6.075546741485596
Epoch 740, val loss: 0.7237129807472229
Epoch 750, training loss: 12.251258850097656 = 0.1018463596701622 + 2.0 * 6.074706077575684
Epoch 750, val loss: 0.7275317907333374
Epoch 760, training loss: 12.247450828552246 = 0.09732241183519363 + 2.0 * 6.075064182281494
Epoch 760, val loss: 0.7314335703849792
Epoch 770, training loss: 12.24034309387207 = 0.09305230528116226 + 2.0 * 6.07364559173584
Epoch 770, val loss: 0.7353168725967407
Epoch 780, training loss: 12.23082447052002 = 0.08904416859149933 + 2.0 * 6.070889949798584
Epoch 780, val loss: 0.7395566701889038
Epoch 790, training loss: 12.22389030456543 = 0.08526244014501572 + 2.0 * 6.069314002990723
Epoch 790, val loss: 0.7438175082206726
Epoch 800, training loss: 12.222040176391602 = 0.0816783532500267 + 2.0 * 6.070180892944336
Epoch 800, val loss: 0.7482417225837708
Epoch 810, training loss: 12.211904525756836 = 0.07826723903417587 + 2.0 * 6.066818714141846
Epoch 810, val loss: 0.7526350021362305
Epoch 820, training loss: 12.206839561462402 = 0.07504773885011673 + 2.0 * 6.065896034240723
Epoch 820, val loss: 0.7571675181388855
Epoch 830, training loss: 12.206021308898926 = 0.07199563086032867 + 2.0 * 6.067012786865234
Epoch 830, val loss: 0.7618920207023621
Epoch 840, training loss: 12.207430839538574 = 0.06908518075942993 + 2.0 * 6.0691728591918945
Epoch 840, val loss: 0.7663801312446594
Epoch 850, training loss: 12.193595886230469 = 0.0663553774356842 + 2.0 * 6.063620090484619
Epoch 850, val loss: 0.771041214466095
Epoch 860, training loss: 12.186837196350098 = 0.06376203894615173 + 2.0 * 6.061537742614746
Epoch 860, val loss: 0.7757951617240906
Epoch 870, training loss: 12.181536674499512 = 0.061287179589271545 + 2.0 * 6.06012487411499
Epoch 870, val loss: 0.7805513143539429
Epoch 880, training loss: 12.176968574523926 = 0.05891964212059975 + 2.0 * 6.059024333953857
Epoch 880, val loss: 0.7853575944900513
Epoch 890, training loss: 12.173968315124512 = 0.05665399134159088 + 2.0 * 6.058657169342041
Epoch 890, val loss: 0.7902026772499084
Epoch 900, training loss: 12.172638893127441 = 0.054486557841300964 + 2.0 * 6.059076309204102
Epoch 900, val loss: 0.7950555682182312
Epoch 910, training loss: 12.170948028564453 = 0.05242374911904335 + 2.0 * 6.059262275695801
Epoch 910, val loss: 0.7998649477958679
Epoch 920, training loss: 12.162386894226074 = 0.05046188458800316 + 2.0 * 6.055962562561035
Epoch 920, val loss: 0.8047944903373718
Epoch 930, training loss: 12.160199165344238 = 0.04858402907848358 + 2.0 * 6.055807590484619
Epoch 930, val loss: 0.8098088502883911
Epoch 940, training loss: 12.16088581085205 = 0.04678977280855179 + 2.0 * 6.0570478439331055
Epoch 940, val loss: 0.8146737813949585
Epoch 950, training loss: 12.156035423278809 = 0.04506657272577286 + 2.0 * 6.055484294891357
Epoch 950, val loss: 0.8195757269859314
Epoch 960, training loss: 12.149137496948242 = 0.04343145340681076 + 2.0 * 6.052853107452393
Epoch 960, val loss: 0.8244239687919617
Epoch 970, training loss: 12.14609432220459 = 0.04186480492353439 + 2.0 * 6.052114963531494
Epoch 970, val loss: 0.8294077515602112
Epoch 980, training loss: 12.146723747253418 = 0.04037371650338173 + 2.0 * 6.05317497253418
Epoch 980, val loss: 0.8341978788375854
Epoch 990, training loss: 12.139058113098145 = 0.03894896060228348 + 2.0 * 6.050054550170898
Epoch 990, val loss: 0.8391516208648682
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7815
Overall ASR: 0.6753
Flip ASR: 0.6267/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.692535400390625 = 1.9447052478790283 + 2.0 * 8.37391471862793
Epoch 0, val loss: 1.9446161985397339
Epoch 10, training loss: 18.681859970092773 = 1.9347641468048096 + 2.0 * 8.373547554016113
Epoch 10, val loss: 1.9343630075454712
Epoch 20, training loss: 18.663738250732422 = 1.922184705734253 + 2.0 * 8.370777130126953
Epoch 20, val loss: 1.921310544013977
Epoch 30, training loss: 18.60609245300293 = 1.9048947095870972 + 2.0 * 8.35059928894043
Epoch 30, val loss: 1.903503656387329
Epoch 40, training loss: 18.328001022338867 = 1.8829725980758667 + 2.0 * 8.222514152526855
Epoch 40, val loss: 1.8819605112075806
Epoch 50, training loss: 17.422805786132812 = 1.8604803085327148 + 2.0 * 7.781162261962891
Epoch 50, val loss: 1.8608826398849487
Epoch 60, training loss: 16.603818893432617 = 1.8405026197433472 + 2.0 * 7.381658554077148
Epoch 60, val loss: 1.8424187898635864
Epoch 70, training loss: 15.658238410949707 = 1.8245915174484253 + 2.0 * 6.916823387145996
Epoch 70, val loss: 1.8267935514450073
Epoch 80, training loss: 15.251949310302734 = 1.8095663785934448 + 2.0 * 6.72119140625
Epoch 80, val loss: 1.8122053146362305
Epoch 90, training loss: 15.009259223937988 = 1.7931467294692993 + 2.0 * 6.60805606842041
Epoch 90, val loss: 1.7972596883773804
Epoch 100, training loss: 14.83238410949707 = 1.7758897542953491 + 2.0 * 6.528247356414795
Epoch 100, val loss: 1.781744360923767
Epoch 110, training loss: 14.695130348205566 = 1.7581872940063477 + 2.0 * 6.468471527099609
Epoch 110, val loss: 1.7656437158584595
Epoch 120, training loss: 14.589641571044922 = 1.7391153573989868 + 2.0 * 6.425262928009033
Epoch 120, val loss: 1.7483655214309692
Epoch 130, training loss: 14.506543159484863 = 1.7187271118164062 + 2.0 * 6.3939080238342285
Epoch 130, val loss: 1.7301241159439087
Epoch 140, training loss: 14.42012882232666 = 1.696785569190979 + 2.0 * 6.361671447753906
Epoch 140, val loss: 1.7109867334365845
Epoch 150, training loss: 14.346776962280273 = 1.6726999282836914 + 2.0 * 6.337038516998291
Epoch 150, val loss: 1.6907689571380615
Epoch 160, training loss: 14.275911331176758 = 1.6459078788757324 + 2.0 * 6.315001964569092
Epoch 160, val loss: 1.6690294742584229
Epoch 170, training loss: 14.21143913269043 = 1.6159719228744507 + 2.0 * 6.297733783721924
Epoch 170, val loss: 1.645479679107666
Epoch 180, training loss: 14.143880844116211 = 1.5828754901885986 + 2.0 * 6.280502796173096
Epoch 180, val loss: 1.6197758913040161
Epoch 190, training loss: 14.075271606445312 = 1.546216368675232 + 2.0 * 6.264527797698975
Epoch 190, val loss: 1.591727375984192
Epoch 200, training loss: 14.01211166381836 = 1.505675196647644 + 2.0 * 6.253218173980713
Epoch 200, val loss: 1.561099886894226
Epoch 210, training loss: 13.940675735473633 = 1.4616819620132446 + 2.0 * 6.23949670791626
Epoch 210, val loss: 1.527917742729187
Epoch 220, training loss: 13.873760223388672 = 1.4144121408462524 + 2.0 * 6.229673862457275
Epoch 220, val loss: 1.4923673868179321
Epoch 230, training loss: 13.806182861328125 = 1.364384412765503 + 2.0 * 6.2208991050720215
Epoch 230, val loss: 1.4549096822738647
Epoch 240, training loss: 13.737245559692383 = 1.3118561506271362 + 2.0 * 6.2126946449279785
Epoch 240, val loss: 1.4158239364624023
Epoch 250, training loss: 13.674302101135254 = 1.2572590112686157 + 2.0 * 6.208521366119385
Epoch 250, val loss: 1.3753515481948853
Epoch 260, training loss: 13.601619720458984 = 1.2016489505767822 + 2.0 * 6.199985504150391
Epoch 260, val loss: 1.3345112800598145
Epoch 270, training loss: 13.533002853393555 = 1.145682692527771 + 2.0 * 6.193660259246826
Epoch 270, val loss: 1.2935888767242432
Epoch 280, training loss: 13.46549129486084 = 1.0894289016723633 + 2.0 * 6.188031196594238
Epoch 280, val loss: 1.2528053522109985
Epoch 290, training loss: 13.39970874786377 = 1.0335015058517456 + 2.0 * 6.183103561401367
Epoch 290, val loss: 1.2127997875213623
Epoch 300, training loss: 13.334739685058594 = 0.978955864906311 + 2.0 * 6.177891731262207
Epoch 300, val loss: 1.1739853620529175
Epoch 310, training loss: 13.272013664245605 = 0.9258648753166199 + 2.0 * 6.173074245452881
Epoch 310, val loss: 1.1367945671081543
Epoch 320, training loss: 13.214520454406738 = 0.8748250603675842 + 2.0 * 6.16984748840332
Epoch 320, val loss: 1.101836085319519
Epoch 330, training loss: 13.152032852172852 = 0.8265905976295471 + 2.0 * 6.162721157073975
Epoch 330, val loss: 1.0691910982131958
Epoch 340, training loss: 13.095085144042969 = 0.7809172868728638 + 2.0 * 6.157083988189697
Epoch 340, val loss: 1.0390520095825195
Epoch 350, training loss: 13.05093765258789 = 0.7377934455871582 + 2.0 * 6.156572341918945
Epoch 350, val loss: 1.0114470720291138
Epoch 360, training loss: 12.994039535522461 = 0.6976099014282227 + 2.0 * 6.148214817047119
Epoch 360, val loss: 0.9864576458930969
Epoch 370, training loss: 12.949417114257812 = 0.6603299975395203 + 2.0 * 6.144543647766113
Epoch 370, val loss: 0.9640929102897644
Epoch 380, training loss: 12.904723167419434 = 0.6253839135169983 + 2.0 * 6.139669418334961
Epoch 380, val loss: 0.9441191554069519
Epoch 390, training loss: 12.886781692504883 = 0.5925946235656738 + 2.0 * 6.147093296051025
Epoch 390, val loss: 0.9261115789413452
Epoch 400, training loss: 12.835188865661621 = 0.56193608045578 + 2.0 * 6.136626243591309
Epoch 400, val loss: 0.9102723598480225
Epoch 410, training loss: 12.790410041809082 = 0.5332902669906616 + 2.0 * 6.1285600662231445
Epoch 410, val loss: 0.896273136138916
Epoch 420, training loss: 12.756156921386719 = 0.5061264038085938 + 2.0 * 6.1250152587890625
Epoch 420, val loss: 0.883857250213623
Epoch 430, training loss: 12.723223686218262 = 0.4801490902900696 + 2.0 * 6.121537208557129
Epoch 430, val loss: 0.8726872205734253
Epoch 440, training loss: 12.693826675415039 = 0.4553174078464508 + 2.0 * 6.1192545890808105
Epoch 440, val loss: 0.8626281023025513
Epoch 450, training loss: 12.665631294250488 = 0.4316309988498688 + 2.0 * 6.117000102996826
Epoch 450, val loss: 0.8536485433578491
Epoch 460, training loss: 12.635499000549316 = 0.4089621603488922 + 2.0 * 6.1132683753967285
Epoch 460, val loss: 0.8457818627357483
Epoch 470, training loss: 12.612735748291016 = 0.38723325729370117 + 2.0 * 6.112751483917236
Epoch 470, val loss: 0.8389182686805725
Epoch 480, training loss: 12.588367462158203 = 0.36641955375671387 + 2.0 * 6.110973834991455
Epoch 480, val loss: 0.8329344391822815
Epoch 490, training loss: 12.55695915222168 = 0.34672483801841736 + 2.0 * 6.105117321014404
Epoch 490, val loss: 0.8279446363449097
Epoch 500, training loss: 12.53282642364502 = 0.32793286442756653 + 2.0 * 6.102446556091309
Epoch 500, val loss: 0.8240098357200623
Epoch 510, training loss: 12.534711837768555 = 0.30999308824539185 + 2.0 * 6.112359523773193
Epoch 510, val loss: 0.8210891485214233
Epoch 520, training loss: 12.492105484008789 = 0.2930881679058075 + 2.0 * 6.099508762359619
Epoch 520, val loss: 0.8190272450447083
Epoch 530, training loss: 12.470962524414062 = 0.2770724296569824 + 2.0 * 6.096945285797119
Epoch 530, val loss: 0.818010151386261
Epoch 540, training loss: 12.450607299804688 = 0.26191389560699463 + 2.0 * 6.094346523284912
Epoch 540, val loss: 0.8180248737335205
Epoch 550, training loss: 12.440345764160156 = 0.24756695330142975 + 2.0 * 6.096389293670654
Epoch 550, val loss: 0.8188856244087219
Epoch 560, training loss: 12.424914360046387 = 0.23412668704986572 + 2.0 * 6.095393657684326
Epoch 560, val loss: 0.8205768465995789
Epoch 570, training loss: 12.40129280090332 = 0.22149477899074554 + 2.0 * 6.089899063110352
Epoch 570, val loss: 0.8230288028717041
Epoch 580, training loss: 12.385004997253418 = 0.20961233973503113 + 2.0 * 6.087696552276611
Epoch 580, val loss: 0.8262807130813599
Epoch 590, training loss: 12.382896423339844 = 0.1984414905309677 + 2.0 * 6.092227458953857
Epoch 590, val loss: 0.8302922248840332
Epoch 600, training loss: 12.356376647949219 = 0.18793590366840363 + 2.0 * 6.0842204093933105
Epoch 600, val loss: 0.8348209261894226
Epoch 610, training loss: 12.344130516052246 = 0.17810575664043427 + 2.0 * 6.083012580871582
Epoch 610, val loss: 0.8399105072021484
Epoch 620, training loss: 12.33283519744873 = 0.16885612905025482 + 2.0 * 6.081989765167236
Epoch 620, val loss: 0.845518171787262
Epoch 630, training loss: 12.320241928100586 = 0.16015052795410156 + 2.0 * 6.080045700073242
Epoch 630, val loss: 0.8515098094940186
Epoch 640, training loss: 12.311406135559082 = 0.15196681022644043 + 2.0 * 6.079719543457031
Epoch 640, val loss: 0.8579347133636475
Epoch 650, training loss: 12.312088012695312 = 0.1443069726228714 + 2.0 * 6.083890438079834
Epoch 650, val loss: 0.8647968173027039
Epoch 660, training loss: 12.292031288146973 = 0.13716015219688416 + 2.0 * 6.077435493469238
Epoch 660, val loss: 0.871840238571167
Epoch 670, training loss: 12.278912544250488 = 0.13047711551189423 + 2.0 * 6.074217796325684
Epoch 670, val loss: 0.8794071674346924
Epoch 680, training loss: 12.269780158996582 = 0.12418654561042786 + 2.0 * 6.072796821594238
Epoch 680, val loss: 0.8872197270393372
Epoch 690, training loss: 12.261662483215332 = 0.1182674765586853 + 2.0 * 6.07169771194458
Epoch 690, val loss: 0.8952170014381409
Epoch 700, training loss: 12.264382362365723 = 0.1127253919839859 + 2.0 * 6.075828552246094
Epoch 700, val loss: 0.9034035801887512
Epoch 710, training loss: 12.252121925354004 = 0.1075390949845314 + 2.0 * 6.072291374206543
Epoch 710, val loss: 0.9116240739822388
Epoch 720, training loss: 12.240422248840332 = 0.10266593098640442 + 2.0 * 6.068878173828125
Epoch 720, val loss: 0.9200239777565002
Epoch 730, training loss: 12.232378005981445 = 0.09811113774776459 + 2.0 * 6.06713342666626
Epoch 730, val loss: 0.9285869002342224
Epoch 740, training loss: 12.22684097290039 = 0.09381777793169022 + 2.0 * 6.066511631011963
Epoch 740, val loss: 0.937251627445221
Epoch 750, training loss: 12.227487564086914 = 0.0897725373506546 + 2.0 * 6.068857669830322
Epoch 750, val loss: 0.9459604620933533
Epoch 760, training loss: 12.222641944885254 = 0.08599472045898438 + 2.0 * 6.068323612213135
Epoch 760, val loss: 0.9546750783920288
Epoch 770, training loss: 12.210135459899902 = 0.08244256675243378 + 2.0 * 6.063846588134766
Epoch 770, val loss: 0.9634219408035278
Epoch 780, training loss: 12.201836585998535 = 0.07908855378627777 + 2.0 * 6.061374187469482
Epoch 780, val loss: 0.9721723794937134
Epoch 790, training loss: 12.198493003845215 = 0.07592277228832245 + 2.0 * 6.061285018920898
Epoch 790, val loss: 0.9809930920600891
Epoch 800, training loss: 12.19831371307373 = 0.07293655723333359 + 2.0 * 6.06268835067749
Epoch 800, val loss: 0.9897608757019043
Epoch 810, training loss: 12.187849998474121 = 0.07012103497982025 + 2.0 * 6.058864593505859
Epoch 810, val loss: 0.9984450936317444
Epoch 820, training loss: 12.185214042663574 = 0.06746374070644379 + 2.0 * 6.05887508392334
Epoch 820, val loss: 1.0071877241134644
Epoch 830, training loss: 12.187492370605469 = 0.0649421364068985 + 2.0 * 6.061275005340576
Epoch 830, val loss: 1.015924334526062
Epoch 840, training loss: 12.175891876220703 = 0.06255875527858734 + 2.0 * 6.056666374206543
Epoch 840, val loss: 1.024583339691162
Epoch 850, training loss: 12.170588493347168 = 0.0602998211979866 + 2.0 * 6.055144309997559
Epoch 850, val loss: 1.0332045555114746
Epoch 860, training loss: 12.180618286132812 = 0.05815993621945381 + 2.0 * 6.061229228973389
Epoch 860, val loss: 1.0418729782104492
Epoch 870, training loss: 12.167898178100586 = 0.05611031502485275 + 2.0 * 6.055893898010254
Epoch 870, val loss: 1.050324559211731
Epoch 880, training loss: 12.157297134399414 = 0.05417877063155174 + 2.0 * 6.051558971405029
Epoch 880, val loss: 1.0587936639785767
Epoch 890, training loss: 12.159485816955566 = 0.05234009772539139 + 2.0 * 6.053572654724121
Epoch 890, val loss: 1.0672672986984253
Epoch 900, training loss: 12.153205871582031 = 0.05059049651026726 + 2.0 * 6.051307678222656
Epoch 900, val loss: 1.0757250785827637
Epoch 910, training loss: 12.14973258972168 = 0.04892740398645401 + 2.0 * 6.050402641296387
Epoch 910, val loss: 1.0839693546295166
Epoch 920, training loss: 12.144303321838379 = 0.04733520373702049 + 2.0 * 6.048483848571777
Epoch 920, val loss: 1.0922763347625732
Epoch 930, training loss: 12.14500904083252 = 0.04581467807292938 + 2.0 * 6.049597263336182
Epoch 930, val loss: 1.1004799604415894
Epoch 940, training loss: 12.146145820617676 = 0.04437104985117912 + 2.0 * 6.050887584686279
Epoch 940, val loss: 1.1085931062698364
Epoch 950, training loss: 12.139409065246582 = 0.042988371104002 + 2.0 * 6.048210144042969
Epoch 950, val loss: 1.116550326347351
Epoch 960, training loss: 12.131065368652344 = 0.04167548939585686 + 2.0 * 6.044694900512695
Epoch 960, val loss: 1.124570369720459
Epoch 970, training loss: 12.132255554199219 = 0.040416423231363297 + 2.0 * 6.045919418334961
Epoch 970, val loss: 1.1326130628585815
Epoch 980, training loss: 12.129524230957031 = 0.039206188172101974 + 2.0 * 6.045158863067627
Epoch 980, val loss: 1.1403658390045166
Epoch 990, training loss: 12.129680633544922 = 0.03805488720536232 + 2.0 * 6.045813083648682
Epoch 990, val loss: 1.1480331420898438
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7778
Overall ASR: 0.7970
Flip ASR: 0.7600/225 nodes
The final ASR:0.65068, 0.13072, Accuracy:0.76667, 0.01839
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11608])
remove edge: torch.Size([2, 9576])
updated graph: torch.Size([2, 10628])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9668
Flip ASR: 0.9600/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
The final ASR:0.97294, 0.00460, Accuracy:0.83457, 0.00630
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.3 der2:0.4 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 18.70829963684082 = 1.960640549659729 + 2.0 * 8.37382984161377
Epoch 0, val loss: 1.9578371047973633
Epoch 10, training loss: 18.69532012939453 = 1.948871374130249 + 2.0 * 8.373224258422852
Epoch 10, val loss: 1.9468865394592285
Epoch 20, training loss: 18.67351531982422 = 1.9341785907745361 + 2.0 * 8.369668006896973
Epoch 20, val loss: 1.9329922199249268
Epoch 30, training loss: 18.60614776611328 = 1.914536714553833 + 2.0 * 8.345805168151855
Epoch 30, val loss: 1.914352536201477
Epoch 40, training loss: 18.22783660888672 = 1.8913947343826294 + 2.0 * 8.168220520019531
Epoch 40, val loss: 1.892974853515625
Epoch 50, training loss: 16.829561233520508 = 1.8662950992584229 + 2.0 * 7.481632709503174
Epoch 50, val loss: 1.869839072227478
Epoch 60, training loss: 16.155595779418945 = 1.8485900163650513 + 2.0 * 7.153502464294434
Epoch 60, val loss: 1.8544188737869263
Epoch 70, training loss: 15.639951705932617 = 1.8383013010025024 + 2.0 * 6.900825023651123
Epoch 70, val loss: 1.8449721336364746
Epoch 80, training loss: 15.263720512390137 = 1.8263052701950073 + 2.0 * 6.71870756149292
Epoch 80, val loss: 1.8340731859207153
Epoch 90, training loss: 15.01739501953125 = 1.8149231672286987 + 2.0 * 6.601235866546631
Epoch 90, val loss: 1.8237550258636475
Epoch 100, training loss: 14.818277359008789 = 1.8045518398284912 + 2.0 * 6.506862640380859
Epoch 100, val loss: 1.8143573999404907
Epoch 110, training loss: 14.673638343811035 = 1.7957574129104614 + 2.0 * 6.438940525054932
Epoch 110, val loss: 1.8062715530395508
Epoch 120, training loss: 14.54960823059082 = 1.7877922058105469 + 2.0 * 6.380908012390137
Epoch 120, val loss: 1.7986819744110107
Epoch 130, training loss: 14.453896522521973 = 1.7802774906158447 + 2.0 * 6.3368096351623535
Epoch 130, val loss: 1.791330337524414
Epoch 140, training loss: 14.376606941223145 = 1.772563099861145 + 2.0 * 6.3020219802856445
Epoch 140, val loss: 1.7838886976242065
Epoch 150, training loss: 14.315373420715332 = 1.7640492916107178 + 2.0 * 6.275661945343018
Epoch 150, val loss: 1.7760943174362183
Epoch 160, training loss: 14.263326644897461 = 1.7544925212860107 + 2.0 * 6.2544169425964355
Epoch 160, val loss: 1.7678096294403076
Epoch 170, training loss: 14.215924263000488 = 1.7438026666641235 + 2.0 * 6.236060619354248
Epoch 170, val loss: 1.7588096857070923
Epoch 180, training loss: 14.173614501953125 = 1.7317100763320923 + 2.0 * 6.220952033996582
Epoch 180, val loss: 1.7488164901733398
Epoch 190, training loss: 14.131985664367676 = 1.7177932262420654 + 2.0 * 6.207096099853516
Epoch 190, val loss: 1.7375359535217285
Epoch 200, training loss: 14.094581604003906 = 1.7015807628631592 + 2.0 * 6.196500301361084
Epoch 200, val loss: 1.7245376110076904
Epoch 210, training loss: 14.05463981628418 = 1.682936191558838 + 2.0 * 6.185851573944092
Epoch 210, val loss: 1.7096922397613525
Epoch 220, training loss: 14.013442993164062 = 1.6614737510681152 + 2.0 * 6.175984859466553
Epoch 220, val loss: 1.6925365924835205
Epoch 230, training loss: 13.970362663269043 = 1.6365820169448853 + 2.0 * 6.1668901443481445
Epoch 230, val loss: 1.6726309061050415
Epoch 240, training loss: 13.933475494384766 = 1.6078652143478394 + 2.0 * 6.162805080413818
Epoch 240, val loss: 1.649633765220642
Epoch 250, training loss: 13.880434036254883 = 1.5755469799041748 + 2.0 * 6.1524434089660645
Epoch 250, val loss: 1.623688817024231
Epoch 260, training loss: 13.830272674560547 = 1.5394816398620605 + 2.0 * 6.145395755767822
Epoch 260, val loss: 1.594642162322998
Epoch 270, training loss: 13.78683853149414 = 1.4998886585235596 + 2.0 * 6.14347505569458
Epoch 270, val loss: 1.5628793239593506
Epoch 280, training loss: 13.73177719116211 = 1.45843505859375 + 2.0 * 6.13667106628418
Epoch 280, val loss: 1.529769778251648
Epoch 290, training loss: 13.6747465133667 = 1.41571044921875 + 2.0 * 6.129518032073975
Epoch 290, val loss: 1.495753526687622
Epoch 300, training loss: 13.623207092285156 = 1.372220754623413 + 2.0 * 6.125493049621582
Epoch 300, val loss: 1.4612890481948853
Epoch 310, training loss: 13.577836990356445 = 1.3295140266418457 + 2.0 * 6.124161720275879
Epoch 310, val loss: 1.4280539751052856
Epoch 320, training loss: 13.524157524108887 = 1.2885351181030273 + 2.0 * 6.11781120300293
Epoch 320, val loss: 1.3964533805847168
Epoch 330, training loss: 13.474250793457031 = 1.2487130165100098 + 2.0 * 6.11276912689209
Epoch 330, val loss: 1.365943193435669
Epoch 340, training loss: 13.43587875366211 = 1.2102323770523071 + 2.0 * 6.112823009490967
Epoch 340, val loss: 1.3369497060775757
Epoch 350, training loss: 13.385165214538574 = 1.173667073249817 + 2.0 * 6.105749130249023
Epoch 350, val loss: 1.3096891641616821
Epoch 360, training loss: 13.345839500427246 = 1.1386287212371826 + 2.0 * 6.103605270385742
Epoch 360, val loss: 1.2837135791778564
Epoch 370, training loss: 13.303089141845703 = 1.1049941778182983 + 2.0 * 6.099047660827637
Epoch 370, val loss: 1.25896155834198
Epoch 380, training loss: 13.26491641998291 = 1.072637677192688 + 2.0 * 6.096139430999756
Epoch 380, val loss: 1.2355320453643799
Epoch 390, training loss: 13.230371475219727 = 1.0416183471679688 + 2.0 * 6.094376564025879
Epoch 390, val loss: 1.2133344411849976
Epoch 400, training loss: 13.201985359191895 = 1.0124315023422241 + 2.0 * 6.0947771072387695
Epoch 400, val loss: 1.1928765773773193
Epoch 410, training loss: 13.15942096710205 = 0.9849397540092468 + 2.0 * 6.087240695953369
Epoch 410, val loss: 1.1739269495010376
Epoch 420, training loss: 13.127619743347168 = 0.958452045917511 + 2.0 * 6.084583759307861
Epoch 420, val loss: 1.1558547019958496
Epoch 430, training loss: 13.108957290649414 = 0.9326624870300293 + 2.0 * 6.088147163391113
Epoch 430, val loss: 1.1384793519973755
Epoch 440, training loss: 13.066856384277344 = 0.9076675176620483 + 2.0 * 6.079594612121582
Epoch 440, val loss: 1.1218078136444092
Epoch 450, training loss: 13.037826538085938 = 0.8829471468925476 + 2.0 * 6.077439785003662
Epoch 450, val loss: 1.1055201292037964
Epoch 460, training loss: 13.0095853805542 = 0.8580136895179749 + 2.0 * 6.0757856369018555
Epoch 460, val loss: 1.0892078876495361
Epoch 470, training loss: 12.99222469329834 = 0.8329630494117737 + 2.0 * 6.0796308517456055
Epoch 470, val loss: 1.0726474523544312
Epoch 480, training loss: 12.950664520263672 = 0.8075607419013977 + 2.0 * 6.07155179977417
Epoch 480, val loss: 1.0560526847839355
Epoch 490, training loss: 12.91975212097168 = 0.7814946174621582 + 2.0 * 6.06912899017334
Epoch 490, val loss: 1.0387715101242065
Epoch 500, training loss: 12.898598670959473 = 0.7546530365943909 + 2.0 * 6.071972846984863
Epoch 500, val loss: 1.0208569765090942
Epoch 510, training loss: 12.864625930786133 = 0.7272911667823792 + 2.0 * 6.068667411804199
Epoch 510, val loss: 1.0025475025177002
Epoch 520, training loss: 12.829998970031738 = 0.6996553540229797 + 2.0 * 6.065171718597412
Epoch 520, val loss: 0.9843042492866516
Epoch 530, training loss: 12.799644470214844 = 0.6717907190322876 + 2.0 * 6.063926696777344
Epoch 530, val loss: 0.96611487865448
Epoch 540, training loss: 12.76883602142334 = 0.6441173553466797 + 2.0 * 6.06235933303833
Epoch 540, val loss: 0.9484738707542419
Epoch 550, training loss: 12.739402770996094 = 0.6169611811637878 + 2.0 * 6.061220645904541
Epoch 550, val loss: 0.9319505095481873
Epoch 560, training loss: 12.708769798278809 = 0.5904747247695923 + 2.0 * 6.059147357940674
Epoch 560, val loss: 0.9164866209030151
Epoch 570, training loss: 12.687553405761719 = 0.5648631453514099 + 2.0 * 6.061345100402832
Epoch 570, val loss: 0.9025020003318787
Epoch 580, training loss: 12.657232284545898 = 0.5404456853866577 + 2.0 * 6.058393478393555
Epoch 580, val loss: 0.8901824355125427
Epoch 590, training loss: 12.628265380859375 = 0.5172090530395508 + 2.0 * 6.055528163909912
Epoch 590, val loss: 0.8794368505477905
Epoch 600, training loss: 12.602900505065918 = 0.4950638711452484 + 2.0 * 6.053918361663818
Epoch 600, val loss: 0.870158851146698
Epoch 610, training loss: 12.584807395935059 = 0.47394004464149475 + 2.0 * 6.055433750152588
Epoch 610, val loss: 0.8621417880058289
Epoch 620, training loss: 12.56151008605957 = 0.45382118225097656 + 2.0 * 6.053844451904297
Epoch 620, val loss: 0.8554526567459106
Epoch 630, training loss: 12.545269966125488 = 0.4348796308040619 + 2.0 * 6.055195331573486
Epoch 630, val loss: 0.8498335480690002
Epoch 640, training loss: 12.517992973327637 = 0.4168640077114105 + 2.0 * 6.050564289093018
Epoch 640, val loss: 0.8454890251159668
Epoch 650, training loss: 12.496660232543945 = 0.399591326713562 + 2.0 * 6.048534393310547
Epoch 650, val loss: 0.8418369293212891
Epoch 660, training loss: 12.499650001525879 = 0.3829866349697113 + 2.0 * 6.058331489562988
Epoch 660, val loss: 0.8389086127281189
Epoch 670, training loss: 12.458864212036133 = 0.36716896295547485 + 2.0 * 6.045847415924072
Epoch 670, val loss: 0.8368555307388306
Epoch 680, training loss: 12.4431734085083 = 0.3519047498703003 + 2.0 * 6.0456342697143555
Epoch 680, val loss: 0.8354548811912537
Epoch 690, training loss: 12.42408275604248 = 0.3370981514453888 + 2.0 * 6.043492317199707
Epoch 690, val loss: 0.8344086408615112
Epoch 700, training loss: 12.41427230834961 = 0.32268762588500977 + 2.0 * 6.045792579650879
Epoch 700, val loss: 0.8338069319725037
Epoch 710, training loss: 12.404647827148438 = 0.3087979257106781 + 2.0 * 6.047924995422363
Epoch 710, val loss: 0.8335204720497131
Epoch 720, training loss: 12.38009262084961 = 0.29540419578552246 + 2.0 * 6.042344093322754
Epoch 720, val loss: 0.8337578177452087
Epoch 730, training loss: 12.36510181427002 = 0.28236663341522217 + 2.0 * 6.041367530822754
Epoch 730, val loss: 0.8340948224067688
Epoch 740, training loss: 12.353057861328125 = 0.26970604062080383 + 2.0 * 6.041676044464111
Epoch 740, val loss: 0.8345862627029419
Epoch 750, training loss: 12.338064193725586 = 0.2575075328350067 + 2.0 * 6.040278434753418
Epoch 750, val loss: 0.8354098200798035
Epoch 760, training loss: 12.32251262664795 = 0.2456839680671692 + 2.0 * 6.038414478302002
Epoch 760, val loss: 0.8364244103431702
Epoch 770, training loss: 12.314833641052246 = 0.23421914875507355 + 2.0 * 6.04030704498291
Epoch 770, val loss: 0.837520182132721
Epoch 780, training loss: 12.296899795532227 = 0.2231970578432083 + 2.0 * 6.036851406097412
Epoch 780, val loss: 0.838913083076477
Epoch 790, training loss: 12.284215927124023 = 0.21251536905765533 + 2.0 * 6.0358500480651855
Epoch 790, val loss: 0.8403968811035156
Epoch 800, training loss: 12.2825345993042 = 0.20225872099399567 + 2.0 * 6.040137767791748
Epoch 800, val loss: 0.8419604301452637
Epoch 810, training loss: 12.26242446899414 = 0.1924384981393814 + 2.0 * 6.0349931716918945
Epoch 810, val loss: 0.8438165783882141
Epoch 820, training loss: 12.250393867492676 = 0.18302954733371735 + 2.0 * 6.033682346343994
Epoch 820, val loss: 0.8458606600761414
Epoch 830, training loss: 12.252421379089355 = 0.1740105152130127 + 2.0 * 6.039205551147461
Epoch 830, val loss: 0.847951352596283
Epoch 840, training loss: 12.239195823669434 = 0.1654576063156128 + 2.0 * 6.036869049072266
Epoch 840, val loss: 0.8502098917961121
Epoch 850, training loss: 12.221567153930664 = 0.1573573499917984 + 2.0 * 6.032104969024658
Epoch 850, val loss: 0.8528866171836853
Epoch 860, training loss: 12.210614204406738 = 0.1496487408876419 + 2.0 * 6.030482769012451
Epoch 860, val loss: 0.8555362820625305
Epoch 870, training loss: 12.205945014953613 = 0.14231958985328674 + 2.0 * 6.03181266784668
Epoch 870, val loss: 0.858431339263916
Epoch 880, training loss: 12.193825721740723 = 0.13537950813770294 + 2.0 * 6.0292229652404785
Epoch 880, val loss: 0.8615249991416931
Epoch 890, training loss: 12.20383358001709 = 0.128836527466774 + 2.0 * 6.037498474121094
Epoch 890, val loss: 0.8648346662521362
Epoch 900, training loss: 12.180984497070312 = 0.12271611392498016 + 2.0 * 6.029134273529053
Epoch 900, val loss: 0.8682422637939453
Epoch 910, training loss: 12.172770500183105 = 0.11695633083581924 + 2.0 * 6.027906894683838
Epoch 910, val loss: 0.8720336556434631
Epoch 920, training loss: 12.163843154907227 = 0.11150635033845901 + 2.0 * 6.026168346405029
Epoch 920, val loss: 0.8758449554443359
Epoch 930, training loss: 12.184526443481445 = 0.10637670755386353 + 2.0 * 6.039074897766113
Epoch 930, val loss: 0.8797656297683716
Epoch 940, training loss: 12.151402473449707 = 0.10160468518733978 + 2.0 * 6.024899005889893
Epoch 940, val loss: 0.8838214874267578
Epoch 950, training loss: 12.147660255432129 = 0.09712209552526474 + 2.0 * 6.025269031524658
Epoch 950, val loss: 0.8882264494895935
Epoch 960, training loss: 12.141124725341797 = 0.09288524091243744 + 2.0 * 6.024119853973389
Epoch 960, val loss: 0.8926075100898743
Epoch 970, training loss: 12.146062850952148 = 0.08889671415090561 + 2.0 * 6.02858304977417
Epoch 970, val loss: 0.8970500826835632
Epoch 980, training loss: 12.133589744567871 = 0.08516860008239746 + 2.0 * 6.024210453033447
Epoch 980, val loss: 0.9018020033836365
Epoch 990, training loss: 12.130170822143555 = 0.08166688680648804 + 2.0 * 6.024251937866211
Epoch 990, val loss: 0.906486451625824
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7815
Overall ASR: 0.6236
Flip ASR: 0.5600/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.70392608642578 = 1.9563325643539429 + 2.0 * 8.373796463012695
Epoch 0, val loss: 1.9572572708129883
Epoch 10, training loss: 18.690576553344727 = 1.9454721212387085 + 2.0 * 8.372551918029785
Epoch 10, val loss: 1.9453520774841309
Epoch 20, training loss: 18.666297912597656 = 1.9318188428878784 + 2.0 * 8.367239952087402
Epoch 20, val loss: 1.9292566776275635
Epoch 30, training loss: 18.601287841796875 = 1.9133548736572266 + 2.0 * 8.343966484069824
Epoch 30, val loss: 1.9070043563842773
Epoch 40, training loss: 18.32115936279297 = 1.891750454902649 + 2.0 * 8.214704513549805
Epoch 40, val loss: 1.8819119930267334
Epoch 50, training loss: 17.36457633972168 = 1.8708112239837646 + 2.0 * 7.746882915496826
Epoch 50, val loss: 1.857995867729187
Epoch 60, training loss: 16.422937393188477 = 1.849430799484253 + 2.0 * 7.286753177642822
Epoch 60, val loss: 1.8353224992752075
Epoch 70, training loss: 15.7012357711792 = 1.8316751718521118 + 2.0 * 6.934780120849609
Epoch 70, val loss: 1.817997932434082
Epoch 80, training loss: 15.313421249389648 = 1.8163485527038574 + 2.0 * 6.748536109924316
Epoch 80, val loss: 1.803011178970337
Epoch 90, training loss: 15.10558032989502 = 1.8043715953826904 + 2.0 * 6.650604248046875
Epoch 90, val loss: 1.7907583713531494
Epoch 100, training loss: 14.944640159606934 = 1.7916374206542969 + 2.0 * 6.576501369476318
Epoch 100, val loss: 1.777435302734375
Epoch 110, training loss: 14.822559356689453 = 1.7802492380142212 + 2.0 * 6.521154880523682
Epoch 110, val loss: 1.7655785083770752
Epoch 120, training loss: 14.731746673583984 = 1.7699965238571167 + 2.0 * 6.480875015258789
Epoch 120, val loss: 1.754893183708191
Epoch 130, training loss: 14.639257431030273 = 1.7602674961090088 + 2.0 * 6.439495086669922
Epoch 130, val loss: 1.7450565099716187
Epoch 140, training loss: 14.545731544494629 = 1.751657485961914 + 2.0 * 6.397037029266357
Epoch 140, val loss: 1.7366691827774048
Epoch 150, training loss: 14.456127166748047 = 1.7437145709991455 + 2.0 * 6.35620641708374
Epoch 150, val loss: 1.72900390625
Epoch 160, training loss: 14.383442878723145 = 1.7348889112472534 + 2.0 * 6.324276924133301
Epoch 160, val loss: 1.7209770679473877
Epoch 170, training loss: 14.315393447875977 = 1.7244936227798462 + 2.0 * 6.295449733734131
Epoch 170, val loss: 1.712094783782959
Epoch 180, training loss: 14.257955551147461 = 1.7124675512313843 + 2.0 * 6.272744178771973
Epoch 180, val loss: 1.7022051811218262
Epoch 190, training loss: 14.207891464233398 = 1.6985926628112793 + 2.0 * 6.2546491622924805
Epoch 190, val loss: 1.6910489797592163
Epoch 200, training loss: 14.159186363220215 = 1.6826993227005005 + 2.0 * 6.238243579864502
Epoch 200, val loss: 1.6781868934631348
Epoch 210, training loss: 14.111265182495117 = 1.6641554832458496 + 2.0 * 6.223555088043213
Epoch 210, val loss: 1.6635056734085083
Epoch 220, training loss: 14.065591812133789 = 1.6425747871398926 + 2.0 * 6.211508750915527
Epoch 220, val loss: 1.6463199853897095
Epoch 230, training loss: 14.0159330368042 = 1.617329716682434 + 2.0 * 6.199301719665527
Epoch 230, val loss: 1.6264631748199463
Epoch 240, training loss: 13.965332984924316 = 1.5879203081130981 + 2.0 * 6.188706398010254
Epoch 240, val loss: 1.6031907796859741
Epoch 250, training loss: 13.918390274047852 = 1.55393648147583 + 2.0 * 6.18222713470459
Epoch 250, val loss: 1.5761619806289673
Epoch 260, training loss: 13.860286712646484 = 1.515403151512146 + 2.0 * 6.1724419593811035
Epoch 260, val loss: 1.5460131168365479
Epoch 270, training loss: 13.803255081176758 = 1.473102331161499 + 2.0 * 6.16507625579834
Epoch 270, val loss: 1.5126876831054688
Epoch 280, training loss: 13.751136779785156 = 1.4272531270980835 + 2.0 * 6.161942005157471
Epoch 280, val loss: 1.4767194986343384
Epoch 290, training loss: 13.688799858093262 = 1.3800175189971924 + 2.0 * 6.154391288757324
Epoch 290, val loss: 1.4401028156280518
Epoch 300, training loss: 13.629990577697754 = 1.332352638244629 + 2.0 * 6.1488189697265625
Epoch 300, val loss: 1.403632402420044
Epoch 310, training loss: 13.572280883789062 = 1.2849184274673462 + 2.0 * 6.143681049346924
Epoch 310, val loss: 1.3675670623779297
Epoch 320, training loss: 13.521157264709473 = 1.2392491102218628 + 2.0 * 6.14095401763916
Epoch 320, val loss: 1.3329728841781616
Epoch 330, training loss: 13.466382026672363 = 1.1958580017089844 + 2.0 * 6.1352620124816895
Epoch 330, val loss: 1.3006830215454102
Epoch 340, training loss: 13.413920402526855 = 1.154536485671997 + 2.0 * 6.129692077636719
Epoch 340, val loss: 1.2700225114822388
Epoch 350, training loss: 13.375235557556152 = 1.1152602434158325 + 2.0 * 6.129987716674805
Epoch 350, val loss: 1.240881085395813
Epoch 360, training loss: 13.32344913482666 = 1.0784772634506226 + 2.0 * 6.122486114501953
Epoch 360, val loss: 1.213724970817566
Epoch 370, training loss: 13.28169059753418 = 1.0439938306808472 + 2.0 * 6.1188483238220215
Epoch 370, val loss: 1.1882902383804321
Epoch 380, training loss: 13.240012168884277 = 1.0115699768066406 + 2.0 * 6.114221096038818
Epoch 380, val loss: 1.1642988920211792
Epoch 390, training loss: 13.203536033630371 = 0.9809277653694153 + 2.0 * 6.11130428314209
Epoch 390, val loss: 1.141831398010254
Epoch 400, training loss: 13.166560173034668 = 0.951764702796936 + 2.0 * 6.107397556304932
Epoch 400, val loss: 1.1203093528747559
Epoch 410, training loss: 13.136890411376953 = 0.9235836863517761 + 2.0 * 6.106653213500977
Epoch 410, val loss: 1.0997331142425537
Epoch 420, training loss: 13.106430053710938 = 0.8963290452957153 + 2.0 * 6.105050563812256
Epoch 420, val loss: 1.079977035522461
Epoch 430, training loss: 13.070167541503906 = 0.870015025138855 + 2.0 * 6.100076198577881
Epoch 430, val loss: 1.0608727931976318
Epoch 440, training loss: 13.036493301391602 = 0.8439850211143494 + 2.0 * 6.096254348754883
Epoch 440, val loss: 1.0420910120010376
Epoch 450, training loss: 13.019476890563965 = 0.8179894089698792 + 2.0 * 6.100743770599365
Epoch 450, val loss: 1.0233176946640015
Epoch 460, training loss: 12.974398612976074 = 0.7918369770050049 + 2.0 * 6.091280937194824
Epoch 460, val loss: 1.00467848777771
Epoch 470, training loss: 12.942961692810059 = 0.765574038028717 + 2.0 * 6.088693618774414
Epoch 470, val loss: 0.9859743714332581
Epoch 480, training loss: 12.913811683654785 = 0.7391517162322998 + 2.0 * 6.087329864501953
Epoch 480, val loss: 0.9671469330787659
Epoch 490, training loss: 12.891806602478027 = 0.7129697799682617 + 2.0 * 6.089418411254883
Epoch 490, val loss: 0.9483931660652161
Epoch 500, training loss: 12.853452682495117 = 0.6871116161346436 + 2.0 * 6.083170413970947
Epoch 500, val loss: 0.9301287531852722
Epoch 510, training loss: 12.820841789245605 = 0.66159588098526 + 2.0 * 6.079622745513916
Epoch 510, val loss: 0.9121251106262207
Epoch 520, training loss: 12.791370391845703 = 0.6363433599472046 + 2.0 * 6.077513694763184
Epoch 520, val loss: 0.8943304419517517
Epoch 530, training loss: 12.771642684936523 = 0.6117287278175354 + 2.0 * 6.079957008361816
Epoch 530, val loss: 0.8770807385444641
Epoch 540, training loss: 12.741080284118652 = 0.5882910490036011 + 2.0 * 6.076394557952881
Epoch 540, val loss: 0.8607995510101318
Epoch 550, training loss: 12.71052074432373 = 0.5657474994659424 + 2.0 * 6.072386741638184
Epoch 550, val loss: 0.8454675078392029
Epoch 560, training loss: 12.698542594909668 = 0.5440031290054321 + 2.0 * 6.077269554138184
Epoch 560, val loss: 0.8307555317878723
Epoch 570, training loss: 12.668534278869629 = 0.5231508612632751 + 2.0 * 6.072691917419434
Epoch 570, val loss: 0.8171655535697937
Epoch 580, training loss: 12.637306213378906 = 0.5033977031707764 + 2.0 * 6.066954135894775
Epoch 580, val loss: 0.8044837713241577
Epoch 590, training loss: 12.616061210632324 = 0.4844554364681244 + 2.0 * 6.065803050994873
Epoch 590, val loss: 0.7927284836769104
Epoch 600, training loss: 12.595287322998047 = 0.4663563668727875 + 2.0 * 6.064465522766113
Epoch 600, val loss: 0.7817971706390381
Epoch 610, training loss: 12.579424858093262 = 0.4492224156856537 + 2.0 * 6.065101146697998
Epoch 610, val loss: 0.771713137626648
Epoch 620, training loss: 12.55781364440918 = 0.4330238103866577 + 2.0 * 6.062395095825195
Epoch 620, val loss: 0.7627938389778137
Epoch 630, training loss: 12.538114547729492 = 0.41760390996932983 + 2.0 * 6.060255527496338
Epoch 630, val loss: 0.7547475099563599
Epoch 640, training loss: 12.521893501281738 = 0.40286093950271606 + 2.0 * 6.059516429901123
Epoch 640, val loss: 0.7474973201751709
Epoch 650, training loss: 12.510371208190918 = 0.38886621594429016 + 2.0 * 6.0607523918151855
Epoch 650, val loss: 0.7409767508506775
Epoch 660, training loss: 12.490036964416504 = 0.3755088150501251 + 2.0 * 6.0572638511657715
Epoch 660, val loss: 0.7354038953781128
Epoch 670, training loss: 12.471923828125 = 0.36268025636672974 + 2.0 * 6.054621696472168
Epoch 670, val loss: 0.7305521368980408
Epoch 680, training loss: 12.456048965454102 = 0.35019442439079285 + 2.0 * 6.052927494049072
Epoch 680, val loss: 0.7261912226676941
Epoch 690, training loss: 12.446992874145508 = 0.3380611836910248 + 2.0 * 6.0544657707214355
Epoch 690, val loss: 0.7224104404449463
Epoch 700, training loss: 12.441205978393555 = 0.3263380527496338 + 2.0 * 6.05743408203125
Epoch 700, val loss: 0.7191351056098938
Epoch 710, training loss: 12.417978286743164 = 0.31502866744995117 + 2.0 * 6.0514750480651855
Epoch 710, val loss: 0.7163768410682678
Epoch 720, training loss: 12.40181827545166 = 0.3039678931236267 + 2.0 * 6.048925399780273
Epoch 720, val loss: 0.7139650583267212
Epoch 730, training loss: 12.4072265625 = 0.2931308448314667 + 2.0 * 6.0570478439331055
Epoch 730, val loss: 0.7118842601776123
Epoch 740, training loss: 12.38620662689209 = 0.28251761198043823 + 2.0 * 6.051844596862793
Epoch 740, val loss: 0.710132896900177
Epoch 750, training loss: 12.36813735961914 = 0.272235244512558 + 2.0 * 6.0479512214660645
Epoch 750, val loss: 0.7087740302085876
Epoch 760, training loss: 12.352142333984375 = 0.2621188461780548 + 2.0 * 6.045011520385742
Epoch 760, val loss: 0.7075754404067993
Epoch 770, training loss: 12.339991569519043 = 0.2521590292453766 + 2.0 * 6.04391622543335
Epoch 770, val loss: 0.7066217064857483
Epoch 780, training loss: 12.333732604980469 = 0.24235841631889343 + 2.0 * 6.045687198638916
Epoch 780, val loss: 0.7059506177902222
Epoch 790, training loss: 12.33255386352539 = 0.23295475542545319 + 2.0 * 6.04979944229126
Epoch 790, val loss: 0.7054710984230042
Epoch 800, training loss: 12.312093734741211 = 0.2238151878118515 + 2.0 * 6.044139385223389
Epoch 800, val loss: 0.7054083347320557
Epoch 810, training loss: 12.297587394714355 = 0.21497677266597748 + 2.0 * 6.0413055419921875
Epoch 810, val loss: 0.7055405974388123
Epoch 820, training loss: 12.28668212890625 = 0.2063877284526825 + 2.0 * 6.040147304534912
Epoch 820, val loss: 0.7058321237564087
Epoch 830, training loss: 12.299585342407227 = 0.1980932205915451 + 2.0 * 6.050745964050293
Epoch 830, val loss: 0.7063454985618591
Epoch 840, training loss: 12.269643783569336 = 0.19020584225654602 + 2.0 * 6.039719104766846
Epoch 840, val loss: 0.7071316838264465
Epoch 850, training loss: 12.259811401367188 = 0.18259531259536743 + 2.0 * 6.038608074188232
Epoch 850, val loss: 0.7082201242446899
Epoch 860, training loss: 12.249210357666016 = 0.17530103027820587 + 2.0 * 6.036954879760742
Epoch 860, val loss: 0.709407389163971
Epoch 870, training loss: 12.25412368774414 = 0.16832906007766724 + 2.0 * 6.0428972244262695
Epoch 870, val loss: 0.7107259631156921
Epoch 880, training loss: 12.236030578613281 = 0.1616937667131424 + 2.0 * 6.037168502807617
Epoch 880, val loss: 0.7123028039932251
Epoch 890, training loss: 12.230446815490723 = 0.15537667274475098 + 2.0 * 6.037535190582275
Epoch 890, val loss: 0.7140416502952576
Epoch 900, training loss: 12.217926025390625 = 0.14932292699813843 + 2.0 * 6.0343017578125
Epoch 900, val loss: 0.7159046530723572
Epoch 910, training loss: 12.213924407958984 = 0.14354918897151947 + 2.0 * 6.035187721252441
Epoch 910, val loss: 0.7180077433586121
Epoch 920, training loss: 12.213537216186523 = 0.13806095719337463 + 2.0 * 6.03773832321167
Epoch 920, val loss: 0.7201528549194336
Epoch 930, training loss: 12.200996398925781 = 0.13285109400749207 + 2.0 * 6.0340728759765625
Epoch 930, val loss: 0.7224855422973633
Epoch 940, training loss: 12.192850112915039 = 0.1278376281261444 + 2.0 * 6.032506465911865
Epoch 940, val loss: 0.724944531917572
Epoch 950, training loss: 12.184576988220215 = 0.12307164818048477 + 2.0 * 6.030752658843994
Epoch 950, val loss: 0.7274710536003113
Epoch 960, training loss: 12.185656547546387 = 0.11850833892822266 + 2.0 * 6.033574104309082
Epoch 960, val loss: 0.7300726771354675
Epoch 970, training loss: 12.182868003845215 = 0.1141686663031578 + 2.0 * 6.03434944152832
Epoch 970, val loss: 0.7328163981437683
Epoch 980, training loss: 12.181052207946777 = 0.11007165163755417 + 2.0 * 6.0354905128479
Epoch 980, val loss: 0.7355984449386597
Epoch 990, training loss: 12.163725852966309 = 0.10616166144609451 + 2.0 * 6.028781890869141
Epoch 990, val loss: 0.7384765148162842
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.6458
Flip ASR: 0.6178/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.69210433959961 = 1.944533348083496 + 2.0 * 8.373785972595215
Epoch 0, val loss: 1.9473804235458374
Epoch 10, training loss: 18.680715560913086 = 1.9344806671142578 + 2.0 * 8.373117446899414
Epoch 10, val loss: 1.9372807741165161
Epoch 20, training loss: 18.659990310668945 = 1.922090768814087 + 2.0 * 8.368949890136719
Epoch 20, val loss: 1.924882411956787
Epoch 30, training loss: 18.590192794799805 = 1.9051156044006348 + 2.0 * 8.342538833618164
Epoch 30, val loss: 1.9081859588623047
Epoch 40, training loss: 18.2260684967041 = 1.8852707147598267 + 2.0 * 8.170398712158203
Epoch 40, val loss: 1.8894915580749512
Epoch 50, training loss: 16.995603561401367 = 1.863709807395935 + 2.0 * 7.565946578979492
Epoch 50, val loss: 1.8690024614334106
Epoch 60, training loss: 16.042213439941406 = 1.8477038145065308 + 2.0 * 7.097254753112793
Epoch 60, val loss: 1.855058193206787
Epoch 70, training loss: 15.257179260253906 = 1.8395438194274902 + 2.0 * 6.708817481994629
Epoch 70, val loss: 1.8474942445755005
Epoch 80, training loss: 14.952475547790527 = 1.832137107849121 + 2.0 * 6.560169219970703
Epoch 80, val loss: 1.8399267196655273
Epoch 90, training loss: 14.762214660644531 = 1.824260950088501 + 2.0 * 6.468976974487305
Epoch 90, val loss: 1.8313037157058716
Epoch 100, training loss: 14.627662658691406 = 1.8163514137268066 + 2.0 * 6.405655384063721
Epoch 100, val loss: 1.8227254152297974
Epoch 110, training loss: 14.519704818725586 = 1.8092303276062012 + 2.0 * 6.3552374839782715
Epoch 110, val loss: 1.814945936203003
Epoch 120, training loss: 14.432223320007324 = 1.8029381036758423 + 2.0 * 6.314642429351807
Epoch 120, val loss: 1.8079321384429932
Epoch 130, training loss: 14.360357284545898 = 1.7968676090240479 + 2.0 * 6.281744956970215
Epoch 130, val loss: 1.8012815713882446
Epoch 140, training loss: 14.304664611816406 = 1.7905904054641724 + 2.0 * 6.257037162780762
Epoch 140, val loss: 1.7945923805236816
Epoch 150, training loss: 14.256315231323242 = 1.7839659452438354 + 2.0 * 6.236174583435059
Epoch 150, val loss: 1.7877641916275024
Epoch 160, training loss: 14.22028923034668 = 1.776847004890442 + 2.0 * 6.221721172332764
Epoch 160, val loss: 1.7808332443237305
Epoch 170, training loss: 14.179644584655762 = 1.7691584825515747 + 2.0 * 6.205243110656738
Epoch 170, val loss: 1.7736245393753052
Epoch 180, training loss: 14.145041465759277 = 1.7607059478759766 + 2.0 * 6.19216775894165
Epoch 180, val loss: 1.7660468816757202
Epoch 190, training loss: 14.113668441772461 = 1.7512049674987793 + 2.0 * 6.18123197555542
Epoch 190, val loss: 1.757793664932251
Epoch 200, training loss: 14.084370613098145 = 1.7404464483261108 + 2.0 * 6.171962261199951
Epoch 200, val loss: 1.7487415075302124
Epoch 210, training loss: 14.053131103515625 = 1.728272795677185 + 2.0 * 6.162429332733154
Epoch 210, val loss: 1.7386858463287354
Epoch 220, training loss: 14.023195266723633 = 1.7142335176467896 + 2.0 * 6.154480934143066
Epoch 220, val loss: 1.727260708808899
Epoch 230, training loss: 13.991804122924805 = 1.6979045867919922 + 2.0 * 6.146949768066406
Epoch 230, val loss: 1.7140711545944214
Epoch 240, training loss: 13.960851669311523 = 1.6786552667617798 + 2.0 * 6.1410980224609375
Epoch 240, val loss: 1.6987197399139404
Epoch 250, training loss: 13.925521850585938 = 1.6563913822174072 + 2.0 * 6.134565353393555
Epoch 250, val loss: 1.6808547973632812
Epoch 260, training loss: 13.888284683227539 = 1.6303167343139648 + 2.0 * 6.128983974456787
Epoch 260, val loss: 1.6599769592285156
Epoch 270, training loss: 13.850358963012695 = 1.5998705625534058 + 2.0 * 6.125244140625
Epoch 270, val loss: 1.6355807781219482
Epoch 280, training loss: 13.807753562927246 = 1.5650111436843872 + 2.0 * 6.121371269226074
Epoch 280, val loss: 1.607534408569336
Epoch 290, training loss: 13.760082244873047 = 1.525836706161499 + 2.0 * 6.117122650146484
Epoch 290, val loss: 1.5760217905044556
Epoch 300, training loss: 13.707771301269531 = 1.4820435047149658 + 2.0 * 6.112864017486572
Epoch 300, val loss: 1.540547251701355
Epoch 310, training loss: 13.652748107910156 = 1.433493971824646 + 2.0 * 6.1096272468566895
Epoch 310, val loss: 1.5010132789611816
Epoch 320, training loss: 13.596019744873047 = 1.3817293643951416 + 2.0 * 6.107145309448242
Epoch 320, val loss: 1.4594545364379883
Epoch 330, training loss: 13.539773941040039 = 1.328994870185852 + 2.0 * 6.105389595031738
Epoch 330, val loss: 1.417220115661621
Epoch 340, training loss: 13.47904109954834 = 1.2757900953292847 + 2.0 * 6.101625442504883
Epoch 340, val loss: 1.374725580215454
Epoch 350, training loss: 13.42851448059082 = 1.2228105068206787 + 2.0 * 6.102851867675781
Epoch 350, val loss: 1.3326475620269775
Epoch 360, training loss: 13.364441871643066 = 1.1716376543045044 + 2.0 * 6.096402168273926
Epoch 360, val loss: 1.2926784753799438
Epoch 370, training loss: 13.310771942138672 = 1.1226435899734497 + 2.0 * 6.094064235687256
Epoch 370, val loss: 1.2546905279159546
Epoch 380, training loss: 13.263230323791504 = 1.0756741762161255 + 2.0 * 6.093778133392334
Epoch 380, val loss: 1.2185184955596924
Epoch 390, training loss: 13.21296501159668 = 1.0314046144485474 + 2.0 * 6.090780258178711
Epoch 390, val loss: 1.1846559047698975
Epoch 400, training loss: 13.163397789001465 = 0.9894735813140869 + 2.0 * 6.0869622230529785
Epoch 400, val loss: 1.1527491807937622
Epoch 410, training loss: 13.12003231048584 = 0.9493070840835571 + 2.0 * 6.085362434387207
Epoch 410, val loss: 1.1224616765975952
Epoch 420, training loss: 13.07382583618164 = 0.9108055830001831 + 2.0 * 6.081510066986084
Epoch 420, val loss: 1.0938377380371094
Epoch 430, training loss: 13.040077209472656 = 0.8739395141601562 + 2.0 * 6.08306884765625
Epoch 430, val loss: 1.0664949417114258
Epoch 440, training loss: 12.995361328125 = 0.8384326100349426 + 2.0 * 6.078464508056641
Epoch 440, val loss: 1.0407720804214478
Epoch 450, training loss: 12.955131530761719 = 0.804208517074585 + 2.0 * 6.075461387634277
Epoch 450, val loss: 1.0160855054855347
Epoch 460, training loss: 12.92654800415039 = 0.7708637714385986 + 2.0 * 6.0778422355651855
Epoch 460, val loss: 0.9924010038375854
Epoch 470, training loss: 12.8834228515625 = 0.7389885187149048 + 2.0 * 6.072216987609863
Epoch 470, val loss: 0.9698927402496338
Epoch 480, training loss: 12.847626686096191 = 0.7081195116043091 + 2.0 * 6.069753646850586
Epoch 480, val loss: 0.9485563039779663
Epoch 490, training loss: 12.82313346862793 = 0.678108274936676 + 2.0 * 6.072512626647949
Epoch 490, val loss: 0.9280222058296204
Epoch 500, training loss: 12.786097526550293 = 0.6489433646202087 + 2.0 * 6.068577289581299
Epoch 500, val loss: 0.9085623621940613
Epoch 510, training loss: 12.7511568069458 = 0.6207543611526489 + 2.0 * 6.065201282501221
Epoch 510, val loss: 0.8900037407875061
Epoch 520, training loss: 12.72150707244873 = 0.593157172203064 + 2.0 * 6.064175128936768
Epoch 520, val loss: 0.8721571564674377
Epoch 530, training loss: 12.697929382324219 = 0.5661624670028687 + 2.0 * 6.065883636474609
Epoch 530, val loss: 0.8550791144371033
Epoch 540, training loss: 12.662489891052246 = 0.5399686694145203 + 2.0 * 6.06126070022583
Epoch 540, val loss: 0.8388496041297913
Epoch 550, training loss: 12.632238388061523 = 0.5141953229904175 + 2.0 * 6.059021472930908
Epoch 550, val loss: 0.8232996463775635
Epoch 560, training loss: 12.606206893920898 = 0.4888089895248413 + 2.0 * 6.058699131011963
Epoch 560, val loss: 0.8082787394523621
Epoch 570, training loss: 12.57914924621582 = 0.46387889981269836 + 2.0 * 6.057635307312012
Epoch 570, val loss: 0.7939780354499817
Epoch 580, training loss: 12.550487518310547 = 0.43975070118904114 + 2.0 * 6.055368423461914
Epoch 580, val loss: 0.7803890705108643
Epoch 590, training loss: 12.524673461914062 = 0.41606196761131287 + 2.0 * 6.054305553436279
Epoch 590, val loss: 0.7674668431282043
Epoch 600, training loss: 12.500509262084961 = 0.3931034207344055 + 2.0 * 6.0537028312683105
Epoch 600, val loss: 0.7551262378692627
Epoch 610, training loss: 12.484393119812012 = 0.37112754583358765 + 2.0 * 6.056632995605469
Epoch 610, val loss: 0.7437268495559692
Epoch 620, training loss: 12.454496383666992 = 0.35016047954559326 + 2.0 * 6.052167892456055
Epoch 620, val loss: 0.733268678188324
Epoch 630, training loss: 12.429086685180664 = 0.33010348677635193 + 2.0 * 6.0494914054870605
Epoch 630, val loss: 0.7237160801887512
Epoch 640, training loss: 12.406574249267578 = 0.31096792221069336 + 2.0 * 6.0478034019470215
Epoch 640, val loss: 0.7149851322174072
Epoch 650, training loss: 12.412246704101562 = 0.2928023934364319 + 2.0 * 6.059721946716309
Epoch 650, val loss: 0.707065999507904
Epoch 660, training loss: 12.376018524169922 = 0.2759874165058136 + 2.0 * 6.050015449523926
Epoch 660, val loss: 0.7002062797546387
Epoch 670, training loss: 12.351487159729004 = 0.26017266511917114 + 2.0 * 6.045657157897949
Epoch 670, val loss: 0.6943464279174805
Epoch 680, training loss: 12.332866668701172 = 0.2452765703201294 + 2.0 * 6.043795108795166
Epoch 680, val loss: 0.6891204714775085
Epoch 690, training loss: 12.327046394348145 = 0.23132818937301636 + 2.0 * 6.047859191894531
Epoch 690, val loss: 0.6845805644989014
Epoch 700, training loss: 12.30656623840332 = 0.21850936114788055 + 2.0 * 6.044028282165527
Epoch 700, val loss: 0.6809407472610474
Epoch 710, training loss: 12.289542198181152 = 0.2065218836069107 + 2.0 * 6.041510105133057
Epoch 710, val loss: 0.6781153678894043
Epoch 720, training loss: 12.279284477233887 = 0.1952589750289917 + 2.0 * 6.042012691497803
Epoch 720, val loss: 0.6757545471191406
Epoch 730, training loss: 12.262269020080566 = 0.1847350001335144 + 2.0 * 6.038766860961914
Epoch 730, val loss: 0.6738846302032471
Epoch 740, training loss: 12.250185012817383 = 0.17490637302398682 + 2.0 * 6.037639141082764
Epoch 740, val loss: 0.6726941466331482
Epoch 750, training loss: 12.238927841186523 = 0.16567720472812653 + 2.0 * 6.036625385284424
Epoch 750, val loss: 0.6719295382499695
Epoch 760, training loss: 12.256514549255371 = 0.15701812505722046 + 2.0 * 6.049748420715332
Epoch 760, val loss: 0.6716001033782959
Epoch 770, training loss: 12.222355842590332 = 0.1490941345691681 + 2.0 * 6.036630630493164
Epoch 770, val loss: 0.6716479063034058
Epoch 780, training loss: 12.210567474365234 = 0.1417044848203659 + 2.0 * 6.034431457519531
Epoch 780, val loss: 0.6722903251647949
Epoch 790, training loss: 12.200825691223145 = 0.13474956154823303 + 2.0 * 6.033038139343262
Epoch 790, val loss: 0.6731751561164856
Epoch 800, training loss: 12.192872047424316 = 0.12820430099964142 + 2.0 * 6.032333850860596
Epoch 800, val loss: 0.6743053793907166
Epoch 810, training loss: 12.196274757385254 = 0.12206940352916718 + 2.0 * 6.037102699279785
Epoch 810, val loss: 0.6757860779762268
Epoch 820, training loss: 12.18880558013916 = 0.11635421216487885 + 2.0 * 6.03622579574585
Epoch 820, val loss: 0.6772810220718384
Epoch 830, training loss: 12.174251556396484 = 0.11107123643159866 + 2.0 * 6.031589984893799
Epoch 830, val loss: 0.6792427897453308
Epoch 840, training loss: 12.16667652130127 = 0.10609900951385498 + 2.0 * 6.0302886962890625
Epoch 840, val loss: 0.6813545227050781
Epoch 850, training loss: 12.162686347961426 = 0.10141604393720627 + 2.0 * 6.030635356903076
Epoch 850, val loss: 0.683556079864502
Epoch 860, training loss: 12.152605056762695 = 0.09700214117765427 + 2.0 * 6.027801513671875
Epoch 860, val loss: 0.6860018968582153
Epoch 870, training loss: 12.156753540039062 = 0.09284376353025436 + 2.0 * 6.031954765319824
Epoch 870, val loss: 0.6886478066444397
Epoch 880, training loss: 12.152113914489746 = 0.08897089958190918 + 2.0 * 6.031571388244629
Epoch 880, val loss: 0.6912946105003357
Epoch 890, training loss: 12.13721752166748 = 0.08532661199569702 + 2.0 * 6.025945663452148
Epoch 890, val loss: 0.6942170262336731
Epoch 900, training loss: 12.132399559020996 = 0.0818786695599556 + 2.0 * 6.0252604484558105
Epoch 900, val loss: 0.6972178220748901
Epoch 910, training loss: 12.127718925476074 = 0.07862143218517303 + 2.0 * 6.024548530578613
Epoch 910, val loss: 0.7003253102302551
Epoch 920, training loss: 12.142759323120117 = 0.07552522420883179 + 2.0 * 6.03361701965332
Epoch 920, val loss: 0.70347660779953
Epoch 930, training loss: 12.121624946594238 = 0.07263960689306259 + 2.0 * 6.0244927406311035
Epoch 930, val loss: 0.7066436409950256
Epoch 940, training loss: 12.116608619689941 = 0.06993202865123749 + 2.0 * 6.023338317871094
Epoch 940, val loss: 0.7100388407707214
Epoch 950, training loss: 12.112088203430176 = 0.06734070181846619 + 2.0 * 6.022373676300049
Epoch 950, val loss: 0.7133848667144775
Epoch 960, training loss: 12.108116149902344 = 0.06487570703029633 + 2.0 * 6.021620273590088
Epoch 960, val loss: 0.7168378829956055
Epoch 970, training loss: 12.121213912963867 = 0.0625324696302414 + 2.0 * 6.029340744018555
Epoch 970, val loss: 0.7202776670455933
Epoch 980, training loss: 12.110332489013672 = 0.060362812131643295 + 2.0 * 6.024984836578369
Epoch 980, val loss: 0.7238437533378601
Epoch 990, training loss: 12.097518920898438 = 0.05827530100941658 + 2.0 * 6.019621849060059
Epoch 990, val loss: 0.7274117469787598
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.8339
Flip ASR: 0.8000/225 nodes
The final ASR:0.70111, 0.09437, Accuracy:0.81235, 0.02188
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11654])
remove edge: torch.Size([2, 9502])
updated graph: torch.Size([2, 10600])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9889
Flip ASR: 0.9867/225 nodes
The final ASR:0.98155, 0.00522, Accuracy:0.83210, 0.00698
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.3 der2:0.4 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 18.706832885742188 = 1.9592492580413818 + 2.0 * 8.373791694641113
Epoch 0, val loss: 1.9631279706954956
Epoch 10, training loss: 18.69533348083496 = 1.9491324424743652 + 2.0 * 8.373100280761719
Epoch 10, val loss: 1.953943133354187
Epoch 20, training loss: 18.674596786499023 = 1.936327338218689 + 2.0 * 8.369134902954102
Epoch 20, val loss: 1.9420158863067627
Epoch 30, training loss: 18.603008270263672 = 1.9187233448028564 + 2.0 * 8.342142105102539
Epoch 30, val loss: 1.9255809783935547
Epoch 40, training loss: 18.153274536132812 = 1.898099660873413 + 2.0 * 8.12758731842041
Epoch 40, val loss: 1.906976580619812
Epoch 50, training loss: 16.599599838256836 = 1.8738300800323486 + 2.0 * 7.362884521484375
Epoch 50, val loss: 1.8849834203720093
Epoch 60, training loss: 15.986834526062012 = 1.8552881479263306 + 2.0 * 7.065773010253906
Epoch 60, val loss: 1.8684457540512085
Epoch 70, training loss: 15.512751579284668 = 1.842281460762024 + 2.0 * 6.835235118865967
Epoch 70, val loss: 1.8562109470367432
Epoch 80, training loss: 15.128662109375 = 1.8304638862609863 + 2.0 * 6.649098873138428
Epoch 80, val loss: 1.8452428579330444
Epoch 90, training loss: 14.901636123657227 = 1.8197327852249146 + 2.0 * 6.540951728820801
Epoch 90, val loss: 1.8349539041519165
Epoch 100, training loss: 14.753536224365234 = 1.808950424194336 + 2.0 * 6.472292900085449
Epoch 100, val loss: 1.824483871459961
Epoch 110, training loss: 14.646061897277832 = 1.798913836479187 + 2.0 * 6.423573970794678
Epoch 110, val loss: 1.8141945600509644
Epoch 120, training loss: 14.538841247558594 = 1.7903695106506348 + 2.0 * 6.3742356300354
Epoch 120, val loss: 1.804908275604248
Epoch 130, training loss: 14.44524097442627 = 1.7832237482070923 + 2.0 * 6.331008434295654
Epoch 130, val loss: 1.7968568801879883
Epoch 140, training loss: 14.370471000671387 = 1.7762118577957153 + 2.0 * 6.2971296310424805
Epoch 140, val loss: 1.789066195487976
Epoch 150, training loss: 14.302680015563965 = 1.7684303522109985 + 2.0 * 6.267124652862549
Epoch 150, val loss: 1.7811110019683838
Epoch 160, training loss: 14.24608039855957 = 1.7597737312316895 + 2.0 * 6.2431535720825195
Epoch 160, val loss: 1.7728298902511597
Epoch 170, training loss: 14.19692325592041 = 1.749955177307129 + 2.0 * 6.223484039306641
Epoch 170, val loss: 1.7640173435211182
Epoch 180, training loss: 14.151872634887695 = 1.7387568950653076 + 2.0 * 6.206557750701904
Epoch 180, val loss: 1.7543352842330933
Epoch 190, training loss: 14.10914421081543 = 1.7256935834884644 + 2.0 * 6.191725254058838
Epoch 190, val loss: 1.7432057857513428
Epoch 200, training loss: 14.07455062866211 = 1.710289478302002 + 2.0 * 6.182130813598633
Epoch 200, val loss: 1.7302441596984863
Epoch 210, training loss: 14.029834747314453 = 1.6924389600753784 + 2.0 * 6.168697834014893
Epoch 210, val loss: 1.715517520904541
Epoch 220, training loss: 13.989035606384277 = 1.671636700630188 + 2.0 * 6.1586995124816895
Epoch 220, val loss: 1.6983572244644165
Epoch 230, training loss: 13.947649002075195 = 1.6472171545028687 + 2.0 * 6.150216102600098
Epoch 230, val loss: 1.6781643629074097
Epoch 240, training loss: 13.905672073364258 = 1.6186022758483887 + 2.0 * 6.143535137176514
Epoch 240, val loss: 1.6544245481491089
Epoch 250, training loss: 13.858057022094727 = 1.585514783859253 + 2.0 * 6.136270999908447
Epoch 250, val loss: 1.6268737316131592
Epoch 260, training loss: 13.80979061126709 = 1.5479403734207153 + 2.0 * 6.130925178527832
Epoch 260, val loss: 1.5955926179885864
Epoch 270, training loss: 13.756997108459473 = 1.5063217878341675 + 2.0 * 6.125337600708008
Epoch 270, val loss: 1.5610215663909912
Epoch 280, training loss: 13.703414916992188 = 1.4609779119491577 + 2.0 * 6.121218681335449
Epoch 280, val loss: 1.5233399868011475
Epoch 290, training loss: 13.647926330566406 = 1.4132428169250488 + 2.0 * 6.117341995239258
Epoch 290, val loss: 1.4836879968643188
Epoch 300, training loss: 13.588629722595215 = 1.3641328811645508 + 2.0 * 6.112248420715332
Epoch 300, val loss: 1.4431992769241333
Epoch 310, training loss: 13.532331466674805 = 1.3150562047958374 + 2.0 * 6.108637809753418
Epoch 310, val loss: 1.4032340049743652
Epoch 320, training loss: 13.477468490600586 = 1.2670674324035645 + 2.0 * 6.10520076751709
Epoch 320, val loss: 1.3643732070922852
Epoch 330, training loss: 13.424054145812988 = 1.2208576202392578 + 2.0 * 6.101598262786865
Epoch 330, val loss: 1.3275401592254639
Epoch 340, training loss: 13.372722625732422 = 1.1767656803131104 + 2.0 * 6.097978591918945
Epoch 340, val loss: 1.2927753925323486
Epoch 350, training loss: 13.327290534973145 = 1.1348363161087036 + 2.0 * 6.096227169036865
Epoch 350, val loss: 1.260276198387146
Epoch 360, training loss: 13.278297424316406 = 1.0954391956329346 + 2.0 * 6.091429233551025
Epoch 360, val loss: 1.230102300643921
Epoch 370, training loss: 13.237207412719727 = 1.0582809448242188 + 2.0 * 6.089463233947754
Epoch 370, val loss: 1.2021607160568237
Epoch 380, training loss: 13.194567680358887 = 1.0232429504394531 + 2.0 * 6.085662364959717
Epoch 380, val loss: 1.176118016242981
Epoch 390, training loss: 13.155909538269043 = 0.9900397658348083 + 2.0 * 6.082934856414795
Epoch 390, val loss: 1.151855230331421
Epoch 400, training loss: 13.11886215209961 = 0.958293080329895 + 2.0 * 6.080284595489502
Epoch 400, val loss: 1.1289396286010742
Epoch 410, training loss: 13.090561866760254 = 0.9278687834739685 + 2.0 * 6.08134651184082
Epoch 410, val loss: 1.1072959899902344
Epoch 420, training loss: 13.052017211914062 = 0.898697018623352 + 2.0 * 6.07666015625
Epoch 420, val loss: 1.0867435932159424
Epoch 430, training loss: 13.022401809692383 = 0.870158851146698 + 2.0 * 6.0761213302612305
Epoch 430, val loss: 1.0666569471359253
Epoch 440, training loss: 12.98602294921875 = 0.8420710563659668 + 2.0 * 6.0719757080078125
Epoch 440, val loss: 1.0470291376113892
Epoch 450, training loss: 12.952783584594727 = 0.8142585158348083 + 2.0 * 6.069262504577637
Epoch 450, val loss: 1.027631163597107
Epoch 460, training loss: 12.9218168258667 = 0.7864426970481873 + 2.0 * 6.067687034606934
Epoch 460, val loss: 1.008201003074646
Epoch 470, training loss: 12.896702766418457 = 0.7586635947227478 + 2.0 * 6.069019794464111
Epoch 470, val loss: 0.9886569380760193
Epoch 480, training loss: 12.862122535705566 = 0.7312974333763123 + 2.0 * 6.065412521362305
Epoch 480, val loss: 0.9693633913993835
Epoch 490, training loss: 12.830472946166992 = 0.7041884660720825 + 2.0 * 6.0631422996521
Epoch 490, val loss: 0.950080931186676
Epoch 500, training loss: 12.800761222839355 = 0.6772959232330322 + 2.0 * 6.061732769012451
Epoch 500, val loss: 0.9309645891189575
Epoch 510, training loss: 12.779489517211914 = 0.6509093046188354 + 2.0 * 6.0642900466918945
Epoch 510, val loss: 0.912165641784668
Epoch 520, training loss: 12.742454528808594 = 0.6254749894142151 + 2.0 * 6.058489799499512
Epoch 520, val loss: 0.8941303491592407
Epoch 530, training loss: 12.714254379272461 = 0.6008142232894897 + 2.0 * 6.05672025680542
Epoch 530, val loss: 0.8767438530921936
Epoch 540, training loss: 12.687005996704102 = 0.5768831372261047 + 2.0 * 6.055061340332031
Epoch 540, val loss: 0.8600883483886719
Epoch 550, training loss: 12.680292129516602 = 0.553747296333313 + 2.0 * 6.063272476196289
Epoch 550, val loss: 0.8442491888999939
Epoch 560, training loss: 12.637158393859863 = 0.5318284630775452 + 2.0 * 6.052664756774902
Epoch 560, val loss: 0.8295283317565918
Epoch 570, training loss: 12.612720489501953 = 0.5108461380004883 + 2.0 * 6.050937175750732
Epoch 570, val loss: 0.8158265948295593
Epoch 580, training loss: 12.591155052185059 = 0.4906271994113922 + 2.0 * 6.05026388168335
Epoch 580, val loss: 0.802959144115448
Epoch 590, training loss: 12.569936752319336 = 0.47119569778442383 + 2.0 * 6.049370765686035
Epoch 590, val loss: 0.7909781336784363
Epoch 600, training loss: 12.548129081726074 = 0.4525054097175598 + 2.0 * 6.047811985015869
Epoch 600, val loss: 0.7799493074417114
Epoch 610, training loss: 12.527946472167969 = 0.4344150125980377 + 2.0 * 6.0467658042907715
Epoch 610, val loss: 0.7696382999420166
Epoch 620, training loss: 12.510296821594238 = 0.4168444573879242 + 2.0 * 6.046726226806641
Epoch 620, val loss: 0.7599689960479736
Epoch 630, training loss: 12.497509002685547 = 0.3998546600341797 + 2.0 * 6.048827171325684
Epoch 630, val loss: 0.7509730458259583
Epoch 640, training loss: 12.471745491027832 = 0.38335487246513367 + 2.0 * 6.044195175170898
Epoch 640, val loss: 0.7427476644515991
Epoch 650, training loss: 12.450055122375488 = 0.36730319261550903 + 2.0 * 6.041376113891602
Epoch 650, val loss: 0.7349438667297363
Epoch 660, training loss: 12.434187889099121 = 0.3515874445438385 + 2.0 * 6.041300296783447
Epoch 660, val loss: 0.7276214957237244
Epoch 670, training loss: 12.424505233764648 = 0.3363507390022278 + 2.0 * 6.044077396392822
Epoch 670, val loss: 0.7207390666007996
Epoch 680, training loss: 12.400777816772461 = 0.3216507136821747 + 2.0 * 6.0395636558532715
Epoch 680, val loss: 0.7145582437515259
Epoch 690, training loss: 12.382059097290039 = 0.3073507249355316 + 2.0 * 6.037353992462158
Epoch 690, val loss: 0.7087456583976746
Epoch 700, training loss: 12.372272491455078 = 0.2934321165084839 + 2.0 * 6.039420127868652
Epoch 700, val loss: 0.7034310102462769
Epoch 710, training loss: 12.367443084716797 = 0.28008031845092773 + 2.0 * 6.043681621551514
Epoch 710, val loss: 0.6986678838729858
Epoch 720, training loss: 12.339476585388184 = 0.2673070728778839 + 2.0 * 6.0360846519470215
Epoch 720, val loss: 0.6944658756256104
Epoch 730, training loss: 12.321889877319336 = 0.2550104260444641 + 2.0 * 6.033439636230469
Epoch 730, val loss: 0.6907193660736084
Epoch 740, training loss: 12.3092622756958 = 0.2431553155183792 + 2.0 * 6.033053398132324
Epoch 740, val loss: 0.6874216198921204
Epoch 750, training loss: 12.30395793914795 = 0.2317964881658554 + 2.0 * 6.036080837249756
Epoch 750, val loss: 0.6846082806587219
Epoch 760, training loss: 12.284071922302246 = 0.22103150188922882 + 2.0 * 6.031520366668701
Epoch 760, val loss: 0.6823925375938416
Epoch 770, training loss: 12.272411346435547 = 0.21074485778808594 + 2.0 * 6.0308332443237305
Epoch 770, val loss: 0.6806135773658752
Epoch 780, training loss: 12.258627891540527 = 0.20088715851306915 + 2.0 * 6.028870582580566
Epoch 780, val loss: 0.6791820526123047
Epoch 790, training loss: 12.25359058380127 = 0.19146302342414856 + 2.0 * 6.031063556671143
Epoch 790, val loss: 0.6782217621803284
Epoch 800, training loss: 12.244739532470703 = 0.18254055082798004 + 2.0 * 6.031099319458008
Epoch 800, val loss: 0.677581250667572
Epoch 810, training loss: 12.233437538146973 = 0.17409899830818176 + 2.0 * 6.029669284820557
Epoch 810, val loss: 0.6775104999542236
Epoch 820, training loss: 12.22201156616211 = 0.1660904884338379 + 2.0 * 6.027960300445557
Epoch 820, val loss: 0.6776718497276306
Epoch 830, training loss: 12.209894180297852 = 0.15849077701568604 + 2.0 * 6.025701522827148
Epoch 830, val loss: 0.6782006621360779
Epoch 840, training loss: 12.202038764953613 = 0.1512525975704193 + 2.0 * 6.025393009185791
Epoch 840, val loss: 0.6789838671684265
Epoch 850, training loss: 12.196182250976562 = 0.14438080787658691 + 2.0 * 6.025900840759277
Epoch 850, val loss: 0.6800622344017029
Epoch 860, training loss: 12.18981647491455 = 0.13789331912994385 + 2.0 * 6.025961399078369
Epoch 860, val loss: 0.6814630031585693
Epoch 870, training loss: 12.184043884277344 = 0.13178977370262146 + 2.0 * 6.026126861572266
Epoch 870, val loss: 0.6830016374588013
Epoch 880, training loss: 12.179932594299316 = 0.12602154910564423 + 2.0 * 6.026955604553223
Epoch 880, val loss: 0.6848073601722717
Epoch 890, training loss: 12.16584587097168 = 0.12058165669441223 + 2.0 * 6.022632122039795
Epoch 890, val loss: 0.6867454648017883
Epoch 900, training loss: 12.156867980957031 = 0.11543700098991394 + 2.0 * 6.020715713500977
Epoch 900, val loss: 0.6889910697937012
Epoch 910, training loss: 12.150960922241211 = 0.11054534465074539 + 2.0 * 6.02020788192749
Epoch 910, val loss: 0.6913180351257324
Epoch 920, training loss: 12.157564163208008 = 0.10590323805809021 + 2.0 * 6.025830268859863
Epoch 920, val loss: 0.6938100457191467
Epoch 930, training loss: 12.141988754272461 = 0.10151892155408859 + 2.0 * 6.020235061645508
Epoch 930, val loss: 0.6964545845985413
Epoch 940, training loss: 12.13896369934082 = 0.09737151116132736 + 2.0 * 6.020796298980713
Epoch 940, val loss: 0.6992179751396179
Epoch 950, training loss: 12.133841514587402 = 0.09346592426300049 + 2.0 * 6.020187854766846
Epoch 950, val loss: 0.7019928693771362
Epoch 960, training loss: 12.125107765197754 = 0.08974845707416534 + 2.0 * 6.017679691314697
Epoch 960, val loss: 0.7050280570983887
Epoch 970, training loss: 12.119094848632812 = 0.08622656017541885 + 2.0 * 6.016434192657471
Epoch 970, val loss: 0.7080461978912354
Epoch 980, training loss: 12.118895530700684 = 0.08287092298269272 + 2.0 * 6.018012523651123
Epoch 980, val loss: 0.711105227470398
Epoch 990, training loss: 12.115571975708008 = 0.07970035821199417 + 2.0 * 6.017935752868652
Epoch 990, val loss: 0.7142726182937622
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.6236
Flip ASR: 0.5511/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.68150520324707 = 1.9342176914215088 + 2.0 * 8.37364387512207
Epoch 0, val loss: 1.9381372928619385
Epoch 10, training loss: 18.666645050048828 = 1.924818754196167 + 2.0 * 8.3709135055542
Epoch 10, val loss: 1.927626132965088
Epoch 20, training loss: 18.635793685913086 = 1.9131622314453125 + 2.0 * 8.361315727233887
Epoch 20, val loss: 1.9138798713684082
Epoch 30, training loss: 18.5068359375 = 1.8980234861373901 + 2.0 * 8.30440616607666
Epoch 30, val loss: 1.8956447839736938
Epoch 40, training loss: 17.741504669189453 = 1.8819166421890259 + 2.0 * 7.929793834686279
Epoch 40, val loss: 1.8755472898483276
Epoch 50, training loss: 16.500083923339844 = 1.8653669357299805 + 2.0 * 7.31735897064209
Epoch 50, val loss: 1.8555041551589966
Epoch 60, training loss: 15.59485912322998 = 1.852400779724121 + 2.0 * 6.87122917175293
Epoch 60, val loss: 1.8409565687179565
Epoch 70, training loss: 15.166059494018555 = 1.8423744440078735 + 2.0 * 6.661842346191406
Epoch 70, val loss: 1.830003261566162
Epoch 80, training loss: 14.91463851928711 = 1.8344252109527588 + 2.0 * 6.540106773376465
Epoch 80, val loss: 1.8208452463150024
Epoch 90, training loss: 14.699902534484863 = 1.825923204421997 + 2.0 * 6.436989784240723
Epoch 90, val loss: 1.8112807273864746
Epoch 100, training loss: 14.565164566040039 = 1.8178210258483887 + 2.0 * 6.373672008514404
Epoch 100, val loss: 1.8018289804458618
Epoch 110, training loss: 14.466880798339844 = 1.8100587129592896 + 2.0 * 6.328411102294922
Epoch 110, val loss: 1.792694330215454
Epoch 120, training loss: 14.381143569946289 = 1.802946925163269 + 2.0 * 6.289098262786865
Epoch 120, val loss: 1.784226655960083
Epoch 130, training loss: 14.314708709716797 = 1.7965055704116821 + 2.0 * 6.259101390838623
Epoch 130, val loss: 1.7766534090042114
Epoch 140, training loss: 14.257942199707031 = 1.7904359102249146 + 2.0 * 6.233753204345703
Epoch 140, val loss: 1.7696330547332764
Epoch 150, training loss: 14.210996627807617 = 1.7844557762145996 + 2.0 * 6.213270664215088
Epoch 150, val loss: 1.763008713722229
Epoch 160, training loss: 14.170047760009766 = 1.7783602476119995 + 2.0 * 6.195843696594238
Epoch 160, val loss: 1.7566275596618652
Epoch 170, training loss: 14.139814376831055 = 1.7718950510025024 + 2.0 * 6.183959484100342
Epoch 170, val loss: 1.7502541542053223
Epoch 180, training loss: 14.10501480102539 = 1.76487398147583 + 2.0 * 6.170070171356201
Epoch 180, val loss: 1.7438488006591797
Epoch 190, training loss: 14.075826644897461 = 1.7572418451309204 + 2.0 * 6.159292221069336
Epoch 190, val loss: 1.7371817827224731
Epoch 200, training loss: 14.048933982849121 = 1.748706579208374 + 2.0 * 6.150113582611084
Epoch 200, val loss: 1.7300833463668823
Epoch 210, training loss: 14.02632999420166 = 1.7390377521514893 + 2.0 * 6.143646240234375
Epoch 210, val loss: 1.7222516536712646
Epoch 220, training loss: 13.998486518859863 = 1.7279647588729858 + 2.0 * 6.135261058807373
Epoch 220, val loss: 1.713485598564148
Epoch 230, training loss: 13.972650527954102 = 1.7151713371276855 + 2.0 * 6.128739356994629
Epoch 230, val loss: 1.7036017179489136
Epoch 240, training loss: 13.944317817687988 = 1.7004249095916748 + 2.0 * 6.121946334838867
Epoch 240, val loss: 1.6921316385269165
Epoch 250, training loss: 13.916457176208496 = 1.6830976009368896 + 2.0 * 6.116679668426514
Epoch 250, val loss: 1.6787253618240356
Epoch 260, training loss: 13.889918327331543 = 1.6626050472259521 + 2.0 * 6.113656520843506
Epoch 260, val loss: 1.6627929210662842
Epoch 270, training loss: 13.85649299621582 = 1.6384508609771729 + 2.0 * 6.109021186828613
Epoch 270, val loss: 1.6440287828445435
Epoch 280, training loss: 13.819740295410156 = 1.6103060245513916 + 2.0 * 6.104717254638672
Epoch 280, val loss: 1.6219987869262695
Epoch 290, training loss: 13.777971267700195 = 1.5776997804641724 + 2.0 * 6.100135803222656
Epoch 290, val loss: 1.5962055921554565
Epoch 300, training loss: 13.735086441040039 = 1.540086030960083 + 2.0 * 6.097500324249268
Epoch 300, val loss: 1.5663566589355469
Epoch 310, training loss: 13.686605453491211 = 1.4977097511291504 + 2.0 * 6.094447612762451
Epoch 310, val loss: 1.5328449010849
Epoch 320, training loss: 13.637182235717773 = 1.4517312049865723 + 2.0 * 6.09272575378418
Epoch 320, val loss: 1.4964078664779663
Epoch 330, training loss: 13.581038475036621 = 1.403254747390747 + 2.0 * 6.088891983032227
Epoch 330, val loss: 1.458383321762085
Epoch 340, training loss: 13.531341552734375 = 1.3538748025894165 + 2.0 * 6.088733196258545
Epoch 340, val loss: 1.4200525283813477
Epoch 350, training loss: 13.479347229003906 = 1.30564546585083 + 2.0 * 6.086851119995117
Epoch 350, val loss: 1.3826783895492554
Epoch 360, training loss: 13.425268173217773 = 1.2592099905014038 + 2.0 * 6.083029270172119
Epoch 360, val loss: 1.347535252571106
Epoch 370, training loss: 13.37874698638916 = 1.2150297164916992 + 2.0 * 6.0818586349487305
Epoch 370, val loss: 1.3145451545715332
Epoch 380, training loss: 13.329800605773926 = 1.1731077432632446 + 2.0 * 6.078346252441406
Epoch 380, val loss: 1.283657431602478
Epoch 390, training loss: 13.287429809570312 = 1.1326817274093628 + 2.0 * 6.07737398147583
Epoch 390, val loss: 1.2542822360992432
Epoch 400, training loss: 13.240860939025879 = 1.093471884727478 + 2.0 * 6.073694705963135
Epoch 400, val loss: 1.2257856130599976
Epoch 410, training loss: 13.198395729064941 = 1.0546703338623047 + 2.0 * 6.071862697601318
Epoch 410, val loss: 1.1979589462280273
Epoch 420, training loss: 13.165216445922852 = 1.016002893447876 + 2.0 * 6.074606895446777
Epoch 420, val loss: 1.1700820922851562
Epoch 430, training loss: 13.117172241210938 = 0.977555513381958 + 2.0 * 6.069808483123779
Epoch 430, val loss: 1.1423503160476685
Epoch 440, training loss: 13.072454452514648 = 0.9388874769210815 + 2.0 * 6.066783428192139
Epoch 440, val loss: 1.1141847372055054
Epoch 450, training loss: 13.03264045715332 = 0.8999883532524109 + 2.0 * 6.066326141357422
Epoch 450, val loss: 1.08578360080719
Epoch 460, training loss: 12.987992286682129 = 0.8612890839576721 + 2.0 * 6.063351631164551
Epoch 460, val loss: 1.0574580430984497
Epoch 470, training loss: 12.947115898132324 = 0.8230310082435608 + 2.0 * 6.062042236328125
Epoch 470, val loss: 1.0294450521469116
Epoch 480, training loss: 12.910480499267578 = 0.785443127155304 + 2.0 * 6.06251859664917
Epoch 480, val loss: 1.001891016960144
Epoch 490, training loss: 12.87564468383789 = 0.7490507364273071 + 2.0 * 6.063296794891357
Epoch 490, val loss: 0.9751756191253662
Epoch 500, training loss: 12.830002784729004 = 0.7141508460044861 + 2.0 * 6.057926177978516
Epoch 500, val loss: 0.9499222636222839
Epoch 510, training loss: 12.792444229125977 = 0.6804051399230957 + 2.0 * 6.0560197830200195
Epoch 510, val loss: 0.9257683157920837
Epoch 520, training loss: 12.757928848266602 = 0.6477551460266113 + 2.0 * 6.055086612701416
Epoch 520, val loss: 0.9028251767158508
Epoch 530, training loss: 12.725931167602539 = 0.6163097620010376 + 2.0 * 6.054810523986816
Epoch 530, val loss: 0.8810944557189941
Epoch 540, training loss: 12.695011138916016 = 0.5863112211227417 + 2.0 * 6.054349899291992
Epoch 540, val loss: 0.8608620762825012
Epoch 550, training loss: 12.65938663482666 = 0.5575200319290161 + 2.0 * 6.050933361053467
Epoch 550, val loss: 0.8420612215995789
Epoch 560, training loss: 12.628636360168457 = 0.5296346545219421 + 2.0 * 6.049500942230225
Epoch 560, val loss: 0.8243157863616943
Epoch 570, training loss: 12.60119342803955 = 0.5026296973228455 + 2.0 * 6.049282073974609
Epoch 570, val loss: 0.8077911734580994
Epoch 580, training loss: 12.570446968078613 = 0.4766176640987396 + 2.0 * 6.046914577484131
Epoch 580, val loss: 0.7925309538841248
Epoch 590, training loss: 12.547694206237793 = 0.4515947997570038 + 2.0 * 6.0480499267578125
Epoch 590, val loss: 0.7784357070922852
Epoch 600, training loss: 12.516739845275879 = 0.42784950137138367 + 2.0 * 6.044445037841797
Epoch 600, val loss: 0.7656256556510925
Epoch 610, training loss: 12.492953300476074 = 0.4051738381385803 + 2.0 * 6.04388952255249
Epoch 610, val loss: 0.754236102104187
Epoch 620, training loss: 12.475279808044434 = 0.38357654213905334 + 2.0 * 6.045851707458496
Epoch 620, val loss: 0.7440897226333618
Epoch 630, training loss: 12.453280448913574 = 0.3632701337337494 + 2.0 * 6.0450053215026855
Epoch 630, val loss: 0.735041618347168
Epoch 640, training loss: 12.423002243041992 = 0.3441500663757324 + 2.0 * 6.039425849914551
Epoch 640, val loss: 0.7274590730667114
Epoch 650, training loss: 12.404947280883789 = 0.32608509063720703 + 2.0 * 6.039431095123291
Epoch 650, val loss: 0.7208770513534546
Epoch 660, training loss: 12.387784957885742 = 0.30909451842308044 + 2.0 * 6.0393452644348145
Epoch 660, val loss: 0.7152169346809387
Epoch 670, training loss: 12.370813369750977 = 0.29317522048950195 + 2.0 * 6.038819313049316
Epoch 670, val loss: 0.710595965385437
Epoch 680, training loss: 12.351149559020996 = 0.2783378064632416 + 2.0 * 6.03640604019165
Epoch 680, val loss: 0.7068456411361694
Epoch 690, training loss: 12.333539962768555 = 0.2643345892429352 + 2.0 * 6.034602642059326
Epoch 690, val loss: 0.7037720680236816
Epoch 700, training loss: 12.33207893371582 = 0.2511667013168335 + 2.0 * 6.040456295013428
Epoch 700, val loss: 0.7013256549835205
Epoch 710, training loss: 12.3074369430542 = 0.23876020312309265 + 2.0 * 6.034338474273682
Epoch 710, val loss: 0.6996083855628967
Epoch 720, training loss: 12.294228553771973 = 0.22709234058856964 + 2.0 * 6.033567905426025
Epoch 720, val loss: 0.698368489742279
Epoch 730, training loss: 12.279675483703613 = 0.21607308089733124 + 2.0 * 6.031801223754883
Epoch 730, val loss: 0.697496771812439
Epoch 740, training loss: 12.273292541503906 = 0.20572037994861603 + 2.0 * 6.033786296844482
Epoch 740, val loss: 0.6972000002861023
Epoch 750, training loss: 12.253527641296387 = 0.19600124657154083 + 2.0 * 6.028763294219971
Epoch 750, val loss: 0.6973909139633179
Epoch 760, training loss: 12.244830131530762 = 0.18677683174610138 + 2.0 * 6.029026508331299
Epoch 760, val loss: 0.6979390978813171
Epoch 770, training loss: 12.24240493774414 = 0.1780327558517456 + 2.0 * 6.032186031341553
Epoch 770, val loss: 0.6987189054489136
Epoch 780, training loss: 12.22464656829834 = 0.16980957984924316 + 2.0 * 6.027418613433838
Epoch 780, val loss: 0.6999314427375793
Epoch 790, training loss: 12.215158462524414 = 0.16206932067871094 + 2.0 * 6.026544570922852
Epoch 790, val loss: 0.7014883160591125
Epoch 800, training loss: 12.207746505737305 = 0.15473192930221558 + 2.0 * 6.026507377624512
Epoch 800, val loss: 0.703224241733551
Epoch 810, training loss: 12.201725959777832 = 0.14781907200813293 + 2.0 * 6.026953220367432
Epoch 810, val loss: 0.7052842974662781
Epoch 820, training loss: 12.191117286682129 = 0.1413024216890335 + 2.0 * 6.02490758895874
Epoch 820, val loss: 0.7076128721237183
Epoch 830, training loss: 12.181328773498535 = 0.135110005736351 + 2.0 * 6.023109436035156
Epoch 830, val loss: 0.7101070284843445
Epoch 840, training loss: 12.183479309082031 = 0.12924480438232422 + 2.0 * 6.0271172523498535
Epoch 840, val loss: 0.7128468155860901
Epoch 850, training loss: 12.173541069030762 = 0.12374352663755417 + 2.0 * 6.024899005889893
Epoch 850, val loss: 0.7158374190330505
Epoch 860, training loss: 12.164324760437012 = 0.11856263875961304 + 2.0 * 6.022881031036377
Epoch 860, val loss: 0.7189613580703735
Epoch 870, training loss: 12.155073165893555 = 0.11369642615318298 + 2.0 * 6.020688533782959
Epoch 870, val loss: 0.7222622036933899
Epoch 880, training loss: 12.149545669555664 = 0.10908083617687225 + 2.0 * 6.020232200622559
Epoch 880, val loss: 0.7257533669471741
Epoch 890, training loss: 12.148098945617676 = 0.10470841825008392 + 2.0 * 6.021695137023926
Epoch 890, val loss: 0.7293433547019958
Epoch 900, training loss: 12.148154258728027 = 0.10057160258293152 + 2.0 * 6.023791313171387
Epoch 900, val loss: 0.7330867052078247
Epoch 910, training loss: 12.141797065734863 = 0.09667850285768509 + 2.0 * 6.02255916595459
Epoch 910, val loss: 0.7368234992027283
Epoch 920, training loss: 12.125813484191895 = 0.09302281588315964 + 2.0 * 6.016395568847656
Epoch 920, val loss: 0.7407543063163757
Epoch 930, training loss: 12.122915267944336 = 0.08955048024654388 + 2.0 * 6.0166826248168945
Epoch 930, val loss: 0.74472576379776
Epoch 940, training loss: 12.124367713928223 = 0.0862387865781784 + 2.0 * 6.019064426422119
Epoch 940, val loss: 0.7487069964408875
Epoch 950, training loss: 12.115618705749512 = 0.08309105038642883 + 2.0 * 6.016263961791992
Epoch 950, val loss: 0.7527781128883362
Epoch 960, training loss: 12.111672401428223 = 0.08013475686311722 + 2.0 * 6.015769004821777
Epoch 960, val loss: 0.7569248080253601
Epoch 970, training loss: 12.107380867004395 = 0.07730701565742493 + 2.0 * 6.0150370597839355
Epoch 970, val loss: 0.7611851096153259
Epoch 980, training loss: 12.11007308959961 = 0.07462155073881149 + 2.0 * 6.017725944519043
Epoch 980, val loss: 0.765364944934845
Epoch 990, training loss: 12.105888366699219 = 0.07208063453435898 + 2.0 * 6.016903877258301
Epoch 990, val loss: 0.7696542739868164
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.7011
Flip ASR: 0.6533/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.69475746154785 = 1.947448492050171 + 2.0 * 8.37365436553955
Epoch 0, val loss: 1.9480504989624023
Epoch 10, training loss: 18.682697296142578 = 1.9381667375564575 + 2.0 * 8.372264862060547
Epoch 10, val loss: 1.9387245178222656
Epoch 20, training loss: 18.653457641601562 = 1.926841378211975 + 2.0 * 8.36330795288086
Epoch 20, val loss: 1.927256464958191
Epoch 30, training loss: 18.52695083618164 = 1.9123836755752563 + 2.0 * 8.307283401489258
Epoch 30, val loss: 1.9128094911575317
Epoch 40, training loss: 17.740291595458984 = 1.897446870803833 + 2.0 * 7.921422481536865
Epoch 40, val loss: 1.8980220556259155
Epoch 50, training loss: 16.21336555480957 = 1.880371332168579 + 2.0 * 7.166496753692627
Epoch 50, val loss: 1.8815110921859741
Epoch 60, training loss: 15.534421920776367 = 1.8671669960021973 + 2.0 * 6.833627223968506
Epoch 60, val loss: 1.8689759969711304
Epoch 70, training loss: 15.098204612731934 = 1.856788992881775 + 2.0 * 6.620707988739014
Epoch 70, val loss: 1.858612298965454
Epoch 80, training loss: 14.854263305664062 = 1.8482959270477295 + 2.0 * 6.502983570098877
Epoch 80, val loss: 1.8502013683319092
Epoch 90, training loss: 14.70619010925293 = 1.8395774364471436 + 2.0 * 6.4333062171936035
Epoch 90, val loss: 1.8414225578308105
Epoch 100, training loss: 14.591026306152344 = 1.8304715156555176 + 2.0 * 6.380277633666992
Epoch 100, val loss: 1.8323643207550049
Epoch 110, training loss: 14.491890907287598 = 1.8216283321380615 + 2.0 * 6.3351311683654785
Epoch 110, val loss: 1.8235113620758057
Epoch 120, training loss: 14.412393569946289 = 1.813980221748352 + 2.0 * 6.299206733703613
Epoch 120, val loss: 1.8156659603118896
Epoch 130, training loss: 14.345626831054688 = 1.807121753692627 + 2.0 * 6.269252777099609
Epoch 130, val loss: 1.8085566759109497
Epoch 140, training loss: 14.293571472167969 = 1.8003897666931152 + 2.0 * 6.246590614318848
Epoch 140, val loss: 1.8015592098236084
Epoch 150, training loss: 14.245438575744629 = 1.7934974431991577 + 2.0 * 6.22597074508667
Epoch 150, val loss: 1.7945849895477295
Epoch 160, training loss: 14.204200744628906 = 1.7863930463790894 + 2.0 * 6.208903789520264
Epoch 160, val loss: 1.7875688076019287
Epoch 170, training loss: 14.166961669921875 = 1.7787812948226929 + 2.0 * 6.194090366363525
Epoch 170, val loss: 1.780291199684143
Epoch 180, training loss: 14.134452819824219 = 1.7704205513000488 + 2.0 * 6.182016372680664
Epoch 180, val loss: 1.772526502609253
Epoch 190, training loss: 14.101846694946289 = 1.7611184120178223 + 2.0 * 6.1703643798828125
Epoch 190, val loss: 1.764237880706787
Epoch 200, training loss: 14.068989753723145 = 1.7506250143051147 + 2.0 * 6.159182548522949
Epoch 200, val loss: 1.7551233768463135
Epoch 210, training loss: 14.038612365722656 = 1.73866605758667 + 2.0 * 6.149972915649414
Epoch 210, val loss: 1.7449811697006226
Epoch 220, training loss: 14.010191917419434 = 1.7247838973999023 + 2.0 * 6.142704010009766
Epoch 220, val loss: 1.7335478067398071
Epoch 230, training loss: 13.976604461669922 = 1.7087665796279907 + 2.0 * 6.133918762207031
Epoch 230, val loss: 1.7204217910766602
Epoch 240, training loss: 13.94635009765625 = 1.6899422407150269 + 2.0 * 6.128203868865967
Epoch 240, val loss: 1.7052371501922607
Epoch 250, training loss: 13.91007137298584 = 1.6679275035858154 + 2.0 * 6.121071815490723
Epoch 250, val loss: 1.6876132488250732
Epoch 260, training loss: 13.877189636230469 = 1.6421408653259277 + 2.0 * 6.11752462387085
Epoch 260, val loss: 1.6669700145721436
Epoch 270, training loss: 13.834052085876465 = 1.612245798110962 + 2.0 * 6.110903263092041
Epoch 270, val loss: 1.6430827379226685
Epoch 280, training loss: 13.790705680847168 = 1.5774505138397217 + 2.0 * 6.106627464294434
Epoch 280, val loss: 1.6153450012207031
Epoch 290, training loss: 13.743071556091309 = 1.5372291803359985 + 2.0 * 6.102921009063721
Epoch 290, val loss: 1.5832291841506958
Epoch 300, training loss: 13.692673683166504 = 1.491658091545105 + 2.0 * 6.100507736206055
Epoch 300, val loss: 1.5469168424606323
Epoch 310, training loss: 13.636531829833984 = 1.4417649507522583 + 2.0 * 6.097383499145508
Epoch 310, val loss: 1.507033348083496
Epoch 320, training loss: 13.576570510864258 = 1.387618899345398 + 2.0 * 6.094475746154785
Epoch 320, val loss: 1.4636512994766235
Epoch 330, training loss: 13.513771057128906 = 1.3302953243255615 + 2.0 * 6.091737747192383
Epoch 330, val loss: 1.4176976680755615
Epoch 340, training loss: 13.450204849243164 = 1.2712758779525757 + 2.0 * 6.0894646644592285
Epoch 340, val loss: 1.3702279329299927
Epoch 350, training loss: 13.393294334411621 = 1.2121890783309937 + 2.0 * 6.090552806854248
Epoch 350, val loss: 1.3225312232971191
Epoch 360, training loss: 13.325540542602539 = 1.1546955108642578 + 2.0 * 6.085422515869141
Epoch 360, val loss: 1.276464819908142
Epoch 370, training loss: 13.264216423034668 = 1.0997902154922485 + 2.0 * 6.082212924957275
Epoch 370, val loss: 1.2324270009994507
Epoch 380, training loss: 13.206356048583984 = 1.0473763942718506 + 2.0 * 6.079489707946777
Epoch 380, val loss: 1.1906113624572754
Epoch 390, training loss: 13.155009269714355 = 0.9979632496833801 + 2.0 * 6.0785231590271
Epoch 390, val loss: 1.1515370607376099
Epoch 400, training loss: 13.105299949645996 = 0.9523095488548279 + 2.0 * 6.076495170593262
Epoch 400, val loss: 1.1158775091171265
Epoch 410, training loss: 13.058168411254883 = 0.9102762341499329 + 2.0 * 6.073945999145508
Epoch 410, val loss: 1.083693027496338
Epoch 420, training loss: 13.01247787475586 = 0.8709586262702942 + 2.0 * 6.0707597732543945
Epoch 420, val loss: 1.0539672374725342
Epoch 430, training loss: 12.973047256469727 = 0.8340344429016113 + 2.0 * 6.0695061683654785
Epoch 430, val loss: 1.026384949684143
Epoch 440, training loss: 12.939663887023926 = 0.7994471788406372 + 2.0 * 6.070108413696289
Epoch 440, val loss: 1.0008772611618042
Epoch 450, training loss: 12.901150703430176 = 0.7671538591384888 + 2.0 * 6.066998481750488
Epoch 450, val loss: 0.9773748517036438
Epoch 460, training loss: 12.862448692321777 = 0.7365260720252991 + 2.0 * 6.062961101531982
Epoch 460, val loss: 0.9554216265678406
Epoch 470, training loss: 12.837451934814453 = 0.7072789669036865 + 2.0 * 6.065086364746094
Epoch 470, val loss: 0.9346084594726562
Epoch 480, training loss: 12.801692008972168 = 0.6796055436134338 + 2.0 * 6.0610432624816895
Epoch 480, val loss: 0.9149818420410156
Epoch 490, training loss: 12.768933296203613 = 0.6532703042030334 + 2.0 * 6.057831287384033
Epoch 490, val loss: 0.8966553807258606
Epoch 500, training loss: 12.739962577819824 = 0.6278756260871887 + 2.0 * 6.05604362487793
Epoch 500, val loss: 0.879002571105957
Epoch 510, training loss: 12.711901664733887 = 0.603346586227417 + 2.0 * 6.054277420043945
Epoch 510, val loss: 0.8620476722717285
Epoch 520, training loss: 12.688721656799316 = 0.5795908570289612 + 2.0 * 6.0545654296875
Epoch 520, val loss: 0.8457834124565125
Epoch 530, training loss: 12.672765731811523 = 0.5570082664489746 + 2.0 * 6.0578789710998535
Epoch 530, val loss: 0.8303549885749817
Epoch 540, training loss: 12.637008666992188 = 0.5356247425079346 + 2.0 * 6.050692081451416
Epoch 540, val loss: 0.8161370158195496
Epoch 550, training loss: 12.613714218139648 = 0.5151227712631226 + 2.0 * 6.049295902252197
Epoch 550, val loss: 0.8027278184890747
Epoch 560, training loss: 12.592145919799805 = 0.49551039934158325 + 2.0 * 6.048317909240723
Epoch 560, val loss: 0.7901435494422913
Epoch 570, training loss: 12.576680183410645 = 0.47682228684425354 + 2.0 * 6.049929141998291
Epoch 570, val loss: 0.7782936096191406
Epoch 580, training loss: 12.550836563110352 = 0.4590611159801483 + 2.0 * 6.0458879470825195
Epoch 580, val loss: 0.7674950361251831
Epoch 590, training loss: 12.529972076416016 = 0.4420742988586426 + 2.0 * 6.043949127197266
Epoch 590, val loss: 0.7574414014816284
Epoch 600, training loss: 12.51168441772461 = 0.4256148040294647 + 2.0 * 6.04303503036499
Epoch 600, val loss: 0.7479985356330872
Epoch 610, training loss: 12.503457069396973 = 0.40968838334083557 + 2.0 * 6.046884536743164
Epoch 610, val loss: 0.7391298413276672
Epoch 620, training loss: 12.476146697998047 = 0.3942815363407135 + 2.0 * 6.040932655334473
Epoch 620, val loss: 0.7308478951454163
Epoch 630, training loss: 12.460291862487793 = 0.3793013393878937 + 2.0 * 6.0404953956604
Epoch 630, val loss: 0.7231190204620361
Epoch 640, training loss: 12.442587852478027 = 0.3646511137485504 + 2.0 * 6.038968563079834
Epoch 640, val loss: 0.7157654166221619
Epoch 650, training loss: 12.432805061340332 = 0.3503201901912689 + 2.0 * 6.041242599487305
Epoch 650, val loss: 0.7088562846183777
Epoch 660, training loss: 12.409618377685547 = 0.3362850248813629 + 2.0 * 6.0366668701171875
Epoch 660, val loss: 0.7023247480392456
Epoch 670, training loss: 12.394025802612305 = 0.3224959671497345 + 2.0 * 6.035764694213867
Epoch 670, val loss: 0.6961154341697693
Epoch 680, training loss: 12.3854341506958 = 0.30893054604530334 + 2.0 * 6.038251876831055
Epoch 680, val loss: 0.6902356743812561
Epoch 690, training loss: 12.36666488647461 = 0.29576748609542847 + 2.0 * 6.0354485511779785
Epoch 690, val loss: 0.6847012042999268
Epoch 700, training loss: 12.350923538208008 = 0.28286027908325195 + 2.0 * 6.034031391143799
Epoch 700, val loss: 0.6796061992645264
Epoch 710, training loss: 12.341903686523438 = 0.2703307867050171 + 2.0 * 6.0357866287231445
Epoch 710, val loss: 0.6748209595680237
Epoch 720, training loss: 12.321640014648438 = 0.25816693902015686 + 2.0 * 6.031736373901367
Epoch 720, val loss: 0.6704163551330566
Epoch 730, training loss: 12.310052871704102 = 0.24639959633350372 + 2.0 * 6.031826496124268
Epoch 730, val loss: 0.666466474533081
Epoch 740, training loss: 12.296724319458008 = 0.2350439578294754 + 2.0 * 6.0308403968811035
Epoch 740, val loss: 0.6629164814949036
Epoch 750, training loss: 12.283487319946289 = 0.22409211099147797 + 2.0 * 6.029697418212891
Epoch 750, val loss: 0.6597665548324585
Epoch 760, training loss: 12.273881912231445 = 0.21356475353240967 + 2.0 * 6.030158519744873
Epoch 760, val loss: 0.6570222973823547
Epoch 770, training loss: 12.257635116577148 = 0.20347322523593903 + 2.0 * 6.02708101272583
Epoch 770, val loss: 0.6546202301979065
Epoch 780, training loss: 12.257296562194824 = 0.1937924027442932 + 2.0 * 6.031752109527588
Epoch 780, val loss: 0.65260249376297
Epoch 790, training loss: 12.237492561340332 = 0.18453635275363922 + 2.0 * 6.026478290557861
Epoch 790, val loss: 0.6508772969245911
Epoch 800, training loss: 12.233893394470215 = 0.17571023106575012 + 2.0 * 6.0290913581848145
Epoch 800, val loss: 0.6495154500007629
Epoch 810, training loss: 12.223146438598633 = 0.16733647882938385 + 2.0 * 6.027904987335205
Epoch 810, val loss: 0.6484358906745911
Epoch 820, training loss: 12.206948280334473 = 0.15941165387630463 + 2.0 * 6.023768424987793
Epoch 820, val loss: 0.6477057933807373
Epoch 830, training loss: 12.197549819946289 = 0.1518554538488388 + 2.0 * 6.0228471755981445
Epoch 830, val loss: 0.6472511887550354
Epoch 840, training loss: 12.189313888549805 = 0.1446562111377716 + 2.0 * 6.022328853607178
Epoch 840, val loss: 0.6470838785171509
Epoch 850, training loss: 12.185659408569336 = 0.13786429166793823 + 2.0 * 6.023897647857666
Epoch 850, val loss: 0.6471683382987976
Epoch 860, training loss: 12.175570487976074 = 0.13148283958435059 + 2.0 * 6.022043704986572
Epoch 860, val loss: 0.6475529074668884
Epoch 870, training loss: 12.166163444519043 = 0.12545327842235565 + 2.0 * 6.020355224609375
Epoch 870, val loss: 0.6482025980949402
Epoch 880, training loss: 12.167237281799316 = 0.11976594477891922 + 2.0 * 6.023735523223877
Epoch 880, val loss: 0.6490797996520996
Epoch 890, training loss: 12.155263900756836 = 0.11440832167863846 + 2.0 * 6.020427703857422
Epoch 890, val loss: 0.6502514481544495
Epoch 900, training loss: 12.152610778808594 = 0.10935194045305252 + 2.0 * 6.021629333496094
Epoch 900, val loss: 0.6514988541603088
Epoch 910, training loss: 12.142711639404297 = 0.10459791123867035 + 2.0 * 6.019056797027588
Epoch 910, val loss: 0.6529735326766968
Epoch 920, training loss: 12.136850357055664 = 0.10012499988079071 + 2.0 * 6.018362522125244
Epoch 920, val loss: 0.6546076536178589
Epoch 930, training loss: 12.132478713989258 = 0.0959203839302063 + 2.0 * 6.018279075622559
Epoch 930, val loss: 0.6564441919326782
Epoch 940, training loss: 12.123191833496094 = 0.09195175766944885 + 2.0 * 6.015620231628418
Epoch 940, val loss: 0.6584063768386841
Epoch 950, training loss: 12.118273735046387 = 0.0881992056965828 + 2.0 * 6.0150370597839355
Epoch 950, val loss: 0.6605193614959717
Epoch 960, training loss: 12.114630699157715 = 0.08464614301919937 + 2.0 * 6.0149922370910645
Epoch 960, val loss: 0.6628000736236572
Epoch 970, training loss: 12.119120597839355 = 0.08129342645406723 + 2.0 * 6.018913745880127
Epoch 970, val loss: 0.6651275753974915
Epoch 980, training loss: 12.104941368103027 = 0.07813228666782379 + 2.0 * 6.013404369354248
Epoch 980, val loss: 0.6676117777824402
Epoch 990, training loss: 12.101810455322266 = 0.07513684779405594 + 2.0 * 6.013336658477783
Epoch 990, val loss: 0.6701406240463257
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9705
Flip ASR: 0.9644/225 nodes
The final ASR:0.76507, 0.14865, Accuracy:0.82346, 0.00462
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11578])
remove edge: torch.Size([2, 9502])
updated graph: torch.Size([2, 10524])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9705
Flip ASR: 0.9644/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9852
Flip ASR: 0.9822/225 nodes
The final ASR:0.97786, 0.00603, Accuracy:0.82716, 0.00462
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.3 der2:0.4 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 18.682126998901367 = 1.9343445301055908 + 2.0 * 8.37389087677002
Epoch 0, val loss: 1.9219439029693604
Epoch 10, training loss: 18.670957565307617 = 1.924044132232666 + 2.0 * 8.373456954956055
Epoch 10, val loss: 1.9114501476287842
Epoch 20, training loss: 18.65226173400879 = 1.9108917713165283 + 2.0 * 8.370684623718262
Epoch 20, val loss: 1.8980735540390015
Epoch 30, training loss: 18.592041015625 = 1.8930422067642212 + 2.0 * 8.349499702453613
Epoch 30, val loss: 1.8801796436309814
Epoch 40, training loss: 18.244583129882812 = 1.8710265159606934 + 2.0 * 8.18677806854248
Epoch 40, val loss: 1.8588786125183105
Epoch 50, training loss: 17.057844161987305 = 1.848280429840088 + 2.0 * 7.604781627655029
Epoch 50, val loss: 1.8384751081466675
Epoch 60, training loss: 16.266040802001953 = 1.8350707292556763 + 2.0 * 7.215485095977783
Epoch 60, val loss: 1.8274503946304321
Epoch 70, training loss: 15.646191596984863 = 1.8269035816192627 + 2.0 * 6.90964412689209
Epoch 70, val loss: 1.8205938339233398
Epoch 80, training loss: 15.228302955627441 = 1.8196218013763428 + 2.0 * 6.70434045791626
Epoch 80, val loss: 1.8149715662002563
Epoch 90, training loss: 14.973548889160156 = 1.8132025003433228 + 2.0 * 6.580173015594482
Epoch 90, val loss: 1.810092568397522
Epoch 100, training loss: 14.777704238891602 = 1.8072291612625122 + 2.0 * 6.4852375984191895
Epoch 100, val loss: 1.8053274154663086
Epoch 110, training loss: 14.63708782196045 = 1.801387906074524 + 2.0 * 6.417850017547607
Epoch 110, val loss: 1.800408959388733
Epoch 120, training loss: 14.539435386657715 = 1.7960491180419922 + 2.0 * 6.371693134307861
Epoch 120, val loss: 1.7959232330322266
Epoch 130, training loss: 14.467510223388672 = 1.791068434715271 + 2.0 * 6.338221073150635
Epoch 130, val loss: 1.7919448614120483
Epoch 140, training loss: 14.403759002685547 = 1.7860621213912964 + 2.0 * 6.3088483810424805
Epoch 140, val loss: 1.7880544662475586
Epoch 150, training loss: 14.349576950073242 = 1.7806923389434814 + 2.0 * 6.28444242477417
Epoch 150, val loss: 1.7838701009750366
Epoch 160, training loss: 14.310510635375977 = 1.7747315168380737 + 2.0 * 6.267889499664307
Epoch 160, val loss: 1.7792229652404785
Epoch 170, training loss: 14.26732349395752 = 1.7680647373199463 + 2.0 * 6.249629497528076
Epoch 170, val loss: 1.7740554809570312
Epoch 180, training loss: 14.229418754577637 = 1.7607030868530273 + 2.0 * 6.234357833862305
Epoch 180, val loss: 1.768349051475525
Epoch 190, training loss: 14.193822860717773 = 1.7523303031921387 + 2.0 * 6.2207465171813965
Epoch 190, val loss: 1.7618803977966309
Epoch 200, training loss: 14.172257423400879 = 1.742709755897522 + 2.0 * 6.214773654937744
Epoch 200, val loss: 1.7544488906860352
Epoch 210, training loss: 14.129838943481445 = 1.7317317724227905 + 2.0 * 6.199053764343262
Epoch 210, val loss: 1.7460004091262817
Epoch 220, training loss: 14.097941398620605 = 1.7192236185073853 + 2.0 * 6.189358711242676
Epoch 220, val loss: 1.7363789081573486
Epoch 230, training loss: 14.065468788146973 = 1.7047243118286133 + 2.0 * 6.18037223815918
Epoch 230, val loss: 1.7251542806625366
Epoch 240, training loss: 14.037508010864258 = 1.6878292560577393 + 2.0 * 6.174839496612549
Epoch 240, val loss: 1.7119454145431519
Epoch 250, training loss: 13.99942398071289 = 1.66823410987854 + 2.0 * 6.165595054626465
Epoch 250, val loss: 1.696598768234253
Epoch 260, training loss: 13.963446617126465 = 1.6455446481704712 + 2.0 * 6.1589508056640625
Epoch 260, val loss: 1.6787317991256714
Epoch 270, training loss: 13.926329612731934 = 1.6191545724868774 + 2.0 * 6.153587341308594
Epoch 270, val loss: 1.657716155052185
Epoch 280, training loss: 13.885981559753418 = 1.5887610912322998 + 2.0 * 6.1486101150512695
Epoch 280, val loss: 1.633284568786621
Epoch 290, training loss: 13.841787338256836 = 1.5541672706604004 + 2.0 * 6.143810272216797
Epoch 290, val loss: 1.6053537130355835
Epoch 300, training loss: 13.791952133178711 = 1.5153781175613403 + 2.0 * 6.13828706741333
Epoch 300, val loss: 1.5738623142242432
Epoch 310, training loss: 13.741496086120605 = 1.4724066257476807 + 2.0 * 6.134544849395752
Epoch 310, val loss: 1.5389034748077393
Epoch 320, training loss: 13.688462257385254 = 1.4261661767959595 + 2.0 * 6.131147861480713
Epoch 320, val loss: 1.5011793375015259
Epoch 330, training loss: 13.632719993591309 = 1.3776919841766357 + 2.0 * 6.127513885498047
Epoch 330, val loss: 1.4619128704071045
Epoch 340, training loss: 13.576835632324219 = 1.3277568817138672 + 2.0 * 6.124539375305176
Epoch 340, val loss: 1.4216606616973877
Epoch 350, training loss: 13.519357681274414 = 1.2773890495300293 + 2.0 * 6.1209845542907715
Epoch 350, val loss: 1.3812938928604126
Epoch 360, training loss: 13.469552040100098 = 1.2270017862319946 + 2.0 * 6.121274948120117
Epoch 360, val loss: 1.3412830829620361
Epoch 370, training loss: 13.408658027648926 = 1.1778041124343872 + 2.0 * 6.115427017211914
Epoch 370, val loss: 1.302356481552124
Epoch 380, training loss: 13.353119850158691 = 1.1297000646591187 + 2.0 * 6.111710071563721
Epoch 380, val loss: 1.2646050453186035
Epoch 390, training loss: 13.307046890258789 = 1.0827336311340332 + 2.0 * 6.112156391143799
Epoch 390, val loss: 1.227838158607483
Epoch 400, training loss: 13.249277114868164 = 1.0375460386276245 + 2.0 * 6.105865478515625
Epoch 400, val loss: 1.1927210092544556
Epoch 410, training loss: 13.200949668884277 = 0.9940778017044067 + 2.0 * 6.10343599319458
Epoch 410, val loss: 1.1589784622192383
Epoch 420, training loss: 13.161273956298828 = 0.9523314237594604 + 2.0 * 6.104471206665039
Epoch 420, val loss: 1.1267589330673218
Epoch 430, training loss: 13.116246223449707 = 0.9128252267837524 + 2.0 * 6.101710319519043
Epoch 430, val loss: 1.0964475870132446
Epoch 440, training loss: 13.068846702575684 = 0.8757336139678955 + 2.0 * 6.096556663513184
Epoch 440, val loss: 1.0679832696914673
Epoch 450, training loss: 13.026612281799316 = 0.840650200843811 + 2.0 * 6.092980861663818
Epoch 450, val loss: 1.0414670705795288
Epoch 460, training loss: 12.988510131835938 = 0.8072770237922668 + 2.0 * 6.090616703033447
Epoch 460, val loss: 1.0165013074874878
Epoch 470, training loss: 12.969768524169922 = 0.7755880355834961 + 2.0 * 6.097090244293213
Epoch 470, val loss: 0.9932172894477844
Epoch 480, training loss: 12.923142433166504 = 0.7462195158004761 + 2.0 * 6.088461399078369
Epoch 480, val loss: 0.9721369743347168
Epoch 490, training loss: 12.887909889221191 = 0.7187286019325256 + 2.0 * 6.084590435028076
Epoch 490, val loss: 0.9528144001960754
Epoch 500, training loss: 12.85726547241211 = 0.6926157474517822 + 2.0 * 6.082324981689453
Epoch 500, val loss: 0.9349698424339294
Epoch 510, training loss: 12.839253425598145 = 0.667862594127655 + 2.0 * 6.085695266723633
Epoch 510, val loss: 0.9186818599700928
Epoch 520, training loss: 12.803803443908691 = 0.644598662853241 + 2.0 * 6.079602241516113
Epoch 520, val loss: 0.9038874506950378
Epoch 530, training loss: 12.777250289916992 = 0.6226441264152527 + 2.0 * 6.077302932739258
Epoch 530, val loss: 0.8906800150871277
Epoch 540, training loss: 12.757862091064453 = 0.6017661094665527 + 2.0 * 6.078048229217529
Epoch 540, val loss: 0.8786754608154297
Epoch 550, training loss: 12.741585731506348 = 0.5819361805915833 + 2.0 * 6.079824924468994
Epoch 550, val loss: 0.8678742051124573
Epoch 560, training loss: 12.706955909729004 = 0.5631086230278015 + 2.0 * 6.071923732757568
Epoch 560, val loss: 0.8583196401596069
Epoch 570, training loss: 12.68497371673584 = 0.5450471639633179 + 2.0 * 6.069963455200195
Epoch 570, val loss: 0.8497290015220642
Epoch 580, training loss: 12.671281814575195 = 0.5275686979293823 + 2.0 * 6.071856498718262
Epoch 580, val loss: 0.8419027328491211
Epoch 590, training loss: 12.65088176727295 = 0.5106860399246216 + 2.0 * 6.070097923278809
Epoch 590, val loss: 0.8349445462226868
Epoch 600, training loss: 12.627327919006348 = 0.4944305419921875 + 2.0 * 6.06644868850708
Epoch 600, val loss: 0.8287663459777832
Epoch 610, training loss: 12.607585906982422 = 0.4785197675228119 + 2.0 * 6.064533233642578
Epoch 610, val loss: 0.8231557607650757
Epoch 620, training loss: 12.60324764251709 = 0.4629744589328766 + 2.0 * 6.070136547088623
Epoch 620, val loss: 0.8181948065757751
Epoch 630, training loss: 12.57240104675293 = 0.4478908181190491 + 2.0 * 6.062254905700684
Epoch 630, val loss: 0.8137664794921875
Epoch 640, training loss: 12.559272766113281 = 0.43307074904441833 + 2.0 * 6.063100814819336
Epoch 640, val loss: 0.8099637031555176
Epoch 650, training loss: 12.538115501403809 = 0.41857483983039856 + 2.0 * 6.059770107269287
Epoch 650, val loss: 0.8065464496612549
Epoch 660, training loss: 12.530555725097656 = 0.40433844923973083 + 2.0 * 6.063108444213867
Epoch 660, val loss: 0.8037418723106384
Epoch 670, training loss: 12.510689735412598 = 0.3903464078903198 + 2.0 * 6.060171604156494
Epoch 670, val loss: 0.8013770580291748
Epoch 680, training loss: 12.490557670593262 = 0.37674689292907715 + 2.0 * 6.056905269622803
Epoch 680, val loss: 0.799647331237793
Epoch 690, training loss: 12.475748062133789 = 0.36335229873657227 + 2.0 * 6.056197643280029
Epoch 690, val loss: 0.798346996307373
Epoch 700, training loss: 12.462616920471191 = 0.35016319155693054 + 2.0 * 6.05622673034668
Epoch 700, val loss: 0.7975869178771973
Epoch 710, training loss: 12.452234268188477 = 0.33721724152565 + 2.0 * 6.05750846862793
Epoch 710, val loss: 0.7973148226737976
Epoch 720, training loss: 12.431007385253906 = 0.32458463311195374 + 2.0 * 6.053211212158203
Epoch 720, val loss: 0.7974499464035034
Epoch 730, training loss: 12.41653060913086 = 0.31216979026794434 + 2.0 * 6.052180290222168
Epoch 730, val loss: 0.7981242537498474
Epoch 740, training loss: 12.418129920959473 = 0.29996246099472046 + 2.0 * 6.059083938598633
Epoch 740, val loss: 0.7992395758628845
Epoch 750, training loss: 12.394436836242676 = 0.2880840301513672 + 2.0 * 6.053176403045654
Epoch 750, val loss: 0.8006095886230469
Epoch 760, training loss: 12.375853538513184 = 0.27650442719459534 + 2.0 * 6.0496745109558105
Epoch 760, val loss: 0.8026540875434875
Epoch 770, training loss: 12.36764144897461 = 0.2652342915534973 + 2.0 * 6.051203727722168
Epoch 770, val loss: 0.8049956560134888
Epoch 780, training loss: 12.354714393615723 = 0.25435489416122437 + 2.0 * 6.050179958343506
Epoch 780, val loss: 0.8078153729438782
Epoch 790, training loss: 12.340200424194336 = 0.24378900229930878 + 2.0 * 6.048205852508545
Epoch 790, val loss: 0.8110330104827881
Epoch 800, training loss: 12.326018333435059 = 0.2336161583662033 + 2.0 * 6.046201229095459
Epoch 800, val loss: 0.8147488832473755
Epoch 810, training loss: 12.313970565795898 = 0.22380971908569336 + 2.0 * 6.045080184936523
Epoch 810, val loss: 0.8188623785972595
Epoch 820, training loss: 12.320086479187012 = 0.21440845727920532 + 2.0 * 6.0528388023376465
Epoch 820, val loss: 0.8233013153076172
Epoch 830, training loss: 12.291471481323242 = 0.20542356371879578 + 2.0 * 6.043024063110352
Epoch 830, val loss: 0.8280929327011108
Epoch 840, training loss: 12.286417007446289 = 0.19685640931129456 + 2.0 * 6.044780254364014
Epoch 840, val loss: 0.8333017230033875
Epoch 850, training loss: 12.272682189941406 = 0.1886969655752182 + 2.0 * 6.041992664337158
Epoch 850, val loss: 0.8386620283126831
Epoch 860, training loss: 12.268522262573242 = 0.1809932291507721 + 2.0 * 6.043764591217041
Epoch 860, val loss: 0.8445211052894592
Epoch 870, training loss: 12.258172988891602 = 0.17362384498119354 + 2.0 * 6.042274475097656
Epoch 870, val loss: 0.8505985140800476
Epoch 880, training loss: 12.249178886413574 = 0.1666303128004074 + 2.0 * 6.041274070739746
Epoch 880, val loss: 0.8568457365036011
Epoch 890, training loss: 12.23960018157959 = 0.16001279652118683 + 2.0 * 6.039793491363525
Epoch 890, val loss: 0.8633397221565247
Epoch 900, training loss: 12.230725288391113 = 0.15372182428836823 + 2.0 * 6.038501739501953
Epoch 900, val loss: 0.8701380491256714
Epoch 910, training loss: 12.236759185791016 = 0.14773961901664734 + 2.0 * 6.0445098876953125
Epoch 910, val loss: 0.8770819902420044
Epoch 920, training loss: 12.2211332321167 = 0.14205192029476166 + 2.0 * 6.039540767669678
Epoch 920, val loss: 0.8840728402137756
Epoch 930, training loss: 12.216085433959961 = 0.13667532801628113 + 2.0 * 6.039705276489258
Epoch 930, val loss: 0.8913819193840027
Epoch 940, training loss: 12.205402374267578 = 0.1315642148256302 + 2.0 * 6.036919116973877
Epoch 940, val loss: 0.8987393379211426
Epoch 950, training loss: 12.199278831481934 = 0.1266995370388031 + 2.0 * 6.036289691925049
Epoch 950, val loss: 0.9062835574150085
Epoch 960, training loss: 12.20119857788086 = 0.12207406014204025 + 2.0 * 6.039562225341797
Epoch 960, val loss: 0.9137917757034302
Epoch 970, training loss: 12.18592643737793 = 0.11768946051597595 + 2.0 * 6.03411865234375
Epoch 970, val loss: 0.9214718341827393
Epoch 980, training loss: 12.180368423461914 = 0.11350064724683762 + 2.0 * 6.03343391418457
Epoch 980, val loss: 0.9292609691619873
Epoch 990, training loss: 12.186034202575684 = 0.10950273275375366 + 2.0 * 6.038265705108643
Epoch 990, val loss: 0.9369885921478271
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7889
Overall ASR: 0.4760
Flip ASR: 0.3822/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.698253631591797 = 1.9505794048309326 + 2.0 * 8.3738374710083
Epoch 0, val loss: 1.955903172492981
Epoch 10, training loss: 18.686765670776367 = 1.9403398036956787 + 2.0 * 8.373212814331055
Epoch 10, val loss: 1.9451160430908203
Epoch 20, training loss: 18.665756225585938 = 1.9278866052627563 + 2.0 * 8.368934631347656
Epoch 20, val loss: 1.9318156242370605
Epoch 30, training loss: 18.588476181030273 = 1.9115054607391357 + 2.0 * 8.338485717773438
Epoch 30, val loss: 1.9144212007522583
Epoch 40, training loss: 18.134605407714844 = 1.8932076692581177 + 2.0 * 8.120698928833008
Epoch 40, val loss: 1.8956488370895386
Epoch 50, training loss: 16.77362632751465 = 1.8747727870941162 + 2.0 * 7.449427127838135
Epoch 50, val loss: 1.8772095441818237
Epoch 60, training loss: 15.965635299682617 = 1.861538290977478 + 2.0 * 7.052048683166504
Epoch 60, val loss: 1.864586353302002
Epoch 70, training loss: 15.478656768798828 = 1.8505679368972778 + 2.0 * 6.81404447555542
Epoch 70, val loss: 1.8535048961639404
Epoch 80, training loss: 15.210590362548828 = 1.8397126197814941 + 2.0 * 6.685438632965088
Epoch 80, val loss: 1.8425956964492798
Epoch 90, training loss: 15.001752853393555 = 1.8301366567611694 + 2.0 * 6.585808277130127
Epoch 90, val loss: 1.8328580856323242
Epoch 100, training loss: 14.81576156616211 = 1.8217211961746216 + 2.0 * 6.497020244598389
Epoch 100, val loss: 1.824095368385315
Epoch 110, training loss: 14.677817344665527 = 1.8137177228927612 + 2.0 * 6.432049751281738
Epoch 110, val loss: 1.8155802488327026
Epoch 120, training loss: 14.58395767211914 = 1.8060917854309082 + 2.0 * 6.388933181762695
Epoch 120, val loss: 1.8073054552078247
Epoch 130, training loss: 14.502692222595215 = 1.7989956140518188 + 2.0 * 6.351848125457764
Epoch 130, val loss: 1.7993955612182617
Epoch 140, training loss: 14.437309265136719 = 1.7920305728912354 + 2.0 * 6.322639465332031
Epoch 140, val loss: 1.7919223308563232
Epoch 150, training loss: 14.382843017578125 = 1.7849719524383545 + 2.0 * 6.298935413360596
Epoch 150, val loss: 1.7848103046417236
Epoch 160, training loss: 14.33547592163086 = 1.777686357498169 + 2.0 * 6.278894901275635
Epoch 160, val loss: 1.777753472328186
Epoch 170, training loss: 14.29282283782959 = 1.769982099533081 + 2.0 * 6.261420249938965
Epoch 170, val loss: 1.7705785036087036
Epoch 180, training loss: 14.254297256469727 = 1.7615323066711426 + 2.0 * 6.246382236480713
Epoch 180, val loss: 1.7629972696304321
Epoch 190, training loss: 14.219490051269531 = 1.7520848512649536 + 2.0 * 6.233702659606934
Epoch 190, val loss: 1.7548352479934692
Epoch 200, training loss: 14.186334609985352 = 1.7415035963058472 + 2.0 * 6.222415447235107
Epoch 200, val loss: 1.7459527254104614
Epoch 210, training loss: 14.150219917297363 = 1.7296264171600342 + 2.0 * 6.210296630859375
Epoch 210, val loss: 1.736183762550354
Epoch 220, training loss: 14.117116928100586 = 1.7160183191299438 + 2.0 * 6.200549125671387
Epoch 220, val loss: 1.7251429557800293
Epoch 230, training loss: 14.08532428741455 = 1.7003873586654663 + 2.0 * 6.192468643188477
Epoch 230, val loss: 1.7125619649887085
Epoch 240, training loss: 14.047836303710938 = 1.6824512481689453 + 2.0 * 6.182692527770996
Epoch 240, val loss: 1.698336124420166
Epoch 250, training loss: 14.009866714477539 = 1.6617504358291626 + 2.0 * 6.174057960510254
Epoch 250, val loss: 1.681897759437561
Epoch 260, training loss: 13.982516288757324 = 1.6376886367797852 + 2.0 * 6.1724138259887695
Epoch 260, val loss: 1.6626813411712646
Epoch 270, training loss: 13.934992790222168 = 1.6099683046340942 + 2.0 * 6.162512302398682
Epoch 270, val loss: 1.6408228874206543
Epoch 280, training loss: 13.886432647705078 = 1.578467845916748 + 2.0 * 6.153982162475586
Epoch 280, val loss: 1.6158547401428223
Epoch 290, training loss: 13.839644432067871 = 1.5425487756729126 + 2.0 * 6.148547649383545
Epoch 290, val loss: 1.5872480869293213
Epoch 300, training loss: 13.796836853027344 = 1.50186026096344 + 2.0 * 6.147488117218018
Epoch 300, val loss: 1.5547139644622803
Epoch 310, training loss: 13.734152793884277 = 1.4570354223251343 + 2.0 * 6.138558864593506
Epoch 310, val loss: 1.518725037574768
Epoch 320, training loss: 13.677263259887695 = 1.4083425998687744 + 2.0 * 6.13446044921875
Epoch 320, val loss: 1.4796119928359985
Epoch 330, training loss: 13.619311332702637 = 1.3562949895858765 + 2.0 * 6.1315083503723145
Epoch 330, val loss: 1.4376832246780396
Epoch 340, training loss: 13.563101768493652 = 1.3022249937057495 + 2.0 * 6.130438327789307
Epoch 340, val loss: 1.3944175243377686
Epoch 350, training loss: 13.496878623962402 = 1.2478865385055542 + 2.0 * 6.124495983123779
Epoch 350, val loss: 1.3506406545639038
Epoch 360, training loss: 13.435104370117188 = 1.1935769319534302 + 2.0 * 6.120763778686523
Epoch 360, val loss: 1.3068379163742065
Epoch 370, training loss: 13.375470161437988 = 1.1404691934585571 + 2.0 * 6.117500305175781
Epoch 370, val loss: 1.263823390007019
Epoch 380, training loss: 13.317781448364258 = 1.0895196199417114 + 2.0 * 6.114130973815918
Epoch 380, val loss: 1.2228121757507324
Epoch 390, training loss: 13.261078834533691 = 1.0411497354507446 + 2.0 * 6.109964370727539
Epoch 390, val loss: 1.1838830709457397
Epoch 400, training loss: 13.220219612121582 = 0.9954128861427307 + 2.0 * 6.112403392791748
Epoch 400, val loss: 1.1472176313400269
Epoch 410, training loss: 13.16816234588623 = 0.9527326226234436 + 2.0 * 6.107714653015137
Epoch 410, val loss: 1.1133042573928833
Epoch 420, training loss: 13.116400718688965 = 0.9129492044448853 + 2.0 * 6.1017255783081055
Epoch 420, val loss: 1.0822943449020386
Epoch 430, training loss: 13.071261405944824 = 0.8756398558616638 + 2.0 * 6.097810745239258
Epoch 430, val loss: 1.053303599357605
Epoch 440, training loss: 13.031314849853516 = 0.8402202129364014 + 2.0 * 6.095547199249268
Epoch 440, val loss: 1.0262516736984253
Epoch 450, training loss: 13.00197982788086 = 0.8066754937171936 + 2.0 * 6.097651958465576
Epoch 450, val loss: 1.0010299682617188
Epoch 460, training loss: 12.963027000427246 = 0.775473952293396 + 2.0 * 6.093776702880859
Epoch 460, val loss: 0.9778140783309937
Epoch 470, training loss: 12.922411918640137 = 0.7457812428474426 + 2.0 * 6.088315486907959
Epoch 470, val loss: 0.9563129544258118
Epoch 480, training loss: 12.8880033493042 = 0.7173252701759338 + 2.0 * 6.085339069366455
Epoch 480, val loss: 0.9360640048980713
Epoch 490, training loss: 12.87362289428711 = 0.6899896264076233 + 2.0 * 6.091816425323486
Epoch 490, val loss: 0.9168552160263062
Epoch 500, training loss: 12.82724666595459 = 0.6640501022338867 + 2.0 * 6.081598281860352
Epoch 500, val loss: 0.8993363976478577
Epoch 510, training loss: 12.801770210266113 = 0.6395944356918335 + 2.0 * 6.081088066101074
Epoch 510, val loss: 0.8832382559776306
Epoch 520, training loss: 12.771499633789062 = 0.6159524321556091 + 2.0 * 6.077773571014404
Epoch 520, val loss: 0.8680526614189148
Epoch 530, training loss: 12.770159721374512 = 0.5932317972183228 + 2.0 * 6.08846378326416
Epoch 530, val loss: 0.8536973595619202
Epoch 540, training loss: 12.726133346557617 = 0.5714644193649292 + 2.0 * 6.077334403991699
Epoch 540, val loss: 0.8405702114105225
Epoch 550, training loss: 12.697259902954102 = 0.550751805305481 + 2.0 * 6.073254108428955
Epoch 550, val loss: 0.8284657001495361
Epoch 560, training loss: 12.672012329101562 = 0.5306851267814636 + 2.0 * 6.0706634521484375
Epoch 560, val loss: 0.8170487880706787
Epoch 570, training loss: 12.660700798034668 = 0.511278510093689 + 2.0 * 6.074711322784424
Epoch 570, val loss: 0.8063561320304871
Epoch 580, training loss: 12.641520500183105 = 0.49276337027549744 + 2.0 * 6.074378490447998
Epoch 580, val loss: 0.7965317368507385
Epoch 590, training loss: 12.611679077148438 = 0.4749583601951599 + 2.0 * 6.068360328674316
Epoch 590, val loss: 0.7876003384590149
Epoch 600, training loss: 12.588634490966797 = 0.45788705348968506 + 2.0 * 6.06537389755249
Epoch 600, val loss: 0.7793237566947937
Epoch 610, training loss: 12.577069282531738 = 0.44120460748672485 + 2.0 * 6.06793212890625
Epoch 610, val loss: 0.7716349959373474
Epoch 620, training loss: 12.554616928100586 = 0.42518743872642517 + 2.0 * 6.0647149085998535
Epoch 620, val loss: 0.7644991874694824
Epoch 630, training loss: 12.532852172851562 = 0.4095570147037506 + 2.0 * 6.061647415161133
Epoch 630, val loss: 0.7579863667488098
Epoch 640, training loss: 12.526504516601562 = 0.39433351159095764 + 2.0 * 6.066085338592529
Epoch 640, val loss: 0.7518839836120605
Epoch 650, training loss: 12.505488395690918 = 0.3794815242290497 + 2.0 * 6.0630035400390625
Epoch 650, val loss: 0.7463160157203674
Epoch 660, training loss: 12.483783721923828 = 0.3651309013366699 + 2.0 * 6.059326171875
Epoch 660, val loss: 0.7412289381027222
Epoch 670, training loss: 12.465388298034668 = 0.3510777950286865 + 2.0 * 6.057155132293701
Epoch 670, val loss: 0.736505389213562
Epoch 680, training loss: 12.454817771911621 = 0.3372649848461151 + 2.0 * 6.058776378631592
Epoch 680, val loss: 0.7321071624755859
Epoch 690, training loss: 12.439581871032715 = 0.32370468974113464 + 2.0 * 6.057938575744629
Epoch 690, val loss: 0.7280541658401489
Epoch 700, training loss: 12.422760963439941 = 0.3104661703109741 + 2.0 * 6.056147575378418
Epoch 700, val loss: 0.7244880795478821
Epoch 710, training loss: 12.406543731689453 = 0.29754912853240967 + 2.0 * 6.054497241973877
Epoch 710, val loss: 0.7212306261062622
Epoch 720, training loss: 12.39277172088623 = 0.2848915159702301 + 2.0 * 6.053940296173096
Epoch 720, val loss: 0.7183654308319092
Epoch 730, training loss: 12.379812240600586 = 0.27257776260375977 + 2.0 * 6.053617000579834
Epoch 730, val loss: 0.7159014940261841
Epoch 740, training loss: 12.361108779907227 = 0.2606601417064667 + 2.0 * 6.050224304199219
Epoch 740, val loss: 0.7139248847961426
Epoch 750, training loss: 12.349273681640625 = 0.24907426536083221 + 2.0 * 6.050099849700928
Epoch 750, val loss: 0.7123438119888306
Epoch 760, training loss: 12.347236633300781 = 0.23784509301185608 + 2.0 * 6.0546956062316895
Epoch 760, val loss: 0.7111836075782776
Epoch 770, training loss: 12.32685375213623 = 0.2270902842283249 + 2.0 * 6.049881935119629
Epoch 770, val loss: 0.7103812098503113
Epoch 780, training loss: 12.312952041625977 = 0.2168157398700714 + 2.0 * 6.048068046569824
Epoch 780, val loss: 0.710056483745575
Epoch 790, training loss: 12.30046558380127 = 0.2070087492465973 + 2.0 * 6.046728610992432
Epoch 790, val loss: 0.7101179957389832
Epoch 800, training loss: 12.295677185058594 = 0.19762757420539856 + 2.0 * 6.04902458190918
Epoch 800, val loss: 0.7105864882469177
Epoch 810, training loss: 12.2860689163208 = 0.1887650191783905 + 2.0 * 6.048652172088623
Epoch 810, val loss: 0.7114724516868591
Epoch 820, training loss: 12.26978874206543 = 0.18044663965702057 + 2.0 * 6.044671058654785
Epoch 820, val loss: 0.7127857804298401
Epoch 830, training loss: 12.257868766784668 = 0.17255382239818573 + 2.0 * 6.042657375335693
Epoch 830, val loss: 0.7144351005554199
Epoch 840, training loss: 12.259485244750977 = 0.1650789976119995 + 2.0 * 6.047203063964844
Epoch 840, val loss: 0.7164624333381653
Epoch 850, training loss: 12.25014591217041 = 0.15804773569107056 + 2.0 * 6.046049118041992
Epoch 850, val loss: 0.718803346157074
Epoch 860, training loss: 12.238285064697266 = 0.15147796273231506 + 2.0 * 6.043403625488281
Epoch 860, val loss: 0.7214738726615906
Epoch 870, training loss: 12.231863021850586 = 0.14533166587352753 + 2.0 * 6.0432658195495605
Epoch 870, val loss: 0.7244973182678223
Epoch 880, training loss: 12.219209671020508 = 0.1395689994096756 + 2.0 * 6.039820194244385
Epoch 880, val loss: 0.7276226878166199
Epoch 890, training loss: 12.21178913116455 = 0.13413125276565552 + 2.0 * 6.0388288497924805
Epoch 890, val loss: 0.7310757040977478
Epoch 900, training loss: 12.217277526855469 = 0.12901127338409424 + 2.0 * 6.044133186340332
Epoch 900, val loss: 0.7347555160522461
Epoch 910, training loss: 12.20339298248291 = 0.12421493977308273 + 2.0 * 6.039588928222656
Epoch 910, val loss: 0.7384054660797119
Epoch 920, training loss: 12.195259094238281 = 0.11974077671766281 + 2.0 * 6.037759304046631
Epoch 920, val loss: 0.7424474358558655
Epoch 930, training loss: 12.187442779541016 = 0.11550315469503403 + 2.0 * 6.0359697341918945
Epoch 930, val loss: 0.7465242147445679
Epoch 940, training loss: 12.207490921020508 = 0.11149676889181137 + 2.0 * 6.047996997833252
Epoch 940, val loss: 0.7506836652755737
Epoch 950, training loss: 12.180304527282715 = 0.10779184103012085 + 2.0 * 6.036256313323975
Epoch 950, val loss: 0.7549446821212769
Epoch 960, training loss: 12.173667907714844 = 0.10428723692893982 + 2.0 * 6.0346903800964355
Epoch 960, val loss: 0.7593414187431335
Epoch 970, training loss: 12.168547630310059 = 0.10095647722482681 + 2.0 * 6.033795356750488
Epoch 970, val loss: 0.7638289928436279
Epoch 980, training loss: 12.168737411499023 = 0.09781055152416229 + 2.0 * 6.035463333129883
Epoch 980, val loss: 0.7683465480804443
Epoch 990, training loss: 12.157833099365234 = 0.09486981481313705 + 2.0 * 6.031481742858887
Epoch 990, val loss: 0.7729613184928894
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8074
Overall ASR: 0.8118
Flip ASR: 0.7733/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.703556060791016 = 1.9560410976409912 + 2.0 * 8.373757362365723
Epoch 0, val loss: 1.965513825416565
Epoch 10, training loss: 18.688037872314453 = 1.9457106590270996 + 2.0 * 8.371163368225098
Epoch 10, val loss: 1.9537426233291626
Epoch 20, training loss: 18.657615661621094 = 1.9333046674728394 + 2.0 * 8.36215591430664
Epoch 20, val loss: 1.9387398958206177
Epoch 30, training loss: 18.55573272705078 = 1.9175846576690674 + 2.0 * 8.319073677062988
Epoch 30, val loss: 1.9194562435150146
Epoch 40, training loss: 17.759151458740234 = 1.9001362323760986 + 2.0 * 7.929507732391357
Epoch 40, val loss: 1.8982163667678833
Epoch 50, training loss: 16.384061813354492 = 1.8821247816085815 + 2.0 * 7.250968933105469
Epoch 50, val loss: 1.8779953718185425
Epoch 60, training loss: 15.866510391235352 = 1.8666300773620605 + 2.0 * 6.999940395355225
Epoch 60, val loss: 1.8620139360427856
Epoch 70, training loss: 15.59142780303955 = 1.8541885614395142 + 2.0 * 6.868619441986084
Epoch 70, val loss: 1.8490961790084839
Epoch 80, training loss: 15.326567649841309 = 1.843286156654358 + 2.0 * 6.741640567779541
Epoch 80, val loss: 1.8379663228988647
Epoch 90, training loss: 15.03661060333252 = 1.8338251113891602 + 2.0 * 6.60139274597168
Epoch 90, val loss: 1.8285813331604004
Epoch 100, training loss: 14.789127349853516 = 1.8257129192352295 + 2.0 * 6.4817070960998535
Epoch 100, val loss: 1.8207098245620728
Epoch 110, training loss: 14.636551856994629 = 1.817692756652832 + 2.0 * 6.409429550170898
Epoch 110, val loss: 1.8123351335525513
Epoch 120, training loss: 14.530938148498535 = 1.8091217279434204 + 2.0 * 6.360908031463623
Epoch 120, val loss: 1.8028002977371216
Epoch 130, training loss: 14.445674896240234 = 1.8004823923110962 + 2.0 * 6.322596073150635
Epoch 130, val loss: 1.793330430984497
Epoch 140, training loss: 14.375557899475098 = 1.7923527956008911 + 2.0 * 6.291602611541748
Epoch 140, val loss: 1.7843362092971802
Epoch 150, training loss: 14.316434860229492 = 1.7845109701156616 + 2.0 * 6.26596212387085
Epoch 150, val loss: 1.775870442390442
Epoch 160, training loss: 14.264681816101074 = 1.7768603563308716 + 2.0 * 6.243910789489746
Epoch 160, val loss: 1.7675983905792236
Epoch 170, training loss: 14.218484878540039 = 1.7690012454986572 + 2.0 * 6.2247419357299805
Epoch 170, val loss: 1.7594190835952759
Epoch 180, training loss: 14.188238143920898 = 1.7607148885726929 + 2.0 * 6.213761806488037
Epoch 180, val loss: 1.751255989074707
Epoch 190, training loss: 14.146400451660156 = 1.7517170906066895 + 2.0 * 6.1973419189453125
Epoch 190, val loss: 1.7429711818695068
Epoch 200, training loss: 14.111519813537598 = 1.741883397102356 + 2.0 * 6.184818267822266
Epoch 200, val loss: 1.7344729900360107
Epoch 210, training loss: 14.07958984375 = 1.7308385372161865 + 2.0 * 6.174375534057617
Epoch 210, val loss: 1.725315809249878
Epoch 220, training loss: 14.055404663085938 = 1.7182599306106567 + 2.0 * 6.168572425842285
Epoch 220, val loss: 1.7151459455490112
Epoch 230, training loss: 14.01953125 = 1.7037994861602783 + 2.0 * 6.15786600112915
Epoch 230, val loss: 1.7038791179656982
Epoch 240, training loss: 13.989145278930664 = 1.6872903108596802 + 2.0 * 6.150927543640137
Epoch 240, val loss: 1.6910631656646729
Epoch 250, training loss: 13.960071563720703 = 1.6681809425354004 + 2.0 * 6.145945072174072
Epoch 250, val loss: 1.6762553453445435
Epoch 260, training loss: 13.923428535461426 = 1.6457602977752686 + 2.0 * 6.138833999633789
Epoch 260, val loss: 1.6591567993164062
Epoch 270, training loss: 13.885947227478027 = 1.6198030710220337 + 2.0 * 6.1330718994140625
Epoch 270, val loss: 1.6391258239746094
Epoch 280, training loss: 13.848685264587402 = 1.5897114276885986 + 2.0 * 6.129487037658691
Epoch 280, val loss: 1.6160001754760742
Epoch 290, training loss: 13.806029319763184 = 1.5556167364120483 + 2.0 * 6.125206470489502
Epoch 290, val loss: 1.5895473957061768
Epoch 300, training loss: 13.756651878356934 = 1.5176774263381958 + 2.0 * 6.119487285614014
Epoch 300, val loss: 1.5600841045379639
Epoch 310, training loss: 13.708236694335938 = 1.4758472442626953 + 2.0 * 6.116194725036621
Epoch 310, val loss: 1.5276893377304077
Epoch 320, training loss: 13.66199779510498 = 1.4310697317123413 + 2.0 * 6.115464210510254
Epoch 320, val loss: 1.493201494216919
Epoch 330, training loss: 13.606390953063965 = 1.3853206634521484 + 2.0 * 6.110535144805908
Epoch 330, val loss: 1.45791757106781
Epoch 340, training loss: 13.551497459411621 = 1.3392497301101685 + 2.0 * 6.106123924255371
Epoch 340, val loss: 1.4226210117340088
Epoch 350, training loss: 13.500178337097168 = 1.2937442064285278 + 2.0 * 6.103217124938965
Epoch 350, val loss: 1.3880120515823364
Epoch 360, training loss: 13.454093933105469 = 1.24965238571167 + 2.0 * 6.1022210121154785
Epoch 360, val loss: 1.3549221754074097
Epoch 370, training loss: 13.402328491210938 = 1.2073392868041992 + 2.0 * 6.097494602203369
Epoch 370, val loss: 1.3238234519958496
Epoch 380, training loss: 13.357413291931152 = 1.1665557622909546 + 2.0 * 6.095428943634033
Epoch 380, val loss: 1.294216513633728
Epoch 390, training loss: 13.312254905700684 = 1.126906156539917 + 2.0 * 6.092674255371094
Epoch 390, val loss: 1.2658413648605347
Epoch 400, training loss: 13.268754959106445 = 1.0880753993988037 + 2.0 * 6.090339660644531
Epoch 400, val loss: 1.23829185962677
Epoch 410, training loss: 13.227693557739258 = 1.0491530895233154 + 2.0 * 6.089270114898682
Epoch 410, val loss: 1.2111504077911377
Epoch 420, training loss: 13.18563461303711 = 1.0105078220367432 + 2.0 * 6.087563514709473
Epoch 420, val loss: 1.184019923210144
Epoch 430, training loss: 13.137872695922852 = 0.9717121124267578 + 2.0 * 6.083080291748047
Epoch 430, val loss: 1.1570384502410889
Epoch 440, training loss: 13.110565185546875 = 0.9328535199165344 + 2.0 * 6.088855743408203
Epoch 440, val loss: 1.1300944089889526
Epoch 450, training loss: 13.058736801147461 = 0.8951442837715149 + 2.0 * 6.081796169281006
Epoch 450, val loss: 1.1036244630813599
Epoch 460, training loss: 13.013168334960938 = 0.8581311702728271 + 2.0 * 6.077518463134766
Epoch 460, val loss: 1.0778828859329224
Epoch 470, training loss: 12.97224235534668 = 0.8220294713973999 + 2.0 * 6.075106620788574
Epoch 470, val loss: 1.0528968572616577
Epoch 480, training loss: 12.941811561584473 = 0.7870978116989136 + 2.0 * 6.077356815338135
Epoch 480, val loss: 1.028887152671814
Epoch 490, training loss: 12.910401344299316 = 0.7538233995437622 + 2.0 * 6.078289031982422
Epoch 490, val loss: 1.0066643953323364
Epoch 500, training loss: 12.866878509521484 = 0.7228230834007263 + 2.0 * 6.072027683258057
Epoch 500, val loss: 0.9860190153121948
Epoch 510, training loss: 12.832249641418457 = 0.693504273891449 + 2.0 * 6.069372653961182
Epoch 510, val loss: 0.9669888019561768
Epoch 520, training loss: 12.804046630859375 = 0.6656994819641113 + 2.0 * 6.069173812866211
Epoch 520, val loss: 0.9495300054550171
Epoch 530, training loss: 12.773180961608887 = 0.6395046710968018 + 2.0 * 6.066838264465332
Epoch 530, val loss: 0.9335758686065674
Epoch 540, training loss: 12.773310661315918 = 0.6147755980491638 + 2.0 * 6.079267501831055
Epoch 540, val loss: 0.91903156042099
Epoch 550, training loss: 12.722543716430664 = 0.5914015173912048 + 2.0 * 6.065571308135986
Epoch 550, val loss: 0.9060235023498535
Epoch 560, training loss: 12.691441535949707 = 0.5689834356307983 + 2.0 * 6.061229228973389
Epoch 560, val loss: 0.8942843079566956
Epoch 570, training loss: 12.665801048278809 = 0.5473108887672424 + 2.0 * 6.0592451095581055
Epoch 570, val loss: 0.8834472298622131
Epoch 580, training loss: 12.641427040100098 = 0.526332437992096 + 2.0 * 6.057547092437744
Epoch 580, val loss: 0.8735488653182983
Epoch 590, training loss: 12.61889934539795 = 0.5060628056526184 + 2.0 * 6.056418418884277
Epoch 590, val loss: 0.8647341132164001
Epoch 600, training loss: 12.604120254516602 = 0.4866190552711487 + 2.0 * 6.058750629425049
Epoch 600, val loss: 0.8568525910377502
Epoch 610, training loss: 12.585886001586914 = 0.4683537483215332 + 2.0 * 6.058765888214111
Epoch 610, val loss: 0.8502420783042908
Epoch 620, training loss: 12.558467864990234 = 0.45104289054870605 + 2.0 * 6.053712368011475
Epoch 620, val loss: 0.8447554707527161
Epoch 630, training loss: 12.545985221862793 = 0.4344756305217743 + 2.0 * 6.055754661560059
Epoch 630, val loss: 0.8400211930274963
Epoch 640, training loss: 12.526422500610352 = 0.4186244606971741 + 2.0 * 6.053898811340332
Epoch 640, val loss: 0.8361830115318298
Epoch 650, training loss: 12.50670051574707 = 0.4036894142627716 + 2.0 * 6.0515055656433105
Epoch 650, val loss: 0.833221971988678
Epoch 660, training loss: 12.487088203430176 = 0.3893372714519501 + 2.0 * 6.048875331878662
Epoch 660, val loss: 0.8309549689292908
Epoch 670, training loss: 12.482990264892578 = 0.37559348344802856 + 2.0 * 6.053698539733887
Epoch 670, val loss: 0.8292810916900635
Epoch 680, training loss: 12.460396766662598 = 0.36242038011550903 + 2.0 * 6.048988342285156
Epoch 680, val loss: 0.8282012939453125
Epoch 690, training loss: 12.461027145385742 = 0.3498969078063965 + 2.0 * 6.055564880371094
Epoch 690, val loss: 0.82768315076828
Epoch 700, training loss: 12.439471244812012 = 0.3378331661224365 + 2.0 * 6.050818920135498
Epoch 700, val loss: 0.8274639844894409
Epoch 710, training loss: 12.41690444946289 = 0.32641762495040894 + 2.0 * 6.045243263244629
Epoch 710, val loss: 0.8278164267539978
Epoch 720, training loss: 12.401089668273926 = 0.3153580129146576 + 2.0 * 6.042865753173828
Epoch 720, val loss: 0.8283763527870178
Epoch 730, training loss: 12.388786315917969 = 0.30464527010917664 + 2.0 * 6.042070388793945
Epoch 730, val loss: 0.8293063640594482
Epoch 740, training loss: 12.407515525817871 = 0.29427269101142883 + 2.0 * 6.056621551513672
Epoch 740, val loss: 0.8304373025894165
Epoch 750, training loss: 12.36784553527832 = 0.28442713618278503 + 2.0 * 6.0417094230651855
Epoch 750, val loss: 0.8318281769752502
Epoch 760, training loss: 12.359086990356445 = 0.2749798893928528 + 2.0 * 6.042053699493408
Epoch 760, val loss: 0.8335216641426086
Epoch 770, training loss: 12.344396591186523 = 0.2659246325492859 + 2.0 * 6.039236068725586
Epoch 770, val loss: 0.8353847861289978
Epoch 780, training loss: 12.342073440551758 = 0.257162868976593 + 2.0 * 6.042455196380615
Epoch 780, val loss: 0.8374253511428833
Epoch 790, training loss: 12.324714660644531 = 0.24867050349712372 + 2.0 * 6.038022041320801
Epoch 790, val loss: 0.8395386338233948
Epoch 800, training loss: 12.317893981933594 = 0.24051149189472198 + 2.0 * 6.03869104385376
Epoch 800, val loss: 0.8418319225311279
Epoch 810, training loss: 12.308770179748535 = 0.23270770907402039 + 2.0 * 6.038031101226807
Epoch 810, val loss: 0.844296932220459
Epoch 820, training loss: 12.302706718444824 = 0.22513562440872192 + 2.0 * 6.038785457611084
Epoch 820, val loss: 0.8468644022941589
Epoch 830, training loss: 12.299489974975586 = 0.2178923636674881 + 2.0 * 6.040798664093018
Epoch 830, val loss: 0.8495078682899475
Epoch 840, training loss: 12.283523559570312 = 0.2109876573085785 + 2.0 * 6.0362677574157715
Epoch 840, val loss: 0.8523314595222473
Epoch 850, training loss: 12.271038055419922 = 0.20427733659744263 + 2.0 * 6.033380508422852
Epoch 850, val loss: 0.8551914095878601
Epoch 860, training loss: 12.265481948852539 = 0.19780687987804413 + 2.0 * 6.03383731842041
Epoch 860, val loss: 0.858221709728241
Epoch 870, training loss: 12.264047622680664 = 0.1915574073791504 + 2.0 * 6.036245346069336
Epoch 870, val loss: 0.8612650036811829
Epoch 880, training loss: 12.256451606750488 = 0.18557773530483246 + 2.0 * 6.035437107086182
Epoch 880, val loss: 0.8644623160362244
Epoch 890, training loss: 12.248946189880371 = 0.17979788780212402 + 2.0 * 6.034574031829834
Epoch 890, val loss: 0.8675957322120667
Epoch 900, training loss: 12.23784065246582 = 0.1742752343416214 + 2.0 * 6.031782627105713
Epoch 900, val loss: 0.8708317279815674
Epoch 910, training loss: 12.228312492370605 = 0.16892662644386292 + 2.0 * 6.029693126678467
Epoch 910, val loss: 0.874183177947998
Epoch 920, training loss: 12.225576400756836 = 0.16376690566539764 + 2.0 * 6.030904769897461
Epoch 920, val loss: 0.8775442838668823
Epoch 930, training loss: 12.21728801727295 = 0.1587745100259781 + 2.0 * 6.029256820678711
Epoch 930, val loss: 0.8808131217956543
Epoch 940, training loss: 12.214091300964355 = 0.15399901568889618 + 2.0 * 6.030045986175537
Epoch 940, val loss: 0.8842701315879822
Epoch 950, training loss: 12.205916404724121 = 0.14936307072639465 + 2.0 * 6.028276443481445
Epoch 950, val loss: 0.8877975940704346
Epoch 960, training loss: 12.206920623779297 = 0.14488865435123444 + 2.0 * 6.031015872955322
Epoch 960, val loss: 0.8912060856819153
Epoch 970, training loss: 12.195521354675293 = 0.14058731496334076 + 2.0 * 6.027467250823975
Epoch 970, val loss: 0.8947489857673645
Epoch 980, training loss: 12.190232276916504 = 0.13639821112155914 + 2.0 * 6.026916980743408
Epoch 980, val loss: 0.8983154296875
Epoch 990, training loss: 12.202113151550293 = 0.13233426213264465 + 2.0 * 6.034889221191406
Epoch 990, val loss: 0.901813805103302
=== picking the best model according to the performance on validation ===
target class rate on Vs: 0.8000
accuracy on clean test nodes: 0.7852
Overall ASR: 0.2694
Flip ASR: 0.2978/225 nodes
The final ASR:0.51907, 0.22353, Accuracy:0.79383, 0.00972
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11612])
remove edge: torch.Size([2, 9468])
updated graph: torch.Size([2, 10524])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9852
Flip ASR: 0.9822/225 nodes
The final ASR:0.98032, 0.00348, Accuracy:0.83333, 0.00302
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.3 der2:0.4 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 18.699764251708984 = 1.9520306587219238 + 2.0 * 8.37386703491211
Epoch 0, val loss: 1.9479860067367554
Epoch 10, training loss: 18.687931060791016 = 1.9411062002182007 + 2.0 * 8.373412132263184
Epoch 10, val loss: 1.9380311965942383
Epoch 20, training loss: 18.66799545288086 = 1.9276973009109497 + 2.0 * 8.370148658752441
Epoch 20, val loss: 1.9254534244537354
Epoch 30, training loss: 18.605806350708008 = 1.9091535806655884 + 2.0 * 8.348326683044434
Epoch 30, val loss: 1.9079688787460327
Epoch 40, training loss: 18.293682098388672 = 1.8872591257095337 + 2.0 * 8.203211784362793
Epoch 40, val loss: 1.8879090547561646
Epoch 50, training loss: 17.214622497558594 = 1.8634315729141235 + 2.0 * 7.675595283508301
Epoch 50, val loss: 1.8657770156860352
Epoch 60, training loss: 16.32671546936035 = 1.8441039323806763 + 2.0 * 7.241305828094482
Epoch 60, val loss: 1.8490315675735474
Epoch 70, training loss: 15.701107025146484 = 1.8322010040283203 + 2.0 * 6.934453010559082
Epoch 70, val loss: 1.8391236066818237
Epoch 80, training loss: 15.333508491516113 = 1.8185917139053345 + 2.0 * 6.757458209991455
Epoch 80, val loss: 1.8274285793304443
Epoch 90, training loss: 15.031286239624023 = 1.8045775890350342 + 2.0 * 6.613354206085205
Epoch 90, val loss: 1.8151265382766724
Epoch 100, training loss: 14.850440979003906 = 1.7910104990005493 + 2.0 * 6.529715061187744
Epoch 100, val loss: 1.8034675121307373
Epoch 110, training loss: 14.713601112365723 = 1.7763007879257202 + 2.0 * 6.4686503410339355
Epoch 110, val loss: 1.7907651662826538
Epoch 120, training loss: 14.598546981811523 = 1.7610355615615845 + 2.0 * 6.418755531311035
Epoch 120, val loss: 1.7774755954742432
Epoch 130, training loss: 14.503929138183594 = 1.7455686330795288 + 2.0 * 6.379180431365967
Epoch 130, val loss: 1.7639540433883667
Epoch 140, training loss: 14.424888610839844 = 1.7288540601730347 + 2.0 * 6.34801721572876
Epoch 140, val loss: 1.7494323253631592
Epoch 150, training loss: 14.348952293395996 = 1.7103431224822998 + 2.0 * 6.319304466247559
Epoch 150, val loss: 1.7336326837539673
Epoch 160, training loss: 14.277305603027344 = 1.6898725032806396 + 2.0 * 6.2937164306640625
Epoch 160, val loss: 1.7164829969406128
Epoch 170, training loss: 14.212002754211426 = 1.6668707132339478 + 2.0 * 6.272565841674805
Epoch 170, val loss: 1.6974869966506958
Epoch 180, training loss: 14.15218448638916 = 1.6409633159637451 + 2.0 * 6.255610466003418
Epoch 180, val loss: 1.6762412786483765
Epoch 190, training loss: 14.091172218322754 = 1.6119271516799927 + 2.0 * 6.239622592926025
Epoch 190, val loss: 1.6525954008102417
Epoch 200, training loss: 14.037662506103516 = 1.5793824195861816 + 2.0 * 6.229140281677246
Epoch 200, val loss: 1.6263362169265747
Epoch 210, training loss: 13.974735260009766 = 1.5438932180404663 + 2.0 * 6.215421199798584
Epoch 210, val loss: 1.597909927368164
Epoch 220, training loss: 13.916666030883789 = 1.50558602809906 + 2.0 * 6.205540180206299
Epoch 220, val loss: 1.567580223083496
Epoch 230, training loss: 13.862659454345703 = 1.4645798206329346 + 2.0 * 6.199039936065674
Epoch 230, val loss: 1.5356385707855225
Epoch 240, training loss: 13.798585891723633 = 1.4216703176498413 + 2.0 * 6.18845796585083
Epoch 240, val loss: 1.5028878450393677
Epoch 250, training loss: 13.739594459533691 = 1.377362608909607 + 2.0 * 6.181116104125977
Epoch 250, val loss: 1.4696837663650513
Epoch 260, training loss: 13.685033798217773 = 1.3321537971496582 + 2.0 * 6.1764397621154785
Epoch 260, val loss: 1.4366005659103394
Epoch 270, training loss: 13.623597145080566 = 1.2871462106704712 + 2.0 * 6.168225288391113
Epoch 270, val loss: 1.404320240020752
Epoch 280, training loss: 13.570145606994629 = 1.2425600290298462 + 2.0 * 6.163792610168457
Epoch 280, val loss: 1.3730642795562744
Epoch 290, training loss: 13.515028953552246 = 1.1992321014404297 + 2.0 * 6.157898426055908
Epoch 290, val loss: 1.3432167768478394
Epoch 300, training loss: 13.46063232421875 = 1.1573312282562256 + 2.0 * 6.151650428771973
Epoch 300, val loss: 1.3148881196975708
Epoch 310, training loss: 13.414225578308105 = 1.1168463230133057 + 2.0 * 6.1486897468566895
Epoch 310, val loss: 1.288004755973816
Epoch 320, training loss: 13.363011360168457 = 1.0781967639923096 + 2.0 * 6.142407417297363
Epoch 320, val loss: 1.2626758813858032
Epoch 330, training loss: 13.316481590270996 = 1.041459321975708 + 2.0 * 6.137511253356934
Epoch 330, val loss: 1.2389198541641235
Epoch 340, training loss: 13.273006439208984 = 1.0062336921691895 + 2.0 * 6.133386611938477
Epoch 340, val loss: 1.2164585590362549
Epoch 350, training loss: 13.2351655960083 = 0.972635805606842 + 2.0 * 6.131264686584473
Epoch 350, val loss: 1.195347785949707
Epoch 360, training loss: 13.191621780395508 = 0.9406683444976807 + 2.0 * 6.125476837158203
Epoch 360, val loss: 1.1758105754852295
Epoch 370, training loss: 13.15350341796875 = 0.9100068211555481 + 2.0 * 6.121748447418213
Epoch 370, val loss: 1.1577166318893433
Epoch 380, training loss: 13.117623329162598 = 0.8806040287017822 + 2.0 * 6.118509769439697
Epoch 380, val loss: 1.1410785913467407
Epoch 390, training loss: 13.084911346435547 = 0.8524863719940186 + 2.0 * 6.116212368011475
Epoch 390, val loss: 1.12594735622406
Epoch 400, training loss: 13.049592018127441 = 0.8254918456077576 + 2.0 * 6.1120500564575195
Epoch 400, val loss: 1.1123390197753906
Epoch 410, training loss: 13.016833305358887 = 0.799412190914154 + 2.0 * 6.108710765838623
Epoch 410, val loss: 1.1000739336013794
Epoch 420, training loss: 12.984620094299316 = 0.7741504311561584 + 2.0 * 6.105234622955322
Epoch 420, val loss: 1.0891456604003906
Epoch 430, training loss: 12.95644760131836 = 0.7495642304420471 + 2.0 * 6.1034417152404785
Epoch 430, val loss: 1.0794419050216675
Epoch 440, training loss: 12.92988395690918 = 0.7256354093551636 + 2.0 * 6.102124214172363
Epoch 440, val loss: 1.0708247423171997
Epoch 450, training loss: 12.897750854492188 = 0.7025341987609863 + 2.0 * 6.0976080894470215
Epoch 450, val loss: 1.0633922815322876
Epoch 460, training loss: 12.870870590209961 = 0.6800201535224915 + 2.0 * 6.095425128936768
Epoch 460, val loss: 1.0569337606430054
Epoch 470, training loss: 12.846844673156738 = 0.6580865383148193 + 2.0 * 6.09437894821167
Epoch 470, val loss: 1.0514185428619385
Epoch 480, training loss: 12.819735527038574 = 0.6368322968482971 + 2.0 * 6.091451644897461
Epoch 480, val loss: 1.046723484992981
Epoch 490, training loss: 12.792667388916016 = 0.6161544322967529 + 2.0 * 6.088256359100342
Epoch 490, val loss: 1.0427840948104858
Epoch 500, training loss: 12.770489692687988 = 0.5960230231285095 + 2.0 * 6.087233543395996
Epoch 500, val loss: 1.0394610166549683
Epoch 510, training loss: 12.746509552001953 = 0.5765396356582642 + 2.0 * 6.08498477935791
Epoch 510, val loss: 1.0366586446762085
Epoch 520, training loss: 12.722148895263672 = 0.5574791431427002 + 2.0 * 6.082334995269775
Epoch 520, val loss: 1.0343178510665894
Epoch 530, training loss: 12.717740058898926 = 0.538808286190033 + 2.0 * 6.089466094970703
Epoch 530, val loss: 1.0322479009628296
Epoch 540, training loss: 12.680174827575684 = 0.5205705165863037 + 2.0 * 6.0798020362854
Epoch 540, val loss: 1.030420184135437
Epoch 550, training loss: 12.655865669250488 = 0.5025761723518372 + 2.0 * 6.0766448974609375
Epoch 550, val loss: 1.0287803411483765
Epoch 560, training loss: 12.635383605957031 = 0.484744131565094 + 2.0 * 6.075319766998291
Epoch 560, val loss: 1.0271000862121582
Epoch 570, training loss: 12.615073204040527 = 0.4671522080898285 + 2.0 * 6.073960304260254
Epoch 570, val loss: 1.025577187538147
Epoch 580, training loss: 12.597789764404297 = 0.4498558044433594 + 2.0 * 6.073966979980469
Epoch 580, val loss: 1.0241279602050781
Epoch 590, training loss: 12.575241088867188 = 0.4328036308288574 + 2.0 * 6.071218967437744
Epoch 590, val loss: 1.0227893590927124
Epoch 600, training loss: 12.553897857666016 = 0.41602623462677 + 2.0 * 6.068935871124268
Epoch 600, val loss: 1.0215950012207031
Epoch 610, training loss: 12.544984817504883 = 0.3996238112449646 + 2.0 * 6.072680473327637
Epoch 610, val loss: 1.0205928087234497
Epoch 620, training loss: 12.521907806396484 = 0.3836190700531006 + 2.0 * 6.069144248962402
Epoch 620, val loss: 1.0198211669921875
Epoch 630, training loss: 12.498693466186523 = 0.3681560754776001 + 2.0 * 6.065268516540527
Epoch 630, val loss: 1.019363522529602
Epoch 640, training loss: 12.485270500183105 = 0.3532426357269287 + 2.0 * 6.066013813018799
Epoch 640, val loss: 1.0192296504974365
Epoch 650, training loss: 12.462791442871094 = 0.33889174461364746 + 2.0 * 6.061949729919434
Epoch 650, val loss: 1.019405722618103
Epoch 660, training loss: 12.448237419128418 = 0.3250470459461212 + 2.0 * 6.0615949630737305
Epoch 660, val loss: 1.0199943780899048
Epoch 670, training loss: 12.439022064208984 = 0.3117820918560028 + 2.0 * 6.063620090484619
Epoch 670, val loss: 1.020912766456604
Epoch 680, training loss: 12.419095039367676 = 0.29904815554618835 + 2.0 * 6.060023307800293
Epoch 680, val loss: 1.0221192836761475
Epoch 690, training loss: 12.401671409606934 = 0.2868349552154541 + 2.0 * 6.057418346405029
Epoch 690, val loss: 1.0235847234725952
Epoch 700, training loss: 12.386750221252441 = 0.27499186992645264 + 2.0 * 6.05587911605835
Epoch 700, val loss: 1.0252336263656616
Epoch 710, training loss: 12.375434875488281 = 0.26350149512290955 + 2.0 * 6.055966854095459
Epoch 710, val loss: 1.0270967483520508
Epoch 720, training loss: 12.37077522277832 = 0.2523006498813629 + 2.0 * 6.059237480163574
Epoch 720, val loss: 1.0290409326553345
Epoch 730, training loss: 12.354330062866211 = 0.24143588542938232 + 2.0 * 6.0564470291137695
Epoch 730, val loss: 1.0311201810836792
Epoch 740, training loss: 12.335988998413086 = 0.23088118433952332 + 2.0 * 6.052554130554199
Epoch 740, val loss: 1.0332077741622925
Epoch 750, training loss: 12.321426391601562 = 0.2205536812543869 + 2.0 * 6.050436496734619
Epoch 750, val loss: 1.0354290008544922
Epoch 760, training loss: 12.312366485595703 = 0.21043284237384796 + 2.0 * 6.050966739654541
Epoch 760, val loss: 1.037768840789795
Epoch 770, training loss: 12.301604270935059 = 0.200577974319458 + 2.0 * 6.05051326751709
Epoch 770, val loss: 1.0401954650878906
Epoch 780, training loss: 12.287784576416016 = 0.19100186228752136 + 2.0 * 6.048391342163086
Epoch 780, val loss: 1.0428752899169922
Epoch 790, training loss: 12.27454662322998 = 0.18173550069332123 + 2.0 * 6.046405792236328
Epoch 790, val loss: 1.045641541481018
Epoch 800, training loss: 12.267090797424316 = 0.1727755218744278 + 2.0 * 6.0471577644348145
Epoch 800, val loss: 1.04867422580719
Epoch 810, training loss: 12.254949569702148 = 0.16415368020534515 + 2.0 * 6.045397758483887
Epoch 810, val loss: 1.0518317222595215
Epoch 820, training loss: 12.247657775878906 = 0.15591613948345184 + 2.0 * 6.045870780944824
Epoch 820, val loss: 1.0552924871444702
Epoch 830, training loss: 12.234769821166992 = 0.14809772372245789 + 2.0 * 6.043335914611816
Epoch 830, val loss: 1.059051752090454
Epoch 840, training loss: 12.226036071777344 = 0.14065493643283844 + 2.0 * 6.042690753936768
Epoch 840, val loss: 1.0629876852035522
Epoch 850, training loss: 12.220855712890625 = 0.13360120356082916 + 2.0 * 6.0436272621154785
Epoch 850, val loss: 1.0671902894973755
Epoch 860, training loss: 12.212801933288574 = 0.126931294798851 + 2.0 * 6.042935371398926
Epoch 860, val loss: 1.0715975761413574
Epoch 870, training loss: 12.199957847595215 = 0.12067186832427979 + 2.0 * 6.039642810821533
Epoch 870, val loss: 1.0763779878616333
Epoch 880, training loss: 12.192811012268066 = 0.11477519571781158 + 2.0 * 6.039017677307129
Epoch 880, val loss: 1.08126962184906
Epoch 890, training loss: 12.195413589477539 = 0.10922767221927643 + 2.0 * 6.043092727661133
Epoch 890, val loss: 1.0864201784133911
Epoch 900, training loss: 12.18147087097168 = 0.10401847958564758 + 2.0 * 6.038726329803467
Epoch 900, val loss: 1.0917558670043945
Epoch 910, training loss: 12.173163414001465 = 0.0991423949599266 + 2.0 * 6.037010669708252
Epoch 910, val loss: 1.0973519086837769
Epoch 920, training loss: 12.170462608337402 = 0.09455931186676025 + 2.0 * 6.037951469421387
Epoch 920, val loss: 1.1029645204544067
Epoch 930, training loss: 12.160609245300293 = 0.09026402980089188 + 2.0 * 6.035172462463379
Epoch 930, val loss: 1.108833909034729
Epoch 940, training loss: 12.1544189453125 = 0.08620209246873856 + 2.0 * 6.034108638763428
Epoch 940, val loss: 1.1148351430892944
Epoch 950, training loss: 12.14833927154541 = 0.08235763758420944 + 2.0 * 6.0329909324646
Epoch 950, val loss: 1.1209584474563599
Epoch 960, training loss: 12.154239654541016 = 0.07874849438667297 + 2.0 * 6.037745475769043
Epoch 960, val loss: 1.127099633216858
Epoch 970, training loss: 12.151503562927246 = 0.07536192238330841 + 2.0 * 6.0380706787109375
Epoch 970, val loss: 1.1333264112472534
Epoch 980, training loss: 12.135753631591797 = 0.07218370586633682 + 2.0 * 6.031785011291504
Epoch 980, val loss: 1.1396398544311523
Epoch 990, training loss: 12.13094425201416 = 0.06918127089738846 + 2.0 * 6.030881404876709
Epoch 990, val loss: 1.1459219455718994
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7481
Overall ASR: 0.5830
Flip ASR: 0.5111/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.69679069519043 = 1.949102520942688 + 2.0 * 8.373844146728516
Epoch 0, val loss: 1.9475895166397095
Epoch 10, training loss: 18.684465408325195 = 1.938624620437622 + 2.0 * 8.372920036315918
Epoch 10, val loss: 1.9372944831848145
Epoch 20, training loss: 18.660446166992188 = 1.9258617162704468 + 2.0 * 8.367292404174805
Epoch 20, val loss: 1.9239013195037842
Epoch 30, training loss: 18.584434509277344 = 1.9090473651885986 + 2.0 * 8.337693214416504
Epoch 30, val loss: 1.9057445526123047
Epoch 40, training loss: 18.187530517578125 = 1.8906270265579224 + 2.0 * 8.148451805114746
Epoch 40, val loss: 1.8863551616668701
Epoch 50, training loss: 16.779647827148438 = 1.8715649843215942 + 2.0 * 7.454041481018066
Epoch 50, val loss: 1.8655173778533936
Epoch 60, training loss: 16.37171745300293 = 1.8516343832015991 + 2.0 * 7.260041236877441
Epoch 60, val loss: 1.8446706533432007
Epoch 70, training loss: 15.959842681884766 = 1.8353818655014038 + 2.0 * 7.062230587005615
Epoch 70, val loss: 1.8288053274154663
Epoch 80, training loss: 15.505314826965332 = 1.8212610483169556 + 2.0 * 6.842026710510254
Epoch 80, val loss: 1.8151838779449463
Epoch 90, training loss: 15.084321022033691 = 1.8094698190689087 + 2.0 * 6.637425422668457
Epoch 90, val loss: 1.8040324449539185
Epoch 100, training loss: 14.863690376281738 = 1.797377347946167 + 2.0 * 6.533156394958496
Epoch 100, val loss: 1.7923921346664429
Epoch 110, training loss: 14.701915740966797 = 1.7828729152679443 + 2.0 * 6.459521293640137
Epoch 110, val loss: 1.7787513732910156
Epoch 120, training loss: 14.583006858825684 = 1.7680212259292603 + 2.0 * 6.407492637634277
Epoch 120, val loss: 1.7650065422058105
Epoch 130, training loss: 14.49863052368164 = 1.7531026601791382 + 2.0 * 6.3727641105651855
Epoch 130, val loss: 1.7514551877975464
Epoch 140, training loss: 14.414745330810547 = 1.7376329898834229 + 2.0 * 6.338556289672852
Epoch 140, val loss: 1.7374881505966187
Epoch 150, training loss: 14.34493637084961 = 1.7208898067474365 + 2.0 * 6.312023162841797
Epoch 150, val loss: 1.723145842552185
Epoch 160, training loss: 14.28208065032959 = 1.702606439590454 + 2.0 * 6.289737224578857
Epoch 160, val loss: 1.70793616771698
Epoch 170, training loss: 14.22231674194336 = 1.6820578575134277 + 2.0 * 6.270129680633545
Epoch 170, val loss: 1.6913241147994995
Epoch 180, training loss: 14.165313720703125 = 1.6585899591445923 + 2.0 * 6.253361701965332
Epoch 180, val loss: 1.6728112697601318
Epoch 190, training loss: 14.109039306640625 = 1.632025957107544 + 2.0 * 6.23850679397583
Epoch 190, val loss: 1.652052879333496
Epoch 200, training loss: 14.05439567565918 = 1.6020989418029785 + 2.0 * 6.22614860534668
Epoch 200, val loss: 1.6289169788360596
Epoch 210, training loss: 13.997836112976074 = 1.5686525106430054 + 2.0 * 6.214591979980469
Epoch 210, val loss: 1.6031478643417358
Epoch 220, training loss: 13.939757347106934 = 1.5315172672271729 + 2.0 * 6.20412015914917
Epoch 220, val loss: 1.574775218963623
Epoch 230, training loss: 13.88818359375 = 1.491438388824463 + 2.0 * 6.1983723640441895
Epoch 230, val loss: 1.544766902923584
Epoch 240, training loss: 13.826372146606445 = 1.4501463174819946 + 2.0 * 6.188112735748291
Epoch 240, val loss: 1.5136363506317139
Epoch 250, training loss: 13.769787788391113 = 1.407795786857605 + 2.0 * 6.180995941162109
Epoch 250, val loss: 1.4827114343643188
Epoch 260, training loss: 13.713897705078125 = 1.3651024103164673 + 2.0 * 6.1743974685668945
Epoch 260, val loss: 1.451432228088379
Epoch 270, training loss: 13.662178039550781 = 1.3229364156723022 + 2.0 * 6.169620990753174
Epoch 270, val loss: 1.4210617542266846
Epoch 280, training loss: 13.6133451461792 = 1.2826135158538818 + 2.0 * 6.165365695953369
Epoch 280, val loss: 1.3928287029266357
Epoch 290, training loss: 13.56228256225586 = 1.2447619438171387 + 2.0 * 6.158760070800781
Epoch 290, val loss: 1.3666877746582031
Epoch 300, training loss: 13.516901969909668 = 1.2088136672973633 + 2.0 * 6.154044151306152
Epoch 300, val loss: 1.3423341512680054
Epoch 310, training loss: 13.477489471435547 = 1.17451810836792 + 2.0 * 6.151485443115234
Epoch 310, val loss: 1.3195520639419556
Epoch 320, training loss: 13.435205459594727 = 1.1419428586959839 + 2.0 * 6.146631240844727
Epoch 320, val loss: 1.2979795932769775
Epoch 330, training loss: 13.392372131347656 = 1.1101847887039185 + 2.0 * 6.141093730926514
Epoch 330, val loss: 1.2771568298339844
Epoch 340, training loss: 13.353053092956543 = 1.078817367553711 + 2.0 * 6.137117862701416
Epoch 340, val loss: 1.2566238641738892
Epoch 350, training loss: 13.313467979431152 = 1.0478078126907349 + 2.0 * 6.1328301429748535
Epoch 350, val loss: 1.2362631559371948
Epoch 360, training loss: 13.276408195495605 = 1.016957402229309 + 2.0 * 6.129725456237793
Epoch 360, val loss: 1.2160639762878418
Epoch 370, training loss: 13.237512588500977 = 0.9860131144523621 + 2.0 * 6.125749588012695
Epoch 370, val loss: 1.195692539215088
Epoch 380, training loss: 13.222557067871094 = 0.9550122618675232 + 2.0 * 6.133772373199463
Epoch 380, val loss: 1.174909234046936
Epoch 390, training loss: 13.16268253326416 = 0.9239384531974792 + 2.0 * 6.1193718910217285
Epoch 390, val loss: 1.1541889905929565
Epoch 400, training loss: 13.126553535461426 = 0.8929631114006042 + 2.0 * 6.116795063018799
Epoch 400, val loss: 1.1332476139068604
Epoch 410, training loss: 13.088653564453125 = 0.8617401123046875 + 2.0 * 6.113456726074219
Epoch 410, val loss: 1.1120773553848267
Epoch 420, training loss: 13.052972793579102 = 0.830187201499939 + 2.0 * 6.111392974853516
Epoch 420, val loss: 1.0906612873077393
Epoch 430, training loss: 13.02230453491211 = 0.7986745834350586 + 2.0 * 6.111814975738525
Epoch 430, val loss: 1.069034218788147
Epoch 440, training loss: 12.980569839477539 = 0.7673813104629517 + 2.0 * 6.106594085693359
Epoch 440, val loss: 1.0478919744491577
Epoch 450, training loss: 12.942939758300781 = 0.7363206148147583 + 2.0 * 6.103309631347656
Epoch 450, val loss: 1.027134656906128
Epoch 460, training loss: 12.925008773803711 = 0.7057058811187744 + 2.0 * 6.109651565551758
Epoch 460, val loss: 1.00691556930542
Epoch 470, training loss: 12.879178047180176 = 0.6760455965995789 + 2.0 * 6.101566314697266
Epoch 470, val loss: 0.9876260161399841
Epoch 480, training loss: 12.84227466583252 = 0.6473804712295532 + 2.0 * 6.097446918487549
Epoch 480, val loss: 0.9695088863372803
Epoch 490, training loss: 12.809504508972168 = 0.6196289658546448 + 2.0 * 6.094937801361084
Epoch 490, val loss: 0.9525523781776428
Epoch 500, training loss: 12.789702415466309 = 0.5929115414619446 + 2.0 * 6.098395347595215
Epoch 500, val loss: 0.9368647336959839
Epoch 510, training loss: 12.752756118774414 = 0.5673178434371948 + 2.0 * 6.092719078063965
Epoch 510, val loss: 0.9226489067077637
Epoch 520, training loss: 12.723512649536133 = 0.542961597442627 + 2.0 * 6.090275287628174
Epoch 520, val loss: 0.9098947048187256
Epoch 530, training loss: 12.696880340576172 = 0.5196180939674377 + 2.0 * 6.0886311531066895
Epoch 530, val loss: 0.8986060619354248
Epoch 540, training loss: 12.674506187438965 = 0.4972633123397827 + 2.0 * 6.088621616363525
Epoch 540, val loss: 0.8885723948478699
Epoch 550, training loss: 12.644686698913574 = 0.47573909163475037 + 2.0 * 6.084473609924316
Epoch 550, val loss: 0.8797892332077026
Epoch 560, training loss: 12.621939659118652 = 0.4550642669200897 + 2.0 * 6.083437919616699
Epoch 560, val loss: 0.8722956776618958
Epoch 570, training loss: 12.604297637939453 = 0.43514010310173035 + 2.0 * 6.084578990936279
Epoch 570, val loss: 0.8658959865570068
Epoch 580, training loss: 12.575787544250488 = 0.41600432991981506 + 2.0 * 6.079891681671143
Epoch 580, val loss: 0.8606155514717102
Epoch 590, training loss: 12.555084228515625 = 0.3976888060569763 + 2.0 * 6.078697681427002
Epoch 590, val loss: 0.8564634919166565
Epoch 600, training loss: 12.5347261428833 = 0.37995070219039917 + 2.0 * 6.077387809753418
Epoch 600, val loss: 0.853330671787262
Epoch 610, training loss: 12.513764381408691 = 0.3628776967525482 + 2.0 * 6.075443267822266
Epoch 610, val loss: 0.8508878946304321
Epoch 620, training loss: 12.497843742370605 = 0.34646129608154297 + 2.0 * 6.075691223144531
Epoch 620, val loss: 0.8495793342590332
Epoch 630, training loss: 12.474800109863281 = 0.33072882890701294 + 2.0 * 6.072035789489746
Epoch 630, val loss: 0.8489356637001038
Epoch 640, training loss: 12.467369079589844 = 0.31560608744621277 + 2.0 * 6.075881481170654
Epoch 640, val loss: 0.8490339517593384
Epoch 650, training loss: 12.440984725952148 = 0.30110418796539307 + 2.0 * 6.069940090179443
Epoch 650, val loss: 0.8499488830566406
Epoch 660, training loss: 12.431217193603516 = 0.28730154037475586 + 2.0 * 6.071958065032959
Epoch 660, val loss: 0.8515915870666504
Epoch 670, training loss: 12.411059379577637 = 0.274123877286911 + 2.0 * 6.068467617034912
Epoch 670, val loss: 0.8539166450500488
Epoch 680, training loss: 12.394134521484375 = 0.2616147994995117 + 2.0 * 6.066259860992432
Epoch 680, val loss: 0.8569355010986328
Epoch 690, training loss: 12.382365226745605 = 0.249623641371727 + 2.0 * 6.066370964050293
Epoch 690, val loss: 0.8605074286460876
Epoch 700, training loss: 12.377053260803223 = 0.2382950484752655 + 2.0 * 6.0693793296813965
Epoch 700, val loss: 0.8644603490829468
Epoch 710, training loss: 12.35434627532959 = 0.22757002711296082 + 2.0 * 6.063388347625732
Epoch 710, val loss: 0.8690453767776489
Epoch 720, training loss: 12.341787338256836 = 0.21744020283222198 + 2.0 * 6.062173366546631
Epoch 720, val loss: 0.8741814494132996
Epoch 730, training loss: 12.33130931854248 = 0.20780763030052185 + 2.0 * 6.061750888824463
Epoch 730, val loss: 0.8796467185020447
Epoch 740, training loss: 12.317166328430176 = 0.19871503114700317 + 2.0 * 6.059225559234619
Epoch 740, val loss: 0.8853967189788818
Epoch 750, training loss: 12.308798789978027 = 0.19012482464313507 + 2.0 * 6.059337139129639
Epoch 750, val loss: 0.8916947841644287
Epoch 760, training loss: 12.297811508178711 = 0.1819918304681778 + 2.0 * 6.057909965515137
Epoch 760, val loss: 0.8983662128448486
Epoch 770, training loss: 12.301078796386719 = 0.17428219318389893 + 2.0 * 6.063398361206055
Epoch 770, val loss: 0.9052623510360718
Epoch 780, training loss: 12.282309532165527 = 0.1669415831565857 + 2.0 * 6.057683944702148
Epoch 780, val loss: 0.9122748970985413
Epoch 790, training loss: 12.27169132232666 = 0.16001132130622864 + 2.0 * 6.055840015411377
Epoch 790, val loss: 0.9198330640792847
Epoch 800, training loss: 12.262101173400879 = 0.15340979397296906 + 2.0 * 6.054345607757568
Epoch 800, val loss: 0.9274570345878601
Epoch 810, training loss: 12.255392074584961 = 0.14712974429130554 + 2.0 * 6.054131031036377
Epoch 810, val loss: 0.9353049993515015
Epoch 820, training loss: 12.25133991241455 = 0.14114585518836975 + 2.0 * 6.0550971031188965
Epoch 820, val loss: 0.9432357549667358
Epoch 830, training loss: 12.238450050354004 = 0.1354495733976364 + 2.0 * 6.05150032043457
Epoch 830, val loss: 0.9512805938720703
Epoch 840, training loss: 12.237635612487793 = 0.1300441175699234 + 2.0 * 6.05379581451416
Epoch 840, val loss: 0.9595746994018555
Epoch 850, training loss: 12.229787826538086 = 0.1248803585767746 + 2.0 * 6.052453517913818
Epoch 850, val loss: 0.9676029682159424
Epoch 860, training loss: 12.217693328857422 = 0.11997327953577042 + 2.0 * 6.0488600730896
Epoch 860, val loss: 0.9759994149208069
Epoch 870, training loss: 12.210883140563965 = 0.11525464802980423 + 2.0 * 6.04781436920166
Epoch 870, val loss: 0.9844736456871033
Epoch 880, training loss: 12.204678535461426 = 0.1107177883386612 + 2.0 * 6.046980381011963
Epoch 880, val loss: 0.9929410815238953
Epoch 890, training loss: 12.21786117553711 = 0.10639192909002304 + 2.0 * 6.055734634399414
Epoch 890, val loss: 1.0014690160751343
Epoch 900, training loss: 12.197135925292969 = 0.10222215205430984 + 2.0 * 6.047456741333008
Epoch 900, val loss: 1.0098956823349
Epoch 910, training loss: 12.190298080444336 = 0.09821230173110962 + 2.0 * 6.0460429191589355
Epoch 910, val loss: 1.0186614990234375
Epoch 920, training loss: 12.185441970825195 = 0.09434372186660767 + 2.0 * 6.045548915863037
Epoch 920, val loss: 1.0273782014846802
Epoch 930, training loss: 12.17809772491455 = 0.09061041474342346 + 2.0 * 6.04374361038208
Epoch 930, val loss: 1.0360970497131348
Epoch 940, training loss: 12.178834915161133 = 0.08699662983417511 + 2.0 * 6.045918941497803
Epoch 940, val loss: 1.0448182821273804
Epoch 950, training loss: 12.171391487121582 = 0.0835108831524849 + 2.0 * 6.04394006729126
Epoch 950, val loss: 1.0536235570907593
Epoch 960, training loss: 12.169726371765137 = 0.08012688905000687 + 2.0 * 6.0447998046875
Epoch 960, val loss: 1.062369465827942
Epoch 970, training loss: 12.160531997680664 = 0.07685906440019608 + 2.0 * 6.041836261749268
Epoch 970, val loss: 1.0711523294448853
Epoch 980, training loss: 12.157472610473633 = 0.07367125898599625 + 2.0 * 6.041900634765625
Epoch 980, val loss: 1.0800726413726807
Epoch 990, training loss: 12.150254249572754 = 0.07056628912687302 + 2.0 * 6.039844036102295
Epoch 990, val loss: 1.088964581489563
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7593
Overall ASR: 0.1771
Flip ASR: 0.1911/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.68244743347168 = 1.9347312450408936 + 2.0 * 8.373858451843262
Epoch 0, val loss: 1.9419745206832886
Epoch 10, training loss: 18.67096710205078 = 1.9243963956832886 + 2.0 * 8.373285293579102
Epoch 10, val loss: 1.9312807321548462
Epoch 20, training loss: 18.649248123168945 = 1.9115620851516724 + 2.0 * 8.368843078613281
Epoch 20, val loss: 1.9179260730743408
Epoch 30, training loss: 18.564165115356445 = 1.8945739269256592 + 2.0 * 8.334795951843262
Epoch 30, val loss: 1.9003499746322632
Epoch 40, training loss: 18.099607467651367 = 1.8759479522705078 + 2.0 * 8.11182975769043
Epoch 40, val loss: 1.8819060325622559
Epoch 50, training loss: 17.286081314086914 = 1.856167197227478 + 2.0 * 7.714957237243652
Epoch 50, val loss: 1.8628098964691162
Epoch 60, training loss: 16.487096786499023 = 1.839085578918457 + 2.0 * 7.324005603790283
Epoch 60, val loss: 1.8473584651947021
Epoch 70, training loss: 15.615808486938477 = 1.8256360292434692 + 2.0 * 6.895086288452148
Epoch 70, val loss: 1.8345345258712769
Epoch 80, training loss: 15.194106101989746 = 1.8138532638549805 + 2.0 * 6.690126419067383
Epoch 80, val loss: 1.822965145111084
Epoch 90, training loss: 14.943016052246094 = 1.8002326488494873 + 2.0 * 6.571391582489014
Epoch 90, val loss: 1.8103456497192383
Epoch 100, training loss: 14.76850414276123 = 1.7858922481536865 + 2.0 * 6.491305828094482
Epoch 100, val loss: 1.7969883680343628
Epoch 110, training loss: 14.641895294189453 = 1.771291732788086 + 2.0 * 6.435301780700684
Epoch 110, val loss: 1.7833404541015625
Epoch 120, training loss: 14.541767120361328 = 1.7560497522354126 + 2.0 * 6.392858505249023
Epoch 120, val loss: 1.7694932222366333
Epoch 130, training loss: 14.455549240112305 = 1.7394022941589355 + 2.0 * 6.3580732345581055
Epoch 130, val loss: 1.7549548149108887
Epoch 140, training loss: 14.381288528442383 = 1.7208802700042725 + 2.0 * 6.330204010009766
Epoch 140, val loss: 1.7392789125442505
Epoch 150, training loss: 14.31036376953125 = 1.7000828981399536 + 2.0 * 6.305140495300293
Epoch 150, val loss: 1.7220441102981567
Epoch 160, training loss: 14.248534202575684 = 1.6765414476394653 + 2.0 * 6.285996437072754
Epoch 160, val loss: 1.7027636766433716
Epoch 170, training loss: 14.188940048217773 = 1.6499416828155518 + 2.0 * 6.2694993019104
Epoch 170, val loss: 1.6810057163238525
Epoch 180, training loss: 14.128724098205566 = 1.6197845935821533 + 2.0 * 6.254469871520996
Epoch 180, val loss: 1.656635046005249
Epoch 190, training loss: 14.067305564880371 = 1.5859533548355103 + 2.0 * 6.240675926208496
Epoch 190, val loss: 1.6293351650238037
Epoch 200, training loss: 14.01037883758545 = 1.5480515956878662 + 2.0 * 6.231163501739502
Epoch 200, val loss: 1.5988600254058838
Epoch 210, training loss: 13.947534561157227 = 1.5062594413757324 + 2.0 * 6.220637321472168
Epoch 210, val loss: 1.5653094053268433
Epoch 220, training loss: 13.882864952087402 = 1.460748314857483 + 2.0 * 6.211058139801025
Epoch 220, val loss: 1.5288913249969482
Epoch 230, training loss: 13.817879676818848 = 1.4117848873138428 + 2.0 * 6.203047275543213
Epoch 230, val loss: 1.4898955821990967
Epoch 240, training loss: 13.755390167236328 = 1.3599928617477417 + 2.0 * 6.197698593139648
Epoch 240, val loss: 1.4487895965576172
Epoch 250, training loss: 13.687246322631836 = 1.3064885139465332 + 2.0 * 6.190378665924072
Epoch 250, val loss: 1.4064160585403442
Epoch 260, training loss: 13.623361587524414 = 1.2520580291748047 + 2.0 * 6.185651779174805
Epoch 260, val loss: 1.3637069463729858
Epoch 270, training loss: 13.55510425567627 = 1.198425054550171 + 2.0 * 6.17833948135376
Epoch 270, val loss: 1.3218286037445068
Epoch 280, training loss: 13.49075984954834 = 1.1458836793899536 + 2.0 * 6.172438144683838
Epoch 280, val loss: 1.2812542915344238
Epoch 290, training loss: 13.431329727172852 = 1.094861388206482 + 2.0 * 6.168234348297119
Epoch 290, val loss: 1.2421021461486816
Epoch 300, training loss: 13.375473022460938 = 1.0456891059875488 + 2.0 * 6.164891719818115
Epoch 300, val loss: 1.204759955406189
Epoch 310, training loss: 13.313953399658203 = 0.9987071752548218 + 2.0 * 6.157623291015625
Epoch 310, val loss: 1.1692728996276855
Epoch 320, training loss: 13.26016616821289 = 0.9534729719161987 + 2.0 * 6.153346538543701
Epoch 320, val loss: 1.1353838443756104
Epoch 330, training loss: 13.223393440246582 = 0.9097986817359924 + 2.0 * 6.156797409057617
Epoch 330, val loss: 1.1027816534042358
Epoch 340, training loss: 13.165387153625488 = 0.8678033351898193 + 2.0 * 6.148791790008545
Epoch 340, val loss: 1.0718791484832764
Epoch 350, training loss: 13.111727714538574 = 0.8274642825126648 + 2.0 * 6.142131805419922
Epoch 350, val loss: 1.0424984693527222
Epoch 360, training loss: 13.064423561096191 = 0.7883891463279724 + 2.0 * 6.138017177581787
Epoch 360, val loss: 1.014480471611023
Epoch 370, training loss: 13.035892486572266 = 0.7505215406417847 + 2.0 * 6.142685413360596
Epoch 370, val loss: 0.9878238439559937
Epoch 380, training loss: 12.979166030883789 = 0.7141814231872559 + 2.0 * 6.1324920654296875
Epoch 380, val loss: 0.9627410769462585
Epoch 390, training loss: 12.936579704284668 = 0.6792477965354919 + 2.0 * 6.128665924072266
Epoch 390, val loss: 0.9394617676734924
Epoch 400, training loss: 12.90270709991455 = 0.6457327604293823 + 2.0 * 6.1284871101379395
Epoch 400, val loss: 0.9178785681724548
Epoch 410, training loss: 12.860958099365234 = 0.6137648224830627 + 2.0 * 6.123596668243408
Epoch 410, val loss: 0.8981513381004333
Epoch 420, training loss: 12.823006629943848 = 0.5831964015960693 + 2.0 * 6.1199049949646
Epoch 420, val loss: 0.8801504969596863
Epoch 430, training loss: 12.787227630615234 = 0.5537905693054199 + 2.0 * 6.116718769073486
Epoch 430, val loss: 0.8637030124664307
Epoch 440, training loss: 12.76538372039795 = 0.525421679019928 + 2.0 * 6.119980812072754
Epoch 440, val loss: 0.8486508131027222
Epoch 450, training loss: 12.720760345458984 = 0.49817681312561035 + 2.0 * 6.111291885375977
Epoch 450, val loss: 0.8350349068641663
Epoch 460, training loss: 12.701106071472168 = 0.471903920173645 + 2.0 * 6.114601135253906
Epoch 460, val loss: 0.8227609395980835
Epoch 470, training loss: 12.661520957946777 = 0.44658544659614563 + 2.0 * 6.1074676513671875
Epoch 470, val loss: 0.8117471933364868
Epoch 480, training loss: 12.630989074707031 = 0.4221124053001404 + 2.0 * 6.104438304901123
Epoch 480, val loss: 0.801932156085968
Epoch 490, training loss: 12.611844062805176 = 0.3984220325946808 + 2.0 * 6.106710910797119
Epoch 490, val loss: 0.7931600213050842
Epoch 500, training loss: 12.581915855407715 = 0.3755680322647095 + 2.0 * 6.103173732757568
Epoch 500, val loss: 0.7855077981948853
Epoch 510, training loss: 12.549537658691406 = 0.3537185788154602 + 2.0 * 6.097909450531006
Epoch 510, val loss: 0.7788970470428467
Epoch 520, training loss: 12.526776313781738 = 0.33274516463279724 + 2.0 * 6.097015380859375
Epoch 520, val loss: 0.7733204960823059
Epoch 530, training loss: 12.506026268005371 = 0.31270089745521545 + 2.0 * 6.096662521362305
Epoch 530, val loss: 0.7686941027641296
Epoch 540, training loss: 12.480599403381348 = 0.293735533952713 + 2.0 * 6.0934319496154785
Epoch 540, val loss: 0.7650988698005676
Epoch 550, training loss: 12.455414772033691 = 0.27585262060165405 + 2.0 * 6.089781284332275
Epoch 550, val loss: 0.7625542879104614
Epoch 560, training loss: 12.434615135192871 = 0.25900596380233765 + 2.0 * 6.087804794311523
Epoch 560, val loss: 0.7610712051391602
Epoch 570, training loss: 12.414632797241211 = 0.24316643178462982 + 2.0 * 6.085733413696289
Epoch 570, val loss: 0.7605192065238953
Epoch 580, training loss: 12.407549858093262 = 0.2283218652009964 + 2.0 * 6.089613914489746
Epoch 580, val loss: 0.7608472108840942
Epoch 590, training loss: 12.396748542785645 = 0.21453402936458588 + 2.0 * 6.091107368469238
Epoch 590, val loss: 0.761985719203949
Epoch 600, training loss: 12.367399215698242 = 0.2017921507358551 + 2.0 * 6.082803726196289
Epoch 600, val loss: 0.7639480829238892
Epoch 610, training loss: 12.348904609680176 = 0.18996238708496094 + 2.0 * 6.079471111297607
Epoch 610, val loss: 0.7666605114936829
Epoch 620, training loss: 12.334479331970215 = 0.1789749264717102 + 2.0 * 6.077752113342285
Epoch 620, val loss: 0.7700585126876831
Epoch 630, training loss: 12.327461242675781 = 0.16875521838665009 + 2.0 * 6.079352855682373
Epoch 630, val loss: 0.7740252017974854
Epoch 640, training loss: 12.316076278686523 = 0.15922661125659943 + 2.0 * 6.07842493057251
Epoch 640, val loss: 0.778448760509491
Epoch 650, training loss: 12.298850059509277 = 0.15044604241847992 + 2.0 * 6.074202060699463
Epoch 650, val loss: 0.7832927703857422
Epoch 660, training loss: 12.291889190673828 = 0.14229834079742432 + 2.0 * 6.074795246124268
Epoch 660, val loss: 0.7886013388633728
Epoch 670, training loss: 12.274551391601562 = 0.13472068309783936 + 2.0 * 6.069915294647217
Epoch 670, val loss: 0.7942336797714233
Epoch 680, training loss: 12.264951705932617 = 0.12765681743621826 + 2.0 * 6.068647384643555
Epoch 680, val loss: 0.8002183437347412
Epoch 690, training loss: 12.254475593566895 = 0.12107367068529129 + 2.0 * 6.0667009353637695
Epoch 690, val loss: 0.8064833283424377
Epoch 700, training loss: 12.252107620239258 = 0.11491337418556213 + 2.0 * 6.068597316741943
Epoch 700, val loss: 0.8130084276199341
Epoch 710, training loss: 12.241606712341309 = 0.10914067178964615 + 2.0 * 6.066233158111572
Epoch 710, val loss: 0.8196536302566528
Epoch 720, training loss: 12.23310661315918 = 0.1037687361240387 + 2.0 * 6.064669132232666
Epoch 720, val loss: 0.8265051245689392
Epoch 730, training loss: 12.221272468566895 = 0.09873948991298676 + 2.0 * 6.0612664222717285
Epoch 730, val loss: 0.8335477709770203
Epoch 740, training loss: 12.222402572631836 = 0.09401758760213852 + 2.0 * 6.064192295074463
Epoch 740, val loss: 0.8406988978385925
Epoch 750, training loss: 12.212251663208008 = 0.08959010243415833 + 2.0 * 6.061330795288086
Epoch 750, val loss: 0.8479400873184204
Epoch 760, training loss: 12.207353591918945 = 0.08543701469898224 + 2.0 * 6.060958385467529
Epoch 760, val loss: 0.8552152514457703
Epoch 770, training loss: 12.20082950592041 = 0.08155081421136856 + 2.0 * 6.0596394538879395
Epoch 770, val loss: 0.8626586198806763
Epoch 780, training loss: 12.189827919006348 = 0.07790450751781464 + 2.0 * 6.055961608886719
Epoch 780, val loss: 0.8701428174972534
Epoch 790, training loss: 12.184276580810547 = 0.07446925342082977 + 2.0 * 6.054903507232666
Epoch 790, val loss: 0.8777106404304504
Epoch 800, training loss: 12.19172477722168 = 0.07124603539705276 + 2.0 * 6.060239315032959
Epoch 800, val loss: 0.8852891325950623
Epoch 810, training loss: 12.177641868591309 = 0.06818687170743942 + 2.0 * 6.054727554321289
Epoch 810, val loss: 0.8927965760231018
Epoch 820, training loss: 12.178359985351562 = 0.0653342455625534 + 2.0 * 6.056512832641602
Epoch 820, val loss: 0.9003705978393555
Epoch 830, training loss: 12.167232513427734 = 0.06264019757509232 + 2.0 * 6.052296161651611
Epoch 830, val loss: 0.908008873462677
Epoch 840, training loss: 12.15989875793457 = 0.06009738892316818 + 2.0 * 6.049900531768799
Epoch 840, val loss: 0.9155987501144409
Epoch 850, training loss: 12.17142391204834 = 0.0577046275138855 + 2.0 * 6.056859493255615
Epoch 850, val loss: 0.9231862425804138
Epoch 860, training loss: 12.160476684570312 = 0.055424340069293976 + 2.0 * 6.052525997161865
Epoch 860, val loss: 0.9306962490081787
Epoch 870, training loss: 12.146529197692871 = 0.053285691887140274 + 2.0 * 6.046621799468994
Epoch 870, val loss: 0.9382068514823914
Epoch 880, training loss: 12.142570495605469 = 0.0512637235224247 + 2.0 * 6.045653343200684
Epoch 880, val loss: 0.9457539319992065
Epoch 890, training loss: 12.14682388305664 = 0.04934532567858696 + 2.0 * 6.048739433288574
Epoch 890, val loss: 0.95322185754776
Epoch 900, training loss: 12.139047622680664 = 0.04752226918935776 + 2.0 * 6.045762538909912
Epoch 900, val loss: 0.9606434106826782
Epoch 910, training loss: 12.133708000183105 = 0.045799318701028824 + 2.0 * 6.043954372406006
Epoch 910, val loss: 0.9680527448654175
Epoch 920, training loss: 12.145294189453125 = 0.044163335114717484 + 2.0 * 6.050565242767334
Epoch 920, val loss: 0.9753631353378296
Epoch 930, training loss: 12.129379272460938 = 0.04260281100869179 + 2.0 * 6.043388366699219
Epoch 930, val loss: 0.9826812744140625
Epoch 940, training loss: 12.1224946975708 = 0.04112705960869789 + 2.0 * 6.040683746337891
Epoch 940, val loss: 0.9900000691413879
Epoch 950, training loss: 12.124626159667969 = 0.039721567183732986 + 2.0 * 6.042452335357666
Epoch 950, val loss: 0.9972332715988159
Epoch 960, training loss: 12.11732292175293 = 0.03837875649333 + 2.0 * 6.0394721031188965
Epoch 960, val loss: 1.0043526887893677
Epoch 970, training loss: 12.119942665100098 = 0.03710516542196274 + 2.0 * 6.041418552398682
Epoch 970, val loss: 1.0114314556121826
Epoch 980, training loss: 12.110994338989258 = 0.035892974585294724 + 2.0 * 6.037550449371338
Epoch 980, val loss: 1.0184911489486694
Epoch 990, training loss: 12.109522819519043 = 0.03473861888051033 + 2.0 * 6.0373921394348145
Epoch 990, val loss: 1.0255343914031982
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.7638
Flip ASR: 0.7244/225 nodes
The final ASR:0.50800, 0.24533, Accuracy:0.77531, 0.03089
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11600])
remove edge: torch.Size([2, 9502])
updated graph: torch.Size([2, 10546])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
The final ASR:0.97663, 0.00348, Accuracy:0.82963, 0.00605
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.3 der2:0.4 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 18.696441650390625 = 1.9487347602844238 + 2.0 * 8.37385368347168
Epoch 0, val loss: 1.9502873420715332
Epoch 10, training loss: 18.683977127075195 = 1.937391757965088 + 2.0 * 8.373292922973633
Epoch 10, val loss: 1.9386314153671265
Epoch 20, training loss: 18.66169548034668 = 1.9233551025390625 + 2.0 * 8.369170188903809
Epoch 20, val loss: 1.9242124557495117
Epoch 30, training loss: 18.584653854370117 = 1.9043666124343872 + 2.0 * 8.340143203735352
Epoch 30, val loss: 1.9050898551940918
Epoch 40, training loss: 18.180265426635742 = 1.8832563161849976 + 2.0 * 8.148504257202148
Epoch 40, val loss: 1.8853734731674194
Epoch 50, training loss: 17.258140563964844 = 1.861574411392212 + 2.0 * 7.6982831954956055
Epoch 50, val loss: 1.8657127618789673
Epoch 60, training loss: 16.376445770263672 = 1.8446824550628662 + 2.0 * 7.2658820152282715
Epoch 60, val loss: 1.8508528470993042
Epoch 70, training loss: 15.522521018981934 = 1.8333847522735596 + 2.0 * 6.844568252563477
Epoch 70, val loss: 1.8406012058258057
Epoch 80, training loss: 15.05513858795166 = 1.824202537536621 + 2.0 * 6.6154680252075195
Epoch 80, val loss: 1.8317447900772095
Epoch 90, training loss: 14.789255142211914 = 1.8120739459991455 + 2.0 * 6.488590717315674
Epoch 90, val loss: 1.8205827474594116
Epoch 100, training loss: 14.630781173706055 = 1.7987266778945923 + 2.0 * 6.416027069091797
Epoch 100, val loss: 1.8082720041275024
Epoch 110, training loss: 14.513975143432617 = 1.7853273153305054 + 2.0 * 6.36432409286499
Epoch 110, val loss: 1.7957944869995117
Epoch 120, training loss: 14.426061630249023 = 1.771816611289978 + 2.0 * 6.327122688293457
Epoch 120, val loss: 1.7831515073776245
Epoch 130, training loss: 14.353408813476562 = 1.757825493812561 + 2.0 * 6.297791481018066
Epoch 130, val loss: 1.77024245262146
Epoch 140, training loss: 14.291500091552734 = 1.7429202795028687 + 2.0 * 6.274290084838867
Epoch 140, val loss: 1.7567723989486694
Epoch 150, training loss: 14.244579315185547 = 1.7266385555267334 + 2.0 * 6.258970260620117
Epoch 150, val loss: 1.7422308921813965
Epoch 160, training loss: 14.185711860656738 = 1.7084342241287231 + 2.0 * 6.238638877868652
Epoch 160, val loss: 1.726280689239502
Epoch 170, training loss: 14.137289047241211 = 1.6880031824111938 + 2.0 * 6.224642753601074
Epoch 170, val loss: 1.7084689140319824
Epoch 180, training loss: 14.090775489807129 = 1.6648880243301392 + 2.0 * 6.2129435539245605
Epoch 180, val loss: 1.6885274648666382
Epoch 190, training loss: 14.042085647583008 = 1.6388479471206665 + 2.0 * 6.201618671417236
Epoch 190, val loss: 1.6663296222686768
Epoch 200, training loss: 13.994717597961426 = 1.609637975692749 + 2.0 * 6.192539691925049
Epoch 200, val loss: 1.6415306329727173
Epoch 210, training loss: 13.941211700439453 = 1.5769933462142944 + 2.0 * 6.182109355926514
Epoch 210, val loss: 1.613992691040039
Epoch 220, training loss: 13.888635635375977 = 1.5404802560806274 + 2.0 * 6.17407751083374
Epoch 220, val loss: 1.583430290222168
Epoch 230, training loss: 13.83928108215332 = 1.5000126361846924 + 2.0 * 6.1696343421936035
Epoch 230, val loss: 1.549710988998413
Epoch 240, training loss: 13.77669620513916 = 1.4562381505966187 + 2.0 * 6.160229206085205
Epoch 240, val loss: 1.513582468032837
Epoch 250, training loss: 13.71794605255127 = 1.4093704223632812 + 2.0 * 6.154287815093994
Epoch 250, val loss: 1.475071907043457
Epoch 260, training loss: 13.657984733581543 = 1.3600610494613647 + 2.0 * 6.148962020874023
Epoch 260, val loss: 1.4348152875900269
Epoch 270, training loss: 13.605584144592285 = 1.3091458082199097 + 2.0 * 6.148219108581543
Epoch 270, val loss: 1.3936878442764282
Epoch 280, training loss: 13.53853702545166 = 1.2584571838378906 + 2.0 * 6.140039920806885
Epoch 280, val loss: 1.353139877319336
Epoch 290, training loss: 13.47817325592041 = 1.2083193063735962 + 2.0 * 6.134926795959473
Epoch 290, val loss: 1.313356637954712
Epoch 300, training loss: 13.419291496276855 = 1.159179449081421 + 2.0 * 6.130055904388428
Epoch 300, val loss: 1.2749570608139038
Epoch 310, training loss: 13.363725662231445 = 1.1113859415054321 + 2.0 * 6.126169681549072
Epoch 310, val loss: 1.2379634380340576
Epoch 320, training loss: 13.321817398071289 = 1.0657014846801758 + 2.0 * 6.128057956695557
Epoch 320, val loss: 1.2031315565109253
Epoch 330, training loss: 13.260043144226074 = 1.0225692987442017 + 2.0 * 6.118736743927002
Epoch 330, val loss: 1.1707522869110107
Epoch 340, training loss: 13.211989402770996 = 0.9815552830696106 + 2.0 * 6.115217208862305
Epoch 340, val loss: 1.1403918266296387
Epoch 350, training loss: 13.16511058807373 = 0.9423736333847046 + 2.0 * 6.111368656158447
Epoch 350, val loss: 1.1118762493133545
Epoch 360, training loss: 13.125005722045898 = 0.905022382736206 + 2.0 * 6.109991550445557
Epoch 360, val loss: 1.0852043628692627
Epoch 370, training loss: 13.081092834472656 = 0.8696805834770203 + 2.0 * 6.105706214904785
Epoch 370, val loss: 1.0601744651794434
Epoch 380, training loss: 13.040809631347656 = 0.8359655737876892 + 2.0 * 6.10242223739624
Epoch 380, val loss: 1.0366131067276
Epoch 390, training loss: 13.002348899841309 = 0.8035756945610046 + 2.0 * 6.099386692047119
Epoch 390, val loss: 1.014311671257019
Epoch 400, training loss: 12.970043182373047 = 0.7725206613540649 + 2.0 * 6.098761081695557
Epoch 400, val loss: 0.993204653263092
Epoch 410, training loss: 12.932979583740234 = 0.7428755164146423 + 2.0 * 6.095052242279053
Epoch 410, val loss: 0.9732969999313354
Epoch 420, training loss: 12.899857521057129 = 0.7146491408348083 + 2.0 * 6.092604160308838
Epoch 420, val loss: 0.9546012878417969
Epoch 430, training loss: 12.864411354064941 = 0.6877365112304688 + 2.0 * 6.088337421417236
Epoch 430, val loss: 0.9369789361953735
Epoch 440, training loss: 12.836455345153809 = 0.661925733089447 + 2.0 * 6.0872650146484375
Epoch 440, val loss: 0.9205517768859863
Epoch 450, training loss: 12.81455135345459 = 0.6373565793037415 + 2.0 * 6.088597297668457
Epoch 450, val loss: 0.9053525924682617
Epoch 460, training loss: 12.779457092285156 = 0.6139904260635376 + 2.0 * 6.082733154296875
Epoch 460, val loss: 0.8910490274429321
Epoch 470, training loss: 12.75062370300293 = 0.5915154218673706 + 2.0 * 6.079554080963135
Epoch 470, val loss: 0.8777535557746887
Epoch 480, training loss: 12.725665092468262 = 0.5697849988937378 + 2.0 * 6.077939987182617
Epoch 480, val loss: 0.8652763962745667
Epoch 490, training loss: 12.71218204498291 = 0.5488011240959167 + 2.0 * 6.081690311431885
Epoch 490, val loss: 0.8535078167915344
Epoch 500, training loss: 12.675210952758789 = 0.5285890698432922 + 2.0 * 6.073310852050781
Epoch 500, val loss: 0.842529833316803
Epoch 510, training loss: 12.652440071105957 = 0.5090247988700867 + 2.0 * 6.071707725524902
Epoch 510, val loss: 0.8321399688720703
Epoch 520, training loss: 12.629897117614746 = 0.4899244010448456 + 2.0 * 6.069986343383789
Epoch 520, val loss: 0.8223353624343872
Epoch 530, training loss: 12.61825180053711 = 0.47122976183891296 + 2.0 * 6.073511123657227
Epoch 530, val loss: 0.8130826950073242
Epoch 540, training loss: 12.588387489318848 = 0.4529503285884857 + 2.0 * 6.067718505859375
Epoch 540, val loss: 0.8042325973510742
Epoch 550, training loss: 12.571379661560059 = 0.43506893515586853 + 2.0 * 6.068155288696289
Epoch 550, val loss: 0.7959088087081909
Epoch 560, training loss: 12.546886444091797 = 0.41761714220046997 + 2.0 * 6.064634799957275
Epoch 560, val loss: 0.7879875898361206
Epoch 570, training loss: 12.525117874145508 = 0.40044623613357544 + 2.0 * 6.062335968017578
Epoch 570, val loss: 0.7803982496261597
Epoch 580, training loss: 12.505513191223145 = 0.38351747393608093 + 2.0 * 6.06099796295166
Epoch 580, val loss: 0.7731061577796936
Epoch 590, training loss: 12.494889259338379 = 0.3668327033519745 + 2.0 * 6.064028263092041
Epoch 590, val loss: 0.7661978602409363
Epoch 600, training loss: 12.472341537475586 = 0.3504653573036194 + 2.0 * 6.060937881469727
Epoch 600, val loss: 0.7596629858016968
Epoch 610, training loss: 12.454410552978516 = 0.3343692421913147 + 2.0 * 6.060020446777344
Epoch 610, val loss: 0.7534320950508118
Epoch 620, training loss: 12.431875228881836 = 0.31864234805107117 + 2.0 * 6.056616306304932
Epoch 620, val loss: 0.7474948167800903
Epoch 630, training loss: 12.413928031921387 = 0.30318742990493774 + 2.0 * 6.055370330810547
Epoch 630, val loss: 0.7420762777328491
Epoch 640, training loss: 12.399730682373047 = 0.28801825642585754 + 2.0 * 6.055856227874756
Epoch 640, val loss: 0.7369880676269531
Epoch 650, training loss: 12.377579689025879 = 0.27323493361473083 + 2.0 * 6.0521721839904785
Epoch 650, val loss: 0.7321540117263794
Epoch 660, training loss: 12.361051559448242 = 0.25883716344833374 + 2.0 * 6.051107406616211
Epoch 660, val loss: 0.7277798056602478
Epoch 670, training loss: 12.353541374206543 = 0.24485258758068085 + 2.0 * 6.054344177246094
Epoch 670, val loss: 0.7238644361495972
Epoch 680, training loss: 12.339433670043945 = 0.23134960234165192 + 2.0 * 6.054041862487793
Epoch 680, val loss: 0.720453143119812
Epoch 690, training loss: 12.318562507629395 = 0.21844257414340973 + 2.0 * 6.050059795379639
Epoch 690, val loss: 0.7175131440162659
Epoch 700, training loss: 12.300503730773926 = 0.2060868889093399 + 2.0 * 6.047208309173584
Epoch 700, val loss: 0.7151726484298706
Epoch 710, training loss: 12.291459083557129 = 0.19431188702583313 + 2.0 * 6.0485734939575195
Epoch 710, val loss: 0.7133854627609253
Epoch 720, training loss: 12.276000022888184 = 0.18317213654518127 + 2.0 * 6.046413898468018
Epoch 720, val loss: 0.7120487689971924
Epoch 730, training loss: 12.262938499450684 = 0.17263001203536987 + 2.0 * 6.045154094696045
Epoch 730, val loss: 0.7113037109375
Epoch 740, training loss: 12.248722076416016 = 0.1627146452665329 + 2.0 * 6.043003559112549
Epoch 740, val loss: 0.7110350728034973
Epoch 750, training loss: 12.243145942687988 = 0.15338103473186493 + 2.0 * 6.044882297515869
Epoch 750, val loss: 0.7112914323806763
Epoch 760, training loss: 12.233996391296387 = 0.1446630358695984 + 2.0 * 6.044666767120361
Epoch 760, val loss: 0.711891233921051
Epoch 770, training loss: 12.220020294189453 = 0.136519655585289 + 2.0 * 6.041750431060791
Epoch 770, val loss: 0.7129641175270081
Epoch 780, training loss: 12.209177017211914 = 0.12893202900886536 + 2.0 * 6.0401225090026855
Epoch 780, val loss: 0.7144131064414978
Epoch 790, training loss: 12.209667205810547 = 0.12186819314956665 + 2.0 * 6.0438995361328125
Epoch 790, val loss: 0.716156542301178
Epoch 800, training loss: 12.193534851074219 = 0.11526674032211304 + 2.0 * 6.0391340255737305
Epoch 800, val loss: 0.7182328104972839
Epoch 810, training loss: 12.182700157165527 = 0.10912325233221054 + 2.0 * 6.036788463592529
Epoch 810, val loss: 0.720668375492096
Epoch 820, training loss: 12.176717758178711 = 0.10338087379932404 + 2.0 * 6.036668300628662
Epoch 820, val loss: 0.7233200073242188
Epoch 830, training loss: 12.168253898620605 = 0.0980304405093193 + 2.0 * 6.035111904144287
Epoch 830, val loss: 0.7263404726982117
Epoch 840, training loss: 12.165790557861328 = 0.09306631982326508 + 2.0 * 6.036362171173096
Epoch 840, val loss: 0.7294570207595825
Epoch 850, training loss: 12.154778480529785 = 0.08842238038778305 + 2.0 * 6.033177852630615
Epoch 850, val loss: 0.7328254580497742
Epoch 860, training loss: 12.154860496520996 = 0.08409269899129868 + 2.0 * 6.035383701324463
Epoch 860, val loss: 0.7364533543586731
Epoch 870, training loss: 12.143133163452148 = 0.08004934340715408 + 2.0 * 6.03154182434082
Epoch 870, val loss: 0.740118145942688
Epoch 880, training loss: 12.15242862701416 = 0.07626303285360336 + 2.0 * 6.038082599639893
Epoch 880, val loss: 0.7440494894981384
Epoch 890, training loss: 12.136310577392578 = 0.07274569571018219 + 2.0 * 6.031782627105713
Epoch 890, val loss: 0.7479649782180786
Epoch 900, training loss: 12.12894058227539 = 0.06945104151964188 + 2.0 * 6.029744625091553
Epoch 900, val loss: 0.7520706653594971
Epoch 910, training loss: 12.123113632202148 = 0.06635347008705139 + 2.0 * 6.028379917144775
Epoch 910, val loss: 0.7562031149864197
Epoch 920, training loss: 12.134060859680176 = 0.06345093995332718 + 2.0 * 6.035305023193359
Epoch 920, val loss: 0.7604746222496033
Epoch 930, training loss: 12.123024940490723 = 0.06072032451629639 + 2.0 * 6.031152248382568
Epoch 930, val loss: 0.7647503018379211
Epoch 940, training loss: 12.112568855285645 = 0.05817263573408127 + 2.0 * 6.027198314666748
Epoch 940, val loss: 0.7691213488578796
Epoch 950, training loss: 12.106741905212402 = 0.05576923489570618 + 2.0 * 6.025486469268799
Epoch 950, val loss: 0.7734773755073547
Epoch 960, training loss: 12.106084823608398 = 0.05350850149989128 + 2.0 * 6.026288032531738
Epoch 960, val loss: 0.7779126167297363
Epoch 970, training loss: 12.101019859313965 = 0.05137442424893379 + 2.0 * 6.02482271194458
Epoch 970, val loss: 0.7824262976646423
Epoch 980, training loss: 12.119866371154785 = 0.04936052858829498 + 2.0 * 6.035253047943115
Epoch 980, val loss: 0.7868896126747131
Epoch 990, training loss: 12.09754467010498 = 0.04747753590345383 + 2.0 * 6.025033473968506
Epoch 990, val loss: 0.7914612293243408
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 730, in <module>
    torch.cuda.empty_cache()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/cuda/memory.py", line 121, in empty_cache
    torch._C._cuda_emptyCache()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.3 der2:0.4 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 18.67613410949707 = 1.9283679723739624 + 2.0 * 8.373883247375488
Epoch 0, val loss: 1.9267840385437012
Epoch 10, training loss: 18.66534423828125 = 1.918552279472351 + 2.0 * 8.373395919799805
Epoch 10, val loss: 1.9169750213623047
Epoch 20, training loss: 18.645984649658203 = 1.9063856601715088 + 2.0 * 8.369799613952637
Epoch 20, val loss: 1.9046746492385864
Epoch 30, training loss: 18.577478408813477 = 1.8899778127670288 + 2.0 * 8.34375
Epoch 30, val loss: 1.88828706741333
Epoch 40, training loss: 18.20293617248535 = 1.8706735372543335 + 2.0 * 8.166131019592285
Epoch 40, val loss: 1.8698680400848389
Epoch 50, training loss: 17.026325225830078 = 1.8509032726287842 + 2.0 * 7.587710857391357
Epoch 50, val loss: 1.851245403289795
Epoch 60, training loss: 16.06879425048828 = 1.8359909057617188 + 2.0 * 7.1164021492004395
Epoch 60, val loss: 1.8374634981155396
Epoch 70, training loss: 15.567102432250977 = 1.8248401880264282 + 2.0 * 6.87113094329834
Epoch 70, val loss: 1.8267768621444702
Epoch 80, training loss: 15.262482643127441 = 1.811449408531189 + 2.0 * 6.7255167961120605
Epoch 80, val loss: 1.813998818397522
Epoch 90, training loss: 15.01980972290039 = 1.7970519065856934 + 2.0 * 6.6113786697387695
Epoch 90, val loss: 1.8007057905197144
Epoch 100, training loss: 14.851983070373535 = 1.783212661743164 + 2.0 * 6.5343852043151855
Epoch 100, val loss: 1.787878155708313
Epoch 110, training loss: 14.724302291870117 = 1.7685871124267578 + 2.0 * 6.47785758972168
Epoch 110, val loss: 1.774234414100647
Epoch 120, training loss: 14.618327140808105 = 1.752941370010376 + 2.0 * 6.432693004608154
Epoch 120, val loss: 1.7599533796310425
Epoch 130, training loss: 14.535804748535156 = 1.736344814300537 + 2.0 * 6.3997297286987305
Epoch 130, val loss: 1.7449204921722412
Epoch 140, training loss: 14.453971862792969 = 1.7184256315231323 + 2.0 * 6.367773056030273
Epoch 140, val loss: 1.7288075685501099
Epoch 150, training loss: 14.391050338745117 = 1.6985139846801758 + 2.0 * 6.346268177032471
Epoch 150, val loss: 1.7113232612609863
Epoch 160, training loss: 14.318096160888672 = 1.6764222383499146 + 2.0 * 6.320837020874023
Epoch 160, val loss: 1.6919370889663696
Epoch 170, training loss: 14.252293586730957 = 1.651748538017273 + 2.0 * 6.300272464752197
Epoch 170, val loss: 1.6705598831176758
Epoch 180, training loss: 14.190652847290039 = 1.623942255973816 + 2.0 * 6.283355236053467
Epoch 180, val loss: 1.6466184854507446
Epoch 190, training loss: 14.132179260253906 = 1.592807650566101 + 2.0 * 6.269685745239258
Epoch 190, val loss: 1.6199650764465332
Epoch 200, training loss: 14.070680618286133 = 1.5586202144622803 + 2.0 * 6.256030082702637
Epoch 200, val loss: 1.5907384157180786
Epoch 210, training loss: 14.00786304473877 = 1.5210696458816528 + 2.0 * 6.243396759033203
Epoch 210, val loss: 1.5588302612304688
Epoch 220, training loss: 13.945012092590332 = 1.479981541633606 + 2.0 * 6.232515335083008
Epoch 220, val loss: 1.5241236686706543
Epoch 230, training loss: 13.884145736694336 = 1.4356383085250854 + 2.0 * 6.2242536544799805
Epoch 230, val loss: 1.4869569540023804
Epoch 240, training loss: 13.821779251098633 = 1.3891874551773071 + 2.0 * 6.2162957191467285
Epoch 240, val loss: 1.448000431060791
Epoch 250, training loss: 13.75535774230957 = 1.3406647443771362 + 2.0 * 6.207346439361572
Epoch 250, val loss: 1.4077813625335693
Epoch 260, training loss: 13.691319465637207 = 1.290582537651062 + 2.0 * 6.200368404388428
Epoch 260, val loss: 1.3665372133255005
Epoch 270, training loss: 13.633834838867188 = 1.2400239706039429 + 2.0 * 6.196905612945557
Epoch 270, val loss: 1.3248461484909058
Epoch 280, training loss: 13.566658973693848 = 1.189474105834961 + 2.0 * 6.188592433929443
Epoch 280, val loss: 1.2837060689926147
Epoch 290, training loss: 13.503910064697266 = 1.1393481492996216 + 2.0 * 6.182281017303467
Epoch 290, val loss: 1.243161678314209
Epoch 300, training loss: 13.449612617492676 = 1.0902248620986938 + 2.0 * 6.179693698883057
Epoch 300, val loss: 1.2037593126296997
Epoch 310, training loss: 13.389425277709961 = 1.0428847074508667 + 2.0 * 6.173270225524902
Epoch 310, val loss: 1.1659530401229858
Epoch 320, training loss: 13.333253860473633 = 0.9973527789115906 + 2.0 * 6.167950630187988
Epoch 320, val loss: 1.129767894744873
Epoch 330, training loss: 13.28857421875 = 0.9535778164863586 + 2.0 * 6.1674981117248535
Epoch 330, val loss: 1.0950250625610352
Epoch 340, training loss: 13.233233451843262 = 0.9118624925613403 + 2.0 * 6.1606855392456055
Epoch 340, val loss: 1.062278389930725
Epoch 350, training loss: 13.183235168457031 = 0.8723771572113037 + 2.0 * 6.155428886413574
Epoch 350, val loss: 1.0312929153442383
Epoch 360, training loss: 13.148467063903809 = 0.8347364068031311 + 2.0 * 6.156865119934082
Epoch 360, val loss: 1.001941204071045
Epoch 370, training loss: 13.097893714904785 = 0.7990700006484985 + 2.0 * 6.149411678314209
Epoch 370, val loss: 0.9747108817100525
Epoch 380, training loss: 13.053421974182129 = 0.7655118107795715 + 2.0 * 6.143955230712891
Epoch 380, val loss: 0.949022650718689
Epoch 390, training loss: 13.014907836914062 = 0.7336023449897766 + 2.0 * 6.140652656555176
Epoch 390, val loss: 0.9250884652137756
Epoch 400, training loss: 12.980724334716797 = 0.7031422257423401 + 2.0 * 6.138791084289551
Epoch 400, val loss: 0.9026029109954834
Epoch 410, training loss: 12.948251724243164 = 0.6740221381187439 + 2.0 * 6.137115001678467
Epoch 410, val loss: 0.8814976215362549
Epoch 420, training loss: 12.910332679748535 = 0.6460726261138916 + 2.0 * 6.132130146026611
Epoch 420, val loss: 0.8618715405464172
Epoch 430, training loss: 12.87460708618164 = 0.6193398237228394 + 2.0 * 6.127633571624756
Epoch 430, val loss: 0.8434668779373169
Epoch 440, training loss: 12.844934463500977 = 0.5934757590293884 + 2.0 * 6.125729560852051
Epoch 440, val loss: 0.8262674808502197
Epoch 450, training loss: 12.817032814025879 = 0.5684204697608948 + 2.0 * 6.1243062019348145
Epoch 450, val loss: 0.810322105884552
Epoch 460, training loss: 12.79164981842041 = 0.5442066192626953 + 2.0 * 6.123721599578857
Epoch 460, val loss: 0.7955109477043152
Epoch 470, training loss: 12.760931015014648 = 0.5210332870483398 + 2.0 * 6.119948863983154
Epoch 470, val loss: 0.7818291187286377
Epoch 480, training loss: 12.728033065795898 = 0.49858003854751587 + 2.0 * 6.114726543426514
Epoch 480, val loss: 0.7693514227867126
Epoch 490, training loss: 12.702667236328125 = 0.47694849967956543 + 2.0 * 6.11285924911499
Epoch 490, val loss: 0.7578729391098022
Epoch 500, training loss: 12.678614616394043 = 0.45599669218063354 + 2.0 * 6.111309051513672
Epoch 500, val loss: 0.7474062442779541
Epoch 510, training loss: 12.652544021606445 = 0.4358029067516327 + 2.0 * 6.108370780944824
Epoch 510, val loss: 0.7378676533699036
Epoch 520, training loss: 12.628901481628418 = 0.4163264036178589 + 2.0 * 6.106287479400635
Epoch 520, val loss: 0.7293068170547485
Epoch 530, training loss: 12.606435775756836 = 0.39747360348701477 + 2.0 * 6.104481220245361
Epoch 530, val loss: 0.7216795682907104
Epoch 540, training loss: 12.587026596069336 = 0.3792870342731476 + 2.0 * 6.103869915008545
Epoch 540, val loss: 0.7149286866188049
Epoch 550, training loss: 12.56209945678711 = 0.3617940843105316 + 2.0 * 6.100152492523193
Epoch 550, val loss: 0.7088994383811951
Epoch 560, training loss: 12.538830757141113 = 0.3448241949081421 + 2.0 * 6.09700345993042
Epoch 560, val loss: 0.7037171125411987
Epoch 570, training loss: 12.529128074645996 = 0.32843056321144104 + 2.0 * 6.100348949432373
Epoch 570, val loss: 0.6992018222808838
Epoch 580, training loss: 12.505845069885254 = 0.31261104345321655 + 2.0 * 6.096617221832275
Epoch 580, val loss: 0.6953442692756653
Epoch 590, training loss: 12.481657981872559 = 0.2974701225757599 + 2.0 * 6.0920939445495605
Epoch 590, val loss: 0.6920832395553589
Epoch 600, training loss: 12.461427688598633 = 0.2828780710697174 + 2.0 * 6.089274883270264
Epoch 600, val loss: 0.6894892454147339
Epoch 610, training loss: 12.45544147491455 = 0.2688443660736084 + 2.0 * 6.093298435211182
Epoch 610, val loss: 0.6875488758087158
Epoch 620, training loss: 12.435550689697266 = 0.2552885413169861 + 2.0 * 6.0901312828063965
Epoch 620, val loss: 0.6860309839248657
Epoch 630, training loss: 12.411117553710938 = 0.24241401255130768 + 2.0 * 6.084351539611816
Epoch 630, val loss: 0.685134768486023
Epoch 640, training loss: 12.409165382385254 = 0.2300911545753479 + 2.0 * 6.089537143707275
Epoch 640, val loss: 0.684847891330719
Epoch 650, training loss: 12.381065368652344 = 0.2183428406715393 + 2.0 * 6.081361293792725
Epoch 650, val loss: 0.6850114464759827
Epoch 660, training loss: 12.366739273071289 = 0.2071501761674881 + 2.0 * 6.079794406890869
Epoch 660, val loss: 0.6856401562690735
Epoch 670, training loss: 12.352826118469238 = 0.19654157757759094 + 2.0 * 6.078142166137695
Epoch 670, val loss: 0.6868135929107666
Epoch 680, training loss: 12.355010986328125 = 0.18645307421684265 + 2.0 * 6.0842790603637695
Epoch 680, val loss: 0.688369870185852
Epoch 690, training loss: 12.327627182006836 = 0.17691755294799805 + 2.0 * 6.07535457611084
Epoch 690, val loss: 0.6902722120285034
Epoch 700, training loss: 12.31836223602295 = 0.16794820129871368 + 2.0 * 6.075207233428955
Epoch 700, val loss: 0.6925573945045471
Epoch 710, training loss: 12.312782287597656 = 0.1594843864440918 + 2.0 * 6.076649188995361
Epoch 710, val loss: 0.6951785087585449
Epoch 720, training loss: 12.294476509094238 = 0.15153230726718903 + 2.0 * 6.07147216796875
Epoch 720, val loss: 0.6980968713760376
Epoch 730, training loss: 12.2958345413208 = 0.14404883980751038 + 2.0 * 6.075892925262451
Epoch 730, val loss: 0.7012463808059692
Epoch 740, training loss: 12.2771635055542 = 0.1369774490594864 + 2.0 * 6.070093154907227
Epoch 740, val loss: 0.7045198082923889
Epoch 750, training loss: 12.264875411987305 = 0.13034389913082123 + 2.0 * 6.06726598739624
Epoch 750, val loss: 0.7081544995307922
Epoch 760, training loss: 12.256148338317871 = 0.1240767315030098 + 2.0 * 6.066035747528076
Epoch 760, val loss: 0.7119360566139221
Epoch 770, training loss: 12.274551391601562 = 0.11815925687551498 + 2.0 * 6.078196048736572
Epoch 770, val loss: 0.7158387303352356
Epoch 780, training loss: 12.242302894592285 = 0.11264481395483017 + 2.0 * 6.064828872680664
Epoch 780, val loss: 0.7199636101722717
Epoch 790, training loss: 12.232107162475586 = 0.10743817687034607 + 2.0 * 6.0623345375061035
Epoch 790, val loss: 0.7241740226745605
Epoch 800, training loss: 12.225374221801758 = 0.10254314541816711 + 2.0 * 6.061415672302246
Epoch 800, val loss: 0.7285470366477966
Epoch 810, training loss: 12.220969200134277 = 0.09792663902044296 + 2.0 * 6.061521053314209
Epoch 810, val loss: 0.7330474853515625
Epoch 820, training loss: 12.21363353729248 = 0.09357323497533798 + 2.0 * 6.060029983520508
Epoch 820, val loss: 0.7376812696456909
Epoch 830, training loss: 12.211730003356934 = 0.08948487043380737 + 2.0 * 6.061122417449951
Epoch 830, val loss: 0.7422888278961182
Epoch 840, training loss: 12.20089054107666 = 0.08562301099300385 + 2.0 * 6.057633876800537
Epoch 840, val loss: 0.7469226121902466
Epoch 850, training loss: 12.194208145141602 = 0.08196946233510971 + 2.0 * 6.056119441986084
Epoch 850, val loss: 0.7516002655029297
Epoch 860, training loss: 12.195416450500488 = 0.07852736115455627 + 2.0 * 6.058444499969482
Epoch 860, val loss: 0.756437361240387
Epoch 870, training loss: 12.195130348205566 = 0.07528668642044067 + 2.0 * 6.059921741485596
Epoch 870, val loss: 0.7613672018051147
Epoch 880, training loss: 12.18722915649414 = 0.07218357920646667 + 2.0 * 6.057522773742676
Epoch 880, val loss: 0.7660927772521973
Epoch 890, training loss: 12.172924041748047 = 0.06927606463432312 + 2.0 * 6.05182409286499
Epoch 890, val loss: 0.7709090709686279
Epoch 900, training loss: 12.169133186340332 = 0.0665178894996643 + 2.0 * 6.051307678222656
Epoch 900, val loss: 0.7758316993713379
Epoch 910, training loss: 12.169618606567383 = 0.06390061974525452 + 2.0 * 6.052858829498291
Epoch 910, val loss: 0.7807531356811523
Epoch 920, training loss: 12.161404609680176 = 0.061421167105436325 + 2.0 * 6.049991607666016
Epoch 920, val loss: 0.7858389019966125
Epoch 930, training loss: 12.158544540405273 = 0.05908162519335747 + 2.0 * 6.049731254577637
Epoch 930, val loss: 0.7908611297607422
Epoch 940, training loss: 12.153457641601562 = 0.056846581399440765 + 2.0 * 6.048305511474609
Epoch 940, val loss: 0.7958966493606567
Epoch 950, training loss: 12.155912399291992 = 0.054742638021707535 + 2.0 * 6.05058479309082
Epoch 950, val loss: 0.8010477423667908
Epoch 960, training loss: 12.158047676086426 = 0.052727680653333664 + 2.0 * 6.05265998840332
Epoch 960, val loss: 0.8059007525444031
Epoch 970, training loss: 12.14097785949707 = 0.05083677917718887 + 2.0 * 6.045070648193359
Epoch 970, val loss: 0.8110232949256897
Epoch 980, training loss: 12.13791275024414 = 0.04903918132185936 + 2.0 * 6.044436931610107
Epoch 980, val loss: 0.8159881830215454
Epoch 990, training loss: 12.134018898010254 = 0.0473216213285923 + 2.0 * 6.043348789215088
Epoch 990, val loss: 0.8210105299949646
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 730, in <module>
    torch.cuda.empty_cache()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/cuda/memory.py", line 121, in empty_cache
    torch._C._cuda_emptyCache()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.4 der2:0 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 18.700634002685547 = 1.9529579877853394 + 2.0 * 8.373838424682617
Epoch 0, val loss: 1.961792230606079
Epoch 10, training loss: 18.688697814941406 = 1.942192554473877 + 2.0 * 8.373252868652344
Epoch 10, val loss: 1.9511620998382568
Epoch 20, training loss: 18.668291091918945 = 1.928593397140503 + 2.0 * 8.36984920501709
Epoch 20, val loss: 1.9377940893173218
Epoch 30, training loss: 18.602140426635742 = 1.9100515842437744 + 2.0 * 8.346044540405273
Epoch 30, val loss: 1.920069694519043
Epoch 40, training loss: 18.21636390686035 = 1.887410044670105 + 2.0 * 8.164477348327637
Epoch 40, val loss: 1.899267554283142
Epoch 50, training loss: 16.479869842529297 = 1.862032175064087 + 2.0 * 7.308918476104736
Epoch 50, val loss: 1.8760671615600586
Epoch 60, training loss: 15.711216926574707 = 1.844354510307312 + 2.0 * 6.933431148529053
Epoch 60, val loss: 1.8606443405151367
Epoch 70, training loss: 15.258996963500977 = 1.8316166400909424 + 2.0 * 6.713690280914307
Epoch 70, val loss: 1.8483728170394897
Epoch 80, training loss: 14.963547706604004 = 1.8202296495437622 + 2.0 * 6.571659088134766
Epoch 80, val loss: 1.836945652961731
Epoch 90, training loss: 14.779815673828125 = 1.810593843460083 + 2.0 * 6.4846110343933105
Epoch 90, val loss: 1.8264188766479492
Epoch 100, training loss: 14.644311904907227 = 1.800926685333252 + 2.0 * 6.421692371368408
Epoch 100, val loss: 1.8161739110946655
Epoch 110, training loss: 14.531549453735352 = 1.7918790578842163 + 2.0 * 6.369835376739502
Epoch 110, val loss: 1.8065690994262695
Epoch 120, training loss: 14.43731689453125 = 1.7831016778945923 + 2.0 * 6.3271074295043945
Epoch 120, val loss: 1.7975648641586304
Epoch 130, training loss: 14.365313529968262 = 1.7745048999786377 + 2.0 * 6.295404434204102
Epoch 130, val loss: 1.7889655828475952
Epoch 140, training loss: 14.303030014038086 = 1.7655465602874756 + 2.0 * 6.268741607666016
Epoch 140, val loss: 1.7802053689956665
Epoch 150, training loss: 14.24880599975586 = 1.7556941509246826 + 2.0 * 6.246555805206299
Epoch 150, val loss: 1.7708970308303833
Epoch 160, training loss: 14.200020790100098 = 1.7446428537368774 + 2.0 * 6.227688789367676
Epoch 160, val loss: 1.7610375881195068
Epoch 170, training loss: 14.157315254211426 = 1.7321809530258179 + 2.0 * 6.212567329406738
Epoch 170, val loss: 1.750333547592163
Epoch 180, training loss: 14.114690780639648 = 1.7178508043289185 + 2.0 * 6.19842004776001
Epoch 180, val loss: 1.738372564315796
Epoch 190, training loss: 14.07371711730957 = 1.701218605041504 + 2.0 * 6.186249256134033
Epoch 190, val loss: 1.7246625423431396
Epoch 200, training loss: 14.030736923217773 = 1.6819921731948853 + 2.0 * 6.17437219619751
Epoch 200, val loss: 1.7090202569961548
Epoch 210, training loss: 13.987024307250977 = 1.6597111225128174 + 2.0 * 6.163656711578369
Epoch 210, val loss: 1.6910475492477417
Epoch 220, training loss: 13.942388534545898 = 1.6336045265197754 + 2.0 * 6.154392242431641
Epoch 220, val loss: 1.6700384616851807
Epoch 230, training loss: 13.898679733276367 = 1.6028541326522827 + 2.0 * 6.147912979125977
Epoch 230, val loss: 1.6453777551651
Epoch 240, training loss: 13.845843315124512 = 1.5672237873077393 + 2.0 * 6.139309883117676
Epoch 240, val loss: 1.6167147159576416
Epoch 250, training loss: 13.790924072265625 = 1.5260870456695557 + 2.0 * 6.132418632507324
Epoch 250, val loss: 1.5836844444274902
Epoch 260, training loss: 13.733512878417969 = 1.4791569709777832 + 2.0 * 6.127178192138672
Epoch 260, val loss: 1.545937418937683
Epoch 270, training loss: 13.672426223754883 = 1.4274431467056274 + 2.0 * 6.122491359710693
Epoch 270, val loss: 1.504355549812317
Epoch 280, training loss: 13.609565734863281 = 1.3720765113830566 + 2.0 * 6.118744850158691
Epoch 280, val loss: 1.4599367380142212
Epoch 290, training loss: 13.541790008544922 = 1.3146682977676392 + 2.0 * 6.113560676574707
Epoch 290, val loss: 1.4137717485427856
Epoch 300, training loss: 13.475316047668457 = 1.2561321258544922 + 2.0 * 6.109591960906982
Epoch 300, val loss: 1.367161750793457
Epoch 310, training loss: 13.415682792663574 = 1.198163390159607 + 2.0 * 6.108759880065918
Epoch 310, val loss: 1.3211854696273804
Epoch 320, training loss: 13.348139762878418 = 1.142348051071167 + 2.0 * 6.102895736694336
Epoch 320, val loss: 1.277298092842102
Epoch 330, training loss: 13.291003227233887 = 1.0889652967453003 + 2.0 * 6.101018905639648
Epoch 330, val loss: 1.2356643676757812
Epoch 340, training loss: 13.233314514160156 = 1.038663387298584 + 2.0 * 6.097325325012207
Epoch 340, val loss: 1.1963473558425903
Epoch 350, training loss: 13.178701400756836 = 0.9906837344169617 + 2.0 * 6.094008922576904
Epoch 350, val loss: 1.1589436531066895
Epoch 360, training loss: 13.140743255615234 = 0.9452195167541504 + 2.0 * 6.097761631011963
Epoch 360, val loss: 1.1233206987380981
Epoch 370, training loss: 13.081022262573242 = 0.9028470516204834 + 2.0 * 6.08908748626709
Epoch 370, val loss: 1.0902009010314941
Epoch 380, training loss: 13.035122871398926 = 0.8628178238868713 + 2.0 * 6.08615255355835
Epoch 380, val loss: 1.0589087009429932
Epoch 390, training loss: 12.99074649810791 = 0.8246487379074097 + 2.0 * 6.0830488204956055
Epoch 390, val loss: 1.028921365737915
Epoch 400, training loss: 12.958593368530273 = 0.7882744073867798 + 2.0 * 6.0851593017578125
Epoch 400, val loss: 1.0005916357040405
Epoch 410, training loss: 12.913619995117188 = 0.7545180916786194 + 2.0 * 6.079550743103027
Epoch 410, val loss: 0.9744848012924194
Epoch 420, training loss: 12.878342628479004 = 0.7230266332626343 + 2.0 * 6.077658176422119
Epoch 420, val loss: 0.9506553411483765
Epoch 430, training loss: 12.84200382232666 = 0.6932910084724426 + 2.0 * 6.074356555938721
Epoch 430, val loss: 0.9286311864852905
Epoch 440, training loss: 12.821226119995117 = 0.6652502417564392 + 2.0 * 6.077988147735596
Epoch 440, val loss: 0.90850430727005
Epoch 450, training loss: 12.781907081604004 = 0.6392349004745483 + 2.0 * 6.071336269378662
Epoch 450, val loss: 0.8905962705612183
Epoch 460, training loss: 12.752263069152832 = 0.6147421598434448 + 2.0 * 6.068760395050049
Epoch 460, val loss: 0.8746161460876465
Epoch 470, training loss: 12.727910041809082 = 0.5914385318756104 + 2.0 * 6.068235874176025
Epoch 470, val loss: 0.8601667284965515
Epoch 480, training loss: 12.704294204711914 = 0.5693914294242859 + 2.0 * 6.067451477050781
Epoch 480, val loss: 0.8473638296127319
Epoch 490, training loss: 12.676374435424805 = 0.5484910011291504 + 2.0 * 6.063941955566406
Epoch 490, val loss: 0.8360675573348999
Epoch 500, training loss: 12.66077709197998 = 0.5285269618034363 + 2.0 * 6.06612491607666
Epoch 500, val loss: 0.8259258270263672
Epoch 510, training loss: 12.632164001464844 = 0.5095755457878113 + 2.0 * 6.061294078826904
Epoch 510, val loss: 0.8171047568321228
Epoch 520, training loss: 12.611869812011719 = 0.49133384227752686 + 2.0 * 6.060267925262451
Epoch 520, val loss: 0.8092792630195618
Epoch 530, training loss: 12.589659690856934 = 0.4736408591270447 + 2.0 * 6.058009624481201
Epoch 530, val loss: 0.8022488355636597
Epoch 540, training loss: 12.56970500946045 = 0.45643168687820435 + 2.0 * 6.056636810302734
Epoch 540, val loss: 0.7960351705551147
Epoch 550, training loss: 12.559066772460938 = 0.4397379457950592 + 2.0 * 6.059664249420166
Epoch 550, val loss: 0.7904853820800781
Epoch 560, training loss: 12.534585952758789 = 0.4237716794013977 + 2.0 * 6.0554070472717285
Epoch 560, val loss: 0.7857678532600403
Epoch 570, training loss: 12.516913414001465 = 0.4082616865634918 + 2.0 * 6.054326057434082
Epoch 570, val loss: 0.7817655801773071
Epoch 580, training loss: 12.502453804016113 = 0.39316853880882263 + 2.0 * 6.054642677307129
Epoch 580, val loss: 0.7781953811645508
Epoch 590, training loss: 12.482685089111328 = 0.3785310685634613 + 2.0 * 6.052076816558838
Epoch 590, val loss: 0.77521812915802
Epoch 600, training loss: 12.465144157409668 = 0.3641965687274933 + 2.0 * 6.050473690032959
Epoch 600, val loss: 0.7728163003921509
Epoch 610, training loss: 12.454673767089844 = 0.3501335382461548 + 2.0 * 6.05226993560791
Epoch 610, val loss: 0.770842432975769
Epoch 620, training loss: 12.443374633789062 = 0.33644795417785645 + 2.0 * 6.053463459014893
Epoch 620, val loss: 0.7692394852638245
Epoch 630, training loss: 12.420482635498047 = 0.32313188910484314 + 2.0 * 6.048675537109375
Epoch 630, val loss: 0.7681059837341309
Epoch 640, training loss: 12.40447998046875 = 0.3101158142089844 + 2.0 * 6.047182083129883
Epoch 640, val loss: 0.7674219012260437
Epoch 650, training loss: 12.387786865234375 = 0.2973552942276001 + 2.0 * 6.045215606689453
Epoch 650, val loss: 0.7670751214027405
Epoch 660, training loss: 12.375699043273926 = 0.2848357856273651 + 2.0 * 6.045431613922119
Epoch 660, val loss: 0.7671002149581909
Epoch 670, training loss: 12.367159843444824 = 0.2726499140262604 + 2.0 * 6.047255039215088
Epoch 670, val loss: 0.7673738598823547
Epoch 680, training loss: 12.347389221191406 = 0.2608977258205414 + 2.0 * 6.043245792388916
Epoch 680, val loss: 0.7680265307426453
Epoch 690, training loss: 12.333844184875488 = 0.24945811927318573 + 2.0 * 6.0421929359436035
Epoch 690, val loss: 0.7690892219543457
Epoch 700, training loss: 12.321796417236328 = 0.23833049833774567 + 2.0 * 6.0417327880859375
Epoch 700, val loss: 0.7704745531082153
Epoch 710, training loss: 12.315766334533691 = 0.22757486999034882 + 2.0 * 6.044095516204834
Epoch 710, val loss: 0.7721399068832397
Epoch 720, training loss: 12.302370071411133 = 0.21729815006256104 + 2.0 * 6.042535781860352
Epoch 720, val loss: 0.774201512336731
Epoch 730, training loss: 12.286026000976562 = 0.20737852156162262 + 2.0 * 6.039323806762695
Epoch 730, val loss: 0.7766647934913635
Epoch 740, training loss: 12.273653030395508 = 0.19782070815563202 + 2.0 * 6.03791618347168
Epoch 740, val loss: 0.7794840335845947
Epoch 750, training loss: 12.270099639892578 = 0.18861691653728485 + 2.0 * 6.040741443634033
Epoch 750, val loss: 0.7827180027961731
Epoch 760, training loss: 12.261398315429688 = 0.17986643314361572 + 2.0 * 6.040765762329102
Epoch 760, val loss: 0.786017894744873
Epoch 770, training loss: 12.24273681640625 = 0.17151924967765808 + 2.0 * 6.035608768463135
Epoch 770, val loss: 0.7899186015129089
Epoch 780, training loss: 12.233588218688965 = 0.16354091465473175 + 2.0 * 6.0350236892700195
Epoch 780, val loss: 0.7941893935203552
Epoch 790, training loss: 12.233675956726074 = 0.15591266751289368 + 2.0 * 6.038881778717041
Epoch 790, val loss: 0.798728346824646
Epoch 800, training loss: 12.222421646118164 = 0.1487145870923996 + 2.0 * 6.036853313446045
Epoch 800, val loss: 0.8034613728523254
Epoch 810, training loss: 12.208224296569824 = 0.141866073012352 + 2.0 * 6.03317928314209
Epoch 810, val loss: 0.8085666298866272
Epoch 820, training loss: 12.198210716247559 = 0.13535042107105255 + 2.0 * 6.031430244445801
Epoch 820, val loss: 0.8139806389808655
Epoch 830, training loss: 12.204029083251953 = 0.12915386259555817 + 2.0 * 6.037437438964844
Epoch 830, val loss: 0.819696843624115
Epoch 840, training loss: 12.190216064453125 = 0.1233087033033371 + 2.0 * 6.033453464508057
Epoch 840, val loss: 0.8254095315933228
Epoch 850, training loss: 12.17733383178711 = 0.11781224608421326 + 2.0 * 6.029760837554932
Epoch 850, val loss: 0.831446647644043
Epoch 860, training loss: 12.169990539550781 = 0.11258513480424881 + 2.0 * 6.028702735900879
Epoch 860, val loss: 0.8377067446708679
Epoch 870, training loss: 12.164835929870605 = 0.10761281847953796 + 2.0 * 6.028611660003662
Epoch 870, val loss: 0.8441698551177979
Epoch 880, training loss: 12.160510063171387 = 0.10290785878896713 + 2.0 * 6.028800964355469
Epoch 880, val loss: 0.8506660461425781
Epoch 890, training loss: 12.155348777770996 = 0.09847594052553177 + 2.0 * 6.028436183929443
Epoch 890, val loss: 0.8573631644248962
Epoch 900, training loss: 12.145866394042969 = 0.0942646935582161 + 2.0 * 6.025800704956055
Epoch 900, val loss: 0.8641745448112488
Epoch 910, training loss: 12.146410942077637 = 0.09026233851909637 + 2.0 * 6.028074264526367
Epoch 910, val loss: 0.871058464050293
Epoch 920, training loss: 12.13815975189209 = 0.08648999780416489 + 2.0 * 6.025835037231445
Epoch 920, val loss: 0.8779299855232239
Epoch 930, training loss: 12.132036209106445 = 0.08291081339120865 + 2.0 * 6.024562835693359
Epoch 930, val loss: 0.8849735856056213
Epoch 940, training loss: 12.160061836242676 = 0.07950387895107269 + 2.0 * 6.040278911590576
Epoch 940, val loss: 0.8919689655303955
Epoch 950, training loss: 12.124411582946777 = 0.07633820921182632 + 2.0 * 6.024036884307861
Epoch 950, val loss: 0.8987197279930115
Epoch 960, training loss: 12.117449760437012 = 0.07332254201173782 + 2.0 * 6.022063732147217
Epoch 960, val loss: 0.9057663679122925
Epoch 970, training loss: 12.112688064575195 = 0.07044769078493118 + 2.0 * 6.021120071411133
Epoch 970, val loss: 0.9127801656723022
Epoch 980, training loss: 12.108492851257324 = 0.06770849972963333 + 2.0 * 6.020391941070557
Epoch 980, val loss: 0.9197821021080017
Epoch 990, training loss: 12.104480743408203 = 0.06509393453598022 + 2.0 * 6.019693374633789
Epoch 990, val loss: 0.9268269538879395
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 730, in <module>
    torch.cuda.empty_cache()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/cuda/memory.py", line 121, in empty_cache
    torch._C._cuda_emptyCache()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.4 der2:0 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 18.692426681518555 = 1.944893717765808 + 2.0 * 8.373766899108887
Epoch 0, val loss: 1.946874737739563
Epoch 10, training loss: 18.680767059326172 = 1.934841513633728 + 2.0 * 8.372962951660156
Epoch 10, val loss: 1.9374911785125732
Epoch 20, training loss: 18.658279418945312 = 1.9220836162567139 + 2.0 * 8.368098258972168
Epoch 20, val loss: 1.92502760887146
Epoch 30, training loss: 18.577129364013672 = 1.904491662979126 + 2.0 * 8.336318969726562
Epoch 30, val loss: 1.907496690750122
Epoch 40, training loss: 18.08624267578125 = 1.8835822343826294 + 2.0 * 8.101329803466797
Epoch 40, val loss: 1.8872212171554565
Epoch 50, training loss: 16.0459041595459 = 1.8606327772140503 + 2.0 * 7.0926361083984375
Epoch 50, val loss: 1.8657822608947754
Epoch 60, training loss: 15.434371948242188 = 1.8454996347427368 + 2.0 * 6.794435977935791
Epoch 60, val loss: 1.852372407913208
Epoch 70, training loss: 15.046235084533691 = 1.8347634077072144 + 2.0 * 6.605735778808594
Epoch 70, val loss: 1.8419020175933838
Epoch 80, training loss: 14.819496154785156 = 1.824493646621704 + 2.0 * 6.497501373291016
Epoch 80, val loss: 1.8314814567565918
Epoch 90, training loss: 14.673991203308105 = 1.8147857189178467 + 2.0 * 6.42960262298584
Epoch 90, val loss: 1.821635365486145
Epoch 100, training loss: 14.579200744628906 = 1.8049230575561523 + 2.0 * 6.387138843536377
Epoch 100, val loss: 1.8117161989212036
Epoch 110, training loss: 14.506782531738281 = 1.7957292795181274 + 2.0 * 6.355526447296143
Epoch 110, val loss: 1.802600622177124
Epoch 120, training loss: 14.444005012512207 = 1.786881685256958 + 2.0 * 6.328561782836914
Epoch 120, val loss: 1.793938159942627
Epoch 130, training loss: 14.38717269897461 = 1.7781825065612793 + 2.0 * 6.304495334625244
Epoch 130, val loss: 1.7856810092926025
Epoch 140, training loss: 14.335620880126953 = 1.769065022468567 + 2.0 * 6.283277988433838
Epoch 140, val loss: 1.7773072719573975
Epoch 150, training loss: 14.284887313842773 = 1.759197473526001 + 2.0 * 6.262845039367676
Epoch 150, val loss: 1.7685805559158325
Epoch 160, training loss: 14.232686042785645 = 1.7484124898910522 + 2.0 * 6.2421369552612305
Epoch 160, val loss: 1.7594233751296997
Epoch 170, training loss: 14.1824312210083 = 1.736424207687378 + 2.0 * 6.223003387451172
Epoch 170, val loss: 1.7496123313903809
Epoch 180, training loss: 14.128927230834961 = 1.7228176593780518 + 2.0 * 6.203054904937744
Epoch 180, val loss: 1.7388248443603516
Epoch 190, training loss: 14.079240798950195 = 1.7069467306137085 + 2.0 * 6.186147212982178
Epoch 190, val loss: 1.72637140750885
Epoch 200, training loss: 14.034944534301758 = 1.6878966093063354 + 2.0 * 6.173523902893066
Epoch 200, val loss: 1.7113839387893677
Epoch 210, training loss: 13.987893104553223 = 1.6648662090301514 + 2.0 * 6.161513328552246
Epoch 210, val loss: 1.6932343244552612
Epoch 220, training loss: 13.941987991333008 = 1.6370807886123657 + 2.0 * 6.152453422546387
Epoch 220, val loss: 1.6712205410003662
Epoch 230, training loss: 13.893253326416016 = 1.6037217378616333 + 2.0 * 6.144765853881836
Epoch 230, val loss: 1.6445972919464111
Epoch 240, training loss: 13.84156322479248 = 1.5643116235733032 + 2.0 * 6.138625621795654
Epoch 240, val loss: 1.6130996942520142
Epoch 250, training loss: 13.782845497131348 = 1.5188560485839844 + 2.0 * 6.131994724273682
Epoch 250, val loss: 1.5764333009719849
Epoch 260, training loss: 13.721094131469727 = 1.4674136638641357 + 2.0 * 6.126840114593506
Epoch 260, val loss: 1.5347082614898682
Epoch 270, training loss: 13.660944938659668 = 1.4115277528762817 + 2.0 * 6.124708652496338
Epoch 270, val loss: 1.4893568754196167
Epoch 280, training loss: 13.58870792388916 = 1.3534799814224243 + 2.0 * 6.117613792419434
Epoch 280, val loss: 1.4422693252563477
Epoch 290, training loss: 13.52104377746582 = 1.2942413091659546 + 2.0 * 6.113401412963867
Epoch 290, val loss: 1.3943673372268677
Epoch 300, training loss: 13.454141616821289 = 1.235053539276123 + 2.0 * 6.109544277191162
Epoch 300, val loss: 1.3467906713485718
Epoch 310, training loss: 13.394563674926758 = 1.178141713142395 + 2.0 * 6.108211040496826
Epoch 310, val loss: 1.301863193511963
Epoch 320, training loss: 13.328125953674316 = 1.1241446733474731 + 2.0 * 6.101990699768066
Epoch 320, val loss: 1.2592744827270508
Epoch 330, training loss: 13.268645286560059 = 1.0717653036117554 + 2.0 * 6.098440170288086
Epoch 330, val loss: 1.2184478044509888
Epoch 340, training loss: 13.212186813354492 = 1.0209341049194336 + 2.0 * 6.095626354217529
Epoch 340, val loss: 1.1792511940002441
Epoch 350, training loss: 13.159022331237793 = 0.9725045561790466 + 2.0 * 6.093258857727051
Epoch 350, val loss: 1.1421303749084473
Epoch 360, training loss: 13.102901458740234 = 0.925529420375824 + 2.0 * 6.088685989379883
Epoch 360, val loss: 1.1062663793563843
Epoch 370, training loss: 13.054717063903809 = 0.879862904548645 + 2.0 * 6.087427139282227
Epoch 370, val loss: 1.0713921785354614
Epoch 380, training loss: 13.005926132202148 = 0.8361874222755432 + 2.0 * 6.084869384765625
Epoch 380, val loss: 1.0379893779754639
Epoch 390, training loss: 12.95882797241211 = 0.7945668697357178 + 2.0 * 6.082130432128906
Epoch 390, val loss: 1.0062634944915771
Epoch 400, training loss: 12.913220405578613 = 0.7550358176231384 + 2.0 * 6.079092502593994
Epoch 400, val loss: 0.9762698411941528
Epoch 410, training loss: 12.871916770935059 = 0.71744304895401 + 2.0 * 6.077236652374268
Epoch 410, val loss: 0.9480488896369934
Epoch 420, training loss: 12.832562446594238 = 0.6818885803222656 + 2.0 * 6.075336933135986
Epoch 420, val loss: 0.921470582485199
Epoch 430, training loss: 12.793471336364746 = 0.6481751203536987 + 2.0 * 6.072648048400879
Epoch 430, val loss: 0.8966355323791504
Epoch 440, training loss: 12.758243560791016 = 0.616077184677124 + 2.0 * 6.071083068847656
Epoch 440, val loss: 0.8733291625976562
Epoch 450, training loss: 12.723977088928223 = 0.5856313109397888 + 2.0 * 6.0691728591918945
Epoch 450, val loss: 0.8513550162315369
Epoch 460, training loss: 12.687468528747559 = 0.5565893054008484 + 2.0 * 6.065439701080322
Epoch 460, val loss: 0.830754280090332
Epoch 470, training loss: 12.655193328857422 = 0.5285239219665527 + 2.0 * 6.0633344650268555
Epoch 470, val loss: 0.8112144470214844
Epoch 480, training loss: 12.627591133117676 = 0.5016218423843384 + 2.0 * 6.062984466552734
Epoch 480, val loss: 0.7927268743515015
Epoch 490, training loss: 12.596654891967773 = 0.47610780596733093 + 2.0 * 6.06027364730835
Epoch 490, val loss: 0.7756102085113525
Epoch 500, training loss: 12.567545890808105 = 0.451556533575058 + 2.0 * 6.057994842529297
Epoch 500, val loss: 0.7595010995864868
Epoch 510, training loss: 12.54016399383545 = 0.4277995526790619 + 2.0 * 6.056182384490967
Epoch 510, val loss: 0.7443627715110779
Epoch 520, training loss: 12.51476001739502 = 0.4050038158893585 + 2.0 * 6.054878234863281
Epoch 520, val loss: 0.7303105592727661
Epoch 530, training loss: 12.489273071289062 = 0.38321012258529663 + 2.0 * 6.0530314445495605
Epoch 530, val loss: 0.717490017414093
Epoch 540, training loss: 12.468196868896484 = 0.362273246049881 + 2.0 * 6.052961826324463
Epoch 540, val loss: 0.7056096792221069
Epoch 550, training loss: 12.442995071411133 = 0.34219804406166077 + 2.0 * 6.050398349761963
Epoch 550, val loss: 0.6947101950645447
Epoch 560, training loss: 12.419631958007812 = 0.32298436760902405 + 2.0 * 6.048323631286621
Epoch 560, val loss: 0.684851348400116
Epoch 570, training loss: 12.405094146728516 = 0.3045900762081146 + 2.0 * 6.0502519607543945
Epoch 570, val loss: 0.6758797764778137
Epoch 580, training loss: 12.384742736816406 = 0.2871752977371216 + 2.0 * 6.048783779144287
Epoch 580, val loss: 0.6680299639701843
Epoch 590, training loss: 12.361604690551758 = 0.27079030871391296 + 2.0 * 6.045407295227051
Epoch 590, val loss: 0.661116898059845
Epoch 600, training loss: 12.342503547668457 = 0.25529950857162476 + 2.0 * 6.043601989746094
Epoch 600, val loss: 0.6551159024238586
Epoch 610, training loss: 12.336175918579102 = 0.2406446784734726 + 2.0 * 6.047765731811523
Epoch 610, val loss: 0.6499665975570679
Epoch 620, training loss: 12.309475898742676 = 0.22692102193832397 + 2.0 * 6.0412774085998535
Epoch 620, val loss: 0.6457346677780151
Epoch 630, training loss: 12.292370796203613 = 0.21398326754570007 + 2.0 * 6.039193630218506
Epoch 630, val loss: 0.642251193523407
Epoch 640, training loss: 12.309574127197266 = 0.20183485746383667 + 2.0 * 6.053869724273682
Epoch 640, val loss: 0.6395420432090759
Epoch 650, training loss: 12.273086547851562 = 0.1906013786792755 + 2.0 * 6.041242599487305
Epoch 650, val loss: 0.6375918984413147
Epoch 660, training loss: 12.253800392150879 = 0.1801040768623352 + 2.0 * 6.036848068237305
Epoch 660, val loss: 0.6362084150314331
Epoch 670, training loss: 12.239721298217773 = 0.17021459341049194 + 2.0 * 6.034753322601318
Epoch 670, val loss: 0.635442852973938
Epoch 680, training loss: 12.233264923095703 = 0.16091251373291016 + 2.0 * 6.0361762046813965
Epoch 680, val loss: 0.6353746056556702
Epoch 690, training loss: 12.23216438293457 = 0.15231472253799438 + 2.0 * 6.039924621582031
Epoch 690, val loss: 0.6358116865158081
Epoch 700, training loss: 12.209619522094727 = 0.14435744285583496 + 2.0 * 6.032630920410156
Epoch 700, val loss: 0.6367261409759521
Epoch 710, training loss: 12.198707580566406 = 0.13689635694026947 + 2.0 * 6.030905723571777
Epoch 710, val loss: 0.6380639672279358
Epoch 720, training loss: 12.191949844360352 = 0.12988097965717316 + 2.0 * 6.031034469604492
Epoch 720, val loss: 0.6398545503616333
Epoch 730, training loss: 12.185266494750977 = 0.12333152443170547 + 2.0 * 6.030967712402344
Epoch 730, val loss: 0.6420707702636719
Epoch 740, training loss: 12.173925399780273 = 0.11725901812314987 + 2.0 * 6.0283331871032715
Epoch 740, val loss: 0.6445736885070801
Epoch 750, training loss: 12.166421890258789 = 0.11156368255615234 + 2.0 * 6.027429103851318
Epoch 750, val loss: 0.6474167108535767
Epoch 760, training loss: 12.15943431854248 = 0.10619734227657318 + 2.0 * 6.026618480682373
Epoch 760, val loss: 0.6505759358406067
Epoch 770, training loss: 12.166276931762695 = 0.10116706043481827 + 2.0 * 6.032555103302002
Epoch 770, val loss: 0.6540085673332214
Epoch 780, training loss: 12.153169631958008 = 0.09649249166250229 + 2.0 * 6.028338432312012
Epoch 780, val loss: 0.6576246023178101
Epoch 790, training loss: 12.143088340759277 = 0.09214472770690918 + 2.0 * 6.0254716873168945
Epoch 790, val loss: 0.6614282727241516
Epoch 800, training loss: 12.135137557983398 = 0.08804677426815033 + 2.0 * 6.023545265197754
Epoch 800, val loss: 0.6653466820716858
Epoch 810, training loss: 12.128541946411133 = 0.08417791873216629 + 2.0 * 6.022181987762451
Epoch 810, val loss: 0.6694754362106323
Epoch 820, training loss: 12.135663032531738 = 0.08052697032690048 + 2.0 * 6.0275678634643555
Epoch 820, val loss: 0.6737547516822815
Epoch 830, training loss: 12.126530647277832 = 0.07712580263614655 + 2.0 * 6.024702548980713
Epoch 830, val loss: 0.6780060529708862
Epoch 840, training loss: 12.117518424987793 = 0.07395607233047485 + 2.0 * 6.021780967712402
Epoch 840, val loss: 0.6823529005050659
Epoch 850, training loss: 12.110997200012207 = 0.07095639407634735 + 2.0 * 6.020020484924316
Epoch 850, val loss: 0.6867762804031372
Epoch 860, training loss: 12.106143951416016 = 0.06810958683490753 + 2.0 * 6.019017219543457
Epoch 860, val loss: 0.6912904381752014
Epoch 870, training loss: 12.12530517578125 = 0.06541924923658371 + 2.0 * 6.029942989349365
Epoch 870, val loss: 0.695872962474823
Epoch 880, training loss: 12.0995512008667 = 0.06291788071393967 + 2.0 * 6.018316745758057
Epoch 880, val loss: 0.7004382014274597
Epoch 890, training loss: 12.09400749206543 = 0.06055150553584099 + 2.0 * 6.016727924346924
Epoch 890, val loss: 0.7050302028656006
Epoch 900, training loss: 12.091704368591309 = 0.05829774960875511 + 2.0 * 6.016703128814697
Epoch 900, val loss: 0.7096642255783081
Epoch 910, training loss: 12.107398986816406 = 0.0561610609292984 + 2.0 * 6.025619029998779
Epoch 910, val loss: 0.7142983078956604
Epoch 920, training loss: 12.086752891540527 = 0.05415046587586403 + 2.0 * 6.016301155090332
Epoch 920, val loss: 0.718949019908905
Epoch 930, training loss: 12.084068298339844 = 0.052247535437345505 + 2.0 * 6.0159101486206055
Epoch 930, val loss: 0.7236001491546631
Epoch 940, training loss: 12.081583976745605 = 0.050442133098840714 + 2.0 * 6.015571117401123
Epoch 940, val loss: 0.7281665802001953
Epoch 950, training loss: 12.077717781066895 = 0.04873433709144592 + 2.0 * 6.014491558074951
Epoch 950, val loss: 0.7327715754508972
Epoch 960, training loss: 12.07320499420166 = 0.04710572957992554 + 2.0 * 6.013049602508545
Epoch 960, val loss: 0.7373600006103516
Epoch 970, training loss: 12.07260513305664 = 0.04554515704512596 + 2.0 * 6.0135297775268555
Epoch 970, val loss: 0.7419636249542236
Epoch 980, training loss: 12.069636344909668 = 0.044062186032533646 + 2.0 * 6.012786865234375
Epoch 980, val loss: 0.746571958065033
Epoch 990, training loss: 12.067421913146973 = 0.04265613481402397 + 2.0 * 6.012382984161377
Epoch 990, val loss: 0.7511124610900879
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 730, in <module>
    torch.cuda.empty_cache()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/cuda/memory.py", line 121, in empty_cache
    torch._C._cuda_emptyCache()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.4 der2:0 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 18.677698135375977 = 1.9299548864364624 + 2.0 * 8.373871803283691
Epoch 0, val loss: 1.9226669073104858
Epoch 10, training loss: 18.667037963867188 = 1.9203450679779053 + 2.0 * 8.373346328735352
Epoch 10, val loss: 1.9135491847991943
Epoch 20, training loss: 18.647695541381836 = 1.908420443534851 + 2.0 * 8.369637489318848
Epoch 20, val loss: 1.9018784761428833
Epoch 30, training loss: 18.569486618041992 = 1.8924680948257446 + 2.0 * 8.338509559631348
Epoch 30, val loss: 1.886183738708496
Epoch 40, training loss: 18.003726959228516 = 1.8740311861038208 + 2.0 * 8.064847946166992
Epoch 40, val loss: 1.868838906288147
Epoch 50, training loss: 16.454025268554688 = 1.8564152717590332 + 2.0 * 7.298805236816406
Epoch 50, val loss: 1.8530224561691284
Epoch 60, training loss: 15.585494041442871 = 1.8456627130508423 + 2.0 * 6.86991548538208
Epoch 60, val loss: 1.8429476022720337
Epoch 70, training loss: 15.103650093078613 = 1.837915062904358 + 2.0 * 6.632867336273193
Epoch 70, val loss: 1.835661768913269
Epoch 80, training loss: 14.901596069335938 = 1.829598307609558 + 2.0 * 6.535998821258545
Epoch 80, val loss: 1.8279374837875366
Epoch 90, training loss: 14.758671760559082 = 1.820736289024353 + 2.0 * 6.468967914581299
Epoch 90, val loss: 1.82003915309906
Epoch 100, training loss: 14.64584732055664 = 1.8129162788391113 + 2.0 * 6.416465759277344
Epoch 100, val loss: 1.813083529472351
Epoch 110, training loss: 14.555060386657715 = 1.8064004182815552 + 2.0 * 6.374330043792725
Epoch 110, val loss: 1.8071857690811157
Epoch 120, training loss: 14.47336483001709 = 1.8007700443267822 + 2.0 * 6.336297512054443
Epoch 120, val loss: 1.8020844459533691
Epoch 130, training loss: 14.407590866088867 = 1.7952567338943481 + 2.0 * 6.306167125701904
Epoch 130, val loss: 1.7971954345703125
Epoch 140, training loss: 14.341703414916992 = 1.789541482925415 + 2.0 * 6.276081085205078
Epoch 140, val loss: 1.7921926975250244
Epoch 150, training loss: 14.282708168029785 = 1.7835556268692017 + 2.0 * 6.249576091766357
Epoch 150, val loss: 1.7870500087738037
Epoch 160, training loss: 14.238218307495117 = 1.776936650276184 + 2.0 * 6.230640888214111
Epoch 160, val loss: 1.781380534172058
Epoch 170, training loss: 14.18886947631836 = 1.7693737745285034 + 2.0 * 6.209747791290283
Epoch 170, val loss: 1.7749677896499634
Epoch 180, training loss: 14.1500244140625 = 1.7607096433639526 + 2.0 * 6.194657325744629
Epoch 180, val loss: 1.76780366897583
Epoch 190, training loss: 14.116912841796875 = 1.7507356405258179 + 2.0 * 6.183088779449463
Epoch 190, val loss: 1.7597202062606812
Epoch 200, training loss: 14.081650733947754 = 1.739283800125122 + 2.0 * 6.1711835861206055
Epoch 200, val loss: 1.7505383491516113
Epoch 210, training loss: 14.048212051391602 = 1.7260761260986328 + 2.0 * 6.161067962646484
Epoch 210, val loss: 1.740085244178772
Epoch 220, training loss: 14.014348983764648 = 1.7107698917388916 + 2.0 * 6.151789665222168
Epoch 220, val loss: 1.7280888557434082
Epoch 230, training loss: 13.979351043701172 = 1.69293212890625 + 2.0 * 6.143209457397461
Epoch 230, val loss: 1.7141673564910889
Epoch 240, training loss: 13.942414283752441 = 1.6721165180206299 + 2.0 * 6.135149002075195
Epoch 240, val loss: 1.697920799255371
Epoch 250, training loss: 13.90339469909668 = 1.6478365659713745 + 2.0 * 6.127779006958008
Epoch 250, val loss: 1.678970456123352
Epoch 260, training loss: 13.860992431640625 = 1.619428277015686 + 2.0 * 6.120781898498535
Epoch 260, val loss: 1.6567314863204956
Epoch 270, training loss: 13.818490028381348 = 1.5864230394363403 + 2.0 * 6.116033554077148
Epoch 270, val loss: 1.630814552307129
Epoch 280, training loss: 13.768814086914062 = 1.5481867790222168 + 2.0 * 6.110313892364502
Epoch 280, val loss: 1.6006124019622803
Epoch 290, training loss: 13.71670913696289 = 1.5046972036361694 + 2.0 * 6.106006145477295
Epoch 290, val loss: 1.5661574602127075
Epoch 300, training loss: 13.664990425109863 = 1.4568942785263062 + 2.0 * 6.104048252105713
Epoch 300, val loss: 1.5285857915878296
Epoch 310, training loss: 13.601420402526855 = 1.4057931900024414 + 2.0 * 6.097813606262207
Epoch 310, val loss: 1.4886665344238281
Epoch 320, training loss: 13.5408353805542 = 1.3522255420684814 + 2.0 * 6.094305038452148
Epoch 320, val loss: 1.4470939636230469
Epoch 330, training loss: 13.491495132446289 = 1.2971802949905396 + 2.0 * 6.0971574783325195
Epoch 330, val loss: 1.4049311876296997
Epoch 340, training loss: 13.424762725830078 = 1.2431187629699707 + 2.0 * 6.090821743011475
Epoch 340, val loss: 1.3641843795776367
Epoch 350, training loss: 13.365320205688477 = 1.1906805038452148 + 2.0 * 6.087319850921631
Epoch 350, val loss: 1.3251270055770874
Epoch 360, training loss: 13.308426856994629 = 1.139958143234253 + 2.0 * 6.084234237670898
Epoch 360, val loss: 1.287907361984253
Epoch 370, training loss: 13.280245780944824 = 1.0913563966751099 + 2.0 * 6.094444751739502
Epoch 370, val loss: 1.2528005838394165
Epoch 380, training loss: 13.207916259765625 = 1.0463534593582153 + 2.0 * 6.08078145980835
Epoch 380, val loss: 1.2206647396087646
Epoch 390, training loss: 13.161609649658203 = 1.0040479898452759 + 2.0 * 6.078780651092529
Epoch 390, val loss: 1.191066861152649
Epoch 400, training loss: 13.114559173583984 = 0.9636268615722656 + 2.0 * 6.075466156005859
Epoch 400, val loss: 1.1631150245666504
Epoch 410, training loss: 13.071880340576172 = 0.9247245192527771 + 2.0 * 6.073577880859375
Epoch 410, val loss: 1.1366405487060547
Epoch 420, training loss: 13.049938201904297 = 0.8871169090270996 + 2.0 * 6.081410884857178
Epoch 420, val loss: 1.1116260290145874
Epoch 430, training loss: 12.996360778808594 = 0.8510006070137024 + 2.0 * 6.0726799964904785
Epoch 430, val loss: 1.088142991065979
Epoch 440, training loss: 12.95363998413086 = 0.8157157301902771 + 2.0 * 6.068962097167969
Epoch 440, val loss: 1.0657188892364502
Epoch 450, training loss: 12.917627334594727 = 0.7807047963142395 + 2.0 * 6.0684614181518555
Epoch 450, val loss: 1.0438824892044067
Epoch 460, training loss: 12.882473945617676 = 0.7461610436439514 + 2.0 * 6.0681562423706055
Epoch 460, val loss: 1.0226033926010132
Epoch 470, training loss: 12.842158317565918 = 0.7119492292404175 + 2.0 * 6.0651044845581055
Epoch 470, val loss: 1.001952052116394
Epoch 480, training loss: 12.80675220489502 = 0.6779550909996033 + 2.0 * 6.064398765563965
Epoch 480, val loss: 0.9817132949829102
Epoch 490, training loss: 12.772488594055176 = 0.644341766834259 + 2.0 * 6.06407356262207
Epoch 490, val loss: 0.9618946313858032
Epoch 500, training loss: 12.731322288513184 = 0.6113988161087036 + 2.0 * 6.059961795806885
Epoch 500, val loss: 0.9428284168243408
Epoch 510, training loss: 12.701471328735352 = 0.5789269804954529 + 2.0 * 6.061272144317627
Epoch 510, val loss: 0.9244182705879211
Epoch 520, training loss: 12.668004989624023 = 0.5472168922424316 + 2.0 * 6.060394287109375
Epoch 520, val loss: 0.90684574842453
Epoch 530, training loss: 12.630876541137695 = 0.5165234804153442 + 2.0 * 6.05717658996582
Epoch 530, val loss: 0.890234112739563
Epoch 540, training loss: 12.601699829101562 = 0.48670244216918945 + 2.0 * 6.057498455047607
Epoch 540, val loss: 0.8746528029441833
Epoch 550, training loss: 12.5674409866333 = 0.4579869210720062 + 2.0 * 6.054727077484131
Epoch 550, val loss: 0.8602057099342346
Epoch 560, training loss: 12.538372039794922 = 0.43029725551605225 + 2.0 * 6.054037570953369
Epoch 560, val loss: 0.8469077944755554
Epoch 570, training loss: 12.50722599029541 = 0.40373921394348145 + 2.0 * 6.051743507385254
Epoch 570, val loss: 0.8346821069717407
Epoch 580, training loss: 12.479877471923828 = 0.3783584237098694 + 2.0 * 6.050759315490723
Epoch 580, val loss: 0.823664128780365
Epoch 590, training loss: 12.462396621704102 = 0.3541579246520996 + 2.0 * 6.05411958694458
Epoch 590, val loss: 0.813686192035675
Epoch 600, training loss: 12.432321548461914 = 0.3312651216983795 + 2.0 * 6.050528049468994
Epoch 600, val loss: 0.804810106754303
Epoch 610, training loss: 12.405206680297852 = 0.3097318708896637 + 2.0 * 6.0477375984191895
Epoch 610, val loss: 0.7971386909484863
Epoch 620, training loss: 12.380249977111816 = 0.289344847202301 + 2.0 * 6.04545259475708
Epoch 620, val loss: 0.7904284000396729
Epoch 630, training loss: 12.371882438659668 = 0.27010589838027954 + 2.0 * 6.0508880615234375
Epoch 630, val loss: 0.7845876812934875
Epoch 640, training loss: 12.34371566772461 = 0.2522214651107788 + 2.0 * 6.04574728012085
Epoch 640, val loss: 0.7796365022659302
Epoch 650, training loss: 12.321439743041992 = 0.23561663925647736 + 2.0 * 6.042911529541016
Epoch 650, val loss: 0.7756673097610474
Epoch 660, training loss: 12.303759574890137 = 0.2202068716287613 + 2.0 * 6.041776180267334
Epoch 660, val loss: 0.7725651860237122
Epoch 670, training loss: 12.289552688598633 = 0.2058970183134079 + 2.0 * 6.04182767868042
Epoch 670, val loss: 0.7703399062156677
Epoch 680, training loss: 12.27270221710205 = 0.19271999597549438 + 2.0 * 6.0399909019470215
Epoch 680, val loss: 0.7687828540802002
Epoch 690, training loss: 12.259915351867676 = 0.18065877258777618 + 2.0 * 6.039628505706787
Epoch 690, val loss: 0.7681470513343811
Epoch 700, training loss: 12.249156951904297 = 0.16963258385658264 + 2.0 * 6.039762020111084
Epoch 700, val loss: 0.7681045532226562
Epoch 710, training loss: 12.23297119140625 = 0.1595139503479004 + 2.0 * 6.036728858947754
Epoch 710, val loss: 0.7687703371047974
Epoch 720, training loss: 12.22612476348877 = 0.15026186406612396 + 2.0 * 6.037931442260742
Epoch 720, val loss: 0.7699905037879944
Epoch 730, training loss: 12.209744453430176 = 0.14180639386177063 + 2.0 * 6.033968925476074
Epoch 730, val loss: 0.7718209028244019
Epoch 740, training loss: 12.199834823608398 = 0.13402174413204193 + 2.0 * 6.032906532287598
Epoch 740, val loss: 0.7741098403930664
Epoch 750, training loss: 12.212194442749023 = 0.1268540769815445 + 2.0 * 6.042670249938965
Epoch 750, val loss: 0.7767426371574402
Epoch 760, training loss: 12.183358192443848 = 0.12031430751085281 + 2.0 * 6.031521797180176
Epoch 760, val loss: 0.7797951102256775
Epoch 770, training loss: 12.177587509155273 = 0.11430187523365021 + 2.0 * 6.031642913818359
Epoch 770, val loss: 0.7831971049308777
Epoch 780, training loss: 12.179563522338867 = 0.108732670545578 + 2.0 * 6.0354156494140625
Epoch 780, val loss: 0.7868227362632751
Epoch 790, training loss: 12.171942710876465 = 0.10361405462026596 + 2.0 * 6.0341644287109375
Epoch 790, val loss: 0.7906306982040405
Epoch 800, training loss: 12.155255317687988 = 0.0988805964589119 + 2.0 * 6.028187274932861
Epoch 800, val loss: 0.7946649789810181
Epoch 810, training loss: 12.148785591125488 = 0.09447722136974335 + 2.0 * 6.027153968811035
Epoch 810, val loss: 0.7989064455032349
Epoch 820, training loss: 12.142929077148438 = 0.09035894274711609 + 2.0 * 6.026285171508789
Epoch 820, val loss: 0.8033298254013062
Epoch 830, training loss: 12.1572904586792 = 0.08651796728372574 + 2.0 * 6.035386085510254
Epoch 830, val loss: 0.8078060746192932
Epoch 840, training loss: 12.132872581481934 = 0.08294239640235901 + 2.0 * 6.024965286254883
Epoch 840, val loss: 0.8123892545700073
Epoch 850, training loss: 12.127866744995117 = 0.07960260659456253 + 2.0 * 6.024132251739502
Epoch 850, val loss: 0.8171589374542236
Epoch 860, training loss: 12.122910499572754 = 0.07645761966705322 + 2.0 * 6.023226261138916
Epoch 860, val loss: 0.821929931640625
Epoch 870, training loss: 12.133394241333008 = 0.07349605858325958 + 2.0 * 6.029949188232422
Epoch 870, val loss: 0.8267552256584167
Epoch 880, training loss: 12.122052192687988 = 0.07072515040636063 + 2.0 * 6.025663375854492
Epoch 880, val loss: 0.8315640687942505
Epoch 890, training loss: 12.11396598815918 = 0.06812784820795059 + 2.0 * 6.022919178009033
Epoch 890, val loss: 0.8364382982254028
Epoch 900, training loss: 12.105910301208496 = 0.06567201018333435 + 2.0 * 6.0201191902160645
Epoch 900, val loss: 0.8413469195365906
Epoch 910, training loss: 12.107085227966309 = 0.06333564221858978 + 2.0 * 6.021874904632568
Epoch 910, val loss: 0.8463453054428101
Epoch 920, training loss: 12.099143028259277 = 0.06112373247742653 + 2.0 * 6.019009590148926
Epoch 920, val loss: 0.8512319922447205
Epoch 930, training loss: 12.096701622009277 = 0.0590348020195961 + 2.0 * 6.018833637237549
Epoch 930, val loss: 0.8562225103378296
Epoch 940, training loss: 12.098626136779785 = 0.05705327168107033 + 2.0 * 6.020786285400391
Epoch 940, val loss: 0.861175000667572
Epoch 950, training loss: 12.091588020324707 = 0.055164437741041183 + 2.0 * 6.018211841583252
Epoch 950, val loss: 0.8659960627555847
Epoch 960, training loss: 12.090712547302246 = 0.053382277488708496 + 2.0 * 6.018665313720703
Epoch 960, val loss: 0.8709640502929688
Epoch 970, training loss: 12.085733413696289 = 0.05168057605624199 + 2.0 * 6.017026424407959
Epoch 970, val loss: 0.875784158706665
Epoch 980, training loss: 12.091116905212402 = 0.05005517974495888 + 2.0 * 6.020530700683594
Epoch 980, val loss: 0.8805493712425232
Epoch 990, training loss: 12.086519241333008 = 0.04851570725440979 + 2.0 * 6.0190019607543945
Epoch 990, val loss: 0.885348379611969
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 730, in <module>
    torch.cuda.empty_cache()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/cuda/memory.py", line 121, in empty_cache
    torch._C._cuda_emptyCache()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.4 der2:0 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 18.6914005279541 = 1.9436066150665283 + 2.0 * 8.373896598815918
Epoch 0, val loss: 1.9403358697891235
Epoch 10, training loss: 18.6806697845459 = 1.9337316751480103 + 2.0 * 8.373469352722168
Epoch 10, val loss: 1.9308351278305054
Epoch 20, training loss: 18.66216278076172 = 1.9215255975723267 + 2.0 * 8.370318412780762
Epoch 20, val loss: 1.9191765785217285
Epoch 30, training loss: 18.59621810913086 = 1.904847264289856 + 2.0 * 8.345685005187988
Epoch 30, val loss: 1.9035298824310303
Epoch 40, training loss: 18.196725845336914 = 1.8846524953842163 + 2.0 * 8.156036376953125
Epoch 40, val loss: 1.8855012655258179
Epoch 50, training loss: 16.969436645507812 = 1.8634743690490723 + 2.0 * 7.552980899810791
Epoch 50, val loss: 1.8667337894439697
Epoch 60, training loss: 16.243221282958984 = 1.8472775220870972 + 2.0 * 7.197971820831299
Epoch 60, val loss: 1.852488398551941
Epoch 70, training loss: 15.65582275390625 = 1.834852695465088 + 2.0 * 6.91048526763916
Epoch 70, val loss: 1.8412339687347412
Epoch 80, training loss: 15.187962532043457 = 1.823576807975769 + 2.0 * 6.682192802429199
Epoch 80, val loss: 1.8314416408538818
Epoch 90, training loss: 14.912357330322266 = 1.8124890327453613 + 2.0 * 6.549934387207031
Epoch 90, val loss: 1.8217699527740479
Epoch 100, training loss: 14.72037124633789 = 1.8006415367126465 + 2.0 * 6.459865093231201
Epoch 100, val loss: 1.8115352392196655
Epoch 110, training loss: 14.590221405029297 = 1.787921667098999 + 2.0 * 6.401149749755859
Epoch 110, val loss: 1.8006534576416016
Epoch 120, training loss: 14.496158599853516 = 1.7745895385742188 + 2.0 * 6.360784530639648
Epoch 120, val loss: 1.7893061637878418
Epoch 130, training loss: 14.418667793273926 = 1.7608100175857544 + 2.0 * 6.3289289474487305
Epoch 130, val loss: 1.7775472402572632
Epoch 140, training loss: 14.358053207397461 = 1.7462596893310547 + 2.0 * 6.305896759033203
Epoch 140, val loss: 1.7652921676635742
Epoch 150, training loss: 14.299386024475098 = 1.7303842306137085 + 2.0 * 6.284501075744629
Epoch 150, val loss: 1.7521350383758545
Epoch 160, training loss: 14.24359130859375 = 1.7128256559371948 + 2.0 * 6.265382766723633
Epoch 160, val loss: 1.7376378774642944
Epoch 170, training loss: 14.191740036010742 = 1.692964792251587 + 2.0 * 6.249387741088867
Epoch 170, val loss: 1.7214068174362183
Epoch 180, training loss: 14.144294738769531 = 1.6704380512237549 + 2.0 * 6.236928462982178
Epoch 180, val loss: 1.7031197547912598
Epoch 190, training loss: 14.09111213684082 = 1.6452398300170898 + 2.0 * 6.222936153411865
Epoch 190, val loss: 1.6827278137207031
Epoch 200, training loss: 14.038971900939941 = 1.6169542074203491 + 2.0 * 6.2110090255737305
Epoch 200, val loss: 1.6598902940750122
Epoch 210, training loss: 13.985912322998047 = 1.5851949453353882 + 2.0 * 6.200358867645264
Epoch 210, val loss: 1.634141206741333
Epoch 220, training loss: 13.932737350463867 = 1.5496935844421387 + 2.0 * 6.191521644592285
Epoch 220, val loss: 1.6052597761154175
Epoch 230, training loss: 13.884159088134766 = 1.5109832286834717 + 2.0 * 6.186587810516357
Epoch 230, val loss: 1.5737872123718262
Epoch 240, training loss: 13.823812484741211 = 1.4697383642196655 + 2.0 * 6.177037239074707
Epoch 240, val loss: 1.5403754711151123
Epoch 250, training loss: 13.765223503112793 = 1.4263006448745728 + 2.0 * 6.169461250305176
Epoch 250, val loss: 1.5052987337112427
Epoch 260, training loss: 13.709500312805176 = 1.3810203075408936 + 2.0 * 6.164239883422852
Epoch 260, val loss: 1.4690637588500977
Epoch 270, training loss: 13.652469635009766 = 1.3348270654678345 + 2.0 * 6.158821105957031
Epoch 270, val loss: 1.4325119256973267
Epoch 280, training loss: 13.596512794494629 = 1.2884044647216797 + 2.0 * 6.154054164886475
Epoch 280, val loss: 1.396317720413208
Epoch 290, training loss: 13.539690971374512 = 1.2421973943710327 + 2.0 * 6.148746967315674
Epoch 290, val loss: 1.3608062267303467
Epoch 300, training loss: 13.482398986816406 = 1.1961323022842407 + 2.0 * 6.143133163452148
Epoch 300, val loss: 1.3260518312454224
Epoch 310, training loss: 13.437521934509277 = 1.150451898574829 + 2.0 * 6.143535137176514
Epoch 310, val loss: 1.2921432256698608
Epoch 320, training loss: 13.375670433044434 = 1.1056290864944458 + 2.0 * 6.135020732879639
Epoch 320, val loss: 1.2593055963516235
Epoch 330, training loss: 13.322281837463379 = 1.0612126588821411 + 2.0 * 6.130534648895264
Epoch 330, val loss: 1.2270374298095703
Epoch 340, training loss: 13.269331932067871 = 1.0169066190719604 + 2.0 * 6.1262125968933105
Epoch 340, val loss: 1.195005178451538
Epoch 350, training loss: 13.221519470214844 = 0.9728354215621948 + 2.0 * 6.12434196472168
Epoch 350, val loss: 1.1632946729660034
Epoch 360, training loss: 13.16830062866211 = 0.9295474290847778 + 2.0 * 6.1193766593933105
Epoch 360, val loss: 1.1323102712631226
Epoch 370, training loss: 13.119327545166016 = 0.88695228099823 + 2.0 * 6.116187572479248
Epoch 370, val loss: 1.1020175218582153
Epoch 380, training loss: 13.069499969482422 = 0.8450610041618347 + 2.0 * 6.112219333648682
Epoch 380, val loss: 1.072394609451294
Epoch 390, training loss: 13.041184425354004 = 0.8041016459465027 + 2.0 * 6.118541240692139
Epoch 390, val loss: 1.043662190437317
Epoch 400, training loss: 12.98584270477295 = 0.7646982073783875 + 2.0 * 6.110572338104248
Epoch 400, val loss: 1.0163884162902832
Epoch 410, training loss: 12.936494827270508 = 0.7273275852203369 + 2.0 * 6.104583740234375
Epoch 410, val loss: 0.9909032583236694
Epoch 420, training loss: 12.89478588104248 = 0.691741943359375 + 2.0 * 6.101521968841553
Epoch 420, val loss: 0.9673107862472534
Epoch 430, training loss: 12.853680610656738 = 0.6580052971839905 + 2.0 * 6.097837448120117
Epoch 430, val loss: 0.9454541206359863
Epoch 440, training loss: 12.82669734954834 = 0.626051127910614 + 2.0 * 6.10032320022583
Epoch 440, val loss: 0.9254775643348694
Epoch 450, training loss: 12.783451080322266 = 0.596238374710083 + 2.0 * 6.093606472015381
Epoch 450, val loss: 0.9076108932495117
Epoch 460, training loss: 12.750081062316895 = 0.568411111831665 + 2.0 * 6.090835094451904
Epoch 460, val loss: 0.8917549252510071
Epoch 470, training loss: 12.719870567321777 = 0.5422378182411194 + 2.0 * 6.088816165924072
Epoch 470, val loss: 0.8775595426559448
Epoch 480, training loss: 12.6979398727417 = 0.5176016092300415 + 2.0 * 6.0901689529418945
Epoch 480, val loss: 0.8649460673332214
Epoch 490, training loss: 12.664804458618164 = 0.49456334114074707 + 2.0 * 6.085120677947998
Epoch 490, val loss: 0.8537211418151855
Epoch 500, training loss: 12.637914657592773 = 0.4727262258529663 + 2.0 * 6.082594394683838
Epoch 500, val loss: 0.8436579704284668
Epoch 510, training loss: 12.621576309204102 = 0.45184996724128723 + 2.0 * 6.084863185882568
Epoch 510, val loss: 0.8345920443534851
Epoch 520, training loss: 12.595168113708496 = 0.43178173899650574 + 2.0 * 6.081693172454834
Epoch 520, val loss: 0.8263958692550659
Epoch 530, training loss: 12.565423011779785 = 0.4124848246574402 + 2.0 * 6.0764689445495605
Epoch 530, val loss: 0.8189204931259155
Epoch 540, training loss: 12.5499906539917 = 0.39374443888664246 + 2.0 * 6.078123092651367
Epoch 540, val loss: 0.8120434880256653
Epoch 550, training loss: 12.521873474121094 = 0.3754534125328064 + 2.0 * 6.0732102394104
Epoch 550, val loss: 0.8056015372276306
Epoch 560, training loss: 12.499699592590332 = 0.3575252294540405 + 2.0 * 6.07108736038208
Epoch 560, val loss: 0.7995154857635498
Epoch 570, training loss: 12.479635238647461 = 0.339936763048172 + 2.0 * 6.069849014282227
Epoch 570, val loss: 0.793831467628479
Epoch 580, training loss: 12.462928771972656 = 0.32269933819770813 + 2.0 * 6.070114612579346
Epoch 580, val loss: 0.7885366082191467
Epoch 590, training loss: 12.440261840820312 = 0.3059704601764679 + 2.0 * 6.067145824432373
Epoch 590, val loss: 0.7836374640464783
Epoch 600, training loss: 12.423724174499512 = 0.2896948456764221 + 2.0 * 6.067014694213867
Epoch 600, val loss: 0.7791995406150818
Epoch 610, training loss: 12.402558326721191 = 0.2739039659500122 + 2.0 * 6.064327239990234
Epoch 610, val loss: 0.7752267718315125
Epoch 620, training loss: 12.38387393951416 = 0.2586584985256195 + 2.0 * 6.062607765197754
Epoch 620, val loss: 0.7717073559761047
Epoch 630, training loss: 12.370987892150879 = 0.243966743350029 + 2.0 * 6.063510417938232
Epoch 630, val loss: 0.7686762809753418
Epoch 640, training loss: 12.357154846191406 = 0.2299308031797409 + 2.0 * 6.06361198425293
Epoch 640, val loss: 0.7661647200584412
Epoch 650, training loss: 12.342103958129883 = 0.21657414734363556 + 2.0 * 6.062765121459961
Epoch 650, val loss: 0.7641639113426208
Epoch 660, training loss: 12.323060035705566 = 0.2040172964334488 + 2.0 * 6.059521198272705
Epoch 660, val loss: 0.7626824378967285
Epoch 670, training loss: 12.305838584899902 = 0.19213272631168365 + 2.0 * 6.0568528175354
Epoch 670, val loss: 0.7617025971412659
Epoch 680, training loss: 12.297223091125488 = 0.1809673011302948 + 2.0 * 6.0581278800964355
Epoch 680, val loss: 0.7612415552139282
Epoch 690, training loss: 12.279479026794434 = 0.17053568363189697 + 2.0 * 6.054471492767334
Epoch 690, val loss: 0.7612822651863098
Epoch 700, training loss: 12.272459983825684 = 0.16073793172836304 + 2.0 * 6.055860996246338
Epoch 700, val loss: 0.7617730498313904
Epoch 710, training loss: 12.258095741271973 = 0.15161307156085968 + 2.0 * 6.05324125289917
Epoch 710, val loss: 0.7626627683639526
Epoch 720, training loss: 12.249794006347656 = 0.14312654733657837 + 2.0 * 6.053333759307861
Epoch 720, val loss: 0.7639060020446777
Epoch 730, training loss: 12.236457824707031 = 0.13524273037910461 + 2.0 * 6.050607681274414
Epoch 730, val loss: 0.7655518651008606
Epoch 740, training loss: 12.230082511901855 = 0.12786777317523956 + 2.0 * 6.051107406616211
Epoch 740, val loss: 0.7674993872642517
Epoch 750, training loss: 12.221116065979004 = 0.12101306766271591 + 2.0 * 6.050051689147949
Epoch 750, val loss: 0.7698560357093811
Epoch 760, training loss: 12.21091079711914 = 0.11464279145002365 + 2.0 * 6.048133850097656
Epoch 760, val loss: 0.7724886536598206
Epoch 770, training loss: 12.200709342956543 = 0.10870692878961563 + 2.0 * 6.046001434326172
Epoch 770, val loss: 0.7754201292991638
Epoch 780, training loss: 12.203424453735352 = 0.10318180918693542 + 2.0 * 6.050121307373047
Epoch 780, val loss: 0.778590202331543
Epoch 790, training loss: 12.186771392822266 = 0.09802766144275665 + 2.0 * 6.044372081756592
Epoch 790, val loss: 0.7820013165473938
Epoch 800, training loss: 12.189952850341797 = 0.09322970360517502 + 2.0 * 6.048361778259277
Epoch 800, val loss: 0.785642683506012
Epoch 810, training loss: 12.182576179504395 = 0.08877526968717575 + 2.0 * 6.046900272369385
Epoch 810, val loss: 0.7893845438957214
Epoch 820, training loss: 12.167481422424316 = 0.08461377024650574 + 2.0 * 6.041433811187744
Epoch 820, val loss: 0.7932952046394348
Epoch 830, training loss: 12.161487579345703 = 0.08073259890079498 + 2.0 * 6.040377616882324
Epoch 830, val loss: 0.7973114252090454
Epoch 840, training loss: 12.155776023864746 = 0.0770755484700203 + 2.0 * 6.0393500328063965
Epoch 840, val loss: 0.8014380931854248
Epoch 850, training loss: 12.161393165588379 = 0.07364429533481598 + 2.0 * 6.043874263763428
Epoch 850, val loss: 0.8057029843330383
Epoch 860, training loss: 12.16150951385498 = 0.07045252621173859 + 2.0 * 6.045528411865234
Epoch 860, val loss: 0.8100082278251648
Epoch 870, training loss: 12.142824172973633 = 0.06743495166301727 + 2.0 * 6.037694454193115
Epoch 870, val loss: 0.814403772354126
Epoch 880, training loss: 12.136273384094238 = 0.06461597979068756 + 2.0 * 6.035828590393066
Epoch 880, val loss: 0.8188945651054382
Epoch 890, training loss: 12.132699012756348 = 0.06195690855383873 + 2.0 * 6.035370826721191
Epoch 890, val loss: 0.8234258890151978
Epoch 900, training loss: 12.143376350402832 = 0.05943705141544342 + 2.0 * 6.0419697761535645
Epoch 900, val loss: 0.8280171155929565
Epoch 910, training loss: 12.133588790893555 = 0.05708800256252289 + 2.0 * 6.03825044631958
Epoch 910, val loss: 0.8326664566993713
Epoch 920, training loss: 12.124526023864746 = 0.05487328767776489 + 2.0 * 6.034826278686523
Epoch 920, val loss: 0.8373439908027649
Epoch 930, training loss: 12.11728572845459 = 0.05277417227625847 + 2.0 * 6.03225564956665
Epoch 930, val loss: 0.8420373201370239
Epoch 940, training loss: 12.115535736083984 = 0.0507940798997879 + 2.0 * 6.0323710441589355
Epoch 940, val loss: 0.8467608094215393
Epoch 950, training loss: 12.121698379516602 = 0.04892117902636528 + 2.0 * 6.036388397216797
Epoch 950, val loss: 0.851469874382019
Epoch 960, training loss: 12.110384941101074 = 0.04715012013912201 + 2.0 * 6.031617641448975
Epoch 960, val loss: 0.8561751246452332
Epoch 970, training loss: 12.1057767868042 = 0.04547348991036415 + 2.0 * 6.030151844024658
Epoch 970, val loss: 0.8608796000480652
Epoch 980, training loss: 12.102334976196289 = 0.0438803993165493 + 2.0 * 6.029227256774902
Epoch 980, val loss: 0.8655626773834229
Epoch 990, training loss: 12.109123229980469 = 0.042368803173303604 + 2.0 * 6.033377170562744
Epoch 990, val loss: 0.8702023029327393
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7852
Overall ASR: 0.4797
Flip ASR: 0.3867/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.694690704345703 = 1.947056770324707 + 2.0 * 8.373817443847656
Epoch 0, val loss: 1.9454463720321655
Epoch 10, training loss: 18.678695678710938 = 1.936536192893982 + 2.0 * 8.371079444885254
Epoch 10, val loss: 1.9348416328430176
Epoch 20, training loss: 18.646390914916992 = 1.923718810081482 + 2.0 * 8.361335754394531
Epoch 20, val loss: 1.9212242364883423
Epoch 30, training loss: 18.549604415893555 = 1.906864881515503 + 2.0 * 8.321370124816895
Epoch 30, val loss: 1.9027786254882812
Epoch 40, training loss: 17.938579559326172 = 1.8877540826797485 + 2.0 * 8.025412559509277
Epoch 40, val loss: 1.8819642066955566
Epoch 50, training loss: 16.841276168823242 = 1.8694891929626465 + 2.0 * 7.485893726348877
Epoch 50, val loss: 1.86172616481781
Epoch 60, training loss: 16.145479202270508 = 1.850315809249878 + 2.0 * 7.147581577301025
Epoch 60, val loss: 1.8421173095703125
Epoch 70, training loss: 15.639877319335938 = 1.831322431564331 + 2.0 * 6.904277324676514
Epoch 70, val loss: 1.8233575820922852
Epoch 80, training loss: 15.303627014160156 = 1.8155590295791626 + 2.0 * 6.7440338134765625
Epoch 80, val loss: 1.8077973127365112
Epoch 90, training loss: 15.018174171447754 = 1.8014215230941772 + 2.0 * 6.608376502990723
Epoch 90, val loss: 1.7932974100112915
Epoch 100, training loss: 14.807069778442383 = 1.7875523567199707 + 2.0 * 6.509758472442627
Epoch 100, val loss: 1.779212236404419
Epoch 110, training loss: 14.675847053527832 = 1.773274302482605 + 2.0 * 6.451286315917969
Epoch 110, val loss: 1.7650549411773682
Epoch 120, training loss: 14.568523406982422 = 1.757853627204895 + 2.0 * 6.405334949493408
Epoch 120, val loss: 1.7502087354660034
Epoch 130, training loss: 14.466896057128906 = 1.7415941953659058 + 2.0 * 6.3626508712768555
Epoch 130, val loss: 1.7348328828811646
Epoch 140, training loss: 14.378511428833008 = 1.7243856191635132 + 2.0 * 6.327063083648682
Epoch 140, val loss: 1.718601942062378
Epoch 150, training loss: 14.302242279052734 = 1.705559253692627 + 2.0 * 6.298341274261475
Epoch 150, val loss: 1.7012423276901245
Epoch 160, training loss: 14.232958793640137 = 1.6845391988754272 + 2.0 * 6.274209976196289
Epoch 160, val loss: 1.6821720600128174
Epoch 170, training loss: 14.176620483398438 = 1.6608638763427734 + 2.0 * 6.257878303527832
Epoch 170, val loss: 1.6608617305755615
Epoch 180, training loss: 14.110265731811523 = 1.6342267990112305 + 2.0 * 6.2380194664001465
Epoch 180, val loss: 1.637553334236145
Epoch 190, training loss: 14.049556732177734 = 1.6048270463943481 + 2.0 * 6.222364902496338
Epoch 190, val loss: 1.6121160984039307
Epoch 200, training loss: 13.991552352905273 = 1.5725260972976685 + 2.0 * 6.209513187408447
Epoch 200, val loss: 1.584566593170166
Epoch 210, training loss: 13.937463760375977 = 1.5374834537506104 + 2.0 * 6.199990272521973
Epoch 210, val loss: 1.5552788972854614
Epoch 220, training loss: 13.87862777709961 = 1.50053071975708 + 2.0 * 6.189048767089844
Epoch 220, val loss: 1.524949073791504
Epoch 230, training loss: 13.818741798400879 = 1.4619903564453125 + 2.0 * 6.178375720977783
Epoch 230, val loss: 1.494231104850769
Epoch 240, training loss: 13.759916305541992 = 1.4228074550628662 + 2.0 * 6.168554306030273
Epoch 240, val loss: 1.4636175632476807
Epoch 250, training loss: 13.711580276489258 = 1.383655309677124 + 2.0 * 6.163962364196777
Epoch 250, val loss: 1.4337313175201416
Epoch 260, training loss: 13.65915298461914 = 1.3450744152069092 + 2.0 * 6.157039165496826
Epoch 260, val loss: 1.4055535793304443
Epoch 270, training loss: 13.600223541259766 = 1.3081343173980713 + 2.0 * 6.146044731140137
Epoch 270, val loss: 1.3791673183441162
Epoch 280, training loss: 13.552757263183594 = 1.2726521492004395 + 2.0 * 6.140052318572998
Epoch 280, val loss: 1.3545842170715332
Epoch 290, training loss: 13.514369010925293 = 1.238573431968689 + 2.0 * 6.137897968292236
Epoch 290, val loss: 1.3317525386810303
Epoch 300, training loss: 13.466096878051758 = 1.2062761783599854 + 2.0 * 6.129910469055176
Epoch 300, val loss: 1.3103083372116089
Epoch 310, training loss: 13.421368598937988 = 1.1749308109283447 + 2.0 * 6.123219013214111
Epoch 310, val loss: 1.2899056673049927
Epoch 320, training loss: 13.383099555969238 = 1.1441861391067505 + 2.0 * 6.119456768035889
Epoch 320, val loss: 1.2698873281478882
Epoch 330, training loss: 13.356148719787598 = 1.1136106252670288 + 2.0 * 6.121269226074219
Epoch 330, val loss: 1.2498528957366943
Epoch 340, training loss: 13.305310249328613 = 1.083242654800415 + 2.0 * 6.111033916473389
Epoch 340, val loss: 1.2296890020370483
Epoch 350, training loss: 13.266721725463867 = 1.0524163246154785 + 2.0 * 6.107152462005615
Epoch 350, val loss: 1.208987832069397
Epoch 360, training loss: 13.226994514465332 = 1.0207008123397827 + 2.0 * 6.103147029876709
Epoch 360, val loss: 1.1874921321868896
Epoch 370, training loss: 13.190010070800781 = 0.9879984259605408 + 2.0 * 6.101006031036377
Epoch 370, val loss: 1.16512131690979
Epoch 380, training loss: 13.157876014709473 = 0.9545326232910156 + 2.0 * 6.1016716957092285
Epoch 380, val loss: 1.1419637203216553
Epoch 390, training loss: 13.109338760375977 = 0.9208367466926575 + 2.0 * 6.0942511558532715
Epoch 390, val loss: 1.118465781211853
Epoch 400, training loss: 13.069624900817871 = 0.886802613735199 + 2.0 * 6.091411113739014
Epoch 400, val loss: 1.0947344303131104
Epoch 410, training loss: 13.031673431396484 = 0.8526783585548401 + 2.0 * 6.0894975662231445
Epoch 410, val loss: 1.0709333419799805
Epoch 420, training loss: 12.997895240783691 = 0.8186689615249634 + 2.0 * 6.08961296081543
Epoch 420, val loss: 1.047364592552185
Epoch 430, training loss: 12.957623481750488 = 0.7853663563728333 + 2.0 * 6.0861287117004395
Epoch 430, val loss: 1.0243515968322754
Epoch 440, training loss: 12.918733596801758 = 0.7526278495788574 + 2.0 * 6.083052635192871
Epoch 440, val loss: 1.0020979642868042
Epoch 450, training loss: 12.880228996276855 = 0.7206870913505554 + 2.0 * 6.079771041870117
Epoch 450, val loss: 0.9806121587753296
Epoch 460, training loss: 12.847963333129883 = 0.6895115971565247 + 2.0 * 6.079226016998291
Epoch 460, val loss: 0.960040271282196
Epoch 470, training loss: 12.817123413085938 = 0.6592519879341125 + 2.0 * 6.078935623168945
Epoch 470, val loss: 0.940357506275177
Epoch 480, training loss: 12.779572486877441 = 0.6301044225692749 + 2.0 * 6.074734210968018
Epoch 480, val loss: 0.9220981001853943
Epoch 490, training loss: 12.7487211227417 = 0.602044403553009 + 2.0 * 6.073338508605957
Epoch 490, val loss: 0.9049715399742126
Epoch 500, training loss: 12.71641731262207 = 0.5750935077667236 + 2.0 * 6.070662021636963
Epoch 500, val loss: 0.8890910744667053
Epoch 510, training loss: 12.689485549926758 = 0.5491011738777161 + 2.0 * 6.070192337036133
Epoch 510, val loss: 0.8743342161178589
Epoch 520, training loss: 12.668258666992188 = 0.5241654515266418 + 2.0 * 6.072046756744385
Epoch 520, val loss: 0.8609984517097473
Epoch 530, training loss: 12.633548736572266 = 0.5003724098205566 + 2.0 * 6.066588401794434
Epoch 530, val loss: 0.8490570187568665
Epoch 540, training loss: 12.606317520141602 = 0.47765135765075684 + 2.0 * 6.064332962036133
Epoch 540, val loss: 0.8383405804634094
Epoch 550, training loss: 12.58285140991211 = 0.4558412730693817 + 2.0 * 6.063505172729492
Epoch 550, val loss: 0.8286834955215454
Epoch 560, training loss: 12.564248085021973 = 0.4349600374698639 + 2.0 * 6.064643859863281
Epoch 560, val loss: 0.8202164769172668
Epoch 570, training loss: 12.534425735473633 = 0.4150123596191406 + 2.0 * 6.059706687927246
Epoch 570, val loss: 0.8129777908325195
Epoch 580, training loss: 12.514043807983398 = 0.39602357149124146 + 2.0 * 6.059010028839111
Epoch 580, val loss: 0.8067001104354858
Epoch 590, training loss: 12.505261421203613 = 0.37794730067253113 + 2.0 * 6.063657283782959
Epoch 590, val loss: 0.8014727830886841
Epoch 600, training loss: 12.475491523742676 = 0.3608160614967346 + 2.0 * 6.057337760925293
Epoch 600, val loss: 0.7970899939537048
Epoch 610, training loss: 12.454084396362305 = 0.34462735056877136 + 2.0 * 6.0547285079956055
Epoch 610, val loss: 0.7937588691711426
Epoch 620, training loss: 12.435943603515625 = 0.3293238878250122 + 2.0 * 6.053309917449951
Epoch 620, val loss: 0.79123455286026
Epoch 630, training loss: 12.418693542480469 = 0.3147982656955719 + 2.0 * 6.051947593688965
Epoch 630, val loss: 0.7894273996353149
Epoch 640, training loss: 12.401917457580566 = 0.3010011315345764 + 2.0 * 6.050457954406738
Epoch 640, val loss: 0.7883262634277344
Epoch 650, training loss: 12.402130126953125 = 0.2879038453102112 + 2.0 * 6.057113170623779
Epoch 650, val loss: 0.7877784967422485
Epoch 660, training loss: 12.387943267822266 = 0.27539578080177307 + 2.0 * 6.056273937225342
Epoch 660, val loss: 0.7875946164131165
Epoch 670, training loss: 12.360316276550293 = 0.2636338174343109 + 2.0 * 6.048341274261475
Epoch 670, val loss: 0.7880778908729553
Epoch 680, training loss: 12.344832420349121 = 0.25242677330970764 + 2.0 * 6.046202659606934
Epoch 680, val loss: 0.788854718208313
Epoch 690, training loss: 12.332149505615234 = 0.24174675345420837 + 2.0 * 6.045201301574707
Epoch 690, val loss: 0.7899855971336365
Epoch 700, training loss: 12.322173118591309 = 0.2315303534269333 + 2.0 * 6.045321464538574
Epoch 700, val loss: 0.7913177609443665
Epoch 710, training loss: 12.317155838012695 = 0.22178751230239868 + 2.0 * 6.047684192657471
Epoch 710, val loss: 0.792866051197052
Epoch 720, training loss: 12.299717903137207 = 0.212540864944458 + 2.0 * 6.043588638305664
Epoch 720, val loss: 0.794641375541687
Epoch 730, training loss: 12.286991119384766 = 0.2037377953529358 + 2.0 * 6.041626453399658
Epoch 730, val loss: 0.796647846698761
Epoch 740, training loss: 12.275740623474121 = 0.19533173739910126 + 2.0 * 6.0402045249938965
Epoch 740, val loss: 0.7988997101783752
Epoch 750, training loss: 12.275811195373535 = 0.18729084730148315 + 2.0 * 6.044260025024414
Epoch 750, val loss: 0.8012425303459167
Epoch 760, training loss: 12.271178245544434 = 0.17961439490318298 + 2.0 * 6.045782089233398
Epoch 760, val loss: 0.8036518096923828
Epoch 770, training loss: 12.251208305358887 = 0.17231588065624237 + 2.0 * 6.0394463539123535
Epoch 770, val loss: 0.806367039680481
Epoch 780, training loss: 12.238980293273926 = 0.16537880897521973 + 2.0 * 6.036800861358643
Epoch 780, val loss: 0.8092968463897705
Epoch 790, training loss: 12.231400489807129 = 0.15875479578971863 + 2.0 * 6.036323070526123
Epoch 790, val loss: 0.8124048709869385
Epoch 800, training loss: 12.238105773925781 = 0.15243259072303772 + 2.0 * 6.042836666107178
Epoch 800, val loss: 0.8155642151832581
Epoch 810, training loss: 12.218426704406738 = 0.14636419713497162 + 2.0 * 6.036031246185303
Epoch 810, val loss: 0.8188618421554565
Epoch 820, training loss: 12.207842826843262 = 0.140611931681633 + 2.0 * 6.033615589141846
Epoch 820, val loss: 0.8224391341209412
Epoch 830, training loss: 12.209216117858887 = 0.13509823381900787 + 2.0 * 6.0370588302612305
Epoch 830, val loss: 0.826088011264801
Epoch 840, training loss: 12.199045181274414 = 0.12982992827892303 + 2.0 * 6.034607410430908
Epoch 840, val loss: 0.8298907279968262
Epoch 850, training loss: 12.188911437988281 = 0.12480592727661133 + 2.0 * 6.032052993774414
Epoch 850, val loss: 0.8338557481765747
Epoch 860, training loss: 12.186393737792969 = 0.11998483538627625 + 2.0 * 6.033204555511475
Epoch 860, val loss: 0.837997555732727
Epoch 870, training loss: 12.1799955368042 = 0.11537887156009674 + 2.0 * 6.032308101654053
Epoch 870, val loss: 0.8421982526779175
Epoch 880, training loss: 12.173479080200195 = 0.11098106950521469 + 2.0 * 6.031249046325684
Epoch 880, val loss: 0.8465437293052673
Epoch 890, training loss: 12.164352416992188 = 0.10678236931562424 + 2.0 * 6.028785228729248
Epoch 890, val loss: 0.8511655926704407
Epoch 900, training loss: 12.164331436157227 = 0.10274022817611694 + 2.0 * 6.030795574188232
Epoch 900, val loss: 0.8558414578437805
Epoch 910, training loss: 12.155200004577637 = 0.09885919094085693 + 2.0 * 6.028170585632324
Epoch 910, val loss: 0.8606224060058594
Epoch 920, training loss: 12.152512550354004 = 0.09511375427246094 + 2.0 * 6.0286993980407715
Epoch 920, val loss: 0.8656254410743713
Epoch 930, training loss: 12.147765159606934 = 0.09152678400278091 + 2.0 * 6.028119087219238
Epoch 930, val loss: 0.8705528974533081
Epoch 940, training loss: 12.139508247375488 = 0.08808769285678864 + 2.0 * 6.025710105895996
Epoch 940, val loss: 0.8757679462432861
Epoch 950, training loss: 12.13440227508545 = 0.08477135747671127 + 2.0 * 6.024815559387207
Epoch 950, val loss: 0.8810974955558777
Epoch 960, training loss: 12.132994651794434 = 0.08156097680330276 + 2.0 * 6.025716781616211
Epoch 960, val loss: 0.8865100145339966
Epoch 970, training loss: 12.137744903564453 = 0.07843075692653656 + 2.0 * 6.029656887054443
Epoch 970, val loss: 0.8919459581375122
Epoch 980, training loss: 12.128305435180664 = 0.07541964203119278 + 2.0 * 6.026443004608154
Epoch 980, val loss: 0.8974037766456604
Epoch 990, training loss: 12.119306564331055 = 0.07251643389463425 + 2.0 * 6.02339506149292
Epoch 990, val loss: 0.9030976891517639
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7741
Overall ASR: 0.8044
Flip ASR: 0.7733/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.702253341674805 = 1.9544941186904907 + 2.0 * 8.373879432678223
Epoch 0, val loss: 1.9543389081954956
Epoch 10, training loss: 18.69091033935547 = 1.9441653490066528 + 2.0 * 8.373372077941895
Epoch 10, val loss: 1.9437063932418823
Epoch 20, training loss: 18.670427322387695 = 1.9319250583648682 + 2.0 * 8.369251251220703
Epoch 20, val loss: 1.9311631917953491
Epoch 30, training loss: 18.595314025878906 = 1.9160122871398926 + 2.0 * 8.339651107788086
Epoch 30, val loss: 1.9148675203323364
Epoch 40, training loss: 18.244823455810547 = 1.8972445726394653 + 2.0 * 8.173789024353027
Epoch 40, val loss: 1.896221399307251
Epoch 50, training loss: 17.452850341796875 = 1.8764268159866333 + 2.0 * 7.788211345672607
Epoch 50, val loss: 1.8755004405975342
Epoch 60, training loss: 16.759193420410156 = 1.8573503494262695 + 2.0 * 7.450922012329102
Epoch 60, val loss: 1.8569501638412476
Epoch 70, training loss: 15.9912109375 = 1.8419122695922852 + 2.0 * 7.074649333953857
Epoch 70, val loss: 1.8421276807785034
Epoch 80, training loss: 15.463313102722168 = 1.8274917602539062 + 2.0 * 6.817910671234131
Epoch 80, val loss: 1.8279316425323486
Epoch 90, training loss: 15.068854331970215 = 1.813497543334961 + 2.0 * 6.627678394317627
Epoch 90, val loss: 1.8144714832305908
Epoch 100, training loss: 14.853130340576172 = 1.7996652126312256 + 2.0 * 6.526732444763184
Epoch 100, val loss: 1.8010506629943848
Epoch 110, training loss: 14.69906997680664 = 1.784348964691162 + 2.0 * 6.457360744476318
Epoch 110, val loss: 1.7859493494033813
Epoch 120, training loss: 14.584378242492676 = 1.7683095932006836 + 2.0 * 6.408034324645996
Epoch 120, val loss: 1.7703262567520142
Epoch 130, training loss: 14.482080459594727 = 1.7521166801452637 + 2.0 * 6.364981651306152
Epoch 130, val loss: 1.7548052072525024
Epoch 140, training loss: 14.399568557739258 = 1.7350716590881348 + 2.0 * 6.332248210906982
Epoch 140, val loss: 1.738940715789795
Epoch 150, training loss: 14.322949409484863 = 1.7164537906646729 + 2.0 * 6.303247928619385
Epoch 150, val loss: 1.7219946384429932
Epoch 160, training loss: 14.252802848815918 = 1.6959060430526733 + 2.0 * 6.278448581695557
Epoch 160, val loss: 1.7035491466522217
Epoch 170, training loss: 14.189830780029297 = 1.6729211807250977 + 2.0 * 6.2584547996521
Epoch 170, val loss: 1.6832002401351929
Epoch 180, training loss: 14.13084888458252 = 1.6473541259765625 + 2.0 * 6.2417473793029785
Epoch 180, val loss: 1.6607542037963867
Epoch 190, training loss: 14.07421875 = 1.6187851428985596 + 2.0 * 6.22771692276001
Epoch 190, val loss: 1.6360887289047241
Epoch 200, training loss: 14.018013000488281 = 1.5868293046951294 + 2.0 * 6.215591907501221
Epoch 200, val loss: 1.6088060140609741
Epoch 210, training loss: 13.960063934326172 = 1.551277756690979 + 2.0 * 6.204392910003662
Epoch 210, val loss: 1.5787427425384521
Epoch 220, training loss: 13.901016235351562 = 1.5119333267211914 + 2.0 * 6.1945414543151855
Epoch 220, val loss: 1.545859932899475
Epoch 230, training loss: 13.841216087341309 = 1.4685977697372437 + 2.0 * 6.186309337615967
Epoch 230, val loss: 1.5100805759429932
Epoch 240, training loss: 13.779922485351562 = 1.4216055870056152 + 2.0 * 6.1791582107543945
Epoch 240, val loss: 1.4717785120010376
Epoch 250, training loss: 13.719282150268555 = 1.3716695308685303 + 2.0 * 6.173806190490723
Epoch 250, val loss: 1.4316133260726929
Epoch 260, training loss: 13.655425071716309 = 1.3195656538009644 + 2.0 * 6.167929649353027
Epoch 260, val loss: 1.3900240659713745
Epoch 270, training loss: 13.590356826782227 = 1.265786051750183 + 2.0 * 6.162285327911377
Epoch 270, val loss: 1.3476983308792114
Epoch 280, training loss: 13.525546073913574 = 1.2111399173736572 + 2.0 * 6.157203197479248
Epoch 280, val loss: 1.3050296306610107
Epoch 290, training loss: 13.461670875549316 = 1.1563091278076172 + 2.0 * 6.15268087387085
Epoch 290, val loss: 1.2627687454223633
Epoch 300, training loss: 13.398783683776855 = 1.1023492813110352 + 2.0 * 6.14821720123291
Epoch 300, val loss: 1.2215858697891235
Epoch 310, training loss: 13.343365669250488 = 1.050165057182312 + 2.0 * 6.146600246429443
Epoch 310, val loss: 1.1824235916137695
Epoch 320, training loss: 13.282751083374023 = 1.000520944595337 + 2.0 * 6.141115188598633
Epoch 320, val loss: 1.1458097696304321
Epoch 330, training loss: 13.225944519042969 = 0.953606128692627 + 2.0 * 6.136168956756592
Epoch 330, val loss: 1.11171555519104
Epoch 340, training loss: 13.174298286437988 = 0.9089862704277039 + 2.0 * 6.132656097412109
Epoch 340, val loss: 1.079953908920288
Epoch 350, training loss: 13.121297836303711 = 0.8666247129440308 + 2.0 * 6.127336502075195
Epoch 350, val loss: 1.0505452156066895
Epoch 360, training loss: 13.08141803741455 = 0.8263915181159973 + 2.0 * 6.127513408660889
Epoch 360, val loss: 1.0232422351837158
Epoch 370, training loss: 13.035650253295898 = 0.788491427898407 + 2.0 * 6.123579502105713
Epoch 370, val loss: 0.997998058795929
Epoch 380, training loss: 12.988973617553711 = 0.7526325583457947 + 2.0 * 6.118170738220215
Epoch 380, val loss: 0.9749045968055725
Epoch 390, training loss: 12.947851181030273 = 0.7185539603233337 + 2.0 * 6.114648818969727
Epoch 390, val loss: 0.9535447955131531
Epoch 400, training loss: 12.913026809692383 = 0.6859959363937378 + 2.0 * 6.113515377044678
Epoch 400, val loss: 0.9336914420127869
Epoch 410, training loss: 12.871817588806152 = 0.6549659967422485 + 2.0 * 6.108425617218018
Epoch 410, val loss: 0.9152703881263733
Epoch 420, training loss: 12.838288307189941 = 0.6251736879348755 + 2.0 * 6.106557369232178
Epoch 420, val loss: 0.8981611132621765
Epoch 430, training loss: 12.80771255493164 = 0.5968146324157715 + 2.0 * 6.1054487228393555
Epoch 430, val loss: 0.8822208046913147
Epoch 440, training loss: 12.771920204162598 = 0.5696398019790649 + 2.0 * 6.101140022277832
Epoch 440, val loss: 0.8675563931465149
Epoch 450, training loss: 12.739523887634277 = 0.5435975193977356 + 2.0 * 6.097963333129883
Epoch 450, val loss: 0.8539116978645325
Epoch 460, training loss: 12.710874557495117 = 0.5185550451278687 + 2.0 * 6.096159934997559
Epoch 460, val loss: 0.8411920070648193
Epoch 470, training loss: 12.681293487548828 = 0.49441954493522644 + 2.0 * 6.093437194824219
Epoch 470, val loss: 0.8293812870979309
Epoch 480, training loss: 12.65286922454834 = 0.47118258476257324 + 2.0 * 6.090843200683594
Epoch 480, val loss: 0.8182734251022339
Epoch 490, training loss: 12.638219833374023 = 0.44867056608200073 + 2.0 * 6.0947747230529785
Epoch 490, val loss: 0.8078892827033997
Epoch 500, training loss: 12.600071907043457 = 0.427085280418396 + 2.0 * 6.086493492126465
Epoch 500, val loss: 0.7982317209243774
Epoch 510, training loss: 12.574155807495117 = 0.406241238117218 + 2.0 * 6.083957195281982
Epoch 510, val loss: 0.7891976237297058
Epoch 520, training loss: 12.549500465393066 = 0.38599395751953125 + 2.0 * 6.081753253936768
Epoch 520, val loss: 0.7807437181472778
Epoch 530, training loss: 12.53903865814209 = 0.36634379625320435 + 2.0 * 6.086347579956055
Epoch 530, val loss: 0.7727599740028381
Epoch 540, training loss: 12.506490707397461 = 0.3473265767097473 + 2.0 * 6.079582214355469
Epoch 540, val loss: 0.7653911709785461
Epoch 550, training loss: 12.482987403869629 = 0.32903313636779785 + 2.0 * 6.076977252960205
Epoch 550, val loss: 0.7586029171943665
Epoch 560, training loss: 12.460103988647461 = 0.3113803267478943 + 2.0 * 6.074361801147461
Epoch 560, val loss: 0.7523086667060852
Epoch 570, training loss: 12.4472017288208 = 0.29436299204826355 + 2.0 * 6.076419353485107
Epoch 570, val loss: 0.746489942073822
Epoch 580, training loss: 12.42519760131836 = 0.2779938280582428 + 2.0 * 6.073601722717285
Epoch 580, val loss: 0.7412665486335754
Epoch 590, training loss: 12.405411720275879 = 0.2624365985393524 + 2.0 * 6.0714874267578125
Epoch 590, val loss: 0.7366284728050232
Epoch 600, training loss: 12.386502265930176 = 0.24765072762966156 + 2.0 * 6.069425582885742
Epoch 600, val loss: 0.7325146794319153
Epoch 610, training loss: 12.366415977478027 = 0.23364146053791046 + 2.0 * 6.066387176513672
Epoch 610, val loss: 0.7290223836898804
Epoch 620, training loss: 12.350122451782227 = 0.22037243843078613 + 2.0 * 6.06487512588501
Epoch 620, val loss: 0.72599858045578
Epoch 630, training loss: 12.33860969543457 = 0.2078302502632141 + 2.0 * 6.065389633178711
Epoch 630, val loss: 0.7235108613967896
Epoch 640, training loss: 12.32145881652832 = 0.19604593515396118 + 2.0 * 6.062706470489502
Epoch 640, val loss: 0.7214736938476562
Epoch 650, training loss: 12.311503410339355 = 0.18499134480953217 + 2.0 * 6.06325626373291
Epoch 650, val loss: 0.7199287414550781
Epoch 660, training loss: 12.295564651489258 = 0.17468036711215973 + 2.0 * 6.060441970825195
Epoch 660, val loss: 0.7188591957092285
Epoch 670, training loss: 12.28045654296875 = 0.16503988206386566 + 2.0 * 6.057708263397217
Epoch 670, val loss: 0.7182551026344299
Epoch 680, training loss: 12.268400192260742 = 0.1560411900281906 + 2.0 * 6.056179523468018
Epoch 680, val loss: 0.7180588841438293
Epoch 690, training loss: 12.265130043029785 = 0.14760759472846985 + 2.0 * 6.058761119842529
Epoch 690, val loss: 0.7182214856147766
Epoch 700, training loss: 12.24821662902832 = 0.13975684344768524 + 2.0 * 6.054229736328125
Epoch 700, val loss: 0.7188313603401184
Epoch 710, training loss: 12.250895500183105 = 0.13246290385723114 + 2.0 * 6.059216499328613
Epoch 710, val loss: 0.7197506427764893
Epoch 720, training loss: 12.230224609375 = 0.1256343126296997 + 2.0 * 6.052295207977295
Epoch 720, val loss: 0.720975935459137
Epoch 730, training loss: 12.221039772033691 = 0.11931527405977249 + 2.0 * 6.0508623123168945
Epoch 730, val loss: 0.7224549651145935
Epoch 740, training loss: 12.210752487182617 = 0.11341391503810883 + 2.0 * 6.048669338226318
Epoch 740, val loss: 0.7242376804351807
Epoch 750, training loss: 12.204619407653809 = 0.10788141936063766 + 2.0 * 6.04836893081665
Epoch 750, val loss: 0.7262285947799683
Epoch 760, training loss: 12.200715065002441 = 0.10271154344081879 + 2.0 * 6.049001693725586
Epoch 760, val loss: 0.728428304195404
Epoch 770, training loss: 12.191523551940918 = 0.09788130223751068 + 2.0 * 6.046821117401123
Epoch 770, val loss: 0.730810821056366
Epoch 780, training loss: 12.183935165405273 = 0.09337178617715836 + 2.0 * 6.045281887054443
Epoch 780, val loss: 0.7334215641021729
Epoch 790, training loss: 12.18676471710205 = 0.08914389461278915 + 2.0 * 6.0488104820251465
Epoch 790, val loss: 0.7361266016960144
Epoch 800, training loss: 12.177433013916016 = 0.08516412228345871 + 2.0 * 6.0461344718933105
Epoch 800, val loss: 0.739051103591919
Epoch 810, training loss: 12.16687297821045 = 0.08145304024219513 + 2.0 * 6.042709827423096
Epoch 810, val loss: 0.7420709729194641
Epoch 820, training loss: 12.163521766662598 = 0.07796268910169601 + 2.0 * 6.042779445648193
Epoch 820, val loss: 0.7452112436294556
Epoch 830, training loss: 12.156559944152832 = 0.07466796040534973 + 2.0 * 6.040946006774902
Epoch 830, val loss: 0.7483769655227661
Epoch 840, training loss: 12.152070999145508 = 0.07157179713249207 + 2.0 * 6.040249824523926
Epoch 840, val loss: 0.75163334608078
Epoch 850, training loss: 12.147866249084473 = 0.06866135448217392 + 2.0 * 6.039602279663086
Epoch 850, val loss: 0.7549745440483093
Epoch 860, training loss: 12.146705627441406 = 0.06590276211500168 + 2.0 * 6.040401458740234
Epoch 860, val loss: 0.7582942843437195
Epoch 870, training loss: 12.137640953063965 = 0.06329946219921112 + 2.0 * 6.037170886993408
Epoch 870, val loss: 0.7617855668067932
Epoch 880, training loss: 12.132678985595703 = 0.06084921956062317 + 2.0 * 6.035914897918701
Epoch 880, val loss: 0.7652493119239807
Epoch 890, training loss: 12.149506568908691 = 0.058517083525657654 + 2.0 * 6.045494556427002
Epoch 890, val loss: 0.7687863111495972
Epoch 900, training loss: 12.130106925964355 = 0.05633204057812691 + 2.0 * 6.0368876457214355
Epoch 900, val loss: 0.7723113298416138
Epoch 910, training loss: 12.124079704284668 = 0.05426524579524994 + 2.0 * 6.034907341003418
Epoch 910, val loss: 0.7759007215499878
Epoch 920, training loss: 12.118919372558594 = 0.052303966134786606 + 2.0 * 6.0333075523376465
Epoch 920, val loss: 0.7794936299324036
Epoch 930, training loss: 12.119297981262207 = 0.05044098570942879 + 2.0 * 6.034428596496582
Epoch 930, val loss: 0.7830740213394165
Epoch 940, training loss: 12.117300987243652 = 0.04867340251803398 + 2.0 * 6.034313678741455
Epoch 940, val loss: 0.7866178750991821
Epoch 950, training loss: 12.1091947555542 = 0.0469982773065567 + 2.0 * 6.031098365783691
Epoch 950, val loss: 0.7902243733406067
Epoch 960, training loss: 12.10698127746582 = 0.045406270772218704 + 2.0 * 6.030787467956543
Epoch 960, val loss: 0.7938060760498047
Epoch 970, training loss: 12.104266166687012 = 0.04388219863176346 + 2.0 * 6.030191898345947
Epoch 970, val loss: 0.7974096536636353
Epoch 980, training loss: 12.106444358825684 = 0.042434126138687134 + 2.0 * 6.032005310058594
Epoch 980, val loss: 0.8009515404701233
Epoch 990, training loss: 12.0997314453125 = 0.041053276509046555 + 2.0 * 6.02933931350708
Epoch 990, val loss: 0.8045170307159424
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8000
Overall ASR: 0.8413
Flip ASR: 0.8089/225 nodes
The final ASR:0.70849, 0.16247, Accuracy:0.78642, 0.01062
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11558])
remove edge: torch.Size([2, 9490])
updated graph: torch.Size([2, 10492])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9889
Flip ASR: 0.9867/225 nodes
The final ASR:0.97909, 0.00696, Accuracy:0.82840, 0.00462
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.4 der2:0 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 18.70060920715332 = 1.9527426958084106 + 2.0 * 8.373932838439941
Epoch 0, val loss: 1.9546488523483276
Epoch 10, training loss: 18.689804077148438 = 1.9424535036087036 + 2.0 * 8.373675346374512
Epoch 10, val loss: 1.9445157051086426
Epoch 20, training loss: 18.673498153686523 = 1.92960786819458 + 2.0 * 8.37194538116455
Epoch 20, val loss: 1.9317281246185303
Epoch 30, training loss: 18.6300048828125 = 1.9116060733795166 + 2.0 * 8.359199523925781
Epoch 30, val loss: 1.9137485027313232
Epoch 40, training loss: 18.442094802856445 = 1.8868247270584106 + 2.0 * 8.277634620666504
Epoch 40, val loss: 1.8894851207733154
Epoch 50, training loss: 17.496292114257812 = 1.8578805923461914 + 2.0 * 7.819205284118652
Epoch 50, val loss: 1.8618221282958984
Epoch 60, training loss: 16.529001235961914 = 1.832749366760254 + 2.0 * 7.34812593460083
Epoch 60, val loss: 1.839798927307129
Epoch 70, training loss: 15.722774505615234 = 1.8195279836654663 + 2.0 * 6.951623439788818
Epoch 70, val loss: 1.827803373336792
Epoch 80, training loss: 15.330544471740723 = 1.8077411651611328 + 2.0 * 6.761401653289795
Epoch 80, val loss: 1.8170126676559448
Epoch 90, training loss: 15.066076278686523 = 1.7934966087341309 + 2.0 * 6.636290073394775
Epoch 90, val loss: 1.8036209344863892
Epoch 100, training loss: 14.870551109313965 = 1.7787144184112549 + 2.0 * 6.5459184646606445
Epoch 100, val loss: 1.7905497550964355
Epoch 110, training loss: 14.725471496582031 = 1.7650716304779053 + 2.0 * 6.480199813842773
Epoch 110, val loss: 1.7788554430007935
Epoch 120, training loss: 14.614198684692383 = 1.7512431144714355 + 2.0 * 6.4314775466918945
Epoch 120, val loss: 1.7668179273605347
Epoch 130, training loss: 14.519468307495117 = 1.7362701892852783 + 2.0 * 6.391599178314209
Epoch 130, val loss: 1.7536624670028687
Epoch 140, training loss: 14.435918807983398 = 1.719887375831604 + 2.0 * 6.358015537261963
Epoch 140, val loss: 1.739430546760559
Epoch 150, training loss: 14.362947463989258 = 1.7016324996948242 + 2.0 * 6.330657482147217
Epoch 150, val loss: 1.7238879203796387
Epoch 160, training loss: 14.290678024291992 = 1.6812087297439575 + 2.0 * 6.304734706878662
Epoch 160, val loss: 1.706700086593628
Epoch 170, training loss: 14.22878360748291 = 1.6582720279693604 + 2.0 * 6.2852559089660645
Epoch 170, val loss: 1.6876245737075806
Epoch 180, training loss: 14.160539627075195 = 1.632487177848816 + 2.0 * 6.264026165008545
Epoch 180, val loss: 1.666373610496521
Epoch 190, training loss: 14.10008716583252 = 1.6035104990005493 + 2.0 * 6.248288154602051
Epoch 190, val loss: 1.6424942016601562
Epoch 200, training loss: 14.043821334838867 = 1.5710514783859253 + 2.0 * 6.236384868621826
Epoch 200, val loss: 1.6159075498580933
Epoch 210, training loss: 13.979351997375488 = 1.53542959690094 + 2.0 * 6.22196102142334
Epoch 210, val loss: 1.5868968963623047
Epoch 220, training loss: 13.91782283782959 = 1.4967881441116333 + 2.0 * 6.210517406463623
Epoch 220, val loss: 1.5556713342666626
Epoch 230, training loss: 13.856128692626953 = 1.4553805589675903 + 2.0 * 6.200374126434326
Epoch 230, val loss: 1.522447109222412
Epoch 240, training loss: 13.794281005859375 = 1.412350058555603 + 2.0 * 6.19096565246582
Epoch 240, val loss: 1.4883806705474854
Epoch 250, training loss: 13.733844757080078 = 1.3687738180160522 + 2.0 * 6.182535648345947
Epoch 250, val loss: 1.4543355703353882
Epoch 260, training loss: 13.673540115356445 = 1.3245915174484253 + 2.0 * 6.174474239349365
Epoch 260, val loss: 1.420389175415039
Epoch 270, training loss: 13.62978458404541 = 1.2804487943649292 + 2.0 * 6.174667835235596
Epoch 270, val loss: 1.3866666555404663
Epoch 280, training loss: 13.564302444458008 = 1.2372699975967407 + 2.0 * 6.163516044616699
Epoch 280, val loss: 1.3546839952468872
Epoch 290, training loss: 13.50424861907959 = 1.1951946020126343 + 2.0 * 6.154527187347412
Epoch 290, val loss: 1.323815107345581
Epoch 300, training loss: 13.452126502990723 = 1.1540279388427734 + 2.0 * 6.149049282073975
Epoch 300, val loss: 1.2936452627182007
Epoch 310, training loss: 13.422229766845703 = 1.1139140129089355 + 2.0 * 6.154158115386963
Epoch 310, val loss: 1.2643510103225708
Epoch 320, training loss: 13.355971336364746 = 1.0755665302276611 + 2.0 * 6.140202522277832
Epoch 320, val loss: 1.2363653182983398
Epoch 330, training loss: 13.307493209838867 = 1.0383902788162231 + 2.0 * 6.134551525115967
Epoch 330, val loss: 1.2091679573059082
Epoch 340, training loss: 13.262378692626953 = 1.0019450187683105 + 2.0 * 6.1302170753479
Epoch 340, val loss: 1.1823877096176147
Epoch 350, training loss: 13.218001365661621 = 0.9660271406173706 + 2.0 * 6.1259870529174805
Epoch 350, val loss: 1.1557503938674927
Epoch 360, training loss: 13.17616081237793 = 0.9306176900863647 + 2.0 * 6.122771739959717
Epoch 360, val loss: 1.129367470741272
Epoch 370, training loss: 13.138538360595703 = 0.8959057331085205 + 2.0 * 6.121316432952881
Epoch 370, val loss: 1.1031644344329834
Epoch 380, training loss: 13.093421936035156 = 0.861539900302887 + 2.0 * 6.115941047668457
Epoch 380, val loss: 1.0768860578536987
Epoch 390, training loss: 13.052098274230957 = 0.8270944952964783 + 2.0 * 6.112502098083496
Epoch 390, val loss: 1.0504202842712402
Epoch 400, training loss: 13.028131484985352 = 0.7926803827285767 + 2.0 * 6.117725372314453
Epoch 400, val loss: 1.023770809173584
Epoch 410, training loss: 12.976945877075195 = 0.7586351633071899 + 2.0 * 6.109155178070068
Epoch 410, val loss: 0.9974085092544556
Epoch 420, training loss: 12.934732437133789 = 0.7250613570213318 + 2.0 * 6.104835510253906
Epoch 420, val loss: 0.9713669419288635
Epoch 430, training loss: 12.900871276855469 = 0.6919102072715759 + 2.0 * 6.104480743408203
Epoch 430, val loss: 0.9457111954689026
Epoch 440, training loss: 12.865697860717773 = 0.6596135497093201 + 2.0 * 6.103042125701904
Epoch 440, val loss: 0.9205392003059387
Epoch 450, training loss: 12.823142051696777 = 0.6282582879066467 + 2.0 * 6.097441673278809
Epoch 450, val loss: 0.8965568542480469
Epoch 460, training loss: 12.79041576385498 = 0.5978383421897888 + 2.0 * 6.096288681030273
Epoch 460, val loss: 0.873674750328064
Epoch 470, training loss: 12.757118225097656 = 0.5685239434242249 + 2.0 * 6.094296932220459
Epoch 470, val loss: 0.8518993854522705
Epoch 480, training loss: 12.726920127868652 = 0.5402854681015015 + 2.0 * 6.09331750869751
Epoch 480, val loss: 0.8315056562423706
Epoch 490, training loss: 12.693193435668945 = 0.5134207010269165 + 2.0 * 6.08988618850708
Epoch 490, val loss: 0.8124532699584961
Epoch 500, training loss: 12.670904159545898 = 0.48765066266059875 + 2.0 * 6.0916266441345215
Epoch 500, val loss: 0.7948991656303406
Epoch 510, training loss: 12.6354398727417 = 0.4631567597389221 + 2.0 * 6.086141586303711
Epoch 510, val loss: 0.7788175940513611
Epoch 520, training loss: 12.60838794708252 = 0.43987521529197693 + 2.0 * 6.084256172180176
Epoch 520, val loss: 0.763995349407196
Epoch 530, training loss: 12.601201057434082 = 0.4177035093307495 + 2.0 * 6.0917487144470215
Epoch 530, val loss: 0.7503318786621094
Epoch 540, training loss: 12.557607650756836 = 0.39659664034843445 + 2.0 * 6.08050537109375
Epoch 540, val loss: 0.73806232213974
Epoch 550, training loss: 12.534795761108398 = 0.37661540508270264 + 2.0 * 6.079090118408203
Epoch 550, val loss: 0.7269412875175476
Epoch 560, training loss: 12.521451950073242 = 0.35759255290031433 + 2.0 * 6.081929683685303
Epoch 560, val loss: 0.7167940735816956
Epoch 570, training loss: 12.503271102905273 = 0.3396332859992981 + 2.0 * 6.0818190574646
Epoch 570, val loss: 0.7076316475868225
Epoch 580, training loss: 12.475494384765625 = 0.3226553797721863 + 2.0 * 6.076419353485107
Epoch 580, val loss: 0.699430525302887
Epoch 590, training loss: 12.453245162963867 = 0.3064958155155182 + 2.0 * 6.0733747482299805
Epoch 590, val loss: 0.6921104192733765
Epoch 600, training loss: 12.43679141998291 = 0.29105889797210693 + 2.0 * 6.072866439819336
Epoch 600, val loss: 0.6855565309524536
Epoch 610, training loss: 12.421891212463379 = 0.27635347843170166 + 2.0 * 6.072768688201904
Epoch 610, val loss: 0.6796184182167053
Epoch 620, training loss: 12.401597023010254 = 0.2624364495277405 + 2.0 * 6.069580078125
Epoch 620, val loss: 0.6744354963302612
Epoch 630, training loss: 12.385970115661621 = 0.24915288388729095 + 2.0 * 6.068408489227295
Epoch 630, val loss: 0.6698867082595825
Epoch 640, training loss: 12.375198364257812 = 0.2364596724510193 + 2.0 * 6.069369316101074
Epoch 640, val loss: 0.6658837795257568
Epoch 650, training loss: 12.36300277709961 = 0.22436748445034027 + 2.0 * 6.069317817687988
Epoch 650, val loss: 0.662534773349762
Epoch 660, training loss: 12.348286628723145 = 0.21282856166362762 + 2.0 * 6.0677289962768555
Epoch 660, val loss: 0.6597936749458313
Epoch 670, training loss: 12.32823657989502 = 0.20192813873291016 + 2.0 * 6.063154220581055
Epoch 670, val loss: 0.6576571464538574
Epoch 680, training loss: 12.315363883972168 = 0.1915319263935089 + 2.0 * 6.061915874481201
Epoch 680, val loss: 0.6560744643211365
Epoch 690, training loss: 12.323505401611328 = 0.1816485971212387 + 2.0 * 6.070928573608398
Epoch 690, val loss: 0.6550279855728149
Epoch 700, training loss: 12.29448127746582 = 0.17225125432014465 + 2.0 * 6.06111478805542
Epoch 700, val loss: 0.6545354127883911
Epoch 710, training loss: 12.283039093017578 = 0.16345921158790588 + 2.0 * 6.059790134429932
Epoch 710, val loss: 0.6545366048812866
Epoch 720, training loss: 12.270846366882324 = 0.15513716638088226 + 2.0 * 6.057854652404785
Epoch 720, val loss: 0.6549844741821289
Epoch 730, training loss: 12.259318351745605 = 0.14726637303829193 + 2.0 * 6.056025981903076
Epoch 730, val loss: 0.6558793187141418
Epoch 740, training loss: 12.262659072875977 = 0.1398349106311798 + 2.0 * 6.0614118576049805
Epoch 740, val loss: 0.6572211384773254
Epoch 750, training loss: 12.253890991210938 = 0.13283082842826843 + 2.0 * 6.060530185699463
Epoch 750, val loss: 0.658937394618988
Epoch 760, training loss: 12.238997459411621 = 0.1263412982225418 + 2.0 * 6.056328296661377
Epoch 760, val loss: 0.6609406471252441
Epoch 770, training loss: 12.225764274597168 = 0.1202378049492836 + 2.0 * 6.05276346206665
Epoch 770, val loss: 0.6633317470550537
Epoch 780, training loss: 12.218754768371582 = 0.11448837071657181 + 2.0 * 6.052133083343506
Epoch 780, val loss: 0.6660585999488831
Epoch 790, training loss: 12.228018760681152 = 0.10907977819442749 + 2.0 * 6.059469699859619
Epoch 790, val loss: 0.6690385341644287
Epoch 800, training loss: 12.207930564880371 = 0.1040484681725502 + 2.0 * 6.05194091796875
Epoch 800, val loss: 0.6722328066825867
Epoch 810, training loss: 12.196367263793945 = 0.09928302466869354 + 2.0 * 6.048542022705078
Epoch 810, val loss: 0.6756794452667236
Epoch 820, training loss: 12.190889358520508 = 0.09481114894151688 + 2.0 * 6.048038959503174
Epoch 820, val loss: 0.6793903112411499
Epoch 830, training loss: 12.189332008361816 = 0.09059283137321472 + 2.0 * 6.049369812011719
Epoch 830, val loss: 0.6832365989685059
Epoch 840, training loss: 12.183087348937988 = 0.0866648405790329 + 2.0 * 6.048211097717285
Epoch 840, val loss: 0.6872496604919434
Epoch 850, training loss: 12.175907135009766 = 0.08297006785869598 + 2.0 * 6.046468734741211
Epoch 850, val loss: 0.6914284229278564
Epoch 860, training loss: 12.168407440185547 = 0.07948863506317139 + 2.0 * 6.044459342956543
Epoch 860, val loss: 0.6957597136497498
Epoch 870, training loss: 12.162492752075195 = 0.0762123242020607 + 2.0 * 6.043140411376953
Epoch 870, val loss: 0.7001760601997375
Epoch 880, training loss: 12.169452667236328 = 0.07311417162418365 + 2.0 * 6.048169136047363
Epoch 880, val loss: 0.7046141624450684
Epoch 890, training loss: 12.156267166137695 = 0.07022305577993393 + 2.0 * 6.043022155761719
Epoch 890, val loss: 0.7090884447097778
Epoch 900, training loss: 12.151476860046387 = 0.06747858226299286 + 2.0 * 6.041999340057373
Epoch 900, val loss: 0.7136510610580444
Epoch 910, training loss: 12.145187377929688 = 0.06489898264408112 + 2.0 * 6.040143966674805
Epoch 910, val loss: 0.7183083295822144
Epoch 920, training loss: 12.147417068481445 = 0.062439948320388794 + 2.0 * 6.0424885749816895
Epoch 920, val loss: 0.723038375377655
Epoch 930, training loss: 12.144362449645996 = 0.060114387422800064 + 2.0 * 6.042123794555664
Epoch 930, val loss: 0.7276832461357117
Epoch 940, training loss: 12.13446044921875 = 0.05792437493801117 + 2.0 * 6.038268089294434
Epoch 940, val loss: 0.7323777079582214
Epoch 950, training loss: 12.131192207336426 = 0.055854521691799164 + 2.0 * 6.037668704986572
Epoch 950, val loss: 0.7370858788490295
Epoch 960, training loss: 12.140256881713867 = 0.05388540029525757 + 2.0 * 6.043185710906982
Epoch 960, val loss: 0.7418131828308105
Epoch 970, training loss: 12.128201484680176 = 0.052018728107213974 + 2.0 * 6.03809118270874
Epoch 970, val loss: 0.7464854121208191
Epoch 980, training loss: 12.125765800476074 = 0.05025479197502136 + 2.0 * 6.037755489349365
Epoch 980, val loss: 0.7512043714523315
Epoch 990, training loss: 12.120015144348145 = 0.04857105389237404 + 2.0 * 6.035722255706787
Epoch 990, val loss: 0.7558848857879639
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.5314
Flip ASR: 0.4356/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.6850528717041 = 1.9372668266296387 + 2.0 * 8.373892784118652
Epoch 0, val loss: 1.931909441947937
Epoch 10, training loss: 18.67378044128418 = 1.9267220497131348 + 2.0 * 8.373529434204102
Epoch 10, val loss: 1.9208545684814453
Epoch 20, training loss: 18.655324935913086 = 1.9136242866516113 + 2.0 * 8.370850563049316
Epoch 20, val loss: 1.906937837600708
Epoch 30, training loss: 18.59934425354004 = 1.895828366279602 + 2.0 * 8.351758003234863
Epoch 30, val loss: 1.8876885175704956
Epoch 40, training loss: 18.31538963317871 = 1.8740218877792358 + 2.0 * 8.220684051513672
Epoch 40, val loss: 1.8646618127822876
Epoch 50, training loss: 16.927204132080078 = 1.8521593809127808 + 2.0 * 7.537522792816162
Epoch 50, val loss: 1.8420816659927368
Epoch 60, training loss: 16.231027603149414 = 1.8305038213729858 + 2.0 * 7.20026159286499
Epoch 60, val loss: 1.8207416534423828
Epoch 70, training loss: 15.757598876953125 = 1.8101816177368164 + 2.0 * 6.973708629608154
Epoch 70, val loss: 1.8015203475952148
Epoch 80, training loss: 15.350578308105469 = 1.7949728965759277 + 2.0 * 6.777802467346191
Epoch 80, val loss: 1.7874741554260254
Epoch 90, training loss: 15.063109397888184 = 1.7831958532333374 + 2.0 * 6.639956951141357
Epoch 90, val loss: 1.776196837425232
Epoch 100, training loss: 14.841747283935547 = 1.7708120346069336 + 2.0 * 6.535467624664307
Epoch 100, val loss: 1.7640972137451172
Epoch 110, training loss: 14.700006484985352 = 1.7566438913345337 + 2.0 * 6.471681118011475
Epoch 110, val loss: 1.7506651878356934
Epoch 120, training loss: 14.576233863830566 = 1.7404687404632568 + 2.0 * 6.417882442474365
Epoch 120, val loss: 1.7363377809524536
Epoch 130, training loss: 14.479310989379883 = 1.7233489751815796 + 2.0 * 6.377981185913086
Epoch 130, val loss: 1.721450924873352
Epoch 140, training loss: 14.397275924682617 = 1.7047128677368164 + 2.0 * 6.3462815284729
Epoch 140, val loss: 1.7053309679031372
Epoch 150, training loss: 14.319917678833008 = 1.6834455728530884 + 2.0 * 6.318235874176025
Epoch 150, val loss: 1.6873712539672852
Epoch 160, training loss: 14.248272895812988 = 1.6593239307403564 + 2.0 * 6.2944746017456055
Epoch 160, val loss: 1.667288064956665
Epoch 170, training loss: 14.185428619384766 = 1.6318669319152832 + 2.0 * 6.27678108215332
Epoch 170, val loss: 1.644776463508606
Epoch 180, training loss: 14.113866806030273 = 1.6011817455291748 + 2.0 * 6.25634241104126
Epoch 180, val loss: 1.6198675632476807
Epoch 190, training loss: 14.047223091125488 = 1.5667937994003296 + 2.0 * 6.240214824676514
Epoch 190, val loss: 1.5922681093215942
Epoch 200, training loss: 13.991813659667969 = 1.5285207033157349 + 2.0 * 6.231646537780762
Epoch 200, val loss: 1.561543345451355
Epoch 210, training loss: 13.913885116577148 = 1.486413836479187 + 2.0 * 6.213735580444336
Epoch 210, val loss: 1.5286046266555786
Epoch 220, training loss: 13.844223022460938 = 1.4404877424240112 + 2.0 * 6.201867580413818
Epoch 220, val loss: 1.4927771091461182
Epoch 230, training loss: 13.778661727905273 = 1.390985369682312 + 2.0 * 6.193838119506836
Epoch 230, val loss: 1.4542356729507446
Epoch 240, training loss: 13.704276084899902 = 1.338667631149292 + 2.0 * 6.182804107666016
Epoch 240, val loss: 1.414137840270996
Epoch 250, training loss: 13.633993148803711 = 1.2840771675109863 + 2.0 * 6.174958229064941
Epoch 250, val loss: 1.3726400136947632
Epoch 260, training loss: 13.57059097290039 = 1.2278614044189453 + 2.0 * 6.171364784240723
Epoch 260, val loss: 1.3303920030593872
Epoch 270, training loss: 13.496872901916504 = 1.1715987920761108 + 2.0 * 6.162637233734131
Epoch 270, val loss: 1.28860342502594
Epoch 280, training loss: 13.428813934326172 = 1.1159768104553223 + 2.0 * 6.156418323516846
Epoch 280, val loss: 1.247700572013855
Epoch 290, training loss: 13.367947578430176 = 1.061980962753296 + 2.0 * 6.15298318862915
Epoch 290, val loss: 1.2086480855941772
Epoch 300, training loss: 13.302075386047363 = 1.010536789894104 + 2.0 * 6.145769119262695
Epoch 300, val loss: 1.1720082759857178
Epoch 310, training loss: 13.242410659790039 = 0.9614405632019043 + 2.0 * 6.1404852867126465
Epoch 310, val loss: 1.1377092599868774
Epoch 320, training loss: 13.205253601074219 = 0.9149737358093262 + 2.0 * 6.145139694213867
Epoch 320, val loss: 1.105952501296997
Epoch 330, training loss: 13.140488624572754 = 0.8717755675315857 + 2.0 * 6.134356498718262
Epoch 330, val loss: 1.0774415731430054
Epoch 340, training loss: 13.087201118469238 = 0.8316017389297485 + 2.0 * 6.1277995109558105
Epoch 340, val loss: 1.0517412424087524
Epoch 350, training loss: 13.05756950378418 = 0.7940546274185181 + 2.0 * 6.1317572593688965
Epoch 350, val loss: 1.0285946130752563
Epoch 360, training loss: 13.002182960510254 = 0.7593870162963867 + 2.0 * 6.121397972106934
Epoch 360, val loss: 1.0081229209899902
Epoch 370, training loss: 12.961344718933105 = 0.7272447347640991 + 2.0 * 6.1170501708984375
Epoch 370, val loss: 0.9900440573692322
Epoch 380, training loss: 12.939725875854492 = 0.6971532106399536 + 2.0 * 6.121286392211914
Epoch 380, val loss: 0.973976194858551
Epoch 390, training loss: 12.893104553222656 = 0.6694840788841248 + 2.0 * 6.111810207366943
Epoch 390, val loss: 0.9598582983016968
Epoch 400, training loss: 12.858867645263672 = 0.643690288066864 + 2.0 * 6.107588768005371
Epoch 400, val loss: 0.947624921798706
Epoch 410, training loss: 12.83350944519043 = 0.619470477104187 + 2.0 * 6.107019424438477
Epoch 410, val loss: 0.936824381351471
Epoch 420, training loss: 12.802298545837402 = 0.5966456532478333 + 2.0 * 6.1028265953063965
Epoch 420, val loss: 0.9274687767028809
Epoch 430, training loss: 12.772425651550293 = 0.5752418637275696 + 2.0 * 6.0985918045043945
Epoch 430, val loss: 0.9193057417869568
Epoch 440, training loss: 12.756101608276367 = 0.554969847202301 + 2.0 * 6.1005659103393555
Epoch 440, val loss: 0.9122326374053955
Epoch 450, training loss: 12.726369857788086 = 0.5358623266220093 + 2.0 * 6.095253944396973
Epoch 450, val loss: 0.9061965346336365
Epoch 460, training loss: 12.701448440551758 = 0.5177959203720093 + 2.0 * 6.091826438903809
Epoch 460, val loss: 0.9011725187301636
Epoch 470, training loss: 12.685112953186035 = 0.5006811618804932 + 2.0 * 6.0922160148620605
Epoch 470, val loss: 0.8969867825508118
Epoch 480, training loss: 12.663190841674805 = 0.484330952167511 + 2.0 * 6.08942985534668
Epoch 480, val loss: 0.893615186214447
Epoch 490, training loss: 12.637090682983398 = 0.46880677342414856 + 2.0 * 6.084141731262207
Epoch 490, val loss: 0.8910142779350281
Epoch 500, training loss: 12.625288009643555 = 0.4538441300392151 + 2.0 * 6.085721969604492
Epoch 500, val loss: 0.889133095741272
Epoch 510, training loss: 12.602046966552734 = 0.43945035338401794 + 2.0 * 6.081298351287842
Epoch 510, val loss: 0.8877027034759521
Epoch 520, training loss: 12.58253002166748 = 0.4255325198173523 + 2.0 * 6.078498840332031
Epoch 520, val loss: 0.8868889808654785
Epoch 530, training loss: 12.571523666381836 = 0.4120086431503296 + 2.0 * 6.0797576904296875
Epoch 530, val loss: 0.8864046931266785
Epoch 540, training loss: 12.551078796386719 = 0.3989121913909912 + 2.0 * 6.076083183288574
Epoch 540, val loss: 0.8862999677658081
Epoch 550, training loss: 12.533282279968262 = 0.3860863447189331 + 2.0 * 6.0735979080200195
Epoch 550, val loss: 0.8866063952445984
Epoch 560, training loss: 12.522945404052734 = 0.3734429180622101 + 2.0 * 6.074751377105713
Epoch 560, val loss: 0.8871651291847229
Epoch 570, training loss: 12.506856918334961 = 0.3610181212425232 + 2.0 * 6.0729193687438965
Epoch 570, val loss: 0.887850821018219
Epoch 580, training loss: 12.486014366149902 = 0.3487497866153717 + 2.0 * 6.068632125854492
Epoch 580, val loss: 0.8887743353843689
Epoch 590, training loss: 12.472223281860352 = 0.3366105258464813 + 2.0 * 6.067806243896484
Epoch 590, val loss: 0.8898813724517822
Epoch 600, training loss: 12.461546897888184 = 0.324564665555954 + 2.0 * 6.068490982055664
Epoch 600, val loss: 0.8912490010261536
Epoch 610, training loss: 12.44194221496582 = 0.31261593103408813 + 2.0 * 6.064662933349609
Epoch 610, val loss: 0.8927105665206909
Epoch 620, training loss: 12.427648544311523 = 0.3008899986743927 + 2.0 * 6.063379287719727
Epoch 620, val loss: 0.8944798111915588
Epoch 630, training loss: 12.413992881774902 = 0.2893715798854828 + 2.0 * 6.062310695648193
Epoch 630, val loss: 0.8963803648948669
Epoch 640, training loss: 12.403030395507812 = 0.27805644273757935 + 2.0 * 6.0624871253967285
Epoch 640, val loss: 0.8985148668289185
Epoch 650, training loss: 12.387640953063965 = 0.266945481300354 + 2.0 * 6.060347557067871
Epoch 650, val loss: 0.9008045196533203
Epoch 660, training loss: 12.373204231262207 = 0.2560870051383972 + 2.0 * 6.058558464050293
Epoch 660, val loss: 0.9034767746925354
Epoch 670, training loss: 12.359541893005371 = 0.24547678232192993 + 2.0 * 6.057032585144043
Epoch 670, val loss: 0.9063238501548767
Epoch 680, training loss: 12.355178833007812 = 0.23516002297401428 + 2.0 * 6.060009479522705
Epoch 680, val loss: 0.9094731211662292
Epoch 690, training loss: 12.336494445800781 = 0.22521266341209412 + 2.0 * 6.055640697479248
Epoch 690, val loss: 0.9129743576049805
Epoch 700, training loss: 12.321670532226562 = 0.21554315090179443 + 2.0 * 6.053063869476318
Epoch 700, val loss: 0.9168010354042053
Epoch 710, training loss: 12.308586120605469 = 0.20621761679649353 + 2.0 * 6.051184177398682
Epoch 710, val loss: 0.9209612607955933
Epoch 720, training loss: 12.306558609008789 = 0.19719889760017395 + 2.0 * 6.054679870605469
Epoch 720, val loss: 0.9255173802375793
Epoch 730, training loss: 12.299424171447754 = 0.1886306256055832 + 2.0 * 6.055396556854248
Epoch 730, val loss: 0.9302253723144531
Epoch 740, training loss: 12.282440185546875 = 0.18045662343502045 + 2.0 * 6.050992012023926
Epoch 740, val loss: 0.9352549910545349
Epoch 750, training loss: 12.266891479492188 = 0.17263707518577576 + 2.0 * 6.0471272468566895
Epoch 750, val loss: 0.9405667185783386
Epoch 760, training loss: 12.259071350097656 = 0.16515079140663147 + 2.0 * 6.046960353851318
Epoch 760, val loss: 0.9462685585021973
Epoch 770, training loss: 12.258630752563477 = 0.15799827873706818 + 2.0 * 6.050316333770752
Epoch 770, val loss: 0.952277421951294
Epoch 780, training loss: 12.242185592651367 = 0.15122468769550323 + 2.0 * 6.045480251312256
Epoch 780, val loss: 0.9585301280021667
Epoch 790, training loss: 12.235206604003906 = 0.14476794004440308 + 2.0 * 6.045219421386719
Epoch 790, val loss: 0.9650210738182068
Epoch 800, training loss: 12.22526741027832 = 0.13859626650810242 + 2.0 * 6.043335437774658
Epoch 800, val loss: 0.9716817140579224
Epoch 810, training loss: 12.219917297363281 = 0.13272330164909363 + 2.0 * 6.043597221374512
Epoch 810, val loss: 0.9786381721496582
Epoch 820, training loss: 12.214641571044922 = 0.12713958323001862 + 2.0 * 6.043750762939453
Epoch 820, val loss: 0.9857752919197083
Epoch 830, training loss: 12.20468807220459 = 0.12184613943099976 + 2.0 * 6.041420936584473
Epoch 830, val loss: 0.9931161403656006
Epoch 840, training loss: 12.19754409790039 = 0.11678444594144821 + 2.0 * 6.040380001068115
Epoch 840, val loss: 1.0005183219909668
Epoch 850, training loss: 12.19909954071045 = 0.11198382079601288 + 2.0 * 6.043557643890381
Epoch 850, val loss: 1.008171796798706
Epoch 860, training loss: 12.18529987335205 = 0.10739661008119583 + 2.0 * 6.038951396942139
Epoch 860, val loss: 1.0157896280288696
Epoch 870, training loss: 12.177619934082031 = 0.1030261293053627 + 2.0 * 6.037296772003174
Epoch 870, val loss: 1.0235897302627563
Epoch 880, training loss: 12.17263412475586 = 0.09885627031326294 + 2.0 * 6.03688907623291
Epoch 880, val loss: 1.0314680337905884
Epoch 890, training loss: 12.176811218261719 = 0.09488470107316971 + 2.0 * 6.040963172912598
Epoch 890, val loss: 1.0393774509429932
Epoch 900, training loss: 12.162505149841309 = 0.0911366194486618 + 2.0 * 6.035684108734131
Epoch 900, val loss: 1.047361969947815
Epoch 910, training loss: 12.156914710998535 = 0.08756851404905319 + 2.0 * 6.03467321395874
Epoch 910, val loss: 1.0553613901138306
Epoch 920, training loss: 12.150588035583496 = 0.08416315168142319 + 2.0 * 6.033212661743164
Epoch 920, val loss: 1.063428282737732
Epoch 930, training loss: 12.168745040893555 = 0.08092264086008072 + 2.0 * 6.043910980224609
Epoch 930, val loss: 1.0714521408081055
Epoch 940, training loss: 12.151008605957031 = 0.07783210277557373 + 2.0 * 6.036588191986084
Epoch 940, val loss: 1.0792962312698364
Epoch 950, training loss: 12.137653350830078 = 0.0748903751373291 + 2.0 * 6.031381607055664
Epoch 950, val loss: 1.0871963500976562
Epoch 960, training loss: 12.134868621826172 = 0.07208211719989777 + 2.0 * 6.031393051147461
Epoch 960, val loss: 1.095123052597046
Epoch 970, training loss: 12.138571739196777 = 0.06939882040023804 + 2.0 * 6.034586429595947
Epoch 970, val loss: 1.103125810623169
Epoch 980, training loss: 12.132193565368652 = 0.06683999300003052 + 2.0 * 6.032676696777344
Epoch 980, val loss: 1.1108533143997192
Epoch 990, training loss: 12.129738807678223 = 0.06441996991634369 + 2.0 * 6.032659530639648
Epoch 990, val loss: 1.1186326742172241
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7481
Overall ASR: 0.9926
Flip ASR: 0.9911/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.714040756225586 = 1.9662960767745972 + 2.0 * 8.373872756958008
Epoch 0, val loss: 1.9604895114898682
Epoch 10, training loss: 18.701030731201172 = 1.9545921087265015 + 2.0 * 8.37321949005127
Epoch 10, val loss: 1.9493509531021118
Epoch 20, training loss: 18.67745018005371 = 1.9402729272842407 + 2.0 * 8.3685884475708
Epoch 20, val loss: 1.9353266954421997
Epoch 30, training loss: 18.605806350708008 = 1.9213663339614868 + 2.0 * 8.342220306396484
Epoch 30, val loss: 1.916534423828125
Epoch 40, training loss: 18.282495498657227 = 1.8988980054855347 + 2.0 * 8.19179916381836
Epoch 40, val loss: 1.8948003053665161
Epoch 50, training loss: 17.233644485473633 = 1.8758713006973267 + 2.0 * 7.678886890411377
Epoch 50, val loss: 1.8725374937057495
Epoch 60, training loss: 16.295734405517578 = 1.855290412902832 + 2.0 * 7.220222473144531
Epoch 60, val loss: 1.8529051542282104
Epoch 70, training loss: 15.527668952941895 = 1.8367608785629272 + 2.0 * 6.845454216003418
Epoch 70, val loss: 1.8342396020889282
Epoch 80, training loss: 15.128705978393555 = 1.8194867372512817 + 2.0 * 6.654609680175781
Epoch 80, val loss: 1.817883849143982
Epoch 90, training loss: 14.93053913116455 = 1.8024593591690063 + 2.0 * 6.564039707183838
Epoch 90, val loss: 1.8018622398376465
Epoch 100, training loss: 14.789278030395508 = 1.7844327688217163 + 2.0 * 6.50242280960083
Epoch 100, val loss: 1.7848987579345703
Epoch 110, training loss: 14.677947998046875 = 1.7657493352890015 + 2.0 * 6.456099510192871
Epoch 110, val loss: 1.7672008275985718
Epoch 120, training loss: 14.575129508972168 = 1.7467272281646729 + 2.0 * 6.414201259613037
Epoch 120, val loss: 1.749086618423462
Epoch 130, training loss: 14.481331825256348 = 1.7271337509155273 + 2.0 * 6.37709903717041
Epoch 130, val loss: 1.730520248413086
Epoch 140, training loss: 14.401468276977539 = 1.7062047719955444 + 2.0 * 6.347631931304932
Epoch 140, val loss: 1.7109835147857666
Epoch 150, training loss: 14.33080005645752 = 1.6829742193222046 + 2.0 * 6.323913097381592
Epoch 150, val loss: 1.6898188591003418
Epoch 160, training loss: 14.2650728225708 = 1.6570913791656494 + 2.0 * 6.303990840911865
Epoch 160, val loss: 1.6668734550476074
Epoch 170, training loss: 14.196014404296875 = 1.6284289360046387 + 2.0 * 6.283792495727539
Epoch 170, val loss: 1.6420857906341553
Epoch 180, training loss: 14.143746376037598 = 1.5967470407485962 + 2.0 * 6.273499488830566
Epoch 180, val loss: 1.615328073501587
Epoch 190, training loss: 14.068053245544434 = 1.562198519706726 + 2.0 * 6.252927303314209
Epoch 190, val loss: 1.586916208267212
Epoch 200, training loss: 14.004758834838867 = 1.524731159210205 + 2.0 * 6.24001407623291
Epoch 200, val loss: 1.5563946962356567
Epoch 210, training loss: 13.940912246704102 = 1.484196424484253 + 2.0 * 6.228357791900635
Epoch 210, val loss: 1.5238686800003052
Epoch 220, training loss: 13.877532958984375 = 1.4406849145889282 + 2.0 * 6.218423843383789
Epoch 220, val loss: 1.489594578742981
Epoch 230, training loss: 13.813515663146973 = 1.3953382968902588 + 2.0 * 6.2090888023376465
Epoch 230, val loss: 1.4546496868133545
Epoch 240, training loss: 13.750798225402832 = 1.348950743675232 + 2.0 * 6.200923919677734
Epoch 240, val loss: 1.4195325374603271
Epoch 250, training loss: 13.686624526977539 = 1.3018089532852173 + 2.0 * 6.192407608032227
Epoch 250, val loss: 1.3845735788345337
Epoch 260, training loss: 13.629341125488281 = 1.2547781467437744 + 2.0 * 6.187281608581543
Epoch 260, val loss: 1.3506819009780884
Epoch 270, training loss: 13.564909934997559 = 1.2090320587158203 + 2.0 * 6.177938938140869
Epoch 270, val loss: 1.318114161491394
Epoch 280, training loss: 13.506916999816895 = 1.1643236875534058 + 2.0 * 6.1712965965271
Epoch 280, val loss: 1.2868210077285767
Epoch 290, training loss: 13.450122833251953 = 1.1206443309783936 + 2.0 * 6.16473913192749
Epoch 290, val loss: 1.2566845417022705
Epoch 300, training loss: 13.398796081542969 = 1.0781854391098022 + 2.0 * 6.160305500030518
Epoch 300, val loss: 1.2276663780212402
Epoch 310, training loss: 13.34950065612793 = 1.0373594760894775 + 2.0 * 6.156070709228516
Epoch 310, val loss: 1.2000937461853027
Epoch 320, training loss: 13.29812240600586 = 0.998405396938324 + 2.0 * 6.149858474731445
Epoch 320, val loss: 1.17392897605896
Epoch 330, training loss: 13.248424530029297 = 0.9607378244400024 + 2.0 * 6.143843173980713
Epoch 330, val loss: 1.1489553451538086
Epoch 340, training loss: 13.202990531921387 = 0.9240483641624451 + 2.0 * 6.139471054077148
Epoch 340, val loss: 1.1249115467071533
Epoch 350, training loss: 13.204070091247559 = 0.8884268403053284 + 2.0 * 6.1578216552734375
Epoch 350, val loss: 1.101678729057312
Epoch 360, training loss: 13.124051094055176 = 0.8540483117103577 + 2.0 * 6.135001182556152
Epoch 360, val loss: 1.079671859741211
Epoch 370, training loss: 13.08067798614502 = 0.8208938241004944 + 2.0 * 6.129891872406006
Epoch 370, val loss: 1.0590063333511353
Epoch 380, training loss: 13.03895378112793 = 0.7885346412658691 + 2.0 * 6.125209331512451
Epoch 380, val loss: 1.0391300916671753
Epoch 390, training loss: 12.999669075012207 = 0.7567983865737915 + 2.0 * 6.121435165405273
Epoch 390, val loss: 1.0200819969177246
Epoch 400, training loss: 12.961884498596191 = 0.7257659435272217 + 2.0 * 6.118059158325195
Epoch 400, val loss: 1.0018380880355835
Epoch 410, training loss: 12.925408363342285 = 0.6954951882362366 + 2.0 * 6.114956378936768
Epoch 410, val loss: 0.9846307635307312
Epoch 420, training loss: 12.89832592010498 = 0.6663052439689636 + 2.0 * 6.1160101890563965
Epoch 420, val loss: 0.9686242938041687
Epoch 430, training loss: 12.859753608703613 = 0.6384583115577698 + 2.0 * 6.110647678375244
Epoch 430, val loss: 0.9541262984275818
Epoch 440, training loss: 12.828407287597656 = 0.6118231415748596 + 2.0 * 6.108292102813721
Epoch 440, val loss: 0.9411752820014954
Epoch 450, training loss: 12.79391098022461 = 0.5862056612968445 + 2.0 * 6.10385274887085
Epoch 450, val loss: 0.9295513033866882
Epoch 460, training loss: 12.763762474060059 = 0.5615095496177673 + 2.0 * 6.101126670837402
Epoch 460, val loss: 0.9191783666610718
Epoch 470, training loss: 12.735360145568848 = 0.5376851558685303 + 2.0 * 6.098837375640869
Epoch 470, val loss: 0.9099512100219727
Epoch 480, training loss: 12.711771011352539 = 0.5147759318351746 + 2.0 * 6.09849739074707
Epoch 480, val loss: 0.9018049836158752
Epoch 490, training loss: 12.695549011230469 = 0.4929295778274536 + 2.0 * 6.101309776306152
Epoch 490, val loss: 0.8947387933731079
Epoch 500, training loss: 12.65595817565918 = 0.47225895524024963 + 2.0 * 6.0918498039245605
Epoch 500, val loss: 0.8887518048286438
Epoch 510, training loss: 12.632997512817383 = 0.4524364769458771 + 2.0 * 6.090280532836914
Epoch 510, val loss: 0.8836429715156555
Epoch 520, training loss: 12.609228134155273 = 0.43339112401008606 + 2.0 * 6.087918281555176
Epoch 520, val loss: 0.8792308568954468
Epoch 530, training loss: 12.587212562561035 = 0.41513046622276306 + 2.0 * 6.08604097366333
Epoch 530, val loss: 0.8754456639289856
Epoch 540, training loss: 12.595453262329102 = 0.39770999550819397 + 2.0 * 6.09887170791626
Epoch 540, val loss: 0.8723026514053345
Epoch 550, training loss: 12.546164512634277 = 0.3810169994831085 + 2.0 * 6.082573890686035
Epoch 550, val loss: 0.8696462512016296
Epoch 560, training loss: 12.526227951049805 = 0.3651747703552246 + 2.0 * 6.080526828765869
Epoch 560, val loss: 0.8677572011947632
Epoch 570, training loss: 12.507243156433105 = 0.3500109016895294 + 2.0 * 6.078616142272949
Epoch 570, val loss: 0.8665782809257507
Epoch 580, training loss: 12.487873077392578 = 0.335439532995224 + 2.0 * 6.076216697692871
Epoch 580, val loss: 0.8660247325897217
Epoch 590, training loss: 12.481754302978516 = 0.32148611545562744 + 2.0 * 6.08013391494751
Epoch 590, val loss: 0.8659243583679199
Epoch 600, training loss: 12.456419944763184 = 0.3081647753715515 + 2.0 * 6.074127674102783
Epoch 600, val loss: 0.866179883480072
Epoch 610, training loss: 12.438093185424805 = 0.29545730352401733 + 2.0 * 6.07131814956665
Epoch 610, val loss: 0.8670129179954529
Epoch 620, training loss: 12.422813415527344 = 0.28321707248687744 + 2.0 * 6.069797992706299
Epoch 620, val loss: 0.8682949542999268
Epoch 630, training loss: 12.416074752807617 = 0.27141663432121277 + 2.0 * 6.072329044342041
Epoch 630, val loss: 0.8699356317520142
Epoch 640, training loss: 12.396064758300781 = 0.2601458430290222 + 2.0 * 6.067959308624268
Epoch 640, val loss: 0.8719080686569214
Epoch 650, training loss: 12.383831024169922 = 0.24927623569965363 + 2.0 * 6.067277431488037
Epoch 650, val loss: 0.8742444515228271
Epoch 660, training loss: 12.368350982666016 = 0.23884598910808563 + 2.0 * 6.064752578735352
Epoch 660, val loss: 0.8770137429237366
Epoch 670, training loss: 12.36164379119873 = 0.22874955832958221 + 2.0 * 6.0664472579956055
Epoch 670, val loss: 0.8800545334815979
Epoch 680, training loss: 12.349297523498535 = 0.21910831332206726 + 2.0 * 6.065094470977783
Epoch 680, val loss: 0.8833602070808411
Epoch 690, training loss: 12.32994556427002 = 0.20984546840190887 + 2.0 * 6.060050010681152
Epoch 690, val loss: 0.8868775367736816
Epoch 700, training loss: 12.320834159851074 = 0.20092874765396118 + 2.0 * 6.059952735900879
Epoch 700, val loss: 0.8907135725021362
Epoch 710, training loss: 12.312971115112305 = 0.19234323501586914 + 2.0 * 6.060314178466797
Epoch 710, val loss: 0.8947933912277222
Epoch 720, training loss: 12.298921585083008 = 0.1840946525335312 + 2.0 * 6.057413578033447
Epoch 720, val loss: 0.8990103006362915
Epoch 730, training loss: 12.305419921875 = 0.17615866661071777 + 2.0 * 6.064630508422852
Epoch 730, val loss: 0.9033506512641907
Epoch 740, training loss: 12.281298637390137 = 0.1686386615037918 + 2.0 * 6.05633020401001
Epoch 740, val loss: 0.9079070091247559
Epoch 750, training loss: 12.267854690551758 = 0.16137278079986572 + 2.0 * 6.053240776062012
Epoch 750, val loss: 0.9126963019371033
Epoch 760, training loss: 12.259129524230957 = 0.1544097661972046 + 2.0 * 6.0523600578308105
Epoch 760, val loss: 0.9177437424659729
Epoch 770, training loss: 12.263157844543457 = 0.1477142870426178 + 2.0 * 6.0577216148376465
Epoch 770, val loss: 0.9229186773300171
Epoch 780, training loss: 12.245198249816895 = 0.1413678377866745 + 2.0 * 6.051915168762207
Epoch 780, val loss: 0.9282986521720886
Epoch 790, training loss: 12.235478401184082 = 0.1352647840976715 + 2.0 * 6.050107002258301
Epoch 790, val loss: 0.9337719082832336
Epoch 800, training loss: 12.236077308654785 = 0.12946410477161407 + 2.0 * 6.053306579589844
Epoch 800, val loss: 0.9394646286964417
Epoch 810, training loss: 12.220497131347656 = 0.1239250972867012 + 2.0 * 6.048285961151123
Epoch 810, val loss: 0.9451525211334229
Epoch 820, training loss: 12.21186351776123 = 0.11864801496267319 + 2.0 * 6.046607971191406
Epoch 820, val loss: 0.9509278535842896
Epoch 830, training loss: 12.24152946472168 = 0.11364637315273285 + 2.0 * 6.063941478729248
Epoch 830, val loss: 0.9568698406219482
Epoch 840, training loss: 12.199467658996582 = 0.10885276645421982 + 2.0 * 6.045307636260986
Epoch 840, val loss: 0.9626393914222717
Epoch 850, training loss: 12.191662788391113 = 0.10433075577020645 + 2.0 * 6.043665885925293
Epoch 850, val loss: 0.9686221480369568
Epoch 860, training loss: 12.186018943786621 = 0.10003077238798141 + 2.0 * 6.042994022369385
Epoch 860, val loss: 0.974749743938446
Epoch 870, training loss: 12.18726634979248 = 0.09593622386455536 + 2.0 * 6.045665264129639
Epoch 870, val loss: 0.9810084700584412
Epoch 880, training loss: 12.173466682434082 = 0.09202150255441666 + 2.0 * 6.040722370147705
Epoch 880, val loss: 0.9871587753295898
Epoch 890, training loss: 12.168296813964844 = 0.08831769973039627 + 2.0 * 6.039989471435547
Epoch 890, val loss: 0.993448793888092
Epoch 900, training loss: 12.163098335266113 = 0.08479014039039612 + 2.0 * 6.039154052734375
Epoch 900, val loss: 0.9997845888137817
Epoch 910, training loss: 12.159689903259277 = 0.08142991364002228 + 2.0 * 6.039130210876465
Epoch 910, val loss: 1.0061780214309692
Epoch 920, training loss: 12.165287017822266 = 0.07823516428470612 + 2.0 * 6.043525695800781
Epoch 920, val loss: 1.012617826461792
Epoch 930, training loss: 12.161083221435547 = 0.0752115398645401 + 2.0 * 6.042935848236084
Epoch 930, val loss: 1.0188980102539062
Epoch 940, training loss: 12.147538185119629 = 0.07232765853404999 + 2.0 * 6.037605285644531
Epoch 940, val loss: 1.0251901149749756
Epoch 950, training loss: 12.143024444580078 = 0.06959415972232819 + 2.0 * 6.036715030670166
Epoch 950, val loss: 1.0315693616867065
Epoch 960, training loss: 12.145442962646484 = 0.06700165569782257 + 2.0 * 6.039220809936523
Epoch 960, val loss: 1.037940263748169
Epoch 970, training loss: 12.135156631469727 = 0.06451641768217087 + 2.0 * 6.035320281982422
Epoch 970, val loss: 1.0441752672195435
Epoch 980, training loss: 12.133387565612793 = 0.06215595081448555 + 2.0 * 6.035615921020508
Epoch 980, val loss: 1.0504859685897827
Epoch 990, training loss: 12.136387825012207 = 0.05992628633975983 + 2.0 * 6.038230895996094
Epoch 990, val loss: 1.05679190158844
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7704
Overall ASR: 0.9410
Flip ASR: 0.9289/225 nodes
The final ASR:0.82165, 0.20634, Accuracy:0.77901, 0.02937
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11634])
remove edge: torch.Size([2, 9516])
updated graph: torch.Size([2, 10594])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9926
Flip ASR: 0.9911/225 nodes
The final ASR:0.98278, 0.00696, Accuracy:0.82963, 0.00302
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.4 der2:0 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 18.710845947265625 = 1.9630024433135986 + 2.0 * 8.373921394348145
Epoch 0, val loss: 1.9655628204345703
Epoch 10, training loss: 18.699113845825195 = 1.952020525932312 + 2.0 * 8.373546600341797
Epoch 10, val loss: 1.9551817178726196
Epoch 20, training loss: 18.67929458618164 = 1.9378613233566284 + 2.0 * 8.37071704864502
Epoch 20, val loss: 1.9411578178405762
Epoch 30, training loss: 18.617767333984375 = 1.9177420139312744 + 2.0 * 8.35001277923584
Epoch 30, val loss: 1.9208747148513794
Epoch 40, training loss: 18.30134391784668 = 1.8917187452316284 + 2.0 * 8.204813003540039
Epoch 40, val loss: 1.8957546949386597
Epoch 50, training loss: 17.008453369140625 = 1.863429307937622 + 2.0 * 7.572512149810791
Epoch 50, val loss: 1.869431972503662
Epoch 60, training loss: 16.15851593017578 = 1.8431273698806763 + 2.0 * 7.157693862915039
Epoch 60, val loss: 1.8516210317611694
Epoch 70, training loss: 15.55156135559082 = 1.8286356925964355 + 2.0 * 6.8614630699157715
Epoch 70, val loss: 1.8385869264602661
Epoch 80, training loss: 15.125249862670898 = 1.815111517906189 + 2.0 * 6.655069351196289
Epoch 80, val loss: 1.8268511295318604
Epoch 90, training loss: 14.909733772277832 = 1.8013206720352173 + 2.0 * 6.554206371307373
Epoch 90, val loss: 1.814995288848877
Epoch 100, training loss: 14.764689445495605 = 1.7870129346847534 + 2.0 * 6.488838195800781
Epoch 100, val loss: 1.8024719953536987
Epoch 110, training loss: 14.648369789123535 = 1.7726625204086304 + 2.0 * 6.437853813171387
Epoch 110, val loss: 1.7896660566329956
Epoch 120, training loss: 14.557092666625977 = 1.7582942247390747 + 2.0 * 6.399399280548096
Epoch 120, val loss: 1.7768477201461792
Epoch 130, training loss: 14.478553771972656 = 1.743501901626587 + 2.0 * 6.367526054382324
Epoch 130, val loss: 1.7636325359344482
Epoch 140, training loss: 14.405938148498535 = 1.7276567220687866 + 2.0 * 6.339140892028809
Epoch 140, val loss: 1.7499616146087646
Epoch 150, training loss: 14.335331916809082 = 1.7105525732040405 + 2.0 * 6.312389850616455
Epoch 150, val loss: 1.7354319095611572
Epoch 160, training loss: 14.271886825561523 = 1.691566824913025 + 2.0 * 6.290160179138184
Epoch 160, val loss: 1.7195559740066528
Epoch 170, training loss: 14.219571113586426 = 1.670328140258789 + 2.0 * 6.274621486663818
Epoch 170, val loss: 1.70203697681427
Epoch 180, training loss: 14.160625457763672 = 1.6468569040298462 + 2.0 * 6.2568840980529785
Epoch 180, val loss: 1.682653546333313
Epoch 190, training loss: 14.105253219604492 = 1.6204363107681274 + 2.0 * 6.242408275604248
Epoch 190, val loss: 1.661038875579834
Epoch 200, training loss: 14.054255485534668 = 1.590780258178711 + 2.0 * 6.2317376136779785
Epoch 200, val loss: 1.6368459463119507
Epoch 210, training loss: 14.001457214355469 = 1.5580986738204956 + 2.0 * 6.221679210662842
Epoch 210, val loss: 1.6102595329284668
Epoch 220, training loss: 13.945267677307129 = 1.5223873853683472 + 2.0 * 6.211440086364746
Epoch 220, val loss: 1.581251859664917
Epoch 230, training loss: 13.889997482299805 = 1.4835491180419922 + 2.0 * 6.203224182128906
Epoch 230, val loss: 1.5498716831207275
Epoch 240, training loss: 13.835031509399414 = 1.441680669784546 + 2.0 * 6.1966753005981445
Epoch 240, val loss: 1.5162659883499146
Epoch 250, training loss: 13.777437210083008 = 1.3976820707321167 + 2.0 * 6.189877510070801
Epoch 250, val loss: 1.4811118841171265
Epoch 260, training loss: 13.71617603302002 = 1.3520376682281494 + 2.0 * 6.182069301605225
Epoch 260, val loss: 1.4451121091842651
Epoch 270, training loss: 13.656981468200684 = 1.3051040172576904 + 2.0 * 6.175938606262207
Epoch 270, val loss: 1.4085520505905151
Epoch 280, training loss: 13.597941398620605 = 1.257283329963684 + 2.0 * 6.1703290939331055
Epoch 280, val loss: 1.3718171119689941
Epoch 290, training loss: 13.540549278259277 = 1.2091575860977173 + 2.0 * 6.165695667266846
Epoch 290, val loss: 1.335551142692566
Epoch 300, training loss: 13.486669540405273 = 1.1617478132247925 + 2.0 * 6.162460803985596
Epoch 300, val loss: 1.3001829385757446
Epoch 310, training loss: 13.42686939239502 = 1.1148747205734253 + 2.0 * 6.155997276306152
Epoch 310, val loss: 1.2655811309814453
Epoch 320, training loss: 13.38422966003418 = 1.0687397718429565 + 2.0 * 6.157744884490967
Epoch 320, val loss: 1.2317930459976196
Epoch 330, training loss: 13.323053359985352 = 1.0238171815872192 + 2.0 * 6.149618148803711
Epoch 330, val loss: 1.1995775699615479
Epoch 340, training loss: 13.2688627243042 = 0.9803062081336975 + 2.0 * 6.144278049468994
Epoch 340, val loss: 1.168746829032898
Epoch 350, training loss: 13.218173027038574 = 0.9379879236221313 + 2.0 * 6.140092372894287
Epoch 350, val loss: 1.139271855354309
Epoch 360, training loss: 13.182164192199707 = 0.8970524668693542 + 2.0 * 6.1425557136535645
Epoch 360, val loss: 1.1112760305404663
Epoch 370, training loss: 13.128166198730469 = 0.8578571677207947 + 2.0 * 6.135154724121094
Epoch 370, val loss: 1.0850542783737183
Epoch 380, training loss: 13.081439018249512 = 0.8203489780426025 + 2.0 * 6.130545139312744
Epoch 380, val loss: 1.0606040954589844
Epoch 390, training loss: 13.038467407226562 = 0.7844473123550415 + 2.0 * 6.127009868621826
Epoch 390, val loss: 1.037563443183899
Epoch 400, training loss: 13.000154495239258 = 0.749925971031189 + 2.0 * 6.125114440917969
Epoch 400, val loss: 1.015829086303711
Epoch 410, training loss: 12.962434768676758 = 0.7169209718704224 + 2.0 * 6.1227569580078125
Epoch 410, val loss: 0.9952305555343628
Epoch 420, training loss: 12.922577857971191 = 0.6853986382484436 + 2.0 * 6.118589401245117
Epoch 420, val loss: 0.9759594798088074
Epoch 430, training loss: 12.886064529418945 = 0.6552346348762512 + 2.0 * 6.115415096282959
Epoch 430, val loss: 0.9577836990356445
Epoch 440, training loss: 12.86356258392334 = 0.6262310743331909 + 2.0 * 6.11866569519043
Epoch 440, val loss: 0.940623939037323
Epoch 450, training loss: 12.82026481628418 = 0.5986437797546387 + 2.0 * 6.11081075668335
Epoch 450, val loss: 0.9245750308036804
Epoch 460, training loss: 12.792956352233887 = 0.5722612142562866 + 2.0 * 6.110347747802734
Epoch 460, val loss: 0.909666121006012
Epoch 470, training loss: 12.758552551269531 = 0.5471136569976807 + 2.0 * 6.105719566345215
Epoch 470, val loss: 0.8959519267082214
Epoch 480, training loss: 12.72948169708252 = 0.5232326984405518 + 2.0 * 6.103124618530273
Epoch 480, val loss: 0.883309006690979
Epoch 490, training loss: 12.708477973937988 = 0.5003808736801147 + 2.0 * 6.104048728942871
Epoch 490, val loss: 0.8716800808906555
Epoch 500, training loss: 12.677762031555176 = 0.47873467206954956 + 2.0 * 6.099513530731201
Epoch 500, val loss: 0.8611055612564087
Epoch 510, training loss: 12.649703979492188 = 0.45798608660697937 + 2.0 * 6.095859050750732
Epoch 510, val loss: 0.8515195250511169
Epoch 520, training loss: 12.626611709594727 = 0.43793001770973206 + 2.0 * 6.094340801239014
Epoch 520, val loss: 0.8426983952522278
Epoch 530, training loss: 12.60306167602539 = 0.41846397519111633 + 2.0 * 6.092298984527588
Epoch 530, val loss: 0.8345472812652588
Epoch 540, training loss: 12.583066940307617 = 0.39951539039611816 + 2.0 * 6.091775894165039
Epoch 540, val loss: 0.8270759582519531
Epoch 550, training loss: 12.558109283447266 = 0.38110822439193726 + 2.0 * 6.088500499725342
Epoch 550, val loss: 0.8200487494468689
Epoch 560, training loss: 12.534197807312012 = 0.3630976974964142 + 2.0 * 6.085549831390381
Epoch 560, val loss: 0.8136507868766785
Epoch 570, training loss: 12.51274585723877 = 0.3454333245754242 + 2.0 * 6.083656311035156
Epoch 570, val loss: 0.8076480627059937
Epoch 580, training loss: 12.501437187194824 = 0.3280332088470459 + 2.0 * 6.0867018699646
Epoch 580, val loss: 0.802105724811554
Epoch 590, training loss: 12.478018760681152 = 0.31110021471977234 + 2.0 * 6.083459377288818
Epoch 590, val loss: 0.7968298196792603
Epoch 600, training loss: 12.453142166137695 = 0.29451319575309753 + 2.0 * 6.079314708709717
Epoch 600, val loss: 0.7920882105827332
Epoch 610, training loss: 12.433013916015625 = 0.27835333347320557 + 2.0 * 6.077330112457275
Epoch 610, val loss: 0.787766695022583
Epoch 620, training loss: 12.424824714660645 = 0.2626263201236725 + 2.0 * 6.081099033355713
Epoch 620, val loss: 0.7839042544364929
Epoch 630, training loss: 12.402372360229492 = 0.24751083552837372 + 2.0 * 6.077430725097656
Epoch 630, val loss: 0.7804993391036987
Epoch 640, training loss: 12.379399299621582 = 0.23301537334918976 + 2.0 * 6.073192119598389
Epoch 640, val loss: 0.7777243852615356
Epoch 650, training loss: 12.362776756286621 = 0.2192051261663437 + 2.0 * 6.071785926818848
Epoch 650, val loss: 0.7755293250083923
Epoch 660, training loss: 12.350526809692383 = 0.20610597729682922 + 2.0 * 6.072210311889648
Epoch 660, val loss: 0.7738387584686279
Epoch 670, training loss: 12.340349197387695 = 0.19376163184642792 + 2.0 * 6.073293685913086
Epoch 670, val loss: 0.7727256417274475
Epoch 680, training loss: 12.317380905151367 = 0.1822008192539215 + 2.0 * 6.067590236663818
Epoch 680, val loss: 0.7722167372703552
Epoch 690, training loss: 12.307353973388672 = 0.17138810455799103 + 2.0 * 6.067983150482178
Epoch 690, val loss: 0.7723166346549988
Epoch 700, training loss: 12.29183578491211 = 0.16127167642116547 + 2.0 * 6.065281867980957
Epoch 700, val loss: 0.7729198932647705
Epoch 710, training loss: 12.282074928283691 = 0.1518401950597763 + 2.0 * 6.065117359161377
Epoch 710, val loss: 0.7740257382392883
Epoch 720, training loss: 12.271825790405273 = 0.14308413863182068 + 2.0 * 6.064370632171631
Epoch 720, val loss: 0.7757205367088318
Epoch 730, training loss: 12.25926685333252 = 0.13491281867027283 + 2.0 * 6.0621771812438965
Epoch 730, val loss: 0.7778387665748596
Epoch 740, training loss: 12.25082015991211 = 0.12732714414596558 + 2.0 * 6.061746597290039
Epoch 740, val loss: 0.7804521918296814
Epoch 750, training loss: 12.242565155029297 = 0.12026428431272507 + 2.0 * 6.061150550842285
Epoch 750, val loss: 0.7834466099739075
Epoch 760, training loss: 12.234382629394531 = 0.11374887824058533 + 2.0 * 6.060317039489746
Epoch 760, val loss: 0.7869448661804199
Epoch 770, training loss: 12.222864151000977 = 0.10765305906534195 + 2.0 * 6.057605743408203
Epoch 770, val loss: 0.7907573580741882
Epoch 780, training loss: 12.215063095092773 = 0.10200590640306473 + 2.0 * 6.056528568267822
Epoch 780, val loss: 0.7949560284614563
Epoch 790, training loss: 12.211461067199707 = 0.09675013273954391 + 2.0 * 6.0573554039001465
Epoch 790, val loss: 0.7994197010993958
Epoch 800, training loss: 12.202404022216797 = 0.09185336530208588 + 2.0 * 6.0552754402160645
Epoch 800, val loss: 0.804062008857727
Epoch 810, training loss: 12.20099925994873 = 0.08730031549930573 + 2.0 * 6.056849479675293
Epoch 810, val loss: 0.8089902997016907
Epoch 820, training loss: 12.190170288085938 = 0.08304861932992935 + 2.0 * 6.053560733795166
Epoch 820, val loss: 0.8140208721160889
Epoch 830, training loss: 12.185733795166016 = 0.0791056752204895 + 2.0 * 6.053314208984375
Epoch 830, val loss: 0.8192894458770752
Epoch 840, training loss: 12.177238464355469 = 0.0753985196352005 + 2.0 * 6.050920009613037
Epoch 840, val loss: 0.8247238993644714
Epoch 850, training loss: 12.17629337310791 = 0.07194044440984726 + 2.0 * 6.052176475524902
Epoch 850, val loss: 0.8302390575408936
Epoch 860, training loss: 12.17096996307373 = 0.06870230287313461 + 2.0 * 6.051133632659912
Epoch 860, val loss: 0.8357343673706055
Epoch 870, training loss: 12.162888526916504 = 0.06568186730146408 + 2.0 * 6.048603534698486
Epoch 870, val loss: 0.8414753079414368
Epoch 880, training loss: 12.157715797424316 = 0.06284842640161514 + 2.0 * 6.047433853149414
Epoch 880, val loss: 0.8471527099609375
Epoch 890, training loss: 12.157698631286621 = 0.06018833443522453 + 2.0 * 6.048755168914795
Epoch 890, val loss: 0.8529922962188721
Epoch 900, training loss: 12.149666786193848 = 0.05767606198787689 + 2.0 * 6.045995235443115
Epoch 900, val loss: 0.8587799668312073
Epoch 910, training loss: 12.151779174804688 = 0.05532684549689293 + 2.0 * 6.048226356506348
Epoch 910, val loss: 0.8647058606147766
Epoch 920, training loss: 12.1456298828125 = 0.05311308056116104 + 2.0 * 6.046258449554443
Epoch 920, val loss: 0.8705093860626221
Epoch 930, training loss: 12.138776779174805 = 0.05103154480457306 + 2.0 * 6.043872833251953
Epoch 930, val loss: 0.8764625191688538
Epoch 940, training loss: 12.134857177734375 = 0.049054861068725586 + 2.0 * 6.042901039123535
Epoch 940, val loss: 0.8823330402374268
Epoch 950, training loss: 12.137372970581055 = 0.0471956729888916 + 2.0 * 6.045088768005371
Epoch 950, val loss: 0.8882713317871094
Epoch 960, training loss: 12.13049030303955 = 0.04544055834412575 + 2.0 * 6.042524814605713
Epoch 960, val loss: 0.8941698670387268
Epoch 970, training loss: 12.129767417907715 = 0.043775420635938644 + 2.0 * 6.042995929718018
Epoch 970, val loss: 0.9000194668769836
Epoch 980, training loss: 12.124277114868164 = 0.04220525175333023 + 2.0 * 6.041036128997803
Epoch 980, val loss: 0.9058984518051147
Epoch 990, training loss: 12.134942054748535 = 0.0407230518758297 + 2.0 * 6.047109603881836
Epoch 990, val loss: 0.9117103815078735
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7370
Overall ASR: 0.6790
Flip ASR: 0.6267/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.683124542236328 = 1.9353485107421875 + 2.0 * 8.37388801574707
Epoch 0, val loss: 1.940153956413269
Epoch 10, training loss: 18.671127319335938 = 1.925449252128601 + 2.0 * 8.372838973999023
Epoch 10, val loss: 1.9292713403701782
Epoch 20, training loss: 18.647920608520508 = 1.913343906402588 + 2.0 * 8.367288589477539
Epoch 20, val loss: 1.9151530265808105
Epoch 30, training loss: 18.559600830078125 = 1.89738130569458 + 2.0 * 8.331110000610352
Epoch 30, val loss: 1.896004557609558
Epoch 40, training loss: 18.012664794921875 = 1.8794819116592407 + 2.0 * 8.066591262817383
Epoch 40, val loss: 1.8752321004867554
Epoch 50, training loss: 17.154117584228516 = 1.8624452352523804 + 2.0 * 7.645836353302002
Epoch 50, val loss: 1.855655550956726
Epoch 60, training loss: 16.30113983154297 = 1.8456342220306396 + 2.0 * 7.227753162384033
Epoch 60, val loss: 1.836915135383606
Epoch 70, training loss: 15.704793930053711 = 1.8302881717681885 + 2.0 * 6.937252998352051
Epoch 70, val loss: 1.8201348781585693
Epoch 80, training loss: 15.387289047241211 = 1.8172476291656494 + 2.0 * 6.78502082824707
Epoch 80, val loss: 1.8065379858016968
Epoch 90, training loss: 15.076894760131836 = 1.8051087856292725 + 2.0 * 6.635892868041992
Epoch 90, val loss: 1.794083595275879
Epoch 100, training loss: 14.844846725463867 = 1.7933290004730225 + 2.0 * 6.525758743286133
Epoch 100, val loss: 1.7818398475646973
Epoch 110, training loss: 14.685233116149902 = 1.7807629108428955 + 2.0 * 6.452235221862793
Epoch 110, val loss: 1.7686452865600586
Epoch 120, training loss: 14.572546005249023 = 1.767258644104004 + 2.0 * 6.40264368057251
Epoch 120, val loss: 1.7545557022094727
Epoch 130, training loss: 14.476017951965332 = 1.7525808811187744 + 2.0 * 6.361718654632568
Epoch 130, val loss: 1.739872694015503
Epoch 140, training loss: 14.398017883300781 = 1.7368614673614502 + 2.0 * 6.330578327178955
Epoch 140, val loss: 1.7247294187545776
Epoch 150, training loss: 14.325433731079102 = 1.7197356224060059 + 2.0 * 6.302849292755127
Epoch 150, val loss: 1.7090879678726196
Epoch 160, training loss: 14.260334014892578 = 1.7006911039352417 + 2.0 * 6.279821395874023
Epoch 160, val loss: 1.6921573877334595
Epoch 170, training loss: 14.202021598815918 = 1.6791307926177979 + 2.0 * 6.26144552230835
Epoch 170, val loss: 1.6734551191329956
Epoch 180, training loss: 14.14450454711914 = 1.6545023918151855 + 2.0 * 6.245001316070557
Epoch 180, val loss: 1.652480959892273
Epoch 190, training loss: 14.088229179382324 = 1.6261528730392456 + 2.0 * 6.2310380935668945
Epoch 190, val loss: 1.6285593509674072
Epoch 200, training loss: 14.03191089630127 = 1.5935858488082886 + 2.0 * 6.219162464141846
Epoch 200, val loss: 1.6012169122695923
Epoch 210, training loss: 13.974064826965332 = 1.557050108909607 + 2.0 * 6.208507537841797
Epoch 210, val loss: 1.570631504058838
Epoch 220, training loss: 13.913785934448242 = 1.5162818431854248 + 2.0 * 6.198751926422119
Epoch 220, val loss: 1.5366719961166382
Epoch 230, training loss: 13.859512329101562 = 1.4715754985809326 + 2.0 * 6.193968296051025
Epoch 230, val loss: 1.4998353719711304
Epoch 240, training loss: 13.78842544555664 = 1.4237070083618164 + 2.0 * 6.182359218597412
Epoch 240, val loss: 1.4603763818740845
Epoch 250, training loss: 13.723075866699219 = 1.3731414079666138 + 2.0 * 6.174967288970947
Epoch 250, val loss: 1.4192543029785156
Epoch 260, training loss: 13.659061431884766 = 1.3212711811065674 + 2.0 * 6.168895244598389
Epoch 260, val loss: 1.3776768445968628
Epoch 270, training loss: 13.595077514648438 = 1.2691940069198608 + 2.0 * 6.162941932678223
Epoch 270, val loss: 1.3363845348358154
Epoch 280, training loss: 13.531123161315918 = 1.2173842191696167 + 2.0 * 6.156869411468506
Epoch 280, val loss: 1.296196699142456
Epoch 290, training loss: 13.470556259155273 = 1.1664904356002808 + 2.0 * 6.152032852172852
Epoch 290, val loss: 1.2571943998336792
Epoch 300, training loss: 13.409587860107422 = 1.1162943840026855 + 2.0 * 6.146646499633789
Epoch 300, val loss: 1.2194725275039673
Epoch 310, training loss: 13.366768836975098 = 1.0672615766525269 + 2.0 * 6.149753570556641
Epoch 310, val loss: 1.1830490827560425
Epoch 320, training loss: 13.295218467712402 = 1.0195796489715576 + 2.0 * 6.137819290161133
Epoch 320, val loss: 1.148574709892273
Epoch 330, training loss: 13.247722625732422 = 0.9736758470535278 + 2.0 * 6.137023448944092
Epoch 330, val loss: 1.1158543825149536
Epoch 340, training loss: 13.191407203674316 = 0.9295958280563354 + 2.0 * 6.130905628204346
Epoch 340, val loss: 1.0848103761672974
Epoch 350, training loss: 13.14051342010498 = 0.8872567415237427 + 2.0 * 6.126628398895264
Epoch 350, val loss: 1.0554695129394531
Epoch 360, training loss: 13.099114418029785 = 0.8467060327529907 + 2.0 * 6.126204013824463
Epoch 360, val loss: 1.0279076099395752
Epoch 370, training loss: 13.048069953918457 = 0.8081782460212708 + 2.0 * 6.119946002960205
Epoch 370, val loss: 1.002260446548462
Epoch 380, training loss: 13.024641990661621 = 0.7716876864433289 + 2.0 * 6.126477241516113
Epoch 380, val loss: 0.9785364270210266
Epoch 390, training loss: 12.970076560974121 = 0.7375982403755188 + 2.0 * 6.116239070892334
Epoch 390, val loss: 0.9567726254463196
Epoch 400, training loss: 12.928229331970215 = 0.7055261135101318 + 2.0 * 6.111351490020752
Epoch 400, val loss: 0.936958909034729
Epoch 410, training loss: 12.896279335021973 = 0.6753445863723755 + 2.0 * 6.110467433929443
Epoch 410, val loss: 0.9188033938407898
Epoch 420, training loss: 12.859004020690918 = 0.6469749808311462 + 2.0 * 6.106014728546143
Epoch 420, val loss: 0.902236819267273
Epoch 430, training loss: 12.82604694366455 = 0.6204502582550049 + 2.0 * 6.1027984619140625
Epoch 430, val loss: 0.887309730052948
Epoch 440, training loss: 12.809004783630371 = 0.5955004096031189 + 2.0 * 6.106752395629883
Epoch 440, val loss: 0.8739664554595947
Epoch 450, training loss: 12.77081298828125 = 0.5722101926803589 + 2.0 * 6.099301338195801
Epoch 450, val loss: 0.8618929982185364
Epoch 460, training loss: 12.74380111694336 = 0.5502710342407227 + 2.0 * 6.096765041351318
Epoch 460, val loss: 0.8512067198753357
Epoch 470, training loss: 12.721972465515137 = 0.5295979976654053 + 2.0 * 6.096187114715576
Epoch 470, val loss: 0.8416604995727539
Epoch 480, training loss: 12.697530746459961 = 0.5099480748176575 + 2.0 * 6.093791484832764
Epoch 480, val loss: 0.8330939412117004
Epoch 490, training loss: 12.669686317443848 = 0.49134960770606995 + 2.0 * 6.089168548583984
Epoch 490, val loss: 0.8255246877670288
Epoch 500, training loss: 12.648638725280762 = 0.47354209423065186 + 2.0 * 6.08754825592041
Epoch 500, val loss: 0.8187978863716125
Epoch 510, training loss: 12.64376449584961 = 0.4564270079135895 + 2.0 * 6.0936689376831055
Epoch 510, val loss: 0.8127965927124023
Epoch 520, training loss: 12.609764099121094 = 0.44009798765182495 + 2.0 * 6.084833145141602
Epoch 520, val loss: 0.8074073195457458
Epoch 530, training loss: 12.59049129486084 = 0.4243341386318207 + 2.0 * 6.083078384399414
Epoch 530, val loss: 0.8026363849639893
Epoch 540, training loss: 12.569751739501953 = 0.408969908952713 + 2.0 * 6.080390930175781
Epoch 540, val loss: 0.7984374165534973
Epoch 550, training loss: 12.552992820739746 = 0.39400139451026917 + 2.0 * 6.079495906829834
Epoch 550, val loss: 0.7946467995643616
Epoch 560, training loss: 12.531664848327637 = 0.3793843984603882 + 2.0 * 6.076140403747559
Epoch 560, val loss: 0.7914320230484009
Epoch 570, training loss: 12.529548645019531 = 0.36507323384284973 + 2.0 * 6.082237720489502
Epoch 570, val loss: 0.7887077927589417
Epoch 580, training loss: 12.510787010192871 = 0.3509528934955597 + 2.0 * 6.079916954040527
Epoch 580, val loss: 0.7862766981124878
Epoch 590, training loss: 12.48445987701416 = 0.3373001217842102 + 2.0 * 6.073579788208008
Epoch 590, val loss: 0.7843023538589478
Epoch 600, training loss: 12.465951919555664 = 0.3238776624202728 + 2.0 * 6.071037292480469
Epoch 600, val loss: 0.7828813791275024
Epoch 610, training loss: 12.457192420959473 = 0.3107140362262726 + 2.0 * 6.073239326477051
Epoch 610, val loss: 0.7817428112030029
Epoch 620, training loss: 12.435201644897461 = 0.2977621257305145 + 2.0 * 6.068719863891602
Epoch 620, val loss: 0.7811038494110107
Epoch 630, training loss: 12.419665336608887 = 0.2851674556732178 + 2.0 * 6.067248821258545
Epoch 630, val loss: 0.7808184027671814
Epoch 640, training loss: 12.403528213500977 = 0.27284756302833557 + 2.0 * 6.065340518951416
Epoch 640, val loss: 0.7810032963752747
Epoch 650, training loss: 12.390205383300781 = 0.26087018847465515 + 2.0 * 6.064667701721191
Epoch 650, val loss: 0.7815592288970947
Epoch 660, training loss: 12.400167465209961 = 0.24926218390464783 + 2.0 * 6.07545280456543
Epoch 660, val loss: 0.7825061678886414
Epoch 670, training loss: 12.371593475341797 = 0.23810303211212158 + 2.0 * 6.066745281219482
Epoch 670, val loss: 0.7837958335876465
Epoch 680, training loss: 12.35107421875 = 0.22742784023284912 + 2.0 * 6.06182336807251
Epoch 680, val loss: 0.7854851484298706
Epoch 690, training loss: 12.336616516113281 = 0.21718156337738037 + 2.0 * 6.059717655181885
Epoch 690, val loss: 0.7876281142234802
Epoch 700, training loss: 12.340249061584473 = 0.20738129317760468 + 2.0 * 6.066433906555176
Epoch 700, val loss: 0.7901093363761902
Epoch 710, training loss: 12.319130897521973 = 0.1979541778564453 + 2.0 * 6.060588359832764
Epoch 710, val loss: 0.7927372455596924
Epoch 720, training loss: 12.302058219909668 = 0.18901358544826508 + 2.0 * 6.056522369384766
Epoch 720, val loss: 0.7958694100379944
Epoch 730, training loss: 12.291725158691406 = 0.18049760162830353 + 2.0 * 6.055613994598389
Epoch 730, val loss: 0.7993530631065369
Epoch 740, training loss: 12.299799919128418 = 0.17235566675662994 + 2.0 * 6.063722133636475
Epoch 740, val loss: 0.8031669855117798
Epoch 750, training loss: 12.278034210205078 = 0.1646590232849121 + 2.0 * 6.056687355041504
Epoch 750, val loss: 0.8070806264877319
Epoch 760, training loss: 12.268085479736328 = 0.1573210209608078 + 2.0 * 6.055382251739502
Epoch 760, val loss: 0.8114694952964783
Epoch 770, training loss: 12.254651069641113 = 0.15037648379802704 + 2.0 * 6.05213737487793
Epoch 770, val loss: 0.8159948587417603
Epoch 780, training loss: 12.244799613952637 = 0.1437695473432541 + 2.0 * 6.050515174865723
Epoch 780, val loss: 0.8208110928535461
Epoch 790, training loss: 12.248653411865234 = 0.1374804675579071 + 2.0 * 6.055586338043213
Epoch 790, val loss: 0.8256748914718628
Epoch 800, training loss: 12.248050689697266 = 0.13150614500045776 + 2.0 * 6.058272361755371
Epoch 800, val loss: 0.8309472799301147
Epoch 810, training loss: 12.224693298339844 = 0.1258818507194519 + 2.0 * 6.049405574798584
Epoch 810, val loss: 0.8360623717308044
Epoch 820, training loss: 12.215361595153809 = 0.12053152173757553 + 2.0 * 6.047415256500244
Epoch 820, val loss: 0.8415480256080627
Epoch 830, training loss: 12.206806182861328 = 0.11545421928167343 + 2.0 * 6.045675754547119
Epoch 830, val loss: 0.8471373915672302
Epoch 840, training loss: 12.200331687927246 = 0.11062328517436981 + 2.0 * 6.044854164123535
Epoch 840, val loss: 0.8528768420219421
Epoch 850, training loss: 12.194738388061523 = 0.10602470487356186 + 2.0 * 6.044356822967529
Epoch 850, val loss: 0.858794629573822
Epoch 860, training loss: 12.204224586486816 = 0.10165850073099136 + 2.0 * 6.05128288269043
Epoch 860, val loss: 0.8648582696914673
Epoch 870, training loss: 12.19251823425293 = 0.09751180559396744 + 2.0 * 6.0475029945373535
Epoch 870, val loss: 0.8706910014152527
Epoch 880, training loss: 12.178034782409668 = 0.09360010921955109 + 2.0 * 6.042217254638672
Epoch 880, val loss: 0.8768536448478699
Epoch 890, training loss: 12.171452522277832 = 0.08988185971975327 + 2.0 * 6.040785312652588
Epoch 890, val loss: 0.8830781579017639
Epoch 900, training loss: 12.171955108642578 = 0.08634333312511444 + 2.0 * 6.0428056716918945
Epoch 900, val loss: 0.889316737651825
Epoch 910, training loss: 12.162239074707031 = 0.08298244327306747 + 2.0 * 6.039628505706787
Epoch 910, val loss: 0.8958812952041626
Epoch 920, training loss: 12.161053657531738 = 0.07979563623666763 + 2.0 * 6.040628910064697
Epoch 920, val loss: 0.9019713401794434
Epoch 930, training loss: 12.153667449951172 = 0.0767713114619255 + 2.0 * 6.038447856903076
Epoch 930, val loss: 0.9085729122161865
Epoch 940, training loss: 12.14858627319336 = 0.0738828182220459 + 2.0 * 6.037351608276367
Epoch 940, val loss: 0.9150336980819702
Epoch 950, training loss: 12.167684555053711 = 0.07113979756832123 + 2.0 * 6.048272609710693
Epoch 950, val loss: 0.9216367602348328
Epoch 960, training loss: 12.14120864868164 = 0.0685243234038353 + 2.0 * 6.036342144012451
Epoch 960, val loss: 0.9280940294265747
Epoch 970, training loss: 12.138900756835938 = 0.0660359114408493 + 2.0 * 6.036432266235352
Epoch 970, val loss: 0.9346061944961548
Epoch 980, training loss: 12.132478713989258 = 0.06367235630750656 + 2.0 * 6.034403324127197
Epoch 980, val loss: 0.9412668943405151
Epoch 990, training loss: 12.135704040527344 = 0.06141418591141701 + 2.0 * 6.037145137786865
Epoch 990, val loss: 0.9479143023490906
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7778
Overall ASR: 0.6974
Flip ASR: 0.6356/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.702342987060547 = 1.9544345140457153 + 2.0 * 8.373953819274902
Epoch 0, val loss: 1.9430854320526123
Epoch 10, training loss: 18.691619873046875 = 1.944069266319275 + 2.0 * 8.373775482177734
Epoch 10, val loss: 1.9335381984710693
Epoch 20, training loss: 18.676197052001953 = 1.9314537048339844 + 2.0 * 8.372371673583984
Epoch 20, val loss: 1.921647548675537
Epoch 30, training loss: 18.636075973510742 = 1.9138658046722412 + 2.0 * 8.361104965209961
Epoch 30, val loss: 1.9049201011657715
Epoch 40, training loss: 18.473628997802734 = 1.8900034427642822 + 2.0 * 8.291812896728516
Epoch 40, val loss: 1.8825360536575317
Epoch 50, training loss: 17.746212005615234 = 1.8623442649841309 + 2.0 * 7.941934108734131
Epoch 50, val loss: 1.8568291664123535
Epoch 60, training loss: 17.125537872314453 = 1.8350070714950562 + 2.0 * 7.645265579223633
Epoch 60, val loss: 1.8322794437408447
Epoch 70, training loss: 16.299516677856445 = 1.8152374029159546 + 2.0 * 7.2421393394470215
Epoch 70, val loss: 1.8139947652816772
Epoch 80, training loss: 15.527186393737793 = 1.8001059293746948 + 2.0 * 6.863540172576904
Epoch 80, val loss: 1.7988471984863281
Epoch 90, training loss: 15.225879669189453 = 1.7851285934448242 + 2.0 * 6.7203755378723145
Epoch 90, val loss: 1.7830710411071777
Epoch 100, training loss: 15.03677749633789 = 1.7645264863967896 + 2.0 * 6.636125564575195
Epoch 100, val loss: 1.763818383216858
Epoch 110, training loss: 14.863168716430664 = 1.742783546447754 + 2.0 * 6.560192584991455
Epoch 110, val loss: 1.744377851486206
Epoch 120, training loss: 14.715473175048828 = 1.7217613458633423 + 2.0 * 6.496855735778809
Epoch 120, val loss: 1.7250226736068726
Epoch 130, training loss: 14.589674949645996 = 1.7001419067382812 + 2.0 * 6.444766521453857
Epoch 130, val loss: 1.705040693283081
Epoch 140, training loss: 14.478280067443848 = 1.6768893003463745 + 2.0 * 6.400695323944092
Epoch 140, val loss: 1.6844100952148438
Epoch 150, training loss: 14.391193389892578 = 1.6510158777236938 + 2.0 * 6.370088577270508
Epoch 150, val loss: 1.6624070405960083
Epoch 160, training loss: 14.307538986206055 = 1.6220147609710693 + 2.0 * 6.342761993408203
Epoch 160, val loss: 1.638365387916565
Epoch 170, training loss: 14.229622840881348 = 1.5900133848190308 + 2.0 * 6.319804668426514
Epoch 170, val loss: 1.6121398210525513
Epoch 180, training loss: 14.165903091430664 = 1.5548127889633179 + 2.0 * 6.305545330047607
Epoch 180, val loss: 1.5835193395614624
Epoch 190, training loss: 14.088823318481445 = 1.5166889429092407 + 2.0 * 6.286067008972168
Epoch 190, val loss: 1.5531247854232788
Epoch 200, training loss: 14.01550006866455 = 1.4758909940719604 + 2.0 * 6.26980447769165
Epoch 200, val loss: 1.5209531784057617
Epoch 210, training loss: 13.946852684020996 = 1.432590365409851 + 2.0 * 6.257131099700928
Epoch 210, val loss: 1.4871814250946045
Epoch 220, training loss: 13.886881828308105 = 1.3871532678604126 + 2.0 * 6.249864101409912
Epoch 220, val loss: 1.4524139165878296
Epoch 230, training loss: 13.817230224609375 = 1.3409780263900757 + 2.0 * 6.238126277923584
Epoch 230, val loss: 1.4177186489105225
Epoch 240, training loss: 13.75122356414795 = 1.2946176528930664 + 2.0 * 6.228302955627441
Epoch 240, val loss: 1.3834606409072876
Epoch 250, training loss: 13.687820434570312 = 1.248372197151184 + 2.0 * 6.219724178314209
Epoch 250, val loss: 1.3500200510025024
Epoch 260, training loss: 13.634103775024414 = 1.202880859375 + 2.0 * 6.215611457824707
Epoch 260, val loss: 1.3179757595062256
Epoch 270, training loss: 13.572762489318848 = 1.159000277519226 + 2.0 * 6.206881046295166
Epoch 270, val loss: 1.2878990173339844
Epoch 280, training loss: 13.514298439025879 = 1.1164882183074951 + 2.0 * 6.198904991149902
Epoch 280, val loss: 1.2593904733657837
Epoch 290, training loss: 13.459269523620605 = 1.0749375820159912 + 2.0 * 6.192165851593018
Epoch 290, val loss: 1.231992244720459
Epoch 300, training loss: 13.406224250793457 = 1.0343726873397827 + 2.0 * 6.1859259605407715
Epoch 300, val loss: 1.2057957649230957
Epoch 310, training loss: 13.376022338867188 = 0.9947084784507751 + 2.0 * 6.190657138824463
Epoch 310, val loss: 1.1806070804595947
Epoch 320, training loss: 13.311566352844238 = 0.9566237926483154 + 2.0 * 6.177471160888672
Epoch 320, val loss: 1.156693458557129
Epoch 330, training loss: 13.25888729095459 = 0.919887125492096 + 2.0 * 6.16949987411499
Epoch 330, val loss: 1.133927345275879
Epoch 340, training loss: 13.21220588684082 = 0.8842912912368774 + 2.0 * 6.163957118988037
Epoch 340, val loss: 1.1119788885116577
Epoch 350, training loss: 13.186444282531738 = 0.8498589396476746 + 2.0 * 6.16829252243042
Epoch 350, val loss: 1.0908490419387817
Epoch 360, training loss: 13.130184173583984 = 0.8170390725135803 + 2.0 * 6.156572341918945
Epoch 360, val loss: 1.0706647634506226
Epoch 370, training loss: 13.086814880371094 = 0.7855486273765564 + 2.0 * 6.150633335113525
Epoch 370, val loss: 1.0513396263122559
Epoch 380, training loss: 13.047208786010742 = 0.7550965547561646 + 2.0 * 6.146056175231934
Epoch 380, val loss: 1.0326930284500122
Epoch 390, training loss: 13.015649795532227 = 0.7255520820617676 + 2.0 * 6.14504861831665
Epoch 390, val loss: 1.0145856142044067
Epoch 400, training loss: 12.98039436340332 = 0.6969709992408752 + 2.0 * 6.141711711883545
Epoch 400, val loss: 0.9970921277999878
Epoch 410, training loss: 12.93945026397705 = 0.6692997217178345 + 2.0 * 6.135075092315674
Epoch 410, val loss: 0.9802607893943787
Epoch 420, training loss: 12.906609535217285 = 0.6423003077507019 + 2.0 * 6.13215446472168
Epoch 420, val loss: 0.9639374017715454
Epoch 430, training loss: 12.882901191711426 = 0.6159417629241943 + 2.0 * 6.133479595184326
Epoch 430, val loss: 0.948027491569519
Epoch 440, training loss: 12.846419334411621 = 0.5902796983718872 + 2.0 * 6.128069877624512
Epoch 440, val loss: 0.9326117038726807
Epoch 450, training loss: 12.811732292175293 = 0.5652207136154175 + 2.0 * 6.123255729675293
Epoch 450, val loss: 0.9176616668701172
Epoch 460, training loss: 12.78083610534668 = 0.5407015085220337 + 2.0 * 6.120067119598389
Epoch 460, val loss: 0.9031171798706055
Epoch 470, training loss: 12.76270866394043 = 0.5167820453643799 + 2.0 * 6.1229634284973145
Epoch 470, val loss: 0.8891362547874451
Epoch 480, training loss: 12.727538108825684 = 0.49364060163497925 + 2.0 * 6.11694860458374
Epoch 480, val loss: 0.8756512999534607
Epoch 490, training loss: 12.6998929977417 = 0.47144895792007446 + 2.0 * 6.114222049713135
Epoch 490, val loss: 0.8630809783935547
Epoch 500, training loss: 12.673623085021973 = 0.4499906301498413 + 2.0 * 6.11181640625
Epoch 500, val loss: 0.8511062860488892
Epoch 510, training loss: 12.646651268005371 = 0.42920342087745667 + 2.0 * 6.108724117279053
Epoch 510, val loss: 0.8396952748298645
Epoch 520, training loss: 12.629498481750488 = 0.40908974409103394 + 2.0 * 6.110204219818115
Epoch 520, val loss: 0.8289430737495422
Epoch 530, training loss: 12.610769271850586 = 0.3896918296813965 + 2.0 * 6.110538482666016
Epoch 530, val loss: 0.818802535533905
Epoch 540, training loss: 12.579761505126953 = 0.3710983991622925 + 2.0 * 6.1043314933776855
Epoch 540, val loss: 0.8094766139984131
Epoch 550, training loss: 12.555792808532715 = 0.3531648814678192 + 2.0 * 6.101314067840576
Epoch 550, val loss: 0.800853967666626
Epoch 560, training loss: 12.544970512390137 = 0.3357705771923065 + 2.0 * 6.104599952697754
Epoch 560, val loss: 0.7928230166435242
Epoch 570, training loss: 12.51503849029541 = 0.3189352750778198 + 2.0 * 6.09805154800415
Epoch 570, val loss: 0.7853653430938721
Epoch 580, training loss: 12.4933500289917 = 0.3026277422904968 + 2.0 * 6.095361232757568
Epoch 580, val loss: 0.7787232398986816
Epoch 590, training loss: 12.505226135253906 = 0.28682366013526917 + 2.0 * 6.109201431274414
Epoch 590, val loss: 0.7727158069610596
Epoch 600, training loss: 12.465400695800781 = 0.2717435956001282 + 2.0 * 6.096828460693359
Epoch 600, val loss: 0.7675793766975403
Epoch 610, training loss: 12.439544677734375 = 0.2572098672389984 + 2.0 * 6.091167449951172
Epoch 610, val loss: 0.763202965259552
Epoch 620, training loss: 12.42008113861084 = 0.24320368468761444 + 2.0 * 6.088438510894775
Epoch 620, val loss: 0.7595471143722534
Epoch 630, training loss: 12.403557777404785 = 0.2297503650188446 + 2.0 * 6.0869035720825195
Epoch 630, val loss: 0.7566206455230713
Epoch 640, training loss: 12.408313751220703 = 0.21689291298389435 + 2.0 * 6.095710277557373
Epoch 640, val loss: 0.7544042468070984
Epoch 650, training loss: 12.372700691223145 = 0.20464374125003815 + 2.0 * 6.084028244018555
Epoch 650, val loss: 0.7527756094932556
Epoch 660, training loss: 12.357416152954102 = 0.19305624067783356 + 2.0 * 6.082180023193359
Epoch 660, val loss: 0.7518623471260071
Epoch 670, training loss: 12.342921257019043 = 0.18204720318317413 + 2.0 * 6.080437183380127
Epoch 670, val loss: 0.7515336871147156
Epoch 680, training loss: 12.353281021118164 = 0.17163118720054626 + 2.0 * 6.090825080871582
Epoch 680, val loss: 0.7515803575515747
Epoch 690, training loss: 12.324665069580078 = 0.16189728677272797 + 2.0 * 6.08138370513916
Epoch 690, val loss: 0.7521672248840332
Epoch 700, training loss: 12.30673599243164 = 0.15275631844997406 + 2.0 * 6.076989650726318
Epoch 700, val loss: 0.7532289028167725
Epoch 710, training loss: 12.293598175048828 = 0.1441929042339325 + 2.0 * 6.074702739715576
Epoch 710, val loss: 0.7547304630279541
Epoch 720, training loss: 12.297661781311035 = 0.13615134358406067 + 2.0 * 6.080755233764648
Epoch 720, val loss: 0.7564902305603027
Epoch 730, training loss: 12.27484130859375 = 0.1286676675081253 + 2.0 * 6.073086738586426
Epoch 730, val loss: 0.7585629224777222
Epoch 740, training loss: 12.263848304748535 = 0.12167593091726303 + 2.0 * 6.071086406707764
Epoch 740, val loss: 0.7610026597976685
Epoch 750, training loss: 12.253594398498535 = 0.11513875424861908 + 2.0 * 6.069227695465088
Epoch 750, val loss: 0.7636405825614929
Epoch 760, training loss: 12.287530899047852 = 0.10899736732244492 + 2.0 * 6.089266777038574
Epoch 760, val loss: 0.7663682699203491
Epoch 770, training loss: 12.241222381591797 = 0.10330568253993988 + 2.0 * 6.068958282470703
Epoch 770, val loss: 0.7693670392036438
Epoch 780, training loss: 12.230000495910645 = 0.0979933962225914 + 2.0 * 6.066003322601318
Epoch 780, val loss: 0.7727537751197815
Epoch 790, training loss: 12.222129821777344 = 0.09301821142435074 + 2.0 * 6.064555644989014
Epoch 790, val loss: 0.776156485080719
Epoch 800, training loss: 12.215109825134277 = 0.08834440261125565 + 2.0 * 6.063382625579834
Epoch 800, val loss: 0.779678225517273
Epoch 810, training loss: 12.208346366882324 = 0.08395116031169891 + 2.0 * 6.062197685241699
Epoch 810, val loss: 0.7833483815193176
Epoch 820, training loss: 12.233755111694336 = 0.07982604950666428 + 2.0 * 6.076964378356934
Epoch 820, val loss: 0.7870674133300781
Epoch 830, training loss: 12.198691368103027 = 0.07598347216844559 + 2.0 * 6.061354160308838
Epoch 830, val loss: 0.7907707691192627
Epoch 840, training loss: 12.191466331481934 = 0.07238982617855072 + 2.0 * 6.0595383644104
Epoch 840, val loss: 0.7947775721549988
Epoch 850, training loss: 12.186299324035645 = 0.06902699917554855 + 2.0 * 6.05863618850708
Epoch 850, val loss: 0.7988538146018982
Epoch 860, training loss: 12.180530548095703 = 0.06586164236068726 + 2.0 * 6.0573344230651855
Epoch 860, val loss: 0.8028712272644043
Epoch 870, training loss: 12.215689659118652 = 0.06287555396556854 + 2.0 * 6.076406955718994
Epoch 870, val loss: 0.8068081140518188
Epoch 880, training loss: 12.1746187210083 = 0.06009010225534439 + 2.0 * 6.05726432800293
Epoch 880, val loss: 0.8108806610107422
Epoch 890, training loss: 12.167747497558594 = 0.0574837327003479 + 2.0 * 6.055131912231445
Epoch 890, val loss: 0.815246045589447
Epoch 900, training loss: 12.163406372070312 = 0.05503925681114197 + 2.0 * 6.054183483123779
Epoch 900, val loss: 0.8195372819900513
Epoch 910, training loss: 12.158960342407227 = 0.05273336544632912 + 2.0 * 6.0531134605407715
Epoch 910, val loss: 0.8237470984458923
Epoch 920, training loss: 12.156030654907227 = 0.05055289342999458 + 2.0 * 6.052738666534424
Epoch 920, val loss: 0.8279894590377808
Epoch 930, training loss: 12.158742904663086 = 0.0484953448176384 + 2.0 * 6.055123805999756
Epoch 930, val loss: 0.8321319818496704
Epoch 940, training loss: 12.151516914367676 = 0.04656348004937172 + 2.0 * 6.05247688293457
Epoch 940, val loss: 0.8364568948745728
Epoch 950, training loss: 12.147415161132812 = 0.04475049301981926 + 2.0 * 6.051332473754883
Epoch 950, val loss: 0.8407476544380188
Epoch 960, training loss: 12.146947860717773 = 0.04303432255983353 + 2.0 * 6.051956653594971
Epoch 960, val loss: 0.8450765013694763
Epoch 970, training loss: 12.143071174621582 = 0.04141155630350113 + 2.0 * 6.050829887390137
Epoch 970, val loss: 0.8493909239768982
Epoch 980, training loss: 12.134944915771484 = 0.03987763822078705 + 2.0 * 6.0475335121154785
Epoch 980, val loss: 0.8535985946655273
Epoch 990, training loss: 12.132110595703125 = 0.03842707350850105 + 2.0 * 6.046841621398926
Epoch 990, val loss: 0.8579215407371521
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7704
Overall ASR: 0.9926
Flip ASR: 0.9911/225 nodes
The final ASR:0.78967, 0.14371, Accuracy:0.76173, 0.01772
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11602])
remove edge: torch.Size([2, 9512])
updated graph: torch.Size([2, 10558])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9705
Flip ASR: 0.9644/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9852
Flip ASR: 0.9822/225 nodes
The final ASR:0.97786, 0.00603, Accuracy:0.82840, 0.00630
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.4 der2:0.2 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 18.702524185180664 = 1.954862356185913 + 2.0 * 8.373830795288086
Epoch 0, val loss: 1.9665799140930176
Epoch 10, training loss: 18.691518783569336 = 1.944993257522583 + 2.0 * 8.373262405395508
Epoch 10, val loss: 1.9566247463226318
Epoch 20, training loss: 18.67186164855957 = 1.9323406219482422 + 2.0 * 8.369760513305664
Epoch 20, val loss: 1.94351327419281
Epoch 30, training loss: 18.608753204345703 = 1.9146912097930908 + 2.0 * 8.347030639648438
Epoch 30, val loss: 1.9253178834915161
Epoch 40, training loss: 18.301795959472656 = 1.894095778465271 + 2.0 * 8.203849792480469
Epoch 40, val loss: 1.905186653137207
Epoch 50, training loss: 16.81776237487793 = 1.8730120658874512 + 2.0 * 7.472375392913818
Epoch 50, val loss: 1.884582757949829
Epoch 60, training loss: 15.944839477539062 = 1.8583747148513794 + 2.0 * 7.043232440948486
Epoch 60, val loss: 1.871033787727356
Epoch 70, training loss: 15.40183162689209 = 1.8462282419204712 + 2.0 * 6.777801513671875
Epoch 70, val loss: 1.8591469526290894
Epoch 80, training loss: 15.097966194152832 = 1.8347002267837524 + 2.0 * 6.6316328048706055
Epoch 80, val loss: 1.8478682041168213
Epoch 90, training loss: 14.9000244140625 = 1.824837565422058 + 2.0 * 6.537593364715576
Epoch 90, val loss: 1.837704062461853
Epoch 100, training loss: 14.740964889526367 = 1.8153371810913086 + 2.0 * 6.462813854217529
Epoch 100, val loss: 1.8277382850646973
Epoch 110, training loss: 14.601202964782715 = 1.806545615196228 + 2.0 * 6.397328853607178
Epoch 110, val loss: 1.8185720443725586
Epoch 120, training loss: 14.49360179901123 = 1.798717737197876 + 2.0 * 6.347442150115967
Epoch 120, val loss: 1.8104456663131714
Epoch 130, training loss: 14.413595199584961 = 1.790886640548706 + 2.0 * 6.311354160308838
Epoch 130, val loss: 1.8022897243499756
Epoch 140, training loss: 14.353303909301758 = 1.7824475765228271 + 2.0 * 6.285428047180176
Epoch 140, val loss: 1.7938196659088135
Epoch 150, training loss: 14.301897048950195 = 1.7733219861984253 + 2.0 * 6.26428747177124
Epoch 150, val loss: 1.7850685119628906
Epoch 160, training loss: 14.260334968566895 = 1.7634837627410889 + 2.0 * 6.248425483703613
Epoch 160, val loss: 1.7760376930236816
Epoch 170, training loss: 14.2153959274292 = 1.7527488470077515 + 2.0 * 6.231323719024658
Epoch 170, val loss: 1.7666738033294678
Epoch 180, training loss: 14.17577838897705 = 1.7407572269439697 + 2.0 * 6.21751070022583
Epoch 180, val loss: 1.7564982175827026
Epoch 190, training loss: 14.13846492767334 = 1.7269277572631836 + 2.0 * 6.205768585205078
Epoch 190, val loss: 1.7450367212295532
Epoch 200, training loss: 14.108319282531738 = 1.710854172706604 + 2.0 * 6.198732376098633
Epoch 200, val loss: 1.7317695617675781
Epoch 210, training loss: 14.066035270690918 = 1.692178726196289 + 2.0 * 6.1869282722473145
Epoch 210, val loss: 1.71672785282135
Epoch 220, training loss: 14.025752067565918 = 1.6703191995620728 + 2.0 * 6.177716255187988
Epoch 220, val loss: 1.6991958618164062
Epoch 230, training loss: 13.983686447143555 = 1.6447114944458008 + 2.0 * 6.169487476348877
Epoch 230, val loss: 1.6785507202148438
Epoch 240, training loss: 13.940065383911133 = 1.614502191543579 + 2.0 * 6.162781715393066
Epoch 240, val loss: 1.6542304754257202
Epoch 250, training loss: 13.893350601196289 = 1.5791572332382202 + 2.0 * 6.157096862792969
Epoch 250, val loss: 1.6258083581924438
Epoch 260, training loss: 13.838221549987793 = 1.5387170314788818 + 2.0 * 6.149752140045166
Epoch 260, val loss: 1.5933092832565308
Epoch 270, training loss: 13.7821626663208 = 1.4932085275650024 + 2.0 * 6.144476890563965
Epoch 270, val loss: 1.5567662715911865
Epoch 280, training loss: 13.725800514221191 = 1.443238377571106 + 2.0 * 6.1412811279296875
Epoch 280, val loss: 1.516728401184082
Epoch 290, training loss: 13.660797119140625 = 1.390979290008545 + 2.0 * 6.134909152984619
Epoch 290, val loss: 1.475278615951538
Epoch 300, training loss: 13.598333358764648 = 1.3381991386413574 + 2.0 * 6.130066871643066
Epoch 300, val loss: 1.4336551427841187
Epoch 310, training loss: 13.54294204711914 = 1.286067247390747 + 2.0 * 6.128437519073486
Epoch 310, val loss: 1.3930401802062988
Epoch 320, training loss: 13.480575561523438 = 1.2364578247070312 + 2.0 * 6.122058868408203
Epoch 320, val loss: 1.3548650741577148
Epoch 330, training loss: 13.427175521850586 = 1.189724087715149 + 2.0 * 6.118725776672363
Epoch 330, val loss: 1.3196138143539429
Epoch 340, training loss: 13.374713897705078 = 1.1457023620605469 + 2.0 * 6.114505767822266
Epoch 340, val loss: 1.2868025302886963
Epoch 350, training loss: 13.331867218017578 = 1.1037101745605469 + 2.0 * 6.114078521728516
Epoch 350, val loss: 1.255972146987915
Epoch 360, training loss: 13.279553413391113 = 1.0637580156326294 + 2.0 * 6.107897758483887
Epoch 360, val loss: 1.2267405986785889
Epoch 370, training loss: 13.234492301940918 = 1.0249614715576172 + 2.0 * 6.10476541519165
Epoch 370, val loss: 1.198578953742981
Epoch 380, training loss: 13.192781448364258 = 0.9872887134552002 + 2.0 * 6.102746486663818
Epoch 380, val loss: 1.171340823173523
Epoch 390, training loss: 13.150683403015137 = 0.9510372877120972 + 2.0 * 6.099822998046875
Epoch 390, val loss: 1.1452968120574951
Epoch 400, training loss: 13.108331680297852 = 0.9157683253288269 + 2.0 * 6.0962815284729
Epoch 400, val loss: 1.1199171543121338
Epoch 410, training loss: 13.073275566101074 = 0.8811758756637573 + 2.0 * 6.096049785614014
Epoch 410, val loss: 1.094892978668213
Epoch 420, training loss: 13.037748336791992 = 0.8473549485206604 + 2.0 * 6.095196723937988
Epoch 420, val loss: 1.0707579851150513
Epoch 430, training loss: 12.995010375976562 = 0.8145347833633423 + 2.0 * 6.090237617492676
Epoch 430, val loss: 1.0472575426101685
Epoch 440, training loss: 12.956607818603516 = 0.7822726964950562 + 2.0 * 6.087167739868164
Epoch 440, val loss: 1.0240987539291382
Epoch 450, training loss: 12.925955772399902 = 0.7504860758781433 + 2.0 * 6.087734699249268
Epoch 450, val loss: 1.001357913017273
Epoch 460, training loss: 12.890679359436035 = 0.7193810939788818 + 2.0 * 6.085649013519287
Epoch 460, val loss: 0.9793625473976135
Epoch 470, training loss: 12.858880996704102 = 0.6892052888870239 + 2.0 * 6.084837913513184
Epoch 470, val loss: 0.9581611752510071
Epoch 480, training loss: 12.821349143981934 = 0.660064697265625 + 2.0 * 6.080642223358154
Epoch 480, val loss: 0.9380331039428711
Epoch 490, training loss: 12.78696060180664 = 0.6318774223327637 + 2.0 * 6.077541828155518
Epoch 490, val loss: 0.9190766215324402
Epoch 500, training loss: 12.756019592285156 = 0.604486882686615 + 2.0 * 6.075766563415527
Epoch 500, val loss: 0.9010178446769714
Epoch 510, training loss: 12.729923248291016 = 0.578018069267273 + 2.0 * 6.075952529907227
Epoch 510, val loss: 0.8840428590774536
Epoch 520, training loss: 12.698629379272461 = 0.5529119372367859 + 2.0 * 6.072858810424805
Epoch 520, val loss: 0.8684327006340027
Epoch 530, training loss: 12.671767234802246 = 0.528788685798645 + 2.0 * 6.071489334106445
Epoch 530, val loss: 0.854080855846405
Epoch 540, training loss: 12.649163246154785 = 0.5056751370429993 + 2.0 * 6.071743965148926
Epoch 540, val loss: 0.8406935930252075
Epoch 550, training loss: 12.61788558959961 = 0.4836263656616211 + 2.0 * 6.067129611968994
Epoch 550, val loss: 0.8285582661628723
Epoch 560, training loss: 12.59605884552002 = 0.46250757575035095 + 2.0 * 6.066775798797607
Epoch 560, val loss: 0.8174523115158081
Epoch 570, training loss: 12.576022148132324 = 0.44233253598213196 + 2.0 * 6.066844940185547
Epoch 570, val loss: 0.8073323965072632
Epoch 580, training loss: 12.549159049987793 = 0.4232425391674042 + 2.0 * 6.062958240509033
Epoch 580, val loss: 0.7984399199485779
Epoch 590, training loss: 12.536518096923828 = 0.404987096786499 + 2.0 * 6.065765380859375
Epoch 590, val loss: 0.7903861403465271
Epoch 600, training loss: 12.511085510253906 = 0.38759779930114746 + 2.0 * 6.06174373626709
Epoch 600, val loss: 0.7832449674606323
Epoch 610, training loss: 12.49506950378418 = 0.3709567189216614 + 2.0 * 6.062056541442871
Epoch 610, val loss: 0.7770839333534241
Epoch 620, training loss: 12.472050666809082 = 0.3550145626068115 + 2.0 * 6.058517932891846
Epoch 620, val loss: 0.771723210811615
Epoch 630, training loss: 12.46046257019043 = 0.3397132456302643 + 2.0 * 6.060374736785889
Epoch 630, val loss: 0.767102062702179
Epoch 640, training loss: 12.440970420837402 = 0.32508841156959534 + 2.0 * 6.05794095993042
Epoch 640, val loss: 0.7630667090415955
Epoch 650, training loss: 12.4188232421875 = 0.3110370635986328 + 2.0 * 6.053893089294434
Epoch 650, val loss: 0.7598357796669006
Epoch 660, training loss: 12.405657768249512 = 0.29747891426086426 + 2.0 * 6.054089546203613
Epoch 660, val loss: 0.7571722269058228
Epoch 670, training loss: 12.388487815856934 = 0.28444525599479675 + 2.0 * 6.052021503448486
Epoch 670, val loss: 0.7550225853919983
Epoch 680, training loss: 12.376603126525879 = 0.27195653319358826 + 2.0 * 6.052323341369629
Epoch 680, val loss: 0.7535638809204102
Epoch 690, training loss: 12.37452220916748 = 0.2599218785762787 + 2.0 * 6.057300090789795
Epoch 690, val loss: 0.7525410652160645
Epoch 700, training loss: 12.347289085388184 = 0.24836137890815735 + 2.0 * 6.049463748931885
Epoch 700, val loss: 0.7519944310188293
Epoch 710, training loss: 12.333562850952148 = 0.23725058138370514 + 2.0 * 6.048156261444092
Epoch 710, val loss: 0.7519834637641907
Epoch 720, training loss: 12.3230562210083 = 0.22649304568767548 + 2.0 * 6.048281669616699
Epoch 720, val loss: 0.7523418068885803
Epoch 730, training loss: 12.30955982208252 = 0.2160935252904892 + 2.0 * 6.046733379364014
Epoch 730, val loss: 0.7530046701431274
Epoch 740, training loss: 12.299928665161133 = 0.20607370138168335 + 2.0 * 6.046927452087402
Epoch 740, val loss: 0.7541147470474243
Epoch 750, training loss: 12.286731719970703 = 0.1964220106601715 + 2.0 * 6.045155048370361
Epoch 750, val loss: 0.7555633783340454
Epoch 760, training loss: 12.276080131530762 = 0.18711304664611816 + 2.0 * 6.044483661651611
Epoch 760, val loss: 0.757380485534668
Epoch 770, training loss: 12.268589973449707 = 0.1781337410211563 + 2.0 * 6.045228004455566
Epoch 770, val loss: 0.7595364451408386
Epoch 780, training loss: 12.259944915771484 = 0.16952793300151825 + 2.0 * 6.04520845413208
Epoch 780, val loss: 0.7619428038597107
Epoch 790, training loss: 12.246710777282715 = 0.16130751371383667 + 2.0 * 6.042701721191406
Epoch 790, val loss: 0.7648139595985413
Epoch 800, training loss: 12.238109588623047 = 0.1534222960472107 + 2.0 * 6.042343616485596
Epoch 800, val loss: 0.7678941488265991
Epoch 810, training loss: 12.225900650024414 = 0.14590559899806976 + 2.0 * 6.039997577667236
Epoch 810, val loss: 0.7712819576263428
Epoch 820, training loss: 12.221003532409668 = 0.13870899379253387 + 2.0 * 6.041147232055664
Epoch 820, val loss: 0.7750272154808044
Epoch 830, training loss: 12.209637641906738 = 0.13189293444156647 + 2.0 * 6.038872241973877
Epoch 830, val loss: 0.7790288925170898
Epoch 840, training loss: 12.19985294342041 = 0.1254073530435562 + 2.0 * 6.037222862243652
Epoch 840, val loss: 0.7833228707313538
Epoch 850, training loss: 12.20008659362793 = 0.11926846951246262 + 2.0 * 6.040409088134766
Epoch 850, val loss: 0.7878068685531616
Epoch 860, training loss: 12.192983627319336 = 0.11351624876260757 + 2.0 * 6.03973388671875
Epoch 860, val loss: 0.7923484444618225
Epoch 870, training loss: 12.181407928466797 = 0.1081199124455452 + 2.0 * 6.036643981933594
Epoch 870, val loss: 0.7972162961959839
Epoch 880, training loss: 12.170785903930664 = 0.10302838683128357 + 2.0 * 6.033878803253174
Epoch 880, val loss: 0.8022487163543701
Epoch 890, training loss: 12.164438247680664 = 0.09821046143770218 + 2.0 * 6.033113956451416
Epoch 890, val loss: 0.8074975609779358
Epoch 900, training loss: 12.158123016357422 = 0.09364806860685349 + 2.0 * 6.032237529754639
Epoch 900, val loss: 0.8129582405090332
Epoch 910, training loss: 12.185236930847168 = 0.08933942019939423 + 2.0 * 6.047948837280273
Epoch 910, val loss: 0.8184420466423035
Epoch 920, training loss: 12.148801803588867 = 0.08533912152051926 + 2.0 * 6.031731128692627
Epoch 920, val loss: 0.8239392638206482
Epoch 930, training loss: 12.142985343933105 = 0.08159754425287247 + 2.0 * 6.030694007873535
Epoch 930, val loss: 0.8296621441841125
Epoch 940, training loss: 12.137845993041992 = 0.07806261628866196 + 2.0 * 6.029891490936279
Epoch 940, val loss: 0.8353512287139893
Epoch 950, training loss: 12.140947341918945 = 0.07472429424524307 + 2.0 * 6.033111572265625
Epoch 950, val loss: 0.8412078022956848
Epoch 960, training loss: 12.136380195617676 = 0.07157132029533386 + 2.0 * 6.03240442276001
Epoch 960, val loss: 0.8469005823135376
Epoch 970, training loss: 12.127148628234863 = 0.06863627582788467 + 2.0 * 6.029256343841553
Epoch 970, val loss: 0.8527491688728333
Epoch 980, training loss: 12.120546340942383 = 0.06584435701370239 + 2.0 * 6.027350902557373
Epoch 980, val loss: 0.8585417866706848
Epoch 990, training loss: 12.118990898132324 = 0.06319742649793625 + 2.0 * 6.027896881103516
Epoch 990, val loss: 0.8644033074378967
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.6937
Flip ASR: 0.6356/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.683629989624023 = 1.9359822273254395 + 2.0 * 8.373824119567871
Epoch 0, val loss: 1.9430383443832397
Epoch 10, training loss: 18.67228126525879 = 1.926195740699768 + 2.0 * 8.373043060302734
Epoch 10, val loss: 1.9327281713485718
Epoch 20, training loss: 18.64898109436035 = 1.9137098789215088 + 2.0 * 8.367635726928711
Epoch 20, val loss: 1.9190007448196411
Epoch 30, training loss: 18.550783157348633 = 1.89719820022583 + 2.0 * 8.32679271697998
Epoch 30, val loss: 1.9008077383041382
Epoch 40, training loss: 17.708101272583008 = 1.878838300704956 + 2.0 * 7.9146318435668945
Epoch 40, val loss: 1.8811743259429932
Epoch 50, training loss: 16.353029251098633 = 1.8607525825500488 + 2.0 * 7.246138572692871
Epoch 50, val loss: 1.862485647201538
Epoch 60, training loss: 15.723685264587402 = 1.847524881362915 + 2.0 * 6.938080310821533
Epoch 60, val loss: 1.849257230758667
Epoch 70, training loss: 15.238224029541016 = 1.8373849391937256 + 2.0 * 6.7004194259643555
Epoch 70, val loss: 1.838470458984375
Epoch 80, training loss: 14.943532943725586 = 1.8296637535095215 + 2.0 * 6.556934356689453
Epoch 80, val loss: 1.8297078609466553
Epoch 90, training loss: 14.729521751403809 = 1.8221253156661987 + 2.0 * 6.45369815826416
Epoch 90, val loss: 1.8208802938461304
Epoch 100, training loss: 14.58099365234375 = 1.814816951751709 + 2.0 * 6.383088111877441
Epoch 100, val loss: 1.812252163887024
Epoch 110, training loss: 14.476798057556152 = 1.807247519493103 + 2.0 * 6.334775447845459
Epoch 110, val loss: 1.803167700767517
Epoch 120, training loss: 14.39792537689209 = 1.7994877099990845 + 2.0 * 6.299218654632568
Epoch 120, val loss: 1.7936400175094604
Epoch 130, training loss: 14.334336280822754 = 1.7916624546051025 + 2.0 * 6.271337032318115
Epoch 130, val loss: 1.7841278314590454
Epoch 140, training loss: 14.28366756439209 = 1.783890962600708 + 2.0 * 6.2498884201049805
Epoch 140, val loss: 1.774909257888794
Epoch 150, training loss: 14.236295700073242 = 1.7759127616882324 + 2.0 * 6.230191707611084
Epoch 150, val loss: 1.7659282684326172
Epoch 160, training loss: 14.195703506469727 = 1.7676254510879517 + 2.0 * 6.214038848876953
Epoch 160, val loss: 1.7569811344146729
Epoch 170, training loss: 14.161371231079102 = 1.7588026523590088 + 2.0 * 6.201284408569336
Epoch 170, val loss: 1.7479890584945679
Epoch 180, training loss: 14.133399963378906 = 1.7491741180419922 + 2.0 * 6.192112922668457
Epoch 180, val loss: 1.7387948036193848
Epoch 190, training loss: 14.09753704071045 = 1.738750696182251 + 2.0 * 6.179393291473389
Epoch 190, val loss: 1.729200005531311
Epoch 200, training loss: 14.067779541015625 = 1.727156400680542 + 2.0 * 6.170311450958252
Epoch 200, val loss: 1.7190227508544922
Epoch 210, training loss: 14.037314414978027 = 1.714034080505371 + 2.0 * 6.161640167236328
Epoch 210, val loss: 1.7079589366912842
Epoch 220, training loss: 14.01212215423584 = 1.699083685874939 + 2.0 * 6.156519412994385
Epoch 220, val loss: 1.695753812789917
Epoch 230, training loss: 13.974569320678711 = 1.6820470094680786 + 2.0 * 6.146261215209961
Epoch 230, val loss: 1.6821048259735107
Epoch 240, training loss: 13.94015121459961 = 1.6622395515441895 + 2.0 * 6.138956069946289
Epoch 240, val loss: 1.666437029838562
Epoch 250, training loss: 13.903968811035156 = 1.6388506889343262 + 2.0 * 6.132558822631836
Epoch 250, val loss: 1.648142695426941
Epoch 260, training loss: 13.867534637451172 = 1.611190676689148 + 2.0 * 6.128171920776367
Epoch 260, val loss: 1.626649022102356
Epoch 270, training loss: 13.822208404541016 = 1.579424262046814 + 2.0 * 6.121392250061035
Epoch 270, val loss: 1.6018576622009277
Epoch 280, training loss: 13.7741117477417 = 1.542424201965332 + 2.0 * 6.115843772888184
Epoch 280, val loss: 1.573026180267334
Epoch 290, training loss: 13.722626686096191 = 1.4995757341384888 + 2.0 * 6.111525535583496
Epoch 290, val loss: 1.539667010307312
Epoch 300, training loss: 13.671346664428711 = 1.4509402513504028 + 2.0 * 6.110203266143799
Epoch 300, val loss: 1.5019538402557373
Epoch 310, training loss: 13.607481002807617 = 1.3984954357147217 + 2.0 * 6.104492664337158
Epoch 310, val loss: 1.4613529443740845
Epoch 320, training loss: 13.545059204101562 = 1.3424073457717896 + 2.0 * 6.101325988769531
Epoch 320, val loss: 1.4182099103927612
Epoch 330, training loss: 13.48373031616211 = 1.285516381263733 + 2.0 * 6.099106788635254
Epoch 330, val loss: 1.3747525215148926
Epoch 340, training loss: 13.419462203979492 = 1.229402780532837 + 2.0 * 6.095029830932617
Epoch 340, val loss: 1.3322385549545288
Epoch 350, training loss: 13.360384941101074 = 1.1750110387802124 + 2.0 * 6.092687129974365
Epoch 350, val loss: 1.2911653518676758
Epoch 360, training loss: 13.30989933013916 = 1.1235743761062622 + 2.0 * 6.093162536621094
Epoch 360, val loss: 1.2528464794158936
Epoch 370, training loss: 13.250747680664062 = 1.0759475231170654 + 2.0 * 6.087399959564209
Epoch 370, val loss: 1.217279314994812
Epoch 380, training loss: 13.198890686035156 = 1.0313951969146729 + 2.0 * 6.083747863769531
Epoch 380, val loss: 1.1842254400253296
Epoch 390, training loss: 13.154411315917969 = 0.9895762801170349 + 2.0 * 6.0824174880981445
Epoch 390, val loss: 1.1533246040344238
Epoch 400, training loss: 13.118017196655273 = 0.9506224393844604 + 2.0 * 6.083697319030762
Epoch 400, val loss: 1.124744176864624
Epoch 410, training loss: 13.06794548034668 = 0.9144788980484009 + 2.0 * 6.076733112335205
Epoch 410, val loss: 1.0980843305587769
Epoch 420, training loss: 13.029046058654785 = 0.8801727890968323 + 2.0 * 6.074436664581299
Epoch 420, val loss: 1.0728791952133179
Epoch 430, training loss: 12.994139671325684 = 0.8472177386283875 + 2.0 * 6.073461055755615
Epoch 430, val loss: 1.0487335920333862
Epoch 440, training loss: 12.961716651916504 = 0.8153326511383057 + 2.0 * 6.073192119598389
Epoch 440, val loss: 1.025696873664856
Epoch 450, training loss: 12.921340942382812 = 0.7845322489738464 + 2.0 * 6.068404197692871
Epoch 450, val loss: 1.0034977197647095
Epoch 460, training loss: 12.891194343566895 = 0.7543439865112305 + 2.0 * 6.068425178527832
Epoch 460, val loss: 0.9818693995475769
Epoch 470, training loss: 12.861295700073242 = 0.7247568368911743 + 2.0 * 6.0682692527771
Epoch 470, val loss: 0.9607720375061035
Epoch 480, training loss: 12.823363304138184 = 0.6958451867103577 + 2.0 * 6.063758850097656
Epoch 480, val loss: 0.9403276443481445
Epoch 490, training loss: 12.791474342346191 = 0.6673780679702759 + 2.0 * 6.062047958374023
Epoch 490, val loss: 0.920440673828125
Epoch 500, training loss: 12.76233959197998 = 0.6392422914505005 + 2.0 * 6.061548709869385
Epoch 500, val loss: 0.9009421467781067
Epoch 510, training loss: 12.745615005493164 = 0.6114882230758667 + 2.0 * 6.067063331604004
Epoch 510, val loss: 0.8818717002868652
Epoch 520, training loss: 12.699934005737305 = 0.5843294262886047 + 2.0 * 6.057802200317383
Epoch 520, val loss: 0.8635008931159973
Epoch 530, training loss: 12.66958236694336 = 0.5576429963111877 + 2.0 * 6.055969715118408
Epoch 530, val loss: 0.8457006812095642
Epoch 540, training loss: 12.65226936340332 = 0.5312954187393188 + 2.0 * 6.060486793518066
Epoch 540, val loss: 0.8284947872161865
Epoch 550, training loss: 12.615104675292969 = 0.5056021809577942 + 2.0 * 6.054751396179199
Epoch 550, val loss: 0.8120077252388
Epoch 560, training loss: 12.587862968444824 = 0.48043784499168396 + 2.0 * 6.053712368011475
Epoch 560, val loss: 0.7962947487831116
Epoch 570, training loss: 12.56540298461914 = 0.4558446407318115 + 2.0 * 6.054779052734375
Epoch 570, val loss: 0.7814033031463623
Epoch 580, training loss: 12.535279273986816 = 0.43194836378097534 + 2.0 * 6.051665306091309
Epoch 580, val loss: 0.7674900889396667
Epoch 590, training loss: 12.508097648620605 = 0.4088250398635864 + 2.0 * 6.049636363983154
Epoch 590, val loss: 0.7546523809432983
Epoch 600, training loss: 12.488134384155273 = 0.3865056037902832 + 2.0 * 6.050814628601074
Epoch 600, val loss: 0.7428311109542847
Epoch 610, training loss: 12.467781066894531 = 0.3651547431945801 + 2.0 * 6.0513129234313965
Epoch 610, val loss: 0.7320096492767334
Epoch 620, training loss: 12.441282272338867 = 0.3447619676589966 + 2.0 * 6.04826021194458
Epoch 620, val loss: 0.7224577069282532
Epoch 630, training loss: 12.416812896728516 = 0.3254227936267853 + 2.0 * 6.045694828033447
Epoch 630, val loss: 0.7139517068862915
Epoch 640, training loss: 12.396821975708008 = 0.3070451319217682 + 2.0 * 6.044888496398926
Epoch 640, val loss: 0.7066252827644348
Epoch 650, training loss: 12.375299453735352 = 0.2895905077457428 + 2.0 * 6.042854309082031
Epoch 650, val loss: 0.70033860206604
Epoch 660, training loss: 12.369781494140625 = 0.273105263710022 + 2.0 * 6.048337936401367
Epoch 660, val loss: 0.6950513124465942
Epoch 670, training loss: 12.345365524291992 = 0.25775840878486633 + 2.0 * 6.043803691864014
Epoch 670, val loss: 0.6907211542129517
Epoch 680, training loss: 12.324972152709961 = 0.24338649213314056 + 2.0 * 6.040792942047119
Epoch 680, val loss: 0.6873113512992859
Epoch 690, training loss: 12.322674751281738 = 0.22990117967128754 + 2.0 * 6.04638671875
Epoch 690, val loss: 0.684675395488739
Epoch 700, training loss: 12.295053482055664 = 0.21728792786598206 + 2.0 * 6.038882732391357
Epoch 700, val loss: 0.6827322244644165
Epoch 710, training loss: 12.282079696655273 = 0.2055392563343048 + 2.0 * 6.038269996643066
Epoch 710, val loss: 0.6815089583396912
Epoch 720, training loss: 12.268380165100098 = 0.1945696473121643 + 2.0 * 6.036905288696289
Epoch 720, val loss: 0.6809196472167969
Epoch 730, training loss: 12.256916999816895 = 0.1843482404947281 + 2.0 * 6.036284446716309
Epoch 730, val loss: 0.6808749437332153
Epoch 740, training loss: 12.258028984069824 = 0.17480744421482086 + 2.0 * 6.0416107177734375
Epoch 740, val loss: 0.6813461780548096
Epoch 750, training loss: 12.234463691711426 = 0.1659385859966278 + 2.0 * 6.034262657165527
Epoch 750, val loss: 0.6822279095649719
Epoch 760, training loss: 12.223752975463867 = 0.15765829384326935 + 2.0 * 6.033047199249268
Epoch 760, val loss: 0.6835582852363586
Epoch 770, training loss: 12.21692943572998 = 0.14988136291503906 + 2.0 * 6.033524036407471
Epoch 770, val loss: 0.6852965354919434
Epoch 780, training loss: 12.204975128173828 = 0.14259999990463257 + 2.0 * 6.031187534332275
Epoch 780, val loss: 0.6872825622558594
Epoch 790, training loss: 12.19924259185791 = 0.13578049838542938 + 2.0 * 6.031731128692627
Epoch 790, val loss: 0.6895721554756165
Epoch 800, training loss: 12.188200950622559 = 0.1293697953224182 + 2.0 * 6.029415607452393
Epoch 800, val loss: 0.6921694278717041
Epoch 810, training loss: 12.182455062866211 = 0.1233396977186203 + 2.0 * 6.029557704925537
Epoch 810, val loss: 0.6950035691261292
Epoch 820, training loss: 12.183743476867676 = 0.11767315119504929 + 2.0 * 6.0330352783203125
Epoch 820, val loss: 0.6979916095733643
Epoch 830, training loss: 12.16731071472168 = 0.11235309392213821 + 2.0 * 6.0274786949157715
Epoch 830, val loss: 0.701198399066925
Epoch 840, training loss: 12.164253234863281 = 0.1073521301150322 + 2.0 * 6.0284504890441895
Epoch 840, val loss: 0.7045800089836121
Epoch 850, training loss: 12.15841007232666 = 0.10264773666858673 + 2.0 * 6.027881145477295
Epoch 850, val loss: 0.7080965042114258
Epoch 860, training loss: 12.15108585357666 = 0.09823412448167801 + 2.0 * 6.026425838470459
Epoch 860, val loss: 0.7116987109184265
Epoch 870, training loss: 12.143006324768066 = 0.09404423832893372 + 2.0 * 6.024480819702148
Epoch 870, val loss: 0.715429961681366
Epoch 880, training loss: 12.142260551452637 = 0.0900837853550911 + 2.0 * 6.026088237762451
Epoch 880, val loss: 0.7193053364753723
Epoch 890, training loss: 12.13881778717041 = 0.0863579586148262 + 2.0 * 6.0262298583984375
Epoch 890, val loss: 0.7231151461601257
Epoch 900, training loss: 12.131447792053223 = 0.0828653946518898 + 2.0 * 6.024291038513184
Epoch 900, val loss: 0.7270110249519348
Epoch 910, training loss: 12.132039070129395 = 0.079570472240448 + 2.0 * 6.026234149932861
Epoch 910, val loss: 0.7309792637825012
Epoch 920, training loss: 12.119389533996582 = 0.07645051181316376 + 2.0 * 6.021469593048096
Epoch 920, val loss: 0.7350178956985474
Epoch 930, training loss: 12.115468978881836 = 0.07351114600896835 + 2.0 * 6.020978927612305
Epoch 930, val loss: 0.7390272617340088
Epoch 940, training loss: 12.120529174804688 = 0.0707203671336174 + 2.0 * 6.024904251098633
Epoch 940, val loss: 0.7431191802024841
Epoch 950, training loss: 12.10895824432373 = 0.06808973103761673 + 2.0 * 6.020434379577637
Epoch 950, val loss: 0.7471625804901123
Epoch 960, training loss: 12.104004859924316 = 0.06558843702077866 + 2.0 * 6.0192084312438965
Epoch 960, val loss: 0.751312792301178
Epoch 970, training loss: 12.10981559753418 = 0.06320744007825851 + 2.0 * 6.023303985595703
Epoch 970, val loss: 0.7554866671562195
Epoch 980, training loss: 12.099870681762695 = 0.060967497527599335 + 2.0 * 6.01945161819458
Epoch 980, val loss: 0.7595917582511902
Epoch 990, training loss: 12.094632148742676 = 0.05884906277060509 + 2.0 * 6.0178914070129395
Epoch 990, val loss: 0.7637204527854919
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.6384
Flip ASR: 0.5778/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.70608901977539 = 1.958483338356018 + 2.0 * 8.37380313873291
Epoch 0, val loss: 1.9543664455413818
Epoch 10, training loss: 18.693649291992188 = 1.9474992752075195 + 2.0 * 8.373075485229492
Epoch 10, val loss: 1.943345308303833
Epoch 20, training loss: 18.66979217529297 = 1.9338815212249756 + 2.0 * 8.367955207824707
Epoch 20, val loss: 1.9296703338623047
Epoch 30, training loss: 18.573680877685547 = 1.9156898260116577 + 2.0 * 8.328995704650879
Epoch 30, val loss: 1.9116276502609253
Epoch 40, training loss: 17.962339401245117 = 1.8954284191131592 + 2.0 * 8.033455848693848
Epoch 40, val loss: 1.8920433521270752
Epoch 50, training loss: 16.430713653564453 = 1.8752816915512085 + 2.0 * 7.277715682983398
Epoch 50, val loss: 1.8732614517211914
Epoch 60, training loss: 15.526022911071777 = 1.8624545335769653 + 2.0 * 6.831784248352051
Epoch 60, val loss: 1.8607275485992432
Epoch 70, training loss: 15.112109184265137 = 1.8512494564056396 + 2.0 * 6.630429744720459
Epoch 70, val loss: 1.8499170541763306
Epoch 80, training loss: 14.879556655883789 = 1.8407056331634521 + 2.0 * 6.519425392150879
Epoch 80, val loss: 1.839969277381897
Epoch 90, training loss: 14.694672584533691 = 1.830122947692871 + 2.0 * 6.43227481842041
Epoch 90, val loss: 1.830357313156128
Epoch 100, training loss: 14.55543041229248 = 1.8209377527236938 + 2.0 * 6.367246150970459
Epoch 100, val loss: 1.8221371173858643
Epoch 110, training loss: 14.451360702514648 = 1.813138484954834 + 2.0 * 6.319110870361328
Epoch 110, val loss: 1.8149467706680298
Epoch 120, training loss: 14.371739387512207 = 1.8060839176177979 + 2.0 * 6.282827854156494
Epoch 120, val loss: 1.8082754611968994
Epoch 130, training loss: 14.307918548583984 = 1.7992665767669678 + 2.0 * 6.254325866699219
Epoch 130, val loss: 1.8018172979354858
Epoch 140, training loss: 14.256664276123047 = 1.7924424409866333 + 2.0 * 6.232110977172852
Epoch 140, val loss: 1.795418381690979
Epoch 150, training loss: 14.212493896484375 = 1.7853955030441284 + 2.0 * 6.2135491371154785
Epoch 150, val loss: 1.7889378070831299
Epoch 160, training loss: 14.171659469604492 = 1.777932047843933 + 2.0 * 6.196863651275635
Epoch 160, val loss: 1.7821921110153198
Epoch 170, training loss: 14.136558532714844 = 1.7697798013687134 + 2.0 * 6.183389186859131
Epoch 170, val loss: 1.7750393152236938
Epoch 180, training loss: 14.103333473205566 = 1.7607883214950562 + 2.0 * 6.1712727546691895
Epoch 180, val loss: 1.7673600912094116
Epoch 190, training loss: 14.073914527893066 = 1.750676155090332 + 2.0 * 6.161619186401367
Epoch 190, val loss: 1.7589848041534424
Epoch 200, training loss: 14.044342041015625 = 1.739305019378662 + 2.0 * 6.152518272399902
Epoch 200, val loss: 1.7497068643569946
Epoch 210, training loss: 14.014359474182129 = 1.726367712020874 + 2.0 * 6.143995761871338
Epoch 210, val loss: 1.7393137216567993
Epoch 220, training loss: 13.984474182128906 = 1.7114877700805664 + 2.0 * 6.13649320602417
Epoch 220, val loss: 1.7274317741394043
Epoch 230, training loss: 13.961586952209473 = 1.6942321062088013 + 2.0 * 6.1336774826049805
Epoch 230, val loss: 1.7136906385421753
Epoch 240, training loss: 13.9258451461792 = 1.6744271516799927 + 2.0 * 6.125709056854248
Epoch 240, val loss: 1.6979212760925293
Epoch 250, training loss: 13.891292572021484 = 1.6515264511108398 + 2.0 * 6.119883060455322
Epoch 250, val loss: 1.6796514987945557
Epoch 260, training loss: 13.854874610900879 = 1.624943494796753 + 2.0 * 6.114965438842773
Epoch 260, val loss: 1.6583399772644043
Epoch 270, training loss: 13.819047927856445 = 1.5941643714904785 + 2.0 * 6.1124420166015625
Epoch 270, val loss: 1.6334377527236938
Epoch 280, training loss: 13.776068687438965 = 1.5592520236968994 + 2.0 * 6.108408451080322
Epoch 280, val loss: 1.605157732963562
Epoch 290, training loss: 13.728459358215332 = 1.5204801559448242 + 2.0 * 6.103989601135254
Epoch 290, val loss: 1.5736427307128906
Epoch 300, training loss: 13.681756973266602 = 1.4781107902526855 + 2.0 * 6.101822853088379
Epoch 300, val loss: 1.5391700267791748
Epoch 310, training loss: 13.634044647216797 = 1.4334282875061035 + 2.0 * 6.100308418273926
Epoch 310, val loss: 1.5027357339859009
Epoch 320, training loss: 13.579391479492188 = 1.3875025510787964 + 2.0 * 6.095944404602051
Epoch 320, val loss: 1.4655489921569824
Epoch 330, training loss: 13.527212142944336 = 1.3411955833435059 + 2.0 * 6.093008041381836
Epoch 330, val loss: 1.4282444715499878
Epoch 340, training loss: 13.47957992553711 = 1.2954089641571045 + 2.0 * 6.092085361480713
Epoch 340, val loss: 1.3917008638381958
Epoch 350, training loss: 13.430173873901367 = 1.250892162322998 + 2.0 * 6.0896406173706055
Epoch 350, val loss: 1.3566296100616455
Epoch 360, training loss: 13.378751754760742 = 1.207383632659912 + 2.0 * 6.085684299468994
Epoch 360, val loss: 1.3227087259292603
Epoch 370, training loss: 13.339824676513672 = 1.1644949913024902 + 2.0 * 6.08766508102417
Epoch 370, val loss: 1.2896277904510498
Epoch 380, training loss: 13.289972305297852 = 1.1223840713500977 + 2.0 * 6.083794116973877
Epoch 380, val loss: 1.257400631904602
Epoch 390, training loss: 13.239837646484375 = 1.080639123916626 + 2.0 * 6.079599380493164
Epoch 390, val loss: 1.2256672382354736
Epoch 400, training loss: 13.193410873413086 = 1.0384794473648071 + 2.0 * 6.077465534210205
Epoch 400, val loss: 1.193751573562622
Epoch 410, training loss: 13.150504112243652 = 0.9961354732513428 + 2.0 * 6.077184200286865
Epoch 410, val loss: 1.161893606185913
Epoch 420, training loss: 13.10360050201416 = 0.9538729786872864 + 2.0 * 6.074863910675049
Epoch 420, val loss: 1.1301355361938477
Epoch 430, training loss: 13.0557861328125 = 0.9113945364952087 + 2.0 * 6.072196006774902
Epoch 430, val loss: 1.0982011556625366
Epoch 440, training loss: 13.022795677185059 = 0.8691037893295288 + 2.0 * 6.076846122741699
Epoch 440, val loss: 1.0663796663284302
Epoch 450, training loss: 12.969842910766602 = 0.8282971382141113 + 2.0 * 6.070773124694824
Epoch 450, val loss: 1.035674810409546
Epoch 460, training loss: 12.92440414428711 = 0.7887712121009827 + 2.0 * 6.067816257476807
Epoch 460, val loss: 1.0060211420059204
Epoch 470, training loss: 12.882644653320312 = 0.7505460977554321 + 2.0 * 6.066049098968506
Epoch 470, val loss: 0.9774938225746155
Epoch 480, training loss: 12.843453407287598 = 0.7139900922775269 + 2.0 * 6.064731597900391
Epoch 480, val loss: 0.9506077766418457
Epoch 490, training loss: 12.809712409973145 = 0.6794263124465942 + 2.0 * 6.06514310836792
Epoch 490, val loss: 0.9253286719322205
Epoch 500, training loss: 12.771822929382324 = 0.6466391682624817 + 2.0 * 6.062592029571533
Epoch 500, val loss: 0.9017598032951355
Epoch 510, training loss: 12.736510276794434 = 0.6155616641044617 + 2.0 * 6.060474395751953
Epoch 510, val loss: 0.8798215389251709
Epoch 520, training loss: 12.7191801071167 = 0.5858209133148193 + 2.0 * 6.06667947769165
Epoch 520, val loss: 0.8593243956565857
Epoch 530, training loss: 12.673511505126953 = 0.5576429963111877 + 2.0 * 6.057934284210205
Epoch 530, val loss: 0.8403815627098083
Epoch 540, training loss: 12.642143249511719 = 0.530650794506073 + 2.0 * 6.055746078491211
Epoch 540, val loss: 0.8226308226585388
Epoch 550, training loss: 12.61880111694336 = 0.5046413540840149 + 2.0 * 6.057079792022705
Epoch 550, val loss: 0.8058766722679138
Epoch 560, training loss: 12.587532043457031 = 0.4796309769153595 + 2.0 * 6.053950309753418
Epoch 560, val loss: 0.7903236746788025
Epoch 570, training loss: 12.561151504516602 = 0.45552361011505127 + 2.0 * 6.05281400680542
Epoch 570, val loss: 0.775709331035614
Epoch 580, training loss: 12.53676986694336 = 0.4322790205478668 + 2.0 * 6.052245616912842
Epoch 580, val loss: 0.7620240449905396
Epoch 590, training loss: 12.519268035888672 = 0.4097934663295746 + 2.0 * 6.054737091064453
Epoch 590, val loss: 0.7491273283958435
Epoch 600, training loss: 12.485538482666016 = 0.3882567584514618 + 2.0 * 6.048640727996826
Epoch 600, val loss: 0.7371537685394287
Epoch 610, training loss: 12.462762832641602 = 0.36749908328056335 + 2.0 * 6.047631740570068
Epoch 610, val loss: 0.7259014844894409
Epoch 620, training loss: 12.447907447814941 = 0.34753668308258057 + 2.0 * 6.050185203552246
Epoch 620, val loss: 0.7153704166412354
Epoch 630, training loss: 12.424631118774414 = 0.328582763671875 + 2.0 * 6.0480241775512695
Epoch 630, val loss: 0.7056941390037537
Epoch 640, training loss: 12.400460243225098 = 0.3106297552585602 + 2.0 * 6.044915199279785
Epoch 640, val loss: 0.6969389319419861
Epoch 650, training loss: 12.378501892089844 = 0.2934865653514862 + 2.0 * 6.042507648468018
Epoch 650, val loss: 0.688850462436676
Epoch 660, training loss: 12.361810684204102 = 0.277121365070343 + 2.0 * 6.042344570159912
Epoch 660, val loss: 0.6814327239990234
Epoch 670, training loss: 12.351350784301758 = 0.2616308331489563 + 2.0 * 6.044859886169434
Epoch 670, val loss: 0.6747897863388062
Epoch 680, training loss: 12.327984809875488 = 0.24721744656562805 + 2.0 * 6.040383815765381
Epoch 680, val loss: 0.6689409613609314
Epoch 690, training loss: 12.311857223510742 = 0.23360569775104523 + 2.0 * 6.039125919342041
Epoch 690, val loss: 0.6638271808624268
Epoch 700, training loss: 12.296529769897461 = 0.22072693705558777 + 2.0 * 6.037901401519775
Epoch 700, val loss: 0.6593428254127502
Epoch 710, training loss: 12.3031587600708 = 0.20854716002941132 + 2.0 * 6.047305583953857
Epoch 710, val loss: 0.6554805040359497
Epoch 720, training loss: 12.269827842712402 = 0.19721609354019165 + 2.0 * 6.036305904388428
Epoch 720, val loss: 0.6522897481918335
Epoch 730, training loss: 12.257725715637207 = 0.18654416501522064 + 2.0 * 6.035590648651123
Epoch 730, val loss: 0.6496927738189697
Epoch 740, training loss: 12.269338607788086 = 0.1765056699514389 + 2.0 * 6.046416282653809
Epoch 740, val loss: 0.6475368142127991
Epoch 750, training loss: 12.235703468322754 = 0.16714663803577423 + 2.0 * 6.034278392791748
Epoch 750, val loss: 0.6458751559257507
Epoch 760, training loss: 12.223278999328613 = 0.1583646982908249 + 2.0 * 6.03245735168457
Epoch 760, val loss: 0.6447319984436035
Epoch 770, training loss: 12.215054512023926 = 0.15010260045528412 + 2.0 * 6.03247594833374
Epoch 770, val loss: 0.644014835357666
Epoch 780, training loss: 12.211936950683594 = 0.1423637419939041 + 2.0 * 6.034786701202393
Epoch 780, val loss: 0.6437035799026489
Epoch 790, training loss: 12.198780059814453 = 0.1351320892572403 + 2.0 * 6.031824111938477
Epoch 790, val loss: 0.6439078450202942
Epoch 800, training loss: 12.189824104309082 = 0.12836295366287231 + 2.0 * 6.030730724334717
Epoch 800, val loss: 0.6443929672241211
Epoch 810, training loss: 12.181164741516113 = 0.12202330678701401 + 2.0 * 6.029570579528809
Epoch 810, val loss: 0.6452693939208984
Epoch 820, training loss: 12.172926902770996 = 0.1160537526011467 + 2.0 * 6.028436660766602
Epoch 820, val loss: 0.6464559435844421
Epoch 830, training loss: 12.173495292663574 = 0.11045660823583603 + 2.0 * 6.031519412994385
Epoch 830, val loss: 0.6479222774505615
Epoch 840, training loss: 12.157639503479004 = 0.1052236557006836 + 2.0 * 6.02620792388916
Epoch 840, val loss: 0.6496837735176086
Epoch 850, training loss: 12.155058860778809 = 0.10030875355005264 + 2.0 * 6.027375221252441
Epoch 850, val loss: 0.6516995429992676
Epoch 860, training loss: 12.149637222290039 = 0.09571308642625809 + 2.0 * 6.0269622802734375
Epoch 860, val loss: 0.6539456248283386
Epoch 870, training loss: 12.140091896057129 = 0.09141046553850174 + 2.0 * 6.024340629577637
Epoch 870, val loss: 0.6563599109649658
Epoch 880, training loss: 12.137816429138184 = 0.08736734837293625 + 2.0 * 6.025224685668945
Epoch 880, val loss: 0.658997654914856
Epoch 890, training loss: 12.138580322265625 = 0.0835723876953125 + 2.0 * 6.027503967285156
Epoch 890, val loss: 0.6617150902748108
Epoch 900, training loss: 12.12879753112793 = 0.07998891174793243 + 2.0 * 6.024404525756836
Epoch 900, val loss: 0.6645228862762451
Epoch 910, training loss: 12.122248649597168 = 0.07663463056087494 + 2.0 * 6.0228071212768555
Epoch 910, val loss: 0.6675249338150024
Epoch 920, training loss: 12.116803169250488 = 0.07346078008413315 + 2.0 * 6.021671295166016
Epoch 920, val loss: 0.6705763936042786
Epoch 930, training loss: 12.12585163116455 = 0.0704689621925354 + 2.0 * 6.02769136428833
Epoch 930, val loss: 0.6737516522407532
Epoch 940, training loss: 12.116534233093262 = 0.0676455944776535 + 2.0 * 6.024444103240967
Epoch 940, val loss: 0.6769535541534424
Epoch 950, training loss: 12.105234146118164 = 0.06501501053571701 + 2.0 * 6.0201096534729
Epoch 950, val loss: 0.68025803565979
Epoch 960, training loss: 12.100335121154785 = 0.06252521276473999 + 2.0 * 6.018905162811279
Epoch 960, val loss: 0.6835928559303284
Epoch 970, training loss: 12.108308792114258 = 0.06016242131590843 + 2.0 * 6.024073123931885
Epoch 970, val loss: 0.6870108842849731
Epoch 980, training loss: 12.099410057067871 = 0.057930510491132736 + 2.0 * 6.020739555358887
Epoch 980, val loss: 0.6902918815612793
Epoch 990, training loss: 12.092257499694824 = 0.055828362703323364 + 2.0 * 6.018214702606201
Epoch 990, val loss: 0.6938121318817139
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.8376
Flip ASR: 0.8044/225 nodes
The final ASR:0.72325, 0.08398, Accuracy:0.82840, 0.00924
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11602])
remove edge: torch.Size([2, 9508])
updated graph: torch.Size([2, 10554])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9852
Flip ASR: 0.9822/225 nodes
The final ASR:0.98032, 0.00460, Accuracy:0.83210, 0.00349
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.4 der2:0.2 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 18.689146041870117 = 1.941685438156128 + 2.0 * 8.373730659484863
Epoch 0, val loss: 1.9412256479263306
Epoch 10, training loss: 18.676530838012695 = 1.9311275482177734 + 2.0 * 8.372701644897461
Epoch 10, val loss: 1.9313541650772095
Epoch 20, training loss: 18.64969253540039 = 1.9178435802459717 + 2.0 * 8.365924835205078
Epoch 20, val loss: 1.9185596704483032
Epoch 30, training loss: 18.539058685302734 = 1.8999128341674805 + 2.0 * 8.319573402404785
Epoch 30, val loss: 1.901228427886963
Epoch 40, training loss: 17.89156723022461 = 1.8797283172607422 + 2.0 * 8.005919456481934
Epoch 40, val loss: 1.8819663524627686
Epoch 50, training loss: 16.32419776916504 = 1.8574864864349365 + 2.0 * 7.233355522155762
Epoch 50, val loss: 1.8609309196472168
Epoch 60, training loss: 15.736645698547363 = 1.8434144258499146 + 2.0 * 6.946615695953369
Epoch 60, val loss: 1.8478938341140747
Epoch 70, training loss: 15.259669303894043 = 1.8334105014801025 + 2.0 * 6.71312952041626
Epoch 70, val loss: 1.8381435871124268
Epoch 80, training loss: 14.963808059692383 = 1.8234726190567017 + 2.0 * 6.570167541503906
Epoch 80, val loss: 1.8291009664535522
Epoch 90, training loss: 14.780596733093262 = 1.81514573097229 + 2.0 * 6.482725620269775
Epoch 90, val loss: 1.8212825059890747
Epoch 100, training loss: 14.615968704223633 = 1.8073923587799072 + 2.0 * 6.404288291931152
Epoch 100, val loss: 1.8142515420913696
Epoch 110, training loss: 14.49452018737793 = 1.8008065223693848 + 2.0 * 6.346857070922852
Epoch 110, val loss: 1.8081705570220947
Epoch 120, training loss: 14.41335391998291 = 1.794393539428711 + 2.0 * 6.3094801902771
Epoch 120, val loss: 1.8018196821212769
Epoch 130, training loss: 14.346199989318848 = 1.7874600887298584 + 2.0 * 6.279369831085205
Epoch 130, val loss: 1.794974684715271
Epoch 140, training loss: 14.287384986877441 = 1.7800700664520264 + 2.0 * 6.253657341003418
Epoch 140, val loss: 1.7878843545913696
Epoch 150, training loss: 14.237754821777344 = 1.7722185850143433 + 2.0 * 6.2327680587768555
Epoch 150, val loss: 1.7807244062423706
Epoch 160, training loss: 14.195170402526855 = 1.7634319067001343 + 2.0 * 6.215869426727295
Epoch 160, val loss: 1.7730939388275146
Epoch 170, training loss: 14.15325927734375 = 1.7536349296569824 + 2.0 * 6.199812412261963
Epoch 170, val loss: 1.764830231666565
Epoch 180, training loss: 14.112751960754395 = 1.7425200939178467 + 2.0 * 6.185115814208984
Epoch 180, val loss: 1.7556596994400024
Epoch 190, training loss: 14.075286865234375 = 1.729799509048462 + 2.0 * 6.172743797302246
Epoch 190, val loss: 1.7453104257583618
Epoch 200, training loss: 14.043109893798828 = 1.7150840759277344 + 2.0 * 6.164012908935547
Epoch 200, val loss: 1.7333710193634033
Epoch 210, training loss: 14.006054878234863 = 1.69809889793396 + 2.0 * 6.153977870941162
Epoch 210, val loss: 1.7198102474212646
Epoch 220, training loss: 13.966833114624023 = 1.6785639524459839 + 2.0 * 6.144134521484375
Epoch 220, val loss: 1.704171061515808
Epoch 230, training loss: 13.929713249206543 = 1.6557711362838745 + 2.0 * 6.1369709968566895
Epoch 230, val loss: 1.68586003780365
Epoch 240, training loss: 13.890775680541992 = 1.6290916204452515 + 2.0 * 6.130842208862305
Epoch 240, val loss: 1.664359211921692
Epoch 250, training loss: 13.84948444366455 = 1.598327875137329 + 2.0 * 6.1255784034729
Epoch 250, val loss: 1.6395236253738403
Epoch 260, training loss: 13.802974700927734 = 1.5632274150848389 + 2.0 * 6.119873523712158
Epoch 260, val loss: 1.6112321615219116
Epoch 270, training loss: 13.754316329956055 = 1.5237289667129517 + 2.0 * 6.115293502807617
Epoch 270, val loss: 1.5793529748916626
Epoch 280, training loss: 13.702099800109863 = 1.4802998304367065 + 2.0 * 6.110899925231934
Epoch 280, val loss: 1.544262409210205
Epoch 290, training loss: 13.648446083068848 = 1.4339447021484375 + 2.0 * 6.107250690460205
Epoch 290, val loss: 1.507084608078003
Epoch 300, training loss: 13.595159530639648 = 1.386205792427063 + 2.0 * 6.1044769287109375
Epoch 300, val loss: 1.469071865081787
Epoch 310, training loss: 13.539191246032715 = 1.3382714986801147 + 2.0 * 6.100460052490234
Epoch 310, val loss: 1.4311425685882568
Epoch 320, training loss: 13.489073753356934 = 1.2907053232192993 + 2.0 * 6.099184036254883
Epoch 320, val loss: 1.3939450979232788
Epoch 330, training loss: 13.43305778503418 = 1.2444415092468262 + 2.0 * 6.094308376312256
Epoch 330, val loss: 1.3578922748565674
Epoch 340, training loss: 13.380693435668945 = 1.199160099029541 + 2.0 * 6.090766429901123
Epoch 340, val loss: 1.3232288360595703
Epoch 350, training loss: 13.34111499786377 = 1.1549111604690552 + 2.0 * 6.093101978302002
Epoch 350, val loss: 1.2896308898925781
Epoch 360, training loss: 13.2837495803833 = 1.1120017766952515 + 2.0 * 6.085874080657959
Epoch 360, val loss: 1.2574183940887451
Epoch 370, training loss: 13.232892036437988 = 1.069934606552124 + 2.0 * 6.081478595733643
Epoch 370, val loss: 1.2260273694992065
Epoch 380, training loss: 13.186735153198242 = 1.0283045768737793 + 2.0 * 6.079215049743652
Epoch 380, val loss: 1.195069432258606
Epoch 390, training loss: 13.140871047973633 = 0.9873690605163574 + 2.0 * 6.076751232147217
Epoch 390, val loss: 1.1649080514907837
Epoch 400, training loss: 13.095080375671387 = 0.9475768208503723 + 2.0 * 6.073751926422119
Epoch 400, val loss: 1.1356903314590454
Epoch 410, training loss: 13.053539276123047 = 0.9086405038833618 + 2.0 * 6.072449207305908
Epoch 410, val loss: 1.107222318649292
Epoch 420, training loss: 13.012682914733887 = 0.8709450960159302 + 2.0 * 6.070868968963623
Epoch 420, val loss: 1.0796781778335571
Epoch 430, training loss: 12.969511032104492 = 0.8344786763191223 + 2.0 * 6.067516326904297
Epoch 430, val loss: 1.0532796382904053
Epoch 440, training loss: 12.935308456420898 = 0.7994233965873718 + 2.0 * 6.0679426193237305
Epoch 440, val loss: 1.028178095817566
Epoch 450, training loss: 12.899545669555664 = 0.7665103077888489 + 2.0 * 6.0665178298950195
Epoch 450, val loss: 1.0046590566635132
Epoch 460, training loss: 12.858071327209473 = 0.7350109219551086 + 2.0 * 6.061530113220215
Epoch 460, val loss: 0.9825140833854675
Epoch 470, training loss: 12.824203491210938 = 0.7046228051185608 + 2.0 * 6.059790134429932
Epoch 470, val loss: 0.9614410400390625
Epoch 480, training loss: 12.791447639465332 = 0.6752435564994812 + 2.0 * 6.058102130889893
Epoch 480, val loss: 0.9413943886756897
Epoch 490, training loss: 12.761761665344238 = 0.6470018029212952 + 2.0 * 6.057379722595215
Epoch 490, val loss: 0.9223067164421082
Epoch 500, training loss: 12.736662864685059 = 0.6201630234718323 + 2.0 * 6.0582499504089355
Epoch 500, val loss: 0.9046204090118408
Epoch 510, training loss: 12.701950073242188 = 0.5944740176200867 + 2.0 * 6.053738117218018
Epoch 510, val loss: 0.8880296349525452
Epoch 520, training loss: 12.673468589782715 = 0.5696694254875183 + 2.0 * 6.051899433135986
Epoch 520, val loss: 0.8723231554031372
Epoch 530, training loss: 12.648017883300781 = 0.5456628799438477 + 2.0 * 6.051177501678467
Epoch 530, val loss: 0.8575007915496826
Epoch 540, training loss: 12.625426292419434 = 0.5224872827529907 + 2.0 * 6.051469326019287
Epoch 540, val loss: 0.8435441851615906
Epoch 550, training loss: 12.60086441040039 = 0.5003003478050232 + 2.0 * 6.050282001495361
Epoch 550, val loss: 0.8305650353431702
Epoch 560, training loss: 12.572723388671875 = 0.4788777530193329 + 2.0 * 6.04692268371582
Epoch 560, val loss: 0.8184176087379456
Epoch 570, training loss: 12.552499771118164 = 0.458148717880249 + 2.0 * 6.047175407409668
Epoch 570, val loss: 0.8069795370101929
Epoch 580, training loss: 12.526670455932617 = 0.43810564279556274 + 2.0 * 6.04428243637085
Epoch 580, val loss: 0.7962309122085571
Epoch 590, training loss: 12.504645347595215 = 0.41870683431625366 + 2.0 * 6.042969226837158
Epoch 590, val loss: 0.7862657308578491
Epoch 600, training loss: 12.496713638305664 = 0.39989152550697327 + 2.0 * 6.048410892486572
Epoch 600, val loss: 0.7769805192947388
Epoch 610, training loss: 12.46739673614502 = 0.38192254304885864 + 2.0 * 6.042737007141113
Epoch 610, val loss: 0.7683727741241455
Epoch 620, training loss: 12.444190979003906 = 0.36460354924201965 + 2.0 * 6.039793491363525
Epoch 620, val loss: 0.7607401609420776
Epoch 630, training loss: 12.425239562988281 = 0.34780353307724 + 2.0 * 6.038718223571777
Epoch 630, val loss: 0.7537037134170532
Epoch 640, training loss: 12.417274475097656 = 0.331468790769577 + 2.0 * 6.042902946472168
Epoch 640, val loss: 0.7472431659698486
Epoch 650, training loss: 12.396830558776855 = 0.3158435523509979 + 2.0 * 6.040493488311768
Epoch 650, val loss: 0.7414750456809998
Epoch 660, training loss: 12.37381362915039 = 0.30073991417884827 + 2.0 * 6.036536693572998
Epoch 660, val loss: 0.7365473508834839
Epoch 670, training loss: 12.355077743530273 = 0.2861289381980896 + 2.0 * 6.0344743728637695
Epoch 670, val loss: 0.732207715511322
Epoch 680, training loss: 12.340076446533203 = 0.27200230956077576 + 2.0 * 6.034037113189697
Epoch 680, val loss: 0.7284398078918457
Epoch 690, training loss: 12.325776100158691 = 0.258464515209198 + 2.0 * 6.033655643463135
Epoch 690, val loss: 0.7251415252685547
Epoch 700, training loss: 12.313652992248535 = 0.2456030696630478 + 2.0 * 6.034025192260742
Epoch 700, val loss: 0.722536027431488
Epoch 710, training loss: 12.296483039855957 = 0.2333299219608307 + 2.0 * 6.031576633453369
Epoch 710, val loss: 0.7205356955528259
Epoch 720, training loss: 12.283427238464355 = 0.22158877551555634 + 2.0 * 6.030919075012207
Epoch 720, val loss: 0.7189692854881287
Epoch 730, training loss: 12.27518367767334 = 0.21038402616977692 + 2.0 * 6.032399654388428
Epoch 730, val loss: 0.7178566455841064
Epoch 740, training loss: 12.266833305358887 = 0.19979068636894226 + 2.0 * 6.0335211753845215
Epoch 740, val loss: 0.7170615196228027
Epoch 750, training loss: 12.248576164245605 = 0.18978114426136017 + 2.0 * 6.029397487640381
Epoch 750, val loss: 0.7167625427246094
Epoch 760, training loss: 12.234566688537598 = 0.18032802641391754 + 2.0 * 6.027119159698486
Epoch 760, val loss: 0.7167839407920837
Epoch 770, training loss: 12.226051330566406 = 0.17135639488697052 + 2.0 * 6.027347564697266
Epoch 770, val loss: 0.717129647731781
Epoch 780, training loss: 12.215045928955078 = 0.16292178630828857 + 2.0 * 6.02606201171875
Epoch 780, val loss: 0.7176871299743652
Epoch 790, training loss: 12.207770347595215 = 0.15503939986228943 + 2.0 * 6.026365280151367
Epoch 790, val loss: 0.7185500264167786
Epoch 800, training loss: 12.19678020477295 = 0.14760857820510864 + 2.0 * 6.024585723876953
Epoch 800, val loss: 0.7197051644325256
Epoch 810, training loss: 12.188210487365723 = 0.14059004187583923 + 2.0 * 6.023810386657715
Epoch 810, val loss: 0.7210544943809509
Epoch 820, training loss: 12.18600082397461 = 0.13402634859085083 + 2.0 * 6.025987148284912
Epoch 820, val loss: 0.7225490808486938
Epoch 830, training loss: 12.17935848236084 = 0.12791073322296143 + 2.0 * 6.025723934173584
Epoch 830, val loss: 0.7241605520248413
Epoch 840, training loss: 12.166535377502441 = 0.1221713274717331 + 2.0 * 6.022181987762451
Epoch 840, val loss: 0.7259864807128906
Epoch 850, training loss: 12.158103942871094 = 0.11676502972841263 + 2.0 * 6.020669460296631
Epoch 850, val loss: 0.7280011177062988
Epoch 860, training loss: 12.16471004486084 = 0.11166056990623474 + 2.0 * 6.026524543762207
Epoch 860, val loss: 0.7301074862480164
Epoch 870, training loss: 12.151605606079102 = 0.10688702762126923 + 2.0 * 6.022359371185303
Epoch 870, val loss: 0.7323104739189148
Epoch 880, training loss: 12.14200210571289 = 0.10240232944488525 + 2.0 * 6.019799709320068
Epoch 880, val loss: 0.734612226486206
Epoch 890, training loss: 12.13683795928955 = 0.09816709160804749 + 2.0 * 6.0193352699279785
Epoch 890, val loss: 0.7370015382766724
Epoch 900, training loss: 12.130303382873535 = 0.09415826946496964 + 2.0 * 6.018072605133057
Epoch 900, val loss: 0.7395203113555908
Epoch 910, training loss: 12.131189346313477 = 0.09036186337471008 + 2.0 * 6.020413875579834
Epoch 910, val loss: 0.7421213984489441
Epoch 920, training loss: 12.121989250183105 = 0.08678626269102097 + 2.0 * 6.017601490020752
Epoch 920, val loss: 0.7447447776794434
Epoch 930, training loss: 12.117477416992188 = 0.0834038108587265 + 2.0 * 6.0170369148254395
Epoch 930, val loss: 0.7474356293678284
Epoch 940, training loss: 12.111381530761719 = 0.08020887523889542 + 2.0 * 6.0155863761901855
Epoch 940, val loss: 0.7502310276031494
Epoch 950, training loss: 12.114411354064941 = 0.07718119025230408 + 2.0 * 6.018615245819092
Epoch 950, val loss: 0.7530037760734558
Epoch 960, training loss: 12.105253219604492 = 0.07431329041719437 + 2.0 * 6.015470027923584
Epoch 960, val loss: 0.7557530403137207
Epoch 970, training loss: 12.102632522583008 = 0.07159923017024994 + 2.0 * 6.015516757965088
Epoch 970, val loss: 0.7585961818695068
Epoch 980, training loss: 12.097878456115723 = 0.06901747733354568 + 2.0 * 6.014430522918701
Epoch 980, val loss: 0.7614018321037292
Epoch 990, training loss: 12.09322738647461 = 0.06656337529420853 + 2.0 * 6.013331890106201
Epoch 990, val loss: 0.7642495036125183
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 730, in <module>
    torch.cuda.empty_cache()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/cuda/memory.py", line 121, in empty_cache
    torch._C._cuda_emptyCache()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.4 der2:0.2 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 18.695003509521484 = 1.9473512172698975 + 2.0 * 8.373826026916504
Epoch 0, val loss: 1.9491093158721924
Epoch 10, training loss: 18.683834075927734 = 1.9374979734420776 + 2.0 * 8.373167991638184
Epoch 10, val loss: 1.9399449825286865
Epoch 20, training loss: 18.66260528564453 = 1.9253419637680054 + 2.0 * 8.368631362915039
Epoch 20, val loss: 1.9281061887741089
Epoch 30, training loss: 18.578176498413086 = 1.9093804359436035 + 2.0 * 8.33439826965332
Epoch 30, val loss: 1.9123259782791138
Epoch 40, training loss: 18.05243682861328 = 1.890794277191162 + 2.0 * 8.08082103729248
Epoch 40, val loss: 1.8945279121398926
Epoch 50, training loss: 16.846359252929688 = 1.8716298341751099 + 2.0 * 7.487364292144775
Epoch 50, val loss: 1.876873254776001
Epoch 60, training loss: 15.941695213317871 = 1.858966588973999 + 2.0 * 7.0413641929626465
Epoch 60, val loss: 1.865606665611267
Epoch 70, training loss: 15.339908599853516 = 1.8495259284973145 + 2.0 * 6.7451910972595215
Epoch 70, val loss: 1.8565959930419922
Epoch 80, training loss: 15.030981063842773 = 1.8408302068710327 + 2.0 * 6.595075607299805
Epoch 80, val loss: 1.8482130765914917
Epoch 90, training loss: 14.81836986541748 = 1.8314446210861206 + 2.0 * 6.493462562561035
Epoch 90, val loss: 1.839320182800293
Epoch 100, training loss: 14.67044734954834 = 1.8225094079971313 + 2.0 * 6.42396879196167
Epoch 100, val loss: 1.8306546211242676
Epoch 110, training loss: 14.568448066711426 = 1.8145455121994019 + 2.0 * 6.376951217651367
Epoch 110, val loss: 1.8228509426116943
Epoch 120, training loss: 14.479601860046387 = 1.8074125051498413 + 2.0 * 6.336094856262207
Epoch 120, val loss: 1.8156324625015259
Epoch 130, training loss: 14.406028747558594 = 1.8008918762207031 + 2.0 * 6.302568435668945
Epoch 130, val loss: 1.808949589729309
Epoch 140, training loss: 14.34062671661377 = 1.7947413921356201 + 2.0 * 6.272942543029785
Epoch 140, val loss: 1.8025989532470703
Epoch 150, training loss: 14.286130905151367 = 1.7884596586227417 + 2.0 * 6.248835563659668
Epoch 150, val loss: 1.7963571548461914
Epoch 160, training loss: 14.241870880126953 = 1.7816815376281738 + 2.0 * 6.230094909667969
Epoch 160, val loss: 1.7900595664978027
Epoch 170, training loss: 14.19921588897705 = 1.7743359804153442 + 2.0 * 6.212440013885498
Epoch 170, val loss: 1.783570408821106
Epoch 180, training loss: 14.16187858581543 = 1.7662711143493652 + 2.0 * 6.197803974151611
Epoch 180, val loss: 1.776713490486145
Epoch 190, training loss: 14.130398750305176 = 1.7572208642959595 + 2.0 * 6.186588764190674
Epoch 190, val loss: 1.7692890167236328
Epoch 200, training loss: 14.095666885375977 = 1.7469816207885742 + 2.0 * 6.174342632293701
Epoch 200, val loss: 1.7610583305358887
Epoch 210, training loss: 14.07157039642334 = 1.735231637954712 + 2.0 * 6.1681694984436035
Epoch 210, val loss: 1.7517976760864258
Epoch 220, training loss: 14.03557300567627 = 1.7217912673950195 + 2.0 * 6.156890869140625
Epoch 220, val loss: 1.7411651611328125
Epoch 230, training loss: 14.003171920776367 = 1.7063082456588745 + 2.0 * 6.148431777954102
Epoch 230, val loss: 1.7290037870407104
Epoch 240, training loss: 13.971339225769043 = 1.6882799863815308 + 2.0 * 6.141529560089111
Epoch 240, val loss: 1.7148356437683105
Epoch 250, training loss: 13.938498497009277 = 1.6672512292861938 + 2.0 * 6.135623455047607
Epoch 250, val loss: 1.6982399225234985
Epoch 260, training loss: 13.901565551757812 = 1.6428091526031494 + 2.0 * 6.129378318786621
Epoch 260, val loss: 1.6789809465408325
Epoch 270, training loss: 13.862051010131836 = 1.6144949197769165 + 2.0 * 6.123777866363525
Epoch 270, val loss: 1.6564621925354004
Epoch 280, training loss: 13.823898315429688 = 1.581696629524231 + 2.0 * 6.121100902557373
Epoch 280, val loss: 1.6303362846374512
Epoch 290, training loss: 13.776007652282715 = 1.5446735620498657 + 2.0 * 6.11566686630249
Epoch 290, val loss: 1.6005183458328247
Epoch 300, training loss: 13.725347518920898 = 1.5032075643539429 + 2.0 * 6.111070156097412
Epoch 300, val loss: 1.5670068264007568
Epoch 310, training loss: 13.680448532104492 = 1.4575995206832886 + 2.0 * 6.111424446105957
Epoch 310, val loss: 1.5298153162002563
Epoch 320, training loss: 13.618399620056152 = 1.409108281135559 + 2.0 * 6.104645729064941
Epoch 320, val loss: 1.4902899265289307
Epoch 330, training loss: 13.561151504516602 = 1.3586745262145996 + 2.0 * 6.10123872756958
Epoch 330, val loss: 1.449077844619751
Epoch 340, training loss: 13.511990547180176 = 1.307349443435669 + 2.0 * 6.102320671081543
Epoch 340, val loss: 1.4070765972137451
Epoch 350, training loss: 13.449922561645508 = 1.2568472623825073 + 2.0 * 6.0965375900268555
Epoch 350, val loss: 1.3660463094711304
Epoch 360, training loss: 13.394176483154297 = 1.208086371421814 + 2.0 * 6.093045234680176
Epoch 360, val loss: 1.326812982559204
Epoch 370, training loss: 13.343086242675781 = 1.1613190174102783 + 2.0 * 6.090883731842041
Epoch 370, val loss: 1.2893587350845337
Epoch 380, training loss: 13.303323745727539 = 1.1171118021011353 + 2.0 * 6.093105792999268
Epoch 380, val loss: 1.2549699544906616
Epoch 390, training loss: 13.246939659118652 = 1.076160192489624 + 2.0 * 6.085389614105225
Epoch 390, val loss: 1.22306489944458
Epoch 400, training loss: 13.201677322387695 = 1.0374846458435059 + 2.0 * 6.082096099853516
Epoch 400, val loss: 1.1934237480163574
Epoch 410, training loss: 13.160057067871094 = 1.0006189346313477 + 2.0 * 6.079719066619873
Epoch 410, val loss: 1.1655555963516235
Epoch 420, training loss: 13.131034851074219 = 0.9653653502464294 + 2.0 * 6.082834720611572
Epoch 420, val loss: 1.139194369316101
Epoch 430, training loss: 13.08579158782959 = 0.9316930770874023 + 2.0 * 6.077049255371094
Epoch 430, val loss: 1.1144224405288696
Epoch 440, training loss: 13.049210548400879 = 0.8995428085327148 + 2.0 * 6.074833869934082
Epoch 440, val loss: 1.090866208076477
Epoch 450, training loss: 13.010972023010254 = 0.8680865168571472 + 2.0 * 6.071442604064941
Epoch 450, val loss: 1.0679324865341187
Epoch 460, training loss: 12.985342025756836 = 0.8370064496994019 + 2.0 * 6.074167728424072
Epoch 460, val loss: 1.0453046560287476
Epoch 470, training loss: 12.946035385131836 = 0.8065485954284668 + 2.0 * 6.069743633270264
Epoch 470, val loss: 1.0231131315231323
Epoch 480, training loss: 12.911005973815918 = 0.7766938805580139 + 2.0 * 6.067155838012695
Epoch 480, val loss: 1.0012301206588745
Epoch 490, training loss: 12.88484001159668 = 0.7470417022705078 + 2.0 * 6.068899154663086
Epoch 490, val loss: 0.9793435335159302
Epoch 500, training loss: 12.848429679870605 = 0.7177630662918091 + 2.0 * 6.065333366394043
Epoch 500, val loss: 0.9578133821487427
Epoch 510, training loss: 12.814876556396484 = 0.6888159513473511 + 2.0 * 6.063030242919922
Epoch 510, val loss: 0.9363894462585449
Epoch 520, training loss: 12.78481388092041 = 0.6601664423942566 + 2.0 * 6.062323570251465
Epoch 520, val loss: 0.91529780626297
Epoch 530, training loss: 12.756792068481445 = 0.6322577595710754 + 2.0 * 6.062267303466797
Epoch 530, val loss: 0.8944488763809204
Epoch 540, training loss: 12.722134590148926 = 0.6049872636795044 + 2.0 * 6.0585737228393555
Epoch 540, val loss: 0.8746437430381775
Epoch 550, training loss: 12.694310188293457 = 0.5784121751785278 + 2.0 * 6.057949066162109
Epoch 550, val loss: 0.8554241061210632
Epoch 560, training loss: 12.679112434387207 = 0.5525328516960144 + 2.0 * 6.063289642333984
Epoch 560, val loss: 0.8369240760803223
Epoch 570, training loss: 12.63969898223877 = 0.5277026295661926 + 2.0 * 6.0559983253479
Epoch 570, val loss: 0.8194975852966309
Epoch 580, training loss: 12.624187469482422 = 0.5038339495658875 + 2.0 * 6.060176849365234
Epoch 580, val loss: 0.8030943274497986
Epoch 590, training loss: 12.59375286102295 = 0.4809633493423462 + 2.0 * 6.056394577026367
Epoch 590, val loss: 0.7877523303031921
Epoch 600, training loss: 12.563058853149414 = 0.45906490087509155 + 2.0 * 6.051997184753418
Epoch 600, val loss: 0.7734701037406921
Epoch 610, training loss: 12.54069709777832 = 0.43797972798347473 + 2.0 * 6.051358699798584
Epoch 610, val loss: 0.760048508644104
Epoch 620, training loss: 12.525782585144043 = 0.41767579317092896 + 2.0 * 6.05405330657959
Epoch 620, val loss: 0.7474588751792908
Epoch 630, training loss: 12.504652976989746 = 0.39830684661865234 + 2.0 * 6.053173065185547
Epoch 630, val loss: 0.7358708381652832
Epoch 640, training loss: 12.475812911987305 = 0.3798186480998993 + 2.0 * 6.047996997833252
Epoch 640, val loss: 0.7251938581466675
Epoch 650, training loss: 12.457320213317871 = 0.3620830774307251 + 2.0 * 6.047618389129639
Epoch 650, val loss: 0.7152660489082336
Epoch 660, training loss: 12.456323623657227 = 0.3450593948364258 + 2.0 * 6.0556321144104
Epoch 660, val loss: 0.7062018513679504
Epoch 670, training loss: 12.421014785766602 = 0.3290344774723053 + 2.0 * 6.045989990234375
Epoch 670, val loss: 0.6978470087051392
Epoch 680, training loss: 12.405267715454102 = 0.31367939710617065 + 2.0 * 6.0457940101623535
Epoch 680, val loss: 0.6903048753738403
Epoch 690, training loss: 12.390759468078613 = 0.29904356598854065 + 2.0 * 6.045857906341553
Epoch 690, val loss: 0.6834357380867004
Epoch 700, training loss: 12.370113372802734 = 0.2851005494594574 + 2.0 * 6.042506217956543
Epoch 700, val loss: 0.6773377656936646
Epoch 710, training loss: 12.35546588897705 = 0.2717466950416565 + 2.0 * 6.0418596267700195
Epoch 710, val loss: 0.6717782020568848
Epoch 720, training loss: 12.356592178344727 = 0.25897619128227234 + 2.0 * 6.0488080978393555
Epoch 720, val loss: 0.666826069355011
Epoch 730, training loss: 12.327311515808105 = 0.2468487173318863 + 2.0 * 6.040231227874756
Epoch 730, val loss: 0.6625679135322571
Epoch 740, training loss: 12.316174507141113 = 0.23526518046855927 + 2.0 * 6.040454864501953
Epoch 740, val loss: 0.6587955355644226
Epoch 750, training loss: 12.310592651367188 = 0.22419773042201996 + 2.0 * 6.0431976318359375
Epoch 750, val loss: 0.6554247736930847
Epoch 760, training loss: 12.295358657836914 = 0.2136593461036682 + 2.0 * 6.040849685668945
Epoch 760, val loss: 0.6525814533233643
Epoch 770, training loss: 12.27901554107666 = 0.2036752700805664 + 2.0 * 6.037670135498047
Epoch 770, val loss: 0.6502212285995483
Epoch 780, training loss: 12.270133972167969 = 0.19414149224758148 + 2.0 * 6.037996292114258
Epoch 780, val loss: 0.6482084393501282
Epoch 790, training loss: 12.259654998779297 = 0.18506014347076416 + 2.0 * 6.037297248840332
Epoch 790, val loss: 0.6465970873832703
Epoch 800, training loss: 12.252126693725586 = 0.17644824087619781 + 2.0 * 6.037839412689209
Epoch 800, val loss: 0.6452318429946899
Epoch 810, training loss: 12.238192558288574 = 0.16826392710208893 + 2.0 * 6.034964084625244
Epoch 810, val loss: 0.6441380381584167
Epoch 820, training loss: 12.226807594299316 = 0.16046077013015747 + 2.0 * 6.033173561096191
Epoch 820, val loss: 0.6433799266815186
Epoch 830, training loss: 12.230170249938965 = 0.15306949615478516 + 2.0 * 6.03855037689209
Epoch 830, val loss: 0.6428755521774292
Epoch 840, training loss: 12.214439392089844 = 0.14604200422763824 + 2.0 * 6.034198760986328
Epoch 840, val loss: 0.6426507234573364
Epoch 850, training loss: 12.207098960876465 = 0.13942478597164154 + 2.0 * 6.03383731842041
Epoch 850, val loss: 0.6425980925559998
Epoch 860, training loss: 12.198076248168945 = 0.13315725326538086 + 2.0 * 6.032459735870361
Epoch 860, val loss: 0.642729640007019
Epoch 870, training loss: 12.187832832336426 = 0.12720367312431335 + 2.0 * 6.0303144454956055
Epoch 870, val loss: 0.6430715918540955
Epoch 880, training loss: 12.178506851196289 = 0.12154177576303482 + 2.0 * 6.028482437133789
Epoch 880, val loss: 0.6435860991477966
Epoch 890, training loss: 12.18359088897705 = 0.11614805459976196 + 2.0 * 6.033721446990967
Epoch 890, val loss: 0.6443287134170532
Epoch 900, training loss: 12.175981521606445 = 0.11109787225723267 + 2.0 * 6.03244161605835
Epoch 900, val loss: 0.6450961828231812
Epoch 910, training loss: 12.162858963012695 = 0.106279656291008 + 2.0 * 6.028289794921875
Epoch 910, val loss: 0.6459977030754089
Epoch 920, training loss: 12.153182983398438 = 0.10173533111810684 + 2.0 * 6.025723934173584
Epoch 920, val loss: 0.6470603942871094
Epoch 930, training loss: 12.156986236572266 = 0.09741044789552689 + 2.0 * 6.029788017272949
Epoch 930, val loss: 0.6482734084129333
Epoch 940, training loss: 12.149568557739258 = 0.0933319553732872 + 2.0 * 6.028118133544922
Epoch 940, val loss: 0.6494189500808716
Epoch 950, training loss: 12.13971996307373 = 0.08947759121656418 + 2.0 * 6.025121212005615
Epoch 950, val loss: 0.6508176922798157
Epoch 960, training loss: 12.132567405700684 = 0.08581455796957016 + 2.0 * 6.02337646484375
Epoch 960, val loss: 0.6522396206855774
Epoch 970, training loss: 12.133516311645508 = 0.08231604099273682 + 2.0 * 6.025599956512451
Epoch 970, val loss: 0.6537584066390991
Epoch 980, training loss: 12.123011589050293 = 0.07902243733406067 + 2.0 * 6.021994590759277
Epoch 980, val loss: 0.6554083228111267
Epoch 990, training loss: 12.122087478637695 = 0.07588481903076172 + 2.0 * 6.023101329803467
Epoch 990, val loss: 0.6570534110069275
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 730, in <module>
    torch.cuda.empty_cache()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/cuda/memory.py", line 121, in empty_cache
    torch._C._cuda_emptyCache()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.4 der2:0.2 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 18.69264030456543 = 1.9448177814483643 + 2.0 * 8.373910903930664
Epoch 0, val loss: 1.9423389434814453
Epoch 10, training loss: 18.681598663330078 = 1.9345356225967407 + 2.0 * 8.373531341552734
Epoch 10, val loss: 1.9326969385147095
Epoch 20, training loss: 18.663379669189453 = 1.9215611219406128 + 2.0 * 8.370909690856934
Epoch 20, val loss: 1.9202378988265991
Epoch 30, training loss: 18.608375549316406 = 1.9039462804794312 + 2.0 * 8.352214813232422
Epoch 30, val loss: 1.9034792184829712
Epoch 40, training loss: 18.348312377929688 = 1.882072925567627 + 2.0 * 8.23311996459961
Epoch 40, val loss: 1.88348388671875
Epoch 50, training loss: 17.27482795715332 = 1.8592228889465332 + 2.0 * 7.707802772521973
Epoch 50, val loss: 1.8632451295852661
Epoch 60, training loss: 16.379945755004883 = 1.8411140441894531 + 2.0 * 7.269415855407715
Epoch 60, val loss: 1.847167730331421
Epoch 70, training loss: 15.633712768554688 = 1.8276903629302979 + 2.0 * 6.903011322021484
Epoch 70, val loss: 1.834581732749939
Epoch 80, training loss: 15.199061393737793 = 1.8147536516189575 + 2.0 * 6.6921539306640625
Epoch 80, val loss: 1.8220765590667725
Epoch 90, training loss: 14.925317764282227 = 1.8007079362869263 + 2.0 * 6.562304973602295
Epoch 90, val loss: 1.8092718124389648
Epoch 100, training loss: 14.750069618225098 = 1.7862576246261597 + 2.0 * 6.481905937194824
Epoch 100, val loss: 1.7964595556259155
Epoch 110, training loss: 14.62007999420166 = 1.7714003324508667 + 2.0 * 6.424339771270752
Epoch 110, val loss: 1.783390760421753
Epoch 120, training loss: 14.517271041870117 = 1.7559163570404053 + 2.0 * 6.380677223205566
Epoch 120, val loss: 1.7699207067489624
Epoch 130, training loss: 14.432025909423828 = 1.7396939992904663 + 2.0 * 6.346166133880615
Epoch 130, val loss: 1.755934238433838
Epoch 140, training loss: 14.355809211730957 = 1.7222826480865479 + 2.0 * 6.316763401031494
Epoch 140, val loss: 1.7411668300628662
Epoch 150, training loss: 14.289803504943848 = 1.703016996383667 + 2.0 * 6.293393135070801
Epoch 150, val loss: 1.7250099182128906
Epoch 160, training loss: 14.229783058166504 = 1.6812429428100586 + 2.0 * 6.274270057678223
Epoch 160, val loss: 1.7071460485458374
Epoch 170, training loss: 14.171415328979492 = 1.6565275192260742 + 2.0 * 6.257443904876709
Epoch 170, val loss: 1.6872156858444214
Epoch 180, training loss: 14.11506462097168 = 1.6283327341079712 + 2.0 * 6.24336576461792
Epoch 180, val loss: 1.6645922660827637
Epoch 190, training loss: 14.062482833862305 = 1.5959734916687012 + 2.0 * 6.233254432678223
Epoch 190, val loss: 1.6388627290725708
Epoch 200, training loss: 14.004910469055176 = 1.5597333908081055 + 2.0 * 6.222588539123535
Epoch 200, val loss: 1.610185146331787
Epoch 210, training loss: 13.94654655456543 = 1.5190588235855103 + 2.0 * 6.213743686676025
Epoch 210, val loss: 1.5781604051589966
Epoch 220, training loss: 13.889467239379883 = 1.4742708206176758 + 2.0 * 6.2075982093811035
Epoch 220, val loss: 1.5430108308792114
Epoch 230, training loss: 13.825817108154297 = 1.4263310432434082 + 2.0 * 6.199743270874023
Epoch 230, val loss: 1.5054408311843872
Epoch 240, training loss: 13.762922286987305 = 1.375919222831726 + 2.0 * 6.1935014724731445
Epoch 240, val loss: 1.4662519693374634
Epoch 250, training loss: 13.700971603393555 = 1.323533296585083 + 2.0 * 6.188719272613525
Epoch 250, val loss: 1.42589271068573
Epoch 260, training loss: 13.640043258666992 = 1.2707765102386475 + 2.0 * 6.184633255004883
Epoch 260, val loss: 1.3856858015060425
Epoch 270, training loss: 13.573216438293457 = 1.2187089920043945 + 2.0 * 6.177253723144531
Epoch 270, val loss: 1.3464995622634888
Epoch 280, training loss: 13.513559341430664 = 1.1678911447525024 + 2.0 * 6.1728339195251465
Epoch 280, val loss: 1.3088078498840332
Epoch 290, training loss: 13.453696250915527 = 1.1191600561141968 + 2.0 * 6.1672682762146
Epoch 290, val loss: 1.273276686668396
Epoch 300, training loss: 13.418722152709961 = 1.0727782249450684 + 2.0 * 6.172971725463867
Epoch 300, val loss: 1.2401750087738037
Epoch 310, training loss: 13.34760570526123 = 1.0296076536178589 + 2.0 * 6.158998966217041
Epoch 310, val loss: 1.2097177505493164
Epoch 320, training loss: 13.295639038085938 = 0.9888808727264404 + 2.0 * 6.153378963470459
Epoch 320, val loss: 1.1813369989395142
Epoch 330, training loss: 13.245187759399414 = 0.9499899744987488 + 2.0 * 6.147598743438721
Epoch 330, val loss: 1.1545883417129517
Epoch 340, training loss: 13.204201698303223 = 0.9127887487411499 + 2.0 * 6.145706653594971
Epoch 340, val loss: 1.1292153596878052
Epoch 350, training loss: 13.157139778137207 = 0.8775014877319336 + 2.0 * 6.139819145202637
Epoch 350, val loss: 1.1053357124328613
Epoch 360, training loss: 13.115267753601074 = 0.8440439701080322 + 2.0 * 6.1356120109558105
Epoch 360, val loss: 1.082784652709961
Epoch 370, training loss: 13.076000213623047 = 0.8120256066322327 + 2.0 * 6.13198709487915
Epoch 370, val loss: 1.0613235235214233
Epoch 380, training loss: 13.035615921020508 = 0.7814396023750305 + 2.0 * 6.1270880699157715
Epoch 380, val loss: 1.0409053564071655
Epoch 390, training loss: 13.011600494384766 = 0.7522622346878052 + 2.0 * 6.129669189453125
Epoch 390, val loss: 1.0216509103775024
Epoch 400, training loss: 12.968058586120605 = 0.7246193289756775 + 2.0 * 6.121719837188721
Epoch 400, val loss: 1.0035582780838013
Epoch 410, training loss: 12.93311595916748 = 0.698235273361206 + 2.0 * 6.117440223693848
Epoch 410, val loss: 0.9865642189979553
Epoch 420, training loss: 12.90760612487793 = 0.6729705333709717 + 2.0 * 6.1173176765441895
Epoch 420, val loss: 0.9705311059951782
Epoch 430, training loss: 12.875110626220703 = 0.6488243937492371 + 2.0 * 6.113142967224121
Epoch 430, val loss: 0.9556561708450317
Epoch 440, training loss: 12.843574523925781 = 0.6257435083389282 + 2.0 * 6.108915328979492
Epoch 440, val loss: 0.9417853355407715
Epoch 450, training loss: 12.8162202835083 = 0.603411853313446 + 2.0 * 6.1064043045043945
Epoch 450, val loss: 0.9287928938865662
Epoch 460, training loss: 12.788663864135742 = 0.5819098353385925 + 2.0 * 6.103376865386963
Epoch 460, val loss: 0.9166901707649231
Epoch 470, training loss: 12.763620376586914 = 0.5611894726753235 + 2.0 * 6.101215362548828
Epoch 470, val loss: 0.9054581522941589
Epoch 480, training loss: 12.73855209350586 = 0.5410596132278442 + 2.0 * 6.098746299743652
Epoch 480, val loss: 0.8949761390686035
Epoch 490, training loss: 12.734169960021973 = 0.5214717984199524 + 2.0 * 6.106348991394043
Epoch 490, val loss: 0.8852604627609253
Epoch 500, training loss: 12.69576644897461 = 0.5025554895401001 + 2.0 * 6.09660530090332
Epoch 500, val loss: 0.8764092922210693
Epoch 510, training loss: 12.669342994689941 = 0.48413053154945374 + 2.0 * 6.092606067657471
Epoch 510, val loss: 0.868354082107544
Epoch 520, training loss: 12.646005630493164 = 0.4661141037940979 + 2.0 * 6.0899457931518555
Epoch 520, val loss: 0.8609995245933533
Epoch 530, training loss: 12.625381469726562 = 0.44846367835998535 + 2.0 * 6.088459014892578
Epoch 530, val loss: 0.854371964931488
Epoch 540, training loss: 12.61699104309082 = 0.4312158524990082 + 2.0 * 6.0928874015808105
Epoch 540, val loss: 0.8484436869621277
Epoch 550, training loss: 12.588357925415039 = 0.4144614636898041 + 2.0 * 6.086948394775391
Epoch 550, val loss: 0.8433322310447693
Epoch 560, training loss: 12.563031196594238 = 0.3980753421783447 + 2.0 * 6.082478046417236
Epoch 560, val loss: 0.8389195203781128
Epoch 570, training loss: 12.555316925048828 = 0.38201165199279785 + 2.0 * 6.086652755737305
Epoch 570, val loss: 0.8351149559020996
Epoch 580, training loss: 12.528568267822266 = 0.3662475645542145 + 2.0 * 6.081160545349121
Epoch 580, val loss: 0.8318876028060913
Epoch 590, training loss: 12.50627326965332 = 0.35090476274490356 + 2.0 * 6.07768440246582
Epoch 590, val loss: 0.8293618559837341
Epoch 600, training loss: 12.487306594848633 = 0.33583784103393555 + 2.0 * 6.075734615325928
Epoch 600, val loss: 0.8272954225540161
Epoch 610, training loss: 12.469922065734863 = 0.32103434205055237 + 2.0 * 6.074443817138672
Epoch 610, val loss: 0.8257693648338318
Epoch 620, training loss: 12.464045524597168 = 0.30654221773147583 + 2.0 * 6.078751564025879
Epoch 620, val loss: 0.8247293829917908
Epoch 630, training loss: 12.441397666931152 = 0.29249003529548645 + 2.0 * 6.074453830718994
Epoch 630, val loss: 0.8242202997207642
Epoch 640, training loss: 12.42066764831543 = 0.27880874276161194 + 2.0 * 6.070929527282715
Epoch 640, val loss: 0.824202299118042
Epoch 650, training loss: 12.404029846191406 = 0.26548153162002563 + 2.0 * 6.069273948669434
Epoch 650, val loss: 0.8246564269065857
Epoch 660, training loss: 12.399145126342773 = 0.2525288462638855 + 2.0 * 6.073307991027832
Epoch 660, val loss: 0.8256232738494873
Epoch 670, training loss: 12.378125190734863 = 0.24002014100551605 + 2.0 * 6.069052696228027
Epoch 670, val loss: 0.8270965218544006
Epoch 680, training loss: 12.359230995178223 = 0.22797131538391113 + 2.0 * 6.065629959106445
Epoch 680, val loss: 0.8289856314659119
Epoch 690, training loss: 12.345307350158691 = 0.21637025475502014 + 2.0 * 6.0644683837890625
Epoch 690, val loss: 0.831392765045166
Epoch 700, training loss: 12.3319673538208 = 0.20526564121246338 + 2.0 * 6.063350677490234
Epoch 700, val loss: 0.8341071009635925
Epoch 710, training loss: 12.320228576660156 = 0.1946955919265747 + 2.0 * 6.0627665519714355
Epoch 710, val loss: 0.837408721446991
Epoch 720, training loss: 12.306290626525879 = 0.18461103737354279 + 2.0 * 6.060839653015137
Epoch 720, val loss: 0.8410894870758057
Epoch 730, training loss: 12.29430866241455 = 0.1750076711177826 + 2.0 * 6.059650421142578
Epoch 730, val loss: 0.8450665473937988
Epoch 740, training loss: 12.293166160583496 = 0.16588349640369415 + 2.0 * 6.063641548156738
Epoch 740, val loss: 0.8493907451629639
Epoch 750, training loss: 12.278684616088867 = 0.1572253257036209 + 2.0 * 6.060729503631592
Epoch 750, val loss: 0.8538982272148132
Epoch 760, training loss: 12.262038230895996 = 0.14906443655490875 + 2.0 * 6.056487083435059
Epoch 760, val loss: 0.8587996959686279
Epoch 770, training loss: 12.256636619567871 = 0.14134597778320312 + 2.0 * 6.057645320892334
Epoch 770, val loss: 0.8639048337936401
Epoch 780, training loss: 12.245954513549805 = 0.13408444821834564 + 2.0 * 6.055934906005859
Epoch 780, val loss: 0.8691654205322266
Epoch 790, training loss: 12.235898971557617 = 0.12723809480667114 + 2.0 * 6.054330348968506
Epoch 790, val loss: 0.8746871948242188
Epoch 800, training loss: 12.226387023925781 = 0.1207941472530365 + 2.0 * 6.052796363830566
Epoch 800, val loss: 0.88025963306427
Epoch 810, training loss: 12.229118347167969 = 0.11471124738454819 + 2.0 * 6.057203769683838
Epoch 810, val loss: 0.8859522342681885
Epoch 820, training loss: 12.218425750732422 = 0.10901977121829987 + 2.0 * 6.0547027587890625
Epoch 820, val loss: 0.8918296098709106
Epoch 830, training loss: 12.204503059387207 = 0.1036677286028862 + 2.0 * 6.050417900085449
Epoch 830, val loss: 0.8975903391838074
Epoch 840, training loss: 12.198498725891113 = 0.09864947944879532 + 2.0 * 6.049924850463867
Epoch 840, val loss: 0.9035427570343018
Epoch 850, training loss: 12.192837715148926 = 0.09393143653869629 + 2.0 * 6.049453258514404
Epoch 850, val loss: 0.9094361066818237
Epoch 860, training loss: 12.185708045959473 = 0.08950138092041016 + 2.0 * 6.048103332519531
Epoch 860, val loss: 0.9152413010597229
Epoch 870, training loss: 12.18011474609375 = 0.08534421771764755 + 2.0 * 6.047385215759277
Epoch 870, val loss: 0.9212710857391357
Epoch 880, training loss: 12.171614646911621 = 0.08143958449363708 + 2.0 * 6.0450873374938965
Epoch 880, val loss: 0.9271453619003296
Epoch 890, training loss: 12.168551445007324 = 0.07776942104101181 + 2.0 * 6.045391082763672
Epoch 890, val loss: 0.9329574108123779
Epoch 900, training loss: 12.165343284606934 = 0.07431496679782867 + 2.0 * 6.045514106750488
Epoch 900, val loss: 0.9386272430419922
Epoch 910, training loss: 12.162753105163574 = 0.071080282330513 + 2.0 * 6.045836448669434
Epoch 910, val loss: 0.9444250464439392
Epoch 920, training loss: 12.151700973510742 = 0.06803732365369797 + 2.0 * 6.041831970214844
Epoch 920, val loss: 0.950126588344574
Epoch 930, training loss: 12.149200439453125 = 0.06517235934734344 + 2.0 * 6.042014122009277
Epoch 930, val loss: 0.9556981921195984
Epoch 940, training loss: 12.157476425170898 = 0.06247209012508392 + 2.0 * 6.047502040863037
Epoch 940, val loss: 0.9611741900444031
Epoch 950, training loss: 12.143577575683594 = 0.05993879586458206 + 2.0 * 6.0418195724487305
Epoch 950, val loss: 0.9666616916656494
Epoch 960, training loss: 12.137418746948242 = 0.057551365345716476 + 2.0 * 6.039933681488037
Epoch 960, val loss: 0.9722306728363037
Epoch 970, training loss: 12.133220672607422 = 0.055292271077632904 + 2.0 * 6.03896427154541
Epoch 970, val loss: 0.9775703549385071
Epoch 980, training loss: 12.134138107299805 = 0.053158313035964966 + 2.0 * 6.040489673614502
Epoch 980, val loss: 0.9828874468803406
Epoch 990, training loss: 12.129095077514648 = 0.051135748624801636 + 2.0 * 6.038979530334473
Epoch 990, val loss: 0.9881038069725037
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 730, in <module>
    torch.cuda.empty_cache()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/cuda/memory.py", line 121, in empty_cache
    torch._C._cuda_emptyCache()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.4 der2:0.2 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 18.68960952758789 = 1.9419385194778442 + 2.0 * 8.373835563659668
Epoch 0, val loss: 1.9430675506591797
Epoch 10, training loss: 18.678756713867188 = 1.9322882890701294 + 2.0 * 8.373233795166016
Epoch 10, val loss: 1.9335907697677612
Epoch 20, training loss: 18.658864974975586 = 1.9201496839523315 + 2.0 * 8.36935806274414
Epoch 20, val loss: 1.921784520149231
Epoch 30, training loss: 18.591835021972656 = 1.9037444591522217 + 2.0 * 8.344045639038086
Epoch 30, val loss: 1.9060142040252686
Epoch 40, training loss: 18.239145278930664 = 1.883784532546997 + 2.0 * 8.177680015563965
Epoch 40, val loss: 1.8871995210647583
Epoch 50, training loss: 16.75786018371582 = 1.8601741790771484 + 2.0 * 7.448843002319336
Epoch 50, val loss: 1.864956021308899
Epoch 60, training loss: 16.04703712463379 = 1.841068148612976 + 2.0 * 7.10298490524292
Epoch 60, val loss: 1.8481558561325073
Epoch 70, training loss: 15.490683555603027 = 1.8272373676300049 + 2.0 * 6.831723213195801
Epoch 70, val loss: 1.8350598812103271
Epoch 80, training loss: 15.126958847045898 = 1.8143868446350098 + 2.0 * 6.656286239624023
Epoch 80, val loss: 1.8240153789520264
Epoch 90, training loss: 14.906084060668945 = 1.802192211151123 + 2.0 * 6.551945686340332
Epoch 90, val loss: 1.8131885528564453
Epoch 100, training loss: 14.762910842895508 = 1.788611888885498 + 2.0 * 6.487149715423584
Epoch 100, val loss: 1.8007982969284058
Epoch 110, training loss: 14.641937255859375 = 1.7746080160140991 + 2.0 * 6.433664798736572
Epoch 110, val loss: 1.787825345993042
Epoch 120, training loss: 14.55056381225586 = 1.7604037523269653 + 2.0 * 6.395080089569092
Epoch 120, val loss: 1.775022029876709
Epoch 130, training loss: 14.469533920288086 = 1.7456278800964355 + 2.0 * 6.361952781677246
Epoch 130, val loss: 1.761847734451294
Epoch 140, training loss: 14.396228790283203 = 1.729735016822815 + 2.0 * 6.33324670791626
Epoch 140, val loss: 1.747603416442871
Epoch 150, training loss: 14.332215309143066 = 1.7121316194534302 + 2.0 * 6.310041904449463
Epoch 150, val loss: 1.7318331003189087
Epoch 160, training loss: 14.271880149841309 = 1.6921701431274414 + 2.0 * 6.289855003356934
Epoch 160, val loss: 1.7141729593276978
Epoch 170, training loss: 14.209773063659668 = 1.6694183349609375 + 2.0 * 6.270177364349365
Epoch 170, val loss: 1.694577932357788
Epoch 180, training loss: 14.150713920593262 = 1.6435699462890625 + 2.0 * 6.2535719871521
Epoch 180, val loss: 1.672425627708435
Epoch 190, training loss: 14.092496871948242 = 1.6141754388809204 + 2.0 * 6.239160537719727
Epoch 190, val loss: 1.6472911834716797
Epoch 200, training loss: 14.034479141235352 = 1.581220030784607 + 2.0 * 6.226629734039307
Epoch 200, val loss: 1.6193605661392212
Epoch 210, training loss: 13.9725980758667 = 1.5442737340927124 + 2.0 * 6.214162349700928
Epoch 210, val loss: 1.588366985321045
Epoch 220, training loss: 13.910279273986816 = 1.5032216310501099 + 2.0 * 6.203528881072998
Epoch 220, val loss: 1.554137945175171
Epoch 230, training loss: 13.853315353393555 = 1.4584208726882935 + 2.0 * 6.197447299957275
Epoch 230, val loss: 1.517319917678833
Epoch 240, training loss: 13.78321647644043 = 1.411571741104126 + 2.0 * 6.185822486877441
Epoch 240, val loss: 1.4791858196258545
Epoch 250, training loss: 13.718891143798828 = 1.3631285429000854 + 2.0 * 6.177881240844727
Epoch 250, val loss: 1.4403237104415894
Epoch 260, training loss: 13.656936645507812 = 1.3137472867965698 + 2.0 * 6.171594619750977
Epoch 260, val loss: 1.4013395309448242
Epoch 270, training loss: 13.597270011901855 = 1.2648738622665405 + 2.0 * 6.166198253631592
Epoch 270, val loss: 1.3630619049072266
Epoch 280, training loss: 13.535894393920898 = 1.2167918682098389 + 2.0 * 6.15955114364624
Epoch 280, val loss: 1.3263273239135742
Epoch 290, training loss: 13.476019859313965 = 1.1696557998657227 + 2.0 * 6.153182029724121
Epoch 290, val loss: 1.290701985359192
Epoch 300, training loss: 13.420161247253418 = 1.1232963800430298 + 2.0 * 6.14843225479126
Epoch 300, val loss: 1.256144404411316
Epoch 310, training loss: 13.368853569030762 = 1.0775635242462158 + 2.0 * 6.1456451416015625
Epoch 310, val loss: 1.2223749160766602
Epoch 320, training loss: 13.310625076293945 = 1.0325562953948975 + 2.0 * 6.139034271240234
Epoch 320, val loss: 1.189679741859436
Epoch 330, training loss: 13.257230758666992 = 0.9882423281669617 + 2.0 * 6.134494304656982
Epoch 330, val loss: 1.15780770778656
Epoch 340, training loss: 13.207133293151855 = 0.9442970156669617 + 2.0 * 6.131418228149414
Epoch 340, val loss: 1.1266138553619385
Epoch 350, training loss: 13.16392993927002 = 0.9014419317245483 + 2.0 * 6.13124418258667
Epoch 350, val loss: 1.0960478782653809
Epoch 360, training loss: 13.108410835266113 = 0.859658420085907 + 2.0 * 6.12437629699707
Epoch 360, val loss: 1.066850185394287
Epoch 370, training loss: 13.05925464630127 = 0.8189283609390259 + 2.0 * 6.1201629638671875
Epoch 370, val loss: 1.0385276079177856
Epoch 380, training loss: 13.020959854125977 = 0.77940833568573 + 2.0 * 6.1207756996154785
Epoch 380, val loss: 1.01103675365448
Epoch 390, training loss: 12.971861839294434 = 0.7416870594024658 + 2.0 * 6.115087509155273
Epoch 390, val loss: 0.9847894906997681
Epoch 400, training loss: 12.928936004638672 = 0.7058501243591309 + 2.0 * 6.111542701721191
Epoch 400, val loss: 0.9601234793663025
Epoch 410, training loss: 12.89006233215332 = 0.6718624830245972 + 2.0 * 6.109099864959717
Epoch 410, val loss: 0.9368690848350525
Epoch 420, training loss: 12.854531288146973 = 0.6399258971214294 + 2.0 * 6.107302665710449
Epoch 420, val loss: 0.9152423143386841
Epoch 430, training loss: 12.817366600036621 = 0.6102349758148193 + 2.0 * 6.103565692901611
Epoch 430, val loss: 0.8956232070922852
Epoch 440, training loss: 12.783766746520996 = 0.5824534296989441 + 2.0 * 6.100656509399414
Epoch 440, val loss: 0.8777247071266174
Epoch 450, training loss: 12.753388404846191 = 0.5564157962799072 + 2.0 * 6.098486423492432
Epoch 450, val loss: 0.8613736033439636
Epoch 460, training loss: 12.729125022888184 = 0.5322712659835815 + 2.0 * 6.098426818847656
Epoch 460, val loss: 0.8465920686721802
Epoch 470, training loss: 12.700124740600586 = 0.5098885297775269 + 2.0 * 6.095118045806885
Epoch 470, val loss: 0.8337051272392273
Epoch 480, training loss: 12.67258358001709 = 0.4888927638530731 + 2.0 * 6.091845512390137
Epoch 480, val loss: 0.822223424911499
Epoch 490, training loss: 12.652131080627441 = 0.46902453899383545 + 2.0 * 6.091553211212158
Epoch 490, val loss: 0.8120682835578918
Epoch 500, training loss: 12.625962257385254 = 0.450271338224411 + 2.0 * 6.087845325469971
Epoch 500, val loss: 0.8030634522438049
Epoch 510, training loss: 12.60449504852295 = 0.43223822116851807 + 2.0 * 6.086128234863281
Epoch 510, val loss: 0.7951274514198303
Epoch 520, training loss: 12.581456184387207 = 0.41496407985687256 + 2.0 * 6.083246231079102
Epoch 520, val loss: 0.7881192564964294
Epoch 530, training loss: 12.56100082397461 = 0.3980659246444702 + 2.0 * 6.081467628479004
Epoch 530, val loss: 0.7819167375564575
Epoch 540, training loss: 12.547834396362305 = 0.38162073493003845 + 2.0 * 6.083106994628906
Epoch 540, val loss: 0.7762468457221985
Epoch 550, training loss: 12.523103713989258 = 0.36551323533058167 + 2.0 * 6.078795433044434
Epoch 550, val loss: 0.7713369131088257
Epoch 560, training loss: 12.501182556152344 = 0.3497123718261719 + 2.0 * 6.075735092163086
Epoch 560, val loss: 0.7669692039489746
Epoch 570, training loss: 12.487372398376465 = 0.3341273069381714 + 2.0 * 6.076622486114502
Epoch 570, val loss: 0.7630444169044495
Epoch 580, training loss: 12.467269897460938 = 0.31890082359313965 + 2.0 * 6.074184417724609
Epoch 580, val loss: 0.7595482468605042
Epoch 590, training loss: 12.444890975952148 = 0.3038925230503082 + 2.0 * 6.070499420166016
Epoch 590, val loss: 0.7565963268280029
Epoch 600, training loss: 12.429316520690918 = 0.28921273350715637 + 2.0 * 6.070051670074463
Epoch 600, val loss: 0.7540584802627563
Epoch 610, training loss: 12.411140441894531 = 0.27492743730545044 + 2.0 * 6.068106651306152
Epoch 610, val loss: 0.7519251704216003
Epoch 620, training loss: 12.396782875061035 = 0.2611548900604248 + 2.0 * 6.067813873291016
Epoch 620, val loss: 0.7502458691596985
Epoch 630, training loss: 12.381891250610352 = 0.24789437651634216 + 2.0 * 6.066998481750488
Epoch 630, val loss: 0.7490565776824951
Epoch 640, training loss: 12.362533569335938 = 0.23521685600280762 + 2.0 * 6.063658237457275
Epoch 640, val loss: 0.748261034488678
Epoch 650, training loss: 12.347846984863281 = 0.22308407723903656 + 2.0 * 6.062381267547607
Epoch 650, val loss: 0.7480301260948181
Epoch 660, training loss: 12.33754825592041 = 0.2115383893251419 + 2.0 * 6.063004970550537
Epoch 660, val loss: 0.7482520937919617
Epoch 670, training loss: 12.326091766357422 = 0.20070624351501465 + 2.0 * 6.062692642211914
Epoch 670, val loss: 0.748866856098175
Epoch 680, training loss: 12.306360244750977 = 0.1905106157064438 + 2.0 * 6.057924747467041
Epoch 680, val loss: 0.7500444650650024
Epoch 690, training loss: 12.295385360717773 = 0.18088103830814362 + 2.0 * 6.057251930236816
Epoch 690, val loss: 0.7516942024230957
Epoch 700, training loss: 12.29780387878418 = 0.17182059586048126 + 2.0 * 6.062991619110107
Epoch 700, val loss: 0.753764808177948
Epoch 710, training loss: 12.274898529052734 = 0.16330763697624207 + 2.0 * 6.055795669555664
Epoch 710, val loss: 0.7560535073280334
Epoch 720, training loss: 12.266193389892578 = 0.15536953508853912 + 2.0 * 6.0554118156433105
Epoch 720, val loss: 0.7587932348251343
Epoch 730, training loss: 12.25271987915039 = 0.14791148900985718 + 2.0 * 6.052404403686523
Epoch 730, val loss: 0.7618778347969055
Epoch 740, training loss: 12.247879981994629 = 0.14088107645511627 + 2.0 * 6.053499221801758
Epoch 740, val loss: 0.7653270959854126
Epoch 750, training loss: 12.243515014648438 = 0.13428455591201782 + 2.0 * 6.054615020751953
Epoch 750, val loss: 0.7690019607543945
Epoch 760, training loss: 12.230664253234863 = 0.12813115119934082 + 2.0 * 6.051266670227051
Epoch 760, val loss: 0.7728458046913147
Epoch 770, training loss: 12.218503952026367 = 0.12229986488819122 + 2.0 * 6.048101902008057
Epoch 770, val loss: 0.7770235538482666
Epoch 780, training loss: 12.213358879089355 = 0.11679448932409286 + 2.0 * 6.048282146453857
Epoch 780, val loss: 0.7814692258834839
Epoch 790, training loss: 12.205143928527832 = 0.11164513975381851 + 2.0 * 6.046749591827393
Epoch 790, val loss: 0.7858719229698181
Epoch 800, training loss: 12.2018461227417 = 0.10679751634597778 + 2.0 * 6.047524452209473
Epoch 800, val loss: 0.7905387878417969
Epoch 810, training loss: 12.193119049072266 = 0.10222975164651871 + 2.0 * 6.045444488525391
Epoch 810, val loss: 0.7953401207923889
Epoch 820, training loss: 12.184224128723145 = 0.09789839386940002 + 2.0 * 6.043162822723389
Epoch 820, val loss: 0.800326943397522
Epoch 830, training loss: 12.190563201904297 = 0.09381333738565445 + 2.0 * 6.048375129699707
Epoch 830, val loss: 0.805435836315155
Epoch 840, training loss: 12.182072639465332 = 0.0899435356259346 + 2.0 * 6.046064376831055
Epoch 840, val loss: 0.8106837868690491
Epoch 850, training loss: 12.171743392944336 = 0.08630531281232834 + 2.0 * 6.042718887329102
Epoch 850, val loss: 0.816035807132721
Epoch 860, training loss: 12.164091110229492 = 0.08284526318311691 + 2.0 * 6.040622711181641
Epoch 860, val loss: 0.8214266300201416
Epoch 870, training loss: 12.158600807189941 = 0.0795634537935257 + 2.0 * 6.0395188331604
Epoch 870, val loss: 0.8269409537315369
Epoch 880, training loss: 12.160847663879395 = 0.07643741369247437 + 2.0 * 6.042205333709717
Epoch 880, val loss: 0.8325595259666443
Epoch 890, training loss: 12.156254768371582 = 0.07349561899900436 + 2.0 * 6.041379451751709
Epoch 890, val loss: 0.8381896615028381
Epoch 900, training loss: 12.14893913269043 = 0.07068146020174026 + 2.0 * 6.03912878036499
Epoch 900, val loss: 0.8438794612884521
Epoch 910, training loss: 12.142948150634766 = 0.06801769137382507 + 2.0 * 6.0374650955200195
Epoch 910, val loss: 0.8496133685112
Epoch 920, training loss: 12.136290550231934 = 0.06546847522258759 + 2.0 * 6.0354108810424805
Epoch 920, val loss: 0.8553692102432251
Epoch 930, training loss: 12.135016441345215 = 0.0630418211221695 + 2.0 * 6.035987377166748
Epoch 930, val loss: 0.8611997365951538
Epoch 940, training loss: 12.136784553527832 = 0.0607287771999836 + 2.0 * 6.038027763366699
Epoch 940, val loss: 0.8670172095298767
Epoch 950, training loss: 12.126985549926758 = 0.05855004861950874 + 2.0 * 6.034217834472656
Epoch 950, val loss: 0.8727578520774841
Epoch 960, training loss: 12.123942375183105 = 0.0564618818461895 + 2.0 * 6.033740043640137
Epoch 960, val loss: 0.8786462545394897
Epoch 970, training loss: 12.118432998657227 = 0.05447624623775482 + 2.0 * 6.031978607177734
Epoch 970, val loss: 0.8845425844192505
Epoch 980, training loss: 12.119388580322266 = 0.05257248878479004 + 2.0 * 6.033408164978027
Epoch 980, val loss: 0.8904257416725159
Epoch 990, training loss: 12.114360809326172 = 0.05075076222419739 + 2.0 * 6.031805038452148
Epoch 990, val loss: 0.8962195515632629
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 730, in <module>
    torch.cuda.empty_cache()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/cuda/memory.py", line 121, in empty_cache
    torch._C._cuda_emptyCache()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.4 der2:0.2 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 18.681522369384766 = 1.9337040185928345 + 2.0 * 8.373908996582031
Epoch 0, val loss: 1.9280297756195068
Epoch 10, training loss: 18.670991897583008 = 1.924009919166565 + 2.0 * 8.373491287231445
Epoch 10, val loss: 1.9190988540649414
Epoch 20, training loss: 18.653038024902344 = 1.912074089050293 + 2.0 * 8.370482444763184
Epoch 20, val loss: 1.907760739326477
Epoch 30, training loss: 18.59518814086914 = 1.8957836627960205 + 2.0 * 8.349701881408691
Epoch 30, val loss: 1.8920681476593018
Epoch 40, training loss: 18.306983947753906 = 1.875542402267456 + 2.0 * 8.215721130371094
Epoch 40, val loss: 1.873400092124939
Epoch 50, training loss: 16.98248291015625 = 1.8526558876037598 + 2.0 * 7.564913272857666
Epoch 50, val loss: 1.8531098365783691
Epoch 60, training loss: 16.37281036376953 = 1.8327049016952515 + 2.0 * 7.270052909851074
Epoch 60, val loss: 1.8361093997955322
Epoch 70, training loss: 15.981546401977539 = 1.8172173500061035 + 2.0 * 7.082164764404297
Epoch 70, val loss: 1.8228553533554077
Epoch 80, training loss: 15.555057525634766 = 1.8018746376037598 + 2.0 * 6.876591682434082
Epoch 80, val loss: 1.809606909751892
Epoch 90, training loss: 15.215665817260742 = 1.7877826690673828 + 2.0 * 6.71394157409668
Epoch 90, val loss: 1.7973576784133911
Epoch 100, training loss: 14.969361305236816 = 1.774279236793518 + 2.0 * 6.597540855407715
Epoch 100, val loss: 1.7852163314819336
Epoch 110, training loss: 14.790051460266113 = 1.759817123413086 + 2.0 * 6.515117168426514
Epoch 110, val loss: 1.772141933441162
Epoch 120, training loss: 14.655370712280273 = 1.7440255880355835 + 2.0 * 6.455672740936279
Epoch 120, val loss: 1.758468508720398
Epoch 130, training loss: 14.54455280303955 = 1.7270129919052124 + 2.0 * 6.4087700843811035
Epoch 130, val loss: 1.7437490224838257
Epoch 140, training loss: 14.456799507141113 = 1.707890272140503 + 2.0 * 6.374454498291016
Epoch 140, val loss: 1.726917028427124
Epoch 150, training loss: 14.383298873901367 = 1.685776948928833 + 2.0 * 6.348761081695557
Epoch 150, val loss: 1.707350492477417
Epoch 160, training loss: 14.314709663391113 = 1.6604214906692505 + 2.0 * 6.327144145965576
Epoch 160, val loss: 1.6851896047592163
Epoch 170, training loss: 14.249260902404785 = 1.631718397140503 + 2.0 * 6.308771133422852
Epoch 170, val loss: 1.660353660583496
Epoch 180, training loss: 14.190276145935059 = 1.5996910333633423 + 2.0 * 6.295292377471924
Epoch 180, val loss: 1.6328041553497314
Epoch 190, training loss: 14.119885444641113 = 1.564144492149353 + 2.0 * 6.2778706550598145
Epoch 190, val loss: 1.6025553941726685
Epoch 200, training loss: 14.051819801330566 = 1.5248950719833374 + 2.0 * 6.263462543487549
Epoch 200, val loss: 1.5691609382629395
Epoch 210, training loss: 13.98396110534668 = 1.482311725616455 + 2.0 * 6.250824928283691
Epoch 210, val loss: 1.5332947969436646
Epoch 220, training loss: 13.922155380249023 = 1.4367855787277222 + 2.0 * 6.242684841156006
Epoch 220, val loss: 1.495387077331543
Epoch 230, training loss: 13.849337577819824 = 1.3892821073532104 + 2.0 * 6.230027675628662
Epoch 230, val loss: 1.4561659097671509
Epoch 240, training loss: 13.77974796295166 = 1.3400895595550537 + 2.0 * 6.219829082489014
Epoch 240, val loss: 1.4162036180496216
Epoch 250, training loss: 13.715563774108887 = 1.2901115417480469 + 2.0 * 6.21272611618042
Epoch 250, val loss: 1.3764772415161133
Epoch 260, training loss: 13.648200035095215 = 1.2406054735183716 + 2.0 * 6.203797340393066
Epoch 260, val loss: 1.337546706199646
Epoch 270, training loss: 13.584230422973633 = 1.1915885210037231 + 2.0 * 6.1963210105896
Epoch 270, val loss: 1.2995127439498901
Epoch 280, training loss: 13.536981582641602 = 1.1431498527526855 + 2.0 * 6.196915626525879
Epoch 280, val loss: 1.2623658180236816
Epoch 290, training loss: 13.469289779663086 = 1.0961416959762573 + 2.0 * 6.1865739822387695
Epoch 290, val loss: 1.2267539501190186
Epoch 300, training loss: 13.405733108520508 = 1.050319790840149 + 2.0 * 6.177706718444824
Epoch 300, val loss: 1.1923847198486328
Epoch 310, training loss: 13.358118057250977 = 1.00556218624115 + 2.0 * 6.176278114318848
Epoch 310, val loss: 1.1588670015335083
Epoch 320, training loss: 13.29711627960205 = 0.9618751406669617 + 2.0 * 6.167620658874512
Epoch 320, val loss: 1.126516580581665
Epoch 330, training loss: 13.2434663772583 = 0.9193829298019409 + 2.0 * 6.162041664123535
Epoch 330, val loss: 1.0951008796691895
Epoch 340, training loss: 13.192070007324219 = 0.8777917623519897 + 2.0 * 6.157139301300049
Epoch 340, val loss: 1.0645127296447754
Epoch 350, training loss: 13.148810386657715 = 0.8370444774627686 + 2.0 * 6.155882835388184
Epoch 350, val loss: 1.034730315208435
Epoch 360, training loss: 13.09614086151123 = 0.7976109385490417 + 2.0 * 6.149264812469482
Epoch 360, val loss: 1.0060654878616333
Epoch 370, training loss: 13.048957824707031 = 0.7596108317375183 + 2.0 * 6.1446733474731445
Epoch 370, val loss: 0.9787442684173584
Epoch 380, training loss: 13.014995574951172 = 0.7230163216590881 + 2.0 * 6.145989418029785
Epoch 380, val loss: 0.9528632164001465
Epoch 390, training loss: 12.962576866149902 = 0.6883733868598938 + 2.0 * 6.137101650238037
Epoch 390, val loss: 0.9285978674888611
Epoch 400, training loss: 12.922063827514648 = 0.6554465889930725 + 2.0 * 6.133308410644531
Epoch 400, val loss: 0.9060636162757874
Epoch 410, training loss: 12.887605667114258 = 0.6243962049484253 + 2.0 * 6.1316046714782715
Epoch 410, val loss: 0.8853642344474792
Epoch 420, training loss: 12.857239723205566 = 0.5953220725059509 + 2.0 * 6.1309590339660645
Epoch 420, val loss: 0.8666479587554932
Epoch 430, training loss: 12.817988395690918 = 0.5682264566421509 + 2.0 * 6.124880790710449
Epoch 430, val loss: 0.84979248046875
Epoch 440, training loss: 12.784988403320312 = 0.5428223609924316 + 2.0 * 6.121082782745361
Epoch 440, val loss: 0.8346824049949646
Epoch 450, training loss: 12.755510330200195 = 0.5188905596733093 + 2.0 * 6.11830997467041
Epoch 450, val loss: 0.8212426900863647
Epoch 460, training loss: 12.751165390014648 = 0.4965115487575531 + 2.0 * 6.127326965332031
Epoch 460, val loss: 0.8091819286346436
Epoch 470, training loss: 12.701519966125488 = 0.47536683082580566 + 2.0 * 6.113076686859131
Epoch 470, val loss: 0.7986869812011719
Epoch 480, training loss: 12.677468299865723 = 0.4553312063217163 + 2.0 * 6.1110687255859375
Epoch 480, val loss: 0.7893823981285095
Epoch 490, training loss: 12.653107643127441 = 0.4361077547073364 + 2.0 * 6.108500003814697
Epoch 490, val loss: 0.781092643737793
Epoch 500, training loss: 12.628921508789062 = 0.4174197018146515 + 2.0 * 6.105751037597656
Epoch 500, val loss: 0.7737206220626831
Epoch 510, training loss: 12.613547325134277 = 0.3992001414299011 + 2.0 * 6.107173442840576
Epoch 510, val loss: 0.7671634554862976
Epoch 520, training loss: 12.591938972473145 = 0.38137659430503845 + 2.0 * 6.105281352996826
Epoch 520, val loss: 0.7613555788993835
Epoch 530, training loss: 12.565043449401855 = 0.3640318512916565 + 2.0 * 6.100505828857422
Epoch 530, val loss: 0.7563279867172241
Epoch 540, training loss: 12.542479515075684 = 0.34701743721961975 + 2.0 * 6.097731113433838
Epoch 540, val loss: 0.7520047426223755
Epoch 550, training loss: 12.535482406616211 = 0.33031532168388367 + 2.0 * 6.102583408355713
Epoch 550, val loss: 0.7483930587768555
Epoch 560, training loss: 12.505902290344238 = 0.31400755047798157 + 2.0 * 6.095947265625
Epoch 560, val loss: 0.7453692555427551
Epoch 570, training loss: 12.483658790588379 = 0.2981993854045868 + 2.0 * 6.092729568481445
Epoch 570, val loss: 0.7431065440177917
Epoch 580, training loss: 12.464859962463379 = 0.2828490138053894 + 2.0 * 6.091005325317383
Epoch 580, val loss: 0.7415217757225037
Epoch 590, training loss: 12.451868057250977 = 0.267988383769989 + 2.0 * 6.091939926147461
Epoch 590, val loss: 0.7405929565429688
Epoch 600, training loss: 12.428659439086914 = 0.2536609172821045 + 2.0 * 6.087499141693115
Epoch 600, val loss: 0.7403146624565125
Epoch 610, training loss: 12.410548210144043 = 0.23997190594673157 + 2.0 * 6.085288047790527
Epoch 610, val loss: 0.7406930923461914
Epoch 620, training loss: 12.392766952514648 = 0.22687676548957825 + 2.0 * 6.082944869995117
Epoch 620, val loss: 0.7416592240333557
Epoch 630, training loss: 12.388556480407715 = 0.21441714465618134 + 2.0 * 6.087069511413574
Epoch 630, val loss: 0.7431840896606445
Epoch 640, training loss: 12.368678092956543 = 0.2026273012161255 + 2.0 * 6.0830254554748535
Epoch 640, val loss: 0.7452172636985779
Epoch 650, training loss: 12.349119186401367 = 0.1916155368089676 + 2.0 * 6.078752040863037
Epoch 650, val loss: 0.7477656602859497
Epoch 660, training loss: 12.336560249328613 = 0.18123187124729156 + 2.0 * 6.077664375305176
Epoch 660, val loss: 0.7508829832077026
Epoch 670, training loss: 12.322997093200684 = 0.171426922082901 + 2.0 * 6.075785160064697
Epoch 670, val loss: 0.7545068860054016
Epoch 680, training loss: 12.311294555664062 = 0.1621667593717575 + 2.0 * 6.074563980102539
Epoch 680, val loss: 0.7586081624031067
Epoch 690, training loss: 12.302982330322266 = 0.1534791737794876 + 2.0 * 6.074751377105713
Epoch 690, val loss: 0.7631211280822754
Epoch 700, training loss: 12.292473793029785 = 0.14536665380001068 + 2.0 * 6.073553562164307
Epoch 700, val loss: 0.7679736018180847
Epoch 710, training loss: 12.279723167419434 = 0.13778533041477203 + 2.0 * 6.070969104766846
Epoch 710, val loss: 0.7732744216918945
Epoch 720, training loss: 12.26952075958252 = 0.13066349923610687 + 2.0 * 6.069428443908691
Epoch 720, val loss: 0.7789119482040405
Epoch 730, training loss: 12.268084526062012 = 0.12398792058229446 + 2.0 * 6.072048187255859
Epoch 730, val loss: 0.7849187254905701
Epoch 740, training loss: 12.253424644470215 = 0.11777134239673615 + 2.0 * 6.067826747894287
Epoch 740, val loss: 0.7911295294761658
Epoch 750, training loss: 12.242759704589844 = 0.11194699257612228 + 2.0 * 6.065406322479248
Epoch 750, val loss: 0.7976809740066528
Epoch 760, training loss: 12.243817329406738 = 0.10650423914194107 + 2.0 * 6.0686564445495605
Epoch 760, val loss: 0.804449737071991
Epoch 770, training loss: 12.234703063964844 = 0.10138429701328278 + 2.0 * 6.066659450531006
Epoch 770, val loss: 0.8113888502120972
Epoch 780, training loss: 12.2221097946167 = 0.09662159532308578 + 2.0 * 6.062744140625
Epoch 780, val loss: 0.8185611367225647
Epoch 790, training loss: 12.213899612426758 = 0.0921478420495987 + 2.0 * 6.06087589263916
Epoch 790, val loss: 0.8259314894676208
Epoch 800, training loss: 12.207314491271973 = 0.08793891966342926 + 2.0 * 6.059687614440918
Epoch 800, val loss: 0.8334560394287109
Epoch 810, training loss: 12.226000785827637 = 0.08396653085947037 + 2.0 * 6.071017265319824
Epoch 810, val loss: 0.8410400152206421
Epoch 820, training loss: 12.20026683807373 = 0.08027740567922592 + 2.0 * 6.059994697570801
Epoch 820, val loss: 0.8486395478248596
Epoch 830, training loss: 12.192296981811523 = 0.07679613679647446 + 2.0 * 6.057750225067139
Epoch 830, val loss: 0.8563525080680847
Epoch 840, training loss: 12.185050964355469 = 0.07353715598583221 + 2.0 * 6.05575704574585
Epoch 840, val loss: 0.8641592264175415
Epoch 850, training loss: 12.184639930725098 = 0.07045888155698776 + 2.0 * 6.057090759277344
Epoch 850, val loss: 0.8720451593399048
Epoch 860, training loss: 12.175514221191406 = 0.06754355877637863 + 2.0 * 6.053985118865967
Epoch 860, val loss: 0.8798384070396423
Epoch 870, training loss: 12.173030853271484 = 0.06478622555732727 + 2.0 * 6.054122447967529
Epoch 870, val loss: 0.8877418637275696
Epoch 880, training loss: 12.168811798095703 = 0.062181465327739716 + 2.0 * 6.053315162658691
Epoch 880, val loss: 0.8956618309020996
Epoch 890, training loss: 12.162276268005371 = 0.05972570925951004 + 2.0 * 6.051275253295898
Epoch 890, val loss: 0.9035183191299438
Epoch 900, training loss: 12.161120414733887 = 0.05740584433078766 + 2.0 * 6.0518574714660645
Epoch 900, val loss: 0.9114505052566528
Epoch 910, training loss: 12.154383659362793 = 0.05521458759903908 + 2.0 * 6.04958438873291
Epoch 910, val loss: 0.9193646907806396
Epoch 920, training loss: 12.164475440979004 = 0.05314212664961815 + 2.0 * 6.055666446685791
Epoch 920, val loss: 0.9271987080574036
Epoch 930, training loss: 12.155645370483398 = 0.05115639790892601 + 2.0 * 6.052244663238525
Epoch 930, val loss: 0.9348613023757935
Epoch 940, training loss: 12.148144721984863 = 0.049295540899038315 + 2.0 * 6.049424648284912
Epoch 940, val loss: 0.9426401257514954
Epoch 950, training loss: 12.14091968536377 = 0.04752541333436966 + 2.0 * 6.04669713973999
Epoch 950, val loss: 0.9503432512283325
Epoch 960, training loss: 12.136577606201172 = 0.04584481567144394 + 2.0 * 6.045366287231445
Epoch 960, val loss: 0.9580011367797852
Epoch 970, training loss: 12.138190269470215 = 0.044244274497032166 + 2.0 * 6.04697322845459
Epoch 970, val loss: 0.9655765295028687
Epoch 980, training loss: 12.138091087341309 = 0.04272367060184479 + 2.0 * 6.0476837158203125
Epoch 980, val loss: 0.973122239112854
Epoch 990, training loss: 12.135003089904785 = 0.04127158969640732 + 2.0 * 6.046865940093994
Epoch 990, val loss: 0.9805892109870911
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 730, in <module>
    torch.cuda.empty_cache()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/cuda/memory.py", line 121, in empty_cache
    torch._C._cuda_emptyCache()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.4 der2:0.3 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.3, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 18.690914154052734 = 1.9433670043945312 + 2.0 * 8.373773574829102
Epoch 0, val loss: 1.9362159967422485
Epoch 10, training loss: 18.679594039916992 = 1.9335778951644897 + 2.0 * 8.373007774353027
Epoch 10, val loss: 1.9272058010101318
Epoch 20, training loss: 18.65822410583496 = 1.9215419292449951 + 2.0 * 8.368341445922852
Epoch 20, val loss: 1.91556715965271
Epoch 30, training loss: 18.583219528198242 = 1.9055562019348145 + 2.0 * 8.338831901550293
Epoch 30, val loss: 1.8996152877807617
Epoch 40, training loss: 18.164794921875 = 1.8873058557510376 + 2.0 * 8.138744354248047
Epoch 40, val loss: 1.8821899890899658
Epoch 50, training loss: 16.610456466674805 = 1.8672339916229248 + 2.0 * 7.371611595153809
Epoch 50, val loss: 1.8622773885726929
Epoch 60, training loss: 15.930865287780762 = 1.8524442911148071 + 2.0 * 7.039210319519043
Epoch 60, val loss: 1.8489501476287842
Epoch 70, training loss: 15.451668739318848 = 1.839860200881958 + 2.0 * 6.805904388427734
Epoch 70, val loss: 1.837493658065796
Epoch 80, training loss: 15.121039390563965 = 1.828182578086853 + 2.0 * 6.64642858505249
Epoch 80, val loss: 1.8269201517105103
Epoch 90, training loss: 14.907906532287598 = 1.8186014890670776 + 2.0 * 6.544652462005615
Epoch 90, val loss: 1.8185011148452759
Epoch 100, training loss: 14.704594612121582 = 1.8105695247650146 + 2.0 * 6.447012424468994
Epoch 100, val loss: 1.811489224433899
Epoch 110, training loss: 14.57072639465332 = 1.803985357284546 + 2.0 * 6.383370399475098
Epoch 110, val loss: 1.805478811264038
Epoch 120, training loss: 14.482404708862305 = 1.7969937324523926 + 2.0 * 6.342705249786377
Epoch 120, val loss: 1.7988721132278442
Epoch 130, training loss: 14.409873962402344 = 1.7895575761795044 + 2.0 * 6.3101582527160645
Epoch 130, val loss: 1.7919549942016602
Epoch 140, training loss: 14.35043716430664 = 1.78225839138031 + 2.0 * 6.2840895652771
Epoch 140, val loss: 1.7853796482086182
Epoch 150, training loss: 14.298372268676758 = 1.7748461961746216 + 2.0 * 6.261763095855713
Epoch 150, val loss: 1.778908371925354
Epoch 160, training loss: 14.249166488647461 = 1.7668858766555786 + 2.0 * 6.241140365600586
Epoch 160, val loss: 1.772021770477295
Epoch 170, training loss: 14.206006050109863 = 1.7579716444015503 + 2.0 * 6.224017143249512
Epoch 170, val loss: 1.7644805908203125
Epoch 180, training loss: 14.166352272033691 = 1.7478338479995728 + 2.0 * 6.209259033203125
Epoch 180, val loss: 1.7561180591583252
Epoch 190, training loss: 14.127111434936523 = 1.7362581491470337 + 2.0 * 6.1954264640808105
Epoch 190, val loss: 1.746604084968567
Epoch 200, training loss: 14.090558052062988 = 1.7229363918304443 + 2.0 * 6.183810710906982
Epoch 200, val loss: 1.735766887664795
Epoch 210, training loss: 14.055447578430176 = 1.7074064016342163 + 2.0 * 6.174020767211914
Epoch 210, val loss: 1.723215103149414
Epoch 220, training loss: 14.022850036621094 = 1.6893696784973145 + 2.0 * 6.1667399406433105
Epoch 220, val loss: 1.7085015773773193
Epoch 230, training loss: 13.98361587524414 = 1.6683169603347778 + 2.0 * 6.157649517059326
Epoch 230, val loss: 1.6916289329528809
Epoch 240, training loss: 13.945377349853516 = 1.6437077522277832 + 2.0 * 6.150835037231445
Epoch 240, val loss: 1.6717289686203003
Epoch 250, training loss: 13.903566360473633 = 1.6147061586380005 + 2.0 * 6.144430160522461
Epoch 250, val loss: 1.6482353210449219
Epoch 260, training loss: 13.860926628112793 = 1.5804365873336792 + 2.0 * 6.140244960784912
Epoch 260, val loss: 1.6204402446746826
Epoch 270, training loss: 13.808813095092773 = 1.5409226417541504 + 2.0 * 6.133945465087891
Epoch 270, val loss: 1.588286280632019
Epoch 280, training loss: 13.754860877990723 = 1.495642900466919 + 2.0 * 6.129609107971191
Epoch 280, val loss: 1.5514863729476929
Epoch 290, training loss: 13.695561408996582 = 1.444397211074829 + 2.0 * 6.125582218170166
Epoch 290, val loss: 1.509718418121338
Epoch 300, training loss: 13.638110160827637 = 1.3880250453948975 + 2.0 * 6.12504243850708
Epoch 300, val loss: 1.463896632194519
Epoch 310, training loss: 13.566974639892578 = 1.3289419412612915 + 2.0 * 6.119016170501709
Epoch 310, val loss: 1.4164869785308838
Epoch 320, training loss: 13.501022338867188 = 1.268494963645935 + 2.0 * 6.1162638664245605
Epoch 320, val loss: 1.3680912256240845
Epoch 330, training loss: 13.433220863342285 = 1.2075330018997192 + 2.0 * 6.112843990325928
Epoch 330, val loss: 1.319539189338684
Epoch 340, training loss: 13.367359161376953 = 1.1474500894546509 + 2.0 * 6.109954357147217
Epoch 340, val loss: 1.2723215818405151
Epoch 350, training loss: 13.306897163391113 = 1.0898691415786743 + 2.0 * 6.108513832092285
Epoch 350, val loss: 1.227805256843567
Epoch 360, training loss: 13.24936294555664 = 1.0364741086959839 + 2.0 * 6.106444358825684
Epoch 360, val loss: 1.1870558261871338
Epoch 370, training loss: 13.189860343933105 = 0.986857533454895 + 2.0 * 6.10150146484375
Epoch 370, val loss: 1.149657964706421
Epoch 380, training loss: 13.139605522155762 = 0.9405294060707092 + 2.0 * 6.0995378494262695
Epoch 380, val loss: 1.1152169704437256
Epoch 390, training loss: 13.091657638549805 = 0.897165834903717 + 2.0 * 6.097245693206787
Epoch 390, val loss: 1.0836901664733887
Epoch 400, training loss: 13.045982360839844 = 0.8569959998130798 + 2.0 * 6.094493389129639
Epoch 400, val loss: 1.0545525550842285
Epoch 410, training loss: 13.002010345458984 = 0.8193166851997375 + 2.0 * 6.091346740722656
Epoch 410, val loss: 1.027837872505188
Epoch 420, training loss: 12.960742950439453 = 0.7835284471511841 + 2.0 * 6.088607311248779
Epoch 420, val loss: 1.0026116371154785
Epoch 430, training loss: 12.921658515930176 = 0.7492562532424927 + 2.0 * 6.086201190948486
Epoch 430, val loss: 0.9785131216049194
Epoch 440, training loss: 12.884650230407715 = 0.7163485288619995 + 2.0 * 6.084150791168213
Epoch 440, val loss: 0.9556875824928284
Epoch 450, training loss: 12.848377227783203 = 0.6844106912612915 + 2.0 * 6.0819830894470215
Epoch 450, val loss: 0.9334957599639893
Epoch 460, training loss: 12.81674861907959 = 0.653384268283844 + 2.0 * 6.081682205200195
Epoch 460, val loss: 0.9119155406951904
Epoch 470, training loss: 12.778942108154297 = 0.6234803199768066 + 2.0 * 6.077730655670166
Epoch 470, val loss: 0.8912858366966248
Epoch 480, training loss: 12.747802734375 = 0.5943178534507751 + 2.0 * 6.076742649078369
Epoch 480, val loss: 0.8713136911392212
Epoch 490, training loss: 12.715795516967773 = 0.566246747970581 + 2.0 * 6.074774265289307
Epoch 490, val loss: 0.8520156741142273
Epoch 500, training loss: 12.683934211730957 = 0.5390568375587463 + 2.0 * 6.072438716888428
Epoch 500, val loss: 0.8338814377784729
Epoch 510, training loss: 12.656987190246582 = 0.5128874182701111 + 2.0 * 6.072050094604492
Epoch 510, val loss: 0.816687285900116
Epoch 520, training loss: 12.627617835998535 = 0.4878922998905182 + 2.0 * 6.0698628425598145
Epoch 520, val loss: 0.8007523417472839
Epoch 530, training loss: 12.59919548034668 = 0.46387654542922974 + 2.0 * 6.067659378051758
Epoch 530, val loss: 0.7860197424888611
Epoch 540, training loss: 12.57735824584961 = 0.44069740176200867 + 2.0 * 6.06833028793335
Epoch 540, val loss: 0.7725246548652649
Epoch 550, training loss: 12.555264472961426 = 0.4186568856239319 + 2.0 * 6.06830358505249
Epoch 550, val loss: 0.7602545022964478
Epoch 560, training loss: 12.524044036865234 = 0.3975847363471985 + 2.0 * 6.063229560852051
Epoch 560, val loss: 0.7494358420372009
Epoch 570, training loss: 12.498326301574707 = 0.3772578835487366 + 2.0 * 6.0605340003967285
Epoch 570, val loss: 0.7397915720939636
Epoch 580, training loss: 12.487071990966797 = 0.3576538562774658 + 2.0 * 6.064709186553955
Epoch 580, val loss: 0.7312594652175903
Epoch 590, training loss: 12.466297149658203 = 0.3391021192073822 + 2.0 * 6.063597679138184
Epoch 590, val loss: 0.7237527370452881
Epoch 600, training loss: 12.43923282623291 = 0.32131922245025635 + 2.0 * 6.058956623077393
Epoch 600, val loss: 0.7174651026725769
Epoch 610, training loss: 12.418871879577637 = 0.30427393317222595 + 2.0 * 6.0572991371154785
Epoch 610, val loss: 0.7119935154914856
Epoch 620, training loss: 12.398177146911621 = 0.2879311144351959 + 2.0 * 6.0551228523254395
Epoch 620, val loss: 0.7074175477027893
Epoch 630, training loss: 12.38151741027832 = 0.27233386039733887 + 2.0 * 6.054591655731201
Epoch 630, val loss: 0.7035906314849854
Epoch 640, training loss: 12.36209487915039 = 0.2575130760669708 + 2.0 * 6.052290916442871
Epoch 640, val loss: 0.7006664276123047
Epoch 650, training loss: 12.349626541137695 = 0.24340800940990448 + 2.0 * 6.053109169006348
Epoch 650, val loss: 0.6984766125679016
Epoch 660, training loss: 12.334939002990723 = 0.23007707297801971 + 2.0 * 6.052431106567383
Epoch 660, val loss: 0.6969998478889465
Epoch 670, training loss: 12.31675910949707 = 0.2175833284854889 + 2.0 * 6.049587726593018
Epoch 670, val loss: 0.6963787078857422
Epoch 680, training loss: 12.311155319213867 = 0.20581243932247162 + 2.0 * 6.052671432495117
Epoch 680, val loss: 0.6963353753089905
Epoch 690, training loss: 12.288290023803711 = 0.1948317289352417 + 2.0 * 6.04672908782959
Epoch 690, val loss: 0.6970707178115845
Epoch 700, training loss: 12.275419235229492 = 0.1844547539949417 + 2.0 * 6.045482158660889
Epoch 700, val loss: 0.6984442472457886
Epoch 710, training loss: 12.270995140075684 = 0.17469647526741028 + 2.0 * 6.048149108886719
Epoch 710, val loss: 0.7004019021987915
Epoch 720, training loss: 12.263871192932129 = 0.16565918922424316 + 2.0 * 6.049106121063232
Epoch 720, val loss: 0.702747642993927
Epoch 730, training loss: 12.245136260986328 = 0.15724152326583862 + 2.0 * 6.043947219848633
Epoch 730, val loss: 0.7057507634162903
Epoch 740, training loss: 12.231675148010254 = 0.14934562146663666 + 2.0 * 6.041164875030518
Epoch 740, val loss: 0.7091222405433655
Epoch 750, training loss: 12.238728523254395 = 0.14189855754375458 + 2.0 * 6.048415184020996
Epoch 750, val loss: 0.7128294110298157
Epoch 760, training loss: 12.215925216674805 = 0.13502933084964752 + 2.0 * 6.04044771194458
Epoch 760, val loss: 0.7168940305709839
Epoch 770, training loss: 12.207751274108887 = 0.12857350707054138 + 2.0 * 6.039588928222656
Epoch 770, val loss: 0.7213870882987976
Epoch 780, training loss: 12.198277473449707 = 0.1224786788225174 + 2.0 * 6.037899494171143
Epoch 780, val loss: 0.7261132597923279
Epoch 790, training loss: 12.202909469604492 = 0.11675067245960236 + 2.0 * 6.043079376220703
Epoch 790, val loss: 0.7310634851455688
Epoch 800, training loss: 12.193388938903809 = 0.1113935336470604 + 2.0 * 6.040997505187988
Epoch 800, val loss: 0.7360920906066895
Epoch 810, training loss: 12.180449485778809 = 0.10641682147979736 + 2.0 * 6.03701639175415
Epoch 810, val loss: 0.7414301037788391
Epoch 820, training loss: 12.170262336730957 = 0.10171680897474289 + 2.0 * 6.03427267074585
Epoch 820, val loss: 0.7469201683998108
Epoch 830, training loss: 12.165069580078125 = 0.09725767374038696 + 2.0 * 6.033905982971191
Epoch 830, val loss: 0.7525035738945007
Epoch 840, training loss: 12.173236846923828 = 0.09306547045707703 + 2.0 * 6.040085792541504
Epoch 840, val loss: 0.7581230998039246
Epoch 850, training loss: 12.156732559204102 = 0.08911441266536713 + 2.0 * 6.033809185028076
Epoch 850, val loss: 0.7638962864875793
Epoch 860, training loss: 12.147256851196289 = 0.08539200574159622 + 2.0 * 6.030932426452637
Epoch 860, val loss: 0.7698414921760559
Epoch 870, training loss: 12.143529891967773 = 0.08185503631830215 + 2.0 * 6.030837535858154
Epoch 870, val loss: 0.775795578956604
Epoch 880, training loss: 12.1612548828125 = 0.0785045176744461 + 2.0 * 6.041375160217285
Epoch 880, val loss: 0.7817404866218567
Epoch 890, training loss: 12.133732795715332 = 0.07536836713552475 + 2.0 * 6.029182434082031
Epoch 890, val loss: 0.7876482009887695
Epoch 900, training loss: 12.130437850952148 = 0.07240033149719238 + 2.0 * 6.029018878936768
Epoch 900, val loss: 0.7937285900115967
Epoch 910, training loss: 12.124977111816406 = 0.06957244873046875 + 2.0 * 6.027702331542969
Epoch 910, val loss: 0.799790620803833
Epoch 920, training loss: 12.126899719238281 = 0.06688375025987625 + 2.0 * 6.030007839202881
Epoch 920, val loss: 0.8059266805648804
Epoch 930, training loss: 12.126307487487793 = 0.06433884799480438 + 2.0 * 6.030984401702881
Epoch 930, val loss: 0.8118073344230652
Epoch 940, training loss: 12.115840911865234 = 0.06193840131163597 + 2.0 * 6.026951313018799
Epoch 940, val loss: 0.8178555369377136
Epoch 950, training loss: 12.109619140625 = 0.059662073850631714 + 2.0 * 6.0249786376953125
Epoch 950, val loss: 0.8238648176193237
Epoch 960, training loss: 12.107046127319336 = 0.05748775228857994 + 2.0 * 6.024779319763184
Epoch 960, val loss: 0.829811692237854
Epoch 970, training loss: 12.111721992492676 = 0.055418990552425385 + 2.0 * 6.028151512145996
Epoch 970, val loss: 0.8357719779014587
Epoch 980, training loss: 12.110276222229004 = 0.05344928056001663 + 2.0 * 6.02841329574585
Epoch 980, val loss: 0.8416545391082764
Epoch 990, training loss: 12.102721214294434 = 0.05159728601574898 + 2.0 * 6.025561809539795
Epoch 990, val loss: 0.847439169883728
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 730, in <module>
    torch.cuda.empty_cache()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/cuda/memory.py", line 121, in empty_cache
    torch._C._cuda_emptyCache()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.4 der2:0.3 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.3, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 18.693391799926758 = 1.9458043575286865 + 2.0 * 8.373793601989746
Epoch 0, val loss: 1.9392647743225098
Epoch 10, training loss: 18.681682586669922 = 1.9355828762054443 + 2.0 * 8.37304973602295
Epoch 10, val loss: 1.9302575588226318
Epoch 20, training loss: 18.65958023071289 = 1.9229521751403809 + 2.0 * 8.368313789367676
Epoch 20, val loss: 1.9188499450683594
Epoch 30, training loss: 18.579444885253906 = 1.9061100482940674 + 2.0 * 8.33666706085205
Epoch 30, val loss: 1.9035372734069824
Epoch 40, training loss: 18.121028900146484 = 1.8869253396987915 + 2.0 * 8.11705207824707
Epoch 40, val loss: 1.8862203359603882
Epoch 50, training loss: 16.9185848236084 = 1.8654810190200806 + 2.0 * 7.526551723480225
Epoch 50, val loss: 1.8667845726013184
Epoch 60, training loss: 16.107309341430664 = 1.8482601642608643 + 2.0 * 7.129524230957031
Epoch 60, val loss: 1.8525034189224243
Epoch 70, training loss: 15.36651611328125 = 1.8377727270126343 + 2.0 * 6.764371871948242
Epoch 70, val loss: 1.8436977863311768
Epoch 80, training loss: 14.981817245483398 = 1.829032063484192 + 2.0 * 6.576392650604248
Epoch 80, val loss: 1.8358752727508545
Epoch 90, training loss: 14.786110877990723 = 1.8199739456176758 + 2.0 * 6.483068466186523
Epoch 90, val loss: 1.8279907703399658
Epoch 100, training loss: 14.634124755859375 = 1.811144471168518 + 2.0 * 6.411489963531494
Epoch 100, val loss: 1.8205492496490479
Epoch 110, training loss: 14.518393516540527 = 1.8035008907318115 + 2.0 * 6.357446193695068
Epoch 110, val loss: 1.8137952089309692
Epoch 120, training loss: 14.432268142700195 = 1.7968907356262207 + 2.0 * 6.317688941955566
Epoch 120, val loss: 1.8071794509887695
Epoch 130, training loss: 14.363186836242676 = 1.7902575731277466 + 2.0 * 6.286464691162109
Epoch 130, val loss: 1.8002958297729492
Epoch 140, training loss: 14.305981636047363 = 1.7834731340408325 + 2.0 * 6.26125431060791
Epoch 140, val loss: 1.793463110923767
Epoch 150, training loss: 14.252108573913574 = 1.7765063047409058 + 2.0 * 6.2378010749816895
Epoch 150, val loss: 1.7868256568908691
Epoch 160, training loss: 14.205560684204102 = 1.7690777778625488 + 2.0 * 6.2182416915893555
Epoch 160, val loss: 1.780112862586975
Epoch 170, training loss: 14.164444923400879 = 1.7607702016830444 + 2.0 * 6.201837539672852
Epoch 170, val loss: 1.7728655338287354
Epoch 180, training loss: 14.1263427734375 = 1.7513774633407593 + 2.0 * 6.187482833862305
Epoch 180, val loss: 1.7649471759796143
Epoch 190, training loss: 14.092275619506836 = 1.7408294677734375 + 2.0 * 6.175723075866699
Epoch 190, val loss: 1.756229043006897
Epoch 200, training loss: 14.058880805969238 = 1.7288774251937866 + 2.0 * 6.16500186920166
Epoch 200, val loss: 1.7464605569839478
Epoch 210, training loss: 14.024163246154785 = 1.715158462524414 + 2.0 * 6.1545023918151855
Epoch 210, val loss: 1.7354507446289062
Epoch 220, training loss: 13.990863800048828 = 1.6993016004562378 + 2.0 * 6.14578104019165
Epoch 220, val loss: 1.7227870225906372
Epoch 230, training loss: 13.957754135131836 = 1.6807745695114136 + 2.0 * 6.138489723205566
Epoch 230, val loss: 1.7080063819885254
Epoch 240, training loss: 13.924095153808594 = 1.6592495441436768 + 2.0 * 6.132422924041748
Epoch 240, val loss: 1.6907967329025269
Epoch 250, training loss: 13.885377883911133 = 1.6342425346374512 + 2.0 * 6.125567436218262
Epoch 250, val loss: 1.6708123683929443
Epoch 260, training loss: 13.847600936889648 = 1.6051075458526611 + 2.0 * 6.121246814727783
Epoch 260, val loss: 1.6475486755371094
Epoch 270, training loss: 13.803519248962402 = 1.5714975595474243 + 2.0 * 6.116010665893555
Epoch 270, val loss: 1.620443344116211
Epoch 280, training loss: 13.759455680847168 = 1.532599687576294 + 2.0 * 6.113428115844727
Epoch 280, val loss: 1.5891472101211548
Epoch 290, training loss: 13.706158638000488 = 1.4893919229507446 + 2.0 * 6.1083831787109375
Epoch 290, val loss: 1.5542409420013428
Epoch 300, training loss: 13.653840065002441 = 1.4420878887176514 + 2.0 * 6.1058759689331055
Epoch 300, val loss: 1.5159133672714233
Epoch 310, training loss: 13.594514846801758 = 1.3915468454360962 + 2.0 * 6.1014838218688965
Epoch 310, val loss: 1.474916934967041
Epoch 320, training loss: 13.539607048034668 = 1.338314414024353 + 2.0 * 6.100646495819092
Epoch 320, val loss: 1.4321036338806152
Epoch 330, training loss: 13.477150917053223 = 1.2843259572982788 + 2.0 * 6.096412658691406
Epoch 330, val loss: 1.3889484405517578
Epoch 340, training loss: 13.417010307312012 = 1.2304214239120483 + 2.0 * 6.093294620513916
Epoch 340, val loss: 1.3461511135101318
Epoch 350, training loss: 13.365964889526367 = 1.1771807670593262 + 2.0 * 6.094391822814941
Epoch 350, val loss: 1.304412603378296
Epoch 360, training loss: 13.304410934448242 = 1.1262670755386353 + 2.0 * 6.089071750640869
Epoch 360, val loss: 1.2644003629684448
Epoch 370, training loss: 13.249189376831055 = 1.0773799419403076 + 2.0 * 6.085904598236084
Epoch 370, val loss: 1.2262096405029297
Epoch 380, training loss: 13.205901145935059 = 1.0306700468063354 + 2.0 * 6.087615489959717
Epoch 380, val loss: 1.1899480819702148
Epoch 390, training loss: 13.151606559753418 = 0.9865856766700745 + 2.0 * 6.082510471343994
Epoch 390, val loss: 1.1561309099197388
Epoch 400, training loss: 13.103236198425293 = 0.9452600479125977 + 2.0 * 6.078988075256348
Epoch 400, val loss: 1.12452232837677
Epoch 410, training loss: 13.058571815490723 = 0.9058874845504761 + 2.0 * 6.0763421058654785
Epoch 410, val loss: 1.0947009325027466
Epoch 420, training loss: 13.027183532714844 = 0.868247926235199 + 2.0 * 6.0794677734375
Epoch 420, val loss: 1.066559910774231
Epoch 430, training loss: 12.980443000793457 = 0.8328211903572083 + 2.0 * 6.073811054229736
Epoch 430, val loss: 1.0401579141616821
Epoch 440, training loss: 12.94359302520752 = 0.7991526126861572 + 2.0 * 6.072220325469971
Epoch 440, val loss: 1.0158058404922485
Epoch 450, training loss: 12.904577255249023 = 0.7671899199485779 + 2.0 * 6.0686936378479
Epoch 450, val loss: 0.9929601550102234
Epoch 460, training loss: 12.869491577148438 = 0.7362130284309387 + 2.0 * 6.066639423370361
Epoch 460, val loss: 0.9713592529296875
Epoch 470, training loss: 12.835378646850586 = 0.7060260772705078 + 2.0 * 6.064676284790039
Epoch 470, val loss: 0.9508333802223206
Epoch 480, training loss: 12.806180953979492 = 0.6767141222953796 + 2.0 * 6.064733505249023
Epoch 480, val loss: 0.9312320947647095
Epoch 490, training loss: 12.773568153381348 = 0.6483268737792969 + 2.0 * 6.062620639801025
Epoch 490, val loss: 0.9128891825675964
Epoch 500, training loss: 12.741284370422363 = 0.6206759214401245 + 2.0 * 6.060304164886475
Epoch 500, val loss: 0.8955274224281311
Epoch 510, training loss: 12.714399337768555 = 0.593640923500061 + 2.0 * 6.0603790283203125
Epoch 510, val loss: 0.8789776563644409
Epoch 520, training loss: 12.68161678314209 = 0.5675233006477356 + 2.0 * 6.057046890258789
Epoch 520, val loss: 0.8634846210479736
Epoch 530, training loss: 12.653636932373047 = 0.5420557260513306 + 2.0 * 6.055790424346924
Epoch 530, val loss: 0.8488927483558655
Epoch 540, training loss: 12.62507152557373 = 0.5173345804214478 + 2.0 * 6.053868293762207
Epoch 540, val loss: 0.8353357315063477
Epoch 550, training loss: 12.598552703857422 = 0.4937041103839874 + 2.0 * 6.052424430847168
Epoch 550, val loss: 0.8228716254234314
Epoch 560, training loss: 12.573783874511719 = 0.4708496332168579 + 2.0 * 6.051466941833496
Epoch 560, val loss: 0.811253547668457
Epoch 570, training loss: 12.556471824645996 = 0.4488007426261902 + 2.0 * 6.053835391998291
Epoch 570, val loss: 0.8005101084709167
Epoch 580, training loss: 12.539689064025879 = 0.42759421467781067 + 2.0 * 6.056047439575195
Epoch 580, val loss: 0.7908192873001099
Epoch 590, training loss: 12.504030227661133 = 0.4074161648750305 + 2.0 * 6.048306941986084
Epoch 590, val loss: 0.7819430828094482
Epoch 600, training loss: 12.480823516845703 = 0.3881678283214569 + 2.0 * 6.046328067779541
Epoch 600, val loss: 0.7738943099975586
Epoch 610, training loss: 12.459577560424805 = 0.3696020841598511 + 2.0 * 6.044987678527832
Epoch 610, val loss: 0.7665084004402161
Epoch 620, training loss: 12.456503868103027 = 0.3518218994140625 + 2.0 * 6.052340984344482
Epoch 620, val loss: 0.759787380695343
Epoch 630, training loss: 12.424838066101074 = 0.33491817116737366 + 2.0 * 6.044960021972656
Epoch 630, val loss: 0.7538638114929199
Epoch 640, training loss: 12.402749061584473 = 0.31881457567214966 + 2.0 * 6.041967391967773
Epoch 640, val loss: 0.7485936880111694
Epoch 650, training loss: 12.38766098022461 = 0.3033691346645355 + 2.0 * 6.042145729064941
Epoch 650, val loss: 0.7436780333518982
Epoch 660, training loss: 12.373221397399902 = 0.28861239552497864 + 2.0 * 6.042304515838623
Epoch 660, val loss: 0.7392786741256714
Epoch 670, training loss: 12.354056358337402 = 0.2746647000312805 + 2.0 * 6.039695739746094
Epoch 670, val loss: 0.735559344291687
Epoch 680, training loss: 12.338224411010742 = 0.2613063156604767 + 2.0 * 6.038458824157715
Epoch 680, val loss: 0.732215404510498
Epoch 690, training loss: 12.325947761535645 = 0.24853515625 + 2.0 * 6.038706302642822
Epoch 690, val loss: 0.7291967272758484
Epoch 700, training loss: 12.310063362121582 = 0.236357182264328 + 2.0 * 6.036853313446045
Epoch 700, val loss: 0.7266974449157715
Epoch 710, training loss: 12.298009872436523 = 0.2248046100139618 + 2.0 * 6.03660249710083
Epoch 710, val loss: 0.724527895450592
Epoch 720, training loss: 12.28412914276123 = 0.21391147375106812 + 2.0 * 6.035109043121338
Epoch 720, val loss: 0.7228421568870544
Epoch 730, training loss: 12.270538330078125 = 0.20352229475975037 + 2.0 * 6.033507823944092
Epoch 730, val loss: 0.7215303778648376
Epoch 740, training loss: 12.277655601501465 = 0.19366773962974548 + 2.0 * 6.041994094848633
Epoch 740, val loss: 0.7205799221992493
Epoch 750, training loss: 12.252886772155762 = 0.1843445897102356 + 2.0 * 6.034271240234375
Epoch 750, val loss: 0.7198845148086548
Epoch 760, training loss: 12.2385835647583 = 0.17557357251644135 + 2.0 * 6.031505107879639
Epoch 760, val loss: 0.7197837233543396
Epoch 770, training loss: 12.227556228637695 = 0.16724121570587158 + 2.0 * 6.030157566070557
Epoch 770, val loss: 0.7198624014854431
Epoch 780, training loss: 12.233177185058594 = 0.1593390256166458 + 2.0 * 6.036919116973877
Epoch 780, val loss: 0.720232367515564
Epoch 790, training loss: 12.209331512451172 = 0.1519658863544464 + 2.0 * 6.028682708740234
Epoch 790, val loss: 0.7209140062332153
Epoch 800, training loss: 12.2109375 = 0.14498358964920044 + 2.0 * 6.032977104187012
Epoch 800, val loss: 0.721962034702301
Epoch 810, training loss: 12.195677757263184 = 0.13841421902179718 + 2.0 * 6.028631687164307
Epoch 810, val loss: 0.7231537103652954
Epoch 820, training loss: 12.186759948730469 = 0.13221406936645508 + 2.0 * 6.027272701263428
Epoch 820, val loss: 0.7247666716575623
Epoch 830, training loss: 12.185020446777344 = 0.12636806070804596 + 2.0 * 6.02932596206665
Epoch 830, val loss: 0.7264103293418884
Epoch 840, training loss: 12.171359062194824 = 0.1208522617816925 + 2.0 * 6.0252532958984375
Epoch 840, val loss: 0.7284209728240967
Epoch 850, training loss: 12.163656234741211 = 0.11564429849386215 + 2.0 * 6.024005889892578
Epoch 850, val loss: 0.7306032180786133
Epoch 860, training loss: 12.16345500946045 = 0.11070118844509125 + 2.0 * 6.026376724243164
Epoch 860, val loss: 0.7329663038253784
Epoch 870, training loss: 12.160587310791016 = 0.10606878995895386 + 2.0 * 6.027259349822998
Epoch 870, val loss: 0.7354755401611328
Epoch 880, training loss: 12.149063110351562 = 0.10172602534294128 + 2.0 * 6.0236687660217285
Epoch 880, val loss: 0.7382618188858032
Epoch 890, training loss: 12.146742820739746 = 0.09759549051523209 + 2.0 * 6.024573802947998
Epoch 890, val loss: 0.7410832047462463
Epoch 900, training loss: 12.136363983154297 = 0.09370199590921402 + 2.0 * 6.021330833435059
Epoch 900, val loss: 0.7440106868743896
Epoch 910, training loss: 12.13220500946045 = 0.09000403434038162 + 2.0 * 6.0211005210876465
Epoch 910, val loss: 0.7471033930778503
Epoch 920, training loss: 12.135509490966797 = 0.08651310205459595 + 2.0 * 6.024497985839844
Epoch 920, val loss: 0.7502359747886658
Epoch 930, training loss: 12.121672630310059 = 0.08320680260658264 + 2.0 * 6.019232749938965
Epoch 930, val loss: 0.7534501552581787
Epoch 940, training loss: 12.118316650390625 = 0.08007734268903732 + 2.0 * 6.019119739532471
Epoch 940, val loss: 0.7568656206130981
Epoch 950, training loss: 12.113844871520996 = 0.07709380984306335 + 2.0 * 6.018375396728516
Epoch 950, val loss: 0.760223925113678
Epoch 960, training loss: 12.117880821228027 = 0.07425571233034134 + 2.0 * 6.021812438964844
Epoch 960, val loss: 0.7636940479278564
Epoch 970, training loss: 12.119579315185547 = 0.07157882302999496 + 2.0 * 6.02400016784668
Epoch 970, val loss: 0.7671038508415222
Epoch 980, training loss: 12.107075691223145 = 0.06906178593635559 + 2.0 * 6.019006729125977
Epoch 980, val loss: 0.7706554532051086
Epoch 990, training loss: 12.09885311126709 = 0.06667181849479675 + 2.0 * 6.0160908699035645
Epoch 990, val loss: 0.7742945551872253
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 730, in <module>
    torch.cuda.empty_cache()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/cuda/memory.py", line 121, in empty_cache
    torch._C._cuda_emptyCache()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.4 der2:0.3 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.3, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 18.690292358398438 = 1.942566990852356 + 2.0 * 8.373862266540527
Epoch 0, val loss: 1.9458487033843994
Epoch 10, training loss: 18.679534912109375 = 1.9326889514923096 + 2.0 * 8.373422622680664
Epoch 10, val loss: 1.9363867044448853
Epoch 20, training loss: 18.662324905395508 = 1.920719861984253 + 2.0 * 8.370802879333496
Epoch 20, val loss: 1.924641489982605
Epoch 30, training loss: 18.610877990722656 = 1.9047460556030273 + 2.0 * 8.353066444396973
Epoch 30, val loss: 1.908661961555481
Epoch 40, training loss: 18.320945739746094 = 1.8850544691085815 + 2.0 * 8.21794605255127
Epoch 40, val loss: 1.8891061544418335
Epoch 50, training loss: 16.616708755493164 = 1.8633627891540527 + 2.0 * 7.376672744750977
Epoch 50, val loss: 1.8670047521591187
Epoch 60, training loss: 15.818358421325684 = 1.847483515739441 + 2.0 * 6.985437393188477
Epoch 60, val loss: 1.8526227474212646
Epoch 70, training loss: 15.327825546264648 = 1.837699055671692 + 2.0 * 6.745063304901123
Epoch 70, val loss: 1.8430819511413574
Epoch 80, training loss: 15.058150291442871 = 1.8274202346801758 + 2.0 * 6.615365028381348
Epoch 80, val loss: 1.83291757106781
Epoch 90, training loss: 14.896955490112305 = 1.817630648612976 + 2.0 * 6.5396623611450195
Epoch 90, val loss: 1.8230102062225342
Epoch 100, training loss: 14.747220039367676 = 1.8087536096572876 + 2.0 * 6.46923303604126
Epoch 100, val loss: 1.8141175508499146
Epoch 110, training loss: 14.620282173156738 = 1.8013285398483276 + 2.0 * 6.4094767570495605
Epoch 110, val loss: 1.8063852787017822
Epoch 120, training loss: 14.524604797363281 = 1.794094443321228 + 2.0 * 6.365255355834961
Epoch 120, val loss: 1.7987534999847412
Epoch 130, training loss: 14.447088241577148 = 1.786637306213379 + 2.0 * 6.330225467681885
Epoch 130, val loss: 1.7909718751907349
Epoch 140, training loss: 14.382501602172852 = 1.779259204864502 + 2.0 * 6.301620960235596
Epoch 140, val loss: 1.783444881439209
Epoch 150, training loss: 14.32540225982666 = 1.7715731859207153 + 2.0 * 6.276914596557617
Epoch 150, val loss: 1.7758809328079224
Epoch 160, training loss: 14.278919219970703 = 1.7631144523620605 + 2.0 * 6.257902145385742
Epoch 160, val loss: 1.7679044008255005
Epoch 170, training loss: 14.235006332397461 = 1.7537481784820557 + 2.0 * 6.240629196166992
Epoch 170, val loss: 1.7594788074493408
Epoch 180, training loss: 14.19577693939209 = 1.743259310722351 + 2.0 * 6.226258754730225
Epoch 180, val loss: 1.750212550163269
Epoch 190, training loss: 14.163023948669434 = 1.7311954498291016 + 2.0 * 6.215914249420166
Epoch 190, val loss: 1.7398566007614136
Epoch 200, training loss: 14.127043724060059 = 1.7173157930374146 + 2.0 * 6.204864025115967
Epoch 200, val loss: 1.7280781269073486
Epoch 210, training loss: 14.092023849487305 = 1.7012995481491089 + 2.0 * 6.195362091064453
Epoch 210, val loss: 1.714674472808838
Epoch 220, training loss: 14.058423042297363 = 1.6826227903366089 + 2.0 * 6.187900066375732
Epoch 220, val loss: 1.6991413831710815
Epoch 230, training loss: 14.020978927612305 = 1.6607985496520996 + 2.0 * 6.180090427398682
Epoch 230, val loss: 1.6809742450714111
Epoch 240, training loss: 13.982701301574707 = 1.635404348373413 + 2.0 * 6.173648357391357
Epoch 240, val loss: 1.659975528717041
Epoch 250, training loss: 13.940200805664062 = 1.606057047843933 + 2.0 * 6.16707181930542
Epoch 250, val loss: 1.6357648372650146
Epoch 260, training loss: 13.893701553344727 = 1.5721555948257446 + 2.0 * 6.160772800445557
Epoch 260, val loss: 1.6078423261642456
Epoch 270, training loss: 13.844226837158203 = 1.5332759618759155 + 2.0 * 6.155475616455078
Epoch 270, val loss: 1.5758566856384277
Epoch 280, training loss: 13.793532371520996 = 1.4900144338607788 + 2.0 * 6.151759147644043
Epoch 280, val loss: 1.5405981540679932
Epoch 290, training loss: 13.73728084564209 = 1.4436671733856201 + 2.0 * 6.146806716918945
Epoch 290, val loss: 1.5030531883239746
Epoch 300, training loss: 13.678041458129883 = 1.3944541215896606 + 2.0 * 6.141793727874756
Epoch 300, val loss: 1.4634170532226562
Epoch 310, training loss: 13.620997428894043 = 1.3434501886367798 + 2.0 * 6.138773441314697
Epoch 310, val loss: 1.422791838645935
Epoch 320, training loss: 13.563270568847656 = 1.2926651239395142 + 2.0 * 6.135302543640137
Epoch 320, val loss: 1.383052945137024
Epoch 330, training loss: 13.50321102142334 = 1.2426031827926636 + 2.0 * 6.130303859710693
Epoch 330, val loss: 1.3443931341171265
Epoch 340, training loss: 13.448073387145996 = 1.1932878494262695 + 2.0 * 6.127392768859863
Epoch 340, val loss: 1.3068584203720093
Epoch 350, training loss: 13.393526077270508 = 1.14552903175354 + 2.0 * 6.123998641967773
Epoch 350, val loss: 1.2707964181900024
Epoch 360, training loss: 13.337333679199219 = 1.0989084243774414 + 2.0 * 6.119212627410889
Epoch 360, val loss: 1.236036777496338
Epoch 370, training loss: 13.285205841064453 = 1.052748441696167 + 2.0 * 6.1162285804748535
Epoch 370, val loss: 1.2017724514007568
Epoch 380, training loss: 13.243694305419922 = 1.007035732269287 + 2.0 * 6.118329048156738
Epoch 380, val loss: 1.1680574417114258
Epoch 390, training loss: 13.184697151184082 = 0.9626434445381165 + 2.0 * 6.111026763916016
Epoch 390, val loss: 1.1349830627441406
Epoch 400, training loss: 13.133296966552734 = 0.9189639091491699 + 2.0 * 6.107166290283203
Epoch 400, val loss: 1.1022425889968872
Epoch 410, training loss: 13.084630012512207 = 0.8757779598236084 + 2.0 * 6.10442590713501
Epoch 410, val loss: 1.0697137117385864
Epoch 420, training loss: 13.046502113342285 = 0.8334812521934509 + 2.0 * 6.106510639190674
Epoch 420, val loss: 1.0376906394958496
Epoch 430, training loss: 12.992112159729004 = 0.7926642298698425 + 2.0 * 6.099723815917969
Epoch 430, val loss: 1.006614089012146
Epoch 440, training loss: 12.955245971679688 = 0.7534478902816772 + 2.0 * 6.1008992195129395
Epoch 440, val loss: 0.9766652584075928
Epoch 450, training loss: 12.90648365020752 = 0.7163328528404236 + 2.0 * 6.095075607299805
Epoch 450, val loss: 0.9483147263526917
Epoch 460, training loss: 12.866063117980957 = 0.6810693740844727 + 2.0 * 6.092496871948242
Epoch 460, val loss: 0.9215202927589417
Epoch 470, training loss: 12.839720726013184 = 0.6476641297340393 + 2.0 * 6.0960283279418945
Epoch 470, val loss: 0.8963291645050049
Epoch 480, training loss: 12.794417381286621 = 0.6162713766098022 + 2.0 * 6.089073181152344
Epoch 480, val loss: 0.8732492923736572
Epoch 490, training loss: 12.761612892150879 = 0.5867533087730408 + 2.0 * 6.087430000305176
Epoch 490, val loss: 0.8519784808158875
Epoch 500, training loss: 12.728742599487305 = 0.5589364171028137 + 2.0 * 6.084903240203857
Epoch 500, val loss: 0.8323507905006409
Epoch 510, training loss: 12.705322265625 = 0.5326637625694275 + 2.0 * 6.086329460144043
Epoch 510, val loss: 0.8144764304161072
Epoch 520, training loss: 12.674993515014648 = 0.5080548524856567 + 2.0 * 6.083469390869141
Epoch 520, val loss: 0.7982861995697021
Epoch 530, training loss: 12.642478942871094 = 0.48477447032928467 + 2.0 * 6.07885217666626
Epoch 530, val loss: 0.7836573719978333
Epoch 540, training loss: 12.616731643676758 = 0.46249625086784363 + 2.0 * 6.077117919921875
Epoch 540, val loss: 0.7702210545539856
Epoch 550, training loss: 12.594918251037598 = 0.44116276502609253 + 2.0 * 6.076877593994141
Epoch 550, val loss: 0.7579630613327026
Epoch 560, training loss: 12.567230224609375 = 0.4208180904388428 + 2.0 * 6.073205947875977
Epoch 560, val loss: 0.7467400431632996
Epoch 570, training loss: 12.544979095458984 = 0.40115198493003845 + 2.0 * 6.071913719177246
Epoch 570, val loss: 0.7363557815551758
Epoch 580, training loss: 12.535367012023926 = 0.38203439116477966 + 2.0 * 6.076666355133057
Epoch 580, val loss: 0.7266366481781006
Epoch 590, training loss: 12.501191139221191 = 0.36371880769729614 + 2.0 * 6.0687360763549805
Epoch 590, val loss: 0.7176440954208374
Epoch 600, training loss: 12.481515884399414 = 0.34609147906303406 + 2.0 * 6.067712306976318
Epoch 600, val loss: 0.7094882726669312
Epoch 610, training loss: 12.461566925048828 = 0.3289812505245209 + 2.0 * 6.066292762756348
Epoch 610, val loss: 0.7017045617103577
Epoch 620, training loss: 12.460380554199219 = 0.3124404847621918 + 2.0 * 6.073969841003418
Epoch 620, val loss: 0.6944177746772766
Epoch 630, training loss: 12.424936294555664 = 0.2966305911540985 + 2.0 * 6.064152717590332
Epoch 630, val loss: 0.6877391934394836
Epoch 640, training loss: 12.408023834228516 = 0.2815633714199066 + 2.0 * 6.063230037689209
Epoch 640, val loss: 0.6816222071647644
Epoch 650, training loss: 12.395453453063965 = 0.26720595359802246 + 2.0 * 6.064123630523682
Epoch 650, val loss: 0.6759445667266846
Epoch 660, training loss: 12.373658180236816 = 0.2535742223262787 + 2.0 * 6.060041904449463
Epoch 660, val loss: 0.6707967519760132
Epoch 670, training loss: 12.362241744995117 = 0.2406773567199707 + 2.0 * 6.060782432556152
Epoch 670, val loss: 0.6660994291305542
Epoch 680, training loss: 12.34565544128418 = 0.22852222621440887 + 2.0 * 6.058566570281982
Epoch 680, val loss: 0.6618311405181885
Epoch 690, training loss: 12.336616516113281 = 0.2170959860086441 + 2.0 * 6.059760093688965
Epoch 690, val loss: 0.6580991148948669
Epoch 700, training loss: 12.321372985839844 = 0.20636916160583496 + 2.0 * 6.057501792907715
Epoch 700, val loss: 0.6547482013702393
Epoch 710, training loss: 12.305530548095703 = 0.19630922377109528 + 2.0 * 6.054610729217529
Epoch 710, val loss: 0.6517960429191589
Epoch 720, training loss: 12.29428482055664 = 0.18681800365447998 + 2.0 * 6.0537333488464355
Epoch 720, val loss: 0.6492960453033447
Epoch 730, training loss: 12.293427467346191 = 0.17787519097328186 + 2.0 * 6.057775974273682
Epoch 730, val loss: 0.6471219658851624
Epoch 740, training loss: 12.277192115783691 = 0.16951502859592438 + 2.0 * 6.053838729858398
Epoch 740, val loss: 0.6453118920326233
Epoch 750, training loss: 12.271446228027344 = 0.16165536642074585 + 2.0 * 6.054895401000977
Epoch 750, val loss: 0.6438563466072083
Epoch 760, training loss: 12.254411697387695 = 0.15429863333702087 + 2.0 * 6.050056457519531
Epoch 760, val loss: 0.6426563262939453
Epoch 770, training loss: 12.245076179504395 = 0.14733090996742249 + 2.0 * 6.048872470855713
Epoch 770, val loss: 0.6417898535728455
Epoch 780, training loss: 12.245203018188477 = 0.14075486361980438 + 2.0 * 6.052224159240723
Epoch 780, val loss: 0.6412109732627869
Epoch 790, training loss: 12.230489730834961 = 0.13457024097442627 + 2.0 * 6.047959804534912
Epoch 790, val loss: 0.6408473253250122
Epoch 800, training loss: 12.21922779083252 = 0.12871146202087402 + 2.0 * 6.045258045196533
Epoch 800, val loss: 0.640777051448822
Epoch 810, training loss: 12.224593162536621 = 0.12314371019601822 + 2.0 * 6.050724506378174
Epoch 810, val loss: 0.6409531235694885
Epoch 820, training loss: 12.214179992675781 = 0.11794057488441467 + 2.0 * 6.04811954498291
Epoch 820, val loss: 0.641299843788147
Epoch 830, training loss: 12.201253890991211 = 0.11301826685667038 + 2.0 * 6.0441179275512695
Epoch 830, val loss: 0.6418437957763672
Epoch 840, training loss: 12.192340850830078 = 0.10833828151226044 + 2.0 * 6.042001247406006
Epoch 840, val loss: 0.6425408124923706
Epoch 850, training loss: 12.189794540405273 = 0.10387082397937775 + 2.0 * 6.042962074279785
Epoch 850, val loss: 0.6434451937675476
Epoch 860, training loss: 12.186966896057129 = 0.09965655952692032 + 2.0 * 6.0436553955078125
Epoch 860, val loss: 0.644564688205719
Epoch 870, training loss: 12.176066398620605 = 0.09567388892173767 + 2.0 * 6.040196418762207
Epoch 870, val loss: 0.6457675099372864
Epoch 880, training loss: 12.172893524169922 = 0.09189461171627045 + 2.0 * 6.040499687194824
Epoch 880, val loss: 0.647116482257843
Epoch 890, training loss: 12.166071891784668 = 0.08829338103532791 + 2.0 * 6.038889408111572
Epoch 890, val loss: 0.6485921144485474
Epoch 900, training loss: 12.160634994506836 = 0.08486627787351608 + 2.0 * 6.03788423538208
Epoch 900, val loss: 0.6502354741096497
Epoch 910, training loss: 12.162689208984375 = 0.08160746097564697 + 2.0 * 6.04054069519043
Epoch 910, val loss: 0.6519597172737122
Epoch 920, training loss: 12.167895317077637 = 0.07852131873369217 + 2.0 * 6.044686794281006
Epoch 920, val loss: 0.653782308101654
Epoch 930, training loss: 12.145827293395996 = 0.07558120042085648 + 2.0 * 6.035122871398926
Epoch 930, val loss: 0.6557198166847229
Epoch 940, training loss: 12.141763687133789 = 0.07279355078935623 + 2.0 * 6.03448486328125
Epoch 940, val loss: 0.6577399373054504
Epoch 950, training loss: 12.138434410095215 = 0.0701204463839531 + 2.0 * 6.034156799316406
Epoch 950, val loss: 0.6598734259605408
Epoch 960, training loss: 12.142199516296387 = 0.0675756111741066 + 2.0 * 6.037312030792236
Epoch 960, val loss: 0.6620935201644897
Epoch 970, training loss: 12.131160736083984 = 0.06515836715698242 + 2.0 * 6.033000946044922
Epoch 970, val loss: 0.6643787622451782
Epoch 980, training loss: 12.127161026000977 = 0.06286538392305374 + 2.0 * 6.0321478843688965
Epoch 980, val loss: 0.6667162179946899
Epoch 990, training loss: 12.131366729736328 = 0.06067086011171341 + 2.0 * 6.035347938537598
Epoch 990, val loss: 0.6691421866416931
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.6531
Flip ASR: 0.5956/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.69674301147461 = 1.9489938020706177 + 2.0 * 8.37387466430664
Epoch 0, val loss: 1.9525514841079712
Epoch 10, training loss: 18.68559455871582 = 1.938768982887268 + 2.0 * 8.3734130859375
Epoch 10, val loss: 1.9424865245819092
Epoch 20, training loss: 18.667152404785156 = 1.9257841110229492 + 2.0 * 8.370684623718262
Epoch 20, val loss: 1.929290533065796
Epoch 30, training loss: 18.611337661743164 = 1.908073902130127 + 2.0 * 8.351632118225098
Epoch 30, val loss: 1.9111640453338623
Epoch 40, training loss: 18.30316734313965 = 1.8864222764968872 + 2.0 * 8.208372116088867
Epoch 40, val loss: 1.8898663520812988
Epoch 50, training loss: 16.886241912841797 = 1.862087368965149 + 2.0 * 7.512077331542969
Epoch 50, val loss: 1.865507960319519
Epoch 60, training loss: 16.095998764038086 = 1.8412977457046509 + 2.0 * 7.127350330352783
Epoch 60, val loss: 1.8454923629760742
Epoch 70, training loss: 15.586289405822754 = 1.8298813104629517 + 2.0 * 6.878203868865967
Epoch 70, val loss: 1.8338419198989868
Epoch 80, training loss: 15.212993621826172 = 1.8194355964660645 + 2.0 * 6.696779251098633
Epoch 80, val loss: 1.822840929031372
Epoch 90, training loss: 15.023774147033691 = 1.8111095428466797 + 2.0 * 6.606332302093506
Epoch 90, val loss: 1.813530683517456
Epoch 100, training loss: 14.879440307617188 = 1.8010114431381226 + 2.0 * 6.539214611053467
Epoch 100, val loss: 1.8021559715270996
Epoch 110, training loss: 14.744373321533203 = 1.7925703525543213 + 2.0 * 6.4759016036987305
Epoch 110, val loss: 1.7927337884902954
Epoch 120, training loss: 14.631134033203125 = 1.7860757112503052 + 2.0 * 6.422529220581055
Epoch 120, val loss: 1.7854630947113037
Epoch 130, training loss: 14.544638633728027 = 1.7796114683151245 + 2.0 * 6.382513523101807
Epoch 130, val loss: 1.7785199880599976
Epoch 140, training loss: 14.471824645996094 = 1.7725282907485962 + 2.0 * 6.3496479988098145
Epoch 140, val loss: 1.7714637517929077
Epoch 150, training loss: 14.409416198730469 = 1.7648673057556152 + 2.0 * 6.322274684906006
Epoch 150, val loss: 1.7643018960952759
Epoch 160, training loss: 14.351250648498535 = 1.7564420700073242 + 2.0 * 6.2974042892456055
Epoch 160, val loss: 1.7568923234939575
Epoch 170, training loss: 14.298892974853516 = 1.7470606565475464 + 2.0 * 6.27591609954834
Epoch 170, val loss: 1.749039888381958
Epoch 180, training loss: 14.256267547607422 = 1.7364473342895508 + 2.0 * 6.2599101066589355
Epoch 180, val loss: 1.7404558658599854
Epoch 190, training loss: 14.206018447875977 = 1.7244170904159546 + 2.0 * 6.240800857543945
Epoch 190, val loss: 1.730828881263733
Epoch 200, training loss: 14.161847114562988 = 1.7106170654296875 + 2.0 * 6.22561502456665
Epoch 200, val loss: 1.7199509143829346
Epoch 210, training loss: 14.130980491638184 = 1.6944013833999634 + 2.0 * 6.218289375305176
Epoch 210, val loss: 1.7074048519134521
Epoch 220, training loss: 14.079741477966309 = 1.6756330728530884 + 2.0 * 6.202054023742676
Epoch 220, val loss: 1.692735195159912
Epoch 230, training loss: 14.038311004638672 = 1.6537073850631714 + 2.0 * 6.1923017501831055
Epoch 230, val loss: 1.6758159399032593
Epoch 240, training loss: 13.996172904968262 = 1.627988576889038 + 2.0 * 6.184092044830322
Epoch 240, val loss: 1.6559414863586426
Epoch 250, training loss: 13.950660705566406 = 1.5982915163040161 + 2.0 * 6.17618465423584
Epoch 250, val loss: 1.6327383518218994
Epoch 260, training loss: 13.900003433227539 = 1.564030408859253 + 2.0 * 6.1679863929748535
Epoch 260, val loss: 1.6058825254440308
Epoch 270, training loss: 13.854011535644531 = 1.524738073348999 + 2.0 * 6.164636611938477
Epoch 270, val loss: 1.57499098777771
Epoch 280, training loss: 13.792869567871094 = 1.4808542728424072 + 2.0 * 6.156007766723633
Epoch 280, val loss: 1.5403815507888794
Epoch 290, training loss: 13.733108520507812 = 1.432453989982605 + 2.0 * 6.150327205657959
Epoch 290, val loss: 1.5019524097442627
Epoch 300, training loss: 13.678569793701172 = 1.3800536394119263 + 2.0 * 6.149258136749268
Epoch 300, val loss: 1.4601118564605713
Epoch 310, training loss: 13.609789848327637 = 1.3255341053009033 + 2.0 * 6.142127990722656
Epoch 310, val loss: 1.416321873664856
Epoch 320, training loss: 13.545804977416992 = 1.2702571153640747 + 2.0 * 6.1377739906311035
Epoch 320, val loss: 1.3720414638519287
Epoch 330, training loss: 13.481733322143555 = 1.21549654006958 + 2.0 * 6.133118629455566
Epoch 330, val loss: 1.3280596733093262
Epoch 340, training loss: 13.421833992004395 = 1.1622244119644165 + 2.0 * 6.129804611206055
Epoch 340, val loss: 1.2852674722671509
Epoch 350, training loss: 13.365948677062988 = 1.1113210916519165 + 2.0 * 6.127313613891602
Epoch 350, val loss: 1.2445417642593384
Epoch 360, training loss: 13.308778762817383 = 1.0635418891906738 + 2.0 * 6.122618675231934
Epoch 360, val loss: 1.2063206434249878
Epoch 370, training loss: 13.254761695861816 = 1.0186240673065186 + 2.0 * 6.118068695068359
Epoch 370, val loss: 1.1704810857772827
Epoch 380, training loss: 13.208124160766602 = 0.97605299949646 + 2.0 * 6.116035461425781
Epoch 380, val loss: 1.1365668773651123
Epoch 390, training loss: 13.168928146362305 = 0.9361928701400757 + 2.0 * 6.116367816925049
Epoch 390, val loss: 1.1044467687606812
Epoch 400, training loss: 13.119174003601074 = 0.8988199830055237 + 2.0 * 6.110177040100098
Epoch 400, val loss: 1.074750304222107
Epoch 410, training loss: 13.074628829956055 = 0.8635434508323669 + 2.0 * 6.1055426597595215
Epoch 410, val loss: 1.0467956066131592
Epoch 420, training loss: 13.047327041625977 = 0.8301040530204773 + 2.0 * 6.108611583709717
Epoch 420, val loss: 1.0203694105148315
Epoch 430, training loss: 13.001787185668945 = 0.7989884614944458 + 2.0 * 6.1013994216918945
Epoch 430, val loss: 0.9960328340530396
Epoch 440, training loss: 12.962517738342285 = 0.769924521446228 + 2.0 * 6.096296787261963
Epoch 440, val loss: 0.9736508131027222
Epoch 450, training loss: 12.932865142822266 = 0.742540717124939 + 2.0 * 6.095162391662598
Epoch 450, val loss: 0.9529609680175781
Epoch 460, training loss: 12.904631614685059 = 0.7168802618980408 + 2.0 * 6.093875885009766
Epoch 460, val loss: 0.9341883063316345
Epoch 470, training loss: 12.874825477600098 = 0.6930012106895447 + 2.0 * 6.090912342071533
Epoch 470, val loss: 0.9173632264137268
Epoch 480, training loss: 12.846233367919922 = 0.6706233620643616 + 2.0 * 6.087804794311523
Epoch 480, val loss: 0.9023123979568481
Epoch 490, training loss: 12.820857048034668 = 0.6494635343551636 + 2.0 * 6.085696697235107
Epoch 490, val loss: 0.8887931108474731
Epoch 500, training loss: 12.799844741821289 = 0.6293303370475769 + 2.0 * 6.085257053375244
Epoch 500, val loss: 0.8765743970870972
Epoch 510, training loss: 12.773625373840332 = 0.6102224588394165 + 2.0 * 6.081701278686523
Epoch 510, val loss: 0.8656190633773804
Epoch 520, training loss: 12.749496459960938 = 0.5918323993682861 + 2.0 * 6.078832149505615
Epoch 520, val loss: 0.855711042881012
Epoch 530, training loss: 12.733473777770996 = 0.5740858912467957 + 2.0 * 6.079693794250488
Epoch 530, val loss: 0.8466518521308899
Epoch 540, training loss: 12.719263076782227 = 0.557059645652771 + 2.0 * 6.081101894378662
Epoch 540, val loss: 0.8385178446769714
Epoch 550, training loss: 12.69121265411377 = 0.5407280325889587 + 2.0 * 6.075242519378662
Epoch 550, val loss: 0.8312732577323914
Epoch 560, training loss: 12.669108390808105 = 0.5248452425003052 + 2.0 * 6.072131633758545
Epoch 560, val loss: 0.8246587514877319
Epoch 570, training loss: 12.656988143920898 = 0.5093205571174622 + 2.0 * 6.07383394241333
Epoch 570, val loss: 0.8186766505241394
Epoch 580, training loss: 12.643253326416016 = 0.4942522943019867 + 2.0 * 6.074500560760498
Epoch 580, val loss: 0.8132942914962769
Epoch 590, training loss: 12.615415573120117 = 0.47962990403175354 + 2.0 * 6.067893028259277
Epoch 590, val loss: 0.8085614442825317
Epoch 600, training loss: 12.59843921661377 = 0.46525928378105164 + 2.0 * 6.066589832305908
Epoch 600, val loss: 0.804378092288971
Epoch 610, training loss: 12.589032173156738 = 0.45118770003318787 + 2.0 * 6.06892204284668
Epoch 610, val loss: 0.8006702065467834
Epoch 620, training loss: 12.568285942077637 = 0.4374411702156067 + 2.0 * 6.065422534942627
Epoch 620, val loss: 0.7974997162818909
Epoch 630, training loss: 12.548280715942383 = 0.42387107014656067 + 2.0 * 6.062204837799072
Epoch 630, val loss: 0.7948436141014099
Epoch 640, training loss: 12.532054901123047 = 0.41038787364959717 + 2.0 * 6.06083345413208
Epoch 640, val loss: 0.7926149368286133
Epoch 650, training loss: 12.53783130645752 = 0.3970643877983093 + 2.0 * 6.070383548736572
Epoch 650, val loss: 0.7907901406288147
Epoch 660, training loss: 12.511144638061523 = 0.3840043544769287 + 2.0 * 6.063570022583008
Epoch 660, val loss: 0.7894698977470398
Epoch 670, training loss: 12.489511489868164 = 0.37110915780067444 + 2.0 * 6.059201240539551
Epoch 670, val loss: 0.7886852025985718
Epoch 680, training loss: 12.471954345703125 = 0.3582930564880371 + 2.0 * 6.056830406188965
Epoch 680, val loss: 0.788276731967926
Epoch 690, training loss: 12.475964546203613 = 0.3455609381198883 + 2.0 * 6.065201759338379
Epoch 690, val loss: 0.7882399559020996
Epoch 700, training loss: 12.445901870727539 = 0.33312156796455383 + 2.0 * 6.056390285491943
Epoch 700, val loss: 0.7885627746582031
Epoch 710, training loss: 12.431107521057129 = 0.3208464980125427 + 2.0 * 6.055130481719971
Epoch 710, val loss: 0.7893788814544678
Epoch 720, training loss: 12.4169921875 = 0.30873289704322815 + 2.0 * 6.054129600524902
Epoch 720, val loss: 0.7906214594841003
Epoch 730, training loss: 12.403388977050781 = 0.296871155500412 + 2.0 * 6.053258895874023
Epoch 730, val loss: 0.7921585440635681
Epoch 740, training loss: 12.389433860778809 = 0.2853049039840698 + 2.0 * 6.052064418792725
Epoch 740, val loss: 0.7941192388534546
Epoch 750, training loss: 12.380462646484375 = 0.27402177453041077 + 2.0 * 6.053220272064209
Epoch 750, val loss: 0.7963947057723999
Epoch 760, training loss: 12.372991561889648 = 0.26305437088012695 + 2.0 * 6.054968357086182
Epoch 760, val loss: 0.7989333271980286
Epoch 770, training loss: 12.350421905517578 = 0.2524447739124298 + 2.0 * 6.048988342285156
Epoch 770, val loss: 0.8017289638519287
Epoch 780, training loss: 12.338981628417969 = 0.24213087558746338 + 2.0 * 6.048425197601318
Epoch 780, val loss: 0.8048059344291687
Epoch 790, training loss: 12.328907012939453 = 0.23208560049533844 + 2.0 * 6.048410892486572
Epoch 790, val loss: 0.8080971837043762
Epoch 800, training loss: 12.32097339630127 = 0.22239574790000916 + 2.0 * 6.049288749694824
Epoch 800, val loss: 0.8114815354347229
Epoch 810, training loss: 12.307517051696777 = 0.21306274831295013 + 2.0 * 6.047227382659912
Epoch 810, val loss: 0.8151091933250427
Epoch 820, training loss: 12.298376083374023 = 0.2040477991104126 + 2.0 * 6.047163963317871
Epoch 820, val loss: 0.8188112378120422
Epoch 830, training loss: 12.284725189208984 = 0.1953415870666504 + 2.0 * 6.044691562652588
Epoch 830, val loss: 0.8226818442344666
Epoch 840, training loss: 12.28748607635498 = 0.1869351863861084 + 2.0 * 6.0502753257751465
Epoch 840, val loss: 0.8266350030899048
Epoch 850, training loss: 12.273538589477539 = 0.17891444265842438 + 2.0 * 6.047312259674072
Epoch 850, val loss: 0.8306944966316223
Epoch 860, training loss: 12.259621620178223 = 0.17124073207378387 + 2.0 * 6.044190406799316
Epoch 860, val loss: 0.8349599838256836
Epoch 870, training loss: 12.249161720275879 = 0.16387824714183807 + 2.0 * 6.042641639709473
Epoch 870, val loss: 0.839291512966156
Epoch 880, training loss: 12.239448547363281 = 0.15680697560310364 + 2.0 * 6.04132080078125
Epoch 880, val loss: 0.8437377214431763
Epoch 890, training loss: 12.232901573181152 = 0.15001514554023743 + 2.0 * 6.041443347930908
Epoch 890, val loss: 0.8482880592346191
Epoch 900, training loss: 12.234039306640625 = 0.14353281259536743 + 2.0 * 6.045253276824951
Epoch 900, val loss: 0.8527965545654297
Epoch 910, training loss: 12.223520278930664 = 0.13738015294075012 + 2.0 * 6.043069839477539
Epoch 910, val loss: 0.857386589050293
Epoch 920, training loss: 12.210050582885742 = 0.1315164864063263 + 2.0 * 6.039267063140869
Epoch 920, val loss: 0.8621593117713928
Epoch 930, training loss: 12.201271057128906 = 0.12592490017414093 + 2.0 * 6.037672996520996
Epoch 930, val loss: 0.8670200109481812
Epoch 940, training loss: 12.201836585998535 = 0.12058848887681961 + 2.0 * 6.040624141693115
Epoch 940, val loss: 0.8719578385353088
Epoch 950, training loss: 12.191213607788086 = 0.11552324146032333 + 2.0 * 6.037845134735107
Epoch 950, val loss: 0.8769476413726807
Epoch 960, training loss: 12.186406135559082 = 0.1107063889503479 + 2.0 * 6.0378499031066895
Epoch 960, val loss: 0.8820406198501587
Epoch 970, training loss: 12.184782028198242 = 0.10612484812736511 + 2.0 * 6.039328575134277
Epoch 970, val loss: 0.8872250318527222
Epoch 980, training loss: 12.172018051147461 = 0.1017882227897644 + 2.0 * 6.035114765167236
Epoch 980, val loss: 0.8923918604850769
Epoch 990, training loss: 12.166119575500488 = 0.09765985608100891 + 2.0 * 6.034229755401611
Epoch 990, val loss: 0.8977716565132141
=== picking the best model according to the performance on validation ===
target class rate on Vs: 0.6000
accuracy on clean test nodes: 0.8111
Overall ASR: 0.4834
Flip ASR: 0.4000/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.690078735351562 = 1.9423654079437256 + 2.0 * 8.373856544494629
Epoch 0, val loss: 1.9385857582092285
Epoch 10, training loss: 18.67768096923828 = 1.9316531419754028 + 2.0 * 8.373013496398926
Epoch 10, val loss: 1.9271159172058105
Epoch 20, training loss: 18.657712936401367 = 1.918674111366272 + 2.0 * 8.369519233703613
Epoch 20, val loss: 1.9129351377487183
Epoch 30, training loss: 18.603954315185547 = 1.9011861085891724 + 2.0 * 8.351384162902832
Epoch 30, val loss: 1.8938109874725342
Epoch 40, training loss: 18.32674789428711 = 1.880037546157837 + 2.0 * 8.223355293273926
Epoch 40, val loss: 1.8715496063232422
Epoch 50, training loss: 16.979900360107422 = 1.8586468696594238 + 2.0 * 7.560626983642578
Epoch 50, val loss: 1.8492481708526611
Epoch 60, training loss: 16.020492553710938 = 1.842849850654602 + 2.0 * 7.0888214111328125
Epoch 60, val loss: 1.8331178426742554
Epoch 70, training loss: 15.49450397491455 = 1.8313357830047607 + 2.0 * 6.8315839767456055
Epoch 70, val loss: 1.8202064037322998
Epoch 80, training loss: 15.240371704101562 = 1.8191484212875366 + 2.0 * 6.710611820220947
Epoch 80, val loss: 1.8072972297668457
Epoch 90, training loss: 15.052443504333496 = 1.8084781169891357 + 2.0 * 6.621982574462891
Epoch 90, val loss: 1.796113133430481
Epoch 100, training loss: 14.860636711120605 = 1.7987390756607056 + 2.0 * 6.530948638916016
Epoch 100, val loss: 1.7863490581512451
Epoch 110, training loss: 14.70217514038086 = 1.7906538248062134 + 2.0 * 6.455760478973389
Epoch 110, val loss: 1.7779946327209473
Epoch 120, training loss: 14.590225219726562 = 1.7827677726745605 + 2.0 * 6.40372896194458
Epoch 120, val loss: 1.7696709632873535
Epoch 130, training loss: 14.498248100280762 = 1.7741565704345703 + 2.0 * 6.362045764923096
Epoch 130, val loss: 1.7608351707458496
Epoch 140, training loss: 14.426102638244629 = 1.7653480768203735 + 2.0 * 6.330377101898193
Epoch 140, val loss: 1.751896619796753
Epoch 150, training loss: 14.366958618164062 = 1.7563681602478027 + 2.0 * 6.305295467376709
Epoch 150, val loss: 1.7430217266082764
Epoch 160, training loss: 14.316746711730957 = 1.746690034866333 + 2.0 * 6.285028457641602
Epoch 160, val loss: 1.7337690591812134
Epoch 170, training loss: 14.268692016601562 = 1.7361254692077637 + 2.0 * 6.2662835121154785
Epoch 170, val loss: 1.7240424156188965
Epoch 180, training loss: 14.221868515014648 = 1.7243871688842773 + 2.0 * 6.2487406730651855
Epoch 180, val loss: 1.7136403322219849
Epoch 190, training loss: 14.178377151489258 = 1.7111005783081055 + 2.0 * 6.233638286590576
Epoch 190, val loss: 1.702237606048584
Epoch 200, training loss: 14.136510848999023 = 1.695697546005249 + 2.0 * 6.220406532287598
Epoch 200, val loss: 1.6894313097000122
Epoch 210, training loss: 14.094101905822754 = 1.6776906251907349 + 2.0 * 6.208205699920654
Epoch 210, val loss: 1.6746779680252075
Epoch 220, training loss: 14.05290412902832 = 1.6565704345703125 + 2.0 * 6.198166847229004
Epoch 220, val loss: 1.6575815677642822
Epoch 230, training loss: 14.006784439086914 = 1.6316252946853638 + 2.0 * 6.18757963180542
Epoch 230, val loss: 1.6376358270645142
Epoch 240, training loss: 13.960760116577148 = 1.6022529602050781 + 2.0 * 6.179253578186035
Epoch 240, val loss: 1.6143003702163696
Epoch 250, training loss: 13.911616325378418 = 1.5678414106369019 + 2.0 * 6.171887397766113
Epoch 250, val loss: 1.5870497226715088
Epoch 260, training loss: 13.864906311035156 = 1.529068112373352 + 2.0 * 6.167919158935547
Epoch 260, val loss: 1.5566718578338623
Epoch 270, training loss: 13.808733940124512 = 1.487170934677124 + 2.0 * 6.160781383514404
Epoch 270, val loss: 1.5241467952728271
Epoch 280, training loss: 13.751562118530273 = 1.442597508430481 + 2.0 * 6.154482364654541
Epoch 280, val loss: 1.4896687269210815
Epoch 290, training loss: 13.702857971191406 = 1.3961814641952515 + 2.0 * 6.153338432312012
Epoch 290, val loss: 1.4540011882781982
Epoch 300, training loss: 13.638895034790039 = 1.3502236604690552 + 2.0 * 6.144335746765137
Epoch 300, val loss: 1.4192355871200562
Epoch 310, training loss: 13.58530044555664 = 1.305516004562378 + 2.0 * 6.139892101287842
Epoch 310, val loss: 1.385723352432251
Epoch 320, training loss: 13.531557083129883 = 1.262061357498169 + 2.0 * 6.1347479820251465
Epoch 320, val loss: 1.3535194396972656
Epoch 330, training loss: 13.495884895324707 = 1.2199653387069702 + 2.0 * 6.137959957122803
Epoch 330, val loss: 1.3226466178894043
Epoch 340, training loss: 13.434027671813965 = 1.1796391010284424 + 2.0 * 6.127194404602051
Epoch 340, val loss: 1.2935069799423218
Epoch 350, training loss: 13.388265609741211 = 1.1401654481887817 + 2.0 * 6.124050140380859
Epoch 350, val loss: 1.2652217149734497
Epoch 360, training loss: 13.34012222290039 = 1.1004562377929688 + 2.0 * 6.119832992553711
Epoch 360, val loss: 1.2368210554122925
Epoch 370, training loss: 13.297769546508789 = 1.0602096319198608 + 2.0 * 6.118780136108398
Epoch 370, val loss: 1.208109974861145
Epoch 380, training loss: 13.251973152160645 = 1.019826889038086 + 2.0 * 6.116073131561279
Epoch 380, val loss: 1.1791672706604004
Epoch 390, training loss: 13.202094078063965 = 0.9789860248565674 + 2.0 * 6.111554145812988
Epoch 390, val loss: 1.1498448848724365
Epoch 400, training loss: 13.156906127929688 = 0.9377623796463013 + 2.0 * 6.109571933746338
Epoch 400, val loss: 1.1201990842819214
Epoch 410, training loss: 13.112479209899902 = 0.8968384265899658 + 2.0 * 6.107820510864258
Epoch 410, val loss: 1.0904914140701294
Epoch 420, training loss: 13.066800117492676 = 0.8570989370346069 + 2.0 * 6.104850769042969
Epoch 420, val loss: 1.0615503787994385
Epoch 430, training loss: 13.021958351135254 = 0.8188683390617371 + 2.0 * 6.1015448570251465
Epoch 430, val loss: 1.033855676651001
Epoch 440, training loss: 12.986611366271973 = 0.7823197841644287 + 2.0 * 6.102145671844482
Epoch 440, val loss: 1.0074400901794434
Epoch 450, training loss: 12.945526123046875 = 0.7485549449920654 + 2.0 * 6.098485469818115
Epoch 450, val loss: 0.9829169511795044
Epoch 460, training loss: 12.905738830566406 = 0.7172242403030396 + 2.0 * 6.094257354736328
Epoch 460, val loss: 0.9608322978019714
Epoch 470, training loss: 12.871685981750488 = 0.6880596280097961 + 2.0 * 6.091813087463379
Epoch 470, val loss: 0.9405431151390076
Epoch 480, training loss: 12.865287780761719 = 0.660874605178833 + 2.0 * 6.102206707000732
Epoch 480, val loss: 0.9220296144485474
Epoch 490, training loss: 12.816133499145508 = 0.6357495784759521 + 2.0 * 6.090191841125488
Epoch 490, val loss: 0.9056376814842224
Epoch 500, training loss: 12.784920692443848 = 0.6122658252716064 + 2.0 * 6.08632755279541
Epoch 500, val loss: 0.8909699320793152
Epoch 510, training loss: 12.757054328918457 = 0.5899584293365479 + 2.0 * 6.083548069000244
Epoch 510, val loss: 0.8775455355644226
Epoch 520, training loss: 12.766617774963379 = 0.5687205791473389 + 2.0 * 6.0989484786987305
Epoch 520, val loss: 0.8651469349861145
Epoch 530, training loss: 12.717314720153809 = 0.5485942959785461 + 2.0 * 6.084360122680664
Epoch 530, val loss: 0.8540875315666199
Epoch 540, training loss: 12.685115814208984 = 0.5292826294898987 + 2.0 * 6.077916622161865
Epoch 540, val loss: 0.8440889120101929
Epoch 550, training loss: 12.662101745605469 = 0.5104846954345703 + 2.0 * 6.075808525085449
Epoch 550, val loss: 0.8346446752548218
Epoch 560, training loss: 12.660478591918945 = 0.49212104082107544 + 2.0 * 6.084178924560547
Epoch 560, val loss: 0.8256354331970215
Epoch 570, training loss: 12.62805461883545 = 0.4742579162120819 + 2.0 * 6.076898574829102
Epoch 570, val loss: 0.8172926902770996
Epoch 580, training loss: 12.601228713989258 = 0.4569898545742035 + 2.0 * 6.072119235992432
Epoch 580, val loss: 0.8095793128013611
Epoch 590, training loss: 12.578987121582031 = 0.4399639368057251 + 2.0 * 6.069511413574219
Epoch 590, val loss: 0.802122950553894
Epoch 600, training loss: 12.562050819396973 = 0.4231133759021759 + 2.0 * 6.0694684982299805
Epoch 600, val loss: 0.7948721647262573
Epoch 610, training loss: 12.5435791015625 = 0.4065169095993042 + 2.0 * 6.068531036376953
Epoch 610, val loss: 0.7878966927528381
Epoch 620, training loss: 12.525267601013184 = 0.39021530747413635 + 2.0 * 6.067526340484619
Epoch 620, val loss: 0.7812841534614563
Epoch 630, training loss: 12.507705688476562 = 0.37406978011131287 + 2.0 * 6.066817760467529
Epoch 630, val loss: 0.7749858498573303
Epoch 640, training loss: 12.48818302154541 = 0.35835152864456177 + 2.0 * 6.064915657043457
Epoch 640, val loss: 0.7690175771713257
Epoch 650, training loss: 12.46959400177002 = 0.3428345024585724 + 2.0 * 6.063379764556885
Epoch 650, val loss: 0.7634526491165161
Epoch 660, training loss: 12.450080871582031 = 0.32766997814178467 + 2.0 * 6.0612053871154785
Epoch 660, val loss: 0.7582834959030151
Epoch 670, training loss: 12.436176300048828 = 0.3128368556499481 + 2.0 * 6.061669826507568
Epoch 670, val loss: 0.7533870339393616
Epoch 680, training loss: 12.420644760131836 = 0.29843705892562866 + 2.0 * 6.061103820800781
Epoch 680, val loss: 0.7490131855010986
Epoch 690, training loss: 12.400501251220703 = 0.2844869792461395 + 2.0 * 6.05800724029541
Epoch 690, val loss: 0.7451627254486084
Epoch 700, training loss: 12.389670372009277 = 0.2708972096443176 + 2.0 * 6.059386730194092
Epoch 700, val loss: 0.7416495680809021
Epoch 710, training loss: 12.377157211303711 = 0.2577817738056183 + 2.0 * 6.059687614440918
Epoch 710, val loss: 0.7386845946311951
Epoch 720, training loss: 12.359355926513672 = 0.2452680766582489 + 2.0 * 6.05704402923584
Epoch 720, val loss: 0.7362453937530518
Epoch 730, training loss: 12.342826843261719 = 0.23338375985622406 + 2.0 * 6.054721355438232
Epoch 730, val loss: 0.734407901763916
Epoch 740, training loss: 12.328487396240234 = 0.2219904661178589 + 2.0 * 6.053248405456543
Epoch 740, val loss: 0.7330763936042786
Epoch 750, training loss: 12.323783874511719 = 0.2111590951681137 + 2.0 * 6.056312561035156
Epoch 750, val loss: 0.7321833968162537
Epoch 760, training loss: 12.306254386901855 = 0.20090481638908386 + 2.0 * 6.052674770355225
Epoch 760, val loss: 0.7317911386489868
Epoch 770, training loss: 12.298259735107422 = 0.1912626177072525 + 2.0 * 6.0534987449646
Epoch 770, val loss: 0.7319473028182983
Epoch 780, training loss: 12.286511421203613 = 0.18210896849632263 + 2.0 * 6.052201271057129
Epoch 780, val loss: 0.732425332069397
Epoch 790, training loss: 12.277052879333496 = 0.1735156625509262 + 2.0 * 6.051768779754639
Epoch 790, val loss: 0.7332895398139954
Epoch 800, training loss: 12.265442848205566 = 0.16536763310432434 + 2.0 * 6.050037384033203
Epoch 800, val loss: 0.7344062328338623
Epoch 810, training loss: 12.253154754638672 = 0.15768375992774963 + 2.0 * 6.047735691070557
Epoch 810, val loss: 0.7359037399291992
Epoch 820, training loss: 12.248076438903809 = 0.15041765570640564 + 2.0 * 6.048829555511475
Epoch 820, val loss: 0.737704873085022
Epoch 830, training loss: 12.237401008605957 = 0.14358322322368622 + 2.0 * 6.046908855438232
Epoch 830, val loss: 0.7398761510848999
Epoch 840, training loss: 12.225581169128418 = 0.13719463348388672 + 2.0 * 6.044193267822266
Epoch 840, val loss: 0.742506742477417
Epoch 850, training loss: 12.225624084472656 = 0.13118748366832733 + 2.0 * 6.047218322753906
Epoch 850, val loss: 0.7455136775970459
Epoch 860, training loss: 12.220523834228516 = 0.1255355179309845 + 2.0 * 6.047493934631348
Epoch 860, val loss: 0.7486760020256042
Epoch 870, training loss: 12.204490661621094 = 0.12030701339244843 + 2.0 * 6.0420918464660645
Epoch 870, val loss: 0.7522814869880676
Epoch 880, training loss: 12.1981201171875 = 0.11535831540822983 + 2.0 * 6.041380882263184
Epoch 880, val loss: 0.7561265230178833
Epoch 890, training loss: 12.191123008728027 = 0.1106768324971199 + 2.0 * 6.040223121643066
Epoch 890, val loss: 0.7601653337478638
Epoch 900, training loss: 12.21484661102295 = 0.10625655949115753 + 2.0 * 6.054295063018799
Epoch 900, val loss: 0.7643043398857117
Epoch 910, training loss: 12.181509017944336 = 0.10209324955940247 + 2.0 * 6.039707660675049
Epoch 910, val loss: 0.7684160470962524
Epoch 920, training loss: 12.175518989562988 = 0.09816718101501465 + 2.0 * 6.038675785064697
Epoch 920, val loss: 0.7727600932121277
Epoch 930, training loss: 12.177447319030762 = 0.09443767368793488 + 2.0 * 6.041504859924316
Epoch 930, val loss: 0.7771411538124084
Epoch 940, training loss: 12.166266441345215 = 0.09089972078800201 + 2.0 * 6.037683486938477
Epoch 940, val loss: 0.7816022038459778
Epoch 950, training loss: 12.160585403442383 = 0.08753090351819992 + 2.0 * 6.036527156829834
Epoch 950, val loss: 0.7861789464950562
Epoch 960, training loss: 12.164461135864258 = 0.08432414382696152 + 2.0 * 6.040068626403809
Epoch 960, val loss: 0.7907176613807678
Epoch 970, training loss: 12.150711059570312 = 0.08130332082509995 + 2.0 * 6.034703731536865
Epoch 970, val loss: 0.795311450958252
Epoch 980, training loss: 12.14845085144043 = 0.07840442657470703 + 2.0 * 6.035023212432861
Epoch 980, val loss: 0.7999640703201294
Epoch 990, training loss: 12.142579078674316 = 0.0756482258439064 + 2.0 * 6.033465385437012
Epoch 990, val loss: 0.8045977354049683
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8111
Overall ASR: 0.8635
Flip ASR: 0.8400/225 nodes
The final ASR:0.66667, 0.15546, Accuracy:0.81852, 0.01048
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11620])
remove edge: torch.Size([2, 9530])
updated graph: torch.Size([2, 10594])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8519
Overall ASR: 0.9705
Flip ASR: 0.9644/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9852
Flip ASR: 0.9822/225 nodes
The final ASR:0.97663, 0.00627, Accuracy:0.83704, 0.01210
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.4 der2:0.3 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 18.676666259765625 = 1.9289096593856812 + 2.0 * 8.373878479003906
Epoch 0, val loss: 1.9325271844863892
Epoch 10, training loss: 18.666851043701172 = 1.9200193881988525 + 2.0 * 8.37341594696045
Epoch 10, val loss: 1.9233362674713135
Epoch 20, training loss: 18.649248123168945 = 1.9087146520614624 + 2.0 * 8.370266914367676
Epoch 20, val loss: 1.9114235639572144
Epoch 30, training loss: 18.589330673217773 = 1.8928158283233643 + 2.0 * 8.348257064819336
Epoch 30, val loss: 1.8947255611419678
Epoch 40, training loss: 18.259836196899414 = 1.8738555908203125 + 2.0 * 8.19299030303955
Epoch 40, val loss: 1.876083493232727
Epoch 50, training loss: 17.229455947875977 = 1.8535053730010986 + 2.0 * 7.68797492980957
Epoch 50, val loss: 1.8559619188308716
Epoch 60, training loss: 16.359344482421875 = 1.8364379405975342 + 2.0 * 7.261453628540039
Epoch 60, val loss: 1.8399658203125
Epoch 70, training loss: 15.640063285827637 = 1.8260117769241333 + 2.0 * 6.9070258140563965
Epoch 70, val loss: 1.8301982879638672
Epoch 80, training loss: 15.309850692749023 = 1.8162922859191895 + 2.0 * 6.746779441833496
Epoch 80, val loss: 1.820436954498291
Epoch 90, training loss: 15.046210289001465 = 1.8036459684371948 + 2.0 * 6.62128210067749
Epoch 90, val loss: 1.8082787990570068
Epoch 100, training loss: 14.85083293914795 = 1.7933505773544312 + 2.0 * 6.528741359710693
Epoch 100, val loss: 1.798540711402893
Epoch 110, training loss: 14.714238166809082 = 1.7833529710769653 + 2.0 * 6.465442657470703
Epoch 110, val loss: 1.7890205383300781
Epoch 120, training loss: 14.602380752563477 = 1.7728582620620728 + 2.0 * 6.414761066436768
Epoch 120, val loss: 1.7793521881103516
Epoch 130, training loss: 14.511293411254883 = 1.7618193626403809 + 2.0 * 6.37473726272583
Epoch 130, val loss: 1.7692948579788208
Epoch 140, training loss: 14.433795928955078 = 1.7496147155761719 + 2.0 * 6.342090606689453
Epoch 140, val loss: 1.758562445640564
Epoch 150, training loss: 14.361739158630371 = 1.7358982563018799 + 2.0 * 6.312920570373535
Epoch 150, val loss: 1.7469145059585571
Epoch 160, training loss: 14.299781799316406 = 1.72041654586792 + 2.0 * 6.289682865142822
Epoch 160, val loss: 1.734028935432434
Epoch 170, training loss: 14.240093231201172 = 1.7028049230575562 + 2.0 * 6.268644332885742
Epoch 170, val loss: 1.7194310426712036
Epoch 180, training loss: 14.187783241271973 = 1.6826173067092896 + 2.0 * 6.252583026885986
Epoch 180, val loss: 1.7028591632843018
Epoch 190, training loss: 14.133538246154785 = 1.6596510410308838 + 2.0 * 6.23694372177124
Epoch 190, val loss: 1.6839407682418823
Epoch 200, training loss: 14.08386516571045 = 1.633372187614441 + 2.0 * 6.225246429443359
Epoch 200, val loss: 1.662394404411316
Epoch 210, training loss: 14.032293319702148 = 1.6035199165344238 + 2.0 * 6.214386463165283
Epoch 210, val loss: 1.6381765604019165
Epoch 220, training loss: 13.97710132598877 = 1.5700960159301758 + 2.0 * 6.203502655029297
Epoch 220, val loss: 1.611079454421997
Epoch 230, training loss: 13.921537399291992 = 1.5326086282730103 + 2.0 * 6.194464206695557
Epoch 230, val loss: 1.5808671712875366
Epoch 240, training loss: 13.889376640319824 = 1.4909573793411255 + 2.0 * 6.199209690093994
Epoch 240, val loss: 1.5476408004760742
Epoch 250, training loss: 13.812261581420898 = 1.446738839149475 + 2.0 * 6.182761192321777
Epoch 250, val loss: 1.5121592283248901
Epoch 260, training loss: 13.746944427490234 = 1.3996076583862305 + 2.0 * 6.173668384552002
Epoch 260, val loss: 1.4750404357910156
Epoch 270, training loss: 13.683417320251465 = 1.3502442836761475 + 2.0 * 6.166586399078369
Epoch 270, val loss: 1.4363059997558594
Epoch 280, training loss: 13.61982250213623 = 1.299088716506958 + 2.0 * 6.160367012023926
Epoch 280, val loss: 1.3966565132141113
Epoch 290, training loss: 13.571958541870117 = 1.2470223903656006 + 2.0 * 6.162467956542969
Epoch 290, val loss: 1.3567253351211548
Epoch 300, training loss: 13.496888160705566 = 1.195717215538025 + 2.0 * 6.150585651397705
Epoch 300, val loss: 1.3176119327545166
Epoch 310, training loss: 13.436500549316406 = 1.1454845666885376 + 2.0 * 6.1455078125
Epoch 310, val loss: 1.279852032661438
Epoch 320, training loss: 13.38240909576416 = 1.096548080444336 + 2.0 * 6.142930507659912
Epoch 320, val loss: 1.2435206174850464
Epoch 330, training loss: 13.325920104980469 = 1.0494999885559082 + 2.0 * 6.138209819793701
Epoch 330, val loss: 1.2087702751159668
Epoch 340, training loss: 13.270085334777832 = 1.0044156312942505 + 2.0 * 6.1328349113464355
Epoch 340, val loss: 1.17593252658844
Epoch 350, training loss: 13.225823402404785 = 0.9612024426460266 + 2.0 * 6.132310390472412
Epoch 350, val loss: 1.1449270248413086
Epoch 360, training loss: 13.171520233154297 = 0.9200921058654785 + 2.0 * 6.12571382522583
Epoch 360, val loss: 1.115430474281311
Epoch 370, training loss: 13.123459815979004 = 0.8807815909385681 + 2.0 * 6.121339321136475
Epoch 370, val loss: 1.0876502990722656
Epoch 380, training loss: 13.078988075256348 = 0.8430131077766418 + 2.0 * 6.117987632751465
Epoch 380, val loss: 1.0612136125564575
Epoch 390, training loss: 13.053494453430176 = 0.8066741228103638 + 2.0 * 6.123410224914551
Epoch 390, val loss: 1.0359667539596558
Epoch 400, training loss: 12.999113082885742 = 0.7719613313674927 + 2.0 * 6.1135759353637695
Epoch 400, val loss: 1.0122950077056885
Epoch 410, training loss: 12.959540367126465 = 0.7390545010566711 + 2.0 * 6.11024284362793
Epoch 410, val loss: 0.9903545379638672
Epoch 420, training loss: 12.920252799987793 = 0.7075098752975464 + 2.0 * 6.1063714027404785
Epoch 420, val loss: 0.9697390794754028
Epoch 430, training loss: 12.88563346862793 = 0.6773017644882202 + 2.0 * 6.104166030883789
Epoch 430, val loss: 0.9505568742752075
Epoch 440, training loss: 12.8555269241333 = 0.6486653089523315 + 2.0 * 6.10343074798584
Epoch 440, val loss: 0.9328086972236633
Epoch 450, training loss: 12.821837425231934 = 0.6217988133430481 + 2.0 * 6.100019454956055
Epoch 450, val loss: 0.9169874787330627
Epoch 460, training loss: 12.790644645690918 = 0.5965790152549744 + 2.0 * 6.0970330238342285
Epoch 460, val loss: 0.9028492569923401
Epoch 470, training loss: 12.776166915893555 = 0.5729299187660217 + 2.0 * 6.10161828994751
Epoch 470, val loss: 0.8903116583824158
Epoch 480, training loss: 12.736350059509277 = 0.5509881377220154 + 2.0 * 6.092680931091309
Epoch 480, val loss: 0.8794322609901428
Epoch 490, training loss: 12.711713790893555 = 0.5305140614509583 + 2.0 * 6.09060001373291
Epoch 490, val loss: 0.8701625466346741
Epoch 500, training loss: 12.68964672088623 = 0.51133793592453 + 2.0 * 6.089154243469238
Epoch 500, val loss: 0.8622370958328247
Epoch 510, training loss: 12.66963005065918 = 0.49334588646888733 + 2.0 * 6.088141918182373
Epoch 510, val loss: 0.8555552959442139
Epoch 520, training loss: 12.645465850830078 = 0.47639867663383484 + 2.0 * 6.08453369140625
Epoch 520, val loss: 0.85003262758255
Epoch 530, training loss: 12.627351760864258 = 0.46029630303382874 + 2.0 * 6.083527565002441
Epoch 530, val loss: 0.845368504524231
Epoch 540, training loss: 12.609122276306152 = 0.44483059644699097 + 2.0 * 6.082145690917969
Epoch 540, val loss: 0.8414500951766968
Epoch 550, training loss: 12.589022636413574 = 0.42984673380851746 + 2.0 * 6.079587936401367
Epoch 550, val loss: 0.8382203578948975
Epoch 560, training loss: 12.570608139038086 = 0.41524001955986023 + 2.0 * 6.077683925628662
Epoch 560, val loss: 0.8354566693305969
Epoch 570, training loss: 12.562506675720215 = 0.40081122517585754 + 2.0 * 6.08084774017334
Epoch 570, val loss: 0.8328595757484436
Epoch 580, training loss: 12.5352144241333 = 0.3867000639438629 + 2.0 * 6.0742573738098145
Epoch 580, val loss: 0.8307143449783325
Epoch 590, training loss: 12.517497062683105 = 0.3726954758167267 + 2.0 * 6.0724005699157715
Epoch 590, val loss: 0.8287734389305115
Epoch 600, training loss: 12.512462615966797 = 0.3587642014026642 + 2.0 * 6.076848983764648
Epoch 600, val loss: 0.8270222544670105
Epoch 610, training loss: 12.487799644470215 = 0.3450658917427063 + 2.0 * 6.071366786956787
Epoch 610, val loss: 0.8253294825553894
Epoch 620, training loss: 12.467606544494629 = 0.3314780294895172 + 2.0 * 6.068064212799072
Epoch 620, val loss: 0.8239602446556091
Epoch 630, training loss: 12.450772285461426 = 0.31815165281295776 + 2.0 * 6.066310405731201
Epoch 630, val loss: 0.8227810859680176
Epoch 640, training loss: 12.453282356262207 = 0.30508893728256226 + 2.0 * 6.0740966796875
Epoch 640, val loss: 0.8219123482704163
Epoch 650, training loss: 12.422453880310059 = 0.2923550307750702 + 2.0 * 6.065049648284912
Epoch 650, val loss: 0.8210704326629639
Epoch 660, training loss: 12.405313491821289 = 0.280047208070755 + 2.0 * 6.062633037567139
Epoch 660, val loss: 0.8206175565719604
Epoch 670, training loss: 12.390295028686523 = 0.26810702681541443 + 2.0 * 6.061093807220459
Epoch 670, val loss: 0.820472002029419
Epoch 680, training loss: 12.38561725616455 = 0.2565224766731262 + 2.0 * 6.064547538757324
Epoch 680, val loss: 0.8205527663230896
Epoch 690, training loss: 12.375242233276367 = 0.2453174591064453 + 2.0 * 6.064962387084961
Epoch 690, val loss: 0.8209362626075745
Epoch 700, training loss: 12.351737976074219 = 0.2345276176929474 + 2.0 * 6.058605194091797
Epoch 700, val loss: 0.821617066860199
Epoch 710, training loss: 12.342392921447754 = 0.22408711910247803 + 2.0 * 6.059153079986572
Epoch 710, val loss: 0.8225451707839966
Epoch 720, training loss: 12.326037406921387 = 0.21405407786369324 + 2.0 * 6.0559916496276855
Epoch 720, val loss: 0.8236075043678284
Epoch 730, training loss: 12.31270694732666 = 0.20426005125045776 + 2.0 * 6.054223537445068
Epoch 730, val loss: 0.8249140381813049
Epoch 740, training loss: 12.30119800567627 = 0.1947866827249527 + 2.0 * 6.053205490112305
Epoch 740, val loss: 0.8263827562332153
Epoch 750, training loss: 12.295019149780273 = 0.18559008836746216 + 2.0 * 6.054714679718018
Epoch 750, val loss: 0.8280502557754517
Epoch 760, training loss: 12.286673545837402 = 0.1767134964466095 + 2.0 * 6.0549798011779785
Epoch 760, val loss: 0.8300442695617676
Epoch 770, training loss: 12.269756317138672 = 0.16813746094703674 + 2.0 * 6.050809383392334
Epoch 770, val loss: 0.8319721221923828
Epoch 780, training loss: 12.272915840148926 = 0.15991109609603882 + 2.0 * 6.056502342224121
Epoch 780, val loss: 0.8342739343643188
Epoch 790, training loss: 12.254130363464355 = 0.15193834900856018 + 2.0 * 6.051095962524414
Epoch 790, val loss: 0.8366377353668213
Epoch 800, training loss: 12.238956451416016 = 0.1443670094013214 + 2.0 * 6.047294616699219
Epoch 800, val loss: 0.8392394781112671
Epoch 810, training loss: 12.22894287109375 = 0.137080579996109 + 2.0 * 6.045931339263916
Epoch 810, val loss: 0.8421255946159363
Epoch 820, training loss: 12.22110652923584 = 0.13012024760246277 + 2.0 * 6.045493125915527
Epoch 820, val loss: 0.8451538681983948
Epoch 830, training loss: 12.22012996673584 = 0.12348850071430206 + 2.0 * 6.048320770263672
Epoch 830, val loss: 0.8483615517616272
Epoch 840, training loss: 12.205102920532227 = 0.11719465255737305 + 2.0 * 6.043953895568848
Epoch 840, val loss: 0.8517314195632935
Epoch 850, training loss: 12.198127746582031 = 0.11124186217784882 + 2.0 * 6.043442726135254
Epoch 850, val loss: 0.855339527130127
Epoch 860, training loss: 12.196576118469238 = 0.10561536252498627 + 2.0 * 6.045480251312256
Epoch 860, val loss: 0.859071671962738
Epoch 870, training loss: 12.18352222442627 = 0.1002747043967247 + 2.0 * 6.041623592376709
Epoch 870, val loss: 0.8631029725074768
Epoch 880, training loss: 12.17575454711914 = 0.09525652229785919 + 2.0 * 6.040248870849609
Epoch 880, val loss: 0.8671675324440002
Epoch 890, training loss: 12.181554794311523 = 0.0905321016907692 + 2.0 * 6.045511245727539
Epoch 890, val loss: 0.8713057041168213
Epoch 900, training loss: 12.16396713256836 = 0.08606715500354767 + 2.0 * 6.038949966430664
Epoch 900, val loss: 0.8756512403488159
Epoch 910, training loss: 12.15809440612793 = 0.08187299221754074 + 2.0 * 6.038110733032227
Epoch 910, val loss: 0.8801917433738708
Epoch 920, training loss: 12.151108741760254 = 0.07795174419879913 + 2.0 * 6.03657865524292
Epoch 920, val loss: 0.8848716020584106
Epoch 930, training loss: 12.1578950881958 = 0.07424306124448776 + 2.0 * 6.041826248168945
Epoch 930, val loss: 0.8896476626396179
Epoch 940, training loss: 12.151167869567871 = 0.07075966149568558 + 2.0 * 6.040204048156738
Epoch 940, val loss: 0.8942518830299377
Epoch 950, training loss: 12.13687801361084 = 0.067496158182621 + 2.0 * 6.034690856933594
Epoch 950, val loss: 0.8991787433624268
Epoch 960, training loss: 12.13204574584961 = 0.06443720310926437 + 2.0 * 6.033804416656494
Epoch 960, val loss: 0.904052197933197
Epoch 970, training loss: 12.130864143371582 = 0.061559684574604034 + 2.0 * 6.034652233123779
Epoch 970, val loss: 0.9089952111244202
Epoch 980, training loss: 12.12443733215332 = 0.05884464830160141 + 2.0 * 6.032796382904053
Epoch 980, val loss: 0.9139705300331116
Epoch 990, training loss: 12.120462417602539 = 0.056290991604328156 + 2.0 * 6.03208589553833
Epoch 990, val loss: 0.9189193248748779
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7556
Overall ASR: 0.5756
Flip ASR: 0.4978/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.69618797302246 = 1.948358178138733 + 2.0 * 8.37391471862793
Epoch 0, val loss: 1.9517892599105835
Epoch 10, training loss: 18.68543815612793 = 1.9382117986679077 + 2.0 * 8.373613357543945
Epoch 10, val loss: 1.941197395324707
Epoch 20, training loss: 18.66865348815918 = 1.925569772720337 + 2.0 * 8.371541976928711
Epoch 20, val loss: 1.9276243448257446
Epoch 30, training loss: 18.6220760345459 = 1.9080113172531128 + 2.0 * 8.357032775878906
Epoch 30, val loss: 1.90859854221344
Epoch 40, training loss: 18.443519592285156 = 1.885680913925171 + 2.0 * 8.278919219970703
Epoch 40, val loss: 1.8851537704467773
Epoch 50, training loss: 17.66492462158203 = 1.8642573356628418 + 2.0 * 7.900333404541016
Epoch 50, val loss: 1.8633999824523926
Epoch 60, training loss: 16.75505828857422 = 1.8439264297485352 + 2.0 * 7.45556640625
Epoch 60, val loss: 1.8430720567703247
Epoch 70, training loss: 16.04759979248047 = 1.8293111324310303 + 2.0 * 7.10914421081543
Epoch 70, val loss: 1.829014539718628
Epoch 80, training loss: 15.59398365020752 = 1.814160943031311 + 2.0 * 6.88991117477417
Epoch 80, val loss: 1.8129708766937256
Epoch 90, training loss: 15.207406044006348 = 1.7987521886825562 + 2.0 * 6.70432710647583
Epoch 90, val loss: 1.797694206237793
Epoch 100, training loss: 14.95305061340332 = 1.7841999530792236 + 2.0 * 6.584425449371338
Epoch 100, val loss: 1.7832194566726685
Epoch 110, training loss: 14.778464317321777 = 1.768747091293335 + 2.0 * 6.504858493804932
Epoch 110, val loss: 1.7683770656585693
Epoch 120, training loss: 14.641091346740723 = 1.752035140991211 + 2.0 * 6.444528102874756
Epoch 120, val loss: 1.7530189752578735
Epoch 130, training loss: 14.534871101379395 = 1.7344655990600586 + 2.0 * 6.400202751159668
Epoch 130, val loss: 1.7372050285339355
Epoch 140, training loss: 14.45650577545166 = 1.7155019044876099 + 2.0 * 6.37050199508667
Epoch 140, val loss: 1.7202481031417847
Epoch 150, training loss: 14.378948211669922 = 1.6943615674972534 + 2.0 * 6.3422932624816895
Epoch 150, val loss: 1.70195472240448
Epoch 160, training loss: 14.31130313873291 = 1.6707344055175781 + 2.0 * 6.320284366607666
Epoch 160, val loss: 1.6819640398025513
Epoch 170, training loss: 14.245224952697754 = 1.6441317796707153 + 2.0 * 6.300546646118164
Epoch 170, val loss: 1.6599760055541992
Epoch 180, training loss: 14.182008743286133 = 1.6142027378082275 + 2.0 * 6.283903121948242
Epoch 180, val loss: 1.6355177164077759
Epoch 190, training loss: 14.115153312683105 = 1.5807546377182007 + 2.0 * 6.267199516296387
Epoch 190, val loss: 1.6085243225097656
Epoch 200, training loss: 14.049022674560547 = 1.5437225103378296 + 2.0 * 6.252650260925293
Epoch 200, val loss: 1.5788058042526245
Epoch 210, training loss: 13.986884117126465 = 1.5027799606323242 + 2.0 * 6.24205207824707
Epoch 210, val loss: 1.545991063117981
Epoch 220, training loss: 13.915464401245117 = 1.4581303596496582 + 2.0 * 6.228667259216309
Epoch 220, val loss: 1.510596513748169
Epoch 230, training loss: 13.848362922668457 = 1.4102795124053955 + 2.0 * 6.21904182434082
Epoch 230, val loss: 1.472774624824524
Epoch 240, training loss: 13.780980110168457 = 1.359671950340271 + 2.0 * 6.210654258728027
Epoch 240, val loss: 1.4331984519958496
Epoch 250, training loss: 13.715746879577637 = 1.3078712224960327 + 2.0 * 6.203938007354736
Epoch 250, val loss: 1.3931211233139038
Epoch 260, training loss: 13.64611530303955 = 1.2558180093765259 + 2.0 * 6.195148468017578
Epoch 260, val loss: 1.3533920049667358
Epoch 270, training loss: 13.580764770507812 = 1.2043304443359375 + 2.0 * 6.1882171630859375
Epoch 270, val loss: 1.3147315979003906
Epoch 280, training loss: 13.518054962158203 = 1.1541013717651367 + 2.0 * 6.181976795196533
Epoch 280, val loss: 1.2775803804397583
Epoch 290, training loss: 13.462955474853516 = 1.1060807704925537 + 2.0 * 6.178437232971191
Epoch 290, val loss: 1.2425936460494995
Epoch 300, training loss: 13.404081344604492 = 1.061015009880066 + 2.0 * 6.171533107757568
Epoch 300, val loss: 1.210600733757019
Epoch 310, training loss: 13.34954833984375 = 1.0185986757278442 + 2.0 * 6.165474891662598
Epoch 310, val loss: 1.1811587810516357
Epoch 320, training loss: 13.311908721923828 = 0.978556215763092 + 2.0 * 6.166676044464111
Epoch 320, val loss: 1.1540815830230713
Epoch 330, training loss: 13.255460739135742 = 0.941443920135498 + 2.0 * 6.157008647918701
Epoch 330, val loss: 1.1293377876281738
Epoch 340, training loss: 13.210386276245117 = 0.9065569639205933 + 2.0 * 6.151914596557617
Epoch 340, val loss: 1.106717824935913
Epoch 350, training loss: 13.17078971862793 = 0.873616099357605 + 2.0 * 6.148586750030518
Epoch 350, val loss: 1.0858792066574097
Epoch 360, training loss: 13.133788108825684 = 0.8425320386886597 + 2.0 * 6.145627975463867
Epoch 360, val loss: 1.0666924715042114
Epoch 370, training loss: 13.092428207397461 = 0.8132174611091614 + 2.0 * 6.139605522155762
Epoch 370, val loss: 1.04901123046875
Epoch 380, training loss: 13.057209014892578 = 0.7852028012275696 + 2.0 * 6.136003017425537
Epoch 380, val loss: 1.032534122467041
Epoch 390, training loss: 13.029671669006348 = 0.7582014203071594 + 2.0 * 6.135735034942627
Epoch 390, val loss: 1.0170719623565674
Epoch 400, training loss: 12.994384765625 = 0.7320894598960876 + 2.0 * 6.131147861480713
Epoch 400, val loss: 1.0023455619812012
Epoch 410, training loss: 12.95974349975586 = 0.7066922783851624 + 2.0 * 6.126525402069092
Epoch 410, val loss: 0.9882516264915466
Epoch 420, training loss: 12.930336952209473 = 0.6817271709442139 + 2.0 * 6.12430477142334
Epoch 420, val loss: 0.974525511264801
Epoch 430, training loss: 12.901559829711914 = 0.6571319699287415 + 2.0 * 6.122213840484619
Epoch 430, val loss: 0.9610422849655151
Epoch 440, training loss: 12.870356559753418 = 0.6328275799751282 + 2.0 * 6.118764400482178
Epoch 440, val loss: 0.9478005766868591
Epoch 450, training loss: 12.841168403625488 = 0.6086225509643555 + 2.0 * 6.116272926330566
Epoch 450, val loss: 0.934695839881897
Epoch 460, training loss: 12.825063705444336 = 0.5845303535461426 + 2.0 * 6.120266914367676
Epoch 460, val loss: 0.9215933084487915
Epoch 470, training loss: 12.78565502166748 = 0.5604852437973022 + 2.0 * 6.112585067749023
Epoch 470, val loss: 0.9086738228797913
Epoch 480, training loss: 12.757314682006836 = 0.5367258787155151 + 2.0 * 6.110294342041016
Epoch 480, val loss: 0.8959028720855713
Epoch 490, training loss: 12.732282638549805 = 0.5131855607032776 + 2.0 * 6.109548568725586
Epoch 490, val loss: 0.8833097815513611
Epoch 500, training loss: 12.70191764831543 = 0.48999449610710144 + 2.0 * 6.105961799621582
Epoch 500, val loss: 0.8708993196487427
Epoch 510, training loss: 12.6812744140625 = 0.46723154187202454 + 2.0 * 6.107021331787109
Epoch 510, val loss: 0.8589063882827759
Epoch 520, training loss: 12.654097557067871 = 0.445159375667572 + 2.0 * 6.104469299316406
Epoch 520, val loss: 0.8472309708595276
Epoch 530, training loss: 12.625137329101562 = 0.4238041639328003 + 2.0 * 6.100666522979736
Epoch 530, val loss: 0.8362683653831482
Epoch 540, training loss: 12.600318908691406 = 0.40316787362098694 + 2.0 * 6.098575592041016
Epoch 540, val loss: 0.8258486986160278
Epoch 550, training loss: 12.586636543273926 = 0.3833172917366028 + 2.0 * 6.101659774780273
Epoch 550, val loss: 0.815980851650238
Epoch 560, training loss: 12.553908348083496 = 0.3643617630004883 + 2.0 * 6.094773292541504
Epoch 560, val loss: 0.8067746162414551
Epoch 570, training loss: 12.53348159790039 = 0.3462180197238922 + 2.0 * 6.093631744384766
Epoch 570, val loss: 0.7981864809989929
Epoch 580, training loss: 12.525333404541016 = 0.3288199305534363 + 2.0 * 6.098256587982178
Epoch 580, val loss: 0.7901671528816223
Epoch 590, training loss: 12.492765426635742 = 0.3123834431171417 + 2.0 * 6.090190887451172
Epoch 590, val loss: 0.7827608585357666
Epoch 600, training loss: 12.473590850830078 = 0.2966892719268799 + 2.0 * 6.088450908660889
Epoch 600, val loss: 0.7759618759155273
Epoch 610, training loss: 12.464019775390625 = 0.28167760372161865 + 2.0 * 6.0911712646484375
Epoch 610, val loss: 0.7697468996047974
Epoch 620, training loss: 12.443039894104004 = 0.2674392759799957 + 2.0 * 6.0878005027771
Epoch 620, val loss: 0.7640137672424316
Epoch 630, training loss: 12.423184394836426 = 0.2539431154727936 + 2.0 * 6.084620475769043
Epoch 630, val loss: 0.758944034576416
Epoch 640, training loss: 12.406286239624023 = 0.24103769659996033 + 2.0 * 6.082624435424805
Epoch 640, val loss: 0.7544039487838745
Epoch 650, training loss: 12.390240669250488 = 0.22871528565883636 + 2.0 * 6.08076286315918
Epoch 650, val loss: 0.7503243088722229
Epoch 660, training loss: 12.396910667419434 = 0.21693556010723114 + 2.0 * 6.089987754821777
Epoch 660, val loss: 0.7466555237770081
Epoch 670, training loss: 12.368075370788574 = 0.20579400658607483 + 2.0 * 6.081140518188477
Epoch 670, val loss: 0.7434718608856201
Epoch 680, training loss: 12.351034164428711 = 0.1952229142189026 + 2.0 * 6.077905654907227
Epoch 680, val loss: 0.7407923340797424
Epoch 690, training loss: 12.337983131408691 = 0.18516387045383453 + 2.0 * 6.076409816741943
Epoch 690, val loss: 0.7385662794113159
Epoch 700, training loss: 12.331977844238281 = 0.17562179267406464 + 2.0 * 6.0781779289245605
Epoch 700, val loss: 0.7367838621139526
Epoch 710, training loss: 12.323655128479004 = 0.1665612757205963 + 2.0 * 6.07854700088501
Epoch 710, val loss: 0.7353147268295288
Epoch 720, training loss: 12.304558753967285 = 0.1580067127943039 + 2.0 * 6.073276042938232
Epoch 720, val loss: 0.7343359589576721
Epoch 730, training loss: 12.291779518127441 = 0.14990225434303284 + 2.0 * 6.070938587188721
Epoch 730, val loss: 0.7337490320205688
Epoch 740, training loss: 12.28140640258789 = 0.1422136276960373 + 2.0 * 6.069596290588379
Epoch 740, val loss: 0.7334758639335632
Epoch 750, training loss: 12.271871566772461 = 0.1349155157804489 + 2.0 * 6.068478107452393
Epoch 750, val loss: 0.7335240244865417
Epoch 760, training loss: 12.290329933166504 = 0.12802666425704956 + 2.0 * 6.081151485443115
Epoch 760, val loss: 0.7338739633560181
Epoch 770, training loss: 12.25619125366211 = 0.12151943147182465 + 2.0 * 6.067336082458496
Epoch 770, val loss: 0.7345188856124878
Epoch 780, training loss: 12.247109413146973 = 0.11542370915412903 + 2.0 * 6.065842628479004
Epoch 780, val loss: 0.7355183959007263
Epoch 790, training loss: 12.240073204040527 = 0.10967301577329636 + 2.0 * 6.065200328826904
Epoch 790, val loss: 0.7367332577705383
Epoch 800, training loss: 12.230878829956055 = 0.10425453633069992 + 2.0 * 6.06331205368042
Epoch 800, val loss: 0.7381590604782104
Epoch 810, training loss: 12.236562728881836 = 0.09914340078830719 + 2.0 * 6.068709850311279
Epoch 810, val loss: 0.7397252917289734
Epoch 820, training loss: 12.216279029846191 = 0.09437321871519089 + 2.0 * 6.060953140258789
Epoch 820, val loss: 0.7414935827255249
Epoch 830, training loss: 12.211368560791016 = 0.08989165723323822 + 2.0 * 6.060738563537598
Epoch 830, val loss: 0.7435333728790283
Epoch 840, training loss: 12.203243255615234 = 0.08567717671394348 + 2.0 * 6.058783054351807
Epoch 840, val loss: 0.7457414269447327
Epoch 850, training loss: 12.206945419311523 = 0.08171170204877853 + 2.0 * 6.06261682510376
Epoch 850, val loss: 0.7480692267417908
Epoch 860, training loss: 12.205920219421387 = 0.07801614701747894 + 2.0 * 6.0639519691467285
Epoch 860, val loss: 0.750479519367218
Epoch 870, training loss: 12.190644264221191 = 0.07456760853528976 + 2.0 * 6.058038234710693
Epoch 870, val loss: 0.7530909776687622
Epoch 880, training loss: 12.182762145996094 = 0.07132538408041 + 2.0 * 6.055718421936035
Epoch 880, val loss: 0.755814790725708
Epoch 890, training loss: 12.183917045593262 = 0.06827958673238754 + 2.0 * 6.05781888961792
Epoch 890, val loss: 0.7586278915405273
Epoch 900, training loss: 12.17369556427002 = 0.06539633870124817 + 2.0 * 6.054149627685547
Epoch 900, val loss: 0.761497437953949
Epoch 910, training loss: 12.169912338256836 = 0.0627012774348259 + 2.0 * 6.053605556488037
Epoch 910, val loss: 0.7645366787910461
Epoch 920, training loss: 12.172470092773438 = 0.06014973297715187 + 2.0 * 6.056159973144531
Epoch 920, val loss: 0.7676087617874146
Epoch 930, training loss: 12.162149429321289 = 0.057744864374399185 + 2.0 * 6.052202224731445
Epoch 930, val loss: 0.7706418037414551
Epoch 940, training loss: 12.158550262451172 = 0.05547862499952316 + 2.0 * 6.051535606384277
Epoch 940, val loss: 0.7738706469535828
Epoch 950, training loss: 12.15252685546875 = 0.05332975834608078 + 2.0 * 6.049598693847656
Epoch 950, val loss: 0.7770829200744629
Epoch 960, training loss: 12.150887489318848 = 0.05129925534129143 + 2.0 * 6.0497941970825195
Epoch 960, val loss: 0.7803680896759033
Epoch 970, training loss: 12.147689819335938 = 0.04937916249036789 + 2.0 * 6.049155235290527
Epoch 970, val loss: 0.783706545829773
Epoch 980, training loss: 12.144238471984863 = 0.04756193980574608 + 2.0 * 6.048338413238525
Epoch 980, val loss: 0.7871151566505432
Epoch 990, training loss: 12.138323783874512 = 0.0458359532058239 + 2.0 * 6.046244144439697
Epoch 990, val loss: 0.7905287742614746
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7963
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.70692253112793 = 1.9591413736343384 + 2.0 * 8.37389087677002
Epoch 0, val loss: 1.9662089347839355
Epoch 10, training loss: 18.695650100708008 = 1.9486041069030762 + 2.0 * 8.373522758483887
Epoch 10, val loss: 1.9557169675827026
Epoch 20, training loss: 18.677860260009766 = 1.9359130859375 + 2.0 * 8.370973587036133
Epoch 20, val loss: 1.9424537420272827
Epoch 30, training loss: 18.624969482421875 = 1.9188551902770996 + 2.0 * 8.353056907653809
Epoch 30, val loss: 1.924139142036438
Epoch 40, training loss: 18.36182403564453 = 1.8978077173233032 + 2.0 * 8.23200798034668
Epoch 40, val loss: 1.9021662473678589
Epoch 50, training loss: 17.189680099487305 = 1.876967191696167 + 2.0 * 7.656356334686279
Epoch 50, val loss: 1.8806233406066895
Epoch 60, training loss: 16.31333351135254 = 1.8593207597732544 + 2.0 * 7.227005958557129
Epoch 60, val loss: 1.8629515171051025
Epoch 70, training loss: 15.70159912109375 = 1.8443416357040405 + 2.0 * 6.928628921508789
Epoch 70, val loss: 1.8472187519073486
Epoch 80, training loss: 15.353583335876465 = 1.8293391466140747 + 2.0 * 6.76212215423584
Epoch 80, val loss: 1.831496000289917
Epoch 90, training loss: 15.08433723449707 = 1.813612699508667 + 2.0 * 6.635362148284912
Epoch 90, val loss: 1.815316081047058
Epoch 100, training loss: 14.882872581481934 = 1.7979843616485596 + 2.0 * 6.542444229125977
Epoch 100, val loss: 1.7993059158325195
Epoch 110, training loss: 14.732738494873047 = 1.7820757627487183 + 2.0 * 6.4753313064575195
Epoch 110, val loss: 1.7828278541564941
Epoch 120, training loss: 14.604021072387695 = 1.7655115127563477 + 2.0 * 6.419254779815674
Epoch 120, val loss: 1.765600323677063
Epoch 130, training loss: 14.510784149169922 = 1.7480177879333496 + 2.0 * 6.381382942199707
Epoch 130, val loss: 1.747802734375
Epoch 140, training loss: 14.41318130493164 = 1.729515552520752 + 2.0 * 6.341832637786865
Epoch 140, val loss: 1.7293792963027954
Epoch 150, training loss: 14.336345672607422 = 1.7095379829406738 + 2.0 * 6.313403606414795
Epoch 150, val loss: 1.7102161645889282
Epoch 160, training loss: 14.265846252441406 = 1.687849998474121 + 2.0 * 6.288998126983643
Epoch 160, val loss: 1.6900581121444702
Epoch 170, training loss: 14.214744567871094 = 1.6637651920318604 + 2.0 * 6.275489807128906
Epoch 170, val loss: 1.668483018875122
Epoch 180, training loss: 14.140588760375977 = 1.637378454208374 + 2.0 * 6.251605033874512
Epoch 180, val loss: 1.6453365087509155
Epoch 190, training loss: 14.080331802368164 = 1.6081218719482422 + 2.0 * 6.236104965209961
Epoch 190, val loss: 1.6203140020370483
Epoch 200, training loss: 14.022026062011719 = 1.5755125284194946 + 2.0 * 6.223256587982178
Epoch 200, val loss: 1.592828392982483
Epoch 210, training loss: 13.968610763549805 = 1.5394642353057861 + 2.0 * 6.214573383331299
Epoch 210, val loss: 1.5629966259002686
Epoch 220, training loss: 13.904824256896973 = 1.5000663995742798 + 2.0 * 6.202378749847412
Epoch 220, val loss: 1.5306587219238281
Epoch 230, training loss: 13.843027114868164 = 1.4570388793945312 + 2.0 * 6.192994117736816
Epoch 230, val loss: 1.495889663696289
Epoch 240, training loss: 13.791579246520996 = 1.4107999801635742 + 2.0 * 6.190389633178711
Epoch 240, val loss: 1.4590657949447632
Epoch 250, training loss: 13.720588684082031 = 1.3622232675552368 + 2.0 * 6.179182529449463
Epoch 250, val loss: 1.4210326671600342
Epoch 260, training loss: 13.653843879699707 = 1.311626672744751 + 2.0 * 6.171108722686768
Epoch 260, val loss: 1.3819692134857178
Epoch 270, training loss: 13.58938980102539 = 1.2591303586959839 + 2.0 * 6.165129661560059
Epoch 270, val loss: 1.3421646356582642
Epoch 280, training loss: 13.533258438110352 = 1.2053627967834473 + 2.0 * 6.163948059082031
Epoch 280, val loss: 1.3023629188537598
Epoch 290, training loss: 13.460919380187988 = 1.1520955562591553 + 2.0 * 6.154411792755127
Epoch 290, val loss: 1.2634962797164917
Epoch 300, training loss: 13.399646759033203 = 1.0995038747787476 + 2.0 * 6.150071620941162
Epoch 300, val loss: 1.226042628288269
Epoch 310, training loss: 13.339372634887695 = 1.0479189157485962 + 2.0 * 6.145726680755615
Epoch 310, val loss: 1.1901696920394897
Epoch 320, training loss: 13.284575462341309 = 0.9979467988014221 + 2.0 * 6.143314361572266
Epoch 320, val loss: 1.1561880111694336
Epoch 330, training loss: 13.227622032165527 = 0.9504567980766296 + 2.0 * 6.138582706451416
Epoch 330, val loss: 1.124517560005188
Epoch 340, training loss: 13.173830032348633 = 0.9052746295928955 + 2.0 * 6.134277820587158
Epoch 340, val loss: 1.0950959920883179
Epoch 350, training loss: 13.125899314880371 = 0.8624712824821472 + 2.0 * 6.1317138671875
Epoch 350, val loss: 1.0679734945297241
Epoch 360, training loss: 13.079002380371094 = 0.8223839402198792 + 2.0 * 6.12830924987793
Epoch 360, val loss: 1.0430740118026733
Epoch 370, training loss: 13.033422470092773 = 0.7845884561538696 + 2.0 * 6.124416828155518
Epoch 370, val loss: 1.0202136039733887
Epoch 380, training loss: 13.008296012878418 = 0.7488569021224976 + 2.0 * 6.1297197341918945
Epoch 380, val loss: 0.9993061423301697
Epoch 390, training loss: 12.956512451171875 = 0.7156373262405396 + 2.0 * 6.1204376220703125
Epoch 390, val loss: 0.9801816344261169
Epoch 400, training loss: 12.917401313781738 = 0.6843099594116211 + 2.0 * 6.116545677185059
Epoch 400, val loss: 0.9627284407615662
Epoch 410, training loss: 12.881606101989746 = 0.6546114683151245 + 2.0 * 6.113497257232666
Epoch 410, val loss: 0.9466564655303955
Epoch 420, training loss: 12.847585678100586 = 0.6263487935066223 + 2.0 * 6.110618591308594
Epoch 420, val loss: 0.9318835139274597
Epoch 430, training loss: 12.816824913024902 = 0.5995548963546753 + 2.0 * 6.108634948730469
Epoch 430, val loss: 0.9182472229003906
Epoch 440, training loss: 12.783586502075195 = 0.5738328695297241 + 2.0 * 6.10487699508667
Epoch 440, val loss: 0.9057869911193848
Epoch 450, training loss: 12.766064643859863 = 0.5490625500679016 + 2.0 * 6.108500957489014
Epoch 450, val loss: 0.8942408561706543
Epoch 460, training loss: 12.72635555267334 = 0.5253236889839172 + 2.0 * 6.100515842437744
Epoch 460, val loss: 0.8834927678108215
Epoch 470, training loss: 12.70130443572998 = 0.5023401975631714 + 2.0 * 6.09948205947876
Epoch 470, val loss: 0.8735546469688416
Epoch 480, training loss: 12.672455787658691 = 0.4800812005996704 + 2.0 * 6.096187114715576
Epoch 480, val loss: 0.8643226623535156
Epoch 490, training loss: 12.661049842834473 = 0.4585307240486145 + 2.0 * 6.101259708404541
Epoch 490, val loss: 0.8557333946228027
Epoch 500, training loss: 12.624199867248535 = 0.4376125633716583 + 2.0 * 6.0932936668396
Epoch 500, val loss: 0.8477881550788879
Epoch 510, training loss: 12.596742630004883 = 0.4173636734485626 + 2.0 * 6.089689254760742
Epoch 510, val loss: 0.840416669845581
Epoch 520, training loss: 12.57287883758545 = 0.3976069390773773 + 2.0 * 6.0876359939575195
Epoch 520, val loss: 0.833600640296936
Epoch 530, training loss: 12.5703125 = 0.37838953733444214 + 2.0 * 6.095961570739746
Epoch 530, val loss: 0.8273152112960815
Epoch 540, training loss: 12.530378341674805 = 0.35984885692596436 + 2.0 * 6.085264682769775
Epoch 540, val loss: 0.8215691447257996
Epoch 550, training loss: 12.508506774902344 = 0.3419346511363983 + 2.0 * 6.083286285400391
Epoch 550, val loss: 0.8164148330688477
Epoch 560, training loss: 12.491036415100098 = 0.3245900571346283 + 2.0 * 6.083223342895508
Epoch 560, val loss: 0.8119149804115295
Epoch 570, training loss: 12.469812393188477 = 0.30785784125328064 + 2.0 * 6.080977439880371
Epoch 570, val loss: 0.8079196214675903
Epoch 580, training loss: 12.44819450378418 = 0.2917896509170532 + 2.0 * 6.078202247619629
Epoch 580, val loss: 0.8046619296073914
Epoch 590, training loss: 12.429167747497559 = 0.2763032615184784 + 2.0 * 6.076432228088379
Epoch 590, val loss: 0.8020801544189453
Epoch 600, training loss: 12.415583610534668 = 0.2614472210407257 + 2.0 * 6.077068328857422
Epoch 600, val loss: 0.8002217411994934
Epoch 610, training loss: 12.408590316772461 = 0.24729153513908386 + 2.0 * 6.080649375915527
Epoch 610, val loss: 0.7991225719451904
Epoch 620, training loss: 12.381500244140625 = 0.2338901162147522 + 2.0 * 6.07380485534668
Epoch 620, val loss: 0.7986487746238708
Epoch 630, training loss: 12.361946105957031 = 0.2212117612361908 + 2.0 * 6.070367336273193
Epoch 630, val loss: 0.7988787293434143
Epoch 640, training loss: 12.3463773727417 = 0.2091938704252243 + 2.0 * 6.068591594696045
Epoch 640, val loss: 0.7997761368751526
Epoch 650, training loss: 12.34170913696289 = 0.1978558450937271 + 2.0 * 6.071926593780518
Epoch 650, val loss: 0.8012743592262268
Epoch 660, training loss: 12.328080177307129 = 0.1872585564851761 + 2.0 * 6.07041072845459
Epoch 660, val loss: 0.8032487034797668
Epoch 670, training loss: 12.311344146728516 = 0.17732959985733032 + 2.0 * 6.067007064819336
Epoch 670, val loss: 0.8057394027709961
Epoch 680, training loss: 12.296591758728027 = 0.1680133044719696 + 2.0 * 6.064289093017578
Epoch 680, val loss: 0.8086848258972168
Epoch 690, training loss: 12.288284301757812 = 0.15930704772472382 + 2.0 * 6.064488410949707
Epoch 690, val loss: 0.8120518326759338
Epoch 700, training loss: 12.27463150024414 = 0.1511787325143814 + 2.0 * 6.0617265701293945
Epoch 700, val loss: 0.8157595992088318
Epoch 710, training loss: 12.27373218536377 = 0.14358893036842346 + 2.0 * 6.0650715827941895
Epoch 710, val loss: 0.8197619318962097
Epoch 720, training loss: 12.253840446472168 = 0.13651302456855774 + 2.0 * 6.058663845062256
Epoch 720, val loss: 0.82402104139328
Epoch 730, training loss: 12.247417449951172 = 0.12988340854644775 + 2.0 * 6.058766841888428
Epoch 730, val loss: 0.82856285572052
Epoch 740, training loss: 12.245644569396973 = 0.12366244196891785 + 2.0 * 6.060991287231445
Epoch 740, val loss: 0.8333275318145752
Epoch 750, training loss: 12.231010437011719 = 0.11782124638557434 + 2.0 * 6.056594371795654
Epoch 750, val loss: 0.8382495045661926
Epoch 760, training loss: 12.240204811096191 = 0.11238037049770355 + 2.0 * 6.063912391662598
Epoch 760, val loss: 0.8433848023414612
Epoch 770, training loss: 12.223770141601562 = 0.10724248737096786 + 2.0 * 6.058263778686523
Epoch 770, val loss: 0.8484675884246826
Epoch 780, training loss: 12.209243774414062 = 0.10245070606470108 + 2.0 * 6.053396701812744
Epoch 780, val loss: 0.8537136912345886
Epoch 790, training loss: 12.200579643249512 = 0.0979301705956459 + 2.0 * 6.051324844360352
Epoch 790, val loss: 0.8592020273208618
Epoch 800, training loss: 12.194112777709961 = 0.09365245699882507 + 2.0 * 6.050230026245117
Epoch 800, val loss: 0.8647934794425964
Epoch 810, training loss: 12.189835548400879 = 0.0895969420671463 + 2.0 * 6.050119400024414
Epoch 810, val loss: 0.8704640865325928
Epoch 820, training loss: 12.185880661010742 = 0.08577648550271988 + 2.0 * 6.050052165985107
Epoch 820, val loss: 0.8762033581733704
Epoch 830, training loss: 12.183197975158691 = 0.0821826159954071 + 2.0 * 6.050507545471191
Epoch 830, val loss: 0.8818247318267822
Epoch 840, training loss: 12.172154426574707 = 0.07878649979829788 + 2.0 * 6.0466837882995605
Epoch 840, val loss: 0.8876612186431885
Epoch 850, training loss: 12.180609703063965 = 0.0755600556731224 + 2.0 * 6.052525043487549
Epoch 850, val loss: 0.8935285806655884
Epoch 860, training loss: 12.167078018188477 = 0.07252777367830276 + 2.0 * 6.047275066375732
Epoch 860, val loss: 0.8993735909461975
Epoch 870, training loss: 12.160284996032715 = 0.06962926685810089 + 2.0 * 6.045327663421631
Epoch 870, val loss: 0.9052078127861023
Epoch 880, training loss: 12.160784721374512 = 0.06688576191663742 + 2.0 * 6.04694938659668
Epoch 880, val loss: 0.9111701846122742
Epoch 890, training loss: 12.150421142578125 = 0.06430349498987198 + 2.0 * 6.0430588722229
Epoch 890, val loss: 0.9171331524848938
Epoch 900, training loss: 12.147159576416016 = 0.06182757392525673 + 2.0 * 6.042665958404541
Epoch 900, val loss: 0.9230374693870544
Epoch 910, training loss: 12.142090797424316 = 0.059488169848918915 + 2.0 * 6.041301250457764
Epoch 910, val loss: 0.9291464686393738
Epoch 920, training loss: 12.143821716308594 = 0.05725150927901268 + 2.0 * 6.043284893035889
Epoch 920, val loss: 0.9352147579193115
Epoch 930, training loss: 12.148786544799805 = 0.05513972043991089 + 2.0 * 6.046823501586914
Epoch 930, val loss: 0.9413021206855774
Epoch 940, training loss: 12.134040832519531 = 0.05314142256975174 + 2.0 * 6.040449619293213
Epoch 940, val loss: 0.9471081495285034
Epoch 950, training loss: 12.129287719726562 = 0.051245078444480896 + 2.0 * 6.0390214920043945
Epoch 950, val loss: 0.9531288146972656
Epoch 960, training loss: 12.125584602355957 = 0.049435775727033615 + 2.0 * 6.038074493408203
Epoch 960, val loss: 0.9592238664627075
Epoch 970, training loss: 12.134081840515137 = 0.0477178692817688 + 2.0 * 6.043181896209717
Epoch 970, val loss: 0.9653486609458923
Epoch 980, training loss: 12.120420455932617 = 0.046064186841249466 + 2.0 * 6.037178039550781
Epoch 980, val loss: 0.971274733543396
Epoch 990, training loss: 12.115666389465332 = 0.0445011742413044 + 2.0 * 6.035582542419434
Epoch 990, val loss: 0.9773293733596802
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7815
Overall ASR: 0.8524
Flip ASR: 0.8222/225 nodes
The final ASR:0.80320, 0.16932, Accuracy:0.77778, 0.01684
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11574])
remove edge: torch.Size([2, 9542])
updated graph: torch.Size([2, 10560])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8481
Overall ASR: 0.9446
Flip ASR: 0.9333/225 nodes
The final ASR:0.96679, 0.01566, Accuracy:0.83333, 0.01090
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.4 der2:0.3 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 18.69266128540039 = 1.9449199438095093 + 2.0 * 8.373870849609375
Epoch 0, val loss: 1.9496749639511108
Epoch 10, training loss: 18.681438446044922 = 1.9346448183059692 + 2.0 * 8.373396873474121
Epoch 10, val loss: 1.939881443977356
Epoch 20, training loss: 18.66277503967285 = 1.9221701622009277 + 2.0 * 8.370302200317383
Epoch 20, val loss: 1.927626371383667
Epoch 30, training loss: 18.602033615112305 = 1.9055535793304443 + 2.0 * 8.34823989868164
Epoch 30, val loss: 1.9111350774765015
Epoch 40, training loss: 18.25942039489746 = 1.8849724531173706 + 2.0 * 8.187224388122559
Epoch 40, val loss: 1.8913506269454956
Epoch 50, training loss: 16.84917640686035 = 1.8626559972763062 + 2.0 * 7.493260383605957
Epoch 50, val loss: 1.8705493211746216
Epoch 60, training loss: 16.19379997253418 = 1.8459450006484985 + 2.0 * 7.173927307128906
Epoch 60, val loss: 1.8557249307632446
Epoch 70, training loss: 15.641035079956055 = 1.8336635828018188 + 2.0 * 6.903685569763184
Epoch 70, val loss: 1.8448612689971924
Epoch 80, training loss: 15.221868515014648 = 1.821254014968872 + 2.0 * 6.700307369232178
Epoch 80, val loss: 1.8339853286743164
Epoch 90, training loss: 14.989042282104492 = 1.8083000183105469 + 2.0 * 6.590371131896973
Epoch 90, val loss: 1.8224838972091675
Epoch 100, training loss: 14.784744262695312 = 1.7953733205795288 + 2.0 * 6.494685649871826
Epoch 100, val loss: 1.8108139038085938
Epoch 110, training loss: 14.645576477050781 = 1.7830240726470947 + 2.0 * 6.431276321411133
Epoch 110, val loss: 1.7993031740188599
Epoch 120, training loss: 14.529511451721191 = 1.77001953125 + 2.0 * 6.379745960235596
Epoch 120, val loss: 1.7872449159622192
Epoch 130, training loss: 14.432494163513184 = 1.7563399076461792 + 2.0 * 6.338077068328857
Epoch 130, val loss: 1.774755835533142
Epoch 140, training loss: 14.350470542907715 = 1.741766333580017 + 2.0 * 6.304352283477783
Epoch 140, val loss: 1.7616016864776611
Epoch 150, training loss: 14.282787322998047 = 1.725635290145874 + 2.0 * 6.278575897216797
Epoch 150, val loss: 1.7472615242004395
Epoch 160, training loss: 14.221678733825684 = 1.7077280282974243 + 2.0 * 6.256975173950195
Epoch 160, val loss: 1.731585144996643
Epoch 170, training loss: 14.166086196899414 = 1.6876128911972046 + 2.0 * 6.239236831665039
Epoch 170, val loss: 1.7143285274505615
Epoch 180, training loss: 14.112663269042969 = 1.6650058031082153 + 2.0 * 6.2238287925720215
Epoch 180, val loss: 1.695099115371704
Epoch 190, training loss: 14.061056137084961 = 1.6395331621170044 + 2.0 * 6.210761547088623
Epoch 190, val loss: 1.673558235168457
Epoch 200, training loss: 14.01021957397461 = 1.6107802391052246 + 2.0 * 6.1997199058532715
Epoch 200, val loss: 1.6493444442749023
Epoch 210, training loss: 13.959939002990723 = 1.578898549079895 + 2.0 * 6.190520286560059
Epoch 210, val loss: 1.6226435899734497
Epoch 220, training loss: 13.90494155883789 = 1.5440526008605957 + 2.0 * 6.180444717407227
Epoch 220, val loss: 1.5935896635055542
Epoch 230, training loss: 13.850308418273926 = 1.506130576133728 + 2.0 * 6.172089099884033
Epoch 230, val loss: 1.562099814414978
Epoch 240, training loss: 13.798542022705078 = 1.4654209613800049 + 2.0 * 6.166560649871826
Epoch 240, val loss: 1.5286184549331665
Epoch 250, training loss: 13.741331100463867 = 1.4229164123535156 + 2.0 * 6.159207344055176
Epoch 250, val loss: 1.494092345237732
Epoch 260, training loss: 13.683969497680664 = 1.3791117668151855 + 2.0 * 6.152429103851318
Epoch 260, val loss: 1.4591153860092163
Epoch 270, training loss: 13.630331993103027 = 1.334674596786499 + 2.0 * 6.147828578948975
Epoch 270, val loss: 1.4243558645248413
Epoch 280, training loss: 13.57614803314209 = 1.2905348539352417 + 2.0 * 6.142806529998779
Epoch 280, val loss: 1.3905842304229736
Epoch 290, training loss: 13.522308349609375 = 1.246986985206604 + 2.0 * 6.137660503387451
Epoch 290, val loss: 1.3580456972122192
Epoch 300, training loss: 13.481873512268066 = 1.2042036056518555 + 2.0 * 6.1388349533081055
Epoch 300, val loss: 1.3264535665512085
Epoch 310, training loss: 13.42329216003418 = 1.1625962257385254 + 2.0 * 6.130348205566406
Epoch 310, val loss: 1.2958753108978271
Epoch 320, training loss: 13.373188972473145 = 1.1214609146118164 + 2.0 * 6.125864028930664
Epoch 320, val loss: 1.2655805349349976
Epoch 330, training loss: 13.331768035888672 = 1.0803016424179077 + 2.0 * 6.125733375549316
Epoch 330, val loss: 1.2351268529891968
Epoch 340, training loss: 13.280523300170898 = 1.039319396018982 + 2.0 * 6.120602130889893
Epoch 340, val loss: 1.2042666673660278
Epoch 350, training loss: 13.232283592224121 = 0.9980112314224243 + 2.0 * 6.117136001586914
Epoch 350, val loss: 1.172908067703247
Epoch 360, training loss: 13.184795379638672 = 0.9562664031982422 + 2.0 * 6.114264488220215
Epoch 360, val loss: 1.1407512426376343
Epoch 370, training loss: 13.139793395996094 = 0.9143142700195312 + 2.0 * 6.112739562988281
Epoch 370, val loss: 1.1082358360290527
Epoch 380, training loss: 13.090503692626953 = 0.872498631477356 + 2.0 * 6.109002590179443
Epoch 380, val loss: 1.0752828121185303
Epoch 390, training loss: 13.045995712280273 = 0.8308768272399902 + 2.0 * 6.1075592041015625
Epoch 390, val loss: 1.0421589612960815
Epoch 400, training loss: 12.998519897460938 = 0.7897839546203613 + 2.0 * 6.104368209838867
Epoch 400, val loss: 1.0092501640319824
Epoch 410, training loss: 12.952323913574219 = 0.7494897246360779 + 2.0 * 6.101417064666748
Epoch 410, val loss: 0.9768503904342651
Epoch 420, training loss: 12.913240432739258 = 0.7101759314537048 + 2.0 * 6.101532459259033
Epoch 420, val loss: 0.9453440308570862
Epoch 430, training loss: 12.875965118408203 = 0.6725102663040161 + 2.0 * 6.101727485656738
Epoch 430, val loss: 0.9149619340896606
Epoch 440, training loss: 12.828062057495117 = 0.636591374874115 + 2.0 * 6.095735549926758
Epoch 440, val loss: 0.8864965438842773
Epoch 450, training loss: 12.788739204406738 = 0.6024314761161804 + 2.0 * 6.093153953552246
Epoch 450, val loss: 0.8598288297653198
Epoch 460, training loss: 12.751232147216797 = 0.5698695182800293 + 2.0 * 6.090681076049805
Epoch 460, val loss: 0.8348585963249207
Epoch 470, training loss: 12.731341361999512 = 0.5388439893722534 + 2.0 * 6.096248626708984
Epoch 470, val loss: 0.8116230964660645
Epoch 480, training loss: 12.690356254577637 = 0.50960373878479 + 2.0 * 6.090376377105713
Epoch 480, val loss: 0.7902446985244751
Epoch 490, training loss: 12.653887748718262 = 0.4819101095199585 + 2.0 * 6.085988998413086
Epoch 490, val loss: 0.7707700133323669
Epoch 500, training loss: 12.623269081115723 = 0.4554898142814636 + 2.0 * 6.083889484405518
Epoch 500, val loss: 0.752826452255249
Epoch 510, training loss: 12.599418640136719 = 0.4301885664463043 + 2.0 * 6.084615230560303
Epoch 510, val loss: 0.7362020611763
Epoch 520, training loss: 12.569157600402832 = 0.40595149993896484 + 2.0 * 6.081603050231934
Epoch 520, val loss: 0.7209194898605347
Epoch 530, training loss: 12.5496826171875 = 0.38270264863967896 + 2.0 * 6.083489894866943
Epoch 530, val loss: 0.7066662311553955
Epoch 540, training loss: 12.516790390014648 = 0.36054742336273193 + 2.0 * 6.078121662139893
Epoch 540, val loss: 0.6934831738471985
Epoch 550, training loss: 12.491271018981934 = 0.33921143412590027 + 2.0 * 6.0760297775268555
Epoch 550, val loss: 0.6814125776290894
Epoch 560, training loss: 12.465693473815918 = 0.3186991512775421 + 2.0 * 6.073497295379639
Epoch 560, val loss: 0.6701924800872803
Epoch 570, training loss: 12.457225799560547 = 0.2990128993988037 + 2.0 * 6.079106330871582
Epoch 570, val loss: 0.6598855257034302
Epoch 580, training loss: 12.424907684326172 = 0.28039202094078064 + 2.0 * 6.072257995605469
Epoch 580, val loss: 0.6505997180938721
Epoch 590, training loss: 12.401924133300781 = 0.2626318335533142 + 2.0 * 6.06964635848999
Epoch 590, val loss: 0.6422862410545349
Epoch 600, training loss: 12.381481170654297 = 0.24572837352752686 + 2.0 * 6.06787633895874
Epoch 600, val loss: 0.6349678635597229
Epoch 610, training loss: 12.364031791687012 = 0.22969664633274078 + 2.0 * 6.06716775894165
Epoch 610, val loss: 0.6285606026649475
Epoch 620, training loss: 12.346195220947266 = 0.21464459598064423 + 2.0 * 6.065775394439697
Epoch 620, val loss: 0.6231445670127869
Epoch 630, training loss: 12.338615417480469 = 0.20072050392627716 + 2.0 * 6.0689473152160645
Epoch 630, val loss: 0.618637204170227
Epoch 640, training loss: 12.314289093017578 = 0.18783923983573914 + 2.0 * 6.063224792480469
Epoch 640, val loss: 0.6149972081184387
Epoch 650, training loss: 12.298785209655762 = 0.1758708655834198 + 2.0 * 6.06145715713501
Epoch 650, val loss: 0.6122435331344604
Epoch 660, training loss: 12.284985542297363 = 0.16478341817855835 + 2.0 * 6.06010103225708
Epoch 660, val loss: 0.6103403568267822
Epoch 670, training loss: 12.284994125366211 = 0.1545361876487732 + 2.0 * 6.0652289390563965
Epoch 670, val loss: 0.609203577041626
Epoch 680, training loss: 12.264808654785156 = 0.14515237510204315 + 2.0 * 6.059828281402588
Epoch 680, val loss: 0.6087706089019775
Epoch 690, training loss: 12.252008438110352 = 0.13656792044639587 + 2.0 * 6.057720184326172
Epoch 690, val loss: 0.608856737613678
Epoch 700, training loss: 12.240453720092773 = 0.12866029143333435 + 2.0 * 6.055896759033203
Epoch 700, val loss: 0.6096272468566895
Epoch 710, training loss: 12.239595413208008 = 0.12137022614479065 + 2.0 * 6.059112548828125
Epoch 710, val loss: 0.6109567880630493
Epoch 720, training loss: 12.235048294067383 = 0.11467337608337402 + 2.0 * 6.060187339782715
Epoch 720, val loss: 0.6128002405166626
Epoch 730, training loss: 12.217101097106934 = 0.1085304394364357 + 2.0 * 6.054285526275635
Epoch 730, val loss: 0.6149126291275024
Epoch 740, training loss: 12.207009315490723 = 0.10283205658197403 + 2.0 * 6.052088737487793
Epoch 740, val loss: 0.6174583435058594
Epoch 750, training loss: 12.21029281616211 = 0.09754113852977753 + 2.0 * 6.056375980377197
Epoch 750, val loss: 0.6203924417495728
Epoch 760, training loss: 12.195053100585938 = 0.0926450565457344 + 2.0 * 6.051204204559326
Epoch 760, val loss: 0.6235874891281128
Epoch 770, training loss: 12.186687469482422 = 0.08807661384344101 + 2.0 * 6.049305438995361
Epoch 770, val loss: 0.6270121335983276
Epoch 780, training loss: 12.181878089904785 = 0.0838344395160675 + 2.0 * 6.0490217208862305
Epoch 780, val loss: 0.6306919455528259
Epoch 790, training loss: 12.173810958862305 = 0.07989215850830078 + 2.0 * 6.046959400177002
Epoch 790, val loss: 0.6344664692878723
Epoch 800, training loss: 12.174829483032227 = 0.07619457691907883 + 2.0 * 6.049317359924316
Epoch 800, val loss: 0.6384332776069641
Epoch 810, training loss: 12.165891647338867 = 0.07277072221040726 + 2.0 * 6.046560287475586
Epoch 810, val loss: 0.6425122022628784
Epoch 820, training loss: 12.157927513122559 = 0.06954530626535416 + 2.0 * 6.044190883636475
Epoch 820, val loss: 0.646700382232666
Epoch 830, training loss: 12.151728630065918 = 0.06652630120515823 + 2.0 * 6.042601108551025
Epoch 830, val loss: 0.6509896516799927
Epoch 840, training loss: 12.160401344299316 = 0.06369363516569138 + 2.0 * 6.048353672027588
Epoch 840, val loss: 0.6553786993026733
Epoch 850, training loss: 12.152997970581055 = 0.06103917211294174 + 2.0 * 6.0459794998168945
Epoch 850, val loss: 0.6598345637321472
Epoch 860, training loss: 12.14277458190918 = 0.05855482816696167 + 2.0 * 6.042109966278076
Epoch 860, val loss: 0.6642060875892639
Epoch 870, training loss: 12.135249137878418 = 0.056216467171907425 + 2.0 * 6.039516448974609
Epoch 870, val loss: 0.6686946749687195
Epoch 880, training loss: 12.130520820617676 = 0.054003749042749405 + 2.0 * 6.0382585525512695
Epoch 880, val loss: 0.6732588410377502
Epoch 890, training loss: 12.140616416931152 = 0.05191373452544212 + 2.0 * 6.044351577758789
Epoch 890, val loss: 0.6778939962387085
Epoch 900, training loss: 12.131670951843262 = 0.04995330423116684 + 2.0 * 6.040858745574951
Epoch 900, val loss: 0.6824341416358948
Epoch 910, training loss: 12.123290061950684 = 0.04812110587954521 + 2.0 * 6.03758430480957
Epoch 910, val loss: 0.6868295073509216
Epoch 920, training loss: 12.117870330810547 = 0.046383071690797806 + 2.0 * 6.035743713378906
Epoch 920, val loss: 0.6912581920623779
Epoch 930, training loss: 12.114107131958008 = 0.04472789913415909 + 2.0 * 6.034689426422119
Epoch 930, val loss: 0.695810854434967
Epoch 940, training loss: 12.140986442565918 = 0.04314989224076271 + 2.0 * 6.0489182472229
Epoch 940, val loss: 0.700372576713562
Epoch 950, training loss: 12.110057830810547 = 0.04168473929166794 + 2.0 * 6.034186363220215
Epoch 950, val loss: 0.704855740070343
Epoch 960, training loss: 12.108630180358887 = 0.0402795746922493 + 2.0 * 6.034175395965576
Epoch 960, val loss: 0.7092376351356506
Epoch 970, training loss: 12.102178573608398 = 0.038937851786613464 + 2.0 * 6.031620502471924
Epoch 970, val loss: 0.7137019038200378
Epoch 980, training loss: 12.099949836730957 = 0.03765680640935898 + 2.0 * 6.03114652633667
Epoch 980, val loss: 0.7182037830352783
Epoch 990, training loss: 12.104945182800293 = 0.03643915802240372 + 2.0 * 6.034253120422363
Epoch 990, val loss: 0.7227275967597961
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 730, in <module>
    torch.cuda.empty_cache()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/cuda/memory.py", line 121, in empty_cache
    torch._C._cuda_emptyCache()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.4 der2:0.3 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 18.698287963867188 = 1.9504508972167969 + 2.0 * 8.373918533325195
Epoch 0, val loss: 1.950169563293457
Epoch 10, training loss: 18.68790054321289 = 1.940720558166504 + 2.0 * 8.373589515686035
Epoch 10, val loss: 1.9413399696350098
Epoch 20, training loss: 18.67095375061035 = 1.9285145998001099 + 2.0 * 8.371219635009766
Epoch 20, val loss: 1.9298174381256104
Epoch 30, training loss: 18.61992073059082 = 1.9114471673965454 + 2.0 * 8.354236602783203
Epoch 30, val loss: 1.9134047031402588
Epoch 40, training loss: 18.38642120361328 = 1.889430046081543 + 2.0 * 8.248496055603027
Epoch 40, val loss: 1.8928362131118774
Epoch 50, training loss: 17.61954689025879 = 1.8647738695144653 + 2.0 * 7.877386569976807
Epoch 50, val loss: 1.8698794841766357
Epoch 60, training loss: 16.96582794189453 = 1.8413803577423096 + 2.0 * 7.5622239112854
Epoch 60, val loss: 1.8491218090057373
Epoch 70, training loss: 16.242929458618164 = 1.8240302801132202 + 2.0 * 7.209449291229248
Epoch 70, val loss: 1.8340498208999634
Epoch 80, training loss: 15.68809986114502 = 1.8095484972000122 + 2.0 * 6.939275741577148
Epoch 80, val loss: 1.821733832359314
Epoch 90, training loss: 15.258915901184082 = 1.7961711883544922 + 2.0 * 6.731372356414795
Epoch 90, val loss: 1.8100967407226562
Epoch 100, training loss: 15.066662788391113 = 1.780967354774475 + 2.0 * 6.642847537994385
Epoch 100, val loss: 1.7966498136520386
Epoch 110, training loss: 14.933173179626465 = 1.7642582654953003 + 2.0 * 6.5844573974609375
Epoch 110, val loss: 1.7823292016983032
Epoch 120, training loss: 14.804296493530273 = 1.7482846975326538 + 2.0 * 6.528006076812744
Epoch 120, val loss: 1.7686233520507812
Epoch 130, training loss: 14.695425987243652 = 1.7323664426803589 + 2.0 * 6.481529712677002
Epoch 130, val loss: 1.7547744512557983
Epoch 140, training loss: 14.592446327209473 = 1.714938998222351 + 2.0 * 6.438753604888916
Epoch 140, val loss: 1.7397977113723755
Epoch 150, training loss: 14.502368927001953 = 1.6957029104232788 + 2.0 * 6.4033331871032715
Epoch 150, val loss: 1.7236958742141724
Epoch 160, training loss: 14.424168586730957 = 1.6742645502090454 + 2.0 * 6.3749518394470215
Epoch 160, val loss: 1.7061033248901367
Epoch 170, training loss: 14.360255241394043 = 1.6501743793487549 + 2.0 * 6.355040550231934
Epoch 170, val loss: 1.6863830089569092
Epoch 180, training loss: 14.286134719848633 = 1.6238017082214355 + 2.0 * 6.331166744232178
Epoch 180, val loss: 1.6647363901138306
Epoch 190, training loss: 14.222710609436035 = 1.5951974391937256 + 2.0 * 6.313756465911865
Epoch 190, val loss: 1.6410706043243408
Epoch 200, training loss: 14.157889366149902 = 1.5640699863433838 + 2.0 * 6.296909809112549
Epoch 200, val loss: 1.6151703596115112
Epoch 210, training loss: 14.093475341796875 = 1.5308763980865479 + 2.0 * 6.281299591064453
Epoch 210, val loss: 1.587631344795227
Epoch 220, training loss: 14.027515411376953 = 1.4961347579956055 + 2.0 * 6.265690326690674
Epoch 220, val loss: 1.5588033199310303
Epoch 230, training loss: 13.970551490783691 = 1.460015058517456 + 2.0 * 6.255268096923828
Epoch 230, val loss: 1.5290229320526123
Epoch 240, training loss: 13.904691696166992 = 1.4231414794921875 + 2.0 * 6.240775108337402
Epoch 240, val loss: 1.498746633529663
Epoch 250, training loss: 13.84429931640625 = 1.3855829238891602 + 2.0 * 6.229358196258545
Epoch 250, val loss: 1.4680653810501099
Epoch 260, training loss: 13.790371894836426 = 1.3477439880371094 + 2.0 * 6.221313953399658
Epoch 260, val loss: 1.437633991241455
Epoch 270, training loss: 13.734020233154297 = 1.310244083404541 + 2.0 * 6.211887836456299
Epoch 270, val loss: 1.4076931476593018
Epoch 280, training loss: 13.680133819580078 = 1.2727649211883545 + 2.0 * 6.203684329986572
Epoch 280, val loss: 1.3781183958053589
Epoch 290, training loss: 13.641573905944824 = 1.235158085823059 + 2.0 * 6.203207969665527
Epoch 290, val loss: 1.3487939834594727
Epoch 300, training loss: 13.585461616516113 = 1.1978023052215576 + 2.0 * 6.193829536437988
Epoch 300, val loss: 1.3198412656784058
Epoch 310, training loss: 13.531471252441406 = 1.160273551940918 + 2.0 * 6.185598850250244
Epoch 310, val loss: 1.2911709547042847
Epoch 320, training loss: 13.481876373291016 = 1.121964454650879 + 2.0 * 6.179955959320068
Epoch 320, val loss: 1.2620424032211304
Epoch 330, training loss: 13.432120323181152 = 1.0828132629394531 + 2.0 * 6.17465353012085
Epoch 330, val loss: 1.23227858543396
Epoch 340, training loss: 13.384324073791504 = 1.042728066444397 + 2.0 * 6.170797824859619
Epoch 340, val loss: 1.2019364833831787
Epoch 350, training loss: 13.332420349121094 = 1.0016080141067505 + 2.0 * 6.165406227111816
Epoch 350, val loss: 1.1706135272979736
Epoch 360, training loss: 13.299459457397461 = 0.9592915773391724 + 2.0 * 6.170083999633789
Epoch 360, val loss: 1.1383305788040161
Epoch 370, training loss: 13.236978530883789 = 0.9164897203445435 + 2.0 * 6.160244464874268
Epoch 370, val loss: 1.1054131984710693
Epoch 380, training loss: 13.181199073791504 = 0.8734735250473022 + 2.0 * 6.153862953186035
Epoch 380, val loss: 1.072141408920288
Epoch 390, training loss: 13.140257835388184 = 0.8303636908531189 + 2.0 * 6.154947280883789
Epoch 390, val loss: 1.0387439727783203
Epoch 400, training loss: 13.084210395812988 = 0.7881633639335632 + 2.0 * 6.14802360534668
Epoch 400, val loss: 1.0058040618896484
Epoch 410, training loss: 13.03460693359375 = 0.7469700574874878 + 2.0 * 6.143818378448486
Epoch 410, val loss: 0.9739742875099182
Epoch 420, training loss: 12.989819526672363 = 0.7072668671607971 + 2.0 * 6.1412763595581055
Epoch 420, val loss: 0.9437008500099182
Epoch 430, training loss: 12.948101997375488 = 0.6697096228599548 + 2.0 * 6.139196395874023
Epoch 430, val loss: 0.9154118895530701
Epoch 440, training loss: 12.90339469909668 = 0.6343708038330078 + 2.0 * 6.134511947631836
Epoch 440, val loss: 0.8896872401237488
Epoch 450, training loss: 12.863921165466309 = 0.6013166904449463 + 2.0 * 6.131302356719971
Epoch 450, val loss: 0.8664605021476746
Epoch 460, training loss: 12.842984199523926 = 0.5703397393226624 + 2.0 * 6.136322021484375
Epoch 460, val loss: 0.8456515073776245
Epoch 470, training loss: 12.796873092651367 = 0.5415412783622742 + 2.0 * 6.127665996551514
Epoch 470, val loss: 0.8271744847297668
Epoch 480, training loss: 12.765764236450195 = 0.5146336555480957 + 2.0 * 6.125565052032471
Epoch 480, val loss: 0.8108693361282349
Epoch 490, training loss: 12.732400894165039 = 0.4894638955593109 + 2.0 * 6.121468544006348
Epoch 490, val loss: 0.7964751124382019
Epoch 500, training loss: 12.705471992492676 = 0.4658021926879883 + 2.0 * 6.119834899902344
Epoch 500, val loss: 0.783805787563324
Epoch 510, training loss: 12.677781105041504 = 0.44332653284072876 + 2.0 * 6.117227077484131
Epoch 510, val loss: 0.7725651264190674
Epoch 520, training loss: 12.656498908996582 = 0.4219861328601837 + 2.0 * 6.117256164550781
Epoch 520, val loss: 0.7627779841423035
Epoch 530, training loss: 12.627898216247559 = 0.4018664062023163 + 2.0 * 6.113016128540039
Epoch 530, val loss: 0.7540470957756042
Epoch 540, training loss: 12.603645324707031 = 0.382661908864975 + 2.0 * 6.110491752624512
Epoch 540, val loss: 0.746448814868927
Epoch 550, training loss: 12.580710411071777 = 0.36423829197883606 + 2.0 * 6.108235836029053
Epoch 550, val loss: 0.7397609353065491
Epoch 560, training loss: 12.562445640563965 = 0.34649762511253357 + 2.0 * 6.107974052429199
Epoch 560, val loss: 0.7338129878044128
Epoch 570, training loss: 12.55831527709961 = 0.32943981885910034 + 2.0 * 6.114437580108643
Epoch 570, val loss: 0.7285314202308655
Epoch 580, training loss: 12.52580738067627 = 0.31314411759376526 + 2.0 * 6.106331825256348
Epoch 580, val loss: 0.7239450812339783
Epoch 590, training loss: 12.499855041503906 = 0.2975325584411621 + 2.0 * 6.101161479949951
Epoch 590, val loss: 0.7200341820716858
Epoch 600, training loss: 12.481682777404785 = 0.2824898362159729 + 2.0 * 6.0995965003967285
Epoch 600, val loss: 0.7166922688484192
Epoch 610, training loss: 12.478181838989258 = 0.2680280804634094 + 2.0 * 6.105076789855957
Epoch 610, val loss: 0.7138242125511169
Epoch 620, training loss: 12.449929237365723 = 0.25424033403396606 + 2.0 * 6.09784460067749
Epoch 620, val loss: 0.7114384770393372
Epoch 630, training loss: 12.4305419921875 = 0.24108311533927917 + 2.0 * 6.094729423522949
Epoch 630, val loss: 0.7096204161643982
Epoch 640, training loss: 12.4166898727417 = 0.22854523360729218 + 2.0 * 6.094072341918945
Epoch 640, val loss: 0.7083405256271362
Epoch 650, training loss: 12.403000831604004 = 0.21661639213562012 + 2.0 * 6.093192100524902
Epoch 650, val loss: 0.707524299621582
Epoch 660, training loss: 12.389914512634277 = 0.20535053312778473 + 2.0 * 6.092281818389893
Epoch 660, val loss: 0.7071942090988159
Epoch 670, training loss: 12.373133659362793 = 0.19471898674964905 + 2.0 * 6.089207172393799
Epoch 670, val loss: 0.7073708772659302
Epoch 680, training loss: 12.363032341003418 = 0.18467476963996887 + 2.0 * 6.089178562164307
Epoch 680, val loss: 0.7080265879631042
Epoch 690, training loss: 12.347763061523438 = 0.17516013979911804 + 2.0 * 6.086301326751709
Epoch 690, val loss: 0.7090628743171692
Epoch 700, training loss: 12.339902877807617 = 0.16623851656913757 + 2.0 * 6.086832046508789
Epoch 700, val loss: 0.71048903465271
Epoch 710, training loss: 12.325153350830078 = 0.15780112147331238 + 2.0 * 6.083676338195801
Epoch 710, val loss: 0.7122491002082825
Epoch 720, training loss: 12.31548023223877 = 0.14986801147460938 + 2.0 * 6.08280611038208
Epoch 720, val loss: 0.7144024968147278
Epoch 730, training loss: 12.306584358215332 = 0.1423690915107727 + 2.0 * 6.0821075439453125
Epoch 730, val loss: 0.7169317007064819
Epoch 740, training loss: 12.304513931274414 = 0.13530363142490387 + 2.0 * 6.0846052169799805
Epoch 740, val loss: 0.7196506857872009
Epoch 750, training loss: 12.288822174072266 = 0.12866289913654327 + 2.0 * 6.080079555511475
Epoch 750, val loss: 0.7226480841636658
Epoch 760, training loss: 12.281929969787598 = 0.12239360064268112 + 2.0 * 6.079768180847168
Epoch 760, val loss: 0.7260435223579407
Epoch 770, training loss: 12.268769264221191 = 0.1164761334657669 + 2.0 * 6.076146602630615
Epoch 770, val loss: 0.7296512126922607
Epoch 780, training loss: 12.261249542236328 = 0.11088288575410843 + 2.0 * 6.075183391571045
Epoch 780, val loss: 0.733615517616272
Epoch 790, training loss: 12.253721237182617 = 0.10558407008647919 + 2.0 * 6.074068546295166
Epoch 790, val loss: 0.7377147078514099
Epoch 800, training loss: 12.270293235778809 = 0.10055238753557205 + 2.0 * 6.084870338439941
Epoch 800, val loss: 0.742111325263977
Epoch 810, training loss: 12.24326229095459 = 0.0958576649427414 + 2.0 * 6.073702335357666
Epoch 810, val loss: 0.7465629577636719
Epoch 820, training loss: 12.236777305603027 = 0.09139382094144821 + 2.0 * 6.072691917419434
Epoch 820, val loss: 0.7513245344161987
Epoch 830, training loss: 12.232254981994629 = 0.08715982735157013 + 2.0 * 6.072547435760498
Epoch 830, val loss: 0.756093442440033
Epoch 840, training loss: 12.220788955688477 = 0.08316504955291748 + 2.0 * 6.068811893463135
Epoch 840, val loss: 0.7611340284347534
Epoch 850, training loss: 12.21805191040039 = 0.07938771694898605 + 2.0 * 6.069332122802734
Epoch 850, val loss: 0.7661541700363159
Epoch 860, training loss: 12.215232849121094 = 0.07583735883235931 + 2.0 * 6.069697856903076
Epoch 860, val loss: 0.7713668942451477
Epoch 870, training loss: 12.209178924560547 = 0.07248583436012268 + 2.0 * 6.0683465003967285
Epoch 870, val loss: 0.7765778303146362
Epoch 880, training loss: 12.19968032836914 = 0.06932804733514786 + 2.0 * 6.065176010131836
Epoch 880, val loss: 0.7819741368293762
Epoch 890, training loss: 12.195523262023926 = 0.06634567677974701 + 2.0 * 6.064589023590088
Epoch 890, val loss: 0.7873049974441528
Epoch 900, training loss: 12.199533462524414 = 0.06353221833705902 + 2.0 * 6.068000793457031
Epoch 900, val loss: 0.792767345905304
Epoch 910, training loss: 12.191311836242676 = 0.060880038887262344 + 2.0 * 6.065216064453125
Epoch 910, val loss: 0.7982626557350159
Epoch 920, training loss: 12.182003021240234 = 0.05836379528045654 + 2.0 * 6.061819553375244
Epoch 920, val loss: 0.8036919236183167
Epoch 930, training loss: 12.17750358581543 = 0.05599670484662056 + 2.0 * 6.060753345489502
Epoch 930, val loss: 0.8091806769371033
Epoch 940, training loss: 12.193927764892578 = 0.05374594032764435 + 2.0 * 6.0700907707214355
Epoch 940, val loss: 0.8144721984863281
Epoch 950, training loss: 12.17380428314209 = 0.051650259643793106 + 2.0 * 6.061077117919922
Epoch 950, val loss: 0.819861888885498
Epoch 960, training loss: 12.16443920135498 = 0.049659714102745056 + 2.0 * 6.057389736175537
Epoch 960, val loss: 0.8253594040870667
Epoch 970, training loss: 12.161700248718262 = 0.04777635261416435 + 2.0 * 6.056962013244629
Epoch 970, val loss: 0.8307519555091858
Epoch 980, training loss: 12.160991668701172 = 0.045984070748090744 + 2.0 * 6.057503700256348
Epoch 980, val loss: 0.8361093997955322
Epoch 990, training loss: 12.157003402709961 = 0.04428636282682419 + 2.0 * 6.056358337402344
Epoch 990, val loss: 0.8414115905761719
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7889
Overall ASR: 0.6790
Flip ASR: 0.6222/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.698545455932617 = 1.9507122039794922 + 2.0 * 8.373916625976562
Epoch 0, val loss: 1.9573694467544556
Epoch 10, training loss: 18.687246322631836 = 1.940124750137329 + 2.0 * 8.373560905456543
Epoch 10, val loss: 1.9458345174789429
Epoch 20, training loss: 18.66877555847168 = 1.9271048307418823 + 2.0 * 8.370835304260254
Epoch 20, val loss: 1.9314810037612915
Epoch 30, training loss: 18.607650756835938 = 1.909355878829956 + 2.0 * 8.34914779663086
Epoch 30, val loss: 1.9120813608169556
Epoch 40, training loss: 18.279644012451172 = 1.8875479698181152 + 2.0 * 8.19604778289795
Epoch 40, val loss: 1.8893184661865234
Epoch 50, training loss: 17.353958129882812 = 1.8636102676391602 + 2.0 * 7.745174407958984
Epoch 50, val loss: 1.8648254871368408
Epoch 60, training loss: 16.37276268005371 = 1.844591498374939 + 2.0 * 7.264085292816162
Epoch 60, val loss: 1.8467326164245605
Epoch 70, training loss: 15.676809310913086 = 1.8325484991073608 + 2.0 * 6.922130584716797
Epoch 70, val loss: 1.834670066833496
Epoch 80, training loss: 15.312325477600098 = 1.8185354471206665 + 2.0 * 6.746894836425781
Epoch 80, val loss: 1.8208138942718506
Epoch 90, training loss: 15.062082290649414 = 1.80158531665802 + 2.0 * 6.630248546600342
Epoch 90, val loss: 1.804584264755249
Epoch 100, training loss: 14.87503433227539 = 1.785364031791687 + 2.0 * 6.544835090637207
Epoch 100, val loss: 1.7889078855514526
Epoch 110, training loss: 14.730271339416504 = 1.7698670625686646 + 2.0 * 6.4802021980285645
Epoch 110, val loss: 1.773711085319519
Epoch 120, training loss: 14.614521026611328 = 1.7542753219604492 + 2.0 * 6.4301228523254395
Epoch 120, val loss: 1.7584480047225952
Epoch 130, training loss: 14.516275405883789 = 1.7380645275115967 + 2.0 * 6.389105319976807
Epoch 130, val loss: 1.7429426908493042
Epoch 140, training loss: 14.431527137756348 = 1.7208045721054077 + 2.0 * 6.355361461639404
Epoch 140, val loss: 1.7269904613494873
Epoch 150, training loss: 14.356712341308594 = 1.7021769285202026 + 2.0 * 6.327267646789551
Epoch 150, val loss: 1.710363507270813
Epoch 160, training loss: 14.289422035217285 = 1.6817586421966553 + 2.0 * 6.303831577301025
Epoch 160, val loss: 1.6927086114883423
Epoch 170, training loss: 14.224353790283203 = 1.6591641902923584 + 2.0 * 6.282594680786133
Epoch 170, val loss: 1.673836350440979
Epoch 180, training loss: 14.170703887939453 = 1.6341382265090942 + 2.0 * 6.268282890319824
Epoch 180, val loss: 1.6534538269042969
Epoch 190, training loss: 14.108575820922852 = 1.6065826416015625 + 2.0 * 6.2509965896606445
Epoch 190, val loss: 1.631401538848877
Epoch 200, training loss: 14.052176475524902 = 1.5761786699295044 + 2.0 * 6.237998962402344
Epoch 200, val loss: 1.607398271560669
Epoch 210, training loss: 13.995928764343262 = 1.5427629947662354 + 2.0 * 6.226583003997803
Epoch 210, val loss: 1.581308126449585
Epoch 220, training loss: 13.94061279296875 = 1.5067812204360962 + 2.0 * 6.216915607452393
Epoch 220, val loss: 1.5533853769302368
Epoch 230, training loss: 13.886524200439453 = 1.4683769941329956 + 2.0 * 6.209073543548584
Epoch 230, val loss: 1.5239557027816772
Epoch 240, training loss: 13.82795524597168 = 1.428348422050476 + 2.0 * 6.199803352355957
Epoch 240, val loss: 1.4933868646621704
Epoch 250, training loss: 13.769540786743164 = 1.3865611553192139 + 2.0 * 6.1914896965026855
Epoch 250, val loss: 1.461932897567749
Epoch 260, training loss: 13.726202964782715 = 1.3436146974563599 + 2.0 * 6.191294193267822
Epoch 260, val loss: 1.4299547672271729
Epoch 270, training loss: 13.661477088928223 = 1.3006227016448975 + 2.0 * 6.180427074432373
Epoch 270, val loss: 1.398377776145935
Epoch 280, training loss: 13.603276252746582 = 1.2577019929885864 + 2.0 * 6.172787189483643
Epoch 280, val loss: 1.3671793937683105
Epoch 290, training loss: 13.554462432861328 = 1.2148785591125488 + 2.0 * 6.1697916984558105
Epoch 290, val loss: 1.3362557888031006
Epoch 300, training loss: 13.499673843383789 = 1.1727288961410522 + 2.0 * 6.163472652435303
Epoch 300, val loss: 1.3056957721710205
Epoch 310, training loss: 13.445853233337402 = 1.1312066316604614 + 2.0 * 6.157323360443115
Epoch 310, val loss: 1.2756776809692383
Epoch 320, training loss: 13.394431114196777 = 1.0900726318359375 + 2.0 * 6.15217924118042
Epoch 320, val loss: 1.2460728883743286
Epoch 330, training loss: 13.34681224822998 = 1.049478530883789 + 2.0 * 6.148666858673096
Epoch 330, val loss: 1.2169170379638672
Epoch 340, training loss: 13.298576354980469 = 1.0095621347427368 + 2.0 * 6.144506931304932
Epoch 340, val loss: 1.1882356405258179
Epoch 350, training loss: 13.251760482788086 = 0.9707751274108887 + 2.0 * 6.1404924392700195
Epoch 350, val loss: 1.1605144739151
Epoch 360, training loss: 13.21293830871582 = 0.9331148862838745 + 2.0 * 6.139911651611328
Epoch 360, val loss: 1.1337058544158936
Epoch 370, training loss: 13.161360740661621 = 0.8967008590698242 + 2.0 * 6.132329940795898
Epoch 370, val loss: 1.108013391494751
Epoch 380, training loss: 13.120062828063965 = 0.8616119027137756 + 2.0 * 6.129225254058838
Epoch 380, val loss: 1.0833871364593506
Epoch 390, training loss: 13.079492568969727 = 0.8276781439781189 + 2.0 * 6.1259074211120605
Epoch 390, val loss: 1.0597714185714722
Epoch 400, training loss: 13.044994354248047 = 0.7948225140571594 + 2.0 * 6.125085830688477
Epoch 400, val loss: 1.0370887517929077
Epoch 410, training loss: 13.0067138671875 = 0.7634320259094238 + 2.0 * 6.121641159057617
Epoch 410, val loss: 1.0155714750289917
Epoch 420, training loss: 12.967606544494629 = 0.7330237627029419 + 2.0 * 6.117291450500488
Epoch 420, val loss: 0.995007336139679
Epoch 430, training loss: 12.933576583862305 = 0.7032936215400696 + 2.0 * 6.11514139175415
Epoch 430, val loss: 0.9751614332199097
Epoch 440, training loss: 12.898760795593262 = 0.6744067668914795 + 2.0 * 6.112176895141602
Epoch 440, val loss: 0.9561925530433655
Epoch 450, training loss: 12.870899200439453 = 0.6462631821632385 + 2.0 * 6.11231803894043
Epoch 450, val loss: 0.9381300806999207
Epoch 460, training loss: 12.833901405334473 = 0.6187572479248047 + 2.0 * 6.107572078704834
Epoch 460, val loss: 0.920952558517456
Epoch 470, training loss: 12.800921440124512 = 0.5918242335319519 + 2.0 * 6.104548454284668
Epoch 470, val loss: 0.9044173955917358
Epoch 480, training loss: 12.784976959228516 = 0.5654609203338623 + 2.0 * 6.109757900238037
Epoch 480, val loss: 0.8886950016021729
Epoch 490, training loss: 12.740294456481934 = 0.5398768186569214 + 2.0 * 6.100208759307861
Epoch 490, val loss: 0.8736465573310852
Epoch 500, training loss: 12.711566925048828 = 0.5149632096290588 + 2.0 * 6.098301887512207
Epoch 500, val loss: 0.8594653010368347
Epoch 510, training loss: 12.694893836975098 = 0.49085527658462524 + 2.0 * 6.102019309997559
Epoch 510, val loss: 0.8462751507759094
Epoch 520, training loss: 12.661092758178711 = 0.4676467180252075 + 2.0 * 6.0967230796813965
Epoch 520, val loss: 0.8341449499130249
Epoch 530, training loss: 12.630433082580566 = 0.44530367851257324 + 2.0 * 6.092564582824707
Epoch 530, val loss: 0.823059618473053
Epoch 540, training loss: 12.604650497436523 = 0.42362266778945923 + 2.0 * 6.090513706207275
Epoch 540, val loss: 0.8127739429473877
Epoch 550, training loss: 12.594539642333984 = 0.402536541223526 + 2.0 * 6.096001625061035
Epoch 550, val loss: 0.8031739592552185
Epoch 560, training loss: 12.559992790222168 = 0.38219666481018066 + 2.0 * 6.088898181915283
Epoch 560, val loss: 0.7946310639381409
Epoch 570, training loss: 12.535008430480957 = 0.3625188171863556 + 2.0 * 6.086244583129883
Epoch 570, val loss: 0.7868296504020691
Epoch 580, training loss: 12.511579513549805 = 0.3435244560241699 + 2.0 * 6.084027290344238
Epoch 580, val loss: 0.7798576354980469
Epoch 590, training loss: 12.489629745483398 = 0.3253326416015625 + 2.0 * 6.082148551940918
Epoch 590, val loss: 0.7739176750183105
Epoch 600, training loss: 12.469968795776367 = 0.30792760848999023 + 2.0 * 6.081020355224609
Epoch 600, val loss: 0.7685502171516418
Epoch 610, training loss: 12.44954776763916 = 0.29135534167289734 + 2.0 * 6.07909631729126
Epoch 610, val loss: 0.7641016840934753
Epoch 620, training loss: 12.430113792419434 = 0.2755459249019623 + 2.0 * 6.07728385925293
Epoch 620, val loss: 0.7604575157165527
Epoch 630, training loss: 12.416664123535156 = 0.2604213356971741 + 2.0 * 6.078121185302734
Epoch 630, val loss: 0.7575516104698181
Epoch 640, training loss: 12.400153160095215 = 0.24601879715919495 + 2.0 * 6.0770673751831055
Epoch 640, val loss: 0.7553120255470276
Epoch 650, training loss: 12.37946891784668 = 0.23239992558956146 + 2.0 * 6.0735344886779785
Epoch 650, val loss: 0.7537738084793091
Epoch 660, training loss: 12.364108085632324 = 0.21940486133098602 + 2.0 * 6.072351455688477
Epoch 660, val loss: 0.7528256773948669
Epoch 670, training loss: 12.34961223602295 = 0.20705105364322662 + 2.0 * 6.071280479431152
Epoch 670, val loss: 0.7525421977043152
Epoch 680, training loss: 12.338239669799805 = 0.1954123079776764 + 2.0 * 6.071413516998291
Epoch 680, val loss: 0.7528027296066284
Epoch 690, training loss: 12.323928833007812 = 0.1844351887702942 + 2.0 * 6.069746971130371
Epoch 690, val loss: 0.7537733912467957
Epoch 700, training loss: 12.309507369995117 = 0.17414003610610962 + 2.0 * 6.067683696746826
Epoch 700, val loss: 0.7553839683532715
Epoch 710, training loss: 12.310236930847168 = 0.16445133090019226 + 2.0 * 6.072892665863037
Epoch 710, val loss: 0.7575942277908325
Epoch 720, training loss: 12.293506622314453 = 0.1554127186536789 + 2.0 * 6.069046974182129
Epoch 720, val loss: 0.760132372379303
Epoch 730, training loss: 12.274415016174316 = 0.14694342017173767 + 2.0 * 6.0637359619140625
Epoch 730, val loss: 0.763282835483551
Epoch 740, training loss: 12.263989448547363 = 0.13902533054351807 + 2.0 * 6.062481880187988
Epoch 740, val loss: 0.7667331099510193
Epoch 750, training loss: 12.256762504577637 = 0.13159562647342682 + 2.0 * 6.0625834465026855
Epoch 750, val loss: 0.7705192565917969
Epoch 760, training loss: 12.255372047424316 = 0.12472119182348251 + 2.0 * 6.0653252601623535
Epoch 760, val loss: 0.7745850086212158
Epoch 770, training loss: 12.239315032958984 = 0.11840732395648956 + 2.0 * 6.06045389175415
Epoch 770, val loss: 0.7790659666061401
Epoch 780, training loss: 12.229211807250977 = 0.11251368373632431 + 2.0 * 6.058349132537842
Epoch 780, val loss: 0.7838356494903564
Epoch 790, training loss: 12.220585823059082 = 0.10698799788951874 + 2.0 * 6.056798934936523
Epoch 790, val loss: 0.7888010144233704
Epoch 800, training loss: 12.213519096374512 = 0.10180258005857468 + 2.0 * 6.055858135223389
Epoch 800, val loss: 0.7939934134483337
Epoch 810, training loss: 12.209737777709961 = 0.09693637490272522 + 2.0 * 6.056400775909424
Epoch 810, val loss: 0.7993539571762085
Epoch 820, training loss: 12.212128639221191 = 0.09237285703420639 + 2.0 * 6.059877872467041
Epoch 820, val loss: 0.8047972321510315
Epoch 830, training loss: 12.19716739654541 = 0.08814848959445953 + 2.0 * 6.05450963973999
Epoch 830, val loss: 0.8103675842285156
Epoch 840, training loss: 12.189589500427246 = 0.0841904878616333 + 2.0 * 6.052699565887451
Epoch 840, val loss: 0.8160889744758606
Epoch 850, training loss: 12.182711601257324 = 0.0804673582315445 + 2.0 * 6.051122188568115
Epoch 850, val loss: 0.8218945264816284
Epoch 860, training loss: 12.181903839111328 = 0.07695301622152328 + 2.0 * 6.052475452423096
Epoch 860, val loss: 0.8277326822280884
Epoch 870, training loss: 12.175966262817383 = 0.07364188879728317 + 2.0 * 6.051162242889404
Epoch 870, val loss: 0.8334437012672424
Epoch 880, training loss: 12.171812057495117 = 0.0705346092581749 + 2.0 * 6.050638675689697
Epoch 880, val loss: 0.8393513560295105
Epoch 890, training loss: 12.164828300476074 = 0.06761040538549423 + 2.0 * 6.048608779907227
Epoch 890, val loss: 0.8452707529067993
Epoch 900, training loss: 12.168768882751465 = 0.06484349817037582 + 2.0 * 6.051962852478027
Epoch 900, val loss: 0.8511277437210083
Epoch 910, training loss: 12.157303810119629 = 0.06223856657743454 + 2.0 * 6.047532558441162
Epoch 910, val loss: 0.8570437431335449
Epoch 920, training loss: 12.151293754577637 = 0.059779178351163864 + 2.0 * 6.045757293701172
Epoch 920, val loss: 0.8629728555679321
Epoch 930, training loss: 12.146584510803223 = 0.057442788034677505 + 2.0 * 6.0445709228515625
Epoch 930, val loss: 0.8688948750495911
Epoch 940, training loss: 12.145883560180664 = 0.05522529035806656 + 2.0 * 6.0453290939331055
Epoch 940, val loss: 0.8748072385787964
Epoch 950, training loss: 12.145975112915039 = 0.05312137305736542 + 2.0 * 6.046426773071289
Epoch 950, val loss: 0.8806290030479431
Epoch 960, training loss: 12.148992538452148 = 0.05112556368112564 + 2.0 * 6.048933506011963
Epoch 960, val loss: 0.8864406943321228
Epoch 970, training loss: 12.139164924621582 = 0.04925334453582764 + 2.0 * 6.044955730438232
Epoch 970, val loss: 0.8921741247177124
Epoch 980, training loss: 12.132981300354004 = 0.0474642850458622 + 2.0 * 6.042758464813232
Epoch 980, val loss: 0.8979101181030273
Epoch 990, training loss: 12.126565933227539 = 0.045767735689878464 + 2.0 * 6.040399074554443
Epoch 990, val loss: 0.9036730527877808
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8148
Overall ASR: 0.6568
Flip ASR: 0.5867/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.713346481323242 = 1.9655048847198486 + 2.0 * 8.373920440673828
Epoch 0, val loss: 1.9661675691604614
Epoch 10, training loss: 18.701913833618164 = 1.954744577407837 + 2.0 * 8.373584747314453
Epoch 10, val loss: 1.9554405212402344
Epoch 20, training loss: 18.684141159057617 = 1.9419727325439453 + 2.0 * 8.371084213256836
Epoch 20, val loss: 1.9420968294143677
Epoch 30, training loss: 18.62875747680664 = 1.9252638816833496 + 2.0 * 8.351746559143066
Epoch 30, val loss: 1.924017310142517
Epoch 40, training loss: 18.329341888427734 = 1.9049879312515259 + 2.0 * 8.212177276611328
Epoch 40, val loss: 1.9023611545562744
Epoch 50, training loss: 16.894563674926758 = 1.8855966329574585 + 2.0 * 7.504483699798584
Epoch 50, val loss: 1.8825119733810425
Epoch 60, training loss: 16.262998580932617 = 1.868517279624939 + 2.0 * 7.197240352630615
Epoch 60, val loss: 1.866172432899475
Epoch 70, training loss: 15.672061920166016 = 1.8534770011901855 + 2.0 * 6.909292221069336
Epoch 70, val loss: 1.851146936416626
Epoch 80, training loss: 15.259353637695312 = 1.839388132095337 + 2.0 * 6.709982872009277
Epoch 80, val loss: 1.8369781970977783
Epoch 90, training loss: 15.022868156433105 = 1.8241184949874878 + 2.0 * 6.599374771118164
Epoch 90, val loss: 1.8219335079193115
Epoch 100, training loss: 14.81938648223877 = 1.8077991008758545 + 2.0 * 6.505793571472168
Epoch 100, val loss: 1.8062670230865479
Epoch 110, training loss: 14.676876068115234 = 1.791702389717102 + 2.0 * 6.442586898803711
Epoch 110, val loss: 1.7908732891082764
Epoch 120, training loss: 14.568291664123535 = 1.7753502130508423 + 2.0 * 6.396470546722412
Epoch 120, val loss: 1.7751202583312988
Epoch 130, training loss: 14.480334281921387 = 1.7583943605422974 + 2.0 * 6.3609700202941895
Epoch 130, val loss: 1.7587671279907227
Epoch 140, training loss: 14.404505729675293 = 1.7407243251800537 + 2.0 * 6.33189058303833
Epoch 140, val loss: 1.7420886754989624
Epoch 150, training loss: 14.336252212524414 = 1.7219290733337402 + 2.0 * 6.307161331176758
Epoch 150, val loss: 1.7247337102890015
Epoch 160, training loss: 14.274986267089844 = 1.7014143466949463 + 2.0 * 6.286786079406738
Epoch 160, val loss: 1.7061799764633179
Epoch 170, training loss: 14.227912902832031 = 1.6786751747131348 + 2.0 * 6.274619102478027
Epoch 170, val loss: 1.6860289573669434
Epoch 180, training loss: 14.170398712158203 = 1.653388500213623 + 2.0 * 6.258504867553711
Epoch 180, val loss: 1.6642249822616577
Epoch 190, training loss: 14.111767768859863 = 1.6253297328948975 + 2.0 * 6.243218898773193
Epoch 190, val loss: 1.6404727697372437
Epoch 200, training loss: 14.057334899902344 = 1.5940663814544678 + 2.0 * 6.231634140014648
Epoch 200, val loss: 1.6144459247589111
Epoch 210, training loss: 14.005145072937012 = 1.559342384338379 + 2.0 * 6.222901344299316
Epoch 210, val loss: 1.5860447883605957
Epoch 220, training loss: 13.948870658874512 = 1.521503210067749 + 2.0 * 6.213683605194092
Epoch 220, val loss: 1.5556080341339111
Epoch 230, training loss: 13.889583587646484 = 1.4806724786758423 + 2.0 * 6.204455375671387
Epoch 230, val loss: 1.523311972618103
Epoch 240, training loss: 13.82964038848877 = 1.4367969036102295 + 2.0 * 6.1964216232299805
Epoch 240, val loss: 1.4891918897628784
Epoch 250, training loss: 13.768908500671387 = 1.3900240659713745 + 2.0 * 6.189442157745361
Epoch 250, val loss: 1.4535152912139893
Epoch 260, training loss: 13.709339141845703 = 1.3409355878829956 + 2.0 * 6.184201717376709
Epoch 260, val loss: 1.416961908340454
Epoch 270, training loss: 13.64893627166748 = 1.2910308837890625 + 2.0 * 6.178952693939209
Epoch 270, val loss: 1.3802151679992676
Epoch 280, training loss: 13.583328247070312 = 1.2407360076904297 + 2.0 * 6.171296119689941
Epoch 280, val loss: 1.3437484502792358
Epoch 290, training loss: 13.522435188293457 = 1.1901566982269287 + 2.0 * 6.166139125823975
Epoch 290, val loss: 1.3075048923492432
Epoch 300, training loss: 13.466651916503906 = 1.1397526264190674 + 2.0 * 6.163449764251709
Epoch 300, val loss: 1.2718074321746826
Epoch 310, training loss: 13.40799331665039 = 1.09042227268219 + 2.0 * 6.158785343170166
Epoch 310, val loss: 1.236997127532959
Epoch 320, training loss: 13.347357749938965 = 1.0426002740859985 + 2.0 * 6.152378559112549
Epoch 320, val loss: 1.2036327123641968
Epoch 330, training loss: 13.293431282043457 = 0.9962395429611206 + 2.0 * 6.148595809936523
Epoch 330, val loss: 1.1716715097427368
Epoch 340, training loss: 13.244619369506836 = 0.9519349336624146 + 2.0 * 6.1463422775268555
Epoch 340, val loss: 1.141227126121521
Epoch 350, training loss: 13.194239616394043 = 0.909695029258728 + 2.0 * 6.142272472381592
Epoch 350, val loss: 1.1127346754074097
Epoch 360, training loss: 13.142206192016602 = 0.8693956732749939 + 2.0 * 6.1364054679870605
Epoch 360, val loss: 1.086024284362793
Epoch 370, training loss: 13.100879669189453 = 0.8308694958686829 + 2.0 * 6.135004997253418
Epoch 370, val loss: 1.0608789920806885
Epoch 380, training loss: 13.056927680969238 = 0.7941572666168213 + 2.0 * 6.131385326385498
Epoch 380, val loss: 1.0373876094818115
Epoch 390, training loss: 13.016942024230957 = 0.7592303156852722 + 2.0 * 6.1288557052612305
Epoch 390, val loss: 1.0152760744094849
Epoch 400, training loss: 12.972801208496094 = 0.7258992791175842 + 2.0 * 6.123450756072998
Epoch 400, val loss: 0.9948537349700928
Epoch 410, training loss: 12.940378189086914 = 0.693903923034668 + 2.0 * 6.123237133026123
Epoch 410, val loss: 0.9754180908203125
Epoch 420, training loss: 12.902875900268555 = 0.6630973219871521 + 2.0 * 6.119889259338379
Epoch 420, val loss: 0.957156240940094
Epoch 430, training loss: 12.863801002502441 = 0.6335353851318359 + 2.0 * 6.115132808685303
Epoch 430, val loss: 0.9400514364242554
Epoch 440, training loss: 12.829444885253906 = 0.6049833297729492 + 2.0 * 6.1122307777404785
Epoch 440, val loss: 0.9237463474273682
Epoch 450, training loss: 12.802120208740234 = 0.5772891044616699 + 2.0 * 6.112415313720703
Epoch 450, val loss: 0.9083104729652405
Epoch 460, training loss: 12.776880264282227 = 0.550611138343811 + 2.0 * 6.113134384155273
Epoch 460, val loss: 0.8937009572982788
Epoch 470, training loss: 12.736220359802246 = 0.5251979231834412 + 2.0 * 6.10551118850708
Epoch 470, val loss: 0.8799693584442139
Epoch 480, training loss: 12.707417488098145 = 0.5008372664451599 + 2.0 * 6.10329008102417
Epoch 480, val loss: 0.867377758026123
Epoch 490, training loss: 12.68049430847168 = 0.477450966835022 + 2.0 * 6.1015214920043945
Epoch 490, val loss: 0.8556363582611084
Epoch 500, training loss: 12.659727096557617 = 0.45505866408348083 + 2.0 * 6.102334022521973
Epoch 500, val loss: 0.8449313640594482
Epoch 510, training loss: 12.630806922912598 = 0.43387189507484436 + 2.0 * 6.0984673500061035
Epoch 510, val loss: 0.8351002931594849
Epoch 520, training loss: 12.612900733947754 = 0.41364121437072754 + 2.0 * 6.099629878997803
Epoch 520, val loss: 0.8265327215194702
Epoch 530, training loss: 12.583582878112793 = 0.3945348858833313 + 2.0 * 6.094523906707764
Epoch 530, val loss: 0.8188090324401855
Epoch 540, training loss: 12.560159683227539 = 0.3763764798641205 + 2.0 * 6.091891765594482
Epoch 540, val loss: 0.8122297525405884
Epoch 550, training loss: 12.541261672973633 = 0.35911038517951965 + 2.0 * 6.091075420379639
Epoch 550, val loss: 0.8065319061279297
Epoch 560, training loss: 12.523494720458984 = 0.3426878750324249 + 2.0 * 6.0904035568237305
Epoch 560, val loss: 0.8018037676811218
Epoch 570, training loss: 12.505474090576172 = 0.32715466618537903 + 2.0 * 6.0891594886779785
Epoch 570, val loss: 0.797844409942627
Epoch 580, training loss: 12.483948707580566 = 0.312328040599823 + 2.0 * 6.08581018447876
Epoch 580, val loss: 0.7947913408279419
Epoch 590, training loss: 12.463777542114258 = 0.2981792688369751 + 2.0 * 6.082798957824707
Epoch 590, val loss: 0.7925779223442078
Epoch 600, training loss: 12.459802627563477 = 0.28459325432777405 + 2.0 * 6.087604522705078
Epoch 600, val loss: 0.7910540699958801
Epoch 610, training loss: 12.439988136291504 = 0.271506667137146 + 2.0 * 6.084240913391113
Epoch 610, val loss: 0.7902657985687256
Epoch 620, training loss: 12.416933059692383 = 0.25902631878852844 + 2.0 * 6.078953266143799
Epoch 620, val loss: 0.7899829149246216
Epoch 630, training loss: 12.403609275817871 = 0.2470121532678604 + 2.0 * 6.078298568725586
Epoch 630, val loss: 0.7903560400009155
Epoch 640, training loss: 12.393424034118652 = 0.2353902906179428 + 2.0 * 6.07901668548584
Epoch 640, val loss: 0.791256308555603
Epoch 650, training loss: 12.377828598022461 = 0.22429142892360687 + 2.0 * 6.076768398284912
Epoch 650, val loss: 0.7925548553466797
Epoch 660, training loss: 12.361292839050293 = 0.21360166370868683 + 2.0 * 6.073845386505127
Epoch 660, val loss: 0.794286847114563
Epoch 670, training loss: 12.349547386169434 = 0.20339812338352203 + 2.0 * 6.073074817657471
Epoch 670, val loss: 0.7964187860488892
Epoch 680, training loss: 12.337735176086426 = 0.19360215961933136 + 2.0 * 6.072066307067871
Epoch 680, val loss: 0.7990497946739197
Epoch 690, training loss: 12.32451343536377 = 0.1842678189277649 + 2.0 * 6.070122718811035
Epoch 690, val loss: 0.8017989993095398
Epoch 700, training loss: 12.311637878417969 = 0.17537783086299896 + 2.0 * 6.068130016326904
Epoch 700, val loss: 0.8049602508544922
Epoch 710, training loss: 12.317453384399414 = 0.16693364083766937 + 2.0 * 6.075259685516357
Epoch 710, val loss: 0.808345377445221
Epoch 720, training loss: 12.295413970947266 = 0.15891528129577637 + 2.0 * 6.068249225616455
Epoch 720, val loss: 0.8119393587112427
Epoch 730, training loss: 12.283513069152832 = 0.15135149657726288 + 2.0 * 6.066080570220947
Epoch 730, val loss: 0.8156845569610596
Epoch 740, training loss: 12.270711898803711 = 0.14422346651554108 + 2.0 * 6.063244342803955
Epoch 740, val loss: 0.8196884393692017
Epoch 750, training loss: 12.267539978027344 = 0.13747023046016693 + 2.0 * 6.065034866333008
Epoch 750, val loss: 0.8238885998725891
Epoch 760, training loss: 12.25635051727295 = 0.13106253743171692 + 2.0 * 6.062644004821777
Epoch 760, val loss: 0.8281510472297668
Epoch 770, training loss: 12.257119178771973 = 0.1250593662261963 + 2.0 * 6.066030025482178
Epoch 770, val loss: 0.8325605988502502
Epoch 780, training loss: 12.23995590209961 = 0.11941289156675339 + 2.0 * 6.060271739959717
Epoch 780, val loss: 0.8368816375732422
Epoch 790, training loss: 12.230179786682129 = 0.11407386511564255 + 2.0 * 6.058053016662598
Epoch 790, val loss: 0.8414512872695923
Epoch 800, training loss: 12.225435256958008 = 0.10902919620275497 + 2.0 * 6.058203220367432
Epoch 800, val loss: 0.8461342453956604
Epoch 810, training loss: 12.21973705291748 = 0.10425129532814026 + 2.0 * 6.057743072509766
Epoch 810, val loss: 0.8508716821670532
Epoch 820, training loss: 12.215003967285156 = 0.09975229948759079 + 2.0 * 6.057625770568848
Epoch 820, val loss: 0.8555944561958313
Epoch 830, training loss: 12.2076416015625 = 0.09549745917320251 + 2.0 * 6.056072235107422
Epoch 830, val loss: 0.8602614998817444
Epoch 840, training loss: 12.19892406463623 = 0.09147368371486664 + 2.0 * 6.053725242614746
Epoch 840, val loss: 0.8650962114334106
Epoch 850, training loss: 12.202239036560059 = 0.08766782283782959 + 2.0 * 6.057285785675049
Epoch 850, val loss: 0.8699322938919067
Epoch 860, training loss: 12.188087463378906 = 0.08406036347150803 + 2.0 * 6.052013397216797
Epoch 860, val loss: 0.8748872876167297
Epoch 870, training loss: 12.183042526245117 = 0.08066069334745407 + 2.0 * 6.0511908531188965
Epoch 870, val loss: 0.8797823786735535
Epoch 880, training loss: 12.193236351013184 = 0.077437624335289 + 2.0 * 6.057899475097656
Epoch 880, val loss: 0.8847965002059937
Epoch 890, training loss: 12.176417350769043 = 0.07442368566989899 + 2.0 * 6.050996780395508
Epoch 890, val loss: 0.8897189497947693
Epoch 900, training loss: 12.168559074401855 = 0.07154028117656708 + 2.0 * 6.04850959777832
Epoch 900, val loss: 0.8947088122367859
Epoch 910, training loss: 12.163354873657227 = 0.0688173770904541 + 2.0 * 6.047268867492676
Epoch 910, val loss: 0.8997759222984314
Epoch 920, training loss: 12.170951843261719 = 0.06621988862752914 + 2.0 * 6.052365779876709
Epoch 920, val loss: 0.9048317670822144
Epoch 930, training loss: 12.16789436340332 = 0.06377607583999634 + 2.0 * 6.052059173583984
Epoch 930, val loss: 0.9099169969558716
Epoch 940, training loss: 12.151951789855957 = 0.06144653633236885 + 2.0 * 6.045252799987793
Epoch 940, val loss: 0.9148574471473694
Epoch 950, training loss: 12.148353576660156 = 0.0592372864484787 + 2.0 * 6.044558048248291
Epoch 950, val loss: 0.9198518991470337
Epoch 960, training loss: 12.148841857910156 = 0.05713614821434021 + 2.0 * 6.0458526611328125
Epoch 960, val loss: 0.9249003529548645
Epoch 970, training loss: 12.145962715148926 = 0.055142782628536224 + 2.0 * 6.04541015625
Epoch 970, val loss: 0.9299304485321045
Epoch 980, training loss: 12.137726783752441 = 0.05324706435203552 + 2.0 * 6.042239665985107
Epoch 980, val loss: 0.9347690939903259
Epoch 990, training loss: 12.140358924865723 = 0.05144176632165909 + 2.0 * 6.044458389282227
Epoch 990, val loss: 0.9397668242454529
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7667
Overall ASR: 0.8081
Flip ASR: 0.7733/225 nodes
The final ASR:0.71464, 0.06672, Accuracy:0.79012, 0.01968
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11648])
remove edge: torch.Size([2, 9454])
updated graph: torch.Size([2, 10546])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9889
Flip ASR: 0.9867/225 nodes
The final ASR:0.98155, 0.00603, Accuracy:0.82840, 0.00349
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.4 der2:0.4 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.4, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 18.695133209228516 = 1.947640299797058 + 2.0 * 8.373746871948242
Epoch 0, val loss: 1.9425462484359741
Epoch 10, training loss: 18.682662963867188 = 1.9368778467178345 + 2.0 * 8.372892379760742
Epoch 10, val loss: 1.9327245950698853
Epoch 20, training loss: 18.658418655395508 = 1.923448920249939 + 2.0 * 8.367485046386719
Epoch 20, val loss: 1.9201065301895142
Epoch 30, training loss: 18.566951751708984 = 1.9054147005081177 + 2.0 * 8.330768585205078
Epoch 30, val loss: 1.9028849601745605
Epoch 40, training loss: 18.03994369506836 = 1.8847676515579224 + 2.0 * 8.077588081359863
Epoch 40, val loss: 1.8831188678741455
Epoch 50, training loss: 16.973691940307617 = 1.8594988584518433 + 2.0 * 7.5570969581604
Epoch 50, val loss: 1.8588752746582031
Epoch 60, training loss: 16.29439926147461 = 1.8418022394180298 + 2.0 * 7.2262983322143555
Epoch 60, val loss: 1.8434616327285767
Epoch 70, training loss: 15.595682144165039 = 1.8339893817901611 + 2.0 * 6.8808465003967285
Epoch 70, val loss: 1.8367167711257935
Epoch 80, training loss: 15.295233726501465 = 1.825707197189331 + 2.0 * 6.734763145446777
Epoch 80, val loss: 1.8291863203048706
Epoch 90, training loss: 15.032519340515137 = 1.814732551574707 + 2.0 * 6.608893394470215
Epoch 90, val loss: 1.8193944692611694
Epoch 100, training loss: 14.862173080444336 = 1.8050884008407593 + 2.0 * 6.528542518615723
Epoch 100, val loss: 1.8114068508148193
Epoch 110, training loss: 14.6986083984375 = 1.7981293201446533 + 2.0 * 6.450239658355713
Epoch 110, val loss: 1.8055108785629272
Epoch 120, training loss: 14.584508895874023 = 1.7917786836624146 + 2.0 * 6.396365165710449
Epoch 120, val loss: 1.7996190786361694
Epoch 130, training loss: 14.492715835571289 = 1.7851117849349976 + 2.0 * 6.35380220413208
Epoch 130, val loss: 1.7932132482528687
Epoch 140, training loss: 14.414905548095703 = 1.7784756422042847 + 2.0 * 6.3182148933410645
Epoch 140, val loss: 1.786827564239502
Epoch 150, training loss: 14.348945617675781 = 1.7715272903442383 + 2.0 * 6.2887091636657715
Epoch 150, val loss: 1.7803603410720825
Epoch 160, training loss: 14.295282363891602 = 1.763685703277588 + 2.0 * 6.265798568725586
Epoch 160, val loss: 1.773405909538269
Epoch 170, training loss: 14.246590614318848 = 1.7548433542251587 + 2.0 * 6.24587345123291
Epoch 170, val loss: 1.765852689743042
Epoch 180, training loss: 14.200563430786133 = 1.7449835538864136 + 2.0 * 6.227789878845215
Epoch 180, val loss: 1.757689356803894
Epoch 190, training loss: 14.159564018249512 = 1.733827829360962 + 2.0 * 6.2128682136535645
Epoch 190, val loss: 1.748638391494751
Epoch 200, training loss: 14.1195650100708 = 1.7210556268692017 + 2.0 * 6.199254512786865
Epoch 200, val loss: 1.7383863925933838
Epoch 210, training loss: 14.082219123840332 = 1.7062946557998657 + 2.0 * 6.187962055206299
Epoch 210, val loss: 1.7265604734420776
Epoch 220, training loss: 14.044862747192383 = 1.6891059875488281 + 2.0 * 6.177878379821777
Epoch 220, val loss: 1.7128185033798218
Epoch 230, training loss: 14.006629943847656 = 1.669049620628357 + 2.0 * 6.168790340423584
Epoch 230, val loss: 1.696805715560913
Epoch 240, training loss: 13.96947193145752 = 1.6456544399261475 + 2.0 * 6.1619086265563965
Epoch 240, val loss: 1.6779855489730835
Epoch 250, training loss: 13.928004264831543 = 1.6185694932937622 + 2.0 * 6.154717445373535
Epoch 250, val loss: 1.6563085317611694
Epoch 260, training loss: 13.882214546203613 = 1.587653398513794 + 2.0 * 6.147280693054199
Epoch 260, val loss: 1.6312081813812256
Epoch 270, training loss: 13.837751388549805 = 1.5524382591247559 + 2.0 * 6.142656326293945
Epoch 270, val loss: 1.6025049686431885
Epoch 280, training loss: 13.788474082946777 = 1.5134403705596924 + 2.0 * 6.137516975402832
Epoch 280, val loss: 1.5707523822784424
Epoch 290, training loss: 13.735082626342773 = 1.4714277982711792 + 2.0 * 6.131827354431152
Epoch 290, val loss: 1.536397933959961
Epoch 300, training loss: 13.687044143676758 = 1.4271025657653809 + 2.0 * 6.129970550537109
Epoch 300, val loss: 1.5002723932266235
Epoch 310, training loss: 13.6283597946167 = 1.3820006847381592 + 2.0 * 6.1231794357299805
Epoch 310, val loss: 1.4636503458023071
Epoch 320, training loss: 13.578670501708984 = 1.3371694087982178 + 2.0 * 6.120750427246094
Epoch 320, val loss: 1.4276988506317139
Epoch 330, training loss: 13.527168273925781 = 1.2933439016342163 + 2.0 * 6.116912364959717
Epoch 330, val loss: 1.3929953575134277
Epoch 340, training loss: 13.475603103637695 = 1.2510132789611816 + 2.0 * 6.112295150756836
Epoch 340, val loss: 1.3600835800170898
Epoch 350, training loss: 13.427438735961914 = 1.2101168632507324 + 2.0 * 6.10866117477417
Epoch 350, val loss: 1.3285022974014282
Epoch 360, training loss: 13.380615234375 = 1.170135498046875 + 2.0 * 6.1052398681640625
Epoch 360, val loss: 1.2982428073883057
Epoch 370, training loss: 13.334807395935059 = 1.1310665607452393 + 2.0 * 6.101870536804199
Epoch 370, val loss: 1.2690073251724243
Epoch 380, training loss: 13.291482925415039 = 1.0927722454071045 + 2.0 * 6.099355220794678
Epoch 380, val loss: 1.240767240524292
Epoch 390, training loss: 13.247352600097656 = 1.0553202629089355 + 2.0 * 6.096015930175781
Epoch 390, val loss: 1.2134736776351929
Epoch 400, training loss: 13.205973625183105 = 1.0188026428222656 + 2.0 * 6.09358549118042
Epoch 400, val loss: 1.1872211694717407
Epoch 410, training loss: 13.164563179016113 = 0.9831868410110474 + 2.0 * 6.090688228607178
Epoch 410, val loss: 1.1618008613586426
Epoch 420, training loss: 13.130512237548828 = 0.9487493634223938 + 2.0 * 6.09088134765625
Epoch 420, val loss: 1.1374574899673462
Epoch 430, training loss: 13.08834457397461 = 0.9160427451133728 + 2.0 * 6.086151123046875
Epoch 430, val loss: 1.1146310567855835
Epoch 440, training loss: 13.05135726928711 = 0.8849474191665649 + 2.0 * 6.083204746246338
Epoch 440, val loss: 1.0932742357254028
Epoch 450, training loss: 13.018529891967773 = 0.8557844161987305 + 2.0 * 6.0813727378845215
Epoch 450, val loss: 1.0735617876052856
Epoch 460, training loss: 12.988114356994629 = 0.8289060592651367 + 2.0 * 6.079604148864746
Epoch 460, val loss: 1.0557949542999268
Epoch 470, training loss: 12.958548545837402 = 0.8038008809089661 + 2.0 * 6.07737398147583
Epoch 470, val loss: 1.0395907163619995
Epoch 480, training loss: 12.946940422058105 = 0.7802891135215759 + 2.0 * 6.0833258628845215
Epoch 480, val loss: 1.0247339010238647
Epoch 490, training loss: 12.906253814697266 = 0.7583200931549072 + 2.0 * 6.073966979980469
Epoch 490, val loss: 1.011332631111145
Epoch 500, training loss: 12.8804292678833 = 0.7375465631484985 + 2.0 * 6.071441173553467
Epoch 500, val loss: 0.9990316033363342
Epoch 510, training loss: 12.856494903564453 = 0.7176193594932556 + 2.0 * 6.0694379806518555
Epoch 510, val loss: 0.9875797033309937
Epoch 520, training loss: 12.835488319396973 = 0.6983553171157837 + 2.0 * 6.06856632232666
Epoch 520, val loss: 0.9766462445259094
Epoch 530, training loss: 12.814216613769531 = 0.6796314120292664 + 2.0 * 6.0672926902771
Epoch 530, val loss: 0.9663403630256653
Epoch 540, training loss: 12.790279388427734 = 0.6611436605453491 + 2.0 * 6.064568042755127
Epoch 540, val loss: 0.9563095569610596
Epoch 550, training loss: 12.771227836608887 = 0.6426054835319519 + 2.0 * 6.0643110275268555
Epoch 550, val loss: 0.9463245868682861
Epoch 560, training loss: 12.747302055358887 = 0.6239088773727417 + 2.0 * 6.061696529388428
Epoch 560, val loss: 0.9363028407096863
Epoch 570, training loss: 12.727681159973145 = 0.6049570441246033 + 2.0 * 6.061362266540527
Epoch 570, val loss: 0.9261330366134644
Epoch 580, training loss: 12.703993797302246 = 0.5856817960739136 + 2.0 * 6.0591559410095215
Epoch 580, val loss: 0.9158526659011841
Epoch 590, training loss: 12.680441856384277 = 0.5659775137901306 + 2.0 * 6.05723237991333
Epoch 590, val loss: 0.9052602052688599
Epoch 600, training loss: 12.680352210998535 = 0.5457586646080017 + 2.0 * 6.067296981811523
Epoch 600, val loss: 0.8942624926567078
Epoch 610, training loss: 12.641165733337402 = 0.5254863500595093 + 2.0 * 6.057839870452881
Epoch 610, val loss: 0.8831520676612854
Epoch 620, training loss: 12.615133285522461 = 0.504960298538208 + 2.0 * 6.055086612701416
Epoch 620, val loss: 0.8721259236335754
Epoch 630, training loss: 12.590774536132812 = 0.4842255413532257 + 2.0 * 6.053274631500244
Epoch 630, val loss: 0.8608342409133911
Epoch 640, training loss: 12.567943572998047 = 0.4633115828037262 + 2.0 * 6.052316188812256
Epoch 640, val loss: 0.8494712114334106
Epoch 650, training loss: 12.549765586853027 = 0.4424278438091278 + 2.0 * 6.053668975830078
Epoch 650, val loss: 0.8380787372589111
Epoch 660, training loss: 12.528714179992676 = 0.4218277335166931 + 2.0 * 6.053443431854248
Epoch 660, val loss: 0.827174186706543
Epoch 670, training loss: 12.505224227905273 = 0.4015204608440399 + 2.0 * 6.051851749420166
Epoch 670, val loss: 0.816453218460083
Epoch 680, training loss: 12.480978965759277 = 0.3815620243549347 + 2.0 * 6.049708366394043
Epoch 680, val loss: 0.8061230182647705
Epoch 690, training loss: 12.46100902557373 = 0.3619025945663452 + 2.0 * 6.049553394317627
Epoch 690, val loss: 0.7963127493858337
Epoch 700, training loss: 12.439760208129883 = 0.34276753664016724 + 2.0 * 6.048496246337891
Epoch 700, val loss: 0.7868199944496155
Epoch 710, training loss: 12.416776657104492 = 0.32411912083625793 + 2.0 * 6.046328544616699
Epoch 710, val loss: 0.7780773639678955
Epoch 720, training loss: 12.405014038085938 = 0.30600571632385254 + 2.0 * 6.049504280090332
Epoch 720, val loss: 0.7698402404785156
Epoch 730, training loss: 12.383585929870605 = 0.28856927156448364 + 2.0 * 6.047508239746094
Epoch 730, val loss: 0.7622121572494507
Epoch 740, training loss: 12.361798286437988 = 0.2718413770198822 + 2.0 * 6.044978618621826
Epoch 740, val loss: 0.7552822828292847
Epoch 750, training loss: 12.342503547668457 = 0.2558314800262451 + 2.0 * 6.043335914611816
Epoch 750, val loss: 0.7490509152412415
Epoch 760, training loss: 12.325077056884766 = 0.24055486917495728 + 2.0 * 6.042261123657227
Epoch 760, val loss: 0.7434567213058472
Epoch 770, training loss: 12.312576293945312 = 0.22601072490215302 + 2.0 * 6.043282985687256
Epoch 770, val loss: 0.7384287714958191
Epoch 780, training loss: 12.300073623657227 = 0.21227073669433594 + 2.0 * 6.043901443481445
Epoch 780, val loss: 0.7340275645256042
Epoch 790, training loss: 12.278045654296875 = 0.19947578012943268 + 2.0 * 6.039284706115723
Epoch 790, val loss: 0.7303523421287537
Epoch 800, training loss: 12.26565170288086 = 0.18748056888580322 + 2.0 * 6.039085388183594
Epoch 800, val loss: 0.7273213267326355
Epoch 810, training loss: 12.259208679199219 = 0.17625181376934052 + 2.0 * 6.041478633880615
Epoch 810, val loss: 0.7247911691665649
Epoch 820, training loss: 12.243946075439453 = 0.1658618003129959 + 2.0 * 6.039041996002197
Epoch 820, val loss: 0.7228474617004395
Epoch 830, training loss: 12.23099136352539 = 0.15620407462120056 + 2.0 * 6.037393569946289
Epoch 830, val loss: 0.7214625477790833
Epoch 840, training loss: 12.220176696777344 = 0.14725185930728912 + 2.0 * 6.036462306976318
Epoch 840, val loss: 0.7205578684806824
Epoch 850, training loss: 12.2092866897583 = 0.13896021246910095 + 2.0 * 6.035163402557373
Epoch 850, val loss: 0.7201833724975586
Epoch 860, training loss: 12.204414367675781 = 0.13127972185611725 + 2.0 * 6.036567211151123
Epoch 860, val loss: 0.7201026678085327
Epoch 870, training loss: 12.194568634033203 = 0.124221071600914 + 2.0 * 6.0351738929748535
Epoch 870, val loss: 0.7205537557601929
Epoch 880, training loss: 12.181706428527832 = 0.11767078936100006 + 2.0 * 6.032017707824707
Epoch 880, val loss: 0.7213833928108215
Epoch 890, training loss: 12.174530982971191 = 0.11157062649726868 + 2.0 * 6.031480312347412
Epoch 890, val loss: 0.7225133180618286
Epoch 900, training loss: 12.184764862060547 = 0.10588278621435165 + 2.0 * 6.039441108703613
Epoch 900, val loss: 0.7238444685935974
Epoch 910, training loss: 12.162047386169434 = 0.10067695379257202 + 2.0 * 6.0306854248046875
Epoch 910, val loss: 0.7254472374916077
Epoch 920, training loss: 12.155235290527344 = 0.09582311660051346 + 2.0 * 6.029706001281738
Epoch 920, val loss: 0.7274168133735657
Epoch 930, training loss: 12.148462295532227 = 0.09126918762922287 + 2.0 * 6.0285964012146
Epoch 930, val loss: 0.7295390367507935
Epoch 940, training loss: 12.159392356872559 = 0.08703585714101791 + 2.0 * 6.036178112030029
Epoch 940, val loss: 0.7318363785743713
Epoch 950, training loss: 12.1398286819458 = 0.08308427780866623 + 2.0 * 6.028372287750244
Epoch 950, val loss: 0.7342612147331238
Epoch 960, training loss: 12.131823539733887 = 0.07939132302999496 + 2.0 * 6.02621603012085
Epoch 960, val loss: 0.7369986176490784
Epoch 970, training loss: 12.130248069763184 = 0.07592883706092834 + 2.0 * 6.027159690856934
Epoch 970, val loss: 0.7398138642311096
Epoch 980, training loss: 12.131242752075195 = 0.0726957619190216 + 2.0 * 6.029273509979248
Epoch 980, val loss: 0.7425737977027893
Epoch 990, training loss: 12.122044563293457 = 0.06969496607780457 + 2.0 * 6.026175022125244
Epoch 990, val loss: 0.7456440925598145
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8148
Overall ASR: 0.5646
Flip ASR: 0.4889/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.707719802856445 = 1.9602025747299194 + 2.0 * 8.373758316040039
Epoch 0, val loss: 1.9590688943862915
Epoch 10, training loss: 18.69495391845703 = 1.949164867401123 + 2.0 * 8.372894287109375
Epoch 10, val loss: 1.9471629858016968
Epoch 20, training loss: 18.670244216918945 = 1.93556547164917 + 2.0 * 8.367339134216309
Epoch 20, val loss: 1.9321528673171997
Epoch 30, training loss: 18.58373260498047 = 1.9179521799087524 + 2.0 * 8.332890510559082
Epoch 30, val loss: 1.9128302335739136
Epoch 40, training loss: 18.093536376953125 = 1.8994500637054443 + 2.0 * 8.09704303741455
Epoch 40, val loss: 1.8934118747711182
Epoch 50, training loss: 16.464698791503906 = 1.8791910409927368 + 2.0 * 7.292754173278809
Epoch 50, val loss: 1.8720654249191284
Epoch 60, training loss: 15.881526947021484 = 1.8633733987808228 + 2.0 * 7.0090765953063965
Epoch 60, val loss: 1.8565895557403564
Epoch 70, training loss: 15.433594703674316 = 1.8501087427139282 + 2.0 * 6.79174280166626
Epoch 70, val loss: 1.8431646823883057
Epoch 80, training loss: 15.121891021728516 = 1.8377875089645386 + 2.0 * 6.642051696777344
Epoch 80, val loss: 1.8307331800460815
Epoch 90, training loss: 14.913028717041016 = 1.8271098136901855 + 2.0 * 6.542959690093994
Epoch 90, val loss: 1.8197250366210938
Epoch 100, training loss: 14.76432991027832 = 1.8161147832870483 + 2.0 * 6.47410774230957
Epoch 100, val loss: 1.8082109689712524
Epoch 110, training loss: 14.636996269226074 = 1.8053470849990845 + 2.0 * 6.4158244132995605
Epoch 110, val loss: 1.79684579372406
Epoch 120, training loss: 14.530261993408203 = 1.7956156730651855 + 2.0 * 6.367323398590088
Epoch 120, val loss: 1.7866545915603638
Epoch 130, training loss: 14.44955825805664 = 1.7867037057876587 + 2.0 * 6.331427097320557
Epoch 130, val loss: 1.7772738933563232
Epoch 140, training loss: 14.386207580566406 = 1.777799129486084 + 2.0 * 6.30420446395874
Epoch 140, val loss: 1.7682961225509644
Epoch 150, training loss: 14.331585884094238 = 1.7685097455978394 + 2.0 * 6.281538009643555
Epoch 150, val loss: 1.7591606378555298
Epoch 160, training loss: 14.285432815551758 = 1.758762240409851 + 2.0 * 6.263335227966309
Epoch 160, val loss: 1.7498148679733276
Epoch 170, training loss: 14.240811347961426 = 1.748194694519043 + 2.0 * 6.246308326721191
Epoch 170, val loss: 1.7403048276901245
Epoch 180, training loss: 14.201089859008789 = 1.736517071723938 + 2.0 * 6.23228645324707
Epoch 180, val loss: 1.7300068140029907
Epoch 190, training loss: 14.162741661071777 = 1.7233585119247437 + 2.0 * 6.219691753387451
Epoch 190, val loss: 1.718707799911499
Epoch 200, training loss: 14.12816333770752 = 1.7085132598876953 + 2.0 * 6.209825038909912
Epoch 200, val loss: 1.706389307975769
Epoch 210, training loss: 14.089808464050293 = 1.6918601989746094 + 2.0 * 6.198974132537842
Epoch 210, val loss: 1.6928627490997314
Epoch 220, training loss: 14.051071166992188 = 1.67286217212677 + 2.0 * 6.1891045570373535
Epoch 220, val loss: 1.677610993385315
Epoch 230, training loss: 14.011030197143555 = 1.6508125066757202 + 2.0 * 6.180109024047852
Epoch 230, val loss: 1.6601864099502563
Epoch 240, training loss: 13.981090545654297 = 1.6250196695327759 + 2.0 * 6.178035259246826
Epoch 240, val loss: 1.6400316953659058
Epoch 250, training loss: 13.928935050964355 = 1.5958019495010376 + 2.0 * 6.166566371917725
Epoch 250, val loss: 1.6169663667678833
Epoch 260, training loss: 13.880568504333496 = 1.5625022649765015 + 2.0 * 6.159033298492432
Epoch 260, val loss: 1.5907642841339111
Epoch 270, training loss: 13.831404685974121 = 1.5244948863983154 + 2.0 * 6.153454780578613
Epoch 270, val loss: 1.5608770847320557
Epoch 280, training loss: 13.778141021728516 = 1.481619954109192 + 2.0 * 6.148260593414307
Epoch 280, val loss: 1.527058720588684
Epoch 290, training loss: 13.72339153289795 = 1.4347654581069946 + 2.0 * 6.144312858581543
Epoch 290, val loss: 1.4903063774108887
Epoch 300, training loss: 13.666295051574707 = 1.3858593702316284 + 2.0 * 6.1402177810668945
Epoch 300, val loss: 1.452256202697754
Epoch 310, training loss: 13.608003616333008 = 1.335837483406067 + 2.0 * 6.136083126068115
Epoch 310, val loss: 1.4135684967041016
Epoch 320, training loss: 13.549079895019531 = 1.2858009338378906 + 2.0 * 6.13163948059082
Epoch 320, val loss: 1.375618577003479
Epoch 330, training loss: 13.495415687561035 = 1.2373615503311157 + 2.0 * 6.129026889801025
Epoch 330, val loss: 1.3390722274780273
Epoch 340, training loss: 13.439851760864258 = 1.1909337043762207 + 2.0 * 6.124459266662598
Epoch 340, val loss: 1.3045902252197266
Epoch 350, training loss: 13.387048721313477 = 1.1462725400924683 + 2.0 * 6.120388031005859
Epoch 350, val loss: 1.2718292474746704
Epoch 360, training loss: 13.349571228027344 = 1.1031553745269775 + 2.0 * 6.123208045959473
Epoch 360, val loss: 1.2405688762664795
Epoch 370, training loss: 13.29484748840332 = 1.062366008758545 + 2.0 * 6.116240501403809
Epoch 370, val loss: 1.2109994888305664
Epoch 380, training loss: 13.244037628173828 = 1.0235120058059692 + 2.0 * 6.110262870788574
Epoch 380, val loss: 1.1829174757003784
Epoch 390, training loss: 13.200797080993652 = 0.9860605001449585 + 2.0 * 6.107368469238281
Epoch 390, val loss: 1.1559803485870361
Epoch 400, training loss: 13.171012878417969 = 0.9501142501831055 + 2.0 * 6.110449314117432
Epoch 400, val loss: 1.1302282810211182
Epoch 410, training loss: 13.122127532958984 = 0.915961503982544 + 2.0 * 6.10308313369751
Epoch 410, val loss: 1.1060858964920044
Epoch 420, training loss: 13.079938888549805 = 0.8831073641777039 + 2.0 * 6.098415851593018
Epoch 420, val loss: 1.0828862190246582
Epoch 430, training loss: 13.04245376586914 = 0.8512300252914429 + 2.0 * 6.095612049102783
Epoch 430, val loss: 1.060409665107727
Epoch 440, training loss: 13.01882553100586 = 0.8200317621231079 + 2.0 * 6.099396705627441
Epoch 440, val loss: 1.038551926612854
Epoch 450, training loss: 12.973674774169922 = 0.7899842858314514 + 2.0 * 6.0918450355529785
Epoch 450, val loss: 1.0174176692962646
Epoch 460, training loss: 12.939462661743164 = 0.7608685493469238 + 2.0 * 6.089297294616699
Epoch 460, val loss: 0.9970517754554749
Epoch 470, training loss: 12.908228874206543 = 0.732514500617981 + 2.0 * 6.087857246398926
Epoch 470, val loss: 0.9771904945373535
Epoch 480, training loss: 12.874939918518066 = 0.7047545909881592 + 2.0 * 6.085092544555664
Epoch 480, val loss: 0.9579528570175171
Epoch 490, training loss: 12.844493865966797 = 0.6779526472091675 + 2.0 * 6.08327054977417
Epoch 490, val loss: 0.9391915202140808
Epoch 500, training loss: 12.81675910949707 = 0.65183025598526 + 2.0 * 6.082464218139648
Epoch 500, val loss: 0.9210065007209778
Epoch 510, training loss: 12.789055824279785 = 0.626688539981842 + 2.0 * 6.081183433532715
Epoch 510, val loss: 0.9035451412200928
Epoch 520, training loss: 12.756385803222656 = 0.6026811599731445 + 2.0 * 6.076852321624756
Epoch 520, val loss: 0.8868386745452881
Epoch 530, training loss: 12.731303215026855 = 0.5797508358955383 + 2.0 * 6.075776100158691
Epoch 530, val loss: 0.8711289167404175
Epoch 540, training loss: 12.714048385620117 = 0.558064341545105 + 2.0 * 6.077991962432861
Epoch 540, val loss: 0.8563805818557739
Epoch 550, training loss: 12.681300163269043 = 0.5376859903335571 + 2.0 * 6.071806907653809
Epoch 550, val loss: 0.842989444732666
Epoch 560, training loss: 12.661033630371094 = 0.5186991095542908 + 2.0 * 6.071167469024658
Epoch 560, val loss: 0.830761730670929
Epoch 570, training loss: 12.640800476074219 = 0.5009446740150452 + 2.0 * 6.06992769241333
Epoch 570, val loss: 0.8197241425514221
Epoch 580, training loss: 12.6196928024292 = 0.4845193028450012 + 2.0 * 6.067586898803711
Epoch 580, val loss: 0.8099784851074219
Epoch 590, training loss: 12.606773376464844 = 0.4691212475299835 + 2.0 * 6.068826198577881
Epoch 590, val loss: 0.801288366317749
Epoch 600, training loss: 12.584229469299316 = 0.4547092318534851 + 2.0 * 6.064760208129883
Epoch 600, val loss: 0.7937600612640381
Epoch 610, training loss: 12.566771507263184 = 0.4411071240901947 + 2.0 * 6.062832355499268
Epoch 610, val loss: 0.7871502041816711
Epoch 620, training loss: 12.554523468017578 = 0.4281390905380249 + 2.0 * 6.063192367553711
Epoch 620, val loss: 0.7812594175338745
Epoch 630, training loss: 12.535111427307129 = 0.41579195857048035 + 2.0 * 6.059659957885742
Epoch 630, val loss: 0.776108980178833
Epoch 640, training loss: 12.527436256408691 = 0.40380221605300903 + 2.0 * 6.061817169189453
Epoch 640, val loss: 0.7715258598327637
Epoch 650, training loss: 12.51223373413086 = 0.3921683728694916 + 2.0 * 6.060032844543457
Epoch 650, val loss: 0.7674593329429626
Epoch 660, training loss: 12.493243217468262 = 0.38079580664634705 + 2.0 * 6.0562238693237305
Epoch 660, val loss: 0.7638201117515564
Epoch 670, training loss: 12.477577209472656 = 0.3695926070213318 + 2.0 * 6.05399227142334
Epoch 670, val loss: 0.7606040835380554
Epoch 680, training loss: 12.464015007019043 = 0.3584049344062805 + 2.0 * 6.052804946899414
Epoch 680, val loss: 0.7576661109924316
Epoch 690, training loss: 12.460808753967285 = 0.3471809923648834 + 2.0 * 6.056813716888428
Epoch 690, val loss: 0.7549353837966919
Epoch 700, training loss: 12.444907188415527 = 0.33593857288360596 + 2.0 * 6.0544843673706055
Epoch 700, val loss: 0.7524213790893555
Epoch 710, training loss: 12.42819595336914 = 0.3247236907482147 + 2.0 * 6.051736354827881
Epoch 710, val loss: 0.7502309679985046
Epoch 720, training loss: 12.412947654724121 = 0.31346961855888367 + 2.0 * 6.049738883972168
Epoch 720, val loss: 0.7483081221580505
Epoch 730, training loss: 12.399751663208008 = 0.30221760272979736 + 2.0 * 6.04876708984375
Epoch 730, val loss: 0.7465491890907288
Epoch 740, training loss: 12.385469436645508 = 0.2909140884876251 + 2.0 * 6.047277450561523
Epoch 740, val loss: 0.7450596690177917
Epoch 750, training loss: 12.395034790039062 = 0.27965450286865234 + 2.0 * 6.057690143585205
Epoch 750, val loss: 0.7435624003410339
Epoch 760, training loss: 12.364242553710938 = 0.26854753494262695 + 2.0 * 6.047847270965576
Epoch 760, val loss: 0.7425305247306824
Epoch 770, training loss: 12.347084999084473 = 0.257601797580719 + 2.0 * 6.044741630554199
Epoch 770, val loss: 0.7418673634529114
Epoch 780, training loss: 12.335304260253906 = 0.24679262936115265 + 2.0 * 6.04425573348999
Epoch 780, val loss: 0.7413607835769653
Epoch 790, training loss: 12.322394371032715 = 0.236250102519989 + 2.0 * 6.04307222366333
Epoch 790, val loss: 0.7409541010856628
Epoch 800, training loss: 12.313671112060547 = 0.22609198093414307 + 2.0 * 6.043789386749268
Epoch 800, val loss: 0.7410886287689209
Epoch 810, training loss: 12.299278259277344 = 0.21632066369056702 + 2.0 * 6.041478633880615
Epoch 810, val loss: 0.7415543794631958
Epoch 820, training loss: 12.287192344665527 = 0.20691385865211487 + 2.0 * 6.040139198303223
Epoch 820, val loss: 0.7422730326652527
Epoch 830, training loss: 12.304283142089844 = 0.19793714582920074 + 2.0 * 6.053173065185547
Epoch 830, val loss: 0.7432918548583984
Epoch 840, training loss: 12.268260955810547 = 0.18942993879318237 + 2.0 * 6.03941535949707
Epoch 840, val loss: 0.7445158958435059
Epoch 850, training loss: 12.260364532470703 = 0.18144848942756653 + 2.0 * 6.03945779800415
Epoch 850, val loss: 0.7461449503898621
Epoch 860, training loss: 12.25855541229248 = 0.17387554049491882 + 2.0 * 6.04233980178833
Epoch 860, val loss: 0.7479326128959656
Epoch 870, training loss: 12.246233940124512 = 0.16672782599925995 + 2.0 * 6.039752960205078
Epoch 870, val loss: 0.7499210238456726
Epoch 880, training loss: 12.234729766845703 = 0.15999434888362885 + 2.0 * 6.037367820739746
Epoch 880, val loss: 0.7522400617599487
Epoch 890, training loss: 12.23934268951416 = 0.15361985564231873 + 2.0 * 6.042861461639404
Epoch 890, val loss: 0.7546677589416504
Epoch 900, training loss: 12.219659805297852 = 0.14764562249183655 + 2.0 * 6.036006927490234
Epoch 900, val loss: 0.7572070956230164
Epoch 910, training loss: 12.210189819335938 = 0.14197532832622528 + 2.0 * 6.034107208251953
Epoch 910, val loss: 0.7600506544113159
Epoch 920, training loss: 12.202971458435059 = 0.13658548891544342 + 2.0 * 6.033193111419678
Epoch 920, val loss: 0.7629861235618591
Epoch 930, training loss: 12.207125663757324 = 0.13145135343074799 + 2.0 * 6.037837028503418
Epoch 930, val loss: 0.7661083340644836
Epoch 940, training loss: 12.216076850891113 = 0.12667419016361237 + 2.0 * 6.044701099395752
Epoch 940, val loss: 0.7690462470054626
Epoch 950, training loss: 12.192732810974121 = 0.12217845022678375 + 2.0 * 6.035277366638184
Epoch 950, val loss: 0.7724418640136719
Epoch 960, training loss: 12.181294441223145 = 0.11791098117828369 + 2.0 * 6.031691551208496
Epoch 960, val loss: 0.7758444547653198
Epoch 970, training loss: 12.1739501953125 = 0.11381930112838745 + 2.0 * 6.030065536499023
Epoch 970, val loss: 0.7793217301368713
Epoch 980, training loss: 12.16779899597168 = 0.10990237444639206 + 2.0 * 6.0289483070373535
Epoch 980, val loss: 0.7829557657241821
Epoch 990, training loss: 12.171420097351074 = 0.10615535080432892 + 2.0 * 6.032632350921631
Epoch 990, val loss: 0.786626935005188
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8074
Overall ASR: 0.6273
Flip ASR: 0.5822/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.676044464111328 = 1.9285324811935425 + 2.0 * 8.373756408691406
Epoch 0, val loss: 1.9298624992370605
Epoch 10, training loss: 18.66452980041504 = 1.9189940690994263 + 2.0 * 8.372767448425293
Epoch 10, val loss: 1.9203163385391235
Epoch 20, training loss: 18.640594482421875 = 1.9070171117782593 + 2.0 * 8.366788864135742
Epoch 20, val loss: 1.9076381921768188
Epoch 30, training loss: 18.551631927490234 = 1.891104817390442 + 2.0 * 8.330263137817383
Epoch 30, val loss: 1.8903207778930664
Epoch 40, training loss: 17.957738876342773 = 1.8727182149887085 + 2.0 * 8.042510032653809
Epoch 40, val loss: 1.8703736066818237
Epoch 50, training loss: 16.221586227416992 = 1.852419376373291 + 2.0 * 7.18458366394043
Epoch 50, val loss: 1.8480664491653442
Epoch 60, training loss: 15.573376655578613 = 1.8389075994491577 + 2.0 * 6.867234706878662
Epoch 60, val loss: 1.8343194723129272
Epoch 70, training loss: 15.125303268432617 = 1.829078197479248 + 2.0 * 6.6481122970581055
Epoch 70, val loss: 1.8232707977294922
Epoch 80, training loss: 14.903223991394043 = 1.8196675777435303 + 2.0 * 6.541778087615967
Epoch 80, val loss: 1.8129184246063232
Epoch 90, training loss: 14.744606018066406 = 1.8097615242004395 + 2.0 * 6.4674224853515625
Epoch 90, val loss: 1.8011428117752075
Epoch 100, training loss: 14.61876392364502 = 1.800508737564087 + 2.0 * 6.409127712249756
Epoch 100, val loss: 1.7902811765670776
Epoch 110, training loss: 14.521389961242676 = 1.7921743392944336 + 2.0 * 6.364607810974121
Epoch 110, val loss: 1.7805191278457642
Epoch 120, training loss: 14.440614700317383 = 1.784388780593872 + 2.0 * 6.328113079071045
Epoch 120, val loss: 1.7715890407562256
Epoch 130, training loss: 14.373842239379883 = 1.776256799697876 + 2.0 * 6.298792839050293
Epoch 130, val loss: 1.7626512050628662
Epoch 140, training loss: 14.317117691040039 = 1.76754891872406 + 2.0 * 6.274784564971924
Epoch 140, val loss: 1.7533692121505737
Epoch 150, training loss: 14.268707275390625 = 1.758205771446228 + 2.0 * 6.255250930786133
Epoch 150, val loss: 1.7437630891799927
Epoch 160, training loss: 14.224237442016602 = 1.7480370998382568 + 2.0 * 6.238100051879883
Epoch 160, val loss: 1.733739972114563
Epoch 170, training loss: 14.184307098388672 = 1.7367558479309082 + 2.0 * 6.223775386810303
Epoch 170, val loss: 1.7230783700942993
Epoch 180, training loss: 14.144950866699219 = 1.7241969108581543 + 2.0 * 6.210376739501953
Epoch 180, val loss: 1.7117054462432861
Epoch 190, training loss: 14.1068696975708 = 1.7101157903671265 + 2.0 * 6.1983771324157715
Epoch 190, val loss: 1.6993629932403564
Epoch 200, training loss: 14.07275104522705 = 1.6940348148345947 + 2.0 * 6.189358234405518
Epoch 200, val loss: 1.6856083869934082
Epoch 210, training loss: 14.034745216369629 = 1.6754544973373413 + 2.0 * 6.179645538330078
Epoch 210, val loss: 1.670265555381775
Epoch 220, training loss: 13.995018005371094 = 1.6540908813476562 + 2.0 * 6.170463562011719
Epoch 220, val loss: 1.652872920036316
Epoch 230, training loss: 13.95445728302002 = 1.6291908025741577 + 2.0 * 6.162633419036865
Epoch 230, val loss: 1.6328678131103516
Epoch 240, training loss: 13.912007331848145 = 1.5999690294265747 + 2.0 * 6.15601921081543
Epoch 240, val loss: 1.6095296144485474
Epoch 250, training loss: 13.867511749267578 = 1.5657682418823242 + 2.0 * 6.150871753692627
Epoch 250, val loss: 1.5823490619659424
Epoch 260, training loss: 13.817285537719727 = 1.5264908075332642 + 2.0 * 6.145397186279297
Epoch 260, val loss: 1.5511447191238403
Epoch 270, training loss: 13.7625150680542 = 1.481476902961731 + 2.0 * 6.140519142150879
Epoch 270, val loss: 1.5155532360076904
Epoch 280, training loss: 13.704364776611328 = 1.430631399154663 + 2.0 * 6.136866569519043
Epoch 280, val loss: 1.4754976034164429
Epoch 290, training loss: 13.648113250732422 = 1.3750107288360596 + 2.0 * 6.136551380157471
Epoch 290, val loss: 1.4321839809417725
Epoch 300, training loss: 13.577309608459473 = 1.3167164325714111 + 2.0 * 6.13029670715332
Epoch 300, val loss: 1.386995553970337
Epoch 310, training loss: 13.508621215820312 = 1.2562600374221802 + 2.0 * 6.126180648803711
Epoch 310, val loss: 1.3406531810760498
Epoch 320, training loss: 13.441532135009766 = 1.1947283744812012 + 2.0 * 6.123401641845703
Epoch 320, val loss: 1.293765902519226
Epoch 330, training loss: 13.38217830657959 = 1.1339002847671509 + 2.0 * 6.124138832092285
Epoch 330, val loss: 1.248395323753357
Epoch 340, training loss: 13.310842514038086 = 1.0755306482315063 + 2.0 * 6.1176557540893555
Epoch 340, val loss: 1.2046555280685425
Epoch 350, training loss: 13.246918678283691 = 1.0190412998199463 + 2.0 * 6.113938808441162
Epoch 350, val loss: 1.16270112991333
Epoch 360, training loss: 13.194053649902344 = 0.9649806022644043 + 2.0 * 6.114536762237549
Epoch 360, val loss: 1.122664451599121
Epoch 370, training loss: 13.13119888305664 = 0.914421558380127 + 2.0 * 6.108388423919678
Epoch 370, val loss: 1.0856178998947144
Epoch 380, training loss: 13.07798957824707 = 0.8670029640197754 + 2.0 * 6.105493068695068
Epoch 380, val loss: 1.0510722398757935
Epoch 390, training loss: 13.027682304382324 = 0.8221510648727417 + 2.0 * 6.1027655601501465
Epoch 390, val loss: 1.0186738967895508
Epoch 400, training loss: 12.985872268676758 = 0.7801587581634521 + 2.0 * 6.102856636047363
Epoch 400, val loss: 0.9887374043464661
Epoch 410, training loss: 12.937576293945312 = 0.7412230372428894 + 2.0 * 6.0981764793396
Epoch 410, val loss: 0.9614356160163879
Epoch 420, training loss: 12.900459289550781 = 0.7049394249916077 + 2.0 * 6.09775972366333
Epoch 420, val loss: 0.9365232586860657
Epoch 430, training loss: 12.859771728515625 = 0.671145498752594 + 2.0 * 6.094313144683838
Epoch 430, val loss: 0.9140262007713318
Epoch 440, training loss: 12.828213691711426 = 0.639762282371521 + 2.0 * 6.094225883483887
Epoch 440, val loss: 0.8935005068778992
Epoch 450, training loss: 12.788649559020996 = 0.6105398535728455 + 2.0 * 6.089055061340332
Epoch 450, val loss: 0.8749049305915833
Epoch 460, training loss: 12.756820678710938 = 0.5832386612892151 + 2.0 * 6.086791038513184
Epoch 460, val loss: 0.8580238223075867
Epoch 470, training loss: 12.732197761535645 = 0.5577055811882019 + 2.0 * 6.087245941162109
Epoch 470, val loss: 0.842691957950592
Epoch 480, training loss: 12.714080810546875 = 0.534083902835846 + 2.0 * 6.089998245239258
Epoch 480, val loss: 0.8289175629615784
Epoch 490, training loss: 12.674247741699219 = 0.5122982263565063 + 2.0 * 6.080974578857422
Epoch 490, val loss: 0.8166845440864563
Epoch 500, training loss: 12.647590637207031 = 0.4919130206108093 + 2.0 * 6.077838897705078
Epoch 500, val loss: 0.8057021498680115
Epoch 510, training loss: 12.630804061889648 = 0.472660094499588 + 2.0 * 6.079071998596191
Epoch 510, val loss: 0.7958395481109619
Epoch 520, training loss: 12.606781959533691 = 0.45472365617752075 + 2.0 * 6.076029300689697
Epoch 520, val loss: 0.7871984839439392
Epoch 530, training loss: 12.58230209350586 = 0.43792203068733215 + 2.0 * 6.072189807891846
Epoch 530, val loss: 0.7796823382377625
Epoch 540, training loss: 12.563006401062012 = 0.4219411015510559 + 2.0 * 6.07053279876709
Epoch 540, val loss: 0.7730435132980347
Epoch 550, training loss: 12.576766967773438 = 0.40663111209869385 + 2.0 * 6.0850677490234375
Epoch 550, val loss: 0.7672010064125061
Epoch 560, training loss: 12.535286903381348 = 0.39226558804512024 + 2.0 * 6.0715107917785645
Epoch 560, val loss: 0.762151300907135
Epoch 570, training loss: 12.512395858764648 = 0.3785041570663452 + 2.0 * 6.066946029663086
Epoch 570, val loss: 0.7577774524688721
Epoch 580, training loss: 12.49357795715332 = 0.3651069104671478 + 2.0 * 6.064235687255859
Epoch 580, val loss: 0.7538808584213257
Epoch 590, training loss: 12.489779472351074 = 0.3520054519176483 + 2.0 * 6.068887233734131
Epoch 590, val loss: 0.7503806948661804
Epoch 600, training loss: 12.466011047363281 = 0.339264452457428 + 2.0 * 6.06337308883667
Epoch 600, val loss: 0.747356116771698
Epoch 610, training loss: 12.448821067810059 = 0.3268912136554718 + 2.0 * 6.060965061187744
Epoch 610, val loss: 0.7447258234024048
Epoch 620, training loss: 12.438786506652832 = 0.3147297501564026 + 2.0 * 6.062028408050537
Epoch 620, val loss: 0.7424378991127014
Epoch 630, training loss: 12.41994571685791 = 0.30277225375175476 + 2.0 * 6.058586597442627
Epoch 630, val loss: 0.7404496073722839
Epoch 640, training loss: 12.40356731414795 = 0.29098811745643616 + 2.0 * 6.0562896728515625
Epoch 640, val loss: 0.7388253808021545
Epoch 650, training loss: 12.402482032775879 = 0.27937260270118713 + 2.0 * 6.061554908752441
Epoch 650, val loss: 0.7374659180641174
Epoch 660, training loss: 12.37864875793457 = 0.2680853009223938 + 2.0 * 6.055281639099121
Epoch 660, val loss: 0.7364158034324646
Epoch 670, training loss: 12.364007949829102 = 0.2570883631706238 + 2.0 * 6.053459644317627
Epoch 670, val loss: 0.7357433438301086
Epoch 680, training loss: 12.352641105651855 = 0.2463257908821106 + 2.0 * 6.053157806396484
Epoch 680, val loss: 0.7353571653366089
Epoch 690, training loss: 12.346206665039062 = 0.23588405549526215 + 2.0 * 6.055161476135254
Epoch 690, val loss: 0.7352288961410522
Epoch 700, training loss: 12.327461242675781 = 0.22580139338970184 + 2.0 * 6.050829887390137
Epoch 700, val loss: 0.7354745864868164
Epoch 710, training loss: 12.325855255126953 = 0.21607030928134918 + 2.0 * 6.054892539978027
Epoch 710, val loss: 0.7360190153121948
Epoch 720, training loss: 12.31290340423584 = 0.20677360892295837 + 2.0 * 6.053064823150635
Epoch 720, val loss: 0.7367044687271118
Epoch 730, training loss: 12.293804168701172 = 0.1979110985994339 + 2.0 * 6.047946453094482
Epoch 730, val loss: 0.7377936244010925
Epoch 740, training loss: 12.282623291015625 = 0.1893717348575592 + 2.0 * 6.04662561416626
Epoch 740, val loss: 0.7391416430473328
Epoch 750, training loss: 12.286624908447266 = 0.18120282888412476 + 2.0 * 6.052711009979248
Epoch 750, val loss: 0.7406154870986938
Epoch 760, training loss: 12.26512336730957 = 0.17347519099712372 + 2.0 * 6.04582405090332
Epoch 760, val loss: 0.7423108816146851
Epoch 770, training loss: 12.254801750183105 = 0.16610291600227356 + 2.0 * 6.044349193572998
Epoch 770, val loss: 0.7443620562553406
Epoch 780, training loss: 12.255793571472168 = 0.15906208753585815 + 2.0 * 6.048365592956543
Epoch 780, val loss: 0.7465651631355286
Epoch 790, training loss: 12.23990249633789 = 0.15239936113357544 + 2.0 * 6.0437517166137695
Epoch 790, val loss: 0.7488994002342224
Epoch 800, training loss: 12.229666709899902 = 0.14603261649608612 + 2.0 * 6.0418171882629395
Epoch 800, val loss: 0.7515162825584412
Epoch 810, training loss: 12.231585502624512 = 0.13998273015022278 + 2.0 * 6.045801162719727
Epoch 810, val loss: 0.7542479634284973
Epoch 820, training loss: 12.215883255004883 = 0.13424468040466309 + 2.0 * 6.04081916809082
Epoch 820, val loss: 0.7572124600410461
Epoch 830, training loss: 12.206880569458008 = 0.12880979478359222 + 2.0 * 6.039035320281982
Epoch 830, val loss: 0.7603397965431213
Epoch 840, training loss: 12.21326732635498 = 0.12363798171281815 + 2.0 * 6.044814586639404
Epoch 840, val loss: 0.7634596824645996
Epoch 850, training loss: 12.196392059326172 = 0.11877907812595367 + 2.0 * 6.038806438446045
Epoch 850, val loss: 0.7667577862739563
Epoch 860, training loss: 12.18857479095459 = 0.11416952311992645 + 2.0 * 6.037202835083008
Epoch 860, val loss: 0.770279049873352
Epoch 870, training loss: 12.187668800354004 = 0.10977436602115631 + 2.0 * 6.038947105407715
Epoch 870, val loss: 0.7738183736801147
Epoch 880, training loss: 12.175341606140137 = 0.10559217631816864 + 2.0 * 6.03487491607666
Epoch 880, val loss: 0.7773939371109009
Epoch 890, training loss: 12.180196762084961 = 0.1016177386045456 + 2.0 * 6.039289474487305
Epoch 890, val loss: 0.7811660766601562
Epoch 900, training loss: 12.17203426361084 = 0.09787733107805252 + 2.0 * 6.037078380584717
Epoch 900, val loss: 0.7849186658859253
Epoch 910, training loss: 12.162198066711426 = 0.09432844072580338 + 2.0 * 6.033934593200684
Epoch 910, val loss: 0.7888162732124329
Epoch 920, training loss: 12.155115127563477 = 0.09094120562076569 + 2.0 * 6.0320868492126465
Epoch 920, val loss: 0.7928094863891602
Epoch 930, training loss: 12.150388717651367 = 0.0876941978931427 + 2.0 * 6.031347274780273
Epoch 930, val loss: 0.7968177795410156
Epoch 940, training loss: 12.159811973571777 = 0.08458004891872406 + 2.0 * 6.037615776062012
Epoch 940, val loss: 0.8008831739425659
Epoch 950, training loss: 12.149123191833496 = 0.08163752406835556 + 2.0 * 6.033742904663086
Epoch 950, val loss: 0.8048139810562134
Epoch 960, training loss: 12.150697708129883 = 0.07884261757135391 + 2.0 * 6.035927772521973
Epoch 960, val loss: 0.8089499473571777
Epoch 970, training loss: 12.13945198059082 = 0.07617216557264328 + 2.0 * 6.03164005279541
Epoch 970, val loss: 0.8129310011863708
Epoch 980, training loss: 12.131677627563477 = 0.07364077866077423 + 2.0 * 6.029018402099609
Epoch 980, val loss: 0.8170925378799438
Epoch 990, training loss: 12.127114295959473 = 0.07120216637849808 + 2.0 * 6.027956008911133
Epoch 990, val loss: 0.8211597800254822
=== picking the best model according to the performance on validation ===
target class rate on Vs: 0.8000
accuracy on clean test nodes: 0.8148
Overall ASR: 0.8007
Flip ASR: 0.7644/225 nodes
The final ASR:0.66421, 0.09988, Accuracy:0.81235, 0.00349
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11656])
remove edge: torch.Size([2, 9488])
updated graph: torch.Size([2, 10588])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9889
Flip ASR: 0.9867/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9889
Flip ASR: 0.9867/225 nodes
The final ASR:0.98524, 0.00522, Accuracy:0.82716, 0.00462
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.4 der2:0.4 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.4, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 18.683622360229492 = 1.936116337776184 + 2.0 * 8.37375259399414
Epoch 0, val loss: 1.93675696849823
Epoch 10, training loss: 18.67207908630371 = 1.9265105724334717 + 2.0 * 8.372784614562988
Epoch 10, val loss: 1.9276529550552368
Epoch 20, training loss: 18.648725509643555 = 1.9142353534698486 + 2.0 * 8.367244720458984
Epoch 20, val loss: 1.9156630039215088
Epoch 30, training loss: 18.552331924438477 = 1.8978759050369263 + 2.0 * 8.327227592468262
Epoch 30, val loss: 1.8995490074157715
Epoch 40, training loss: 17.806461334228516 = 1.8797779083251953 + 2.0 * 7.963342189788818
Epoch 40, val loss: 1.8822447061538696
Epoch 50, training loss: 15.932238578796387 = 1.8627482652664185 + 2.0 * 7.034745216369629
Epoch 50, val loss: 1.8664593696594238
Epoch 60, training loss: 15.326037406921387 = 1.8513575792312622 + 2.0 * 6.737339973449707
Epoch 60, val loss: 1.8557952642440796
Epoch 70, training loss: 15.046599388122559 = 1.8415782451629639 + 2.0 * 6.602510452270508
Epoch 70, val loss: 1.8462692499160767
Epoch 80, training loss: 14.815197944641113 = 1.8318203687667847 + 2.0 * 6.4916887283325195
Epoch 80, val loss: 1.8371317386627197
Epoch 90, training loss: 14.648780822753906 = 1.8232123851776123 + 2.0 * 6.412784099578857
Epoch 90, val loss: 1.82904851436615
Epoch 100, training loss: 14.535916328430176 = 1.8159421682357788 + 2.0 * 6.359987258911133
Epoch 100, val loss: 1.8218803405761719
Epoch 110, training loss: 14.449812889099121 = 1.8091095685958862 + 2.0 * 6.320351600646973
Epoch 110, val loss: 1.8149226903915405
Epoch 120, training loss: 14.380820274353027 = 1.8022520542144775 + 2.0 * 6.2892842292785645
Epoch 120, val loss: 1.8080110549926758
Epoch 130, training loss: 14.325570106506348 = 1.7954180240631104 + 2.0 * 6.265076160430908
Epoch 130, val loss: 1.8013066053390503
Epoch 140, training loss: 14.28007698059082 = 1.7884254455566406 + 2.0 * 6.24582576751709
Epoch 140, val loss: 1.7946597337722778
Epoch 150, training loss: 14.236706733703613 = 1.7810274362564087 + 2.0 * 6.227839469909668
Epoch 150, val loss: 1.7879090309143066
Epoch 160, training loss: 14.196969985961914 = 1.773058295249939 + 2.0 * 6.211956024169922
Epoch 160, val loss: 1.7808992862701416
Epoch 170, training loss: 14.164363861083984 = 1.7642661333084106 + 2.0 * 6.200048923492432
Epoch 170, val loss: 1.773419976234436
Epoch 180, training loss: 14.128215789794922 = 1.754411220550537 + 2.0 * 6.1869025230407715
Epoch 180, val loss: 1.7652579545974731
Epoch 190, training loss: 14.091902732849121 = 1.7432459592819214 + 2.0 * 6.174328327178955
Epoch 190, val loss: 1.75624680519104
Epoch 200, training loss: 14.05849552154541 = 1.7304556369781494 + 2.0 * 6.16402006149292
Epoch 200, val loss: 1.7460362911224365
Epoch 210, training loss: 14.025581359863281 = 1.7155197858810425 + 2.0 * 6.155030727386475
Epoch 210, val loss: 1.734263300895691
Epoch 220, training loss: 13.994564056396484 = 1.6979855298995972 + 2.0 * 6.148289203643799
Epoch 220, val loss: 1.720553994178772
Epoch 230, training loss: 13.959015846252441 = 1.6773744821548462 + 2.0 * 6.140820503234863
Epoch 230, val loss: 1.7044339179992676
Epoch 240, training loss: 13.920598983764648 = 1.6532658338546753 + 2.0 * 6.133666515350342
Epoch 240, val loss: 1.6853729486465454
Epoch 250, training loss: 13.878684997558594 = 1.6245942115783691 + 2.0 * 6.127045631408691
Epoch 250, val loss: 1.6627644300460815
Epoch 260, training loss: 13.833980560302734 = 1.5905156135559082 + 2.0 * 6.121732711791992
Epoch 260, val loss: 1.6357488632202148
Epoch 270, training loss: 13.78799057006836 = 1.5498864650726318 + 2.0 * 6.119051933288574
Epoch 270, val loss: 1.6032190322875977
Epoch 280, training loss: 13.728658676147461 = 1.5030577182769775 + 2.0 * 6.112800598144531
Epoch 280, val loss: 1.5656449794769287
Epoch 290, training loss: 13.673481941223145 = 1.4499179124832153 + 2.0 * 6.111782073974609
Epoch 290, val loss: 1.522847294807434
Epoch 300, training loss: 13.605271339416504 = 1.3917720317840576 + 2.0 * 6.106749534606934
Epoch 300, val loss: 1.4759337902069092
Epoch 310, training loss: 13.534777641296387 = 1.3292878866195679 + 2.0 * 6.102745056152344
Epoch 310, val loss: 1.425599217414856
Epoch 320, training loss: 13.46615219116211 = 1.2642534971237183 + 2.0 * 6.100949287414551
Epoch 320, val loss: 1.3734017610549927
Epoch 330, training loss: 13.399771690368652 = 1.199706792831421 + 2.0 * 6.100032329559326
Epoch 330, val loss: 1.321807861328125
Epoch 340, training loss: 13.328666687011719 = 1.1377378702163696 + 2.0 * 6.09546422958374
Epoch 340, val loss: 1.2725049257278442
Epoch 350, training loss: 13.263452529907227 = 1.0787829160690308 + 2.0 * 6.092334747314453
Epoch 350, val loss: 1.2258234024047852
Epoch 360, training loss: 13.205087661743164 = 1.0236921310424805 + 2.0 * 6.090697765350342
Epoch 360, val loss: 1.1825461387634277
Epoch 370, training loss: 13.149751663208008 = 0.9732167720794678 + 2.0 * 6.0882673263549805
Epoch 370, val loss: 1.1431307792663574
Epoch 380, training loss: 13.094961166381836 = 0.9269640445709229 + 2.0 * 6.083998680114746
Epoch 380, val loss: 1.107096791267395
Epoch 390, training loss: 13.047934532165527 = 0.8841356039047241 + 2.0 * 6.081899642944336
Epoch 390, val loss: 1.073994517326355
Epoch 400, training loss: 13.007747650146484 = 0.8446757793426514 + 2.0 * 6.081535816192627
Epoch 400, val loss: 1.0437533855438232
Epoch 410, training loss: 12.964103698730469 = 0.8086757063865662 + 2.0 * 6.077713966369629
Epoch 410, val loss: 1.0163236856460571
Epoch 420, training loss: 12.923738479614258 = 0.7751966118812561 + 2.0 * 6.074270725250244
Epoch 420, val loss: 0.9911438822746277
Epoch 430, training loss: 12.889250755310059 = 0.7437809705734253 + 2.0 * 6.072734832763672
Epoch 430, val loss: 0.9678860306739807
Epoch 440, training loss: 12.856282234191895 = 0.7145121097564697 + 2.0 * 6.070885181427002
Epoch 440, val loss: 0.9464630484580994
Epoch 450, training loss: 12.822894096374512 = 0.6871488094329834 + 2.0 * 6.067872524261475
Epoch 450, val loss: 0.9270768761634827
Epoch 460, training loss: 12.793874740600586 = 0.6611984372138977 + 2.0 * 6.066338062286377
Epoch 460, val loss: 0.909068763256073
Epoch 470, training loss: 12.777061462402344 = 0.6364785432815552 + 2.0 * 6.070291519165039
Epoch 470, val loss: 0.8924188613891602
Epoch 480, training loss: 12.736709594726562 = 0.613025426864624 + 2.0 * 6.06184196472168
Epoch 480, val loss: 0.8772961497306824
Epoch 490, training loss: 12.714977264404297 = 0.5906313061714172 + 2.0 * 6.062172889709473
Epoch 490, val loss: 0.8635044693946838
Epoch 500, training loss: 12.691094398498535 = 0.5692535042762756 + 2.0 * 6.060920238494873
Epoch 500, val loss: 0.8509508371353149
Epoch 510, training loss: 12.664814949035645 = 0.5488094091415405 + 2.0 * 6.058002948760986
Epoch 510, val loss: 0.8395925760269165
Epoch 520, training loss: 12.646081924438477 = 0.5291069149971008 + 2.0 * 6.058487415313721
Epoch 520, val loss: 0.8292500972747803
Epoch 530, training loss: 12.619482040405273 = 0.5101736187934875 + 2.0 * 6.054654121398926
Epoch 530, val loss: 0.819993793964386
Epoch 540, training loss: 12.598687171936035 = 0.4919196665287018 + 2.0 * 6.053383827209473
Epoch 540, val loss: 0.8116375803947449
Epoch 550, training loss: 12.581305503845215 = 0.47426894307136536 + 2.0 * 6.053518295288086
Epoch 550, val loss: 0.8041658401489258
Epoch 560, training loss: 12.561471939086914 = 0.45743247866630554 + 2.0 * 6.0520195960998535
Epoch 560, val loss: 0.7976548075675964
Epoch 570, training loss: 12.544239044189453 = 0.4411776661872864 + 2.0 * 6.051530838012695
Epoch 570, val loss: 0.7919383645057678
Epoch 580, training loss: 12.52051067352295 = 0.4254997670650482 + 2.0 * 6.0475053787231445
Epoch 580, val loss: 0.7870116233825684
Epoch 590, training loss: 12.502388000488281 = 0.41041019558906555 + 2.0 * 6.045989036560059
Epoch 590, val loss: 0.7827996015548706
Epoch 600, training loss: 12.488593101501465 = 0.3957802355289459 + 2.0 * 6.046406269073486
Epoch 600, val loss: 0.7792373895645142
Epoch 610, training loss: 12.481636047363281 = 0.38167113065719604 + 2.0 * 6.04998254776001
Epoch 610, val loss: 0.7762373089790344
Epoch 620, training loss: 12.456487655639648 = 0.36815688014030457 + 2.0 * 6.04416561126709
Epoch 620, val loss: 0.7739077806472778
Epoch 630, training loss: 12.439597129821777 = 0.3550393283367157 + 2.0 * 6.04227876663208
Epoch 630, val loss: 0.7721186876296997
Epoch 640, training loss: 12.428448677062988 = 0.34230461716651917 + 2.0 * 6.04307222366333
Epoch 640, val loss: 0.7707437872886658
Epoch 650, training loss: 12.414742469787598 = 0.3299303948879242 + 2.0 * 6.04240608215332
Epoch 650, val loss: 0.7698333859443665
Epoch 660, training loss: 12.397202491760254 = 0.31788402795791626 + 2.0 * 6.039659023284912
Epoch 660, val loss: 0.7693878412246704
Epoch 670, training loss: 12.383275985717773 = 0.3060942590236664 + 2.0 * 6.038590908050537
Epoch 670, val loss: 0.7692568898200989
Epoch 680, training loss: 12.376355171203613 = 0.29452016949653625 + 2.0 * 6.04091739654541
Epoch 680, val loss: 0.7693819999694824
Epoch 690, training loss: 12.357172966003418 = 0.2832255959510803 + 2.0 * 6.036973476409912
Epoch 690, val loss: 0.7698526978492737
Epoch 700, training loss: 12.354133605957031 = 0.2720840871334076 + 2.0 * 6.041024684906006
Epoch 700, val loss: 0.7705326080322266
Epoch 710, training loss: 12.332741737365723 = 0.26115313172340393 + 2.0 * 6.035794258117676
Epoch 710, val loss: 0.7713961005210876
Epoch 720, training loss: 12.319658279418945 = 0.25042709708213806 + 2.0 * 6.034615516662598
Epoch 720, val loss: 0.7725234627723694
Epoch 730, training loss: 12.30766773223877 = 0.23989711701869965 + 2.0 * 6.033885478973389
Epoch 730, val loss: 0.7738222479820251
Epoch 740, training loss: 12.300424575805664 = 0.2296007126569748 + 2.0 * 6.035411834716797
Epoch 740, val loss: 0.7753947973251343
Epoch 750, training loss: 12.282205581665039 = 0.21953058242797852 + 2.0 * 6.031337261199951
Epoch 750, val loss: 0.7771478295326233
Epoch 760, training loss: 12.273157119750977 = 0.20970222353935242 + 2.0 * 6.031727313995361
Epoch 760, val loss: 0.7792503237724304
Epoch 770, training loss: 12.266542434692383 = 0.20015832781791687 + 2.0 * 6.033192157745361
Epoch 770, val loss: 0.7814804911613464
Epoch 780, training loss: 12.257672309875488 = 0.19095931947231293 + 2.0 * 6.033356666564941
Epoch 780, val loss: 0.7839491963386536
Epoch 790, training loss: 12.239340782165527 = 0.18211540579795837 + 2.0 * 6.0286126136779785
Epoch 790, val loss: 0.7867119312286377
Epoch 800, training loss: 12.229019165039062 = 0.17362698912620544 + 2.0 * 6.027696132659912
Epoch 800, val loss: 0.7896148562431335
Epoch 810, training loss: 12.223706245422363 = 0.16552069783210754 + 2.0 * 6.029092788696289
Epoch 810, val loss: 0.7926591634750366
Epoch 820, training loss: 12.210014343261719 = 0.1578187644481659 + 2.0 * 6.026097774505615
Epoch 820, val loss: 0.7960554957389832
Epoch 830, training loss: 12.202892303466797 = 0.15048900246620178 + 2.0 * 6.0262017250061035
Epoch 830, val loss: 0.7996836304664612
Epoch 840, training loss: 12.208888053894043 = 0.14354734122753143 + 2.0 * 6.032670497894287
Epoch 840, val loss: 0.8033742308616638
Epoch 850, training loss: 12.185626983642578 = 0.13706262409687042 + 2.0 * 6.024281978607178
Epoch 850, val loss: 0.807385265827179
Epoch 860, training loss: 12.178648948669434 = 0.13091805577278137 + 2.0 * 6.023865222930908
Epoch 860, val loss: 0.811694860458374
Epoch 870, training loss: 12.169577598571777 = 0.12508758902549744 + 2.0 * 6.022244930267334
Epoch 870, val loss: 0.8159984946250916
Epoch 880, training loss: 12.163520812988281 = 0.1195802092552185 + 2.0 * 6.021970272064209
Epoch 880, val loss: 0.8206040263175964
Epoch 890, training loss: 12.163775444030762 = 0.11438204348087311 + 2.0 * 6.0246968269348145
Epoch 890, val loss: 0.8253327012062073
Epoch 900, training loss: 12.160117149353027 = 0.10951525717973709 + 2.0 * 6.025300979614258
Epoch 900, val loss: 0.8302662968635559
Epoch 910, training loss: 12.14785099029541 = 0.10495439916849136 + 2.0 * 6.021448135375977
Epoch 910, val loss: 0.8353167772293091
Epoch 920, training loss: 12.139235496520996 = 0.10064013302326202 + 2.0 * 6.0192975997924805
Epoch 920, val loss: 0.8405506014823914
Epoch 930, training loss: 12.14348030090332 = 0.09656178206205368 + 2.0 * 6.023459434509277
Epoch 930, val loss: 0.8458508849143982
Epoch 940, training loss: 12.13393497467041 = 0.09274373203516006 + 2.0 * 6.020595550537109
Epoch 940, val loss: 0.8512686491012573
Epoch 950, training loss: 12.126313209533691 = 0.08912646770477295 + 2.0 * 6.0185933113098145
Epoch 950, val loss: 0.8568997979164124
Epoch 960, training loss: 12.128268241882324 = 0.085706427693367 + 2.0 * 6.021280765533447
Epoch 960, val loss: 0.8625162243843079
Epoch 970, training loss: 12.115546226501465 = 0.08249545842409134 + 2.0 * 6.0165252685546875
Epoch 970, val loss: 0.8682601451873779
Epoch 980, training loss: 12.111623764038086 = 0.07944688946008682 + 2.0 * 6.016088485717773
Epoch 980, val loss: 0.8741433620452881
Epoch 990, training loss: 12.1096830368042 = 0.07655870914459229 + 2.0 * 6.016561985015869
Epoch 990, val loss: 0.8800069689750671
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8037
Overall ASR: 0.4797
Flip ASR: 0.3867/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.678293228149414 = 1.9308714866638184 + 2.0 * 8.373710632324219
Epoch 0, val loss: 1.9276292324066162
Epoch 10, training loss: 18.66623878479004 = 1.9209617376327515 + 2.0 * 8.372638702392578
Epoch 10, val loss: 1.9174058437347412
Epoch 20, training loss: 18.64120101928711 = 1.9082896709442139 + 2.0 * 8.366456031799316
Epoch 20, val loss: 1.9042654037475586
Epoch 30, training loss: 18.550485610961914 = 1.8916429281234741 + 2.0 * 8.329421043395996
Epoch 30, val loss: 1.887114405632019
Epoch 40, training loss: 18.04355812072754 = 1.8736437559127808 + 2.0 * 8.084957122802734
Epoch 40, val loss: 1.8689076900482178
Epoch 50, training loss: 16.332799911499023 = 1.854433536529541 + 2.0 * 7.23918342590332
Epoch 50, val loss: 1.8494151830673218
Epoch 60, training loss: 15.610085487365723 = 1.8420521020889282 + 2.0 * 6.884016513824463
Epoch 60, val loss: 1.837749719619751
Epoch 70, training loss: 15.191006660461426 = 1.832579255104065 + 2.0 * 6.679213523864746
Epoch 70, val loss: 1.8281490802764893
Epoch 80, training loss: 14.993317604064941 = 1.822576642036438 + 2.0 * 6.5853705406188965
Epoch 80, val loss: 1.8177545070648193
Epoch 90, training loss: 14.832775115966797 = 1.8128116130828857 + 2.0 * 6.509981632232666
Epoch 90, val loss: 1.8076200485229492
Epoch 100, training loss: 14.693647384643555 = 1.805037021636963 + 2.0 * 6.444304943084717
Epoch 100, val loss: 1.7994598150253296
Epoch 110, training loss: 14.581997871398926 = 1.7987269163131714 + 2.0 * 6.391635417938232
Epoch 110, val loss: 1.792668342590332
Epoch 120, training loss: 14.497462272644043 = 1.7926219701766968 + 2.0 * 6.352420330047607
Epoch 120, val loss: 1.7860535383224487
Epoch 130, training loss: 14.430233001708984 = 1.7861019372940063 + 2.0 * 6.322065353393555
Epoch 130, val loss: 1.7793309688568115
Epoch 140, training loss: 14.373916625976562 = 1.7790932655334473 + 2.0 * 6.297411918640137
Epoch 140, val loss: 1.7724813222885132
Epoch 150, training loss: 14.323434829711914 = 1.7715094089508057 + 2.0 * 6.275962829589844
Epoch 150, val loss: 1.7654035091400146
Epoch 160, training loss: 14.276568412780762 = 1.7631880044937134 + 2.0 * 6.25669002532959
Epoch 160, val loss: 1.7579001188278198
Epoch 170, training loss: 14.234017372131348 = 1.753963828086853 + 2.0 * 6.240026950836182
Epoch 170, val loss: 1.7498689889907837
Epoch 180, training loss: 14.196165084838867 = 1.7434511184692383 + 2.0 * 6.2263569831848145
Epoch 180, val loss: 1.7409366369247437
Epoch 190, training loss: 14.157976150512695 = 1.7312541007995605 + 2.0 * 6.2133612632751465
Epoch 190, val loss: 1.7308486700057983
Epoch 200, training loss: 14.121191024780273 = 1.7171261310577393 + 2.0 * 6.202032566070557
Epoch 200, val loss: 1.7193615436553955
Epoch 210, training loss: 14.084405899047852 = 1.7006253004074097 + 2.0 * 6.191890239715576
Epoch 210, val loss: 1.706114649772644
Epoch 220, training loss: 14.046405792236328 = 1.6813478469848633 + 2.0 * 6.182528972625732
Epoch 220, val loss: 1.6907151937484741
Epoch 230, training loss: 14.007009506225586 = 1.658751368522644 + 2.0 * 6.174129009246826
Epoch 230, val loss: 1.6727830171585083
Epoch 240, training loss: 13.963281631469727 = 1.6322510242462158 + 2.0 * 6.165515422821045
Epoch 240, val loss: 1.6517589092254639
Epoch 250, training loss: 13.917095184326172 = 1.6010284423828125 + 2.0 * 6.15803337097168
Epoch 250, val loss: 1.626868486404419
Epoch 260, training loss: 13.871705055236816 = 1.5645558834075928 + 2.0 * 6.153574466705322
Epoch 260, val loss: 1.5978552103042603
Epoch 270, training loss: 13.814850807189941 = 1.523270845413208 + 2.0 * 6.145790100097656
Epoch 270, val loss: 1.5647399425506592
Epoch 280, training loss: 13.757135391235352 = 1.476553201675415 + 2.0 * 6.140291213989258
Epoch 280, val loss: 1.527151107788086
Epoch 290, training loss: 13.694183349609375 = 1.4241880178451538 + 2.0 * 6.134997844696045
Epoch 290, val loss: 1.4849011898040771
Epoch 300, training loss: 13.631877899169922 = 1.366817593574524 + 2.0 * 6.132530212402344
Epoch 300, val loss: 1.4385076761245728
Epoch 310, training loss: 13.560803413391113 = 1.3068078756332397 + 2.0 * 6.126997947692871
Epoch 310, val loss: 1.38987398147583
Epoch 320, training loss: 13.491083145141602 = 1.2451064586639404 + 2.0 * 6.122988224029541
Epoch 320, val loss: 1.3397849798202515
Epoch 330, training loss: 13.422372817993164 = 1.1830583810806274 + 2.0 * 6.119657039642334
Epoch 330, val loss: 1.2894151210784912
Epoch 340, training loss: 13.356746673583984 = 1.122110366821289 + 2.0 * 6.117318153381348
Epoch 340, val loss: 1.239925503730774
Epoch 350, training loss: 13.287976264953613 = 1.0629513263702393 + 2.0 * 6.112512588500977
Epoch 350, val loss: 1.1919763088226318
Epoch 360, training loss: 13.22753620147705 = 1.0057475566864014 + 2.0 * 6.110894203186035
Epoch 360, val loss: 1.1457237005233765
Epoch 370, training loss: 13.169438362121582 = 0.9516378045082092 + 2.0 * 6.10890007019043
Epoch 370, val loss: 1.1024506092071533
Epoch 380, training loss: 13.108349800109863 = 0.9008780121803284 + 2.0 * 6.10373592376709
Epoch 380, val loss: 1.0621243715286255
Epoch 390, training loss: 13.055442810058594 = 0.8528395891189575 + 2.0 * 6.101301670074463
Epoch 390, val loss: 1.024367332458496
Epoch 400, training loss: 13.006356239318848 = 0.8078567981719971 + 2.0 * 6.099249839782715
Epoch 400, val loss: 0.9894549250602722
Epoch 410, training loss: 12.957021713256836 = 0.7662534117698669 + 2.0 * 6.095384120941162
Epoch 410, val loss: 0.9578927755355835
Epoch 420, training loss: 12.913204193115234 = 0.7275339961051941 + 2.0 * 6.092834949493408
Epoch 420, val loss: 0.9291670918464661
Epoch 430, training loss: 12.879919052124023 = 0.6916941404342651 + 2.0 * 6.094112396240234
Epoch 430, val loss: 0.9033212661743164
Epoch 440, training loss: 12.841917991638184 = 0.6590732932090759 + 2.0 * 6.0914225578308105
Epoch 440, val loss: 0.8804565072059631
Epoch 450, training loss: 12.802164077758789 = 0.6293111443519592 + 2.0 * 6.086426258087158
Epoch 450, val loss: 0.8605482578277588
Epoch 460, training loss: 12.768142700195312 = 0.6019142866134644 + 2.0 * 6.083114147186279
Epoch 460, val loss: 0.8430401682853699
Epoch 470, training loss: 12.739376068115234 = 0.576449453830719 + 2.0 * 6.08146333694458
Epoch 470, val loss: 0.8275431990623474
Epoch 480, training loss: 12.712926864624023 = 0.5528439283370972 + 2.0 * 6.080041408538818
Epoch 480, val loss: 0.8140166401863098
Epoch 490, training loss: 12.686507225036621 = 0.5310113430023193 + 2.0 * 6.077747821807861
Epoch 490, val loss: 0.8022341132164001
Epoch 500, training loss: 12.660115242004395 = 0.5104674696922302 + 2.0 * 6.07482385635376
Epoch 500, val loss: 0.7917904257774353
Epoch 510, training loss: 12.63866901397705 = 0.49107858538627625 + 2.0 * 6.073795318603516
Epoch 510, val loss: 0.7826012969017029
Epoch 520, training loss: 12.614256858825684 = 0.4727787673473358 + 2.0 * 6.070739269256592
Epoch 520, val loss: 0.7744758725166321
Epoch 530, training loss: 12.595935821533203 = 0.4552992284297943 + 2.0 * 6.070318222045898
Epoch 530, val loss: 0.7672832608222961
Epoch 540, training loss: 12.576930046081543 = 0.43863269686698914 + 2.0 * 6.069148540496826
Epoch 540, val loss: 0.7609232664108276
Epoch 550, training loss: 12.554848670959473 = 0.42262738943099976 + 2.0 * 6.066110610961914
Epoch 550, val loss: 0.755242109298706
Epoch 560, training loss: 12.534565925598145 = 0.40712133049964905 + 2.0 * 6.063722133636475
Epoch 560, val loss: 0.7501637935638428
Epoch 570, training loss: 12.517799377441406 = 0.3920316696166992 + 2.0 * 6.0628838539123535
Epoch 570, val loss: 0.7456048130989075
Epoch 580, training loss: 12.497838973999023 = 0.377362459897995 + 2.0 * 6.060238361358643
Epoch 580, val loss: 0.7415529489517212
Epoch 590, training loss: 12.499321937561035 = 0.36299365758895874 + 2.0 * 6.068164348602295
Epoch 590, val loss: 0.7379489541053772
Epoch 600, training loss: 12.464829444885254 = 0.34904801845550537 + 2.0 * 6.057890892028809
Epoch 600, val loss: 0.7347360253334045
Epoch 610, training loss: 12.447410583496094 = 0.3354090452194214 + 2.0 * 6.056000709533691
Epoch 610, val loss: 0.7318813800811768
Epoch 620, training loss: 12.433751106262207 = 0.32195335626602173 + 2.0 * 6.055898666381836
Epoch 620, val loss: 0.7293163537979126
Epoch 630, training loss: 12.421608924865723 = 0.30880090594291687 + 2.0 * 6.056404113769531
Epoch 630, val loss: 0.7270767688751221
Epoch 640, training loss: 12.402156829833984 = 0.29595255851745605 + 2.0 * 6.053102016448975
Epoch 640, val loss: 0.7251171469688416
Epoch 650, training loss: 12.390420913696289 = 0.2833302617073059 + 2.0 * 6.0535454750061035
Epoch 650, val loss: 0.7234392166137695
Epoch 660, training loss: 12.374761581420898 = 0.27099740505218506 + 2.0 * 6.051882266998291
Epoch 660, val loss: 0.7220755219459534
Epoch 670, training loss: 12.358089447021484 = 0.25894618034362793 + 2.0 * 6.049571514129639
Epoch 670, val loss: 0.7208936214447021
Epoch 680, training loss: 12.34477710723877 = 0.24719668924808502 + 2.0 * 6.048789978027344
Epoch 680, val loss: 0.7200394868850708
Epoch 690, training loss: 12.336931228637695 = 0.23583652079105377 + 2.0 * 6.050547122955322
Epoch 690, val loss: 0.7194322347640991
Epoch 700, training loss: 12.32188892364502 = 0.22488169372081757 + 2.0 * 6.048503398895264
Epoch 700, val loss: 0.7190694212913513
Epoch 710, training loss: 12.305498123168945 = 0.21432186663150787 + 2.0 * 6.04558801651001
Epoch 710, val loss: 0.7189728617668152
Epoch 720, training loss: 12.300146102905273 = 0.2041466385126114 + 2.0 * 6.047999858856201
Epoch 720, val loss: 0.719176173210144
Epoch 730, training loss: 12.291172981262207 = 0.19449639320373535 + 2.0 * 6.048338413238525
Epoch 730, val loss: 0.7195712327957153
Epoch 740, training loss: 12.273019790649414 = 0.18525664508342743 + 2.0 * 6.043881416320801
Epoch 740, val loss: 0.7202072739601135
Epoch 750, training loss: 12.260329246520996 = 0.17644542455673218 + 2.0 * 6.041942119598389
Epoch 750, val loss: 0.7210735082626343
Epoch 760, training loss: 12.256418228149414 = 0.16801626980304718 + 2.0 * 6.044200897216797
Epoch 760, val loss: 0.7221640348434448
Epoch 770, training loss: 12.244475364685059 = 0.16002793610095978 + 2.0 * 6.042223930358887
Epoch 770, val loss: 0.7235501408576965
Epoch 780, training loss: 12.238258361816406 = 0.1524399071931839 + 2.0 * 6.042909145355225
Epoch 780, val loss: 0.7250796556472778
Epoch 790, training loss: 12.224201202392578 = 0.14527462422847748 + 2.0 * 6.039463520050049
Epoch 790, val loss: 0.7268168330192566
Epoch 800, training loss: 12.215367317199707 = 0.1384739726781845 + 2.0 * 6.03844690322876
Epoch 800, val loss: 0.7287842035293579
Epoch 810, training loss: 12.207621574401855 = 0.13199639320373535 + 2.0 * 6.03781270980835
Epoch 810, val loss: 0.7309362292289734
Epoch 820, training loss: 12.211554527282715 = 0.1258583515882492 + 2.0 * 6.042848110198975
Epoch 820, val loss: 0.7332282066345215
Epoch 830, training loss: 12.199244499206543 = 0.12010076642036438 + 2.0 * 6.039571762084961
Epoch 830, val loss: 0.7355917692184448
Epoch 840, training loss: 12.186968803405762 = 0.11464818567037582 + 2.0 * 6.036160469055176
Epoch 840, val loss: 0.7380497455596924
Epoch 850, training loss: 12.177995681762695 = 0.10946666449308395 + 2.0 * 6.03426456451416
Epoch 850, val loss: 0.7405971884727478
Epoch 860, training loss: 12.172056198120117 = 0.1045432984828949 + 2.0 * 6.033756256103516
Epoch 860, val loss: 0.7433117628097534
Epoch 870, training loss: 12.171260833740234 = 0.09984830021858215 + 2.0 * 6.035706043243408
Epoch 870, val loss: 0.7461621761322021
Epoch 880, training loss: 12.16947078704834 = 0.0954378992319107 + 2.0 * 6.03701639175415
Epoch 880, val loss: 0.7490358352661133
Epoch 890, training loss: 12.159769058227539 = 0.09124664962291718 + 2.0 * 6.034261226654053
Epoch 890, val loss: 0.7519013285636902
Epoch 900, training loss: 12.154374122619629 = 0.08728662133216858 + 2.0 * 6.033543586730957
Epoch 900, val loss: 0.754891574382782
Epoch 910, training loss: 12.144957542419434 = 0.08354248851537704 + 2.0 * 6.030707359313965
Epoch 910, val loss: 0.7578878998756409
Epoch 920, training loss: 12.138484954833984 = 0.07997070997953415 + 2.0 * 6.029257297515869
Epoch 920, val loss: 0.7609636187553406
Epoch 930, training loss: 12.136890411376953 = 0.07657485455274582 + 2.0 * 6.030157566070557
Epoch 930, val loss: 0.7641404271125793
Epoch 940, training loss: 12.130645751953125 = 0.07337815314531326 + 2.0 * 6.0286335945129395
Epoch 940, val loss: 0.7673375606536865
Epoch 950, training loss: 12.127429008483887 = 0.07033431529998779 + 2.0 * 6.028547286987305
Epoch 950, val loss: 0.7705585360527039
Epoch 960, training loss: 12.128700256347656 = 0.06746233999729156 + 2.0 * 6.030619144439697
Epoch 960, val loss: 0.7738742232322693
Epoch 970, training loss: 12.11882209777832 = 0.06472443044185638 + 2.0 * 6.0270490646362305
Epoch 970, val loss: 0.7771128416061401
Epoch 980, training loss: 12.11344051361084 = 0.06212977319955826 + 2.0 * 6.025655269622803
Epoch 980, val loss: 0.7804536819458008
Epoch 990, training loss: 12.117223739624023 = 0.05965987220406532 + 2.0 * 6.028781890869141
Epoch 990, val loss: 0.7838084101676941
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.8044
Flip ASR: 0.7644/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.696279525756836 = 1.9489325284957886 + 2.0 * 8.373673439025879
Epoch 0, val loss: 1.9511035680770874
Epoch 10, training loss: 18.67982292175293 = 1.9383013248443604 + 2.0 * 8.370760917663574
Epoch 10, val loss: 1.9396021366119385
Epoch 20, training loss: 18.654497146606445 = 1.9250338077545166 + 2.0 * 8.364731788635254
Epoch 20, val loss: 1.9240309000015259
Epoch 30, training loss: 18.575071334838867 = 1.907310128211975 + 2.0 * 8.333880424499512
Epoch 30, val loss: 1.90274178981781
Epoch 40, training loss: 18.097185134887695 = 1.887174367904663 + 2.0 * 8.105005264282227
Epoch 40, val loss: 1.8792850971221924
Epoch 50, training loss: 16.54340934753418 = 1.867780327796936 + 2.0 * 7.337814807891846
Epoch 50, val loss: 1.8576562404632568
Epoch 60, training loss: 15.7240629196167 = 1.8527624607086182 + 2.0 * 6.93565034866333
Epoch 60, val loss: 1.841339349746704
Epoch 70, training loss: 15.290473937988281 = 1.840380072593689 + 2.0 * 6.7250471115112305
Epoch 70, val loss: 1.8278318643569946
Epoch 80, training loss: 14.999366760253906 = 1.8295866250991821 + 2.0 * 6.584889888763428
Epoch 80, val loss: 1.8162871599197388
Epoch 90, training loss: 14.800519943237305 = 1.8197537660598755 + 2.0 * 6.490383148193359
Epoch 90, val loss: 1.805667757987976
Epoch 100, training loss: 14.655816078186035 = 1.8101884126663208 + 2.0 * 6.422813892364502
Epoch 100, val loss: 1.795249581336975
Epoch 110, training loss: 14.549230575561523 = 1.8007892370224 + 2.0 * 6.374220848083496
Epoch 110, val loss: 1.7849040031433105
Epoch 120, training loss: 14.457747459411621 = 1.791952133178711 + 2.0 * 6.332897663116455
Epoch 120, val loss: 1.7751599550247192
Epoch 130, training loss: 14.386922836303711 = 1.7839902639389038 + 2.0 * 6.301466464996338
Epoch 130, val loss: 1.766295313835144
Epoch 140, training loss: 14.322020530700684 = 1.7763969898223877 + 2.0 * 6.2728118896484375
Epoch 140, val loss: 1.758152723312378
Epoch 150, training loss: 14.268989562988281 = 1.768922209739685 + 2.0 * 6.250033855438232
Epoch 150, val loss: 1.750274419784546
Epoch 160, training loss: 14.22402286529541 = 1.7611796855926514 + 2.0 * 6.23142147064209
Epoch 160, val loss: 1.7425317764282227
Epoch 170, training loss: 14.181926727294922 = 1.7531447410583496 + 2.0 * 6.214390754699707
Epoch 170, val loss: 1.7348955869674683
Epoch 180, training loss: 14.142762184143066 = 1.7445567846298218 + 2.0 * 6.199102878570557
Epoch 180, val loss: 1.7272130250930786
Epoch 190, training loss: 14.116460800170898 = 1.7352216243743896 + 2.0 * 6.190619468688965
Epoch 190, val loss: 1.7191812992095947
Epoch 200, training loss: 14.07519817352295 = 1.724826455116272 + 2.0 * 6.175185680389404
Epoch 200, val loss: 1.7106664180755615
Epoch 210, training loss: 14.041110038757324 = 1.7132447957992554 + 2.0 * 6.163932800292969
Epoch 210, val loss: 1.7014257907867432
Epoch 220, training loss: 14.00838565826416 = 1.7001110315322876 + 2.0 * 6.154137134552002
Epoch 220, val loss: 1.6910511255264282
Epoch 230, training loss: 13.975278854370117 = 1.6844966411590576 + 2.0 * 6.14539098739624
Epoch 230, val loss: 1.6788537502288818
Epoch 240, training loss: 13.942639350891113 = 1.6661126613616943 + 2.0 * 6.13826322555542
Epoch 240, val loss: 1.664584994316101
Epoch 250, training loss: 13.907419204711914 = 1.6447159051895142 + 2.0 * 6.131351470947266
Epoch 250, val loss: 1.6480658054351807
Epoch 260, training loss: 13.86886978149414 = 1.6198331117630005 + 2.0 * 6.124518394470215
Epoch 260, val loss: 1.629069209098816
Epoch 270, training loss: 13.830565452575684 = 1.5908515453338623 + 2.0 * 6.119856834411621
Epoch 270, val loss: 1.606960654258728
Epoch 280, training loss: 13.784723281860352 = 1.5574384927749634 + 2.0 * 6.11364221572876
Epoch 280, val loss: 1.5812230110168457
Epoch 290, training loss: 13.739326477050781 = 1.5190799236297607 + 2.0 * 6.110123157501221
Epoch 290, val loss: 1.5518466234207153
Epoch 300, training loss: 13.686241149902344 = 1.475939154624939 + 2.0 * 6.105151176452637
Epoch 300, val loss: 1.5191760063171387
Epoch 310, training loss: 13.636815071105957 = 1.4286203384399414 + 2.0 * 6.104097366333008
Epoch 310, val loss: 1.4835243225097656
Epoch 320, training loss: 13.579204559326172 = 1.379441738128662 + 2.0 * 6.099881172180176
Epoch 320, val loss: 1.4467121362686157
Epoch 330, training loss: 13.522953033447266 = 1.3293375968933105 + 2.0 * 6.096807956695557
Epoch 330, val loss: 1.4099394083023071
Epoch 340, training loss: 13.465373992919922 = 1.278838872909546 + 2.0 * 6.093267440795898
Epoch 340, val loss: 1.3733344078063965
Epoch 350, training loss: 13.410235404968262 = 1.2284132242202759 + 2.0 * 6.090910911560059
Epoch 350, val loss: 1.3375670909881592
Epoch 360, training loss: 13.357179641723633 = 1.1791235208511353 + 2.0 * 6.0890278816223145
Epoch 360, val loss: 1.3036259412765503
Epoch 370, training loss: 13.301900863647461 = 1.1316351890563965 + 2.0 * 6.085132598876953
Epoch 370, val loss: 1.2711544036865234
Epoch 380, training loss: 13.250970840454102 = 1.0850815773010254 + 2.0 * 6.082944393157959
Epoch 380, val loss: 1.239619255065918
Epoch 390, training loss: 13.199820518493652 = 1.039056658744812 + 2.0 * 6.080381870269775
Epoch 390, val loss: 1.2084084749221802
Epoch 400, training loss: 13.15572738647461 = 0.9937901496887207 + 2.0 * 6.080968379974365
Epoch 400, val loss: 1.177501916885376
Epoch 410, training loss: 13.105677604675293 = 0.9498196244239807 + 2.0 * 6.0779290199279785
Epoch 410, val loss: 1.1471953392028809
Epoch 420, training loss: 13.056306838989258 = 0.9067720174789429 + 2.0 * 6.074767589569092
Epoch 420, val loss: 1.1170254945755005
Epoch 430, training loss: 13.012718200683594 = 0.8645024299621582 + 2.0 * 6.074107646942139
Epoch 430, val loss: 1.0869498252868652
Epoch 440, training loss: 12.968936920166016 = 0.8233844041824341 + 2.0 * 6.0727763175964355
Epoch 440, val loss: 1.057275414466858
Epoch 450, training loss: 12.927652359008789 = 0.7839691042900085 + 2.0 * 6.071841716766357
Epoch 450, val loss: 1.028541922569275
Epoch 460, training loss: 12.882831573486328 = 0.7466935515403748 + 2.0 * 6.068068981170654
Epoch 460, val loss: 1.0010144710540771
Epoch 470, training loss: 12.841777801513672 = 0.7108979225158691 + 2.0 * 6.065439701080322
Epoch 470, val loss: 0.9745622277259827
Epoch 480, training loss: 12.803780555725098 = 0.6765772700309753 + 2.0 * 6.063601493835449
Epoch 480, val loss: 0.949096143245697
Epoch 490, training loss: 12.782516479492188 = 0.6437291502952576 + 2.0 * 6.069393634796143
Epoch 490, val loss: 0.9247093200683594
Epoch 500, training loss: 12.73626708984375 = 0.6123344898223877 + 2.0 * 6.061966419219971
Epoch 500, val loss: 0.9019867181777954
Epoch 510, training loss: 12.701571464538574 = 0.582569420337677 + 2.0 * 6.0595011711120605
Epoch 510, val loss: 0.8806183934211731
Epoch 520, training loss: 12.672405242919922 = 0.5540449023246765 + 2.0 * 6.05918025970459
Epoch 520, val loss: 0.8605207800865173
Epoch 530, training loss: 12.648177146911621 = 0.5268155336380005 + 2.0 * 6.060680866241455
Epoch 530, val loss: 0.8419579267501831
Epoch 540, training loss: 12.615285873413086 = 0.5010156631469727 + 2.0 * 6.057135105133057
Epoch 540, val loss: 0.8248065114021301
Epoch 550, training loss: 12.583916664123535 = 0.4761841595172882 + 2.0 * 6.053866386413574
Epoch 550, val loss: 0.8091469407081604
Epoch 560, training loss: 12.563029289245605 = 0.4523135721683502 + 2.0 * 6.055357933044434
Epoch 560, val loss: 0.7947691679000854
Epoch 570, training loss: 12.543635368347168 = 0.4295187294483185 + 2.0 * 6.057058334350586
Epoch 570, val loss: 0.7816783785820007
Epoch 580, training loss: 12.506400108337402 = 0.40783965587615967 + 2.0 * 6.049280166625977
Epoch 580, val loss: 0.7698080539703369
Epoch 590, training loss: 12.484953880310059 = 0.3870297372341156 + 2.0 * 6.048962116241455
Epoch 590, val loss: 0.7591350674629211
Epoch 600, training loss: 12.461601257324219 = 0.36703768372535706 + 2.0 * 6.047281742095947
Epoch 600, val loss: 0.7495535612106323
Epoch 610, training loss: 12.446306228637695 = 0.34799724817276 + 2.0 * 6.049154281616211
Epoch 610, val loss: 0.7410522699356079
Epoch 620, training loss: 12.421722412109375 = 0.33008190989494324 + 2.0 * 6.045820236206055
Epoch 620, val loss: 0.7337965965270996
Epoch 630, training loss: 12.400275230407715 = 0.3130762279033661 + 2.0 * 6.043599605560303
Epoch 630, val loss: 0.727594792842865
Epoch 640, training loss: 12.382058143615723 = 0.2968842089176178 + 2.0 * 6.042586803436279
Epoch 640, val loss: 0.7222448587417603
Epoch 650, training loss: 12.380464553833008 = 0.28154829144477844 + 2.0 * 6.049458026885986
Epoch 650, val loss: 0.7177112698554993
Epoch 660, training loss: 12.349442481994629 = 0.2671049237251282 + 2.0 * 6.041168689727783
Epoch 660, val loss: 0.7139972448348999
Epoch 670, training loss: 12.333802223205566 = 0.2534957230091095 + 2.0 * 6.0401530265808105
Epoch 670, val loss: 0.7110366225242615
Epoch 680, training loss: 12.319295883178711 = 0.24062685668468475 + 2.0 * 6.039334297180176
Epoch 680, val loss: 0.7086959481239319
Epoch 690, training loss: 12.310029029846191 = 0.22848345339298248 + 2.0 * 6.040772914886475
Epoch 690, val loss: 0.7070742845535278
Epoch 700, training loss: 12.293903350830078 = 0.2171952724456787 + 2.0 * 6.03835391998291
Epoch 700, val loss: 0.7059920430183411
Epoch 710, training loss: 12.279349327087402 = 0.20658394694328308 + 2.0 * 6.036382675170898
Epoch 710, val loss: 0.7055034637451172
Epoch 720, training loss: 12.267139434814453 = 0.19658643007278442 + 2.0 * 6.035276412963867
Epoch 720, val loss: 0.7053936719894409
Epoch 730, training loss: 12.269721984863281 = 0.18714627623558044 + 2.0 * 6.041287899017334
Epoch 730, val loss: 0.7056885957717896
Epoch 740, training loss: 12.25480842590332 = 0.17840120196342468 + 2.0 * 6.038203716278076
Epoch 740, val loss: 0.7062996625900269
Epoch 750, training loss: 12.236860275268555 = 0.17016352713108063 + 2.0 * 6.033348560333252
Epoch 750, val loss: 0.7073927521705627
Epoch 760, training loss: 12.22581672668457 = 0.16242122650146484 + 2.0 * 6.031697750091553
Epoch 760, val loss: 0.7088097333908081
Epoch 770, training loss: 12.216261863708496 = 0.1550932675600052 + 2.0 * 6.030584335327148
Epoch 770, val loss: 0.7104688882827759
Epoch 780, training loss: 12.232571601867676 = 0.1481774002313614 + 2.0 * 6.042197227478027
Epoch 780, val loss: 0.7123191356658936
Epoch 790, training loss: 12.199588775634766 = 0.14168810844421387 + 2.0 * 6.028950214385986
Epoch 790, val loss: 0.7144315838813782
Epoch 800, training loss: 12.193560600280762 = 0.13556738197803497 + 2.0 * 6.028996467590332
Epoch 800, val loss: 0.716822624206543
Epoch 810, training loss: 12.186975479125977 = 0.12975578010082245 + 2.0 * 6.028609752655029
Epoch 810, val loss: 0.7193231582641602
Epoch 820, training loss: 12.18581771850586 = 0.12425540387630463 + 2.0 * 6.030781269073486
Epoch 820, val loss: 0.7219533920288086
Epoch 830, training loss: 12.17310619354248 = 0.11908625811338425 + 2.0 * 6.027009963989258
Epoch 830, val loss: 0.7247452139854431
Epoch 840, training loss: 12.166200637817383 = 0.11419224739074707 + 2.0 * 6.026004314422607
Epoch 840, val loss: 0.7277292609214783
Epoch 850, training loss: 12.16165828704834 = 0.1095283031463623 + 2.0 * 6.026064872741699
Epoch 850, val loss: 0.7307983040809631
Epoch 860, training loss: 12.159668922424316 = 0.10510466247797012 + 2.0 * 6.027282238006592
Epoch 860, val loss: 0.7339386343955994
Epoch 870, training loss: 12.150464057922363 = 0.10088331252336502 + 2.0 * 6.024790287017822
Epoch 870, val loss: 0.7372238039970398
Epoch 880, training loss: 12.142450332641602 = 0.0968909040093422 + 2.0 * 6.022779941558838
Epoch 880, val loss: 0.7405946850776672
Epoch 890, training loss: 12.142472267150879 = 0.09307949990034103 + 2.0 * 6.024696350097656
Epoch 890, val loss: 0.7440289258956909
Epoch 900, training loss: 12.151200294494629 = 0.08944739401340485 + 2.0 * 6.030876636505127
Epoch 900, val loss: 0.7474558353424072
Epoch 910, training loss: 12.132707595825195 = 0.08605831116437912 + 2.0 * 6.023324489593506
Epoch 910, val loss: 0.7508970499038696
Epoch 920, training loss: 12.124862670898438 = 0.08282142132520676 + 2.0 * 6.021020412445068
Epoch 920, val loss: 0.7544600963592529
Epoch 930, training loss: 12.118654251098633 = 0.07973472774028778 + 2.0 * 6.0194597244262695
Epoch 930, val loss: 0.7580497860908508
Epoch 940, training loss: 12.114297866821289 = 0.07678564637899399 + 2.0 * 6.018755912780762
Epoch 940, val loss: 0.7616950273513794
Epoch 950, training loss: 12.110818862915039 = 0.07396339625120163 + 2.0 * 6.018427848815918
Epoch 950, val loss: 0.7654168009757996
Epoch 960, training loss: 12.121131896972656 = 0.07127036899328232 + 2.0 * 6.024930953979492
Epoch 960, val loss: 0.7690042853355408
Epoch 970, training loss: 12.110963821411133 = 0.06874176859855652 + 2.0 * 6.021111011505127
Epoch 970, val loss: 0.7726368308067322
Epoch 980, training loss: 12.101654052734375 = 0.06634154915809631 + 2.0 * 6.017656326293945
Epoch 980, val loss: 0.776406466960907
Epoch 990, training loss: 12.098154067993164 = 0.06404072791337967 + 2.0 * 6.017056465148926
Epoch 990, val loss: 0.7801139950752258
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 1.0000
Flip ASR: 1.0000/225 nodes
The final ASR:0.76138, 0.21458, Accuracy:0.82099, 0.01222
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11632])
remove edge: torch.Size([2, 9502])
updated graph: torch.Size([2, 10578])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9889
Flip ASR: 0.9867/225 nodes
The final ASR:0.98155, 0.00522, Accuracy:0.82840, 0.00462
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.4 der2:0.4 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.4, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 18.711849212646484 = 1.9642367362976074 + 2.0 * 8.37380599975586
Epoch 0, val loss: 1.9639874696731567
Epoch 10, training loss: 18.69912338256836 = 1.9532365798950195 + 2.0 * 8.372942924499512
Epoch 10, val loss: 1.9526147842407227
Epoch 20, training loss: 18.672698974609375 = 1.9395830631256104 + 2.0 * 8.366558074951172
Epoch 20, val loss: 1.9383118152618408
Epoch 30, training loss: 18.54938507080078 = 1.9219005107879639 + 2.0 * 8.313742637634277
Epoch 30, val loss: 1.9199858903884888
Epoch 40, training loss: 17.746862411499023 = 1.9022127389907837 + 2.0 * 7.9223246574401855
Epoch 40, val loss: 1.9000605344772339
Epoch 50, training loss: 16.52271270751953 = 1.8837727308273315 + 2.0 * 7.319469928741455
Epoch 50, val loss: 1.8826040029525757
Epoch 60, training loss: 15.740472793579102 = 1.8716506958007812 + 2.0 * 6.93441104888916
Epoch 60, val loss: 1.8711650371551514
Epoch 70, training loss: 15.222990036010742 = 1.8612103462219238 + 2.0 * 6.68088960647583
Epoch 70, val loss: 1.861138939857483
Epoch 80, training loss: 14.91828727722168 = 1.8525323867797852 + 2.0 * 6.532877445220947
Epoch 80, val loss: 1.8528802394866943
Epoch 90, training loss: 14.738810539245605 = 1.8431835174560547 + 2.0 * 6.447813510894775
Epoch 90, val loss: 1.8440698385238647
Epoch 100, training loss: 14.616811752319336 = 1.8337197303771973 + 2.0 * 6.39154577255249
Epoch 100, val loss: 1.8352073431015015
Epoch 110, training loss: 14.519165992736816 = 1.8254419565200806 + 2.0 * 6.346861839294434
Epoch 110, val loss: 1.827402114868164
Epoch 120, training loss: 14.438796997070312 = 1.8183625936508179 + 2.0 * 6.310217380523682
Epoch 120, val loss: 1.8206787109375
Epoch 130, training loss: 14.372272491455078 = 1.8121148347854614 + 2.0 * 6.280078887939453
Epoch 130, val loss: 1.8146820068359375
Epoch 140, training loss: 14.316866874694824 = 1.806372046470642 + 2.0 * 6.255247592926025
Epoch 140, val loss: 1.8091751337051392
Epoch 150, training loss: 14.270711898803711 = 1.8007705211639404 + 2.0 * 6.234970569610596
Epoch 150, val loss: 1.8038766384124756
Epoch 160, training loss: 14.231791496276855 = 1.7950928211212158 + 2.0 * 6.218349456787109
Epoch 160, val loss: 1.7986212968826294
Epoch 170, training loss: 14.198149681091309 = 1.789194941520691 + 2.0 * 6.204477310180664
Epoch 170, val loss: 1.7932523488998413
Epoch 180, training loss: 14.164933204650879 = 1.7829269170761108 + 2.0 * 6.191003322601318
Epoch 180, val loss: 1.787716269493103
Epoch 190, training loss: 14.138461112976074 = 1.7760342359542847 + 2.0 * 6.18121337890625
Epoch 190, val loss: 1.7818450927734375
Epoch 200, training loss: 14.109907150268555 = 1.7684309482574463 + 2.0 * 6.170738220214844
Epoch 200, val loss: 1.7755323648452759
Epoch 210, training loss: 14.082624435424805 = 1.7599718570709229 + 2.0 * 6.1613264083862305
Epoch 210, val loss: 1.7686495780944824
Epoch 220, training loss: 14.062897682189941 = 1.7503575086593628 + 2.0 * 6.1562700271606445
Epoch 220, val loss: 1.7609716653823853
Epoch 230, training loss: 14.03168773651123 = 1.7393285036087036 + 2.0 * 6.146179676055908
Epoch 230, val loss: 1.7522426843643188
Epoch 240, training loss: 14.006532669067383 = 1.7266075611114502 + 2.0 * 6.139962673187256
Epoch 240, val loss: 1.7422630786895752
Epoch 250, training loss: 13.980345726013184 = 1.7118165493011475 + 2.0 * 6.1342644691467285
Epoch 250, val loss: 1.7307319641113281
Epoch 260, training loss: 13.952756881713867 = 1.6946280002593994 + 2.0 * 6.129064559936523
Epoch 260, val loss: 1.7173417806625366
Epoch 270, training loss: 13.923198699951172 = 1.6744152307510376 + 2.0 * 6.124391555786133
Epoch 270, val loss: 1.7016013860702515
Epoch 280, training loss: 13.893563270568848 = 1.6506788730621338 + 2.0 * 6.1214423179626465
Epoch 280, val loss: 1.682984471321106
Epoch 290, training loss: 13.85804271697998 = 1.6230523586273193 + 2.0 * 6.117495059967041
Epoch 290, val loss: 1.6612082719802856
Epoch 300, training loss: 13.819110870361328 = 1.5911097526550293 + 2.0 * 6.11400032043457
Epoch 300, val loss: 1.63590407371521
Epoch 310, training loss: 13.774908065795898 = 1.5546917915344238 + 2.0 * 6.110107898712158
Epoch 310, val loss: 1.6069543361663818
Epoch 320, training loss: 13.727621078491211 = 1.513994574546814 + 2.0 * 6.106813430786133
Epoch 320, val loss: 1.574346899986267
Epoch 330, training loss: 13.676931381225586 = 1.4690419435501099 + 2.0 * 6.103944778442383
Epoch 330, val loss: 1.5381932258605957
Epoch 340, training loss: 13.624804496765137 = 1.4209442138671875 + 2.0 * 6.101930141448975
Epoch 340, val loss: 1.4994443655014038
Epoch 350, training loss: 13.572227478027344 = 1.3713778257369995 + 2.0 * 6.100424766540527
Epoch 350, val loss: 1.4595987796783447
Epoch 360, training loss: 13.517038345336914 = 1.3212913274765015 + 2.0 * 6.097873687744141
Epoch 360, val loss: 1.419487714767456
Epoch 370, training loss: 13.461071968078613 = 1.2717106342315674 + 2.0 * 6.0946807861328125
Epoch 370, val loss: 1.3799796104431152
Epoch 380, training loss: 13.411649703979492 = 1.2232056856155396 + 2.0 * 6.094222068786621
Epoch 380, val loss: 1.3417564630508423
Epoch 390, training loss: 13.36361312866211 = 1.1768521070480347 + 2.0 * 6.093380451202393
Epoch 390, val loss: 1.3055100440979004
Epoch 400, training loss: 13.30947494506836 = 1.132832407951355 + 2.0 * 6.088321208953857
Epoch 400, val loss: 1.2716703414916992
Epoch 410, training loss: 13.26456069946289 = 1.0908704996109009 + 2.0 * 6.0868449211120605
Epoch 410, val loss: 1.2396550178527832
Epoch 420, training loss: 13.217236518859863 = 1.0507944822311401 + 2.0 * 6.083220958709717
Epoch 420, val loss: 1.209387183189392
Epoch 430, training loss: 13.180728912353516 = 1.0127005577087402 + 2.0 * 6.084013938903809
Epoch 430, val loss: 1.1805869340896606
Epoch 440, training loss: 13.138401985168457 = 0.976640522480011 + 2.0 * 6.080880641937256
Epoch 440, val loss: 1.1536074876785278
Epoch 450, training loss: 13.096925735473633 = 0.9422353506088257 + 2.0 * 6.077345371246338
Epoch 450, val loss: 1.1276308298110962
Epoch 460, training loss: 13.073481559753418 = 0.9091530442237854 + 2.0 * 6.082164287567139
Epoch 460, val loss: 1.1026320457458496
Epoch 470, training loss: 13.032938003540039 = 0.8777098655700684 + 2.0 * 6.077613830566406
Epoch 470, val loss: 1.0787140130996704
Epoch 480, training loss: 12.994236946105957 = 0.8476560711860657 + 2.0 * 6.0732903480529785
Epoch 480, val loss: 1.0557959079742432
Epoch 490, training loss: 12.958488464355469 = 0.8187888860702515 + 2.0 * 6.069849967956543
Epoch 490, val loss: 1.0337296724319458
Epoch 500, training loss: 12.927769660949707 = 0.7907713651657104 + 2.0 * 6.0684990882873535
Epoch 500, val loss: 1.01227605342865
Epoch 510, training loss: 12.906336784362793 = 0.7637345194816589 + 2.0 * 6.071300983428955
Epoch 510, val loss: 0.9914239048957825
Epoch 520, training loss: 12.867855072021484 = 0.737743616104126 + 2.0 * 6.065055847167969
Epoch 520, val loss: 0.9715177416801453
Epoch 530, training loss: 12.83959674835205 = 0.7126006484031677 + 2.0 * 6.063498020172119
Epoch 530, val loss: 0.9523692727088928
Epoch 540, training loss: 12.82217025756836 = 0.6881663203239441 + 2.0 * 6.067001819610596
Epoch 540, val loss: 0.9340819120407104
Epoch 550, training loss: 12.79015064239502 = 0.6646714806556702 + 2.0 * 6.062739372253418
Epoch 550, val loss: 0.9168750643730164
Epoch 560, training loss: 12.760320663452148 = 0.6421648859977722 + 2.0 * 6.059077739715576
Epoch 560, val loss: 0.9007853865623474
Epoch 570, training loss: 12.737666130065918 = 0.6200946569442749 + 2.0 * 6.058785915374756
Epoch 570, val loss: 0.8854303359985352
Epoch 580, training loss: 12.712894439697266 = 0.5985820293426514 + 2.0 * 6.057156085968018
Epoch 580, val loss: 0.8708577752113342
Epoch 590, training loss: 12.69133472442627 = 0.5776827931404114 + 2.0 * 6.056826114654541
Epoch 590, val loss: 0.8574408888816833
Epoch 600, training loss: 12.674942016601562 = 0.557192325592041 + 2.0 * 6.05887508392334
Epoch 600, val loss: 0.8446385860443115
Epoch 610, training loss: 12.649598121643066 = 0.5371283292770386 + 2.0 * 6.056234836578369
Epoch 610, val loss: 0.8326875567436218
Epoch 620, training loss: 12.621988296508789 = 0.517581582069397 + 2.0 * 6.052203178405762
Epoch 620, val loss: 0.8216330409049988
Epoch 630, training loss: 12.600204467773438 = 0.498329758644104 + 2.0 * 6.050937175750732
Epoch 630, val loss: 0.8111388683319092
Epoch 640, training loss: 12.589323043823242 = 0.47938990592956543 + 2.0 * 6.054966449737549
Epoch 640, val loss: 0.8013433814048767
Epoch 650, training loss: 12.57247543334961 = 0.4608461856842041 + 2.0 * 6.055814743041992
Epoch 650, val loss: 0.7922252416610718
Epoch 660, training loss: 12.543468475341797 = 0.4429318308830261 + 2.0 * 6.050268173217773
Epoch 660, val loss: 0.7838864326477051
Epoch 670, training loss: 12.519621849060059 = 0.4252772927284241 + 2.0 * 6.0471720695495605
Epoch 670, val loss: 0.7761322855949402
Epoch 680, training loss: 12.500421524047852 = 0.40786993503570557 + 2.0 * 6.046275615692139
Epoch 680, val loss: 0.7689317464828491
Epoch 690, training loss: 12.487837791442871 = 0.3908214271068573 + 2.0 * 6.048508167266846
Epoch 690, val loss: 0.7620346546173096
Epoch 700, training loss: 12.468098640441895 = 0.3743104636669159 + 2.0 * 6.046894073486328
Epoch 700, val loss: 0.7562109231948853
Epoch 710, training loss: 12.447857856750488 = 0.3581180274486542 + 2.0 * 6.044869899749756
Epoch 710, val loss: 0.750739336013794
Epoch 720, training loss: 12.4281587600708 = 0.3421802222728729 + 2.0 * 6.042989253997803
Epoch 720, val loss: 0.7457018494606018
Epoch 730, training loss: 12.430655479431152 = 0.3264930546283722 + 2.0 * 6.052081108093262
Epoch 730, val loss: 0.7411766648292542
Epoch 740, training loss: 12.401363372802734 = 0.31141194701194763 + 2.0 * 6.044975757598877
Epoch 740, val loss: 0.7370226383209229
Epoch 750, training loss: 12.380552291870117 = 0.2967519462108612 + 2.0 * 6.041900157928467
Epoch 750, val loss: 0.7337047457695007
Epoch 760, training loss: 12.362129211425781 = 0.2824915051460266 + 2.0 * 6.03981876373291
Epoch 760, val loss: 0.7306624054908752
Epoch 770, training loss: 12.360912322998047 = 0.268695205450058 + 2.0 * 6.046108722686768
Epoch 770, val loss: 0.7280676364898682
Epoch 780, training loss: 12.336801528930664 = 0.2555168569087982 + 2.0 * 6.040642261505127
Epoch 780, val loss: 0.7258815169334412
Epoch 790, training loss: 12.33275032043457 = 0.2429312914609909 + 2.0 * 6.044909477233887
Epoch 790, val loss: 0.7241989970207214
Epoch 800, training loss: 12.307868957519531 = 0.23102541267871857 + 2.0 * 6.038421630859375
Epoch 800, val loss: 0.723084568977356
Epoch 810, training loss: 12.292119026184082 = 0.2196943163871765 + 2.0 * 6.03621244430542
Epoch 810, val loss: 0.7223894000053406
Epoch 820, training loss: 12.27940845489502 = 0.20893388986587524 + 2.0 * 6.0352373123168945
Epoch 820, val loss: 0.7220697402954102
Epoch 830, training loss: 12.27070140838623 = 0.19869986176490784 + 2.0 * 6.036000728607178
Epoch 830, val loss: 0.7221774458885193
Epoch 840, training loss: 12.263188362121582 = 0.1890942007303238 + 2.0 * 6.037046909332275
Epoch 840, val loss: 0.7225435376167297
Epoch 850, training loss: 12.246588706970215 = 0.18012139201164246 + 2.0 * 6.033233642578125
Epoch 850, val loss: 0.7236143350601196
Epoch 860, training loss: 12.236085891723633 = 0.17166028916835785 + 2.0 * 6.032212734222412
Epoch 860, val loss: 0.7248300909996033
Epoch 870, training loss: 12.227249145507812 = 0.16365574300289154 + 2.0 * 6.031796932220459
Epoch 870, val loss: 0.7264226675033569
Epoch 880, training loss: 12.243064880371094 = 0.15611647069454193 + 2.0 * 6.043474197387695
Epoch 880, val loss: 0.728331983089447
Epoch 890, training loss: 12.219307899475098 = 0.14904367923736572 + 2.0 * 6.035131931304932
Epoch 890, val loss: 0.7304129600524902
Epoch 900, training loss: 12.204327583312988 = 0.14245961606502533 + 2.0 * 6.030933856964111
Epoch 900, val loss: 0.7330707311630249
Epoch 910, training loss: 12.194823265075684 = 0.1362377554178238 + 2.0 * 6.029292583465576
Epoch 910, val loss: 0.7357364296913147
Epoch 920, training loss: 12.210869789123535 = 0.13037249445915222 + 2.0 * 6.040248870849609
Epoch 920, val loss: 0.738700807094574
Epoch 930, training loss: 12.185168266296387 = 0.12486319988965988 + 2.0 * 6.030152320861816
Epoch 930, val loss: 0.7418168783187866
Epoch 940, training loss: 12.174460411071777 = 0.11969470977783203 + 2.0 * 6.027382850646973
Epoch 940, val loss: 0.745235025882721
Epoch 950, training loss: 12.168851852416992 = 0.11480793356895447 + 2.0 * 6.027021884918213
Epoch 950, val loss: 0.7487035989761353
Epoch 960, training loss: 12.18320369720459 = 0.11016900837421417 + 2.0 * 6.036517143249512
Epoch 960, val loss: 0.7520866394042969
Epoch 970, training loss: 12.157896041870117 = 0.10583866387605667 + 2.0 * 6.026028633117676
Epoch 970, val loss: 0.7559143304824829
Epoch 980, training loss: 12.152109146118164 = 0.10173693299293518 + 2.0 * 6.025186061859131
Epoch 980, val loss: 0.759826123714447
Epoch 990, training loss: 12.145889282226562 = 0.09782842546701431 + 2.0 * 6.0240302085876465
Epoch 990, val loss: 0.7637331485748291
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8111
Overall ASR: 0.5424
Flip ASR: 0.4667/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.708492279052734 = 1.9608134031295776 + 2.0 * 8.373839378356934
Epoch 0, val loss: 1.9602794647216797
Epoch 10, training loss: 18.696128845214844 = 1.9495518207550049 + 2.0 * 8.37328815460205
Epoch 10, val loss: 1.9480997323989868
Epoch 20, training loss: 18.67510223388672 = 1.9355708360671997 + 2.0 * 8.369765281677246
Epoch 20, val loss: 1.9328749179840088
Epoch 30, training loss: 18.606523513793945 = 1.9164620637893677 + 2.0 * 8.345030784606934
Epoch 30, val loss: 1.912232756614685
Epoch 40, training loss: 18.240209579467773 = 1.893311858177185 + 2.0 * 8.17344856262207
Epoch 40, val loss: 1.888478398323059
Epoch 50, training loss: 17.15284538269043 = 1.8678948879241943 + 2.0 * 7.642475128173828
Epoch 50, val loss: 1.8631561994552612
Epoch 60, training loss: 16.174654006958008 = 1.8528130054473877 + 2.0 * 7.160920143127441
Epoch 60, val loss: 1.8493727445602417
Epoch 70, training loss: 15.464564323425293 = 1.844068169593811 + 2.0 * 6.810247898101807
Epoch 70, val loss: 1.8403362035751343
Epoch 80, training loss: 15.090503692626953 = 1.8350129127502441 + 2.0 * 6.627745151519775
Epoch 80, val loss: 1.8304725885391235
Epoch 90, training loss: 14.847975730895996 = 1.8236628770828247 + 2.0 * 6.5121564865112305
Epoch 90, val loss: 1.8188315629959106
Epoch 100, training loss: 14.690657615661621 = 1.8126822710037231 + 2.0 * 6.438987731933594
Epoch 100, val loss: 1.8079655170440674
Epoch 110, training loss: 14.588489532470703 = 1.8029906749725342 + 2.0 * 6.392749309539795
Epoch 110, val loss: 1.798346996307373
Epoch 120, training loss: 14.505151748657227 = 1.7942016124725342 + 2.0 * 6.355474948883057
Epoch 120, val loss: 1.7895094156265259
Epoch 130, training loss: 14.438068389892578 = 1.7859396934509277 + 2.0 * 6.326064586639404
Epoch 130, val loss: 1.781205177307129
Epoch 140, training loss: 14.383993148803711 = 1.7779738903045654 + 2.0 * 6.303009510040283
Epoch 140, val loss: 1.7731969356536865
Epoch 150, training loss: 14.331689834594727 = 1.769964575767517 + 2.0 * 6.280862808227539
Epoch 150, val loss: 1.7653508186340332
Epoch 160, training loss: 14.287100791931152 = 1.7616957426071167 + 2.0 * 6.262702465057373
Epoch 160, val loss: 1.7574515342712402
Epoch 170, training loss: 14.244033813476562 = 1.7528526782989502 + 2.0 * 6.245590686798096
Epoch 170, val loss: 1.7493596076965332
Epoch 180, training loss: 14.203704833984375 = 1.7432993650436401 + 2.0 * 6.230202674865723
Epoch 180, val loss: 1.7409347295761108
Epoch 190, training loss: 14.16796875 = 1.7327536344528198 + 2.0 * 6.217607498168945
Epoch 190, val loss: 1.7319313287734985
Epoch 200, training loss: 14.135778427124023 = 1.7209426164627075 + 2.0 * 6.207417964935303
Epoch 200, val loss: 1.7221152782440186
Epoch 210, training loss: 14.09869384765625 = 1.7077405452728271 + 2.0 * 6.195476531982422
Epoch 210, val loss: 1.7113723754882812
Epoch 220, training loss: 14.063458442687988 = 1.6929290294647217 + 2.0 * 6.185264587402344
Epoch 220, val loss: 1.6995227336883545
Epoch 230, training loss: 14.029207229614258 = 1.6760430335998535 + 2.0 * 6.176581859588623
Epoch 230, val loss: 1.6861828565597534
Epoch 240, training loss: 14.00034236907959 = 1.656729817390442 + 2.0 * 6.171806335449219
Epoch 240, val loss: 1.6710811853408813
Epoch 250, training loss: 13.958486557006836 = 1.6349170207977295 + 2.0 * 6.161784648895264
Epoch 250, val loss: 1.6541184186935425
Epoch 260, training loss: 13.918200492858887 = 1.6100343465805054 + 2.0 * 6.154083251953125
Epoch 260, val loss: 1.634772539138794
Epoch 270, training loss: 13.882573127746582 = 1.581565022468567 + 2.0 * 6.150504112243652
Epoch 270, val loss: 1.6125835180282593
Epoch 280, training loss: 13.83823013305664 = 1.549567699432373 + 2.0 * 6.144330978393555
Epoch 280, val loss: 1.5875141620635986
Epoch 290, training loss: 13.790390014648438 = 1.5143213272094727 + 2.0 * 6.138034343719482
Epoch 290, val loss: 1.5597150325775146
Epoch 300, training loss: 13.743522644042969 = 1.4761260747909546 + 2.0 * 6.133698463439941
Epoch 300, val loss: 1.5295369625091553
Epoch 310, training loss: 13.694960594177246 = 1.4359551668167114 + 2.0 * 6.129502773284912
Epoch 310, val loss: 1.4976762533187866
Epoch 320, training loss: 13.650253295898438 = 1.3948359489440918 + 2.0 * 6.127708435058594
Epoch 320, val loss: 1.465129017829895
Epoch 330, training loss: 13.59859848022461 = 1.3538819551467896 + 2.0 * 6.122358322143555
Epoch 330, val loss: 1.432896375656128
Epoch 340, training loss: 13.55166244506836 = 1.3137637376785278 + 2.0 * 6.1189494132995605
Epoch 340, val loss: 1.4015557765960693
Epoch 350, training loss: 13.504234313964844 = 1.2748466730117798 + 2.0 * 6.114693641662598
Epoch 350, val loss: 1.3713500499725342
Epoch 360, training loss: 13.461514472961426 = 1.2372232675552368 + 2.0 * 6.11214542388916
Epoch 360, val loss: 1.342223048210144
Epoch 370, training loss: 13.421965599060059 = 1.2007197141647339 + 2.0 * 6.110622882843018
Epoch 370, val loss: 1.3144149780273438
Epoch 380, training loss: 13.375819206237793 = 1.16530442237854 + 2.0 * 6.105257511138916
Epoch 380, val loss: 1.2875148057937622
Epoch 390, training loss: 13.3365478515625 = 1.1301383972167969 + 2.0 * 6.103204727172852
Epoch 390, val loss: 1.260885238647461
Epoch 400, training loss: 13.296760559082031 = 1.0949140787124634 + 2.0 * 6.10092306137085
Epoch 400, val loss: 1.23444664478302
Epoch 410, training loss: 13.25754165649414 = 1.059665560722351 + 2.0 * 6.09893798828125
Epoch 410, val loss: 1.207953929901123
Epoch 420, training loss: 13.21484661102295 = 1.0240823030471802 + 2.0 * 6.095382213592529
Epoch 420, val loss: 1.1812189817428589
Epoch 430, training loss: 13.176865577697754 = 0.9881073236465454 + 2.0 * 6.09437894821167
Epoch 430, val loss: 1.1541368961334229
Epoch 440, training loss: 13.138285636901855 = 0.9519959688186646 + 2.0 * 6.09314489364624
Epoch 440, val loss: 1.1270254850387573
Epoch 450, training loss: 13.097898483276367 = 0.916303277015686 + 2.0 * 6.090797424316406
Epoch 450, val loss: 1.1000226736068726
Epoch 460, training loss: 13.056102752685547 = 0.8810214996337891 + 2.0 * 6.087540626525879
Epoch 460, val loss: 1.0735200643539429
Epoch 470, training loss: 13.019153594970703 = 0.8464844822883606 + 2.0 * 6.086334705352783
Epoch 470, val loss: 1.0477101802825928
Epoch 480, training loss: 12.982902526855469 = 0.8130927681922913 + 2.0 * 6.084904670715332
Epoch 480, val loss: 1.0227875709533691
Epoch 490, training loss: 12.948273658752441 = 0.7811906337738037 + 2.0 * 6.083541393280029
Epoch 490, val loss: 0.9992765188217163
Epoch 500, training loss: 12.913368225097656 = 0.751018762588501 + 2.0 * 6.081174850463867
Epoch 500, val loss: 0.9771828055381775
Epoch 510, training loss: 12.878473281860352 = 0.7226958274841309 + 2.0 * 6.077888488769531
Epoch 510, val loss: 0.9568403363227844
Epoch 520, training loss: 12.849096298217773 = 0.6960027813911438 + 2.0 * 6.076546669006348
Epoch 520, val loss: 0.937976062297821
Epoch 530, training loss: 12.824934959411621 = 0.6707569360733032 + 2.0 * 6.077088832855225
Epoch 530, val loss: 0.9206406474113464
Epoch 540, training loss: 12.8026123046875 = 0.6470237374305725 + 2.0 * 6.077794075012207
Epoch 540, val loss: 0.9047583341598511
Epoch 550, training loss: 12.76810073852539 = 0.6247971653938293 + 2.0 * 6.071651935577393
Epoch 550, val loss: 0.8904533386230469
Epoch 560, training loss: 12.744959831237793 = 0.6036462783813477 + 2.0 * 6.070656776428223
Epoch 560, val loss: 0.8772580623626709
Epoch 570, training loss: 12.722161293029785 = 0.5833946466445923 + 2.0 * 6.069383144378662
Epoch 570, val loss: 0.8651042580604553
Epoch 580, training loss: 12.71490478515625 = 0.5639491081237793 + 2.0 * 6.075477600097656
Epoch 580, val loss: 0.8538692593574524
Epoch 590, training loss: 12.679540634155273 = 0.545322835445404 + 2.0 * 6.067109107971191
Epoch 590, val loss: 0.8436071276664734
Epoch 600, training loss: 12.656618118286133 = 0.5273148417472839 + 2.0 * 6.0646514892578125
Epoch 600, val loss: 0.8343172073364258
Epoch 610, training loss: 12.650249481201172 = 0.5099384188652039 + 2.0 * 6.070155620574951
Epoch 610, val loss: 0.825747549533844
Epoch 620, training loss: 12.617403984069824 = 0.49319490790367126 + 2.0 * 6.06210470199585
Epoch 620, val loss: 0.8182154893875122
Epoch 630, training loss: 12.599067687988281 = 0.4770314395427704 + 2.0 * 6.061017990112305
Epoch 630, val loss: 0.8113452792167664
Epoch 640, training loss: 12.589024543762207 = 0.4614323079586029 + 2.0 * 6.063796043395996
Epoch 640, val loss: 0.8051022887229919
Epoch 650, training loss: 12.564508438110352 = 0.44642511010169983 + 2.0 * 6.059041500091553
Epoch 650, val loss: 0.7997132539749146
Epoch 660, training loss: 12.56387996673584 = 0.4320569634437561 + 2.0 * 6.065911293029785
Epoch 660, val loss: 0.7950485348701477
Epoch 670, training loss: 12.531872749328613 = 0.4182650148868561 + 2.0 * 6.0568037033081055
Epoch 670, val loss: 0.7911543846130371
Epoch 680, training loss: 12.516400337219238 = 0.40511634945869446 + 2.0 * 6.055642127990723
Epoch 680, val loss: 0.7880614995956421
Epoch 690, training loss: 12.50793743133545 = 0.39251887798309326 + 2.0 * 6.057709217071533
Epoch 690, val loss: 0.7855269312858582
Epoch 700, training loss: 12.498897552490234 = 0.3806017339229584 + 2.0 * 6.059147834777832
Epoch 700, val loss: 0.7835846543312073
Epoch 710, training loss: 12.474143028259277 = 0.36926111578941345 + 2.0 * 6.052441120147705
Epoch 710, val loss: 0.7824618220329285
Epoch 720, training loss: 12.459810256958008 = 0.35842305421829224 + 2.0 * 6.050693511962891
Epoch 720, val loss: 0.7817947268486023
Epoch 730, training loss: 12.45112133026123 = 0.34803181886672974 + 2.0 * 6.051544666290283
Epoch 730, val loss: 0.7815917730331421
Epoch 740, training loss: 12.44340991973877 = 0.338168203830719 + 2.0 * 6.052620887756348
Epoch 740, val loss: 0.7817296385765076
Epoch 750, training loss: 12.432490348815918 = 0.32878121733665466 + 2.0 * 6.051854610443115
Epoch 750, val loss: 0.782454252243042
Epoch 760, training loss: 12.41722297668457 = 0.31987082958221436 + 2.0 * 6.048676013946533
Epoch 760, val loss: 0.7835606336593628
Epoch 770, training loss: 12.40457534790039 = 0.31131842732429504 + 2.0 * 6.046628475189209
Epoch 770, val loss: 0.785006582736969
Epoch 780, training loss: 12.396533966064453 = 0.3031062185764313 + 2.0 * 6.046713829040527
Epoch 780, val loss: 0.7867715358734131
Epoch 790, training loss: 12.392898559570312 = 0.29523414373397827 + 2.0 * 6.048832416534424
Epoch 790, val loss: 0.788741409778595
Epoch 800, training loss: 12.378232955932617 = 0.28773653507232666 + 2.0 * 6.045248031616211
Epoch 800, val loss: 0.7910412549972534
Epoch 810, training loss: 12.37124252319336 = 0.28056836128234863 + 2.0 * 6.045337200164795
Epoch 810, val loss: 0.7935937643051147
Epoch 820, training loss: 12.359238624572754 = 0.2736959755420685 + 2.0 * 6.042771339416504
Epoch 820, val loss: 0.7963839173316956
Epoch 830, training loss: 12.355280876159668 = 0.2670592665672302 + 2.0 * 6.0441107749938965
Epoch 830, val loss: 0.7993122339248657
Epoch 840, training loss: 12.343263626098633 = 0.26072096824645996 + 2.0 * 6.041271209716797
Epoch 840, val loss: 0.8023315668106079
Epoch 850, training loss: 12.341053009033203 = 0.25458773970603943 + 2.0 * 6.043232440948486
Epoch 850, val loss: 0.8055933713912964
Epoch 860, training loss: 12.328900337219238 = 0.24870288372039795 + 2.0 * 6.040098667144775
Epoch 860, val loss: 0.8089569807052612
Epoch 870, training loss: 12.320096969604492 = 0.2429676502943039 + 2.0 * 6.038564682006836
Epoch 870, val loss: 0.8124421238899231
Epoch 880, training loss: 12.330491065979004 = 0.23739489912986755 + 2.0 * 6.046547889709473
Epoch 880, val loss: 0.815934956073761
Epoch 890, training loss: 12.312689781188965 = 0.23199088871479034 + 2.0 * 6.04034948348999
Epoch 890, val loss: 0.8194928765296936
Epoch 900, training loss: 12.307470321655273 = 0.22670118510723114 + 2.0 * 6.040384769439697
Epoch 900, val loss: 0.8232635259628296
Epoch 910, training loss: 12.29693603515625 = 0.2215437889099121 + 2.0 * 6.03769588470459
Epoch 910, val loss: 0.8269203305244446
Epoch 920, training loss: 12.313812255859375 = 0.21641284227371216 + 2.0 * 6.048699855804443
Epoch 920, val loss: 0.8306309580802917
Epoch 930, training loss: 12.28359603881836 = 0.21137785911560059 + 2.0 * 6.03610897064209
Epoch 930, val loss: 0.8343218564987183
Epoch 940, training loss: 12.275959968566895 = 0.2063538283109665 + 2.0 * 6.0348029136657715
Epoch 940, val loss: 0.8382304906845093
Epoch 950, training loss: 12.269247055053711 = 0.20127660036087036 + 2.0 * 6.033985137939453
Epoch 950, val loss: 0.8420709371566772
Epoch 960, training loss: 12.291422843933105 = 0.19613566994667053 + 2.0 * 6.047643661499023
Epoch 960, val loss: 0.8458095192909241
Epoch 970, training loss: 12.259018898010254 = 0.19094616174697876 + 2.0 * 6.034036159515381
Epoch 970, val loss: 0.8495259881019592
Epoch 980, training loss: 12.252243995666504 = 0.18565067648887634 + 2.0 * 6.033296585083008
Epoch 980, val loss: 0.8534758687019348
Epoch 990, training loss: 12.244040489196777 = 0.180215522646904 + 2.0 * 6.031912326812744
Epoch 990, val loss: 0.8572947382926941
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7889
Overall ASR: 0.9557
Flip ASR: 0.9467/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.716310501098633 = 1.9686375856399536 + 2.0 * 8.373836517333984
Epoch 0, val loss: 1.9634923934936523
Epoch 10, training loss: 18.704269409179688 = 1.9575042724609375 + 2.0 * 8.373382568359375
Epoch 10, val loss: 1.9526333808898926
Epoch 20, training loss: 18.685203552246094 = 1.9440279006958008 + 2.0 * 8.370587348937988
Epoch 20, val loss: 1.9388651847839355
Epoch 30, training loss: 18.63041114807129 = 1.9257166385650635 + 2.0 * 8.352347373962402
Epoch 30, val loss: 1.919759750366211
Epoch 40, training loss: 18.375778198242188 = 1.9033113718032837 + 2.0 * 8.236233711242676
Epoch 40, val loss: 1.8967663049697876
Epoch 50, training loss: 17.286941528320312 = 1.8790868520736694 + 2.0 * 7.703927040100098
Epoch 50, val loss: 1.8719760179519653
Epoch 60, training loss: 16.316341400146484 = 1.859473705291748 + 2.0 * 7.228434085845947
Epoch 60, val loss: 1.8537052869796753
Epoch 70, training loss: 15.599308967590332 = 1.847214698791504 + 2.0 * 6.876047134399414
Epoch 70, val loss: 1.842464566230774
Epoch 80, training loss: 15.223893165588379 = 1.8364856243133545 + 2.0 * 6.693703651428223
Epoch 80, val loss: 1.8314629793167114
Epoch 90, training loss: 15.0042724609375 = 1.8254562616348267 + 2.0 * 6.589407920837402
Epoch 90, val loss: 1.8200721740722656
Epoch 100, training loss: 14.834421157836914 = 1.814511775970459 + 2.0 * 6.509954929351807
Epoch 100, val loss: 1.8092105388641357
Epoch 110, training loss: 14.691765785217285 = 1.804993748664856 + 2.0 * 6.443386077880859
Epoch 110, val loss: 1.7998424768447876
Epoch 120, training loss: 14.572869300842285 = 1.7970227003097534 + 2.0 * 6.387923240661621
Epoch 120, val loss: 1.7919495105743408
Epoch 130, training loss: 14.482210159301758 = 1.7896031141281128 + 2.0 * 6.346303462982178
Epoch 130, val loss: 1.7845335006713867
Epoch 140, training loss: 14.404916763305664 = 1.782112956047058 + 2.0 * 6.311401844024658
Epoch 140, val loss: 1.7770811319351196
Epoch 150, training loss: 14.344182968139648 = 1.7744176387786865 + 2.0 * 6.284882545471191
Epoch 150, val loss: 1.769491195678711
Epoch 160, training loss: 14.293235778808594 = 1.7661664485931396 + 2.0 * 6.2635345458984375
Epoch 160, val loss: 1.7615690231323242
Epoch 170, training loss: 14.247957229614258 = 1.7571120262145996 + 2.0 * 6.24542236328125
Epoch 170, val loss: 1.753180980682373
Epoch 180, training loss: 14.209209442138672 = 1.7470790147781372 + 2.0 * 6.231065273284912
Epoch 180, val loss: 1.7441707849502563
Epoch 190, training loss: 14.171162605285645 = 1.7358776330947876 + 2.0 * 6.217642307281494
Epoch 190, val loss: 1.7343389987945557
Epoch 200, training loss: 14.135522842407227 = 1.7233009338378906 + 2.0 * 6.206110954284668
Epoch 200, val loss: 1.7235112190246582
Epoch 210, training loss: 14.09928035736084 = 1.7090113162994385 + 2.0 * 6.19513463973999
Epoch 210, val loss: 1.7114561796188354
Epoch 220, training loss: 14.064009666442871 = 1.692752718925476 + 2.0 * 6.185628414154053
Epoch 220, val loss: 1.697876214981079
Epoch 230, training loss: 14.032315254211426 = 1.6740063428878784 + 2.0 * 6.179154396057129
Epoch 230, val loss: 1.6825364828109741
Epoch 240, training loss: 13.99201488494873 = 1.6524975299835205 + 2.0 * 6.1697587966918945
Epoch 240, val loss: 1.6651532649993896
Epoch 250, training loss: 13.953289985656738 = 1.6278455257415771 + 2.0 * 6.162722110748291
Epoch 250, val loss: 1.6453733444213867
Epoch 260, training loss: 13.916767120361328 = 1.599482774734497 + 2.0 * 6.158642292022705
Epoch 260, val loss: 1.6229557991027832
Epoch 270, training loss: 13.870805740356445 = 1.5675498247146606 + 2.0 * 6.151628017425537
Epoch 270, val loss: 1.597748875617981
Epoch 280, training loss: 13.824392318725586 = 1.531568169593811 + 2.0 * 6.146411895751953
Epoch 280, val loss: 1.5696150064468384
Epoch 290, training loss: 13.775070190429688 = 1.4913413524627686 + 2.0 * 6.14186429977417
Epoch 290, val loss: 1.5382673740386963
Epoch 300, training loss: 13.725997924804688 = 1.4469099044799805 + 2.0 * 6.1395440101623535
Epoch 300, val loss: 1.5038535594940186
Epoch 310, training loss: 13.669340133666992 = 1.399528980255127 + 2.0 * 6.1349053382873535
Epoch 310, val loss: 1.467549443244934
Epoch 320, training loss: 13.613330841064453 = 1.3501701354980469 + 2.0 * 6.131580352783203
Epoch 320, val loss: 1.4299101829528809
Epoch 330, training loss: 13.5549955368042 = 1.2987792491912842 + 2.0 * 6.128108024597168
Epoch 330, val loss: 1.391141653060913
Epoch 340, training loss: 13.504035949707031 = 1.2461332082748413 + 2.0 * 6.128951549530029
Epoch 340, val loss: 1.3518826961517334
Epoch 350, training loss: 13.438387870788574 = 1.193161964416504 + 2.0 * 6.122612953186035
Epoch 350, val loss: 1.3127326965332031
Epoch 360, training loss: 13.386889457702637 = 1.140224575996399 + 2.0 * 6.123332500457764
Epoch 360, val loss: 1.2736812829971313
Epoch 370, training loss: 13.323286056518555 = 1.0886558294296265 + 2.0 * 6.117315292358398
Epoch 370, val loss: 1.2357263565063477
Epoch 380, training loss: 13.269840240478516 = 1.0387922525405884 + 2.0 * 6.115523815155029
Epoch 380, val loss: 1.1993050575256348
Epoch 390, training loss: 13.215679168701172 = 0.9908728003501892 + 2.0 * 6.112403392791748
Epoch 390, val loss: 1.1646745204925537
Epoch 400, training loss: 13.166584968566895 = 0.9456478357315063 + 2.0 * 6.11046838760376
Epoch 400, val loss: 1.1320937871932983
Epoch 410, training loss: 13.119911193847656 = 0.9032156467437744 + 2.0 * 6.1083478927612305
Epoch 410, val loss: 1.102088212966919
Epoch 420, training loss: 13.072640419006348 = 0.8639649748802185 + 2.0 * 6.104337692260742
Epoch 420, val loss: 1.0746830701828003
Epoch 430, training loss: 13.035794258117676 = 0.8271680474281311 + 2.0 * 6.104312896728516
Epoch 430, val loss: 1.049808382987976
Epoch 440, training loss: 12.996880531311035 = 0.7933884859085083 + 2.0 * 6.101746082305908
Epoch 440, val loss: 1.0273035764694214
Epoch 450, training loss: 12.962869644165039 = 0.7620347738265991 + 2.0 * 6.100417613983154
Epoch 450, val loss: 1.007382869720459
Epoch 460, training loss: 12.922825813293457 = 0.7328335642814636 + 2.0 * 6.094995975494385
Epoch 460, val loss: 0.9893251657485962
Epoch 470, training loss: 12.889528274536133 = 0.7053097486495972 + 2.0 * 6.092109203338623
Epoch 470, val loss: 0.973029375076294
Epoch 480, training loss: 12.87185001373291 = 0.6792572140693665 + 2.0 * 6.096296310424805
Epoch 480, val loss: 0.958040714263916
Epoch 490, training loss: 12.833840370178223 = 0.6549419164657593 + 2.0 * 6.089449405670166
Epoch 490, val loss: 0.9446696639060974
Epoch 500, training loss: 12.804137229919434 = 0.6318552494049072 + 2.0 * 6.086141109466553
Epoch 500, val loss: 0.9324637651443481
Epoch 510, training loss: 12.77659797668457 = 0.6099097728729248 + 2.0 * 6.083343982696533
Epoch 510, val loss: 0.9212338328361511
Epoch 520, training loss: 12.751665115356445 = 0.5889358520507812 + 2.0 * 6.081364631652832
Epoch 520, val loss: 0.910997211933136
Epoch 530, training loss: 12.727616310119629 = 0.5688087344169617 + 2.0 * 6.079403877258301
Epoch 530, val loss: 0.9014862179756165
Epoch 540, training loss: 12.708938598632812 = 0.5494961738586426 + 2.0 * 6.079720973968506
Epoch 540, val loss: 0.8927685618400574
Epoch 550, training loss: 12.690465927124023 = 0.5310786962509155 + 2.0 * 6.079693794250488
Epoch 550, val loss: 0.884992778301239
Epoch 560, training loss: 12.664600372314453 = 0.513411819934845 + 2.0 * 6.075594425201416
Epoch 560, val loss: 0.8777883648872375
Epoch 570, training loss: 12.640591621398926 = 0.49623778462409973 + 2.0 * 6.072176933288574
Epoch 570, val loss: 0.8712440133094788
Epoch 580, training loss: 12.624418258666992 = 0.47944220900535583 + 2.0 * 6.072487831115723
Epoch 580, val loss: 0.8651895523071289
Epoch 590, training loss: 12.605569839477539 = 0.46319738030433655 + 2.0 * 6.071186065673828
Epoch 590, val loss: 0.8596504330635071
Epoch 600, training loss: 12.58200740814209 = 0.44717422127723694 + 2.0 * 6.067416667938232
Epoch 600, val loss: 0.8546597957611084
Epoch 610, training loss: 12.575244903564453 = 0.43148890137672424 + 2.0 * 6.071877956390381
Epoch 610, val loss: 0.8500670194625854
Epoch 620, training loss: 12.549996376037598 = 0.4162220358848572 + 2.0 * 6.066887378692627
Epoch 620, val loss: 0.8459300994873047
Epoch 630, training loss: 12.52881908416748 = 0.40124309062957764 + 2.0 * 6.063787937164307
Epoch 630, val loss: 0.8423919677734375
Epoch 640, training loss: 12.513418197631836 = 0.38645583391189575 + 2.0 * 6.063481330871582
Epoch 640, val loss: 0.8391602039337158
Epoch 650, training loss: 12.49671459197998 = 0.37186992168426514 + 2.0 * 6.062422275543213
Epoch 650, val loss: 0.836253821849823
Epoch 660, training loss: 12.492788314819336 = 0.3575557470321655 + 2.0 * 6.0676164627075195
Epoch 660, val loss: 0.8337698578834534
Epoch 670, training loss: 12.463240623474121 = 0.3435627222061157 + 2.0 * 6.059838771820068
Epoch 670, val loss: 0.8316357731819153
Epoch 680, training loss: 12.445281982421875 = 0.32979318499565125 + 2.0 * 6.05774450302124
Epoch 680, val loss: 0.8299998641014099
Epoch 690, training loss: 12.430855751037598 = 0.316226065158844 + 2.0 * 6.057314872741699
Epoch 690, val loss: 0.8287385106086731
Epoch 700, training loss: 12.418182373046875 = 0.30299443006515503 + 2.0 * 6.057593822479248
Epoch 700, val loss: 0.8278518915176392
Epoch 710, training loss: 12.40303897857666 = 0.2901741862297058 + 2.0 * 6.056432247161865
Epoch 710, val loss: 0.8275330662727356
Epoch 720, training loss: 12.385843276977539 = 0.27764633297920227 + 2.0 * 6.054098606109619
Epoch 720, val loss: 0.8275967836380005
Epoch 730, training loss: 12.37698745727539 = 0.26541852951049805 + 2.0 * 6.055784225463867
Epoch 730, val loss: 0.8280247449874878
Epoch 740, training loss: 12.36609172821045 = 0.2536994516849518 + 2.0 * 6.056196212768555
Epoch 740, val loss: 0.8288573622703552
Epoch 750, training loss: 12.345376968383789 = 0.24237960577011108 + 2.0 * 6.051498889923096
Epoch 750, val loss: 0.8301825523376465
Epoch 760, training loss: 12.3365478515625 = 0.23151108622550964 + 2.0 * 6.052518367767334
Epoch 760, val loss: 0.8318774104118347
Epoch 770, training loss: 12.321084976196289 = 0.22114641964435577 + 2.0 * 6.04996919631958
Epoch 770, val loss: 0.8339670896530151
Epoch 780, training loss: 12.30876636505127 = 0.2112690806388855 + 2.0 * 6.04874849319458
Epoch 780, val loss: 0.8365598320960999
Epoch 790, training loss: 12.300152778625488 = 0.20184417068958282 + 2.0 * 6.049154281616211
Epoch 790, val loss: 0.8395068645477295
Epoch 800, training loss: 12.29321002960205 = 0.19297373294830322 + 2.0 * 6.0501179695129395
Epoch 800, val loss: 0.8426138162612915
Epoch 810, training loss: 12.277843475341797 = 0.18460212647914886 + 2.0 * 6.046620845794678
Epoch 810, val loss: 0.8462547063827515
Epoch 820, training loss: 12.267627716064453 = 0.17664924263954163 + 2.0 * 6.045489311218262
Epoch 820, val loss: 0.850088357925415
Epoch 830, training loss: 12.265519142150879 = 0.16910827159881592 + 2.0 * 6.048205375671387
Epoch 830, val loss: 0.8541473746299744
Epoch 840, training loss: 12.25901985168457 = 0.16198685765266418 + 2.0 * 6.048516273498535
Epoch 840, val loss: 0.8584144711494446
Epoch 850, training loss: 12.243687629699707 = 0.15530113875865936 + 2.0 * 6.044193267822266
Epoch 850, val loss: 0.8628541231155396
Epoch 860, training loss: 12.23566722869873 = 0.14896391332149506 + 2.0 * 6.043351650238037
Epoch 860, val loss: 0.8675726652145386
Epoch 870, training loss: 12.22614574432373 = 0.14296160638332367 + 2.0 * 6.041592121124268
Epoch 870, val loss: 0.8725406527519226
Epoch 880, training loss: 12.227216720581055 = 0.13725963234901428 + 2.0 * 6.044978618621826
Epoch 880, val loss: 0.8776274919509888
Epoch 890, training loss: 12.215933799743652 = 0.1318713277578354 + 2.0 * 6.042031288146973
Epoch 890, val loss: 0.8827365636825562
Epoch 900, training loss: 12.217301368713379 = 0.1267748922109604 + 2.0 * 6.045263290405273
Epoch 900, val loss: 0.8880585432052612
Epoch 910, training loss: 12.203885078430176 = 0.1219831183552742 + 2.0 * 6.040950775146484
Epoch 910, val loss: 0.8936043977737427
Epoch 920, training loss: 12.195935249328613 = 0.11739657074213028 + 2.0 * 6.03926944732666
Epoch 920, val loss: 0.8991289734840393
Epoch 930, training loss: 12.191007614135742 = 0.11305725574493408 + 2.0 * 6.038975238800049
Epoch 930, val loss: 0.9047401547431946
Epoch 940, training loss: 12.184463500976562 = 0.10893064737319946 + 2.0 * 6.037766456604004
Epoch 940, val loss: 0.9105433821678162
Epoch 950, training loss: 12.17794132232666 = 0.10499407351016998 + 2.0 * 6.036473751068115
Epoch 950, val loss: 0.9163476824760437
Epoch 960, training loss: 12.175583839416504 = 0.10127291083335876 + 2.0 * 6.037155628204346
Epoch 960, val loss: 0.9219736456871033
Epoch 970, training loss: 12.171346664428711 = 0.09777778387069702 + 2.0 * 6.036784648895264
Epoch 970, val loss: 0.9278962016105652
Epoch 980, training loss: 12.163914680480957 = 0.09444130957126617 + 2.0 * 6.034736633300781
Epoch 980, val loss: 0.9337921142578125
Epoch 990, training loss: 12.158797264099121 = 0.09124425053596497 + 2.0 * 6.03377628326416
Epoch 990, val loss: 0.9396862387657166
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7741
Overall ASR: 0.9557
Flip ASR: 0.9467/225 nodes
The final ASR:0.81796, 0.19482, Accuracy:0.79136, 0.01522
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11626])
remove edge: torch.Size([2, 9486])
updated graph: torch.Size([2, 10556])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9889
Flip ASR: 0.9867/225 nodes
The final ASR:0.98155, 0.00522, Accuracy:0.83333, 0.00524
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.4 der2:0.4 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 18.6899356842041 = 1.9421570301055908 + 2.0 * 8.373888969421387
Epoch 0, val loss: 1.9491981267929077
Epoch 10, training loss: 18.678422927856445 = 1.9314988851547241 + 2.0 * 8.373461723327637
Epoch 10, val loss: 1.9380452632904053
Epoch 20, training loss: 18.659189224243164 = 1.9180009365081787 + 2.0 * 8.370594024658203
Epoch 20, val loss: 1.9235587120056152
Epoch 30, training loss: 18.60028839111328 = 1.8993078470230103 + 2.0 * 8.35049057006836
Epoch 30, val loss: 1.9034672975540161
Epoch 40, training loss: 18.281373977661133 = 1.8776487112045288 + 2.0 * 8.201862335205078
Epoch 40, val loss: 1.8814442157745361
Epoch 50, training loss: 17.33454704284668 = 1.856439471244812 + 2.0 * 7.739054203033447
Epoch 50, val loss: 1.8602083921432495
Epoch 60, training loss: 16.44135856628418 = 1.8410509824752808 + 2.0 * 7.300153732299805
Epoch 60, val loss: 1.846104621887207
Epoch 70, training loss: 15.603567123413086 = 1.8298132419586182 + 2.0 * 6.886877059936523
Epoch 70, val loss: 1.835718035697937
Epoch 80, training loss: 15.182978630065918 = 1.8197805881500244 + 2.0 * 6.681599140167236
Epoch 80, val loss: 1.82607102394104
Epoch 90, training loss: 14.934158325195312 = 1.80732262134552 + 2.0 * 6.563417911529541
Epoch 90, val loss: 1.8139822483062744
Epoch 100, training loss: 14.76642894744873 = 1.793785810470581 + 2.0 * 6.486321449279785
Epoch 100, val loss: 1.8010785579681396
Epoch 110, training loss: 14.63690185546875 = 1.7801344394683838 + 2.0 * 6.428383827209473
Epoch 110, val loss: 1.7883119583129883
Epoch 120, training loss: 14.529292106628418 = 1.7663296461105347 + 2.0 * 6.381481170654297
Epoch 120, val loss: 1.7758198976516724
Epoch 130, training loss: 14.442183494567871 = 1.7516614198684692 + 2.0 * 6.345261096954346
Epoch 130, val loss: 1.7628200054168701
Epoch 140, training loss: 14.366056442260742 = 1.7354589700698853 + 2.0 * 6.315298557281494
Epoch 140, val loss: 1.7486480474472046
Epoch 150, training loss: 14.301047325134277 = 1.717290997505188 + 2.0 * 6.2918782234191895
Epoch 150, val loss: 1.7331560850143433
Epoch 160, training loss: 14.239768028259277 = 1.6969679594039917 + 2.0 * 6.271399974822998
Epoch 160, val loss: 1.715969204902649
Epoch 170, training loss: 14.187175750732422 = 1.6739197969436646 + 2.0 * 6.256628036499023
Epoch 170, val loss: 1.6967519521713257
Epoch 180, training loss: 14.127652168273926 = 1.6478956937789917 + 2.0 * 6.239878177642822
Epoch 180, val loss: 1.6752134561538696
Epoch 190, training loss: 14.071511268615723 = 1.6185057163238525 + 2.0 * 6.226502895355225
Epoch 190, val loss: 1.6511437892913818
Epoch 200, training loss: 14.01552677154541 = 1.5856201648712158 + 2.0 * 6.214953422546387
Epoch 200, val loss: 1.6244081258773804
Epoch 210, training loss: 13.96012020111084 = 1.5492764711380005 + 2.0 * 6.2054219245910645
Epoch 210, val loss: 1.5951255559921265
Epoch 220, training loss: 13.899988174438477 = 1.5095062255859375 + 2.0 * 6.1952409744262695
Epoch 220, val loss: 1.5632133483886719
Epoch 230, training loss: 13.842544555664062 = 1.466380000114441 + 2.0 * 6.188082218170166
Epoch 230, val loss: 1.5287680625915527
Epoch 240, training loss: 13.783905982971191 = 1.4205046892166138 + 2.0 * 6.181700706481934
Epoch 240, val loss: 1.4925915002822876
Epoch 250, training loss: 13.721170425415039 = 1.3727331161499023 + 2.0 * 6.174218654632568
Epoch 250, val loss: 1.4551339149475098
Epoch 260, training loss: 13.656604766845703 = 1.323864221572876 + 2.0 * 6.166370391845703
Epoch 260, val loss: 1.4170161485671997
Epoch 270, training loss: 13.59399127960205 = 1.2746580839157104 + 2.0 * 6.159666538238525
Epoch 270, val loss: 1.3787983655929565
Epoch 280, training loss: 13.544896125793457 = 1.2262122631072998 + 2.0 * 6.159341812133789
Epoch 280, val loss: 1.341401219367981
Epoch 290, training loss: 13.480993270874023 = 1.1798648834228516 + 2.0 * 6.150564193725586
Epoch 290, val loss: 1.3061609268188477
Epoch 300, training loss: 13.423539161682129 = 1.1359742879867554 + 2.0 * 6.143782615661621
Epoch 300, val loss: 1.2731127738952637
Epoch 310, training loss: 13.377469062805176 = 1.094589352607727 + 2.0 * 6.141439914703369
Epoch 310, val loss: 1.2422330379486084
Epoch 320, training loss: 13.326334953308105 = 1.0557304620742798 + 2.0 * 6.1353020668029785
Epoch 320, val loss: 1.213388442993164
Epoch 330, training loss: 13.286288261413574 = 1.0190632343292236 + 2.0 * 6.133612632751465
Epoch 330, val loss: 1.1866908073425293
Epoch 340, training loss: 13.238020896911621 = 0.984572172164917 + 2.0 * 6.1267242431640625
Epoch 340, val loss: 1.1618363857269287
Epoch 350, training loss: 13.195571899414062 = 0.9515897035598755 + 2.0 * 6.121991157531738
Epoch 350, val loss: 1.138541340827942
Epoch 360, training loss: 13.155333518981934 = 0.9198998808860779 + 2.0 * 6.1177167892456055
Epoch 360, val loss: 1.1164251565933228
Epoch 370, training loss: 13.120526313781738 = 0.8890594840049744 + 2.0 * 6.115733623504639
Epoch 370, val loss: 1.0952438116073608
Epoch 380, training loss: 13.087175369262695 = 0.8591369986534119 + 2.0 * 6.114019393920898
Epoch 380, val loss: 1.0749098062515259
Epoch 390, training loss: 13.046564102172852 = 0.8301129341125488 + 2.0 * 6.1082258224487305
Epoch 390, val loss: 1.0554840564727783
Epoch 400, training loss: 13.018759727478027 = 0.8019466996192932 + 2.0 * 6.1084065437316895
Epoch 400, val loss: 1.0369386672973633
Epoch 410, training loss: 12.982234954833984 = 0.7748939394950867 + 2.0 * 6.103670597076416
Epoch 410, val loss: 1.0193042755126953
Epoch 420, training loss: 12.947113990783691 = 0.7487556338310242 + 2.0 * 6.099179267883301
Epoch 420, val loss: 1.0026932954788208
Epoch 430, training loss: 12.925928115844727 = 0.7236990928649902 + 2.0 * 6.101114273071289
Epoch 430, val loss: 0.987001895904541
Epoch 440, training loss: 12.890617370605469 = 0.6995391249656677 + 2.0 * 6.095539093017578
Epoch 440, val loss: 0.9722940921783447
Epoch 450, training loss: 12.861257553100586 = 0.6763466000556946 + 2.0 * 6.0924553871154785
Epoch 450, val loss: 0.958432137966156
Epoch 460, training loss: 12.834790229797363 = 0.6539448499679565 + 2.0 * 6.090422630310059
Epoch 460, val loss: 0.9453391432762146
Epoch 470, training loss: 12.81064224243164 = 0.6322152614593506 + 2.0 * 6.0892133712768555
Epoch 470, val loss: 0.9329691529273987
Epoch 480, training loss: 12.790263175964355 = 0.6110790371894836 + 2.0 * 6.089591979980469
Epoch 480, val loss: 0.921187698841095
Epoch 490, training loss: 12.75629711151123 = 0.590660810470581 + 2.0 * 6.082818031311035
Epoch 490, val loss: 0.9099830985069275
Epoch 500, training loss: 12.733500480651855 = 0.5706960558891296 + 2.0 * 6.08140230178833
Epoch 500, val loss: 0.8993257880210876
Epoch 510, training loss: 12.713693618774414 = 0.5511135458946228 + 2.0 * 6.081290245056152
Epoch 510, val loss: 0.8891711235046387
Epoch 520, training loss: 12.69407844543457 = 0.5319660902023315 + 2.0 * 6.081056118011475
Epoch 520, val loss: 0.8793675899505615
Epoch 530, training loss: 12.665224075317383 = 0.5132911205291748 + 2.0 * 6.0759663581848145
Epoch 530, val loss: 0.8701183795928955
Epoch 540, training loss: 12.648829460144043 = 0.4950231611728668 + 2.0 * 6.076903343200684
Epoch 540, val loss: 0.8613831996917725
Epoch 550, training loss: 12.623518943786621 = 0.47719481587409973 + 2.0 * 6.073162078857422
Epoch 550, val loss: 0.8530400991439819
Epoch 560, training loss: 12.600733757019043 = 0.4596441686153412 + 2.0 * 6.070544719696045
Epoch 560, val loss: 0.8452180624008179
Epoch 570, training loss: 12.584097862243652 = 0.4423987567424774 + 2.0 * 6.070849418640137
Epoch 570, val loss: 0.8377705812454224
Epoch 580, training loss: 12.565584182739258 = 0.4253651201725006 + 2.0 * 6.0701093673706055
Epoch 580, val loss: 0.8307750821113586
Epoch 590, training loss: 12.544519424438477 = 0.40870794653892517 + 2.0 * 6.067905902862549
Epoch 590, val loss: 0.8242567777633667
Epoch 600, training loss: 12.522077560424805 = 0.39233213663101196 + 2.0 * 6.064872741699219
Epoch 600, val loss: 0.8181777596473694
Epoch 610, training loss: 12.503828048706055 = 0.3762010931968689 + 2.0 * 6.06381368637085
Epoch 610, val loss: 0.8124622702598572
Epoch 620, training loss: 12.490365028381348 = 0.3603646159172058 + 2.0 * 6.065000057220459
Epoch 620, val loss: 0.807097852230072
Epoch 630, training loss: 12.468338966369629 = 0.3447735905647278 + 2.0 * 6.0617828369140625
Epoch 630, val loss: 0.8021784424781799
Epoch 640, training loss: 12.45177936553955 = 0.32944416999816895 + 2.0 * 6.0611677169799805
Epoch 640, val loss: 0.7976016402244568
Epoch 650, training loss: 12.430685997009277 = 0.3144271969795227 + 2.0 * 6.05812931060791
Epoch 650, val loss: 0.7934733629226685
Epoch 660, training loss: 12.413812637329102 = 0.29965832829475403 + 2.0 * 6.057076930999756
Epoch 660, val loss: 0.789769172668457
Epoch 670, training loss: 12.408336639404297 = 0.2851670980453491 + 2.0 * 6.061584949493408
Epoch 670, val loss: 0.7863675951957703
Epoch 680, training loss: 12.384167671203613 = 0.2709871530532837 + 2.0 * 6.0565900802612305
Epoch 680, val loss: 0.7834238409996033
Epoch 690, training loss: 12.365152359008789 = 0.25720569491386414 + 2.0 * 6.053973197937012
Epoch 690, val loss: 0.780977189540863
Epoch 700, training loss: 12.36058521270752 = 0.24377313256263733 + 2.0 * 6.058405876159668
Epoch 700, val loss: 0.7790127396583557
Epoch 710, training loss: 12.336450576782227 = 0.23081988096237183 + 2.0 * 6.0528154373168945
Epoch 710, val loss: 0.7775705456733704
Epoch 720, training loss: 12.321737289428711 = 0.21830549836158752 + 2.0 * 6.051715850830078
Epoch 720, val loss: 0.7766625881195068
Epoch 730, training loss: 12.304441452026367 = 0.2062717080116272 + 2.0 * 6.049084663391113
Epoch 730, val loss: 0.776341438293457
Epoch 740, training loss: 12.294870376586914 = 0.19475483894348145 + 2.0 * 6.050057888031006
Epoch 740, val loss: 0.7766051292419434
Epoch 750, training loss: 12.281542778015137 = 0.18381522595882416 + 2.0 * 6.048863887786865
Epoch 750, val loss: 0.7773988246917725
Epoch 760, training loss: 12.268730163574219 = 0.17349304258823395 + 2.0 * 6.047618389129639
Epoch 760, val loss: 0.7787318229675293
Epoch 770, training loss: 12.255502700805664 = 0.16376341879367828 + 2.0 * 6.045869827270508
Epoch 770, val loss: 0.7806130051612854
Epoch 780, training loss: 12.253395080566406 = 0.15462303161621094 + 2.0 * 6.049386024475098
Epoch 780, val loss: 0.7829159498214722
Epoch 790, training loss: 12.23752212524414 = 0.14606396853923798 + 2.0 * 6.045729160308838
Epoch 790, val loss: 0.7857608199119568
Epoch 800, training loss: 12.224310874938965 = 0.13808824121952057 + 2.0 * 6.043111324310303
Epoch 800, val loss: 0.789024829864502
Epoch 810, training loss: 12.213478088378906 = 0.13064050674438477 + 2.0 * 6.04141902923584
Epoch 810, val loss: 0.7926432490348816
Epoch 820, training loss: 12.212042808532715 = 0.1236962154507637 + 2.0 * 6.044173240661621
Epoch 820, val loss: 0.7966389060020447
Epoch 830, training loss: 12.203398704528809 = 0.11727424710988998 + 2.0 * 6.043062210083008
Epoch 830, val loss: 0.8009336590766907
Epoch 840, training loss: 12.192200660705566 = 0.11128593981266022 + 2.0 * 6.040457248687744
Epoch 840, val loss: 0.8054581880569458
Epoch 850, training loss: 12.181622505187988 = 0.10575760155916214 + 2.0 * 6.037932395935059
Epoch 850, val loss: 0.8103122115135193
Epoch 860, training loss: 12.175865173339844 = 0.10060678422451019 + 2.0 * 6.037629127502441
Epoch 860, val loss: 0.8154160976409912
Epoch 870, training loss: 12.17523193359375 = 0.09580215811729431 + 2.0 * 6.039714813232422
Epoch 870, val loss: 0.8206280469894409
Epoch 880, training loss: 12.168468475341797 = 0.09133461117744446 + 2.0 * 6.038567066192627
Epoch 880, val loss: 0.826061487197876
Epoch 890, training loss: 12.158933639526367 = 0.08715779334306717 + 2.0 * 6.035887718200684
Epoch 890, val loss: 0.8316646814346313
Epoch 900, training loss: 12.151314735412598 = 0.08325589448213577 + 2.0 * 6.034029483795166
Epoch 900, val loss: 0.8373608589172363
Epoch 910, training loss: 12.145874977111816 = 0.07960150390863419 + 2.0 * 6.03313684463501
Epoch 910, val loss: 0.843119204044342
Epoch 920, training loss: 12.14192008972168 = 0.07616265118122101 + 2.0 * 6.032878875732422
Epoch 920, val loss: 0.8490039110183716
Epoch 930, training loss: 12.146154403686523 = 0.07292313873767853 + 2.0 * 6.03661584854126
Epoch 930, val loss: 0.8548963069915771
Epoch 940, training loss: 12.132662773132324 = 0.0698927715420723 + 2.0 * 6.0313849449157715
Epoch 940, val loss: 0.8608604073524475
Epoch 950, training loss: 12.126067161560059 = 0.06703940033912659 + 2.0 * 6.029513835906982
Epoch 950, val loss: 0.8668524026870728
Epoch 960, training loss: 12.122069358825684 = 0.0643426924943924 + 2.0 * 6.028863430023193
Epoch 960, val loss: 0.8728669881820679
Epoch 970, training loss: 12.129363059997559 = 0.06179581210017204 + 2.0 * 6.033783435821533
Epoch 970, val loss: 0.878848671913147
Epoch 980, training loss: 12.124841690063477 = 0.059394802898168564 + 2.0 * 6.032723426818848
Epoch 980, val loss: 0.8849504590034485
Epoch 990, training loss: 12.110949516296387 = 0.057120371609926224 + 2.0 * 6.026914596557617
Epoch 990, val loss: 0.8909913897514343
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7593
Overall ASR: 0.4428
Flip ASR: 0.3422/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.694028854370117 = 1.946344256401062 + 2.0 * 8.373842239379883
Epoch 0, val loss: 1.9404816627502441
Epoch 10, training loss: 18.68193817138672 = 1.9355741739273071 + 2.0 * 8.37318229675293
Epoch 10, val loss: 1.929269552230835
Epoch 20, training loss: 18.66024398803711 = 1.9222955703735352 + 2.0 * 8.368973731994629
Epoch 20, val loss: 1.9149411916732788
Epoch 30, training loss: 18.59295654296875 = 1.9045202732086182 + 2.0 * 8.344218254089355
Epoch 30, val loss: 1.8955544233322144
Epoch 40, training loss: 18.306676864624023 = 1.8849791288375854 + 2.0 * 8.210848808288574
Epoch 40, val loss: 1.8749769926071167
Epoch 50, training loss: 17.229814529418945 = 1.8652220964431763 + 2.0 * 7.682295799255371
Epoch 50, val loss: 1.8535830974578857
Epoch 60, training loss: 16.140520095825195 = 1.8468775749206543 + 2.0 * 7.14682149887085
Epoch 60, val loss: 1.8342310190200806
Epoch 70, training loss: 15.512982368469238 = 1.831188440322876 + 2.0 * 6.840897083282471
Epoch 70, val loss: 1.8167866468429565
Epoch 80, training loss: 15.180249214172363 = 1.8152379989624023 + 2.0 * 6.6825056076049805
Epoch 80, val loss: 1.8004802465438843
Epoch 90, training loss: 14.980236053466797 = 1.7998207807540894 + 2.0 * 6.590207576751709
Epoch 90, val loss: 1.7850390672683716
Epoch 100, training loss: 14.825695037841797 = 1.7843966484069824 + 2.0 * 6.520649433135986
Epoch 100, val loss: 1.7700453996658325
Epoch 110, training loss: 14.692588806152344 = 1.7688826322555542 + 2.0 * 6.46185302734375
Epoch 110, val loss: 1.7554535865783691
Epoch 120, training loss: 14.589451789855957 = 1.7533212900161743 + 2.0 * 6.418065071105957
Epoch 120, val loss: 1.7408857345581055
Epoch 130, training loss: 14.502817153930664 = 1.7364383935928345 + 2.0 * 6.3831892013549805
Epoch 130, val loss: 1.7252196073532104
Epoch 140, training loss: 14.427717208862305 = 1.7179592847824097 + 2.0 * 6.354878902435303
Epoch 140, val loss: 1.7085157632827759
Epoch 150, training loss: 14.354385375976562 = 1.6977840662002563 + 2.0 * 6.328300476074219
Epoch 150, val loss: 1.6907755136489868
Epoch 160, training loss: 14.288164138793945 = 1.6753076314926147 + 2.0 * 6.3064284324646
Epoch 160, val loss: 1.6713401079177856
Epoch 170, training loss: 14.226686477661133 = 1.649998664855957 + 2.0 * 6.288343906402588
Epoch 170, val loss: 1.649750828742981
Epoch 180, training loss: 14.16500473022461 = 1.6213383674621582 + 2.0 * 6.271833419799805
Epoch 180, val loss: 1.6256027221679688
Epoch 190, training loss: 14.10361385345459 = 1.5889618396759033 + 2.0 * 6.257326126098633
Epoch 190, val loss: 1.5985071659088135
Epoch 200, training loss: 14.040739059448242 = 1.5527160167694092 + 2.0 * 6.244011402130127
Epoch 200, val loss: 1.568498134613037
Epoch 210, training loss: 13.983973503112793 = 1.5132755041122437 + 2.0 * 6.235349178314209
Epoch 210, val loss: 1.5364842414855957
Epoch 220, training loss: 13.915783882141113 = 1.4720964431762695 + 2.0 * 6.221843719482422
Epoch 220, val loss: 1.5035808086395264
Epoch 230, training loss: 13.85366153717041 = 1.4295283555984497 + 2.0 * 6.212066650390625
Epoch 230, val loss: 1.4701472520828247
Epoch 240, training loss: 13.792671203613281 = 1.386258602142334 + 2.0 * 6.203206539154053
Epoch 240, val loss: 1.436833143234253
Epoch 250, training loss: 13.74435043334961 = 1.3433303833007812 + 2.0 * 6.200510025024414
Epoch 250, val loss: 1.4044160842895508
Epoch 260, training loss: 13.68173885345459 = 1.3022772073745728 + 2.0 * 6.189730644226074
Epoch 260, val loss: 1.3744674921035767
Epoch 270, training loss: 13.62568473815918 = 1.2630692720413208 + 2.0 * 6.181307792663574
Epoch 270, val loss: 1.3466907739639282
Epoch 280, training loss: 13.572747230529785 = 1.2253963947296143 + 2.0 * 6.173675537109375
Epoch 280, val loss: 1.3206945657730103
Epoch 290, training loss: 13.524145126342773 = 1.1889678239822388 + 2.0 * 6.167588710784912
Epoch 290, val loss: 1.2961103916168213
Epoch 300, training loss: 13.488689422607422 = 1.1536879539489746 + 2.0 * 6.167500972747803
Epoch 300, val loss: 1.2728569507598877
Epoch 310, training loss: 13.43354320526123 = 1.1196273565292358 + 2.0 * 6.156958103179932
Epoch 310, val loss: 1.2505542039871216
Epoch 320, training loss: 13.389552116394043 = 1.0858910083770752 + 2.0 * 6.151830673217773
Epoch 320, val loss: 1.2288706302642822
Epoch 330, training loss: 13.345693588256836 = 1.0522606372833252 + 2.0 * 6.146716594696045
Epoch 330, val loss: 1.2073040008544922
Epoch 340, training loss: 13.311960220336914 = 1.0185332298278809 + 2.0 * 6.1467132568359375
Epoch 340, val loss: 1.1858484745025635
Epoch 350, training loss: 13.26470947265625 = 0.9851970672607422 + 2.0 * 6.139756202697754
Epoch 350, val loss: 1.164609670639038
Epoch 360, training loss: 13.221951484680176 = 0.9523048400878906 + 2.0 * 6.134823322296143
Epoch 360, val loss: 1.1438478231430054
Epoch 370, training loss: 13.180418968200684 = 0.9199026226997375 + 2.0 * 6.130258083343506
Epoch 370, val loss: 1.1235929727554321
Epoch 380, training loss: 13.152973175048828 = 0.8882532715797424 + 2.0 * 6.132359981536865
Epoch 380, val loss: 1.104036808013916
Epoch 390, training loss: 13.106447219848633 = 0.8575283885002136 + 2.0 * 6.124459266662598
Epoch 390, val loss: 1.0853222608566284
Epoch 400, training loss: 13.068336486816406 = 0.8278404474258423 + 2.0 * 6.120247840881348
Epoch 400, val loss: 1.067635178565979
Epoch 410, training loss: 13.032939910888672 = 0.799028754234314 + 2.0 * 6.116955757141113
Epoch 410, val loss: 1.0507563352584839
Epoch 420, training loss: 12.999445915222168 = 0.7709799408912659 + 2.0 * 6.114233016967773
Epoch 420, val loss: 1.034602165222168
Epoch 430, training loss: 12.968850135803223 = 0.7436647415161133 + 2.0 * 6.112592697143555
Epoch 430, val loss: 1.0190876722335815
Epoch 440, training loss: 12.941934585571289 = 0.7171120047569275 + 2.0 * 6.1124114990234375
Epoch 440, val loss: 1.0041407346725464
Epoch 450, training loss: 12.904879570007324 = 0.6913416385650635 + 2.0 * 6.10676908493042
Epoch 450, val loss: 0.9897804260253906
Epoch 460, training loss: 12.87345027923584 = 0.66595458984375 + 2.0 * 6.103747844696045
Epoch 460, val loss: 0.9756577014923096
Epoch 470, training loss: 12.842798233032227 = 0.6409353613853455 + 2.0 * 6.100931644439697
Epoch 470, val loss: 0.9616849422454834
Epoch 480, training loss: 12.832305908203125 = 0.6161247491836548 + 2.0 * 6.108090400695801
Epoch 480, val loss: 0.9477735161781311
Epoch 490, training loss: 12.790169715881348 = 0.5918394923210144 + 2.0 * 6.099164962768555
Epoch 490, val loss: 0.9341093897819519
Epoch 500, training loss: 12.758058547973633 = 0.5678713917732239 + 2.0 * 6.095093727111816
Epoch 500, val loss: 0.9207021594047546
Epoch 510, training loss: 12.730243682861328 = 0.5442615151405334 + 2.0 * 6.092990875244141
Epoch 510, val loss: 0.9073585867881775
Epoch 520, training loss: 12.702951431274414 = 0.5210627317428589 + 2.0 * 6.090944290161133
Epoch 520, val loss: 0.8942639827728271
Epoch 530, training loss: 12.678483009338379 = 0.4984169900417328 + 2.0 * 6.090033054351807
Epoch 530, val loss: 0.8815300464630127
Epoch 540, training loss: 12.648920059204102 = 0.47628962993621826 + 2.0 * 6.086315155029297
Epoch 540, val loss: 0.8691924810409546
Epoch 550, training loss: 12.625397682189941 = 0.45457401871681213 + 2.0 * 6.08541202545166
Epoch 550, val loss: 0.8572272658348083
Epoch 560, training loss: 12.60295295715332 = 0.43332749605178833 + 2.0 * 6.084812641143799
Epoch 560, val loss: 0.8457799553871155
Epoch 570, training loss: 12.575459480285645 = 0.4127093255519867 + 2.0 * 6.0813751220703125
Epoch 570, val loss: 0.8350380659103394
Epoch 580, training loss: 12.551046371459961 = 0.39257875084877014 + 2.0 * 6.079233646392822
Epoch 580, val loss: 0.8247851729393005
Epoch 590, training loss: 12.533052444458008 = 0.3728753626346588 + 2.0 * 6.0800886154174805
Epoch 590, val loss: 0.8150293231010437
Epoch 600, training loss: 12.513675689697266 = 0.35364824533462524 + 2.0 * 6.080013751983643
Epoch 600, val loss: 0.8058704137802124
Epoch 610, training loss: 12.485219955444336 = 0.33500799536705017 + 2.0 * 6.075106143951416
Epoch 610, val loss: 0.7974691987037659
Epoch 620, training loss: 12.464700698852539 = 0.31693366169929504 + 2.0 * 6.073883533477783
Epoch 620, val loss: 0.7896783947944641
Epoch 630, training loss: 12.456079483032227 = 0.2994857430458069 + 2.0 * 6.078296661376953
Epoch 630, val loss: 0.7825566530227661
Epoch 640, training loss: 12.428295135498047 = 0.28266772627830505 + 2.0 * 6.072813510894775
Epoch 640, val loss: 0.7761051058769226
Epoch 650, training loss: 12.404547691345215 = 0.2667302191257477 + 2.0 * 6.06890869140625
Epoch 650, val loss: 0.7705039381980896
Epoch 660, training loss: 12.388567924499512 = 0.2515595257282257 + 2.0 * 6.068504333496094
Epoch 660, val loss: 0.7656375765800476
Epoch 670, training loss: 12.37062931060791 = 0.23723545670509338 + 2.0 * 6.066697120666504
Epoch 670, val loss: 0.7615302205085754
Epoch 680, training loss: 12.357128143310547 = 0.2238597273826599 + 2.0 * 6.066634178161621
Epoch 680, val loss: 0.7582749128341675
Epoch 690, training loss: 12.338881492614746 = 0.21134354174137115 + 2.0 * 6.0637688636779785
Epoch 690, val loss: 0.7558334469795227
Epoch 700, training loss: 12.331280708312988 = 0.19967225193977356 + 2.0 * 6.0658040046691895
Epoch 700, val loss: 0.754174530506134
Epoch 710, training loss: 12.321187973022461 = 0.18884244561195374 + 2.0 * 6.0661725997924805
Epoch 710, val loss: 0.7532142996788025
Epoch 720, training loss: 12.299771308898926 = 0.17879632115364075 + 2.0 * 6.060487270355225
Epoch 720, val loss: 0.7530811429023743
Epoch 730, training loss: 12.28707504272461 = 0.16948959231376648 + 2.0 * 6.058792591094971
Epoch 730, val loss: 0.7536345720291138
Epoch 740, training loss: 12.278464317321777 = 0.16083920001983643 + 2.0 * 6.058812618255615
Epoch 740, val loss: 0.7548452019691467
Epoch 750, training loss: 12.266983985900879 = 0.15276694297790527 + 2.0 * 6.057108402252197
Epoch 750, val loss: 0.756546139717102
Epoch 760, training loss: 12.258894920349121 = 0.14530940353870392 + 2.0 * 6.056792736053467
Epoch 760, val loss: 0.7589182257652283
Epoch 770, training loss: 12.248687744140625 = 0.13835392892360687 + 2.0 * 6.055166721343994
Epoch 770, val loss: 0.7617409229278564
Epoch 780, training loss: 12.241387367248535 = 0.13183918595314026 + 2.0 * 6.054774284362793
Epoch 780, val loss: 0.7649869322776794
Epoch 790, training loss: 12.236739158630371 = 0.12573714554309845 + 2.0 * 6.0555009841918945
Epoch 790, val loss: 0.7685664296150208
Epoch 800, training loss: 12.227494239807129 = 0.12002327293157578 + 2.0 * 6.053735256195068
Epoch 800, val loss: 0.7725043892860413
Epoch 810, training loss: 12.215683937072754 = 0.11465146392583847 + 2.0 * 6.050516128540039
Epoch 810, val loss: 0.776709258556366
Epoch 820, training loss: 12.20920467376709 = 0.10956376791000366 + 2.0 * 6.049820423126221
Epoch 820, val loss: 0.7811751365661621
Epoch 830, training loss: 12.210189819335938 = 0.10474952310323715 + 2.0 * 6.052720069885254
Epoch 830, val loss: 0.7858918309211731
Epoch 840, training loss: 12.197775840759277 = 0.10020702332258224 + 2.0 * 6.048784255981445
Epoch 840, val loss: 0.7909274697303772
Epoch 850, training loss: 12.189931869506836 = 0.09588152915239334 + 2.0 * 6.047025203704834
Epoch 850, val loss: 0.7959668040275574
Epoch 860, training loss: 12.191985130310059 = 0.09176434576511383 + 2.0 * 6.050110340118408
Epoch 860, val loss: 0.8012088537216187
Epoch 870, training loss: 12.18265151977539 = 0.08785852789878845 + 2.0 * 6.047396659851074
Epoch 870, val loss: 0.8065091371536255
Epoch 880, training loss: 12.174480438232422 = 0.08414850383996964 + 2.0 * 6.045166015625
Epoch 880, val loss: 0.811944842338562
Epoch 890, training loss: 12.169072151184082 = 0.08061689138412476 + 2.0 * 6.044227600097656
Epoch 890, val loss: 0.8173580765724182
Epoch 900, training loss: 12.162664413452148 = 0.07725488394498825 + 2.0 * 6.0427045822143555
Epoch 900, val loss: 0.8229150772094727
Epoch 910, training loss: 12.170564651489258 = 0.07405666261911392 + 2.0 * 6.048254013061523
Epoch 910, val loss: 0.8284673690795898
Epoch 920, training loss: 12.153403282165527 = 0.07100433111190796 + 2.0 * 6.041199684143066
Epoch 920, val loss: 0.8340612649917603
Epoch 930, training loss: 12.148024559020996 = 0.06811334937810898 + 2.0 * 6.0399556159973145
Epoch 930, val loss: 0.8396689295768738
Epoch 940, training loss: 12.144451141357422 = 0.0653596743941307 + 2.0 * 6.03954553604126
Epoch 940, val loss: 0.845275342464447
Epoch 950, training loss: 12.15109920501709 = 0.06273156404495239 + 2.0 * 6.044183731079102
Epoch 950, val loss: 0.8509087562561035
Epoch 960, training loss: 12.14051342010498 = 0.06025969609618187 + 2.0 * 6.040126800537109
Epoch 960, val loss: 0.856535792350769
Epoch 970, training loss: 12.135787963867188 = 0.05789090692996979 + 2.0 * 6.0389485359191895
Epoch 970, val loss: 0.8620890378952026
Epoch 980, training loss: 12.127538681030273 = 0.0556514672935009 + 2.0 * 6.035943508148193
Epoch 980, val loss: 0.8675948977470398
Epoch 990, training loss: 12.125778198242188 = 0.05352000519633293 + 2.0 * 6.036128997802734
Epoch 990, val loss: 0.8731286525726318
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.1993
Flip ASR: 0.2000/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.69277572631836 = 1.9450504779815674 + 2.0 * 8.373862266540527
Epoch 0, val loss: 1.9348176717758179
Epoch 10, training loss: 18.681840896606445 = 1.9350203275680542 + 2.0 * 8.37341022491455
Epoch 10, val loss: 1.925628900527954
Epoch 20, training loss: 18.66358757019043 = 1.9230995178222656 + 2.0 * 8.370244026184082
Epoch 20, val loss: 1.9144567251205444
Epoch 30, training loss: 18.604955673217773 = 1.9074300527572632 + 2.0 * 8.348762512207031
Epoch 30, val loss: 1.899674892425537
Epoch 40, training loss: 18.310239791870117 = 1.8890713453292847 + 2.0 * 8.21058464050293
Epoch 40, val loss: 1.8828589916229248
Epoch 50, training loss: 17.18221092224121 = 1.8698917627334595 + 2.0 * 7.6561598777771
Epoch 50, val loss: 1.86494779586792
Epoch 60, training loss: 16.171186447143555 = 1.8544036149978638 + 2.0 * 7.15839147567749
Epoch 60, val loss: 1.8510360717773438
Epoch 70, training loss: 15.496630668640137 = 1.8412679433822632 + 2.0 * 6.827681541442871
Epoch 70, val loss: 1.8387783765792847
Epoch 80, training loss: 15.182645797729492 = 1.8286081552505493 + 2.0 * 6.677018642425537
Epoch 80, val loss: 1.826887845993042
Epoch 90, training loss: 14.95843505859375 = 1.8155431747436523 + 2.0 * 6.571445941925049
Epoch 90, val loss: 1.8149234056472778
Epoch 100, training loss: 14.79235553741455 = 1.8027862310409546 + 2.0 * 6.494784832000732
Epoch 100, val loss: 1.8036028146743774
Epoch 110, training loss: 14.666838645935059 = 1.7900758981704712 + 2.0 * 6.438381195068359
Epoch 110, val loss: 1.7925117015838623
Epoch 120, training loss: 14.565573692321777 = 1.7774118185043335 + 2.0 * 6.394081115722656
Epoch 120, val loss: 1.7812020778656006
Epoch 130, training loss: 14.487259864807129 = 1.7641962766647339 + 2.0 * 6.361531734466553
Epoch 130, val loss: 1.7691773176193237
Epoch 140, training loss: 14.417060852050781 = 1.7497248649597168 + 2.0 * 6.333668231964111
Epoch 140, val loss: 1.7560237646102905
Epoch 150, training loss: 14.353754997253418 = 1.7337232828140259 + 2.0 * 6.310015678405762
Epoch 150, val loss: 1.741654634475708
Epoch 160, training loss: 14.294939041137695 = 1.7156033515930176 + 2.0 * 6.289668083190918
Epoch 160, val loss: 1.7256356477737427
Epoch 170, training loss: 14.243696212768555 = 1.6947901248931885 + 2.0 * 6.274453163146973
Epoch 170, val loss: 1.7075692415237427
Epoch 180, training loss: 14.188436508178711 = 1.6714829206466675 + 2.0 * 6.258476734161377
Epoch 180, val loss: 1.687442421913147
Epoch 190, training loss: 14.137109756469727 = 1.6449910402297974 + 2.0 * 6.246059417724609
Epoch 190, val loss: 1.6648094654083252
Epoch 200, training loss: 14.08493423461914 = 1.6150593757629395 + 2.0 * 6.23493766784668
Epoch 200, val loss: 1.6394617557525635
Epoch 210, training loss: 14.029870986938477 = 1.5817055702209473 + 2.0 * 6.224082946777344
Epoch 210, val loss: 1.611666202545166
Epoch 220, training loss: 13.9747314453125 = 1.5451693534851074 + 2.0 * 6.214780807495117
Epoch 220, val loss: 1.5811508893966675
Epoch 230, training loss: 13.915246963500977 = 1.5054821968078613 + 2.0 * 6.204882621765137
Epoch 230, val loss: 1.5487871170043945
Epoch 240, training loss: 13.858297348022461 = 1.4633152484893799 + 2.0 * 6.19749116897583
Epoch 240, val loss: 1.5144826173782349
Epoch 250, training loss: 13.79966926574707 = 1.4191808700561523 + 2.0 * 6.190244197845459
Epoch 250, val loss: 1.479295015335083
Epoch 260, training loss: 13.73635196685791 = 1.374017357826233 + 2.0 * 6.181167125701904
Epoch 260, val loss: 1.443422555923462
Epoch 270, training loss: 13.67661190032959 = 1.327879548072815 + 2.0 * 6.174365997314453
Epoch 270, val loss: 1.4072591066360474
Epoch 280, training loss: 13.624650955200195 = 1.2815144062042236 + 2.0 * 6.171568393707275
Epoch 280, val loss: 1.371170163154602
Epoch 290, training loss: 13.568920135498047 = 1.2363393306732178 + 2.0 * 6.166290283203125
Epoch 290, val loss: 1.3360003232955933
Epoch 300, training loss: 13.50893497467041 = 1.1912004947662354 + 2.0 * 6.158867359161377
Epoch 300, val loss: 1.301445722579956
Epoch 310, training loss: 13.452101707458496 = 1.1462093591690063 + 2.0 * 6.1529459953308105
Epoch 310, val loss: 1.2667980194091797
Epoch 320, training loss: 13.398139953613281 = 1.1008819341659546 + 2.0 * 6.148629188537598
Epoch 320, val loss: 1.2320283651351929
Epoch 330, training loss: 13.354873657226562 = 1.0553613901138306 + 2.0 * 6.149755954742432
Epoch 330, val loss: 1.1969525814056396
Epoch 340, training loss: 13.298213958740234 = 1.010197401046753 + 2.0 * 6.144008159637451
Epoch 340, val loss: 1.1623691320419312
Epoch 350, training loss: 13.239898681640625 = 0.9657235145568848 + 2.0 * 6.137087821960449
Epoch 350, val loss: 1.1281484365463257
Epoch 360, training loss: 13.18828010559082 = 0.9217016100883484 + 2.0 * 6.133289337158203
Epoch 360, val loss: 1.0944573879241943
Epoch 370, training loss: 13.137550354003906 = 0.8782573938369751 + 2.0 * 6.129646301269531
Epoch 370, val loss: 1.0611953735351562
Epoch 380, training loss: 13.10021686553955 = 0.8359106779098511 + 2.0 * 6.132153034210205
Epoch 380, val loss: 1.0286791324615479
Epoch 390, training loss: 13.044781684875488 = 0.7950401306152344 + 2.0 * 6.124870777130127
Epoch 390, val loss: 0.9978871941566467
Epoch 400, training loss: 12.997856140136719 = 0.7562452554702759 + 2.0 * 6.120805263519287
Epoch 400, val loss: 0.9686844348907471
Epoch 410, training loss: 12.953177452087402 = 0.7191786170005798 + 2.0 * 6.116999626159668
Epoch 410, val loss: 0.9413357377052307
Epoch 420, training loss: 12.91421127319336 = 0.6838214993476868 + 2.0 * 6.115194797515869
Epoch 420, val loss: 0.915675163269043
Epoch 430, training loss: 12.881376266479492 = 0.6505005955696106 + 2.0 * 6.115437984466553
Epoch 430, val loss: 0.8918164372444153
Epoch 440, training loss: 12.839677810668945 = 0.6190353631973267 + 2.0 * 6.110321044921875
Epoch 440, val loss: 0.8701658248901367
Epoch 450, training loss: 12.80099105834961 = 0.5892120599746704 + 2.0 * 6.105889320373535
Epoch 450, val loss: 0.8501298427581787
Epoch 460, training loss: 12.767802238464355 = 0.5606371760368347 + 2.0 * 6.103582382202148
Epoch 460, val loss: 0.8317338824272156
Epoch 470, training loss: 12.742070198059082 = 0.5334083437919617 + 2.0 * 6.104331016540527
Epoch 470, val loss: 0.8146555423736572
Epoch 480, training loss: 12.708850860595703 = 0.5074103474617004 + 2.0 * 6.100720405578613
Epoch 480, val loss: 0.7991845011711121
Epoch 490, training loss: 12.675890922546387 = 0.4823739230632782 + 2.0 * 6.0967583656311035
Epoch 490, val loss: 0.7849414348602295
Epoch 500, training loss: 12.649018287658691 = 0.45810946822166443 + 2.0 * 6.095454216003418
Epoch 500, val loss: 0.7718105316162109
Epoch 510, training loss: 12.62148380279541 = 0.4347572922706604 + 2.0 * 6.093363285064697
Epoch 510, val loss: 0.7594882845878601
Epoch 520, training loss: 12.593093872070312 = 0.4121187925338745 + 2.0 * 6.090487480163574
Epoch 520, val loss: 0.7484336495399475
Epoch 530, training loss: 12.568098068237305 = 0.3902629017829895 + 2.0 * 6.0889177322387695
Epoch 530, val loss: 0.7383167743682861
Epoch 540, training loss: 12.542662620544434 = 0.3690805435180664 + 2.0 * 6.086791038513184
Epoch 540, val loss: 0.7289736866950989
Epoch 550, training loss: 12.525179862976074 = 0.34861820936203003 + 2.0 * 6.08828067779541
Epoch 550, val loss: 0.7204775810241699
Epoch 560, training loss: 12.498910903930664 = 0.3289840519428253 + 2.0 * 6.084963321685791
Epoch 560, val loss: 0.7128552198410034
Epoch 570, training loss: 12.47394847869873 = 0.31016209721565247 + 2.0 * 6.081892967224121
Epoch 570, val loss: 0.706234335899353
Epoch 580, training loss: 12.45174789428711 = 0.29216161370277405 + 2.0 * 6.0797929763793945
Epoch 580, val loss: 0.7004921436309814
Epoch 590, training loss: 12.449825286865234 = 0.27504798769950867 + 2.0 * 6.087388515472412
Epoch 590, val loss: 0.6955568194389343
Epoch 600, training loss: 12.412343978881836 = 0.2587865889072418 + 2.0 * 6.076778888702393
Epoch 600, val loss: 0.6916186213493347
Epoch 610, training loss: 12.395590782165527 = 0.24355076253414154 + 2.0 * 6.076020240783691
Epoch 610, val loss: 0.6885268092155457
Epoch 620, training loss: 12.38359260559082 = 0.22925372421741486 + 2.0 * 6.077169418334961
Epoch 620, val loss: 0.6862825751304626
Epoch 630, training loss: 12.36152172088623 = 0.21591444313526154 + 2.0 * 6.072803497314453
Epoch 630, val loss: 0.6848040223121643
Epoch 640, training loss: 12.347599029541016 = 0.20343264937400818 + 2.0 * 6.072082996368408
Epoch 640, val loss: 0.6842004060745239
Epoch 650, training loss: 12.331137657165527 = 0.19185170531272888 + 2.0 * 6.069643020629883
Epoch 650, val loss: 0.6842495203018188
Epoch 660, training loss: 12.318835258483887 = 0.18111668527126312 + 2.0 * 6.068859100341797
Epoch 660, val loss: 0.684885561466217
Epoch 670, training loss: 12.309935569763184 = 0.17112648487091064 + 2.0 * 6.069404602050781
Epoch 670, val loss: 0.686271607875824
Epoch 680, training loss: 12.294219017028809 = 0.16188453137874603 + 2.0 * 6.06616735458374
Epoch 680, val loss: 0.6881636381149292
Epoch 690, training loss: 12.28083610534668 = 0.15330640971660614 + 2.0 * 6.063765048980713
Epoch 690, val loss: 0.6905957460403442
Epoch 700, training loss: 12.271535873413086 = 0.14531806111335754 + 2.0 * 6.063108921051025
Epoch 700, val loss: 0.6935257911682129
Epoch 710, training loss: 12.265806198120117 = 0.13789743185043335 + 2.0 * 6.0639543533325195
Epoch 710, val loss: 0.6967580318450928
Epoch 720, training loss: 12.254636764526367 = 0.13105791807174683 + 2.0 * 6.061789512634277
Epoch 720, val loss: 0.7003973126411438
Epoch 730, training loss: 12.243091583251953 = 0.12471359223127365 + 2.0 * 6.0591888427734375
Epoch 730, val loss: 0.7043742537498474
Epoch 740, training loss: 12.232935905456543 = 0.11878082901239395 + 2.0 * 6.057077407836914
Epoch 740, val loss: 0.7088192701339722
Epoch 750, training loss: 12.234862327575684 = 0.11323370039463043 + 2.0 * 6.060814380645752
Epoch 750, val loss: 0.7135015726089478
Epoch 760, training loss: 12.225866317749023 = 0.10800399631261826 + 2.0 * 6.058931350708008
Epoch 760, val loss: 0.7184587121009827
Epoch 770, training loss: 12.210354804992676 = 0.10316815972328186 + 2.0 * 6.053593158721924
Epoch 770, val loss: 0.7235043048858643
Epoch 780, training loss: 12.21281623840332 = 0.0986170694231987 + 2.0 * 6.05709981918335
Epoch 780, val loss: 0.7289382815361023
Epoch 790, training loss: 12.205740928649902 = 0.0943465307354927 + 2.0 * 6.055696964263916
Epoch 790, val loss: 0.7344793081283569
Epoch 800, training loss: 12.191690444946289 = 0.0903465673327446 + 2.0 * 6.0506720542907715
Epoch 800, val loss: 0.7401346564292908
Epoch 810, training loss: 12.190316200256348 = 0.08658382296562195 + 2.0 * 6.051866054534912
Epoch 810, val loss: 0.7460092902183533
Epoch 820, training loss: 12.179244995117188 = 0.08302821218967438 + 2.0 * 6.0481085777282715
Epoch 820, val loss: 0.7519193291664124
Epoch 830, training loss: 12.175113677978516 = 0.07967673987150192 + 2.0 * 6.047718524932861
Epoch 830, val loss: 0.7579506039619446
Epoch 840, training loss: 12.175068855285645 = 0.07651708275079727 + 2.0 * 6.049275875091553
Epoch 840, val loss: 0.7641729116439819
Epoch 850, training loss: 12.167750358581543 = 0.07351230829954147 + 2.0 * 6.047119140625
Epoch 850, val loss: 0.7704460024833679
Epoch 860, training loss: 12.161844253540039 = 0.07069434970617294 + 2.0 * 6.045575141906738
Epoch 860, val loss: 0.7767139077186584
Epoch 870, training loss: 12.154317855834961 = 0.06801944226026535 + 2.0 * 6.043148994445801
Epoch 870, val loss: 0.7830885052680969
Epoch 880, training loss: 12.15343189239502 = 0.06547698378562927 + 2.0 * 6.0439772605896
Epoch 880, val loss: 0.7895897626876831
Epoch 890, training loss: 12.147684097290039 = 0.06307312101125717 + 2.0 * 6.0423054695129395
Epoch 890, val loss: 0.7961447238922119
Epoch 900, training loss: 12.145197868347168 = 0.060801710933446884 + 2.0 * 6.042198181152344
Epoch 900, val loss: 0.8025847673416138
Epoch 910, training loss: 12.138284683227539 = 0.058642271906137466 + 2.0 * 6.039821147918701
Epoch 910, val loss: 0.8090623617172241
Epoch 920, training loss: 12.158452033996582 = 0.056581661105155945 + 2.0 * 6.0509352684021
Epoch 920, val loss: 0.8156489133834839
Epoch 930, training loss: 12.136297225952148 = 0.05464085191488266 + 2.0 * 6.040828227996826
Epoch 930, val loss: 0.8221144676208496
Epoch 940, training loss: 12.130266189575195 = 0.05279478430747986 + 2.0 * 6.038735866546631
Epoch 940, val loss: 0.8284491896629333
Epoch 950, training loss: 12.123879432678223 = 0.05103085935115814 + 2.0 * 6.036424160003662
Epoch 950, val loss: 0.8349349498748779
Epoch 960, training loss: 12.12247085571289 = 0.04934665933251381 + 2.0 * 6.036561965942383
Epoch 960, val loss: 0.8414757251739502
Epoch 970, training loss: 12.121946334838867 = 0.0477418377995491 + 2.0 * 6.037102222442627
Epoch 970, val loss: 0.8479676842689514
Epoch 980, training loss: 12.12302017211914 = 0.04621177539229393 + 2.0 * 6.0384039878845215
Epoch 980, val loss: 0.854331374168396
Epoch 990, training loss: 12.111943244934082 = 0.044746700674295425 + 2.0 * 6.03359842300415
Epoch 990, val loss: 0.8606383800506592
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7889
Overall ASR: 0.9889
Flip ASR: 0.9867/225 nodes
The final ASR:0.54367, 0.33018, Accuracy:0.79012, 0.02572
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11596])
remove edge: torch.Size([2, 9520])
updated graph: torch.Size([2, 10560])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9926
Flip ASR: 0.9911/225 nodes
The final ASR:0.98401, 0.00627, Accuracy:0.83333, 0.00800
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.4 der2:0.4 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 18.6805419921875 = 1.9328206777572632 + 2.0 * 8.373860359191895
Epoch 0, val loss: 1.924615740776062
Epoch 10, training loss: 18.67032814025879 = 1.9236524105072021 + 2.0 * 8.373337745666504
Epoch 10, val loss: 1.9163786172866821
Epoch 20, training loss: 18.651399612426758 = 1.912410020828247 + 2.0 * 8.369494438171387
Epoch 20, val loss: 1.9059371948242188
Epoch 30, training loss: 18.577667236328125 = 1.897432804107666 + 2.0 * 8.340117454528809
Epoch 30, val loss: 1.8918066024780273
Epoch 40, training loss: 18.122814178466797 = 1.8800684213638306 + 2.0 * 8.121373176574707
Epoch 40, val loss: 1.8758189678192139
Epoch 50, training loss: 17.23938751220703 = 1.85955810546875 + 2.0 * 7.689915180206299
Epoch 50, val loss: 1.8571830987930298
Epoch 60, training loss: 16.637990951538086 = 1.8423936367034912 + 2.0 * 7.397799015045166
Epoch 60, val loss: 1.8424609899520874
Epoch 70, training loss: 15.878863334655762 = 1.8313199281692505 + 2.0 * 7.0237717628479
Epoch 70, val loss: 1.8334966897964478
Epoch 80, training loss: 15.398392677307129 = 1.8234659433364868 + 2.0 * 6.787463188171387
Epoch 80, val loss: 1.826497197151184
Epoch 90, training loss: 15.06495189666748 = 1.8122899532318115 + 2.0 * 6.626330852508545
Epoch 90, val loss: 1.8167073726654053
Epoch 100, training loss: 14.856599807739258 = 1.8008934259414673 + 2.0 * 6.527853012084961
Epoch 100, val loss: 1.806908130645752
Epoch 110, training loss: 14.711355209350586 = 1.790518879890442 + 2.0 * 6.460418224334717
Epoch 110, val loss: 1.7978520393371582
Epoch 120, training loss: 14.600175857543945 = 1.780484914779663 + 2.0 * 6.409845352172852
Epoch 120, val loss: 1.7889899015426636
Epoch 130, training loss: 14.50654411315918 = 1.7697577476501465 + 2.0 * 6.3683929443359375
Epoch 130, val loss: 1.7795528173446655
Epoch 140, training loss: 14.42612361907959 = 1.7584823369979858 + 2.0 * 6.333820819854736
Epoch 140, val loss: 1.7698200941085815
Epoch 150, training loss: 14.359912872314453 = 1.7462687492370605 + 2.0 * 6.306822299957275
Epoch 150, val loss: 1.7594579458236694
Epoch 160, training loss: 14.298398971557617 = 1.7325851917266846 + 2.0 * 6.282907009124756
Epoch 160, val loss: 1.7478405237197876
Epoch 170, training loss: 14.2410888671875 = 1.7171226739883423 + 2.0 * 6.2619829177856445
Epoch 170, val loss: 1.7348072528839111
Epoch 180, training loss: 14.189836502075195 = 1.6996285915374756 + 2.0 * 6.24510383605957
Epoch 180, val loss: 1.7201257944107056
Epoch 190, training loss: 14.141148567199707 = 1.6796598434448242 + 2.0 * 6.230744361877441
Epoch 190, val loss: 1.7035164833068848
Epoch 200, training loss: 14.09019947052002 = 1.6569926738739014 + 2.0 * 6.2166032791137695
Epoch 200, val loss: 1.6847224235534668
Epoch 210, training loss: 14.045761108398438 = 1.631110429763794 + 2.0 * 6.207325458526611
Epoch 210, val loss: 1.663550615310669
Epoch 220, training loss: 13.991847038269043 = 1.6021136045455933 + 2.0 * 6.19486665725708
Epoch 220, val loss: 1.6396050453186035
Epoch 230, training loss: 13.940665245056152 = 1.569665789604187 + 2.0 * 6.185499668121338
Epoch 230, val loss: 1.6128649711608887
Epoch 240, training loss: 13.888254165649414 = 1.5335590839385986 + 2.0 * 6.177347660064697
Epoch 240, val loss: 1.583102822303772
Epoch 250, training loss: 13.83397388458252 = 1.4938058853149414 + 2.0 * 6.170083999633789
Epoch 250, val loss: 1.5503960847854614
Epoch 260, training loss: 13.777421951293945 = 1.4508700370788574 + 2.0 * 6.163275718688965
Epoch 260, val loss: 1.5150933265686035
Epoch 270, training loss: 13.723587036132812 = 1.404934048652649 + 2.0 * 6.159326553344727
Epoch 270, val loss: 1.4774901866912842
Epoch 280, training loss: 13.662226676940918 = 1.3569982051849365 + 2.0 * 6.152614116668701
Epoch 280, val loss: 1.4384509325027466
Epoch 290, training loss: 13.599708557128906 = 1.30751371383667 + 2.0 * 6.146097660064697
Epoch 290, val loss: 1.3981174230575562
Epoch 300, training loss: 13.551369667053223 = 1.2571191787719727 + 2.0 * 6.147125244140625
Epoch 300, val loss: 1.3575797080993652
Epoch 310, training loss: 13.482426643371582 = 1.207607626914978 + 2.0 * 6.137409687042236
Epoch 310, val loss: 1.3180370330810547
Epoch 320, training loss: 13.423551559448242 = 1.1592134237289429 + 2.0 * 6.132169246673584
Epoch 320, val loss: 1.2799761295318604
Epoch 330, training loss: 13.377819061279297 = 1.1123274564743042 + 2.0 * 6.132745742797852
Epoch 330, val loss: 1.243640422821045
Epoch 340, training loss: 13.314515113830566 = 1.067541480064392 + 2.0 * 6.1234869956970215
Epoch 340, val loss: 1.2098376750946045
Epoch 350, training loss: 13.263740539550781 = 1.0253627300262451 + 2.0 * 6.1191887855529785
Epoch 350, val loss: 1.1784582138061523
Epoch 360, training loss: 13.216123580932617 = 0.9853243827819824 + 2.0 * 6.115399360656738
Epoch 360, val loss: 1.1493977308273315
Epoch 370, training loss: 13.196503639221191 = 0.9474368691444397 + 2.0 * 6.124533176422119
Epoch 370, val loss: 1.122679591178894
Epoch 380, training loss: 13.134685516357422 = 0.9122998118400574 + 2.0 * 6.11119270324707
Epoch 380, val loss: 1.0982861518859863
Epoch 390, training loss: 13.092586517333984 = 0.8794826865196228 + 2.0 * 6.1065521240234375
Epoch 390, val loss: 1.0761592388153076
Epoch 400, training loss: 13.053787231445312 = 0.8483495116233826 + 2.0 * 6.102718830108643
Epoch 400, val loss: 1.0556936264038086
Epoch 410, training loss: 13.01672077178955 = 0.818623960018158 + 2.0 * 6.099048614501953
Epoch 410, val loss: 1.0364134311676025
Epoch 420, training loss: 12.987165451049805 = 0.7900970578193665 + 2.0 * 6.098534107208252
Epoch 420, val loss: 1.0181323289871216
Epoch 430, training loss: 12.959373474121094 = 0.7626124620437622 + 2.0 * 6.0983805656433105
Epoch 430, val loss: 1.0009273290634155
Epoch 440, training loss: 12.921540260314941 = 0.7362872362136841 + 2.0 * 6.092626571655273
Epoch 440, val loss: 0.9846468567848206
Epoch 450, training loss: 12.888955116271973 = 0.7106297612190247 + 2.0 * 6.089162826538086
Epoch 450, val loss: 0.9689412713050842
Epoch 460, training loss: 12.865409851074219 = 0.6854755282402039 + 2.0 * 6.089967250823975
Epoch 460, val loss: 0.9536129236221313
Epoch 470, training loss: 12.837628364562988 = 0.6607944965362549 + 2.0 * 6.088417053222656
Epoch 470, val loss: 0.9387233257293701
Epoch 480, training loss: 12.802323341369629 = 0.6366986632347107 + 2.0 * 6.082812309265137
Epoch 480, val loss: 0.9242352247238159
Epoch 490, training loss: 12.773659706115723 = 0.6129864454269409 + 2.0 * 6.080336570739746
Epoch 490, val loss: 0.9100995659828186
Epoch 500, training loss: 12.752657890319824 = 0.5896879434585571 + 2.0 * 6.081484794616699
Epoch 500, val loss: 0.8962376713752747
Epoch 510, training loss: 12.729804039001465 = 0.5668445825576782 + 2.0 * 6.081479549407959
Epoch 510, val loss: 0.8828163743019104
Epoch 520, training loss: 12.6964111328125 = 0.5448161959648132 + 2.0 * 6.0757975578308105
Epoch 520, val loss: 0.8699033856391907
Epoch 530, training loss: 12.670920372009277 = 0.5234180688858032 + 2.0 * 6.073750972747803
Epoch 530, val loss: 0.8575834035873413
Epoch 540, training loss: 12.647255897521973 = 0.5025678873062134 + 2.0 * 6.072343826293945
Epoch 540, val loss: 0.8457658290863037
Epoch 550, training loss: 12.626670837402344 = 0.4823480546474457 + 2.0 * 6.0721611976623535
Epoch 550, val loss: 0.8344822525978088
Epoch 560, training loss: 12.60403060913086 = 0.4627433121204376 + 2.0 * 6.070643424987793
Epoch 560, val loss: 0.823862612247467
Epoch 570, training loss: 12.581011772155762 = 0.4439599812030792 + 2.0 * 6.068525791168213
Epoch 570, val loss: 0.8138368725776672
Epoch 580, training loss: 12.557363510131836 = 0.42581838369369507 + 2.0 * 6.065772533416748
Epoch 580, val loss: 0.804505467414856
Epoch 590, training loss: 12.53674030303955 = 0.40825048089027405 + 2.0 * 6.064244747161865
Epoch 590, val loss: 0.7957740426063538
Epoch 600, training loss: 12.53638744354248 = 0.3912816643714905 + 2.0 * 6.072552680969238
Epoch 600, val loss: 0.7876567840576172
Epoch 610, training loss: 12.500798225402832 = 0.3749513328075409 + 2.0 * 6.062923431396484
Epoch 610, val loss: 0.780243456363678
Epoch 620, training loss: 12.48111629486084 = 0.35933107137680054 + 2.0 * 6.060892581939697
Epoch 620, val loss: 0.7735483050346375
Epoch 630, training loss: 12.464940071105957 = 0.3442610204219818 + 2.0 * 6.060339450836182
Epoch 630, val loss: 0.7675767540931702
Epoch 640, training loss: 12.451882362365723 = 0.32971933484077454 + 2.0 * 6.061081409454346
Epoch 640, val loss: 0.7622005343437195
Epoch 650, training loss: 12.431093215942383 = 0.3157285749912262 + 2.0 * 6.057682514190674
Epoch 650, val loss: 0.7574201226234436
Epoch 660, training loss: 12.414586067199707 = 0.30230429768562317 + 2.0 * 6.056140899658203
Epoch 660, val loss: 0.7534327507019043
Epoch 670, training loss: 12.398762702941895 = 0.2892840802669525 + 2.0 * 6.054739475250244
Epoch 670, val loss: 0.7498716115951538
Epoch 680, training loss: 12.39270305633545 = 0.27666181325912476 + 2.0 * 6.05802059173584
Epoch 680, val loss: 0.7469040155410767
Epoch 690, training loss: 12.37930679321289 = 0.2644864022731781 + 2.0 * 6.05741024017334
Epoch 690, val loss: 0.7443365454673767
Epoch 700, training loss: 12.358644485473633 = 0.25266557931900024 + 2.0 * 6.052989482879639
Epoch 700, val loss: 0.7425438761711121
Epoch 710, training loss: 12.341909408569336 = 0.2411574125289917 + 2.0 * 6.050375938415527
Epoch 710, val loss: 0.7412129640579224
Epoch 720, training loss: 12.335370063781738 = 0.22990070283412933 + 2.0 * 6.052734851837158
Epoch 720, val loss: 0.7401826977729797
Epoch 730, training loss: 12.316439628601074 = 0.21896687150001526 + 2.0 * 6.048736572265625
Epoch 730, val loss: 0.7397288084030151
Epoch 740, training loss: 12.30705451965332 = 0.2082950621843338 + 2.0 * 6.049379825592041
Epoch 740, val loss: 0.7395376563072205
Epoch 750, training loss: 12.290578842163086 = 0.19792801141738892 + 2.0 * 6.046325206756592
Epoch 750, val loss: 0.7399406433105469
Epoch 760, training loss: 12.279960632324219 = 0.1879614144563675 + 2.0 * 6.045999526977539
Epoch 760, val loss: 0.7406829595565796
Epoch 770, training loss: 12.272483825683594 = 0.17836278676986694 + 2.0 * 6.047060489654541
Epoch 770, val loss: 0.7418338060379028
Epoch 780, training loss: 12.260839462280273 = 0.16924194991588593 + 2.0 * 6.0457987785339355
Epoch 780, val loss: 0.7432714700698853
Epoch 790, training loss: 12.245935440063477 = 0.16052667796611786 + 2.0 * 6.0427045822143555
Epoch 790, val loss: 0.7452365756034851
Epoch 800, training loss: 12.235822677612305 = 0.15228283405303955 + 2.0 * 6.041769981384277
Epoch 800, val loss: 0.7474170327186584
Epoch 810, training loss: 12.226338386535645 = 0.14447084069252014 + 2.0 * 6.040933609008789
Epoch 810, val loss: 0.7498471736907959
Epoch 820, training loss: 12.23408317565918 = 0.1370902806520462 + 2.0 * 6.048496246337891
Epoch 820, val loss: 0.752379298210144
Epoch 830, training loss: 12.214799880981445 = 0.1302001178264618 + 2.0 * 6.042299747467041
Epoch 830, val loss: 0.7551974654197693
Epoch 840, training loss: 12.200882911682129 = 0.12372752279043198 + 2.0 * 6.038577556610107
Epoch 840, val loss: 0.7583711743354797
Epoch 850, training loss: 12.193380355834961 = 0.11765716969966888 + 2.0 * 6.0378618240356445
Epoch 850, val loss: 0.7615156173706055
Epoch 860, training loss: 12.192316055297852 = 0.11193712055683136 + 2.0 * 6.040189266204834
Epoch 860, val loss: 0.7647982239723206
Epoch 870, training loss: 12.179753303527832 = 0.10655444115400314 + 2.0 * 6.036599636077881
Epoch 870, val loss: 0.7679579854011536
Epoch 880, training loss: 12.175543785095215 = 0.10152897238731384 + 2.0 * 6.0370073318481445
Epoch 880, val loss: 0.7716068029403687
Epoch 890, training loss: 12.168744087219238 = 0.09681200981140137 + 2.0 * 6.035965919494629
Epoch 890, val loss: 0.7750142216682434
Epoch 900, training loss: 12.163387298583984 = 0.09239998459815979 + 2.0 * 6.035493850708008
Epoch 900, val loss: 0.7787045240402222
Epoch 910, training loss: 12.15551471710205 = 0.08822337538003922 + 2.0 * 6.0336456298828125
Epoch 910, val loss: 0.7824399471282959
Epoch 920, training loss: 12.155731201171875 = 0.08432711660861969 + 2.0 * 6.035702228546143
Epoch 920, val loss: 0.7860905528068542
Epoch 930, training loss: 12.144367218017578 = 0.08066971600055695 + 2.0 * 6.031848907470703
Epoch 930, val loss: 0.7899713516235352
Epoch 940, training loss: 12.139376640319824 = 0.0772300586104393 + 2.0 * 6.031073093414307
Epoch 940, val loss: 0.7938861846923828
Epoch 950, training loss: 12.133484840393066 = 0.07397500425577164 + 2.0 * 6.029755115509033
Epoch 950, val loss: 0.797825276851654
Epoch 960, training loss: 12.14656925201416 = 0.07090934365987778 + 2.0 * 6.037829875946045
Epoch 960, val loss: 0.8016646504402161
Epoch 970, training loss: 12.134132385253906 = 0.06803471595048904 + 2.0 * 6.033048629760742
Epoch 970, val loss: 0.8055152893066406
Epoch 980, training loss: 12.12389087677002 = 0.06532742083072662 + 2.0 * 6.0292816162109375
Epoch 980, val loss: 0.8097940683364868
Epoch 990, training loss: 12.116560935974121 = 0.06278226524591446 + 2.0 * 6.026889324188232
Epoch 990, val loss: 0.8138892650604248
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7963
Overall ASR: 0.8118
Flip ASR: 0.7733/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.7047176361084 = 1.9570518732070923 + 2.0 * 8.373832702636719
Epoch 0, val loss: 1.9590598344802856
Epoch 10, training loss: 18.692874908447266 = 1.9464726448059082 + 2.0 * 8.373201370239258
Epoch 10, val loss: 1.9485986232757568
Epoch 20, training loss: 18.672147750854492 = 1.9335483312606812 + 2.0 * 8.36929988861084
Epoch 20, val loss: 1.9355841875076294
Epoch 30, training loss: 18.6036434173584 = 1.9165191650390625 + 2.0 * 8.343562126159668
Epoch 30, val loss: 1.9184550046920776
Epoch 40, training loss: 18.17955207824707 = 1.896090030670166 + 2.0 * 8.141731262207031
Epoch 40, val loss: 1.8980075120925903
Epoch 50, training loss: 16.480314254760742 = 1.8725780248641968 + 2.0 * 7.303868293762207
Epoch 50, val loss: 1.873837947845459
Epoch 60, training loss: 16.103775024414062 = 1.8530139923095703 + 2.0 * 7.125380039215088
Epoch 60, val loss: 1.855305790901184
Epoch 70, training loss: 15.649410247802734 = 1.8382235765457153 + 2.0 * 6.905593395233154
Epoch 70, val loss: 1.8407166004180908
Epoch 80, training loss: 15.303192138671875 = 1.8222978115081787 + 2.0 * 6.740447044372559
Epoch 80, val loss: 1.8252636194229126
Epoch 90, training loss: 15.0133695602417 = 1.809227705001831 + 2.0 * 6.6020708084106445
Epoch 90, val loss: 1.8121124505996704
Epoch 100, training loss: 14.828203201293945 = 1.7961136102676392 + 2.0 * 6.516044616699219
Epoch 100, val loss: 1.7990862131118774
Epoch 110, training loss: 14.694573402404785 = 1.7812869548797607 + 2.0 * 6.456643104553223
Epoch 110, val loss: 1.7847384214401245
Epoch 120, training loss: 14.592467308044434 = 1.7660925388336182 + 2.0 * 6.413187503814697
Epoch 120, val loss: 1.7698599100112915
Epoch 130, training loss: 14.509843826293945 = 1.750564694404602 + 2.0 * 6.379639625549316
Epoch 130, val loss: 1.754492163658142
Epoch 140, training loss: 14.439163208007812 = 1.733781337738037 + 2.0 * 6.352690696716309
Epoch 140, val loss: 1.738429307937622
Epoch 150, training loss: 14.369264602661133 = 1.715705394744873 + 2.0 * 6.326779365539551
Epoch 150, val loss: 1.7216373682022095
Epoch 160, training loss: 14.30250358581543 = 1.6959168910980225 + 2.0 * 6.303293228149414
Epoch 160, val loss: 1.7037431001663208
Epoch 170, training loss: 14.239611625671387 = 1.673620343208313 + 2.0 * 6.282995700836182
Epoch 170, val loss: 1.684293508529663
Epoch 180, training loss: 14.176700592041016 = 1.6486161947250366 + 2.0 * 6.264042377471924
Epoch 180, val loss: 1.6627819538116455
Epoch 190, training loss: 14.116921424865723 = 1.6201061010360718 + 2.0 * 6.24840784072876
Epoch 190, val loss: 1.6387156248092651
Epoch 200, training loss: 14.059974670410156 = 1.5878852605819702 + 2.0 * 6.236044883728027
Epoch 200, val loss: 1.612062931060791
Epoch 210, training loss: 13.997566223144531 = 1.5521316528320312 + 2.0 * 6.22271728515625
Epoch 210, val loss: 1.5827319622039795
Epoch 220, training loss: 13.934603691101074 = 1.5123814344406128 + 2.0 * 6.211111068725586
Epoch 220, val loss: 1.5504783391952515
Epoch 230, training loss: 13.87321662902832 = 1.4684884548187256 + 2.0 * 6.202363967895508
Epoch 230, val loss: 1.5154234170913696
Epoch 240, training loss: 13.806288719177246 = 1.4214879274368286 + 2.0 * 6.1924004554748535
Epoch 240, val loss: 1.4784154891967773
Epoch 250, training loss: 13.741011619567871 = 1.3718311786651611 + 2.0 * 6.1845903396606445
Epoch 250, val loss: 1.4397213459014893
Epoch 260, training loss: 13.679725646972656 = 1.3204741477966309 + 2.0 * 6.179625511169434
Epoch 260, val loss: 1.3998935222625732
Epoch 270, training loss: 13.611374855041504 = 1.2689193487167358 + 2.0 * 6.171227931976318
Epoch 270, val loss: 1.3605515956878662
Epoch 280, training loss: 13.549980163574219 = 1.218587875366211 + 2.0 * 6.165696144104004
Epoch 280, val loss: 1.3224177360534668
Epoch 290, training loss: 13.493608474731445 = 1.170590877532959 + 2.0 * 6.161509037017822
Epoch 290, val loss: 1.2864444255828857
Epoch 300, training loss: 13.436141967773438 = 1.1258397102355957 + 2.0 * 6.1551513671875
Epoch 300, val loss: 1.2530637979507446
Epoch 310, training loss: 13.38424301147461 = 1.0841211080551147 + 2.0 * 6.150061130523682
Epoch 310, val loss: 1.222277045249939
Epoch 320, training loss: 13.342416763305664 = 1.0454926490783691 + 2.0 * 6.148461818695068
Epoch 320, val loss: 1.1940598487854004
Epoch 330, training loss: 13.294387817382812 = 1.0098669528961182 + 2.0 * 6.142260551452637
Epoch 330, val loss: 1.1684671640396118
Epoch 340, training loss: 13.251205444335938 = 0.9766724109649658 + 2.0 * 6.137266635894775
Epoch 340, val loss: 1.1450040340423584
Epoch 350, training loss: 13.210721015930176 = 0.9452770352363586 + 2.0 * 6.132721900939941
Epoch 350, val loss: 1.1231075525283813
Epoch 360, training loss: 13.185423851013184 = 0.9150902628898621 + 2.0 * 6.135166645050049
Epoch 360, val loss: 1.1023972034454346
Epoch 370, training loss: 13.140750885009766 = 0.8860824704170227 + 2.0 * 6.127334117889404
Epoch 370, val loss: 1.0827142000198364
Epoch 380, training loss: 13.102971076965332 = 0.8575973510742188 + 2.0 * 6.122686862945557
Epoch 380, val loss: 1.0636705160140991
Epoch 390, training loss: 13.072233200073242 = 0.8290913701057434 + 2.0 * 6.121571063995361
Epoch 390, val loss: 1.044919729232788
Epoch 400, training loss: 13.038498878479004 = 0.8003880977630615 + 2.0 * 6.119055271148682
Epoch 400, val loss: 1.026273488998413
Epoch 410, training loss: 13.00037670135498 = 0.7714380621910095 + 2.0 * 6.114469528198242
Epoch 410, val loss: 1.007596731185913
Epoch 420, training loss: 12.96889877319336 = 0.7421532869338989 + 2.0 * 6.113372802734375
Epoch 420, val loss: 0.9889052510261536
Epoch 430, training loss: 12.931990623474121 = 0.7124985456466675 + 2.0 * 6.109745979309082
Epoch 430, val loss: 0.970280647277832
Epoch 440, training loss: 12.89671802520752 = 0.682683527469635 + 2.0 * 6.1070170402526855
Epoch 440, val loss: 0.9517139792442322
Epoch 450, training loss: 12.870817184448242 = 0.6527681350708008 + 2.0 * 6.109024524688721
Epoch 450, val loss: 0.933319628238678
Epoch 460, training loss: 12.83076000213623 = 0.6232427954673767 + 2.0 * 6.103758811950684
Epoch 460, val loss: 0.915524423122406
Epoch 470, training loss: 12.796411514282227 = 0.594149112701416 + 2.0 * 6.101130962371826
Epoch 470, val loss: 0.8984531760215759
Epoch 480, training loss: 12.764315605163574 = 0.5656914114952087 + 2.0 * 6.0993123054504395
Epoch 480, val loss: 0.8821054697036743
Epoch 490, training loss: 12.731842041015625 = 0.5380296111106873 + 2.0 * 6.0969061851501465
Epoch 490, val loss: 0.8667072057723999
Epoch 500, training loss: 12.705347061157227 = 0.5113847255706787 + 2.0 * 6.096981048583984
Epoch 500, val loss: 0.8523325324058533
Epoch 510, training loss: 12.674052238464355 = 0.48592206835746765 + 2.0 * 6.094065189361572
Epoch 510, val loss: 0.8393804430961609
Epoch 520, training loss: 12.643782615661621 = 0.46147865056991577 + 2.0 * 6.091152191162109
Epoch 520, val loss: 0.8276569843292236
Epoch 530, training loss: 12.622929573059082 = 0.43797144293785095 + 2.0 * 6.092479228973389
Epoch 530, val loss: 0.8171019554138184
Epoch 540, training loss: 12.595415115356445 = 0.4155031740665436 + 2.0 * 6.089955806732178
Epoch 540, val loss: 0.8077857494354248
Epoch 550, training loss: 12.565607070922852 = 0.39415040612220764 + 2.0 * 6.085728168487549
Epoch 550, val loss: 0.7999224662780762
Epoch 560, training loss: 12.540351867675781 = 0.37364211678504944 + 2.0 * 6.083354949951172
Epoch 560, val loss: 0.7931610345840454
Epoch 570, training loss: 12.51904296875 = 0.3539067506790161 + 2.0 * 6.082568168640137
Epoch 570, val loss: 0.7875628471374512
Epoch 580, training loss: 12.498970031738281 = 0.3351319134235382 + 2.0 * 6.081919193267822
Epoch 580, val loss: 0.7829259634017944
Epoch 590, training loss: 12.474048614501953 = 0.3173424005508423 + 2.0 * 6.078352928161621
Epoch 590, val loss: 0.7796720266342163
Epoch 600, training loss: 12.454025268554688 = 0.3004472851753235 + 2.0 * 6.076788902282715
Epoch 600, val loss: 0.7774026393890381
Epoch 610, training loss: 12.453347206115723 = 0.28446611762046814 + 2.0 * 6.0844407081604
Epoch 610, val loss: 0.7760513424873352
Epoch 620, training loss: 12.421517372131348 = 0.26940199732780457 + 2.0 * 6.0760579109191895
Epoch 620, val loss: 0.7757410407066345
Epoch 630, training loss: 12.401679039001465 = 0.2552737295627594 + 2.0 * 6.073202610015869
Epoch 630, val loss: 0.7764527201652527
Epoch 640, training loss: 12.386504173278809 = 0.24199707806110382 + 2.0 * 6.072253704071045
Epoch 640, val loss: 0.7779871821403503
Epoch 650, training loss: 12.369093894958496 = 0.2295684516429901 + 2.0 * 6.069762706756592
Epoch 650, val loss: 0.7802971005439758
Epoch 660, training loss: 12.355814933776855 = 0.21791669726371765 + 2.0 * 6.068949222564697
Epoch 660, val loss: 0.7834401726722717
Epoch 670, training loss: 12.340780258178711 = 0.20699749886989594 + 2.0 * 6.066891193389893
Epoch 670, val loss: 0.7872419953346252
Epoch 680, training loss: 12.348637580871582 = 0.19678132236003876 + 2.0 * 6.075928211212158
Epoch 680, val loss: 0.7916520833969116
Epoch 690, training loss: 12.316332817077637 = 0.18731960654258728 + 2.0 * 6.064506530761719
Epoch 690, val loss: 0.7965816259384155
Epoch 700, training loss: 12.305112838745117 = 0.17848385870456696 + 2.0 * 6.063314437866211
Epoch 700, val loss: 0.8021916151046753
Epoch 710, training loss: 12.294142723083496 = 0.1701686680316925 + 2.0 * 6.061986923217773
Epoch 710, val loss: 0.8081194758415222
Epoch 720, training loss: 12.29810619354248 = 0.16233229637145996 + 2.0 * 6.067886829376221
Epoch 720, val loss: 0.8144148588180542
Epoch 730, training loss: 12.274683952331543 = 0.15505391359329224 + 2.0 * 6.059814929962158
Epoch 730, val loss: 0.8210967779159546
Epoch 740, training loss: 12.265129089355469 = 0.14820493757724762 + 2.0 * 6.058462142944336
Epoch 740, val loss: 0.8282000422477722
Epoch 750, training loss: 12.256342887878418 = 0.14174103736877441 + 2.0 * 6.057301044464111
Epoch 750, val loss: 0.8355368971824646
Epoch 760, training loss: 12.259568214416504 = 0.13565553724765778 + 2.0 * 6.061956405639648
Epoch 760, val loss: 0.8430548310279846
Epoch 770, training loss: 12.24454116821289 = 0.12992578744888306 + 2.0 * 6.057307720184326
Epoch 770, val loss: 0.850908637046814
Epoch 780, training loss: 12.232552528381348 = 0.12453813850879669 + 2.0 * 6.054007053375244
Epoch 780, val loss: 0.8589370250701904
Epoch 790, training loss: 12.226673126220703 = 0.11943698674440384 + 2.0 * 6.05361795425415
Epoch 790, val loss: 0.8670790791511536
Epoch 800, training loss: 12.218832015991211 = 0.11462832242250443 + 2.0 * 6.0521016120910645
Epoch 800, val loss: 0.875273585319519
Epoch 810, training loss: 12.21547794342041 = 0.11012052744626999 + 2.0 * 6.05267858505249
Epoch 810, val loss: 0.8837562799453735
Epoch 820, training loss: 12.205560684204102 = 0.10585758090019226 + 2.0 * 6.049851417541504
Epoch 820, val loss: 0.8922942280769348
Epoch 830, training loss: 12.21094036102295 = 0.10180523246526718 + 2.0 * 6.054567337036133
Epoch 830, val loss: 0.9009295701980591
Epoch 840, training loss: 12.198748588562012 = 0.09799612313508987 + 2.0 * 6.0503764152526855
Epoch 840, val loss: 0.9096014499664307
Epoch 850, training loss: 12.194498062133789 = 0.0943448394536972 + 2.0 * 6.050076484680176
Epoch 850, val loss: 0.9184612035751343
Epoch 860, training loss: 12.191911697387695 = 0.09088706225156784 + 2.0 * 6.050512313842773
Epoch 860, val loss: 0.9272599816322327
Epoch 870, training loss: 12.180673599243164 = 0.08761755377054214 + 2.0 * 6.046527862548828
Epoch 870, val loss: 0.9361307621002197
Epoch 880, training loss: 12.173298835754395 = 0.08447936177253723 + 2.0 * 6.04440975189209
Epoch 880, val loss: 0.9451072812080383
Epoch 890, training loss: 12.16850757598877 = 0.0814945325255394 + 2.0 * 6.043506622314453
Epoch 890, val loss: 0.954116940498352
Epoch 900, training loss: 12.16496753692627 = 0.0786295160651207 + 2.0 * 6.043169021606445
Epoch 900, val loss: 0.9630985856056213
Epoch 910, training loss: 12.173672676086426 = 0.07589669525623322 + 2.0 * 6.048888206481934
Epoch 910, val loss: 0.9719298481941223
Epoch 920, training loss: 12.157825469970703 = 0.07328464835882187 + 2.0 * 6.042270183563232
Epoch 920, val loss: 0.9808591604232788
Epoch 930, training loss: 12.152192115783691 = 0.07078845053911209 + 2.0 * 6.040701866149902
Epoch 930, val loss: 0.9898511171340942
Epoch 940, training loss: 12.147847175598145 = 0.06838614493608475 + 2.0 * 6.039730548858643
Epoch 940, val loss: 0.9986932873725891
Epoch 950, training loss: 12.151297569274902 = 0.06607624888420105 + 2.0 * 6.0426106452941895
Epoch 950, val loss: 1.007506251335144
Epoch 960, training loss: 12.144775390625 = 0.06386441737413406 + 2.0 * 6.040455341339111
Epoch 960, val loss: 1.0163578987121582
Epoch 970, training loss: 12.1383638381958 = 0.06172174587845802 + 2.0 * 6.038321018218994
Epoch 970, val loss: 1.0252134799957275
Epoch 980, training loss: 12.141093254089355 = 0.05967428907752037 + 2.0 * 6.040709495544434
Epoch 980, val loss: 1.0339629650115967
Epoch 990, training loss: 12.13129997253418 = 0.05769634246826172 + 2.0 * 6.036801815032959
Epoch 990, val loss: 1.0427629947662354
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7852
Overall ASR: 0.7085
Flip ASR: 0.6622/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.71814727783203 = 1.9703716039657593 + 2.0 * 8.37388801574707
Epoch 0, val loss: 1.980131983757019
Epoch 10, training loss: 18.706939697265625 = 1.9599330425262451 + 2.0 * 8.373503684997559
Epoch 10, val loss: 1.9693623781204224
Epoch 20, training loss: 18.689115524291992 = 1.9470617771148682 + 2.0 * 8.371026992797852
Epoch 20, val loss: 1.955933928489685
Epoch 30, training loss: 18.636154174804688 = 1.9292500019073486 + 2.0 * 8.3534517288208
Epoch 30, val loss: 1.9372649192810059
Epoch 40, training loss: 18.329788208007812 = 1.907207727432251 + 2.0 * 8.21129035949707
Epoch 40, val loss: 1.9149954319000244
Epoch 50, training loss: 16.719301223754883 = 1.885551929473877 + 2.0 * 7.416874408721924
Epoch 50, val loss: 1.8935678005218506
Epoch 60, training loss: 15.782735824584961 = 1.8706231117248535 + 2.0 * 6.956056594848633
Epoch 60, val loss: 1.8794468641281128
Epoch 70, training loss: 15.266700744628906 = 1.8587923049926758 + 2.0 * 6.703954219818115
Epoch 70, val loss: 1.8680535554885864
Epoch 80, training loss: 15.012033462524414 = 1.8463118076324463 + 2.0 * 6.582860946655273
Epoch 80, val loss: 1.8562061786651611
Epoch 90, training loss: 14.832620620727539 = 1.8321908712387085 + 2.0 * 6.50021505355835
Epoch 90, val loss: 1.8429012298583984
Epoch 100, training loss: 14.682666778564453 = 1.8179737329483032 + 2.0 * 6.432346343994141
Epoch 100, val loss: 1.8297423124313354
Epoch 110, training loss: 14.573322296142578 = 1.8041677474975586 + 2.0 * 6.38457727432251
Epoch 110, val loss: 1.817111611366272
Epoch 120, training loss: 14.487330436706543 = 1.7907193899154663 + 2.0 * 6.348305702209473
Epoch 120, val loss: 1.8047796487808228
Epoch 130, training loss: 14.418951034545898 = 1.7770969867706299 + 2.0 * 6.320927143096924
Epoch 130, val loss: 1.7923656702041626
Epoch 140, training loss: 14.356165885925293 = 1.7628109455108643 + 2.0 * 6.296677589416504
Epoch 140, val loss: 1.779586672782898
Epoch 150, training loss: 14.298377990722656 = 1.7476098537445068 + 2.0 * 6.275383949279785
Epoch 150, val loss: 1.7661924362182617
Epoch 160, training loss: 14.24655532836914 = 1.7308870553970337 + 2.0 * 6.257833957672119
Epoch 160, val loss: 1.7516685724258423
Epoch 170, training loss: 14.194453239440918 = 1.7121293544769287 + 2.0 * 6.241161823272705
Epoch 170, val loss: 1.7356232404708862
Epoch 180, training loss: 14.143256187438965 = 1.6908838748931885 + 2.0 * 6.226186275482178
Epoch 180, val loss: 1.7177001237869263
Epoch 190, training loss: 14.09603500366211 = 1.6666944026947021 + 2.0 * 6.214670181274414
Epoch 190, val loss: 1.6974512338638306
Epoch 200, training loss: 14.041910171508789 = 1.6392027139663696 + 2.0 * 6.201353549957275
Epoch 200, val loss: 1.6746680736541748
Epoch 210, training loss: 13.9907808303833 = 1.6080232858657837 + 2.0 * 6.191378593444824
Epoch 210, val loss: 1.649011492729187
Epoch 220, training loss: 13.936766624450684 = 1.5724817514419556 + 2.0 * 6.18214225769043
Epoch 220, val loss: 1.619737982749939
Epoch 230, training loss: 13.89216423034668 = 1.5317548513412476 + 2.0 * 6.18020486831665
Epoch 230, val loss: 1.5865187644958496
Epoch 240, training loss: 13.82213020324707 = 1.4872545003890991 + 2.0 * 6.16743803024292
Epoch 240, val loss: 1.5501205921173096
Epoch 250, training loss: 13.760973930358887 = 1.4383773803710938 + 2.0 * 6.1612982749938965
Epoch 250, val loss: 1.5105421543121338
Epoch 260, training loss: 13.695577621459961 = 1.385113000869751 + 2.0 * 6.1552324295043945
Epoch 260, val loss: 1.4676569700241089
Epoch 270, training loss: 13.631308555603027 = 1.3281618356704712 + 2.0 * 6.151573181152344
Epoch 270, val loss: 1.4223295450210571
Epoch 280, training loss: 13.560470581054688 = 1.2695214748382568 + 2.0 * 6.145474433898926
Epoch 280, val loss: 1.3761653900146484
Epoch 290, training loss: 13.491332054138184 = 1.2094097137451172 + 2.0 * 6.140961170196533
Epoch 290, val loss: 1.329355001449585
Epoch 300, training loss: 13.423809051513672 = 1.1485925912857056 + 2.0 * 6.137608051300049
Epoch 300, val loss: 1.2825632095336914
Epoch 310, training loss: 13.362540245056152 = 1.0889443159103394 + 2.0 * 6.136797904968262
Epoch 310, val loss: 1.2374954223632812
Epoch 320, training loss: 13.290818214416504 = 1.0321766138076782 + 2.0 * 6.1293206214904785
Epoch 320, val loss: 1.1951134204864502
Epoch 330, training loss: 13.232865333557129 = 0.9780813455581665 + 2.0 * 6.127391815185547
Epoch 330, val loss: 1.1556222438812256
Epoch 340, training loss: 13.178081512451172 = 0.9277217388153076 + 2.0 * 6.125179767608643
Epoch 340, val loss: 1.1197257041931152
Epoch 350, training loss: 13.119926452636719 = 0.8809725642204285 + 2.0 * 6.119476795196533
Epoch 350, val loss: 1.0874042510986328
Epoch 360, training loss: 13.074954986572266 = 0.8377026319503784 + 2.0 * 6.118626117706299
Epoch 360, val loss: 1.0584115982055664
Epoch 370, training loss: 13.026313781738281 = 0.7979015707969666 + 2.0 * 6.114206314086914
Epoch 370, val loss: 1.0327801704406738
Epoch 380, training loss: 12.983190536499023 = 0.7614655494689941 + 2.0 * 6.1108622550964355
Epoch 380, val loss: 1.01007878780365
Epoch 390, training loss: 12.941909790039062 = 0.7277859449386597 + 2.0 * 6.107061862945557
Epoch 390, val loss: 0.9899566769599915
Epoch 400, training loss: 12.910576820373535 = 0.6964322924613953 + 2.0 * 6.107072353363037
Epoch 400, val loss: 0.972122311592102
Epoch 410, training loss: 12.87143325805664 = 0.6673581600189209 + 2.0 * 6.10203742980957
Epoch 410, val loss: 0.956294059753418
Epoch 420, training loss: 12.84075927734375 = 0.6402135491371155 + 2.0 * 6.1002726554870605
Epoch 420, val loss: 0.942193865776062
Epoch 430, training loss: 12.808351516723633 = 0.6148590445518494 + 2.0 * 6.096746444702148
Epoch 430, val loss: 0.9297240376472473
Epoch 440, training loss: 12.778228759765625 = 0.5908374190330505 + 2.0 * 6.093695640563965
Epoch 440, val loss: 0.9185770750045776
Epoch 450, training loss: 12.750679016113281 = 0.5680156350135803 + 2.0 * 6.091331481933594
Epoch 450, val loss: 0.9084795713424683
Epoch 460, training loss: 12.728287696838379 = 0.5463274717330933 + 2.0 * 6.090980052947998
Epoch 460, val loss: 0.8993512392044067
Epoch 470, training loss: 12.703484535217285 = 0.5258842706680298 + 2.0 * 6.088799953460693
Epoch 470, val loss: 0.8913497924804688
Epoch 480, training loss: 12.679791450500488 = 0.5063518285751343 + 2.0 * 6.086719989776611
Epoch 480, val loss: 0.8842455744743347
Epoch 490, training loss: 12.659924507141113 = 0.4876387119293213 + 2.0 * 6.0861430168151855
Epoch 490, val loss: 0.8779216408729553
Epoch 500, training loss: 12.634281158447266 = 0.4695878326892853 + 2.0 * 6.082346439361572
Epoch 500, val loss: 0.8723496198654175
Epoch 510, training loss: 12.620558738708496 = 0.4523095488548279 + 2.0 * 6.084124565124512
Epoch 510, val loss: 0.8674843311309814
Epoch 520, training loss: 12.59506607055664 = 0.4357542097568512 + 2.0 * 6.07965612411499
Epoch 520, val loss: 0.8632696270942688
Epoch 530, training loss: 12.571335792541504 = 0.41977939009666443 + 2.0 * 6.075778007507324
Epoch 530, val loss: 0.8596839904785156
Epoch 540, training loss: 12.56694221496582 = 0.4043278098106384 + 2.0 * 6.081307411193848
Epoch 540, val loss: 0.8565722703933716
Epoch 550, training loss: 12.539739608764648 = 0.3895762860774994 + 2.0 * 6.075081825256348
Epoch 550, val loss: 0.8539963960647583
Epoch 560, training loss: 12.517918586730957 = 0.37531429529190063 + 2.0 * 6.0713019371032715
Epoch 560, val loss: 0.8519474267959595
Epoch 570, training loss: 12.502653121948242 = 0.3615092933177948 + 2.0 * 6.0705718994140625
Epoch 570, val loss: 0.8502609729766846
Epoch 580, training loss: 12.489376068115234 = 0.3481261730194092 + 2.0 * 6.070624828338623
Epoch 580, val loss: 0.8489406108856201
Epoch 590, training loss: 12.470293998718262 = 0.33519354462623596 + 2.0 * 6.067550182342529
Epoch 590, val loss: 0.8480889797210693
Epoch 600, training loss: 12.46963882446289 = 0.3225739598274231 + 2.0 * 6.073532581329346
Epoch 600, val loss: 0.84742671251297
Epoch 610, training loss: 12.440387725830078 = 0.3105300962924957 + 2.0 * 6.064929008483887
Epoch 610, val loss: 0.8471593260765076
Epoch 620, training loss: 12.424071311950684 = 0.29880934953689575 + 2.0 * 6.062631130218506
Epoch 620, val loss: 0.8473465442657471
Epoch 630, training loss: 12.409955024719238 = 0.2874074876308441 + 2.0 * 6.061273574829102
Epoch 630, val loss: 0.8478546738624573
Epoch 640, training loss: 12.399127006530762 = 0.2762909531593323 + 2.0 * 6.061418056488037
Epoch 640, val loss: 0.8486979007720947
Epoch 650, training loss: 12.397945404052734 = 0.2655235826969147 + 2.0 * 6.066210746765137
Epoch 650, val loss: 0.8496655821800232
Epoch 660, training loss: 12.374593734741211 = 0.25517699122428894 + 2.0 * 6.059708595275879
Epoch 660, val loss: 0.851205587387085
Epoch 670, training loss: 12.360162734985352 = 0.24511124193668365 + 2.0 * 6.057525634765625
Epoch 670, val loss: 0.8528861999511719
Epoch 680, training loss: 12.346268653869629 = 0.23532497882843018 + 2.0 * 6.055471897125244
Epoch 680, val loss: 0.8548560738563538
Epoch 690, training loss: 12.335421562194824 = 0.22581440210342407 + 2.0 * 6.054803371429443
Epoch 690, val loss: 0.8570913672447205
Epoch 700, training loss: 12.327225685119629 = 0.21661755442619324 + 2.0 * 6.055304050445557
Epoch 700, val loss: 0.8593940138816833
Epoch 710, training loss: 12.320682525634766 = 0.2077685445547104 + 2.0 * 6.056457042694092
Epoch 710, val loss: 0.8621556162834167
Epoch 720, training loss: 12.306171417236328 = 0.1992451697587967 + 2.0 * 6.053462982177734
Epoch 720, val loss: 0.8650898933410645
Epoch 730, training loss: 12.293439865112305 = 0.19106076657772064 + 2.0 * 6.051189422607422
Epoch 730, val loss: 0.8682861924171448
Epoch 740, training loss: 12.282866477966309 = 0.18317557871341705 + 2.0 * 6.049845218658447
Epoch 740, val loss: 0.8717922568321228
Epoch 750, training loss: 12.274612426757812 = 0.17559340596199036 + 2.0 * 6.049509525299072
Epoch 750, val loss: 0.8755170702934265
Epoch 760, training loss: 12.265693664550781 = 0.16832710802555084 + 2.0 * 6.048683166503906
Epoch 760, val loss: 0.8793767690658569
Epoch 770, training loss: 12.259620666503906 = 0.16140906512737274 + 2.0 * 6.049105644226074
Epoch 770, val loss: 0.8836007118225098
Epoch 780, training loss: 12.24647331237793 = 0.1547924131155014 + 2.0 * 6.045840263366699
Epoch 780, val loss: 0.8881102800369263
Epoch 790, training loss: 12.238651275634766 = 0.14845998585224152 + 2.0 * 6.045095443725586
Epoch 790, val loss: 0.8927947878837585
Epoch 800, training loss: 12.258421897888184 = 0.14241865277290344 + 2.0 * 6.058001518249512
Epoch 800, val loss: 0.8974913954734802
Epoch 810, training loss: 12.225234031677246 = 0.13665971159934998 + 2.0 * 6.044287204742432
Epoch 810, val loss: 0.9023931622505188
Epoch 820, training loss: 12.216775894165039 = 0.13120843470096588 + 2.0 * 6.042783737182617
Epoch 820, val loss: 0.9076862335205078
Epoch 830, training loss: 12.21038818359375 = 0.1260109692811966 + 2.0 * 6.04218864440918
Epoch 830, val loss: 0.9130803942680359
Epoch 840, training loss: 12.221482276916504 = 0.12106958776712418 + 2.0 * 6.050206184387207
Epoch 840, val loss: 0.9186461567878723
Epoch 850, training loss: 12.205723762512207 = 0.11634338647127151 + 2.0 * 6.044690132141113
Epoch 850, val loss: 0.9239142537117004
Epoch 860, training loss: 12.19351863861084 = 0.11188752204179764 + 2.0 * 6.040815353393555
Epoch 860, val loss: 0.9297720193862915
Epoch 870, training loss: 12.18596076965332 = 0.10764865577220917 + 2.0 * 6.039155960083008
Epoch 870, val loss: 0.9356516003608704
Epoch 880, training loss: 12.182598114013672 = 0.10359033942222595 + 2.0 * 6.039504051208496
Epoch 880, val loss: 0.9415251612663269
Epoch 890, training loss: 12.179224014282227 = 0.09972415864467621 + 2.0 * 6.039750099182129
Epoch 890, val loss: 0.947359561920166
Epoch 900, training loss: 12.172544479370117 = 0.09604530781507492 + 2.0 * 6.038249492645264
Epoch 900, val loss: 0.9533961415290833
Epoch 910, training loss: 12.181679725646973 = 0.09252433478832245 + 2.0 * 6.044577598571777
Epoch 910, val loss: 0.9593895673751831
Epoch 920, training loss: 12.167078971862793 = 0.08918847143650055 + 2.0 * 6.038945198059082
Epoch 920, val loss: 0.965502917766571
Epoch 930, training loss: 12.155963897705078 = 0.08600480109453201 + 2.0 * 6.034979343414307
Epoch 930, val loss: 0.9717037677764893
Epoch 940, training loss: 12.151671409606934 = 0.08296840637922287 + 2.0 * 6.034351348876953
Epoch 940, val loss: 0.9780282378196716
Epoch 950, training loss: 12.16135311126709 = 0.08006516844034195 + 2.0 * 6.04064416885376
Epoch 950, val loss: 0.984214723110199
Epoch 960, training loss: 12.148892402648926 = 0.07727697491645813 + 2.0 * 6.0358076095581055
Epoch 960, val loss: 0.9902176260948181
Epoch 970, training loss: 12.146073341369629 = 0.07463692128658295 + 2.0 * 6.0357184410095215
Epoch 970, val loss: 0.9965885877609253
Epoch 980, training loss: 12.136756896972656 = 0.07211600244045258 + 2.0 * 6.032320499420166
Epoch 980, val loss: 1.002894639968872
Epoch 990, training loss: 12.132817268371582 = 0.06969772279262543 + 2.0 * 6.031559944152832
Epoch 990, val loss: 1.009218454360962
=== picking the best model according to the performance on validation ===
target class rate on Vs: 0.8000
accuracy on clean test nodes: 0.7630
Overall ASR: 0.8044
Flip ASR: 0.7689/225 nodes
The final ASR:0.77491, 0.04706, Accuracy:0.78148, 0.01386
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11588])
remove edge: torch.Size([2, 9626])
updated graph: torch.Size([2, 10658])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
The final ASR:0.97417, 0.00000, Accuracy:0.83210, 0.00761
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.4 der2:0.4 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 18.696861267089844 = 1.94901442527771 + 2.0 * 8.373923301696777
Epoch 0, val loss: 1.9544622898101807
Epoch 10, training loss: 18.685409545898438 = 1.938157558441162 + 2.0 * 8.373625755310059
Epoch 10, val loss: 1.9441591501235962
Epoch 20, training loss: 18.667659759521484 = 1.9244142770767212 + 2.0 * 8.371623039245605
Epoch 20, val loss: 1.930543065071106
Epoch 30, training loss: 18.621259689331055 = 1.9049687385559082 + 2.0 * 8.358145713806152
Epoch 30, val loss: 1.910923957824707
Epoch 40, training loss: 18.425273895263672 = 1.8798980712890625 + 2.0 * 8.272687911987305
Epoch 40, val loss: 1.8866088390350342
Epoch 50, training loss: 17.420427322387695 = 1.85368013381958 + 2.0 * 7.7833733558654785
Epoch 50, val loss: 1.8624082803726196
Epoch 60, training loss: 16.348236083984375 = 1.8339756727218628 + 2.0 * 7.257130146026611
Epoch 60, val loss: 1.8458245992660522
Epoch 70, training loss: 15.730500221252441 = 1.823650598526001 + 2.0 * 6.95342493057251
Epoch 70, val loss: 1.836656093597412
Epoch 80, training loss: 15.385249137878418 = 1.8107601404190063 + 2.0 * 6.7872443199157715
Epoch 80, val loss: 1.8245413303375244
Epoch 90, training loss: 15.097822189331055 = 1.7961909770965576 + 2.0 * 6.650815486907959
Epoch 90, val loss: 1.811127781867981
Epoch 100, training loss: 14.928183555603027 = 1.781495451927185 + 2.0 * 6.5733442306518555
Epoch 100, val loss: 1.7978323698043823
Epoch 110, training loss: 14.78794002532959 = 1.7666816711425781 + 2.0 * 6.510629177093506
Epoch 110, val loss: 1.7845516204833984
Epoch 120, training loss: 14.67336654663086 = 1.75132155418396 + 2.0 * 6.46102237701416
Epoch 120, val loss: 1.7710598707199097
Epoch 130, training loss: 14.583307266235352 = 1.734908938407898 + 2.0 * 6.424199104309082
Epoch 130, val loss: 1.7568297386169434
Epoch 140, training loss: 14.500850677490234 = 1.7169990539550781 + 2.0 * 6.391925811767578
Epoch 140, val loss: 1.7417221069335938
Epoch 150, training loss: 14.424970626831055 = 1.6973210573196411 + 2.0 * 6.363824844360352
Epoch 150, val loss: 1.7253661155700684
Epoch 160, training loss: 14.355484962463379 = 1.6754714250564575 + 2.0 * 6.3400068283081055
Epoch 160, val loss: 1.7073266506195068
Epoch 170, training loss: 14.293356895446777 = 1.6511157751083374 + 2.0 * 6.321120738983154
Epoch 170, val loss: 1.6874009370803833
Epoch 180, training loss: 14.226678848266602 = 1.6242953538894653 + 2.0 * 6.301191806793213
Epoch 180, val loss: 1.6655975580215454
Epoch 190, training loss: 14.163880348205566 = 1.5948063135147095 + 2.0 * 6.284536838531494
Epoch 190, val loss: 1.641383171081543
Epoch 200, training loss: 14.101045608520508 = 1.561970829963684 + 2.0 * 6.269537448883057
Epoch 200, val loss: 1.6144638061523438
Epoch 210, training loss: 14.047446250915527 = 1.5262435674667358 + 2.0 * 6.26060152053833
Epoch 210, val loss: 1.5852948427200317
Epoch 220, training loss: 13.97996711730957 = 1.4884655475616455 + 2.0 * 6.245750904083252
Epoch 220, val loss: 1.5544548034667969
Epoch 230, training loss: 13.919377326965332 = 1.4488646984100342 + 2.0 * 6.235256195068359
Epoch 230, val loss: 1.52232027053833
Epoch 240, training loss: 13.859676361083984 = 1.4077582359313965 + 2.0 * 6.225959300994873
Epoch 240, val loss: 1.489342212677002
Epoch 250, training loss: 13.807257652282715 = 1.366134762763977 + 2.0 * 6.220561504364014
Epoch 250, val loss: 1.456644058227539
Epoch 260, training loss: 13.744043350219727 = 1.3250741958618164 + 2.0 * 6.209484577178955
Epoch 260, val loss: 1.424815058708191
Epoch 270, training loss: 13.689441680908203 = 1.2844934463500977 + 2.0 * 6.202474117279053
Epoch 270, val loss: 1.394027590751648
Epoch 280, training loss: 13.654587745666504 = 1.2446937561035156 + 2.0 * 6.204946994781494
Epoch 280, val loss: 1.3642969131469727
Epoch 290, training loss: 13.588868141174316 = 1.2061880826950073 + 2.0 * 6.19133996963501
Epoch 290, val loss: 1.3362318277359009
Epoch 300, training loss: 13.536710739135742 = 1.1689648628234863 + 2.0 * 6.183872699737549
Epoch 300, val loss: 1.3094499111175537
Epoch 310, training loss: 13.487456321716309 = 1.132598638534546 + 2.0 * 6.177428722381592
Epoch 310, val loss: 1.2837239503860474
Epoch 320, training loss: 13.44852066040039 = 1.0967909097671509 + 2.0 * 6.1758646965026855
Epoch 320, val loss: 1.2587692737579346
Epoch 330, training loss: 13.396013259887695 = 1.0620324611663818 + 2.0 * 6.166990280151367
Epoch 330, val loss: 1.2347562313079834
Epoch 340, training loss: 13.35156536102295 = 1.0279170274734497 + 2.0 * 6.1618242263793945
Epoch 340, val loss: 1.2114448547363281
Epoch 350, training loss: 13.308089256286621 = 0.9942716360092163 + 2.0 * 6.156908988952637
Epoch 350, val loss: 1.1886749267578125
Epoch 360, training loss: 13.281646728515625 = 0.9608932137489319 + 2.0 * 6.16037654876709
Epoch 360, val loss: 1.1662037372589111
Epoch 370, training loss: 13.227217674255371 = 0.9278188347816467 + 2.0 * 6.1496992111206055
Epoch 370, val loss: 1.144081950187683
Epoch 380, training loss: 13.184596061706543 = 0.8949607610702515 + 2.0 * 6.14481782913208
Epoch 380, val loss: 1.1222208738327026
Epoch 390, training loss: 13.142263412475586 = 0.8619707226753235 + 2.0 * 6.140146255493164
Epoch 390, val loss: 1.100342869758606
Epoch 400, training loss: 13.111397743225098 = 0.8286566138267517 + 2.0 * 6.14137077331543
Epoch 400, val loss: 1.0783135890960693
Epoch 410, training loss: 13.064489364624023 = 0.7952845692634583 + 2.0 * 6.1346025466918945
Epoch 410, val loss: 1.056107759475708
Epoch 420, training loss: 13.023528099060059 = 0.7616731524467468 + 2.0 * 6.130927562713623
Epoch 420, val loss: 1.0337862968444824
Epoch 430, training loss: 12.984830856323242 = 0.727938711643219 + 2.0 * 6.128446102142334
Epoch 430, val loss: 1.0114415884017944
Epoch 440, training loss: 12.94094467163086 = 0.6944348216056824 + 2.0 * 6.123254776000977
Epoch 440, val loss: 0.9892206192016602
Epoch 450, training loss: 12.904645919799805 = 0.6611955165863037 + 2.0 * 6.121725082397461
Epoch 450, val loss: 0.9674112796783447
Epoch 460, training loss: 12.866496086120605 = 0.6284579038619995 + 2.0 * 6.119019031524658
Epoch 460, val loss: 0.9463511109352112
Epoch 470, training loss: 12.829031944274902 = 0.5965517163276672 + 2.0 * 6.11624002456665
Epoch 470, val loss: 0.9263976216316223
Epoch 480, training loss: 12.793550491333008 = 0.56545090675354 + 2.0 * 6.114049911499023
Epoch 480, val loss: 0.9076809287071228
Epoch 490, training loss: 12.761831283569336 = 0.5354622602462769 + 2.0 * 6.113184452056885
Epoch 490, val loss: 0.8902914524078369
Epoch 500, training loss: 12.72416877746582 = 0.5067528486251831 + 2.0 * 6.108707904815674
Epoch 500, val loss: 0.8745156526565552
Epoch 510, training loss: 12.698341369628906 = 0.4792041778564453 + 2.0 * 6.1095685958862305
Epoch 510, val loss: 0.8605266213417053
Epoch 520, training loss: 12.66187858581543 = 0.4530220627784729 + 2.0 * 6.104428291320801
Epoch 520, val loss: 0.8481999039649963
Epoch 530, training loss: 12.636995315551758 = 0.42816582322120667 + 2.0 * 6.104414939880371
Epoch 530, val loss: 0.8376772999763489
Epoch 540, training loss: 12.607913970947266 = 0.4046543836593628 + 2.0 * 6.101629734039307
Epoch 540, val loss: 0.8288821578025818
Epoch 550, training loss: 12.581204414367676 = 0.3824441432952881 + 2.0 * 6.099380016326904
Epoch 550, val loss: 0.821607232093811
Epoch 560, training loss: 12.555092811584473 = 0.3615887463092804 + 2.0 * 6.096752166748047
Epoch 560, val loss: 0.8158362507820129
Epoch 570, training loss: 12.530375480651855 = 0.3419283330440521 + 2.0 * 6.094223499298096
Epoch 570, val loss: 0.8113866448402405
Epoch 580, training loss: 12.510049819946289 = 0.3233902156352997 + 2.0 * 6.093329906463623
Epoch 580, val loss: 0.8081005811691284
Epoch 590, training loss: 12.492142677307129 = 0.3059973120689392 + 2.0 * 6.093072891235352
Epoch 590, val loss: 0.805975615978241
Epoch 600, training loss: 12.469974517822266 = 0.2896401882171631 + 2.0 * 6.090167045593262
Epoch 600, val loss: 0.8048717379570007
Epoch 610, training loss: 12.459489822387695 = 0.27429553866386414 + 2.0 * 6.092597007751465
Epoch 610, val loss: 0.8046493530273438
Epoch 620, training loss: 12.435306549072266 = 0.2597840130329132 + 2.0 * 6.087761402130127
Epoch 620, val loss: 0.8052940368652344
Epoch 630, training loss: 12.414934158325195 = 0.24617630243301392 + 2.0 * 6.084378719329834
Epoch 630, val loss: 0.8067188858985901
Epoch 640, training loss: 12.402694702148438 = 0.2333000749349594 + 2.0 * 6.084697246551514
Epoch 640, val loss: 0.8089136481285095
Epoch 650, training loss: 12.386722564697266 = 0.2212357521057129 + 2.0 * 6.082743167877197
Epoch 650, val loss: 0.8115848302841187
Epoch 660, training loss: 12.381908416748047 = 0.20988056063652039 + 2.0 * 6.0860137939453125
Epoch 660, val loss: 0.8150064945220947
Epoch 670, training loss: 12.3558988571167 = 0.1991785764694214 + 2.0 * 6.078360080718994
Epoch 670, val loss: 0.8188744187355042
Epoch 680, training loss: 12.342469215393066 = 0.18916848301887512 + 2.0 * 6.076650142669678
Epoch 680, val loss: 0.8233240246772766
Epoch 690, training loss: 12.32955265045166 = 0.1796850562095642 + 2.0 * 6.074934005737305
Epoch 690, val loss: 0.8282615542411804
Epoch 700, training loss: 12.324868202209473 = 0.170753613114357 + 2.0 * 6.077057361602783
Epoch 700, val loss: 0.8336591124534607
Epoch 710, training loss: 12.314164161682129 = 0.16229717433452606 + 2.0 * 6.075933456420898
Epoch 710, val loss: 0.8392707109451294
Epoch 720, training loss: 12.299094200134277 = 0.15443386137485504 + 2.0 * 6.072329998016357
Epoch 720, val loss: 0.8453781604766846
Epoch 730, training loss: 12.294051170349121 = 0.14700911939144135 + 2.0 * 6.073521137237549
Epoch 730, val loss: 0.8516981601715088
Epoch 740, training loss: 12.279557228088379 = 0.14004622399806976 + 2.0 * 6.069755554199219
Epoch 740, val loss: 0.8582812547683716
Epoch 750, training loss: 12.267264366149902 = 0.13347339630126953 + 2.0 * 6.066895484924316
Epoch 750, val loss: 0.8652141690254211
Epoch 760, training loss: 12.259182929992676 = 0.1272851973772049 + 2.0 * 6.065948963165283
Epoch 760, val loss: 0.8723900318145752
Epoch 770, training loss: 12.258913040161133 = 0.12144069373607635 + 2.0 * 6.0687360763549805
Epoch 770, val loss: 0.8796370029449463
Epoch 780, training loss: 12.248054504394531 = 0.11601048707962036 + 2.0 * 6.066021919250488
Epoch 780, val loss: 0.8871810436248779
Epoch 790, training loss: 12.237390518188477 = 0.11087666451931 + 2.0 * 6.063256740570068
Epoch 790, val loss: 0.8948903679847717
Epoch 800, training loss: 12.229440689086914 = 0.10603287816047668 + 2.0 * 6.061703681945801
Epoch 800, val loss: 0.9026519656181335
Epoch 810, training loss: 12.239542961120605 = 0.10144968330860138 + 2.0 * 6.069046497344971
Epoch 810, val loss: 0.91045081615448
Epoch 820, training loss: 12.225811958312988 = 0.09716876596212387 + 2.0 * 6.064321517944336
Epoch 820, val loss: 0.9183303117752075
Epoch 830, training loss: 12.212738990783691 = 0.09313897788524628 + 2.0 * 6.059800148010254
Epoch 830, val loss: 0.9263786673545837
Epoch 840, training loss: 12.203751564025879 = 0.08931954950094223 + 2.0 * 6.057216167449951
Epoch 840, val loss: 0.9343658685684204
Epoch 850, training loss: 12.201498985290527 = 0.08569245785474777 + 2.0 * 6.057903289794922
Epoch 850, val loss: 0.9423819184303284
Epoch 860, training loss: 12.1965913772583 = 0.08227506279945374 + 2.0 * 6.05715799331665
Epoch 860, val loss: 0.9505132436752319
Epoch 870, training loss: 12.189706802368164 = 0.0790339782834053 + 2.0 * 6.0553364753723145
Epoch 870, val loss: 0.9585608839988708
Epoch 880, training loss: 12.18378734588623 = 0.07597476243972778 + 2.0 * 6.053906440734863
Epoch 880, val loss: 0.966629683971405
Epoch 890, training loss: 12.178130149841309 = 0.07307012379169464 + 2.0 * 6.052529811859131
Epoch 890, val loss: 0.9747596979141235
Epoch 900, training loss: 12.189933776855469 = 0.0703035369515419 + 2.0 * 6.059814929962158
Epoch 900, val loss: 0.9827564358711243
Epoch 910, training loss: 12.173225402832031 = 0.06770990788936615 + 2.0 * 6.052757740020752
Epoch 910, val loss: 0.9907914996147156
Epoch 920, training loss: 12.166547775268555 = 0.0652349442243576 + 2.0 * 6.050656318664551
Epoch 920, val loss: 0.9988738894462585
Epoch 930, training loss: 12.161520004272461 = 0.06287524849176407 + 2.0 * 6.049322605133057
Epoch 930, val loss: 1.0068535804748535
Epoch 940, training loss: 12.166509628295898 = 0.060635317116975784 + 2.0 * 6.052937030792236
Epoch 940, val loss: 1.0147627592086792
Epoch 950, training loss: 12.15562629699707 = 0.058510564267635345 + 2.0 * 6.048557758331299
Epoch 950, val loss: 1.0225778818130493
Epoch 960, training loss: 12.15330982208252 = 0.05647432059049606 + 2.0 * 6.048417568206787
Epoch 960, val loss: 1.0304210186004639
Epoch 970, training loss: 12.146146774291992 = 0.054547231644392014 + 2.0 * 6.045799732208252
Epoch 970, val loss: 1.0381149053573608
Epoch 980, training loss: 12.142412185668945 = 0.05270956829190254 + 2.0 * 6.044851303100586
Epoch 980, val loss: 1.0459388494491577
Epoch 990, training loss: 12.143096923828125 = 0.05095415562391281 + 2.0 * 6.046071529388428
Epoch 990, val loss: 1.0536144971847534
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8000
Overall ASR: 0.4502
Flip ASR: 0.3556/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.687625885009766 = 1.9399158954620361 + 2.0 * 8.373854637145996
Epoch 0, val loss: 1.9349290132522583
Epoch 10, training loss: 18.675539016723633 = 1.9292398691177368 + 2.0 * 8.373149871826172
Epoch 10, val loss: 1.9244095087051392
Epoch 20, training loss: 18.65239715576172 = 1.9159326553344727 + 2.0 * 8.368231773376465
Epoch 20, val loss: 1.9111027717590332
Epoch 30, training loss: 18.568477630615234 = 1.8983560800552368 + 2.0 * 8.335061073303223
Epoch 30, val loss: 1.8935365676879883
Epoch 40, training loss: 17.96371841430664 = 1.8788074254989624 + 2.0 * 8.042455673217773
Epoch 40, val loss: 1.8746968507766724
Epoch 50, training loss: 16.70207977294922 = 1.8596692085266113 + 2.0 * 7.421205043792725
Epoch 50, val loss: 1.8566405773162842
Epoch 60, training loss: 16.16108512878418 = 1.8444119691848755 + 2.0 * 7.158336162567139
Epoch 60, val loss: 1.842185139656067
Epoch 70, training loss: 15.612275123596191 = 1.8315080404281616 + 2.0 * 6.890383720397949
Epoch 70, val loss: 1.8300048112869263
Epoch 80, training loss: 15.191732406616211 = 1.820570468902588 + 2.0 * 6.685580730438232
Epoch 80, val loss: 1.819582462310791
Epoch 90, training loss: 14.940958976745605 = 1.8076045513153076 + 2.0 * 6.566677093505859
Epoch 90, val loss: 1.8072141408920288
Epoch 100, training loss: 14.771932601928711 = 1.792906641960144 + 2.0 * 6.489512920379639
Epoch 100, val loss: 1.7931170463562012
Epoch 110, training loss: 14.648380279541016 = 1.7774803638458252 + 2.0 * 6.435450077056885
Epoch 110, val loss: 1.778398036956787
Epoch 120, training loss: 14.548054695129395 = 1.7615865468978882 + 2.0 * 6.3932342529296875
Epoch 120, val loss: 1.7630952596664429
Epoch 130, training loss: 14.462858200073242 = 1.744697093963623 + 2.0 * 6.3590803146362305
Epoch 130, val loss: 1.746851921081543
Epoch 140, training loss: 14.389933586120605 = 1.7262251377105713 + 2.0 * 6.331854343414307
Epoch 140, val loss: 1.7294055223464966
Epoch 150, training loss: 14.326044082641602 = 1.7055797576904297 + 2.0 * 6.310232162475586
Epoch 150, val loss: 1.710423231124878
Epoch 160, training loss: 14.259536743164062 = 1.6825617551803589 + 2.0 * 6.288487434387207
Epoch 160, val loss: 1.6897131204605103
Epoch 170, training loss: 14.197904586791992 = 1.65699303150177 + 2.0 * 6.270455837249756
Epoch 170, val loss: 1.667349934577942
Epoch 180, training loss: 14.139688491821289 = 1.6285412311553955 + 2.0 * 6.255573749542236
Epoch 180, val loss: 1.6429061889648438
Epoch 190, training loss: 14.078872680664062 = 1.5971332788467407 + 2.0 * 6.240869522094727
Epoch 190, val loss: 1.616426944732666
Epoch 200, training loss: 14.018254280090332 = 1.562646746635437 + 2.0 * 6.227803707122803
Epoch 200, val loss: 1.587876796722412
Epoch 210, training loss: 13.96251106262207 = 1.5252691507339478 + 2.0 * 6.218620777130127
Epoch 210, val loss: 1.5574804544448853
Epoch 220, training loss: 13.900381088256836 = 1.4853307008743286 + 2.0 * 6.207525253295898
Epoch 220, val loss: 1.5253279209136963
Epoch 230, training loss: 13.839852333068848 = 1.4430878162384033 + 2.0 * 6.198382377624512
Epoch 230, val loss: 1.491868019104004
Epoch 240, training loss: 13.780713081359863 = 1.399045705795288 + 2.0 * 6.190833568572998
Epoch 240, val loss: 1.457559585571289
Epoch 250, training loss: 13.722858428955078 = 1.354215145111084 + 2.0 * 6.184321880340576
Epoch 250, val loss: 1.4232168197631836
Epoch 260, training loss: 13.662421226501465 = 1.3092154264450073 + 2.0 * 6.176602840423584
Epoch 260, val loss: 1.3894292116165161
Epoch 270, training loss: 13.607600212097168 = 1.2644675970077515 + 2.0 * 6.171566486358643
Epoch 270, val loss: 1.3563382625579834
Epoch 280, training loss: 13.55220890045166 = 1.220388650894165 + 2.0 * 6.165910243988037
Epoch 280, val loss: 1.3243963718414307
Epoch 290, training loss: 13.496790885925293 = 1.176886796951294 + 2.0 * 6.159952163696289
Epoch 290, val loss: 1.2931774854660034
Epoch 300, training loss: 13.446883201599121 = 1.1339839696884155 + 2.0 * 6.156449794769287
Epoch 300, val loss: 1.2628047466278076
Epoch 310, training loss: 13.394353866577148 = 1.0918601751327515 + 2.0 * 6.151247024536133
Epoch 310, val loss: 1.233264446258545
Epoch 320, training loss: 13.342312812805176 = 1.0501497983932495 + 2.0 * 6.146081447601318
Epoch 320, val loss: 1.204248309135437
Epoch 330, training loss: 13.299745559692383 = 1.0089613199234009 + 2.0 * 6.145391941070557
Epoch 330, val loss: 1.1758413314819336
Epoch 340, training loss: 13.245197296142578 = 0.9686382412910461 + 2.0 * 6.138279438018799
Epoch 340, val loss: 1.1480578184127808
Epoch 350, training loss: 13.204422950744629 = 0.9290573596954346 + 2.0 * 6.137682914733887
Epoch 350, val loss: 1.1209982633590698
Epoch 360, training loss: 13.153948783874512 = 0.8906841278076172 + 2.0 * 6.131632328033447
Epoch 360, val loss: 1.0949454307556152
Epoch 370, training loss: 13.109790802001953 = 0.8534114956855774 + 2.0 * 6.128189563751221
Epoch 370, val loss: 1.070012092590332
Epoch 380, training loss: 13.074159622192383 = 0.8175438046455383 + 2.0 * 6.128307819366455
Epoch 380, val loss: 1.046345829963684
Epoch 390, training loss: 13.0288724899292 = 0.7833716869354248 + 2.0 * 6.122750282287598
Epoch 390, val loss: 1.0241323709487915
Epoch 400, training loss: 12.987889289855957 = 0.7506580352783203 + 2.0 * 6.118615627288818
Epoch 400, val loss: 1.0034557580947876
Epoch 410, training loss: 12.953400611877441 = 0.7193884253501892 + 2.0 * 6.117006301879883
Epoch 410, val loss: 0.9842183589935303
Epoch 420, training loss: 12.934444427490234 = 0.6897481083869934 + 2.0 * 6.122348308563232
Epoch 420, val loss: 0.966400682926178
Epoch 430, training loss: 12.886962890625 = 0.6616940498352051 + 2.0 * 6.112634181976318
Epoch 430, val loss: 0.9503692388534546
Epoch 440, training loss: 12.85063362121582 = 0.6350968480110168 + 2.0 * 6.107768535614014
Epoch 440, val loss: 0.9356571435928345
Epoch 450, training loss: 12.82203483581543 = 0.6096913814544678 + 2.0 * 6.106171607971191
Epoch 450, val loss: 0.9223152995109558
Epoch 460, training loss: 12.796213150024414 = 0.5855023264884949 + 2.0 * 6.105355262756348
Epoch 460, val loss: 0.910033643245697
Epoch 470, training loss: 12.769257545471191 = 0.5624961256980896 + 2.0 * 6.1033806800842285
Epoch 470, val loss: 0.8987960815429688
Epoch 480, training loss: 12.737845420837402 = 0.540546715259552 + 2.0 * 6.098649501800537
Epoch 480, val loss: 0.8886388540267944
Epoch 490, training loss: 12.712788581848145 = 0.519433856010437 + 2.0 * 6.096677303314209
Epoch 490, val loss: 0.879361629486084
Epoch 500, training loss: 12.709527015686035 = 0.4991600811481476 + 2.0 * 6.1051836013793945
Epoch 500, val loss: 0.8707708716392517
Epoch 510, training loss: 12.667559623718262 = 0.4796309173107147 + 2.0 * 6.093964576721191
Epoch 510, val loss: 0.8630537390708923
Epoch 520, training loss: 12.643795013427734 = 0.4609678089618683 + 2.0 * 6.091413497924805
Epoch 520, val loss: 0.8561412692070007
Epoch 530, training loss: 12.620816230773926 = 0.44289857149124146 + 2.0 * 6.088958740234375
Epoch 530, val loss: 0.8499115109443665
Epoch 540, training loss: 12.60371208190918 = 0.42539486289024353 + 2.0 * 6.089158535003662
Epoch 540, val loss: 0.8442403078079224
Epoch 550, training loss: 12.584977149963379 = 0.4084590971469879 + 2.0 * 6.088259220123291
Epoch 550, val loss: 0.8391730785369873
Epoch 560, training loss: 12.561042785644531 = 0.3920249938964844 + 2.0 * 6.084508895874023
Epoch 560, val loss: 0.834815263748169
Epoch 570, training loss: 12.540915489196777 = 0.3760603368282318 + 2.0 * 6.082427501678467
Epoch 570, val loss: 0.8310186862945557
Epoch 580, training loss: 12.528111457824707 = 0.36053040623664856 + 2.0 * 6.083790302276611
Epoch 580, val loss: 0.8277503252029419
Epoch 590, training loss: 12.50759220123291 = 0.34550926089286804 + 2.0 * 6.08104133605957
Epoch 590, val loss: 0.8250094652175903
Epoch 600, training loss: 12.485761642456055 = 0.33092135190963745 + 2.0 * 6.077420234680176
Epoch 600, val loss: 0.8229507803916931
Epoch 610, training loss: 12.473958015441895 = 0.31675538420677185 + 2.0 * 6.078601360321045
Epoch 610, val loss: 0.8213949203491211
Epoch 620, training loss: 12.454907417297363 = 0.30303752422332764 + 2.0 * 6.075934886932373
Epoch 620, val loss: 0.8201873898506165
Epoch 630, training loss: 12.441934585571289 = 0.28969806432724 + 2.0 * 6.076118469238281
Epoch 630, val loss: 0.8195449709892273
Epoch 640, training loss: 12.422245025634766 = 0.276800274848938 + 2.0 * 6.072722434997559
Epoch 640, val loss: 0.819313645362854
Epoch 650, training loss: 12.410147666931152 = 0.2643510103225708 + 2.0 * 6.0728983879089355
Epoch 650, val loss: 0.8195868730545044
Epoch 660, training loss: 12.391571044921875 = 0.25231683254241943 + 2.0 * 6.069627285003662
Epoch 660, val loss: 0.8203675746917725
Epoch 670, training loss: 12.376996994018555 = 0.24075815081596375 + 2.0 * 6.068119525909424
Epoch 670, val loss: 0.8215973973274231
Epoch 680, training loss: 12.375642776489258 = 0.229606032371521 + 2.0 * 6.073018550872803
Epoch 680, val loss: 0.8231762647628784
Epoch 690, training loss: 12.348488807678223 = 0.21888171136379242 + 2.0 * 6.064803600311279
Epoch 690, val loss: 0.8251637816429138
Epoch 700, training loss: 12.338098526000977 = 0.20860782265663147 + 2.0 * 6.0647454261779785
Epoch 700, val loss: 0.827583909034729
Epoch 710, training loss: 12.327204704284668 = 0.19872887432575226 + 2.0 * 6.06423807144165
Epoch 710, val loss: 0.8303172588348389
Epoch 720, training loss: 12.316655158996582 = 0.1892455518245697 + 2.0 * 6.063704967498779
Epoch 720, val loss: 0.8333589434623718
Epoch 730, training loss: 12.303641319274902 = 0.18019552528858185 + 2.0 * 6.061722755432129
Epoch 730, val loss: 0.8368261456489563
Epoch 740, training loss: 12.29818344116211 = 0.1715778261423111 + 2.0 * 6.063302993774414
Epoch 740, val loss: 0.8405395746231079
Epoch 750, training loss: 12.28482437133789 = 0.1633177250623703 + 2.0 * 6.060753345489502
Epoch 750, val loss: 0.8445590138435364
Epoch 760, training loss: 12.270989418029785 = 0.15548111498355865 + 2.0 * 6.057754039764404
Epoch 760, val loss: 0.8488997220993042
Epoch 770, training loss: 12.260822296142578 = 0.14799362421035767 + 2.0 * 6.0564141273498535
Epoch 770, val loss: 0.8535722494125366
Epoch 780, training loss: 12.249991416931152 = 0.14084716141223907 + 2.0 * 6.054572105407715
Epoch 780, val loss: 0.858435869216919
Epoch 790, training loss: 12.251317024230957 = 0.13401828706264496 + 2.0 * 6.05864953994751
Epoch 790, val loss: 0.8634747862815857
Epoch 800, training loss: 12.24500560760498 = 0.12751096487045288 + 2.0 * 6.058747291564941
Epoch 800, val loss: 0.8686840534210205
Epoch 810, training loss: 12.229037284851074 = 0.12136666476726532 + 2.0 * 6.053835391998291
Epoch 810, val loss: 0.8739874958992004
Epoch 820, training loss: 12.219242095947266 = 0.11552713066339493 + 2.0 * 6.0518574714660645
Epoch 820, val loss: 0.8795434832572937
Epoch 830, training loss: 12.211074829101562 = 0.1099688857793808 + 2.0 * 6.050552845001221
Epoch 830, val loss: 0.8852300047874451
Epoch 840, training loss: 12.20964527130127 = 0.10468792170286179 + 2.0 * 6.052478790283203
Epoch 840, val loss: 0.8910865187644958
Epoch 850, training loss: 12.205331802368164 = 0.09972063452005386 + 2.0 * 6.052805423736572
Epoch 850, val loss: 0.8971636891365051
Epoch 860, training loss: 12.19565200805664 = 0.0950162336230278 + 2.0 * 6.050317764282227
Epoch 860, val loss: 0.9033647775650024
Epoch 870, training loss: 12.18375301361084 = 0.09061282873153687 + 2.0 * 6.046570301055908
Epoch 870, val loss: 0.9097229242324829
Epoch 880, training loss: 12.179255485534668 = 0.08644194900989532 + 2.0 * 6.0464067459106445
Epoch 880, val loss: 0.9162182211875916
Epoch 890, training loss: 12.187362670898438 = 0.08249665051698685 + 2.0 * 6.052433013916016
Epoch 890, val loss: 0.9227643013000488
Epoch 900, training loss: 12.175843238830566 = 0.07879210263490677 + 2.0 * 6.048525333404541
Epoch 900, val loss: 0.9294279217720032
Epoch 910, training loss: 12.164749145507812 = 0.07528673857450485 + 2.0 * 6.044731140136719
Epoch 910, val loss: 0.9361369609832764
Epoch 920, training loss: 12.156638145446777 = 0.07199438661336899 + 2.0 * 6.042321681976318
Epoch 920, val loss: 0.9429458975791931
Epoch 930, training loss: 12.15263557434082 = 0.06887613236904144 + 2.0 * 6.041879653930664
Epoch 930, val loss: 0.949752688407898
Epoch 940, training loss: 12.153054237365723 = 0.06592762470245361 + 2.0 * 6.043563365936279
Epoch 940, val loss: 0.9566156268119812
Epoch 950, training loss: 12.146282196044922 = 0.06313806027173996 + 2.0 * 6.041572093963623
Epoch 950, val loss: 0.9635180234909058
Epoch 960, training loss: 12.144285202026367 = 0.06052776798605919 + 2.0 * 6.041878700256348
Epoch 960, val loss: 0.9704456329345703
Epoch 970, training loss: 12.137578010559082 = 0.058055609464645386 + 2.0 * 6.039761066436768
Epoch 970, val loss: 0.9774805307388306
Epoch 980, training loss: 12.147294998168945 = 0.05572464317083359 + 2.0 * 6.045784950256348
Epoch 980, val loss: 0.98446124792099
Epoch 990, training loss: 12.135984420776367 = 0.0535263754427433 + 2.0 * 6.041229248046875
Epoch 990, val loss: 0.9913681745529175
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7593
Overall ASR: 0.6974
Flip ASR: 0.6489/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.709365844726562 = 1.9615241289138794 + 2.0 * 8.373920440673828
Epoch 0, val loss: 1.9569485187530518
Epoch 10, training loss: 18.69753646850586 = 1.9505349397659302 + 2.0 * 8.37350082397461
Epoch 10, val loss: 1.9462370872497559
Epoch 20, training loss: 18.678733825683594 = 1.937161922454834 + 2.0 * 8.3707857131958
Epoch 20, val loss: 1.9328703880310059
Epoch 30, training loss: 18.630020141601562 = 1.9191503524780273 + 2.0 * 8.35543441772461
Epoch 30, val loss: 1.9145139455795288
Epoch 40, training loss: 18.43245506286621 = 1.8966782093048096 + 2.0 * 8.267888069152832
Epoch 40, val loss: 1.8923460245132446
Epoch 50, training loss: 17.50334930419922 = 1.874197006225586 + 2.0 * 7.814576625823975
Epoch 50, val loss: 1.8702869415283203
Epoch 60, training loss: 16.598289489746094 = 1.8507704734802246 + 2.0 * 7.3737592697143555
Epoch 60, val loss: 1.847805380821228
Epoch 70, training loss: 16.06368064880371 = 1.8310295343399048 + 2.0 * 7.116325855255127
Epoch 70, val loss: 1.8294090032577515
Epoch 80, training loss: 15.649270057678223 = 1.8126798868179321 + 2.0 * 6.918294906616211
Epoch 80, val loss: 1.8119698762893677
Epoch 90, training loss: 15.311197280883789 = 1.796229600906372 + 2.0 * 6.757483959197998
Epoch 90, val loss: 1.7967488765716553
Epoch 100, training loss: 15.050073623657227 = 1.779630422592163 + 2.0 * 6.635221481323242
Epoch 100, val loss: 1.7820302248001099
Epoch 110, training loss: 14.861847877502441 = 1.7625049352645874 + 2.0 * 6.549671649932861
Epoch 110, val loss: 1.7668460607528687
Epoch 120, training loss: 14.711426734924316 = 1.7446614503860474 + 2.0 * 6.483382701873779
Epoch 120, val loss: 1.750459909439087
Epoch 130, training loss: 14.58868408203125 = 1.7255750894546509 + 2.0 * 6.431554317474365
Epoch 130, val loss: 1.7329936027526855
Epoch 140, training loss: 14.491545677185059 = 1.7045406103134155 + 2.0 * 6.393502712249756
Epoch 140, val loss: 1.7142629623413086
Epoch 150, training loss: 14.402619361877441 = 1.6811838150024414 + 2.0 * 6.3607177734375
Epoch 150, val loss: 1.694136381149292
Epoch 160, training loss: 14.326796531677246 = 1.6552820205688477 + 2.0 * 6.335757255554199
Epoch 160, val loss: 1.6722071170806885
Epoch 170, training loss: 14.253503799438477 = 1.6265959739685059 + 2.0 * 6.313453674316406
Epoch 170, val loss: 1.6483317613601685
Epoch 180, training loss: 14.185245513916016 = 1.5946286916732788 + 2.0 * 6.295308589935303
Epoch 180, val loss: 1.622204065322876
Epoch 190, training loss: 14.11798095703125 = 1.5591371059417725 + 2.0 * 6.279421806335449
Epoch 190, val loss: 1.5937100648880005
Epoch 200, training loss: 14.05197811126709 = 1.5201408863067627 + 2.0 * 6.265918731689453
Epoch 200, val loss: 1.5630035400390625
Epoch 210, training loss: 13.987112998962402 = 1.4780994653701782 + 2.0 * 6.254506587982178
Epoch 210, val loss: 1.5302060842514038
Epoch 220, training loss: 13.919936180114746 = 1.4330805540084839 + 2.0 * 6.243427753448486
Epoch 220, val loss: 1.4954453706741333
Epoch 230, training loss: 13.86160659790039 = 1.3856356143951416 + 2.0 * 6.237985610961914
Epoch 230, val loss: 1.4593024253845215
Epoch 240, training loss: 13.787274360656738 = 1.3370344638824463 + 2.0 * 6.2251200675964355
Epoch 240, val loss: 1.4231033325195312
Epoch 250, training loss: 13.719500541687012 = 1.2878609895706177 + 2.0 * 6.215819835662842
Epoch 250, val loss: 1.3869162797927856
Epoch 260, training loss: 13.655357360839844 = 1.2386502027511597 + 2.0 * 6.208353519439697
Epoch 260, val loss: 1.351212501525879
Epoch 270, training loss: 13.590073585510254 = 1.1902425289154053 + 2.0 * 6.199915409088135
Epoch 270, val loss: 1.3165643215179443
Epoch 280, training loss: 13.52779483795166 = 1.1426162719726562 + 2.0 * 6.192589282989502
Epoch 280, val loss: 1.2829549312591553
Epoch 290, training loss: 13.477997779846191 = 1.0957767963409424 + 2.0 * 6.191110610961914
Epoch 290, val loss: 1.2506022453308105
Epoch 300, training loss: 13.410749435424805 = 1.0504810810089111 + 2.0 * 6.180134296417236
Epoch 300, val loss: 1.219342827796936
Epoch 310, training loss: 13.356350898742676 = 1.006443738937378 + 2.0 * 6.174953460693359
Epoch 310, val loss: 1.1895360946655273
Epoch 320, training loss: 13.312602043151855 = 0.9638639688491821 + 2.0 * 6.174368858337402
Epoch 320, val loss: 1.1609621047973633
Epoch 330, training loss: 13.25308609008789 = 0.9229096174240112 + 2.0 * 6.165088176727295
Epoch 330, val loss: 1.1341371536254883
Epoch 340, training loss: 13.202465057373047 = 0.8837814927101135 + 2.0 * 6.159341812133789
Epoch 340, val loss: 1.1088563203811646
Epoch 350, training loss: 13.159244537353516 = 0.8462624549865723 + 2.0 * 6.156491279602051
Epoch 350, val loss: 1.0849170684814453
Epoch 360, training loss: 13.11745548248291 = 0.8104702234268188 + 2.0 * 6.153492450714111
Epoch 360, val loss: 1.0626699924468994
Epoch 370, training loss: 13.070837020874023 = 0.7769041061401367 + 2.0 * 6.146966457366943
Epoch 370, val loss: 1.0418941974639893
Epoch 380, training loss: 13.031086921691895 = 0.7452674508094788 + 2.0 * 6.142909526824951
Epoch 380, val loss: 1.0229495763778687
Epoch 390, training loss: 12.993021965026855 = 0.7153083682060242 + 2.0 * 6.138856887817383
Epoch 390, val loss: 1.005218267440796
Epoch 400, training loss: 12.96151351928711 = 0.6868470311164856 + 2.0 * 6.137333393096924
Epoch 400, val loss: 0.9885961413383484
Epoch 410, training loss: 12.924095153808594 = 0.6597864627838135 + 2.0 * 6.13215446472168
Epoch 410, val loss: 0.9733356833457947
Epoch 420, training loss: 12.895868301391602 = 0.6342748999595642 + 2.0 * 6.130796909332275
Epoch 420, val loss: 0.9590420126914978
Epoch 430, training loss: 12.865043640136719 = 0.6100019812583923 + 2.0 * 6.12752103805542
Epoch 430, val loss: 0.9459365606307983
Epoch 440, training loss: 12.83205795288086 = 0.5869925618171692 + 2.0 * 6.122532844543457
Epoch 440, val loss: 0.9335957765579224
Epoch 450, training loss: 12.8037691116333 = 0.5649999380111694 + 2.0 * 6.119384765625
Epoch 450, val loss: 0.9221526980400085
Epoch 460, training loss: 12.789493560791016 = 0.5439434051513672 + 2.0 * 6.122775077819824
Epoch 460, val loss: 0.9115123152732849
Epoch 470, training loss: 12.755571365356445 = 0.5237559080123901 + 2.0 * 6.115907669067383
Epoch 470, val loss: 0.9016175270080566
Epoch 480, training loss: 12.73434829711914 = 0.5044835209846497 + 2.0 * 6.114932537078857
Epoch 480, val loss: 0.892487108707428
Epoch 490, training loss: 12.704702377319336 = 0.48601868748664856 + 2.0 * 6.109341621398926
Epoch 490, val loss: 0.8842020034790039
Epoch 500, training loss: 12.681272506713867 = 0.4683467149734497 + 2.0 * 6.1064629554748535
Epoch 500, val loss: 0.8765655159950256
Epoch 510, training loss: 12.661641120910645 = 0.45125243067741394 + 2.0 * 6.105194568634033
Epoch 510, val loss: 0.8696030378341675
Epoch 520, training loss: 12.643104553222656 = 0.4348021149635315 + 2.0 * 6.104151248931885
Epoch 520, val loss: 0.8631752729415894
Epoch 530, training loss: 12.624211311340332 = 0.418933242559433 + 2.0 * 6.102639198303223
Epoch 530, val loss: 0.8573073744773865
Epoch 540, training loss: 12.603672981262207 = 0.40363186597824097 + 2.0 * 6.100020408630371
Epoch 540, val loss: 0.8520838618278503
Epoch 550, training loss: 12.581742286682129 = 0.3888549506664276 + 2.0 * 6.0964436531066895
Epoch 550, val loss: 0.8473187685012817
Epoch 560, training loss: 12.563176155090332 = 0.3744502365589142 + 2.0 * 6.094362735748291
Epoch 560, val loss: 0.8431103825569153
Epoch 570, training loss: 12.567221641540527 = 0.36043623089790344 + 2.0 * 6.103392601013184
Epoch 570, val loss: 0.8391749858856201
Epoch 580, training loss: 12.529314041137695 = 0.34669029712677 + 2.0 * 6.091311931610107
Epoch 580, val loss: 0.8357712626457214
Epoch 590, training loss: 12.514948844909668 = 0.3333268463611603 + 2.0 * 6.090810775756836
Epoch 590, val loss: 0.8327645659446716
Epoch 600, training loss: 12.496440887451172 = 0.3202251195907593 + 2.0 * 6.088108062744141
Epoch 600, val loss: 0.8299490213394165
Epoch 610, training loss: 12.483160018920898 = 0.30729353427886963 + 2.0 * 6.08793306350708
Epoch 610, val loss: 0.8274668455123901
Epoch 620, training loss: 12.466985702514648 = 0.2945218086242676 + 2.0 * 6.0862321853637695
Epoch 620, val loss: 0.8252698183059692
Epoch 630, training loss: 12.450691223144531 = 0.28207165002822876 + 2.0 * 6.0843095779418945
Epoch 630, val loss: 0.8231831192970276
Epoch 640, training loss: 12.435572624206543 = 0.26981067657470703 + 2.0 * 6.082880973815918
Epoch 640, val loss: 0.8215218782424927
Epoch 650, training loss: 12.421491622924805 = 0.2578621506690979 + 2.0 * 6.081814765930176
Epoch 650, val loss: 0.8201329708099365
Epoch 660, training loss: 12.406983375549316 = 0.24608114361763 + 2.0 * 6.080451011657715
Epoch 660, val loss: 0.819130539894104
Epoch 670, training loss: 12.390450477600098 = 0.23458856344223022 + 2.0 * 6.077930927276611
Epoch 670, val loss: 0.8184517621994019
Epoch 680, training loss: 12.374308586120605 = 0.2234373241662979 + 2.0 * 6.075435638427734
Epoch 680, val loss: 0.8181091547012329
Epoch 690, training loss: 12.369935035705566 = 0.21258750557899475 + 2.0 * 6.078673839569092
Epoch 690, val loss: 0.8181092143058777
Epoch 700, training loss: 12.356874465942383 = 0.20207622647285461 + 2.0 * 6.077399253845215
Epoch 700, val loss: 0.8185566663742065
Epoch 710, training loss: 12.338303565979004 = 0.19198590517044067 + 2.0 * 6.0731587409973145
Epoch 710, val loss: 0.8193948864936829
Epoch 720, training loss: 12.324461936950684 = 0.1822900027036667 + 2.0 * 6.0710859298706055
Epoch 720, val loss: 0.820671796798706
Epoch 730, training loss: 12.321161270141602 = 0.17301702499389648 + 2.0 * 6.074071884155273
Epoch 730, val loss: 0.8223571181297302
Epoch 740, training loss: 12.30036735534668 = 0.16418328881263733 + 2.0 * 6.068091869354248
Epoch 740, val loss: 0.8244041204452515
Epoch 750, training loss: 12.291916847229004 = 0.1557626873254776 + 2.0 * 6.068077087402344
Epoch 750, val loss: 0.8269726634025574
Epoch 760, training loss: 12.292253494262695 = 0.14773698151111603 + 2.0 * 6.072258472442627
Epoch 760, val loss: 0.8300653696060181
Epoch 770, training loss: 12.268434524536133 = 0.14013256132602692 + 2.0 * 6.064150810241699
Epoch 770, val loss: 0.8333199620246887
Epoch 780, training loss: 12.261882781982422 = 0.1329580694437027 + 2.0 * 6.064462184906006
Epoch 780, val loss: 0.8370541334152222
Epoch 790, training loss: 12.253631591796875 = 0.12617528438568115 + 2.0 * 6.063728332519531
Epoch 790, val loss: 0.8411858677864075
Epoch 800, training loss: 12.245400428771973 = 0.11974263191223145 + 2.0 * 6.06282901763916
Epoch 800, val loss: 0.845573902130127
Epoch 810, training loss: 12.248210906982422 = 0.11367261409759521 + 2.0 * 6.067269325256348
Epoch 810, val loss: 0.8503979444503784
Epoch 820, training loss: 12.232646942138672 = 0.1080133467912674 + 2.0 * 6.06231689453125
Epoch 820, val loss: 0.8551512956619263
Epoch 830, training loss: 12.219472885131836 = 0.1026841476559639 + 2.0 * 6.058394432067871
Epoch 830, val loss: 0.8605304956436157
Epoch 840, training loss: 12.211145401000977 = 0.09767313301563263 + 2.0 * 6.056735992431641
Epoch 840, val loss: 0.8660771250724792
Epoch 850, training loss: 12.210145950317383 = 0.09293796867132187 + 2.0 * 6.058603763580322
Epoch 850, val loss: 0.8718363642692566
Epoch 860, training loss: 12.204246520996094 = 0.08849185705184937 + 2.0 * 6.057877540588379
Epoch 860, val loss: 0.8777485489845276
Epoch 870, training loss: 12.197922706604004 = 0.08432237058877945 + 2.0 * 6.056800365447998
Epoch 870, val loss: 0.8837929964065552
Epoch 880, training loss: 12.189126968383789 = 0.08041151612997055 + 2.0 * 6.054357528686523
Epoch 880, val loss: 0.8899500966072083
Epoch 890, training loss: 12.184403419494629 = 0.07672294974327087 + 2.0 * 6.053840160369873
Epoch 890, val loss: 0.896207869052887
Epoch 900, training loss: 12.179871559143066 = 0.07326842844486237 + 2.0 * 6.0533013343811035
Epoch 900, val loss: 0.902556836605072
Epoch 910, training loss: 12.171295166015625 = 0.07001500576734543 + 2.0 * 6.050640106201172
Epoch 910, val loss: 0.9089270830154419
Epoch 920, training loss: 12.176319122314453 = 0.06695910543203354 + 2.0 * 6.054679870605469
Epoch 920, val loss: 0.9153017401695251
Epoch 930, training loss: 12.171634674072266 = 0.06407660990953445 + 2.0 * 6.053779125213623
Epoch 930, val loss: 0.9217550754547119
Epoch 940, training loss: 12.157875061035156 = 0.06136507913470268 + 2.0 * 6.04825496673584
Epoch 940, val loss: 0.9282558560371399
Epoch 950, training loss: 12.152400970458984 = 0.05880464240908623 + 2.0 * 6.046798229217529
Epoch 950, val loss: 0.9347605109214783
Epoch 960, training loss: 12.165897369384766 = 0.056388791650533676 + 2.0 * 6.054754257202148
Epoch 960, val loss: 0.9413281083106995
Epoch 970, training loss: 12.150423049926758 = 0.054131366312503815 + 2.0 * 6.048145771026611
Epoch 970, val loss: 0.9475865364074707
Epoch 980, training loss: 12.142740249633789 = 0.051977258175611496 + 2.0 * 6.045381546020508
Epoch 980, val loss: 0.9541100263595581
Epoch 990, training loss: 12.137603759765625 = 0.04995938763022423 + 2.0 * 6.043822288513184
Epoch 990, val loss: 0.9604839086532593
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7704
Overall ASR: 0.9705
Flip ASR: 0.9644/225 nodes
The final ASR:0.70603, 0.21250, Accuracy:0.77654, 0.01720
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11644])
remove edge: torch.Size([2, 9448])
updated graph: torch.Size([2, 10536])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
The final ASR:0.97909, 0.00174, Accuracy:0.83210, 0.00630
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.5 der2:0 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 18.687583923339844 = 1.939921259880066 + 2.0 * 8.373831748962402
Epoch 0, val loss: 1.93778395652771
Epoch 10, training loss: 18.676223754882812 = 1.9298831224441528 + 2.0 * 8.373169898986816
Epoch 10, val loss: 1.9286309480667114
Epoch 20, training loss: 18.655254364013672 = 1.9170552492141724 + 2.0 * 8.369099617004395
Epoch 20, val loss: 1.916845679283142
Epoch 30, training loss: 18.581493377685547 = 1.8996224403381348 + 2.0 * 8.340935707092285
Epoch 30, val loss: 1.900899052619934
Epoch 40, training loss: 18.12352180480957 = 1.879202961921692 + 2.0 * 8.122159004211426
Epoch 40, val loss: 1.8826053142547607
Epoch 50, training loss: 16.487850189208984 = 1.8576076030731201 + 2.0 * 7.315121650695801
Epoch 50, val loss: 1.8639885187149048
Epoch 60, training loss: 15.790984153747559 = 1.84502375125885 + 2.0 * 6.97298002243042
Epoch 60, val loss: 1.8527628183364868
Epoch 70, training loss: 15.300132751464844 = 1.8360300064086914 + 2.0 * 6.732051372528076
Epoch 70, val loss: 1.8435810804367065
Epoch 80, training loss: 15.015352249145508 = 1.8260291814804077 + 2.0 * 6.594661712646484
Epoch 80, val loss: 1.8339674472808838
Epoch 90, training loss: 14.829625129699707 = 1.8168326616287231 + 2.0 * 6.506396293640137
Epoch 90, val loss: 1.825036883354187
Epoch 100, training loss: 14.68067741394043 = 1.8078148365020752 + 2.0 * 6.436431407928467
Epoch 100, val loss: 1.816516637802124
Epoch 110, training loss: 14.573342323303223 = 1.7990761995315552 + 2.0 * 6.3871331214904785
Epoch 110, val loss: 1.808184266090393
Epoch 120, training loss: 14.484046936035156 = 1.7905536890029907 + 2.0 * 6.346746444702148
Epoch 120, val loss: 1.799902319908142
Epoch 130, training loss: 14.412995338439941 = 1.7821201086044312 + 2.0 * 6.3154377937316895
Epoch 130, val loss: 1.7916231155395508
Epoch 140, training loss: 14.355262756347656 = 1.7733150720596313 + 2.0 * 6.290973663330078
Epoch 140, val loss: 1.783223271369934
Epoch 150, training loss: 14.301569938659668 = 1.7638537883758545 + 2.0 * 6.268857955932617
Epoch 150, val loss: 1.774492859840393
Epoch 160, training loss: 14.2522554397583 = 1.753507375717163 + 2.0 * 6.249373912811279
Epoch 160, val loss: 1.7653374671936035
Epoch 170, training loss: 14.208990097045898 = 1.741910457611084 + 2.0 * 6.233540058135986
Epoch 170, val loss: 1.7553731203079224
Epoch 180, training loss: 14.165597915649414 = 1.7287195920944214 + 2.0 * 6.218439102172852
Epoch 180, val loss: 1.7442048788070679
Epoch 190, training loss: 14.123332977294922 = 1.713642954826355 + 2.0 * 6.204844951629639
Epoch 190, val loss: 1.7316689491271973
Epoch 200, training loss: 14.08137035369873 = 1.6962231397628784 + 2.0 * 6.192573547363281
Epoch 200, val loss: 1.7172877788543701
Epoch 210, training loss: 14.044979095458984 = 1.675871729850769 + 2.0 * 6.184553623199463
Epoch 210, val loss: 1.700494647026062
Epoch 220, training loss: 13.998359680175781 = 1.6521605253219604 + 2.0 * 6.173099517822266
Epoch 220, val loss: 1.6810483932495117
Epoch 230, training loss: 13.953296661376953 = 1.6244481801986694 + 2.0 * 6.164424419403076
Epoch 230, val loss: 1.658326268196106
Epoch 240, training loss: 13.909151077270508 = 1.592004656791687 + 2.0 * 6.158573150634766
Epoch 240, val loss: 1.6316616535186768
Epoch 250, training loss: 13.858190536499023 = 1.5546492338180542 + 2.0 * 6.15177059173584
Epoch 250, val loss: 1.6009167432785034
Epoch 260, training loss: 13.803613662719727 = 1.5125892162322998 + 2.0 * 6.145512104034424
Epoch 260, val loss: 1.566112995147705
Epoch 270, training loss: 13.745195388793945 = 1.4660344123840332 + 2.0 * 6.139580726623535
Epoch 270, val loss: 1.5276765823364258
Epoch 280, training loss: 13.686650276184082 = 1.4154731035232544 + 2.0 * 6.135588645935059
Epoch 280, val loss: 1.4860098361968994
Epoch 290, training loss: 13.626092910766602 = 1.362794041633606 + 2.0 * 6.131649494171143
Epoch 290, val loss: 1.4427305459976196
Epoch 300, training loss: 13.5636568069458 = 1.3096736669540405 + 2.0 * 6.1269917488098145
Epoch 300, val loss: 1.3994369506835938
Epoch 310, training loss: 13.501943588256836 = 1.256779432296753 + 2.0 * 6.122581958770752
Epoch 310, val loss: 1.3565946817398071
Epoch 320, training loss: 13.444531440734863 = 1.2052316665649414 + 2.0 * 6.119649887084961
Epoch 320, val loss: 1.3151929378509521
Epoch 330, training loss: 13.389565467834473 = 1.1563128232955933 + 2.0 * 6.116626262664795
Epoch 330, val loss: 1.2764863967895508
Epoch 340, training loss: 13.332205772399902 = 1.109549641609192 + 2.0 * 6.111328125
Epoch 340, val loss: 1.239914059638977
Epoch 350, training loss: 13.280290603637695 = 1.0646233558654785 + 2.0 * 6.107833385467529
Epoch 350, val loss: 1.2050445079803467
Epoch 360, training loss: 13.232805252075195 = 1.021751880645752 + 2.0 * 6.105526924133301
Epoch 360, val loss: 1.1720598936080933
Epoch 370, training loss: 13.189428329467773 = 0.9816046953201294 + 2.0 * 6.103911876678467
Epoch 370, val loss: 1.141313910484314
Epoch 380, training loss: 13.141392707824707 = 0.9434831142425537 + 2.0 * 6.098954677581787
Epoch 380, val loss: 1.112201452255249
Epoch 390, training loss: 13.098875045776367 = 0.9068452715873718 + 2.0 * 6.096014976501465
Epoch 390, val loss: 1.0841964483261108
Epoch 400, training loss: 13.059303283691406 = 0.8717790246009827 + 2.0 * 6.093761920928955
Epoch 400, val loss: 1.0575114488601685
Epoch 410, training loss: 13.019701957702637 = 0.8384230732917786 + 2.0 * 6.090639591217041
Epoch 410, val loss: 1.0321041345596313
Epoch 420, training loss: 12.981677055358887 = 0.8064736127853394 + 2.0 * 6.087601661682129
Epoch 420, val loss: 1.0078319311141968
Epoch 430, training loss: 12.950370788574219 = 0.7758686542510986 + 2.0 * 6.08725118637085
Epoch 430, val loss: 0.9847525358200073
Epoch 440, training loss: 12.916313171386719 = 0.746815025806427 + 2.0 * 6.084749221801758
Epoch 440, val loss: 0.9631389379501343
Epoch 450, training loss: 12.880390167236328 = 0.7190451622009277 + 2.0 * 6.080672740936279
Epoch 450, val loss: 0.9428436756134033
Epoch 460, training loss: 12.85610580444336 = 0.6924059987068176 + 2.0 * 6.081850051879883
Epoch 460, val loss: 0.9237006902694702
Epoch 470, training loss: 12.825925827026367 = 0.6668837070465088 + 2.0 * 6.079521179199219
Epoch 470, val loss: 0.9057556390762329
Epoch 480, training loss: 12.793506622314453 = 0.6423913240432739 + 2.0 * 6.075557708740234
Epoch 480, val loss: 0.8890233635902405
Epoch 490, training loss: 12.764043807983398 = 0.618727445602417 + 2.0 * 6.072658061981201
Epoch 490, val loss: 0.8731351494789124
Epoch 500, training loss: 12.740008354187012 = 0.5956909656524658 + 2.0 * 6.0721588134765625
Epoch 500, val loss: 0.8580449223518372
Epoch 510, training loss: 12.726646423339844 = 0.5733694434165955 + 2.0 * 6.076638698577881
Epoch 510, val loss: 0.8437747955322266
Epoch 520, training loss: 12.690120697021484 = 0.5519859194755554 + 2.0 * 6.069067478179932
Epoch 520, val loss: 0.8304394483566284
Epoch 530, training loss: 12.663926124572754 = 0.5312358140945435 + 2.0 * 6.06634521484375
Epoch 530, val loss: 0.8178673982620239
Epoch 540, training loss: 12.640204429626465 = 0.5109793543815613 + 2.0 * 6.06461238861084
Epoch 540, val loss: 0.8058847784996033
Epoch 550, training loss: 12.633444786071777 = 0.4912896454334259 + 2.0 * 6.071077346801758
Epoch 550, val loss: 0.7945845723152161
Epoch 560, training loss: 12.6005277633667 = 0.472279816865921 + 2.0 * 6.06412410736084
Epoch 560, val loss: 0.784087598323822
Epoch 570, training loss: 12.575953483581543 = 0.453822523355484 + 2.0 * 6.061065673828125
Epoch 570, val loss: 0.7741988301277161
Epoch 580, training loss: 12.553890228271484 = 0.43582579493522644 + 2.0 * 6.059032440185547
Epoch 580, val loss: 0.7648149728775024
Epoch 590, training loss: 12.554083824157715 = 0.418288916349411 + 2.0 * 6.067897319793701
Epoch 590, val loss: 0.7559865117073059
Epoch 600, training loss: 12.517862319946289 = 0.40137019753456116 + 2.0 * 6.05824613571167
Epoch 600, val loss: 0.7477349638938904
Epoch 610, training loss: 12.499578475952148 = 0.3849876821041107 + 2.0 * 6.057295322418213
Epoch 610, val loss: 0.7399924993515015
Epoch 620, training loss: 12.48082160949707 = 0.3691018223762512 + 2.0 * 6.0558600425720215
Epoch 620, val loss: 0.7326861619949341
Epoch 630, training loss: 12.463116645812988 = 0.3536760210990906 + 2.0 * 6.054720401763916
Epoch 630, val loss: 0.7258762121200562
Epoch 640, training loss: 12.444311141967773 = 0.3386431634426117 + 2.0 * 6.0528340339660645
Epoch 640, val loss: 0.7194754481315613
Epoch 650, training loss: 12.437819480895996 = 0.32401788234710693 + 2.0 * 6.056900978088379
Epoch 650, val loss: 0.7134756445884705
Epoch 660, training loss: 12.412915229797363 = 0.30987486243247986 + 2.0 * 6.051520347595215
Epoch 660, val loss: 0.7079026103019714
Epoch 670, training loss: 12.401036262512207 = 0.2961519956588745 + 2.0 * 6.0524420738220215
Epoch 670, val loss: 0.7026606202125549
Epoch 680, training loss: 12.381237030029297 = 0.28283068537712097 + 2.0 * 6.049203395843506
Epoch 680, val loss: 0.6977930665016174
Epoch 690, training loss: 12.36802864074707 = 0.2699595093727112 + 2.0 * 6.049034595489502
Epoch 690, val loss: 0.693326473236084
Epoch 700, training loss: 12.360795974731445 = 0.25749415159225464 + 2.0 * 6.0516510009765625
Epoch 700, val loss: 0.6891526579856873
Epoch 710, training loss: 12.340563774108887 = 0.2454860806465149 + 2.0 * 6.047538757324219
Epoch 710, val loss: 0.685386061668396
Epoch 720, training loss: 12.325526237487793 = 0.23387092351913452 + 2.0 * 6.045827865600586
Epoch 720, val loss: 0.6820329427719116
Epoch 730, training loss: 12.315031051635742 = 0.22264136373996735 + 2.0 * 6.046195030212402
Epoch 730, val loss: 0.6790745258331299
Epoch 740, training loss: 12.305273056030273 = 0.2118605375289917 + 2.0 * 6.046706199645996
Epoch 740, val loss: 0.6764799952507019
Epoch 750, training loss: 12.300474166870117 = 0.20162759721279144 + 2.0 * 6.0494232177734375
Epoch 750, val loss: 0.6743184328079224
Epoch 760, training loss: 12.279306411743164 = 0.191890150308609 + 2.0 * 6.043708324432373
Epoch 760, val loss: 0.6724627614021301
Epoch 770, training loss: 12.265875816345215 = 0.18255111575126648 + 2.0 * 6.041662216186523
Epoch 770, val loss: 0.6709591150283813
Epoch 780, training loss: 12.260394096374512 = 0.17363615334033966 + 2.0 * 6.043378829956055
Epoch 780, val loss: 0.6698706150054932
Epoch 790, training loss: 12.248722076416016 = 0.16516900062561035 + 2.0 * 6.041776657104492
Epoch 790, val loss: 0.669154703617096
Epoch 800, training loss: 12.243091583251953 = 0.15718503296375275 + 2.0 * 6.0429534912109375
Epoch 800, val loss: 0.6687870025634766
Epoch 810, training loss: 12.231270790100098 = 0.14961639046669006 + 2.0 * 6.04082727432251
Epoch 810, val loss: 0.6687184572219849
Epoch 820, training loss: 12.218648910522461 = 0.14244984090328217 + 2.0 * 6.038099765777588
Epoch 820, val loss: 0.6689994931221008
Epoch 830, training loss: 12.21167278289795 = 0.13566593825817108 + 2.0 * 6.038003444671631
Epoch 830, val loss: 0.6695860624313354
Epoch 840, training loss: 12.209563255310059 = 0.12923914194107056 + 2.0 * 6.040162086486816
Epoch 840, val loss: 0.6704346537590027
Epoch 850, training loss: 12.200495719909668 = 0.12322316318750381 + 2.0 * 6.038636207580566
Epoch 850, val loss: 0.6715561747550964
Epoch 860, training loss: 12.193695068359375 = 0.11755811423063278 + 2.0 * 6.0380682945251465
Epoch 860, val loss: 0.6729459762573242
Epoch 870, training loss: 12.18196964263916 = 0.11222369223833084 + 2.0 * 6.034873008728027
Epoch 870, val loss: 0.6744900941848755
Epoch 880, training loss: 12.174986839294434 = 0.10717456787824631 + 2.0 * 6.033905982971191
Epoch 880, val loss: 0.6763074994087219
Epoch 890, training loss: 12.178905487060547 = 0.10240785032510757 + 2.0 * 6.0382490158081055
Epoch 890, val loss: 0.6782976984977722
Epoch 900, training loss: 12.168815612792969 = 0.09792360663414001 + 2.0 * 6.0354461669921875
Epoch 900, val loss: 0.6803562641143799
Epoch 910, training loss: 12.165775299072266 = 0.09372745454311371 + 2.0 * 6.03602409362793
Epoch 910, val loss: 0.6826525330543518
Epoch 920, training loss: 12.153009414672852 = 0.0897744670510292 + 2.0 * 6.031617641448975
Epoch 920, val loss: 0.6850030422210693
Epoch 930, training loss: 12.146595001220703 = 0.08604248613119125 + 2.0 * 6.030276298522949
Epoch 930, val loss: 0.6874836087226868
Epoch 940, training loss: 12.140393257141113 = 0.08250635117292404 + 2.0 * 6.0289435386657715
Epoch 940, val loss: 0.6900924444198608
Epoch 950, training loss: 12.155890464782715 = 0.07915635406970978 + 2.0 * 6.03836727142334
Epoch 950, val loss: 0.6928018927574158
Epoch 960, training loss: 12.136287689208984 = 0.07601635903120041 + 2.0 * 6.030135631561279
Epoch 960, val loss: 0.6955047845840454
Epoch 970, training loss: 12.12990665435791 = 0.07307003438472748 + 2.0 * 6.02841854095459
Epoch 970, val loss: 0.6983243823051453
Epoch 980, training loss: 12.12364387512207 = 0.0702727884054184 + 2.0 * 6.02668571472168
Epoch 980, val loss: 0.701214611530304
Epoch 990, training loss: 12.140583038330078 = 0.06762629002332687 + 2.0 * 6.036478519439697
Epoch 990, val loss: 0.7041583061218262
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.6015
Flip ASR: 0.5289/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.68506622314453 = 1.9374101161956787 + 2.0 * 8.373827934265137
Epoch 0, val loss: 1.9363007545471191
Epoch 10, training loss: 18.673919677734375 = 1.9275398254394531 + 2.0 * 8.373189926147461
Epoch 10, val loss: 1.9264872074127197
Epoch 20, training loss: 18.654022216796875 = 1.9151208400726318 + 2.0 * 8.369450569152832
Epoch 20, val loss: 1.913635492324829
Epoch 30, training loss: 18.586389541625977 = 1.8983662128448486 + 2.0 * 8.344011306762695
Epoch 30, val loss: 1.8960009813308716
Epoch 40, training loss: 18.151262283325195 = 1.878303050994873 + 2.0 * 8.136479377746582
Epoch 40, val loss: 1.875186562538147
Epoch 50, training loss: 16.224485397338867 = 1.8572394847869873 + 2.0 * 7.18362283706665
Epoch 50, val loss: 1.853472352027893
Epoch 60, training loss: 15.509056091308594 = 1.844772219657898 + 2.0 * 6.832141876220703
Epoch 60, val loss: 1.8413598537445068
Epoch 70, training loss: 15.155879020690918 = 1.8353168964385986 + 2.0 * 6.660281181335449
Epoch 70, val loss: 1.831440806388855
Epoch 80, training loss: 14.917518615722656 = 1.8259820938110352 + 2.0 * 6.5457682609558105
Epoch 80, val loss: 1.8216801881790161
Epoch 90, training loss: 14.741226196289062 = 1.8167915344238281 + 2.0 * 6.462217330932617
Epoch 90, val loss: 1.8121871948242188
Epoch 100, training loss: 14.619804382324219 = 1.8078687191009521 + 2.0 * 6.405967712402344
Epoch 100, val loss: 1.8030322790145874
Epoch 110, training loss: 14.52357292175293 = 1.7995578050613403 + 2.0 * 6.3620076179504395
Epoch 110, val loss: 1.7944858074188232
Epoch 120, training loss: 14.450090408325195 = 1.7915472984313965 + 2.0 * 6.3292717933654785
Epoch 120, val loss: 1.7861829996109009
Epoch 130, training loss: 14.390398025512695 = 1.7834689617156982 + 2.0 * 6.303464412689209
Epoch 130, val loss: 1.7779126167297363
Epoch 140, training loss: 14.33587646484375 = 1.7750670909881592 + 2.0 * 6.280404567718506
Epoch 140, val loss: 1.7695914506912231
Epoch 150, training loss: 14.287317276000977 = 1.7662243843078613 + 2.0 * 6.260546684265137
Epoch 150, val loss: 1.7611533403396606
Epoch 160, training loss: 14.237507820129395 = 1.7567002773284912 + 2.0 * 6.240403652191162
Epoch 160, val loss: 1.7523664236068726
Epoch 170, training loss: 14.193394660949707 = 1.746156930923462 + 2.0 * 6.223618984222412
Epoch 170, val loss: 1.7428969144821167
Epoch 180, training loss: 14.15178108215332 = 1.7341526746749878 + 2.0 * 6.2088141441345215
Epoch 180, val loss: 1.7323540449142456
Epoch 190, training loss: 14.113215446472168 = 1.7203537225723267 + 2.0 * 6.196430683135986
Epoch 190, val loss: 1.7204879522323608
Epoch 200, training loss: 14.074389457702637 = 1.7045633792877197 + 2.0 * 6.184913158416748
Epoch 200, val loss: 1.707040786743164
Epoch 210, training loss: 14.035778045654297 = 1.6862785816192627 + 2.0 * 6.174749851226807
Epoch 210, val loss: 1.6916468143463135
Epoch 220, training loss: 13.996994018554688 = 1.6651827096939087 + 2.0 * 6.165905475616455
Epoch 220, val loss: 1.6739578247070312
Epoch 230, training loss: 13.953781127929688 = 1.6407700777053833 + 2.0 * 6.156505584716797
Epoch 230, val loss: 1.6536519527435303
Epoch 240, training loss: 13.913468360900879 = 1.6123327016830444 + 2.0 * 6.150568008422852
Epoch 240, val loss: 1.6300804615020752
Epoch 250, training loss: 13.864384651184082 = 1.580093264579773 + 2.0 * 6.14214563369751
Epoch 250, val loss: 1.6034908294677734
Epoch 260, training loss: 13.815190315246582 = 1.5432404279708862 + 2.0 * 6.135974884033203
Epoch 260, val loss: 1.5729442834854126
Epoch 270, training loss: 13.76218318939209 = 1.5013421773910522 + 2.0 * 6.130420684814453
Epoch 270, val loss: 1.538351058959961
Epoch 280, training loss: 13.710695266723633 = 1.4561913013458252 + 2.0 * 6.127252101898193
Epoch 280, val loss: 1.5016462802886963
Epoch 290, training loss: 13.651848793029785 = 1.4096401929855347 + 2.0 * 6.1211042404174805
Epoch 290, val loss: 1.4645631313323975
Epoch 300, training loss: 13.59721851348877 = 1.361928105354309 + 2.0 * 6.117645263671875
Epoch 300, val loss: 1.4272812604904175
Epoch 310, training loss: 13.545816421508789 = 1.3137129545211792 + 2.0 * 6.11605167388916
Epoch 310, val loss: 1.390388011932373
Epoch 320, training loss: 13.487627029418945 = 1.2663129568099976 + 2.0 * 6.110657215118408
Epoch 320, val loss: 1.3547786474227905
Epoch 330, training loss: 13.433931350708008 = 1.2196764945983887 + 2.0 * 6.1071271896362305
Epoch 330, val loss: 1.3203620910644531
Epoch 340, training loss: 13.38916015625 = 1.1738057136535645 + 2.0 * 6.107677459716797
Epoch 340, val loss: 1.2867556810379028
Epoch 350, training loss: 13.332071304321289 = 1.1291356086730957 + 2.0 * 6.101468086242676
Epoch 350, val loss: 1.2541388273239136
Epoch 360, training loss: 13.282357215881348 = 1.085361123085022 + 2.0 * 6.0984978675842285
Epoch 360, val loss: 1.2222503423690796
Epoch 370, training loss: 13.239765167236328 = 1.0424678325653076 + 2.0 * 6.098648548126221
Epoch 370, val loss: 1.1908485889434814
Epoch 380, training loss: 13.188966751098633 = 1.000854730606079 + 2.0 * 6.094056129455566
Epoch 380, val loss: 1.160260558128357
Epoch 390, training loss: 13.141885757446289 = 0.9603407979011536 + 2.0 * 6.09077262878418
Epoch 390, val loss: 1.1302392482757568
Epoch 400, training loss: 13.104652404785156 = 0.9208741784095764 + 2.0 * 6.091888904571533
Epoch 400, val loss: 1.100844144821167
Epoch 410, training loss: 13.05630111694336 = 0.8832077980041504 + 2.0 * 6.086546421051025
Epoch 410, val loss: 1.0725665092468262
Epoch 420, training loss: 13.016471862792969 = 0.8471822142601013 + 2.0 * 6.084644794464111
Epoch 420, val loss: 1.0456098318099976
Epoch 430, training loss: 12.983664512634277 = 0.812759518623352 + 2.0 * 6.085452556610107
Epoch 430, val loss: 1.0198734998703003
Epoch 440, training loss: 12.939830780029297 = 0.7801337242126465 + 2.0 * 6.079848289489746
Epoch 440, val loss: 0.9956340789794922
Epoch 450, training loss: 12.906468391418457 = 0.7490137219429016 + 2.0 * 6.0787272453308105
Epoch 450, val loss: 0.9727851152420044
Epoch 460, training loss: 12.873703956604004 = 0.7194516062736511 + 2.0 * 6.0771260261535645
Epoch 460, val loss: 0.9513821005821228
Epoch 470, training loss: 12.83911418914795 = 0.6915712952613831 + 2.0 * 6.0737714767456055
Epoch 470, val loss: 0.9316580891609192
Epoch 480, training loss: 12.80846118927002 = 0.664907693862915 + 2.0 * 6.071776866912842
Epoch 480, val loss: 0.9132309556007385
Epoch 490, training loss: 12.787007331848145 = 0.6392101645469666 + 2.0 * 6.073898792266846
Epoch 490, val loss: 0.8959977030754089
Epoch 500, training loss: 12.757458686828613 = 0.6145391464233398 + 2.0 * 6.071459770202637
Epoch 500, val loss: 0.8799883127212524
Epoch 510, training loss: 12.726899147033691 = 0.5909030437469482 + 2.0 * 6.067997932434082
Epoch 510, val loss: 0.8652347922325134
Epoch 520, training loss: 12.69935131072998 = 0.5678154230117798 + 2.0 * 6.065767765045166
Epoch 520, val loss: 0.8513628840446472
Epoch 530, training loss: 12.673198699951172 = 0.545075535774231 + 2.0 * 6.064061641693115
Epoch 530, val loss: 0.8383095264434814
Epoch 540, training loss: 12.657417297363281 = 0.5226653814315796 + 2.0 * 6.067376136779785
Epoch 540, val loss: 0.8259538412094116
Epoch 550, training loss: 12.626169204711914 = 0.500863254070282 + 2.0 * 6.062653064727783
Epoch 550, val loss: 0.8144431710243225
Epoch 560, training loss: 12.599360466003418 = 0.4794132113456726 + 2.0 * 6.05997371673584
Epoch 560, val loss: 0.8037100434303284
Epoch 570, training loss: 12.57530403137207 = 0.4582328796386719 + 2.0 * 6.058535575866699
Epoch 570, val loss: 0.7935243248939514
Epoch 580, training loss: 12.565359115600586 = 0.4373365342617035 + 2.0 * 6.064011096954346
Epoch 580, val loss: 0.7839730978012085
Epoch 590, training loss: 12.533208847045898 = 0.4170572757720947 + 2.0 * 6.058075904846191
Epoch 590, val loss: 0.7751051187515259
Epoch 600, training loss: 12.507708549499512 = 0.3972501754760742 + 2.0 * 6.055229187011719
Epoch 600, val loss: 0.7671266198158264
Epoch 610, training loss: 12.486328125 = 0.3779570758342743 + 2.0 * 6.054185390472412
Epoch 610, val loss: 0.7598044276237488
Epoch 620, training loss: 12.47323989868164 = 0.35929155349731445 + 2.0 * 6.056973934173584
Epoch 620, val loss: 0.7531209588050842
Epoch 630, training loss: 12.446779251098633 = 0.3414681851863861 + 2.0 * 6.0526556968688965
Epoch 630, val loss: 0.747373104095459
Epoch 640, training loss: 12.427116394042969 = 0.32434114813804626 + 2.0 * 6.051387786865234
Epoch 640, val loss: 0.7424876093864441
Epoch 650, training loss: 12.406389236450195 = 0.3077959418296814 + 2.0 * 6.049296855926514
Epoch 650, val loss: 0.7381598353385925
Epoch 660, training loss: 12.401498794555664 = 0.291860967874527 + 2.0 * 6.054819107055664
Epoch 660, val loss: 0.7344448566436768
Epoch 670, training loss: 12.376079559326172 = 0.2768199145793915 + 2.0 * 6.0496296882629395
Epoch 670, val loss: 0.7313320636749268
Epoch 680, training loss: 12.355828285217285 = 0.2625855803489685 + 2.0 * 6.046621322631836
Epoch 680, val loss: 0.7290480136871338
Epoch 690, training loss: 12.33958911895752 = 0.24908894300460815 + 2.0 * 6.045249938964844
Epoch 690, val loss: 0.7272345423698425
Epoch 700, training loss: 12.336189270019531 = 0.23620429635047913 + 2.0 * 6.049992561340332
Epoch 700, val loss: 0.7259072661399841
Epoch 710, training loss: 12.318817138671875 = 0.2241031974554062 + 2.0 * 6.047357082366943
Epoch 710, val loss: 0.7251392602920532
Epoch 720, training loss: 12.299379348754883 = 0.21269212663173676 + 2.0 * 6.043343544006348
Epoch 720, val loss: 0.7248418927192688
Epoch 730, training loss: 12.28558349609375 = 0.2019215226173401 + 2.0 * 6.041831016540527
Epoch 730, val loss: 0.7249471545219421
Epoch 740, training loss: 12.276247024536133 = 0.19175183773040771 + 2.0 * 6.042247772216797
Epoch 740, val loss: 0.7254971861839294
Epoch 750, training loss: 12.266192436218262 = 0.18217357993125916 + 2.0 * 6.042009353637695
Epoch 750, val loss: 0.7263453006744385
Epoch 760, training loss: 12.25798225402832 = 0.17322883009910583 + 2.0 * 6.042376518249512
Epoch 760, val loss: 0.7274646759033203
Epoch 770, training loss: 12.240838050842285 = 0.16486947238445282 + 2.0 * 6.037984371185303
Epoch 770, val loss: 0.7290920615196228
Epoch 780, training loss: 12.230392456054688 = 0.15697740018367767 + 2.0 * 6.036707401275635
Epoch 780, val loss: 0.7309488654136658
Epoch 790, training loss: 12.220810890197754 = 0.1495145559310913 + 2.0 * 6.035648345947266
Epoch 790, val loss: 0.733086347579956
Epoch 800, training loss: 12.234893798828125 = 0.14246714115142822 + 2.0 * 6.046213150024414
Epoch 800, val loss: 0.7354449033737183
Epoch 810, training loss: 12.207958221435547 = 0.13591912388801575 + 2.0 * 6.036019325256348
Epoch 810, val loss: 0.7379404902458191
Epoch 820, training loss: 12.20114517211914 = 0.12975502014160156 + 2.0 * 6.0356950759887695
Epoch 820, val loss: 0.740833580493927
Epoch 830, training loss: 12.191446304321289 = 0.12393148988485336 + 2.0 * 6.033757209777832
Epoch 830, val loss: 0.7436709403991699
Epoch 840, training loss: 12.182538986206055 = 0.11843867599964142 + 2.0 * 6.032050132751465
Epoch 840, val loss: 0.7467544674873352
Epoch 850, training loss: 12.1898775100708 = 0.1132422611117363 + 2.0 * 6.038317680358887
Epoch 850, val loss: 0.750026285648346
Epoch 860, training loss: 12.178085327148438 = 0.10837984085083008 + 2.0 * 6.034852504730225
Epoch 860, val loss: 0.7532266974449158
Epoch 870, training loss: 12.164190292358398 = 0.10380801558494568 + 2.0 * 6.030190944671631
Epoch 870, val loss: 0.7568186521530151
Epoch 880, training loss: 12.15895938873291 = 0.09947121888399124 + 2.0 * 6.0297441482543945
Epoch 880, val loss: 0.7603732347488403
Epoch 890, training loss: 12.157297134399414 = 0.0953674241900444 + 2.0 * 6.0309648513793945
Epoch 890, val loss: 0.7639369368553162
Epoch 900, training loss: 12.148893356323242 = 0.09149163216352463 + 2.0 * 6.028700828552246
Epoch 900, val loss: 0.7676471471786499
Epoch 910, training loss: 12.142033576965332 = 0.08780431002378464 + 2.0 * 6.0271148681640625
Epoch 910, val loss: 0.771486759185791
Epoch 920, training loss: 12.145904541015625 = 0.08430913090705872 + 2.0 * 6.030797481536865
Epoch 920, val loss: 0.7752261161804199
Epoch 930, training loss: 12.135431289672852 = 0.08101850748062134 + 2.0 * 6.0272064208984375
Epoch 930, val loss: 0.7791078686714172
Epoch 940, training loss: 12.133441925048828 = 0.07789450883865356 + 2.0 * 6.027773857116699
Epoch 940, val loss: 0.7830552458763123
Epoch 950, training loss: 12.125337600708008 = 0.07495162636041641 + 2.0 * 6.025193214416504
Epoch 950, val loss: 0.7869359850883484
Epoch 960, training loss: 12.134798049926758 = 0.07215201109647751 + 2.0 * 6.031322956085205
Epoch 960, val loss: 0.7908070683479309
Epoch 970, training loss: 12.118732452392578 = 0.06952431052923203 + 2.0 * 6.024603843688965
Epoch 970, val loss: 0.794677734375
Epoch 980, training loss: 12.111983299255371 = 0.06702034920454025 + 2.0 * 6.022481441497803
Epoch 980, val loss: 0.7986543774604797
Epoch 990, training loss: 12.107885360717773 = 0.06463439762592316 + 2.0 * 6.021625518798828
Epoch 990, val loss: 0.802549421787262
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.7491
Flip ASR: 0.7111/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.687223434448242 = 1.9396711587905884 + 2.0 * 8.37377643585205
Epoch 0, val loss: 1.9422465562820435
Epoch 10, training loss: 18.675081253051758 = 1.9299594163894653 + 2.0 * 8.372560501098633
Epoch 10, val loss: 1.9321273565292358
Epoch 20, training loss: 18.649219512939453 = 1.9182121753692627 + 2.0 * 8.365503311157227
Epoch 20, val loss: 1.919198751449585
Epoch 30, training loss: 18.54608154296875 = 1.903427004814148 + 2.0 * 8.321327209472656
Epoch 30, val loss: 1.9026174545288086
Epoch 40, training loss: 17.964921951293945 = 1.887247920036316 + 2.0 * 8.038837432861328
Epoch 40, val loss: 1.8845280408859253
Epoch 50, training loss: 16.514936447143555 = 1.8699003458023071 + 2.0 * 7.3225178718566895
Epoch 50, val loss: 1.865466833114624
Epoch 60, training loss: 15.778724670410156 = 1.8565900325775146 + 2.0 * 6.961067199707031
Epoch 60, val loss: 1.8514416217803955
Epoch 70, training loss: 15.35816478729248 = 1.8459004163742065 + 2.0 * 6.756132125854492
Epoch 70, val loss: 1.839439868927002
Epoch 80, training loss: 15.04853343963623 = 1.8351755142211914 + 2.0 * 6.6066789627075195
Epoch 80, val loss: 1.8282489776611328
Epoch 90, training loss: 14.886839866638184 = 1.826880693435669 + 2.0 * 6.529979705810547
Epoch 90, val loss: 1.819172739982605
Epoch 100, training loss: 14.747069358825684 = 1.8182408809661865 + 2.0 * 6.464414119720459
Epoch 100, val loss: 1.809677004814148
Epoch 110, training loss: 14.615995407104492 = 1.8097773790359497 + 2.0 * 6.403109073638916
Epoch 110, val loss: 1.7999813556671143
Epoch 120, training loss: 14.51626968383789 = 1.8021063804626465 + 2.0 * 6.357081890106201
Epoch 120, val loss: 1.7911957502365112
Epoch 130, training loss: 14.437405586242676 = 1.7941347360610962 + 2.0 * 6.3216352462768555
Epoch 130, val loss: 1.7822149991989136
Epoch 140, training loss: 14.370187759399414 = 1.7857145071029663 + 2.0 * 6.292236804962158
Epoch 140, val loss: 1.773009181022644
Epoch 150, training loss: 14.314474105834961 = 1.7769855260849 + 2.0 * 6.268744468688965
Epoch 150, val loss: 1.763755202293396
Epoch 160, training loss: 14.26740550994873 = 1.767874836921692 + 2.0 * 6.249765396118164
Epoch 160, val loss: 1.754507303237915
Epoch 170, training loss: 14.223729133605957 = 1.7582331895828247 + 2.0 * 6.232748031616211
Epoch 170, val loss: 1.745079517364502
Epoch 180, training loss: 14.188102722167969 = 1.747886300086975 + 2.0 * 6.2201080322265625
Epoch 180, val loss: 1.735366702079773
Epoch 190, training loss: 14.147170066833496 = 1.7365511655807495 + 2.0 * 6.2053093910217285
Epoch 190, val loss: 1.7252557277679443
Epoch 200, training loss: 14.107547760009766 = 1.7241597175598145 + 2.0 * 6.1916937828063965
Epoch 200, val loss: 1.7145148515701294
Epoch 210, training loss: 14.070218086242676 = 1.7102251052856445 + 2.0 * 6.179996490478516
Epoch 210, val loss: 1.7028573751449585
Epoch 220, training loss: 14.035371780395508 = 1.6942696571350098 + 2.0 * 6.170551300048828
Epoch 220, val loss: 1.689726710319519
Epoch 230, training loss: 14.00328254699707 = 1.6755176782608032 + 2.0 * 6.163882255554199
Epoch 230, val loss: 1.6748640537261963
Epoch 240, training loss: 13.964012145996094 = 1.6540107727050781 + 2.0 * 6.155000686645508
Epoch 240, val loss: 1.6578459739685059
Epoch 250, training loss: 13.923957824707031 = 1.629065990447998 + 2.0 * 6.1474456787109375
Epoch 250, val loss: 1.6382936239242554
Epoch 260, training loss: 13.883500099182129 = 1.5999337434768677 + 2.0 * 6.141783237457275
Epoch 260, val loss: 1.6156107187271118
Epoch 270, training loss: 13.842880249023438 = 1.5659810304641724 + 2.0 * 6.138449668884277
Epoch 270, val loss: 1.589337944984436
Epoch 280, training loss: 13.791065216064453 = 1.5270359516143799 + 2.0 * 6.132014751434326
Epoch 280, val loss: 1.5590696334838867
Epoch 290, training loss: 13.736438751220703 = 1.4823378324508667 + 2.0 * 6.127050399780273
Epoch 290, val loss: 1.524307131767273
Epoch 300, training loss: 13.68930721282959 = 1.4318196773529053 + 2.0 * 6.128743648529053
Epoch 300, val loss: 1.4849190711975098
Epoch 310, training loss: 13.618000984191895 = 1.376654028892517 + 2.0 * 6.120673656463623
Epoch 310, val loss: 1.4420855045318604
Epoch 320, training loss: 13.55042839050293 = 1.3175930976867676 + 2.0 * 6.116417407989502
Epoch 320, val loss: 1.3961917161941528
Epoch 330, training loss: 13.483020782470703 = 1.255462408065796 + 2.0 * 6.113779067993164
Epoch 330, val loss: 1.3479870557785034
Epoch 340, training loss: 13.419086456298828 = 1.1924406290054321 + 2.0 * 6.113322734832764
Epoch 340, val loss: 1.2991507053375244
Epoch 350, training loss: 13.346298217773438 = 1.130230188369751 + 2.0 * 6.108034133911133
Epoch 350, val loss: 1.251339316368103
Epoch 360, training loss: 13.291923522949219 = 1.0696892738342285 + 2.0 * 6.111117362976074
Epoch 360, val loss: 1.2049509286880493
Epoch 370, training loss: 13.219301223754883 = 1.0120254755020142 + 2.0 * 6.1036376953125
Epoch 370, val loss: 1.1613163948059082
Epoch 380, training loss: 13.157197952270508 = 0.9574908018112183 + 2.0 * 6.099853515625
Epoch 380, val loss: 1.1202843189239502
Epoch 390, training loss: 13.106696128845215 = 0.906073272228241 + 2.0 * 6.100311279296875
Epoch 390, val loss: 1.0820883512496948
Epoch 400, training loss: 13.049383163452148 = 0.8588263988494873 + 2.0 * 6.095278263092041
Epoch 400, val loss: 1.0469460487365723
Epoch 410, training loss: 12.998133659362793 = 0.8152810335159302 + 2.0 * 6.091426372528076
Epoch 410, val loss: 1.0150115489959717
Epoch 420, training loss: 12.951986312866211 = 0.7747853994369507 + 2.0 * 6.0886006355285645
Epoch 420, val loss: 0.9855991005897522
Epoch 430, training loss: 12.949515342712402 = 0.7372294664382935 + 2.0 * 6.106142997741699
Epoch 430, val loss: 0.9585955739021301
Epoch 440, training loss: 12.878582000732422 = 0.7030985951423645 + 2.0 * 6.087741851806641
Epoch 440, val loss: 0.934565544128418
Epoch 450, training loss: 12.838699340820312 = 0.6717557311058044 + 2.0 * 6.083471775054932
Epoch 450, val loss: 0.9128671288490295
Epoch 460, training loss: 12.802328109741211 = 0.6424674987792969 + 2.0 * 6.079930305480957
Epoch 460, val loss: 0.8929786086082458
Epoch 470, training loss: 12.768499374389648 = 0.614769458770752 + 2.0 * 6.076865196228027
Epoch 470, val loss: 0.8747291564941406
Epoch 480, training loss: 12.738792419433594 = 0.588678240776062 + 2.0 * 6.075057029724121
Epoch 480, val loss: 0.8581262826919556
Epoch 490, training loss: 12.711760520935059 = 0.5642874240875244 + 2.0 * 6.073736667633057
Epoch 490, val loss: 0.8431618809700012
Epoch 500, training loss: 12.684870719909668 = 0.5411363840103149 + 2.0 * 6.071866989135742
Epoch 500, val loss: 0.8295501470565796
Epoch 510, training loss: 12.657889366149902 = 0.5189625024795532 + 2.0 * 6.06946325302124
Epoch 510, val loss: 0.8171769976615906
Epoch 520, training loss: 12.646648406982422 = 0.4977376163005829 + 2.0 * 6.074455261230469
Epoch 520, val loss: 0.8059557676315308
Epoch 530, training loss: 12.613739967346191 = 0.4775937795639038 + 2.0 * 6.068073272705078
Epoch 530, val loss: 0.7959731221199036
Epoch 540, training loss: 12.587804794311523 = 0.4582744836807251 + 2.0 * 6.064764976501465
Epoch 540, val loss: 0.7869724631309509
Epoch 550, training loss: 12.578478813171387 = 0.4395982027053833 + 2.0 * 6.0694403648376465
Epoch 550, val loss: 0.7787808775901794
Epoch 560, training loss: 12.547914505004883 = 0.4218153953552246 + 2.0 * 6.063049793243408
Epoch 560, val loss: 0.7714151740074158
Epoch 570, training loss: 12.526552200317383 = 0.404700368642807 + 2.0 * 6.0609259605407715
Epoch 570, val loss: 0.7648540139198303
Epoch 580, training loss: 12.50639533996582 = 0.3881584405899048 + 2.0 * 6.059118270874023
Epoch 580, val loss: 0.7589901685714722
Epoch 590, training loss: 12.490730285644531 = 0.3721368610858917 + 2.0 * 6.059296607971191
Epoch 590, val loss: 0.7537067532539368
Epoch 600, training loss: 12.475611686706543 = 0.35673031210899353 + 2.0 * 6.059440612792969
Epoch 600, val loss: 0.7490449547767639
Epoch 610, training loss: 12.453571319580078 = 0.34194812178611755 + 2.0 * 6.055811405181885
Epoch 610, val loss: 0.7450075149536133
Epoch 620, training loss: 12.43991756439209 = 0.32765331864356995 + 2.0 * 6.0561323165893555
Epoch 620, val loss: 0.741415798664093
Epoch 630, training loss: 12.421119689941406 = 0.31387877464294434 + 2.0 * 6.053620338439941
Epoch 630, val loss: 0.7382197976112366
Epoch 640, training loss: 12.405440330505371 = 0.3005809485912323 + 2.0 * 6.052429676055908
Epoch 640, val loss: 0.7354512810707092
Epoch 650, training loss: 12.400809288024902 = 0.2876869738101959 + 2.0 * 6.05656099319458
Epoch 650, val loss: 0.7330218553543091
Epoch 660, training loss: 12.376959800720215 = 0.2753056585788727 + 2.0 * 6.0508270263671875
Epoch 660, val loss: 0.7308323383331299
Epoch 670, training loss: 12.362082481384277 = 0.26333463191986084 + 2.0 * 6.049374103546143
Epoch 670, val loss: 0.7289673686027527
Epoch 680, training loss: 12.353166580200195 = 0.2516855001449585 + 2.0 * 6.050740718841553
Epoch 680, val loss: 0.727349042892456
Epoch 690, training loss: 12.338224411010742 = 0.24049712717533112 + 2.0 * 6.048863410949707
Epoch 690, val loss: 0.7258612513542175
Epoch 700, training loss: 12.324166297912598 = 0.2297392338514328 + 2.0 * 6.047213554382324
Epoch 700, val loss: 0.7247267961502075
Epoch 710, training loss: 12.310702323913574 = 0.21937522292137146 + 2.0 * 6.045663356781006
Epoch 710, val loss: 0.723829448223114
Epoch 720, training loss: 12.313713073730469 = 0.20937460660934448 + 2.0 * 6.052169322967529
Epoch 720, val loss: 0.72314453125
Epoch 730, training loss: 12.288928985595703 = 0.19986434280872345 + 2.0 * 6.044532299041748
Epoch 730, val loss: 0.7225939631462097
Epoch 740, training loss: 12.276461601257324 = 0.19081540405750275 + 2.0 * 6.042823314666748
Epoch 740, val loss: 0.7224094867706299
Epoch 750, training loss: 12.265942573547363 = 0.1821497082710266 + 2.0 * 6.041896343231201
Epoch 750, val loss: 0.7224642038345337
Epoch 760, training loss: 12.260918617248535 = 0.17386379837989807 + 2.0 * 6.043527603149414
Epoch 760, val loss: 0.722746729850769
Epoch 770, training loss: 12.260289192199707 = 0.16602656245231628 + 2.0 * 6.047131538391113
Epoch 770, val loss: 0.7232462763786316
Epoch 780, training loss: 12.23913288116455 = 0.1586187779903412 + 2.0 * 6.040256977081299
Epoch 780, val loss: 0.7241243720054626
Epoch 790, training loss: 12.228922843933105 = 0.15157711505889893 + 2.0 * 6.038672924041748
Epoch 790, val loss: 0.725263237953186
Epoch 800, training loss: 12.220328330993652 = 0.14486247301101685 + 2.0 * 6.03773307800293
Epoch 800, val loss: 0.7265270352363586
Epoch 810, training loss: 12.21288776397705 = 0.13846519589424133 + 2.0 * 6.0372114181518555
Epoch 810, val loss: 0.7279787063598633
Epoch 820, training loss: 12.227662086486816 = 0.13238312304019928 + 2.0 * 6.0476393699646
Epoch 820, val loss: 0.7295623421669006
Epoch 830, training loss: 12.207301139831543 = 0.1266813576221466 + 2.0 * 6.040309906005859
Epoch 830, val loss: 0.7313079237937927
Epoch 840, training loss: 12.193347930908203 = 0.12128178775310516 + 2.0 * 6.0360331535339355
Epoch 840, val loss: 0.7332996726036072
Epoch 850, training loss: 12.184549331665039 = 0.11613023281097412 + 2.0 * 6.034209728240967
Epoch 850, val loss: 0.7353687286376953
Epoch 860, training loss: 12.188288688659668 = 0.11122608184814453 + 2.0 * 6.038531303405762
Epoch 860, val loss: 0.7375229597091675
Epoch 870, training loss: 12.17813777923584 = 0.10655961185693741 + 2.0 * 6.0357890129089355
Epoch 870, val loss: 0.7397900223731995
Epoch 880, training loss: 12.16796875 = 0.10215085744857788 + 2.0 * 6.032908916473389
Epoch 880, val loss: 0.7422276735305786
Epoch 890, training loss: 12.162125587463379 = 0.09793270379304886 + 2.0 * 6.0320963859558105
Epoch 890, val loss: 0.7448453903198242
Epoch 900, training loss: 12.165093421936035 = 0.0939250960946083 + 2.0 * 6.035583972930908
Epoch 900, val loss: 0.7474424242973328
Epoch 910, training loss: 12.151494026184082 = 0.09014427661895752 + 2.0 * 6.030674934387207
Epoch 910, val loss: 0.7500689625740051
Epoch 920, training loss: 12.145419120788574 = 0.08653882145881653 + 2.0 * 6.029439926147461
Epoch 920, val loss: 0.7528087496757507
Epoch 930, training loss: 12.14066219329834 = 0.08310399949550629 + 2.0 * 6.028779029846191
Epoch 930, val loss: 0.7555750608444214
Epoch 940, training loss: 12.141050338745117 = 0.07982899248600006 + 2.0 * 6.03061056137085
Epoch 940, val loss: 0.7583584189414978
Epoch 950, training loss: 12.135985374450684 = 0.07672759145498276 + 2.0 * 6.029628753662109
Epoch 950, val loss: 0.7611297965049744
Epoch 960, training loss: 12.130178451538086 = 0.07380379736423492 + 2.0 * 6.028187274932861
Epoch 960, val loss: 0.7640190124511719
Epoch 970, training loss: 12.125476837158203 = 0.07101830095052719 + 2.0 * 6.027229309082031
Epoch 970, val loss: 0.7669848799705505
Epoch 980, training loss: 12.123575210571289 = 0.068357452750206 + 2.0 * 6.027608871459961
Epoch 980, val loss: 0.7699677348136902
Epoch 990, training loss: 12.118077278137207 = 0.06583404541015625 + 2.0 * 6.026121616363525
Epoch 990, val loss: 0.7729138135910034
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8111
Overall ASR: 0.8413
Flip ASR: 0.8089/225 nodes
The final ASR:0.73063, 0.09878, Accuracy:0.82840, 0.01222
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11620])
remove edge: torch.Size([2, 9560])
updated graph: torch.Size([2, 10624])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9852
Flip ASR: 0.9822/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9926
Flip ASR: 0.9911/225 nodes
The final ASR:0.98524, 0.00603, Accuracy:0.83333, 0.00800
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.5 der2:0 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 18.696762084960938 = 1.9491099119186401 + 2.0 * 8.373826026916504
Epoch 0, val loss: 1.9397562742233276
Epoch 10, training loss: 18.685508728027344 = 1.9390790462493896 + 2.0 * 8.373214721679688
Epoch 10, val loss: 1.930673360824585
Epoch 20, training loss: 18.665821075439453 = 1.9265199899673462 + 2.0 * 8.369650840759277
Epoch 20, val loss: 1.9190555810928345
Epoch 30, training loss: 18.59986114501953 = 1.9095019102096558 + 2.0 * 8.345179557800293
Epoch 30, val loss: 1.9032574892044067
Epoch 40, training loss: 18.183183670043945 = 1.8889086246490479 + 2.0 * 8.147137641906738
Epoch 40, val loss: 1.8843674659729004
Epoch 50, training loss: 16.523290634155273 = 1.8657060861587524 + 2.0 * 7.328792095184326
Epoch 50, val loss: 1.8627798557281494
Epoch 60, training loss: 15.78335952758789 = 1.849048376083374 + 2.0 * 6.967155456542969
Epoch 60, val loss: 1.8487058877944946
Epoch 70, training loss: 15.176700592041016 = 1.8398911952972412 + 2.0 * 6.668404579162598
Epoch 70, val loss: 1.841271162033081
Epoch 80, training loss: 14.890253067016602 = 1.8291112184524536 + 2.0 * 6.530570983886719
Epoch 80, val loss: 1.8319321870803833
Epoch 90, training loss: 14.718878746032715 = 1.8192243576049805 + 2.0 * 6.449827194213867
Epoch 90, val loss: 1.823381781578064
Epoch 100, training loss: 14.608325004577637 = 1.8098323345184326 + 2.0 * 6.3992462158203125
Epoch 100, val loss: 1.8151572942733765
Epoch 110, training loss: 14.522472381591797 = 1.8016502857208252 + 2.0 * 6.360411167144775
Epoch 110, val loss: 1.8078629970550537
Epoch 120, training loss: 14.450860977172852 = 1.7937712669372559 + 2.0 * 6.328544616699219
Epoch 120, val loss: 1.8007302284240723
Epoch 130, training loss: 14.385886192321777 = 1.7861974239349365 + 2.0 * 6.299844264984131
Epoch 130, val loss: 1.793696641921997
Epoch 140, training loss: 14.325952529907227 = 1.7786985635757446 + 2.0 * 6.273626804351807
Epoch 140, val loss: 1.7867313623428345
Epoch 150, training loss: 14.271997451782227 = 1.7708783149719238 + 2.0 * 6.250559329986572
Epoch 150, val loss: 1.7796655893325806
Epoch 160, training loss: 14.22442626953125 = 1.7622653245925903 + 2.0 * 6.231080532073975
Epoch 160, val loss: 1.7721154689788818
Epoch 170, training loss: 14.184188842773438 = 1.7524793148040771 + 2.0 * 6.215854644775391
Epoch 170, val loss: 1.7637461423873901
Epoch 180, training loss: 14.142602920532227 = 1.7413301467895508 + 2.0 * 6.200636386871338
Epoch 180, val loss: 1.7545714378356934
Epoch 190, training loss: 14.10558795928955 = 1.728600263595581 + 2.0 * 6.188493728637695
Epoch 190, val loss: 1.74420964717865
Epoch 200, training loss: 14.069847106933594 = 1.7138932943344116 + 2.0 * 6.177977085113525
Epoch 200, val loss: 1.7323552370071411
Epoch 210, training loss: 14.03424072265625 = 1.6967743635177612 + 2.0 * 6.1687331199646
Epoch 210, val loss: 1.7187360525131226
Epoch 220, training loss: 13.996725082397461 = 1.676839828491211 + 2.0 * 6.159942626953125
Epoch 220, val loss: 1.7028982639312744
Epoch 230, training loss: 13.95859146118164 = 1.6533490419387817 + 2.0 * 6.152621269226074
Epoch 230, val loss: 1.6841466426849365
Epoch 240, training loss: 13.920186042785645 = 1.6254608631134033 + 2.0 * 6.14736270904541
Epoch 240, val loss: 1.661733865737915
Epoch 250, training loss: 13.874043464660645 = 1.5923194885253906 + 2.0 * 6.140861988067627
Epoch 250, val loss: 1.6350281238555908
Epoch 260, training loss: 13.825213432312012 = 1.5539495944976807 + 2.0 * 6.135632038116455
Epoch 260, val loss: 1.604140281677246
Epoch 270, training loss: 13.771051406860352 = 1.510308027267456 + 2.0 * 6.130371570587158
Epoch 270, val loss: 1.568953037261963
Epoch 280, training loss: 13.715439796447754 = 1.4611319303512573 + 2.0 * 6.1271538734436035
Epoch 280, val loss: 1.5291239023208618
Epoch 290, training loss: 13.652617454528809 = 1.4080268144607544 + 2.0 * 6.122295379638672
Epoch 290, val loss: 1.4861102104187012
Epoch 300, training loss: 13.589327812194824 = 1.3528364896774292 + 2.0 * 6.118245601654053
Epoch 300, val loss: 1.4419026374816895
Epoch 310, training loss: 13.52545166015625 = 1.2971317768096924 + 2.0 * 6.114160060882568
Epoch 310, val loss: 1.3972954750061035
Epoch 320, training loss: 13.464879035949707 = 1.2419205904006958 + 2.0 * 6.11147928237915
Epoch 320, val loss: 1.3534818887710571
Epoch 330, training loss: 13.406700134277344 = 1.1889004707336426 + 2.0 * 6.10890007019043
Epoch 330, val loss: 1.311998724937439
Epoch 340, training loss: 13.34814453125 = 1.1385703086853027 + 2.0 * 6.104787349700928
Epoch 340, val loss: 1.272923231124878
Epoch 350, training loss: 13.293960571289062 = 1.090509295463562 + 2.0 * 6.1017255783081055
Epoch 350, val loss: 1.236194372177124
Epoch 360, training loss: 13.240160942077637 = 1.0443439483642578 + 2.0 * 6.0979084968566895
Epoch 360, val loss: 1.2014490365982056
Epoch 370, training loss: 13.190947532653809 = 0.9996926784515381 + 2.0 * 6.095627307891846
Epoch 370, val loss: 1.168138861656189
Epoch 380, training loss: 13.143813133239746 = 0.9563716053962708 + 2.0 * 6.09372091293335
Epoch 380, val loss: 1.1364269256591797
Epoch 390, training loss: 13.09527587890625 = 0.9145435094833374 + 2.0 * 6.090366363525391
Epoch 390, val loss: 1.1056971549987793
Epoch 400, training loss: 13.048938751220703 = 0.8739287257194519 + 2.0 * 6.087504863739014
Epoch 400, val loss: 1.0762184858322144
Epoch 410, training loss: 13.004382133483887 = 0.8343695402145386 + 2.0 * 6.085006237030029
Epoch 410, val loss: 1.0476148128509521
Epoch 420, training loss: 12.960506439208984 = 0.7954637408256531 + 2.0 * 6.082521438598633
Epoch 420, val loss: 1.0196791887283325
Epoch 430, training loss: 12.917623519897461 = 0.7570800185203552 + 2.0 * 6.0802717208862305
Epoch 430, val loss: 0.9922293424606323
Epoch 440, training loss: 12.883099555969238 = 0.7193278074264526 + 2.0 * 6.081885814666748
Epoch 440, val loss: 0.9653385877609253
Epoch 450, training loss: 12.8353910446167 = 0.6825811862945557 + 2.0 * 6.076405048370361
Epoch 450, val loss: 0.9395039081573486
Epoch 460, training loss: 12.797069549560547 = 0.6471221446990967 + 2.0 * 6.0749735832214355
Epoch 460, val loss: 0.9150469899177551
Epoch 470, training loss: 12.757393836975098 = 0.6131131052970886 + 2.0 * 6.072140216827393
Epoch 470, val loss: 0.8918312788009644
Epoch 480, training loss: 12.73208999633789 = 0.5805922150611877 + 2.0 * 6.075748920440674
Epoch 480, val loss: 0.8701806664466858
Epoch 490, training loss: 12.690967559814453 = 0.5499911308288574 + 2.0 * 6.070487976074219
Epoch 490, val loss: 0.850618302822113
Epoch 500, training loss: 12.654213905334473 = 0.5210910439491272 + 2.0 * 6.066561222076416
Epoch 500, val loss: 0.8330221176147461
Epoch 510, training loss: 12.625194549560547 = 0.49381884932518005 + 2.0 * 6.065687656402588
Epoch 510, val loss: 0.8171544075012207
Epoch 520, training loss: 12.603458404541016 = 0.4682615399360657 + 2.0 * 6.067598342895508
Epoch 520, val loss: 0.8032517433166504
Epoch 530, training loss: 12.56928539276123 = 0.4445666968822479 + 2.0 * 6.06235933303833
Epoch 530, val loss: 0.7913540601730347
Epoch 540, training loss: 12.543912887573242 = 0.42232221364974976 + 2.0 * 6.060795307159424
Epoch 540, val loss: 0.7810105085372925
Epoch 550, training loss: 12.522568702697754 = 0.4014829993247986 + 2.0 * 6.060543060302734
Epoch 550, val loss: 0.7721297144889832
Epoch 560, training loss: 12.497903823852539 = 0.38206708431243896 + 2.0 * 6.057918548583984
Epoch 560, val loss: 0.764663815498352
Epoch 570, training loss: 12.474930763244629 = 0.36380934715270996 + 2.0 * 6.05556058883667
Epoch 570, val loss: 0.7584232091903687
Epoch 580, training loss: 12.458677291870117 = 0.34655019640922546 + 2.0 * 6.056063652038574
Epoch 580, val loss: 0.7530176639556885
Epoch 590, training loss: 12.437891960144043 = 0.33023712038993835 + 2.0 * 6.053827285766602
Epoch 590, val loss: 0.7484585642814636
Epoch 600, training loss: 12.430627822875977 = 0.314782053232193 + 2.0 * 6.057922840118408
Epoch 600, val loss: 0.744672417640686
Epoch 610, training loss: 12.402276039123535 = 0.30017513036727905 + 2.0 * 6.051050662994385
Epoch 610, val loss: 0.741593062877655
Epoch 620, training loss: 12.385109901428223 = 0.2861734628677368 + 2.0 * 6.049468040466309
Epoch 620, val loss: 0.7390682101249695
Epoch 630, training loss: 12.369282722473145 = 0.2726806402206421 + 2.0 * 6.0483012199401855
Epoch 630, val loss: 0.7368517518043518
Epoch 640, training loss: 12.353679656982422 = 0.2596806585788727 + 2.0 * 6.046999454498291
Epoch 640, val loss: 0.7349713444709778
Epoch 650, training loss: 12.338469505310059 = 0.24720415472984314 + 2.0 * 6.045632839202881
Epoch 650, val loss: 0.7335929274559021
Epoch 660, training loss: 12.32420825958252 = 0.23510722815990448 + 2.0 * 6.04455041885376
Epoch 660, val loss: 0.7324153184890747
Epoch 670, training loss: 12.320281982421875 = 0.22338293492794037 + 2.0 * 6.048449516296387
Epoch 670, val loss: 0.7313306331634521
Epoch 680, training loss: 12.299545288085938 = 0.21203990280628204 + 2.0 * 6.043752670288086
Epoch 680, val loss: 0.7304937839508057
Epoch 690, training loss: 12.299324989318848 = 0.20114243030548096 + 2.0 * 6.049091339111328
Epoch 690, val loss: 0.7300255298614502
Epoch 700, training loss: 12.274148941040039 = 0.19068987667560577 + 2.0 * 6.04172945022583
Epoch 700, val loss: 0.729590654373169
Epoch 710, training loss: 12.260878562927246 = 0.18071140348911285 + 2.0 * 6.040083408355713
Epoch 710, val loss: 0.7296655774116516
Epoch 720, training loss: 12.248497009277344 = 0.1711760014295578 + 2.0 * 6.038660526275635
Epoch 720, val loss: 0.7297980785369873
Epoch 730, training loss: 12.239079475402832 = 0.16209636628627777 + 2.0 * 6.038491725921631
Epoch 730, val loss: 0.7302543520927429
Epoch 740, training loss: 12.234292984008789 = 0.15354189276695251 + 2.0 * 6.040375709533691
Epoch 740, val loss: 0.7308713793754578
Epoch 750, training loss: 12.219476699829102 = 0.14553403854370117 + 2.0 * 6.036971569061279
Epoch 750, val loss: 0.7318593263626099
Epoch 760, training loss: 12.208816528320312 = 0.13803787529468536 + 2.0 * 6.035389423370361
Epoch 760, val loss: 0.7331299781799316
Epoch 770, training loss: 12.201702117919922 = 0.13098779320716858 + 2.0 * 6.0353569984436035
Epoch 770, val loss: 0.7345011830329895
Epoch 780, training loss: 12.195551872253418 = 0.12441105395555496 + 2.0 * 6.0355706214904785
Epoch 780, val loss: 0.7361788153648376
Epoch 790, training loss: 12.188703536987305 = 0.1182897761464119 + 2.0 * 6.0352067947387695
Epoch 790, val loss: 0.7381739020347595
Epoch 800, training loss: 12.179834365844727 = 0.11258109658956528 + 2.0 * 6.033626556396484
Epoch 800, val loss: 0.7404186129570007
Epoch 810, training loss: 12.169831275939941 = 0.10723882913589478 + 2.0 * 6.031296253204346
Epoch 810, val loss: 0.7427951693534851
Epoch 820, training loss: 12.166790008544922 = 0.10223248600959778 + 2.0 * 6.032278537750244
Epoch 820, val loss: 0.7454277873039246
Epoch 830, training loss: 12.160613059997559 = 0.09756194055080414 + 2.0 * 6.031525611877441
Epoch 830, val loss: 0.7481529712677002
Epoch 840, training loss: 12.1557035446167 = 0.09319571405649185 + 2.0 * 6.031253814697266
Epoch 840, val loss: 0.751132607460022
Epoch 850, training loss: 12.150233268737793 = 0.08912558108568192 + 2.0 * 6.030553817749023
Epoch 850, val loss: 0.7542487978935242
Epoch 860, training loss: 12.142606735229492 = 0.08530942350625992 + 2.0 * 6.028648853302002
Epoch 860, val loss: 0.7574904561042786
Epoch 870, training loss: 12.135859489440918 = 0.08172313868999481 + 2.0 * 6.027068138122559
Epoch 870, val loss: 0.7607954144477844
Epoch 880, training loss: 12.132964134216309 = 0.07834237813949585 + 2.0 * 6.027310848236084
Epoch 880, val loss: 0.7642748355865479
Epoch 890, training loss: 12.13258171081543 = 0.07515862584114075 + 2.0 * 6.028711318969727
Epoch 890, val loss: 0.7677023410797119
Epoch 900, training loss: 12.122965812683105 = 0.07218234986066818 + 2.0 * 6.025391578674316
Epoch 900, val loss: 0.7713826894760132
Epoch 910, training loss: 12.119211196899414 = 0.06937933713197708 + 2.0 * 6.02491569519043
Epoch 910, val loss: 0.775191068649292
Epoch 920, training loss: 12.131488800048828 = 0.06672660261392593 + 2.0 * 6.032381057739258
Epoch 920, val loss: 0.7788330912590027
Epoch 930, training loss: 12.115966796875 = 0.06425108015537262 + 2.0 * 6.025857925415039
Epoch 930, val loss: 0.7825981974601746
Epoch 940, training loss: 12.107006072998047 = 0.06190088391304016 + 2.0 * 6.022552490234375
Epoch 940, val loss: 0.7866676449775696
Epoch 950, training loss: 12.102191925048828 = 0.05966836214065552 + 2.0 * 6.021261692047119
Epoch 950, val loss: 0.7905723452568054
Epoch 960, training loss: 12.098587989807129 = 0.05754318833351135 + 2.0 * 6.020522594451904
Epoch 960, val loss: 0.7945435643196106
Epoch 970, training loss: 12.105270385742188 = 0.05552154406905174 + 2.0 * 6.024874210357666
Epoch 970, val loss: 0.7985776662826538
Epoch 980, training loss: 12.103169441223145 = 0.0536099411547184 + 2.0 * 6.024779796600342
Epoch 980, val loss: 0.8025442957878113
Epoch 990, training loss: 12.090522766113281 = 0.05180466175079346 + 2.0 * 6.019359111785889
Epoch 990, val loss: 0.80681973695755
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.7638
Flip ASR: 0.7200/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.6798095703125 = 1.9325474500656128 + 2.0 * 8.373631477355957
Epoch 0, val loss: 1.9271235466003418
Epoch 10, training loss: 18.66373634338379 = 1.9223530292510986 + 2.0 * 8.370691299438477
Epoch 10, val loss: 1.9167823791503906
Epoch 20, training loss: 18.629409790039062 = 1.9098812341690063 + 2.0 * 8.359764099121094
Epoch 20, val loss: 1.9033417701721191
Epoch 30, training loss: 18.51629066467285 = 1.8936409950256348 + 2.0 * 8.311325073242188
Epoch 30, val loss: 1.8851344585418701
Epoch 40, training loss: 17.748144149780273 = 1.8758656978607178 + 2.0 * 7.9361395835876465
Epoch 40, val loss: 1.864745020866394
Epoch 50, training loss: 16.326505661010742 = 1.8581082820892334 + 2.0 * 7.234199047088623
Epoch 50, val loss: 1.8447010517120361
Epoch 60, training loss: 15.614340782165527 = 1.8442434072494507 + 2.0 * 6.885048866271973
Epoch 60, val loss: 1.8290345668792725
Epoch 70, training loss: 15.208330154418945 = 1.8318657875061035 + 2.0 * 6.688231945037842
Epoch 70, val loss: 1.8159303665161133
Epoch 80, training loss: 14.956624031066895 = 1.8216651678085327 + 2.0 * 6.567479610443115
Epoch 80, val loss: 1.8043310642242432
Epoch 90, training loss: 14.782590866088867 = 1.8122667074203491 + 2.0 * 6.485162258148193
Epoch 90, val loss: 1.7936121225357056
Epoch 100, training loss: 14.634427070617676 = 1.8038939237594604 + 2.0 * 6.415266513824463
Epoch 100, val loss: 1.7841826677322388
Epoch 110, training loss: 14.515853881835938 = 1.7964365482330322 + 2.0 * 6.359708786010742
Epoch 110, val loss: 1.7759323120117188
Epoch 120, training loss: 14.422567367553711 = 1.7892582416534424 + 2.0 * 6.316654682159424
Epoch 120, val loss: 1.767988920211792
Epoch 130, training loss: 14.352527618408203 = 1.7818646430969238 + 2.0 * 6.285331726074219
Epoch 130, val loss: 1.7599499225616455
Epoch 140, training loss: 14.289422988891602 = 1.7742220163345337 + 2.0 * 6.2576003074646
Epoch 140, val loss: 1.7519152164459229
Epoch 150, training loss: 14.237848281860352 = 1.7662663459777832 + 2.0 * 6.235790729522705
Epoch 150, val loss: 1.7441338300704956
Epoch 160, training loss: 14.193414688110352 = 1.7577805519104004 + 2.0 * 6.2178168296813965
Epoch 160, val loss: 1.736255168914795
Epoch 170, training loss: 14.15542221069336 = 1.7484718561172485 + 2.0 * 6.203474998474121
Epoch 170, val loss: 1.7279728651046753
Epoch 180, training loss: 14.119592666625977 = 1.7381632328033447 + 2.0 * 6.1907148361206055
Epoch 180, val loss: 1.7190395593643188
Epoch 190, training loss: 14.08443546295166 = 1.7265187501907349 + 2.0 * 6.178958415985107
Epoch 190, val loss: 1.7092026472091675
Epoch 200, training loss: 14.050187110900879 = 1.7130768299102783 + 2.0 * 6.16855525970459
Epoch 200, val loss: 1.6980280876159668
Epoch 210, training loss: 14.021267890930176 = 1.6973092555999756 + 2.0 * 6.1619791984558105
Epoch 210, val loss: 1.685274600982666
Epoch 220, training loss: 13.985469818115234 = 1.6791579723358154 + 2.0 * 6.15315580368042
Epoch 220, val loss: 1.6706117391586304
Epoch 230, training loss: 13.945320129394531 = 1.6581705808639526 + 2.0 * 6.1435747146606445
Epoch 230, val loss: 1.653686285018921
Epoch 240, training loss: 13.906250953674316 = 1.6334086656570435 + 2.0 * 6.136421203613281
Epoch 240, val loss: 1.6339534521102905
Epoch 250, training loss: 13.864720344543457 = 1.6040719747543335 + 2.0 * 6.130324363708496
Epoch 250, val loss: 1.610506296157837
Epoch 260, training loss: 13.819709777832031 = 1.5693132877349854 + 2.0 * 6.1251983642578125
Epoch 260, val loss: 1.5827383995056152
Epoch 270, training loss: 13.77407169342041 = 1.529132604598999 + 2.0 * 6.122469425201416
Epoch 270, val loss: 1.550716757774353
Epoch 280, training loss: 13.717085838317871 = 1.4838923215866089 + 2.0 * 6.116596698760986
Epoch 280, val loss: 1.514898419380188
Epoch 290, training loss: 13.662896156311035 = 1.4340893030166626 + 2.0 * 6.114403247833252
Epoch 290, val loss: 1.4755847454071045
Epoch 300, training loss: 13.602962493896484 = 1.3812133073806763 + 2.0 * 6.110874652862549
Epoch 300, val loss: 1.4343554973602295
Epoch 310, training loss: 13.540507316589355 = 1.327051043510437 + 2.0 * 6.1067280769348145
Epoch 310, val loss: 1.392212986946106
Epoch 320, training loss: 13.485771179199219 = 1.2727956771850586 + 2.0 * 6.10648775100708
Epoch 320, val loss: 1.3508373498916626
Epoch 330, training loss: 13.422133445739746 = 1.220564842224121 + 2.0 * 6.1007843017578125
Epoch 330, val loss: 1.3112128973007202
Epoch 340, training loss: 13.366536140441895 = 1.17037034034729 + 2.0 * 6.098083019256592
Epoch 340, val loss: 1.2736127376556396
Epoch 350, training loss: 13.318564414978027 = 1.122205138206482 + 2.0 * 6.098179817199707
Epoch 350, val loss: 1.2377053499221802
Epoch 360, training loss: 13.260300636291504 = 1.075964331626892 + 2.0 * 6.09216833114624
Epoch 360, val loss: 1.2035530805587769
Epoch 370, training loss: 13.209939002990723 = 1.0309083461761475 + 2.0 * 6.089515209197998
Epoch 370, val loss: 1.1704851388931274
Epoch 380, training loss: 13.167438507080078 = 0.9869978427886963 + 2.0 * 6.0902204513549805
Epoch 380, val loss: 1.1381338834762573
Epoch 390, training loss: 13.114716529846191 = 0.944783627986908 + 2.0 * 6.084966659545898
Epoch 390, val loss: 1.107060194015503
Epoch 400, training loss: 13.066658020019531 = 0.9038218855857849 + 2.0 * 6.081418037414551
Epoch 400, val loss: 1.0768483877182007
Epoch 410, training loss: 13.027406692504883 = 0.8640990853309631 + 2.0 * 6.081653594970703
Epoch 410, val loss: 1.0476795434951782
Epoch 420, training loss: 12.983108520507812 = 0.8264679908752441 + 2.0 * 6.078320503234863
Epoch 420, val loss: 1.0200307369232178
Epoch 430, training loss: 12.94349479675293 = 0.7913126349449158 + 2.0 * 6.076091289520264
Epoch 430, val loss: 0.9943798780441284
Epoch 440, training loss: 12.905294418334961 = 0.7582132816314697 + 2.0 * 6.073540687561035
Epoch 440, val loss: 0.9705002307891846
Epoch 450, training loss: 12.86793041229248 = 0.7269939184188843 + 2.0 * 6.070468425750732
Epoch 450, val loss: 0.9482141733169556
Epoch 460, training loss: 12.859539031982422 = 0.6975926160812378 + 2.0 * 6.080973148345947
Epoch 460, val loss: 0.9276711940765381
Epoch 470, training loss: 12.811232566833496 = 0.6705332398414612 + 2.0 * 6.07034969329834
Epoch 470, val loss: 0.9089611768722534
Epoch 480, training loss: 12.776286125183105 = 0.6451430916786194 + 2.0 * 6.065571308135986
Epoch 480, val loss: 0.8919172286987305
Epoch 490, training loss: 12.747161865234375 = 0.6210851073265076 + 2.0 * 6.063038349151611
Epoch 490, val loss: 0.8763267993927002
Epoch 500, training loss: 12.739234924316406 = 0.5982262492179871 + 2.0 * 6.070504188537598
Epoch 500, val loss: 0.862046480178833
Epoch 510, training loss: 12.698958396911621 = 0.5766357779502869 + 2.0 * 6.061161518096924
Epoch 510, val loss: 0.8490687608718872
Epoch 520, training loss: 12.673168182373047 = 0.5561016201972961 + 2.0 * 6.058533191680908
Epoch 520, val loss: 0.8372147679328918
Epoch 530, training loss: 12.654788970947266 = 0.5363026261329651 + 2.0 * 6.059243202209473
Epoch 530, val loss: 0.8262256383895874
Epoch 540, training loss: 12.632061958312988 = 0.5172914266586304 + 2.0 * 6.057385444641113
Epoch 540, val loss: 0.8159927725791931
Epoch 550, training loss: 12.604766845703125 = 0.4989011883735657 + 2.0 * 6.0529327392578125
Epoch 550, val loss: 0.8065474629402161
Epoch 560, training loss: 12.586853981018066 = 0.4810068905353546 + 2.0 * 6.052923679351807
Epoch 560, val loss: 0.7976989150047302
Epoch 570, training loss: 12.567390441894531 = 0.46358802914619446 + 2.0 * 6.051901340484619
Epoch 570, val loss: 0.7894012928009033
Epoch 580, training loss: 12.550089836120605 = 0.4466079771518707 + 2.0 * 6.051741123199463
Epoch 580, val loss: 0.781621515750885
Epoch 590, training loss: 12.538839340209961 = 0.4302210509777069 + 2.0 * 6.054309368133545
Epoch 590, val loss: 0.7742986083030701
Epoch 600, training loss: 12.50915241241455 = 0.41426530480384827 + 2.0 * 6.047443389892578
Epoch 600, val loss: 0.7675154805183411
Epoch 610, training loss: 12.48941421508789 = 0.39859747886657715 + 2.0 * 6.045408248901367
Epoch 610, val loss: 0.7610710859298706
Epoch 620, training loss: 12.472259521484375 = 0.3831371068954468 + 2.0 * 6.044561386108398
Epoch 620, val loss: 0.7549734711647034
Epoch 630, training loss: 12.455171585083008 = 0.3678808808326721 + 2.0 * 6.04364538192749
Epoch 630, val loss: 0.7490801215171814
Epoch 640, training loss: 12.446231842041016 = 0.35292208194732666 + 2.0 * 6.04665470123291
Epoch 640, val loss: 0.7434511780738831
Epoch 650, training loss: 12.425044059753418 = 0.3385027348995209 + 2.0 * 6.043270587921143
Epoch 650, val loss: 0.738018274307251
Epoch 660, training loss: 12.407866477966309 = 0.3243739902973175 + 2.0 * 6.041746139526367
Epoch 660, val loss: 0.7328206300735474
Epoch 670, training loss: 12.390850067138672 = 0.3105372488498688 + 2.0 * 6.040156364440918
Epoch 670, val loss: 0.727828323841095
Epoch 680, training loss: 12.393954277038574 = 0.2970411479473114 + 2.0 * 6.04845666885376
Epoch 680, val loss: 0.7230303287506104
Epoch 690, training loss: 12.361496925354004 = 0.2839399576187134 + 2.0 * 6.038778305053711
Epoch 690, val loss: 0.7184705138206482
Epoch 700, training loss: 12.346688270568848 = 0.2712368071079254 + 2.0 * 6.037725925445557
Epoch 700, val loss: 0.7141219973564148
Epoch 710, training loss: 12.344429969787598 = 0.2589033246040344 + 2.0 * 6.0427632331848145
Epoch 710, val loss: 0.7099916338920593
Epoch 720, training loss: 12.325833320617676 = 0.24701815843582153 + 2.0 * 6.039407730102539
Epoch 720, val loss: 0.706156313419342
Epoch 730, training loss: 12.305560111999512 = 0.23559196293354034 + 2.0 * 6.034984111785889
Epoch 730, val loss: 0.7025653123855591
Epoch 740, training loss: 12.292980194091797 = 0.22454838454723358 + 2.0 * 6.034215927124023
Epoch 740, val loss: 0.6993007063865662
Epoch 750, training loss: 12.290308952331543 = 0.21390701830387115 + 2.0 * 6.038200855255127
Epoch 750, val loss: 0.696268618106842
Epoch 760, training loss: 12.278388023376465 = 0.2037268429994583 + 2.0 * 6.037330627441406
Epoch 760, val loss: 0.6935546398162842
Epoch 770, training loss: 12.259852409362793 = 0.1941286325454712 + 2.0 * 6.032861709594727
Epoch 770, val loss: 0.6910956501960754
Epoch 780, training loss: 12.254260063171387 = 0.18495571613311768 + 2.0 * 6.034652233123779
Epoch 780, val loss: 0.688974142074585
Epoch 790, training loss: 12.23659610748291 = 0.17622998356819153 + 2.0 * 6.030182838439941
Epoch 790, val loss: 0.6871306300163269
Epoch 800, training loss: 12.227394104003906 = 0.1679276078939438 + 2.0 * 6.029733180999756
Epoch 800, val loss: 0.6855558753013611
Epoch 810, training loss: 12.218639373779297 = 0.16003376245498657 + 2.0 * 6.029302597045898
Epoch 810, val loss: 0.6842615604400635
Epoch 820, training loss: 12.220161437988281 = 0.15253888070583344 + 2.0 * 6.033811092376709
Epoch 820, val loss: 0.6832221150398254
Epoch 830, training loss: 12.206314086914062 = 0.14544180035591125 + 2.0 * 6.030436038970947
Epoch 830, val loss: 0.6824651956558228
Epoch 840, training loss: 12.1990385055542 = 0.13875067234039307 + 2.0 * 6.030143737792969
Epoch 840, val loss: 0.6819814443588257
Epoch 850, training loss: 12.187236785888672 = 0.13243091106414795 + 2.0 * 6.027402877807617
Epoch 850, val loss: 0.6817205548286438
Epoch 860, training loss: 12.178141593933105 = 0.12645265460014343 + 2.0 * 6.025844573974609
Epoch 860, val loss: 0.681704044342041
Epoch 870, training loss: 12.182106018066406 = 0.12080433964729309 + 2.0 * 6.030650615692139
Epoch 870, val loss: 0.6819140315055847
Epoch 880, training loss: 12.167400360107422 = 0.11550004035234451 + 2.0 * 6.025949954986572
Epoch 880, val loss: 0.6822524070739746
Epoch 890, training loss: 12.163249015808105 = 0.11046507209539413 + 2.0 * 6.026391983032227
Epoch 890, val loss: 0.6828550100326538
Epoch 900, training loss: 12.152314186096191 = 0.10571610182523727 + 2.0 * 6.023299217224121
Epoch 900, val loss: 0.6836318373680115
Epoch 910, training loss: 12.145841598510742 = 0.10121747106313705 + 2.0 * 6.022312164306641
Epoch 910, val loss: 0.6845635175704956
Epoch 920, training loss: 12.144012451171875 = 0.0969606265425682 + 2.0 * 6.023525714874268
Epoch 920, val loss: 0.6856738924980164
Epoch 930, training loss: 12.143924713134766 = 0.0929543748497963 + 2.0 * 6.025485038757324
Epoch 930, val loss: 0.6868701577186584
Epoch 940, training loss: 12.132049560546875 = 0.08918163925409317 + 2.0 * 6.0214338302612305
Epoch 940, val loss: 0.6881546378135681
Epoch 950, training loss: 12.126409530639648 = 0.08562073856592178 + 2.0 * 6.020394325256348
Epoch 950, val loss: 0.6896443963050842
Epoch 960, training loss: 12.121086120605469 = 0.08222544193267822 + 2.0 * 6.019430160522461
Epoch 960, val loss: 0.6912292838096619
Epoch 970, training loss: 12.119053840637207 = 0.0789860412478447 + 2.0 * 6.020033836364746
Epoch 970, val loss: 0.6929644346237183
Epoch 980, training loss: 12.116987228393555 = 0.07591217756271362 + 2.0 * 6.020537376403809
Epoch 980, val loss: 0.6947611570358276
Epoch 990, training loss: 12.109833717346191 = 0.07299425452947617 + 2.0 * 6.0184197425842285
Epoch 990, val loss: 0.6966407895088196
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8148
Overall ASR: 0.5092
Flip ASR: 0.4756/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.707447052001953 = 1.9599379301071167 + 2.0 * 8.373754501342773
Epoch 0, val loss: 1.9535287618637085
Epoch 10, training loss: 18.695003509521484 = 1.949361801147461 + 2.0 * 8.372820854187012
Epoch 10, val loss: 1.9433921575546265
Epoch 20, training loss: 18.669925689697266 = 1.935533881187439 + 2.0 * 8.367196083068848
Epoch 20, val loss: 1.929574728012085
Epoch 30, training loss: 18.577693939208984 = 1.916252851486206 + 2.0 * 8.330720901489258
Epoch 30, val loss: 1.9103015661239624
Epoch 40, training loss: 17.97108268737793 = 1.8942431211471558 + 2.0 * 8.038419723510742
Epoch 40, val loss: 1.8892018795013428
Epoch 50, training loss: 16.205711364746094 = 1.8693890571594238 + 2.0 * 7.168160915374756
Epoch 50, val loss: 1.86554753780365
Epoch 60, training loss: 15.501126289367676 = 1.853545069694519 + 2.0 * 6.823790550231934
Epoch 60, val loss: 1.8513082265853882
Epoch 70, training loss: 15.144060134887695 = 1.8423917293548584 + 2.0 * 6.650834083557129
Epoch 70, val loss: 1.8405693769454956
Epoch 80, training loss: 14.917192459106445 = 1.8304152488708496 + 2.0 * 6.543388366699219
Epoch 80, val loss: 1.8294106721878052
Epoch 90, training loss: 14.746461868286133 = 1.8206140995025635 + 2.0 * 6.462924003601074
Epoch 90, val loss: 1.8200582265853882
Epoch 100, training loss: 14.618982315063477 = 1.8111448287963867 + 2.0 * 6.403918743133545
Epoch 100, val loss: 1.8111779689788818
Epoch 110, training loss: 14.511604309082031 = 1.8029425144195557 + 2.0 * 6.354331016540527
Epoch 110, val loss: 1.8031861782073975
Epoch 120, training loss: 14.423579216003418 = 1.7953004837036133 + 2.0 * 6.314139366149902
Epoch 120, val loss: 1.795663595199585
Epoch 130, training loss: 14.351723670959473 = 1.7879838943481445 + 2.0 * 6.281869888305664
Epoch 130, val loss: 1.788312315940857
Epoch 140, training loss: 14.29371166229248 = 1.7803497314453125 + 2.0 * 6.256680965423584
Epoch 140, val loss: 1.7808278799057007
Epoch 150, training loss: 14.24264144897461 = 1.7720293998718262 + 2.0 * 6.2353057861328125
Epoch 150, val loss: 1.7730424404144287
Epoch 160, training loss: 14.199833869934082 = 1.762888789176941 + 2.0 * 6.218472480773926
Epoch 160, val loss: 1.7648746967315674
Epoch 170, training loss: 14.160411834716797 = 1.752819538116455 + 2.0 * 6.203795909881592
Epoch 170, val loss: 1.7561392784118652
Epoch 180, training loss: 14.124120712280273 = 1.7415549755096436 + 2.0 * 6.191282749176025
Epoch 180, val loss: 1.74667489528656
Epoch 190, training loss: 14.086809158325195 = 1.7288585901260376 + 2.0 * 6.1789751052856445
Epoch 190, val loss: 1.7361679077148438
Epoch 200, training loss: 14.05128002166748 = 1.7143205404281616 + 2.0 * 6.168479919433594
Epoch 200, val loss: 1.7243481874465942
Epoch 210, training loss: 14.015481948852539 = 1.6975849866867065 + 2.0 * 6.1589484214782715
Epoch 210, val loss: 1.7108057737350464
Epoch 220, training loss: 13.978203773498535 = 1.678183913230896 + 2.0 * 6.150010108947754
Epoch 220, val loss: 1.6952500343322754
Epoch 230, training loss: 13.94074535369873 = 1.655800223350525 + 2.0 * 6.142472743988037
Epoch 230, val loss: 1.6773043870925903
Epoch 240, training loss: 13.901132583618164 = 1.6297441720962524 + 2.0 * 6.1356940269470215
Epoch 240, val loss: 1.6563986539840698
Epoch 250, training loss: 13.85867691040039 = 1.5993266105651855 + 2.0 * 6.129674911499023
Epoch 250, val loss: 1.631956696510315
Epoch 260, training loss: 13.815839767456055 = 1.5643656253814697 + 2.0 * 6.125737190246582
Epoch 260, val loss: 1.6038026809692383
Epoch 270, training loss: 13.766395568847656 = 1.525399923324585 + 2.0 * 6.120497703552246
Epoch 270, val loss: 1.5723761320114136
Epoch 280, training loss: 13.712915420532227 = 1.4821408987045288 + 2.0 * 6.115387439727783
Epoch 280, val loss: 1.5374678373336792
Epoch 290, training loss: 13.657873153686523 = 1.4348044395446777 + 2.0 * 6.111534595489502
Epoch 290, val loss: 1.4992057085037231
Epoch 300, training loss: 13.600666999816895 = 1.3845778703689575 + 2.0 * 6.108044624328613
Epoch 300, val loss: 1.4589463472366333
Epoch 310, training loss: 13.544525146484375 = 1.3332455158233643 + 2.0 * 6.105639934539795
Epoch 310, val loss: 1.4179350137710571
Epoch 320, training loss: 13.48339557647705 = 1.281080961227417 + 2.0 * 6.101157188415527
Epoch 320, val loss: 1.3763535022735596
Epoch 330, training loss: 13.424077987670898 = 1.228494644165039 + 2.0 * 6.09779167175293
Epoch 330, val loss: 1.3346909284591675
Epoch 340, training loss: 13.3695068359375 = 1.1763925552368164 + 2.0 * 6.096557140350342
Epoch 340, val loss: 1.2936604022979736
Epoch 350, training loss: 13.310700416564941 = 1.125738501548767 + 2.0 * 6.0924811363220215
Epoch 350, val loss: 1.2538135051727295
Epoch 360, training loss: 13.25340461730957 = 1.0760334730148315 + 2.0 * 6.088685512542725
Epoch 360, val loss: 1.214883804321289
Epoch 370, training loss: 13.198616027832031 = 1.0274690389633179 + 2.0 * 6.085573673248291
Epoch 370, val loss: 1.1770025491714478
Epoch 380, training loss: 13.152957916259766 = 0.9808631539344788 + 2.0 * 6.086047172546387
Epoch 380, val loss: 1.1405103206634521
Epoch 390, training loss: 13.098075866699219 = 0.9371180534362793 + 2.0 * 6.080478668212891
Epoch 390, val loss: 1.1070730686187744
Epoch 400, training loss: 13.05179214477539 = 0.8956788778305054 + 2.0 * 6.078056812286377
Epoch 400, val loss: 1.0752933025360107
Epoch 410, training loss: 13.007284164428711 = 0.8562982678413391 + 2.0 * 6.075492858886719
Epoch 410, val loss: 1.0452604293823242
Epoch 420, training loss: 12.969048500061035 = 0.8189502358436584 + 2.0 * 6.075048923492432
Epoch 420, val loss: 1.0172823667526245
Epoch 430, training loss: 12.927490234375 = 0.7841355204582214 + 2.0 * 6.071677207946777
Epoch 430, val loss: 0.9916596412658691
Epoch 440, training loss: 12.89291000366211 = 0.7517643570899963 + 2.0 * 6.070572853088379
Epoch 440, val loss: 0.9683583378791809
Epoch 450, training loss: 12.857356071472168 = 0.7216331958770752 + 2.0 * 6.067861557006836
Epoch 450, val loss: 0.9472808241844177
Epoch 460, training loss: 12.823724746704102 = 0.6932655572891235 + 2.0 * 6.065229415893555
Epoch 460, val loss: 0.9280610680580139
Epoch 470, training loss: 12.807572364807129 = 0.6663733720779419 + 2.0 * 6.070599555969238
Epoch 470, val loss: 0.9105694890022278
Epoch 480, training loss: 12.76783275604248 = 0.6411864161491394 + 2.0 * 6.063323020935059
Epoch 480, val loss: 0.8947917819023132
Epoch 490, training loss: 12.737325668334961 = 0.6171699166297913 + 2.0 * 6.060077667236328
Epoch 490, val loss: 0.8804122805595398
Epoch 500, training loss: 12.710774421691895 = 0.5940284132957458 + 2.0 * 6.058372974395752
Epoch 500, val loss: 0.8670684695243835
Epoch 510, training loss: 12.692044258117676 = 0.5716993808746338 + 2.0 * 6.0601725578308105
Epoch 510, val loss: 0.854669451713562
Epoch 520, training loss: 12.665063858032227 = 0.5503929257392883 + 2.0 * 6.057335376739502
Epoch 520, val loss: 0.8434119820594788
Epoch 530, training loss: 12.638982772827148 = 0.5297408699989319 + 2.0 * 6.054620742797852
Epoch 530, val loss: 0.8329277634620667
Epoch 540, training loss: 12.615923881530762 = 0.509604275226593 + 2.0 * 6.053159713745117
Epoch 540, val loss: 0.8230745196342468
Epoch 550, training loss: 12.60163402557373 = 0.48998039960861206 + 2.0 * 6.055826663970947
Epoch 550, val loss: 0.8138781785964966
Epoch 560, training loss: 12.572107315063477 = 0.4710369110107422 + 2.0 * 6.050535202026367
Epoch 560, val loss: 0.8054036498069763
Epoch 570, training loss: 12.552632331848145 = 0.4525721073150635 + 2.0 * 6.05003023147583
Epoch 570, val loss: 0.7975314855575562
Epoch 580, training loss: 12.542035102844238 = 0.43474167585372925 + 2.0 * 6.053646564483643
Epoch 580, val loss: 0.7901614308357239
Epoch 590, training loss: 12.51463508605957 = 0.4176729917526245 + 2.0 * 6.048480987548828
Epoch 590, val loss: 0.783583402633667
Epoch 600, training loss: 12.493792533874512 = 0.4011662006378174 + 2.0 * 6.046313285827637
Epoch 600, val loss: 0.7774922251701355
Epoch 610, training loss: 12.47472095489502 = 0.3851841390132904 + 2.0 * 6.044768333435059
Epoch 610, val loss: 0.7718932628631592
Epoch 620, training loss: 12.475807189941406 = 0.3697946071624756 + 2.0 * 6.053006172180176
Epoch 620, val loss: 0.7668647766113281
Epoch 630, training loss: 12.44122314453125 = 0.3551976680755615 + 2.0 * 6.043012619018555
Epoch 630, val loss: 0.7623963952064514
Epoch 640, training loss: 12.426456451416016 = 0.3413505554199219 + 2.0 * 6.042552947998047
Epoch 640, val loss: 0.758603036403656
Epoch 650, training loss: 12.410451889038086 = 0.3280980587005615 + 2.0 * 6.041176795959473
Epoch 650, val loss: 0.7552876472473145
Epoch 660, training loss: 12.403225898742676 = 0.315455824136734 + 2.0 * 6.043885231018066
Epoch 660, val loss: 0.7524842023849487
Epoch 670, training loss: 12.388802528381348 = 0.30348822474479675 + 2.0 * 6.042657375335693
Epoch 670, val loss: 0.7502085566520691
Epoch 680, training loss: 12.370821952819824 = 0.2921619117259979 + 2.0 * 6.039330005645752
Epoch 680, val loss: 0.7483969926834106
Epoch 690, training loss: 12.357190132141113 = 0.2813933789730072 + 2.0 * 6.037898540496826
Epoch 690, val loss: 0.747069776058197
Epoch 700, training loss: 12.34819221496582 = 0.2711186408996582 + 2.0 * 6.03853702545166
Epoch 700, val loss: 0.7462038397789001
Epoch 710, training loss: 12.340983390808105 = 0.26132792234420776 + 2.0 * 6.039827823638916
Epoch 710, val loss: 0.7457402348518372
Epoch 720, training loss: 12.324228286743164 = 0.2519403398036957 + 2.0 * 6.036143779754639
Epoch 720, val loss: 0.7455781102180481
Epoch 730, training loss: 12.310382843017578 = 0.24292349815368652 + 2.0 * 6.033729553222656
Epoch 730, val loss: 0.7457807064056396
Epoch 740, training loss: 12.307938575744629 = 0.2342117428779602 + 2.0 * 6.036863327026367
Epoch 740, val loss: 0.7463365793228149
Epoch 750, training loss: 12.299430847167969 = 0.22580944001674652 + 2.0 * 6.036810874938965
Epoch 750, val loss: 0.7470541000366211
Epoch 760, training loss: 12.280773162841797 = 0.21764974296092987 + 2.0 * 6.031561851501465
Epoch 760, val loss: 0.7480457425117493
Epoch 770, training loss: 12.279132843017578 = 0.20970210433006287 + 2.0 * 6.034715175628662
Epoch 770, val loss: 0.7493771910667419
Epoch 780, training loss: 12.267195701599121 = 0.2019805908203125 + 2.0 * 6.032607555389404
Epoch 780, val loss: 0.7508401274681091
Epoch 790, training loss: 12.253731727600098 = 0.19442585110664368 + 2.0 * 6.029653072357178
Epoch 790, val loss: 0.7525248527526855
Epoch 800, training loss: 12.243629455566406 = 0.1870211511850357 + 2.0 * 6.028304100036621
Epoch 800, val loss: 0.7545192837715149
Epoch 810, training loss: 12.241020202636719 = 0.17976583540439606 + 2.0 * 6.030627250671387
Epoch 810, val loss: 0.7567301392555237
Epoch 820, training loss: 12.232491493225098 = 0.17269650101661682 + 2.0 * 6.029897689819336
Epoch 820, val loss: 0.7592397928237915
Epoch 830, training loss: 12.224198341369629 = 0.16581708192825317 + 2.0 * 6.029190540313721
Epoch 830, val loss: 0.7618691325187683
Epoch 840, training loss: 12.211296081542969 = 0.15914440155029297 + 2.0 * 6.026075839996338
Epoch 840, val loss: 0.7647956609725952
Epoch 850, training loss: 12.202573776245117 = 0.15262873470783234 + 2.0 * 6.024972438812256
Epoch 850, val loss: 0.7678802609443665
Epoch 860, training loss: 12.204934120178223 = 0.1463281512260437 + 2.0 * 6.029303073883057
Epoch 860, val loss: 0.7711836695671082
Epoch 870, training loss: 12.195204734802246 = 0.14026115834712982 + 2.0 * 6.027472019195557
Epoch 870, val loss: 0.7746639251708984
Epoch 880, training loss: 12.182247161865234 = 0.134425550699234 + 2.0 * 6.023910999298096
Epoch 880, val loss: 0.7781947255134583
Epoch 890, training loss: 12.180914878845215 = 0.12883390486240387 + 2.0 * 6.026040554046631
Epoch 890, val loss: 0.7819803953170776
Epoch 900, training loss: 12.168672561645508 = 0.12348278611898422 + 2.0 * 6.022594928741455
Epoch 900, val loss: 0.7858872413635254
Epoch 910, training loss: 12.160497665405273 = 0.11832472681999207 + 2.0 * 6.021086692810059
Epoch 910, val loss: 0.7899380922317505
Epoch 920, training loss: 12.156756401062012 = 0.11337880790233612 + 2.0 * 6.021688938140869
Epoch 920, val loss: 0.7941928505897522
Epoch 930, training loss: 12.153109550476074 = 0.10865778475999832 + 2.0 * 6.022225856781006
Epoch 930, val loss: 0.7985764741897583
Epoch 940, training loss: 12.147067070007324 = 0.1041741669178009 + 2.0 * 6.021446228027344
Epoch 940, val loss: 0.8029927611351013
Epoch 950, training loss: 12.140318870544434 = 0.09989773482084274 + 2.0 * 6.0202107429504395
Epoch 950, val loss: 0.8074706196784973
Epoch 960, training loss: 12.142189025878906 = 0.0958375558257103 + 2.0 * 6.0231757164001465
Epoch 960, val loss: 0.812028706073761
Epoch 970, training loss: 12.1315279006958 = 0.09197703748941422 + 2.0 * 6.019775390625
Epoch 970, val loss: 0.816565752029419
Epoch 980, training loss: 12.124715805053711 = 0.08830536901950836 + 2.0 * 6.018205165863037
Epoch 980, val loss: 0.8212246298789978
Epoch 990, training loss: 12.120222091674805 = 0.08480491489171982 + 2.0 * 6.017708778381348
Epoch 990, val loss: 0.8259018659591675
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.8782
Flip ASR: 0.8533/225 nodes
The final ASR:0.71710, 0.15423, Accuracy:0.81728, 0.00175
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11624])
remove edge: torch.Size([2, 9526])
updated graph: torch.Size([2, 10594])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
The final ASR:0.97540, 0.00174, Accuracy:0.83333, 0.00800
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.5 der2:0 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 18.69110107421875 = 1.9432324171066284 + 2.0 * 8.373934745788574
Epoch 0, val loss: 1.943873405456543
Epoch 10, training loss: 18.6812801361084 = 1.933951735496521 + 2.0 * 8.373663902282715
Epoch 10, val loss: 1.9351648092269897
Epoch 20, training loss: 18.666515350341797 = 1.9223783016204834 + 2.0 * 8.372068405151367
Epoch 20, val loss: 1.9239411354064941
Epoch 30, training loss: 18.626529693603516 = 1.9064545631408691 + 2.0 * 8.360037803649902
Epoch 30, val loss: 1.908522129058838
Epoch 40, training loss: 18.414716720581055 = 1.885703682899475 + 2.0 * 8.264506340026855
Epoch 40, val loss: 1.8888689279556274
Epoch 50, training loss: 17.345693588256836 = 1.862058162689209 + 2.0 * 7.741817951202393
Epoch 50, val loss: 1.866845726966858
Epoch 60, training loss: 16.153547286987305 = 1.8447482585906982 + 2.0 * 7.154399394989014
Epoch 60, val loss: 1.8514586687088013
Epoch 70, training loss: 15.382919311523438 = 1.8343713283538818 + 2.0 * 6.774273872375488
Epoch 70, val loss: 1.840681552886963
Epoch 80, training loss: 15.039229393005371 = 1.823909878730774 + 2.0 * 6.607659816741943
Epoch 80, val loss: 1.8306841850280762
Epoch 90, training loss: 14.84373664855957 = 1.8138331174850464 + 2.0 * 6.514951705932617
Epoch 90, val loss: 1.8215535879135132
Epoch 100, training loss: 14.703253746032715 = 1.8046150207519531 + 2.0 * 6.449319362640381
Epoch 100, val loss: 1.8130807876586914
Epoch 110, training loss: 14.587543487548828 = 1.7960402965545654 + 2.0 * 6.395751476287842
Epoch 110, val loss: 1.8048747777938843
Epoch 120, training loss: 14.487221717834473 = 1.7883449792861938 + 2.0 * 6.349438190460205
Epoch 120, val loss: 1.7973488569259644
Epoch 130, training loss: 14.409153938293457 = 1.7810770273208618 + 2.0 * 6.314038276672363
Epoch 130, val loss: 1.7904340028762817
Epoch 140, training loss: 14.352066993713379 = 1.7733620405197144 + 2.0 * 6.2893524169921875
Epoch 140, val loss: 1.7833902835845947
Epoch 150, training loss: 14.30148696899414 = 1.7647252082824707 + 2.0 * 6.268381118774414
Epoch 150, val loss: 1.7758193016052246
Epoch 160, training loss: 14.257397651672363 = 1.7551286220550537 + 2.0 * 6.251134395599365
Epoch 160, val loss: 1.7676657438278198
Epoch 170, training loss: 14.219433784484863 = 1.744452714920044 + 2.0 * 6.237490653991699
Epoch 170, val loss: 1.7588127851486206
Epoch 180, training loss: 14.177003860473633 = 1.7324109077453613 + 2.0 * 6.222296714782715
Epoch 180, val loss: 1.7489999532699585
Epoch 190, training loss: 14.138761520385742 = 1.7186815738677979 + 2.0 * 6.210040092468262
Epoch 190, val loss: 1.7379509210586548
Epoch 200, training loss: 14.101387023925781 = 1.7028001546859741 + 2.0 * 6.199293613433838
Epoch 200, val loss: 1.725327730178833
Epoch 210, training loss: 14.06623649597168 = 1.6843475103378296 + 2.0 * 6.190944671630859
Epoch 210, val loss: 1.7107774019241333
Epoch 220, training loss: 14.02325439453125 = 1.6629596948623657 + 2.0 * 6.180147171020508
Epoch 220, val loss: 1.6938862800598145
Epoch 230, training loss: 13.982464790344238 = 1.63802170753479 + 2.0 * 6.172221660614014
Epoch 230, val loss: 1.6742485761642456
Epoch 240, training loss: 13.945106506347656 = 1.6088314056396484 + 2.0 * 6.168137550354004
Epoch 240, val loss: 1.651146411895752
Epoch 250, training loss: 13.895132064819336 = 1.5751769542694092 + 2.0 * 6.159977436065674
Epoch 250, val loss: 1.6243499517440796
Epoch 260, training loss: 13.84300708770752 = 1.5368732213974 + 2.0 * 6.153067111968994
Epoch 260, val loss: 1.5938835144042969
Epoch 270, training loss: 13.787693977355957 = 1.4937952756881714 + 2.0 * 6.146949291229248
Epoch 270, val loss: 1.559475064277649
Epoch 280, training loss: 13.734457015991211 = 1.4460783004760742 + 2.0 * 6.144189357757568
Epoch 280, val loss: 1.5212347507476807
Epoch 290, training loss: 13.672239303588867 = 1.3947103023529053 + 2.0 * 6.138764381408691
Epoch 290, val loss: 1.4804537296295166
Epoch 300, training loss: 13.609237670898438 = 1.3413944244384766 + 2.0 * 6.1339216232299805
Epoch 300, val loss: 1.4380199909210205
Epoch 310, training loss: 13.546677589416504 = 1.2874953746795654 + 2.0 * 6.12959098815918
Epoch 310, val loss: 1.3954355716705322
Epoch 320, training loss: 13.484293937683105 = 1.2338567972183228 + 2.0 * 6.125218391418457
Epoch 320, val loss: 1.3533481359481812
Epoch 330, training loss: 13.43493366241455 = 1.1814959049224854 + 2.0 * 6.126718997955322
Epoch 330, val loss: 1.312416672706604
Epoch 340, training loss: 13.370002746582031 = 1.131633996963501 + 2.0 * 6.119184494018555
Epoch 340, val loss: 1.2740249633789062
Epoch 350, training loss: 13.315210342407227 = 1.0844107866287231 + 2.0 * 6.1153998374938965
Epoch 350, val loss: 1.23784339427948
Epoch 360, training loss: 13.286249160766602 = 1.039468765258789 + 2.0 * 6.123390197753906
Epoch 360, val loss: 1.2034672498703003
Epoch 370, training loss: 13.222671508789062 = 0.9975592494010925 + 2.0 * 6.112555980682373
Epoch 370, val loss: 1.1717721223831177
Epoch 380, training loss: 13.168980598449707 = 0.95811927318573 + 2.0 * 6.105430603027344
Epoch 380, val loss: 1.1419451236724854
Epoch 390, training loss: 13.125232696533203 = 0.9202514886856079 + 2.0 * 6.102490425109863
Epoch 390, val loss: 1.1132605075836182
Epoch 400, training loss: 13.085862159729004 = 0.8837040662765503 + 2.0 * 6.101078987121582
Epoch 400, val loss: 1.0857226848602295
Epoch 410, training loss: 13.049337387084961 = 0.8488469123840332 + 2.0 * 6.100245475769043
Epoch 410, val loss: 1.0590641498565674
Epoch 420, training loss: 13.005115509033203 = 0.8153824210166931 + 2.0 * 6.094866752624512
Epoch 420, val loss: 1.0337826013565063
Epoch 430, training loss: 12.973320007324219 = 0.7831311225891113 + 2.0 * 6.095094680786133
Epoch 430, val loss: 1.009299874305725
Epoch 440, training loss: 12.931135177612305 = 0.7520538568496704 + 2.0 * 6.089540481567383
Epoch 440, val loss: 0.9858364462852478
Epoch 450, training loss: 12.897509574890137 = 0.722007155418396 + 2.0 * 6.087751388549805
Epoch 450, val loss: 0.9631858468055725
Epoch 460, training loss: 12.869630813598633 = 0.6929740309715271 + 2.0 * 6.0883283615112305
Epoch 460, val loss: 0.9413604140281677
Epoch 470, training loss: 12.835314750671387 = 0.6651278138160706 + 2.0 * 6.0850934982299805
Epoch 470, val loss: 0.9207901954650879
Epoch 480, training loss: 12.80362319946289 = 0.6383569836616516 + 2.0 * 6.082633018493652
Epoch 480, val loss: 0.9013394713401794
Epoch 490, training loss: 12.775062561035156 = 0.6124918460845947 + 2.0 * 6.08128547668457
Epoch 490, val loss: 0.8828750252723694
Epoch 500, training loss: 12.748575210571289 = 0.5876085162162781 + 2.0 * 6.080483436584473
Epoch 500, val loss: 0.8655033111572266
Epoch 510, training loss: 12.71821403503418 = 0.5637944340705872 + 2.0 * 6.077209949493408
Epoch 510, val loss: 0.8491968512535095
Epoch 520, training loss: 12.697966575622559 = 0.5407763719558716 + 2.0 * 6.078595161437988
Epoch 520, val loss: 0.8340480327606201
Epoch 530, training loss: 12.669905662536621 = 0.5187414884567261 + 2.0 * 6.075582027435303
Epoch 530, val loss: 0.8198420405387878
Epoch 540, training loss: 12.641260147094727 = 0.49755334854125977 + 2.0 * 6.071853160858154
Epoch 540, val loss: 0.8066851496696472
Epoch 550, training loss: 12.622568130493164 = 0.4770505428314209 + 2.0 * 6.072758674621582
Epoch 550, val loss: 0.7943409085273743
Epoch 560, training loss: 12.59716510772705 = 0.4572327733039856 + 2.0 * 6.0699663162231445
Epoch 560, val loss: 0.7828903794288635
Epoch 570, training loss: 12.578202247619629 = 0.438203364610672 + 2.0 * 6.0699992179870605
Epoch 570, val loss: 0.7723451256752014
Epoch 580, training loss: 12.557657241821289 = 0.41982004046440125 + 2.0 * 6.068918704986572
Epoch 580, val loss: 0.7625763416290283
Epoch 590, training loss: 12.536879539489746 = 0.4020591080188751 + 2.0 * 6.067409992218018
Epoch 590, val loss: 0.7535830140113831
Epoch 600, training loss: 12.513928413391113 = 0.3849054276943207 + 2.0 * 6.064511299133301
Epoch 600, val loss: 0.7453739047050476
Epoch 610, training loss: 12.499338150024414 = 0.36827313899993896 + 2.0 * 6.065532684326172
Epoch 610, val loss: 0.7377387881278992
Epoch 620, training loss: 12.483335494995117 = 0.3522074222564697 + 2.0 * 6.065564155578613
Epoch 620, val loss: 0.7307712435722351
Epoch 630, training loss: 12.460983276367188 = 0.3368450403213501 + 2.0 * 6.062068939208984
Epoch 630, val loss: 0.7245702743530273
Epoch 640, training loss: 12.443000793457031 = 0.3220149874687195 + 2.0 * 6.060492992401123
Epoch 640, val loss: 0.7190288305282593
Epoch 650, training loss: 12.425581932067871 = 0.3077138066291809 + 2.0 * 6.058934211730957
Epoch 650, val loss: 0.7141191959381104
Epoch 660, training loss: 12.411869049072266 = 0.29388970136642456 + 2.0 * 6.058989524841309
Epoch 660, val loss: 0.7097711563110352
Epoch 670, training loss: 12.400300025939941 = 0.2805613577365875 + 2.0 * 6.059869289398193
Epoch 670, val loss: 0.7059922814369202
Epoch 680, training loss: 12.383747100830078 = 0.26782742142677307 + 2.0 * 6.057960033416748
Epoch 680, val loss: 0.7028494477272034
Epoch 690, training loss: 12.365111351013184 = 0.2555571496486664 + 2.0 * 6.054777145385742
Epoch 690, val loss: 0.7002357244491577
Epoch 700, training loss: 12.355596542358398 = 0.24370071291923523 + 2.0 * 6.055947780609131
Epoch 700, val loss: 0.6981573104858398
Epoch 710, training loss: 12.352012634277344 = 0.23232829570770264 + 2.0 * 6.059842109680176
Epoch 710, val loss: 0.6963353753089905
Epoch 720, training loss: 12.330400466918945 = 0.2215563952922821 + 2.0 * 6.054421901702881
Epoch 720, val loss: 0.6952171921730042
Epoch 730, training loss: 12.315507888793945 = 0.21120145916938782 + 2.0 * 6.05215311050415
Epoch 730, val loss: 0.6944593191146851
Epoch 740, training loss: 12.30205249786377 = 0.20124639570713043 + 2.0 * 6.050403118133545
Epoch 740, val loss: 0.6940851211547852
Epoch 750, training loss: 12.289576530456543 = 0.19164703786373138 + 2.0 * 6.048964977264404
Epoch 750, val loss: 0.6941487193107605
Epoch 760, training loss: 12.306184768676758 = 0.18245172500610352 + 2.0 * 6.061866283416748
Epoch 760, val loss: 0.6945751905441284
Epoch 770, training loss: 12.274650573730469 = 0.17371566593647003 + 2.0 * 6.050467491149902
Epoch 770, val loss: 0.6952396631240845
Epoch 780, training loss: 12.261155128479004 = 0.16547337174415588 + 2.0 * 6.0478410720825195
Epoch 780, val loss: 0.6962551474571228
Epoch 790, training loss: 12.250115394592285 = 0.15757279098033905 + 2.0 * 6.046271324157715
Epoch 790, val loss: 0.6976150870323181
Epoch 800, training loss: 12.241400718688965 = 0.15000519156455994 + 2.0 * 6.0456976890563965
Epoch 800, val loss: 0.6992778778076172
Epoch 810, training loss: 12.242890357971191 = 0.14279009401798248 + 2.0 * 6.050050258636475
Epoch 810, val loss: 0.7011805772781372
Epoch 820, training loss: 12.224014282226562 = 0.13598603010177612 + 2.0 * 6.044013977050781
Epoch 820, val loss: 0.7033734917640686
Epoch 830, training loss: 12.216032028198242 = 0.12949852645397186 + 2.0 * 6.043266773223877
Epoch 830, val loss: 0.7057502269744873
Epoch 840, training loss: 12.214868545532227 = 0.12335439771413803 + 2.0 * 6.045757293701172
Epoch 840, val loss: 0.7082721590995789
Epoch 850, training loss: 12.203027725219727 = 0.11757711321115494 + 2.0 * 6.042725086212158
Epoch 850, val loss: 0.7110510468482971
Epoch 860, training loss: 12.192961692810059 = 0.11210222542285919 + 2.0 * 6.040429592132568
Epoch 860, val loss: 0.7139571905136108
Epoch 870, training loss: 12.188228607177734 = 0.10689213871955872 + 2.0 * 6.04066801071167
Epoch 870, val loss: 0.7170910239219666
Epoch 880, training loss: 12.185042381286621 = 0.10196904093027115 + 2.0 * 6.041536808013916
Epoch 880, val loss: 0.720350980758667
Epoch 890, training loss: 12.17509937286377 = 0.09732648730278015 + 2.0 * 6.038886547088623
Epoch 890, val loss: 0.7236625552177429
Epoch 900, training loss: 12.172649383544922 = 0.09293199330568314 + 2.0 * 6.039858818054199
Epoch 900, val loss: 0.7271621227264404
Epoch 910, training loss: 12.182744979858398 = 0.08880287408828735 + 2.0 * 6.046970844268799
Epoch 910, val loss: 0.7307012677192688
Epoch 920, training loss: 12.162372589111328 = 0.08491106331348419 + 2.0 * 6.038730621337891
Epoch 920, val loss: 0.7342222332954407
Epoch 930, training loss: 12.15295124053955 = 0.08125641942024231 + 2.0 * 6.035847187042236
Epoch 930, val loss: 0.7378670573234558
Epoch 940, training loss: 12.147161483764648 = 0.07778500020503998 + 2.0 * 6.034688472747803
Epoch 940, val loss: 0.7415196895599365
Epoch 950, training loss: 12.15368366241455 = 0.07448823750019073 + 2.0 * 6.039597511291504
Epoch 950, val loss: 0.7453622817993164
Epoch 960, training loss: 12.141311645507812 = 0.0713905543088913 + 2.0 * 6.034960746765137
Epoch 960, val loss: 0.749100923538208
Epoch 970, training loss: 12.134916305541992 = 0.0684637725353241 + 2.0 * 6.033226490020752
Epoch 970, val loss: 0.7529295682907104
Epoch 980, training loss: 12.134410858154297 = 0.06569337844848633 + 2.0 * 6.034358501434326
Epoch 980, val loss: 0.7567602396011353
Epoch 990, training loss: 12.126604080200195 = 0.06307413429021835 + 2.0 * 6.031764984130859
Epoch 990, val loss: 0.7605578899383545
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.7085
Flip ASR: 0.6489/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.69576644897461 = 1.9480072259902954 + 2.0 * 8.373879432678223
Epoch 0, val loss: 1.9525600671768188
Epoch 10, training loss: 18.684009552001953 = 1.9373033046722412 + 2.0 * 8.373353004455566
Epoch 10, val loss: 1.9422118663787842
Epoch 20, training loss: 18.664518356323242 = 1.924322247505188 + 2.0 * 8.370098114013672
Epoch 20, val loss: 1.9292007684707642
Epoch 30, training loss: 18.607545852661133 = 1.9069732427597046 + 2.0 * 8.350286483764648
Epoch 30, val loss: 1.9115403890609741
Epoch 40, training loss: 18.311349868774414 = 1.8858615159988403 + 2.0 * 8.212743759155273
Epoch 40, val loss: 1.8903119564056396
Epoch 50, training loss: 16.88941192626953 = 1.8655352592468262 + 2.0 * 7.511938571929932
Epoch 50, val loss: 1.869456171989441
Epoch 60, training loss: 16.00981903076172 = 1.8492581844329834 + 2.0 * 7.080280780792236
Epoch 60, val loss: 1.8532418012619019
Epoch 70, training loss: 15.496650695800781 = 1.837145447731018 + 2.0 * 6.829752445220947
Epoch 70, val loss: 1.8407825231552124
Epoch 80, training loss: 15.17984390258789 = 1.825134515762329 + 2.0 * 6.67735481262207
Epoch 80, val loss: 1.828015685081482
Epoch 90, training loss: 14.960417747497559 = 1.8140851259231567 + 2.0 * 6.573166370391846
Epoch 90, val loss: 1.8158892393112183
Epoch 100, training loss: 14.830350875854492 = 1.803228735923767 + 2.0 * 6.513561248779297
Epoch 100, val loss: 1.8032810688018799
Epoch 110, training loss: 14.726707458496094 = 1.7928212881088257 + 2.0 * 6.466943264007568
Epoch 110, val loss: 1.7910667657852173
Epoch 120, training loss: 14.625386238098145 = 1.7833331823349 + 2.0 * 6.421026706695557
Epoch 120, val loss: 1.7801965475082397
Epoch 130, training loss: 14.540448188781738 = 1.7747637033462524 + 2.0 * 6.382842063903809
Epoch 130, val loss: 1.7702170610427856
Epoch 140, training loss: 14.457686424255371 = 1.765970230102539 + 2.0 * 6.345858097076416
Epoch 140, val loss: 1.7603527307510376
Epoch 150, training loss: 14.388116836547852 = 1.7566800117492676 + 2.0 * 6.315718173980713
Epoch 150, val loss: 1.7502832412719727
Epoch 160, training loss: 14.334312438964844 = 1.746617078781128 + 2.0 * 6.293847560882568
Epoch 160, val loss: 1.7398782968521118
Epoch 170, training loss: 14.276544570922852 = 1.7355751991271973 + 2.0 * 6.270484447479248
Epoch 170, val loss: 1.7293472290039062
Epoch 180, training loss: 14.225955963134766 = 1.723406195640564 + 2.0 * 6.251275062561035
Epoch 180, val loss: 1.7181755304336548
Epoch 190, training loss: 14.179414749145508 = 1.7097617387771606 + 2.0 * 6.234826564788818
Epoch 190, val loss: 1.705894112586975
Epoch 200, training loss: 14.136669158935547 = 1.6941760778427124 + 2.0 * 6.221246719360352
Epoch 200, val loss: 1.6923208236694336
Epoch 210, training loss: 14.097493171691895 = 1.676357388496399 + 2.0 * 6.210567951202393
Epoch 210, val loss: 1.6770799160003662
Epoch 220, training loss: 14.052375793457031 = 1.6560779809951782 + 2.0 * 6.198148727416992
Epoch 220, val loss: 1.6601978540420532
Epoch 230, training loss: 14.007573127746582 = 1.6328967809677124 + 2.0 * 6.187338352203369
Epoch 230, val loss: 1.6410702466964722
Epoch 240, training loss: 13.961503028869629 = 1.6062654256820679 + 2.0 * 6.177618980407715
Epoch 240, val loss: 1.6193357706069946
Epoch 250, training loss: 13.922082901000977 = 1.5760656595230103 + 2.0 * 6.173008441925049
Epoch 250, val loss: 1.5945103168487549
Epoch 260, training loss: 13.865787506103516 = 1.5423316955566406 + 2.0 * 6.1617279052734375
Epoch 260, val loss: 1.5674819946289062
Epoch 270, training loss: 13.816044807434082 = 1.5054702758789062 + 2.0 * 6.155287265777588
Epoch 270, val loss: 1.5377036333084106
Epoch 280, training loss: 13.764557838439941 = 1.4657037258148193 + 2.0 * 6.1494269371032715
Epoch 280, val loss: 1.5056062936782837
Epoch 290, training loss: 13.722171783447266 = 1.4237487316131592 + 2.0 * 6.149211406707764
Epoch 290, val loss: 1.4722353219985962
Epoch 300, training loss: 13.662643432617188 = 1.3817028999328613 + 2.0 * 6.140470504760742
Epoch 300, val loss: 1.4388384819030762
Epoch 310, training loss: 13.609965324401855 = 1.340134620666504 + 2.0 * 6.134915351867676
Epoch 310, val loss: 1.4063012599945068
Epoch 320, training loss: 13.569202423095703 = 1.299714207649231 + 2.0 * 6.134744167327881
Epoch 320, val loss: 1.3747820854187012
Epoch 330, training loss: 13.515567779541016 = 1.2611982822418213 + 2.0 * 6.127184867858887
Epoch 330, val loss: 1.3455898761749268
Epoch 340, training loss: 13.470450401306152 = 1.2248135805130005 + 2.0 * 6.122818470001221
Epoch 340, val loss: 1.3181655406951904
Epoch 350, training loss: 13.42604923248291 = 1.1896065473556519 + 2.0 * 6.118221282958984
Epoch 350, val loss: 1.2918696403503418
Epoch 360, training loss: 13.38475227355957 = 1.155200481414795 + 2.0 * 6.114776134490967
Epoch 360, val loss: 1.2663249969482422
Epoch 370, training loss: 13.346049308776855 = 1.1212867498397827 + 2.0 * 6.112381458282471
Epoch 370, val loss: 1.241409420967102
Epoch 380, training loss: 13.306044578552246 = 1.087844729423523 + 2.0 * 6.109099864959717
Epoch 380, val loss: 1.2170398235321045
Epoch 390, training loss: 13.267383575439453 = 1.0543502569198608 + 2.0 * 6.1065168380737305
Epoch 390, val loss: 1.1926541328430176
Epoch 400, training loss: 13.228498458862305 = 1.0202982425689697 + 2.0 * 6.104100227355957
Epoch 400, val loss: 1.167891502380371
Epoch 410, training loss: 13.190045356750488 = 0.9856060147285461 + 2.0 * 6.102219581604004
Epoch 410, val loss: 1.1426987648010254
Epoch 420, training loss: 13.15333366394043 = 0.9504759907722473 + 2.0 * 6.101428985595703
Epoch 420, val loss: 1.1172010898590088
Epoch 430, training loss: 13.109725952148438 = 0.9148756265640259 + 2.0 * 6.0974249839782715
Epoch 430, val loss: 1.0911511182785034
Epoch 440, training loss: 13.069365501403809 = 0.8786757588386536 + 2.0 * 6.0953450202941895
Epoch 440, val loss: 1.064629316329956
Epoch 450, training loss: 13.030682563781738 = 0.842063844203949 + 2.0 * 6.094309329986572
Epoch 450, val loss: 1.0375889539718628
Epoch 460, training loss: 12.987327575683594 = 0.8057077527046204 + 2.0 * 6.0908098220825195
Epoch 460, val loss: 1.0107648372650146
Epoch 470, training loss: 12.949745178222656 = 0.7699373364448547 + 2.0 * 6.089903831481934
Epoch 470, val loss: 0.9840468764305115
Epoch 480, training loss: 12.910514831542969 = 0.7347659468650818 + 2.0 * 6.087874412536621
Epoch 480, val loss: 0.9578538537025452
Epoch 490, training loss: 12.873607635498047 = 0.7009708285331726 + 2.0 * 6.086318492889404
Epoch 490, val loss: 0.9325991272926331
Epoch 500, training loss: 12.836775779724121 = 0.6685453057289124 + 2.0 * 6.084115028381348
Epoch 500, val loss: 0.9083290100097656
Epoch 510, training loss: 12.802282333374023 = 0.637648344039917 + 2.0 * 6.082316875457764
Epoch 510, val loss: 0.8853191137313843
Epoch 520, training loss: 12.770767211914062 = 0.6084588766098022 + 2.0 * 6.0811543464660645
Epoch 520, val loss: 0.8638735413551331
Epoch 530, training loss: 12.739021301269531 = 0.5808842778205872 + 2.0 * 6.079068660736084
Epoch 530, val loss: 0.8439267873764038
Epoch 540, training loss: 12.712752342224121 = 0.5549255609512329 + 2.0 * 6.07891321182251
Epoch 540, val loss: 0.8254283666610718
Epoch 550, training loss: 12.681900978088379 = 0.5307167172431946 + 2.0 * 6.075592041015625
Epoch 550, val loss: 0.8087127804756165
Epoch 560, training loss: 12.667908668518066 = 0.5079210996627808 + 2.0 * 6.079993724822998
Epoch 560, val loss: 0.7935066223144531
Epoch 570, training loss: 12.630249977111816 = 0.48655498027801514 + 2.0 * 6.071847438812256
Epoch 570, val loss: 0.7796469330787659
Epoch 580, training loss: 12.607016563415527 = 0.46619269251823425 + 2.0 * 6.0704121589660645
Epoch 580, val loss: 0.7670148015022278
Epoch 590, training loss: 12.583319664001465 = 0.44660839438438416 + 2.0 * 6.068355560302734
Epoch 590, val loss: 0.7554556727409363
Epoch 600, training loss: 12.566572189331055 = 0.42777612805366516 + 2.0 * 6.069397926330566
Epoch 600, val loss: 0.7448511719703674
Epoch 610, training loss: 12.545806884765625 = 0.4097096621990204 + 2.0 * 6.068048477172852
Epoch 610, val loss: 0.7353472709655762
Epoch 620, training loss: 12.522383689880371 = 0.39217624068260193 + 2.0 * 6.065103530883789
Epoch 620, val loss: 0.7266864776611328
Epoch 630, training loss: 12.50394058227539 = 0.3749651312828064 + 2.0 * 6.064487934112549
Epoch 630, val loss: 0.7185386419296265
Epoch 640, training loss: 12.482518196105957 = 0.3581155836582184 + 2.0 * 6.062201499938965
Epoch 640, val loss: 0.7109228372573853
Epoch 650, training loss: 12.46709156036377 = 0.3416367471218109 + 2.0 * 6.062727451324463
Epoch 650, val loss: 0.7040935754776001
Epoch 660, training loss: 12.443593978881836 = 0.3255198895931244 + 2.0 * 6.059037208557129
Epoch 660, val loss: 0.697881281375885
Epoch 670, training loss: 12.429304122924805 = 0.30969157814979553 + 2.0 * 6.0598063468933105
Epoch 670, val loss: 0.6922386288642883
Epoch 680, training loss: 12.415290832519531 = 0.29432618618011475 + 2.0 * 6.060482501983643
Epoch 680, val loss: 0.6871246099472046
Epoch 690, training loss: 12.394441604614258 = 0.27952471375465393 + 2.0 * 6.057458400726318
Epoch 690, val loss: 0.6826972961425781
Epoch 700, training loss: 12.376914024353027 = 0.26525554060935974 + 2.0 * 6.055829048156738
Epoch 700, val loss: 0.6789185404777527
Epoch 710, training loss: 12.362282752990723 = 0.2515474259853363 + 2.0 * 6.055367469787598
Epoch 710, val loss: 0.6758069396018982
Epoch 720, training loss: 12.357138633728027 = 0.2385195791721344 + 2.0 * 6.059309482574463
Epoch 720, val loss: 0.6732566356658936
Epoch 730, training loss: 12.332196235656738 = 0.22618182003498077 + 2.0 * 6.053007125854492
Epoch 730, val loss: 0.671415388584137
Epoch 740, training loss: 12.315574645996094 = 0.2145727276802063 + 2.0 * 6.050500869750977
Epoch 740, val loss: 0.6701265573501587
Epoch 750, training loss: 12.305009841918945 = 0.20358790457248688 + 2.0 * 6.050711154937744
Epoch 750, val loss: 0.6693640351295471
Epoch 760, training loss: 12.2955322265625 = 0.19325430691242218 + 2.0 * 6.051138877868652
Epoch 760, val loss: 0.6690496802330017
Epoch 770, training loss: 12.286438941955566 = 0.18362601101398468 + 2.0 * 6.051406383514404
Epoch 770, val loss: 0.6691662669181824
Epoch 780, training loss: 12.268234252929688 = 0.17459799349308014 + 2.0 * 6.046818256378174
Epoch 780, val loss: 0.6697281002998352
Epoch 790, training loss: 12.257201194763184 = 0.1660938560962677 + 2.0 * 6.045553684234619
Epoch 790, val loss: 0.6706982254981995
Epoch 800, training loss: 12.248055458068848 = 0.15803968906402588 + 2.0 * 6.045007705688477
Epoch 800, val loss: 0.6720247268676758
Epoch 810, training loss: 12.253981590270996 = 0.15040405094623566 + 2.0 * 6.051788806915283
Epoch 810, val loss: 0.6735463738441467
Epoch 820, training loss: 12.231386184692383 = 0.1432049423456192 + 2.0 * 6.044090747833252
Epoch 820, val loss: 0.6754326224327087
Epoch 830, training loss: 12.222886085510254 = 0.13642536103725433 + 2.0 * 6.0432305335998535
Epoch 830, val loss: 0.6776050925254822
Epoch 840, training loss: 12.21303653717041 = 0.13001881539821625 + 2.0 * 6.041508674621582
Epoch 840, val loss: 0.6799682378768921
Epoch 850, training loss: 12.21514892578125 = 0.12390835583209991 + 2.0 * 6.045620441436768
Epoch 850, val loss: 0.6824803948402405
Epoch 860, training loss: 12.198258399963379 = 0.11810562759637833 + 2.0 * 6.04007625579834
Epoch 860, val loss: 0.685239315032959
Epoch 870, training loss: 12.192486763000488 = 0.11260434985160828 + 2.0 * 6.039941310882568
Epoch 870, val loss: 0.6881558895111084
Epoch 880, training loss: 12.188535690307617 = 0.1073874831199646 + 2.0 * 6.040574073791504
Epoch 880, val loss: 0.6912246942520142
Epoch 890, training loss: 12.18017864227295 = 0.10244426876306534 + 2.0 * 6.038866996765137
Epoch 890, val loss: 0.6944727301597595
Epoch 900, training loss: 12.180535316467285 = 0.0977761372923851 + 2.0 * 6.041379451751709
Epoch 900, val loss: 0.6978195905685425
Epoch 910, training loss: 12.172950744628906 = 0.09343614429235458 + 2.0 * 6.039757251739502
Epoch 910, val loss: 0.7011959552764893
Epoch 920, training loss: 12.160120964050293 = 0.08938771486282349 + 2.0 * 6.035366535186768
Epoch 920, val loss: 0.7047547698020935
Epoch 930, training loss: 12.154707908630371 = 0.08559265732765198 + 2.0 * 6.034557819366455
Epoch 930, val loss: 0.7083893418312073
Epoch 940, training loss: 12.15666389465332 = 0.08204219490289688 + 2.0 * 6.03731107711792
Epoch 940, val loss: 0.7121241688728333
Epoch 950, training loss: 12.148722648620605 = 0.0787203386425972 + 2.0 * 6.035001277923584
Epoch 950, val loss: 0.7158417701721191
Epoch 960, training loss: 12.142475128173828 = 0.07560925930738449 + 2.0 * 6.033432960510254
Epoch 960, val loss: 0.7196518778800964
Epoch 970, training loss: 12.137170791625977 = 0.07266537100076675 + 2.0 * 6.032252788543701
Epoch 970, val loss: 0.7235236167907715
Epoch 980, training loss: 12.141751289367676 = 0.06987454742193222 + 2.0 * 6.035938262939453
Epoch 980, val loss: 0.7273944020271301
Epoch 990, training loss: 12.134231567382812 = 0.06725011020898819 + 2.0 * 6.0334906578063965
Epoch 990, val loss: 0.7312481999397278
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.7860
Flip ASR: 0.7511/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.678770065307617 = 1.9310657978057861 + 2.0 * 8.373851776123047
Epoch 0, val loss: 1.9323886632919312
Epoch 10, training loss: 18.668359756469727 = 1.9219005107879639 + 2.0 * 8.37322998046875
Epoch 10, val loss: 1.9227774143218994
Epoch 20, training loss: 18.649150848388672 = 1.9109443426132202 + 2.0 * 8.36910343170166
Epoch 20, val loss: 1.910948634147644
Epoch 30, training loss: 18.572696685791016 = 1.896986961364746 + 2.0 * 8.337855339050293
Epoch 30, val loss: 1.895665168762207
Epoch 40, training loss: 18.041179656982422 = 1.8809950351715088 + 2.0 * 8.080092430114746
Epoch 40, val loss: 1.8784602880477905
Epoch 50, training loss: 16.359336853027344 = 1.8640156984329224 + 2.0 * 7.247660160064697
Epoch 50, val loss: 1.8605858087539673
Epoch 60, training loss: 15.897574424743652 = 1.851045846939087 + 2.0 * 7.023264408111572
Epoch 60, val loss: 1.848021388053894
Epoch 70, training loss: 15.405970573425293 = 1.840693712234497 + 2.0 * 6.7826385498046875
Epoch 70, val loss: 1.8378591537475586
Epoch 80, training loss: 15.047582626342773 = 1.8318946361541748 + 2.0 * 6.60784387588501
Epoch 80, val loss: 1.8286521434783936
Epoch 90, training loss: 14.836113929748535 = 1.8240300416946411 + 2.0 * 6.506042003631592
Epoch 90, val loss: 1.8203188180923462
Epoch 100, training loss: 14.690428733825684 = 1.8161890506744385 + 2.0 * 6.437119960784912
Epoch 100, val loss: 1.8120628595352173
Epoch 110, training loss: 14.587260246276855 = 1.8087921142578125 + 2.0 * 6.3892340660095215
Epoch 110, val loss: 1.8040324449539185
Epoch 120, training loss: 14.50564193725586 = 1.8017675876617432 + 2.0 * 6.351937294006348
Epoch 120, val loss: 1.7964439392089844
Epoch 130, training loss: 14.436075210571289 = 1.7950034141540527 + 2.0 * 6.320535659790039
Epoch 130, val loss: 1.7893329858779907
Epoch 140, training loss: 14.379859924316406 = 1.7882485389709473 + 2.0 * 6.29580545425415
Epoch 140, val loss: 1.7824881076812744
Epoch 150, training loss: 14.327980995178223 = 1.7811956405639648 + 2.0 * 6.273392677307129
Epoch 150, val loss: 1.7754954099655151
Epoch 160, training loss: 14.280845642089844 = 1.7737332582473755 + 2.0 * 6.253556251525879
Epoch 160, val loss: 1.7683336734771729
Epoch 170, training loss: 14.23582649230957 = 1.7657489776611328 + 2.0 * 6.235038757324219
Epoch 170, val loss: 1.7609665393829346
Epoch 180, training loss: 14.194994926452637 = 1.7569156885147095 + 2.0 * 6.219039440155029
Epoch 180, val loss: 1.7530988454818726
Epoch 190, training loss: 14.161155700683594 = 1.7468831539154053 + 2.0 * 6.207136154174805
Epoch 190, val loss: 1.7443673610687256
Epoch 200, training loss: 14.12215805053711 = 1.7353785037994385 + 2.0 * 6.193389892578125
Epoch 200, val loss: 1.7345576286315918
Epoch 210, training loss: 14.088422775268555 = 1.722168207168579 + 2.0 * 6.183127403259277
Epoch 210, val loss: 1.7234503030776978
Epoch 220, training loss: 14.057271003723145 = 1.7068320512771606 + 2.0 * 6.175219535827637
Epoch 220, val loss: 1.710678219795227
Epoch 230, training loss: 14.021261215209961 = 1.6889963150024414 + 2.0 * 6.16613245010376
Epoch 230, val loss: 1.6959871053695679
Epoch 240, training loss: 13.985486030578613 = 1.6681478023529053 + 2.0 * 6.1586689949035645
Epoch 240, val loss: 1.6789638996124268
Epoch 250, training loss: 13.948304176330566 = 1.6436747312545776 + 2.0 * 6.15231466293335
Epoch 250, val loss: 1.6590826511383057
Epoch 260, training loss: 13.909719467163086 = 1.6149322986602783 + 2.0 * 6.147393703460693
Epoch 260, val loss: 1.6358460187911987
Epoch 270, training loss: 13.864494323730469 = 1.5814862251281738 + 2.0 * 6.141504287719727
Epoch 270, val loss: 1.6088109016418457
Epoch 280, training loss: 13.815702438354492 = 1.5425677299499512 + 2.0 * 6.13656759262085
Epoch 280, val loss: 1.5772731304168701
Epoch 290, training loss: 13.766975402832031 = 1.4975712299346924 + 2.0 * 6.134702205657959
Epoch 290, val loss: 1.5406529903411865
Epoch 300, training loss: 13.702409744262695 = 1.4462971687316895 + 2.0 * 6.128056049346924
Epoch 300, val loss: 1.4990078210830688
Epoch 310, training loss: 13.638364791870117 = 1.3889864683151245 + 2.0 * 6.124689102172852
Epoch 310, val loss: 1.452309012413025
Epoch 320, training loss: 13.573220252990723 = 1.326885461807251 + 2.0 * 6.123167514801025
Epoch 320, val loss: 1.4014836549758911
Epoch 330, training loss: 13.499109268188477 = 1.2622007131576538 + 2.0 * 6.118454456329346
Epoch 330, val loss: 1.348502516746521
Epoch 340, training loss: 13.42647933959961 = 1.1961300373077393 + 2.0 * 6.115174770355225
Epoch 340, val loss: 1.2943947315216064
Epoch 350, training loss: 13.36505126953125 = 1.1302785873413086 + 2.0 * 6.117386341094971
Epoch 350, val loss: 1.240686297416687
Epoch 360, training loss: 13.290972709655762 = 1.0675876140594482 + 2.0 * 6.111692428588867
Epoch 360, val loss: 1.1899815797805786
Epoch 370, training loss: 13.221445083618164 = 1.0082941055297852 + 2.0 * 6.1065754890441895
Epoch 370, val loss: 1.142378807067871
Epoch 380, training loss: 13.160116195678711 = 0.9520885348320007 + 2.0 * 6.104013919830322
Epoch 380, val loss: 1.097550630569458
Epoch 390, training loss: 13.104070663452148 = 0.8991988897323608 + 2.0 * 6.102436065673828
Epoch 390, val loss: 1.055722951889038
Epoch 400, training loss: 13.056258201599121 = 0.8499967455863953 + 2.0 * 6.10313081741333
Epoch 400, val loss: 1.0170316696166992
Epoch 410, training loss: 12.997977256774902 = 0.8042766451835632 + 2.0 * 6.096850395202637
Epoch 410, val loss: 0.9811636209487915
Epoch 420, training loss: 12.94759464263916 = 0.7610539793968201 + 2.0 * 6.093270301818848
Epoch 420, val loss: 0.9474875926971436
Epoch 430, training loss: 12.901853561401367 = 0.7199200391769409 + 2.0 * 6.090966701507568
Epoch 430, val loss: 0.915644109249115
Epoch 440, training loss: 12.862974166870117 = 0.6809638738632202 + 2.0 * 6.091005325317383
Epoch 440, val loss: 0.8858569264411926
Epoch 450, training loss: 12.820670127868652 = 0.6445686221122742 + 2.0 * 6.088050842285156
Epoch 450, val loss: 0.8583071231842041
Epoch 460, training loss: 12.778998374938965 = 0.6101415157318115 + 2.0 * 6.084428310394287
Epoch 460, val loss: 0.8327859044075012
Epoch 470, training loss: 12.746430397033691 = 0.5774267315864563 + 2.0 * 6.08450174331665
Epoch 470, val loss: 0.8090552687644958
Epoch 480, training loss: 12.713603973388672 = 0.5467435121536255 + 2.0 * 6.083430290222168
Epoch 480, val loss: 0.7873255014419556
Epoch 490, training loss: 12.674583435058594 = 0.5178402662277222 + 2.0 * 6.078371524810791
Epoch 490, val loss: 0.7676709294319153
Epoch 500, training loss: 12.64680004119873 = 0.49060165882110596 + 2.0 * 6.078099250793457
Epoch 500, val loss: 0.749902606010437
Epoch 510, training loss: 12.617071151733398 = 0.46506476402282715 + 2.0 * 6.076003074645996
Epoch 510, val loss: 0.7338865399360657
Epoch 520, training loss: 12.587226867675781 = 0.44082775712013245 + 2.0 * 6.07319974899292
Epoch 520, val loss: 0.7193647027015686
Epoch 530, training loss: 12.575140953063965 = 0.4177338182926178 + 2.0 * 6.0787034034729
Epoch 530, val loss: 0.7062293887138367
Epoch 540, training loss: 12.535331726074219 = 0.39604276418685913 + 2.0 * 6.069644451141357
Epoch 540, val loss: 0.6944410800933838
Epoch 550, training loss: 12.511615753173828 = 0.37542158365249634 + 2.0 * 6.068097114562988
Epoch 550, val loss: 0.6838767528533936
Epoch 560, training loss: 12.50072193145752 = 0.35578542947769165 + 2.0 * 6.072468280792236
Epoch 560, val loss: 0.6743173003196716
Epoch 570, training loss: 12.472538948059082 = 0.3373139202594757 + 2.0 * 6.067612648010254
Epoch 570, val loss: 0.6658890843391418
Epoch 580, training loss: 12.44782829284668 = 0.319940447807312 + 2.0 * 6.063943862915039
Epoch 580, val loss: 0.6583154201507568
Epoch 590, training loss: 12.428201675415039 = 0.30338290333747864 + 2.0 * 6.062409400939941
Epoch 590, val loss: 0.651587724685669
Epoch 600, training loss: 12.41244888305664 = 0.28758060932159424 + 2.0 * 6.062434196472168
Epoch 600, val loss: 0.6456402540206909
Epoch 610, training loss: 12.393353462219238 = 0.27263715863227844 + 2.0 * 6.060358047485352
Epoch 610, val loss: 0.6404345631599426
Epoch 620, training loss: 12.377835273742676 = 0.25850167870521545 + 2.0 * 6.059666633605957
Epoch 620, val loss: 0.635920524597168
Epoch 630, training loss: 12.363816261291504 = 0.24508938193321228 + 2.0 * 6.05936336517334
Epoch 630, val loss: 0.632089376449585
Epoch 640, training loss: 12.347169876098633 = 0.23240140080451965 + 2.0 * 6.057384014129639
Epoch 640, val loss: 0.6289351582527161
Epoch 650, training loss: 12.337594985961914 = 0.22050625085830688 + 2.0 * 6.058544158935547
Epoch 650, val loss: 0.6263654828071594
Epoch 660, training loss: 12.319940567016602 = 0.2092684954404831 + 2.0 * 6.055335998535156
Epoch 660, val loss: 0.6244292259216309
Epoch 670, training loss: 12.30721664428711 = 0.1986771821975708 + 2.0 * 6.054269790649414
Epoch 670, val loss: 0.6230299472808838
Epoch 680, training loss: 12.29460334777832 = 0.18869814276695251 + 2.0 * 6.052952766418457
Epoch 680, val loss: 0.6221225261688232
Epoch 690, training loss: 12.286754608154297 = 0.17929261922836304 + 2.0 * 6.0537309646606445
Epoch 690, val loss: 0.6216796040534973
Epoch 700, training loss: 12.276158332824707 = 0.17047399282455444 + 2.0 * 6.052842140197754
Epoch 700, val loss: 0.6217078566551208
Epoch 710, training loss: 12.26372241973877 = 0.16217690706253052 + 2.0 * 6.050772666931152
Epoch 710, val loss: 0.62212073802948
Epoch 720, training loss: 12.259751319885254 = 0.15439526736736298 + 2.0 * 6.052678108215332
Epoch 720, val loss: 0.6229029893875122
Epoch 730, training loss: 12.243477821350098 = 0.14710131287574768 + 2.0 * 6.048188209533691
Epoch 730, val loss: 0.6239892840385437
Epoch 740, training loss: 12.233125686645508 = 0.14025692641735077 + 2.0 * 6.04643440246582
Epoch 740, val loss: 0.6253979206085205
Epoch 750, training loss: 12.22424602508545 = 0.13378013670444489 + 2.0 * 6.045232772827148
Epoch 750, val loss: 0.627119243144989
Epoch 760, training loss: 12.235801696777344 = 0.12769261002540588 + 2.0 * 6.0540547370910645
Epoch 760, val loss: 0.6290839314460754
Epoch 770, training loss: 12.21448802947998 = 0.12201078981161118 + 2.0 * 6.046238422393799
Epoch 770, val loss: 0.6311806440353394
Epoch 780, training loss: 12.204482078552246 = 0.1166815310716629 + 2.0 * 6.043900489807129
Epoch 780, val loss: 0.6334806084632874
Epoch 790, training loss: 12.20048713684082 = 0.11165231466293335 + 2.0 * 6.044417381286621
Epoch 790, val loss: 0.6359899044036865
Epoch 800, training loss: 12.190045356750488 = 0.10689647495746613 + 2.0 * 6.041574478149414
Epoch 800, val loss: 0.6386256814002991
Epoch 810, training loss: 12.183956146240234 = 0.10241233557462692 + 2.0 * 6.040771961212158
Epoch 810, val loss: 0.6414121985435486
Epoch 820, training loss: 12.18648624420166 = 0.09816431999206543 + 2.0 * 6.044160842895508
Epoch 820, val loss: 0.64432293176651
Epoch 830, training loss: 12.180566787719727 = 0.09417309612035751 + 2.0 * 6.043196678161621
Epoch 830, val loss: 0.6472558975219727
Epoch 840, training loss: 12.165345191955566 = 0.09041749686002731 + 2.0 * 6.037463665008545
Epoch 840, val loss: 0.6502668857574463
Epoch 850, training loss: 12.16484546661377 = 0.08685809373855591 + 2.0 * 6.038993835449219
Epoch 850, val loss: 0.6534440517425537
Epoch 860, training loss: 12.155364990234375 = 0.08348847925662994 + 2.0 * 6.035938262939453
Epoch 860, val loss: 0.6566429734230042
Epoch 870, training loss: 12.153813362121582 = 0.08030238747596741 + 2.0 * 6.036755561828613
Epoch 870, val loss: 0.6598864793777466
Epoch 880, training loss: 12.146909713745117 = 0.0772790014743805 + 2.0 * 6.034815311431885
Epoch 880, val loss: 0.6631603837013245
Epoch 890, training loss: 12.142629623413086 = 0.07440648227930069 + 2.0 * 6.034111499786377
Epoch 890, val loss: 0.666484534740448
Epoch 900, training loss: 12.138932228088379 = 0.07167746871709824 + 2.0 * 6.033627510070801
Epoch 900, val loss: 0.6698355674743652
Epoch 910, training loss: 12.141778945922852 = 0.06908456236124039 + 2.0 * 6.036347389221191
Epoch 910, val loss: 0.6732022762298584
Epoch 920, training loss: 12.133097648620605 = 0.0666467621922493 + 2.0 * 6.0332255363464355
Epoch 920, val loss: 0.6765391826629639
Epoch 930, training loss: 12.128959655761719 = 0.06433145701885223 + 2.0 * 6.032314300537109
Epoch 930, val loss: 0.6798093914985657
Epoch 940, training loss: 12.12179183959961 = 0.062131378799676895 + 2.0 * 6.029830455780029
Epoch 940, val loss: 0.6831231713294983
Epoch 950, training loss: 12.12393569946289 = 0.06002260744571686 + 2.0 * 6.031956672668457
Epoch 950, val loss: 0.6864607334136963
Epoch 960, training loss: 12.120997428894043 = 0.05803130567073822 + 2.0 * 6.031483173370361
Epoch 960, val loss: 0.6897960305213928
Epoch 970, training loss: 12.115252494812012 = 0.05613815039396286 + 2.0 * 6.029557228088379
Epoch 970, val loss: 0.6929845809936523
Epoch 980, training loss: 12.112757682800293 = 0.0543421134352684 + 2.0 * 6.029207706451416
Epoch 980, val loss: 0.6962077617645264
Epoch 990, training loss: 12.108479499816895 = 0.052621882408857346 + 2.0 * 6.027928829193115
Epoch 990, val loss: 0.699468195438385
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.8782
Flip ASR: 0.8533/225 nodes
The final ASR:0.79090, 0.06938, Accuracy:0.82469, 0.00349
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11642])
remove edge: torch.Size([2, 9582])
updated graph: torch.Size([2, 10668])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9889
Flip ASR: 0.9867/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9852
Flip ASR: 0.9822/225 nodes
The final ASR:0.98401, 0.00460, Accuracy:0.82963, 0.00524
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.5 der2:0 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 18.702022552490234 = 1.954224944114685 + 2.0 * 8.37389850616455
Epoch 0, val loss: 1.953919529914856
Epoch 10, training loss: 18.69034194946289 = 1.9433624744415283 + 2.0 * 8.373489379882812
Epoch 10, val loss: 1.944091558456421
Epoch 20, training loss: 18.670276641845703 = 1.929821252822876 + 2.0 * 8.370227813720703
Epoch 20, val loss: 1.931458592414856
Epoch 30, training loss: 18.601665496826172 = 1.911253809928894 + 2.0 * 8.345206260681152
Epoch 30, val loss: 1.9140831232070923
Epoch 40, training loss: 18.237308502197266 = 1.8881444931030273 + 2.0 * 8.174581527709961
Epoch 40, val loss: 1.8927693367004395
Epoch 50, training loss: 17.443885803222656 = 1.8621206283569336 + 2.0 * 7.7908830642700195
Epoch 50, val loss: 1.8688626289367676
Epoch 60, training loss: 16.919511795043945 = 1.8398035764694214 + 2.0 * 7.539854526519775
Epoch 60, val loss: 1.8487731218338013
Epoch 70, training loss: 16.219223022460938 = 1.8251409530639648 + 2.0 * 7.197040557861328
Epoch 70, val loss: 1.8368802070617676
Epoch 80, training loss: 15.559886932373047 = 1.8171677589416504 + 2.0 * 6.871359825134277
Epoch 80, val loss: 1.830222725868225
Epoch 90, training loss: 15.178544044494629 = 1.8043516874313354 + 2.0 * 6.687096118927002
Epoch 90, val loss: 1.8184785842895508
Epoch 100, training loss: 14.946300506591797 = 1.788153886795044 + 2.0 * 6.579073429107666
Epoch 100, val loss: 1.8046365976333618
Epoch 110, training loss: 14.782343864440918 = 1.7731996774673462 + 2.0 * 6.504571914672852
Epoch 110, val loss: 1.7918869256973267
Epoch 120, training loss: 14.660369873046875 = 1.7585711479187012 + 2.0 * 6.450899600982666
Epoch 120, val loss: 1.7789103984832764
Epoch 130, training loss: 14.550063133239746 = 1.742823839187622 + 2.0 * 6.403619766235352
Epoch 130, val loss: 1.7649801969528198
Epoch 140, training loss: 14.469894409179688 = 1.7254424095153809 + 2.0 * 6.372226238250732
Epoch 140, val loss: 1.7501757144927979
Epoch 150, training loss: 14.383111000061035 = 1.7061387300491333 + 2.0 * 6.338486194610596
Epoch 150, val loss: 1.7341474294662476
Epoch 160, training loss: 14.310108184814453 = 1.6845327615737915 + 2.0 * 6.3127875328063965
Epoch 160, val loss: 1.7166576385498047
Epoch 170, training loss: 14.244686126708984 = 1.6602931022644043 + 2.0 * 6.292196273803711
Epoch 170, val loss: 1.697075605392456
Epoch 180, training loss: 14.179777145385742 = 1.633244276046753 + 2.0 * 6.273266315460205
Epoch 180, val loss: 1.6754752397537231
Epoch 190, training loss: 14.120288848876953 = 1.6034095287322998 + 2.0 * 6.258439540863037
Epoch 190, val loss: 1.6517140865325928
Epoch 200, training loss: 14.059853553771973 = 1.5705211162567139 + 2.0 * 6.24466609954834
Epoch 200, val loss: 1.6256937980651855
Epoch 210, training loss: 14.004711151123047 = 1.534697413444519 + 2.0 * 6.235006809234619
Epoch 210, val loss: 1.597404956817627
Epoch 220, training loss: 13.93973445892334 = 1.496390461921692 + 2.0 * 6.221672058105469
Epoch 220, val loss: 1.567104697227478
Epoch 230, training loss: 13.879980087280273 = 1.4561206102371216 + 2.0 * 6.211929798126221
Epoch 230, val loss: 1.5354145765304565
Epoch 240, training loss: 13.819732666015625 = 1.414169430732727 + 2.0 * 6.202781677246094
Epoch 240, val loss: 1.502742886543274
Epoch 250, training loss: 13.76287841796875 = 1.371266484260559 + 2.0 * 6.19580602645874
Epoch 250, val loss: 1.4697602987289429
Epoch 260, training loss: 13.702778816223145 = 1.328157663345337 + 2.0 * 6.187310695648193
Epoch 260, val loss: 1.4370009899139404
Epoch 270, training loss: 13.652780532836914 = 1.2849503755569458 + 2.0 * 6.183915138244629
Epoch 270, val loss: 1.4046494960784912
Epoch 280, training loss: 13.59395980834961 = 1.2423299551010132 + 2.0 * 6.175815105438232
Epoch 280, val loss: 1.3731051683425903
Epoch 290, training loss: 13.539839744567871 = 1.2005481719970703 + 2.0 * 6.1696457862854
Epoch 290, val loss: 1.3426287174224854
Epoch 300, training loss: 13.48721981048584 = 1.1593666076660156 + 2.0 * 6.163926601409912
Epoch 300, val loss: 1.3129163980484009
Epoch 310, training loss: 13.43925666809082 = 1.1187312602996826 + 2.0 * 6.160262584686279
Epoch 310, val loss: 1.2837717533111572
Epoch 320, training loss: 13.395947456359863 = 1.078896164894104 + 2.0 * 6.158525466918945
Epoch 320, val loss: 1.255629301071167
Epoch 330, training loss: 13.341123580932617 = 1.0402450561523438 + 2.0 * 6.150439262390137
Epoch 330, val loss: 1.2281402349472046
Epoch 340, training loss: 13.292482376098633 = 1.0025529861450195 + 2.0 * 6.144964694976807
Epoch 340, val loss: 1.2013299465179443
Epoch 350, training loss: 13.248044967651367 = 0.965843677520752 + 2.0 * 6.141100883483887
Epoch 350, val loss: 1.175152063369751
Epoch 360, training loss: 13.218485832214355 = 0.9302553534507751 + 2.0 * 6.144115447998047
Epoch 360, val loss: 1.1498699188232422
Epoch 370, training loss: 13.16324234008789 = 0.896294355392456 + 2.0 * 6.133473873138428
Epoch 370, val loss: 1.125483512878418
Epoch 380, training loss: 13.122727394104004 = 0.8636481165885925 + 2.0 * 6.129539489746094
Epoch 380, val loss: 1.10224449634552
Epoch 390, training loss: 13.085694313049316 = 0.8320947289466858 + 2.0 * 6.126799583435059
Epoch 390, val loss: 1.0797353982925415
Epoch 400, training loss: 13.0509614944458 = 0.8016011118888855 + 2.0 * 6.124680042266846
Epoch 400, val loss: 1.0579569339752197
Epoch 410, training loss: 13.011104583740234 = 0.7720755338668823 + 2.0 * 6.119514465332031
Epoch 410, val loss: 1.0370548963546753
Epoch 420, training loss: 12.976802825927734 = 0.7432931065559387 + 2.0 * 6.11675500869751
Epoch 420, val loss: 1.0167043209075928
Epoch 430, training loss: 12.945951461791992 = 0.7150692343711853 + 2.0 * 6.11544132232666
Epoch 430, val loss: 0.9968719482421875
Epoch 440, training loss: 12.911879539489746 = 0.6874279379844666 + 2.0 * 6.1122260093688965
Epoch 440, val loss: 0.9772433638572693
Epoch 450, training loss: 12.87702465057373 = 0.660102903842926 + 2.0 * 6.108460903167725
Epoch 450, val loss: 0.958152711391449
Epoch 460, training loss: 12.844339370727539 = 0.6330485343933105 + 2.0 * 6.105645179748535
Epoch 460, val loss: 0.9393500685691833
Epoch 470, training loss: 12.819202423095703 = 0.6063618063926697 + 2.0 * 6.106420516967773
Epoch 470, val loss: 0.9209101796150208
Epoch 480, training loss: 12.78170108795166 = 0.5801154971122742 + 2.0 * 6.10079288482666
Epoch 480, val loss: 0.9031460881233215
Epoch 490, training loss: 12.751133918762207 = 0.5543810129165649 + 2.0 * 6.098376274108887
Epoch 490, val loss: 0.8861247301101685
Epoch 500, training loss: 12.722368240356445 = 0.5292710065841675 + 2.0 * 6.096548557281494
Epoch 500, val loss: 0.8698974251747131
Epoch 510, training loss: 12.701034545898438 = 0.5047841668128967 + 2.0 * 6.098124980926514
Epoch 510, val loss: 0.8547856211662292
Epoch 520, training loss: 12.668618202209473 = 0.48137596249580383 + 2.0 * 6.093621253967285
Epoch 520, val loss: 0.8408809900283813
Epoch 530, training loss: 12.63874626159668 = 0.4587956368923187 + 2.0 * 6.089975357055664
Epoch 530, val loss: 0.8283755779266357
Epoch 540, training loss: 12.612025260925293 = 0.4370702803134918 + 2.0 * 6.087477684020996
Epoch 540, val loss: 0.8171101808547974
Epoch 550, training loss: 12.603425979614258 = 0.4162168502807617 + 2.0 * 6.093604564666748
Epoch 550, val loss: 0.8071737885475159
Epoch 560, training loss: 12.570245742797852 = 0.3963586390018463 + 2.0 * 6.086943626403809
Epoch 560, val loss: 0.7984548211097717
Epoch 570, training loss: 12.54211711883545 = 0.37744444608688354 + 2.0 * 6.08233642578125
Epoch 570, val loss: 0.791039228439331
Epoch 580, training loss: 12.519543647766113 = 0.3592957854270935 + 2.0 * 6.0801239013671875
Epoch 580, val loss: 0.7848169803619385
Epoch 590, training loss: 12.516489028930664 = 0.34187933802604675 + 2.0 * 6.087305068969727
Epoch 590, val loss: 0.779696524143219
Epoch 600, training loss: 12.479888916015625 = 0.32516273856163025 + 2.0 * 6.077363014221191
Epoch 600, val loss: 0.7758963704109192
Epoch 610, training loss: 12.460772514343262 = 0.3091341257095337 + 2.0 * 6.07581901550293
Epoch 610, val loss: 0.7731719017028809
Epoch 620, training loss: 12.442362785339355 = 0.2936275899410248 + 2.0 * 6.074367523193359
Epoch 620, val loss: 0.7714124321937561
Epoch 630, training loss: 12.431414604187012 = 0.27863597869873047 + 2.0 * 6.076389312744141
Epoch 630, val loss: 0.7704874277114868
Epoch 640, training loss: 12.40924072265625 = 0.26425397396087646 + 2.0 * 6.072493553161621
Epoch 640, val loss: 0.77041095495224
Epoch 650, training loss: 12.389147758483887 = 0.25035494565963745 + 2.0 * 6.069396495819092
Epoch 650, val loss: 0.7711785435676575
Epoch 660, training loss: 12.373146057128906 = 0.23694954812526703 + 2.0 * 6.068098068237305
Epoch 660, val loss: 0.7726576924324036
Epoch 670, training loss: 12.370177268981934 = 0.2240677922964096 + 2.0 * 6.073054790496826
Epoch 670, val loss: 0.7747858762741089
Epoch 680, training loss: 12.348819732666016 = 0.2117733210325241 + 2.0 * 6.068523406982422
Epoch 680, val loss: 0.7775719165802002
Epoch 690, training loss: 12.329099655151367 = 0.20005303621292114 + 2.0 * 6.064523220062256
Epoch 690, val loss: 0.7809882164001465
Epoch 700, training loss: 12.3143892288208 = 0.18887189030647278 + 2.0 * 6.062758445739746
Epoch 700, val loss: 0.7849432229995728
Epoch 710, training loss: 12.310133934020996 = 0.17827026546001434 + 2.0 * 6.065931797027588
Epoch 710, val loss: 0.7894008159637451
Epoch 720, training loss: 12.29288387298584 = 0.16832521557807922 + 2.0 * 6.062279224395752
Epoch 720, val loss: 0.794189989566803
Epoch 730, training loss: 12.27667236328125 = 0.15896841883659363 + 2.0 * 6.058852195739746
Epoch 730, val loss: 0.7994828820228577
Epoch 740, training loss: 12.267529487609863 = 0.15016517043113708 + 2.0 * 6.058681964874268
Epoch 740, val loss: 0.8051011562347412
Epoch 750, training loss: 12.26593017578125 = 0.1419088989496231 + 2.0 * 6.062010765075684
Epoch 750, val loss: 0.8109948635101318
Epoch 760, training loss: 12.245882987976074 = 0.13419164717197418 + 2.0 * 6.055845737457275
Epoch 760, val loss: 0.8171146512031555
Epoch 770, training loss: 12.235391616821289 = 0.12699459493160248 + 2.0 * 6.054198741912842
Epoch 770, val loss: 0.8235081434249878
Epoch 780, training loss: 12.228154182434082 = 0.12026593089103699 + 2.0 * 6.053944110870361
Epoch 780, val loss: 0.8300178050994873
Epoch 790, training loss: 12.220723152160645 = 0.11397917568683624 + 2.0 * 6.053371906280518
Epoch 790, val loss: 0.8366063237190247
Epoch 800, training loss: 12.214338302612305 = 0.10813994705677032 + 2.0 * 6.053099155426025
Epoch 800, val loss: 0.8433293700218201
Epoch 810, training loss: 12.202657699584961 = 0.10267673432826996 + 2.0 * 6.049990653991699
Epoch 810, val loss: 0.8501790165901184
Epoch 820, training loss: 12.205053329467773 = 0.09756709635257721 + 2.0 * 6.0537428855896
Epoch 820, val loss: 0.8571030497550964
Epoch 830, training loss: 12.195374488830566 = 0.09283419698476791 + 2.0 * 6.051270008087158
Epoch 830, val loss: 0.8640062212944031
Epoch 840, training loss: 12.185853004455566 = 0.08838465064764023 + 2.0 * 6.048734188079834
Epoch 840, val loss: 0.8710335493087769
Epoch 850, training loss: 12.193507194519043 = 0.08424291014671326 + 2.0 * 6.054632186889648
Epoch 850, val loss: 0.8779374361038208
Epoch 860, training loss: 12.17515754699707 = 0.08039973676204681 + 2.0 * 6.047379016876221
Epoch 860, val loss: 0.8848661184310913
Epoch 870, training loss: 12.166377067565918 = 0.07679123431444168 + 2.0 * 6.044793128967285
Epoch 870, val loss: 0.8919677138328552
Epoch 880, training loss: 12.161629676818848 = 0.07340548932552338 + 2.0 * 6.044112205505371
Epoch 880, val loss: 0.899006724357605
Epoch 890, training loss: 12.162251472473145 = 0.07022642344236374 + 2.0 * 6.0460124015808105
Epoch 890, val loss: 0.9060120582580566
Epoch 900, training loss: 12.150303840637207 = 0.06723576039075851 + 2.0 * 6.041533946990967
Epoch 900, val loss: 0.9129169583320618
Epoch 910, training loss: 12.152457237243652 = 0.06442759931087494 + 2.0 * 6.044014930725098
Epoch 910, val loss: 0.9198947548866272
Epoch 920, training loss: 12.14981746673584 = 0.06178157404065132 + 2.0 * 6.044017791748047
Epoch 920, val loss: 0.926787793636322
Epoch 930, training loss: 12.141996383666992 = 0.059296365827322006 + 2.0 * 6.0413498878479
Epoch 930, val loss: 0.9335528612136841
Epoch 940, training loss: 12.138762474060059 = 0.05695120245218277 + 2.0 * 6.040905475616455
Epoch 940, val loss: 0.940373957157135
Epoch 950, training loss: 12.134337425231934 = 0.054743677377700806 + 2.0 * 6.039796829223633
Epoch 950, val loss: 0.947136402130127
Epoch 960, training loss: 12.13339900970459 = 0.05265967547893524 + 2.0 * 6.040369510650635
Epoch 960, val loss: 0.9539938569068909
Epoch 970, training loss: 12.130413055419922 = 0.05068226531147957 + 2.0 * 6.039865493774414
Epoch 970, val loss: 0.9605874419212341
Epoch 980, training loss: 12.122106552124023 = 0.04882305860519409 + 2.0 * 6.036641597747803
Epoch 980, val loss: 0.9672170877456665
Epoch 990, training loss: 12.117268562316895 = 0.047052111476659775 + 2.0 * 6.0351080894470215
Epoch 990, val loss: 0.9737996459007263
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7667
Overall ASR: 0.6531
Flip ASR: 0.5956/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.685625076293945 = 1.9379551410675049 + 2.0 * 8.373834609985352
Epoch 0, val loss: 1.9404394626617432
Epoch 10, training loss: 18.67474937438965 = 1.9286060333251953 + 2.0 * 8.373071670532227
Epoch 10, val loss: 1.931447148323059
Epoch 20, training loss: 18.651042938232422 = 1.916916847229004 + 2.0 * 8.36706256866455
Epoch 20, val loss: 1.9197629690170288
Epoch 30, training loss: 18.54720115661621 = 1.901358962059021 + 2.0 * 8.322920799255371
Epoch 30, val loss: 1.9041420221328735
Epoch 40, training loss: 17.988555908203125 = 1.8828775882720947 + 2.0 * 8.052839279174805
Epoch 40, val loss: 1.8857918977737427
Epoch 50, training loss: 17.02406883239746 = 1.8632991313934326 + 2.0 * 7.580385208129883
Epoch 50, val loss: 1.8674107789993286
Epoch 60, training loss: 15.976274490356445 = 1.8486380577087402 + 2.0 * 7.063818454742432
Epoch 60, val loss: 1.853985071182251
Epoch 70, training loss: 15.332045555114746 = 1.8369944095611572 + 2.0 * 6.747525691986084
Epoch 70, val loss: 1.843038558959961
Epoch 80, training loss: 14.986634254455566 = 1.8263628482818604 + 2.0 * 6.580135822296143
Epoch 80, val loss: 1.8328211307525635
Epoch 90, training loss: 14.785384178161621 = 1.8149904012680054 + 2.0 * 6.485197067260742
Epoch 90, val loss: 1.8218451738357544
Epoch 100, training loss: 14.64504623413086 = 1.8025010824203491 + 2.0 * 6.4212727546691895
Epoch 100, val loss: 1.809893012046814
Epoch 110, training loss: 14.536580085754395 = 1.7892863750457764 + 2.0 * 6.3736467361450195
Epoch 110, val loss: 1.797385811805725
Epoch 120, training loss: 14.4497652053833 = 1.775856614112854 + 2.0 * 6.336954116821289
Epoch 120, val loss: 1.7847700119018555
Epoch 130, training loss: 14.381921768188477 = 1.7618504762649536 + 2.0 * 6.310035705566406
Epoch 130, val loss: 1.7717556953430176
Epoch 140, training loss: 14.318037033081055 = 1.7465369701385498 + 2.0 * 6.285749912261963
Epoch 140, val loss: 1.7577874660491943
Epoch 150, training loss: 14.262174606323242 = 1.7295105457305908 + 2.0 * 6.266332149505615
Epoch 150, val loss: 1.7426753044128418
Epoch 160, training loss: 14.213319778442383 = 1.7104841470718384 + 2.0 * 6.251417636871338
Epoch 160, val loss: 1.7261238098144531
Epoch 170, training loss: 14.168683052062988 = 1.6890908479690552 + 2.0 * 6.239796161651611
Epoch 170, val loss: 1.7079564332962036
Epoch 180, training loss: 14.116376876831055 = 1.6651064157485962 + 2.0 * 6.225635051727295
Epoch 180, val loss: 1.6878297328948975
Epoch 190, training loss: 14.066555976867676 = 1.6381107568740845 + 2.0 * 6.214222431182861
Epoch 190, val loss: 1.6654202938079834
Epoch 200, training loss: 14.023395538330078 = 1.6076236963272095 + 2.0 * 6.2078857421875
Epoch 200, val loss: 1.6404709815979004
Epoch 210, training loss: 13.965309143066406 = 1.5738247632980347 + 2.0 * 6.195742130279541
Epoch 210, val loss: 1.6128267049789429
Epoch 220, training loss: 13.908900260925293 = 1.5363874435424805 + 2.0 * 6.186256408691406
Epoch 220, val loss: 1.582440972328186
Epoch 230, training loss: 13.860054016113281 = 1.4952738285064697 + 2.0 * 6.182390213012695
Epoch 230, val loss: 1.549295425415039
Epoch 240, training loss: 13.791366577148438 = 1.4509825706481934 + 2.0 * 6.170191764831543
Epoch 240, val loss: 1.514082908630371
Epoch 250, training loss: 13.740777969360352 = 1.4038496017456055 + 2.0 * 6.168464183807373
Epoch 250, val loss: 1.476791262626648
Epoch 260, training loss: 13.670416831970215 = 1.354577660560608 + 2.0 * 6.157919406890869
Epoch 260, val loss: 1.4381208419799805
Epoch 270, training loss: 13.605648040771484 = 1.3034415245056152 + 2.0 * 6.151103496551514
Epoch 270, val loss: 1.3982831239700317
Epoch 280, training loss: 13.548099517822266 = 1.2509772777557373 + 2.0 * 6.148561000823975
Epoch 280, val loss: 1.357797384262085
Epoch 290, training loss: 13.480059623718262 = 1.1983054876327515 + 2.0 * 6.1408772468566895
Epoch 290, val loss: 1.3177604675292969
Epoch 300, training loss: 13.417070388793945 = 1.1461496353149414 + 2.0 * 6.135460376739502
Epoch 300, val loss: 1.2784714698791504
Epoch 310, training loss: 13.355693817138672 = 1.0948044061660767 + 2.0 * 6.130444526672363
Epoch 310, val loss: 1.2402689456939697
Epoch 320, training loss: 13.312949180603027 = 1.0446860790252686 + 2.0 * 6.13413143157959
Epoch 320, val loss: 1.2036241292953491
Epoch 330, training loss: 13.247323036193848 = 0.9970806241035461 + 2.0 * 6.125121116638184
Epoch 330, val loss: 1.16916024684906
Epoch 340, training loss: 13.19023323059082 = 0.9517538547515869 + 2.0 * 6.119239807128906
Epoch 340, val loss: 1.137069582939148
Epoch 350, training loss: 13.139835357666016 = 0.9084997773170471 + 2.0 * 6.115667819976807
Epoch 350, val loss: 1.107125997543335
Epoch 360, training loss: 13.091621398925781 = 0.8672316074371338 + 2.0 * 6.112195014953613
Epoch 360, val loss: 1.0792219638824463
Epoch 370, training loss: 13.047039985656738 = 0.8282119035720825 + 2.0 * 6.109414100646973
Epoch 370, val loss: 1.053426742553711
Epoch 380, training loss: 13.002057075500488 = 0.7908702492713928 + 2.0 * 6.105593204498291
Epoch 380, val loss: 1.029358983039856
Epoch 390, training loss: 12.962328910827637 = 0.7555152773857117 + 2.0 * 6.10340690612793
Epoch 390, val loss: 1.0072453022003174
Epoch 400, training loss: 12.922501564025879 = 0.7224270105361938 + 2.0 * 6.100037097930908
Epoch 400, val loss: 0.9872063398361206
Epoch 410, training loss: 12.888952255249023 = 0.691012978553772 + 2.0 * 6.098969459533691
Epoch 410, val loss: 0.9688214063644409
Epoch 420, training loss: 12.85148811340332 = 0.6614202857017517 + 2.0 * 6.095034122467041
Epoch 420, val loss: 0.9520184397697449
Epoch 430, training loss: 12.818110466003418 = 0.6333208680152893 + 2.0 * 6.092394828796387
Epoch 430, val loss: 0.936778724193573
Epoch 440, training loss: 12.787701606750488 = 0.6065760850906372 + 2.0 * 6.09056282043457
Epoch 440, val loss: 0.9229297637939453
Epoch 450, training loss: 12.765198707580566 = 0.5811066031455994 + 2.0 * 6.09204626083374
Epoch 450, val loss: 0.9103270769119263
Epoch 460, training loss: 12.73326301574707 = 0.5569226145744324 + 2.0 * 6.088170051574707
Epoch 460, val loss: 0.8992236256599426
Epoch 470, training loss: 12.702622413635254 = 0.533888041973114 + 2.0 * 6.084367275238037
Epoch 470, val loss: 0.8892688751220703
Epoch 480, training loss: 12.677450180053711 = 0.5117077827453613 + 2.0 * 6.082870960235596
Epoch 480, val loss: 0.8803057074546814
Epoch 490, training loss: 12.650583267211914 = 0.4903293251991272 + 2.0 * 6.080126762390137
Epoch 490, val loss: 0.8723931908607483
Epoch 500, training loss: 12.62596321105957 = 0.46967828273773193 + 2.0 * 6.0781426429748535
Epoch 500, val loss: 0.8652346134185791
Epoch 510, training loss: 12.61191463470459 = 0.4495465159416199 + 2.0 * 6.081183910369873
Epoch 510, val loss: 0.858893096446991
Epoch 520, training loss: 12.58154582977295 = 0.4300318658351898 + 2.0 * 6.075757026672363
Epoch 520, val loss: 0.8531520366668701
Epoch 530, training loss: 12.563776969909668 = 0.4110051393508911 + 2.0 * 6.076385974884033
Epoch 530, val loss: 0.8481271266937256
Epoch 540, training loss: 12.538565635681152 = 0.39245882630348206 + 2.0 * 6.073053359985352
Epoch 540, val loss: 0.843845546245575
Epoch 550, training loss: 12.517308235168457 = 0.374428391456604 + 2.0 * 6.071439743041992
Epoch 550, val loss: 0.8402525186538696
Epoch 560, training loss: 12.495564460754395 = 0.35680821537971497 + 2.0 * 6.069377899169922
Epoch 560, val loss: 0.8373614549636841
Epoch 570, training loss: 12.480379104614258 = 0.3397030830383301 + 2.0 * 6.070337772369385
Epoch 570, val loss: 0.8350666761398315
Epoch 580, training loss: 12.456156730651855 = 0.3229689300060272 + 2.0 * 6.066594123840332
Epoch 580, val loss: 0.8334670662879944
Epoch 590, training loss: 12.435714721679688 = 0.3068453073501587 + 2.0 * 6.06443452835083
Epoch 590, val loss: 0.832511842250824
Epoch 600, training loss: 12.43362808227539 = 0.2911936938762665 + 2.0 * 6.071217060089111
Epoch 600, val loss: 0.8321444988250732
Epoch 610, training loss: 12.401045799255371 = 0.2762282192707062 + 2.0 * 6.062408924102783
Epoch 610, val loss: 0.8323308229446411
Epoch 620, training loss: 12.382573127746582 = 0.2619164288043976 + 2.0 * 6.060328483581543
Epoch 620, val loss: 0.8332166075706482
Epoch 630, training loss: 12.365707397460938 = 0.24821293354034424 + 2.0 * 6.058747291564941
Epoch 630, val loss: 0.8347234129905701
Epoch 640, training loss: 12.349481582641602 = 0.2351059913635254 + 2.0 * 6.057187557220459
Epoch 640, val loss: 0.8368465304374695
Epoch 650, training loss: 12.337656021118164 = 0.2226049304008484 + 2.0 * 6.057525634765625
Epoch 650, val loss: 0.8395708203315735
Epoch 660, training loss: 12.325220108032227 = 0.21083642542362213 + 2.0 * 6.057191848754883
Epoch 660, val loss: 0.8428245186805725
Epoch 670, training loss: 12.307944297790527 = 0.19973991811275482 + 2.0 * 6.054102420806885
Epoch 670, val loss: 0.8466833829879761
Epoch 680, training loss: 12.29814338684082 = 0.18930131196975708 + 2.0 * 6.0544209480285645
Epoch 680, val loss: 0.8511629104614258
Epoch 690, training loss: 12.283921241760254 = 0.17951513826847076 + 2.0 * 6.052203178405762
Epoch 690, val loss: 0.8562207221984863
Epoch 700, training loss: 12.27320384979248 = 0.17032486200332642 + 2.0 * 6.05143928527832
Epoch 700, val loss: 0.8617839217185974
Epoch 710, training loss: 12.268167495727539 = 0.16171476244926453 + 2.0 * 6.053226470947266
Epoch 710, val loss: 0.8678601980209351
Epoch 720, training loss: 12.252008438110352 = 0.15363842248916626 + 2.0 * 6.049184799194336
Epoch 720, val loss: 0.8743846416473389
Epoch 730, training loss: 12.24719524383545 = 0.1460920125246048 + 2.0 * 6.050551414489746
Epoch 730, val loss: 0.8813705444335938
Epoch 740, training loss: 12.233763694763184 = 0.13898003101348877 + 2.0 * 6.047391891479492
Epoch 740, val loss: 0.8886005282402039
Epoch 750, training loss: 12.224562644958496 = 0.13233694434165955 + 2.0 * 6.046113014221191
Epoch 750, val loss: 0.8963074684143066
Epoch 760, training loss: 12.215581893920898 = 0.12608028948307037 + 2.0 * 6.044750690460205
Epoch 760, val loss: 0.9042758345603943
Epoch 770, training loss: 12.227477073669434 = 0.12021991610527039 + 2.0 * 6.053628444671631
Epoch 770, val loss: 0.9125357270240784
Epoch 780, training loss: 12.204901695251465 = 0.11469056457281113 + 2.0 * 6.045105457305908
Epoch 780, val loss: 0.920940101146698
Epoch 790, training loss: 12.19666576385498 = 0.1095278337597847 + 2.0 * 6.043569087982178
Epoch 790, val loss: 0.9296426177024841
Epoch 800, training loss: 12.187640190124512 = 0.10466113686561584 + 2.0 * 6.041489601135254
Epoch 800, val loss: 0.9385323524475098
Epoch 810, training loss: 12.192238807678223 = 0.1000736877322197 + 2.0 * 6.046082496643066
Epoch 810, val loss: 0.9475610852241516
Epoch 820, training loss: 12.181440353393555 = 0.09572937339544296 + 2.0 * 6.042855262756348
Epoch 820, val loss: 0.9566645622253418
Epoch 830, training loss: 12.171652793884277 = 0.09165496379137039 + 2.0 * 6.039999008178711
Epoch 830, val loss: 0.9659311771392822
Epoch 840, training loss: 12.164604187011719 = 0.0877993255853653 + 2.0 * 6.038402557373047
Epoch 840, val loss: 0.975296676158905
Epoch 850, training loss: 12.16170597076416 = 0.08416056632995605 + 2.0 * 6.0387725830078125
Epoch 850, val loss: 0.9847701191902161
Epoch 860, training loss: 12.162103652954102 = 0.08071819692850113 + 2.0 * 6.0406928062438965
Epoch 860, val loss: 0.9942379593849182
Epoch 870, training loss: 12.152048110961914 = 0.07744356989860535 + 2.0 * 6.037302494049072
Epoch 870, val loss: 1.0035876035690308
Epoch 880, training loss: 12.161952018737793 = 0.07436797767877579 + 2.0 * 6.043792247772217
Epoch 880, val loss: 1.013069748878479
Epoch 890, training loss: 12.141201972961426 = 0.07146153599023819 + 2.0 * 6.034870147705078
Epoch 890, val loss: 1.0225046873092651
Epoch 900, training loss: 12.134882926940918 = 0.06869642436504364 + 2.0 * 6.033093452453613
Epoch 900, val loss: 1.0319652557373047
Epoch 910, training loss: 12.129676818847656 = 0.06607702374458313 + 2.0 * 6.031799793243408
Epoch 910, val loss: 1.0415242910385132
Epoch 920, training loss: 12.127045631408691 = 0.06357453018426895 + 2.0 * 6.031735420227051
Epoch 920, val loss: 1.0510104894638062
Epoch 930, training loss: 12.129895210266113 = 0.06120641157031059 + 2.0 * 6.03434419631958
Epoch 930, val loss: 1.0603798627853394
Epoch 940, training loss: 12.120617866516113 = 0.058951180428266525 + 2.0 * 6.0308332443237305
Epoch 940, val loss: 1.0697543621063232
Epoch 950, training loss: 12.119119644165039 = 0.05681384727358818 + 2.0 * 6.031152725219727
Epoch 950, val loss: 1.0790929794311523
Epoch 960, training loss: 12.112235069274902 = 0.05477520823478699 + 2.0 * 6.0287299156188965
Epoch 960, val loss: 1.0882799625396729
Epoch 970, training loss: 12.111531257629395 = 0.052834317088127136 + 2.0 * 6.029348373413086
Epoch 970, val loss: 1.0974359512329102
Epoch 980, training loss: 12.108176231384277 = 0.05099964886903763 + 2.0 * 6.02858829498291
Epoch 980, val loss: 1.1065200567245483
Epoch 990, training loss: 12.102884292602539 = 0.049242109060287476 + 2.0 * 6.026821136474609
Epoch 990, val loss: 1.1155070066452026
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8074
Overall ASR: 0.8708
Flip ASR: 0.8444/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.699485778808594 = 1.9516483545303345 + 2.0 * 8.373918533325195
Epoch 0, val loss: 1.9453423023223877
Epoch 10, training loss: 18.688308715820312 = 1.9411109685897827 + 2.0 * 8.3735990524292
Epoch 10, val loss: 1.935181736946106
Epoch 20, training loss: 18.670866012573242 = 1.9280498027801514 + 2.0 * 8.371408462524414
Epoch 20, val loss: 1.9221082925796509
Epoch 30, training loss: 18.622346878051758 = 1.9099003076553345 + 2.0 * 8.356223106384277
Epoch 30, val loss: 1.9037493467330933
Epoch 40, training loss: 18.411848068237305 = 1.8865166902542114 + 2.0 * 8.262665748596191
Epoch 40, val loss: 1.8809890747070312
Epoch 50, training loss: 17.628725051879883 = 1.8605366945266724 + 2.0 * 7.884093761444092
Epoch 50, val loss: 1.8563460111618042
Epoch 60, training loss: 16.942371368408203 = 1.8360556364059448 + 2.0 * 7.553158283233643
Epoch 60, val loss: 1.8349428176879883
Epoch 70, training loss: 16.119369506835938 = 1.8202247619628906 + 2.0 * 7.149572849273682
Epoch 70, val loss: 1.821836233139038
Epoch 80, training loss: 15.517638206481934 = 1.8081454038619995 + 2.0 * 6.854746341705322
Epoch 80, val loss: 1.8113385438919067
Epoch 90, training loss: 15.210556983947754 = 1.7930269241333008 + 2.0 * 6.708765029907227
Epoch 90, val loss: 1.7979469299316406
Epoch 100, training loss: 14.981332778930664 = 1.776412844657898 + 2.0 * 6.602459907531738
Epoch 100, val loss: 1.78342604637146
Epoch 110, training loss: 14.792792320251465 = 1.7610876560211182 + 2.0 * 6.515852451324463
Epoch 110, val loss: 1.769710898399353
Epoch 120, training loss: 14.655102729797363 = 1.7457600831985474 + 2.0 * 6.454671382904053
Epoch 120, val loss: 1.7559514045715332
Epoch 130, training loss: 14.55474853515625 = 1.7294880151748657 + 2.0 * 6.412630081176758
Epoch 130, val loss: 1.741379976272583
Epoch 140, training loss: 14.474106788635254 = 1.7116042375564575 + 2.0 * 6.381251335144043
Epoch 140, val loss: 1.7256395816802979
Epoch 150, training loss: 14.399093627929688 = 1.6917990446090698 + 2.0 * 6.353647232055664
Epoch 150, val loss: 1.7086602449417114
Epoch 160, training loss: 14.329522132873535 = 1.6697720289230347 + 2.0 * 6.3298749923706055
Epoch 160, val loss: 1.6901066303253174
Epoch 170, training loss: 14.2640962600708 = 1.6451479196548462 + 2.0 * 6.309473991394043
Epoch 170, val loss: 1.669560194015503
Epoch 180, training loss: 14.200519561767578 = 1.617713451385498 + 2.0 * 6.291402816772461
Epoch 180, val loss: 1.6468194723129272
Epoch 190, training loss: 14.136536598205566 = 1.5870832204818726 + 2.0 * 6.274726867675781
Epoch 190, val loss: 1.621443510055542
Epoch 200, training loss: 14.079840660095215 = 1.5530261993408203 + 2.0 * 6.263407230377197
Epoch 200, val loss: 1.5932061672210693
Epoch 210, training loss: 14.010350227355957 = 1.515928030014038 + 2.0 * 6.24721097946167
Epoch 210, val loss: 1.5624659061431885
Epoch 220, training loss: 13.94609260559082 = 1.4757624864578247 + 2.0 * 6.235165119171143
Epoch 220, val loss: 1.529173731803894
Epoch 230, training loss: 13.882648468017578 = 1.432491660118103 + 2.0 * 6.225078582763672
Epoch 230, val loss: 1.4933308362960815
Epoch 240, training loss: 13.820182800292969 = 1.3866286277770996 + 2.0 * 6.216777324676514
Epoch 240, val loss: 1.4558050632476807
Epoch 250, training loss: 13.753402709960938 = 1.339240550994873 + 2.0 * 6.207080841064453
Epoch 250, val loss: 1.4170504808425903
Epoch 260, training loss: 13.688236236572266 = 1.2907811403274536 + 2.0 * 6.198727607727051
Epoch 260, val loss: 1.3777505159378052
Epoch 270, training loss: 13.625563621520996 = 1.2416101694107056 + 2.0 * 6.191976547241211
Epoch 270, val loss: 1.3385878801345825
Epoch 280, training loss: 13.573860168457031 = 1.1925723552703857 + 2.0 * 6.190643787384033
Epoch 280, val loss: 1.300429344177246
Epoch 290, training loss: 13.506322860717773 = 1.1452443599700928 + 2.0 * 6.180539131164551
Epoch 290, val loss: 1.2640842199325562
Epoch 300, training loss: 13.446555137634277 = 1.0992799997329712 + 2.0 * 6.173637390136719
Epoch 300, val loss: 1.2297455072402954
Epoch 310, training loss: 13.394346237182617 = 1.0548033714294434 + 2.0 * 6.169771671295166
Epoch 310, val loss: 1.1973040103912354
Epoch 320, training loss: 13.34672737121582 = 1.01224946975708 + 2.0 * 6.167239189147949
Epoch 320, val loss: 1.1669915914535522
Epoch 330, training loss: 13.291115760803223 = 0.9718419909477234 + 2.0 * 6.159636974334717
Epoch 330, val loss: 1.1389950513839722
Epoch 340, training loss: 13.248306274414062 = 0.9334180355072021 + 2.0 * 6.157444000244141
Epoch 340, val loss: 1.1129661798477173
Epoch 350, training loss: 13.199803352355957 = 0.896897554397583 + 2.0 * 6.151453018188477
Epoch 350, val loss: 1.0888104438781738
Epoch 360, training loss: 13.154946327209473 = 0.8623029589653015 + 2.0 * 6.146321773529053
Epoch 360, val loss: 1.0662074089050293
Epoch 370, training loss: 13.113863945007324 = 0.8291602730751038 + 2.0 * 6.1423516273498535
Epoch 370, val loss: 1.0449161529541016
Epoch 380, training loss: 13.079935073852539 = 0.7973158955574036 + 2.0 * 6.14130973815918
Epoch 380, val loss: 1.0248377323150635
Epoch 390, training loss: 13.0383939743042 = 0.7669588327407837 + 2.0 * 6.135717391967773
Epoch 390, val loss: 1.0058648586273193
Epoch 400, training loss: 13.001220703125 = 0.7375193238258362 + 2.0 * 6.131850719451904
Epoch 400, val loss: 0.9877861738204956
Epoch 410, training loss: 12.96643352508545 = 0.708788275718689 + 2.0 * 6.1288228034973145
Epoch 410, val loss: 0.9703434705734253
Epoch 420, training loss: 12.930814743041992 = 0.6806261539459229 + 2.0 * 6.125094413757324
Epoch 420, val loss: 0.9534317851066589
Epoch 430, training loss: 12.905618667602539 = 0.6529994606971741 + 2.0 * 6.126309394836426
Epoch 430, val loss: 0.9370078444480896
Epoch 440, training loss: 12.872797966003418 = 0.6259898543357849 + 2.0 * 6.123404026031494
Epoch 440, val loss: 0.9211875200271606
Epoch 450, training loss: 12.833757400512695 = 0.599463164806366 + 2.0 * 6.117146968841553
Epoch 450, val loss: 0.9058449864387512
Epoch 460, training loss: 12.802915573120117 = 0.573320209980011 + 2.0 * 6.114797592163086
Epoch 460, val loss: 0.8909472227096558
Epoch 470, training loss: 12.7774076461792 = 0.5475820302963257 + 2.0 * 6.114912986755371
Epoch 470, val loss: 0.8765419125556946
Epoch 480, training loss: 12.746548652648926 = 0.5225107669830322 + 2.0 * 6.112019062042236
Epoch 480, val loss: 0.8627404570579529
Epoch 490, training loss: 12.714163780212402 = 0.49808254837989807 + 2.0 * 6.108040809631348
Epoch 490, val loss: 0.8498024940490723
Epoch 500, training loss: 12.698527336120605 = 0.474414587020874 + 2.0 * 6.112056255340576
Epoch 500, val loss: 0.8376838564872742
Epoch 510, training loss: 12.659536361694336 = 0.45186343789100647 + 2.0 * 6.103836536407471
Epoch 510, val loss: 0.8264250159263611
Epoch 520, training loss: 12.633975982666016 = 0.4303014576435089 + 2.0 * 6.101837158203125
Epoch 520, val loss: 0.8162962198257446
Epoch 530, training loss: 12.608417510986328 = 0.40966787934303284 + 2.0 * 6.099374771118164
Epoch 530, val loss: 0.8070813417434692
Epoch 540, training loss: 12.585944175720215 = 0.38994142413139343 + 2.0 * 6.098001480102539
Epoch 540, val loss: 0.7987068891525269
Epoch 550, training loss: 12.564424514770508 = 0.37117108702659607 + 2.0 * 6.0966267585754395
Epoch 550, val loss: 0.7912445664405823
Epoch 560, training loss: 12.54340648651123 = 0.35342973470687866 + 2.0 * 6.0949883460998535
Epoch 560, val loss: 0.7847619652748108
Epoch 570, training loss: 12.524227142333984 = 0.3365735411643982 + 2.0 * 6.093826770782471
Epoch 570, val loss: 0.7791852951049805
Epoch 580, training loss: 12.503585815429688 = 0.3205402195453644 + 2.0 * 6.091522693634033
Epoch 580, val loss: 0.7743906378746033
Epoch 590, training loss: 12.4863920211792 = 0.3052411377429962 + 2.0 * 6.090575218200684
Epoch 590, val loss: 0.7703732252120972
Epoch 600, training loss: 12.469954490661621 = 0.2906852066516876 + 2.0 * 6.089634418487549
Epoch 600, val loss: 0.767098069190979
Epoch 610, training loss: 12.455220222473145 = 0.2768464982509613 + 2.0 * 6.089186668395996
Epoch 610, val loss: 0.7646550536155701
Epoch 620, training loss: 12.43303394317627 = 0.26365745067596436 + 2.0 * 6.084688186645508
Epoch 620, val loss: 0.7628856301307678
Epoch 630, training loss: 12.416812896728516 = 0.2510572373867035 + 2.0 * 6.0828776359558105
Epoch 630, val loss: 0.7617926001548767
Epoch 640, training loss: 12.412073135375977 = 0.23904256522655487 + 2.0 * 6.086515426635742
Epoch 640, val loss: 0.76143878698349
Epoch 650, training loss: 12.398749351501465 = 0.22757185995578766 + 2.0 * 6.0855889320373535
Epoch 650, val loss: 0.7615808844566345
Epoch 660, training loss: 12.377552032470703 = 0.21672992408275604 + 2.0 * 6.080410957336426
Epoch 660, val loss: 0.7624269127845764
Epoch 670, training loss: 12.360820770263672 = 0.2063787579536438 + 2.0 * 6.077220916748047
Epoch 670, val loss: 0.7638964056968689
Epoch 680, training loss: 12.36105728149414 = 0.19651006162166595 + 2.0 * 6.082273483276367
Epoch 680, val loss: 0.7659082412719727
Epoch 690, training loss: 12.346090316772461 = 0.18713048100471497 + 2.0 * 6.079479694366455
Epoch 690, val loss: 0.7684155702590942
Epoch 700, training loss: 12.325153350830078 = 0.17823408544063568 + 2.0 * 6.073459625244141
Epoch 700, val loss: 0.7714917659759521
Epoch 710, training loss: 12.317157745361328 = 0.16978450119495392 + 2.0 * 6.073686599731445
Epoch 710, val loss: 0.7749999165534973
Epoch 720, training loss: 12.3067045211792 = 0.16175952553749084 + 2.0 * 6.07247257232666
Epoch 720, val loss: 0.7789475321769714
Epoch 730, training loss: 12.297762870788574 = 0.1541833132505417 + 2.0 * 6.071789741516113
Epoch 730, val loss: 0.7832930684089661
Epoch 740, training loss: 12.287035942077637 = 0.1470116227865219 + 2.0 * 6.070012092590332
Epoch 740, val loss: 0.7880772352218628
Epoch 750, training loss: 12.277179718017578 = 0.14020587503910065 + 2.0 * 6.06848669052124
Epoch 750, val loss: 0.7932939529418945
Epoch 760, training loss: 12.270837783813477 = 0.13376227021217346 + 2.0 * 6.068537712097168
Epoch 760, val loss: 0.7988550662994385
Epoch 770, training loss: 12.259536743164062 = 0.12764546275138855 + 2.0 * 6.065945625305176
Epoch 770, val loss: 0.8047411441802979
Epoch 780, training loss: 12.259150505065918 = 0.1218772828578949 + 2.0 * 6.068636417388916
Epoch 780, val loss: 0.8109925389289856
Epoch 790, training loss: 12.24560832977295 = 0.11643694341182709 + 2.0 * 6.0645856857299805
Epoch 790, val loss: 0.8172849416732788
Epoch 800, training loss: 12.236440658569336 = 0.11130036413669586 + 2.0 * 6.062570095062256
Epoch 800, val loss: 0.8239848017692566
Epoch 810, training loss: 12.22788143157959 = 0.10643291473388672 + 2.0 * 6.060724258422852
Epoch 810, val loss: 0.8308247327804565
Epoch 820, training loss: 12.236734390258789 = 0.10183785110712051 + 2.0 * 6.067448139190674
Epoch 820, val loss: 0.8378893733024597
Epoch 830, training loss: 12.218690872192383 = 0.09748119860887527 + 2.0 * 6.060605049133301
Epoch 830, val loss: 0.8450011610984802
Epoch 840, training loss: 12.21612548828125 = 0.09337494522333145 + 2.0 * 6.061375141143799
Epoch 840, val loss: 0.8522600531578064
Epoch 850, training loss: 12.206027030944824 = 0.08949117362499237 + 2.0 * 6.058268070220947
Epoch 850, val loss: 0.8596720099449158
Epoch 860, training loss: 12.199130058288574 = 0.0858224481344223 + 2.0 * 6.05665397644043
Epoch 860, val loss: 0.8671457171440125
Epoch 870, training loss: 12.193487167358398 = 0.08233670145273209 + 2.0 * 6.055575370788574
Epoch 870, val loss: 0.8747435212135315
Epoch 880, training loss: 12.214577674865723 = 0.07903140038251877 + 2.0 * 6.067773342132568
Epoch 880, val loss: 0.8823359608650208
Epoch 890, training loss: 12.187368392944336 = 0.075943723320961 + 2.0 * 6.0557122230529785
Epoch 890, val loss: 0.8899263143539429
Epoch 900, training loss: 12.17815113067627 = 0.07300711423158646 + 2.0 * 6.052571773529053
Epoch 900, val loss: 0.8976653218269348
Epoch 910, training loss: 12.173542022705078 = 0.07021570205688477 + 2.0 * 6.051663398742676
Epoch 910, val loss: 0.9053950309753418
Epoch 920, training loss: 12.168987274169922 = 0.06755568087100983 + 2.0 * 6.050715923309326
Epoch 920, val loss: 0.9131678938865662
Epoch 930, training loss: 12.192784309387207 = 0.06504598259925842 + 2.0 * 6.063868999481201
Epoch 930, val loss: 0.9208722710609436
Epoch 940, training loss: 12.164490699768066 = 0.06265240907669067 + 2.0 * 6.050919055938721
Epoch 940, val loss: 0.9284582138061523
Epoch 950, training loss: 12.156628608703613 = 0.060393162071704865 + 2.0 * 6.048117637634277
Epoch 950, val loss: 0.9361525177955627
Epoch 960, training loss: 12.154555320739746 = 0.058241069316864014 + 2.0 * 6.048157215118408
Epoch 960, val loss: 0.9438253045082092
Epoch 970, training loss: 12.170369148254395 = 0.056197356432676315 + 2.0 * 6.057085990905762
Epoch 970, val loss: 0.9514520764350891
Epoch 980, training loss: 12.148776054382324 = 0.054252900183200836 + 2.0 * 6.047261714935303
Epoch 980, val loss: 0.9588672518730164
Epoch 990, training loss: 12.144445419311523 = 0.052407123148441315 + 2.0 * 6.046019077301025
Epoch 990, val loss: 0.9663884043693542
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7926
Overall ASR: 0.9041
Flip ASR: 0.8844/225 nodes
The final ASR:0.80935, 0.11129, Accuracy:0.78889, 0.01684
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11664])
remove edge: torch.Size([2, 9536])
updated graph: torch.Size([2, 10644])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8481
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
The final ASR:0.97540, 0.00174, Accuracy:0.83457, 0.01062
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.5 der2:0 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 18.696063995361328 = 1.9482502937316895 + 2.0 * 8.373907089233398
Epoch 0, val loss: 1.9420284032821655
Epoch 10, training loss: 18.685373306274414 = 1.938178300857544 + 2.0 * 8.373597145080566
Epoch 10, val loss: 1.9324073791503906
Epoch 20, training loss: 18.668354034423828 = 1.9254175424575806 + 2.0 * 8.371468544006348
Epoch 20, val loss: 1.9196630716323853
Epoch 30, training loss: 18.619230270385742 = 1.9071987867355347 + 2.0 * 8.356016159057617
Epoch 30, val loss: 1.9013535976409912
Epoch 40, training loss: 18.41128921508789 = 1.883265733718872 + 2.0 * 8.26401138305664
Epoch 40, val loss: 1.8781886100769043
Epoch 50, training loss: 17.536640167236328 = 1.8576260805130005 + 2.0 * 7.83950662612915
Epoch 50, val loss: 1.8534140586853027
Epoch 60, training loss: 16.976285934448242 = 1.8320666551589966 + 2.0 * 7.572109699249268
Epoch 60, val loss: 1.830492615699768
Epoch 70, training loss: 16.258136749267578 = 1.8144558668136597 + 2.0 * 7.2218403816223145
Epoch 70, val loss: 1.8162511587142944
Epoch 80, training loss: 15.607881546020508 = 1.8042508363723755 + 2.0 * 6.901815414428711
Epoch 80, val loss: 1.8085094690322876
Epoch 90, training loss: 15.270204544067383 = 1.7941697835922241 + 2.0 * 6.738017559051514
Epoch 90, val loss: 1.7996257543563843
Epoch 100, training loss: 14.980986595153809 = 1.7814775705337524 + 2.0 * 6.599754333496094
Epoch 100, val loss: 1.7880781888961792
Epoch 110, training loss: 14.811728477478027 = 1.768478274345398 + 2.0 * 6.52162504196167
Epoch 110, val loss: 1.7763103246688843
Epoch 120, training loss: 14.679755210876465 = 1.7550021409988403 + 2.0 * 6.462376594543457
Epoch 120, val loss: 1.764359474182129
Epoch 130, training loss: 14.577901840209961 = 1.7404950857162476 + 2.0 * 6.418703556060791
Epoch 130, val loss: 1.7516182661056519
Epoch 140, training loss: 14.491868019104004 = 1.7240068912506104 + 2.0 * 6.383930683135986
Epoch 140, val loss: 1.737336277961731
Epoch 150, training loss: 14.412057876586914 = 1.7057205438613892 + 2.0 * 6.353168487548828
Epoch 150, val loss: 1.7217261791229248
Epoch 160, training loss: 14.33930778503418 = 1.6853309869766235 + 2.0 * 6.326988220214844
Epoch 160, val loss: 1.704551339149475
Epoch 170, training loss: 14.275129318237305 = 1.6625404357910156 + 2.0 * 6.3062944412231445
Epoch 170, val loss: 1.6855309009552002
Epoch 180, training loss: 14.207541465759277 = 1.6370298862457275 + 2.0 * 6.2852559089660645
Epoch 180, val loss: 1.6645357608795166
Epoch 190, training loss: 14.145166397094727 = 1.6085124015808105 + 2.0 * 6.268327236175537
Epoch 190, val loss: 1.6410484313964844
Epoch 200, training loss: 14.08535385131836 = 1.5768189430236816 + 2.0 * 6.254267692565918
Epoch 200, val loss: 1.615034580230713
Epoch 210, training loss: 14.028027534484863 = 1.5425024032592773 + 2.0 * 6.242762565612793
Epoch 210, val loss: 1.5870449542999268
Epoch 220, training loss: 13.967793464660645 = 1.5063616037368774 + 2.0 * 6.230715751647949
Epoch 220, val loss: 1.557878851890564
Epoch 230, training loss: 13.908954620361328 = 1.4690613746643066 + 2.0 * 6.219946384429932
Epoch 230, val loss: 1.5281791687011719
Epoch 240, training loss: 13.853729248046875 = 1.4312928915023804 + 2.0 * 6.211218357086182
Epoch 240, val loss: 1.4986176490783691
Epoch 250, training loss: 13.798502922058105 = 1.394137978553772 + 2.0 * 6.202182292938232
Epoch 250, val loss: 1.4698939323425293
Epoch 260, training loss: 13.74563217163086 = 1.3579245805740356 + 2.0 * 6.193853855133057
Epoch 260, val loss: 1.4425594806671143
Epoch 270, training loss: 13.691396713256836 = 1.3231767416000366 + 2.0 * 6.184110164642334
Epoch 270, val loss: 1.4167124032974243
Epoch 280, training loss: 13.641827583312988 = 1.2895139455795288 + 2.0 * 6.176156997680664
Epoch 280, val loss: 1.3921399116516113
Epoch 290, training loss: 13.610103607177734 = 1.2567944526672363 + 2.0 * 6.17665433883667
Epoch 290, val loss: 1.3686883449554443
Epoch 300, training loss: 13.55598258972168 = 1.225324034690857 + 2.0 * 6.165329456329346
Epoch 300, val loss: 1.3463122844696045
Epoch 310, training loss: 13.50903034210205 = 1.1944379806518555 + 2.0 * 6.157296180725098
Epoch 310, val loss: 1.324812650680542
Epoch 320, training loss: 13.468267440795898 = 1.1635560989379883 + 2.0 * 6.152355670928955
Epoch 320, val loss: 1.3034765720367432
Epoch 330, training loss: 13.432807922363281 = 1.1322575807571411 + 2.0 * 6.150275230407715
Epoch 330, val loss: 1.2819645404815674
Epoch 340, training loss: 13.388489723205566 = 1.1008050441741943 + 2.0 * 6.1438422203063965
Epoch 340, val loss: 1.2602417469024658
Epoch 350, training loss: 13.350114822387695 = 1.0689693689346313 + 2.0 * 6.140572547912598
Epoch 350, val loss: 1.2384122610092163
Epoch 360, training loss: 13.307576179504395 = 1.0365225076675415 + 2.0 * 6.135526657104492
Epoch 360, val loss: 1.216073989868164
Epoch 370, training loss: 13.278426170349121 = 1.0034114122390747 + 2.0 * 6.137507438659668
Epoch 370, val loss: 1.1931345462799072
Epoch 380, training loss: 13.226937294006348 = 0.9701299071311951 + 2.0 * 6.128403663635254
Epoch 380, val loss: 1.170153021812439
Epoch 390, training loss: 13.187787055969238 = 0.9368650317192078 + 2.0 * 6.125461101531982
Epoch 390, val loss: 1.1471998691558838
Epoch 400, training loss: 13.148419380187988 = 0.9037185311317444 + 2.0 * 6.122350215911865
Epoch 400, val loss: 1.1243529319763184
Epoch 410, training loss: 13.117620468139648 = 0.8713103532791138 + 2.0 * 6.123155117034912
Epoch 410, val loss: 1.101919412612915
Epoch 420, training loss: 13.074564933776855 = 0.8400445580482483 + 2.0 * 6.117259979248047
Epoch 420, val loss: 1.0805103778839111
Epoch 430, training loss: 13.0360746383667 = 0.8097628355026245 + 2.0 * 6.113155841827393
Epoch 430, val loss: 1.0600510835647583
Epoch 440, training loss: 13.002323150634766 = 0.7804141044616699 + 2.0 * 6.110954761505127
Epoch 440, val loss: 1.0403480529785156
Epoch 450, training loss: 12.976887702941895 = 0.7522595524787903 + 2.0 * 6.112314224243164
Epoch 450, val loss: 1.0214955806732178
Epoch 460, training loss: 12.940839767456055 = 0.7252581715583801 + 2.0 * 6.107790946960449
Epoch 460, val loss: 1.0038318634033203
Epoch 470, training loss: 12.907341003417969 = 0.6993493437767029 + 2.0 * 6.1039958000183105
Epoch 470, val loss: 0.9872922301292419
Epoch 480, training loss: 12.877288818359375 = 0.6743141412734985 + 2.0 * 6.101487159729004
Epoch 480, val loss: 0.9715789556503296
Epoch 490, training loss: 12.864648818969727 = 0.650092601776123 + 2.0 * 6.107277870178223
Epoch 490, val loss: 0.956744372844696
Epoch 500, training loss: 12.826642990112305 = 0.6269575953483582 + 2.0 * 6.099842548370361
Epoch 500, val loss: 0.9427294731140137
Epoch 510, training loss: 12.795625686645508 = 0.6046611070632935 + 2.0 * 6.095482349395752
Epoch 510, val loss: 0.9297786951065063
Epoch 520, training loss: 12.770681381225586 = 0.5830549597740173 + 2.0 * 6.093813419342041
Epoch 520, val loss: 0.9175426363945007
Epoch 530, training loss: 12.746267318725586 = 0.5619747638702393 + 2.0 * 6.092146396636963
Epoch 530, val loss: 0.9058295488357544
Epoch 540, training loss: 12.732810020446777 = 0.5414264798164368 + 2.0 * 6.095691680908203
Epoch 540, val loss: 0.8946309685707092
Epoch 550, training loss: 12.704483032226562 = 0.5215331315994263 + 2.0 * 6.091475009918213
Epoch 550, val loss: 0.8840342164039612
Epoch 560, training loss: 12.677973747253418 = 0.5021783113479614 + 2.0 * 6.087897777557373
Epoch 560, val loss: 0.8741806149482727
Epoch 570, training loss: 12.654932022094727 = 0.4832429885864258 + 2.0 * 6.08584451675415
Epoch 570, val loss: 0.8647263646125793
Epoch 580, training loss: 12.636744499206543 = 0.4646846652030945 + 2.0 * 6.086030006408691
Epoch 580, val loss: 0.8557044863700867
Epoch 590, training loss: 12.612848281860352 = 0.4464845061302185 + 2.0 * 6.083181858062744
Epoch 590, val loss: 0.8470886945724487
Epoch 600, training loss: 12.593378067016602 = 0.42868316173553467 + 2.0 * 6.082347393035889
Epoch 600, val loss: 0.8390120267868042
Epoch 610, training loss: 12.579514503479004 = 0.4111713171005249 + 2.0 * 6.084171772003174
Epoch 610, val loss: 0.8313156962394714
Epoch 620, training loss: 12.552844047546387 = 0.3940257132053375 + 2.0 * 6.079409122467041
Epoch 620, val loss: 0.8238799571990967
Epoch 630, training loss: 12.532710075378418 = 0.37714213132858276 + 2.0 * 6.077784061431885
Epoch 630, val loss: 0.8170149922370911
Epoch 640, training loss: 12.522769927978516 = 0.36051705479621887 + 2.0 * 6.0811262130737305
Epoch 640, val loss: 0.8104294538497925
Epoch 650, training loss: 12.494047164916992 = 0.3442385196685791 + 2.0 * 6.074904441833496
Epoch 650, val loss: 0.8042354583740234
Epoch 660, training loss: 12.47550106048584 = 0.3282516896724701 + 2.0 * 6.073624610900879
Epoch 660, val loss: 0.7985517382621765
Epoch 670, training loss: 12.465357780456543 = 0.3125836253166199 + 2.0 * 6.07638692855835
Epoch 670, val loss: 0.7932072877883911
Epoch 680, training loss: 12.44543170928955 = 0.2973704934120178 + 2.0 * 6.07403039932251
Epoch 680, val loss: 0.7882717847824097
Epoch 690, training loss: 12.424473762512207 = 0.28258541226387024 + 2.0 * 6.070944309234619
Epoch 690, val loss: 0.7838152647018433
Epoch 700, training loss: 12.404546737670898 = 0.26825761795043945 + 2.0 * 6.068144798278809
Epoch 700, val loss: 0.7799050211906433
Epoch 710, training loss: 12.393014907836914 = 0.25443190336227417 + 2.0 * 6.069291591644287
Epoch 710, val loss: 0.7764387726783752
Epoch 720, training loss: 12.379569053649902 = 0.24121308326721191 + 2.0 * 6.069178104400635
Epoch 720, val loss: 0.7732800841331482
Epoch 730, training loss: 12.361087799072266 = 0.22867947816848755 + 2.0 * 6.066204071044922
Epoch 730, val loss: 0.7706689238548279
Epoch 740, training loss: 12.34571361541748 = 0.2168324738740921 + 2.0 * 6.064440727233887
Epoch 740, val loss: 0.768676221370697
Epoch 750, training loss: 12.332345962524414 = 0.20561949908733368 + 2.0 * 6.063363075256348
Epoch 750, val loss: 0.7672764658927917
Epoch 760, training loss: 12.331690788269043 = 0.19507358968257904 + 2.0 * 6.0683088302612305
Epoch 760, val loss: 0.766291618347168
Epoch 770, training loss: 12.312066078186035 = 0.18515540659427643 + 2.0 * 6.063455104827881
Epoch 770, val loss: 0.7656413912773132
Epoch 780, training loss: 12.297551155090332 = 0.17593252658843994 + 2.0 * 6.060809135437012
Epoch 780, val loss: 0.7656179070472717
Epoch 790, training loss: 12.284512519836426 = 0.1672857403755188 + 2.0 * 6.058613300323486
Epoch 790, val loss: 0.7661513090133667
Epoch 800, training loss: 12.273842811584473 = 0.15915833413600922 + 2.0 * 6.057342052459717
Epoch 800, val loss: 0.7671284675598145
Epoch 810, training loss: 12.269978523254395 = 0.15151454508304596 + 2.0 * 6.059231758117676
Epoch 810, val loss: 0.7685016989707947
Epoch 820, training loss: 12.26953411102295 = 0.14435097575187683 + 2.0 * 6.062591552734375
Epoch 820, val loss: 0.7699818015098572
Epoch 830, training loss: 12.250883102416992 = 0.13765430450439453 + 2.0 * 6.056614398956299
Epoch 830, val loss: 0.7719135284423828
Epoch 840, training loss: 12.242412567138672 = 0.13136491179466248 + 2.0 * 6.055523872375488
Epoch 840, val loss: 0.7742728590965271
Epoch 850, training loss: 12.235679626464844 = 0.12544235587120056 + 2.0 * 6.055118560791016
Epoch 850, val loss: 0.776812732219696
Epoch 860, training loss: 12.224003791809082 = 0.11986032873392105 + 2.0 * 6.052071571350098
Epoch 860, val loss: 0.7796575427055359
Epoch 870, training loss: 12.216484069824219 = 0.11459953337907791 + 2.0 * 6.050942420959473
Epoch 870, val loss: 0.7828183770179749
Epoch 880, training loss: 12.218461036682129 = 0.10962653160095215 + 2.0 * 6.054417133331299
Epoch 880, val loss: 0.7862357497215271
Epoch 890, training loss: 12.207858085632324 = 0.10491491109132767 + 2.0 * 6.051471710205078
Epoch 890, val loss: 0.7896252274513245
Epoch 900, training loss: 12.199326515197754 = 0.1004730612039566 + 2.0 * 6.049426555633545
Epoch 900, val loss: 0.7933956980705261
Epoch 910, training loss: 12.190756797790527 = 0.09625154733657837 + 2.0 * 6.047252655029297
Epoch 910, val loss: 0.7973823547363281
Epoch 920, training loss: 12.188497543334961 = 0.09223838895559311 + 2.0 * 6.048129558563232
Epoch 920, val loss: 0.8015213012695312
Epoch 930, training loss: 12.181375503540039 = 0.08841562271118164 + 2.0 * 6.046480178833008
Epoch 930, val loss: 0.8055417537689209
Epoch 940, training loss: 12.178451538085938 = 0.08478700369596481 + 2.0 * 6.046832084655762
Epoch 940, val loss: 0.8098466396331787
Epoch 950, training loss: 12.178603172302246 = 0.08132974803447723 + 2.0 * 6.0486369132995605
Epoch 950, val loss: 0.8143130540847778
Epoch 960, training loss: 12.165294647216797 = 0.07802008092403412 + 2.0 * 6.043637275695801
Epoch 960, val loss: 0.8187750577926636
Epoch 970, training loss: 12.16025447845459 = 0.07486162334680557 + 2.0 * 6.042696475982666
Epoch 970, val loss: 0.8234096765518188
Epoch 980, training loss: 12.15788745880127 = 0.07184381783008575 + 2.0 * 6.0430216789245605
Epoch 980, val loss: 0.8281059861183167
Epoch 990, training loss: 12.162240028381348 = 0.06896619498729706 + 2.0 * 6.046637058258057
Epoch 990, val loss: 0.8326978087425232
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 730, in <module>
    torch.cuda.empty_cache()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/cuda/memory.py", line 121, in empty_cache
    torch._C._cuda_emptyCache()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.5 der2:0 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 18.705490112304688 = 1.9576102495193481 + 2.0 * 8.373939514160156
Epoch 0, val loss: 1.9548163414001465
Epoch 10, training loss: 18.69493865966797 = 1.947510004043579 + 2.0 * 8.373714447021484
Epoch 10, val loss: 1.945433497428894
Epoch 20, training loss: 18.67913055419922 = 1.9347590208053589 + 2.0 * 8.372185707092285
Epoch 20, val loss: 1.9330629110336304
Epoch 30, training loss: 18.63998794555664 = 1.9165383577346802 + 2.0 * 8.361724853515625
Epoch 30, val loss: 1.915141224861145
Epoch 40, training loss: 18.504554748535156 = 1.8916033506393433 + 2.0 * 8.306475639343262
Epoch 40, val loss: 1.8913040161132812
Epoch 50, training loss: 17.89206314086914 = 1.8654985427856445 + 2.0 * 8.013282775878906
Epoch 50, val loss: 1.8670952320098877
Epoch 60, training loss: 17.252758026123047 = 1.8391722440719604 + 2.0 * 7.706793308258057
Epoch 60, val loss: 1.844359278678894
Epoch 70, training loss: 16.3321590423584 = 1.819931149482727 + 2.0 * 7.256113529205322
Epoch 70, val loss: 1.8277029991149902
Epoch 80, training loss: 15.54023265838623 = 1.8028806447982788 + 2.0 * 6.86867618560791
Epoch 80, val loss: 1.8125388622283936
Epoch 90, training loss: 15.166350364685059 = 1.7876394987106323 + 2.0 * 6.689355373382568
Epoch 90, val loss: 1.7988383769989014
Epoch 100, training loss: 14.948610305786133 = 1.7713127136230469 + 2.0 * 6.588648796081543
Epoch 100, val loss: 1.7846848964691162
Epoch 110, training loss: 14.800070762634277 = 1.7532933950424194 + 2.0 * 6.523388862609863
Epoch 110, val loss: 1.7694214582443237
Epoch 120, training loss: 14.691329956054688 = 1.7346010208129883 + 2.0 * 6.47836446762085
Epoch 120, val loss: 1.7533392906188965
Epoch 130, training loss: 14.595094680786133 = 1.715351939201355 + 2.0 * 6.439871311187744
Epoch 130, val loss: 1.736404538154602
Epoch 140, training loss: 14.515461921691895 = 1.6947041749954224 + 2.0 * 6.410378932952881
Epoch 140, val loss: 1.7184871435165405
Epoch 150, training loss: 14.430624961853027 = 1.6718785762786865 + 2.0 * 6.379373073577881
Epoch 150, val loss: 1.6992170810699463
Epoch 160, training loss: 14.356330871582031 = 1.6464390754699707 + 2.0 * 6.354945659637451
Epoch 160, val loss: 1.6781708002090454
Epoch 170, training loss: 14.288753509521484 = 1.6178643703460693 + 2.0 * 6.335444450378418
Epoch 170, val loss: 1.6546486616134644
Epoch 180, training loss: 14.22307014465332 = 1.5860815048217773 + 2.0 * 6.3184943199157715
Epoch 180, val loss: 1.6287907361984253
Epoch 190, training loss: 14.156988143920898 = 1.550958514213562 + 2.0 * 6.303014755249023
Epoch 190, val loss: 1.600318431854248
Epoch 200, training loss: 14.09101390838623 = 1.5127809047698975 + 2.0 * 6.289116382598877
Epoch 200, val loss: 1.569710612297058
Epoch 210, training loss: 14.025155067443848 = 1.4716871976852417 + 2.0 * 6.276733875274658
Epoch 210, val loss: 1.5371994972229004
Epoch 220, training loss: 13.959122657775879 = 1.4280742406845093 + 2.0 * 6.265524387359619
Epoch 220, val loss: 1.5031415224075317
Epoch 230, training loss: 13.897649765014648 = 1.3826934099197388 + 2.0 * 6.2574782371521
Epoch 230, val loss: 1.4681727886199951
Epoch 240, training loss: 13.830528259277344 = 1.33641517162323 + 2.0 * 6.247056484222412
Epoch 240, val loss: 1.4327858686447144
Epoch 250, training loss: 13.764444351196289 = 1.2894673347473145 + 2.0 * 6.237488746643066
Epoch 250, val loss: 1.3972113132476807
Epoch 260, training loss: 13.705462455749512 = 1.2423230409622192 + 2.0 * 6.231569766998291
Epoch 260, val loss: 1.3618700504302979
Epoch 270, training loss: 13.641462326049805 = 1.196184515953064 + 2.0 * 6.222639083862305
Epoch 270, val loss: 1.3277854919433594
Epoch 280, training loss: 13.580153465270996 = 1.151326298713684 + 2.0 * 6.214413642883301
Epoch 280, val loss: 1.2950724363327026
Epoch 290, training loss: 13.521894454956055 = 1.1077227592468262 + 2.0 * 6.207085609436035
Epoch 290, val loss: 1.2635687589645386
Epoch 300, training loss: 13.471535682678223 = 1.0656147003173828 + 2.0 * 6.20296049118042
Epoch 300, val loss: 1.2335035800933838
Epoch 310, training loss: 13.415834426879883 = 1.0255131721496582 + 2.0 * 6.195160865783691
Epoch 310, val loss: 1.2049628496170044
Epoch 320, training loss: 13.36411190032959 = 0.9867920279502869 + 2.0 * 6.188660144805908
Epoch 320, val loss: 1.1778151988983154
Epoch 330, training loss: 13.313796043395996 = 0.9492028951644897 + 2.0 * 6.1822967529296875
Epoch 330, val loss: 1.1515699625015259
Epoch 340, training loss: 13.267868995666504 = 0.9125502705574036 + 2.0 * 6.177659511566162
Epoch 340, val loss: 1.1261097192764282
Epoch 350, training loss: 13.228403091430664 = 0.8766541481018066 + 2.0 * 6.17587423324585
Epoch 350, val loss: 1.1014628410339355
Epoch 360, training loss: 13.179094314575195 = 0.8417986035346985 + 2.0 * 6.168647766113281
Epoch 360, val loss: 1.0772268772125244
Epoch 370, training loss: 13.13275146484375 = 0.8073501586914062 + 2.0 * 6.162700653076172
Epoch 370, val loss: 1.0534913539886475
Epoch 380, training loss: 13.089988708496094 = 0.7733612060546875 + 2.0 * 6.158313751220703
Epoch 380, val loss: 1.030210018157959
Epoch 390, training loss: 13.053375244140625 = 0.739992082118988 + 2.0 * 6.156691551208496
Epoch 390, val loss: 1.0073950290679932
Epoch 400, training loss: 13.013324737548828 = 0.7074861526489258 + 2.0 * 6.152919292449951
Epoch 400, val loss: 0.9855036735534668
Epoch 410, training loss: 12.972444534301758 = 0.6761741042137146 + 2.0 * 6.148135185241699
Epoch 410, val loss: 0.9647872447967529
Epoch 420, training loss: 12.935778617858887 = 0.6461824178695679 + 2.0 * 6.144798278808594
Epoch 420, val loss: 0.9453275203704834
Epoch 430, training loss: 12.903512001037598 = 0.6177341341972351 + 2.0 * 6.142889022827148
Epoch 430, val loss: 0.9273002743721008
Epoch 440, training loss: 12.86949348449707 = 0.5911393165588379 + 2.0 * 6.139177322387695
Epoch 440, val loss: 0.9110630750656128
Epoch 450, training loss: 12.835502624511719 = 0.566133975982666 + 2.0 * 6.134684085845947
Epoch 450, val loss: 0.8966849446296692
Epoch 460, training loss: 12.81141185760498 = 0.542713463306427 + 2.0 * 6.134349346160889
Epoch 460, val loss: 0.8840705752372742
Epoch 470, training loss: 12.791471481323242 = 0.5209839940071106 + 2.0 * 6.135243892669678
Epoch 470, val loss: 0.8730112910270691
Epoch 480, training loss: 12.757225036621094 = 0.5008424520492554 + 2.0 * 6.1281914710998535
Epoch 480, val loss: 0.8635951280593872
Epoch 490, training loss: 12.731169700622559 = 0.4819391965866089 + 2.0 * 6.12461519241333
Epoch 490, val loss: 0.8556563854217529
Epoch 500, training loss: 12.709774017333984 = 0.4641478955745697 + 2.0 * 6.1228132247924805
Epoch 500, val loss: 0.8489642143249512
Epoch 510, training loss: 12.690298080444336 = 0.4473273456096649 + 2.0 * 6.121485233306885
Epoch 510, val loss: 0.8434112071990967
Epoch 520, training loss: 12.671552658081055 = 0.43139490485191345 + 2.0 * 6.120079040527344
Epoch 520, val loss: 0.8388798236846924
Epoch 530, training loss: 12.649284362792969 = 0.4161699116230011 + 2.0 * 6.1165571212768555
Epoch 530, val loss: 0.8351632356643677
Epoch 540, training loss: 12.6305513381958 = 0.40149369835853577 + 2.0 * 6.114528656005859
Epoch 540, val loss: 0.8322492241859436
Epoch 550, training loss: 12.610987663269043 = 0.38732483983039856 + 2.0 * 6.111831188201904
Epoch 550, val loss: 0.8299486041069031
Epoch 560, training loss: 12.597281455993652 = 0.37348294258117676 + 2.0 * 6.111899375915527
Epoch 560, val loss: 0.8282859325408936
Epoch 570, training loss: 12.579740524291992 = 0.3600030541419983 + 2.0 * 6.10986852645874
Epoch 570, val loss: 0.8270604014396667
Epoch 580, training loss: 12.559819221496582 = 0.3468324840068817 + 2.0 * 6.1064934730529785
Epoch 580, val loss: 0.8264116644859314
Epoch 590, training loss: 12.543313980102539 = 0.33398008346557617 + 2.0 * 6.104666709899902
Epoch 590, val loss: 0.8262495398521423
Epoch 600, training loss: 12.527006149291992 = 0.32143184542655945 + 2.0 * 6.102787017822266
Epoch 600, val loss: 0.826614260673523
Epoch 610, training loss: 12.508849143981934 = 0.3091906011104584 + 2.0 * 6.099829196929932
Epoch 610, val loss: 0.8275193572044373
Epoch 620, training loss: 12.4935302734375 = 0.29720449447631836 + 2.0 * 6.09816312789917
Epoch 620, val loss: 0.828993022441864
Epoch 630, training loss: 12.491762161254883 = 0.28553640842437744 + 2.0 * 6.103112697601318
Epoch 630, val loss: 0.8310046792030334
Epoch 640, training loss: 12.464888572692871 = 0.2741421163082123 + 2.0 * 6.095373153686523
Epoch 640, val loss: 0.8335151076316833
Epoch 650, training loss: 12.447705268859863 = 0.263109028339386 + 2.0 * 6.0922980308532715
Epoch 650, val loss: 0.836591362953186
Epoch 660, training loss: 12.44459342956543 = 0.2523528039455414 + 2.0 * 6.096120357513428
Epoch 660, val loss: 0.8401086926460266
Epoch 670, training loss: 12.42216682434082 = 0.24191191792488098 + 2.0 * 6.090127468109131
Epoch 670, val loss: 0.844059407711029
Epoch 680, training loss: 12.408120155334473 = 0.23178023099899292 + 2.0 * 6.088170051574707
Epoch 680, val loss: 0.8484208583831787
Epoch 690, training loss: 12.394986152648926 = 0.22193054854869843 + 2.0 * 6.0865278244018555
Epoch 690, val loss: 0.8532223105430603
Epoch 700, training loss: 12.381505966186523 = 0.21230781078338623 + 2.0 * 6.084599018096924
Epoch 700, val loss: 0.8582890629768372
Epoch 710, training loss: 12.408785820007324 = 0.20289994776248932 + 2.0 * 6.102942943572998
Epoch 710, val loss: 0.8635987639427185
Epoch 720, training loss: 12.363265991210938 = 0.1938435137271881 + 2.0 * 6.084711074829102
Epoch 720, val loss: 0.8691750168800354
Epoch 730, training loss: 12.347744941711426 = 0.1850345879793167 + 2.0 * 6.081355094909668
Epoch 730, val loss: 0.8750103712081909
Epoch 740, training loss: 12.33475399017334 = 0.1764802783727646 + 2.0 * 6.079136848449707
Epoch 740, val loss: 0.8812075853347778
Epoch 750, training loss: 12.323197364807129 = 0.16817161440849304 + 2.0 * 6.077512741088867
Epoch 750, val loss: 0.8876025080680847
Epoch 760, training loss: 12.325380325317383 = 0.1601410061120987 + 2.0 * 6.082619667053223
Epoch 760, val loss: 0.8942068219184875
Epoch 770, training loss: 12.308244705200195 = 0.15240782499313354 + 2.0 * 6.077918529510498
Epoch 770, val loss: 0.9010728597640991
Epoch 780, training loss: 12.294910430908203 = 0.1450018584728241 + 2.0 * 6.074954509735107
Epoch 780, val loss: 0.9080982804298401
Epoch 790, training loss: 12.296624183654785 = 0.13791722059249878 + 2.0 * 6.079353332519531
Epoch 790, val loss: 0.9152557849884033
Epoch 800, training loss: 12.284385681152344 = 0.13120262324810028 + 2.0 * 6.076591491699219
Epoch 800, val loss: 0.92274409532547
Epoch 810, training loss: 12.266210556030273 = 0.1247919350862503 + 2.0 * 6.070709228515625
Epoch 810, val loss: 0.9302494525909424
Epoch 820, training loss: 12.258077621459961 = 0.1187269315123558 + 2.0 * 6.069675445556641
Epoch 820, val loss: 0.937978208065033
Epoch 830, training loss: 12.249734878540039 = 0.11297775059938431 + 2.0 * 6.068378448486328
Epoch 830, val loss: 0.945931077003479
Epoch 840, training loss: 12.262107849121094 = 0.10754559934139252 + 2.0 * 6.0772809982299805
Epoch 840, val loss: 0.9540183544158936
Epoch 850, training loss: 12.236967086791992 = 0.10240472853183746 + 2.0 * 6.067281246185303
Epoch 850, val loss: 0.9620804190635681
Epoch 860, training loss: 12.229323387145996 = 0.09758228808641434 + 2.0 * 6.065870761871338
Epoch 860, val loss: 0.9703419804573059
Epoch 870, training loss: 12.221816062927246 = 0.09303843975067139 + 2.0 * 6.064388751983643
Epoch 870, val loss: 0.978696346282959
Epoch 880, training loss: 12.23586654663086 = 0.08875592052936554 + 2.0 * 6.0735554695129395
Epoch 880, val loss: 0.9870490431785583
Epoch 890, training loss: 12.212137222290039 = 0.08473782241344452 + 2.0 * 6.063699722290039
Epoch 890, val loss: 0.9953982830047607
Epoch 900, training loss: 12.205373764038086 = 0.08098143339157104 + 2.0 * 6.062196254730225
Epoch 900, val loss: 1.0039106607437134
Epoch 910, training loss: 12.198796272277832 = 0.07743363827466965 + 2.0 * 6.060681343078613
Epoch 910, val loss: 1.012455940246582
Epoch 920, training loss: 12.197834968566895 = 0.0740966796875 + 2.0 * 6.061869144439697
Epoch 920, val loss: 1.0210233926773071
Epoch 930, training loss: 12.189197540283203 = 0.07095050811767578 + 2.0 * 6.059123516082764
Epoch 930, val loss: 1.0294831991195679
Epoch 940, training loss: 12.183568000793457 = 0.0679885521531105 + 2.0 * 6.0577898025512695
Epoch 940, val loss: 1.0379550457000732
Epoch 950, training loss: 12.178813934326172 = 0.06520331650972366 + 2.0 * 6.05680513381958
Epoch 950, val loss: 1.0464764833450317
Epoch 960, training loss: 12.177783012390137 = 0.06257347762584686 + 2.0 * 6.057604789733887
Epoch 960, val loss: 1.0549219846725464
Epoch 970, training loss: 12.170650482177734 = 0.06009136140346527 + 2.0 * 6.055279731750488
Epoch 970, val loss: 1.0631659030914307
Epoch 980, training loss: 12.170768737792969 = 0.05774747580289841 + 2.0 * 6.0565104484558105
Epoch 980, val loss: 1.0716367959976196
Epoch 990, training loss: 12.162741661071777 = 0.05552896112203598 + 2.0 * 6.0536065101623535
Epoch 990, val loss: 1.0799001455307007
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 730, in <module>
    torch.cuda.empty_cache()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/cuda/memory.py", line 121, in empty_cache
    torch._C._cuda_emptyCache()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.5 der2:0.2 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 18.6890869140625 = 1.9414936304092407 + 2.0 * 8.373796463012695
Epoch 0, val loss: 1.9402884244918823
Epoch 10, training loss: 18.676956176757812 = 1.9309202432632446 + 2.0 * 8.373018264770508
Epoch 10, val loss: 1.9301294088363647
Epoch 20, training loss: 18.654163360595703 = 1.917393684387207 + 2.0 * 8.36838436126709
Epoch 20, val loss: 1.9166698455810547
Epoch 30, training loss: 18.574108123779297 = 1.899100422859192 + 2.0 * 8.337503433227539
Epoch 30, val loss: 1.89835524559021
Epoch 40, training loss: 18.14820098876953 = 1.8779797554016113 + 2.0 * 8.135110855102539
Epoch 40, val loss: 1.8778350353240967
Epoch 50, training loss: 16.665464401245117 = 1.8553917407989502 + 2.0 * 7.405036449432373
Epoch 50, val loss: 1.8559601306915283
Epoch 60, training loss: 15.85208797454834 = 1.8423086404800415 + 2.0 * 7.004889488220215
Epoch 60, val loss: 1.84463632106781
Epoch 70, training loss: 15.388586044311523 = 1.832719326019287 + 2.0 * 6.777933120727539
Epoch 70, val loss: 1.8361380100250244
Epoch 80, training loss: 15.080869674682617 = 1.823350191116333 + 2.0 * 6.628759860992432
Epoch 80, val loss: 1.8279565572738647
Epoch 90, training loss: 14.87350845336914 = 1.8148019313812256 + 2.0 * 6.529353141784668
Epoch 90, val loss: 1.82033371925354
Epoch 100, training loss: 14.709901809692383 = 1.8073383569717407 + 2.0 * 6.451281547546387
Epoch 100, val loss: 1.8132814168930054
Epoch 110, training loss: 14.585631370544434 = 1.8010666370391846 + 2.0 * 6.392282485961914
Epoch 110, val loss: 1.8070595264434814
Epoch 120, training loss: 14.492663383483887 = 1.7952021360397339 + 2.0 * 6.348730564117432
Epoch 120, val loss: 1.801129937171936
Epoch 130, training loss: 14.419246673583984 = 1.7891346216201782 + 2.0 * 6.315055847167969
Epoch 130, val loss: 1.7951279878616333
Epoch 140, training loss: 14.357358932495117 = 1.782693862915039 + 2.0 * 6.287332534790039
Epoch 140, val loss: 1.7888970375061035
Epoch 150, training loss: 14.302604675292969 = 1.7757091522216797 + 2.0 * 6.2634477615356445
Epoch 150, val loss: 1.7823601961135864
Epoch 160, training loss: 14.257808685302734 = 1.7679985761642456 + 2.0 * 6.2449049949646
Epoch 160, val loss: 1.7753736972808838
Epoch 170, training loss: 14.21010971069336 = 1.759433627128601 + 2.0 * 6.225337982177734
Epoch 170, val loss: 1.7678457498550415
Epoch 180, training loss: 14.169569969177246 = 1.7497198581695557 + 2.0 * 6.209925174713135
Epoch 180, val loss: 1.7594765424728394
Epoch 190, training loss: 14.133064270019531 = 1.73856782913208 + 2.0 * 6.197248458862305
Epoch 190, val loss: 1.7500039339065552
Epoch 200, training loss: 14.095523834228516 = 1.7256801128387451 + 2.0 * 6.184921741485596
Epoch 200, val loss: 1.7391774654388428
Epoch 210, training loss: 14.059657096862793 = 1.710716962814331 + 2.0 * 6.174469947814941
Epoch 210, val loss: 1.7267348766326904
Epoch 220, training loss: 14.025447845458984 = 1.6933190822601318 + 2.0 * 6.166064262390137
Epoch 220, val loss: 1.7124371528625488
Epoch 230, training loss: 13.987382888793945 = 1.6731102466583252 + 2.0 * 6.1571364402771
Epoch 230, val loss: 1.6958264112472534
Epoch 240, training loss: 13.948704719543457 = 1.649579644203186 + 2.0 * 6.149562358856201
Epoch 240, val loss: 1.6764448881149292
Epoch 250, training loss: 13.90749740600586 = 1.6221928596496582 + 2.0 * 6.1426520347595215
Epoch 250, val loss: 1.6537963151931763
Epoch 260, training loss: 13.864924430847168 = 1.5907087326049805 + 2.0 * 6.137107849121094
Epoch 260, val loss: 1.6277172565460205
Epoch 270, training loss: 13.818222045898438 = 1.5548956394195557 + 2.0 * 6.1316633224487305
Epoch 270, val loss: 1.59787118434906
Epoch 280, training loss: 13.768942832946777 = 1.5150762796401978 + 2.0 * 6.1269330978393555
Epoch 280, val loss: 1.5647447109222412
Epoch 290, training loss: 13.71863842010498 = 1.4718592166900635 + 2.0 * 6.123389720916748
Epoch 290, val loss: 1.5291085243225098
Epoch 300, training loss: 13.663379669189453 = 1.4265484809875488 + 2.0 * 6.118415355682373
Epoch 300, val loss: 1.4920381307601929
Epoch 310, training loss: 13.61106014251709 = 1.380420446395874 + 2.0 * 6.115319728851318
Epoch 310, val loss: 1.4547783136367798
Epoch 320, training loss: 13.56329345703125 = 1.3353251218795776 + 2.0 * 6.113984107971191
Epoch 320, val loss: 1.4191418886184692
Epoch 330, training loss: 13.509576797485352 = 1.2924100160598755 + 2.0 * 6.108583450317383
Epoch 330, val loss: 1.3856860399246216
Epoch 340, training loss: 13.459661483764648 = 1.251293420791626 + 2.0 * 6.104184150695801
Epoch 340, val loss: 1.354081392288208
Epoch 350, training loss: 13.41313362121582 = 1.2116719484329224 + 2.0 * 6.100730895996094
Epoch 350, val loss: 1.3240282535552979
Epoch 360, training loss: 13.369244575500488 = 1.1732043027877808 + 2.0 * 6.098020076751709
Epoch 360, val loss: 1.295343041419983
Epoch 370, training loss: 13.33193302154541 = 1.1357651948928833 + 2.0 * 6.098083972930908
Epoch 370, val loss: 1.2678899765014648
Epoch 380, training loss: 13.287004470825195 = 1.0992045402526855 + 2.0 * 6.093899726867676
Epoch 380, val loss: 1.241458773612976
Epoch 390, training loss: 13.243965148925781 = 1.0629441738128662 + 2.0 * 6.090510368347168
Epoch 390, val loss: 1.21519935131073
Epoch 400, training loss: 13.201017379760742 = 1.026278018951416 + 2.0 * 6.087369441986084
Epoch 400, val loss: 1.1887036561965942
Epoch 410, training loss: 13.159018516540527 = 0.9891136288642883 + 2.0 * 6.084952354431152
Epoch 410, val loss: 1.161691665649414
Epoch 420, training loss: 13.125043869018555 = 0.95137619972229 + 2.0 * 6.086833953857422
Epoch 420, val loss: 1.1342118978500366
Epoch 430, training loss: 13.076409339904785 = 0.913459837436676 + 2.0 * 6.081474781036377
Epoch 430, val loss: 1.1064246892929077
Epoch 440, training loss: 13.055685043334961 = 0.875523567199707 + 2.0 * 6.090080738067627
Epoch 440, val loss: 1.0786981582641602
Epoch 450, training loss: 12.997969627380371 = 0.8385359048843384 + 2.0 * 6.079716682434082
Epoch 450, val loss: 1.051265001296997
Epoch 460, training loss: 12.953474998474121 = 0.8021572828292847 + 2.0 * 6.075658798217773
Epoch 460, val loss: 1.0242949724197388
Epoch 470, training loss: 12.913890838623047 = 0.7663623094558716 + 2.0 * 6.073764324188232
Epoch 470, val loss: 0.9977429509162903
Epoch 480, training loss: 12.875694274902344 = 0.7312039136886597 + 2.0 * 6.072245121002197
Epoch 480, val loss: 0.971840500831604
Epoch 490, training loss: 12.838790893554688 = 0.6969832181930542 + 2.0 * 6.070903778076172
Epoch 490, val loss: 0.9468472599983215
Epoch 500, training loss: 12.802785873413086 = 0.6640479564666748 + 2.0 * 6.069368839263916
Epoch 500, val loss: 0.9231148958206177
Epoch 510, training loss: 12.767007827758789 = 0.6321455836296082 + 2.0 * 6.0674309730529785
Epoch 510, val loss: 0.9004125595092773
Epoch 520, training loss: 12.733007431030273 = 0.6011824011802673 + 2.0 * 6.06591272354126
Epoch 520, val loss: 0.8787692785263062
Epoch 530, training loss: 12.715251922607422 = 0.5713204145431519 + 2.0 * 6.07196569442749
Epoch 530, val loss: 0.8582490086555481
Epoch 540, training loss: 12.669731140136719 = 0.5428133010864258 + 2.0 * 6.0634589195251465
Epoch 540, val loss: 0.839294970035553
Epoch 550, training loss: 12.64123821258545 = 0.5155032873153687 + 2.0 * 6.062867641448975
Epoch 550, val loss: 0.8214665055274963
Epoch 560, training loss: 12.608956336975098 = 0.48911064863204956 + 2.0 * 6.059922695159912
Epoch 560, val loss: 0.804544985294342
Epoch 570, training loss: 12.581012725830078 = 0.46349668502807617 + 2.0 * 6.058757781982422
Epoch 570, val loss: 0.7886095643043518
Epoch 580, training loss: 12.558089256286621 = 0.4387612044811249 + 2.0 * 6.059664249420166
Epoch 580, val loss: 0.7735519409179688
Epoch 590, training loss: 12.531963348388672 = 0.4150538146495819 + 2.0 * 6.058454990386963
Epoch 590, val loss: 0.7594937086105347
Epoch 600, training loss: 12.508275032043457 = 0.3923113942146301 + 2.0 * 6.057981967926025
Epoch 600, val loss: 0.746338427066803
Epoch 610, training loss: 12.480090141296387 = 0.370508074760437 + 2.0 * 6.05479097366333
Epoch 610, val loss: 0.7341263890266418
Epoch 620, training loss: 12.454572677612305 = 0.3496694266796112 + 2.0 * 6.0524516105651855
Epoch 620, val loss: 0.7228054404258728
Epoch 630, training loss: 12.43349552154541 = 0.3296596109867096 + 2.0 * 6.051918029785156
Epoch 630, val loss: 0.7123154997825623
Epoch 640, training loss: 12.426926612854004 = 0.31048116087913513 + 2.0 * 6.058222770690918
Epoch 640, val loss: 0.7026341557502747
Epoch 650, training loss: 12.39589786529541 = 0.2922126054763794 + 2.0 * 6.05184268951416
Epoch 650, val loss: 0.6938351988792419
Epoch 660, training loss: 12.370475769042969 = 0.2749010920524597 + 2.0 * 6.047787189483643
Epoch 660, val loss: 0.6858944892883301
Epoch 670, training loss: 12.353047370910645 = 0.2583732306957245 + 2.0 * 6.047337055206299
Epoch 670, val loss: 0.6787058115005493
Epoch 680, training loss: 12.343976020812988 = 0.24264691770076752 + 2.0 * 6.05066442489624
Epoch 680, val loss: 0.6722204685211182
Epoch 690, training loss: 12.317739486694336 = 0.2279222011566162 + 2.0 * 6.04490852355957
Epoch 690, val loss: 0.6665862798690796
Epoch 700, training loss: 12.302489280700684 = 0.21398106217384338 + 2.0 * 6.044254302978516
Epoch 700, val loss: 0.6616837382316589
Epoch 710, training loss: 12.300140380859375 = 0.20088571310043335 + 2.0 * 6.049627304077148
Epoch 710, val loss: 0.6575199365615845
Epoch 720, training loss: 12.275055885314941 = 0.18874195218086243 + 2.0 * 6.04315710067749
Epoch 720, val loss: 0.6541115641593933
Epoch 730, training loss: 12.259332656860352 = 0.17740583419799805 + 2.0 * 6.040963172912598
Epoch 730, val loss: 0.6514041423797607
Epoch 740, training loss: 12.261764526367188 = 0.16680920124053955 + 2.0 * 6.047477722167969
Epoch 740, val loss: 0.6492940187454224
Epoch 750, training loss: 12.241593360900879 = 0.15711398422718048 + 2.0 * 6.042239665985107
Epoch 750, val loss: 0.647875189781189
Epoch 760, training loss: 12.224252700805664 = 0.14802849292755127 + 2.0 * 6.038112163543701
Epoch 760, val loss: 0.6470834016799927
Epoch 770, training loss: 12.214634895324707 = 0.13962523639202118 + 2.0 * 6.03750467300415
Epoch 770, val loss: 0.6468154788017273
Epoch 780, training loss: 12.210946083068848 = 0.1318470537662506 + 2.0 * 6.039549350738525
Epoch 780, val loss: 0.6469626426696777
Epoch 790, training loss: 12.200262069702148 = 0.12468194216489792 + 2.0 * 6.037790298461914
Epoch 790, val loss: 0.6476934552192688
Epoch 800, training loss: 12.187731742858887 = 0.11804839223623276 + 2.0 * 6.034841537475586
Epoch 800, val loss: 0.6487762928009033
Epoch 810, training loss: 12.179612159729004 = 0.11185574531555176 + 2.0 * 6.033878326416016
Epoch 810, val loss: 0.6502094268798828
Epoch 820, training loss: 12.187620162963867 = 0.1060962900519371 + 2.0 * 6.040761947631836
Epoch 820, val loss: 0.6519431471824646
Epoch 830, training loss: 12.173040390014648 = 0.10077663511037827 + 2.0 * 6.036131858825684
Epoch 830, val loss: 0.6539455652236938
Epoch 840, training loss: 12.16037654876709 = 0.09588988125324249 + 2.0 * 6.032243251800537
Epoch 840, val loss: 0.6562555432319641
Epoch 850, training loss: 12.15396785736084 = 0.09131616353988647 + 2.0 * 6.031325817108154
Epoch 850, val loss: 0.658767580986023
Epoch 860, training loss: 12.14739990234375 = 0.0870419591665268 + 2.0 * 6.030179023742676
Epoch 860, val loss: 0.6614097952842712
Epoch 870, training loss: 12.14395809173584 = 0.08302638679742813 + 2.0 * 6.030466079711914
Epoch 870, val loss: 0.664226233959198
Epoch 880, training loss: 12.139420509338379 = 0.07928718626499176 + 2.0 * 6.03006649017334
Epoch 880, val loss: 0.6671274304389954
Epoch 890, training loss: 12.13596248626709 = 0.07581078261137009 + 2.0 * 6.030076026916504
Epoch 890, val loss: 0.6701919436454773
Epoch 900, training loss: 12.128111839294434 = 0.07254548370838165 + 2.0 * 6.027783393859863
Epoch 900, val loss: 0.6733271479606628
Epoch 910, training loss: 12.132651329040527 = 0.06947600841522217 + 2.0 * 6.031587600708008
Epoch 910, val loss: 0.676505982875824
Epoch 920, training loss: 12.129602432250977 = 0.06660547852516174 + 2.0 * 6.031498432159424
Epoch 920, val loss: 0.6797518134117126
Epoch 930, training loss: 12.115955352783203 = 0.06393042951822281 + 2.0 * 6.026012420654297
Epoch 930, val loss: 0.6830984354019165
Epoch 940, training loss: 12.111777305603027 = 0.06139227747917175 + 2.0 * 6.025192737579346
Epoch 940, val loss: 0.6864825487136841
Epoch 950, training loss: 12.106867790222168 = 0.05898800492286682 + 2.0 * 6.023940086364746
Epoch 950, val loss: 0.689910888671875
Epoch 960, training loss: 12.117755889892578 = 0.05670557916164398 + 2.0 * 6.030525207519531
Epoch 960, val loss: 0.6933209300041199
Epoch 970, training loss: 12.108399391174316 = 0.054588790982961655 + 2.0 * 6.026905536651611
Epoch 970, val loss: 0.6968495845794678
Epoch 980, training loss: 12.099021911621094 = 0.052574522793293 + 2.0 * 6.023223876953125
Epoch 980, val loss: 0.7004119753837585
Epoch 990, training loss: 12.103912353515625 = 0.05067276582121849 + 2.0 * 6.026619911193848
Epoch 990, val loss: 0.7038922309875488
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 730, in <module>
    torch.cuda.empty_cache()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/cuda/memory.py", line 121, in empty_cache
    torch._C._cuda_emptyCache()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.5 der2:0.2 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 18.693151473999023 = 1.9455854892730713 + 2.0 * 8.373783111572266
Epoch 0, val loss: 1.9461534023284912
Epoch 10, training loss: 18.681556701660156 = 1.935276985168457 + 2.0 * 8.373140335083008
Epoch 10, val loss: 1.936330795288086
Epoch 20, training loss: 18.661020278930664 = 1.9218685626983643 + 2.0 * 8.369575500488281
Epoch 20, val loss: 1.923186182975769
Epoch 30, training loss: 18.597942352294922 = 1.9031141996383667 + 2.0 * 8.347414016723633
Epoch 30, val loss: 1.904693603515625
Epoch 40, training loss: 18.267589569091797 = 1.880481481552124 + 2.0 * 8.193553924560547
Epoch 40, val loss: 1.8833503723144531
Epoch 50, training loss: 16.775278091430664 = 1.8561756610870361 + 2.0 * 7.4595513343811035
Epoch 50, val loss: 1.8604590892791748
Epoch 60, training loss: 15.901060104370117 = 1.8382151126861572 + 2.0 * 7.0314226150512695
Epoch 60, val loss: 1.8449808359146118
Epoch 70, training loss: 15.329150199890137 = 1.8294275999069214 + 2.0 * 6.749861240386963
Epoch 70, val loss: 1.8369485139846802
Epoch 80, training loss: 15.037946701049805 = 1.8202351331710815 + 2.0 * 6.608855724334717
Epoch 80, val loss: 1.8287009000778198
Epoch 90, training loss: 14.80914306640625 = 1.8109604120254517 + 2.0 * 6.499091148376465
Epoch 90, val loss: 1.8204015493392944
Epoch 100, training loss: 14.666193962097168 = 1.802242636680603 + 2.0 * 6.431975841522217
Epoch 100, val loss: 1.812558650970459
Epoch 110, training loss: 14.561176300048828 = 1.7940473556518555 + 2.0 * 6.383564472198486
Epoch 110, val loss: 1.8049372434616089
Epoch 120, training loss: 14.471952438354492 = 1.7858332395553589 + 2.0 * 6.343059539794922
Epoch 120, val loss: 1.7971112728118896
Epoch 130, training loss: 14.39523696899414 = 1.7776174545288086 + 2.0 * 6.308809757232666
Epoch 130, val loss: 1.7892149686813354
Epoch 140, training loss: 14.336077690124512 = 1.7691246271133423 + 2.0 * 6.28347635269165
Epoch 140, val loss: 1.7813360691070557
Epoch 150, training loss: 14.282371520996094 = 1.7598131895065308 + 2.0 * 6.261279106140137
Epoch 150, val loss: 1.7730207443237305
Epoch 160, training loss: 14.233274459838867 = 1.7494726181030273 + 2.0 * 6.24190092086792
Epoch 160, val loss: 1.7641808986663818
Epoch 170, training loss: 14.190853118896484 = 1.7378833293914795 + 2.0 * 6.226484775543213
Epoch 170, val loss: 1.7545186281204224
Epoch 180, training loss: 14.145660400390625 = 1.7246179580688477 + 2.0 * 6.210521221160889
Epoch 180, val loss: 1.7436877489089966
Epoch 190, training loss: 14.102859497070312 = 1.709263801574707 + 2.0 * 6.196797847747803
Epoch 190, val loss: 1.7314088344573975
Epoch 200, training loss: 14.063031196594238 = 1.6914430856704712 + 2.0 * 6.185793876647949
Epoch 200, val loss: 1.7172186374664307
Epoch 210, training loss: 14.0206937789917 = 1.6707464456558228 + 2.0 * 6.174973487854004
Epoch 210, val loss: 1.7009162902832031
Epoch 220, training loss: 13.977252960205078 = 1.6466370820999146 + 2.0 * 6.165307998657227
Epoch 220, val loss: 1.6819459199905396
Epoch 230, training loss: 13.933076858520508 = 1.6184340715408325 + 2.0 * 6.157321453094482
Epoch 230, val loss: 1.659685730934143
Epoch 240, training loss: 13.886112213134766 = 1.5858101844787598 + 2.0 * 6.150151252746582
Epoch 240, val loss: 1.6337810754776
Epoch 250, training loss: 13.836103439331055 = 1.548517107963562 + 2.0 * 6.143793106079102
Epoch 250, val loss: 1.6042566299438477
Epoch 260, training loss: 13.786653518676758 = 1.5063552856445312 + 2.0 * 6.140149116516113
Epoch 260, val loss: 1.5707460641860962
Epoch 270, training loss: 13.727069854736328 = 1.4600809812545776 + 2.0 * 6.1334943771362305
Epoch 270, val loss: 1.5337469577789307
Epoch 280, training loss: 13.669441223144531 = 1.4102592468261719 + 2.0 * 6.12959098815918
Epoch 280, val loss: 1.4939125776290894
Epoch 290, training loss: 13.608758926391602 = 1.3582566976547241 + 2.0 * 6.125251293182373
Epoch 290, val loss: 1.4524633884429932
Epoch 300, training loss: 13.547619819641113 = 1.3055917024612427 + 2.0 * 6.12101411819458
Epoch 300, val loss: 1.4106382131576538
Epoch 310, training loss: 13.487207412719727 = 1.2532484531402588 + 2.0 * 6.116979598999023
Epoch 310, val loss: 1.369372844696045
Epoch 320, training loss: 13.442233085632324 = 1.202136754989624 + 2.0 * 6.1200480461120605
Epoch 320, val loss: 1.3291481733322144
Epoch 330, training loss: 13.376124382019043 = 1.1536986827850342 + 2.0 * 6.111212730407715
Epoch 330, val loss: 1.291733741760254
Epoch 340, training loss: 13.322103500366211 = 1.107833743095398 + 2.0 * 6.107134819030762
Epoch 340, val loss: 1.2564287185668945
Epoch 350, training loss: 13.273731231689453 = 1.0643789768218994 + 2.0 * 6.104676246643066
Epoch 350, val loss: 1.2229890823364258
Epoch 360, training loss: 13.228252410888672 = 1.0237956047058105 + 2.0 * 6.10222864151001
Epoch 360, val loss: 1.19203782081604
Epoch 370, training loss: 13.183006286621094 = 0.9858025312423706 + 2.0 * 6.098601818084717
Epoch 370, val loss: 1.163275122642517
Epoch 380, training loss: 13.139615058898926 = 0.9493324160575867 + 2.0 * 6.095141410827637
Epoch 380, val loss: 1.1359277963638306
Epoch 390, training loss: 13.11324691772461 = 0.9139922857284546 + 2.0 * 6.099627494812012
Epoch 390, val loss: 1.109529972076416
Epoch 400, training loss: 13.064173698425293 = 0.8800402283668518 + 2.0 * 6.092066764831543
Epoch 400, val loss: 1.0842385292053223
Epoch 410, training loss: 13.022713661193848 = 0.8467007279396057 + 2.0 * 6.088006496429443
Epoch 410, val loss: 1.059631586074829
Epoch 420, training loss: 12.987451553344727 = 0.8135110139846802 + 2.0 * 6.086970329284668
Epoch 420, val loss: 1.0353782176971436
Epoch 430, training loss: 12.951001167297363 = 0.7804602980613708 + 2.0 * 6.085270404815674
Epoch 430, val loss: 1.011582851409912
Epoch 440, training loss: 12.916054725646973 = 0.747857391834259 + 2.0 * 6.084098815917969
Epoch 440, val loss: 0.9884302020072937
Epoch 450, training loss: 12.875946998596191 = 0.7155532836914062 + 2.0 * 6.080196857452393
Epoch 450, val loss: 0.9661008715629578
Epoch 460, training loss: 12.838903427124023 = 0.6835195422172546 + 2.0 * 6.077692031860352
Epoch 460, val loss: 0.9443978071212769
Epoch 470, training loss: 12.803256034851074 = 0.6518325805664062 + 2.0 * 6.075711727142334
Epoch 470, val loss: 0.923337459564209
Epoch 480, training loss: 12.776965141296387 = 0.6209157109260559 + 2.0 * 6.078024864196777
Epoch 480, val loss: 0.9032685160636902
Epoch 490, training loss: 12.741943359375 = 0.5914692282676697 + 2.0 * 6.075237274169922
Epoch 490, val loss: 0.884778618812561
Epoch 500, training loss: 12.704447746276855 = 0.5632128119468689 + 2.0 * 6.07061767578125
Epoch 500, val loss: 0.8676776885986328
Epoch 510, training loss: 12.674745559692383 = 0.5361039638519287 + 2.0 * 6.0693206787109375
Epoch 510, val loss: 0.8517096042633057
Epoch 520, training loss: 12.647635459899902 = 0.5102466344833374 + 2.0 * 6.068694591522217
Epoch 520, val loss: 0.8370290994644165
Epoch 530, training loss: 12.628849029541016 = 0.4859183132648468 + 2.0 * 6.071465492248535
Epoch 530, val loss: 0.823913037776947
Epoch 540, training loss: 12.595864295959473 = 0.4631933271884918 + 2.0 * 6.066335678100586
Epoch 540, val loss: 0.8123477101325989
Epoch 550, training loss: 12.567632675170898 = 0.44183292984962463 + 2.0 * 6.062900066375732
Epoch 550, val loss: 0.8019548058509827
Epoch 560, training loss: 12.544574737548828 = 0.4216904640197754 + 2.0 * 6.0614423751831055
Epoch 560, val loss: 0.7927606701850891
Epoch 570, training loss: 12.52437973022461 = 0.40258151292800903 + 2.0 * 6.060899257659912
Epoch 570, val loss: 0.7845537662506104
Epoch 580, training loss: 12.513086318969727 = 0.38457155227661133 + 2.0 * 6.0642571449279785
Epoch 580, val loss: 0.7772354483604431
Epoch 590, training loss: 12.482821464538574 = 0.3675641119480133 + 2.0 * 6.057628631591797
Epoch 590, val loss: 0.7709599137306213
Epoch 600, training loss: 12.468274116516113 = 0.3515317440032959 + 2.0 * 6.058371067047119
Epoch 600, val loss: 0.7654332518577576
Epoch 610, training loss: 12.445354461669922 = 0.3362993896007538 + 2.0 * 6.054527759552002
Epoch 610, val loss: 0.7605959177017212
Epoch 620, training loss: 12.428618431091309 = 0.32181471586227417 + 2.0 * 6.053401947021484
Epoch 620, val loss: 0.7564026713371277
Epoch 630, training loss: 12.420981407165527 = 0.3079568147659302 + 2.0 * 6.056512355804443
Epoch 630, val loss: 0.7527275085449219
Epoch 640, training loss: 12.399532318115234 = 0.29478567838668823 + 2.0 * 6.05237340927124
Epoch 640, val loss: 0.7496406435966492
Epoch 650, training loss: 12.383275985717773 = 0.2822359502315521 + 2.0 * 6.050519943237305
Epoch 650, val loss: 0.7471455931663513
Epoch 660, training loss: 12.36703109741211 = 0.2701214849948883 + 2.0 * 6.048454761505127
Epoch 660, val loss: 0.7450172901153564
Epoch 670, training loss: 12.352387428283691 = 0.25837036967277527 + 2.0 * 6.047008514404297
Epoch 670, val loss: 0.7433843612670898
Epoch 680, training loss: 12.356532096862793 = 0.2469715178012848 + 2.0 * 6.05478048324585
Epoch 680, val loss: 0.7421915531158447
Epoch 690, training loss: 12.332448959350586 = 0.2360798418521881 + 2.0 * 6.048184394836426
Epoch 690, val loss: 0.7414472699165344
Epoch 700, training loss: 12.314319610595703 = 0.2254858762025833 + 2.0 * 6.044416904449463
Epoch 700, val loss: 0.7410926818847656
Epoch 710, training loss: 12.304300308227539 = 0.21518491208553314 + 2.0 * 6.044557571411133
Epoch 710, val loss: 0.7410647869110107
Epoch 720, training loss: 12.289957046508789 = 0.2052316665649414 + 2.0 * 6.042362689971924
Epoch 720, val loss: 0.7413102984428406
Epoch 730, training loss: 12.283073425292969 = 0.19561661779880524 + 2.0 * 6.043728351593018
Epoch 730, val loss: 0.7418796420097351
Epoch 740, training loss: 12.267242431640625 = 0.18637776374816895 + 2.0 * 6.040432453155518
Epoch 740, val loss: 0.7427474856376648
Epoch 750, training loss: 12.257192611694336 = 0.1775062531232834 + 2.0 * 6.0398430824279785
Epoch 750, val loss: 0.7439593076705933
Epoch 760, training loss: 12.257238388061523 = 0.16904211044311523 + 2.0 * 6.044097900390625
Epoch 760, val loss: 0.74539715051651
Epoch 770, training loss: 12.240631103515625 = 0.16109228134155273 + 2.0 * 6.039769649505615
Epoch 770, val loss: 0.747057318687439
Epoch 780, training loss: 12.228498458862305 = 0.1535341888666153 + 2.0 * 6.037482261657715
Epoch 780, val loss: 0.7490652203559875
Epoch 790, training loss: 12.222843170166016 = 0.14640608429908752 + 2.0 * 6.0382184982299805
Epoch 790, val loss: 0.7512759566307068
Epoch 800, training loss: 12.20900821685791 = 0.1396796554327011 + 2.0 * 6.034664154052734
Epoch 800, val loss: 0.7536782622337341
Epoch 810, training loss: 12.202962875366211 = 0.13333772122859955 + 2.0 * 6.0348124504089355
Epoch 810, val loss: 0.7563767433166504
Epoch 820, training loss: 12.200126647949219 = 0.1273515224456787 + 2.0 * 6.0363874435424805
Epoch 820, val loss: 0.7592575550079346
Epoch 830, training loss: 12.19123363494873 = 0.12179547548294067 + 2.0 * 6.034718990325928
Epoch 830, val loss: 0.7623281478881836
Epoch 840, training loss: 12.1812744140625 = 0.11656855791807175 + 2.0 * 6.032352924346924
Epoch 840, val loss: 0.7655872702598572
Epoch 850, training loss: 12.173391342163086 = 0.11165623366832733 + 2.0 * 6.030867576599121
Epoch 850, val loss: 0.7689835429191589
Epoch 860, training loss: 12.166444778442383 = 0.10701624304056168 + 2.0 * 6.029714107513428
Epoch 860, val loss: 0.7724941968917847
Epoch 870, training loss: 12.180335998535156 = 0.10263442248106003 + 2.0 * 6.038850784301758
Epoch 870, val loss: 0.7760821580886841
Epoch 880, training loss: 12.156311988830566 = 0.09853910654783249 + 2.0 * 6.028886318206787
Epoch 880, val loss: 0.7798076272010803
Epoch 890, training loss: 12.150654792785645 = 0.09467470645904541 + 2.0 * 6.027989864349365
Epoch 890, val loss: 0.7837695479393005
Epoch 900, training loss: 12.147016525268555 = 0.09101670235395432 + 2.0 * 6.0279998779296875
Epoch 900, val loss: 0.7877312898635864
Epoch 910, training loss: 12.143656730651855 = 0.08756262063980103 + 2.0 * 6.02804708480835
Epoch 910, val loss: 0.7916741967201233
Epoch 920, training loss: 12.135052680969238 = 0.08429689705371857 + 2.0 * 6.0253777503967285
Epoch 920, val loss: 0.7957471609115601
Epoch 930, training loss: 12.131752014160156 = 0.08119771629571915 + 2.0 * 6.025277137756348
Epoch 930, val loss: 0.7998977303504944
Epoch 940, training loss: 12.129542350769043 = 0.07825326174497604 + 2.0 * 6.025644779205322
Epoch 940, val loss: 0.8040606379508972
Epoch 950, training loss: 12.131534576416016 = 0.07546519488096237 + 2.0 * 6.028034687042236
Epoch 950, val loss: 0.808329701423645
Epoch 960, training loss: 12.11837100982666 = 0.07281969487667084 + 2.0 * 6.022775650024414
Epoch 960, val loss: 0.8125428557395935
Epoch 970, training loss: 12.114739418029785 = 0.07031266391277313 + 2.0 * 6.022213459014893
Epoch 970, val loss: 0.8169211745262146
Epoch 980, training loss: 12.109892845153809 = 0.06791061162948608 + 2.0 * 6.020991325378418
Epoch 980, val loss: 0.8213065266609192
Epoch 990, training loss: 12.106405258178711 = 0.0656094178557396 + 2.0 * 6.020398139953613
Epoch 990, val loss: 0.8257468342781067
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8000
Overall ASR: 0.6310
Flip ASR: 0.5556/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.700702667236328 = 1.953282356262207 + 2.0 * 8.373709678649902
Epoch 0, val loss: 1.9470456838607788
Epoch 10, training loss: 18.687721252441406 = 1.9423933029174805 + 2.0 * 8.372663497924805
Epoch 10, val loss: 1.9365781545639038
Epoch 20, training loss: 18.662246704101562 = 1.9291160106658936 + 2.0 * 8.366565704345703
Epoch 20, val loss: 1.9235464334487915
Epoch 30, training loss: 18.571239471435547 = 1.9118661880493164 + 2.0 * 8.329686164855957
Epoch 30, val loss: 1.9064429998397827
Epoch 40, training loss: 18.001602172851562 = 1.8929054737091064 + 2.0 * 8.05434799194336
Epoch 40, val loss: 1.8879892826080322
Epoch 50, training loss: 16.34749984741211 = 1.8731842041015625 + 2.0 * 7.237157344818115
Epoch 50, val loss: 1.868933081626892
Epoch 60, training loss: 15.6774263381958 = 1.8585994243621826 + 2.0 * 6.9094133377075195
Epoch 60, val loss: 1.8543875217437744
Epoch 70, training loss: 15.259838104248047 = 1.8465266227722168 + 2.0 * 6.706655502319336
Epoch 70, val loss: 1.8419326543807983
Epoch 80, training loss: 14.963499069213867 = 1.8366631269454956 + 2.0 * 6.563417911529541
Epoch 80, val loss: 1.8314759731292725
Epoch 90, training loss: 14.755691528320312 = 1.827430248260498 + 2.0 * 6.464130878448486
Epoch 90, val loss: 1.8218209743499756
Epoch 100, training loss: 14.602574348449707 = 1.8188456296920776 + 2.0 * 6.39186429977417
Epoch 100, val loss: 1.8128442764282227
Epoch 110, training loss: 14.49351692199707 = 1.8107376098632812 + 2.0 * 6.3413896560668945
Epoch 110, val loss: 1.8043298721313477
Epoch 120, training loss: 14.409080505371094 = 1.802863359451294 + 2.0 * 6.3031086921691895
Epoch 120, val loss: 1.7960268259048462
Epoch 130, training loss: 14.342336654663086 = 1.7948685884475708 + 2.0 * 6.273734092712402
Epoch 130, val loss: 1.78764808177948
Epoch 140, training loss: 14.287019729614258 = 1.7867716550827026 + 2.0 * 6.250123977661133
Epoch 140, val loss: 1.7792891263961792
Epoch 150, training loss: 14.23718547821045 = 1.7785240411758423 + 2.0 * 6.229330539703369
Epoch 150, val loss: 1.7709481716156006
Epoch 160, training loss: 14.19324779510498 = 1.7699174880981445 + 2.0 * 6.211665153503418
Epoch 160, val loss: 1.7624614238739014
Epoch 170, training loss: 14.15397834777832 = 1.7605057954788208 + 2.0 * 6.1967363357543945
Epoch 170, val loss: 1.7534844875335693
Epoch 180, training loss: 14.118460655212402 = 1.750025749206543 + 2.0 * 6.18421745300293
Epoch 180, val loss: 1.7437747716903687
Epoch 190, training loss: 14.083211898803711 = 1.738183617591858 + 2.0 * 6.172513961791992
Epoch 190, val loss: 1.7331684827804565
Epoch 200, training loss: 14.048431396484375 = 1.7246736288070679 + 2.0 * 6.161879062652588
Epoch 200, val loss: 1.721290111541748
Epoch 210, training loss: 14.014508247375488 = 1.7089499235153198 + 2.0 * 6.1527791023254395
Epoch 210, val loss: 1.7077850103378296
Epoch 220, training loss: 13.98249340057373 = 1.6906061172485352 + 2.0 * 6.145943641662598
Epoch 220, val loss: 1.6923620700836182
Epoch 230, training loss: 13.945433616638184 = 1.669411301612854 + 2.0 * 6.1380109786987305
Epoch 230, val loss: 1.6747617721557617
Epoch 240, training loss: 13.907251358032227 = 1.6446796655654907 + 2.0 * 6.131285667419434
Epoch 240, val loss: 1.6544520854949951
Epoch 250, training loss: 13.866394996643066 = 1.6156747341156006 + 2.0 * 6.125360012054443
Epoch 250, val loss: 1.6308188438415527
Epoch 260, training loss: 13.823915481567383 = 1.582054615020752 + 2.0 * 6.1209306716918945
Epoch 260, val loss: 1.6036521196365356
Epoch 270, training loss: 13.777750968933105 = 1.544391393661499 + 2.0 * 6.116679668426514
Epoch 270, val loss: 1.5734652280807495
Epoch 280, training loss: 13.725768089294434 = 1.5026532411575317 + 2.0 * 6.111557483673096
Epoch 280, val loss: 1.540009617805481
Epoch 290, training loss: 13.672026634216309 = 1.4570115804672241 + 2.0 * 6.107507705688477
Epoch 290, val loss: 1.50365149974823
Epoch 300, training loss: 13.626307487487793 = 1.4087997674942017 + 2.0 * 6.108753681182861
Epoch 300, val loss: 1.4655393362045288
Epoch 310, training loss: 13.564434051513672 = 1.3601813316345215 + 2.0 * 6.102126121520996
Epoch 310, val loss: 1.4276660680770874
Epoch 320, training loss: 13.507680892944336 = 1.3120206594467163 + 2.0 * 6.097830295562744
Epoch 320, val loss: 1.3904303312301636
Epoch 330, training loss: 13.46272087097168 = 1.265144944190979 + 2.0 * 6.098787784576416
Epoch 330, val loss: 1.3546024560928345
Epoch 340, training loss: 13.410735130310059 = 1.2212587594985962 + 2.0 * 6.094738006591797
Epoch 340, val loss: 1.3214653730392456
Epoch 350, training loss: 13.357455253601074 = 1.1797982454299927 + 2.0 * 6.0888285636901855
Epoch 350, val loss: 1.2905328273773193
Epoch 360, training loss: 13.311342239379883 = 1.1398335695266724 + 2.0 * 6.08575439453125
Epoch 360, val loss: 1.2608743906021118
Epoch 370, training loss: 13.268369674682617 = 1.1008706092834473 + 2.0 * 6.083749771118164
Epoch 370, val loss: 1.23210871219635
Epoch 380, training loss: 13.229368209838867 = 1.0629521608352661 + 2.0 * 6.083208084106445
Epoch 380, val loss: 1.2043951749801636
Epoch 390, training loss: 13.18354320526123 = 1.0259114503860474 + 2.0 * 6.078815937042236
Epoch 390, val loss: 1.1772669553756714
Epoch 400, training loss: 13.14412784576416 = 0.9893057942390442 + 2.0 * 6.07741117477417
Epoch 400, val loss: 1.150545597076416
Epoch 410, training loss: 13.104753494262695 = 0.9532151818275452 + 2.0 * 6.075768947601318
Epoch 410, val loss: 1.124169945716858
Epoch 420, training loss: 13.061071395874023 = 0.9177363514900208 + 2.0 * 6.071667671203613
Epoch 420, val loss: 1.0980865955352783
Epoch 430, training loss: 13.025252342224121 = 0.8826590776443481 + 2.0 * 6.071296691894531
Epoch 430, val loss: 1.072461485862732
Epoch 440, training loss: 12.986291885375977 = 0.8483161926269531 + 2.0 * 6.068987846374512
Epoch 440, val loss: 1.0472091436386108
Epoch 450, training loss: 12.946650505065918 = 0.8144517540931702 + 2.0 * 6.066099166870117
Epoch 450, val loss: 1.0224018096923828
Epoch 460, training loss: 12.921527862548828 = 0.7810527682304382 + 2.0 * 6.070237636566162
Epoch 460, val loss: 0.9978723526000977
Epoch 470, training loss: 12.874971389770508 = 0.7484371662139893 + 2.0 * 6.063267230987549
Epoch 470, val loss: 0.9741840362548828
Epoch 480, training loss: 12.840412139892578 = 0.7168022394180298 + 2.0 * 6.06180477142334
Epoch 480, val loss: 0.9512984752655029
Epoch 490, training loss: 12.805766105651855 = 0.6858569383621216 + 2.0 * 6.059954643249512
Epoch 490, val loss: 0.9289996027946472
Epoch 500, training loss: 12.773946762084961 = 0.6556494832038879 + 2.0 * 6.059148788452148
Epoch 500, val loss: 0.907416045665741
Epoch 510, training loss: 12.74459171295166 = 0.6264944076538086 + 2.0 * 6.059048652648926
Epoch 510, val loss: 0.8866057991981506
Epoch 520, training loss: 12.726428985595703 = 0.5986857414245605 + 2.0 * 6.06387186050415
Epoch 520, val loss: 0.867054283618927
Epoch 530, training loss: 12.682735443115234 = 0.5724460482597351 + 2.0 * 6.055144786834717
Epoch 530, val loss: 0.8486813306808472
Epoch 540, training loss: 12.654830932617188 = 0.5474510192871094 + 2.0 * 6.053689956665039
Epoch 540, val loss: 0.8314744830131531
Epoch 550, training loss: 12.627307891845703 = 0.5236450433731079 + 2.0 * 6.051831245422363
Epoch 550, val loss: 0.8153788447380066
Epoch 560, training loss: 12.610158920288086 = 0.5010025501251221 + 2.0 * 6.0545783042907715
Epoch 560, val loss: 0.8003671765327454
Epoch 570, training loss: 12.585684776306152 = 0.4796915650367737 + 2.0 * 6.052996635437012
Epoch 570, val loss: 0.7867065668106079
Epoch 580, training loss: 12.557618141174316 = 0.4597894847393036 + 2.0 * 6.048914432525635
Epoch 580, val loss: 0.7743824124336243
Epoch 590, training loss: 12.536003112792969 = 0.4408319294452667 + 2.0 * 6.047585487365723
Epoch 590, val loss: 0.76308673620224
Epoch 600, training loss: 12.514671325683594 = 0.4226844310760498 + 2.0 * 6.045993328094482
Epoch 600, val loss: 0.7527024149894714
Epoch 610, training loss: 12.509505271911621 = 0.4052753150463104 + 2.0 * 6.052114963531494
Epoch 610, val loss: 0.7432228326797485
Epoch 620, training loss: 12.481553077697754 = 0.38884854316711426 + 2.0 * 6.046352386474609
Epoch 620, val loss: 0.7345721125602722
Epoch 630, training loss: 12.462139129638672 = 0.3732099235057831 + 2.0 * 6.044464588165283
Epoch 630, val loss: 0.7269785404205322
Epoch 640, training loss: 12.44202995300293 = 0.3581313490867615 + 2.0 * 6.041949272155762
Epoch 640, val loss: 0.7199856638908386
Epoch 650, training loss: 12.43035888671875 = 0.3435523211956024 + 2.0 * 6.043403148651123
Epoch 650, val loss: 0.7135916352272034
Epoch 660, training loss: 12.420307159423828 = 0.3295438885688782 + 2.0 * 6.045381546020508
Epoch 660, val loss: 0.7078317999839783
Epoch 670, training loss: 12.39597225189209 = 0.3162078559398651 + 2.0 * 6.039882183074951
Epoch 670, val loss: 0.7028045058250427
Epoch 680, training loss: 12.378788948059082 = 0.30330222845077515 + 2.0 * 6.03774356842041
Epoch 680, val loss: 0.6983634829521179
Epoch 690, training loss: 12.364130973815918 = 0.2907889187335968 + 2.0 * 6.036671161651611
Epoch 690, val loss: 0.6944235563278198
Epoch 700, training loss: 12.369451522827148 = 0.2786816656589508 + 2.0 * 6.045384883880615
Epoch 700, val loss: 0.6910474300384521
Epoch 710, training loss: 12.343775749206543 = 0.2671853005886078 + 2.0 * 6.038295269012451
Epoch 710, val loss: 0.6881360411643982
Epoch 720, training loss: 12.326923370361328 = 0.25621458888053894 + 2.0 * 6.0353546142578125
Epoch 720, val loss: 0.6858553290367126
Epoch 730, training loss: 12.312849998474121 = 0.24565370380878448 + 2.0 * 6.033597946166992
Epoch 730, val loss: 0.6841074824333191
Epoch 740, training loss: 12.301350593566895 = 0.23551158607006073 + 2.0 * 6.032919406890869
Epoch 740, val loss: 0.6827182769775391
Epoch 750, training loss: 12.293407440185547 = 0.2258141040802002 + 2.0 * 6.033796787261963
Epoch 750, val loss: 0.6817358136177063
Epoch 760, training loss: 12.27985668182373 = 0.21657943725585938 + 2.0 * 6.0316386222839355
Epoch 760, val loss: 0.6811977028846741
Epoch 770, training loss: 12.27705192565918 = 0.20775431394577026 + 2.0 * 6.034648895263672
Epoch 770, val loss: 0.6810317635536194
Epoch 780, training loss: 12.260649681091309 = 0.19941933453083038 + 2.0 * 6.030615329742432
Epoch 780, val loss: 0.6811794638633728
Epoch 790, training loss: 12.250455856323242 = 0.1914471834897995 + 2.0 * 6.029504299163818
Epoch 790, val loss: 0.6816735863685608
Epoch 800, training loss: 12.241555213928223 = 0.18389253318309784 + 2.0 * 6.028831481933594
Epoch 800, val loss: 0.6823846101760864
Epoch 810, training loss: 12.233705520629883 = 0.17673765122890472 + 2.0 * 6.028483867645264
Epoch 810, val loss: 0.6834054589271545
Epoch 820, training loss: 12.223431587219238 = 0.1698915958404541 + 2.0 * 6.026770114898682
Epoch 820, val loss: 0.684748113155365
Epoch 830, training loss: 12.226581573486328 = 0.16334617137908936 + 2.0 * 6.031617641448975
Epoch 830, val loss: 0.6861966848373413
Epoch 840, training loss: 12.212761878967285 = 0.15715539455413818 + 2.0 * 6.027803421020508
Epoch 840, val loss: 0.6878452897071838
Epoch 850, training loss: 12.199638366699219 = 0.15124040842056274 + 2.0 * 6.02419900894165
Epoch 850, val loss: 0.6897376775741577
Epoch 860, training loss: 12.194917678833008 = 0.14558212459087372 + 2.0 * 6.024667739868164
Epoch 860, val loss: 0.69173663854599
Epoch 870, training loss: 12.186957359313965 = 0.14019082486629486 + 2.0 * 6.023383140563965
Epoch 870, val loss: 0.6938480138778687
Epoch 880, training loss: 12.18206787109375 = 0.13506321609020233 + 2.0 * 6.023502349853516
Epoch 880, val loss: 0.6961438059806824
Epoch 890, training loss: 12.178337097167969 = 0.13014529645442963 + 2.0 * 6.0240960121154785
Epoch 890, val loss: 0.6986349821090698
Epoch 900, training loss: 12.16746997833252 = 0.1254684180021286 + 2.0 * 6.021000862121582
Epoch 900, val loss: 0.7011298537254333
Epoch 910, training loss: 12.165377616882324 = 0.1209903210401535 + 2.0 * 6.022193431854248
Epoch 910, val loss: 0.7037965655326843
Epoch 920, training loss: 12.160957336425781 = 0.11672870069742203 + 2.0 * 6.022114276885986
Epoch 920, val loss: 0.706475019454956
Epoch 930, training loss: 12.153280258178711 = 0.11266189813613892 + 2.0 * 6.020308971405029
Epoch 930, val loss: 0.7093230485916138
Epoch 940, training loss: 12.148050308227539 = 0.10875948518514633 + 2.0 * 6.0196452140808105
Epoch 940, val loss: 0.7122892141342163
Epoch 950, training loss: 12.145041465759277 = 0.1050269827246666 + 2.0 * 6.020007133483887
Epoch 950, val loss: 0.7152281403541565
Epoch 960, training loss: 12.141274452209473 = 0.10145062208175659 + 2.0 * 6.019911766052246
Epoch 960, val loss: 0.71827632188797
Epoch 970, training loss: 12.13627815246582 = 0.0980384349822998 + 2.0 * 6.019119739532471
Epoch 970, val loss: 0.7213314771652222
Epoch 980, training loss: 12.129295349121094 = 0.0947774276137352 + 2.0 * 6.017259120941162
Epoch 980, val loss: 0.7244967222213745
Epoch 990, training loss: 12.122894287109375 = 0.09163977950811386 + 2.0 * 6.015627384185791
Epoch 990, val loss: 0.7277497053146362
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.5055
Flip ASR: 0.4533/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.685014724731445 = 1.9373873472213745 + 2.0 * 8.37381362915039
Epoch 0, val loss: 1.9342280626296997
Epoch 10, training loss: 18.674392700195312 = 1.9279404878616333 + 2.0 * 8.373226165771484
Epoch 10, val loss: 1.9248394966125488
Epoch 20, training loss: 18.656169891357422 = 1.9161484241485596 + 2.0 * 8.370010375976562
Epoch 20, val loss: 1.9126203060150146
Epoch 30, training loss: 18.599464416503906 = 1.9000844955444336 + 2.0 * 8.349690437316895
Epoch 30, val loss: 1.8956260681152344
Epoch 40, training loss: 18.26601791381836 = 1.880629539489746 + 2.0 * 8.192694664001465
Epoch 40, val loss: 1.8756353855133057
Epoch 50, training loss: 16.308958053588867 = 1.860640287399292 + 2.0 * 7.224159240722656
Epoch 50, val loss: 1.855170488357544
Epoch 60, training loss: 15.662995338439941 = 1.8452517986297607 + 2.0 * 6.908871650695801
Epoch 60, val loss: 1.8395682573318481
Epoch 70, training loss: 15.270920753479004 = 1.832464337348938 + 2.0 * 6.719228267669678
Epoch 70, val loss: 1.825775384902954
Epoch 80, training loss: 15.023082733154297 = 1.8196568489074707 + 2.0 * 6.601712703704834
Epoch 80, val loss: 1.8118611574172974
Epoch 90, training loss: 14.843063354492188 = 1.8078157901763916 + 2.0 * 6.5176239013671875
Epoch 90, val loss: 1.7992002964019775
Epoch 100, training loss: 14.702184677124023 = 1.7970627546310425 + 2.0 * 6.452560901641846
Epoch 100, val loss: 1.7879787683486938
Epoch 110, training loss: 14.589961051940918 = 1.7873576879501343 + 2.0 * 6.401301860809326
Epoch 110, val loss: 1.7777912616729736
Epoch 120, training loss: 14.494561195373535 = 1.7776957750320435 + 2.0 * 6.358432769775391
Epoch 120, val loss: 1.7676159143447876
Epoch 130, training loss: 14.412137985229492 = 1.7676234245300293 + 2.0 * 6.3222575187683105
Epoch 130, val loss: 1.7572429180145264
Epoch 140, training loss: 14.34342098236084 = 1.7569993734359741 + 2.0 * 6.293210983276367
Epoch 140, val loss: 1.7467331886291504
Epoch 150, training loss: 14.283616065979004 = 1.7455044984817505 + 2.0 * 6.2690558433532715
Epoch 150, val loss: 1.7358006238937378
Epoch 160, training loss: 14.22579288482666 = 1.7329113483428955 + 2.0 * 6.246440887451172
Epoch 160, val loss: 1.724569320678711
Epoch 170, training loss: 14.173646926879883 = 1.719081163406372 + 2.0 * 6.227283000946045
Epoch 170, val loss: 1.7126967906951904
Epoch 180, training loss: 14.123514175415039 = 1.7035900354385376 + 2.0 * 6.209961891174316
Epoch 180, val loss: 1.6997698545455933
Epoch 190, training loss: 14.074772834777832 = 1.6856781244277954 + 2.0 * 6.194547176361084
Epoch 190, val loss: 1.6852649450302124
Epoch 200, training loss: 14.028181076049805 = 1.6646806001663208 + 2.0 * 6.181750297546387
Epoch 200, val loss: 1.668603539466858
Epoch 210, training loss: 13.980701446533203 = 1.6402302980422974 + 2.0 * 6.170235633850098
Epoch 210, val loss: 1.6494159698486328
Epoch 220, training loss: 13.932395935058594 = 1.6117771863937378 + 2.0 * 6.160309314727783
Epoch 220, val loss: 1.6272821426391602
Epoch 230, training loss: 13.884624481201172 = 1.5784404277801514 + 2.0 * 6.153091907501221
Epoch 230, val loss: 1.6014145612716675
Epoch 240, training loss: 13.827925682067871 = 1.5404396057128906 + 2.0 * 6.14374303817749
Epoch 240, val loss: 1.5717934370040894
Epoch 250, training loss: 13.772217750549316 = 1.497481346130371 + 2.0 * 6.137368202209473
Epoch 250, val loss: 1.5383918285369873
Epoch 260, training loss: 13.715616226196289 = 1.4498330354690552 + 2.0 * 6.132891654968262
Epoch 260, val loss: 1.5015523433685303
Epoch 270, training loss: 13.6538667678833 = 1.399234414100647 + 2.0 * 6.127315998077393
Epoch 270, val loss: 1.4629024267196655
Epoch 280, training loss: 13.591899871826172 = 1.346943974494934 + 2.0 * 6.122478008270264
Epoch 280, val loss: 1.4233626127243042
Epoch 290, training loss: 13.538859367370605 = 1.2941755056381226 + 2.0 * 6.122342109680176
Epoch 290, val loss: 1.384074091911316
Epoch 300, training loss: 13.474319458007812 = 1.2429684400558472 + 2.0 * 6.115675449371338
Epoch 300, val loss: 1.3463819026947021
Epoch 310, training loss: 13.415083885192871 = 1.1929903030395508 + 2.0 * 6.11104679107666
Epoch 310, val loss: 1.3100330829620361
Epoch 320, training loss: 13.362950325012207 = 1.1437900066375732 + 2.0 * 6.109580039978027
Epoch 320, val loss: 1.2746607065200806
Epoch 330, training loss: 13.309179306030273 = 1.0958051681518555 + 2.0 * 6.106687068939209
Epoch 330, val loss: 1.2402336597442627
Epoch 340, training loss: 13.251433372497559 = 1.0487823486328125 + 2.0 * 6.101325511932373
Epoch 340, val loss: 1.2065908908843994
Epoch 350, training loss: 13.198465347290039 = 1.0018527507781982 + 2.0 * 6.098306179046631
Epoch 350, val loss: 1.1728445291519165
Epoch 360, training loss: 13.145415306091309 = 0.9549604058265686 + 2.0 * 6.095227241516113
Epoch 360, val loss: 1.1390299797058105
Epoch 370, training loss: 13.094547271728516 = 0.9086930155754089 + 2.0 * 6.092926979064941
Epoch 370, val loss: 1.1056071519851685
Epoch 380, training loss: 13.046012878417969 = 0.8640692234039307 + 2.0 * 6.090971946716309
Epoch 380, val loss: 1.0732903480529785
Epoch 390, training loss: 12.998298645019531 = 0.8217427730560303 + 2.0 * 6.088277816772461
Epoch 390, val loss: 1.0423952341079712
Epoch 400, training loss: 12.952690124511719 = 0.7820679545402527 + 2.0 * 6.085310935974121
Epoch 400, val loss: 1.0136457681655884
Epoch 410, training loss: 12.910648345947266 = 0.7453068494796753 + 2.0 * 6.08267068862915
Epoch 410, val loss: 0.9870546460151672
Epoch 420, training loss: 12.877660751342773 = 0.7117575407028198 + 2.0 * 6.082951545715332
Epoch 420, val loss: 0.9629601240158081
Epoch 430, training loss: 12.842511177062988 = 0.6815853118896484 + 2.0 * 6.08046293258667
Epoch 430, val loss: 0.9418719410896301
Epoch 440, training loss: 12.805587768554688 = 0.6540033221244812 + 2.0 * 6.07579231262207
Epoch 440, val loss: 0.9230745434761047
Epoch 450, training loss: 12.776955604553223 = 0.6283026337623596 + 2.0 * 6.074326515197754
Epoch 450, val loss: 0.9060251712799072
Epoch 460, training loss: 12.748983383178711 = 0.6042168140411377 + 2.0 * 6.072383403778076
Epoch 460, val loss: 0.8906390070915222
Epoch 470, training loss: 12.721030235290527 = 0.5814821124076843 + 2.0 * 6.069774150848389
Epoch 470, val loss: 0.8765617609024048
Epoch 480, training loss: 12.693899154663086 = 0.5596293807029724 + 2.0 * 6.067134857177734
Epoch 480, val loss: 0.8635597825050354
Epoch 490, training loss: 12.678532600402832 = 0.5385066866874695 + 2.0 * 6.070013046264648
Epoch 490, val loss: 0.8514981865882874
Epoch 500, training loss: 12.646066665649414 = 0.5183308124542236 + 2.0 * 6.063868045806885
Epoch 500, val loss: 0.8403424620628357
Epoch 510, training loss: 12.622802734375 = 0.49869847297668457 + 2.0 * 6.062052249908447
Epoch 510, val loss: 0.8301527500152588
Epoch 520, training loss: 12.61650276184082 = 0.4794572591781616 + 2.0 * 6.068522930145264
Epoch 520, val loss: 0.8206570148468018
Epoch 530, training loss: 12.578908920288086 = 0.4608965218067169 + 2.0 * 6.059006214141846
Epoch 530, val loss: 0.8118071556091309
Epoch 540, training loss: 12.557387351989746 = 0.4427759349346161 + 2.0 * 6.057305812835693
Epoch 540, val loss: 0.8038443922996521
Epoch 550, training loss: 12.537932395935059 = 0.42498043179512024 + 2.0 * 6.05647611618042
Epoch 550, val loss: 0.7964918613433838
Epoch 560, training loss: 12.520566940307617 = 0.40756529569625854 + 2.0 * 6.0565009117126465
Epoch 560, val loss: 0.7897246479988098
Epoch 570, training loss: 12.499972343444824 = 0.3906707167625427 + 2.0 * 6.054650783538818
Epoch 570, val loss: 0.7836771011352539
Epoch 580, training loss: 12.47903060913086 = 0.37412673234939575 + 2.0 * 6.052452087402344
Epoch 580, val loss: 0.7782694101333618
Epoch 590, training loss: 12.461122512817383 = 0.35789692401885986 + 2.0 * 6.051612854003906
Epoch 590, val loss: 0.7734245657920837
Epoch 600, training loss: 12.451024055480957 = 0.3421567976474762 + 2.0 * 6.054433822631836
Epoch 600, val loss: 0.7690258622169495
Epoch 610, training loss: 12.425982475280762 = 0.3269854485988617 + 2.0 * 6.049498558044434
Epoch 610, val loss: 0.7653847932815552
Epoch 620, training loss: 12.407684326171875 = 0.3122992217540741 + 2.0 * 6.047692775726318
Epoch 620, val loss: 0.7623854279518127
Epoch 630, training loss: 12.391423225402832 = 0.29802456498146057 + 2.0 * 6.046699523925781
Epoch 630, val loss: 0.759936511516571
Epoch 640, training loss: 12.384805679321289 = 0.284258097410202 + 2.0 * 6.050273895263672
Epoch 640, val loss: 0.7580766677856445
Epoch 650, training loss: 12.360124588012695 = 0.2710557281970978 + 2.0 * 6.044534206390381
Epoch 650, val loss: 0.7568863034248352
Epoch 660, training loss: 12.345918655395508 = 0.2584136426448822 + 2.0 * 6.043752670288086
Epoch 660, val loss: 0.7563523650169373
Epoch 670, training loss: 12.333073616027832 = 0.24631956219673157 + 2.0 * 6.043376922607422
Epoch 670, val loss: 0.7563514709472656
Epoch 680, training loss: 12.323747634887695 = 0.23485395312309265 + 2.0 * 6.04444694519043
Epoch 680, val loss: 0.7567063570022583
Epoch 690, training loss: 12.306955337524414 = 0.22413219511508942 + 2.0 * 6.041411399841309
Epoch 690, val loss: 0.7578015923500061
Epoch 700, training loss: 12.293509483337402 = 0.21396702527999878 + 2.0 * 6.03977108001709
Epoch 700, val loss: 0.7594490051269531
Epoch 710, training loss: 12.28152847290039 = 0.2042893022298813 + 2.0 * 6.038619518280029
Epoch 710, val loss: 0.7614871263504028
Epoch 720, training loss: 12.278884887695312 = 0.1950976848602295 + 2.0 * 6.041893482208252
Epoch 720, val loss: 0.7640339732170105
Epoch 730, training loss: 12.268101692199707 = 0.18642593920230865 + 2.0 * 6.04083776473999
Epoch 730, val loss: 0.7667608857154846
Epoch 740, training loss: 12.253543853759766 = 0.1782705932855606 + 2.0 * 6.037636756896973
Epoch 740, val loss: 0.7700135707855225
Epoch 750, training loss: 12.246098518371582 = 0.17052499949932098 + 2.0 * 6.037786960601807
Epoch 750, val loss: 0.7735576033592224
Epoch 760, training loss: 12.233509063720703 = 0.16320401430130005 + 2.0 * 6.035152435302734
Epoch 760, val loss: 0.7773336172103882
Epoch 770, training loss: 12.23204517364502 = 0.15628360211849213 + 2.0 * 6.037880897521973
Epoch 770, val loss: 0.7813959121704102
Epoch 780, training loss: 12.219076156616211 = 0.14976418018341064 + 2.0 * 6.034656047821045
Epoch 780, val loss: 0.7856646776199341
Epoch 790, training loss: 12.212300300598145 = 0.14356525242328644 + 2.0 * 6.034367561340332
Epoch 790, val loss: 0.7901347279548645
Epoch 800, training loss: 12.205172538757324 = 0.13768824934959412 + 2.0 * 6.0337419509887695
Epoch 800, val loss: 0.7947802543640137
Epoch 810, training loss: 12.19575023651123 = 0.13209740817546844 + 2.0 * 6.031826496124268
Epoch 810, val loss: 0.7996313571929932
Epoch 820, training loss: 12.189348220825195 = 0.12678763270378113 + 2.0 * 6.031280517578125
Epoch 820, val loss: 0.8046131134033203
Epoch 830, training loss: 12.183019638061523 = 0.12171962857246399 + 2.0 * 6.0306501388549805
Epoch 830, val loss: 0.8097051978111267
Epoch 840, training loss: 12.179047584533691 = 0.1169089525938034 + 2.0 * 6.031069278717041
Epoch 840, val loss: 0.8148196339607239
Epoch 850, training loss: 12.174628257751465 = 0.112363301217556 + 2.0 * 6.031132698059082
Epoch 850, val loss: 0.8200574517250061
Epoch 860, training loss: 12.165359497070312 = 0.10804800689220428 + 2.0 * 6.028655529022217
Epoch 860, val loss: 0.8254131078720093
Epoch 870, training loss: 12.15814208984375 = 0.10392384231090546 + 2.0 * 6.027109146118164
Epoch 870, val loss: 0.8308380842208862
Epoch 880, training loss: 12.170746803283691 = 0.09998764842748642 + 2.0 * 6.035379409790039
Epoch 880, val loss: 0.8362410664558411
Epoch 890, training loss: 12.147441864013672 = 0.09628110378980637 + 2.0 * 6.025580406188965
Epoch 890, val loss: 0.8416557312011719
Epoch 900, training loss: 12.144037246704102 = 0.09274499118328094 + 2.0 * 6.025646209716797
Epoch 900, val loss: 0.8472188711166382
Epoch 910, training loss: 12.138185501098633 = 0.08934998512268066 + 2.0 * 6.024417877197266
Epoch 910, val loss: 0.8527878522872925
Epoch 920, training loss: 12.147937774658203 = 0.08609927445650101 + 2.0 * 6.030919075012207
Epoch 920, val loss: 0.8583208322525024
Epoch 930, training loss: 12.140785217285156 = 0.08302628993988037 + 2.0 * 6.028879642486572
Epoch 930, val loss: 0.8639471530914307
Epoch 940, training loss: 12.125371932983398 = 0.08009067922830582 + 2.0 * 6.022640705108643
Epoch 940, val loss: 0.869627058506012
Epoch 950, training loss: 12.122203826904297 = 0.0772804245352745 + 2.0 * 6.022461891174316
Epoch 950, val loss: 0.8752959370613098
Epoch 960, training loss: 12.1238431930542 = 0.07458535581827164 + 2.0 * 6.02462911605835
Epoch 960, val loss: 0.8810068964958191
Epoch 970, training loss: 12.116226196289062 = 0.07202146202325821 + 2.0 * 6.022102355957031
Epoch 970, val loss: 0.8866642713546753
Epoch 980, training loss: 12.113212585449219 = 0.06957068294286728 + 2.0 * 6.021821022033691
Epoch 980, val loss: 0.8924745917320251
Epoch 990, training loss: 12.113656044006348 = 0.0672287568449974 + 2.0 * 6.023213863372803
Epoch 990, val loss: 0.8982037305831909
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.4834
Flip ASR: 0.4667/225 nodes
The final ASR:0.53998, 0.06499, Accuracy:0.81605, 0.01145
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11558])
remove edge: torch.Size([2, 9532])
updated graph: torch.Size([2, 10534])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9705
Flip ASR: 0.9644/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8481
Overall ASR: 0.9668
Flip ASR: 0.9600/225 nodes
The final ASR:0.97048, 0.00301, Accuracy:0.83704, 0.01090
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.5 der2:0.2 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 18.694461822509766 = 1.9466849565505981 + 2.0 * 8.37388801574707
Epoch 0, val loss: 1.9523857831954956
Epoch 10, training loss: 18.683591842651367 = 1.936516284942627 + 2.0 * 8.37353801727295
Epoch 10, val loss: 1.9422334432601929
Epoch 20, training loss: 18.667091369628906 = 1.924057960510254 + 2.0 * 8.371517181396484
Epoch 20, val loss: 1.929829478263855
Epoch 30, training loss: 18.620431900024414 = 1.9070713520050049 + 2.0 * 8.356679916381836
Epoch 30, val loss: 1.9131022691726685
Epoch 40, training loss: 18.35186004638672 = 1.8851224184036255 + 2.0 * 8.233368873596191
Epoch 40, val loss: 1.8917605876922607
Epoch 50, training loss: 16.892127990722656 = 1.8613654375076294 + 2.0 * 7.515380859375
Epoch 50, val loss: 1.8688260316848755
Epoch 60, training loss: 16.004404067993164 = 1.8464795351028442 + 2.0 * 7.078962326049805
Epoch 60, val loss: 1.8552688360214233
Epoch 70, training loss: 15.481948852539062 = 1.8375563621520996 + 2.0 * 6.822196006774902
Epoch 70, val loss: 1.8467650413513184
Epoch 80, training loss: 15.216066360473633 = 1.8271359205245972 + 2.0 * 6.694465160369873
Epoch 80, val loss: 1.8362826108932495
Epoch 90, training loss: 14.990020751953125 = 1.816588044166565 + 2.0 * 6.586716175079346
Epoch 90, val loss: 1.8255404233932495
Epoch 100, training loss: 14.814229965209961 = 1.8073513507843018 + 2.0 * 6.503439426422119
Epoch 100, val loss: 1.8160966634750366
Epoch 110, training loss: 14.672952651977539 = 1.7995588779449463 + 2.0 * 6.436697006225586
Epoch 110, val loss: 1.8078402280807495
Epoch 120, training loss: 14.555563926696777 = 1.7923004627227783 + 2.0 * 6.381631851196289
Epoch 120, val loss: 1.800110936164856
Epoch 130, training loss: 14.472945213317871 = 1.784743309020996 + 2.0 * 6.3441009521484375
Epoch 130, val loss: 1.792314887046814
Epoch 140, training loss: 14.404881477355957 = 1.7766189575195312 + 2.0 * 6.314131259918213
Epoch 140, val loss: 1.7843378782272339
Epoch 150, training loss: 14.344642639160156 = 1.7681785821914673 + 2.0 * 6.28823184967041
Epoch 150, val loss: 1.776321291923523
Epoch 160, training loss: 14.292683601379395 = 1.7593142986297607 + 2.0 * 6.266684532165527
Epoch 160, val loss: 1.7681396007537842
Epoch 170, training loss: 14.247238159179688 = 1.7496650218963623 + 2.0 * 6.248786449432373
Epoch 170, val loss: 1.7595969438552856
Epoch 180, training loss: 14.204096794128418 = 1.7388970851898193 + 2.0 * 6.23259973526001
Epoch 180, val loss: 1.7503091096878052
Epoch 190, training loss: 14.162507057189941 = 1.7265840768814087 + 2.0 * 6.217961311340332
Epoch 190, val loss: 1.7399636507034302
Epoch 200, training loss: 14.12351131439209 = 1.7123119831085205 + 2.0 * 6.205599784851074
Epoch 200, val loss: 1.7281835079193115
Epoch 210, training loss: 14.086685180664062 = 1.6956934928894043 + 2.0 * 6.195496082305908
Epoch 210, val loss: 1.714557409286499
Epoch 220, training loss: 14.04500675201416 = 1.6763850450515747 + 2.0 * 6.1843109130859375
Epoch 220, val loss: 1.698831558227539
Epoch 230, training loss: 14.009584426879883 = 1.6537680625915527 + 2.0 * 6.177907943725586
Epoch 230, val loss: 1.6804383993148804
Epoch 240, training loss: 13.961858749389648 = 1.627518653869629 + 2.0 * 6.16717004776001
Epoch 240, val loss: 1.6590853929519653
Epoch 250, training loss: 13.916070938110352 = 1.5970966815948486 + 2.0 * 6.159487247467041
Epoch 250, val loss: 1.6342992782592773
Epoch 260, training loss: 13.868568420410156 = 1.5625046491622925 + 2.0 * 6.153031826019287
Epoch 260, val loss: 1.6060036420822144
Epoch 270, training loss: 13.816994667053223 = 1.5242164134979248 + 2.0 * 6.146389007568359
Epoch 270, val loss: 1.574692964553833
Epoch 280, training loss: 13.764060020446777 = 1.4823620319366455 + 2.0 * 6.1408491134643555
Epoch 280, val loss: 1.5405068397521973
Epoch 290, training loss: 13.713056564331055 = 1.4380857944488525 + 2.0 * 6.137485504150391
Epoch 290, val loss: 1.5044358968734741
Epoch 300, training loss: 13.656349182128906 = 1.393038272857666 + 2.0 * 6.131655216217041
Epoch 300, val loss: 1.4680081605911255
Epoch 310, training loss: 13.600320816040039 = 1.347944736480713 + 2.0 * 6.126188278198242
Epoch 310, val loss: 1.4316003322601318
Epoch 320, training loss: 13.547652244567871 = 1.303378939628601 + 2.0 * 6.12213659286499
Epoch 320, val loss: 1.395627737045288
Epoch 330, training loss: 13.505637168884277 = 1.2602894306182861 + 2.0 * 6.122673988342285
Epoch 330, val loss: 1.3614554405212402
Epoch 340, training loss: 13.448259353637695 = 1.2193872928619385 + 2.0 * 6.114436149597168
Epoch 340, val loss: 1.3293919563293457
Epoch 350, training loss: 13.400941848754883 = 1.180009126663208 + 2.0 * 6.110466480255127
Epoch 350, val loss: 1.298764705657959
Epoch 360, training loss: 13.354583740234375 = 1.141649842262268 + 2.0 * 6.106466770172119
Epoch 360, val loss: 1.2693723440170288
Epoch 370, training loss: 13.312654495239258 = 1.1041673421859741 + 2.0 * 6.104243755340576
Epoch 370, val loss: 1.2408782243728638
Epoch 380, training loss: 13.268095016479492 = 1.0677719116210938 + 2.0 * 6.100161552429199
Epoch 380, val loss: 1.213566780090332
Epoch 390, training loss: 13.226210594177246 = 1.0319420099258423 + 2.0 * 6.097134113311768
Epoch 390, val loss: 1.1868008375167847
Epoch 400, training loss: 13.19305419921875 = 0.9964551329612732 + 2.0 * 6.098299503326416
Epoch 400, val loss: 1.1604727506637573
Epoch 410, training loss: 13.14608097076416 = 0.9617825150489807 + 2.0 * 6.092149257659912
Epoch 410, val loss: 1.134877324104309
Epoch 420, training loss: 13.107307434082031 = 0.9277614951133728 + 2.0 * 6.089773178100586
Epoch 420, val loss: 1.1097663640975952
Epoch 430, training loss: 13.075233459472656 = 0.8943202495574951 + 2.0 * 6.090456485748291
Epoch 430, val loss: 1.085084080696106
Epoch 440, training loss: 13.034553527832031 = 0.8617852926254272 + 2.0 * 6.086384296417236
Epoch 440, val loss: 1.0613826513290405
Epoch 450, training loss: 12.996330261230469 = 0.8298981785774231 + 2.0 * 6.083216190338135
Epoch 450, val loss: 1.038059115409851
Epoch 460, training loss: 12.960529327392578 = 0.7984872460365295 + 2.0 * 6.081020832061768
Epoch 460, val loss: 1.0151336193084717
Epoch 470, training loss: 12.940601348876953 = 0.7677013874053955 + 2.0 * 6.086450099945068
Epoch 470, val loss: 0.992835283279419
Epoch 480, training loss: 12.893848419189453 = 0.7377284169197083 + 2.0 * 6.078060150146484
Epoch 480, val loss: 0.9714438915252686
Epoch 490, training loss: 12.860363006591797 = 0.7085375785827637 + 2.0 * 6.075912952423096
Epoch 490, val loss: 0.9507684707641602
Epoch 500, training loss: 12.833919525146484 = 0.6801259517669678 + 2.0 * 6.076896667480469
Epoch 500, val loss: 0.9309550523757935
Epoch 510, training loss: 12.799306869506836 = 0.652691662311554 + 2.0 * 6.073307514190674
Epoch 510, val loss: 0.9123135805130005
Epoch 520, training loss: 12.7683687210083 = 0.625952959060669 + 2.0 * 6.0712080001831055
Epoch 520, val loss: 0.8945994973182678
Epoch 530, training loss: 12.744115829467773 = 0.5998358726501465 + 2.0 * 6.072140216827393
Epoch 530, val loss: 0.8776893615722656
Epoch 540, training loss: 12.715351104736328 = 0.5745031833648682 + 2.0 * 6.0704240798950195
Epoch 540, val loss: 0.8617761135101318
Epoch 550, training loss: 12.68517780303955 = 0.5499356389045715 + 2.0 * 6.067621231079102
Epoch 550, val loss: 0.8467589616775513
Epoch 560, training loss: 12.6674165725708 = 0.5260274410247803 + 2.0 * 6.070694446563721
Epoch 560, val loss: 0.832551121711731
Epoch 570, training loss: 12.63450813293457 = 0.5030159950256348 + 2.0 * 6.065746307373047
Epoch 570, val loss: 0.8193894624710083
Epoch 580, training loss: 12.609333038330078 = 0.4807843267917633 + 2.0 * 6.064274311065674
Epoch 580, val loss: 0.8071075081825256
Epoch 590, training loss: 12.582531929016113 = 0.4592251479625702 + 2.0 * 6.0616536140441895
Epoch 590, val loss: 0.7956681847572327
Epoch 600, training loss: 12.56653881072998 = 0.43830859661102295 + 2.0 * 6.064115047454834
Epoch 600, val loss: 0.7850976586341858
Epoch 610, training loss: 12.547527313232422 = 0.4183185398578644 + 2.0 * 6.06460428237915
Epoch 610, val loss: 0.7754013538360596
Epoch 620, training loss: 12.518322944641113 = 0.39913928508758545 + 2.0 * 6.059591770172119
Epoch 620, val loss: 0.7667368054389954
Epoch 630, training loss: 12.495227813720703 = 0.3806176781654358 + 2.0 * 6.057304859161377
Epoch 630, val loss: 0.7588010430335999
Epoch 640, training loss: 12.476739883422852 = 0.3626886010169983 + 2.0 * 6.05702543258667
Epoch 640, val loss: 0.7516674399375916
Epoch 650, training loss: 12.456355094909668 = 0.3454187214374542 + 2.0 * 6.0554680824279785
Epoch 650, val loss: 0.7452459335327148
Epoch 660, training loss: 12.442862510681152 = 0.32889848947525024 + 2.0 * 6.056982040405273
Epoch 660, val loss: 0.7396311163902283
Epoch 670, training loss: 12.418659210205078 = 0.31300657987594604 + 2.0 * 6.052826404571533
Epoch 670, val loss: 0.7347317337989807
Epoch 680, training loss: 12.404684066772461 = 0.29771196842193604 + 2.0 * 6.053485870361328
Epoch 680, val loss: 0.7304851412773132
Epoch 690, training loss: 12.389575004577637 = 0.2830370366573334 + 2.0 * 6.053268909454346
Epoch 690, val loss: 0.7269561886787415
Epoch 700, training loss: 12.379800796508789 = 0.2690885365009308 + 2.0 * 6.055356025695801
Epoch 700, val loss: 0.7238817811012268
Epoch 710, training loss: 12.356056213378906 = 0.2558867633342743 + 2.0 * 6.050084590911865
Epoch 710, val loss: 0.7217504382133484
Epoch 720, training loss: 12.338815689086914 = 0.24327345192432404 + 2.0 * 6.047770977020264
Epoch 720, val loss: 0.7200643420219421
Epoch 730, training loss: 12.324566841125488 = 0.23122619092464447 + 2.0 * 6.046670436859131
Epoch 730, val loss: 0.71896892786026
Epoch 740, training loss: 12.349603652954102 = 0.21978412568569183 + 2.0 * 6.064909934997559
Epoch 740, val loss: 0.7183249592781067
Epoch 750, training loss: 12.30927848815918 = 0.2091117948293686 + 2.0 * 6.050083160400391
Epoch 750, val loss: 0.7180909514427185
Epoch 760, training loss: 12.28990364074707 = 0.19907428324222565 + 2.0 * 6.045414447784424
Epoch 760, val loss: 0.7184908986091614
Epoch 770, training loss: 12.275795936584473 = 0.18956124782562256 + 2.0 * 6.043117523193359
Epoch 770, val loss: 0.7191751003265381
Epoch 780, training loss: 12.269266128540039 = 0.18053968250751495 + 2.0 * 6.044363021850586
Epoch 780, val loss: 0.7203043699264526
Epoch 790, training loss: 12.256251335144043 = 0.17202891409397125 + 2.0 * 6.042111396789551
Epoch 790, val loss: 0.721616804599762
Epoch 800, training loss: 12.247024536132812 = 0.16401268541812897 + 2.0 * 6.041505813598633
Epoch 800, val loss: 0.7234400510787964
Epoch 810, training loss: 12.237767219543457 = 0.15641997754573822 + 2.0 * 6.040673732757568
Epoch 810, val loss: 0.7254096269607544
Epoch 820, training loss: 12.240912437438965 = 0.1492338627576828 + 2.0 * 6.045839309692383
Epoch 820, val loss: 0.7275822758674622
Epoch 830, training loss: 12.225321769714355 = 0.14248155057430267 + 2.0 * 6.041419982910156
Epoch 830, val loss: 0.7301300764083862
Epoch 840, training loss: 12.217171669006348 = 0.1361093670129776 + 2.0 * 6.040531158447266
Epoch 840, val loss: 0.7326094508171082
Epoch 850, training loss: 12.204510688781738 = 0.1301155388355255 + 2.0 * 6.037197589874268
Epoch 850, val loss: 0.7354876399040222
Epoch 860, training loss: 12.197199821472168 = 0.12442333996295929 + 2.0 * 6.036388397216797
Epoch 860, val loss: 0.7384214401245117
Epoch 870, training loss: 12.190556526184082 = 0.11902707815170288 + 2.0 * 6.035764694213867
Epoch 870, val loss: 0.7414759397506714
Epoch 880, training loss: 12.187457084655762 = 0.1139184832572937 + 2.0 * 6.036769390106201
Epoch 880, val loss: 0.7444795370101929
Epoch 890, training loss: 12.178508758544922 = 0.10910467058420181 + 2.0 * 6.034701824188232
Epoch 890, val loss: 0.7477880120277405
Epoch 900, training loss: 12.173710823059082 = 0.10454503446817398 + 2.0 * 6.03458309173584
Epoch 900, val loss: 0.7511181235313416
Epoch 910, training loss: 12.172517776489258 = 0.10023318231105804 + 2.0 * 6.036142349243164
Epoch 910, val loss: 0.754381000995636
Epoch 920, training loss: 12.160075187683105 = 0.09615717828273773 + 2.0 * 6.031959056854248
Epoch 920, val loss: 0.7579578161239624
Epoch 930, training loss: 12.155869483947754 = 0.09228570759296417 + 2.0 * 6.031791687011719
Epoch 930, val loss: 0.7615886330604553
Epoch 940, training loss: 12.155416488647461 = 0.08860527724027634 + 2.0 * 6.033405780792236
Epoch 940, val loss: 0.7651176452636719
Epoch 950, training loss: 12.14832878112793 = 0.08513116836547852 + 2.0 * 6.0315985679626465
Epoch 950, val loss: 0.7687282562255859
Epoch 960, training loss: 12.140195846557617 = 0.08183314651250839 + 2.0 * 6.029181480407715
Epoch 960, val loss: 0.7725493311882019
Epoch 970, training loss: 12.136038780212402 = 0.07869300246238708 + 2.0 * 6.028672695159912
Epoch 970, val loss: 0.7763962745666504
Epoch 980, training loss: 12.131463050842285 = 0.07570450752973557 + 2.0 * 6.027879238128662
Epoch 980, val loss: 0.7803040742874146
Epoch 990, training loss: 12.162622451782227 = 0.07286740839481354 + 2.0 * 6.044877529144287
Epoch 990, val loss: 0.7841344475746155
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7963
Overall ASR: 0.5720
Flip ASR: 0.5022/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.706382751464844 = 1.958706021308899 + 2.0 * 8.373838424682617
Epoch 0, val loss: 1.9705215692520142
Epoch 10, training loss: 18.695430755615234 = 1.9486908912658691 + 2.0 * 8.373370170593262
Epoch 10, val loss: 1.9602911472320557
Epoch 20, training loss: 18.676671981811523 = 1.9365406036376953 + 2.0 * 8.370065689086914
Epoch 20, val loss: 1.9474754333496094
Epoch 30, training loss: 18.606704711914062 = 1.9201716184616089 + 2.0 * 8.343266487121582
Epoch 30, val loss: 1.9300779104232788
Epoch 40, training loss: 18.02242660522461 = 1.9012744426727295 + 2.0 * 8.060576438903809
Epoch 40, val loss: 1.9103773832321167
Epoch 50, training loss: 16.23977279663086 = 1.882676601409912 + 2.0 * 7.1785478591918945
Epoch 50, val loss: 1.891201376914978
Epoch 60, training loss: 15.621803283691406 = 1.869240403175354 + 2.0 * 6.876281261444092
Epoch 60, val loss: 1.8780863285064697
Epoch 70, training loss: 15.234140396118164 = 1.8586481809616089 + 2.0 * 6.687746047973633
Epoch 70, val loss: 1.8669662475585938
Epoch 80, training loss: 14.992637634277344 = 1.8482942581176758 + 2.0 * 6.572171688079834
Epoch 80, val loss: 1.856250524520874
Epoch 90, training loss: 14.840629577636719 = 1.8379929065704346 + 2.0 * 6.501318454742432
Epoch 90, val loss: 1.8450394868850708
Epoch 100, training loss: 14.70128345489502 = 1.8279532194137573 + 2.0 * 6.436665058135986
Epoch 100, val loss: 1.8342944383621216
Epoch 110, training loss: 14.582539558410645 = 1.8189610242843628 + 2.0 * 6.381789207458496
Epoch 110, val loss: 1.8242460489273071
Epoch 120, training loss: 14.488283157348633 = 1.8108086585998535 + 2.0 * 6.338737487792969
Epoch 120, val loss: 1.814921498298645
Epoch 130, training loss: 14.407553672790527 = 1.8028229475021362 + 2.0 * 6.302365303039551
Epoch 130, val loss: 1.8058829307556152
Epoch 140, training loss: 14.338163375854492 = 1.7950996160507202 + 2.0 * 6.27153205871582
Epoch 140, val loss: 1.797146201133728
Epoch 150, training loss: 14.28300666809082 = 1.7874351739883423 + 2.0 * 6.247785568237305
Epoch 150, val loss: 1.7888282537460327
Epoch 160, training loss: 14.232648849487305 = 1.7796034812927246 + 2.0 * 6.226522445678711
Epoch 160, val loss: 1.780606985092163
Epoch 170, training loss: 14.187769889831543 = 1.7711340188980103 + 2.0 * 6.208317756652832
Epoch 170, val loss: 1.7722983360290527
Epoch 180, training loss: 14.149004936218262 = 1.7618144750595093 + 2.0 * 6.1935954093933105
Epoch 180, val loss: 1.7635353803634644
Epoch 190, training loss: 14.111211776733398 = 1.7514580488204956 + 2.0 * 6.179876804351807
Epoch 190, val loss: 1.7542314529418945
Epoch 200, training loss: 14.076958656311035 = 1.7397593259811401 + 2.0 * 6.168599605560303
Epoch 200, val loss: 1.744059443473816
Epoch 210, training loss: 14.044577598571777 = 1.726357340812683 + 2.0 * 6.159110069274902
Epoch 210, val loss: 1.7326539754867554
Epoch 220, training loss: 14.01340389251709 = 1.710921049118042 + 2.0 * 6.151241302490234
Epoch 220, val loss: 1.7198386192321777
Epoch 230, training loss: 13.980424880981445 = 1.693354845046997 + 2.0 * 6.143535137176514
Epoch 230, val loss: 1.7052927017211914
Epoch 240, training loss: 13.944735527038574 = 1.6730022430419922 + 2.0 * 6.135866641998291
Epoch 240, val loss: 1.6886686086654663
Epoch 250, training loss: 13.908707618713379 = 1.6492856740951538 + 2.0 * 6.129711151123047
Epoch 250, val loss: 1.6693024635314941
Epoch 260, training loss: 13.880985260009766 = 1.6215519905090332 + 2.0 * 6.129716396331787
Epoch 260, val loss: 1.6466445922851562
Epoch 270, training loss: 13.833142280578613 = 1.5897942781448364 + 2.0 * 6.121674060821533
Epoch 270, val loss: 1.6208746433258057
Epoch 280, training loss: 13.785591125488281 = 1.553675889968872 + 2.0 * 6.115957736968994
Epoch 280, val loss: 1.5914034843444824
Epoch 290, training loss: 13.738357543945312 = 1.5126211643218994 + 2.0 * 6.112868309020996
Epoch 290, val loss: 1.557724952697754
Epoch 300, training loss: 13.689107894897461 = 1.4669712781906128 + 2.0 * 6.111068248748779
Epoch 300, val loss: 1.5207351446151733
Epoch 310, training loss: 13.629684448242188 = 1.4179526567459106 + 2.0 * 6.105865955352783
Epoch 310, val loss: 1.4807804822921753
Epoch 320, training loss: 13.571481704711914 = 1.3659100532531738 + 2.0 * 6.102785587310791
Epoch 320, val loss: 1.4384623765945435
Epoch 330, training loss: 13.522488594055176 = 1.3120079040527344 + 2.0 * 6.105240345001221
Epoch 330, val loss: 1.3948826789855957
Epoch 340, training loss: 13.456756591796875 = 1.258536458015442 + 2.0 * 6.099110126495361
Epoch 340, val loss: 1.3522456884384155
Epoch 350, training loss: 13.396784782409668 = 1.206412434577942 + 2.0 * 6.095186233520508
Epoch 350, val loss: 1.3107675313949585
Epoch 360, training loss: 13.342757225036621 = 1.155938744544983 + 2.0 * 6.093409061431885
Epoch 360, val loss: 1.2709819078445435
Epoch 370, training loss: 13.289916038513184 = 1.1080037355422974 + 2.0 * 6.090956211090088
Epoch 370, val loss: 1.2332383394241333
Epoch 380, training loss: 13.246105194091797 = 1.0627163648605347 + 2.0 * 6.091694355010986
Epoch 380, val loss: 1.1981757879257202
Epoch 390, training loss: 13.192363739013672 = 1.0206871032714844 + 2.0 * 6.085838317871094
Epoch 390, val loss: 1.16523277759552
Epoch 400, training loss: 13.14482593536377 = 0.9809216856956482 + 2.0 * 6.081952095031738
Epoch 400, val loss: 1.134355068206787
Epoch 410, training loss: 13.101765632629395 = 0.9432027339935303 + 2.0 * 6.079281330108643
Epoch 410, val loss: 1.1050169467926025
Epoch 420, training loss: 13.081710815429688 = 0.9072414040565491 + 2.0 * 6.0872344970703125
Epoch 420, val loss: 1.0771597623825073
Epoch 430, training loss: 13.028482437133789 = 0.8736557364463806 + 2.0 * 6.077413558959961
Epoch 430, val loss: 1.0510752201080322
Epoch 440, training loss: 12.988805770874023 = 0.8418193459510803 + 2.0 * 6.073493003845215
Epoch 440, val loss: 1.0263766050338745
Epoch 450, training loss: 12.957281112670898 = 0.8115429282188416 + 2.0 * 6.072869300842285
Epoch 450, val loss: 1.003091812133789
Epoch 460, training loss: 12.92212200164795 = 0.7829960584640503 + 2.0 * 6.069562911987305
Epoch 460, val loss: 0.9812278747558594
Epoch 470, training loss: 12.891505241394043 = 0.7558727264404297 + 2.0 * 6.067816257476807
Epoch 470, val loss: 0.9607886672019958
Epoch 480, training loss: 12.870805740356445 = 0.7302265167236328 + 2.0 * 6.070289611816406
Epoch 480, val loss: 0.941642701625824
Epoch 490, training loss: 12.840709686279297 = 0.7060292959213257 + 2.0 * 6.06734037399292
Epoch 490, val loss: 0.9241604804992676
Epoch 500, training loss: 12.81014347076416 = 0.6834238171577454 + 2.0 * 6.06335973739624
Epoch 500, val loss: 0.9080509543418884
Epoch 510, training loss: 12.784123420715332 = 0.6619710326194763 + 2.0 * 6.0610761642456055
Epoch 510, val loss: 0.8933197259902954
Epoch 520, training loss: 12.773537635803223 = 0.6414773464202881 + 2.0 * 6.066030025482178
Epoch 520, val loss: 0.8796511888504028
Epoch 530, training loss: 12.738885879516602 = 0.6219332814216614 + 2.0 * 6.058476448059082
Epoch 530, val loss: 0.8670077323913574
Epoch 540, training loss: 12.717072486877441 = 0.6031157970428467 + 2.0 * 6.056978225708008
Epoch 540, val loss: 0.8553100228309631
Epoch 550, training loss: 12.697028160095215 = 0.5848252177238464 + 2.0 * 6.056101322174072
Epoch 550, val loss: 0.8443675637245178
Epoch 560, training loss: 12.687082290649414 = 0.5669800043106079 + 2.0 * 6.060050964355469
Epoch 560, val loss: 0.8340635299682617
Epoch 570, training loss: 12.660871505737305 = 0.5497847199440002 + 2.0 * 6.055543422698975
Epoch 570, val loss: 0.8242648243904114
Epoch 580, training loss: 12.637358665466309 = 0.5328385829925537 + 2.0 * 6.052259922027588
Epoch 580, val loss: 0.8151047825813293
Epoch 590, training loss: 12.617568016052246 = 0.5161249041557312 + 2.0 * 6.050721645355225
Epoch 590, val loss: 0.8062323927879333
Epoch 600, training loss: 12.61850357055664 = 0.4995611310005188 + 2.0 * 6.059471130371094
Epoch 600, val loss: 0.7976571917533875
Epoch 610, training loss: 12.58067798614502 = 0.4832625687122345 + 2.0 * 6.048707485198975
Epoch 610, val loss: 0.7894545197486877
Epoch 620, training loss: 12.563859939575195 = 0.4671488404273987 + 2.0 * 6.048355579376221
Epoch 620, val loss: 0.7816141247749329
Epoch 630, training loss: 12.54405403137207 = 0.4511604309082031 + 2.0 * 6.046446800231934
Epoch 630, val loss: 0.7740136981010437
Epoch 640, training loss: 12.528464317321777 = 0.43528997898101807 + 2.0 * 6.046586990356445
Epoch 640, val loss: 0.7667637467384338
Epoch 650, training loss: 12.530487060546875 = 0.4196651577949524 + 2.0 * 6.055410861968994
Epoch 650, val loss: 0.7596331238746643
Epoch 660, training loss: 12.49789810180664 = 0.4044508934020996 + 2.0 * 6.046723365783691
Epoch 660, val loss: 0.753079891204834
Epoch 670, training loss: 12.47498607635498 = 0.38942837715148926 + 2.0 * 6.042778968811035
Epoch 670, val loss: 0.74685138463974
Epoch 680, training loss: 12.458654403686523 = 0.3745732605457306 + 2.0 * 6.0420403480529785
Epoch 680, val loss: 0.7408446669578552
Epoch 690, training loss: 12.442429542541504 = 0.359876811504364 + 2.0 * 6.041276454925537
Epoch 690, val loss: 0.735106885433197
Epoch 700, training loss: 12.45007038116455 = 0.34542152285575867 + 2.0 * 6.052324295043945
Epoch 700, val loss: 0.7297008633613586
Epoch 710, training loss: 12.413787841796875 = 0.33155596256256104 + 2.0 * 6.041115760803223
Epoch 710, val loss: 0.724593997001648
Epoch 720, training loss: 12.398866653442383 = 0.31807732582092285 + 2.0 * 6.0403947830200195
Epoch 720, val loss: 0.7200727462768555
Epoch 730, training loss: 12.382691383361816 = 0.30496928095817566 + 2.0 * 6.038861274719238
Epoch 730, val loss: 0.7158682942390442
Epoch 740, training loss: 12.367349624633789 = 0.2922196388244629 + 2.0 * 6.037565231323242
Epoch 740, val loss: 0.7121210098266602
Epoch 750, training loss: 12.363506317138672 = 0.2798606753349304 + 2.0 * 6.041822910308838
Epoch 750, val loss: 0.7087387442588806
Epoch 760, training loss: 12.347686767578125 = 0.2679542899131775 + 2.0 * 6.0398664474487305
Epoch 760, val loss: 0.7059001326560974
Epoch 770, training loss: 12.329324722290039 = 0.25656989216804504 + 2.0 * 6.036377429962158
Epoch 770, val loss: 0.7034159302711487
Epoch 780, training loss: 12.314823150634766 = 0.24562959372997284 + 2.0 * 6.034596920013428
Epoch 780, val loss: 0.701470136642456
Epoch 790, training loss: 12.302605628967285 = 0.23509719967842102 + 2.0 * 6.033754348754883
Epoch 790, val loss: 0.6998873353004456
Epoch 800, training loss: 12.29212760925293 = 0.22495374083518982 + 2.0 * 6.0335869789123535
Epoch 800, val loss: 0.6987490653991699
Epoch 810, training loss: 12.289661407470703 = 0.21523529291152954 + 2.0 * 6.03721284866333
Epoch 810, val loss: 0.6979126334190369
Epoch 820, training loss: 12.275588989257812 = 0.20605230331420898 + 2.0 * 6.034768581390381
Epoch 820, val loss: 0.6976349949836731
Epoch 830, training loss: 12.260978698730469 = 0.19726119935512543 + 2.0 * 6.031858921051025
Epoch 830, val loss: 0.697590708732605
Epoch 840, training loss: 12.250597953796387 = 0.18881918489933014 + 2.0 * 6.030889511108398
Epoch 840, val loss: 0.6978924870491028
Epoch 850, training loss: 12.249433517456055 = 0.18074443936347961 + 2.0 * 6.034344673156738
Epoch 850, val loss: 0.6984556913375854
Epoch 860, training loss: 12.236398696899414 = 0.17317360639572144 + 2.0 * 6.031612396240234
Epoch 860, val loss: 0.699338436126709
Epoch 870, training loss: 12.224241256713867 = 0.16592131555080414 + 2.0 * 6.029160022735596
Epoch 870, val loss: 0.7005330324172974
Epoch 880, training loss: 12.214365005493164 = 0.15899042785167694 + 2.0 * 6.027687072753906
Epoch 880, val loss: 0.7020080089569092
Epoch 890, training loss: 12.206503868103027 = 0.15235525369644165 + 2.0 * 6.027074337005615
Epoch 890, val loss: 0.7037804126739502
Epoch 900, training loss: 12.209152221679688 = 0.14601002633571625 + 2.0 * 6.031570911407471
Epoch 900, val loss: 0.7058316469192505
Epoch 910, training loss: 12.209999084472656 = 0.13995355367660522 + 2.0 * 6.035022735595703
Epoch 910, val loss: 0.708018958568573
Epoch 920, training loss: 12.18875503540039 = 0.1342773735523224 + 2.0 * 6.027238845825195
Epoch 920, val loss: 0.7103020548820496
Epoch 930, training loss: 12.180758476257324 = 0.12883158028125763 + 2.0 * 6.025963306427002
Epoch 930, val loss: 0.7129111289978027
Epoch 940, training loss: 12.17177677154541 = 0.12364199757575989 + 2.0 * 6.024067401885986
Epoch 940, val loss: 0.715667724609375
Epoch 950, training loss: 12.171564102172852 = 0.1186739057302475 + 2.0 * 6.026444911956787
Epoch 950, val loss: 0.7186688780784607
Epoch 960, training loss: 12.160656929016113 = 0.11394558101892471 + 2.0 * 6.023355484008789
Epoch 960, val loss: 0.7217904329299927
Epoch 970, training loss: 12.155051231384277 = 0.10944750159978867 + 2.0 * 6.022801876068115
Epoch 970, val loss: 0.7251356840133667
Epoch 980, training loss: 12.167051315307617 = 0.10515798628330231 + 2.0 * 6.030946731567383
Epoch 980, val loss: 0.7286660671234131
Epoch 990, training loss: 12.147161483764648 = 0.10108187049627304 + 2.0 * 6.023039817810059
Epoch 990, val loss: 0.732194185256958
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9004
Flip ASR: 0.8800/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.69133949279785 = 1.9438139200210571 + 2.0 * 8.373763084411621
Epoch 0, val loss: 1.942535161972046
Epoch 10, training loss: 18.67698860168457 = 1.933092474937439 + 2.0 * 8.3719482421875
Epoch 10, val loss: 1.931153416633606
Epoch 20, training loss: 18.652915954589844 = 1.9198373556137085 + 2.0 * 8.366539001464844
Epoch 20, val loss: 1.9163402318954468
Epoch 30, training loss: 18.586336135864258 = 1.902127981185913 + 2.0 * 8.342103958129883
Epoch 30, val loss: 1.8962945938110352
Epoch 40, training loss: 18.233888626098633 = 1.8821395635604858 + 2.0 * 8.175874710083008
Epoch 40, val loss: 1.8740204572677612
Epoch 50, training loss: 17.246870040893555 = 1.8626986742019653 + 2.0 * 7.6920857429504395
Epoch 50, val loss: 1.8527138233184814
Epoch 60, training loss: 16.565092086791992 = 1.8447487354278564 + 2.0 * 7.360171318054199
Epoch 60, val loss: 1.8345184326171875
Epoch 70, training loss: 15.985466957092285 = 1.8295422792434692 + 2.0 * 7.077962398529053
Epoch 70, val loss: 1.8197128772735596
Epoch 80, training loss: 15.39206314086914 = 1.820178508758545 + 2.0 * 6.785942077636719
Epoch 80, val loss: 1.8107420206069946
Epoch 90, training loss: 15.029008865356445 = 1.8136359453201294 + 2.0 * 6.607686519622803
Epoch 90, val loss: 1.8037242889404297
Epoch 100, training loss: 14.828316688537598 = 1.806025743484497 + 2.0 * 6.51114559173584
Epoch 100, val loss: 1.795393943786621
Epoch 110, training loss: 14.675865173339844 = 1.7970690727233887 + 2.0 * 6.439397811889648
Epoch 110, val loss: 1.7861881256103516
Epoch 120, training loss: 14.565027236938477 = 1.7880487442016602 + 2.0 * 6.388489246368408
Epoch 120, val loss: 1.777300238609314
Epoch 130, training loss: 14.480510711669922 = 1.7796220779418945 + 2.0 * 6.350444316864014
Epoch 130, val loss: 1.7689956426620483
Epoch 140, training loss: 14.411229133605957 = 1.7717784643173218 + 2.0 * 6.319725513458252
Epoch 140, val loss: 1.7611697912216187
Epoch 150, training loss: 14.354642868041992 = 1.7641241550445557 + 2.0 * 6.295259475708008
Epoch 150, val loss: 1.7538201808929443
Epoch 160, training loss: 14.30427360534668 = 1.7563594579696655 + 2.0 * 6.273957252502441
Epoch 160, val loss: 1.7466049194335938
Epoch 170, training loss: 14.260695457458496 = 1.7480791807174683 + 2.0 * 6.256308078765869
Epoch 170, val loss: 1.7393823862075806
Epoch 180, training loss: 14.222691535949707 = 1.739190936088562 + 2.0 * 6.241750240325928
Epoch 180, val loss: 1.7319756746292114
Epoch 190, training loss: 14.182855606079102 = 1.7294734716415405 + 2.0 * 6.226691246032715
Epoch 190, val loss: 1.7243303060531616
Epoch 200, training loss: 14.146204948425293 = 1.7188453674316406 + 2.0 * 6.213679790496826
Epoch 200, val loss: 1.7162072658538818
Epoch 210, training loss: 14.111645698547363 = 1.7067233324050903 + 2.0 * 6.202461242675781
Epoch 210, val loss: 1.7071536779403687
Epoch 220, training loss: 14.077668190002441 = 1.692765474319458 + 2.0 * 6.192451477050781
Epoch 220, val loss: 1.6968348026275635
Epoch 230, training loss: 14.049829483032227 = 1.6766173839569092 + 2.0 * 6.186605930328369
Epoch 230, val loss: 1.6850303411483765
Epoch 240, training loss: 14.01003360748291 = 1.6580657958984375 + 2.0 * 6.175983905792236
Epoch 240, val loss: 1.6714452505111694
Epoch 250, training loss: 13.972163200378418 = 1.6364864110946655 + 2.0 * 6.1678385734558105
Epoch 250, val loss: 1.6556448936462402
Epoch 260, training loss: 13.932748794555664 = 1.6109113693237305 + 2.0 * 6.160918712615967
Epoch 260, val loss: 1.6367696523666382
Epoch 270, training loss: 13.901620864868164 = 1.5809067487716675 + 2.0 * 6.1603569984436035
Epoch 270, val loss: 1.6147129535675049
Epoch 280, training loss: 13.845748901367188 = 1.5470025539398193 + 2.0 * 6.1493730545043945
Epoch 280, val loss: 1.5893763303756714
Epoch 290, training loss: 13.795520782470703 = 1.508707880973816 + 2.0 * 6.143406391143799
Epoch 290, val loss: 1.560865044593811
Epoch 300, training loss: 13.74504566192627 = 1.466560959815979 + 2.0 * 6.139242172241211
Epoch 300, val loss: 1.5292736291885376
Epoch 310, training loss: 13.690860748291016 = 1.4210920333862305 + 2.0 * 6.134884357452393
Epoch 310, val loss: 1.4952515363693237
Epoch 320, training loss: 13.633588790893555 = 1.3738023042678833 + 2.0 * 6.1298933029174805
Epoch 320, val loss: 1.4593182802200317
Epoch 330, training loss: 13.57957649230957 = 1.3261302709579468 + 2.0 * 6.126723289489746
Epoch 330, val loss: 1.423182487487793
Epoch 340, training loss: 13.52238655090332 = 1.278841257095337 + 2.0 * 6.121772766113281
Epoch 340, val loss: 1.38735032081604
Epoch 350, training loss: 13.472785949707031 = 1.2327531576156616 + 2.0 * 6.120016574859619
Epoch 350, val loss: 1.3524681329727173
Epoch 360, training loss: 13.419602394104004 = 1.1885623931884766 + 2.0 * 6.115520000457764
Epoch 360, val loss: 1.319198727607727
Epoch 370, training loss: 13.369253158569336 = 1.145645260810852 + 2.0 * 6.111804008483887
Epoch 370, val loss: 1.2870125770568848
Epoch 380, training loss: 13.328173637390137 = 1.1034858226776123 + 2.0 * 6.112343788146973
Epoch 380, val loss: 1.2558099031448364
Epoch 390, training loss: 13.27466869354248 = 1.0623074769973755 + 2.0 * 6.106180667877197
Epoch 390, val loss: 1.2250341176986694
Epoch 400, training loss: 13.229581832885742 = 1.021348237991333 + 2.0 * 6.104116916656494
Epoch 400, val loss: 1.1948350667953491
Epoch 410, training loss: 13.185564041137695 = 0.9806711673736572 + 2.0 * 6.102446556091309
Epoch 410, val loss: 1.1646186113357544
Epoch 420, training loss: 13.138668060302734 = 0.940195620059967 + 2.0 * 6.099236011505127
Epoch 420, val loss: 1.1345833539962769
Epoch 430, training loss: 13.0912504196167 = 0.90031498670578 + 2.0 * 6.095467567443848
Epoch 430, val loss: 1.1049131155014038
Epoch 440, training loss: 13.073164939880371 = 0.8613196015357971 + 2.0 * 6.105922698974609
Epoch 440, val loss: 1.0759460926055908
Epoch 450, training loss: 13.009319305419922 = 0.8244022727012634 + 2.0 * 6.092458724975586
Epoch 450, val loss: 1.048277497291565
Epoch 460, training loss: 12.968382835388184 = 0.7892256379127502 + 2.0 * 6.089578628540039
Epoch 460, val loss: 1.0220518112182617
Epoch 470, training loss: 12.929327011108398 = 0.7556934356689453 + 2.0 * 6.086816787719727
Epoch 470, val loss: 0.9972907304763794
Epoch 480, training loss: 12.893815994262695 = 0.7240769267082214 + 2.0 * 6.084869384765625
Epoch 480, val loss: 0.9740710258483887
Epoch 490, training loss: 12.8617582321167 = 0.6946240067481995 + 2.0 * 6.083567142486572
Epoch 490, val loss: 0.9527876377105713
Epoch 500, training loss: 12.829229354858398 = 0.6669363975524902 + 2.0 * 6.081146240234375
Epoch 500, val loss: 0.9332386255264282
Epoch 510, training loss: 12.81198501586914 = 0.6409835815429688 + 2.0 * 6.085500717163086
Epoch 510, val loss: 0.9152404069900513
Epoch 520, training loss: 12.77229118347168 = 0.6168835163116455 + 2.0 * 6.077703952789307
Epoch 520, val loss: 0.8992094993591309
Epoch 530, training loss: 12.745016098022461 = 0.5942943096160889 + 2.0 * 6.0753607749938965
Epoch 530, val loss: 0.8846598267555237
Epoch 540, training loss: 12.720304489135742 = 0.5728606581687927 + 2.0 * 6.073721885681152
Epoch 540, val loss: 0.8713237643241882
Epoch 550, training loss: 12.71242618560791 = 0.5525414943695068 + 2.0 * 6.079942226409912
Epoch 550, val loss: 0.8591139912605286
Epoch 560, training loss: 12.679496765136719 = 0.5333417057991028 + 2.0 * 6.07307767868042
Epoch 560, val loss: 0.8483366966247559
Epoch 570, training loss: 12.652052879333496 = 0.5150894522666931 + 2.0 * 6.068481922149658
Epoch 570, val loss: 0.8384827971458435
Epoch 580, training loss: 12.631867408752441 = 0.4974622428417206 + 2.0 * 6.067202568054199
Epoch 580, val loss: 0.829525351524353
Epoch 590, training loss: 12.627691268920898 = 0.4804084002971649 + 2.0 * 6.073641300201416
Epoch 590, val loss: 0.8213365077972412
Epoch 600, training loss: 12.599297523498535 = 0.46413540840148926 + 2.0 * 6.0675811767578125
Epoch 600, val loss: 0.8137426376342773
Epoch 610, training loss: 12.575311660766602 = 0.4482593536376953 + 2.0 * 6.063526153564453
Epoch 610, val loss: 0.8069323301315308
Epoch 620, training loss: 12.558712005615234 = 0.43285101652145386 + 2.0 * 6.062930583953857
Epoch 620, val loss: 0.8005357980728149
Epoch 630, training loss: 12.539441108703613 = 0.41786861419677734 + 2.0 * 6.060786247253418
Epoch 630, val loss: 0.7946807146072388
Epoch 640, training loss: 12.552828788757324 = 0.4031251072883606 + 2.0 * 6.074851989746094
Epoch 640, val loss: 0.7892255783081055
Epoch 650, training loss: 12.50842571258545 = 0.38894832134246826 + 2.0 * 6.059738636016846
Epoch 650, val loss: 0.7840975522994995
Epoch 660, training loss: 12.489289283752441 = 0.37500786781311035 + 2.0 * 6.057140827178955
Epoch 660, val loss: 0.7795026302337646
Epoch 670, training loss: 12.472407341003418 = 0.36126795411109924 + 2.0 * 6.055569648742676
Epoch 670, val loss: 0.7751753926277161
Epoch 680, training loss: 12.468017578125 = 0.34765639901161194 + 2.0 * 6.0601806640625
Epoch 680, val loss: 0.7710572481155396
Epoch 690, training loss: 12.446578979492188 = 0.3343949317932129 + 2.0 * 6.056092262268066
Epoch 690, val loss: 0.7670561671257019
Epoch 700, training loss: 12.429803848266602 = 0.3212616741657257 + 2.0 * 6.054271221160889
Epoch 700, val loss: 0.763577401638031
Epoch 710, training loss: 12.412912368774414 = 0.308410108089447 + 2.0 * 6.05225133895874
Epoch 710, val loss: 0.7603004574775696
Epoch 720, training loss: 12.411638259887695 = 0.29573267698287964 + 2.0 * 6.057952880859375
Epoch 720, val loss: 0.757401704788208
Epoch 730, training loss: 12.397140502929688 = 0.28349608182907104 + 2.0 * 6.056822299957275
Epoch 730, val loss: 0.754837155342102
Epoch 740, training loss: 12.37597942352295 = 0.271621435880661 + 2.0 * 6.052178859710693
Epoch 740, val loss: 0.7529839277267456
Epoch 750, training loss: 12.357620239257812 = 0.2601974606513977 + 2.0 * 6.04871129989624
Epoch 750, val loss: 0.7517266869544983
Epoch 760, training loss: 12.34536075592041 = 0.2491680532693863 + 2.0 * 6.048096179962158
Epoch 760, val loss: 0.7511889338493347
Epoch 770, training loss: 12.343796730041504 = 0.23858384788036346 + 2.0 * 6.052606582641602
Epoch 770, val loss: 0.7513415217399597
Epoch 780, training loss: 12.324300765991211 = 0.22844178974628448 + 2.0 * 6.047929286956787
Epoch 780, val loss: 0.7522681355476379
Epoch 790, training loss: 12.325146675109863 = 0.21885938942432404 + 2.0 * 6.053143501281738
Epoch 790, val loss: 0.7535634636878967
Epoch 800, training loss: 12.299556732177734 = 0.20969828963279724 + 2.0 * 6.044929027557373
Epoch 800, val loss: 0.7552987337112427
Epoch 810, training loss: 12.288904190063477 = 0.20100916922092438 + 2.0 * 6.043947696685791
Epoch 810, val loss: 0.7573148608207703
Epoch 820, training loss: 12.277153015136719 = 0.1926708221435547 + 2.0 * 6.042241096496582
Epoch 820, val loss: 0.7596269845962524
Epoch 830, training loss: 12.26989459991455 = 0.1846960037946701 + 2.0 * 6.042599201202393
Epoch 830, val loss: 0.7622473239898682
Epoch 840, training loss: 12.267718315124512 = 0.17711278796195984 + 2.0 * 6.045302867889404
Epoch 840, val loss: 0.7650588154792786
Epoch 850, training loss: 12.255456924438477 = 0.1699647605419159 + 2.0 * 6.042746067047119
Epoch 850, val loss: 0.7681174278259277
Epoch 860, training loss: 12.250266075134277 = 0.16319185495376587 + 2.0 * 6.043537139892578
Epoch 860, val loss: 0.7714619636535645
Epoch 870, training loss: 12.234723091125488 = 0.1567821204662323 + 2.0 * 6.038970470428467
Epoch 870, val loss: 0.7749277949333191
Epoch 880, training loss: 12.228888511657715 = 0.15065939724445343 + 2.0 * 6.039114475250244
Epoch 880, val loss: 0.7786877155303955
Epoch 890, training loss: 12.218141555786133 = 0.14479053020477295 + 2.0 * 6.036675453186035
Epoch 890, val loss: 0.7826213240623474
Epoch 900, training loss: 12.212024688720703 = 0.13918957114219666 + 2.0 * 6.036417484283447
Epoch 900, val loss: 0.7867076396942139
Epoch 910, training loss: 12.220931053161621 = 0.13387151062488556 + 2.0 * 6.043529987335205
Epoch 910, val loss: 0.7908263206481934
Epoch 920, training loss: 12.212653160095215 = 0.1288398802280426 + 2.0 * 6.041906833648682
Epoch 920, val loss: 0.7950272560119629
Epoch 930, training loss: 12.195239067077637 = 0.12407601624727249 + 2.0 * 6.035581588745117
Epoch 930, val loss: 0.7994387745857239
Epoch 940, training loss: 12.187826156616211 = 0.11951868236064911 + 2.0 * 6.034153938293457
Epoch 940, val loss: 0.8039066791534424
Epoch 950, training loss: 12.185197830200195 = 0.11515633016824722 + 2.0 * 6.03502082824707
Epoch 950, val loss: 0.8084805011749268
Epoch 960, training loss: 12.188545227050781 = 0.11099015176296234 + 2.0 * 6.0387773513793945
Epoch 960, val loss: 0.813150942325592
Epoch 970, training loss: 12.173130989074707 = 0.10705818980932236 + 2.0 * 6.033036231994629
Epoch 970, val loss: 0.8177959322929382
Epoch 980, training loss: 12.166892051696777 = 0.10328235477209091 + 2.0 * 6.031805038452148
Epoch 980, val loss: 0.8226141929626465
Epoch 990, training loss: 12.160064697265625 = 0.09966640174388885 + 2.0 * 6.03019905090332
Epoch 990, val loss: 0.8274034261703491
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7963
Overall ASR: 0.2878
Flip ASR: 0.2978/225 nodes
The final ASR:0.58672, 0.25029, Accuracy:0.81111, 0.02095
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11638])
remove edge: torch.Size([2, 9530])
updated graph: torch.Size([2, 10612])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9926
Flip ASR: 0.9911/225 nodes
The final ASR:0.98155, 0.00797, Accuracy:0.83210, 0.00630
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.5 der2:0.2 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 18.68181610107422 = 1.9341254234313965 + 2.0 * 8.373845100402832
Epoch 0, val loss: 1.926321268081665
Epoch 10, training loss: 18.670867919921875 = 1.9243725538253784 + 2.0 * 8.373248100280762
Epoch 10, val loss: 1.9166706800460815
Epoch 20, training loss: 18.650598526000977 = 1.9121326208114624 + 2.0 * 8.369233131408691
Epoch 20, val loss: 1.9045382738113403
Epoch 30, training loss: 18.574077606201172 = 1.8956607580184937 + 2.0 * 8.339208602905273
Epoch 30, val loss: 1.8885356187820435
Epoch 40, training loss: 18.02694320678711 = 1.8773788213729858 + 2.0 * 8.074782371520996
Epoch 40, val loss: 1.8714767694473267
Epoch 50, training loss: 16.634227752685547 = 1.8579869270324707 + 2.0 * 7.388120174407959
Epoch 50, val loss: 1.853118658065796
Epoch 60, training loss: 15.871164321899414 = 1.8436552286148071 + 2.0 * 7.013754367828369
Epoch 60, val loss: 1.8396425247192383
Epoch 70, training loss: 15.45468807220459 = 1.8312280178070068 + 2.0 * 6.811729907989502
Epoch 70, val loss: 1.827841877937317
Epoch 80, training loss: 15.187421798706055 = 1.8183445930480957 + 2.0 * 6.684538841247559
Epoch 80, val loss: 1.8163787126541138
Epoch 90, training loss: 14.979445457458496 = 1.8064943552017212 + 2.0 * 6.586475372314453
Epoch 90, val loss: 1.8063627481460571
Epoch 100, training loss: 14.827071189880371 = 1.795017123222351 + 2.0 * 6.516026973724365
Epoch 100, val loss: 1.7966816425323486
Epoch 110, training loss: 14.686293601989746 = 1.782796859741211 + 2.0 * 6.451748371124268
Epoch 110, val loss: 1.7866003513336182
Epoch 120, training loss: 14.582376480102539 = 1.7706397771835327 + 2.0 * 6.4058685302734375
Epoch 120, val loss: 1.7766847610473633
Epoch 130, training loss: 14.492523193359375 = 1.7578670978546143 + 2.0 * 6.36732816696167
Epoch 130, val loss: 1.7662211656570435
Epoch 140, training loss: 14.41604232788086 = 1.7441104650497437 + 2.0 * 6.335966110229492
Epoch 140, val loss: 1.7548024654388428
Epoch 150, training loss: 14.347308158874512 = 1.7292755842208862 + 2.0 * 6.309016227722168
Epoch 150, val loss: 1.7425618171691895
Epoch 160, training loss: 14.287519454956055 = 1.7129687070846558 + 2.0 * 6.287275314331055
Epoch 160, val loss: 1.7293462753295898
Epoch 170, training loss: 14.226760864257812 = 1.6949464082717896 + 2.0 * 6.265907287597656
Epoch 170, val loss: 1.7148005962371826
Epoch 180, training loss: 14.174955368041992 = 1.6747373342514038 + 2.0 * 6.2501091957092285
Epoch 180, val loss: 1.6986279487609863
Epoch 190, training loss: 14.119112014770508 = 1.6520177125930786 + 2.0 * 6.233547210693359
Epoch 190, val loss: 1.6805777549743652
Epoch 200, training loss: 14.06684398651123 = 1.6266400814056396 + 2.0 * 6.220101833343506
Epoch 200, val loss: 1.6604732275009155
Epoch 210, training loss: 14.021198272705078 = 1.5983151197433472 + 2.0 * 6.211441516876221
Epoch 210, val loss: 1.6381652355194092
Epoch 220, training loss: 13.965346336364746 = 1.5670133829116821 + 2.0 * 6.199166297912598
Epoch 220, val loss: 1.6136633157730103
Epoch 230, training loss: 13.912156105041504 = 1.5327379703521729 + 2.0 * 6.189709186553955
Epoch 230, val loss: 1.5869706869125366
Epoch 240, training loss: 13.859589576721191 = 1.4954990148544312 + 2.0 * 6.1820454597473145
Epoch 240, val loss: 1.558100700378418
Epoch 250, training loss: 13.805949211120605 = 1.4556522369384766 + 2.0 * 6.1751484870910645
Epoch 250, val loss: 1.52749764919281
Epoch 260, training loss: 13.753273963928223 = 1.4137277603149414 + 2.0 * 6.169773101806641
Epoch 260, val loss: 1.4956867694854736
Epoch 270, training loss: 13.694937705993652 = 1.370160460472107 + 2.0 * 6.162388801574707
Epoch 270, val loss: 1.4629336595535278
Epoch 280, training loss: 13.639934539794922 = 1.3253368139266968 + 2.0 * 6.157299041748047
Epoch 280, val loss: 1.4295744895935059
Epoch 290, training loss: 13.590275764465332 = 1.2798389196395874 + 2.0 * 6.155218601226807
Epoch 290, val loss: 1.396281123161316
Epoch 300, training loss: 13.530794143676758 = 1.2348527908325195 + 2.0 * 6.147970676422119
Epoch 300, val loss: 1.3636902570724487
Epoch 310, training loss: 13.477476119995117 = 1.1904546022415161 + 2.0 * 6.143510818481445
Epoch 310, val loss: 1.3319376707077026
Epoch 320, training loss: 13.434770584106445 = 1.1468383073806763 + 2.0 * 6.143966197967529
Epoch 320, val loss: 1.3009785413742065
Epoch 330, training loss: 13.377388954162598 = 1.1043754816055298 + 2.0 * 6.1365065574646
Epoch 330, val loss: 1.271186351776123
Epoch 340, training loss: 13.328993797302246 = 1.0633798837661743 + 2.0 * 6.132806777954102
Epoch 340, val loss: 1.242557168006897
Epoch 350, training loss: 13.282090187072754 = 1.0235825777053833 + 2.0 * 6.12925386428833
Epoch 350, val loss: 1.2148524522781372
Epoch 360, training loss: 13.236720085144043 = 0.9849967360496521 + 2.0 * 6.125861644744873
Epoch 360, val loss: 1.1881428956985474
Epoch 370, training loss: 13.193060874938965 = 0.9477712512016296 + 2.0 * 6.122644901275635
Epoch 370, val loss: 1.1623940467834473
Epoch 380, training loss: 13.151395797729492 = 0.9115424752235413 + 2.0 * 6.119926452636719
Epoch 380, val loss: 1.1377182006835938
Epoch 390, training loss: 13.111164093017578 = 0.8762230277061462 + 2.0 * 6.117470741271973
Epoch 390, val loss: 1.113708734512329
Epoch 400, training loss: 13.070352554321289 = 0.8417969942092896 + 2.0 * 6.1142778396606445
Epoch 400, val loss: 1.0905143022537231
Epoch 410, training loss: 13.031038284301758 = 0.8079915046691895 + 2.0 * 6.111523151397705
Epoch 410, val loss: 1.0681113004684448
Epoch 420, training loss: 12.998663902282715 = 0.7745586633682251 + 2.0 * 6.1120524406433105
Epoch 420, val loss: 1.046201467514038
Epoch 430, training loss: 12.96326732635498 = 0.7418397068977356 + 2.0 * 6.110713958740234
Epoch 430, val loss: 1.024683952331543
Epoch 440, training loss: 12.92020320892334 = 0.7095797657966614 + 2.0 * 6.105311870574951
Epoch 440, val loss: 1.0040310621261597
Epoch 450, training loss: 12.882753372192383 = 0.6777147054672241 + 2.0 * 6.102519512176514
Epoch 450, val loss: 0.983959972858429
Epoch 460, training loss: 12.84870719909668 = 0.6461129188537598 + 2.0 * 6.101297378540039
Epoch 460, val loss: 0.9641793966293335
Epoch 470, training loss: 12.81836223602295 = 0.6150646209716797 + 2.0 * 6.101648807525635
Epoch 470, val loss: 0.9447866082191467
Epoch 480, training loss: 12.778584480285645 = 0.5846129059791565 + 2.0 * 6.096985816955566
Epoch 480, val loss: 0.9263898134231567
Epoch 490, training loss: 12.750560760498047 = 0.5547900795936584 + 2.0 * 6.0978851318359375
Epoch 490, val loss: 0.9086735844612122
Epoch 500, training loss: 12.71493911743164 = 0.5257628560066223 + 2.0 * 6.094588279724121
Epoch 500, val loss: 0.8919805288314819
Epoch 510, training loss: 12.680871963500977 = 0.49751338362693787 + 2.0 * 6.091679096221924
Epoch 510, val loss: 0.8761163949966431
Epoch 520, training loss: 12.649313926696777 = 0.47008365392684937 + 2.0 * 6.089615345001221
Epoch 520, val loss: 0.8612715601921082
Epoch 530, training loss: 12.626184463500977 = 0.4435442388057709 + 2.0 * 6.091320037841797
Epoch 530, val loss: 0.847246527671814
Epoch 540, training loss: 12.594104766845703 = 0.41809460520744324 + 2.0 * 6.088005065917969
Epoch 540, val loss: 0.8343487977981567
Epoch 550, training loss: 12.56314468383789 = 0.39365053176879883 + 2.0 * 6.084747314453125
Epoch 550, val loss: 0.8225902318954468
Epoch 560, training loss: 12.539261817932129 = 0.3702985346317291 + 2.0 * 6.084481716156006
Epoch 560, val loss: 0.8116927742958069
Epoch 570, training loss: 12.520605087280273 = 0.3481411933898926 + 2.0 * 6.0862321853637695
Epoch 570, val loss: 0.8018527626991272
Epoch 580, training loss: 12.490288734436035 = 0.3271947205066681 + 2.0 * 6.081546783447266
Epoch 580, val loss: 0.792896032333374
Epoch 590, training loss: 12.46455192565918 = 0.30736789107322693 + 2.0 * 6.078591823577881
Epoch 590, val loss: 0.7850302457809448
Epoch 600, training loss: 12.443317413330078 = 0.28865665197372437 + 2.0 * 6.077330589294434
Epoch 600, val loss: 0.778084933757782
Epoch 610, training loss: 12.42185115814209 = 0.2710810899734497 + 2.0 * 6.075385093688965
Epoch 610, val loss: 0.7719731330871582
Epoch 620, training loss: 12.404016494750977 = 0.25474703311920166 + 2.0 * 6.074634552001953
Epoch 620, val loss: 0.7667121887207031
Epoch 630, training loss: 12.385260581970215 = 0.2394774705171585 + 2.0 * 6.072891712188721
Epoch 630, val loss: 0.7625436186790466
Epoch 640, training loss: 12.382406234741211 = 0.2252351939678192 + 2.0 * 6.078585624694824
Epoch 640, val loss: 0.7589675784111023
Epoch 650, training loss: 12.357893943786621 = 0.21207253634929657 + 2.0 * 6.072910785675049
Epoch 650, val loss: 0.7563222646713257
Epoch 660, training loss: 12.339365005493164 = 0.1998540759086609 + 2.0 * 6.069755554199219
Epoch 660, val loss: 0.7546001672744751
Epoch 670, training loss: 12.323091506958008 = 0.18850386142730713 + 2.0 * 6.067293643951416
Epoch 670, val loss: 0.7534732818603516
Epoch 680, training loss: 12.316455841064453 = 0.177960604429245 + 2.0 * 6.069247722625732
Epoch 680, val loss: 0.7529116868972778
Epoch 690, training loss: 12.299092292785645 = 0.16816599667072296 + 2.0 * 6.065463066101074
Epoch 690, val loss: 0.7530304193496704
Epoch 700, training loss: 12.287482261657715 = 0.15911835432052612 + 2.0 * 6.064181804656982
Epoch 700, val loss: 0.7536888122558594
Epoch 710, training loss: 12.278356552124023 = 0.15072907507419586 + 2.0 * 6.06381368637085
Epoch 710, val loss: 0.7549406290054321
Epoch 720, training loss: 12.264880180358887 = 0.1429470330476761 + 2.0 * 6.060966491699219
Epoch 720, val loss: 0.7568331956863403
Epoch 730, training loss: 12.254266738891602 = 0.13565991818904877 + 2.0 * 6.059303283691406
Epoch 730, val loss: 0.7590882778167725
Epoch 740, training loss: 12.25985050201416 = 0.12886415421962738 + 2.0 * 6.065493106842041
Epoch 740, val loss: 0.7615700960159302
Epoch 750, training loss: 12.246828079223633 = 0.12252341955900192 + 2.0 * 6.06215238571167
Epoch 750, val loss: 0.764371395111084
Epoch 760, training loss: 12.232189178466797 = 0.11664973944425583 + 2.0 * 6.057769775390625
Epoch 760, val loss: 0.7677221298217773
Epoch 770, training loss: 12.221007347106934 = 0.11114662885665894 + 2.0 * 6.054930210113525
Epoch 770, val loss: 0.7713454961776733
Epoch 780, training loss: 12.226558685302734 = 0.10599756240844727 + 2.0 * 6.060280799865723
Epoch 780, val loss: 0.7752184271812439
Epoch 790, training loss: 12.220061302185059 = 0.10120449960231781 + 2.0 * 6.0594282150268555
Epoch 790, val loss: 0.7789564728736877
Epoch 800, training loss: 12.202614784240723 = 0.09670975804328918 + 2.0 * 6.052952289581299
Epoch 800, val loss: 0.7833844423294067
Epoch 810, training loss: 12.194605827331543 = 0.09249987453222275 + 2.0 * 6.051053047180176
Epoch 810, val loss: 0.7879153490066528
Epoch 820, training loss: 12.190845489501953 = 0.08853166550397873 + 2.0 * 6.051156997680664
Epoch 820, val loss: 0.7926212549209595
Epoch 830, training loss: 12.182836532592773 = 0.08481550216674805 + 2.0 * 6.049010276794434
Epoch 830, val loss: 0.7973019480705261
Epoch 840, training loss: 12.181628227233887 = 0.08131523430347443 + 2.0 * 6.050156593322754
Epoch 840, val loss: 0.8021857738494873
Epoch 850, training loss: 12.171953201293945 = 0.07802698016166687 + 2.0 * 6.046963214874268
Epoch 850, val loss: 0.8072926998138428
Epoch 860, training loss: 12.16661548614502 = 0.07491496205329895 + 2.0 * 6.0458502769470215
Epoch 860, val loss: 0.8124207258224487
Epoch 870, training loss: 12.173629760742188 = 0.07198355346918106 + 2.0 * 6.050823211669922
Epoch 870, val loss: 0.8176514506340027
Epoch 880, training loss: 12.174717903137207 = 0.06917987018823624 + 2.0 * 6.052769184112549
Epoch 880, val loss: 0.822537899017334
Epoch 890, training loss: 12.158077239990234 = 0.06657914817333221 + 2.0 * 6.045749187469482
Epoch 890, val loss: 0.8279879689216614
Epoch 900, training loss: 12.149528503417969 = 0.06411249190568924 + 2.0 * 6.042707920074463
Epoch 900, val loss: 0.833471953868866
Epoch 910, training loss: 12.14626407623291 = 0.06177356466650963 + 2.0 * 6.042245388031006
Epoch 910, val loss: 0.8388486504554749
Epoch 920, training loss: 12.147598266601562 = 0.05955372005701065 + 2.0 * 6.044022083282471
Epoch 920, val loss: 0.8441994786262512
Epoch 930, training loss: 12.137600898742676 = 0.05744287744164467 + 2.0 * 6.040079116821289
Epoch 930, val loss: 0.8497166633605957
Epoch 940, training loss: 12.136480331420898 = 0.055438779294490814 + 2.0 * 6.040520668029785
Epoch 940, val loss: 0.8553789258003235
Epoch 950, training loss: 12.142297744750977 = 0.053546082228422165 + 2.0 * 6.044375896453857
Epoch 950, val loss: 0.8607543110847473
Epoch 960, training loss: 12.130022048950195 = 0.05172865465283394 + 2.0 * 6.039146900177002
Epoch 960, val loss: 0.8662077188491821
Epoch 970, training loss: 12.130717277526855 = 0.0500137023627758 + 2.0 * 6.040351867675781
Epoch 970, val loss: 0.8718727231025696
Epoch 980, training loss: 12.121984481811523 = 0.0483761727809906 + 2.0 * 6.03680419921875
Epoch 980, val loss: 0.8771509528160095
Epoch 990, training loss: 12.118890762329102 = 0.04682010039687157 + 2.0 * 6.036035537719727
Epoch 990, val loss: 0.8827421069145203
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7963
Overall ASR: 0.7122
Flip ASR: 0.6622/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.681964874267578 = 1.9341535568237305 + 2.0 * 8.373905181884766
Epoch 0, val loss: 1.9346247911453247
Epoch 10, training loss: 18.671178817749023 = 1.924139380455017 + 2.0 * 8.373519897460938
Epoch 10, val loss: 1.9238137006759644
Epoch 20, training loss: 18.653011322021484 = 1.9114547967910767 + 2.0 * 8.37077808380127
Epoch 20, val loss: 1.9099647998809814
Epoch 30, training loss: 18.597549438476562 = 1.8938930034637451 + 2.0 * 8.351828575134277
Epoch 30, val loss: 1.8906307220458984
Epoch 40, training loss: 18.33294677734375 = 1.8731729984283447 + 2.0 * 8.229887008666992
Epoch 40, val loss: 1.8687760829925537
Epoch 50, training loss: 17.541584014892578 = 1.8520170450210571 + 2.0 * 7.844783782958984
Epoch 50, val loss: 1.8469796180725098
Epoch 60, training loss: 16.67122459411621 = 1.8350310325622559 + 2.0 * 7.418097019195557
Epoch 60, val loss: 1.831502914428711
Epoch 70, training loss: 15.775638580322266 = 1.8224291801452637 + 2.0 * 6.97660493850708
Epoch 70, val loss: 1.8204196691513062
Epoch 80, training loss: 15.294539451599121 = 1.8119820356369019 + 2.0 * 6.741278648376465
Epoch 80, val loss: 1.8111720085144043
Epoch 90, training loss: 15.028360366821289 = 1.7994215488433838 + 2.0 * 6.614469528198242
Epoch 90, val loss: 1.7995600700378418
Epoch 100, training loss: 14.847034454345703 = 1.7858041524887085 + 2.0 * 6.530615329742432
Epoch 100, val loss: 1.7870680093765259
Epoch 110, training loss: 14.717037200927734 = 1.7722572088241577 + 2.0 * 6.472390174865723
Epoch 110, val loss: 1.774946689605713
Epoch 120, training loss: 14.610461235046387 = 1.758183479309082 + 2.0 * 6.426138877868652
Epoch 120, val loss: 1.7624351978302002
Epoch 130, training loss: 14.518692970275879 = 1.742984414100647 + 2.0 * 6.387854099273682
Epoch 130, val loss: 1.7490746974945068
Epoch 140, training loss: 14.437884330749512 = 1.7262369394302368 + 2.0 * 6.355823516845703
Epoch 140, val loss: 1.734518051147461
Epoch 150, training loss: 14.369373321533203 = 1.7073029279708862 + 2.0 * 6.331035137176514
Epoch 150, val loss: 1.7183451652526855
Epoch 160, training loss: 14.297508239746094 = 1.685909628868103 + 2.0 * 6.30579948425293
Epoch 160, val loss: 1.7001278400421143
Epoch 170, training loss: 14.230679512023926 = 1.6616284847259521 + 2.0 * 6.284525394439697
Epoch 170, val loss: 1.6795722246170044
Epoch 180, training loss: 14.179046630859375 = 1.6339058876037598 + 2.0 * 6.2725701332092285
Epoch 180, val loss: 1.6561779975891113
Epoch 190, training loss: 14.10746955871582 = 1.6028826236724854 + 2.0 * 6.252293586730957
Epoch 190, val loss: 1.6301616430282593
Epoch 200, training loss: 14.043176651000977 = 1.5682154893875122 + 2.0 * 6.237480640411377
Epoch 200, val loss: 1.601134181022644
Epoch 210, training loss: 13.980746269226074 = 1.5297515392303467 + 2.0 * 6.225497245788574
Epoch 210, val loss: 1.5692040920257568
Epoch 220, training loss: 13.925581932067871 = 1.4875158071517944 + 2.0 * 6.219033241271973
Epoch 220, val loss: 1.534546971321106
Epoch 230, training loss: 13.853548049926758 = 1.442717432975769 + 2.0 * 6.20541524887085
Epoch 230, val loss: 1.4979008436203003
Epoch 240, training loss: 13.790349006652832 = 1.3956247568130493 + 2.0 * 6.197361946105957
Epoch 240, val loss: 1.4599969387054443
Epoch 250, training loss: 13.726633071899414 = 1.347335934638977 + 2.0 * 6.189648628234863
Epoch 250, val loss: 1.4215824604034424
Epoch 260, training loss: 13.661157608032227 = 1.298686146736145 + 2.0 * 6.1812357902526855
Epoch 260, val loss: 1.3835099935531616
Epoch 270, training loss: 13.605071067810059 = 1.2504730224609375 + 2.0 * 6.1772990226745605
Epoch 270, val loss: 1.346406102180481
Epoch 280, training loss: 13.541234970092773 = 1.2038862705230713 + 2.0 * 6.168674468994141
Epoch 280, val loss: 1.3109970092773438
Epoch 290, training loss: 13.486328125 = 1.1591763496398926 + 2.0 * 6.163576126098633
Epoch 290, val loss: 1.2775212526321411
Epoch 300, training loss: 13.431402206420898 = 1.116611123085022 + 2.0 * 6.157395362854004
Epoch 300, val loss: 1.2461488246917725
Epoch 310, training loss: 13.389175415039062 = 1.0760865211486816 + 2.0 * 6.1565446853637695
Epoch 310, val loss: 1.2166632413864136
Epoch 320, training loss: 13.335882186889648 = 1.0379328727722168 + 2.0 * 6.148974418640137
Epoch 320, val loss: 1.189334511756897
Epoch 330, training loss: 13.287126541137695 = 1.0016392469406128 + 2.0 * 6.1427435874938965
Epoch 330, val loss: 1.1635066270828247
Epoch 340, training loss: 13.245096206665039 = 0.9667561650276184 + 2.0 * 6.139170169830322
Epoch 340, val loss: 1.138889193534851
Epoch 350, training loss: 13.205699920654297 = 0.9332337379455566 + 2.0 * 6.136232852935791
Epoch 350, val loss: 1.1153455972671509
Epoch 360, training loss: 13.172136306762695 = 0.9007236957550049 + 2.0 * 6.135706424713135
Epoch 360, val loss: 1.0927467346191406
Epoch 370, training loss: 13.123772621154785 = 0.8693443536758423 + 2.0 * 6.127213954925537
Epoch 370, val loss: 1.0706748962402344
Epoch 380, training loss: 13.085053443908691 = 0.8384377360343933 + 2.0 * 6.123307704925537
Epoch 380, val loss: 1.0491153001785278
Epoch 390, training loss: 13.048073768615723 = 0.8079105615615845 + 2.0 * 6.120081424713135
Epoch 390, val loss: 1.027924656867981
Epoch 400, training loss: 13.017253875732422 = 0.7778909206390381 + 2.0 * 6.119681358337402
Epoch 400, val loss: 1.0071537494659424
Epoch 410, training loss: 12.977313041687012 = 0.7485262155532837 + 2.0 * 6.11439323425293
Epoch 410, val loss: 0.9871560335159302
Epoch 420, training loss: 12.942342758178711 = 0.7197808027267456 + 2.0 * 6.111280918121338
Epoch 420, val loss: 0.9679845571517944
Epoch 430, training loss: 12.912758827209473 = 0.6915619969367981 + 2.0 * 6.110598564147949
Epoch 430, val loss: 0.9496558308601379
Epoch 440, training loss: 12.880343437194824 = 0.6641647219657898 + 2.0 * 6.108089447021484
Epoch 440, val loss: 0.9322155714035034
Epoch 450, training loss: 12.844343185424805 = 0.6374405026435852 + 2.0 * 6.103451251983643
Epoch 450, val loss: 0.916037380695343
Epoch 460, training loss: 12.823942184448242 = 0.6115939021110535 + 2.0 * 6.106173992156982
Epoch 460, val loss: 0.9010266065597534
Epoch 470, training loss: 12.784649848937988 = 0.5865572690963745 + 2.0 * 6.099046230316162
Epoch 470, val loss: 0.8872881531715393
Epoch 480, training loss: 12.762682914733887 = 0.5623468160629272 + 2.0 * 6.100168228149414
Epoch 480, val loss: 0.8747443556785583
Epoch 490, training loss: 12.732951164245605 = 0.5389940738677979 + 2.0 * 6.096978664398193
Epoch 490, val loss: 0.8633567094802856
Epoch 500, training loss: 12.703038215637207 = 0.5163027048110962 + 2.0 * 6.093367576599121
Epoch 500, val loss: 0.8530962467193604
Epoch 510, training loss: 12.675148963928223 = 0.4942482113838196 + 2.0 * 6.090450286865234
Epoch 510, val loss: 0.8437957167625427
Epoch 520, training loss: 12.667411804199219 = 0.4727287292480469 + 2.0 * 6.097341537475586
Epoch 520, val loss: 0.8353988528251648
Epoch 530, training loss: 12.626241683959961 = 0.45192787051200867 + 2.0 * 6.087156772613525
Epoch 530, val loss: 0.8279352188110352
Epoch 540, training loss: 12.608242988586426 = 0.4316551983356476 + 2.0 * 6.08829402923584
Epoch 540, val loss: 0.8211632370948792
Epoch 550, training loss: 12.581422805786133 = 0.4118421673774719 + 2.0 * 6.084790229797363
Epoch 550, val loss: 0.815115749835968
Epoch 560, training loss: 12.558090209960938 = 0.39260149002075195 + 2.0 * 6.082744121551514
Epoch 560, val loss: 0.8097586035728455
Epoch 570, training loss: 12.536737442016602 = 0.37384700775146484 + 2.0 * 6.081445217132568
Epoch 570, val loss: 0.8050879240036011
Epoch 580, training loss: 12.52093505859375 = 0.35573846101760864 + 2.0 * 6.0825982093811035
Epoch 580, val loss: 0.8009551167488098
Epoch 590, training loss: 12.493916511535645 = 0.3382018506526947 + 2.0 * 6.077857494354248
Epoch 590, val loss: 0.797533392906189
Epoch 600, training loss: 12.472417831420898 = 0.3213476240634918 + 2.0 * 6.075535297393799
Epoch 600, val loss: 0.7946811318397522
Epoch 610, training loss: 12.457070350646973 = 0.305146723985672 + 2.0 * 6.075961589813232
Epoch 610, val loss: 0.7924617528915405
Epoch 620, training loss: 12.443719863891602 = 0.2895989716053009 + 2.0 * 6.077060222625732
Epoch 620, val loss: 0.7909148335456848
Epoch 630, training loss: 12.423998832702637 = 0.2748926877975464 + 2.0 * 6.0745530128479
Epoch 630, val loss: 0.7899794578552246
Epoch 640, training loss: 12.403579711914062 = 0.2608664929866791 + 2.0 * 6.071356773376465
Epoch 640, val loss: 0.7896201014518738
Epoch 650, training loss: 12.385446548461914 = 0.247584730386734 + 2.0 * 6.0689311027526855
Epoch 650, val loss: 0.78982013463974
Epoch 660, training loss: 12.380851745605469 = 0.23501178622245789 + 2.0 * 6.072919845581055
Epoch 660, val loss: 0.7905468344688416
Epoch 670, training loss: 12.36052417755127 = 0.22315089404582977 + 2.0 * 6.068686485290527
Epoch 670, val loss: 0.7918728590011597
Epoch 680, training loss: 12.347394943237305 = 0.21196745336055756 + 2.0 * 6.067713737487793
Epoch 680, val loss: 0.7936901450157166
Epoch 690, training loss: 12.332010269165039 = 0.20146791636943817 + 2.0 * 6.065271377563477
Epoch 690, val loss: 0.7960326671600342
Epoch 700, training loss: 12.319501876831055 = 0.19157777726650238 + 2.0 * 6.063961982727051
Epoch 700, val loss: 0.7987730503082275
Epoch 710, training loss: 12.311014175415039 = 0.18230271339416504 + 2.0 * 6.064355850219727
Epoch 710, val loss: 0.8019289374351501
Epoch 720, training loss: 12.297528266906738 = 0.17363759875297546 + 2.0 * 6.06194543838501
Epoch 720, val loss: 0.8054183721542358
Epoch 730, training loss: 12.293996810913086 = 0.165482759475708 + 2.0 * 6.0642571449279785
Epoch 730, val loss: 0.8092533349990845
Epoch 740, training loss: 12.281418800354004 = 0.1578987091779709 + 2.0 * 6.061759948730469
Epoch 740, val loss: 0.8134384155273438
Epoch 750, training loss: 12.26721477508545 = 0.15079082548618317 + 2.0 * 6.058211803436279
Epoch 750, val loss: 0.8178636431694031
Epoch 760, training loss: 12.258478164672852 = 0.14412498474121094 + 2.0 * 6.05717658996582
Epoch 760, val loss: 0.8225361704826355
Epoch 770, training loss: 12.248161315917969 = 0.13789275288581848 + 2.0 * 6.055134296417236
Epoch 770, val loss: 0.827552318572998
Epoch 780, training loss: 12.251173973083496 = 0.13203835487365723 + 2.0 * 6.059567928314209
Epoch 780, val loss: 0.8328080773353577
Epoch 790, training loss: 12.239252090454102 = 0.12652254104614258 + 2.0 * 6.056365013122559
Epoch 790, val loss: 0.8379806280136108
Epoch 800, training loss: 12.226130485534668 = 0.12137065827846527 + 2.0 * 6.052380084991455
Epoch 800, val loss: 0.8435379266738892
Epoch 810, training loss: 12.217555046081543 = 0.11650725454092026 + 2.0 * 6.05052375793457
Epoch 810, val loss: 0.8492500185966492
Epoch 820, training loss: 12.218090057373047 = 0.11190178990364075 + 2.0 * 6.053093910217285
Epoch 820, val loss: 0.8550481796264648
Epoch 830, training loss: 12.214716911315918 = 0.10759337246417999 + 2.0 * 6.053561687469482
Epoch 830, val loss: 0.8610442280769348
Epoch 840, training loss: 12.199673652648926 = 0.1034722775220871 + 2.0 * 6.048100471496582
Epoch 840, val loss: 0.8670133948326111
Epoch 850, training loss: 12.19545841217041 = 0.09957965463399887 + 2.0 * 6.047939300537109
Epoch 850, val loss: 0.8731014132499695
Epoch 860, training loss: 12.1942720413208 = 0.09588589519262314 + 2.0 * 6.049192905426025
Epoch 860, val loss: 0.8794048428535461
Epoch 870, training loss: 12.195476531982422 = 0.09236908704042435 + 2.0 * 6.051553726196289
Epoch 870, val loss: 0.8855754137039185
Epoch 880, training loss: 12.181468963623047 = 0.08901263028383255 + 2.0 * 6.046227931976318
Epoch 880, val loss: 0.8917887210845947
Epoch 890, training loss: 12.173005104064941 = 0.0858050286769867 + 2.0 * 6.043600082397461
Epoch 890, val loss: 0.8980758190155029
Epoch 900, training loss: 12.168658256530762 = 0.08274038881063461 + 2.0 * 6.042958736419678
Epoch 900, val loss: 0.9044440984725952
Epoch 910, training loss: 12.178091049194336 = 0.07980106770992279 + 2.0 * 6.049145221710205
Epoch 910, val loss: 0.9108579158782959
Epoch 920, training loss: 12.168600082397461 = 0.07699806988239288 + 2.0 * 6.045801162719727
Epoch 920, val loss: 0.9170805811882019
Epoch 930, training loss: 12.158644676208496 = 0.07431672513484955 + 2.0 * 6.042163848876953
Epoch 930, val loss: 0.9234432578086853
Epoch 940, training loss: 12.150431632995605 = 0.07174143940210342 + 2.0 * 6.0393452644348145
Epoch 940, val loss: 0.9298357367515564
Epoch 950, training loss: 12.146885871887207 = 0.06926277279853821 + 2.0 * 6.038811683654785
Epoch 950, val loss: 0.9362574219703674
Epoch 960, training loss: 12.149584770202637 = 0.0668688416481018 + 2.0 * 6.04135799407959
Epoch 960, val loss: 0.9426711797714233
Epoch 970, training loss: 12.144757270812988 = 0.06456281244754791 + 2.0 * 6.040097236633301
Epoch 970, val loss: 0.9490216374397278
Epoch 980, training loss: 12.140349388122559 = 0.062348514795303345 + 2.0 * 6.039000511169434
Epoch 980, val loss: 0.9553870558738708
Epoch 990, training loss: 12.137269020080566 = 0.060217421501874924 + 2.0 * 6.038525581359863
Epoch 990, val loss: 0.9617358446121216
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7778
Overall ASR: 0.8155
Flip ASR: 0.7822/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.679786682128906 = 1.9319838285446167 + 2.0 * 8.3739013671875
Epoch 0, val loss: 1.9221831560134888
Epoch 10, training loss: 18.668851852416992 = 1.921856164932251 + 2.0 * 8.37349796295166
Epoch 10, val loss: 1.9123731851577759
Epoch 20, training loss: 18.650665283203125 = 1.9090012311935425 + 2.0 * 8.370832443237305
Epoch 20, val loss: 1.899601936340332
Epoch 30, training loss: 18.596172332763672 = 1.8912343978881836 + 2.0 * 8.352469444274902
Epoch 30, val loss: 1.8817622661590576
Epoch 40, training loss: 18.337934494018555 = 1.869613766670227 + 2.0 * 8.234160423278809
Epoch 40, val loss: 1.8610566854476929
Epoch 50, training loss: 17.270633697509766 = 1.8469352722167969 + 2.0 * 7.711849212646484
Epoch 50, val loss: 1.839648723602295
Epoch 60, training loss: 16.428539276123047 = 1.8275278806686401 + 2.0 * 7.300506114959717
Epoch 60, val loss: 1.8222463130950928
Epoch 70, training loss: 15.673052787780762 = 1.8157800436019897 + 2.0 * 6.92863655090332
Epoch 70, val loss: 1.8115863800048828
Epoch 80, training loss: 15.292664527893066 = 1.80459463596344 + 2.0 * 6.744034767150879
Epoch 80, val loss: 1.8016207218170166
Epoch 90, training loss: 15.042500495910645 = 1.7918449640274048 + 2.0 * 6.6253275871276855
Epoch 90, val loss: 1.7902328968048096
Epoch 100, training loss: 14.860296249389648 = 1.778918743133545 + 2.0 * 6.540688991546631
Epoch 100, val loss: 1.7787036895751953
Epoch 110, training loss: 14.726021766662598 = 1.7671339511871338 + 2.0 * 6.4794440269470215
Epoch 110, val loss: 1.7683058977127075
Epoch 120, training loss: 14.614175796508789 = 1.7549617290496826 + 2.0 * 6.429606914520264
Epoch 120, val loss: 1.7576565742492676
Epoch 130, training loss: 14.518206596374512 = 1.741188406944275 + 2.0 * 6.388509273529053
Epoch 130, val loss: 1.7458680868148804
Epoch 140, training loss: 14.430171012878418 = 1.725692629814148 + 2.0 * 6.35223913192749
Epoch 140, val loss: 1.7327836751937866
Epoch 150, training loss: 14.35161018371582 = 1.7084920406341553 + 2.0 * 6.321558952331543
Epoch 150, val loss: 1.7184182405471802
Epoch 160, training loss: 14.281078338623047 = 1.6889275312423706 + 2.0 * 6.296075344085693
Epoch 160, val loss: 1.7021600008010864
Epoch 170, training loss: 14.219717979431152 = 1.666298508644104 + 2.0 * 6.27670955657959
Epoch 170, val loss: 1.6834162473678589
Epoch 180, training loss: 14.16059684753418 = 1.64039945602417 + 2.0 * 6.260098457336426
Epoch 180, val loss: 1.6621170043945312
Epoch 190, training loss: 14.097464561462402 = 1.6109954118728638 + 2.0 * 6.243234634399414
Epoch 190, val loss: 1.6379393339157104
Epoch 200, training loss: 14.03803825378418 = 1.5777554512023926 + 2.0 * 6.230141639709473
Epoch 200, val loss: 1.6106302738189697
Epoch 210, training loss: 13.986655235290527 = 1.540331482887268 + 2.0 * 6.223161697387695
Epoch 210, val loss: 1.5800116062164307
Epoch 220, training loss: 13.920548439025879 = 1.4995248317718506 + 2.0 * 6.210511684417725
Epoch 220, val loss: 1.5467292070388794
Epoch 230, training loss: 13.856149673461914 = 1.4552971124649048 + 2.0 * 6.20042610168457
Epoch 230, val loss: 1.5108397006988525
Epoch 240, training loss: 13.791727066040039 = 1.407930612564087 + 2.0 * 6.191898345947266
Epoch 240, val loss: 1.4726969003677368
Epoch 250, training loss: 13.736797332763672 = 1.3580732345581055 + 2.0 * 6.189362049102783
Epoch 250, val loss: 1.4328789710998535
Epoch 260, training loss: 13.664275169372559 = 1.3070648908615112 + 2.0 * 6.178605079650879
Epoch 260, val loss: 1.3927175998687744
Epoch 270, training loss: 13.598544120788574 = 1.25546133518219 + 2.0 * 6.171541213989258
Epoch 270, val loss: 1.3525415658950806
Epoch 280, training loss: 13.545682907104492 = 1.2036497592926025 + 2.0 * 6.171016693115234
Epoch 280, val loss: 1.312780737876892
Epoch 290, training loss: 13.47609806060791 = 1.1529287099838257 + 2.0 * 6.161584854125977
Epoch 290, val loss: 1.2742762565612793
Epoch 300, training loss: 13.413278579711914 = 1.1031053066253662 + 2.0 * 6.155086517333984
Epoch 300, val loss: 1.2371209859848022
Epoch 310, training loss: 13.358489990234375 = 1.0543030500411987 + 2.0 * 6.152093410491943
Epoch 310, val loss: 1.2011693716049194
Epoch 320, training loss: 13.303236961364746 = 1.006966471672058 + 2.0 * 6.148135185241699
Epoch 320, val loss: 1.1669921875
Epoch 330, training loss: 13.246651649475098 = 0.9613245129585266 + 2.0 * 6.142663478851318
Epoch 330, val loss: 1.1347287893295288
Epoch 340, training loss: 13.193947792053223 = 0.917310893535614 + 2.0 * 6.1383185386657715
Epoch 340, val loss: 1.104142427444458
Epoch 350, training loss: 13.156285285949707 = 0.875002920627594 + 2.0 * 6.140641212463379
Epoch 350, val loss: 1.0753364562988281
Epoch 360, training loss: 13.096982955932617 = 0.8346868753433228 + 2.0 * 6.131147861480713
Epoch 360, val loss: 1.0488429069519043
Epoch 370, training loss: 13.053067207336426 = 0.7963406443595886 + 2.0 * 6.128363132476807
Epoch 370, val loss: 1.0244990587234497
Epoch 380, training loss: 13.012649536132812 = 0.7597556114196777 + 2.0 * 6.1264472007751465
Epoch 380, val loss: 1.0020453929901123
Epoch 390, training loss: 12.97440242767334 = 0.7251223921775818 + 2.0 * 6.124639987945557
Epoch 390, val loss: 0.9814780950546265
Epoch 400, training loss: 12.931014060974121 = 0.692267119884491 + 2.0 * 6.119373321533203
Epoch 400, val loss: 0.9629815220832825
Epoch 410, training loss: 12.893739700317383 = 0.660836398601532 + 2.0 * 6.116451740264893
Epoch 410, val loss: 0.9460065960884094
Epoch 420, training loss: 12.859833717346191 = 0.6305941343307495 + 2.0 * 6.114619731903076
Epoch 420, val loss: 0.9302547574043274
Epoch 430, training loss: 12.836821556091309 = 0.6018036007881165 + 2.0 * 6.117508888244629
Epoch 430, val loss: 0.9158493876457214
Epoch 440, training loss: 12.794754981994629 = 0.5743506550788879 + 2.0 * 6.110202312469482
Epoch 440, val loss: 0.9026431441307068
Epoch 450, training loss: 12.765289306640625 = 0.5480399131774902 + 2.0 * 6.1086249351501465
Epoch 450, val loss: 0.8903750777244568
Epoch 460, training loss: 12.734928131103516 = 0.522766649723053 + 2.0 * 6.106080532073975
Epoch 460, val loss: 0.879166841506958
Epoch 470, training loss: 12.703802108764648 = 0.4985014498233795 + 2.0 * 6.102650165557861
Epoch 470, val loss: 0.8685982823371887
Epoch 480, training loss: 12.67634391784668 = 0.4750807285308838 + 2.0 * 6.1006317138671875
Epoch 480, val loss: 0.8587709069252014
Epoch 490, training loss: 12.657991409301758 = 0.4525476098060608 + 2.0 * 6.102721691131592
Epoch 490, val loss: 0.8494699597358704
Epoch 500, training loss: 12.62739372253418 = 0.43095412850379944 + 2.0 * 6.098219871520996
Epoch 500, val loss: 0.8410642147064209
Epoch 510, training loss: 12.600592613220215 = 0.4101823568344116 + 2.0 * 6.095205307006836
Epoch 510, val loss: 0.8333439230918884
Epoch 520, training loss: 12.58105754852295 = 0.39010196924209595 + 2.0 * 6.09547758102417
Epoch 520, val loss: 0.8261949419975281
Epoch 530, training loss: 12.55700397491455 = 0.37083113193511963 + 2.0 * 6.093086242675781
Epoch 530, val loss: 0.819656491279602
Epoch 540, training loss: 12.539909362792969 = 0.35220831632614136 + 2.0 * 6.093850612640381
Epoch 540, val loss: 0.8138986229896545
Epoch 550, training loss: 12.511098861694336 = 0.3343924880027771 + 2.0 * 6.088353157043457
Epoch 550, val loss: 0.8086015582084656
Epoch 560, training loss: 12.488615036010742 = 0.3171461224555969 + 2.0 * 6.0857343673706055
Epoch 560, val loss: 0.8038880228996277
Epoch 570, training loss: 12.46923828125 = 0.30046913027763367 + 2.0 * 6.084384441375732
Epoch 570, val loss: 0.799731969833374
Epoch 580, training loss: 12.469419479370117 = 0.28434374928474426 + 2.0 * 6.092537879943848
Epoch 580, val loss: 0.7959949970245361
Epoch 590, training loss: 12.436802864074707 = 0.2689453065395355 + 2.0 * 6.08392858505249
Epoch 590, val loss: 0.7929388284683228
Epoch 600, training loss: 12.41443157196045 = 0.254206120967865 + 2.0 * 6.080112934112549
Epoch 600, val loss: 0.7902640700340271
Epoch 610, training loss: 12.396085739135742 = 0.24006327986717224 + 2.0 * 6.0780110359191895
Epoch 610, val loss: 0.7881918549537659
Epoch 620, training loss: 12.380131721496582 = 0.22649477422237396 + 2.0 * 6.076818466186523
Epoch 620, val loss: 0.7868514060974121
Epoch 630, training loss: 12.374002456665039 = 0.21357442438602448 + 2.0 * 6.080214023590088
Epoch 630, val loss: 0.7858428359031677
Epoch 640, training loss: 12.353859901428223 = 0.20142734050750732 + 2.0 * 6.076216220855713
Epoch 640, val loss: 0.7856960892677307
Epoch 650, training loss: 12.3359375 = 0.1899476945400238 + 2.0 * 6.072994709014893
Epoch 650, val loss: 0.7860962152481079
Epoch 660, training loss: 12.322342872619629 = 0.17908933758735657 + 2.0 * 6.071626663208008
Epoch 660, val loss: 0.7870067358016968
Epoch 670, training loss: 12.320219993591309 = 0.16882863640785217 + 2.0 * 6.075695514678955
Epoch 670, val loss: 0.7885531187057495
Epoch 680, training loss: 12.302051544189453 = 0.1592278778553009 + 2.0 * 6.071411609649658
Epoch 680, val loss: 0.7906007766723633
Epoch 690, training loss: 12.286965370178223 = 0.15024434030056 + 2.0 * 6.068360328674316
Epoch 690, val loss: 0.7930711507797241
Epoch 700, training loss: 12.28504753112793 = 0.14180800318717957 + 2.0 * 6.071619987487793
Epoch 700, val loss: 0.796055257320404
Epoch 710, training loss: 12.270486831665039 = 0.13395388424396515 + 2.0 * 6.06826639175415
Epoch 710, val loss: 0.7996730804443359
Epoch 720, training loss: 12.254814147949219 = 0.1266215294599533 + 2.0 * 6.064096450805664
Epoch 720, val loss: 0.8036008477210999
Epoch 730, training loss: 12.251696586608887 = 0.11977973580360413 + 2.0 * 6.065958499908447
Epoch 730, val loss: 0.8079010844230652
Epoch 740, training loss: 12.240006446838379 = 0.11336904764175415 + 2.0 * 6.063318729400635
Epoch 740, val loss: 0.8125672340393066
Epoch 750, training loss: 12.23270034790039 = 0.10742856562137604 + 2.0 * 6.062635898590088
Epoch 750, val loss: 0.8178001642227173
Epoch 760, training loss: 12.222792625427246 = 0.10189040005207062 + 2.0 * 6.060451030731201
Epoch 760, val loss: 0.8230133056640625
Epoch 770, training loss: 12.214713096618652 = 0.09671027213335037 + 2.0 * 6.059001445770264
Epoch 770, val loss: 0.8287902474403381
Epoch 780, training loss: 12.223591804504395 = 0.09185662865638733 + 2.0 * 6.0658674240112305
Epoch 780, val loss: 0.8347393870353699
Epoch 790, training loss: 12.207328796386719 = 0.0873761922121048 + 2.0 * 6.059976100921631
Epoch 790, val loss: 0.8405924439430237
Epoch 800, training loss: 12.19681167602539 = 0.0831628069281578 + 2.0 * 6.056824207305908
Epoch 800, val loss: 0.8468393683433533
Epoch 810, training loss: 12.188159942626953 = 0.07923051714897156 + 2.0 * 6.054464817047119
Epoch 810, val loss: 0.8532058000564575
Epoch 820, training loss: 12.183053970336914 = 0.07553289830684662 + 2.0 * 6.053760528564453
Epoch 820, val loss: 0.8597521781921387
Epoch 830, training loss: 12.20165729522705 = 0.07205972075462341 + 2.0 * 6.064798831939697
Epoch 830, val loss: 0.8664411306381226
Epoch 840, training loss: 12.174249649047852 = 0.06884418427944183 + 2.0 * 6.052702903747559
Epoch 840, val loss: 0.8730136156082153
Epoch 850, training loss: 12.169548034667969 = 0.0658225268125534 + 2.0 * 6.051862716674805
Epoch 850, val loss: 0.87987220287323
Epoch 860, training loss: 12.17402172088623 = 0.0629882961511612 + 2.0 * 6.055516719818115
Epoch 860, val loss: 0.8867944478988647
Epoch 870, training loss: 12.16238784790039 = 0.06032683700323105 + 2.0 * 6.05103063583374
Epoch 870, val loss: 0.8937643766403198
Epoch 880, training loss: 12.157896041870117 = 0.05782092362642288 + 2.0 * 6.050037384033203
Epoch 880, val loss: 0.9006832838058472
Epoch 890, training loss: 12.150680541992188 = 0.055462226271629333 + 2.0 * 6.047609329223633
Epoch 890, val loss: 0.9076035618782043
Epoch 900, training loss: 12.147109031677246 = 0.053232211619615555 + 2.0 * 6.046938419342041
Epoch 900, val loss: 0.914474606513977
Epoch 910, training loss: 12.159001350402832 = 0.05113517493009567 + 2.0 * 6.053933143615723
Epoch 910, val loss: 0.9212337136268616
Epoch 920, training loss: 12.139734268188477 = 0.04915754869580269 + 2.0 * 6.045288562774658
Epoch 920, val loss: 0.9283764958381653
Epoch 930, training loss: 12.138121604919434 = 0.047293320298194885 + 2.0 * 6.045413970947266
Epoch 930, val loss: 0.9353976845741272
Epoch 940, training loss: 12.1373872756958 = 0.04553388059139252 + 2.0 * 6.045926570892334
Epoch 940, val loss: 0.9421266317367554
Epoch 950, training loss: 12.1310453414917 = 0.04385857284069061 + 2.0 * 6.043593406677246
Epoch 950, val loss: 0.9490050077438354
Epoch 960, training loss: 12.130812644958496 = 0.0422915555536747 + 2.0 * 6.044260501861572
Epoch 960, val loss: 0.9556212425231934
Epoch 970, training loss: 12.12388801574707 = 0.0407910980284214 + 2.0 * 6.041548252105713
Epoch 970, val loss: 0.9624040126800537
Epoch 980, training loss: 12.120597839355469 = 0.03936830908060074 + 2.0 * 6.040614604949951
Epoch 980, val loss: 0.9691298604011536
Epoch 990, training loss: 12.121496200561523 = 0.0380161814391613 + 2.0 * 6.0417399406433105
Epoch 990, val loss: 0.9759111404418945
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8074
Overall ASR: 0.8524
Flip ASR: 0.8222/225 nodes
The final ASR:0.79336, 0.05935, Accuracy:0.79383, 0.01222
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11610])
remove edge: torch.Size([2, 9492])
updated graph: torch.Size([2, 10546])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
The final ASR:0.97663, 0.00174, Accuracy:0.83580, 0.00698
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.5 der2:0.2 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 18.71849250793457 = 1.9707220792770386 + 2.0 * 8.373885154724121
Epoch 0, val loss: 1.9640295505523682
Epoch 10, training loss: 18.706119537353516 = 1.9591240882873535 + 2.0 * 8.37349796295166
Epoch 10, val loss: 1.952845573425293
Epoch 20, training loss: 18.68662452697754 = 1.9450185298919678 + 2.0 * 8.370802879333496
Epoch 20, val loss: 1.9388089179992676
Epoch 30, training loss: 18.626461029052734 = 1.925807237625122 + 2.0 * 8.350326538085938
Epoch 30, val loss: 1.919465184211731
Epoch 40, training loss: 18.27311134338379 = 1.9025293588638306 + 2.0 * 8.185291290283203
Epoch 40, val loss: 1.8969320058822632
Epoch 50, training loss: 17.119659423828125 = 1.8776270151138306 + 2.0 * 7.621016025543213
Epoch 50, val loss: 1.873003363609314
Epoch 60, training loss: 16.24722671508789 = 1.8601179122924805 + 2.0 * 7.193554878234863
Epoch 60, val loss: 1.857885718345642
Epoch 70, training loss: 15.565842628479004 = 1.8476378917694092 + 2.0 * 6.859102249145508
Epoch 70, val loss: 1.8470373153686523
Epoch 80, training loss: 15.197604179382324 = 1.8349974155426025 + 2.0 * 6.68130350112915
Epoch 80, val loss: 1.8359918594360352
Epoch 90, training loss: 14.954647064208984 = 1.8205844163894653 + 2.0 * 6.567031383514404
Epoch 90, val loss: 1.8233020305633545
Epoch 100, training loss: 14.779012680053711 = 1.806944489479065 + 2.0 * 6.486033916473389
Epoch 100, val loss: 1.8116544485092163
Epoch 110, training loss: 14.649703025817871 = 1.794926404953003 + 2.0 * 6.4273881912231445
Epoch 110, val loss: 1.8017407655715942
Epoch 120, training loss: 14.544474601745605 = 1.7836488485336304 + 2.0 * 6.380413055419922
Epoch 120, val loss: 1.792266607284546
Epoch 130, training loss: 14.460761070251465 = 1.7720584869384766 + 2.0 * 6.344351291656494
Epoch 130, val loss: 1.7823901176452637
Epoch 140, training loss: 14.387801170349121 = 1.7597485780715942 + 2.0 * 6.314026355743408
Epoch 140, val loss: 1.7718892097473145
Epoch 150, training loss: 14.328365325927734 = 1.7462661266326904 + 2.0 * 6.291049480438232
Epoch 150, val loss: 1.7605745792388916
Epoch 160, training loss: 14.270479202270508 = 1.731343388557434 + 2.0 * 6.269567966461182
Epoch 160, val loss: 1.7482103109359741
Epoch 170, training loss: 14.218533515930176 = 1.714564561843872 + 2.0 * 6.251984596252441
Epoch 170, val loss: 1.7344599962234497
Epoch 180, training loss: 14.168148040771484 = 1.6956956386566162 + 2.0 * 6.2362260818481445
Epoch 180, val loss: 1.719042420387268
Epoch 190, training loss: 14.11823844909668 = 1.674283504486084 + 2.0 * 6.221977233886719
Epoch 190, val loss: 1.7017406225204468
Epoch 200, training loss: 14.073302268981934 = 1.6498852968215942 + 2.0 * 6.2117085456848145
Epoch 200, val loss: 1.6820948123931885
Epoch 210, training loss: 14.020803451538086 = 1.6226134300231934 + 2.0 * 6.199095249176025
Epoch 210, val loss: 1.6602435111999512
Epoch 220, training loss: 13.970794677734375 = 1.592244267463684 + 2.0 * 6.18927526473999
Epoch 220, val loss: 1.6359472274780273
Epoch 230, training loss: 13.919842720031738 = 1.558592438697815 + 2.0 * 6.180624961853027
Epoch 230, val loss: 1.609013557434082
Epoch 240, training loss: 13.869482040405273 = 1.5217312574386597 + 2.0 * 6.173875331878662
Epoch 240, val loss: 1.5797604322433472
Epoch 250, training loss: 13.814472198486328 = 1.4823713302612305 + 2.0 * 6.166050434112549
Epoch 250, val loss: 1.5485363006591797
Epoch 260, training loss: 13.759454727172852 = 1.4406907558441162 + 2.0 * 6.159381866455078
Epoch 260, val loss: 1.5157219171524048
Epoch 270, training loss: 13.714414596557617 = 1.3971295356750488 + 2.0 * 6.158642768859863
Epoch 270, val loss: 1.4817708730697632
Epoch 280, training loss: 13.652444839477539 = 1.3531558513641357 + 2.0 * 6.149644374847412
Epoch 280, val loss: 1.4475880861282349
Epoch 290, training loss: 13.5977144241333 = 1.3092401027679443 + 2.0 * 6.144237041473389
Epoch 290, val loss: 1.4137593507766724
Epoch 300, training loss: 13.543127059936523 = 1.2657982110977173 + 2.0 * 6.138664245605469
Epoch 300, val loss: 1.380549430847168
Epoch 310, training loss: 13.517638206481934 = 1.223658800125122 + 2.0 * 6.146989822387695
Epoch 310, val loss: 1.3485482931137085
Epoch 320, training loss: 13.45092487335205 = 1.184056043624878 + 2.0 * 6.133434295654297
Epoch 320, val loss: 1.3189635276794434
Epoch 330, training loss: 13.402649879455566 = 1.1469411849975586 + 2.0 * 6.127854347229004
Epoch 330, val loss: 1.291576862335205
Epoch 340, training loss: 13.358114242553711 = 1.1121230125427246 + 2.0 * 6.122995376586914
Epoch 340, val loss: 1.2663120031356812
Epoch 350, training loss: 13.325492858886719 = 1.0795223712921143 + 2.0 * 6.122985363006592
Epoch 350, val loss: 1.2430051565170288
Epoch 360, training loss: 13.282670021057129 = 1.0492967367172241 + 2.0 * 6.116686820983887
Epoch 360, val loss: 1.2216163873672485
Epoch 370, training loss: 13.246872901916504 = 1.0208817720413208 + 2.0 * 6.112995624542236
Epoch 370, val loss: 1.202144980430603
Epoch 380, training loss: 13.215255737304688 = 0.993994951248169 + 2.0 * 6.110630512237549
Epoch 380, val loss: 1.1841856241226196
Epoch 390, training loss: 13.186457633972168 = 0.9683346152305603 + 2.0 * 6.1090617179870605
Epoch 390, val loss: 1.167340874671936
Epoch 400, training loss: 13.15605354309082 = 0.9435516595840454 + 2.0 * 6.106250762939453
Epoch 400, val loss: 1.151525616645813
Epoch 410, training loss: 13.121683120727539 = 0.9191415309906006 + 2.0 * 6.10127067565918
Epoch 410, val loss: 1.1363193988800049
Epoch 420, training loss: 13.097658157348633 = 0.8947624564170837 + 2.0 * 6.101448059082031
Epoch 420, val loss: 1.1213380098342896
Epoch 430, training loss: 13.066252708435059 = 0.8701658248901367 + 2.0 * 6.098043441772461
Epoch 430, val loss: 1.1065828800201416
Epoch 440, training loss: 13.032307624816895 = 0.8450777530670166 + 2.0 * 6.0936150550842285
Epoch 440, val loss: 1.0917762517929077
Epoch 450, training loss: 13.000871658325195 = 0.8192394375801086 + 2.0 * 6.090816020965576
Epoch 450, val loss: 1.0767401456832886
Epoch 460, training loss: 12.983797073364258 = 0.792547345161438 + 2.0 * 6.095624923706055
Epoch 460, val loss: 1.061246395111084
Epoch 470, training loss: 12.945329666137695 = 0.7651746869087219 + 2.0 * 6.0900774002075195
Epoch 470, val loss: 1.04555082321167
Epoch 480, training loss: 12.912109375 = 0.7373716235160828 + 2.0 * 6.087368965148926
Epoch 480, val loss: 1.0296896696090698
Epoch 490, training loss: 12.875412940979004 = 0.7089520692825317 + 2.0 * 6.083230495452881
Epoch 490, val loss: 1.013690710067749
Epoch 500, training loss: 12.844571113586426 = 0.6799124479293823 + 2.0 * 6.082329273223877
Epoch 500, val loss: 0.9976121783256531
Epoch 510, training loss: 12.817201614379883 = 0.6505655646324158 + 2.0 * 6.08331823348999
Epoch 510, val loss: 0.9815602898597717
Epoch 520, training loss: 12.778376579284668 = 0.6211431622505188 + 2.0 * 6.078616619110107
Epoch 520, val loss: 0.9659740328788757
Epoch 530, training loss: 12.744382858276367 = 0.5915828943252563 + 2.0 * 6.076399803161621
Epoch 530, val loss: 0.9507489204406738
Epoch 540, training loss: 12.739629745483398 = 0.5621874928474426 + 2.0 * 6.08872127532959
Epoch 540, val loss: 0.9358279705047607
Epoch 550, training loss: 12.681721687316895 = 0.5331687927246094 + 2.0 * 6.074276447296143
Epoch 550, val loss: 0.9216217398643494
Epoch 560, training loss: 12.649356842041016 = 0.5047599077224731 + 2.0 * 6.072298526763916
Epoch 560, val loss: 0.9083530306816101
Epoch 570, training loss: 12.61813735961914 = 0.47691184282302856 + 2.0 * 6.070612907409668
Epoch 570, val loss: 0.8958556652069092
Epoch 580, training loss: 12.610856056213379 = 0.44981494545936584 + 2.0 * 6.0805206298828125
Epoch 580, val loss: 0.8842743635177612
Epoch 590, training loss: 12.562053680419922 = 0.42376890778541565 + 2.0 * 6.0691423416137695
Epoch 590, val loss: 0.8734258413314819
Epoch 600, training loss: 12.534586906433105 = 0.3988094627857208 + 2.0 * 6.0678887367248535
Epoch 600, val loss: 0.8638145327568054
Epoch 610, training loss: 12.5065336227417 = 0.37492579221725464 + 2.0 * 6.0658040046691895
Epoch 610, val loss: 0.8551445007324219
Epoch 620, training loss: 12.480690002441406 = 0.35215693712234497 + 2.0 * 6.064266681671143
Epoch 620, val loss: 0.8474759459495544
Epoch 630, training loss: 12.473687171936035 = 0.3305933177471161 + 2.0 * 6.071547031402588
Epoch 630, val loss: 0.8407944440841675
Epoch 640, training loss: 12.441075325012207 = 0.31050175428390503 + 2.0 * 6.065286636352539
Epoch 640, val loss: 0.8351297974586487
Epoch 650, training loss: 12.415149688720703 = 0.29167500138282776 + 2.0 * 6.061737537384033
Epoch 650, val loss: 0.8306493163108826
Epoch 660, training loss: 12.39311408996582 = 0.274059534072876 + 2.0 * 6.059527397155762
Epoch 660, val loss: 0.8272068500518799
Epoch 670, training loss: 12.388077735900879 = 0.25764378905296326 + 2.0 * 6.065217018127441
Epoch 670, val loss: 0.8246961832046509
Epoch 680, training loss: 12.356184959411621 = 0.24240677058696747 + 2.0 * 6.056889057159424
Epoch 680, val loss: 0.8232146501541138
Epoch 690, training loss: 12.345367431640625 = 0.228275865316391 + 2.0 * 6.0585455894470215
Epoch 690, val loss: 0.8226358890533447
Epoch 700, training loss: 12.325502395629883 = 0.21518492698669434 + 2.0 * 6.055158615112305
Epoch 700, val loss: 0.8228737711906433
Epoch 710, training loss: 12.310555458068848 = 0.20304298400878906 + 2.0 * 6.053756237030029
Epoch 710, val loss: 0.8238581418991089
Epoch 720, training loss: 12.297228813171387 = 0.1917700618505478 + 2.0 * 6.052729606628418
Epoch 720, val loss: 0.8256976008415222
Epoch 730, training loss: 12.295802116394043 = 0.18133008480072021 + 2.0 * 6.057236194610596
Epoch 730, val loss: 0.8281017541885376
Epoch 740, training loss: 12.274279594421387 = 0.1716478168964386 + 2.0 * 6.051315784454346
Epoch 740, val loss: 0.8311631679534912
Epoch 750, training loss: 12.264657020568848 = 0.1626584827899933 + 2.0 * 6.050999164581299
Epoch 750, val loss: 0.8347108960151672
Epoch 760, training loss: 12.252172470092773 = 0.15427833795547485 + 2.0 * 6.048946857452393
Epoch 760, val loss: 0.8388272523880005
Epoch 770, training loss: 12.243732452392578 = 0.14646592736244202 + 2.0 * 6.048633098602295
Epoch 770, val loss: 0.8435212969779968
Epoch 780, training loss: 12.2327241897583 = 0.13918128609657288 + 2.0 * 6.04677152633667
Epoch 780, val loss: 0.8484389185905457
Epoch 790, training loss: 12.229732513427734 = 0.13238242268562317 + 2.0 * 6.048675060272217
Epoch 790, val loss: 0.8538956642150879
Epoch 800, training loss: 12.217092514038086 = 0.12602679431438446 + 2.0 * 6.045532703399658
Epoch 800, val loss: 0.8596104979515076
Epoch 810, training loss: 12.212065696716309 = 0.12007326632738113 + 2.0 * 6.045996189117432
Epoch 810, val loss: 0.8656547665596008
Epoch 820, training loss: 12.198908805847168 = 0.11448618769645691 + 2.0 * 6.042211532592773
Epoch 820, val loss: 0.8720447421073914
Epoch 830, training loss: 12.204509735107422 = 0.10922237485647202 + 2.0 * 6.047643661499023
Epoch 830, val loss: 0.8786976933479309
Epoch 840, training loss: 12.19524097442627 = 0.10424505919218063 + 2.0 * 6.045497894287109
Epoch 840, val loss: 0.8851948380470276
Epoch 850, training loss: 12.180899620056152 = 0.09960091859102249 + 2.0 * 6.0406494140625
Epoch 850, val loss: 0.8919784426689148
Epoch 860, training loss: 12.1746187210083 = 0.09520354121923447 + 2.0 * 6.039707660675049
Epoch 860, val loss: 0.8989419937133789
Epoch 870, training loss: 12.172423362731934 = 0.09102529287338257 + 2.0 * 6.040699005126953
Epoch 870, val loss: 0.9060793519020081
Epoch 880, training loss: 12.1640043258667 = 0.08708798885345459 + 2.0 * 6.038458347320557
Epoch 880, val loss: 0.9131507873535156
Epoch 890, training loss: 12.157922744750977 = 0.08333507925271988 + 2.0 * 6.037293910980225
Epoch 890, val loss: 0.9204472303390503
Epoch 900, training loss: 12.15771198272705 = 0.07977358996868134 + 2.0 * 6.038969039916992
Epoch 900, val loss: 0.9277519583702087
Epoch 910, training loss: 12.151233673095703 = 0.07640877366065979 + 2.0 * 6.037412643432617
Epoch 910, val loss: 0.9350025653839111
Epoch 920, training loss: 12.143417358398438 = 0.07321789860725403 + 2.0 * 6.035099506378174
Epoch 920, val loss: 0.9424144625663757
Epoch 930, training loss: 12.137738227844238 = 0.0701918676495552 + 2.0 * 6.033772945404053
Epoch 930, val loss: 0.9498660564422607
Epoch 940, training loss: 12.141876220703125 = 0.06732892990112305 + 2.0 * 6.037273406982422
Epoch 940, val loss: 0.9572767615318298
Epoch 950, training loss: 12.135720252990723 = 0.06461675465106964 + 2.0 * 6.03555154800415
Epoch 950, val loss: 0.964604377746582
Epoch 960, training loss: 12.130900382995605 = 0.062062013894319534 + 2.0 * 6.034419059753418
Epoch 960, val loss: 0.971846878528595
Epoch 970, training loss: 12.125446319580078 = 0.05963633581995964 + 2.0 * 6.032905101776123
Epoch 970, val loss: 0.9792317748069763
Epoch 980, training loss: 12.119946479797363 = 0.057338543236255646 + 2.0 * 6.031303882598877
Epoch 980, val loss: 0.9864069223403931
Epoch 990, training loss: 12.122939109802246 = 0.05515127629041672 + 2.0 * 6.033894062042236
Epoch 990, val loss: 0.9935764074325562
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.5720
Flip ASR: 0.4978/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.696269989013672 = 1.9484769105911255 + 2.0 * 8.373896598815918
Epoch 0, val loss: 1.946478247642517
Epoch 10, training loss: 18.685544967651367 = 1.9384844303131104 + 2.0 * 8.373530387878418
Epoch 10, val loss: 1.9362984895706177
Epoch 20, training loss: 18.668418884277344 = 1.9263274669647217 + 2.0 * 8.37104606628418
Epoch 20, val loss: 1.9238792657852173
Epoch 30, training loss: 18.618146896362305 = 1.9097193479537964 + 2.0 * 8.35421371459961
Epoch 30, val loss: 1.9069374799728394
Epoch 40, training loss: 18.400297164916992 = 1.8890434503555298 + 2.0 * 8.255626678466797
Epoch 40, val loss: 1.886641025543213
Epoch 50, training loss: 17.558895111083984 = 1.866652250289917 + 2.0 * 7.846121311187744
Epoch 50, val loss: 1.8647875785827637
Epoch 60, training loss: 16.610177993774414 = 1.8445030450820923 + 2.0 * 7.382837772369385
Epoch 60, val loss: 1.8436307907104492
Epoch 70, training loss: 15.9268217086792 = 1.8293687105178833 + 2.0 * 7.048726558685303
Epoch 70, val loss: 1.829486608505249
Epoch 80, training loss: 15.606921195983887 = 1.8151205778121948 + 2.0 * 6.895900249481201
Epoch 80, val loss: 1.8155437707901
Epoch 90, training loss: 15.35905933380127 = 1.796286702156067 + 2.0 * 6.781386375427246
Epoch 90, val loss: 1.7974977493286133
Epoch 100, training loss: 15.080416679382324 = 1.7793813943862915 + 2.0 * 6.650517463684082
Epoch 100, val loss: 1.7816632986068726
Epoch 110, training loss: 14.865044593811035 = 1.7656623125076294 + 2.0 * 6.549691200256348
Epoch 110, val loss: 1.7690815925598145
Epoch 120, training loss: 14.730417251586914 = 1.7481547594070435 + 2.0 * 6.49113130569458
Epoch 120, val loss: 1.7530303001403809
Epoch 130, training loss: 14.608270645141602 = 1.7295076847076416 + 2.0 * 6.4393815994262695
Epoch 130, val loss: 1.7362959384918213
Epoch 140, training loss: 14.502317428588867 = 1.7106902599334717 + 2.0 * 6.395813465118408
Epoch 140, val loss: 1.7195358276367188
Epoch 150, training loss: 14.412660598754883 = 1.6901845932006836 + 2.0 * 6.3612380027771
Epoch 150, val loss: 1.701434850692749
Epoch 160, training loss: 14.339299201965332 = 1.667080283164978 + 2.0 * 6.336109638214111
Epoch 160, val loss: 1.6812490224838257
Epoch 170, training loss: 14.2609224319458 = 1.6413774490356445 + 2.0 * 6.309772491455078
Epoch 170, val loss: 1.6590756177902222
Epoch 180, training loss: 14.192070007324219 = 1.6130441427230835 + 2.0 * 6.289513111114502
Epoch 180, val loss: 1.6350407600402832
Epoch 190, training loss: 14.126459121704102 = 1.581825852394104 + 2.0 * 6.2723164558410645
Epoch 190, val loss: 1.6088322401046753
Epoch 200, training loss: 14.059250831604004 = 1.5481903553009033 + 2.0 * 6.25553035736084
Epoch 200, val loss: 1.5807949304580688
Epoch 210, training loss: 13.993366241455078 = 1.5124354362487793 + 2.0 * 6.24046516418457
Epoch 210, val loss: 1.5514549016952515
Epoch 220, training loss: 13.929323196411133 = 1.4752825498580933 + 2.0 * 6.227020263671875
Epoch 220, val loss: 1.5211327075958252
Epoch 230, training loss: 13.866283416748047 = 1.436879277229309 + 2.0 * 6.214702129364014
Epoch 230, val loss: 1.4902738332748413
Epoch 240, training loss: 13.816086769104004 = 1.3980515003204346 + 2.0 * 6.209017753601074
Epoch 240, val loss: 1.4593881368637085
Epoch 250, training loss: 13.750569343566895 = 1.3595054149627686 + 2.0 * 6.195531845092773
Epoch 250, val loss: 1.4293732643127441
Epoch 260, training loss: 13.6937894821167 = 1.3213069438934326 + 2.0 * 6.186241149902344
Epoch 260, val loss: 1.3997114896774292
Epoch 270, training loss: 13.643932342529297 = 1.2833458185195923 + 2.0 * 6.180293083190918
Epoch 270, val loss: 1.3705081939697266
Epoch 280, training loss: 13.590195655822754 = 1.2461795806884766 + 2.0 * 6.172008037567139
Epoch 280, val loss: 1.3422691822052002
Epoch 290, training loss: 13.542583465576172 = 1.2096619606018066 + 2.0 * 6.1664605140686035
Epoch 290, val loss: 1.3147591352462769
Epoch 300, training loss: 13.4940185546875 = 1.1738126277923584 + 2.0 * 6.160102844238281
Epoch 300, val loss: 1.2876769304275513
Epoch 310, training loss: 13.446151733398438 = 1.1382627487182617 + 2.0 * 6.153944492340088
Epoch 310, val loss: 1.2610586881637573
Epoch 320, training loss: 13.40098762512207 = 1.1030181646347046 + 2.0 * 6.148984909057617
Epoch 320, val loss: 1.2345588207244873
Epoch 330, training loss: 13.361814498901367 = 1.0681921243667603 + 2.0 * 6.146811008453369
Epoch 330, val loss: 1.2083805799484253
Epoch 340, training loss: 13.313911437988281 = 1.0339009761810303 + 2.0 * 6.140005111694336
Epoch 340, val loss: 1.1825448274612427
Epoch 350, training loss: 13.26972484588623 = 1.0000450611114502 + 2.0 * 6.13484001159668
Epoch 350, val loss: 1.1569241285324097
Epoch 360, training loss: 13.230618476867676 = 0.9664849638938904 + 2.0 * 6.13206672668457
Epoch 360, val loss: 1.1315217018127441
Epoch 370, training loss: 13.194272994995117 = 0.9335823655128479 + 2.0 * 6.130345344543457
Epoch 370, val loss: 1.106408715248108
Epoch 380, training loss: 13.147944450378418 = 0.9014002084732056 + 2.0 * 6.123271942138672
Epoch 380, val loss: 1.0819100141525269
Epoch 390, training loss: 13.108542442321777 = 0.8698958158493042 + 2.0 * 6.119323253631592
Epoch 390, val loss: 1.0578807592391968
Epoch 400, training loss: 13.072991371154785 = 0.8389933705329895 + 2.0 * 6.11699914932251
Epoch 400, val loss: 1.034420371055603
Epoch 410, training loss: 13.038092613220215 = 0.8090029358863831 + 2.0 * 6.114544868469238
Epoch 410, val loss: 1.0116750001907349
Epoch 420, training loss: 13.000941276550293 = 0.7799869179725647 + 2.0 * 6.110476970672607
Epoch 420, val loss: 0.9900332689285278
Epoch 430, training loss: 12.971403121948242 = 0.7520649433135986 + 2.0 * 6.109669208526611
Epoch 430, val loss: 0.9695640206336975
Epoch 440, training loss: 12.935710906982422 = 0.725256085395813 + 2.0 * 6.105227470397949
Epoch 440, val loss: 0.9504308700561523
Epoch 450, training loss: 12.92674446105957 = 0.6995408535003662 + 2.0 * 6.1136016845703125
Epoch 450, val loss: 0.932734489440918
Epoch 460, training loss: 12.880794525146484 = 0.675329864025116 + 2.0 * 6.102732181549072
Epoch 460, val loss: 0.9164845943450928
Epoch 470, training loss: 12.84942626953125 = 0.652250349521637 + 2.0 * 6.098587989807129
Epoch 470, val loss: 0.9017646312713623
Epoch 480, training loss: 12.820379257202148 = 0.6301913261413574 + 2.0 * 6.095093727111816
Epoch 480, val loss: 0.8884165287017822
Epoch 490, training loss: 12.794060707092285 = 0.608984112739563 + 2.0 * 6.092538356781006
Epoch 490, val loss: 0.8762571811676025
Epoch 500, training loss: 12.789730072021484 = 0.5886473655700684 + 2.0 * 6.100541591644287
Epoch 500, val loss: 0.8652098774909973
Epoch 510, training loss: 12.750391006469727 = 0.5692266821861267 + 2.0 * 6.090582370758057
Epoch 510, val loss: 0.8553214073181152
Epoch 520, training loss: 12.72296142578125 = 0.550616443157196 + 2.0 * 6.086172580718994
Epoch 520, val loss: 0.8463992476463318
Epoch 530, training loss: 12.7012300491333 = 0.5326449871063232 + 2.0 * 6.084292411804199
Epoch 530, val loss: 0.8383727669715881
Epoch 540, training loss: 12.704054832458496 = 0.5152161121368408 + 2.0 * 6.094419479370117
Epoch 540, val loss: 0.8311344981193542
Epoch 550, training loss: 12.667095184326172 = 0.498520165681839 + 2.0 * 6.084287643432617
Epoch 550, val loss: 0.8245154023170471
Epoch 560, training loss: 12.64122486114502 = 0.4823395013809204 + 2.0 * 6.079442501068115
Epoch 560, val loss: 0.8186196088790894
Epoch 570, training loss: 12.621597290039062 = 0.4665745496749878 + 2.0 * 6.077511310577393
Epoch 570, val loss: 0.8133164644241333
Epoch 580, training loss: 12.604418754577637 = 0.4512191712856293 + 2.0 * 6.076599597930908
Epoch 580, val loss: 0.8084328770637512
Epoch 590, training loss: 12.58881950378418 = 0.4362961947917938 + 2.0 * 6.076261520385742
Epoch 590, val loss: 0.804121732711792
Epoch 600, training loss: 12.568507194519043 = 0.42168349027633667 + 2.0 * 6.07341194152832
Epoch 600, val loss: 0.8002825975418091
Epoch 610, training loss: 12.556337356567383 = 0.40732672810554504 + 2.0 * 6.07450532913208
Epoch 610, val loss: 0.7968451976776123
Epoch 620, training loss: 12.5349760055542 = 0.39318424463272095 + 2.0 * 6.070895671844482
Epoch 620, val loss: 0.7937585711479187
Epoch 630, training loss: 12.518256187438965 = 0.37924832105636597 + 2.0 * 6.0695037841796875
Epoch 630, val loss: 0.7911027073860168
Epoch 640, training loss: 12.499957084655762 = 0.36546215415000916 + 2.0 * 6.06724739074707
Epoch 640, val loss: 0.7887627482414246
Epoch 650, training loss: 12.484387397766113 = 0.35184985399246216 + 2.0 * 6.0662689208984375
Epoch 650, val loss: 0.7868357300758362
Epoch 660, training loss: 12.4752779006958 = 0.3383665978908539 + 2.0 * 6.068455696105957
Epoch 660, val loss: 0.7852739691734314
Epoch 670, training loss: 12.453442573547363 = 0.32502663135528564 + 2.0 * 6.064208030700684
Epoch 670, val loss: 0.7839129567146301
Epoch 680, training loss: 12.435749053955078 = 0.3118765354156494 + 2.0 * 6.061936378479004
Epoch 680, val loss: 0.7830262184143066
Epoch 690, training loss: 12.420943260192871 = 0.29887109994888306 + 2.0 * 6.061036109924316
Epoch 690, val loss: 0.7824862003326416
Epoch 700, training loss: 12.411248207092285 = 0.2860933840274811 + 2.0 * 6.062577247619629
Epoch 700, val loss: 0.7822445631027222
Epoch 710, training loss: 12.39430046081543 = 0.2735724151134491 + 2.0 * 6.060364246368408
Epoch 710, val loss: 0.7825363278388977
Epoch 720, training loss: 12.380592346191406 = 0.2613678574562073 + 2.0 * 6.059612274169922
Epoch 720, val loss: 0.7831657528877258
Epoch 730, training loss: 12.365452766418457 = 0.24954922497272491 + 2.0 * 6.057951927185059
Epoch 730, val loss: 0.7841511368751526
Epoch 740, training loss: 12.348250389099121 = 0.2380831092596054 + 2.0 * 6.055083751678467
Epoch 740, val loss: 0.7855498790740967
Epoch 750, training loss: 12.334482192993164 = 0.22704149782657623 + 2.0 * 6.053720474243164
Epoch 750, val loss: 0.7874109148979187
Epoch 760, training loss: 12.324753761291504 = 0.21644316613674164 + 2.0 * 6.054155349731445
Epoch 760, val loss: 0.789673388004303
Epoch 770, training loss: 12.310125350952148 = 0.20627868175506592 + 2.0 * 6.0519232749938965
Epoch 770, val loss: 0.7922260761260986
Epoch 780, training loss: 12.300496101379395 = 0.19665572047233582 + 2.0 * 6.051920413970947
Epoch 780, val loss: 0.7952439188957214
Epoch 790, training loss: 12.288122177124023 = 0.18749001622200012 + 2.0 * 6.050315856933594
Epoch 790, val loss: 0.7985901832580566
Epoch 800, training loss: 12.286935806274414 = 0.17878292500972748 + 2.0 * 6.054076671600342
Epoch 800, val loss: 0.8022727966308594
Epoch 810, training loss: 12.266611099243164 = 0.17058056592941284 + 2.0 * 6.048015117645264
Epoch 810, val loss: 0.8061227202415466
Epoch 820, training loss: 12.256239891052246 = 0.16281232237815857 + 2.0 * 6.046713829040527
Epoch 820, val loss: 0.8102790713310242
Epoch 830, training loss: 12.261581420898438 = 0.15547126531600952 + 2.0 * 6.053055286407471
Epoch 830, val loss: 0.8146655559539795
Epoch 840, training loss: 12.239482879638672 = 0.14855921268463135 + 2.0 * 6.045461654663086
Epoch 840, val loss: 0.8192132711410522
Epoch 850, training loss: 12.233020782470703 = 0.14204491674900055 + 2.0 * 6.045487880706787
Epoch 850, val loss: 0.8240266442298889
Epoch 860, training loss: 12.225028038024902 = 0.13591323792934418 + 2.0 * 6.044557571411133
Epoch 860, val loss: 0.8289560675621033
Epoch 870, training loss: 12.218098640441895 = 0.13011887669563293 + 2.0 * 6.043989658355713
Epoch 870, val loss: 0.8340920209884644
Epoch 880, training loss: 12.213693618774414 = 0.12463932484388351 + 2.0 * 6.044527053833008
Epoch 880, val loss: 0.8392061591148376
Epoch 890, training loss: 12.201200485229492 = 0.11947925388813019 + 2.0 * 6.040860652923584
Epoch 890, val loss: 0.8443921208381653
Epoch 900, training loss: 12.19441032409668 = 0.11457355320453644 + 2.0 * 6.039918422698975
Epoch 900, val loss: 0.8497440218925476
Epoch 910, training loss: 12.188353538513184 = 0.10990777611732483 + 2.0 * 6.039222717285156
Epoch 910, val loss: 0.8550841212272644
Epoch 920, training loss: 12.201751708984375 = 0.1054542288184166 + 2.0 * 6.0481486320495605
Epoch 920, val loss: 0.8603200912475586
Epoch 930, training loss: 12.17795467376709 = 0.10126811265945435 + 2.0 * 6.03834342956543
Epoch 930, val loss: 0.8656089901924133
Epoch 940, training loss: 12.171716690063477 = 0.0972808375954628 + 2.0 * 6.03721809387207
Epoch 940, val loss: 0.8710910677909851
Epoch 950, training loss: 12.166560173034668 = 0.09347648918628693 + 2.0 * 6.036541938781738
Epoch 950, val loss: 0.8765344619750977
Epoch 960, training loss: 12.170416831970215 = 0.08986197412014008 + 2.0 * 6.040277481079102
Epoch 960, val loss: 0.8818350434303284
Epoch 970, training loss: 12.15641975402832 = 0.0864233449101448 + 2.0 * 6.034998416900635
Epoch 970, val loss: 0.8871312141418457
Epoch 980, training loss: 12.152200698852539 = 0.08314572274684906 + 2.0 * 6.03452730178833
Epoch 980, val loss: 0.8925355076789856
Epoch 990, training loss: 12.150947570800781 = 0.08001098036766052 + 2.0 * 6.035468101501465
Epoch 990, val loss: 0.8979226350784302
=== picking the best model according to the performance on validation ===
target class rate on Vs: 0.8000
accuracy on clean test nodes: 0.7704
Overall ASR: 0.7970
Flip ASR: 0.7600/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.699153900146484 = 1.951422095298767 + 2.0 * 8.373866081237793
Epoch 0, val loss: 1.9561166763305664
Epoch 10, training loss: 18.68840789794922 = 1.9416841268539429 + 2.0 * 8.373361587524414
Epoch 10, val loss: 1.9466925859451294
Epoch 20, training loss: 18.67063331604004 = 1.9299756288528442 + 2.0 * 8.370328903198242
Epoch 20, val loss: 1.9352586269378662
Epoch 30, training loss: 18.615528106689453 = 1.9145002365112305 + 2.0 * 8.350513458251953
Epoch 30, val loss: 1.9202139377593994
Epoch 40, training loss: 18.32699203491211 = 1.8954904079437256 + 2.0 * 8.215750694274902
Epoch 40, val loss: 1.9022252559661865
Epoch 50, training loss: 16.790470123291016 = 1.8747421503067017 + 2.0 * 7.457863807678223
Epoch 50, val loss: 1.8823838233947754
Epoch 60, training loss: 15.967763900756836 = 1.8565659523010254 + 2.0 * 7.055599212646484
Epoch 60, val loss: 1.8652334213256836
Epoch 70, training loss: 15.50500202178955 = 1.8425428867340088 + 2.0 * 6.8312296867370605
Epoch 70, val loss: 1.851628303527832
Epoch 80, training loss: 15.228129386901855 = 1.8277205228805542 + 2.0 * 6.700204372406006
Epoch 80, val loss: 1.8379915952682495
Epoch 90, training loss: 15.0098876953125 = 1.812528371810913 + 2.0 * 6.598679542541504
Epoch 90, val loss: 1.8237950801849365
Epoch 100, training loss: 14.857271194458008 = 1.796777606010437 + 2.0 * 6.530246734619141
Epoch 100, val loss: 1.8091243505477905
Epoch 110, training loss: 14.7570161819458 = 1.7815163135528564 + 2.0 * 6.487750053405762
Epoch 110, val loss: 1.7946324348449707
Epoch 120, training loss: 14.665206909179688 = 1.766554594039917 + 2.0 * 6.449326038360596
Epoch 120, val loss: 1.780652403831482
Epoch 130, training loss: 14.576571464538574 = 1.751666784286499 + 2.0 * 6.412452220916748
Epoch 130, val loss: 1.7668094635009766
Epoch 140, training loss: 14.496526718139648 = 1.7361979484558105 + 2.0 * 6.38016414642334
Epoch 140, val loss: 1.7523940801620483
Epoch 150, training loss: 14.419499397277832 = 1.7193421125411987 + 2.0 * 6.350078582763672
Epoch 150, val loss: 1.7370268106460571
Epoch 160, training loss: 14.352578163146973 = 1.7007001638412476 + 2.0 * 6.325939178466797
Epoch 160, val loss: 1.7202785015106201
Epoch 170, training loss: 14.289306640625 = 1.6798992156982422 + 2.0 * 6.304703712463379
Epoch 170, val loss: 1.7017000913619995
Epoch 180, training loss: 14.229328155517578 = 1.6562891006469727 + 2.0 * 6.286519527435303
Epoch 180, val loss: 1.6810433864593506
Epoch 190, training loss: 14.171974182128906 = 1.6295733451843262 + 2.0 * 6.271200656890869
Epoch 190, val loss: 1.6580541133880615
Epoch 200, training loss: 14.114130020141602 = 1.599573016166687 + 2.0 * 6.2572784423828125
Epoch 200, val loss: 1.632261872291565
Epoch 210, training loss: 14.054850578308105 = 1.5657854080200195 + 2.0 * 6.244532585144043
Epoch 210, val loss: 1.603226900100708
Epoch 220, training loss: 13.99921703338623 = 1.527894139289856 + 2.0 * 6.235661506652832
Epoch 220, val loss: 1.570723056793213
Epoch 230, training loss: 13.933721542358398 = 1.4862520694732666 + 2.0 * 6.2237348556518555
Epoch 230, val loss: 1.5348631143569946
Epoch 240, training loss: 13.870241165161133 = 1.440535068511963 + 2.0 * 6.214853286743164
Epoch 240, val loss: 1.4951695203781128
Epoch 250, training loss: 13.805179595947266 = 1.3912824392318726 + 2.0 * 6.206948757171631
Epoch 250, val loss: 1.4527915716171265
Epoch 260, training loss: 13.739130020141602 = 1.3396086692810059 + 2.0 * 6.199760437011719
Epoch 260, val loss: 1.4084677696228027
Epoch 270, training loss: 13.672158241271973 = 1.2865971326828003 + 2.0 * 6.192780494689941
Epoch 270, val loss: 1.3632746934890747
Epoch 280, training loss: 13.606097221374512 = 1.2330530881881714 + 2.0 * 6.186522006988525
Epoch 280, val loss: 1.3183649778366089
Epoch 290, training loss: 13.5481595993042 = 1.1797198057174683 + 2.0 * 6.184219837188721
Epoch 290, val loss: 1.274601936340332
Epoch 300, training loss: 13.477376937866211 = 1.128068447113037 + 2.0 * 6.174654483795166
Epoch 300, val loss: 1.232553243637085
Epoch 310, training loss: 13.415117263793945 = 1.0777491331100464 + 2.0 * 6.168684005737305
Epoch 310, val loss: 1.1922376155853271
Epoch 320, training loss: 13.35896110534668 = 1.0289525985717773 + 2.0 * 6.165004253387451
Epoch 320, val loss: 1.1536985635757446
Epoch 330, training loss: 13.299459457397461 = 0.982200562953949 + 2.0 * 6.158629417419434
Epoch 330, val loss: 1.1172420978546143
Epoch 340, training loss: 13.244569778442383 = 0.9369372725486755 + 2.0 * 6.153816223144531
Epoch 340, val loss: 1.0825358629226685
Epoch 350, training loss: 13.2017240524292 = 0.8928962349891663 + 2.0 * 6.15441370010376
Epoch 350, val loss: 1.0491156578063965
Epoch 360, training loss: 13.144660949707031 = 0.8505645990371704 + 2.0 * 6.147047996520996
Epoch 360, val loss: 1.0173038244247437
Epoch 370, training loss: 13.093101501464844 = 0.809624195098877 + 2.0 * 6.1417388916015625
Epoch 370, val loss: 0.9869214296340942
Epoch 380, training loss: 13.046513557434082 = 0.7699150443077087 + 2.0 * 6.138299465179443
Epoch 380, val loss: 0.9577906727790833
Epoch 390, training loss: 13.002406120300293 = 0.7316754460334778 + 2.0 * 6.1353654861450195
Epoch 390, val loss: 0.9300275444984436
Epoch 400, training loss: 12.955500602722168 = 0.69520503282547 + 2.0 * 6.130147933959961
Epoch 400, val loss: 0.9039944410324097
Epoch 410, training loss: 12.913641929626465 = 0.660194993019104 + 2.0 * 6.126723289489746
Epoch 410, val loss: 0.8793404698371887
Epoch 420, training loss: 12.872814178466797 = 0.6264966726303101 + 2.0 * 6.123158931732178
Epoch 420, val loss: 0.8560281991958618
Epoch 430, training loss: 12.846813201904297 = 0.5942273139953613 + 2.0 * 6.126293182373047
Epoch 430, val loss: 0.8341346383094788
Epoch 440, training loss: 12.799928665161133 = 0.5637979507446289 + 2.0 * 6.118065357208252
Epoch 440, val loss: 0.8141290545463562
Epoch 450, training loss: 12.764655113220215 = 0.5348464846611023 + 2.0 * 6.114904403686523
Epoch 450, val loss: 0.7957285642623901
Epoch 460, training loss: 12.731579780578613 = 0.507187008857727 + 2.0 * 6.112196445465088
Epoch 460, val loss: 0.7786644101142883
Epoch 470, training loss: 12.699346542358398 = 0.4808151125907898 + 2.0 * 6.1092658042907715
Epoch 470, val loss: 0.7630058526992798
Epoch 480, training loss: 12.677804946899414 = 0.4557324945926666 + 2.0 * 6.11103630065918
Epoch 480, val loss: 0.7489202618598938
Epoch 490, training loss: 12.640600204467773 = 0.4319542348384857 + 2.0 * 6.104322910308838
Epoch 490, val loss: 0.7363182902336121
Epoch 500, training loss: 12.612025260925293 = 0.4092731177806854 + 2.0 * 6.101376056671143
Epoch 500, val loss: 0.7249777913093567
Epoch 510, training loss: 12.586828231811523 = 0.3875744044780731 + 2.0 * 6.0996270179748535
Epoch 510, val loss: 0.7148456573486328
Epoch 520, training loss: 12.570941925048828 = 0.3669480085372925 + 2.0 * 6.101996898651123
Epoch 520, val loss: 0.7058451175689697
Epoch 530, training loss: 12.540960311889648 = 0.34756097197532654 + 2.0 * 6.0966997146606445
Epoch 530, val loss: 0.6979983448982239
Epoch 540, training loss: 12.515828132629395 = 0.3292095363140106 + 2.0 * 6.09330940246582
Epoch 540, val loss: 0.6911916136741638
Epoch 550, training loss: 12.494681358337402 = 0.3117855191230774 + 2.0 * 6.091447830200195
Epoch 550, val loss: 0.6851888298988342
Epoch 560, training loss: 12.477743148803711 = 0.29532191157341003 + 2.0 * 6.091210842132568
Epoch 560, val loss: 0.6799995303153992
Epoch 570, training loss: 12.45714282989502 = 0.2799198031425476 + 2.0 * 6.088611602783203
Epoch 570, val loss: 0.675578773021698
Epoch 580, training loss: 12.437994003295898 = 0.26533183455467224 + 2.0 * 6.086330890655518
Epoch 580, val loss: 0.6718024015426636
Epoch 590, training loss: 12.42176342010498 = 0.25151389837265015 + 2.0 * 6.085124969482422
Epoch 590, val loss: 0.668512225151062
Epoch 600, training loss: 12.408795356750488 = 0.23851661384105682 + 2.0 * 6.085139274597168
Epoch 600, val loss: 0.665622889995575
Epoch 610, training loss: 12.390338897705078 = 0.22626546025276184 + 2.0 * 6.08203649520874
Epoch 610, val loss: 0.6633192896842957
Epoch 620, training loss: 12.374375343322754 = 0.2147609144449234 + 2.0 * 6.079807281494141
Epoch 620, val loss: 0.6615048050880432
Epoch 630, training loss: 12.363326072692871 = 0.20388346910476685 + 2.0 * 6.079721450805664
Epoch 630, val loss: 0.660000741481781
Epoch 640, training loss: 12.348724365234375 = 0.19366058707237244 + 2.0 * 6.077531814575195
Epoch 640, val loss: 0.6587467193603516
Epoch 650, training loss: 12.334389686584473 = 0.1840049773454666 + 2.0 * 6.075192451477051
Epoch 650, val loss: 0.6579968333244324
Epoch 660, training loss: 12.32149600982666 = 0.1748768836259842 + 2.0 * 6.073309421539307
Epoch 660, val loss: 0.6575362682342529
Epoch 670, training loss: 12.341955184936523 = 0.16629241406917572 + 2.0 * 6.087831497192383
Epoch 670, val loss: 0.6573986411094666
Epoch 680, training loss: 12.303441047668457 = 0.1582467257976532 + 2.0 * 6.072597026824951
Epoch 680, val loss: 0.6574410200119019
Epoch 690, training loss: 12.291356086730957 = 0.15068309009075165 + 2.0 * 6.07033634185791
Epoch 690, val loss: 0.6578931212425232
Epoch 700, training loss: 12.279723167419434 = 0.14354680478572845 + 2.0 * 6.068088054656982
Epoch 700, val loss: 0.6585379242897034
Epoch 710, training loss: 12.30147933959961 = 0.13682900369167328 + 2.0 * 6.082324981689453
Epoch 710, val loss: 0.6593717336654663
Epoch 720, training loss: 12.266935348510742 = 0.13052666187286377 + 2.0 * 6.068204402923584
Epoch 720, val loss: 0.6605158448219299
Epoch 730, training loss: 12.255510330200195 = 0.12460004538297653 + 2.0 * 6.065454959869385
Epoch 730, val loss: 0.6619662642478943
Epoch 740, training loss: 12.245610237121582 = 0.11899291723966599 + 2.0 * 6.0633087158203125
Epoch 740, val loss: 0.6636235117912292
Epoch 750, training loss: 12.241752624511719 = 0.11368320137262344 + 2.0 * 6.064034938812256
Epoch 750, val loss: 0.665500819683075
Epoch 760, training loss: 12.232646942138672 = 0.10869571566581726 + 2.0 * 6.061975479125977
Epoch 760, val loss: 0.6674953103065491
Epoch 770, training loss: 12.225082397460938 = 0.10398136079311371 + 2.0 * 6.060550689697266
Epoch 770, val loss: 0.6697999238967896
Epoch 780, training loss: 12.216816902160645 = 0.09953038394451141 + 2.0 * 6.058643341064453
Epoch 780, val loss: 0.6722559332847595
Epoch 790, training loss: 12.224268913269043 = 0.09532956779003143 + 2.0 * 6.064469814300537
Epoch 790, val loss: 0.6748140454292297
Epoch 800, training loss: 12.209131240844727 = 0.09138346463441849 + 2.0 * 6.058873653411865
Epoch 800, val loss: 0.6774991750717163
Epoch 810, training loss: 12.199112892150879 = 0.08765123784542084 + 2.0 * 6.055730819702148
Epoch 810, val loss: 0.6804270148277283
Epoch 820, training loss: 12.19344425201416 = 0.08411452919244766 + 2.0 * 6.0546650886535645
Epoch 820, val loss: 0.6834964752197266
Epoch 830, training loss: 12.201549530029297 = 0.08076303452253342 + 2.0 * 6.060393333435059
Epoch 830, val loss: 0.6866270303726196
Epoch 840, training loss: 12.186890602111816 = 0.0775802955031395 + 2.0 * 6.054655075073242
Epoch 840, val loss: 0.6900204420089722
Epoch 850, training loss: 12.178958892822266 = 0.0745757520198822 + 2.0 * 6.052191734313965
Epoch 850, val loss: 0.6933974027633667
Epoch 860, training loss: 12.188241004943848 = 0.07171335816383362 + 2.0 * 6.058263778686523
Epoch 860, val loss: 0.6969197392463684
Epoch 870, training loss: 12.171487808227539 = 0.06901556253433228 + 2.0 * 6.051236152648926
Epoch 870, val loss: 0.7004519701004028
Epoch 880, training loss: 12.165841102600098 = 0.06645667552947998 + 2.0 * 6.049692153930664
Epoch 880, val loss: 0.7041015028953552
Epoch 890, training loss: 12.162360191345215 = 0.06401513516902924 + 2.0 * 6.049172401428223
Epoch 890, val loss: 0.7078288197517395
Epoch 900, training loss: 12.160353660583496 = 0.06169906631112099 + 2.0 * 6.049327373504639
Epoch 900, val loss: 0.7115008234977722
Epoch 910, training loss: 12.15339183807373 = 0.05950101092457771 + 2.0 * 6.046945571899414
Epoch 910, val loss: 0.7152784466743469
Epoch 920, training loss: 12.155193328857422 = 0.057414136826992035 + 2.0 * 6.048889636993408
Epoch 920, val loss: 0.7191227674484253
Epoch 930, training loss: 12.14653205871582 = 0.05543460324406624 + 2.0 * 6.045548915863037
Epoch 930, val loss: 0.7230533361434937
Epoch 940, training loss: 12.142355918884277 = 0.05353836342692375 + 2.0 * 6.044408798217773
Epoch 940, val loss: 0.7269179821014404
Epoch 950, training loss: 12.155801773071289 = 0.05173022300004959 + 2.0 * 6.052035808563232
Epoch 950, val loss: 0.7309078574180603
Epoch 960, training loss: 12.137324333190918 = 0.05002766475081444 + 2.0 * 6.0436482429504395
Epoch 960, val loss: 0.7346462607383728
Epoch 970, training loss: 12.132805824279785 = 0.04839527979493141 + 2.0 * 6.042205333709717
Epoch 970, val loss: 0.7386341691017151
Epoch 980, training loss: 12.129088401794434 = 0.046832554042339325 + 2.0 * 6.041128158569336
Epoch 980, val loss: 0.7425793409347534
Epoch 990, training loss: 12.148594856262207 = 0.04534229636192322 + 2.0 * 6.051626205444336
Epoch 990, val loss: 0.7465193271636963
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8148
Overall ASR: 0.8192
Flip ASR: 0.7822/225 nodes
The final ASR:0.72940, 0.11169, Accuracy:0.80247, 0.02290
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11600])
remove edge: torch.Size([2, 9454])
updated graph: torch.Size([2, 10498])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9889
Flip ASR: 0.9867/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
The final ASR:0.98155, 0.00522, Accuracy:0.83210, 0.00175
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.5 der2:0.2 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 18.682174682617188 = 1.9342995882034302 + 2.0 * 8.373937606811523
Epoch 0, val loss: 1.929198980331421
Epoch 10, training loss: 18.671794891357422 = 1.9244943857192993 + 2.0 * 8.373650550842285
Epoch 10, val loss: 1.9203029870986938
Epoch 20, training loss: 18.655153274536133 = 1.9123660326004028 + 2.0 * 8.371393203735352
Epoch 20, val loss: 1.9090112447738647
Epoch 30, training loss: 18.60296630859375 = 1.8957815170288086 + 2.0 * 8.353591918945312
Epoch 30, val loss: 1.8934252262115479
Epoch 40, training loss: 18.334299087524414 = 1.8751062154769897 + 2.0 * 8.229596138000488
Epoch 40, val loss: 1.8742916584014893
Epoch 50, training loss: 17.474018096923828 = 1.8526809215545654 + 2.0 * 7.810668468475342
Epoch 50, val loss: 1.853214979171753
Epoch 60, training loss: 16.767309188842773 = 1.833877682685852 + 2.0 * 7.4667158126831055
Epoch 60, val loss: 1.8358467817306519
Epoch 70, training loss: 16.052942276000977 = 1.8205734491348267 + 2.0 * 7.116184234619141
Epoch 70, val loss: 1.8238826990127563
Epoch 80, training loss: 15.595179557800293 = 1.8080929517745972 + 2.0 * 6.893543243408203
Epoch 80, val loss: 1.8130420446395874
Epoch 90, training loss: 15.259186744689941 = 1.7941213846206665 + 2.0 * 6.732532501220703
Epoch 90, val loss: 1.801903486251831
Epoch 100, training loss: 15.038812637329102 = 1.7801733016967773 + 2.0 * 6.629319667816162
Epoch 100, val loss: 1.7904783487319946
Epoch 110, training loss: 14.884140014648438 = 1.7650787830352783 + 2.0 * 6.559530735015869
Epoch 110, val loss: 1.7774678468704224
Epoch 120, training loss: 14.763811111450195 = 1.7486927509307861 + 2.0 * 6.507559299468994
Epoch 120, val loss: 1.763210654258728
Epoch 130, training loss: 14.661975860595703 = 1.7310913801193237 + 2.0 * 6.465442180633545
Epoch 130, val loss: 1.7478147745132446
Epoch 140, training loss: 14.568305015563965 = 1.7119789123535156 + 2.0 * 6.428163051605225
Epoch 140, val loss: 1.731255292892456
Epoch 150, training loss: 14.481961250305176 = 1.6907873153686523 + 2.0 * 6.395586967468262
Epoch 150, val loss: 1.7132083177566528
Epoch 160, training loss: 14.405280113220215 = 1.666943907737732 + 2.0 * 6.369168281555176
Epoch 160, val loss: 1.6931350231170654
Epoch 170, training loss: 14.33002758026123 = 1.6403656005859375 + 2.0 * 6.3448309898376465
Epoch 170, val loss: 1.6709662675857544
Epoch 180, training loss: 14.26035213470459 = 1.6108132600784302 + 2.0 * 6.324769496917725
Epoch 180, val loss: 1.6463520526885986
Epoch 190, training loss: 14.195199012756348 = 1.5781408548355103 + 2.0 * 6.308528900146484
Epoch 190, val loss: 1.6192424297332764
Epoch 200, training loss: 14.123363494873047 = 1.5426921844482422 + 2.0 * 6.290335655212402
Epoch 200, val loss: 1.5901188850402832
Epoch 210, training loss: 14.058036804199219 = 1.5048696994781494 + 2.0 * 6.276583671569824
Epoch 210, val loss: 1.559326410293579
Epoch 220, training loss: 13.992292404174805 = 1.464939832687378 + 2.0 * 6.263676166534424
Epoch 220, val loss: 1.5271762609481812
Epoch 230, training loss: 13.930272102355957 = 1.4233583211898804 + 2.0 * 6.253457069396973
Epoch 230, val loss: 1.494210958480835
Epoch 240, training loss: 13.864969253540039 = 1.3811883926391602 + 2.0 * 6.2418904304504395
Epoch 240, val loss: 1.4612308740615845
Epoch 250, training loss: 13.807633399963379 = 1.3387278318405151 + 2.0 * 6.234452724456787
Epoch 250, val loss: 1.4287079572677612
Epoch 260, training loss: 13.742379188537598 = 1.2965563535690308 + 2.0 * 6.222911357879639
Epoch 260, val loss: 1.3969829082489014
Epoch 270, training loss: 13.689498901367188 = 1.2545955181121826 + 2.0 * 6.217451572418213
Epoch 270, val loss: 1.3661270141601562
Epoch 280, training loss: 13.627846717834473 = 1.2134056091308594 + 2.0 * 6.207220554351807
Epoch 280, val loss: 1.3363096714019775
Epoch 290, training loss: 13.571805000305176 = 1.1727896928787231 + 2.0 * 6.199507713317871
Epoch 290, val loss: 1.3072530031204224
Epoch 300, training loss: 13.517839431762695 = 1.1322368383407593 + 2.0 * 6.192801475524902
Epoch 300, val loss: 1.2785999774932861
Epoch 310, training loss: 13.474617958068848 = 1.0915563106536865 + 2.0 * 6.191530704498291
Epoch 310, val loss: 1.250174641609192
Epoch 320, training loss: 13.413654327392578 = 1.051037073135376 + 2.0 * 6.181308746337891
Epoch 320, val loss: 1.2219619750976562
Epoch 330, training loss: 13.361123085021973 = 1.0106103420257568 + 2.0 * 6.175256252288818
Epoch 330, val loss: 1.1940836906433105
Epoch 340, training loss: 13.311580657958984 = 0.9704406261444092 + 2.0 * 6.170569896697998
Epoch 340, val loss: 1.1667741537094116
Epoch 350, training loss: 13.263811111450195 = 0.9305234551429749 + 2.0 * 6.1666436195373535
Epoch 350, val loss: 1.1399650573730469
Epoch 360, training loss: 13.218229293823242 = 0.8910737037658691 + 2.0 * 6.163578033447266
Epoch 360, val loss: 1.1138999462127686
Epoch 370, training loss: 13.171856880187988 = 0.852493166923523 + 2.0 * 6.159681797027588
Epoch 370, val loss: 1.0887774229049683
Epoch 380, training loss: 13.11988639831543 = 0.8147833943367004 + 2.0 * 6.152551651000977
Epoch 380, val loss: 1.0646814107894897
Epoch 390, training loss: 13.077167510986328 = 0.7779958248138428 + 2.0 * 6.149585723876953
Epoch 390, val loss: 1.041642665863037
Epoch 400, training loss: 13.033563613891602 = 0.7423257827758789 + 2.0 * 6.145618915557861
Epoch 400, val loss: 1.0198087692260742
Epoch 410, training loss: 12.99121379852295 = 0.7078858613967896 + 2.0 * 6.141664028167725
Epoch 410, val loss: 0.9992644190788269
Epoch 420, training loss: 12.952850341796875 = 0.6748810410499573 + 2.0 * 6.138984680175781
Epoch 420, val loss: 0.9800726771354675
Epoch 430, training loss: 12.91453742980957 = 0.6433244347572327 + 2.0 * 6.135606288909912
Epoch 430, val loss: 0.9622552990913391
Epoch 440, training loss: 12.8754243850708 = 0.613309919834137 + 2.0 * 6.131057262420654
Epoch 440, val loss: 0.9459860920906067
Epoch 450, training loss: 12.846489906311035 = 0.5847824215888977 + 2.0 * 6.130853652954102
Epoch 450, val loss: 0.9311822652816772
Epoch 460, training loss: 12.814544677734375 = 0.5578402876853943 + 2.0 * 6.128352165222168
Epoch 460, val loss: 0.9178603291511536
Epoch 470, training loss: 12.776412010192871 = 0.5325305461883545 + 2.0 * 6.121940612792969
Epoch 470, val loss: 0.9059818983078003
Epoch 480, training loss: 12.745279312133789 = 0.5085666179656982 + 2.0 * 6.118356227874756
Epoch 480, val loss: 0.8954120874404907
Epoch 490, training loss: 12.737239837646484 = 0.4857105016708374 + 2.0 * 6.125764846801758
Epoch 490, val loss: 0.8859251141548157
Epoch 500, training loss: 12.691895484924316 = 0.46385905146598816 + 2.0 * 6.114018440246582
Epoch 500, val loss: 0.8775244951248169
Epoch 510, training loss: 12.665462493896484 = 0.4429491460323334 + 2.0 * 6.1112565994262695
Epoch 510, val loss: 0.8699818253517151
Epoch 520, training loss: 12.638640403747559 = 0.4227268099784851 + 2.0 * 6.107956886291504
Epoch 520, val loss: 0.8631706833839417
Epoch 530, training loss: 12.630254745483398 = 0.40302780270576477 + 2.0 * 6.113613605499268
Epoch 530, val loss: 0.8570782542228699
Epoch 540, training loss: 12.595438957214355 = 0.3839130699634552 + 2.0 * 6.105762958526611
Epoch 540, val loss: 0.8512950539588928
Epoch 550, training loss: 12.570316314697266 = 0.36520907282829285 + 2.0 * 6.102553844451904
Epoch 550, val loss: 0.8461266756057739
Epoch 560, training loss: 12.5501070022583 = 0.34692487120628357 + 2.0 * 6.101591110229492
Epoch 560, val loss: 0.8415629863739014
Epoch 570, training loss: 12.526156425476074 = 0.32909220457077026 + 2.0 * 6.098532199859619
Epoch 570, val loss: 0.8373895883560181
Epoch 580, training loss: 12.506932258605957 = 0.311771422624588 + 2.0 * 6.097580432891846
Epoch 580, val loss: 0.8337182402610779
Epoch 590, training loss: 12.483282089233398 = 0.29500705003738403 + 2.0 * 6.094137668609619
Epoch 590, val loss: 0.8304633498191833
Epoch 600, training loss: 12.464004516601562 = 0.278766393661499 + 2.0 * 6.092618942260742
Epoch 600, val loss: 0.8278772830963135
Epoch 610, training loss: 12.4483642578125 = 0.26317405700683594 + 2.0 * 6.092595100402832
Epoch 610, val loss: 0.8258214592933655
Epoch 620, training loss: 12.425644874572754 = 0.2482859045267105 + 2.0 * 6.088679313659668
Epoch 620, val loss: 0.8243341445922852
Epoch 630, training loss: 12.423055648803711 = 0.23411524295806885 + 2.0 * 6.094470024108887
Epoch 630, val loss: 0.8234416246414185
Epoch 640, training loss: 12.398571968078613 = 0.22077786922454834 + 2.0 * 6.088897228240967
Epoch 640, val loss: 0.8231859803199768
Epoch 650, training loss: 12.37759780883789 = 0.20822669565677643 + 2.0 * 6.084685325622559
Epoch 650, val loss: 0.8233612179756165
Epoch 660, training loss: 12.359292030334473 = 0.19642607867717743 + 2.0 * 6.081432819366455
Epoch 660, val loss: 0.8242002129554749
Epoch 670, training loss: 12.360133171081543 = 0.18537765741348267 + 2.0 * 6.087377548217773
Epoch 670, val loss: 0.8256554007530212
Epoch 680, training loss: 12.343559265136719 = 0.17503713071346283 + 2.0 * 6.084260940551758
Epoch 680, val loss: 0.8274644613265991
Epoch 690, training loss: 12.32108211517334 = 0.16544407606124878 + 2.0 * 6.077818870544434
Epoch 690, val loss: 0.8297855854034424
Epoch 700, training loss: 12.308001518249512 = 0.15650366246700287 + 2.0 * 6.075748920440674
Epoch 700, val loss: 0.8326783776283264
Epoch 710, training loss: 12.301984786987305 = 0.14817114174365997 + 2.0 * 6.076906681060791
Epoch 710, val loss: 0.8359235525131226
Epoch 720, training loss: 12.302577018737793 = 0.14038361608982086 + 2.0 * 6.081096649169922
Epoch 720, val loss: 0.8397014737129211
Epoch 730, training loss: 12.279502868652344 = 0.13320665061473846 + 2.0 * 6.073148250579834
Epoch 730, val loss: 0.8435459733009338
Epoch 740, training loss: 12.26817512512207 = 0.12654155492782593 + 2.0 * 6.070816993713379
Epoch 740, val loss: 0.8477600812911987
Epoch 750, training loss: 12.26001262664795 = 0.12032487243413925 + 2.0 * 6.069843769073486
Epoch 750, val loss: 0.8523126244544983
Epoch 760, training loss: 12.259021759033203 = 0.1145218014717102 + 2.0 * 6.072249889373779
Epoch 760, val loss: 0.8571630716323853
Epoch 770, training loss: 12.24691104888916 = 0.10909523814916611 + 2.0 * 6.068907737731934
Epoch 770, val loss: 0.8620907664299011
Epoch 780, training loss: 12.236403465270996 = 0.10405510663986206 + 2.0 * 6.066174030303955
Epoch 780, val loss: 0.8673145174980164
Epoch 790, training loss: 12.233357429504395 = 0.09932983666658401 + 2.0 * 6.067013740539551
Epoch 790, val loss: 0.87264084815979
Epoch 800, training loss: 12.225669860839844 = 0.09488919377326965 + 2.0 * 6.065390110015869
Epoch 800, val loss: 0.8781235814094543
Epoch 810, training loss: 12.21599292755127 = 0.09074459224939346 + 2.0 * 6.062623977661133
Epoch 810, val loss: 0.883602499961853
Epoch 820, training loss: 12.21066951751709 = 0.08684113621711731 + 2.0 * 6.061913967132568
Epoch 820, val loss: 0.8892409205436707
Epoch 830, training loss: 12.210676193237305 = 0.08317362517118454 + 2.0 * 6.063751220703125
Epoch 830, val loss: 0.8950755000114441
Epoch 840, training loss: 12.20181655883789 = 0.07971066236495972 + 2.0 * 6.0610527992248535
Epoch 840, val loss: 0.9008359909057617
Epoch 850, training loss: 12.193713188171387 = 0.07646007090806961 + 2.0 * 6.058626651763916
Epoch 850, val loss: 0.9067256450653076
Epoch 860, training loss: 12.191603660583496 = 0.07338052988052368 + 2.0 * 6.059111595153809
Epoch 860, val loss: 0.912639319896698
Epoch 870, training loss: 12.185345649719238 = 0.07048049569129944 + 2.0 * 6.057432651519775
Epoch 870, val loss: 0.9185172319412231
Epoch 880, training loss: 12.179732322692871 = 0.06773237138986588 + 2.0 * 6.055999755859375
Epoch 880, val loss: 0.9244489073753357
Epoch 890, training loss: 12.175131797790527 = 0.0651361495256424 + 2.0 * 6.05499792098999
Epoch 890, val loss: 0.9304271936416626
Epoch 900, training loss: 12.179101943969727 = 0.06267255544662476 + 2.0 * 6.0582146644592285
Epoch 900, val loss: 0.93639075756073
Epoch 910, training loss: 12.166428565979004 = 0.06033506989479065 + 2.0 * 6.053046703338623
Epoch 910, val loss: 0.9424006938934326
Epoch 920, training loss: 12.164735794067383 = 0.05811503902077675 + 2.0 * 6.053310394287109
Epoch 920, val loss: 0.9482855796813965
Epoch 930, training loss: 12.163330078125 = 0.05601551756262779 + 2.0 * 6.053657054901123
Epoch 930, val loss: 0.9543555378913879
Epoch 940, training loss: 12.160351753234863 = 0.054003745317459106 + 2.0 * 6.053174018859863
Epoch 940, val loss: 0.9604008197784424
Epoch 950, training loss: 12.151257514953613 = 0.052102744579315186 + 2.0 * 6.049577236175537
Epoch 950, val loss: 0.9663310647010803
Epoch 960, training loss: 12.154960632324219 = 0.05029271915555 + 2.0 * 6.052333831787109
Epoch 960, val loss: 0.9722222089767456
Epoch 970, training loss: 12.145136833190918 = 0.04856720194220543 + 2.0 * 6.048285007476807
Epoch 970, val loss: 0.9782378077507019
Epoch 980, training loss: 12.141436576843262 = 0.046924181282520294 + 2.0 * 6.047255992889404
Epoch 980, val loss: 0.9840146899223328
Epoch 990, training loss: 12.139484405517578 = 0.045361608266830444 + 2.0 * 6.047061443328857
Epoch 990, val loss: 0.9899749755859375
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8148
Overall ASR: 0.6273
Flip ASR: 0.5556/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.708646774291992 = 1.9607740640640259 + 2.0 * 8.373936653137207
Epoch 0, val loss: 1.9661461114883423
Epoch 10, training loss: 18.697620391845703 = 1.9501792192459106 + 2.0 * 8.373720169067383
Epoch 10, val loss: 1.955078125
Epoch 20, training loss: 18.681880950927734 = 1.9373931884765625 + 2.0 * 8.372243881225586
Epoch 20, val loss: 1.9414987564086914
Epoch 30, training loss: 18.642745971679688 = 1.9199254512786865 + 2.0 * 8.361410140991211
Epoch 30, val loss: 1.9227547645568848
Epoch 40, training loss: 18.485761642456055 = 1.8969082832336426 + 2.0 * 8.294426918029785
Epoch 40, val loss: 1.8986461162567139
Epoch 50, training loss: 17.71196746826172 = 1.87209951877594 + 2.0 * 7.919934272766113
Epoch 50, val loss: 1.8737459182739258
Epoch 60, training loss: 16.70082664489746 = 1.850969910621643 + 2.0 * 7.424928188323975
Epoch 60, val loss: 1.8533858060836792
Epoch 70, training loss: 15.853602409362793 = 1.8351354598999023 + 2.0 * 7.009233474731445
Epoch 70, val loss: 1.8372031450271606
Epoch 80, training loss: 15.456128120422363 = 1.8197048902511597 + 2.0 * 6.818211555480957
Epoch 80, val loss: 1.8216075897216797
Epoch 90, training loss: 15.20158576965332 = 1.80324125289917 + 2.0 * 6.699172496795654
Epoch 90, val loss: 1.8049094676971436
Epoch 100, training loss: 15.012626647949219 = 1.785523533821106 + 2.0 * 6.613551616668701
Epoch 100, val loss: 1.7871174812316895
Epoch 110, training loss: 14.854413986206055 = 1.7671685218811035 + 2.0 * 6.543622970581055
Epoch 110, val loss: 1.7692246437072754
Epoch 120, training loss: 14.730592727661133 = 1.7486305236816406 + 2.0 * 6.490981101989746
Epoch 120, val loss: 1.7513153553009033
Epoch 130, training loss: 14.623754501342773 = 1.7290284633636475 + 2.0 * 6.447362899780273
Epoch 130, val loss: 1.7326151132583618
Epoch 140, training loss: 14.528565406799316 = 1.707592248916626 + 2.0 * 6.410486698150635
Epoch 140, val loss: 1.7129310369491577
Epoch 150, training loss: 14.434224128723145 = 1.6843452453613281 + 2.0 * 6.374939441680908
Epoch 150, val loss: 1.6920993328094482
Epoch 160, training loss: 14.351730346679688 = 1.658507227897644 + 2.0 * 6.346611499786377
Epoch 160, val loss: 1.669567346572876
Epoch 170, training loss: 14.272659301757812 = 1.6295514106750488 + 2.0 * 6.321554183959961
Epoch 170, val loss: 1.644888162612915
Epoch 180, training loss: 14.196087837219238 = 1.5973615646362305 + 2.0 * 6.299363136291504
Epoch 180, val loss: 1.617867112159729
Epoch 190, training loss: 14.12783432006836 = 1.5617218017578125 + 2.0 * 6.283056259155273
Epoch 190, val loss: 1.588498592376709
Epoch 200, training loss: 14.05618953704834 = 1.522877812385559 + 2.0 * 6.266655921936035
Epoch 200, val loss: 1.5570886135101318
Epoch 210, training loss: 13.987060546875 = 1.4809508323669434 + 2.0 * 6.253054618835449
Epoch 210, val loss: 1.5237269401550293
Epoch 220, training loss: 13.919156074523926 = 1.4361413717269897 + 2.0 * 6.241507530212402
Epoch 220, val loss: 1.4887958765029907
Epoch 230, training loss: 13.858671188354492 = 1.3894323110580444 + 2.0 * 6.234619617462158
Epoch 230, val loss: 1.4529914855957031
Epoch 240, training loss: 13.78372859954834 = 1.3420172929763794 + 2.0 * 6.220855712890625
Epoch 240, val loss: 1.4173527956008911
Epoch 250, training loss: 13.719685554504395 = 1.2939841747283936 + 2.0 * 6.212850570678711
Epoch 250, val loss: 1.381919026374817
Epoch 260, training loss: 13.662449836730957 = 1.2458457946777344 + 2.0 * 6.208302021026611
Epoch 260, val loss: 1.3469938039779663
Epoch 270, training loss: 13.59365177154541 = 1.1985300779342651 + 2.0 * 6.197560787200928
Epoch 270, val loss: 1.3128517866134644
Epoch 280, training loss: 13.535497665405273 = 1.15205717086792 + 2.0 * 6.191720008850098
Epoch 280, val loss: 1.2796310186386108
Epoch 290, training loss: 13.481587409973145 = 1.1066752672195435 + 2.0 * 6.187456130981445
Epoch 290, val loss: 1.2471994161605835
Epoch 300, training loss: 13.422907829284668 = 1.0625113248825073 + 2.0 * 6.1801981925964355
Epoch 300, val loss: 1.2157121896743774
Epoch 310, training loss: 13.371760368347168 = 1.0197473764419556 + 2.0 * 6.176006317138672
Epoch 310, val loss: 1.1848962306976318
Epoch 320, training loss: 13.317512512207031 = 0.9783364534378052 + 2.0 * 6.169588088989258
Epoch 320, val loss: 1.155068278312683
Epoch 330, training loss: 13.266669273376465 = 0.9384559988975525 + 2.0 * 6.164106845855713
Epoch 330, val loss: 1.126205325126648
Epoch 340, training loss: 13.227629661560059 = 0.8999109268188477 + 2.0 * 6.1638593673706055
Epoch 340, val loss: 1.0984386205673218
Epoch 350, training loss: 13.174897193908691 = 0.863235354423523 + 2.0 * 6.1558308601379395
Epoch 350, val loss: 1.071951985359192
Epoch 360, training loss: 13.13017749786377 = 0.8279990553855896 + 2.0 * 6.151089191436768
Epoch 360, val loss: 1.0468333959579468
Epoch 370, training loss: 13.087423324584961 = 0.7942115664482117 + 2.0 * 6.146605968475342
Epoch 370, val loss: 1.0230568647384644
Epoch 380, training loss: 13.055559158325195 = 0.7619782090187073 + 2.0 * 6.146790504455566
Epoch 380, val loss: 1.000701904296875
Epoch 390, training loss: 13.012149810791016 = 0.7316770553588867 + 2.0 * 6.1402363777160645
Epoch 390, val loss: 0.9800626635551453
Epoch 400, training loss: 12.97592544555664 = 0.7029776573181152 + 2.0 * 6.136473655700684
Epoch 400, val loss: 0.9609793424606323
Epoch 410, training loss: 12.94746208190918 = 0.6757538318634033 + 2.0 * 6.135854244232178
Epoch 410, val loss: 0.9432767629623413
Epoch 420, training loss: 12.911498069763184 = 0.6500412225723267 + 2.0 * 6.130728244781494
Epoch 420, val loss: 0.9269469976425171
Epoch 430, training loss: 12.88113021850586 = 0.6255630254745483 + 2.0 * 6.12778377532959
Epoch 430, val loss: 0.9117710590362549
Epoch 440, training loss: 12.860694885253906 = 0.6020156145095825 + 2.0 * 6.129339694976807
Epoch 440, val loss: 0.8975836038589478
Epoch 450, training loss: 12.825284957885742 = 0.5792450904846191 + 2.0 * 6.123020172119141
Epoch 450, val loss: 0.8842061758041382
Epoch 460, training loss: 12.796059608459473 = 0.5572644472122192 + 2.0 * 6.1193976402282715
Epoch 460, val loss: 0.8714576363563538
Epoch 470, training loss: 12.770197868347168 = 0.5357983708381653 + 2.0 * 6.117199897766113
Epoch 470, val loss: 0.8593304753303528
Epoch 480, training loss: 12.746574401855469 = 0.5150209069252014 + 2.0 * 6.115776538848877
Epoch 480, val loss: 0.8476866483688354
Epoch 490, training loss: 12.721683502197266 = 0.4947567880153656 + 2.0 * 6.113463401794434
Epoch 490, val loss: 0.8366526961326599
Epoch 500, training loss: 12.694075584411621 = 0.4748867154121399 + 2.0 * 6.109594345092773
Epoch 500, val loss: 0.826048731803894
Epoch 510, training loss: 12.681264877319336 = 0.45517614483833313 + 2.0 * 6.113044261932373
Epoch 510, val loss: 0.8157686591148376
Epoch 520, training loss: 12.650764465332031 = 0.43585404753685 + 2.0 * 6.107455253601074
Epoch 520, val loss: 0.8056186437606812
Epoch 530, training loss: 12.624566078186035 = 0.41664642095565796 + 2.0 * 6.103960037231445
Epoch 530, val loss: 0.796000063419342
Epoch 540, training loss: 12.608238220214844 = 0.3976288437843323 + 2.0 * 6.105304718017578
Epoch 540, val loss: 0.7867175936698914
Epoch 550, training loss: 12.580599784851074 = 0.3788316547870636 + 2.0 * 6.100883960723877
Epoch 550, val loss: 0.7776398658752441
Epoch 560, training loss: 12.556694030761719 = 0.3602152168750763 + 2.0 * 6.098239421844482
Epoch 560, val loss: 0.7690022587776184
Epoch 570, training loss: 12.54938793182373 = 0.34193089604377747 + 2.0 * 6.103728294372559
Epoch 570, val loss: 0.7607986927032471
Epoch 580, training loss: 12.520450592041016 = 0.32409465312957764 + 2.0 * 6.098177909851074
Epoch 580, val loss: 0.753322422504425
Epoch 590, training loss: 12.493437767028809 = 0.30676335096359253 + 2.0 * 6.093337059020996
Epoch 590, val loss: 0.7465313673019409
Epoch 600, training loss: 12.473941802978516 = 0.2899409830570221 + 2.0 * 6.092000484466553
Epoch 600, val loss: 0.740459680557251
Epoch 610, training loss: 12.454378128051758 = 0.273722767829895 + 2.0 * 6.090327739715576
Epoch 610, val loss: 0.7350742220878601
Epoch 620, training loss: 12.449300765991211 = 0.2581411898136139 + 2.0 * 6.095579624176025
Epoch 620, val loss: 0.7305478453636169
Epoch 630, training loss: 12.423184394836426 = 0.24319717288017273 + 2.0 * 6.089993476867676
Epoch 630, val loss: 0.7269306182861328
Epoch 640, training loss: 12.405261039733887 = 0.22909265756607056 + 2.0 * 6.0880842208862305
Epoch 640, val loss: 0.7239665389060974
Epoch 650, training loss: 12.385560989379883 = 0.21571528911590576 + 2.0 * 6.084922790527344
Epoch 650, val loss: 0.7218422889709473
Epoch 660, training loss: 12.37121295928955 = 0.20313909649848938 + 2.0 * 6.084036827087402
Epoch 660, val loss: 0.7205322980880737
Epoch 670, training loss: 12.362850189208984 = 0.1913669854402542 + 2.0 * 6.0857415199279785
Epoch 670, val loss: 0.7198949456214905
Epoch 680, training loss: 12.343560218811035 = 0.18038254976272583 + 2.0 * 6.0815887451171875
Epoch 680, val loss: 0.7198647856712341
Epoch 690, training loss: 12.33110237121582 = 0.17013651132583618 + 2.0 * 6.0804829597473145
Epoch 690, val loss: 0.7206920385360718
Epoch 700, training loss: 12.327213287353516 = 0.16064120829105377 + 2.0 * 6.083285808563232
Epoch 700, val loss: 0.722029447555542
Epoch 710, training loss: 12.311524391174316 = 0.1518208235502243 + 2.0 * 6.0798516273498535
Epoch 710, val loss: 0.7240055799484253
Epoch 720, training loss: 12.297722816467285 = 0.14370863139629364 + 2.0 * 6.077007293701172
Epoch 720, val loss: 0.7263439893722534
Epoch 730, training loss: 12.286150932312012 = 0.1361737996339798 + 2.0 * 6.07498836517334
Epoch 730, val loss: 0.7292960286140442
Epoch 740, training loss: 12.293167114257812 = 0.12919506430625916 + 2.0 * 6.081985950469971
Epoch 740, val loss: 0.7326902747154236
Epoch 750, training loss: 12.279528617858887 = 0.12277642637491226 + 2.0 * 6.078376293182373
Epoch 750, val loss: 0.7362977862358093
Epoch 760, training loss: 12.261937141418457 = 0.11682439595460892 + 2.0 * 6.072556495666504
Epoch 760, val loss: 0.7401123642921448
Epoch 770, training loss: 12.253414154052734 = 0.11131938546895981 + 2.0 * 6.071047306060791
Epoch 770, val loss: 0.7444214224815369
Epoch 780, training loss: 12.245051383972168 = 0.10620816797018051 + 2.0 * 6.069421768188477
Epoch 780, val loss: 0.7489821910858154
Epoch 790, training loss: 12.241904258728027 = 0.10143866389989853 + 2.0 * 6.07023286819458
Epoch 790, val loss: 0.7537535429000854
Epoch 800, training loss: 12.231230735778809 = 0.09699584543704987 + 2.0 * 6.067117214202881
Epoch 800, val loss: 0.7586770057678223
Epoch 810, training loss: 12.244668006896973 = 0.09284727275371552 + 2.0 * 6.075910568237305
Epoch 810, val loss: 0.7637885212898254
Epoch 820, training loss: 12.223790168762207 = 0.08900948613882065 + 2.0 * 6.067390441894531
Epoch 820, val loss: 0.7688097953796387
Epoch 830, training loss: 12.215694427490234 = 0.08540156483650208 + 2.0 * 6.065146446228027
Epoch 830, val loss: 0.774046778678894
Epoch 840, training loss: 12.20907211303711 = 0.08201246708631516 + 2.0 * 6.063529968261719
Epoch 840, val loss: 0.7794959545135498
Epoch 850, training loss: 12.218080520629883 = 0.07881636917591095 + 2.0 * 6.069632053375244
Epoch 850, val loss: 0.7849317193031311
Epoch 860, training loss: 12.202348709106445 = 0.07580367475748062 + 2.0 * 6.063272476196289
Epoch 860, val loss: 0.7904930114746094
Epoch 870, training loss: 12.193826675415039 = 0.07295749336481094 + 2.0 * 6.060434818267822
Epoch 870, val loss: 0.7960162162780762
Epoch 880, training loss: 12.192628860473633 = 0.07026343047618866 + 2.0 * 6.061182498931885
Epoch 880, val loss: 0.8015806674957275
Epoch 890, training loss: 12.184937477111816 = 0.06769920140504837 + 2.0 * 6.058619022369385
Epoch 890, val loss: 0.8073076605796814
Epoch 900, training loss: 12.183554649353027 = 0.06526918709278107 + 2.0 * 6.059142589569092
Epoch 900, val loss: 0.8128445148468018
Epoch 910, training loss: 12.177762031555176 = 0.06295938789844513 + 2.0 * 6.057401180267334
Epoch 910, val loss: 0.818659782409668
Epoch 920, training loss: 12.172595024108887 = 0.06076158210635185 + 2.0 * 6.055916786193848
Epoch 920, val loss: 0.8243367075920105
Epoch 930, training loss: 12.180829048156738 = 0.05865641310811043 + 2.0 * 6.061086177825928
Epoch 930, val loss: 0.8300074934959412
Epoch 940, training loss: 12.173391342163086 = 0.05662842467427254 + 2.0 * 6.0583815574646
Epoch 940, val loss: 0.8358786702156067
Epoch 950, training loss: 12.16239070892334 = 0.05469378083944321 + 2.0 * 6.0538482666015625
Epoch 950, val loss: 0.8415444493293762
Epoch 960, training loss: 12.160199165344238 = 0.05283987522125244 + 2.0 * 6.053679466247559
Epoch 960, val loss: 0.8471591472625732
Epoch 970, training loss: 12.162516593933105 = 0.05105478689074516 + 2.0 * 6.055730819702148
Epoch 970, val loss: 0.8529083728790283
Epoch 980, training loss: 12.156654357910156 = 0.049333300441503525 + 2.0 * 6.0536603927612305
Epoch 980, val loss: 0.8588686585426331
Epoch 990, training loss: 12.154047966003418 = 0.04767522215843201 + 2.0 * 6.053186416625977
Epoch 990, val loss: 0.8645358681678772
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7852
Overall ASR: 0.6531
Flip ASR: 0.6000/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.688749313354492 = 1.9408643245697021 + 2.0 * 8.373942375183105
Epoch 0, val loss: 1.9334346055984497
Epoch 10, training loss: 18.678607940673828 = 1.9311413764953613 + 2.0 * 8.373733520507812
Epoch 10, val loss: 1.9242370128631592
Epoch 20, training loss: 18.663818359375 = 1.919129490852356 + 2.0 * 8.372344017028809
Epoch 20, val loss: 1.9124712944030762
Epoch 30, training loss: 18.624908447265625 = 1.9027886390686035 + 2.0 * 8.36106014251709
Epoch 30, val loss: 1.8961719274520874
Epoch 40, training loss: 18.42772102355957 = 1.8817623853683472 + 2.0 * 8.272979736328125
Epoch 40, val loss: 1.8757613897323608
Epoch 50, training loss: 17.22521209716797 = 1.8599185943603516 + 2.0 * 7.68264627456665
Epoch 50, val loss: 1.8553963899612427
Epoch 60, training loss: 16.173471450805664 = 1.842641830444336 + 2.0 * 7.165414810180664
Epoch 60, val loss: 1.8405365943908691
Epoch 70, training loss: 15.558525085449219 = 1.829469084739685 + 2.0 * 6.864528179168701
Epoch 70, val loss: 1.8283705711364746
Epoch 80, training loss: 15.226649284362793 = 1.8154261112213135 + 2.0 * 6.705611705780029
Epoch 80, val loss: 1.815624713897705
Epoch 90, training loss: 15.035152435302734 = 1.8005439043045044 + 2.0 * 6.61730432510376
Epoch 90, val loss: 1.8023443222045898
Epoch 100, training loss: 14.901970863342285 = 1.784611463546753 + 2.0 * 6.558679580688477
Epoch 100, val loss: 1.7886641025543213
Epoch 110, training loss: 14.782858848571777 = 1.7688796520233154 + 2.0 * 6.506989479064941
Epoch 110, val loss: 1.7752588987350464
Epoch 120, training loss: 14.666860580444336 = 1.7532787322998047 + 2.0 * 6.456790924072266
Epoch 120, val loss: 1.76187002658844
Epoch 130, training loss: 14.56371021270752 = 1.7370538711547852 + 2.0 * 6.413328170776367
Epoch 130, val loss: 1.748067855834961
Epoch 140, training loss: 14.46750259399414 = 1.7193361520767212 + 2.0 * 6.374083042144775
Epoch 140, val loss: 1.7332487106323242
Epoch 150, training loss: 14.385307312011719 = 1.699414849281311 + 2.0 * 6.3429460525512695
Epoch 150, val loss: 1.7166413068771362
Epoch 160, training loss: 14.32058334350586 = 1.676915168762207 + 2.0 * 6.321834087371826
Epoch 160, val loss: 1.6979953050613403
Epoch 170, training loss: 14.247467994689941 = 1.6516731977462769 + 2.0 * 6.2978973388671875
Epoch 170, val loss: 1.6773244142532349
Epoch 180, training loss: 14.183453559875488 = 1.623353123664856 + 2.0 * 6.280050277709961
Epoch 180, val loss: 1.6542036533355713
Epoch 190, training loss: 14.121749877929688 = 1.5914337635040283 + 2.0 * 6.265158176422119
Epoch 190, val loss: 1.6283990144729614
Epoch 200, training loss: 14.06134033203125 = 1.555871844291687 + 2.0 * 6.252734184265137
Epoch 200, val loss: 1.6001628637313843
Epoch 210, training loss: 13.995147705078125 = 1.5169605016708374 + 2.0 * 6.239093780517578
Epoch 210, val loss: 1.5695830583572388
Epoch 220, training loss: 13.933060646057129 = 1.4747178554534912 + 2.0 * 6.229171276092529
Epoch 220, val loss: 1.5368807315826416
Epoch 230, training loss: 13.870553970336914 = 1.4297919273376465 + 2.0 * 6.220381259918213
Epoch 230, val loss: 1.502684473991394
Epoch 240, training loss: 13.803072929382324 = 1.3828098773956299 + 2.0 * 6.210131645202637
Epoch 240, val loss: 1.4677269458770752
Epoch 250, training loss: 13.739079475402832 = 1.334141492843628 + 2.0 * 6.2024688720703125
Epoch 250, val loss: 1.4322357177734375
Epoch 260, training loss: 13.683749198913574 = 1.2843416929244995 + 2.0 * 6.199703693389893
Epoch 260, val loss: 1.3966212272644043
Epoch 270, training loss: 13.615445137023926 = 1.2344015836715698 + 2.0 * 6.190521717071533
Epoch 270, val loss: 1.3615268468856812
Epoch 280, training loss: 13.55162239074707 = 1.184450387954712 + 2.0 * 6.183586120605469
Epoch 280, val loss: 1.3268431425094604
Epoch 290, training loss: 13.49993896484375 = 1.1346303224563599 + 2.0 * 6.18265438079834
Epoch 290, val loss: 1.292415738105774
Epoch 300, training loss: 13.432065963745117 = 1.085474967956543 + 2.0 * 6.173295497894287
Epoch 300, val loss: 1.2587668895721436
Epoch 310, training loss: 13.373666763305664 = 1.0372018814086914 + 2.0 * 6.168232440948486
Epoch 310, val loss: 1.2255817651748657
Epoch 320, training loss: 13.316407203674316 = 0.9897070527076721 + 2.0 * 6.1633501052856445
Epoch 320, val loss: 1.1927963495254517
Epoch 330, training loss: 13.262657165527344 = 0.9432700872421265 + 2.0 * 6.159693717956543
Epoch 330, val loss: 1.1606075763702393
Epoch 340, training loss: 13.208283424377441 = 0.8984469771385193 + 2.0 * 6.154918193817139
Epoch 340, val loss: 1.1293820142745972
Epoch 350, training loss: 13.1571626663208 = 0.8552290797233582 + 2.0 * 6.150966644287109
Epoch 350, val loss: 1.0992259979248047
Epoch 360, training loss: 13.1120023727417 = 0.8136804699897766 + 2.0 * 6.149160861968994
Epoch 360, val loss: 1.070165753364563
Epoch 370, training loss: 13.066133499145508 = 0.7739478945732117 + 2.0 * 6.146092891693115
Epoch 370, val loss: 1.042575716972351
Epoch 380, training loss: 13.021388053894043 = 0.7363420724868774 + 2.0 * 6.142522811889648
Epoch 380, val loss: 1.016519546508789
Epoch 390, training loss: 12.974998474121094 = 0.7006663680076599 + 2.0 * 6.1371660232543945
Epoch 390, val loss: 0.9920562505722046
Epoch 400, training loss: 12.932425498962402 = 0.6665861010551453 + 2.0 * 6.132919788360596
Epoch 400, val loss: 0.968971848487854
Epoch 410, training loss: 12.896856307983398 = 0.6338896751403809 + 2.0 * 6.13148307800293
Epoch 410, val loss: 0.9472066164016724
Epoch 420, training loss: 12.87299919128418 = 0.6026932001113892 + 2.0 * 6.135152816772461
Epoch 420, val loss: 0.9269480109214783
Epoch 430, training loss: 12.824962615966797 = 0.5728791356086731 + 2.0 * 6.126041889190674
Epoch 430, val loss: 0.9079582095146179
Epoch 440, training loss: 12.786602973937988 = 0.5441983938217163 + 2.0 * 6.12120246887207
Epoch 440, val loss: 0.8903367519378662
Epoch 450, training loss: 12.753108024597168 = 0.5164662599563599 + 2.0 * 6.118320941925049
Epoch 450, val loss: 0.8738861680030823
Epoch 460, training loss: 12.736013412475586 = 0.48954644799232483 + 2.0 * 6.123233318328857
Epoch 460, val loss: 0.8585712909698486
Epoch 470, training loss: 12.691190719604492 = 0.4637206494808197 + 2.0 * 6.113735198974609
Epoch 470, val loss: 0.8442622423171997
Epoch 480, training loss: 12.661701202392578 = 0.43877968192100525 + 2.0 * 6.1114606857299805
Epoch 480, val loss: 0.8312206268310547
Epoch 490, training loss: 12.636627197265625 = 0.41471531987190247 + 2.0 * 6.110955715179443
Epoch 490, val loss: 0.8193469047546387
Epoch 500, training loss: 12.61431884765625 = 0.3916352391242981 + 2.0 * 6.111341953277588
Epoch 500, val loss: 0.808558464050293
Epoch 510, training loss: 12.580554962158203 = 0.36963143944740295 + 2.0 * 6.105461597442627
Epoch 510, val loss: 0.798858106136322
Epoch 520, training loss: 12.552801132202148 = 0.348675400018692 + 2.0 * 6.102062702178955
Epoch 520, val loss: 0.790274977684021
Epoch 530, training loss: 12.530512809753418 = 0.32873111963272095 + 2.0 * 6.100890636444092
Epoch 530, val loss: 0.7826584577560425
Epoch 540, training loss: 12.5057954788208 = 0.30986863374710083 + 2.0 * 6.097963333129883
Epoch 540, val loss: 0.7760653495788574
Epoch 550, training loss: 12.502204895019531 = 0.29208672046661377 + 2.0 * 6.1050591468811035
Epoch 550, val loss: 0.7702960968017578
Epoch 560, training loss: 12.465576171875 = 0.27546435594558716 + 2.0 * 6.095056056976318
Epoch 560, val loss: 0.7654918432235718
Epoch 570, training loss: 12.447664260864258 = 0.2599487602710724 + 2.0 * 6.093857765197754
Epoch 570, val loss: 0.7615824937820435
Epoch 580, training loss: 12.426859855651855 = 0.24548348784446716 + 2.0 * 6.090688228607178
Epoch 580, val loss: 0.7584516406059265
Epoch 590, training loss: 12.4129056930542 = 0.23195581138134003 + 2.0 * 6.090475082397461
Epoch 590, val loss: 0.7560080289840698
Epoch 600, training loss: 12.40180778503418 = 0.21933023631572723 + 2.0 * 6.091238975524902
Epoch 600, val loss: 0.7542689442634583
Epoch 610, training loss: 12.386255264282227 = 0.2075725644826889 + 2.0 * 6.089341163635254
Epoch 610, val loss: 0.7531843185424805
Epoch 620, training loss: 12.36398983001709 = 0.19668826460838318 + 2.0 * 6.083650588989258
Epoch 620, val loss: 0.7527164220809937
Epoch 630, training loss: 12.351105690002441 = 0.18652400374412537 + 2.0 * 6.0822906494140625
Epoch 630, val loss: 0.7528678178787231
Epoch 640, training loss: 12.338926315307617 = 0.1769767701625824 + 2.0 * 6.080974578857422
Epoch 640, val loss: 0.7535149455070496
Epoch 650, training loss: 12.335699081420898 = 0.16802416741847992 + 2.0 * 6.083837509155273
Epoch 650, val loss: 0.7545286417007446
Epoch 660, training loss: 12.322077751159668 = 0.15969187021255493 + 2.0 * 6.081192970275879
Epoch 660, val loss: 0.7558622360229492
Epoch 670, training loss: 12.305214881896973 = 0.15194909274578094 + 2.0 * 6.076632976531982
Epoch 670, val loss: 0.7577499151229858
Epoch 680, training loss: 12.294301986694336 = 0.14465545117855072 + 2.0 * 6.074823379516602
Epoch 680, val loss: 0.7600853443145752
Epoch 690, training loss: 12.284624099731445 = 0.1377532035112381 + 2.0 * 6.073435306549072
Epoch 690, val loss: 0.7627624273300171
Epoch 700, training loss: 12.285502433776855 = 0.13121861219406128 + 2.0 * 6.077141761779785
Epoch 700, val loss: 0.765673041343689
Epoch 710, training loss: 12.275400161743164 = 0.12508903443813324 + 2.0 * 6.075155735015869
Epoch 710, val loss: 0.7689680457115173
Epoch 720, training loss: 12.258630752563477 = 0.11932412534952164 + 2.0 * 6.069653511047363
Epoch 720, val loss: 0.7724941968917847
Epoch 730, training loss: 12.251768112182617 = 0.1138799786567688 + 2.0 * 6.068943977355957
Epoch 730, val loss: 0.7763074040412903
Epoch 740, training loss: 12.262577056884766 = 0.10874325782060623 + 2.0 * 6.076916694641113
Epoch 740, val loss: 0.7803117632865906
Epoch 750, training loss: 12.244178771972656 = 0.10392852872610092 + 2.0 * 6.070125102996826
Epoch 750, val loss: 0.7845433950424194
Epoch 760, training loss: 12.233765602111816 = 0.09942004084587097 + 2.0 * 6.067173004150391
Epoch 760, val loss: 0.7890303134918213
Epoch 770, training loss: 12.22324275970459 = 0.09515251964330673 + 2.0 * 6.064044952392578
Epoch 770, val loss: 0.7937573194503784
Epoch 780, training loss: 12.217658042907715 = 0.0911056399345398 + 2.0 * 6.063276290893555
Epoch 780, val loss: 0.7986055612564087
Epoch 790, training loss: 12.219062805175781 = 0.08726103603839874 + 2.0 * 6.065900802612305
Epoch 790, val loss: 0.8035721778869629
Epoch 800, training loss: 12.21211051940918 = 0.08362918347120285 + 2.0 * 6.064240455627441
Epoch 800, val loss: 0.8086566925048828
Epoch 810, training loss: 12.202655792236328 = 0.08020991086959839 + 2.0 * 6.061223030090332
Epoch 810, val loss: 0.8138830661773682
Epoch 820, training loss: 12.193829536437988 = 0.0769517719745636 + 2.0 * 6.058438777923584
Epoch 820, val loss: 0.8192761540412903
Epoch 830, training loss: 12.194596290588379 = 0.07385660707950592 + 2.0 * 6.060369968414307
Epoch 830, val loss: 0.8247774243354797
Epoch 840, training loss: 12.19284439086914 = 0.07093207538127899 + 2.0 * 6.060956001281738
Epoch 840, val loss: 0.8303740620613098
Epoch 850, training loss: 12.185711860656738 = 0.06814451515674591 + 2.0 * 6.058783531188965
Epoch 850, val loss: 0.8360493779182434
Epoch 860, training loss: 12.178784370422363 = 0.06551141291856766 + 2.0 * 6.056636333465576
Epoch 860, val loss: 0.8418579697608948
Epoch 870, training loss: 12.1720609664917 = 0.06299740821123123 + 2.0 * 6.054531574249268
Epoch 870, val loss: 0.8476973176002502
Epoch 880, training loss: 12.172404289245605 = 0.06061229854822159 + 2.0 * 6.055895805358887
Epoch 880, val loss: 0.8535894155502319
Epoch 890, training loss: 12.168373107910156 = 0.05833544582128525 + 2.0 * 6.055018901824951
Epoch 890, val loss: 0.8595681190490723
Epoch 900, training loss: 12.158644676208496 = 0.05616653338074684 + 2.0 * 6.051239013671875
Epoch 900, val loss: 0.865527331829071
Epoch 910, training loss: 12.155694961547852 = 0.05410781130194664 + 2.0 * 6.050793647766113
Epoch 910, val loss: 0.8716190457344055
Epoch 920, training loss: 12.168755531311035 = 0.0521499402821064 + 2.0 * 6.058302879333496
Epoch 920, val loss: 0.8776906728744507
Epoch 930, training loss: 12.151412963867188 = 0.05029234662652016 + 2.0 * 6.050560474395752
Epoch 930, val loss: 0.8837219476699829
Epoch 940, training loss: 12.145461082458496 = 0.048522546887397766 + 2.0 * 6.048469066619873
Epoch 940, val loss: 0.8898670077323914
Epoch 950, training loss: 12.14034366607666 = 0.04683562368154526 + 2.0 * 6.046753883361816
Epoch 950, val loss: 0.8960672616958618
Epoch 960, training loss: 12.138687133789062 = 0.045218292623758316 + 2.0 * 6.04673433303833
Epoch 960, val loss: 0.9023154973983765
Epoch 970, training loss: 12.146157264709473 = 0.04367231950163841 + 2.0 * 6.051242351531982
Epoch 970, val loss: 0.9084230661392212
Epoch 980, training loss: 12.137389183044434 = 0.0422038696706295 + 2.0 * 6.047592639923096
Epoch 980, val loss: 0.9145479798316956
Epoch 990, training loss: 12.128678321838379 = 0.040812473744153976 + 2.0 * 6.043932914733887
Epoch 990, val loss: 0.9207302927970886
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8037
Overall ASR: 0.8930
Flip ASR: 0.8711/225 nodes
The final ASR:0.72448, 0.11962, Accuracy:0.80123, 0.01222
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11622])
remove edge: torch.Size([2, 9586])
updated graph: torch.Size([2, 10652])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
The final ASR:0.97540, 0.00174, Accuracy:0.83457, 0.00630
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.5 der2:0.3 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.3, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 18.681638717651367 = 1.9341256618499756 + 2.0 * 8.373756408691406
Epoch 0, val loss: 1.9318221807479858
Epoch 10, training loss: 18.67087745666504 = 1.9251492023468018 + 2.0 * 8.37286376953125
Epoch 10, val loss: 1.9234894514083862
Epoch 20, training loss: 18.64822006225586 = 1.9141606092453003 + 2.0 * 8.367030143737793
Epoch 20, val loss: 1.9127857685089111
Epoch 30, training loss: 18.550424575805664 = 1.8996249437332153 + 2.0 * 8.325399398803711
Epoch 30, val loss: 1.898376703262329
Epoch 40, training loss: 17.777774810791016 = 1.884074330329895 + 2.0 * 7.946849822998047
Epoch 40, val loss: 1.8832975625991821
Epoch 50, training loss: 16.10296058654785 = 1.8687753677368164 + 2.0 * 7.117092609405518
Epoch 50, val loss: 1.8689672946929932
Epoch 60, training loss: 15.481780052185059 = 1.8579769134521484 + 2.0 * 6.811901569366455
Epoch 60, val loss: 1.8594058752059937
Epoch 70, training loss: 15.097023010253906 = 1.8481999635696411 + 2.0 * 6.624411582946777
Epoch 70, val loss: 1.8503122329711914
Epoch 80, training loss: 14.86258602142334 = 1.839140772819519 + 2.0 * 6.511722564697266
Epoch 80, val loss: 1.8417435884475708
Epoch 90, training loss: 14.701735496520996 = 1.8305656909942627 + 2.0 * 6.435585021972656
Epoch 90, val loss: 1.8334052562713623
Epoch 100, training loss: 14.586413383483887 = 1.822583556175232 + 2.0 * 6.381915092468262
Epoch 100, val loss: 1.8257073163986206
Epoch 110, training loss: 14.488353729248047 = 1.8152796030044556 + 2.0 * 6.336536884307861
Epoch 110, val loss: 1.8186618089675903
Epoch 120, training loss: 14.407279014587402 = 1.8086642026901245 + 2.0 * 6.299307346343994
Epoch 120, val loss: 1.8121509552001953
Epoch 130, training loss: 14.342367172241211 = 1.8023667335510254 + 2.0 * 6.269999980926514
Epoch 130, val loss: 1.8059412240982056
Epoch 140, training loss: 14.288981437683105 = 1.7961925268173218 + 2.0 * 6.246394634246826
Epoch 140, val loss: 1.799946904182434
Epoch 150, training loss: 14.24355411529541 = 1.7898966073989868 + 2.0 * 6.226828575134277
Epoch 150, val loss: 1.7939993143081665
Epoch 160, training loss: 14.202561378479004 = 1.7833576202392578 + 2.0 * 6.209601879119873
Epoch 160, val loss: 1.788079023361206
Epoch 170, training loss: 14.16651725769043 = 1.7763690948486328 + 2.0 * 6.195074081420898
Epoch 170, val loss: 1.7819783687591553
Epoch 180, training loss: 14.133844375610352 = 1.7686848640441895 + 2.0 * 6.18257999420166
Epoch 180, val loss: 1.7754888534545898
Epoch 190, training loss: 14.100484848022461 = 1.760097861289978 + 2.0 * 6.170193672180176
Epoch 190, val loss: 1.7684465646743774
Epoch 200, training loss: 14.071977615356445 = 1.7503615617752075 + 2.0 * 6.160808086395264
Epoch 200, val loss: 1.7606768608093262
Epoch 210, training loss: 14.044232368469238 = 1.7393455505371094 + 2.0 * 6.1524434089660645
Epoch 210, val loss: 1.7519867420196533
Epoch 220, training loss: 14.013612747192383 = 1.7268025875091553 + 2.0 * 6.143404960632324
Epoch 220, val loss: 1.7421362400054932
Epoch 230, training loss: 13.984768867492676 = 1.7122856378555298 + 2.0 * 6.136241436004639
Epoch 230, val loss: 1.7308627367019653
Epoch 240, training loss: 13.955114364624023 = 1.6954255104064941 + 2.0 * 6.1298441886901855
Epoch 240, val loss: 1.7178685665130615
Epoch 250, training loss: 13.927345275878906 = 1.675940990447998 + 2.0 * 6.125701904296875
Epoch 250, val loss: 1.702881932258606
Epoch 260, training loss: 13.890966415405273 = 1.6533803939819336 + 2.0 * 6.11879301071167
Epoch 260, val loss: 1.6856375932693481
Epoch 270, training loss: 13.854314804077148 = 1.6272903680801392 + 2.0 * 6.11351203918457
Epoch 270, val loss: 1.6656244993209839
Epoch 280, training loss: 13.816276550292969 = 1.5970444679260254 + 2.0 * 6.109616279602051
Epoch 280, val loss: 1.6423051357269287
Epoch 290, training loss: 13.775171279907227 = 1.5622532367706299 + 2.0 * 6.106459140777588
Epoch 290, val loss: 1.6154478788375854
Epoch 300, training loss: 13.727187156677246 = 1.5227824449539185 + 2.0 * 6.102202415466309
Epoch 300, val loss: 1.5849053859710693
Epoch 310, training loss: 13.675661087036133 = 1.4787156581878662 + 2.0 * 6.098472595214844
Epoch 310, val loss: 1.550614595413208
Epoch 320, training loss: 13.620759963989258 = 1.430073857307434 + 2.0 * 6.095343112945557
Epoch 320, val loss: 1.512627124786377
Epoch 330, training loss: 13.568203926086426 = 1.3773542642593384 + 2.0 * 6.095424652099609
Epoch 330, val loss: 1.4711419343948364
Epoch 340, training loss: 13.503628730773926 = 1.3221975564956665 + 2.0 * 6.090715408325195
Epoch 340, val loss: 1.427992820739746
Epoch 350, training loss: 13.441332817077637 = 1.2660530805587769 + 2.0 * 6.087639808654785
Epoch 350, val loss: 1.3838752508163452
Epoch 360, training loss: 13.379892349243164 = 1.2093777656555176 + 2.0 * 6.085257530212402
Epoch 360, val loss: 1.3389886617660522
Epoch 370, training loss: 13.333695411682129 = 1.15354323387146 + 2.0 * 6.090075969696045
Epoch 370, val loss: 1.2947500944137573
Epoch 380, training loss: 13.264217376708984 = 1.1007078886032104 + 2.0 * 6.081754684448242
Epoch 380, val loss: 1.252900242805481
Epoch 390, training loss: 13.209335327148438 = 1.0503185987472534 + 2.0 * 6.079508304595947
Epoch 390, val loss: 1.212971806526184
Epoch 400, training loss: 13.156481742858887 = 1.0018638372421265 + 2.0 * 6.0773091316223145
Epoch 400, val loss: 1.174615502357483
Epoch 410, training loss: 13.108725547790527 = 0.9553123116493225 + 2.0 * 6.076706409454346
Epoch 410, val loss: 1.1378202438354492
Epoch 420, training loss: 13.057764053344727 = 0.9110147356987 + 2.0 * 6.0733747482299805
Epoch 420, val loss: 1.1028763055801392
Epoch 430, training loss: 13.010307312011719 = 0.868140459060669 + 2.0 * 6.0710835456848145
Epoch 430, val loss: 1.0691043138504028
Epoch 440, training loss: 12.975577354431152 = 0.8262569308280945 + 2.0 * 6.074660301208496
Epoch 440, val loss: 1.0360915660858154
Epoch 450, training loss: 12.922414779663086 = 0.7855879664421082 + 2.0 * 6.068413257598877
Epoch 450, val loss: 1.0043193101882935
Epoch 460, training loss: 12.878545761108398 = 0.7461902499198914 + 2.0 * 6.066177845001221
Epoch 460, val loss: 0.9735153913497925
Epoch 470, training loss: 12.838747024536133 = 0.7079088091850281 + 2.0 * 6.0654191970825195
Epoch 470, val loss: 0.9437694549560547
Epoch 480, training loss: 12.815173149108887 = 0.6709221601486206 + 2.0 * 6.072125434875488
Epoch 480, val loss: 0.9151986837387085
Epoch 490, training loss: 12.762937545776367 = 0.6357194185256958 + 2.0 * 6.0636091232299805
Epoch 490, val loss: 0.8883239030838013
Epoch 500, training loss: 12.723845481872559 = 0.6020951867103577 + 2.0 * 6.060874938964844
Epoch 500, val loss: 0.8631977438926697
Epoch 510, training loss: 12.688189506530762 = 0.5700857043266296 + 2.0 * 6.059051990509033
Epoch 510, val loss: 0.8399040699005127
Epoch 520, training loss: 12.656045913696289 = 0.5399004220962524 + 2.0 * 6.058072566986084
Epoch 520, val loss: 0.8186705708503723
Epoch 530, training loss: 12.622102737426758 = 0.5112234354019165 + 2.0 * 6.055439472198486
Epoch 530, val loss: 0.7993735671043396
Epoch 540, training loss: 12.598087310791016 = 0.4841824471950531 + 2.0 * 6.056952476501465
Epoch 540, val loss: 0.7819799184799194
Epoch 550, training loss: 12.56745433807373 = 0.45898446440696716 + 2.0 * 6.054234981536865
Epoch 550, val loss: 0.766607403755188
Epoch 560, training loss: 12.538628578186035 = 0.4352599084377289 + 2.0 * 6.051684379577637
Epoch 560, val loss: 0.7529367804527283
Epoch 570, training loss: 12.518362998962402 = 0.4128607213497162 + 2.0 * 6.052751064300537
Epoch 570, val loss: 0.7407463788986206
Epoch 580, training loss: 12.495203971862793 = 0.39187735319137573 + 2.0 * 6.051663398742676
Epoch 580, val loss: 0.7300199866294861
Epoch 590, training loss: 12.470854759216309 = 0.37228304147720337 + 2.0 * 6.049285888671875
Epoch 590, val loss: 0.7206688523292542
Epoch 600, training loss: 12.446135520935059 = 0.35399913787841797 + 2.0 * 6.04606819152832
Epoch 600, val loss: 0.7126114964485168
Epoch 610, training loss: 12.426436424255371 = 0.33677229285240173 + 2.0 * 6.044832229614258
Epoch 610, val loss: 0.7055535316467285
Epoch 620, training loss: 12.408828735351562 = 0.32049524784088135 + 2.0 * 6.044166564941406
Epoch 620, val loss: 0.6994852423667908
Epoch 630, training loss: 12.393243789672852 = 0.3052433431148529 + 2.0 * 6.044000148773193
Epoch 630, val loss: 0.6943304538726807
Epoch 640, training loss: 12.376874923706055 = 0.29103201627731323 + 2.0 * 6.042921543121338
Epoch 640, val loss: 0.6901440620422363
Epoch 650, training loss: 12.35948371887207 = 0.27772316336631775 + 2.0 * 6.04088020324707
Epoch 650, val loss: 0.6868212223052979
Epoch 660, training loss: 12.343362808227539 = 0.26510441303253174 + 2.0 * 6.039129257202148
Epoch 660, val loss: 0.6841792464256287
Epoch 670, training loss: 12.328058242797852 = 0.25310826301574707 + 2.0 * 6.037475109100342
Epoch 670, val loss: 0.6822368502616882
Epoch 680, training loss: 12.369054794311523 = 0.24176408350467682 + 2.0 * 6.063645362854004
Epoch 680, val loss: 0.6809453368186951
Epoch 690, training loss: 12.311943054199219 = 0.23121212422847748 + 2.0 * 6.040365695953369
Epoch 690, val loss: 0.6801795959472656
Epoch 700, training loss: 12.29222583770752 = 0.2212297022342682 + 2.0 * 6.035498142242432
Epoch 700, val loss: 0.6800568699836731
Epoch 710, training loss: 12.280332565307617 = 0.2116851955652237 + 2.0 * 6.034323692321777
Epoch 710, val loss: 0.6803988218307495
Epoch 720, training loss: 12.268007278442383 = 0.2025308907032013 + 2.0 * 6.032738208770752
Epoch 720, val loss: 0.6812067031860352
Epoch 730, training loss: 12.275383949279785 = 0.19375434517860413 + 2.0 * 6.0408148765563965
Epoch 730, val loss: 0.6824671030044556
Epoch 740, training loss: 12.251664161682129 = 0.1854458600282669 + 2.0 * 6.033109188079834
Epoch 740, val loss: 0.6840255856513977
Epoch 750, training loss: 12.241732597351074 = 0.17752745747566223 + 2.0 * 6.032102584838867
Epoch 750, val loss: 0.6860561966896057
Epoch 760, training loss: 12.236936569213867 = 0.16995254158973694 + 2.0 * 6.033492088317871
Epoch 760, val loss: 0.6884506940841675
Epoch 770, training loss: 12.226314544677734 = 0.16272063553333282 + 2.0 * 6.031796932220459
Epoch 770, val loss: 0.6911870837211609
Epoch 780, training loss: 12.215047836303711 = 0.15581171214580536 + 2.0 * 6.029618263244629
Epoch 780, val loss: 0.6942633390426636
Epoch 790, training loss: 12.206992149353027 = 0.14920574426651 + 2.0 * 6.028892993927002
Epoch 790, val loss: 0.6976277232170105
Epoch 800, training loss: 12.200671195983887 = 0.14291347563266754 + 2.0 * 6.028878688812256
Epoch 800, val loss: 0.7012690305709839
Epoch 810, training loss: 12.191786766052246 = 0.1369541585445404 + 2.0 * 6.027416229248047
Epoch 810, val loss: 0.7051523923873901
Epoch 820, training loss: 12.181707382202148 = 0.1312473565340042 + 2.0 * 6.0252299308776855
Epoch 820, val loss: 0.7093057036399841
Epoch 830, training loss: 12.178465843200684 = 0.12577621638774872 + 2.0 * 6.0263447761535645
Epoch 830, val loss: 0.7136842608451843
Epoch 840, training loss: 12.172154426574707 = 0.12059754878282547 + 2.0 * 6.025778293609619
Epoch 840, val loss: 0.71820068359375
Epoch 850, training loss: 12.164703369140625 = 0.11567924916744232 + 2.0 * 6.02451229095459
Epoch 850, val loss: 0.7229661345481873
Epoch 860, training loss: 12.156304359436035 = 0.11098792403936386 + 2.0 * 6.022658348083496
Epoch 860, val loss: 0.7279465198516846
Epoch 870, training loss: 12.154572486877441 = 0.10650629550218582 + 2.0 * 6.024033069610596
Epoch 870, val loss: 0.7331017851829529
Epoch 880, training loss: 12.144742965698242 = 0.10224922001361847 + 2.0 * 6.021246910095215
Epoch 880, val loss: 0.7383820414543152
Epoch 890, training loss: 12.143425941467285 = 0.09820625185966492 + 2.0 * 6.022609710693359
Epoch 890, val loss: 0.7438574433326721
Epoch 900, training loss: 12.13613510131836 = 0.09437209367752075 + 2.0 * 6.020881652832031
Epoch 900, val loss: 0.749483048915863
Epoch 910, training loss: 12.130996704101562 = 0.09072217345237732 + 2.0 * 6.020137310028076
Epoch 910, val loss: 0.7552379965782166
Epoch 920, training loss: 12.126996040344238 = 0.08723986148834229 + 2.0 * 6.019877910614014
Epoch 920, val loss: 0.7611238360404968
Epoch 930, training loss: 12.127734184265137 = 0.08392679691314697 + 2.0 * 6.0219035148620605
Epoch 930, val loss: 0.7670981884002686
Epoch 940, training loss: 12.125260353088379 = 0.08078522235155106 + 2.0 * 6.022237777709961
Epoch 940, val loss: 0.7731066942214966
Epoch 950, training loss: 12.11426067352295 = 0.07779757678508759 + 2.0 * 6.018231391906738
Epoch 950, val loss: 0.7793249487876892
Epoch 960, training loss: 12.120290756225586 = 0.07495205104351044 + 2.0 * 6.022669315338135
Epoch 960, val loss: 0.7855091691017151
Epoch 970, training loss: 12.108601570129395 = 0.07224812358617783 + 2.0 * 6.018176555633545
Epoch 970, val loss: 0.7917781472206116
Epoch 980, training loss: 12.10335636138916 = 0.06967515498399734 + 2.0 * 6.01684045791626
Epoch 980, val loss: 0.7980867624282837
Epoch 990, training loss: 12.101914405822754 = 0.06721492111682892 + 2.0 * 6.017349720001221
Epoch 990, val loss: 0.8044477701187134
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.5830
Flip ASR: 0.5022/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.70346450805664 = 1.9557647705078125 + 2.0 * 8.373849868774414
Epoch 0, val loss: 1.9496732950210571
Epoch 10, training loss: 18.691255569458008 = 1.9445568323135376 + 2.0 * 8.3733491897583
Epoch 10, val loss: 1.9391920566558838
Epoch 20, training loss: 18.6715145111084 = 1.930765986442566 + 2.0 * 8.37037467956543
Epoch 20, val loss: 1.9260107278823853
Epoch 30, training loss: 18.613534927368164 = 1.9123680591583252 + 2.0 * 8.35058307647705
Epoch 30, val loss: 1.9082177877426147
Epoch 40, training loss: 18.330957412719727 = 1.8907920122146606 + 2.0 * 8.22008228302002
Epoch 40, val loss: 1.887731909751892
Epoch 50, training loss: 16.973039627075195 = 1.8708800077438354 + 2.0 * 7.551079750061035
Epoch 50, val loss: 1.868107795715332
Epoch 60, training loss: 16.08759117126465 = 1.8546901941299438 + 2.0 * 7.116450786590576
Epoch 60, val loss: 1.8522205352783203
Epoch 70, training loss: 15.475269317626953 = 1.8434144258499146 + 2.0 * 6.815927505493164
Epoch 70, val loss: 1.8415746688842773
Epoch 80, training loss: 15.064300537109375 = 1.8327807188034058 + 2.0 * 6.61575984954834
Epoch 80, val loss: 1.8320432901382446
Epoch 90, training loss: 14.84372329711914 = 1.8219430446624756 + 2.0 * 6.510890007019043
Epoch 90, val loss: 1.8218177556991577
Epoch 100, training loss: 14.684871673583984 = 1.8097602128982544 + 2.0 * 6.43755578994751
Epoch 100, val loss: 1.8104079961776733
Epoch 110, training loss: 14.566671371459961 = 1.7980252504348755 + 2.0 * 6.3843231201171875
Epoch 110, val loss: 1.7992935180664062
Epoch 120, training loss: 14.476436614990234 = 1.7875680923461914 + 2.0 * 6.3444342613220215
Epoch 120, val loss: 1.7891746759414673
Epoch 130, training loss: 14.404009819030762 = 1.7773487567901611 + 2.0 * 6.31333065032959
Epoch 130, val loss: 1.7791997194290161
Epoch 140, training loss: 14.345084190368652 = 1.7667913436889648 + 2.0 * 6.289146423339844
Epoch 140, val loss: 1.7689756155014038
Epoch 150, training loss: 14.293865203857422 = 1.7554306983947754 + 2.0 * 6.269217491149902
Epoch 150, val loss: 1.7581961154937744
Epoch 160, training loss: 14.246980667114258 = 1.7430927753448486 + 2.0 * 6.251944065093994
Epoch 160, val loss: 1.7467883825302124
Epoch 170, training loss: 14.198193550109863 = 1.7295342683792114 + 2.0 * 6.234329700469971
Epoch 170, val loss: 1.7344902753829956
Epoch 180, training loss: 14.15178394317627 = 1.7141268253326416 + 2.0 * 6.2188286781311035
Epoch 180, val loss: 1.7209808826446533
Epoch 190, training loss: 14.112570762634277 = 1.6962382793426514 + 2.0 * 6.208166122436523
Epoch 190, val loss: 1.7057313919067383
Epoch 200, training loss: 14.065195083618164 = 1.6756489276885986 + 2.0 * 6.194773197174072
Epoch 200, val loss: 1.6883823871612549
Epoch 210, training loss: 14.019023895263672 = 1.6517139673233032 + 2.0 * 6.18365478515625
Epoch 210, val loss: 1.6684373617172241
Epoch 220, training loss: 13.972371101379395 = 1.6237080097198486 + 2.0 * 6.1743316650390625
Epoch 220, val loss: 1.6452826261520386
Epoch 230, training loss: 13.926092147827148 = 1.5911550521850586 + 2.0 * 6.167468547821045
Epoch 230, val loss: 1.618697166442871
Epoch 240, training loss: 13.872797966003418 = 1.554308295249939 + 2.0 * 6.159245014190674
Epoch 240, val loss: 1.5886181592941284
Epoch 250, training loss: 13.817172050476074 = 1.5129350423812866 + 2.0 * 6.152118682861328
Epoch 250, val loss: 1.5550018548965454
Epoch 260, training loss: 13.760170936584473 = 1.46769118309021 + 2.0 * 6.146239757537842
Epoch 260, val loss: 1.5186344385147095
Epoch 270, training loss: 13.70272445678711 = 1.4203388690948486 + 2.0 * 6.14119291305542
Epoch 270, val loss: 1.4807634353637695
Epoch 280, training loss: 13.643043518066406 = 1.3717095851898193 + 2.0 * 6.135666847229004
Epoch 280, val loss: 1.4420539140701294
Epoch 290, training loss: 13.590070724487305 = 1.3227688074111938 + 2.0 * 6.133650779724121
Epoch 290, val loss: 1.4033366441726685
Epoch 300, training loss: 13.529919624328613 = 1.2751643657684326 + 2.0 * 6.127377510070801
Epoch 300, val loss: 1.3663372993469238
Epoch 310, training loss: 13.473569869995117 = 1.2295002937316895 + 2.0 * 6.122035026550293
Epoch 310, val loss: 1.3311113119125366
Epoch 320, training loss: 13.420992851257324 = 1.1853348016738892 + 2.0 * 6.117828845977783
Epoch 320, val loss: 1.2972921133041382
Epoch 330, training loss: 13.374153137207031 = 1.1425153017044067 + 2.0 * 6.115818977355957
Epoch 330, val loss: 1.2646905183792114
Epoch 340, training loss: 13.32509994506836 = 1.101230263710022 + 2.0 * 6.111934661865234
Epoch 340, val loss: 1.2334452867507935
Epoch 350, training loss: 13.275208473205566 = 1.0611460208892822 + 2.0 * 6.107031345367432
Epoch 350, val loss: 1.2032848596572876
Epoch 360, training loss: 13.231402397155762 = 1.0220516920089722 + 2.0 * 6.10467529296875
Epoch 360, val loss: 1.1739243268966675
Epoch 370, training loss: 13.185943603515625 = 0.9841609001159668 + 2.0 * 6.100891590118408
Epoch 370, val loss: 1.145477533340454
Epoch 380, training loss: 13.142704010009766 = 0.9474844932556152 + 2.0 * 6.097609996795654
Epoch 380, val loss: 1.118116855621338
Epoch 390, training loss: 13.101619720458984 = 0.9118379950523376 + 2.0 * 6.09489107131958
Epoch 390, val loss: 1.0915098190307617
Epoch 400, training loss: 13.069687843322754 = 0.8777405023574829 + 2.0 * 6.095973491668701
Epoch 400, val loss: 1.066110610961914
Epoch 410, training loss: 13.02613639831543 = 0.8457985520362854 + 2.0 * 6.0901689529418945
Epoch 410, val loss: 1.0426099300384521
Epoch 420, training loss: 12.989415168762207 = 0.815622091293335 + 2.0 * 6.0868964195251465
Epoch 420, val loss: 1.020695686340332
Epoch 430, training loss: 12.959707260131836 = 0.7870258092880249 + 2.0 * 6.08634090423584
Epoch 430, val loss: 1.0002071857452393
Epoch 440, training loss: 12.928831100463867 = 0.7605670690536499 + 2.0 * 6.084132194519043
Epoch 440, val loss: 0.9814496040344238
Epoch 450, training loss: 12.894817352294922 = 0.7360208034515381 + 2.0 * 6.079398155212402
Epoch 450, val loss: 0.9649527072906494
Epoch 460, training loss: 12.867074966430664 = 0.7130632400512695 + 2.0 * 6.077005863189697
Epoch 460, val loss: 0.9499067664146423
Epoch 470, training loss: 12.841739654541016 = 0.691616952419281 + 2.0 * 6.075061321258545
Epoch 470, val loss: 0.9362941384315491
Epoch 480, training loss: 12.825551986694336 = 0.6716309189796448 + 2.0 * 6.076960563659668
Epoch 480, val loss: 0.9243471026420593
Epoch 490, training loss: 12.797392845153809 = 0.6529015898704529 + 2.0 * 6.0722455978393555
Epoch 490, val loss: 0.9136619567871094
Epoch 500, training loss: 12.772992134094238 = 0.6350915431976318 + 2.0 * 6.068950176239014
Epoch 500, val loss: 0.9040880799293518
Epoch 510, training loss: 12.75744915008545 = 0.6179607510566711 + 2.0 * 6.069744110107422
Epoch 510, val loss: 0.8954684734344482
Epoch 520, training loss: 12.737772941589355 = 0.6014935374259949 + 2.0 * 6.068139553070068
Epoch 520, val loss: 0.8875805735588074
Epoch 530, training loss: 12.71469783782959 = 0.5856305956840515 + 2.0 * 6.064533710479736
Epoch 530, val loss: 0.8805415630340576
Epoch 540, training loss: 12.695413589477539 = 0.5700221061706543 + 2.0 * 6.062695503234863
Epoch 540, val loss: 0.8739745020866394
Epoch 550, training loss: 12.677878379821777 = 0.5545645356178284 + 2.0 * 6.061656951904297
Epoch 550, val loss: 0.8677940368652344
Epoch 560, training loss: 12.664752006530762 = 0.5392531156539917 + 2.0 * 6.06274938583374
Epoch 560, val loss: 0.8619315028190613
Epoch 570, training loss: 12.642858505249023 = 0.5242077708244324 + 2.0 * 6.059325218200684
Epoch 570, val loss: 0.8565796613693237
Epoch 580, training loss: 12.624354362487793 = 0.5091570615768433 + 2.0 * 6.05759859085083
Epoch 580, val loss: 0.851588249206543
Epoch 590, training loss: 12.605660438537598 = 0.49403247237205505 + 2.0 * 6.055813789367676
Epoch 590, val loss: 0.8467936515808105
Epoch 600, training loss: 12.593734741210938 = 0.47881001234054565 + 2.0 * 6.057462215423584
Epoch 600, val loss: 0.8422393798828125
Epoch 610, training loss: 12.582048416137695 = 0.4635910093784332 + 2.0 * 6.059228897094727
Epoch 610, val loss: 0.8379151821136475
Epoch 620, training loss: 12.557145118713379 = 0.4486803412437439 + 2.0 * 6.054232597351074
Epoch 620, val loss: 0.8340983390808105
Epoch 630, training loss: 12.539117813110352 = 0.43370959162712097 + 2.0 * 6.052704334259033
Epoch 630, val loss: 0.8304643630981445
Epoch 640, training loss: 12.519784927368164 = 0.41863560676574707 + 2.0 * 6.050574779510498
Epoch 640, val loss: 0.827004611492157
Epoch 650, training loss: 12.503106117248535 = 0.4034580886363983 + 2.0 * 6.049824237823486
Epoch 650, val loss: 0.823837399482727
Epoch 660, training loss: 12.495382308959961 = 0.3883056342601776 + 2.0 * 6.0535383224487305
Epoch 660, val loss: 0.8208707571029663
Epoch 670, training loss: 12.475417137145996 = 0.3733378052711487 + 2.0 * 6.051039695739746
Epoch 670, val loss: 0.8183120489120483
Epoch 680, training loss: 12.45771598815918 = 0.35854372382164 + 2.0 * 6.049586296081543
Epoch 680, val loss: 0.8161441087722778
Epoch 690, training loss: 12.436622619628906 = 0.34397685527801514 + 2.0 * 6.046322822570801
Epoch 690, val loss: 0.8142135739326477
Epoch 700, training loss: 12.420778274536133 = 0.3296883702278137 + 2.0 * 6.0455451011657715
Epoch 700, val loss: 0.8127866387367249
Epoch 710, training loss: 12.409906387329102 = 0.3156954050064087 + 2.0 * 6.047105312347412
Epoch 710, val loss: 0.811788022518158
Epoch 720, training loss: 12.399209022521973 = 0.3020935654640198 + 2.0 * 6.048557758331299
Epoch 720, val loss: 0.811122715473175
Epoch 730, training loss: 12.379114151000977 = 0.28903061151504517 + 2.0 * 6.045041561126709
Epoch 730, val loss: 0.8110334277153015
Epoch 740, training loss: 12.363728523254395 = 0.27636823058128357 + 2.0 * 6.043680191040039
Epoch 740, val loss: 0.8113803863525391
Epoch 750, training loss: 12.349398612976074 = 0.2642541825771332 + 2.0 * 6.042572021484375
Epoch 750, val loss: 0.8121291399002075
Epoch 760, training loss: 12.336074829101562 = 0.252682626247406 + 2.0 * 6.041696071624756
Epoch 760, val loss: 0.8134704828262329
Epoch 770, training loss: 12.324725151062012 = 0.2416120320558548 + 2.0 * 6.041556358337402
Epoch 770, val loss: 0.815265417098999
Epoch 780, training loss: 12.314035415649414 = 0.2311420738697052 + 2.0 * 6.041446685791016
Epoch 780, val loss: 0.8174058794975281
Epoch 790, training loss: 12.30228042602539 = 0.2212497740983963 + 2.0 * 6.040515422821045
Epoch 790, val loss: 0.819959819316864
Epoch 800, training loss: 12.287599563598633 = 0.21193212270736694 + 2.0 * 6.0378336906433105
Epoch 800, val loss: 0.8229491710662842
Epoch 810, training loss: 12.282188415527344 = 0.20306116342544556 + 2.0 * 6.0395636558532715
Epoch 810, val loss: 0.8262526988983154
Epoch 820, training loss: 12.272591590881348 = 0.19471189379692078 + 2.0 * 6.038939952850342
Epoch 820, val loss: 0.8297460675239563
Epoch 830, training loss: 12.269721031188965 = 0.1868816763162613 + 2.0 * 6.041419506072998
Epoch 830, val loss: 0.8336042165756226
Epoch 840, training loss: 12.250900268554688 = 0.17948898673057556 + 2.0 * 6.03570556640625
Epoch 840, val loss: 0.8376845121383667
Epoch 850, training loss: 12.240469932556152 = 0.17249783873558044 + 2.0 * 6.0339860916137695
Epoch 850, val loss: 0.842011570930481
Epoch 860, training loss: 12.23704719543457 = 0.16586166620254517 + 2.0 * 6.035592555999756
Epoch 860, val loss: 0.8465352654457092
Epoch 870, training loss: 12.227327346801758 = 0.1595860719680786 + 2.0 * 6.033870697021484
Epoch 870, val loss: 0.8512303829193115
Epoch 880, training loss: 12.222973823547363 = 0.1537073850631714 + 2.0 * 6.034633159637451
Epoch 880, val loss: 0.8561562299728394
Epoch 890, training loss: 12.213871955871582 = 0.14813381433486938 + 2.0 * 6.0328688621521
Epoch 890, val loss: 0.8611387014389038
Epoch 900, training loss: 12.2049560546875 = 0.14287278056144714 + 2.0 * 6.031041622161865
Epoch 900, val loss: 0.8663880825042725
Epoch 910, training loss: 12.197977066040039 = 0.13785211741924286 + 2.0 * 6.030062675476074
Epoch 910, val loss: 0.8717060089111328
Epoch 920, training loss: 12.195571899414062 = 0.13305428624153137 + 2.0 * 6.031258583068848
Epoch 920, val loss: 0.877167820930481
Epoch 930, training loss: 12.190147399902344 = 0.12849950790405273 + 2.0 * 6.030824184417725
Epoch 930, val loss: 0.8824383616447449
Epoch 940, training loss: 12.1818265914917 = 0.12421512603759766 + 2.0 * 6.028805732727051
Epoch 940, val loss: 0.8880589008331299
Epoch 950, training loss: 12.17623233795166 = 0.12009909003973007 + 2.0 * 6.028066635131836
Epoch 950, val loss: 0.8937273621559143
Epoch 960, training loss: 12.171226501464844 = 0.11614605039358139 + 2.0 * 6.02754020690918
Epoch 960, val loss: 0.8994442820549011
Epoch 970, training loss: 12.166532516479492 = 0.11236946284770966 + 2.0 * 6.027081489562988
Epoch 970, val loss: 0.9050390124320984
Epoch 980, training loss: 12.160865783691406 = 0.10877884179353714 + 2.0 * 6.02604341506958
Epoch 980, val loss: 0.9109207391738892
Epoch 990, training loss: 12.158088684082031 = 0.10532152652740479 + 2.0 * 6.026383399963379
Epoch 990, val loss: 0.9167983531951904
=== picking the best model according to the performance on validation ===
target class rate on Vs: 0.8000
accuracy on clean test nodes: 0.7963
Overall ASR: 0.7196
Flip ASR: 0.6667/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.681529998779297 = 1.9339300394058228 + 2.0 * 8.373800277709961
Epoch 0, val loss: 1.9292057752609253
Epoch 10, training loss: 18.670740127563477 = 1.9246151447296143 + 2.0 * 8.373062133789062
Epoch 10, val loss: 1.919920802116394
Epoch 20, training loss: 18.650720596313477 = 1.9129759073257446 + 2.0 * 8.36887264251709
Epoch 20, val loss: 1.9081988334655762
Epoch 30, training loss: 18.578149795532227 = 1.8974900245666504 + 2.0 * 8.340330123901367
Epoch 30, val loss: 1.8926196098327637
Epoch 40, training loss: 18.106733322143555 = 1.8793636560440063 + 2.0 * 8.11368465423584
Epoch 40, val loss: 1.8747609853744507
Epoch 50, training loss: 16.598983764648438 = 1.8589774370193481 + 2.0 * 7.370002746582031
Epoch 50, val loss: 1.8546022176742554
Epoch 60, training loss: 15.830499649047852 = 1.8467564582824707 + 2.0 * 6.991871356964111
Epoch 60, val loss: 1.8438570499420166
Epoch 70, training loss: 15.373493194580078 = 1.8382682800292969 + 2.0 * 6.767612457275391
Epoch 70, val loss: 1.8352112770080566
Epoch 80, training loss: 15.050813674926758 = 1.8275127410888672 + 2.0 * 6.611650466918945
Epoch 80, val loss: 1.824862003326416
Epoch 90, training loss: 14.859307289123535 = 1.8174618482589722 + 2.0 * 6.520922660827637
Epoch 90, val loss: 1.8153258562088013
Epoch 100, training loss: 14.720930099487305 = 1.808858871459961 + 2.0 * 6.456035614013672
Epoch 100, val loss: 1.8075083494186401
Epoch 110, training loss: 14.610630989074707 = 1.8015776872634888 + 2.0 * 6.404526710510254
Epoch 110, val loss: 1.8007218837738037
Epoch 120, training loss: 14.525754928588867 = 1.7948839664459229 + 2.0 * 6.365435600280762
Epoch 120, val loss: 1.7941251993179321
Epoch 130, training loss: 14.451244354248047 = 1.787908673286438 + 2.0 * 6.331667900085449
Epoch 130, val loss: 1.7872165441513062
Epoch 140, training loss: 14.389747619628906 = 1.7807101011276245 + 2.0 * 6.304518699645996
Epoch 140, val loss: 1.7802072763442993
Epoch 150, training loss: 14.337323188781738 = 1.7731183767318726 + 2.0 * 6.282102584838867
Epoch 150, val loss: 1.772994875907898
Epoch 160, training loss: 14.289294242858887 = 1.7648118734359741 + 2.0 * 6.262241363525391
Epoch 160, val loss: 1.7653909921646118
Epoch 170, training loss: 14.244146347045898 = 1.7555478811264038 + 2.0 * 6.244299411773682
Epoch 170, val loss: 1.7572534084320068
Epoch 180, training loss: 14.201624870300293 = 1.7451013326644897 + 2.0 * 6.228261947631836
Epoch 180, val loss: 1.748311161994934
Epoch 190, training loss: 14.165990829467773 = 1.7330553531646729 + 2.0 * 6.21646785736084
Epoch 190, val loss: 1.738239049911499
Epoch 200, training loss: 14.124289512634277 = 1.7191835641860962 + 2.0 * 6.202552795410156
Epoch 200, val loss: 1.7267037630081177
Epoch 210, training loss: 14.085602760314941 = 1.7029749155044556 + 2.0 * 6.191313743591309
Epoch 210, val loss: 1.7133781909942627
Epoch 220, training loss: 14.04709243774414 = 1.6839375495910645 + 2.0 * 6.181577682495117
Epoch 220, val loss: 1.6977663040161133
Epoch 230, training loss: 14.007004737854004 = 1.6615040302276611 + 2.0 * 6.172750473022461
Epoch 230, val loss: 1.6794512271881104
Epoch 240, training loss: 13.964940071105957 = 1.635151743888855 + 2.0 * 6.164894104003906
Epoch 240, val loss: 1.6578062772750854
Epoch 250, training loss: 13.918737411499023 = 1.6042437553405762 + 2.0 * 6.1572465896606445
Epoch 250, val loss: 1.6324195861816406
Epoch 260, training loss: 13.871102333068848 = 1.568462610244751 + 2.0 * 6.151319980621338
Epoch 260, val loss: 1.602983832359314
Epoch 270, training loss: 13.817893981933594 = 1.5276551246643066 + 2.0 * 6.145119667053223
Epoch 270, val loss: 1.569272756576538
Epoch 280, training loss: 13.764222145080566 = 1.4822853803634644 + 2.0 * 6.140968322753906
Epoch 280, val loss: 1.5319417715072632
Epoch 290, training loss: 13.70419692993164 = 1.4336273670196533 + 2.0 * 6.135284900665283
Epoch 290, val loss: 1.491943359375
Epoch 300, training loss: 13.64566421508789 = 1.3827403783798218 + 2.0 * 6.131462097167969
Epoch 300, val loss: 1.450408935546875
Epoch 310, training loss: 13.58353328704834 = 1.3311001062393188 + 2.0 * 6.126216411590576
Epoch 310, val loss: 1.4084398746490479
Epoch 320, training loss: 13.531496047973633 = 1.2798582315444946 + 2.0 * 6.125818729400635
Epoch 320, val loss: 1.3673007488250732
Epoch 330, training loss: 13.468379020690918 = 1.230788230895996 + 2.0 * 6.118795394897461
Epoch 330, val loss: 1.328601360321045
Epoch 340, training loss: 13.414338111877441 = 1.183929443359375 + 2.0 * 6.115204334259033
Epoch 340, val loss: 1.2919118404388428
Epoch 350, training loss: 13.36097526550293 = 1.1388850212097168 + 2.0 * 6.111044883728027
Epoch 350, val loss: 1.2569810152053833
Epoch 360, training loss: 13.315953254699707 = 1.095633864402771 + 2.0 * 6.110159873962402
Epoch 360, val loss: 1.2239444255828857
Epoch 370, training loss: 13.264861106872559 = 1.0544993877410889 + 2.0 * 6.105180740356445
Epoch 370, val loss: 1.1928967237472534
Epoch 380, training loss: 13.220721244812012 = 1.014945387840271 + 2.0 * 6.102888107299805
Epoch 380, val loss: 1.1632839441299438
Epoch 390, training loss: 13.177846908569336 = 0.977088451385498 + 2.0 * 6.100379467010498
Epoch 390, val loss: 1.135138750076294
Epoch 400, training loss: 13.132281303405762 = 0.9408339262008667 + 2.0 * 6.095723628997803
Epoch 400, val loss: 1.108588457107544
Epoch 410, training loss: 13.092998504638672 = 0.905856728553772 + 2.0 * 6.093570709228516
Epoch 410, val loss: 1.083127737045288
Epoch 420, training loss: 13.056536674499512 = 0.8719972968101501 + 2.0 * 6.0922698974609375
Epoch 420, val loss: 1.0588078498840332
Epoch 430, training loss: 13.017084121704102 = 0.8393890261650085 + 2.0 * 6.088847637176514
Epoch 430, val loss: 1.035568356513977
Epoch 440, training loss: 12.981348037719727 = 0.8076682090759277 + 2.0 * 6.08683967590332
Epoch 440, val loss: 1.013242244720459
Epoch 450, training loss: 12.948723793029785 = 0.7765272259712219 + 2.0 * 6.0860981941223145
Epoch 450, val loss: 0.9915482401847839
Epoch 460, training loss: 12.91567611694336 = 0.7461128234863281 + 2.0 * 6.084781646728516
Epoch 460, val loss: 0.9704681634902954
Epoch 470, training loss: 12.878406524658203 = 0.7162530422210693 + 2.0 * 6.081076622009277
Epoch 470, val loss: 0.9501014947891235
Epoch 480, training loss: 12.845442771911621 = 0.6869844794273376 + 2.0 * 6.079229354858398
Epoch 480, val loss: 0.9301832318305969
Epoch 490, training loss: 12.818225860595703 = 0.6582352519035339 + 2.0 * 6.079995155334473
Epoch 490, val loss: 0.9107387065887451
Epoch 500, training loss: 12.78282356262207 = 0.6302871704101562 + 2.0 * 6.076268196105957
Epoch 500, val loss: 0.8919436931610107
Epoch 510, training loss: 12.752571105957031 = 0.6030150055885315 + 2.0 * 6.074778079986572
Epoch 510, val loss: 0.873802125453949
Epoch 520, training loss: 12.727559089660645 = 0.5764650702476501 + 2.0 * 6.075547218322754
Epoch 520, val loss: 0.8563662767410278
Epoch 530, training loss: 12.694889068603516 = 0.550934374332428 + 2.0 * 6.071977138519287
Epoch 530, val loss: 0.8397176861763
Epoch 540, training loss: 12.665603637695312 = 0.5262500643730164 + 2.0 * 6.069676876068115
Epoch 540, val loss: 0.8240188956260681
Epoch 550, training loss: 12.643824577331543 = 0.5024817585945129 + 2.0 * 6.070671558380127
Epoch 550, val loss: 0.8091577291488647
Epoch 560, training loss: 12.615168571472168 = 0.47972291707992554 + 2.0 * 6.067722797393799
Epoch 560, val loss: 0.7953869700431824
Epoch 570, training loss: 12.590850830078125 = 0.45789748430252075 + 2.0 * 6.066476821899414
Epoch 570, val loss: 0.782579243183136
Epoch 580, training loss: 12.564218521118164 = 0.4369623064994812 + 2.0 * 6.063628196716309
Epoch 580, val loss: 0.7706019282341003
Epoch 590, training loss: 12.547913551330566 = 0.41678717732429504 + 2.0 * 6.065563201904297
Epoch 590, val loss: 0.7594404816627502
Epoch 600, training loss: 12.524109840393066 = 0.3972688615322113 + 2.0 * 6.063420295715332
Epoch 600, val loss: 0.7491058111190796
Epoch 610, training loss: 12.501870155334473 = 0.37862133979797363 + 2.0 * 6.061624526977539
Epoch 610, val loss: 0.7394769787788391
Epoch 620, training loss: 12.47973918914795 = 0.3607276380062103 + 2.0 * 6.059505939483643
Epoch 620, val loss: 0.7306061387062073
Epoch 630, training loss: 12.460289001464844 = 0.34341901540756226 + 2.0 * 6.058434963226318
Epoch 630, val loss: 0.7223494648933411
Epoch 640, training loss: 12.448758125305176 = 0.32667139172554016 + 2.0 * 6.0610432624816895
Epoch 640, val loss: 0.7146903276443481
Epoch 650, training loss: 12.427566528320312 = 0.3105310797691345 + 2.0 * 6.058517932891846
Epoch 650, val loss: 0.7076999545097351
Epoch 660, training loss: 12.403901100158691 = 0.2950165867805481 + 2.0 * 6.054442405700684
Epoch 660, val loss: 0.7013168334960938
Epoch 670, training loss: 12.38725471496582 = 0.2799968421459198 + 2.0 * 6.053628921508789
Epoch 670, val loss: 0.6954776644706726
Epoch 680, training loss: 12.371190071105957 = 0.26546141505241394 + 2.0 * 6.0528645515441895
Epoch 680, val loss: 0.6902419924736023
Epoch 690, training loss: 12.362652778625488 = 0.2515380084514618 + 2.0 * 6.0555572509765625
Epoch 690, val loss: 0.6855648756027222
Epoch 700, training loss: 12.345695495605469 = 0.23824363946914673 + 2.0 * 6.053725719451904
Epoch 700, val loss: 0.6815463304519653
Epoch 710, training loss: 12.326924324035645 = 0.22561118006706238 + 2.0 * 6.050656795501709
Epoch 710, val loss: 0.6781160831451416
Epoch 720, training loss: 12.317760467529297 = 0.21359172463417053 + 2.0 * 6.052084445953369
Epoch 720, val loss: 0.6752352714538574
Epoch 730, training loss: 12.300469398498535 = 0.2022336721420288 + 2.0 * 6.0491180419921875
Epoch 730, val loss: 0.6729063391685486
Epoch 740, training loss: 12.29384994506836 = 0.19156020879745483 + 2.0 * 6.051145076751709
Epoch 740, val loss: 0.671181857585907
Epoch 750, training loss: 12.275243759155273 = 0.18152204155921936 + 2.0 * 6.046860694885254
Epoch 750, val loss: 0.669953465461731
Epoch 760, training loss: 12.263503074645996 = 0.17216014862060547 + 2.0 * 6.045671463012695
Epoch 760, val loss: 0.669220507144928
Epoch 770, training loss: 12.264754295349121 = 0.16344843804836273 + 2.0 * 6.050652980804443
Epoch 770, val loss: 0.6689029932022095
Epoch 780, training loss: 12.242070198059082 = 0.15535114705562592 + 2.0 * 6.043359756469727
Epoch 780, val loss: 0.668950617313385
Epoch 790, training loss: 12.233319282531738 = 0.14780010282993317 + 2.0 * 6.042759418487549
Epoch 790, val loss: 0.6693968176841736
Epoch 800, training loss: 12.224568367004395 = 0.1407540738582611 + 2.0 * 6.04190731048584
Epoch 800, val loss: 0.6702449917793274
Epoch 810, training loss: 12.21878433227539 = 0.1342114508152008 + 2.0 * 6.042286396026611
Epoch 810, val loss: 0.6713294386863708
Epoch 820, training loss: 12.21279239654541 = 0.1281442791223526 + 2.0 * 6.042324066162109
Epoch 820, val loss: 0.6726477742195129
Epoch 830, training loss: 12.202207565307617 = 0.12248454988002777 + 2.0 * 6.039861679077148
Epoch 830, val loss: 0.6742638945579529
Epoch 840, training loss: 12.197846412658691 = 0.11717942357063293 + 2.0 * 6.040333271026611
Epoch 840, val loss: 0.676138162612915
Epoch 850, training loss: 12.188491821289062 = 0.1121961697936058 + 2.0 * 6.038147926330566
Epoch 850, val loss: 0.6781226396560669
Epoch 860, training loss: 12.181892395019531 = 0.10753108561038971 + 2.0 * 6.037180423736572
Epoch 860, val loss: 0.6803373694419861
Epoch 870, training loss: 12.175246238708496 = 0.10312071442604065 + 2.0 * 6.036062717437744
Epoch 870, val loss: 0.6827148199081421
Epoch 880, training loss: 12.18310832977295 = 0.09895871579647064 + 2.0 * 6.042074680328369
Epoch 880, val loss: 0.6852313280105591
Epoch 890, training loss: 12.168147087097168 = 0.09505763649940491 + 2.0 * 6.0365447998046875
Epoch 890, val loss: 0.6877732276916504
Epoch 900, training loss: 12.158695220947266 = 0.09136650711297989 + 2.0 * 6.033664226531982
Epoch 900, val loss: 0.6904749274253845
Epoch 910, training loss: 12.171380996704102 = 0.08786680549383163 + 2.0 * 6.041757106781006
Epoch 910, val loss: 0.6932063698768616
Epoch 920, training loss: 12.151400566101074 = 0.08457252383232117 + 2.0 * 6.033413887023926
Epoch 920, val loss: 0.6960200071334839
Epoch 930, training loss: 12.145126342773438 = 0.08144958317279816 + 2.0 * 6.031838417053223
Epoch 930, val loss: 0.6988951563835144
Epoch 940, training loss: 12.140388488769531 = 0.07847753912210464 + 2.0 * 6.0309553146362305
Epoch 940, val loss: 0.7018929123878479
Epoch 950, training loss: 12.15396785736084 = 0.07564980536699295 + 2.0 * 6.039158821105957
Epoch 950, val loss: 0.7049468159675598
Epoch 960, training loss: 12.137675285339355 = 0.0729418620467186 + 2.0 * 6.032366752624512
Epoch 960, val loss: 0.7079002261161804
Epoch 970, training loss: 12.12830924987793 = 0.07040208578109741 + 2.0 * 6.028953552246094
Epoch 970, val loss: 0.7110105156898499
Epoch 980, training loss: 12.124507904052734 = 0.06796583533287048 + 2.0 * 6.028271198272705
Epoch 980, val loss: 0.7142082452774048
Epoch 990, training loss: 12.135869026184082 = 0.06562772393226624 + 2.0 * 6.035120487213135
Epoch 990, val loss: 0.7174246907234192
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.8561
Flip ASR: 0.8267/225 nodes
The final ASR:0.71956, 0.11148, Accuracy:0.81358, 0.01222
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11590])
remove edge: torch.Size([2, 9494])
updated graph: torch.Size([2, 10528])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
The final ASR:0.97663, 0.00174, Accuracy:0.83210, 0.00761
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.5 der2:0.3 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.3, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 18.708654403686523 = 1.9610843658447266 + 2.0 * 8.373785018920898
Epoch 0, val loss: 1.9687308073043823
Epoch 10, training loss: 18.696395874023438 = 1.9502583742141724 + 2.0 * 8.373068809509277
Epoch 10, val loss: 1.957795262336731
Epoch 20, training loss: 18.674654006958008 = 1.9365793466567993 + 2.0 * 8.369037628173828
Epoch 20, val loss: 1.943974256515503
Epoch 30, training loss: 18.608816146850586 = 1.917930245399475 + 2.0 * 8.345442771911621
Epoch 30, val loss: 1.9253870248794556
Epoch 40, training loss: 18.305015563964844 = 1.8967108726501465 + 2.0 * 8.20415210723877
Epoch 40, val loss: 1.9052646160125732
Epoch 50, training loss: 17.026851654052734 = 1.8744248151779175 + 2.0 * 7.576213836669922
Epoch 50, val loss: 1.8833690881729126
Epoch 60, training loss: 16.05328941345215 = 1.8546284437179565 + 2.0 * 7.099330902099609
Epoch 60, val loss: 1.8653228282928467
Epoch 70, training loss: 15.443958282470703 = 1.8409829139709473 + 2.0 * 6.801487922668457
Epoch 70, val loss: 1.8525545597076416
Epoch 80, training loss: 15.102965354919434 = 1.8277448415756226 + 2.0 * 6.63761043548584
Epoch 80, val loss: 1.8398470878601074
Epoch 90, training loss: 14.855611801147461 = 1.816103219985962 + 2.0 * 6.519754409790039
Epoch 90, val loss: 1.8279370069503784
Epoch 100, training loss: 14.687066078186035 = 1.806043028831482 + 2.0 * 6.440511703491211
Epoch 100, val loss: 1.8172883987426758
Epoch 110, training loss: 14.555867195129395 = 1.7968090772628784 + 2.0 * 6.379528999328613
Epoch 110, val loss: 1.8074994087219238
Epoch 120, training loss: 14.46615982055664 = 1.7877165079116821 + 2.0 * 6.339221477508545
Epoch 120, val loss: 1.7980315685272217
Epoch 130, training loss: 14.393718719482422 = 1.7786610126495361 + 2.0 * 6.307528972625732
Epoch 130, val loss: 1.7887517213821411
Epoch 140, training loss: 14.330293655395508 = 1.769646167755127 + 2.0 * 6.2803239822387695
Epoch 140, val loss: 1.7799129486083984
Epoch 150, training loss: 14.27653980255127 = 1.7603155374526978 + 2.0 * 6.258111953735352
Epoch 150, val loss: 1.7712416648864746
Epoch 160, training loss: 14.229013442993164 = 1.750096082687378 + 2.0 * 6.2394585609436035
Epoch 160, val loss: 1.762316346168518
Epoch 170, training loss: 14.184393882751465 = 1.7385327816009521 + 2.0 * 6.222930431365967
Epoch 170, val loss: 1.7526427507400513
Epoch 180, training loss: 14.144370079040527 = 1.7252753973007202 + 2.0 * 6.209547519683838
Epoch 180, val loss: 1.7418161630630493
Epoch 190, training loss: 14.102004051208496 = 1.7101515531539917 + 2.0 * 6.195926189422607
Epoch 190, val loss: 1.729574203491211
Epoch 200, training loss: 14.061881065368652 = 1.6925805807113647 + 2.0 * 6.184650421142578
Epoch 200, val loss: 1.7155377864837646
Epoch 210, training loss: 14.022124290466309 = 1.6719605922698975 + 2.0 * 6.175081729888916
Epoch 210, val loss: 1.6991431713104248
Epoch 220, training loss: 13.981210708618164 = 1.6478660106658936 + 2.0 * 6.166672229766846
Epoch 220, val loss: 1.6799798011779785
Epoch 230, training loss: 13.93832015991211 = 1.619713306427002 + 2.0 * 6.159303665161133
Epoch 230, val loss: 1.657707691192627
Epoch 240, training loss: 13.890556335449219 = 1.5872087478637695 + 2.0 * 6.151673793792725
Epoch 240, val loss: 1.631882905960083
Epoch 250, training loss: 13.840107917785645 = 1.5497140884399414 + 2.0 * 6.145196914672852
Epoch 250, val loss: 1.6021028757095337
Epoch 260, training loss: 13.786155700683594 = 1.507069706916809 + 2.0 * 6.139543056488037
Epoch 260, val loss: 1.5680434703826904
Epoch 270, training loss: 13.732527732849121 = 1.4595917463302612 + 2.0 * 6.136467933654785
Epoch 270, val loss: 1.5300904512405396
Epoch 280, training loss: 13.673486709594727 = 1.4088784456253052 + 2.0 * 6.1323041915893555
Epoch 280, val loss: 1.4896713495254517
Epoch 290, training loss: 13.610030174255371 = 1.3562158346176147 + 2.0 * 6.1269073486328125
Epoch 290, val loss: 1.4477591514587402
Epoch 300, training loss: 13.547911643981934 = 1.3028910160064697 + 2.0 * 6.1225104331970215
Epoch 300, val loss: 1.405297040939331
Epoch 310, training loss: 13.488204002380371 = 1.250130534172058 + 2.0 * 6.119036674499512
Epoch 310, val loss: 1.3636176586151123
Epoch 320, training loss: 13.437660217285156 = 1.1997183561325073 + 2.0 * 6.11897087097168
Epoch 320, val loss: 1.3241263628005981
Epoch 330, training loss: 13.378211975097656 = 1.1530098915100098 + 2.0 * 6.112601280212402
Epoch 330, val loss: 1.288012981414795
Epoch 340, training loss: 13.327796936035156 = 1.1096034049987793 + 2.0 * 6.109096527099609
Epoch 340, val loss: 1.2550054788589478
Epoch 350, training loss: 13.280604362487793 = 1.0693203210830688 + 2.0 * 6.105641841888428
Epoch 350, val loss: 1.2247365713119507
Epoch 360, training loss: 13.239777565002441 = 1.031956434249878 + 2.0 * 6.103910446166992
Epoch 360, val loss: 1.197224497795105
Epoch 370, training loss: 13.201993942260742 = 0.9975409507751465 + 2.0 * 6.102226734161377
Epoch 370, val loss: 1.1723631620407104
Epoch 380, training loss: 13.16055679321289 = 0.9655031561851501 + 2.0 * 6.097527027130127
Epoch 380, val loss: 1.1500110626220703
Epoch 390, training loss: 13.12448501586914 = 0.935314416885376 + 2.0 * 6.094585418701172
Epoch 390, val loss: 1.1292855739593506
Epoch 400, training loss: 13.089473724365234 = 0.9061958193778992 + 2.0 * 6.091639041900635
Epoch 400, val loss: 1.1097248792648315
Epoch 410, training loss: 13.06041145324707 = 0.8778340220451355 + 2.0 * 6.0912885665893555
Epoch 410, val loss: 1.090914249420166
Epoch 420, training loss: 13.026483535766602 = 0.8501591682434082 + 2.0 * 6.088162422180176
Epoch 420, val loss: 1.0731303691864014
Epoch 430, training loss: 12.993402481079102 = 0.8227934241294861 + 2.0 * 6.0853047370910645
Epoch 430, val loss: 1.0557414293289185
Epoch 440, training loss: 12.960675239562988 = 0.7955235838890076 + 2.0 * 6.082575798034668
Epoch 440, val loss: 1.0386070013046265
Epoch 450, training loss: 12.938496589660645 = 0.7681581377983093 + 2.0 * 6.085169315338135
Epoch 450, val loss: 1.0216766595840454
Epoch 460, training loss: 12.904459953308105 = 0.7409469485282898 + 2.0 * 6.081756591796875
Epoch 460, val loss: 1.005082368850708
Epoch 470, training loss: 12.869917869567871 = 0.7138857245445251 + 2.0 * 6.07801628112793
Epoch 470, val loss: 0.9888932108879089
Epoch 480, training loss: 12.838393211364746 = 0.6867960691452026 + 2.0 * 6.075798511505127
Epoch 480, val loss: 0.972750723361969
Epoch 490, training loss: 12.820063591003418 = 0.6597908735275269 + 2.0 * 6.080136299133301
Epoch 490, val loss: 0.9568098187446594
Epoch 500, training loss: 12.77888011932373 = 0.6330786347389221 + 2.0 * 6.072900772094727
Epoch 500, val loss: 0.9415026307106018
Epoch 510, training loss: 12.750688552856445 = 0.606867790222168 + 2.0 * 6.071910381317139
Epoch 510, val loss: 0.9266433119773865
Epoch 520, training loss: 12.721199989318848 = 0.5810808539390564 + 2.0 * 6.070059776306152
Epoch 520, val loss: 0.9120588898658752
Epoch 530, training loss: 12.69428539276123 = 0.5558826923370361 + 2.0 * 6.069201469421387
Epoch 530, val loss: 0.8979185223579407
Epoch 540, training loss: 12.667943954467773 = 0.5314821004867554 + 2.0 * 6.068231105804443
Epoch 540, val loss: 0.8847572207450867
Epoch 550, training loss: 12.641166687011719 = 0.5078200697898865 + 2.0 * 6.066673278808594
Epoch 550, val loss: 0.8721522688865662
Epoch 560, training loss: 12.618185043334961 = 0.48488736152648926 + 2.0 * 6.066648960113525
Epoch 560, val loss: 0.8601397275924683
Epoch 570, training loss: 12.593942642211914 = 0.46277034282684326 + 2.0 * 6.065586090087891
Epoch 570, val loss: 0.8488366603851318
Epoch 580, training loss: 12.571380615234375 = 0.44147560000419617 + 2.0 * 6.064952373504639
Epoch 580, val loss: 0.8382511138916016
Epoch 590, training loss: 12.543972969055176 = 0.42106351256370544 + 2.0 * 6.061454772949219
Epoch 590, val loss: 0.828581690788269
Epoch 600, training loss: 12.520491600036621 = 0.4013057053089142 + 2.0 * 6.0595927238464355
Epoch 600, val loss: 0.8194661140441895
Epoch 610, training loss: 12.516988754272461 = 0.38214343786239624 + 2.0 * 6.067422866821289
Epoch 610, val loss: 0.8110595941543579
Epoch 620, training loss: 12.478850364685059 = 0.36399954557418823 + 2.0 * 6.057425498962402
Epoch 620, val loss: 0.8033121824264526
Epoch 630, training loss: 12.458852767944336 = 0.34653258323669434 + 2.0 * 6.056159973144531
Epoch 630, val loss: 0.7964539527893066
Epoch 640, training loss: 12.43929672241211 = 0.329720139503479 + 2.0 * 6.054788112640381
Epoch 640, val loss: 0.7900409698486328
Epoch 650, training loss: 12.438645362854004 = 0.31357792019844055 + 2.0 * 6.062533855438232
Epoch 650, val loss: 0.7843391299247742
Epoch 660, training loss: 12.405929565429688 = 0.29831448197364807 + 2.0 * 6.053807735443115
Epoch 660, val loss: 0.7790783047676086
Epoch 670, training loss: 12.389558792114258 = 0.2837914228439331 + 2.0 * 6.052883625030518
Epoch 670, val loss: 0.7747681736946106
Epoch 680, training loss: 12.37160587310791 = 0.26990994811058044 + 2.0 * 6.050848007202148
Epoch 680, val loss: 0.7707314491271973
Epoch 690, training loss: 12.37160873413086 = 0.2566405236721039 + 2.0 * 6.057484149932861
Epoch 690, val loss: 0.767221987247467
Epoch 700, training loss: 12.343470573425293 = 0.24404828250408173 + 2.0 * 6.049711227416992
Epoch 700, val loss: 0.7642064690589905
Epoch 710, training loss: 12.327448844909668 = 0.23210199177265167 + 2.0 * 6.047673225402832
Epoch 710, val loss: 0.76191246509552
Epoch 720, training loss: 12.31635570526123 = 0.22067882120609283 + 2.0 * 6.04783821105957
Epoch 720, val loss: 0.7598326802253723
Epoch 730, training loss: 12.303698539733887 = 0.2097635120153427 + 2.0 * 6.046967506408691
Epoch 730, val loss: 0.758179247379303
Epoch 740, training loss: 12.288443565368652 = 0.19934310019016266 + 2.0 * 6.04455041885376
Epoch 740, val loss: 0.7569959163665771
Epoch 750, training loss: 12.276694297790527 = 0.1893835961818695 + 2.0 * 6.0436553955078125
Epoch 750, val loss: 0.7561843991279602
Epoch 760, training loss: 12.2689847946167 = 0.17986173927783966 + 2.0 * 6.044561386108398
Epoch 760, val loss: 0.755659818649292
Epoch 770, training loss: 12.260220527648926 = 0.17083005607128143 + 2.0 * 6.0446953773498535
Epoch 770, val loss: 0.7553269863128662
Epoch 780, training loss: 12.24828815460205 = 0.16225124895572662 + 2.0 * 6.043018341064453
Epoch 780, val loss: 0.7553640007972717
Epoch 790, training loss: 12.233198165893555 = 0.15412138402462006 + 2.0 * 6.039538383483887
Epoch 790, val loss: 0.7557129859924316
Epoch 800, training loss: 12.224568367004395 = 0.14637675881385803 + 2.0 * 6.039095878601074
Epoch 800, val loss: 0.7561996579170227
Epoch 810, training loss: 12.23218059539795 = 0.13900285959243774 + 2.0 * 6.046588897705078
Epoch 810, val loss: 0.7569074034690857
Epoch 820, training loss: 12.207425117492676 = 0.13212484121322632 + 2.0 * 6.037650108337402
Epoch 820, val loss: 0.7577082514762878
Epoch 830, training loss: 12.200632095336914 = 0.12561093270778656 + 2.0 * 6.037510395050049
Epoch 830, val loss: 0.7590633630752563
Epoch 840, training loss: 12.195466041564941 = 0.11948247998952866 + 2.0 * 6.037992000579834
Epoch 840, val loss: 0.760259747505188
Epoch 850, training loss: 12.183941841125488 = 0.11370405554771423 + 2.0 * 6.03511905670166
Epoch 850, val loss: 0.7619015574455261
Epoch 860, training loss: 12.176236152648926 = 0.10826891660690308 + 2.0 * 6.0339837074279785
Epoch 860, val loss: 0.7636027932167053
Epoch 870, training loss: 12.179563522338867 = 0.10312914103269577 + 2.0 * 6.038217067718506
Epoch 870, val loss: 0.7655040621757507
Epoch 880, training loss: 12.175500869750977 = 0.09837888181209564 + 2.0 * 6.03856086730957
Epoch 880, val loss: 0.7671950459480286
Epoch 890, training loss: 12.159113883972168 = 0.09390144050121307 + 2.0 * 6.03260612487793
Epoch 890, val loss: 0.7695704698562622
Epoch 900, training loss: 12.150561332702637 = 0.08969482779502869 + 2.0 * 6.030433177947998
Epoch 900, val loss: 0.7717838287353516
Epoch 910, training loss: 12.147666931152344 = 0.0857301652431488 + 2.0 * 6.030968189239502
Epoch 910, val loss: 0.7741088271141052
Epoch 920, training loss: 12.140486717224121 = 0.08200684934854507 + 2.0 * 6.029240131378174
Epoch 920, val loss: 0.7764214873313904
Epoch 930, training loss: 12.138364791870117 = 0.07852113991975784 + 2.0 * 6.029922008514404
Epoch 930, val loss: 0.7791077494621277
Epoch 940, training loss: 12.134591102600098 = 0.07522965222597122 + 2.0 * 6.0296807289123535
Epoch 940, val loss: 0.7817187905311584
Epoch 950, training loss: 12.126487731933594 = 0.07213258743286133 + 2.0 * 6.027177810668945
Epoch 950, val loss: 0.7843058705329895
Epoch 960, training loss: 12.126569747924805 = 0.06921705603599548 + 2.0 * 6.028676509857178
Epoch 960, val loss: 0.7871652841567993
Epoch 970, training loss: 12.118508338928223 = 0.06646562367677689 + 2.0 * 6.026021480560303
Epoch 970, val loss: 0.7898699045181274
Epoch 980, training loss: 12.115100860595703 = 0.06387414783239365 + 2.0 * 6.025613307952881
Epoch 980, val loss: 0.7928850650787354
Epoch 990, training loss: 12.11028003692627 = 0.06141828000545502 + 2.0 * 6.024430751800537
Epoch 990, val loss: 0.7957643866539001
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8111
Overall ASR: 0.6236
Flip ASR: 0.5511/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.699495315551758 = 1.9519898891448975 + 2.0 * 8.37375259399414
Epoch 0, val loss: 1.9584171772003174
Epoch 10, training loss: 18.686283111572266 = 1.9406403303146362 + 2.0 * 8.372821807861328
Epoch 10, val loss: 1.9457849264144897
Epoch 20, training loss: 18.660823822021484 = 1.925985336303711 + 2.0 * 8.367419242858887
Epoch 20, val loss: 1.9293384552001953
Epoch 30, training loss: 18.5771484375 = 1.906391978263855 + 2.0 * 8.335378646850586
Epoch 30, val loss: 1.9077483415603638
Epoch 40, training loss: 18.11325454711914 = 1.8853693008422852 + 2.0 * 8.113943099975586
Epoch 40, val loss: 1.886177897453308
Epoch 50, training loss: 16.970626831054688 = 1.8615355491638184 + 2.0 * 7.554545879364014
Epoch 50, val loss: 1.8623987436294556
Epoch 60, training loss: 16.172548294067383 = 1.8459477424621582 + 2.0 * 7.163300514221191
Epoch 60, val loss: 1.8487207889556885
Epoch 70, training loss: 15.547457695007324 = 1.835463285446167 + 2.0 * 6.855997085571289
Epoch 70, val loss: 1.8387537002563477
Epoch 80, training loss: 15.210247039794922 = 1.8252887725830078 + 2.0 * 6.692479133605957
Epoch 80, val loss: 1.828452467918396
Epoch 90, training loss: 14.957376480102539 = 1.8152070045471191 + 2.0 * 6.571084976196289
Epoch 90, val loss: 1.8177261352539062
Epoch 100, training loss: 14.761449813842773 = 1.8065663576126099 + 2.0 * 6.477441787719727
Epoch 100, val loss: 1.808363676071167
Epoch 110, training loss: 14.629858016967773 = 1.7992024421691895 + 2.0 * 6.415328025817871
Epoch 110, val loss: 1.8001230955123901
Epoch 120, training loss: 14.519100189208984 = 1.7922996282577515 + 2.0 * 6.363400459289551
Epoch 120, val loss: 1.7921762466430664
Epoch 130, training loss: 14.434333801269531 = 1.7855007648468018 + 2.0 * 6.324416637420654
Epoch 130, val loss: 1.784473180770874
Epoch 140, training loss: 14.366930961608887 = 1.778462290763855 + 2.0 * 6.294234275817871
Epoch 140, val loss: 1.7769078016281128
Epoch 150, training loss: 14.308361053466797 = 1.770915150642395 + 2.0 * 6.268723011016846
Epoch 150, val loss: 1.7693204879760742
Epoch 160, training loss: 14.25698471069336 = 1.7626280784606934 + 2.0 * 6.247178077697754
Epoch 160, val loss: 1.761436104774475
Epoch 170, training loss: 14.211350440979004 = 1.753373146057129 + 2.0 * 6.2289886474609375
Epoch 170, val loss: 1.7530447244644165
Epoch 180, training loss: 14.168850898742676 = 1.7429981231689453 + 2.0 * 6.212926387786865
Epoch 180, val loss: 1.7438759803771973
Epoch 190, training loss: 14.128092765808105 = 1.7311925888061523 + 2.0 * 6.198450088500977
Epoch 190, val loss: 1.7336983680725098
Epoch 200, training loss: 14.094246864318848 = 1.7176162004470825 + 2.0 * 6.188315391540527
Epoch 200, val loss: 1.7222105264663696
Epoch 210, training loss: 14.053566932678223 = 1.7022697925567627 + 2.0 * 6.1756486892700195
Epoch 210, val loss: 1.709362268447876
Epoch 220, training loss: 14.013936996459961 = 1.6846394538879395 + 2.0 * 6.16464900970459
Epoch 220, val loss: 1.6947773694992065
Epoch 230, training loss: 13.975210189819336 = 1.6642990112304688 + 2.0 * 6.155455589294434
Epoch 230, val loss: 1.6780062913894653
Epoch 240, training loss: 13.93978214263916 = 1.6406892538070679 + 2.0 * 6.1495466232299805
Epoch 240, val loss: 1.6586378812789917
Epoch 250, training loss: 13.89498233795166 = 1.6136239767074585 + 2.0 * 6.140679359436035
Epoch 250, val loss: 1.6365478038787842
Epoch 260, training loss: 13.855632781982422 = 1.5829882621765137 + 2.0 * 6.136322021484375
Epoch 260, val loss: 1.6116693019866943
Epoch 270, training loss: 13.806841850280762 = 1.549090027809143 + 2.0 * 6.128875732421875
Epoch 270, val loss: 1.5841898918151855
Epoch 280, training loss: 13.758734703063965 = 1.5119234323501587 + 2.0 * 6.123405456542969
Epoch 280, val loss: 1.5542876720428467
Epoch 290, training loss: 13.711580276489258 = 1.4719743728637695 + 2.0 * 6.119802951812744
Epoch 290, val loss: 1.5223414897918701
Epoch 300, training loss: 13.662714958190918 = 1.4301241636276245 + 2.0 * 6.116295337677002
Epoch 300, val loss: 1.4891694784164429
Epoch 310, training loss: 13.609061241149902 = 1.3872281312942505 + 2.0 * 6.110916614532471
Epoch 310, val loss: 1.4554566144943237
Epoch 320, training loss: 13.564192771911621 = 1.3437334299087524 + 2.0 * 6.1102294921875
Epoch 320, val loss: 1.421711802482605
Epoch 330, training loss: 13.509167671203613 = 1.3007371425628662 + 2.0 * 6.104215145111084
Epoch 330, val loss: 1.3889249563217163
Epoch 340, training loss: 13.460612297058105 = 1.2583272457122803 + 2.0 * 6.101142406463623
Epoch 340, val loss: 1.3566961288452148
Epoch 350, training loss: 13.411198616027832 = 1.2163761854171753 + 2.0 * 6.097411155700684
Epoch 350, val loss: 1.3250925540924072
Epoch 360, training loss: 13.365069389343262 = 1.1746424436569214 + 2.0 * 6.095213413238525
Epoch 360, val loss: 1.2938835620880127
Epoch 370, training loss: 13.318094253540039 = 1.1332728862762451 + 2.0 * 6.092410564422607
Epoch 370, val loss: 1.262955904006958
Epoch 380, training loss: 13.273771286010742 = 1.0922842025756836 + 2.0 * 6.090743541717529
Epoch 380, val loss: 1.2324684858322144
Epoch 390, training loss: 13.227859497070312 = 1.0520552396774292 + 2.0 * 6.087902069091797
Epoch 390, val loss: 1.202600121498108
Epoch 400, training loss: 13.182577133178711 = 1.0126310586929321 + 2.0 * 6.084972858428955
Epoch 400, val loss: 1.1734415292739868
Epoch 410, training loss: 13.13751220703125 = 0.9740535616874695 + 2.0 * 6.081729412078857
Epoch 410, val loss: 1.144851565361023
Epoch 420, training loss: 13.105042457580566 = 0.9363618493080139 + 2.0 * 6.0843400955200195
Epoch 420, val loss: 1.1168705224990845
Epoch 430, training loss: 13.055529594421387 = 0.9001580476760864 + 2.0 * 6.077685832977295
Epoch 430, val loss: 1.0903526544570923
Epoch 440, training loss: 13.015782356262207 = 0.8652005791664124 + 2.0 * 6.075290679931641
Epoch 440, val loss: 1.0647649765014648
Epoch 450, training loss: 12.978262901306152 = 0.8311733603477478 + 2.0 * 6.073544979095459
Epoch 450, val loss: 1.0398672819137573
Epoch 460, training loss: 12.961030960083008 = 0.7980129718780518 + 2.0 * 6.081509113311768
Epoch 460, val loss: 1.0157370567321777
Epoch 470, training loss: 12.914558410644531 = 0.7664791941642761 + 2.0 * 6.074039459228516
Epoch 470, val loss: 0.993026077747345
Epoch 480, training loss: 12.874119758605957 = 0.7361322045326233 + 2.0 * 6.06899356842041
Epoch 480, val loss: 0.9713751673698425
Epoch 490, training loss: 12.842617988586426 = 0.7067586183547974 + 2.0 * 6.067929744720459
Epoch 490, val loss: 0.9505423307418823
Epoch 500, training loss: 12.811932563781738 = 0.6782809495925903 + 2.0 * 6.066825866699219
Epoch 500, val loss: 0.9305126070976257
Epoch 510, training loss: 12.782999992370605 = 0.650836169719696 + 2.0 * 6.066082000732422
Epoch 510, val loss: 0.9115042090415955
Epoch 520, training loss: 12.755668640136719 = 0.6246027946472168 + 2.0 * 6.065532684326172
Epoch 520, val loss: 0.8932222127914429
Epoch 530, training loss: 12.725205421447754 = 0.5995765924453735 + 2.0 * 6.062814235687256
Epoch 530, val loss: 0.8762871026992798
Epoch 540, training loss: 12.696520805358887 = 0.5758271813392639 + 2.0 * 6.060346603393555
Epoch 540, val loss: 0.8603730797767639
Epoch 550, training loss: 12.675673484802246 = 0.5531843304634094 + 2.0 * 6.061244487762451
Epoch 550, val loss: 0.8454801440238953
Epoch 560, training loss: 12.650550842285156 = 0.531770646572113 + 2.0 * 6.059390068054199
Epoch 560, val loss: 0.8316866159439087
Epoch 570, training loss: 12.623024940490723 = 0.5115317106246948 + 2.0 * 6.055746555328369
Epoch 570, val loss: 0.8190661668777466
Epoch 580, training loss: 12.602643013000488 = 0.4922015368938446 + 2.0 * 6.055220603942871
Epoch 580, val loss: 0.8073617219924927
Epoch 590, training loss: 12.58454704284668 = 0.47378814220428467 + 2.0 * 6.055379390716553
Epoch 590, val loss: 0.7965818047523499
Epoch 600, training loss: 12.561359405517578 = 0.45624831318855286 + 2.0 * 6.052555561065674
Epoch 600, val loss: 0.7866752743721008
Epoch 610, training loss: 12.546545028686523 = 0.4393805265426636 + 2.0 * 6.053582191467285
Epoch 610, val loss: 0.7775624394416809
Epoch 620, training loss: 12.526023864746094 = 0.42314618825912476 + 2.0 * 6.051438808441162
Epoch 620, val loss: 0.7691382169723511
Epoch 630, training loss: 12.506598472595215 = 0.40752747654914856 + 2.0 * 6.049535274505615
Epoch 630, val loss: 0.7613131999969482
Epoch 640, training loss: 12.488127708435059 = 0.3924783766269684 + 2.0 * 6.047824859619141
Epoch 640, val loss: 0.7542176842689514
Epoch 650, training loss: 12.47053337097168 = 0.3778010308742523 + 2.0 * 6.046366214752197
Epoch 650, val loss: 0.7476604580879211
Epoch 660, training loss: 12.454559326171875 = 0.36347436904907227 + 2.0 * 6.0455427169799805
Epoch 660, val loss: 0.7415512800216675
Epoch 670, training loss: 12.44266414642334 = 0.3495452404022217 + 2.0 * 6.0465593338012695
Epoch 670, val loss: 0.7359659075737
Epoch 680, training loss: 12.42407512664795 = 0.3360995352268219 + 2.0 * 6.04398775100708
Epoch 680, val loss: 0.7308859825134277
Epoch 690, training loss: 12.408095359802246 = 0.3229619264602661 + 2.0 * 6.042566776275635
Epoch 690, val loss: 0.7263177633285522
Epoch 700, training loss: 12.402935028076172 = 0.3100963532924652 + 2.0 * 6.046419143676758
Epoch 700, val loss: 0.7221789360046387
Epoch 710, training loss: 12.385880470275879 = 0.2976519763469696 + 2.0 * 6.044114112854004
Epoch 710, val loss: 0.7185240983963013
Epoch 720, training loss: 12.365816116333008 = 0.28554847836494446 + 2.0 * 6.040133953094482
Epoch 720, val loss: 0.7153258919715881
Epoch 730, training loss: 12.352279663085938 = 0.27374333143234253 + 2.0 * 6.0392680168151855
Epoch 730, val loss: 0.7126399278640747
Epoch 740, training loss: 12.350680351257324 = 0.2622775435447693 + 2.0 * 6.044201374053955
Epoch 740, val loss: 0.7104058861732483
Epoch 750, training loss: 12.338878631591797 = 0.2512344419956207 + 2.0 * 6.043822288513184
Epoch 750, val loss: 0.70859694480896
Epoch 760, training loss: 12.316043853759766 = 0.24070966243743896 + 2.0 * 6.037667274475098
Epoch 760, val loss: 0.7071887850761414
Epoch 770, training loss: 12.303058624267578 = 0.23054328560829163 + 2.0 * 6.036257743835449
Epoch 770, val loss: 0.7062798738479614
Epoch 780, training loss: 12.291025161743164 = 0.2207173854112625 + 2.0 * 6.035153865814209
Epoch 780, val loss: 0.705833911895752
Epoch 790, training loss: 12.295794486999512 = 0.21128018200397491 + 2.0 * 6.042257308959961
Epoch 790, val loss: 0.7058049440383911
Epoch 800, training loss: 12.269185066223145 = 0.20227394998073578 + 2.0 * 6.0334553718566895
Epoch 800, val loss: 0.7062438130378723
Epoch 810, training loss: 12.260381698608398 = 0.19365160167217255 + 2.0 * 6.033365249633789
Epoch 810, val loss: 0.7071324586868286
Epoch 820, training loss: 12.249914169311523 = 0.18537472188472748 + 2.0 * 6.0322699546813965
Epoch 820, val loss: 0.7084591388702393
Epoch 830, training loss: 12.248958587646484 = 0.1774686574935913 + 2.0 * 6.035745143890381
Epoch 830, val loss: 0.7101837396621704
Epoch 840, training loss: 12.237090110778809 = 0.16997072100639343 + 2.0 * 6.033559799194336
Epoch 840, val loss: 0.7122104167938232
Epoch 850, training loss: 12.22692584991455 = 0.16286298632621765 + 2.0 * 6.032031536102295
Epoch 850, val loss: 0.7145456075668335
Epoch 860, training loss: 12.216252326965332 = 0.15608134865760803 + 2.0 * 6.030085563659668
Epoch 860, val loss: 0.7172902226448059
Epoch 870, training loss: 12.207259178161621 = 0.14960525929927826 + 2.0 * 6.02882719039917
Epoch 870, val loss: 0.7203471660614014
Epoch 880, training loss: 12.205371856689453 = 0.14342442154884338 + 2.0 * 6.0309739112854
Epoch 880, val loss: 0.7237040400505066
Epoch 890, training loss: 12.201539039611816 = 0.13753929734230042 + 2.0 * 6.0320000648498535
Epoch 890, val loss: 0.7272982597351074
Epoch 900, training loss: 12.18506145477295 = 0.13198095560073853 + 2.0 * 6.026540279388428
Epoch 900, val loss: 0.7311487197875977
Epoch 910, training loss: 12.178791046142578 = 0.12667515873908997 + 2.0 * 6.026057720184326
Epoch 910, val loss: 0.7352550029754639
Epoch 920, training loss: 12.176602363586426 = 0.12160135805606842 + 2.0 * 6.027500629425049
Epoch 920, val loss: 0.7395406365394592
Epoch 930, training loss: 12.172224044799805 = 0.11681465804576874 + 2.0 * 6.02770471572876
Epoch 930, val loss: 0.7439514994621277
Epoch 940, training loss: 12.16135025024414 = 0.11230167746543884 + 2.0 * 6.024524211883545
Epoch 940, val loss: 0.7484269738197327
Epoch 950, training loss: 12.15546989440918 = 0.10797540098428726 + 2.0 * 6.023747444152832
Epoch 950, val loss: 0.7531052231788635
Epoch 960, training loss: 12.14902114868164 = 0.1038348376750946 + 2.0 * 6.022593021392822
Epoch 960, val loss: 0.7579458951950073
Epoch 970, training loss: 12.144853591918945 = 0.09986729919910431 + 2.0 * 6.022493362426758
Epoch 970, val loss: 0.7629525065422058
Epoch 980, training loss: 12.148123741149902 = 0.09610120207071304 + 2.0 * 6.0260114669799805
Epoch 980, val loss: 0.7679752111434937
Epoch 990, training loss: 12.140040397644043 = 0.09254159778356552 + 2.0 * 6.023749351501465
Epoch 990, val loss: 0.7729718089103699
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.702547073364258 = 1.9550656080245972 + 2.0 * 8.373741149902344
Epoch 0, val loss: 1.9491021633148193
Epoch 10, training loss: 18.691043853759766 = 1.9453229904174805 + 2.0 * 8.372859954833984
Epoch 10, val loss: 1.9401088953018188
Epoch 20, training loss: 18.66905403137207 = 1.9337679147720337 + 2.0 * 8.367643356323242
Epoch 20, val loss: 1.9289894104003906
Epoch 30, training loss: 18.58574676513672 = 1.919304609298706 + 2.0 * 8.333221435546875
Epoch 30, val loss: 1.9148645401000977
Epoch 40, training loss: 18.065853118896484 = 1.90328049659729 + 2.0 * 8.081286430358887
Epoch 40, val loss: 1.8989741802215576
Epoch 50, training loss: 16.720218658447266 = 1.8828543424606323 + 2.0 * 7.41868257522583
Epoch 50, val loss: 1.8786481618881226
Epoch 60, training loss: 16.076772689819336 = 1.8650401830673218 + 2.0 * 7.105865955352783
Epoch 60, val loss: 1.8621463775634766
Epoch 70, training loss: 15.491488456726074 = 1.8523502349853516 + 2.0 * 6.819569110870361
Epoch 70, val loss: 1.8497809171676636
Epoch 80, training loss: 15.182056427001953 = 1.837872862815857 + 2.0 * 6.672091960906982
Epoch 80, val loss: 1.8358620405197144
Epoch 90, training loss: 14.937944412231445 = 1.826485276222229 + 2.0 * 6.555729389190674
Epoch 90, val loss: 1.82479727268219
Epoch 100, training loss: 14.759758949279785 = 1.8169466257095337 + 2.0 * 6.471405982971191
Epoch 100, val loss: 1.8156596422195435
Epoch 110, training loss: 14.628483772277832 = 1.8078324794769287 + 2.0 * 6.410325527191162
Epoch 110, val loss: 1.8067457675933838
Epoch 120, training loss: 14.532781600952148 = 1.79915189743042 + 2.0 * 6.366815090179443
Epoch 120, val loss: 1.7980061769485474
Epoch 130, training loss: 14.466066360473633 = 1.7910727262496948 + 2.0 * 6.337496757507324
Epoch 130, val loss: 1.7896555662155151
Epoch 140, training loss: 14.417876243591309 = 1.783136010169983 + 2.0 * 6.3173699378967285
Epoch 140, val loss: 1.7815046310424805
Epoch 150, training loss: 14.360429763793945 = 1.7750139236450195 + 2.0 * 6.292707920074463
Epoch 150, val loss: 1.7735371589660645
Epoch 160, training loss: 14.30880069732666 = 1.7667157649993896 + 2.0 * 6.271042346954346
Epoch 160, val loss: 1.7656550407409668
Epoch 170, training loss: 14.26025104522705 = 1.7578790187835693 + 2.0 * 6.251185894012451
Epoch 170, val loss: 1.757568120956421
Epoch 180, training loss: 14.217178344726562 = 1.7480882406234741 + 2.0 * 6.2345452308654785
Epoch 180, val loss: 1.7488545179367065
Epoch 190, training loss: 14.1759672164917 = 1.7371028661727905 + 2.0 * 6.219432353973389
Epoch 190, val loss: 1.7394354343414307
Epoch 200, training loss: 14.137804985046387 = 1.7246137857437134 + 2.0 * 6.206595420837402
Epoch 200, val loss: 1.7287687063217163
Epoch 210, training loss: 14.100245475769043 = 1.7102423906326294 + 2.0 * 6.195001602172852
Epoch 210, val loss: 1.7168337106704712
Epoch 220, training loss: 14.062715530395508 = 1.693841814994812 + 2.0 * 6.184436798095703
Epoch 220, val loss: 1.703155279159546
Epoch 230, training loss: 14.024175643920898 = 1.6748443841934204 + 2.0 * 6.174665451049805
Epoch 230, val loss: 1.687466025352478
Epoch 240, training loss: 13.987571716308594 = 1.6527248620986938 + 2.0 * 6.167423248291016
Epoch 240, val loss: 1.6691228151321411
Epoch 250, training loss: 13.943183898925781 = 1.6270912885665894 + 2.0 * 6.158046245574951
Epoch 250, val loss: 1.6478642225265503
Epoch 260, training loss: 13.89997673034668 = 1.5973466634750366 + 2.0 * 6.151315212249756
Epoch 260, val loss: 1.6231887340545654
Epoch 270, training loss: 13.851778984069824 = 1.5627387762069702 + 2.0 * 6.144520282745361
Epoch 270, val loss: 1.5942778587341309
Epoch 280, training loss: 13.802412033081055 = 1.5225385427474976 + 2.0 * 6.139936923980713
Epoch 280, val loss: 1.5606484413146973
Epoch 290, training loss: 13.745230674743652 = 1.4773024320602417 + 2.0 * 6.1339640617370605
Epoch 290, val loss: 1.5226644277572632
Epoch 300, training loss: 13.687922477722168 = 1.4272106885910034 + 2.0 * 6.1303558349609375
Epoch 300, val loss: 1.4807716608047485
Epoch 310, training loss: 13.62489128112793 = 1.3729283809661865 + 2.0 * 6.125981330871582
Epoch 310, val loss: 1.4352986812591553
Epoch 320, training loss: 13.565240859985352 = 1.3160158395767212 + 2.0 * 6.124612331390381
Epoch 320, val loss: 1.3876198530197144
Epoch 330, training loss: 13.496464729309082 = 1.257895827293396 + 2.0 * 6.119284629821777
Epoch 330, val loss: 1.3392870426177979
Epoch 340, training loss: 13.430352210998535 = 1.1996270418167114 + 2.0 * 6.115362644195557
Epoch 340, val loss: 1.290930151939392
Epoch 350, training loss: 13.37087631225586 = 1.1421658992767334 + 2.0 * 6.114355087280273
Epoch 350, val loss: 1.2435325384140015
Epoch 360, training loss: 13.30825138092041 = 1.087555170059204 + 2.0 * 6.110348224639893
Epoch 360, val loss: 1.1986792087554932
Epoch 370, training loss: 13.249231338500977 = 1.0361177921295166 + 2.0 * 6.1065568923950195
Epoch 370, val loss: 1.1568440198898315
Epoch 380, training loss: 13.19430160522461 = 0.9874253273010254 + 2.0 * 6.103437900543213
Epoch 380, val loss: 1.1176810264587402
Epoch 390, training loss: 13.153759956359863 = 0.9413378238677979 + 2.0 * 6.106211185455322
Epoch 390, val loss: 1.0812737941741943
Epoch 400, training loss: 13.09687328338623 = 0.8984596133232117 + 2.0 * 6.099206924438477
Epoch 400, val loss: 1.0479028224945068
Epoch 410, training loss: 13.050697326660156 = 0.8581933379173279 + 2.0 * 6.096251964569092
Epoch 410, val loss: 1.01705801486969
Epoch 420, training loss: 13.007074356079102 = 0.8203667998313904 + 2.0 * 6.093353748321533
Epoch 420, val loss: 0.9885907173156738
Epoch 430, training loss: 12.965460777282715 = 0.7847698926925659 + 2.0 * 6.09034538269043
Epoch 430, val loss: 0.962489128112793
Epoch 440, training loss: 12.932512283325195 = 0.7510740756988525 + 2.0 * 6.090719223022461
Epoch 440, val loss: 0.9383092522621155
Epoch 450, training loss: 12.893460273742676 = 0.7195662260055542 + 2.0 * 6.086946964263916
Epoch 450, val loss: 0.9161925315856934
Epoch 460, training loss: 12.856902122497559 = 0.6899491548538208 + 2.0 * 6.083476543426514
Epoch 460, val loss: 0.8960745930671692
Epoch 470, training loss: 12.823719024658203 = 0.661920428276062 + 2.0 * 6.080899238586426
Epoch 470, val loss: 0.8775074481964111
Epoch 480, training loss: 12.797566413879395 = 0.6353076100349426 + 2.0 * 6.081129550933838
Epoch 480, val loss: 0.8603161573410034
Epoch 490, training loss: 12.766666412353516 = 0.6102410554885864 + 2.0 * 6.078212738037109
Epoch 490, val loss: 0.8446871638298035
Epoch 500, training loss: 12.738736152648926 = 0.5867243409156799 + 2.0 * 6.076005935668945
Epoch 500, val loss: 0.8304718732833862
Epoch 510, training loss: 12.710356712341309 = 0.5642470717430115 + 2.0 * 6.073054790496826
Epoch 510, val loss: 0.8172925710678101
Epoch 520, training loss: 12.685361862182617 = 0.5426170825958252 + 2.0 * 6.0713725090026855
Epoch 520, val loss: 0.8049875497817993
Epoch 530, training loss: 12.664412498474121 = 0.5218932628631592 + 2.0 * 6.071259498596191
Epoch 530, val loss: 0.7935478687286377
Epoch 540, training loss: 12.640701293945312 = 0.5021090507507324 + 2.0 * 6.069296360015869
Epoch 540, val loss: 0.7830792665481567
Epoch 550, training loss: 12.621110916137695 = 0.4829952120780945 + 2.0 * 6.069057941436768
Epoch 550, val loss: 0.7733021974563599
Epoch 560, training loss: 12.593903541564941 = 0.4643106460571289 + 2.0 * 6.064796447753906
Epoch 560, val loss: 0.7641509175300598
Epoch 570, training loss: 12.575079917907715 = 0.4460662305355072 + 2.0 * 6.064507007598877
Epoch 570, val loss: 0.755469799041748
Epoch 580, training loss: 12.554915428161621 = 0.4282427132129669 + 2.0 * 6.063336372375488
Epoch 580, val loss: 0.7472710609436035
Epoch 590, training loss: 12.530835151672363 = 0.4106425344944 + 2.0 * 6.060096263885498
Epoch 590, val loss: 0.7396277785301208
Epoch 600, training loss: 12.510828971862793 = 0.39329054951667786 + 2.0 * 6.058769226074219
Epoch 600, val loss: 0.7322664856910706
Epoch 610, training loss: 12.500468254089355 = 0.3760356307029724 + 2.0 * 6.062216281890869
Epoch 610, val loss: 0.7253161072731018
Epoch 620, training loss: 12.474929809570312 = 0.3592153489589691 + 2.0 * 6.057857036590576
Epoch 620, val loss: 0.7187061905860901
Epoch 630, training loss: 12.451621055603027 = 0.3426704406738281 + 2.0 * 6.0544753074646
Epoch 630, val loss: 0.7126604914665222
Epoch 640, training loss: 12.432673454284668 = 0.3263638913631439 + 2.0 * 6.053154945373535
Epoch 640, val loss: 0.7068891525268555
Epoch 650, training loss: 12.430913925170898 = 0.3103039860725403 + 2.0 * 6.060305118560791
Epoch 650, val loss: 0.7014728784561157
Epoch 660, training loss: 12.401095390319824 = 0.2948121428489685 + 2.0 * 6.0531415939331055
Epoch 660, val loss: 0.6964960694313049
Epoch 670, training loss: 12.380054473876953 = 0.27984172105789185 + 2.0 * 6.050106525421143
Epoch 670, val loss: 0.6920232176780701
Epoch 680, training loss: 12.366058349609375 = 0.265336275100708 + 2.0 * 6.050361156463623
Epoch 680, val loss: 0.6879771947860718
Epoch 690, training loss: 12.349457740783691 = 0.2513754963874817 + 2.0 * 6.049041271209717
Epoch 690, val loss: 0.6844359040260315
Epoch 700, training loss: 12.335264205932617 = 0.23798884451389313 + 2.0 * 6.048637866973877
Epoch 700, val loss: 0.6813994646072388
Epoch 710, training loss: 12.322941780090332 = 0.2252800166606903 + 2.0 * 6.048830986022949
Epoch 710, val loss: 0.6788532733917236
Epoch 720, training loss: 12.309264183044434 = 0.21322911977767944 + 2.0 * 6.048017501831055
Epoch 720, val loss: 0.67684406042099
Epoch 730, training loss: 12.2954683303833 = 0.2019372582435608 + 2.0 * 6.046765327453613
Epoch 730, val loss: 0.6753741502761841
Epoch 740, training loss: 12.279318809509277 = 0.19134868681430817 + 2.0 * 6.043984889984131
Epoch 740, val loss: 0.6743605732917786
Epoch 750, training loss: 12.27168083190918 = 0.18135857582092285 + 2.0 * 6.045161247253418
Epoch 750, val loss: 0.6738518476486206
Epoch 760, training loss: 12.253966331481934 = 0.1720275580883026 + 2.0 * 6.040969371795654
Epoch 760, val loss: 0.6737797260284424
Epoch 770, training loss: 12.244290351867676 = 0.1632539927959442 + 2.0 * 6.040518283843994
Epoch 770, val loss: 0.6741661429405212
Epoch 780, training loss: 12.237083435058594 = 0.1550276130437851 + 2.0 * 6.041028022766113
Epoch 780, val loss: 0.6749813556671143
Epoch 790, training loss: 12.235796928405762 = 0.14737258851528168 + 2.0 * 6.044212341308594
Epoch 790, val loss: 0.676145613193512
Epoch 800, training loss: 12.216160774230957 = 0.14030972123146057 + 2.0 * 6.037925720214844
Epoch 800, val loss: 0.6775964498519897
Epoch 810, training loss: 12.207880973815918 = 0.13369634747505188 + 2.0 * 6.037092208862305
Epoch 810, val loss: 0.6794356107711792
Epoch 820, training loss: 12.198747634887695 = 0.12747998535633087 + 2.0 * 6.0356340408325195
Epoch 820, val loss: 0.6815937757492065
Epoch 830, training loss: 12.209004402160645 = 0.12166406959295273 + 2.0 * 6.043670177459717
Epoch 830, val loss: 0.6839979290962219
Epoch 840, training loss: 12.192370414733887 = 0.11621291190385818 + 2.0 * 6.038078784942627
Epoch 840, val loss: 0.686676561832428
Epoch 850, training loss: 12.178349494934082 = 0.1111467257142067 + 2.0 * 6.0336012840271
Epoch 850, val loss: 0.6895765066146851
Epoch 860, training loss: 12.171961784362793 = 0.10637704282999039 + 2.0 * 6.032792568206787
Epoch 860, val loss: 0.6926689743995667
Epoch 870, training loss: 12.181092262268066 = 0.10186463594436646 + 2.0 * 6.039613723754883
Epoch 870, val loss: 0.695966899394989
Epoch 880, training loss: 12.16530704498291 = 0.09766718000173569 + 2.0 * 6.033820152282715
Epoch 880, val loss: 0.6994002461433411
Epoch 890, training loss: 12.158474922180176 = 0.09369843453168869 + 2.0 * 6.032388210296631
Epoch 890, val loss: 0.7029570937156677
Epoch 900, training loss: 12.149916648864746 = 0.08996675908565521 + 2.0 * 6.029974937438965
Epoch 900, val loss: 0.7066100239753723
Epoch 910, training loss: 12.145562171936035 = 0.0864388570189476 + 2.0 * 6.029561519622803
Epoch 910, val loss: 0.7103963494300842
Epoch 920, training loss: 12.13956069946289 = 0.0830865353345871 + 2.0 * 6.0282368659973145
Epoch 920, val loss: 0.7142989635467529
Epoch 930, training loss: 12.143231391906738 = 0.07991909235715866 + 2.0 * 6.031656265258789
Epoch 930, val loss: 0.7182666063308716
Epoch 940, training loss: 12.135882377624512 = 0.07693792879581451 + 2.0 * 6.029472351074219
Epoch 940, val loss: 0.7222592234611511
Epoch 950, training loss: 12.126372337341309 = 0.0741233304142952 + 2.0 * 6.026124477386475
Epoch 950, val loss: 0.7263396382331848
Epoch 960, training loss: 12.127102851867676 = 0.0714353621006012 + 2.0 * 6.027833938598633
Epoch 960, val loss: 0.7304898500442505
Epoch 970, training loss: 12.119879722595215 = 0.06888614594936371 + 2.0 * 6.025496959686279
Epoch 970, val loss: 0.7346749305725098
Epoch 980, training loss: 12.115461349487305 = 0.0664735659956932 + 2.0 * 6.02449369430542
Epoch 980, val loss: 0.7388439774513245
Epoch 990, training loss: 12.122650146484375 = 0.06418357044458389 + 2.0 * 6.029233455657959
Epoch 990, val loss: 0.7430195212364197
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8148
Overall ASR: 0.9594
Flip ASR: 0.9511/225 nodes
The final ASR:0.85486, 0.16376, Accuracy:0.81481, 0.00302
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11640])
remove edge: torch.Size([2, 9570])
updated graph: torch.Size([2, 10654])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9705
Flip ASR: 0.9644/225 nodes
The final ASR:0.97540, 0.00348, Accuracy:0.83457, 0.00349
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.5 der2:0.3 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.3, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 18.69701385498047 = 1.9494552612304688 + 2.0 * 8.373779296875
Epoch 0, val loss: 1.946325421333313
Epoch 10, training loss: 18.684816360473633 = 1.9389058351516724 + 2.0 * 8.372955322265625
Epoch 10, val loss: 1.9360896348953247
Epoch 20, training loss: 18.66159439086914 = 1.9257659912109375 + 2.0 * 8.367914199829102
Epoch 20, val loss: 1.9227937459945679
Epoch 30, training loss: 18.578245162963867 = 1.9083571434020996 + 2.0 * 8.334943771362305
Epoch 30, val loss: 1.9048787355422974
Epoch 40, training loss: 18.078170776367188 = 1.8890631198883057 + 2.0 * 8.09455394744873
Epoch 40, val loss: 1.8856587409973145
Epoch 50, training loss: 16.33255386352539 = 1.8682368993759155 + 2.0 * 7.232158184051514
Epoch 50, val loss: 1.8652561902999878
Epoch 60, training loss: 15.716763496398926 = 1.855015516281128 + 2.0 * 6.930873870849609
Epoch 60, val loss: 1.8536150455474854
Epoch 70, training loss: 15.276291847229004 = 1.8444260358810425 + 2.0 * 6.715932846069336
Epoch 70, val loss: 1.843101143836975
Epoch 80, training loss: 15.000531196594238 = 1.8323501348495483 + 2.0 * 6.584090709686279
Epoch 80, val loss: 1.8315119743347168
Epoch 90, training loss: 14.825824737548828 = 1.8221534490585327 + 2.0 * 6.501835823059082
Epoch 90, val loss: 1.821195363998413
Epoch 100, training loss: 14.708377838134766 = 1.8120427131652832 + 2.0 * 6.448167324066162
Epoch 100, val loss: 1.8111779689788818
Epoch 110, training loss: 14.61703109741211 = 1.8027652502059937 + 2.0 * 6.407133102416992
Epoch 110, val loss: 1.8018569946289062
Epoch 120, training loss: 14.538183212280273 = 1.7942891120910645 + 2.0 * 6.371946811676025
Epoch 120, val loss: 1.7935535907745361
Epoch 130, training loss: 14.46822452545166 = 1.7863168716430664 + 2.0 * 6.340953826904297
Epoch 130, val loss: 1.7860339879989624
Epoch 140, training loss: 14.406864166259766 = 1.7787785530090332 + 2.0 * 6.314043045043945
Epoch 140, val loss: 1.7791944742202759
Epoch 150, training loss: 14.347935676574707 = 1.7712651491165161 + 2.0 * 6.28833532333374
Epoch 150, val loss: 1.7726210355758667
Epoch 160, training loss: 14.296602249145508 = 1.7632745504379272 + 2.0 * 6.266664028167725
Epoch 160, val loss: 1.7658792734146118
Epoch 170, training loss: 14.251394271850586 = 1.754404902458191 + 2.0 * 6.248494625091553
Epoch 170, val loss: 1.7585667371749878
Epoch 180, training loss: 14.210789680480957 = 1.7444524765014648 + 2.0 * 6.233168601989746
Epoch 180, val loss: 1.7504761219024658
Epoch 190, training loss: 14.168339729309082 = 1.733163595199585 + 2.0 * 6.217587947845459
Epoch 190, val loss: 1.741420030593872
Epoch 200, training loss: 14.12977409362793 = 1.7203235626220703 + 2.0 * 6.20472526550293
Epoch 200, val loss: 1.7311861515045166
Epoch 210, training loss: 14.093361854553223 = 1.705509901046753 + 2.0 * 6.193925857543945
Epoch 210, val loss: 1.7193529605865479
Epoch 220, training loss: 14.055532455444336 = 1.688449740409851 + 2.0 * 6.183541297912598
Epoch 220, val loss: 1.7057040929794312
Epoch 230, training loss: 14.018753051757812 = 1.6687393188476562 + 2.0 * 6.175006866455078
Epoch 230, val loss: 1.6900056600570679
Epoch 240, training loss: 13.980762481689453 = 1.645920753479004 + 2.0 * 6.167420864105225
Epoch 240, val loss: 1.6717227697372437
Epoch 250, training loss: 13.942939758300781 = 1.6195181608200073 + 2.0 * 6.161710739135742
Epoch 250, val loss: 1.6505063772201538
Epoch 260, training loss: 13.8978853225708 = 1.5892192125320435 + 2.0 * 6.154333114624023
Epoch 260, val loss: 1.6261917352676392
Epoch 270, training loss: 13.852611541748047 = 1.5547839403152466 + 2.0 * 6.148913860321045
Epoch 270, val loss: 1.5983511209487915
Epoch 280, training loss: 13.803840637207031 = 1.5157828330993652 + 2.0 * 6.144028663635254
Epoch 280, val loss: 1.5668662786483765
Epoch 290, training loss: 13.757024765014648 = 1.4725430011749268 + 2.0 * 6.14224100112915
Epoch 290, val loss: 1.5321426391601562
Epoch 300, training loss: 13.696796417236328 = 1.426326870918274 + 2.0 * 6.135234832763672
Epoch 300, val loss: 1.4950121641159058
Epoch 310, training loss: 13.638761520385742 = 1.377432942390442 + 2.0 * 6.130664348602295
Epoch 310, val loss: 1.4561140537261963
Epoch 320, training loss: 13.580923080444336 = 1.32662034034729 + 2.0 * 6.1271514892578125
Epoch 320, val loss: 1.416028380393982
Epoch 330, training loss: 13.524238586425781 = 1.275007963180542 + 2.0 * 6.12461519241333
Epoch 330, val loss: 1.376029133796692
Epoch 340, training loss: 13.466456413269043 = 1.2243014574050903 + 2.0 * 6.121077537536621
Epoch 340, val loss: 1.337340235710144
Epoch 350, training loss: 13.410338401794434 = 1.1746081113815308 + 2.0 * 6.117865085601807
Epoch 350, val loss: 1.2999104261398315
Epoch 360, training loss: 13.356066703796387 = 1.1260898113250732 + 2.0 * 6.114988327026367
Epoch 360, val loss: 1.2636886835098267
Epoch 370, training loss: 13.302938461303711 = 1.0792316198349 + 2.0 * 6.11185359954834
Epoch 370, val loss: 1.2290260791778564
Epoch 380, training loss: 13.252567291259766 = 1.0343204736709595 + 2.0 * 6.109123229980469
Epoch 380, val loss: 1.1961846351623535
Epoch 390, training loss: 13.203571319580078 = 0.9914640188217163 + 2.0 * 6.106053829193115
Epoch 390, val loss: 1.1651685237884521
Epoch 400, training loss: 13.16411018371582 = 0.9510946273803711 + 2.0 * 6.106507778167725
Epoch 400, val loss: 1.1362894773483276
Epoch 410, training loss: 13.11440658569336 = 0.9133098721504211 + 2.0 * 6.100548267364502
Epoch 410, val loss: 1.109864592552185
Epoch 420, training loss: 13.072770118713379 = 0.8776072263717651 + 2.0 * 6.097581386566162
Epoch 420, val loss: 1.0850632190704346
Epoch 430, training loss: 13.03394603729248 = 0.8436213135719299 + 2.0 * 6.095162391662598
Epoch 430, val loss: 1.0619292259216309
Epoch 440, training loss: 12.998991966247559 = 0.8113994598388672 + 2.0 * 6.093796253204346
Epoch 440, val loss: 1.0404982566833496
Epoch 450, training loss: 12.964400291442871 = 0.7809686064720154 + 2.0 * 6.0917158126831055
Epoch 450, val loss: 1.020761251449585
Epoch 460, training loss: 12.931859970092773 = 0.7519738078117371 + 2.0 * 6.089942932128906
Epoch 460, val loss: 1.002225399017334
Epoch 470, training loss: 12.89778995513916 = 0.724204421043396 + 2.0 * 6.086792945861816
Epoch 470, val loss: 0.9847779273986816
Epoch 480, training loss: 12.869333267211914 = 0.6971745491027832 + 2.0 * 6.086079120635986
Epoch 480, val loss: 0.9681406617164612
Epoch 490, training loss: 12.839006423950195 = 0.6708993315696716 + 2.0 * 6.0840535163879395
Epoch 490, val loss: 0.9524270296096802
Epoch 500, training loss: 12.808696746826172 = 0.6454030871391296 + 2.0 * 6.081646919250488
Epoch 500, val loss: 0.9374573230743408
Epoch 510, training loss: 12.778524398803711 = 0.620309591293335 + 2.0 * 6.079107284545898
Epoch 510, val loss: 0.9230794906616211
Epoch 520, training loss: 12.755081176757812 = 0.5958194732666016 + 2.0 * 6.0796308517456055
Epoch 520, val loss: 0.9091781973838806
Epoch 530, training loss: 12.725186347961426 = 0.5722241997718811 + 2.0 * 6.076480865478516
Epoch 530, val loss: 0.8963320851325989
Epoch 540, training loss: 12.698089599609375 = 0.5491095185279846 + 2.0 * 6.074490070343018
Epoch 540, val loss: 0.8839733004570007
Epoch 550, training loss: 12.670902252197266 = 0.5263693332672119 + 2.0 * 6.072266578674316
Epoch 550, val loss: 0.8721505403518677
Epoch 560, training loss: 12.662176132202148 = 0.5039517879486084 + 2.0 * 6.0791120529174805
Epoch 560, val loss: 0.8608163595199585
Epoch 570, training loss: 12.622257232666016 = 0.4821985960006714 + 2.0 * 6.070029258728027
Epoch 570, val loss: 0.8500663042068481
Epoch 580, training loss: 12.599028587341309 = 0.46096014976501465 + 2.0 * 6.069034099578857
Epoch 580, val loss: 0.8399961590766907
Epoch 590, training loss: 12.576668739318848 = 0.440176784992218 + 2.0 * 6.068245887756348
Epoch 590, val loss: 0.8303395509719849
Epoch 600, training loss: 12.55687141418457 = 0.4199068248271942 + 2.0 * 6.068482398986816
Epoch 600, val loss: 0.8213422298431396
Epoch 610, training loss: 12.530200958251953 = 0.40006718039512634 + 2.0 * 6.065066814422607
Epoch 610, val loss: 0.8128549456596375
Epoch 620, training loss: 12.507233619689941 = 0.38066577911376953 + 2.0 * 6.063283920288086
Epoch 620, val loss: 0.8048512935638428
Epoch 630, training loss: 12.496099472045898 = 0.3616180419921875 + 2.0 * 6.0672407150268555
Epoch 630, val loss: 0.7973243594169617
Epoch 640, training loss: 12.469934463500977 = 0.34311750531196594 + 2.0 * 6.063408374786377
Epoch 640, val loss: 0.7903761863708496
Epoch 650, training loss: 12.445754051208496 = 0.32517144083976746 + 2.0 * 6.060291290283203
Epoch 650, val loss: 0.7840941548347473
Epoch 660, training loss: 12.426029205322266 = 0.30767008662223816 + 2.0 * 6.059179782867432
Epoch 660, val loss: 0.7783394455909729
Epoch 670, training loss: 12.419351577758789 = 0.29072996973991394 + 2.0 * 6.0643110275268555
Epoch 670, val loss: 0.7731398344039917
Epoch 680, training loss: 12.389421463012695 = 0.2744307518005371 + 2.0 * 6.0574951171875
Epoch 680, val loss: 0.768717348575592
Epoch 690, training loss: 12.379480361938477 = 0.2587915360927582 + 2.0 * 6.060344219207764
Epoch 690, val loss: 0.7649168372154236
Epoch 700, training loss: 12.357955932617188 = 0.24388882517814636 + 2.0 * 6.057033538818359
Epoch 700, val loss: 0.7618279457092285
Epoch 710, training loss: 12.337225914001465 = 0.22968636453151703 + 2.0 * 6.053769588470459
Epoch 710, val loss: 0.759406328201294
Epoch 720, training loss: 12.321390151977539 = 0.2161550372838974 + 2.0 * 6.05261754989624
Epoch 720, val loss: 0.7576691508293152
Epoch 730, training loss: 12.310243606567383 = 0.20337630808353424 + 2.0 * 6.053433418273926
Epoch 730, val loss: 0.7565398812294006
Epoch 740, training loss: 12.2971830368042 = 0.191458061337471 + 2.0 * 6.052862644195557
Epoch 740, val loss: 0.756146252155304
Epoch 750, training loss: 12.286215782165527 = 0.18028880655765533 + 2.0 * 6.0529632568359375
Epoch 750, val loss: 0.7563983201980591
Epoch 760, training loss: 12.26844596862793 = 0.16993966698646545 + 2.0 * 6.049252986907959
Epoch 760, val loss: 0.7572944760322571
Epoch 770, training loss: 12.259058952331543 = 0.16028670966625214 + 2.0 * 6.049386024475098
Epoch 770, val loss: 0.7587662935256958
Epoch 780, training loss: 12.247954368591309 = 0.15133655071258545 + 2.0 * 6.048308849334717
Epoch 780, val loss: 0.7607546448707581
Epoch 790, training loss: 12.236557006835938 = 0.1430794596672058 + 2.0 * 6.046738624572754
Epoch 790, val loss: 0.7632640600204468
Epoch 800, training loss: 12.227581977844238 = 0.13538870215415955 + 2.0 * 6.0460968017578125
Epoch 800, val loss: 0.7661985158920288
Epoch 810, training loss: 12.217848777770996 = 0.12825900316238403 + 2.0 * 6.044795036315918
Epoch 810, val loss: 0.7695111036300659
Epoch 820, training loss: 12.209328651428223 = 0.12166064232587814 + 2.0 * 6.043834209442139
Epoch 820, val loss: 0.7731738090515137
Epoch 830, training loss: 12.209123611450195 = 0.11553394794464111 + 2.0 * 6.046794891357422
Epoch 830, val loss: 0.7770928740501404
Epoch 840, training loss: 12.193881034851074 = 0.10985997319221497 + 2.0 * 6.042010307312012
Epoch 840, val loss: 0.78119295835495
Epoch 850, training loss: 12.187430381774902 = 0.10460253059864044 + 2.0 * 6.0414137840271
Epoch 850, val loss: 0.7856065034866333
Epoch 860, training loss: 12.179119110107422 = 0.09966936707496643 + 2.0 * 6.039724826812744
Epoch 860, val loss: 0.7902053594589233
Epoch 870, training loss: 12.177741050720215 = 0.09504152834415436 + 2.0 * 6.0413498878479
Epoch 870, val loss: 0.7949056029319763
Epoch 880, training loss: 12.167486190795898 = 0.09073835611343384 + 2.0 * 6.038373947143555
Epoch 880, val loss: 0.7997191548347473
Epoch 890, training loss: 12.164875030517578 = 0.08672695606946945 + 2.0 * 6.039073944091797
Epoch 890, val loss: 0.8046313524246216
Epoch 900, training loss: 12.156536102294922 = 0.0829605981707573 + 2.0 * 6.036787986755371
Epoch 900, val loss: 0.8097010850906372
Epoch 910, training loss: 12.177177429199219 = 0.07940521836280823 + 2.0 * 6.048886299133301
Epoch 910, val loss: 0.8147256374359131
Epoch 920, training loss: 12.147322654724121 = 0.07613455504179001 + 2.0 * 6.0355939865112305
Epoch 920, val loss: 0.8197721838951111
Epoch 930, training loss: 12.143427848815918 = 0.07304717600345612 + 2.0 * 6.035190105438232
Epoch 930, val loss: 0.8249762654304504
Epoch 940, training loss: 12.137654304504395 = 0.07011251151561737 + 2.0 * 6.03377103805542
Epoch 940, val loss: 0.830189049243927
Epoch 950, training loss: 12.149740219116211 = 0.06734658777713776 + 2.0 * 6.041196823120117
Epoch 950, val loss: 0.8353713154792786
Epoch 960, training loss: 12.132170677185059 = 0.06475207954645157 + 2.0 * 6.033709526062012
Epoch 960, val loss: 0.8405634164810181
Epoch 970, training loss: 12.124542236328125 = 0.06229546666145325 + 2.0 * 6.031123161315918
Epoch 970, val loss: 0.8458040356636047
Epoch 980, training loss: 12.148703575134277 = 0.0599692203104496 + 2.0 * 6.04436731338501
Epoch 980, val loss: 0.8510215282440186
Epoch 990, training loss: 12.117884635925293 = 0.057787925004959106 + 2.0 * 6.030048370361328
Epoch 990, val loss: 0.8561165928840637
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.7417
Flip ASR: 0.6889/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.686298370361328 = 1.9386703968048096 + 2.0 * 8.37381362915039
Epoch 0, val loss: 1.9381991624832153
Epoch 10, training loss: 18.675140380859375 = 1.9290409088134766 + 2.0 * 8.37304973602295
Epoch 10, val loss: 1.9284778833389282
Epoch 20, training loss: 18.653358459472656 = 1.9174472093582153 + 2.0 * 8.367955207824707
Epoch 20, val loss: 1.916259765625
Epoch 30, training loss: 18.55708885192871 = 1.9026134014129639 + 2.0 * 8.327238082885742
Epoch 30, val loss: 1.9003702402114868
Epoch 40, training loss: 17.788314819335938 = 1.8857319355010986 + 2.0 * 7.951291561126709
Epoch 40, val loss: 1.8823647499084473
Epoch 50, training loss: 16.39821434020996 = 1.8696011304855347 + 2.0 * 7.264306545257568
Epoch 50, val loss: 1.8661168813705444
Epoch 60, training loss: 15.522889137268066 = 1.859075665473938 + 2.0 * 6.831906795501709
Epoch 60, val loss: 1.8562557697296143
Epoch 70, training loss: 15.149545669555664 = 1.8496527671813965 + 2.0 * 6.649946689605713
Epoch 70, val loss: 1.8469793796539307
Epoch 80, training loss: 14.907352447509766 = 1.841198444366455 + 2.0 * 6.533076763153076
Epoch 80, val loss: 1.8381203413009644
Epoch 90, training loss: 14.742467880249023 = 1.832808017730713 + 2.0 * 6.454830169677734
Epoch 90, val loss: 1.829316258430481
Epoch 100, training loss: 14.625005722045898 = 1.8247895240783691 + 2.0 * 6.4001078605651855
Epoch 100, val loss: 1.8208539485931396
Epoch 110, training loss: 14.526318550109863 = 1.8174810409545898 + 2.0 * 6.354418754577637
Epoch 110, val loss: 1.8129472732543945
Epoch 120, training loss: 14.448384284973145 = 1.8106406927108765 + 2.0 * 6.318871974945068
Epoch 120, val loss: 1.8053680658340454
Epoch 130, training loss: 14.384122848510742 = 1.8039665222167969 + 2.0 * 6.290078163146973
Epoch 130, val loss: 1.7978249788284302
Epoch 140, training loss: 14.331568717956543 = 1.7972437143325806 + 2.0 * 6.267162322998047
Epoch 140, val loss: 1.7902923822402954
Epoch 150, training loss: 14.284300804138184 = 1.7903411388397217 + 2.0 * 6.246979713439941
Epoch 150, val loss: 1.7828489542007446
Epoch 160, training loss: 14.241217613220215 = 1.78315007686615 + 2.0 * 6.229033946990967
Epoch 160, val loss: 1.775323748588562
Epoch 170, training loss: 14.207082748413086 = 1.7753465175628662 + 2.0 * 6.21586799621582
Epoch 170, val loss: 1.7674607038497925
Epoch 180, training loss: 14.168428421020508 = 1.7666716575622559 + 2.0 * 6.200878620147705
Epoch 180, val loss: 1.7591300010681152
Epoch 190, training loss: 14.13220500946045 = 1.7569829225540161 + 2.0 * 6.187611103057861
Epoch 190, val loss: 1.7499969005584717
Epoch 200, training loss: 14.105815887451172 = 1.7458946704864502 + 2.0 * 6.17996072769165
Epoch 200, val loss: 1.7399065494537354
Epoch 210, training loss: 14.068771362304688 = 1.7333030700683594 + 2.0 * 6.167734146118164
Epoch 210, val loss: 1.7286930084228516
Epoch 220, training loss: 14.037192344665527 = 1.7187612056732178 + 2.0 * 6.159215450286865
Epoch 220, val loss: 1.7160618305206299
Epoch 230, training loss: 14.005043983459473 = 1.7017595767974854 + 2.0 * 6.151642322540283
Epoch 230, val loss: 1.70160710811615
Epoch 240, training loss: 13.972175598144531 = 1.6816927194595337 + 2.0 * 6.1452412605285645
Epoch 240, val loss: 1.6848453283309937
Epoch 250, training loss: 13.93726921081543 = 1.658261775970459 + 2.0 * 6.139503479003906
Epoch 250, val loss: 1.6655633449554443
Epoch 260, training loss: 13.899070739746094 = 1.6305584907531738 + 2.0 * 6.134256362915039
Epoch 260, val loss: 1.643025279045105
Epoch 270, training loss: 13.856709480285645 = 1.597625494003296 + 2.0 * 6.129541873931885
Epoch 270, val loss: 1.6163939237594604
Epoch 280, training loss: 13.81335735321045 = 1.5595065355300903 + 2.0 * 6.126925468444824
Epoch 280, val loss: 1.5856754779815674
Epoch 290, training loss: 13.761122703552246 = 1.5161300897598267 + 2.0 * 6.122496128082275
Epoch 290, val loss: 1.550946831703186
Epoch 300, training loss: 13.704324722290039 = 1.4669826030731201 + 2.0 * 6.11867094039917
Epoch 300, val loss: 1.5115629434585571
Epoch 310, training loss: 13.656046867370605 = 1.4135514497756958 + 2.0 * 6.1212477684021
Epoch 310, val loss: 1.4687143564224243
Epoch 320, training loss: 13.597053527832031 = 1.3602688312530518 + 2.0 * 6.118392467498779
Epoch 320, val loss: 1.426843523979187
Epoch 330, training loss: 13.530010223388672 = 1.3083909749984741 + 2.0 * 6.110809803009033
Epoch 330, val loss: 1.386427879333496
Epoch 340, training loss: 13.47258186340332 = 1.2581729888916016 + 2.0 * 6.107204437255859
Epoch 340, val loss: 1.347565770149231
Epoch 350, training loss: 13.417877197265625 = 1.2098780870437622 + 2.0 * 6.103999614715576
Epoch 350, val loss: 1.3107244968414307
Epoch 360, training loss: 13.36535358428955 = 1.1636888980865479 + 2.0 * 6.100832462310791
Epoch 360, val loss: 1.2760173082351685
Epoch 370, training loss: 13.335777282714844 = 1.1194241046905518 + 2.0 * 6.1081767082214355
Epoch 370, val loss: 1.243224859237671
Epoch 380, training loss: 13.280963897705078 = 1.077807903289795 + 2.0 * 6.101578235626221
Epoch 380, val loss: 1.2129255533218384
Epoch 390, training loss: 13.224115371704102 = 1.0378451347351074 + 2.0 * 6.093135356903076
Epoch 390, val loss: 1.1842273473739624
Epoch 400, training loss: 13.180960655212402 = 0.9989156723022461 + 2.0 * 6.091022491455078
Epoch 400, val loss: 1.1563289165496826
Epoch 410, training loss: 13.137030601501465 = 0.9607563018798828 + 2.0 * 6.088137149810791
Epoch 410, val loss: 1.1290479898452759
Epoch 420, training loss: 13.112897872924805 = 0.9233319759368896 + 2.0 * 6.094782829284668
Epoch 420, val loss: 1.1023842096328735
Epoch 430, training loss: 13.061225891113281 = 0.8875036835670471 + 2.0 * 6.0868611335754395
Epoch 430, val loss: 1.0766808986663818
Epoch 440, training loss: 13.017930030822754 = 0.8528468012809753 + 2.0 * 6.082541465759277
Epoch 440, val loss: 1.0519793033599854
Epoch 450, training loss: 12.982294082641602 = 0.8192392587661743 + 2.0 * 6.081527233123779
Epoch 450, val loss: 1.0280534029006958
Epoch 460, training loss: 12.950992584228516 = 0.7868529558181763 + 2.0 * 6.0820698738098145
Epoch 460, val loss: 1.0051853656768799
Epoch 470, training loss: 12.909967422485352 = 0.7559492588043213 + 2.0 * 6.077009201049805
Epoch 470, val loss: 0.9834424257278442
Epoch 480, training loss: 12.875837326049805 = 0.7261267900466919 + 2.0 * 6.074855327606201
Epoch 480, val loss: 0.9627148509025574
Epoch 490, training loss: 12.86595630645752 = 0.6972299218177795 + 2.0 * 6.084362983703613
Epoch 490, val loss: 0.9427803158760071
Epoch 500, training loss: 12.818782806396484 = 0.6697186827659607 + 2.0 * 6.0745320320129395
Epoch 500, val loss: 0.9239196181297302
Epoch 510, training loss: 12.785816192626953 = 0.643194854259491 + 2.0 * 6.071310520172119
Epoch 510, val loss: 0.9060701131820679
Epoch 520, training loss: 12.754645347595215 = 0.617343008518219 + 2.0 * 6.06865119934082
Epoch 520, val loss: 0.8888468742370605
Epoch 530, training loss: 12.727241516113281 = 0.5920252203941345 + 2.0 * 6.06760835647583
Epoch 530, val loss: 0.8721262812614441
Epoch 540, training loss: 12.700307846069336 = 0.5674238801002502 + 2.0 * 6.066442012786865
Epoch 540, val loss: 0.8560458421707153
Epoch 550, training loss: 12.67294979095459 = 0.5437819361686707 + 2.0 * 6.064583778381348
Epoch 550, val loss: 0.8406758308410645
Epoch 560, training loss: 12.655024528503418 = 0.5208280086517334 + 2.0 * 6.067098140716553
Epoch 560, val loss: 0.8259180784225464
Epoch 570, training loss: 12.62621784210205 = 0.4985179305076599 + 2.0 * 6.063849925994873
Epoch 570, val loss: 0.8118680119514465
Epoch 580, training loss: 12.600787162780762 = 0.47700831294059753 + 2.0 * 6.0618896484375
Epoch 580, val loss: 0.7984026074409485
Epoch 590, training loss: 12.575629234313965 = 0.45602133870124817 + 2.0 * 6.0598039627075195
Epoch 590, val loss: 0.7855929732322693
Epoch 600, training loss: 12.551950454711914 = 0.43564170598983765 + 2.0 * 6.058154582977295
Epoch 600, val loss: 0.7733668684959412
Epoch 610, training loss: 12.532913208007812 = 0.4159502387046814 + 2.0 * 6.058481693267822
Epoch 610, val loss: 0.7618195414543152
Epoch 620, training loss: 12.50973129272461 = 0.3970572352409363 + 2.0 * 6.056336879730225
Epoch 620, val loss: 0.7511230111122131
Epoch 630, training loss: 12.488961219787598 = 0.37878507375717163 + 2.0 * 6.055088043212891
Epoch 630, val loss: 0.7411264181137085
Epoch 640, training loss: 12.482311248779297 = 0.3610628843307495 + 2.0 * 6.060624122619629
Epoch 640, val loss: 0.7318547368049622
Epoch 650, training loss: 12.45330810546875 = 0.34406358003616333 + 2.0 * 6.054622173309326
Epoch 650, val loss: 0.72330641746521
Epoch 660, training loss: 12.43398666381836 = 0.32757997512817383 + 2.0 * 6.053203105926514
Epoch 660, val loss: 0.7154927849769592
Epoch 670, training loss: 12.421481132507324 = 0.3116924464702606 + 2.0 * 6.05489444732666
Epoch 670, val loss: 0.7084352970123291
Epoch 680, training loss: 12.40308952331543 = 0.29655101895332336 + 2.0 * 6.053269386291504
Epoch 680, val loss: 0.7019331455230713
Epoch 690, training loss: 12.38072395324707 = 0.28206029534339905 + 2.0 * 6.0493316650390625
Epoch 690, val loss: 0.6961712837219238
Epoch 700, training loss: 12.366866111755371 = 0.26815998554229736 + 2.0 * 6.049353122711182
Epoch 700, val loss: 0.6910837292671204
Epoch 710, training loss: 12.349974632263184 = 0.25491800904273987 + 2.0 * 6.047528266906738
Epoch 710, val loss: 0.6865638494491577
Epoch 720, training loss: 12.333157539367676 = 0.24235528707504272 + 2.0 * 6.045401096343994
Epoch 720, val loss: 0.6826257109642029
Epoch 730, training loss: 12.319849014282227 = 0.2304295152425766 + 2.0 * 6.0447096824646
Epoch 730, val loss: 0.6792958378791809
Epoch 740, training loss: 12.31644058227539 = 0.21916469931602478 + 2.0 * 6.048637866973877
Epoch 740, val loss: 0.6765176653862
Epoch 750, training loss: 12.300093650817871 = 0.20863577723503113 + 2.0 * 6.045729160308838
Epoch 750, val loss: 0.6742728352546692
Epoch 760, training loss: 12.283995628356934 = 0.1987609565258026 + 2.0 * 6.042617321014404
Epoch 760, val loss: 0.6724891662597656
Epoch 770, training loss: 12.276968002319336 = 0.18942031264305115 + 2.0 * 6.043773651123047
Epoch 770, val loss: 0.6712027788162231
Epoch 780, training loss: 12.263298988342285 = 0.18067863583564758 + 2.0 * 6.0413103103637695
Epoch 780, val loss: 0.6703184247016907
Epoch 790, training loss: 12.260326385498047 = 0.1725122034549713 + 2.0 * 6.043907165527344
Epoch 790, val loss: 0.6697903275489807
Epoch 800, training loss: 12.24482250213623 = 0.16484826803207397 + 2.0 * 6.039987087249756
Epoch 800, val loss: 0.669623076915741
Epoch 810, training loss: 12.23470401763916 = 0.15763607621192932 + 2.0 * 6.038534164428711
Epoch 810, val loss: 0.6698216795921326
Epoch 820, training loss: 12.232865333557129 = 0.15082593262195587 + 2.0 * 6.041019916534424
Epoch 820, val loss: 0.6703062057495117
Epoch 830, training loss: 12.22141170501709 = 0.14444759488105774 + 2.0 * 6.038482189178467
Epoch 830, val loss: 0.6711082458496094
Epoch 840, training loss: 12.211960792541504 = 0.13840225338935852 + 2.0 * 6.036779403686523
Epoch 840, val loss: 0.6722394824028015
Epoch 850, training loss: 12.217658996582031 = 0.1326824575662613 + 2.0 * 6.042488098144531
Epoch 850, val loss: 0.6734921932220459
Epoch 860, training loss: 12.20068073272705 = 0.12731873989105225 + 2.0 * 6.036681175231934
Epoch 860, val loss: 0.6751169562339783
Epoch 870, training loss: 12.201067924499512 = 0.12222800403833389 + 2.0 * 6.039420127868652
Epoch 870, val loss: 0.676800549030304
Epoch 880, training loss: 12.186407089233398 = 0.11742264032363892 + 2.0 * 6.034492015838623
Epoch 880, val loss: 0.6787126660346985
Epoch 890, training loss: 12.178831100463867 = 0.11287356168031693 + 2.0 * 6.032978534698486
Epoch 890, val loss: 0.6807701587677002
Epoch 900, training loss: 12.181501388549805 = 0.10852782428264618 + 2.0 * 6.036486625671387
Epoch 900, val loss: 0.6829751133918762
Epoch 910, training loss: 12.170909881591797 = 0.1043916791677475 + 2.0 * 6.03325891494751
Epoch 910, val loss: 0.6852380633354187
Epoch 920, training loss: 12.167341232299805 = 0.10046100616455078 + 2.0 * 6.033440113067627
Epoch 920, val loss: 0.6876580119132996
Epoch 930, training loss: 12.155040740966797 = 0.09671048820018768 + 2.0 * 6.029165267944336
Epoch 930, val loss: 0.6902245283126831
Epoch 940, training loss: 12.150887489318848 = 0.0931195393204689 + 2.0 * 6.028883934020996
Epoch 940, val loss: 0.6928783655166626
Epoch 950, training loss: 12.152077674865723 = 0.08966302126646042 + 2.0 * 6.03120756149292
Epoch 950, val loss: 0.6956154108047485
Epoch 960, training loss: 12.150156021118164 = 0.0863499566912651 + 2.0 * 6.031903266906738
Epoch 960, val loss: 0.6983804702758789
Epoch 970, training loss: 12.141952514648438 = 0.08322560042142868 + 2.0 * 6.029363632202148
Epoch 970, val loss: 0.7012640833854675
Epoch 980, training loss: 12.133720397949219 = 0.0802074521780014 + 2.0 * 6.026756286621094
Epoch 980, val loss: 0.7041553854942322
Epoch 990, training loss: 12.128595352172852 = 0.07731162756681442 + 2.0 * 6.025641918182373
Epoch 990, val loss: 0.7071129679679871
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.6568
Flip ASR: 0.6089/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.672719955444336 = 1.924975037574768 + 2.0 * 8.373872756958008
Epoch 0, val loss: 1.926609754562378
Epoch 10, training loss: 18.662708282470703 = 1.9158470630645752 + 2.0 * 8.373430252075195
Epoch 10, val loss: 1.9175177812576294
Epoch 20, training loss: 18.645978927612305 = 1.9048372507095337 + 2.0 * 8.37057113647461
Epoch 20, val loss: 1.9062316417694092
Epoch 30, training loss: 18.587696075439453 = 1.8904212713241577 + 2.0 * 8.348637580871582
Epoch 30, val loss: 1.8913688659667969
Epoch 40, training loss: 18.21393585205078 = 1.8732633590698242 + 2.0 * 8.170336723327637
Epoch 40, val loss: 1.8741698265075684
Epoch 50, training loss: 17.13864517211914 = 1.8554739952087402 + 2.0 * 7.641585826873779
Epoch 50, val loss: 1.8570793867111206
Epoch 60, training loss: 16.2258243560791 = 1.8449995517730713 + 2.0 * 7.1904120445251465
Epoch 60, val loss: 1.8475422859191895
Epoch 70, training loss: 15.437527656555176 = 1.8385745286941528 + 2.0 * 6.799476623535156
Epoch 70, val loss: 1.8410718441009521
Epoch 80, training loss: 15.12035083770752 = 1.83100163936615 + 2.0 * 6.644674777984619
Epoch 80, val loss: 1.8329908847808838
Epoch 90, training loss: 14.88506031036377 = 1.8225858211517334 + 2.0 * 6.5312371253967285
Epoch 90, val loss: 1.8245996236801147
Epoch 100, training loss: 14.73508358001709 = 1.8155291080474854 + 2.0 * 6.459777355194092
Epoch 100, val loss: 1.8176515102386475
Epoch 110, training loss: 14.626794815063477 = 1.8093630075454712 + 2.0 * 6.408715724945068
Epoch 110, val loss: 1.8113566637039185
Epoch 120, training loss: 14.541980743408203 = 1.8033865690231323 + 2.0 * 6.369297027587891
Epoch 120, val loss: 1.8051356077194214
Epoch 130, training loss: 14.47092056274414 = 1.797471523284912 + 2.0 * 6.336724281311035
Epoch 130, val loss: 1.7990578413009644
Epoch 140, training loss: 14.4094877243042 = 1.791598916053772 + 2.0 * 6.308944225311279
Epoch 140, val loss: 1.7931773662567139
Epoch 150, training loss: 14.353174209594727 = 1.785567283630371 + 2.0 * 6.283803462982178
Epoch 150, val loss: 1.787329912185669
Epoch 160, training loss: 14.302523612976074 = 1.7791147232055664 + 2.0 * 6.261704444885254
Epoch 160, val loss: 1.7812860012054443
Epoch 170, training loss: 14.260577201843262 = 1.7719991207122803 + 2.0 * 6.244288921356201
Epoch 170, val loss: 1.7748757600784302
Epoch 180, training loss: 14.21998405456543 = 1.7640620470046997 + 2.0 * 6.22796106338501
Epoch 180, val loss: 1.7679719924926758
Epoch 190, training loss: 14.183317184448242 = 1.7550745010375977 + 2.0 * 6.214121341705322
Epoch 190, val loss: 1.7604018449783325
Epoch 200, training loss: 14.14908218383789 = 1.7448811531066895 + 2.0 * 6.20210075378418
Epoch 200, val loss: 1.7520207166671753
Epoch 210, training loss: 14.116195678710938 = 1.7332370281219482 + 2.0 * 6.191479206085205
Epoch 210, val loss: 1.7425655126571655
Epoch 220, training loss: 14.08086109161377 = 1.7198678255081177 + 2.0 * 6.180496692657471
Epoch 220, val loss: 1.731900691986084
Epoch 230, training loss: 14.047621726989746 = 1.7044845819473267 + 2.0 * 6.171568393707275
Epoch 230, val loss: 1.7197015285491943
Epoch 240, training loss: 14.014615058898926 = 1.686629295349121 + 2.0 * 6.163992881774902
Epoch 240, val loss: 1.7056427001953125
Epoch 250, training loss: 13.981766700744629 = 1.6659482717514038 + 2.0 * 6.157909393310547
Epoch 250, val loss: 1.6893583536148071
Epoch 260, training loss: 13.94106388092041 = 1.642013430595398 + 2.0 * 6.149525165557861
Epoch 260, val loss: 1.6705098152160645
Epoch 270, training loss: 13.901432991027832 = 1.6143312454223633 + 2.0 * 6.143550872802734
Epoch 270, val loss: 1.6486743688583374
Epoch 280, training loss: 13.867758750915527 = 1.5823919773101807 + 2.0 * 6.142683506011963
Epoch 280, val loss: 1.623430609703064
Epoch 290, training loss: 13.81371784210205 = 1.5463504791259766 + 2.0 * 6.133683681488037
Epoch 290, val loss: 1.5949612855911255
Epoch 300, training loss: 13.764572143554688 = 1.5060561895370483 + 2.0 * 6.129258155822754
Epoch 300, val loss: 1.5631341934204102
Epoch 310, training loss: 13.711892127990723 = 1.46135675907135 + 2.0 * 6.125267505645752
Epoch 310, val loss: 1.527827262878418
Epoch 320, training loss: 13.674566268920898 = 1.4124424457550049 + 2.0 * 6.131062030792236
Epoch 320, val loss: 1.4893295764923096
Epoch 330, training loss: 13.611656188964844 = 1.361736536026001 + 2.0 * 6.124959945678711
Epoch 330, val loss: 1.4494844675064087
Epoch 340, training loss: 13.542722702026367 = 1.3102787733078003 + 2.0 * 6.116221904754639
Epoch 340, val loss: 1.4091897010803223
Epoch 350, training loss: 13.485786437988281 = 1.2584593296051025 + 2.0 * 6.113663673400879
Epoch 350, val loss: 1.368788242340088
Epoch 360, training loss: 13.430685997009277 = 1.207023024559021 + 2.0 * 6.1118316650390625
Epoch 360, val loss: 1.3288992643356323
Epoch 370, training loss: 13.374305725097656 = 1.157090425491333 + 2.0 * 6.108607769012451
Epoch 370, val loss: 1.2903388738632202
Epoch 380, training loss: 13.32427978515625 = 1.1090110540390015 + 2.0 * 6.107634544372559
Epoch 380, val loss: 1.25328528881073
Epoch 390, training loss: 13.26931095123291 = 1.0631663799285889 + 2.0 * 6.103072166442871
Epoch 390, val loss: 1.2182276248931885
Epoch 400, training loss: 13.224024772644043 = 1.0194236040115356 + 2.0 * 6.102300643920898
Epoch 400, val loss: 1.1848002672195435
Epoch 410, training loss: 13.172354698181152 = 0.9778015613555908 + 2.0 * 6.09727668762207
Epoch 410, val loss: 1.1531438827514648
Epoch 420, training loss: 13.12768840789795 = 0.9378023743629456 + 2.0 * 6.094943046569824
Epoch 420, val loss: 1.1228207349777222
Epoch 430, training loss: 13.086702346801758 = 0.8994308710098267 + 2.0 * 6.093635559082031
Epoch 430, val loss: 1.0937129259109497
Epoch 440, training loss: 13.043427467346191 = 0.862821638584137 + 2.0 * 6.09030294418335
Epoch 440, val loss: 1.066232681274414
Epoch 450, training loss: 13.00723648071289 = 0.8274137377738953 + 2.0 * 6.089911460876465
Epoch 450, val loss: 1.0397312641143799
Epoch 460, training loss: 12.970748901367188 = 0.7935010194778442 + 2.0 * 6.088624000549316
Epoch 460, val loss: 1.0144318342208862
Epoch 470, training loss: 12.927536010742188 = 0.7608399391174316 + 2.0 * 6.083347797393799
Epoch 470, val loss: 0.9904646277427673
Epoch 480, training loss: 12.892340660095215 = 0.7293436527252197 + 2.0 * 6.081498622894287
Epoch 480, val loss: 0.9676060676574707
Epoch 490, training loss: 12.877690315246582 = 0.6990628838539124 + 2.0 * 6.089313507080078
Epoch 490, val loss: 0.945973813533783
Epoch 500, training loss: 12.83626651763916 = 0.6702953577041626 + 2.0 * 6.0829854011535645
Epoch 500, val loss: 0.92585289478302
Epoch 510, training loss: 12.797481536865234 = 0.6430280804634094 + 2.0 * 6.077226638793945
Epoch 510, val loss: 0.9075214266777039
Epoch 520, training loss: 12.764622688293457 = 0.61671382188797 + 2.0 * 6.0739545822143555
Epoch 520, val loss: 0.890339732170105
Epoch 530, training loss: 12.737846374511719 = 0.5911543369293213 + 2.0 * 6.073346138000488
Epoch 530, val loss: 0.874301016330719
Epoch 540, training loss: 12.717646598815918 = 0.5665000081062317 + 2.0 * 6.075573444366455
Epoch 540, val loss: 0.8595542907714844
Epoch 550, training loss: 12.68377685546875 = 0.5428152084350586 + 2.0 * 6.070480823516846
Epoch 550, val loss: 0.8461621403694153
Epoch 560, training loss: 12.655023574829102 = 0.5197509527206421 + 2.0 * 6.067636489868164
Epoch 560, val loss: 0.8336592316627502
Epoch 570, training loss: 12.643877983093262 = 0.4972124397754669 + 2.0 * 6.073332786560059
Epoch 570, val loss: 0.8221322894096375
Epoch 580, training loss: 12.60700798034668 = 0.47545287013053894 + 2.0 * 6.065777778625488
Epoch 580, val loss: 0.8115781545639038
Epoch 590, training loss: 12.592906951904297 = 0.45440977811813354 + 2.0 * 6.069248676300049
Epoch 590, val loss: 0.8020127415657043
Epoch 600, training loss: 12.562292098999023 = 0.4341382086277008 + 2.0 * 6.064076900482178
Epoch 600, val loss: 0.7933285236358643
Epoch 610, training loss: 12.537546157836914 = 0.414422869682312 + 2.0 * 6.061561584472656
Epoch 610, val loss: 0.7854182124137878
Epoch 620, training loss: 12.51594352722168 = 0.3952975273132324 + 2.0 * 6.060323238372803
Epoch 620, val loss: 0.7782110571861267
Epoch 630, training loss: 12.504292488098145 = 0.3767702877521515 + 2.0 * 6.063761234283447
Epoch 630, val loss: 0.7716630101203918
Epoch 640, training loss: 12.477899551391602 = 0.359006404876709 + 2.0 * 6.059446334838867
Epoch 640, val loss: 0.7658402323722839
Epoch 650, training loss: 12.45695972442627 = 0.3418739140033722 + 2.0 * 6.05754280090332
Epoch 650, val loss: 0.7605925798416138
Epoch 660, training loss: 12.455220222473145 = 0.3253878355026245 + 2.0 * 6.064916133880615
Epoch 660, val loss: 0.7559330463409424
Epoch 670, training loss: 12.42632007598877 = 0.30959010124206543 + 2.0 * 6.0583648681640625
Epoch 670, val loss: 0.7517421245574951
Epoch 680, training loss: 12.404623031616211 = 0.2945512533187866 + 2.0 * 6.0550360679626465
Epoch 680, val loss: 0.7482571601867676
Epoch 690, training loss: 12.385927200317383 = 0.28017857670783997 + 2.0 * 6.0528740882873535
Epoch 690, val loss: 0.7452754974365234
Epoch 700, training loss: 12.37065601348877 = 0.26642048358917236 + 2.0 * 6.052117824554443
Epoch 700, val loss: 0.742840588092804
Epoch 710, training loss: 12.356797218322754 = 0.2533731460571289 + 2.0 * 6.0517120361328125
Epoch 710, val loss: 0.7407676577568054
Epoch 720, training loss: 12.345175743103027 = 0.24107392132282257 + 2.0 * 6.052051067352295
Epoch 720, val loss: 0.7392669916152954
Epoch 730, training loss: 12.328742980957031 = 0.2294403612613678 + 2.0 * 6.049651145935059
Epoch 730, val loss: 0.7382243871688843
Epoch 740, training loss: 12.316380500793457 = 0.21843068301677704 + 2.0 * 6.048974990844727
Epoch 740, val loss: 0.737586498260498
Epoch 750, training loss: 12.3037691116333 = 0.20807920396327972 + 2.0 * 6.047844886779785
Epoch 750, val loss: 0.7373775839805603
Epoch 760, training loss: 12.291358947753906 = 0.19834396243095398 + 2.0 * 6.046507358551025
Epoch 760, val loss: 0.7375960350036621
Epoch 770, training loss: 12.285223007202148 = 0.18919439613819122 + 2.0 * 6.048014163970947
Epoch 770, val loss: 0.7381432056427002
Epoch 780, training loss: 12.267621040344238 = 0.18062403798103333 + 2.0 * 6.043498516082764
Epoch 780, val loss: 0.7391024827957153
Epoch 790, training loss: 12.258279800415039 = 0.17254185676574707 + 2.0 * 6.0428690910339355
Epoch 790, val loss: 0.7404254078865051
Epoch 800, training loss: 12.26833724975586 = 0.16493435204029083 + 2.0 * 6.051701545715332
Epoch 800, val loss: 0.7420013546943665
Epoch 810, training loss: 12.24158763885498 = 0.1578308790922165 + 2.0 * 6.0418782234191895
Epoch 810, val loss: 0.7439290285110474
Epoch 820, training loss: 12.23260498046875 = 0.15112008154392242 + 2.0 * 6.04074239730835
Epoch 820, val loss: 0.7461191415786743
Epoch 830, training loss: 12.22455883026123 = 0.1447739154100418 + 2.0 * 6.039892673492432
Epoch 830, val loss: 0.7485755681991577
Epoch 840, training loss: 12.226251602172852 = 0.13878028094768524 + 2.0 * 6.043735504150391
Epoch 840, val loss: 0.7512269020080566
Epoch 850, training loss: 12.211445808410645 = 0.13310451805591583 + 2.0 * 6.039170742034912
Epoch 850, val loss: 0.7540150880813599
Epoch 860, training loss: 12.202807426452637 = 0.12773823738098145 + 2.0 * 6.037534713745117
Epoch 860, val loss: 0.7569816708564758
Epoch 870, training loss: 12.2035493850708 = 0.12266441434621811 + 2.0 * 6.04044246673584
Epoch 870, val loss: 0.7600994110107422
Epoch 880, training loss: 12.19044303894043 = 0.11787575483322144 + 2.0 * 6.036283493041992
Epoch 880, val loss: 0.7634003162384033
Epoch 890, training loss: 12.196293830871582 = 0.11333178728818893 + 2.0 * 6.041481018066406
Epoch 890, val loss: 0.7667964100837708
Epoch 900, training loss: 12.182016372680664 = 0.10900765657424927 + 2.0 * 6.03650426864624
Epoch 900, val loss: 0.7701277136802673
Epoch 910, training loss: 12.173484802246094 = 0.10492485761642456 + 2.0 * 6.034279823303223
Epoch 910, val loss: 0.7737182378768921
Epoch 920, training loss: 12.168153762817383 = 0.10101403295993805 + 2.0 * 6.033569812774658
Epoch 920, val loss: 0.7773078680038452
Epoch 930, training loss: 12.16797161102295 = 0.09729979187250137 + 2.0 * 6.035336017608643
Epoch 930, val loss: 0.7809128761291504
Epoch 940, training loss: 12.158526420593262 = 0.09377864003181458 + 2.0 * 6.032373905181885
Epoch 940, val loss: 0.7845976948738098
Epoch 950, training loss: 12.153055191040039 = 0.09041664749383926 + 2.0 * 6.0313191413879395
Epoch 950, val loss: 0.7883914113044739
Epoch 960, training loss: 12.164555549621582 = 0.08720913529396057 + 2.0 * 6.038673400878906
Epoch 960, val loss: 0.7921296954154968
Epoch 970, training loss: 12.147053718566895 = 0.08415773510932922 + 2.0 * 6.031447887420654
Epoch 970, val loss: 0.7959297895431519
Epoch 980, training loss: 12.143299102783203 = 0.08124634623527527 + 2.0 * 6.031026363372803
Epoch 980, val loss: 0.7997677326202393
Epoch 990, training loss: 12.139830589294434 = 0.07846977561712265 + 2.0 * 6.030680179595947
Epoch 990, val loss: 0.8035328388214111
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
The final ASR:0.79213, 0.13583, Accuracy:0.82346, 0.00175
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11638])
remove edge: torch.Size([2, 9570])
updated graph: torch.Size([2, 10652])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
The final ASR:0.97786, 0.00000, Accuracy:0.83210, 0.00761
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.5 der2:0.3 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 18.689292907714844 = 1.9415674209594727 + 2.0 * 8.373863220214844
Epoch 0, val loss: 1.9435725212097168
Epoch 10, training loss: 18.678964614868164 = 1.9322057962417603 + 2.0 * 8.373379707336426
Epoch 10, val loss: 1.9341349601745605
Epoch 20, training loss: 18.661331176757812 = 1.9208003282546997 + 2.0 * 8.370265007019043
Epoch 20, val loss: 1.9224119186401367
Epoch 30, training loss: 18.606281280517578 = 1.9054107666015625 + 2.0 * 8.350435256958008
Epoch 30, val loss: 1.9068570137023926
Epoch 40, training loss: 18.333492279052734 = 1.8871710300445557 + 2.0 * 8.223160743713379
Epoch 40, val loss: 1.8893486261367798
Epoch 50, training loss: 17.0729923248291 = 1.8677486181259155 + 2.0 * 7.602622032165527
Epoch 50, val loss: 1.8705196380615234
Epoch 60, training loss: 16.211181640625 = 1.8509454727172852 + 2.0 * 7.180117607116699
Epoch 60, val loss: 1.8553739786148071
Epoch 70, training loss: 15.69083309173584 = 1.836660385131836 + 2.0 * 6.927086353302002
Epoch 70, val loss: 1.842166543006897
Epoch 80, training loss: 15.376664161682129 = 1.821824312210083 + 2.0 * 6.7774200439453125
Epoch 80, val loss: 1.8284038305282593
Epoch 90, training loss: 15.132051467895508 = 1.8068773746490479 + 2.0 * 6.6625871658325195
Epoch 90, val loss: 1.8144925832748413
Epoch 100, training loss: 14.958505630493164 = 1.791969656944275 + 2.0 * 6.583268165588379
Epoch 100, val loss: 1.8006458282470703
Epoch 110, training loss: 14.811005592346191 = 1.7770808935165405 + 2.0 * 6.51696252822876
Epoch 110, val loss: 1.7871606349945068
Epoch 120, training loss: 14.67933464050293 = 1.7620015144348145 + 2.0 * 6.458666801452637
Epoch 120, val loss: 1.773396372795105
Epoch 130, training loss: 14.575980186462402 = 1.7455825805664062 + 2.0 * 6.415198802947998
Epoch 130, val loss: 1.7586684226989746
Epoch 140, training loss: 14.493646621704102 = 1.7273184061050415 + 2.0 * 6.383163928985596
Epoch 140, val loss: 1.7423291206359863
Epoch 150, training loss: 14.42041301727295 = 1.7068475484848022 + 2.0 * 6.356782913208008
Epoch 150, val loss: 1.7242350578308105
Epoch 160, training loss: 14.348203659057617 = 1.683887243270874 + 2.0 * 6.332158088684082
Epoch 160, val loss: 1.7044246196746826
Epoch 170, training loss: 14.277713775634766 = 1.6579957008361816 + 2.0 * 6.309858798980713
Epoch 170, val loss: 1.6822644472122192
Epoch 180, training loss: 14.212747573852539 = 1.628523826599121 + 2.0 * 6.292111873626709
Epoch 180, val loss: 1.6572107076644897
Epoch 190, training loss: 14.143770217895508 = 1.5951143503189087 + 2.0 * 6.274327754974365
Epoch 190, val loss: 1.6289253234863281
Epoch 200, training loss: 14.076096534729004 = 1.55742609500885 + 2.0 * 6.259335041046143
Epoch 200, val loss: 1.59696626663208
Epoch 210, training loss: 14.007076263427734 = 1.5152857303619385 + 2.0 * 6.2458953857421875
Epoch 210, val loss: 1.5614264011383057
Epoch 220, training loss: 13.937323570251465 = 1.4691578149795532 + 2.0 * 6.2340826988220215
Epoch 220, val loss: 1.5225439071655273
Epoch 230, training loss: 13.865453720092773 = 1.4190974235534668 + 2.0 * 6.223177909851074
Epoch 230, val loss: 1.4805792570114136
Epoch 240, training loss: 13.79544448852539 = 1.3655924797058105 + 2.0 * 6.214925765991211
Epoch 240, val loss: 1.4360759258270264
Epoch 250, training loss: 13.722856521606445 = 1.3100636005401611 + 2.0 * 6.206396579742432
Epoch 250, val loss: 1.3902456760406494
Epoch 260, training loss: 13.646947860717773 = 1.2534981966018677 + 2.0 * 6.196724891662598
Epoch 260, val loss: 1.3442093133926392
Epoch 270, training loss: 13.575398445129395 = 1.1967360973358154 + 2.0 * 6.1893310546875
Epoch 270, val loss: 1.2981946468353271
Epoch 280, training loss: 13.506940841674805 = 1.140573501586914 + 2.0 * 6.183183670043945
Epoch 280, val loss: 1.2531640529632568
Epoch 290, training loss: 13.438577651977539 = 1.0857175588607788 + 2.0 * 6.1764302253723145
Epoch 290, val loss: 1.209570288658142
Epoch 300, training loss: 13.373373985290527 = 1.032268762588501 + 2.0 * 6.170552730560303
Epoch 300, val loss: 1.167543649673462
Epoch 310, training loss: 13.313301086425781 = 0.9804918766021729 + 2.0 * 6.166404724121094
Epoch 310, val loss: 1.1273481845855713
Epoch 320, training loss: 13.251928329467773 = 0.9313594102859497 + 2.0 * 6.160284519195557
Epoch 320, val loss: 1.0892664194107056
Epoch 330, training loss: 13.198652267456055 = 0.8844895362854004 + 2.0 * 6.157081604003906
Epoch 330, val loss: 1.05353581905365
Epoch 340, training loss: 13.141855239868164 = 0.8404276371002197 + 2.0 * 6.150713920593262
Epoch 340, val loss: 1.0201505422592163
Epoch 350, training loss: 13.092057228088379 = 0.7987274527549744 + 2.0 * 6.146665096282959
Epoch 350, val loss: 0.9891760945320129
Epoch 360, training loss: 13.044882774353027 = 0.7599342465400696 + 2.0 * 6.142474174499512
Epoch 360, val loss: 0.9605540037155151
Epoch 370, training loss: 13.000164985656738 = 0.7236711382865906 + 2.0 * 6.138247013092041
Epoch 370, val loss: 0.9345343708992004
Epoch 380, training loss: 12.95949649810791 = 0.6898004412651062 + 2.0 * 6.134848117828369
Epoch 380, val loss: 0.9107180833816528
Epoch 390, training loss: 12.922111511230469 = 0.6579719185829163 + 2.0 * 6.1320695877075195
Epoch 390, val loss: 0.8888761401176453
Epoch 400, training loss: 12.885480880737305 = 0.6278321743011475 + 2.0 * 6.128824234008789
Epoch 400, val loss: 0.8687644004821777
Epoch 410, training loss: 12.852638244628906 = 0.5991834998130798 + 2.0 * 6.12672758102417
Epoch 410, val loss: 0.8503128290176392
Epoch 420, training loss: 12.818731307983398 = 0.5720633268356323 + 2.0 * 6.123333930969238
Epoch 420, val loss: 0.8331747055053711
Epoch 430, training loss: 12.78394889831543 = 0.5459578633308411 + 2.0 * 6.118995666503906
Epoch 430, val loss: 0.8175584077835083
Epoch 440, training loss: 12.753028869628906 = 0.5207858681678772 + 2.0 * 6.116121292114258
Epoch 440, val loss: 0.8030648231506348
Epoch 450, training loss: 12.725763320922852 = 0.4964399039745331 + 2.0 * 6.114661693572998
Epoch 450, val loss: 0.7897213697433472
Epoch 460, training loss: 12.698962211608887 = 0.4730561077594757 + 2.0 * 6.112953186035156
Epoch 460, val loss: 0.7773603796958923
Epoch 470, training loss: 12.672652244567871 = 0.45054855942726135 + 2.0 * 6.1110520362854
Epoch 470, val loss: 0.766217052936554
Epoch 480, training loss: 12.643478393554688 = 0.4290635585784912 + 2.0 * 6.107207298278809
Epoch 480, val loss: 0.756149411201477
Epoch 490, training loss: 12.624258041381836 = 0.40835636854171753 + 2.0 * 6.107950687408447
Epoch 490, val loss: 0.7471410632133484
Epoch 500, training loss: 12.595348358154297 = 0.38851258158683777 + 2.0 * 6.103417873382568
Epoch 500, val loss: 0.7390536665916443
Epoch 510, training loss: 12.570489883422852 = 0.36946508288383484 + 2.0 * 6.100512504577637
Epoch 510, val loss: 0.731911301612854
Epoch 520, training loss: 12.552083969116211 = 0.3512665033340454 + 2.0 * 6.100408554077148
Epoch 520, val loss: 0.7256119847297668
Epoch 530, training loss: 12.527562141418457 = 0.3338435888290405 + 2.0 * 6.096859455108643
Epoch 530, val loss: 0.7202014327049255
Epoch 540, training loss: 12.50732707977295 = 0.3171563744544983 + 2.0 * 6.095085144042969
Epoch 540, val loss: 0.7155840396881104
Epoch 550, training loss: 12.487774848937988 = 0.3012268543243408 + 2.0 * 6.093274116516113
Epoch 550, val loss: 0.7116389870643616
Epoch 560, training loss: 12.470712661743164 = 0.286017507314682 + 2.0 * 6.092347621917725
Epoch 560, val loss: 0.7083250880241394
Epoch 570, training loss: 12.450244903564453 = 0.271427720785141 + 2.0 * 6.0894083976745605
Epoch 570, val loss: 0.7056569457054138
Epoch 580, training loss: 12.442731857299805 = 0.25747305154800415 + 2.0 * 6.092629432678223
Epoch 580, val loss: 0.7035609483718872
Epoch 590, training loss: 12.419076919555664 = 0.24420437216758728 + 2.0 * 6.087436199188232
Epoch 590, val loss: 0.7018921375274658
Epoch 600, training loss: 12.420978546142578 = 0.2316206693649292 + 2.0 * 6.09467887878418
Epoch 600, val loss: 0.7007507681846619
Epoch 610, training loss: 12.385963439941406 = 0.21961258351802826 + 2.0 * 6.0831756591796875
Epoch 610, val loss: 0.7001065611839294
Epoch 620, training loss: 12.371071815490723 = 0.20823615789413452 + 2.0 * 6.081418037414551
Epoch 620, val loss: 0.6999132633209229
Epoch 630, training loss: 12.356623649597168 = 0.19744862616062164 + 2.0 * 6.079587459564209
Epoch 630, val loss: 0.7001608610153198
Epoch 640, training loss: 12.347206115722656 = 0.1871688812971115 + 2.0 * 6.080018520355225
Epoch 640, val loss: 0.700822651386261
Epoch 650, training loss: 12.341883659362793 = 0.1774519383907318 + 2.0 * 6.082215785980225
Epoch 650, val loss: 0.7018463015556335
Epoch 660, training loss: 12.32181453704834 = 0.16834568977355957 + 2.0 * 6.07673454284668
Epoch 660, val loss: 0.7032077312469482
Epoch 670, training loss: 12.31633186340332 = 0.15976299345493317 + 2.0 * 6.07828426361084
Epoch 670, val loss: 0.7049379944801331
Epoch 680, training loss: 12.301676750183105 = 0.1516784280538559 + 2.0 * 6.0749993324279785
Epoch 680, val loss: 0.7070531845092773
Epoch 690, training loss: 12.29002571105957 = 0.14409060776233673 + 2.0 * 6.072967529296875
Epoch 690, val loss: 0.7094480991363525
Epoch 700, training loss: 12.287325859069824 = 0.13693256676197052 + 2.0 * 6.075196743011475
Epoch 700, val loss: 0.7121724486351013
Epoch 710, training loss: 12.27031421661377 = 0.1302388608455658 + 2.0 * 6.070037841796875
Epoch 710, val loss: 0.7150616645812988
Epoch 720, training loss: 12.261119842529297 = 0.12393423169851303 + 2.0 * 6.0685930252075195
Epoch 720, val loss: 0.7181751728057861
Epoch 730, training loss: 12.260003089904785 = 0.11803191900253296 + 2.0 * 6.070985794067383
Epoch 730, val loss: 0.7215398550033569
Epoch 740, training loss: 12.24402904510498 = 0.11244452744722366 + 2.0 * 6.065792083740234
Epoch 740, val loss: 0.725047767162323
Epoch 750, training loss: 12.236598014831543 = 0.10722518712282181 + 2.0 * 6.064686298370361
Epoch 750, val loss: 0.7287592887878418
Epoch 760, training loss: 12.237778663635254 = 0.10232309252023697 + 2.0 * 6.067727565765381
Epoch 760, val loss: 0.7326775193214417
Epoch 770, training loss: 12.223063468933105 = 0.09768009930849075 + 2.0 * 6.062691688537598
Epoch 770, val loss: 0.7366838455200195
Epoch 780, training loss: 12.2243013381958 = 0.09333264827728271 + 2.0 * 6.065484523773193
Epoch 780, val loss: 0.7408149838447571
Epoch 790, training loss: 12.21042251586914 = 0.08925612270832062 + 2.0 * 6.060583114624023
Epoch 790, val loss: 0.745003342628479
Epoch 800, training loss: 12.20926570892334 = 0.08539464324712753 + 2.0 * 6.0619354248046875
Epoch 800, val loss: 0.7493361234664917
Epoch 810, training loss: 12.198481559753418 = 0.0817682221531868 + 2.0 * 6.058356761932373
Epoch 810, val loss: 0.7537415623664856
Epoch 820, training loss: 12.192079544067383 = 0.0783410593867302 + 2.0 * 6.056869029998779
Epoch 820, val loss: 0.7582091093063354
Epoch 830, training loss: 12.187495231628418 = 0.07509825378656387 + 2.0 * 6.056198596954346
Epoch 830, val loss: 0.7627694606781006
Epoch 840, training loss: 12.193768501281738 = 0.07203345000743866 + 2.0 * 6.0608673095703125
Epoch 840, val loss: 0.7673847675323486
Epoch 850, training loss: 12.183160781860352 = 0.06916937977075577 + 2.0 * 6.056995868682861
Epoch 850, val loss: 0.7719208598136902
Epoch 860, training loss: 12.172505378723145 = 0.06645947694778442 + 2.0 * 6.053022861480713
Epoch 860, val loss: 0.7765762209892273
Epoch 870, training loss: 12.168950080871582 = 0.06389115005731583 + 2.0 * 6.052529335021973
Epoch 870, val loss: 0.7812754511833191
Epoch 880, training loss: 12.177873611450195 = 0.061450663954019547 + 2.0 * 6.058211326599121
Epoch 880, val loss: 0.78596031665802
Epoch 890, training loss: 12.164772033691406 = 0.059162698686122894 + 2.0 * 6.052804470062256
Epoch 890, val loss: 0.7907192707061768
Epoch 900, training loss: 12.156732559204102 = 0.05696922913193703 + 2.0 * 6.049881458282471
Epoch 900, val loss: 0.7953903079032898
Epoch 910, training loss: 12.160123825073242 = 0.0549003966152668 + 2.0 * 6.052611827850342
Epoch 910, val loss: 0.8001514673233032
Epoch 920, training loss: 12.150479316711426 = 0.05294017121195793 + 2.0 * 6.048769474029541
Epoch 920, val loss: 0.804890513420105
Epoch 930, training loss: 12.148674011230469 = 0.05106889083981514 + 2.0 * 6.048802375793457
Epoch 930, val loss: 0.8095381855964661
Epoch 940, training loss: 12.150321006774902 = 0.0493033267557621 + 2.0 * 6.050508975982666
Epoch 940, val loss: 0.8143221139907837
Epoch 950, training loss: 12.142032623291016 = 0.04762635380029678 + 2.0 * 6.047203063964844
Epoch 950, val loss: 0.818924069404602
Epoch 960, training loss: 12.135401725769043 = 0.04602541774511337 + 2.0 * 6.0446882247924805
Epoch 960, val loss: 0.823609471321106
Epoch 970, training loss: 12.132793426513672 = 0.044500116258859634 + 2.0 * 6.044146537780762
Epoch 970, val loss: 0.8283252120018005
Epoch 980, training loss: 12.1372652053833 = 0.04304390400648117 + 2.0 * 6.047110557556152
Epoch 980, val loss: 0.8330353498458862
Epoch 990, training loss: 12.127894401550293 = 0.04166163131594658 + 2.0 * 6.043116569519043
Epoch 990, val loss: 0.8375077843666077
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7741
Overall ASR: 0.7454
Flip ASR: 0.6933/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.6993465423584 = 1.9516701698303223 + 2.0 * 8.373838424682617
Epoch 0, val loss: 1.948612928390503
Epoch 10, training loss: 18.687593460083008 = 1.9410032033920288 + 2.0 * 8.373294830322266
Epoch 10, val loss: 1.9382233619689941
Epoch 20, training loss: 18.667591094970703 = 1.9279221296310425 + 2.0 * 8.369834899902344
Epoch 20, val loss: 1.9249660968780518
Epoch 30, training loss: 18.605310440063477 = 1.9100905656814575 + 2.0 * 8.347609519958496
Epoch 30, val loss: 1.9067862033843994
Epoch 40, training loss: 18.303144454956055 = 1.8883763551712036 + 2.0 * 8.20738410949707
Epoch 40, val loss: 1.8858048915863037
Epoch 50, training loss: 17.332353591918945 = 1.8637570142745972 + 2.0 * 7.734298229217529
Epoch 50, val loss: 1.862561583518982
Epoch 60, training loss: 16.64871597290039 = 1.8403534889221191 + 2.0 * 7.404181003570557
Epoch 60, val loss: 1.8412084579467773
Epoch 70, training loss: 15.954557418823242 = 1.8248711824417114 + 2.0 * 7.06484317779541
Epoch 70, val loss: 1.826422095298767
Epoch 80, training loss: 15.475311279296875 = 1.8120464086532593 + 2.0 * 6.831632614135742
Epoch 80, val loss: 1.8136240243911743
Epoch 90, training loss: 15.164793968200684 = 1.7993687391281128 + 2.0 * 6.682712554931641
Epoch 90, val loss: 1.801002860069275
Epoch 100, training loss: 14.94629192352295 = 1.7856885194778442 + 2.0 * 6.580301761627197
Epoch 100, val loss: 1.787919044494629
Epoch 110, training loss: 14.780998229980469 = 1.7722744941711426 + 2.0 * 6.504362106323242
Epoch 110, val loss: 1.7750872373580933
Epoch 120, training loss: 14.664299964904785 = 1.7584915161132812 + 2.0 * 6.452904224395752
Epoch 120, val loss: 1.7617937326431274
Epoch 130, training loss: 14.568659782409668 = 1.7438421249389648 + 2.0 * 6.412408828735352
Epoch 130, val loss: 1.7481809854507446
Epoch 140, training loss: 14.485419273376465 = 1.7280784845352173 + 2.0 * 6.3786702156066895
Epoch 140, val loss: 1.7338801622390747
Epoch 150, training loss: 14.412528038024902 = 1.7105828523635864 + 2.0 * 6.350972652435303
Epoch 150, val loss: 1.7185412645339966
Epoch 160, training loss: 14.347345352172852 = 1.6909172534942627 + 2.0 * 6.328214168548584
Epoch 160, val loss: 1.7017149925231934
Epoch 170, training loss: 14.282102584838867 = 1.6688958406448364 + 2.0 * 6.30660343170166
Epoch 170, val loss: 1.6832094192504883
Epoch 180, training loss: 14.22323989868164 = 1.6440129280090332 + 2.0 * 6.289613246917725
Epoch 180, val loss: 1.662719488143921
Epoch 190, training loss: 14.15899658203125 = 1.6159777641296387 + 2.0 * 6.271509647369385
Epoch 190, val loss: 1.640188217163086
Epoch 200, training loss: 14.096734046936035 = 1.5846737623214722 + 2.0 * 6.256030082702637
Epoch 200, val loss: 1.6151959896087646
Epoch 210, training loss: 14.038305282592773 = 1.549555778503418 + 2.0 * 6.244374752044678
Epoch 210, val loss: 1.5874519348144531
Epoch 220, training loss: 13.972043991088867 = 1.5111615657806396 + 2.0 * 6.230441093444824
Epoch 220, val loss: 1.557337760925293
Epoch 230, training loss: 13.909189224243164 = 1.4694633483886719 + 2.0 * 6.219862937927246
Epoch 230, val loss: 1.5249217748641968
Epoch 240, training loss: 13.850255012512207 = 1.425289511680603 + 2.0 * 6.212482929229736
Epoch 240, val loss: 1.4910697937011719
Epoch 250, training loss: 13.783273696899414 = 1.3798388242721558 + 2.0 * 6.201717376708984
Epoch 250, val loss: 1.4569240808486938
Epoch 260, training loss: 13.7220458984375 = 1.3336973190307617 + 2.0 * 6.194174289703369
Epoch 260, val loss: 1.4228851795196533
Epoch 270, training loss: 13.664115905761719 = 1.287462592124939 + 2.0 * 6.188326835632324
Epoch 270, val loss: 1.3896669149398804
Epoch 280, training loss: 13.605384826660156 = 1.2423605918884277 + 2.0 * 6.181512355804443
Epoch 280, val loss: 1.3578789234161377
Epoch 290, training loss: 13.548667907714844 = 1.198357105255127 + 2.0 * 6.1751556396484375
Epoch 290, val loss: 1.327798843383789
Epoch 300, training loss: 13.502776145935059 = 1.15557861328125 + 2.0 * 6.173598766326904
Epoch 300, val loss: 1.2991465330123901
Epoch 310, training loss: 13.447093963623047 = 1.1144393682479858 + 2.0 * 6.166327476501465
Epoch 310, val loss: 1.2722485065460205
Epoch 320, training loss: 13.395845413208008 = 1.0749964714050293 + 2.0 * 6.160424709320068
Epoch 320, val loss: 1.246601939201355
Epoch 330, training loss: 13.347773551940918 = 1.0367189645767212 + 2.0 * 6.155527114868164
Epoch 330, val loss: 1.22205650806427
Epoch 340, training loss: 13.314008712768555 = 0.9996387958526611 + 2.0 * 6.157185077667236
Epoch 340, val loss: 1.1983129978179932
Epoch 350, training loss: 13.262324333190918 = 0.9640122652053833 + 2.0 * 6.149156093597412
Epoch 350, val loss: 1.1756449937820435
Epoch 360, training loss: 13.217247009277344 = 0.929677426815033 + 2.0 * 6.143784999847412
Epoch 360, val loss: 1.1537772417068481
Epoch 370, training loss: 13.177061080932617 = 0.8963003754615784 + 2.0 * 6.140380382537842
Epoch 370, val loss: 1.132529616355896
Epoch 380, training loss: 13.136948585510254 = 0.8637664914131165 + 2.0 * 6.136590957641602
Epoch 380, val loss: 1.1119714975357056
Epoch 390, training loss: 13.106130599975586 = 0.8320217132568359 + 2.0 * 6.137054443359375
Epoch 390, val loss: 1.0920186042785645
Epoch 400, training loss: 13.060786247253418 = 0.8011708855628967 + 2.0 * 6.129807472229004
Epoch 400, val loss: 1.0728040933609009
Epoch 410, training loss: 13.024958610534668 = 0.7710755467414856 + 2.0 * 6.126941680908203
Epoch 410, val loss: 1.0542486906051636
Epoch 420, training loss: 12.989326477050781 = 0.7415029406547546 + 2.0 * 6.1239118576049805
Epoch 420, val loss: 1.036285400390625
Epoch 430, training loss: 12.963677406311035 = 0.7123953104019165 + 2.0 * 6.125640869140625
Epoch 430, val loss: 1.0189075469970703
Epoch 440, training loss: 12.925284385681152 = 0.684194028377533 + 2.0 * 6.120545387268066
Epoch 440, val loss: 1.002206802368164
Epoch 450, training loss: 12.890722274780273 = 0.6569043397903442 + 2.0 * 6.116909027099609
Epoch 450, val loss: 0.9866446256637573
Epoch 460, training loss: 12.85787296295166 = 0.6303437352180481 + 2.0 * 6.113764762878418
Epoch 460, val loss: 0.9718979597091675
Epoch 470, training loss: 12.841822624206543 = 0.604343056678772 + 2.0 * 6.118739604949951
Epoch 470, val loss: 0.9579449892044067
Epoch 480, training loss: 12.796947479248047 = 0.5792112946510315 + 2.0 * 6.10886812210083
Epoch 480, val loss: 0.9447211027145386
Epoch 490, training loss: 12.766860961914062 = 0.5547080636024475 + 2.0 * 6.106076240539551
Epoch 490, val loss: 0.9323736429214478
Epoch 500, training loss: 12.739819526672363 = 0.530771791934967 + 2.0 * 6.104523658752441
Epoch 500, val loss: 0.9207955002784729
Epoch 510, training loss: 12.715259552001953 = 0.5075292587280273 + 2.0 * 6.103865146636963
Epoch 510, val loss: 0.9099830985069275
Epoch 520, training loss: 12.688945770263672 = 0.4851018786430359 + 2.0 * 6.101922035217285
Epoch 520, val loss: 0.9001334309577942
Epoch 530, training loss: 12.660470008850098 = 0.46334657073020935 + 2.0 * 6.098561763763428
Epoch 530, val loss: 0.8911389112472534
Epoch 540, training loss: 12.642757415771484 = 0.44224509596824646 + 2.0 * 6.100255966186523
Epoch 540, val loss: 0.8830323219299316
Epoch 550, training loss: 12.613039016723633 = 0.4219883382320404 + 2.0 * 6.09552526473999
Epoch 550, val loss: 0.8758673667907715
Epoch 560, training loss: 12.593092918395996 = 0.40241968631744385 + 2.0 * 6.095336437225342
Epoch 560, val loss: 0.8697267770767212
Epoch 570, training loss: 12.569522857666016 = 0.38361433148384094 + 2.0 * 6.092954158782959
Epoch 570, val loss: 0.8647223711013794
Epoch 580, training loss: 12.544168472290039 = 0.3656548857688904 + 2.0 * 6.089256763458252
Epoch 580, val loss: 0.8607398271560669
Epoch 590, training loss: 12.537010192871094 = 0.34847015142440796 + 2.0 * 6.0942702293396
Epoch 590, val loss: 0.8577345609664917
Epoch 600, training loss: 12.50537109375 = 0.3320552408695221 + 2.0 * 6.086658000946045
Epoch 600, val loss: 0.8556942343711853
Epoch 610, training loss: 12.495594024658203 = 0.31641340255737305 + 2.0 * 6.089590072631836
Epoch 610, val loss: 0.8546196222305298
Epoch 620, training loss: 12.468442916870117 = 0.3015780448913574 + 2.0 * 6.083432197570801
Epoch 620, val loss: 0.8543364405632019
Epoch 630, training loss: 12.44915771484375 = 0.2873389422893524 + 2.0 * 6.080909252166748
Epoch 630, val loss: 0.8548079133033752
Epoch 640, training loss: 12.432682037353516 = 0.2737029790878296 + 2.0 * 6.079489707946777
Epoch 640, val loss: 0.8559098243713379
Epoch 650, training loss: 12.429039001464844 = 0.2606689929962158 + 2.0 * 6.0841851234436035
Epoch 650, val loss: 0.8576035499572754
Epoch 660, training loss: 12.402700424194336 = 0.2482614815235138 + 2.0 * 6.077219486236572
Epoch 660, val loss: 0.8599764108657837
Epoch 670, training loss: 12.386710166931152 = 0.2364538609981537 + 2.0 * 6.075128078460693
Epoch 670, val loss: 0.8630305528640747
Epoch 680, training loss: 12.37838077545166 = 0.2251732051372528 + 2.0 * 6.076603889465332
Epoch 680, val loss: 0.8665710687637329
Epoch 690, training loss: 12.373491287231445 = 0.21445496380329132 + 2.0 * 6.0795183181762695
Epoch 690, val loss: 0.8703070878982544
Epoch 700, training loss: 12.347747802734375 = 0.2043347805738449 + 2.0 * 6.071706295013428
Epoch 700, val loss: 0.8748535513877869
Epoch 710, training loss: 12.333464622497559 = 0.19470013678073883 + 2.0 * 6.069382190704346
Epoch 710, val loss: 0.879784345626831
Epoch 720, training loss: 12.32156753540039 = 0.18551954627037048 + 2.0 * 6.068024158477783
Epoch 720, val loss: 0.8851454854011536
Epoch 730, training loss: 12.333104133605957 = 0.17677949368953705 + 2.0 * 6.07816219329834
Epoch 730, val loss: 0.8908928036689758
Epoch 740, training loss: 12.30258846282959 = 0.16855010390281677 + 2.0 * 6.067018985748291
Epoch 740, val loss: 0.8970004320144653
Epoch 750, training loss: 12.290899276733398 = 0.1607605516910553 + 2.0 * 6.065069198608398
Epoch 750, val loss: 0.9034936428070068
Epoch 760, training loss: 12.282315254211426 = 0.15336956083774567 + 2.0 * 6.064472675323486
Epoch 760, val loss: 0.9102492928504944
Epoch 770, training loss: 12.276134490966797 = 0.14635483920574188 + 2.0 * 6.064889907836914
Epoch 770, val loss: 0.9172025322914124
Epoch 780, training loss: 12.261798858642578 = 0.13970531523227692 + 2.0 * 6.061046600341797
Epoch 780, val loss: 0.9245370030403137
Epoch 790, training loss: 12.252981185913086 = 0.13341060280799866 + 2.0 * 6.05978536605835
Epoch 790, val loss: 0.9320387840270996
Epoch 800, training loss: 12.24573040008545 = 0.12743066251277924 + 2.0 * 6.059149742126465
Epoch 800, val loss: 0.9397100806236267
Epoch 810, training loss: 12.263891220092773 = 0.12173189222812653 + 2.0 * 6.071079730987549
Epoch 810, val loss: 0.9474712610244751
Epoch 820, training loss: 12.235225677490234 = 0.11641444265842438 + 2.0 * 6.05940580368042
Epoch 820, val loss: 0.9552367925643921
Epoch 830, training loss: 12.227188110351562 = 0.1113695427775383 + 2.0 * 6.0579094886779785
Epoch 830, val loss: 0.9632758498191833
Epoch 840, training loss: 12.217230796813965 = 0.10657785832881927 + 2.0 * 6.055326461791992
Epoch 840, val loss: 0.9712861180305481
Epoch 850, training loss: 12.209501266479492 = 0.10202278941869736 + 2.0 * 6.053739070892334
Epoch 850, val loss: 0.9794191718101501
Epoch 860, training loss: 12.221068382263184 = 0.09768738597631454 + 2.0 * 6.061690330505371
Epoch 860, val loss: 0.9875809550285339
Epoch 870, training loss: 12.199308395385742 = 0.09359552711248398 + 2.0 * 6.0528564453125
Epoch 870, val loss: 0.9958592057228088
Epoch 880, training loss: 12.193340301513672 = 0.0897178128361702 + 2.0 * 6.051811218261719
Epoch 880, val loss: 1.0042518377304077
Epoch 890, training loss: 12.196630477905273 = 0.08603768050670624 + 2.0 * 6.055296421051025
Epoch 890, val loss: 1.0125548839569092
Epoch 900, training loss: 12.184491157531738 = 0.08255979418754578 + 2.0 * 6.050965785980225
Epoch 900, val loss: 1.0208935737609863
Epoch 910, training loss: 12.179279327392578 = 0.07925347983837128 + 2.0 * 6.050013065338135
Epoch 910, val loss: 1.0293834209442139
Epoch 920, training loss: 12.17726993560791 = 0.07612661272287369 + 2.0 * 6.050571441650391
Epoch 920, val loss: 1.0378092527389526
Epoch 930, training loss: 12.171321868896484 = 0.07315849512815475 + 2.0 * 6.049081802368164
Epoch 930, val loss: 1.0463367700576782
Epoch 940, training loss: 12.165237426757812 = 0.07033558934926987 + 2.0 * 6.047451019287109
Epoch 940, val loss: 1.0548032522201538
Epoch 950, training loss: 12.157511711120605 = 0.06766096502542496 + 2.0 * 6.044925212860107
Epoch 950, val loss: 1.0632656812667847
Epoch 960, training loss: 12.156144142150879 = 0.06511933356523514 + 2.0 * 6.0455121994018555
Epoch 960, val loss: 1.0717105865478516
Epoch 970, training loss: 12.154438972473145 = 0.06270988285541534 + 2.0 * 6.045864582061768
Epoch 970, val loss: 1.0799256563186646
Epoch 980, training loss: 12.150205612182617 = 0.060426753014326096 + 2.0 * 6.044889450073242
Epoch 980, val loss: 1.0883464813232422
Epoch 990, training loss: 12.142041206359863 = 0.05825319513678551 + 2.0 * 6.04189395904541
Epoch 990, val loss: 1.0966168642044067
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7815
Overall ASR: 0.7232
Flip ASR: 0.6756/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.670059204101562 = 1.9222731590270996 + 2.0 * 8.373892784118652
Epoch 0, val loss: 1.9157871007919312
Epoch 10, training loss: 18.660423278808594 = 1.913460612297058 + 2.0 * 8.373481750488281
Epoch 10, val loss: 1.9073443412780762
Epoch 20, training loss: 18.64472770690918 = 1.9025832414627075 + 2.0 * 8.371071815490723
Epoch 20, val loss: 1.8963960409164429
Epoch 30, training loss: 18.598979949951172 = 1.8878268003463745 + 2.0 * 8.355576515197754
Epoch 30, val loss: 1.881133794784546
Epoch 40, training loss: 18.36077117919922 = 1.8696039915084839 + 2.0 * 8.245583534240723
Epoch 40, val loss: 1.863064169883728
Epoch 50, training loss: 16.988265991210938 = 1.8501291275024414 + 2.0 * 7.569068431854248
Epoch 50, val loss: 1.8446762561798096
Epoch 60, training loss: 16.076953887939453 = 1.8333954811096191 + 2.0 * 7.121778964996338
Epoch 60, val loss: 1.8297128677368164
Epoch 70, training loss: 15.537291526794434 = 1.8196742534637451 + 2.0 * 6.858808517456055
Epoch 70, val loss: 1.8166860342025757
Epoch 80, training loss: 15.255169868469238 = 1.8062381744384766 + 2.0 * 6.724465847015381
Epoch 80, val loss: 1.8038932085037231
Epoch 90, training loss: 15.038453102111816 = 1.7935054302215576 + 2.0 * 6.62247371673584
Epoch 90, val loss: 1.791235089302063
Epoch 100, training loss: 14.851709365844727 = 1.7802300453186035 + 2.0 * 6.535739421844482
Epoch 100, val loss: 1.7783513069152832
Epoch 110, training loss: 14.715903282165527 = 1.7664611339569092 + 2.0 * 6.4747209548950195
Epoch 110, val loss: 1.7649837732315063
Epoch 120, training loss: 14.598494529724121 = 1.7513443231582642 + 2.0 * 6.423574924468994
Epoch 120, val loss: 1.7505807876586914
Epoch 130, training loss: 14.495306968688965 = 1.7348487377166748 + 2.0 * 6.3802289962768555
Epoch 130, val loss: 1.7351211309432983
Epoch 140, training loss: 14.408003807067871 = 1.7167799472808838 + 2.0 * 6.345612049102783
Epoch 140, val loss: 1.7186588048934937
Epoch 150, training loss: 14.3292875289917 = 1.6967312097549438 + 2.0 * 6.316277980804443
Epoch 150, val loss: 1.7005845308303833
Epoch 160, training loss: 14.259432792663574 = 1.6738075017929077 + 2.0 * 6.292812824249268
Epoch 160, val loss: 1.6802029609680176
Epoch 170, training loss: 14.195773124694824 = 1.647473931312561 + 2.0 * 6.274149417877197
Epoch 170, val loss: 1.6568998098373413
Epoch 180, training loss: 14.138679504394531 = 1.617530345916748 + 2.0 * 6.2605743408203125
Epoch 180, val loss: 1.6304636001586914
Epoch 190, training loss: 14.076692581176758 = 1.583828330039978 + 2.0 * 6.246432304382324
Epoch 190, val loss: 1.6010271310806274
Epoch 200, training loss: 14.015753746032715 = 1.54607355594635 + 2.0 * 6.234839916229248
Epoch 200, val loss: 1.5682822465896606
Epoch 210, training loss: 13.953128814697266 = 1.504075527191162 + 2.0 * 6.224526405334473
Epoch 210, val loss: 1.5321403741836548
Epoch 220, training loss: 13.890256881713867 = 1.4578267335891724 + 2.0 * 6.216215133666992
Epoch 220, val loss: 1.492698311805725
Epoch 230, training loss: 13.8217191696167 = 1.4082956314086914 + 2.0 * 6.206711769104004
Epoch 230, val loss: 1.4511889219284058
Epoch 240, training loss: 13.754833221435547 = 1.3562698364257812 + 2.0 * 6.199281692504883
Epoch 240, val loss: 1.4080784320831299
Epoch 250, training loss: 13.685297012329102 = 1.302074670791626 + 2.0 * 6.191611289978027
Epoch 250, val loss: 1.363734483718872
Epoch 260, training loss: 13.618165969848633 = 1.2466990947723389 + 2.0 * 6.185733318328857
Epoch 260, val loss: 1.3188978433609009
Epoch 270, training loss: 13.549234390258789 = 1.1913906335830688 + 2.0 * 6.178921699523926
Epoch 270, val loss: 1.2746076583862305
Epoch 280, training loss: 13.482073783874512 = 1.1368424892425537 + 2.0 * 6.1726155281066895
Epoch 280, val loss: 1.2313075065612793
Epoch 290, training loss: 13.421868324279785 = 1.0838367938995361 + 2.0 * 6.169015884399414
Epoch 290, val loss: 1.1898908615112305
Epoch 300, training loss: 13.357548713684082 = 1.0333445072174072 + 2.0 * 6.162102222442627
Epoch 300, val loss: 1.1509075164794922
Epoch 310, training loss: 13.298454284667969 = 0.9849880337715149 + 2.0 * 6.15673303604126
Epoch 310, val loss: 1.1142148971557617
Epoch 320, training loss: 13.243558883666992 = 0.9386125206947327 + 2.0 * 6.152472972869873
Epoch 320, val loss: 1.0796042680740356
Epoch 330, training loss: 13.191433906555176 = 0.894434928894043 + 2.0 * 6.148499488830566
Epoch 330, val loss: 1.047303557395935
Epoch 340, training loss: 13.143973350524902 = 0.8525897860527039 + 2.0 * 6.145691871643066
Epoch 340, val loss: 1.017512321472168
Epoch 350, training loss: 13.094507217407227 = 0.8126673698425293 + 2.0 * 6.1409196853637695
Epoch 350, val loss: 0.9898220300674438
Epoch 360, training loss: 13.05522632598877 = 0.7743872404098511 + 2.0 * 6.1404194831848145
Epoch 360, val loss: 0.9640346765518188
Epoch 370, training loss: 13.008749008178711 = 0.737984836101532 + 2.0 * 6.135382175445557
Epoch 370, val loss: 0.9400765895843506
Epoch 380, training loss: 12.965105056762695 = 0.7031455039978027 + 2.0 * 6.130979537963867
Epoch 380, val loss: 0.9180808663368225
Epoch 390, training loss: 12.929240226745605 = 0.6699061989784241 + 2.0 * 6.129666805267334
Epoch 390, val loss: 0.8977251052856445
Epoch 400, training loss: 12.888055801391602 = 0.6380761861801147 + 2.0 * 6.124989986419678
Epoch 400, val loss: 0.8790857195854187
Epoch 410, training loss: 12.851861953735352 = 0.607617199420929 + 2.0 * 6.122122287750244
Epoch 410, val loss: 0.8618577122688293
Epoch 420, training loss: 12.819459915161133 = 0.5784224271774292 + 2.0 * 6.120518684387207
Epoch 420, val loss: 0.8460524082183838
Epoch 430, training loss: 12.785913467407227 = 0.5505416989326477 + 2.0 * 6.117685794830322
Epoch 430, val loss: 0.8316546082496643
Epoch 440, training loss: 12.755650520324707 = 0.5237714648246765 + 2.0 * 6.115939617156982
Epoch 440, val loss: 0.8185223937034607
Epoch 450, training loss: 12.722930908203125 = 0.4980551600456238 + 2.0 * 6.112437725067139
Epoch 450, val loss: 0.8066880702972412
Epoch 460, training loss: 12.694028854370117 = 0.4733937084674835 + 2.0 * 6.110317707061768
Epoch 460, val loss: 0.7960115671157837
Epoch 470, training loss: 12.671759605407715 = 0.4498036503791809 + 2.0 * 6.110978126525879
Epoch 470, val loss: 0.7865649461746216
Epoch 480, training loss: 12.638317108154297 = 0.4273807108402252 + 2.0 * 6.105468273162842
Epoch 480, val loss: 0.7781562805175781
Epoch 490, training loss: 12.617280006408691 = 0.40592995285987854 + 2.0 * 6.105675220489502
Epoch 490, val loss: 0.7707707285881042
Epoch 500, training loss: 12.594888687133789 = 0.3857046067714691 + 2.0 * 6.1045918464660645
Epoch 500, val loss: 0.7642990350723267
Epoch 510, training loss: 12.56401252746582 = 0.3663516342639923 + 2.0 * 6.098830223083496
Epoch 510, val loss: 0.7587976455688477
Epoch 520, training loss: 12.53961181640625 = 0.34791040420532227 + 2.0 * 6.095850944519043
Epoch 520, val loss: 0.754033088684082
Epoch 530, training loss: 12.518760681152344 = 0.3302857279777527 + 2.0 * 6.094237327575684
Epoch 530, val loss: 0.7499889135360718
Epoch 540, training loss: 12.500336647033691 = 0.31350573897361755 + 2.0 * 6.093415260314941
Epoch 540, val loss: 0.7464871406555176
Epoch 550, training loss: 12.48099136352539 = 0.29764825105667114 + 2.0 * 6.091671466827393
Epoch 550, val loss: 0.7435529828071594
Epoch 560, training loss: 12.461718559265137 = 0.28259196877479553 + 2.0 * 6.089563369750977
Epoch 560, val loss: 0.7411414384841919
Epoch 570, training loss: 12.443533897399902 = 0.2682183086872101 + 2.0 * 6.087657928466797
Epoch 570, val loss: 0.7391334176063538
Epoch 580, training loss: 12.425314903259277 = 0.2545074224472046 + 2.0 * 6.085403919219971
Epoch 580, val loss: 0.7375491261482239
Epoch 590, training loss: 12.412240982055664 = 0.24144110083580017 + 2.0 * 6.085400104522705
Epoch 590, val loss: 0.7363620400428772
Epoch 600, training loss: 12.39870548248291 = 0.22911706566810608 + 2.0 * 6.084794044494629
Epoch 600, val loss: 0.7354442477226257
Epoch 610, training loss: 12.378714561462402 = 0.21741747856140137 + 2.0 * 6.080648422241211
Epoch 610, val loss: 0.7350094318389893
Epoch 620, training loss: 12.363585472106934 = 0.20634448528289795 + 2.0 * 6.078620433807373
Epoch 620, val loss: 0.7348979711532593
Epoch 630, training loss: 12.352483749389648 = 0.19585636258125305 + 2.0 * 6.078313827514648
Epoch 630, val loss: 0.7350968718528748
Epoch 640, training loss: 12.336945533752441 = 0.18589606881141663 + 2.0 * 6.075524806976318
Epoch 640, val loss: 0.7354370951652527
Epoch 650, training loss: 12.327439308166504 = 0.1765097677707672 + 2.0 * 6.075464725494385
Epoch 650, val loss: 0.7362226247787476
Epoch 660, training loss: 12.316165924072266 = 0.16763752698898315 + 2.0 * 6.074264049530029
Epoch 660, val loss: 0.7370806932449341
Epoch 670, training loss: 12.300610542297363 = 0.15932169556617737 + 2.0 * 6.070644378662109
Epoch 670, val loss: 0.7383658289909363
Epoch 680, training loss: 12.291544914245605 = 0.1514694094657898 + 2.0 * 6.070037841796875
Epoch 680, val loss: 0.7398751378059387
Epoch 690, training loss: 12.287822723388672 = 0.14404824376106262 + 2.0 * 6.071887016296387
Epoch 690, val loss: 0.7415056228637695
Epoch 700, training loss: 12.275870323181152 = 0.1370537281036377 + 2.0 * 6.069408416748047
Epoch 700, val loss: 0.7435203194618225
Epoch 710, training loss: 12.26169490814209 = 0.13048167526721954 + 2.0 * 6.065606594085693
Epoch 710, val loss: 0.7456431984901428
Epoch 720, training loss: 12.25930118560791 = 0.12428715825080872 + 2.0 * 6.067506790161133
Epoch 720, val loss: 0.7480189204216003
Epoch 730, training loss: 12.247127532958984 = 0.11846575886011124 + 2.0 * 6.0643310546875
Epoch 730, val loss: 0.7507109045982361
Epoch 740, training loss: 12.239337921142578 = 0.11297734081745148 + 2.0 * 6.063180446624756
Epoch 740, val loss: 0.7533241510391235
Epoch 750, training loss: 12.230645179748535 = 0.10781987756490707 + 2.0 * 6.061412811279297
Epoch 750, val loss: 0.7563503980636597
Epoch 760, training loss: 12.221577644348145 = 0.10295034199953079 + 2.0 * 6.059313774108887
Epoch 760, val loss: 0.7593368291854858
Epoch 770, training loss: 12.213092803955078 = 0.09835851937532425 + 2.0 * 6.057367324829102
Epoch 770, val loss: 0.762558102607727
Epoch 780, training loss: 12.228507995605469 = 0.09400153905153275 + 2.0 * 6.067253112792969
Epoch 780, val loss: 0.7658828496932983
Epoch 790, training loss: 12.204723358154297 = 0.08992411196231842 + 2.0 * 6.057399749755859
Epoch 790, val loss: 0.7694357633590698
Epoch 800, training loss: 12.194979667663574 = 0.08609182387590408 + 2.0 * 6.054443836212158
Epoch 800, val loss: 0.7730353474617004
Epoch 810, training loss: 12.190742492675781 = 0.08246248215436935 + 2.0 * 6.054140090942383
Epoch 810, val loss: 0.7766966223716736
Epoch 820, training loss: 12.186626434326172 = 0.07903580367565155 + 2.0 * 6.053795337677002
Epoch 820, val loss: 0.7805341482162476
Epoch 830, training loss: 12.182342529296875 = 0.07579229027032852 + 2.0 * 6.053275108337402
Epoch 830, val loss: 0.7843297123908997
Epoch 840, training loss: 12.175594329833984 = 0.07269978523254395 + 2.0 * 6.05144739151001
Epoch 840, val loss: 0.788219153881073
Epoch 850, training loss: 12.183480262756348 = 0.06978197395801544 + 2.0 * 6.056849002838135
Epoch 850, val loss: 0.7922208309173584
Epoch 860, training loss: 12.16593074798584 = 0.06703780591487885 + 2.0 * 6.0494465827941895
Epoch 860, val loss: 0.7964028120040894
Epoch 870, training loss: 12.15875244140625 = 0.06442594528198242 + 2.0 * 6.047163009643555
Epoch 870, val loss: 0.8005291819572449
Epoch 880, training loss: 12.158014297485352 = 0.06195177882909775 + 2.0 * 6.048031330108643
Epoch 880, val loss: 0.8046807050704956
Epoch 890, training loss: 12.149932861328125 = 0.05960643291473389 + 2.0 * 6.045163154602051
Epoch 890, val loss: 0.809032678604126
Epoch 900, training loss: 12.146042823791504 = 0.05737835913896561 + 2.0 * 6.044332027435303
Epoch 900, val loss: 0.8133552074432373
Epoch 910, training loss: 12.156213760375977 = 0.055274609476327896 + 2.0 * 6.050469398498535
Epoch 910, val loss: 0.8178179264068604
Epoch 920, training loss: 12.142651557922363 = 0.05325014889240265 + 2.0 * 6.044700622558594
Epoch 920, val loss: 0.8220081925392151
Epoch 930, training loss: 12.138555526733398 = 0.05135295167565346 + 2.0 * 6.0436015129089355
Epoch 930, val loss: 0.826526403427124
Epoch 940, training loss: 12.133481979370117 = 0.04953111708164215 + 2.0 * 6.041975498199463
Epoch 940, val loss: 0.830891489982605
Epoch 950, training loss: 12.129181861877441 = 0.04780182987451553 + 2.0 * 6.040689945220947
Epoch 950, val loss: 0.8353644609451294
Epoch 960, training loss: 12.127803802490234 = 0.04615962877869606 + 2.0 * 6.0408220291137695
Epoch 960, val loss: 0.8398290276527405
Epoch 970, training loss: 12.124324798583984 = 0.04458291456103325 + 2.0 * 6.039870738983154
Epoch 970, val loss: 0.8442538380622864
Epoch 980, training loss: 12.120857238769531 = 0.04308991879224777 + 2.0 * 6.038883686065674
Epoch 980, val loss: 0.8487887978553772
Epoch 990, training loss: 12.117436408996582 = 0.041663721203804016 + 2.0 * 6.037886142730713
Epoch 990, val loss: 0.853390634059906
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8037
Overall ASR: 0.9373
Flip ASR: 0.9244/225 nodes
The final ASR:0.80197, 0.09610, Accuracy:0.78642, 0.01259
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11574])
remove edge: torch.Size([2, 9492])
updated graph: torch.Size([2, 10510])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
The final ASR:0.97786, 0.00301, Accuracy:0.83210, 0.00761
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.5 der2:0.3 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 18.686779022216797 = 1.9390900135040283 + 2.0 * 8.373844146728516
Epoch 0, val loss: 1.9463647603988647
Epoch 10, training loss: 18.675710678100586 = 1.9292768239974976 + 2.0 * 8.37321662902832
Epoch 10, val loss: 1.9370180368423462
Epoch 20, training loss: 18.655109405517578 = 1.9168922901153564 + 2.0 * 8.369108200073242
Epoch 20, val loss: 1.9248039722442627
Epoch 30, training loss: 18.58428192138672 = 1.900091528892517 + 2.0 * 8.342095375061035
Epoch 30, val loss: 1.9079034328460693
Epoch 40, training loss: 18.18683624267578 = 1.8814321756362915 + 2.0 * 8.152702331542969
Epoch 40, val loss: 1.88979971408844
Epoch 50, training loss: 17.022741317749023 = 1.8609248399734497 + 2.0 * 7.580907821655273
Epoch 50, val loss: 1.8702175617218018
Epoch 60, training loss: 16.23228645324707 = 1.846603274345398 + 2.0 * 7.19284200668335
Epoch 60, val loss: 1.857292890548706
Epoch 70, training loss: 15.626211166381836 = 1.8350539207458496 + 2.0 * 6.895578861236572
Epoch 70, val loss: 1.8458863496780396
Epoch 80, training loss: 15.165695190429688 = 1.823353886604309 + 2.0 * 6.671170711517334
Epoch 80, val loss: 1.8345918655395508
Epoch 90, training loss: 14.949939727783203 = 1.810855746269226 + 2.0 * 6.569541931152344
Epoch 90, val loss: 1.8226139545440674
Epoch 100, training loss: 14.797783851623535 = 1.7977231740951538 + 2.0 * 6.500030517578125
Epoch 100, val loss: 1.8099607229232788
Epoch 110, training loss: 14.674263000488281 = 1.7850128412246704 + 2.0 * 6.444624900817871
Epoch 110, val loss: 1.797616958618164
Epoch 120, training loss: 14.575867652893066 = 1.773026466369629 + 2.0 * 6.401420593261719
Epoch 120, val loss: 1.7860335111618042
Epoch 130, training loss: 14.494653701782227 = 1.7605725526809692 + 2.0 * 6.367040634155273
Epoch 130, val loss: 1.7743449211120605
Epoch 140, training loss: 14.425286293029785 = 1.7471941709518433 + 2.0 * 6.339046001434326
Epoch 140, val loss: 1.762162208557129
Epoch 150, training loss: 14.357933044433594 = 1.7325502634048462 + 2.0 * 6.3126912117004395
Epoch 150, val loss: 1.7492549419403076
Epoch 160, training loss: 14.298078536987305 = 1.7164113521575928 + 2.0 * 6.290833473205566
Epoch 160, val loss: 1.7354240417480469
Epoch 170, training loss: 14.24483585357666 = 1.6982982158660889 + 2.0 * 6.273268699645996
Epoch 170, val loss: 1.7202789783477783
Epoch 180, training loss: 14.19157600402832 = 1.6780325174331665 + 2.0 * 6.256771564483643
Epoch 180, val loss: 1.703486680984497
Epoch 190, training loss: 14.14115047454834 = 1.655293583869934 + 2.0 * 6.242928504943848
Epoch 190, val loss: 1.6848032474517822
Epoch 200, training loss: 14.089933395385742 = 1.629762887954712 + 2.0 * 6.230085372924805
Epoch 200, val loss: 1.6639680862426758
Epoch 210, training loss: 14.045695304870605 = 1.6014986038208008 + 2.0 * 6.222098350524902
Epoch 210, val loss: 1.6409796476364136
Epoch 220, training loss: 13.988219261169434 = 1.570654273033142 + 2.0 * 6.20878267288208
Epoch 220, val loss: 1.6159740686416626
Epoch 230, training loss: 13.93491268157959 = 1.536965012550354 + 2.0 * 6.198973655700684
Epoch 230, val loss: 1.588721752166748
Epoch 240, training loss: 13.891249656677246 = 1.5004252195358276 + 2.0 * 6.1954121589660645
Epoch 240, val loss: 1.559322714805603
Epoch 250, training loss: 13.828857421875 = 1.4615217447280884 + 2.0 * 6.1836676597595215
Epoch 250, val loss: 1.5281331539154053
Epoch 260, training loss: 13.771097183227539 = 1.4203144311904907 + 2.0 * 6.17539119720459
Epoch 260, val loss: 1.4951657056808472
Epoch 270, training loss: 13.714183807373047 = 1.376808524131775 + 2.0 * 6.16868782043457
Epoch 270, val loss: 1.4603972434997559
Epoch 280, training loss: 13.662145614624023 = 1.3316025733947754 + 2.0 * 6.165271759033203
Epoch 280, val loss: 1.4246271848678589
Epoch 290, training loss: 13.599355697631836 = 1.28557288646698 + 2.0 * 6.156891345977783
Epoch 290, val loss: 1.388018012046814
Epoch 300, training loss: 13.540177345275879 = 1.238605260848999 + 2.0 * 6.15078592300415
Epoch 300, val loss: 1.3507764339447021
Epoch 310, training loss: 13.484844207763672 = 1.1915819644927979 + 2.0 * 6.146631240844727
Epoch 310, val loss: 1.313690423965454
Epoch 320, training loss: 13.428109169006348 = 1.145065426826477 + 2.0 * 6.14152193069458
Epoch 320, val loss: 1.2770494222640991
Epoch 330, training loss: 13.372381210327148 = 1.098976969718933 + 2.0 * 6.136702060699463
Epoch 330, val loss: 1.2409319877624512
Epoch 340, training loss: 13.32877254486084 = 1.0534658432006836 + 2.0 * 6.137653350830078
Epoch 340, val loss: 1.2055273056030273
Epoch 350, training loss: 13.270208358764648 = 1.0093960762023926 + 2.0 * 6.130405902862549
Epoch 350, val loss: 1.1713165044784546
Epoch 360, training loss: 13.217958450317383 = 0.9665278196334839 + 2.0 * 6.125715255737305
Epoch 360, val loss: 1.1381771564483643
Epoch 370, training loss: 13.167898178100586 = 0.9246146082878113 + 2.0 * 6.121641635894775
Epoch 370, val loss: 1.1061036586761475
Epoch 380, training loss: 13.131747245788574 = 0.8837106823921204 + 2.0 * 6.12401819229126
Epoch 380, val loss: 1.0751540660858154
Epoch 390, training loss: 13.076406478881836 = 0.8442601561546326 + 2.0 * 6.116073131561279
Epoch 390, val loss: 1.0455543994903564
Epoch 400, training loss: 13.032448768615723 = 0.8063089847564697 + 2.0 * 6.113070011138916
Epoch 400, val loss: 1.0173215866088867
Epoch 410, training loss: 12.989248275756836 = 0.7695446610450745 + 2.0 * 6.109851837158203
Epoch 410, val loss: 0.9903064966201782
Epoch 420, training loss: 12.959054946899414 = 0.734049379825592 + 2.0 * 6.112502574920654
Epoch 420, val loss: 0.9646799564361572
Epoch 430, training loss: 12.909748077392578 = 0.7001990079879761 + 2.0 * 6.104774475097656
Epoch 430, val loss: 0.9404476881027222
Epoch 440, training loss: 12.872958183288574 = 0.6680174469947815 + 2.0 * 6.102470397949219
Epoch 440, val loss: 0.9178970456123352
Epoch 450, training loss: 12.837530136108398 = 0.6373946070671082 + 2.0 * 6.100067615509033
Epoch 450, val loss: 0.8968777656555176
Epoch 460, training loss: 12.803979873657227 = 0.6084707975387573 + 2.0 * 6.09775447845459
Epoch 460, val loss: 0.8775200247764587
Epoch 470, training loss: 12.774540901184082 = 0.5814477205276489 + 2.0 * 6.096546649932861
Epoch 470, val loss: 0.8600090146064758
Epoch 480, training loss: 12.744250297546387 = 0.5559571385383606 + 2.0 * 6.094146728515625
Epoch 480, val loss: 0.8441559672355652
Epoch 490, training loss: 12.71394157409668 = 0.5318074822425842 + 2.0 * 6.091066837310791
Epoch 490, val loss: 0.8297016620635986
Epoch 500, training loss: 12.68989372253418 = 0.5089221596717834 + 2.0 * 6.090485572814941
Epoch 500, val loss: 0.8165595531463623
Epoch 510, training loss: 12.6697359085083 = 0.4872990548610687 + 2.0 * 6.0912184715271
Epoch 510, val loss: 0.8047113418579102
Epoch 520, training loss: 12.63862419128418 = 0.4668395221233368 + 2.0 * 6.085892200469971
Epoch 520, val loss: 0.7941280603408813
Epoch 530, training loss: 12.616846084594727 = 0.44734880328178406 + 2.0 * 6.0847487449646
Epoch 530, val loss: 0.7845322489738464
Epoch 540, training loss: 12.591410636901855 = 0.4286997616291046 + 2.0 * 6.081355571746826
Epoch 540, val loss: 0.7758225798606873
Epoch 550, training loss: 12.571410179138184 = 0.41077545285224915 + 2.0 * 6.080317497253418
Epoch 550, val loss: 0.767867386341095
Epoch 560, training loss: 12.554503440856934 = 0.39340662956237793 + 2.0 * 6.080548286437988
Epoch 560, val loss: 0.7605363130569458
Epoch 570, training loss: 12.53472900390625 = 0.3765525817871094 + 2.0 * 6.07908821105957
Epoch 570, val loss: 0.7537950873374939
Epoch 580, training loss: 12.511154174804688 = 0.36011257767677307 + 2.0 * 6.075520992279053
Epoch 580, val loss: 0.747547447681427
Epoch 590, training loss: 12.494911193847656 = 0.34402981400489807 + 2.0 * 6.075440883636475
Epoch 590, val loss: 0.7416801452636719
Epoch 600, training loss: 12.473315238952637 = 0.328534871339798 + 2.0 * 6.072390079498291
Epoch 600, val loss: 0.7363891005516052
Epoch 610, training loss: 12.454485893249512 = 0.3134051561355591 + 2.0 * 6.070540428161621
Epoch 610, val loss: 0.7316029071807861
Epoch 620, training loss: 12.435807228088379 = 0.2986532151699066 + 2.0 * 6.068576812744141
Epoch 620, val loss: 0.7271856069564819
Epoch 630, training loss: 12.42901611328125 = 0.2841677963733673 + 2.0 * 6.072423934936523
Epoch 630, val loss: 0.7231829762458801
Epoch 640, training loss: 12.417927742004395 = 0.2700733542442322 + 2.0 * 6.073927402496338
Epoch 640, val loss: 0.7193760871887207
Epoch 650, training loss: 12.390506744384766 = 0.2565333843231201 + 2.0 * 6.066986560821533
Epoch 650, val loss: 0.7159997224807739
Epoch 660, training loss: 12.370992660522461 = 0.24324220418930054 + 2.0 * 6.063875198364258
Epoch 660, val loss: 0.7130206227302551
Epoch 670, training loss: 12.354585647583008 = 0.2302892506122589 + 2.0 * 6.062148094177246
Epoch 670, val loss: 0.7103663086891174
Epoch 680, training loss: 12.341809272766113 = 0.21765674650669098 + 2.0 * 6.062076091766357
Epoch 680, val loss: 0.7080312967300415
Epoch 690, training loss: 12.326685905456543 = 0.20544400811195374 + 2.0 * 6.0606207847595215
Epoch 690, val loss: 0.7061054110527039
Epoch 700, training loss: 12.314122200012207 = 0.19374065101146698 + 2.0 * 6.060190677642822
Epoch 700, val loss: 0.7046570777893066
Epoch 710, training loss: 12.297708511352539 = 0.18253332376480103 + 2.0 * 6.057587623596191
Epoch 710, val loss: 0.7035844326019287
Epoch 720, training loss: 12.289789199829102 = 0.1717890352010727 + 2.0 * 6.059000015258789
Epoch 720, val loss: 0.7028921246528625
Epoch 730, training loss: 12.280075073242188 = 0.16169874370098114 + 2.0 * 6.059188365936279
Epoch 730, val loss: 0.7024616599082947
Epoch 740, training loss: 12.263435363769531 = 0.15221478044986725 + 2.0 * 6.055610179901123
Epoch 740, val loss: 0.7027574181556702
Epoch 750, training loss: 12.250152587890625 = 0.1433417797088623 + 2.0 * 6.053405284881592
Epoch 750, val loss: 0.7033074498176575
Epoch 760, training loss: 12.239961624145508 = 0.13505934178829193 + 2.0 * 6.052451133728027
Epoch 760, val loss: 0.7043122053146362
Epoch 770, training loss: 12.239603042602539 = 0.12730742990970612 + 2.0 * 6.056147575378418
Epoch 770, val loss: 0.705640435218811
Epoch 780, training loss: 12.224451065063477 = 0.12016510218381882 + 2.0 * 6.052143096923828
Epoch 780, val loss: 0.7074529528617859
Epoch 790, training loss: 12.222228050231934 = 0.11351925134658813 + 2.0 * 6.054354190826416
Epoch 790, val loss: 0.7093667387962341
Epoch 800, training loss: 12.206582069396973 = 0.10737185180187225 + 2.0 * 6.049604892730713
Epoch 800, val loss: 0.711615800857544
Epoch 810, training loss: 12.19651985168457 = 0.10169707983732224 + 2.0 * 6.0474114418029785
Epoch 810, val loss: 0.7139875292778015
Epoch 820, training loss: 12.189311981201172 = 0.09642039984464645 + 2.0 * 6.046445846557617
Epoch 820, val loss: 0.7166633009910583
Epoch 830, training loss: 12.19131088256836 = 0.09152833372354507 + 2.0 * 6.049891471862793
Epoch 830, val loss: 0.7193810343742371
Epoch 840, training loss: 12.175673484802246 = 0.0870039239525795 + 2.0 * 6.044334888458252
Epoch 840, val loss: 0.7223302125930786
Epoch 850, training loss: 12.175909996032715 = 0.08281519263982773 + 2.0 * 6.0465474128723145
Epoch 850, val loss: 0.7252678871154785
Epoch 860, training loss: 12.165735244750977 = 0.07890937477350235 + 2.0 * 6.043413162231445
Epoch 860, val loss: 0.728255033493042
Epoch 870, training loss: 12.157674789428711 = 0.07526735961437225 + 2.0 * 6.041203498840332
Epoch 870, val loss: 0.7315347790718079
Epoch 880, training loss: 12.153218269348145 = 0.07187727838754654 + 2.0 * 6.040670394897461
Epoch 880, val loss: 0.7348117232322693
Epoch 890, training loss: 12.14915943145752 = 0.06868594139814377 + 2.0 * 6.040236949920654
Epoch 890, val loss: 0.7381091117858887
Epoch 900, training loss: 12.14769458770752 = 0.06570978462696075 + 2.0 * 6.040992259979248
Epoch 900, val loss: 0.7412066459655762
Epoch 910, training loss: 12.145501136779785 = 0.06292912364006042 + 2.0 * 6.041285991668701
Epoch 910, val loss: 0.7446090579032898
Epoch 920, training loss: 12.134791374206543 = 0.06033950671553612 + 2.0 * 6.037225723266602
Epoch 920, val loss: 0.748007595539093
Epoch 930, training loss: 12.130142211914062 = 0.0579025074839592 + 2.0 * 6.0361199378967285
Epoch 930, val loss: 0.7514253854751587
Epoch 940, training loss: 12.12760066986084 = 0.055595673620700836 + 2.0 * 6.0360026359558105
Epoch 940, val loss: 0.7548359036445618
Epoch 950, training loss: 12.130945205688477 = 0.05342472344636917 + 2.0 * 6.038760185241699
Epoch 950, val loss: 0.7582315802574158
Epoch 960, training loss: 12.12163257598877 = 0.05137823894619942 + 2.0 * 6.03512716293335
Epoch 960, val loss: 0.7614795565605164
Epoch 970, training loss: 12.11965274810791 = 0.04945903643965721 + 2.0 * 6.035096645355225
Epoch 970, val loss: 0.7648882269859314
Epoch 980, training loss: 12.111640930175781 = 0.04764009267091751 + 2.0 * 6.032000541687012
Epoch 980, val loss: 0.7683011889457703
Epoch 990, training loss: 12.108808517456055 = 0.04591933265328407 + 2.0 * 6.031444549560547
Epoch 990, val loss: 0.771696150302887
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.7712
Flip ASR: 0.7244/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.710975646972656 = 1.9632474184036255 + 2.0 * 8.37386417388916
Epoch 0, val loss: 1.968043565750122
Epoch 10, training loss: 18.700218200683594 = 1.9534388780593872 + 2.0 * 8.37338924407959
Epoch 10, val loss: 1.9578145742416382
Epoch 20, training loss: 18.68197250366211 = 1.941487431526184 + 2.0 * 8.37024211883545
Epoch 20, val loss: 1.9452285766601562
Epoch 30, training loss: 18.625864028930664 = 1.925321102142334 + 2.0 * 8.350271224975586
Epoch 30, val loss: 1.9280673265457153
Epoch 40, training loss: 18.356773376464844 = 1.905375599861145 + 2.0 * 8.225698471069336
Epoch 40, val loss: 1.9076086282730103
Epoch 50, training loss: 17.2390079498291 = 1.884464979171753 + 2.0 * 7.677271842956543
Epoch 50, val loss: 1.8866686820983887
Epoch 60, training loss: 16.186826705932617 = 1.8679585456848145 + 2.0 * 7.1594343185424805
Epoch 60, val loss: 1.8708566427230835
Epoch 70, training loss: 15.550944328308105 = 1.8558037281036377 + 2.0 * 6.847570419311523
Epoch 70, val loss: 1.8578025102615356
Epoch 80, training loss: 15.165971755981445 = 1.8424732685089111 + 2.0 * 6.661749362945557
Epoch 80, val loss: 1.8441680669784546
Epoch 90, training loss: 14.913990020751953 = 1.8291881084442139 + 2.0 * 6.54240083694458
Epoch 90, val loss: 1.8303632736206055
Epoch 100, training loss: 14.761741638183594 = 1.8144611120224 + 2.0 * 6.473640441894531
Epoch 100, val loss: 1.8153023719787598
Epoch 110, training loss: 14.64789867401123 = 1.7985827922821045 + 2.0 * 6.424657821655273
Epoch 110, val loss: 1.799251675605774
Epoch 120, training loss: 14.54828929901123 = 1.7832038402557373 + 2.0 * 6.382542610168457
Epoch 120, val loss: 1.783894419670105
Epoch 130, training loss: 14.464214324951172 = 1.7684518098831177 + 2.0 * 6.347881317138672
Epoch 130, val loss: 1.7689570188522339
Epoch 140, training loss: 14.392478942871094 = 1.7533247470855713 + 2.0 * 6.319577217102051
Epoch 140, val loss: 1.7537956237792969
Epoch 150, training loss: 14.325400352478027 = 1.7372410297393799 + 2.0 * 6.294079780578613
Epoch 150, val loss: 1.7380735874176025
Epoch 160, training loss: 14.271373748779297 = 1.7196013927459717 + 2.0 * 6.275886058807373
Epoch 160, val loss: 1.7212191820144653
Epoch 170, training loss: 14.21603775024414 = 1.7001657485961914 + 2.0 * 6.257936000823975
Epoch 170, val loss: 1.703040599822998
Epoch 180, training loss: 14.161687850952148 = 1.6783971786499023 + 2.0 * 6.241645336151123
Epoch 180, val loss: 1.6832168102264404
Epoch 190, training loss: 14.109655380249023 = 1.6538636684417725 + 2.0 * 6.227895736694336
Epoch 190, val loss: 1.6612564325332642
Epoch 200, training loss: 14.059076309204102 = 1.6261863708496094 + 2.0 * 6.216444969177246
Epoch 200, val loss: 1.636941909790039
Epoch 210, training loss: 14.00802230834961 = 1.5953975915908813 + 2.0 * 6.20631217956543
Epoch 210, val loss: 1.610209584236145
Epoch 220, training loss: 13.95183277130127 = 1.5609041452407837 + 2.0 * 6.195464134216309
Epoch 220, val loss: 1.5805237293243408
Epoch 230, training loss: 13.902467727661133 = 1.5224881172180176 + 2.0 * 6.189990043640137
Epoch 230, val loss: 1.5479650497436523
Epoch 240, training loss: 13.840377807617188 = 1.4810136556625366 + 2.0 * 6.17968225479126
Epoch 240, val loss: 1.5132251977920532
Epoch 250, training loss: 13.779768943786621 = 1.436269760131836 + 2.0 * 6.171749591827393
Epoch 250, val loss: 1.4763402938842773
Epoch 260, training loss: 13.724437713623047 = 1.3890560865402222 + 2.0 * 6.167690753936768
Epoch 260, val loss: 1.438241958618164
Epoch 270, training loss: 13.65997314453125 = 1.3407421112060547 + 2.0 * 6.159615516662598
Epoch 270, val loss: 1.399902582168579
Epoch 280, training loss: 13.600629806518555 = 1.2919001579284668 + 2.0 * 6.154364585876465
Epoch 280, val loss: 1.362093210220337
Epoch 290, training loss: 13.54140567779541 = 1.2435449361801147 + 2.0 * 6.148930549621582
Epoch 290, val loss: 1.3254586458206177
Epoch 300, training loss: 13.482780456542969 = 1.1957476139068604 + 2.0 * 6.143516540527344
Epoch 300, val loss: 1.2901568412780762
Epoch 310, training loss: 13.435407638549805 = 1.1487318277359009 + 2.0 * 6.143337726593018
Epoch 310, val loss: 1.2564325332641602
Epoch 320, training loss: 13.37810230255127 = 1.1039807796478271 + 2.0 * 6.137060642242432
Epoch 320, val loss: 1.2251096963882446
Epoch 330, training loss: 13.32203483581543 = 1.0613179206848145 + 2.0 * 6.130358695983887
Epoch 330, val loss: 1.1958340406417847
Epoch 340, training loss: 13.273439407348633 = 1.0202127695083618 + 2.0 * 6.126613140106201
Epoch 340, val loss: 1.1683956384658813
Epoch 350, training loss: 13.241002082824707 = 0.9805453419685364 + 2.0 * 6.130228519439697
Epoch 350, val loss: 1.1425552368164062
Epoch 360, training loss: 13.185677528381348 = 0.9425907135009766 + 2.0 * 6.1215434074401855
Epoch 360, val loss: 1.1186468601226807
Epoch 370, training loss: 13.141063690185547 = 0.906552791595459 + 2.0 * 6.117255687713623
Epoch 370, val loss: 1.0965579748153687
Epoch 380, training loss: 13.096817016601562 = 0.8719946146011353 + 2.0 * 6.112411022186279
Epoch 380, val loss: 1.0759050846099854
Epoch 390, training loss: 13.073868751525879 = 0.8387032151222229 + 2.0 * 6.11758279800415
Epoch 390, val loss: 1.056553602218628
Epoch 400, training loss: 13.020740509033203 = 0.8069939613342285 + 2.0 * 6.106873512268066
Epoch 400, val loss: 1.0386826992034912
Epoch 410, training loss: 12.984850883483887 = 0.7767341732978821 + 2.0 * 6.104058265686035
Epoch 410, val loss: 1.0220277309417725
Epoch 420, training loss: 12.952582359313965 = 0.7476091384887695 + 2.0 * 6.102486610412598
Epoch 420, val loss: 1.006468415260315
Epoch 430, training loss: 12.920736312866211 = 0.7196562886238098 + 2.0 * 6.1005401611328125
Epoch 430, val loss: 0.9918057918548584
Epoch 440, training loss: 12.886016845703125 = 0.6929176449775696 + 2.0 * 6.0965495109558105
Epoch 440, val loss: 0.9782091975212097
Epoch 450, training loss: 12.868143081665039 = 0.6671667695045471 + 2.0 * 6.100488185882568
Epoch 450, val loss: 0.9654360413551331
Epoch 460, training loss: 12.829452514648438 = 0.6427151560783386 + 2.0 * 6.0933685302734375
Epoch 460, val loss: 0.9532892107963562
Epoch 470, training loss: 12.800199508666992 = 0.6191446185112 + 2.0 * 6.090527534484863
Epoch 470, val loss: 0.9420697093009949
Epoch 480, training loss: 12.772562980651855 = 0.5963627099990845 + 2.0 * 6.088099956512451
Epoch 480, val loss: 0.9314767718315125
Epoch 490, training loss: 12.764969825744629 = 0.5743480920791626 + 2.0 * 6.095310688018799
Epoch 490, val loss: 0.9214345812797546
Epoch 500, training loss: 12.725200653076172 = 0.5532375574111938 + 2.0 * 6.085981369018555
Epoch 500, val loss: 0.9121246933937073
Epoch 510, training loss: 12.699419975280762 = 0.5330108404159546 + 2.0 * 6.083204746246338
Epoch 510, val loss: 0.9035778641700745
Epoch 520, training loss: 12.676328659057617 = 0.5133867859840393 + 2.0 * 6.081470966339111
Epoch 520, val loss: 0.8956629633903503
Epoch 530, training loss: 12.659027099609375 = 0.49437224864959717 + 2.0 * 6.082327365875244
Epoch 530, val loss: 0.8882550001144409
Epoch 540, training loss: 12.632987022399902 = 0.4759753942489624 + 2.0 * 6.078505992889404
Epoch 540, val loss: 0.8815295100212097
Epoch 550, training loss: 12.615119934082031 = 0.45823362469673157 + 2.0 * 6.0784430503845215
Epoch 550, val loss: 0.8754274845123291
Epoch 560, training loss: 12.603472709655762 = 0.44103509187698364 + 2.0 * 6.081218719482422
Epoch 560, val loss: 0.8698312640190125
Epoch 570, training loss: 12.574565887451172 = 0.4244000315666199 + 2.0 * 6.075082778930664
Epoch 570, val loss: 0.8649491667747498
Epoch 580, training loss: 12.553108215332031 = 0.4083070456981659 + 2.0 * 6.0724005699157715
Epoch 580, val loss: 0.8606487512588501
Epoch 590, training loss: 12.534435272216797 = 0.39262154698371887 + 2.0 * 6.070906639099121
Epoch 590, val loss: 0.8569054007530212
Epoch 600, training loss: 12.53464126586914 = 0.3773207664489746 + 2.0 * 6.078660011291504
Epoch 600, val loss: 0.8536695837974548
Epoch 610, training loss: 12.500185012817383 = 0.3625939190387726 + 2.0 * 6.068795680999756
Epoch 610, val loss: 0.8509407639503479
Epoch 620, training loss: 12.483572006225586 = 0.34827643632888794 + 2.0 * 6.067647933959961
Epoch 620, val loss: 0.8487609624862671
Epoch 630, training loss: 12.469655990600586 = 0.33429887890815735 + 2.0 * 6.067678451538086
Epoch 630, val loss: 0.8469668626785278
Epoch 640, training loss: 12.451112747192383 = 0.32075196504592896 + 2.0 * 6.06518030166626
Epoch 640, val loss: 0.8455438613891602
Epoch 650, training loss: 12.437278747558594 = 0.30767175555229187 + 2.0 * 6.064803600311279
Epoch 650, val loss: 0.8446736931800842
Epoch 660, training loss: 12.421863555908203 = 0.29504889249801636 + 2.0 * 6.0634074211120605
Epoch 660, val loss: 0.8443191051483154
Epoch 670, training loss: 12.412321090698242 = 0.28284546732902527 + 2.0 * 6.064737796783447
Epoch 670, val loss: 0.8443196415901184
Epoch 680, training loss: 12.395730972290039 = 0.27110329270362854 + 2.0 * 6.062314033508301
Epoch 680, val loss: 0.8448371887207031
Epoch 690, training loss: 12.381357192993164 = 0.25980639457702637 + 2.0 * 6.060775279998779
Epoch 690, val loss: 0.8456900119781494
Epoch 700, training loss: 12.367216110229492 = 0.2489745318889618 + 2.0 * 6.0591206550598145
Epoch 700, val loss: 0.8470109701156616
Epoch 710, training loss: 12.356008529663086 = 0.23858137428760529 + 2.0 * 6.058713436126709
Epoch 710, val loss: 0.8487144112586975
Epoch 720, training loss: 12.34589672088623 = 0.22863060235977173 + 2.0 * 6.058632850646973
Epoch 720, val loss: 0.8507453799247742
Epoch 730, training loss: 12.332296371459961 = 0.2191668152809143 + 2.0 * 6.056564807891846
Epoch 730, val loss: 0.8532378077507019
Epoch 740, training loss: 12.337634086608887 = 0.21009461581707 + 2.0 * 6.063769817352295
Epoch 740, val loss: 0.8560675382614136
Epoch 750, training loss: 12.315858840942383 = 0.2015615552663803 + 2.0 * 6.057148456573486
Epoch 750, val loss: 0.8591329455375671
Epoch 760, training loss: 12.29900074005127 = 0.19340208172798157 + 2.0 * 6.052799224853516
Epoch 760, val loss: 0.8627527356147766
Epoch 770, training loss: 12.28963851928711 = 0.18562376499176025 + 2.0 * 6.05200719833374
Epoch 770, val loss: 0.8666182160377502
Epoch 780, training loss: 12.28044605255127 = 0.17821162939071655 + 2.0 * 6.051117420196533
Epoch 780, val loss: 0.8707618713378906
Epoch 790, training loss: 12.28762435913086 = 0.17114052176475525 + 2.0 * 6.058241844177246
Epoch 790, val loss: 0.8751139640808105
Epoch 800, training loss: 12.26391887664795 = 0.16442392766475677 + 2.0 * 6.049747467041016
Epoch 800, val loss: 0.8796902894973755
Epoch 810, training loss: 12.263483047485352 = 0.1580371856689453 + 2.0 * 6.052722930908203
Epoch 810, val loss: 0.8845936059951782
Epoch 820, training loss: 12.249513626098633 = 0.1519317626953125 + 2.0 * 6.04879093170166
Epoch 820, val loss: 0.889579176902771
Epoch 830, training loss: 12.24087142944336 = 0.14610746502876282 + 2.0 * 6.04738187789917
Epoch 830, val loss: 0.8948881030082703
Epoch 840, training loss: 12.242472648620605 = 0.14052961766719818 + 2.0 * 6.050971508026123
Epoch 840, val loss: 0.9002448916435242
Epoch 850, training loss: 12.228562355041504 = 0.13521462678909302 + 2.0 * 6.046673774719238
Epoch 850, val loss: 0.905733585357666
Epoch 860, training loss: 12.221481323242188 = 0.13013988733291626 + 2.0 * 6.045670509338379
Epoch 860, val loss: 0.9114472270011902
Epoch 870, training loss: 12.22268009185791 = 0.12527857720851898 + 2.0 * 6.04870080947876
Epoch 870, val loss: 0.9171147346496582
Epoch 880, training loss: 12.211264610290527 = 0.1206606924533844 + 2.0 * 6.045301914215088
Epoch 880, val loss: 0.9230402112007141
Epoch 890, training loss: 12.205589294433594 = 0.11622577160596848 + 2.0 * 6.044681549072266
Epoch 890, val loss: 0.9289185404777527
Epoch 900, training loss: 12.195405006408691 = 0.11200045049190521 + 2.0 * 6.0417022705078125
Epoch 900, val loss: 0.9349130392074585
Epoch 910, training loss: 12.191712379455566 = 0.10795154422521591 + 2.0 * 6.0418806076049805
Epoch 910, val loss: 0.9409980177879333
Epoch 920, training loss: 12.187883377075195 = 0.1040753722190857 + 2.0 * 6.041903972625732
Epoch 920, val loss: 0.9470237493515015
Epoch 930, training loss: 12.184306144714355 = 0.10036467015743256 + 2.0 * 6.041970729827881
Epoch 930, val loss: 0.953071653842926
Epoch 940, training loss: 12.17629623413086 = 0.09681924432516098 + 2.0 * 6.039738655090332
Epoch 940, val loss: 0.9590917229652405
Epoch 950, training loss: 12.18386459350586 = 0.09342918545007706 + 2.0 * 6.045217514038086
Epoch 950, val loss: 0.9651176333427429
Epoch 960, training loss: 12.168023109436035 = 0.09017062932252884 + 2.0 * 6.038926124572754
Epoch 960, val loss: 0.9710870981216431
Epoch 970, training loss: 12.16085433959961 = 0.08705808967351913 + 2.0 * 6.036898136138916
Epoch 970, val loss: 0.9771096706390381
Epoch 980, training loss: 12.156994819641113 = 0.08406537771224976 + 2.0 * 6.036464691162109
Epoch 980, val loss: 0.9831550717353821
Epoch 990, training loss: 12.157719612121582 = 0.0811973288655281 + 2.0 * 6.0382609367370605
Epoch 990, val loss: 0.9890802502632141
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7963
Overall ASR: 0.5092
Flip ASR: 0.4267/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.683013916015625 = 1.9353615045547485 + 2.0 * 8.373826026916504
Epoch 0, val loss: 1.9259365797042847
Epoch 10, training loss: 18.67169952392578 = 1.9254242181777954 + 2.0 * 8.373137474060059
Epoch 10, val loss: 1.9160093069076538
Epoch 20, training loss: 18.65069007873535 = 1.9125810861587524 + 2.0 * 8.369054794311523
Epoch 20, val loss: 1.903050422668457
Epoch 30, training loss: 18.580549240112305 = 1.8947807550430298 + 2.0 * 8.342884063720703
Epoch 30, val loss: 1.8850624561309814
Epoch 40, training loss: 18.175352096557617 = 1.8743208646774292 + 2.0 * 8.15051555633545
Epoch 40, val loss: 1.8652337789535522
Epoch 50, training loss: 16.7270450592041 = 1.8514915704727173 + 2.0 * 7.437776565551758
Epoch 50, val loss: 1.8434401750564575
Epoch 60, training loss: 15.893030166625977 = 1.8351081609725952 + 2.0 * 7.028961181640625
Epoch 60, val loss: 1.8289039134979248
Epoch 70, training loss: 15.432538032531738 = 1.8238471746444702 + 2.0 * 6.804345607757568
Epoch 70, val loss: 1.8189469575881958
Epoch 80, training loss: 15.210253715515137 = 1.8119374513626099 + 2.0 * 6.699158191680908
Epoch 80, val loss: 1.8082131147384644
Epoch 90, training loss: 15.04049015045166 = 1.799230694770813 + 2.0 * 6.620629787445068
Epoch 90, val loss: 1.7969822883605957
Epoch 100, training loss: 14.899175643920898 = 1.7867166996002197 + 2.0 * 6.556229591369629
Epoch 100, val loss: 1.78642737865448
Epoch 110, training loss: 14.773526191711426 = 1.7750823497772217 + 2.0 * 6.4992218017578125
Epoch 110, val loss: 1.7767032384872437
Epoch 120, training loss: 14.660774230957031 = 1.763200283050537 + 2.0 * 6.448787212371826
Epoch 120, val loss: 1.766562819480896
Epoch 130, training loss: 14.554119110107422 = 1.75006902217865 + 2.0 * 6.40202522277832
Epoch 130, val loss: 1.7553503513336182
Epoch 140, training loss: 14.468179702758789 = 1.73546302318573 + 2.0 * 6.366358280181885
Epoch 140, val loss: 1.7430362701416016
Epoch 150, training loss: 14.394234657287598 = 1.7189387083053589 + 2.0 * 6.337647914886475
Epoch 150, val loss: 1.7290267944335938
Epoch 160, training loss: 14.32618236541748 = 1.6998382806777954 + 2.0 * 6.313171863555908
Epoch 160, val loss: 1.7129892110824585
Epoch 170, training loss: 14.262137413024902 = 1.6778843402862549 + 2.0 * 6.292126655578613
Epoch 170, val loss: 1.6946834325790405
Epoch 180, training loss: 14.19994831085205 = 1.6528867483139038 + 2.0 * 6.273530960083008
Epoch 180, val loss: 1.6740177869796753
Epoch 190, training loss: 14.140506744384766 = 1.6244356632232666 + 2.0 * 6.258035659790039
Epoch 190, val loss: 1.6507786512374878
Epoch 200, training loss: 14.078620910644531 = 1.5926092863082886 + 2.0 * 6.243005752563477
Epoch 200, val loss: 1.6247516870498657
Epoch 210, training loss: 14.017724990844727 = 1.5569090843200684 + 2.0 * 6.230408191680908
Epoch 210, val loss: 1.5956758260726929
Epoch 220, training loss: 13.955263137817383 = 1.5172115564346313 + 2.0 * 6.219025611877441
Epoch 220, val loss: 1.5634517669677734
Epoch 230, training loss: 13.896364212036133 = 1.4738770723342896 + 2.0 * 6.211243629455566
Epoch 230, val loss: 1.5284963846206665
Epoch 240, training loss: 13.828128814697266 = 1.4280117750167847 + 2.0 * 6.200058460235596
Epoch 240, val loss: 1.491667628288269
Epoch 250, training loss: 13.763824462890625 = 1.3796820640563965 + 2.0 * 6.192071437835693
Epoch 250, val loss: 1.452924132347107
Epoch 260, training loss: 13.702655792236328 = 1.329135537147522 + 2.0 * 6.186759948730469
Epoch 260, val loss: 1.4125316143035889
Epoch 270, training loss: 13.633293151855469 = 1.2773433923721313 + 2.0 * 6.177974700927734
Epoch 270, val loss: 1.3714994192123413
Epoch 280, training loss: 13.567237854003906 = 1.2249794006347656 + 2.0 * 6.17112922668457
Epoch 280, val loss: 1.330175518989563
Epoch 290, training loss: 13.504533767700195 = 1.1727408170700073 + 2.0 * 6.165896415710449
Epoch 290, val loss: 1.2893089056015015
Epoch 300, training loss: 13.445663452148438 = 1.1218007802963257 + 2.0 * 6.16193151473999
Epoch 300, val loss: 1.24966299533844
Epoch 310, training loss: 13.382109642028809 = 1.0719707012176514 + 2.0 * 6.155069351196289
Epoch 310, val loss: 1.211193561553955
Epoch 320, training loss: 13.324714660644531 = 1.0232443809509277 + 2.0 * 6.150734901428223
Epoch 320, val loss: 1.1739072799682617
Epoch 330, training loss: 13.278748512268066 = 0.976192057132721 + 2.0 * 6.151278018951416
Epoch 330, val loss: 1.1385364532470703
Epoch 340, training loss: 13.219517707824707 = 0.9320439100265503 + 2.0 * 6.143736839294434
Epoch 340, val loss: 1.1053333282470703
Epoch 350, training loss: 13.167301177978516 = 0.8894429206848145 + 2.0 * 6.1389288902282715
Epoch 350, val loss: 1.074149489402771
Epoch 360, training loss: 13.119032859802246 = 0.8486254811286926 + 2.0 * 6.135203838348389
Epoch 360, val loss: 1.0445502996444702
Epoch 370, training loss: 13.073165893554688 = 0.8093429803848267 + 2.0 * 6.131911277770996
Epoch 370, val loss: 1.016837477684021
Epoch 380, training loss: 13.030496597290039 = 0.771990954875946 + 2.0 * 6.129252910614014
Epoch 380, val loss: 0.9911264181137085
Epoch 390, training loss: 12.988734245300293 = 0.736821174621582 + 2.0 * 6.1259565353393555
Epoch 390, val loss: 0.967598557472229
Epoch 400, training loss: 12.951166152954102 = 0.7035354375839233 + 2.0 * 6.123815536499023
Epoch 400, val loss: 0.946039080619812
Epoch 410, training loss: 12.91883659362793 = 0.6722042560577393 + 2.0 * 6.123316287994385
Epoch 410, val loss: 0.9265299439430237
Epoch 420, training loss: 12.87907600402832 = 0.6429352760314941 + 2.0 * 6.118070602416992
Epoch 420, val loss: 0.9091063737869263
Epoch 430, training loss: 12.847177505493164 = 0.6155243515968323 + 2.0 * 6.115826606750488
Epoch 430, val loss: 0.8935304880142212
Epoch 440, training loss: 12.81520938873291 = 0.5899745225906372 + 2.0 * 6.112617492675781
Epoch 440, val loss: 0.8798092603683472
Epoch 450, training loss: 12.793305397033691 = 0.5660635232925415 + 2.0 * 6.113620758056641
Epoch 450, val loss: 0.8677829504013062
Epoch 460, training loss: 12.761124610900879 = 0.5436059236526489 + 2.0 * 6.10875940322876
Epoch 460, val loss: 0.8573356866836548
Epoch 470, training loss: 12.731623649597168 = 0.5225855708122253 + 2.0 * 6.104518890380859
Epoch 470, val loss: 0.8483244776725769
Epoch 480, training loss: 12.706948280334473 = 0.5026561617851257 + 2.0 * 6.102146148681641
Epoch 480, val loss: 0.8405112028121948
Epoch 490, training loss: 12.70875072479248 = 0.4836781919002533 + 2.0 * 6.112536430358887
Epoch 490, val loss: 0.8337470293045044
Epoch 500, training loss: 12.67308521270752 = 0.4657038450241089 + 2.0 * 6.1036906242370605
Epoch 500, val loss: 0.827954113483429
Epoch 510, training loss: 12.638473510742188 = 0.44851285219192505 + 2.0 * 6.094980239868164
Epoch 510, val loss: 0.8231070637702942
Epoch 520, training loss: 12.618776321411133 = 0.43168753385543823 + 2.0 * 6.0935444831848145
Epoch 520, val loss: 0.8188135027885437
Epoch 530, training loss: 12.597247123718262 = 0.41509687900543213 + 2.0 * 6.0910749435424805
Epoch 530, val loss: 0.8149596452713013
Epoch 540, training loss: 12.592650413513184 = 0.39860042929649353 + 2.0 * 6.097024917602539
Epoch 540, val loss: 0.8113996386528015
Epoch 550, training loss: 12.558145523071289 = 0.38214853405952454 + 2.0 * 6.087998390197754
Epoch 550, val loss: 0.8080765008926392
Epoch 560, training loss: 12.548524856567383 = 0.3658151924610138 + 2.0 * 6.091354846954346
Epoch 560, val loss: 0.8050113320350647
Epoch 570, training loss: 12.525962829589844 = 0.3494904637336731 + 2.0 * 6.088236331939697
Epoch 570, val loss: 0.802145779132843
Epoch 580, training loss: 12.499613761901855 = 0.3332483768463135 + 2.0 * 6.0831828117370605
Epoch 580, val loss: 0.7994933128356934
Epoch 590, training loss: 12.477229118347168 = 0.31691300868988037 + 2.0 * 6.080158233642578
Epoch 590, val loss: 0.7969650030136108
Epoch 600, training loss: 12.457616806030273 = 0.3005681335926056 + 2.0 * 6.078524112701416
Epoch 600, val loss: 0.7945747375488281
Epoch 610, training loss: 12.459768295288086 = 0.2843594253063202 + 2.0 * 6.087704658508301
Epoch 610, val loss: 0.792361319065094
Epoch 620, training loss: 12.419017791748047 = 0.2685185372829437 + 2.0 * 6.075249671936035
Epoch 620, val loss: 0.7904952168464661
Epoch 630, training loss: 12.401917457580566 = 0.25318169593811035 + 2.0 * 6.074368000030518
Epoch 630, val loss: 0.7890826463699341
Epoch 640, training loss: 12.386689186096191 = 0.23836441338062286 + 2.0 * 6.074162483215332
Epoch 640, val loss: 0.7881250381469727
Epoch 650, training loss: 12.368151664733887 = 0.22431528568267822 + 2.0 * 6.07191801071167
Epoch 650, val loss: 0.7875391244888306
Epoch 660, training loss: 12.353660583496094 = 0.211024671792984 + 2.0 * 6.07131814956665
Epoch 660, val loss: 0.7877053022384644
Epoch 670, training loss: 12.346653938293457 = 0.19859133660793304 + 2.0 * 6.074031352996826
Epoch 670, val loss: 0.788439154624939
Epoch 680, training loss: 12.324053764343262 = 0.18704913556575775 + 2.0 * 6.068502426147461
Epoch 680, val loss: 0.7896757125854492
Epoch 690, training loss: 12.313138008117676 = 0.17628097534179688 + 2.0 * 6.0684285163879395
Epoch 690, val loss: 0.791474461555481
Epoch 700, training loss: 12.299860000610352 = 0.1663726568222046 + 2.0 * 6.066743850708008
Epoch 700, val loss: 0.7937545776367188
Epoch 710, training loss: 12.285115242004395 = 0.15718722343444824 + 2.0 * 6.063963890075684
Epoch 710, val loss: 0.7965720891952515
Epoch 720, training loss: 12.274410247802734 = 0.14867517352104187 + 2.0 * 6.062867641448975
Epoch 720, val loss: 0.7998445630073547
Epoch 730, training loss: 12.287890434265137 = 0.1407782882452011 + 2.0 * 6.073555946350098
Epoch 730, val loss: 0.8034620881080627
Epoch 740, training loss: 12.257366180419922 = 0.13361212611198425 + 2.0 * 6.061877250671387
Epoch 740, val loss: 0.8072280287742615
Epoch 750, training loss: 12.245769500732422 = 0.12693528831005096 + 2.0 * 6.059417247772217
Epoch 750, val loss: 0.8115589022636414
Epoch 760, training loss: 12.236955642700195 = 0.12071014195680618 + 2.0 * 6.058122634887695
Epoch 760, val loss: 0.8161282539367676
Epoch 770, training loss: 12.228217124938965 = 0.11488471180200577 + 2.0 * 6.056666374206543
Epoch 770, val loss: 0.8209249377250671
Epoch 780, training loss: 12.224702835083008 = 0.10941922664642334 + 2.0 * 6.057641983032227
Epoch 780, val loss: 0.8259279727935791
Epoch 790, training loss: 12.224334716796875 = 0.10437043011188507 + 2.0 * 6.0599822998046875
Epoch 790, val loss: 0.8310900926589966
Epoch 800, training loss: 12.209750175476074 = 0.09964263439178467 + 2.0 * 6.0550537109375
Epoch 800, val loss: 0.8365331888198853
Epoch 810, training loss: 12.201662063598633 = 0.0952046662569046 + 2.0 * 6.053228855133057
Epoch 810, val loss: 0.8421459794044495
Epoch 820, training loss: 12.204304695129395 = 0.09103243052959442 + 2.0 * 6.056636333465576
Epoch 820, val loss: 0.8478567004203796
Epoch 830, training loss: 12.19886302947998 = 0.08709856867790222 + 2.0 * 6.055882453918457
Epoch 830, val loss: 0.8537200093269348
Epoch 840, training loss: 12.184396743774414 = 0.08342321962118149 + 2.0 * 6.0504865646362305
Epoch 840, val loss: 0.859627902507782
Epoch 850, training loss: 12.17885971069336 = 0.0799437016248703 + 2.0 * 6.049458026885986
Epoch 850, val loss: 0.8657670617103577
Epoch 860, training loss: 12.174216270446777 = 0.07663445174694061 + 2.0 * 6.04879093170166
Epoch 860, val loss: 0.8719589710235596
Epoch 870, training loss: 12.18461799621582 = 0.07351930439472198 + 2.0 * 6.055549144744873
Epoch 870, val loss: 0.8781274557113647
Epoch 880, training loss: 12.163578987121582 = 0.07055939733982086 + 2.0 * 6.046509742736816
Epoch 880, val loss: 0.8844786286354065
Epoch 890, training loss: 12.161580085754395 = 0.06776373088359833 + 2.0 * 6.046908378601074
Epoch 890, val loss: 0.8909624218940735
Epoch 900, training loss: 12.15546703338623 = 0.06510616093873978 + 2.0 * 6.045180320739746
Epoch 900, val loss: 0.8975407481193542
Epoch 910, training loss: 12.160028457641602 = 0.0625704675912857 + 2.0 * 6.048728942871094
Epoch 910, val loss: 0.9041364789009094
Epoch 920, training loss: 12.1563081741333 = 0.060166507959365845 + 2.0 * 6.048070907592773
Epoch 920, val loss: 0.9105968475341797
Epoch 930, training loss: 12.14555549621582 = 0.05790040269494057 + 2.0 * 6.043827533721924
Epoch 930, val loss: 0.9172558784484863
Epoch 940, training loss: 12.155755996704102 = 0.05573338270187378 + 2.0 * 6.050011157989502
Epoch 940, val loss: 0.9239301681518555
Epoch 950, training loss: 12.141291618347168 = 0.05370132997632027 + 2.0 * 6.043795108795166
Epoch 950, val loss: 0.9305818676948547
Epoch 960, training loss: 12.134074211120605 = 0.05173565819859505 + 2.0 * 6.041169166564941
Epoch 960, val loss: 0.9373267889022827
Epoch 970, training loss: 12.130688667297363 = 0.04986874759197235 + 2.0 * 6.040410041809082
Epoch 970, val loss: 0.944050669670105
Epoch 980, training loss: 12.137269973754883 = 0.04809243604540825 + 2.0 * 6.044588565826416
Epoch 980, val loss: 0.9506853818893433
Epoch 990, training loss: 12.12353229522705 = 0.046395063400268555 + 2.0 * 6.038568496704102
Epoch 990, val loss: 0.9574035406112671
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7852
Overall ASR: 0.8192
Flip ASR: 0.7822/225 nodes
The final ASR:0.69988, 0.13623, Accuracy:0.80000, 0.01386
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11614])
remove edge: torch.Size([2, 9462])
updated graph: torch.Size([2, 10520])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9705
Flip ASR: 0.9644/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8481
Overall ASR: 0.9668
Flip ASR: 0.9600/225 nodes
The final ASR:0.97048, 0.00301, Accuracy:0.83333, 0.01048
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.5 der2:0.3 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 18.697755813598633 = 1.9499516487121582 + 2.0 * 8.373902320861816
Epoch 0, val loss: 1.952101469039917
Epoch 10, training loss: 18.686845779418945 = 1.9398274421691895 + 2.0 * 8.373509407043457
Epoch 10, val loss: 1.9426156282424927
Epoch 20, training loss: 18.668821334838867 = 1.9270402193069458 + 2.0 * 8.370890617370605
Epoch 20, val loss: 1.9299962520599365
Epoch 30, training loss: 18.614614486694336 = 1.9091660976409912 + 2.0 * 8.352724075317383
Epoch 30, val loss: 1.9120142459869385
Epoch 40, training loss: 18.324918746948242 = 1.8872445821762085 + 2.0 * 8.218836784362793
Epoch 40, val loss: 1.8910038471221924
Epoch 50, training loss: 17.07227897644043 = 1.8635027408599854 + 2.0 * 7.6043877601623535
Epoch 50, val loss: 1.8685529232025146
Epoch 60, training loss: 16.385154724121094 = 1.8444693088531494 + 2.0 * 7.270342826843262
Epoch 60, val loss: 1.8517022132873535
Epoch 70, training loss: 15.777059555053711 = 1.8313400745391846 + 2.0 * 6.972859859466553
Epoch 70, val loss: 1.8394240140914917
Epoch 80, training loss: 15.373650550842285 = 1.8173609972000122 + 2.0 * 6.778144836425781
Epoch 80, val loss: 1.826756477355957
Epoch 90, training loss: 15.10960865020752 = 1.8019722700119019 + 2.0 * 6.653818130493164
Epoch 90, val loss: 1.8132494688034058
Epoch 100, training loss: 14.888568878173828 = 1.7866169214248657 + 2.0 * 6.550975799560547
Epoch 100, val loss: 1.8000894784927368
Epoch 110, training loss: 14.73729133605957 = 1.7709197998046875 + 2.0 * 6.483185768127441
Epoch 110, val loss: 1.7861872911453247
Epoch 120, training loss: 14.624524116516113 = 1.7539079189300537 + 2.0 * 6.43530797958374
Epoch 120, val loss: 1.770682454109192
Epoch 130, training loss: 14.530359268188477 = 1.735107183456421 + 2.0 * 6.397625923156738
Epoch 130, val loss: 1.7534892559051514
Epoch 140, training loss: 14.452341079711914 = 1.7141079902648926 + 2.0 * 6.369116306304932
Epoch 140, val loss: 1.734670639038086
Epoch 150, training loss: 14.37736988067627 = 1.6906973123550415 + 2.0 * 6.34333610534668
Epoch 150, val loss: 1.713869571685791
Epoch 160, training loss: 14.310186386108398 = 1.6642775535583496 + 2.0 * 6.3229546546936035
Epoch 160, val loss: 1.690748691558838
Epoch 170, training loss: 14.244471549987793 = 1.6343473196029663 + 2.0 * 6.305062294006348
Epoch 170, val loss: 1.6647738218307495
Epoch 180, training loss: 14.180103302001953 = 1.6006463766098022 + 2.0 * 6.28972864151001
Epoch 180, val loss: 1.6358823776245117
Epoch 190, training loss: 14.11752700805664 = 1.563453197479248 + 2.0 * 6.277037143707275
Epoch 190, val loss: 1.604209303855896
Epoch 200, training loss: 14.05023193359375 = 1.5225820541381836 + 2.0 * 6.263824939727783
Epoch 200, val loss: 1.5698274374008179
Epoch 210, training loss: 13.983015060424805 = 1.478431224822998 + 2.0 * 6.252291679382324
Epoch 210, val loss: 1.5331170558929443
Epoch 220, training loss: 13.918885231018066 = 1.4319175481796265 + 2.0 * 6.243484020233154
Epoch 220, val loss: 1.4950926303863525
Epoch 230, training loss: 13.85358715057373 = 1.3846043348312378 + 2.0 * 6.234491348266602
Epoch 230, val loss: 1.4570685625076294
Epoch 240, training loss: 13.787101745605469 = 1.3368325233459473 + 2.0 * 6.225134372711182
Epoch 240, val loss: 1.4194444417953491
Epoch 250, training loss: 13.722187995910645 = 1.2894134521484375 + 2.0 * 6.2163872718811035
Epoch 250, val loss: 1.3828448057174683
Epoch 260, training loss: 13.661371231079102 = 1.2426613569259644 + 2.0 * 6.209354877471924
Epoch 260, val loss: 1.347508430480957
Epoch 270, training loss: 13.60679817199707 = 1.1966930627822876 + 2.0 * 6.205052375793457
Epoch 270, val loss: 1.3133419752120972
Epoch 280, training loss: 13.545979499816895 = 1.1516978740692139 + 2.0 * 6.197140693664551
Epoch 280, val loss: 1.2805086374282837
Epoch 290, training loss: 13.487754821777344 = 1.1077362298965454 + 2.0 * 6.190009117126465
Epoch 290, val loss: 1.2487714290618896
Epoch 300, training loss: 13.432321548461914 = 1.0641427040100098 + 2.0 * 6.184089183807373
Epoch 300, val loss: 1.217590093612671
Epoch 310, training loss: 13.378454208374023 = 1.0207759141921997 + 2.0 * 6.178839206695557
Epoch 310, val loss: 1.1866973638534546
Epoch 320, training loss: 13.333523750305176 = 0.978064239025116 + 2.0 * 6.177729606628418
Epoch 320, val loss: 1.1564385890960693
Epoch 330, training loss: 13.274918556213379 = 0.9365676045417786 + 2.0 * 6.169175624847412
Epoch 330, val loss: 1.1271677017211914
Epoch 340, training loss: 13.225305557250977 = 0.8963444828987122 + 2.0 * 6.164480686187744
Epoch 340, val loss: 1.0988956689834595
Epoch 350, training loss: 13.184233665466309 = 0.8573390245437622 + 2.0 * 6.163447380065918
Epoch 350, val loss: 1.0717370510101318
Epoch 360, training loss: 13.131671905517578 = 0.8197153210639954 + 2.0 * 6.155978202819824
Epoch 360, val loss: 1.04586660861969
Epoch 370, training loss: 13.086993217468262 = 0.7836949825286865 + 2.0 * 6.151648998260498
Epoch 370, val loss: 1.021288275718689
Epoch 380, training loss: 13.054336547851562 = 0.7492794990539551 + 2.0 * 6.152528285980225
Epoch 380, val loss: 0.9981100559234619
Epoch 390, training loss: 13.005267143249512 = 0.7165077924728394 + 2.0 * 6.144379615783691
Epoch 390, val loss: 0.9765592217445374
Epoch 400, training loss: 12.965517044067383 = 0.6850596070289612 + 2.0 * 6.140228748321533
Epoch 400, val loss: 0.9563990831375122
Epoch 410, training loss: 12.929695129394531 = 0.65472012758255 + 2.0 * 6.137487411499023
Epoch 410, val loss: 0.9373653531074524
Epoch 420, training loss: 12.90417194366455 = 0.6254391074180603 + 2.0 * 6.139366626739502
Epoch 420, val loss: 0.9197835326194763
Epoch 430, training loss: 12.862096786499023 = 0.5974659323692322 + 2.0 * 6.132315635681152
Epoch 430, val loss: 0.9033983945846558
Epoch 440, training loss: 12.825983047485352 = 0.5703788995742798 + 2.0 * 6.127801895141602
Epoch 440, val loss: 0.8881632089614868
Epoch 450, training loss: 12.802653312683105 = 0.5440120697021484 + 2.0 * 6.1293206214904785
Epoch 450, val loss: 0.8739308714866638
Epoch 460, training loss: 12.764594078063965 = 0.5184303522109985 + 2.0 * 6.123081684112549
Epoch 460, val loss: 0.8607677221298218
Epoch 470, training loss: 12.733098983764648 = 0.4934532046318054 + 2.0 * 6.119822978973389
Epoch 470, val loss: 0.8485515117645264
Epoch 480, training loss: 12.70987319946289 = 0.4691603183746338 + 2.0 * 6.120356559753418
Epoch 480, val loss: 0.837242066860199
Epoch 490, training loss: 12.678186416625977 = 0.44548770785331726 + 2.0 * 6.116349220275879
Epoch 490, val loss: 0.8268650770187378
Epoch 500, training loss: 12.647263526916504 = 0.4224911034107208 + 2.0 * 6.112386226654053
Epoch 500, val loss: 0.8174681663513184
Epoch 510, training loss: 12.633825302124023 = 0.4000546932220459 + 2.0 * 6.116885185241699
Epoch 510, val loss: 0.8088781237602234
Epoch 520, training loss: 12.5994873046875 = 0.3784714639186859 + 2.0 * 6.110507965087891
Epoch 520, val loss: 0.8011049032211304
Epoch 530, training loss: 12.57036018371582 = 0.35756585001945496 + 2.0 * 6.1063971519470215
Epoch 530, val loss: 0.7943567037582397
Epoch 540, training loss: 12.546370506286621 = 0.3373832106590271 + 2.0 * 6.104493618011475
Epoch 540, val loss: 0.7884488701820374
Epoch 550, training loss: 12.53065299987793 = 0.31795644760131836 + 2.0 * 6.106348514556885
Epoch 550, val loss: 0.7834228277206421
Epoch 560, training loss: 12.50296401977539 = 0.299377977848053 + 2.0 * 6.101792812347412
Epoch 560, val loss: 0.7792125940322876
Epoch 570, training loss: 12.479738235473633 = 0.2817024290561676 + 2.0 * 6.099018096923828
Epoch 570, val loss: 0.7759860754013062
Epoch 580, training loss: 12.463342666625977 = 0.2649863660335541 + 2.0 * 6.099178314208984
Epoch 580, val loss: 0.773652195930481
Epoch 590, training loss: 12.446468353271484 = 0.2492581009864807 + 2.0 * 6.098605155944824
Epoch 590, val loss: 0.7720516920089722
Epoch 600, training loss: 12.422295570373535 = 0.2345539629459381 + 2.0 * 6.093870639801025
Epoch 600, val loss: 0.7713846564292908
Epoch 610, training loss: 12.403277397155762 = 0.2208019644021988 + 2.0 * 6.091237545013428
Epoch 610, val loss: 0.7715479135513306
Epoch 620, training loss: 12.396507263183594 = 0.20795592665672302 + 2.0 * 6.09427547454834
Epoch 620, val loss: 0.7724801301956177
Epoch 630, training loss: 12.382267951965332 = 0.1959947645664215 + 2.0 * 6.093136787414551
Epoch 630, val loss: 0.7740695476531982
Epoch 640, training loss: 12.35781192779541 = 0.1849452406167984 + 2.0 * 6.086433410644531
Epoch 640, val loss: 0.7764835953712463
Epoch 650, training loss: 12.347654342651367 = 0.17468084394931793 + 2.0 * 6.08648681640625
Epoch 650, val loss: 0.7795321345329285
Epoch 660, training loss: 12.353716850280762 = 0.1651187390089035 + 2.0 * 6.094298839569092
Epoch 660, val loss: 0.7830565571784973
Epoch 670, training loss: 12.322587966918945 = 0.15630175173282623 + 2.0 * 6.08314323425293
Epoch 670, val loss: 0.7872014045715332
Epoch 680, training loss: 12.310532569885254 = 0.14810971915721893 + 2.0 * 6.081211566925049
Epoch 680, val loss: 0.7919183373451233
Epoch 690, training loss: 12.29800796508789 = 0.14045479893684387 + 2.0 * 6.0787763595581055
Epoch 690, val loss: 0.797059178352356
Epoch 700, training loss: 12.291857719421387 = 0.13330911099910736 + 2.0 * 6.0792741775512695
Epoch 700, val loss: 0.8026084899902344
Epoch 710, training loss: 12.282017707824707 = 0.1266036331653595 + 2.0 * 6.077706813812256
Epoch 710, val loss: 0.8084167242050171
Epoch 720, training loss: 12.27139663696289 = 0.12038520723581314 + 2.0 * 6.07550573348999
Epoch 720, val loss: 0.8146551251411438
Epoch 730, training loss: 12.27230453491211 = 0.11455173045396805 + 2.0 * 6.078876495361328
Epoch 730, val loss: 0.8211241364479065
Epoch 740, training loss: 12.259907722473145 = 0.1091088354587555 + 2.0 * 6.075399398803711
Epoch 740, val loss: 0.8279280066490173
Epoch 750, training loss: 12.24510669708252 = 0.10400644689798355 + 2.0 * 6.070549964904785
Epoch 750, val loss: 0.8348950147628784
Epoch 760, training loss: 12.239265441894531 = 0.09921514987945557 + 2.0 * 6.0700249671936035
Epoch 760, val loss: 0.8420566320419312
Epoch 770, training loss: 12.243529319763184 = 0.09470837563276291 + 2.0 * 6.074410438537598
Epoch 770, val loss: 0.8493018746376038
Epoch 780, training loss: 12.231874465942383 = 0.09049450606107712 + 2.0 * 6.070690155029297
Epoch 780, val loss: 0.8565990328788757
Epoch 790, training loss: 12.22024154663086 = 0.08655736595392227 + 2.0 * 6.066842079162598
Epoch 790, val loss: 0.8641312122344971
Epoch 800, training loss: 12.21232795715332 = 0.08284260332584381 + 2.0 * 6.064742565155029
Epoch 800, val loss: 0.871752917766571
Epoch 810, training loss: 12.212251663208008 = 0.0793299600481987 + 2.0 * 6.066461086273193
Epoch 810, val loss: 0.8794419169425964
Epoch 820, training loss: 12.203256607055664 = 0.07601222395896912 + 2.0 * 6.063621997833252
Epoch 820, val loss: 0.8870029449462891
Epoch 830, training loss: 12.19994831085205 = 0.0728890672326088 + 2.0 * 6.0635294914245605
Epoch 830, val loss: 0.8947502374649048
Epoch 840, training loss: 12.193732261657715 = 0.06994723528623581 + 2.0 * 6.061892509460449
Epoch 840, val loss: 0.9025046825408936
Epoch 850, training loss: 12.187498092651367 = 0.06716716289520264 + 2.0 * 6.0601654052734375
Epoch 850, val loss: 0.9102703928947449
Epoch 860, training loss: 12.192218780517578 = 0.06453283876180649 + 2.0 * 6.0638427734375
Epoch 860, val loss: 0.9180096387863159
Epoch 870, training loss: 12.180941581726074 = 0.06202969700098038 + 2.0 * 6.059455871582031
Epoch 870, val loss: 0.925654411315918
Epoch 880, training loss: 12.171831130981445 = 0.05967956781387329 + 2.0 * 6.056075572967529
Epoch 880, val loss: 0.9333882927894592
Epoch 890, training loss: 12.173873901367188 = 0.057443294674158096 + 2.0 * 6.058215141296387
Epoch 890, val loss: 0.9410921335220337
Epoch 900, training loss: 12.168135643005371 = 0.05531236529350281 + 2.0 * 6.0564117431640625
Epoch 900, val loss: 0.9486843347549438
Epoch 910, training loss: 12.162481307983398 = 0.05330827832221985 + 2.0 * 6.054586410522461
Epoch 910, val loss: 0.9563221335411072
Epoch 920, training loss: 12.1576509475708 = 0.05140077322721481 + 2.0 * 6.053124904632568
Epoch 920, val loss: 0.96397864818573
Epoch 930, training loss: 12.154969215393066 = 0.04958329722285271 + 2.0 * 6.052692890167236
Epoch 930, val loss: 0.9714853763580322
Epoch 940, training loss: 12.153728485107422 = 0.04785405844449997 + 2.0 * 6.052937030792236
Epoch 940, val loss: 0.979016125202179
Epoch 950, training loss: 12.157449722290039 = 0.046209871768951416 + 2.0 * 6.055619716644287
Epoch 950, val loss: 0.9865321516990662
Epoch 960, training loss: 12.143636703491211 = 0.044633809477090836 + 2.0 * 6.049501419067383
Epoch 960, val loss: 0.9938055872917175
Epoch 970, training loss: 12.140817642211914 = 0.04315245524048805 + 2.0 * 6.048832416534424
Epoch 970, val loss: 1.001268744468689
Epoch 980, training loss: 12.137346267700195 = 0.041730981320142746 + 2.0 * 6.047807693481445
Epoch 980, val loss: 1.0085842609405518
Epoch 990, training loss: 12.14525318145752 = 0.040371865034103394 + 2.0 * 6.052440643310547
Epoch 990, val loss: 1.0158497095108032
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7481
Overall ASR: 0.6125
Flip ASR: 0.5378/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.699647903442383 = 1.9518989324569702 + 2.0 * 8.37387466430664
Epoch 0, val loss: 1.9560024738311768
Epoch 10, training loss: 18.685726165771484 = 1.9394763708114624 + 2.0 * 8.373125076293945
Epoch 10, val loss: 1.942640781402588
Epoch 20, training loss: 18.663379669189453 = 1.9234601259231567 + 2.0 * 8.369959831237793
Epoch 20, val loss: 1.924737811088562
Epoch 30, training loss: 18.600433349609375 = 1.9015048742294312 + 2.0 * 8.349464416503906
Epoch 30, val loss: 1.9000612497329712
Epoch 40, training loss: 18.32316017150879 = 1.8758817911148071 + 2.0 * 8.223639488220215
Epoch 40, val loss: 1.8727571964263916
Epoch 50, training loss: 17.511825561523438 = 1.8513437509536743 + 2.0 * 7.8302412033081055
Epoch 50, val loss: 1.8477466106414795
Epoch 60, training loss: 16.612939834594727 = 1.8321112394332886 + 2.0 * 7.390414714813232
Epoch 60, val loss: 1.828418254852295
Epoch 70, training loss: 15.797685623168945 = 1.815363883972168 + 2.0 * 6.991160869598389
Epoch 70, val loss: 1.8102344274520874
Epoch 80, training loss: 15.340733528137207 = 1.8004552125930786 + 2.0 * 6.770139217376709
Epoch 80, val loss: 1.7948535680770874
Epoch 90, training loss: 15.056354522705078 = 1.7852768898010254 + 2.0 * 6.635538578033447
Epoch 90, val loss: 1.7797110080718994
Epoch 100, training loss: 14.859540939331055 = 1.7680447101593018 + 2.0 * 6.545748233795166
Epoch 100, val loss: 1.7631127834320068
Epoch 110, training loss: 14.706685066223145 = 1.750951886177063 + 2.0 * 6.4778666496276855
Epoch 110, val loss: 1.7469830513000488
Epoch 120, training loss: 14.585428237915039 = 1.7341960668563843 + 2.0 * 6.425616264343262
Epoch 120, val loss: 1.731453776359558
Epoch 130, training loss: 14.481672286987305 = 1.7168848514556885 + 2.0 * 6.382393836975098
Epoch 130, val loss: 1.7159315347671509
Epoch 140, training loss: 14.395591735839844 = 1.6980617046356201 + 2.0 * 6.348764896392822
Epoch 140, val loss: 1.6996393203735352
Epoch 150, training loss: 14.319024085998535 = 1.677168846130371 + 2.0 * 6.320927619934082
Epoch 150, val loss: 1.6821179389953613
Epoch 160, training loss: 14.253902435302734 = 1.6538612842559814 + 2.0 * 6.300020694732666
Epoch 160, val loss: 1.6630266904830933
Epoch 170, training loss: 14.191191673278809 = 1.628019094467163 + 2.0 * 6.281586170196533
Epoch 170, val loss: 1.6422338485717773
Epoch 180, training loss: 14.12601089477539 = 1.599412441253662 + 2.0 * 6.263299465179443
Epoch 180, val loss: 1.6196757555007935
Epoch 190, training loss: 14.06680965423584 = 1.567983627319336 + 2.0 * 6.249413013458252
Epoch 190, val loss: 1.5951530933380127
Epoch 200, training loss: 14.008747100830078 = 1.5337083339691162 + 2.0 * 6.237519264221191
Epoch 200, val loss: 1.5688693523406982
Epoch 210, training loss: 13.952302932739258 = 1.4970074892044067 + 2.0 * 6.22764778137207
Epoch 210, val loss: 1.541266918182373
Epoch 220, training loss: 13.89011287689209 = 1.4586995840072632 + 2.0 * 6.215706825256348
Epoch 220, val loss: 1.512992262840271
Epoch 230, training loss: 13.83354377746582 = 1.4193146228790283 + 2.0 * 6.2071146965026855
Epoch 230, val loss: 1.4847733974456787
Epoch 240, training loss: 13.779756546020508 = 1.3799158334732056 + 2.0 * 6.199920177459717
Epoch 240, val loss: 1.4573562145233154
Epoch 250, training loss: 13.722108840942383 = 1.341028094291687 + 2.0 * 6.190540313720703
Epoch 250, val loss: 1.4312762022018433
Epoch 260, training loss: 13.671845436096191 = 1.3029507398605347 + 2.0 * 6.184447288513184
Epoch 260, val loss: 1.4065901041030884
Epoch 270, training loss: 13.626738548278809 = 1.265882134437561 + 2.0 * 6.1804280281066895
Epoch 270, val loss: 1.3836863040924072
Epoch 280, training loss: 13.574689865112305 = 1.2302007675170898 + 2.0 * 6.172244548797607
Epoch 280, val loss: 1.3621677160263062
Epoch 290, training loss: 13.526437759399414 = 1.195379376411438 + 2.0 * 6.165529251098633
Epoch 290, val loss: 1.341799020767212
Epoch 300, training loss: 13.483542442321777 = 1.1610729694366455 + 2.0 * 6.1612348556518555
Epoch 300, val loss: 1.3221014738082886
Epoch 310, training loss: 13.450194358825684 = 1.1271716356277466 + 2.0 * 6.161511421203613
Epoch 310, val loss: 1.3028138875961304
Epoch 320, training loss: 13.39909839630127 = 1.0936044454574585 + 2.0 * 6.15274715423584
Epoch 320, val loss: 1.2838221788406372
Epoch 330, training loss: 13.354771614074707 = 1.0600855350494385 + 2.0 * 6.147343158721924
Epoch 330, val loss: 1.2648050785064697
Epoch 340, training loss: 13.314433097839355 = 1.0264455080032349 + 2.0 * 6.143993854522705
Epoch 340, val loss: 1.2456737756729126
Epoch 350, training loss: 13.272480964660645 = 0.9927022457122803 + 2.0 * 6.139889240264893
Epoch 350, val loss: 1.2263431549072266
Epoch 360, training loss: 13.233007431030273 = 0.9588260650634766 + 2.0 * 6.137090682983398
Epoch 360, val loss: 1.2069437503814697
Epoch 370, training loss: 13.201635360717773 = 0.9251874089241028 + 2.0 * 6.138224124908447
Epoch 370, val loss: 1.1873430013656616
Epoch 380, training loss: 13.154659271240234 = 0.8917536735534668 + 2.0 * 6.131453037261963
Epoch 380, val loss: 1.167838454246521
Epoch 390, training loss: 13.112051010131836 = 0.8584842085838318 + 2.0 * 6.12678337097168
Epoch 390, val loss: 1.1484872102737427
Epoch 400, training loss: 13.076498985290527 = 0.8253742456436157 + 2.0 * 6.1255621910095215
Epoch 400, val loss: 1.1291849613189697
Epoch 410, training loss: 13.039478302001953 = 0.7926514744758606 + 2.0 * 6.123413562774658
Epoch 410, val loss: 1.1099889278411865
Epoch 420, training loss: 12.997933387756348 = 0.7604433298110962 + 2.0 * 6.118744850158691
Epoch 420, val loss: 1.09114408493042
Epoch 430, training loss: 12.960976600646973 = 0.7288335561752319 + 2.0 * 6.116071701049805
Epoch 430, val loss: 1.072715163230896
Epoch 440, training loss: 12.945106506347656 = 0.6979811787605286 + 2.0 * 6.123562812805176
Epoch 440, val loss: 1.054871678352356
Epoch 450, training loss: 12.894618034362793 = 0.6680329442024231 + 2.0 * 6.113292694091797
Epoch 450, val loss: 1.037724256515503
Epoch 460, training loss: 12.857932090759277 = 0.6393325328826904 + 2.0 * 6.109299659729004
Epoch 460, val loss: 1.021531105041504
Epoch 470, training loss: 12.829922676086426 = 0.6117990612983704 + 2.0 * 6.1090617179870605
Epoch 470, val loss: 1.0063503980636597
Epoch 480, training loss: 12.798918724060059 = 0.5855173468589783 + 2.0 * 6.106700897216797
Epoch 480, val loss: 0.9922825694084167
Epoch 490, training loss: 12.766813278198242 = 0.5606780052185059 + 2.0 * 6.103067398071289
Epoch 490, val loss: 0.9793689846992493
Epoch 500, training loss: 12.738103866577148 = 0.5371154546737671 + 2.0 * 6.100494384765625
Epoch 500, val loss: 0.9676821231842041
Epoch 510, training loss: 12.72597885131836 = 0.5148590803146362 + 2.0 * 6.105559825897217
Epoch 510, val loss: 0.9571519494056702
Epoch 520, training loss: 12.693588256835938 = 0.49410977959632874 + 2.0 * 6.099739074707031
Epoch 520, val loss: 0.9477686285972595
Epoch 530, training loss: 12.665868759155273 = 0.4744933843612671 + 2.0 * 6.0956878662109375
Epoch 530, val loss: 0.93956059217453
Epoch 540, training loss: 12.640254020690918 = 0.4558495581150055 + 2.0 * 6.092202186584473
Epoch 540, val loss: 0.9323234558105469
Epoch 550, training loss: 12.618977546691895 = 0.43812620639801025 + 2.0 * 6.090425491333008
Epoch 550, val loss: 0.9259985089302063
Epoch 560, training loss: 12.62285041809082 = 0.4212093651294708 + 2.0 * 6.100820541381836
Epoch 560, val loss: 0.9203652143478394
Epoch 570, training loss: 12.582287788391113 = 0.4051409661769867 + 2.0 * 6.088573455810547
Epoch 570, val loss: 0.9154843688011169
Epoch 580, training loss: 12.560565948486328 = 0.3897474408149719 + 2.0 * 6.085409164428711
Epoch 580, val loss: 0.9112491607666016
Epoch 590, training loss: 12.557158470153809 = 0.37489184737205505 + 2.0 * 6.091133117675781
Epoch 590, val loss: 0.9076183438301086
Epoch 600, training loss: 12.5255765914917 = 0.36067062616348267 + 2.0 * 6.082452774047852
Epoch 600, val loss: 0.9045382142066956
Epoch 610, training loss: 12.506970405578613 = 0.34681734442710876 + 2.0 * 6.080076694488525
Epoch 610, val loss: 0.9019343852996826
Epoch 620, training loss: 12.492218971252441 = 0.33327993750572205 + 2.0 * 6.079469680786133
Epoch 620, val loss: 0.8997302055358887
Epoch 630, training loss: 12.4846830368042 = 0.3200766444206238 + 2.0 * 6.082303047180176
Epoch 630, val loss: 0.8980050683021545
Epoch 640, training loss: 12.462800979614258 = 0.3072994649410248 + 2.0 * 6.0777506828308105
Epoch 640, val loss: 0.8964958190917969
Epoch 650, training loss: 12.443593978881836 = 0.29484841227531433 + 2.0 * 6.0743727684021
Epoch 650, val loss: 0.8954206109046936
Epoch 660, training loss: 12.428794860839844 = 0.2827027142047882 + 2.0 * 6.0730462074279785
Epoch 660, val loss: 0.8947533369064331
Epoch 670, training loss: 12.417340278625488 = 0.2708491086959839 + 2.0 * 6.073245525360107
Epoch 670, val loss: 0.89445561170578
Epoch 680, training loss: 12.403916358947754 = 0.2593015730381012 + 2.0 * 6.072307586669922
Epoch 680, val loss: 0.8945043683052063
Epoch 690, training loss: 12.39008903503418 = 0.2480379045009613 + 2.0 * 6.071025371551514
Epoch 690, val loss: 0.8948837518692017
Epoch 700, training loss: 12.372828483581543 = 0.23710967600345612 + 2.0 * 6.067859172821045
Epoch 700, val loss: 0.8956151604652405
Epoch 710, training loss: 12.364962577819824 = 0.22650115191936493 + 2.0 * 6.069230556488037
Epoch 710, val loss: 0.8967205286026001
Epoch 720, training loss: 12.346925735473633 = 0.21624135971069336 + 2.0 * 6.065342426300049
Epoch 720, val loss: 0.8981855511665344
Epoch 730, training loss: 12.341480255126953 = 0.2063513547182083 + 2.0 * 6.067564487457275
Epoch 730, val loss: 0.8999215364456177
Epoch 740, training loss: 12.328263282775879 = 0.19677600264549255 + 2.0 * 6.065743446350098
Epoch 740, val loss: 0.9019126892089844
Epoch 750, training loss: 12.314682960510254 = 0.18764309585094452 + 2.0 * 6.0635199546813965
Epoch 750, val loss: 0.9042995572090149
Epoch 760, training loss: 12.29982852935791 = 0.17886310815811157 + 2.0 * 6.060482501983643
Epoch 760, val loss: 0.9069883823394775
Epoch 770, training loss: 12.297553062438965 = 0.17048558592796326 + 2.0 * 6.063533782958984
Epoch 770, val loss: 0.9101027250289917
Epoch 780, training loss: 12.287749290466309 = 0.16248805820941925 + 2.0 * 6.062630653381348
Epoch 780, val loss: 0.9133796691894531
Epoch 790, training loss: 12.273096084594727 = 0.15486904978752136 + 2.0 * 6.059113502502441
Epoch 790, val loss: 0.9170428514480591
Epoch 800, training loss: 12.25951862335205 = 0.14765191078186035 + 2.0 * 6.055933475494385
Epoch 800, val loss: 0.9209200739860535
Epoch 810, training loss: 12.258495330810547 = 0.14079098403453827 + 2.0 * 6.058852195739746
Epoch 810, val loss: 0.9250112771987915
Epoch 820, training loss: 12.252182960510254 = 0.13428467512130737 + 2.0 * 6.058948993682861
Epoch 820, val loss: 0.9294402003288269
Epoch 830, training loss: 12.237071990966797 = 0.12814593315124512 + 2.0 * 6.054462909698486
Epoch 830, val loss: 0.9339241981506348
Epoch 840, training loss: 12.227919578552246 = 0.12231963872909546 + 2.0 * 6.052800178527832
Epoch 840, val loss: 0.9386200308799744
Epoch 850, training loss: 12.22208309173584 = 0.1167868971824646 + 2.0 * 6.052648067474365
Epoch 850, val loss: 0.9436180591583252
Epoch 860, training loss: 12.222421646118164 = 0.11155296862125397 + 2.0 * 6.055434226989746
Epoch 860, val loss: 0.9487000703811646
Epoch 870, training loss: 12.214608192443848 = 0.10662589222192764 + 2.0 * 6.053991317749023
Epoch 870, val loss: 0.953898549079895
Epoch 880, training loss: 12.198630332946777 = 0.10196314007043839 + 2.0 * 6.048333644866943
Epoch 880, val loss: 0.9591889381408691
Epoch 890, training loss: 12.193846702575684 = 0.09755094349384308 + 2.0 * 6.048147678375244
Epoch 890, val loss: 0.9646083116531372
Epoch 900, training loss: 12.207015037536621 = 0.09337085485458374 + 2.0 * 6.056822299957275
Epoch 900, val loss: 0.9700989723205566
Epoch 910, training loss: 12.187418937683105 = 0.08939921855926514 + 2.0 * 6.049009799957275
Epoch 910, val loss: 0.9754687547683716
Epoch 920, training loss: 12.178414344787598 = 0.08565610647201538 + 2.0 * 6.046379089355469
Epoch 920, val loss: 0.9809744358062744
Epoch 930, training loss: 12.183492660522461 = 0.08210631459951401 + 2.0 * 6.050693035125732
Epoch 930, val loss: 0.9865221977233887
Epoch 940, training loss: 12.169278144836426 = 0.07872817665338516 + 2.0 * 6.0452752113342285
Epoch 940, val loss: 0.9920207858085632
Epoch 950, training loss: 12.172533988952637 = 0.0755595862865448 + 2.0 * 6.048487186431885
Epoch 950, val loss: 0.9974806904792786
Epoch 960, training loss: 12.158082008361816 = 0.07253395020961761 + 2.0 * 6.042774200439453
Epoch 960, val loss: 1.0027714967727661
Epoch 970, training loss: 12.152064323425293 = 0.06968357414007187 + 2.0 * 6.041190147399902
Epoch 970, val loss: 1.0081502199172974
Epoch 980, training loss: 12.148894309997559 = 0.06699400395154953 + 2.0 * 6.040950298309326
Epoch 980, val loss: 1.0135334730148315
Epoch 990, training loss: 12.1484375 = 0.06443099677562714 + 2.0 * 6.042003154754639
Epoch 990, val loss: 1.0187870264053345
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8037
Overall ASR: 0.3173
Flip ASR: 0.3200/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.691858291625977 = 1.9441004991531372 + 2.0 * 8.373878479003906
Epoch 0, val loss: 1.938359022140503
Epoch 10, training loss: 18.68060874938965 = 1.933782696723938 + 2.0 * 8.3734130859375
Epoch 10, val loss: 1.9273934364318848
Epoch 20, training loss: 18.6618709564209 = 1.9212876558303833 + 2.0 * 8.370291709899902
Epoch 20, val loss: 1.91371750831604
Epoch 30, training loss: 18.608531951904297 = 1.9049999713897705 + 2.0 * 8.351765632629395
Epoch 30, val loss: 1.8957282304763794
Epoch 40, training loss: 18.396297454833984 = 1.8857296705245972 + 2.0 * 8.255284309387207
Epoch 40, val loss: 1.8751132488250732
Epoch 50, training loss: 17.687421798706055 = 1.8657475709915161 + 2.0 * 7.910836696624756
Epoch 50, val loss: 1.8538527488708496
Epoch 60, training loss: 16.860136032104492 = 1.847454309463501 + 2.0 * 7.506340980529785
Epoch 60, val loss: 1.8352594375610352
Epoch 70, training loss: 15.917333602905273 = 1.8320086002349854 + 2.0 * 7.042662620544434
Epoch 70, val loss: 1.8191587924957275
Epoch 80, training loss: 15.34433364868164 = 1.818401575088501 + 2.0 * 6.762966156005859
Epoch 80, val loss: 1.8050827980041504
Epoch 90, training loss: 15.060196876525879 = 1.8036941289901733 + 2.0 * 6.628251552581787
Epoch 90, val loss: 1.7901527881622314
Epoch 100, training loss: 14.874763488769531 = 1.7863621711730957 + 2.0 * 6.544200897216797
Epoch 100, val loss: 1.7733598947525024
Epoch 110, training loss: 14.732219696044922 = 1.7687410116195679 + 2.0 * 6.481739521026611
Epoch 110, val loss: 1.7566821575164795
Epoch 120, training loss: 14.625922203063965 = 1.7513145208358765 + 2.0 * 6.4373040199279785
Epoch 120, val loss: 1.7407567501068115
Epoch 130, training loss: 14.53205394744873 = 1.73338782787323 + 2.0 * 6.3993330001831055
Epoch 130, val loss: 1.724684238433838
Epoch 140, training loss: 14.450508117675781 = 1.7141884565353394 + 2.0 * 6.368159770965576
Epoch 140, val loss: 1.707889199256897
Epoch 150, training loss: 14.379964828491211 = 1.6935787200927734 + 2.0 * 6.343193054199219
Epoch 150, val loss: 1.690204381942749
Epoch 160, training loss: 14.309088706970215 = 1.6710054874420166 + 2.0 * 6.319041728973389
Epoch 160, val loss: 1.6712661981582642
Epoch 170, training loss: 14.242236137390137 = 1.645747184753418 + 2.0 * 6.298244476318359
Epoch 170, val loss: 1.6503701210021973
Epoch 180, training loss: 14.180170059204102 = 1.617236614227295 + 2.0 * 6.281466960906982
Epoch 180, val loss: 1.6269752979278564
Epoch 190, training loss: 14.121593475341797 = 1.585254192352295 + 2.0 * 6.268169403076172
Epoch 190, val loss: 1.6006988286972046
Epoch 200, training loss: 14.056879043579102 = 1.5495121479034424 + 2.0 * 6.253683567047119
Epoch 200, val loss: 1.5714921951293945
Epoch 210, training loss: 13.992841720581055 = 1.5095741748809814 + 2.0 * 6.241633892059326
Epoch 210, val loss: 1.5388237237930298
Epoch 220, training loss: 13.927549362182617 = 1.4651930332183838 + 2.0 * 6.231178283691406
Epoch 220, val loss: 1.5025817155838013
Epoch 230, training loss: 13.863129615783691 = 1.4165496826171875 + 2.0 * 6.223289966583252
Epoch 230, val loss: 1.4630389213562012
Epoch 240, training loss: 13.79163932800293 = 1.3652634620666504 + 2.0 * 6.213188171386719
Epoch 240, val loss: 1.4211751222610474
Epoch 250, training loss: 13.723102569580078 = 1.3113232851028442 + 2.0 * 6.205889701843262
Epoch 250, val loss: 1.3773791790008545
Epoch 260, training loss: 13.654122352600098 = 1.2554033994674683 + 2.0 * 6.19935941696167
Epoch 260, val loss: 1.3321715593338013
Epoch 270, training loss: 13.592513084411621 = 1.1993732452392578 + 2.0 * 6.196569919586182
Epoch 270, val loss: 1.2866902351379395
Epoch 280, training loss: 13.517940521240234 = 1.1439194679260254 + 2.0 * 6.187010288238525
Epoch 280, val loss: 1.2420685291290283
Epoch 290, training loss: 13.451247215270996 = 1.089542031288147 + 2.0 * 6.18085241317749
Epoch 290, val loss: 1.1985197067260742
Epoch 300, training loss: 13.388981819152832 = 1.0367063283920288 + 2.0 * 6.176137924194336
Epoch 300, val loss: 1.1564700603485107
Epoch 310, training loss: 13.334745407104492 = 0.986481249332428 + 2.0 * 6.174131870269775
Epoch 310, val loss: 1.1164976358413696
Epoch 320, training loss: 13.27074146270752 = 0.9388695955276489 + 2.0 * 6.16593599319458
Epoch 320, val loss: 1.079240083694458
Epoch 330, training loss: 13.216797828674316 = 0.8938226103782654 + 2.0 * 6.161487579345703
Epoch 330, val loss: 1.044316291809082
Epoch 340, training loss: 13.174189567565918 = 0.8514249920845032 + 2.0 * 6.16138219833374
Epoch 340, val loss: 1.011742353439331
Epoch 350, training loss: 13.118876457214355 = 0.8120078444480896 + 2.0 * 6.1534342765808105
Epoch 350, val loss: 0.9819788932800293
Epoch 360, training loss: 13.074800491333008 = 0.775301456451416 + 2.0 * 6.149749755859375
Epoch 360, val loss: 0.9546531438827515
Epoch 370, training loss: 13.032278060913086 = 0.7409113645553589 + 2.0 * 6.145683288574219
Epoch 370, val loss: 0.9295173287391663
Epoch 380, training loss: 12.993424415588379 = 0.7086541652679443 + 2.0 * 6.142385005950928
Epoch 380, val loss: 0.9064730405807495
Epoch 390, training loss: 12.95745849609375 = 0.6784436106681824 + 2.0 * 6.139507293701172
Epoch 390, val loss: 0.8853048086166382
Epoch 400, training loss: 12.921196937561035 = 0.649911105632782 + 2.0 * 6.135643005371094
Epoch 400, val loss: 0.8658653497695923
Epoch 410, training loss: 12.886648178100586 = 0.6227768063545227 + 2.0 * 6.1319355964660645
Epoch 410, val loss: 0.8477963805198669
Epoch 420, training loss: 12.85610580444336 = 0.596887469291687 + 2.0 * 6.129609107971191
Epoch 420, val loss: 0.8309404253959656
Epoch 430, training loss: 12.83066463470459 = 0.5722646713256836 + 2.0 * 6.129199981689453
Epoch 430, val loss: 0.8152456879615784
Epoch 440, training loss: 12.795697212219238 = 0.5486811995506287 + 2.0 * 6.123507976531982
Epoch 440, val loss: 0.8008136749267578
Epoch 450, training loss: 12.766707420349121 = 0.5260444283485413 + 2.0 * 6.120331287384033
Epoch 450, val loss: 0.7871875762939453
Epoch 460, training loss: 12.746143341064453 = 0.504209578037262 + 2.0 * 6.120966911315918
Epoch 460, val loss: 0.7743794918060303
Epoch 470, training loss: 12.720406532287598 = 0.4830418825149536 + 2.0 * 6.118682384490967
Epoch 470, val loss: 0.7625498175621033
Epoch 480, training loss: 12.69070053100586 = 0.46279674768447876 + 2.0 * 6.113951683044434
Epoch 480, val loss: 0.7513218522071838
Epoch 490, training loss: 12.664950370788574 = 0.4432070851325989 + 2.0 * 6.1108717918396
Epoch 490, val loss: 0.7410258054733276
Epoch 500, training loss: 12.642189025878906 = 0.42422887682914734 + 2.0 * 6.108980178833008
Epoch 500, val loss: 0.731395959854126
Epoch 510, training loss: 12.624018669128418 = 0.4058358669281006 + 2.0 * 6.109091281890869
Epoch 510, val loss: 0.7224205136299133
Epoch 520, training loss: 12.59807014465332 = 0.38815513253211975 + 2.0 * 6.104957580566406
Epoch 520, val loss: 0.714062511920929
Epoch 530, training loss: 12.576033592224121 = 0.37101268768310547 + 2.0 * 6.102510452270508
Epoch 530, val loss: 0.706329882144928
Epoch 540, training loss: 12.568450927734375 = 0.3544001877307892 + 2.0 * 6.107025146484375
Epoch 540, val loss: 0.6990521550178528
Epoch 550, training loss: 12.539466857910156 = 0.3382047712802887 + 2.0 * 6.100631237030029
Epoch 550, val loss: 0.6923942565917969
Epoch 560, training loss: 12.515472412109375 = 0.32263657450675964 + 2.0 * 6.0964179039001465
Epoch 560, val loss: 0.6860789656639099
Epoch 570, training loss: 12.504109382629395 = 0.30755606293678284 + 2.0 * 6.098276615142822
Epoch 570, val loss: 0.6801403760910034
Epoch 580, training loss: 12.484319686889648 = 0.29297855496406555 + 2.0 * 6.095670700073242
Epoch 580, val loss: 0.6746528744697571
Epoch 590, training loss: 12.467672348022461 = 0.2789957523345947 + 2.0 * 6.094338417053223
Epoch 590, val loss: 0.6695626378059387
Epoch 600, training loss: 12.447161674499512 = 0.2655152380466461 + 2.0 * 6.090823173522949
Epoch 600, val loss: 0.6649560928344727
Epoch 610, training loss: 12.429760932922363 = 0.2525772452354431 + 2.0 * 6.088592052459717
Epoch 610, val loss: 0.6607200503349304
Epoch 620, training loss: 12.41972827911377 = 0.24014617502689362 + 2.0 * 6.0897908210754395
Epoch 620, val loss: 0.6569465398788452
Epoch 630, training loss: 12.400236129760742 = 0.22830988466739655 + 2.0 * 6.085963249206543
Epoch 630, val loss: 0.6534540057182312
Epoch 640, training loss: 12.385123252868652 = 0.2169552743434906 + 2.0 * 6.0840840339660645
Epoch 640, val loss: 0.6504856944084167
Epoch 650, training loss: 12.390268325805664 = 0.20616401731967926 + 2.0 * 6.092051982879639
Epoch 650, val loss: 0.6479979157447815
Epoch 660, training loss: 12.362774848937988 = 0.1958344429731369 + 2.0 * 6.083470344543457
Epoch 660, val loss: 0.6458341479301453
Epoch 670, training loss: 12.346256256103516 = 0.1860588639974594 + 2.0 * 6.080098628997803
Epoch 670, val loss: 0.6441817283630371
Epoch 680, training loss: 12.33298397064209 = 0.17673435807228088 + 2.0 * 6.078125
Epoch 680, val loss: 0.6430392861366272
Epoch 690, training loss: 12.322171211242676 = 0.16784724593162537 + 2.0 * 6.07716178894043
Epoch 690, val loss: 0.6423037052154541
Epoch 700, training loss: 12.31993293762207 = 0.15937134623527527 + 2.0 * 6.080280780792236
Epoch 700, val loss: 0.6419519782066345
Epoch 710, training loss: 12.305527687072754 = 0.15138113498687744 + 2.0 * 6.077073097229004
Epoch 710, val loss: 0.6419469714164734
Epoch 720, training loss: 12.29462718963623 = 0.14380934834480286 + 2.0 * 6.075408935546875
Epoch 720, val loss: 0.6423207521438599
Epoch 730, training loss: 12.281208992004395 = 0.13662166893482208 + 2.0 * 6.072293758392334
Epoch 730, val loss: 0.6432213187217712
Epoch 740, training loss: 12.274113655090332 = 0.12980861961841583 + 2.0 * 6.072152614593506
Epoch 740, val loss: 0.6444510221481323
Epoch 750, training loss: 12.264094352722168 = 0.1233593001961708 + 2.0 * 6.070367336273193
Epoch 750, val loss: 0.6460508108139038
Epoch 760, training loss: 12.254920959472656 = 0.11725982278585434 + 2.0 * 6.068830490112305
Epoch 760, val loss: 0.6480495929718018
Epoch 770, training loss: 12.254650115966797 = 0.11147608608007431 + 2.0 * 6.071587085723877
Epoch 770, val loss: 0.6503053307533264
Epoch 780, training loss: 12.241657257080078 = 0.10606198012828827 + 2.0 * 6.067797660827637
Epoch 780, val loss: 0.6527602672576904
Epoch 790, training loss: 12.230616569519043 = 0.10092948377132416 + 2.0 * 6.064843654632568
Epoch 790, val loss: 0.655540406703949
Epoch 800, training loss: 12.224188804626465 = 0.09609968215227127 + 2.0 * 6.06404447555542
Epoch 800, val loss: 0.6585594415664673
Epoch 810, training loss: 12.22021484375 = 0.09155584871768951 + 2.0 * 6.064329624176025
Epoch 810, val loss: 0.6617245078086853
Epoch 820, training loss: 12.21613597869873 = 0.08727094531059265 + 2.0 * 6.064432621002197
Epoch 820, val loss: 0.6650590896606445
Epoch 830, training loss: 12.204895973205566 = 0.08324449509382248 + 2.0 * 6.060825824737549
Epoch 830, val loss: 0.6684740781784058
Epoch 840, training loss: 12.196508407592773 = 0.07947017252445221 + 2.0 * 6.058518886566162
Epoch 840, val loss: 0.6720643639564514
Epoch 850, training loss: 12.200878143310547 = 0.07590613514184952 + 2.0 * 6.062486171722412
Epoch 850, val loss: 0.6757892370223999
Epoch 860, training loss: 12.198359489440918 = 0.07254128903150558 + 2.0 * 6.062909126281738
Epoch 860, val loss: 0.6794824600219727
Epoch 870, training loss: 12.18166446685791 = 0.0693979561328888 + 2.0 * 6.056133270263672
Epoch 870, val loss: 0.6832563281059265
Epoch 880, training loss: 12.177321434020996 = 0.06642205268144608 + 2.0 * 6.055449485778809
Epoch 880, val loss: 0.6871565580368042
Epoch 890, training loss: 12.189452171325684 = 0.06360871344804764 + 2.0 * 6.062921524047852
Epoch 890, val loss: 0.6911516189575195
Epoch 900, training loss: 12.174182891845703 = 0.06099611520767212 + 2.0 * 6.056593418121338
Epoch 900, val loss: 0.6950007081031799
Epoch 910, training loss: 12.162788391113281 = 0.05849334970116615 + 2.0 * 6.052147388458252
Epoch 910, val loss: 0.699032187461853
Epoch 920, training loss: 12.158589363098145 = 0.056147556751966476 + 2.0 * 6.051220893859863
Epoch 920, val loss: 0.7030879855155945
Epoch 930, training loss: 12.17020034790039 = 0.05393648147583008 + 2.0 * 6.058132171630859
Epoch 930, val loss: 0.7071436047554016
Epoch 940, training loss: 12.155534744262695 = 0.05181169509887695 + 2.0 * 6.05186128616333
Epoch 940, val loss: 0.7111204862594604
Epoch 950, training loss: 12.155160903930664 = 0.049831029027700424 + 2.0 * 6.052664756774902
Epoch 950, val loss: 0.7151274681091309
Epoch 960, training loss: 12.148083686828613 = 0.04794763773679733 + 2.0 * 6.050067901611328
Epoch 960, val loss: 0.7191976308822632
Epoch 970, training loss: 12.141446113586426 = 0.04617641493678093 + 2.0 * 6.047635078430176
Epoch 970, val loss: 0.7231900691986084
Epoch 980, training loss: 12.137218475341797 = 0.04448551684617996 + 2.0 * 6.0463666915893555
Epoch 980, val loss: 0.7272272109985352
Epoch 990, training loss: 12.139395713806152 = 0.04288588464260101 + 2.0 * 6.04825496673584
Epoch 990, val loss: 0.7312605977058411
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8000
Overall ASR: 0.1882
Flip ASR: 0.2133/225 nodes
The final ASR:0.37269, 0.17761, Accuracy:0.78395, 0.02536
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11606])
remove edge: torch.Size([2, 9518])
updated graph: torch.Size([2, 10568])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
The final ASR:0.97909, 0.00348, Accuracy:0.83210, 0.00761
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.5 der2:0.4 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.4, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 18.693817138671875 = 1.9463204145431519 + 2.0 * 8.373748779296875
Epoch 0, val loss: 1.9528858661651611
Epoch 10, training loss: 18.68299102783203 = 1.9373607635498047 + 2.0 * 8.372815132141113
Epoch 10, val loss: 1.9440522193908691
Epoch 20, training loss: 18.660598754882812 = 1.9263842105865479 + 2.0 * 8.367107391357422
Epoch 20, val loss: 1.9331436157226562
Epoch 30, training loss: 18.569005966186523 = 1.911967158317566 + 2.0 * 8.328519821166992
Epoch 30, val loss: 1.9187780618667603
Epoch 40, training loss: 17.98756980895996 = 1.8958178758621216 + 2.0 * 8.045875549316406
Epoch 40, val loss: 1.9025311470031738
Epoch 50, training loss: 16.362226486206055 = 1.8769763708114624 + 2.0 * 7.2426252365112305
Epoch 50, val loss: 1.8835830688476562
Epoch 60, training loss: 15.55780029296875 = 1.864088773727417 + 2.0 * 6.846855640411377
Epoch 60, val loss: 1.8711228370666504
Epoch 70, training loss: 15.172882080078125 = 1.8538373708724976 + 2.0 * 6.659522533416748
Epoch 70, val loss: 1.8599343299865723
Epoch 80, training loss: 14.948859214782715 = 1.8431072235107422 + 2.0 * 6.552875995635986
Epoch 80, val loss: 1.8488969802856445
Epoch 90, training loss: 14.791759490966797 = 1.833385944366455 + 2.0 * 6.47918701171875
Epoch 90, val loss: 1.8383395671844482
Epoch 100, training loss: 14.673656463623047 = 1.8241013288497925 + 2.0 * 6.424777507781982
Epoch 100, val loss: 1.828277587890625
Epoch 110, training loss: 14.57014274597168 = 1.8156161308288574 + 2.0 * 6.377263069152832
Epoch 110, val loss: 1.818802833557129
Epoch 120, training loss: 14.485321044921875 = 1.8079771995544434 + 2.0 * 6.338672161102295
Epoch 120, val loss: 1.810417890548706
Epoch 130, training loss: 14.415142059326172 = 1.8005990982055664 + 2.0 * 6.307271480560303
Epoch 130, val loss: 1.8025871515274048
Epoch 140, training loss: 14.354574203491211 = 1.7934398651123047 + 2.0 * 6.280567169189453
Epoch 140, val loss: 1.7952332496643066
Epoch 150, training loss: 14.300676345825195 = 1.786365032196045 + 2.0 * 6.257155418395996
Epoch 150, val loss: 1.7882448434829712
Epoch 160, training loss: 14.253534317016602 = 1.7791122198104858 + 2.0 * 6.237211227416992
Epoch 160, val loss: 1.7813366651535034
Epoch 170, training loss: 14.210921287536621 = 1.77134108543396 + 2.0 * 6.219789981842041
Epoch 170, val loss: 1.7743170261383057
Epoch 180, training loss: 14.17109203338623 = 1.7628802061080933 + 2.0 * 6.204105854034424
Epoch 180, val loss: 1.7668116092681885
Epoch 190, training loss: 14.134857177734375 = 1.7534300088882446 + 2.0 * 6.190713405609131
Epoch 190, val loss: 1.7586270570755005
Epoch 200, training loss: 14.108125686645508 = 1.742687463760376 + 2.0 * 6.1827192306518555
Epoch 200, val loss: 1.7494640350341797
Epoch 210, training loss: 14.069831848144531 = 1.7304569482803345 + 2.0 * 6.169687271118164
Epoch 210, val loss: 1.7392370700836182
Epoch 220, training loss: 14.038639068603516 = 1.716497778892517 + 2.0 * 6.161070823669434
Epoch 220, val loss: 1.7277404069900513
Epoch 230, training loss: 14.006365776062012 = 1.7003203630447388 + 2.0 * 6.153022766113281
Epoch 230, val loss: 1.7144500017166138
Epoch 240, training loss: 13.97517204284668 = 1.6813812255859375 + 2.0 * 6.146895408630371
Epoch 240, val loss: 1.699009656906128
Epoch 250, training loss: 13.940242767333984 = 1.659186840057373 + 2.0 * 6.140527725219727
Epoch 250, val loss: 1.6811107397079468
Epoch 260, training loss: 13.901410102844238 = 1.6333465576171875 + 2.0 * 6.134031772613525
Epoch 260, val loss: 1.6602413654327393
Epoch 270, training loss: 13.860342025756836 = 1.6029688119888306 + 2.0 * 6.128686428070068
Epoch 270, val loss: 1.6357269287109375
Epoch 280, training loss: 13.82467269897461 = 1.5674313306808472 + 2.0 * 6.128620624542236
Epoch 280, val loss: 1.6069897413253784
Epoch 290, training loss: 13.768617630004883 = 1.526504635810852 + 2.0 * 6.12105655670166
Epoch 290, val loss: 1.5740599632263184
Epoch 300, training loss: 13.712068557739258 = 1.4796295166015625 + 2.0 * 6.116219520568848
Epoch 300, val loss: 1.5361006259918213
Epoch 310, training loss: 13.660036087036133 = 1.426938772201538 + 2.0 * 6.116548538208008
Epoch 310, val loss: 1.4933148622512817
Epoch 320, training loss: 13.5911226272583 = 1.3699662685394287 + 2.0 * 6.1105780601501465
Epoch 320, val loss: 1.4471235275268555
Epoch 330, training loss: 13.524128913879395 = 1.3097975254058838 + 2.0 * 6.107165813446045
Epoch 330, val loss: 1.3982539176940918
Epoch 340, training loss: 13.457058906555176 = 1.2479051351547241 + 2.0 * 6.10457706451416
Epoch 340, val loss: 1.3480390310287476
Epoch 350, training loss: 13.396737098693848 = 1.1870628595352173 + 2.0 * 6.104836940765381
Epoch 350, val loss: 1.29871666431427
Epoch 360, training loss: 13.329606056213379 = 1.129088282585144 + 2.0 * 6.100258827209473
Epoch 360, val loss: 1.25204598903656
Epoch 370, training loss: 13.2681303024292 = 1.0744800567626953 + 2.0 * 6.096825122833252
Epoch 370, val loss: 1.2083158493041992
Epoch 380, training loss: 13.220805168151855 = 1.0238088369369507 + 2.0 * 6.098498344421387
Epoch 380, val loss: 1.168310523033142
Epoch 390, training loss: 13.16278076171875 = 0.9779695272445679 + 2.0 * 6.092405796051025
Epoch 390, val loss: 1.132536768913269
Epoch 400, training loss: 13.115080833435059 = 0.9356987476348877 + 2.0 * 6.089691162109375
Epoch 400, val loss: 1.1000466346740723
Epoch 410, training loss: 13.070418357849121 = 0.8961980938911438 + 2.0 * 6.0871100425720215
Epoch 410, val loss: 1.0702675580978394
Epoch 420, training loss: 13.042176246643066 = 0.8590725660324097 + 2.0 * 6.091551780700684
Epoch 420, val loss: 1.0427552461624146
Epoch 430, training loss: 12.995059967041016 = 0.8246760368347168 + 2.0 * 6.08519172668457
Epoch 430, val loss: 1.0179815292358398
Epoch 440, training loss: 12.955617904663086 = 0.7925381660461426 + 2.0 * 6.081540107727051
Epoch 440, val loss: 0.9951436519622803
Epoch 450, training loss: 12.919719696044922 = 0.7618657946586609 + 2.0 * 6.078927040100098
Epoch 450, val loss: 0.9737363457679749
Epoch 460, training loss: 12.88684368133545 = 0.7324215769767761 + 2.0 * 6.077210903167725
Epoch 460, val loss: 0.9536051154136658
Epoch 470, training loss: 12.862326622009277 = 0.704272449016571 + 2.0 * 6.07902717590332
Epoch 470, val loss: 0.9346593618392944
Epoch 480, training loss: 12.828259468078613 = 0.6775720715522766 + 2.0 * 6.075343608856201
Epoch 480, val loss: 0.9169981479644775
Epoch 490, training loss: 12.795302391052246 = 0.651839017868042 + 2.0 * 6.0717315673828125
Epoch 490, val loss: 0.900259256362915
Epoch 500, training loss: 12.767017364501953 = 0.6268255114555359 + 2.0 * 6.070096015930176
Epoch 500, val loss: 0.8842434883117676
Epoch 510, training loss: 12.751594543457031 = 0.6025080680847168 + 2.0 * 6.074542999267578
Epoch 510, val loss: 0.8688933253288269
Epoch 520, training loss: 12.716032028198242 = 0.5790797472000122 + 2.0 * 6.06847620010376
Epoch 520, val loss: 0.8543698787689209
Epoch 530, training loss: 12.688702583312988 = 0.5564868450164795 + 2.0 * 6.066107749938965
Epoch 530, val loss: 0.8406734466552734
Epoch 540, training loss: 12.663393020629883 = 0.5344023704528809 + 2.0 * 6.06449556350708
Epoch 540, val loss: 0.8274864554405212
Epoch 550, training loss: 12.638147354125977 = 0.5127155184745789 + 2.0 * 6.062716007232666
Epoch 550, val loss: 0.8147284388542175
Epoch 560, training loss: 12.616106986999512 = 0.49131813645362854 + 2.0 * 6.062394618988037
Epoch 560, val loss: 0.8023999333381653
Epoch 570, training loss: 12.59506607055664 = 0.47023048996925354 + 2.0 * 6.062417984008789
Epoch 570, val loss: 0.7905458807945251
Epoch 580, training loss: 12.570000648498535 = 0.44970962405204773 + 2.0 * 6.060145378112793
Epoch 580, val loss: 0.7793192267417908
Epoch 590, training loss: 12.546391487121582 = 0.42948710918426514 + 2.0 * 6.058452129364014
Epoch 590, val loss: 0.7686252593994141
Epoch 600, training loss: 12.522631645202637 = 0.4094368815422058 + 2.0 * 6.0565972328186035
Epoch 600, val loss: 0.7583575248718262
Epoch 610, training loss: 12.499585151672363 = 0.38946375250816345 + 2.0 * 6.055060863494873
Epoch 610, val loss: 0.7486632466316223
Epoch 620, training loss: 12.487040519714355 = 0.36962971091270447 + 2.0 * 6.0587053298950195
Epoch 620, val loss: 0.7394809126853943
Epoch 630, training loss: 12.464858055114746 = 0.35024866461753845 + 2.0 * 6.057304859161377
Epoch 630, val loss: 0.7308945059776306
Epoch 640, training loss: 12.437326431274414 = 0.33144453167915344 + 2.0 * 6.052940845489502
Epoch 640, val loss: 0.7232415080070496
Epoch 650, training loss: 12.42028522491455 = 0.31309667229652405 + 2.0 * 6.05359411239624
Epoch 650, val loss: 0.7163182497024536
Epoch 660, training loss: 12.401102066040039 = 0.2953680455684662 + 2.0 * 6.0528669357299805
Epoch 660, val loss: 0.7100876569747925
Epoch 670, training loss: 12.376538276672363 = 0.2784566879272461 + 2.0 * 6.049040794372559
Epoch 670, val loss: 0.7048372626304626
Epoch 680, training loss: 12.35791301727295 = 0.26229727268218994 + 2.0 * 6.047807693481445
Epoch 680, val loss: 0.7003902196884155
Epoch 690, training loss: 12.35004711151123 = 0.24699054658412933 + 2.0 * 6.051528453826904
Epoch 690, val loss: 0.6967318058013916
Epoch 700, training loss: 12.332635879516602 = 0.23266880214214325 + 2.0 * 6.049983501434326
Epoch 700, val loss: 0.6939671039581299
Epoch 710, training loss: 12.312276840209961 = 0.21933233737945557 + 2.0 * 6.046472072601318
Epoch 710, val loss: 0.6920410394668579
Epoch 720, training loss: 12.294716835021973 = 0.206974059343338 + 2.0 * 6.0438714027404785
Epoch 720, val loss: 0.6908982396125793
Epoch 730, training loss: 12.286349296569824 = 0.19544072449207306 + 2.0 * 6.045454502105713
Epoch 730, val loss: 0.6904420256614685
Epoch 740, training loss: 12.279589653015137 = 0.18485131859779358 + 2.0 * 6.047369003295898
Epoch 740, val loss: 0.6905419826507568
Epoch 750, training loss: 12.259477615356445 = 0.17511586844921112 + 2.0 * 6.042181015014648
Epoch 750, val loss: 0.6914101243019104
Epoch 760, training loss: 12.246171951293945 = 0.16608072817325592 + 2.0 * 6.040045738220215
Epoch 760, val loss: 0.6927851438522339
Epoch 770, training loss: 12.245748519897461 = 0.15767468512058258 + 2.0 * 6.044036865234375
Epoch 770, val loss: 0.6945708394050598
Epoch 780, training loss: 12.233112335205078 = 0.14993621408939362 + 2.0 * 6.041587829589844
Epoch 780, val loss: 0.696777880191803
Epoch 790, training loss: 12.223611831665039 = 0.14275237917900085 + 2.0 * 6.040429592132568
Epoch 790, val loss: 0.6994171142578125
Epoch 800, training loss: 12.209716796875 = 0.13605719804763794 + 2.0 * 6.036829948425293
Epoch 800, val loss: 0.7023243308067322
Epoch 810, training loss: 12.200066566467285 = 0.12982189655303955 + 2.0 * 6.035122394561768
Epoch 810, val loss: 0.7055606842041016
Epoch 820, training loss: 12.191947937011719 = 0.123959481716156 + 2.0 * 6.033994197845459
Epoch 820, val loss: 0.7090812921524048
Epoch 830, training loss: 12.216952323913574 = 0.11846553534269333 + 2.0 * 6.049243450164795
Epoch 830, val loss: 0.7126966714859009
Epoch 840, training loss: 12.189352035522461 = 0.1133936271071434 + 2.0 * 6.0379791259765625
Epoch 840, val loss: 0.7164626121520996
Epoch 850, training loss: 12.175990104675293 = 0.10864816606044769 + 2.0 * 6.033670902252197
Epoch 850, val loss: 0.7206005454063416
Epoch 860, training loss: 12.167413711547852 = 0.10415180772542953 + 2.0 * 6.031630992889404
Epoch 860, val loss: 0.7247769832611084
Epoch 870, training loss: 12.172492980957031 = 0.09990334510803223 + 2.0 * 6.036294937133789
Epoch 870, val loss: 0.7290999889373779
Epoch 880, training loss: 12.160195350646973 = 0.0958825871348381 + 2.0 * 6.032156467437744
Epoch 880, val loss: 0.7334626913070679
Epoch 890, training loss: 12.15145206451416 = 0.09211435914039612 + 2.0 * 6.029668807983398
Epoch 890, val loss: 0.7380590438842773
Epoch 900, training loss: 12.158124923706055 = 0.08852314203977585 + 2.0 * 6.034801006317139
Epoch 900, val loss: 0.7426685094833374
Epoch 910, training loss: 12.147650718688965 = 0.08514352142810822 + 2.0 * 6.031253814697266
Epoch 910, val loss: 0.747253954410553
Epoch 920, training loss: 12.136092185974121 = 0.08195234090089798 + 2.0 * 6.027070045471191
Epoch 920, val loss: 0.7520402669906616
Epoch 930, training loss: 12.131464958190918 = 0.07890995591878891 + 2.0 * 6.026277542114258
Epoch 930, val loss: 0.7568025588989258
Epoch 940, training loss: 12.137003898620605 = 0.07600194215774536 + 2.0 * 6.030500888824463
Epoch 940, val loss: 0.7616100311279297
Epoch 950, training loss: 12.130629539489746 = 0.07327154278755188 + 2.0 * 6.028678894042969
Epoch 950, val loss: 0.7663666605949402
Epoch 960, training loss: 12.118470191955566 = 0.07067067176103592 + 2.0 * 6.023899555206299
Epoch 960, val loss: 0.7712299227714539
Epoch 970, training loss: 12.116008758544922 = 0.06818433105945587 + 2.0 * 6.02391242980957
Epoch 970, val loss: 0.776072084903717
Epoch 980, training loss: 12.119894027709961 = 0.06580839306116104 + 2.0 * 6.027042865753174
Epoch 980, val loss: 0.7809168100357056
Epoch 990, training loss: 12.113948822021484 = 0.0635656788945198 + 2.0 * 6.025191783905029
Epoch 990, val loss: 0.7856545448303223
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.7048
Flip ASR: 0.6444/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.695703506469727 = 1.948125958442688 + 2.0 * 8.373788833618164
Epoch 0, val loss: 1.944330096244812
Epoch 10, training loss: 18.683237075805664 = 1.937147617340088 + 2.0 * 8.373044967651367
Epoch 10, val loss: 1.933634877204895
Epoch 20, training loss: 18.660703659057617 = 1.9236186742782593 + 2.0 * 8.368542671203613
Epoch 20, val loss: 1.9202674627304077
Epoch 30, training loss: 18.583145141601562 = 1.9058122634887695 + 2.0 * 8.338665962219238
Epoch 30, val loss: 1.9026647806167603
Epoch 40, training loss: 18.12348175048828 = 1.8856507539749146 + 2.0 * 8.118915557861328
Epoch 40, val loss: 1.8833858966827393
Epoch 50, training loss: 16.540950775146484 = 1.8644434213638306 + 2.0 * 7.338253498077393
Epoch 50, val loss: 1.8634008169174194
Epoch 60, training loss: 15.734567642211914 = 1.8518474102020264 + 2.0 * 6.941359996795654
Epoch 60, val loss: 1.8512827157974243
Epoch 70, training loss: 15.245052337646484 = 1.841454267501831 + 2.0 * 6.701798915863037
Epoch 70, val loss: 1.8405883312225342
Epoch 80, training loss: 14.98901653289795 = 1.8319333791732788 + 2.0 * 6.5785417556762695
Epoch 80, val loss: 1.830649495124817
Epoch 90, training loss: 14.812087059020996 = 1.8219856023788452 + 2.0 * 6.49505090713501
Epoch 90, val loss: 1.8203428983688354
Epoch 100, training loss: 14.66579532623291 = 1.8131572008132935 + 2.0 * 6.426319122314453
Epoch 100, val loss: 1.8109290599822998
Epoch 110, training loss: 14.549324989318848 = 1.8053005933761597 + 2.0 * 6.372012138366699
Epoch 110, val loss: 1.802558183670044
Epoch 120, training loss: 14.460275650024414 = 1.79793119430542 + 2.0 * 6.331172466278076
Epoch 120, val loss: 1.7945436239242554
Epoch 130, training loss: 14.390582084655762 = 1.7904912233352661 + 2.0 * 6.300045490264893
Epoch 130, val loss: 1.786417007446289
Epoch 140, training loss: 14.331681251525879 = 1.782757043838501 + 2.0 * 6.2744622230529785
Epoch 140, val loss: 1.7782713174819946
Epoch 150, training loss: 14.282998085021973 = 1.7747337818145752 + 2.0 * 6.254132270812988
Epoch 150, val loss: 1.7701349258422852
Epoch 160, training loss: 14.234698295593262 = 1.7662137746810913 + 2.0 * 6.2342424392700195
Epoch 160, val loss: 1.7618769407272339
Epoch 170, training loss: 14.19301986694336 = 1.7568773031234741 + 2.0 * 6.218071460723877
Epoch 170, val loss: 1.7532258033752441
Epoch 180, training loss: 14.155351638793945 = 1.7463960647583008 + 2.0 * 6.204477787017822
Epoch 180, val loss: 1.7438335418701172
Epoch 190, training loss: 14.118040084838867 = 1.7344449758529663 + 2.0 * 6.191797733306885
Epoch 190, val loss: 1.7334182262420654
Epoch 200, training loss: 14.08275318145752 = 1.7206313610076904 + 2.0 * 6.181060791015625
Epoch 200, val loss: 1.7215845584869385
Epoch 210, training loss: 14.046836853027344 = 1.7044644355773926 + 2.0 * 6.171186447143555
Epoch 210, val loss: 1.707924723625183
Epoch 220, training loss: 14.011348724365234 = 1.685572862625122 + 2.0 * 6.162888050079346
Epoch 220, val loss: 1.6920530796051025
Epoch 230, training loss: 13.974333763122559 = 1.6634548902511597 + 2.0 * 6.155439376831055
Epoch 230, val loss: 1.6738141775131226
Epoch 240, training loss: 13.933614730834961 = 1.6377015113830566 + 2.0 * 6.147956371307373
Epoch 240, val loss: 1.6525309085845947
Epoch 250, training loss: 13.890609741210938 = 1.6074323654174805 + 2.0 * 6.1415886878967285
Epoch 250, val loss: 1.627591848373413
Epoch 260, training loss: 13.848531723022461 = 1.5718786716461182 + 2.0 * 6.138326644897461
Epoch 260, val loss: 1.5984448194503784
Epoch 270, training loss: 13.79458236694336 = 1.5313036441802979 + 2.0 * 6.13163948059082
Epoch 270, val loss: 1.5651648044586182
Epoch 280, training loss: 13.742058753967285 = 1.4856414794921875 + 2.0 * 6.128208637237549
Epoch 280, val loss: 1.5277221202850342
Epoch 290, training loss: 13.681859016418457 = 1.4348175525665283 + 2.0 * 6.123520851135254
Epoch 290, val loss: 1.4861361980438232
Epoch 300, training loss: 13.619512557983398 = 1.3793635368347168 + 2.0 * 6.120074272155762
Epoch 300, val loss: 1.4409440755844116
Epoch 310, training loss: 13.558479309082031 = 1.3207428455352783 + 2.0 * 6.118868350982666
Epoch 310, val loss: 1.393558144569397
Epoch 320, training loss: 13.492785453796387 = 1.2614619731903076 + 2.0 * 6.11566162109375
Epoch 320, val loss: 1.3455339670181274
Epoch 330, training loss: 13.423927307128906 = 1.2018049955368042 + 2.0 * 6.111061096191406
Epoch 330, val loss: 1.297669768333435
Epoch 340, training loss: 13.363809585571289 = 1.1424213647842407 + 2.0 * 6.11069393157959
Epoch 340, val loss: 1.2502260208129883
Epoch 350, training loss: 13.295573234558105 = 1.0847346782684326 + 2.0 * 6.105419158935547
Epoch 350, val loss: 1.204753041267395
Epoch 360, training loss: 13.2337064743042 = 1.029237151145935 + 2.0 * 6.102234840393066
Epoch 360, val loss: 1.1611303091049194
Epoch 370, training loss: 13.179357528686523 = 0.9758995175361633 + 2.0 * 6.101728916168213
Epoch 370, val loss: 1.1193546056747437
Epoch 380, training loss: 13.124974250793457 = 0.9257285594940186 + 2.0 * 6.09962272644043
Epoch 380, val loss: 1.0805922746658325
Epoch 390, training loss: 13.06865119934082 = 0.879247784614563 + 2.0 * 6.094701766967773
Epoch 390, val loss: 1.0448487997055054
Epoch 400, training loss: 13.020524978637695 = 0.8360582590103149 + 2.0 * 6.092233180999756
Epoch 400, val loss: 1.0119987726211548
Epoch 410, training loss: 12.980388641357422 = 0.7961911559104919 + 2.0 * 6.092098712921143
Epoch 410, val loss: 0.982340395450592
Epoch 420, training loss: 12.935400009155273 = 0.7600835561752319 + 2.0 * 6.087658405303955
Epoch 420, val loss: 0.9560307264328003
Epoch 430, training loss: 12.895720481872559 = 0.7269937992095947 + 2.0 * 6.0843634605407715
Epoch 430, val loss: 0.9326269626617432
Epoch 440, training loss: 12.871615409851074 = 0.696578860282898 + 2.0 * 6.087518215179443
Epoch 440, val loss: 0.9118230938911438
Epoch 450, training loss: 12.828638076782227 = 0.6686594486236572 + 2.0 * 6.079989433288574
Epoch 450, val loss: 0.8937212824821472
Epoch 460, training loss: 12.802807807922363 = 0.6429428458213806 + 2.0 * 6.079932689666748
Epoch 460, val loss: 0.8777707815170288
Epoch 470, training loss: 12.775964736938477 = 0.6190078258514404 + 2.0 * 6.0784783363342285
Epoch 470, val loss: 0.8637886047363281
Epoch 480, training loss: 12.743471145629883 = 0.5966699719429016 + 2.0 * 6.073400497436523
Epoch 480, val loss: 0.8513585329055786
Epoch 490, training loss: 12.718896865844727 = 0.5754783153533936 + 2.0 * 6.071709156036377
Epoch 490, val loss: 0.8401200175285339
Epoch 500, training loss: 12.698277473449707 = 0.5552550554275513 + 2.0 * 6.071511268615723
Epoch 500, val loss: 0.8299285173416138
Epoch 510, training loss: 12.674620628356934 = 0.5360468029975891 + 2.0 * 6.069286823272705
Epoch 510, val loss: 0.8206723928451538
Epoch 520, training loss: 12.648880004882812 = 0.5174885988235474 + 2.0 * 6.065695762634277
Epoch 520, val loss: 0.8121941685676575
Epoch 530, training loss: 12.639571189880371 = 0.4994986951351166 + 2.0 * 6.0700364112854
Epoch 530, val loss: 0.8043524026870728
Epoch 540, training loss: 12.610386848449707 = 0.48209646344184875 + 2.0 * 6.064145088195801
Epoch 540, val loss: 0.7971656918525696
Epoch 550, training loss: 12.589300155639648 = 0.46528467535972595 + 2.0 * 6.062007904052734
Epoch 550, val loss: 0.7905267477035522
Epoch 560, training loss: 12.569683074951172 = 0.4488111436367035 + 2.0 * 6.060435771942139
Epoch 560, val loss: 0.784386932849884
Epoch 570, training loss: 12.563950538635254 = 0.4326418340206146 + 2.0 * 6.065654277801514
Epoch 570, val loss: 0.7786463499069214
Epoch 580, training loss: 12.533357620239258 = 0.4168100655078888 + 2.0 * 6.058273792266846
Epoch 580, val loss: 0.7734606266021729
Epoch 590, training loss: 12.516077041625977 = 0.40140071511268616 + 2.0 * 6.057338237762451
Epoch 590, val loss: 0.7686323523521423
Epoch 600, training loss: 12.502761840820312 = 0.3862834572792053 + 2.0 * 6.058238983154297
Epoch 600, val loss: 0.764229953289032
Epoch 610, training loss: 12.480563163757324 = 0.37159454822540283 + 2.0 * 6.0544843673706055
Epoch 610, val loss: 0.7602031230926514
Epoch 620, training loss: 12.463157653808594 = 0.3571992814540863 + 2.0 * 6.052978992462158
Epoch 620, val loss: 0.7566207051277161
Epoch 630, training loss: 12.454201698303223 = 0.3431358337402344 + 2.0 * 6.055532932281494
Epoch 630, val loss: 0.7534345984458923
Epoch 640, training loss: 12.440471649169922 = 0.32949379086494446 + 2.0 * 6.0554890632629395
Epoch 640, val loss: 0.7506529092788696
Epoch 650, training loss: 12.418930053710938 = 0.3163772523403168 + 2.0 * 6.051276206970215
Epoch 650, val loss: 0.7483533024787903
Epoch 660, training loss: 12.401198387145996 = 0.3036203980445862 + 2.0 * 6.048789024353027
Epoch 660, val loss: 0.74656742811203
Epoch 670, training loss: 12.392407417297363 = 0.29124248027801514 + 2.0 * 6.050582408905029
Epoch 670, val loss: 0.7451896667480469
Epoch 680, training loss: 12.386350631713867 = 0.2792801856994629 + 2.0 * 6.053535461425781
Epoch 680, val loss: 0.7442947030067444
Epoch 690, training loss: 12.364158630371094 = 0.26784181594848633 + 2.0 * 6.048158168792725
Epoch 690, val loss: 0.7438194751739502
Epoch 700, training loss: 12.350484848022461 = 0.2568502724170685 + 2.0 * 6.046817302703857
Epoch 700, val loss: 0.7438037991523743
Epoch 710, training loss: 12.335392951965332 = 0.24619022011756897 + 2.0 * 6.0446014404296875
Epoch 710, val loss: 0.7442181706428528
Epoch 720, training loss: 12.32443618774414 = 0.2358904331922531 + 2.0 * 6.0442728996276855
Epoch 720, val loss: 0.745025098323822
Epoch 730, training loss: 12.319453239440918 = 0.22596655786037445 + 2.0 * 6.046743392944336
Epoch 730, val loss: 0.7462010383605957
Epoch 740, training loss: 12.30368423461914 = 0.2165154665708542 + 2.0 * 6.04358434677124
Epoch 740, val loss: 0.7477381229400635
Epoch 750, training loss: 12.292762756347656 = 0.20744287967681885 + 2.0 * 6.042659759521484
Epoch 750, val loss: 0.7496737837791443
Epoch 760, training loss: 12.281721115112305 = 0.1987137794494629 + 2.0 * 6.04150390625
Epoch 760, val loss: 0.7519468069076538
Epoch 770, training loss: 12.270236015319824 = 0.1903347671031952 + 2.0 * 6.039950847625732
Epoch 770, val loss: 0.754542887210846
Epoch 780, training loss: 12.274923324584961 = 0.18228605389595032 + 2.0 * 6.046318531036377
Epoch 780, val loss: 0.757485032081604
Epoch 790, training loss: 12.256195068359375 = 0.17468170821666718 + 2.0 * 6.040756702423096
Epoch 790, val loss: 0.7606424689292908
Epoch 800, training loss: 12.243797302246094 = 0.16744238138198853 + 2.0 * 6.038177490234375
Epoch 800, val loss: 0.7640678882598877
Epoch 810, training loss: 12.23511791229248 = 0.16054469347000122 + 2.0 * 6.037286758422852
Epoch 810, val loss: 0.7678358554840088
Epoch 820, training loss: 12.241817474365234 = 0.15396444499492645 + 2.0 * 6.04392671585083
Epoch 820, val loss: 0.7717886567115784
Epoch 830, training loss: 12.223670959472656 = 0.14772790670394897 + 2.0 * 6.037971496582031
Epoch 830, val loss: 0.7758933305740356
Epoch 840, training loss: 12.2118558883667 = 0.1417924165725708 + 2.0 * 6.035031795501709
Epoch 840, val loss: 0.7802226543426514
Epoch 850, training loss: 12.216107368469238 = 0.13612054288387299 + 2.0 * 6.0399932861328125
Epoch 850, val loss: 0.7847246527671814
Epoch 860, training loss: 12.201249122619629 = 0.13076360523700714 + 2.0 * 6.035242557525635
Epoch 860, val loss: 0.7893226146697998
Epoch 870, training loss: 12.191352844238281 = 0.12564124166965485 + 2.0 * 6.032855987548828
Epoch 870, val loss: 0.7940450310707092
Epoch 880, training loss: 12.189693450927734 = 0.1207507774233818 + 2.0 * 6.03447151184082
Epoch 880, val loss: 0.7989528775215149
Epoch 890, training loss: 12.179927825927734 = 0.11612363159656525 + 2.0 * 6.031902313232422
Epoch 890, val loss: 0.8038629293441772
Epoch 900, training loss: 12.175790786743164 = 0.11171034723520279 + 2.0 * 6.032040119171143
Epoch 900, val loss: 0.8088231682777405
Epoch 910, training loss: 12.16788101196289 = 0.10752540081739426 + 2.0 * 6.030177593231201
Epoch 910, val loss: 0.8139395117759705
Epoch 920, training loss: 12.180792808532715 = 0.10350248217582703 + 2.0 * 6.038645267486572
Epoch 920, val loss: 0.8191297650337219
Epoch 930, training loss: 12.163392066955566 = 0.09968645125627518 + 2.0 * 6.031852722167969
Epoch 930, val loss: 0.8242430090904236
Epoch 940, training loss: 12.155722618103027 = 0.09607606381177902 + 2.0 * 6.029823303222656
Epoch 940, val loss: 0.8294099569320679
Epoch 950, training loss: 12.148513793945312 = 0.09261125326156616 + 2.0 * 6.027951240539551
Epoch 950, val loss: 0.8347762227058411
Epoch 960, training loss: 12.142661094665527 = 0.08927200734615326 + 2.0 * 6.0266947746276855
Epoch 960, val loss: 0.8401929140090942
Epoch 970, training loss: 12.165414810180664 = 0.08608295023441315 + 2.0 * 6.039665699005127
Epoch 970, val loss: 0.8456252217292786
Epoch 980, training loss: 12.137290954589844 = 0.08304061740636826 + 2.0 * 6.027125358581543
Epoch 980, val loss: 0.8509302139282227
Epoch 990, training loss: 12.133450508117676 = 0.08016953617334366 + 2.0 * 6.02664041519165
Epoch 990, val loss: 0.856260359287262
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8111
Overall ASR: 0.4244
Flip ASR: 0.3467/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.709924697875977 = 1.9624289274215698 + 2.0 * 8.373747825622559
Epoch 0, val loss: 1.9702175855636597
Epoch 10, training loss: 18.69757652282715 = 1.9519009590148926 + 2.0 * 8.372838020324707
Epoch 10, val loss: 1.9599778652191162
Epoch 20, training loss: 18.673442840576172 = 1.938639521598816 + 2.0 * 8.367402076721191
Epoch 20, val loss: 1.946772575378418
Epoch 30, training loss: 18.582286834716797 = 1.9206444025039673 + 2.0 * 8.33082103729248
Epoch 30, val loss: 1.9287548065185547
Epoch 40, training loss: 17.978824615478516 = 1.900348424911499 + 2.0 * 8.039237976074219
Epoch 40, val loss: 1.9089491367340088
Epoch 50, training loss: 16.375633239746094 = 1.878092646598816 + 2.0 * 7.248770236968994
Epoch 50, val loss: 1.8878672122955322
Epoch 60, training loss: 15.58668041229248 = 1.8648300170898438 + 2.0 * 6.860925197601318
Epoch 60, val loss: 1.8756005764007568
Epoch 70, training loss: 15.156112670898438 = 1.8535078763961792 + 2.0 * 6.651302337646484
Epoch 70, val loss: 1.864288091659546
Epoch 80, training loss: 14.88908576965332 = 1.8415780067443848 + 2.0 * 6.523753643035889
Epoch 80, val loss: 1.8521499633789062
Epoch 90, training loss: 14.713119506835938 = 1.830284595489502 + 2.0 * 6.441417217254639
Epoch 90, val loss: 1.8403184413909912
Epoch 100, training loss: 14.591012954711914 = 1.8207515478134155 + 2.0 * 6.385130882263184
Epoch 100, val loss: 1.8300909996032715
Epoch 110, training loss: 14.500882148742676 = 1.8122036457061768 + 2.0 * 6.344339370727539
Epoch 110, val loss: 1.820424199104309
Epoch 120, training loss: 14.428041458129883 = 1.8042645454406738 + 2.0 * 6.311888694763184
Epoch 120, val loss: 1.811176061630249
Epoch 130, training loss: 14.368199348449707 = 1.7967387437820435 + 2.0 * 6.285730361938477
Epoch 130, val loss: 1.8024985790252686
Epoch 140, training loss: 14.314336776733398 = 1.7894394397735596 + 2.0 * 6.262448787689209
Epoch 140, val loss: 1.7944289445877075
Epoch 150, training loss: 14.266715049743652 = 1.7820669412612915 + 2.0 * 6.242323875427246
Epoch 150, val loss: 1.7866992950439453
Epoch 160, training loss: 14.224898338317871 = 1.7742329835891724 + 2.0 * 6.225332736968994
Epoch 160, val loss: 1.7789955139160156
Epoch 170, training loss: 14.186893463134766 = 1.765671968460083 + 2.0 * 6.210610866546631
Epoch 170, val loss: 1.770956039428711
Epoch 180, training loss: 14.152315139770508 = 1.7560372352600098 + 2.0 * 6.19813871383667
Epoch 180, val loss: 1.7623640298843384
Epoch 190, training loss: 14.119170188903809 = 1.7451517581939697 + 2.0 * 6.187009334564209
Epoch 190, val loss: 1.7529622316360474
Epoch 200, training loss: 14.085517883300781 = 1.7327133417129517 + 2.0 * 6.1764020919799805
Epoch 200, val loss: 1.74249267578125
Epoch 210, training loss: 14.057016372680664 = 1.7183829545974731 + 2.0 * 6.16931676864624
Epoch 210, val loss: 1.730609655380249
Epoch 220, training loss: 14.020934104919434 = 1.7017782926559448 + 2.0 * 6.1595778465271
Epoch 220, val loss: 1.717099666595459
Epoch 230, training loss: 13.986032485961914 = 1.6824898719787598 + 2.0 * 6.151771068572998
Epoch 230, val loss: 1.701534390449524
Epoch 240, training loss: 13.949520111083984 = 1.659903883934021 + 2.0 * 6.144808292388916
Epoch 240, val loss: 1.6833488941192627
Epoch 250, training loss: 13.915879249572754 = 1.633428692817688 + 2.0 * 6.141225337982178
Epoch 250, val loss: 1.6620227098464966
Epoch 260, training loss: 13.870648384094238 = 1.6028238534927368 + 2.0 * 6.133912086486816
Epoch 260, val loss: 1.637465238571167
Epoch 270, training loss: 13.823974609375 = 1.5676881074905396 + 2.0 * 6.128143310546875
Epoch 270, val loss: 1.6091915369033813
Epoch 280, training loss: 13.77454948425293 = 1.5276193618774414 + 2.0 * 6.123465061187744
Epoch 280, val loss: 1.5768249034881592
Epoch 290, training loss: 13.727161407470703 = 1.4828171730041504 + 2.0 * 6.1221723556518555
Epoch 290, val loss: 1.54059636592865
Epoch 300, training loss: 13.667895317077637 = 1.434770941734314 + 2.0 * 6.116562366485596
Epoch 300, val loss: 1.5017168521881104
Epoch 310, training loss: 13.609219551086426 = 1.384148359298706 + 2.0 * 6.11253547668457
Epoch 310, val loss: 1.4607981443405151
Epoch 320, training loss: 13.550427436828613 = 1.331674337387085 + 2.0 * 6.109376430511475
Epoch 320, val loss: 1.4184091091156006
Epoch 330, training loss: 13.492899894714355 = 1.279075026512146 + 2.0 * 6.106912612915039
Epoch 330, val loss: 1.3763210773468018
Epoch 340, training loss: 13.435617446899414 = 1.2282310724258423 + 2.0 * 6.103693008422852
Epoch 340, val loss: 1.3359441757202148
Epoch 350, training loss: 13.380219459533691 = 1.1796925067901611 + 2.0 * 6.100263595581055
Epoch 350, val loss: 1.2977694272994995
Epoch 360, training loss: 13.331048011779785 = 1.133580207824707 + 2.0 * 6.098733901977539
Epoch 360, val loss: 1.2617323398590088
Epoch 370, training loss: 13.28117561340332 = 1.0904144048690796 + 2.0 * 6.095380783081055
Epoch 370, val loss: 1.2284120321273804
Epoch 380, training loss: 13.23373794555664 = 1.0501999855041504 + 2.0 * 6.091769218444824
Epoch 380, val loss: 1.1976864337921143
Epoch 390, training loss: 13.197516441345215 = 1.0125094652175903 + 2.0 * 6.092503547668457
Epoch 390, val loss: 1.1692156791687012
Epoch 400, training loss: 13.155662536621094 = 0.9773896932601929 + 2.0 * 6.089136600494385
Epoch 400, val loss: 1.1430108547210693
Epoch 410, training loss: 13.113606452941895 = 0.944387674331665 + 2.0 * 6.084609508514404
Epoch 410, val loss: 1.1186009645462036
Epoch 420, training loss: 13.081812858581543 = 0.9130056500434875 + 2.0 * 6.0844035148620605
Epoch 420, val loss: 1.095399022102356
Epoch 430, training loss: 13.045119285583496 = 0.8829910159111023 + 2.0 * 6.081064224243164
Epoch 430, val loss: 1.073248267173767
Epoch 440, training loss: 13.01171875 = 0.8539681434631348 + 2.0 * 6.078875541687012
Epoch 440, val loss: 1.0520399808883667
Epoch 450, training loss: 12.977004051208496 = 0.8258383870124817 + 2.0 * 6.075582981109619
Epoch 450, val loss: 1.0315395593643188
Epoch 460, training loss: 12.95403003692627 = 0.7985939383506775 + 2.0 * 6.077718257904053
Epoch 460, val loss: 1.0115888118743896
Epoch 470, training loss: 12.920312881469727 = 0.7722795009613037 + 2.0 * 6.074016571044922
Epoch 470, val loss: 0.9926928281784058
Epoch 480, training loss: 12.889395713806152 = 0.74689781665802 + 2.0 * 6.071249008178711
Epoch 480, val loss: 0.9745296835899353
Epoch 490, training loss: 12.859055519104004 = 0.7222976684570312 + 2.0 * 6.068378925323486
Epoch 490, val loss: 0.957204282283783
Epoch 500, training loss: 12.832132339477539 = 0.6984055042266846 + 2.0 * 6.066863536834717
Epoch 500, val loss: 0.9408051371574402
Epoch 510, training loss: 12.811665534973145 = 0.6751791834831238 + 2.0 * 6.068243026733398
Epoch 510, val loss: 0.925358772277832
Epoch 520, training loss: 12.78554916381836 = 0.6529075503349304 + 2.0 * 6.066320896148682
Epoch 520, val loss: 0.9107947945594788
Epoch 530, training loss: 12.758807182312012 = 0.6312883496284485 + 2.0 * 6.0637593269348145
Epoch 530, val loss: 0.8973485231399536
Epoch 540, training loss: 12.730764389038086 = 0.6102887988090515 + 2.0 * 6.060237884521484
Epoch 540, val loss: 0.8846662640571594
Epoch 550, training loss: 12.708135604858398 = 0.5897880792617798 + 2.0 * 6.059173583984375
Epoch 550, val loss: 0.8727843761444092
Epoch 560, training loss: 12.697015762329102 = 0.5696791410446167 + 2.0 * 6.063668251037598
Epoch 560, val loss: 0.8614716529846191
Epoch 570, training loss: 12.665267944335938 = 0.5501154065132141 + 2.0 * 6.0575761795043945
Epoch 570, val loss: 0.8507905006408691
Epoch 580, training loss: 12.64236831665039 = 0.5308952927589417 + 2.0 * 6.055736541748047
Epoch 580, val loss: 0.8406995534896851
Epoch 590, training loss: 12.636189460754395 = 0.5119978785514832 + 2.0 * 6.062095642089844
Epoch 590, val loss: 0.8309547901153564
Epoch 600, training loss: 12.601785659790039 = 0.49351251125335693 + 2.0 * 6.054136753082275
Epoch 600, val loss: 0.8218727111816406
Epoch 610, training loss: 12.579194068908691 = 0.4754389524459839 + 2.0 * 6.051877498626709
Epoch 610, val loss: 0.8133793473243713
Epoch 620, training loss: 12.561989784240723 = 0.4576515555381775 + 2.0 * 6.052169322967529
Epoch 620, val loss: 0.8053043484687805
Epoch 630, training loss: 12.54134750366211 = 0.4402962625026703 + 2.0 * 6.050525665283203
Epoch 630, val loss: 0.7977806925773621
Epoch 640, training loss: 12.52423095703125 = 0.42338472604751587 + 2.0 * 6.0504231452941895
Epoch 640, val loss: 0.7909097075462341
Epoch 650, training loss: 12.50368881225586 = 0.4068832993507385 + 2.0 * 6.048402786254883
Epoch 650, val loss: 0.7844961285591125
Epoch 660, training loss: 12.483028411865234 = 0.3907387852668762 + 2.0 * 6.046144962310791
Epoch 660, val loss: 0.7785789966583252
Epoch 670, training loss: 12.480658531188965 = 0.37499797344207764 + 2.0 * 6.052830219268799
Epoch 670, val loss: 0.7730903625488281
Epoch 680, training loss: 12.448568344116211 = 0.3597862422466278 + 2.0 * 6.04439115524292
Epoch 680, val loss: 0.7681769132614136
Epoch 690, training loss: 12.435223579406738 = 0.34509190917015076 + 2.0 * 6.045065879821777
Epoch 690, val loss: 0.7638261914253235
Epoch 700, training loss: 12.423124313354492 = 0.33090877532958984 + 2.0 * 6.046107769012451
Epoch 700, val loss: 0.759894847869873
Epoch 710, training loss: 12.401116371154785 = 0.31725576519966125 + 2.0 * 6.041930198669434
Epoch 710, val loss: 0.7565097808837891
Epoch 720, training loss: 12.386703491210938 = 0.30409562587738037 + 2.0 * 6.041304111480713
Epoch 720, val loss: 0.7536188960075378
Epoch 730, training loss: 12.376571655273438 = 0.291395366191864 + 2.0 * 6.042588233947754
Epoch 730, val loss: 0.7511013746261597
Epoch 740, training loss: 12.358560562133789 = 0.27920204401016235 + 2.0 * 6.039679050445557
Epoch 740, val loss: 0.7489905953407288
Epoch 750, training loss: 12.354239463806152 = 0.26746073365211487 + 2.0 * 6.043389320373535
Epoch 750, val loss: 0.7473043203353882
Epoch 760, training loss: 12.33642864227295 = 0.2562650442123413 + 2.0 * 6.040081977844238
Epoch 760, val loss: 0.7459515929222107
Epoch 770, training loss: 12.320008277893066 = 0.24549023807048798 + 2.0 * 6.037259101867676
Epoch 770, val loss: 0.745048463344574
Epoch 780, training loss: 12.3106689453125 = 0.23512928187847137 + 2.0 * 6.037769794464111
Epoch 780, val loss: 0.7444251179695129
Epoch 790, training loss: 12.29604434967041 = 0.22518344223499298 + 2.0 * 6.035430431365967
Epoch 790, val loss: 0.7440767288208008
Epoch 800, training loss: 12.285438537597656 = 0.21567261219024658 + 2.0 * 6.03488302230835
Epoch 800, val loss: 0.7441891431808472
Epoch 810, training loss: 12.273717880249023 = 0.20652155578136444 + 2.0 * 6.033597946166992
Epoch 810, val loss: 0.7445421814918518
Epoch 820, training loss: 12.272727012634277 = 0.19773222506046295 + 2.0 * 6.037497520446777
Epoch 820, val loss: 0.745223343372345
Epoch 830, training loss: 12.266119003295898 = 0.1894274353981018 + 2.0 * 6.038345813751221
Epoch 830, val loss: 0.7460651993751526
Epoch 840, training loss: 12.249192237854004 = 0.18150784075260162 + 2.0 * 6.033842086791992
Epoch 840, val loss: 0.7472380995750427
Epoch 850, training loss: 12.238103866577148 = 0.17392946779727936 + 2.0 * 6.032087326049805
Epoch 850, val loss: 0.7485665678977966
Epoch 860, training loss: 12.234710693359375 = 0.16668574512004852 + 2.0 * 6.034012317657471
Epoch 860, val loss: 0.7500858902931213
Epoch 870, training loss: 12.219867706298828 = 0.15981720387935638 + 2.0 * 6.030025482177734
Epoch 870, val loss: 0.751854658126831
Epoch 880, training loss: 12.2163724899292 = 0.15325313806533813 + 2.0 * 6.031559467315674
Epoch 880, val loss: 0.7537849545478821
Epoch 890, training loss: 12.205913543701172 = 0.14699092507362366 + 2.0 * 6.02946138381958
Epoch 890, val loss: 0.7558721303939819
Epoch 900, training loss: 12.197545051574707 = 0.1410246044397354 + 2.0 * 6.028260231018066
Epoch 900, val loss: 0.7581404447555542
Epoch 910, training loss: 12.19034194946289 = 0.13532954454421997 + 2.0 * 6.027506351470947
Epoch 910, val loss: 0.7605689167976379
Epoch 920, training loss: 12.18481731414795 = 0.12987416982650757 + 2.0 * 6.027471542358398
Epoch 920, val loss: 0.7631394267082214
Epoch 930, training loss: 12.18216323852539 = 0.12469112128019333 + 2.0 * 6.028736114501953
Epoch 930, val loss: 0.7657885551452637
Epoch 940, training loss: 12.175241470336914 = 0.11979611217975616 + 2.0 * 6.0277228355407715
Epoch 940, val loss: 0.768555760383606
Epoch 950, training loss: 12.163414001464844 = 0.11512146145105362 + 2.0 * 6.02414608001709
Epoch 950, val loss: 0.7714816927909851
Epoch 960, training loss: 12.158276557922363 = 0.1106400266289711 + 2.0 * 6.023818492889404
Epoch 960, val loss: 0.7745143175125122
Epoch 970, training loss: 12.158352851867676 = 0.10634812712669373 + 2.0 * 6.026002407073975
Epoch 970, val loss: 0.7776590585708618
Epoch 980, training loss: 12.14726734161377 = 0.10227498412132263 + 2.0 * 6.022496223449707
Epoch 980, val loss: 0.7808758020401001
Epoch 990, training loss: 12.14906120300293 = 0.09838403016328812 + 2.0 * 6.025338649749756
Epoch 990, val loss: 0.7842286825180054
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.7454
Flip ASR: 0.7022/225 nodes
The final ASR:0.62485, 0.14273, Accuracy:0.81975, 0.00630
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11656])
remove edge: torch.Size([2, 9492])
updated graph: torch.Size([2, 10592])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
The final ASR:0.98032, 0.00174, Accuracy:0.83210, 0.00761
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.5 der2:0.4 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.4, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 18.692176818847656 = 1.9446804523468018 + 2.0 * 8.373747825622559
Epoch 0, val loss: 1.9353655576705933
Epoch 10, training loss: 18.679929733276367 = 1.934110403060913 + 2.0 * 8.372909545898438
Epoch 10, val loss: 1.925629734992981
Epoch 20, training loss: 18.65742301940918 = 1.9211680889129639 + 2.0 * 8.368127822875977
Epoch 20, val loss: 1.9134609699249268
Epoch 30, training loss: 18.580175399780273 = 1.9042664766311646 + 2.0 * 8.3379545211792
Epoch 30, val loss: 1.8975610733032227
Epoch 40, training loss: 18.122482299804688 = 1.885782241821289 + 2.0 * 8.1183500289917
Epoch 40, val loss: 1.8806370496749878
Epoch 50, training loss: 16.589176177978516 = 1.8643970489501953 + 2.0 * 7.36238956451416
Epoch 50, val loss: 1.861379861831665
Epoch 60, training loss: 15.908097267150879 = 1.847820520401001 + 2.0 * 7.0301384925842285
Epoch 60, val loss: 1.8477309942245483
Epoch 70, training loss: 15.419360160827637 = 1.8379313945770264 + 2.0 * 6.790714263916016
Epoch 70, val loss: 1.838724970817566
Epoch 80, training loss: 15.115194320678711 = 1.827372670173645 + 2.0 * 6.643910884857178
Epoch 80, val loss: 1.829616904258728
Epoch 90, training loss: 14.890995025634766 = 1.81954026222229 + 2.0 * 6.535727500915527
Epoch 90, val loss: 1.8228801488876343
Epoch 100, training loss: 14.723127365112305 = 1.8117843866348267 + 2.0 * 6.455671310424805
Epoch 100, val loss: 1.8162078857421875
Epoch 110, training loss: 14.597243309020996 = 1.8045178651809692 + 2.0 * 6.396362781524658
Epoch 110, val loss: 1.8095207214355469
Epoch 120, training loss: 14.501059532165527 = 1.797300100326538 + 2.0 * 6.351879596710205
Epoch 120, val loss: 1.8026527166366577
Epoch 130, training loss: 14.430252075195312 = 1.7899807691574097 + 2.0 * 6.320135593414307
Epoch 130, val loss: 1.7956619262695312
Epoch 140, training loss: 14.373002052307129 = 1.782462477684021 + 2.0 * 6.295269966125488
Epoch 140, val loss: 1.7885839939117432
Epoch 150, training loss: 14.320135116577148 = 1.774609923362732 + 2.0 * 6.272762775421143
Epoch 150, val loss: 1.7813818454742432
Epoch 160, training loss: 14.27157974243164 = 1.766129970550537 + 2.0 * 6.252724647521973
Epoch 160, val loss: 1.7738324403762817
Epoch 170, training loss: 14.230488777160645 = 1.7566845417022705 + 2.0 * 6.236902236938477
Epoch 170, val loss: 1.765690803527832
Epoch 180, training loss: 14.184957504272461 = 1.7460755109786987 + 2.0 * 6.219440937042236
Epoch 180, val loss: 1.756792664527893
Epoch 190, training loss: 14.143495559692383 = 1.7340534925460815 + 2.0 * 6.204720973968506
Epoch 190, val loss: 1.746860384941101
Epoch 200, training loss: 14.105566024780273 = 1.7200675010681152 + 2.0 * 6.1927490234375
Epoch 200, val loss: 1.7354360818862915
Epoch 210, training loss: 14.067033767700195 = 1.7038207054138184 + 2.0 * 6.181606769561768
Epoch 210, val loss: 1.7222594022750854
Epoch 220, training loss: 14.029253005981445 = 1.6849446296691895 + 2.0 * 6.172154426574707
Epoch 220, val loss: 1.7070151567459106
Epoch 230, training loss: 13.989317893981934 = 1.6628220081329346 + 2.0 * 6.163248062133789
Epoch 230, val loss: 1.6891943216323853
Epoch 240, training loss: 13.951513290405273 = 1.636876106262207 + 2.0 * 6.157318592071533
Epoch 240, val loss: 1.6683093309402466
Epoch 250, training loss: 13.905550003051758 = 1.6068546772003174 + 2.0 * 6.14934778213501
Epoch 250, val loss: 1.644161343574524
Epoch 260, training loss: 13.858743667602539 = 1.572275996208191 + 2.0 * 6.143233776092529
Epoch 260, val loss: 1.6163657903671265
Epoch 270, training loss: 13.809463500976562 = 1.5328669548034668 + 2.0 * 6.138298511505127
Epoch 270, val loss: 1.5846799612045288
Epoch 280, training loss: 13.756303787231445 = 1.4889265298843384 + 2.0 * 6.133688449859619
Epoch 280, val loss: 1.5493289232254028
Epoch 290, training loss: 13.698997497558594 = 1.4412026405334473 + 2.0 * 6.128897190093994
Epoch 290, val loss: 1.5111935138702393
Epoch 300, training loss: 13.639437675476074 = 1.3907192945480347 + 2.0 * 6.124359130859375
Epoch 300, val loss: 1.471049427986145
Epoch 310, training loss: 13.58997917175293 = 1.3387939929962158 + 2.0 * 6.1255927085876465
Epoch 310, val loss: 1.4302902221679688
Epoch 320, training loss: 13.52571964263916 = 1.2873667478561401 + 2.0 * 6.119176387786865
Epoch 320, val loss: 1.390404224395752
Epoch 330, training loss: 13.464469909667969 = 1.2373782396316528 + 2.0 * 6.113545894622803
Epoch 330, val loss: 1.3519890308380127
Epoch 340, training loss: 13.411869049072266 = 1.1891875267028809 + 2.0 * 6.111340522766113
Epoch 340, val loss: 1.3153976202011108
Epoch 350, training loss: 13.35947322845459 = 1.1437129974365234 + 2.0 * 6.107880115509033
Epoch 350, val loss: 1.281136393547058
Epoch 360, training loss: 13.30798625946045 = 1.1010034084320068 + 2.0 * 6.103491306304932
Epoch 360, val loss: 1.249232292175293
Epoch 370, training loss: 13.262551307678223 = 1.0607513189315796 + 2.0 * 6.100900173187256
Epoch 370, val loss: 1.2194215059280396
Epoch 380, training loss: 13.224156379699707 = 1.023348093032837 + 2.0 * 6.100404262542725
Epoch 380, val loss: 1.1920228004455566
Epoch 390, training loss: 13.178118705749512 = 0.9885092973709106 + 2.0 * 6.094804763793945
Epoch 390, val loss: 1.1667392253875732
Epoch 400, training loss: 13.138510704040527 = 0.9555619359016418 + 2.0 * 6.091474533081055
Epoch 400, val loss: 1.142899513244629
Epoch 410, training loss: 13.105985641479492 = 0.9240579605102539 + 2.0 * 6.090963840484619
Epoch 410, val loss: 1.120402216911316
Epoch 420, training loss: 13.065775871276855 = 0.8938981890678406 + 2.0 * 6.085938930511475
Epoch 420, val loss: 1.0989484786987305
Epoch 430, training loss: 13.032455444335938 = 0.8645167946815491 + 2.0 * 6.0839691162109375
Epoch 430, val loss: 1.0782244205474854
Epoch 440, training loss: 12.999399185180664 = 0.8354092836380005 + 2.0 * 6.081995010375977
Epoch 440, val loss: 1.0577051639556885
Epoch 450, training loss: 12.970696449279785 = 0.8066734075546265 + 2.0 * 6.082011699676514
Epoch 450, val loss: 1.0372904539108276
Epoch 460, training loss: 12.933859825134277 = 0.7779783606529236 + 2.0 * 6.077940940856934
Epoch 460, val loss: 1.0171749591827393
Epoch 470, training loss: 12.904801368713379 = 0.7492679357528687 + 2.0 * 6.0777668952941895
Epoch 470, val loss: 0.9970473647117615
Epoch 480, training loss: 12.868535041809082 = 0.7206532955169678 + 2.0 * 6.073940753936768
Epoch 480, val loss: 0.9771687984466553
Epoch 490, training loss: 12.837435722351074 = 0.6921409964561462 + 2.0 * 6.072647571563721
Epoch 490, val loss: 0.9575066566467285
Epoch 500, training loss: 12.804252624511719 = 0.6639184355735779 + 2.0 * 6.070167064666748
Epoch 500, val loss: 0.9382140040397644
Epoch 510, training loss: 12.771763801574707 = 0.6358762383460999 + 2.0 * 6.067943572998047
Epoch 510, val loss: 0.9193967580795288
Epoch 520, training loss: 12.745966911315918 = 0.6080335378646851 + 2.0 * 6.068966865539551
Epoch 520, val loss: 0.9010490775108337
Epoch 530, training loss: 12.71774673461914 = 0.5807891488075256 + 2.0 * 6.068478584289551
Epoch 530, val loss: 0.8833512663841248
Epoch 540, training loss: 12.683149337768555 = 0.5541638135910034 + 2.0 * 6.064492702484131
Epoch 540, val loss: 0.8667632937431335
Epoch 550, training loss: 12.654227256774902 = 0.5280757546424866 + 2.0 * 6.063075542449951
Epoch 550, val loss: 0.8508614897727966
Epoch 560, training loss: 12.622937202453613 = 0.5027220845222473 + 2.0 * 6.060107707977295
Epoch 560, val loss: 0.8359601497650146
Epoch 570, training loss: 12.609387397766113 = 0.4780832827091217 + 2.0 * 6.065651893615723
Epoch 570, val loss: 0.8220676183700562
Epoch 580, training loss: 12.57082462310791 = 0.45454174280166626 + 2.0 * 6.058141231536865
Epoch 580, val loss: 0.8092598915100098
Epoch 590, training loss: 12.544458389282227 = 0.431941956281662 + 2.0 * 6.056258201599121
Epoch 590, val loss: 0.7977454662322998
Epoch 600, training loss: 12.52001667022705 = 0.4102838337421417 + 2.0 * 6.054866313934326
Epoch 600, val loss: 0.7872453927993774
Epoch 610, training loss: 12.49977970123291 = 0.3896191418170929 + 2.0 * 6.055080413818359
Epoch 610, val loss: 0.7777880430221558
Epoch 620, training loss: 12.475744247436523 = 0.3701401948928833 + 2.0 * 6.052802085876465
Epoch 620, val loss: 0.7697237133979797
Epoch 630, training loss: 12.453435897827148 = 0.3516042232513428 + 2.0 * 6.050915718078613
Epoch 630, val loss: 0.7626339793205261
Epoch 640, training loss: 12.43286418914795 = 0.3339446485042572 + 2.0 * 6.049459934234619
Epoch 640, val loss: 0.7564793825149536
Epoch 650, training loss: 12.427865028381348 = 0.31717512011528015 + 2.0 * 6.055345058441162
Epoch 650, val loss: 0.7512266635894775
Epoch 660, training loss: 12.400979995727539 = 0.30147451162338257 + 2.0 * 6.049752712249756
Epoch 660, val loss: 0.7469935417175293
Epoch 670, training loss: 12.38152027130127 = 0.2865333557128906 + 2.0 * 6.0474934577941895
Epoch 670, val loss: 0.743467390537262
Epoch 680, training loss: 12.363029479980469 = 0.2723032236099243 + 2.0 * 6.045362949371338
Epoch 680, val loss: 0.7405983209609985
Epoch 690, training loss: 12.346790313720703 = 0.258770614862442 + 2.0 * 6.044009685516357
Epoch 690, val loss: 0.7384531497955322
Epoch 700, training loss: 12.334857940673828 = 0.24578650295734406 + 2.0 * 6.0445356369018555
Epoch 700, val loss: 0.7367978692054749
Epoch 710, training loss: 12.319563865661621 = 0.23347079753875732 + 2.0 * 6.043046474456787
Epoch 710, val loss: 0.7356196045875549
Epoch 720, training loss: 12.304214477539062 = 0.22165673971176147 + 2.0 * 6.041278839111328
Epoch 720, val loss: 0.7350307106971741
Epoch 730, training loss: 12.301823616027832 = 0.2103923112154007 + 2.0 * 6.045715808868408
Epoch 730, val loss: 0.7347919344902039
Epoch 740, training loss: 12.278985023498535 = 0.19973741471767426 + 2.0 * 6.039623737335205
Epoch 740, val loss: 0.7349880933761597
Epoch 750, training loss: 12.26555347442627 = 0.18957647681236267 + 2.0 * 6.037988662719727
Epoch 750, val loss: 0.7356004118919373
Epoch 760, training loss: 12.261204719543457 = 0.1798984855413437 + 2.0 * 6.040653228759766
Epoch 760, val loss: 0.7366328835487366
Epoch 770, training loss: 12.246150016784668 = 0.17076700925827026 + 2.0 * 6.037691593170166
Epoch 770, val loss: 0.7379627227783203
Epoch 780, training loss: 12.238801002502441 = 0.16214050352573395 + 2.0 * 6.038330078125
Epoch 780, val loss: 0.7397149801254272
Epoch 790, training loss: 12.223580360412598 = 0.15404342114925385 + 2.0 * 6.034768581390381
Epoch 790, val loss: 0.7418098449707031
Epoch 800, training loss: 12.213449478149414 = 0.14637595415115356 + 2.0 * 6.033536911010742
Epoch 800, val loss: 0.7442290782928467
Epoch 810, training loss: 12.203577995300293 = 0.13915954530239105 + 2.0 * 6.032209396362305
Epoch 810, val loss: 0.7469556927680969
Epoch 820, training loss: 12.213408470153809 = 0.1323883980512619 + 2.0 * 6.040510177612305
Epoch 820, val loss: 0.7499204874038696
Epoch 830, training loss: 12.188389778137207 = 0.12605072557926178 + 2.0 * 6.031169414520264
Epoch 830, val loss: 0.7531058192253113
Epoch 840, training loss: 12.185784339904785 = 0.12010834366083145 + 2.0 * 6.032837867736816
Epoch 840, val loss: 0.7564928531646729
Epoch 850, training loss: 12.174458503723145 = 0.1145404651761055 + 2.0 * 6.029959201812744
Epoch 850, val loss: 0.760071337223053
Epoch 860, training loss: 12.168701171875 = 0.10929553955793381 + 2.0 * 6.029702663421631
Epoch 860, val loss: 0.7638586163520813
Epoch 870, training loss: 12.163174629211426 = 0.10437963157892227 + 2.0 * 6.029397487640381
Epoch 870, val loss: 0.767835795879364
Epoch 880, training loss: 12.155621528625488 = 0.09976229816675186 + 2.0 * 6.027929782867432
Epoch 880, val loss: 0.771937906742096
Epoch 890, training loss: 12.149908065795898 = 0.0954229086637497 + 2.0 * 6.027242660522461
Epoch 890, val loss: 0.776172399520874
Epoch 900, training loss: 12.143628120422363 = 0.09133924543857574 + 2.0 * 6.026144504547119
Epoch 900, val loss: 0.7805473208427429
Epoch 910, training loss: 12.145644187927246 = 0.08750269562005997 + 2.0 * 6.029070854187012
Epoch 910, val loss: 0.7849821448326111
Epoch 920, training loss: 12.1344633102417 = 0.08389879763126373 + 2.0 * 6.025282382965088
Epoch 920, val loss: 0.7894517779350281
Epoch 930, training loss: 12.137572288513184 = 0.08049102127552032 + 2.0 * 6.02854061126709
Epoch 930, val loss: 0.7940118312835693
Epoch 940, training loss: 12.125980377197266 = 0.07730717211961746 + 2.0 * 6.024336814880371
Epoch 940, val loss: 0.7985877394676208
Epoch 950, training loss: 12.121145248413086 = 0.0742848664522171 + 2.0 * 6.023430347442627
Epoch 950, val loss: 0.8032099604606628
Epoch 960, training loss: 12.12376594543457 = 0.07142899930477142 + 2.0 * 6.026168346405029
Epoch 960, val loss: 0.8078004717826843
Epoch 970, training loss: 12.112143516540527 = 0.06874531507492065 + 2.0 * 6.021698951721191
Epoch 970, val loss: 0.8124586939811707
Epoch 980, training loss: 12.109689712524414 = 0.06618113815784454 + 2.0 * 6.021754264831543
Epoch 980, val loss: 0.8170610666275024
Epoch 990, training loss: 12.107292175292969 = 0.06375963240861893 + 2.0 * 6.021766185760498
Epoch 990, val loss: 0.8216543793678284
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.7417
Flip ASR: 0.6889/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.685224533081055 = 1.9376956224441528 + 2.0 * 8.373764038085938
Epoch 0, val loss: 1.9332630634307861
Epoch 10, training loss: 18.673248291015625 = 1.9275413751602173 + 2.0 * 8.37285327911377
Epoch 10, val loss: 1.922611951828003
Epoch 20, training loss: 18.65089225769043 = 1.915325403213501 + 2.0 * 8.367783546447754
Epoch 20, val loss: 1.909565806388855
Epoch 30, training loss: 18.585498809814453 = 1.8992128372192383 + 2.0 * 8.34314250946045
Epoch 30, val loss: 1.8924411535263062
Epoch 40, training loss: 18.25244903564453 = 1.8806123733520508 + 2.0 * 8.185918807983398
Epoch 40, val loss: 1.8731496334075928
Epoch 50, training loss: 16.972318649291992 = 1.8607754707336426 + 2.0 * 7.555771350860596
Epoch 50, val loss: 1.8517241477966309
Epoch 60, training loss: 16.292903900146484 = 1.839023232460022 + 2.0 * 7.226940631866455
Epoch 60, val loss: 1.830214262008667
Epoch 70, training loss: 15.731915473937988 = 1.8240107297897339 + 2.0 * 6.953952312469482
Epoch 70, val loss: 1.8154807090759277
Epoch 80, training loss: 15.322998046875 = 1.8124959468841553 + 2.0 * 6.755250930786133
Epoch 80, val loss: 1.8039348125457764
Epoch 90, training loss: 15.066617012023926 = 1.8024176359176636 + 2.0 * 6.632099628448486
Epoch 90, val loss: 1.7928540706634521
Epoch 100, training loss: 14.877314567565918 = 1.7921943664550781 + 2.0 * 6.54256010055542
Epoch 100, val loss: 1.7816675901412964
Epoch 110, training loss: 14.7468900680542 = 1.782649278640747 + 2.0 * 6.482120513916016
Epoch 110, val loss: 1.7714223861694336
Epoch 120, training loss: 14.642674446105957 = 1.7733980417251587 + 2.0 * 6.434638023376465
Epoch 120, val loss: 1.7616946697235107
Epoch 130, training loss: 14.565049171447754 = 1.7639564275741577 + 2.0 * 6.400546550750732
Epoch 130, val loss: 1.752025842666626
Epoch 140, training loss: 14.482498168945312 = 1.7543809413909912 + 2.0 * 6.364058494567871
Epoch 140, val loss: 1.7425798177719116
Epoch 150, training loss: 14.412485122680664 = 1.744825005531311 + 2.0 * 6.333829879760742
Epoch 150, val loss: 1.733513593673706
Epoch 160, training loss: 14.35104751586914 = 1.7345118522644043 + 2.0 * 6.308268070220947
Epoch 160, val loss: 1.723967432975769
Epoch 170, training loss: 14.293597221374512 = 1.722707986831665 + 2.0 * 6.285444736480713
Epoch 170, val loss: 1.7135965824127197
Epoch 180, training loss: 14.238046646118164 = 1.7092632055282593 + 2.0 * 6.264391899108887
Epoch 180, val loss: 1.7020431756973267
Epoch 190, training loss: 14.185263633728027 = 1.6938364505767822 + 2.0 * 6.245713710784912
Epoch 190, val loss: 1.6890960931777954
Epoch 200, training loss: 14.133578300476074 = 1.6759599447250366 + 2.0 * 6.228809356689453
Epoch 200, val loss: 1.6745001077651978
Epoch 210, training loss: 14.082382202148438 = 1.6550241708755493 + 2.0 * 6.21367883682251
Epoch 210, val loss: 1.6574935913085938
Epoch 220, training loss: 14.03145980834961 = 1.6301140785217285 + 2.0 * 6.200672626495361
Epoch 220, val loss: 1.63723623752594
Epoch 230, training loss: 13.979713439941406 = 1.6004018783569336 + 2.0 * 6.189655780792236
Epoch 230, val loss: 1.613133192062378
Epoch 240, training loss: 13.926185607910156 = 1.5656447410583496 + 2.0 * 6.180270195007324
Epoch 240, val loss: 1.5848902463912964
Epoch 250, training loss: 13.868454933166504 = 1.5252710580825806 + 2.0 * 6.171591758728027
Epoch 250, val loss: 1.5520209074020386
Epoch 260, training loss: 13.807068824768066 = 1.478974461555481 + 2.0 * 6.1640472412109375
Epoch 260, val loss: 1.5143654346466064
Epoch 270, training loss: 13.745100975036621 = 1.4278137683868408 + 2.0 * 6.15864372253418
Epoch 270, val loss: 1.4731210470199585
Epoch 280, training loss: 13.679973602294922 = 1.3741549253463745 + 2.0 * 6.152909278869629
Epoch 280, val loss: 1.4304406642913818
Epoch 290, training loss: 13.612935066223145 = 1.3196526765823364 + 2.0 * 6.146641254425049
Epoch 290, val loss: 1.3872345685958862
Epoch 300, training loss: 13.560232162475586 = 1.2655532360076904 + 2.0 * 6.147339344024658
Epoch 300, val loss: 1.3450899124145508
Epoch 310, training loss: 13.491537094116211 = 1.2143608331680298 + 2.0 * 6.138587951660156
Epoch 310, val loss: 1.3053845167160034
Epoch 320, training loss: 13.430098533630371 = 1.1653683185577393 + 2.0 * 6.1323652267456055
Epoch 320, val loss: 1.2679307460784912
Epoch 330, training loss: 13.376075744628906 = 1.1179156303405762 + 2.0 * 6.129079818725586
Epoch 330, val loss: 1.2324672937393188
Epoch 340, training loss: 13.323018074035645 = 1.0719804763793945 + 2.0 * 6.125518798828125
Epoch 340, val loss: 1.1983121633529663
Epoch 350, training loss: 13.267374038696289 = 1.0267415046691895 + 2.0 * 6.120316028594971
Epoch 350, val loss: 1.165051817893982
Epoch 360, training loss: 13.221449851989746 = 0.9816344380378723 + 2.0 * 6.119907855987549
Epoch 360, val loss: 1.1319776773452759
Epoch 370, training loss: 13.165640830993652 = 0.9368859529495239 + 2.0 * 6.114377498626709
Epoch 370, val loss: 1.099518895149231
Epoch 380, training loss: 13.113433837890625 = 0.8927620053291321 + 2.0 * 6.110335826873779
Epoch 380, val loss: 1.067328691482544
Epoch 390, training loss: 13.064666748046875 = 0.8494348526000977 + 2.0 * 6.107615947723389
Epoch 390, val loss: 1.0358264446258545
Epoch 400, training loss: 13.017230987548828 = 0.8078868389129639 + 2.0 * 6.104671955108643
Epoch 400, val loss: 1.00568687915802
Epoch 410, training loss: 12.970870971679688 = 0.7684330940246582 + 2.0 * 6.101219177246094
Epoch 410, val loss: 0.9770824313163757
Epoch 420, training loss: 12.930543899536133 = 0.7314028143882751 + 2.0 * 6.0995707511901855
Epoch 420, val loss: 0.9505805969238281
Epoch 430, training loss: 12.890981674194336 = 0.6965309381484985 + 2.0 * 6.097225189208984
Epoch 430, val loss: 0.9257634282112122
Epoch 440, training loss: 12.855857849121094 = 0.6639735698699951 + 2.0 * 6.09594202041626
Epoch 440, val loss: 0.903211772441864
Epoch 450, training loss: 12.817511558532715 = 0.6337047815322876 + 2.0 * 6.091903209686279
Epoch 450, val loss: 0.8825711607933044
Epoch 460, training loss: 12.782686233520508 = 0.6051974296569824 + 2.0 * 6.088744640350342
Epoch 460, val loss: 0.8636353611946106
Epoch 470, training loss: 12.75548267364502 = 0.5784348249435425 + 2.0 * 6.088523864746094
Epoch 470, val loss: 0.8464429974555969
Epoch 480, training loss: 12.722678184509277 = 0.5534458160400391 + 2.0 * 6.084616184234619
Epoch 480, val loss: 0.8309927582740784
Epoch 490, training loss: 12.694500923156738 = 0.5297939777374268 + 2.0 * 6.082353591918945
Epoch 490, val loss: 0.8169180154800415
Epoch 500, training loss: 12.666868209838867 = 0.5072219967842102 + 2.0 * 6.079823017120361
Epoch 500, val loss: 0.8040012121200562
Epoch 510, training loss: 12.65587329864502 = 0.4856608211994171 + 2.0 * 6.085106372833252
Epoch 510, val loss: 0.792246401309967
Epoch 520, training loss: 12.618537902832031 = 0.46536216139793396 + 2.0 * 6.076587677001953
Epoch 520, val loss: 0.7816097736358643
Epoch 530, training loss: 12.59770679473877 = 0.4461069405078888 + 2.0 * 6.075799942016602
Epoch 530, val loss: 0.7721336483955383
Epoch 540, training loss: 12.574106216430664 = 0.42778798937797546 + 2.0 * 6.073159217834473
Epoch 540, val loss: 0.7635645270347595
Epoch 550, training loss: 12.552087783813477 = 0.41019201278686523 + 2.0 * 6.070947647094727
Epoch 550, val loss: 0.7558342814445496
Epoch 560, training loss: 12.542420387268066 = 0.39329853653907776 + 2.0 * 6.07456111907959
Epoch 560, val loss: 0.7488031387329102
Epoch 570, training loss: 12.516854286193848 = 0.37716177105903625 + 2.0 * 6.069846153259277
Epoch 570, val loss: 0.74257493019104
Epoch 580, training loss: 12.49533748626709 = 0.36165595054626465 + 2.0 * 6.066840648651123
Epoch 580, val loss: 0.7369371056556702
Epoch 590, training loss: 12.478189468383789 = 0.34659165143966675 + 2.0 * 6.065798759460449
Epoch 590, val loss: 0.7318218350410461
Epoch 600, training loss: 12.458721160888672 = 0.33203235268592834 + 2.0 * 6.063344478607178
Epoch 600, val loss: 0.727100133895874
Epoch 610, training loss: 12.445422172546387 = 0.31793704628944397 + 2.0 * 6.063742637634277
Epoch 610, val loss: 0.7229337692260742
Epoch 620, training loss: 12.427165031433105 = 0.3042040169239044 + 2.0 * 6.061480522155762
Epoch 620, val loss: 0.7190602421760559
Epoch 630, training loss: 12.412008285522461 = 0.29089540243148804 + 2.0 * 6.060556411743164
Epoch 630, val loss: 0.7154802083969116
Epoch 640, training loss: 12.395442008972168 = 0.27799659967422485 + 2.0 * 6.058722496032715
Epoch 640, val loss: 0.7123100757598877
Epoch 650, training loss: 12.389538764953613 = 0.2655175030231476 + 2.0 * 6.062010765075684
Epoch 650, val loss: 0.7094720005989075
Epoch 660, training loss: 12.369121551513672 = 0.25354161858558655 + 2.0 * 6.0577898025512695
Epoch 660, val loss: 0.7070654034614563
Epoch 670, training loss: 12.353330612182617 = 0.2420373111963272 + 2.0 * 6.0556464195251465
Epoch 670, val loss: 0.7049667835235596
Epoch 680, training loss: 12.342042922973633 = 0.23097653687000275 + 2.0 * 6.055533409118652
Epoch 680, val loss: 0.7032255530357361
Epoch 690, training loss: 12.333285331726074 = 0.22043035924434662 + 2.0 * 6.056427478790283
Epoch 690, val loss: 0.7018275260925293
Epoch 700, training loss: 12.316825866699219 = 0.21049395203590393 + 2.0 * 6.053165912628174
Epoch 700, val loss: 0.7008498311042786
Epoch 710, training loss: 12.305289268493652 = 0.20107026398181915 + 2.0 * 6.052109718322754
Epoch 710, val loss: 0.7001463770866394
Epoch 720, training loss: 12.291189193725586 = 0.1921108216047287 + 2.0 * 6.049539089202881
Epoch 720, val loss: 0.6998340487480164
Epoch 730, training loss: 12.292634963989258 = 0.18358564376831055 + 2.0 * 6.0545244216918945
Epoch 730, val loss: 0.699870228767395
Epoch 740, training loss: 12.280120849609375 = 0.17562606930732727 + 2.0 * 6.052247524261475
Epoch 740, val loss: 0.7000364661216736
Epoch 750, training loss: 12.264314651489258 = 0.1681361347436905 + 2.0 * 6.048089027404785
Epoch 750, val loss: 0.7006829380989075
Epoch 760, training loss: 12.252989768981934 = 0.16104334592819214 + 2.0 * 6.045973300933838
Epoch 760, val loss: 0.7015047669410706
Epoch 770, training loss: 12.243329048156738 = 0.154295414686203 + 2.0 * 6.0445170402526855
Epoch 770, val loss: 0.7026069164276123
Epoch 780, training loss: 12.2350492477417 = 0.14787347614765167 + 2.0 * 6.043587684631348
Epoch 780, val loss: 0.7039808630943298
Epoch 790, training loss: 12.246708869934082 = 0.14178141951560974 + 2.0 * 6.052463531494141
Epoch 790, val loss: 0.705544650554657
Epoch 800, training loss: 12.225725173950195 = 0.13608506321907043 + 2.0 * 6.0448198318481445
Epoch 800, val loss: 0.7073184847831726
Epoch 810, training loss: 12.2129545211792 = 0.13068240880966187 + 2.0 * 6.041136264801025
Epoch 810, val loss: 0.7093479633331299
Epoch 820, training loss: 12.205695152282715 = 0.1255234032869339 + 2.0 * 6.040085792541504
Epoch 820, val loss: 0.7115240693092346
Epoch 830, training loss: 12.229621887207031 = 0.12059903889894485 + 2.0 * 6.054511547088623
Epoch 830, val loss: 0.7138766646385193
Epoch 840, training loss: 12.197368621826172 = 0.11600825190544128 + 2.0 * 6.040680408477783
Epoch 840, val loss: 0.716325044631958
Epoch 850, training loss: 12.186406135559082 = 0.11163581162691116 + 2.0 * 6.037384986877441
Epoch 850, val loss: 0.7190057635307312
Epoch 860, training loss: 12.181000709533691 = 0.10745233297348022 + 2.0 * 6.036774158477783
Epoch 860, val loss: 0.7217484712600708
Epoch 870, training loss: 12.180105209350586 = 0.10343907028436661 + 2.0 * 6.038332939147949
Epoch 870, val loss: 0.7246556282043457
Epoch 880, training loss: 12.179816246032715 = 0.09962503612041473 + 2.0 * 6.040095806121826
Epoch 880, val loss: 0.7277450561523438
Epoch 890, training loss: 12.165660858154297 = 0.09600304812192917 + 2.0 * 6.034829139709473
Epoch 890, val loss: 0.7308773994445801
Epoch 900, training loss: 12.15941047668457 = 0.09253495931625366 + 2.0 * 6.033437728881836
Epoch 900, val loss: 0.7340950965881348
Epoch 910, training loss: 12.155603408813477 = 0.08920445293188095 + 2.0 * 6.033199310302734
Epoch 910, val loss: 0.7374043464660645
Epoch 920, training loss: 12.166918754577637 = 0.08602342009544373 + 2.0 * 6.04044771194458
Epoch 920, val loss: 0.7407549023628235
Epoch 930, training loss: 12.155611991882324 = 0.08299058675765991 + 2.0 * 6.03631067276001
Epoch 930, val loss: 0.7442270517349243
Epoch 940, training loss: 12.143060684204102 = 0.08012339472770691 + 2.0 * 6.031468868255615
Epoch 940, val loss: 0.7476953864097595
Epoch 950, training loss: 12.137754440307617 = 0.07737709581851959 + 2.0 * 6.03018856048584
Epoch 950, val loss: 0.7512186169624329
Epoch 960, training loss: 12.13347053527832 = 0.0747351422905922 + 2.0 * 6.029367923736572
Epoch 960, val loss: 0.7548547983169556
Epoch 970, training loss: 12.147286415100098 = 0.07220441848039627 + 2.0 * 6.037540912628174
Epoch 970, val loss: 0.7584902048110962
Epoch 980, training loss: 12.13128662109375 = 0.06979098916053772 + 2.0 * 6.030747890472412
Epoch 980, val loss: 0.7622338533401489
Epoch 990, training loss: 12.12270736694336 = 0.06749039143323898 + 2.0 * 6.027608394622803
Epoch 990, val loss: 0.7659084796905518
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.6236
Flip ASR: 0.5778/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.68284034729004 = 1.93534255027771 + 2.0 * 8.373748779296875
Epoch 0, val loss: 1.938522458076477
Epoch 10, training loss: 18.67190170288086 = 1.9259178638458252 + 2.0 * 8.372991561889648
Epoch 10, val loss: 1.9286094903945923
Epoch 20, training loss: 18.65204620361328 = 1.914490818977356 + 2.0 * 8.36877727508545
Epoch 20, val loss: 1.9163309335708618
Epoch 30, training loss: 18.587139129638672 = 1.899617314338684 + 2.0 * 8.34376049041748
Epoch 30, val loss: 1.900197148323059
Epoch 40, training loss: 18.21645736694336 = 1.8827159404754639 + 2.0 * 8.166871070861816
Epoch 40, val loss: 1.8826837539672852
Epoch 50, training loss: 16.499252319335938 = 1.863858699798584 + 2.0 * 7.317696571350098
Epoch 50, val loss: 1.8632299900054932
Epoch 60, training loss: 15.796367645263672 = 1.848820447921753 + 2.0 * 6.97377347946167
Epoch 60, val loss: 1.8480818271636963
Epoch 70, training loss: 15.253384590148926 = 1.837532639503479 + 2.0 * 6.707925796508789
Epoch 70, val loss: 1.8366034030914307
Epoch 80, training loss: 14.962631225585938 = 1.827500343322754 + 2.0 * 6.567565441131592
Epoch 80, val loss: 1.8261741399765015
Epoch 90, training loss: 14.776327133178711 = 1.818564772605896 + 2.0 * 6.478881359100342
Epoch 90, val loss: 1.8167251348495483
Epoch 100, training loss: 14.653417587280273 = 1.8095297813415527 + 2.0 * 6.4219441413879395
Epoch 100, val loss: 1.807114601135254
Epoch 110, training loss: 14.555322647094727 = 1.8015090227127075 + 2.0 * 6.376906871795654
Epoch 110, val loss: 1.7986788749694824
Epoch 120, training loss: 14.473102569580078 = 1.794052004814148 + 2.0 * 6.33952522277832
Epoch 120, val loss: 1.790985107421875
Epoch 130, training loss: 14.404020309448242 = 1.7867320775985718 + 2.0 * 6.3086442947387695
Epoch 130, val loss: 1.7836010456085205
Epoch 140, training loss: 14.34375 = 1.7792869806289673 + 2.0 * 6.282231330871582
Epoch 140, val loss: 1.7762304544448853
Epoch 150, training loss: 14.289735794067383 = 1.77134370803833 + 2.0 * 6.2591962814331055
Epoch 150, val loss: 1.7687782049179077
Epoch 160, training loss: 14.242192268371582 = 1.7626434564590454 + 2.0 * 6.239774227142334
Epoch 160, val loss: 1.7608835697174072
Epoch 170, training loss: 14.200604438781738 = 1.7529510259628296 + 2.0 * 6.223826885223389
Epoch 170, val loss: 1.7523119449615479
Epoch 180, training loss: 14.159296035766602 = 1.7420754432678223 + 2.0 * 6.2086100578308105
Epoch 180, val loss: 1.7430102825164795
Epoch 190, training loss: 14.121289253234863 = 1.7298657894134521 + 2.0 * 6.195711612701416
Epoch 190, val loss: 1.7326456308364868
Epoch 200, training loss: 14.084551811218262 = 1.7158814668655396 + 2.0 * 6.184335231781006
Epoch 200, val loss: 1.721015453338623
Epoch 210, training loss: 14.051265716552734 = 1.6997594833374023 + 2.0 * 6.175753116607666
Epoch 210, val loss: 1.7076510190963745
Epoch 220, training loss: 14.012103080749512 = 1.6810836791992188 + 2.0 * 6.1655097007751465
Epoch 220, val loss: 1.6926155090332031
Epoch 230, training loss: 13.973421096801758 = 1.6593916416168213 + 2.0 * 6.157014846801758
Epoch 230, val loss: 1.6751773357391357
Epoch 240, training loss: 13.936779975891113 = 1.633981466293335 + 2.0 * 6.1513991355896
Epoch 240, val loss: 1.6548494100570679
Epoch 250, training loss: 13.891024589538574 = 1.604244589805603 + 2.0 * 6.14339017868042
Epoch 250, val loss: 1.631085991859436
Epoch 260, training loss: 13.844362258911133 = 1.569667100906372 + 2.0 * 6.13734769821167
Epoch 260, val loss: 1.6035819053649902
Epoch 270, training loss: 13.795369148254395 = 1.529796838760376 + 2.0 * 6.132786273956299
Epoch 270, val loss: 1.5718917846679688
Epoch 280, training loss: 13.739705085754395 = 1.4844214916229248 + 2.0 * 6.127641677856445
Epoch 280, val loss: 1.5359909534454346
Epoch 290, training loss: 13.679215431213379 = 1.433659553527832 + 2.0 * 6.122777938842773
Epoch 290, val loss: 1.4954019784927368
Epoch 300, training loss: 13.62021255493164 = 1.377930998802185 + 2.0 * 6.121140956878662
Epoch 300, val loss: 1.451076865196228
Epoch 310, training loss: 13.552313804626465 = 1.3196169137954712 + 2.0 * 6.1163482666015625
Epoch 310, val loss: 1.4044010639190674
Epoch 320, training loss: 13.48513412475586 = 1.2590454816818237 + 2.0 * 6.113044261932373
Epoch 320, val loss: 1.355951189994812
Epoch 330, training loss: 13.421937942504883 = 1.1968131065368652 + 2.0 * 6.11256217956543
Epoch 330, val loss: 1.3060983419418335
Epoch 340, training loss: 13.349830627441406 = 1.1348299980163574 + 2.0 * 6.107500076293945
Epoch 340, val loss: 1.2566856145858765
Epoch 350, training loss: 13.283169746398926 = 1.073918104171753 + 2.0 * 6.104625701904297
Epoch 350, val loss: 1.2080750465393066
Epoch 360, training loss: 13.217469215393066 = 1.0139367580413818 + 2.0 * 6.101766109466553
Epoch 360, val loss: 1.1600743532180786
Epoch 370, training loss: 13.155975341796875 = 0.9558834433555603 + 2.0 * 6.100046157836914
Epoch 370, val loss: 1.1142544746398926
Epoch 380, training loss: 13.09766674041748 = 0.9015823006629944 + 2.0 * 6.098042011260986
Epoch 380, val loss: 1.071324348449707
Epoch 390, training loss: 13.039427757263184 = 0.8505275249481201 + 2.0 * 6.094449996948242
Epoch 390, val loss: 1.0312141180038452
Epoch 400, training loss: 12.98740005493164 = 0.803027868270874 + 2.0 * 6.092185974121094
Epoch 400, val loss: 0.9943474531173706
Epoch 410, training loss: 12.938270568847656 = 0.7593796849250793 + 2.0 * 6.0894455909729
Epoch 410, val loss: 0.9608779549598694
Epoch 420, training loss: 12.892529487609863 = 0.7189178466796875 + 2.0 * 6.086805820465088
Epoch 420, val loss: 0.9302436709403992
Epoch 430, training loss: 12.857873916625977 = 0.6813681125640869 + 2.0 * 6.088253021240234
Epoch 430, val loss: 0.9023187160491943
Epoch 440, training loss: 12.815585136413574 = 0.6467963457107544 + 2.0 * 6.084394454956055
Epoch 440, val loss: 0.8772659301757812
Epoch 450, training loss: 12.776853561401367 = 0.6149818301200867 + 2.0 * 6.080935955047607
Epoch 450, val loss: 0.8546925783157349
Epoch 460, training loss: 12.744272232055664 = 0.5852482914924622 + 2.0 * 6.079512119293213
Epoch 460, val loss: 0.8343244791030884
Epoch 470, training loss: 12.709095001220703 = 0.557579755783081 + 2.0 * 6.0757575035095215
Epoch 470, val loss: 0.8158151507377625
Epoch 480, training loss: 12.67978286743164 = 0.5315516591072083 + 2.0 * 6.074115753173828
Epoch 480, val loss: 0.7991176843643188
Epoch 490, training loss: 12.65402603149414 = 0.5071013569831848 + 2.0 * 6.07346248626709
Epoch 490, val loss: 0.7841840982437134
Epoch 500, training loss: 12.629565238952637 = 0.48435285687446594 + 2.0 * 6.072606086730957
Epoch 500, val loss: 0.7709143161773682
Epoch 510, training loss: 12.606143951416016 = 0.46305131912231445 + 2.0 * 6.0715460777282715
Epoch 510, val loss: 0.7591460347175598
Epoch 520, training loss: 12.578230857849121 = 0.4430038332939148 + 2.0 * 6.06761360168457
Epoch 520, val loss: 0.7487992644309998
Epoch 530, training loss: 12.558592796325684 = 0.424042284488678 + 2.0 * 6.067275047302246
Epoch 530, val loss: 0.739657461643219
Epoch 540, training loss: 12.536205291748047 = 0.4061082899570465 + 2.0 * 6.065048694610596
Epoch 540, val loss: 0.731773316860199
Epoch 550, training loss: 12.513879776000977 = 0.38918522000312805 + 2.0 * 6.062347412109375
Epoch 550, val loss: 0.7248945832252502
Epoch 560, training loss: 12.502767562866211 = 0.3730195462703705 + 2.0 * 6.064874172210693
Epoch 560, val loss: 0.7189515233039856
Epoch 570, training loss: 12.479167938232422 = 0.3576079308986664 + 2.0 * 6.060780048370361
Epoch 570, val loss: 0.7138624787330627
Epoch 580, training loss: 12.46489429473877 = 0.3428899943828583 + 2.0 * 6.061002254486084
Epoch 580, val loss: 0.7094775438308716
Epoch 590, training loss: 12.443135261535645 = 0.328757643699646 + 2.0 * 6.057188987731934
Epoch 590, val loss: 0.7057474255561829
Epoch 600, training loss: 12.426908493041992 = 0.31504034996032715 + 2.0 * 6.055933952331543
Epoch 600, val loss: 0.7026261687278748
Epoch 610, training loss: 12.4104642868042 = 0.3017691671848297 + 2.0 * 6.054347515106201
Epoch 610, val loss: 0.6999828219413757
Epoch 620, training loss: 12.396791458129883 = 0.288973331451416 + 2.0 * 6.0539093017578125
Epoch 620, val loss: 0.6978737115859985
Epoch 630, training loss: 12.380075454711914 = 0.2765137255191803 + 2.0 * 6.051780700683594
Epoch 630, val loss: 0.6961900591850281
Epoch 640, training loss: 12.379146575927734 = 0.26438137888908386 + 2.0 * 6.057382583618164
Epoch 640, val loss: 0.6949278116226196
Epoch 650, training loss: 12.352559089660645 = 0.25274941325187683 + 2.0 * 6.049904823303223
Epoch 650, val loss: 0.6940483450889587
Epoch 660, training loss: 12.338936805725098 = 0.24148817360401154 + 2.0 * 6.048724174499512
Epoch 660, val loss: 0.6936138272285461
Epoch 670, training loss: 12.328936576843262 = 0.230569988489151 + 2.0 * 6.049183368682861
Epoch 670, val loss: 0.6935901641845703
Epoch 680, training loss: 12.31669807434082 = 0.22007325291633606 + 2.0 * 6.048312187194824
Epoch 680, val loss: 0.6939312815666199
Epoch 690, training loss: 12.302813529968262 = 0.21008458733558655 + 2.0 * 6.0463643074035645
Epoch 690, val loss: 0.6946160197257996
Epoch 700, training loss: 12.289299964904785 = 0.20049645006656647 + 2.0 * 6.0444016456604
Epoch 700, val loss: 0.6957403421401978
Epoch 710, training loss: 12.290862083435059 = 0.19133242964744568 + 2.0 * 6.049764633178711
Epoch 710, val loss: 0.6972553133964539
Epoch 720, training loss: 12.267765045166016 = 0.18269570171833038 + 2.0 * 6.042534828186035
Epoch 720, val loss: 0.6990652680397034
Epoch 730, training loss: 12.257990837097168 = 0.17449666559696198 + 2.0 * 6.041747093200684
Epoch 730, val loss: 0.7012267708778381
Epoch 740, training loss: 12.247982025146484 = 0.1666945517063141 + 2.0 * 6.040643692016602
Epoch 740, val loss: 0.7038020491600037
Epoch 750, training loss: 12.260241508483887 = 0.1592862457036972 + 2.0 * 6.050477504730225
Epoch 750, val loss: 0.7067184448242188
Epoch 760, training loss: 12.230603218078613 = 0.15232501924037933 + 2.0 * 6.039139270782471
Epoch 760, val loss: 0.7098119258880615
Epoch 770, training loss: 12.226597785949707 = 0.14575056731700897 + 2.0 * 6.040423393249512
Epoch 770, val loss: 0.7131776809692383
Epoch 780, training loss: 12.216836929321289 = 0.1395338773727417 + 2.0 * 6.038651466369629
Epoch 780, val loss: 0.7168300151824951
Epoch 790, training loss: 12.208168983459473 = 0.1336345672607422 + 2.0 * 6.037267208099365
Epoch 790, val loss: 0.7207129597663879
Epoch 800, training loss: 12.200016975402832 = 0.12804017961025238 + 2.0 * 6.0359883308410645
Epoch 800, val loss: 0.7248328328132629
Epoch 810, training loss: 12.201380729675293 = 0.1227416843175888 + 2.0 * 6.0393195152282715
Epoch 810, val loss: 0.7291156649589539
Epoch 820, training loss: 12.18967342376709 = 0.11777915805578232 + 2.0 * 6.035947322845459
Epoch 820, val loss: 0.7335089445114136
Epoch 830, training loss: 12.18073558807373 = 0.1130874827504158 + 2.0 * 6.0338239669799805
Epoch 830, val loss: 0.7380190491676331
Epoch 840, training loss: 12.175466537475586 = 0.10862662643194199 + 2.0 * 6.033420085906982
Epoch 840, val loss: 0.7427866458892822
Epoch 850, training loss: 12.179854393005371 = 0.10438010841608047 + 2.0 * 6.0377373695373535
Epoch 850, val loss: 0.7476746439933777
Epoch 860, training loss: 12.16817569732666 = 0.1003945842385292 + 2.0 * 6.033890724182129
Epoch 860, val loss: 0.7526038885116577
Epoch 870, training loss: 12.159124374389648 = 0.09658323973417282 + 2.0 * 6.031270503997803
Epoch 870, val loss: 0.7576010823249817
Epoch 880, training loss: 12.153818130493164 = 0.09296181797981262 + 2.0 * 6.030427932739258
Epoch 880, val loss: 0.7627887725830078
Epoch 890, training loss: 12.157652854919434 = 0.08951316028833389 + 2.0 * 6.034070014953613
Epoch 890, val loss: 0.7679896950721741
Epoch 900, training loss: 12.148427963256836 = 0.08625456690788269 + 2.0 * 6.0310869216918945
Epoch 900, val loss: 0.7731555700302124
Epoch 910, training loss: 12.139575004577637 = 0.08316202461719513 + 2.0 * 6.0282063484191895
Epoch 910, val loss: 0.778400182723999
Epoch 920, training loss: 12.13550090789795 = 0.08020804077386856 + 2.0 * 6.027646541595459
Epoch 920, val loss: 0.7837799787521362
Epoch 930, training loss: 12.138758659362793 = 0.07738577574491501 + 2.0 * 6.030686378479004
Epoch 930, val loss: 0.7891906499862671
Epoch 940, training loss: 12.132023811340332 = 0.07468429207801819 + 2.0 * 6.028669834136963
Epoch 940, val loss: 0.7945281267166138
Epoch 950, training loss: 12.124938011169434 = 0.07212194055318832 + 2.0 * 6.0264081954956055
Epoch 950, val loss: 0.7999153137207031
Epoch 960, training loss: 12.1266450881958 = 0.06965701282024384 + 2.0 * 6.028493881225586
Epoch 960, val loss: 0.8053203225135803
Epoch 970, training loss: 12.119729995727539 = 0.06732026487588882 + 2.0 * 6.026205062866211
Epoch 970, val loss: 0.810722827911377
Epoch 980, training loss: 12.117464065551758 = 0.06508653610944748 + 2.0 * 6.026188850402832
Epoch 980, val loss: 0.8160602450370789
Epoch 990, training loss: 12.111737251281738 = 0.06294563412666321 + 2.0 * 6.024395942687988
Epoch 990, val loss: 0.8214623928070068
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.8819
Flip ASR: 0.8578/225 nodes
The final ASR:0.74908, 0.10558, Accuracy:0.83457, 0.00630
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11636])
remove edge: torch.Size([2, 9504])
updated graph: torch.Size([2, 10584])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
The final ASR:0.97786, 0.00301, Accuracy:0.83086, 0.00462
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.5 der2:0.4 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.4, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 18.710514068603516 = 1.9628515243530273 + 2.0 * 8.373830795288086
Epoch 0, val loss: 1.9639809131622314
Epoch 10, training loss: 18.698238372802734 = 1.9515748023986816 + 2.0 * 8.373332023620605
Epoch 10, val loss: 1.9528833627700806
Epoch 20, training loss: 18.678546905517578 = 1.9374139308929443 + 2.0 * 8.370566368103027
Epoch 20, val loss: 1.9388090372085571
Epoch 30, training loss: 18.620527267456055 = 1.9179843664169312 + 2.0 * 8.351271629333496
Epoch 30, val loss: 1.9198179244995117
Epoch 40, training loss: 18.291484832763672 = 1.8951536417007446 + 2.0 * 8.198165893554688
Epoch 40, val loss: 1.8985977172851562
Epoch 50, training loss: 16.625625610351562 = 1.8712844848632812 + 2.0 * 7.377171039581299
Epoch 50, val loss: 1.8766230344772339
Epoch 60, training loss: 15.85372543334961 = 1.8564159870147705 + 2.0 * 6.998654842376709
Epoch 60, val loss: 1.8639882802963257
Epoch 70, training loss: 15.373695373535156 = 1.8456640243530273 + 2.0 * 6.7640156745910645
Epoch 70, val loss: 1.8537153005599976
Epoch 80, training loss: 15.076557159423828 = 1.8339067697525024 + 2.0 * 6.6213250160217285
Epoch 80, val loss: 1.843163251876831
Epoch 90, training loss: 14.892414093017578 = 1.822868824005127 + 2.0 * 6.534772872924805
Epoch 90, val loss: 1.833034634590149
Epoch 100, training loss: 14.750618934631348 = 1.8125464916229248 + 2.0 * 6.469036102294922
Epoch 100, val loss: 1.8237563371658325
Epoch 110, training loss: 14.64742374420166 = 1.803614616394043 + 2.0 * 6.421904563903809
Epoch 110, val loss: 1.815608024597168
Epoch 120, training loss: 14.563222885131836 = 1.7954926490783691 + 2.0 * 6.383864879608154
Epoch 120, val loss: 1.8079687356948853
Epoch 130, training loss: 14.492323875427246 = 1.7875970602035522 + 2.0 * 6.352363586425781
Epoch 130, val loss: 1.8004578351974487
Epoch 140, training loss: 14.430879592895508 = 1.7797775268554688 + 2.0 * 6.3255510330200195
Epoch 140, val loss: 1.7930313348770142
Epoch 150, training loss: 14.375845909118652 = 1.771619439125061 + 2.0 * 6.302113056182861
Epoch 150, val loss: 1.7855089902877808
Epoch 160, training loss: 14.33010482788086 = 1.7627719640731812 + 2.0 * 6.283666610717773
Epoch 160, val loss: 1.7776999473571777
Epoch 170, training loss: 14.281120300292969 = 1.7530359029769897 + 2.0 * 6.264042377471924
Epoch 170, val loss: 1.7694077491760254
Epoch 180, training loss: 14.23726749420166 = 1.7421042919158936 + 2.0 * 6.247581481933594
Epoch 180, val loss: 1.7603620290756226
Epoch 190, training loss: 14.196127891540527 = 1.7296332120895386 + 2.0 * 6.23324728012085
Epoch 190, val loss: 1.7502533197402954
Epoch 200, training loss: 14.155047416687012 = 1.7153640985488892 + 2.0 * 6.219841480255127
Epoch 200, val loss: 1.7388421297073364
Epoch 210, training loss: 14.117019653320312 = 1.6988921165466309 + 2.0 * 6.20906400680542
Epoch 210, val loss: 1.7257413864135742
Epoch 220, training loss: 14.074869155883789 = 1.679853081703186 + 2.0 * 6.197507858276367
Epoch 220, val loss: 1.7107261419296265
Epoch 230, training loss: 14.03283977508545 = 1.657959222793579 + 2.0 * 6.187440395355225
Epoch 230, val loss: 1.6933414936065674
Epoch 240, training loss: 13.988319396972656 = 1.6325207948684692 + 2.0 * 6.177899360656738
Epoch 240, val loss: 1.6730067729949951
Epoch 250, training loss: 13.941607475280762 = 1.603217363357544 + 2.0 * 6.169195175170898
Epoch 250, val loss: 1.649388074874878
Epoch 260, training loss: 13.893792152404785 = 1.5700857639312744 + 2.0 * 6.161853313446045
Epoch 260, val loss: 1.6225254535675049
Epoch 270, training loss: 13.842758178710938 = 1.5331368446350098 + 2.0 * 6.154810905456543
Epoch 270, val loss: 1.5924961566925049
Epoch 280, training loss: 13.789780616760254 = 1.4929803609848022 + 2.0 * 6.14840030670166
Epoch 280, val loss: 1.5595431327819824
Epoch 290, training loss: 13.73598861694336 = 1.4501147270202637 + 2.0 * 6.142936706542969
Epoch 290, val loss: 1.5243124961853027
Epoch 300, training loss: 13.682628631591797 = 1.4059088230133057 + 2.0 * 6.138360023498535
Epoch 300, val loss: 1.487989902496338
Epoch 310, training loss: 13.626166343688965 = 1.3615655899047852 + 2.0 * 6.13230037689209
Epoch 310, val loss: 1.4517005681991577
Epoch 320, training loss: 13.577112197875977 = 1.3178917169570923 + 2.0 * 6.129610061645508
Epoch 320, val loss: 1.416216254234314
Epoch 330, training loss: 13.525773048400879 = 1.2756025791168213 + 2.0 * 6.125085353851318
Epoch 330, val loss: 1.382501482963562
Epoch 340, training loss: 13.476179122924805 = 1.2351226806640625 + 2.0 * 6.120528221130371
Epoch 340, val loss: 1.3508379459381104
Epoch 350, training loss: 13.430679321289062 = 1.1962454319000244 + 2.0 * 6.117217063903809
Epoch 350, val loss: 1.3209311962127686
Epoch 360, training loss: 13.383883476257324 = 1.1583813428878784 + 2.0 * 6.112751007080078
Epoch 360, val loss: 1.292344570159912
Epoch 370, training loss: 13.340059280395508 = 1.1211107969284058 + 2.0 * 6.109474182128906
Epoch 370, val loss: 1.264638066291809
Epoch 380, training loss: 13.304515838623047 = 1.0841126441955566 + 2.0 * 6.110201358795166
Epoch 380, val loss: 1.2377105951309204
Epoch 390, training loss: 13.257709503173828 = 1.0477242469787598 + 2.0 * 6.104992866516113
Epoch 390, val loss: 1.2113462686538696
Epoch 400, training loss: 13.214139938354492 = 1.0114784240722656 + 2.0 * 6.101330757141113
Epoch 400, val loss: 1.1852319240570068
Epoch 410, training loss: 13.183131217956543 = 0.9752727150917053 + 2.0 * 6.103929042816162
Epoch 410, val loss: 1.1592402458190918
Epoch 420, training loss: 13.13184642791748 = 0.9396480917930603 + 2.0 * 6.096099376678467
Epoch 420, val loss: 1.133835792541504
Epoch 430, training loss: 13.09587574005127 = 0.90456622838974 + 2.0 * 6.0956549644470215
Epoch 430, val loss: 1.1088659763336182
Epoch 440, training loss: 13.058314323425293 = 0.8702399730682373 + 2.0 * 6.094037055969238
Epoch 440, val loss: 1.0845906734466553
Epoch 450, training loss: 13.01661491394043 = 0.8368337154388428 + 2.0 * 6.089890480041504
Epoch 450, val loss: 1.0611438751220703
Epoch 460, training loss: 12.980241775512695 = 0.8041883111000061 + 2.0 * 6.088026523590088
Epoch 460, val loss: 1.0382694005966187
Epoch 470, training loss: 12.952985763549805 = 0.7722365260124207 + 2.0 * 6.09037446975708
Epoch 470, val loss: 1.0160906314849854
Epoch 480, training loss: 12.91225814819336 = 0.7411208152770996 + 2.0 * 6.085568428039551
Epoch 480, val loss: 0.994853138923645
Epoch 490, training loss: 12.88037395477295 = 0.7109706997871399 + 2.0 * 6.0847015380859375
Epoch 490, val loss: 0.9743918180465698
Epoch 500, training loss: 12.844339370727539 = 0.68165123462677 + 2.0 * 6.081344127655029
Epoch 500, val loss: 0.9546778798103333
Epoch 510, training loss: 12.829832077026367 = 0.6531230211257935 + 2.0 * 6.088354587554932
Epoch 510, val loss: 0.9357144832611084
Epoch 520, training loss: 12.78593921661377 = 0.6257748007774353 + 2.0 * 6.080082416534424
Epoch 520, val loss: 0.9177875518798828
Epoch 530, training loss: 12.753423690795898 = 0.5995025038719177 + 2.0 * 6.076960563659668
Epoch 530, val loss: 0.9008617401123047
Epoch 540, training loss: 12.724058151245117 = 0.5741424560546875 + 2.0 * 6.074957847595215
Epoch 540, val loss: 0.8847879767417908
Epoch 550, training loss: 12.709714889526367 = 0.5498185157775879 + 2.0 * 6.079948425292969
Epoch 550, val loss: 0.8696117997169495
Epoch 560, training loss: 12.676573753356934 = 0.5267934799194336 + 2.0 * 6.07489013671875
Epoch 560, val loss: 0.8558320999145508
Epoch 570, training loss: 12.647165298461914 = 0.5048646926879883 + 2.0 * 6.071150302886963
Epoch 570, val loss: 0.843202531337738
Epoch 580, training loss: 12.622699737548828 = 0.48391029238700867 + 2.0 * 6.069394588470459
Epoch 580, val loss: 0.8316336870193481
Epoch 590, training loss: 12.60720443725586 = 0.46393051743507385 + 2.0 * 6.071637153625488
Epoch 590, val loss: 0.821090042591095
Epoch 600, training loss: 12.585758209228516 = 0.4450679123401642 + 2.0 * 6.070344924926758
Epoch 600, val loss: 0.8117288947105408
Epoch 610, training loss: 12.559042930603027 = 0.42705532908439636 + 2.0 * 6.065993785858154
Epoch 610, val loss: 0.8034539818763733
Epoch 620, training loss: 12.540157318115234 = 0.4097314178943634 + 2.0 * 6.065212726593018
Epoch 620, val loss: 0.7960453033447266
Epoch 630, training loss: 12.523269653320312 = 0.393091082572937 + 2.0 * 6.065089225769043
Epoch 630, val loss: 0.7893906831741333
Epoch 640, training loss: 12.502744674682617 = 0.377096563577652 + 2.0 * 6.062824249267578
Epoch 640, val loss: 0.783602237701416
Epoch 650, training loss: 12.483549118041992 = 0.3617303967475891 + 2.0 * 6.060909271240234
Epoch 650, val loss: 0.7786564230918884
Epoch 660, training loss: 12.465916633605957 = 0.34683695435523987 + 2.0 * 6.059539794921875
Epoch 660, val loss: 0.7743131518363953
Epoch 670, training loss: 12.453194618225098 = 0.33243757486343384 + 2.0 * 6.060378551483154
Epoch 670, val loss: 0.7705769538879395
Epoch 680, training loss: 12.435873031616211 = 0.318535178899765 + 2.0 * 6.058669090270996
Epoch 680, val loss: 0.76739901304245
Epoch 690, training loss: 12.425226211547852 = 0.3052808940410614 + 2.0 * 6.059972763061523
Epoch 690, val loss: 0.7648016214370728
Epoch 700, training loss: 12.403350830078125 = 0.2925705015659332 + 2.0 * 6.055390357971191
Epoch 700, val loss: 0.7628685832023621
Epoch 710, training loss: 12.397034645080566 = 0.28034907579421997 + 2.0 * 6.058342933654785
Epoch 710, val loss: 0.7613710761070251
Epoch 720, training loss: 12.375779151916504 = 0.26862943172454834 + 2.0 * 6.053575038909912
Epoch 720, val loss: 0.7602559924125671
Epoch 730, training loss: 12.36168098449707 = 0.2573530077934265 + 2.0 * 6.052164077758789
Epoch 730, val loss: 0.7596136927604675
Epoch 740, training loss: 12.3609619140625 = 0.2464737445116043 + 2.0 * 6.057244300842285
Epoch 740, val loss: 0.7593173384666443
Epoch 750, training loss: 12.339849472045898 = 0.23610427975654602 + 2.0 * 6.051872730255127
Epoch 750, val loss: 0.7593179941177368
Epoch 760, training loss: 12.328841209411621 = 0.22609005868434906 + 2.0 * 6.051375389099121
Epoch 760, val loss: 0.7597358226776123
Epoch 770, training loss: 12.319119453430176 = 0.2164897471666336 + 2.0 * 6.051314830780029
Epoch 770, val loss: 0.760266125202179
Epoch 780, training loss: 12.302786827087402 = 0.2072683423757553 + 2.0 * 6.047759056091309
Epoch 780, val loss: 0.7611554861068726
Epoch 790, training loss: 12.293583869934082 = 0.19839425384998322 + 2.0 * 6.047595024108887
Epoch 790, val loss: 0.7623078227043152
Epoch 800, training loss: 12.283278465270996 = 0.18981793522834778 + 2.0 * 6.046730041503906
Epoch 800, val loss: 0.7636043429374695
Epoch 810, training loss: 12.278542518615723 = 0.18156029284000397 + 2.0 * 6.04849100112915
Epoch 810, val loss: 0.7650244832038879
Epoch 820, training loss: 12.268938064575195 = 0.17365539073944092 + 2.0 * 6.047641277313232
Epoch 820, val loss: 0.7667748332023621
Epoch 830, training loss: 12.255231857299805 = 0.16610658168792725 + 2.0 * 6.044562816619873
Epoch 830, val loss: 0.7686597108840942
Epoch 840, training loss: 12.24587631225586 = 0.15885543823242188 + 2.0 * 6.043510437011719
Epoch 840, val loss: 0.7707590460777283
Epoch 850, training loss: 12.24328327178955 = 0.15190988779067993 + 2.0 * 6.045686721801758
Epoch 850, val loss: 0.7727954387664795
Epoch 860, training loss: 12.227951049804688 = 0.14527414739131927 + 2.0 * 6.0413384437561035
Epoch 860, val loss: 0.7752909064292908
Epoch 870, training loss: 12.219537734985352 = 0.13893193006515503 + 2.0 * 6.040302753448486
Epoch 870, val loss: 0.777717113494873
Epoch 880, training loss: 12.215664863586426 = 0.13285590708255768 + 2.0 * 6.0414042472839355
Epoch 880, val loss: 0.7802321314811707
Epoch 890, training loss: 12.209076881408691 = 0.12707845866680145 + 2.0 * 6.040999412536621
Epoch 890, val loss: 0.7827564477920532
Epoch 900, training loss: 12.204305648803711 = 0.12160609662532806 + 2.0 * 6.0413498878479
Epoch 900, val loss: 0.785453200340271
Epoch 910, training loss: 12.190896034240723 = 0.11640395224094391 + 2.0 * 6.037246227264404
Epoch 910, val loss: 0.7882727384567261
Epoch 920, training loss: 12.185041427612305 = 0.11144822090864182 + 2.0 * 6.036796569824219
Epoch 920, val loss: 0.7911041378974915
Epoch 930, training loss: 12.193181037902832 = 0.10671208053827286 + 2.0 * 6.043234348297119
Epoch 930, val loss: 0.79400235414505
Epoch 940, training loss: 12.182366371154785 = 0.1022617518901825 + 2.0 * 6.04005241394043
Epoch 940, val loss: 0.796912670135498
Epoch 950, training loss: 12.168319702148438 = 0.09804181009531021 + 2.0 * 6.035139083862305
Epoch 950, val loss: 0.8000848293304443
Epoch 960, training loss: 12.162349700927734 = 0.09402123838663101 + 2.0 * 6.0341644287109375
Epoch 960, val loss: 0.8031174540519714
Epoch 970, training loss: 12.159831047058105 = 0.09019657969474792 + 2.0 * 6.034817218780518
Epoch 970, val loss: 0.8062528371810913
Epoch 980, training loss: 12.157350540161133 = 0.08658250421285629 + 2.0 * 6.035384178161621
Epoch 980, val loss: 0.8092730641365051
Epoch 990, training loss: 12.152985572814941 = 0.083160400390625 + 2.0 * 6.034912586212158
Epoch 990, val loss: 0.8125945329666138
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8037
Overall ASR: 0.6790
Flip ASR: 0.6311/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.69170379638672 = 1.943990707397461 + 2.0 * 8.373856544494629
Epoch 0, val loss: 1.9430180788040161
Epoch 10, training loss: 18.68052864074707 = 1.9336724281311035 + 2.0 * 8.373428344726562
Epoch 10, val loss: 1.933029294013977
Epoch 20, training loss: 18.662796020507812 = 1.9208908081054688 + 2.0 * 8.370952606201172
Epoch 20, val loss: 1.9202419519424438
Epoch 30, training loss: 18.60964584350586 = 1.9037665128707886 + 2.0 * 8.35293960571289
Epoch 30, val loss: 1.9028819799423218
Epoch 40, training loss: 18.243139266967773 = 1.882806420326233 + 2.0 * 8.180166244506836
Epoch 40, val loss: 1.8818202018737793
Epoch 50, training loss: 16.203603744506836 = 1.860524296760559 + 2.0 * 7.171539306640625
Epoch 50, val loss: 1.8594367504119873
Epoch 60, training loss: 15.763465881347656 = 1.8446097373962402 + 2.0 * 6.959428310394287
Epoch 60, val loss: 1.8440256118774414
Epoch 70, training loss: 15.350643157958984 = 1.834159255027771 + 2.0 * 6.758242130279541
Epoch 70, val loss: 1.833771824836731
Epoch 80, training loss: 15.087496757507324 = 1.8229459524154663 + 2.0 * 6.632275581359863
Epoch 80, val loss: 1.8227410316467285
Epoch 90, training loss: 14.918054580688477 = 1.8135706186294556 + 2.0 * 6.552241802215576
Epoch 90, val loss: 1.8138395547866821
Epoch 100, training loss: 14.765222549438477 = 1.8044562339782715 + 2.0 * 6.480382919311523
Epoch 100, val loss: 1.8051315546035767
Epoch 110, training loss: 14.636212348937988 = 1.7972707748413086 + 2.0 * 6.41947078704834
Epoch 110, val loss: 1.7981597185134888
Epoch 120, training loss: 14.53305435180664 = 1.7903423309326172 + 2.0 * 6.371356010437012
Epoch 120, val loss: 1.7912673950195312
Epoch 130, training loss: 14.451737403869629 = 1.7830522060394287 + 2.0 * 6.3343424797058105
Epoch 130, val loss: 1.7840007543563843
Epoch 140, training loss: 14.3856201171875 = 1.7751717567443848 + 2.0 * 6.305224418640137
Epoch 140, val loss: 1.77632474899292
Epoch 150, training loss: 14.327521324157715 = 1.766594648361206 + 2.0 * 6.280463218688965
Epoch 150, val loss: 1.7683392763137817
Epoch 160, training loss: 14.27597427368164 = 1.757248878479004 + 2.0 * 6.259362697601318
Epoch 160, val loss: 1.7599352598190308
Epoch 170, training loss: 14.22793960571289 = 1.7467293739318848 + 2.0 * 6.240604877471924
Epoch 170, val loss: 1.7508007287979126
Epoch 180, training loss: 14.18674373626709 = 1.7347182035446167 + 2.0 * 6.226012706756592
Epoch 180, val loss: 1.7405760288238525
Epoch 190, training loss: 14.141939163208008 = 1.7208778858184814 + 2.0 * 6.210530757904053
Epoch 190, val loss: 1.7290593385696411
Epoch 200, training loss: 14.100607872009277 = 1.704777717590332 + 2.0 * 6.197915077209473
Epoch 200, val loss: 1.7158204317092896
Epoch 210, training loss: 14.062520980834961 = 1.6858946084976196 + 2.0 * 6.188313007354736
Epoch 210, val loss: 1.700378179550171
Epoch 220, training loss: 14.020604133605957 = 1.6637862920761108 + 2.0 * 6.178409099578857
Epoch 220, val loss: 1.6824935674667358
Epoch 230, training loss: 13.978154182434082 = 1.6377170085906982 + 2.0 * 6.170218467712402
Epoch 230, val loss: 1.6612646579742432
Epoch 240, training loss: 13.93681812286377 = 1.6068919897079468 + 2.0 * 6.164963245391846
Epoch 240, val loss: 1.636200189590454
Epoch 250, training loss: 13.884639739990234 = 1.5709705352783203 + 2.0 * 6.156834602355957
Epoch 250, val loss: 1.606979489326477
Epoch 260, training loss: 13.832225799560547 = 1.5298014879226685 + 2.0 * 6.151212215423584
Epoch 260, val loss: 1.5734213590621948
Epoch 270, training loss: 13.783392906188965 = 1.4835885763168335 + 2.0 * 6.14990234375
Epoch 270, val loss: 1.5358225107192993
Epoch 280, training loss: 13.71870231628418 = 1.4341678619384766 + 2.0 * 6.142267227172852
Epoch 280, val loss: 1.4959161281585693
Epoch 290, training loss: 13.656412124633789 = 1.3823574781417847 + 2.0 * 6.137027263641357
Epoch 290, val loss: 1.4539844989776611
Epoch 300, training loss: 13.59430980682373 = 1.3287791013717651 + 2.0 * 6.132765293121338
Epoch 300, val loss: 1.4107553958892822
Epoch 310, training loss: 13.550668716430664 = 1.2749499082565308 + 2.0 * 6.137859344482422
Epoch 310, val loss: 1.3676068782806396
Epoch 320, training loss: 13.477557182312012 = 1.2231121063232422 + 2.0 * 6.127222537994385
Epoch 320, val loss: 1.3265074491500854
Epoch 330, training loss: 13.41660213470459 = 1.1731245517730713 + 2.0 * 6.121738910675049
Epoch 330, val loss: 1.286972165107727
Epoch 340, training loss: 13.363733291625977 = 1.1245335340499878 + 2.0 * 6.11959981918335
Epoch 340, val loss: 1.2487409114837646
Epoch 350, training loss: 13.316377639770508 = 1.0781060457229614 + 2.0 * 6.119135856628418
Epoch 350, val loss: 1.212634801864624
Epoch 360, training loss: 13.257705688476562 = 1.0340304374694824 + 2.0 * 6.111837387084961
Epoch 360, val loss: 1.1785157918930054
Epoch 370, training loss: 13.208380699157715 = 0.9914520382881165 + 2.0 * 6.108464241027832
Epoch 370, val loss: 1.1457200050354004
Epoch 380, training loss: 13.161770820617676 = 0.9504681825637817 + 2.0 * 6.105651378631592
Epoch 380, val loss: 1.1143877506256104
Epoch 390, training loss: 13.116485595703125 = 0.9112603664398193 + 2.0 * 6.102612495422363
Epoch 390, val loss: 1.0846121311187744
Epoch 400, training loss: 13.078370094299316 = 0.8741210103034973 + 2.0 * 6.1021246910095215
Epoch 400, val loss: 1.0568727254867554
Epoch 410, training loss: 13.03596305847168 = 0.8394249677658081 + 2.0 * 6.098268985748291
Epoch 410, val loss: 1.031388521194458
Epoch 420, training loss: 12.995735168457031 = 0.8063399791717529 + 2.0 * 6.09469747543335
Epoch 420, val loss: 1.0075029134750366
Epoch 430, training loss: 12.958507537841797 = 0.7747446894645691 + 2.0 * 6.091881275177002
Epoch 430, val loss: 0.9852797985076904
Epoch 440, training loss: 12.93244743347168 = 0.7446341514587402 + 2.0 * 6.093906879425049
Epoch 440, val loss: 0.9645717144012451
Epoch 450, training loss: 12.893070220947266 = 0.7163087725639343 + 2.0 * 6.088380813598633
Epoch 450, val loss: 0.9458633661270142
Epoch 460, training loss: 12.862436294555664 = 0.689285159111023 + 2.0 * 6.086575508117676
Epoch 460, val loss: 0.9285613298416138
Epoch 470, training loss: 12.832942008972168 = 0.6633855700492859 + 2.0 * 6.084778308868408
Epoch 470, val loss: 0.9125206470489502
Epoch 480, training loss: 12.803220748901367 = 0.6387345790863037 + 2.0 * 6.082242965698242
Epoch 480, val loss: 0.8979960083961487
Epoch 490, training loss: 12.774033546447754 = 0.6150655150413513 + 2.0 * 6.079483985900879
Epoch 490, val loss: 0.8846287131309509
Epoch 500, training loss: 12.756950378417969 = 0.5923264026641846 + 2.0 * 6.082312107086182
Epoch 500, val loss: 0.8724596500396729
Epoch 510, training loss: 12.724839210510254 = 0.5705820918083191 + 2.0 * 6.0771284103393555
Epoch 510, val loss: 0.8614747524261475
Epoch 520, training loss: 12.699164390563965 = 0.5496931672096252 + 2.0 * 6.074735641479492
Epoch 520, val loss: 0.8515884280204773
Epoch 530, training loss: 12.674412727355957 = 0.5296079516410828 + 2.0 * 6.072402477264404
Epoch 530, val loss: 0.8426528573036194
Epoch 540, training loss: 12.652334213256836 = 0.5102936029434204 + 2.0 * 6.071020126342773
Epoch 540, val loss: 0.8347326517105103
Epoch 550, training loss: 12.641866683959961 = 0.4916868209838867 + 2.0 * 6.075089931488037
Epoch 550, val loss: 0.8277020454406738
Epoch 560, training loss: 12.6137113571167 = 0.4739525616168976 + 2.0 * 6.069879531860352
Epoch 560, val loss: 0.8216956257820129
Epoch 570, training loss: 12.592452049255371 = 0.45697200298309326 + 2.0 * 6.067739963531494
Epoch 570, val loss: 0.8166594505310059
Epoch 580, training loss: 12.575689315795898 = 0.440679132938385 + 2.0 * 6.0675048828125
Epoch 580, val loss: 0.812491774559021
Epoch 590, training loss: 12.558247566223145 = 0.42518872022628784 + 2.0 * 6.066529273986816
Epoch 590, val loss: 0.8092155456542969
Epoch 600, training loss: 12.536652565002441 = 0.41047975420951843 + 2.0 * 6.06308650970459
Epoch 600, val loss: 0.8069199919700623
Epoch 610, training loss: 12.518205642700195 = 0.39649873971939087 + 2.0 * 6.060853481292725
Epoch 610, val loss: 0.8055232763290405
Epoch 620, training loss: 12.511902809143066 = 0.38328707218170166 + 2.0 * 6.064307689666748
Epoch 620, val loss: 0.8049256205558777
Epoch 630, training loss: 12.49311637878418 = 0.3709750473499298 + 2.0 * 6.061070442199707
Epoch 630, val loss: 0.8052699565887451
Epoch 640, training loss: 12.47800064086914 = 0.35938560962677 + 2.0 * 6.05930757522583
Epoch 640, val loss: 0.8063721656799316
Epoch 650, training loss: 12.463622093200684 = 0.3484867811203003 + 2.0 * 6.057567596435547
Epoch 650, val loss: 0.8080832958221436
Epoch 660, training loss: 12.449470520019531 = 0.33823782205581665 + 2.0 * 6.05561637878418
Epoch 660, val loss: 0.810478150844574
Epoch 670, training loss: 12.441998481750488 = 0.3285321891307831 + 2.0 * 6.056733131408691
Epoch 670, val loss: 0.813345730304718
Epoch 680, training loss: 12.427506446838379 = 0.31937453150749207 + 2.0 * 6.054066181182861
Epoch 680, val loss: 0.8165876269340515
Epoch 690, training loss: 12.419305801391602 = 0.31070032715797424 + 2.0 * 6.05430269241333
Epoch 690, val loss: 0.820359468460083
Epoch 700, training loss: 12.405966758728027 = 0.3024579584598541 + 2.0 * 6.051754474639893
Epoch 700, val loss: 0.824411153793335
Epoch 710, training loss: 12.396536827087402 = 0.29458746314048767 + 2.0 * 6.0509748458862305
Epoch 710, val loss: 0.8287924528121948
Epoch 720, training loss: 12.395220756530762 = 0.2870503067970276 + 2.0 * 6.0540852546691895
Epoch 720, val loss: 0.833308219909668
Epoch 730, training loss: 12.381672859191895 = 0.2798370122909546 + 2.0 * 6.050918102264404
Epoch 730, val loss: 0.8380942940711975
Epoch 740, training loss: 12.369929313659668 = 0.2728986144065857 + 2.0 * 6.048515319824219
Epoch 740, val loss: 0.842985212802887
Epoch 750, training loss: 12.361177444458008 = 0.26616978645324707 + 2.0 * 6.04750394821167
Epoch 750, val loss: 0.8480215668678284
Epoch 760, training loss: 12.365144729614258 = 0.25965067744255066 + 2.0 * 6.0527472496032715
Epoch 760, val loss: 0.8530450463294983
Epoch 770, training loss: 12.346183776855469 = 0.2533468008041382 + 2.0 * 6.0464186668396
Epoch 770, val loss: 0.8581441640853882
Epoch 780, training loss: 12.337068557739258 = 0.2472223937511444 + 2.0 * 6.044923305511475
Epoch 780, val loss: 0.8633500337600708
Epoch 790, training loss: 12.328277587890625 = 0.24120190739631653 + 2.0 * 6.043537616729736
Epoch 790, val loss: 0.8685829043388367
Epoch 800, training loss: 12.334809303283691 = 0.23528720438480377 + 2.0 * 6.049760818481445
Epoch 800, val loss: 0.8738730549812317
Epoch 810, training loss: 12.318203926086426 = 0.22946996986865997 + 2.0 * 6.044366836547852
Epoch 810, val loss: 0.8789920210838318
Epoch 820, training loss: 12.311527252197266 = 0.22373823821544647 + 2.0 * 6.043894290924072
Epoch 820, val loss: 0.8842545747756958
Epoch 830, training loss: 12.299286842346191 = 0.21807259321212769 + 2.0 * 6.04060697555542
Epoch 830, val loss: 0.8893468976020813
Epoch 840, training loss: 12.293025970458984 = 0.21245335042476654 + 2.0 * 6.040286540985107
Epoch 840, val loss: 0.8945202231407166
Epoch 850, training loss: 12.28575325012207 = 0.20686373114585876 + 2.0 * 6.039444923400879
Epoch 850, val loss: 0.8996672630310059
Epoch 860, training loss: 12.287887573242188 = 0.20130681991577148 + 2.0 * 6.043290138244629
Epoch 860, val loss: 0.9046186208724976
Epoch 870, training loss: 12.272992134094238 = 0.1957940012216568 + 2.0 * 6.038599014282227
Epoch 870, val loss: 0.9095974564552307
Epoch 880, training loss: 12.267547607421875 = 0.1903063803911209 + 2.0 * 6.038620471954346
Epoch 880, val loss: 0.9145072102546692
Epoch 890, training loss: 12.262325286865234 = 0.18483874201774597 + 2.0 * 6.038743495941162
Epoch 890, val loss: 0.9194376468658447
Epoch 900, training loss: 12.25321102142334 = 0.17939630150794983 + 2.0 * 6.036907196044922
Epoch 900, val loss: 0.9240850210189819
Epoch 910, training loss: 12.244454383850098 = 0.1739819049835205 + 2.0 * 6.035236358642578
Epoch 910, val loss: 0.9289141893386841
Epoch 920, training loss: 12.240766525268555 = 0.16860100626945496 + 2.0 * 6.036082744598389
Epoch 920, val loss: 0.9336363077163696
Epoch 930, training loss: 12.234784126281738 = 0.16324841976165771 + 2.0 * 6.035768032073975
Epoch 930, val loss: 0.9383143186569214
Epoch 940, training loss: 12.225862503051758 = 0.1579553335905075 + 2.0 * 6.033953666687012
Epoch 940, val loss: 0.9429505467414856
Epoch 950, training loss: 12.21921157836914 = 0.15272007882595062 + 2.0 * 6.03324556350708
Epoch 950, val loss: 0.9475638270378113
Epoch 960, training loss: 12.215974807739258 = 0.14754989743232727 + 2.0 * 6.034212589263916
Epoch 960, val loss: 0.9520834684371948
Epoch 970, training loss: 12.208941459655762 = 0.14246340095996857 + 2.0 * 6.033238887786865
Epoch 970, val loss: 0.9565443396568298
Epoch 980, training loss: 12.205970764160156 = 0.13749051094055176 + 2.0 * 6.034240245819092
Epoch 980, val loss: 0.9610804915428162
Epoch 990, training loss: 12.19626235961914 = 0.13264887034893036 + 2.0 * 6.031806945800781
Epoch 990, val loss: 0.9656297564506531
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7630
Overall ASR: 0.7565
Flip ASR: 0.7111/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.70118522644043 = 1.953597903251648 + 2.0 * 8.373793601989746
Epoch 0, val loss: 1.9587531089782715
Epoch 10, training loss: 18.68914222717285 = 1.9430135488510132 + 2.0 * 8.373064041137695
Epoch 10, val loss: 1.948704719543457
Epoch 20, training loss: 18.66799545288086 = 1.9300427436828613 + 2.0 * 8.368976593017578
Epoch 20, val loss: 1.9358984231948853
Epoch 30, training loss: 18.598100662231445 = 1.9128473997116089 + 2.0 * 8.342626571655273
Epoch 30, val loss: 1.9186618328094482
Epoch 40, training loss: 18.207361221313477 = 1.893061876296997 + 2.0 * 8.157149314880371
Epoch 40, val loss: 1.8991016149520874
Epoch 50, training loss: 16.593303680419922 = 1.8716610670089722 + 2.0 * 7.36082124710083
Epoch 50, val loss: 1.8775180578231812
Epoch 60, training loss: 15.886415481567383 = 1.8561934232711792 + 2.0 * 7.015110969543457
Epoch 60, val loss: 1.863969087600708
Epoch 70, training loss: 15.43492603302002 = 1.8453691005706787 + 2.0 * 6.794778347015381
Epoch 70, val loss: 1.8539782762527466
Epoch 80, training loss: 15.11033821105957 = 1.8335261344909668 + 2.0 * 6.638405799865723
Epoch 80, val loss: 1.842976450920105
Epoch 90, training loss: 14.885200500488281 = 1.823033332824707 + 2.0 * 6.531083583831787
Epoch 90, val loss: 1.832657814025879
Epoch 100, training loss: 14.739580154418945 = 1.8137840032577515 + 2.0 * 6.462898254394531
Epoch 100, val loss: 1.8231050968170166
Epoch 110, training loss: 14.62448501586914 = 1.80536687374115 + 2.0 * 6.40955924987793
Epoch 110, val loss: 1.8139853477478027
Epoch 120, training loss: 14.536602973937988 = 1.7974814176559448 + 2.0 * 6.369560718536377
Epoch 120, val loss: 1.8053138256072998
Epoch 130, training loss: 14.464305877685547 = 1.7898011207580566 + 2.0 * 6.337252140045166
Epoch 130, val loss: 1.7968741655349731
Epoch 140, training loss: 14.40256404876709 = 1.7823126316070557 + 2.0 * 6.310125827789307
Epoch 140, val loss: 1.7889254093170166
Epoch 150, training loss: 14.347756385803223 = 1.7745054960250854 + 2.0 * 6.286625385284424
Epoch 150, val loss: 1.7810434103012085
Epoch 160, training loss: 14.29951286315918 = 1.7659969329833984 + 2.0 * 6.266757965087891
Epoch 160, val loss: 1.7730193138122559
Epoch 170, training loss: 14.255441665649414 = 1.756580114364624 + 2.0 * 6.2494306564331055
Epoch 170, val loss: 1.7646571397781372
Epoch 180, training loss: 14.214064598083496 = 1.7459925413131714 + 2.0 * 6.234035968780518
Epoch 180, val loss: 1.7556284666061401
Epoch 190, training loss: 14.17823314666748 = 1.7338860034942627 + 2.0 * 6.222173690795898
Epoch 190, val loss: 1.7456120252609253
Epoch 200, training loss: 14.139057159423828 = 1.7200438976287842 + 2.0 * 6.209506511688232
Epoch 200, val loss: 1.7343554496765137
Epoch 210, training loss: 14.101472854614258 = 1.7040342092514038 + 2.0 * 6.198719501495361
Epoch 210, val loss: 1.7214422225952148
Epoch 220, training loss: 14.064619064331055 = 1.6853275299072266 + 2.0 * 6.189645767211914
Epoch 220, val loss: 1.7064095735549927
Epoch 230, training loss: 14.027015686035156 = 1.6635314226150513 + 2.0 * 6.181742191314697
Epoch 230, val loss: 1.6888387203216553
Epoch 240, training loss: 13.985027313232422 = 1.638308048248291 + 2.0 * 6.173359394073486
Epoch 240, val loss: 1.6685459613800049
Epoch 250, training loss: 13.941246032714844 = 1.6092581748962402 + 2.0 * 6.165994167327881
Epoch 250, val loss: 1.6450682878494263
Epoch 260, training loss: 13.896538734436035 = 1.5759748220443726 + 2.0 * 6.160282135009766
Epoch 260, val loss: 1.6180384159088135
Epoch 270, training loss: 13.847021102905273 = 1.538683533668518 + 2.0 * 6.154168605804443
Epoch 270, val loss: 1.5876891613006592
Epoch 280, training loss: 13.79732608795166 = 1.498076319694519 + 2.0 * 6.149624824523926
Epoch 280, val loss: 1.5543550252914429
Epoch 290, training loss: 13.74268913269043 = 1.4549988508224487 + 2.0 * 6.143845081329346
Epoch 290, val loss: 1.5188952684402466
Epoch 300, training loss: 13.691354751586914 = 1.410328984260559 + 2.0 * 6.140512943267822
Epoch 300, val loss: 1.4820948839187622
Epoch 310, training loss: 13.638971328735352 = 1.3652479648590088 + 2.0 * 6.136861801147461
Epoch 310, val loss: 1.445168375968933
Epoch 320, training loss: 13.58591365814209 = 1.320911169052124 + 2.0 * 6.132501125335693
Epoch 320, val loss: 1.4091589450836182
Epoch 330, training loss: 13.533321380615234 = 1.2776780128479004 + 2.0 * 6.127821922302246
Epoch 330, val loss: 1.3741142749786377
Epoch 340, training loss: 13.485490798950195 = 1.235451579093933 + 2.0 * 6.125019550323486
Epoch 340, val loss: 1.3402568101882935
Epoch 350, training loss: 13.440357208251953 = 1.194330096244812 + 2.0 * 6.123013496398926
Epoch 350, val loss: 1.3076903820037842
Epoch 360, training loss: 13.389981269836426 = 1.1547269821166992 + 2.0 * 6.117627143859863
Epoch 360, val loss: 1.2765977382659912
Epoch 370, training loss: 13.348821640014648 = 1.1160787343978882 + 2.0 * 6.1163716316223145
Epoch 370, val loss: 1.24640953540802
Epoch 380, training loss: 13.301776885986328 = 1.078441858291626 + 2.0 * 6.111667633056641
Epoch 380, val loss: 1.2173887491226196
Epoch 390, training loss: 13.26484489440918 = 1.0418304204940796 + 2.0 * 6.111507415771484
Epoch 390, val loss: 1.1892324686050415
Epoch 400, training loss: 13.218478202819824 = 1.0062531232833862 + 2.0 * 6.106112480163574
Epoch 400, val loss: 1.1621826887130737
Epoch 410, training loss: 13.176881790161133 = 0.9718624353408813 + 2.0 * 6.102509498596191
Epoch 410, val loss: 1.1360141038894653
Epoch 420, training loss: 13.137959480285645 = 0.938364565372467 + 2.0 * 6.099797248840332
Epoch 420, val loss: 1.1105866432189941
Epoch 430, training loss: 13.101571083068848 = 0.9055719971656799 + 2.0 * 6.097999572753906
Epoch 430, val loss: 1.0857394933700562
Epoch 440, training loss: 13.063431739807129 = 0.8734787702560425 + 2.0 * 6.094976425170898
Epoch 440, val loss: 1.0615757703781128
Epoch 450, training loss: 13.039884567260742 = 0.8421263694763184 + 2.0 * 6.098879337310791
Epoch 450, val loss: 1.0379849672317505
Epoch 460, training loss: 12.995365142822266 = 0.811739981174469 + 2.0 * 6.091812610626221
Epoch 460, val loss: 1.0152699947357178
Epoch 470, training loss: 12.958626747131348 = 0.782115638256073 + 2.0 * 6.088255405426025
Epoch 470, val loss: 0.9931493401527405
Epoch 480, training loss: 12.924115180969238 = 0.7529180645942688 + 2.0 * 6.085598468780518
Epoch 480, val loss: 0.9714294672012329
Epoch 490, training loss: 12.906439781188965 = 0.724252462387085 + 2.0 * 6.09109354019165
Epoch 490, val loss: 0.9500823020935059
Epoch 500, training loss: 12.867371559143066 = 0.6964074373245239 + 2.0 * 6.085482120513916
Epoch 500, val loss: 0.929819643497467
Epoch 510, training loss: 12.83160400390625 = 0.6693740487098694 + 2.0 * 6.081114768981934
Epoch 510, val loss: 0.9102544188499451
Epoch 520, training loss: 12.800591468811035 = 0.643013060092926 + 2.0 * 6.078789234161377
Epoch 520, val loss: 0.8916168212890625
Epoch 530, training loss: 12.769966125488281 = 0.6171748638153076 + 2.0 * 6.076395511627197
Epoch 530, val loss: 0.8735201954841614
Epoch 540, training loss: 12.749856948852539 = 0.591947078704834 + 2.0 * 6.078955173492432
Epoch 540, val loss: 0.8562106490135193
Epoch 550, training loss: 12.718534469604492 = 0.567577600479126 + 2.0 * 6.075478553771973
Epoch 550, val loss: 0.8398711085319519
Epoch 560, training loss: 12.687137603759766 = 0.5438934564590454 + 2.0 * 6.071621894836426
Epoch 560, val loss: 0.8243571519851685
Epoch 570, training loss: 12.678553581237793 = 0.5208225846290588 + 2.0 * 6.0788655281066895
Epoch 570, val loss: 0.8095412850379944
Epoch 580, training loss: 12.63735580444336 = 0.4985041916370392 + 2.0 * 6.069425582885742
Epoch 580, val loss: 0.7957547307014465
Epoch 590, training loss: 12.612753868103027 = 0.47693535685539246 + 2.0 * 6.067909240722656
Epoch 590, val loss: 0.7828313708305359
Epoch 600, training loss: 12.587870597839355 = 0.45577847957611084 + 2.0 * 6.066046237945557
Epoch 600, val loss: 0.7704803943634033
Epoch 610, training loss: 12.571624755859375 = 0.43501871824264526 + 2.0 * 6.068303108215332
Epoch 610, val loss: 0.758836567401886
Epoch 620, training loss: 12.551279067993164 = 0.41493406891822815 + 2.0 * 6.068172454833984
Epoch 620, val loss: 0.7478485107421875
Epoch 630, training loss: 12.521672248840332 = 0.3954038619995117 + 2.0 * 6.06313419342041
Epoch 630, val loss: 0.7376875281333923
Epoch 640, training loss: 12.515800476074219 = 0.37635135650634766 + 2.0 * 6.0697245597839355
Epoch 640, val loss: 0.7281203269958496
Epoch 650, training loss: 12.482142448425293 = 0.3581606447696686 + 2.0 * 6.061990737915039
Epoch 650, val loss: 0.7191749811172485
Epoch 660, training loss: 12.460586547851562 = 0.34060972929000854 + 2.0 * 6.059988498687744
Epoch 660, val loss: 0.7112563252449036
Epoch 670, training loss: 12.439717292785645 = 0.32360532879829407 + 2.0 * 6.058055877685547
Epoch 670, val loss: 0.703788161277771
Epoch 680, training loss: 12.420103073120117 = 0.30712535977363586 + 2.0 * 6.056488990783691
Epoch 680, val loss: 0.696893572807312
Epoch 690, training loss: 12.402850151062012 = 0.291206955909729 + 2.0 * 6.055821418762207
Epoch 690, val loss: 0.6905826926231384
Epoch 700, training loss: 12.390542030334473 = 0.2759697139263153 + 2.0 * 6.057286262512207
Epoch 700, val loss: 0.684765100479126
Epoch 710, training loss: 12.379525184631348 = 0.26156672835350037 + 2.0 * 6.058979034423828
Epoch 710, val loss: 0.6796743273735046
Epoch 720, training loss: 12.35440731048584 = 0.24793964624404907 + 2.0 * 6.053233623504639
Epoch 720, val loss: 0.6751216053962708
Epoch 730, training loss: 12.337799072265625 = 0.2349787801504135 + 2.0 * 6.05141019821167
Epoch 730, val loss: 0.6711058020591736
Epoch 740, training loss: 12.326654434204102 = 0.2226330041885376 + 2.0 * 6.052010536193848
Epoch 740, val loss: 0.6675216555595398
Epoch 750, training loss: 12.322818756103516 = 0.2110026478767395 + 2.0 * 6.055908203125
Epoch 750, val loss: 0.6644028425216675
Epoch 760, training loss: 12.300456047058105 = 0.20009197294712067 + 2.0 * 6.050181865692139
Epoch 760, val loss: 0.6617993712425232
Epoch 770, training loss: 12.286972999572754 = 0.18983949720859528 + 2.0 * 6.048566818237305
Epoch 770, val loss: 0.6596660017967224
Epoch 780, training loss: 12.274787902832031 = 0.18008317053318024 + 2.0 * 6.047352313995361
Epoch 780, val loss: 0.6578100919723511
Epoch 790, training loss: 12.276701927185059 = 0.17090362310409546 + 2.0 * 6.052899360656738
Epoch 790, val loss: 0.6562718152999878
Epoch 800, training loss: 12.2544584274292 = 0.16230247914791107 + 2.0 * 6.046078205108643
Epoch 800, val loss: 0.6551702618598938
Epoch 810, training loss: 12.245909690856934 = 0.15420645475387573 + 2.0 * 6.045851707458496
Epoch 810, val loss: 0.6544415950775146
Epoch 820, training loss: 12.235321044921875 = 0.14659751951694489 + 2.0 * 6.044361591339111
Epoch 820, val loss: 0.6539269685745239
Epoch 830, training loss: 12.226730346679688 = 0.13943009078502655 + 2.0 * 6.043650150299072
Epoch 830, val loss: 0.6538030505180359
Epoch 840, training loss: 12.22114086151123 = 0.13267844915390015 + 2.0 * 6.044231414794922
Epoch 840, val loss: 0.6538727283477783
Epoch 850, training loss: 12.219776153564453 = 0.1263238936662674 + 2.0 * 6.046726226806641
Epoch 850, val loss: 0.6542335152626038
Epoch 860, training loss: 12.205085754394531 = 0.12038853764533997 + 2.0 * 6.042348384857178
Epoch 860, val loss: 0.6549525260925293
Epoch 870, training loss: 12.194068908691406 = 0.11476586759090424 + 2.0 * 6.039651393890381
Epoch 870, val loss: 0.6558102369308472
Epoch 880, training loss: 12.188186645507812 = 0.10945766419172287 + 2.0 * 6.039364337921143
Epoch 880, val loss: 0.6569088101387024
Epoch 890, training loss: 12.20784854888916 = 0.10444700717926025 + 2.0 * 6.051700592041016
Epoch 890, val loss: 0.6581952571868896
Epoch 900, training loss: 12.177712440490723 = 0.09984015673398972 + 2.0 * 6.038936138153076
Epoch 900, val loss: 0.6595699787139893
Epoch 910, training loss: 12.171205520629883 = 0.0954926386475563 + 2.0 * 6.037856578826904
Epoch 910, val loss: 0.6613614559173584
Epoch 920, training loss: 12.165716171264648 = 0.09136609733104706 + 2.0 * 6.037175178527832
Epoch 920, val loss: 0.6631590127944946
Epoch 930, training loss: 12.159512519836426 = 0.08748378604650497 + 2.0 * 6.036014556884766
Epoch 930, val loss: 0.665049135684967
Epoch 940, training loss: 12.154129981994629 = 0.08382507413625717 + 2.0 * 6.035152435302734
Epoch 940, val loss: 0.6671724319458008
Epoch 950, training loss: 12.148451805114746 = 0.08037876337766647 + 2.0 * 6.034036636352539
Epoch 950, val loss: 0.6693971753120422
Epoch 960, training loss: 12.150307655334473 = 0.0770932137966156 + 2.0 * 6.036607265472412
Epoch 960, val loss: 0.6717032194137573
Epoch 970, training loss: 12.149620056152344 = 0.0740327462553978 + 2.0 * 6.0377936363220215
Epoch 970, val loss: 0.6739473938941956
Epoch 980, training loss: 12.13804817199707 = 0.07115749269723892 + 2.0 * 6.033445358276367
Epoch 980, val loss: 0.6764804124832153
Epoch 990, training loss: 12.132868766784668 = 0.06843356043100357 + 2.0 * 6.032217502593994
Epoch 990, val loss: 0.6790928840637207
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.9299
Flip ASR: 0.9156/225 nodes
The final ASR:0.78844, 0.10490, Accuracy:0.79506, 0.02349
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11630])
remove edge: torch.Size([2, 9424])
updated graph: torch.Size([2, 10498])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9852
Flip ASR: 0.9822/225 nodes
The final ASR:0.97909, 0.00460, Accuracy:0.83333, 0.00800
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.5 der2:0.4 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 18.69761848449707 = 1.949890375137329 + 2.0 * 8.37386417388916
Epoch 0, val loss: 1.9531939029693604
Epoch 10, training loss: 18.68512535095215 = 1.9385932683944702 + 2.0 * 8.373266220092773
Epoch 10, val loss: 1.9415181875228882
Epoch 20, training loss: 18.663206100463867 = 1.923761248588562 + 2.0 * 8.369722366333008
Epoch 20, val loss: 1.926011323928833
Epoch 30, training loss: 18.59815788269043 = 1.9030232429504395 + 2.0 * 8.347567558288574
Epoch 30, val loss: 1.9046664237976074
Epoch 40, training loss: 18.28089714050293 = 1.8794783353805542 + 2.0 * 8.200709342956543
Epoch 40, val loss: 1.8822753429412842
Epoch 50, training loss: 17.316120147705078 = 1.854151725769043 + 2.0 * 7.730984210968018
Epoch 50, val loss: 1.858244776725769
Epoch 60, training loss: 16.42301368713379 = 1.8341087102890015 + 2.0 * 7.29445219039917
Epoch 60, val loss: 1.8406599760055542
Epoch 70, training loss: 15.617396354675293 = 1.8215906620025635 + 2.0 * 6.897902965545654
Epoch 70, val loss: 1.8296689987182617
Epoch 80, training loss: 15.178231239318848 = 1.8095778226852417 + 2.0 * 6.684326648712158
Epoch 80, val loss: 1.819120168685913
Epoch 90, training loss: 14.985574722290039 = 1.7944196462631226 + 2.0 * 6.595577716827393
Epoch 90, val loss: 1.8054975271224976
Epoch 100, training loss: 14.828689575195312 = 1.778250813484192 + 2.0 * 6.525219440460205
Epoch 100, val loss: 1.7913298606872559
Epoch 110, training loss: 14.705011367797852 = 1.7639515399932861 + 2.0 * 6.470530033111572
Epoch 110, val loss: 1.7790911197662354
Epoch 120, training loss: 14.597413063049316 = 1.7507489919662476 + 2.0 * 6.423332214355469
Epoch 120, val loss: 1.7679334878921509
Epoch 130, training loss: 14.511611938476562 = 1.7369142770767212 + 2.0 * 6.387348651885986
Epoch 130, val loss: 1.7563625574111938
Epoch 140, training loss: 14.433355331420898 = 1.7217326164245605 + 2.0 * 6.35581111907959
Epoch 140, val loss: 1.7439351081848145
Epoch 150, training loss: 14.363142013549805 = 1.7049939632415771 + 2.0 * 6.329073905944824
Epoch 150, val loss: 1.730548620223999
Epoch 160, training loss: 14.300771713256836 = 1.6864460706710815 + 2.0 * 6.307162761688232
Epoch 160, val loss: 1.7160779237747192
Epoch 170, training loss: 14.240814208984375 = 1.6659483909606934 + 2.0 * 6.28743314743042
Epoch 170, val loss: 1.700241208076477
Epoch 180, training loss: 14.184375762939453 = 1.6432969570159912 + 2.0 * 6.270539283752441
Epoch 180, val loss: 1.682822823524475
Epoch 190, training loss: 14.129592895507812 = 1.6182349920272827 + 2.0 * 6.255679130554199
Epoch 190, val loss: 1.6635899543762207
Epoch 200, training loss: 14.074875831604004 = 1.5905970335006714 + 2.0 * 6.2421393394470215
Epoch 200, val loss: 1.6422871351242065
Epoch 210, training loss: 14.021583557128906 = 1.5604192018508911 + 2.0 * 6.230582237243652
Epoch 210, val loss: 1.6192950010299683
Epoch 220, training loss: 13.964280128479004 = 1.5278006792068481 + 2.0 * 6.218239784240723
Epoch 220, val loss: 1.5944308042526245
Epoch 230, training loss: 13.909605979919434 = 1.493013620376587 + 2.0 * 6.208296298980713
Epoch 230, val loss: 1.5680334568023682
Epoch 240, training loss: 13.852855682373047 = 1.4560595750808716 + 2.0 * 6.198398113250732
Epoch 240, val loss: 1.5399974584579468
Epoch 250, training loss: 13.796579360961914 = 1.4174729585647583 + 2.0 * 6.189553260803223
Epoch 250, val loss: 1.5110887289047241
Epoch 260, training loss: 13.740485191345215 = 1.377676010131836 + 2.0 * 6.1814045906066895
Epoch 260, val loss: 1.481540322303772
Epoch 270, training loss: 13.686185836791992 = 1.3373385667800903 + 2.0 * 6.174423694610596
Epoch 270, val loss: 1.4517258405685425
Epoch 280, training loss: 13.632843971252441 = 1.2968552112579346 + 2.0 * 6.167994499206543
Epoch 280, val loss: 1.4220738410949707
Epoch 290, training loss: 13.579549789428711 = 1.2566170692443848 + 2.0 * 6.161466598510742
Epoch 290, val loss: 1.3930370807647705
Epoch 300, training loss: 13.52777099609375 = 1.2170400619506836 + 2.0 * 6.155365467071533
Epoch 300, val loss: 1.3643920421600342
Epoch 310, training loss: 13.477439880371094 = 1.1783312559127808 + 2.0 * 6.149554252624512
Epoch 310, val loss: 1.3370022773742676
Epoch 320, training loss: 13.431882858276367 = 1.1411513090133667 + 2.0 * 6.1453657150268555
Epoch 320, val loss: 1.3109681606292725
Epoch 330, training loss: 13.387816429138184 = 1.105844497680664 + 2.0 * 6.14098596572876
Epoch 330, val loss: 1.2867355346679688
Epoch 340, training loss: 13.341280937194824 = 1.072348952293396 + 2.0 * 6.134466171264648
Epoch 340, val loss: 1.2640796899795532
Epoch 350, training loss: 13.303559303283691 = 1.0403064489364624 + 2.0 * 6.131626605987549
Epoch 350, val loss: 1.2425094842910767
Epoch 360, training loss: 13.26555061340332 = 1.0098048448562622 + 2.0 * 6.127872943878174
Epoch 360, val loss: 1.2222468852996826
Epoch 370, training loss: 13.224178314208984 = 0.9806260466575623 + 2.0 * 6.121776103973389
Epoch 370, val loss: 1.2028599977493286
Epoch 380, training loss: 13.189498901367188 = 0.9521733522415161 + 2.0 * 6.1186628341674805
Epoch 380, val loss: 1.184136152267456
Epoch 390, training loss: 13.15464973449707 = 0.9241920113563538 + 2.0 * 6.115228652954102
Epoch 390, val loss: 1.1657562255859375
Epoch 400, training loss: 13.12065315246582 = 0.8963911533355713 + 2.0 * 6.112131118774414
Epoch 400, val loss: 1.1474412679672241
Epoch 410, training loss: 13.087030410766602 = 0.8684925436973572 + 2.0 * 6.109269142150879
Epoch 410, val loss: 1.128763198852539
Epoch 420, training loss: 13.052702903747559 = 0.840291440486908 + 2.0 * 6.106205940246582
Epoch 420, val loss: 1.1097428798675537
Epoch 430, training loss: 13.017684936523438 = 0.8115566372871399 + 2.0 * 6.103064060211182
Epoch 430, val loss: 1.090141773223877
Epoch 440, training loss: 12.993722915649414 = 0.7822163105010986 + 2.0 * 6.105753421783447
Epoch 440, val loss: 1.0698796510696411
Epoch 450, training loss: 12.94981861114502 = 0.7524221539497375 + 2.0 * 6.098698139190674
Epoch 450, val loss: 1.0494894981384277
Epoch 460, training loss: 12.913777351379395 = 0.7223961353302002 + 2.0 * 6.095690727233887
Epoch 460, val loss: 1.028990626335144
Epoch 470, training loss: 12.87903118133545 = 0.6922420859336853 + 2.0 * 6.093394756317139
Epoch 470, val loss: 1.008575201034546
Epoch 480, training loss: 12.855123519897461 = 0.6621904373168945 + 2.0 * 6.096466541290283
Epoch 480, val loss: 0.9883845448493958
Epoch 490, training loss: 12.811702728271484 = 0.6327498555183411 + 2.0 * 6.089476585388184
Epoch 490, val loss: 0.9689734578132629
Epoch 500, training loss: 12.780060768127441 = 0.6039432287216187 + 2.0 * 6.088058948516846
Epoch 500, val loss: 0.9503539800643921
Epoch 510, training loss: 12.761849403381348 = 0.5759477019309998 + 2.0 * 6.092950820922852
Epoch 510, val loss: 0.9327329397201538
Epoch 520, training loss: 12.719462394714355 = 0.5490332245826721 + 2.0 * 6.085214614868164
Epoch 520, val loss: 0.916474461555481
Epoch 530, training loss: 12.688536643981934 = 0.5232221484184265 + 2.0 * 6.082657337188721
Epoch 530, val loss: 0.9015133380889893
Epoch 540, training loss: 12.661513328552246 = 0.49840766191482544 + 2.0 * 6.081552982330322
Epoch 540, val loss: 0.8877363801002502
Epoch 550, training loss: 12.639876365661621 = 0.4745781421661377 + 2.0 * 6.082649230957031
Epoch 550, val loss: 0.87505042552948
Epoch 560, training loss: 12.609945297241211 = 0.45177698135375977 + 2.0 * 6.079084396362305
Epoch 560, val loss: 0.8636195063591003
Epoch 570, training loss: 12.583040237426758 = 0.42989078164100647 + 2.0 * 6.076574802398682
Epoch 570, val loss: 0.8531783223152161
Epoch 580, training loss: 12.567023277282715 = 0.408875048160553 + 2.0 * 6.079073905944824
Epoch 580, val loss: 0.843686580657959
Epoch 590, training loss: 12.538677215576172 = 0.38866162300109863 + 2.0 * 6.075007915496826
Epoch 590, val loss: 0.835157036781311
Epoch 600, training loss: 12.512561798095703 = 0.3692916929721832 + 2.0 * 6.0716352462768555
Epoch 600, val loss: 0.8273913264274597
Epoch 610, training loss: 12.494668960571289 = 0.35058313608169556 + 2.0 * 6.072042942047119
Epoch 610, val loss: 0.8202291131019592
Epoch 620, training loss: 12.474822998046875 = 0.3325577676296234 + 2.0 * 6.071132659912109
Epoch 620, val loss: 0.8137130737304688
Epoch 630, training loss: 12.453962326049805 = 0.3151960074901581 + 2.0 * 6.069383144378662
Epoch 630, val loss: 0.8079193234443665
Epoch 640, training loss: 12.431676864624023 = 0.29840967059135437 + 2.0 * 6.066633701324463
Epoch 640, val loss: 0.8027569055557251
Epoch 650, training loss: 12.413254737854004 = 0.28216552734375 + 2.0 * 6.065544605255127
Epoch 650, val loss: 0.7980992794036865
Epoch 660, training loss: 12.398480415344238 = 0.2664429843425751 + 2.0 * 6.066018581390381
Epoch 660, val loss: 0.793820321559906
Epoch 670, training loss: 12.379916191101074 = 0.25132185220718384 + 2.0 * 6.064297199249268
Epoch 670, val loss: 0.7903232574462891
Epoch 680, training loss: 12.363639831542969 = 0.23683738708496094 + 2.0 * 6.063401222229004
Epoch 680, val loss: 0.7874484062194824
Epoch 690, training loss: 12.34506607055664 = 0.22301146388053894 + 2.0 * 6.061027526855469
Epoch 690, val loss: 0.7850790023803711
Epoch 700, training loss: 12.328774452209473 = 0.20984184741973877 + 2.0 * 6.059466361999512
Epoch 700, val loss: 0.7836329936981201
Epoch 710, training loss: 12.317181587219238 = 0.19736582040786743 + 2.0 * 6.059907913208008
Epoch 710, val loss: 0.7827792167663574
Epoch 720, training loss: 12.303339958190918 = 0.18561118841171265 + 2.0 * 6.058864593505859
Epoch 720, val loss: 0.7825738191604614
Epoch 730, training loss: 12.288739204406738 = 0.1746106594800949 + 2.0 * 6.057064056396484
Epoch 730, val loss: 0.7831649780273438
Epoch 740, training loss: 12.274463653564453 = 0.16432705521583557 + 2.0 * 6.055068492889404
Epoch 740, val loss: 0.7844670414924622
Epoch 750, training loss: 12.268608093261719 = 0.15474480390548706 + 2.0 * 6.056931495666504
Epoch 750, val loss: 0.7862511277198792
Epoch 760, training loss: 12.26318359375 = 0.14585928618907928 + 2.0 * 6.058661937713623
Epoch 760, val loss: 0.788676917552948
Epoch 770, training loss: 12.24333381652832 = 0.13762351870536804 + 2.0 * 6.052855014801025
Epoch 770, val loss: 0.7916496992111206
Epoch 780, training loss: 12.233283996582031 = 0.1300029754638672 + 2.0 * 6.051640510559082
Epoch 780, val loss: 0.7951270341873169
Epoch 790, training loss: 12.231797218322754 = 0.12294076383113861 + 2.0 * 6.0544281005859375
Epoch 790, val loss: 0.7990667223930359
Epoch 800, training loss: 12.218945503234863 = 0.11639110743999481 + 2.0 * 6.051277160644531
Epoch 800, val loss: 0.8032580614089966
Epoch 810, training loss: 12.208189010620117 = 0.11028410494327545 + 2.0 * 6.048952579498291
Epoch 810, val loss: 0.8078586459159851
Epoch 820, training loss: 12.20067310333252 = 0.10461924970149994 + 2.0 * 6.048027038574219
Epoch 820, val loss: 0.8128376603126526
Epoch 830, training loss: 12.204444885253906 = 0.09934795647859573 + 2.0 * 6.052548408508301
Epoch 830, val loss: 0.8180032968521118
Epoch 840, training loss: 12.191859245300293 = 0.09444369375705719 + 2.0 * 6.048707962036133
Epoch 840, val loss: 0.8234098553657532
Epoch 850, training loss: 12.18486213684082 = 0.08987993746995926 + 2.0 * 6.047491073608398
Epoch 850, val loss: 0.828910231590271
Epoch 860, training loss: 12.174427032470703 = 0.0856383666396141 + 2.0 * 6.044394493103027
Epoch 860, val loss: 0.8349135518074036
Epoch 870, training loss: 12.17344856262207 = 0.08167048543691635 + 2.0 * 6.045888900756836
Epoch 870, val loss: 0.8408443927764893
Epoch 880, training loss: 12.16330623626709 = 0.07796203345060349 + 2.0 * 6.042672157287598
Epoch 880, val loss: 0.8467328548431396
Epoch 890, training loss: 12.158783912658691 = 0.07448365539312363 + 2.0 * 6.042150020599365
Epoch 890, val loss: 0.8531107902526855
Epoch 900, training loss: 12.152887344360352 = 0.07123265415430069 + 2.0 * 6.04082727432251
Epoch 900, val loss: 0.8592597246170044
Epoch 910, training loss: 12.158037185668945 = 0.06817587465047836 + 2.0 * 6.044930458068848
Epoch 910, val loss: 0.8655925989151001
Epoch 920, training loss: 12.147093772888184 = 0.06529975682497025 + 2.0 * 6.040896892547607
Epoch 920, val loss: 0.8717749714851379
Epoch 930, training loss: 12.140079498291016 = 0.06260788440704346 + 2.0 * 6.038735866546631
Epoch 930, val loss: 0.8782821297645569
Epoch 940, training loss: 12.161828994750977 = 0.060089111328125 + 2.0 * 6.050869941711426
Epoch 940, val loss: 0.8846055269241333
Epoch 950, training loss: 12.133906364440918 = 0.05769883096218109 + 2.0 * 6.0381035804748535
Epoch 950, val loss: 0.8908326029777527
Epoch 960, training loss: 12.129080772399902 = 0.05546162649989128 + 2.0 * 6.03680944442749
Epoch 960, val loss: 0.8974263668060303
Epoch 970, training loss: 12.123601913452148 = 0.05335763469338417 + 2.0 * 6.035121917724609
Epoch 970, val loss: 0.9038733243942261
Epoch 980, training loss: 12.133166313171387 = 0.05136384814977646 + 2.0 * 6.040901184082031
Epoch 980, val loss: 0.9101210236549377
Epoch 990, training loss: 12.12261962890625 = 0.04948262870311737 + 2.0 * 6.036568641662598
Epoch 990, val loss: 0.916473925113678
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.5867
Flip ASR: 0.5111/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.67987823486328 = 1.9321452379226685 + 2.0 * 8.373866081237793
Epoch 0, val loss: 1.93060302734375
Epoch 10, training loss: 18.668373107910156 = 1.9217654466629028 + 2.0 * 8.373303413391113
Epoch 10, val loss: 1.9198683500289917
Epoch 20, training loss: 18.649051666259766 = 1.9090014696121216 + 2.0 * 8.370024681091309
Epoch 20, val loss: 1.9064232110977173
Epoch 30, training loss: 18.592178344726562 = 1.8916449546813965 + 2.0 * 8.350266456604004
Epoch 30, val loss: 1.8880698680877686
Epoch 40, training loss: 18.288076400756836 = 1.8707644939422607 + 2.0 * 8.208656311035156
Epoch 40, val loss: 1.8668665885925293
Epoch 50, training loss: 16.991207122802734 = 1.8473846912384033 + 2.0 * 7.571911334991455
Epoch 50, val loss: 1.8433403968811035
Epoch 60, training loss: 16.302371978759766 = 1.8287949562072754 + 2.0 * 7.236788272857666
Epoch 60, val loss: 1.8253257274627686
Epoch 70, training loss: 15.752442359924316 = 1.8153789043426514 + 2.0 * 6.968531608581543
Epoch 70, val loss: 1.8116644620895386
Epoch 80, training loss: 15.450414657592773 = 1.8004435300827026 + 2.0 * 6.824985504150391
Epoch 80, val loss: 1.797070860862732
Epoch 90, training loss: 15.175322532653809 = 1.7844833135604858 + 2.0 * 6.695419788360596
Epoch 90, val loss: 1.7822691202163696
Epoch 100, training loss: 14.975561141967773 = 1.7706177234649658 + 2.0 * 6.602471828460693
Epoch 100, val loss: 1.769225001335144
Epoch 110, training loss: 14.808675765991211 = 1.7569489479064941 + 2.0 * 6.525863170623779
Epoch 110, val loss: 1.7563592195510864
Epoch 120, training loss: 14.680135726928711 = 1.742175579071045 + 2.0 * 6.468980312347412
Epoch 120, val loss: 1.7428756952285767
Epoch 130, training loss: 14.577848434448242 = 1.7258524894714355 + 2.0 * 6.425998210906982
Epoch 130, val loss: 1.7283509969711304
Epoch 140, training loss: 14.495284080505371 = 1.7075437307357788 + 2.0 * 6.3938703536987305
Epoch 140, val loss: 1.7125794887542725
Epoch 150, training loss: 14.41944408416748 = 1.68710196018219 + 2.0 * 6.366170883178711
Epoch 150, val loss: 1.6953465938568115
Epoch 160, training loss: 14.349661827087402 = 1.6642245054244995 + 2.0 * 6.342718601226807
Epoch 160, val loss: 1.6765671968460083
Epoch 170, training loss: 14.283910751342773 = 1.6384072303771973 + 2.0 * 6.322751998901367
Epoch 170, val loss: 1.6560107469558716
Epoch 180, training loss: 14.223016738891602 = 1.6095044612884521 + 2.0 * 6.306756019592285
Epoch 180, val loss: 1.633323073387146
Epoch 190, training loss: 14.153454780578613 = 1.5775126218795776 + 2.0 * 6.287971019744873
Epoch 190, val loss: 1.608858346939087
Epoch 200, training loss: 14.086946487426758 = 1.5426113605499268 + 2.0 * 6.272167682647705
Epoch 200, val loss: 1.5823341608047485
Epoch 210, training loss: 14.021818161010742 = 1.504638433456421 + 2.0 * 6.258589744567871
Epoch 210, val loss: 1.553804636001587
Epoch 220, training loss: 13.958032608032227 = 1.4642188549041748 + 2.0 * 6.246906757354736
Epoch 220, val loss: 1.5239031314849854
Epoch 230, training loss: 13.895759582519531 = 1.4226057529449463 + 2.0 * 6.236577033996582
Epoch 230, val loss: 1.4936785697937012
Epoch 240, training loss: 13.835058212280273 = 1.3803486824035645 + 2.0 * 6.227354526519775
Epoch 240, val loss: 1.4637067317962646
Epoch 250, training loss: 13.776020050048828 = 1.3383033275604248 + 2.0 * 6.218858242034912
Epoch 250, val loss: 1.4347007274627686
Epoch 260, training loss: 13.720229148864746 = 1.2973498106002808 + 2.0 * 6.211439609527588
Epoch 260, val loss: 1.4072452783584595
Epoch 270, training loss: 13.664388656616211 = 1.25747549533844 + 2.0 * 6.203456401824951
Epoch 270, val loss: 1.3812674283981323
Epoch 280, training loss: 13.624205589294434 = 1.2185906171798706 + 2.0 * 6.202807426452637
Epoch 280, val loss: 1.3566762208938599
Epoch 290, training loss: 13.56363582611084 = 1.1812201738357544 + 2.0 * 6.1912078857421875
Epoch 290, val loss: 1.3333749771118164
Epoch 300, training loss: 13.512787818908691 = 1.144694447517395 + 2.0 * 6.184046745300293
Epoch 300, val loss: 1.3110684156417847
Epoch 310, training loss: 13.46870231628418 = 1.108519196510315 + 2.0 * 6.180091381072998
Epoch 310, val loss: 1.2892512083053589
Epoch 320, training loss: 13.420575141906738 = 1.0728598833084106 + 2.0 * 6.173857688903809
Epoch 320, val loss: 1.2677390575408936
Epoch 330, training loss: 13.373882293701172 = 1.0375360250473022 + 2.0 * 6.168173313140869
Epoch 330, val loss: 1.2463840246200562
Epoch 340, training loss: 13.334529876708984 = 1.002363681793213 + 2.0 * 6.166082859039307
Epoch 340, val loss: 1.225010633468628
Epoch 350, training loss: 13.287734031677246 = 0.967369794845581 + 2.0 * 6.160181999206543
Epoch 350, val loss: 1.2036235332489014
Epoch 360, training loss: 13.240726470947266 = 0.9328984022140503 + 2.0 * 6.153913974761963
Epoch 360, val loss: 1.1821873188018799
Epoch 370, training loss: 13.209368705749512 = 0.8987132906913757 + 2.0 * 6.155327796936035
Epoch 370, val loss: 1.1607755422592163
Epoch 380, training loss: 13.156707763671875 = 0.8652796745300293 + 2.0 * 6.145713806152344
Epoch 380, val loss: 1.139541506767273
Epoch 390, training loss: 13.116243362426758 = 0.832426905632019 + 2.0 * 6.141908168792725
Epoch 390, val loss: 1.1187129020690918
Epoch 400, training loss: 13.077970504760742 = 0.8000010848045349 + 2.0 * 6.138984680175781
Epoch 400, val loss: 1.0981935262680054
Epoch 410, training loss: 13.047473907470703 = 0.7681490182876587 + 2.0 * 6.139662265777588
Epoch 410, val loss: 1.0778852701187134
Epoch 420, training loss: 13.002553939819336 = 0.7370782494544983 + 2.0 * 6.132737636566162
Epoch 420, val loss: 1.058323621749878
Epoch 430, training loss: 12.963375091552734 = 0.7065113186836243 + 2.0 * 6.128431797027588
Epoch 430, val loss: 1.039482831954956
Epoch 440, training loss: 12.93478775024414 = 0.6765295267105103 + 2.0 * 6.129128932952881
Epoch 440, val loss: 1.0212253332138062
Epoch 450, training loss: 12.894855499267578 = 0.6472042202949524 + 2.0 * 6.123825550079346
Epoch 450, val loss: 1.003915786743164
Epoch 460, training loss: 12.860481262207031 = 0.6186115145683289 + 2.0 * 6.120934963226318
Epoch 460, val loss: 0.9874618053436279
Epoch 470, training loss: 12.826896667480469 = 0.5907759070396423 + 2.0 * 6.11806058883667
Epoch 470, val loss: 0.9719813466072083
Epoch 480, training loss: 12.799981117248535 = 0.5636067986488342 + 2.0 * 6.118186950683594
Epoch 480, val loss: 0.9576700925827026
Epoch 490, training loss: 12.761478424072266 = 0.5371856689453125 + 2.0 * 6.112146377563477
Epoch 490, val loss: 0.9444199204444885
Epoch 500, training loss: 12.731428146362305 = 0.5116225481033325 + 2.0 * 6.109902858734131
Epoch 500, val loss: 0.9323161244392395
Epoch 510, training loss: 12.707045555114746 = 0.4867546856403351 + 2.0 * 6.110145568847656
Epoch 510, val loss: 0.9213107824325562
Epoch 520, training loss: 12.680148124694824 = 0.46280086040496826 + 2.0 * 6.108673572540283
Epoch 520, val loss: 0.9113495349884033
Epoch 530, training loss: 12.648079872131348 = 0.4398500919342041 + 2.0 * 6.104115009307861
Epoch 530, val loss: 0.9025540947914124
Epoch 540, training loss: 12.61965274810791 = 0.417728066444397 + 2.0 * 6.100962162017822
Epoch 540, val loss: 0.8948612809181213
Epoch 550, training loss: 12.608330726623535 = 0.39644044637680054 + 2.0 * 6.105945110321045
Epoch 550, val loss: 0.888029932975769
Epoch 560, training loss: 12.571019172668457 = 0.37598785758018494 + 2.0 * 6.09751558303833
Epoch 560, val loss: 0.8822229504585266
Epoch 570, training loss: 12.54620361328125 = 0.3563540577888489 + 2.0 * 6.0949249267578125
Epoch 570, val loss: 0.8774096965789795
Epoch 580, training loss: 12.531131744384766 = 0.33758020401000977 + 2.0 * 6.096776008605957
Epoch 580, val loss: 0.8734339475631714
Epoch 590, training loss: 12.502779006958008 = 0.3195711076259613 + 2.0 * 6.091603755950928
Epoch 590, val loss: 0.87045818567276
Epoch 600, training loss: 12.48128890991211 = 0.30242905020713806 + 2.0 * 6.08942985534668
Epoch 600, val loss: 0.8684616088867188
Epoch 610, training loss: 12.469290733337402 = 0.2859569787979126 + 2.0 * 6.0916666984558105
Epoch 610, val loss: 0.8673497438430786
Epoch 620, training loss: 12.445448875427246 = 0.270378440618515 + 2.0 * 6.087535381317139
Epoch 620, val loss: 0.8671962022781372
Epoch 630, training loss: 12.424323081970215 = 0.25557196140289307 + 2.0 * 6.084375381469727
Epoch 630, val loss: 0.8680810928344727
Epoch 640, training loss: 12.412360191345215 = 0.24151365458965302 + 2.0 * 6.085423469543457
Epoch 640, val loss: 0.8698434233665466
Epoch 650, training loss: 12.39815616607666 = 0.22824837267398834 + 2.0 * 6.084953784942627
Epoch 650, val loss: 0.8724197745323181
Epoch 660, training loss: 12.374631881713867 = 0.21574775874614716 + 2.0 * 6.079442024230957
Epoch 660, val loss: 0.8758497834205627
Epoch 670, training loss: 12.374343872070312 = 0.20396551489830017 + 2.0 * 6.085189342498779
Epoch 670, val loss: 0.8799955248832703
Epoch 680, training loss: 12.349313735961914 = 0.19274266064167023 + 2.0 * 6.0782856941223145
Epoch 680, val loss: 0.8847004771232605
Epoch 690, training loss: 12.333927154541016 = 0.18215370178222656 + 2.0 * 6.0758867263793945
Epoch 690, val loss: 0.8901572227478027
Epoch 700, training loss: 12.319092750549316 = 0.17207252979278564 + 2.0 * 6.07351016998291
Epoch 700, val loss: 0.8962076306343079
Epoch 710, training loss: 12.308547019958496 = 0.16245390474796295 + 2.0 * 6.073046684265137
Epoch 710, val loss: 0.902775764465332
Epoch 720, training loss: 12.296891212463379 = 0.15335595607757568 + 2.0 * 6.071767807006836
Epoch 720, val loss: 0.9097459316253662
Epoch 730, training loss: 12.288093566894531 = 0.14480216801166534 + 2.0 * 6.071645736694336
Epoch 730, val loss: 0.9174056053161621
Epoch 740, training loss: 12.275156021118164 = 0.13671237230300903 + 2.0 * 6.0692219734191895
Epoch 740, val loss: 0.9254205822944641
Epoch 750, training loss: 12.26887321472168 = 0.1291140615940094 + 2.0 * 6.069879531860352
Epoch 750, val loss: 0.9339264035224915
Epoch 760, training loss: 12.255804061889648 = 0.12204445898532867 + 2.0 * 6.066879749298096
Epoch 760, val loss: 0.9428739547729492
Epoch 770, training loss: 12.24892807006836 = 0.11540085822343826 + 2.0 * 6.066763401031494
Epoch 770, val loss: 0.9522480368614197
Epoch 780, training loss: 12.237323760986328 = 0.10914196074008942 + 2.0 * 6.064090728759766
Epoch 780, val loss: 0.9619623422622681
Epoch 790, training loss: 12.229486465454102 = 0.10332797467708588 + 2.0 * 6.063079357147217
Epoch 790, val loss: 0.9720954298973083
Epoch 800, training loss: 12.2273530960083 = 0.09796036779880524 + 2.0 * 6.064696311950684
Epoch 800, val loss: 0.9825395345687866
Epoch 810, training loss: 12.22173023223877 = 0.09302398562431335 + 2.0 * 6.064352989196777
Epoch 810, val loss: 0.9932515621185303
Epoch 820, training loss: 12.208280563354492 = 0.08844549208879471 + 2.0 * 6.059917449951172
Epoch 820, val loss: 1.0040645599365234
Epoch 830, training loss: 12.201186180114746 = 0.08416260033845901 + 2.0 * 6.058511734008789
Epoch 830, val loss: 1.0149400234222412
Epoch 840, training loss: 12.194720268249512 = 0.08014968037605286 + 2.0 * 6.057285308837891
Epoch 840, val loss: 1.0257937908172607
Epoch 850, training loss: 12.194775581359863 = 0.07639459520578384 + 2.0 * 6.059190273284912
Epoch 850, val loss: 1.036525845527649
Epoch 860, training loss: 12.188799858093262 = 0.0728716030716896 + 2.0 * 6.057964324951172
Epoch 860, val loss: 1.0472595691680908
Epoch 870, training loss: 12.178695678710938 = 0.06957317888736725 + 2.0 * 6.054561138153076
Epoch 870, val loss: 1.0580871105194092
Epoch 880, training loss: 12.18431568145752 = 0.06647036969661713 + 2.0 * 6.05892276763916
Epoch 880, val loss: 1.0688103437423706
Epoch 890, training loss: 12.173463821411133 = 0.0635562464594841 + 2.0 * 6.054953575134277
Epoch 890, val loss: 1.0794119834899902
Epoch 900, training loss: 12.165781021118164 = 0.060824692249298096 + 2.0 * 6.052478313446045
Epoch 900, val loss: 1.0901129245758057
Epoch 910, training loss: 12.168877601623535 = 0.058248456567525864 + 2.0 * 6.055314540863037
Epoch 910, val loss: 1.1006686687469482
Epoch 920, training loss: 12.159031867980957 = 0.055825039744377136 + 2.0 * 6.051603317260742
Epoch 920, val loss: 1.1111057996749878
Epoch 930, training loss: 12.15942668914795 = 0.05354395881295204 + 2.0 * 6.05294132232666
Epoch 930, val loss: 1.1214828491210938
Epoch 940, training loss: 12.148043632507324 = 0.05140388011932373 + 2.0 * 6.0483198165893555
Epoch 940, val loss: 1.1317694187164307
Epoch 950, training loss: 12.145867347717285 = 0.049383535981178284 + 2.0 * 6.048242092132568
Epoch 950, val loss: 1.1419886350631714
Epoch 960, training loss: 12.14279556274414 = 0.04746321588754654 + 2.0 * 6.047666072845459
Epoch 960, val loss: 1.1520659923553467
Epoch 970, training loss: 12.147566795349121 = 0.045640524476766586 + 2.0 * 6.050962924957275
Epoch 970, val loss: 1.1619386672973633
Epoch 980, training loss: 12.135610580444336 = 0.04393603280186653 + 2.0 * 6.04583740234375
Epoch 980, val loss: 1.1717993021011353
Epoch 990, training loss: 12.130722999572754 = 0.04232683777809143 + 2.0 * 6.044198036193848
Epoch 990, val loss: 1.181601643562317
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7556
Overall ASR: 0.9114
Flip ASR: 0.8933/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.69116973876953 = 1.9433555603027344 + 2.0 * 8.373907089233398
Epoch 0, val loss: 1.945459008216858
Epoch 10, training loss: 18.680606842041016 = 1.9334869384765625 + 2.0 * 8.373559951782227
Epoch 10, val loss: 1.9360253810882568
Epoch 20, training loss: 18.66329574584961 = 1.9214287996292114 + 2.0 * 8.370933532714844
Epoch 20, val loss: 1.923909068107605
Epoch 30, training loss: 18.607568740844727 = 1.9047141075134277 + 2.0 * 8.35142707824707
Epoch 30, val loss: 1.9070184230804443
Epoch 40, training loss: 18.281892776489258 = 1.8842641115188599 + 2.0 * 8.198814392089844
Epoch 40, val loss: 1.8869768381118774
Epoch 50, training loss: 17.336177825927734 = 1.8622554540634155 + 2.0 * 7.736961364746094
Epoch 50, val loss: 1.8658828735351562
Epoch 60, training loss: 16.629358291625977 = 1.8468173742294312 + 2.0 * 7.391270160675049
Epoch 60, val loss: 1.8517042398452759
Epoch 70, training loss: 15.930866241455078 = 1.8384990692138672 + 2.0 * 7.0461835861206055
Epoch 70, val loss: 1.8437068462371826
Epoch 80, training loss: 15.293899536132812 = 1.828991413116455 + 2.0 * 6.732454299926758
Epoch 80, val loss: 1.8341989517211914
Epoch 90, training loss: 14.945534706115723 = 1.8172200918197632 + 2.0 * 6.564157485961914
Epoch 90, val loss: 1.8227812051773071
Epoch 100, training loss: 14.759414672851562 = 1.80421781539917 + 2.0 * 6.477598667144775
Epoch 100, val loss: 1.8102307319641113
Epoch 110, training loss: 14.62631893157959 = 1.7905731201171875 + 2.0 * 6.417872905731201
Epoch 110, val loss: 1.797084093093872
Epoch 120, training loss: 14.527745246887207 = 1.7766574621200562 + 2.0 * 6.37554407119751
Epoch 120, val loss: 1.7837880849838257
Epoch 130, training loss: 14.449495315551758 = 1.762287974357605 + 2.0 * 6.343603610992432
Epoch 130, val loss: 1.7701572179794312
Epoch 140, training loss: 14.379165649414062 = 1.7469841241836548 + 2.0 * 6.3160905838012695
Epoch 140, val loss: 1.7559545040130615
Epoch 150, training loss: 14.31767463684082 = 1.730278730392456 + 2.0 * 6.293697834014893
Epoch 150, val loss: 1.7406352758407593
Epoch 160, training loss: 14.274907112121582 = 1.7117141485214233 + 2.0 * 6.281596660614014
Epoch 160, val loss: 1.723892331123352
Epoch 170, training loss: 14.213497161865234 = 1.691068172454834 + 2.0 * 6.261214733123779
Epoch 170, val loss: 1.7055487632751465
Epoch 180, training loss: 14.158585548400879 = 1.6681187152862549 + 2.0 * 6.245233535766602
Epoch 180, val loss: 1.6852660179138184
Epoch 190, training loss: 14.108439445495605 = 1.6422646045684814 + 2.0 * 6.233087539672852
Epoch 190, val loss: 1.6626399755477905
Epoch 200, training loss: 14.057062149047852 = 1.6133934259414673 + 2.0 * 6.221834182739258
Epoch 200, val loss: 1.6375447511672974
Epoch 210, training loss: 14.003693580627441 = 1.5813610553741455 + 2.0 * 6.2111663818359375
Epoch 210, val loss: 1.6099308729171753
Epoch 220, training loss: 13.957542419433594 = 1.545886516571045 + 2.0 * 6.205827713012695
Epoch 220, val loss: 1.5796440839767456
Epoch 230, training loss: 13.895601272583008 = 1.5075362920761108 + 2.0 * 6.194032669067383
Epoch 230, val loss: 1.5470526218414307
Epoch 240, training loss: 13.849498748779297 = 1.4663424491882324 + 2.0 * 6.191578388214111
Epoch 240, val loss: 1.5124677419662476
Epoch 250, training loss: 13.783706665039062 = 1.4231802225112915 + 2.0 * 6.180263042449951
Epoch 250, val loss: 1.4768147468566895
Epoch 260, training loss: 13.72635269165039 = 1.378519892692566 + 2.0 * 6.173916339874268
Epoch 260, val loss: 1.4403923749923706
Epoch 270, training loss: 13.67242431640625 = 1.3328068256378174 + 2.0 * 6.169808864593506
Epoch 270, val loss: 1.4035879373550415
Epoch 280, training loss: 13.617449760437012 = 1.2865850925445557 + 2.0 * 6.165432453155518
Epoch 280, val loss: 1.3670918941497803
Epoch 290, training loss: 13.556050300598145 = 1.240610122680664 + 2.0 * 6.15772008895874
Epoch 290, val loss: 1.3311717510223389
Epoch 300, training loss: 13.501483917236328 = 1.1949396133422852 + 2.0 * 6.1532721519470215
Epoch 300, val loss: 1.296152949333191
Epoch 310, training loss: 13.45267391204834 = 1.1500129699707031 + 2.0 * 6.151330471038818
Epoch 310, val loss: 1.262159824371338
Epoch 320, training loss: 13.397018432617188 = 1.1062812805175781 + 2.0 * 6.145368576049805
Epoch 320, val loss: 1.229626178741455
Epoch 330, training loss: 13.343301773071289 = 1.063807725906372 + 2.0 * 6.139747142791748
Epoch 330, val loss: 1.1983954906463623
Epoch 340, training loss: 13.30446720123291 = 1.022546410560608 + 2.0 * 6.140960216522217
Epoch 340, val loss: 1.1686681509017944
Epoch 350, training loss: 13.251951217651367 = 0.9829788208007812 + 2.0 * 6.134486198425293
Epoch 350, val loss: 1.140442967414856
Epoch 360, training loss: 13.204876899719238 = 0.9450780749320984 + 2.0 * 6.129899501800537
Epoch 360, val loss: 1.114029049873352
Epoch 370, training loss: 13.160927772521973 = 0.9087947607040405 + 2.0 * 6.1260666847229
Epoch 370, val loss: 1.0891796350479126
Epoch 380, training loss: 13.122869491577148 = 0.8740739226341248 + 2.0 * 6.1243977546691895
Epoch 380, val loss: 1.0658891201019287
Epoch 390, training loss: 13.080045700073242 = 0.8409925699234009 + 2.0 * 6.119526386260986
Epoch 390, val loss: 1.0440531969070435
Epoch 400, training loss: 13.049161911010742 = 0.8093910813331604 + 2.0 * 6.119885444641113
Epoch 400, val loss: 1.0237747430801392
Epoch 410, training loss: 13.008748054504395 = 0.779276430606842 + 2.0 * 6.1147356033325195
Epoch 410, val loss: 1.0048748254776
Epoch 420, training loss: 12.976668357849121 = 0.7504844665527344 + 2.0 * 6.113091945648193
Epoch 420, val loss: 0.9872429966926575
Epoch 430, training loss: 12.94113826751709 = 0.7228639721870422 + 2.0 * 6.109137058258057
Epoch 430, val loss: 0.9706488847732544
Epoch 440, training loss: 12.916378021240234 = 0.6961984038352966 + 2.0 * 6.1100897789001465
Epoch 440, val loss: 0.9550364017486572
Epoch 450, training loss: 12.881316184997559 = 0.6705299019813538 + 2.0 * 6.105392932891846
Epoch 450, val loss: 0.9402568340301514
Epoch 460, training loss: 12.848468780517578 = 0.6457500457763672 + 2.0 * 6.1013593673706055
Epoch 460, val loss: 0.9262771010398865
Epoch 470, training loss: 12.81948184967041 = 0.6217015981674194 + 2.0 * 6.09889030456543
Epoch 470, val loss: 0.9129626154899597
Epoch 480, training loss: 12.790534973144531 = 0.5982632637023926 + 2.0 * 6.09613561630249
Epoch 480, val loss: 0.9002270102500916
Epoch 490, training loss: 12.778816223144531 = 0.5753478407859802 + 2.0 * 6.101734161376953
Epoch 490, val loss: 0.8879693150520325
Epoch 500, training loss: 12.738789558410645 = 0.5529299378395081 + 2.0 * 6.092929840087891
Epoch 500, val loss: 0.8762410879135132
Epoch 510, training loss: 12.716825485229492 = 0.5310549139976501 + 2.0 * 6.092885494232178
Epoch 510, val loss: 0.8649289608001709
Epoch 520, training loss: 12.68782901763916 = 0.5096160173416138 + 2.0 * 6.089106559753418
Epoch 520, val loss: 0.8540193438529968
Epoch 530, training loss: 12.665182113647461 = 0.4885413646697998 + 2.0 * 6.088320255279541
Epoch 530, val loss: 0.8434885740280151
Epoch 540, training loss: 12.64255428314209 = 0.46789848804473877 + 2.0 * 6.08732795715332
Epoch 540, val loss: 0.8334171175956726
Epoch 550, training loss: 12.615362167358398 = 0.44769322872161865 + 2.0 * 6.083834648132324
Epoch 550, val loss: 0.8238107562065125
Epoch 560, training loss: 12.593624114990234 = 0.4279289245605469 + 2.0 * 6.082847595214844
Epoch 560, val loss: 0.8146849870681763
Epoch 570, training loss: 12.572035789489746 = 0.4085649251937866 + 2.0 * 6.081735610961914
Epoch 570, val loss: 0.8060514330863953
Epoch 580, training loss: 12.5469970703125 = 0.389701247215271 + 2.0 * 6.078648090362549
Epoch 580, val loss: 0.7979989051818848
Epoch 590, training loss: 12.525510787963867 = 0.3713289797306061 + 2.0 * 6.077090740203857
Epoch 590, val loss: 0.7906181216239929
Epoch 600, training loss: 12.505894660949707 = 0.35351380705833435 + 2.0 * 6.07619047164917
Epoch 600, val loss: 0.7839382886886597
Epoch 610, training loss: 12.489282608032227 = 0.3362920582294464 + 2.0 * 6.076495170593262
Epoch 610, val loss: 0.7780036926269531
Epoch 620, training loss: 12.470192909240723 = 0.3197670876979828 + 2.0 * 6.0752129554748535
Epoch 620, val loss: 0.7728686928749084
Epoch 630, training loss: 12.447196960449219 = 0.30397307872772217 + 2.0 * 6.0716118812561035
Epoch 630, val loss: 0.7686037421226501
Epoch 640, training loss: 12.427787780761719 = 0.2888834774494171 + 2.0 * 6.069452285766602
Epoch 640, val loss: 0.7652130126953125
Epoch 650, training loss: 12.419039726257324 = 0.2745095491409302 + 2.0 * 6.072265148162842
Epoch 650, val loss: 0.7625959515571594
Epoch 660, training loss: 12.406370162963867 = 0.2608785033226013 + 2.0 * 6.0727458000183105
Epoch 660, val loss: 0.7606927752494812
Epoch 670, training loss: 12.380963325500488 = 0.24804078042507172 + 2.0 * 6.066461086273193
Epoch 670, val loss: 0.7594594955444336
Epoch 680, training loss: 12.374262809753418 = 0.23591294884681702 + 2.0 * 6.069174766540527
Epoch 680, val loss: 0.75889652967453
Epoch 690, training loss: 12.356215476989746 = 0.22444234788417816 + 2.0 * 6.065886497497559
Epoch 690, val loss: 0.7588367462158203
Epoch 700, training loss: 12.343402862548828 = 0.21367314457893372 + 2.0 * 6.064864635467529
Epoch 700, val loss: 0.7594150900840759
Epoch 710, training loss: 12.329118728637695 = 0.20350973308086395 + 2.0 * 6.062804698944092
Epoch 710, val loss: 0.7603761553764343
Epoch 720, training loss: 12.321531295776367 = 0.19391708076000214 + 2.0 * 6.063807010650635
Epoch 720, val loss: 0.7617987990379333
Epoch 730, training loss: 12.3131742477417 = 0.18487001955509186 + 2.0 * 6.064152240753174
Epoch 730, val loss: 0.7634161114692688
Epoch 740, training loss: 12.295171737670898 = 0.1763439029455185 + 2.0 * 6.059413909912109
Epoch 740, val loss: 0.7654907703399658
Epoch 750, training loss: 12.28249740600586 = 0.16827630996704102 + 2.0 * 6.057110786437988
Epoch 750, val loss: 0.767871081829071
Epoch 760, training loss: 12.279227256774902 = 0.16062702238559723 + 2.0 * 6.059299945831299
Epoch 760, val loss: 0.7704572677612305
Epoch 770, training loss: 12.264846801757812 = 0.15338444709777832 + 2.0 * 6.055731296539307
Epoch 770, val loss: 0.773224949836731
Epoch 780, training loss: 12.264287948608398 = 0.14653098583221436 + 2.0 * 6.058878421783447
Epoch 780, val loss: 0.7761944532394409
Epoch 790, training loss: 12.249480247497559 = 0.1400105506181717 + 2.0 * 6.054734706878662
Epoch 790, val loss: 0.7792494893074036
Epoch 800, training loss: 12.238093376159668 = 0.1338585764169693 + 2.0 * 6.052117347717285
Epoch 800, val loss: 0.7825645804405212
Epoch 810, training loss: 12.229963302612305 = 0.12801122665405273 + 2.0 * 6.050976276397705
Epoch 810, val loss: 0.7860406637191772
Epoch 820, training loss: 12.229608535766602 = 0.12244756519794464 + 2.0 * 6.053580284118652
Epoch 820, val loss: 0.7896940112113953
Epoch 830, training loss: 12.224599838256836 = 0.11715329438447952 + 2.0 * 6.053723335266113
Epoch 830, val loss: 0.7932743430137634
Epoch 840, training loss: 12.20905876159668 = 0.11215474456548691 + 2.0 * 6.048451900482178
Epoch 840, val loss: 0.7970591187477112
Epoch 850, training loss: 12.203216552734375 = 0.107403963804245 + 2.0 * 6.047906398773193
Epoch 850, val loss: 0.8010226488113403
Epoch 860, training loss: 12.197174072265625 = 0.1028715968132019 + 2.0 * 6.0471510887146
Epoch 860, val loss: 0.8050746321678162
Epoch 870, training loss: 12.193528175354004 = 0.09857488423585892 + 2.0 * 6.047476768493652
Epoch 870, val loss: 0.8091433048248291
Epoch 880, training loss: 12.190977096557617 = 0.09451858699321747 + 2.0 * 6.048229217529297
Epoch 880, val loss: 0.8132988810539246
Epoch 890, training loss: 12.180411338806152 = 0.09067528694868088 + 2.0 * 6.044867992401123
Epoch 890, val loss: 0.8176485896110535
Epoch 900, training loss: 12.182538986206055 = 0.08701075613498688 + 2.0 * 6.047764301300049
Epoch 900, val loss: 0.8220361471176147
Epoch 910, training loss: 12.172538757324219 = 0.08352083712816238 + 2.0 * 6.044508934020996
Epoch 910, val loss: 0.8264166116714478
Epoch 920, training loss: 12.167037963867188 = 0.08021176606416702 + 2.0 * 6.043413162231445
Epoch 920, val loss: 0.830932080745697
Epoch 930, training loss: 12.158646583557129 = 0.07706262171268463 + 2.0 * 6.040791988372803
Epoch 930, val loss: 0.8354964852333069
Epoch 940, training loss: 12.169307708740234 = 0.07407533377408981 + 2.0 * 6.047616004943848
Epoch 940, val loss: 0.84009850025177
Epoch 950, training loss: 12.157495498657227 = 0.07123100757598877 + 2.0 * 6.043132305145264
Epoch 950, val loss: 0.8447619676589966
Epoch 960, training loss: 12.146678924560547 = 0.06853576749563217 + 2.0 * 6.039071559906006
Epoch 960, val loss: 0.8495011329650879
Epoch 970, training loss: 12.143868446350098 = 0.06596414744853973 + 2.0 * 6.038952350616455
Epoch 970, val loss: 0.8542816638946533
Epoch 980, training loss: 12.145675659179688 = 0.06353653967380524 + 2.0 * 6.041069507598877
Epoch 980, val loss: 0.8590473532676697
Epoch 990, training loss: 12.135705947875977 = 0.061214983463287354 + 2.0 * 6.037245273590088
Epoch 990, val loss: 0.863801121711731
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7926
Overall ASR: 0.8155
Flip ASR: 0.7822/225 nodes
The final ASR:0.77122, 0.13622, Accuracy:0.79506, 0.03331
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11676])
remove edge: torch.Size([2, 9564])
updated graph: torch.Size([2, 10684])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8519
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
The final ASR:0.97909, 0.00174, Accuracy:0.83951, 0.01062
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.5 der2:0.4 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 18.7027587890625 = 1.9549767971038818 + 2.0 * 8.37389087677002
Epoch 0, val loss: 1.9540990591049194
Epoch 10, training loss: 18.69144630432129 = 1.9444537162780762 + 2.0 * 8.373496055603027
Epoch 10, val loss: 1.9446827173233032
Epoch 20, training loss: 18.67316436767578 = 1.9313850402832031 + 2.0 * 8.370889663696289
Epoch 20, val loss: 1.9326938390731812
Epoch 30, training loss: 18.62059211730957 = 1.913608431816101 + 2.0 * 8.35349178314209
Epoch 30, val loss: 1.9164562225341797
Epoch 40, training loss: 18.364452362060547 = 1.8921825885772705 + 2.0 * 8.23613452911377
Epoch 40, val loss: 1.8978606462478638
Epoch 50, training loss: 17.341773986816406 = 1.8692716360092163 + 2.0 * 7.736251354217529
Epoch 50, val loss: 1.8784245252609253
Epoch 60, training loss: 16.363731384277344 = 1.8512896299362183 + 2.0 * 7.256221294403076
Epoch 60, val loss: 1.8630212545394897
Epoch 70, training loss: 15.640466690063477 = 1.8384349346160889 + 2.0 * 6.901015758514404
Epoch 70, val loss: 1.8510661125183105
Epoch 80, training loss: 15.27255630493164 = 1.8268381357192993 + 2.0 * 6.722858905792236
Epoch 80, val loss: 1.840617060661316
Epoch 90, training loss: 15.013442993164062 = 1.8136131763458252 + 2.0 * 6.599915027618408
Epoch 90, val loss: 1.828980803489685
Epoch 100, training loss: 14.82973861694336 = 1.799912929534912 + 2.0 * 6.5149126052856445
Epoch 100, val loss: 1.8170853853225708
Epoch 110, training loss: 14.690136909484863 = 1.7865737676620483 + 2.0 * 6.451781749725342
Epoch 110, val loss: 1.8051211833953857
Epoch 120, training loss: 14.584036827087402 = 1.7729700803756714 + 2.0 * 6.405533313751221
Epoch 120, val loss: 1.7927054166793823
Epoch 130, training loss: 14.497163772583008 = 1.7587159872055054 + 2.0 * 6.3692240715026855
Epoch 130, val loss: 1.7797839641571045
Epoch 140, training loss: 14.420478820800781 = 1.7438688278198242 + 2.0 * 6.3383049964904785
Epoch 140, val loss: 1.7665326595306396
Epoch 150, training loss: 14.357154846191406 = 1.7279244661331177 + 2.0 * 6.314615249633789
Epoch 150, val loss: 1.7527018785476685
Epoch 160, training loss: 14.293485641479492 = 1.7104750871658325 + 2.0 * 6.291505336761475
Epoch 160, val loss: 1.737993597984314
Epoch 170, training loss: 14.237531661987305 = 1.691141128540039 + 2.0 * 6.273195266723633
Epoch 170, val loss: 1.7220240831375122
Epoch 180, training loss: 14.184310913085938 = 1.6694250106811523 + 2.0 * 6.257442951202393
Epoch 180, val loss: 1.7043683528900146
Epoch 190, training loss: 14.129712104797363 = 1.6452070474624634 + 2.0 * 6.242252349853516
Epoch 190, val loss: 1.6848506927490234
Epoch 200, training loss: 14.078875541687012 = 1.618105173110962 + 2.0 * 6.2303853034973145
Epoch 200, val loss: 1.6631981134414673
Epoch 210, training loss: 14.02420711517334 = 1.5880047082901 + 2.0 * 6.2181010246276855
Epoch 210, val loss: 1.6393301486968994
Epoch 220, training loss: 13.97156810760498 = 1.5549557209014893 + 2.0 * 6.208306312561035
Epoch 220, val loss: 1.6132911443710327
Epoch 230, training loss: 13.91952896118164 = 1.5193891525268555 + 2.0 * 6.200069904327393
Epoch 230, val loss: 1.5851964950561523
Epoch 240, training loss: 13.862542152404785 = 1.481663465499878 + 2.0 * 6.190439224243164
Epoch 240, val loss: 1.5556105375289917
Epoch 250, training loss: 13.807924270629883 = 1.4420162439346313 + 2.0 * 6.182953834533691
Epoch 250, val loss: 1.524662971496582
Epoch 260, training loss: 13.754941940307617 = 1.4015535116195679 + 2.0 * 6.176694393157959
Epoch 260, val loss: 1.4933619499206543
Epoch 270, training loss: 13.700701713562012 = 1.361183762550354 + 2.0 * 6.1697587966918945
Epoch 270, val loss: 1.4625389575958252
Epoch 280, training loss: 13.649075508117676 = 1.3214733600616455 + 2.0 * 6.163801193237305
Epoch 280, val loss: 1.432328701019287
Epoch 290, training loss: 13.603347778320312 = 1.282309889793396 + 2.0 * 6.160519123077393
Epoch 290, val loss: 1.4028215408325195
Epoch 300, training loss: 13.553400039672852 = 1.2441240549087524 + 2.0 * 6.154637813568115
Epoch 300, val loss: 1.374464988708496
Epoch 310, training loss: 13.504013061523438 = 1.207054615020752 + 2.0 * 6.148478984832764
Epoch 310, val loss: 1.3468544483184814
Epoch 320, training loss: 13.457633018493652 = 1.1703860759735107 + 2.0 * 6.143623352050781
Epoch 320, val loss: 1.3198143243789673
Epoch 330, training loss: 13.41528606414795 = 1.1338746547698975 + 2.0 * 6.140705585479736
Epoch 330, val loss: 1.2928239107131958
Epoch 340, training loss: 13.383243560791016 = 1.0971803665161133 + 2.0 * 6.143031597137451
Epoch 340, val loss: 1.2659810781478882
Epoch 350, training loss: 13.328439712524414 = 1.060563564300537 + 2.0 * 6.133937835693359
Epoch 350, val loss: 1.2389777898788452
Epoch 360, training loss: 13.27936840057373 = 1.0237433910369873 + 2.0 * 6.127812385559082
Epoch 360, val loss: 1.2117578983306885
Epoch 370, training loss: 13.234807968139648 = 0.9864228963851929 + 2.0 * 6.124192714691162
Epoch 370, val loss: 1.184218168258667
Epoch 380, training loss: 13.197680473327637 = 0.9487541913986206 + 2.0 * 6.124463081359863
Epoch 380, val loss: 1.15639066696167
Epoch 390, training loss: 13.153464317321777 = 0.9111512303352356 + 2.0 * 6.121156692504883
Epoch 390, val loss: 1.128488302230835
Epoch 400, training loss: 13.106973648071289 = 0.8737743496894836 + 2.0 * 6.1165995597839355
Epoch 400, val loss: 1.1011685132980347
Epoch 410, training loss: 13.06346321105957 = 0.8369448184967041 + 2.0 * 6.113259315490723
Epoch 410, val loss: 1.07404363155365
Epoch 420, training loss: 13.021734237670898 = 0.8005518913269043 + 2.0 * 6.110591411590576
Epoch 420, val loss: 1.0473124980926514
Epoch 430, training loss: 12.984272003173828 = 0.7649600505828857 + 2.0 * 6.109655857086182
Epoch 430, val loss: 1.021234393119812
Epoch 440, training loss: 12.944608688354492 = 0.7305948734283447 + 2.0 * 6.107007026672363
Epoch 440, val loss: 0.9961227774620056
Epoch 450, training loss: 12.905397415161133 = 0.6976620554924011 + 2.0 * 6.103867530822754
Epoch 450, val loss: 0.9723133444786072
Epoch 460, training loss: 12.867680549621582 = 0.6662769913673401 + 2.0 * 6.100701808929443
Epoch 460, val loss: 0.9497640132904053
Epoch 470, training loss: 12.851982116699219 = 0.6364388465881348 + 2.0 * 6.107771396636963
Epoch 470, val loss: 0.9287230372428894
Epoch 480, training loss: 12.802838325500488 = 0.6087872982025146 + 2.0 * 6.097025394439697
Epoch 480, val loss: 0.9093121290206909
Epoch 490, training loss: 12.771408081054688 = 0.5829759240150452 + 2.0 * 6.0942158699035645
Epoch 490, val loss: 0.8918240666389465
Epoch 500, training loss: 12.74266242980957 = 0.5588803291320801 + 2.0 * 6.091891288757324
Epoch 500, val loss: 0.8759936690330505
Epoch 510, training loss: 12.717312812805176 = 0.5363702178001404 + 2.0 * 6.090471267700195
Epoch 510, val loss: 0.8617555499076843
Epoch 520, training loss: 12.698137283325195 = 0.5153695344924927 + 2.0 * 6.091383934020996
Epoch 520, val loss: 0.8490659594535828
Epoch 530, training loss: 12.670714378356934 = 0.49598228931427 + 2.0 * 6.087366104125977
Epoch 530, val loss: 0.8380377888679504
Epoch 540, training loss: 12.647374153137207 = 0.47785401344299316 + 2.0 * 6.0847601890563965
Epoch 540, val loss: 0.8285475373268127
Epoch 550, training loss: 12.627507209777832 = 0.4607742428779602 + 2.0 * 6.083366394042969
Epoch 550, val loss: 0.8202652335166931
Epoch 560, training loss: 12.605691909790039 = 0.44465556740760803 + 2.0 * 6.0805182456970215
Epoch 560, val loss: 0.8130335211753845
Epoch 570, training loss: 12.603248596191406 = 0.4294408857822418 + 2.0 * 6.086904048919678
Epoch 570, val loss: 0.8068960905075073
Epoch 580, training loss: 12.573094367980957 = 0.414822518825531 + 2.0 * 6.079135894775391
Epoch 580, val loss: 0.8017056584358215
Epoch 590, training loss: 12.551436424255371 = 0.40086251497268677 + 2.0 * 6.075286865234375
Epoch 590, val loss: 0.7972996234893799
Epoch 600, training loss: 12.535282135009766 = 0.3872905373573303 + 2.0 * 6.073995590209961
Epoch 600, val loss: 0.7935964465141296
Epoch 610, training loss: 12.51877212524414 = 0.374060720205307 + 2.0 * 6.0723557472229
Epoch 610, val loss: 0.7905486226081848
Epoch 620, training loss: 12.516051292419434 = 0.36119750142097473 + 2.0 * 6.077426910400391
Epoch 620, val loss: 0.7880982756614685
Epoch 630, training loss: 12.487445831298828 = 0.34869569540023804 + 2.0 * 6.069375038146973
Epoch 630, val loss: 0.7862834930419922
Epoch 640, training loss: 12.47258472442627 = 0.3365062177181244 + 2.0 * 6.068039417266846
Epoch 640, val loss: 0.7852203249931335
Epoch 650, training loss: 12.457193374633789 = 0.3245467245578766 + 2.0 * 6.066323280334473
Epoch 650, val loss: 0.7845907211303711
Epoch 660, training loss: 12.446320533752441 = 0.3127797544002533 + 2.0 * 6.066770553588867
Epoch 660, val loss: 0.784469485282898
Epoch 670, training loss: 12.435314178466797 = 0.301179975271225 + 2.0 * 6.0670671463012695
Epoch 670, val loss: 0.7847642302513123
Epoch 680, training loss: 12.415242195129395 = 0.28987154364585876 + 2.0 * 6.062685489654541
Epoch 680, val loss: 0.7855579853057861
Epoch 690, training loss: 12.40266227722168 = 0.27872234582901 + 2.0 * 6.061969757080078
Epoch 690, val loss: 0.7867528796195984
Epoch 700, training loss: 12.388720512390137 = 0.26769712567329407 + 2.0 * 6.060511589050293
Epoch 700, val loss: 0.7882298827171326
Epoch 710, training loss: 12.387383460998535 = 0.25679415464401245 + 2.0 * 6.0652947425842285
Epoch 710, val loss: 0.7900443077087402
Epoch 720, training loss: 12.369538307189941 = 0.24601943790912628 + 2.0 * 6.0617594718933105
Epoch 720, val loss: 0.7920912504196167
Epoch 730, training loss: 12.351140022277832 = 0.23546309769153595 + 2.0 * 6.057838439941406
Epoch 730, val loss: 0.7944923043251038
Epoch 740, training loss: 12.337629318237305 = 0.22501538693904877 + 2.0 * 6.056306838989258
Epoch 740, val loss: 0.7970675826072693
Epoch 750, training loss: 12.332615852355957 = 0.2147253304719925 + 2.0 * 6.058945178985596
Epoch 750, val loss: 0.799892783164978
Epoch 760, training loss: 12.324849128723145 = 0.20465828478336334 + 2.0 * 6.060095310211182
Epoch 760, val loss: 0.8028042912483215
Epoch 770, training loss: 12.304337501525879 = 0.19481490552425385 + 2.0 * 6.0547614097595215
Epoch 770, val loss: 0.8060234189033508
Epoch 780, training loss: 12.289684295654297 = 0.18523412942886353 + 2.0 * 6.052225112915039
Epoch 780, val loss: 0.8093974590301514
Epoch 790, training loss: 12.278648376464844 = 0.17592747509479523 + 2.0 * 6.051360607147217
Epoch 790, val loss: 0.8129422664642334
Epoch 800, training loss: 12.28731632232666 = 0.1669401079416275 + 2.0 * 6.060188293457031
Epoch 800, val loss: 0.8166912794113159
Epoch 810, training loss: 12.259486198425293 = 0.15829762816429138 + 2.0 * 6.050594329833984
Epoch 810, val loss: 0.8203817009925842
Epoch 820, training loss: 12.249460220336914 = 0.15009267628192902 + 2.0 * 6.049683570861816
Epoch 820, val loss: 0.8244324922561646
Epoch 830, training loss: 12.238631248474121 = 0.1422819197177887 + 2.0 * 6.048174858093262
Epoch 830, val loss: 0.8286193013191223
Epoch 840, training loss: 12.232770919799805 = 0.13488511741161346 + 2.0 * 6.048943042755127
Epoch 840, val loss: 0.83303302526474
Epoch 850, training loss: 12.227402687072754 = 0.12788687646389008 + 2.0 * 6.049757957458496
Epoch 850, val loss: 0.8374623656272888
Epoch 860, training loss: 12.215328216552734 = 0.12137669324874878 + 2.0 * 6.046975612640381
Epoch 860, val loss: 0.8422021865844727
Epoch 870, training loss: 12.206300735473633 = 0.1152728721499443 + 2.0 * 6.045514106750488
Epoch 870, val loss: 0.847186267375946
Epoch 880, training loss: 12.19633960723877 = 0.1095619797706604 + 2.0 * 6.043388843536377
Epoch 880, val loss: 0.8522432446479797
Epoch 890, training loss: 12.19103717803955 = 0.10422270745038986 + 2.0 * 6.043407440185547
Epoch 890, val loss: 0.8574163317680359
Epoch 900, training loss: 12.19321060180664 = 0.09922796487808228 + 2.0 * 6.046991348266602
Epoch 900, val loss: 0.862572431564331
Epoch 910, training loss: 12.17711067199707 = 0.09460560232400894 + 2.0 * 6.041252613067627
Epoch 910, val loss: 0.8678575158119202
Epoch 920, training loss: 12.172680854797363 = 0.09029405564069748 + 2.0 * 6.04119348526001
Epoch 920, val loss: 0.8733241558074951
Epoch 930, training loss: 12.166411399841309 = 0.08626052737236023 + 2.0 * 6.040075302124023
Epoch 930, val loss: 0.8787218928337097
Epoch 940, training loss: 12.166738510131836 = 0.0825020968914032 + 2.0 * 6.042118072509766
Epoch 940, val loss: 0.884167492389679
Epoch 950, training loss: 12.162267684936523 = 0.07898586988449097 + 2.0 * 6.041640758514404
Epoch 950, val loss: 0.8896119594573975
Epoch 960, training loss: 12.152847290039062 = 0.07570410519838333 + 2.0 * 6.038571357727051
Epoch 960, val loss: 0.8950150609016418
Epoch 970, training loss: 12.146798133850098 = 0.07262437045574188 + 2.0 * 6.0370869636535645
Epoch 970, val loss: 0.9005993604660034
Epoch 980, training loss: 12.140931129455566 = 0.06974798440933228 + 2.0 * 6.0355916023254395
Epoch 980, val loss: 0.9061420559883118
Epoch 990, training loss: 12.141302108764648 = 0.06704402714967728 + 2.0 * 6.037128925323486
Epoch 990, val loss: 0.9116760492324829
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7815
Overall ASR: 0.7712
Flip ASR: 0.7244/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.70670509338379 = 1.9591636657714844 + 2.0 * 8.373770713806152
Epoch 0, val loss: 1.9661858081817627
Epoch 10, training loss: 18.69340705871582 = 1.9483269453048706 + 2.0 * 8.372540473937988
Epoch 10, val loss: 1.9542667865753174
Epoch 20, training loss: 18.66793441772461 = 1.935328483581543 + 2.0 * 8.366302490234375
Epoch 20, val loss: 1.9393925666809082
Epoch 30, training loss: 18.576438903808594 = 1.9186618328094482 + 2.0 * 8.328888893127441
Epoch 30, val loss: 1.9200505018234253
Epoch 40, training loss: 17.820018768310547 = 1.9002466201782227 + 2.0 * 7.95988655090332
Epoch 40, val loss: 1.898737907409668
Epoch 50, training loss: 16.402725219726562 = 1.8820147514343262 + 2.0 * 7.260354995727539
Epoch 50, val loss: 1.8778655529022217
Epoch 60, training loss: 15.805957794189453 = 1.8667140007019043 + 2.0 * 6.9696221351623535
Epoch 60, val loss: 1.8615577220916748
Epoch 70, training loss: 15.44237995147705 = 1.8526421785354614 + 2.0 * 6.7948689460754395
Epoch 70, val loss: 1.846601128578186
Epoch 80, training loss: 15.125450134277344 = 1.8399146795272827 + 2.0 * 6.642767906188965
Epoch 80, val loss: 1.833253026008606
Epoch 90, training loss: 14.924478530883789 = 1.8279838562011719 + 2.0 * 6.548247337341309
Epoch 90, val loss: 1.8207405805587769
Epoch 100, training loss: 14.7696533203125 = 1.8156640529632568 + 2.0 * 6.476994514465332
Epoch 100, val loss: 1.8079471588134766
Epoch 110, training loss: 14.642542839050293 = 1.803096055984497 + 2.0 * 6.4197235107421875
Epoch 110, val loss: 1.7948124408721924
Epoch 120, training loss: 14.543455123901367 = 1.7907732725143433 + 2.0 * 6.376340866088867
Epoch 120, val loss: 1.7819738388061523
Epoch 130, training loss: 14.459569931030273 = 1.7784210443496704 + 2.0 * 6.340574264526367
Epoch 130, val loss: 1.7692886590957642
Epoch 140, training loss: 14.388572692871094 = 1.7658562660217285 + 2.0 * 6.311358451843262
Epoch 140, val loss: 1.7564617395401
Epoch 150, training loss: 14.327384948730469 = 1.7527079582214355 + 2.0 * 6.2873382568359375
Epoch 150, val loss: 1.7435569763183594
Epoch 160, training loss: 14.279195785522461 = 1.738677740097046 + 2.0 * 6.270258903503418
Epoch 160, val loss: 1.7302402257919312
Epoch 170, training loss: 14.22811508178711 = 1.723509669303894 + 2.0 * 6.252302646636963
Epoch 170, val loss: 1.7164158821105957
Epoch 180, training loss: 14.179418563842773 = 1.7069121599197388 + 2.0 * 6.236253261566162
Epoch 180, val loss: 1.701819658279419
Epoch 190, training loss: 14.13259506225586 = 1.6884373426437378 + 2.0 * 6.222078800201416
Epoch 190, val loss: 1.6860777139663696
Epoch 200, training loss: 14.088066101074219 = 1.6676982641220093 + 2.0 * 6.210184097290039
Epoch 200, val loss: 1.6688395738601685
Epoch 210, training loss: 14.04304027557373 = 1.6445811986923218 + 2.0 * 6.199229717254639
Epoch 210, val loss: 1.6499626636505127
Epoch 220, training loss: 13.9949369430542 = 1.6185762882232666 + 2.0 * 6.188180446624756
Epoch 220, val loss: 1.6289997100830078
Epoch 230, training loss: 13.948234558105469 = 1.5892986059188843 + 2.0 * 6.179468154907227
Epoch 230, val loss: 1.6057209968566895
Epoch 240, training loss: 13.899425506591797 = 1.5570259094238281 + 2.0 * 6.171199798583984
Epoch 240, val loss: 1.5800691843032837
Epoch 250, training loss: 13.84866714477539 = 1.5216127634048462 + 2.0 * 6.163527011871338
Epoch 250, val loss: 1.5520893335342407
Epoch 260, training loss: 13.795964241027832 = 1.4830600023269653 + 2.0 * 6.156452178955078
Epoch 260, val loss: 1.5216894149780273
Epoch 270, training loss: 13.741121292114258 = 1.4411745071411133 + 2.0 * 6.149973392486572
Epoch 270, val loss: 1.4888828992843628
Epoch 280, training loss: 13.686990737915039 = 1.3963948488235474 + 2.0 * 6.145298004150391
Epoch 280, val loss: 1.4539223909378052
Epoch 290, training loss: 13.628498077392578 = 1.3499125242233276 + 2.0 * 6.1392927169799805
Epoch 290, val loss: 1.4178990125656128
Epoch 300, training loss: 13.573221206665039 = 1.3023954629898071 + 2.0 * 6.135412693023682
Epoch 300, val loss: 1.3815653324127197
Epoch 310, training loss: 13.514657974243164 = 1.2541632652282715 + 2.0 * 6.130247116088867
Epoch 310, val loss: 1.3449434041976929
Epoch 320, training loss: 13.4572172164917 = 1.205467939376831 + 2.0 * 6.1258745193481445
Epoch 320, val loss: 1.3083691596984863
Epoch 330, training loss: 13.402437210083008 = 1.1569507122039795 + 2.0 * 6.122743129730225
Epoch 330, val loss: 1.272165060043335
Epoch 340, training loss: 13.34665298461914 = 1.109243631362915 + 2.0 * 6.118704795837402
Epoch 340, val loss: 1.2369797229766846
Epoch 350, training loss: 13.292764663696289 = 1.062240481376648 + 2.0 * 6.115262031555176
Epoch 350, val loss: 1.2023212909698486
Epoch 360, training loss: 13.246114730834961 = 1.0160106420516968 + 2.0 * 6.115052223205566
Epoch 360, val loss: 1.1684569120407104
Epoch 370, training loss: 13.188841819763184 = 0.9713916182518005 + 2.0 * 6.108725070953369
Epoch 370, val loss: 1.1355113983154297
Epoch 380, training loss: 13.138229370117188 = 0.9281554222106934 + 2.0 * 6.105036735534668
Epoch 380, val loss: 1.1038419008255005
Epoch 390, training loss: 13.098363876342773 = 0.8867642283439636 + 2.0 * 6.105799674987793
Epoch 390, val loss: 1.0733771324157715
Epoch 400, training loss: 13.051396369934082 = 0.8476149439811707 + 2.0 * 6.101890563964844
Epoch 400, val loss: 1.0447791814804077
Epoch 410, training loss: 13.007189750671387 = 0.8112219572067261 + 2.0 * 6.0979838371276855
Epoch 410, val loss: 1.017928957939148
Epoch 420, training loss: 12.965364456176758 = 0.7771573662757874 + 2.0 * 6.0941033363342285
Epoch 420, val loss: 0.9930323958396912
Epoch 430, training loss: 12.939803123474121 = 0.7455025911331177 + 2.0 * 6.0971503257751465
Epoch 430, val loss: 0.969921886920929
Epoch 440, training loss: 12.896842956542969 = 0.7161498665809631 + 2.0 * 6.090346336364746
Epoch 440, val loss: 0.9488649964332581
Epoch 450, training loss: 12.863125801086426 = 0.6888182163238525 + 2.0 * 6.087153911590576
Epoch 450, val loss: 0.9294325709342957
Epoch 460, training loss: 12.836355209350586 = 0.6629348993301392 + 2.0 * 6.086709976196289
Epoch 460, val loss: 0.9114945530891418
Epoch 470, training loss: 12.809016227722168 = 0.6385916471481323 + 2.0 * 6.085212230682373
Epoch 470, val loss: 0.8946672677993774
Epoch 480, training loss: 12.774955749511719 = 0.6152483820915222 + 2.0 * 6.079853534698486
Epoch 480, val loss: 0.8791179060935974
Epoch 490, training loss: 12.752790451049805 = 0.5927063226699829 + 2.0 * 6.080041885375977
Epoch 490, val loss: 0.8642803430557251
Epoch 500, training loss: 12.72402286529541 = 0.5707535147666931 + 2.0 * 6.076634883880615
Epoch 500, val loss: 0.8500290513038635
Epoch 510, training loss: 12.697421073913574 = 0.5493499040603638 + 2.0 * 6.07403564453125
Epoch 510, val loss: 0.8363593816757202
Epoch 520, training loss: 12.676692008972168 = 0.5281028747558594 + 2.0 * 6.074294567108154
Epoch 520, val loss: 0.8231097459793091
Epoch 530, training loss: 12.653242111206055 = 0.50705885887146 + 2.0 * 6.073091506958008
Epoch 530, val loss: 0.8103112578392029
Epoch 540, training loss: 12.625951766967773 = 0.486275315284729 + 2.0 * 6.069838047027588
Epoch 540, val loss: 0.7979571223258972
Epoch 550, training loss: 12.600772857666016 = 0.4657383859157562 + 2.0 * 6.067517280578613
Epoch 550, val loss: 0.785965085029602
Epoch 560, training loss: 12.575071334838867 = 0.4451677203178406 + 2.0 * 6.0649518966674805
Epoch 560, val loss: 0.7745408415794373
Epoch 570, training loss: 12.558143615722656 = 0.4248064160346985 + 2.0 * 6.066668510437012
Epoch 570, val loss: 0.7635176777839661
Epoch 580, training loss: 12.529435157775879 = 0.4048253297805786 + 2.0 * 6.062304973602295
Epoch 580, val loss: 0.7529755234718323
Epoch 590, training loss: 12.507772445678711 = 0.3852486312389374 + 2.0 * 6.061262130737305
Epoch 590, val loss: 0.743175745010376
Epoch 600, training loss: 12.486985206604004 = 0.36612293124198914 + 2.0 * 6.060431003570557
Epoch 600, val loss: 0.7340958118438721
Epoch 610, training loss: 12.471121788024902 = 0.3475836217403412 + 2.0 * 6.061769008636475
Epoch 610, val loss: 0.7257269620895386
Epoch 620, training loss: 12.443950653076172 = 0.32965052127838135 + 2.0 * 6.057149887084961
Epoch 620, val loss: 0.7182204723358154
Epoch 630, training loss: 12.423746109008789 = 0.312406063079834 + 2.0 * 6.055669784545898
Epoch 630, val loss: 0.7115651965141296
Epoch 640, training loss: 12.416157722473145 = 0.2958384156227112 + 2.0 * 6.060159683227539
Epoch 640, val loss: 0.7057799100875854
Epoch 650, training loss: 12.388466835021973 = 0.280154824256897 + 2.0 * 6.0541558265686035
Epoch 650, val loss: 0.700775682926178
Epoch 660, training loss: 12.367470741271973 = 0.2651303708553314 + 2.0 * 6.051170349121094
Epoch 660, val loss: 0.6967547535896301
Epoch 670, training loss: 12.360222816467285 = 0.25086089968681335 + 2.0 * 6.054680824279785
Epoch 670, val loss: 0.693629801273346
Epoch 680, training loss: 12.348613739013672 = 0.23739483952522278 + 2.0 * 6.055609226226807
Epoch 680, val loss: 0.691223680973053
Epoch 690, training loss: 12.324650764465332 = 0.22482669353485107 + 2.0 * 6.049911975860596
Epoch 690, val loss: 0.6896883249282837
Epoch 700, training loss: 12.306639671325684 = 0.2129041999578476 + 2.0 * 6.046867847442627
Epoch 700, val loss: 0.6889879107475281
Epoch 710, training loss: 12.292709350585938 = 0.2016223967075348 + 2.0 * 6.045543670654297
Epoch 710, val loss: 0.6887553930282593
Epoch 720, training loss: 12.2980318069458 = 0.1909615695476532 + 2.0 * 6.053534984588623
Epoch 720, val loss: 0.6891947984695435
Epoch 730, training loss: 12.267694473266602 = 0.18108202517032623 + 2.0 * 6.043306350708008
Epoch 730, val loss: 0.6901780962944031
Epoch 740, training loss: 12.258365631103516 = 0.17179937660694122 + 2.0 * 6.043282985687256
Epoch 740, val loss: 0.6917292475700378
Epoch 750, training loss: 12.251036643981934 = 0.16311410069465637 + 2.0 * 6.043961048126221
Epoch 750, val loss: 0.6936976313591003
Epoch 760, training loss: 12.238553047180176 = 0.1550743132829666 + 2.0 * 6.041739463806152
Epoch 760, val loss: 0.6961056590080261
Epoch 770, training loss: 12.229048728942871 = 0.1475650817155838 + 2.0 * 6.040741920471191
Epoch 770, val loss: 0.6988665461540222
Epoch 780, training loss: 12.223180770874023 = 0.14053881168365479 + 2.0 * 6.04132080078125
Epoch 780, val loss: 0.7019692063331604
Epoch 790, training loss: 12.207898139953613 = 0.13400930166244507 + 2.0 * 6.036944389343262
Epoch 790, val loss: 0.7053443789482117
Epoch 800, training loss: 12.203390121459961 = 0.1278957724571228 + 2.0 * 6.037747383117676
Epoch 800, val loss: 0.7090509533882141
Epoch 810, training loss: 12.201207160949707 = 0.12215610593557358 + 2.0 * 6.039525508880615
Epoch 810, val loss: 0.712948203086853
Epoch 820, training loss: 12.19411849975586 = 0.1168111190199852 + 2.0 * 6.03865385055542
Epoch 820, val loss: 0.7170192003250122
Epoch 830, training loss: 12.179108619689941 = 0.11182135343551636 + 2.0 * 6.03364372253418
Epoch 830, val loss: 0.7213551998138428
Epoch 840, training loss: 12.174112319946289 = 0.10711783170700073 + 2.0 * 6.033497333526611
Epoch 840, val loss: 0.7258572578430176
Epoch 850, training loss: 12.180481910705566 = 0.10269571095705032 + 2.0 * 6.038893222808838
Epoch 850, val loss: 0.7305235862731934
Epoch 860, training loss: 12.170185089111328 = 0.09848862886428833 + 2.0 * 6.035848140716553
Epoch 860, val loss: 0.7352812886238098
Epoch 870, training loss: 12.160477638244629 = 0.09458541125059128 + 2.0 * 6.0329461097717285
Epoch 870, val loss: 0.7401924133300781
Epoch 880, training loss: 12.150959968566895 = 0.09087026119232178 + 2.0 * 6.030045032501221
Epoch 880, val loss: 0.7452191710472107
Epoch 890, training loss: 12.14507007598877 = 0.08735807985067368 + 2.0 * 6.028855800628662
Epoch 890, val loss: 0.7503702640533447
Epoch 900, training loss: 12.149508476257324 = 0.08402311056852341 + 2.0 * 6.032742500305176
Epoch 900, val loss: 0.7555861473083496
Epoch 910, training loss: 12.143656730651855 = 0.08086862415075302 + 2.0 * 6.031394004821777
Epoch 910, val loss: 0.7608045935630798
Epoch 920, training loss: 12.137115478515625 = 0.07787708938121796 + 2.0 * 6.029619216918945
Epoch 920, val loss: 0.7661654949188232
Epoch 930, training loss: 12.127715110778809 = 0.07504419982433319 + 2.0 * 6.0263352394104
Epoch 930, val loss: 0.7715653777122498
Epoch 940, training loss: 12.126652717590332 = 0.07233433425426483 + 2.0 * 6.027159214019775
Epoch 940, val loss: 0.7770100831985474
Epoch 950, training loss: 12.120465278625488 = 0.06975606828927994 + 2.0 * 6.025354385375977
Epoch 950, val loss: 0.7825121879577637
Epoch 960, training loss: 12.128135681152344 = 0.06730198860168457 + 2.0 * 6.030416965484619
Epoch 960, val loss: 0.7879859805107117
Epoch 970, training loss: 12.117502212524414 = 0.06497912853956223 + 2.0 * 6.026261329650879
Epoch 970, val loss: 0.7935423254966736
Epoch 980, training loss: 12.109424591064453 = 0.0627477690577507 + 2.0 * 6.023338317871094
Epoch 980, val loss: 0.7989974021911621
Epoch 990, training loss: 12.105390548706055 = 0.06061568111181259 + 2.0 * 6.022387504577637
Epoch 990, val loss: 0.8045900464057922
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7963
Overall ASR: 0.2620
Flip ASR: 0.1867/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.694292068481445 = 1.946560263633728 + 2.0 * 8.373866081237793
Epoch 0, val loss: 1.9543994665145874
Epoch 10, training loss: 18.6829891204834 = 1.9369202852249146 + 2.0 * 8.373034477233887
Epoch 10, val loss: 1.943200707435608
Epoch 20, training loss: 18.663331985473633 = 1.9247324466705322 + 2.0 * 8.36929988861084
Epoch 20, val loss: 1.9280675649642944
Epoch 30, training loss: 18.611066818237305 = 1.9079104661941528 + 2.0 * 8.351577758789062
Epoch 30, val loss: 1.906535029411316
Epoch 40, training loss: 18.351076126098633 = 1.8874138593673706 + 2.0 * 8.231831550598145
Epoch 40, val loss: 1.8807066679000854
Epoch 50, training loss: 17.348583221435547 = 1.8669090270996094 + 2.0 * 7.740837097167969
Epoch 50, val loss: 1.8555307388305664
Epoch 60, training loss: 16.476041793823242 = 1.8483355045318604 + 2.0 * 7.313852787017822
Epoch 60, val loss: 1.8353006839752197
Epoch 70, training loss: 15.757680892944336 = 1.8318768739700317 + 2.0 * 6.962902069091797
Epoch 70, val loss: 1.8190861940383911
Epoch 80, training loss: 15.302453994750977 = 1.8193498849868774 + 2.0 * 6.741551876068115
Epoch 80, val loss: 1.8072640895843506
Epoch 90, training loss: 15.0144681930542 = 1.807750940322876 + 2.0 * 6.603358745574951
Epoch 90, val loss: 1.7958390712738037
Epoch 100, training loss: 14.817214965820312 = 1.7944557666778564 + 2.0 * 6.511379718780518
Epoch 100, val loss: 1.782387137413025
Epoch 110, training loss: 14.674689292907715 = 1.7804467678070068 + 2.0 * 6.4471211433410645
Epoch 110, val loss: 1.768850326538086
Epoch 120, training loss: 14.568925857543945 = 1.7666290998458862 + 2.0 * 6.401148319244385
Epoch 120, val loss: 1.7560073137283325
Epoch 130, training loss: 14.480724334716797 = 1.7527393102645874 + 2.0 * 6.363992691040039
Epoch 130, val loss: 1.7432907819747925
Epoch 140, training loss: 14.405046463012695 = 1.7383290529251099 + 2.0 * 6.3333587646484375
Epoch 140, val loss: 1.7304589748382568
Epoch 150, training loss: 14.34566879272461 = 1.7229102849960327 + 2.0 * 6.311379432678223
Epoch 150, val loss: 1.7172290086746216
Epoch 160, training loss: 14.280168533325195 = 1.7062020301818848 + 2.0 * 6.286983013153076
Epoch 160, val loss: 1.7032243013381958
Epoch 170, training loss: 14.224998474121094 = 1.687512755393982 + 2.0 * 6.26874303817749
Epoch 170, val loss: 1.6880134344100952
Epoch 180, training loss: 14.180353164672852 = 1.666375994682312 + 2.0 * 6.256988525390625
Epoch 180, val loss: 1.6713119745254517
Epoch 190, training loss: 14.123875617980957 = 1.6428107023239136 + 2.0 * 6.240532398223877
Epoch 190, val loss: 1.6528338193893433
Epoch 200, training loss: 14.070524215698242 = 1.6161885261535645 + 2.0 * 6.22716760635376
Epoch 200, val loss: 1.6322990655899048
Epoch 210, training loss: 14.017653465270996 = 1.586064100265503 + 2.0 * 6.215794563293457
Epoch 210, val loss: 1.6092897653579712
Epoch 220, training loss: 13.9716796875 = 1.5521256923675537 + 2.0 * 6.209776878356934
Epoch 220, val loss: 1.5834846496582031
Epoch 230, training loss: 13.91549301147461 = 1.5146416425704956 + 2.0 * 6.200425624847412
Epoch 230, val loss: 1.5555744171142578
Epoch 240, training loss: 13.85505199432373 = 1.4742332696914673 + 2.0 * 6.190409183502197
Epoch 240, val loss: 1.5256292819976807
Epoch 250, training loss: 13.795164108276367 = 1.4309430122375488 + 2.0 * 6.182110786437988
Epoch 250, val loss: 1.493667721748352
Epoch 260, training loss: 13.735464096069336 = 1.3850740194320679 + 2.0 * 6.175195217132568
Epoch 260, val loss: 1.4602707624435425
Epoch 270, training loss: 13.687532424926758 = 1.3374485969543457 + 2.0 * 6.175042152404785
Epoch 270, val loss: 1.4262232780456543
Epoch 280, training loss: 13.619120597839355 = 1.2899141311645508 + 2.0 * 6.164603233337402
Epoch 280, val loss: 1.3925020694732666
Epoch 290, training loss: 13.559244155883789 = 1.2427440881729126 + 2.0 * 6.158249855041504
Epoch 290, val loss: 1.359758973121643
Epoch 300, training loss: 13.502662658691406 = 1.1964478492736816 + 2.0 * 6.153107643127441
Epoch 300, val loss: 1.3281759023666382
Epoch 310, training loss: 13.449084281921387 = 1.151842474937439 + 2.0 * 6.148621082305908
Epoch 310, val loss: 1.2980289459228516
Epoch 320, training loss: 13.395883560180664 = 1.109102725982666 + 2.0 * 6.14339017868042
Epoch 320, val loss: 1.2697734832763672
Epoch 330, training loss: 13.344720840454102 = 1.0684040784835815 + 2.0 * 6.138158321380615
Epoch 330, val loss: 1.242800235748291
Epoch 340, training loss: 13.296504974365234 = 1.0294177532196045 + 2.0 * 6.133543491363525
Epoch 340, val loss: 1.217004656791687
Epoch 350, training loss: 13.257718086242676 = 0.9921196699142456 + 2.0 * 6.13279914855957
Epoch 350, val loss: 1.1924009323120117
Epoch 360, training loss: 13.215113639831543 = 0.9569210410118103 + 2.0 * 6.129096508026123
Epoch 360, val loss: 1.1689636707305908
Epoch 370, training loss: 13.169459342956543 = 0.9233154058456421 + 2.0 * 6.123072147369385
Epoch 370, val loss: 1.146278738975525
Epoch 380, training loss: 13.13010311126709 = 0.8907201290130615 + 2.0 * 6.119691371917725
Epoch 380, val loss: 1.1241888999938965
Epoch 390, training loss: 13.106741905212402 = 0.8590153455734253 + 2.0 * 6.123863220214844
Epoch 390, val loss: 1.1027547121047974
Epoch 400, training loss: 13.057475090026855 = 0.8288710713386536 + 2.0 * 6.114302158355713
Epoch 400, val loss: 1.081886649131775
Epoch 410, training loss: 13.018837928771973 = 0.7992936372756958 + 2.0 * 6.109772205352783
Epoch 410, val loss: 1.0615071058273315
Epoch 420, training loss: 12.98485279083252 = 0.7702800631523132 + 2.0 * 6.10728645324707
Epoch 420, val loss: 1.0414741039276123
Epoch 430, training loss: 12.953105926513672 = 0.7417373061180115 + 2.0 * 6.105684280395508
Epoch 430, val loss: 1.021835446357727
Epoch 440, training loss: 12.92578411102295 = 0.7140101194381714 + 2.0 * 6.105886936187744
Epoch 440, val loss: 1.0025463104248047
Epoch 450, training loss: 12.88603401184082 = 0.6869280338287354 + 2.0 * 6.099553108215332
Epoch 450, val loss: 0.9839868545532227
Epoch 460, training loss: 12.854375839233398 = 0.6604313850402832 + 2.0 * 6.0969719886779785
Epoch 460, val loss: 0.9659765362739563
Epoch 470, training loss: 12.824670791625977 = 0.6344267725944519 + 2.0 * 6.09512186050415
Epoch 470, val loss: 0.9485267996788025
Epoch 480, training loss: 12.802783012390137 = 0.6090632677078247 + 2.0 * 6.096859931945801
Epoch 480, val loss: 0.9316599369049072
Epoch 490, training loss: 12.771574020385742 = 0.5844816565513611 + 2.0 * 6.093546390533447
Epoch 490, val loss: 0.9158517718315125
Epoch 500, training loss: 12.738080978393555 = 0.5604342818260193 + 2.0 * 6.088823318481445
Epoch 500, val loss: 0.9008374810218811
Epoch 510, training loss: 12.709843635559082 = 0.536745011806488 + 2.0 * 6.086549282073975
Epoch 510, val loss: 0.8864978551864624
Epoch 520, training loss: 12.694254875183105 = 0.5134338736534119 + 2.0 * 6.0904107093811035
Epoch 520, val loss: 0.8728814721107483
Epoch 530, training loss: 12.657608032226562 = 0.49083852767944336 + 2.0 * 6.083384990692139
Epoch 530, val loss: 0.8601089119911194
Epoch 540, training loss: 12.632468223571777 = 0.4688758850097656 + 2.0 * 6.081796169281006
Epoch 540, val loss: 0.8484222292900085
Epoch 550, training loss: 12.607129096984863 = 0.44759616255760193 + 2.0 * 6.079766273498535
Epoch 550, val loss: 0.8376849293708801
Epoch 560, training loss: 12.61033821105957 = 0.42707642912864685 + 2.0 * 6.091630935668945
Epoch 560, val loss: 0.8278699517250061
Epoch 570, training loss: 12.561351776123047 = 0.40730932354927063 + 2.0 * 6.07702112197876
Epoch 570, val loss: 0.8192071914672852
Epoch 580, training loss: 12.537593841552734 = 0.38849732279777527 + 2.0 * 6.074548244476318
Epoch 580, val loss: 0.8115983605384827
Epoch 590, training loss: 12.516304016113281 = 0.37041938304901123 + 2.0 * 6.07294225692749
Epoch 590, val loss: 0.8049750328063965
Epoch 600, training loss: 12.495949745178223 = 0.3530721068382263 + 2.0 * 6.071438789367676
Epoch 600, val loss: 0.7992282509803772
Epoch 610, training loss: 12.508716583251953 = 0.33652880787849426 + 2.0 * 6.086093902587891
Epoch 610, val loss: 0.7943243980407715
Epoch 620, training loss: 12.468413352966309 = 0.32073357701301575 + 2.0 * 6.0738396644592285
Epoch 620, val loss: 0.7902851700782776
Epoch 630, training loss: 12.443734169006348 = 0.30582064390182495 + 2.0 * 6.0689568519592285
Epoch 630, val loss: 0.7870813608169556
Epoch 640, training loss: 12.423443794250488 = 0.29153814911842346 + 2.0 * 6.065952777862549
Epoch 640, val loss: 0.7845271229743958
Epoch 650, training loss: 12.40613842010498 = 0.277820348739624 + 2.0 * 6.064158916473389
Epoch 650, val loss: 0.7824908494949341
Epoch 660, training loss: 12.3912353515625 = 0.2646619379520416 + 2.0 * 6.063286781311035
Epoch 660, val loss: 0.7810022830963135
Epoch 670, training loss: 12.383759498596191 = 0.2520691454410553 + 2.0 * 6.065845012664795
Epoch 670, val loss: 0.780010461807251
Epoch 680, training loss: 12.365789413452148 = 0.2401794046163559 + 2.0 * 6.06280517578125
Epoch 680, val loss: 0.7795315384864807
Epoch 690, training loss: 12.348017692565918 = 0.22882114350795746 + 2.0 * 6.059598445892334
Epoch 690, val loss: 0.7795699238777161
Epoch 700, training loss: 12.34693717956543 = 0.21800848841667175 + 2.0 * 6.064464569091797
Epoch 700, val loss: 0.779999852180481
Epoch 710, training loss: 12.328404426574707 = 0.20770592987537384 + 2.0 * 6.060349464416504
Epoch 710, val loss: 0.7807826995849609
Epoch 720, training loss: 12.310860633850098 = 0.19797484576702118 + 2.0 * 6.056442737579346
Epoch 720, val loss: 0.7819909453392029
Epoch 730, training loss: 12.304686546325684 = 0.1886948198080063 + 2.0 * 6.057995796203613
Epoch 730, val loss: 0.7834963202476501
Epoch 740, training loss: 12.290196418762207 = 0.17993637919425964 + 2.0 * 6.0551300048828125
Epoch 740, val loss: 0.7853114008903503
Epoch 750, training loss: 12.280840873718262 = 0.1715947836637497 + 2.0 * 6.054623126983643
Epoch 750, val loss: 0.7874397039413452
Epoch 760, training loss: 12.269365310668945 = 0.16373267769813538 + 2.0 * 6.052816390991211
Epoch 760, val loss: 0.7898643016815186
Epoch 770, training loss: 12.261117935180664 = 0.15631751716136932 + 2.0 * 6.0524001121521
Epoch 770, val loss: 0.7925906181335449
Epoch 780, training loss: 12.258383750915527 = 0.14924675226211548 + 2.0 * 6.054568290710449
Epoch 780, val loss: 0.7955121994018555
Epoch 790, training loss: 12.241472244262695 = 0.14263033866882324 + 2.0 * 6.0494208335876465
Epoch 790, val loss: 0.7987316250801086
Epoch 800, training loss: 12.232234954833984 = 0.13633224368095398 + 2.0 * 6.0479512214660645
Epoch 800, val loss: 0.8021474480628967
Epoch 810, training loss: 12.23036003112793 = 0.13038015365600586 + 2.0 * 6.049989700317383
Epoch 810, val loss: 0.8057775497436523
Epoch 820, training loss: 12.220258712768555 = 0.12475103884935379 + 2.0 * 6.047753810882568
Epoch 820, val loss: 0.8095414042472839
Epoch 830, training loss: 12.21116828918457 = 0.11942163854837418 + 2.0 * 6.045873165130615
Epoch 830, val loss: 0.8135230541229248
Epoch 840, training loss: 12.209737777709961 = 0.11438902467489243 + 2.0 * 6.047674179077148
Epoch 840, val loss: 0.8176369667053223
Epoch 850, training loss: 12.19677734375 = 0.10961838811635971 + 2.0 * 6.043579578399658
Epoch 850, val loss: 0.8218545913696289
Epoch 860, training loss: 12.192131042480469 = 0.10512322932481766 + 2.0 * 6.043503761291504
Epoch 860, val loss: 0.8262729048728943
Epoch 870, training loss: 12.190035820007324 = 0.10086129605770111 + 2.0 * 6.044587135314941
Epoch 870, val loss: 0.8307766318321228
Epoch 880, training loss: 12.179744720458984 = 0.0968431904911995 + 2.0 * 6.0414509773254395
Epoch 880, val loss: 0.8353583216667175
Epoch 890, training loss: 12.172906875610352 = 0.09302318841218948 + 2.0 * 6.039941787719727
Epoch 890, val loss: 0.8400832414627075
Epoch 900, training loss: 12.169166564941406 = 0.08939085155725479 + 2.0 * 6.03988790512085
Epoch 900, val loss: 0.8448083996772766
Epoch 910, training loss: 12.163660049438477 = 0.08594372868537903 + 2.0 * 6.038857936859131
Epoch 910, val loss: 0.8495597839355469
Epoch 920, training loss: 12.1665678024292 = 0.08267971128225327 + 2.0 * 6.0419440269470215
Epoch 920, val loss: 0.8544450402259827
Epoch 930, training loss: 12.156610488891602 = 0.07957620173692703 + 2.0 * 6.038516998291016
Epoch 930, val loss: 0.8592007756233215
Epoch 940, training loss: 12.148221015930176 = 0.07664976269006729 + 2.0 * 6.035785675048828
Epoch 940, val loss: 0.8641668558120728
Epoch 950, training loss: 12.145155906677246 = 0.07385047525167465 + 2.0 * 6.0356526374816895
Epoch 950, val loss: 0.869105339050293
Epoch 960, training loss: 12.146217346191406 = 0.07118835300207138 + 2.0 * 6.037514686584473
Epoch 960, val loss: 0.8740220665931702
Epoch 970, training loss: 12.1367826461792 = 0.06868109852075577 + 2.0 * 6.034050941467285
Epoch 970, val loss: 0.8790915012359619
Epoch 980, training loss: 12.132025718688965 = 0.06626514345407486 + 2.0 * 6.0328803062438965
Epoch 980, val loss: 0.8840950727462769
Epoch 990, training loss: 12.128294944763184 = 0.06397023797035217 + 2.0 * 6.032162189483643
Epoch 990, val loss: 0.8891597390174866
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8074
Overall ASR: 0.1845
Flip ASR: 0.2000/225 nodes
The final ASR:0.40590, 0.26025, Accuracy:0.79506, 0.01062
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11576])
remove edge: torch.Size([2, 9442])
updated graph: torch.Size([2, 10462])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9668
Flip ASR: 0.9600/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9852
Flip ASR: 0.9822/225 nodes
The final ASR:0.97540, 0.00758, Accuracy:0.83210, 0.00761
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.5 der2:0.4 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 18.690900802612305 = 1.9431445598602295 + 2.0 * 8.373878479003906
Epoch 0, val loss: 1.9508155584335327
Epoch 10, training loss: 18.6805419921875 = 1.9336928129196167 + 2.0 * 8.373424530029297
Epoch 10, val loss: 1.9416056871414185
Epoch 20, training loss: 18.662813186645508 = 1.9218709468841553 + 2.0 * 8.370471000671387
Epoch 20, val loss: 1.9295274019241333
Epoch 30, training loss: 18.607301712036133 = 1.9055454730987549 + 2.0 * 8.35087776184082
Epoch 30, val loss: 1.912571668624878
Epoch 40, training loss: 18.290775299072266 = 1.886597990989685 + 2.0 * 8.202088356018066
Epoch 40, val loss: 1.89374840259552
Epoch 50, training loss: 16.965303421020508 = 1.8681811094284058 + 2.0 * 7.548561096191406
Epoch 50, val loss: 1.8758244514465332
Epoch 60, training loss: 16.21327018737793 = 1.8534860610961914 + 2.0 * 7.179892063140869
Epoch 60, val loss: 1.8628926277160645
Epoch 70, training loss: 15.55078125 = 1.8403130769729614 + 2.0 * 6.855234146118164
Epoch 70, val loss: 1.850411295890808
Epoch 80, training loss: 15.162083625793457 = 1.8266700506210327 + 2.0 * 6.6677069664001465
Epoch 80, val loss: 1.8375273942947388
Epoch 90, training loss: 14.94887638092041 = 1.8117485046386719 + 2.0 * 6.568563938140869
Epoch 90, val loss: 1.8235951662063599
Epoch 100, training loss: 14.790438652038574 = 1.7960978746414185 + 2.0 * 6.497170448303223
Epoch 100, val loss: 1.8092347383499146
Epoch 110, training loss: 14.671026229858398 = 1.7800374031066895 + 2.0 * 6.445494174957275
Epoch 110, val loss: 1.7948647737503052
Epoch 120, training loss: 14.57233715057373 = 1.7636754512786865 + 2.0 * 6.404330730438232
Epoch 120, val loss: 1.7804678678512573
Epoch 130, training loss: 14.48794937133789 = 1.7464700937271118 + 2.0 * 6.370739459991455
Epoch 130, val loss: 1.765643835067749
Epoch 140, training loss: 14.41525936126709 = 1.7281116247177124 + 2.0 * 6.343574047088623
Epoch 140, val loss: 1.7500286102294922
Epoch 150, training loss: 14.343779563903809 = 1.7080259323120117 + 2.0 * 6.317876815795898
Epoch 150, val loss: 1.733307123184204
Epoch 160, training loss: 14.28015422821045 = 1.685660719871521 + 2.0 * 6.297246932983398
Epoch 160, val loss: 1.7148399353027344
Epoch 170, training loss: 14.227500915527344 = 1.6605738401412964 + 2.0 * 6.283463478088379
Epoch 170, val loss: 1.6942994594573975
Epoch 180, training loss: 14.1632661819458 = 1.6328378915786743 + 2.0 * 6.265213966369629
Epoch 180, val loss: 1.6716101169586182
Epoch 190, training loss: 14.103877067565918 = 1.6021051406860352 + 2.0 * 6.250885963439941
Epoch 190, val loss: 1.6465119123458862
Epoch 200, training loss: 14.044635772705078 = 1.5681225061416626 + 2.0 * 6.238256454467773
Epoch 200, val loss: 1.6187797784805298
Epoch 210, training loss: 13.998854637145996 = 1.5308502912521362 + 2.0 * 6.234002113342285
Epoch 210, val loss: 1.5883128643035889
Epoch 220, training loss: 13.930096626281738 = 1.4908361434936523 + 2.0 * 6.219630241394043
Epoch 220, val loss: 1.5557626485824585
Epoch 230, training loss: 13.8668212890625 = 1.4483561515808105 + 2.0 * 6.209232807159424
Epoch 230, val loss: 1.52131986618042
Epoch 240, training loss: 13.811505317687988 = 1.4035924673080444 + 2.0 * 6.203956604003906
Epoch 240, val loss: 1.4853508472442627
Epoch 250, training loss: 13.745357513427734 = 1.357666015625 + 2.0 * 6.193845748901367
Epoch 250, val loss: 1.4484925270080566
Epoch 260, training loss: 13.684290885925293 = 1.310962438583374 + 2.0 * 6.18666410446167
Epoch 260, val loss: 1.4116408824920654
Epoch 270, training loss: 13.624500274658203 = 1.2639905214309692 + 2.0 * 6.180254936218262
Epoch 270, val loss: 1.3749122619628906
Epoch 280, training loss: 13.568918228149414 = 1.2173370122909546 + 2.0 * 6.175790786743164
Epoch 280, val loss: 1.3390581607818604
Epoch 290, training loss: 13.510028839111328 = 1.1718153953552246 + 2.0 * 6.169106960296631
Epoch 290, val loss: 1.3045457601547241
Epoch 300, training loss: 13.45292854309082 = 1.1276319026947021 + 2.0 * 6.1626482009887695
Epoch 300, val loss: 1.2716429233551025
Epoch 310, training loss: 13.410512924194336 = 1.0848182439804077 + 2.0 * 6.162847518920898
Epoch 310, val loss: 1.2402887344360352
Epoch 320, training loss: 13.348536491394043 = 1.04314124584198 + 2.0 * 6.152697563171387
Epoch 320, val loss: 1.2106486558914185
Epoch 330, training loss: 13.299513816833496 = 1.0025460720062256 + 2.0 * 6.148483753204346
Epoch 330, val loss: 1.1821388006210327
Epoch 340, training loss: 13.256782531738281 = 0.9628464579582214 + 2.0 * 6.146967887878418
Epoch 340, val loss: 1.1546682119369507
Epoch 350, training loss: 13.20739459991455 = 0.9242225885391235 + 2.0 * 6.141585826873779
Epoch 350, val loss: 1.1280328035354614
Epoch 360, training loss: 13.160782814025879 = 0.8865416646003723 + 2.0 * 6.137120723724365
Epoch 360, val loss: 1.1026060581207275
Epoch 370, training loss: 13.114997863769531 = 0.8499646186828613 + 2.0 * 6.132516860961914
Epoch 370, val loss: 1.0783743858337402
Epoch 380, training loss: 13.072436332702637 = 0.8143036365509033 + 2.0 * 6.129066467285156
Epoch 380, val loss: 1.0552000999450684
Epoch 390, training loss: 13.031524658203125 = 0.7796886563301086 + 2.0 * 6.125917911529541
Epoch 390, val loss: 1.0330713987350464
Epoch 400, training loss: 12.99213981628418 = 0.7463545203208923 + 2.0 * 6.1228928565979
Epoch 400, val loss: 1.0122959613800049
Epoch 410, training loss: 12.955408096313477 = 0.7142084240913391 + 2.0 * 6.120599746704102
Epoch 410, val loss: 0.992672860622406
Epoch 420, training loss: 12.916068077087402 = 0.683120608329773 + 2.0 * 6.11647367477417
Epoch 420, val loss: 0.9740265011787415
Epoch 430, training loss: 12.88776683807373 = 0.6529613733291626 + 2.0 * 6.11740255355835
Epoch 430, val loss: 0.95634526014328
Epoch 440, training loss: 12.84862232208252 = 0.6237517595291138 + 2.0 * 6.112435340881348
Epoch 440, val loss: 0.9394565224647522
Epoch 450, training loss: 12.814083099365234 = 0.5952891111373901 + 2.0 * 6.109396934509277
Epoch 450, val loss: 0.9233670234680176
Epoch 460, training loss: 12.793147087097168 = 0.5675898790359497 + 2.0 * 6.112778663635254
Epoch 460, val loss: 0.9077737927436829
Epoch 470, training loss: 12.7515230178833 = 0.5403985977172852 + 2.0 * 6.105562210083008
Epoch 470, val loss: 0.8927744626998901
Epoch 480, training loss: 12.719287872314453 = 0.5141171813011169 + 2.0 * 6.102585315704346
Epoch 480, val loss: 0.8783055543899536
Epoch 490, training loss: 12.687363624572754 = 0.48848459124565125 + 2.0 * 6.09943962097168
Epoch 490, val loss: 0.8642587065696716
Epoch 500, training loss: 12.673685073852539 = 0.46354997158050537 + 2.0 * 6.105067729949951
Epoch 500, val loss: 0.8505904078483582
Epoch 510, training loss: 12.633745193481445 = 0.43943479657173157 + 2.0 * 6.0971550941467285
Epoch 510, val loss: 0.8374876976013184
Epoch 520, training loss: 12.603464126586914 = 0.416276216506958 + 2.0 * 6.093594074249268
Epoch 520, val loss: 0.8250117301940918
Epoch 530, training loss: 12.579444885253906 = 0.3940053880214691 + 2.0 * 6.092719554901123
Epoch 530, val loss: 0.8131344318389893
Epoch 540, training loss: 12.554758071899414 = 0.37279337644577026 + 2.0 * 6.090982437133789
Epoch 540, val loss: 0.8017997741699219
Epoch 550, training loss: 12.529833793640137 = 0.3526362180709839 + 2.0 * 6.088598728179932
Epoch 550, val loss: 0.7914769053459167
Epoch 560, training loss: 12.509044647216797 = 0.33347374200820923 + 2.0 * 6.087785243988037
Epoch 560, val loss: 0.7818451523780823
Epoch 570, training loss: 12.485657691955566 = 0.31524908542633057 + 2.0 * 6.085204124450684
Epoch 570, val loss: 0.7729954719543457
Epoch 580, training loss: 12.46580696105957 = 0.29799342155456543 + 2.0 * 6.083906650543213
Epoch 580, val loss: 0.765007734298706
Epoch 590, training loss: 12.445170402526855 = 0.28163984417915344 + 2.0 * 6.081765174865723
Epoch 590, val loss: 0.7578572034835815
Epoch 600, training loss: 12.425230026245117 = 0.266055166721344 + 2.0 * 6.079587459564209
Epoch 600, val loss: 0.7514716982841492
Epoch 610, training loss: 12.416627883911133 = 0.25123676657676697 + 2.0 * 6.082695484161377
Epoch 610, val loss: 0.7458040118217468
Epoch 620, training loss: 12.391721725463867 = 0.23719212412834167 + 2.0 * 6.077264785766602
Epoch 620, val loss: 0.7410247921943665
Epoch 630, training loss: 12.376566886901855 = 0.22388479113578796 + 2.0 * 6.076341152191162
Epoch 630, val loss: 0.7369976043701172
Epoch 640, training loss: 12.359947204589844 = 0.2112288475036621 + 2.0 * 6.07435941696167
Epoch 640, val loss: 0.733752429485321
Epoch 650, training loss: 12.34732723236084 = 0.1992713361978531 + 2.0 * 6.074028015136719
Epoch 650, val loss: 0.731217086315155
Epoch 660, training loss: 12.33294677734375 = 0.18796135485172272 + 2.0 * 6.072492599487305
Epoch 660, val loss: 0.7293874025344849
Epoch 670, training loss: 12.3165864944458 = 0.17735496163368225 + 2.0 * 6.069615840911865
Epoch 670, val loss: 0.7282260060310364
Epoch 680, training loss: 12.303729057312012 = 0.16735227406024933 + 2.0 * 6.068188190460205
Epoch 680, val loss: 0.7277489304542542
Epoch 690, training loss: 12.298885345458984 = 0.15799503028392792 + 2.0 * 6.0704450607299805
Epoch 690, val loss: 0.7277289628982544
Epoch 700, training loss: 12.28446102142334 = 0.14924116432666779 + 2.0 * 6.067609786987305
Epoch 700, val loss: 0.7284626364707947
Epoch 710, training loss: 12.270008087158203 = 0.14111366868019104 + 2.0 * 6.064447402954102
Epoch 710, val loss: 0.7297415733337402
Epoch 720, training loss: 12.259196281433105 = 0.13354922831058502 + 2.0 * 6.062823295593262
Epoch 720, val loss: 0.7315268516540527
Epoch 730, training loss: 12.258983612060547 = 0.12648016214370728 + 2.0 * 6.066251754760742
Epoch 730, val loss: 0.7337188124656677
Epoch 740, training loss: 12.243427276611328 = 0.1199280321598053 + 2.0 * 6.061749458312988
Epoch 740, val loss: 0.7363786101341248
Epoch 750, training loss: 12.233871459960938 = 0.11378969252109528 + 2.0 * 6.0600409507751465
Epoch 750, val loss: 0.7394492626190186
Epoch 760, training loss: 12.23276424407959 = 0.10807323455810547 + 2.0 * 6.062345504760742
Epoch 760, val loss: 0.742794930934906
Epoch 770, training loss: 12.222742080688477 = 0.10274690389633179 + 2.0 * 6.05999755859375
Epoch 770, val loss: 0.7465326189994812
Epoch 780, training loss: 12.209807395935059 = 0.09776641428470612 + 2.0 * 6.056020259857178
Epoch 780, val loss: 0.7504721879959106
Epoch 790, training loss: 12.20304012298584 = 0.09311993420124054 + 2.0 * 6.054960250854492
Epoch 790, val loss: 0.7547200322151184
Epoch 800, training loss: 12.21008586883545 = 0.08876463770866394 + 2.0 * 6.0606608390808105
Epoch 800, val loss: 0.759145200252533
Epoch 810, training loss: 12.195910453796387 = 0.08470696955919266 + 2.0 * 6.055601596832275
Epoch 810, val loss: 0.7637975215911865
Epoch 820, training loss: 12.184856414794922 = 0.0808863490819931 + 2.0 * 6.051985263824463
Epoch 820, val loss: 0.7686483263969421
Epoch 830, training loss: 12.178624153137207 = 0.07730773836374283 + 2.0 * 6.050658226013184
Epoch 830, val loss: 0.7737154364585876
Epoch 840, training loss: 12.19001293182373 = 0.0739271342754364 + 2.0 * 6.058043003082275
Epoch 840, val loss: 0.7788539528846741
Epoch 850, training loss: 12.17435073852539 = 0.07074104994535446 + 2.0 * 6.051805019378662
Epoch 850, val loss: 0.7840791940689087
Epoch 860, training loss: 12.16641902923584 = 0.06776994466781616 + 2.0 * 6.0493245124816895
Epoch 860, val loss: 0.7895339727401733
Epoch 870, training loss: 12.158163070678711 = 0.06495567411184311 + 2.0 * 6.046603679656982
Epoch 870, val loss: 0.7949699759483337
Epoch 880, training loss: 12.15540599822998 = 0.06229357421398163 + 2.0 * 6.046555995941162
Epoch 880, val loss: 0.8005111813545227
Epoch 890, training loss: 12.161515235900879 = 0.059773873537778854 + 2.0 * 6.050870895385742
Epoch 890, val loss: 0.8059753775596619
Epoch 900, training loss: 12.153223037719727 = 0.057392627000808716 + 2.0 * 6.047914981842041
Epoch 900, val loss: 0.8116326928138733
Epoch 910, training loss: 12.143257141113281 = 0.055147334933280945 + 2.0 * 6.044054985046387
Epoch 910, val loss: 0.8172902464866638
Epoch 920, training loss: 12.138361930847168 = 0.05302143841981888 + 2.0 * 6.042670249938965
Epoch 920, val loss: 0.8229719400405884
Epoch 930, training loss: 12.13578987121582 = 0.050996024161577225 + 2.0 * 6.0423970222473145
Epoch 930, val loss: 0.8287052512168884
Epoch 940, training loss: 12.143457412719727 = 0.049068085849285126 + 2.0 * 6.047194480895996
Epoch 940, val loss: 0.8343814015388489
Epoch 950, training loss: 12.1281156539917 = 0.04724617674946785 + 2.0 * 6.040434837341309
Epoch 950, val loss: 0.8401305079460144
Epoch 960, training loss: 12.126334190368652 = 0.045527152717113495 + 2.0 * 6.040403366088867
Epoch 960, val loss: 0.8458903431892395
Epoch 970, training loss: 12.126365661621094 = 0.04387841746211052 + 2.0 * 6.041243553161621
Epoch 970, val loss: 0.8516265153884888
Epoch 980, training loss: 12.119376182556152 = 0.04231002926826477 + 2.0 * 6.0385332107543945
Epoch 980, val loss: 0.8572763800621033
Epoch 990, training loss: 12.117551803588867 = 0.040824417024850845 + 2.0 * 6.038363456726074
Epoch 990, val loss: 0.8630211353302002
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8000
Overall ASR: 0.6937
Flip ASR: 0.6311/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.700056076049805 = 1.9522342681884766 + 2.0 * 8.373910903930664
Epoch 0, val loss: 1.9453790187835693
Epoch 10, training loss: 18.688478469848633 = 1.9412264823913574 + 2.0 * 8.373625755310059
Epoch 10, val loss: 1.9343576431274414
Epoch 20, training loss: 18.671384811401367 = 1.9276145696640015 + 2.0 * 8.371885299682617
Epoch 20, val loss: 1.9207723140716553
Epoch 30, training loss: 18.62764549255371 = 1.9087284803390503 + 2.0 * 8.359458923339844
Epoch 30, val loss: 1.9022578001022339
Epoch 40, training loss: 18.424257278442383 = 1.883926510810852 + 2.0 * 8.27016544342041
Epoch 40, val loss: 1.8789465427398682
Epoch 50, training loss: 17.390743255615234 = 1.8578531742095947 + 2.0 * 7.766445159912109
Epoch 50, val loss: 1.8554214239120483
Epoch 60, training loss: 16.406509399414062 = 1.8390709161758423 + 2.0 * 7.283719539642334
Epoch 60, val loss: 1.839004397392273
Epoch 70, training loss: 15.644787788391113 = 1.8261126279830933 + 2.0 * 6.909337520599365
Epoch 70, val loss: 1.8266445398330688
Epoch 80, training loss: 15.229150772094727 = 1.8131316900253296 + 2.0 * 6.708009719848633
Epoch 80, val loss: 1.8144686222076416
Epoch 90, training loss: 14.959019660949707 = 1.7978811264038086 + 2.0 * 6.580569267272949
Epoch 90, val loss: 1.8008431196212769
Epoch 100, training loss: 14.789628028869629 = 1.7816123962402344 + 2.0 * 6.504007816314697
Epoch 100, val loss: 1.7865151166915894
Epoch 110, training loss: 14.671298027038574 = 1.7645323276519775 + 2.0 * 6.453382968902588
Epoch 110, val loss: 1.7713593244552612
Epoch 120, training loss: 14.576261520385742 = 1.7465739250183105 + 2.0 * 6.414843559265137
Epoch 120, val loss: 1.7553579807281494
Epoch 130, training loss: 14.494436264038086 = 1.7275947332382202 + 2.0 * 6.383420944213867
Epoch 130, val loss: 1.7384272813796997
Epoch 140, training loss: 14.42043399810791 = 1.7070395946502686 + 2.0 * 6.356697082519531
Epoch 140, val loss: 1.7202681303024292
Epoch 150, training loss: 14.35019302368164 = 1.6842247247695923 + 2.0 * 6.33298397064209
Epoch 150, val loss: 1.7003885507583618
Epoch 160, training loss: 14.291694641113281 = 1.6586426496505737 + 2.0 * 6.316525936126709
Epoch 160, val loss: 1.6784262657165527
Epoch 170, training loss: 14.225232124328613 = 1.630078911781311 + 2.0 * 6.297576427459717
Epoch 170, val loss: 1.6540995836257935
Epoch 180, training loss: 14.165511131286621 = 1.5984164476394653 + 2.0 * 6.283547401428223
Epoch 180, val loss: 1.6274433135986328
Epoch 190, training loss: 14.100860595703125 = 1.5638269186019897 + 2.0 * 6.268517017364502
Epoch 190, val loss: 1.5987316370010376
Epoch 200, training loss: 14.03931713104248 = 1.526097059249878 + 2.0 * 6.256609916687012
Epoch 200, val loss: 1.5679527521133423
Epoch 210, training loss: 13.974596977233887 = 1.4861088991165161 + 2.0 * 6.24424409866333
Epoch 210, val loss: 1.5361028909683228
Epoch 220, training loss: 13.912128448486328 = 1.4442830085754395 + 2.0 * 6.233922481536865
Epoch 220, val loss: 1.5033833980560303
Epoch 230, training loss: 13.856715202331543 = 1.400791883468628 + 2.0 * 6.227961540222168
Epoch 230, val loss: 1.46998131275177
Epoch 240, training loss: 13.788291931152344 = 1.3565181493759155 + 2.0 * 6.215887069702148
Epoch 240, val loss: 1.4364972114562988
Epoch 250, training loss: 13.725906372070312 = 1.3114501237869263 + 2.0 * 6.207228183746338
Epoch 250, val loss: 1.402980923652649
Epoch 260, training loss: 13.664202690124512 = 1.2658607959747314 + 2.0 * 6.19917106628418
Epoch 260, val loss: 1.3694384098052979
Epoch 270, training loss: 13.610767364501953 = 1.2202742099761963 + 2.0 * 6.195246696472168
Epoch 270, val loss: 1.3364806175231934
Epoch 280, training loss: 13.548367500305176 = 1.1753997802734375 + 2.0 * 6.186483860015869
Epoch 280, val loss: 1.3041406869888306
Epoch 290, training loss: 13.489741325378418 = 1.1309715509414673 + 2.0 * 6.179384708404541
Epoch 290, val loss: 1.272393822669983
Epoch 300, training loss: 13.435406684875488 = 1.0872418880462646 + 2.0 * 6.174082279205322
Epoch 300, val loss: 1.2412546873092651
Epoch 310, training loss: 13.382394790649414 = 1.044550895690918 + 2.0 * 6.168921947479248
Epoch 310, val loss: 1.2110830545425415
Epoch 320, training loss: 13.329259872436523 = 1.0030360221862793 + 2.0 * 6.163112163543701
Epoch 320, val loss: 1.1817448139190674
Epoch 330, training loss: 13.280696868896484 = 0.9629630446434021 + 2.0 * 6.158866882324219
Epoch 330, val loss: 1.1535663604736328
Epoch 340, training loss: 13.236091613769531 = 0.9242919087409973 + 2.0 * 6.155900001525879
Epoch 340, val loss: 1.1264746189117432
Epoch 350, training loss: 13.190053939819336 = 0.8875182271003723 + 2.0 * 6.151268005371094
Epoch 350, val loss: 1.100730538368225
Epoch 360, training loss: 13.142985343933105 = 0.8524981141090393 + 2.0 * 6.1452436447143555
Epoch 360, val loss: 1.0766079425811768
Epoch 370, training loss: 13.101899147033691 = 0.819334089756012 + 2.0 * 6.141282558441162
Epoch 370, val loss: 1.0540297031402588
Epoch 380, training loss: 13.06822395324707 = 0.7882066369056702 + 2.0 * 6.140008449554443
Epoch 380, val loss: 1.032924771308899
Epoch 390, training loss: 13.027836799621582 = 0.7591636180877686 + 2.0 * 6.134336471557617
Epoch 390, val loss: 1.0136497020721436
Epoch 400, training loss: 12.993159294128418 = 0.7319781184196472 + 2.0 * 6.130590438842773
Epoch 400, val loss: 0.9960734248161316
Epoch 410, training loss: 12.975175857543945 = 0.7063810229301453 + 2.0 * 6.134397506713867
Epoch 410, val loss: 0.9799665808677673
Epoch 420, training loss: 12.934562683105469 = 0.6823442578315735 + 2.0 * 6.1261091232299805
Epoch 420, val loss: 0.9650737643241882
Epoch 430, training loss: 12.903233528137207 = 0.6595795154571533 + 2.0 * 6.121827125549316
Epoch 430, val loss: 0.951443612575531
Epoch 440, training loss: 12.876642227172852 = 0.6378721594810486 + 2.0 * 6.119385242462158
Epoch 440, val loss: 0.9386608004570007
Epoch 450, training loss: 12.85641860961914 = 0.6170167326927185 + 2.0 * 6.119700908660889
Epoch 450, val loss: 0.926708996295929
Epoch 460, training loss: 12.829000473022461 = 0.5970125198364258 + 2.0 * 6.115993976593018
Epoch 460, val loss: 0.9155434370040894
Epoch 470, training loss: 12.809258460998535 = 0.5775917172431946 + 2.0 * 6.115833282470703
Epoch 470, val loss: 0.9049544930458069
Epoch 480, training loss: 12.778275489807129 = 0.558838427066803 + 2.0 * 6.109718322753906
Epoch 480, val loss: 0.8951231837272644
Epoch 490, training loss: 12.755678176879883 = 0.5405627489089966 + 2.0 * 6.107557773590088
Epoch 490, val loss: 0.8859203457832336
Epoch 500, training loss: 12.737263679504395 = 0.5227027535438538 + 2.0 * 6.107280254364014
Epoch 500, val loss: 0.8773060441017151
Epoch 510, training loss: 12.718400955200195 = 0.5052075386047363 + 2.0 * 6.106596946716309
Epoch 510, val loss: 0.8691685199737549
Epoch 520, training loss: 12.691842079162598 = 0.48803800344467163 + 2.0 * 6.101902008056641
Epoch 520, val loss: 0.8616684675216675
Epoch 530, training loss: 12.677081108093262 = 0.4711340069770813 + 2.0 * 6.102973461151123
Epoch 530, val loss: 0.8547848463058472
Epoch 540, training loss: 12.653447151184082 = 0.45434439182281494 + 2.0 * 6.099551200866699
Epoch 540, val loss: 0.8484354019165039
Epoch 550, training loss: 12.631428718566895 = 0.4377884566783905 + 2.0 * 6.09682035446167
Epoch 550, val loss: 0.8426238298416138
Epoch 560, training loss: 12.61963939666748 = 0.42134395241737366 + 2.0 * 6.099147796630859
Epoch 560, val loss: 0.8374015688896179
Epoch 570, training loss: 12.591568946838379 = 0.4050513803958893 + 2.0 * 6.093258857727051
Epoch 570, val loss: 0.8327603936195374
Epoch 580, training loss: 12.571061134338379 = 0.3890104591846466 + 2.0 * 6.091025352478027
Epoch 580, val loss: 0.8288002610206604
Epoch 590, training loss: 12.577411651611328 = 0.3731425702571869 + 2.0 * 6.102134704589844
Epoch 590, val loss: 0.8254486322402954
Epoch 600, training loss: 12.534209251403809 = 0.3577285408973694 + 2.0 * 6.088240146636963
Epoch 600, val loss: 0.8228504657745361
Epoch 610, training loss: 12.514655113220215 = 0.34267908334732056 + 2.0 * 6.0859880447387695
Epoch 610, val loss: 0.8210119605064392
Epoch 620, training loss: 12.495824813842773 = 0.3280552923679352 + 2.0 * 6.0838847160339355
Epoch 620, val loss: 0.8199206590652466
Epoch 630, training loss: 12.479585647583008 = 0.313860148191452 + 2.0 * 6.082862854003906
Epoch 630, val loss: 0.8195858597755432
Epoch 640, training loss: 12.48032283782959 = 0.3001202940940857 + 2.0 * 6.09010124206543
Epoch 640, val loss: 0.819902241230011
Epoch 650, training loss: 12.452142715454102 = 0.2869704067707062 + 2.0 * 6.082586288452148
Epoch 650, val loss: 0.8208879232406616
Epoch 660, training loss: 12.432409286499023 = 0.27433982491493225 + 2.0 * 6.079034805297852
Epoch 660, val loss: 0.8225364685058594
Epoch 670, training loss: 12.416855812072754 = 0.26213857531547546 + 2.0 * 6.077358722686768
Epoch 670, val loss: 0.8246810436248779
Epoch 680, training loss: 12.417062759399414 = 0.25037604570388794 + 2.0 * 6.083343505859375
Epoch 680, val loss: 0.827315628528595
Epoch 690, training loss: 12.391618728637695 = 0.23905307054519653 + 2.0 * 6.076282978057861
Epoch 690, val loss: 0.8303451538085938
Epoch 700, training loss: 12.376690864562988 = 0.22810098528862 + 2.0 * 6.0742950439453125
Epoch 700, val loss: 0.8338521718978882
Epoch 710, training loss: 12.362086296081543 = 0.21753373742103577 + 2.0 * 6.0722761154174805
Epoch 710, val loss: 0.837691068649292
Epoch 720, training loss: 12.355945587158203 = 0.2072974145412445 + 2.0 * 6.074324131011963
Epoch 720, val loss: 0.8418893218040466
Epoch 730, training loss: 12.337005615234375 = 0.1973513811826706 + 2.0 * 6.069827079772949
Epoch 730, val loss: 0.8462170362472534
Epoch 740, training loss: 12.32552433013916 = 0.1877768486738205 + 2.0 * 6.068873882293701
Epoch 740, val loss: 0.850963830947876
Epoch 750, training loss: 12.328049659729004 = 0.17853288352489471 + 2.0 * 6.074758529663086
Epoch 750, val loss: 0.8557942509651184
Epoch 760, training loss: 12.30798625946045 = 0.16967308521270752 + 2.0 * 6.069156646728516
Epoch 760, val loss: 0.8610864281654358
Epoch 770, training loss: 12.291254997253418 = 0.16114474833011627 + 2.0 * 6.065054893493652
Epoch 770, val loss: 0.8666095733642578
Epoch 780, training loss: 12.280806541442871 = 0.15296971797943115 + 2.0 * 6.063918590545654
Epoch 780, val loss: 0.8723974823951721
Epoch 790, training loss: 12.271703720092773 = 0.14514218270778656 + 2.0 * 6.0632805824279785
Epoch 790, val loss: 0.8784973621368408
Epoch 800, training loss: 12.264371871948242 = 0.13770119845867157 + 2.0 * 6.063335418701172
Epoch 800, val loss: 0.8845852017402649
Epoch 810, training loss: 12.259620666503906 = 0.130661278963089 + 2.0 * 6.064479827880859
Epoch 810, val loss: 0.891096293926239
Epoch 820, training loss: 12.24435043334961 = 0.12401124089956284 + 2.0 * 6.060169696807861
Epoch 820, val loss: 0.8976467251777649
Epoch 830, training loss: 12.244498252868652 = 0.11773603409528732 + 2.0 * 6.063381195068359
Epoch 830, val loss: 0.9042918086051941
Epoch 840, training loss: 12.231756210327148 = 0.11183620989322662 + 2.0 * 6.059959888458252
Epoch 840, val loss: 0.9112740755081177
Epoch 850, training loss: 12.221888542175293 = 0.1062769740819931 + 2.0 * 6.057806015014648
Epoch 850, val loss: 0.9182974696159363
Epoch 860, training loss: 12.220602989196777 = 0.10106058418750763 + 2.0 * 6.0597710609436035
Epoch 860, val loss: 0.9254634976387024
Epoch 870, training loss: 12.21796703338623 = 0.09612754732370377 + 2.0 * 6.060919761657715
Epoch 870, val loss: 0.9323760271072388
Epoch 880, training loss: 12.2002592086792 = 0.09153782576322556 + 2.0 * 6.054360866546631
Epoch 880, val loss: 0.9395949244499207
Epoch 890, training loss: 12.195192337036133 = 0.08723395317792892 + 2.0 * 6.053979396820068
Epoch 890, val loss: 0.9467706084251404
Epoch 900, training loss: 12.189033508300781 = 0.08318496495485306 + 2.0 * 6.052924156188965
Epoch 900, val loss: 0.9538996815681458
Epoch 910, training loss: 12.191961288452148 = 0.07938330620527267 + 2.0 * 6.056289196014404
Epoch 910, val loss: 0.9610546827316284
Epoch 920, training loss: 12.178322792053223 = 0.07578910142183304 + 2.0 * 6.051266670227051
Epoch 920, val loss: 0.968106210231781
Epoch 930, training loss: 12.172675132751465 = 0.0724228024482727 + 2.0 * 6.050126075744629
Epoch 930, val loss: 0.9751573204994202
Epoch 940, training loss: 12.175613403320312 = 0.06925343722105026 + 2.0 * 6.05318021774292
Epoch 940, val loss: 0.9820573329925537
Epoch 950, training loss: 12.168821334838867 = 0.06626565754413605 + 2.0 * 6.0512776374816895
Epoch 950, val loss: 0.9888648986816406
Epoch 960, training loss: 12.159306526184082 = 0.06346214562654495 + 2.0 * 6.047922134399414
Epoch 960, val loss: 0.9956386089324951
Epoch 970, training loss: 12.156590461730957 = 0.060814399272203445 + 2.0 * 6.047887802124023
Epoch 970, val loss: 1.0023272037506104
Epoch 980, training loss: 12.158520698547363 = 0.058326613157987595 + 2.0 * 6.0500969886779785
Epoch 980, val loss: 1.0088931322097778
Epoch 990, training loss: 12.149938583374023 = 0.05597307160496712 + 2.0 * 6.046982765197754
Epoch 990, val loss: 1.01541006565094
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7889
Overall ASR: 0.7675
Flip ASR: 0.7244/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.695960998535156 = 1.9481619596481323 + 2.0 * 8.373899459838867
Epoch 0, val loss: 1.943654179573059
Epoch 10, training loss: 18.684383392333984 = 1.937290906906128 + 2.0 * 8.373546600341797
Epoch 10, val loss: 1.9323748350143433
Epoch 20, training loss: 18.66660499572754 = 1.924100637435913 + 2.0 * 8.371252059936523
Epoch 20, val loss: 1.9186010360717773
Epoch 30, training loss: 18.61687660217285 = 1.9066141843795776 + 2.0 * 8.355131149291992
Epoch 30, val loss: 1.9006577730178833
Epoch 40, training loss: 18.386287689208984 = 1.8851633071899414 + 2.0 * 8.25056266784668
Epoch 40, val loss: 1.8797231912612915
Epoch 50, training loss: 17.58226203918457 = 1.8618992567062378 + 2.0 * 7.8601813316345215
Epoch 50, val loss: 1.8576135635375977
Epoch 60, training loss: 16.771053314208984 = 1.84146249294281 + 2.0 * 7.464795112609863
Epoch 60, val loss: 1.8393077850341797
Epoch 70, training loss: 15.913407325744629 = 1.8281461000442505 + 2.0 * 7.042630672454834
Epoch 70, val loss: 1.8269215822219849
Epoch 80, training loss: 15.391646385192871 = 1.8162599802017212 + 2.0 * 6.787693023681641
Epoch 80, val loss: 1.815358281135559
Epoch 90, training loss: 15.053215026855469 = 1.8028315305709839 + 2.0 * 6.625191688537598
Epoch 90, val loss: 1.8025102615356445
Epoch 100, training loss: 14.85672664642334 = 1.7876676321029663 + 2.0 * 6.534529685974121
Epoch 100, val loss: 1.7882987260818481
Epoch 110, training loss: 14.726861953735352 = 1.7712643146514893 + 2.0 * 6.477798938751221
Epoch 110, val loss: 1.773017168045044
Epoch 120, training loss: 14.624935150146484 = 1.7544636726379395 + 2.0 * 6.435235977172852
Epoch 120, val loss: 1.7572100162506104
Epoch 130, training loss: 14.537067413330078 = 1.7372444868087769 + 2.0 * 6.399911403656006
Epoch 130, val loss: 1.7409082651138306
Epoch 140, training loss: 14.458162307739258 = 1.7186497449874878 + 2.0 * 6.36975622177124
Epoch 140, val loss: 1.7233660221099854
Epoch 150, training loss: 14.385735511779785 = 1.698124885559082 + 2.0 * 6.343805313110352
Epoch 150, val loss: 1.7043612003326416
Epoch 160, training loss: 14.315370559692383 = 1.6753809452056885 + 2.0 * 6.319994926452637
Epoch 160, val loss: 1.683523178100586
Epoch 170, training loss: 14.254631042480469 = 1.6499600410461426 + 2.0 * 6.302335739135742
Epoch 170, val loss: 1.6605340242385864
Epoch 180, training loss: 14.190929412841797 = 1.6216588020324707 + 2.0 * 6.284635066986084
Epoch 180, val loss: 1.635285496711731
Epoch 190, training loss: 14.132662773132324 = 1.5902070999145508 + 2.0 * 6.271227836608887
Epoch 190, val loss: 1.6075575351715088
Epoch 200, training loss: 14.073145866394043 = 1.5556880235671997 + 2.0 * 6.258728981018066
Epoch 200, val loss: 1.5775419473648071
Epoch 210, training loss: 14.011094093322754 = 1.5181211233139038 + 2.0 * 6.246486663818359
Epoch 210, val loss: 1.5456147193908691
Epoch 220, training loss: 13.950285911560059 = 1.4776153564453125 + 2.0 * 6.236335277557373
Epoch 220, val loss: 1.5116865634918213
Epoch 230, training loss: 13.897682189941406 = 1.4346468448638916 + 2.0 * 6.231517791748047
Epoch 230, val loss: 1.4763705730438232
Epoch 240, training loss: 13.83198356628418 = 1.3902366161346436 + 2.0 * 6.2208733558654785
Epoch 240, val loss: 1.4406520128250122
Epoch 250, training loss: 13.764769554138184 = 1.3447633981704712 + 2.0 * 6.210002899169922
Epoch 250, val loss: 1.4047139883041382
Epoch 260, training loss: 13.703043937683105 = 1.2982500791549683 + 2.0 * 6.202396869659424
Epoch 260, val loss: 1.368451476097107
Epoch 270, training loss: 13.645646095275879 = 1.2509404420852661 + 2.0 * 6.197352886199951
Epoch 270, val loss: 1.3320400714874268
Epoch 280, training loss: 13.583757400512695 = 1.2033815383911133 + 2.0 * 6.190187931060791
Epoch 280, val loss: 1.2958242893218994
Epoch 290, training loss: 13.521109580993652 = 1.1557320356369019 + 2.0 * 6.1826887130737305
Epoch 290, val loss: 1.2599446773529053
Epoch 300, training loss: 13.461698532104492 = 1.1080480813980103 + 2.0 * 6.176825046539307
Epoch 300, val loss: 1.2244263887405396
Epoch 310, training loss: 13.410231590270996 = 1.0605872869491577 + 2.0 * 6.1748223304748535
Epoch 310, val loss: 1.1895463466644287
Epoch 320, training loss: 13.34908390045166 = 1.0139883756637573 + 2.0 * 6.167547702789307
Epoch 320, val loss: 1.15550696849823
Epoch 330, training loss: 13.293020248413086 = 0.9682989716529846 + 2.0 * 6.162360668182373
Epoch 330, val loss: 1.122282862663269
Epoch 340, training loss: 13.237911224365234 = 0.9235182404518127 + 2.0 * 6.157196521759033
Epoch 340, val loss: 1.0899580717086792
Epoch 350, training loss: 13.186042785644531 = 0.8797584176063538 + 2.0 * 6.153141975402832
Epoch 350, val loss: 1.0586687326431274
Epoch 360, training loss: 13.14551067352295 = 0.8374250531196594 + 2.0 * 6.154042720794678
Epoch 360, val loss: 1.028646469116211
Epoch 370, training loss: 13.093445777893066 = 0.7969749569892883 + 2.0 * 6.148235321044922
Epoch 370, val loss: 1.000458002090454
Epoch 380, training loss: 13.043771743774414 = 0.7587755918502808 + 2.0 * 6.142498016357422
Epoch 380, val loss: 0.9741504788398743
Epoch 390, training loss: 13.000239372253418 = 0.722598671913147 + 2.0 * 6.138820171356201
Epoch 390, val loss: 0.9498291015625
Epoch 400, training loss: 12.959226608276367 = 0.6884703040122986 + 2.0 * 6.135378360748291
Epoch 400, val loss: 0.9274728298187256
Epoch 410, training loss: 12.945006370544434 = 0.6563569903373718 + 2.0 * 6.144324779510498
Epoch 410, val loss: 0.9072169065475464
Epoch 420, training loss: 12.886568069458008 = 0.6266359686851501 + 2.0 * 6.1299662590026855
Epoch 420, val loss: 0.8889078497886658
Epoch 430, training loss: 12.85425090789795 = 0.5987411141395569 + 2.0 * 6.1277546882629395
Epoch 430, val loss: 0.8726809620857239
Epoch 440, training loss: 12.82107925415039 = 0.5724557042121887 + 2.0 * 6.124311923980713
Epoch 440, val loss: 0.8580551743507385
Epoch 450, training loss: 12.790948867797852 = 0.5474649667739868 + 2.0 * 6.121741771697998
Epoch 450, val loss: 0.8449345231056213
Epoch 460, training loss: 12.761874198913574 = 0.5238369107246399 + 2.0 * 6.1190185546875
Epoch 460, val loss: 0.8329955339431763
Epoch 470, training loss: 12.73468017578125 = 0.5011348724365234 + 2.0 * 6.116772651672363
Epoch 470, val loss: 0.822211503982544
Epoch 480, training loss: 12.707765579223633 = 0.4791732728481293 + 2.0 * 6.114295959472656
Epoch 480, val loss: 0.8123771548271179
Epoch 490, training loss: 12.69580078125 = 0.4577835202217102 + 2.0 * 6.119008541107178
Epoch 490, val loss: 0.8033960461616516
Epoch 500, training loss: 12.657279968261719 = 0.4371121823787689 + 2.0 * 6.110084056854248
Epoch 500, val loss: 0.7950988411903381
Epoch 510, training loss: 12.632781982421875 = 0.41695523262023926 + 2.0 * 6.107913494110107
Epoch 510, val loss: 0.7875384092330933
Epoch 520, training loss: 12.607510566711426 = 0.39718255400657654 + 2.0 * 6.105164051055908
Epoch 520, val loss: 0.7806919813156128
Epoch 530, training loss: 12.59873104095459 = 0.3777814507484436 + 2.0 * 6.110474586486816
Epoch 530, val loss: 0.774547278881073
Epoch 540, training loss: 12.56285572052002 = 0.3589073717594147 + 2.0 * 6.101974010467529
Epoch 540, val loss: 0.7689149379730225
Epoch 550, training loss: 12.540201187133789 = 0.34055352210998535 + 2.0 * 6.099823951721191
Epoch 550, val loss: 0.7640369534492493
Epoch 560, training loss: 12.522369384765625 = 0.3227371275424957 + 2.0 * 6.09981632232666
Epoch 560, val loss: 0.7598779201507568
Epoch 570, training loss: 12.502830505371094 = 0.3054831922054291 + 2.0 * 6.0986738204956055
Epoch 570, val loss: 0.7564572095870972
Epoch 580, training loss: 12.480460166931152 = 0.28899407386779785 + 2.0 * 6.095733165740967
Epoch 580, val loss: 0.7535616755485535
Epoch 590, training loss: 12.459840774536133 = 0.2731226086616516 + 2.0 * 6.093358993530273
Epoch 590, val loss: 0.7516314387321472
Epoch 600, training loss: 12.441065788269043 = 0.2579898536205292 + 2.0 * 6.091537952423096
Epoch 600, val loss: 0.750302255153656
Epoch 610, training loss: 12.444082260131836 = 0.24355462193489075 + 2.0 * 6.100263595581055
Epoch 610, val loss: 0.7497860193252563
Epoch 620, training loss: 12.41443920135498 = 0.23005904257297516 + 2.0 * 6.092190265655518
Epoch 620, val loss: 0.7497535347938538
Epoch 630, training loss: 12.39346694946289 = 0.21734781563282013 + 2.0 * 6.088059425354004
Epoch 630, val loss: 0.7505287528038025
Epoch 640, training loss: 12.375931739807129 = 0.20540878176689148 + 2.0 * 6.085261344909668
Epoch 640, val loss: 0.7519876956939697
Epoch 650, training loss: 12.368553161621094 = 0.1941853016614914 + 2.0 * 6.087183952331543
Epoch 650, val loss: 0.7540561556816101
Epoch 660, training loss: 12.360398292541504 = 0.1836867779493332 + 2.0 * 6.088355541229248
Epoch 660, val loss: 0.7565277814865112
Epoch 670, training loss: 12.335712432861328 = 0.17389260232448578 + 2.0 * 6.080909729003906
Epoch 670, val loss: 0.7596385478973389
Epoch 680, training loss: 12.323287963867188 = 0.16473332047462463 + 2.0 * 6.079277515411377
Epoch 680, val loss: 0.7632806897163391
Epoch 690, training loss: 12.311363220214844 = 0.1561581790447235 + 2.0 * 6.077602386474609
Epoch 690, val loss: 0.7673192620277405
Epoch 700, training loss: 12.336270332336426 = 0.14817491173744202 + 2.0 * 6.094047546386719
Epoch 700, val loss: 0.7716691493988037
Epoch 710, training loss: 12.29374885559082 = 0.14064355194568634 + 2.0 * 6.076552867889404
Epoch 710, val loss: 0.7763328552246094
Epoch 720, training loss: 12.28195858001709 = 0.1336410492658615 + 2.0 * 6.074158668518066
Epoch 720, val loss: 0.7813779711723328
Epoch 730, training loss: 12.271709442138672 = 0.1270948350429535 + 2.0 * 6.072307109832764
Epoch 730, val loss: 0.7867720127105713
Epoch 740, training loss: 12.265013694763184 = 0.12094561755657196 + 2.0 * 6.072033882141113
Epoch 740, val loss: 0.7924308180809021
Epoch 750, training loss: 12.256711959838867 = 0.11517734080553055 + 2.0 * 6.070767402648926
Epoch 750, val loss: 0.7983113527297974
Epoch 760, training loss: 12.251228332519531 = 0.109774149954319 + 2.0 * 6.0707268714904785
Epoch 760, val loss: 0.8043212890625
Epoch 770, training loss: 12.239925384521484 = 0.10471026599407196 + 2.0 * 6.067607402801514
Epoch 770, val loss: 0.8105620741844177
Epoch 780, training loss: 12.244255065917969 = 0.09993308037519455 + 2.0 * 6.0721611976623535
Epoch 780, val loss: 0.8169045448303223
Epoch 790, training loss: 12.231193542480469 = 0.09542597830295563 + 2.0 * 6.0678839683532715
Epoch 790, val loss: 0.8233740329742432
Epoch 800, training loss: 12.223515510559082 = 0.09119100868701935 + 2.0 * 6.066162109375
Epoch 800, val loss: 0.8299757838249207
Epoch 810, training loss: 12.218728065490723 = 0.08718883246183395 + 2.0 * 6.065769672393799
Epoch 810, val loss: 0.8366917371749878
Epoch 820, training loss: 12.211674690246582 = 0.08341053873300552 + 2.0 * 6.064132213592529
Epoch 820, val loss: 0.843498945236206
Epoch 830, training loss: 12.202796936035156 = 0.07983174175024033 + 2.0 * 6.0614824295043945
Epoch 830, val loss: 0.850460946559906
Epoch 840, training loss: 12.199824333190918 = 0.07644923776388168 + 2.0 * 6.061687469482422
Epoch 840, val loss: 0.8574486374855042
Epoch 850, training loss: 12.19464111328125 = 0.07323835045099258 + 2.0 * 6.060701370239258
Epoch 850, val loss: 0.864463746547699
Epoch 860, training loss: 12.188029289245605 = 0.07019968330860138 + 2.0 * 6.058914661407471
Epoch 860, val loss: 0.8715583086013794
Epoch 870, training loss: 12.188673973083496 = 0.06733052432537079 + 2.0 * 6.060671806335449
Epoch 870, val loss: 0.8786449432373047
Epoch 880, training loss: 12.17835807800293 = 0.06459242105484009 + 2.0 * 6.056882858276367
Epoch 880, val loss: 0.8857159614562988
Epoch 890, training loss: 12.173063278198242 = 0.06200708821415901 + 2.0 * 6.055528163909912
Epoch 890, val loss: 0.892870306968689
Epoch 900, training loss: 12.170483589172363 = 0.05954728275537491 + 2.0 * 6.0554680824279785
Epoch 900, val loss: 0.9000358581542969
Epoch 910, training loss: 12.168954849243164 = 0.057213980704545975 + 2.0 * 6.055870532989502
Epoch 910, val loss: 0.9071680903434753
Epoch 920, training loss: 12.167317390441895 = 0.05500531941652298 + 2.0 * 6.056156158447266
Epoch 920, val loss: 0.9142488837242126
Epoch 930, training loss: 12.162674903869629 = 0.05291062965989113 + 2.0 * 6.054882049560547
Epoch 930, val loss: 0.9213823080062866
Epoch 940, training loss: 12.154718399047852 = 0.050931427627801895 + 2.0 * 6.051893711090088
Epoch 940, val loss: 0.9284661412239075
Epoch 950, training loss: 12.15368938446045 = 0.04905189946293831 + 2.0 * 6.052318572998047
Epoch 950, val loss: 0.9355108737945557
Epoch 960, training loss: 12.145511627197266 = 0.04725971817970276 + 2.0 * 6.049126148223877
Epoch 960, val loss: 0.9425233006477356
Epoch 970, training loss: 12.151291847229004 = 0.04556630551815033 + 2.0 * 6.052862644195557
Epoch 970, val loss: 0.9495134353637695
Epoch 980, training loss: 12.144370079040527 = 0.04396450147032738 + 2.0 * 6.0502028465271
Epoch 980, val loss: 0.9564061164855957
Epoch 990, training loss: 12.136065483093262 = 0.0424305684864521 + 2.0 * 6.046817302703857
Epoch 990, val loss: 0.9633039832115173
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7926
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
The final ASR:0.81181, 0.11869, Accuracy:0.79383, 0.00462
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11612])
remove edge: torch.Size([2, 9542])
updated graph: torch.Size([2, 10598])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
The final ASR:0.97909, 0.00348, Accuracy:0.82963, 0.00524
Begin epxeriment: cont_weight: 2 epoch:2000 der1:0.3 der2:0 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 18.702425003051758 = 1.954712986946106 + 2.0 * 8.373855590820312
Epoch 0, val loss: 1.9559510946273804
Epoch 10, training loss: 18.69083023071289 = 1.9440680742263794 + 2.0 * 8.373380661010742
Epoch 10, val loss: 1.9458824396133423
Epoch 20, training loss: 18.671297073364258 = 1.9308393001556396 + 2.0 * 8.37022876739502
Epoch 20, val loss: 1.932830810546875
Epoch 30, training loss: 18.610183715820312 = 1.912898302078247 + 2.0 * 8.348642349243164
Epoch 30, val loss: 1.914708137512207
Epoch 40, training loss: 18.30961799621582 = 1.8912123441696167 + 2.0 * 8.209202766418457
Epoch 40, val loss: 1.8933210372924805
Epoch 50, training loss: 16.99452018737793 = 1.86765718460083 + 2.0 * 7.563431739807129
Epoch 50, val loss: 1.8701914548873901
Epoch 60, training loss: 16.09531593322754 = 1.8505887985229492 + 2.0 * 7.122363567352295
Epoch 60, val loss: 1.8545663356781006
Epoch 70, training loss: 15.41712760925293 = 1.8389147520065308 + 2.0 * 6.789106369018555
Epoch 70, val loss: 1.843302845954895
Epoch 80, training loss: 15.029701232910156 = 1.8279361724853516 + 2.0 * 6.600882530212402
Epoch 80, val loss: 1.832839012145996
Epoch 90, training loss: 14.798022270202637 = 1.8184783458709717 + 2.0 * 6.489771842956543
Epoch 90, val loss: 1.8232535123825073
Epoch 100, training loss: 14.64283275604248 = 1.8094972372055054 + 2.0 * 6.416667938232422
Epoch 100, val loss: 1.814196228981018
Epoch 110, training loss: 14.523317337036133 = 1.8012629747390747 + 2.0 * 6.361027240753174
Epoch 110, val loss: 1.805999517440796
Epoch 120, training loss: 14.423896789550781 = 1.7937089204788208 + 2.0 * 6.315093994140625
Epoch 120, val loss: 1.7987221479415894
Epoch 130, training loss: 14.346806526184082 = 1.7863560914993286 + 2.0 * 6.2802252769470215
Epoch 130, val loss: 1.791764497756958
Epoch 140, training loss: 14.283632278442383 = 1.778808355331421 + 2.0 * 6.252411842346191
Epoch 140, val loss: 1.7847048044204712
Epoch 150, training loss: 14.235240936279297 = 1.7706570625305176 + 2.0 * 6.2322916984558105
Epoch 150, val loss: 1.7773226499557495
Epoch 160, training loss: 14.189617156982422 = 1.7617764472961426 + 2.0 * 6.213920593261719
Epoch 160, val loss: 1.7694275379180908
Epoch 170, training loss: 14.148918151855469 = 1.7519561052322388 + 2.0 * 6.19848108291626
Epoch 170, val loss: 1.760931134223938
Epoch 180, training loss: 14.113853454589844 = 1.7409868240356445 + 2.0 * 6.1864333152771
Epoch 180, val loss: 1.7516381740570068
Epoch 190, training loss: 14.075544357299805 = 1.7285236120224 + 2.0 * 6.173510551452637
Epoch 190, val loss: 1.7413679361343384
Epoch 200, training loss: 14.041348457336426 = 1.7142255306243896 + 2.0 * 6.1635613441467285
Epoch 200, val loss: 1.729783058166504
Epoch 210, training loss: 14.009284019470215 = 1.697684407234192 + 2.0 * 6.155799865722656
Epoch 210, val loss: 1.71654212474823
Epoch 220, training loss: 13.974627494812012 = 1.6786810159683228 + 2.0 * 6.14797306060791
Epoch 220, val loss: 1.7012888193130493
Epoch 230, training loss: 13.938514709472656 = 1.6565510034561157 + 2.0 * 6.140981674194336
Epoch 230, val loss: 1.683749794960022
Epoch 240, training loss: 13.900341987609863 = 1.6307839155197144 + 2.0 * 6.13477897644043
Epoch 240, val loss: 1.6633107662200928
Epoch 250, training loss: 13.86211109161377 = 1.6005951166152954 + 2.0 * 6.130757808685303
Epoch 250, val loss: 1.639451026916504
Epoch 260, training loss: 13.815995216369629 = 1.5661245584487915 + 2.0 * 6.124935150146484
Epoch 260, val loss: 1.6119259595870972
Epoch 270, training loss: 13.768630027770996 = 1.526828408241272 + 2.0 * 6.120900630950928
Epoch 270, val loss: 1.5806490182876587
Epoch 280, training loss: 13.715819358825684 = 1.4825048446655273 + 2.0 * 6.116657257080078
Epoch 280, val loss: 1.5451757907867432
Epoch 290, training loss: 13.659477233886719 = 1.4331716299057007 + 2.0 * 6.113152980804443
Epoch 290, val loss: 1.5054950714111328
Epoch 300, training loss: 13.60265064239502 = 1.3800525665283203 + 2.0 * 6.11129903793335
Epoch 300, val loss: 1.4628112316131592
Epoch 310, training loss: 13.537832260131836 = 1.3245457410812378 + 2.0 * 6.106643199920654
Epoch 310, val loss: 1.4181488752365112
Epoch 320, training loss: 13.473445892333984 = 1.267432689666748 + 2.0 * 6.103006839752197
Epoch 320, val loss: 1.3723151683807373
Epoch 330, training loss: 13.421711921691895 = 1.2100039720535278 + 2.0 * 6.105854034423828
Epoch 330, val loss: 1.3263615369796753
Epoch 340, training loss: 13.34814739227295 = 1.154276728630066 + 2.0 * 6.096935272216797
Epoch 340, val loss: 1.2820903062820435
Epoch 350, training loss: 13.28993034362793 = 1.1007189750671387 + 2.0 * 6.094605445861816
Epoch 350, val loss: 1.2397178411483765
Epoch 360, training loss: 13.236372947692871 = 1.0493582487106323 + 2.0 * 6.093507289886475
Epoch 360, val loss: 1.19941246509552
Epoch 370, training loss: 13.181011199951172 = 1.0013947486877441 + 2.0 * 6.089807987213135
Epoch 370, val loss: 1.1615382432937622
Epoch 380, training loss: 13.128337860107422 = 0.9560484290122986 + 2.0 * 6.086144924163818
Epoch 380, val loss: 1.1262946128845215
Epoch 390, training loss: 13.086793899536133 = 0.9127967357635498 + 2.0 * 6.086998462677002
Epoch 390, val loss: 1.0928531885147095
Epoch 400, training loss: 13.034416198730469 = 0.8716978430747986 + 2.0 * 6.081359386444092
Epoch 400, val loss: 1.061231017112732
Epoch 410, training loss: 12.99089527130127 = 0.8322630524635315 + 2.0 * 6.079316139221191
Epoch 410, val loss: 1.0311659574508667
Epoch 420, training loss: 12.947696685791016 = 0.7938883304595947 + 2.0 * 6.076904296875
Epoch 420, val loss: 1.001988172531128
Epoch 430, training loss: 12.91773509979248 = 0.7565503716468811 + 2.0 * 6.080592155456543
Epoch 430, val loss: 0.9738283753395081
Epoch 440, training loss: 12.86766242980957 = 0.7207232117652893 + 2.0 * 6.073469638824463
Epoch 440, val loss: 0.947169303894043
Epoch 450, training loss: 12.831075668334961 = 0.6863542199134827 + 2.0 * 6.072360515594482
Epoch 450, val loss: 0.9219352006912231
Epoch 460, training loss: 12.793034553527832 = 0.6536952257156372 + 2.0 * 6.069669723510742
Epoch 460, val loss: 0.8982008695602417
Epoch 470, training loss: 12.761999130249023 = 0.6226832270622253 + 2.0 * 6.069657802581787
Epoch 470, val loss: 0.8763004541397095
Epoch 480, training loss: 12.728121757507324 = 0.5935364961624146 + 2.0 * 6.0672926902771
Epoch 480, val loss: 0.8565235733985901
Epoch 490, training loss: 12.694239616394043 = 0.5664027333259583 + 2.0 * 6.063918590545654
Epoch 490, val loss: 0.8388125896453857
Epoch 500, training loss: 12.665098190307617 = 0.540778636932373 + 2.0 * 6.062160015106201
Epoch 500, val loss: 0.8229877352714539
Epoch 510, training loss: 12.64431381225586 = 0.5166782140731812 + 2.0 * 6.063817977905273
Epoch 510, val loss: 0.8089804649353027
Epoch 520, training loss: 12.613815307617188 = 0.49416282773017883 + 2.0 * 6.059826374053955
Epoch 520, val loss: 0.7970209717750549
Epoch 530, training loss: 12.587706565856934 = 0.472790390253067 + 2.0 * 6.05745792388916
Epoch 530, val loss: 0.7867404818534851
Epoch 540, training loss: 12.581007957458496 = 0.4524107277393341 + 2.0 * 6.064298629760742
Epoch 540, val loss: 0.777747392654419
Epoch 550, training loss: 12.543063163757324 = 0.43295490741729736 + 2.0 * 6.055054187774658
Epoch 550, val loss: 0.7700877785682678
Epoch 560, training loss: 12.520940780639648 = 0.41437000036239624 + 2.0 * 6.053285598754883
Epoch 560, val loss: 0.7639017701148987
Epoch 570, training loss: 12.500102996826172 = 0.3962949812412262 + 2.0 * 6.051904201507568
Epoch 570, val loss: 0.7585938572883606
Epoch 580, training loss: 12.497060775756836 = 0.3786480724811554 + 2.0 * 6.059206485748291
Epoch 580, val loss: 0.7540888786315918
Epoch 590, training loss: 12.465892791748047 = 0.36159569025039673 + 2.0 * 6.052148342132568
Epoch 590, val loss: 0.7505432963371277
Epoch 600, training loss: 12.444138526916504 = 0.3450368344783783 + 2.0 * 6.049551010131836
Epoch 600, val loss: 0.7480180859565735
Epoch 610, training loss: 12.425850868225098 = 0.32884103059768677 + 2.0 * 6.048504829406738
Epoch 610, val loss: 0.7460176944732666
Epoch 620, training loss: 12.411338806152344 = 0.31303203105926514 + 2.0 * 6.0491533279418945
Epoch 620, val loss: 0.7445554733276367
Epoch 630, training loss: 12.388236999511719 = 0.29775524139404297 + 2.0 * 6.045240879058838
Epoch 630, val loss: 0.7439417243003845
Epoch 640, training loss: 12.373671531677246 = 0.2829309403896332 + 2.0 * 6.045370101928711
Epoch 640, val loss: 0.7440186142921448
Epoch 650, training loss: 12.361044883728027 = 0.26861685514450073 + 2.0 * 6.0462141036987305
Epoch 650, val loss: 0.7443910241127014
Epoch 660, training loss: 12.340964317321777 = 0.2548198401927948 + 2.0 * 6.04307222366333
Epoch 660, val loss: 0.7455496788024902
Epoch 670, training loss: 12.330095291137695 = 0.2416258603334427 + 2.0 * 6.044234752655029
Epoch 670, val loss: 0.7472013235092163
Epoch 680, training loss: 12.317838668823242 = 0.22910182178020477 + 2.0 * 6.044368267059326
Epoch 680, val loss: 0.7489430904388428
Epoch 690, training loss: 12.297286033630371 = 0.21726258099079132 + 2.0 * 6.040011882781982
Epoch 690, val loss: 0.7516636848449707
Epoch 700, training loss: 12.283125877380371 = 0.20603610575199127 + 2.0 * 6.038544654846191
Epoch 700, val loss: 0.7546618580818176
Epoch 710, training loss: 12.27892780303955 = 0.19537939131259918 + 2.0 * 6.041774272918701
Epoch 710, val loss: 0.757767915725708
Epoch 720, training loss: 12.259102821350098 = 0.1854225993156433 + 2.0 * 6.036839962005615
Epoch 720, val loss: 0.7614045739173889
Epoch 730, training loss: 12.24841594696045 = 0.17603743076324463 + 2.0 * 6.036189079284668
Epoch 730, val loss: 0.7654514312744141
Epoch 740, training loss: 12.238367080688477 = 0.16720794141292572 + 2.0 * 6.035579681396484
Epoch 740, val loss: 0.7695150971412659
Epoch 750, training loss: 12.227907180786133 = 0.15895123779773712 + 2.0 * 6.034478187561035
Epoch 750, val loss: 0.774125874042511
Epoch 760, training loss: 12.220207214355469 = 0.15119034051895142 + 2.0 * 6.034508228302002
Epoch 760, val loss: 0.7788798809051514
Epoch 770, training loss: 12.209943771362305 = 0.14394155144691467 + 2.0 * 6.033000946044922
Epoch 770, val loss: 0.7836593985557556
Epoch 780, training loss: 12.198339462280273 = 0.13717538118362427 + 2.0 * 6.030581951141357
Epoch 780, val loss: 0.7889490127563477
Epoch 790, training loss: 12.190604209899902 = 0.13081757724285126 + 2.0 * 6.029893398284912
Epoch 790, val loss: 0.7943561673164368
Epoch 800, training loss: 12.200958251953125 = 0.12483192980289459 + 2.0 * 6.038063049316406
Epoch 800, val loss: 0.7996938228607178
Epoch 810, training loss: 12.177136421203613 = 0.1192043274641037 + 2.0 * 6.028965950012207
Epoch 810, val loss: 0.8052141666412354
Epoch 820, training loss: 12.16881275177002 = 0.11395241320133209 + 2.0 * 6.027430057525635
Epoch 820, val loss: 0.8111895322799683
Epoch 830, training loss: 12.173701286315918 = 0.10899516195058823 + 2.0 * 6.032352924346924
Epoch 830, val loss: 0.8168609142303467
Epoch 840, training loss: 12.157934188842773 = 0.10432372987270355 + 2.0 * 6.026805400848389
Epoch 840, val loss: 0.8226927518844604
Epoch 850, training loss: 12.148809432983398 = 0.0999395027756691 + 2.0 * 6.024435043334961
Epoch 850, val loss: 0.828840434551239
Epoch 860, training loss: 12.142024993896484 = 0.09577950835227966 + 2.0 * 6.023122787475586
Epoch 860, val loss: 0.8348312973976135
Epoch 870, training loss: 12.158136367797852 = 0.09184803068637848 + 2.0 * 6.033143997192383
Epoch 870, val loss: 0.8408095240592957
Epoch 880, training loss: 12.136777877807617 = 0.08814020454883575 + 2.0 * 6.024318695068359
Epoch 880, val loss: 0.8466106057167053
Epoch 890, training loss: 12.130953788757324 = 0.08468315005302429 + 2.0 * 6.023135185241699
Epoch 890, val loss: 0.8528943061828613
Epoch 900, training loss: 12.126504898071289 = 0.0814073458313942 + 2.0 * 6.022548675537109
Epoch 900, val loss: 0.8589005470275879
Epoch 910, training loss: 12.123021125793457 = 0.07830213010311127 + 2.0 * 6.022359371185303
Epoch 910, val loss: 0.8648321032524109
Epoch 920, training loss: 12.118635177612305 = 0.07536107301712036 + 2.0 * 6.021636962890625
Epoch 920, val loss: 0.8709142208099365
Epoch 930, training loss: 12.10966968536377 = 0.07258124649524689 + 2.0 * 6.0185441970825195
Epoch 930, val loss: 0.8768763542175293
Epoch 940, training loss: 12.10515022277832 = 0.06993310898542404 + 2.0 * 6.017608642578125
Epoch 940, val loss: 0.8828481435775757
Epoch 950, training loss: 12.111115455627441 = 0.06741057336330414 + 2.0 * 6.021852493286133
Epoch 950, val loss: 0.8886770009994507
Epoch 960, training loss: 12.118532180786133 = 0.06501886248588562 + 2.0 * 6.026756763458252
Epoch 960, val loss: 0.894359827041626
Epoch 970, training loss: 12.100197792053223 = 0.06278891116380692 + 2.0 * 6.018704414367676
Epoch 970, val loss: 0.900143027305603
Epoch 980, training loss: 12.091979026794434 = 0.06066441163420677 + 2.0 * 6.015657424926758
Epoch 980, val loss: 0.9060501456260681
Epoch 990, training loss: 12.088132858276367 = 0.05863180756568909 + 2.0 * 6.0147504806518555
Epoch 990, val loss: 0.9116047024726868
Epoch 1000, training loss: 12.09616756439209 = 0.05668189004063606 + 2.0 * 6.019742965698242
Epoch 1000, val loss: 0.9171186685562134
Epoch 1010, training loss: 12.092986106872559 = 0.05483655259013176 + 2.0 * 6.0190749168396
Epoch 1010, val loss: 0.9225534796714783
Epoch 1020, training loss: 12.081254959106445 = 0.05308625474572182 + 2.0 * 6.014084339141846
Epoch 1020, val loss: 0.928295373916626
Epoch 1030, training loss: 12.075828552246094 = 0.051416728645563126 + 2.0 * 6.012206077575684
Epoch 1030, val loss: 0.9337702989578247
Epoch 1040, training loss: 12.073368072509766 = 0.04980502650141716 + 2.0 * 6.011781692504883
Epoch 1040, val loss: 0.9390744566917419
Epoch 1050, training loss: 12.088288307189941 = 0.04825565591454506 + 2.0 * 6.020016193389893
Epoch 1050, val loss: 0.944415271282196
Epoch 1060, training loss: 12.081612586975098 = 0.04679030552506447 + 2.0 * 6.017411231994629
Epoch 1060, val loss: 0.9493362903594971
Epoch 1070, training loss: 12.071514129638672 = 0.04540921002626419 + 2.0 * 6.013052463531494
Epoch 1070, val loss: 0.9549907445907593
Epoch 1080, training loss: 12.066567420959473 = 0.04408547282218933 + 2.0 * 6.0112409591674805
Epoch 1080, val loss: 0.9602766036987305
Epoch 1090, training loss: 12.07457160949707 = 0.04280460253357887 + 2.0 * 6.015883445739746
Epoch 1090, val loss: 0.965088427066803
Epoch 1100, training loss: 12.063301086425781 = 0.041576433926820755 + 2.0 * 6.010862350463867
Epoch 1100, val loss: 0.9702228903770447
Epoch 1110, training loss: 12.057966232299805 = 0.04040568694472313 + 2.0 * 6.008780479431152
Epoch 1110, val loss: 0.9754873514175415
Epoch 1120, training loss: 12.054885864257812 = 0.03927119821310043 + 2.0 * 6.00780725479126
Epoch 1120, val loss: 0.9804916977882385
Epoch 1130, training loss: 12.07398796081543 = 0.038174547255039215 + 2.0 * 6.017906665802002
Epoch 1130, val loss: 0.9854485392570496
Epoch 1140, training loss: 12.061637878417969 = 0.03714052960276604 + 2.0 * 6.012248516082764
Epoch 1140, val loss: 0.9902248978614807
Epoch 1150, training loss: 12.049668312072754 = 0.036150746047496796 + 2.0 * 6.006758689880371
Epoch 1150, val loss: 0.9954788684844971
Epoch 1160, training loss: 12.048408508300781 = 0.03519632667303085 + 2.0 * 6.006606101989746
Epoch 1160, val loss: 1.0004351139068604
Epoch 1170, training loss: 12.04736042022705 = 0.03427175059914589 + 2.0 * 6.00654411315918
Epoch 1170, val loss: 1.0052080154418945
Epoch 1180, training loss: 12.054643630981445 = 0.033378977328538895 + 2.0 * 6.010632514953613
Epoch 1180, val loss: 1.0098224878311157
Epoch 1190, training loss: 12.045175552368164 = 0.03253041207790375 + 2.0 * 6.006322383880615
Epoch 1190, val loss: 1.014725923538208
Epoch 1200, training loss: 12.043045997619629 = 0.03171830624341965 + 2.0 * 6.005663871765137
Epoch 1200, val loss: 1.0196325778961182
Epoch 1210, training loss: 12.041082382202148 = 0.030931249260902405 + 2.0 * 6.005075454711914
Epoch 1210, val loss: 1.024314045906067
Epoch 1220, training loss: 12.043777465820312 = 0.030169455334544182 + 2.0 * 6.0068039894104
Epoch 1220, val loss: 1.0289180278778076
Epoch 1230, training loss: 12.043707847595215 = 0.02943391725420952 + 2.0 * 6.007136821746826
Epoch 1230, val loss: 1.033634066581726
Epoch 1240, training loss: 12.035483360290527 = 0.028727922588586807 + 2.0 * 6.003377914428711
Epoch 1240, val loss: 1.0383244752883911
Epoch 1250, training loss: 12.033574104309082 = 0.028045402839779854 + 2.0 * 6.0027642250061035
Epoch 1250, val loss: 1.0429656505584717
Epoch 1260, training loss: 12.0388765335083 = 0.02738163061439991 + 2.0 * 6.005747318267822
Epoch 1260, val loss: 1.0474553108215332
Epoch 1270, training loss: 12.037790298461914 = 0.026745347306132317 + 2.0 * 6.00552225112915
Epoch 1270, val loss: 1.0518261194229126
Epoch 1280, training loss: 12.031892776489258 = 0.02613743394613266 + 2.0 * 6.002877712249756
Epoch 1280, val loss: 1.0565179586410522
Epoch 1290, training loss: 12.028743743896484 = 0.02555323950946331 + 2.0 * 6.0015950202941895
Epoch 1290, val loss: 1.061187982559204
Epoch 1300, training loss: 12.030096054077148 = 0.024980131536722183 + 2.0 * 6.002557754516602
Epoch 1300, val loss: 1.0655184984207153
Epoch 1310, training loss: 12.029056549072266 = 0.024425603449344635 + 2.0 * 6.002315521240234
Epoch 1310, val loss: 1.0696231126785278
Epoch 1320, training loss: 12.025616645812988 = 0.023894229903817177 + 2.0 * 6.000861167907715
Epoch 1320, val loss: 1.074187159538269
Epoch 1330, training loss: 12.02490520477295 = 0.023382702842354774 + 2.0 * 6.000761032104492
Epoch 1330, val loss: 1.078665852546692
Epoch 1340, training loss: 12.03267765045166 = 0.022882699966430664 + 2.0 * 6.004897594451904
Epoch 1340, val loss: 1.0827492475509644
Epoch 1350, training loss: 12.025972366333008 = 0.022399846464395523 + 2.0 * 6.001786231994629
Epoch 1350, val loss: 1.0870153903961182
Epoch 1360, training loss: 12.020956039428711 = 0.021933238953351974 + 2.0 * 5.999511241912842
Epoch 1360, val loss: 1.0913348197937012
Epoch 1370, training loss: 12.018156051635742 = 0.021480994299054146 + 2.0 * 5.998337745666504
Epoch 1370, val loss: 1.095649242401123
Epoch 1380, training loss: 12.022144317626953 = 0.02103951945900917 + 2.0 * 6.000552177429199
Epoch 1380, val loss: 1.0997569561004639
Epoch 1390, training loss: 12.019152641296387 = 0.02060891129076481 + 2.0 * 5.999271869659424
Epoch 1390, val loss: 1.10366952419281
Epoch 1400, training loss: 12.016253471374512 = 0.020198343321681023 + 2.0 * 5.998027801513672
Epoch 1400, val loss: 1.107950210571289
Epoch 1410, training loss: 12.017634391784668 = 0.019801035523414612 + 2.0 * 5.9989166259765625
Epoch 1410, val loss: 1.112186074256897
Epoch 1420, training loss: 12.022408485412598 = 0.019412651658058167 + 2.0 * 6.001497745513916
Epoch 1420, val loss: 1.11605966091156
Epoch 1430, training loss: 12.013365745544434 = 0.019038738682866096 + 2.0 * 5.99716329574585
Epoch 1430, val loss: 1.1199613809585571
Epoch 1440, training loss: 12.011449813842773 = 0.018676143139600754 + 2.0 * 5.996387004852295
Epoch 1440, val loss: 1.1241366863250732
Epoch 1450, training loss: 12.009786605834961 = 0.018319739028811455 + 2.0 * 5.995733261108398
Epoch 1450, val loss: 1.128103256225586
Epoch 1460, training loss: 12.016565322875977 = 0.01797052100300789 + 2.0 * 5.999297618865967
Epoch 1460, val loss: 1.1319630146026611
Epoch 1470, training loss: 12.01400375366211 = 0.017633607611060143 + 2.0 * 5.998185157775879
Epoch 1470, val loss: 1.135589599609375
Epoch 1480, training loss: 12.01744270324707 = 0.017308814451098442 + 2.0 * 6.000066757202148
Epoch 1480, val loss: 1.1395879983901978
Epoch 1490, training loss: 12.007506370544434 = 0.016997365280985832 + 2.0 * 5.9952545166015625
Epoch 1490, val loss: 1.1435656547546387
Epoch 1500, training loss: 12.006627082824707 = 0.016691040247678757 + 2.0 * 5.994967937469482
Epoch 1500, val loss: 1.1474400758743286
Epoch 1510, training loss: 12.006940841674805 = 0.016390485689044 + 2.0 * 5.995275020599365
Epoch 1510, val loss: 1.1511363983154297
Epoch 1520, training loss: 12.014226913452148 = 0.01609601266682148 + 2.0 * 5.999065399169922
Epoch 1520, val loss: 1.1546218395233154
Epoch 1530, training loss: 12.005382537841797 = 0.01581598073244095 + 2.0 * 5.994783401489258
Epoch 1530, val loss: 1.1584653854370117
Epoch 1540, training loss: 12.003713607788086 = 0.0155425313860178 + 2.0 * 5.994085311889648
Epoch 1540, val loss: 1.1623730659484863
Epoch 1550, training loss: 12.011321067810059 = 0.015273042023181915 + 2.0 * 5.998023986816406
Epoch 1550, val loss: 1.1658337116241455
Epoch 1560, training loss: 12.004149436950684 = 0.015012597665190697 + 2.0 * 5.994568347930908
Epoch 1560, val loss: 1.1695176362991333
Epoch 1570, training loss: 12.006305694580078 = 0.014758383855223656 + 2.0 * 5.995773792266846
Epoch 1570, val loss: 1.1732345819473267
Epoch 1580, training loss: 12.000099182128906 = 0.014509515836834908 + 2.0 * 5.992794990539551
Epoch 1580, val loss: 1.1766812801361084
Epoch 1590, training loss: 11.999844551086426 = 0.014268040657043457 + 2.0 * 5.992788314819336
Epoch 1590, val loss: 1.1802946329116821
Epoch 1600, training loss: 11.99733829498291 = 0.014030855149030685 + 2.0 * 5.991653919219971
Epoch 1600, val loss: 1.183894395828247
Epoch 1610, training loss: 12.008312225341797 = 0.013798377476632595 + 2.0 * 5.997256755828857
Epoch 1610, val loss: 1.1873278617858887
Epoch 1620, training loss: 12.003193855285645 = 0.01357015036046505 + 2.0 * 5.99481201171875
Epoch 1620, val loss: 1.1905333995819092
Epoch 1630, training loss: 11.998788833618164 = 0.013354453258216381 + 2.0 * 5.992717266082764
Epoch 1630, val loss: 1.1942694187164307
Epoch 1640, training loss: 11.99826431274414 = 0.013143107295036316 + 2.0 * 5.992560386657715
Epoch 1640, val loss: 1.1978670358657837
Epoch 1650, training loss: 12.002885818481445 = 0.012933645397424698 + 2.0 * 5.994976043701172
Epoch 1650, val loss: 1.201012372970581
Epoch 1660, training loss: 12.000652313232422 = 0.012729155831038952 + 2.0 * 5.993961811065674
Epoch 1660, val loss: 1.2043792009353638
Epoch 1670, training loss: 11.996145248413086 = 0.012533258646726608 + 2.0 * 5.9918060302734375
Epoch 1670, val loss: 1.2078793048858643
Epoch 1680, training loss: 11.994226455688477 = 0.012339174747467041 + 2.0 * 5.990943431854248
Epoch 1680, val loss: 1.2113157510757446
Epoch 1690, training loss: 11.995307922363281 = 0.012148790061473846 + 2.0 * 5.991579532623291
Epoch 1690, val loss: 1.2145472764968872
Epoch 1700, training loss: 11.997136116027832 = 0.011963142082095146 + 2.0 * 5.992586612701416
Epoch 1700, val loss: 1.2178006172180176
Epoch 1710, training loss: 11.995261192321777 = 0.011782277375459671 + 2.0 * 5.991739273071289
Epoch 1710, val loss: 1.2210233211517334
Epoch 1720, training loss: 11.995031356811523 = 0.011606143787503242 + 2.0 * 5.99171257019043
Epoch 1720, val loss: 1.224442720413208
Epoch 1730, training loss: 11.997600555419922 = 0.0114351911470294 + 2.0 * 5.993082523345947
Epoch 1730, val loss: 1.2275772094726562
Epoch 1740, training loss: 11.990785598754883 = 0.011266881600022316 + 2.0 * 5.98975944519043
Epoch 1740, val loss: 1.2307488918304443
Epoch 1750, training loss: 11.988966941833496 = 0.011103087104856968 + 2.0 * 5.988932132720947
Epoch 1750, val loss: 1.2342194318771362
Epoch 1760, training loss: 11.989439010620117 = 0.010939971543848515 + 2.0 * 5.9892497062683105
Epoch 1760, val loss: 1.2373151779174805
Epoch 1770, training loss: 12.001474380493164 = 0.01077924482524395 + 2.0 * 5.995347499847412
Epoch 1770, val loss: 1.2402222156524658
Epoch 1780, training loss: 11.99716567993164 = 0.010625721886754036 + 2.0 * 5.993269920349121
Epoch 1780, val loss: 1.243237853050232
Epoch 1790, training loss: 11.986184120178223 = 0.010477278381586075 + 2.0 * 5.987853527069092
Epoch 1790, val loss: 1.246574878692627
Epoch 1800, training loss: 11.985596656799316 = 0.010332001373171806 + 2.0 * 5.9876322746276855
Epoch 1800, val loss: 1.249814748764038
Epoch 1810, training loss: 11.986167907714844 = 0.010186177678406239 + 2.0 * 5.987990856170654
Epoch 1810, val loss: 1.2527903318405151
Epoch 1820, training loss: 11.995418548583984 = 0.01004213560372591 + 2.0 * 5.992688179016113
Epoch 1820, val loss: 1.2556874752044678
Epoch 1830, training loss: 11.994938850402832 = 0.009903556667268276 + 2.0 * 5.992517471313477
Epoch 1830, val loss: 1.2586464881896973
Epoch 1840, training loss: 11.987289428710938 = 0.009769225493073463 + 2.0 * 5.988759994506836
Epoch 1840, val loss: 1.2617747783660889
Epoch 1850, training loss: 11.9871187210083 = 0.009638066403567791 + 2.0 * 5.98874044418335
Epoch 1850, val loss: 1.264843463897705
Epoch 1860, training loss: 11.987829208374023 = 0.009508165530860424 + 2.0 * 5.989160537719727
Epoch 1860, val loss: 1.2677431106567383
Epoch 1870, training loss: 11.982967376708984 = 0.009379617869853973 + 2.0 * 5.9867939949035645
Epoch 1870, val loss: 1.2706520557403564
Epoch 1880, training loss: 11.987181663513184 = 0.009254621341824532 + 2.0 * 5.988963603973389
Epoch 1880, val loss: 1.2738147974014282
Epoch 1890, training loss: 11.983251571655273 = 0.009131812490522861 + 2.0 * 5.987060070037842
Epoch 1890, val loss: 1.2763980627059937
Epoch 1900, training loss: 11.980567932128906 = 0.009013587608933449 + 2.0 * 5.985777378082275
Epoch 1900, val loss: 1.2793803215026855
Epoch 1910, training loss: 11.980146408081055 = 0.008897168561816216 + 2.0 * 5.98562479019165
Epoch 1910, val loss: 1.282464861869812
Epoch 1920, training loss: 11.981091499328613 = 0.008781112730503082 + 2.0 * 5.986155033111572
Epoch 1920, val loss: 1.2852208614349365
Epoch 1930, training loss: 11.992429733276367 = 0.008666140958666801 + 2.0 * 5.991881847381592
Epoch 1930, val loss: 1.2878836393356323
Epoch 1940, training loss: 11.987744331359863 = 0.008555258624255657 + 2.0 * 5.989594459533691
Epoch 1940, val loss: 1.290757417678833
Epoch 1950, training loss: 11.984246253967285 = 0.008447528816759586 + 2.0 * 5.987899303436279
Epoch 1950, val loss: 1.2935174703598022
Epoch 1960, training loss: 11.986212730407715 = 0.008341917768120766 + 2.0 * 5.988935470581055
Epoch 1960, val loss: 1.296233057975769
Epoch 1970, training loss: 11.979425430297852 = 0.008240762166678905 + 2.0 * 5.985592365264893
Epoch 1970, val loss: 1.2992112636566162
Epoch 1980, training loss: 11.976821899414062 = 0.008139615878462791 + 2.0 * 5.984341144561768
Epoch 1980, val loss: 1.3021410703659058
Epoch 1990, training loss: 11.976458549499512 = 0.008038598112761974 + 2.0 * 5.984210014343262
Epoch 1990, val loss: 1.3048620223999023
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8000
Overall ASR: 0.7934
Flip ASR: 0.7556/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.69768524169922 = 1.9501876831054688 + 2.0 * 8.373748779296875
Epoch 0, val loss: 1.946015477180481
Epoch 10, training loss: 18.683137893676758 = 1.9388519525527954 + 2.0 * 8.372142791748047
Epoch 10, val loss: 1.9345004558563232
Epoch 20, training loss: 18.648691177368164 = 1.9248650074005127 + 2.0 * 8.361912727355957
Epoch 20, val loss: 1.9196408987045288
Epoch 30, training loss: 18.491024017333984 = 1.9074586629867554 + 2.0 * 8.29178237915039
Epoch 30, val loss: 1.9010390043258667
Epoch 40, training loss: 17.612218856811523 = 1.8891514539718628 + 2.0 * 7.861534118652344
Epoch 40, val loss: 1.8816629648208618
Epoch 50, training loss: 16.48456573486328 = 1.8705564737319946 + 2.0 * 7.307004451751709
Epoch 50, val loss: 1.8624060153961182
Epoch 60, training loss: 15.630936622619629 = 1.8564884662628174 + 2.0 * 6.887224197387695
Epoch 60, val loss: 1.8480812311172485
Epoch 70, training loss: 15.163349151611328 = 1.8444128036499023 + 2.0 * 6.659468173980713
Epoch 70, val loss: 1.834891676902771
Epoch 80, training loss: 14.872209548950195 = 1.834926724433899 + 2.0 * 6.518641471862793
Epoch 80, val loss: 1.8241411447525024
Epoch 90, training loss: 14.68281078338623 = 1.825222373008728 + 2.0 * 6.4287943840026855
Epoch 90, val loss: 1.813609004020691
Epoch 100, training loss: 14.552116394042969 = 1.8155080080032349 + 2.0 * 6.368304252624512
Epoch 100, val loss: 1.8032214641571045
Epoch 110, training loss: 14.448562622070312 = 1.8063479661941528 + 2.0 * 6.321107387542725
Epoch 110, val loss: 1.793487548828125
Epoch 120, training loss: 14.364799499511719 = 1.7978078126907349 + 2.0 * 6.283495903015137
Epoch 120, val loss: 1.784542202949524
Epoch 130, training loss: 14.289922714233398 = 1.7899179458618164 + 2.0 * 6.250002384185791
Epoch 130, val loss: 1.7761694192886353
Epoch 140, training loss: 14.230259895324707 = 1.7822449207305908 + 2.0 * 6.224007606506348
Epoch 140, val loss: 1.7682069540023804
Epoch 150, training loss: 14.183537483215332 = 1.7743512392044067 + 2.0 * 6.204593181610107
Epoch 150, val loss: 1.7602137327194214
Epoch 160, training loss: 14.141807556152344 = 1.7660417556762695 + 2.0 * 6.187882900238037
Epoch 160, val loss: 1.752152681350708
Epoch 170, training loss: 14.103816986083984 = 1.7572575807571411 + 2.0 * 6.173279762268066
Epoch 170, val loss: 1.7440040111541748
Epoch 180, training loss: 14.072210311889648 = 1.74772047996521 + 2.0 * 6.16224479675293
Epoch 180, val loss: 1.7355754375457764
Epoch 190, training loss: 14.035650253295898 = 1.7371011972427368 + 2.0 * 6.1492743492126465
Epoch 190, val loss: 1.726648211479187
Epoch 200, training loss: 14.003872871398926 = 1.7252099514007568 + 2.0 * 6.139331340789795
Epoch 200, val loss: 1.7169088125228882
Epoch 210, training loss: 13.970539093017578 = 1.7116570472717285 + 2.0 * 6.129440784454346
Epoch 210, val loss: 1.7059576511383057
Epoch 220, training loss: 13.940153121948242 = 1.6959291696548462 + 2.0 * 6.122111797332764
Epoch 220, val loss: 1.6934326887130737
Epoch 230, training loss: 13.909564971923828 = 1.677487850189209 + 2.0 * 6.1160383224487305
Epoch 230, val loss: 1.67903733253479
Epoch 240, training loss: 13.872984886169434 = 1.6561096906661987 + 2.0 * 6.108437538146973
Epoch 240, val loss: 1.6622639894485474
Epoch 250, training loss: 13.837364196777344 = 1.630794882774353 + 2.0 * 6.10328483581543
Epoch 250, val loss: 1.6423851251602173
Epoch 260, training loss: 13.801565170288086 = 1.6009265184402466 + 2.0 * 6.1003193855285645
Epoch 260, val loss: 1.6190615892410278
Epoch 270, training loss: 13.760004997253418 = 1.5671871900558472 + 2.0 * 6.096408843994141
Epoch 270, val loss: 1.5921974182128906
Epoch 280, training loss: 13.710803985595703 = 1.5287690162658691 + 2.0 * 6.091017723083496
Epoch 280, val loss: 1.5619556903839111
Epoch 290, training loss: 13.661661148071289 = 1.4859437942504883 + 2.0 * 6.0878586769104
Epoch 290, val loss: 1.5281357765197754
Epoch 300, training loss: 13.61064338684082 = 1.4390883445739746 + 2.0 * 6.085777759552002
Epoch 300, val loss: 1.4913495779037476
Epoch 310, training loss: 13.559494972229004 = 1.3901960849761963 + 2.0 * 6.084649562835693
Epoch 310, val loss: 1.452951192855835
Epoch 320, training loss: 13.502335548400879 = 1.3404858112335205 + 2.0 * 6.080924987792969
Epoch 320, val loss: 1.4148659706115723
Epoch 330, training loss: 13.447795867919922 = 1.2912968397140503 + 2.0 * 6.078249454498291
Epoch 330, val loss: 1.3775445222854614
Epoch 340, training loss: 13.398886680603027 = 1.2435017824172974 + 2.0 * 6.07769250869751
Epoch 340, val loss: 1.3417445421218872
Epoch 350, training loss: 13.345325469970703 = 1.1976919174194336 + 2.0 * 6.073816776275635
Epoch 350, val loss: 1.308140516281128
Epoch 360, training loss: 13.299482345581055 = 1.1544384956359863 + 2.0 * 6.072521686553955
Epoch 360, val loss: 1.276518702507019
Epoch 370, training loss: 13.253774642944336 = 1.1136634349822998 + 2.0 * 6.0700554847717285
Epoch 370, val loss: 1.2473797798156738
Epoch 380, training loss: 13.213729858398438 = 1.0757131576538086 + 2.0 * 6.0690083503723145
Epoch 380, val loss: 1.2202242612838745
Epoch 390, training loss: 13.169014930725098 = 1.0400334596633911 + 2.0 * 6.064490795135498
Epoch 390, val loss: 1.1951115131378174
Epoch 400, training loss: 13.129199981689453 = 1.0064260959625244 + 2.0 * 6.061387062072754
Epoch 400, val loss: 1.171417474746704
Epoch 410, training loss: 13.100532531738281 = 0.9744282960891724 + 2.0 * 6.063052177429199
Epoch 410, val loss: 1.1489946842193604
Epoch 420, training loss: 13.06794548034668 = 0.9441413283348083 + 2.0 * 6.061902046203613
Epoch 420, val loss: 1.1279656887054443
Epoch 430, training loss: 13.028092384338379 = 0.9154525995254517 + 2.0 * 6.056319713592529
Epoch 430, val loss: 1.1082775592803955
Epoch 440, training loss: 12.995352745056152 = 0.8878363966941833 + 2.0 * 6.053758144378662
Epoch 440, val loss: 1.0894523859024048
Epoch 450, training loss: 12.966087341308594 = 0.8606711626052856 + 2.0 * 6.052708148956299
Epoch 450, val loss: 1.0710983276367188
Epoch 460, training loss: 12.934366226196289 = 0.8338243961334229 + 2.0 * 6.050271034240723
Epoch 460, val loss: 1.0532158613204956
Epoch 470, training loss: 12.909066200256348 = 0.8072883486747742 + 2.0 * 6.050889015197754
Epoch 470, val loss: 1.0356320142745972
Epoch 480, training loss: 12.877355575561523 = 0.7807483077049255 + 2.0 * 6.048303604125977
Epoch 480, val loss: 1.0182807445526123
Epoch 490, training loss: 12.847640037536621 = 0.7542644739151001 + 2.0 * 6.046687602996826
Epoch 490, val loss: 1.0011770725250244
Epoch 500, training loss: 12.82505989074707 = 0.7278184294700623 + 2.0 * 6.048620700836182
Epoch 500, val loss: 0.984207808971405
Epoch 510, training loss: 12.794065475463867 = 0.7016319036483765 + 2.0 * 6.04621696472168
Epoch 510, val loss: 0.9674907326698303
Epoch 520, training loss: 12.761706352233887 = 0.6757040023803711 + 2.0 * 6.043001174926758
Epoch 520, val loss: 0.9512193202972412
Epoch 530, training loss: 12.732504844665527 = 0.6502282619476318 + 2.0 * 6.041138172149658
Epoch 530, val loss: 0.9356215000152588
Epoch 540, training loss: 12.72470474243164 = 0.625266432762146 + 2.0 * 6.049719333648682
Epoch 540, val loss: 0.9206382632255554
Epoch 550, training loss: 12.681153297424316 = 0.6011063456535339 + 2.0 * 6.040023326873779
Epoch 550, val loss: 0.9065766334533691
Epoch 560, training loss: 12.655362129211426 = 0.577882707118988 + 2.0 * 6.0387396812438965
Epoch 560, val loss: 0.8937543630599976
Epoch 570, training loss: 12.633464813232422 = 0.5555585026741028 + 2.0 * 6.0389533042907715
Epoch 570, val loss: 0.8818702697753906
Epoch 580, training loss: 12.606616020202637 = 0.5342051386833191 + 2.0 * 6.036205291748047
Epoch 580, val loss: 0.8711960911750793
Epoch 590, training loss: 12.582048416137695 = 0.5138051509857178 + 2.0 * 6.034121513366699
Epoch 590, val loss: 0.8616891503334045
Epoch 600, training loss: 12.561363220214844 = 0.4942893981933594 + 2.0 * 6.033536911010742
Epoch 600, val loss: 0.8531328439712524
Epoch 610, training loss: 12.55132007598877 = 0.47570163011550903 + 2.0 * 6.037809371948242
Epoch 610, val loss: 0.8455488681793213
Epoch 620, training loss: 12.52241325378418 = 0.45804107189178467 + 2.0 * 6.032186031341553
Epoch 620, val loss: 0.839202344417572
Epoch 630, training loss: 12.501806259155273 = 0.4413309097290039 + 2.0 * 6.030237674713135
Epoch 630, val loss: 0.8339031934738159
Epoch 640, training loss: 12.48420238494873 = 0.42527690529823303 + 2.0 * 6.029462814331055
Epoch 640, val loss: 0.8293344378471375
Epoch 650, training loss: 12.473337173461914 = 0.4098786413669586 + 2.0 * 6.031729221343994
Epoch 650, val loss: 0.825377881526947
Epoch 660, training loss: 12.454620361328125 = 0.3952856957912445 + 2.0 * 6.029667377471924
Epoch 660, val loss: 0.8222442269325256
Epoch 670, training loss: 12.433649063110352 = 0.38129231333732605 + 2.0 * 6.026178359985352
Epoch 670, val loss: 0.8199367523193359
Epoch 680, training loss: 12.42347240447998 = 0.36778131127357483 + 2.0 * 6.02784538269043
Epoch 680, val loss: 0.8180292248725891
Epoch 690, training loss: 12.41409969329834 = 0.3547298312187195 + 2.0 * 6.029685020446777
Epoch 690, val loss: 0.8163748383522034
Epoch 700, training loss: 12.388836860656738 = 0.3422060012817383 + 2.0 * 6.0233154296875
Epoch 700, val loss: 0.8154191970825195
Epoch 710, training loss: 12.375929832458496 = 0.3300873339176178 + 2.0 * 6.022921085357666
Epoch 710, val loss: 0.8148534297943115
Epoch 720, training loss: 12.36495590209961 = 0.3182363510131836 + 2.0 * 6.023359775543213
Epoch 720, val loss: 0.8144029974937439
Epoch 730, training loss: 12.349563598632812 = 0.30675557255744934 + 2.0 * 6.021403789520264
Epoch 730, val loss: 0.8140323162078857
Epoch 740, training loss: 12.337578773498535 = 0.29559165239334106 + 2.0 * 6.020993709564209
Epoch 740, val loss: 0.8141292333602905
Epoch 750, training loss: 12.340766906738281 = 0.2846689522266388 + 2.0 * 6.028048992156982
Epoch 750, val loss: 0.8142834901809692
Epoch 760, training loss: 12.319172859191895 = 0.27414262294769287 + 2.0 * 6.022515296936035
Epoch 760, val loss: 0.8143936395645142
Epoch 770, training loss: 12.30008602142334 = 0.2638269364833832 + 2.0 * 6.018129348754883
Epoch 770, val loss: 0.8149016499519348
Epoch 780, training loss: 12.287906646728516 = 0.2537635266780853 + 2.0 * 6.017071723937988
Epoch 780, val loss: 0.8154045343399048
Epoch 790, training loss: 12.293933868408203 = 0.24392811954021454 + 2.0 * 6.025002956390381
Epoch 790, val loss: 0.815819501876831
Epoch 800, training loss: 12.266654014587402 = 0.23434825241565704 + 2.0 * 6.016152858734131
Epoch 800, val loss: 0.8163555860519409
Epoch 810, training loss: 12.256596565246582 = 0.2250792235136032 + 2.0 * 6.015758514404297
Epoch 810, val loss: 0.8171795010566711
Epoch 820, training loss: 12.245292663574219 = 0.21602599322795868 + 2.0 * 6.0146331787109375
Epoch 820, val loss: 0.8179075717926025
Epoch 830, training loss: 12.243551254272461 = 0.20724180340766907 + 2.0 * 6.018154621124268
Epoch 830, val loss: 0.8186522722244263
Epoch 840, training loss: 12.229424476623535 = 0.19876796007156372 + 2.0 * 6.015328407287598
Epoch 840, val loss: 0.8194507956504822
Epoch 850, training loss: 12.21839714050293 = 0.1906754970550537 + 2.0 * 6.013860702514648
Epoch 850, val loss: 0.8206131458282471
Epoch 860, training loss: 12.205866813659668 = 0.18286576867103577 + 2.0 * 6.011500358581543
Epoch 860, val loss: 0.8218115568161011
Epoch 870, training loss: 12.20030403137207 = 0.1753348857164383 + 2.0 * 6.012484550476074
Epoch 870, val loss: 0.8230171203613281
Epoch 880, training loss: 12.202845573425293 = 0.1681399643421173 + 2.0 * 6.01735258102417
Epoch 880, val loss: 0.824206531047821
Epoch 890, training loss: 12.191802024841309 = 0.16127640008926392 + 2.0 * 6.015262603759766
Epoch 890, val loss: 0.8252972364425659
Epoch 900, training loss: 12.176504135131836 = 0.1548406481742859 + 2.0 * 6.010831832885742
Epoch 900, val loss: 0.8269274830818176
Epoch 910, training loss: 12.166742324829102 = 0.14867031574249268 + 2.0 * 6.009036064147949
Epoch 910, val loss: 0.8285133242607117
Epoch 920, training loss: 12.159217834472656 = 0.142735093832016 + 2.0 * 6.008241176605225
Epoch 920, val loss: 0.8300040364265442
Epoch 930, training loss: 12.151632308959961 = 0.1370498687028885 + 2.0 * 6.007291316986084
Epoch 930, val loss: 0.8317586779594421
Epoch 940, training loss: 12.149107933044434 = 0.13161501288414001 + 2.0 * 6.00874662399292
Epoch 940, val loss: 0.8336318731307983
Epoch 950, training loss: 12.139287948608398 = 0.12644486129283905 + 2.0 * 6.0064215660095215
Epoch 950, val loss: 0.8352828025817871
Epoch 960, training loss: 12.138615608215332 = 0.12159095704555511 + 2.0 * 6.008512496948242
Epoch 960, val loss: 0.8373610377311707
Epoch 970, training loss: 12.13000202178955 = 0.11699836701154709 + 2.0 * 6.0065016746521
Epoch 970, val loss: 0.8395677804946899
Epoch 980, training loss: 12.132594108581543 = 0.11266271024942398 + 2.0 * 6.009965896606445
Epoch 980, val loss: 0.8416259288787842
Epoch 990, training loss: 12.118415832519531 = 0.1085515022277832 + 2.0 * 6.004931926727295
Epoch 990, val loss: 0.8438470959663391
Epoch 1000, training loss: 12.115797996520996 = 0.10468517243862152 + 2.0 * 6.005556583404541
Epoch 1000, val loss: 0.8464173674583435
Epoch 1010, training loss: 12.10807991027832 = 0.10099518299102783 + 2.0 * 6.003542423248291
Epoch 1010, val loss: 0.8488097190856934
Epoch 1020, training loss: 12.106687545776367 = 0.09747747331857681 + 2.0 * 6.004604816436768
Epoch 1020, val loss: 0.8513041734695435
Epoch 1030, training loss: 12.099286079406738 = 0.0941242203116417 + 2.0 * 6.0025811195373535
Epoch 1030, val loss: 0.8536900281906128
Epoch 1040, training loss: 12.100802421569824 = 0.09096081554889679 + 2.0 * 6.004920959472656
Epoch 1040, val loss: 0.856316089630127
Epoch 1050, training loss: 12.105066299438477 = 0.08795014768838882 + 2.0 * 6.00855827331543
Epoch 1050, val loss: 0.8589208722114563
Epoch 1060, training loss: 12.091784477233887 = 0.08505871891975403 + 2.0 * 6.003362655639648
Epoch 1060, val loss: 0.8615474104881287
Epoch 1070, training loss: 12.084674835205078 = 0.08233241736888885 + 2.0 * 6.001171112060547
Epoch 1070, val loss: 0.8643684983253479
Epoch 1080, training loss: 12.080323219299316 = 0.07969716936349869 + 2.0 * 6.000312805175781
Epoch 1080, val loss: 0.8671619892120361
Epoch 1090, training loss: 12.0903902053833 = 0.07716863602399826 + 2.0 * 6.006610870361328
Epoch 1090, val loss: 0.8699369430541992
Epoch 1100, training loss: 12.08150577545166 = 0.07474008947610855 + 2.0 * 6.003382682800293
Epoch 1100, val loss: 0.8725512027740479
Epoch 1110, training loss: 12.072460174560547 = 0.07244671881198883 + 2.0 * 6.000006675720215
Epoch 1110, val loss: 0.8755809664726257
Epoch 1120, training loss: 12.067107200622559 = 0.07023932784795761 + 2.0 * 5.998434066772461
Epoch 1120, val loss: 0.878483235836029
Epoch 1130, training loss: 12.074172973632812 = 0.06810249388217926 + 2.0 * 6.003035068511963
Epoch 1130, val loss: 0.8812531232833862
Epoch 1140, training loss: 12.064507484436035 = 0.06607376039028168 + 2.0 * 5.9992170333862305
Epoch 1140, val loss: 0.8840529918670654
Epoch 1150, training loss: 12.06187915802002 = 0.06410927325487137 + 2.0 * 5.998885154724121
Epoch 1150, val loss: 0.8870788216590881
Epoch 1160, training loss: 12.057318687438965 = 0.062220897525548935 + 2.0 * 5.997549057006836
Epoch 1160, val loss: 0.8901400566101074
Epoch 1170, training loss: 12.071846961975098 = 0.06037985160946846 + 2.0 * 6.005733489990234
Epoch 1170, val loss: 0.8929312229156494
Epoch 1180, training loss: 12.05587100982666 = 0.05860646441578865 + 2.0 * 5.998632431030273
Epoch 1180, val loss: 0.8958280086517334
Epoch 1190, training loss: 12.049348831176758 = 0.05692540481686592 + 2.0 * 5.996211528778076
Epoch 1190, val loss: 0.8989768028259277
Epoch 1200, training loss: 12.050355911254883 = 0.05529501661658287 + 2.0 * 5.997530460357666
Epoch 1200, val loss: 0.9019269943237305
Epoch 1210, training loss: 12.045523643493652 = 0.05371498316526413 + 2.0 * 5.995904445648193
Epoch 1210, val loss: 0.9048436880111694
Epoch 1220, training loss: 12.042746543884277 = 0.05218645930290222 + 2.0 * 5.9952802658081055
Epoch 1220, val loss: 0.9079682230949402
Epoch 1230, training loss: 12.043011665344238 = 0.05067857354879379 + 2.0 * 5.996166706085205
Epoch 1230, val loss: 0.9110472202301025
Epoch 1240, training loss: 12.047922134399414 = 0.04917922243475914 + 2.0 * 5.999371528625488
Epoch 1240, val loss: 0.9138571619987488
Epoch 1250, training loss: 12.035767555236816 = 0.04768683761358261 + 2.0 * 5.994040489196777
Epoch 1250, val loss: 0.9167912602424622
Epoch 1260, training loss: 12.033709526062012 = 0.04624148830771446 + 2.0 * 5.993733882904053
Epoch 1260, val loss: 0.919914722442627
Epoch 1270, training loss: 12.039291381835938 = 0.04483626410365105 + 2.0 * 5.997227668762207
Epoch 1270, val loss: 0.9227698445320129
Epoch 1280, training loss: 12.030428886413574 = 0.0435139462351799 + 2.0 * 5.993457317352295
Epoch 1280, val loss: 0.9257282018661499
Epoch 1290, training loss: 12.029993057250977 = 0.042257338762283325 + 2.0 * 5.993867874145508
Epoch 1290, val loss: 0.9289605617523193
Epoch 1300, training loss: 12.029345512390137 = 0.04105395823717117 + 2.0 * 5.99414587020874
Epoch 1300, val loss: 0.9321098327636719
Epoch 1310, training loss: 12.026772499084473 = 0.03989073634147644 + 2.0 * 5.993441104888916
Epoch 1310, val loss: 0.9351862668991089
Epoch 1320, training loss: 12.033736228942871 = 0.038768161088228226 + 2.0 * 5.99748420715332
Epoch 1320, val loss: 0.9381788969039917
Epoch 1330, training loss: 12.022353172302246 = 0.03769177198410034 + 2.0 * 5.992330551147461
Epoch 1330, val loss: 0.9413664937019348
Epoch 1340, training loss: 12.018651008605957 = 0.03665713593363762 + 2.0 * 5.990996837615967
Epoch 1340, val loss: 0.9444952011108398
Epoch 1350, training loss: 12.019414901733398 = 0.03565629571676254 + 2.0 * 5.991879463195801
Epoch 1350, val loss: 0.947586715221405
Epoch 1360, training loss: 12.025236129760742 = 0.03469320759177208 + 2.0 * 5.995271682739258
Epoch 1360, val loss: 0.9504766464233398
Epoch 1370, training loss: 12.016765594482422 = 0.03378758579492569 + 2.0 * 5.991488933563232
Epoch 1370, val loss: 0.9536590576171875
Epoch 1380, training loss: 12.01404094696045 = 0.03292255476117134 + 2.0 * 5.990559101104736
Epoch 1380, val loss: 0.9570688009262085
Epoch 1390, training loss: 12.010916709899902 = 0.03208564966917038 + 2.0 * 5.989415645599365
Epoch 1390, val loss: 0.9601956009864807
Epoch 1400, training loss: 12.019766807556152 = 0.03127897530794144 + 2.0 * 5.99424409866333
Epoch 1400, val loss: 0.9633703231811523
Epoch 1410, training loss: 12.010407447814941 = 0.030507031828165054 + 2.0 * 5.989950180053711
Epoch 1410, val loss: 0.9664533734321594
Epoch 1420, training loss: 12.014537811279297 = 0.029769159853458405 + 2.0 * 5.992384433746338
Epoch 1420, val loss: 0.9698280692100525
Epoch 1430, training loss: 12.008721351623535 = 0.02906188927590847 + 2.0 * 5.9898295402526855
Epoch 1430, val loss: 0.973007321357727
Epoch 1440, training loss: 12.014480590820312 = 0.028379103168845177 + 2.0 * 5.993050575256348
Epoch 1440, val loss: 0.9762408137321472
Epoch 1450, training loss: 12.00495433807373 = 0.027728725224733353 + 2.0 * 5.988612651824951
Epoch 1450, val loss: 0.9795012474060059
Epoch 1460, training loss: 12.007838249206543 = 0.02709788642823696 + 2.0 * 5.990370273590088
Epoch 1460, val loss: 0.9828358888626099
Epoch 1470, training loss: 12.007709503173828 = 0.026485182344913483 + 2.0 * 5.990612030029297
Epoch 1470, val loss: 0.9859995245933533
Epoch 1480, training loss: 12.00012493133545 = 0.025895120576024055 + 2.0 * 5.987114906311035
Epoch 1480, val loss: 0.9892569184303284
Epoch 1490, training loss: 12.000019073486328 = 0.025322325527668 + 2.0 * 5.987348556518555
Epoch 1490, val loss: 0.9926105737686157
Epoch 1500, training loss: 12.006458282470703 = 0.02475983090698719 + 2.0 * 5.990849018096924
Epoch 1500, val loss: 0.9959163069725037
Epoch 1510, training loss: 12.0089111328125 = 0.024223994463682175 + 2.0 * 5.992343425750732
Epoch 1510, val loss: 0.9988477826118469
Epoch 1520, training loss: 12.00307559967041 = 0.023713018745183945 + 2.0 * 5.989681243896484
Epoch 1520, val loss: 1.0021015405654907
Epoch 1530, training loss: 11.995107650756836 = 0.02322307601571083 + 2.0 * 5.985942363739014
Epoch 1530, val loss: 1.0054755210876465
Epoch 1540, training loss: 11.993956565856934 = 0.022741621360182762 + 2.0 * 5.985607624053955
Epoch 1540, val loss: 1.0085783004760742
Epoch 1550, training loss: 12.003247261047363 = 0.02227051369845867 + 2.0 * 5.990488529205322
Epoch 1550, val loss: 1.0116419792175293
Epoch 1560, training loss: 11.992654800415039 = 0.021820196881890297 + 2.0 * 5.985417366027832
Epoch 1560, val loss: 1.0147347450256348
Epoch 1570, training loss: 11.991615295410156 = 0.02138398215174675 + 2.0 * 5.9851155281066895
Epoch 1570, val loss: 1.0180178880691528
Epoch 1580, training loss: 11.993754386901855 = 0.020961539819836617 + 2.0 * 5.986396312713623
Epoch 1580, val loss: 1.0212342739105225
Epoch 1590, training loss: 11.995142936706543 = 0.02054843306541443 + 2.0 * 5.987297058105469
Epoch 1590, val loss: 1.024169921875
Epoch 1600, training loss: 11.991713523864746 = 0.02014678344130516 + 2.0 * 5.985783576965332
Epoch 1600, val loss: 1.0273371934890747
Epoch 1610, training loss: 12.000256538391113 = 0.01975907012820244 + 2.0 * 5.990248680114746
Epoch 1610, val loss: 1.03036630153656
Epoch 1620, training loss: 11.988651275634766 = 0.019386546686291695 + 2.0 * 5.98463249206543
Epoch 1620, val loss: 1.0332990884780884
Epoch 1630, training loss: 11.986181259155273 = 0.019021496176719666 + 2.0 * 5.983580112457275
Epoch 1630, val loss: 1.0364658832550049
Epoch 1640, training loss: 11.987630844116211 = 0.018665678799152374 + 2.0 * 5.984482765197754
Epoch 1640, val loss: 1.0394865274429321
Epoch 1650, training loss: 11.99818229675293 = 0.018320666626095772 + 2.0 * 5.989930629730225
Epoch 1650, val loss: 1.0423462390899658
Epoch 1660, training loss: 11.986648559570312 = 0.017983989790081978 + 2.0 * 5.984332084655762
Epoch 1660, val loss: 1.0453672409057617
Epoch 1670, training loss: 11.985516548156738 = 0.017658807337284088 + 2.0 * 5.983928680419922
Epoch 1670, val loss: 1.0484333038330078
Epoch 1680, training loss: 11.990610122680664 = 0.01733982190489769 + 2.0 * 5.986635208129883
Epoch 1680, val loss: 1.0512521266937256
Epoch 1690, training loss: 11.989852905273438 = 0.01702798716723919 + 2.0 * 5.986412525177002
Epoch 1690, val loss: 1.054002046585083
Epoch 1700, training loss: 11.98200511932373 = 0.016731631010770798 + 2.0 * 5.98263692855835
Epoch 1700, val loss: 1.057142734527588
Epoch 1710, training loss: 11.980057716369629 = 0.016438553109765053 + 2.0 * 5.981809616088867
Epoch 1710, val loss: 1.060058832168579
Epoch 1720, training loss: 11.982542037963867 = 0.01614883914589882 + 2.0 * 5.98319673538208
Epoch 1720, val loss: 1.062872052192688
Epoch 1730, training loss: 11.991475105285645 = 0.0158680472522974 + 2.0 * 5.9878034591674805
Epoch 1730, val loss: 1.0654504299163818
Epoch 1740, training loss: 11.978880882263184 = 0.01559604424983263 + 2.0 * 5.981642246246338
Epoch 1740, val loss: 1.0682545900344849
Epoch 1750, training loss: 11.979065895080566 = 0.015336062759160995 + 2.0 * 5.981864929199219
Epoch 1750, val loss: 1.0713456869125366
Epoch 1760, training loss: 11.981118202209473 = 0.015079758130013943 + 2.0 * 5.9830193519592285
Epoch 1760, val loss: 1.0740948915481567
Epoch 1770, training loss: 11.982784271240234 = 0.014827188104391098 + 2.0 * 5.983978748321533
Epoch 1770, val loss: 1.0766685009002686
Epoch 1780, training loss: 11.981334686279297 = 0.014584868215024471 + 2.0 * 5.983375072479248
Epoch 1780, val loss: 1.0794548988342285
Epoch 1790, training loss: 11.986047744750977 = 0.014347289688885212 + 2.0 * 5.9858503341674805
Epoch 1790, val loss: 1.0822335481643677
Epoch 1800, training loss: 11.978693962097168 = 0.014114508405327797 + 2.0 * 5.982289791107178
Epoch 1800, val loss: 1.0849556922912598
Epoch 1810, training loss: 11.97685718536377 = 0.01388661190867424 + 2.0 * 5.981485366821289
Epoch 1810, val loss: 1.0877217054367065
Epoch 1820, training loss: 11.975948333740234 = 0.013663816265761852 + 2.0 * 5.981142044067383
Epoch 1820, val loss: 1.090423583984375
Epoch 1830, training loss: 11.973793029785156 = 0.013445122167468071 + 2.0 * 5.9801740646362305
Epoch 1830, val loss: 1.093138337135315
Epoch 1840, training loss: 11.98167610168457 = 0.013231064192950726 + 2.0 * 5.984222412109375
Epoch 1840, val loss: 1.0957456827163696
Epoch 1850, training loss: 11.97721004486084 = 0.01302271243184805 + 2.0 * 5.982093811035156
Epoch 1850, val loss: 1.0982847213745117
Epoch 1860, training loss: 11.974077224731445 = 0.012820897623896599 + 2.0 * 5.98062801361084
Epoch 1860, val loss: 1.1009818315505981
Epoch 1870, training loss: 11.979811668395996 = 0.012625345028936863 + 2.0 * 5.983592987060547
Epoch 1870, val loss: 1.1037652492523193
Epoch 1880, training loss: 11.973251342773438 = 0.012433460913598537 + 2.0 * 5.980409145355225
Epoch 1880, val loss: 1.1061046123504639
Epoch 1890, training loss: 11.9707670211792 = 0.012247789651155472 + 2.0 * 5.979259490966797
Epoch 1890, val loss: 1.108886480331421
Epoch 1900, training loss: 11.969167709350586 = 0.01206395123153925 + 2.0 * 5.978551864624023
Epoch 1900, val loss: 1.1115434169769287
Epoch 1910, training loss: 11.972335815429688 = 0.011883159168064594 + 2.0 * 5.980226516723633
Epoch 1910, val loss: 1.1140443086624146
Epoch 1920, training loss: 11.974678993225098 = 0.011706049554049969 + 2.0 * 5.9814863204956055
Epoch 1920, val loss: 1.1161844730377197
Epoch 1930, training loss: 11.968001365661621 = 0.011535380966961384 + 2.0 * 5.9782328605651855
Epoch 1930, val loss: 1.1187728643417358
Epoch 1940, training loss: 11.968194961547852 = 0.011370674706995487 + 2.0 * 5.97841215133667
Epoch 1940, val loss: 1.1215949058532715
Epoch 1950, training loss: 11.966582298278809 = 0.01120668277144432 + 2.0 * 5.977687835693359
Epoch 1950, val loss: 1.124055027961731
Epoch 1960, training loss: 11.967866897583008 = 0.011043660342693329 + 2.0 * 5.978411674499512
Epoch 1960, val loss: 1.1265716552734375
Epoch 1970, training loss: 11.980833053588867 = 0.010886109434068203 + 2.0 * 5.984973430633545
Epoch 1970, val loss: 1.1287918090820312
Epoch 1980, training loss: 11.967291831970215 = 0.010730463080108166 + 2.0 * 5.978280544281006
Epoch 1980, val loss: 1.1311832666397095
Epoch 1990, training loss: 11.965580940246582 = 0.010581595823168755 + 2.0 * 5.977499485015869
Epoch 1990, val loss: 1.1338268518447876
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7926
Overall ASR: 0.8561
Flip ASR: 0.8267/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.700824737548828 = 1.9532227516174316 + 2.0 * 8.373801231384277
Epoch 0, val loss: 1.9524775743484497
Epoch 10, training loss: 18.687850952148438 = 1.9418435096740723 + 2.0 * 8.373003959655762
Epoch 10, val loss: 1.9413373470306396
Epoch 20, training loss: 18.662700653076172 = 1.9281200170516968 + 2.0 * 8.367290496826172
Epoch 20, val loss: 1.9273496866226196
Epoch 30, training loss: 18.56523895263672 = 1.9102287292480469 + 2.0 * 8.327505111694336
Epoch 30, val loss: 1.9087809324264526
Epoch 40, training loss: 17.94043731689453 = 1.890193223953247 + 2.0 * 8.025121688842773
Epoch 40, val loss: 1.8886253833770752
Epoch 50, training loss: 16.37244415283203 = 1.870510220527649 + 2.0 * 7.250966548919678
Epoch 50, val loss: 1.8691116571426392
Epoch 60, training loss: 15.825787544250488 = 1.8568426370620728 + 2.0 * 6.984472274780273
Epoch 60, val loss: 1.855048418045044
Epoch 70, training loss: 15.338813781738281 = 1.8452563285827637 + 2.0 * 6.74677848815918
Epoch 70, val loss: 1.8431179523468018
Epoch 80, training loss: 14.93448257446289 = 1.8357642889022827 + 2.0 * 6.549359321594238
Epoch 80, val loss: 1.8335591554641724
Epoch 90, training loss: 14.717195510864258 = 1.8265166282653809 + 2.0 * 6.445339679718018
Epoch 90, val loss: 1.824209213256836
Epoch 100, training loss: 14.535163879394531 = 1.8173279762268066 + 2.0 * 6.358918190002441
Epoch 100, val loss: 1.8145719766616821
Epoch 110, training loss: 14.405539512634277 = 1.808489441871643 + 2.0 * 6.298524856567383
Epoch 110, val loss: 1.8050569295883179
Epoch 120, training loss: 14.31395435333252 = 1.7999955415725708 + 2.0 * 6.256979465484619
Epoch 120, val loss: 1.7957483530044556
Epoch 130, training loss: 14.24820327758789 = 1.7915806770324707 + 2.0 * 6.228311061859131
Epoch 130, val loss: 1.7863789796829224
Epoch 140, training loss: 14.19748306274414 = 1.783091425895691 + 2.0 * 6.20719575881958
Epoch 140, val loss: 1.777015209197998
Epoch 150, training loss: 14.151589393615723 = 1.7744643688201904 + 2.0 * 6.188562393188477
Epoch 150, val loss: 1.7677710056304932
Epoch 160, training loss: 14.112517356872559 = 1.765559434890747 + 2.0 * 6.173479080200195
Epoch 160, val loss: 1.7586132287979126
Epoch 170, training loss: 14.082059860229492 = 1.7561750411987305 + 2.0 * 6.162942409515381
Epoch 170, val loss: 1.7494537830352783
Epoch 180, training loss: 14.045296669006348 = 1.7462046146392822 + 2.0 * 6.149546146392822
Epoch 180, val loss: 1.7400875091552734
Epoch 190, training loss: 14.015007019042969 = 1.735275149345398 + 2.0 * 6.139865875244141
Epoch 190, val loss: 1.7303500175476074
Epoch 200, training loss: 13.985285758972168 = 1.722997784614563 + 2.0 * 6.131144046783447
Epoch 200, val loss: 1.7198394536972046
Epoch 210, training loss: 13.962202072143555 = 1.708878755569458 + 2.0 * 6.126661777496338
Epoch 210, val loss: 1.7082377672195435
Epoch 220, training loss: 13.928977012634277 = 1.6927543878555298 + 2.0 * 6.1181111335754395
Epoch 220, val loss: 1.6951625347137451
Epoch 230, training loss: 13.897134780883789 = 1.6740649938583374 + 2.0 * 6.11153507232666
Epoch 230, val loss: 1.6803014278411865
Epoch 240, training loss: 13.86488151550293 = 1.6521596908569336 + 2.0 * 6.106360912322998
Epoch 240, val loss: 1.6629911661148071
Epoch 250, training loss: 13.832653999328613 = 1.6264246702194214 + 2.0 * 6.103114604949951
Epoch 250, val loss: 1.6430286169052124
Epoch 260, training loss: 13.78986930847168 = 1.5967092514038086 + 2.0 * 6.0965800285339355
Epoch 260, val loss: 1.619775414466858
Epoch 270, training loss: 13.746772766113281 = 1.5622280836105347 + 2.0 * 6.0922722816467285
Epoch 270, val loss: 1.5928308963775635
Epoch 280, training loss: 13.702116012573242 = 1.5228780508041382 + 2.0 * 6.089619159698486
Epoch 280, val loss: 1.5621402263641357
Epoch 290, training loss: 13.650191307067871 = 1.4796055555343628 + 2.0 * 6.085292816162109
Epoch 290, val loss: 1.5285334587097168
Epoch 300, training loss: 13.597504615783691 = 1.4332568645477295 + 2.0 * 6.082123756408691
Epoch 300, val loss: 1.4926033020019531
Epoch 310, training loss: 13.544434547424316 = 1.3851786851882935 + 2.0 * 6.079627990722656
Epoch 310, val loss: 1.455417513847351
Epoch 320, training loss: 13.498528480529785 = 1.3372743129730225 + 2.0 * 6.080626964569092
Epoch 320, val loss: 1.418684720993042
Epoch 330, training loss: 13.438960075378418 = 1.2912064790725708 + 2.0 * 6.073876857757568
Epoch 330, val loss: 1.3841081857681274
Epoch 340, training loss: 13.389998435974121 = 1.247695803642273 + 2.0 * 6.071151256561279
Epoch 340, val loss: 1.3516454696655273
Epoch 350, training loss: 13.349340438842773 = 1.2066277265548706 + 2.0 * 6.071356296539307
Epoch 350, val loss: 1.3217124938964844
Epoch 360, training loss: 13.303041458129883 = 1.168292760848999 + 2.0 * 6.067374229431152
Epoch 360, val loss: 1.2941092252731323
Epoch 370, training loss: 13.261091232299805 = 1.1317052841186523 + 2.0 * 6.064692974090576
Epoch 370, val loss: 1.2683793306350708
Epoch 380, training loss: 13.22069263458252 = 1.0961261987686157 + 2.0 * 6.062283039093018
Epoch 380, val loss: 1.2436567544937134
Epoch 390, training loss: 13.183926582336426 = 1.0609462261199951 + 2.0 * 6.061490058898926
Epoch 390, val loss: 1.2194188833236694
Epoch 400, training loss: 13.142095565795898 = 1.0260076522827148 + 2.0 * 6.058043956756592
Epoch 400, val loss: 1.1953309774398804
Epoch 410, training loss: 13.102547645568848 = 0.9907239079475403 + 2.0 * 6.055912017822266
Epoch 410, val loss: 1.1708256006240845
Epoch 420, training loss: 13.078536987304688 = 0.9547851085662842 + 2.0 * 6.061875820159912
Epoch 420, val loss: 1.145754098892212
Epoch 430, training loss: 13.027067184448242 = 0.9190335869789124 + 2.0 * 6.054016590118408
Epoch 430, val loss: 1.1201422214508057
Epoch 440, training loss: 12.985808372497559 = 0.8831091523170471 + 2.0 * 6.051349639892578
Epoch 440, val loss: 1.0942758321762085
Epoch 450, training loss: 12.94570541381836 = 0.8471574783325195 + 2.0 * 6.04927396774292
Epoch 450, val loss: 1.0680805444717407
Epoch 460, training loss: 12.929119110107422 = 0.8114701509475708 + 2.0 * 6.05882453918457
Epoch 460, val loss: 1.041811466217041
Epoch 470, training loss: 12.876200675964355 = 0.7768693566322327 + 2.0 * 6.049665451049805
Epoch 470, val loss: 1.016281008720398
Epoch 480, training loss: 12.834639549255371 = 0.7434431910514832 + 2.0 * 6.045598030090332
Epoch 480, val loss: 0.991645097732544
Epoch 490, training loss: 12.807157516479492 = 0.7111116647720337 + 2.0 * 6.048022747039795
Epoch 490, val loss: 0.9679599404335022
Epoch 500, training loss: 12.772171020507812 = 0.6802229881286621 + 2.0 * 6.045974254608154
Epoch 500, val loss: 0.9456512928009033
Epoch 510, training loss: 12.736205101013184 = 0.6509696245193481 + 2.0 * 6.0426177978515625
Epoch 510, val loss: 0.9247243404388428
Epoch 520, training loss: 12.704044342041016 = 0.6230533719062805 + 2.0 * 6.0404953956604
Epoch 520, val loss: 0.9053543210029602
Epoch 530, training loss: 12.682122230529785 = 0.5963717699050903 + 2.0 * 6.042875289916992
Epoch 530, val loss: 0.8873099088668823
Epoch 540, training loss: 12.647624969482422 = 0.5709418058395386 + 2.0 * 6.038341522216797
Epoch 540, val loss: 0.870769739151001
Epoch 550, training loss: 12.628190994262695 = 0.5465925335884094 + 2.0 * 6.040799140930176
Epoch 550, val loss: 0.8557013869285583
Epoch 560, training loss: 12.60197925567627 = 0.5234432816505432 + 2.0 * 6.0392680168151855
Epoch 560, val loss: 0.841867208480835
Epoch 570, training loss: 12.570828437805176 = 0.5013365149497986 + 2.0 * 6.034746170043945
Epoch 570, val loss: 0.8294434547424316
Epoch 580, training loss: 12.549463272094727 = 0.4801407754421234 + 2.0 * 6.034661293029785
Epoch 580, val loss: 0.818245530128479
Epoch 590, training loss: 12.527473449707031 = 0.459800660610199 + 2.0 * 6.033836364746094
Epoch 590, val loss: 0.808194637298584
Epoch 600, training loss: 12.511251449584961 = 0.4403703212738037 + 2.0 * 6.035440444946289
Epoch 600, val loss: 0.7993296980857849
Epoch 610, training loss: 12.487038612365723 = 0.4218488335609436 + 2.0 * 6.032594680786133
Epoch 610, val loss: 0.7916173934936523
Epoch 620, training loss: 12.465447425842285 = 0.40421849489212036 + 2.0 * 6.030614376068115
Epoch 620, val loss: 0.7850306630134583
Epoch 630, training loss: 12.444964408874512 = 0.3874013423919678 + 2.0 * 6.028781414031982
Epoch 630, val loss: 0.7793922424316406
Epoch 640, training loss: 12.43738842010498 = 0.37128183245658875 + 2.0 * 6.033053398132324
Epoch 640, val loss: 0.7746118903160095
Epoch 650, training loss: 12.414827346801758 = 0.3559880256652832 + 2.0 * 6.029419422149658
Epoch 650, val loss: 0.7707821130752563
Epoch 660, training loss: 12.407194137573242 = 0.3414837718009949 + 2.0 * 6.032855033874512
Epoch 660, val loss: 0.7677815556526184
Epoch 670, training loss: 12.382018089294434 = 0.3276617228984833 + 2.0 * 6.0271782875061035
Epoch 670, val loss: 0.7654675245285034
Epoch 680, training loss: 12.364218711853027 = 0.3145279288291931 + 2.0 * 6.024845600128174
Epoch 680, val loss: 0.7638254165649414
Epoch 690, training loss: 12.348039627075195 = 0.30189627408981323 + 2.0 * 6.023071765899658
Epoch 690, val loss: 0.762640655040741
Epoch 700, training loss: 12.344002723693848 = 0.28970369696617126 + 2.0 * 6.027149677276611
Epoch 700, val loss: 0.7619085907936096
Epoch 710, training loss: 12.333372116088867 = 0.27798905968666077 + 2.0 * 6.02769136428833
Epoch 710, val loss: 0.7614464163780212
Epoch 720, training loss: 12.312237739562988 = 0.2668515741825104 + 2.0 * 6.022693157196045
Epoch 720, val loss: 0.7615482211112976
Epoch 730, training loss: 12.297096252441406 = 0.25603657960891724 + 2.0 * 6.020529747009277
Epoch 730, val loss: 0.7619152069091797
Epoch 740, training loss: 12.284547805786133 = 0.24546919763088226 + 2.0 * 6.0195393562316895
Epoch 740, val loss: 0.7624726295471191
Epoch 750, training loss: 12.287439346313477 = 0.23512820899486542 + 2.0 * 6.026155471801758
Epoch 750, val loss: 0.763229489326477
Epoch 760, training loss: 12.268339157104492 = 0.22514601051807404 + 2.0 * 6.021596431732178
Epoch 760, val loss: 0.7641128897666931
Epoch 770, training loss: 12.252004623413086 = 0.21540239453315735 + 2.0 * 6.018301010131836
Epoch 770, val loss: 0.7652906775474548
Epoch 780, training loss: 12.244375228881836 = 0.2059250921010971 + 2.0 * 6.019225120544434
Epoch 780, val loss: 0.7665393352508545
Epoch 790, training loss: 12.229991912841797 = 0.1967543214559555 + 2.0 * 6.016618728637695
Epoch 790, val loss: 0.767979621887207
Epoch 800, training loss: 12.22072696685791 = 0.18784993886947632 + 2.0 * 6.0164384841918945
Epoch 800, val loss: 0.7696056962013245
Epoch 810, training loss: 12.219108581542969 = 0.17925401031970978 + 2.0 * 6.019927501678467
Epoch 810, val loss: 0.7712811827659607
Epoch 820, training loss: 12.207035064697266 = 0.17103996872901917 + 2.0 * 6.017997741699219
Epoch 820, val loss: 0.7731192708015442
Epoch 830, training loss: 12.192571640014648 = 0.16319668292999268 + 2.0 * 6.014687538146973
Epoch 830, val loss: 0.7753047347068787
Epoch 840, training loss: 12.182143211364746 = 0.15566693246364594 + 2.0 * 6.013237953186035
Epoch 840, val loss: 0.7776901721954346
Epoch 850, training loss: 12.17890739440918 = 0.1484667956829071 + 2.0 * 6.0152201652526855
Epoch 850, val loss: 0.7802730202674866
Epoch 860, training loss: 12.18327522277832 = 0.1416824609041214 + 2.0 * 6.020796298980713
Epoch 860, val loss: 0.7828456163406372
Epoch 870, training loss: 12.163488388061523 = 0.13531774282455444 + 2.0 * 6.014085292816162
Epoch 870, val loss: 0.7857941389083862
Epoch 880, training loss: 12.153035163879395 = 0.12930437922477722 + 2.0 * 6.011865615844727
Epoch 880, val loss: 0.7889834642410278
Epoch 890, training loss: 12.146364212036133 = 0.12359462678432465 + 2.0 * 6.011384963989258
Epoch 890, val loss: 0.792301595211029
Epoch 900, training loss: 12.146241188049316 = 0.11819631606340408 + 2.0 * 6.014022350311279
Epoch 900, val loss: 0.7957503199577332
Epoch 910, training loss: 12.139413833618164 = 0.11313394457101822 + 2.0 * 6.013139724731445
Epoch 910, val loss: 0.7993514537811279
Epoch 920, training loss: 12.1294527053833 = 0.10839442908763885 + 2.0 * 6.010529041290283
Epoch 920, val loss: 0.8032264113426208
Epoch 930, training loss: 12.12185001373291 = 0.10388176143169403 + 2.0 * 6.008984088897705
Epoch 930, val loss: 0.8072133660316467
Epoch 940, training loss: 12.116958618164062 = 0.0996130183339119 + 2.0 * 6.008672714233398
Epoch 940, val loss: 0.8113136291503906
Epoch 950, training loss: 12.120919227600098 = 0.09557366371154785 + 2.0 * 6.0126729011535645
Epoch 950, val loss: 0.8154916763305664
Epoch 960, training loss: 12.11848258972168 = 0.0917738676071167 + 2.0 * 6.013354301452637
Epoch 960, val loss: 0.819733202457428
Epoch 970, training loss: 12.103487968444824 = 0.08818209916353226 + 2.0 * 6.007652759552002
Epoch 970, val loss: 0.8239657282829285
Epoch 980, training loss: 12.097854614257812 = 0.084794782102108 + 2.0 * 6.006529808044434
Epoch 980, val loss: 0.8284257054328918
Epoch 990, training loss: 12.093914031982422 = 0.0815642923116684 + 2.0 * 6.0061750411987305
Epoch 990, val loss: 0.8328608870506287
Epoch 1000, training loss: 12.109545707702637 = 0.07848504185676575 + 2.0 * 6.015530109405518
Epoch 1000, val loss: 0.8372218012809753
Epoch 1010, training loss: 12.087499618530273 = 0.07560572773218155 + 2.0 * 6.005947113037109
Epoch 1010, val loss: 0.8416698575019836
Epoch 1020, training loss: 12.081900596618652 = 0.07285678386688232 + 2.0 * 6.00452184677124
Epoch 1020, val loss: 0.846238911151886
Epoch 1030, training loss: 12.079062461853027 = 0.07022908329963684 + 2.0 * 6.004416465759277
Epoch 1030, val loss: 0.8508196473121643
Epoch 1040, training loss: 12.090229988098145 = 0.06772480905056 + 2.0 * 6.011252403259277
Epoch 1040, val loss: 0.8553911447525024
Epoch 1050, training loss: 12.078268051147461 = 0.06534362584352493 + 2.0 * 6.006462097167969
Epoch 1050, val loss: 0.8598259687423706
Epoch 1060, training loss: 12.07224178314209 = 0.06308083981275558 + 2.0 * 6.004580497741699
Epoch 1060, val loss: 0.8644288182258606
Epoch 1070, training loss: 12.075135231018066 = 0.06093261390924454 + 2.0 * 6.007101535797119
Epoch 1070, val loss: 0.8689890503883362
Epoch 1080, training loss: 12.064175605773926 = 0.058872636407613754 + 2.0 * 6.002651691436768
Epoch 1080, val loss: 0.8734431862831116
Epoch 1090, training loss: 12.060283660888672 = 0.056903962045907974 + 2.0 * 6.001689910888672
Epoch 1090, val loss: 0.878007173538208
Epoch 1100, training loss: 12.059148788452148 = 0.05501580238342285 + 2.0 * 6.002066612243652
Epoch 1100, val loss: 0.8825547695159912
Epoch 1110, training loss: 12.063547134399414 = 0.05320616811513901 + 2.0 * 6.0051703453063965
Epoch 1110, val loss: 0.8870201706886292
Epoch 1120, training loss: 12.056297302246094 = 0.0514855720102787 + 2.0 * 6.002405643463135
Epoch 1120, val loss: 0.8914124369621277
Epoch 1130, training loss: 12.052613258361816 = 0.04985111951828003 + 2.0 * 6.001380920410156
Epoch 1130, val loss: 0.8958364725112915
Epoch 1140, training loss: 12.050185203552246 = 0.048284854739904404 + 2.0 * 6.000950336456299
Epoch 1140, val loss: 0.9003233313560486
Epoch 1150, training loss: 12.05208683013916 = 0.046778347343206406 + 2.0 * 6.002654075622559
Epoch 1150, val loss: 0.9047349095344543
Epoch 1160, training loss: 12.047208786010742 = 0.04534589499235153 + 2.0 * 6.000931262969971
Epoch 1160, val loss: 0.9090670347213745
Epoch 1170, training loss: 12.041677474975586 = 0.0439758375287056 + 2.0 * 5.9988508224487305
Epoch 1170, val loss: 0.9134365916252136
Epoch 1180, training loss: 12.038734436035156 = 0.042659860104322433 + 2.0 * 5.998037338256836
Epoch 1180, val loss: 0.9178401827812195
Epoch 1190, training loss: 12.037259101867676 = 0.04139145836234093 + 2.0 * 5.997933864593506
Epoch 1190, val loss: 0.9222322702407837
Epoch 1200, training loss: 12.0575590133667 = 0.040171608328819275 + 2.0 * 6.008693695068359
Epoch 1200, val loss: 0.9265276193618774
Epoch 1210, training loss: 12.044737815856934 = 0.03900732845067978 + 2.0 * 6.002865314483643
Epoch 1210, val loss: 0.9306363463401794
Epoch 1220, training loss: 12.033175468444824 = 0.037904027849435806 + 2.0 * 5.997635841369629
Epoch 1220, val loss: 0.9348849058151245
Epoch 1230, training loss: 12.030789375305176 = 0.03683917969465256 + 2.0 * 5.996974945068359
Epoch 1230, val loss: 0.9391431212425232
Epoch 1240, training loss: 12.047953605651855 = 0.03581499680876732 + 2.0 * 6.006069183349609
Epoch 1240, val loss: 0.9432404041290283
Epoch 1250, training loss: 12.026851654052734 = 0.03483758866786957 + 2.0 * 5.996006965637207
Epoch 1250, val loss: 0.9472360014915466
Epoch 1260, training loss: 12.027475357055664 = 0.03390133008360863 + 2.0 * 5.996787071228027
Epoch 1260, val loss: 0.9513319134712219
Epoch 1270, training loss: 12.02334976196289 = 0.03299377113580704 + 2.0 * 5.99517822265625
Epoch 1270, val loss: 0.9554442167282104
Epoch 1280, training loss: 12.02706241607666 = 0.03211468085646629 + 2.0 * 5.99747371673584
Epoch 1280, val loss: 0.9594918489456177
Epoch 1290, training loss: 12.020917892456055 = 0.03127055615186691 + 2.0 * 5.994823455810547
Epoch 1290, val loss: 0.9634091854095459
Epoch 1300, training loss: 12.02100944519043 = 0.030464036390185356 + 2.0 * 5.995272636413574
Epoch 1300, val loss: 0.9673656225204468
Epoch 1310, training loss: 12.020780563354492 = 0.029687214642763138 + 2.0 * 5.995546817779541
Epoch 1310, val loss: 0.9714158773422241
Epoch 1320, training loss: 12.02643871307373 = 0.028938865289092064 + 2.0 * 5.998749732971191
Epoch 1320, val loss: 0.9751922488212585
Epoch 1330, training loss: 12.015339851379395 = 0.028220022097229958 + 2.0 * 5.993559837341309
Epoch 1330, val loss: 0.9789625406265259
Epoch 1340, training loss: 12.015968322753906 = 0.027528470382094383 + 2.0 * 5.994219779968262
Epoch 1340, val loss: 0.9828182458877563
Epoch 1350, training loss: 12.01272201538086 = 0.026858536526560783 + 2.0 * 5.992931842803955
Epoch 1350, val loss: 0.9866859912872314
Epoch 1360, training loss: 12.01448917388916 = 0.02620544098317623 + 2.0 * 5.994142055511475
Epoch 1360, val loss: 0.9904829263687134
Epoch 1370, training loss: 12.01106071472168 = 0.02557751163840294 + 2.0 * 5.992741584777832
Epoch 1370, val loss: 0.9940842390060425
Epoch 1380, training loss: 12.014365196228027 = 0.02498273178935051 + 2.0 * 5.994691371917725
Epoch 1380, val loss: 0.9976741075515747
Epoch 1390, training loss: 12.00821304321289 = 0.024406572803854942 + 2.0 * 5.991903305053711
Epoch 1390, val loss: 1.0013978481292725
Epoch 1400, training loss: 12.007048606872559 = 0.023843472823500633 + 2.0 * 5.991602420806885
Epoch 1400, val loss: 1.005082607269287
Epoch 1410, training loss: 12.007073402404785 = 0.023295460268855095 + 2.0 * 5.991888999938965
Epoch 1410, val loss: 1.0087405443191528
Epoch 1420, training loss: 12.013829231262207 = 0.02276550978422165 + 2.0 * 5.995532035827637
Epoch 1420, val loss: 1.0122125148773193
Epoch 1430, training loss: 12.010078430175781 = 0.022260285913944244 + 2.0 * 5.993908882141113
Epoch 1430, val loss: 1.0156089067459106
Epoch 1440, training loss: 12.003189086914062 = 0.021777207031846046 + 2.0 * 5.990705966949463
Epoch 1440, val loss: 1.019142746925354
Epoch 1450, training loss: 12.002718925476074 = 0.021304836496710777 + 2.0 * 5.990706920623779
Epoch 1450, val loss: 1.0226945877075195
Epoch 1460, training loss: 12.005171775817871 = 0.02084388956427574 + 2.0 * 5.992164134979248
Epoch 1460, val loss: 1.026183843612671
Epoch 1470, training loss: 12.005777359008789 = 0.02039678581058979 + 2.0 * 5.992690086364746
Epoch 1470, val loss: 1.0294981002807617
Epoch 1480, training loss: 12.002961158752441 = 0.019965793937444687 + 2.0 * 5.99149751663208
Epoch 1480, val loss: 1.0328190326690674
Epoch 1490, training loss: 12.003210067749023 = 0.019552145153284073 + 2.0 * 5.991828918457031
Epoch 1490, val loss: 1.0361837148666382
Epoch 1500, training loss: 12.00869369506836 = 0.019148800522089005 + 2.0 * 5.994772434234619
Epoch 1500, val loss: 1.039402961730957
Epoch 1510, training loss: 11.99858570098877 = 0.018758432939648628 + 2.0 * 5.989913463592529
Epoch 1510, val loss: 1.0425734519958496
Epoch 1520, training loss: 11.995537757873535 = 0.018381832167506218 + 2.0 * 5.988577842712402
Epoch 1520, val loss: 1.045868158340454
Epoch 1530, training loss: 11.99441146850586 = 0.018012089654803276 + 2.0 * 5.988199710845947
Epoch 1530, val loss: 1.0491235256195068
Epoch 1540, training loss: 12.00183391571045 = 0.017649782821536064 + 2.0 * 5.992092132568359
Epoch 1540, val loss: 1.0523295402526855
Epoch 1550, training loss: 11.994636535644531 = 0.017304863780736923 + 2.0 * 5.98866605758667
Epoch 1550, val loss: 1.0553933382034302
Epoch 1560, training loss: 11.997039794921875 = 0.016969900578260422 + 2.0 * 5.990035057067871
Epoch 1560, val loss: 1.0583852529525757
Epoch 1570, training loss: 11.994882583618164 = 0.016648001968860626 + 2.0 * 5.98911714553833
Epoch 1570, val loss: 1.0615257024765015
Epoch 1580, training loss: 11.99837875366211 = 0.016331059858202934 + 2.0 * 5.991024017333984
Epoch 1580, val loss: 1.0645785331726074
Epoch 1590, training loss: 11.999274253845215 = 0.016023756936192513 + 2.0 * 5.9916253089904785
Epoch 1590, val loss: 1.0675057172775269
Epoch 1600, training loss: 11.991429328918457 = 0.01572892814874649 + 2.0 * 5.987850189208984
Epoch 1600, val loss: 1.0704706907272339
Epoch 1610, training loss: 11.98857307434082 = 0.015440485440194607 + 2.0 * 5.986566066741943
Epoch 1610, val loss: 1.0735092163085938
Epoch 1620, training loss: 11.987744331359863 = 0.015156707726418972 + 2.0 * 5.986293792724609
Epoch 1620, val loss: 1.0765358209609985
Epoch 1630, training loss: 11.996006965637207 = 0.01487942785024643 + 2.0 * 5.990563869476318
Epoch 1630, val loss: 1.0794954299926758
Epoch 1640, training loss: 11.99384880065918 = 0.014612213708460331 + 2.0 * 5.989618301391602
Epoch 1640, val loss: 1.082326054573059
Epoch 1650, training loss: 11.990629196166992 = 0.014355855993926525 + 2.0 * 5.9881367683410645
Epoch 1650, val loss: 1.0850114822387695
Epoch 1660, training loss: 11.98593807220459 = 0.014106589369475842 + 2.0 * 5.985915660858154
Epoch 1660, val loss: 1.087965726852417
Epoch 1670, training loss: 11.984585762023926 = 0.013860413804650307 + 2.0 * 5.985362529754639
Epoch 1670, val loss: 1.090857744216919
Epoch 1680, training loss: 11.988348007202148 = 0.013618825003504753 + 2.0 * 5.987364768981934
Epoch 1680, val loss: 1.0936945676803589
Epoch 1690, training loss: 11.988214492797852 = 0.013384527526795864 + 2.0 * 5.987414836883545
Epoch 1690, val loss: 1.0964515209197998
Epoch 1700, training loss: 11.987284660339355 = 0.013159772381186485 + 2.0 * 5.987062454223633
Epoch 1700, val loss: 1.0991384983062744
Epoch 1710, training loss: 11.984909057617188 = 0.012940601445734501 + 2.0 * 5.9859843254089355
Epoch 1710, val loss: 1.1018426418304443
Epoch 1720, training loss: 11.98222827911377 = 0.012726673856377602 + 2.0 * 5.984750747680664
Epoch 1720, val loss: 1.104572057723999
Epoch 1730, training loss: 11.985191345214844 = 0.01251564733684063 + 2.0 * 5.986337661743164
Epoch 1730, val loss: 1.107321858406067
Epoch 1740, training loss: 11.985231399536133 = 0.012310023419559002 + 2.0 * 5.9864606857299805
Epoch 1740, val loss: 1.1099737882614136
Epoch 1750, training loss: 11.985044479370117 = 0.012110681273043156 + 2.0 * 5.986466884613037
Epoch 1750, val loss: 1.1126470565795898
Epoch 1760, training loss: 11.983672142028809 = 0.011916287243366241 + 2.0 * 5.985877990722656
Epoch 1760, val loss: 1.1153054237365723
Epoch 1770, training loss: 11.983012199401855 = 0.01172630488872528 + 2.0 * 5.985642910003662
Epoch 1770, val loss: 1.1179101467132568
Epoch 1780, training loss: 11.99091911315918 = 0.011540747247636318 + 2.0 * 5.989689350128174
Epoch 1780, val loss: 1.1203410625457764
Epoch 1790, training loss: 11.980003356933594 = 0.011365561746060848 + 2.0 * 5.984318733215332
Epoch 1790, val loss: 1.122950792312622
Epoch 1800, training loss: 11.977194786071777 = 0.011190400458872318 + 2.0 * 5.983002185821533
Epoch 1800, val loss: 1.1255260705947876
Epoch 1810, training loss: 11.976454734802246 = 0.011018498800694942 + 2.0 * 5.982717990875244
Epoch 1810, val loss: 1.1281423568725586
Epoch 1820, training loss: 11.980683326721191 = 0.010848178528249264 + 2.0 * 5.984917640686035
Epoch 1820, val loss: 1.1307542324066162
Epoch 1830, training loss: 11.98007583618164 = 0.01068340614438057 + 2.0 * 5.984696388244629
Epoch 1830, val loss: 1.1332391500473022
Epoch 1840, training loss: 11.978076934814453 = 0.01052304357290268 + 2.0 * 5.983777046203613
Epoch 1840, val loss: 1.1356642246246338
Epoch 1850, training loss: 11.986393928527832 = 0.01036775391548872 + 2.0 * 5.98801326751709
Epoch 1850, val loss: 1.1381542682647705
Epoch 1860, training loss: 11.977167129516602 = 0.01021624356508255 + 2.0 * 5.983475208282471
Epoch 1860, val loss: 1.1405525207519531
Epoch 1870, training loss: 11.974851608276367 = 0.010067945346236229 + 2.0 * 5.982391834259033
Epoch 1870, val loss: 1.1429754495620728
Epoch 1880, training loss: 11.976762771606445 = 0.009922428987920284 + 2.0 * 5.983420372009277
Epoch 1880, val loss: 1.1454510688781738
Epoch 1890, training loss: 11.977142333984375 = 0.009779307059943676 + 2.0 * 5.983681678771973
Epoch 1890, val loss: 1.1477998495101929
Epoch 1900, training loss: 11.98216438293457 = 0.009639998897910118 + 2.0 * 5.986262321472168
Epoch 1900, val loss: 1.1501212120056152
Epoch 1910, training loss: 11.976027488708496 = 0.009504036046564579 + 2.0 * 5.983261585235596
Epoch 1910, val loss: 1.1524569988250732
Epoch 1920, training loss: 11.974894523620605 = 0.00937194749712944 + 2.0 * 5.982761383056641
Epoch 1920, val loss: 1.1547493934631348
Epoch 1930, training loss: 11.971111297607422 = 0.009242947213351727 + 2.0 * 5.980934143066406
Epoch 1930, val loss: 1.1571285724639893
Epoch 1940, training loss: 11.970649719238281 = 0.009114409796893597 + 2.0 * 5.980767726898193
Epoch 1940, val loss: 1.159496545791626
Epoch 1950, training loss: 11.980899810791016 = 0.008987528271973133 + 2.0 * 5.985956192016602
Epoch 1950, val loss: 1.1618225574493408
Epoch 1960, training loss: 11.977388381958008 = 0.008865340612828732 + 2.0 * 5.984261512756348
Epoch 1960, val loss: 1.1640429496765137
Epoch 1970, training loss: 11.972809791564941 = 0.008746892213821411 + 2.0 * 5.982031345367432
Epoch 1970, val loss: 1.166199803352356
Epoch 1980, training loss: 11.970239639282227 = 0.008631300181150436 + 2.0 * 5.980803966522217
Epoch 1980, val loss: 1.168483853340149
Epoch 1990, training loss: 11.980123519897461 = 0.008515933528542519 + 2.0 * 5.985803604125977
Epoch 1990, val loss: 1.170680046081543
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7926
Overall ASR: 0.9557
Flip ASR: 0.9467/225 nodes
The final ASR:0.86839, 0.06685, Accuracy:0.79506, 0.00349
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11630])
remove edge: torch.Size([2, 9470])
updated graph: torch.Size([2, 10544])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9926
Flip ASR: 0.9911/225 nodes
The final ASR:0.98155, 0.00797, Accuracy:0.82963, 0.00800
Begin epxeriment: cont_weight: 2 epoch:2000 der1:0.3 der2:0 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 18.69767189025879 = 1.9500629901885986 + 2.0 * 8.373804092407227
Epoch 0, val loss: 1.9488409757614136
Epoch 10, training loss: 18.685190200805664 = 1.938999891281128 + 2.0 * 8.373095512390137
Epoch 10, val loss: 1.9378236532211304
Epoch 20, training loss: 18.66182518005371 = 1.925469160079956 + 2.0 * 8.368178367614746
Epoch 20, val loss: 1.9242198467254639
Epoch 30, training loss: 18.567790985107422 = 1.9079076051712036 + 2.0 * 8.329941749572754
Epoch 30, val loss: 1.906702995300293
Epoch 40, training loss: 17.910043716430664 = 1.8884568214416504 + 2.0 * 8.010793685913086
Epoch 40, val loss: 1.8881486654281616
Epoch 50, training loss: 16.11680030822754 = 1.8697879314422607 + 2.0 * 7.12350606918335
Epoch 50, val loss: 1.8707327842712402
Epoch 60, training loss: 15.744827270507812 = 1.8554121255874634 + 2.0 * 6.94470739364624
Epoch 60, val loss: 1.8576276302337646
Epoch 70, training loss: 15.331267356872559 = 1.842564344406128 + 2.0 * 6.744351387023926
Epoch 70, val loss: 1.846120834350586
Epoch 80, training loss: 14.990461349487305 = 1.8321579694747925 + 2.0 * 6.579151630401611
Epoch 80, val loss: 1.8367609977722168
Epoch 90, training loss: 14.774089813232422 = 1.8227252960205078 + 2.0 * 6.475682258605957
Epoch 90, val loss: 1.8280528783798218
Epoch 100, training loss: 14.600397109985352 = 1.814035177230835 + 2.0 * 6.393180847167969
Epoch 100, val loss: 1.8200899362564087
Epoch 110, training loss: 14.475018501281738 = 1.8061500787734985 + 2.0 * 6.3344340324401855
Epoch 110, val loss: 1.8127613067626953
Epoch 120, training loss: 14.384263038635254 = 1.7983570098876953 + 2.0 * 6.292953014373779
Epoch 120, val loss: 1.8054273128509521
Epoch 130, training loss: 14.305662155151367 = 1.7905707359313965 + 2.0 * 6.257545471191406
Epoch 130, val loss: 1.798275351524353
Epoch 140, training loss: 14.242907524108887 = 1.7828763723373413 + 2.0 * 6.230015754699707
Epoch 140, val loss: 1.7913272380828857
Epoch 150, training loss: 14.186005592346191 = 1.7748544216156006 + 2.0 * 6.205575466156006
Epoch 150, val loss: 1.7841084003448486
Epoch 160, training loss: 14.140352249145508 = 1.7659192085266113 + 2.0 * 6.187216758728027
Epoch 160, val loss: 1.7763501405715942
Epoch 170, training loss: 14.104266166687012 = 1.7556226253509521 + 2.0 * 6.17432165145874
Epoch 170, val loss: 1.7676961421966553
Epoch 180, training loss: 14.063132286071777 = 1.7438563108444214 + 2.0 * 6.159637928009033
Epoch 180, val loss: 1.7579989433288574
Epoch 190, training loss: 14.033262252807617 = 1.7301974296569824 + 2.0 * 6.151532173156738
Epoch 190, val loss: 1.7470552921295166
Epoch 200, training loss: 13.993478775024414 = 1.7144027948379517 + 2.0 * 6.139537811279297
Epoch 200, val loss: 1.7343543767929077
Epoch 210, training loss: 13.956378936767578 = 1.6957030296325684 + 2.0 * 6.130337715148926
Epoch 210, val loss: 1.7194187641143799
Epoch 220, training loss: 13.919792175292969 = 1.6736767292022705 + 2.0 * 6.123057842254639
Epoch 220, val loss: 1.7019199132919312
Epoch 230, training loss: 13.878686904907227 = 1.6478146314620972 + 2.0 * 6.11543607711792
Epoch 230, val loss: 1.6812853813171387
Epoch 240, training loss: 13.835724830627441 = 1.6175132989883423 + 2.0 * 6.109105587005615
Epoch 240, val loss: 1.6570335626602173
Epoch 250, training loss: 13.792802810668945 = 1.582613468170166 + 2.0 * 6.1050944328308105
Epoch 250, val loss: 1.6289652585983276
Epoch 260, training loss: 13.741458892822266 = 1.5434789657592773 + 2.0 * 6.098989963531494
Epoch 260, val loss: 1.597319483757019
Epoch 270, training loss: 13.688385963439941 = 1.500009298324585 + 2.0 * 6.094188213348389
Epoch 270, val loss: 1.5620977878570557
Epoch 280, training loss: 13.635120391845703 = 1.4528117179870605 + 2.0 * 6.091154098510742
Epoch 280, val loss: 1.5239512920379639
Epoch 290, training loss: 13.581329345703125 = 1.4040828943252563 + 2.0 * 6.088623046875
Epoch 290, val loss: 1.484604001045227
Epoch 300, training loss: 13.52189826965332 = 1.3554255962371826 + 2.0 * 6.083236217498779
Epoch 300, val loss: 1.4456427097320557
Epoch 310, training loss: 13.468220710754395 = 1.3076379299163818 + 2.0 * 6.080291271209717
Epoch 310, val loss: 1.4077140092849731
Epoch 320, training loss: 13.415816307067871 = 1.2614301443099976 + 2.0 * 6.077193260192871
Epoch 320, val loss: 1.3714311122894287
Epoch 330, training loss: 13.36513900756836 = 1.2171730995178223 + 2.0 * 6.0739827156066895
Epoch 330, val loss: 1.3370119333267212
Epoch 340, training loss: 13.317233085632324 = 1.1745880842208862 + 2.0 * 6.071322441101074
Epoch 340, val loss: 1.3042960166931152
Epoch 350, training loss: 13.276948928833008 = 1.1332526206970215 + 2.0 * 6.071848392486572
Epoch 350, val loss: 1.2732080221176147
Epoch 360, training loss: 13.224164962768555 = 1.0933737754821777 + 2.0 * 6.065395355224609
Epoch 360, val loss: 1.2436256408691406
Epoch 370, training loss: 13.187501907348633 = 1.0541224479675293 + 2.0 * 6.066689968109131
Epoch 370, val loss: 1.2147343158721924
Epoch 380, training loss: 13.139975547790527 = 1.015423059463501 + 2.0 * 6.062276363372803
Epoch 380, val loss: 1.1869869232177734
Epoch 390, training loss: 13.093340873718262 = 0.9773820042610168 + 2.0 * 6.057979583740234
Epoch 390, val loss: 1.1597189903259277
Epoch 400, training loss: 13.051265716552734 = 0.9396885633468628 + 2.0 * 6.055788516998291
Epoch 400, val loss: 1.1330252885818481
Epoch 410, training loss: 13.027331352233887 = 0.9026862978935242 + 2.0 * 6.062322616577148
Epoch 410, val loss: 1.107110857963562
Epoch 420, training loss: 12.974546432495117 = 0.8672733902931213 + 2.0 * 6.05363655090332
Epoch 420, val loss: 1.0827449560165405
Epoch 430, training loss: 12.935041427612305 = 0.8331851363182068 + 2.0 * 6.050928115844727
Epoch 430, val loss: 1.059732437133789
Epoch 440, training loss: 12.896196365356445 = 0.800042450428009 + 2.0 * 6.04807710647583
Epoch 440, val loss: 1.0375537872314453
Epoch 450, training loss: 12.860281944274902 = 0.7676524519920349 + 2.0 * 6.046314716339111
Epoch 450, val loss: 1.016281008720398
Epoch 460, training loss: 12.863273620605469 = 0.7359237670898438 + 2.0 * 6.0636749267578125
Epoch 460, val loss: 0.9961101412773132
Epoch 470, training loss: 12.795269012451172 = 0.7060367465019226 + 2.0 * 6.044616222381592
Epoch 470, val loss: 0.9772810339927673
Epoch 480, training loss: 12.762075424194336 = 0.6770297884941101 + 2.0 * 6.04252290725708
Epoch 480, val loss: 0.9600865244865417
Epoch 490, training loss: 12.730231285095215 = 0.648485541343689 + 2.0 * 6.040873050689697
Epoch 490, val loss: 0.9434282183647156
Epoch 500, training loss: 12.699305534362793 = 0.6203596591949463 + 2.0 * 6.039473056793213
Epoch 500, val loss: 0.927609920501709
Epoch 510, training loss: 12.668793678283691 = 0.5927056670188904 + 2.0 * 6.038043975830078
Epoch 510, val loss: 0.912796139717102
Epoch 520, training loss: 12.662569046020508 = 0.5657222270965576 + 2.0 * 6.0484232902526855
Epoch 520, val loss: 0.8990488648414612
Epoch 530, training loss: 12.620209693908691 = 0.5400431156158447 + 2.0 * 6.040083408355713
Epoch 530, val loss: 0.8867001533508301
Epoch 540, training loss: 12.585168838500977 = 0.5154064297676086 + 2.0 * 6.034881114959717
Epoch 540, val loss: 0.8760513663291931
Epoch 550, training loss: 12.55961799621582 = 0.4917154908180237 + 2.0 * 6.033951282501221
Epoch 550, val loss: 0.8664074540138245
Epoch 560, training loss: 12.538159370422363 = 0.4690876603126526 + 2.0 * 6.034535884857178
Epoch 560, val loss: 0.857941210269928
Epoch 570, training loss: 12.51439094543457 = 0.44762155413627625 + 2.0 * 6.033384799957275
Epoch 570, val loss: 0.8511417508125305
Epoch 580, training loss: 12.489226341247559 = 0.4271170496940613 + 2.0 * 6.031054496765137
Epoch 580, val loss: 0.8454318046569824
Epoch 590, training loss: 12.485514640808105 = 0.40743911266326904 + 2.0 * 6.039037704467773
Epoch 590, val loss: 0.8406152129173279
Epoch 600, training loss: 12.446554183959961 = 0.38875913619995117 + 2.0 * 6.028897285461426
Epoch 600, val loss: 0.8370175361633301
Epoch 610, training loss: 12.426627159118652 = 0.3709045648574829 + 2.0 * 6.02786111831665
Epoch 610, val loss: 0.8344925045967102
Epoch 620, training loss: 12.413789749145508 = 0.35371893644332886 + 2.0 * 6.030035495758057
Epoch 620, val loss: 0.8325399160385132
Epoch 630, training loss: 12.39168930053711 = 0.3372349739074707 + 2.0 * 6.027227401733398
Epoch 630, val loss: 0.8314048647880554
Epoch 640, training loss: 12.374192237854004 = 0.32144245505332947 + 2.0 * 6.026374816894531
Epoch 640, val loss: 0.8310073614120483
Epoch 650, training loss: 12.358837127685547 = 0.30625462532043457 + 2.0 * 6.026291370391846
Epoch 650, val loss: 0.8312031030654907
Epoch 660, training loss: 12.339001655578613 = 0.29169467091560364 + 2.0 * 6.023653507232666
Epoch 660, val loss: 0.8318707346916199
Epoch 670, training loss: 12.32695484161377 = 0.27773186564445496 + 2.0 * 6.024611473083496
Epoch 670, val loss: 0.8332541584968567
Epoch 680, training loss: 12.309818267822266 = 0.26438018679618835 + 2.0 * 6.022718906402588
Epoch 680, val loss: 0.8349320292472839
Epoch 690, training loss: 12.295371055603027 = 0.2516331374645233 + 2.0 * 6.02186918258667
Epoch 690, val loss: 0.8371674418449402
Epoch 700, training loss: 12.28381633758545 = 0.23950648307800293 + 2.0 * 6.022154808044434
Epoch 700, val loss: 0.8399937152862549
Epoch 710, training loss: 12.266865730285645 = 0.2280278354883194 + 2.0 * 6.019418716430664
Epoch 710, val loss: 0.8429350852966309
Epoch 720, training loss: 12.254720687866211 = 0.2172016054391861 + 2.0 * 6.018759727478027
Epoch 720, val loss: 0.8465709090232849
Epoch 730, training loss: 12.241575241088867 = 0.20690153539180756 + 2.0 * 6.017336845397949
Epoch 730, val loss: 0.8505008816719055
Epoch 740, training loss: 12.238903999328613 = 0.19711457192897797 + 2.0 * 6.020894527435303
Epoch 740, val loss: 0.8546857237815857
Epoch 750, training loss: 12.229887008666992 = 0.18790021538734436 + 2.0 * 6.020993232727051
Epoch 750, val loss: 0.8590545654296875
Epoch 760, training loss: 12.214380264282227 = 0.17925262451171875 + 2.0 * 6.017563819885254
Epoch 760, val loss: 0.864159107208252
Epoch 770, training loss: 12.203396797180176 = 0.17108197510242462 + 2.0 * 6.016157627105713
Epoch 770, val loss: 0.8691816926002502
Epoch 780, training loss: 12.191967964172363 = 0.16336634755134583 + 2.0 * 6.01430082321167
Epoch 780, val loss: 0.8745109438896179
Epoch 790, training loss: 12.195608139038086 = 0.15607066452503204 + 2.0 * 6.019768714904785
Epoch 790, val loss: 0.8799502849578857
Epoch 800, training loss: 12.18056583404541 = 0.14923441410064697 + 2.0 * 6.015665531158447
Epoch 800, val loss: 0.8854517340660095
Epoch 810, training loss: 12.166829109191895 = 0.14278826117515564 + 2.0 * 6.012020587921143
Epoch 810, val loss: 0.8915377855300903
Epoch 820, training loss: 12.159783363342285 = 0.13668809831142426 + 2.0 * 6.011547565460205
Epoch 820, val loss: 0.8975306153297424
Epoch 830, training loss: 12.157759666442871 = 0.13090507686138153 + 2.0 * 6.013427257537842
Epoch 830, val loss: 0.9036585092544556
Epoch 840, training loss: 12.155027389526367 = 0.12547051906585693 + 2.0 * 6.0147786140441895
Epoch 840, val loss: 0.9098331332206726
Epoch 850, training loss: 12.14003849029541 = 0.12036734819412231 + 2.0 * 6.009835720062256
Epoch 850, val loss: 0.9163254499435425
Epoch 860, training loss: 12.134129524230957 = 0.11554291844367981 + 2.0 * 6.009293079376221
Epoch 860, val loss: 0.9227558374404907
Epoch 870, training loss: 12.129518508911133 = 0.11095251142978668 + 2.0 * 6.009283065795898
Epoch 870, val loss: 0.9292057156562805
Epoch 880, training loss: 12.129727363586426 = 0.10659956932067871 + 2.0 * 6.011563777923584
Epoch 880, val loss: 0.9357109069824219
Epoch 890, training loss: 12.117208480834961 = 0.10249599814414978 + 2.0 * 6.0073561668396
Epoch 890, val loss: 0.9424355030059814
Epoch 900, training loss: 12.113909721374512 = 0.09859675168991089 + 2.0 * 6.007656574249268
Epoch 900, val loss: 0.9491869807243347
Epoch 910, training loss: 12.113726615905762 = 0.09488744288682938 + 2.0 * 6.0094194412231445
Epoch 910, val loss: 0.9558123350143433
Epoch 920, training loss: 12.105743408203125 = 0.09138262271881104 + 2.0 * 6.007180213928223
Epoch 920, val loss: 0.9625431895256042
Epoch 930, training loss: 12.099579811096191 = 0.0880485400557518 + 2.0 * 6.005765438079834
Epoch 930, val loss: 0.9694033861160278
Epoch 940, training loss: 12.107476234436035 = 0.08486980199813843 + 2.0 * 6.011303424835205
Epoch 940, val loss: 0.9760927557945251
Epoch 950, training loss: 12.096158027648926 = 0.08185578137636185 + 2.0 * 6.007151126861572
Epoch 950, val loss: 0.9827789068222046
Epoch 960, training loss: 12.088872909545898 = 0.07898993790149689 + 2.0 * 6.004941463470459
Epoch 960, val loss: 0.9897099137306213
Epoch 970, training loss: 12.088662147521973 = 0.07624919712543488 + 2.0 * 6.006206512451172
Epoch 970, val loss: 0.9963799118995667
Epoch 980, training loss: 12.081555366516113 = 0.07364512979984283 + 2.0 * 6.003954887390137
Epoch 980, val loss: 1.0030783414840698
Epoch 990, training loss: 12.0842924118042 = 0.07116471976041794 + 2.0 * 6.006563663482666
Epoch 990, val loss: 1.0098249912261963
Epoch 1000, training loss: 12.076207160949707 = 0.06879736483097076 + 2.0 * 6.003705024719238
Epoch 1000, val loss: 1.0165846347808838
Epoch 1010, training loss: 12.085165023803711 = 0.06654439866542816 + 2.0 * 6.009310245513916
Epoch 1010, val loss: 1.0231527090072632
Epoch 1020, training loss: 12.067612648010254 = 0.06440254300832748 + 2.0 * 6.001605033874512
Epoch 1020, val loss: 1.029823660850525
Epoch 1030, training loss: 12.064362525939941 = 0.06235335022211075 + 2.0 * 6.001004695892334
Epoch 1030, val loss: 1.0365937948226929
Epoch 1040, training loss: 12.060073852539062 = 0.06038500368595123 + 2.0 * 5.999844551086426
Epoch 1040, val loss: 1.043161153793335
Epoch 1050, training loss: 12.058100700378418 = 0.05849352851510048 + 2.0 * 5.99980354309082
Epoch 1050, val loss: 1.0498625040054321
Epoch 1060, training loss: 12.074739456176758 = 0.056682437658309937 + 2.0 * 6.009028434753418
Epoch 1060, val loss: 1.0562379360198975
Epoch 1070, training loss: 12.055316925048828 = 0.054963093250989914 + 2.0 * 6.000176906585693
Epoch 1070, val loss: 1.0626016855239868
Epoch 1080, training loss: 12.051692962646484 = 0.05332433432340622 + 2.0 * 5.9991841316223145
Epoch 1080, val loss: 1.0692710876464844
Epoch 1090, training loss: 12.048412322998047 = 0.05174524337053299 + 2.0 * 5.99833345413208
Epoch 1090, val loss: 1.0756423473358154
Epoch 1100, training loss: 12.050893783569336 = 0.05022299289703369 + 2.0 * 6.000335216522217
Epoch 1100, val loss: 1.0819551944732666
Epoch 1110, training loss: 12.044599533081055 = 0.0487668439745903 + 2.0 * 5.997916221618652
Epoch 1110, val loss: 1.0882772207260132
Epoch 1120, training loss: 12.049809455871582 = 0.04737004265189171 + 2.0 * 6.001219749450684
Epoch 1120, val loss: 1.094596028327942
Epoch 1130, training loss: 12.053482055664062 = 0.04603832587599754 + 2.0 * 6.003721714019775
Epoch 1130, val loss: 1.100687026977539
Epoch 1140, training loss: 12.042071342468262 = 0.044759396463632584 + 2.0 * 5.998655796051025
Epoch 1140, val loss: 1.106886863708496
Epoch 1150, training loss: 12.036134719848633 = 0.04353438317775726 + 2.0 * 5.996300220489502
Epoch 1150, val loss: 1.1131113767623901
Epoch 1160, training loss: 12.03286075592041 = 0.042345575988292694 + 2.0 * 5.995257377624512
Epoch 1160, val loss: 1.1192007064819336
Epoch 1170, training loss: 12.031447410583496 = 0.04119786247611046 + 2.0 * 5.995124816894531
Epoch 1170, val loss: 1.125343918800354
Epoch 1180, training loss: 12.04483699798584 = 0.04009154811501503 + 2.0 * 6.002372741699219
Epoch 1180, val loss: 1.1314687728881836
Epoch 1190, training loss: 12.033937454223633 = 0.03903581202030182 + 2.0 * 5.997450828552246
Epoch 1190, val loss: 1.1371990442276
Epoch 1200, training loss: 12.027143478393555 = 0.03801925852894783 + 2.0 * 5.994562149047852
Epoch 1200, val loss: 1.1433062553405762
Epoch 1210, training loss: 12.035897254943848 = 0.037039902061223984 + 2.0 * 5.999428749084473
Epoch 1210, val loss: 1.1492469310760498
Epoch 1220, training loss: 12.025781631469727 = 0.03610019013285637 + 2.0 * 5.994840621948242
Epoch 1220, val loss: 1.154848337173462
Epoch 1230, training loss: 12.023605346679688 = 0.03519408404827118 + 2.0 * 5.994205474853516
Epoch 1230, val loss: 1.1608307361602783
Epoch 1240, training loss: 12.027414321899414 = 0.03431909903883934 + 2.0 * 5.996547698974609
Epoch 1240, val loss: 1.1665781736373901
Epoch 1250, training loss: 12.020341873168945 = 0.03347461298108101 + 2.0 * 5.993433475494385
Epoch 1250, val loss: 1.1720296144485474
Epoch 1260, training loss: 12.016801834106445 = 0.032662589102983475 + 2.0 * 5.992069721221924
Epoch 1260, val loss: 1.1778335571289062
Epoch 1270, training loss: 12.016427040100098 = 0.03187550604343414 + 2.0 * 5.992275714874268
Epoch 1270, val loss: 1.1835615634918213
Epoch 1280, training loss: 12.031725883483887 = 0.031113097444176674 + 2.0 * 6.000306606292725
Epoch 1280, val loss: 1.1889903545379639
Epoch 1290, training loss: 12.016029357910156 = 0.03038438782095909 + 2.0 * 5.992822647094727
Epoch 1290, val loss: 1.1944711208343506
Epoch 1300, training loss: 12.011616706848145 = 0.02967810444533825 + 2.0 * 5.990969181060791
Epoch 1300, val loss: 1.2001771926879883
Epoch 1310, training loss: 12.01835823059082 = 0.028993533924221992 + 2.0 * 5.994682312011719
Epoch 1310, val loss: 1.2055492401123047
Epoch 1320, training loss: 12.010522842407227 = 0.0283307284116745 + 2.0 * 5.991096019744873
Epoch 1320, val loss: 1.2105958461761475
Epoch 1330, training loss: 12.010880470275879 = 0.02769763395190239 + 2.0 * 5.991591453552246
Epoch 1330, val loss: 1.2160937786102295
Epoch 1340, training loss: 12.007037162780762 = 0.027081144973635674 + 2.0 * 5.989977836608887
Epoch 1340, val loss: 1.2215323448181152
Epoch 1350, training loss: 12.005279541015625 = 0.02648073062300682 + 2.0 * 5.989399433135986
Epoch 1350, val loss: 1.2267634868621826
Epoch 1360, training loss: 12.018030166625977 = 0.025899283587932587 + 2.0 * 5.996065616607666
Epoch 1360, val loss: 1.231960654258728
Epoch 1370, training loss: 12.01133918762207 = 0.025333790108561516 + 2.0 * 5.993002891540527
Epoch 1370, val loss: 1.236859679222107
Epoch 1380, training loss: 12.004457473754883 = 0.02479480393230915 + 2.0 * 5.989831447601318
Epoch 1380, val loss: 1.2423210144042969
Epoch 1390, training loss: 12.00981616973877 = 0.024270260706543922 + 2.0 * 5.992773056030273
Epoch 1390, val loss: 1.2475248575210571
Epoch 1400, training loss: 12.003459930419922 = 0.023758526891469955 + 2.0 * 5.9898505210876465
Epoch 1400, val loss: 1.252269983291626
Epoch 1410, training loss: 12.006546974182129 = 0.02326640672981739 + 2.0 * 5.991640090942383
Epoch 1410, val loss: 1.257407784461975
Epoch 1420, training loss: 11.999923706054688 = 0.022786855697631836 + 2.0 * 5.988568305969238
Epoch 1420, val loss: 1.2621976137161255
Epoch 1430, training loss: 12.000946044921875 = 0.0223237331956625 + 2.0 * 5.989311218261719
Epoch 1430, val loss: 1.2673016786575317
Epoch 1440, training loss: 11.998884201049805 = 0.02187271974980831 + 2.0 * 5.988505840301514
Epoch 1440, val loss: 1.2721121311187744
Epoch 1450, training loss: 11.99715805053711 = 0.021433526650071144 + 2.0 * 5.9878621101379395
Epoch 1450, val loss: 1.2769434452056885
Epoch 1460, training loss: 11.995914459228516 = 0.021007081493735313 + 2.0 * 5.987453460693359
Epoch 1460, val loss: 1.281835913658142
Epoch 1470, training loss: 12.002514839172363 = 0.020592940971255302 + 2.0 * 5.990961074829102
Epoch 1470, val loss: 1.2865020036697388
Epoch 1480, training loss: 12.000231742858887 = 0.02019236609339714 + 2.0 * 5.990019798278809
Epoch 1480, val loss: 1.291192889213562
Epoch 1490, training loss: 11.99722671508789 = 0.019804278388619423 + 2.0 * 5.988711357116699
Epoch 1490, val loss: 1.2958931922912598
Epoch 1500, training loss: 11.99221420288086 = 0.019427241757512093 + 2.0 * 5.986393451690674
Epoch 1500, val loss: 1.3006283044815063
Epoch 1510, training loss: 11.991280555725098 = 0.019060522317886353 + 2.0 * 5.986110210418701
Epoch 1510, val loss: 1.305301547050476
Epoch 1520, training loss: 12.009958267211914 = 0.018702693283557892 + 2.0 * 5.9956278800964355
Epoch 1520, val loss: 1.3096743822097778
Epoch 1530, training loss: 11.992696762084961 = 0.01835540682077408 + 2.0 * 5.987170696258545
Epoch 1530, val loss: 1.3141446113586426
Epoch 1540, training loss: 11.987652778625488 = 0.01802152954041958 + 2.0 * 5.98481559753418
Epoch 1540, val loss: 1.3189010620117188
Epoch 1550, training loss: 11.98686695098877 = 0.01769249513745308 + 2.0 * 5.9845871925354
Epoch 1550, val loss: 1.3233767747879028
Epoch 1560, training loss: 11.998112678527832 = 0.017371665686368942 + 2.0 * 5.990370273590088
Epoch 1560, val loss: 1.327763557434082
Epoch 1570, training loss: 11.994067192077637 = 0.017058612778782845 + 2.0 * 5.988504409790039
Epoch 1570, val loss: 1.331763505935669
Epoch 1580, training loss: 11.98950481414795 = 0.01676016114652157 + 2.0 * 5.986372470855713
Epoch 1580, val loss: 1.3364490270614624
Epoch 1590, training loss: 11.988388061523438 = 0.016466757282614708 + 2.0 * 5.985960483551025
Epoch 1590, val loss: 1.3407671451568604
Epoch 1600, training loss: 11.986875534057617 = 0.016180558130145073 + 2.0 * 5.985347270965576
Epoch 1600, val loss: 1.3448902368545532
Epoch 1610, training loss: 11.983099937438965 = 0.01590248942375183 + 2.0 * 5.983598709106445
Epoch 1610, val loss: 1.349320650100708
Epoch 1620, training loss: 11.982704162597656 = 0.015628866851329803 + 2.0 * 5.983537673950195
Epoch 1620, val loss: 1.3536174297332764
Epoch 1630, training loss: 11.985763549804688 = 0.01536084059625864 + 2.0 * 5.985201358795166
Epoch 1630, val loss: 1.3578107357025146
Epoch 1640, training loss: 11.98435115814209 = 0.015100010670721531 + 2.0 * 5.984625339508057
Epoch 1640, val loss: 1.3618993759155273
Epoch 1650, training loss: 11.988485336303711 = 0.014849036000669003 + 2.0 * 5.986818313598633
Epoch 1650, val loss: 1.3660277128219604
Epoch 1660, training loss: 11.984880447387695 = 0.014605156145989895 + 2.0 * 5.985137462615967
Epoch 1660, val loss: 1.3701122999191284
Epoch 1670, training loss: 11.980133056640625 = 0.014366745948791504 + 2.0 * 5.982882976531982
Epoch 1670, val loss: 1.3743019104003906
Epoch 1680, training loss: 11.977584838867188 = 0.014133184216916561 + 2.0 * 5.981725692749023
Epoch 1680, val loss: 1.3783929347991943
Epoch 1690, training loss: 11.978447914123535 = 0.013903672806918621 + 2.0 * 5.982272148132324
Epoch 1690, val loss: 1.3824721574783325
Epoch 1700, training loss: 11.9899320602417 = 0.013678607530891895 + 2.0 * 5.988126754760742
Epoch 1700, val loss: 1.3863258361816406
Epoch 1710, training loss: 11.983736991882324 = 0.013461610302329063 + 2.0 * 5.985137462615967
Epoch 1710, val loss: 1.3901474475860596
Epoch 1720, training loss: 11.985257148742676 = 0.013251097872853279 + 2.0 * 5.9860029220581055
Epoch 1720, val loss: 1.3942028284072876
Epoch 1730, training loss: 11.975866317749023 = 0.013044318184256554 + 2.0 * 5.981410980224609
Epoch 1730, val loss: 1.3980588912963867
Epoch 1740, training loss: 11.977751731872559 = 0.012843179516494274 + 2.0 * 5.982454299926758
Epoch 1740, val loss: 1.4019697904586792
Epoch 1750, training loss: 11.983297348022461 = 0.012645936571061611 + 2.0 * 5.985325813293457
Epoch 1750, val loss: 1.4056861400604248
Epoch 1760, training loss: 11.975971221923828 = 0.012454484589397907 + 2.0 * 5.9817585945129395
Epoch 1760, val loss: 1.4095383882522583
Epoch 1770, training loss: 11.974034309387207 = 0.012266328558325768 + 2.0 * 5.980884075164795
Epoch 1770, val loss: 1.4134235382080078
Epoch 1780, training loss: 11.984159469604492 = 0.012082014232873917 + 2.0 * 5.986038684844971
Epoch 1780, val loss: 1.4170902967453003
Epoch 1790, training loss: 11.974255561828613 = 0.011901436373591423 + 2.0 * 5.981176853179932
Epoch 1790, val loss: 1.4206786155700684
Epoch 1800, training loss: 11.972996711730957 = 0.011726572178304195 + 2.0 * 5.980635166168213
Epoch 1800, val loss: 1.4245655536651611
Epoch 1810, training loss: 11.98136043548584 = 0.011554017663002014 + 2.0 * 5.984903335571289
Epoch 1810, val loss: 1.4282716512680054
Epoch 1820, training loss: 11.970848083496094 = 0.011385229416191578 + 2.0 * 5.979731559753418
Epoch 1820, val loss: 1.4315850734710693
Epoch 1830, training loss: 11.969833374023438 = 0.011222190223634243 + 2.0 * 5.979305744171143
Epoch 1830, val loss: 1.4354932308197021
Epoch 1840, training loss: 11.969581604003906 = 0.011061282828450203 + 2.0 * 5.979259967803955
Epoch 1840, val loss: 1.4391230344772339
Epoch 1850, training loss: 11.984128952026367 = 0.0109030706807971 + 2.0 * 5.986612796783447
Epoch 1850, val loss: 1.4425081014633179
Epoch 1860, training loss: 11.974974632263184 = 0.01074776891618967 + 2.0 * 5.982113361358643
Epoch 1860, val loss: 1.445920705795288
Epoch 1870, training loss: 11.971320152282715 = 0.01059943437576294 + 2.0 * 5.980360507965088
Epoch 1870, val loss: 1.4496716260910034
Epoch 1880, training loss: 11.971640586853027 = 0.010452084243297577 + 2.0 * 5.980594158172607
Epoch 1880, val loss: 1.4533178806304932
Epoch 1890, training loss: 11.970660209655762 = 0.010307195596396923 + 2.0 * 5.9801764488220215
Epoch 1890, val loss: 1.4565812349319458
Epoch 1900, training loss: 11.968476295471191 = 0.010164961218833923 + 2.0 * 5.979155540466309
Epoch 1900, val loss: 1.4601068496704102
Epoch 1910, training loss: 11.970718383789062 = 0.010025695897638798 + 2.0 * 5.980346202850342
Epoch 1910, val loss: 1.4634891748428345
Epoch 1920, training loss: 11.97241497039795 = 0.009889466688036919 + 2.0 * 5.981262683868408
Epoch 1920, val loss: 1.4668813943862915
Epoch 1930, training loss: 11.975848197937012 = 0.009757298044860363 + 2.0 * 5.98304557800293
Epoch 1930, val loss: 1.4702893495559692
Epoch 1940, training loss: 11.967211723327637 = 0.00962765421718359 + 2.0 * 5.978792190551758
Epoch 1940, val loss: 1.4736087322235107
Epoch 1950, training loss: 11.9655179977417 = 0.00950072705745697 + 2.0 * 5.97800874710083
Epoch 1950, val loss: 1.477169394493103
Epoch 1960, training loss: 11.97970962524414 = 0.00937474612146616 + 2.0 * 5.985167503356934
Epoch 1960, val loss: 1.4802591800689697
Epoch 1970, training loss: 11.965800285339355 = 0.00925257708877325 + 2.0 * 5.978273868560791
Epoch 1970, val loss: 1.4832899570465088
Epoch 1980, training loss: 11.96388053894043 = 0.009133628569543362 + 2.0 * 5.9773736000061035
Epoch 1980, val loss: 1.48683500289917
Epoch 1990, training loss: 11.96285629272461 = 0.009015964344143867 + 2.0 * 5.976920127868652
Epoch 1990, val loss: 1.4901978969573975
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7815
Overall ASR: 0.8229
Flip ASR: 0.7867/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.69357681274414 = 1.9460002183914185 + 2.0 * 8.373787879943848
Epoch 0, val loss: 1.9447215795516968
Epoch 10, training loss: 18.68175506591797 = 1.9359222650527954 + 2.0 * 8.372916221618652
Epoch 10, val loss: 1.93483567237854
Epoch 20, training loss: 18.657699584960938 = 1.9233912229537964 + 2.0 * 8.367154121398926
Epoch 20, val loss: 1.9222227334976196
Epoch 30, training loss: 18.554466247558594 = 1.9072480201721191 + 2.0 * 8.323609352111816
Epoch 30, val loss: 1.9057140350341797
Epoch 40, training loss: 17.765254974365234 = 1.8901937007904053 + 2.0 * 7.937530517578125
Epoch 40, val loss: 1.8884612321853638
Epoch 50, training loss: 15.904424667358398 = 1.8731012344360352 + 2.0 * 7.015661716461182
Epoch 50, val loss: 1.870829463005066
Epoch 60, training loss: 15.308585166931152 = 1.8595449924468994 + 2.0 * 6.724520206451416
Epoch 60, val loss: 1.857817530632019
Epoch 70, training loss: 15.018465995788574 = 1.8475314378738403 + 2.0 * 6.585467338562012
Epoch 70, val loss: 1.8460289239883423
Epoch 80, training loss: 14.822636604309082 = 1.8365848064422607 + 2.0 * 6.493025779724121
Epoch 80, val loss: 1.8352078199386597
Epoch 90, training loss: 14.664217948913574 = 1.8261680603027344 + 2.0 * 6.41902494430542
Epoch 90, val loss: 1.8245996236801147
Epoch 100, training loss: 14.54824161529541 = 1.816529631614685 + 2.0 * 6.365856170654297
Epoch 100, val loss: 1.8146480321884155
Epoch 110, training loss: 14.444822311401367 = 1.8077887296676636 + 2.0 * 6.318516731262207
Epoch 110, val loss: 1.805523157119751
Epoch 120, training loss: 14.35700798034668 = 1.8001066446304321 + 2.0 * 6.2784504890441895
Epoch 120, val loss: 1.7975341081619263
Epoch 130, training loss: 14.281463623046875 = 1.7928681373596191 + 2.0 * 6.244297504425049
Epoch 130, val loss: 1.7900621891021729
Epoch 140, training loss: 14.22104263305664 = 1.7853258848190308 + 2.0 * 6.21785831451416
Epoch 140, val loss: 1.7825607061386108
Epoch 150, training loss: 14.169615745544434 = 1.7770025730133057 + 2.0 * 6.1963067054748535
Epoch 150, val loss: 1.7745767831802368
Epoch 160, training loss: 14.129537582397461 = 1.7675799131393433 + 2.0 * 6.180978775024414
Epoch 160, val loss: 1.7658699750900269
Epoch 170, training loss: 14.092100143432617 = 1.75679612159729 + 2.0 * 6.167652130126953
Epoch 170, val loss: 1.7563031911849976
Epoch 180, training loss: 14.059032440185547 = 1.7444597482681274 + 2.0 * 6.157286167144775
Epoch 180, val loss: 1.745673418045044
Epoch 190, training loss: 14.023660659790039 = 1.7302162647247314 + 2.0 * 6.146722316741943
Epoch 190, val loss: 1.733656406402588
Epoch 200, training loss: 13.988042831420898 = 1.7135590314865112 + 2.0 * 6.137241840362549
Epoch 200, val loss: 1.7196643352508545
Epoch 210, training loss: 13.953773498535156 = 1.6938791275024414 + 2.0 * 6.129947185516357
Epoch 210, val loss: 1.7033592462539673
Epoch 220, training loss: 13.91321849822998 = 1.6707000732421875 + 2.0 * 6.1212592124938965
Epoch 220, val loss: 1.6842199563980103
Epoch 230, training loss: 13.872328758239746 = 1.6431701183319092 + 2.0 * 6.114579200744629
Epoch 230, val loss: 1.6615692377090454
Epoch 240, training loss: 13.833465576171875 = 1.6104713678359985 + 2.0 * 6.111496925354004
Epoch 240, val loss: 1.6347343921661377
Epoch 250, training loss: 13.779911041259766 = 1.5723570585250854 + 2.0 * 6.103776931762695
Epoch 250, val loss: 1.6036529541015625
Epoch 260, training loss: 13.726447105407715 = 1.528467059135437 + 2.0 * 6.098989963531494
Epoch 260, val loss: 1.5680506229400635
Epoch 270, training loss: 13.667398452758789 = 1.4791902303695679 + 2.0 * 6.094104290008545
Epoch 270, val loss: 1.5281474590301514
Epoch 280, training loss: 13.605703353881836 = 1.42521333694458 + 2.0 * 6.090244770050049
Epoch 280, val loss: 1.4847155809402466
Epoch 290, training loss: 13.558186531066895 = 1.3683913946151733 + 2.0 * 6.094897747039795
Epoch 290, val loss: 1.4394569396972656
Epoch 300, training loss: 13.482087135314941 = 1.3125935792922974 + 2.0 * 6.084746837615967
Epoch 300, val loss: 1.3953715562820435
Epoch 310, training loss: 13.421096801757812 = 1.2582284212112427 + 2.0 * 6.08143424987793
Epoch 310, val loss: 1.3526017665863037
Epoch 320, training loss: 13.361306190490723 = 1.2056688070297241 + 2.0 * 6.077818870544434
Epoch 320, val loss: 1.311618447303772
Epoch 330, training loss: 13.30466079711914 = 1.15506112575531 + 2.0 * 6.07480001449585
Epoch 330, val loss: 1.2726306915283203
Epoch 340, training loss: 13.25342082977295 = 1.1066925525665283 + 2.0 * 6.0733642578125
Epoch 340, val loss: 1.2357045412063599
Epoch 350, training loss: 13.205933570861816 = 1.0613619089126587 + 2.0 * 6.0722856521606445
Epoch 350, val loss: 1.2014869451522827
Epoch 360, training loss: 13.15306282043457 = 1.0179433822631836 + 2.0 * 6.067559719085693
Epoch 360, val loss: 1.1690818071365356
Epoch 370, training loss: 13.106587409973145 = 0.9761079549789429 + 2.0 * 6.065239906311035
Epoch 370, val loss: 1.137895107269287
Epoch 380, training loss: 13.060848236083984 = 0.9359162449836731 + 2.0 * 6.062466144561768
Epoch 380, val loss: 1.1081172227859497
Epoch 390, training loss: 13.02035140991211 = 0.8974318504333496 + 2.0 * 6.061459541320801
Epoch 390, val loss: 1.079821228981018
Epoch 400, training loss: 12.980063438415527 = 0.8608202338218689 + 2.0 * 6.059621810913086
Epoch 400, val loss: 1.052979588508606
Epoch 410, training loss: 12.943338394165039 = 0.8264787197113037 + 2.0 * 6.058429718017578
Epoch 410, val loss: 1.0279669761657715
Epoch 420, training loss: 12.902962684631348 = 0.7938191294670105 + 2.0 * 6.054571628570557
Epoch 420, val loss: 1.0044485330581665
Epoch 430, training loss: 12.893049240112305 = 0.7627856135368347 + 2.0 * 6.065131664276123
Epoch 430, val loss: 0.9822715520858765
Epoch 440, training loss: 12.84373664855957 = 0.7337275743484497 + 2.0 * 6.055004596710205
Epoch 440, val loss: 0.9619879722595215
Epoch 450, training loss: 12.805243492126465 = 0.7064083218574524 + 2.0 * 6.049417495727539
Epoch 450, val loss: 0.9433718919754028
Epoch 460, training loss: 12.775824546813965 = 0.6805078983306885 + 2.0 * 6.047658443450928
Epoch 460, val loss: 0.9260630011558533
Epoch 470, training loss: 12.748414039611816 = 0.655805766582489 + 2.0 * 6.046304225921631
Epoch 470, val loss: 0.9100612998008728
Epoch 480, training loss: 12.722914695739746 = 0.6322479248046875 + 2.0 * 6.045333385467529
Epoch 480, val loss: 0.8953431248664856
Epoch 490, training loss: 12.702713012695312 = 0.6097422242164612 + 2.0 * 6.046485424041748
Epoch 490, val loss: 0.8819449543952942
Epoch 500, training loss: 12.672880172729492 = 0.5881810188293457 + 2.0 * 6.042349815368652
Epoch 500, val loss: 0.8695024847984314
Epoch 510, training loss: 12.646100044250488 = 0.5671500563621521 + 2.0 * 6.039474964141846
Epoch 510, val loss: 0.8581026196479797
Epoch 520, training loss: 12.64154052734375 = 0.5465441346168518 + 2.0 * 6.0474982261657715
Epoch 520, val loss: 0.847417414188385
Epoch 530, training loss: 12.60181713104248 = 0.5264475345611572 + 2.0 * 6.037684917449951
Epoch 530, val loss: 0.8375726938247681
Epoch 540, training loss: 12.579227447509766 = 0.5066688656806946 + 2.0 * 6.036279201507568
Epoch 540, val loss: 0.8285673260688782
Epoch 550, training loss: 12.563592910766602 = 0.48703116178512573 + 2.0 * 6.038280963897705
Epoch 550, val loss: 0.8201764822006226
Epoch 560, training loss: 12.541767120361328 = 0.4677848219871521 + 2.0 * 6.036991119384766
Epoch 560, val loss: 0.8123705387115479
Epoch 570, training loss: 12.512894630432129 = 0.4487326145172119 + 2.0 * 6.032081127166748
Epoch 570, val loss: 0.8053921461105347
Epoch 580, training loss: 12.491023063659668 = 0.42987117171287537 + 2.0 * 6.030575752258301
Epoch 580, val loss: 0.7989605665206909
Epoch 590, training loss: 12.492498397827148 = 0.4111931622028351 + 2.0 * 6.040652751922607
Epoch 590, val loss: 0.7930002212524414
Epoch 600, training loss: 12.457148551940918 = 0.3931032419204712 + 2.0 * 6.032022476196289
Epoch 600, val loss: 0.7876265645027161
Epoch 610, training loss: 12.433394432067871 = 0.37544548511505127 + 2.0 * 6.028974533081055
Epoch 610, val loss: 0.7829297780990601
Epoch 620, training loss: 12.4116849899292 = 0.3581809103488922 + 2.0 * 6.02675199508667
Epoch 620, val loss: 0.7786012887954712
Epoch 630, training loss: 12.39578628540039 = 0.3413124978542328 + 2.0 * 6.0272369384765625
Epoch 630, val loss: 0.7747137546539307
Epoch 640, training loss: 12.37636947631836 = 0.32497578859329224 + 2.0 * 6.025696754455566
Epoch 640, val loss: 0.7713066935539246
Epoch 650, training loss: 12.361505508422852 = 0.3092837333679199 + 2.0 * 6.026110649108887
Epoch 650, val loss: 0.7684500813484192
Epoch 660, training loss: 12.342188835144043 = 0.2942377030849457 + 2.0 * 6.023975372314453
Epoch 660, val loss: 0.7658829689025879
Epoch 670, training loss: 12.331524848937988 = 0.2798602879047394 + 2.0 * 6.025832176208496
Epoch 670, val loss: 0.7638125419616699
Epoch 680, training loss: 12.315893173217773 = 0.2662140429019928 + 2.0 * 6.024839401245117
Epoch 680, val loss: 0.7621863484382629
Epoch 690, training loss: 12.297895431518555 = 0.25336775183677673 + 2.0 * 6.022264003753662
Epoch 690, val loss: 0.7609378099441528
Epoch 700, training loss: 12.281838417053223 = 0.24120162427425385 + 2.0 * 6.020318508148193
Epoch 700, val loss: 0.7602186799049377
Epoch 710, training loss: 12.26815414428711 = 0.22963947057724 + 2.0 * 6.019257545471191
Epoch 710, val loss: 0.7598057985305786
Epoch 720, training loss: 12.273749351501465 = 0.21871867775917053 + 2.0 * 6.027515411376953
Epoch 720, val loss: 0.7598641514778137
Epoch 730, training loss: 12.247674942016602 = 0.20852506160736084 + 2.0 * 6.019575119018555
Epoch 730, val loss: 0.760305643081665
Epoch 740, training loss: 12.235505104064941 = 0.19892863929271698 + 2.0 * 6.0182881355285645
Epoch 740, val loss: 0.7612358331680298
Epoch 750, training loss: 12.227871894836426 = 0.18993975222110748 + 2.0 * 6.018966197967529
Epoch 750, val loss: 0.762343168258667
Epoch 760, training loss: 12.211398124694824 = 0.18150635063648224 + 2.0 * 6.014945983886719
Epoch 760, val loss: 0.763946533203125
Epoch 770, training loss: 12.212248802185059 = 0.17356838285923004 + 2.0 * 6.0193400382995605
Epoch 770, val loss: 0.7658399939537048
Epoch 780, training loss: 12.197734832763672 = 0.16606886684894562 + 2.0 * 6.015832901000977
Epoch 780, val loss: 0.767842710018158
Epoch 790, training loss: 12.18651008605957 = 0.15904437005519867 + 2.0 * 6.01373291015625
Epoch 790, val loss: 0.7703863382339478
Epoch 800, training loss: 12.179901123046875 = 0.1523561328649521 + 2.0 * 6.013772487640381
Epoch 800, val loss: 0.7730721831321716
Epoch 810, training loss: 12.169939994812012 = 0.1460544914007187 + 2.0 * 6.0119428634643555
Epoch 810, val loss: 0.775750458240509
Epoch 820, training loss: 12.168112754821777 = 0.14013256132602692 + 2.0 * 6.0139899253845215
Epoch 820, val loss: 0.7789502739906311
Epoch 830, training loss: 12.161932945251465 = 0.1345086693763733 + 2.0 * 6.013711929321289
Epoch 830, val loss: 0.782191812992096
Epoch 840, training loss: 12.150906562805176 = 0.1291896253824234 + 2.0 * 6.010858535766602
Epoch 840, val loss: 0.7856030464172363
Epoch 850, training loss: 12.14212417602539 = 0.12412279844284058 + 2.0 * 6.009000778198242
Epoch 850, val loss: 0.789212703704834
Epoch 860, training loss: 12.157137870788574 = 0.11929905414581299 + 2.0 * 6.018919467926025
Epoch 860, val loss: 0.7929388284683228
Epoch 870, training loss: 12.13916015625 = 0.11469981074333191 + 2.0 * 6.012230396270752
Epoch 870, val loss: 0.7966787815093994
Epoch 880, training loss: 12.125079154968262 = 0.11035685241222382 + 2.0 * 6.007360935211182
Epoch 880, val loss: 0.8007882237434387
Epoch 890, training loss: 12.120626449584961 = 0.10619595646858215 + 2.0 * 6.0072150230407715
Epoch 890, val loss: 0.8048199415206909
Epoch 900, training loss: 12.130002975463867 = 0.1022077426314354 + 2.0 * 6.01389741897583
Epoch 900, val loss: 0.808960497379303
Epoch 910, training loss: 12.120323181152344 = 0.0984160304069519 + 2.0 * 6.010953426361084
Epoch 910, val loss: 0.8130099773406982
Epoch 920, training loss: 12.107199668884277 = 0.09483399242162704 + 2.0 * 6.006182670593262
Epoch 920, val loss: 0.8174707889556885
Epoch 930, training loss: 12.102474212646484 = 0.09138844907283783 + 2.0 * 6.005542755126953
Epoch 930, val loss: 0.8218538761138916
Epoch 940, training loss: 12.103480339050293 = 0.08809048682451248 + 2.0 * 6.007694721221924
Epoch 940, val loss: 0.82621830701828
Epoch 950, training loss: 12.10852336883545 = 0.08494031429290771 + 2.0 * 6.011791706085205
Epoch 950, val loss: 0.8306487202644348
Epoch 960, training loss: 12.09211254119873 = 0.0819653570652008 + 2.0 * 6.005073547363281
Epoch 960, val loss: 0.8351365327835083
Epoch 970, training loss: 12.085153579711914 = 0.07911253720521927 + 2.0 * 6.003020286560059
Epoch 970, val loss: 0.8397700190544128
Epoch 980, training loss: 12.081245422363281 = 0.07637887448072433 + 2.0 * 6.0024333000183105
Epoch 980, val loss: 0.8443472385406494
Epoch 990, training loss: 12.093153953552246 = 0.07376895844936371 + 2.0 * 6.009692668914795
Epoch 990, val loss: 0.8488842248916626
Epoch 1000, training loss: 12.080179214477539 = 0.07127241790294647 + 2.0 * 6.004453182220459
Epoch 1000, val loss: 0.8534040451049805
Epoch 1010, training loss: 12.078432083129883 = 0.068910151720047 + 2.0 * 6.0047607421875
Epoch 1010, val loss: 0.8581798076629639
Epoch 1020, training loss: 12.068438529968262 = 0.06664879620075226 + 2.0 * 6.000895023345947
Epoch 1020, val loss: 0.8627013564109802
Epoch 1030, training loss: 12.065184593200684 = 0.06449344009160995 + 2.0 * 6.000345706939697
Epoch 1030, val loss: 0.8673815727233887
Epoch 1040, training loss: 12.062180519104004 = 0.062429580837488174 + 2.0 * 5.999875545501709
Epoch 1040, val loss: 0.872124195098877
Epoch 1050, training loss: 12.074816703796387 = 0.06044326350092888 + 2.0 * 6.0071868896484375
Epoch 1050, val loss: 0.87663733959198
Epoch 1060, training loss: 12.066417694091797 = 0.05856645852327347 + 2.0 * 6.003925800323486
Epoch 1060, val loss: 0.8813425898551941
Epoch 1070, training loss: 12.058319091796875 = 0.05676979944109917 + 2.0 * 6.00077486038208
Epoch 1070, val loss: 0.886023998260498
Epoch 1080, training loss: 12.052720069885254 = 0.055048827081918716 + 2.0 * 5.998835563659668
Epoch 1080, val loss: 0.8906193375587463
Epoch 1090, training loss: 12.049924850463867 = 0.053396835923194885 + 2.0 * 5.998263835906982
Epoch 1090, val loss: 0.8953235745429993
Epoch 1100, training loss: 12.056755065917969 = 0.051805395632982254 + 2.0 * 6.002474784851074
Epoch 1100, val loss: 0.8998901844024658
Epoch 1110, training loss: 12.0535306930542 = 0.05029267445206642 + 2.0 * 6.0016188621521
Epoch 1110, val loss: 0.9044144153594971
Epoch 1120, training loss: 12.0476655960083 = 0.048846419900655746 + 2.0 * 5.9994096755981445
Epoch 1120, val loss: 0.9090590476989746
Epoch 1130, training loss: 12.047829627990723 = 0.047460298985242844 + 2.0 * 6.000184535980225
Epoch 1130, val loss: 0.9135753512382507
Epoch 1140, training loss: 12.03956413269043 = 0.04612966999411583 + 2.0 * 5.99671745300293
Epoch 1140, val loss: 0.9181245565414429
Epoch 1150, training loss: 12.036696434020996 = 0.04484754428267479 + 2.0 * 5.995924472808838
Epoch 1150, val loss: 0.922663152217865
Epoch 1160, training loss: 12.041753768920898 = 0.04360843822360039 + 2.0 * 5.999072551727295
Epoch 1160, val loss: 0.9271115064620972
Epoch 1170, training loss: 12.039173126220703 = 0.04241512343287468 + 2.0 * 5.998379230499268
Epoch 1170, val loss: 0.9314682483673096
Epoch 1180, training loss: 12.035006523132324 = 0.04128555580973625 + 2.0 * 5.996860504150391
Epoch 1180, val loss: 0.9359039068222046
Epoch 1190, training loss: 12.032403945922852 = 0.04019837826490402 + 2.0 * 5.996102809906006
Epoch 1190, val loss: 0.9403777122497559
Epoch 1200, training loss: 12.032760620117188 = 0.039149679243564606 + 2.0 * 5.996805667877197
Epoch 1200, val loss: 0.9447413086891174
Epoch 1210, training loss: 12.02621841430664 = 0.038136351853609085 + 2.0 * 5.9940409660339355
Epoch 1210, val loss: 0.9489468336105347
Epoch 1220, training loss: 12.024406433105469 = 0.03715665265917778 + 2.0 * 5.993624687194824
Epoch 1220, val loss: 0.9533032178878784
Epoch 1230, training loss: 12.024617195129395 = 0.036208346486091614 + 2.0 * 5.994204521179199
Epoch 1230, val loss: 0.9575925469398499
Epoch 1240, training loss: 12.026200294494629 = 0.0352928601205349 + 2.0 * 5.995453834533691
Epoch 1240, val loss: 0.9617859721183777
Epoch 1250, training loss: 12.028678894042969 = 0.03441572189331055 + 2.0 * 5.997131824493408
Epoch 1250, val loss: 0.9659969806671143
Epoch 1260, training loss: 12.028277397155762 = 0.03357036039233208 + 2.0 * 5.997353553771973
Epoch 1260, val loss: 0.9701075553894043
Epoch 1270, training loss: 12.022555351257324 = 0.03276203200221062 + 2.0 * 5.99489688873291
Epoch 1270, val loss: 0.9742295742034912
Epoch 1280, training loss: 12.016090393066406 = 0.031986285001039505 + 2.0 * 5.99205207824707
Epoch 1280, val loss: 0.9783297777175903
Epoch 1290, training loss: 12.01421070098877 = 0.031234629452228546 + 2.0 * 5.991487979888916
Epoch 1290, val loss: 0.98249351978302
Epoch 1300, training loss: 12.014162063598633 = 0.030499331653118134 + 2.0 * 5.991831302642822
Epoch 1300, val loss: 0.9864553809165955
Epoch 1310, training loss: 12.023126602172852 = 0.02978820912539959 + 2.0 * 5.996669292449951
Epoch 1310, val loss: 0.9903777241706848
Epoch 1320, training loss: 12.014147758483887 = 0.029103867709636688 + 2.0 * 5.9925217628479
Epoch 1320, val loss: 0.99432373046875
Epoch 1330, training loss: 12.012772560119629 = 0.028441576287150383 + 2.0 * 5.992165565490723
Epoch 1330, val loss: 0.9983465075492859
Epoch 1340, training loss: 12.012308120727539 = 0.027799459174275398 + 2.0 * 5.992254257202148
Epoch 1340, val loss: 1.0021491050720215
Epoch 1350, training loss: 12.018630981445312 = 0.027177926152944565 + 2.0 * 5.995726585388184
Epoch 1350, val loss: 1.0060032606124878
Epoch 1360, training loss: 12.01262092590332 = 0.026585392653942108 + 2.0 * 5.993017673492432
Epoch 1360, val loss: 1.0097980499267578
Epoch 1370, training loss: 12.005427360534668 = 0.02600998617708683 + 2.0 * 5.98970890045166
Epoch 1370, val loss: 1.013609766960144
Epoch 1380, training loss: 12.003913879394531 = 0.025453491136431694 + 2.0 * 5.989230155944824
Epoch 1380, val loss: 1.0174401998519897
Epoch 1390, training loss: 12.005049705505371 = 0.024907443672418594 + 2.0 * 5.9900712966918945
Epoch 1390, val loss: 1.021117925643921
Epoch 1400, training loss: 12.007328987121582 = 0.024378452450037003 + 2.0 * 5.9914751052856445
Epoch 1400, val loss: 1.024794340133667
Epoch 1410, training loss: 12.006893157958984 = 0.02386477030813694 + 2.0 * 5.991514205932617
Epoch 1410, val loss: 1.0284955501556396
Epoch 1420, training loss: 12.000640869140625 = 0.023371156305074692 + 2.0 * 5.988635063171387
Epoch 1420, val loss: 1.0321993827819824
Epoch 1430, training loss: 12.00109577178955 = 0.0228910930454731 + 2.0 * 5.989102363586426
Epoch 1430, val loss: 1.035908818244934
Epoch 1440, training loss: 12.00281047821045 = 0.022424176335334778 + 2.0 * 5.9901933670043945
Epoch 1440, val loss: 1.0394231081008911
Epoch 1450, training loss: 11.998881340026855 = 0.021971074864268303 + 2.0 * 5.988455295562744
Epoch 1450, val loss: 1.0429582595825195
Epoch 1460, training loss: 12.006922721862793 = 0.02153122052550316 + 2.0 * 5.9926958084106445
Epoch 1460, val loss: 1.0465936660766602
Epoch 1470, training loss: 11.994636535644531 = 0.021109435707330704 + 2.0 * 5.9867634773254395
Epoch 1470, val loss: 1.0499484539031982
Epoch 1480, training loss: 11.994426727294922 = 0.020700648427009583 + 2.0 * 5.986863136291504
Epoch 1480, val loss: 1.0535529851913452
Epoch 1490, training loss: 12.001123428344727 = 0.020303262397646904 + 2.0 * 5.990409851074219
Epoch 1490, val loss: 1.0570108890533447
Epoch 1500, training loss: 11.992652893066406 = 0.019915150478482246 + 2.0 * 5.986368656158447
Epoch 1500, val loss: 1.0603147745132446
Epoch 1510, training loss: 11.99000072479248 = 0.01953791081905365 + 2.0 * 5.985231399536133
Epoch 1510, val loss: 1.0638362169265747
Epoch 1520, training loss: 11.995640754699707 = 0.01916837878525257 + 2.0 * 5.988235950469971
Epoch 1520, val loss: 1.06720769405365
Epoch 1530, training loss: 11.992082595825195 = 0.01880876161158085 + 2.0 * 5.986637115478516
Epoch 1530, val loss: 1.0703356266021729
Epoch 1540, training loss: 11.988687515258789 = 0.018463846296072006 + 2.0 * 5.985111713409424
Epoch 1540, val loss: 1.0738019943237305
Epoch 1550, training loss: 11.991786003112793 = 0.018127035349607468 + 2.0 * 5.9868292808532715
Epoch 1550, val loss: 1.0772675275802612
Epoch 1560, training loss: 11.989760398864746 = 0.017796775326132774 + 2.0 * 5.9859819412231445
Epoch 1560, val loss: 1.0802677869796753
Epoch 1570, training loss: 11.989277839660645 = 0.017477812245488167 + 2.0 * 5.985899925231934
Epoch 1570, val loss: 1.0834952592849731
Epoch 1580, training loss: 11.984430313110352 = 0.017169104889035225 + 2.0 * 5.983630657196045
Epoch 1580, val loss: 1.086851954460144
Epoch 1590, training loss: 11.985085487365723 = 0.01686507649719715 + 2.0 * 5.984110355377197
Epoch 1590, val loss: 1.0900332927703857
Epoch 1600, training loss: 11.996270179748535 = 0.01656670682132244 + 2.0 * 5.989851951599121
Epoch 1600, val loss: 1.0930849313735962
Epoch 1610, training loss: 12.002573013305664 = 0.01627604104578495 + 2.0 * 5.993148326873779
Epoch 1610, val loss: 1.0960867404937744
Epoch 1620, training loss: 11.98680591583252 = 0.016002969816327095 + 2.0 * 5.985401630401611
Epoch 1620, val loss: 1.0992600917816162
Epoch 1630, training loss: 11.981542587280273 = 0.015732578933238983 + 2.0 * 5.98290491104126
Epoch 1630, val loss: 1.1025214195251465
Epoch 1640, training loss: 11.980117797851562 = 0.015466840006411076 + 2.0 * 5.982325553894043
Epoch 1640, val loss: 1.1055346727371216
Epoch 1650, training loss: 11.979043960571289 = 0.015204605646431446 + 2.0 * 5.981919765472412
Epoch 1650, val loss: 1.1085259914398193
Epoch 1660, training loss: 11.982317924499512 = 0.014947071671485901 + 2.0 * 5.983685493469238
Epoch 1660, val loss: 1.1116081476211548
Epoch 1670, training loss: 11.98239517211914 = 0.014696587808430195 + 2.0 * 5.98384952545166
Epoch 1670, val loss: 1.114295482635498
Epoch 1680, training loss: 11.987313270568848 = 0.014460409991443157 + 2.0 * 5.98642635345459
Epoch 1680, val loss: 1.1173940896987915
Epoch 1690, training loss: 11.97801685333252 = 0.014229991473257542 + 2.0 * 5.981893539428711
Epoch 1690, val loss: 1.1205854415893555
Epoch 1700, training loss: 11.979548454284668 = 0.014002704061567783 + 2.0 * 5.9827728271484375
Epoch 1700, val loss: 1.1234697103500366
Epoch 1710, training loss: 11.984099388122559 = 0.013778181746602058 + 2.0 * 5.985160827636719
Epoch 1710, val loss: 1.1261544227600098
Epoch 1720, training loss: 11.98101806640625 = 0.013558627106249332 + 2.0 * 5.983729839324951
Epoch 1720, val loss: 1.1289582252502441
Epoch 1730, training loss: 11.975269317626953 = 0.013345244340598583 + 2.0 * 5.980961799621582
Epoch 1730, val loss: 1.1318849325180054
Epoch 1740, training loss: 11.977718353271484 = 0.013137595728039742 + 2.0 * 5.982290267944336
Epoch 1740, val loss: 1.1348819732666016
Epoch 1750, training loss: 11.984371185302734 = 0.012933786027133465 + 2.0 * 5.985718727111816
Epoch 1750, val loss: 1.1376534700393677
Epoch 1760, training loss: 11.984379768371582 = 0.012734665535390377 + 2.0 * 5.985822677612305
Epoch 1760, val loss: 1.140142560005188
Epoch 1770, training loss: 11.976533889770508 = 0.01254205871373415 + 2.0 * 5.981996059417725
Epoch 1770, val loss: 1.14307701587677
Epoch 1780, training loss: 11.972844123840332 = 0.01235557347536087 + 2.0 * 5.980244159698486
Epoch 1780, val loss: 1.1459604501724243
Epoch 1790, training loss: 11.971707344055176 = 0.012168514542281628 + 2.0 * 5.979769229888916
Epoch 1790, val loss: 1.1486064195632935
Epoch 1800, training loss: 11.984085083007812 = 0.011985255405306816 + 2.0 * 5.986050128936768
Epoch 1800, val loss: 1.1512095928192139
Epoch 1810, training loss: 11.972994804382324 = 0.011806497350335121 + 2.0 * 5.980594158172607
Epoch 1810, val loss: 1.1537201404571533
Epoch 1820, training loss: 11.972503662109375 = 0.011635462753474712 + 2.0 * 5.980433940887451
Epoch 1820, val loss: 1.156602144241333
Epoch 1830, training loss: 11.969971656799316 = 0.011467532254755497 + 2.0 * 5.979251861572266
Epoch 1830, val loss: 1.1594314575195312
Epoch 1840, training loss: 11.969099044799805 = 0.011300239711999893 + 2.0 * 5.9788994789123535
Epoch 1840, val loss: 1.1619099378585815
Epoch 1850, training loss: 11.985380172729492 = 0.011135444976389408 + 2.0 * 5.987122535705566
Epoch 1850, val loss: 1.1644115447998047
Epoch 1860, training loss: 11.974774360656738 = 0.01097586378455162 + 2.0 * 5.981899261474609
Epoch 1860, val loss: 1.1669458150863647
Epoch 1870, training loss: 11.971272468566895 = 0.010822141543030739 + 2.0 * 5.980225086212158
Epoch 1870, val loss: 1.16964590549469
Epoch 1880, training loss: 11.978196144104004 = 0.010670454241335392 + 2.0 * 5.983762741088867
Epoch 1880, val loss: 1.1722848415374756
Epoch 1890, training loss: 11.966347694396973 = 0.010523127391934395 + 2.0 * 5.977912425994873
Epoch 1890, val loss: 1.1746516227722168
Epoch 1900, training loss: 11.96606731414795 = 0.010379490442574024 + 2.0 * 5.977843761444092
Epoch 1900, val loss: 1.1772979497909546
Epoch 1910, training loss: 11.966362953186035 = 0.010236975736916065 + 2.0 * 5.978063106536865
Epoch 1910, val loss: 1.1798938512802124
Epoch 1920, training loss: 11.974563598632812 = 0.010096695274114609 + 2.0 * 5.98223352432251
Epoch 1920, val loss: 1.1822978258132935
Epoch 1930, training loss: 11.965720176696777 = 0.009958557784557343 + 2.0 * 5.977880954742432
Epoch 1930, val loss: 1.184693455696106
Epoch 1940, training loss: 11.967642784118652 = 0.009825687855482101 + 2.0 * 5.978908538818359
Epoch 1940, val loss: 1.1874046325683594
Epoch 1950, training loss: 11.973584175109863 = 0.009694086387753487 + 2.0 * 5.981945037841797
Epoch 1950, val loss: 1.1897644996643066
Epoch 1960, training loss: 11.96436595916748 = 0.009564263746142387 + 2.0 * 5.977400779724121
Epoch 1960, val loss: 1.1921641826629639
Epoch 1970, training loss: 11.962525367736816 = 0.009439253248274326 + 2.0 * 5.976542949676514
Epoch 1970, val loss: 1.194734811782837
Epoch 1980, training loss: 11.974480628967285 = 0.009314882569015026 + 2.0 * 5.982583045959473
Epoch 1980, val loss: 1.1970535516738892
Epoch 1990, training loss: 11.967758178710938 = 0.009192810393869877 + 2.0 * 5.979282855987549
Epoch 1990, val loss: 1.1991750001907349
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8148
Overall ASR: 0.7122
Flip ASR: 0.6578/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.679767608642578 = 1.932194471359253 + 2.0 * 8.373786926269531
Epoch 0, val loss: 1.926106572151184
Epoch 10, training loss: 18.6685791015625 = 1.9225527048110962 + 2.0 * 8.373013496398926
Epoch 10, val loss: 1.9173719882965088
Epoch 20, training loss: 18.64577293395996 = 1.9108754396438599 + 2.0 * 8.367448806762695
Epoch 20, val loss: 1.9065213203430176
Epoch 30, training loss: 18.550922393798828 = 1.8955954313278198 + 2.0 * 8.32766342163086
Epoch 30, val loss: 1.892128825187683
Epoch 40, training loss: 18.009687423706055 = 1.8772965669631958 + 2.0 * 8.066195487976074
Epoch 40, val loss: 1.875220775604248
Epoch 50, training loss: 16.49260902404785 = 1.8563052415847778 + 2.0 * 7.318151473999023
Epoch 50, val loss: 1.8560281991958618
Epoch 60, training loss: 15.81003189086914 = 1.8412373065948486 + 2.0 * 6.9843974113464355
Epoch 60, val loss: 1.8429327011108398
Epoch 70, training loss: 15.374931335449219 = 1.8324978351593018 + 2.0 * 6.771216869354248
Epoch 70, val loss: 1.8349461555480957
Epoch 80, training loss: 15.096223831176758 = 1.823294758796692 + 2.0 * 6.636464595794678
Epoch 80, val loss: 1.8262760639190674
Epoch 90, training loss: 14.899070739746094 = 1.8143880367279053 + 2.0 * 6.542341232299805
Epoch 90, val loss: 1.8170841932296753
Epoch 100, training loss: 14.727286338806152 = 1.8059775829315186 + 2.0 * 6.460654258728027
Epoch 100, val loss: 1.8084220886230469
Epoch 110, training loss: 14.598577499389648 = 1.7991715669631958 + 2.0 * 6.399703025817871
Epoch 110, val loss: 1.8012449741363525
Epoch 120, training loss: 14.466229438781738 = 1.793430209159851 + 2.0 * 6.336399555206299
Epoch 120, val loss: 1.7951021194458008
Epoch 130, training loss: 14.369314193725586 = 1.7880622148513794 + 2.0 * 6.290626049041748
Epoch 130, val loss: 1.7891781330108643
Epoch 140, training loss: 14.291881561279297 = 1.7815810441970825 + 2.0 * 6.255150318145752
Epoch 140, val loss: 1.7824475765228271
Epoch 150, training loss: 14.231413841247559 = 1.7740494012832642 + 2.0 * 6.228682041168213
Epoch 150, val loss: 1.7749881744384766
Epoch 160, training loss: 14.181597709655762 = 1.765661358833313 + 2.0 * 6.207968235015869
Epoch 160, val loss: 1.7671600580215454
Epoch 170, training loss: 14.137807846069336 = 1.756331205368042 + 2.0 * 6.190738201141357
Epoch 170, val loss: 1.7588363885879517
Epoch 180, training loss: 14.100685119628906 = 1.7456046342849731 + 2.0 * 6.177540302276611
Epoch 180, val loss: 1.7496428489685059
Epoch 190, training loss: 14.063373565673828 = 1.7331793308258057 + 2.0 * 6.165097236633301
Epoch 190, val loss: 1.7391400337219238
Epoch 200, training loss: 14.02797794342041 = 1.7186403274536133 + 2.0 * 6.154668807983398
Epoch 200, val loss: 1.7271597385406494
Epoch 210, training loss: 13.999326705932617 = 1.7017059326171875 + 2.0 * 6.148810386657715
Epoch 210, val loss: 1.7133897542953491
Epoch 220, training loss: 13.9563627243042 = 1.6820858716964722 + 2.0 * 6.137138366699219
Epoch 220, val loss: 1.6975804567337036
Epoch 230, training loss: 13.916556358337402 = 1.6591907739639282 + 2.0 * 6.128682613372803
Epoch 230, val loss: 1.6791667938232422
Epoch 240, training loss: 13.874733924865723 = 1.6322641372680664 + 2.0 * 6.121234893798828
Epoch 240, val loss: 1.6575337648391724
Epoch 250, training loss: 13.836237907409668 = 1.600536584854126 + 2.0 * 6.1178507804870605
Epoch 250, val loss: 1.6321802139282227
Epoch 260, training loss: 13.783327102661133 = 1.5642212629318237 + 2.0 * 6.10955286026001
Epoch 260, val loss: 1.602786898612976
Epoch 270, training loss: 13.72983169555664 = 1.5227433443069458 + 2.0 * 6.103544235229492
Epoch 270, val loss: 1.56925630569458
Epoch 280, training loss: 13.672541618347168 = 1.4762065410614014 + 2.0 * 6.098167419433594
Epoch 280, val loss: 1.5315752029418945
Epoch 290, training loss: 13.614079475402832 = 1.4254308938980103 + 2.0 * 6.094324111938477
Epoch 290, val loss: 1.4905883073806763
Epoch 300, training loss: 13.552651405334473 = 1.3717237710952759 + 2.0 * 6.090463638305664
Epoch 300, val loss: 1.44734787940979
Epoch 310, training loss: 13.489646911621094 = 1.3157429695129395 + 2.0 * 6.086951732635498
Epoch 310, val loss: 1.4024792909622192
Epoch 320, training loss: 13.431451797485352 = 1.2587921619415283 + 2.0 * 6.086329936981201
Epoch 320, val loss: 1.357157826423645
Epoch 330, training loss: 13.366636276245117 = 1.2025222778320312 + 2.0 * 6.082056999206543
Epoch 330, val loss: 1.3127509355545044
Epoch 340, training loss: 13.32303524017334 = 1.1475322246551514 + 2.0 * 6.087751388549805
Epoch 340, val loss: 1.2697761058807373
Epoch 350, training loss: 13.251557350158691 = 1.0950814485549927 + 2.0 * 6.078238010406494
Epoch 350, val loss: 1.229575753211975
Epoch 360, training loss: 13.193416595458984 = 1.0449150800704956 + 2.0 * 6.0742506980896
Epoch 360, val loss: 1.1914094686508179
Epoch 370, training loss: 13.140238761901855 = 0.9967963099479675 + 2.0 * 6.071721076965332
Epoch 370, val loss: 1.155080795288086
Epoch 380, training loss: 13.090116500854492 = 0.9505890011787415 + 2.0 * 6.069763660430908
Epoch 380, val loss: 1.1206132173538208
Epoch 390, training loss: 13.050413131713867 = 0.90705406665802 + 2.0 * 6.071679592132568
Epoch 390, val loss: 1.088342547416687
Epoch 400, training loss: 12.996657371520996 = 0.8661689162254333 + 2.0 * 6.065244197845459
Epoch 400, val loss: 1.0585908889770508
Epoch 410, training loss: 12.959737777709961 = 0.8273056149482727 + 2.0 * 6.066215991973877
Epoch 410, val loss: 1.0305649042129517
Epoch 420, training loss: 12.91439151763916 = 0.7907182574272156 + 2.0 * 6.0618367195129395
Epoch 420, val loss: 1.004212498664856
Epoch 430, training loss: 12.875822067260742 = 0.7558332085609436 + 2.0 * 6.059994220733643
Epoch 430, val loss: 0.9797275066375732
Epoch 440, training loss: 12.83847427368164 = 0.7224984169006348 + 2.0 * 6.057987689971924
Epoch 440, val loss: 0.956652820110321
Epoch 450, training loss: 12.80301570892334 = 0.6906672716140747 + 2.0 * 6.056174278259277
Epoch 450, val loss: 0.9350784420967102
Epoch 460, training loss: 12.79660701751709 = 0.660001814365387 + 2.0 * 6.068302631378174
Epoch 460, val loss: 0.9148014187812805
Epoch 470, training loss: 12.748414993286133 = 0.6313929557800293 + 2.0 * 6.058510780334473
Epoch 470, val loss: 0.8960297703742981
Epoch 480, training loss: 12.707012176513672 = 0.6041048169136047 + 2.0 * 6.051453590393066
Epoch 480, val loss: 0.8787378668785095
Epoch 490, training loss: 12.676437377929688 = 0.5776414275169373 + 2.0 * 6.049397945404053
Epoch 490, val loss: 0.8621596693992615
Epoch 500, training loss: 12.64700984954834 = 0.5520126819610596 + 2.0 * 6.04749870300293
Epoch 500, val loss: 0.8464292883872986
Epoch 510, training loss: 12.627521514892578 = 0.5272749066352844 + 2.0 * 6.05012321472168
Epoch 510, val loss: 0.8315371870994568
Epoch 520, training loss: 12.594267845153809 = 0.5037625432014465 + 2.0 * 6.045252799987793
Epoch 520, val loss: 0.8177128434181213
Epoch 530, training loss: 12.571002960205078 = 0.481432169675827 + 2.0 * 6.044785499572754
Epoch 530, val loss: 0.805152416229248
Epoch 540, training loss: 12.550512313842773 = 0.46005871891975403 + 2.0 * 6.045226573944092
Epoch 540, val loss: 0.7933846712112427
Epoch 550, training loss: 12.528018951416016 = 0.4396662712097168 + 2.0 * 6.04417610168457
Epoch 550, val loss: 0.7827363014221191
Epoch 560, training loss: 12.49885368347168 = 0.4203358292579651 + 2.0 * 6.03925895690918
Epoch 560, val loss: 0.7731645703315735
Epoch 570, training loss: 12.478919982910156 = 0.40176889300346375 + 2.0 * 6.038575649261475
Epoch 570, val loss: 0.764346718788147
Epoch 580, training loss: 12.480704307556152 = 0.383942186832428 + 2.0 * 6.0483808517456055
Epoch 580, val loss: 0.7563291788101196
Epoch 590, training loss: 12.440845489501953 = 0.3672325313091278 + 2.0 * 6.036806583404541
Epoch 590, val loss: 0.7494180798530579
Epoch 600, training loss: 12.421805381774902 = 0.35118329524993896 + 2.0 * 6.035311222076416
Epoch 600, val loss: 0.7434592843055725
Epoch 610, training loss: 12.403741836547852 = 0.3356190323829651 + 2.0 * 6.034061431884766
Epoch 610, val loss: 0.737854540348053
Epoch 620, training loss: 12.401528358459473 = 0.32053494453430176 + 2.0 * 6.040496826171875
Epoch 620, val loss: 0.732875645160675
Epoch 630, training loss: 12.371801376342773 = 0.3061739504337311 + 2.0 * 6.032813549041748
Epoch 630, val loss: 0.7285387516021729
Epoch 640, training loss: 12.354942321777344 = 0.2923159599304199 + 2.0 * 6.031313419342041
Epoch 640, val loss: 0.7249460816383362
Epoch 650, training loss: 12.339709281921387 = 0.2789154052734375 + 2.0 * 6.030396938323975
Epoch 650, val loss: 0.7217183709144592
Epoch 660, training loss: 12.327302932739258 = 0.26605406403541565 + 2.0 * 6.0306243896484375
Epoch 660, val loss: 0.7188377380371094
Epoch 670, training loss: 12.313111305236816 = 0.25376513600349426 + 2.0 * 6.029673099517822
Epoch 670, val loss: 0.716675341129303
Epoch 680, training loss: 12.296022415161133 = 0.24198217689990997 + 2.0 * 6.02701997756958
Epoch 680, val loss: 0.7149832844734192
Epoch 690, training loss: 12.308235168457031 = 0.23065336048603058 + 2.0 * 6.038790702819824
Epoch 690, val loss: 0.7134991884231567
Epoch 700, training loss: 12.270698547363281 = 0.22002968192100525 + 2.0 * 6.025334358215332
Epoch 700, val loss: 0.7125287055969238
Epoch 710, training loss: 12.26067066192627 = 0.20990172028541565 + 2.0 * 6.025384426116943
Epoch 710, val loss: 0.7122412323951721
Epoch 720, training loss: 12.24740219116211 = 0.20021696388721466 + 2.0 * 6.023592472076416
Epoch 720, val loss: 0.7120957970619202
Epoch 730, training loss: 12.246112823486328 = 0.19098901748657227 + 2.0 * 6.027562141418457
Epoch 730, val loss: 0.7123094797134399
Epoch 740, training loss: 12.233180046081543 = 0.18236570060253143 + 2.0 * 6.025407314300537
Epoch 740, val loss: 0.7127898335456848
Epoch 750, training loss: 12.218202590942383 = 0.1742037832736969 + 2.0 * 6.021999359130859
Epoch 750, val loss: 0.7138644456863403
Epoch 760, training loss: 12.206770896911621 = 0.16646330058574677 + 2.0 * 6.020153999328613
Epoch 760, val loss: 0.7150973081588745
Epoch 770, training loss: 12.209636688232422 = 0.15913493931293488 + 2.0 * 6.0252509117126465
Epoch 770, val loss: 0.7165459394454956
Epoch 780, training loss: 12.199515342712402 = 0.1522803008556366 + 2.0 * 6.023617744445801
Epoch 780, val loss: 0.7182565331459045
Epoch 790, training loss: 12.184046745300293 = 0.14582958817481995 + 2.0 * 6.019108772277832
Epoch 790, val loss: 0.7201495170593262
Epoch 800, training loss: 12.175992965698242 = 0.13975870609283447 + 2.0 * 6.0181169509887695
Epoch 800, val loss: 0.7223905324935913
Epoch 810, training loss: 12.177812576293945 = 0.13399921357631683 + 2.0 * 6.021906852722168
Epoch 810, val loss: 0.7246984839439392
Epoch 820, training loss: 12.163694381713867 = 0.12858520448207855 + 2.0 * 6.017554759979248
Epoch 820, val loss: 0.727129340171814
Epoch 830, training loss: 12.15562629699707 = 0.12346488237380981 + 2.0 * 6.016080856323242
Epoch 830, val loss: 0.7299581170082092
Epoch 840, training loss: 12.154964447021484 = 0.11860711127519608 + 2.0 * 6.018178462982178
Epoch 840, val loss: 0.7327356934547424
Epoch 850, training loss: 12.141955375671387 = 0.1140056774020195 + 2.0 * 6.013974666595459
Epoch 850, val loss: 0.7356442213058472
Epoch 860, training loss: 12.142301559448242 = 0.10964466631412506 + 2.0 * 6.01632833480835
Epoch 860, val loss: 0.7388185858726501
Epoch 870, training loss: 12.135297775268555 = 0.10552556067705154 + 2.0 * 6.014885902404785
Epoch 870, val loss: 0.7419849634170532
Epoch 880, training loss: 12.129222869873047 = 0.10162235796451569 + 2.0 * 6.013800144195557
Epoch 880, val loss: 0.7452499866485596
Epoch 890, training loss: 12.124605178833008 = 0.09791000187397003 + 2.0 * 6.013347625732422
Epoch 890, val loss: 0.748674213886261
Epoch 900, training loss: 12.124246597290039 = 0.09441123902797699 + 2.0 * 6.014917850494385
Epoch 900, val loss: 0.7518942356109619
Epoch 910, training loss: 12.11363410949707 = 0.09110558778047562 + 2.0 * 6.011264324188232
Epoch 910, val loss: 0.7554680705070496
Epoch 920, training loss: 12.10770320892334 = 0.08795301616191864 + 2.0 * 6.009875297546387
Epoch 920, val loss: 0.759091317653656
Epoch 930, training loss: 12.103074073791504 = 0.08492982387542725 + 2.0 * 6.009072303771973
Epoch 930, val loss: 0.7626082301139832
Epoch 940, training loss: 12.106586456298828 = 0.08203859627246857 + 2.0 * 6.012273788452148
Epoch 940, val loss: 0.7662259340286255
Epoch 950, training loss: 12.098604202270508 = 0.07928537577390671 + 2.0 * 6.009659290313721
Epoch 950, val loss: 0.7696360349655151
Epoch 960, training loss: 12.09553050994873 = 0.07668870687484741 + 2.0 * 6.009420871734619
Epoch 960, val loss: 0.7734003663063049
Epoch 970, training loss: 12.088187217712402 = 0.07419557124376297 + 2.0 * 6.006995677947998
Epoch 970, val loss: 0.7771037817001343
Epoch 980, training loss: 12.101768493652344 = 0.07180429995059967 + 2.0 * 6.014982223510742
Epoch 980, val loss: 0.7807110548019409
Epoch 990, training loss: 12.08717155456543 = 0.06951795518398285 + 2.0 * 6.008826732635498
Epoch 990, val loss: 0.7842206954956055
Epoch 1000, training loss: 12.07949161529541 = 0.067349873483181 + 2.0 * 6.006071090698242
Epoch 1000, val loss: 0.7880275249481201
Epoch 1010, training loss: 12.079843521118164 = 0.06525171548128128 + 2.0 * 6.007296085357666
Epoch 1010, val loss: 0.7916528582572937
Epoch 1020, training loss: 12.07237434387207 = 0.06324968487024307 + 2.0 * 6.0045623779296875
Epoch 1020, val loss: 0.7951692938804626
Epoch 1030, training loss: 12.071600914001465 = 0.061348896473646164 + 2.0 * 6.005125999450684
Epoch 1030, val loss: 0.7989165186882019
Epoch 1040, training loss: 12.067667007446289 = 0.059518661350011826 + 2.0 * 6.0040740966796875
Epoch 1040, val loss: 0.8026389479637146
Epoch 1050, training loss: 12.078639030456543 = 0.05775804817676544 + 2.0 * 6.010440349578857
Epoch 1050, val loss: 0.80614173412323
Epoch 1060, training loss: 12.062983512878418 = 0.05607256665825844 + 2.0 * 6.003455638885498
Epoch 1060, val loss: 0.8097559809684753
Epoch 1070, training loss: 12.059148788452148 = 0.05445980653166771 + 2.0 * 6.002344608306885
Epoch 1070, val loss: 0.8134792447090149
Epoch 1080, training loss: 12.06132984161377 = 0.052898190915584564 + 2.0 * 6.004215717315674
Epoch 1080, val loss: 0.816991925239563
Epoch 1090, training loss: 12.056238174438477 = 0.051399555057287216 + 2.0 * 6.002419471740723
Epoch 1090, val loss: 0.8204063773155212
Epoch 1100, training loss: 12.053620338439941 = 0.049970656633377075 + 2.0 * 6.001824855804443
Epoch 1100, val loss: 0.8241053819656372
Epoch 1110, training loss: 12.050958633422852 = 0.04858173057436943 + 2.0 * 6.001188278198242
Epoch 1110, val loss: 0.8276454210281372
Epoch 1120, training loss: 12.06618881225586 = 0.04724641889333725 + 2.0 * 6.009471416473389
Epoch 1120, val loss: 0.8311026096343994
Epoch 1130, training loss: 12.052682876586914 = 0.04596420004963875 + 2.0 * 6.003359317779541
Epoch 1130, val loss: 0.8343586325645447
Epoch 1140, training loss: 12.043621063232422 = 0.04474056139588356 + 2.0 * 5.9994401931762695
Epoch 1140, val loss: 0.8380382061004639
Epoch 1150, training loss: 12.041886329650879 = 0.04355398938059807 + 2.0 * 5.999166011810303
Epoch 1150, val loss: 0.8414638042449951
Epoch 1160, training loss: 12.04934024810791 = 0.042401839047670364 + 2.0 * 6.003468990325928
Epoch 1160, val loss: 0.8447867035865784
Epoch 1170, training loss: 12.046055793762207 = 0.041293490678071976 + 2.0 * 6.002381324768066
Epoch 1170, val loss: 0.8480642437934875
Epoch 1180, training loss: 12.039737701416016 = 0.04023797810077667 + 2.0 * 5.999749660491943
Epoch 1180, val loss: 0.8514831066131592
Epoch 1190, training loss: 12.035439491271973 = 0.03921596333384514 + 2.0 * 5.998111724853516
Epoch 1190, val loss: 0.8548747897148132
Epoch 1200, training loss: 12.032197952270508 = 0.03822430968284607 + 2.0 * 5.9969868659973145
Epoch 1200, val loss: 0.8581821918487549
Epoch 1210, training loss: 12.03921890258789 = 0.03726227581501007 + 2.0 * 6.000978469848633
Epoch 1210, val loss: 0.8614629507064819
Epoch 1220, training loss: 12.030718803405762 = 0.03633794188499451 + 2.0 * 5.997190475463867
Epoch 1220, val loss: 0.8646496534347534
Epoch 1230, training loss: 12.04624080657959 = 0.0354488380253315 + 2.0 * 6.005395889282227
Epoch 1230, val loss: 0.8679596781730652
Epoch 1240, training loss: 12.036796569824219 = 0.034591756761074066 + 2.0 * 6.001102447509766
Epoch 1240, val loss: 0.8707754611968994
Epoch 1250, training loss: 12.025787353515625 = 0.033782314509153366 + 2.0 * 5.996002674102783
Epoch 1250, val loss: 0.8741363883018494
Epoch 1260, training loss: 12.023819923400879 = 0.03299246355891228 + 2.0 * 5.995413780212402
Epoch 1260, val loss: 0.877379298210144
Epoch 1270, training loss: 12.021134376525879 = 0.032218024134635925 + 2.0 * 5.994458198547363
Epoch 1270, val loss: 0.8804538249969482
Epoch 1280, training loss: 12.019248962402344 = 0.03146318346261978 + 2.0 * 5.993892669677734
Epoch 1280, val loss: 0.8835461139678955
Epoch 1290, training loss: 12.017921447753906 = 0.030729049816727638 + 2.0 * 5.993596076965332
Epoch 1290, val loss: 0.8866535425186157
Epoch 1300, training loss: 12.025818824768066 = 0.030015114694833755 + 2.0 * 5.997901916503906
Epoch 1300, val loss: 0.889674961566925
Epoch 1310, training loss: 12.019122123718262 = 0.029332416132092476 + 2.0 * 5.994894981384277
Epoch 1310, val loss: 0.8925341963768005
Epoch 1320, training loss: 12.023935317993164 = 0.028675373643636703 + 2.0 * 5.9976301193237305
Epoch 1320, val loss: 0.8955285549163818
Epoch 1330, training loss: 12.014385223388672 = 0.028043551370501518 + 2.0 * 5.993170738220215
Epoch 1330, val loss: 0.8986261487007141
Epoch 1340, training loss: 12.012176513671875 = 0.02742723934352398 + 2.0 * 5.992374420166016
Epoch 1340, val loss: 0.9016140103340149
Epoch 1350, training loss: 12.01432991027832 = 0.02682405337691307 + 2.0 * 5.993752956390381
Epoch 1350, val loss: 0.9044932126998901
Epoch 1360, training loss: 12.01447868347168 = 0.026237791404128075 + 2.0 * 5.9941205978393555
Epoch 1360, val loss: 0.9072914123535156
Epoch 1370, training loss: 12.011157989501953 = 0.0256736408919096 + 2.0 * 5.99274206161499
Epoch 1370, val loss: 0.9101584553718567
Epoch 1380, training loss: 12.011595726013184 = 0.02513016387820244 + 2.0 * 5.993232727050781
Epoch 1380, val loss: 0.9130733013153076
Epoch 1390, training loss: 12.005899429321289 = 0.02460014447569847 + 2.0 * 5.990649700164795
Epoch 1390, val loss: 0.9158913493156433
Epoch 1400, training loss: 12.00841236114502 = 0.024083664640784264 + 2.0 * 5.992164134979248
Epoch 1400, val loss: 0.9187246561050415
Epoch 1410, training loss: 12.016555786132812 = 0.023581676185131073 + 2.0 * 5.996487140655518
Epoch 1410, val loss: 0.9213878512382507
Epoch 1420, training loss: 12.007894515991211 = 0.023101234808564186 + 2.0 * 5.992396831512451
Epoch 1420, val loss: 0.9242082238197327
Epoch 1430, training loss: 12.009407997131348 = 0.022633379325270653 + 2.0 * 5.993387222290039
Epoch 1430, val loss: 0.9270094633102417
Epoch 1440, training loss: 12.003514289855957 = 0.022177157923579216 + 2.0 * 5.990668773651123
Epoch 1440, val loss: 0.9296727180480957
Epoch 1450, training loss: 12.000765800476074 = 0.021733392030000687 + 2.0 * 5.989516258239746
Epoch 1450, val loss: 0.9324406981468201
Epoch 1460, training loss: 11.99965763092041 = 0.021300772204995155 + 2.0 * 5.989178657531738
Epoch 1460, val loss: 0.9351463913917542
Epoch 1470, training loss: 12.005289077758789 = 0.020879453048110008 + 2.0 * 5.992204666137695
Epoch 1470, val loss: 0.9378029704093933
Epoch 1480, training loss: 12.005499839782715 = 0.0204709991812706 + 2.0 * 5.992514610290527
Epoch 1480, val loss: 0.9402511715888977
Epoch 1490, training loss: 11.999404907226562 = 0.020078377798199654 + 2.0 * 5.989663124084473
Epoch 1490, val loss: 0.9429235458374023
Epoch 1500, training loss: 11.997710227966309 = 0.019698772579431534 + 2.0 * 5.9890055656433105
Epoch 1500, val loss: 0.9456180334091187
Epoch 1510, training loss: 12.003809928894043 = 0.019325170665979385 + 2.0 * 5.992242336273193
Epoch 1510, val loss: 0.9480977058410645
Epoch 1520, training loss: 11.994050979614258 = 0.018960991874337196 + 2.0 * 5.987545013427734
Epoch 1520, val loss: 0.9505731463432312
Epoch 1530, training loss: 11.992605209350586 = 0.018608391284942627 + 2.0 * 5.986998558044434
Epoch 1530, val loss: 0.9532029032707214
Epoch 1540, training loss: 11.993593215942383 = 0.018262049183249474 + 2.0 * 5.98766565322876
Epoch 1540, val loss: 0.9557289481163025
Epoch 1550, training loss: 12.004881858825684 = 0.017923885956406593 + 2.0 * 5.993478775024414
Epoch 1550, val loss: 0.9580917954444885
Epoch 1560, training loss: 11.99569320678711 = 0.017600353807210922 + 2.0 * 5.989046573638916
Epoch 1560, val loss: 0.9604894518852234
Epoch 1570, training loss: 11.992874145507812 = 0.017287379130721092 + 2.0 * 5.987793445587158
Epoch 1570, val loss: 0.9630421996116638
Epoch 1580, training loss: 11.988616943359375 = 0.01698162779211998 + 2.0 * 5.9858174324035645
Epoch 1580, val loss: 0.9655420184135437
Epoch 1590, training loss: 11.98907470703125 = 0.01668078452348709 + 2.0 * 5.986196994781494
Epoch 1590, val loss: 0.9679620862007141
Epoch 1600, training loss: 12.004322052001953 = 0.01638741046190262 + 2.0 * 5.993967533111572
Epoch 1600, val loss: 0.9702081680297852
Epoch 1610, training loss: 11.992572784423828 = 0.016101937741041183 + 2.0 * 5.9882354736328125
Epoch 1610, val loss: 0.9725132584571838
Epoch 1620, training loss: 11.990765571594238 = 0.015828866511583328 + 2.0 * 5.987468242645264
Epoch 1620, val loss: 0.975031852722168
Epoch 1630, training loss: 11.988780975341797 = 0.015558202750980854 + 2.0 * 5.986611366271973
Epoch 1630, val loss: 0.9773178696632385
Epoch 1640, training loss: 11.984148979187012 = 0.015294169075787067 + 2.0 * 5.984427452087402
Epoch 1640, val loss: 0.9796856045722961
Epoch 1650, training loss: 11.98658275604248 = 0.01503544021397829 + 2.0 * 5.98577356338501
Epoch 1650, val loss: 0.9820371270179749
Epoch 1660, training loss: 11.988058090209961 = 0.014782849699258804 + 2.0 * 5.986637592315674
Epoch 1660, val loss: 0.9842721223831177
Epoch 1670, training loss: 11.992659568786621 = 0.01453784853219986 + 2.0 * 5.989060878753662
Epoch 1670, val loss: 0.9865826368331909
Epoch 1680, training loss: 11.982746124267578 = 0.014298449270427227 + 2.0 * 5.98422384262085
Epoch 1680, val loss: 0.9888085126876831
Epoch 1690, training loss: 11.988239288330078 = 0.014065609313547611 + 2.0 * 5.987086772918701
Epoch 1690, val loss: 0.991108775138855
Epoch 1700, training loss: 11.98086166381836 = 0.013838572427630424 + 2.0 * 5.983511447906494
Epoch 1700, val loss: 0.9933094382286072
Epoch 1710, training loss: 11.979923248291016 = 0.013615747913718224 + 2.0 * 5.983153820037842
Epoch 1710, val loss: 0.9956220984458923
Epoch 1720, training loss: 11.979636192321777 = 0.013397331349551678 + 2.0 * 5.983119487762451
Epoch 1720, val loss: 0.9978412985801697
Epoch 1730, training loss: 11.991541862487793 = 0.01318321656435728 + 2.0 * 5.9891791343688965
Epoch 1730, val loss: 0.9999927878379822
Epoch 1740, training loss: 11.983119010925293 = 0.01297540683299303 + 2.0 * 5.985071659088135
Epoch 1740, val loss: 1.0019930601119995
Epoch 1750, training loss: 11.978806495666504 = 0.01277561392635107 + 2.0 * 5.983015537261963
Epoch 1750, val loss: 1.0042765140533447
Epoch 1760, training loss: 11.980305671691895 = 0.012579630129039288 + 2.0 * 5.98386287689209
Epoch 1760, val loss: 1.006494402885437
Epoch 1770, training loss: 11.98282527923584 = 0.012384849600493908 + 2.0 * 5.985220432281494
Epoch 1770, val loss: 1.0084980726242065
Epoch 1780, training loss: 11.976961135864258 = 0.012198112905025482 + 2.0 * 5.982381343841553
Epoch 1780, val loss: 1.01066255569458
Epoch 1790, training loss: 11.977171897888184 = 0.012013477273285389 + 2.0 * 5.982579231262207
Epoch 1790, val loss: 1.0128262042999268
Epoch 1800, training loss: 11.980224609375 = 0.01183177437633276 + 2.0 * 5.984196186065674
Epoch 1800, val loss: 1.0146931409835815
Epoch 1810, training loss: 11.97472858428955 = 0.011656394228339195 + 2.0 * 5.981535911560059
Epoch 1810, val loss: 1.016749620437622
Epoch 1820, training loss: 11.973372459411621 = 0.01148755569010973 + 2.0 * 5.980942249298096
Epoch 1820, val loss: 1.0189175605773926
Epoch 1830, training loss: 11.973904609680176 = 0.011318078264594078 + 2.0 * 5.981293201446533
Epoch 1830, val loss: 1.0209693908691406
Epoch 1840, training loss: 11.983853340148926 = 0.011151648126542568 + 2.0 * 5.986351013183594
Epoch 1840, val loss: 1.0229008197784424
Epoch 1850, training loss: 11.980834007263184 = 0.010991389863193035 + 2.0 * 5.984921455383301
Epoch 1850, val loss: 1.0249085426330566
Epoch 1860, training loss: 11.974717140197754 = 0.010834920220077038 + 2.0 * 5.981941223144531
Epoch 1860, val loss: 1.026786208152771
Epoch 1870, training loss: 11.972101211547852 = 0.010682889260351658 + 2.0 * 5.980709075927734
Epoch 1870, val loss: 1.0288047790527344
Epoch 1880, training loss: 11.969747543334961 = 0.010533896274864674 + 2.0 * 5.979606628417969
Epoch 1880, val loss: 1.0308775901794434
Epoch 1890, training loss: 11.969819068908691 = 0.010385122150182724 + 2.0 * 5.979716777801514
Epoch 1890, val loss: 1.0328258275985718
Epoch 1900, training loss: 11.983113288879395 = 0.010238658636808395 + 2.0 * 5.9864373207092285
Epoch 1900, val loss: 1.0346755981445312
Epoch 1910, training loss: 11.973551750183105 = 0.010096952319145203 + 2.0 * 5.981727600097656
Epoch 1910, val loss: 1.0364947319030762
Epoch 1920, training loss: 11.969348907470703 = 0.009959837421774864 + 2.0 * 5.979694366455078
Epoch 1920, val loss: 1.0385234355926514
Epoch 1930, training loss: 11.975339889526367 = 0.00982354860752821 + 2.0 * 5.982758045196533
Epoch 1930, val loss: 1.0403997898101807
Epoch 1940, training loss: 11.966981887817383 = 0.009690595790743828 + 2.0 * 5.9786458015441895
Epoch 1940, val loss: 1.0422574281692505
Epoch 1950, training loss: 11.967788696289062 = 0.009560704231262207 + 2.0 * 5.979114055633545
Epoch 1950, val loss: 1.044188380241394
Epoch 1960, training loss: 11.985189437866211 = 0.009431582875549793 + 2.0 * 5.987878799438477
Epoch 1960, val loss: 1.0459576845169067
Epoch 1970, training loss: 11.970438957214355 = 0.009308429434895515 + 2.0 * 5.980565071105957
Epoch 1970, val loss: 1.0476700067520142
Epoch 1980, training loss: 11.966229438781738 = 0.009187571704387665 + 2.0 * 5.97852087020874
Epoch 1980, val loss: 1.0495903491973877
Epoch 1990, training loss: 11.964524269104004 = 0.009069239720702171 + 2.0 * 5.97772741317749
Epoch 1990, val loss: 1.051520586013794
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.8819
Flip ASR: 0.8578/225 nodes
The final ASR:0.80566, 0.07036, Accuracy:0.80617, 0.01772
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11618])
remove edge: torch.Size([2, 9454])
updated graph: torch.Size([2, 10516])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9926
Flip ASR: 0.9911/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9668
Flip ASR: 0.9600/225 nodes
The final ASR:0.97909, 0.01058, Accuracy:0.83086, 0.00698
Begin epxeriment: cont_weight: 2 epoch:2000 der1:0.3 der2:0 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 18.693161010742188 = 1.9454302787780762 + 2.0 * 8.373865127563477
Epoch 0, val loss: 1.9478877782821655
Epoch 10, training loss: 18.682514190673828 = 1.9358232021331787 + 2.0 * 8.373345375061035
Epoch 10, val loss: 1.9386367797851562
Epoch 20, training loss: 18.66265106201172 = 1.923760175704956 + 2.0 * 8.36944580078125
Epoch 20, val loss: 1.9270869493484497
Epoch 30, training loss: 18.584890365600586 = 1.907335638999939 + 2.0 * 8.338777542114258
Epoch 30, val loss: 1.911360263824463
Epoch 40, training loss: 18.081811904907227 = 1.8878830671310425 + 2.0 * 8.096964836120605
Epoch 40, val loss: 1.8928321599960327
Epoch 50, training loss: 16.57697105407715 = 1.8682870864868164 + 2.0 * 7.354341983795166
Epoch 50, val loss: 1.874250054359436
Epoch 60, training loss: 15.758075714111328 = 1.854949712753296 + 2.0 * 6.951562881469727
Epoch 60, val loss: 1.8622608184814453
Epoch 70, training loss: 15.251708984375 = 1.8445063829421997 + 2.0 * 6.703601360321045
Epoch 70, val loss: 1.852460503578186
Epoch 80, training loss: 14.952317237854004 = 1.8357785940170288 + 2.0 * 6.558269500732422
Epoch 80, val loss: 1.8441236019134521
Epoch 90, training loss: 14.754793167114258 = 1.827385425567627 + 2.0 * 6.463703632354736
Epoch 90, val loss: 1.8359068632125854
Epoch 100, training loss: 14.617624282836914 = 1.8188761472702026 + 2.0 * 6.399374008178711
Epoch 100, val loss: 1.8275065422058105
Epoch 110, training loss: 14.521039009094238 = 1.8107529878616333 + 2.0 * 6.355143070220947
Epoch 110, val loss: 1.8193566799163818
Epoch 120, training loss: 14.434028625488281 = 1.803679347038269 + 2.0 * 6.315174579620361
Epoch 120, val loss: 1.8119969367980957
Epoch 130, training loss: 14.362250328063965 = 1.7973353862762451 + 2.0 * 6.28245735168457
Epoch 130, val loss: 1.8053313493728638
Epoch 140, training loss: 14.305033683776855 = 1.7910224199295044 + 2.0 * 6.25700569152832
Epoch 140, val loss: 1.798783302307129
Epoch 150, training loss: 14.251442909240723 = 1.7843239307403564 + 2.0 * 6.233559608459473
Epoch 150, val loss: 1.792189121246338
Epoch 160, training loss: 14.205042839050293 = 1.7772268056869507 + 2.0 * 6.2139081954956055
Epoch 160, val loss: 1.785528302192688
Epoch 170, training loss: 14.165815353393555 = 1.769368052482605 + 2.0 * 6.19822359085083
Epoch 170, val loss: 1.7785286903381348
Epoch 180, training loss: 14.131322860717773 = 1.760388731956482 + 2.0 * 6.18546724319458
Epoch 180, val loss: 1.770887851715088
Epoch 190, training loss: 14.09699821472168 = 1.750295639038086 + 2.0 * 6.173351287841797
Epoch 190, val loss: 1.7624505758285522
Epoch 200, training loss: 14.06531047821045 = 1.7386351823806763 + 2.0 * 6.163337707519531
Epoch 200, val loss: 1.7529220581054688
Epoch 210, training loss: 14.038019180297852 = 1.7250778675079346 + 2.0 * 6.156470775604248
Epoch 210, val loss: 1.74191153049469
Epoch 220, training loss: 14.004498481750488 = 1.709328055381775 + 2.0 * 6.147585391998291
Epoch 220, val loss: 1.7292529344558716
Epoch 230, training loss: 13.972620964050293 = 1.6909226179122925 + 2.0 * 6.1408491134643555
Epoch 230, val loss: 1.7145463228225708
Epoch 240, training loss: 13.94080638885498 = 1.6691852807998657 + 2.0 * 6.135810375213623
Epoch 240, val loss: 1.6970093250274658
Epoch 250, training loss: 13.901742935180664 = 1.6436598300933838 + 2.0 * 6.12904167175293
Epoch 250, val loss: 1.6765042543411255
Epoch 260, training loss: 13.863258361816406 = 1.613848090171814 + 2.0 * 6.1247053146362305
Epoch 260, val loss: 1.6523890495300293
Epoch 270, training loss: 13.818086624145508 = 1.5792938470840454 + 2.0 * 6.119396209716797
Epoch 270, val loss: 1.624422550201416
Epoch 280, training loss: 13.769307136535645 = 1.5398035049438477 + 2.0 * 6.114751815795898
Epoch 280, val loss: 1.592293381690979
Epoch 290, training loss: 13.719863891601562 = 1.4953503608703613 + 2.0 * 6.1122565269470215
Epoch 290, val loss: 1.5562245845794678
Epoch 300, training loss: 13.663227081298828 = 1.4469969272613525 + 2.0 * 6.108115196228027
Epoch 300, val loss: 1.517084002494812
Epoch 310, training loss: 13.604429244995117 = 1.3958203792572021 + 2.0 * 6.104304313659668
Epoch 310, val loss: 1.47581946849823
Epoch 320, training loss: 13.546353340148926 = 1.3431062698364258 + 2.0 * 6.10162353515625
Epoch 320, val loss: 1.4336626529693604
Epoch 330, training loss: 13.496603012084961 = 1.2910901308059692 + 2.0 * 6.102756500244141
Epoch 330, val loss: 1.3925330638885498
Epoch 340, training loss: 13.432158470153809 = 1.2408225536346436 + 2.0 * 6.095667839050293
Epoch 340, val loss: 1.3536540269851685
Epoch 350, training loss: 13.380106925964355 = 1.1920897960662842 + 2.0 * 6.094008445739746
Epoch 350, val loss: 1.316374659538269
Epoch 360, training loss: 13.32708740234375 = 1.1449002027511597 + 2.0 * 6.09109354019165
Epoch 360, val loss: 1.2808083295822144
Epoch 370, training loss: 13.277006149291992 = 1.0988186597824097 + 2.0 * 6.0890936851501465
Epoch 370, val loss: 1.2466623783111572
Epoch 380, training loss: 13.22364616394043 = 1.0531944036483765 + 2.0 * 6.085226058959961
Epoch 380, val loss: 1.2133960723876953
Epoch 390, training loss: 13.180581092834473 = 1.0078016519546509 + 2.0 * 6.086389541625977
Epoch 390, val loss: 1.1804052591323853
Epoch 400, training loss: 13.127115249633789 = 0.9629192352294922 + 2.0 * 6.082098007202148
Epoch 400, val loss: 1.1481878757476807
Epoch 410, training loss: 13.075087547302246 = 0.9188107252120972 + 2.0 * 6.07813835144043
Epoch 410, val loss: 1.1165916919708252
Epoch 420, training loss: 13.029006004333496 = 0.8752005100250244 + 2.0 * 6.076902866363525
Epoch 420, val loss: 1.0853639841079712
Epoch 430, training loss: 12.987614631652832 = 0.8327723145484924 + 2.0 * 6.077421188354492
Epoch 430, val loss: 1.0550719499588013
Epoch 440, training loss: 12.938395500183105 = 0.7918495535850525 + 2.0 * 6.073273181915283
Epoch 440, val loss: 1.0261828899383545
Epoch 450, training loss: 12.893498420715332 = 0.7523075342178345 + 2.0 * 6.0705952644348145
Epoch 450, val loss: 0.998385488986969
Epoch 460, training loss: 12.864161491394043 = 0.7144988775253296 + 2.0 * 6.074831485748291
Epoch 460, val loss: 0.9719808101654053
Epoch 470, training loss: 12.814947128295898 = 0.6791878342628479 + 2.0 * 6.067879676818848
Epoch 470, val loss: 0.9479604363441467
Epoch 480, training loss: 12.778237342834473 = 0.6460539698600769 + 2.0 * 6.066091537475586
Epoch 480, val loss: 0.9260040521621704
Epoch 490, training loss: 12.741254806518555 = 0.614799976348877 + 2.0 * 6.06322717666626
Epoch 490, val loss: 0.905954122543335
Epoch 500, training loss: 12.730545043945312 = 0.5854594111442566 + 2.0 * 6.072542667388916
Epoch 500, val loss: 0.8879621028900146
Epoch 510, training loss: 12.684892654418945 = 0.5582582354545593 + 2.0 * 6.06331729888916
Epoch 510, val loss: 0.8721730709075928
Epoch 520, training loss: 12.652728080749512 = 0.5329811573028564 + 2.0 * 6.059873580932617
Epoch 520, val loss: 0.8585373163223267
Epoch 530, training loss: 12.62627124786377 = 0.5092677474021912 + 2.0 * 6.058501720428467
Epoch 530, val loss: 0.8467302322387695
Epoch 540, training loss: 12.598272323608398 = 0.48707517981529236 + 2.0 * 6.055598735809326
Epoch 540, val loss: 0.8366364240646362
Epoch 550, training loss: 12.57494831085205 = 0.4662601351737976 + 2.0 * 6.054344177246094
Epoch 550, val loss: 0.8282938003540039
Epoch 560, training loss: 12.554326057434082 = 0.44667601585388184 + 2.0 * 6.0538249015808105
Epoch 560, val loss: 0.8213691711425781
Epoch 570, training loss: 12.534707069396973 = 0.42819154262542725 + 2.0 * 6.053257942199707
Epoch 570, val loss: 0.8157728314399719
Epoch 580, training loss: 12.516471862792969 = 0.41073277592658997 + 2.0 * 6.0528693199157715
Epoch 580, val loss: 0.8112979531288147
Epoch 590, training loss: 12.493156433105469 = 0.3941372036933899 + 2.0 * 6.049509525299072
Epoch 590, val loss: 0.8079329133033752
Epoch 600, training loss: 12.473623275756836 = 0.3783317506313324 + 2.0 * 6.047645568847656
Epoch 600, val loss: 0.8055297136306763
Epoch 610, training loss: 12.457579612731934 = 0.3631555438041687 + 2.0 * 6.04721212387085
Epoch 610, val loss: 0.8039368987083435
Epoch 620, training loss: 12.442413330078125 = 0.3486120104789734 + 2.0 * 6.046900749206543
Epoch 620, val loss: 0.8030556440353394
Epoch 630, training loss: 12.43044662475586 = 0.3347700238227844 + 2.0 * 6.04783821105957
Epoch 630, val loss: 0.8029443621635437
Epoch 640, training loss: 12.411368370056152 = 0.3214881420135498 + 2.0 * 6.044939994812012
Epoch 640, val loss: 0.803539514541626
Epoch 650, training loss: 12.393423080444336 = 0.3087098002433777 + 2.0 * 6.042356491088867
Epoch 650, val loss: 0.804561197757721
Epoch 660, training loss: 12.37865161895752 = 0.29637613892555237 + 2.0 * 6.0411376953125
Epoch 660, val loss: 0.8061970472335815
Epoch 670, training loss: 12.372902870178223 = 0.28448179364204407 + 2.0 * 6.044210433959961
Epoch 670, val loss: 0.8082204461097717
Epoch 680, training loss: 12.35287857055664 = 0.2730768322944641 + 2.0 * 6.039900779724121
Epoch 680, val loss: 0.8107490539550781
Epoch 690, training loss: 12.342448234558105 = 0.262090802192688 + 2.0 * 6.0401787757873535
Epoch 690, val loss: 0.8136653900146484
Epoch 700, training loss: 12.32931137084961 = 0.25152021646499634 + 2.0 * 6.038895606994629
Epoch 700, val loss: 0.8168596625328064
Epoch 710, training loss: 12.316386222839355 = 0.2413851022720337 + 2.0 * 6.037500381469727
Epoch 710, val loss: 0.8204419612884521
Epoch 720, training loss: 12.30300521850586 = 0.23159721493721008 + 2.0 * 6.035704135894775
Epoch 720, val loss: 0.8244402408599854
Epoch 730, training loss: 12.292398452758789 = 0.2221001833677292 + 2.0 * 6.035149097442627
Epoch 730, val loss: 0.8286699056625366
Epoch 740, training loss: 12.290621757507324 = 0.21294346451759338 + 2.0 * 6.038839340209961
Epoch 740, val loss: 0.83305424451828
Epoch 750, training loss: 12.27650260925293 = 0.2041848748922348 + 2.0 * 6.036159038543701
Epoch 750, val loss: 0.8377067446708679
Epoch 760, training loss: 12.26137638092041 = 0.19576884806156158 + 2.0 * 6.032803535461426
Epoch 760, val loss: 0.8426013588905334
Epoch 770, training loss: 12.252998352050781 = 0.18761825561523438 + 2.0 * 6.032690048217773
Epoch 770, val loss: 0.8476391434669495
Epoch 780, training loss: 12.24510383605957 = 0.1797654777765274 + 2.0 * 6.0326690673828125
Epoch 780, val loss: 0.8527709245681763
Epoch 790, training loss: 12.235418319702148 = 0.1722436100244522 + 2.0 * 6.03158712387085
Epoch 790, val loss: 0.8580490946769714
Epoch 800, training loss: 12.223912239074707 = 0.1650073081254959 + 2.0 * 6.029452323913574
Epoch 800, val loss: 0.8635890483856201
Epoch 810, training loss: 12.22545337677002 = 0.1580166518688202 + 2.0 * 6.033718585968018
Epoch 810, val loss: 0.8691336512565613
Epoch 820, training loss: 12.213862419128418 = 0.1513059437274933 + 2.0 * 6.031278133392334
Epoch 820, val loss: 0.8746829628944397
Epoch 830, training loss: 12.200512886047363 = 0.14489731192588806 + 2.0 * 6.027807712554932
Epoch 830, val loss: 0.8804228901863098
Epoch 840, training loss: 12.192625999450684 = 0.13872340321540833 + 2.0 * 6.026951313018799
Epoch 840, val loss: 0.8861909508705139
Epoch 850, training loss: 12.197694778442383 = 0.1327972710132599 + 2.0 * 6.032448768615723
Epoch 850, val loss: 0.8918935656547546
Epoch 860, training loss: 12.183940887451172 = 0.12715187668800354 + 2.0 * 6.02839469909668
Epoch 860, val loss: 0.8976884484291077
Epoch 870, training loss: 12.175223350524902 = 0.12175881862640381 + 2.0 * 6.026732444763184
Epoch 870, val loss: 0.9034342169761658
Epoch 880, training loss: 12.166049003601074 = 0.11661283671855927 + 2.0 * 6.024718284606934
Epoch 880, val loss: 0.9093585014343262
Epoch 890, training loss: 12.158836364746094 = 0.11169611662626266 + 2.0 * 6.0235700607299805
Epoch 890, val loss: 0.9153109788894653
Epoch 900, training loss: 12.158590316772461 = 0.10699062049388885 + 2.0 * 6.025799751281738
Epoch 900, val loss: 0.9212618470191956
Epoch 910, training loss: 12.147905349731445 = 0.10251039266586304 + 2.0 * 6.022697448730469
Epoch 910, val loss: 0.9272059798240662
Epoch 920, training loss: 12.14162826538086 = 0.09824895858764648 + 2.0 * 6.021689414978027
Epoch 920, val loss: 0.9332378506660461
Epoch 930, training loss: 12.151712417602539 = 0.09419528394937515 + 2.0 * 6.028758525848389
Epoch 930, val loss: 0.9391005039215088
Epoch 940, training loss: 12.139982223510742 = 0.09033603966236115 + 2.0 * 6.024823188781738
Epoch 940, val loss: 0.9449552893638611
Epoch 950, training loss: 12.12568187713623 = 0.08670797199010849 + 2.0 * 6.019486904144287
Epoch 950, val loss: 0.9508660435676575
Epoch 960, training loss: 12.121522903442383 = 0.08324383944272995 + 2.0 * 6.019139766693115
Epoch 960, val loss: 0.9567135572433472
Epoch 970, training loss: 12.134256362915039 = 0.07993073761463165 + 2.0 * 6.027163028717041
Epoch 970, val loss: 0.9625647664070129
Epoch 980, training loss: 12.12548828125 = 0.07681994140148163 + 2.0 * 6.024333953857422
Epoch 980, val loss: 0.9681241512298584
Epoch 990, training loss: 12.110271453857422 = 0.07386854290962219 + 2.0 * 6.0182013511657715
Epoch 990, val loss: 0.9738742113113403
Epoch 1000, training loss: 12.105666160583496 = 0.07106360793113708 + 2.0 * 6.017301082611084
Epoch 1000, val loss: 0.9795178174972534
Epoch 1010, training loss: 12.100152969360352 = 0.06839276850223541 + 2.0 * 6.015880107879639
Epoch 1010, val loss: 0.9851617813110352
Epoch 1020, training loss: 12.116823196411133 = 0.06584573537111282 + 2.0 * 6.02548885345459
Epoch 1020, val loss: 0.9907894730567932
Epoch 1030, training loss: 12.101654052734375 = 0.0634317472577095 + 2.0 * 6.019111156463623
Epoch 1030, val loss: 0.9961503744125366
Epoch 1040, training loss: 12.089959144592285 = 0.0611501969397068 + 2.0 * 6.014404296875
Epoch 1040, val loss: 1.0017359256744385
Epoch 1050, training loss: 12.091165542602539 = 0.058972083032131195 + 2.0 * 6.016096591949463
Epoch 1050, val loss: 1.0072211027145386
Epoch 1060, training loss: 12.085664749145508 = 0.05689682811498642 + 2.0 * 6.014383792877197
Epoch 1060, val loss: 1.0125826597213745
Epoch 1070, training loss: 12.091595649719238 = 0.05493651330471039 + 2.0 * 6.018329620361328
Epoch 1070, val loss: 1.0179787874221802
Epoch 1080, training loss: 12.081977844238281 = 0.0530700646340847 + 2.0 * 6.014453887939453
Epoch 1080, val loss: 1.0231246948242188
Epoch 1090, training loss: 12.07684326171875 = 0.051298338919878006 + 2.0 * 6.012772560119629
Epoch 1090, val loss: 1.028457760810852
Epoch 1100, training loss: 12.0722074508667 = 0.04960399121046066 + 2.0 * 6.011301517486572
Epoch 1100, val loss: 1.0336335897445679
Epoch 1110, training loss: 12.070277214050293 = 0.047981034964323044 + 2.0 * 6.011147975921631
Epoch 1110, val loss: 1.0388239622116089
Epoch 1120, training loss: 12.087974548339844 = 0.0464337132871151 + 2.0 * 6.02077054977417
Epoch 1120, val loss: 1.0439006090164185
Epoch 1130, training loss: 12.0755615234375 = 0.04495704174041748 + 2.0 * 6.0153021812438965
Epoch 1130, val loss: 1.048738956451416
Epoch 1140, training loss: 12.067815780639648 = 0.04357379674911499 + 2.0 * 6.012121200561523
Epoch 1140, val loss: 1.0537763833999634
Epoch 1150, training loss: 12.059958457946777 = 0.042251307517290115 + 2.0 * 6.008853435516357
Epoch 1150, val loss: 1.058777093887329
Epoch 1160, training loss: 12.05880355834961 = 0.04097667336463928 + 2.0 * 6.008913516998291
Epoch 1160, val loss: 1.0636457204818726
Epoch 1170, training loss: 12.057644844055176 = 0.03975123167037964 + 2.0 * 6.008946895599365
Epoch 1170, val loss: 1.0685255527496338
Epoch 1180, training loss: 12.062821388244629 = 0.0385749451816082 + 2.0 * 6.012123107910156
Epoch 1180, val loss: 1.0732696056365967
Epoch 1190, training loss: 12.05936050415039 = 0.03745321184396744 + 2.0 * 6.010953426361084
Epoch 1190, val loss: 1.0779788494110107
Epoch 1200, training loss: 12.05211067199707 = 0.036380719393491745 + 2.0 * 6.007864952087402
Epoch 1200, val loss: 1.0827796459197998
Epoch 1210, training loss: 12.056594848632812 = 0.035348884761333466 + 2.0 * 6.010622978210449
Epoch 1210, val loss: 1.0874767303466797
Epoch 1220, training loss: 12.054393768310547 = 0.0343632847070694 + 2.0 * 6.01001501083374
Epoch 1220, val loss: 1.0918043851852417
Epoch 1230, training loss: 12.04904842376709 = 0.03342370688915253 + 2.0 * 6.0078125
Epoch 1230, val loss: 1.0964279174804688
Epoch 1240, training loss: 12.044617652893066 = 0.0325264073908329 + 2.0 * 6.006045818328857
Epoch 1240, val loss: 1.100972294807434
Epoch 1250, training loss: 12.043824195861816 = 0.03165431320667267 + 2.0 * 6.00608491897583
Epoch 1250, val loss: 1.1053752899169922
Epoch 1260, training loss: 12.050806045532227 = 0.030815359205007553 + 2.0 * 6.009995460510254
Epoch 1260, val loss: 1.1096994876861572
Epoch 1270, training loss: 12.045401573181152 = 0.03000880591571331 + 2.0 * 6.007696151733398
Epoch 1270, val loss: 1.1140755414962769
Epoch 1280, training loss: 12.038073539733887 = 0.029241053387522697 + 2.0 * 6.004416465759277
Epoch 1280, val loss: 1.118436336517334
Epoch 1290, training loss: 12.036996841430664 = 0.02849733829498291 + 2.0 * 6.004249572753906
Epoch 1290, val loss: 1.1228013038635254
Epoch 1300, training loss: 12.0621337890625 = 0.027777984738349915 + 2.0 * 6.017178058624268
Epoch 1300, val loss: 1.126924991607666
Epoch 1310, training loss: 12.037361145019531 = 0.027093790471553802 + 2.0 * 6.005133628845215
Epoch 1310, val loss: 1.1308962106704712
Epoch 1320, training loss: 12.033201217651367 = 0.02644195780158043 + 2.0 * 6.003379821777344
Epoch 1320, val loss: 1.135180950164795
Epoch 1330, training loss: 12.030381202697754 = 0.02580973133444786 + 2.0 * 6.002285957336426
Epoch 1330, val loss: 1.1393098831176758
Epoch 1340, training loss: 12.02871036529541 = 0.02519253082573414 + 2.0 * 6.001759052276611
Epoch 1340, val loss: 1.1433632373809814
Epoch 1350, training loss: 12.03597640991211 = 0.024595191702246666 + 2.0 * 6.005690574645996
Epoch 1350, val loss: 1.1474456787109375
Epoch 1360, training loss: 12.03583812713623 = 0.02402147278189659 + 2.0 * 6.005908489227295
Epoch 1360, val loss: 1.1512091159820557
Epoch 1370, training loss: 12.02961540222168 = 0.023470833897590637 + 2.0 * 6.003072261810303
Epoch 1370, val loss: 1.1551014184951782
Epoch 1380, training loss: 12.024589538574219 = 0.022944150492548943 + 2.0 * 6.0008225440979
Epoch 1380, val loss: 1.1591423749923706
Epoch 1390, training loss: 12.024408340454102 = 0.022429220378398895 + 2.0 * 6.0009894371032715
Epoch 1390, val loss: 1.1629916429519653
Epoch 1400, training loss: 12.02894115447998 = 0.021927829831838608 + 2.0 * 6.003506660461426
Epoch 1400, val loss: 1.166787028312683
Epoch 1410, training loss: 12.022829055786133 = 0.021446343511343002 + 2.0 * 6.0006914138793945
Epoch 1410, val loss: 1.1704890727996826
Epoch 1420, training loss: 12.026256561279297 = 0.02098129689693451 + 2.0 * 6.00263786315918
Epoch 1420, val loss: 1.1742459535598755
Epoch 1430, training loss: 12.02634048461914 = 0.020530829206109047 + 2.0 * 6.002904891967773
Epoch 1430, val loss: 1.177918553352356
Epoch 1440, training loss: 12.022347450256348 = 0.020099123939871788 + 2.0 * 6.001124382019043
Epoch 1440, val loss: 1.1815896034240723
Epoch 1450, training loss: 12.017644882202148 = 0.019676892086863518 + 2.0 * 5.998983860015869
Epoch 1450, val loss: 1.185296654701233
Epoch 1460, training loss: 12.019523620605469 = 0.019267581403255463 + 2.0 * 6.000127792358398
Epoch 1460, val loss: 1.1888929605484009
Epoch 1470, training loss: 12.019501686096191 = 0.018869806081056595 + 2.0 * 6.000316143035889
Epoch 1470, val loss: 1.1923667192459106
Epoch 1480, training loss: 12.019097328186035 = 0.018487202003598213 + 2.0 * 6.00030517578125
Epoch 1480, val loss: 1.1958627700805664
Epoch 1490, training loss: 12.016840934753418 = 0.01811835542321205 + 2.0 * 5.999361515045166
Epoch 1490, val loss: 1.1993626356124878
Epoch 1500, training loss: 12.017448425292969 = 0.017759529873728752 + 2.0 * 5.999844551086426
Epoch 1500, val loss: 1.202901840209961
Epoch 1510, training loss: 12.014256477355957 = 0.01741141267120838 + 2.0 * 5.998422622680664
Epoch 1510, val loss: 1.20613694190979
Epoch 1520, training loss: 12.013577461242676 = 0.017074253410100937 + 2.0 * 5.998251438140869
Epoch 1520, val loss: 1.2096240520477295
Epoch 1530, training loss: 12.017651557922363 = 0.016748109832406044 + 2.0 * 6.000451564788818
Epoch 1530, val loss: 1.212885856628418
Epoch 1540, training loss: 12.010950088500977 = 0.01643071323633194 + 2.0 * 5.997259616851807
Epoch 1540, val loss: 1.2161991596221924
Epoch 1550, training loss: 12.010528564453125 = 0.0161235723644495 + 2.0 * 5.997202396392822
Epoch 1550, val loss: 1.2195320129394531
Epoch 1560, training loss: 12.015165328979492 = 0.01582268998026848 + 2.0 * 5.999671459197998
Epoch 1560, val loss: 1.222699761390686
Epoch 1570, training loss: 12.008366584777832 = 0.01553243026137352 + 2.0 * 5.996417045593262
Epoch 1570, val loss: 1.2259254455566406
Epoch 1580, training loss: 12.009669303894043 = 0.015249795280396938 + 2.0 * 5.997209548950195
Epoch 1580, val loss: 1.2291922569274902
Epoch 1590, training loss: 12.00736141204834 = 0.014973313547670841 + 2.0 * 5.996193885803223
Epoch 1590, val loss: 1.2323616743087769
Epoch 1600, training loss: 12.014593124389648 = 0.014706292189657688 + 2.0 * 5.999943256378174
Epoch 1600, val loss: 1.2355318069458008
Epoch 1610, training loss: 12.005508422851562 = 0.014446577988564968 + 2.0 * 5.99553108215332
Epoch 1610, val loss: 1.2385845184326172
Epoch 1620, training loss: 12.003751754760742 = 0.014194495044648647 + 2.0 * 5.994778633117676
Epoch 1620, val loss: 1.2417919635772705
Epoch 1630, training loss: 12.009891510009766 = 0.013949072919785976 + 2.0 * 5.997971057891846
Epoch 1630, val loss: 1.244805097579956
Epoch 1640, training loss: 12.002172470092773 = 0.013707767240703106 + 2.0 * 5.994232177734375
Epoch 1640, val loss: 1.2477951049804688
Epoch 1650, training loss: 12.004639625549316 = 0.013475298881530762 + 2.0 * 5.995582103729248
Epoch 1650, val loss: 1.2507901191711426
Epoch 1660, training loss: 12.002558708190918 = 0.013249337673187256 + 2.0 * 5.994654655456543
Epoch 1660, val loss: 1.253844976425171
Epoch 1670, training loss: 12.003950119018555 = 0.013028235174715519 + 2.0 * 5.9954609870910645
Epoch 1670, val loss: 1.2567555904388428
Epoch 1680, training loss: 12.000261306762695 = 0.012813162058591843 + 2.0 * 5.9937238693237305
Epoch 1680, val loss: 1.259749412536621
Epoch 1690, training loss: 12.005477905273438 = 0.01260383240878582 + 2.0 * 5.996437072753906
Epoch 1690, val loss: 1.2626644372940063
Epoch 1700, training loss: 12.002080917358398 = 0.012399515137076378 + 2.0 * 5.994840621948242
Epoch 1700, val loss: 1.265533447265625
Epoch 1710, training loss: 12.001205444335938 = 0.012201851233839989 + 2.0 * 5.99450159072876
Epoch 1710, val loss: 1.2684218883514404
Epoch 1720, training loss: 12.00025463104248 = 0.012008092366158962 + 2.0 * 5.994123458862305
Epoch 1720, val loss: 1.2712429761886597
Epoch 1730, training loss: 11.996383666992188 = 0.011819527484476566 + 2.0 * 5.992281913757324
Epoch 1730, val loss: 1.274071455001831
Epoch 1740, training loss: 11.995887756347656 = 0.01163544412702322 + 2.0 * 5.992125988006592
Epoch 1740, val loss: 1.277003288269043
Epoch 1750, training loss: 12.014041900634766 = 0.01145419105887413 + 2.0 * 6.001293659210205
Epoch 1750, val loss: 1.2796896696090698
Epoch 1760, training loss: 11.997669219970703 = 0.011279125697910786 + 2.0 * 5.993195056915283
Epoch 1760, val loss: 1.2823891639709473
Epoch 1770, training loss: 11.992429733276367 = 0.011110355146229267 + 2.0 * 5.990659713745117
Epoch 1770, val loss: 1.2853405475616455
Epoch 1780, training loss: 11.991104125976562 = 0.010942189022898674 + 2.0 * 5.990080833435059
Epoch 1780, val loss: 1.2880964279174805
Epoch 1790, training loss: 12.007452011108398 = 0.010778074152767658 + 2.0 * 5.9983367919921875
Epoch 1790, val loss: 1.2908533811569214
Epoch 1800, training loss: 11.995553016662598 = 0.010616541840136051 + 2.0 * 5.992468357086182
Epoch 1800, val loss: 1.2932326793670654
Epoch 1810, training loss: 11.997359275817871 = 0.010464160703122616 + 2.0 * 5.993447780609131
Epoch 1810, val loss: 1.296043872833252
Epoch 1820, training loss: 11.993146896362305 = 0.010313279926776886 + 2.0 * 5.991416931152344
Epoch 1820, val loss: 1.2986493110656738
Epoch 1830, training loss: 11.989803314208984 = 0.010165109299123287 + 2.0 * 5.989819049835205
Epoch 1830, val loss: 1.3013468980789185
Epoch 1840, training loss: 11.990150451660156 = 0.010020122863352299 + 2.0 * 5.990065097808838
Epoch 1840, val loss: 1.3040262460708618
Epoch 1850, training loss: 11.995126724243164 = 0.009877512231469154 + 2.0 * 5.992624759674072
Epoch 1850, val loss: 1.3066350221633911
Epoch 1860, training loss: 11.989630699157715 = 0.009737404994666576 + 2.0 * 5.9899468421936035
Epoch 1860, val loss: 1.3092349767684937
Epoch 1870, training loss: 11.98920726776123 = 0.009601622819900513 + 2.0 * 5.989802837371826
Epoch 1870, val loss: 1.3117797374725342
Epoch 1880, training loss: 12.0010986328125 = 0.009468114003539085 + 2.0 * 5.995815277099609
Epoch 1880, val loss: 1.314319372177124
Epoch 1890, training loss: 11.98831844329834 = 0.009338481351733208 + 2.0 * 5.989490032196045
Epoch 1890, val loss: 1.3166682720184326
Epoch 1900, training loss: 11.985471725463867 = 0.009213968180119991 + 2.0 * 5.988128662109375
Epoch 1900, val loss: 1.319341778755188
Epoch 1910, training loss: 11.983908653259277 = 0.00908903032541275 + 2.0 * 5.987409591674805
Epoch 1910, val loss: 1.3217931985855103
Epoch 1920, training loss: 11.986340522766113 = 0.008965758606791496 + 2.0 * 5.988687515258789
Epoch 1920, val loss: 1.3242666721343994
Epoch 1930, training loss: 11.991401672363281 = 0.008845237083733082 + 2.0 * 5.991278171539307
Epoch 1930, val loss: 1.326653242111206
Epoch 1940, training loss: 11.992321968078613 = 0.00872861035168171 + 2.0 * 5.991796493530273
Epoch 1940, val loss: 1.3291335105895996
Epoch 1950, training loss: 11.988935470581055 = 0.008613714948296547 + 2.0 * 5.990160942077637
Epoch 1950, val loss: 1.3314677476882935
Epoch 1960, training loss: 11.986732482910156 = 0.008502543903887272 + 2.0 * 5.989114761352539
Epoch 1960, val loss: 1.333896279335022
Epoch 1970, training loss: 11.983634948730469 = 0.008394219912588596 + 2.0 * 5.9876203536987305
Epoch 1970, val loss: 1.336323618888855
Epoch 1980, training loss: 11.987305641174316 = 0.008286883123219013 + 2.0 * 5.989509582519531
Epoch 1980, val loss: 1.3387173414230347
Epoch 1990, training loss: 11.985933303833008 = 0.008180832490324974 + 2.0 * 5.9888763427734375
Epoch 1990, val loss: 1.3409370183944702
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8000
Overall ASR: 0.6347
Flip ASR: 0.5733/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.6875057220459 = 1.9398796558380127 + 2.0 * 8.373812675476074
Epoch 0, val loss: 1.9425528049468994
Epoch 10, training loss: 18.67630386352539 = 1.9302786588668823 + 2.0 * 8.37301254272461
Epoch 10, val loss: 1.9323203563690186
Epoch 20, training loss: 18.652740478515625 = 1.9183510541915894 + 2.0 * 8.367195129394531
Epoch 20, val loss: 1.9195549488067627
Epoch 30, training loss: 18.55560302734375 = 1.9027081727981567 + 2.0 * 8.326447486877441
Epoch 30, val loss: 1.9029057025909424
Epoch 40, training loss: 17.984302520751953 = 1.8848052024841309 + 2.0 * 8.049748420715332
Epoch 40, val loss: 1.884170651435852
Epoch 50, training loss: 16.33733558654785 = 1.865709900856018 + 2.0 * 7.235813140869141
Epoch 50, val loss: 1.8643949031829834
Epoch 60, training loss: 15.547342300415039 = 1.8520716428756714 + 2.0 * 6.847635269165039
Epoch 60, val loss: 1.8512235879898071
Epoch 70, training loss: 15.128738403320312 = 1.8393751382827759 + 2.0 * 6.644681453704834
Epoch 70, val loss: 1.8385002613067627
Epoch 80, training loss: 14.92012882232666 = 1.8286973237991333 + 2.0 * 6.545715808868408
Epoch 80, val loss: 1.8280203342437744
Epoch 90, training loss: 14.741082191467285 = 1.819008708000183 + 2.0 * 6.461036682128906
Epoch 90, val loss: 1.818116545677185
Epoch 100, training loss: 14.599227905273438 = 1.8108245134353638 + 2.0 * 6.394201755523682
Epoch 100, val loss: 1.8093756437301636
Epoch 110, training loss: 14.498490333557129 = 1.8027342557907104 + 2.0 * 6.3478779792785645
Epoch 110, val loss: 1.8006377220153809
Epoch 120, training loss: 14.402000427246094 = 1.794386386871338 + 2.0 * 6.303806781768799
Epoch 120, val loss: 1.791700005531311
Epoch 130, training loss: 14.321319580078125 = 1.7860995531082153 + 2.0 * 6.2676100730896
Epoch 130, val loss: 1.783211588859558
Epoch 140, training loss: 14.253213882446289 = 1.7778111696243286 + 2.0 * 6.237701416015625
Epoch 140, val loss: 1.7750039100646973
Epoch 150, training loss: 14.201265335083008 = 1.7690833806991577 + 2.0 * 6.216091156005859
Epoch 150, val loss: 1.7666056156158447
Epoch 160, training loss: 14.155117988586426 = 1.7593343257904053 + 2.0 * 6.197891712188721
Epoch 160, val loss: 1.757782220840454
Epoch 170, training loss: 14.115403175354004 = 1.7482903003692627 + 2.0 * 6.18355655670166
Epoch 170, val loss: 1.7480792999267578
Epoch 180, training loss: 14.0801362991333 = 1.735541582107544 + 2.0 * 6.172297477722168
Epoch 180, val loss: 1.7372773885726929
Epoch 190, training loss: 14.045207023620605 = 1.720933198928833 + 2.0 * 6.162137031555176
Epoch 190, val loss: 1.7249928712844849
Epoch 200, training loss: 14.011094093322754 = 1.7037546634674072 + 2.0 * 6.153669834136963
Epoch 200, val loss: 1.7107841968536377
Epoch 210, training loss: 13.9756498336792 = 1.683207631111145 + 2.0 * 6.146221160888672
Epoch 210, val loss: 1.693872332572937
Epoch 220, training loss: 13.945005416870117 = 1.6590367555618286 + 2.0 * 6.142984390258789
Epoch 220, val loss: 1.6744410991668701
Epoch 230, training loss: 13.900113105773926 = 1.6309196949005127 + 2.0 * 6.134596824645996
Epoch 230, val loss: 1.6517964601516724
Epoch 240, training loss: 13.854757308959961 = 1.5978732109069824 + 2.0 * 6.12844181060791
Epoch 240, val loss: 1.6252168416976929
Epoch 250, training loss: 13.81485366821289 = 1.559298038482666 + 2.0 * 6.127778053283691
Epoch 250, val loss: 1.5946229696273804
Epoch 260, training loss: 13.756449699401855 = 1.516575574874878 + 2.0 * 6.119936943054199
Epoch 260, val loss: 1.5605568885803223
Epoch 270, training loss: 13.697041511535645 = 1.4696452617645264 + 2.0 * 6.1136980056762695
Epoch 270, val loss: 1.5232356786727905
Epoch 280, training loss: 13.635519027709961 = 1.4190068244934082 + 2.0 * 6.1082563400268555
Epoch 280, val loss: 1.483239769935608
Epoch 290, training loss: 13.573301315307617 = 1.365823745727539 + 2.0 * 6.103738784790039
Epoch 290, val loss: 1.4414699077606201
Epoch 300, training loss: 13.530267715454102 = 1.3117966651916504 + 2.0 * 6.1092352867126465
Epoch 300, val loss: 1.399619221687317
Epoch 310, training loss: 13.457184791564941 = 1.2608507871627808 + 2.0 * 6.0981669425964355
Epoch 310, val loss: 1.3598178625106812
Epoch 320, training loss: 13.397932052612305 = 1.212083339691162 + 2.0 * 6.092924118041992
Epoch 320, val loss: 1.3221906423568726
Epoch 330, training loss: 13.34344482421875 = 1.1656256914138794 + 2.0 * 6.08890962600708
Epoch 330, val loss: 1.2863246202468872
Epoch 340, training loss: 13.298239707946777 = 1.1212328672409058 + 2.0 * 6.088503360748291
Epoch 340, val loss: 1.2521743774414062
Epoch 350, training loss: 13.247295379638672 = 1.0790168046951294 + 2.0 * 6.084139347076416
Epoch 350, val loss: 1.2202167510986328
Epoch 360, training loss: 13.200016021728516 = 1.0388790369033813 + 2.0 * 6.080568313598633
Epoch 360, val loss: 1.1896615028381348
Epoch 370, training loss: 13.155180931091309 = 1.0000654458999634 + 2.0 * 6.077557563781738
Epoch 370, val loss: 1.160083293914795
Epoch 380, training loss: 13.120345115661621 = 0.9625826478004456 + 2.0 * 6.07888126373291
Epoch 380, val loss: 1.131550669670105
Epoch 390, training loss: 13.074028968811035 = 0.926830530166626 + 2.0 * 6.073599338531494
Epoch 390, val loss: 1.1040828227996826
Epoch 400, training loss: 13.032196998596191 = 0.8919775485992432 + 2.0 * 6.070109844207764
Epoch 400, val loss: 1.0774673223495483
Epoch 410, training loss: 12.992345809936523 = 0.8579297661781311 + 2.0 * 6.0672078132629395
Epoch 410, val loss: 1.0513850450515747
Epoch 420, training loss: 12.958197593688965 = 0.8244761824607849 + 2.0 * 6.066860675811768
Epoch 420, val loss: 1.0257905721664429
Epoch 430, training loss: 12.929027557373047 = 0.7919936180114746 + 2.0 * 6.068516731262207
Epoch 430, val loss: 1.0013245344161987
Epoch 440, training loss: 12.88379955291748 = 0.7609302401542664 + 2.0 * 6.061434745788574
Epoch 440, val loss: 0.9778499007225037
Epoch 450, training loss: 12.850255966186523 = 0.7306541204452515 + 2.0 * 6.05980110168457
Epoch 450, val loss: 0.9553037285804749
Epoch 460, training loss: 12.816433906555176 = 0.7010751366615295 + 2.0 * 6.057679176330566
Epoch 460, val loss: 0.9336232542991638
Epoch 470, training loss: 12.794900894165039 = 0.6723056435585022 + 2.0 * 6.061297416687012
Epoch 470, val loss: 0.9128643274307251
Epoch 480, training loss: 12.762199401855469 = 0.6445927619934082 + 2.0 * 6.058803558349609
Epoch 480, val loss: 0.8932679891586304
Epoch 490, training loss: 12.72513484954834 = 0.6178858280181885 + 2.0 * 6.053624629974365
Epoch 490, val loss: 0.8749002814292908
Epoch 500, training loss: 12.694942474365234 = 0.5920312404632568 + 2.0 * 6.051455497741699
Epoch 500, val loss: 0.8575491309165955
Epoch 510, training loss: 12.667837142944336 = 0.5668479800224304 + 2.0 * 6.05049467086792
Epoch 510, val loss: 0.8411727547645569
Epoch 520, training loss: 12.650077819824219 = 0.5424423217773438 + 2.0 * 6.0538177490234375
Epoch 520, val loss: 0.8257673978805542
Epoch 530, training loss: 12.619637489318848 = 0.5187974572181702 + 2.0 * 6.050419807434082
Epoch 530, val loss: 0.811398446559906
Epoch 540, training loss: 12.588099479675293 = 0.4959862232208252 + 2.0 * 6.046056747436523
Epoch 540, val loss: 0.7978683710098267
Epoch 550, training loss: 12.5621976852417 = 0.4737665057182312 + 2.0 * 6.044215679168701
Epoch 550, val loss: 0.7850642204284668
Epoch 560, training loss: 12.552138328552246 = 0.452160120010376 + 2.0 * 6.049989223480225
Epoch 560, val loss: 0.7729406356811523
Epoch 570, training loss: 12.515176773071289 = 0.4313676953315735 + 2.0 * 6.041904449462891
Epoch 570, val loss: 0.7615998983383179
Epoch 580, training loss: 12.49238395690918 = 0.41141265630722046 + 2.0 * 6.040485858917236
Epoch 580, val loss: 0.7509701251983643
Epoch 590, training loss: 12.47036075592041 = 0.3921801745891571 + 2.0 * 6.039090156555176
Epoch 590, val loss: 0.7409616708755493
Epoch 600, training loss: 12.460110664367676 = 0.3736802935600281 + 2.0 * 6.043215274810791
Epoch 600, val loss: 0.7315460443496704
Epoch 610, training loss: 12.433514595031738 = 0.35603174567222595 + 2.0 * 6.038741588592529
Epoch 610, val loss: 0.7228204607963562
Epoch 620, training loss: 12.420790672302246 = 0.3391372859477997 + 2.0 * 6.040826797485352
Epoch 620, val loss: 0.7147693634033203
Epoch 630, training loss: 12.394886016845703 = 0.3230459690093994 + 2.0 * 6.035920143127441
Epoch 630, val loss: 0.7072827816009521
Epoch 640, training loss: 12.375021934509277 = 0.30754801630973816 + 2.0 * 6.0337371826171875
Epoch 640, val loss: 0.7004252672195435
Epoch 650, training loss: 12.359922409057617 = 0.2926170527935028 + 2.0 * 6.0336527824401855
Epoch 650, val loss: 0.6941041350364685
Epoch 660, training loss: 12.34925651550293 = 0.2782886326313019 + 2.0 * 6.0354838371276855
Epoch 660, val loss: 0.6882716417312622
Epoch 670, training loss: 12.330282211303711 = 0.2646421492099762 + 2.0 * 6.032820224761963
Epoch 670, val loss: 0.6829121112823486
Epoch 680, training loss: 12.313212394714355 = 0.25154680013656616 + 2.0 * 6.030832767486572
Epoch 680, val loss: 0.6781209111213684
Epoch 690, training loss: 12.305445671081543 = 0.2390183061361313 + 2.0 * 6.0332136154174805
Epoch 690, val loss: 0.6737517714500427
Epoch 700, training loss: 12.28847599029541 = 0.2270362228155136 + 2.0 * 6.030719757080078
Epoch 700, val loss: 0.6698457598686218
Epoch 710, training loss: 12.271434783935547 = 0.21564339101314545 + 2.0 * 6.027895927429199
Epoch 710, val loss: 0.6663982272148132
Epoch 720, training loss: 12.257096290588379 = 0.2047661542892456 + 2.0 * 6.026165008544922
Epoch 720, val loss: 0.6633861660957336
Epoch 730, training loss: 12.265654563903809 = 0.19442647695541382 + 2.0 * 6.035614013671875
Epoch 730, val loss: 0.6607456207275391
Epoch 740, training loss: 12.23807430267334 = 0.18468840420246124 + 2.0 * 6.026692867279053
Epoch 740, val loss: 0.6584604382514954
Epoch 750, training loss: 12.22677230834961 = 0.17556802928447723 + 2.0 * 6.025602340698242
Epoch 750, val loss: 0.6566795706748962
Epoch 760, training loss: 12.226208686828613 = 0.16697590053081512 + 2.0 * 6.029616355895996
Epoch 760, val loss: 0.6552921533584595
Epoch 770, training loss: 12.207018852233887 = 0.1589040458202362 + 2.0 * 6.024057388305664
Epoch 770, val loss: 0.6542162895202637
Epoch 780, training loss: 12.194665908813477 = 0.15136681497097015 + 2.0 * 6.021649360656738
Epoch 780, val loss: 0.6535362601280212
Epoch 790, training loss: 12.18563461303711 = 0.14427989721298218 + 2.0 * 6.02067756652832
Epoch 790, val loss: 0.6532142758369446
Epoch 800, training loss: 12.19873046875 = 0.13765178620815277 + 2.0 * 6.030539512634277
Epoch 800, val loss: 0.6531856656074524
Epoch 810, training loss: 12.16987133026123 = 0.1314767599105835 + 2.0 * 6.019197463989258
Epoch 810, val loss: 0.6533928513526917
Epoch 820, training loss: 12.176485061645508 = 0.12574239075183868 + 2.0 * 6.025371551513672
Epoch 820, val loss: 0.6539026498794556
Epoch 830, training loss: 12.160796165466309 = 0.12040366977453232 + 2.0 * 6.020196437835693
Epoch 830, val loss: 0.6545864343643188
Epoch 840, training loss: 12.150178909301758 = 0.11541399359703064 + 2.0 * 6.017382621765137
Epoch 840, val loss: 0.6555227041244507
Epoch 850, training loss: 12.143941879272461 = 0.11071933805942535 + 2.0 * 6.016611099243164
Epoch 850, val loss: 0.6566586494445801
Epoch 860, training loss: 12.149188995361328 = 0.10629119724035263 + 2.0 * 6.021449089050293
Epoch 860, val loss: 0.6579200625419617
Epoch 870, training loss: 12.138306617736816 = 0.1021682396531105 + 2.0 * 6.018069267272949
Epoch 870, val loss: 0.6592696309089661
Epoch 880, training loss: 12.128752708435059 = 0.09828848391771317 + 2.0 * 6.015232086181641
Epoch 880, val loss: 0.6608254909515381
Epoch 890, training loss: 12.124069213867188 = 0.0946255773305893 + 2.0 * 6.014721870422363
Epoch 890, val loss: 0.6624529957771301
Epoch 900, training loss: 12.127876281738281 = 0.09115771949291229 + 2.0 * 6.018359184265137
Epoch 900, val loss: 0.664134681224823
Epoch 910, training loss: 12.116368293762207 = 0.08788064122200012 + 2.0 * 6.0142436027526855
Epoch 910, val loss: 0.6659446358680725
Epoch 920, training loss: 12.109709739685059 = 0.08477486670017242 + 2.0 * 6.012467384338379
Epoch 920, val loss: 0.6678265333175659
Epoch 930, training loss: 12.107678413391113 = 0.08182024955749512 + 2.0 * 6.0129289627075195
Epoch 930, val loss: 0.6697915196418762
Epoch 940, training loss: 12.109149932861328 = 0.07901789993047714 + 2.0 * 6.015066146850586
Epoch 940, val loss: 0.6717398166656494
Epoch 950, training loss: 12.10075855255127 = 0.0763648971915245 + 2.0 * 6.012197017669678
Epoch 950, val loss: 0.6737892031669617
Epoch 960, training loss: 12.092541694641113 = 0.07384702563285828 + 2.0 * 6.009347438812256
Epoch 960, val loss: 0.6759093403816223
Epoch 970, training loss: 12.089637756347656 = 0.07143215835094452 + 2.0 * 6.009102821350098
Epoch 970, val loss: 0.6780627965927124
Epoch 980, training loss: 12.094963073730469 = 0.06910498440265656 + 2.0 * 6.0129289627075195
Epoch 980, val loss: 0.6802854537963867
Epoch 990, training loss: 12.090853691101074 = 0.06689076125621796 + 2.0 * 6.01198148727417
Epoch 990, val loss: 0.6824346780776978
Epoch 1000, training loss: 12.085433959960938 = 0.06477642059326172 + 2.0 * 6.010328769683838
Epoch 1000, val loss: 0.684674084186554
Epoch 1010, training loss: 12.077900886535645 = 0.0627550482749939 + 2.0 * 6.007573127746582
Epoch 1010, val loss: 0.6869364976882935
Epoch 1020, training loss: 12.074543952941895 = 0.06080658733844757 + 2.0 * 6.006868839263916
Epoch 1020, val loss: 0.6892256736755371
Epoch 1030, training loss: 12.080828666687012 = 0.0589236319065094 + 2.0 * 6.010952472686768
Epoch 1030, val loss: 0.6915162205696106
Epoch 1040, training loss: 12.071257591247559 = 0.05711263418197632 + 2.0 * 6.007072448730469
Epoch 1040, val loss: 0.6937801837921143
Epoch 1050, training loss: 12.072003364562988 = 0.05538022518157959 + 2.0 * 6.008311748504639
Epoch 1050, val loss: 0.6960884928703308
Epoch 1060, training loss: 12.064606666564941 = 0.053706590086221695 + 2.0 * 6.005450248718262
Epoch 1060, val loss: 0.6984239816665649
Epoch 1070, training loss: 12.062264442443848 = 0.052088603377342224 + 2.0 * 6.005087852478027
Epoch 1070, val loss: 0.7008070349693298
Epoch 1080, training loss: 12.06338119506836 = 0.05051989108324051 + 2.0 * 6.006430625915527
Epoch 1080, val loss: 0.7032055854797363
Epoch 1090, training loss: 12.057012557983398 = 0.04900382459163666 + 2.0 * 6.00400447845459
Epoch 1090, val loss: 0.705559253692627
Epoch 1100, training loss: 12.066123962402344 = 0.04753816872835159 + 2.0 * 6.009293079376221
Epoch 1100, val loss: 0.7079276442527771
Epoch 1110, training loss: 12.056084632873535 = 0.04611925408244133 + 2.0 * 6.0049824714660645
Epoch 1110, val loss: 0.7103105783462524
Epoch 1120, training loss: 12.050976753234863 = 0.04475310817360878 + 2.0 * 6.003111839294434
Epoch 1120, val loss: 0.7127242684364319
Epoch 1130, training loss: 12.048160552978516 = 0.04342534393072128 + 2.0 * 6.0023674964904785
Epoch 1130, val loss: 0.7151026129722595
Epoch 1140, training loss: 12.051459312438965 = 0.04212673753499985 + 2.0 * 6.004666328430176
Epoch 1140, val loss: 0.7175354957580566
Epoch 1150, training loss: 12.047608375549316 = 0.040858738124370575 + 2.0 * 6.003375053405762
Epoch 1150, val loss: 0.7198902368545532
Epoch 1160, training loss: 12.046679496765137 = 0.03962787240743637 + 2.0 * 6.003525733947754
Epoch 1160, val loss: 0.7223036289215088
Epoch 1170, training loss: 12.040328979492188 = 0.03844373673200607 + 2.0 * 6.000942707061768
Epoch 1170, val loss: 0.7246764302253723
Epoch 1180, training loss: 12.04194164276123 = 0.03729560226202011 + 2.0 * 6.002323150634766
Epoch 1180, val loss: 0.7270578742027283
Epoch 1190, training loss: 12.041687965393066 = 0.03617718815803528 + 2.0 * 6.002755165100098
Epoch 1190, val loss: 0.7294742465019226
Epoch 1200, training loss: 12.037115097045898 = 0.03509313613176346 + 2.0 * 6.001010894775391
Epoch 1200, val loss: 0.731921911239624
Epoch 1210, training loss: 12.034514427185059 = 0.03404809162020683 + 2.0 * 6.000233173370361
Epoch 1210, val loss: 0.7343313694000244
Epoch 1220, training loss: 12.037311553955078 = 0.03302818164229393 + 2.0 * 6.00214147567749
Epoch 1220, val loss: 0.7367532849311829
Epoch 1230, training loss: 12.031112670898438 = 0.03204529732465744 + 2.0 * 5.999533653259277
Epoch 1230, val loss: 0.7392070293426514
Epoch 1240, training loss: 12.029211044311523 = 0.03109617903828621 + 2.0 * 5.999057292938232
Epoch 1240, val loss: 0.7416707873344421
Epoch 1250, training loss: 12.044456481933594 = 0.03017311729490757 + 2.0 * 6.007141590118408
Epoch 1250, val loss: 0.7441185116767883
Epoch 1260, training loss: 12.032576560974121 = 0.02929004468023777 + 2.0 * 6.001643180847168
Epoch 1260, val loss: 0.7465858459472656
Epoch 1270, training loss: 12.02385139465332 = 0.028444534167647362 + 2.0 * 5.997703552246094
Epoch 1270, val loss: 0.7490955591201782
Epoch 1280, training loss: 12.026080131530762 = 0.027630072087049484 + 2.0 * 5.99922513961792
Epoch 1280, val loss: 0.7515881061553955
Epoch 1290, training loss: 12.025111198425293 = 0.02684510499238968 + 2.0 * 5.999133110046387
Epoch 1290, val loss: 0.7540785074234009
Epoch 1300, training loss: 12.021184921264648 = 0.02609260380268097 + 2.0 * 5.997546195983887
Epoch 1300, val loss: 0.756622850894928
Epoch 1310, training loss: 12.017807006835938 = 0.02537153847515583 + 2.0 * 5.996217727661133
Epoch 1310, val loss: 0.7591851353645325
Epoch 1320, training loss: 12.01676082611084 = 0.024676160886883736 + 2.0 * 5.996042251586914
Epoch 1320, val loss: 0.7617514729499817
Epoch 1330, training loss: 12.03524398803711 = 0.024009183049201965 + 2.0 * 6.005617618560791
Epoch 1330, val loss: 0.7643096446990967
Epoch 1340, training loss: 12.021953582763672 = 0.023361997678875923 + 2.0 * 5.999295711517334
Epoch 1340, val loss: 0.766922652721405
Epoch 1350, training loss: 12.015741348266602 = 0.022756803780794144 + 2.0 * 5.996492385864258
Epoch 1350, val loss: 0.7694916725158691
Epoch 1360, training loss: 12.012989044189453 = 0.022169047966599464 + 2.0 * 5.995409965515137
Epoch 1360, val loss: 0.7721135020256042
Epoch 1370, training loss: 12.019063949584961 = 0.021601136773824692 + 2.0 * 5.99873161315918
Epoch 1370, val loss: 0.7747074961662292
Epoch 1380, training loss: 12.016877174377441 = 0.021056251600384712 + 2.0 * 5.997910499572754
Epoch 1380, val loss: 0.777338445186615
Epoch 1390, training loss: 12.010320663452148 = 0.020536407828330994 + 2.0 * 5.994892120361328
Epoch 1390, val loss: 0.7799596190452576
Epoch 1400, training loss: 12.00847053527832 = 0.020036542788147926 + 2.0 * 5.9942169189453125
Epoch 1400, val loss: 0.782592236995697
Epoch 1410, training loss: 12.007704734802246 = 0.019552908837795258 + 2.0 * 5.994075775146484
Epoch 1410, val loss: 0.7852330803871155
Epoch 1420, training loss: 12.014703750610352 = 0.019084470346570015 + 2.0 * 5.997809410095215
Epoch 1420, val loss: 0.787912905216217
Epoch 1430, training loss: 12.012760162353516 = 0.01863684132695198 + 2.0 * 5.997061729431152
Epoch 1430, val loss: 0.7905464172363281
Epoch 1440, training loss: 12.008621215820312 = 0.01820755936205387 + 2.0 * 5.995206832885742
Epoch 1440, val loss: 0.7931457161903381
Epoch 1450, training loss: 12.00710678100586 = 0.017798125743865967 + 2.0 * 5.994654178619385
Epoch 1450, val loss: 0.7957207560539246
Epoch 1460, training loss: 12.00285816192627 = 0.01739807426929474 + 2.0 * 5.992730140686035
Epoch 1460, val loss: 0.7983391284942627
Epoch 1470, training loss: 12.004646301269531 = 0.0170122142881155 + 2.0 * 5.99381685256958
Epoch 1470, val loss: 0.8009629249572754
Epoch 1480, training loss: 12.002108573913574 = 0.01663826033473015 + 2.0 * 5.992735385894775
Epoch 1480, val loss: 0.8035866022109985
Epoch 1490, training loss: 12.004011154174805 = 0.016278982162475586 + 2.0 * 5.993865966796875
Epoch 1490, val loss: 0.8062143921852112
Epoch 1500, training loss: 11.998869895935059 = 0.015932250767946243 + 2.0 * 5.991468906402588
Epoch 1500, val loss: 0.8087963461875916
Epoch 1510, training loss: 12.014238357543945 = 0.0155948456376791 + 2.0 * 5.999321937561035
Epoch 1510, val loss: 0.8114150762557983
Epoch 1520, training loss: 12.00180721282959 = 0.01527493353933096 + 2.0 * 5.9932661056518555
Epoch 1520, val loss: 0.8139010667800903
Epoch 1530, training loss: 11.998332977294922 = 0.014962608925998211 + 2.0 * 5.991685390472412
Epoch 1530, val loss: 0.816472589969635
Epoch 1540, training loss: 11.997708320617676 = 0.01466139405965805 + 2.0 * 5.991523265838623
Epoch 1540, val loss: 0.8189758658409119
Epoch 1550, training loss: 11.999133110046387 = 0.0143655464053154 + 2.0 * 5.99238395690918
Epoch 1550, val loss: 0.8215422034263611
Epoch 1560, training loss: 11.996187210083008 = 0.014079850167036057 + 2.0 * 5.991053581237793
Epoch 1560, val loss: 0.8240696787834167
Epoch 1570, training loss: 11.997441291809082 = 0.013802924193441868 + 2.0 * 5.991819381713867
Epoch 1570, val loss: 0.8265957236289978
Epoch 1580, training loss: 12.000911712646484 = 0.013537043705582619 + 2.0 * 5.993687152862549
Epoch 1580, val loss: 0.8290682435035706
Epoch 1590, training loss: 11.994791984558105 = 0.013279606588184834 + 2.0 * 5.990756034851074
Epoch 1590, val loss: 0.8315486311912537
Epoch 1600, training loss: 11.991697311401367 = 0.01303222868591547 + 2.0 * 5.989332675933838
Epoch 1600, val loss: 0.8339602947235107
Epoch 1610, training loss: 11.989404678344727 = 0.012789328582584858 + 2.0 * 5.988307476043701
Epoch 1610, val loss: 0.8364094495773315
Epoch 1620, training loss: 11.988804817199707 = 0.012551630847156048 + 2.0 * 5.988126754760742
Epoch 1620, val loss: 0.8388652205467224
Epoch 1630, training loss: 11.997827529907227 = 0.012319433502852917 + 2.0 * 5.992753982543945
Epoch 1630, val loss: 0.8413323163986206
Epoch 1640, training loss: 11.996736526489258 = 0.012096196413040161 + 2.0 * 5.9923200607299805
Epoch 1640, val loss: 0.8437166810035706
Epoch 1650, training loss: 11.991125106811523 = 0.01188283134251833 + 2.0 * 5.989621162414551
Epoch 1650, val loss: 0.8461036682128906
Epoch 1660, training loss: 11.991312026977539 = 0.011675835587084293 + 2.0 * 5.989818096160889
Epoch 1660, val loss: 0.8484125733375549
Epoch 1670, training loss: 11.99073314666748 = 0.01147291250526905 + 2.0 * 5.989630222320557
Epoch 1670, val loss: 0.8507437705993652
Epoch 1680, training loss: 11.98793888092041 = 0.011275184340775013 + 2.0 * 5.9883317947387695
Epoch 1680, val loss: 0.8530685305595398
Epoch 1690, training loss: 11.99028205871582 = 0.011084853671491146 + 2.0 * 5.989598751068115
Epoch 1690, val loss: 0.8553835153579712
Epoch 1700, training loss: 11.985465049743652 = 0.010896439664065838 + 2.0 * 5.987284183502197
Epoch 1700, val loss: 0.857693612575531
Epoch 1710, training loss: 11.988160133361816 = 0.010714448988437653 + 2.0 * 5.988722801208496
Epoch 1710, val loss: 0.8599917888641357
Epoch 1720, training loss: 11.984749794006348 = 0.010536459274590015 + 2.0 * 5.987106800079346
Epoch 1720, val loss: 0.8622841835021973
Epoch 1730, training loss: 11.982889175415039 = 0.010362752713263035 + 2.0 * 5.986263275146484
Epoch 1730, val loss: 0.864594042301178
Epoch 1740, training loss: 11.984557151794434 = 0.010193722322583199 + 2.0 * 5.987181663513184
Epoch 1740, val loss: 0.8668822050094604
Epoch 1750, training loss: 11.983786582946777 = 0.010028333403170109 + 2.0 * 5.986879348754883
Epoch 1750, val loss: 0.8691188097000122
Epoch 1760, training loss: 12.005627632141113 = 0.009869512170553207 + 2.0 * 5.9978790283203125
Epoch 1760, val loss: 0.8713679909706116
Epoch 1770, training loss: 11.98225212097168 = 0.009715589694678783 + 2.0 * 5.986268043518066
Epoch 1770, val loss: 0.8735578060150146
Epoch 1780, training loss: 11.981828689575195 = 0.009567521512508392 + 2.0 * 5.986130714416504
Epoch 1780, val loss: 0.8757194876670837
Epoch 1790, training loss: 11.97810173034668 = 0.009421933442354202 + 2.0 * 5.984339714050293
Epoch 1790, val loss: 0.8778377771377563
Epoch 1800, training loss: 11.978131294250488 = 0.009276124648749828 + 2.0 * 5.984427452087402
Epoch 1800, val loss: 0.8800018429756165
Epoch 1810, training loss: 12.002936363220215 = 0.009132812730967999 + 2.0 * 5.996901988983154
Epoch 1810, val loss: 0.8822110295295715
Epoch 1820, training loss: 11.985066413879395 = 0.008998261764645576 + 2.0 * 5.988034248352051
Epoch 1820, val loss: 0.8842809796333313
Epoch 1830, training loss: 11.978472709655762 = 0.00886507984250784 + 2.0 * 5.984803676605225
Epoch 1830, val loss: 0.8864171504974365
Epoch 1840, training loss: 11.97630786895752 = 0.008735457435250282 + 2.0 * 5.983786106109619
Epoch 1840, val loss: 0.8884944319725037
Epoch 1850, training loss: 11.995428085327148 = 0.008605379611253738 + 2.0 * 5.993411540985107
Epoch 1850, val loss: 0.8906018733978271
Epoch 1860, training loss: 11.984599113464355 = 0.008483288809657097 + 2.0 * 5.988058090209961
Epoch 1860, val loss: 0.892624020576477
Epoch 1870, training loss: 11.9760160446167 = 0.008362466469407082 + 2.0 * 5.983826637268066
Epoch 1870, val loss: 0.8946548700332642
Epoch 1880, training loss: 11.973557472229004 = 0.008245308883488178 + 2.0 * 5.982656002044678
Epoch 1880, val loss: 0.8966617584228516
Epoch 1890, training loss: 11.975013732910156 = 0.008127830922603607 + 2.0 * 5.983442783355713
Epoch 1890, val loss: 0.8987086415290833
Epoch 1900, training loss: 11.984478950500488 = 0.008013115264475346 + 2.0 * 5.9882330894470215
Epoch 1900, val loss: 0.9007453918457031
Epoch 1910, training loss: 11.97278881072998 = 0.007903007790446281 + 2.0 * 5.982442855834961
Epoch 1910, val loss: 0.9027427434921265
Epoch 1920, training loss: 11.974320411682129 = 0.007795596029609442 + 2.0 * 5.983262538909912
Epoch 1920, val loss: 0.9047224521636963
Epoch 1930, training loss: 11.98384952545166 = 0.007689864840358496 + 2.0 * 5.988080024719238
Epoch 1930, val loss: 0.9066615700721741
Epoch 1940, training loss: 11.971736907958984 = 0.007585423998534679 + 2.0 * 5.9820756912231445
Epoch 1940, val loss: 0.9086548686027527
Epoch 1950, training loss: 11.9710054397583 = 0.007483750581741333 + 2.0 * 5.9817609786987305
Epoch 1950, val loss: 0.9106101989746094
Epoch 1960, training loss: 11.980727195739746 = 0.007383984047919512 + 2.0 * 5.986671447753906
Epoch 1960, val loss: 0.9125839471817017
Epoch 1970, training loss: 11.972545623779297 = 0.007286475505679846 + 2.0 * 5.982629776000977
Epoch 1970, val loss: 0.9144635796546936
Epoch 1980, training loss: 11.970898628234863 = 0.007192337419837713 + 2.0 * 5.981853008270264
Epoch 1980, val loss: 0.9163984656333923
Epoch 1990, training loss: 11.969671249389648 = 0.0070991977117955685 + 2.0 * 5.98128604888916
Epoch 1990, val loss: 0.9182809591293335
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.7380
Flip ASR: 0.7022/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.702348709106445 = 1.9545589685440063 + 2.0 * 8.373894691467285
Epoch 0, val loss: 1.9491307735443115
Epoch 10, training loss: 18.691822052001953 = 1.9446439743041992 + 2.0 * 8.373588562011719
Epoch 10, val loss: 1.9392210245132446
Epoch 20, training loss: 18.67559242248535 = 1.9324787855148315 + 2.0 * 8.371557235717773
Epoch 20, val loss: 1.9269545078277588
Epoch 30, training loss: 18.62661361694336 = 1.9157525300979614 + 2.0 * 8.355430603027344
Epoch 30, val loss: 1.9101287126541138
Epoch 40, training loss: 18.34367561340332 = 1.8940333127975464 + 2.0 * 8.224821090698242
Epoch 40, val loss: 1.8888108730316162
Epoch 50, training loss: 16.970640182495117 = 1.868324637413025 + 2.0 * 7.5511579513549805
Epoch 50, val loss: 1.8637080192565918
Epoch 60, training loss: 16.11342430114746 = 1.8484457731246948 + 2.0 * 7.1324896812438965
Epoch 60, val loss: 1.8452553749084473
Epoch 70, training loss: 15.536759376525879 = 1.8363207578659058 + 2.0 * 6.850219249725342
Epoch 70, val loss: 1.8330401182174683
Epoch 80, training loss: 15.249338150024414 = 1.8253204822540283 + 2.0 * 6.712008953094482
Epoch 80, val loss: 1.8224198818206787
Epoch 90, training loss: 15.04066276550293 = 1.8147960901260376 + 2.0 * 6.612933158874512
Epoch 90, val loss: 1.812081217765808
Epoch 100, training loss: 14.877077102661133 = 1.8048793077468872 + 2.0 * 6.536098957061768
Epoch 100, val loss: 1.802762508392334
Epoch 110, training loss: 14.733993530273438 = 1.7971553802490234 + 2.0 * 6.468419075012207
Epoch 110, val loss: 1.7953076362609863
Epoch 120, training loss: 14.626348495483398 = 1.7908190488815308 + 2.0 * 6.417764663696289
Epoch 120, val loss: 1.788728952407837
Epoch 130, training loss: 14.538949966430664 = 1.7846909761428833 + 2.0 * 6.377129554748535
Epoch 130, val loss: 1.7821698188781738
Epoch 140, training loss: 14.4678373336792 = 1.7784358263015747 + 2.0 * 6.344700813293457
Epoch 140, val loss: 1.7757748365402222
Epoch 150, training loss: 14.404518127441406 = 1.7718734741210938 + 2.0 * 6.316322326660156
Epoch 150, val loss: 1.7694852352142334
Epoch 160, training loss: 14.340583801269531 = 1.7649518251419067 + 2.0 * 6.287816047668457
Epoch 160, val loss: 1.763069748878479
Epoch 170, training loss: 14.280142784118652 = 1.7575092315673828 + 2.0 * 6.261316776275635
Epoch 170, val loss: 1.756567120552063
Epoch 180, training loss: 14.230527877807617 = 1.749192476272583 + 2.0 * 6.240667819976807
Epoch 180, val loss: 1.7494735717773438
Epoch 190, training loss: 14.184930801391602 = 1.7394713163375854 + 2.0 * 6.222729682922363
Epoch 190, val loss: 1.7413578033447266
Epoch 200, training loss: 14.146404266357422 = 1.7280884981155396 + 2.0 * 6.209157943725586
Epoch 200, val loss: 1.7318757772445679
Epoch 210, training loss: 14.104762077331543 = 1.7148231267929077 + 2.0 * 6.194969654083252
Epoch 210, val loss: 1.7209742069244385
Epoch 220, training loss: 14.066153526306152 = 1.6995213031768799 + 2.0 * 6.183316230773926
Epoch 220, val loss: 1.70848548412323
Epoch 230, training loss: 14.028244018554688 = 1.6817078590393066 + 2.0 * 6.1732683181762695
Epoch 230, val loss: 1.693986415863037
Epoch 240, training loss: 13.992071151733398 = 1.6607980728149414 + 2.0 * 6.1656365394592285
Epoch 240, val loss: 1.676964521408081
Epoch 250, training loss: 13.950331687927246 = 1.636438012123108 + 2.0 * 6.156946659088135
Epoch 250, val loss: 1.6571581363677979
Epoch 260, training loss: 13.90637493133545 = 1.6080477237701416 + 2.0 * 6.149163722991943
Epoch 260, val loss: 1.6340218782424927
Epoch 270, training loss: 13.861979484558105 = 1.5748369693756104 + 2.0 * 6.143571376800537
Epoch 270, val loss: 1.6068154573440552
Epoch 280, training loss: 13.813380241394043 = 1.5364272594451904 + 2.0 * 6.138476371765137
Epoch 280, val loss: 1.5752840042114258
Epoch 290, training loss: 13.759096145629883 = 1.4925068616867065 + 2.0 * 6.133294582366943
Epoch 290, val loss: 1.5391782522201538
Epoch 300, training loss: 13.698989868164062 = 1.4431427717208862 + 2.0 * 6.127923488616943
Epoch 300, val loss: 1.498434066772461
Epoch 310, training loss: 13.640778541564941 = 1.388599157333374 + 2.0 * 6.126089572906494
Epoch 310, val loss: 1.4534350633621216
Epoch 320, training loss: 13.572219848632812 = 1.3301029205322266 + 2.0 * 6.121058464050293
Epoch 320, val loss: 1.405213713645935
Epoch 330, training loss: 13.504119873046875 = 1.2684270143508911 + 2.0 * 6.117846488952637
Epoch 330, val loss: 1.3545522689819336
Epoch 340, training loss: 13.434876441955566 = 1.2053455114364624 + 2.0 * 6.114765644073486
Epoch 340, val loss: 1.30284583568573
Epoch 350, training loss: 13.36297607421875 = 1.1420680284500122 + 2.0 * 6.110454082489014
Epoch 350, val loss: 1.2509942054748535
Epoch 360, training loss: 13.295633316040039 = 1.0794347524642944 + 2.0 * 6.108099460601807
Epoch 360, val loss: 1.199804663658142
Epoch 370, training loss: 13.232100486755371 = 1.0194511413574219 + 2.0 * 6.106324672698975
Epoch 370, val loss: 1.150789737701416
Epoch 380, training loss: 13.166886329650879 = 0.962640643119812 + 2.0 * 6.102122783660889
Epoch 380, val loss: 1.1049115657806396
Epoch 390, training loss: 13.107355117797852 = 0.9092622399330139 + 2.0 * 6.099046230316162
Epoch 390, val loss: 1.062242865562439
Epoch 400, training loss: 13.051392555236816 = 0.8593783974647522 + 2.0 * 6.096006870269775
Epoch 400, val loss: 1.0229017734527588
Epoch 410, training loss: 13.006396293640137 = 0.8132395148277283 + 2.0 * 6.096578598022461
Epoch 410, val loss: 0.9871587753295898
Epoch 420, training loss: 12.953754425048828 = 0.770763099193573 + 2.0 * 6.091495513916016
Epoch 420, val loss: 0.955261766910553
Epoch 430, training loss: 12.906715393066406 = 0.7312188148498535 + 2.0 * 6.087748050689697
Epoch 430, val loss: 0.9263877868652344
Epoch 440, training loss: 12.86631965637207 = 0.6944418549537659 + 2.0 * 6.085938930511475
Epoch 440, val loss: 0.9004037380218506
Epoch 450, training loss: 12.826685905456543 = 0.6606028079986572 + 2.0 * 6.083041667938232
Epoch 450, val loss: 0.8775625228881836
Epoch 460, training loss: 12.790498733520508 = 0.6290345191955566 + 2.0 * 6.0807318687438965
Epoch 460, val loss: 0.857258677482605
Epoch 470, training loss: 12.764374732971191 = 0.5993603467941284 + 2.0 * 6.082507133483887
Epoch 470, val loss: 0.8390421867370605
Epoch 480, training loss: 12.734308242797852 = 0.5716589689254761 + 2.0 * 6.081324577331543
Epoch 480, val loss: 0.8229641318321228
Epoch 490, training loss: 12.69478988647461 = 0.5458150506019592 + 2.0 * 6.074487209320068
Epoch 490, val loss: 0.8086727261543274
Epoch 500, training loss: 12.664831161499023 = 0.5213998556137085 + 2.0 * 6.071715831756592
Epoch 500, val loss: 0.7960068583488464
Epoch 510, training loss: 12.638042449951172 = 0.4981359541416168 + 2.0 * 6.069953441619873
Epoch 510, val loss: 0.7844443321228027
Epoch 520, training loss: 12.61672592163086 = 0.47599032521247864 + 2.0 * 6.070367813110352
Epoch 520, val loss: 0.7739520072937012
Epoch 530, training loss: 12.588800430297852 = 0.4550517797470093 + 2.0 * 6.0668745040893555
Epoch 530, val loss: 0.7647097110748291
Epoch 540, training loss: 12.566202163696289 = 0.43501710891723633 + 2.0 * 6.0655927658081055
Epoch 540, val loss: 0.7563941478729248
Epoch 550, training loss: 12.542559623718262 = 0.415639728307724 + 2.0 * 6.063459873199463
Epoch 550, val loss: 0.7487556338310242
Epoch 560, training loss: 12.530050277709961 = 0.39692559838294983 + 2.0 * 6.066562175750732
Epoch 560, val loss: 0.7416461706161499
Epoch 570, training loss: 12.507174491882324 = 0.37902361154556274 + 2.0 * 6.064075469970703
Epoch 570, val loss: 0.7353153228759766
Epoch 580, training loss: 12.482701301574707 = 0.36176547408103943 + 2.0 * 6.060467720031738
Epoch 580, val loss: 0.7295592427253723
Epoch 590, training loss: 12.460880279541016 = 0.3450668454170227 + 2.0 * 6.057906627655029
Epoch 590, val loss: 0.7243126630783081
Epoch 600, training loss: 12.451308250427246 = 0.3288551867008209 + 2.0 * 6.0612263679504395
Epoch 600, val loss: 0.7195062041282654
Epoch 610, training loss: 12.4306058883667 = 0.31332823634147644 + 2.0 * 6.058639049530029
Epoch 610, val loss: 0.7150729894638062
Epoch 620, training loss: 12.4079008102417 = 0.298386812210083 + 2.0 * 6.054757118225098
Epoch 620, val loss: 0.711274266242981
Epoch 630, training loss: 12.390811920166016 = 0.2840103507041931 + 2.0 * 6.053400993347168
Epoch 630, val loss: 0.707936704158783
Epoch 640, training loss: 12.3801908493042 = 0.27019208669662476 + 2.0 * 6.054999351501465
Epoch 640, val loss: 0.7049813270568848
Epoch 650, training loss: 12.360066413879395 = 0.2570168375968933 + 2.0 * 6.051524639129639
Epoch 650, val loss: 0.7024863958358765
Epoch 660, training loss: 12.360696792602539 = 0.24443520605564117 + 2.0 * 6.058130741119385
Epoch 660, val loss: 0.7004668116569519
Epoch 670, training loss: 12.331467628479004 = 0.23253804445266724 + 2.0 * 6.049464702606201
Epoch 670, val loss: 0.698759913444519
Epoch 680, training loss: 12.315115928649902 = 0.22129246592521667 + 2.0 * 6.046911716461182
Epoch 680, val loss: 0.6975582838058472
Epoch 690, training loss: 12.308836936950684 = 0.21061106026172638 + 2.0 * 6.049112796783447
Epoch 690, val loss: 0.6968085169792175
Epoch 700, training loss: 12.294929504394531 = 0.20053529739379883 + 2.0 * 6.047197341918945
Epoch 700, val loss: 0.696336567401886
Epoch 710, training loss: 12.282188415527344 = 0.19108279049396515 + 2.0 * 6.045552730560303
Epoch 710, val loss: 0.6963331699371338
Epoch 720, training loss: 12.270221710205078 = 0.1821516752243042 + 2.0 * 6.044034957885742
Epoch 720, val loss: 0.6966992616653442
Epoch 730, training loss: 12.258935928344727 = 0.17374694347381592 + 2.0 * 6.0425944328308105
Epoch 730, val loss: 0.6973428726196289
Epoch 740, training loss: 12.249080657958984 = 0.16585813462734222 + 2.0 * 6.041611194610596
Epoch 740, val loss: 0.6983412504196167
Epoch 750, training loss: 12.245712280273438 = 0.15842603147029877 + 2.0 * 6.043642997741699
Epoch 750, val loss: 0.6996236443519592
Epoch 760, training loss: 12.228748321533203 = 0.1514609307050705 + 2.0 * 6.038643836975098
Epoch 760, val loss: 0.7012120485305786
Epoch 770, training loss: 12.220999717712402 = 0.14487256109714508 + 2.0 * 6.0380635261535645
Epoch 770, val loss: 0.7031039595603943
Epoch 780, training loss: 12.231151580810547 = 0.13862943649291992 + 2.0 * 6.046260833740234
Epoch 780, val loss: 0.7052268981933594
Epoch 790, training loss: 12.209673881530762 = 0.13277746737003326 + 2.0 * 6.038448333740234
Epoch 790, val loss: 0.7074710726737976
Epoch 800, training loss: 12.202141761779785 = 0.12727442383766174 + 2.0 * 6.037433624267578
Epoch 800, val loss: 0.7100325226783752
Epoch 810, training loss: 12.191402435302734 = 0.12206356227397919 + 2.0 * 6.034669399261475
Epoch 810, val loss: 0.7128451466560364
Epoch 820, training loss: 12.183756828308105 = 0.11711131036281586 + 2.0 * 6.033322811126709
Epoch 820, val loss: 0.715842604637146
Epoch 830, training loss: 12.191004753112793 = 0.11241133511066437 + 2.0 * 6.039296627044678
Epoch 830, val loss: 0.7190204858779907
Epoch 840, training loss: 12.172062873840332 = 0.10797668993473053 + 2.0 * 6.032042980194092
Epoch 840, val loss: 0.7223926782608032
Epoch 850, training loss: 12.168130874633789 = 0.10376504063606262 + 2.0 * 6.032182693481445
Epoch 850, val loss: 0.7259700298309326
Epoch 860, training loss: 12.164053916931152 = 0.09976919740438461 + 2.0 * 6.032142162322998
Epoch 860, val loss: 0.729614794254303
Epoch 870, training loss: 12.15473461151123 = 0.09598841518163681 + 2.0 * 6.0293731689453125
Epoch 870, val loss: 0.7334209084510803
Epoch 880, training loss: 12.153003692626953 = 0.09238741546869278 + 2.0 * 6.030308246612549
Epoch 880, val loss: 0.7374178171157837
Epoch 890, training loss: 12.144667625427246 = 0.08894926309585571 + 2.0 * 6.027859210968018
Epoch 890, val loss: 0.7414950132369995
Epoch 900, training loss: 12.13749885559082 = 0.08566857129335403 + 2.0 * 6.025915145874023
Epoch 900, val loss: 0.7457432746887207
Epoch 910, training loss: 12.148882865905762 = 0.08252628147602081 + 2.0 * 6.033178329467773
Epoch 910, val loss: 0.7501315474510193
Epoch 920, training loss: 12.139996528625488 = 0.07954996079206467 + 2.0 * 6.030223369598389
Epoch 920, val loss: 0.7543664574623108
Epoch 930, training loss: 12.124429702758789 = 0.07673227041959763 + 2.0 * 6.023848533630371
Epoch 930, val loss: 0.7588398456573486
Epoch 940, training loss: 12.121216773986816 = 0.07402830570936203 + 2.0 * 6.023594379425049
Epoch 940, val loss: 0.7634664177894592
Epoch 950, training loss: 12.116255760192871 = 0.07142454385757446 + 2.0 * 6.022415637969971
Epoch 950, val loss: 0.7681800127029419
Epoch 960, training loss: 12.136096954345703 = 0.06893276423215866 + 2.0 * 6.0335822105407715
Epoch 960, val loss: 0.772854208946228
Epoch 970, training loss: 12.116716384887695 = 0.0665823444724083 + 2.0 * 6.02506685256958
Epoch 970, val loss: 0.7775475382804871
Epoch 980, training loss: 12.105890274047852 = 0.06434298306703568 + 2.0 * 6.020773410797119
Epoch 980, val loss: 0.7823893427848816
Epoch 990, training loss: 12.101908683776855 = 0.06218547746539116 + 2.0 * 6.019861698150635
Epoch 990, val loss: 0.7873196601867676
Epoch 1000, training loss: 12.112161636352539 = 0.060110002756118774 + 2.0 * 6.026025772094727
Epoch 1000, val loss: 0.79229736328125
Epoch 1010, training loss: 12.099485397338867 = 0.058122165501117706 + 2.0 * 6.020681381225586
Epoch 1010, val loss: 0.7972017526626587
Epoch 1020, training loss: 12.096471786499023 = 0.056223124265670776 + 2.0 * 6.020124435424805
Epoch 1020, val loss: 0.8022779822349548
Epoch 1030, training loss: 12.097029685974121 = 0.05440066009759903 + 2.0 * 6.02131462097168
Epoch 1030, val loss: 0.8072500228881836
Epoch 1040, training loss: 12.088078498840332 = 0.05266382172703743 + 2.0 * 6.017707347869873
Epoch 1040, val loss: 0.8122541308403015
Epoch 1050, training loss: 12.084223747253418 = 0.05099382996559143 + 2.0 * 6.01661491394043
Epoch 1050, val loss: 0.8173503279685974
Epoch 1060, training loss: 12.095965385437012 = 0.04938558116555214 + 2.0 * 6.023289680480957
Epoch 1060, val loss: 0.8224432468414307
Epoch 1070, training loss: 12.082819938659668 = 0.04785070940852165 + 2.0 * 6.017484664916992
Epoch 1070, val loss: 0.8274280428886414
Epoch 1080, training loss: 12.076882362365723 = 0.04637867584824562 + 2.0 * 6.015251636505127
Epoch 1080, val loss: 0.8325615525245667
Epoch 1090, training loss: 12.078560829162598 = 0.04495936259627342 + 2.0 * 6.016800880432129
Epoch 1090, val loss: 0.8376695513725281
Epoch 1100, training loss: 12.074333190917969 = 0.043601084500551224 + 2.0 * 6.015366077423096
Epoch 1100, val loss: 0.8426516652107239
Epoch 1110, training loss: 12.0723295211792 = 0.042305346578359604 + 2.0 * 6.015012264251709
Epoch 1110, val loss: 0.8476796746253967
Epoch 1120, training loss: 12.077738761901855 = 0.041061367839574814 + 2.0 * 6.018338680267334
Epoch 1120, val loss: 0.8526524901390076
Epoch 1130, training loss: 12.066661834716797 = 0.039869505912065506 + 2.0 * 6.013396263122559
Epoch 1130, val loss: 0.8575822710990906
Epoch 1140, training loss: 12.063339233398438 = 0.03872116655111313 + 2.0 * 6.0123090744018555
Epoch 1140, val loss: 0.8624928593635559
Epoch 1150, training loss: 12.067098617553711 = 0.03761481121182442 + 2.0 * 6.014741897583008
Epoch 1150, val loss: 0.8674131631851196
Epoch 1160, training loss: 12.064233779907227 = 0.036552995443344116 + 2.0 * 6.013840198516846
Epoch 1160, val loss: 0.872309684753418
Epoch 1170, training loss: 12.063615798950195 = 0.035533297806978226 + 2.0 * 6.014041423797607
Epoch 1170, val loss: 0.8770744204521179
Epoch 1180, training loss: 12.056609153747559 = 0.034556951373815536 + 2.0 * 6.011025905609131
Epoch 1180, val loss: 0.8818463683128357
Epoch 1190, training loss: 12.054247856140137 = 0.033615272492170334 + 2.0 * 6.010316371917725
Epoch 1190, val loss: 0.8866233825683594
Epoch 1200, training loss: 12.057894706726074 = 0.03270583972334862 + 2.0 * 6.012594223022461
Epoch 1200, val loss: 0.8913689255714417
Epoch 1210, training loss: 12.0567045211792 = 0.0318308100104332 + 2.0 * 6.012436866760254
Epoch 1210, val loss: 0.8960031270980835
Epoch 1220, training loss: 12.050371170043945 = 0.030991310253739357 + 2.0 * 6.009689807891846
Epoch 1220, val loss: 0.9006025195121765
Epoch 1230, training loss: 12.047430038452148 = 0.03018483705818653 + 2.0 * 6.008622646331787
Epoch 1230, val loss: 0.9052487015724182
Epoch 1240, training loss: 12.048333168029785 = 0.029402632266283035 + 2.0 * 6.009465217590332
Epoch 1240, val loss: 0.9098467230796814
Epoch 1250, training loss: 12.052653312683105 = 0.02864859253168106 + 2.0 * 6.012002468109131
Epoch 1250, val loss: 0.9143430590629578
Epoch 1260, training loss: 12.044425010681152 = 0.027928974479436874 + 2.0 * 6.0082478523254395
Epoch 1260, val loss: 0.9187214374542236
Epoch 1270, training loss: 12.043054580688477 = 0.027236558496952057 + 2.0 * 6.007908821105957
Epoch 1270, val loss: 0.9231539964675903
Epoch 1280, training loss: 12.050045013427734 = 0.026567591354250908 + 2.0 * 6.0117387771606445
Epoch 1280, val loss: 0.9275302886962891
Epoch 1290, training loss: 12.038493156433105 = 0.025921842083334923 + 2.0 * 6.006285667419434
Epoch 1290, val loss: 0.9318004250526428
Epoch 1300, training loss: 12.036211967468262 = 0.02529873512685299 + 2.0 * 6.005456447601318
Epoch 1300, val loss: 0.9361222386360168
Epoch 1310, training loss: 12.03742504119873 = 0.024694066494703293 + 2.0 * 6.0063652992248535
Epoch 1310, val loss: 0.9404271245002747
Epoch 1320, training loss: 12.046987533569336 = 0.02411058358848095 + 2.0 * 6.011438369750977
Epoch 1320, val loss: 0.9445843696594238
Epoch 1330, training loss: 12.033735275268555 = 0.02354918234050274 + 2.0 * 6.005093097686768
Epoch 1330, val loss: 0.9486584663391113
Epoch 1340, training loss: 12.033318519592285 = 0.0230095274746418 + 2.0 * 6.005154609680176
Epoch 1340, val loss: 0.9528467059135437
Epoch 1350, training loss: 12.035618782043457 = 0.02248549647629261 + 2.0 * 6.006566524505615
Epoch 1350, val loss: 0.9569939374923706
Epoch 1360, training loss: 12.030683517456055 = 0.02197621949017048 + 2.0 * 6.0043535232543945
Epoch 1360, val loss: 0.9610092639923096
Epoch 1370, training loss: 12.034307479858398 = 0.021485432982444763 + 2.0 * 6.006411075592041
Epoch 1370, val loss: 0.965045154094696
Epoch 1380, training loss: 12.031265258789062 = 0.021010803058743477 + 2.0 * 6.005127429962158
Epoch 1380, val loss: 0.9690237641334534
Epoch 1390, training loss: 12.026396751403809 = 0.02055172622203827 + 2.0 * 6.002922534942627
Epoch 1390, val loss: 0.9729688763618469
Epoch 1400, training loss: 12.04818058013916 = 0.020107394084334373 + 2.0 * 6.014036655426025
Epoch 1400, val loss: 0.9768708348274231
Epoch 1410, training loss: 12.028956413269043 = 0.01968078687787056 + 2.0 * 6.004637718200684
Epoch 1410, val loss: 0.9805412888526917
Epoch 1420, training loss: 12.022212982177734 = 0.019267937168478966 + 2.0 * 6.001472473144531
Epoch 1420, val loss: 0.9843876361846924
Epoch 1430, training loss: 12.020891189575195 = 0.018864501267671585 + 2.0 * 6.001013278961182
Epoch 1430, val loss: 0.9882155656814575
Epoch 1440, training loss: 12.020750999450684 = 0.0184702780097723 + 2.0 * 6.001140594482422
Epoch 1440, val loss: 0.9920241832733154
Epoch 1450, training loss: 12.041769981384277 = 0.018087824806571007 + 2.0 * 6.011841297149658
Epoch 1450, val loss: 0.9957563877105713
Epoch 1460, training loss: 12.024657249450684 = 0.017719311639666557 + 2.0 * 6.003468990325928
Epoch 1460, val loss: 0.9993032813072205
Epoch 1470, training loss: 12.017565727233887 = 0.017366649582982063 + 2.0 * 6.0000996589660645
Epoch 1470, val loss: 1.0029760599136353
Epoch 1480, training loss: 12.022130966186523 = 0.017021333798766136 + 2.0 * 6.002554893493652
Epoch 1480, val loss: 1.006643295288086
Epoch 1490, training loss: 12.023170471191406 = 0.016684234142303467 + 2.0 * 6.0032429695129395
Epoch 1490, val loss: 1.0101053714752197
Epoch 1500, training loss: 12.016596794128418 = 0.016362933441996574 + 2.0 * 6.00011682510376
Epoch 1500, val loss: 1.013620138168335
Epoch 1510, training loss: 12.015057563781738 = 0.016049489378929138 + 2.0 * 5.999504089355469
Epoch 1510, val loss: 1.0172069072723389
Epoch 1520, training loss: 12.013620376586914 = 0.015741733834147453 + 2.0 * 5.998939514160156
Epoch 1520, val loss: 1.020756721496582
Epoch 1530, training loss: 12.01976490020752 = 0.01544031873345375 + 2.0 * 6.002162456512451
Epoch 1530, val loss: 1.0242719650268555
Epoch 1540, training loss: 12.01928424835205 = 0.015149364247918129 + 2.0 * 6.002067565917969
Epoch 1540, val loss: 1.027646780014038
Epoch 1550, training loss: 12.013933181762695 = 0.014870407991111279 + 2.0 * 5.999531269073486
Epoch 1550, val loss: 1.0310232639312744
Epoch 1560, training loss: 12.012585639953613 = 0.014599405229091644 + 2.0 * 5.998992919921875
Epoch 1560, val loss: 1.034426212310791
Epoch 1570, training loss: 12.014008522033691 = 0.01433408074080944 + 2.0 * 5.999837398529053
Epoch 1570, val loss: 1.037795066833496
Epoch 1580, training loss: 12.010380744934082 = 0.014074333943426609 + 2.0 * 5.998153209686279
Epoch 1580, val loss: 1.0410951375961304
Epoch 1590, training loss: 12.013702392578125 = 0.013822062872350216 + 2.0 * 5.999940395355225
Epoch 1590, val loss: 1.0444583892822266
Epoch 1600, training loss: 12.010220527648926 = 0.013577538542449474 + 2.0 * 5.998321533203125
Epoch 1600, val loss: 1.047700047492981
Epoch 1610, training loss: 12.00794792175293 = 0.013338728807866573 + 2.0 * 5.997304439544678
Epoch 1610, val loss: 1.0509145259857178
Epoch 1620, training loss: 12.010705947875977 = 0.01310624647885561 + 2.0 * 5.998799800872803
Epoch 1620, val loss: 1.0541555881500244
Epoch 1630, training loss: 12.007569313049316 = 0.012880071066319942 + 2.0 * 5.997344493865967
Epoch 1630, val loss: 1.0573430061340332
Epoch 1640, training loss: 12.018182754516602 = 0.012660346925258636 + 2.0 * 6.002761363983154
Epoch 1640, val loss: 1.060503363609314
Epoch 1650, training loss: 12.005387306213379 = 0.012446139939129353 + 2.0 * 5.9964704513549805
Epoch 1650, val loss: 1.0635325908660889
Epoch 1660, training loss: 12.002839088439941 = 0.012238520197570324 + 2.0 * 5.99530029296875
Epoch 1660, val loss: 1.066678762435913
Epoch 1670, training loss: 12.00236988067627 = 0.012034222483634949 + 2.0 * 5.9951677322387695
Epoch 1670, val loss: 1.0697859525680542
Epoch 1680, training loss: 12.026031494140625 = 0.01183569896966219 + 2.0 * 6.0070977210998535
Epoch 1680, val loss: 1.0728625059127808
Epoch 1690, training loss: 12.013419151306152 = 0.011641573160886765 + 2.0 * 6.000888824462891
Epoch 1690, val loss: 1.0756018161773682
Epoch 1700, training loss: 12.001235008239746 = 0.011457615531980991 + 2.0 * 5.994888782501221
Epoch 1700, val loss: 1.078561782836914
Epoch 1710, training loss: 11.9990816116333 = 0.01127573475241661 + 2.0 * 5.993903160095215
Epoch 1710, val loss: 1.0815576314926147
Epoch 1720, training loss: 11.99898624420166 = 0.011095072142779827 + 2.0 * 5.993945598602295
Epoch 1720, val loss: 1.0845204591751099
Epoch 1730, training loss: 12.020208358764648 = 0.010918064042925835 + 2.0 * 6.004645347595215
Epoch 1730, val loss: 1.0874711275100708
Epoch 1740, training loss: 12.007200241088867 = 0.01074775867164135 + 2.0 * 5.998226165771484
Epoch 1740, val loss: 1.0901997089385986
Epoch 1750, training loss: 11.998366355895996 = 0.010582802817225456 + 2.0 * 5.993891716003418
Epoch 1750, val loss: 1.09305739402771
Epoch 1760, training loss: 11.996966361999512 = 0.010421538725495338 + 2.0 * 5.993272304534912
Epoch 1760, val loss: 1.0959641933441162
Epoch 1770, training loss: 12.004105567932129 = 0.010261413641273975 + 2.0 * 5.996922016143799
Epoch 1770, val loss: 1.0988377332687378
Epoch 1780, training loss: 11.999054908752441 = 0.010105106048285961 + 2.0 * 5.9944748878479
Epoch 1780, val loss: 1.1015220880508423
Epoch 1790, training loss: 11.995224952697754 = 0.009954673238098621 + 2.0 * 5.992635250091553
Epoch 1790, val loss: 1.1043000221252441
Epoch 1800, training loss: 11.9943208694458 = 0.009807358495891094 + 2.0 * 5.9922566413879395
Epoch 1800, val loss: 1.1070791482925415
Epoch 1810, training loss: 11.997647285461426 = 0.009661647491157055 + 2.0 * 5.993992805480957
Epoch 1810, val loss: 1.1098883152008057
Epoch 1820, training loss: 12.001399993896484 = 0.009519437327980995 + 2.0 * 5.995940208435059
Epoch 1820, val loss: 1.1125085353851318
Epoch 1830, training loss: 11.993047714233398 = 0.009382931515574455 + 2.0 * 5.991832256317139
Epoch 1830, val loss: 1.1151643991470337
Epoch 1840, training loss: 11.992229461669922 = 0.00924958847463131 + 2.0 * 5.991489887237549
Epoch 1840, val loss: 1.1179062128067017
Epoch 1850, training loss: 11.991833686828613 = 0.009116800501942635 + 2.0 * 5.991358280181885
Epoch 1850, val loss: 1.1206365823745728
Epoch 1860, training loss: 11.998713493347168 = 0.008985468186438084 + 2.0 * 5.994863986968994
Epoch 1860, val loss: 1.123335838317871
Epoch 1870, training loss: 11.994338035583496 = 0.008858351036906242 + 2.0 * 5.992739677429199
Epoch 1870, val loss: 1.1259268522262573
Epoch 1880, training loss: 11.992605209350586 = 0.008735903538763523 + 2.0 * 5.991934776306152
Epoch 1880, val loss: 1.1284680366516113
Epoch 1890, training loss: 11.989961624145508 = 0.008616216480731964 + 2.0 * 5.990672588348389
Epoch 1890, val loss: 1.131046175956726
Epoch 1900, training loss: 11.98823070526123 = 0.00849815085530281 + 2.0 * 5.989866256713867
Epoch 1900, val loss: 1.1337116956710815
Epoch 1910, training loss: 11.988822937011719 = 0.008380522951483727 + 2.0 * 5.99022102355957
Epoch 1910, val loss: 1.1363449096679688
Epoch 1920, training loss: 12.00408935546875 = 0.008265304379165173 + 2.0 * 5.9979119300842285
Epoch 1920, val loss: 1.1389219760894775
Epoch 1930, training loss: 11.995803833007812 = 0.008154264651238918 + 2.0 * 5.9938249588012695
Epoch 1930, val loss: 1.1413547992706299
Epoch 1940, training loss: 11.98928165435791 = 0.00804664846509695 + 2.0 * 5.990617275238037
Epoch 1940, val loss: 1.143875002861023
Epoch 1950, training loss: 11.986090660095215 = 0.007940742187201977 + 2.0 * 5.989075183868408
Epoch 1950, val loss: 1.146433711051941
Epoch 1960, training loss: 11.989603042602539 = 0.007835221476852894 + 2.0 * 5.990883827209473
Epoch 1960, val loss: 1.148991346359253
Epoch 1970, training loss: 11.991616249084473 = 0.007731670048087835 + 2.0 * 5.991942405700684
Epoch 1970, val loss: 1.1513513326644897
Epoch 1980, training loss: 11.9861421585083 = 0.007632023189216852 + 2.0 * 5.989254951477051
Epoch 1980, val loss: 1.1537342071533203
Epoch 1990, training loss: 11.987329483032227 = 0.007534547708928585 + 2.0 * 5.98989725112915
Epoch 1990, val loss: 1.1562228202819824
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8148
Overall ASR: 0.9410
Flip ASR: 0.9289/225 nodes
The final ASR:0.77122, 0.12722, Accuracy:0.81111, 0.00800
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11616])
remove edge: torch.Size([2, 9510])
updated graph: torch.Size([2, 10570])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9705
Flip ASR: 0.9644/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9631
Flip ASR: 0.9556/225 nodes
The final ASR:0.97171, 0.00758, Accuracy:0.83210, 0.00761
Begin epxeriment: cont_weight: 2 epoch:2000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 18.691429138183594 = 1.943631649017334 + 2.0 * 8.37389850616455
Epoch 0, val loss: 1.9387726783752441
Epoch 10, training loss: 18.681110382080078 = 1.934205174446106 + 2.0 * 8.373452186584473
Epoch 10, val loss: 1.9301068782806396
Epoch 20, training loss: 18.662052154541016 = 1.9223589897155762 + 2.0 * 8.36984634399414
Epoch 20, val loss: 1.9189046621322632
Epoch 30, training loss: 18.591196060180664 = 1.9061169624328613 + 2.0 * 8.34253978729248
Epoch 30, val loss: 1.9033606052398682
Epoch 40, training loss: 18.23318099975586 = 1.8856821060180664 + 2.0 * 8.173749923706055
Epoch 40, val loss: 1.884395956993103
Epoch 50, training loss: 17.15571403503418 = 1.8625673055648804 + 2.0 * 7.646573543548584
Epoch 50, val loss: 1.8632837533950806
Epoch 60, training loss: 16.545320510864258 = 1.8429254293441772 + 2.0 * 7.351197242736816
Epoch 60, val loss: 1.8464690446853638
Epoch 70, training loss: 15.654121398925781 = 1.8306840658187866 + 2.0 * 6.911718845367432
Epoch 70, val loss: 1.8362996578216553
Epoch 80, training loss: 15.123262405395508 = 1.8219844102859497 + 2.0 * 6.650639057159424
Epoch 80, val loss: 1.8290047645568848
Epoch 90, training loss: 14.878556251525879 = 1.8099294900894165 + 2.0 * 6.534313201904297
Epoch 90, val loss: 1.8183631896972656
Epoch 100, training loss: 14.70644760131836 = 1.7959657907485962 + 2.0 * 6.455240726470947
Epoch 100, val loss: 1.8060996532440186
Epoch 110, training loss: 14.596187591552734 = 1.7825391292572021 + 2.0 * 6.406824111938477
Epoch 110, val loss: 1.7943130731582642
Epoch 120, training loss: 14.499574661254883 = 1.7694716453552246 + 2.0 * 6.36505126953125
Epoch 120, val loss: 1.7828705310821533
Epoch 130, training loss: 14.40937614440918 = 1.7563334703445435 + 2.0 * 6.326521396636963
Epoch 130, val loss: 1.7715165615081787
Epoch 140, training loss: 14.332290649414062 = 1.7424561977386475 + 2.0 * 6.294917106628418
Epoch 140, val loss: 1.7598416805267334
Epoch 150, training loss: 14.258095741271973 = 1.7270864248275757 + 2.0 * 6.265504837036133
Epoch 150, val loss: 1.7470765113830566
Epoch 160, training loss: 14.1942720413208 = 1.7093069553375244 + 2.0 * 6.242482662200928
Epoch 160, val loss: 1.7325469255447388
Epoch 170, training loss: 14.14204216003418 = 1.6886855363845825 + 2.0 * 6.226678371429443
Epoch 170, val loss: 1.7159489393234253
Epoch 180, training loss: 14.087003707885742 = 1.6654282808303833 + 2.0 * 6.210787773132324
Epoch 180, val loss: 1.6972981691360474
Epoch 190, training loss: 14.033370018005371 = 1.6389399766921997 + 2.0 * 6.1972150802612305
Epoch 190, val loss: 1.676154375076294
Epoch 200, training loss: 13.980019569396973 = 1.608927845954895 + 2.0 * 6.185545921325684
Epoch 200, val loss: 1.652220606803894
Epoch 210, training loss: 13.925580024719238 = 1.5749588012695312 + 2.0 * 6.1753106117248535
Epoch 210, val loss: 1.6253312826156616
Epoch 220, training loss: 13.869415283203125 = 1.5369422435760498 + 2.0 * 6.166236400604248
Epoch 220, val loss: 1.5953385829925537
Epoch 230, training loss: 13.816516876220703 = 1.4953662157058716 + 2.0 * 6.1605753898620605
Epoch 230, val loss: 1.5627957582473755
Epoch 240, training loss: 13.754591941833496 = 1.451280951499939 + 2.0 * 6.151655673980713
Epoch 240, val loss: 1.5286321640014648
Epoch 250, training loss: 13.694051742553711 = 1.4048093557357788 + 2.0 * 6.1446213722229
Epoch 250, val loss: 1.4929234981536865
Epoch 260, training loss: 13.633740425109863 = 1.3566710948944092 + 2.0 * 6.1385345458984375
Epoch 260, val loss: 1.4564470052719116
Epoch 270, training loss: 13.578507423400879 = 1.3079708814620972 + 2.0 * 6.135268211364746
Epoch 270, val loss: 1.4202604293823242
Epoch 280, training loss: 13.519733428955078 = 1.2601666450500488 + 2.0 * 6.1297831535339355
Epoch 280, val loss: 1.3849341869354248
Epoch 290, training loss: 13.462177276611328 = 1.2137198448181152 + 2.0 * 6.1242289543151855
Epoch 290, val loss: 1.35117506980896
Epoch 300, training loss: 13.408576965332031 = 1.1688151359558105 + 2.0 * 6.1198811531066895
Epoch 300, val loss: 1.319115400314331
Epoch 310, training loss: 13.370655059814453 = 1.125989556312561 + 2.0 * 6.122332572937012
Epoch 310, val loss: 1.2891926765441895
Epoch 320, training loss: 13.314021110534668 = 1.086147427558899 + 2.0 * 6.113936901092529
Epoch 320, val loss: 1.2615675926208496
Epoch 330, training loss: 13.267938613891602 = 1.0486172437667847 + 2.0 * 6.109660625457764
Epoch 330, val loss: 1.2362000942230225
Epoch 340, training loss: 13.22487735748291 = 1.0130177736282349 + 2.0 * 6.105929851531982
Epoch 340, val loss: 1.212546706199646
Epoch 350, training loss: 13.185491561889648 = 0.978887677192688 + 2.0 * 6.103302001953125
Epoch 350, val loss: 1.1901291608810425
Epoch 360, training loss: 13.14709186553955 = 0.9461172223091125 + 2.0 * 6.100487232208252
Epoch 360, val loss: 1.1690138578414917
Epoch 370, training loss: 13.108538627624512 = 0.9146031141281128 + 2.0 * 6.096967697143555
Epoch 370, val loss: 1.1488566398620605
Epoch 380, training loss: 13.073002815246582 = 0.8837900757789612 + 2.0 * 6.094606399536133
Epoch 380, val loss: 1.1294018030166626
Epoch 390, training loss: 13.04951000213623 = 0.8534809350967407 + 2.0 * 6.0980143547058105
Epoch 390, val loss: 1.11045503616333
Epoch 400, training loss: 13.007003784179688 = 0.8238973021507263 + 2.0 * 6.091553211212158
Epoch 400, val loss: 1.0921729803085327
Epoch 410, training loss: 12.970742225646973 = 0.7948567867279053 + 2.0 * 6.087942600250244
Epoch 410, val loss: 1.074659824371338
Epoch 420, training loss: 12.949902534484863 = 0.7663530111312866 + 2.0 * 6.091774940490723
Epoch 420, val loss: 1.057673692703247
Epoch 430, training loss: 12.907034873962402 = 0.7384715676307678 + 2.0 * 6.0842814445495605
Epoch 430, val loss: 1.0414267778396606
Epoch 440, training loss: 12.875802993774414 = 0.7114214897155762 + 2.0 * 6.08219051361084
Epoch 440, val loss: 1.0260895490646362
Epoch 450, training loss: 12.844038963317871 = 0.6850386261940002 + 2.0 * 6.079500198364258
Epoch 450, val loss: 1.0115113258361816
Epoch 460, training loss: 12.823525428771973 = 0.6592779755592346 + 2.0 * 6.082123756408691
Epoch 460, val loss: 0.9976389408111572
Epoch 470, training loss: 12.790277481079102 = 0.6343616247177124 + 2.0 * 6.077958106994629
Epoch 470, val loss: 0.984649121761322
Epoch 480, training loss: 12.7594633102417 = 0.6100346446037292 + 2.0 * 6.074714183807373
Epoch 480, val loss: 0.9724915623664856
Epoch 490, training loss: 12.731706619262695 = 0.5861674547195435 + 2.0 * 6.072769641876221
Epoch 490, val loss: 0.9609137773513794
Epoch 500, training loss: 12.720230102539062 = 0.5627097487449646 + 2.0 * 6.078760147094727
Epoch 500, val loss: 0.9498581290245056
Epoch 510, training loss: 12.684606552124023 = 0.5396768450737 + 2.0 * 6.072464942932129
Epoch 510, val loss: 0.9394268989562988
Epoch 520, training loss: 12.654226303100586 = 0.5170736312866211 + 2.0 * 6.068576335906982
Epoch 520, val loss: 0.9296694993972778
Epoch 530, training loss: 12.62757682800293 = 0.4947131872177124 + 2.0 * 6.066431999206543
Epoch 530, val loss: 0.9204152226448059
Epoch 540, training loss: 12.617253303527832 = 0.47255250811576843 + 2.0 * 6.07235050201416
Epoch 540, val loss: 0.9115750193595886
Epoch 550, training loss: 12.57844352722168 = 0.4508535861968994 + 2.0 * 6.06379508972168
Epoch 550, val loss: 0.9033186435699463
Epoch 560, training loss: 12.553351402282715 = 0.4294508993625641 + 2.0 * 6.061950206756592
Epoch 560, val loss: 0.8957602977752686
Epoch 570, training loss: 12.530056953430176 = 0.40833017230033875 + 2.0 * 6.060863494873047
Epoch 570, val loss: 0.888752818107605
Epoch 580, training loss: 12.510860443115234 = 0.38756516575813293 + 2.0 * 6.061647415161133
Epoch 580, val loss: 0.8822419047355652
Epoch 590, training loss: 12.483887672424316 = 0.36736932396888733 + 2.0 * 6.058259010314941
Epoch 590, val loss: 0.8763177990913391
Epoch 600, training loss: 12.475364685058594 = 0.34773769974708557 + 2.0 * 6.06381368637085
Epoch 600, val loss: 0.8709313273429871
Epoch 610, training loss: 12.43979263305664 = 0.32879966497421265 + 2.0 * 6.055496692657471
Epoch 610, val loss: 0.8662026524543762
Epoch 620, training loss: 12.419588088989258 = 0.3106197416782379 + 2.0 * 6.0544843673706055
Epoch 620, val loss: 0.8622183203697205
Epoch 630, training loss: 12.400933265686035 = 0.29313573241233826 + 2.0 * 6.053898811340332
Epoch 630, val loss: 0.8588487505912781
Epoch 640, training loss: 12.384038925170898 = 0.2764781415462494 + 2.0 * 6.053780555725098
Epoch 640, val loss: 0.8561094999313354
Epoch 650, training loss: 12.36128044128418 = 0.2606777846813202 + 2.0 * 6.050301551818848
Epoch 650, val loss: 0.8540894389152527
Epoch 660, training loss: 12.345215797424316 = 0.24566689133644104 + 2.0 * 6.049774646759033
Epoch 660, val loss: 0.8527586460113525
Epoch 670, training loss: 12.342914581298828 = 0.231465145945549 + 2.0 * 6.055724620819092
Epoch 670, val loss: 0.851971447467804
Epoch 680, training loss: 12.312675476074219 = 0.21820907294750214 + 2.0 * 6.0472331047058105
Epoch 680, val loss: 0.8518732190132141
Epoch 690, training loss: 12.298197746276855 = 0.2056901603937149 + 2.0 * 6.046253681182861
Epoch 690, val loss: 0.8523916006088257
Epoch 700, training loss: 12.297337532043457 = 0.19391140341758728 + 2.0 * 6.051712989807129
Epoch 700, val loss: 0.8533793091773987
Epoch 710, training loss: 12.270463943481445 = 0.18288570642471313 + 2.0 * 6.043788909912109
Epoch 710, val loss: 0.8549920320510864
Epoch 720, training loss: 12.258635520935059 = 0.17254945635795593 + 2.0 * 6.04304313659668
Epoch 720, val loss: 0.857142984867096
Epoch 730, training loss: 12.250763893127441 = 0.16286005079746246 + 2.0 * 6.043951988220215
Epoch 730, val loss: 0.8597202897071838
Epoch 740, training loss: 12.240715026855469 = 0.15379872918128967 + 2.0 * 6.043457984924316
Epoch 740, val loss: 0.8626866340637207
Epoch 750, training loss: 12.224862098693848 = 0.14535833895206451 + 2.0 * 6.039752006530762
Epoch 750, val loss: 0.8660693168640137
Epoch 760, training loss: 12.21602725982666 = 0.1374562531709671 + 2.0 * 6.039285659790039
Epoch 760, val loss: 0.8697777390480042
Epoch 770, training loss: 12.230788230895996 = 0.13007277250289917 + 2.0 * 6.050357818603516
Epoch 770, val loss: 0.8737961053848267
Epoch 780, training loss: 12.201047897338867 = 0.12317147105932236 + 2.0 * 6.038938045501709
Epoch 780, val loss: 0.8779858946800232
Epoch 790, training loss: 12.192261695861816 = 0.11675792187452316 + 2.0 * 6.0377516746521
Epoch 790, val loss: 0.8826051950454712
Epoch 800, training loss: 12.181031227111816 = 0.11075084656476974 + 2.0 * 6.035140037536621
Epoch 800, val loss: 0.8874837160110474
Epoch 810, training loss: 12.181112289428711 = 0.10510025173425674 + 2.0 * 6.038005828857422
Epoch 810, val loss: 0.8925774693489075
Epoch 820, training loss: 12.17352294921875 = 0.09982047230005264 + 2.0 * 6.036851406097412
Epoch 820, val loss: 0.8978430032730103
Epoch 830, training loss: 12.16372013092041 = 0.09487246721982956 + 2.0 * 6.034423828125
Epoch 830, val loss: 0.9032120108604431
Epoch 840, training loss: 12.154873847961426 = 0.09025221318006516 + 2.0 * 6.032310962677002
Epoch 840, val loss: 0.9087337255477905
Epoch 850, training loss: 12.150251388549805 = 0.08591040223836899 + 2.0 * 6.032170295715332
Epoch 850, val loss: 0.9144348502159119
Epoch 860, training loss: 12.148701667785645 = 0.08183687180280685 + 2.0 * 6.033432483673096
Epoch 860, val loss: 0.9202005863189697
Epoch 870, training loss: 12.138801574707031 = 0.07800686359405518 + 2.0 * 6.030397415161133
Epoch 870, val loss: 0.9261186122894287
Epoch 880, training loss: 12.134745597839355 = 0.07442569732666016 + 2.0 * 6.030159950256348
Epoch 880, val loss: 0.932084858417511
Epoch 890, training loss: 12.129364967346191 = 0.07105638086795807 + 2.0 * 6.029154300689697
Epoch 890, val loss: 0.938133955001831
Epoch 900, training loss: 12.128530502319336 = 0.0678786188364029 + 2.0 * 6.030325889587402
Epoch 900, val loss: 0.9441787600517273
Epoch 910, training loss: 12.126141548156738 = 0.06489130854606628 + 2.0 * 6.030625343322754
Epoch 910, val loss: 0.9502972364425659
Epoch 920, training loss: 12.11804485321045 = 0.06208520382642746 + 2.0 * 6.027979850769043
Epoch 920, val loss: 0.9563829302787781
Epoch 930, training loss: 12.113019943237305 = 0.05944377928972244 + 2.0 * 6.026788234710693
Epoch 930, val loss: 0.9625416994094849
Epoch 940, training loss: 12.119732856750488 = 0.056951310485601425 + 2.0 * 6.03139066696167
Epoch 940, val loss: 0.9686696529388428
Epoch 950, training loss: 12.106765747070312 = 0.05461753532290459 + 2.0 * 6.026073932647705
Epoch 950, val loss: 0.974818229675293
Epoch 960, training loss: 12.102556228637695 = 0.05240556225180626 + 2.0 * 6.025075435638428
Epoch 960, val loss: 0.9809949994087219
Epoch 970, training loss: 12.107024192810059 = 0.05032704770565033 + 2.0 * 6.028348445892334
Epoch 970, val loss: 0.9870272278785706
Epoch 980, training loss: 12.096750259399414 = 0.04835421219468117 + 2.0 * 6.024198055267334
Epoch 980, val loss: 0.9931043982505798
Epoch 990, training loss: 12.091135025024414 = 0.04650101438164711 + 2.0 * 6.022316932678223
Epoch 990, val loss: 0.9992133378982544
Epoch 1000, training loss: 12.09051513671875 = 0.04473581165075302 + 2.0 * 6.022889614105225
Epoch 1000, val loss: 1.005308985710144
Epoch 1010, training loss: 12.085604667663574 = 0.04307427629828453 + 2.0 * 6.021265029907227
Epoch 1010, val loss: 1.01134192943573
Epoch 1020, training loss: 12.086648941040039 = 0.04149944335222244 + 2.0 * 6.0225749015808105
Epoch 1020, val loss: 1.0173883438110352
Epoch 1030, training loss: 12.081611633300781 = 0.04001317918300629 + 2.0 * 6.020799160003662
Epoch 1030, val loss: 1.0234283208847046
Epoch 1040, training loss: 12.080093383789062 = 0.038601070642471313 + 2.0 * 6.020746231079102
Epoch 1040, val loss: 1.029322624206543
Epoch 1050, training loss: 12.077177047729492 = 0.03725999593734741 + 2.0 * 6.01995849609375
Epoch 1050, val loss: 1.0352394580841064
Epoch 1060, training loss: 12.076498985290527 = 0.03599029406905174 + 2.0 * 6.020254135131836
Epoch 1060, val loss: 1.0411291122436523
Epoch 1070, training loss: 12.072000503540039 = 0.03477911278605461 + 2.0 * 6.01861047744751
Epoch 1070, val loss: 1.0469224452972412
Epoch 1080, training loss: 12.070596694946289 = 0.03363093361258507 + 2.0 * 6.018482685089111
Epoch 1080, val loss: 1.052674651145935
Epoch 1090, training loss: 12.072096824645996 = 0.03253897652029991 + 2.0 * 6.019778728485107
Epoch 1090, val loss: 1.0582592487335205
Epoch 1100, training loss: 12.063705444335938 = 0.031502027064561844 + 2.0 * 6.016101837158203
Epoch 1100, val loss: 1.0639835596084595
Epoch 1110, training loss: 12.062063217163086 = 0.03051268681883812 + 2.0 * 6.015775203704834
Epoch 1110, val loss: 1.069566011428833
Epoch 1120, training loss: 12.06732177734375 = 0.029571963474154472 + 2.0 * 6.0188751220703125
Epoch 1120, val loss: 1.0750905275344849
Epoch 1130, training loss: 12.061922073364258 = 0.02867327444255352 + 2.0 * 6.016624450683594
Epoch 1130, val loss: 1.0805784463882446
Epoch 1140, training loss: 12.06026840209961 = 0.0278151948004961 + 2.0 * 6.016226768493652
Epoch 1140, val loss: 1.0860234498977661
Epoch 1150, training loss: 12.053566932678223 = 0.026992125436663628 + 2.0 * 6.013287544250488
Epoch 1150, val loss: 1.0913665294647217
Epoch 1160, training loss: 12.053038597106934 = 0.026206906884908676 + 2.0 * 6.013415813446045
Epoch 1160, val loss: 1.096746563911438
Epoch 1170, training loss: 12.052454948425293 = 0.02545551024377346 + 2.0 * 6.013499736785889
Epoch 1170, val loss: 1.1020503044128418
Epoch 1180, training loss: 12.051072120666504 = 0.024736374616622925 + 2.0 * 6.013167858123779
Epoch 1180, val loss: 1.107292890548706
Epoch 1190, training loss: 12.06444263458252 = 0.024049725383520126 + 2.0 * 6.020196437835693
Epoch 1190, val loss: 1.1124969720840454
Epoch 1200, training loss: 12.053705215454102 = 0.02339276485145092 + 2.0 * 6.015156269073486
Epoch 1200, val loss: 1.117432951927185
Epoch 1210, training loss: 12.042924880981445 = 0.022767145186662674 + 2.0 * 6.0100789070129395
Epoch 1210, val loss: 1.1225087642669678
Epoch 1220, training loss: 12.042366027832031 = 0.0221642404794693 + 2.0 * 6.010100841522217
Epoch 1220, val loss: 1.127505898475647
Epoch 1230, training loss: 12.040894508361816 = 0.02158578298985958 + 2.0 * 6.009654521942139
Epoch 1230, val loss: 1.1324410438537598
Epoch 1240, training loss: 12.056373596191406 = 0.021032042801380157 + 2.0 * 6.017670631408691
Epoch 1240, val loss: 1.1372727155685425
Epoch 1250, training loss: 12.039994239807129 = 0.020492853596806526 + 2.0 * 6.009750843048096
Epoch 1250, val loss: 1.142072319984436
Epoch 1260, training loss: 12.036086082458496 = 0.01998271606862545 + 2.0 * 6.008051872253418
Epoch 1260, val loss: 1.1467949151992798
Epoch 1270, training loss: 12.042838096618652 = 0.019491111859679222 + 2.0 * 6.011673450469971
Epoch 1270, val loss: 1.1514161825180054
Epoch 1280, training loss: 12.043417930603027 = 0.019018495455384254 + 2.0 * 6.012199878692627
Epoch 1280, val loss: 1.156078577041626
Epoch 1290, training loss: 12.036489486694336 = 0.018562844023108482 + 2.0 * 6.008963108062744
Epoch 1290, val loss: 1.1605892181396484
Epoch 1300, training loss: 12.030680656433105 = 0.018127646297216415 + 2.0 * 6.006276607513428
Epoch 1300, val loss: 1.1651768684387207
Epoch 1310, training loss: 12.029463768005371 = 0.01770608127117157 + 2.0 * 6.005878925323486
Epoch 1310, val loss: 1.1696269512176514
Epoch 1320, training loss: 12.035747528076172 = 0.01730036921799183 + 2.0 * 6.009223461151123
Epoch 1320, val loss: 1.1740202903747559
Epoch 1330, training loss: 12.028472900390625 = 0.016905328258872032 + 2.0 * 6.005783557891846
Epoch 1330, val loss: 1.1783653497695923
Epoch 1340, training loss: 12.025996208190918 = 0.016525018960237503 + 2.0 * 6.004735469818115
Epoch 1340, val loss: 1.1826801300048828
Epoch 1350, training loss: 12.025002479553223 = 0.016159385442733765 + 2.0 * 6.004421710968018
Epoch 1350, val loss: 1.1869477033615112
Epoch 1360, training loss: 12.03791618347168 = 0.015806637704372406 + 2.0 * 6.011054992675781
Epoch 1360, val loss: 1.1911137104034424
Epoch 1370, training loss: 12.025768280029297 = 0.015462604351341724 + 2.0 * 6.005152702331543
Epoch 1370, val loss: 1.195250391960144
Epoch 1380, training loss: 12.022318840026855 = 0.015134061686694622 + 2.0 * 6.003592491149902
Epoch 1380, val loss: 1.1994268894195557
Epoch 1390, training loss: 12.031377792358398 = 0.014815627597272396 + 2.0 * 6.008281230926514
Epoch 1390, val loss: 1.2034358978271484
Epoch 1400, training loss: 12.02794075012207 = 0.014503992162644863 + 2.0 * 6.006718158721924
Epoch 1400, val loss: 1.2074365615844727
Epoch 1410, training loss: 12.020552635192871 = 0.014208463951945305 + 2.0 * 6.003171920776367
Epoch 1410, val loss: 1.2113851308822632
Epoch 1420, training loss: 12.017426490783691 = 0.013921030797064304 + 2.0 * 6.001752853393555
Epoch 1420, val loss: 1.2153154611587524
Epoch 1430, training loss: 12.015833854675293 = 0.013642658479511738 + 2.0 * 6.001095771789551
Epoch 1430, val loss: 1.2192509174346924
Epoch 1440, training loss: 12.014453887939453 = 0.013371865265071392 + 2.0 * 6.0005412101745605
Epoch 1440, val loss: 1.2231422662734985
Epoch 1450, training loss: 12.014537811279297 = 0.013108341954648495 + 2.0 * 6.0007147789001465
Epoch 1450, val loss: 1.227003812789917
Epoch 1460, training loss: 12.036111831665039 = 0.01285505760461092 + 2.0 * 6.011628150939941
Epoch 1460, val loss: 1.2307028770446777
Epoch 1470, training loss: 12.013068199157715 = 0.012605879455804825 + 2.0 * 6.0002312660217285
Epoch 1470, val loss: 1.2344509363174438
Epoch 1480, training loss: 12.013933181762695 = 0.012369063682854176 + 2.0 * 6.000782012939453
Epoch 1480, val loss: 1.2382341623306274
Epoch 1490, training loss: 12.009591102600098 = 0.01213766448199749 + 2.0 * 5.998726844787598
Epoch 1490, val loss: 1.2418849468231201
Epoch 1500, training loss: 12.011506080627441 = 0.011912452057003975 + 2.0 * 5.9997968673706055
Epoch 1500, val loss: 1.2454439401626587
Epoch 1510, training loss: 12.0199556350708 = 0.011692867614328861 + 2.0 * 6.004131317138672
Epoch 1510, val loss: 1.2489612102508545
Epoch 1520, training loss: 12.009121894836426 = 0.011482740752398968 + 2.0 * 5.998819351196289
Epoch 1520, val loss: 1.2525891065597534
Epoch 1530, training loss: 12.007269859313965 = 0.011276853270828724 + 2.0 * 5.9979963302612305
Epoch 1530, val loss: 1.2561440467834473
Epoch 1540, training loss: 12.018779754638672 = 0.011079328134655952 + 2.0 * 6.003849983215332
Epoch 1540, val loss: 1.2595148086547852
Epoch 1550, training loss: 12.008984565734863 = 0.010882379487156868 + 2.0 * 5.999051094055176
Epoch 1550, val loss: 1.262955904006958
Epoch 1560, training loss: 12.00750732421875 = 0.010694525204598904 + 2.0 * 5.998406410217285
Epoch 1560, val loss: 1.2664045095443726
Epoch 1570, training loss: 12.009322166442871 = 0.010510171763598919 + 2.0 * 5.999405860900879
Epoch 1570, val loss: 1.269771933555603
Epoch 1580, training loss: 12.00382137298584 = 0.010331561788916588 + 2.0 * 5.9967451095581055
Epoch 1580, val loss: 1.2731754779815674
Epoch 1590, training loss: 12.007081985473633 = 0.010157751850783825 + 2.0 * 5.998462200164795
Epoch 1590, val loss: 1.2764781713485718
Epoch 1600, training loss: 12.003543853759766 = 0.009988371282815933 + 2.0 * 5.996777534484863
Epoch 1600, val loss: 1.2797454595565796
Epoch 1610, training loss: 12.003159523010254 = 0.009823514148592949 + 2.0 * 5.996667861938477
Epoch 1610, val loss: 1.283002257347107
Epoch 1620, training loss: 12.006123542785645 = 0.00966267567127943 + 2.0 * 5.998230457305908
Epoch 1620, val loss: 1.2861062288284302
Epoch 1630, training loss: 12.000595092773438 = 0.009506932459771633 + 2.0 * 5.995543956756592
Epoch 1630, val loss: 1.2893822193145752
Epoch 1640, training loss: 11.999756813049316 = 0.009355784393846989 + 2.0 * 5.9952006340026855
Epoch 1640, val loss: 1.2925041913986206
Epoch 1650, training loss: 11.99923324584961 = 0.009206632152199745 + 2.0 * 5.995013236999512
Epoch 1650, val loss: 1.2956254482269287
Epoch 1660, training loss: 12.008193016052246 = 0.009062024764716625 + 2.0 * 5.999565601348877
Epoch 1660, val loss: 1.2986125946044922
Epoch 1670, training loss: 12.002227783203125 = 0.008920957334339619 + 2.0 * 5.9966535568237305
Epoch 1670, val loss: 1.3015929460525513
Epoch 1680, training loss: 12.001382827758789 = 0.008784409612417221 + 2.0 * 5.9962992668151855
Epoch 1680, val loss: 1.3046859502792358
Epoch 1690, training loss: 12.000121116638184 = 0.008650895208120346 + 2.0 * 5.995735168457031
Epoch 1690, val loss: 1.3076114654541016
Epoch 1700, training loss: 11.996227264404297 = 0.00852067768573761 + 2.0 * 5.9938530921936035
Epoch 1700, val loss: 1.3105698823928833
Epoch 1710, training loss: 11.994006156921387 = 0.008394055999815464 + 2.0 * 5.9928059577941895
Epoch 1710, val loss: 1.3135409355163574
Epoch 1720, training loss: 11.992840766906738 = 0.008269612677395344 + 2.0 * 5.99228572845459
Epoch 1720, val loss: 1.3164688348770142
Epoch 1730, training loss: 12.009194374084473 = 0.008147146552801132 + 2.0 * 6.000523567199707
Epoch 1730, val loss: 1.3192360401153564
Epoch 1740, training loss: 11.999674797058105 = 0.008030405268073082 + 2.0 * 5.995822429656982
Epoch 1740, val loss: 1.3221359252929688
Epoch 1750, training loss: 11.992342948913574 = 0.007914437912404537 + 2.0 * 5.992214202880859
Epoch 1750, val loss: 1.3248906135559082
Epoch 1760, training loss: 11.989065170288086 = 0.007802968379110098 + 2.0 * 5.990631103515625
Epoch 1760, val loss: 1.3277558088302612
Epoch 1770, training loss: 11.989093780517578 = 0.007693469058722258 + 2.0 * 5.9907002449035645
Epoch 1770, val loss: 1.3305479288101196
Epoch 1780, training loss: 12.011129379272461 = 0.007586831226944923 + 2.0 * 6.001771450042725
Epoch 1780, val loss: 1.3332682847976685
Epoch 1790, training loss: 11.996258735656738 = 0.007480926346033812 + 2.0 * 5.994389057159424
Epoch 1790, val loss: 1.335830569267273
Epoch 1800, training loss: 11.988734245300293 = 0.007378443609923124 + 2.0 * 5.990677833557129
Epoch 1800, val loss: 1.3387447595596313
Epoch 1810, training loss: 11.9865140914917 = 0.00727824354544282 + 2.0 * 5.989617824554443
Epoch 1810, val loss: 1.3413523435592651
Epoch 1820, training loss: 11.994000434875488 = 0.007180841639637947 + 2.0 * 5.993409633636475
Epoch 1820, val loss: 1.343978762626648
Epoch 1830, training loss: 11.991739273071289 = 0.007083852309733629 + 2.0 * 5.992327690124512
Epoch 1830, val loss: 1.3465403318405151
Epoch 1840, training loss: 11.989442825317383 = 0.0069906641729176044 + 2.0 * 5.9912261962890625
Epoch 1840, val loss: 1.3491812944412231
Epoch 1850, training loss: 11.984682083129883 = 0.006899435073137283 + 2.0 * 5.988891124725342
Epoch 1850, val loss: 1.3518420457839966
Epoch 1860, training loss: 11.985526084899902 = 0.006810442078858614 + 2.0 * 5.989357948303223
Epoch 1860, val loss: 1.3543586730957031
Epoch 1870, training loss: 11.993523597717285 = 0.006722382269799709 + 2.0 * 5.993400573730469
Epoch 1870, val loss: 1.3568346500396729
Epoch 1880, training loss: 11.991524696350098 = 0.0066364239901304245 + 2.0 * 5.992444038391113
Epoch 1880, val loss: 1.3593029975891113
Epoch 1890, training loss: 11.99081039428711 = 0.006552002392709255 + 2.0 * 5.992129325866699
Epoch 1890, val loss: 1.3617589473724365
Epoch 1900, training loss: 11.98379135131836 = 0.006470402702689171 + 2.0 * 5.9886603355407715
Epoch 1900, val loss: 1.364157795906067
Epoch 1910, training loss: 11.982109069824219 = 0.006390376947820187 + 2.0 * 5.98785924911499
Epoch 1910, val loss: 1.3666276931762695
Epoch 1920, training loss: 11.980494499206543 = 0.006311838980764151 + 2.0 * 5.987091541290283
Epoch 1920, val loss: 1.3690650463104248
Epoch 1930, training loss: 11.9879732131958 = 0.006235139444470406 + 2.0 * 5.990869045257568
Epoch 1930, val loss: 1.3715239763259888
Epoch 1940, training loss: 11.981803894042969 = 0.006158293690532446 + 2.0 * 5.987823009490967
Epoch 1940, val loss: 1.373708963394165
Epoch 1950, training loss: 11.980061531066895 = 0.006084322463721037 + 2.0 * 5.986988544464111
Epoch 1950, val loss: 1.3761109113693237
Epoch 1960, training loss: 11.97940731048584 = 0.006011820398271084 + 2.0 * 5.986697673797607
Epoch 1960, val loss: 1.378448247909546
Epoch 1970, training loss: 11.979142189025879 = 0.005941550713032484 + 2.0 * 5.986600399017334
Epoch 1970, val loss: 1.3807512521743774
Epoch 1980, training loss: 11.993330955505371 = 0.005872180685400963 + 2.0 * 5.993729591369629
Epoch 1980, val loss: 1.3829363584518433
Epoch 1990, training loss: 11.985871315002441 = 0.00580291822552681 + 2.0 * 5.990034103393555
Epoch 1990, val loss: 1.3850520849227905
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8000
Overall ASR: 0.7712
Flip ASR: 0.7289/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.695730209350586 = 1.947928786277771 + 2.0 * 8.373900413513184
Epoch 0, val loss: 1.9435029029846191
Epoch 10, training loss: 18.68417739868164 = 1.937140941619873 + 2.0 * 8.373517990112305
Epoch 10, val loss: 1.9328187704086304
Epoch 20, training loss: 18.665374755859375 = 1.9239604473114014 + 2.0 * 8.370707511901855
Epoch 20, val loss: 1.919264554977417
Epoch 30, training loss: 18.609188079833984 = 1.9061610698699951 + 2.0 * 8.351513862609863
Epoch 30, val loss: 1.9006781578063965
Epoch 40, training loss: 18.394662857055664 = 1.8835245370864868 + 2.0 * 8.255569458007812
Epoch 40, val loss: 1.8778953552246094
Epoch 50, training loss: 17.621177673339844 = 1.8598268032073975 + 2.0 * 7.880675315856934
Epoch 50, val loss: 1.8544416427612305
Epoch 60, training loss: 17.016273498535156 = 1.8351168632507324 + 2.0 * 7.590578079223633
Epoch 60, val loss: 1.831601619720459
Epoch 70, training loss: 16.27492332458496 = 1.815285325050354 + 2.0 * 7.229818820953369
Epoch 70, val loss: 1.8140270709991455
Epoch 80, training loss: 15.59453010559082 = 1.7985280752182007 + 2.0 * 6.898001194000244
Epoch 80, val loss: 1.7983465194702148
Epoch 90, training loss: 15.263830184936523 = 1.783530592918396 + 2.0 * 6.740149974822998
Epoch 90, val loss: 1.7839716672897339
Epoch 100, training loss: 15.035386085510254 = 1.7681769132614136 + 2.0 * 6.633604526519775
Epoch 100, val loss: 1.7693140506744385
Epoch 110, training loss: 14.856907844543457 = 1.7514468431472778 + 2.0 * 6.552730560302734
Epoch 110, val loss: 1.754065752029419
Epoch 120, training loss: 14.714868545532227 = 1.7344694137573242 + 2.0 * 6.490199565887451
Epoch 120, val loss: 1.7390490770339966
Epoch 130, training loss: 14.58558464050293 = 1.7171568870544434 + 2.0 * 6.434213638305664
Epoch 130, val loss: 1.7239265441894531
Epoch 140, training loss: 14.485686302185059 = 1.6982909440994263 + 2.0 * 6.393697738647461
Epoch 140, val loss: 1.7078322172164917
Epoch 150, training loss: 14.405045509338379 = 1.676543951034546 + 2.0 * 6.364250659942627
Epoch 150, val loss: 1.6898412704467773
Epoch 160, training loss: 14.337200164794922 = 1.6517293453216553 + 2.0 * 6.342735290527344
Epoch 160, val loss: 1.6698858737945557
Epoch 170, training loss: 14.276638984680176 = 1.623387098312378 + 2.0 * 6.326625823974609
Epoch 170, val loss: 1.6472803354263306
Epoch 180, training loss: 14.217363357543945 = 1.5911648273468018 + 2.0 * 6.313099384307861
Epoch 180, val loss: 1.62215256690979
Epoch 190, training loss: 14.152782440185547 = 1.5553975105285645 + 2.0 * 6.29869270324707
Epoch 190, val loss: 1.5944163799285889
Epoch 200, training loss: 14.084295272827148 = 1.515924096107483 + 2.0 * 6.284185409545898
Epoch 200, val loss: 1.5645627975463867
Epoch 210, training loss: 14.01662540435791 = 1.4736801385879517 + 2.0 * 6.271472454071045
Epoch 210, val loss: 1.5330379009246826
Epoch 220, training loss: 13.948291778564453 = 1.4301183223724365 + 2.0 * 6.259086608886719
Epoch 220, val loss: 1.5012269020080566
Epoch 230, training loss: 13.877610206604004 = 1.3861218690872192 + 2.0 * 6.245744228363037
Epoch 230, val loss: 1.4695885181427002
Epoch 240, training loss: 13.808716773986816 = 1.3416297435760498 + 2.0 * 6.233543395996094
Epoch 240, val loss: 1.4380466938018799
Epoch 250, training loss: 13.745141983032227 = 1.297386884689331 + 2.0 * 6.223877429962158
Epoch 250, val loss: 1.4070563316345215
Epoch 260, training loss: 13.678938865661621 = 1.2544971704483032 + 2.0 * 6.212220668792725
Epoch 260, val loss: 1.377492070198059
Epoch 270, training loss: 13.616211891174316 = 1.213001012802124 + 2.0 * 6.201605319976807
Epoch 270, val loss: 1.3491789102554321
Epoch 280, training loss: 13.557422637939453 = 1.1723910570144653 + 2.0 * 6.192515850067139
Epoch 280, val loss: 1.3215481042861938
Epoch 290, training loss: 13.514585494995117 = 1.1326911449432373 + 2.0 * 6.19094705581665
Epoch 290, val loss: 1.2946021556854248
Epoch 300, training loss: 13.457026481628418 = 1.0946818590164185 + 2.0 * 6.1811723709106445
Epoch 300, val loss: 1.268523097038269
Epoch 310, training loss: 13.401244163513184 = 1.056989312171936 + 2.0 * 6.1721272468566895
Epoch 310, val loss: 1.2428017854690552
Epoch 320, training loss: 13.352130889892578 = 1.0194878578186035 + 2.0 * 6.166321277618408
Epoch 320, val loss: 1.2169989347457886
Epoch 330, training loss: 13.30292797088623 = 0.9820120930671692 + 2.0 * 6.160458087921143
Epoch 330, val loss: 1.1911654472351074
Epoch 340, training loss: 13.25885009765625 = 0.9446058869361877 + 2.0 * 6.1571221351623535
Epoch 340, val loss: 1.1653474569320679
Epoch 350, training loss: 13.212176322937012 = 0.907961905002594 + 2.0 * 6.152107238769531
Epoch 350, val loss: 1.1399459838867188
Epoch 360, training loss: 13.163824081420898 = 0.8722425699234009 + 2.0 * 6.1457905769348145
Epoch 360, val loss: 1.11538827419281
Epoch 370, training loss: 13.120562553405762 = 0.8372321724891663 + 2.0 * 6.141664981842041
Epoch 370, val loss: 1.0914314985275269
Epoch 380, training loss: 13.078256607055664 = 0.8029439449310303 + 2.0 * 6.137656211853027
Epoch 380, val loss: 1.0680886507034302
Epoch 390, training loss: 13.03903865814209 = 0.7696371078491211 + 2.0 * 6.134700775146484
Epoch 390, val loss: 1.0455577373504639
Epoch 400, training loss: 13.000024795532227 = 0.7377009391784668 + 2.0 * 6.131162166595459
Epoch 400, val loss: 1.0242618322372437
Epoch 410, training loss: 12.961146354675293 = 0.7068766355514526 + 2.0 * 6.127134799957275
Epoch 410, val loss: 1.0040383338928223
Epoch 420, training loss: 12.92413330078125 = 0.6770887970924377 + 2.0 * 6.1235222816467285
Epoch 420, val loss: 0.984695553779602
Epoch 430, training loss: 12.906438827514648 = 0.6483182311058044 + 2.0 * 6.1290602684021
Epoch 430, val loss: 0.966280460357666
Epoch 440, training loss: 12.860495567321777 = 0.6210394501686096 + 2.0 * 6.119728088378906
Epoch 440, val loss: 0.9491062760353088
Epoch 450, training loss: 12.824836730957031 = 0.5948639512062073 + 2.0 * 6.114986419677734
Epoch 450, val loss: 0.9330729842185974
Epoch 460, training loss: 12.794198989868164 = 0.5696476101875305 + 2.0 * 6.11227560043335
Epoch 460, val loss: 0.9179986119270325
Epoch 470, training loss: 12.771080017089844 = 0.5452972650527954 + 2.0 * 6.11289119720459
Epoch 470, val loss: 0.9038661122322083
Epoch 480, training loss: 12.739583969116211 = 0.5218650102615356 + 2.0 * 6.108859539031982
Epoch 480, val loss: 0.8906573057174683
Epoch 490, training loss: 12.715102195739746 = 0.4992790222167969 + 2.0 * 6.107911586761475
Epoch 490, val loss: 0.8784354329109192
Epoch 500, training loss: 12.68466854095459 = 0.4775203466415405 + 2.0 * 6.103574275970459
Epoch 500, val loss: 0.8671894073486328
Epoch 510, training loss: 12.65770149230957 = 0.4562985897064209 + 2.0 * 6.100701332092285
Epoch 510, val loss: 0.8565489649772644
Epoch 520, training loss: 12.638689994812012 = 0.435558557510376 + 2.0 * 6.101565837860107
Epoch 520, val loss: 0.8465314507484436
Epoch 530, training loss: 12.618340492248535 = 0.41534262895584106 + 2.0 * 6.101499080657959
Epoch 530, val loss: 0.8371431827545166
Epoch 540, training loss: 12.593531608581543 = 0.39554810523986816 + 2.0 * 6.098991870880127
Epoch 540, val loss: 0.8281658291816711
Epoch 550, training loss: 12.56450366973877 = 0.37626707553863525 + 2.0 * 6.094118118286133
Epoch 550, val loss: 0.8199418187141418
Epoch 560, training loss: 12.54004192352295 = 0.35727185010910034 + 2.0 * 6.0913848876953125
Epoch 560, val loss: 0.8122013807296753
Epoch 570, training loss: 12.516653060913086 = 0.33857107162475586 + 2.0 * 6.089041233062744
Epoch 570, val loss: 0.8048585653305054
Epoch 580, training loss: 12.49898910522461 = 0.3201897442340851 + 2.0 * 6.089399814605713
Epoch 580, val loss: 0.7979575991630554
Epoch 590, training loss: 12.487038612365723 = 0.302266001701355 + 2.0 * 6.092386245727539
Epoch 590, val loss: 0.7913920283317566
Epoch 600, training loss: 12.454496383666992 = 0.2849956452846527 + 2.0 * 6.084750175476074
Epoch 600, val loss: 0.7856019735336304
Epoch 610, training loss: 12.433327674865723 = 0.2683013379573822 + 2.0 * 6.082513332366943
Epoch 610, val loss: 0.7805491089820862
Epoch 620, training loss: 12.416400909423828 = 0.2522328794002533 + 2.0 * 6.0820841789245605
Epoch 620, val loss: 0.7761172652244568
Epoch 630, training loss: 12.395298957824707 = 0.23690444231033325 + 2.0 * 6.079197406768799
Epoch 630, val loss: 0.7722244262695312
Epoch 640, training loss: 12.38759994506836 = 0.2223709523677826 + 2.0 * 6.082614421844482
Epoch 640, val loss: 0.7691227197647095
Epoch 650, training loss: 12.36625862121582 = 0.20870307087898254 + 2.0 * 6.07877779006958
Epoch 650, val loss: 0.766564130783081
Epoch 660, training loss: 12.346517562866211 = 0.1959754228591919 + 2.0 * 6.075271129608154
Epoch 660, val loss: 0.7649608850479126
Epoch 670, training loss: 12.330568313598633 = 0.18406571447849274 + 2.0 * 6.073251247406006
Epoch 670, val loss: 0.7640520930290222
Epoch 680, training loss: 12.317843437194824 = 0.1729409396648407 + 2.0 * 6.072451114654541
Epoch 680, val loss: 0.763809323310852
Epoch 690, training loss: 12.315061569213867 = 0.16261878609657288 + 2.0 * 6.076221466064453
Epoch 690, val loss: 0.7641781568527222
Epoch 700, training loss: 12.293764114379883 = 0.15307636559009552 + 2.0 * 6.070343971252441
Epoch 700, val loss: 0.7650228142738342
Epoch 710, training loss: 12.279980659484863 = 0.1442636400461197 + 2.0 * 6.067858695983887
Epoch 710, val loss: 0.7665606737136841
Epoch 720, training loss: 12.267510414123535 = 0.13608098030090332 + 2.0 * 6.0657148361206055
Epoch 720, val loss: 0.7685328722000122
Epoch 730, training loss: 12.266292572021484 = 0.12848398089408875 + 2.0 * 6.068904399871826
Epoch 730, val loss: 0.7709017395973206
Epoch 740, training loss: 12.257285118103027 = 0.12144636362791061 + 2.0 * 6.0679192543029785
Epoch 740, val loss: 0.7735407948493958
Epoch 750, training loss: 12.2412748336792 = 0.11493925750255585 + 2.0 * 6.063167572021484
Epoch 750, val loss: 0.7765933275222778
Epoch 760, training loss: 12.232226371765137 = 0.108883798122406 + 2.0 * 6.061671257019043
Epoch 760, val loss: 0.7800230979919434
Epoch 770, training loss: 12.2396240234375 = 0.10323690623044968 + 2.0 * 6.068193435668945
Epoch 770, val loss: 0.7836266756057739
Epoch 780, training loss: 12.218439102172852 = 0.09805115312337875 + 2.0 * 6.06019401550293
Epoch 780, val loss: 0.787424623966217
Epoch 790, training loss: 12.208231925964355 = 0.09318623691797256 + 2.0 * 6.057522773742676
Epoch 790, val loss: 0.7915769219398499
Epoch 800, training loss: 12.200289726257324 = 0.08863016963005066 + 2.0 * 6.055830001831055
Epoch 800, val loss: 0.7959024906158447
Epoch 810, training loss: 12.193714141845703 = 0.08435939252376556 + 2.0 * 6.054677486419678
Epoch 810, val loss: 0.8003894090652466
Epoch 820, training loss: 12.196690559387207 = 0.08034779876470566 + 2.0 * 6.058171272277832
Epoch 820, val loss: 0.8049473762512207
Epoch 830, training loss: 12.185405731201172 = 0.0766032338142395 + 2.0 * 6.054401397705078
Epoch 830, val loss: 0.8096281290054321
Epoch 840, training loss: 12.177193641662598 = 0.07309073954820633 + 2.0 * 6.052051544189453
Epoch 840, val loss: 0.8144661784172058
Epoch 850, training loss: 12.179876327514648 = 0.06979399919509888 + 2.0 * 6.055041313171387
Epoch 850, val loss: 0.8194252848625183
Epoch 860, training loss: 12.170218467712402 = 0.06668248027563095 + 2.0 * 6.051767826080322
Epoch 860, val loss: 0.8242836594581604
Epoch 870, training loss: 12.163362503051758 = 0.06378006190061569 + 2.0 * 6.04979133605957
Epoch 870, val loss: 0.8293023705482483
Epoch 880, training loss: 12.155719757080078 = 0.06103450059890747 + 2.0 * 6.047342777252197
Epoch 880, val loss: 0.834392786026001
Epoch 890, training loss: 12.167344093322754 = 0.05845017358660698 + 2.0 * 6.054447174072266
Epoch 890, val loss: 0.8394961953163147
Epoch 900, training loss: 12.157099723815918 = 0.056015148758888245 + 2.0 * 6.05054235458374
Epoch 900, val loss: 0.8444382548332214
Epoch 910, training loss: 12.145398139953613 = 0.05373101308941841 + 2.0 * 6.045833587646484
Epoch 910, val loss: 0.8495599627494812
Epoch 920, training loss: 12.138419151306152 = 0.051569316536188126 + 2.0 * 6.0434250831604
Epoch 920, val loss: 0.8547478318214417
Epoch 930, training loss: 12.13539981842041 = 0.0495220422744751 + 2.0 * 6.042938709259033
Epoch 930, val loss: 0.8599348664283752
Epoch 940, training loss: 12.154629707336426 = 0.04759129509329796 + 2.0 * 6.053519248962402
Epoch 940, val loss: 0.8651123046875
Epoch 950, training loss: 12.128762245178223 = 0.045758556574583054 + 2.0 * 6.041501998901367
Epoch 950, val loss: 0.870240330696106
Epoch 960, training loss: 12.124826431274414 = 0.04403168335556984 + 2.0 * 6.0403971672058105
Epoch 960, val loss: 0.8754103183746338
Epoch 970, training loss: 12.124467849731445 = 0.04239562898874283 + 2.0 * 6.041036128997803
Epoch 970, val loss: 0.8806214332580566
Epoch 980, training loss: 12.123037338256836 = 0.0408480204641819 + 2.0 * 6.041094779968262
Epoch 980, val loss: 0.885685920715332
Epoch 990, training loss: 12.119718551635742 = 0.039382029324769974 + 2.0 * 6.040168285369873
Epoch 990, val loss: 0.8908106088638306
Epoch 1000, training loss: 12.112848281860352 = 0.03799184039235115 + 2.0 * 6.037428379058838
Epoch 1000, val loss: 0.8959147930145264
Epoch 1010, training loss: 12.111227989196777 = 0.03667091578245163 + 2.0 * 6.037278652191162
Epoch 1010, val loss: 0.9010331034660339
Epoch 1020, training loss: 12.117351531982422 = 0.03541283309459686 + 2.0 * 6.040969371795654
Epoch 1020, val loss: 0.9060988426208496
Epoch 1030, training loss: 12.108217239379883 = 0.03422393649816513 + 2.0 * 6.036996841430664
Epoch 1030, val loss: 0.9111136794090271
Epoch 1040, training loss: 12.102391242980957 = 0.03308650478720665 + 2.0 * 6.034652233123779
Epoch 1040, val loss: 0.9161484241485596
Epoch 1050, training loss: 12.099645614624023 = 0.032006148248910904 + 2.0 * 6.033819675445557
Epoch 1050, val loss: 0.9211940169334412
Epoch 1060, training loss: 12.105707168579102 = 0.030976850539445877 + 2.0 * 6.037364959716797
Epoch 1060, val loss: 0.9261855483055115
Epoch 1070, training loss: 12.102928161621094 = 0.029992613941431046 + 2.0 * 6.036467552185059
Epoch 1070, val loss: 0.9310650825500488
Epoch 1080, training loss: 12.095985412597656 = 0.029060566797852516 + 2.0 * 6.0334625244140625
Epoch 1080, val loss: 0.9359543919563293
Epoch 1090, training loss: 12.091472625732422 = 0.028169453144073486 + 2.0 * 6.031651496887207
Epoch 1090, val loss: 0.9408507347106934
Epoch 1100, training loss: 12.0935640335083 = 0.027319476008415222 + 2.0 * 6.0331220626831055
Epoch 1100, val loss: 0.9457680583000183
Epoch 1110, training loss: 12.090429306030273 = 0.026507718488574028 + 2.0 * 6.031960964202881
Epoch 1110, val loss: 0.9505478739738464
Epoch 1120, training loss: 12.086799621582031 = 0.025730645284056664 + 2.0 * 6.030534267425537
Epoch 1120, val loss: 0.955291211605072
Epoch 1130, training loss: 12.085021018981934 = 0.024985384196043015 + 2.0 * 6.030017852783203
Epoch 1130, val loss: 0.960001528263092
Epoch 1140, training loss: 12.08095645904541 = 0.024276111274957657 + 2.0 * 6.0283403396606445
Epoch 1140, val loss: 0.964742124080658
Epoch 1150, training loss: 12.081868171691895 = 0.023594122380018234 + 2.0 * 6.029137134552002
Epoch 1150, val loss: 0.9694600701332092
Epoch 1160, training loss: 12.07951831817627 = 0.022940412163734436 + 2.0 * 6.028288841247559
Epoch 1160, val loss: 0.9740846753120422
Epoch 1170, training loss: 12.082030296325684 = 0.022314036265015602 + 2.0 * 6.029858112335205
Epoch 1170, val loss: 0.9785873889923096
Epoch 1180, training loss: 12.08098030090332 = 0.021714191883802414 + 2.0 * 6.029633045196533
Epoch 1180, val loss: 0.9830759763717651
Epoch 1190, training loss: 12.073813438415527 = 0.021144580096006393 + 2.0 * 6.026334285736084
Epoch 1190, val loss: 0.9875743985176086
Epoch 1200, training loss: 12.070943832397461 = 0.020596155896782875 + 2.0 * 6.025173664093018
Epoch 1200, val loss: 0.9920344352722168
Epoch 1210, training loss: 12.072999954223633 = 0.020068436861038208 + 2.0 * 6.026465892791748
Epoch 1210, val loss: 0.9964883327484131
Epoch 1220, training loss: 12.073662757873535 = 0.019560080021619797 + 2.0 * 6.0270514488220215
Epoch 1220, val loss: 1.0008329153060913
Epoch 1230, training loss: 12.069802284240723 = 0.01906711421906948 + 2.0 * 6.025367736816406
Epoch 1230, val loss: 1.005157709121704
Epoch 1240, training loss: 12.069790840148926 = 0.0185993779450655 + 2.0 * 6.025595664978027
Epoch 1240, val loss: 1.0094547271728516
Epoch 1250, training loss: 12.064692497253418 = 0.018145566806197166 + 2.0 * 6.023273468017578
Epoch 1250, val loss: 1.0137122869491577
Epoch 1260, training loss: 12.063763618469238 = 0.017709381878376007 + 2.0 * 6.023026943206787
Epoch 1260, val loss: 1.0178868770599365
Epoch 1270, training loss: 12.069714546203613 = 0.017289046198129654 + 2.0 * 6.026212692260742
Epoch 1270, val loss: 1.022013783454895
Epoch 1280, training loss: 12.060555458068848 = 0.016888126730918884 + 2.0 * 6.021833896636963
Epoch 1280, val loss: 1.0261008739471436
Epoch 1290, training loss: 12.058650970458984 = 0.016499847173690796 + 2.0 * 6.02107572555542
Epoch 1290, val loss: 1.0302238464355469
Epoch 1300, training loss: 12.056601524353027 = 0.01612471230328083 + 2.0 * 6.020238399505615
Epoch 1300, val loss: 1.0343010425567627
Epoch 1310, training loss: 12.071344375610352 = 0.015763068571686745 + 2.0 * 6.027790546417236
Epoch 1310, val loss: 1.0383650064468384
Epoch 1320, training loss: 12.067033767700195 = 0.015413515269756317 + 2.0 * 6.025810241699219
Epoch 1320, val loss: 1.0423028469085693
Epoch 1330, training loss: 12.053105354309082 = 0.015075737610459328 + 2.0 * 6.019014835357666
Epoch 1330, val loss: 1.0461573600769043
Epoch 1340, training loss: 12.05279541015625 = 0.014751158654689789 + 2.0 * 6.019021987915039
Epoch 1340, val loss: 1.050044059753418
Epoch 1350, training loss: 12.049947738647461 = 0.014438604936003685 + 2.0 * 6.017754554748535
Epoch 1350, val loss: 1.0539952516555786
Epoch 1360, training loss: 12.052157402038574 = 0.014134414494037628 + 2.0 * 6.019011497497559
Epoch 1360, val loss: 1.0578240156173706
Epoch 1370, training loss: 12.054961204528809 = 0.013838425278663635 + 2.0 * 6.020561218261719
Epoch 1370, val loss: 1.0615673065185547
Epoch 1380, training loss: 12.0502290725708 = 0.01355205848813057 + 2.0 * 6.018338680267334
Epoch 1380, val loss: 1.065354824066162
Epoch 1390, training loss: 12.049944877624512 = 0.013278831727802753 + 2.0 * 6.0183329582214355
Epoch 1390, val loss: 1.0690627098083496
Epoch 1400, training loss: 12.044459342956543 = 0.013012279756367207 + 2.0 * 6.015723705291748
Epoch 1400, val loss: 1.0726938247680664
Epoch 1410, training loss: 12.04446029663086 = 0.012753902934491634 + 2.0 * 6.015853404998779
Epoch 1410, val loss: 1.076378583908081
Epoch 1420, training loss: 12.050386428833008 = 0.012503734789788723 + 2.0 * 6.018941402435303
Epoch 1420, val loss: 1.0799835920333862
Epoch 1430, training loss: 12.043917655944824 = 0.012260991148650646 + 2.0 * 6.0158281326293945
Epoch 1430, val loss: 1.0835137367248535
Epoch 1440, training loss: 12.05241870880127 = 0.012024080380797386 + 2.0 * 6.02019739151001
Epoch 1440, val loss: 1.087011456489563
Epoch 1450, training loss: 12.040911674499512 = 0.011799871921539307 + 2.0 * 6.014555931091309
Epoch 1450, val loss: 1.090507984161377
Epoch 1460, training loss: 12.039024353027344 = 0.011578250676393509 + 2.0 * 6.013722896575928
Epoch 1460, val loss: 1.0939027070999146
Epoch 1470, training loss: 12.036910057067871 = 0.011365448124706745 + 2.0 * 6.012772083282471
Epoch 1470, val loss: 1.0973647832870483
Epoch 1480, training loss: 12.039985656738281 = 0.011156894266605377 + 2.0 * 6.014414310455322
Epoch 1480, val loss: 1.100813627243042
Epoch 1490, training loss: 12.036796569824219 = 0.010954387485980988 + 2.0 * 6.01292085647583
Epoch 1490, val loss: 1.1040853261947632
Epoch 1500, training loss: 12.03540325164795 = 0.01075687725096941 + 2.0 * 6.012323379516602
Epoch 1500, val loss: 1.1074092388153076
Epoch 1510, training loss: 12.035518646240234 = 0.01056769397109747 + 2.0 * 6.012475490570068
Epoch 1510, val loss: 1.110731601715088
Epoch 1520, training loss: 12.04261589050293 = 0.010382624343037605 + 2.0 * 6.016116619110107
Epoch 1520, val loss: 1.113998532295227
Epoch 1530, training loss: 12.0347318649292 = 0.010200470685958862 + 2.0 * 6.012265682220459
Epoch 1530, val loss: 1.1171820163726807
Epoch 1540, training loss: 12.03638744354248 = 0.0100255711004138 + 2.0 * 6.013180732727051
Epoch 1540, val loss: 1.1204452514648438
Epoch 1550, training loss: 12.032567977905273 = 0.009854769334197044 + 2.0 * 6.011356830596924
Epoch 1550, val loss: 1.1235346794128418
Epoch 1560, training loss: 12.030074119567871 = 0.009690579026937485 + 2.0 * 6.010191917419434
Epoch 1560, val loss: 1.1267421245574951
Epoch 1570, training loss: 12.028006553649902 = 0.009528473019599915 + 2.0 * 6.009239196777344
Epoch 1570, val loss: 1.1298719644546509
Epoch 1580, training loss: 12.033133506774902 = 0.00937166903167963 + 2.0 * 6.011880874633789
Epoch 1580, val loss: 1.1329771280288696
Epoch 1590, training loss: 12.030548095703125 = 0.009217926301062107 + 2.0 * 6.010664939880371
Epoch 1590, val loss: 1.1360310316085815
Epoch 1600, training loss: 12.037158966064453 = 0.00906799454241991 + 2.0 * 6.014045715332031
Epoch 1600, val loss: 1.1390835046768188
Epoch 1610, training loss: 12.028451919555664 = 0.008923443965613842 + 2.0 * 6.009764194488525
Epoch 1610, val loss: 1.142090916633606
Epoch 1620, training loss: 12.02415943145752 = 0.008781826123595238 + 2.0 * 6.007688999176025
Epoch 1620, val loss: 1.145092248916626
Epoch 1630, training loss: 12.02271842956543 = 0.0086445864289999 + 2.0 * 6.0070366859436035
Epoch 1630, val loss: 1.1481050252914429
Epoch 1640, training loss: 12.030647277832031 = 0.008509151637554169 + 2.0 * 6.011069297790527
Epoch 1640, val loss: 1.151075005531311
Epoch 1650, training loss: 12.033743858337402 = 0.00837685912847519 + 2.0 * 6.012683391571045
Epoch 1650, val loss: 1.1539726257324219
Epoch 1660, training loss: 12.022784233093262 = 0.008250360377132893 + 2.0 * 6.007266998291016
Epoch 1660, val loss: 1.1568270921707153
Epoch 1670, training loss: 12.019371032714844 = 0.008126329630613327 + 2.0 * 6.005622386932373
Epoch 1670, val loss: 1.1597775220870972
Epoch 1680, training loss: 12.020203590393066 = 0.008004700765013695 + 2.0 * 6.006099224090576
Epoch 1680, val loss: 1.162574052810669
Epoch 1690, training loss: 12.02903938293457 = 0.00788634642958641 + 2.0 * 6.0105767250061035
Epoch 1690, val loss: 1.1654194593429565
Epoch 1700, training loss: 12.020711898803711 = 0.007770344614982605 + 2.0 * 6.006470680236816
Epoch 1700, val loss: 1.1682848930358887
Epoch 1710, training loss: 12.021247863769531 = 0.0076563735492527485 + 2.0 * 6.006795883178711
Epoch 1710, val loss: 1.1709940433502197
Epoch 1720, training loss: 12.018773078918457 = 0.007546655368059874 + 2.0 * 6.005613327026367
Epoch 1720, val loss: 1.17377769947052
Epoch 1730, training loss: 12.017953872680664 = 0.007438874337822199 + 2.0 * 6.005257606506348
Epoch 1730, val loss: 1.1765360832214355
Epoch 1740, training loss: 12.016891479492188 = 0.007333006244152784 + 2.0 * 6.00477933883667
Epoch 1740, val loss: 1.1792736053466797
Epoch 1750, training loss: 12.016814231872559 = 0.0072295041754841805 + 2.0 * 6.004792213439941
Epoch 1750, val loss: 1.1819179058074951
Epoch 1760, training loss: 12.029207229614258 = 0.007128192111849785 + 2.0 * 6.011039733886719
Epoch 1760, val loss: 1.1845812797546387
Epoch 1770, training loss: 12.016765594482422 = 0.007031555753201246 + 2.0 * 6.004867076873779
Epoch 1770, val loss: 1.1871403455734253
Epoch 1780, training loss: 12.013154029846191 = 0.006935156416147947 + 2.0 * 6.003109455108643
Epoch 1780, val loss: 1.1897486448287964
Epoch 1790, training loss: 12.012369155883789 = 0.006842208560556173 + 2.0 * 6.002763271331787
Epoch 1790, val loss: 1.1923563480377197
Epoch 1800, training loss: 12.017729759216309 = 0.006749981082975864 + 2.0 * 6.005489826202393
Epoch 1800, val loss: 1.19496750831604
Epoch 1810, training loss: 12.011941909790039 = 0.006659853272140026 + 2.0 * 6.002641201019287
Epoch 1810, val loss: 1.1975420713424683
Epoch 1820, training loss: 12.017187118530273 = 0.006571405101567507 + 2.0 * 6.005307674407959
Epoch 1820, val loss: 1.2001070976257324
Epoch 1830, training loss: 12.01067066192627 = 0.006485786288976669 + 2.0 * 6.002092361450195
Epoch 1830, val loss: 1.2026182413101196
Epoch 1840, training loss: 12.008764266967773 = 0.006401644088327885 + 2.0 * 6.001181125640869
Epoch 1840, val loss: 1.2051204442977905
Epoch 1850, training loss: 12.015932083129883 = 0.006319126579910517 + 2.0 * 6.0048065185546875
Epoch 1850, val loss: 1.2076274156570435
Epoch 1860, training loss: 12.008124351501465 = 0.006238649133592844 + 2.0 * 6.000942707061768
Epoch 1860, val loss: 1.210010051727295
Epoch 1870, training loss: 12.025825500488281 = 0.0061611272394657135 + 2.0 * 6.009832382202148
Epoch 1870, val loss: 1.2125004529953003
Epoch 1880, training loss: 12.012560844421387 = 0.006082526873797178 + 2.0 * 6.003239154815674
Epoch 1880, val loss: 1.214646339416504
Epoch 1890, training loss: 12.007475852966309 = 0.006008693482726812 + 2.0 * 6.000733375549316
Epoch 1890, val loss: 1.217067837715149
Epoch 1900, training loss: 12.004704475402832 = 0.00593488710001111 + 2.0 * 5.999384880065918
Epoch 1900, val loss: 1.2194660902023315
Epoch 1910, training loss: 12.00365161895752 = 0.005862484220415354 + 2.0 * 5.998894691467285
Epoch 1910, val loss: 1.2218644618988037
Epoch 1920, training loss: 12.004735946655273 = 0.005790784023702145 + 2.0 * 5.999472618103027
Epoch 1920, val loss: 1.2242605686187744
Epoch 1930, training loss: 12.015898704528809 = 0.005720917135477066 + 2.0 * 6.005088806152344
Epoch 1930, val loss: 1.2265647649765015
Epoch 1940, training loss: 12.006736755371094 = 0.005651596933603287 + 2.0 * 6.000542640686035
Epoch 1940, val loss: 1.2287464141845703
Epoch 1950, training loss: 12.005836486816406 = 0.005585182458162308 + 2.0 * 6.000125885009766
Epoch 1950, val loss: 1.231014370918274
Epoch 1960, training loss: 12.004481315612793 = 0.005519608035683632 + 2.0 * 5.999480724334717
Epoch 1960, val loss: 1.2332556247711182
Epoch 1970, training loss: 12.021918296813965 = 0.005456532351672649 + 2.0 * 6.008230686187744
Epoch 1970, val loss: 1.235534906387329
Epoch 1980, training loss: 12.00617504119873 = 0.005391937680542469 + 2.0 * 6.000391483306885
Epoch 1980, val loss: 1.2375835180282593
Epoch 1990, training loss: 12.001238822937012 = 0.005331050604581833 + 2.0 * 5.99795389175415
Epoch 1990, val loss: 1.2397558689117432
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7963
Overall ASR: 0.9926
Flip ASR: 0.9911/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.688547134399414 = 1.9407824277877808 + 2.0 * 8.373882293701172
Epoch 0, val loss: 1.9324431419372559
Epoch 10, training loss: 18.67717742919922 = 1.9309701919555664 + 2.0 * 8.373103141784668
Epoch 10, val loss: 1.9230924844741821
Epoch 20, training loss: 18.654380798339844 = 1.919175386428833 + 2.0 * 8.367602348327637
Epoch 20, val loss: 1.911386251449585
Epoch 30, training loss: 18.580846786499023 = 1.9037718772888184 + 2.0 * 8.338537216186523
Epoch 30, val loss: 1.8956800699234009
Epoch 40, training loss: 18.18360137939453 = 1.8844414949417114 + 2.0 * 8.149580001831055
Epoch 40, val loss: 1.8759922981262207
Epoch 50, training loss: 16.486637115478516 = 1.8626887798309326 + 2.0 * 7.311974048614502
Epoch 50, val loss: 1.8534289598464966
Epoch 60, training loss: 15.840568542480469 = 1.8435475826263428 + 2.0 * 6.998510360717773
Epoch 60, val loss: 1.8334999084472656
Epoch 70, training loss: 15.39555835723877 = 1.8261983394622803 + 2.0 * 6.784679889678955
Epoch 70, val loss: 1.815406084060669
Epoch 80, training loss: 15.116125106811523 = 1.8112820386886597 + 2.0 * 6.652421474456787
Epoch 80, val loss: 1.8003311157226562
Epoch 90, training loss: 14.877334594726562 = 1.7972501516342163 + 2.0 * 6.540042400360107
Epoch 90, val loss: 1.7857762575149536
Epoch 100, training loss: 14.72658634185791 = 1.7836682796478271 + 2.0 * 6.471458911895752
Epoch 100, val loss: 1.7720625400543213
Epoch 110, training loss: 14.603170394897461 = 1.7696890830993652 + 2.0 * 6.416740894317627
Epoch 110, val loss: 1.7584937810897827
Epoch 120, training loss: 14.495017051696777 = 1.755111575126648 + 2.0 * 6.36995267868042
Epoch 120, val loss: 1.7449405193328857
Epoch 130, training loss: 14.396150588989258 = 1.7400611639022827 + 2.0 * 6.328044891357422
Epoch 130, val loss: 1.731308102607727
Epoch 140, training loss: 14.31285285949707 = 1.7243258953094482 + 2.0 * 6.2942633628845215
Epoch 140, val loss: 1.717201828956604
Epoch 150, training loss: 14.24209976196289 = 1.7071253061294556 + 2.0 * 6.267487049102783
Epoch 150, val loss: 1.7022055387496948
Epoch 160, training loss: 14.173500061035156 = 1.6879136562347412 + 2.0 * 6.242793083190918
Epoch 160, val loss: 1.6859787702560425
Epoch 170, training loss: 14.115386962890625 = 1.6664780378341675 + 2.0 * 6.224454402923584
Epoch 170, val loss: 1.668092966079712
Epoch 180, training loss: 14.05229663848877 = 1.6420977115631104 + 2.0 * 6.205099582672119
Epoch 180, val loss: 1.6480681896209717
Epoch 190, training loss: 13.99463176727295 = 1.6144553422927856 + 2.0 * 6.190088272094727
Epoch 190, val loss: 1.6255807876586914
Epoch 200, training loss: 13.940336227416992 = 1.582983136177063 + 2.0 * 6.178676605224609
Epoch 200, val loss: 1.6001126766204834
Epoch 210, training loss: 13.883203506469727 = 1.5477626323699951 + 2.0 * 6.167720317840576
Epoch 210, val loss: 1.5718923807144165
Epoch 220, training loss: 13.826028823852539 = 1.5090985298156738 + 2.0 * 6.1584649085998535
Epoch 220, val loss: 1.5412752628326416
Epoch 230, training loss: 13.765909194946289 = 1.4667943716049194 + 2.0 * 6.149557590484619
Epoch 230, val loss: 1.5082125663757324
Epoch 240, training loss: 13.705896377563477 = 1.4211980104446411 + 2.0 * 6.1423492431640625
Epoch 240, val loss: 1.47304368019104
Epoch 250, training loss: 13.654004096984863 = 1.3729960918426514 + 2.0 * 6.140503883361816
Epoch 250, val loss: 1.4365111589431763
Epoch 260, training loss: 13.584783554077148 = 1.3234449625015259 + 2.0 * 6.130669116973877
Epoch 260, val loss: 1.4001401662826538
Epoch 270, training loss: 13.52487564086914 = 1.2734073400497437 + 2.0 * 6.125734329223633
Epoch 270, val loss: 1.3638992309570312
Epoch 280, training loss: 13.464458465576172 = 1.2228093147277832 + 2.0 * 6.120824337005615
Epoch 280, val loss: 1.3279879093170166
Epoch 290, training loss: 13.413545608520508 = 1.1719741821289062 + 2.0 * 6.120785713195801
Epoch 290, val loss: 1.2925856113433838
Epoch 300, training loss: 13.357454299926758 = 1.1220744848251343 + 2.0 * 6.117690086364746
Epoch 300, val loss: 1.2579829692840576
Epoch 310, training loss: 13.292657852172852 = 1.07310152053833 + 2.0 * 6.10977840423584
Epoch 310, val loss: 1.224528431892395
Epoch 320, training loss: 13.237968444824219 = 1.0253393650054932 + 2.0 * 6.106314659118652
Epoch 320, val loss: 1.1918278932571411
Epoch 330, training loss: 13.183035850524902 = 0.9782493710517883 + 2.0 * 6.10239315032959
Epoch 330, val loss: 1.1595308780670166
Epoch 340, training loss: 13.132514953613281 = 0.9320199489593506 + 2.0 * 6.100247383117676
Epoch 340, val loss: 1.1277960538864136
Epoch 350, training loss: 13.084664344787598 = 0.8877036571502686 + 2.0 * 6.098480224609375
Epoch 350, val loss: 1.0973738431930542
Epoch 360, training loss: 13.03320598602295 = 0.8458073735237122 + 2.0 * 6.0936994552612305
Epoch 360, val loss: 1.0687655210494995
Epoch 370, training loss: 12.987313270568848 = 0.806049108505249 + 2.0 * 6.09063196182251
Epoch 370, val loss: 1.041823148727417
Epoch 380, training loss: 12.943378448486328 = 0.7683923244476318 + 2.0 * 6.087492942810059
Epoch 380, val loss: 1.0166348218917847
Epoch 390, training loss: 12.902399063110352 = 0.732880175113678 + 2.0 * 6.08475923538208
Epoch 390, val loss: 0.9930887818336487
Epoch 400, training loss: 12.877068519592285 = 0.6995468139648438 + 2.0 * 6.088760852813721
Epoch 400, val loss: 0.9713085889816284
Epoch 410, training loss: 12.830046653747559 = 0.6690540313720703 + 2.0 * 6.080496311187744
Epoch 410, val loss: 0.951551079750061
Epoch 420, training loss: 12.79775619506836 = 0.6404827833175659 + 2.0 * 6.078636646270752
Epoch 420, val loss: 0.9336259365081787
Epoch 430, training loss: 12.766481399536133 = 0.6135626435279846 + 2.0 * 6.0764594078063965
Epoch 430, val loss: 0.9171936511993408
Epoch 440, training loss: 12.735736846923828 = 0.5882395505905151 + 2.0 * 6.073748588562012
Epoch 440, val loss: 0.9021185040473938
Epoch 450, training loss: 12.710647583007812 = 0.5643287897109985 + 2.0 * 6.073159217834473
Epoch 450, val loss: 0.8882639408111572
Epoch 460, training loss: 12.68145751953125 = 0.5415773987770081 + 2.0 * 6.069940090179443
Epoch 460, val loss: 0.8755676746368408
Epoch 470, training loss: 12.658978462219238 = 0.5197789669036865 + 2.0 * 6.069599628448486
Epoch 470, val loss: 0.863835334777832
Epoch 480, training loss: 12.633708953857422 = 0.49884966015815735 + 2.0 * 6.067429542541504
Epoch 480, val loss: 0.8529852628707886
Epoch 490, training loss: 12.607181549072266 = 0.4787512719631195 + 2.0 * 6.064215183258057
Epoch 490, val loss: 0.8430596590042114
Epoch 500, training loss: 12.590994834899902 = 0.4593204855918884 + 2.0 * 6.065837383270264
Epoch 500, val loss: 0.8337807655334473
Epoch 510, training loss: 12.565214157104492 = 0.4406186044216156 + 2.0 * 6.062297821044922
Epoch 510, val loss: 0.8251419067382812
Epoch 520, training loss: 12.546839714050293 = 0.42258891463279724 + 2.0 * 6.062125205993652
Epoch 520, val loss: 0.8171343207359314
Epoch 530, training loss: 12.525408744812012 = 0.40519705414772034 + 2.0 * 6.060105800628662
Epoch 530, val loss: 0.8097575902938843
Epoch 540, training loss: 12.500861167907715 = 0.3883114755153656 + 2.0 * 6.056274890899658
Epoch 540, val loss: 0.8029057383537292
Epoch 550, training loss: 12.481819152832031 = 0.3718983232975006 + 2.0 * 6.054960250854492
Epoch 550, val loss: 0.7965366244316101
Epoch 560, training loss: 12.464303970336914 = 0.35593080520629883 + 2.0 * 6.054186820983887
Epoch 560, val loss: 0.7906072735786438
Epoch 570, training loss: 12.450183868408203 = 0.340448796749115 + 2.0 * 6.054867744445801
Epoch 570, val loss: 0.7851939797401428
Epoch 580, training loss: 12.428511619567871 = 0.3255462050437927 + 2.0 * 6.051482677459717
Epoch 580, val loss: 0.7801787257194519
Epoch 590, training loss: 12.410526275634766 = 0.311056524515152 + 2.0 * 6.049735069274902
Epoch 590, val loss: 0.7754960060119629
Epoch 600, training loss: 12.399266242980957 = 0.2970123887062073 + 2.0 * 6.051126956939697
Epoch 600, val loss: 0.77118980884552
Epoch 610, training loss: 12.388480186462402 = 0.2834858000278473 + 2.0 * 6.052497386932373
Epoch 610, val loss: 0.76701420545578
Epoch 620, training loss: 12.36319637298584 = 0.27047643065452576 + 2.0 * 6.046360015869141
Epoch 620, val loss: 0.7633012533187866
Epoch 630, training loss: 12.348749160766602 = 0.25793424248695374 + 2.0 * 6.045407295227051
Epoch 630, val loss: 0.7599228024482727
Epoch 640, training loss: 12.355074882507324 = 0.24579070508480072 + 2.0 * 6.054642200469971
Epoch 640, val loss: 0.7567104697227478
Epoch 650, training loss: 12.328001022338867 = 0.23426133394241333 + 2.0 * 6.04686975479126
Epoch 650, val loss: 0.7538068890571594
Epoch 660, training loss: 12.308364868164062 = 0.22311809659004211 + 2.0 * 6.042623519897461
Epoch 660, val loss: 0.7510712146759033
Epoch 670, training loss: 12.295401573181152 = 0.21241942048072815 + 2.0 * 6.0414910316467285
Epoch 670, val loss: 0.7485284209251404
Epoch 680, training loss: 12.282004356384277 = 0.20214955508708954 + 2.0 * 6.0399274826049805
Epoch 680, val loss: 0.7463514804840088
Epoch 690, training loss: 12.271286964416504 = 0.1922837495803833 + 2.0 * 6.039501667022705
Epoch 690, val loss: 0.7444761991500854
Epoch 700, training loss: 12.264158248901367 = 0.1828952133655548 + 2.0 * 6.040631294250488
Epoch 700, val loss: 0.7428492307662964
Epoch 710, training loss: 12.25059700012207 = 0.17400619387626648 + 2.0 * 6.038295269012451
Epoch 710, val loss: 0.7415001392364502
Epoch 720, training loss: 12.239748001098633 = 0.16556529700756073 + 2.0 * 6.037091255187988
Epoch 720, val loss: 0.7405681014060974
Epoch 730, training loss: 12.233838081359863 = 0.15753963589668274 + 2.0 * 6.038149356842041
Epoch 730, val loss: 0.7399666905403137
Epoch 740, training loss: 12.223465919494629 = 0.1499338299036026 + 2.0 * 6.036766052246094
Epoch 740, val loss: 0.7394941449165344
Epoch 750, training loss: 12.211275100708008 = 0.14274868369102478 + 2.0 * 6.0342631340026855
Epoch 750, val loss: 0.739469051361084
Epoch 760, training loss: 12.202651023864746 = 0.13596288859844208 + 2.0 * 6.033344268798828
Epoch 760, val loss: 0.7398093342781067
Epoch 770, training loss: 12.194355010986328 = 0.12954063713550568 + 2.0 * 6.032407283782959
Epoch 770, val loss: 0.7404848337173462
Epoch 780, training loss: 12.206562995910645 = 0.12349440902471542 + 2.0 * 6.041534423828125
Epoch 780, val loss: 0.7414334416389465
Epoch 790, training loss: 12.179791450500488 = 0.11777342855930328 + 2.0 * 6.031009197235107
Epoch 790, val loss: 0.7425187230110168
Epoch 800, training loss: 12.173864364624023 = 0.11239846795797348 + 2.0 * 6.030733108520508
Epoch 800, val loss: 0.7438883185386658
Epoch 810, training loss: 12.168599128723145 = 0.1073378250002861 + 2.0 * 6.030630588531494
Epoch 810, val loss: 0.74552983045578
Epoch 820, training loss: 12.160046577453613 = 0.1025545746088028 + 2.0 * 6.028746128082275
Epoch 820, val loss: 0.7473521828651428
Epoch 830, training loss: 12.154159545898438 = 0.09804096072912216 + 2.0 * 6.028059482574463
Epoch 830, val loss: 0.7494120001792908
Epoch 840, training loss: 12.149477005004883 = 0.0937819629907608 + 2.0 * 6.0278472900390625
Epoch 840, val loss: 0.7515749931335449
Epoch 850, training loss: 12.149572372436523 = 0.08974382281303406 + 2.0 * 6.029914379119873
Epoch 850, val loss: 0.7539615035057068
Epoch 860, training loss: 12.144420623779297 = 0.08594902604818344 + 2.0 * 6.02923583984375
Epoch 860, val loss: 0.7563077211380005
Epoch 870, training loss: 12.1332426071167 = 0.08237205445766449 + 2.0 * 6.025435447692871
Epoch 870, val loss: 0.7589722871780396
Epoch 880, training loss: 12.128040313720703 = 0.07898729294538498 + 2.0 * 6.024526596069336
Epoch 880, val loss: 0.7616389393806458
Epoch 890, training loss: 12.132476806640625 = 0.07578368484973907 + 2.0 * 6.028346538543701
Epoch 890, val loss: 0.7645443081855774
Epoch 900, training loss: 12.12147331237793 = 0.0727408155798912 + 2.0 * 6.02436637878418
Epoch 900, val loss: 0.7672969102859497
Epoch 910, training loss: 12.113970756530762 = 0.06986723840236664 + 2.0 * 6.022051811218262
Epoch 910, val loss: 0.7703328728675842
Epoch 920, training loss: 12.11038589477539 = 0.06714069843292236 + 2.0 * 6.021622657775879
Epoch 920, val loss: 0.7733476161956787
Epoch 930, training loss: 12.120530128479004 = 0.06454883515834808 + 2.0 * 6.027990818023682
Epoch 930, val loss: 0.776468813419342
Epoch 940, training loss: 12.1100492477417 = 0.062106575816869736 + 2.0 * 6.0239715576171875
Epoch 940, val loss: 0.7797952890396118
Epoch 950, training loss: 12.098435401916504 = 0.05978779122233391 + 2.0 * 6.019323825836182
Epoch 950, val loss: 0.7829369902610779
Epoch 960, training loss: 12.09523868560791 = 0.057585205882787704 + 2.0 * 6.018826961517334
Epoch 960, val loss: 0.7862890958786011
Epoch 970, training loss: 12.093645095825195 = 0.055490054190158844 + 2.0 * 6.019077301025391
Epoch 970, val loss: 0.7897060513496399
Epoch 980, training loss: 12.097530364990234 = 0.05350254476070404 + 2.0 * 6.022014141082764
Epoch 980, val loss: 0.7931527495384216
Epoch 990, training loss: 12.087752342224121 = 0.05160919576883316 + 2.0 * 6.01807165145874
Epoch 990, val loss: 0.7965112328529358
Epoch 1000, training loss: 12.083427429199219 = 0.04981062561273575 + 2.0 * 6.01680850982666
Epoch 1000, val loss: 0.8000208139419556
Epoch 1010, training loss: 12.080491065979004 = 0.048099856823682785 + 2.0 * 6.016195774078369
Epoch 1010, val loss: 0.8035522103309631
Epoch 1020, training loss: 12.096282958984375 = 0.04646521806716919 + 2.0 * 6.024909019470215
Epoch 1020, val loss: 0.8070905208587646
Epoch 1030, training loss: 12.080495834350586 = 0.04492578282952309 + 2.0 * 6.01778507232666
Epoch 1030, val loss: 0.8107359409332275
Epoch 1040, training loss: 12.072610855102539 = 0.043442051857709885 + 2.0 * 6.014584541320801
Epoch 1040, val loss: 0.8143309950828552
Epoch 1050, training loss: 12.06938362121582 = 0.042031098157167435 + 2.0 * 6.013676166534424
Epoch 1050, val loss: 0.8179619908332825
Epoch 1060, training loss: 12.07070541381836 = 0.04067838564515114 + 2.0 * 6.015013694763184
Epoch 1060, val loss: 0.8217187523841858
Epoch 1070, training loss: 12.06801700592041 = 0.039390936493873596 + 2.0 * 6.014313220977783
Epoch 1070, val loss: 0.825351893901825
Epoch 1080, training loss: 12.066818237304688 = 0.03815649822354317 + 2.0 * 6.014330863952637
Epoch 1080, val loss: 0.8289682269096375
Epoch 1090, training loss: 12.060275077819824 = 0.03698199614882469 + 2.0 * 6.011646747589111
Epoch 1090, val loss: 0.8326152563095093
Epoch 1100, training loss: 12.066368103027344 = 0.035854488611221313 + 2.0 * 6.015256881713867
Epoch 1100, val loss: 0.8363066911697388
Epoch 1110, training loss: 12.058362007141113 = 0.03477897122502327 + 2.0 * 6.011791706085205
Epoch 1110, val loss: 0.8401162624359131
Epoch 1120, training loss: 12.055887222290039 = 0.033751461654901505 + 2.0 * 6.011067867279053
Epoch 1120, val loss: 0.8437381386756897
Epoch 1130, training loss: 12.053313255310059 = 0.03276593238115311 + 2.0 * 6.010273456573486
Epoch 1130, val loss: 0.847506046295166
Epoch 1140, training loss: 12.057024955749512 = 0.03182134032249451 + 2.0 * 6.012601852416992
Epoch 1140, val loss: 0.8512560129165649
Epoch 1150, training loss: 12.050359725952148 = 0.03091173619031906 + 2.0 * 6.009724140167236
Epoch 1150, val loss: 0.8549668192863464
Epoch 1160, training loss: 12.04990005493164 = 0.030040552839636803 + 2.0 * 6.009929656982422
Epoch 1160, val loss: 0.858613133430481
Epoch 1170, training loss: 12.052762985229492 = 0.029209909960627556 + 2.0 * 6.011776447296143
Epoch 1170, val loss: 0.8623605966567993
Epoch 1180, training loss: 12.045806884765625 = 0.02840539813041687 + 2.0 * 6.008700847625732
Epoch 1180, val loss: 0.8659687638282776
Epoch 1190, training loss: 12.045433044433594 = 0.02763628587126732 + 2.0 * 6.0088982582092285
Epoch 1190, val loss: 0.8696032762527466
Epoch 1200, training loss: 12.047037124633789 = 0.02690254896879196 + 2.0 * 6.010067462921143
Epoch 1200, val loss: 0.8733159303665161
Epoch 1210, training loss: 12.040282249450684 = 0.026190361008048058 + 2.0 * 6.007045745849609
Epoch 1210, val loss: 0.8769222497940063
Epoch 1220, training loss: 12.037825584411621 = 0.025508606806397438 + 2.0 * 6.006158351898193
Epoch 1220, val loss: 0.8804624080657959
Epoch 1230, training loss: 12.035910606384277 = 0.02485237456858158 + 2.0 * 6.005528926849365
Epoch 1230, val loss: 0.8840371966362
Epoch 1240, training loss: 12.034148216247559 = 0.02421901375055313 + 2.0 * 6.004964828491211
Epoch 1240, val loss: 0.8876410722732544
Epoch 1250, training loss: 12.035417556762695 = 0.023608732968568802 + 2.0 * 6.005904197692871
Epoch 1250, val loss: 0.8913180232048035
Epoch 1260, training loss: 12.046051025390625 = 0.023025358095765114 + 2.0 * 6.011512756347656
Epoch 1260, val loss: 0.8947975635528564
Epoch 1270, training loss: 12.037691116333008 = 0.02245665155351162 + 2.0 * 6.007616996765137
Epoch 1270, val loss: 0.8982846736907959
Epoch 1280, training loss: 12.030362129211426 = 0.021917684003710747 + 2.0 * 6.004222393035889
Epoch 1280, val loss: 0.9017070531845093
Epoch 1290, training loss: 12.027517318725586 = 0.02139347977936268 + 2.0 * 6.003061771392822
Epoch 1290, val loss: 0.9051960706710815
Epoch 1300, training loss: 12.027966499328613 = 0.020886288955807686 + 2.0 * 6.0035400390625
Epoch 1300, val loss: 0.908698320388794
Epoch 1310, training loss: 12.039042472839355 = 0.02039666846394539 + 2.0 * 6.0093231201171875
Epoch 1310, val loss: 0.9122012257575989
Epoch 1320, training loss: 12.025177955627441 = 0.01992841064929962 + 2.0 * 6.002624988555908
Epoch 1320, val loss: 0.9156934022903442
Epoch 1330, training loss: 12.022854804992676 = 0.01947435736656189 + 2.0 * 6.00169038772583
Epoch 1330, val loss: 0.9191336035728455
Epoch 1340, training loss: 12.0235595703125 = 0.019034797325730324 + 2.0 * 6.002262592315674
Epoch 1340, val loss: 0.9225946664810181
Epoch 1350, training loss: 12.034688949584961 = 0.01861194707453251 + 2.0 * 6.008038520812988
Epoch 1350, val loss: 0.926110029220581
Epoch 1360, training loss: 12.021649360656738 = 0.018197836354374886 + 2.0 * 6.001725673675537
Epoch 1360, val loss: 0.9293817281723022
Epoch 1370, training loss: 12.018280029296875 = 0.01780146360397339 + 2.0 * 6.000239372253418
Epoch 1370, val loss: 0.9327131509780884
Epoch 1380, training loss: 12.017965316772461 = 0.01741693541407585 + 2.0 * 6.000274181365967
Epoch 1380, val loss: 0.9361014366149902
Epoch 1390, training loss: 12.027425765991211 = 0.017042770981788635 + 2.0 * 6.005191326141357
Epoch 1390, val loss: 0.9394493103027344
Epoch 1400, training loss: 12.031474113464355 = 0.016683170571923256 + 2.0 * 6.007395267486572
Epoch 1400, val loss: 0.9427571892738342
Epoch 1410, training loss: 12.01743221282959 = 0.016338177025318146 + 2.0 * 6.000546932220459
Epoch 1410, val loss: 0.9460856318473816
Epoch 1420, training loss: 12.014900207519531 = 0.016002612188458443 + 2.0 * 5.999448776245117
Epoch 1420, val loss: 0.9493196606636047
Epoch 1430, training loss: 12.020089149475098 = 0.015676623210310936 + 2.0 * 6.002206325531006
Epoch 1430, val loss: 0.9526194334030151
Epoch 1440, training loss: 12.01124382019043 = 0.015360690653324127 + 2.0 * 5.997941493988037
Epoch 1440, val loss: 0.9558666944503784
Epoch 1450, training loss: 12.011762619018555 = 0.01505478285253048 + 2.0 * 5.998353958129883
Epoch 1450, val loss: 0.9591134786605835
Epoch 1460, training loss: 12.011954307556152 = 0.014757811091840267 + 2.0 * 5.998598098754883
Epoch 1460, val loss: 0.9623757600784302
Epoch 1470, training loss: 12.021824836730957 = 0.014471370726823807 + 2.0 * 6.003676891326904
Epoch 1470, val loss: 0.9655840992927551
Epoch 1480, training loss: 12.012701034545898 = 0.014186253771185875 + 2.0 * 5.999257564544678
Epoch 1480, val loss: 0.9686932563781738
Epoch 1490, training loss: 12.013960838317871 = 0.013917816802859306 + 2.0 * 6.000021457672119
Epoch 1490, val loss: 0.9718511700630188
Epoch 1500, training loss: 12.008875846862793 = 0.013654354959726334 + 2.0 * 5.997610569000244
Epoch 1500, val loss: 0.9749358296394348
Epoch 1510, training loss: 12.006242752075195 = 0.013398872688412666 + 2.0 * 5.996421813964844
Epoch 1510, val loss: 0.9780160188674927
Epoch 1520, training loss: 12.009788513183594 = 0.013149506412446499 + 2.0 * 5.998319625854492
Epoch 1520, val loss: 0.9811238646507263
Epoch 1530, training loss: 12.005767822265625 = 0.012908054515719414 + 2.0 * 5.996429920196533
Epoch 1530, val loss: 0.9842601418495178
Epoch 1540, training loss: 12.008177757263184 = 0.012673280201852322 + 2.0 * 5.9977521896362305
Epoch 1540, val loss: 0.9872454404830933
Epoch 1550, training loss: 12.013250350952148 = 0.012446833774447441 + 2.0 * 6.000401973724365
Epoch 1550, val loss: 0.9902340173721313
Epoch 1560, training loss: 12.008482933044434 = 0.012225641869008541 + 2.0 * 5.998128414154053
Epoch 1560, val loss: 0.9932193160057068
Epoch 1570, training loss: 12.00144100189209 = 0.012013128958642483 + 2.0 * 5.99471378326416
Epoch 1570, val loss: 0.9961978197097778
Epoch 1580, training loss: 12.000761985778809 = 0.011805628426373005 + 2.0 * 5.994478225708008
Epoch 1580, val loss: 0.9991840720176697
Epoch 1590, training loss: 12.003296852111816 = 0.011603745631873608 + 2.0 * 5.995846748352051
Epoch 1590, val loss: 1.0022573471069336
Epoch 1600, training loss: 12.005730628967285 = 0.011406072415411472 + 2.0 * 5.997162342071533
Epoch 1600, val loss: 1.0052059888839722
Epoch 1610, training loss: 11.999951362609863 = 0.01121023204177618 + 2.0 * 5.994370460510254
Epoch 1610, val loss: 1.0079728364944458
Epoch 1620, training loss: 11.996880531311035 = 0.011023313738405704 + 2.0 * 5.992928504943848
Epoch 1620, val loss: 1.0109138488769531
Epoch 1630, training loss: 12.00012493133545 = 0.010840456001460552 + 2.0 * 5.99464225769043
Epoch 1630, val loss: 1.0138589143753052
Epoch 1640, training loss: 12.006614685058594 = 0.010662217624485493 + 2.0 * 5.997976303100586
Epoch 1640, val loss: 1.016663908958435
Epoch 1650, training loss: 12.000521659851074 = 0.010490589775145054 + 2.0 * 5.995015621185303
Epoch 1650, val loss: 1.019568920135498
Epoch 1660, training loss: 11.998470306396484 = 0.010322253219783306 + 2.0 * 5.994073867797852
Epoch 1660, val loss: 1.0222305059432983
Epoch 1670, training loss: 11.99461841583252 = 0.01015814021229744 + 2.0 * 5.99222993850708
Epoch 1670, val loss: 1.0251106023788452
Epoch 1680, training loss: 11.996532440185547 = 0.00999786239117384 + 2.0 * 5.993267059326172
Epoch 1680, val loss: 1.027930498123169
Epoch 1690, training loss: 12.002130508422852 = 0.009843781590461731 + 2.0 * 5.996143341064453
Epoch 1690, val loss: 1.030769944190979
Epoch 1700, training loss: 11.998028755187988 = 0.009691146202385426 + 2.0 * 5.994168758392334
Epoch 1700, val loss: 1.0334172248840332
Epoch 1710, training loss: 11.992859840393066 = 0.009541558101773262 + 2.0 * 5.991659164428711
Epoch 1710, val loss: 1.0360642671585083
Epoch 1720, training loss: 11.99118423461914 = 0.009398454800248146 + 2.0 * 5.9908928871154785
Epoch 1720, val loss: 1.038776159286499
Epoch 1730, training loss: 11.99199104309082 = 0.009256131015717983 + 2.0 * 5.991367340087891
Epoch 1730, val loss: 1.0415699481964111
Epoch 1740, training loss: 12.002242088317871 = 0.00911943893879652 + 2.0 * 5.996561527252197
Epoch 1740, val loss: 1.044305682182312
Epoch 1750, training loss: 11.992576599121094 = 0.008982712402939796 + 2.0 * 5.991796970367432
Epoch 1750, val loss: 1.0468664169311523
Epoch 1760, training loss: 11.988167762756348 = 0.008851864375174046 + 2.0 * 5.989657878875732
Epoch 1760, val loss: 1.0494955778121948
Epoch 1770, training loss: 11.987631797790527 = 0.008723147213459015 + 2.0 * 5.98945426940918
Epoch 1770, val loss: 1.0521621704101562
Epoch 1780, training loss: 11.99622631072998 = 0.008597915060818195 + 2.0 * 5.993813991546631
Epoch 1780, val loss: 1.054850459098816
Epoch 1790, training loss: 11.995382308959961 = 0.00847508106380701 + 2.0 * 5.993453502655029
Epoch 1790, val loss: 1.0575242042541504
Epoch 1800, training loss: 11.99240779876709 = 0.008356081321835518 + 2.0 * 5.992025852203369
Epoch 1800, val loss: 1.0600191354751587
Epoch 1810, training loss: 11.988085746765137 = 0.008238266222178936 + 2.0 * 5.98992395401001
Epoch 1810, val loss: 1.0624606609344482
Epoch 1820, training loss: 11.984932899475098 = 0.008124619722366333 + 2.0 * 5.988404273986816
Epoch 1820, val loss: 1.065079927444458
Epoch 1830, training loss: 11.986496925354004 = 0.008012184873223305 + 2.0 * 5.9892425537109375
Epoch 1830, val loss: 1.0677218437194824
Epoch 1840, training loss: 11.994871139526367 = 0.007902419194579124 + 2.0 * 5.9934844970703125
Epoch 1840, val loss: 1.0702872276306152
Epoch 1850, training loss: 11.988219261169434 = 0.007796142715960741 + 2.0 * 5.990211486816406
Epoch 1850, val loss: 1.0726921558380127
Epoch 1860, training loss: 11.989343643188477 = 0.0076906513422727585 + 2.0 * 5.990826606750488
Epoch 1860, val loss: 1.0751315355300903
Epoch 1870, training loss: 11.985556602478027 = 0.0075890556909143925 + 2.0 * 5.988983631134033
Epoch 1870, val loss: 1.0776535272598267
Epoch 1880, training loss: 11.984162330627441 = 0.007490220014005899 + 2.0 * 5.988336086273193
Epoch 1880, val loss: 1.0801880359649658
Epoch 1890, training loss: 11.983963012695312 = 0.007391736842691898 + 2.0 * 5.988285541534424
Epoch 1890, val loss: 1.0825964212417603
Epoch 1900, training loss: 11.985387802124023 = 0.007295813877135515 + 2.0 * 5.989046096801758
Epoch 1900, val loss: 1.0850355625152588
Epoch 1910, training loss: 11.987519264221191 = 0.00720285763964057 + 2.0 * 5.9901580810546875
Epoch 1910, val loss: 1.08744478225708
Epoch 1920, training loss: 11.983479499816895 = 0.007111191283911467 + 2.0 * 5.988183975219727
Epoch 1920, val loss: 1.0898253917694092
Epoch 1930, training loss: 11.981781959533691 = 0.007021806668490171 + 2.0 * 5.987380027770996
Epoch 1930, val loss: 1.0921577215194702
Epoch 1940, training loss: 11.983979225158691 = 0.006934594362974167 + 2.0 * 5.988522529602051
Epoch 1940, val loss: 1.0945714712142944
Epoch 1950, training loss: 11.983427047729492 = 0.006848510354757309 + 2.0 * 5.9882893562316895
Epoch 1950, val loss: 1.0969911813735962
Epoch 1960, training loss: 11.978446960449219 = 0.006762995384633541 + 2.0 * 5.985841751098633
Epoch 1960, val loss: 1.0993045568466187
Epoch 1970, training loss: 11.98233413696289 = 0.006680571008473635 + 2.0 * 5.987826824188232
Epoch 1970, val loss: 1.1016634702682495
Epoch 1980, training loss: 11.984445571899414 = 0.006600505672395229 + 2.0 * 5.988922595977783
Epoch 1980, val loss: 1.1040034294128418
Epoch 1990, training loss: 11.979690551757812 = 0.006521299481391907 + 2.0 * 5.986584663391113
Epoch 1990, val loss: 1.1062450408935547
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8148
Overall ASR: 0.7638
Flip ASR: 0.7289/225 nodes
The final ASR:0.84256, 0.10615, Accuracy:0.80370, 0.00800
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11556])
remove edge: torch.Size([2, 9476])
updated graph: torch.Size([2, 10476])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9852
Flip ASR: 0.9822/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9963
Flip ASR: 0.9956/225 nodes
The final ASR:0.98524, 0.00904, Accuracy:0.82963, 0.00605
Begin epxeriment: cont_weight: 2 epoch:2000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 18.697166442871094 = 1.9493962526321411 + 2.0 * 8.373885154724121
Epoch 0, val loss: 1.9488518238067627
Epoch 10, training loss: 18.68573570251465 = 1.938922643661499 + 2.0 * 8.373406410217285
Epoch 10, val loss: 1.9384969472885132
Epoch 20, training loss: 18.665449142456055 = 1.9258826971054077 + 2.0 * 8.369783401489258
Epoch 20, val loss: 1.9255486726760864
Epoch 30, training loss: 18.589115142822266 = 1.9085001945495605 + 2.0 * 8.340307235717773
Epoch 30, val loss: 1.908471941947937
Epoch 40, training loss: 18.075429916381836 = 1.8883779048919678 + 2.0 * 8.093525886535645
Epoch 40, val loss: 1.889709711074829
Epoch 50, training loss: 16.627233505249023 = 1.8683325052261353 + 2.0 * 7.37945032119751
Epoch 50, val loss: 1.8716319799423218
Epoch 60, training loss: 15.928173065185547 = 1.8526501655578613 + 2.0 * 7.037761688232422
Epoch 60, val loss: 1.8577296733856201
Epoch 70, training loss: 15.439576148986816 = 1.8393070697784424 + 2.0 * 6.800134658813477
Epoch 70, val loss: 1.8458219766616821
Epoch 80, training loss: 15.124414443969727 = 1.8260366916656494 + 2.0 * 6.649188995361328
Epoch 80, val loss: 1.8341965675354004
Epoch 90, training loss: 14.81213092803955 = 1.8129910230636597 + 2.0 * 6.499569892883301
Epoch 90, val loss: 1.8232449293136597
Epoch 100, training loss: 14.61156940460205 = 1.8009356260299683 + 2.0 * 6.4053168296813965
Epoch 100, val loss: 1.8130195140838623
Epoch 110, training loss: 14.484442710876465 = 1.7887288331985474 + 2.0 * 6.3478569984436035
Epoch 110, val loss: 1.8026881217956543
Epoch 120, training loss: 14.384798049926758 = 1.7758973836898804 + 2.0 * 6.304450511932373
Epoch 120, val loss: 1.7918310165405273
Epoch 130, training loss: 14.300119400024414 = 1.762411117553711 + 2.0 * 6.268854141235352
Epoch 130, val loss: 1.7807128429412842
Epoch 140, training loss: 14.230111122131348 = 1.748254418373108 + 2.0 * 6.2409281730651855
Epoch 140, val loss: 1.7690913677215576
Epoch 150, training loss: 14.169299125671387 = 1.7330197095870972 + 2.0 * 6.2181396484375
Epoch 150, val loss: 1.756532907485962
Epoch 160, training loss: 14.116539001464844 = 1.7160899639129639 + 2.0 * 6.20022439956665
Epoch 160, val loss: 1.7427728176116943
Epoch 170, training loss: 14.06539249420166 = 1.6971540451049805 + 2.0 * 6.18411922454834
Epoch 170, val loss: 1.7274856567382812
Epoch 180, training loss: 14.019937515258789 = 1.6758112907409668 + 2.0 * 6.17206335067749
Epoch 180, val loss: 1.7103843688964844
Epoch 190, training loss: 13.977846145629883 = 1.6518244743347168 + 2.0 * 6.163010597229004
Epoch 190, val loss: 1.6913262605667114
Epoch 200, training loss: 13.930273056030273 = 1.6251635551452637 + 2.0 * 6.152554512023926
Epoch 200, val loss: 1.6701886653900146
Epoch 210, training loss: 13.883004188537598 = 1.5954058170318604 + 2.0 * 6.143799304962158
Epoch 210, val loss: 1.6467307806015015
Epoch 220, training loss: 13.836057662963867 = 1.562151551246643 + 2.0 * 6.136952877044678
Epoch 220, val loss: 1.6207175254821777
Epoch 230, training loss: 13.793956756591797 = 1.5251281261444092 + 2.0 * 6.134414196014404
Epoch 230, val loss: 1.591969609260559
Epoch 240, training loss: 13.737854957580566 = 1.4849838018417358 + 2.0 * 6.12643575668335
Epoch 240, val loss: 1.5608692169189453
Epoch 250, training loss: 13.685646057128906 = 1.4417824745178223 + 2.0 * 6.121932029724121
Epoch 250, val loss: 1.5275546312332153
Epoch 260, training loss: 13.629424095153809 = 1.3955347537994385 + 2.0 * 6.116944789886475
Epoch 260, val loss: 1.4920408725738525
Epoch 270, training loss: 13.573774337768555 = 1.3464703559875488 + 2.0 * 6.113651752471924
Epoch 270, val loss: 1.4545701742172241
Epoch 280, training loss: 13.516092300415039 = 1.2956160306930542 + 2.0 * 6.110238075256348
Epoch 280, val loss: 1.4159622192382812
Epoch 290, training loss: 13.45781135559082 = 1.243910789489746 + 2.0 * 6.106950283050537
Epoch 290, val loss: 1.377092957496643
Epoch 300, training loss: 13.4014892578125 = 1.191918134689331 + 2.0 * 6.104785442352295
Epoch 300, val loss: 1.338403344154358
Epoch 310, training loss: 13.344009399414062 = 1.14057457447052 + 2.0 * 6.101717472076416
Epoch 310, val loss: 1.3006112575531006
Epoch 320, training loss: 13.287948608398438 = 1.090370535850525 + 2.0 * 6.098789215087891
Epoch 320, val loss: 1.2640104293823242
Epoch 330, training loss: 13.23301887512207 = 1.041741967201233 + 2.0 * 6.095638275146484
Epoch 330, val loss: 1.2290288209915161
Epoch 340, training loss: 13.180517196655273 = 0.9948816299438477 + 2.0 * 6.092817783355713
Epoch 340, val loss: 1.1958767175674438
Epoch 350, training loss: 13.140893936157227 = 0.9500760436058044 + 2.0 * 6.095408916473389
Epoch 350, val loss: 1.1647452116012573
Epoch 360, training loss: 13.082070350646973 = 0.9078710675239563 + 2.0 * 6.087099552154541
Epoch 360, val loss: 1.1356557607650757
Epoch 370, training loss: 13.03693675994873 = 0.8678217530250549 + 2.0 * 6.08455753326416
Epoch 370, val loss: 1.1085608005523682
Epoch 380, training loss: 12.995281219482422 = 0.8297105431556702 + 2.0 * 6.082785129547119
Epoch 380, val loss: 1.0831924676895142
Epoch 390, training loss: 12.965380668640137 = 0.7938077449798584 + 2.0 * 6.08578634262085
Epoch 390, val loss: 1.0596985816955566
Epoch 400, training loss: 12.915912628173828 = 0.7601192593574524 + 2.0 * 6.077896595001221
Epoch 400, val loss: 1.037983775138855
Epoch 410, training loss: 12.878645896911621 = 0.7281786203384399 + 2.0 * 6.075233459472656
Epoch 410, val loss: 1.0178706645965576
Epoch 420, training loss: 12.842655181884766 = 0.6976170539855957 + 2.0 * 6.072518825531006
Epoch 420, val loss: 0.9990676045417786
Epoch 430, training loss: 12.815032958984375 = 0.668267548084259 + 2.0 * 6.07338285446167
Epoch 430, val loss: 0.9813799858093262
Epoch 440, training loss: 12.784975051879883 = 0.6403858065605164 + 2.0 * 6.07229471206665
Epoch 440, val loss: 0.9649892449378967
Epoch 450, training loss: 12.749666213989258 = 0.6135940551757812 + 2.0 * 6.068036079406738
Epoch 450, val loss: 0.9500129818916321
Epoch 460, training loss: 12.718927383422852 = 0.5878096222877502 + 2.0 * 6.065558910369873
Epoch 460, val loss: 0.9360730051994324
Epoch 470, training loss: 12.689291954040527 = 0.5628820657730103 + 2.0 * 6.063204765319824
Epoch 470, val loss: 0.9231888651847839
Epoch 480, training loss: 12.667754173278809 = 0.5387696623802185 + 2.0 * 6.064492225646973
Epoch 480, val loss: 0.9112446904182434
Epoch 490, training loss: 12.633490562438965 = 0.5153884887695312 + 2.0 * 6.059051036834717
Epoch 490, val loss: 0.9002826809883118
Epoch 500, training loss: 12.607871055603027 = 0.4925934374332428 + 2.0 * 6.057638645172119
Epoch 500, val loss: 0.8902667760848999
Epoch 510, training loss: 12.590916633605957 = 0.4703335464000702 + 2.0 * 6.060291767120361
Epoch 510, val loss: 0.8810076117515564
Epoch 520, training loss: 12.557965278625488 = 0.44871988892555237 + 2.0 * 6.054622650146484
Epoch 520, val loss: 0.8725063800811768
Epoch 530, training loss: 12.533768653869629 = 0.4276134669780731 + 2.0 * 6.053077697753906
Epoch 530, val loss: 0.8648867011070251
Epoch 540, training loss: 12.513641357421875 = 0.4069899320602417 + 2.0 * 6.053325653076172
Epoch 540, val loss: 0.8580251336097717
Epoch 550, training loss: 12.492437362670898 = 0.38696378469467163 + 2.0 * 6.052736759185791
Epoch 550, val loss: 0.8516448140144348
Epoch 560, training loss: 12.466033935546875 = 0.36760562658309937 + 2.0 * 6.0492143630981445
Epoch 560, val loss: 0.8461855053901672
Epoch 570, training loss: 12.460498809814453 = 0.3488348126411438 + 2.0 * 6.0558319091796875
Epoch 570, val loss: 0.8413909673690796
Epoch 580, training loss: 12.425210952758789 = 0.3307937681674957 + 2.0 * 6.047208786010742
Epoch 580, val loss: 0.8370978236198425
Epoch 590, training loss: 12.404930114746094 = 0.31352657079696655 + 2.0 * 6.04570198059082
Epoch 590, val loss: 0.8337880373001099
Epoch 600, training loss: 12.384777069091797 = 0.2969341278076172 + 2.0 * 6.04392147064209
Epoch 600, val loss: 0.8311096429824829
Epoch 610, training loss: 12.378141403198242 = 0.281057208776474 + 2.0 * 6.048542022705078
Epoch 610, val loss: 0.8290300369262695
Epoch 620, training loss: 12.35736083984375 = 0.26595786213874817 + 2.0 * 6.045701503753662
Epoch 620, val loss: 0.8275246024131775
Epoch 630, training loss: 12.339250564575195 = 0.25171682238578796 + 2.0 * 6.043766975402832
Epoch 630, val loss: 0.8268435001373291
Epoch 640, training loss: 12.317948341369629 = 0.23821383714675903 + 2.0 * 6.039867401123047
Epoch 640, val loss: 0.8264873027801514
Epoch 650, training loss: 12.30317211151123 = 0.22549842298030853 + 2.0 * 6.03883695602417
Epoch 650, val loss: 0.8268895745277405
Epoch 660, training loss: 12.288662910461426 = 0.21345080435276031 + 2.0 * 6.037606239318848
Epoch 660, val loss: 0.8277895450592041
Epoch 670, training loss: 12.276734352111816 = 0.20206999778747559 + 2.0 * 6.037332057952881
Epoch 670, val loss: 0.8291650414466858
Epoch 680, training loss: 12.276092529296875 = 0.1913774013519287 + 2.0 * 6.042357444763184
Epoch 680, val loss: 0.8310409784317017
Epoch 690, training loss: 12.255895614624023 = 0.1813523918390274 + 2.0 * 6.037271499633789
Epoch 690, val loss: 0.8331089019775391
Epoch 700, training loss: 12.242141723632812 = 0.17199017107486725 + 2.0 * 6.035075664520264
Epoch 700, val loss: 0.8359538316726685
Epoch 710, training loss: 12.231269836425781 = 0.1632004976272583 + 2.0 * 6.034034729003906
Epoch 710, val loss: 0.8391377329826355
Epoch 720, training loss: 12.235366821289062 = 0.15494872629642487 + 2.0 * 6.04020881652832
Epoch 720, val loss: 0.8425002694129944
Epoch 730, training loss: 12.214430809020996 = 0.1472506821155548 + 2.0 * 6.033589839935303
Epoch 730, val loss: 0.8462197780609131
Epoch 740, training loss: 12.20252513885498 = 0.1400143951177597 + 2.0 * 6.03125524520874
Epoch 740, val loss: 0.8503271341323853
Epoch 750, training loss: 12.196367263793945 = 0.13321088254451752 + 2.0 * 6.031578063964844
Epoch 750, val loss: 0.8547523617744446
Epoch 760, training loss: 12.18758487701416 = 0.12681767344474792 + 2.0 * 6.030383586883545
Epoch 760, val loss: 0.8591611385345459
Epoch 770, training loss: 12.190774917602539 = 0.12082895636558533 + 2.0 * 6.03497314453125
Epoch 770, val loss: 0.863903284072876
Epoch 780, training loss: 12.173710823059082 = 0.11525477468967438 + 2.0 * 6.029228210449219
Epoch 780, val loss: 0.8688933253288269
Epoch 790, training loss: 12.165545463562012 = 0.10998114198446274 + 2.0 * 6.027781963348389
Epoch 790, val loss: 0.874081552028656
Epoch 800, training loss: 12.157516479492188 = 0.10501386225223541 + 2.0 * 6.026251316070557
Epoch 800, val loss: 0.8794675469398499
Epoch 810, training loss: 12.157320976257324 = 0.1003267765045166 + 2.0 * 6.028497219085693
Epoch 810, val loss: 0.8849164247512817
Epoch 820, training loss: 12.150589942932129 = 0.09591197967529297 + 2.0 * 6.027338981628418
Epoch 820, val loss: 0.8902763724327087
Epoch 830, training loss: 12.150237083435059 = 0.09177620708942413 + 2.0 * 6.02923059463501
Epoch 830, val loss: 0.8960261344909668
Epoch 840, training loss: 12.136487007141113 = 0.08789423108100891 + 2.0 * 6.024296283721924
Epoch 840, val loss: 0.9019281268119812
Epoch 850, training loss: 12.130352973937988 = 0.08421605825424194 + 2.0 * 6.023068428039551
Epoch 850, val loss: 0.9078753590583801
Epoch 860, training loss: 12.1280517578125 = 0.08073482662439346 + 2.0 * 6.023658275604248
Epoch 860, val loss: 0.913865327835083
Epoch 870, training loss: 12.128128051757812 = 0.07744345813989639 + 2.0 * 6.0253424644470215
Epoch 870, val loss: 0.9198483824729919
Epoch 880, training loss: 12.118617057800293 = 0.07431097328662872 + 2.0 * 6.022152900695801
Epoch 880, val loss: 0.925798773765564
Epoch 890, training loss: 12.116543769836426 = 0.07136939465999603 + 2.0 * 6.022587299346924
Epoch 890, val loss: 0.9320979118347168
Epoch 900, training loss: 12.110300064086914 = 0.06857845932245255 + 2.0 * 6.02086067199707
Epoch 900, val loss: 0.938198447227478
Epoch 910, training loss: 12.104738235473633 = 0.06593421846628189 + 2.0 * 6.019402027130127
Epoch 910, val loss: 0.9444619417190552
Epoch 920, training loss: 12.107973098754883 = 0.06342658400535583 + 2.0 * 6.022273063659668
Epoch 920, val loss: 0.9506954550743103
Epoch 930, training loss: 12.103216171264648 = 0.06104410067200661 + 2.0 * 6.0210862159729
Epoch 930, val loss: 0.9569511413574219
Epoch 940, training loss: 12.095752716064453 = 0.058791715651750565 + 2.0 * 6.01848030090332
Epoch 940, val loss: 0.963129997253418
Epoch 950, training loss: 12.093541145324707 = 0.05664528161287308 + 2.0 * 6.0184478759765625
Epoch 950, val loss: 0.9694783687591553
Epoch 960, training loss: 12.097015380859375 = 0.054608169943094254 + 2.0 * 6.021203517913818
Epoch 960, val loss: 0.9755645990371704
Epoch 970, training loss: 12.093415260314941 = 0.052677031606435776 + 2.0 * 6.020369052886963
Epoch 970, val loss: 0.9817952513694763
Epoch 980, training loss: 12.084566116333008 = 0.05083972215652466 + 2.0 * 6.0168633460998535
Epoch 980, val loss: 0.988070547580719
Epoch 990, training loss: 12.079071998596191 = 0.04909304901957512 + 2.0 * 6.014989376068115
Epoch 990, val loss: 0.9943023920059204
Epoch 1000, training loss: 12.0755615234375 = 0.04741847887635231 + 2.0 * 6.014071464538574
Epoch 1000, val loss: 1.0005264282226562
Epoch 1010, training loss: 12.097030639648438 = 0.0458126924932003 + 2.0 * 6.025609016418457
Epoch 1010, val loss: 1.0065268278121948
Epoch 1020, training loss: 12.078211784362793 = 0.044309910386800766 + 2.0 * 6.016951084136963
Epoch 1020, val loss: 1.012477159500122
Epoch 1030, training loss: 12.069768905639648 = 0.042868319898843765 + 2.0 * 6.0134501457214355
Epoch 1030, val loss: 1.0186573266983032
Epoch 1040, training loss: 12.0654935836792 = 0.04148697853088379 + 2.0 * 6.012003421783447
Epoch 1040, val loss: 1.0247483253479004
Epoch 1050, training loss: 12.063030242919922 = 0.04016536474227905 + 2.0 * 6.011432647705078
Epoch 1050, val loss: 1.0307090282440186
Epoch 1060, training loss: 12.087677955627441 = 0.0389014407992363 + 2.0 * 6.024388313293457
Epoch 1060, val loss: 1.0365703105926514
Epoch 1070, training loss: 12.065085411071777 = 0.037704918533563614 + 2.0 * 6.01369047164917
Epoch 1070, val loss: 1.0420862436294556
Epoch 1080, training loss: 12.061047554016113 = 0.03655689209699631 + 2.0 * 6.012245178222656
Epoch 1080, val loss: 1.0479941368103027
Epoch 1090, training loss: 12.055960655212402 = 0.03546323627233505 + 2.0 * 6.01024866104126
Epoch 1090, val loss: 1.0539125204086304
Epoch 1100, training loss: 12.052470207214355 = 0.034408800303936005 + 2.0 * 6.009030818939209
Epoch 1100, val loss: 1.0596442222595215
Epoch 1110, training loss: 12.05516529083252 = 0.0333956740796566 + 2.0 * 6.010884761810303
Epoch 1110, val loss: 1.0653239488601685
Epoch 1120, training loss: 12.05057144165039 = 0.032423462718725204 + 2.0 * 6.0090742111206055
Epoch 1120, val loss: 1.0707341432571411
Epoch 1130, training loss: 12.049351692199707 = 0.031493861228227615 + 2.0 * 6.0089287757873535
Epoch 1130, val loss: 1.076332688331604
Epoch 1140, training loss: 12.047306060791016 = 0.030602693557739258 + 2.0 * 6.008351802825928
Epoch 1140, val loss: 1.081907033920288
Epoch 1150, training loss: 12.055379867553711 = 0.02974935993552208 + 2.0 * 6.012815475463867
Epoch 1150, val loss: 1.0873684883117676
Epoch 1160, training loss: 12.053915977478027 = 0.028930220752954483 + 2.0 * 6.012492656707764
Epoch 1160, val loss: 1.0925358533859253
Epoch 1170, training loss: 12.041315078735352 = 0.028141623362898827 + 2.0 * 6.00658655166626
Epoch 1170, val loss: 1.097864031791687
Epoch 1180, training loss: 12.040040016174316 = 0.027387386187911034 + 2.0 * 6.006326198577881
Epoch 1180, val loss: 1.103270411491394
Epoch 1190, training loss: 12.037850379943848 = 0.02665787562727928 + 2.0 * 6.005596160888672
Epoch 1190, val loss: 1.1084985733032227
Epoch 1200, training loss: 12.056503295898438 = 0.02595495618879795 + 2.0 * 6.0152740478515625
Epoch 1200, val loss: 1.1135179996490479
Epoch 1210, training loss: 12.037586212158203 = 0.02528764307498932 + 2.0 * 6.0061492919921875
Epoch 1210, val loss: 1.1185693740844727
Epoch 1220, training loss: 12.032546997070312 = 0.02463870868086815 + 2.0 * 6.00395393371582
Epoch 1220, val loss: 1.1237622499465942
Epoch 1230, training loss: 12.040884971618652 = 0.02401529997587204 + 2.0 * 6.008434772491455
Epoch 1230, val loss: 1.1288350820541382
Epoch 1240, training loss: 12.031902313232422 = 0.02341495268046856 + 2.0 * 6.004243850708008
Epoch 1240, val loss: 1.1335179805755615
Epoch 1250, training loss: 12.030304908752441 = 0.022838229313492775 + 2.0 * 6.003733158111572
Epoch 1250, val loss: 1.1385533809661865
Epoch 1260, training loss: 12.028139114379883 = 0.022280428558588028 + 2.0 * 6.002929210662842
Epoch 1260, val loss: 1.1434767246246338
Epoch 1270, training loss: 12.048206329345703 = 0.021747326478362083 + 2.0 * 6.0132293701171875
Epoch 1270, val loss: 1.1483409404754639
Epoch 1280, training loss: 12.033501625061035 = 0.02122199721634388 + 2.0 * 6.006139755249023
Epoch 1280, val loss: 1.1525832414627075
Epoch 1290, training loss: 12.02449893951416 = 0.020727120339870453 + 2.0 * 6.001885890960693
Epoch 1290, val loss: 1.157579779624939
Epoch 1300, training loss: 12.022149085998535 = 0.020243745297193527 + 2.0 * 6.00095272064209
Epoch 1300, val loss: 1.1622978448867798
Epoch 1310, training loss: 12.020940780639648 = 0.019774826243519783 + 2.0 * 6.000583171844482
Epoch 1310, val loss: 1.166904330253601
Epoch 1320, training loss: 12.043489456176758 = 0.01932421140372753 + 2.0 * 6.012082576751709
Epoch 1320, val loss: 1.1715096235275269
Epoch 1330, training loss: 12.027541160583496 = 0.01888377219438553 + 2.0 * 6.004328727722168
Epoch 1330, val loss: 1.1755759716033936
Epoch 1340, training loss: 12.017937660217285 = 0.018466144800186157 + 2.0 * 5.9997358322143555
Epoch 1340, val loss: 1.1802505254745483
Epoch 1350, training loss: 12.018685340881348 = 0.018059566617012024 + 2.0 * 6.000312805175781
Epoch 1350, val loss: 1.1847535371780396
Epoch 1360, training loss: 12.029973030090332 = 0.017667017877101898 + 2.0 * 6.006153106689453
Epoch 1360, val loss: 1.1890290975570679
Epoch 1370, training loss: 12.02037239074707 = 0.017288219183683395 + 2.0 * 6.001542091369629
Epoch 1370, val loss: 1.1932634115219116
Epoch 1380, training loss: 12.017603874206543 = 0.01692088320851326 + 2.0 * 6.000341415405273
Epoch 1380, val loss: 1.1976168155670166
Epoch 1390, training loss: 12.013355255126953 = 0.016563961282372475 + 2.0 * 5.9983954429626465
Epoch 1390, val loss: 1.2018638849258423
Epoch 1400, training loss: 12.016935348510742 = 0.016217656433582306 + 2.0 * 6.000359058380127
Epoch 1400, val loss: 1.2060425281524658
Epoch 1410, training loss: 12.012665748596191 = 0.015883293002843857 + 2.0 * 5.998391151428223
Epoch 1410, val loss: 1.2099922895431519
Epoch 1420, training loss: 12.017836570739746 = 0.015558553859591484 + 2.0 * 6.001139163970947
Epoch 1420, val loss: 1.2140307426452637
Epoch 1430, training loss: 12.009709358215332 = 0.015248861163854599 + 2.0 * 5.997230052947998
Epoch 1430, val loss: 1.2180851697921753
Epoch 1440, training loss: 12.008519172668457 = 0.01494656689465046 + 2.0 * 5.996786117553711
Epoch 1440, val loss: 1.2221916913986206
Epoch 1450, training loss: 12.00772762298584 = 0.01465133298188448 + 2.0 * 5.996538162231445
Epoch 1450, val loss: 1.226162314414978
Epoch 1460, training loss: 12.013755798339844 = 0.014365890994668007 + 2.0 * 5.99969482421875
Epoch 1460, val loss: 1.2300641536712646
Epoch 1470, training loss: 12.008983612060547 = 0.014086265116930008 + 2.0 * 5.997448444366455
Epoch 1470, val loss: 1.2338190078735352
Epoch 1480, training loss: 12.007134437561035 = 0.013816584832966328 + 2.0 * 5.996658802032471
Epoch 1480, val loss: 1.2376465797424316
Epoch 1490, training loss: 12.0042724609375 = 0.01355445384979248 + 2.0 * 5.995358943939209
Epoch 1490, val loss: 1.2414977550506592
Epoch 1500, training loss: 12.003271102905273 = 0.01330066379159689 + 2.0 * 5.994985103607178
Epoch 1500, val loss: 1.245309829711914
Epoch 1510, training loss: 12.011378288269043 = 0.013054613023996353 + 2.0 * 5.999161720275879
Epoch 1510, val loss: 1.2489925622940063
Epoch 1520, training loss: 12.008522033691406 = 0.01281232014298439 + 2.0 * 5.997854709625244
Epoch 1520, val loss: 1.2525312900543213
Epoch 1530, training loss: 12.004837989807129 = 0.01258185226470232 + 2.0 * 5.996128082275391
Epoch 1530, val loss: 1.2561811208724976
Epoch 1540, training loss: 12.006378173828125 = 0.012355455197393894 + 2.0 * 5.997011184692383
Epoch 1540, val loss: 1.259858250617981
Epoch 1550, training loss: 12.00451374053955 = 0.012136608362197876 + 2.0 * 5.996188640594482
Epoch 1550, val loss: 1.2633659839630127
Epoch 1560, training loss: 11.998818397521973 = 0.011922039091587067 + 2.0 * 5.993448257446289
Epoch 1560, val loss: 1.2668664455413818
Epoch 1570, training loss: 11.996976852416992 = 0.01171416137367487 + 2.0 * 5.992631435394287
Epoch 1570, val loss: 1.2704999446868896
Epoch 1580, training loss: 11.99860954284668 = 0.011510414071381092 + 2.0 * 5.993549346923828
Epoch 1580, val loss: 1.2739626169204712
Epoch 1590, training loss: 12.007445335388184 = 0.011311088688671589 + 2.0 * 5.9980669021606445
Epoch 1590, val loss: 1.2771697044372559
Epoch 1600, training loss: 12.003429412841797 = 0.011121316812932491 + 2.0 * 5.996153831481934
Epoch 1600, val loss: 1.2804170846939087
Epoch 1610, training loss: 11.995203971862793 = 0.010936019010841846 + 2.0 * 5.992134094238281
Epoch 1610, val loss: 1.283830165863037
Epoch 1620, training loss: 11.994246482849121 = 0.010755951516330242 + 2.0 * 5.991745471954346
Epoch 1620, val loss: 1.2873677015304565
Epoch 1630, training loss: 11.992477416992188 = 0.010578312911093235 + 2.0 * 5.990949630737305
Epoch 1630, val loss: 1.2906856536865234
Epoch 1640, training loss: 11.999131202697754 = 0.010404238477349281 + 2.0 * 5.994363307952881
Epoch 1640, val loss: 1.2938731908798218
Epoch 1650, training loss: 11.99252986907959 = 0.01023712009191513 + 2.0 * 5.991146564483643
Epoch 1650, val loss: 1.2967915534973145
Epoch 1660, training loss: 11.993121147155762 = 0.010073120705783367 + 2.0 * 5.9915242195129395
Epoch 1660, val loss: 1.3000128269195557
Epoch 1670, training loss: 11.990971565246582 = 0.009914901107549667 + 2.0 * 5.990528106689453
Epoch 1670, val loss: 1.3034030199050903
Epoch 1680, training loss: 11.991670608520508 = 0.009758231230080128 + 2.0 * 5.9909563064575195
Epoch 1680, val loss: 1.3064841032028198
Epoch 1690, training loss: 12.00204849243164 = 0.009607261046767235 + 2.0 * 5.996220588684082
Epoch 1690, val loss: 1.3093534708023071
Epoch 1700, training loss: 11.991522789001465 = 0.009460031054913998 + 2.0 * 5.991031169891357
Epoch 1700, val loss: 1.3122360706329346
Epoch 1710, training loss: 11.99007511138916 = 0.009316535666584969 + 2.0 * 5.990379333496094
Epoch 1710, val loss: 1.3154505491256714
Epoch 1720, training loss: 11.988706588745117 = 0.009176149033010006 + 2.0 * 5.989765167236328
Epoch 1720, val loss: 1.3185064792633057
Epoch 1730, training loss: 11.996575355529785 = 0.009038559161126614 + 2.0 * 5.993768215179443
Epoch 1730, val loss: 1.3213651180267334
Epoch 1740, training loss: 11.98755931854248 = 0.008904201909899712 + 2.0 * 5.989327430725098
Epoch 1740, val loss: 1.3241995573043823
Epoch 1750, training loss: 11.98484992980957 = 0.00877286959439516 + 2.0 * 5.988038539886475
Epoch 1750, val loss: 1.3272022008895874
Epoch 1760, training loss: 11.985114097595215 = 0.008644201792776585 + 2.0 * 5.988234996795654
Epoch 1760, val loss: 1.3301544189453125
Epoch 1770, training loss: 12.006717681884766 = 0.008516458794474602 + 2.0 * 5.999100685119629
Epoch 1770, val loss: 1.3327727317810059
Epoch 1780, training loss: 12.000669479370117 = 0.008399157784879208 + 2.0 * 5.996135234832764
Epoch 1780, val loss: 1.3354307413101196
Epoch 1790, training loss: 11.983564376831055 = 0.008279362693428993 + 2.0 * 5.987642288208008
Epoch 1790, val loss: 1.338291049003601
Epoch 1800, training loss: 11.983844757080078 = 0.008163867518305779 + 2.0 * 5.98784065246582
Epoch 1800, val loss: 1.341233730316162
Epoch 1810, training loss: 11.983597755432129 = 0.00805080495774746 + 2.0 * 5.987773418426514
Epoch 1810, val loss: 1.3440476655960083
Epoch 1820, training loss: 11.99636459350586 = 0.007939761504530907 + 2.0 * 5.994212627410889
Epoch 1820, val loss: 1.3466476202011108
Epoch 1830, training loss: 11.984776496887207 = 0.007829425856471062 + 2.0 * 5.988473415374756
Epoch 1830, val loss: 1.3492851257324219
Epoch 1840, training loss: 11.981221199035645 = 0.007722922600805759 + 2.0 * 5.986749172210693
Epoch 1840, val loss: 1.3520437479019165
Epoch 1850, training loss: 11.979646682739258 = 0.007618109229952097 + 2.0 * 5.986014366149902
Epoch 1850, val loss: 1.3547546863555908
Epoch 1860, training loss: 11.982608795166016 = 0.0075150830671191216 + 2.0 * 5.987546920776367
Epoch 1860, val loss: 1.3573834896087646
Epoch 1870, training loss: 11.989805221557617 = 0.007414416875690222 + 2.0 * 5.991195201873779
Epoch 1870, val loss: 1.3596715927124023
Epoch 1880, training loss: 11.986089706420898 = 0.0073164040222764015 + 2.0 * 5.989386558532715
Epoch 1880, val loss: 1.3620598316192627
Epoch 1890, training loss: 11.981210708618164 = 0.007221955806016922 + 2.0 * 5.98699426651001
Epoch 1890, val loss: 1.3648028373718262
Epoch 1900, training loss: 11.97838306427002 = 0.007129477337002754 + 2.0 * 5.985626697540283
Epoch 1900, val loss: 1.3674975633621216
Epoch 1910, training loss: 11.976794242858887 = 0.007037258241325617 + 2.0 * 5.9848785400390625
Epoch 1910, val loss: 1.3700121641159058
Epoch 1920, training loss: 11.97802734375 = 0.006946096662431955 + 2.0 * 5.985540390014648
Epoch 1920, val loss: 1.372488260269165
Epoch 1930, training loss: 11.988936424255371 = 0.006856972351670265 + 2.0 * 5.991039752960205
Epoch 1930, val loss: 1.3745090961456299
Epoch 1940, training loss: 11.977395057678223 = 0.00677188765257597 + 2.0 * 5.985311508178711
Epoch 1940, val loss: 1.3769242763519287
Epoch 1950, training loss: 11.97743034362793 = 0.006688761059194803 + 2.0 * 5.985370635986328
Epoch 1950, val loss: 1.3795721530914307
Epoch 1960, training loss: 11.974944114685059 = 0.006605887319892645 + 2.0 * 5.984169006347656
Epoch 1960, val loss: 1.3820149898529053
Epoch 1970, training loss: 11.99876880645752 = 0.0065238154493272305 + 2.0 * 5.996122360229492
Epoch 1970, val loss: 1.3841618299484253
Epoch 1980, training loss: 11.982223510742188 = 0.006446558516472578 + 2.0 * 5.987888336181641
Epoch 1980, val loss: 1.386311650276184
Epoch 1990, training loss: 11.976923942565918 = 0.006367904134094715 + 2.0 * 5.985278129577637
Epoch 1990, val loss: 1.3887063264846802
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8111
Overall ASR: 0.6015
Flip ASR: 0.5244/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.694196701049805 = 1.9464200735092163 + 2.0 * 8.37388801574707
Epoch 0, val loss: 1.9482083320617676
Epoch 10, training loss: 18.683242797851562 = 1.9364166259765625 + 2.0 * 8.3734130859375
Epoch 10, val loss: 1.9374465942382812
Epoch 20, training loss: 18.664350509643555 = 1.9247344732284546 + 2.0 * 8.369808197021484
Epoch 20, val loss: 1.9244816303253174
Epoch 30, training loss: 18.599308013916016 = 1.9098848104476929 + 2.0 * 8.344711303710938
Epoch 30, val loss: 1.907606840133667
Epoch 40, training loss: 18.284339904785156 = 1.892308235168457 + 2.0 * 8.196015357971191
Epoch 40, val loss: 1.8878135681152344
Epoch 50, training loss: 16.79345703125 = 1.8749617338180542 + 2.0 * 7.459248065948486
Epoch 50, val loss: 1.868248701095581
Epoch 60, training loss: 15.942071914672852 = 1.8592840433120728 + 2.0 * 7.041393756866455
Epoch 60, val loss: 1.8518766164779663
Epoch 70, training loss: 15.46995735168457 = 1.843763828277588 + 2.0 * 6.813096523284912
Epoch 70, val loss: 1.8356486558914185
Epoch 80, training loss: 15.19650650024414 = 1.8283909559249878 + 2.0 * 6.684057712554932
Epoch 80, val loss: 1.8198895454406738
Epoch 90, training loss: 14.977673530578613 = 1.814662218093872 + 2.0 * 6.58150577545166
Epoch 90, val loss: 1.8060359954833984
Epoch 100, training loss: 14.800359725952148 = 1.8013883829116821 + 2.0 * 6.499485492706299
Epoch 100, val loss: 1.7925430536270142
Epoch 110, training loss: 14.63343620300293 = 1.7884033918380737 + 2.0 * 6.422516345977783
Epoch 110, val loss: 1.7793296575546265
Epoch 120, training loss: 14.52004623413086 = 1.7754548788070679 + 2.0 * 6.37229585647583
Epoch 120, val loss: 1.7661739587783813
Epoch 130, training loss: 14.420823097229004 = 1.7616044282913208 + 2.0 * 6.329609394073486
Epoch 130, val loss: 1.7525309324264526
Epoch 140, training loss: 14.337181091308594 = 1.7471932172775269 + 2.0 * 6.294993877410889
Epoch 140, val loss: 1.7387267351150513
Epoch 150, training loss: 14.265982627868652 = 1.7321317195892334 + 2.0 * 6.26692533493042
Epoch 150, val loss: 1.724600076675415
Epoch 160, training loss: 14.199432373046875 = 1.7161144018173218 + 2.0 * 6.241659164428711
Epoch 160, val loss: 1.710042119026184
Epoch 170, training loss: 14.139938354492188 = 1.6984596252441406 + 2.0 * 6.220739364624023
Epoch 170, val loss: 1.6946338415145874
Epoch 180, training loss: 14.08607292175293 = 1.678364634513855 + 2.0 * 6.203854084014893
Epoch 180, val loss: 1.6775009632110596
Epoch 190, training loss: 14.034311294555664 = 1.6552324295043945 + 2.0 * 6.189539432525635
Epoch 190, val loss: 1.6583526134490967
Epoch 200, training loss: 13.984933853149414 = 1.6289465427398682 + 2.0 * 6.1779937744140625
Epoch 200, val loss: 1.6368366479873657
Epoch 210, training loss: 13.93528938293457 = 1.5988510847091675 + 2.0 * 6.168219089508057
Epoch 210, val loss: 1.6124392747879028
Epoch 220, training loss: 13.883161544799805 = 1.564277172088623 + 2.0 * 6.15944242477417
Epoch 220, val loss: 1.5846245288848877
Epoch 230, training loss: 13.830703735351562 = 1.5248289108276367 + 2.0 * 6.152937412261963
Epoch 230, val loss: 1.5531189441680908
Epoch 240, training loss: 13.77376937866211 = 1.4810258150100708 + 2.0 * 6.146371841430664
Epoch 240, val loss: 1.5181005001068115
Epoch 250, training loss: 13.710586547851562 = 1.4332916736602783 + 2.0 * 6.138647556304932
Epoch 250, val loss: 1.4803667068481445
Epoch 260, training loss: 13.648467063903809 = 1.3819122314453125 + 2.0 * 6.133277416229248
Epoch 260, val loss: 1.4399923086166382
Epoch 270, training loss: 13.584030151367188 = 1.3280603885650635 + 2.0 * 6.127985000610352
Epoch 270, val loss: 1.3981292247772217
Epoch 280, training loss: 13.51972770690918 = 1.2735004425048828 + 2.0 * 6.123113632202148
Epoch 280, val loss: 1.3560148477554321
Epoch 290, training loss: 13.456110000610352 = 1.218972086906433 + 2.0 * 6.1185688972473145
Epoch 290, val loss: 1.3143775463104248
Epoch 300, training loss: 13.39457893371582 = 1.1654582023620605 + 2.0 * 6.114560127258301
Epoch 300, val loss: 1.2737685441970825
Epoch 310, training loss: 13.345062255859375 = 1.1141605377197266 + 2.0 * 6.115450859069824
Epoch 310, val loss: 1.2353469133377075
Epoch 320, training loss: 13.280705451965332 = 1.0661941766738892 + 2.0 * 6.107255458831787
Epoch 320, val loss: 1.1995162963867188
Epoch 330, training loss: 13.2286376953125 = 1.0209583044052124 + 2.0 * 6.103839874267578
Epoch 330, val loss: 1.1659736633300781
Epoch 340, training loss: 13.17951488494873 = 0.9776431322097778 + 2.0 * 6.100935935974121
Epoch 340, val loss: 1.1340821981430054
Epoch 350, training loss: 13.141002655029297 = 0.937474250793457 + 2.0 * 6.10176420211792
Epoch 350, val loss: 1.1043702363967896
Epoch 360, training loss: 13.087553024291992 = 0.8995325565338135 + 2.0 * 6.094010353088379
Epoch 360, val loss: 1.0766435861587524
Epoch 370, training loss: 13.044598579406738 = 0.8634456396102905 + 2.0 * 6.090576648712158
Epoch 370, val loss: 1.0502698421478271
Epoch 380, training loss: 13.003615379333496 = 0.8290059566497803 + 2.0 * 6.087304592132568
Epoch 380, val loss: 1.025223731994629
Epoch 390, training loss: 12.974105834960938 = 0.7961562871932983 + 2.0 * 6.088974952697754
Epoch 390, val loss: 1.0015950202941895
Epoch 400, training loss: 12.932865142822266 = 0.7656087875366211 + 2.0 * 6.083628177642822
Epoch 400, val loss: 0.9797800779342651
Epoch 410, training loss: 12.897940635681152 = 0.7371370196342468 + 2.0 * 6.08040189743042
Epoch 410, val loss: 0.9598818421363831
Epoch 420, training loss: 12.867668151855469 = 0.7104071974754333 + 2.0 * 6.078630447387695
Epoch 420, val loss: 0.94159996509552
Epoch 430, training loss: 12.839761734008789 = 0.6853063106536865 + 2.0 * 6.077227592468262
Epoch 430, val loss: 0.9251521825790405
Epoch 440, training loss: 12.806666374206543 = 0.6619094610214233 + 2.0 * 6.072378635406494
Epoch 440, val loss: 0.9103714823722839
Epoch 450, training loss: 12.781401634216309 = 0.6396628618240356 + 2.0 * 6.070869445800781
Epoch 450, val loss: 0.89698326587677
Epoch 460, training loss: 12.7548189163208 = 0.6182745099067688 + 2.0 * 6.068272113800049
Epoch 460, val loss: 0.8847661018371582
Epoch 470, training loss: 12.756568908691406 = 0.5975774526596069 + 2.0 * 6.079495906829834
Epoch 470, val loss: 0.873589277267456
Epoch 480, training loss: 12.711318969726562 = 0.5779532790184021 + 2.0 * 6.066682815551758
Epoch 480, val loss: 0.8633416891098022
Epoch 490, training loss: 12.687248229980469 = 0.5587604641914368 + 2.0 * 6.064243793487549
Epoch 490, val loss: 0.8540078997612
Epoch 500, training loss: 12.662993431091309 = 0.5398169159889221 + 2.0 * 6.061588287353516
Epoch 500, val loss: 0.8452181220054626
Epoch 510, training loss: 12.640558242797852 = 0.5210152268409729 + 2.0 * 6.059771537780762
Epoch 510, val loss: 0.8368492722511292
Epoch 520, training loss: 12.619272232055664 = 0.5023074150085449 + 2.0 * 6.0584821701049805
Epoch 520, val loss: 0.8288878798484802
Epoch 530, training loss: 12.600424766540527 = 0.4837695062160492 + 2.0 * 6.058327674865723
Epoch 530, val loss: 0.8212616443634033
Epoch 540, training loss: 12.580955505371094 = 0.465584933757782 + 2.0 * 6.057685375213623
Epoch 540, val loss: 0.8141996264457703
Epoch 550, training loss: 12.557560920715332 = 0.4476028382778168 + 2.0 * 6.054978847503662
Epoch 550, val loss: 0.8076046705245972
Epoch 560, training loss: 12.536421775817871 = 0.42977631092071533 + 2.0 * 6.053322792053223
Epoch 560, val loss: 0.8014464378356934
Epoch 570, training loss: 12.517467498779297 = 0.4122832715511322 + 2.0 * 6.0525922775268555
Epoch 570, val loss: 0.795647919178009
Epoch 580, training loss: 12.500194549560547 = 0.3953183591365814 + 2.0 * 6.052438259124756
Epoch 580, val loss: 0.7903808951377869
Epoch 590, training loss: 12.4801664352417 = 0.3786941170692444 + 2.0 * 6.050735950469971
Epoch 590, val loss: 0.7856031060218811
Epoch 600, training loss: 12.459512710571289 = 0.3623509407043457 + 2.0 * 6.048581123352051
Epoch 600, val loss: 0.7812581062316895
Epoch 610, training loss: 12.440595626831055 = 0.34632909297943115 + 2.0 * 6.047133445739746
Epoch 610, val loss: 0.7773211002349854
Epoch 620, training loss: 12.423219680786133 = 0.33063966035842896 + 2.0 * 6.046289920806885
Epoch 620, val loss: 0.7738012075424194
Epoch 630, training loss: 12.406721115112305 = 0.31539011001586914 + 2.0 * 6.045665264129639
Epoch 630, val loss: 0.7707098722457886
Epoch 640, training loss: 12.38992691040039 = 0.3007144331932068 + 2.0 * 6.0446062088012695
Epoch 640, val loss: 0.7680349349975586
Epoch 650, training loss: 12.37381649017334 = 0.28652164340019226 + 2.0 * 6.043647289276123
Epoch 650, val loss: 0.7657518982887268
Epoch 660, training loss: 12.357233047485352 = 0.27279794216156006 + 2.0 * 6.04221773147583
Epoch 660, val loss: 0.7638401985168457
Epoch 670, training loss: 12.361282348632812 = 0.2596021592617035 + 2.0 * 6.050839900970459
Epoch 670, val loss: 0.7623046040534973
Epoch 680, training loss: 12.330785751342773 = 0.2469465434551239 + 2.0 * 6.041919708251953
Epoch 680, val loss: 0.7610903978347778
Epoch 690, training loss: 12.316132545471191 = 0.23495158553123474 + 2.0 * 6.040590286254883
Epoch 690, val loss: 0.7603277564048767
Epoch 700, training loss: 12.310381889343262 = 0.2234797179698944 + 2.0 * 6.043451309204102
Epoch 700, val loss: 0.7599567770957947
Epoch 710, training loss: 12.29065990447998 = 0.2126539945602417 + 2.0 * 6.039002895355225
Epoch 710, val loss: 0.7599626779556274
Epoch 720, training loss: 12.276496887207031 = 0.2023380845785141 + 2.0 * 6.037079334259033
Epoch 720, val loss: 0.7603209614753723
Epoch 730, training loss: 12.26602554321289 = 0.19255498051643372 + 2.0 * 6.0367350578308105
Epoch 730, val loss: 0.7610832452774048
Epoch 740, training loss: 12.259068489074707 = 0.18329952657222748 + 2.0 * 6.037884712219238
Epoch 740, val loss: 0.7621611952781677
Epoch 750, training loss: 12.252318382263184 = 0.17461010813713074 + 2.0 * 6.038854122161865
Epoch 750, val loss: 0.7635573148727417
Epoch 760, training loss: 12.236099243164062 = 0.16641242802143097 + 2.0 * 6.034843444824219
Epoch 760, val loss: 0.7652427554130554
Epoch 770, training loss: 12.222280502319336 = 0.15872521698474884 + 2.0 * 6.031777858734131
Epoch 770, val loss: 0.7673236131668091
Epoch 780, training loss: 12.215339660644531 = 0.1514689028263092 + 2.0 * 6.031935214996338
Epoch 780, val loss: 0.7696985602378845
Epoch 790, training loss: 12.21780014038086 = 0.14465443789958954 + 2.0 * 6.0365729331970215
Epoch 790, val loss: 0.7723230719566345
Epoch 800, training loss: 12.20341968536377 = 0.13823182880878448 + 2.0 * 6.032593727111816
Epoch 800, val loss: 0.7751458883285522
Epoch 810, training loss: 12.192920684814453 = 0.13225308060646057 + 2.0 * 6.030333995819092
Epoch 810, val loss: 0.7783358693122864
Epoch 820, training loss: 12.182793617248535 = 0.12660112977027893 + 2.0 * 6.0280961990356445
Epoch 820, val loss: 0.7816495895385742
Epoch 830, training loss: 12.186750411987305 = 0.12128544598817825 + 2.0 * 6.0327324867248535
Epoch 830, val loss: 0.7851687073707581
Epoch 840, training loss: 12.178624153137207 = 0.11633673310279846 + 2.0 * 6.031143665313721
Epoch 840, val loss: 0.7887423634529114
Epoch 850, training loss: 12.163036346435547 = 0.11168216168880463 + 2.0 * 6.02567720413208
Epoch 850, val loss: 0.7924914956092834
Epoch 860, training loss: 12.158431053161621 = 0.10732702910900116 + 2.0 * 6.025551795959473
Epoch 860, val loss: 0.7964881062507629
Epoch 870, training loss: 12.156168937683105 = 0.10323109477758408 + 2.0 * 6.026468753814697
Epoch 870, val loss: 0.8005680441856384
Epoch 880, training loss: 12.150938034057617 = 0.09939207136631012 + 2.0 * 6.025773048400879
Epoch 880, val loss: 0.8047221302986145
Epoch 890, training loss: 12.152914047241211 = 0.09578710794448853 + 2.0 * 6.028563499450684
Epoch 890, val loss: 0.8089554905891418
Epoch 900, training loss: 12.146957397460938 = 0.09241413325071335 + 2.0 * 6.027271747589111
Epoch 900, val loss: 0.8132021427154541
Epoch 910, training loss: 12.132397651672363 = 0.0892215147614479 + 2.0 * 6.02158784866333
Epoch 910, val loss: 0.817554771900177
Epoch 920, training loss: 12.12876033782959 = 0.0862060934305191 + 2.0 * 6.021276950836182
Epoch 920, val loss: 0.8220440149307251
Epoch 930, training loss: 12.127029418945312 = 0.08335783332586288 + 2.0 * 6.021835803985596
Epoch 930, val loss: 0.8265896439552307
Epoch 940, training loss: 12.12182331085205 = 0.08067531138658524 + 2.0 * 6.02057409286499
Epoch 940, val loss: 0.831219494342804
Epoch 950, training loss: 12.12120532989502 = 0.07813911139965057 + 2.0 * 6.021533012390137
Epoch 950, val loss: 0.835932731628418
Epoch 960, training loss: 12.113635063171387 = 0.0757400393486023 + 2.0 * 6.018947601318359
Epoch 960, val loss: 0.8407216668128967
Epoch 970, training loss: 12.109076499938965 = 0.07345912605524063 + 2.0 * 6.01780891418457
Epoch 970, val loss: 0.8455460667610168
Epoch 980, training loss: 12.11108112335205 = 0.0713052973151207 + 2.0 * 6.019887924194336
Epoch 980, val loss: 0.8503580689430237
Epoch 990, training loss: 12.105216026306152 = 0.0692543089389801 + 2.0 * 6.017981052398682
Epoch 990, val loss: 0.8551573753356934
Epoch 1000, training loss: 12.104781150817871 = 0.0673036128282547 + 2.0 * 6.018738746643066
Epoch 1000, val loss: 0.8600355982780457
Epoch 1010, training loss: 12.095329284667969 = 0.06544141471385956 + 2.0 * 6.014944076538086
Epoch 1010, val loss: 0.8648970723152161
Epoch 1020, training loss: 12.094291687011719 = 0.06366017460823059 + 2.0 * 6.015315532684326
Epoch 1020, val loss: 0.8698473572731018
Epoch 1030, training loss: 12.102313041687012 = 0.061952169984579086 + 2.0 * 6.0201802253723145
Epoch 1030, val loss: 0.8747047185897827
Epoch 1040, training loss: 12.095982551574707 = 0.06033266335725784 + 2.0 * 6.017825126647949
Epoch 1040, val loss: 0.8796504735946655
Epoch 1050, training loss: 12.089215278625488 = 0.0587606318295002 + 2.0 * 6.015227317810059
Epoch 1050, val loss: 0.8844548463821411
Epoch 1060, training loss: 12.084609031677246 = 0.05726199969649315 + 2.0 * 6.013673305511475
Epoch 1060, val loss: 0.889409601688385
Epoch 1070, training loss: 12.080084800720215 = 0.055804263800382614 + 2.0 * 6.012140274047852
Epoch 1070, val loss: 0.8942727446556091
Epoch 1080, training loss: 12.087516784667969 = 0.0543939471244812 + 2.0 * 6.016561508178711
Epoch 1080, val loss: 0.8990748524665833
Epoch 1090, training loss: 12.080269813537598 = 0.05302834138274193 + 2.0 * 6.013620853424072
Epoch 1090, val loss: 0.9036677479743958
Epoch 1100, training loss: 12.076423645019531 = 0.05170463025569916 + 2.0 * 6.012359619140625
Epoch 1100, val loss: 0.9084373116493225
Epoch 1110, training loss: 12.074350357055664 = 0.05042796954512596 + 2.0 * 6.011960983276367
Epoch 1110, val loss: 0.9131054878234863
Epoch 1120, training loss: 12.067716598510742 = 0.04919051751494408 + 2.0 * 6.009263038635254
Epoch 1120, val loss: 0.917780876159668
Epoch 1130, training loss: 12.072432518005371 = 0.04798639565706253 + 2.0 * 6.012223243713379
Epoch 1130, val loss: 0.9224739670753479
Epoch 1140, training loss: 12.067540168762207 = 0.046817194670438766 + 2.0 * 6.010361671447754
Epoch 1140, val loss: 0.9270238280296326
Epoch 1150, training loss: 12.063419342041016 = 0.04568971320986748 + 2.0 * 6.008864879608154
Epoch 1150, val loss: 0.9316372871398926
Epoch 1160, training loss: 12.065215110778809 = 0.04459809884428978 + 2.0 * 6.010308742523193
Epoch 1160, val loss: 0.9362385869026184
Epoch 1170, training loss: 12.062727928161621 = 0.043532684445381165 + 2.0 * 6.0095977783203125
Epoch 1170, val loss: 0.9407402873039246
Epoch 1180, training loss: 12.057550430297852 = 0.042495451867580414 + 2.0 * 6.0075273513793945
Epoch 1180, val loss: 0.9451982975006104
Epoch 1190, training loss: 12.05663013458252 = 0.04148499295115471 + 2.0 * 6.007572650909424
Epoch 1190, val loss: 0.949722945690155
Epoch 1200, training loss: 12.059179306030273 = 0.04049689695239067 + 2.0 * 6.009341239929199
Epoch 1200, val loss: 0.9540821313858032
Epoch 1210, training loss: 12.052310943603516 = 0.039539121091365814 + 2.0 * 6.006385803222656
Epoch 1210, val loss: 0.9585495591163635
Epoch 1220, training loss: 12.049596786499023 = 0.03859742358326912 + 2.0 * 6.005499839782715
Epoch 1220, val loss: 0.9629436135292053
Epoch 1230, training loss: 12.054991722106934 = 0.037689462304115295 + 2.0 * 6.008651256561279
Epoch 1230, val loss: 0.9673058390617371
Epoch 1240, training loss: 12.055103302001953 = 0.03680010139942169 + 2.0 * 6.009151458740234
Epoch 1240, val loss: 0.9715848565101624
Epoch 1250, training loss: 12.0499267578125 = 0.03593737632036209 + 2.0 * 6.006994724273682
Epoch 1250, val loss: 0.9759708046913147
Epoch 1260, training loss: 12.04348373413086 = 0.03508886694908142 + 2.0 * 6.004197597503662
Epoch 1260, val loss: 0.9802209734916687
Epoch 1270, training loss: 12.040607452392578 = 0.0342530757188797 + 2.0 * 6.003177165985107
Epoch 1270, val loss: 0.9845564961433411
Epoch 1280, training loss: 12.04004955291748 = 0.033424753695726395 + 2.0 * 6.003312587738037
Epoch 1280, val loss: 0.9888291954994202
Epoch 1290, training loss: 12.053646087646484 = 0.03260580450296402 + 2.0 * 6.010519981384277
Epoch 1290, val loss: 0.9930306673049927
Epoch 1300, training loss: 12.040339469909668 = 0.0317879244685173 + 2.0 * 6.004275798797607
Epoch 1300, val loss: 0.9969775080680847
Epoch 1310, training loss: 12.03671932220459 = 0.03098849020898342 + 2.0 * 6.002865314483643
Epoch 1310, val loss: 1.001198410987854
Epoch 1320, training loss: 12.038707733154297 = 0.030194438993930817 + 2.0 * 6.004256725311279
Epoch 1320, val loss: 1.0053613185882568
Epoch 1330, training loss: 12.032913208007812 = 0.029413331300020218 + 2.0 * 6.0017499923706055
Epoch 1330, val loss: 1.0093990564346313
Epoch 1340, training loss: 12.03052806854248 = 0.02863389067351818 + 2.0 * 6.000946998596191
Epoch 1340, val loss: 1.0135784149169922
Epoch 1350, training loss: 12.029019355773926 = 0.02784755825996399 + 2.0 * 6.000586032867432
Epoch 1350, val loss: 1.0177079439163208
Epoch 1360, training loss: 12.039152145385742 = 0.027069712057709694 + 2.0 * 6.006041049957275
Epoch 1360, val loss: 1.0217374563217163
Epoch 1370, training loss: 12.032489776611328 = 0.026298319920897484 + 2.0 * 6.003095626831055
Epoch 1370, val loss: 1.0257277488708496
Epoch 1380, training loss: 12.029621124267578 = 0.025536704808473587 + 2.0 * 6.002042293548584
Epoch 1380, val loss: 1.0297722816467285
Epoch 1390, training loss: 12.025308609008789 = 0.024784410372376442 + 2.0 * 6.000262260437012
Epoch 1390, val loss: 1.0337680578231812
Epoch 1400, training loss: 12.022462844848633 = 0.024027349427342415 + 2.0 * 5.999217510223389
Epoch 1400, val loss: 1.0377840995788574
Epoch 1410, training loss: 12.04271411895752 = 0.02326863445341587 + 2.0 * 6.009722709655762
Epoch 1410, val loss: 1.0416748523712158
Epoch 1420, training loss: 12.023200035095215 = 0.022520994767546654 + 2.0 * 6.000339508056641
Epoch 1420, val loss: 1.045623779296875
Epoch 1430, training loss: 12.017396926879883 = 0.02177797071635723 + 2.0 * 5.997809410095215
Epoch 1430, val loss: 1.0496573448181152
Epoch 1440, training loss: 12.016613960266113 = 0.021056504920125008 + 2.0 * 5.99777889251709
Epoch 1440, val loss: 1.0536997318267822
Epoch 1450, training loss: 12.014487266540527 = 0.02035689353942871 + 2.0 * 5.99706506729126
Epoch 1450, val loss: 1.0577759742736816
Epoch 1460, training loss: 12.013358116149902 = 0.019674288108944893 + 2.0 * 5.996841907501221
Epoch 1460, val loss: 1.061814308166504
Epoch 1470, training loss: 12.032092094421387 = 0.019014010205864906 + 2.0 * 6.0065388679504395
Epoch 1470, val loss: 1.0657521486282349
Epoch 1480, training loss: 12.02114200592041 = 0.018368054181337357 + 2.0 * 6.001387119293213
Epoch 1480, val loss: 1.0695306062698364
Epoch 1490, training loss: 12.017521858215332 = 0.01774347387254238 + 2.0 * 5.999889373779297
Epoch 1490, val loss: 1.0734097957611084
Epoch 1500, training loss: 12.009540557861328 = 0.017145907506346703 + 2.0 * 5.99619722366333
Epoch 1500, val loss: 1.0773508548736572
Epoch 1510, training loss: 12.008981704711914 = 0.01657361350953579 + 2.0 * 5.996203899383545
Epoch 1510, val loss: 1.0813063383102417
Epoch 1520, training loss: 12.009186744689941 = 0.016042400151491165 + 2.0 * 5.996572017669678
Epoch 1520, val loss: 1.0852848291397095
Epoch 1530, training loss: 12.016517639160156 = 0.015547293238341808 + 2.0 * 6.000484943389893
Epoch 1530, val loss: 1.0891066789627075
Epoch 1540, training loss: 12.008810043334961 = 0.015083510428667068 + 2.0 * 5.99686336517334
Epoch 1540, val loss: 1.0928667783737183
Epoch 1550, training loss: 12.007617950439453 = 0.014654926024377346 + 2.0 * 5.996481418609619
Epoch 1550, val loss: 1.096808910369873
Epoch 1560, training loss: 12.009323120117188 = 0.014253759756684303 + 2.0 * 5.99753475189209
Epoch 1560, val loss: 1.100480556488037
Epoch 1570, training loss: 12.005477905273438 = 0.01388366799801588 + 2.0 * 5.995797157287598
Epoch 1570, val loss: 1.1041545867919922
Epoch 1580, training loss: 12.012808799743652 = 0.013533275574445724 + 2.0 * 5.999637603759766
Epoch 1580, val loss: 1.1077349185943604
Epoch 1590, training loss: 12.00347900390625 = 0.013200720772147179 + 2.0 * 5.995139122009277
Epoch 1590, val loss: 1.1111465692520142
Epoch 1600, training loss: 11.999635696411133 = 0.012891069985926151 + 2.0 * 5.993372440338135
Epoch 1600, val loss: 1.1147584915161133
Epoch 1610, training loss: 11.998726844787598 = 0.012594313360750675 + 2.0 * 5.993066310882568
Epoch 1610, val loss: 1.1183054447174072
Epoch 1620, training loss: 12.008200645446777 = 0.012309548445045948 + 2.0 * 5.997945785522461
Epoch 1620, val loss: 1.1217471361160278
Epoch 1630, training loss: 11.998701095581055 = 0.01203711237758398 + 2.0 * 5.9933319091796875
Epoch 1630, val loss: 1.125005841255188
Epoch 1640, training loss: 12.000153541564941 = 0.01177900843322277 + 2.0 * 5.994187355041504
Epoch 1640, val loss: 1.1283830404281616
Epoch 1650, training loss: 12.003456115722656 = 0.011532012373209 + 2.0 * 5.995962142944336
Epoch 1650, val loss: 1.1317236423492432
Epoch 1660, training loss: 11.997793197631836 = 0.011298743076622486 + 2.0 * 5.993247032165527
Epoch 1660, val loss: 1.1349761486053467
Epoch 1670, training loss: 11.996395111083984 = 0.011071252636611462 + 2.0 * 5.992661952972412
Epoch 1670, val loss: 1.1382595300674438
Epoch 1680, training loss: 12.004831314086914 = 0.010852980427443981 + 2.0 * 5.9969892501831055
Epoch 1680, val loss: 1.1413824558258057
Epoch 1690, training loss: 11.996363639831543 = 0.01064543891698122 + 2.0 * 5.99285888671875
Epoch 1690, val loss: 1.1445868015289307
Epoch 1700, training loss: 11.994181632995605 = 0.01044350303709507 + 2.0 * 5.99186897277832
Epoch 1700, val loss: 1.1477389335632324
Epoch 1710, training loss: 11.997261047363281 = 0.010247835889458656 + 2.0 * 5.99350643157959
Epoch 1710, val loss: 1.1508539915084839
Epoch 1720, training loss: 11.994847297668457 = 0.010061116889119148 + 2.0 * 5.9923930168151855
Epoch 1720, val loss: 1.1539287567138672
Epoch 1730, training loss: 11.994483947753906 = 0.009882045909762383 + 2.0 * 5.992300987243652
Epoch 1730, val loss: 1.1569747924804688
Epoch 1740, training loss: 11.990017890930176 = 0.009706695564091206 + 2.0 * 5.9901556968688965
Epoch 1740, val loss: 1.1599467992782593
Epoch 1750, training loss: 11.990880012512207 = 0.00953742302954197 + 2.0 * 5.990671157836914
Epoch 1750, val loss: 1.162972092628479
Epoch 1760, training loss: 12.001969337463379 = 0.009372826665639877 + 2.0 * 5.996298313140869
Epoch 1760, val loss: 1.1658728122711182
Epoch 1770, training loss: 11.992243766784668 = 0.009216313250362873 + 2.0 * 5.991513729095459
Epoch 1770, val loss: 1.1687736511230469
Epoch 1780, training loss: 11.989072799682617 = 0.009062646888196468 + 2.0 * 5.990005016326904
Epoch 1780, val loss: 1.1715803146362305
Epoch 1790, training loss: 11.993590354919434 = 0.008913232944905758 + 2.0 * 5.99233865737915
Epoch 1790, val loss: 1.1744574308395386
Epoch 1800, training loss: 11.987895011901855 = 0.008768056519329548 + 2.0 * 5.989563465118408
Epoch 1800, val loss: 1.1772581338882446
Epoch 1810, training loss: 11.989155769348145 = 0.008628183975815773 + 2.0 * 5.990263938903809
Epoch 1810, val loss: 1.1801002025604248
Epoch 1820, training loss: 11.987382888793945 = 0.00849171169102192 + 2.0 * 5.989445686340332
Epoch 1820, val loss: 1.1828285455703735
Epoch 1830, training loss: 11.997421264648438 = 0.008360140025615692 + 2.0 * 5.99453067779541
Epoch 1830, val loss: 1.1855967044830322
Epoch 1840, training loss: 11.988595962524414 = 0.00823209434747696 + 2.0 * 5.990181922912598
Epoch 1840, val loss: 1.1882654428482056
Epoch 1850, training loss: 11.985280990600586 = 0.00810664240270853 + 2.0 * 5.988587379455566
Epoch 1850, val loss: 1.1908987760543823
Epoch 1860, training loss: 11.986900329589844 = 0.007985356263816357 + 2.0 * 5.989457607269287
Epoch 1860, val loss: 1.1936376094818115
Epoch 1870, training loss: 11.986841201782227 = 0.007867383770644665 + 2.0 * 5.9894866943359375
Epoch 1870, val loss: 1.1962783336639404
Epoch 1880, training loss: 11.992012023925781 = 0.007753343321382999 + 2.0 * 5.992129325866699
Epoch 1880, val loss: 1.1989402770996094
Epoch 1890, training loss: 11.982924461364746 = 0.0076397862285375595 + 2.0 * 5.987642288208008
Epoch 1890, val loss: 1.2014491558074951
Epoch 1900, training loss: 11.982603073120117 = 0.007530086673796177 + 2.0 * 5.987536430358887
Epoch 1900, val loss: 1.2039793729782104
Epoch 1910, training loss: 11.984012603759766 = 0.007423342205584049 + 2.0 * 5.98829460144043
Epoch 1910, val loss: 1.2066125869750977
Epoch 1920, training loss: 11.990255355834961 = 0.007319990545511246 + 2.0 * 5.991467475891113
Epoch 1920, val loss: 1.2090903520584106
Epoch 1930, training loss: 11.9845552444458 = 0.007218942977488041 + 2.0 * 5.988667964935303
Epoch 1930, val loss: 1.211498498916626
Epoch 1940, training loss: 11.983875274658203 = 0.0071196784265339375 + 2.0 * 5.988377571105957
Epoch 1940, val loss: 1.2140001058578491
Epoch 1950, training loss: 11.984463691711426 = 0.0070239342749118805 + 2.0 * 5.988719940185547
Epoch 1950, val loss: 1.2164257764816284
Epoch 1960, training loss: 11.982227325439453 = 0.00692996708676219 + 2.0 * 5.9876484870910645
Epoch 1960, val loss: 1.2188323736190796
Epoch 1970, training loss: 11.98089599609375 = 0.006837490946054459 + 2.0 * 5.987029075622559
Epoch 1970, val loss: 1.2212355136871338
Epoch 1980, training loss: 11.98326587677002 = 0.006747247185558081 + 2.0 * 5.988259315490723
Epoch 1980, val loss: 1.2236018180847168
Epoch 1990, training loss: 11.98181438446045 = 0.006658819504082203 + 2.0 * 5.98757791519165
Epoch 1990, val loss: 1.2258800268173218
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7852
Overall ASR: 0.6458
Flip ASR: 0.5956/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.710100173950195 = 1.9623569250106812 + 2.0 * 8.373871803283691
Epoch 0, val loss: 1.9602272510528564
Epoch 10, training loss: 18.698333740234375 = 1.951809287071228 + 2.0 * 8.373262405395508
Epoch 10, val loss: 1.9497662782669067
Epoch 20, training loss: 18.676265716552734 = 1.9388941526412964 + 2.0 * 8.368685722351074
Epoch 20, val loss: 1.9364843368530273
Epoch 30, training loss: 18.598587036132812 = 1.9219456911087036 + 2.0 * 8.3383207321167
Epoch 30, val loss: 1.9187546968460083
Epoch 40, training loss: 18.135272979736328 = 1.9021795988082886 + 2.0 * 8.116546630859375
Epoch 40, val loss: 1.8986495733261108
Epoch 50, training loss: 16.57335662841797 = 1.8828275203704834 + 2.0 * 7.345264911651611
Epoch 50, val loss: 1.8788471221923828
Epoch 60, training loss: 15.970769882202148 = 1.865195631980896 + 2.0 * 7.0527873039245605
Epoch 60, val loss: 1.8614628314971924
Epoch 70, training loss: 15.517752647399902 = 1.848902940750122 + 2.0 * 6.83442497253418
Epoch 70, val loss: 1.8453583717346191
Epoch 80, training loss: 15.166247367858887 = 1.834904432296753 + 2.0 * 6.665671348571777
Epoch 80, val loss: 1.831587553024292
Epoch 90, training loss: 14.910480499267578 = 1.8218755722045898 + 2.0 * 6.544302463531494
Epoch 90, val loss: 1.8186031579971313
Epoch 100, training loss: 14.721686363220215 = 1.8089686632156372 + 2.0 * 6.456358909606934
Epoch 100, val loss: 1.8057197332382202
Epoch 110, training loss: 14.57652759552002 = 1.7962278127670288 + 2.0 * 6.39015007019043
Epoch 110, val loss: 1.7927709817886353
Epoch 120, training loss: 14.46259593963623 = 1.7837258577346802 + 2.0 * 6.33943510055542
Epoch 120, val loss: 1.780011773109436
Epoch 130, training loss: 14.376657485961914 = 1.7709760665893555 + 2.0 * 6.302840709686279
Epoch 130, val loss: 1.766819953918457
Epoch 140, training loss: 14.302252769470215 = 1.757717251777649 + 2.0 * 6.272267818450928
Epoch 140, val loss: 1.7532317638397217
Epoch 150, training loss: 14.239008903503418 = 1.743803858757019 + 2.0 * 6.247602462768555
Epoch 150, val loss: 1.7394390106201172
Epoch 160, training loss: 14.190264701843262 = 1.7288529872894287 + 2.0 * 6.230705738067627
Epoch 160, val loss: 1.7251445055007935
Epoch 170, training loss: 14.138670921325684 = 1.7126166820526123 + 2.0 * 6.213027000427246
Epoch 170, val loss: 1.7099697589874268
Epoch 180, training loss: 14.093731880187988 = 1.6945219039916992 + 2.0 * 6.1996049880981445
Epoch 180, val loss: 1.6937109231948853
Epoch 190, training loss: 14.052255630493164 = 1.674272894859314 + 2.0 * 6.188991546630859
Epoch 190, val loss: 1.6760120391845703
Epoch 200, training loss: 14.005802154541016 = 1.6515328884124756 + 2.0 * 6.1771345138549805
Epoch 200, val loss: 1.6566427946090698
Epoch 210, training loss: 13.963115692138672 = 1.625918984413147 + 2.0 * 6.168598175048828
Epoch 210, val loss: 1.6351838111877441
Epoch 220, training loss: 13.915063858032227 = 1.5971051454544067 + 2.0 * 6.158979415893555
Epoch 220, val loss: 1.6114537715911865
Epoch 230, training loss: 13.868314743041992 = 1.5645325183868408 + 2.0 * 6.151891231536865
Epoch 230, val loss: 1.5848709344863892
Epoch 240, training loss: 13.818231582641602 = 1.528658151626587 + 2.0 * 6.144786834716797
Epoch 240, val loss: 1.555534839630127
Epoch 250, training loss: 13.763525009155273 = 1.489152431488037 + 2.0 * 6.137186527252197
Epoch 250, val loss: 1.5238361358642578
Epoch 260, training loss: 13.709325790405273 = 1.446262240409851 + 2.0 * 6.131531715393066
Epoch 260, val loss: 1.4894640445709229
Epoch 270, training loss: 13.654318809509277 = 1.4005415439605713 + 2.0 * 6.126888751983643
Epoch 270, val loss: 1.453379511833191
Epoch 280, training loss: 13.594526290893555 = 1.3531297445297241 + 2.0 * 6.12069845199585
Epoch 280, val loss: 1.416288137435913
Epoch 290, training loss: 13.535737991333008 = 1.304342269897461 + 2.0 * 6.115697860717773
Epoch 290, val loss: 1.3786261081695557
Epoch 300, training loss: 13.478253364562988 = 1.255062222480774 + 2.0 * 6.111595630645752
Epoch 300, val loss: 1.3410574197769165
Epoch 310, training loss: 13.4273042678833 = 1.2064844369888306 + 2.0 * 6.110409736633301
Epoch 310, val loss: 1.304597020149231
Epoch 320, training loss: 13.367169380187988 = 1.158967137336731 + 2.0 * 6.104101181030273
Epoch 320, val loss: 1.2692066431045532
Epoch 330, training loss: 13.314118385314941 = 1.1121273040771484 + 2.0 * 6.1009955406188965
Epoch 330, val loss: 1.2347880601882935
Epoch 340, training loss: 13.26004409790039 = 1.0658118724822998 + 2.0 * 6.097115993499756
Epoch 340, val loss: 1.2007845640182495
Epoch 350, training loss: 13.215662956237793 = 1.0198557376861572 + 2.0 * 6.097903728485107
Epoch 350, val loss: 1.1673206090927124
Epoch 360, training loss: 13.162976264953613 = 0.9747750163078308 + 2.0 * 6.094100475311279
Epoch 360, val loss: 1.134507656097412
Epoch 370, training loss: 13.108783721923828 = 0.9307684898376465 + 2.0 * 6.08900785446167
Epoch 370, val loss: 1.1024349927902222
Epoch 380, training loss: 13.059196472167969 = 0.8875397443771362 + 2.0 * 6.0858283042907715
Epoch 380, val loss: 1.071181297302246
Epoch 390, training loss: 13.024484634399414 = 0.8454726934432983 + 2.0 * 6.089506149291992
Epoch 390, val loss: 1.0406028032302856
Epoch 400, training loss: 12.967891693115234 = 0.8049084544181824 + 2.0 * 6.081491470336914
Epoch 400, val loss: 1.0116968154907227
Epoch 410, training loss: 12.924134254455566 = 0.766274631023407 + 2.0 * 6.078929901123047
Epoch 410, val loss: 0.9841961860656738
Epoch 420, training loss: 12.882842063903809 = 0.7292239665985107 + 2.0 * 6.076808929443359
Epoch 420, val loss: 0.9581922888755798
Epoch 430, training loss: 12.844954490661621 = 0.6939237117767334 + 2.0 * 6.075515270233154
Epoch 430, val loss: 0.933607816696167
Epoch 440, training loss: 12.810593605041504 = 0.6605555415153503 + 2.0 * 6.075018882751465
Epoch 440, val loss: 0.9107586145401001
Epoch 450, training loss: 12.771044731140137 = 0.628911018371582 + 2.0 * 6.071066856384277
Epoch 450, val loss: 0.8896178603172302
Epoch 460, training loss: 12.737424850463867 = 0.5988010764122009 + 2.0 * 6.06931209564209
Epoch 460, val loss: 0.869804859161377
Epoch 470, training loss: 12.706754684448242 = 0.5701006054878235 + 2.0 * 6.068326950073242
Epoch 470, val loss: 0.8513731956481934
Epoch 480, training loss: 12.675013542175293 = 0.5427599549293518 + 2.0 * 6.066126823425293
Epoch 480, val loss: 0.8343199491500854
Epoch 490, training loss: 12.642967224121094 = 0.5165761113166809 + 2.0 * 6.063195705413818
Epoch 490, val loss: 0.8184332251548767
Epoch 500, training loss: 12.620866775512695 = 0.4913560450077057 + 2.0 * 6.064755439758301
Epoch 500, val loss: 0.8034209609031677
Epoch 510, training loss: 12.596678733825684 = 0.4671003818511963 + 2.0 * 6.064789295196533
Epoch 510, val loss: 0.7896425724029541
Epoch 520, training loss: 12.564900398254395 = 0.44393792748451233 + 2.0 * 6.060481071472168
Epoch 520, val loss: 0.7767046093940735
Epoch 530, training loss: 12.536531448364258 = 0.42154237627983093 + 2.0 * 6.057494640350342
Epoch 530, val loss: 0.76462322473526
Epoch 540, training loss: 12.518464088439941 = 0.39978307485580444 + 2.0 * 6.059340476989746
Epoch 540, val loss: 0.7533764243125916
Epoch 550, training loss: 12.48900032043457 = 0.37886470556259155 + 2.0 * 6.055068016052246
Epoch 550, val loss: 0.7427867650985718
Epoch 560, training loss: 12.46773910522461 = 0.3585090935230255 + 2.0 * 6.054615020751953
Epoch 560, val loss: 0.7332032918930054
Epoch 570, training loss: 12.444009780883789 = 0.3389502167701721 + 2.0 * 6.052529811859131
Epoch 570, val loss: 0.724334180355072
Epoch 580, training loss: 12.427669525146484 = 0.32008013129234314 + 2.0 * 6.053794860839844
Epoch 580, val loss: 0.7163333892822266
Epoch 590, training loss: 12.401479721069336 = 0.30209824442863464 + 2.0 * 6.0496907234191895
Epoch 590, val loss: 0.7091144919395447
Epoch 600, training loss: 12.38121509552002 = 0.2848462462425232 + 2.0 * 6.048184394836426
Epoch 600, val loss: 0.7027681469917297
Epoch 610, training loss: 12.375055313110352 = 0.26853594183921814 + 2.0 * 6.05325984954834
Epoch 610, val loss: 0.6971549391746521
Epoch 620, training loss: 12.348584175109863 = 0.25322163105010986 + 2.0 * 6.0476813316345215
Epoch 620, val loss: 0.6924481391906738
Epoch 630, training loss: 12.327906608581543 = 0.23886063694953918 + 2.0 * 6.044522762298584
Epoch 630, val loss: 0.6885316371917725
Epoch 640, training loss: 12.311319351196289 = 0.2253390997648239 + 2.0 * 6.042990207672119
Epoch 640, val loss: 0.6853951811790466
Epoch 650, training loss: 12.313161849975586 = 0.21263274550437927 + 2.0 * 6.050264358520508
Epoch 650, val loss: 0.682971179485321
Epoch 660, training loss: 12.283401489257812 = 0.2008884698152542 + 2.0 * 6.041256427764893
Epoch 660, val loss: 0.6810015439987183
Epoch 670, training loss: 12.268497467041016 = 0.18986928462982178 + 2.0 * 6.039314270019531
Epoch 670, val loss: 0.6797530651092529
Epoch 680, training loss: 12.262908935546875 = 0.17957687377929688 + 2.0 * 6.041666030883789
Epoch 680, val loss: 0.6791711449623108
Epoch 690, training loss: 12.260025024414062 = 0.17004308104515076 + 2.0 * 6.0449910163879395
Epoch 690, val loss: 0.6789016723632812
Epoch 700, training loss: 12.23659896850586 = 0.1611763834953308 + 2.0 * 6.037711143493652
Epoch 700, val loss: 0.6790801882743835
Epoch 710, training loss: 12.22354793548584 = 0.1528996229171753 + 2.0 * 6.0353240966796875
Epoch 710, val loss: 0.6797914505004883
Epoch 720, training loss: 12.22460651397705 = 0.1451473832130432 + 2.0 * 6.039729595184326
Epoch 720, val loss: 0.6809242367744446
Epoch 730, training loss: 12.217673301696777 = 0.1379266381263733 + 2.0 * 6.039873123168945
Epoch 730, val loss: 0.6823000907897949
Epoch 740, training loss: 12.197966575622559 = 0.13120298087596893 + 2.0 * 6.033381938934326
Epoch 740, val loss: 0.6837976574897766
Epoch 750, training loss: 12.190311431884766 = 0.12488653510808945 + 2.0 * 6.032712459564209
Epoch 750, val loss: 0.6856668591499329
Epoch 760, training loss: 12.192391395568848 = 0.11896069347858429 + 2.0 * 6.036715507507324
Epoch 760, val loss: 0.6878447532653809
Epoch 770, training loss: 12.17859935760498 = 0.11337945610284805 + 2.0 * 6.032609939575195
Epoch 770, val loss: 0.6902130246162415
Epoch 780, training loss: 12.173944473266602 = 0.10816804319620132 + 2.0 * 6.032888412475586
Epoch 780, val loss: 0.6926765441894531
Epoch 790, training loss: 12.163270950317383 = 0.10324087738990784 + 2.0 * 6.030014991760254
Epoch 790, val loss: 0.6953719258308411
Epoch 800, training loss: 12.157726287841797 = 0.0986052080988884 + 2.0 * 6.029560565948486
Epoch 800, val loss: 0.698156476020813
Epoch 810, training loss: 12.152713775634766 = 0.09422074258327484 + 2.0 * 6.0292463302612305
Epoch 810, val loss: 0.7011351585388184
Epoch 820, training loss: 12.146085739135742 = 0.09007328748703003 + 2.0 * 6.028006076812744
Epoch 820, val loss: 0.7041991353034973
Epoch 830, training loss: 12.140628814697266 = 0.08617079257965088 + 2.0 * 6.027228832244873
Epoch 830, val loss: 0.7073473930358887
Epoch 840, training loss: 12.133601188659668 = 0.08247788995504379 + 2.0 * 6.025561809539795
Epoch 840, val loss: 0.7105904817581177
Epoch 850, training loss: 12.127970695495605 = 0.07899302244186401 + 2.0 * 6.024488925933838
Epoch 850, val loss: 0.7139400243759155
Epoch 860, training loss: 12.138251304626465 = 0.07571016252040863 + 2.0 * 6.031270503997803
Epoch 860, val loss: 0.7174652218818665
Epoch 870, training loss: 12.125057220458984 = 0.072658009827137 + 2.0 * 6.026199817657471
Epoch 870, val loss: 0.7208631038665771
Epoch 880, training loss: 12.116114616394043 = 0.06976904720067978 + 2.0 * 6.023172855377197
Epoch 880, val loss: 0.724206805229187
Epoch 890, training loss: 12.116254806518555 = 0.06702607125043869 + 2.0 * 6.024614334106445
Epoch 890, val loss: 0.7278679609298706
Epoch 900, training loss: 12.106832504272461 = 0.06444991379976273 + 2.0 * 6.021191120147705
Epoch 900, val loss: 0.7315356135368347
Epoch 910, training loss: 12.105328559875488 = 0.06200644373893738 + 2.0 * 6.021661281585693
Epoch 910, val loss: 0.7349121570587158
Epoch 920, training loss: 12.099103927612305 = 0.05968919023871422 + 2.0 * 6.019707202911377
Epoch 920, val loss: 0.7385662794113159
Epoch 930, training loss: 12.095317840576172 = 0.057479485869407654 + 2.0 * 6.018918991088867
Epoch 930, val loss: 0.7422868013381958
Epoch 940, training loss: 12.120612144470215 = 0.05539053678512573 + 2.0 * 6.032610893249512
Epoch 940, val loss: 0.7459829449653625
Epoch 950, training loss: 12.094168663024902 = 0.05341052636504173 + 2.0 * 6.020379066467285
Epoch 950, val loss: 0.7496384978294373
Epoch 960, training loss: 12.086506843566895 = 0.05154389515519142 + 2.0 * 6.017481327056885
Epoch 960, val loss: 0.7530360817909241
Epoch 970, training loss: 12.082335472106934 = 0.049762774258852005 + 2.0 * 6.016286373138428
Epoch 970, val loss: 0.7566906213760376
Epoch 980, training loss: 12.079511642456055 = 0.0480581633746624 + 2.0 * 6.015726566314697
Epoch 980, val loss: 0.7604002356529236
Epoch 990, training loss: 12.076645851135254 = 0.04642759636044502 + 2.0 * 6.015109062194824
Epoch 990, val loss: 0.7640249729156494
Epoch 1000, training loss: 12.08422565460205 = 0.044861339032649994 + 2.0 * 6.019681930541992
Epoch 1000, val loss: 0.7676359415054321
Epoch 1010, training loss: 12.091843605041504 = 0.043392885476350784 + 2.0 * 6.024225234985352
Epoch 1010, val loss: 0.7712113261222839
Epoch 1020, training loss: 12.07205867767334 = 0.04199323058128357 + 2.0 * 6.015032768249512
Epoch 1020, val loss: 0.7745883464813232
Epoch 1030, training loss: 12.067582130432129 = 0.040658630430698395 + 2.0 * 6.013461589813232
Epoch 1030, val loss: 0.7780449390411377
Epoch 1040, training loss: 12.068619728088379 = 0.0393797941505909 + 2.0 * 6.014619827270508
Epoch 1040, val loss: 0.7816954851150513
Epoch 1050, training loss: 12.067610740661621 = 0.03815286234021187 + 2.0 * 6.014729022979736
Epoch 1050, val loss: 0.7852613925933838
Epoch 1060, training loss: 12.063658714294434 = 0.03697499632835388 + 2.0 * 6.013341903686523
Epoch 1060, val loss: 0.7886853218078613
Epoch 1070, training loss: 12.061483383178711 = 0.03585481643676758 + 2.0 * 6.012814521789551
Epoch 1070, val loss: 0.7921647429466248
Epoch 1080, training loss: 12.05747127532959 = 0.03477873280644417 + 2.0 * 6.011346340179443
Epoch 1080, val loss: 0.7957137823104858
Epoch 1090, training loss: 12.058309555053711 = 0.03375782072544098 + 2.0 * 6.012275695800781
Epoch 1090, val loss: 0.7990086078643799
Epoch 1100, training loss: 12.053421020507812 = 0.03277505189180374 + 2.0 * 6.0103230476379395
Epoch 1100, val loss: 0.8024004101753235
Epoch 1110, training loss: 12.060830116271973 = 0.03183316811919212 + 2.0 * 6.014498710632324
Epoch 1110, val loss: 0.8058321475982666
Epoch 1120, training loss: 12.051796913146973 = 0.030928710475564003 + 2.0 * 6.010434150695801
Epoch 1120, val loss: 0.809343695640564
Epoch 1130, training loss: 12.048734664916992 = 0.03006524033844471 + 2.0 * 6.009334564208984
Epoch 1130, val loss: 0.8124530911445618
Epoch 1140, training loss: 12.045916557312012 = 0.02923186682164669 + 2.0 * 6.008342266082764
Epoch 1140, val loss: 0.8158442974090576
Epoch 1150, training loss: 12.077207565307617 = 0.028429480269551277 + 2.0 * 6.024389266967773
Epoch 1150, val loss: 0.8192422389984131
Epoch 1160, training loss: 12.046177864074707 = 0.02767363004386425 + 2.0 * 6.009252071380615
Epoch 1160, val loss: 0.8224263787269592
Epoch 1170, training loss: 12.042487144470215 = 0.026944629848003387 + 2.0 * 6.0077714920043945
Epoch 1170, val loss: 0.8254552483558655
Epoch 1180, training loss: 12.038960456848145 = 0.026238609105348587 + 2.0 * 6.00636100769043
Epoch 1180, val loss: 0.8287453651428223
Epoch 1190, training loss: 12.037096977233887 = 0.025556424632668495 + 2.0 * 6.005770206451416
Epoch 1190, val loss: 0.8320053815841675
Epoch 1200, training loss: 12.041772842407227 = 0.024897253140807152 + 2.0 * 6.008437633514404
Epoch 1200, val loss: 0.8351956009864807
Epoch 1210, training loss: 12.041029930114746 = 0.024266276508569717 + 2.0 * 6.0083818435668945
Epoch 1210, val loss: 0.8383924961090088
Epoch 1220, training loss: 12.037894248962402 = 0.023662837222218513 + 2.0 * 6.007115840911865
Epoch 1220, val loss: 0.8414208889007568
Epoch 1230, training loss: 12.032776832580566 = 0.02308746613562107 + 2.0 * 6.004844665527344
Epoch 1230, val loss: 0.8443788886070251
Epoch 1240, training loss: 12.031328201293945 = 0.022526342421770096 + 2.0 * 6.004400730133057
Epoch 1240, val loss: 0.8475558757781982
Epoch 1250, training loss: 12.033021926879883 = 0.021983355283737183 + 2.0 * 6.005519390106201
Epoch 1250, val loss: 0.8506320714950562
Epoch 1260, training loss: 12.037501335144043 = 0.02146025374531746 + 2.0 * 6.008020401000977
Epoch 1260, val loss: 0.8537212014198303
Epoch 1270, training loss: 12.029281616210938 = 0.020953690633177757 + 2.0 * 6.00416374206543
Epoch 1270, val loss: 0.8566621541976929
Epoch 1280, training loss: 12.031402587890625 = 0.02047082409262657 + 2.0 * 6.005465984344482
Epoch 1280, val loss: 0.8596081137657166
Epoch 1290, training loss: 12.024828910827637 = 0.020001763477921486 + 2.0 * 6.002413749694824
Epoch 1290, val loss: 0.8626382946968079
Epoch 1300, training loss: 12.023966789245605 = 0.019547227770090103 + 2.0 * 6.002209663391113
Epoch 1300, val loss: 0.8655810952186584
Epoch 1310, training loss: 12.026548385620117 = 0.019108859822154045 + 2.0 * 6.003719806671143
Epoch 1310, val loss: 0.8685187697410583
Epoch 1320, training loss: 12.024299621582031 = 0.018684525042772293 + 2.0 * 6.0028076171875
Epoch 1320, val loss: 0.8714540004730225
Epoch 1330, training loss: 12.032143592834473 = 0.01827603206038475 + 2.0 * 6.006933689117432
Epoch 1330, val loss: 0.8743095397949219
Epoch 1340, training loss: 12.02928638458252 = 0.017876161262392998 + 2.0 * 6.005704879760742
Epoch 1340, val loss: 0.8772244453430176
Epoch 1350, training loss: 12.01941204071045 = 0.01749800518155098 + 2.0 * 6.000957012176514
Epoch 1350, val loss: 0.8799964189529419
Epoch 1360, training loss: 12.01713752746582 = 0.017130684107542038 + 2.0 * 6.000003337860107
Epoch 1360, val loss: 0.8827646970748901
Epoch 1370, training loss: 12.015375137329102 = 0.016772208735346794 + 2.0 * 5.999301433563232
Epoch 1370, val loss: 0.8855974674224854
Epoch 1380, training loss: 12.016963958740234 = 0.016423726454377174 + 2.0 * 6.000269889831543
Epoch 1380, val loss: 0.8884643316268921
Epoch 1390, training loss: 12.02232837677002 = 0.016087036579847336 + 2.0 * 6.0031208992004395
Epoch 1390, val loss: 0.8913895487785339
Epoch 1400, training loss: 12.01439094543457 = 0.015762558206915855 + 2.0 * 5.999314308166504
Epoch 1400, val loss: 0.8939850926399231
Epoch 1410, training loss: 12.013561248779297 = 0.015449829399585724 + 2.0 * 5.999055862426758
Epoch 1410, val loss: 0.8965305685997009
Epoch 1420, training loss: 12.017300605773926 = 0.01514510065317154 + 2.0 * 6.001077651977539
Epoch 1420, val loss: 0.899402916431427
Epoch 1430, training loss: 12.011397361755371 = 0.014847935177385807 + 2.0 * 5.998274803161621
Epoch 1430, val loss: 0.9021592736244202
Epoch 1440, training loss: 12.009299278259277 = 0.014562276192009449 + 2.0 * 5.997368335723877
Epoch 1440, val loss: 0.9046764969825745
Epoch 1450, training loss: 12.009543418884277 = 0.014282732270658016 + 2.0 * 5.9976301193237305
Epoch 1450, val loss: 0.9073268175125122
Epoch 1460, training loss: 12.023828506469727 = 0.01401533093303442 + 2.0 * 6.00490665435791
Epoch 1460, val loss: 0.9100937843322754
Epoch 1470, training loss: 12.01242446899414 = 0.013749795965850353 + 2.0 * 5.999337196350098
Epoch 1470, val loss: 0.9126916527748108
Epoch 1480, training loss: 12.006575584411621 = 0.013495763763785362 + 2.0 * 5.996540069580078
Epoch 1480, val loss: 0.9151227474212646
Epoch 1490, training loss: 12.00527286529541 = 0.01324774231761694 + 2.0 * 5.9960126876831055
Epoch 1490, val loss: 0.9177622199058533
Epoch 1500, training loss: 12.015037536621094 = 0.013006092980504036 + 2.0 * 6.001015663146973
Epoch 1500, val loss: 0.9203855991363525
Epoch 1510, training loss: 12.007615089416504 = 0.01277264952659607 + 2.0 * 5.9974212646484375
Epoch 1510, val loss: 0.9229727387428284
Epoch 1520, training loss: 12.00661849975586 = 0.012545397505164146 + 2.0 * 5.997036457061768
Epoch 1520, val loss: 0.9253370761871338
Epoch 1530, training loss: 12.003033638000488 = 0.012323661707341671 + 2.0 * 5.995355129241943
Epoch 1530, val loss: 0.9278316497802734
Epoch 1540, training loss: 12.010005950927734 = 0.012109251692891121 + 2.0 * 5.998948574066162
Epoch 1540, val loss: 0.9303550720214844
Epoch 1550, training loss: 12.002568244934082 = 0.011901115998625755 + 2.0 * 5.995333671569824
Epoch 1550, val loss: 0.9329328536987305
Epoch 1560, training loss: 11.999186515808105 = 0.011697212234139442 + 2.0 * 5.993744850158691
Epoch 1560, val loss: 0.9352303147315979
Epoch 1570, training loss: 11.998144149780273 = 0.011498230509459972 + 2.0 * 5.993322849273682
Epoch 1570, val loss: 0.9376799464225769
Epoch 1580, training loss: 11.998007774353027 = 0.01130350586026907 + 2.0 * 5.993351936340332
Epoch 1580, val loss: 0.9401896595954895
Epoch 1590, training loss: 12.017623901367188 = 0.011113125830888748 + 2.0 * 6.003255367279053
Epoch 1590, val loss: 0.9426721334457397
Epoch 1600, training loss: 12.004338264465332 = 0.010931458324193954 + 2.0 * 5.996703624725342
Epoch 1600, val loss: 0.945050060749054
Epoch 1610, training loss: 11.997763633728027 = 0.010753854177892208 + 2.0 * 5.993505001068115
Epoch 1610, val loss: 0.947186291217804
Epoch 1620, training loss: 11.996051788330078 = 0.010582874529063702 + 2.0 * 5.992734432220459
Epoch 1620, val loss: 0.9494582414627075
Epoch 1630, training loss: 11.995251655578613 = 0.010414116084575653 + 2.0 * 5.9924187660217285
Epoch 1630, val loss: 0.9518930912017822
Epoch 1640, training loss: 11.998178482055664 = 0.010248242877423763 + 2.0 * 5.993965148925781
Epoch 1640, val loss: 0.954300045967102
Epoch 1650, training loss: 11.99873161315918 = 0.010085448622703552 + 2.0 * 5.994323253631592
Epoch 1650, val loss: 0.956633985042572
Epoch 1660, training loss: 12.000787734985352 = 0.00992738176137209 + 2.0 * 5.995429992675781
Epoch 1660, val loss: 0.9588548541069031
Epoch 1670, training loss: 11.991837501525879 = 0.009776766411960125 + 2.0 * 5.991030216217041
Epoch 1670, val loss: 0.9610429406166077
Epoch 1680, training loss: 11.990931510925293 = 0.00962953083217144 + 2.0 * 5.9906511306762695
Epoch 1680, val loss: 0.9630807638168335
Epoch 1690, training loss: 11.989015579223633 = 0.009484179317951202 + 2.0 * 5.989765644073486
Epoch 1690, val loss: 0.965365469455719
Epoch 1700, training loss: 11.989554405212402 = 0.009340968914330006 + 2.0 * 5.990106582641602
Epoch 1700, val loss: 0.9677294492721558
Epoch 1710, training loss: 12.006840705871582 = 0.00920091662555933 + 2.0 * 5.998819828033447
Epoch 1710, val loss: 0.9700638651847839
Epoch 1720, training loss: 11.99989128112793 = 0.009066351689398289 + 2.0 * 5.995412349700928
Epoch 1720, val loss: 0.9720656871795654
Epoch 1730, training loss: 11.99447250366211 = 0.00893398467451334 + 2.0 * 5.992769241333008
Epoch 1730, val loss: 0.9741424322128296
Epoch 1740, training loss: 11.988849639892578 = 0.00880588311702013 + 2.0 * 5.990021705627441
Epoch 1740, val loss: 0.9762189984321594
Epoch 1750, training loss: 11.986539840698242 = 0.00868215225636959 + 2.0 * 5.98892879486084
Epoch 1750, val loss: 0.9783132076263428
Epoch 1760, training loss: 11.987405776977539 = 0.00855919811874628 + 2.0 * 5.9894232749938965
Epoch 1760, val loss: 0.9805057048797607
Epoch 1770, training loss: 11.998263359069824 = 0.008438797667622566 + 2.0 * 5.994912147521973
Epoch 1770, val loss: 0.9826862215995789
Epoch 1780, training loss: 11.987558364868164 = 0.00831836648285389 + 2.0 * 5.989620208740234
Epoch 1780, val loss: 0.984697699546814
Epoch 1790, training loss: 11.986659049987793 = 0.008204130455851555 + 2.0 * 5.989227294921875
Epoch 1790, val loss: 0.9866968393325806
Epoch 1800, training loss: 11.990352630615234 = 0.008091604337096214 + 2.0 * 5.991130352020264
Epoch 1800, val loss: 0.9888888597488403
Epoch 1810, training loss: 11.98582649230957 = 0.007982245646417141 + 2.0 * 5.988922119140625
Epoch 1810, val loss: 0.9907488822937012
Epoch 1820, training loss: 11.988868713378906 = 0.007875177077949047 + 2.0 * 5.990496635437012
Epoch 1820, val loss: 0.9927442669868469
Epoch 1830, training loss: 11.992197036743164 = 0.007768658921122551 + 2.0 * 5.992214202880859
Epoch 1830, val loss: 0.9947783350944519
Epoch 1840, training loss: 11.983088493347168 = 0.007666878867894411 + 2.0 * 5.987710952758789
Epoch 1840, val loss: 0.9966384172439575
Epoch 1850, training loss: 11.981451988220215 = 0.007566600572317839 + 2.0 * 5.986942768096924
Epoch 1850, val loss: 0.9985336661338806
Epoch 1860, training loss: 11.980368614196777 = 0.007467380724847317 + 2.0 * 5.986450672149658
Epoch 1860, val loss: 1.000572919845581
Epoch 1870, training loss: 11.987051963806152 = 0.007369102444499731 + 2.0 * 5.989841461181641
Epoch 1870, val loss: 1.0026370286941528
Epoch 1880, training loss: 11.980769157409668 = 0.00727417366579175 + 2.0 * 5.9867472648620605
Epoch 1880, val loss: 1.0046013593673706
Epoch 1890, training loss: 11.977898597717285 = 0.007182283792644739 + 2.0 * 5.985358238220215
Epoch 1890, val loss: 1.0062432289123535
Epoch 1900, training loss: 11.97749137878418 = 0.00709235155954957 + 2.0 * 5.985199451446533
Epoch 1900, val loss: 1.008028507232666
Epoch 1910, training loss: 11.986708641052246 = 0.007003516890108585 + 2.0 * 5.989852428436279
Epoch 1910, val loss: 1.010072112083435
Epoch 1920, training loss: 11.977046012878418 = 0.0069153946824371815 + 2.0 * 5.985065460205078
Epoch 1920, val loss: 1.0120131969451904
Epoch 1930, training loss: 11.978520393371582 = 0.006830085534602404 + 2.0 * 5.98584508895874
Epoch 1930, val loss: 1.0136827230453491
Epoch 1940, training loss: 11.98046588897705 = 0.006746961269527674 + 2.0 * 5.986859321594238
Epoch 1940, val loss: 1.0154364109039307
Epoch 1950, training loss: 11.977256774902344 = 0.00666514877229929 + 2.0 * 5.98529577255249
Epoch 1950, val loss: 1.0173364877700806
Epoch 1960, training loss: 11.99521541595459 = 0.0065851672552526 + 2.0 * 5.994315147399902
Epoch 1960, val loss: 1.0191847085952759
Epoch 1970, training loss: 11.983868598937988 = 0.006506629753857851 + 2.0 * 5.988680839538574
Epoch 1970, val loss: 1.020996332168579
Epoch 1980, training loss: 11.976366996765137 = 0.006431141402572393 + 2.0 * 5.9849677085876465
Epoch 1980, val loss: 1.0224965810775757
Epoch 1990, training loss: 11.973298072814941 = 0.006355765275657177 + 2.0 * 5.983470916748047
Epoch 1990, val loss: 1.0243052244186401
=== picking the best model according to the performance on validation ===
target class rate on Vs: 0.8000
accuracy on clean test nodes: 0.7926
Overall ASR: 0.5609
Flip ASR: 0.5156/225 nodes
The final ASR:0.60271, 0.03466, Accuracy:0.79630, 0.01090
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11596])
remove edge: torch.Size([2, 9434])
updated graph: torch.Size([2, 10474])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9926
Flip ASR: 0.9911/225 nodes
The final ASR:0.98278, 0.00696, Accuracy:0.83457, 0.00175
Begin epxeriment: cont_weight: 2 epoch:2000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 18.70903778076172 = 1.961188793182373 + 2.0 * 8.373924255371094
Epoch 0, val loss: 1.9707971811294556
Epoch 10, training loss: 18.69758415222168 = 1.9503858089447021 + 2.0 * 8.3735990524292
Epoch 10, val loss: 1.9598748683929443
Epoch 20, training loss: 18.678565979003906 = 1.9368071556091309 + 2.0 * 8.370879173278809
Epoch 20, val loss: 1.9461967945098877
Epoch 30, training loss: 18.617006301879883 = 1.9178180694580078 + 2.0 * 8.349594116210938
Epoch 30, val loss: 1.9272372722625732
Epoch 40, training loss: 18.34309196472168 = 1.8933930397033691 + 2.0 * 8.224849700927734
Epoch 40, val loss: 1.9037026166915894
Epoch 50, training loss: 17.35930824279785 = 1.86666738986969 + 2.0 * 7.746320724487305
Epoch 50, val loss: 1.8782289028167725
Epoch 60, training loss: 16.705970764160156 = 1.8426138162612915 + 2.0 * 7.431678771972656
Epoch 60, val loss: 1.8567829132080078
Epoch 70, training loss: 15.942096710205078 = 1.825192928314209 + 2.0 * 7.0584516525268555
Epoch 70, val loss: 1.841125726699829
Epoch 80, training loss: 15.50513744354248 = 1.807413101196289 + 2.0 * 6.848862171173096
Epoch 80, val loss: 1.824832558631897
Epoch 90, training loss: 15.189126014709473 = 1.7904132604599 + 2.0 * 6.699356555938721
Epoch 90, val loss: 1.8087714910507202
Epoch 100, training loss: 14.929115295410156 = 1.7748371362686157 + 2.0 * 6.577138900756836
Epoch 100, val loss: 1.794032096862793
Epoch 110, training loss: 14.741727828979492 = 1.7591664791107178 + 2.0 * 6.491280555725098
Epoch 110, val loss: 1.779179573059082
Epoch 120, training loss: 14.597278594970703 = 1.7419378757476807 + 2.0 * 6.427670478820801
Epoch 120, val loss: 1.76332688331604
Epoch 130, training loss: 14.485363006591797 = 1.7234480381011963 + 2.0 * 6.38095760345459
Epoch 130, val loss: 1.746610403060913
Epoch 140, training loss: 14.382864952087402 = 1.7038248777389526 + 2.0 * 6.33951997756958
Epoch 140, val loss: 1.7291953563690186
Epoch 150, training loss: 14.301115036010742 = 1.6820693016052246 + 2.0 * 6.30952262878418
Epoch 150, val loss: 1.7103933095932007
Epoch 160, training loss: 14.226105690002441 = 1.6575428247451782 + 2.0 * 6.284281253814697
Epoch 160, val loss: 1.6896616220474243
Epoch 170, training loss: 14.163827896118164 = 1.6302858591079712 + 2.0 * 6.266770839691162
Epoch 170, val loss: 1.666826844215393
Epoch 180, training loss: 14.09565258026123 = 1.6004496812820435 + 2.0 * 6.247601509094238
Epoch 180, val loss: 1.641875147819519
Epoch 190, training loss: 14.036832809448242 = 1.5676865577697754 + 2.0 * 6.2345733642578125
Epoch 190, val loss: 1.614554762840271
Epoch 200, training loss: 13.976346015930176 = 1.532414436340332 + 2.0 * 6.221965789794922
Epoch 200, val loss: 1.585480809211731
Epoch 210, training loss: 13.918493270874023 = 1.4951598644256592 + 2.0 * 6.211666584014893
Epoch 210, val loss: 1.555030345916748
Epoch 220, training loss: 13.859966278076172 = 1.4560459852218628 + 2.0 * 6.20196008682251
Epoch 220, val loss: 1.5235726833343506
Epoch 230, training loss: 13.815752983093262 = 1.4155471324920654 + 2.0 * 6.200102806091309
Epoch 230, val loss: 1.4916952848434448
Epoch 240, training loss: 13.7510404586792 = 1.3748902082443237 + 2.0 * 6.188075065612793
Epoch 240, val loss: 1.4604687690734863
Epoch 250, training loss: 13.690473556518555 = 1.3344755172729492 + 2.0 * 6.177999019622803
Epoch 250, val loss: 1.4301955699920654
Epoch 260, training loss: 13.636238098144531 = 1.2944051027297974 + 2.0 * 6.170916557312012
Epoch 260, val loss: 1.4009872674942017
Epoch 270, training loss: 13.582653045654297 = 1.254817008972168 + 2.0 * 6.1639180183410645
Epoch 270, val loss: 1.3728559017181396
Epoch 280, training loss: 13.535428047180176 = 1.2161226272583008 + 2.0 * 6.1596527099609375
Epoch 280, val loss: 1.3461852073669434
Epoch 290, training loss: 13.484895706176758 = 1.1787928342819214 + 2.0 * 6.153051376342773
Epoch 290, val loss: 1.3209590911865234
Epoch 300, training loss: 13.435792922973633 = 1.1424998044967651 + 2.0 * 6.146646499633789
Epoch 300, val loss: 1.2969379425048828
Epoch 310, training loss: 13.3897066116333 = 1.1072367429733276 + 2.0 * 6.141234874725342
Epoch 310, val loss: 1.2740046977996826
Epoch 320, training loss: 13.346747398376465 = 1.0730373859405518 + 2.0 * 6.136855125427246
Epoch 320, val loss: 1.2520439624786377
Epoch 330, training loss: 13.30400562286377 = 1.0396678447723389 + 2.0 * 6.132168769836426
Epoch 330, val loss: 1.2309532165527344
Epoch 340, training loss: 13.265191078186035 = 1.007054090499878 + 2.0 * 6.129068374633789
Epoch 340, val loss: 1.2104159593582153
Epoch 350, training loss: 13.222803115844727 = 0.9751797914505005 + 2.0 * 6.123811721801758
Epoch 350, val loss: 1.190385103225708
Epoch 360, training loss: 13.183611869812012 = 0.9437643885612488 + 2.0 * 6.1199235916137695
Epoch 360, val loss: 1.1708331108093262
Epoch 370, training loss: 13.14616584777832 = 0.9124493598937988 + 2.0 * 6.11685848236084
Epoch 370, val loss: 1.1513997316360474
Epoch 380, training loss: 13.107380867004395 = 0.8811315894126892 + 2.0 * 6.113124847412109
Epoch 380, val loss: 1.1318978071212769
Epoch 390, training loss: 13.070676803588867 = 0.8498236536979675 + 2.0 * 6.110426425933838
Epoch 390, val loss: 1.112565517425537
Epoch 400, training loss: 13.037164688110352 = 0.8184307813644409 + 2.0 * 6.1093668937683105
Epoch 400, val loss: 1.0932525396347046
Epoch 410, training loss: 13.000420570373535 = 0.7870873212814331 + 2.0 * 6.106666564941406
Epoch 410, val loss: 1.074065923690796
Epoch 420, training loss: 12.958162307739258 = 0.7560175061225891 + 2.0 * 6.101072311401367
Epoch 420, val loss: 1.055215835571289
Epoch 430, training loss: 12.922857284545898 = 0.7252292037010193 + 2.0 * 6.098814010620117
Epoch 430, val loss: 1.0369112491607666
Epoch 440, training loss: 12.893898010253906 = 0.6949186325073242 + 2.0 * 6.099489688873291
Epoch 440, val loss: 1.0191351175308228
Epoch 450, training loss: 12.857075691223145 = 0.6654312610626221 + 2.0 * 6.095822334289551
Epoch 450, val loss: 1.002345323562622
Epoch 460, training loss: 12.839670181274414 = 0.636986255645752 + 2.0 * 6.10134220123291
Epoch 460, val loss: 0.986755907535553
Epoch 470, training loss: 12.790288925170898 = 0.6101155877113342 + 2.0 * 6.090086460113525
Epoch 470, val loss: 0.9725997447967529
Epoch 480, training loss: 12.759961128234863 = 0.5845561623573303 + 2.0 * 6.08770227432251
Epoch 480, val loss: 0.9600600600242615
Epoch 490, training loss: 12.731209754943848 = 0.5603189468383789 + 2.0 * 6.085445404052734
Epoch 490, val loss: 0.9489991068840027
Epoch 500, training loss: 12.704764366149902 = 0.5373036861419678 + 2.0 * 6.083730220794678
Epoch 500, val loss: 0.9393230676651001
Epoch 510, training loss: 12.680282592773438 = 0.5154840350151062 + 2.0 * 6.082399368286133
Epoch 510, val loss: 0.9309499859809875
Epoch 520, training loss: 12.660155296325684 = 0.49481531977653503 + 2.0 * 6.082670211791992
Epoch 520, val loss: 0.9238941073417664
Epoch 530, training loss: 12.63265323638916 = 0.4751490652561188 + 2.0 * 6.078752040863037
Epoch 530, val loss: 0.9179624319076538
Epoch 540, training loss: 12.609308242797852 = 0.45625123381614685 + 2.0 * 6.076528549194336
Epoch 540, val loss: 0.9129346013069153
Epoch 550, training loss: 12.587286949157715 = 0.43793463706970215 + 2.0 * 6.074676036834717
Epoch 550, val loss: 0.9087166786193848
Epoch 560, training loss: 12.575439453125 = 0.4201578199863434 + 2.0 * 6.077641010284424
Epoch 560, val loss: 0.9051195979118347
Epoch 570, training loss: 12.546639442443848 = 0.40284091234207153 + 2.0 * 6.0718994140625
Epoch 570, val loss: 0.9020714163780212
Epoch 580, training loss: 12.525365829467773 = 0.38590991497039795 + 2.0 * 6.069727897644043
Epoch 580, val loss: 0.8996237516403198
Epoch 590, training loss: 12.505526542663574 = 0.3692324161529541 + 2.0 * 6.0681471824646
Epoch 590, val loss: 0.8976283073425293
Epoch 600, training loss: 12.503606796264648 = 0.3528561592102051 + 2.0 * 6.075375556945801
Epoch 600, val loss: 0.8960894346237183
Epoch 610, training loss: 12.468923568725586 = 0.33679839968681335 + 2.0 * 6.0660624504089355
Epoch 610, val loss: 0.8949021100997925
Epoch 620, training loss: 12.450545310974121 = 0.3211929202079773 + 2.0 * 6.064676284790039
Epoch 620, val loss: 0.8943127393722534
Epoch 630, training loss: 12.441581726074219 = 0.3059985935688019 + 2.0 * 6.06779146194458
Epoch 630, val loss: 0.8942257761955261
Epoch 640, training loss: 12.422125816345215 = 0.291340708732605 + 2.0 * 6.06539249420166
Epoch 640, val loss: 0.8946152329444885
Epoch 650, training loss: 12.399560928344727 = 0.2771573066711426 + 2.0 * 6.061202049255371
Epoch 650, val loss: 0.8954946398735046
Epoch 660, training loss: 12.380736351013184 = 0.2634226381778717 + 2.0 * 6.058656692504883
Epoch 660, val loss: 0.8968391418457031
Epoch 670, training loss: 12.38079833984375 = 0.2501012682914734 + 2.0 * 6.0653486251831055
Epoch 670, val loss: 0.8985942006111145
Epoch 680, training loss: 12.351914405822754 = 0.23737142980098724 + 2.0 * 6.057271480560303
Epoch 680, val loss: 0.90069580078125
Epoch 690, training loss: 12.338723182678223 = 0.22510956227779388 + 2.0 * 6.056807041168213
Epoch 690, val loss: 0.9031080007553101
Epoch 700, training loss: 12.323521614074707 = 0.2132776379585266 + 2.0 * 6.055121898651123
Epoch 700, val loss: 0.9059286117553711
Epoch 710, training loss: 12.308343887329102 = 0.20179975032806396 + 2.0 * 6.053272247314453
Epoch 710, val loss: 0.9090497493743896
Epoch 720, training loss: 12.294602394104004 = 0.19068235158920288 + 2.0 * 6.051959991455078
Epoch 720, val loss: 0.912412166595459
Epoch 730, training loss: 12.288643836975098 = 0.17994293570518494 + 2.0 * 6.05435037612915
Epoch 730, val loss: 0.9159719347953796
Epoch 740, training loss: 12.27826976776123 = 0.1696397364139557 + 2.0 * 6.054315090179443
Epoch 740, val loss: 0.9197316765785217
Epoch 750, training loss: 12.260224342346191 = 0.15980972349643707 + 2.0 * 6.050207138061523
Epoch 750, val loss: 0.9236764907836914
Epoch 760, training loss: 12.24775505065918 = 0.15044046938419342 + 2.0 * 6.048657417297363
Epoch 760, val loss: 0.9278901219367981
Epoch 770, training loss: 12.236340522766113 = 0.14153169095516205 + 2.0 * 6.0474042892456055
Epoch 770, val loss: 0.9323844909667969
Epoch 780, training loss: 12.243292808532715 = 0.13309623301029205 + 2.0 * 6.055098056793213
Epoch 780, val loss: 0.9371048808097839
Epoch 790, training loss: 12.218476295471191 = 0.12525077164173126 + 2.0 * 6.046612739562988
Epoch 790, val loss: 0.9419474005699158
Epoch 800, training loss: 12.211528778076172 = 0.11790428310632706 + 2.0 * 6.046812057495117
Epoch 800, val loss: 0.947043776512146
Epoch 810, training loss: 12.198111534118652 = 0.11106506735086441 + 2.0 * 6.04352331161499
Epoch 810, val loss: 0.9523757100105286
Epoch 820, training loss: 12.193778991699219 = 0.104704849421978 + 2.0 * 6.04453706741333
Epoch 820, val loss: 0.9579486846923828
Epoch 830, training loss: 12.188994407653809 = 0.09881474822759628 + 2.0 * 6.0450897216796875
Epoch 830, val loss: 0.9636955857276917
Epoch 840, training loss: 12.176431655883789 = 0.09335777163505554 + 2.0 * 6.041536808013916
Epoch 840, val loss: 0.9695317149162292
Epoch 850, training loss: 12.168571472167969 = 0.08832813799381256 + 2.0 * 6.040121555328369
Epoch 850, val loss: 0.9755733013153076
Epoch 860, training loss: 12.163575172424316 = 0.08365880697965622 + 2.0 * 6.0399580001831055
Epoch 860, val loss: 0.9817625880241394
Epoch 870, training loss: 12.16166877746582 = 0.0793360099196434 + 2.0 * 6.041166305541992
Epoch 870, val loss: 0.9880765676498413
Epoch 880, training loss: 12.156412124633789 = 0.07537911832332611 + 2.0 * 6.040516376495361
Epoch 880, val loss: 0.9944426417350769
Epoch 890, training loss: 12.14744758605957 = 0.07169916480779648 + 2.0 * 6.037874221801758
Epoch 890, val loss: 1.0008097887039185
Epoch 900, training loss: 12.141284942626953 = 0.06828118115663528 + 2.0 * 6.036501884460449
Epoch 900, val loss: 1.0073026418685913
Epoch 910, training loss: 12.149726867675781 = 0.06510861217975616 + 2.0 * 6.042309284210205
Epoch 910, val loss: 1.0139787197113037
Epoch 920, training loss: 12.133386611938477 = 0.06216184049844742 + 2.0 * 6.0356125831604
Epoch 920, val loss: 1.0205321311950684
Epoch 930, training loss: 12.128256797790527 = 0.059413399547338486 + 2.0 * 6.034421920776367
Epoch 930, val loss: 1.0271549224853516
Epoch 940, training loss: 12.125248908996582 = 0.056840136647224426 + 2.0 * 6.034204483032227
Epoch 940, val loss: 1.0337693691253662
Epoch 950, training loss: 12.128019332885742 = 0.0544312484562397 + 2.0 * 6.036794185638428
Epoch 950, val loss: 1.0404094457626343
Epoch 960, training loss: 12.12178897857666 = 0.05219254642724991 + 2.0 * 6.0347981452941895
Epoch 960, val loss: 1.047033429145813
Epoch 970, training loss: 12.113932609558105 = 0.05007987096905708 + 2.0 * 6.031926155090332
Epoch 970, val loss: 1.053641676902771
Epoch 980, training loss: 12.111489295959473 = 0.048102494329214096 + 2.0 * 6.031693458557129
Epoch 980, val loss: 1.060237169265747
Epoch 990, training loss: 12.114299774169922 = 0.046233344823122025 + 2.0 * 6.034033298492432
Epoch 990, val loss: 1.066835641860962
Epoch 1000, training loss: 12.109797477722168 = 0.04448595643043518 + 2.0 * 6.032655715942383
Epoch 1000, val loss: 1.073388934135437
Epoch 1010, training loss: 12.105137825012207 = 0.0428352952003479 + 2.0 * 6.031151294708252
Epoch 1010, val loss: 1.079906940460205
Epoch 1020, training loss: 12.09874439239502 = 0.04128823056817055 + 2.0 * 6.028728008270264
Epoch 1020, val loss: 1.0863616466522217
Epoch 1030, training loss: 12.09485912322998 = 0.03981250897049904 + 2.0 * 6.027523517608643
Epoch 1030, val loss: 1.0927903652191162
Epoch 1040, training loss: 12.093063354492188 = 0.03841983899474144 + 2.0 * 6.027321815490723
Epoch 1040, val loss: 1.0992660522460938
Epoch 1050, training loss: 12.098540306091309 = 0.03710096329450607 + 2.0 * 6.030719757080078
Epoch 1050, val loss: 1.1057063341140747
Epoch 1060, training loss: 12.096738815307617 = 0.03584456071257591 + 2.0 * 6.030447006225586
Epoch 1060, val loss: 1.1119904518127441
Epoch 1070, training loss: 12.089603424072266 = 0.03465747460722923 + 2.0 * 6.027472972869873
Epoch 1070, val loss: 1.118286371231079
Epoch 1080, training loss: 12.083067893981934 = 0.033535901457071304 + 2.0 * 6.024765968322754
Epoch 1080, val loss: 1.1245568990707397
Epoch 1090, training loss: 12.081599235534668 = 0.03246397152543068 + 2.0 * 6.024567604064941
Epoch 1090, val loss: 1.1307121515274048
Epoch 1100, training loss: 12.085441589355469 = 0.031446028500795364 + 2.0 * 6.0269975662231445
Epoch 1100, val loss: 1.1369400024414062
Epoch 1110, training loss: 12.080931663513184 = 0.03047201782464981 + 2.0 * 6.0252299308776855
Epoch 1110, val loss: 1.1429988145828247
Epoch 1120, training loss: 12.08174991607666 = 0.029550887644290924 + 2.0 * 6.026099681854248
Epoch 1120, val loss: 1.1490663290023804
Epoch 1130, training loss: 12.07558822631836 = 0.028670072555541992 + 2.0 * 6.023458957672119
Epoch 1130, val loss: 1.1550183296203613
Epoch 1140, training loss: 12.07107162475586 = 0.027826892212033272 + 2.0 * 6.021622180938721
Epoch 1140, val loss: 1.160890817642212
Epoch 1150, training loss: 12.068614959716797 = 0.027026228606700897 + 2.0 * 6.02079439163208
Epoch 1150, val loss: 1.1667720079421997
Epoch 1160, training loss: 12.069538116455078 = 0.026255177333950996 + 2.0 * 6.021641254425049
Epoch 1160, val loss: 1.172594428062439
Epoch 1170, training loss: 12.070643424987793 = 0.02551894448697567 + 2.0 * 6.022562026977539
Epoch 1170, val loss: 1.1784348487854004
Epoch 1180, training loss: 12.062224388122559 = 0.024817053228616714 + 2.0 * 6.018703460693359
Epoch 1180, val loss: 1.1841678619384766
Epoch 1190, training loss: 12.060982704162598 = 0.024142948910593987 + 2.0 * 6.0184197425842285
Epoch 1190, val loss: 1.1898146867752075
Epoch 1200, training loss: 12.063030242919922 = 0.02349734865128994 + 2.0 * 6.019766330718994
Epoch 1200, val loss: 1.195440411567688
Epoch 1210, training loss: 12.065546035766602 = 0.022876743227243423 + 2.0 * 6.021334648132324
Epoch 1210, val loss: 1.201011061668396
Epoch 1220, training loss: 12.062633514404297 = 0.022278212010860443 + 2.0 * 6.020177841186523
Epoch 1220, val loss: 1.2065200805664062
Epoch 1230, training loss: 12.056449890136719 = 0.021708521991968155 + 2.0 * 6.017370700836182
Epoch 1230, val loss: 1.2119288444519043
Epoch 1240, training loss: 12.064152717590332 = 0.02115752547979355 + 2.0 * 6.02149772644043
Epoch 1240, val loss: 1.2172181606292725
Epoch 1250, training loss: 12.053543090820312 = 0.020635487511754036 + 2.0 * 6.016453742980957
Epoch 1250, val loss: 1.2227052450180054
Epoch 1260, training loss: 12.050555229187012 = 0.020125528797507286 + 2.0 * 6.015214920043945
Epoch 1260, val loss: 1.2279000282287598
Epoch 1270, training loss: 12.052289962768555 = 0.01963883265852928 + 2.0 * 6.0163254737854
Epoch 1270, val loss: 1.2331551313400269
Epoch 1280, training loss: 12.0507173538208 = 0.019168322905898094 + 2.0 * 6.015774726867676
Epoch 1280, val loss: 1.2383743524551392
Epoch 1290, training loss: 12.04969310760498 = 0.018716180697083473 + 2.0 * 6.015488624572754
Epoch 1290, val loss: 1.243469476699829
Epoch 1300, training loss: 12.047576904296875 = 0.018279455602169037 + 2.0 * 6.014648914337158
Epoch 1300, val loss: 1.2484544515609741
Epoch 1310, training loss: 12.050777435302734 = 0.017859207466244698 + 2.0 * 6.016458988189697
Epoch 1310, val loss: 1.2534431219100952
Epoch 1320, training loss: 12.043214797973633 = 0.01745487190783024 + 2.0 * 6.012879848480225
Epoch 1320, val loss: 1.2584457397460938
Epoch 1330, training loss: 12.04334545135498 = 0.017063971608877182 + 2.0 * 6.013140678405762
Epoch 1330, val loss: 1.2633541822433472
Epoch 1340, training loss: 12.045849800109863 = 0.016685500741004944 + 2.0 * 6.01458215713501
Epoch 1340, val loss: 1.2682162523269653
Epoch 1350, training loss: 12.040825843811035 = 0.0163192767649889 + 2.0 * 6.012253284454346
Epoch 1350, val loss: 1.272995114326477
Epoch 1360, training loss: 12.047736167907715 = 0.015965403988957405 + 2.0 * 6.015885353088379
Epoch 1360, val loss: 1.2777611017227173
Epoch 1370, training loss: 12.037334442138672 = 0.015625378116965294 + 2.0 * 6.010854721069336
Epoch 1370, val loss: 1.282499074935913
Epoch 1380, training loss: 12.064874649047852 = 0.015295498073101044 + 2.0 * 6.024789810180664
Epoch 1380, val loss: 1.287156105041504
Epoch 1390, training loss: 12.040820121765137 = 0.014980902895331383 + 2.0 * 6.0129194259643555
Epoch 1390, val loss: 1.2917468547821045
Epoch 1400, training loss: 12.034948348999023 = 0.014673743396997452 + 2.0 * 6.01013708114624
Epoch 1400, val loss: 1.2961935997009277
Epoch 1410, training loss: 12.032746315002441 = 0.014377101324498653 + 2.0 * 6.009184837341309
Epoch 1410, val loss: 1.300641655921936
Epoch 1420, training loss: 12.030828475952148 = 0.014087531715631485 + 2.0 * 6.008370399475098
Epoch 1420, val loss: 1.3050894737243652
Epoch 1430, training loss: 12.034683227539062 = 0.013805252499878407 + 2.0 * 6.010438919067383
Epoch 1430, val loss: 1.309467077255249
Epoch 1440, training loss: 12.029966354370117 = 0.01353453379124403 + 2.0 * 6.00821590423584
Epoch 1440, val loss: 1.3140279054641724
Epoch 1450, training loss: 12.03265380859375 = 0.013271793723106384 + 2.0 * 6.00969123840332
Epoch 1450, val loss: 1.318224310874939
Epoch 1460, training loss: 12.027532577514648 = 0.013018614612519741 + 2.0 * 6.007256984710693
Epoch 1460, val loss: 1.3225175142288208
Epoch 1470, training loss: 12.026037216186523 = 0.012770578265190125 + 2.0 * 6.006633281707764
Epoch 1470, val loss: 1.3267295360565186
Epoch 1480, training loss: 12.048410415649414 = 0.012529623694717884 + 2.0 * 6.017940521240234
Epoch 1480, val loss: 1.3308509588241577
Epoch 1490, training loss: 12.034029006958008 = 0.012297049164772034 + 2.0 * 6.010866165161133
Epoch 1490, val loss: 1.3351505994796753
Epoch 1500, training loss: 12.023924827575684 = 0.012071196921169758 + 2.0 * 6.005926609039307
Epoch 1500, val loss: 1.3392322063446045
Epoch 1510, training loss: 12.037802696228027 = 0.011851739138364792 + 2.0 * 6.012975692749023
Epoch 1510, val loss: 1.3432068824768066
Epoch 1520, training loss: 12.024988174438477 = 0.01164062600582838 + 2.0 * 6.006673812866211
Epoch 1520, val loss: 1.3472628593444824
Epoch 1530, training loss: 12.023234367370605 = 0.011434011161327362 + 2.0 * 6.0059003829956055
Epoch 1530, val loss: 1.3512012958526611
Epoch 1540, training loss: 12.02101993560791 = 0.011233489029109478 + 2.0 * 6.0048933029174805
Epoch 1540, val loss: 1.355135440826416
Epoch 1550, training loss: 12.020679473876953 = 0.01103756669908762 + 2.0 * 6.004820823669434
Epoch 1550, val loss: 1.3590693473815918
Epoch 1560, training loss: 12.035399436950684 = 0.010848445817828178 + 2.0 * 6.012275695800781
Epoch 1560, val loss: 1.3629884719848633
Epoch 1570, training loss: 12.021848678588867 = 0.010660368017852306 + 2.0 * 6.005594253540039
Epoch 1570, val loss: 1.3668677806854248
Epoch 1580, training loss: 12.018938064575195 = 0.010481487028300762 + 2.0 * 6.004228115081787
Epoch 1580, val loss: 1.3706480264663696
Epoch 1590, training loss: 12.029431343078613 = 0.010305771604180336 + 2.0 * 6.009562969207764
Epoch 1590, val loss: 1.3743375539779663
Epoch 1600, training loss: 12.018141746520996 = 0.010136925615370274 + 2.0 * 6.004002571105957
Epoch 1600, val loss: 1.378147840499878
Epoch 1610, training loss: 12.01606273651123 = 0.009970198385417461 + 2.0 * 6.003046035766602
Epoch 1610, val loss: 1.3817907571792603
Epoch 1620, training loss: 12.025846481323242 = 0.009809939190745354 + 2.0 * 6.008018493652344
Epoch 1620, val loss: 1.3855174779891968
Epoch 1630, training loss: 12.014602661132812 = 0.0096511235460639 + 2.0 * 6.002475738525391
Epoch 1630, val loss: 1.389110803604126
Epoch 1640, training loss: 12.012392044067383 = 0.009497451595962048 + 2.0 * 6.0014472007751465
Epoch 1640, val loss: 1.3926987648010254
Epoch 1650, training loss: 12.014420509338379 = 0.009348186664283276 + 2.0 * 6.002536296844482
Epoch 1650, val loss: 1.3963000774383545
Epoch 1660, training loss: 12.020501136779785 = 0.009201828390359879 + 2.0 * 6.005649566650391
Epoch 1660, val loss: 1.3998645544052124
Epoch 1670, training loss: 12.021982192993164 = 0.00905870832502842 + 2.0 * 6.0064616203308105
Epoch 1670, val loss: 1.4033052921295166
Epoch 1680, training loss: 12.011336326599121 = 0.008921469561755657 + 2.0 * 6.00120735168457
Epoch 1680, val loss: 1.4067946672439575
Epoch 1690, training loss: 12.009541511535645 = 0.008786946535110474 + 2.0 * 6.000377178192139
Epoch 1690, val loss: 1.4102550745010376
Epoch 1700, training loss: 12.009904861450195 = 0.00865506287664175 + 2.0 * 6.000625133514404
Epoch 1700, val loss: 1.41365647315979
Epoch 1710, training loss: 12.020493507385254 = 0.008527222089469433 + 2.0 * 6.005983352661133
Epoch 1710, val loss: 1.4170949459075928
Epoch 1720, training loss: 12.008947372436523 = 0.008399790152907372 + 2.0 * 6.000273704528809
Epoch 1720, val loss: 1.4204378128051758
Epoch 1730, training loss: 12.006769180297852 = 0.008277151733636856 + 2.0 * 5.999246120452881
Epoch 1730, val loss: 1.4238078594207764
Epoch 1740, training loss: 12.020456314086914 = 0.008157607167959213 + 2.0 * 6.0061492919921875
Epoch 1740, val loss: 1.4270117282867432
Epoch 1750, training loss: 12.009183883666992 = 0.008041083812713623 + 2.0 * 6.000571250915527
Epoch 1750, val loss: 1.4303845167160034
Epoch 1760, training loss: 12.005764961242676 = 0.00792661402374506 + 2.0 * 5.9989190101623535
Epoch 1760, val loss: 1.4335839748382568
Epoch 1770, training loss: 12.016729354858398 = 0.007814227603375912 + 2.0 * 6.004457473754883
Epoch 1770, val loss: 1.4367055892944336
Epoch 1780, training loss: 12.006339073181152 = 0.007706891745328903 + 2.0 * 5.999316215515137
Epoch 1780, val loss: 1.440049648284912
Epoch 1790, training loss: 12.003304481506348 = 0.007599768694490194 + 2.0 * 5.997852325439453
Epoch 1790, val loss: 1.4431248903274536
Epoch 1800, training loss: 12.002275466918945 = 0.007496146485209465 + 2.0 * 5.997389793395996
Epoch 1800, val loss: 1.4463242292404175
Epoch 1810, training loss: 12.010424613952637 = 0.007394514512270689 + 2.0 * 6.001514911651611
Epoch 1810, val loss: 1.4494587182998657
Epoch 1820, training loss: 12.008232116699219 = 0.00729383947327733 + 2.0 * 6.000469207763672
Epoch 1820, val loss: 1.4525418281555176
Epoch 1830, training loss: 12.000284194946289 = 0.007197578903287649 + 2.0 * 5.9965434074401855
Epoch 1830, val loss: 1.4556668996810913
Epoch 1840, training loss: 11.999375343322754 = 0.007102637551724911 + 2.0 * 5.99613618850708
Epoch 1840, val loss: 1.4585952758789062
Epoch 1850, training loss: 11.99853515625 = 0.007009624037891626 + 2.0 * 5.995762825012207
Epoch 1850, val loss: 1.4616193771362305
Epoch 1860, training loss: 11.999459266662598 = 0.006917919032275677 + 2.0 * 5.996270656585693
Epoch 1860, val loss: 1.46461021900177
Epoch 1870, training loss: 12.012476921081543 = 0.006828200072050095 + 2.0 * 6.002824306488037
Epoch 1870, val loss: 1.467591404914856
Epoch 1880, training loss: 12.00674057006836 = 0.006741080898791552 + 2.0 * 5.999999523162842
Epoch 1880, val loss: 1.4706255197525024
Epoch 1890, training loss: 11.999388694763184 = 0.006655976176261902 + 2.0 * 5.996366500854492
Epoch 1890, val loss: 1.4734737873077393
Epoch 1900, training loss: 11.996881484985352 = 0.006572362966835499 + 2.0 * 5.99515438079834
Epoch 1900, val loss: 1.47633957862854
Epoch 1910, training loss: 11.99571704864502 = 0.006489933934062719 + 2.0 * 5.9946136474609375
Epoch 1910, val loss: 1.4791810512542725
Epoch 1920, training loss: 12.011873245239258 = 0.006409220863133669 + 2.0 * 6.002731800079346
Epoch 1920, val loss: 1.4820606708526611
Epoch 1930, training loss: 11.999763488769531 = 0.00633092550560832 + 2.0 * 5.996716499328613
Epoch 1930, val loss: 1.484938383102417
Epoch 1940, training loss: 11.998725891113281 = 0.0062539479695260525 + 2.0 * 5.9962358474731445
Epoch 1940, val loss: 1.4877688884735107
Epoch 1950, training loss: 12.00161075592041 = 0.006179284770041704 + 2.0 * 5.997715950012207
Epoch 1950, val loss: 1.4905190467834473
Epoch 1960, training loss: 11.999287605285645 = 0.006104941945523024 + 2.0 * 5.996591567993164
Epoch 1960, val loss: 1.4931433200836182
Epoch 1970, training loss: 11.99366283416748 = 0.006031477358192205 + 2.0 * 5.993815898895264
Epoch 1970, val loss: 1.495850682258606
Epoch 1980, training loss: 11.993086814880371 = 0.005960895214229822 + 2.0 * 5.993563175201416
Epoch 1980, val loss: 1.4986522197723389
Epoch 1990, training loss: 11.992182731628418 = 0.005890954751521349 + 2.0 * 5.993145942687988
Epoch 1990, val loss: 1.5013395547866821
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7815
Overall ASR: 0.6236
Flip ASR: 0.5511/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.694631576538086 = 1.9468070268630981 + 2.0 * 8.37391185760498
Epoch 0, val loss: 1.9450706243515015
Epoch 10, training loss: 18.683147430419922 = 1.936158537864685 + 2.0 * 8.373494148254395
Epoch 10, val loss: 1.9335119724273682
Epoch 20, training loss: 18.66297149658203 = 1.9231173992156982 + 2.0 * 8.369927406311035
Epoch 20, val loss: 1.919169545173645
Epoch 30, training loss: 18.59507179260254 = 1.9057657718658447 + 2.0 * 8.344653129577637
Epoch 30, val loss: 1.8997613191604614
Epoch 40, training loss: 18.291606903076172 = 1.8845610618591309 + 2.0 * 8.203522682189941
Epoch 40, val loss: 1.876330018043518
Epoch 50, training loss: 17.44342613220215 = 1.8617266416549683 + 2.0 * 7.790849685668945
Epoch 50, val loss: 1.8514951467514038
Epoch 60, training loss: 17.04297637939453 = 1.8390092849731445 + 2.0 * 7.601984024047852
Epoch 60, val loss: 1.8284096717834473
Epoch 70, training loss: 16.289899826049805 = 1.8180948495864868 + 2.0 * 7.235902786254883
Epoch 70, val loss: 1.8071167469024658
Epoch 80, training loss: 15.666956901550293 = 1.7958612442016602 + 2.0 * 6.935547828674316
Epoch 80, val loss: 1.785011649131775
Epoch 90, training loss: 15.26579761505127 = 1.7757394313812256 + 2.0 * 6.745028972625732
Epoch 90, val loss: 1.7654236555099487
Epoch 100, training loss: 15.064748764038086 = 1.756386160850525 + 2.0 * 6.654181480407715
Epoch 100, val loss: 1.7467886209487915
Epoch 110, training loss: 14.88404369354248 = 1.7352440357208252 + 2.0 * 6.574399948120117
Epoch 110, val loss: 1.7273387908935547
Epoch 120, training loss: 14.7194242477417 = 1.7143449783325195 + 2.0 * 6.50253963470459
Epoch 120, val loss: 1.7084249258041382
Epoch 130, training loss: 14.5834379196167 = 1.6926804780960083 + 2.0 * 6.44537878036499
Epoch 130, val loss: 1.6892341375350952
Epoch 140, training loss: 14.47218132019043 = 1.6684565544128418 + 2.0 * 6.401862144470215
Epoch 140, val loss: 1.6679073572158813
Epoch 150, training loss: 14.376485824584961 = 1.6411547660827637 + 2.0 * 6.3676652908325195
Epoch 150, val loss: 1.6443746089935303
Epoch 160, training loss: 14.269083023071289 = 1.6114486455917358 + 2.0 * 6.328817367553711
Epoch 160, val loss: 1.6192820072174072
Epoch 170, training loss: 14.176121711730957 = 1.5796111822128296 + 2.0 * 6.298255443572998
Epoch 170, val loss: 1.5923972129821777
Epoch 180, training loss: 14.095660209655762 = 1.5448732376098633 + 2.0 * 6.275393486022949
Epoch 180, val loss: 1.5634050369262695
Epoch 190, training loss: 14.01995849609375 = 1.5074974298477173 + 2.0 * 6.256230354309082
Epoch 190, val loss: 1.5327894687652588
Epoch 200, training loss: 13.949764251708984 = 1.4684083461761475 + 2.0 * 6.240677833557129
Epoch 200, val loss: 1.5008769035339355
Epoch 210, training loss: 13.890401840209961 = 1.4279149770736694 + 2.0 * 6.23124361038208
Epoch 210, val loss: 1.468589186668396
Epoch 220, training loss: 13.821395874023438 = 1.387601375579834 + 2.0 * 6.216897487640381
Epoch 220, val loss: 1.4367731809616089
Epoch 230, training loss: 13.761346817016602 = 1.347762107849121 + 2.0 * 6.20679235458374
Epoch 230, val loss: 1.4060744047164917
Epoch 240, training loss: 13.705766677856445 = 1.3088401556015015 + 2.0 * 6.198463439941406
Epoch 240, val loss: 1.3767465353012085
Epoch 250, training loss: 13.660506248474121 = 1.2711883783340454 + 2.0 * 6.1946587562561035
Epoch 250, val loss: 1.3489654064178467
Epoch 260, training loss: 13.604670524597168 = 1.235429048538208 + 2.0 * 6.1846208572387695
Epoch 260, val loss: 1.3228434324264526
Epoch 270, training loss: 13.555837631225586 = 1.2007875442504883 + 2.0 * 6.177525043487549
Epoch 270, val loss: 1.2979081869125366
Epoch 280, training loss: 13.509333610534668 = 1.1667377948760986 + 2.0 * 6.171298027038574
Epoch 280, val loss: 1.273526906967163
Epoch 290, training loss: 13.47049617767334 = 1.1326807737350464 + 2.0 * 6.168907642364502
Epoch 290, val loss: 1.249265193939209
Epoch 300, training loss: 13.422208786010742 = 1.098926067352295 + 2.0 * 6.1616411209106445
Epoch 300, val loss: 1.2249565124511719
Epoch 310, training loss: 13.3789644241333 = 1.0649611949920654 + 2.0 * 6.157001495361328
Epoch 310, val loss: 1.2005075216293335
Epoch 320, training loss: 13.335590362548828 = 1.0306202173233032 + 2.0 * 6.152484893798828
Epoch 320, val loss: 1.1757347583770752
Epoch 330, training loss: 13.29101276397705 = 0.9956073760986328 + 2.0 * 6.147702693939209
Epoch 330, val loss: 1.15045964717865
Epoch 340, training loss: 13.2561616897583 = 0.9600445628166199 + 2.0 * 6.1480584144592285
Epoch 340, val loss: 1.124721884727478
Epoch 350, training loss: 13.208772659301758 = 0.9247437119483948 + 2.0 * 6.142014503479004
Epoch 350, val loss: 1.098984718322754
Epoch 360, training loss: 13.164892196655273 = 0.8896384835243225 + 2.0 * 6.137626647949219
Epoch 360, val loss: 1.073432207107544
Epoch 370, training loss: 13.122516632080078 = 0.8549803495407104 + 2.0 * 6.133768081665039
Epoch 370, val loss: 1.0482839345932007
Epoch 380, training loss: 13.084208488464355 = 0.8211634159088135 + 2.0 * 6.1315226554870605
Epoch 380, val loss: 1.0237945318222046
Epoch 390, training loss: 13.045858383178711 = 0.7886210680007935 + 2.0 * 6.1286187171936035
Epoch 390, val loss: 1.0003490447998047
Epoch 400, training loss: 13.007736206054688 = 0.7572588324546814 + 2.0 * 6.12523889541626
Epoch 400, val loss: 0.9780485033988953
Epoch 410, training loss: 12.971536636352539 = 0.727088987827301 + 2.0 * 6.122223854064941
Epoch 410, val loss: 0.9568612575531006
Epoch 420, training loss: 12.942522048950195 = 0.698168933391571 + 2.0 * 6.122176647186279
Epoch 420, val loss: 0.9369021058082581
Epoch 430, training loss: 12.910770416259766 = 0.6706015467643738 + 2.0 * 6.120084285736084
Epoch 430, val loss: 0.9181729555130005
Epoch 440, training loss: 12.87501335144043 = 0.6443840861320496 + 2.0 * 6.115314483642578
Epoch 440, val loss: 0.9008733630180359
Epoch 450, training loss: 12.844313621520996 = 0.6192353367805481 + 2.0 * 6.112539291381836
Epoch 450, val loss: 0.8847567439079285
Epoch 460, training loss: 12.81614875793457 = 0.5950032472610474 + 2.0 * 6.110572814941406
Epoch 460, val loss: 0.8697322607040405
Epoch 470, training loss: 12.794807434082031 = 0.5716138482093811 + 2.0 * 6.111596584320068
Epoch 470, val loss: 0.8556522727012634
Epoch 480, training loss: 12.77009391784668 = 0.5491542816162109 + 2.0 * 6.110469818115234
Epoch 480, val loss: 0.8425682187080383
Epoch 490, training loss: 12.735864639282227 = 0.5274551510810852 + 2.0 * 6.1042046546936035
Epoch 490, val loss: 0.8305085301399231
Epoch 500, training loss: 12.710301399230957 = 0.506499171257019 + 2.0 * 6.101901054382324
Epoch 500, val loss: 0.8194018006324768
Epoch 510, training loss: 12.685723304748535 = 0.4861467480659485 + 2.0 * 6.099788188934326
Epoch 510, val loss: 0.8091033101081848
Epoch 520, training loss: 12.66593074798584 = 0.46633267402648926 + 2.0 * 6.099799156188965
Epoch 520, val loss: 0.799582302570343
Epoch 530, training loss: 12.659342765808105 = 0.44704174995422363 + 2.0 * 6.1061506271362305
Epoch 530, val loss: 0.7907756567001343
Epoch 540, training loss: 12.618531227111816 = 0.4286190867424011 + 2.0 * 6.094955921173096
Epoch 540, val loss: 0.7828850150108337
Epoch 550, training loss: 12.597322463989258 = 0.41077378392219543 + 2.0 * 6.093274116516113
Epoch 550, val loss: 0.7758166193962097
Epoch 560, training loss: 12.575971603393555 = 0.39340996742248535 + 2.0 * 6.091280937194824
Epoch 560, val loss: 0.7694588303565979
Epoch 570, training loss: 12.570955276489258 = 0.37649020552635193 + 2.0 * 6.097232341766357
Epoch 570, val loss: 0.7637903094291687
Epoch 580, training loss: 12.538790702819824 = 0.36027878522872925 + 2.0 * 6.0892558097839355
Epoch 580, val loss: 0.7586730718612671
Epoch 590, training loss: 12.518746376037598 = 0.3445517122745514 + 2.0 * 6.08709716796875
Epoch 590, val loss: 0.7543489336967468
Epoch 600, training loss: 12.500008583068848 = 0.32932713627815247 + 2.0 * 6.08534049987793
Epoch 600, val loss: 0.7507328987121582
Epoch 610, training loss: 12.485796928405762 = 0.3146384358406067 + 2.0 * 6.0855793952941895
Epoch 610, val loss: 0.7477380633354187
Epoch 620, training loss: 12.464254379272461 = 0.30047696828842163 + 2.0 * 6.081888675689697
Epoch 620, val loss: 0.7453895211219788
Epoch 630, training loss: 12.446514129638672 = 0.2868650257587433 + 2.0 * 6.079824447631836
Epoch 630, val loss: 0.74369215965271
Epoch 640, training loss: 12.431417465209961 = 0.2737552523612976 + 2.0 * 6.078831195831299
Epoch 640, val loss: 0.7425803542137146
Epoch 650, training loss: 12.423057556152344 = 0.26119452714920044 + 2.0 * 6.080931663513184
Epoch 650, val loss: 0.7420316934585571
Epoch 660, training loss: 12.409406661987305 = 0.249222531914711 + 2.0 * 6.080091953277588
Epoch 660, val loss: 0.7419951558113098
Epoch 670, training loss: 12.389246940612793 = 0.23784273862838745 + 2.0 * 6.07570219039917
Epoch 670, val loss: 0.742567241191864
Epoch 680, training loss: 12.374013900756836 = 0.2270452231168747 + 2.0 * 6.073484420776367
Epoch 680, val loss: 0.7437064051628113
Epoch 690, training loss: 12.360313415527344 = 0.2167591005563736 + 2.0 * 6.07177734375
Epoch 690, val loss: 0.7453885674476624
Epoch 700, training loss: 12.367083549499512 = 0.20699189603328705 + 2.0 * 6.080045700073242
Epoch 700, val loss: 0.7475141882896423
Epoch 710, training loss: 12.334848403930664 = 0.1976509392261505 + 2.0 * 6.068598747253418
Epoch 710, val loss: 0.7500829696655273
Epoch 720, training loss: 12.32659912109375 = 0.18884144723415375 + 2.0 * 6.068878650665283
Epoch 720, val loss: 0.7530900239944458
Epoch 730, training loss: 12.320978164672852 = 0.18050435185432434 + 2.0 * 6.070236682891846
Epoch 730, val loss: 0.7564810514450073
Epoch 740, training loss: 12.306042671203613 = 0.1725974678993225 + 2.0 * 6.066722393035889
Epoch 740, val loss: 0.7601227164268494
Epoch 750, training loss: 12.294974327087402 = 0.16509172320365906 + 2.0 * 6.06494140625
Epoch 750, val loss: 0.7642173767089844
Epoch 760, training loss: 12.293413162231445 = 0.15797971189022064 + 2.0 * 6.067716598510742
Epoch 760, val loss: 0.7685508131980896
Epoch 770, training loss: 12.275433540344238 = 0.15120941400527954 + 2.0 * 6.062111854553223
Epoch 770, val loss: 0.7731494307518005
Epoch 780, training loss: 12.26535415649414 = 0.14481157064437866 + 2.0 * 6.060271263122559
Epoch 780, val loss: 0.7779968976974487
Epoch 790, training loss: 12.259671211242676 = 0.13872243463993073 + 2.0 * 6.060474395751953
Epoch 790, val loss: 0.7829985618591309
Epoch 800, training loss: 12.249801635742188 = 0.13297048211097717 + 2.0 * 6.058415412902832
Epoch 800, val loss: 0.7881516814231873
Epoch 810, training loss: 12.244571685791016 = 0.12751714885234833 + 2.0 * 6.05852746963501
Epoch 810, val loss: 0.7934597730636597
Epoch 820, training loss: 12.237222671508789 = 0.12233932316303253 + 2.0 * 6.057441711425781
Epoch 820, val loss: 0.7989406585693359
Epoch 830, training loss: 12.236004829406738 = 0.11741296947002411 + 2.0 * 6.059296131134033
Epoch 830, val loss: 0.8044851422309875
Epoch 840, training loss: 12.221668243408203 = 0.11276417970657349 + 2.0 * 6.054451942443848
Epoch 840, val loss: 0.8101027607917786
Epoch 850, training loss: 12.214248657226562 = 0.10836248099803925 + 2.0 * 6.052943229675293
Epoch 850, val loss: 0.8158694505691528
Epoch 860, training loss: 12.209547996520996 = 0.10417443513870239 + 2.0 * 6.05268669128418
Epoch 860, val loss: 0.8216712474822998
Epoch 870, training loss: 12.210445404052734 = 0.1001976802945137 + 2.0 * 6.055123805999756
Epoch 870, val loss: 0.8276011347770691
Epoch 880, training loss: 12.199893951416016 = 0.09639684855937958 + 2.0 * 6.051748752593994
Epoch 880, val loss: 0.8334181308746338
Epoch 890, training loss: 12.193103790283203 = 0.09280938655138016 + 2.0 * 6.05014705657959
Epoch 890, val loss: 0.8393697142601013
Epoch 900, training loss: 12.185997009277344 = 0.08936595916748047 + 2.0 * 6.048315525054932
Epoch 900, val loss: 0.8453671932220459
Epoch 910, training loss: 12.186820030212402 = 0.08605828881263733 + 2.0 * 6.050380706787109
Epoch 910, val loss: 0.8513819575309753
Epoch 920, training loss: 12.186179161071777 = 0.0828666090965271 + 2.0 * 6.051656246185303
Epoch 920, val loss: 0.8573498129844666
Epoch 930, training loss: 12.172863960266113 = 0.07980484515428543 + 2.0 * 6.046529769897461
Epoch 930, val loss: 0.8633103370666504
Epoch 940, training loss: 12.168323516845703 = 0.07685978710651398 + 2.0 * 6.045732021331787
Epoch 940, val loss: 0.8692598938941956
Epoch 950, training loss: 12.17815113067627 = 0.07404026389122009 + 2.0 * 6.052055358886719
Epoch 950, val loss: 0.8751947283744812
Epoch 960, training loss: 12.167534828186035 = 0.07134491205215454 + 2.0 * 6.048094749450684
Epoch 960, val loss: 0.8811535835266113
Epoch 970, training loss: 12.156392097473145 = 0.06878881901502609 + 2.0 * 6.043801784515381
Epoch 970, val loss: 0.8870556354522705
Epoch 980, training loss: 12.151850700378418 = 0.0663580521941185 + 2.0 * 6.042746543884277
Epoch 980, val loss: 0.8929938077926636
Epoch 990, training loss: 12.148241996765137 = 0.06404116004705429 + 2.0 * 6.042100429534912
Epoch 990, val loss: 0.898906409740448
Epoch 1000, training loss: 12.146141052246094 = 0.061826035380363464 + 2.0 * 6.0421576499938965
Epoch 1000, val loss: 0.9047884941101074
Epoch 1010, training loss: 12.142853736877441 = 0.05970523878931999 + 2.0 * 6.041574478149414
Epoch 1010, val loss: 0.9106369614601135
Epoch 1020, training loss: 12.139963150024414 = 0.05768560990691185 + 2.0 * 6.041138648986816
Epoch 1020, val loss: 0.9163721203804016
Epoch 1030, training loss: 12.13258171081543 = 0.05575503036379814 + 2.0 * 6.0384135246276855
Epoch 1030, val loss: 0.922105610370636
Epoch 1040, training loss: 12.131867408752441 = 0.05391126126050949 + 2.0 * 6.038978099822998
Epoch 1040, val loss: 0.9277665615081787
Epoch 1050, training loss: 12.127925872802734 = 0.05214918404817581 + 2.0 * 6.037888526916504
Epoch 1050, val loss: 0.9333661198616028
Epoch 1060, training loss: 12.129720687866211 = 0.05046391487121582 + 2.0 * 6.039628505706787
Epoch 1060, val loss: 0.9388991594314575
Epoch 1070, training loss: 12.132542610168457 = 0.04884246736764908 + 2.0 * 6.0418500900268555
Epoch 1070, val loss: 0.9443304538726807
Epoch 1080, training loss: 12.120677947998047 = 0.04730594903230667 + 2.0 * 6.036685943603516
Epoch 1080, val loss: 0.9497489929199219
Epoch 1090, training loss: 12.115072250366211 = 0.045831743627786636 + 2.0 * 6.03462028503418
Epoch 1090, val loss: 0.9550157189369202
Epoch 1100, training loss: 12.111257553100586 = 0.0444096177816391 + 2.0 * 6.033423900604248
Epoch 1100, val loss: 0.9603635668754578
Epoch 1110, training loss: 12.115118026733398 = 0.0430377833545208 + 2.0 * 6.036040306091309
Epoch 1110, val loss: 0.9655585289001465
Epoch 1120, training loss: 12.111886024475098 = 0.041733209043741226 + 2.0 * 6.03507661819458
Epoch 1120, val loss: 0.9707820415496826
Epoch 1130, training loss: 12.105597496032715 = 0.040469855070114136 + 2.0 * 6.03256368637085
Epoch 1130, val loss: 0.9758868217468262
Epoch 1140, training loss: 12.103522300720215 = 0.039266981184482574 + 2.0 * 6.032127857208252
Epoch 1140, val loss: 0.9810957312583923
Epoch 1150, training loss: 12.10312557220459 = 0.03811247646808624 + 2.0 * 6.032506465911865
Epoch 1150, val loss: 0.9861494302749634
Epoch 1160, training loss: 12.100564956665039 = 0.03700706735253334 + 2.0 * 6.031778812408447
Epoch 1160, val loss: 0.9910850524902344
Epoch 1170, training loss: 12.095579147338867 = 0.03593618422746658 + 2.0 * 6.029821395874023
Epoch 1170, val loss: 0.9960874319076538
Epoch 1180, training loss: 12.092655181884766 = 0.03489183634519577 + 2.0 * 6.028881549835205
Epoch 1180, val loss: 1.001076579093933
Epoch 1190, training loss: 12.099370956420898 = 0.033882442861795425 + 2.0 * 6.032744407653809
Epoch 1190, val loss: 1.0060269832611084
Epoch 1200, training loss: 12.087669372558594 = 0.03292292729020119 + 2.0 * 6.027373313903809
Epoch 1200, val loss: 1.0107439756393433
Epoch 1210, training loss: 12.086719512939453 = 0.03200998529791832 + 2.0 * 6.027354717254639
Epoch 1210, val loss: 1.015593409538269
Epoch 1220, training loss: 12.09323787689209 = 0.03113926202058792 + 2.0 * 6.0310492515563965
Epoch 1220, val loss: 1.020345687866211
Epoch 1230, training loss: 12.08362865447998 = 0.030297663062810898 + 2.0 * 6.026665687561035
Epoch 1230, val loss: 1.0250234603881836
Epoch 1240, training loss: 12.079329490661621 = 0.02949083410203457 + 2.0 * 6.024919509887695
Epoch 1240, val loss: 1.0296790599822998
Epoch 1250, training loss: 12.084993362426758 = 0.02871101163327694 + 2.0 * 6.028141021728516
Epoch 1250, val loss: 1.0343652963638306
Epoch 1260, training loss: 12.081480026245117 = 0.027965262532234192 + 2.0 * 6.02675724029541
Epoch 1260, val loss: 1.038789987564087
Epoch 1270, training loss: 12.07775592803955 = 0.02723987028002739 + 2.0 * 6.0252580642700195
Epoch 1270, val loss: 1.0433039665222168
Epoch 1280, training loss: 12.07190227508545 = 0.026546094566583633 + 2.0 * 6.022677898406982
Epoch 1280, val loss: 1.0477757453918457
Epoch 1290, training loss: 12.071114540100098 = 0.025869468227028847 + 2.0 * 6.022622585296631
Epoch 1290, val loss: 1.0521501302719116
Epoch 1300, training loss: 12.095361709594727 = 0.025220517069101334 + 2.0 * 6.035070419311523
Epoch 1300, val loss: 1.0564707517623901
Epoch 1310, training loss: 12.07632827758789 = 0.024594468995928764 + 2.0 * 6.025866985321045
Epoch 1310, val loss: 1.0607949495315552
Epoch 1320, training loss: 12.066682815551758 = 0.023995665833353996 + 2.0 * 6.02134370803833
Epoch 1320, val loss: 1.0650147199630737
Epoch 1330, training loss: 12.066526412963867 = 0.023417552933096886 + 2.0 * 6.021554470062256
Epoch 1330, val loss: 1.0692014694213867
Epoch 1340, training loss: 12.075224876403809 = 0.02285686880350113 + 2.0 * 6.02618408203125
Epoch 1340, val loss: 1.0734035968780518
Epoch 1350, training loss: 12.064094543457031 = 0.022318271920084953 + 2.0 * 6.020888328552246
Epoch 1350, val loss: 1.0773478746414185
Epoch 1360, training loss: 12.060893058776855 = 0.021797042340040207 + 2.0 * 6.019547939300537
Epoch 1360, val loss: 1.0814988613128662
Epoch 1370, training loss: 12.060612678527832 = 0.021295538172125816 + 2.0 * 6.01965856552124
Epoch 1370, val loss: 1.0855767726898193
Epoch 1380, training loss: 12.064260482788086 = 0.020810281857848167 + 2.0 * 6.021725177764893
Epoch 1380, val loss: 1.0895949602127075
Epoch 1390, training loss: 12.066327095031738 = 0.020342722535133362 + 2.0 * 6.022992134094238
Epoch 1390, val loss: 1.0935640335083008
Epoch 1400, training loss: 12.057544708251953 = 0.019886929541826248 + 2.0 * 6.018828868865967
Epoch 1400, val loss: 1.0973236560821533
Epoch 1410, training loss: 12.05536937713623 = 0.019448645412921906 + 2.0 * 6.017960548400879
Epoch 1410, val loss: 1.1013227701187134
Epoch 1420, training loss: 12.056079864501953 = 0.01902574487030506 + 2.0 * 6.018527030944824
Epoch 1420, val loss: 1.1051312685012817
Epoch 1430, training loss: 12.055645942687988 = 0.018615609034895897 + 2.0 * 6.018515110015869
Epoch 1430, val loss: 1.1089057922363281
Epoch 1440, training loss: 12.052567481994629 = 0.018215911462903023 + 2.0 * 6.017175674438477
Epoch 1440, val loss: 1.1124942302703857
Epoch 1450, training loss: 12.052591323852539 = 0.017834754660725594 + 2.0 * 6.017378330230713
Epoch 1450, val loss: 1.1162837743759155
Epoch 1460, training loss: 12.04957103729248 = 0.017460782080888748 + 2.0 * 6.016055107116699
Epoch 1460, val loss: 1.1198575496673584
Epoch 1470, training loss: 12.046427726745605 = 0.017099447548389435 + 2.0 * 6.014664173126221
Epoch 1470, val loss: 1.1235318183898926
Epoch 1480, training loss: 12.057589530944824 = 0.01675066538155079 + 2.0 * 6.020419597625732
Epoch 1480, val loss: 1.1271113157272339
Epoch 1490, training loss: 12.046964645385742 = 0.016407089307904243 + 2.0 * 6.0152788162231445
Epoch 1490, val loss: 1.1306577920913696
Epoch 1500, training loss: 12.04711627960205 = 0.016078725457191467 + 2.0 * 6.015518665313721
Epoch 1500, val loss: 1.134098768234253
Epoch 1510, training loss: 12.0455322265625 = 0.0157607514411211 + 2.0 * 6.014885902404785
Epoch 1510, val loss: 1.1376079320907593
Epoch 1520, training loss: 12.043600082397461 = 0.01545235887169838 + 2.0 * 6.014073848724365
Epoch 1520, val loss: 1.141022801399231
Epoch 1530, training loss: 12.040300369262695 = 0.015152853913605213 + 2.0 * 6.012573719024658
Epoch 1530, val loss: 1.1444039344787598
Epoch 1540, training loss: 12.052019119262695 = 0.014863894321024418 + 2.0 * 6.018577575683594
Epoch 1540, val loss: 1.1477340459823608
Epoch 1550, training loss: 12.043699264526367 = 0.01458044070750475 + 2.0 * 6.014559268951416
Epoch 1550, val loss: 1.151023268699646
Epoch 1560, training loss: 12.03880500793457 = 0.014306014403700829 + 2.0 * 6.01224946975708
Epoch 1560, val loss: 1.1543657779693604
Epoch 1570, training loss: 12.039432525634766 = 0.01404107827693224 + 2.0 * 6.012695789337158
Epoch 1570, val loss: 1.1576957702636719
Epoch 1580, training loss: 12.042396545410156 = 0.013784104958176613 + 2.0 * 6.01430606842041
Epoch 1580, val loss: 1.1608517169952393
Epoch 1590, training loss: 12.034687042236328 = 0.013529586605727673 + 2.0 * 6.010578632354736
Epoch 1590, val loss: 1.1639331579208374
Epoch 1600, training loss: 12.034194946289062 = 0.013284928165376186 + 2.0 * 6.010455131530762
Epoch 1600, val loss: 1.1671522855758667
Epoch 1610, training loss: 12.0379638671875 = 0.013047345913946629 + 2.0 * 6.012458324432373
Epoch 1610, val loss: 1.170234203338623
Epoch 1620, training loss: 12.036252975463867 = 0.012817179784178734 + 2.0 * 6.011717796325684
Epoch 1620, val loss: 1.1734157800674438
Epoch 1630, training loss: 12.030943870544434 = 0.012592103332281113 + 2.0 * 6.009175777435303
Epoch 1630, val loss: 1.1763924360275269
Epoch 1640, training loss: 12.032186508178711 = 0.012373908422887325 + 2.0 * 6.00990629196167
Epoch 1640, val loss: 1.179456353187561
Epoch 1650, training loss: 12.039422035217285 = 0.01216356735676527 + 2.0 * 6.01362943649292
Epoch 1650, val loss: 1.1824544668197632
Epoch 1660, training loss: 12.031183242797852 = 0.011953404173254967 + 2.0 * 6.009614944458008
Epoch 1660, val loss: 1.1853998899459839
Epoch 1670, training loss: 12.027091026306152 = 0.011753316037356853 + 2.0 * 6.007668972015381
Epoch 1670, val loss: 1.188374638557434
Epoch 1680, training loss: 12.025716781616211 = 0.011559037491679192 + 2.0 * 6.007078647613525
Epoch 1680, val loss: 1.1913282871246338
Epoch 1690, training loss: 12.0256986618042 = 0.011367875151336193 + 2.0 * 6.007165431976318
Epoch 1690, val loss: 1.1942557096481323
Epoch 1700, training loss: 12.043298721313477 = 0.011181661859154701 + 2.0 * 6.016058444976807
Epoch 1700, val loss: 1.1970534324645996
Epoch 1710, training loss: 12.030305862426758 = 0.01100071705877781 + 2.0 * 6.009652614593506
Epoch 1710, val loss: 1.1999380588531494
Epoch 1720, training loss: 12.024763107299805 = 0.010823740623891354 + 2.0 * 6.006969451904297
Epoch 1720, val loss: 1.2026418447494507
Epoch 1730, training loss: 12.022346496582031 = 0.010653164237737656 + 2.0 * 6.0058465003967285
Epoch 1730, val loss: 1.2054927349090576
Epoch 1740, training loss: 12.022003173828125 = 0.010484942235052586 + 2.0 * 6.005759239196777
Epoch 1740, val loss: 1.20821213722229
Epoch 1750, training loss: 12.036478996276855 = 0.010320266708731651 + 2.0 * 6.0130791664123535
Epoch 1750, val loss: 1.2109099626541138
Epoch 1760, training loss: 12.03591537475586 = 0.010161161422729492 + 2.0 * 6.012876987457275
Epoch 1760, val loss: 1.213781714439392
Epoch 1770, training loss: 12.022501945495605 = 0.01000678539276123 + 2.0 * 6.006247520446777
Epoch 1770, val loss: 1.2163424491882324
Epoch 1780, training loss: 12.02155590057373 = 0.009855376556515694 + 2.0 * 6.005850315093994
Epoch 1780, val loss: 1.218937873840332
Epoch 1790, training loss: 12.018455505371094 = 0.009708944708108902 + 2.0 * 6.004373073577881
Epoch 1790, val loss: 1.2216328382492065
Epoch 1800, training loss: 12.018791198730469 = 0.009565010666847229 + 2.0 * 6.004612922668457
Epoch 1800, val loss: 1.2242587804794312
Epoch 1810, training loss: 12.023198127746582 = 0.009423620067536831 + 2.0 * 6.006887435913086
Epoch 1810, val loss: 1.226781964302063
Epoch 1820, training loss: 12.017118453979492 = 0.009285898879170418 + 2.0 * 6.003916263580322
Epoch 1820, val loss: 1.2293310165405273
Epoch 1830, training loss: 12.021595001220703 = 0.009151609614491463 + 2.0 * 6.006221771240234
Epoch 1830, val loss: 1.2318533658981323
Epoch 1840, training loss: 12.017412185668945 = 0.009020308963954449 + 2.0 * 6.0041961669921875
Epoch 1840, val loss: 1.234334945678711
Epoch 1850, training loss: 12.019133567810059 = 0.008893127553164959 + 2.0 * 6.005120277404785
Epoch 1850, val loss: 1.2367967367172241
Epoch 1860, training loss: 12.029730796813965 = 0.008768578991293907 + 2.0 * 6.010480880737305
Epoch 1860, val loss: 1.2391514778137207
Epoch 1870, training loss: 12.01760482788086 = 0.00864437036216259 + 2.0 * 6.004480361938477
Epoch 1870, val loss: 1.24155592918396
Epoch 1880, training loss: 12.013265609741211 = 0.008525286801159382 + 2.0 * 6.002370357513428
Epoch 1880, val loss: 1.243930697441101
Epoch 1890, training loss: 12.011547088623047 = 0.008409330621361732 + 2.0 * 6.001568794250488
Epoch 1890, val loss: 1.2463704347610474
Epoch 1900, training loss: 12.014544486999512 = 0.00829529669135809 + 2.0 * 6.003124713897705
Epoch 1900, val loss: 1.2487660646438599
Epoch 1910, training loss: 12.016769409179688 = 0.008183048106729984 + 2.0 * 6.004292964935303
Epoch 1910, val loss: 1.2510164976119995
Epoch 1920, training loss: 12.013306617736816 = 0.008073254488408566 + 2.0 * 6.002616882324219
Epoch 1920, val loss: 1.2532323598861694
Epoch 1930, training loss: 12.012083053588867 = 0.007967501878738403 + 2.0 * 6.0020575523376465
Epoch 1930, val loss: 1.2556034326553345
Epoch 1940, training loss: 12.021971702575684 = 0.007864114828407764 + 2.0 * 6.007053852081299
Epoch 1940, val loss: 1.2578471899032593
Epoch 1950, training loss: 12.013214111328125 = 0.007759591098874807 + 2.0 * 6.002727031707764
Epoch 1950, val loss: 1.2599583864212036
Epoch 1960, training loss: 12.00772476196289 = 0.007659726310521364 + 2.0 * 6.000032424926758
Epoch 1960, val loss: 1.2622153759002686
Epoch 1970, training loss: 12.006333351135254 = 0.007561940234154463 + 2.0 * 5.999385833740234
Epoch 1970, val loss: 1.2644155025482178
Epoch 1980, training loss: 12.009881973266602 = 0.0074657658115029335 + 2.0 * 6.001208305358887
Epoch 1980, val loss: 1.2666014432907104
Epoch 1990, training loss: 12.010220527648926 = 0.007371251937001944 + 2.0 * 6.001424789428711
Epoch 1990, val loss: 1.2686609029769897
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.2325
Flip ASR: 0.2222/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.696931838989258 = 1.9491181373596191 + 2.0 * 8.373907089233398
Epoch 0, val loss: 1.952520489692688
Epoch 10, training loss: 18.686580657958984 = 1.939500331878662 + 2.0 * 8.373539924621582
Epoch 10, val loss: 1.9429394006729126
Epoch 20, training loss: 18.66936492919922 = 1.927869439125061 + 2.0 * 8.370747566223145
Epoch 20, val loss: 1.9314302206039429
Epoch 30, training loss: 18.611215591430664 = 1.9120208024978638 + 2.0 * 8.349596977233887
Epoch 30, val loss: 1.916062831878662
Epoch 40, training loss: 18.328039169311523 = 1.8916118144989014 + 2.0 * 8.21821403503418
Epoch 40, val loss: 1.89681875705719
Epoch 50, training loss: 17.401371002197266 = 1.8676702976226807 + 2.0 * 7.766850471496582
Epoch 50, val loss: 1.8744778633117676
Epoch 60, training loss: 16.685440063476562 = 1.8443270921707153 + 2.0 * 7.42055606842041
Epoch 60, val loss: 1.8539615869522095
Epoch 70, training loss: 16.180736541748047 = 1.8264275789260864 + 2.0 * 7.177154541015625
Epoch 70, val loss: 1.8377642631530762
Epoch 80, training loss: 15.814242362976074 = 1.8098374605178833 + 2.0 * 7.00220251083374
Epoch 80, val loss: 1.8215527534484863
Epoch 90, training loss: 15.378060340881348 = 1.7918617725372314 + 2.0 * 6.793099403381348
Epoch 90, val loss: 1.804355502128601
Epoch 100, training loss: 15.06797981262207 = 1.77615225315094 + 2.0 * 6.645913600921631
Epoch 100, val loss: 1.7886942625045776
Epoch 110, training loss: 14.91928482055664 = 1.7586205005645752 + 2.0 * 6.580332279205322
Epoch 110, val loss: 1.7715578079223633
Epoch 120, training loss: 14.8014554977417 = 1.7382339239120483 + 2.0 * 6.53161096572876
Epoch 120, val loss: 1.752776026725769
Epoch 130, training loss: 14.696768760681152 = 1.7163972854614258 + 2.0 * 6.490185737609863
Epoch 130, val loss: 1.7334307432174683
Epoch 140, training loss: 14.581184387207031 = 1.6939072608947754 + 2.0 * 6.443638801574707
Epoch 140, val loss: 1.7138421535491943
Epoch 150, training loss: 14.468923568725586 = 1.6702232360839844 + 2.0 * 6.399350166320801
Epoch 150, val loss: 1.6936430931091309
Epoch 160, training loss: 14.363008499145508 = 1.643895149230957 + 2.0 * 6.359556674957275
Epoch 160, val loss: 1.6719882488250732
Epoch 170, training loss: 14.275840759277344 = 1.6142815351486206 + 2.0 * 6.330779552459717
Epoch 170, val loss: 1.648188829421997
Epoch 180, training loss: 14.19422721862793 = 1.58079195022583 + 2.0 * 6.306717872619629
Epoch 180, val loss: 1.6219199895858765
Epoch 190, training loss: 14.115621566772461 = 1.5440151691436768 + 2.0 * 6.285803318023682
Epoch 190, val loss: 1.5933300256729126
Epoch 200, training loss: 14.042234420776367 = 1.5040231943130493 + 2.0 * 6.269105434417725
Epoch 200, val loss: 1.562618374824524
Epoch 210, training loss: 13.962756156921387 = 1.4615066051483154 + 2.0 * 6.250624656677246
Epoch 210, val loss: 1.5301326513290405
Epoch 220, training loss: 13.8849515914917 = 1.416546106338501 + 2.0 * 6.234202861785889
Epoch 220, val loss: 1.495894193649292
Epoch 230, training loss: 13.810508728027344 = 1.3694689273834229 + 2.0 * 6.22052001953125
Epoch 230, val loss: 1.4604755640029907
Epoch 240, training loss: 13.738572120666504 = 1.321316123008728 + 2.0 * 6.208628177642822
Epoch 240, val loss: 1.4241886138916016
Epoch 250, training loss: 13.668451309204102 = 1.272197961807251 + 2.0 * 6.198126792907715
Epoch 250, val loss: 1.3873549699783325
Epoch 260, training loss: 13.601207733154297 = 1.2221816778182983 + 2.0 * 6.189513206481934
Epoch 260, val loss: 1.3501824140548706
Epoch 270, training loss: 13.541219711303711 = 1.1723049879074097 + 2.0 * 6.184457302093506
Epoch 270, val loss: 1.313436508178711
Epoch 280, training loss: 13.472728729248047 = 1.1229875087738037 + 2.0 * 6.174870491027832
Epoch 280, val loss: 1.2776650190353394
Epoch 290, training loss: 13.409482955932617 = 1.0741239786148071 + 2.0 * 6.167679309844971
Epoch 290, val loss: 1.2427700757980347
Epoch 300, training loss: 13.349233627319336 = 1.0256526470184326 + 2.0 * 6.161790370941162
Epoch 300, val loss: 1.2086104154586792
Epoch 310, training loss: 13.301077842712402 = 0.9778524041175842 + 2.0 * 6.161612510681152
Epoch 310, val loss: 1.175683617591858
Epoch 320, training loss: 13.240767478942871 = 0.9317338466644287 + 2.0 * 6.154516696929932
Epoch 320, val loss: 1.1445542573928833
Epoch 330, training loss: 13.182833671569824 = 0.8871744871139526 + 2.0 * 6.147829532623291
Epoch 330, val loss: 1.1153292655944824
Epoch 340, training loss: 13.13353443145752 = 0.84427809715271 + 2.0 * 6.144628047943115
Epoch 340, val loss: 1.088034987449646
Epoch 350, training loss: 13.083616256713867 = 0.8032545447349548 + 2.0 * 6.140181064605713
Epoch 350, val loss: 1.0627237558364868
Epoch 360, training loss: 13.035893440246582 = 0.7643405795097351 + 2.0 * 6.135776519775391
Epoch 360, val loss: 1.0396641492843628
Epoch 370, training loss: 12.991783142089844 = 0.7274429798126221 + 2.0 * 6.1321702003479
Epoch 370, val loss: 1.018709659576416
Epoch 380, training loss: 12.955296516418457 = 0.6926829218864441 + 2.0 * 6.1313066482543945
Epoch 380, val loss: 0.9997143745422363
Epoch 390, training loss: 12.911320686340332 = 0.659904420375824 + 2.0 * 6.125708103179932
Epoch 390, val loss: 0.9825575947761536
Epoch 400, training loss: 12.871426582336426 = 0.6291273236274719 + 2.0 * 6.12114953994751
Epoch 400, val loss: 0.9673191905021667
Epoch 410, training loss: 12.835919380187988 = 0.5999523401260376 + 2.0 * 6.117983341217041
Epoch 410, val loss: 0.953506350517273
Epoch 420, training loss: 12.812211036682129 = 0.5723298192024231 + 2.0 * 6.119940757751465
Epoch 420, val loss: 0.9410054087638855
Epoch 430, training loss: 12.776552200317383 = 0.5465205311775208 + 2.0 * 6.115015983581543
Epoch 430, val loss: 0.9298736453056335
Epoch 440, training loss: 12.742927551269531 = 0.5222058892250061 + 2.0 * 6.110360622406006
Epoch 440, val loss: 0.9200071096420288
Epoch 450, training loss: 12.715537071228027 = 0.4992760717868805 + 2.0 * 6.10813045501709
Epoch 450, val loss: 0.9112504720687866
Epoch 460, training loss: 12.686616897583008 = 0.477662593126297 + 2.0 * 6.1044769287109375
Epoch 460, val loss: 0.9034441709518433
Epoch 470, training loss: 12.660398483276367 = 0.4572339653968811 + 2.0 * 6.101582050323486
Epoch 470, val loss: 0.8966033458709717
Epoch 480, training loss: 12.638901710510254 = 0.43780675530433655 + 2.0 * 6.1005473136901855
Epoch 480, val loss: 0.8906427621841431
Epoch 490, training loss: 12.617130279541016 = 0.41937533020973206 + 2.0 * 6.098877429962158
Epoch 490, val loss: 0.8855494260787964
Epoch 500, training loss: 12.60016918182373 = 0.40187346935272217 + 2.0 * 6.099147796630859
Epoch 500, val loss: 0.8811677694320679
Epoch 510, training loss: 12.56939697265625 = 0.38526487350463867 + 2.0 * 6.092066287994385
Epoch 510, val loss: 0.8776051998138428
Epoch 520, training loss: 12.548418045043945 = 0.3694213330745697 + 2.0 * 6.089498519897461
Epoch 520, val loss: 0.8746669292449951
Epoch 530, training loss: 12.529834747314453 = 0.3542190194129944 + 2.0 * 6.087807655334473
Epoch 530, val loss: 0.8722859025001526
Epoch 540, training loss: 12.519285202026367 = 0.33961132168769836 + 2.0 * 6.089837074279785
Epoch 540, val loss: 0.8704130053520203
Epoch 550, training loss: 12.513564109802246 = 0.3256908059120178 + 2.0 * 6.093936443328857
Epoch 550, val loss: 0.8689205050468445
Epoch 560, training loss: 12.478541374206543 = 0.3123382329940796 + 2.0 * 6.083101749420166
Epoch 560, val loss: 0.867955207824707
Epoch 570, training loss: 12.459734916687012 = 0.29951363801956177 + 2.0 * 6.080110549926758
Epoch 570, val loss: 0.8675085306167603
Epoch 580, training loss: 12.44328498840332 = 0.28713804483413696 + 2.0 * 6.078073501586914
Epoch 580, val loss: 0.8674191236495972
Epoch 590, training loss: 12.432697296142578 = 0.27516308426856995 + 2.0 * 6.0787672996521
Epoch 590, val loss: 0.8676736354827881
Epoch 600, training loss: 12.422768592834473 = 0.2635982632637024 + 2.0 * 6.079585075378418
Epoch 600, val loss: 0.8682448267936707
Epoch 610, training loss: 12.399681091308594 = 0.2525240182876587 + 2.0 * 6.073578357696533
Epoch 610, val loss: 0.8690979480743408
Epoch 620, training loss: 12.386895179748535 = 0.24185225367546082 + 2.0 * 6.072521686553955
Epoch 620, val loss: 0.8703377842903137
Epoch 630, training loss: 12.374383926391602 = 0.23149648308753967 + 2.0 * 6.071443557739258
Epoch 630, val loss: 0.871848464012146
Epoch 640, training loss: 12.362238883972168 = 0.2214917689561844 + 2.0 * 6.07037353515625
Epoch 640, val loss: 0.8736794590950012
Epoch 650, training loss: 12.365093231201172 = 0.2118108570575714 + 2.0 * 6.076641082763672
Epoch 650, val loss: 0.8757508397102356
Epoch 660, training loss: 12.34481430053711 = 0.20261430740356445 + 2.0 * 6.071100234985352
Epoch 660, val loss: 0.8780322074890137
Epoch 670, training loss: 12.323726654052734 = 0.19369572401046753 + 2.0 * 6.0650153160095215
Epoch 670, val loss: 0.8805578351020813
Epoch 680, training loss: 12.312963485717773 = 0.1850697249174118 + 2.0 * 6.063946723937988
Epoch 680, val loss: 0.8833214640617371
Epoch 690, training loss: 12.300716400146484 = 0.17676714062690735 + 2.0 * 6.06197452545166
Epoch 690, val loss: 0.8862819671630859
Epoch 700, training loss: 12.299018859863281 = 0.16874703764915466 + 2.0 * 6.065135955810547
Epoch 700, val loss: 0.8893439769744873
Epoch 710, training loss: 12.289312362670898 = 0.16106611490249634 + 2.0 * 6.064123153686523
Epoch 710, val loss: 0.8927446007728577
Epoch 720, training loss: 12.272768020629883 = 0.15371061861515045 + 2.0 * 6.059528827667236
Epoch 720, val loss: 0.8961156010627747
Epoch 730, training loss: 12.261266708374023 = 0.1466565877199173 + 2.0 * 6.057304859161377
Epoch 730, val loss: 0.899692952632904
Epoch 740, training loss: 12.271631240844727 = 0.13987483084201813 + 2.0 * 6.065878391265869
Epoch 740, val loss: 0.9034746885299683
Epoch 750, training loss: 12.24704360961914 = 0.13344962894916534 + 2.0 * 6.056797027587891
Epoch 750, val loss: 0.9073570966720581
Epoch 760, training loss: 12.23831844329834 = 0.1273162066936493 + 2.0 * 6.0555009841918945
Epoch 760, val loss: 0.9113156795501709
Epoch 770, training loss: 12.226668357849121 = 0.12143496423959732 + 2.0 * 6.052616596221924
Epoch 770, val loss: 0.9154436588287354
Epoch 780, training loss: 12.225419998168945 = 0.11582012474536896 + 2.0 * 6.054800033569336
Epoch 780, val loss: 0.9197079539299011
Epoch 790, training loss: 12.216763496398926 = 0.11045768857002258 + 2.0 * 6.053153038024902
Epoch 790, val loss: 0.9240618348121643
Epoch 800, training loss: 12.21928882598877 = 0.10539199411869049 + 2.0 * 6.056948184967041
Epoch 800, val loss: 0.928593635559082
Epoch 810, training loss: 12.205339431762695 = 0.1005382239818573 + 2.0 * 6.052400588989258
Epoch 810, val loss: 0.9332041144371033
Epoch 820, training loss: 12.193926811218262 = 0.0959722101688385 + 2.0 * 6.048977375030518
Epoch 820, val loss: 0.9378865361213684
Epoch 830, training loss: 12.18523120880127 = 0.09159936755895615 + 2.0 * 6.046815872192383
Epoch 830, val loss: 0.9427549839019775
Epoch 840, training loss: 12.179128646850586 = 0.08745192736387253 + 2.0 * 6.045838356018066
Epoch 840, val loss: 0.9478044509887695
Epoch 850, training loss: 12.174117088317871 = 0.08350022882223129 + 2.0 * 6.045308589935303
Epoch 850, val loss: 0.9530040621757507
Epoch 860, training loss: 12.175106048583984 = 0.07974321395158768 + 2.0 * 6.0476813316345215
Epoch 860, val loss: 0.9582294821739197
Epoch 870, training loss: 12.168103218078613 = 0.07621601969003677 + 2.0 * 6.045943737030029
Epoch 870, val loss: 0.963500440120697
Epoch 880, training loss: 12.158857345581055 = 0.07287503778934479 + 2.0 * 6.0429911613464355
Epoch 880, val loss: 0.9688380360603333
Epoch 890, training loss: 12.153562545776367 = 0.06970017403364182 + 2.0 * 6.04193115234375
Epoch 890, val loss: 0.9743976593017578
Epoch 900, training loss: 12.157405853271484 = 0.06669481098651886 + 2.0 * 6.045355319976807
Epoch 900, val loss: 0.9799886345863342
Epoch 910, training loss: 12.152862548828125 = 0.06383450329303741 + 2.0 * 6.044514179229736
Epoch 910, val loss: 0.9855511784553528
Epoch 920, training loss: 12.141849517822266 = 0.061134472489356995 + 2.0 * 6.04035758972168
Epoch 920, val loss: 0.9912393093109131
Epoch 930, training loss: 12.137139320373535 = 0.05857071280479431 + 2.0 * 6.0392842292785645
Epoch 930, val loss: 0.9969288110733032
Epoch 940, training loss: 12.142064094543457 = 0.05614444613456726 + 2.0 * 6.042959690093994
Epoch 940, val loss: 1.0027642250061035
Epoch 950, training loss: 12.128935813903809 = 0.053853895515203476 + 2.0 * 6.037540912628174
Epoch 950, val loss: 1.00856351852417
Epoch 960, training loss: 12.125044822692871 = 0.051679085940122604 + 2.0 * 6.036683082580566
Epoch 960, val loss: 1.014385461807251
Epoch 970, training loss: 12.132039070129395 = 0.04960833489894867 + 2.0 * 6.041215419769287
Epoch 970, val loss: 1.020311713218689
Epoch 980, training loss: 12.12430477142334 = 0.0476563386619091 + 2.0 * 6.038324356079102
Epoch 980, val loss: 1.0261001586914062
Epoch 990, training loss: 12.118700981140137 = 0.04581039026379585 + 2.0 * 6.036445140838623
Epoch 990, val loss: 1.031989574432373
Epoch 1000, training loss: 12.111891746520996 = 0.04405348747968674 + 2.0 * 6.033919334411621
Epoch 1000, val loss: 1.0379606485366821
Epoch 1010, training loss: 12.109292030334473 = 0.0423872172832489 + 2.0 * 6.03345251083374
Epoch 1010, val loss: 1.0438965559005737
Epoch 1020, training loss: 12.11623764038086 = 0.040803875774145126 + 2.0 * 6.037716865539551
Epoch 1020, val loss: 1.049796223640442
Epoch 1030, training loss: 12.11064624786377 = 0.03931420296430588 + 2.0 * 6.035665988922119
Epoch 1030, val loss: 1.0556589365005493
Epoch 1040, training loss: 12.10361099243164 = 0.0378950834274292 + 2.0 * 6.032857894897461
Epoch 1040, val loss: 1.0615785121917725
Epoch 1050, training loss: 12.105230331420898 = 0.03655170649290085 + 2.0 * 6.034339427947998
Epoch 1050, val loss: 1.0674161911010742
Epoch 1060, training loss: 12.095141410827637 = 0.035264670848846436 + 2.0 * 6.029938220977783
Epoch 1060, val loss: 1.073344111442566
Epoch 1070, training loss: 12.094535827636719 = 0.03404879570007324 + 2.0 * 6.030243396759033
Epoch 1070, val loss: 1.0791503190994263
Epoch 1080, training loss: 12.100810050964355 = 0.032890498638153076 + 2.0 * 6.033959865570068
Epoch 1080, val loss: 1.0849891901016235
Epoch 1090, training loss: 12.092206954956055 = 0.031794801354408264 + 2.0 * 6.030206203460693
Epoch 1090, val loss: 1.0907866954803467
Epoch 1100, training loss: 12.08915901184082 = 0.03074704296886921 + 2.0 * 6.029205799102783
Epoch 1100, val loss: 1.0964562892913818
Epoch 1110, training loss: 12.08891487121582 = 0.02975161001086235 + 2.0 * 6.029581546783447
Epoch 1110, val loss: 1.1022415161132812
Epoch 1120, training loss: 12.085209846496582 = 0.02880374900996685 + 2.0 * 6.028203010559082
Epoch 1120, val loss: 1.1079943180084229
Epoch 1130, training loss: 12.085738182067871 = 0.027904676273465157 + 2.0 * 6.028916835784912
Epoch 1130, val loss: 1.1136832237243652
Epoch 1140, training loss: 12.078667640686035 = 0.027045350521802902 + 2.0 * 6.025811195373535
Epoch 1140, val loss: 1.1192163228988647
Epoch 1150, training loss: 12.075390815734863 = 0.026226595044136047 + 2.0 * 6.0245819091796875
Epoch 1150, val loss: 1.1248620748519897
Epoch 1160, training loss: 12.081108093261719 = 0.02544666826725006 + 2.0 * 6.027830600738525
Epoch 1160, val loss: 1.1304463148117065
Epoch 1170, training loss: 12.081068992614746 = 0.024696780368685722 + 2.0 * 6.028186321258545
Epoch 1170, val loss: 1.135925531387329
Epoch 1180, training loss: 12.072108268737793 = 0.023983106017112732 + 2.0 * 6.024062633514404
Epoch 1180, val loss: 1.1412798166275024
Epoch 1190, training loss: 12.067959785461426 = 0.023301873356103897 + 2.0 * 6.022328853607178
Epoch 1190, val loss: 1.1466953754425049
Epoch 1200, training loss: 12.065702438354492 = 0.022650940343737602 + 2.0 * 6.021525859832764
Epoch 1200, val loss: 1.1521108150482178
Epoch 1210, training loss: 12.070948600769043 = 0.02202632650732994 + 2.0 * 6.024461269378662
Epoch 1210, val loss: 1.1573935747146606
Epoch 1220, training loss: 12.06710147857666 = 0.021428806707262993 + 2.0 * 6.022836208343506
Epoch 1220, val loss: 1.1628907918930054
Epoch 1230, training loss: 12.0661039352417 = 0.020858237519860268 + 2.0 * 6.022623062133789
Epoch 1230, val loss: 1.167880892753601
Epoch 1240, training loss: 12.061690330505371 = 0.02031123824417591 + 2.0 * 6.020689487457275
Epoch 1240, val loss: 1.1730493307113647
Epoch 1250, training loss: 12.065383911132812 = 0.019787490367889404 + 2.0 * 6.02279806137085
Epoch 1250, val loss: 1.1782996654510498
Epoch 1260, training loss: 12.05760383605957 = 0.019282041117548943 + 2.0 * 6.019160747528076
Epoch 1260, val loss: 1.1834548711776733
Epoch 1270, training loss: 12.061300277709961 = 0.018796877935528755 + 2.0 * 6.021251678466797
Epoch 1270, val loss: 1.1884230375289917
Epoch 1280, training loss: 12.0615873336792 = 0.01833212748169899 + 2.0 * 6.021627426147461
Epoch 1280, val loss: 1.1934568881988525
Epoch 1290, training loss: 12.0631742477417 = 0.017883343622088432 + 2.0 * 6.022645473480225
Epoch 1290, val loss: 1.1984704732894897
Epoch 1300, training loss: 12.051826477050781 = 0.01745629869401455 + 2.0 * 6.017185211181641
Epoch 1300, val loss: 1.203378438949585
Epoch 1310, training loss: 12.051520347595215 = 0.017045706510543823 + 2.0 * 6.017237186431885
Epoch 1310, val loss: 1.2082520723342896
Epoch 1320, training loss: 12.04862117767334 = 0.016645975410938263 + 2.0 * 6.015987396240234
Epoch 1320, val loss: 1.2130824327468872
Epoch 1330, training loss: 12.052434921264648 = 0.016261020675301552 + 2.0 * 6.018086910247803
Epoch 1330, val loss: 1.2178559303283691
Epoch 1340, training loss: 12.046515464782715 = 0.015891466289758682 + 2.0 * 6.015312194824219
Epoch 1340, val loss: 1.2226065397262573
Epoch 1350, training loss: 12.050374031066895 = 0.015536952763795853 + 2.0 * 6.017418384552002
Epoch 1350, val loss: 1.2272415161132812
Epoch 1360, training loss: 12.048608779907227 = 0.015193589963018894 + 2.0 * 6.016707420349121
Epoch 1360, val loss: 1.2318494319915771
Epoch 1370, training loss: 12.045889854431152 = 0.014860434457659721 + 2.0 * 6.015514850616455
Epoch 1370, val loss: 1.236391544342041
Epoch 1380, training loss: 12.042964935302734 = 0.014543822966516018 + 2.0 * 6.0142107009887695
Epoch 1380, val loss: 1.2409454584121704
Epoch 1390, training loss: 12.040702819824219 = 0.014233488589525223 + 2.0 * 6.013234615325928
Epoch 1390, val loss: 1.2454586029052734
Epoch 1400, training loss: 12.050206184387207 = 0.013934578746557236 + 2.0 * 6.018136024475098
Epoch 1400, val loss: 1.2499637603759766
Epoch 1410, training loss: 12.047988891601562 = 0.013644438236951828 + 2.0 * 6.017172336578369
Epoch 1410, val loss: 1.2544188499450684
Epoch 1420, training loss: 12.04209041595459 = 0.013367945328354836 + 2.0 * 6.014361381530762
Epoch 1420, val loss: 1.2585804462432861
Epoch 1430, training loss: 12.037967681884766 = 0.013099298812448978 + 2.0 * 6.012434005737305
Epoch 1430, val loss: 1.2629181146621704
Epoch 1440, training loss: 12.038187026977539 = 0.01283902209252119 + 2.0 * 6.012673854827881
Epoch 1440, val loss: 1.2672799825668335
Epoch 1450, training loss: 12.043643951416016 = 0.012587988749146461 + 2.0 * 6.015528202056885
Epoch 1450, val loss: 1.2715270519256592
Epoch 1460, training loss: 12.033818244934082 = 0.012340618297457695 + 2.0 * 6.010738849639893
Epoch 1460, val loss: 1.2757443189620972
Epoch 1470, training loss: 12.037476539611816 = 0.012103683315217495 + 2.0 * 6.012686252593994
Epoch 1470, val loss: 1.279970407485962
Epoch 1480, training loss: 12.052151679992676 = 0.011876143515110016 + 2.0 * 6.020137786865234
Epoch 1480, val loss: 1.2840352058410645
Epoch 1490, training loss: 12.037233352661133 = 0.011651336215436459 + 2.0 * 6.012791156768799
Epoch 1490, val loss: 1.2880431413650513
Epoch 1500, training loss: 12.031096458435059 = 0.011437210254371166 + 2.0 * 6.009829521179199
Epoch 1500, val loss: 1.2920022010803223
Epoch 1510, training loss: 12.029218673706055 = 0.011227740906178951 + 2.0 * 6.008995532989502
Epoch 1510, val loss: 1.2959833145141602
Epoch 1520, training loss: 12.028529167175293 = 0.01102425716817379 + 2.0 * 6.008752346038818
Epoch 1520, val loss: 1.2999958992004395
Epoch 1530, training loss: 12.067118644714355 = 0.010830404236912727 + 2.0 * 6.028143882751465
Epoch 1530, val loss: 1.3039535284042358
Epoch 1540, training loss: 12.032815933227539 = 0.010633977130055428 + 2.0 * 6.0110907554626465
Epoch 1540, val loss: 1.3077653646469116
Epoch 1550, training loss: 12.028266906738281 = 0.010449998080730438 + 2.0 * 6.008908271789551
Epoch 1550, val loss: 1.311402440071106
Epoch 1560, training loss: 12.024559020996094 = 0.010270382277667522 + 2.0 * 6.007144451141357
Epoch 1560, val loss: 1.3153152465820312
Epoch 1570, training loss: 12.023857116699219 = 0.010094977915287018 + 2.0 * 6.006881237030029
Epoch 1570, val loss: 1.3191672563552856
Epoch 1580, training loss: 12.024656295776367 = 0.009923448786139488 + 2.0 * 6.00736665725708
Epoch 1580, val loss: 1.3229464292526245
Epoch 1590, training loss: 12.036776542663574 = 0.009758331812918186 + 2.0 * 6.013509273529053
Epoch 1590, val loss: 1.326708436012268
Epoch 1600, training loss: 12.024567604064941 = 0.00959673523902893 + 2.0 * 6.007485389709473
Epoch 1600, val loss: 1.3302810192108154
Epoch 1610, training loss: 12.023114204406738 = 0.009438952431082726 + 2.0 * 6.006837844848633
Epoch 1610, val loss: 1.333773136138916
Epoch 1620, training loss: 12.021622657775879 = 0.009286744520068169 + 2.0 * 6.006167888641357
Epoch 1620, val loss: 1.3374615907669067
Epoch 1630, training loss: 12.029179573059082 = 0.009137539193034172 + 2.0 * 6.010021209716797
Epoch 1630, val loss: 1.3410725593566895
Epoch 1640, training loss: 12.020278930664062 = 0.008993702940642834 + 2.0 * 6.005642414093018
Epoch 1640, val loss: 1.3444513082504272
Epoch 1650, training loss: 12.025675773620605 = 0.008853302337229252 + 2.0 * 6.008411407470703
Epoch 1650, val loss: 1.3478970527648926
Epoch 1660, training loss: 12.021369934082031 = 0.008715261705219746 + 2.0 * 6.006327152252197
Epoch 1660, val loss: 1.3514479398727417
Epoch 1670, training loss: 12.024223327636719 = 0.008583379909396172 + 2.0 * 6.007820129394531
Epoch 1670, val loss: 1.3548099994659424
Epoch 1680, training loss: 12.017133712768555 = 0.008452230133116245 + 2.0 * 6.004340648651123
Epoch 1680, val loss: 1.3581669330596924
Epoch 1690, training loss: 12.0180082321167 = 0.00832529179751873 + 2.0 * 6.004841327667236
Epoch 1690, val loss: 1.3614860773086548
Epoch 1700, training loss: 12.024008750915527 = 0.008201643824577332 + 2.0 * 6.007903575897217
Epoch 1700, val loss: 1.36485755443573
Epoch 1710, training loss: 12.018604278564453 = 0.008081022650003433 + 2.0 * 6.005261421203613
Epoch 1710, val loss: 1.3681104183197021
Epoch 1720, training loss: 12.015137672424316 = 0.00796364713460207 + 2.0 * 6.003587245941162
Epoch 1720, val loss: 1.3712847232818604
Epoch 1730, training loss: 12.02265453338623 = 0.007848198525607586 + 2.0 * 6.007403373718262
Epoch 1730, val loss: 1.3745005130767822
Epoch 1740, training loss: 12.018949508666992 = 0.0077362810261547565 + 2.0 * 6.005606651306152
Epoch 1740, val loss: 1.377732276916504
Epoch 1750, training loss: 12.014155387878418 = 0.00762743316590786 + 2.0 * 6.0032639503479
Epoch 1750, val loss: 1.3808094263076782
Epoch 1760, training loss: 12.01457691192627 = 0.00752117857336998 + 2.0 * 6.003527641296387
Epoch 1760, val loss: 1.3839643001556396
Epoch 1770, training loss: 12.024014472961426 = 0.0074159870855510235 + 2.0 * 6.008299350738525
Epoch 1770, val loss: 1.3870034217834473
Epoch 1780, training loss: 12.012910842895508 = 0.007316882256418467 + 2.0 * 6.0027971267700195
Epoch 1780, val loss: 1.390013337135315
Epoch 1790, training loss: 12.009079933166504 = 0.007216766010969877 + 2.0 * 6.000931739807129
Epoch 1790, val loss: 1.3931089639663696
Epoch 1800, training loss: 12.009765625 = 0.0071193743497133255 + 2.0 * 6.001323223114014
Epoch 1800, val loss: 1.396168828010559
Epoch 1810, training loss: 12.015625953674316 = 0.007024683989584446 + 2.0 * 6.004300594329834
Epoch 1810, val loss: 1.3991706371307373
Epoch 1820, training loss: 12.010414123535156 = 0.006931818090379238 + 2.0 * 6.0017409324646
Epoch 1820, val loss: 1.4021257162094116
Epoch 1830, training loss: 12.008284568786621 = 0.006840863730758429 + 2.0 * 6.0007219314575195
Epoch 1830, val loss: 1.4049937725067139
Epoch 1840, training loss: 12.017524719238281 = 0.006752898450940847 + 2.0 * 6.005385875701904
Epoch 1840, val loss: 1.4079266786575317
Epoch 1850, training loss: 12.010281562805176 = 0.006665968336164951 + 2.0 * 6.001807689666748
Epoch 1850, val loss: 1.4106920957565308
Epoch 1860, training loss: 12.006817817687988 = 0.006580603774636984 + 2.0 * 6.000118732452393
Epoch 1860, val loss: 1.4135398864746094
Epoch 1870, training loss: 12.005244255065918 = 0.006497776601463556 + 2.0 * 5.999373435974121
Epoch 1870, val loss: 1.4163414239883423
Epoch 1880, training loss: 12.00495433807373 = 0.006416005548089743 + 2.0 * 5.999269008636475
Epoch 1880, val loss: 1.4191625118255615
Epoch 1890, training loss: 12.018780708312988 = 0.006335428915917873 + 2.0 * 6.006222724914551
Epoch 1890, val loss: 1.4219611883163452
Epoch 1900, training loss: 12.005295753479004 = 0.006258902605623007 + 2.0 * 5.999518394470215
Epoch 1900, val loss: 1.4246270656585693
Epoch 1910, training loss: 12.00665283203125 = 0.006182734854519367 + 2.0 * 6.000235080718994
Epoch 1910, val loss: 1.4273563623428345
Epoch 1920, training loss: 12.00602912902832 = 0.006108312867581844 + 2.0 * 5.999960422515869
Epoch 1920, val loss: 1.430066466331482
Epoch 1930, training loss: 12.00586986541748 = 0.006034902762621641 + 2.0 * 5.999917507171631
Epoch 1930, val loss: 1.4327194690704346
Epoch 1940, training loss: 12.005616188049316 = 0.005963699892163277 + 2.0 * 5.999826431274414
Epoch 1940, val loss: 1.4353419542312622
Epoch 1950, training loss: 12.003218650817871 = 0.005893269088119268 + 2.0 * 5.99866247177124
Epoch 1950, val loss: 1.4378067255020142
Epoch 1960, training loss: 12.001219749450684 = 0.005824341904371977 + 2.0 * 5.997697830200195
Epoch 1960, val loss: 1.44045889377594
Epoch 1970, training loss: 12.010190963745117 = 0.005757628474384546 + 2.0 * 6.002216815948486
Epoch 1970, val loss: 1.4430255889892578
Epoch 1980, training loss: 12.001204490661621 = 0.005691070109605789 + 2.0 * 5.997756481170654
Epoch 1980, val loss: 1.4456045627593994
Epoch 1990, training loss: 12.00032901763916 = 0.0056258500553667545 + 2.0 * 5.99735164642334
Epoch 1990, val loss: 1.4480957984924316
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7519
Overall ASR: 0.8303
Flip ASR: 0.8044/225 nodes
The final ASR:0.56212, 0.24789, Accuracy:0.78395, 0.02727
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11596])
remove edge: torch.Size([2, 9496])
updated graph: torch.Size([2, 10536])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
The final ASR:0.97909, 0.00348, Accuracy:0.82840, 0.00630
Begin epxeriment: cont_weight: 2 epoch:2000 der1:0.3 der2:0.2 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 18.684925079345703 = 1.9373036623001099 + 2.0 * 8.373810768127441
Epoch 0, val loss: 1.9348061084747314
Epoch 10, training loss: 18.673818588256836 = 1.9274570941925049 + 2.0 * 8.373180389404297
Epoch 10, val loss: 1.9254921674728394
Epoch 20, training loss: 18.652963638305664 = 1.915324330329895 + 2.0 * 8.368819236755371
Epoch 20, val loss: 1.9141461849212646
Epoch 30, training loss: 18.572792053222656 = 1.8992153406143188 + 2.0 * 8.336788177490234
Epoch 30, val loss: 1.899245023727417
Epoch 40, training loss: 18.110177993774414 = 1.880602240562439 + 2.0 * 8.114788055419922
Epoch 40, val loss: 1.8820385932922363
Epoch 50, training loss: 16.376750946044922 = 1.8612895011901855 + 2.0 * 7.257730484008789
Epoch 50, val loss: 1.8644165992736816
Epoch 60, training loss: 15.797396659851074 = 1.849293828010559 + 2.0 * 6.974051475524902
Epoch 60, val loss: 1.8533390760421753
Epoch 70, training loss: 15.327033996582031 = 1.8396852016448975 + 2.0 * 6.743674278259277
Epoch 70, val loss: 1.8441367149353027
Epoch 80, training loss: 14.96308708190918 = 1.8302299976348877 + 2.0 * 6.5664286613464355
Epoch 80, val loss: 1.8358596563339233
Epoch 90, training loss: 14.734505653381348 = 1.8217979669570923 + 2.0 * 6.456353664398193
Epoch 90, val loss: 1.8282639980316162
Epoch 100, training loss: 14.602312088012695 = 1.8137598037719727 + 2.0 * 6.394276142120361
Epoch 100, val loss: 1.8205931186676025
Epoch 110, training loss: 14.507586479187012 = 1.8055522441864014 + 2.0 * 6.351016998291016
Epoch 110, val loss: 1.8126693964004517
Epoch 120, training loss: 14.426929473876953 = 1.7976056337356567 + 2.0 * 6.314661979675293
Epoch 120, val loss: 1.8052040338516235
Epoch 130, training loss: 14.35930347442627 = 1.7900922298431396 + 2.0 * 6.284605503082275
Epoch 130, val loss: 1.79816472530365
Epoch 140, training loss: 14.302116394042969 = 1.782469630241394 + 2.0 * 6.259823322296143
Epoch 140, val loss: 1.790945291519165
Epoch 150, training loss: 14.252614974975586 = 1.7742915153503418 + 2.0 * 6.239161968231201
Epoch 150, val loss: 1.7833014726638794
Epoch 160, training loss: 14.206189155578613 = 1.7654367685317993 + 2.0 * 6.220376014709473
Epoch 160, val loss: 1.775267481803894
Epoch 170, training loss: 14.16238021850586 = 1.755749225616455 + 2.0 * 6.203315258026123
Epoch 170, val loss: 1.7667826414108276
Epoch 180, training loss: 14.122308731079102 = 1.7448115348815918 + 2.0 * 6.188748359680176
Epoch 180, val loss: 1.757530689239502
Epoch 190, training loss: 14.086868286132812 = 1.7322273254394531 + 2.0 * 6.17732048034668
Epoch 190, val loss: 1.747161626815796
Epoch 200, training loss: 14.05058765411377 = 1.7176589965820312 + 2.0 * 6.166464328765869
Epoch 200, val loss: 1.7353070974349976
Epoch 210, training loss: 14.015707969665527 = 1.7005749940872192 + 2.0 * 6.157566547393799
Epoch 210, val loss: 1.7215567827224731
Epoch 220, training loss: 13.985831260681152 = 1.6804792881011963 + 2.0 * 6.152676105499268
Epoch 220, val loss: 1.705496072769165
Epoch 230, training loss: 13.943947792053223 = 1.6570274829864502 + 2.0 * 6.143460273742676
Epoch 230, val loss: 1.6866904497146606
Epoch 240, training loss: 13.903362274169922 = 1.6295416355133057 + 2.0 * 6.136910438537598
Epoch 240, val loss: 1.6647005081176758
Epoch 250, training loss: 13.859182357788086 = 1.5973045825958252 + 2.0 * 6.13093900680542
Epoch 250, val loss: 1.6390259265899658
Epoch 260, training loss: 13.813008308410645 = 1.5600181818008423 + 2.0 * 6.126494884490967
Epoch 260, val loss: 1.609258770942688
Epoch 270, training loss: 13.762672424316406 = 1.5181572437286377 + 2.0 * 6.122257709503174
Epoch 270, val loss: 1.5758392810821533
Epoch 280, training loss: 13.706313133239746 = 1.471398949623108 + 2.0 * 6.117456912994385
Epoch 280, val loss: 1.5382963418960571
Epoch 290, training loss: 13.645668983459473 = 1.419897437095642 + 2.0 * 6.11288595199585
Epoch 290, val loss: 1.4968360662460327
Epoch 300, training loss: 13.591119766235352 = 1.3642823696136475 + 2.0 * 6.1134185791015625
Epoch 300, val loss: 1.4523084163665771
Epoch 310, training loss: 13.523417472839355 = 1.3073179721832275 + 2.0 * 6.1080498695373535
Epoch 310, val loss: 1.406327724456787
Epoch 320, training loss: 13.455078125 = 1.2501661777496338 + 2.0 * 6.102456092834473
Epoch 320, val loss: 1.3602454662322998
Epoch 330, training loss: 13.391525268554688 = 1.1936753988265991 + 2.0 * 6.0989251136779785
Epoch 330, val loss: 1.3149985074996948
Epoch 340, training loss: 13.334380149841309 = 1.1388990879058838 + 2.0 * 6.097740650177002
Epoch 340, val loss: 1.271400809288025
Epoch 350, training loss: 13.275277137756348 = 1.0874948501586914 + 2.0 * 6.093891143798828
Epoch 350, val loss: 1.230882167816162
Epoch 360, training loss: 13.21824836730957 = 1.0395126342773438 + 2.0 * 6.089367866516113
Epoch 360, val loss: 1.1933835744857788
Epoch 370, training loss: 13.170654296875 = 0.994618833065033 + 2.0 * 6.08801794052124
Epoch 370, val loss: 1.1586781740188599
Epoch 380, training loss: 13.127989768981934 = 0.9530865550041199 + 2.0 * 6.087451457977295
Epoch 380, val loss: 1.1273077726364136
Epoch 390, training loss: 13.0769624710083 = 0.9149184823036194 + 2.0 * 6.081021785736084
Epoch 390, val loss: 1.0988415479660034
Epoch 400, training loss: 13.03645133972168 = 0.8791831135749817 + 2.0 * 6.078634262084961
Epoch 400, val loss: 1.0726124048233032
Epoch 410, training loss: 12.997295379638672 = 0.8458114862442017 + 2.0 * 6.075741767883301
Epoch 410, val loss: 1.0485836267471313
Epoch 420, training loss: 12.962263107299805 = 0.8146149516105652 + 2.0 * 6.073823928833008
Epoch 420, val loss: 1.0268076658248901
Epoch 430, training loss: 12.930540084838867 = 0.7852437496185303 + 2.0 * 6.072648048400879
Epoch 430, val loss: 1.0067161321640015
Epoch 440, training loss: 12.897899627685547 = 0.7574606537818909 + 2.0 * 6.07021951675415
Epoch 440, val loss: 0.988337516784668
Epoch 450, training loss: 12.865212440490723 = 0.7311687469482422 + 2.0 * 6.06702184677124
Epoch 450, val loss: 0.9713844656944275
Epoch 460, training loss: 12.843122482299805 = 0.7061221599578857 + 2.0 * 6.06850004196167
Epoch 460, val loss: 0.9557048678398132
Epoch 470, training loss: 12.810712814331055 = 0.6821045279502869 + 2.0 * 6.064304351806641
Epoch 470, val loss: 0.9412248134613037
Epoch 480, training loss: 12.782203674316406 = 0.6589080095291138 + 2.0 * 6.061647891998291
Epoch 480, val loss: 0.9274923801422119
Epoch 490, training loss: 12.757556915283203 = 0.6363064646720886 + 2.0 * 6.060625076293945
Epoch 490, val loss: 0.9144786596298218
Epoch 500, training loss: 12.741005897521973 = 0.614443838596344 + 2.0 * 6.063281059265137
Epoch 500, val loss: 0.9021583795547485
Epoch 510, training loss: 12.710081100463867 = 0.5932828187942505 + 2.0 * 6.058399200439453
Epoch 510, val loss: 0.8908345103263855
Epoch 520, training loss: 12.689340591430664 = 0.5726224780082703 + 2.0 * 6.058359146118164
Epoch 520, val loss: 0.8799654245376587
Epoch 530, training loss: 12.661296844482422 = 0.5525790452957153 + 2.0 * 6.054358959197998
Epoch 530, val loss: 0.869659423828125
Epoch 540, training loss: 12.638254165649414 = 0.5330014824867249 + 2.0 * 6.052626132965088
Epoch 540, val loss: 0.8600872755050659
Epoch 550, training loss: 12.6258544921875 = 0.5139167904853821 + 2.0 * 6.055968761444092
Epoch 550, val loss: 0.8510498404502869
Epoch 560, training loss: 12.597931861877441 = 0.4954899251461029 + 2.0 * 6.051220893859863
Epoch 560, val loss: 0.8427223563194275
Epoch 570, training loss: 12.579671859741211 = 0.4776597321033478 + 2.0 * 6.051005840301514
Epoch 570, val loss: 0.8352054953575134
Epoch 580, training loss: 12.560585975646973 = 0.4603012800216675 + 2.0 * 6.050142288208008
Epoch 580, val loss: 0.8281329870223999
Epoch 590, training loss: 12.538113594055176 = 0.44338682293891907 + 2.0 * 6.04736328125
Epoch 590, val loss: 0.8218453526496887
Epoch 600, training loss: 12.521007537841797 = 0.42670226097106934 + 2.0 * 6.047152519226074
Epoch 600, val loss: 0.8158993721008301
Epoch 610, training loss: 12.504481315612793 = 0.41034942865371704 + 2.0 * 6.047065734863281
Epoch 610, val loss: 0.8103154897689819
Epoch 620, training loss: 12.484864234924316 = 0.39422085881233215 + 2.0 * 6.045321464538574
Epoch 620, val loss: 0.8053910732269287
Epoch 630, training loss: 12.468727111816406 = 0.37832802534103394 + 2.0 * 6.045199394226074
Epoch 630, val loss: 0.8006760478019714
Epoch 640, training loss: 12.451827049255371 = 0.36256420612335205 + 2.0 * 6.044631481170654
Epoch 640, val loss: 0.7961922287940979
Epoch 650, training loss: 12.430163383483887 = 0.34705081582069397 + 2.0 * 6.041556358337402
Epoch 650, val loss: 0.7921090126037598
Epoch 660, training loss: 12.411580085754395 = 0.3316827714443207 + 2.0 * 6.039948463439941
Epoch 660, val loss: 0.7882804274559021
Epoch 670, training loss: 12.393906593322754 = 0.3164982497692108 + 2.0 * 6.0387043952941895
Epoch 670, val loss: 0.7846779227256775
Epoch 680, training loss: 12.391047477722168 = 0.30154454708099365 + 2.0 * 6.0447516441345215
Epoch 680, val loss: 0.7812711596488953
Epoch 690, training loss: 12.364075660705566 = 0.287026584148407 + 2.0 * 6.038524627685547
Epoch 690, val loss: 0.7782077193260193
Epoch 700, training loss: 12.348427772521973 = 0.2729833424091339 + 2.0 * 6.037722110748291
Epoch 700, val loss: 0.7755996584892273
Epoch 710, training loss: 12.331005096435547 = 0.25934290885925293 + 2.0 * 6.035830974578857
Epoch 710, val loss: 0.7731561660766602
Epoch 720, training loss: 12.315206527709961 = 0.24613326787948608 + 2.0 * 6.034536838531494
Epoch 720, val loss: 0.7710109949111938
Epoch 730, training loss: 12.321026802062988 = 0.23343078792095184 + 2.0 * 6.043797969818115
Epoch 730, val loss: 0.7690569162368774
Epoch 740, training loss: 12.29601764678955 = 0.22146159410476685 + 2.0 * 6.037278175354004
Epoch 740, val loss: 0.7676292061805725
Epoch 750, training loss: 12.276817321777344 = 0.21008282899856567 + 2.0 * 6.033367156982422
Epoch 750, val loss: 0.766628086566925
Epoch 760, training loss: 12.26284408569336 = 0.19920501112937927 + 2.0 * 6.0318193435668945
Epoch 760, val loss: 0.7658390998840332
Epoch 770, training loss: 12.25422191619873 = 0.18885274231433868 + 2.0 * 6.032684803009033
Epoch 770, val loss: 0.7655133008956909
Epoch 780, training loss: 12.244139671325684 = 0.1790514439344406 + 2.0 * 6.032544136047363
Epoch 780, val loss: 0.7654563784599304
Epoch 790, training loss: 12.230217933654785 = 0.1698562055826187 + 2.0 * 6.030180931091309
Epoch 790, val loss: 0.7658789753913879
Epoch 800, training loss: 12.22985553741455 = 0.1611824929714203 + 2.0 * 6.034336566925049
Epoch 800, val loss: 0.7666471004486084
Epoch 810, training loss: 12.212313652038574 = 0.15301086008548737 + 2.0 * 6.029651165008545
Epoch 810, val loss: 0.7678117752075195
Epoch 820, training loss: 12.20093059539795 = 0.14532044529914856 + 2.0 * 6.027804851531982
Epoch 820, val loss: 0.7693968415260315
Epoch 830, training loss: 12.19164752960205 = 0.13805443048477173 + 2.0 * 6.026796340942383
Epoch 830, val loss: 0.771264910697937
Epoch 840, training loss: 12.199705123901367 = 0.13122855126857758 + 2.0 * 6.034238338470459
Epoch 840, val loss: 0.7734325528144836
Epoch 850, training loss: 12.18000602722168 = 0.12488603591918945 + 2.0 * 6.027560234069824
Epoch 850, val loss: 0.7758850455284119
Epoch 860, training loss: 12.168377876281738 = 0.11893750727176666 + 2.0 * 6.024720191955566
Epoch 860, val loss: 0.7785947918891907
Epoch 870, training loss: 12.161898612976074 = 0.11332537233829498 + 2.0 * 6.02428674697876
Epoch 870, val loss: 0.781575083732605
Epoch 880, training loss: 12.162718772888184 = 0.10803314298391342 + 2.0 * 6.027342796325684
Epoch 880, val loss: 0.7848774790763855
Epoch 890, training loss: 12.16328239440918 = 0.103079654276371 + 2.0 * 6.030101299285889
Epoch 890, val loss: 0.788290798664093
Epoch 900, training loss: 12.146495819091797 = 0.0984644889831543 + 2.0 * 6.0240159034729
Epoch 900, val loss: 0.7919603586196899
Epoch 910, training loss: 12.138631820678711 = 0.09411963075399399 + 2.0 * 6.022255897521973
Epoch 910, val loss: 0.7958441972732544
Epoch 920, training loss: 12.13521957397461 = 0.09001019597053528 + 2.0 * 6.022604465484619
Epoch 920, val loss: 0.7998114228248596
Epoch 930, training loss: 12.128864288330078 = 0.08615541458129883 + 2.0 * 6.0213541984558105
Epoch 930, val loss: 0.8040907979011536
Epoch 940, training loss: 12.122435569763184 = 0.0825292244553566 + 2.0 * 6.01995325088501
Epoch 940, val loss: 0.8083811402320862
Epoch 950, training loss: 12.119683265686035 = 0.07910829782485962 + 2.0 * 6.02028751373291
Epoch 950, val loss: 0.8128620982170105
Epoch 960, training loss: 12.114923477172852 = 0.07588468492031097 + 2.0 * 6.019519329071045
Epoch 960, val loss: 0.8173127174377441
Epoch 970, training loss: 12.126581192016602 = 0.07286186516284943 + 2.0 * 6.026859760284424
Epoch 970, val loss: 0.8219588994979858
Epoch 980, training loss: 12.11113452911377 = 0.07000790536403656 + 2.0 * 6.020563125610352
Epoch 980, val loss: 0.8264197111129761
Epoch 990, training loss: 12.102654457092285 = 0.06733372807502747 + 2.0 * 6.017660140991211
Epoch 990, val loss: 0.8311347961425781
Epoch 1000, training loss: 12.096735000610352 = 0.0647817999124527 + 2.0 * 6.015976428985596
Epoch 1000, val loss: 0.8359202146530151
Epoch 1010, training loss: 12.093283653259277 = 0.06235406920313835 + 2.0 * 6.015464782714844
Epoch 1010, val loss: 0.8407450318336487
Epoch 1020, training loss: 12.097060203552246 = 0.060046758502721786 + 2.0 * 6.0185065269470215
Epoch 1020, val loss: 0.8456523418426514
Epoch 1030, training loss: 12.099722862243652 = 0.057865407317876816 + 2.0 * 6.020928859710693
Epoch 1030, val loss: 0.8505325317382812
Epoch 1040, training loss: 12.085165023803711 = 0.055822279304265976 + 2.0 * 6.014671325683594
Epoch 1040, val loss: 0.8552674055099487
Epoch 1050, training loss: 12.08162784576416 = 0.05389031767845154 + 2.0 * 6.013868808746338
Epoch 1050, val loss: 0.8600484728813171
Epoch 1060, training loss: 12.080985069274902 = 0.05204015597701073 + 2.0 * 6.014472484588623
Epoch 1060, val loss: 0.8648281097412109
Epoch 1070, training loss: 12.082066535949707 = 0.05027993023395538 + 2.0 * 6.015893459320068
Epoch 1070, val loss: 0.8695312142372131
Epoch 1080, training loss: 12.079351425170898 = 0.048618849366903305 + 2.0 * 6.015366077423096
Epoch 1080, val loss: 0.8743600845336914
Epoch 1090, training loss: 12.07054615020752 = 0.04703804850578308 + 2.0 * 6.011754035949707
Epoch 1090, val loss: 0.8790338039398193
Epoch 1100, training loss: 12.06981086730957 = 0.04553312063217163 + 2.0 * 6.012138843536377
Epoch 1100, val loss: 0.8838086128234863
Epoch 1110, training loss: 12.072307586669922 = 0.04409516602754593 + 2.0 * 6.014106273651123
Epoch 1110, val loss: 0.8884022235870361
Epoch 1120, training loss: 12.065115928649902 = 0.0427292101085186 + 2.0 * 6.01119327545166
Epoch 1120, val loss: 0.8929904103279114
Epoch 1130, training loss: 12.062887191772461 = 0.04143396019935608 + 2.0 * 6.010726451873779
Epoch 1130, val loss: 0.8975822329521179
Epoch 1140, training loss: 12.063432693481445 = 0.040188390761613846 + 2.0 * 6.011621952056885
Epoch 1140, val loss: 0.9020528197288513
Epoch 1150, training loss: 12.05744743347168 = 0.03900585323572159 + 2.0 * 6.009220600128174
Epoch 1150, val loss: 0.9065110683441162
Epoch 1160, training loss: 12.056361198425293 = 0.03787921741604805 + 2.0 * 6.009241104125977
Epoch 1160, val loss: 0.9110057950019836
Epoch 1170, training loss: 12.078532218933105 = 0.03679826855659485 + 2.0 * 6.020866870880127
Epoch 1170, val loss: 0.9153103828430176
Epoch 1180, training loss: 12.054423332214355 = 0.03577928990125656 + 2.0 * 6.009322166442871
Epoch 1180, val loss: 0.9196228981018066
Epoch 1190, training loss: 12.049369812011719 = 0.03480730578303337 + 2.0 * 6.007281303405762
Epoch 1190, val loss: 0.9238461852073669
Epoch 1200, training loss: 12.047333717346191 = 0.0338667668402195 + 2.0 * 6.006733417510986
Epoch 1200, val loss: 0.9280247092247009
Epoch 1210, training loss: 12.04569149017334 = 0.0329609215259552 + 2.0 * 6.0063652992248535
Epoch 1210, val loss: 0.9322996139526367
Epoch 1220, training loss: 12.061410903930664 = 0.03209406137466431 + 2.0 * 6.014658451080322
Epoch 1220, val loss: 0.9364684820175171
Epoch 1230, training loss: 12.042377471923828 = 0.031263452023267746 + 2.0 * 6.005557060241699
Epoch 1230, val loss: 0.9404616951942444
Epoch 1240, training loss: 12.045829772949219 = 0.030475445091724396 + 2.0 * 6.00767707824707
Epoch 1240, val loss: 0.9445158839225769
Epoch 1250, training loss: 12.040836334228516 = 0.02971472404897213 + 2.0 * 6.005560874938965
Epoch 1250, val loss: 0.9484217762947083
Epoch 1260, training loss: 12.037906646728516 = 0.02898338995873928 + 2.0 * 6.004461765289307
Epoch 1260, val loss: 0.9523913264274597
Epoch 1270, training loss: 12.039051055908203 = 0.028279826045036316 + 2.0 * 6.005385398864746
Epoch 1270, val loss: 0.9563530683517456
Epoch 1280, training loss: 12.040545463562012 = 0.027601607143878937 + 2.0 * 6.006472110748291
Epoch 1280, val loss: 0.9601280093193054
Epoch 1290, training loss: 12.036476135253906 = 0.02694999799132347 + 2.0 * 6.004763126373291
Epoch 1290, val loss: 0.9640256762504578
Epoch 1300, training loss: 12.034626960754395 = 0.026321062818169594 + 2.0 * 6.004152774810791
Epoch 1300, val loss: 0.9678426384925842
Epoch 1310, training loss: 12.035296440124512 = 0.025715116411447525 + 2.0 * 6.004790782928467
Epoch 1310, val loss: 0.9715622663497925
Epoch 1320, training loss: 12.035289764404297 = 0.02513434737920761 + 2.0 * 6.005077838897705
Epoch 1320, val loss: 0.9752566814422607
Epoch 1330, training loss: 12.029998779296875 = 0.024571219459176064 + 2.0 * 6.002713680267334
Epoch 1330, val loss: 0.978899359703064
Epoch 1340, training loss: 12.02823257446289 = 0.024029863998293877 + 2.0 * 6.002101421356201
Epoch 1340, val loss: 0.9824872016906738
Epoch 1350, training loss: 12.030906677246094 = 0.023504963144659996 + 2.0 * 6.0037007331848145
Epoch 1350, val loss: 0.9860649704933167
Epoch 1360, training loss: 12.030662536621094 = 0.022998344153165817 + 2.0 * 6.00383186340332
Epoch 1360, val loss: 0.9896618723869324
Epoch 1370, training loss: 12.029380798339844 = 0.022509822621941566 + 2.0 * 6.0034356117248535
Epoch 1370, val loss: 0.9930814504623413
Epoch 1380, training loss: 12.024252891540527 = 0.022040553390979767 + 2.0 * 6.001106262207031
Epoch 1380, val loss: 0.9964536428451538
Epoch 1390, training loss: 12.021834373474121 = 0.021586930379271507 + 2.0 * 6.000123500823975
Epoch 1390, val loss: 0.9998806715011597
Epoch 1400, training loss: 12.022109031677246 = 0.02114349976181984 + 2.0 * 6.000482559204102
Epoch 1400, val loss: 1.0033440589904785
Epoch 1410, training loss: 12.022838592529297 = 0.02071388065814972 + 2.0 * 6.001062393188477
Epoch 1410, val loss: 1.0066653490066528
Epoch 1420, training loss: 12.021463394165039 = 0.020299160853028297 + 2.0 * 6.000582218170166
Epoch 1420, val loss: 1.0099363327026367
Epoch 1430, training loss: 12.020777702331543 = 0.019898690283298492 + 2.0 * 6.000439643859863
Epoch 1430, val loss: 1.0132795572280884
Epoch 1440, training loss: 12.020792961120605 = 0.0195100549608469 + 2.0 * 6.000641345977783
Epoch 1440, val loss: 1.0164481401443481
Epoch 1450, training loss: 12.020456314086914 = 0.01913394406437874 + 2.0 * 6.000661373138428
Epoch 1450, val loss: 1.0196120738983154
Epoch 1460, training loss: 12.015752792358398 = 0.018768804147839546 + 2.0 * 5.9984917640686035
Epoch 1460, val loss: 1.0228770971298218
Epoch 1470, training loss: 12.014762878417969 = 0.01841466687619686 + 2.0 * 5.99817419052124
Epoch 1470, val loss: 1.0259877443313599
Epoch 1480, training loss: 12.016034126281738 = 0.018070800229907036 + 2.0 * 5.998981475830078
Epoch 1480, val loss: 1.0290634632110596
Epoch 1490, training loss: 12.024011611938477 = 0.017735140398144722 + 2.0 * 6.003138065338135
Epoch 1490, val loss: 1.0321687459945679
Epoch 1500, training loss: 12.013846397399902 = 0.017414113506674767 + 2.0 * 5.998216152191162
Epoch 1500, val loss: 1.0351675748825073
Epoch 1510, training loss: 12.009417533874512 = 0.01710141822695732 + 2.0 * 5.996158123016357
Epoch 1510, val loss: 1.0381860733032227
Epoch 1520, training loss: 12.007904052734375 = 0.016795217990875244 + 2.0 * 5.995554447174072
Epoch 1520, val loss: 1.0412518978118896
Epoch 1530, training loss: 12.015013694763184 = 0.016494901850819588 + 2.0 * 5.9992594718933105
Epoch 1530, val loss: 1.044374942779541
Epoch 1540, training loss: 12.006568908691406 = 0.016202645376324654 + 2.0 * 5.995182991027832
Epoch 1540, val loss: 1.047210931777954
Epoch 1550, training loss: 12.005112648010254 = 0.015923580154776573 + 2.0 * 5.994594573974609
Epoch 1550, val loss: 1.0502214431762695
Epoch 1560, training loss: 12.009420394897461 = 0.015650514513254166 + 2.0 * 5.996884822845459
Epoch 1560, val loss: 1.0531076192855835
Epoch 1570, training loss: 12.005499839782715 = 0.015381809324026108 + 2.0 * 5.995059013366699
Epoch 1570, val loss: 1.0560002326965332
Epoch 1580, training loss: 12.004310607910156 = 0.015123676508665085 + 2.0 * 5.994593620300293
Epoch 1580, val loss: 1.0587553977966309
Epoch 1590, training loss: 12.00462532043457 = 0.014871577732264996 + 2.0 * 5.994876861572266
Epoch 1590, val loss: 1.0616991519927979
Epoch 1600, training loss: 12.008190155029297 = 0.014622691087424755 + 2.0 * 5.99678373336792
Epoch 1600, val loss: 1.0644803047180176
Epoch 1610, training loss: 12.00348949432373 = 0.014381405897438526 + 2.0 * 5.994554042816162
Epoch 1610, val loss: 1.067362904548645
Epoch 1620, training loss: 12.006233215332031 = 0.01414637640118599 + 2.0 * 5.9960432052612305
Epoch 1620, val loss: 1.0701441764831543
Epoch 1630, training loss: 12.000602722167969 = 0.013918568380177021 + 2.0 * 5.99334192276001
Epoch 1630, val loss: 1.0728646516799927
Epoch 1640, training loss: 11.99875545501709 = 0.013694812543690205 + 2.0 * 5.992530345916748
Epoch 1640, val loss: 1.0756433010101318
Epoch 1650, training loss: 12.017796516418457 = 0.013474749401211739 + 2.0 * 6.002161026000977
Epoch 1650, val loss: 1.0783721208572388
Epoch 1660, training loss: 12.009343147277832 = 0.013262292370200157 + 2.0 * 5.998040199279785
Epoch 1660, val loss: 1.0810149908065796
Epoch 1670, training loss: 11.996258735656738 = 0.013061785139143467 + 2.0 * 5.991598606109619
Epoch 1670, val loss: 1.083552360534668
Epoch 1680, training loss: 11.99659252166748 = 0.01286301203072071 + 2.0 * 5.9918646812438965
Epoch 1680, val loss: 1.0862250328063965
Epoch 1690, training loss: 11.994643211364746 = 0.012663736939430237 + 2.0 * 5.990989685058594
Epoch 1690, val loss: 1.0888522863388062
Epoch 1700, training loss: 12.012811660766602 = 0.012470086105167866 + 2.0 * 6.000170707702637
Epoch 1700, val loss: 1.091578483581543
Epoch 1710, training loss: 12.001893997192383 = 0.01228042971342802 + 2.0 * 5.99480676651001
Epoch 1710, val loss: 1.093938946723938
Epoch 1720, training loss: 11.997050285339355 = 0.012100698426365852 + 2.0 * 5.992474555969238
Epoch 1720, val loss: 1.0965733528137207
Epoch 1730, training loss: 11.99573040008545 = 0.011921625584363937 + 2.0 * 5.991904258728027
Epoch 1730, val loss: 1.0991132259368896
Epoch 1740, training loss: 11.995905876159668 = 0.011745588853955269 + 2.0 * 5.992080211639404
Epoch 1740, val loss: 1.1016175746917725
Epoch 1750, training loss: 11.99803638458252 = 0.011573707684874535 + 2.0 * 5.993231296539307
Epoch 1750, val loss: 1.1041852235794067
Epoch 1760, training loss: 11.991467475891113 = 0.01140634436160326 + 2.0 * 5.990030765533447
Epoch 1760, val loss: 1.1066535711288452
Epoch 1770, training loss: 11.991148948669434 = 0.011241992004215717 + 2.0 * 5.989953517913818
Epoch 1770, val loss: 1.1090847253799438
Epoch 1780, training loss: 11.996244430541992 = 0.01107940822839737 + 2.0 * 5.992582321166992
Epoch 1780, val loss: 1.111580729484558
Epoch 1790, training loss: 11.996041297912598 = 0.010922865010797977 + 2.0 * 5.992559432983398
Epoch 1790, val loss: 1.1139050722122192
Epoch 1800, training loss: 11.99077033996582 = 0.010770890861749649 + 2.0 * 5.989999771118164
Epoch 1800, val loss: 1.1164758205413818
Epoch 1810, training loss: 11.988408088684082 = 0.010622306726872921 + 2.0 * 5.988893032073975
Epoch 1810, val loss: 1.118757724761963
Epoch 1820, training loss: 11.991800308227539 = 0.010475161485373974 + 2.0 * 5.990662574768066
Epoch 1820, val loss: 1.121187686920166
Epoch 1830, training loss: 11.989679336547852 = 0.010330446995794773 + 2.0 * 5.9896745681762695
Epoch 1830, val loss: 1.123551368713379
Epoch 1840, training loss: 11.994757652282715 = 0.01019025407731533 + 2.0 * 5.992283821105957
Epoch 1840, val loss: 1.1258800029754639
Epoch 1850, training loss: 11.987807273864746 = 0.010053444653749466 + 2.0 * 5.988876819610596
Epoch 1850, val loss: 1.1282050609588623
Epoch 1860, training loss: 11.98710823059082 = 0.00991871114820242 + 2.0 * 5.9885945320129395
Epoch 1860, val loss: 1.1305062770843506
Epoch 1870, training loss: 11.990253448486328 = 0.009785226546227932 + 2.0 * 5.990233898162842
Epoch 1870, val loss: 1.1328824758529663
Epoch 1880, training loss: 11.985977172851562 = 0.009655407629907131 + 2.0 * 5.988161087036133
Epoch 1880, val loss: 1.135166883468628
Epoch 1890, training loss: 11.9882173538208 = 0.00952833890914917 + 2.0 * 5.989344596862793
Epoch 1890, val loss: 1.137486457824707
Epoch 1900, training loss: 11.986433982849121 = 0.009403254836797714 + 2.0 * 5.988515377044678
Epoch 1900, val loss: 1.1398009061813354
Epoch 1910, training loss: 11.98881721496582 = 0.00928131490945816 + 2.0 * 5.989768028259277
Epoch 1910, val loss: 1.142008900642395
Epoch 1920, training loss: 11.986990928649902 = 0.00916280411183834 + 2.0 * 5.9889140129089355
Epoch 1920, val loss: 1.1442850828170776
Epoch 1930, training loss: 11.98675537109375 = 0.009046696126461029 + 2.0 * 5.98885440826416
Epoch 1930, val loss: 1.1464152336120605
Epoch 1940, training loss: 11.980999946594238 = 0.008933157660067081 + 2.0 * 5.9860334396362305
Epoch 1940, val loss: 1.1484931707382202
Epoch 1950, training loss: 11.981882095336914 = 0.00882021989673376 + 2.0 * 5.986530780792236
Epoch 1950, val loss: 1.1507409811019897
Epoch 1960, training loss: 11.98807144165039 = 0.008708374574780464 + 2.0 * 5.989681720733643
Epoch 1960, val loss: 1.152966856956482
Epoch 1970, training loss: 11.983513832092285 = 0.008600578643381596 + 2.0 * 5.987456798553467
Epoch 1970, val loss: 1.1551576852798462
Epoch 1980, training loss: 11.982307434082031 = 0.008495706133544445 + 2.0 * 5.986906051635742
Epoch 1980, val loss: 1.1572797298431396
Epoch 1990, training loss: 11.984269142150879 = 0.008391925133764744 + 2.0 * 5.987938404083252
Epoch 1990, val loss: 1.1593046188354492
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8074
Overall ASR: 0.7786
Flip ASR: 0.7333/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.698060989379883 = 1.9504259824752808 + 2.0 * 8.373817443847656
Epoch 0, val loss: 1.961035132408142
Epoch 10, training loss: 18.68609619140625 = 1.9395248889923096 + 2.0 * 8.373285293579102
Epoch 10, val loss: 1.949920415878296
Epoch 20, training loss: 18.66572380065918 = 1.9262309074401855 + 2.0 * 8.369746208190918
Epoch 20, val loss: 1.935694694519043
Epoch 30, training loss: 18.599626541137695 = 1.9082304239273071 + 2.0 * 8.345698356628418
Epoch 30, val loss: 1.9162039756774902
Epoch 40, training loss: 18.293415069580078 = 1.8868671655654907 + 2.0 * 8.20327377319336
Epoch 40, val loss: 1.8940685987472534
Epoch 50, training loss: 17.128189086914062 = 1.865200161933899 + 2.0 * 7.631494522094727
Epoch 50, val loss: 1.8721321821212769
Epoch 60, training loss: 16.220447540283203 = 1.8473966121673584 + 2.0 * 7.186525344848633
Epoch 60, val loss: 1.854540467262268
Epoch 70, training loss: 15.512529373168945 = 1.83573579788208 + 2.0 * 6.838397026062012
Epoch 70, val loss: 1.8416764736175537
Epoch 80, training loss: 15.104024887084961 = 1.825271487236023 + 2.0 * 6.639376640319824
Epoch 80, val loss: 1.8291418552398682
Epoch 90, training loss: 14.858155250549316 = 1.8140733242034912 + 2.0 * 6.522040843963623
Epoch 90, val loss: 1.8161169290542603
Epoch 100, training loss: 14.693865776062012 = 1.8029696941375732 + 2.0 * 6.44544792175293
Epoch 100, val loss: 1.8035539388656616
Epoch 110, training loss: 14.556136131286621 = 1.7932167053222656 + 2.0 * 6.381459712982178
Epoch 110, val loss: 1.7926493883132935
Epoch 120, training loss: 14.463356971740723 = 1.7841222286224365 + 2.0 * 6.3396172523498535
Epoch 120, val loss: 1.782453179359436
Epoch 130, training loss: 14.391962051391602 = 1.7742208242416382 + 2.0 * 6.308870792388916
Epoch 130, val loss: 1.7715178728103638
Epoch 140, training loss: 14.332788467407227 = 1.763232707977295 + 2.0 * 6.284777641296387
Epoch 140, val loss: 1.760003924369812
Epoch 150, training loss: 14.279315948486328 = 1.7514621019363403 + 2.0 * 6.263926982879639
Epoch 150, val loss: 1.7483352422714233
Epoch 160, training loss: 14.232723236083984 = 1.7387583255767822 + 2.0 * 6.246982574462891
Epoch 160, val loss: 1.7364578247070312
Epoch 170, training loss: 14.183211326599121 = 1.7247169017791748 + 2.0 * 6.229247093200684
Epoch 170, val loss: 1.723872184753418
Epoch 180, training loss: 14.136213302612305 = 1.7086877822875977 + 2.0 * 6.2137627601623535
Epoch 180, val loss: 1.709958553314209
Epoch 190, training loss: 14.089781761169434 = 1.6901077032089233 + 2.0 * 6.1998372077941895
Epoch 190, val loss: 1.6941832304000854
Epoch 200, training loss: 14.044225692749023 = 1.6683831214904785 + 2.0 * 6.187921524047852
Epoch 200, val loss: 1.6760720014572144
Epoch 210, training loss: 13.998917579650879 = 1.6426578760147095 + 2.0 * 6.17812967300415
Epoch 210, val loss: 1.654942512512207
Epoch 220, training loss: 13.94826602935791 = 1.6123831272125244 + 2.0 * 6.167941570281982
Epoch 220, val loss: 1.6302146911621094
Epoch 230, training loss: 13.8981351852417 = 1.5767396688461304 + 2.0 * 6.160697937011719
Epoch 230, val loss: 1.601367712020874
Epoch 240, training loss: 13.841689109802246 = 1.5357197523117065 + 2.0 * 6.152984619140625
Epoch 240, val loss: 1.5679727792739868
Epoch 250, training loss: 13.782710075378418 = 1.4888781309127808 + 2.0 * 6.146915912628174
Epoch 250, val loss: 1.530195951461792
Epoch 260, training loss: 13.720182418823242 = 1.4373668432235718 + 2.0 * 6.1414079666137695
Epoch 260, val loss: 1.4887477159500122
Epoch 270, training loss: 13.659173965454102 = 1.382260799407959 + 2.0 * 6.138456344604492
Epoch 270, val loss: 1.444551944732666
Epoch 280, training loss: 13.591827392578125 = 1.3259881734848022 + 2.0 * 6.132919788360596
Epoch 280, val loss: 1.3997843265533447
Epoch 290, training loss: 13.526561737060547 = 1.2698267698287964 + 2.0 * 6.1283674240112305
Epoch 290, val loss: 1.3552860021591187
Epoch 300, training loss: 13.465265274047852 = 1.2143759727478027 + 2.0 * 6.125444412231445
Epoch 300, val loss: 1.3116796016693115
Epoch 310, training loss: 13.403851509094238 = 1.161519169807434 + 2.0 * 6.121166229248047
Epoch 310, val loss: 1.2703241109848022
Epoch 320, training loss: 13.3475980758667 = 1.111449122428894 + 2.0 * 6.118074417114258
Epoch 320, val loss: 1.2314788103103638
Epoch 330, training loss: 13.29061222076416 = 1.0631505250930786 + 2.0 * 6.1137309074401855
Epoch 330, val loss: 1.1944016218185425
Epoch 340, training loss: 13.236943244934082 = 1.0161638259887695 + 2.0 * 6.110389709472656
Epoch 340, val loss: 1.1586189270019531
Epoch 350, training loss: 13.190396308898926 = 0.9709309339523315 + 2.0 * 6.109732627868652
Epoch 350, val loss: 1.124480128288269
Epoch 360, training loss: 13.139483451843262 = 0.9281822443008423 + 2.0 * 6.105650424957275
Epoch 360, val loss: 1.0923935174942017
Epoch 370, training loss: 13.090997695922852 = 0.8874493837356567 + 2.0 * 6.101774215698242
Epoch 370, val loss: 1.0621697902679443
Epoch 380, training loss: 13.052461624145508 = 0.8487136363983154 + 2.0 * 6.101873874664307
Epoch 380, val loss: 1.0338057279586792
Epoch 390, training loss: 13.0078125 = 0.8123759031295776 + 2.0 * 6.097718238830566
Epoch 390, val loss: 1.0077532529830933
Epoch 400, training loss: 12.967543601989746 = 0.778364360332489 + 2.0 * 6.094589710235596
Epoch 400, val loss: 0.9837830662727356
Epoch 410, training loss: 12.928502082824707 = 0.7464323043823242 + 2.0 * 6.091034889221191
Epoch 410, val loss: 0.9617893695831299
Epoch 420, training loss: 12.903364181518555 = 0.7164067029953003 + 2.0 * 6.093478679656982
Epoch 420, val loss: 0.9417262077331543
Epoch 430, training loss: 12.863966941833496 = 0.6886917948722839 + 2.0 * 6.087637424468994
Epoch 430, val loss: 0.9237937927246094
Epoch 440, training loss: 12.830763816833496 = 0.6628037691116333 + 2.0 * 6.083980083465576
Epoch 440, val loss: 0.9077076315879822
Epoch 450, training loss: 12.800586700439453 = 0.6384895443916321 + 2.0 * 6.081048488616943
Epoch 450, val loss: 0.8931309580802917
Epoch 460, training loss: 12.782683372497559 = 0.6154810786247253 + 2.0 * 6.083600997924805
Epoch 460, val loss: 0.8799891471862793
Epoch 470, training loss: 12.750407218933105 = 0.5939685702323914 + 2.0 * 6.078219413757324
Epoch 470, val loss: 0.8682156205177307
Epoch 480, training loss: 12.730278015136719 = 0.5736317038536072 + 2.0 * 6.0783233642578125
Epoch 480, val loss: 0.8576153516769409
Epoch 490, training loss: 12.705826759338379 = 0.5543718338012695 + 2.0 * 6.075727462768555
Epoch 490, val loss: 0.8480249047279358
Epoch 500, training loss: 12.67876148223877 = 0.5360339283943176 + 2.0 * 6.071363925933838
Epoch 500, val loss: 0.8392887711524963
Epoch 510, training loss: 12.659947395324707 = 0.5183717608451843 + 2.0 * 6.0707879066467285
Epoch 510, val loss: 0.8313912153244019
Epoch 520, training loss: 12.649280548095703 = 0.5015187859535217 + 2.0 * 6.073880672454834
Epoch 520, val loss: 0.8243277072906494
Epoch 530, training loss: 12.62149429321289 = 0.48537173867225647 + 2.0 * 6.068061351776123
Epoch 530, val loss: 0.8179682493209839
Epoch 540, training loss: 12.599343299865723 = 0.46965116262435913 + 2.0 * 6.064846038818359
Epoch 540, val loss: 0.8121739029884338
Epoch 550, training loss: 12.587904930114746 = 0.45423954725265503 + 2.0 * 6.066832542419434
Epoch 550, val loss: 0.806945264339447
Epoch 560, training loss: 12.567265510559082 = 0.43930643796920776 + 2.0 * 6.063979625701904
Epoch 560, val loss: 0.8021921515464783
Epoch 570, training loss: 12.545761108398438 = 0.42462030053138733 + 2.0 * 6.060570240020752
Epoch 570, val loss: 0.7979244589805603
Epoch 580, training loss: 12.53125 = 0.4100947380065918 + 2.0 * 6.060577392578125
Epoch 580, val loss: 0.794081449508667
Epoch 590, training loss: 12.526230812072754 = 0.39586228132247925 + 2.0 * 6.065184116363525
Epoch 590, val loss: 0.7906239032745361
Epoch 600, training loss: 12.494999885559082 = 0.38198715448379517 + 2.0 * 6.056506156921387
Epoch 600, val loss: 0.7875794768333435
Epoch 610, training loss: 12.479750633239746 = 0.36825263500213623 + 2.0 * 6.05574893951416
Epoch 610, val loss: 0.784911036491394
Epoch 620, training loss: 12.46945858001709 = 0.3545987606048584 + 2.0 * 6.057429790496826
Epoch 620, val loss: 0.7826194167137146
Epoch 630, training loss: 12.451080322265625 = 0.34120458364486694 + 2.0 * 6.054937839508057
Epoch 630, val loss: 0.7806850075721741
Epoch 640, training loss: 12.433377265930176 = 0.32797884941101074 + 2.0 * 6.052699089050293
Epoch 640, val loss: 0.7792161703109741
Epoch 650, training loss: 12.424848556518555 = 0.31498751044273376 + 2.0 * 6.054930686950684
Epoch 650, val loss: 0.7781482338905334
Epoch 660, training loss: 12.410852432250977 = 0.3023053705692291 + 2.0 * 6.05427360534668
Epoch 660, val loss: 0.7775994539260864
Epoch 670, training loss: 12.387650489807129 = 0.29004955291748047 + 2.0 * 6.048800468444824
Epoch 670, val loss: 0.7774814367294312
Epoch 680, training loss: 12.37551498413086 = 0.2781347334384918 + 2.0 * 6.048690319061279
Epoch 680, val loss: 0.7778496146202087
Epoch 690, training loss: 12.367818832397461 = 0.2666071951389313 + 2.0 * 6.050605773925781
Epoch 690, val loss: 0.7787299156188965
Epoch 700, training loss: 12.353912353515625 = 0.25563347339630127 + 2.0 * 6.049139499664307
Epoch 700, val loss: 0.7801230549812317
Epoch 710, training loss: 12.335678100585938 = 0.24517560005187988 + 2.0 * 6.045251369476318
Epoch 710, val loss: 0.7819817066192627
Epoch 720, training loss: 12.32493782043457 = 0.23519103229045868 + 2.0 * 6.044873237609863
Epoch 720, val loss: 0.7842887043952942
Epoch 730, training loss: 12.321749687194824 = 0.225652277469635 + 2.0 * 6.048048496246338
Epoch 730, val loss: 0.787043035030365
Epoch 740, training loss: 12.310641288757324 = 0.21660542488098145 + 2.0 * 6.047018051147461
Epoch 740, val loss: 0.7901927828788757
Epoch 750, training loss: 12.292648315429688 = 0.2080446034669876 + 2.0 * 6.042301654815674
Epoch 750, val loss: 0.7936899065971375
Epoch 760, training loss: 12.289148330688477 = 0.19987140595912933 + 2.0 * 6.044638633728027
Epoch 760, val loss: 0.7975110411643982
Epoch 770, training loss: 12.274327278137207 = 0.19209524989128113 + 2.0 * 6.041116237640381
Epoch 770, val loss: 0.8015823364257812
Epoch 780, training loss: 12.27524185180664 = 0.18467295169830322 + 2.0 * 6.045284271240234
Epoch 780, val loss: 0.80599445104599
Epoch 790, training loss: 12.260004997253418 = 0.17757199704647064 + 2.0 * 6.0412163734436035
Epoch 790, val loss: 0.8106529116630554
Epoch 800, training loss: 12.246219635009766 = 0.17076270282268524 + 2.0 * 6.037728309631348
Epoch 800, val loss: 0.8154840469360352
Epoch 810, training loss: 12.237713813781738 = 0.16422539949417114 + 2.0 * 6.036744117736816
Epoch 810, val loss: 0.820533275604248
Epoch 820, training loss: 12.253718376159668 = 0.15795688331127167 + 2.0 * 6.04788064956665
Epoch 820, val loss: 0.8258047699928284
Epoch 830, training loss: 12.223987579345703 = 0.15198573470115662 + 2.0 * 6.036000728607178
Epoch 830, val loss: 0.8311769962310791
Epoch 840, training loss: 12.215975761413574 = 0.146266371011734 + 2.0 * 6.034854888916016
Epoch 840, val loss: 0.8367723822593689
Epoch 850, training loss: 12.208719253540039 = 0.14076165854930878 + 2.0 * 6.0339789390563965
Epoch 850, val loss: 0.8424878716468811
Epoch 860, training loss: 12.213156700134277 = 0.13547931611537933 + 2.0 * 6.038838863372803
Epoch 860, val loss: 0.8482964634895325
Epoch 870, training loss: 12.21739387512207 = 0.1304587721824646 + 2.0 * 6.0434675216674805
Epoch 870, val loss: 0.8541786670684814
Epoch 880, training loss: 12.195571899414062 = 0.1256716400384903 + 2.0 * 6.034950256347656
Epoch 880, val loss: 0.8601654171943665
Epoch 890, training loss: 12.183048248291016 = 0.12111128866672516 + 2.0 * 6.03096866607666
Epoch 890, val loss: 0.8662970066070557
Epoch 900, training loss: 12.177010536193848 = 0.11671367287635803 + 2.0 * 6.030148506164551
Epoch 900, val loss: 0.872483491897583
Epoch 910, training loss: 12.173985481262207 = 0.11246916651725769 + 2.0 * 6.030758380889893
Epoch 910, val loss: 0.878753125667572
Epoch 920, training loss: 12.167461395263672 = 0.10840629786252975 + 2.0 * 6.02952766418457
Epoch 920, val loss: 0.8850639462471008
Epoch 930, training loss: 12.163613319396973 = 0.10452444106340408 + 2.0 * 6.029544353485107
Epoch 930, val loss: 0.8914390802383423
Epoch 940, training loss: 12.160367012023926 = 0.10078658163547516 + 2.0 * 6.02979040145874
Epoch 940, val loss: 0.8980014324188232
Epoch 950, training loss: 12.151694297790527 = 0.09720291197299957 + 2.0 * 6.02724552154541
Epoch 950, val loss: 0.9044262170791626
Epoch 960, training loss: 12.14971923828125 = 0.09378395229578018 + 2.0 * 6.02796745300293
Epoch 960, val loss: 0.9109111428260803
Epoch 970, training loss: 12.142162322998047 = 0.0905035212635994 + 2.0 * 6.025829315185547
Epoch 970, val loss: 0.9175506234169006
Epoch 980, training loss: 12.144471168518066 = 0.08734316378831863 + 2.0 * 6.028563976287842
Epoch 980, val loss: 0.9242056608200073
Epoch 990, training loss: 12.14767074584961 = 0.08431227505207062 + 2.0 * 6.031679153442383
Epoch 990, val loss: 0.9307609796524048
Epoch 1000, training loss: 12.13247013092041 = 0.08143972605466843 + 2.0 * 6.025515079498291
Epoch 1000, val loss: 0.937306821346283
Epoch 1010, training loss: 12.127311706542969 = 0.07869716733694077 + 2.0 * 6.0243072509765625
Epoch 1010, val loss: 0.9439103007316589
Epoch 1020, training loss: 12.1220064163208 = 0.07603634893894196 + 2.0 * 6.022984981536865
Epoch 1020, val loss: 0.9505118131637573
Epoch 1030, training loss: 12.118597030639648 = 0.07346903532743454 + 2.0 * 6.022563934326172
Epoch 1030, val loss: 0.9571185111999512
Epoch 1040, training loss: 12.120454788208008 = 0.07100362330675125 + 2.0 * 6.024725437164307
Epoch 1040, val loss: 0.9636457562446594
Epoch 1050, training loss: 12.118117332458496 = 0.06866259127855301 + 2.0 * 6.0247273445129395
Epoch 1050, val loss: 0.9701293110847473
Epoch 1060, training loss: 12.10786247253418 = 0.0664089247584343 + 2.0 * 6.020726680755615
Epoch 1060, val loss: 0.9767442345619202
Epoch 1070, training loss: 12.104227066040039 = 0.06423155218362808 + 2.0 * 6.019997596740723
Epoch 1070, val loss: 0.9832690358161926
Epoch 1080, training loss: 12.100226402282715 = 0.0621352344751358 + 2.0 * 6.019045352935791
Epoch 1080, val loss: 0.9898143410682678
Epoch 1090, training loss: 12.10140609741211 = 0.06011684983968735 + 2.0 * 6.020644664764404
Epoch 1090, val loss: 0.9963634610176086
Epoch 1100, training loss: 12.106185913085938 = 0.05818675458431244 + 2.0 * 6.0239996910095215
Epoch 1100, val loss: 1.002755880355835
Epoch 1110, training loss: 12.094225883483887 = 0.05635721981525421 + 2.0 * 6.01893424987793
Epoch 1110, val loss: 1.009105920791626
Epoch 1120, training loss: 12.089245796203613 = 0.05460638180375099 + 2.0 * 6.017319679260254
Epoch 1120, val loss: 1.0155400037765503
Epoch 1130, training loss: 12.086771965026855 = 0.052912529557943344 + 2.0 * 6.016929626464844
Epoch 1130, val loss: 1.0219086408615112
Epoch 1140, training loss: 12.0872220993042 = 0.051275670528411865 + 2.0 * 6.01797342300415
Epoch 1140, val loss: 1.028225064277649
Epoch 1150, training loss: 12.087206840515137 = 0.04970596730709076 + 2.0 * 6.0187506675720215
Epoch 1150, val loss: 1.0344399213790894
Epoch 1160, training loss: 12.084297180175781 = 0.04821417108178139 + 2.0 * 6.018041610717773
Epoch 1160, val loss: 1.0405728816986084
Epoch 1170, training loss: 12.076213836669922 = 0.04678122326731682 + 2.0 * 6.014716148376465
Epoch 1170, val loss: 1.0468014478683472
Epoch 1180, training loss: 12.074868202209473 = 0.04539752006530762 + 2.0 * 6.014735221862793
Epoch 1180, val loss: 1.0529825687408447
Epoch 1190, training loss: 12.07880687713623 = 0.044061508029699326 + 2.0 * 6.0173726081848145
Epoch 1190, val loss: 1.0590684413909912
Epoch 1200, training loss: 12.080921173095703 = 0.04278554767370224 + 2.0 * 6.019067764282227
Epoch 1200, val loss: 1.0650080442428589
Epoch 1210, training loss: 12.073894500732422 = 0.04157547280192375 + 2.0 * 6.016159534454346
Epoch 1210, val loss: 1.0709221363067627
Epoch 1220, training loss: 12.065755844116211 = 0.04041182994842529 + 2.0 * 6.012671947479248
Epoch 1220, val loss: 1.0768901109695435
Epoch 1230, training loss: 12.064593315124512 = 0.039284251630306244 + 2.0 * 6.0126543045043945
Epoch 1230, val loss: 1.0828006267547607
Epoch 1240, training loss: 12.08987045288086 = 0.03820338100194931 + 2.0 * 6.025833606719971
Epoch 1240, val loss: 1.0886138677597046
Epoch 1250, training loss: 12.06405258178711 = 0.03715766966342926 + 2.0 * 6.013447284698486
Epoch 1250, val loss: 1.0942224264144897
Epoch 1260, training loss: 12.059372901916504 = 0.03616400063037872 + 2.0 * 6.011604309082031
Epoch 1260, val loss: 1.099896788597107
Epoch 1270, training loss: 12.057249069213867 = 0.035201042890548706 + 2.0 * 6.011023998260498
Epoch 1270, val loss: 1.1056151390075684
Epoch 1280, training loss: 12.070816040039062 = 0.034268755465745926 + 2.0 * 6.018273830413818
Epoch 1280, val loss: 1.111180067062378
Epoch 1290, training loss: 12.057653427124023 = 0.033377617597579956 + 2.0 * 6.0121378898620605
Epoch 1290, val loss: 1.1166261434555054
Epoch 1300, training loss: 12.052290916442871 = 0.032520800828933716 + 2.0 * 6.009884834289551
Epoch 1300, val loss: 1.1221338510513306
Epoch 1310, training loss: 12.061334609985352 = 0.03169125318527222 + 2.0 * 6.014821529388428
Epoch 1310, val loss: 1.1275906562805176
Epoch 1320, training loss: 12.050167083740234 = 0.030892979353666306 + 2.0 * 6.009636878967285
Epoch 1320, val loss: 1.13291335105896
Epoch 1330, training loss: 12.04751205444336 = 0.03012482076883316 + 2.0 * 6.008693695068359
Epoch 1330, val loss: 1.1382408142089844
Epoch 1340, training loss: 12.065461158752441 = 0.02938060648739338 + 2.0 * 6.018040180206299
Epoch 1340, val loss: 1.1434699296951294
Epoch 1350, training loss: 12.045392036437988 = 0.028670916333794594 + 2.0 * 6.008360385894775
Epoch 1350, val loss: 1.1485940217971802
Epoch 1360, training loss: 12.044378280639648 = 0.027985190972685814 + 2.0 * 6.0081963539123535
Epoch 1360, val loss: 1.1537830829620361
Epoch 1370, training loss: 12.050069808959961 = 0.02732119709253311 + 2.0 * 6.011374473571777
Epoch 1370, val loss: 1.1588704586029053
Epoch 1380, training loss: 12.041537284851074 = 0.02667967788875103 + 2.0 * 6.0074286460876465
Epoch 1380, val loss: 1.1638761758804321
Epoch 1390, training loss: 12.039034843444824 = 0.026061931625008583 + 2.0 * 6.006486415863037
Epoch 1390, val loss: 1.1689116954803467
Epoch 1400, training loss: 12.045684814453125 = 0.025460146367549896 + 2.0 * 6.010112285614014
Epoch 1400, val loss: 1.1739115715026855
Epoch 1410, training loss: 12.036187171936035 = 0.024877985939383507 + 2.0 * 6.005654811859131
Epoch 1410, val loss: 1.1787937879562378
Epoch 1420, training loss: 12.034809112548828 = 0.024317150935530663 + 2.0 * 6.005246162414551
Epoch 1420, val loss: 1.1837633848190308
Epoch 1430, training loss: 12.053587913513184 = 0.02377152256667614 + 2.0 * 6.014908313751221
Epoch 1430, val loss: 1.1886180639266968
Epoch 1440, training loss: 12.039414405822754 = 0.023252325132489204 + 2.0 * 6.008080959320068
Epoch 1440, val loss: 1.1933069229125977
Epoch 1450, training loss: 12.030352592468262 = 0.022750068455934525 + 2.0 * 6.003801345825195
Epoch 1450, val loss: 1.198066234588623
Epoch 1460, training loss: 12.028450012207031 = 0.02226148173213005 + 2.0 * 6.00309419631958
Epoch 1460, val loss: 1.2028213739395142
Epoch 1470, training loss: 12.027971267700195 = 0.021783823147416115 + 2.0 * 6.003093719482422
Epoch 1470, val loss: 1.2074933052062988
Epoch 1480, training loss: 12.049201011657715 = 0.021318556740880013 + 2.0 * 6.013941287994385
Epoch 1480, val loss: 1.2120510339736938
Epoch 1490, training loss: 12.03609561920166 = 0.02087506279349327 + 2.0 * 6.007610321044922
Epoch 1490, val loss: 1.2164855003356934
Epoch 1500, training loss: 12.030192375183105 = 0.02044735476374626 + 2.0 * 6.0048723220825195
Epoch 1500, val loss: 1.2210017442703247
Epoch 1510, training loss: 12.025165557861328 = 0.02003166265785694 + 2.0 * 6.002566814422607
Epoch 1510, val loss: 1.2254849672317505
Epoch 1520, training loss: 12.023666381835938 = 0.01962616667151451 + 2.0 * 6.002019882202148
Epoch 1520, val loss: 1.2299025058746338
Epoch 1530, training loss: 12.021439552307129 = 0.01923159323632717 + 2.0 * 6.00110387802124
Epoch 1530, val loss: 1.2343015670776367
Epoch 1540, training loss: 12.034400939941406 = 0.018846871331334114 + 2.0 * 6.007777214050293
Epoch 1540, val loss: 1.238635778427124
Epoch 1550, training loss: 12.026662826538086 = 0.018473731353878975 + 2.0 * 6.00409460067749
Epoch 1550, val loss: 1.2427912950515747
Epoch 1560, training loss: 12.02074146270752 = 0.01811746321618557 + 2.0 * 6.001311779022217
Epoch 1560, val loss: 1.2470685243606567
Epoch 1570, training loss: 12.018126487731934 = 0.0177681352943182 + 2.0 * 6.000179290771484
Epoch 1570, val loss: 1.2513082027435303
Epoch 1580, training loss: 12.044517517089844 = 0.01742597483098507 + 2.0 * 6.013545989990234
Epoch 1580, val loss: 1.2554913759231567
Epoch 1590, training loss: 12.025507926940918 = 0.017097322270274162 + 2.0 * 6.004205226898193
Epoch 1590, val loss: 1.2594841718673706
Epoch 1600, training loss: 12.015458106994629 = 0.01678241603076458 + 2.0 * 5.999337673187256
Epoch 1600, val loss: 1.2635419368743896
Epoch 1610, training loss: 12.014215469360352 = 0.01647292636334896 + 2.0 * 5.998871326446533
Epoch 1610, val loss: 1.2676711082458496
Epoch 1620, training loss: 12.012496948242188 = 0.01616765186190605 + 2.0 * 5.998164653778076
Epoch 1620, val loss: 1.2717217206954956
Epoch 1630, training loss: 12.01729965209961 = 0.015869732946157455 + 2.0 * 6.0007147789001465
Epoch 1630, val loss: 1.2756948471069336
Epoch 1640, training loss: 12.020893096923828 = 0.015581841580569744 + 2.0 * 6.002655506134033
Epoch 1640, val loss: 1.2795413732528687
Epoch 1650, training loss: 12.020512580871582 = 0.015305032022297382 + 2.0 * 6.002604007720947
Epoch 1650, val loss: 1.2833741903305054
Epoch 1660, training loss: 12.009690284729004 = 0.015038322657346725 + 2.0 * 5.997325897216797
Epoch 1660, val loss: 1.2872685194015503
Epoch 1670, training loss: 12.009712219238281 = 0.014776542782783508 + 2.0 * 5.997467994689941
Epoch 1670, val loss: 1.291191577911377
Epoch 1680, training loss: 12.01212215423584 = 0.014517893083393574 + 2.0 * 5.998802185058594
Epoch 1680, val loss: 1.2950384616851807
Epoch 1690, training loss: 12.011163711547852 = 0.014265813864767551 + 2.0 * 5.998448848724365
Epoch 1690, val loss: 1.2987669706344604
Epoch 1700, training loss: 12.008506774902344 = 0.014022691175341606 + 2.0 * 5.997241973876953
Epoch 1700, val loss: 1.3024702072143555
Epoch 1710, training loss: 12.010066032409668 = 0.013782924041152 + 2.0 * 5.998141765594482
Epoch 1710, val loss: 1.3062483072280884
Epoch 1720, training loss: 12.011673927307129 = 0.01354842260479927 + 2.0 * 5.999062538146973
Epoch 1720, val loss: 1.3099461793899536
Epoch 1730, training loss: 12.005827903747559 = 0.013325301930308342 + 2.0 * 5.996251106262207
Epoch 1730, val loss: 1.3135923147201538
Epoch 1740, training loss: 12.007542610168457 = 0.013105744495987892 + 2.0 * 5.997218608856201
Epoch 1740, val loss: 1.3172403573989868
Epoch 1750, training loss: 12.009238243103027 = 0.0128905288875103 + 2.0 * 5.998173713684082
Epoch 1750, val loss: 1.3208476305007935
Epoch 1760, training loss: 12.006009101867676 = 0.012682285159826279 + 2.0 * 5.996663570404053
Epoch 1760, val loss: 1.324349284172058
Epoch 1770, training loss: 12.004520416259766 = 0.012478947639465332 + 2.0 * 5.996020793914795
Epoch 1770, val loss: 1.32790207862854
Epoch 1780, training loss: 12.005181312561035 = 0.012279568240046501 + 2.0 * 5.996450901031494
Epoch 1780, val loss: 1.3314136266708374
Epoch 1790, training loss: 12.007389068603516 = 0.01208529807627201 + 2.0 * 5.997652053833008
Epoch 1790, val loss: 1.334810733795166
Epoch 1800, training loss: 12.000275611877441 = 0.011895470321178436 + 2.0 * 5.994190216064453
Epoch 1800, val loss: 1.3382965326309204
Epoch 1810, training loss: 12.001544952392578 = 0.011709725484251976 + 2.0 * 5.994917392730713
Epoch 1810, val loss: 1.3417282104492188
Epoch 1820, training loss: 12.010193824768066 = 0.011528437957167625 + 2.0 * 5.999332904815674
Epoch 1820, val loss: 1.345117449760437
Epoch 1830, training loss: 11.9993257522583 = 0.011354702524840832 + 2.0 * 5.993985652923584
Epoch 1830, val loss: 1.3483375310897827
Epoch 1840, training loss: 12.001731872558594 = 0.011184891685843468 + 2.0 * 5.995273590087891
Epoch 1840, val loss: 1.3517142534255981
Epoch 1850, training loss: 12.000415802001953 = 0.011016974225640297 + 2.0 * 5.994699478149414
Epoch 1850, val loss: 1.354996681213379
Epoch 1860, training loss: 11.997032165527344 = 0.010853392072021961 + 2.0 * 5.993089199066162
Epoch 1860, val loss: 1.3583492040634155
Epoch 1870, training loss: 11.997832298278809 = 0.01069230493158102 + 2.0 * 5.993569850921631
Epoch 1870, val loss: 1.3616057634353638
Epoch 1880, training loss: 12.002715110778809 = 0.010533801279962063 + 2.0 * 5.996090888977051
Epoch 1880, val loss: 1.3648264408111572
Epoch 1890, training loss: 11.998697280883789 = 0.010378718376159668 + 2.0 * 5.99415922164917
Epoch 1890, val loss: 1.368012547492981
Epoch 1900, training loss: 12.00539493560791 = 0.01023032795637846 + 2.0 * 5.99758243560791
Epoch 1900, val loss: 1.3711968660354614
Epoch 1910, training loss: 11.99764633178711 = 0.01008550077676773 + 2.0 * 5.993780612945557
Epoch 1910, val loss: 1.3742998838424683
Epoch 1920, training loss: 11.993471145629883 = 0.009943782351911068 + 2.0 * 5.991763591766357
Epoch 1920, val loss: 1.3774679899215698
Epoch 1930, training loss: 11.99167537689209 = 0.009803258813917637 + 2.0 * 5.990936279296875
Epoch 1930, val loss: 1.3806408643722534
Epoch 1940, training loss: 11.997603416442871 = 0.009664556942880154 + 2.0 * 5.993969440460205
Epoch 1940, val loss: 1.3837676048278809
Epoch 1950, training loss: 11.99375057220459 = 0.00952933356165886 + 2.0 * 5.992110729217529
Epoch 1950, val loss: 1.38673734664917
Epoch 1960, training loss: 12.001708984375 = 0.009399732574820518 + 2.0 * 5.99615478515625
Epoch 1960, val loss: 1.3897041082382202
Epoch 1970, training loss: 11.99363899230957 = 0.009272904135286808 + 2.0 * 5.992183208465576
Epoch 1970, val loss: 1.3927063941955566
Epoch 1980, training loss: 11.991930961608887 = 0.009148920886218548 + 2.0 * 5.991391181945801
Epoch 1980, val loss: 1.395706057548523
Epoch 1990, training loss: 11.99460506439209 = 0.009026521816849709 + 2.0 * 5.992789268493652
Epoch 1990, val loss: 1.3986932039260864
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7778
Overall ASR: 0.7454
Flip ASR: 0.7067/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.68549346923828 = 1.937779426574707 + 2.0 * 8.373856544494629
Epoch 0, val loss: 1.9388576745986938
Epoch 10, training loss: 18.675107955932617 = 1.9285444021224976 + 2.0 * 8.373281478881836
Epoch 10, val loss: 1.9303405284881592
Epoch 20, training loss: 18.656049728393555 = 1.9170174598693848 + 2.0 * 8.369516372680664
Epoch 20, val loss: 1.9194921255111694
Epoch 30, training loss: 18.583892822265625 = 1.9015288352966309 + 2.0 * 8.341181755065918
Epoch 30, val loss: 1.9046694040298462
Epoch 40, training loss: 18.149635314941406 = 1.8832285404205322 + 2.0 * 8.133203506469727
Epoch 40, val loss: 1.8871041536331177
Epoch 50, training loss: 16.634920120239258 = 1.86454176902771 + 2.0 * 7.385189056396484
Epoch 50, val loss: 1.8691502809524536
Epoch 60, training loss: 15.76539134979248 = 1.8514964580535889 + 2.0 * 6.956947326660156
Epoch 60, val loss: 1.8571261167526245
Epoch 70, training loss: 15.195816040039062 = 1.8419212102890015 + 2.0 * 6.676947593688965
Epoch 70, val loss: 1.847743034362793
Epoch 80, training loss: 14.866016387939453 = 1.8328158855438232 + 2.0 * 6.516600131988525
Epoch 80, val loss: 1.8384376764297485
Epoch 90, training loss: 14.690494537353516 = 1.8244328498840332 + 2.0 * 6.43303108215332
Epoch 90, val loss: 1.8295702934265137
Epoch 100, training loss: 14.568994522094727 = 1.8163474798202515 + 2.0 * 6.376323699951172
Epoch 100, val loss: 1.8212248086929321
Epoch 110, training loss: 14.47786808013916 = 1.8086822032928467 + 2.0 * 6.334592819213867
Epoch 110, val loss: 1.8132156133651733
Epoch 120, training loss: 14.404973983764648 = 1.8014971017837524 + 2.0 * 6.301738262176514
Epoch 120, val loss: 1.8055126667022705
Epoch 130, training loss: 14.344892501831055 = 1.7946128845214844 + 2.0 * 6.275139808654785
Epoch 130, val loss: 1.7981889247894287
Epoch 140, training loss: 14.297395706176758 = 1.787677526473999 + 2.0 * 6.25485897064209
Epoch 140, val loss: 1.791098713874817
Epoch 150, training loss: 14.252372741699219 = 1.780243992805481 + 2.0 * 6.236064434051514
Epoch 150, val loss: 1.783921241760254
Epoch 160, training loss: 14.215424537658691 = 1.7720375061035156 + 2.0 * 6.221693515777588
Epoch 160, val loss: 1.7764111757278442
Epoch 170, training loss: 14.175043106079102 = 1.762927770614624 + 2.0 * 6.206057548522949
Epoch 170, val loss: 1.7683368921279907
Epoch 180, training loss: 14.138633728027344 = 1.7526570558547974 + 2.0 * 6.192988395690918
Epoch 180, val loss: 1.7595164775848389
Epoch 190, training loss: 14.105208396911621 = 1.7408794164657593 + 2.0 * 6.182164669036865
Epoch 190, val loss: 1.749694585800171
Epoch 200, training loss: 14.072078704833984 = 1.7274315357208252 + 2.0 * 6.172323703765869
Epoch 200, val loss: 1.7386523485183716
Epoch 210, training loss: 14.036965370178223 = 1.711919903755188 + 2.0 * 6.162522792816162
Epoch 210, val loss: 1.7260262966156006
Epoch 220, training loss: 14.000828742980957 = 1.6937764883041382 + 2.0 * 6.153526306152344
Epoch 220, val loss: 1.7113207578659058
Epoch 230, training loss: 13.974486351013184 = 1.6723833084106445 + 2.0 * 6.1510515213012695
Epoch 230, val loss: 1.694021463394165
Epoch 240, training loss: 13.926952362060547 = 1.6475319862365723 + 2.0 * 6.139709949493408
Epoch 240, val loss: 1.673810362815857
Epoch 250, training loss: 13.884889602661133 = 1.6184990406036377 + 2.0 * 6.133195400238037
Epoch 250, val loss: 1.6502424478530884
Epoch 260, training loss: 13.839994430541992 = 1.5845057964324951 + 2.0 * 6.127744197845459
Epoch 260, val loss: 1.6226093769073486
Epoch 270, training loss: 13.79359245300293 = 1.5454280376434326 + 2.0 * 6.124082088470459
Epoch 270, val loss: 1.590575098991394
Epoch 280, training loss: 13.738277435302734 = 1.5012308359146118 + 2.0 * 6.118523120880127
Epoch 280, val loss: 1.5543736219406128
Epoch 290, training loss: 13.682645797729492 = 1.4518779516220093 + 2.0 * 6.115384101867676
Epoch 290, val loss: 1.5138999223709106
Epoch 300, training loss: 13.621994972229004 = 1.398638367652893 + 2.0 * 6.111678123474121
Epoch 300, val loss: 1.4698834419250488
Epoch 310, training loss: 13.555410385131836 = 1.3422300815582275 + 2.0 * 6.106590270996094
Epoch 310, val loss: 1.4234915971755981
Epoch 320, training loss: 13.493358612060547 = 1.28401517868042 + 2.0 * 6.104671478271484
Epoch 320, val loss: 1.3755041360855103
Epoch 330, training loss: 13.429567337036133 = 1.225996494293213 + 2.0 * 6.101785659790039
Epoch 330, val loss: 1.3280760049819946
Epoch 340, training loss: 13.366695404052734 = 1.169687032699585 + 2.0 * 6.098504066467285
Epoch 340, val loss: 1.282151222229004
Epoch 350, training loss: 13.302518844604492 = 1.1154422760009766 + 2.0 * 6.093538284301758
Epoch 350, val loss: 1.2382841110229492
Epoch 360, training loss: 13.259988784790039 = 1.0636852979660034 + 2.0 * 6.098151683807373
Epoch 360, val loss: 1.1966084241867065
Epoch 370, training loss: 13.192380905151367 = 1.015199899673462 + 2.0 * 6.088590621948242
Epoch 370, val loss: 1.158461570739746
Epoch 380, training loss: 13.139062881469727 = 0.9696434140205383 + 2.0 * 6.084709644317627
Epoch 380, val loss: 1.122929334640503
Epoch 390, training loss: 13.089536666870117 = 0.9265331029891968 + 2.0 * 6.0815019607543945
Epoch 390, val loss: 1.0896053314208984
Epoch 400, training loss: 13.066534042358398 = 0.8856108784675598 + 2.0 * 6.090461730957031
Epoch 400, val loss: 1.0584853887557983
Epoch 410, training loss: 13.006426811218262 = 0.8478946089744568 + 2.0 * 6.07926607131958
Epoch 410, val loss: 1.0300148725509644
Epoch 420, training loss: 12.961750030517578 = 0.8124757409095764 + 2.0 * 6.074636936187744
Epoch 420, val loss: 1.0039331912994385
Epoch 430, training loss: 12.92336654663086 = 0.7788453698158264 + 2.0 * 6.07226037979126
Epoch 430, val loss: 0.9794397354125977
Epoch 440, training loss: 12.906815528869629 = 0.7467925548553467 + 2.0 * 6.080011367797852
Epoch 440, val loss: 0.9567037224769592
Epoch 450, training loss: 12.862690925598145 = 0.7170215845108032 + 2.0 * 6.072834491729736
Epoch 450, val loss: 0.93586665391922
Epoch 460, training loss: 12.823344230651855 = 0.6888710260391235 + 2.0 * 6.067236423492432
Epoch 460, val loss: 0.9169908761978149
Epoch 470, training loss: 12.790044784545898 = 0.6619748473167419 + 2.0 * 6.064034938812256
Epoch 470, val loss: 0.8993768692016602
Epoch 480, training loss: 12.768948554992676 = 0.6360965371131897 + 2.0 * 6.066425800323486
Epoch 480, val loss: 0.8829002976417542
Epoch 490, training loss: 12.735479354858398 = 0.611402153968811 + 2.0 * 6.062038421630859
Epoch 490, val loss: 0.867769718170166
Epoch 500, training loss: 12.707002639770508 = 0.5877440571784973 + 2.0 * 6.059629440307617
Epoch 500, val loss: 0.8538834452629089
Epoch 510, training loss: 12.68232250213623 = 0.5649076700210571 + 2.0 * 6.058707237243652
Epoch 510, val loss: 0.8409122824668884
Epoch 520, training loss: 12.65714168548584 = 0.5428712964057922 + 2.0 * 6.057135105133057
Epoch 520, val loss: 0.8288162350654602
Epoch 530, training loss: 12.63884162902832 = 0.5216559171676636 + 2.0 * 6.058592796325684
Epoch 530, val loss: 0.8177691698074341
Epoch 540, training loss: 12.612497329711914 = 0.501311719417572 + 2.0 * 6.055593013763428
Epoch 540, val loss: 0.807550311088562
Epoch 550, training loss: 12.586036682128906 = 0.4815004765987396 + 2.0 * 6.052268028259277
Epoch 550, val loss: 0.7981787919998169
Epoch 560, training loss: 12.56531047821045 = 0.46225348114967346 + 2.0 * 6.051528453826904
Epoch 560, val loss: 0.7894216179847717
Epoch 570, training loss: 12.547113418579102 = 0.44356635212898254 + 2.0 * 6.051773548126221
Epoch 570, val loss: 0.7813231945037842
Epoch 580, training loss: 12.535345077514648 = 0.42558473348617554 + 2.0 * 6.054880142211914
Epoch 580, val loss: 0.7740298509597778
Epoch 590, training loss: 12.505839347839355 = 0.4083121716976166 + 2.0 * 6.048763751983643
Epoch 590, val loss: 0.7674359679222107
Epoch 600, training loss: 12.484810829162598 = 0.3915264904499054 + 2.0 * 6.046642303466797
Epoch 600, val loss: 0.7613649368286133
Epoch 610, training loss: 12.467146873474121 = 0.3751506209373474 + 2.0 * 6.0459980964660645
Epoch 610, val loss: 0.7557575702667236
Epoch 620, training loss: 12.457008361816406 = 0.35922881960868835 + 2.0 * 6.048889636993408
Epoch 620, val loss: 0.75067538022995
Epoch 630, training loss: 12.433484077453613 = 0.3438936471939087 + 2.0 * 6.044795036315918
Epoch 630, val loss: 0.7461665868759155
Epoch 640, training loss: 12.414897918701172 = 0.3289845287799835 + 2.0 * 6.042956829071045
Epoch 640, val loss: 0.7421116232872009
Epoch 650, training loss: 12.411380767822266 = 0.3144477903842926 + 2.0 * 6.048466682434082
Epoch 650, val loss: 0.738415002822876
Epoch 660, training loss: 12.392383575439453 = 0.30037447810173035 + 2.0 * 6.046004772186279
Epoch 660, val loss: 0.7352005839347839
Epoch 670, training loss: 12.368431091308594 = 0.2868453860282898 + 2.0 * 6.040792942047119
Epoch 670, val loss: 0.7323697805404663
Epoch 680, training loss: 12.352557182312012 = 0.2737216353416443 + 2.0 * 6.039417743682861
Epoch 680, val loss: 0.7299068570137024
Epoch 690, training loss: 12.342877388000488 = 0.26098713278770447 + 2.0 * 6.040945053100586
Epoch 690, val loss: 0.7277902364730835
Epoch 700, training loss: 12.327073097229004 = 0.24870364367961884 + 2.0 * 6.0391845703125
Epoch 700, val loss: 0.7260350584983826
Epoch 710, training loss: 12.31724739074707 = 0.23695507645606995 + 2.0 * 6.040146350860596
Epoch 710, val loss: 0.7246973514556885
Epoch 720, training loss: 12.298033714294434 = 0.22571364045143127 + 2.0 * 6.036159992218018
Epoch 720, val loss: 0.7237722277641296
Epoch 730, training loss: 12.287565231323242 = 0.2149118334054947 + 2.0 * 6.036326885223389
Epoch 730, val loss: 0.7232239842414856
Epoch 740, training loss: 12.27683162689209 = 0.20457808673381805 + 2.0 * 6.036126613616943
Epoch 740, val loss: 0.7229772806167603
Epoch 750, training loss: 12.263084411621094 = 0.19475539028644562 + 2.0 * 6.0341644287109375
Epoch 750, val loss: 0.7231332659721375
Epoch 760, training loss: 12.270127296447754 = 0.18538594245910645 + 2.0 * 6.042370796203613
Epoch 760, val loss: 0.7236246466636658
Epoch 770, training loss: 12.241572380065918 = 0.1765531599521637 + 2.0 * 6.032509803771973
Epoch 770, val loss: 0.7244399189949036
Epoch 780, training loss: 12.23147964477539 = 0.16811718046665192 + 2.0 * 6.031681060791016
Epoch 780, val loss: 0.7256091833114624
Epoch 790, training loss: 12.224499702453613 = 0.16009391844272614 + 2.0 * 6.03220272064209
Epoch 790, val loss: 0.727092981338501
Epoch 800, training loss: 12.217535972595215 = 0.15249603986740112 + 2.0 * 6.032519817352295
Epoch 800, val loss: 0.7288559079170227
Epoch 810, training loss: 12.206311225891113 = 0.1453762799501419 + 2.0 * 6.030467510223389
Epoch 810, val loss: 0.7308741211891174
Epoch 820, training loss: 12.19711685180664 = 0.13862916827201843 + 2.0 * 6.0292439460754395
Epoch 820, val loss: 0.7332265973091125
Epoch 830, training loss: 12.188987731933594 = 0.132211834192276 + 2.0 * 6.028388023376465
Epoch 830, val loss: 0.735867977142334
Epoch 840, training loss: 12.191500663757324 = 0.12614023685455322 + 2.0 * 6.032680034637451
Epoch 840, val loss: 0.7387294173240662
Epoch 850, training loss: 12.175810813903809 = 0.1204451248049736 + 2.0 * 6.027682781219482
Epoch 850, val loss: 0.7417522072792053
Epoch 860, training loss: 12.168499946594238 = 0.11506208032369614 + 2.0 * 6.026719093322754
Epoch 860, val loss: 0.7449976801872253
Epoch 870, training loss: 12.169839859008789 = 0.10994361340999603 + 2.0 * 6.0299482345581055
Epoch 870, val loss: 0.7483590841293335
Epoch 880, training loss: 12.163206100463867 = 0.10516688227653503 + 2.0 * 6.029019832611084
Epoch 880, val loss: 0.7518447041511536
Epoch 890, training loss: 12.149675369262695 = 0.10064022988080978 + 2.0 * 6.02451753616333
Epoch 890, val loss: 0.7554670572280884
Epoch 900, training loss: 12.14528751373291 = 0.09636401385068893 + 2.0 * 6.02446174621582
Epoch 900, val loss: 0.7592443823814392
Epoch 910, training loss: 12.145919799804688 = 0.09232024848461151 + 2.0 * 6.02679967880249
Epoch 910, val loss: 0.7630652189254761
Epoch 920, training loss: 12.134346961975098 = 0.08851180225610733 + 2.0 * 6.022917747497559
Epoch 920, val loss: 0.7669609785079956
Epoch 930, training loss: 12.127882957458496 = 0.08488816767930984 + 2.0 * 6.0214972496032715
Epoch 930, val loss: 0.770991325378418
Epoch 940, training loss: 12.12742805480957 = 0.0814485028386116 + 2.0 * 6.022989749908447
Epoch 940, val loss: 0.7750782370567322
Epoch 950, training loss: 12.12773323059082 = 0.07819758355617523 + 2.0 * 6.024767875671387
Epoch 950, val loss: 0.779134213924408
Epoch 960, training loss: 12.118990898132324 = 0.07515498250722885 + 2.0 * 6.021917819976807
Epoch 960, val loss: 0.7832037806510925
Epoch 970, training loss: 12.110729217529297 = 0.07225927710533142 + 2.0 * 6.019235134124756
Epoch 970, val loss: 0.7873721718788147
Epoch 980, training loss: 12.107288360595703 = 0.06949694454669952 + 2.0 * 6.018895626068115
Epoch 980, val loss: 0.7915977835655212
Epoch 990, training loss: 12.112874984741211 = 0.06686247140169144 + 2.0 * 6.023006439208984
Epoch 990, val loss: 0.7958071231842041
Epoch 1000, training loss: 12.11010456085205 = 0.06438406556844711 + 2.0 * 6.022860050201416
Epoch 1000, val loss: 0.7999289035797119
Epoch 1010, training loss: 12.104562759399414 = 0.06203966587781906 + 2.0 * 6.021261692047119
Epoch 1010, val loss: 0.8041672110557556
Epoch 1020, training loss: 12.0963134765625 = 0.0598113089799881 + 2.0 * 6.018250942230225
Epoch 1020, val loss: 0.8083414435386658
Epoch 1030, training loss: 12.092425346374512 = 0.05769623816013336 + 2.0 * 6.017364501953125
Epoch 1030, val loss: 0.8125402927398682
Epoch 1040, training loss: 12.087433815002441 = 0.05567054823040962 + 2.0 * 6.015881538391113
Epoch 1040, val loss: 0.8168085813522339
Epoch 1050, training loss: 12.096587181091309 = 0.05373459681868553 + 2.0 * 6.021426200866699
Epoch 1050, val loss: 0.8210452795028687
Epoch 1060, training loss: 12.08800983428955 = 0.05190584436058998 + 2.0 * 6.018052101135254
Epoch 1060, val loss: 0.8251713514328003
Epoch 1070, training loss: 12.082662582397461 = 0.050169795751571655 + 2.0 * 6.016246318817139
Epoch 1070, val loss: 0.8293811082839966
Epoch 1080, training loss: 12.086751937866211 = 0.04851179197430611 + 2.0 * 6.019120216369629
Epoch 1080, val loss: 0.8335462808609009
Epoch 1090, training loss: 12.076956748962402 = 0.04692991077899933 + 2.0 * 6.015013217926025
Epoch 1090, val loss: 0.8375799655914307
Epoch 1100, training loss: 12.084120750427246 = 0.045426368713378906 + 2.0 * 6.019347190856934
Epoch 1100, val loss: 0.8416956663131714
Epoch 1110, training loss: 12.07254409790039 = 0.04399504140019417 + 2.0 * 6.014274597167969
Epoch 1110, val loss: 0.8456906080245972
Epoch 1120, training loss: 12.066983222961426 = 0.04262283071875572 + 2.0 * 6.012180328369141
Epoch 1120, val loss: 0.8498108983039856
Epoch 1130, training loss: 12.064414024353027 = 0.041304122656583786 + 2.0 * 6.011554718017578
Epoch 1130, val loss: 0.8538808822631836
Epoch 1140, training loss: 12.083626747131348 = 0.04003797098994255 + 2.0 * 6.021794319152832
Epoch 1140, val loss: 0.857888400554657
Epoch 1150, training loss: 12.066061019897461 = 0.03884432092308998 + 2.0 * 6.013608455657959
Epoch 1150, val loss: 0.8617769479751587
Epoch 1160, training loss: 12.065547943115234 = 0.03769971430301666 + 2.0 * 6.0139241218566895
Epoch 1160, val loss: 0.8657172918319702
Epoch 1170, training loss: 12.058098793029785 = 0.036611173301935196 + 2.0 * 6.010743618011475
Epoch 1170, val loss: 0.8695720434188843
Epoch 1180, training loss: 12.05507755279541 = 0.03556397557258606 + 2.0 * 6.009756565093994
Epoch 1180, val loss: 0.8734444975852966
Epoch 1190, training loss: 12.057699203491211 = 0.03455456718802452 + 2.0 * 6.011572360992432
Epoch 1190, val loss: 0.8772707581520081
Epoch 1200, training loss: 12.056262016296387 = 0.033588849008083344 + 2.0 * 6.011336803436279
Epoch 1200, val loss: 0.8810006380081177
Epoch 1210, training loss: 12.05269718170166 = 0.03266673907637596 + 2.0 * 6.01001501083374
Epoch 1210, val loss: 0.8847203254699707
Epoch 1220, training loss: 12.049728393554688 = 0.031780634075403214 + 2.0 * 6.008974075317383
Epoch 1220, val loss: 0.8884359002113342
Epoch 1230, training loss: 12.047335624694824 = 0.030926380306482315 + 2.0 * 6.008204460144043
Epoch 1230, val loss: 0.8921661972999573
Epoch 1240, training loss: 12.048012733459473 = 0.030104948207736015 + 2.0 * 6.008954048156738
Epoch 1240, val loss: 0.8958492875099182
Epoch 1250, training loss: 12.055319786071777 = 0.02931852638721466 + 2.0 * 6.01300048828125
Epoch 1250, val loss: 0.8994335532188416
Epoch 1260, training loss: 12.042166709899902 = 0.02856241911649704 + 2.0 * 6.006802082061768
Epoch 1260, val loss: 0.9029229283332825
Epoch 1270, training loss: 12.040410995483398 = 0.027837473899126053 + 2.0 * 6.00628662109375
Epoch 1270, val loss: 0.9065191149711609
Epoch 1280, training loss: 12.052830696105957 = 0.027138907462358475 + 2.0 * 6.012845993041992
Epoch 1280, val loss: 0.9100384712219238
Epoch 1290, training loss: 12.040816307067871 = 0.0264632236212492 + 2.0 * 6.007176399230957
Epoch 1290, val loss: 0.9135010242462158
Epoch 1300, training loss: 12.036789894104004 = 0.025816110894083977 + 2.0 * 6.005486965179443
Epoch 1300, val loss: 0.9170103073120117
Epoch 1310, training loss: 12.035332679748535 = 0.025187121704220772 + 2.0 * 6.005072593688965
Epoch 1310, val loss: 0.9204903841018677
Epoch 1320, training loss: 12.052804946899414 = 0.02458026632666588 + 2.0 * 6.01411247253418
Epoch 1320, val loss: 0.9239125847816467
Epoch 1330, training loss: 12.042670249938965 = 0.023999428376555443 + 2.0 * 6.009335517883301
Epoch 1330, val loss: 0.9271462559700012
Epoch 1340, training loss: 12.032578468322754 = 0.023443900048732758 + 2.0 * 6.0045671463012695
Epoch 1340, val loss: 0.9305037260055542
Epoch 1350, training loss: 12.036537170410156 = 0.022906340658664703 + 2.0 * 6.006815433502197
Epoch 1350, val loss: 0.9338420629501343
Epoch 1360, training loss: 12.030646324157715 = 0.02238384261727333 + 2.0 * 6.004131317138672
Epoch 1360, val loss: 0.9370741844177246
Epoch 1370, training loss: 12.028848648071289 = 0.021879909560084343 + 2.0 * 6.00348424911499
Epoch 1370, val loss: 0.940311074256897
Epoch 1380, training loss: 12.029878616333008 = 0.021392077207565308 + 2.0 * 6.00424337387085
Epoch 1380, val loss: 0.9435778856277466
Epoch 1390, training loss: 12.037837982177734 = 0.0209211278706789 + 2.0 * 6.008458614349365
Epoch 1390, val loss: 0.9467729330062866
Epoch 1400, training loss: 12.027388572692871 = 0.02046501636505127 + 2.0 * 6.003461837768555
Epoch 1400, val loss: 0.9498828053474426
Epoch 1410, training loss: 12.023614883422852 = 0.020027166232466698 + 2.0 * 6.00179386138916
Epoch 1410, val loss: 0.9530316591262817
Epoch 1420, training loss: 12.02299690246582 = 0.01959928683936596 + 2.0 * 6.001698970794678
Epoch 1420, val loss: 0.9562026262283325
Epoch 1430, training loss: 12.039748191833496 = 0.019183432683348656 + 2.0 * 6.010282516479492
Epoch 1430, val loss: 0.9592480659484863
Epoch 1440, training loss: 12.025407791137695 = 0.018788835033774376 + 2.0 * 6.00330924987793
Epoch 1440, val loss: 0.9621759653091431
Epoch 1450, training loss: 12.02198600769043 = 0.018405649811029434 + 2.0 * 6.0017900466918945
Epoch 1450, val loss: 0.9652178883552551
Epoch 1460, training loss: 12.022361755371094 = 0.01803283765912056 + 2.0 * 6.002164363861084
Epoch 1460, val loss: 0.9682325720787048
Epoch 1470, training loss: 12.022565841674805 = 0.01767033152282238 + 2.0 * 6.002447605133057
Epoch 1470, val loss: 0.9711788892745972
Epoch 1480, training loss: 12.017733573913574 = 0.017319921404123306 + 2.0 * 6.00020694732666
Epoch 1480, val loss: 0.9741466641426086
Epoch 1490, training loss: 12.021124839782715 = 0.016979001462459564 + 2.0 * 6.002072811126709
Epoch 1490, val loss: 0.9771097898483276
Epoch 1500, training loss: 12.014134407043457 = 0.016645757481455803 + 2.0 * 5.998744487762451
Epoch 1500, val loss: 0.9800341725349426
Epoch 1510, training loss: 12.028936386108398 = 0.016321254894137383 + 2.0 * 6.006307601928711
Epoch 1510, val loss: 0.9829346537590027
Epoch 1520, training loss: 12.017664909362793 = 0.01601378433406353 + 2.0 * 6.00082540512085
Epoch 1520, val loss: 0.9857591390609741
Epoch 1530, training loss: 12.016613960266113 = 0.01571248471736908 + 2.0 * 6.000450611114502
Epoch 1530, val loss: 0.9885727763175964
Epoch 1540, training loss: 12.019783973693848 = 0.015422302298247814 + 2.0 * 6.002181053161621
Epoch 1540, val loss: 0.9914348721504211
Epoch 1550, training loss: 12.01955795288086 = 0.015136565081775188 + 2.0 * 6.00221061706543
Epoch 1550, val loss: 0.9941582679748535
Epoch 1560, training loss: 12.010222434997559 = 0.01486296858638525 + 2.0 * 5.997679710388184
Epoch 1560, val loss: 0.9968736171722412
Epoch 1570, training loss: 12.009495735168457 = 0.014594239182770252 + 2.0 * 5.997450828552246
Epoch 1570, val loss: 0.9997172951698303
Epoch 1580, training loss: 12.010027885437012 = 0.014330802485346794 + 2.0 * 5.9978485107421875
Epoch 1580, val loss: 1.0024774074554443
Epoch 1590, training loss: 12.0171537399292 = 0.014074524864554405 + 2.0 * 6.001539707183838
Epoch 1590, val loss: 1.005133032798767
Epoch 1600, training loss: 12.008212089538574 = 0.013828114606440067 + 2.0 * 5.997191905975342
Epoch 1600, val loss: 1.007834792137146
Epoch 1610, training loss: 12.007145881652832 = 0.013586964458227158 + 2.0 * 5.996779441833496
Epoch 1610, val loss: 1.0105605125427246
Epoch 1620, training loss: 12.018158912658691 = 0.013351892121136189 + 2.0 * 6.002403736114502
Epoch 1620, val loss: 1.013150691986084
Epoch 1630, training loss: 12.007630348205566 = 0.013125602155923843 + 2.0 * 5.997252464294434
Epoch 1630, val loss: 1.0158345699310303
Epoch 1640, training loss: 12.005507469177246 = 0.012903823517262936 + 2.0 * 5.996301651000977
Epoch 1640, val loss: 1.018468976020813
Epoch 1650, training loss: 12.007964134216309 = 0.012687327340245247 + 2.0 * 5.99763822555542
Epoch 1650, val loss: 1.0210800170898438
Epoch 1660, training loss: 12.006319999694824 = 0.012475680559873581 + 2.0 * 5.996922016143799
Epoch 1660, val loss: 1.0236709117889404
Epoch 1670, training loss: 12.012465476989746 = 0.012270897626876831 + 2.0 * 6.000097274780273
Epoch 1670, val loss: 1.0261895656585693
Epoch 1680, training loss: 12.007328987121582 = 0.012073502875864506 + 2.0 * 5.9976277351379395
Epoch 1680, val loss: 1.028656005859375
Epoch 1690, training loss: 12.00251579284668 = 0.011878974735736847 + 2.0 * 5.995318412780762
Epoch 1690, val loss: 1.0311914682388306
Epoch 1700, training loss: 11.999704360961914 = 0.011690037325024605 + 2.0 * 5.994007110595703
Epoch 1700, val loss: 1.033737301826477
Epoch 1710, training loss: 12.000597953796387 = 0.011502611450850964 + 2.0 * 5.9945478439331055
Epoch 1710, val loss: 1.0363095998764038
Epoch 1720, training loss: 12.01829719543457 = 0.011321527883410454 + 2.0 * 6.003488063812256
Epoch 1720, val loss: 1.0387684106826782
Epoch 1730, training loss: 11.99985122680664 = 0.011146134696900845 + 2.0 * 5.994352340698242
Epoch 1730, val loss: 1.0410704612731934
Epoch 1740, training loss: 11.997621536254883 = 0.010976644232869148 + 2.0 * 5.993322372436523
Epoch 1740, val loss: 1.0435177087783813
Epoch 1750, training loss: 11.997274398803711 = 0.010809049941599369 + 2.0 * 5.993232727050781
Epoch 1750, val loss: 1.0460172891616821
Epoch 1760, training loss: 12.00390911102295 = 0.010642665438354015 + 2.0 * 5.996633052825928
Epoch 1760, val loss: 1.048426866531372
Epoch 1770, training loss: 11.99619197845459 = 0.010481874458491802 + 2.0 * 5.992855072021484
Epoch 1770, val loss: 1.050809621810913
Epoch 1780, training loss: 12.000277519226074 = 0.010325889103114605 + 2.0 * 5.994976043701172
Epoch 1780, val loss: 1.0531883239746094
Epoch 1790, training loss: 11.997634887695312 = 0.010174095630645752 + 2.0 * 5.993730545043945
Epoch 1790, val loss: 1.055517554283142
Epoch 1800, training loss: 11.9959716796875 = 0.010026218369603157 + 2.0 * 5.9929728507995605
Epoch 1800, val loss: 1.057875633239746
Epoch 1810, training loss: 11.993379592895508 = 0.00987967848777771 + 2.0 * 5.9917497634887695
Epoch 1810, val loss: 1.0602375268936157
Epoch 1820, training loss: 12.003087997436523 = 0.009735365398228168 + 2.0 * 5.996676445007324
Epoch 1820, val loss: 1.0625941753387451
Epoch 1830, training loss: 11.993287086486816 = 0.009596002288162708 + 2.0 * 5.991845607757568
Epoch 1830, val loss: 1.0647605657577515
Epoch 1840, training loss: 11.994673728942871 = 0.00945986807346344 + 2.0 * 5.992607116699219
Epoch 1840, val loss: 1.067095398902893
Epoch 1850, training loss: 11.99261474609375 = 0.009326957166194916 + 2.0 * 5.991643905639648
Epoch 1850, val loss: 1.069333791732788
Epoch 1860, training loss: 11.991029739379883 = 0.009196358732879162 + 2.0 * 5.990916728973389
Epoch 1860, val loss: 1.071630835533142
Epoch 1870, training loss: 12.01916217803955 = 0.009069308638572693 + 2.0 * 6.005046367645264
Epoch 1870, val loss: 1.073857307434082
Epoch 1880, training loss: 11.992267608642578 = 0.008945017121732235 + 2.0 * 5.991661071777344
Epoch 1880, val loss: 1.0758507251739502
Epoch 1890, training loss: 11.988630294799805 = 0.008825941011309624 + 2.0 * 5.989902019500732
Epoch 1890, val loss: 1.0780673027038574
Epoch 1900, training loss: 11.988526344299316 = 0.008706854656338692 + 2.0 * 5.989909648895264
Epoch 1900, val loss: 1.0803396701812744
Epoch 1910, training loss: 11.98861312866211 = 0.008588160388171673 + 2.0 * 5.9900126457214355
Epoch 1910, val loss: 1.0825400352478027
Epoch 1920, training loss: 12.004963874816895 = 0.008472563698887825 + 2.0 * 5.998245716094971
Epoch 1920, val loss: 1.0846548080444336
Epoch 1930, training loss: 11.992410659790039 = 0.008360984735190868 + 2.0 * 5.992024898529053
Epoch 1930, val loss: 1.0866899490356445
Epoch 1940, training loss: 11.986669540405273 = 0.008252372965216637 + 2.0 * 5.989208698272705
Epoch 1940, val loss: 1.0888712406158447
Epoch 1950, training loss: 11.987595558166504 = 0.008144818246364594 + 2.0 * 5.989725589752197
Epoch 1950, val loss: 1.0910440683364868
Epoch 1960, training loss: 12.00358772277832 = 0.008038250729441643 + 2.0 * 5.997774600982666
Epoch 1960, val loss: 1.093074917793274
Epoch 1970, training loss: 11.995659828186035 = 0.007936547510325909 + 2.0 * 5.993861675262451
Epoch 1970, val loss: 1.0950721502304077
Epoch 1980, training loss: 11.987906455993652 = 0.007837381213903427 + 2.0 * 5.990034580230713
Epoch 1980, val loss: 1.0971183776855469
Epoch 1990, training loss: 11.984855651855469 = 0.00773957371711731 + 2.0 * 5.988557815551758
Epoch 1990, val loss: 1.0992389917373657
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8037
Overall ASR: 0.9446
Flip ASR: 0.9333/225 nodes
The final ASR:0.82288, 0.08717, Accuracy:0.79630, 0.01318
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11630])
remove edge: torch.Size([2, 9544])
updated graph: torch.Size([2, 10618])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9889
Flip ASR: 0.9867/225 nodes
The final ASR:0.97909, 0.00696, Accuracy:0.82840, 0.00630
Begin epxeriment: cont_weight: 2 epoch:2000 der1:0.3 der2:0.2 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 18.71320343017578 = 1.9655482769012451 + 2.0 * 8.373827934265137
Epoch 0, val loss: 1.9650530815124512
Epoch 10, training loss: 18.7015380859375 = 1.9551185369491577 + 2.0 * 8.373209953308105
Epoch 10, val loss: 1.9553111791610718
Epoch 20, training loss: 18.680036544799805 = 1.9420758485794067 + 2.0 * 8.368980407714844
Epoch 20, val loss: 1.9425702095031738
Epoch 30, training loss: 18.604909896850586 = 1.9242922067642212 + 2.0 * 8.340309143066406
Epoch 30, val loss: 1.9249716997146606
Epoch 40, training loss: 18.202268600463867 = 1.903687596321106 + 2.0 * 8.149290084838867
Epoch 40, val loss: 1.90545654296875
Epoch 50, training loss: 16.746370315551758 = 1.8807775974273682 + 2.0 * 7.432796478271484
Epoch 50, val loss: 1.8835968971252441
Epoch 60, training loss: 15.970088958740234 = 1.8616904020309448 + 2.0 * 7.05419921875
Epoch 60, val loss: 1.866740345954895
Epoch 70, training loss: 15.442097663879395 = 1.8490887880325317 + 2.0 * 6.796504497528076
Epoch 70, val loss: 1.8547791242599487
Epoch 80, training loss: 15.145367622375488 = 1.8367077112197876 + 2.0 * 6.654329776763916
Epoch 80, val loss: 1.8423810005187988
Epoch 90, training loss: 14.937030792236328 = 1.8250471353530884 + 2.0 * 6.5559916496276855
Epoch 90, val loss: 1.8308923244476318
Epoch 100, training loss: 14.739296913146973 = 1.8147355318069458 + 2.0 * 6.462280750274658
Epoch 100, val loss: 1.820605993270874
Epoch 110, training loss: 14.606229782104492 = 1.805605411529541 + 2.0 * 6.400312423706055
Epoch 110, val loss: 1.8111863136291504
Epoch 120, training loss: 14.503802299499512 = 1.797078251838684 + 2.0 * 6.353362083435059
Epoch 120, val loss: 1.8024555444717407
Epoch 130, training loss: 14.413410186767578 = 1.7896287441253662 + 2.0 * 6.311890602111816
Epoch 130, val loss: 1.7949581146240234
Epoch 140, training loss: 14.34023380279541 = 1.7829183340072632 + 2.0 * 6.278657913208008
Epoch 140, val loss: 1.788140892982483
Epoch 150, training loss: 14.281188011169434 = 1.7757318019866943 + 2.0 * 6.25272798538208
Epoch 150, val loss: 1.781040906906128
Epoch 160, training loss: 14.231256484985352 = 1.7676944732666016 + 2.0 * 6.231781005859375
Epoch 160, val loss: 1.7735474109649658
Epoch 170, training loss: 14.183969497680664 = 1.758852243423462 + 2.0 * 6.212558746337891
Epoch 170, val loss: 1.7656878232955933
Epoch 180, training loss: 14.14172649383545 = 1.7490321397781372 + 2.0 * 6.196347236633301
Epoch 180, val loss: 1.7573188543319702
Epoch 190, training loss: 14.106651306152344 = 1.7379807233810425 + 2.0 * 6.184335231781006
Epoch 190, val loss: 1.7480791807174683
Epoch 200, training loss: 14.067131042480469 = 1.7254971265792847 + 2.0 * 6.170816898345947
Epoch 200, val loss: 1.737831950187683
Epoch 210, training loss: 14.030414581298828 = 1.711229920387268 + 2.0 * 6.159592151641846
Epoch 210, val loss: 1.7262251377105713
Epoch 220, training loss: 13.994913101196289 = 1.6946905851364136 + 2.0 * 6.150111198425293
Epoch 220, val loss: 1.7127997875213623
Epoch 230, training loss: 13.95885181427002 = 1.6752387285232544 + 2.0 * 6.141806602478027
Epoch 230, val loss: 1.6969941854476929
Epoch 240, training loss: 13.927160263061523 = 1.6523422002792358 + 2.0 * 6.137409210205078
Epoch 240, val loss: 1.6784878969192505
Epoch 250, training loss: 13.88510799407959 = 1.6261647939682007 + 2.0 * 6.129471778869629
Epoch 250, val loss: 1.6572561264038086
Epoch 260, training loss: 13.842130661010742 = 1.5961172580718994 + 2.0 * 6.123006820678711
Epoch 260, val loss: 1.6328016519546509
Epoch 270, training loss: 13.797435760498047 = 1.561998963356018 + 2.0 * 6.11771821975708
Epoch 270, val loss: 1.6048849821090698
Epoch 280, training loss: 13.747682571411133 = 1.523643136024475 + 2.0 * 6.1120195388793945
Epoch 280, val loss: 1.5736228227615356
Epoch 290, training loss: 13.6965970993042 = 1.4813514947891235 + 2.0 * 6.1076226234436035
Epoch 290, val loss: 1.5390373468399048
Epoch 300, training loss: 13.645040512084961 = 1.4355887174606323 + 2.0 * 6.1047258377075195
Epoch 300, val loss: 1.501733660697937
Epoch 310, training loss: 13.593323707580566 = 1.3878074884414673 + 2.0 * 6.102757930755615
Epoch 310, val loss: 1.4630974531173706
Epoch 320, training loss: 13.53468132019043 = 1.3392300605773926 + 2.0 * 6.0977253913879395
Epoch 320, val loss: 1.4241973161697388
Epoch 330, training loss: 13.478097915649414 = 1.2903082370758057 + 2.0 * 6.093894958496094
Epoch 330, val loss: 1.3851556777954102
Epoch 340, training loss: 13.422731399536133 = 1.2413709163665771 + 2.0 * 6.090680122375488
Epoch 340, val loss: 1.3465564250946045
Epoch 350, training loss: 13.370434761047363 = 1.1929725408554077 + 2.0 * 6.088731288909912
Epoch 350, val loss: 1.308958888053894
Epoch 360, training loss: 13.318455696105957 = 1.1459627151489258 + 2.0 * 6.086246490478516
Epoch 360, val loss: 1.27253258228302
Epoch 370, training loss: 13.26366138458252 = 1.0996772050857544 + 2.0 * 6.081992149353027
Epoch 370, val loss: 1.236876130104065
Epoch 380, training loss: 13.216700553894043 = 1.0536302328109741 + 2.0 * 6.081535339355469
Epoch 380, val loss: 1.201592206954956
Epoch 390, training loss: 13.163749694824219 = 1.0085560083389282 + 2.0 * 6.077596664428711
Epoch 390, val loss: 1.1672725677490234
Epoch 400, training loss: 13.113439559936523 = 0.9644780158996582 + 2.0 * 6.074481010437012
Epoch 400, val loss: 1.1338756084442139
Epoch 410, training loss: 13.06534194946289 = 0.9209880828857422 + 2.0 * 6.072176933288574
Epoch 410, val loss: 1.1008895635604858
Epoch 420, training loss: 13.03597640991211 = 0.8781652450561523 + 2.0 * 6.0789055824279785
Epoch 420, val loss: 1.0684486627578735
Epoch 430, training loss: 12.979408264160156 = 0.8370060920715332 + 2.0 * 6.071201324462891
Epoch 430, val loss: 1.0373886823654175
Epoch 440, training loss: 12.930257797241211 = 0.7972617149353027 + 2.0 * 6.066497802734375
Epoch 440, val loss: 1.0075234174728394
Epoch 450, training loss: 12.888413429260254 = 0.7586838603019714 + 2.0 * 6.064864635467529
Epoch 450, val loss: 0.9784528017044067
Epoch 460, training loss: 12.858039855957031 = 0.7213858962059021 + 2.0 * 6.068326950073242
Epoch 460, val loss: 0.950355589389801
Epoch 470, training loss: 12.809856414794922 = 0.6860897541046143 + 2.0 * 6.061883449554443
Epoch 470, val loss: 0.9237883687019348
Epoch 480, training loss: 12.77247142791748 = 0.6525563597679138 + 2.0 * 6.059957504272461
Epoch 480, val loss: 0.8986619710922241
Epoch 490, training loss: 12.736917495727539 = 0.6206262111663818 + 2.0 * 6.058145523071289
Epoch 490, val loss: 0.8750147223472595
Epoch 500, training loss: 12.70549201965332 = 0.5905976295471191 + 2.0 * 6.0574469566345215
Epoch 500, val loss: 0.8530903458595276
Epoch 510, training loss: 12.680376052856445 = 0.5628864765167236 + 2.0 * 6.05874490737915
Epoch 510, val loss: 0.8333872556686401
Epoch 520, training loss: 12.643996238708496 = 0.5371293425559998 + 2.0 * 6.053433418273926
Epoch 520, val loss: 0.8154611587524414
Epoch 530, training loss: 12.617644309997559 = 0.5128137469291687 + 2.0 * 6.052415370941162
Epoch 530, val loss: 0.7990385890007019
Epoch 540, training loss: 12.591082572937012 = 0.48977547883987427 + 2.0 * 6.050653457641602
Epoch 540, val loss: 0.7841235995292664
Epoch 550, training loss: 12.568854331970215 = 0.4680595397949219 + 2.0 * 6.0503973960876465
Epoch 550, val loss: 0.7705935835838318
Epoch 560, training loss: 12.544189453125 = 0.4477340579032898 + 2.0 * 6.048227787017822
Epoch 560, val loss: 0.7585702538490295
Epoch 570, training loss: 12.52735710144043 = 0.4285200536251068 + 2.0 * 6.0494184494018555
Epoch 570, val loss: 0.7477677464485168
Epoch 580, training loss: 12.506738662719727 = 0.4103437662124634 + 2.0 * 6.048197269439697
Epoch 580, val loss: 0.7380767464637756
Epoch 590, training loss: 12.481818199157715 = 0.39307254552841187 + 2.0 * 6.044373035430908
Epoch 590, val loss: 0.7294829487800598
Epoch 600, training loss: 12.461647033691406 = 0.37649497389793396 + 2.0 * 6.042575836181641
Epoch 600, val loss: 0.7217121720314026
Epoch 610, training loss: 12.465112686157227 = 0.3605160415172577 + 2.0 * 6.052298545837402
Epoch 610, val loss: 0.7148279547691345
Epoch 620, training loss: 12.427960395812988 = 0.34546738862991333 + 2.0 * 6.04124641418457
Epoch 620, val loss: 0.7087624669075012
Epoch 630, training loss: 12.40960693359375 = 0.3309904634952545 + 2.0 * 6.039308071136475
Epoch 630, val loss: 0.7035059332847595
Epoch 640, training loss: 12.393061637878418 = 0.3170056641101837 + 2.0 * 6.038027763366699
Epoch 640, val loss: 0.698876142501831
Epoch 650, training loss: 12.380595207214355 = 0.303439736366272 + 2.0 * 6.038577556610107
Epoch 650, val loss: 0.6948670148849487
Epoch 660, training loss: 12.37581729888916 = 0.2904011607170105 + 2.0 * 6.042707920074463
Epoch 660, val loss: 0.6914762258529663
Epoch 670, training loss: 12.349869728088379 = 0.27800777554512024 + 2.0 * 6.03593111038208
Epoch 670, val loss: 0.688715934753418
Epoch 680, training loss: 12.334575653076172 = 0.26602065563201904 + 2.0 * 6.034277439117432
Epoch 680, val loss: 0.6864613890647888
Epoch 690, training loss: 12.335960388183594 = 0.25438398122787476 + 2.0 * 6.040788173675537
Epoch 690, val loss: 0.6846942901611328
Epoch 700, training loss: 12.311112403869629 = 0.2432701140642166 + 2.0 * 6.033921241760254
Epoch 700, val loss: 0.683394730091095
Epoch 710, training loss: 12.298437118530273 = 0.23258019983768463 + 2.0 * 6.032928466796875
Epoch 710, val loss: 0.682527482509613
Epoch 720, training loss: 12.284635543823242 = 0.22218802571296692 + 2.0 * 6.031223773956299
Epoch 720, val loss: 0.6820610761642456
Epoch 730, training loss: 12.27346134185791 = 0.21210619807243347 + 2.0 * 6.030677795410156
Epoch 730, val loss: 0.6819688081741333
Epoch 740, training loss: 12.268977165222168 = 0.20235829055309296 + 2.0 * 6.033309459686279
Epoch 740, val loss: 0.6822786331176758
Epoch 750, training loss: 12.251941680908203 = 0.1930522918701172 + 2.0 * 6.029444694519043
Epoch 750, val loss: 0.6829385161399841
Epoch 760, training loss: 12.249043464660645 = 0.1840125471353531 + 2.0 * 6.032515525817871
Epoch 760, val loss: 0.6839178204536438
Epoch 770, training loss: 12.233988761901855 = 0.1753959357738495 + 2.0 * 6.029296398162842
Epoch 770, val loss: 0.6852564215660095
Epoch 780, training loss: 12.220402717590332 = 0.16710928082466125 + 2.0 * 6.026646614074707
Epoch 780, val loss: 0.686842143535614
Epoch 790, training loss: 12.214372634887695 = 0.15914711356163025 + 2.0 * 6.027612686157227
Epoch 790, val loss: 0.6887483596801758
Epoch 800, training loss: 12.203313827514648 = 0.15152695775032043 + 2.0 * 6.025893211364746
Epoch 800, val loss: 0.690977931022644
Epoch 810, training loss: 12.195708274841309 = 0.14425696432590485 + 2.0 * 6.025725841522217
Epoch 810, val loss: 0.6935194730758667
Epoch 820, training loss: 12.18725299835205 = 0.13734012842178345 + 2.0 * 6.024956226348877
Epoch 820, val loss: 0.6963036060333252
Epoch 830, training loss: 12.18106460571289 = 0.13073597848415375 + 2.0 * 6.0251641273498535
Epoch 830, val loss: 0.6993880271911621
Epoch 840, training loss: 12.1693754196167 = 0.12448401004076004 + 2.0 * 6.0224456787109375
Epoch 840, val loss: 0.7027101516723633
Epoch 850, training loss: 12.172749519348145 = 0.11854635924100876 + 2.0 * 6.027101516723633
Epoch 850, val loss: 0.7062305212020874
Epoch 860, training loss: 12.159927368164062 = 0.11291877180337906 + 2.0 * 6.023504257202148
Epoch 860, val loss: 0.7099200487136841
Epoch 870, training loss: 12.149084091186523 = 0.10761884599924088 + 2.0 * 6.020732402801514
Epoch 870, val loss: 0.7138211131095886
Epoch 880, training loss: 12.143302917480469 = 0.10257971286773682 + 2.0 * 6.020361423492432
Epoch 880, val loss: 0.7179180979728699
Epoch 890, training loss: 12.147793769836426 = 0.0978078618645668 + 2.0 * 6.024992942810059
Epoch 890, val loss: 0.7222258448600769
Epoch 900, training loss: 12.13782787322998 = 0.09329336881637573 + 2.0 * 6.0222673416137695
Epoch 900, val loss: 0.7265955805778503
Epoch 910, training loss: 12.12861156463623 = 0.08908058702945709 + 2.0 * 6.019765377044678
Epoch 910, val loss: 0.7311070561408997
Epoch 920, training loss: 12.121796607971191 = 0.0850939005613327 + 2.0 * 6.0183515548706055
Epoch 920, val loss: 0.7357097864151001
Epoch 930, training loss: 12.1159086227417 = 0.0813182145357132 + 2.0 * 6.0172953605651855
Epoch 930, val loss: 0.7404748797416687
Epoch 940, training loss: 12.119229316711426 = 0.07774630188941956 + 2.0 * 6.0207414627075195
Epoch 940, val loss: 0.7453189492225647
Epoch 950, training loss: 12.109333992004395 = 0.07438887655735016 + 2.0 * 6.017472743988037
Epoch 950, val loss: 0.7502685189247131
Epoch 960, training loss: 12.104886054992676 = 0.07122524082660675 + 2.0 * 6.0168304443359375
Epoch 960, val loss: 0.7551913261413574
Epoch 970, training loss: 12.10498046875 = 0.06823990494012833 + 2.0 * 6.018370151519775
Epoch 970, val loss: 0.7601650953292847
Epoch 980, training loss: 12.097076416015625 = 0.06542512774467468 + 2.0 * 6.0158257484436035
Epoch 980, val loss: 0.7651439309120178
Epoch 990, training loss: 12.094743728637695 = 0.06278333067893982 + 2.0 * 6.015980243682861
Epoch 990, val loss: 0.7701321840286255
Epoch 1000, training loss: 12.087308883666992 = 0.06028830632567406 + 2.0 * 6.013510227203369
Epoch 1000, val loss: 0.775169312953949
Epoch 1010, training loss: 12.085390090942383 = 0.057914696633815765 + 2.0 * 6.013737678527832
Epoch 1010, val loss: 0.7802090048789978
Epoch 1020, training loss: 12.085044860839844 = 0.05567186325788498 + 2.0 * 6.014686584472656
Epoch 1020, val loss: 0.7852475643157959
Epoch 1030, training loss: 12.080437660217285 = 0.05354693531990051 + 2.0 * 6.0134453773498535
Epoch 1030, val loss: 0.7902002930641174
Epoch 1040, training loss: 12.081210136413574 = 0.05154956877231598 + 2.0 * 6.014830112457275
Epoch 1040, val loss: 0.7951495051383972
Epoch 1050, training loss: 12.073094367980957 = 0.04967696592211723 + 2.0 * 6.011708736419678
Epoch 1050, val loss: 0.8000560998916626
Epoch 1060, training loss: 12.069770812988281 = 0.047882866114377975 + 2.0 * 6.01094388961792
Epoch 1060, val loss: 0.8049139380455017
Epoch 1070, training loss: 12.069511413574219 = 0.04618041217327118 + 2.0 * 6.011665344238281
Epoch 1070, val loss: 0.8097934126853943
Epoch 1080, training loss: 12.066229820251465 = 0.04456475377082825 + 2.0 * 6.0108323097229
Epoch 1080, val loss: 0.8146128058433533
Epoch 1090, training loss: 12.06915283203125 = 0.043035414069890976 + 2.0 * 6.013058662414551
Epoch 1090, val loss: 0.8194087743759155
Epoch 1100, training loss: 12.059452056884766 = 0.04159311205148697 + 2.0 * 6.008929252624512
Epoch 1100, val loss: 0.8240489363670349
Epoch 1110, training loss: 12.056809425354004 = 0.04021759703755379 + 2.0 * 6.008296012878418
Epoch 1110, val loss: 0.8287878036499023
Epoch 1120, training loss: 12.05459976196289 = 0.03889929875731468 + 2.0 * 6.007850170135498
Epoch 1120, val loss: 0.8334476947784424
Epoch 1130, training loss: 12.064325332641602 = 0.03764953836798668 + 2.0 * 6.013338088989258
Epoch 1130, val loss: 0.8381427526473999
Epoch 1140, training loss: 12.055612564086914 = 0.036453727632761 + 2.0 * 6.009579181671143
Epoch 1140, val loss: 0.8426229357719421
Epoch 1150, training loss: 12.051412582397461 = 0.03532741963863373 + 2.0 * 6.008042812347412
Epoch 1150, val loss: 0.8471314907073975
Epoch 1160, training loss: 12.048016548156738 = 0.03424210101366043 + 2.0 * 6.006887435913086
Epoch 1160, val loss: 0.851557195186615
Epoch 1170, training loss: 12.048807144165039 = 0.033206794410943985 + 2.0 * 6.007800102233887
Epoch 1170, val loss: 0.8560172319412231
Epoch 1180, training loss: 12.047969818115234 = 0.0322192907333374 + 2.0 * 6.007875442504883
Epoch 1180, val loss: 0.8603924512863159
Epoch 1190, training loss: 12.05618667602539 = 0.03128510341048241 + 2.0 * 6.012450695037842
Epoch 1190, val loss: 0.8646396994590759
Epoch 1200, training loss: 12.041851043701172 = 0.030397111549973488 + 2.0 * 6.0057268142700195
Epoch 1200, val loss: 0.8688902258872986
Epoch 1210, training loss: 12.0380220413208 = 0.029543166980147362 + 2.0 * 6.004239559173584
Epoch 1210, val loss: 0.8730422258377075
Epoch 1220, training loss: 12.036044120788574 = 0.02871914766728878 + 2.0 * 6.003662586212158
Epoch 1220, val loss: 0.8772358894348145
Epoch 1230, training loss: 12.05319595336914 = 0.027923548594117165 + 2.0 * 6.012636184692383
Epoch 1230, val loss: 0.8813514709472656
Epoch 1240, training loss: 12.039410591125488 = 0.027187049388885498 + 2.0 * 6.0061116218566895
Epoch 1240, val loss: 0.8854822516441345
Epoch 1250, training loss: 12.031075477600098 = 0.026466095820069313 + 2.0 * 6.002304553985596
Epoch 1250, val loss: 0.8894515633583069
Epoch 1260, training loss: 12.029726028442383 = 0.02577260509133339 + 2.0 * 6.001976490020752
Epoch 1260, val loss: 0.8934139013290405
Epoch 1270, training loss: 12.028670310974121 = 0.025103097781538963 + 2.0 * 6.00178337097168
Epoch 1270, val loss: 0.8974394202232361
Epoch 1280, training loss: 12.050246238708496 = 0.024456854909658432 + 2.0 * 6.012894630432129
Epoch 1280, val loss: 0.9013775587081909
Epoch 1290, training loss: 12.032893180847168 = 0.023854771628975868 + 2.0 * 6.004518985748291
Epoch 1290, val loss: 0.9051987528800964
Epoch 1300, training loss: 12.028058052062988 = 0.023276379331946373 + 2.0 * 6.0023908615112305
Epoch 1300, val loss: 0.9088693857192993
Epoch 1310, training loss: 12.023184776306152 = 0.022713739424943924 + 2.0 * 6.000235557556152
Epoch 1310, val loss: 0.912585973739624
Epoch 1320, training loss: 12.022123336791992 = 0.02216828428208828 + 2.0 * 5.9999775886535645
Epoch 1320, val loss: 0.9163495898246765
Epoch 1330, training loss: 12.022099494934082 = 0.02163996733725071 + 2.0 * 6.000229835510254
Epoch 1330, val loss: 0.9200422167778015
Epoch 1340, training loss: 12.03671646118164 = 0.021135592833161354 + 2.0 * 6.007790565490723
Epoch 1340, val loss: 0.9236487150192261
Epoch 1350, training loss: 12.026963233947754 = 0.020661888644099236 + 2.0 * 6.003150463104248
Epoch 1350, val loss: 0.9270997643470764
Epoch 1360, training loss: 12.018147468566895 = 0.02020340785384178 + 2.0 * 5.998971939086914
Epoch 1360, val loss: 0.9305418729782104
Epoch 1370, training loss: 12.016404151916504 = 0.019753703847527504 + 2.0 * 5.998325347900391
Epoch 1370, val loss: 0.9340700507164001
Epoch 1380, training loss: 12.0155611038208 = 0.019314488396048546 + 2.0 * 5.9981231689453125
Epoch 1380, val loss: 0.9376062750816345
Epoch 1390, training loss: 12.033467292785645 = 0.01888662949204445 + 2.0 * 6.007290363311768
Epoch 1390, val loss: 0.9410358667373657
Epoch 1400, training loss: 12.023056983947754 = 0.01849253475666046 + 2.0 * 6.00228214263916
Epoch 1400, val loss: 0.9444285035133362
Epoch 1410, training loss: 12.013492584228516 = 0.018100963905453682 + 2.0 * 5.9976959228515625
Epoch 1410, val loss: 0.9477030634880066
Epoch 1420, training loss: 12.010967254638672 = 0.017722440883517265 + 2.0 * 5.996622562408447
Epoch 1420, val loss: 0.9510366916656494
Epoch 1430, training loss: 12.010790824890137 = 0.017352759838104248 + 2.0 * 5.996718883514404
Epoch 1430, val loss: 0.9544139504432678
Epoch 1440, training loss: 12.033217430114746 = 0.016994867473840714 + 2.0 * 6.008111476898193
Epoch 1440, val loss: 0.9577319025993347
Epoch 1450, training loss: 12.010322570800781 = 0.01665729656815529 + 2.0 * 5.996832847595215
Epoch 1450, val loss: 0.9608774781227112
Epoch 1460, training loss: 12.00896167755127 = 0.01632842607796192 + 2.0 * 5.996316432952881
Epoch 1460, val loss: 0.9639476537704468
Epoch 1470, training loss: 12.00755500793457 = 0.016005706042051315 + 2.0 * 5.995774745941162
Epoch 1470, val loss: 0.9671334624290466
Epoch 1480, training loss: 12.011507987976074 = 0.015689805150032043 + 2.0 * 5.997909069061279
Epoch 1480, val loss: 0.9702816605567932
Epoch 1490, training loss: 12.005437850952148 = 0.01538845244795084 + 2.0 * 5.995024681091309
Epoch 1490, val loss: 0.9734787344932556
Epoch 1500, training loss: 12.00867748260498 = 0.015097913332283497 + 2.0 * 5.996789932250977
Epoch 1500, val loss: 0.9765322208404541
Epoch 1510, training loss: 12.00522518157959 = 0.014813121408224106 + 2.0 * 5.995205879211426
Epoch 1510, val loss: 0.9795403480529785
Epoch 1520, training loss: 12.004945755004883 = 0.014537696726620197 + 2.0 * 5.995203971862793
Epoch 1520, val loss: 0.9825999140739441
Epoch 1530, training loss: 12.005220413208008 = 0.014269889332354069 + 2.0 * 5.9954752922058105
Epoch 1530, val loss: 0.9855841994285583
Epoch 1540, training loss: 12.006963729858398 = 0.014011154882609844 + 2.0 * 5.996476173400879
Epoch 1540, val loss: 0.9885349869728088
Epoch 1550, training loss: 12.002525329589844 = 0.013757552020251751 + 2.0 * 5.994383811950684
Epoch 1550, val loss: 0.9914339184761047
Epoch 1560, training loss: 12.002474784851074 = 0.01351218856871128 + 2.0 * 5.994481086730957
Epoch 1560, val loss: 0.9943333268165588
Epoch 1570, training loss: 12.000940322875977 = 0.013274959288537502 + 2.0 * 5.993832588195801
Epoch 1570, val loss: 0.9972502589225769
Epoch 1580, training loss: 12.002605438232422 = 0.013044597581028938 + 2.0 * 5.994780540466309
Epoch 1580, val loss: 1.000158667564392
Epoch 1590, training loss: 12.00036334991455 = 0.01281945500522852 + 2.0 * 5.993772029876709
Epoch 1590, val loss: 1.0028947591781616
Epoch 1600, training loss: 11.9989652633667 = 0.012599697336554527 + 2.0 * 5.99318265914917
Epoch 1600, val loss: 1.0056686401367188
Epoch 1610, training loss: 12.003178596496582 = 0.012388690374791622 + 2.0 * 5.995395183563232
Epoch 1610, val loss: 1.0084443092346191
Epoch 1620, training loss: 12.007475852966309 = 0.012184515595436096 + 2.0 * 5.997645854949951
Epoch 1620, val loss: 1.011102318763733
Epoch 1630, training loss: 11.99740982055664 = 0.01198612991720438 + 2.0 * 5.992712020874023
Epoch 1630, val loss: 1.0137923955917358
Epoch 1640, training loss: 11.994009017944336 = 0.011792728677392006 + 2.0 * 5.991107940673828
Epoch 1640, val loss: 1.0164631605148315
Epoch 1650, training loss: 11.992900848388672 = 0.011601497419178486 + 2.0 * 5.990649700164795
Epoch 1650, val loss: 1.0191478729248047
Epoch 1660, training loss: 11.993776321411133 = 0.011413185857236385 + 2.0 * 5.991181373596191
Epoch 1660, val loss: 1.0218825340270996
Epoch 1670, training loss: 12.0032958984375 = 0.011231315322220325 + 2.0 * 5.996032238006592
Epoch 1670, val loss: 1.0245757102966309
Epoch 1680, training loss: 11.996138572692871 = 0.011058495379984379 + 2.0 * 5.992539882659912
Epoch 1680, val loss: 1.0271377563476562
Epoch 1690, training loss: 11.992006301879883 = 0.010888230986893177 + 2.0 * 5.990559101104736
Epoch 1690, val loss: 1.029671311378479
Epoch 1700, training loss: 12.000404357910156 = 0.010723314248025417 + 2.0 * 5.994840621948242
Epoch 1700, val loss: 1.0322052240371704
Epoch 1710, training loss: 11.989272117614746 = 0.010559258051216602 + 2.0 * 5.989356517791748
Epoch 1710, val loss: 1.034758448600769
Epoch 1720, training loss: 11.988004684448242 = 0.010401149280369282 + 2.0 * 5.988801956176758
Epoch 1720, val loss: 1.037283182144165
Epoch 1730, training loss: 11.989235877990723 = 0.010244778357446194 + 2.0 * 5.989495754241943
Epoch 1730, val loss: 1.0398411750793457
Epoch 1740, training loss: 11.997981071472168 = 0.010090518742799759 + 2.0 * 5.993945121765137
Epoch 1740, val loss: 1.0423502922058105
Epoch 1750, training loss: 11.990714073181152 = 0.009944981895387173 + 2.0 * 5.990384578704834
Epoch 1750, val loss: 1.04482901096344
Epoch 1760, training loss: 11.996216773986816 = 0.009801831096410751 + 2.0 * 5.9932074546813965
Epoch 1760, val loss: 1.0471590757369995
Epoch 1770, training loss: 11.986953735351562 = 0.009662852622568607 + 2.0 * 5.988645553588867
Epoch 1770, val loss: 1.049715518951416
Epoch 1780, training loss: 11.985380172729492 = 0.009526397101581097 + 2.0 * 5.987926959991455
Epoch 1780, val loss: 1.0521577596664429
Epoch 1790, training loss: 11.983545303344727 = 0.009389758110046387 + 2.0 * 5.987077713012695
Epoch 1790, val loss: 1.054614782333374
Epoch 1800, training loss: 11.985910415649414 = 0.009255237877368927 + 2.0 * 5.988327503204346
Epoch 1800, val loss: 1.0570536851882935
Epoch 1810, training loss: 11.993243217468262 = 0.009126655757427216 + 2.0 * 5.992058277130127
Epoch 1810, val loss: 1.0595183372497559
Epoch 1820, training loss: 11.984627723693848 = 0.009001398459076881 + 2.0 * 5.9878129959106445
Epoch 1820, val loss: 1.061793565750122
Epoch 1830, training loss: 11.987653732299805 = 0.008880437351763248 + 2.0 * 5.989386558532715
Epoch 1830, val loss: 1.064103126525879
Epoch 1840, training loss: 11.984777450561523 = 0.008761150762438774 + 2.0 * 5.98800802230835
Epoch 1840, val loss: 1.0663535594940186
Epoch 1850, training loss: 11.981919288635254 = 0.00864534080028534 + 2.0 * 5.986637115478516
Epoch 1850, val loss: 1.0687438249588013
Epoch 1860, training loss: 11.98050594329834 = 0.008529088459908962 + 2.0 * 5.985988616943359
Epoch 1860, val loss: 1.0710563659667969
Epoch 1870, training loss: 11.99096393585205 = 0.00841513928025961 + 2.0 * 5.991274356842041
Epoch 1870, val loss: 1.0733342170715332
Epoch 1880, training loss: 11.981518745422363 = 0.008305341005325317 + 2.0 * 5.986606597900391
Epoch 1880, val loss: 1.0756518840789795
Epoch 1890, training loss: 11.97864818572998 = 0.008197670802474022 + 2.0 * 5.985225200653076
Epoch 1890, val loss: 1.0778520107269287
Epoch 1900, training loss: 11.980927467346191 = 0.008091013878583908 + 2.0 * 5.9864182472229
Epoch 1900, val loss: 1.0801010131835938
Epoch 1910, training loss: 11.985033988952637 = 0.007986188866198063 + 2.0 * 5.988523960113525
Epoch 1910, val loss: 1.0823379755020142
Epoch 1920, training loss: 11.983631134033203 = 0.00788656622171402 + 2.0 * 5.987872123718262
Epoch 1920, val loss: 1.084560751914978
Epoch 1930, training loss: 11.979373931884766 = 0.007792314980179071 + 2.0 * 5.985790729522705
Epoch 1930, val loss: 1.0866358280181885
Epoch 1940, training loss: 11.977333068847656 = 0.007695360574871302 + 2.0 * 5.984818935394287
Epoch 1940, val loss: 1.0888011455535889
Epoch 1950, training loss: 11.981919288635254 = 0.007599648088216782 + 2.0 * 5.987159729003906
Epoch 1950, val loss: 1.0910279750823975
Epoch 1960, training loss: 11.974422454833984 = 0.007505418732762337 + 2.0 * 5.983458518981934
Epoch 1960, val loss: 1.0931354761123657
Epoch 1970, training loss: 11.974793434143066 = 0.007413702551275492 + 2.0 * 5.983689785003662
Epoch 1970, val loss: 1.095245361328125
Epoch 1980, training loss: 11.9743013381958 = 0.007323451805859804 + 2.0 * 5.983489036560059
Epoch 1980, val loss: 1.0974254608154297
Epoch 1990, training loss: 11.985440254211426 = 0.007233926560729742 + 2.0 * 5.989103317260742
Epoch 1990, val loss: 1.0995525121688843
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.7048
Flip ASR: 0.6444/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.697282791137695 = 1.949870228767395 + 2.0 * 8.373705863952637
Epoch 0, val loss: 1.9478662014007568
Epoch 10, training loss: 18.68292236328125 = 1.9398689270019531 + 2.0 * 8.371526718139648
Epoch 10, val loss: 1.93732750415802
Epoch 20, training loss: 18.649471282958984 = 1.9272279739379883 + 2.0 * 8.36112117767334
Epoch 20, val loss: 1.9233322143554688
Epoch 30, training loss: 18.510923385620117 = 1.9107348918914795 + 2.0 * 8.300094604492188
Epoch 30, val loss: 1.9046292304992676
Epoch 40, training loss: 17.624361038208008 = 1.8929986953735352 + 2.0 * 7.865681171417236
Epoch 40, val loss: 1.8843472003936768
Epoch 50, training loss: 16.405921936035156 = 1.8739640712738037 + 2.0 * 7.265978813171387
Epoch 50, val loss: 1.8638585805892944
Epoch 60, training loss: 15.861031532287598 = 1.8565791845321655 + 2.0 * 7.00222635269165
Epoch 60, val loss: 1.847110629081726
Epoch 70, training loss: 15.37226676940918 = 1.8431082963943481 + 2.0 * 6.7645792961120605
Epoch 70, val loss: 1.8342101573944092
Epoch 80, training loss: 14.941850662231445 = 1.8340678215026855 + 2.0 * 6.553891181945801
Epoch 80, val loss: 1.8251724243164062
Epoch 90, training loss: 14.748629570007324 = 1.825514554977417 + 2.0 * 6.461557388305664
Epoch 90, val loss: 1.8157320022583008
Epoch 100, training loss: 14.619256019592285 = 1.8153982162475586 + 2.0 * 6.401928901672363
Epoch 100, val loss: 1.805033564567566
Epoch 110, training loss: 14.508720397949219 = 1.8055016994476318 + 2.0 * 6.351609230041504
Epoch 110, val loss: 1.7949049472808838
Epoch 120, training loss: 14.414891242980957 = 1.7973079681396484 + 2.0 * 6.308791637420654
Epoch 120, val loss: 1.786197543144226
Epoch 130, training loss: 14.343188285827637 = 1.7900763750076294 + 2.0 * 6.276556015014648
Epoch 130, val loss: 1.7781239748001099
Epoch 140, training loss: 14.282508850097656 = 1.7826659679412842 + 2.0 * 6.2499213218688965
Epoch 140, val loss: 1.770078182220459
Epoch 150, training loss: 14.2298002243042 = 1.774968147277832 + 2.0 * 6.227416038513184
Epoch 150, val loss: 1.7619990110397339
Epoch 160, training loss: 14.184149742126465 = 1.766874074935913 + 2.0 * 6.208637714385986
Epoch 160, val loss: 1.7538892030715942
Epoch 170, training loss: 14.144073486328125 = 1.7583231925964355 + 2.0 * 6.192875385284424
Epoch 170, val loss: 1.7456778287887573
Epoch 180, training loss: 14.108671188354492 = 1.7491259574890137 + 2.0 * 6.179772853851318
Epoch 180, val loss: 1.7372515201568604
Epoch 190, training loss: 14.07138442993164 = 1.7390594482421875 + 2.0 * 6.166162490844727
Epoch 190, val loss: 1.7282874584197998
Epoch 200, training loss: 14.038495063781738 = 1.7276188135147095 + 2.0 * 6.15543794631958
Epoch 200, val loss: 1.7186503410339355
Epoch 210, training loss: 14.007680892944336 = 1.7144688367843628 + 2.0 * 6.146605968475342
Epoch 210, val loss: 1.7078768014907837
Epoch 220, training loss: 13.977258682250977 = 1.6993120908737183 + 2.0 * 6.138973236083984
Epoch 220, val loss: 1.6956487894058228
Epoch 230, training loss: 13.944256782531738 = 1.681717872619629 + 2.0 * 6.131269454956055
Epoch 230, val loss: 1.6817361116409302
Epoch 240, training loss: 13.91110610961914 = 1.6610301733016968 + 2.0 * 6.125038146972656
Epoch 240, val loss: 1.6655018329620361
Epoch 250, training loss: 13.876788139343262 = 1.6365221738815308 + 2.0 * 6.120132923126221
Epoch 250, val loss: 1.6463799476623535
Epoch 260, training loss: 13.840056419372559 = 1.6077440977096558 + 2.0 * 6.116156101226807
Epoch 260, val loss: 1.6238548755645752
Epoch 270, training loss: 13.795075416564941 = 1.5744848251342773 + 2.0 * 6.110295295715332
Epoch 270, val loss: 1.5977604389190674
Epoch 280, training loss: 13.747810363769531 = 1.5360273122787476 + 2.0 * 6.105891704559326
Epoch 280, val loss: 1.5675075054168701
Epoch 290, training loss: 13.697185516357422 = 1.4923571348190308 + 2.0 * 6.102414131164551
Epoch 290, val loss: 1.5329787731170654
Epoch 300, training loss: 13.642769813537598 = 1.444178581237793 + 2.0 * 6.099295616149902
Epoch 300, val loss: 1.4953218698501587
Epoch 310, training loss: 13.586141586303711 = 1.3935822248458862 + 2.0 * 6.096279621124268
Epoch 310, val loss: 1.4554990530014038
Epoch 320, training loss: 13.52829647064209 = 1.341266393661499 + 2.0 * 6.093514919281006
Epoch 320, val loss: 1.414458990097046
Epoch 330, training loss: 13.477516174316406 = 1.2888251543045044 + 2.0 * 6.094345569610596
Epoch 330, val loss: 1.373458743095398
Epoch 340, training loss: 13.415044784545898 = 1.2377066612243652 + 2.0 * 6.088669300079346
Epoch 340, val loss: 1.334207534790039
Epoch 350, training loss: 13.36424732208252 = 1.1891224384307861 + 2.0 * 6.087562561035156
Epoch 350, val loss: 1.2968292236328125
Epoch 360, training loss: 13.311772346496582 = 1.1433168649673462 + 2.0 * 6.084227561950684
Epoch 360, val loss: 1.262035608291626
Epoch 370, training loss: 13.262125015258789 = 1.1003060340881348 + 2.0 * 6.080909252166748
Epoch 370, val loss: 1.2295445203781128
Epoch 380, training loss: 13.221334457397461 = 1.0600093603134155 + 2.0 * 6.080662727355957
Epoch 380, val loss: 1.1993799209594727
Epoch 390, training loss: 13.174943923950195 = 1.0225698947906494 + 2.0 * 6.0761871337890625
Epoch 390, val loss: 1.1714786291122437
Epoch 400, training loss: 13.136783599853516 = 0.9870494604110718 + 2.0 * 6.074867248535156
Epoch 400, val loss: 1.145308256149292
Epoch 410, training loss: 13.098519325256348 = 0.9534367918968201 + 2.0 * 6.072541236877441
Epoch 410, val loss: 1.1207830905914307
Epoch 420, training loss: 13.06303596496582 = 0.9213718771934509 + 2.0 * 6.070832252502441
Epoch 420, val loss: 1.0976940393447876
Epoch 430, training loss: 13.025031089782715 = 0.8905009627342224 + 2.0 * 6.067265033721924
Epoch 430, val loss: 1.0754361152648926
Epoch 440, training loss: 12.991851806640625 = 0.8604506850242615 + 2.0 * 6.065700531005859
Epoch 440, val loss: 1.0541197061538696
Epoch 450, training loss: 12.964273452758789 = 0.8310251832008362 + 2.0 * 6.066624164581299
Epoch 450, val loss: 1.0334163904190063
Epoch 460, training loss: 12.926961898803711 = 0.8025715947151184 + 2.0 * 6.062195301055908
Epoch 460, val loss: 1.013242244720459
Epoch 470, training loss: 12.89411735534668 = 0.7745691537857056 + 2.0 * 6.059773921966553
Epoch 470, val loss: 0.9936529994010925
Epoch 480, training loss: 12.875968933105469 = 0.7472547292709351 + 2.0 * 6.064357280731201
Epoch 480, val loss: 0.9746360778808594
Epoch 490, training loss: 12.838574409484863 = 0.7207460403442383 + 2.0 * 6.0589141845703125
Epoch 490, val loss: 0.9566304683685303
Epoch 500, training loss: 12.805550575256348 = 0.6954019665718079 + 2.0 * 6.055074214935303
Epoch 500, val loss: 0.939443826675415
Epoch 510, training loss: 12.777602195739746 = 0.670802891254425 + 2.0 * 6.053399562835693
Epoch 510, val loss: 0.92295902967453
Epoch 520, training loss: 12.756834983825684 = 0.6470838785171509 + 2.0 * 6.054875373840332
Epoch 520, val loss: 0.9073438048362732
Epoch 530, training loss: 12.730466842651367 = 0.6245211362838745 + 2.0 * 6.052972793579102
Epoch 530, val loss: 0.892722487449646
Epoch 540, training loss: 12.703885078430176 = 0.6028528809547424 + 2.0 * 6.050516128540039
Epoch 540, val loss: 0.8791127800941467
Epoch 550, training loss: 12.677153587341309 = 0.5820323824882507 + 2.0 * 6.047560691833496
Epoch 550, val loss: 0.8663674592971802
Epoch 560, training loss: 12.66158390045166 = 0.5617803931236267 + 2.0 * 6.049901962280273
Epoch 560, val loss: 0.8543280363082886
Epoch 570, training loss: 12.634632110595703 = 0.5420215725898743 + 2.0 * 6.046305179595947
Epoch 570, val loss: 0.8432121276855469
Epoch 580, training loss: 12.61229133605957 = 0.5229719877243042 + 2.0 * 6.044659614562988
Epoch 580, val loss: 0.8328106999397278
Epoch 590, training loss: 12.58901596069336 = 0.5041424036026001 + 2.0 * 6.042436599731445
Epoch 590, val loss: 0.8230069875717163
Epoch 600, training loss: 12.574528694152832 = 0.4856241047382355 + 2.0 * 6.04445219039917
Epoch 600, val loss: 0.8136918544769287
Epoch 610, training loss: 12.551992416381836 = 0.46737661957740784 + 2.0 * 6.0423078536987305
Epoch 610, val loss: 0.8050400614738464
Epoch 620, training loss: 12.543732643127441 = 0.4494439959526062 + 2.0 * 6.047144412994385
Epoch 620, val loss: 0.7968771457672119
Epoch 630, training loss: 12.511651992797852 = 0.4318166971206665 + 2.0 * 6.039917469024658
Epoch 630, val loss: 0.7892492413520813
Epoch 640, training loss: 12.490364074707031 = 0.4145676791667938 + 2.0 * 6.037898063659668
Epoch 640, val loss: 0.7821112871170044
Epoch 650, training loss: 12.470208168029785 = 0.39741548895835876 + 2.0 * 6.036396503448486
Epoch 650, val loss: 0.7754031419754028
Epoch 660, training loss: 12.453006744384766 = 0.3804832398891449 + 2.0 * 6.036261558532715
Epoch 660, val loss: 0.7691394090652466
Epoch 670, training loss: 12.436970710754395 = 0.36388570070266724 + 2.0 * 6.0365424156188965
Epoch 670, val loss: 0.7634559869766235
Epoch 680, training loss: 12.419784545898438 = 0.3477911055088043 + 2.0 * 6.035996913909912
Epoch 680, val loss: 0.7584530115127563
Epoch 690, training loss: 12.401946067810059 = 0.3321862518787384 + 2.0 * 6.034879684448242
Epoch 690, val loss: 0.7540656328201294
Epoch 700, training loss: 12.380826950073242 = 0.31705912947654724 + 2.0 * 6.031883716583252
Epoch 700, val loss: 0.7503882646560669
Epoch 710, training loss: 12.37158203125 = 0.30250221490859985 + 2.0 * 6.034539699554443
Epoch 710, val loss: 0.7473220229148865
Epoch 720, training loss: 12.355978965759277 = 0.28855401277542114 + 2.0 * 6.033712387084961
Epoch 720, val loss: 0.7449607849121094
Epoch 730, training loss: 12.34281063079834 = 0.27539509534835815 + 2.0 * 6.033707618713379
Epoch 730, val loss: 0.7433354258537292
Epoch 740, training loss: 12.323396682739258 = 0.2628912329673767 + 2.0 * 6.030252933502197
Epoch 740, val loss: 0.7423218488693237
Epoch 750, training loss: 12.31061840057373 = 0.25110000371932983 + 2.0 * 6.029759407043457
Epoch 750, val loss: 0.7418586015701294
Epoch 760, training loss: 12.303585052490234 = 0.23998215794563293 + 2.0 * 6.031801223754883
Epoch 760, val loss: 0.7417489290237427
Epoch 770, training loss: 12.285125732421875 = 0.22953365743160248 + 2.0 * 6.027796268463135
Epoch 770, val loss: 0.7424159049987793
Epoch 780, training loss: 12.270600318908691 = 0.2196933478116989 + 2.0 * 6.025453567504883
Epoch 780, val loss: 0.7434622645378113
Epoch 790, training loss: 12.265120506286621 = 0.21036498248577118 + 2.0 * 6.027377605438232
Epoch 790, val loss: 0.7448338270187378
Epoch 800, training loss: 12.253408432006836 = 0.20159634947776794 + 2.0 * 6.025906085968018
Epoch 800, val loss: 0.7465792894363403
Epoch 810, training loss: 12.248332023620605 = 0.19335106015205383 + 2.0 * 6.027490615844727
Epoch 810, val loss: 0.7486767172813416
Epoch 820, training loss: 12.231205940246582 = 0.1856113225221634 + 2.0 * 6.022797107696533
Epoch 820, val loss: 0.7510965466499329
Epoch 830, training loss: 12.222225189208984 = 0.17825616896152496 + 2.0 * 6.021984577178955
Epoch 830, val loss: 0.75376296043396
Epoch 840, training loss: 12.212859153747559 = 0.17126047611236572 + 2.0 * 6.020799160003662
Epoch 840, val loss: 0.7566424608230591
Epoch 850, training loss: 12.217263221740723 = 0.16460521519184113 + 2.0 * 6.026329040527344
Epoch 850, val loss: 0.7597640156745911
Epoch 860, training loss: 12.213016510009766 = 0.15832462906837463 + 2.0 * 6.027346134185791
Epoch 860, val loss: 0.7629969120025635
Epoch 870, training loss: 12.194696426391602 = 0.15242129564285278 + 2.0 * 6.021137714385986
Epoch 870, val loss: 0.7664543986320496
Epoch 880, training loss: 12.184062004089355 = 0.14678774774074554 + 2.0 * 6.018637180328369
Epoch 880, val loss: 0.770051121711731
Epoch 890, training loss: 12.187495231628418 = 0.14138495922088623 + 2.0 * 6.023055076599121
Epoch 890, val loss: 0.7737351059913635
Epoch 900, training loss: 12.173628807067871 = 0.13629329204559326 + 2.0 * 6.018667697906494
Epoch 900, val loss: 0.7775101065635681
Epoch 910, training loss: 12.167184829711914 = 0.13139037787914276 + 2.0 * 6.017897129058838
Epoch 910, val loss: 0.7814680337905884
Epoch 920, training loss: 12.159214973449707 = 0.1266997754573822 + 2.0 * 6.0162577629089355
Epoch 920, val loss: 0.7854915261268616
Epoch 930, training loss: 12.153717041015625 = 0.12217552959918976 + 2.0 * 6.01577091217041
Epoch 930, val loss: 0.789591372013092
Epoch 940, training loss: 12.162894248962402 = 0.11783210188150406 + 2.0 * 6.022531032562256
Epoch 940, val loss: 0.793685257434845
Epoch 950, training loss: 12.15218734741211 = 0.11372222006320953 + 2.0 * 6.019232749938965
Epoch 950, val loss: 0.7978311777114868
Epoch 960, training loss: 12.142244338989258 = 0.10980679839849472 + 2.0 * 6.016218662261963
Epoch 960, val loss: 0.8020846843719482
Epoch 970, training loss: 12.132723808288574 = 0.10602563619613647 + 2.0 * 6.0133490562438965
Epoch 970, val loss: 0.8063919544219971
Epoch 980, training loss: 12.128408432006836 = 0.10237870365381241 + 2.0 * 6.013014793395996
Epoch 980, val loss: 0.8106654286384583
Epoch 990, training loss: 12.141727447509766 = 0.09886172413825989 + 2.0 * 6.021432876586914
Epoch 990, val loss: 0.8149417638778687
Epoch 1000, training loss: 12.127280235290527 = 0.09553538262844086 + 2.0 * 6.015872478485107
Epoch 1000, val loss: 0.8191568851470947
Epoch 1010, training loss: 12.11584186553955 = 0.09231549501419067 + 2.0 * 6.011763095855713
Epoch 1010, val loss: 0.8234660029411316
Epoch 1020, training loss: 12.11445140838623 = 0.08922585844993591 + 2.0 * 6.012612819671631
Epoch 1020, val loss: 0.827714204788208
Epoch 1030, training loss: 12.106637001037598 = 0.08625725656747818 + 2.0 * 6.010190010070801
Epoch 1030, val loss: 0.8318461179733276
Epoch 1040, training loss: 12.103205680847168 = 0.08341789245605469 + 2.0 * 6.009893894195557
Epoch 1040, val loss: 0.8360214829444885
Epoch 1050, training loss: 12.114482879638672 = 0.0806858018040657 + 2.0 * 6.0168986320495605
Epoch 1050, val loss: 0.8400601148605347
Epoch 1060, training loss: 12.10170841217041 = 0.07809565216302872 + 2.0 * 6.011806488037109
Epoch 1060, val loss: 0.8440754413604736
Epoch 1070, training loss: 12.094468116760254 = 0.07560784369707108 + 2.0 * 6.009429931640625
Epoch 1070, val loss: 0.8481965065002441
Epoch 1080, training loss: 12.089702606201172 = 0.0732167512178421 + 2.0 * 6.008243083953857
Epoch 1080, val loss: 0.8523427248001099
Epoch 1090, training loss: 12.084962844848633 = 0.07089419662952423 + 2.0 * 6.0070343017578125
Epoch 1090, val loss: 0.8565118908882141
Epoch 1100, training loss: 12.097030639648438 = 0.068665511906147 + 2.0 * 6.0141825675964355
Epoch 1100, val loss: 0.8606828451156616
Epoch 1110, training loss: 12.090568542480469 = 0.06651495397090912 + 2.0 * 6.012026786804199
Epoch 1110, val loss: 0.8647560477256775
Epoch 1120, training loss: 12.079023361206055 = 0.06447112560272217 + 2.0 * 6.0072760581970215
Epoch 1120, val loss: 0.8689542412757874
Epoch 1130, training loss: 12.073453903198242 = 0.0624927394092083 + 2.0 * 6.005480766296387
Epoch 1130, val loss: 0.8731322288513184
Epoch 1140, training loss: 12.07308578491211 = 0.06057429686188698 + 2.0 * 6.006255626678467
Epoch 1140, val loss: 0.8772749900817871
Epoch 1150, training loss: 12.072891235351562 = 0.05872329697012901 + 2.0 * 6.007083892822266
Epoch 1150, val loss: 0.8812894821166992
Epoch 1160, training loss: 12.068952560424805 = 0.056953586637973785 + 2.0 * 6.005999565124512
Epoch 1160, val loss: 0.8853418827056885
Epoch 1170, training loss: 12.068632125854492 = 0.0552566796541214 + 2.0 * 6.006687641143799
Epoch 1170, val loss: 0.8894883394241333
Epoch 1180, training loss: 12.063419342041016 = 0.05361166223883629 + 2.0 * 6.004903793334961
Epoch 1180, val loss: 0.8933733701705933
Epoch 1190, training loss: 12.058212280273438 = 0.052038613706827164 + 2.0 * 6.003087043762207
Epoch 1190, val loss: 0.8974276781082153
Epoch 1200, training loss: 12.05611515045166 = 0.05051502585411072 + 2.0 * 6.002799987792969
Epoch 1200, val loss: 0.901461660861969
Epoch 1210, training loss: 12.069145202636719 = 0.04904856160283089 + 2.0 * 6.0100483894348145
Epoch 1210, val loss: 0.9054247140884399
Epoch 1220, training loss: 12.053509712219238 = 0.04761895537376404 + 2.0 * 6.002945423126221
Epoch 1220, val loss: 0.9091079831123352
Epoch 1230, training loss: 12.049749374389648 = 0.04626522585749626 + 2.0 * 6.001741886138916
Epoch 1230, val loss: 0.9131359457969666
Epoch 1240, training loss: 12.046553611755371 = 0.04494273662567139 + 2.0 * 6.000805377960205
Epoch 1240, val loss: 0.9170058369636536
Epoch 1250, training loss: 12.052038192749023 = 0.04366657882928848 + 2.0 * 6.004185676574707
Epoch 1250, val loss: 0.9208877086639404
Epoch 1260, training loss: 12.045780181884766 = 0.042441826313734055 + 2.0 * 6.001669406890869
Epoch 1260, val loss: 0.9244443774223328
Epoch 1270, training loss: 12.04552173614502 = 0.041268397122621536 + 2.0 * 6.002126693725586
Epoch 1270, val loss: 0.9283610582351685
Epoch 1280, training loss: 12.0389986038208 = 0.040131133049726486 + 2.0 * 5.999433517456055
Epoch 1280, val loss: 0.9321381449699402
Epoch 1290, training loss: 12.043749809265137 = 0.039030835032463074 + 2.0 * 6.002359390258789
Epoch 1290, val loss: 0.935829758644104
Epoch 1300, training loss: 12.04173469543457 = 0.03796825185418129 + 2.0 * 6.001883029937744
Epoch 1300, val loss: 0.9393516778945923
Epoch 1310, training loss: 12.040026664733887 = 0.03695705905556679 + 2.0 * 6.001534938812256
Epoch 1310, val loss: 0.9429805278778076
Epoch 1320, training loss: 12.036925315856934 = 0.03598032891750336 + 2.0 * 6.000472545623779
Epoch 1320, val loss: 0.9466107487678528
Epoch 1330, training loss: 12.03315544128418 = 0.03503783419728279 + 2.0 * 5.999058723449707
Epoch 1330, val loss: 0.9501554369926453
Epoch 1340, training loss: 12.0327730178833 = 0.034126926213502884 + 2.0 * 5.999322891235352
Epoch 1340, val loss: 0.953747570514679
Epoch 1350, training loss: 12.038856506347656 = 0.03324580565094948 + 2.0 * 6.002805233001709
Epoch 1350, val loss: 0.9571934342384338
Epoch 1360, training loss: 12.028063774108887 = 0.03241121768951416 + 2.0 * 5.997826099395752
Epoch 1360, val loss: 0.9607263803482056
Epoch 1370, training loss: 12.025973320007324 = 0.03159391134977341 + 2.0 * 5.997189521789551
Epoch 1370, val loss: 0.9642805457115173
Epoch 1380, training loss: 12.025607109069824 = 0.030801737681031227 + 2.0 * 5.997402667999268
Epoch 1380, val loss: 0.9677823781967163
Epoch 1390, training loss: 12.034839630126953 = 0.030044974759221077 + 2.0 * 6.002397537231445
Epoch 1390, val loss: 0.971077024936676
Epoch 1400, training loss: 12.02400016784668 = 0.029314611107110977 + 2.0 * 5.997342586517334
Epoch 1400, val loss: 0.9743929505348206
Epoch 1410, training loss: 12.021389961242676 = 0.028614912182092667 + 2.0 * 5.996387481689453
Epoch 1410, val loss: 0.977879524230957
Epoch 1420, training loss: 12.019872665405273 = 0.027929767966270447 + 2.0 * 5.9959716796875
Epoch 1420, val loss: 0.9812214970588684
Epoch 1430, training loss: 12.032910346984863 = 0.027267152443528175 + 2.0 * 6.002821445465088
Epoch 1430, val loss: 0.9844268560409546
Epoch 1440, training loss: 12.022167205810547 = 0.02664077654480934 + 2.0 * 5.997763156890869
Epoch 1440, val loss: 0.9877678751945496
Epoch 1450, training loss: 12.018823623657227 = 0.02602674998342991 + 2.0 * 5.996398448944092
Epoch 1450, val loss: 0.9910815358161926
Epoch 1460, training loss: 12.014297485351562 = 0.02543305978178978 + 2.0 * 5.99443244934082
Epoch 1460, val loss: 0.9943493008613586
Epoch 1470, training loss: 12.012392044067383 = 0.024858081713318825 + 2.0 * 5.993766784667969
Epoch 1470, val loss: 0.9976339936256409
Epoch 1480, training loss: 12.019721031188965 = 0.024301456287503242 + 2.0 * 5.99770975112915
Epoch 1480, val loss: 1.000868320465088
Epoch 1490, training loss: 12.016916275024414 = 0.02376285009086132 + 2.0 * 5.99657678604126
Epoch 1490, val loss: 1.0038920640945435
Epoch 1500, training loss: 12.016230583190918 = 0.023244883865118027 + 2.0 * 5.996492862701416
Epoch 1500, val loss: 1.0070492029190063
Epoch 1510, training loss: 12.010772705078125 = 0.022745508700609207 + 2.0 * 5.994013786315918
Epoch 1510, val loss: 1.0101879835128784
Epoch 1520, training loss: 12.010238647460938 = 0.02226066030561924 + 2.0 * 5.993988990783691
Epoch 1520, val loss: 1.0133256912231445
Epoch 1530, training loss: 12.012652397155762 = 0.02178707718849182 + 2.0 * 5.9954328536987305
Epoch 1530, val loss: 1.0163203477859497
Epoch 1540, training loss: 12.014299392700195 = 0.02133171819150448 + 2.0 * 5.99648380279541
Epoch 1540, val loss: 1.0193288326263428
Epoch 1550, training loss: 12.007567405700684 = 0.020893171429634094 + 2.0 * 5.993337154388428
Epoch 1550, val loss: 1.0223681926727295
Epoch 1560, training loss: 12.004385948181152 = 0.020466048270463943 + 2.0 * 5.991960048675537
Epoch 1560, val loss: 1.0254826545715332
Epoch 1570, training loss: 12.003291130065918 = 0.020050620660185814 + 2.0 * 5.991620063781738
Epoch 1570, val loss: 1.0285239219665527
Epoch 1580, training loss: 12.022435188293457 = 0.019650593400001526 + 2.0 * 6.001392364501953
Epoch 1580, val loss: 1.0314314365386963
Epoch 1590, training loss: 12.007802963256836 = 0.01925562135875225 + 2.0 * 5.994273662567139
Epoch 1590, val loss: 1.0341060161590576
Epoch 1600, training loss: 12.004414558410645 = 0.018882103264331818 + 2.0 * 5.992766380310059
Epoch 1600, val loss: 1.0371911525726318
Epoch 1610, training loss: 12.0073881149292 = 0.018512554466724396 + 2.0 * 5.9944376945495605
Epoch 1610, val loss: 1.039941668510437
Epoch 1620, training loss: 12.001694679260254 = 0.01815824583172798 + 2.0 * 5.9917683601379395
Epoch 1620, val loss: 1.0428811311721802
Epoch 1630, training loss: 12.002097129821777 = 0.01781059429049492 + 2.0 * 5.992143154144287
Epoch 1630, val loss: 1.0456739664077759
Epoch 1640, training loss: 11.998135566711426 = 0.017473947256803513 + 2.0 * 5.990330696105957
Epoch 1640, val loss: 1.0484806299209595
Epoch 1650, training loss: 12.007946014404297 = 0.0171457938849926 + 2.0 * 5.9953999519348145
Epoch 1650, val loss: 1.0512630939483643
Epoch 1660, training loss: 11.995705604553223 = 0.01683114469051361 + 2.0 * 5.989437103271484
Epoch 1660, val loss: 1.053864598274231
Epoch 1670, training loss: 11.996996879577637 = 0.016524862498044968 + 2.0 * 5.990235805511475
Epoch 1670, val loss: 1.056654930114746
Epoch 1680, training loss: 11.999096870422363 = 0.016224997118115425 + 2.0 * 5.991436004638672
Epoch 1680, val loss: 1.0592769384384155
Epoch 1690, training loss: 11.993810653686523 = 0.015935450792312622 + 2.0 * 5.9889373779296875
Epoch 1690, val loss: 1.0620487928390503
Epoch 1700, training loss: 11.992855072021484 = 0.015650928020477295 + 2.0 * 5.988602161407471
Epoch 1700, val loss: 1.0647289752960205
Epoch 1710, training loss: 11.995328903198242 = 0.01537235639989376 + 2.0 * 5.989978313446045
Epoch 1710, val loss: 1.067379355430603
Epoch 1720, training loss: 11.997798919677734 = 0.015102582052350044 + 2.0 * 5.9913482666015625
Epoch 1720, val loss: 1.0698635578155518
Epoch 1730, training loss: 11.990653991699219 = 0.014844820834696293 + 2.0 * 5.9879045486450195
Epoch 1730, val loss: 1.0724296569824219
Epoch 1740, training loss: 11.990967750549316 = 0.014592867344617844 + 2.0 * 5.988187313079834
Epoch 1740, val loss: 1.075047492980957
Epoch 1750, training loss: 12.003498077392578 = 0.014344193041324615 + 2.0 * 5.994576930999756
Epoch 1750, val loss: 1.077500343322754
Epoch 1760, training loss: 11.991591453552246 = 0.01410753559321165 + 2.0 * 5.988741874694824
Epoch 1760, val loss: 1.0800328254699707
Epoch 1770, training loss: 11.988452911376953 = 0.01387432124465704 + 2.0 * 5.9872894287109375
Epoch 1770, val loss: 1.0825990438461304
Epoch 1780, training loss: 11.987751007080078 = 0.013645602390170097 + 2.0 * 5.987052917480469
Epoch 1780, val loss: 1.0851588249206543
Epoch 1790, training loss: 11.997537612915039 = 0.013423176482319832 + 2.0 * 5.9920573234558105
Epoch 1790, val loss: 1.0875974893569946
Epoch 1800, training loss: 11.994680404663086 = 0.013203785754740238 + 2.0 * 5.990738391876221
Epoch 1800, val loss: 1.0898473262786865
Epoch 1810, training loss: 11.987191200256348 = 0.012993469834327698 + 2.0 * 5.987098693847656
Epoch 1810, val loss: 1.0923771858215332
Epoch 1820, training loss: 11.9852876663208 = 0.012788035906851292 + 2.0 * 5.986249923706055
Epoch 1820, val loss: 1.094804048538208
Epoch 1830, training loss: 11.997185707092285 = 0.012586284428834915 + 2.0 * 5.992299556732178
Epoch 1830, val loss: 1.0971671342849731
Epoch 1840, training loss: 11.98701000213623 = 0.0123897111043334 + 2.0 * 5.98730993270874
Epoch 1840, val loss: 1.0995253324508667
Epoch 1850, training loss: 11.984526634216309 = 0.012198220007121563 + 2.0 * 5.986164093017578
Epoch 1850, val loss: 1.101945400238037
Epoch 1860, training loss: 11.995403289794922 = 0.012009249068796635 + 2.0 * 5.991696834564209
Epoch 1860, val loss: 1.1042746305465698
Epoch 1870, training loss: 11.987220764160156 = 0.011828513815999031 + 2.0 * 5.987696170806885
Epoch 1870, val loss: 1.1065537929534912
Epoch 1880, training loss: 11.992347717285156 = 0.011651724576950073 + 2.0 * 5.990347862243652
Epoch 1880, val loss: 1.1088110208511353
Epoch 1890, training loss: 11.982418060302734 = 0.011477609165012836 + 2.0 * 5.985470294952393
Epoch 1890, val loss: 1.1110762357711792
Epoch 1900, training loss: 11.980074882507324 = 0.011307223699986935 + 2.0 * 5.984384059906006
Epoch 1900, val loss: 1.1134202480316162
Epoch 1910, training loss: 11.981355667114258 = 0.011140064336359501 + 2.0 * 5.985107898712158
Epoch 1910, val loss: 1.1157314777374268
Epoch 1920, training loss: 11.990867614746094 = 0.010975235141813755 + 2.0 * 5.989946365356445
Epoch 1920, val loss: 1.1178531646728516
Epoch 1930, training loss: 11.984301567077637 = 0.01081814430654049 + 2.0 * 5.986741542816162
Epoch 1930, val loss: 1.1199971437454224
Epoch 1940, training loss: 11.980863571166992 = 0.010664921253919601 + 2.0 * 5.9850993156433105
Epoch 1940, val loss: 1.1223498582839966
Epoch 1950, training loss: 11.981822967529297 = 0.010512988083064556 + 2.0 * 5.985654830932617
Epoch 1950, val loss: 1.1245299577713013
Epoch 1960, training loss: 11.97992992401123 = 0.010363150388002396 + 2.0 * 5.984783172607422
Epoch 1960, val loss: 1.1267119646072388
Epoch 1970, training loss: 11.984317779541016 = 0.01021773274987936 + 2.0 * 5.9870500564575195
Epoch 1970, val loss: 1.1289373636245728
Epoch 1980, training loss: 11.982197761535645 = 0.010075842961668968 + 2.0 * 5.986061096191406
Epoch 1980, val loss: 1.1309850215911865
Epoch 1990, training loss: 11.977890014648438 = 0.00993817113339901 + 2.0 * 5.983975887298584
Epoch 1990, val loss: 1.1331959962844849
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8111
Overall ASR: 0.2915
Flip ASR: 0.3022/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.691160202026367 = 1.943481683731079 + 2.0 * 8.373839378356934
Epoch 0, val loss: 1.9536248445510864
Epoch 10, training loss: 18.679546356201172 = 1.9331276416778564 + 2.0 * 8.373208999633789
Epoch 10, val loss: 1.9422385692596436
Epoch 20, training loss: 18.65866470336914 = 1.9203158617019653 + 2.0 * 8.369174003601074
Epoch 20, val loss: 1.92769455909729
Epoch 30, training loss: 18.598068237304688 = 1.9032490253448486 + 2.0 * 8.34740924835205
Epoch 30, val loss: 1.9078805446624756
Epoch 40, training loss: 18.32257080078125 = 1.8833316564559937 + 2.0 * 8.219619750976562
Epoch 40, val loss: 1.8856902122497559
Epoch 50, training loss: 17.135059356689453 = 1.86519193649292 + 2.0 * 7.634933948516846
Epoch 50, val loss: 1.865609049797058
Epoch 60, training loss: 16.142723083496094 = 1.8477672338485718 + 2.0 * 7.147478103637695
Epoch 60, val loss: 1.8469111919403076
Epoch 70, training loss: 15.477762222290039 = 1.8321599960327148 + 2.0 * 6.822801113128662
Epoch 70, val loss: 1.8299059867858887
Epoch 80, training loss: 15.178412437438965 = 1.8181641101837158 + 2.0 * 6.680124282836914
Epoch 80, val loss: 1.8150157928466797
Epoch 90, training loss: 14.947333335876465 = 1.808202862739563 + 2.0 * 6.569565296173096
Epoch 90, val loss: 1.8036158084869385
Epoch 100, training loss: 14.777871131896973 = 1.7994143962860107 + 2.0 * 6.489228248596191
Epoch 100, val loss: 1.7933597564697266
Epoch 110, training loss: 14.624312400817871 = 1.7903814315795898 + 2.0 * 6.416965484619141
Epoch 110, val loss: 1.783124566078186
Epoch 120, training loss: 14.511053085327148 = 1.7811225652694702 + 2.0 * 6.364965438842773
Epoch 120, val loss: 1.773000717163086
Epoch 130, training loss: 14.420392990112305 = 1.7715765237808228 + 2.0 * 6.324408054351807
Epoch 130, val loss: 1.763181209564209
Epoch 140, training loss: 14.34298038482666 = 1.7624293565750122 + 2.0 * 6.290275573730469
Epoch 140, val loss: 1.753849983215332
Epoch 150, training loss: 14.278939247131348 = 1.7528828382492065 + 2.0 * 6.263028144836426
Epoch 150, val loss: 1.7446211576461792
Epoch 160, training loss: 14.221943855285645 = 1.7424347400665283 + 2.0 * 6.239754676818848
Epoch 160, val loss: 1.7349836826324463
Epoch 170, training loss: 14.176806449890137 = 1.730926513671875 + 2.0 * 6.222939968109131
Epoch 170, val loss: 1.7248296737670898
Epoch 180, training loss: 14.12865161895752 = 1.7180663347244263 + 2.0 * 6.205292701721191
Epoch 180, val loss: 1.7139755487442017
Epoch 190, training loss: 14.091543197631836 = 1.7035962343215942 + 2.0 * 6.193973541259766
Epoch 190, val loss: 1.7023913860321045
Epoch 200, training loss: 14.04812240600586 = 1.6874068975448608 + 2.0 * 6.180357933044434
Epoch 200, val loss: 1.6895573139190674
Epoch 210, training loss: 14.009774208068848 = 1.6688185930252075 + 2.0 * 6.170477867126465
Epoch 210, val loss: 1.6752026081085205
Epoch 220, training loss: 13.970824241638184 = 1.6472429037094116 + 2.0 * 6.16179084777832
Epoch 220, val loss: 1.658805251121521
Epoch 230, training loss: 13.931678771972656 = 1.6222470998764038 + 2.0 * 6.1547160148620605
Epoch 230, val loss: 1.6400543451309204
Epoch 240, training loss: 13.887228012084961 = 1.5935280323028564 + 2.0 * 6.146850109100342
Epoch 240, val loss: 1.6184815168380737
Epoch 250, training loss: 13.839254379272461 = 1.5601478815078735 + 2.0 * 6.139553070068359
Epoch 250, val loss: 1.5934295654296875
Epoch 260, training loss: 13.790776252746582 = 1.5217087268829346 + 2.0 * 6.134533882141113
Epoch 260, val loss: 1.5644901990890503
Epoch 270, training loss: 13.745811462402344 = 1.4787564277648926 + 2.0 * 6.1335272789001465
Epoch 270, val loss: 1.5317606925964355
Epoch 280, training loss: 13.678078651428223 = 1.4315353631973267 + 2.0 * 6.123271465301514
Epoch 280, val loss: 1.4960111379623413
Epoch 290, training loss: 13.614790916442871 = 1.380112886428833 + 2.0 * 6.117339134216309
Epoch 290, val loss: 1.4569013118743896
Epoch 300, training loss: 13.549408912658691 = 1.3245410919189453 + 2.0 * 6.112433910369873
Epoch 300, val loss: 1.4144624471664429
Epoch 310, training loss: 13.481422424316406 = 1.265324592590332 + 2.0 * 6.108048915863037
Epoch 310, val loss: 1.369165062904358
Epoch 320, training loss: 13.420702934265137 = 1.2041704654693604 + 2.0 * 6.108266353607178
Epoch 320, val loss: 1.3225915431976318
Epoch 330, training loss: 13.344663619995117 = 1.1431689262390137 + 2.0 * 6.100747585296631
Epoch 330, val loss: 1.2765105962753296
Epoch 340, training loss: 13.276740074157715 = 1.0827229022979736 + 2.0 * 6.09700870513916
Epoch 340, val loss: 1.2306634187698364
Epoch 350, training loss: 13.216813087463379 = 1.0236343145370483 + 2.0 * 6.0965895652771
Epoch 350, val loss: 1.1862159967422485
Epoch 360, training loss: 13.14942741394043 = 0.9678533673286438 + 2.0 * 6.090786933898926
Epoch 360, val loss: 1.1445608139038086
Epoch 370, training loss: 13.093501091003418 = 0.9161083698272705 + 2.0 * 6.088696479797363
Epoch 370, val loss: 1.10628342628479
Epoch 380, training loss: 13.040155410766602 = 0.8689424395561218 + 2.0 * 6.085606575012207
Epoch 380, val loss: 1.0715965032577515
Epoch 390, training loss: 12.990764617919922 = 0.8263837695121765 + 2.0 * 6.08219051361084
Epoch 390, val loss: 1.0406837463378906
Epoch 400, training loss: 12.949333190917969 = 0.7881561517715454 + 2.0 * 6.080588340759277
Epoch 400, val loss: 1.0135539770126343
Epoch 410, training loss: 12.908299446105957 = 0.7540220618247986 + 2.0 * 6.077138900756836
Epoch 410, val loss: 0.9899240136146545
Epoch 420, training loss: 12.87057113647461 = 0.7231392860412598 + 2.0 * 6.073715686798096
Epoch 420, val loss: 0.969254195690155
Epoch 430, training loss: 12.842771530151367 = 0.6948018670082092 + 2.0 * 6.073984622955322
Epoch 430, val loss: 0.9509848356246948
Epoch 440, training loss: 12.808029174804688 = 0.6687620878219604 + 2.0 * 6.069633483886719
Epoch 440, val loss: 0.9346058368682861
Epoch 450, training loss: 12.777864456176758 = 0.6442528367042542 + 2.0 * 6.066805839538574
Epoch 450, val loss: 0.9200148582458496
Epoch 460, training loss: 12.753361701965332 = 0.6210105419158936 + 2.0 * 6.06617546081543
Epoch 460, val loss: 0.9066938757896423
Epoch 470, training loss: 12.734790802001953 = 0.5989017486572266 + 2.0 * 6.067944526672363
Epoch 470, val loss: 0.8945808410644531
Epoch 480, training loss: 12.70067310333252 = 0.5777497887611389 + 2.0 * 6.061461448669434
Epoch 480, val loss: 0.883574903011322
Epoch 490, training loss: 12.679056167602539 = 0.5573934316635132 + 2.0 * 6.060831546783447
Epoch 490, val loss: 0.8734491467475891
Epoch 500, training loss: 12.651639938354492 = 0.5376019477844238 + 2.0 * 6.057018756866455
Epoch 500, val loss: 0.8641478419303894
Epoch 510, training loss: 12.638527870178223 = 0.5182844400405884 + 2.0 * 6.060121536254883
Epoch 510, val loss: 0.8555822968482971
Epoch 520, training loss: 12.617327690124512 = 0.49981364607810974 + 2.0 * 6.0587568283081055
Epoch 520, val loss: 0.8476828336715698
Epoch 530, training loss: 12.588275909423828 = 0.48188316822052 + 2.0 * 6.053196430206299
Epoch 530, val loss: 0.8406461477279663
Epoch 540, training loss: 12.569435119628906 = 0.4644940197467804 + 2.0 * 6.052470684051514
Epoch 540, val loss: 0.8343508243560791
Epoch 550, training loss: 12.553021430969238 = 0.4477755129337311 + 2.0 * 6.0526227951049805
Epoch 550, val loss: 0.8287066221237183
Epoch 560, training loss: 12.52796745300293 = 0.4317667782306671 + 2.0 * 6.048100471496582
Epoch 560, val loss: 0.8237293362617493
Epoch 570, training loss: 12.511114120483398 = 0.4163215160369873 + 2.0 * 6.047396183013916
Epoch 570, val loss: 0.8193846940994263
Epoch 580, training loss: 12.50185489654541 = 0.4013251066207886 + 2.0 * 6.050264835357666
Epoch 580, val loss: 0.8156859874725342
Epoch 590, training loss: 12.481614112854004 = 0.38703039288520813 + 2.0 * 6.0472917556762695
Epoch 590, val loss: 0.8124122619628906
Epoch 600, training loss: 12.46031665802002 = 0.3731935918331146 + 2.0 * 6.0435614585876465
Epoch 600, val loss: 0.8096789121627808
Epoch 610, training loss: 12.443357467651367 = 0.35974785685539246 + 2.0 * 6.041804790496826
Epoch 610, val loss: 0.8073252439498901
Epoch 620, training loss: 12.445859909057617 = 0.3466660678386688 + 2.0 * 6.049596786499023
Epoch 620, val loss: 0.8053746819496155
Epoch 630, training loss: 12.413777351379395 = 0.334091454744339 + 2.0 * 6.0398430824279785
Epoch 630, val loss: 0.8037790656089783
Epoch 640, training loss: 12.399950981140137 = 0.32194146513938904 + 2.0 * 6.039004802703857
Epoch 640, val loss: 0.8025565147399902
Epoch 650, training loss: 12.394742965698242 = 0.3101259171962738 + 2.0 * 6.042308330535889
Epoch 650, val loss: 0.8016552329063416
Epoch 660, training loss: 12.374798774719238 = 0.2985953092575073 + 2.0 * 6.038101673126221
Epoch 660, val loss: 0.8011271953582764
Epoch 670, training loss: 12.358418464660645 = 0.28740131855010986 + 2.0 * 6.035508632659912
Epoch 670, val loss: 0.8007962107658386
Epoch 680, training loss: 12.35172176361084 = 0.276423841714859 + 2.0 * 6.037649154663086
Epoch 680, val loss: 0.8007763624191284
Epoch 690, training loss: 12.338093757629395 = 0.265842080116272 + 2.0 * 6.036125659942627
Epoch 690, val loss: 0.8009301424026489
Epoch 700, training loss: 12.323110580444336 = 0.25560006499290466 + 2.0 * 6.033755302429199
Epoch 700, val loss: 0.8013732433319092
Epoch 710, training loss: 12.309722900390625 = 0.2456800788640976 + 2.0 * 6.032021522521973
Epoch 710, val loss: 0.8020336031913757
Epoch 720, training loss: 12.298873901367188 = 0.236026331782341 + 2.0 * 6.031423568725586
Epoch 720, val loss: 0.8029562830924988
Epoch 730, training loss: 12.28663444519043 = 0.22673162817955017 + 2.0 * 6.029951572418213
Epoch 730, val loss: 0.8040874600410461
Epoch 740, training loss: 12.278097152709961 = 0.21780014038085938 + 2.0 * 6.030148506164551
Epoch 740, val loss: 0.8055106997489929
Epoch 750, training loss: 12.265244483947754 = 0.2091882824897766 + 2.0 * 6.0280280113220215
Epoch 750, val loss: 0.8071500658988953
Epoch 760, training loss: 12.259726524353027 = 0.200909823179245 + 2.0 * 6.0294084548950195
Epoch 760, val loss: 0.809009313583374
Epoch 770, training loss: 12.267046928405762 = 0.1930229514837265 + 2.0 * 6.037012100219727
Epoch 770, val loss: 0.8111192584037781
Epoch 780, training loss: 12.242982864379883 = 0.1855514645576477 + 2.0 * 6.02871561050415
Epoch 780, val loss: 0.8134180307388306
Epoch 790, training loss: 12.229642868041992 = 0.1784367710351944 + 2.0 * 6.0256028175354
Epoch 790, val loss: 0.8158740997314453
Epoch 800, training loss: 12.220699310302734 = 0.17162203788757324 + 2.0 * 6.024538516998291
Epoch 800, val loss: 0.818561315536499
Epoch 810, training loss: 12.218382835388184 = 0.1651095598936081 + 2.0 * 6.026636600494385
Epoch 810, val loss: 0.8214593529701233
Epoch 820, training loss: 12.204044342041016 = 0.15889258682727814 + 2.0 * 6.022575855255127
Epoch 820, val loss: 0.824593722820282
Epoch 830, training loss: 12.198675155639648 = 0.1529741883277893 + 2.0 * 6.022850513458252
Epoch 830, val loss: 0.8279470801353455
Epoch 840, training loss: 12.201220512390137 = 0.14735867083072662 + 2.0 * 6.026930809020996
Epoch 840, val loss: 0.8314883708953857
Epoch 850, training loss: 12.188203811645508 = 0.14199626445770264 + 2.0 * 6.023103713989258
Epoch 850, val loss: 0.8352065086364746
Epoch 860, training loss: 12.185259819030762 = 0.13691338896751404 + 2.0 * 6.024173259735107
Epoch 860, val loss: 0.8390609622001648
Epoch 870, training loss: 12.178150177001953 = 0.13207390904426575 + 2.0 * 6.023037910461426
Epoch 870, val loss: 0.8430778384208679
Epoch 880, training loss: 12.165120124816895 = 0.12747015058994293 + 2.0 * 6.018825054168701
Epoch 880, val loss: 0.847168505191803
Epoch 890, training loss: 12.159967422485352 = 0.1230718344449997 + 2.0 * 6.0184478759765625
Epoch 890, val loss: 0.8513521552085876
Epoch 900, training loss: 12.156961441040039 = 0.11885659396648407 + 2.0 * 6.019052505493164
Epoch 900, val loss: 0.8556028604507446
Epoch 910, training loss: 12.158249855041504 = 0.1148366779088974 + 2.0 * 6.021706581115723
Epoch 910, val loss: 0.8599299192428589
Epoch 920, training loss: 12.148137092590332 = 0.11102622747421265 + 2.0 * 6.018555641174316
Epoch 920, val loss: 0.8642548322677612
Epoch 930, training loss: 12.140424728393555 = 0.10737603902816772 + 2.0 * 6.016524314880371
Epoch 930, val loss: 0.8686326742172241
Epoch 940, training loss: 12.133731842041016 = 0.10387686640024185 + 2.0 * 6.014927387237549
Epoch 940, val loss: 0.8730610013008118
Epoch 950, training loss: 12.136260986328125 = 0.10051128268241882 + 2.0 * 6.017874717712402
Epoch 950, val loss: 0.8774963617324829
Epoch 960, training loss: 12.124120712280273 = 0.0972856879234314 + 2.0 * 6.013417720794678
Epoch 960, val loss: 0.8819633722305298
Epoch 970, training loss: 12.120506286621094 = 0.09421266615390778 + 2.0 * 6.013146877288818
Epoch 970, val loss: 0.8864254951477051
Epoch 980, training loss: 12.116344451904297 = 0.09125056117773056 + 2.0 * 6.012547016143799
Epoch 980, val loss: 0.8909052014350891
Epoch 990, training loss: 12.12749195098877 = 0.0884031131863594 + 2.0 * 6.01954460144043
Epoch 990, val loss: 0.8953908085823059
Epoch 1000, training loss: 12.113117218017578 = 0.08569270372390747 + 2.0 * 6.013712406158447
Epoch 1000, val loss: 0.8999134302139282
Epoch 1010, training loss: 12.109899520874023 = 0.0830661803483963 + 2.0 * 6.013416767120361
Epoch 1010, val loss: 0.9044182300567627
Epoch 1020, training loss: 12.104207038879395 = 0.08055416494607925 + 2.0 * 6.011826515197754
Epoch 1020, val loss: 0.9088943600654602
Epoch 1030, training loss: 12.09926986694336 = 0.07812582701444626 + 2.0 * 6.0105719566345215
Epoch 1030, val loss: 0.9133608341217041
Epoch 1040, training loss: 12.103326797485352 = 0.07580135762691498 + 2.0 * 6.013762950897217
Epoch 1040, val loss: 0.9178146123886108
Epoch 1050, training loss: 12.093531608581543 = 0.07355707138776779 + 2.0 * 6.0099873542785645
Epoch 1050, val loss: 0.9222783446311951
Epoch 1060, training loss: 12.089143753051758 = 0.07138184458017349 + 2.0 * 6.008881092071533
Epoch 1060, val loss: 0.9266119599342346
Epoch 1070, training loss: 12.089266777038574 = 0.06929036229848862 + 2.0 * 6.009988307952881
Epoch 1070, val loss: 0.9309688210487366
Epoch 1080, training loss: 12.087646484375 = 0.0672784373164177 + 2.0 * 6.010183811187744
Epoch 1080, val loss: 0.9352596402168274
Epoch 1090, training loss: 12.082825660705566 = 0.06533476710319519 + 2.0 * 6.0087456703186035
Epoch 1090, val loss: 0.9395498037338257
Epoch 1100, training loss: 12.077869415283203 = 0.06346128135919571 + 2.0 * 6.007204055786133
Epoch 1100, val loss: 0.9437727928161621
Epoch 1110, training loss: 12.07367992401123 = 0.06165873259305954 + 2.0 * 6.00601053237915
Epoch 1110, val loss: 0.9479805827140808
Epoch 1120, training loss: 12.071318626403809 = 0.059907346963882446 + 2.0 * 6.005705833435059
Epoch 1120, val loss: 0.9522166848182678
Epoch 1130, training loss: 12.084089279174805 = 0.05821535363793373 + 2.0 * 6.012937068939209
Epoch 1130, val loss: 0.9564621448516846
Epoch 1140, training loss: 12.066020965576172 = 0.05659842491149902 + 2.0 * 6.004711151123047
Epoch 1140, val loss: 0.9606196880340576
Epoch 1150, training loss: 12.064473152160645 = 0.05503622815012932 + 2.0 * 6.00471830368042
Epoch 1150, val loss: 0.964716911315918
Epoch 1160, training loss: 12.061604499816895 = 0.05351996049284935 + 2.0 * 6.004042148590088
Epoch 1160, val loss: 0.9688410758972168
Epoch 1170, training loss: 12.071786880493164 = 0.05205836147069931 + 2.0 * 6.009864330291748
Epoch 1170, val loss: 0.9729636907577515
Epoch 1180, training loss: 12.059072494506836 = 0.05062995105981827 + 2.0 * 6.004221439361572
Epoch 1180, val loss: 0.9770402908325195
Epoch 1190, training loss: 12.05759334564209 = 0.04926576465368271 + 2.0 * 6.00416374206543
Epoch 1190, val loss: 0.9809629321098328
Epoch 1200, training loss: 12.052408218383789 = 0.04794158414006233 + 2.0 * 6.002233505249023
Epoch 1200, val loss: 0.9849226474761963
Epoch 1210, training loss: 12.05205249786377 = 0.04665612056851387 + 2.0 * 6.0026984214782715
Epoch 1210, val loss: 0.9888345003128052
Epoch 1220, training loss: 12.050933837890625 = 0.04541333392262459 + 2.0 * 6.002760410308838
Epoch 1220, val loss: 0.9927213191986084
Epoch 1230, training loss: 12.053955078125 = 0.04421203210949898 + 2.0 * 6.004871368408203
Epoch 1230, val loss: 0.996653139591217
Epoch 1240, training loss: 12.045753479003906 = 0.04305224120616913 + 2.0 * 6.001350402832031
Epoch 1240, val loss: 1.0004661083221436
Epoch 1250, training loss: 12.043825149536133 = 0.04192541167140007 + 2.0 * 6.000949859619141
Epoch 1250, val loss: 1.0043050050735474
Epoch 1260, training loss: 12.046723365783691 = 0.04082660377025604 + 2.0 * 6.00294828414917
Epoch 1260, val loss: 1.0081177949905396
Epoch 1270, training loss: 12.043354988098145 = 0.03977221995592117 + 2.0 * 6.001791477203369
Epoch 1270, val loss: 1.0118681192398071
Epoch 1280, training loss: 12.04810905456543 = 0.038754820823669434 + 2.0 * 6.0046772956848145
Epoch 1280, val loss: 1.0155941247940063
Epoch 1290, training loss: 12.036701202392578 = 0.037760067731142044 + 2.0 * 5.9994707107543945
Epoch 1290, val loss: 1.0192062854766846
Epoch 1300, training loss: 12.032459259033203 = 0.036806147545576096 + 2.0 * 5.99782657623291
Epoch 1300, val loss: 1.022836446762085
Epoch 1310, training loss: 12.030831336975098 = 0.03587708622217178 + 2.0 * 5.997477054595947
Epoch 1310, val loss: 1.0264742374420166
Epoch 1320, training loss: 12.030323028564453 = 0.034973882138729095 + 2.0 * 5.997674465179443
Epoch 1320, val loss: 1.0301241874694824
Epoch 1330, training loss: 12.042351722717285 = 0.03410191088914871 + 2.0 * 6.004125118255615
Epoch 1330, val loss: 1.0337495803833008
Epoch 1340, training loss: 12.033407211303711 = 0.03325214609503746 + 2.0 * 6.000077724456787
Epoch 1340, val loss: 1.037269949913025
Epoch 1350, training loss: 12.034262657165527 = 0.0324404276907444 + 2.0 * 6.000911235809326
Epoch 1350, val loss: 1.0407729148864746
Epoch 1360, training loss: 12.03027629852295 = 0.031648360192775726 + 2.0 * 5.999313831329346
Epoch 1360, val loss: 1.0442602634429932
Epoch 1370, training loss: 12.024609565734863 = 0.030886614695191383 + 2.0 * 5.996861457824707
Epoch 1370, val loss: 1.047597885131836
Epoch 1380, training loss: 12.020405769348145 = 0.03014526702463627 + 2.0 * 5.9951300621032715
Epoch 1380, val loss: 1.0510319471359253
Epoch 1390, training loss: 12.021346092224121 = 0.029419109225273132 + 2.0 * 5.9959635734558105
Epoch 1390, val loss: 1.0544170141220093
Epoch 1400, training loss: 12.030669212341309 = 0.028717784211039543 + 2.0 * 6.000975608825684
Epoch 1400, val loss: 1.05783212184906
Epoch 1410, training loss: 12.019552230834961 = 0.028047144412994385 + 2.0 * 5.995752334594727
Epoch 1410, val loss: 1.061163067817688
Epoch 1420, training loss: 12.019372940063477 = 0.0273917093873024 + 2.0 * 5.995990753173828
Epoch 1420, val loss: 1.0644822120666504
Epoch 1430, training loss: 12.015356063842773 = 0.026760511100292206 + 2.0 * 5.994297981262207
Epoch 1430, val loss: 1.0677227973937988
Epoch 1440, training loss: 12.017434120178223 = 0.0261483546346426 + 2.0 * 5.99564266204834
Epoch 1440, val loss: 1.071015477180481
Epoch 1450, training loss: 12.015602111816406 = 0.02555128186941147 + 2.0 * 5.995025634765625
Epoch 1450, val loss: 1.0743361711502075
Epoch 1460, training loss: 12.015832901000977 = 0.024975722655653954 + 2.0 * 5.995428562164307
Epoch 1460, val loss: 1.0776162147521973
Epoch 1470, training loss: 12.015456199645996 = 0.024421095848083496 + 2.0 * 5.995517730712891
Epoch 1470, val loss: 1.0808284282684326
Epoch 1480, training loss: 12.00908088684082 = 0.023882947862148285 + 2.0 * 5.992599010467529
Epoch 1480, val loss: 1.084099531173706
Epoch 1490, training loss: 12.01682186126709 = 0.023362789303064346 + 2.0 * 5.996729373931885
Epoch 1490, val loss: 1.0873160362243652
Epoch 1500, training loss: 12.006978034973145 = 0.0228547565639019 + 2.0 * 5.992061614990234
Epoch 1500, val loss: 1.0905752182006836
Epoch 1510, training loss: 12.004962921142578 = 0.02236430160701275 + 2.0 * 5.991299152374268
Epoch 1510, val loss: 1.0936360359191895
Epoch 1520, training loss: 12.003467559814453 = 0.02189023606479168 + 2.0 * 5.990788459777832
Epoch 1520, val loss: 1.0968002080917358
Epoch 1530, training loss: 12.004654884338379 = 0.021426664665341377 + 2.0 * 5.99161434173584
Epoch 1530, val loss: 1.100042700767517
Epoch 1540, training loss: 12.02556324005127 = 0.02097669430077076 + 2.0 * 6.002293109893799
Epoch 1540, val loss: 1.1032509803771973
Epoch 1550, training loss: 12.002143859863281 = 0.020546626299619675 + 2.0 * 5.990798473358154
Epoch 1550, val loss: 1.1063189506530762
Epoch 1560, training loss: 12.003186225891113 = 0.02013189159333706 + 2.0 * 5.991527080535889
Epoch 1560, val loss: 1.1092761754989624
Epoch 1570, training loss: 11.999019622802734 = 0.019726773723959923 + 2.0 * 5.9896464347839355
Epoch 1570, val loss: 1.1123805046081543
Epoch 1580, training loss: 11.998690605163574 = 0.019328534603118896 + 2.0 * 5.989681243896484
Epoch 1580, val loss: 1.115463376045227
Epoch 1590, training loss: 12.012421607971191 = 0.018940748646855354 + 2.0 * 5.996740341186523
Epoch 1590, val loss: 1.1185286045074463
Epoch 1600, training loss: 12.002106666564941 = 0.01857142522931099 + 2.0 * 5.991767406463623
Epoch 1600, val loss: 1.1216280460357666
Epoch 1610, training loss: 11.997771263122559 = 0.018207987770438194 + 2.0 * 5.989781856536865
Epoch 1610, val loss: 1.1245096921920776
Epoch 1620, training loss: 11.995245933532715 = 0.01785893179476261 + 2.0 * 5.988693714141846
Epoch 1620, val loss: 1.1275272369384766
Epoch 1630, training loss: 12.003439903259277 = 0.017516132444143295 + 2.0 * 5.992961883544922
Epoch 1630, val loss: 1.1304945945739746
Epoch 1640, training loss: 11.994080543518066 = 0.017183132469654083 + 2.0 * 5.988448619842529
Epoch 1640, val loss: 1.1335580348968506
Epoch 1650, training loss: 11.995880126953125 = 0.01686386577785015 + 2.0 * 5.989508152008057
Epoch 1650, val loss: 1.1362524032592773
Epoch 1660, training loss: 11.993209838867188 = 0.016551915556192398 + 2.0 * 5.98832893371582
Epoch 1660, val loss: 1.13917875289917
Epoch 1670, training loss: 11.9911470413208 = 0.016247890889644623 + 2.0 * 5.987449645996094
Epoch 1670, val loss: 1.1420514583587646
Epoch 1680, training loss: 12.005120277404785 = 0.015947576612234116 + 2.0 * 5.99458646774292
Epoch 1680, val loss: 1.1449906826019287
Epoch 1690, training loss: 12.000072479248047 = 0.015662919729948044 + 2.0 * 5.992204666137695
Epoch 1690, val loss: 1.147791862487793
Epoch 1700, training loss: 11.992400169372559 = 0.015382866375148296 + 2.0 * 5.988508701324463
Epoch 1700, val loss: 1.1505361795425415
Epoch 1710, training loss: 11.988855361938477 = 0.015115502290427685 + 2.0 * 5.986869812011719
Epoch 1710, val loss: 1.1533347368240356
Epoch 1720, training loss: 11.986745834350586 = 0.014850020408630371 + 2.0 * 5.985948085784912
Epoch 1720, val loss: 1.156149983406067
Epoch 1730, training loss: 11.98721981048584 = 0.014589698053896427 + 2.0 * 5.9863152503967285
Epoch 1730, val loss: 1.1589670181274414
Epoch 1740, training loss: 11.99752140045166 = 0.014335257932543755 + 2.0 * 5.991592884063721
Epoch 1740, val loss: 1.1617629528045654
Epoch 1750, training loss: 11.997735023498535 = 0.014090665616095066 + 2.0 * 5.991822242736816
Epoch 1750, val loss: 1.1645348072052002
Epoch 1760, training loss: 11.989171028137207 = 0.013855722732841969 + 2.0 * 5.98765754699707
Epoch 1760, val loss: 1.167157530784607
Epoch 1770, training loss: 11.985323905944824 = 0.013623112812638283 + 2.0 * 5.9858503341674805
Epoch 1770, val loss: 1.169798731803894
Epoch 1780, training loss: 11.98390007019043 = 0.013395819813013077 + 2.0 * 5.9852519035339355
Epoch 1780, val loss: 1.1725329160690308
Epoch 1790, training loss: 11.994696617126465 = 0.01317377109080553 + 2.0 * 5.9907612800598145
Epoch 1790, val loss: 1.1751903295516968
Epoch 1800, training loss: 11.984708786010742 = 0.012957731261849403 + 2.0 * 5.985875606536865
Epoch 1800, val loss: 1.1778227090835571
Epoch 1810, training loss: 11.983869552612305 = 0.012748646549880505 + 2.0 * 5.985560417175293
Epoch 1810, val loss: 1.1803513765335083
Epoch 1820, training loss: 11.984488487243652 = 0.01254433672875166 + 2.0 * 5.985971927642822
Epoch 1820, val loss: 1.1829915046691895
Epoch 1830, training loss: 11.985245704650879 = 0.012344965711236 + 2.0 * 5.9864501953125
Epoch 1830, val loss: 1.1856117248535156
Epoch 1840, training loss: 11.983778953552246 = 0.01214840542525053 + 2.0 * 5.985815048217773
Epoch 1840, val loss: 1.1882131099700928
Epoch 1850, training loss: 11.980748176574707 = 0.011957122944295406 + 2.0 * 5.984395503997803
Epoch 1850, val loss: 1.190731406211853
Epoch 1860, training loss: 11.985672950744629 = 0.011771046556532383 + 2.0 * 5.986950874328613
Epoch 1860, val loss: 1.1933097839355469
Epoch 1870, training loss: 11.986113548278809 = 0.011591660790145397 + 2.0 * 5.987260818481445
Epoch 1870, val loss: 1.1957907676696777
Epoch 1880, training loss: 11.979857444763184 = 0.011416970752179623 + 2.0 * 5.984220027923584
Epoch 1880, val loss: 1.198286771774292
Epoch 1890, training loss: 11.978907585144043 = 0.01124400831758976 + 2.0 * 5.983831882476807
Epoch 1890, val loss: 1.2007908821105957
Epoch 1900, training loss: 11.984644889831543 = 0.011077060364186764 + 2.0 * 5.986783981323242
Epoch 1900, val loss: 1.2033178806304932
Epoch 1910, training loss: 11.978504180908203 = 0.010911853052675724 + 2.0 * 5.983796119689941
Epoch 1910, val loss: 1.2058042287826538
Epoch 1920, training loss: 11.976820945739746 = 0.010751207359135151 + 2.0 * 5.983035087585449
Epoch 1920, val loss: 1.20819091796875
Epoch 1930, training loss: 11.978353500366211 = 0.010595514439046383 + 2.0 * 5.983879089355469
Epoch 1930, val loss: 1.2106750011444092
Epoch 1940, training loss: 11.984841346740723 = 0.01044162642210722 + 2.0 * 5.987199783325195
Epoch 1940, val loss: 1.2131315469741821
Epoch 1950, training loss: 11.977618217468262 = 0.010291457176208496 + 2.0 * 5.983663558959961
Epoch 1950, val loss: 1.215498685836792
Epoch 1960, training loss: 11.976561546325684 = 0.010146908462047577 + 2.0 * 5.9832072257995605
Epoch 1960, val loss: 1.2178871631622314
Epoch 1970, training loss: 11.985481262207031 = 0.01000444870442152 + 2.0 * 5.987738609313965
Epoch 1970, val loss: 1.2203749418258667
Epoch 1980, training loss: 11.974111557006836 = 0.009862752631306648 + 2.0 * 5.982124328613281
Epoch 1980, val loss: 1.2226994037628174
Epoch 1990, training loss: 11.973084449768066 = 0.009726375341415405 + 2.0 * 5.9816789627075195
Epoch 1990, val loss: 1.2249946594238281
=== picking the best model according to the performance on validation ===
target class rate on Vs: 0.8000
accuracy on clean test nodes: 0.7926
Overall ASR: 0.7269
Flip ASR: 0.6889/225 nodes
The final ASR:0.57442, 0.20025, Accuracy:0.80741, 0.01090
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11606])
remove edge: torch.Size([2, 9500])
updated graph: torch.Size([2, 10550])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8444
Overall ASR: 0.9852
Flip ASR: 0.9822/225 nodes
The final ASR:0.98032, 0.00348, Accuracy:0.83951, 0.00462
Begin epxeriment: cont_weight: 2 epoch:2000 der1:0.3 der2:0.2 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 18.705839157104492 = 1.958142638206482 + 2.0 * 8.373847961425781
Epoch 0, val loss: 1.9532811641693115
Epoch 10, training loss: 18.69402313232422 = 1.9475146532058716 + 2.0 * 8.37325382232666
Epoch 10, val loss: 1.9427282810211182
Epoch 20, training loss: 18.67301368713379 = 1.934244990348816 + 2.0 * 8.369384765625
Epoch 20, val loss: 1.929463505744934
Epoch 30, training loss: 18.598731994628906 = 1.9167163372039795 + 2.0 * 8.341008186340332
Epoch 30, val loss: 1.912230134010315
Epoch 40, training loss: 18.164730072021484 = 1.8970283269882202 + 2.0 * 8.133851051330566
Epoch 40, val loss: 1.8936277627944946
Epoch 50, training loss: 16.4420108795166 = 1.8767693042755127 + 2.0 * 7.282620429992676
Epoch 50, val loss: 1.8744559288024902
Epoch 60, training loss: 15.755318641662598 = 1.8599752187728882 + 2.0 * 6.947671890258789
Epoch 60, val loss: 1.8592710494995117
Epoch 70, training loss: 15.262179374694824 = 1.8475313186645508 + 2.0 * 6.707324028015137
Epoch 70, val loss: 1.8474481105804443
Epoch 80, training loss: 14.965481758117676 = 1.8360143899917603 + 2.0 * 6.564733505249023
Epoch 80, val loss: 1.837245225906372
Epoch 90, training loss: 14.798029899597168 = 1.8259791135787964 + 2.0 * 6.486025333404541
Epoch 90, val loss: 1.8280061483383179
Epoch 100, training loss: 14.670217514038086 = 1.8150485754013062 + 2.0 * 6.427584648132324
Epoch 100, val loss: 1.8183528184890747
Epoch 110, training loss: 14.567341804504395 = 1.8050367832183838 + 2.0 * 6.381152629852295
Epoch 110, val loss: 1.8095557689666748
Epoch 120, training loss: 14.481070518493652 = 1.7965433597564697 + 2.0 * 6.342263698577881
Epoch 120, val loss: 1.8021286725997925
Epoch 130, training loss: 14.405345916748047 = 1.7886604070663452 + 2.0 * 6.308342933654785
Epoch 130, val loss: 1.7951996326446533
Epoch 140, training loss: 14.342415809631348 = 1.7806260585784912 + 2.0 * 6.280894756317139
Epoch 140, val loss: 1.7881598472595215
Epoch 150, training loss: 14.286307334899902 = 1.7723106145858765 + 2.0 * 6.256998538970947
Epoch 150, val loss: 1.7808109521865845
Epoch 160, training loss: 14.235036849975586 = 1.763411045074463 + 2.0 * 6.235812664031982
Epoch 160, val loss: 1.7732861042022705
Epoch 170, training loss: 14.190088272094727 = 1.7534703016281128 + 2.0 * 6.218308925628662
Epoch 170, val loss: 1.7650320529937744
Epoch 180, training loss: 14.152174949645996 = 1.7422574758529663 + 2.0 * 6.204958915710449
Epoch 180, val loss: 1.7557597160339355
Epoch 190, training loss: 14.11532974243164 = 1.7292282581329346 + 2.0 * 6.193050861358643
Epoch 190, val loss: 1.7452462911605835
Epoch 200, training loss: 14.080696105957031 = 1.7141575813293457 + 2.0 * 6.183269500732422
Epoch 200, val loss: 1.7330284118652344
Epoch 210, training loss: 14.046403884887695 = 1.6965107917785645 + 2.0 * 6.1749467849731445
Epoch 210, val loss: 1.7187753915786743
Epoch 220, training loss: 14.014089584350586 = 1.6759676933288574 + 2.0 * 6.169061183929443
Epoch 220, val loss: 1.7022889852523804
Epoch 230, training loss: 13.974345207214355 = 1.652274489402771 + 2.0 * 6.161035537719727
Epoch 230, val loss: 1.6832406520843506
Epoch 240, training loss: 13.931439399719238 = 1.624922513961792 + 2.0 * 6.153258323669434
Epoch 240, val loss: 1.6609846353530884
Epoch 250, training loss: 13.887049674987793 = 1.5930479764938354 + 2.0 * 6.147000789642334
Epoch 250, val loss: 1.634968876838684
Epoch 260, training loss: 13.841192245483398 = 1.5564244985580444 + 2.0 * 6.142384052276611
Epoch 260, val loss: 1.6049201488494873
Epoch 270, training loss: 13.788995742797852 = 1.5155715942382812 + 2.0 * 6.136712074279785
Epoch 270, val loss: 1.5714516639709473
Epoch 280, training loss: 13.733546257019043 = 1.4705355167388916 + 2.0 * 6.131505489349365
Epoch 280, val loss: 1.5344903469085693
Epoch 290, training loss: 13.675530433654785 = 1.421846628189087 + 2.0 * 6.126842021942139
Epoch 290, val loss: 1.4947278499603271
Epoch 300, training loss: 13.618314743041992 = 1.3711016178131104 + 2.0 * 6.1236066818237305
Epoch 300, val loss: 1.4538679122924805
Epoch 310, training loss: 13.557920455932617 = 1.3202775716781616 + 2.0 * 6.118821620941162
Epoch 310, val loss: 1.413184642791748
Epoch 320, training loss: 13.500088691711426 = 1.2696959972381592 + 2.0 * 6.115196228027344
Epoch 320, val loss: 1.373175859451294
Epoch 330, training loss: 13.448619842529297 = 1.2202707529067993 + 2.0 * 6.1141743659973145
Epoch 330, val loss: 1.3346166610717773
Epoch 340, training loss: 13.391058921813965 = 1.1728917360305786 + 2.0 * 6.109083652496338
Epoch 340, val loss: 1.2983916997909546
Epoch 350, training loss: 13.337467193603516 = 1.1273432970046997 + 2.0 * 6.105062007904053
Epoch 350, val loss: 1.2637497186660767
Epoch 360, training loss: 13.286553382873535 = 1.0830737352371216 + 2.0 * 6.101739883422852
Epoch 360, val loss: 1.2303823232650757
Epoch 370, training loss: 13.242351531982422 = 1.040428638458252 + 2.0 * 6.100961208343506
Epoch 370, val loss: 1.198622226715088
Epoch 380, training loss: 13.195545196533203 = 1.0000475645065308 + 2.0 * 6.097748756408691
Epoch 380, val loss: 1.1685878038406372
Epoch 390, training loss: 13.149116516113281 = 0.961097240447998 + 2.0 * 6.094009876251221
Epoch 390, val loss: 1.1395238637924194
Epoch 400, training loss: 13.107512474060059 = 0.923232913017273 + 2.0 * 6.092139720916748
Epoch 400, val loss: 1.1112240552902222
Epoch 410, training loss: 13.071645736694336 = 0.8866028189659119 + 2.0 * 6.092521667480469
Epoch 410, val loss: 1.0841593742370605
Epoch 420, training loss: 13.025964736938477 = 0.8517563343048096 + 2.0 * 6.087104320526123
Epoch 420, val loss: 1.0582654476165771
Epoch 430, training loss: 12.986213684082031 = 0.8180301785469055 + 2.0 * 6.084091663360596
Epoch 430, val loss: 1.0334296226501465
Epoch 440, training loss: 12.9497652053833 = 0.7852621674537659 + 2.0 * 6.08225154876709
Epoch 440, val loss: 1.0096355676651
Epoch 450, training loss: 12.913652420043945 = 0.7535626888275146 + 2.0 * 6.080044746398926
Epoch 450, val loss: 0.9868910908699036
Epoch 460, training loss: 12.879745483398438 = 0.7232616543769836 + 2.0 * 6.07824182510376
Epoch 460, val loss: 0.9656497240066528
Epoch 470, training loss: 12.84640884399414 = 0.6939576268196106 + 2.0 * 6.076225757598877
Epoch 470, val loss: 0.9454204440116882
Epoch 480, training loss: 12.814967155456543 = 0.665454089641571 + 2.0 * 6.074756622314453
Epoch 480, val loss: 0.9261823892593384
Epoch 490, training loss: 12.789338111877441 = 0.6378117799758911 + 2.0 * 6.07576322555542
Epoch 490, val loss: 0.9080690145492554
Epoch 500, training loss: 12.757542610168457 = 0.6113452911376953 + 2.0 * 6.073098659515381
Epoch 500, val loss: 0.8913296461105347
Epoch 510, training loss: 12.726229667663574 = 0.5856353044509888 + 2.0 * 6.0702972412109375
Epoch 510, val loss: 0.8757277131080627
Epoch 520, training loss: 12.696784019470215 = 0.5604525208473206 + 2.0 * 6.0681657791137695
Epoch 520, val loss: 0.8610044717788696
Epoch 530, training loss: 12.682401657104492 = 0.5359523892402649 + 2.0 * 6.0732245445251465
Epoch 530, val loss: 0.8472864627838135
Epoch 540, training loss: 12.647231101989746 = 0.5121607184410095 + 2.0 * 6.067535400390625
Epoch 540, val loss: 0.8348263502120972
Epoch 550, training loss: 12.617307662963867 = 0.4890362620353699 + 2.0 * 6.064135551452637
Epoch 550, val loss: 0.8234362602233887
Epoch 560, training loss: 12.598766326904297 = 0.4664734899997711 + 2.0 * 6.066146373748779
Epoch 560, val loss: 0.8129690289497375
Epoch 570, training loss: 12.570222854614258 = 0.44477424025535583 + 2.0 * 6.0627241134643555
Epoch 570, val loss: 0.803576648235321
Epoch 580, training loss: 12.544635772705078 = 0.4236951470375061 + 2.0 * 6.060470104217529
Epoch 580, val loss: 0.7953794598579407
Epoch 590, training loss: 12.525979042053223 = 0.4034530520439148 + 2.0 * 6.061263084411621
Epoch 590, val loss: 0.7881569266319275
Epoch 600, training loss: 12.498797416687012 = 0.3840758502483368 + 2.0 * 6.057360649108887
Epoch 600, val loss: 0.7820370197296143
Epoch 610, training loss: 12.484291076660156 = 0.3654543161392212 + 2.0 * 6.059418201446533
Epoch 610, val loss: 0.7768716216087341
Epoch 620, training loss: 12.465804100036621 = 0.34766829013824463 + 2.0 * 6.059067726135254
Epoch 620, val loss: 0.7725886702537537
Epoch 630, training loss: 12.441367149353027 = 0.33078205585479736 + 2.0 * 6.05529260635376
Epoch 630, val loss: 0.7691945433616638
Epoch 640, training loss: 12.4212007522583 = 0.31463804841041565 + 2.0 * 6.053281307220459
Epoch 640, val loss: 0.7666195631027222
Epoch 650, training loss: 12.406123161315918 = 0.2991780936717987 + 2.0 * 6.053472518920898
Epoch 650, val loss: 0.7647109627723694
Epoch 660, training loss: 12.39120864868164 = 0.2844226658344269 + 2.0 * 6.0533928871154785
Epoch 660, val loss: 0.7634267807006836
Epoch 670, training loss: 12.382871627807617 = 0.2703263461589813 + 2.0 * 6.056272506713867
Epoch 670, val loss: 0.7626612186431885
Epoch 680, training loss: 12.3593168258667 = 0.2570289671421051 + 2.0 * 6.051144123077393
Epoch 680, val loss: 0.7623992562294006
Epoch 690, training loss: 12.342079162597656 = 0.24435961246490479 + 2.0 * 6.048859596252441
Epoch 690, val loss: 0.7627131342887878
Epoch 700, training loss: 12.325728416442871 = 0.23224873840808868 + 2.0 * 6.0467400550842285
Epoch 700, val loss: 0.7633693814277649
Epoch 710, training loss: 12.322810173034668 = 0.22068876028060913 + 2.0 * 6.051060676574707
Epoch 710, val loss: 0.7644224762916565
Epoch 720, training loss: 12.312572479248047 = 0.2097415328025818 + 2.0 * 6.05141544342041
Epoch 720, val loss: 0.7655410766601562
Epoch 730, training loss: 12.293088912963867 = 0.19947244226932526 + 2.0 * 6.046808242797852
Epoch 730, val loss: 0.7673261165618896
Epoch 740, training loss: 12.277987480163574 = 0.18971101939678192 + 2.0 * 6.044138431549072
Epoch 740, val loss: 0.7692630290985107
Epoch 750, training loss: 12.265219688415527 = 0.18044546246528625 + 2.0 * 6.042387008666992
Epoch 750, val loss: 0.7714309692382812
Epoch 760, training loss: 12.256027221679688 = 0.17164720594882965 + 2.0 * 6.042190074920654
Epoch 760, val loss: 0.7739731669425964
Epoch 770, training loss: 12.257745742797852 = 0.1633371263742447 + 2.0 * 6.047204494476318
Epoch 770, val loss: 0.7766162157058716
Epoch 780, training loss: 12.242912292480469 = 0.15561959147453308 + 2.0 * 6.043646335601807
Epoch 780, val loss: 0.7795439958572388
Epoch 790, training loss: 12.228819847106934 = 0.14834310114383698 + 2.0 * 6.040238380432129
Epoch 790, val loss: 0.7828046083450317
Epoch 800, training loss: 12.218884468078613 = 0.14146101474761963 + 2.0 * 6.0387115478515625
Epoch 800, val loss: 0.7861724495887756
Epoch 810, training loss: 12.228271484375 = 0.13497646152973175 + 2.0 * 6.046647548675537
Epoch 810, val loss: 0.7897887229919434
Epoch 820, training loss: 12.212116241455078 = 0.12884916365146637 + 2.0 * 6.041633605957031
Epoch 820, val loss: 0.7932053804397583
Epoch 830, training loss: 12.19926643371582 = 0.12315531820058823 + 2.0 * 6.038055419921875
Epoch 830, val loss: 0.7971821427345276
Epoch 840, training loss: 12.189903259277344 = 0.11776631325483322 + 2.0 * 6.036068439483643
Epoch 840, val loss: 0.8011902570724487
Epoch 850, training loss: 12.182219505310059 = 0.11266007274389267 + 2.0 * 6.0347795486450195
Epoch 850, val loss: 0.8053135275840759
Epoch 860, training loss: 12.179849624633789 = 0.10782075673341751 + 2.0 * 6.036014556884766
Epoch 860, val loss: 0.8095613121986389
Epoch 870, training loss: 12.170191764831543 = 0.10326895862817764 + 2.0 * 6.033461570739746
Epoch 870, val loss: 0.8137966394424438
Epoch 880, training loss: 12.167436599731445 = 0.09898648411035538 + 2.0 * 6.034224987030029
Epoch 880, val loss: 0.818240225315094
Epoch 890, training loss: 12.160630226135254 = 0.09493791311979294 + 2.0 * 6.032845973968506
Epoch 890, val loss: 0.8227972388267517
Epoch 900, training loss: 12.163490295410156 = 0.09110472351312637 + 2.0 * 6.036192893981934
Epoch 900, val loss: 0.8272429704666138
Epoch 910, training loss: 12.150290489196777 = 0.08751575648784637 + 2.0 * 6.0313873291015625
Epoch 910, val loss: 0.8318060040473938
Epoch 920, training loss: 12.145442008972168 = 0.0841178372502327 + 2.0 * 6.0306620597839355
Epoch 920, val loss: 0.8364949822425842
Epoch 930, training loss: 12.14673137664795 = 0.08088759332895279 + 2.0 * 6.03292179107666
Epoch 930, val loss: 0.8411552309989929
Epoch 940, training loss: 12.1444091796875 = 0.0778249055147171 + 2.0 * 6.033292293548584
Epoch 940, val loss: 0.845716118812561
Epoch 950, training loss: 12.135693550109863 = 0.07494623214006424 + 2.0 * 6.030373573303223
Epoch 950, val loss: 0.8503791093826294
Epoch 960, training loss: 12.127701759338379 = 0.07220792025327682 + 2.0 * 6.02774715423584
Epoch 960, val loss: 0.8551246523857117
Epoch 970, training loss: 12.122843742370605 = 0.06959842145442963 + 2.0 * 6.026622772216797
Epoch 970, val loss: 0.8598427772521973
Epoch 980, training loss: 12.129119873046875 = 0.06711572408676147 + 2.0 * 6.031002044677734
Epoch 980, val loss: 0.8646056056022644
Epoch 990, training loss: 12.123655319213867 = 0.0647590234875679 + 2.0 * 6.02944803237915
Epoch 990, val loss: 0.8691635131835938
Epoch 1000, training loss: 12.116572380065918 = 0.06254445761442184 + 2.0 * 6.027013778686523
Epoch 1000, val loss: 0.8738377094268799
Epoch 1010, training loss: 12.110943794250488 = 0.060428962111473083 + 2.0 * 6.025257587432861
Epoch 1010, val loss: 0.8784617185592651
Epoch 1020, training loss: 12.113785743713379 = 0.05840959772467613 + 2.0 * 6.027688026428223
Epoch 1020, val loss: 0.8830184936523438
Epoch 1030, training loss: 12.107914924621582 = 0.05649956315755844 + 2.0 * 6.025707721710205
Epoch 1030, val loss: 0.8874980211257935
Epoch 1040, training loss: 12.1031494140625 = 0.05468614026904106 + 2.0 * 6.024231433868408
Epoch 1040, val loss: 0.8920107483863831
Epoch 1050, training loss: 12.09709644317627 = 0.052951402962207794 + 2.0 * 6.0220723152160645
Epoch 1050, val loss: 0.89658522605896
Epoch 1060, training loss: 12.094659805297852 = 0.05128157138824463 + 2.0 * 6.021688938140869
Epoch 1060, val loss: 0.9010895490646362
Epoch 1070, training loss: 12.110003471374512 = 0.04968349635601044 + 2.0 * 6.030159950256348
Epoch 1070, val loss: 0.9054059982299805
Epoch 1080, training loss: 12.093990325927734 = 0.048181917518377304 + 2.0 * 6.022904396057129
Epoch 1080, val loss: 0.9097095727920532
Epoch 1090, training loss: 12.086122512817383 = 0.04674848914146423 + 2.0 * 6.019687175750732
Epoch 1090, val loss: 0.9141976833343506
Epoch 1100, training loss: 12.090570449829102 = 0.045363061130046844 + 2.0 * 6.022603511810303
Epoch 1100, val loss: 0.9185481667518616
Epoch 1110, training loss: 12.081833839416504 = 0.04403838887810707 + 2.0 * 6.018897533416748
Epoch 1110, val loss: 0.9227302670478821
Epoch 1120, training loss: 12.080362319946289 = 0.04277372360229492 + 2.0 * 6.018794536590576
Epoch 1120, val loss: 0.9270594120025635
Epoch 1130, training loss: 12.077230453491211 = 0.041556812822818756 + 2.0 * 6.017837047576904
Epoch 1130, val loss: 0.9313347935676575
Epoch 1140, training loss: 12.085518836975098 = 0.04038365185260773 + 2.0 * 6.0225677490234375
Epoch 1140, val loss: 0.9355806112289429
Epoch 1150, training loss: 12.08409595489502 = 0.039260126650333405 + 2.0 * 6.022418022155762
Epoch 1150, val loss: 0.9395707845687866
Epoch 1160, training loss: 12.070145606994629 = 0.038191262632608414 + 2.0 * 6.015977382659912
Epoch 1160, val loss: 0.9436940550804138
Epoch 1170, training loss: 12.069759368896484 = 0.03715875744819641 + 2.0 * 6.016300201416016
Epoch 1170, val loss: 0.9478591084480286
Epoch 1180, training loss: 12.083126068115234 = 0.036157671362161636 + 2.0 * 6.023484230041504
Epoch 1180, val loss: 0.9519308805465698
Epoch 1190, training loss: 12.07004451751709 = 0.03521221876144409 + 2.0 * 6.017416000366211
Epoch 1190, val loss: 0.9559078812599182
Epoch 1200, training loss: 12.065418243408203 = 0.03429132327437401 + 2.0 * 6.015563488006592
Epoch 1200, val loss: 0.9598818421363831
Epoch 1210, training loss: 12.062226295471191 = 0.03340793401002884 + 2.0 * 6.014409065246582
Epoch 1210, val loss: 0.9639073014259338
Epoch 1220, training loss: 12.077044486999512 = 0.032552435994148254 + 2.0 * 6.02224588394165
Epoch 1220, val loss: 0.9678576588630676
Epoch 1230, training loss: 12.062905311584473 = 0.031729090958833694 + 2.0 * 6.015588283538818
Epoch 1230, val loss: 0.9716436266899109
Epoch 1240, training loss: 12.061929702758789 = 0.03094109706580639 + 2.0 * 6.015494346618652
Epoch 1240, val loss: 0.9755579233169556
Epoch 1250, training loss: 12.056081771850586 = 0.030182000249624252 + 2.0 * 6.0129499435424805
Epoch 1250, val loss: 0.9793717265129089
Epoch 1260, training loss: 12.054831504821777 = 0.029448600485920906 + 2.0 * 6.012691497802734
Epoch 1260, val loss: 0.9832496643066406
Epoch 1270, training loss: 12.058938026428223 = 0.028742335736751556 + 2.0 * 6.015097618103027
Epoch 1270, val loss: 0.9870615601539612
Epoch 1280, training loss: 12.050690650939941 = 0.02805706486105919 + 2.0 * 6.011316776275635
Epoch 1280, val loss: 0.9907032251358032
Epoch 1290, training loss: 12.051505088806152 = 0.027397066354751587 + 2.0 * 6.012053966522217
Epoch 1290, val loss: 0.9944305419921875
Epoch 1300, training loss: 12.056836128234863 = 0.026765594258904457 + 2.0 * 6.015035152435303
Epoch 1300, val loss: 0.9980199933052063
Epoch 1310, training loss: 12.046046257019043 = 0.026153989136219025 + 2.0 * 6.009946346282959
Epoch 1310, val loss: 1.0015758275985718
Epoch 1320, training loss: 12.044366836547852 = 0.025566400960087776 + 2.0 * 6.009400367736816
Epoch 1320, val loss: 1.0052212476730347
Epoch 1330, training loss: 12.04318904876709 = 0.024991462007164955 + 2.0 * 6.009099006652832
Epoch 1330, val loss: 1.008859634399414
Epoch 1340, training loss: 12.058356285095215 = 0.024432718753814697 + 2.0 * 6.016961574554443
Epoch 1340, val loss: 1.0123549699783325
Epoch 1350, training loss: 12.04777717590332 = 0.023899370804429054 + 2.0 * 6.01193904876709
Epoch 1350, val loss: 1.0158443450927734
Epoch 1360, training loss: 12.046661376953125 = 0.02338104136288166 + 2.0 * 6.0116400718688965
Epoch 1360, val loss: 1.0192430019378662
Epoch 1370, training loss: 12.038028717041016 = 0.0228812824934721 + 2.0 * 6.00757360458374
Epoch 1370, val loss: 1.0227484703063965
Epoch 1380, training loss: 12.041364669799805 = 0.022395752370357513 + 2.0 * 6.00948429107666
Epoch 1380, val loss: 1.0262395143508911
Epoch 1390, training loss: 12.04179859161377 = 0.021924231201410294 + 2.0 * 6.009937286376953
Epoch 1390, val loss: 1.0295343399047852
Epoch 1400, training loss: 12.035774230957031 = 0.02147085778415203 + 2.0 * 6.0071516036987305
Epoch 1400, val loss: 1.0329248905181885
Epoch 1410, training loss: 12.038281440734863 = 0.02102946862578392 + 2.0 * 6.0086259841918945
Epoch 1410, val loss: 1.036354660987854
Epoch 1420, training loss: 12.040118217468262 = 0.020598875358700752 + 2.0 * 6.009759902954102
Epoch 1420, val loss: 1.0396347045898438
Epoch 1430, training loss: 12.030672073364258 = 0.02018948271870613 + 2.0 * 6.005241394042969
Epoch 1430, val loss: 1.0429189205169678
Epoch 1440, training loss: 12.030411720275879 = 0.019790256395936012 + 2.0 * 6.005310535430908
Epoch 1440, val loss: 1.046236276626587
Epoch 1450, training loss: 12.030595779418945 = 0.019398285076022148 + 2.0 * 6.005598545074463
Epoch 1450, val loss: 1.0495394468307495
Epoch 1460, training loss: 12.045271873474121 = 0.019016331061720848 + 2.0 * 6.01312780380249
Epoch 1460, val loss: 1.0526667833328247
Epoch 1470, training loss: 12.034461975097656 = 0.018651187419891357 + 2.0 * 6.00790548324585
Epoch 1470, val loss: 1.0557987689971924
Epoch 1480, training loss: 12.026640892028809 = 0.01829799823462963 + 2.0 * 6.004171371459961
Epoch 1480, val loss: 1.0590400695800781
Epoch 1490, training loss: 12.02635669708252 = 0.017952561378479004 + 2.0 * 6.004201889038086
Epoch 1490, val loss: 1.0622533559799194
Epoch 1500, training loss: 12.038185119628906 = 0.017616339027881622 + 2.0 * 6.010284423828125
Epoch 1500, val loss: 1.0653290748596191
Epoch 1510, training loss: 12.02755069732666 = 0.017286445945501328 + 2.0 * 6.00513219833374
Epoch 1510, val loss: 1.0683026313781738
Epoch 1520, training loss: 12.028566360473633 = 0.0169709250330925 + 2.0 * 6.005797863006592
Epoch 1520, val loss: 1.0713948011398315
Epoch 1530, training loss: 12.02269172668457 = 0.016660640016198158 + 2.0 * 6.003015518188477
Epoch 1530, val loss: 1.07442307472229
Epoch 1540, training loss: 12.021512031555176 = 0.016362495720386505 + 2.0 * 6.002574920654297
Epoch 1540, val loss: 1.0775387287139893
Epoch 1550, training loss: 12.020130157470703 = 0.016067834571003914 + 2.0 * 6.002031326293945
Epoch 1550, val loss: 1.0806186199188232
Epoch 1560, training loss: 12.029144287109375 = 0.015781696885824203 + 2.0 * 6.006681442260742
Epoch 1560, val loss: 1.083640694618225
Epoch 1570, training loss: 12.022692680358887 = 0.015501800924539566 + 2.0 * 6.003595352172852
Epoch 1570, val loss: 1.0865117311477661
Epoch 1580, training loss: 12.020264625549316 = 0.015232485719025135 + 2.0 * 6.002516269683838
Epoch 1580, val loss: 1.0894397497177124
Epoch 1590, training loss: 12.02098560333252 = 0.014970529824495316 + 2.0 * 6.003007411956787
Epoch 1590, val loss: 1.0923689603805542
Epoch 1600, training loss: 12.020405769348145 = 0.014714686200022697 + 2.0 * 6.002845764160156
Epoch 1600, val loss: 1.0952702760696411
Epoch 1610, training loss: 12.017301559448242 = 0.014468182809650898 + 2.0 * 6.0014166831970215
Epoch 1610, val loss: 1.0981866121292114
Epoch 1620, training loss: 12.015298843383789 = 0.014224771410226822 + 2.0 * 6.000536918640137
Epoch 1620, val loss: 1.1010663509368896
Epoch 1630, training loss: 12.015287399291992 = 0.013987165875732899 + 2.0 * 6.000649929046631
Epoch 1630, val loss: 1.103918433189392
Epoch 1640, training loss: 12.0144681930542 = 0.01375547144562006 + 2.0 * 6.000356197357178
Epoch 1640, val loss: 1.1067928075790405
Epoch 1650, training loss: 12.020905494689941 = 0.013530109077692032 + 2.0 * 6.003687858581543
Epoch 1650, val loss: 1.1095637083053589
Epoch 1660, training loss: 12.011533737182617 = 0.013311323709785938 + 2.0 * 5.999111175537109
Epoch 1660, val loss: 1.1122806072235107
Epoch 1670, training loss: 12.018888473510742 = 0.013100400567054749 + 2.0 * 6.002893924713135
Epoch 1670, val loss: 1.1150906085968018
Epoch 1680, training loss: 12.009866714477539 = 0.012891517020761967 + 2.0 * 5.99848747253418
Epoch 1680, val loss: 1.1177072525024414
Epoch 1690, training loss: 12.008316993713379 = 0.012690689414739609 + 2.0 * 5.9978132247924805
Epoch 1690, val loss: 1.1203632354736328
Epoch 1700, training loss: 12.006936073303223 = 0.012493018060922623 + 2.0 * 5.99722146987915
Epoch 1700, val loss: 1.1231086254119873
Epoch 1710, training loss: 12.006834030151367 = 0.012296919710934162 + 2.0 * 5.9972686767578125
Epoch 1710, val loss: 1.1258662939071655
Epoch 1720, training loss: 12.019000053405762 = 0.012104450725018978 + 2.0 * 6.003448009490967
Epoch 1720, val loss: 1.1285531520843506
Epoch 1730, training loss: 12.021255493164062 = 0.01192179974168539 + 2.0 * 6.004666805267334
Epoch 1730, val loss: 1.1310392618179321
Epoch 1740, training loss: 12.007791519165039 = 0.011742151342332363 + 2.0 * 5.9980244636535645
Epoch 1740, val loss: 1.1334980726242065
Epoch 1750, training loss: 12.004278182983398 = 0.011569622904062271 + 2.0 * 5.996354103088379
Epoch 1750, val loss: 1.136230230331421
Epoch 1760, training loss: 12.00344181060791 = 0.011397517286241055 + 2.0 * 5.9960222244262695
Epoch 1760, val loss: 1.1388719081878662
Epoch 1770, training loss: 12.010370254516602 = 0.011226232163608074 + 2.0 * 5.999571800231934
Epoch 1770, val loss: 1.1414732933044434
Epoch 1780, training loss: 12.006365776062012 = 0.01106209121644497 + 2.0 * 5.997652053833008
Epoch 1780, val loss: 1.1437793970108032
Epoch 1790, training loss: 12.007022857666016 = 0.010904557071626186 + 2.0 * 5.998059272766113
Epoch 1790, val loss: 1.146238923072815
Epoch 1800, training loss: 12.001108169555664 = 0.010750596411526203 + 2.0 * 5.995178699493408
Epoch 1800, val loss: 1.148820161819458
Epoch 1810, training loss: 12.001180648803711 = 0.010595859959721565 + 2.0 * 5.9952921867370605
Epoch 1810, val loss: 1.1513725519180298
Epoch 1820, training loss: 12.01583480834961 = 0.010445811785757542 + 2.0 * 6.002694606781006
Epoch 1820, val loss: 1.153822660446167
Epoch 1830, training loss: 12.00448989868164 = 0.010297410190105438 + 2.0 * 5.997096061706543
Epoch 1830, val loss: 1.1561694145202637
Epoch 1840, training loss: 12.000718116760254 = 0.010155263356864452 + 2.0 * 5.995281219482422
Epoch 1840, val loss: 1.1587282419204712
Epoch 1850, training loss: 12.005305290222168 = 0.01001354493200779 + 2.0 * 5.997645854949951
Epoch 1850, val loss: 1.1611828804016113
Epoch 1860, training loss: 11.997779846191406 = 0.009874943643808365 + 2.0 * 5.99395227432251
Epoch 1860, val loss: 1.163539171218872
Epoch 1870, training loss: 11.998461723327637 = 0.009739409200847149 + 2.0 * 5.99436092376709
Epoch 1870, val loss: 1.1658997535705566
Epoch 1880, training loss: 12.007125854492188 = 0.009606853127479553 + 2.0 * 5.9987592697143555
Epoch 1880, val loss: 1.1683281660079956
Epoch 1890, training loss: 11.998395919799805 = 0.009476865641772747 + 2.0 * 5.994459629058838
Epoch 1890, val loss: 1.1706167459487915
Epoch 1900, training loss: 11.996587753295898 = 0.009350461885333061 + 2.0 * 5.993618488311768
Epoch 1900, val loss: 1.1729028224945068
Epoch 1910, training loss: 11.995264053344727 = 0.009226060472428799 + 2.0 * 5.993019104003906
Epoch 1910, val loss: 1.175367832183838
Epoch 1920, training loss: 12.00184440612793 = 0.00910203531384468 + 2.0 * 5.996371269226074
Epoch 1920, val loss: 1.177738070487976
Epoch 1930, training loss: 11.993734359741211 = 0.008982752449810505 + 2.0 * 5.99237585067749
Epoch 1930, val loss: 1.1799086332321167
Epoch 1940, training loss: 11.992734909057617 = 0.008867468684911728 + 2.0 * 5.991933822631836
Epoch 1940, val loss: 1.1821907758712769
Epoch 1950, training loss: 11.995872497558594 = 0.00875283032655716 + 2.0 * 5.993559837341309
Epoch 1950, val loss: 1.1845297813415527
Epoch 1960, training loss: 11.998788833618164 = 0.008640252985060215 + 2.0 * 5.995074272155762
Epoch 1960, val loss: 1.186802625656128
Epoch 1970, training loss: 11.995218276977539 = 0.008530819788575172 + 2.0 * 5.993343830108643
Epoch 1970, val loss: 1.1889535188674927
Epoch 1980, training loss: 11.994195938110352 = 0.008424345403909683 + 2.0 * 5.992885589599609
Epoch 1980, val loss: 1.1911890506744385
Epoch 1990, training loss: 11.992108345031738 = 0.00831835437566042 + 2.0 * 5.9918951988220215
Epoch 1990, val loss: 1.1935186386108398
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.7011
Flip ASR: 0.6489/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.693740844726562 = 1.9461535215377808 + 2.0 * 8.373793601989746
Epoch 0, val loss: 1.944229006767273
Epoch 10, training loss: 18.68218994140625 = 1.936199426651001 + 2.0 * 8.372995376586914
Epoch 10, val loss: 1.9340159893035889
Epoch 20, training loss: 18.66058349609375 = 1.923869252204895 + 2.0 * 8.368356704711914
Epoch 20, val loss: 1.9208712577819824
Epoch 30, training loss: 18.590646743774414 = 1.9074729681015015 + 2.0 * 8.34158706665039
Epoch 30, val loss: 1.9033148288726807
Epoch 40, training loss: 18.277297973632812 = 1.8887568712234497 + 2.0 * 8.194270133972168
Epoch 40, val loss: 1.8846977949142456
Epoch 50, training loss: 16.954347610473633 = 1.8700529336929321 + 2.0 * 7.542147636413574
Epoch 50, val loss: 1.8665070533752441
Epoch 60, training loss: 16.004316329956055 = 1.855057954788208 + 2.0 * 7.074629306793213
Epoch 60, val loss: 1.8521839380264282
Epoch 70, training loss: 15.409235000610352 = 1.8420709371566772 + 2.0 * 6.7835822105407715
Epoch 70, val loss: 1.8389990329742432
Epoch 80, training loss: 15.062127113342285 = 1.8311699628829956 + 2.0 * 6.615478515625
Epoch 80, val loss: 1.8281421661376953
Epoch 90, training loss: 14.84780502319336 = 1.8211488723754883 + 2.0 * 6.5133280754089355
Epoch 90, val loss: 1.817810297012329
Epoch 100, training loss: 14.710136413574219 = 1.8116998672485352 + 2.0 * 6.449218273162842
Epoch 100, val loss: 1.8079156875610352
Epoch 110, training loss: 14.590418815612793 = 1.8029606342315674 + 2.0 * 6.393729209899902
Epoch 110, val loss: 1.7986623048782349
Epoch 120, training loss: 14.503352165222168 = 1.795031189918518 + 2.0 * 6.354160308837891
Epoch 120, val loss: 1.7901893854141235
Epoch 130, training loss: 14.430169105529785 = 1.78707754611969 + 2.0 * 6.321545600891113
Epoch 130, val loss: 1.7816168069839478
Epoch 140, training loss: 14.366426467895508 = 1.7786604166030884 + 2.0 * 6.293882846832275
Epoch 140, val loss: 1.7727586030960083
Epoch 150, training loss: 14.3103609085083 = 1.7698842287063599 + 2.0 * 6.270238399505615
Epoch 150, val loss: 1.7638294696807861
Epoch 160, training loss: 14.261296272277832 = 1.760580062866211 + 2.0 * 6.2503581047058105
Epoch 160, val loss: 1.7547228336334229
Epoch 170, training loss: 14.217284202575684 = 1.750454068183899 + 2.0 * 6.233415126800537
Epoch 170, val loss: 1.7451132535934448
Epoch 180, training loss: 14.171411514282227 = 1.7392321825027466 + 2.0 * 6.216089725494385
Epoch 180, val loss: 1.7348480224609375
Epoch 190, training loss: 14.130358695983887 = 1.7265769243240356 + 2.0 * 6.20189094543457
Epoch 190, val loss: 1.72357177734375
Epoch 200, training loss: 14.095625877380371 = 1.7119680643081665 + 2.0 * 6.191828727722168
Epoch 200, val loss: 1.7110035419464111
Epoch 210, training loss: 14.057170867919922 = 1.6951794624328613 + 2.0 * 6.180995941162109
Epoch 210, val loss: 1.6966557502746582
Epoch 220, training loss: 14.021297454833984 = 1.6755599975585938 + 2.0 * 6.172868728637695
Epoch 220, val loss: 1.680229663848877
Epoch 230, training loss: 13.984654426574707 = 1.6525096893310547 + 2.0 * 6.166072368621826
Epoch 230, val loss: 1.661215901374817
Epoch 240, training loss: 13.943999290466309 = 1.6255797147750854 + 2.0 * 6.159209728240967
Epoch 240, val loss: 1.6392070055007935
Epoch 250, training loss: 13.899585723876953 = 1.5937860012054443 + 2.0 * 6.152899742126465
Epoch 250, val loss: 1.6133750677108765
Epoch 260, training loss: 13.852513313293457 = 1.5563522577285767 + 2.0 * 6.148080348968506
Epoch 260, val loss: 1.583114743232727
Epoch 270, training loss: 13.80219554901123 = 1.5134814977645874 + 2.0 * 6.144357204437256
Epoch 270, val loss: 1.548478126525879
Epoch 280, training loss: 13.742948532104492 = 1.4660273790359497 + 2.0 * 6.138460636138916
Epoch 280, val loss: 1.5106161832809448
Epoch 290, training loss: 13.681164741516113 = 1.4144246578216553 + 2.0 * 6.1333699226379395
Epoch 290, val loss: 1.4693855047225952
Epoch 300, training loss: 13.618607521057129 = 1.3592578172683716 + 2.0 * 6.129674911499023
Epoch 300, val loss: 1.4254989624023438
Epoch 310, training loss: 13.553093910217285 = 1.3016862869262695 + 2.0 * 6.125703811645508
Epoch 310, val loss: 1.3799965381622314
Epoch 320, training loss: 13.496267318725586 = 1.2430684566497803 + 2.0 * 6.126599311828613
Epoch 320, val loss: 1.3343170881271362
Epoch 330, training loss: 13.429606437683105 = 1.1857930421829224 + 2.0 * 6.121906757354736
Epoch 330, val loss: 1.2906382083892822
Epoch 340, training loss: 13.36237907409668 = 1.1303707361221313 + 2.0 * 6.11600399017334
Epoch 340, val loss: 1.248371958732605
Epoch 350, training loss: 13.303098678588867 = 1.0762238502502441 + 2.0 * 6.113437175750732
Epoch 350, val loss: 1.2072776556015015
Epoch 360, training loss: 13.24759292602539 = 1.0238823890686035 + 2.0 * 6.1118550300598145
Epoch 360, val loss: 1.1680632829666138
Epoch 370, training loss: 13.186482429504395 = 0.9740504622459412 + 2.0 * 6.106215953826904
Epoch 370, val loss: 1.1306042671203613
Epoch 380, training loss: 13.133760452270508 = 0.9264275431632996 + 2.0 * 6.103666305541992
Epoch 380, val loss: 1.0950102806091309
Epoch 390, training loss: 13.088823318481445 = 0.8813722729682922 + 2.0 * 6.103725433349609
Epoch 390, val loss: 1.0613524913787842
Epoch 400, training loss: 13.037369728088379 = 0.8398855924606323 + 2.0 * 6.0987420082092285
Epoch 400, val loss: 1.030752182006836
Epoch 410, training loss: 12.992029190063477 = 0.8019561171531677 + 2.0 * 6.095036506652832
Epoch 410, val loss: 1.0029666423797607
Epoch 420, training loss: 12.951255798339844 = 0.7670440673828125 + 2.0 * 6.092105865478516
Epoch 420, val loss: 0.9778321385383606
Epoch 430, training loss: 12.921346664428711 = 0.7354640364646912 + 2.0 * 6.0929412841796875
Epoch 430, val loss: 0.9553146958351135
Epoch 440, training loss: 12.881160736083984 = 0.7069368362426758 + 2.0 * 6.087111949920654
Epoch 440, val loss: 0.9358792304992676
Epoch 450, training loss: 12.848189353942871 = 0.680776834487915 + 2.0 * 6.083706378936768
Epoch 450, val loss: 0.9187096953392029
Epoch 460, training loss: 12.827919960021973 = 0.6565223336219788 + 2.0 * 6.08569860458374
Epoch 460, val loss: 0.9033474922180176
Epoch 470, training loss: 12.793588638305664 = 0.6341016292572021 + 2.0 * 6.079743385314941
Epoch 470, val loss: 0.8898828625679016
Epoch 480, training loss: 12.76675033569336 = 0.6131689548492432 + 2.0 * 6.076790809631348
Epoch 480, val loss: 0.8778778910636902
Epoch 490, training loss: 12.745800971984863 = 0.5931700468063354 + 2.0 * 6.076315402984619
Epoch 490, val loss: 0.8669031858444214
Epoch 500, training loss: 12.725064277648926 = 0.5740213394165039 + 2.0 * 6.075521469116211
Epoch 500, val loss: 0.856709897518158
Epoch 510, training loss: 12.697318077087402 = 0.555591344833374 + 2.0 * 6.070863246917725
Epoch 510, val loss: 0.8474111557006836
Epoch 520, training loss: 12.678436279296875 = 0.5376619100570679 + 2.0 * 6.070387363433838
Epoch 520, val loss: 0.8386893272399902
Epoch 530, training loss: 12.65427303314209 = 0.5202041268348694 + 2.0 * 6.0670342445373535
Epoch 530, val loss: 0.8303856253623962
Epoch 540, training loss: 12.639692306518555 = 0.5031065940856934 + 2.0 * 6.068292617797852
Epoch 540, val loss: 0.8226680159568787
Epoch 550, training loss: 12.614990234375 = 0.4863694906234741 + 2.0 * 6.064310550689697
Epoch 550, val loss: 0.8155086040496826
Epoch 560, training loss: 12.59431266784668 = 0.4699802100658417 + 2.0 * 6.062166213989258
Epoch 560, val loss: 0.8087351322174072
Epoch 570, training loss: 12.584749221801758 = 0.4538210332393646 + 2.0 * 6.065464019775391
Epoch 570, val loss: 0.8023762106895447
Epoch 580, training loss: 12.557543754577637 = 0.4381062090396881 + 2.0 * 6.059718608856201
Epoch 580, val loss: 0.7965918183326721
Epoch 590, training loss: 12.541095733642578 = 0.4227353036403656 + 2.0 * 6.05918025970459
Epoch 590, val loss: 0.7913374900817871
Epoch 600, training loss: 12.524694442749023 = 0.40767765045166016 + 2.0 * 6.058508396148682
Epoch 600, val loss: 0.7864946722984314
Epoch 610, training loss: 12.51367473602295 = 0.3929893672466278 + 2.0 * 6.060342788696289
Epoch 610, val loss: 0.7821393013000488
Epoch 620, training loss: 12.487539291381836 = 0.37868404388427734 + 2.0 * 6.054427623748779
Epoch 620, val loss: 0.7782416939735413
Epoch 630, training loss: 12.471461296081543 = 0.3647647202014923 + 2.0 * 6.053348064422607
Epoch 630, val loss: 0.774840772151947
Epoch 640, training loss: 12.456467628479004 = 0.35113686323165894 + 2.0 * 6.0526652336120605
Epoch 640, val loss: 0.7717831134796143
Epoch 650, training loss: 12.445892333984375 = 0.33787280321121216 + 2.0 * 6.054009914398193
Epoch 650, val loss: 0.7689803242683411
Epoch 660, training loss: 12.426859855651855 = 0.3251488506793976 + 2.0 * 6.05085563659668
Epoch 660, val loss: 0.7666643857955933
Epoch 670, training loss: 12.411581993103027 = 0.31278175115585327 + 2.0 * 6.049400329589844
Epoch 670, val loss: 0.7647138833999634
Epoch 680, training loss: 12.405001640319824 = 0.30070263147354126 + 2.0 * 6.052149295806885
Epoch 680, val loss: 0.763069748878479
Epoch 690, training loss: 12.392163276672363 = 0.2890683710575104 + 2.0 * 6.051547527313232
Epoch 690, val loss: 0.761734127998352
Epoch 700, training loss: 12.369695663452148 = 0.27774709463119507 + 2.0 * 6.045974254608154
Epoch 700, val loss: 0.7607212662696838
Epoch 710, training loss: 12.357810020446777 = 0.2667054533958435 + 2.0 * 6.0455522537231445
Epoch 710, val loss: 0.7599274516105652
Epoch 720, training loss: 12.3512544631958 = 0.2559163272380829 + 2.0 * 6.047668933868408
Epoch 720, val loss: 0.7593196034431458
Epoch 730, training loss: 12.337411880493164 = 0.24540789425373077 + 2.0 * 6.04600191116333
Epoch 730, val loss: 0.7590376734733582
Epoch 740, training loss: 12.321398735046387 = 0.2351854145526886 + 2.0 * 6.043106555938721
Epoch 740, val loss: 0.7588980793952942
Epoch 750, training loss: 12.315287590026855 = 0.22523653507232666 + 2.0 * 6.04502534866333
Epoch 750, val loss: 0.7589156031608582
Epoch 760, training loss: 12.299788475036621 = 0.2155541181564331 + 2.0 * 6.042117118835449
Epoch 760, val loss: 0.7591212391853333
Epoch 770, training loss: 12.289504051208496 = 0.20622044801712036 + 2.0 * 6.041641712188721
Epoch 770, val loss: 0.7595043182373047
Epoch 780, training loss: 12.279815673828125 = 0.1971966177225113 + 2.0 * 6.041309356689453
Epoch 780, val loss: 0.7600859999656677
Epoch 790, training loss: 12.2737398147583 = 0.18851076066493988 + 2.0 * 6.042614459991455
Epoch 790, val loss: 0.7609047889709473
Epoch 800, training loss: 12.260954856872559 = 0.18023434281349182 + 2.0 * 6.040360450744629
Epoch 800, val loss: 0.761915922164917
Epoch 810, training loss: 12.246193885803223 = 0.17231689393520355 + 2.0 * 6.036938667297363
Epoch 810, val loss: 0.7632321715354919
Epoch 820, training loss: 12.249833106994629 = 0.16473960876464844 + 2.0 * 6.04254674911499
Epoch 820, val loss: 0.7646588087081909
Epoch 830, training loss: 12.235913276672363 = 0.15756838023662567 + 2.0 * 6.039172649383545
Epoch 830, val loss: 0.7662311792373657
Epoch 840, training loss: 12.225478172302246 = 0.15074363350868225 + 2.0 * 6.037367343902588
Epoch 840, val loss: 0.7680966258049011
Epoch 850, training loss: 12.21566104888916 = 0.14428065717220306 + 2.0 * 6.0356903076171875
Epoch 850, val loss: 0.7701473832130432
Epoch 860, training loss: 12.206399917602539 = 0.13811755180358887 + 2.0 * 6.0341410636901855
Epoch 860, val loss: 0.7723798751831055
Epoch 870, training loss: 12.201664924621582 = 0.13225124776363373 + 2.0 * 6.034707069396973
Epoch 870, val loss: 0.7748023271560669
Epoch 880, training loss: 12.194619178771973 = 0.12669070065021515 + 2.0 * 6.033964157104492
Epoch 880, val loss: 0.7773654460906982
Epoch 890, training loss: 12.187582969665527 = 0.12143071740865707 + 2.0 * 6.033076286315918
Epoch 890, val loss: 0.7800713181495667
Epoch 900, training loss: 12.179426193237305 = 0.11644864082336426 + 2.0 * 6.03148889541626
Epoch 900, val loss: 0.7830098867416382
Epoch 910, training loss: 12.175101280212402 = 0.11168964952230453 + 2.0 * 6.031705856323242
Epoch 910, val loss: 0.7860312461853027
Epoch 920, training loss: 12.171286582946777 = 0.10718362033367157 + 2.0 * 6.0320515632629395
Epoch 920, val loss: 0.7891669869422913
Epoch 930, training loss: 12.166003227233887 = 0.10291825979948044 + 2.0 * 6.0315423011779785
Epoch 930, val loss: 0.792469322681427
Epoch 940, training loss: 12.160333633422852 = 0.09887860715389252 + 2.0 * 6.030727386474609
Epoch 940, val loss: 0.7957760095596313
Epoch 950, training loss: 12.151654243469238 = 0.09506729990243912 + 2.0 * 6.028293609619141
Epoch 950, val loss: 0.7992511987686157
Epoch 960, training loss: 12.146072387695312 = 0.09143543988466263 + 2.0 * 6.027318477630615
Epoch 960, val loss: 0.8028687834739685
Epoch 970, training loss: 12.141134262084961 = 0.0879596471786499 + 2.0 * 6.02658748626709
Epoch 970, val loss: 0.8065336346626282
Epoch 980, training loss: 12.153300285339355 = 0.08466050773859024 + 2.0 * 6.034319877624512
Epoch 980, val loss: 0.8103255033493042
Epoch 990, training loss: 12.141398429870605 = 0.0815216526389122 + 2.0 * 6.029938220977783
Epoch 990, val loss: 0.814051628112793
Epoch 1000, training loss: 12.13148307800293 = 0.07857145369052887 + 2.0 * 6.026455879211426
Epoch 1000, val loss: 0.8178858160972595
Epoch 1010, training loss: 12.126082420349121 = 0.07576677948236465 + 2.0 * 6.025157928466797
Epoch 1010, val loss: 0.8217630982398987
Epoch 1020, training loss: 12.123727798461914 = 0.07307998836040497 + 2.0 * 6.025323867797852
Epoch 1020, val loss: 0.8256614208221436
Epoch 1030, training loss: 12.122387886047363 = 0.0705157220363617 + 2.0 * 6.025936126708984
Epoch 1030, val loss: 0.8295062184333801
Epoch 1040, training loss: 12.117032051086426 = 0.06809262931346893 + 2.0 * 6.02446985244751
Epoch 1040, val loss: 0.8334524035453796
Epoch 1050, training loss: 12.111205101013184 = 0.06578579545021057 + 2.0 * 6.022709846496582
Epoch 1050, val loss: 0.8374112248420715
Epoch 1060, training loss: 12.108407020568848 = 0.06357952207326889 + 2.0 * 6.022413730621338
Epoch 1060, val loss: 0.8414393663406372
Epoch 1070, training loss: 12.113368034362793 = 0.06146889179944992 + 2.0 * 6.025949478149414
Epoch 1070, val loss: 0.8454452753067017
Epoch 1080, training loss: 12.110611915588379 = 0.059459421783685684 + 2.0 * 6.025576114654541
Epoch 1080, val loss: 0.8493836522102356
Epoch 1090, training loss: 12.102126121520996 = 0.05756360664963722 + 2.0 * 6.022281169891357
Epoch 1090, val loss: 0.8534356951713562
Epoch 1100, training loss: 12.09662914276123 = 0.05574604868888855 + 2.0 * 6.02044153213501
Epoch 1100, val loss: 0.8574643135070801
Epoch 1110, training loss: 12.092935562133789 = 0.053997792303562164 + 2.0 * 6.019468784332275
Epoch 1110, val loss: 0.8614675998687744
Epoch 1120, training loss: 12.10096549987793 = 0.05231773108243942 + 2.0 * 6.0243239402771
Epoch 1120, val loss: 0.8654723763465881
Epoch 1130, training loss: 12.093331336975098 = 0.05071992799639702 + 2.0 * 6.021305561065674
Epoch 1130, val loss: 0.8693798184394836
Epoch 1140, training loss: 12.088358879089355 = 0.049194395542144775 + 2.0 * 6.019582271575928
Epoch 1140, val loss: 0.8734487891197205
Epoch 1150, training loss: 12.09301471710205 = 0.04773751646280289 + 2.0 * 6.02263879776001
Epoch 1150, val loss: 0.8773320913314819
Epoch 1160, training loss: 12.084318161010742 = 0.04633774608373642 + 2.0 * 6.0189900398254395
Epoch 1160, val loss: 0.8811887502670288
Epoch 1170, training loss: 12.078802108764648 = 0.04500029981136322 + 2.0 * 6.016901016235352
Epoch 1170, val loss: 0.8852152228355408
Epoch 1180, training loss: 12.078432083129883 = 0.04371137171983719 + 2.0 * 6.017360210418701
Epoch 1180, val loss: 0.8891318440437317
Epoch 1190, training loss: 12.081620216369629 = 0.04247305914759636 + 2.0 * 6.01957368850708
Epoch 1190, val loss: 0.8929818272590637
Epoch 1200, training loss: 12.074950218200684 = 0.04128645360469818 + 2.0 * 6.016831874847412
Epoch 1200, val loss: 0.8968708515167236
Epoch 1210, training loss: 12.076949119567871 = 0.0401507206261158 + 2.0 * 6.018399238586426
Epoch 1210, val loss: 0.9007617235183716
Epoch 1220, training loss: 12.071314811706543 = 0.039054468274116516 + 2.0 * 6.016129970550537
Epoch 1220, val loss: 0.9044829607009888
Epoch 1230, training loss: 12.066810607910156 = 0.038006491959095 + 2.0 * 6.014401912689209
Epoch 1230, val loss: 0.9083617925643921
Epoch 1240, training loss: 12.064205169677734 = 0.036993008106946945 + 2.0 * 6.013606071472168
Epoch 1240, val loss: 0.912199854850769
Epoch 1250, training loss: 12.082079887390137 = 0.03601901978254318 + 2.0 * 6.0230302810668945
Epoch 1250, val loss: 0.9160072803497314
Epoch 1260, training loss: 12.067928314208984 = 0.035083819180727005 + 2.0 * 6.016422271728516
Epoch 1260, val loss: 0.9195494055747986
Epoch 1270, training loss: 12.06136703491211 = 0.03419514745473862 + 2.0 * 6.013586044311523
Epoch 1270, val loss: 0.9234086275100708
Epoch 1280, training loss: 12.058195114135742 = 0.03333338722586632 + 2.0 * 6.012430667877197
Epoch 1280, val loss: 0.9270887970924377
Epoch 1290, training loss: 12.055827140808105 = 0.03249497711658478 + 2.0 * 6.011666297912598
Epoch 1290, val loss: 0.9308081865310669
Epoch 1300, training loss: 12.058159828186035 = 0.031683649867773056 + 2.0 * 6.013237953186035
Epoch 1300, val loss: 0.9345763325691223
Epoch 1310, training loss: 12.056368827819824 = 0.030903585255146027 + 2.0 * 6.01273250579834
Epoch 1310, val loss: 0.9381705522537231
Epoch 1320, training loss: 12.05827808380127 = 0.030155478045344353 + 2.0 * 6.014061450958252
Epoch 1320, val loss: 0.9418260455131531
Epoch 1330, training loss: 12.056183815002441 = 0.029438789933919907 + 2.0 * 6.013372421264648
Epoch 1330, val loss: 0.9453617334365845
Epoch 1340, training loss: 12.052624702453613 = 0.02874605543911457 + 2.0 * 6.011939525604248
Epoch 1340, val loss: 0.9489709734916687
Epoch 1350, training loss: 12.050565719604492 = 0.028075609356164932 + 2.0 * 6.011245250701904
Epoch 1350, val loss: 0.9525097608566284
Epoch 1360, training loss: 12.047658920288086 = 0.027427872642874718 + 2.0 * 6.010115623474121
Epoch 1360, val loss: 0.9561241865158081
Epoch 1370, training loss: 12.054365158081055 = 0.02680063247680664 + 2.0 * 6.013782501220703
Epoch 1370, val loss: 0.9595949649810791
Epoch 1380, training loss: 12.048256874084473 = 0.026190413162112236 + 2.0 * 6.011033058166504
Epoch 1380, val loss: 0.9630082845687866
Epoch 1390, training loss: 12.04518985748291 = 0.02560690976679325 + 2.0 * 6.009791374206543
Epoch 1390, val loss: 0.9665647149085999
Epoch 1400, training loss: 12.048713684082031 = 0.025036942213773727 + 2.0 * 6.011838436126709
Epoch 1400, val loss: 0.969989001750946
Epoch 1410, training loss: 12.043423652648926 = 0.02448767051100731 + 2.0 * 6.009468078613281
Epoch 1410, val loss: 0.9733384847640991
Epoch 1420, training loss: 12.044746398925781 = 0.02395522966980934 + 2.0 * 6.010395526885986
Epoch 1420, val loss: 0.9767599701881409
Epoch 1430, training loss: 12.039131164550781 = 0.023445257917046547 + 2.0 * 6.007843017578125
Epoch 1430, val loss: 0.9802514910697937
Epoch 1440, training loss: 12.038187980651855 = 0.02294531650841236 + 2.0 * 6.0076212882995605
Epoch 1440, val loss: 0.9836419224739075
Epoch 1450, training loss: 12.048051834106445 = 0.022460874170064926 + 2.0 * 6.012795448303223
Epoch 1450, val loss: 0.9868799448013306
Epoch 1460, training loss: 12.036568641662598 = 0.02199484221637249 + 2.0 * 6.00728702545166
Epoch 1460, val loss: 0.9901235103607178
Epoch 1470, training loss: 12.033594131469727 = 0.02154463715851307 + 2.0 * 6.0060248374938965
Epoch 1470, val loss: 0.9935030341148376
Epoch 1480, training loss: 12.032094955444336 = 0.02110503986477852 + 2.0 * 6.005495071411133
Epoch 1480, val loss: 0.9967743754386902
Epoch 1490, training loss: 12.041988372802734 = 0.020676597952842712 + 2.0 * 6.010655879974365
Epoch 1490, val loss: 1.0000497102737427
Epoch 1500, training loss: 12.036678314208984 = 0.02026090957224369 + 2.0 * 6.008208751678467
Epoch 1500, val loss: 1.0031408071517944
Epoch 1510, training loss: 12.032818794250488 = 0.019864019006490707 + 2.0 * 6.006477355957031
Epoch 1510, val loss: 1.0064449310302734
Epoch 1520, training loss: 12.035462379455566 = 0.01947484165430069 + 2.0 * 6.007993698120117
Epoch 1520, val loss: 1.009610891342163
Epoch 1530, training loss: 12.028193473815918 = 0.019098596647381783 + 2.0 * 6.004547595977783
Epoch 1530, val loss: 1.0126667022705078
Epoch 1540, training loss: 12.028017044067383 = 0.018733300268650055 + 2.0 * 6.004642009735107
Epoch 1540, val loss: 1.0159162282943726
Epoch 1550, training loss: 12.033644676208496 = 0.018375860527157784 + 2.0 * 6.00763463973999
Epoch 1550, val loss: 1.019063115119934
Epoch 1560, training loss: 12.028068542480469 = 0.01802862249314785 + 2.0 * 6.0050201416015625
Epoch 1560, val loss: 1.0220141410827637
Epoch 1570, training loss: 12.028472900390625 = 0.01769544929265976 + 2.0 * 6.0053887367248535
Epoch 1570, val loss: 1.0251796245574951
Epoch 1580, training loss: 12.02740478515625 = 0.01737016811966896 + 2.0 * 6.005017280578613
Epoch 1580, val loss: 1.0281890630722046
Epoch 1590, training loss: 12.02269458770752 = 0.017054123803973198 + 2.0 * 6.002820014953613
Epoch 1590, val loss: 1.0312772989273071
Epoch 1600, training loss: 12.02358627319336 = 0.016743725165724754 + 2.0 * 6.003421306610107
Epoch 1600, val loss: 1.0343677997589111
Epoch 1610, training loss: 12.027827262878418 = 0.01644224300980568 + 2.0 * 6.005692481994629
Epoch 1610, val loss: 1.0373196601867676
Epoch 1620, training loss: 12.02365779876709 = 0.016149237751960754 + 2.0 * 6.003754138946533
Epoch 1620, val loss: 1.040395736694336
Epoch 1630, training loss: 12.032445907592773 = 0.015866238623857498 + 2.0 * 6.008289813995361
Epoch 1630, val loss: 1.0433903932571411
Epoch 1640, training loss: 12.026237487792969 = 0.015591507777571678 + 2.0 * 6.0053229331970215
Epoch 1640, val loss: 1.0462132692337036
Epoch 1650, training loss: 12.019918441772461 = 0.015323800034821033 + 2.0 * 6.002297401428223
Epoch 1650, val loss: 1.049148678779602
Epoch 1660, training loss: 12.016680717468262 = 0.015062873251736164 + 2.0 * 6.0008087158203125
Epoch 1660, val loss: 1.0521085262298584
Epoch 1670, training loss: 12.018119812011719 = 0.014805753715336323 + 2.0 * 6.001657009124756
Epoch 1670, val loss: 1.0549887418746948
Epoch 1680, training loss: 12.02097225189209 = 0.014555329456925392 + 2.0 * 6.003208637237549
Epoch 1680, val loss: 1.0578316450119019
Epoch 1690, training loss: 12.020743370056152 = 0.014314640313386917 + 2.0 * 6.003214359283447
Epoch 1690, val loss: 1.0607013702392578
Epoch 1700, training loss: 12.01996898651123 = 0.01407826691865921 + 2.0 * 6.002945423126221
Epoch 1700, val loss: 1.0635865926742554
Epoch 1710, training loss: 12.018087387084961 = 0.01384973619133234 + 2.0 * 6.0021185874938965
Epoch 1710, val loss: 1.066296100616455
Epoch 1720, training loss: 12.013681411743164 = 0.01362593099474907 + 2.0 * 6.000027656555176
Epoch 1720, val loss: 1.0690919160842896
Epoch 1730, training loss: 12.011992454528809 = 0.01340785063803196 + 2.0 * 5.999292373657227
Epoch 1730, val loss: 1.0719281435012817
Epoch 1740, training loss: 12.023748397827148 = 0.013192414306104183 + 2.0 * 6.00527811050415
Epoch 1740, val loss: 1.0746484994888306
Epoch 1750, training loss: 12.016263961791992 = 0.01298477966338396 + 2.0 * 6.001639366149902
Epoch 1750, val loss: 1.0774086713790894
Epoch 1760, training loss: 12.01333236694336 = 0.012782646343111992 + 2.0 * 6.000274658203125
Epoch 1760, val loss: 1.080216646194458
Epoch 1770, training loss: 12.0109224319458 = 0.012585361488163471 + 2.0 * 5.999168395996094
Epoch 1770, val loss: 1.0829510688781738
Epoch 1780, training loss: 12.013344764709473 = 0.012390690855681896 + 2.0 * 6.000476837158203
Epoch 1780, val loss: 1.0856280326843262
Epoch 1790, training loss: 12.011770248413086 = 0.012201068922877312 + 2.0 * 5.999784469604492
Epoch 1790, val loss: 1.0882714986801147
Epoch 1800, training loss: 12.01125431060791 = 0.012015041895210743 + 2.0 * 5.999619483947754
Epoch 1800, val loss: 1.0909342765808105
Epoch 1810, training loss: 12.010871887207031 = 0.01183500699698925 + 2.0 * 5.999518394470215
Epoch 1810, val loss: 1.0936800241470337
Epoch 1820, training loss: 12.009148597717285 = 0.011658030562102795 + 2.0 * 5.998745441436768
Epoch 1820, val loss: 1.0962352752685547
Epoch 1830, training loss: 12.01158332824707 = 0.011485784314572811 + 2.0 * 6.000048637390137
Epoch 1830, val loss: 1.0988190174102783
Epoch 1840, training loss: 12.004488945007324 = 0.011318731121718884 + 2.0 * 5.996584892272949
Epoch 1840, val loss: 1.101487159729004
Epoch 1850, training loss: 12.005036354064941 = 0.011155417189002037 + 2.0 * 5.996940612792969
Epoch 1850, val loss: 1.1041815280914307
Epoch 1860, training loss: 12.01252269744873 = 0.010993280448019505 + 2.0 * 6.000764846801758
Epoch 1860, val loss: 1.1067537069320679
Epoch 1870, training loss: 12.004323959350586 = 0.010834784246981144 + 2.0 * 5.996744632720947
Epoch 1870, val loss: 1.109240174293518
Epoch 1880, training loss: 12.004417419433594 = 0.010681177489459515 + 2.0 * 5.996868133544922
Epoch 1880, val loss: 1.1118853092193604
Epoch 1890, training loss: 12.011799812316895 = 0.010531238280236721 + 2.0 * 6.00063419342041
Epoch 1890, val loss: 1.1142897605895996
Epoch 1900, training loss: 12.003976821899414 = 0.010385705158114433 + 2.0 * 5.996795654296875
Epoch 1900, val loss: 1.11684250831604
Epoch 1910, training loss: 12.000397682189941 = 0.010242640972137451 + 2.0 * 5.995077610015869
Epoch 1910, val loss: 1.1194801330566406
Epoch 1920, training loss: 12.001285552978516 = 0.01010124385356903 + 2.0 * 5.99559211730957
Epoch 1920, val loss: 1.1220042705535889
Epoch 1930, training loss: 12.00954532623291 = 0.00996203813701868 + 2.0 * 5.999791622161865
Epoch 1930, val loss: 1.1244187355041504
Epoch 1940, training loss: 12.002483367919922 = 0.009826403111219406 + 2.0 * 5.996328353881836
Epoch 1940, val loss: 1.126889944076538
Epoch 1950, training loss: 12.004669189453125 = 0.00969482772052288 + 2.0 * 5.9974870681762695
Epoch 1950, val loss: 1.12943696975708
Epoch 1960, training loss: 11.999146461486816 = 0.009566417895257473 + 2.0 * 5.994790077209473
Epoch 1960, val loss: 1.1317546367645264
Epoch 1970, training loss: 12.011067390441895 = 0.009441710077226162 + 2.0 * 6.000813007354736
Epoch 1970, val loss: 1.1342158317565918
Epoch 1980, training loss: 11.99794864654541 = 0.009318316355347633 + 2.0 * 5.994315147399902
Epoch 1980, val loss: 1.136521577835083
Epoch 1990, training loss: 11.995272636413574 = 0.009198588319122791 + 2.0 * 5.993037223815918
Epoch 1990, val loss: 1.1390478610992432
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8111
Overall ASR: 0.5904
Flip ASR: 0.5333/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.691654205322266 = 1.9439529180526733 + 2.0 * 8.37385082244873
Epoch 0, val loss: 1.9446076154708862
Epoch 10, training loss: 18.67933464050293 = 1.9333245754241943 + 2.0 * 8.373004913330078
Epoch 10, val loss: 1.9343105554580688
Epoch 20, training loss: 18.656015396118164 = 1.9201991558074951 + 2.0 * 8.367908477783203
Epoch 20, val loss: 1.9210214614868164
Epoch 30, training loss: 18.586572647094727 = 1.9026070833206177 + 2.0 * 8.3419828414917
Epoch 30, val loss: 1.9028396606445312
Epoch 40, training loss: 18.237937927246094 = 1.8810876607894897 + 2.0 * 8.178424835205078
Epoch 40, val loss: 1.8810291290283203
Epoch 50, training loss: 16.91365623474121 = 1.8584768772125244 + 2.0 * 7.527589797973633
Epoch 50, val loss: 1.8579719066619873
Epoch 60, training loss: 16.005992889404297 = 1.8403041362762451 + 2.0 * 7.082844257354736
Epoch 60, val loss: 1.8394197225570679
Epoch 70, training loss: 15.459543228149414 = 1.826130747795105 + 2.0 * 6.81670618057251
Epoch 70, val loss: 1.8245946168899536
Epoch 80, training loss: 15.166910171508789 = 1.8153859376907349 + 2.0 * 6.675762176513672
Epoch 80, val loss: 1.813141942024231
Epoch 90, training loss: 14.943085670471191 = 1.8057457208633423 + 2.0 * 6.56866979598999
Epoch 90, val loss: 1.8025010824203491
Epoch 100, training loss: 14.774616241455078 = 1.7968740463256836 + 2.0 * 6.488871097564697
Epoch 100, val loss: 1.7927124500274658
Epoch 110, training loss: 14.633584976196289 = 1.7890313863754272 + 2.0 * 6.422276973724365
Epoch 110, val loss: 1.783956527709961
Epoch 120, training loss: 14.538450241088867 = 1.7812732458114624 + 2.0 * 6.378588676452637
Epoch 120, val loss: 1.775170087814331
Epoch 130, training loss: 14.450163841247559 = 1.7733756303787231 + 2.0 * 6.3383941650390625
Epoch 130, val loss: 1.7663756608963013
Epoch 140, training loss: 14.37517261505127 = 1.765854001045227 + 2.0 * 6.304659366607666
Epoch 140, val loss: 1.758362889289856
Epoch 150, training loss: 14.31328010559082 = 1.7581546306610107 + 2.0 * 6.277562618255615
Epoch 150, val loss: 1.750537395477295
Epoch 160, training loss: 14.268259048461914 = 1.7497642040252686 + 2.0 * 6.259247303009033
Epoch 160, val loss: 1.7424356937408447
Epoch 170, training loss: 14.215055465698242 = 1.7405476570129395 + 2.0 * 6.237253665924072
Epoch 170, val loss: 1.7341351509094238
Epoch 180, training loss: 14.170702934265137 = 1.7306203842163086 + 2.0 * 6.220041275024414
Epoch 180, val loss: 1.7254828214645386
Epoch 190, training loss: 14.131096839904785 = 1.7196232080459595 + 2.0 * 6.2057366371154785
Epoch 190, val loss: 1.7161673307418823
Epoch 200, training loss: 14.093620300292969 = 1.7070392370224 + 2.0 * 6.193290710449219
Epoch 200, val loss: 1.7058417797088623
Epoch 210, training loss: 14.057251930236816 = 1.6924582719802856 + 2.0 * 6.18239688873291
Epoch 210, val loss: 1.694070816040039
Epoch 220, training loss: 14.032613754272461 = 1.6753567457199097 + 2.0 * 6.178628444671631
Epoch 220, val loss: 1.6805723905563354
Epoch 230, training loss: 13.993509292602539 = 1.655904769897461 + 2.0 * 6.168802261352539
Epoch 230, val loss: 1.6651010513305664
Epoch 240, training loss: 13.950126647949219 = 1.6337437629699707 + 2.0 * 6.158191204071045
Epoch 240, val loss: 1.647678256034851
Epoch 250, training loss: 13.911596298217773 = 1.6081912517547607 + 2.0 * 6.151702404022217
Epoch 250, val loss: 1.6275827884674072
Epoch 260, training loss: 13.869275093078613 = 1.578620195388794 + 2.0 * 6.145327568054199
Epoch 260, val loss: 1.6042824983596802
Epoch 270, training loss: 13.82356071472168 = 1.5444754362106323 + 2.0 * 6.139542579650879
Epoch 270, val loss: 1.57733154296875
Epoch 280, training loss: 13.773909568786621 = 1.5053284168243408 + 2.0 * 6.13429069519043
Epoch 280, val loss: 1.5466713905334473
Epoch 290, training loss: 13.731117248535156 = 1.462464451789856 + 2.0 * 6.134326457977295
Epoch 290, val loss: 1.5129982233047485
Epoch 300, training loss: 13.670368194580078 = 1.417360782623291 + 2.0 * 6.126503944396973
Epoch 300, val loss: 1.47817063331604
Epoch 310, training loss: 13.615331649780273 = 1.3709728717803955 + 2.0 * 6.1221795082092285
Epoch 310, val loss: 1.4425609111785889
Epoch 320, training loss: 13.559427261352539 = 1.3239344358444214 + 2.0 * 6.117746353149414
Epoch 320, val loss: 1.4069712162017822
Epoch 330, training loss: 13.504850387573242 = 1.2769465446472168 + 2.0 * 6.113952159881592
Epoch 330, val loss: 1.3719801902770996
Epoch 340, training loss: 13.451269149780273 = 1.2305903434753418 + 2.0 * 6.110339164733887
Epoch 340, val loss: 1.337918758392334
Epoch 350, training loss: 13.415112495422363 = 1.1851179599761963 + 2.0 * 6.114997386932373
Epoch 350, val loss: 1.3051563501358032
Epoch 360, training loss: 13.351959228515625 = 1.1416597366333008 + 2.0 * 6.105149745941162
Epoch 360, val loss: 1.2740503549575806
Epoch 370, training loss: 13.302452087402344 = 1.0993143320083618 + 2.0 * 6.101568698883057
Epoch 370, val loss: 1.2444360256195068
Epoch 380, training loss: 13.255005836486816 = 1.0576213598251343 + 2.0 * 6.098692417144775
Epoch 380, val loss: 1.2153457403182983
Epoch 390, training loss: 13.209732055664062 = 1.0165976285934448 + 2.0 * 6.096567153930664
Epoch 390, val loss: 1.1864111423492432
Epoch 400, training loss: 13.163392066955566 = 0.9765755534172058 + 2.0 * 6.093408107757568
Epoch 400, val loss: 1.1585392951965332
Epoch 410, training loss: 13.11925983428955 = 0.9373823404312134 + 2.0 * 6.090938568115234
Epoch 410, val loss: 1.1310689449310303
Epoch 420, training loss: 13.0777006149292 = 0.8988242745399475 + 2.0 * 6.089437961578369
Epoch 420, val loss: 1.1040141582489014
Epoch 430, training loss: 13.04007339477539 = 0.8616095185279846 + 2.0 * 6.089231967926025
Epoch 430, val loss: 1.0776441097259521
Epoch 440, training loss: 12.99579906463623 = 0.8257664442062378 + 2.0 * 6.085016250610352
Epoch 440, val loss: 1.0526922941207886
Epoch 450, training loss: 12.959076881408691 = 0.7911097407341003 + 2.0 * 6.083983421325684
Epoch 450, val loss: 1.028598666191101
Epoch 460, training loss: 12.927818298339844 = 0.758171796798706 + 2.0 * 6.084823131561279
Epoch 460, val loss: 1.0053552389144897
Epoch 470, training loss: 12.885802268981934 = 0.7265493869781494 + 2.0 * 6.079626560211182
Epoch 470, val loss: 0.9838570952415466
Epoch 480, training loss: 12.849590301513672 = 0.696111798286438 + 2.0 * 6.076739311218262
Epoch 480, val loss: 0.9632760286331177
Epoch 490, training loss: 12.816641807556152 = 0.6665939688682556 + 2.0 * 6.075024127960205
Epoch 490, val loss: 0.9436593651771545
Epoch 500, training loss: 12.786703109741211 = 0.6378998756408691 + 2.0 * 6.07440185546875
Epoch 500, val loss: 0.9250248670578003
Epoch 510, training loss: 12.77011489868164 = 0.6101925373077393 + 2.0 * 6.07996129989624
Epoch 510, val loss: 0.9074016213417053
Epoch 520, training loss: 12.726085662841797 = 0.5838074684143066 + 2.0 * 6.071138858795166
Epoch 520, val loss: 0.8912215232849121
Epoch 530, training loss: 12.696435928344727 = 0.5582047700881958 + 2.0 * 6.06911563873291
Epoch 530, val loss: 0.8759576082229614
Epoch 540, training loss: 12.669190406799316 = 0.5332412719726562 + 2.0 * 6.06797456741333
Epoch 540, val loss: 0.8615125417709351
Epoch 550, training loss: 12.64789867401123 = 0.5089819431304932 + 2.0 * 6.069458484649658
Epoch 550, val loss: 0.8480849862098694
Epoch 560, training loss: 12.6217041015625 = 0.4857707619667053 + 2.0 * 6.067966461181641
Epoch 560, val loss: 0.8358204364776611
Epoch 570, training loss: 12.59362506866455 = 0.4635641276836395 + 2.0 * 6.065030574798584
Epoch 570, val loss: 0.8247721791267395
Epoch 580, training loss: 12.5673246383667 = 0.442190021276474 + 2.0 * 6.062567234039307
Epoch 580, val loss: 0.8145701885223389
Epoch 590, training loss: 12.543292045593262 = 0.42158567905426025 + 2.0 * 6.060853004455566
Epoch 590, val loss: 0.8052507638931274
Epoch 600, training loss: 12.54202651977539 = 0.40183648467063904 + 2.0 * 6.070095062255859
Epoch 600, val loss: 0.7967785596847534
Epoch 610, training loss: 12.503152847290039 = 0.38297635316848755 + 2.0 * 6.060088157653809
Epoch 610, val loss: 0.7893117666244507
Epoch 620, training loss: 12.480920791625977 = 0.3651290535926819 + 2.0 * 6.057895660400391
Epoch 620, val loss: 0.7827789187431335
Epoch 630, training loss: 12.46062183380127 = 0.3480246663093567 + 2.0 * 6.056298732757568
Epoch 630, val loss: 0.7768407464027405
Epoch 640, training loss: 12.445235252380371 = 0.3316008746623993 + 2.0 * 6.056817054748535
Epoch 640, val loss: 0.7716275453567505
Epoch 650, training loss: 12.433968544006348 = 0.3159540891647339 + 2.0 * 6.059007167816162
Epoch 650, val loss: 0.7669201493263245
Epoch 660, training loss: 12.407527923583984 = 0.3010345995426178 + 2.0 * 6.05324649810791
Epoch 660, val loss: 0.7629580497741699
Epoch 670, training loss: 12.391505241394043 = 0.2867228090763092 + 2.0 * 6.052391052246094
Epoch 670, val loss: 0.7594651579856873
Epoch 680, training loss: 12.375473022460938 = 0.2729214131832123 + 2.0 * 6.051275730133057
Epoch 680, val loss: 0.7563402652740479
Epoch 690, training loss: 12.365907669067383 = 0.2595664858818054 + 2.0 * 6.053170680999756
Epoch 690, val loss: 0.7535896301269531
Epoch 700, training loss: 12.362889289855957 = 0.24672016501426697 + 2.0 * 6.058084487915039
Epoch 700, val loss: 0.7509927749633789
Epoch 710, training loss: 12.337567329406738 = 0.2345396876335144 + 2.0 * 6.051513671875
Epoch 710, val loss: 0.7489879727363586
Epoch 720, training loss: 12.319497108459473 = 0.2228270024061203 + 2.0 * 6.048335075378418
Epoch 720, val loss: 0.7473428845405579
Epoch 730, training loss: 12.304831504821777 = 0.21149210631847382 + 2.0 * 6.0466694831848145
Epoch 730, val loss: 0.7459538578987122
Epoch 740, training loss: 12.291399955749512 = 0.2005750983953476 + 2.0 * 6.045412540435791
Epoch 740, val loss: 0.7449437975883484
Epoch 750, training loss: 12.300479888916016 = 0.19009801745414734 + 2.0 * 6.0551910400390625
Epoch 750, val loss: 0.7442430257797241
Epoch 760, training loss: 12.268037796020508 = 0.1801963895559311 + 2.0 * 6.043920516967773
Epoch 760, val loss: 0.7438573837280273
Epoch 770, training loss: 12.25618839263916 = 0.17081408202648163 + 2.0 * 6.042686939239502
Epoch 770, val loss: 0.7439835071563721
Epoch 780, training loss: 12.249272346496582 = 0.1618945300579071 + 2.0 * 6.043688774108887
Epoch 780, val loss: 0.7444107532501221
Epoch 790, training loss: 12.248196601867676 = 0.1535617560148239 + 2.0 * 6.0473175048828125
Epoch 790, val loss: 0.7447367906570435
Epoch 800, training loss: 12.229267120361328 = 0.14577265083789825 + 2.0 * 6.041747093200684
Epoch 800, val loss: 0.7458403706550598
Epoch 810, training loss: 12.219242095947266 = 0.13844746351242065 + 2.0 * 6.0403971672058105
Epoch 810, val loss: 0.7473207712173462
Epoch 820, training loss: 12.209135055541992 = 0.13153299689292908 + 2.0 * 6.038801193237305
Epoch 820, val loss: 0.7490202784538269
Epoch 830, training loss: 12.200543403625488 = 0.12501820921897888 + 2.0 * 6.037762641906738
Epoch 830, val loss: 0.7510756850242615
Epoch 840, training loss: 12.204717636108398 = 0.11888068169355392 + 2.0 * 6.042918682098389
Epoch 840, val loss: 0.7535073161125183
Epoch 850, training loss: 12.193649291992188 = 0.11318458616733551 + 2.0 * 6.040232181549072
Epoch 850, val loss: 0.7559218406677246
Epoch 860, training loss: 12.18019962310791 = 0.1078619509935379 + 2.0 * 6.036169052124023
Epoch 860, val loss: 0.7587763667106628
Epoch 870, training loss: 12.173202514648438 = 0.10285299271345139 + 2.0 * 6.03517484664917
Epoch 870, val loss: 0.7618362307548523
Epoch 880, training loss: 12.187187194824219 = 0.09816086292266846 + 2.0 * 6.04451322555542
Epoch 880, val loss: 0.7650013566017151
Epoch 890, training loss: 12.161056518554688 = 0.09376027435064316 + 2.0 * 6.0336480140686035
Epoch 890, val loss: 0.7682854533195496
Epoch 900, training loss: 12.156684875488281 = 0.08964402228593826 + 2.0 * 6.033520221710205
Epoch 900, val loss: 0.7718795537948608
Epoch 910, training loss: 12.154960632324219 = 0.08576347678899765 + 2.0 * 6.034598350524902
Epoch 910, val loss: 0.775556743144989
Epoch 920, training loss: 12.144559860229492 = 0.08213044703006744 + 2.0 * 6.031214714050293
Epoch 920, val loss: 0.7791415452957153
Epoch 930, training loss: 12.139497756958008 = 0.0787183865904808 + 2.0 * 6.030389785766602
Epoch 930, val loss: 0.7829630374908447
Epoch 940, training loss: 12.136103630065918 = 0.07548634707927704 + 2.0 * 6.030308723449707
Epoch 940, val loss: 0.7868727445602417
Epoch 950, training loss: 12.136185646057129 = 0.07243785262107849 + 2.0 * 6.03187370300293
Epoch 950, val loss: 0.7906589508056641
Epoch 960, training loss: 12.130637168884277 = 0.06959753483533859 + 2.0 * 6.030519962310791
Epoch 960, val loss: 0.794541597366333
Epoch 970, training loss: 12.122488975524902 = 0.06689996272325516 + 2.0 * 6.027794361114502
Epoch 970, val loss: 0.7986283302307129
Epoch 980, training loss: 12.118979454040527 = 0.06433827430009842 + 2.0 * 6.027320384979248
Epoch 980, val loss: 0.8026747703552246
Epoch 990, training loss: 12.132833480834961 = 0.06190095096826553 + 2.0 * 6.035466194152832
Epoch 990, val loss: 0.8067550659179688
Epoch 1000, training loss: 12.113017082214355 = 0.05962536111474037 + 2.0 * 6.026695728302002
Epoch 1000, val loss: 0.8105664253234863
Epoch 1010, training loss: 12.109007835388184 = 0.05746348947286606 + 2.0 * 6.0257720947265625
Epoch 1010, val loss: 0.8146822452545166
Epoch 1020, training loss: 12.104507446289062 = 0.05541215091943741 + 2.0 * 6.024547576904297
Epoch 1020, val loss: 0.8187782168388367
Epoch 1030, training loss: 12.100921630859375 = 0.05345364660024643 + 2.0 * 6.023734092712402
Epoch 1030, val loss: 0.8228433132171631
Epoch 1040, training loss: 12.117737770080566 = 0.051589518785476685 + 2.0 * 6.033073902130127
Epoch 1040, val loss: 0.8268775939941406
Epoch 1050, training loss: 12.104903221130371 = 0.04983407258987427 + 2.0 * 6.027534484863281
Epoch 1050, val loss: 0.8307381868362427
Epoch 1060, training loss: 12.097490310668945 = 0.048194192349910736 + 2.0 * 6.024648189544678
Epoch 1060, val loss: 0.8347845077514648
Epoch 1070, training loss: 12.090519905090332 = 0.046625297516584396 + 2.0 * 6.021947383880615
Epoch 1070, val loss: 0.8388132452964783
Epoch 1080, training loss: 12.08984661102295 = 0.045122187584638596 + 2.0 * 6.022362232208252
Epoch 1080, val loss: 0.8427561521530151
Epoch 1090, training loss: 12.09158992767334 = 0.04369015619158745 + 2.0 * 6.023950099945068
Epoch 1090, val loss: 0.8465510606765747
Epoch 1100, training loss: 12.082459449768066 = 0.04232358559966087 + 2.0 * 6.020068168640137
Epoch 1100, val loss: 0.8504964709281921
Epoch 1110, training loss: 12.079124450683594 = 0.04102518782019615 + 2.0 * 6.019049644470215
Epoch 1110, val loss: 0.8544731736183167
Epoch 1120, training loss: 12.077366828918457 = 0.039771005511283875 + 2.0 * 6.018797874450684
Epoch 1120, val loss: 0.8583906888961792
Epoch 1130, training loss: 12.075175285339355 = 0.038571156561374664 + 2.0 * 6.018301963806152
Epoch 1130, val loss: 0.8622884750366211
Epoch 1140, training loss: 12.092791557312012 = 0.03742240369319916 + 2.0 * 6.027684688568115
Epoch 1140, val loss: 0.8660931587219238
Epoch 1150, training loss: 12.072239875793457 = 0.03633628785610199 + 2.0 * 6.017951965332031
Epoch 1150, val loss: 0.8697682619094849
Epoch 1160, training loss: 12.071507453918457 = 0.03530867397785187 + 2.0 * 6.018099308013916
Epoch 1160, val loss: 0.8734883666038513
Epoch 1170, training loss: 12.06737232208252 = 0.03431645408272743 + 2.0 * 6.016528129577637
Epoch 1170, val loss: 0.8772754073143005
Epoch 1180, training loss: 12.067893981933594 = 0.033357586711645126 + 2.0 * 6.017268180847168
Epoch 1180, val loss: 0.8810144662857056
Epoch 1190, training loss: 12.06576156616211 = 0.03244443237781525 + 2.0 * 6.016658782958984
Epoch 1190, val loss: 0.8843382000923157
Epoch 1200, training loss: 12.067022323608398 = 0.0315820686519146 + 2.0 * 6.0177202224731445
Epoch 1200, val loss: 0.887911319732666
Epoch 1210, training loss: 12.059612274169922 = 0.03075336292386055 + 2.0 * 6.014429569244385
Epoch 1210, val loss: 0.8915115594863892
Epoch 1220, training loss: 12.057734489440918 = 0.029946813359856606 + 2.0 * 6.0138936042785645
Epoch 1220, val loss: 0.8950722813606262
Epoch 1230, training loss: 12.05989933013916 = 0.029164832085371017 + 2.0 * 6.015367031097412
Epoch 1230, val loss: 0.8986373543739319
Epoch 1240, training loss: 12.054967880249023 = 0.028418153524398804 + 2.0 * 6.013274669647217
Epoch 1240, val loss: 0.9019759297370911
Epoch 1250, training loss: 12.056497573852539 = 0.02770482748746872 + 2.0 * 6.0143961906433105
Epoch 1250, val loss: 0.9054244160652161
Epoch 1260, training loss: 12.051424026489258 = 0.027017943561077118 + 2.0 * 6.012203216552734
Epoch 1260, val loss: 0.9089300632476807
Epoch 1270, training loss: 12.05123233795166 = 0.026350701227784157 + 2.0 * 6.0124406814575195
Epoch 1270, val loss: 0.9123913049697876
Epoch 1280, training loss: 12.062214851379395 = 0.02571006491780281 + 2.0 * 6.018252372741699
Epoch 1280, val loss: 0.9156521558761597
Epoch 1290, training loss: 12.049057960510254 = 0.025094185024499893 + 2.0 * 6.011981964111328
Epoch 1290, val loss: 0.9189953804016113
Epoch 1300, training loss: 12.054413795471191 = 0.024506745859980583 + 2.0 * 6.01495361328125
Epoch 1300, val loss: 0.9222962260246277
Epoch 1310, training loss: 12.044717788696289 = 0.023933537304401398 + 2.0 * 6.010392189025879
Epoch 1310, val loss: 0.9255427718162537
Epoch 1320, training loss: 12.044716835021973 = 0.023381046950817108 + 2.0 * 6.01066780090332
Epoch 1320, val loss: 0.9288350343704224
Epoch 1330, training loss: 12.041436195373535 = 0.02284735068678856 + 2.0 * 6.009294509887695
Epoch 1330, val loss: 0.9321280121803284
Epoch 1340, training loss: 12.050069808959961 = 0.022330600768327713 + 2.0 * 6.013869762420654
Epoch 1340, val loss: 0.9353721737861633
Epoch 1350, training loss: 12.049247741699219 = 0.02183167077600956 + 2.0 * 6.013708114624023
Epoch 1350, val loss: 0.9384130239486694
Epoch 1360, training loss: 12.039754867553711 = 0.021359167993068695 + 2.0 * 6.00919771194458
Epoch 1360, val loss: 0.941542387008667
Epoch 1370, training loss: 12.039315223693848 = 0.020900188013911247 + 2.0 * 6.009207725524902
Epoch 1370, val loss: 0.9447073936462402
Epoch 1380, training loss: 12.05488109588623 = 0.02045533061027527 + 2.0 * 6.017212867736816
Epoch 1380, val loss: 0.947668194770813
Epoch 1390, training loss: 12.040884017944336 = 0.020020827651023865 + 2.0 * 6.01043176651001
Epoch 1390, val loss: 0.9507551193237305
Epoch 1400, training loss: 12.035608291625977 = 0.019609013572335243 + 2.0 * 6.007999420166016
Epoch 1400, val loss: 0.953788161277771
Epoch 1410, training loss: 12.033571243286133 = 0.019201409071683884 + 2.0 * 6.007184982299805
Epoch 1410, val loss: 0.9569143652915955
Epoch 1420, training loss: 12.049927711486816 = 0.01880621910095215 + 2.0 * 6.015560626983643
Epoch 1420, val loss: 0.9598710536956787
Epoch 1430, training loss: 12.036582946777344 = 0.018432259559631348 + 2.0 * 6.009075164794922
Epoch 1430, val loss: 0.9627214074134827
Epoch 1440, training loss: 12.029549598693848 = 0.018065186217427254 + 2.0 * 6.005742073059082
Epoch 1440, val loss: 0.9656336903572083
Epoch 1450, training loss: 12.029361724853516 = 0.017708446830511093 + 2.0 * 6.005826473236084
Epoch 1450, val loss: 0.9686630964279175
Epoch 1460, training loss: 12.041342735290527 = 0.017360864207148552 + 2.0 * 6.011991024017334
Epoch 1460, val loss: 0.9715606570243835
Epoch 1470, training loss: 12.03225326538086 = 0.017026936635375023 + 2.0 * 6.007613182067871
Epoch 1470, val loss: 0.9741123914718628
Epoch 1480, training loss: 12.027774810791016 = 0.01670340821146965 + 2.0 * 6.00553560256958
Epoch 1480, val loss: 0.9770814180374146
Epoch 1490, training loss: 12.029013633728027 = 0.016387606039643288 + 2.0 * 6.006312847137451
Epoch 1490, val loss: 0.9798710346221924
Epoch 1500, training loss: 12.044133186340332 = 0.016084840521216393 + 2.0 * 6.014024257659912
Epoch 1500, val loss: 0.9824542999267578
Epoch 1510, training loss: 12.02784538269043 = 0.015786923468112946 + 2.0 * 6.00602912902832
Epoch 1510, val loss: 0.9851335287094116
Epoch 1520, training loss: 12.024507522583008 = 0.015504833310842514 + 2.0 * 6.0045013427734375
Epoch 1520, val loss: 0.9878414869308472
Epoch 1530, training loss: 12.020865440368652 = 0.015224694274365902 + 2.0 * 6.0028204917907715
Epoch 1530, val loss: 0.9906846880912781
Epoch 1540, training loss: 12.019591331481934 = 0.01494984608143568 + 2.0 * 6.002320766448975
Epoch 1540, val loss: 0.9934021830558777
Epoch 1550, training loss: 12.024857521057129 = 0.014682300388813019 + 2.0 * 6.005087375640869
Epoch 1550, val loss: 0.9961246252059937
Epoch 1560, training loss: 12.02130126953125 = 0.014423663727939129 + 2.0 * 6.003438949584961
Epoch 1560, val loss: 0.9985681772232056
Epoch 1570, training loss: 12.024256706237793 = 0.014175856485962868 + 2.0 * 6.005040645599365
Epoch 1570, val loss: 1.0011235475540161
Epoch 1580, training loss: 12.016082763671875 = 0.013932453468441963 + 2.0 * 6.001075267791748
Epoch 1580, val loss: 1.0037777423858643
Epoch 1590, training loss: 12.015945434570312 = 0.01369441393762827 + 2.0 * 6.001125335693359
Epoch 1590, val loss: 1.0064414739608765
Epoch 1600, training loss: 12.036423683166504 = 0.013464138843119144 + 2.0 * 6.01147985458374
Epoch 1600, val loss: 1.0089396238327026
Epoch 1610, training loss: 12.025320053100586 = 0.01323890220373869 + 2.0 * 6.006040573120117
Epoch 1610, val loss: 1.0112481117248535
Epoch 1620, training loss: 12.013593673706055 = 0.013022245839238167 + 2.0 * 6.000285625457764
Epoch 1620, val loss: 1.0137513875961304
Epoch 1630, training loss: 12.0142183303833 = 0.01280948892235756 + 2.0 * 6.000704288482666
Epoch 1630, val loss: 1.0163205862045288
Epoch 1640, training loss: 12.018364906311035 = 0.012599223293364048 + 2.0 * 6.002882957458496
Epoch 1640, val loss: 1.0188497304916382
Epoch 1650, training loss: 12.014750480651855 = 0.012396668083965778 + 2.0 * 6.001176834106445
Epoch 1650, val loss: 1.021215796470642
Epoch 1660, training loss: 12.013090133666992 = 0.012201344594359398 + 2.0 * 6.000444412231445
Epoch 1660, val loss: 1.0236390829086304
Epoch 1670, training loss: 12.011253356933594 = 0.012006648816168308 + 2.0 * 5.9996232986450195
Epoch 1670, val loss: 1.0260850191116333
Epoch 1680, training loss: 12.020576477050781 = 0.01181811187416315 + 2.0 * 6.0043792724609375
Epoch 1680, val loss: 1.0284991264343262
Epoch 1690, training loss: 12.01342487335205 = 0.011635370552539825 + 2.0 * 6.000894546508789
Epoch 1690, val loss: 1.0308237075805664
Epoch 1700, training loss: 12.016375541687012 = 0.011456211097538471 + 2.0 * 6.002459526062012
Epoch 1700, val loss: 1.0331690311431885
Epoch 1710, training loss: 12.008872032165527 = 0.011284018866717815 + 2.0 * 5.998794078826904
Epoch 1710, val loss: 1.0353611707687378
Epoch 1720, training loss: 12.008715629577637 = 0.011116545647382736 + 2.0 * 5.9987993240356445
Epoch 1720, val loss: 1.0376840829849243
Epoch 1730, training loss: 12.006521224975586 = 0.010949487797915936 + 2.0 * 5.997786045074463
Epoch 1730, val loss: 1.0400351285934448
Epoch 1740, training loss: 12.01366901397705 = 0.010786258615553379 + 2.0 * 6.001441478729248
Epoch 1740, val loss: 1.0422807931900024
Epoch 1750, training loss: 12.006110191345215 = 0.010627306066453457 + 2.0 * 5.997741222381592
Epoch 1750, val loss: 1.0446159839630127
Epoch 1760, training loss: 12.00704574584961 = 0.010473347268998623 + 2.0 * 5.998286247253418
Epoch 1760, val loss: 1.0468207597732544
Epoch 1770, training loss: 12.01356315612793 = 0.010322284884750843 + 2.0 * 6.001620292663574
Epoch 1770, val loss: 1.048953890800476
Epoch 1780, training loss: 12.006516456604004 = 0.010172795504331589 + 2.0 * 5.998171806335449
Epoch 1780, val loss: 1.0512456893920898
Epoch 1790, training loss: 12.008105278015137 = 0.010030883364379406 + 2.0 * 5.999037265777588
Epoch 1790, val loss: 1.0534335374832153
Epoch 1800, training loss: 12.003841400146484 = 0.009890017099678516 + 2.0 * 5.996975898742676
Epoch 1800, val loss: 1.0555236339569092
Epoch 1810, training loss: 12.002758979797363 = 0.009751650504767895 + 2.0 * 5.996503829956055
Epoch 1810, val loss: 1.0577073097229004
Epoch 1820, training loss: 12.005240440368652 = 0.009615554474294186 + 2.0 * 5.997812271118164
Epoch 1820, val loss: 1.0599086284637451
Epoch 1830, training loss: 12.005169868469238 = 0.009484334848821163 + 2.0 * 5.997842788696289
Epoch 1830, val loss: 1.0619298219680786
Epoch 1840, training loss: 12.003571510314941 = 0.009354704059660435 + 2.0 * 5.997108459472656
Epoch 1840, val loss: 1.0639886856079102
Epoch 1850, training loss: 11.999701499938965 = 0.009228631854057312 + 2.0 * 5.995236396789551
Epoch 1850, val loss: 1.066173791885376
Epoch 1860, training loss: 12.002171516418457 = 0.009104553610086441 + 2.0 * 5.996533393859863
Epoch 1860, val loss: 1.0683763027191162
Epoch 1870, training loss: 12.001116752624512 = 0.008982737548649311 + 2.0 * 5.996067047119141
Epoch 1870, val loss: 1.0703939199447632
Epoch 1880, training loss: 11.999917030334473 = 0.008864784613251686 + 2.0 * 5.995526313781738
Epoch 1880, val loss: 1.0724176168441772
Epoch 1890, training loss: 12.008639335632324 = 0.008748527616262436 + 2.0 * 5.999945640563965
Epoch 1890, val loss: 1.074514627456665
Epoch 1900, training loss: 11.99730110168457 = 0.008637093007564545 + 2.0 * 5.9943318367004395
Epoch 1900, val loss: 1.0762447118759155
Epoch 1910, training loss: 11.994443893432617 = 0.008527399972081184 + 2.0 * 5.992958068847656
Epoch 1910, val loss: 1.0784002542495728
Epoch 1920, training loss: 11.994705200195312 = 0.008418338373303413 + 2.0 * 5.993143558502197
Epoch 1920, val loss: 1.0804988145828247
Epoch 1930, training loss: 11.99991226196289 = 0.008311004377901554 + 2.0 * 5.995800495147705
Epoch 1930, val loss: 1.082517385482788
Epoch 1940, training loss: 11.99473762512207 = 0.008205805905163288 + 2.0 * 5.9932661056518555
Epoch 1940, val loss: 1.0842794179916382
Epoch 1950, training loss: 11.992815017700195 = 0.008103652857244015 + 2.0 * 5.992355823516846
Epoch 1950, val loss: 1.0863208770751953
Epoch 1960, training loss: 11.994412422180176 = 0.008004061877727509 + 2.0 * 5.993204116821289
Epoch 1960, val loss: 1.088301420211792
Epoch 1970, training loss: 11.99496841430664 = 0.007904996164143085 + 2.0 * 5.993531703948975
Epoch 1970, val loss: 1.0902092456817627
Epoch 1980, training loss: 11.992570877075195 = 0.007808534428477287 + 2.0 * 5.9923810958862305
Epoch 1980, val loss: 1.0922222137451172
Epoch 1990, training loss: 12.011953353881836 = 0.007715294137597084 + 2.0 * 6.002119064331055
Epoch 1990, val loss: 1.0940656661987305
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.7749
Flip ASR: 0.7378/225 nodes
The final ASR:0.68881, 0.07582, Accuracy:0.82099, 0.00698
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11554])
remove edge: torch.Size([2, 9572])
updated graph: torch.Size([2, 10570])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9889
Flip ASR: 0.9867/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9963
Flip ASR: 0.9956/225 nodes
The final ASR:0.98770, 0.00758, Accuracy:0.83333, 0.00800
Begin epxeriment: cont_weight: 2 epoch:2000 der1:0.3 der2:0.2 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 18.690645217895508 = 1.9428616762161255 + 2.0 * 8.373891830444336
Epoch 0, val loss: 1.9504939317703247
Epoch 10, training loss: 18.679956436157227 = 1.933105230331421 + 2.0 * 8.373425483703613
Epoch 10, val loss: 1.9402002096176147
Epoch 20, training loss: 18.66085433959961 = 1.9210448265075684 + 2.0 * 8.369904518127441
Epoch 20, val loss: 1.9271280765533447
Epoch 30, training loss: 18.596107482910156 = 1.904953956604004 + 2.0 * 8.345577239990234
Epoch 30, val loss: 1.909541368484497
Epoch 40, training loss: 18.298004150390625 = 1.8861017227172852 + 2.0 * 8.205951690673828
Epoch 40, val loss: 1.8899827003479004
Epoch 50, training loss: 17.45185089111328 = 1.8665204048156738 + 2.0 * 7.792665481567383
Epoch 50, val loss: 1.870696783065796
Epoch 60, training loss: 16.99530029296875 = 1.8474934101104736 + 2.0 * 7.5739030838012695
Epoch 60, val loss: 1.8535429239273071
Epoch 70, training loss: 16.254150390625 = 1.830102562904358 + 2.0 * 7.212023735046387
Epoch 70, val loss: 1.837681531906128
Epoch 80, training loss: 15.47659969329834 = 1.8157926797866821 + 2.0 * 6.8304033279418945
Epoch 80, val loss: 1.8240840435028076
Epoch 90, training loss: 15.113560676574707 = 1.8039149045944214 + 2.0 * 6.654822826385498
Epoch 90, val loss: 1.812538743019104
Epoch 100, training loss: 14.909623146057129 = 1.7900407314300537 + 2.0 * 6.559791088104248
Epoch 100, val loss: 1.799570918083191
Epoch 110, training loss: 14.743800163269043 = 1.774916648864746 + 2.0 * 6.484441757202148
Epoch 110, val loss: 1.785821795463562
Epoch 120, training loss: 14.61982250213623 = 1.7598565816879272 + 2.0 * 6.429983139038086
Epoch 120, val loss: 1.772221565246582
Epoch 130, training loss: 14.524434089660645 = 1.7441692352294922 + 2.0 * 6.390132427215576
Epoch 130, val loss: 1.7580622434616089
Epoch 140, training loss: 14.444470405578613 = 1.727092981338501 + 2.0 * 6.358688831329346
Epoch 140, val loss: 1.7432001829147339
Epoch 150, training loss: 14.378667831420898 = 1.7082606554031372 + 2.0 * 6.335203647613525
Epoch 150, val loss: 1.7272541522979736
Epoch 160, training loss: 14.315114974975586 = 1.6871777772903442 + 2.0 * 6.313968658447266
Epoch 160, val loss: 1.709966778755188
Epoch 170, training loss: 14.254733085632324 = 1.663402795791626 + 2.0 * 6.295665264129639
Epoch 170, val loss: 1.6907702684402466
Epoch 180, training loss: 14.193078994750977 = 1.6371160745620728 + 2.0 * 6.277981281280518
Epoch 180, val loss: 1.6697674989700317
Epoch 190, training loss: 14.126954078674316 = 1.607875108718872 + 2.0 * 6.259539604187012
Epoch 190, val loss: 1.646523118019104
Epoch 200, training loss: 14.063899040222168 = 1.575068712234497 + 2.0 * 6.244415283203125
Epoch 200, val loss: 1.6207112073898315
Epoch 210, training loss: 14.000015258789062 = 1.5382921695709229 + 2.0 * 6.230861663818359
Epoch 210, val loss: 1.5919512510299683
Epoch 220, training loss: 13.9384765625 = 1.4972491264343262 + 2.0 * 6.220613956451416
Epoch 220, val loss: 1.5599786043167114
Epoch 230, training loss: 13.87884521484375 = 1.4525169134140015 + 2.0 * 6.213164329528809
Epoch 230, val loss: 1.5254859924316406
Epoch 240, training loss: 13.805727005004883 = 1.4047892093658447 + 2.0 * 6.200469017028809
Epoch 240, val loss: 1.488953709602356
Epoch 250, training loss: 13.73607349395752 = 1.3540781736373901 + 2.0 * 6.19099760055542
Epoch 250, val loss: 1.4502660036087036
Epoch 260, training loss: 13.665641784667969 = 1.3006234169006348 + 2.0 * 6.182509422302246
Epoch 260, val loss: 1.4097566604614258
Epoch 270, training loss: 13.595341682434082 = 1.2450141906738281 + 2.0 * 6.175163745880127
Epoch 270, val loss: 1.367884635925293
Epoch 280, training loss: 13.54224681854248 = 1.1880806684494019 + 2.0 * 6.1770830154418945
Epoch 280, val loss: 1.3252453804016113
Epoch 290, training loss: 13.459012031555176 = 1.1319522857666016 + 2.0 * 6.163529872894287
Epoch 290, val loss: 1.2832320928573608
Epoch 300, training loss: 13.392318725585938 = 1.0766746997833252 + 2.0 * 6.157822132110596
Epoch 300, val loss: 1.2422131299972534
Epoch 310, training loss: 13.327363014221191 = 1.0224157571792603 + 2.0 * 6.152473449707031
Epoch 310, val loss: 1.202422022819519
Epoch 320, training loss: 13.264976501464844 = 0.9697482585906982 + 2.0 * 6.147614002227783
Epoch 320, val loss: 1.1641845703125
Epoch 330, training loss: 13.20604133605957 = 0.919295608997345 + 2.0 * 6.143373012542725
Epoch 330, val loss: 1.1279481649398804
Epoch 340, training loss: 13.148786544799805 = 0.8708882331848145 + 2.0 * 6.138949394226074
Epoch 340, val loss: 1.0935243368148804
Epoch 350, training loss: 13.098041534423828 = 0.8245490789413452 + 2.0 * 6.136746406555176
Epoch 350, val loss: 1.0609967708587646
Epoch 360, training loss: 13.047161102294922 = 0.780631959438324 + 2.0 * 6.133264541625977
Epoch 360, val loss: 1.0305144786834717
Epoch 370, training loss: 12.995728492736816 = 0.7392296195030212 + 2.0 * 6.128249645233154
Epoch 370, val loss: 1.002078890800476
Epoch 380, training loss: 12.949590682983398 = 0.7002540230751038 + 2.0 * 6.124668121337891
Epoch 380, val loss: 0.975682258605957
Epoch 390, training loss: 12.905535697937012 = 0.6640195250511169 + 2.0 * 6.120758056640625
Epoch 390, val loss: 0.9515711069107056
Epoch 400, training loss: 12.86436653137207 = 0.6301042437553406 + 2.0 * 6.117131233215332
Epoch 400, val loss: 0.929565966129303
Epoch 410, training loss: 12.826507568359375 = 0.5982943773269653 + 2.0 * 6.11410665512085
Epoch 410, val loss: 0.9094661474227905
Epoch 420, training loss: 12.793390274047852 = 0.568623423576355 + 2.0 * 6.1123833656311035
Epoch 420, val loss: 0.8913043141365051
Epoch 430, training loss: 12.756342887878418 = 0.5410488247871399 + 2.0 * 6.107646942138672
Epoch 430, val loss: 0.8751073479652405
Epoch 440, training loss: 12.72422981262207 = 0.5151822566986084 + 2.0 * 6.104523658752441
Epoch 440, val loss: 0.8605749011039734
Epoch 450, training loss: 12.7124662399292 = 0.4908945858478546 + 2.0 * 6.110785961151123
Epoch 450, val loss: 0.8475393652915955
Epoch 460, training loss: 12.666817665100098 = 0.4680561125278473 + 2.0 * 6.099380970001221
Epoch 460, val loss: 0.8359294533729553
Epoch 470, training loss: 12.640127182006836 = 0.4465906620025635 + 2.0 * 6.096768379211426
Epoch 470, val loss: 0.8258498311042786
Epoch 480, training loss: 12.61522102355957 = 0.4262000620365143 + 2.0 * 6.094510555267334
Epoch 480, val loss: 0.8168266415596008
Epoch 490, training loss: 12.601781845092773 = 0.40677428245544434 + 2.0 * 6.097503662109375
Epoch 490, val loss: 0.8087959289550781
Epoch 500, training loss: 12.570659637451172 = 0.38819634914398193 + 2.0 * 6.091231822967529
Epoch 500, val loss: 0.8017733097076416
Epoch 510, training loss: 12.54588508605957 = 0.3704334795475006 + 2.0 * 6.087725639343262
Epoch 510, val loss: 0.7956902384757996
Epoch 520, training loss: 12.525147438049316 = 0.35333576798439026 + 2.0 * 6.085906028747559
Epoch 520, val loss: 0.7903217673301697
Epoch 530, training loss: 12.5121488571167 = 0.33680224418640137 + 2.0 * 6.087673187255859
Epoch 530, val loss: 0.7855753898620605
Epoch 540, training loss: 12.488723754882812 = 0.3209007978439331 + 2.0 * 6.083911418914795
Epoch 540, val loss: 0.7816122770309448
Epoch 550, training loss: 12.466702461242676 = 0.30546796321868896 + 2.0 * 6.080617427825928
Epoch 550, val loss: 0.778256356716156
Epoch 560, training loss: 12.446585655212402 = 0.29039695858955383 + 2.0 * 6.078094482421875
Epoch 560, val loss: 0.7752876877784729
Epoch 570, training loss: 12.455245018005371 = 0.27567437291145325 + 2.0 * 6.089785099029541
Epoch 570, val loss: 0.7725641131401062
Epoch 580, training loss: 12.416116714477539 = 0.26155325770378113 + 2.0 * 6.077281951904297
Epoch 580, val loss: 0.7704865336418152
Epoch 590, training loss: 12.394181251525879 = 0.24783118069171906 + 2.0 * 6.073174953460693
Epoch 590, val loss: 0.7689787149429321
Epoch 600, training loss: 12.378327369689941 = 0.23447899520397186 + 2.0 * 6.071924209594727
Epoch 600, val loss: 0.7676047086715698
Epoch 610, training loss: 12.367242813110352 = 0.22154195606708527 + 2.0 * 6.072850227355957
Epoch 610, val loss: 0.7665932178497314
Epoch 620, training loss: 12.350486755371094 = 0.20919393002986908 + 2.0 * 6.070646286010742
Epoch 620, val loss: 0.7660310864448547
Epoch 630, training loss: 12.333929061889648 = 0.19736212491989136 + 2.0 * 6.068283557891846
Epoch 630, val loss: 0.7658620476722717
Epoch 640, training loss: 12.31801700592041 = 0.18615062534809113 + 2.0 * 6.0659332275390625
Epoch 640, val loss: 0.7661746144294739
Epoch 650, training loss: 12.306608200073242 = 0.17551812529563904 + 2.0 * 6.065545082092285
Epoch 650, val loss: 0.7668859362602234
Epoch 660, training loss: 12.296537399291992 = 0.1654791384935379 + 2.0 * 6.0655293464660645
Epoch 660, val loss: 0.7678214907646179
Epoch 670, training loss: 12.284408569335938 = 0.15612003207206726 + 2.0 * 6.064144134521484
Epoch 670, val loss: 0.7694645524024963
Epoch 680, training loss: 12.270108222961426 = 0.1473466157913208 + 2.0 * 6.061380863189697
Epoch 680, val loss: 0.7713715434074402
Epoch 690, training loss: 12.260706901550293 = 0.13915584981441498 + 2.0 * 6.0607757568359375
Epoch 690, val loss: 0.7737568020820618
Epoch 700, training loss: 12.260446548461914 = 0.13154610991477966 + 2.0 * 6.064450263977051
Epoch 700, val loss: 0.7764890193939209
Epoch 710, training loss: 12.243163108825684 = 0.1244397684931755 + 2.0 * 6.059361457824707
Epoch 710, val loss: 0.7795993089675903
Epoch 720, training loss: 12.231310844421387 = 0.11787734180688858 + 2.0 * 6.0567169189453125
Epoch 720, val loss: 0.7831400036811829
Epoch 730, training loss: 12.222358703613281 = 0.11174939572811127 + 2.0 * 6.055304527282715
Epoch 730, val loss: 0.7869784832000732
Epoch 740, training loss: 12.226958274841309 = 0.10601924359798431 + 2.0 * 6.060469627380371
Epoch 740, val loss: 0.7910379767417908
Epoch 750, training loss: 12.214612007141113 = 0.10071426630020142 + 2.0 * 6.056948661804199
Epoch 750, val loss: 0.7954114079475403
Epoch 760, training loss: 12.20417594909668 = 0.09576871991157532 + 2.0 * 6.054203510284424
Epoch 760, val loss: 0.800244927406311
Epoch 770, training loss: 12.204760551452637 = 0.09116590768098831 + 2.0 * 6.056797504425049
Epoch 770, val loss: 0.8050675988197327
Epoch 780, training loss: 12.190977096557617 = 0.08683246374130249 + 2.0 * 6.052072525024414
Epoch 780, val loss: 0.810019850730896
Epoch 790, training loss: 12.184408187866211 = 0.08281107246875763 + 2.0 * 6.050798416137695
Epoch 790, val loss: 0.8153257966041565
Epoch 800, training loss: 12.18110179901123 = 0.0790264829993248 + 2.0 * 6.051037788391113
Epoch 800, val loss: 0.8205875158309937
Epoch 810, training loss: 12.172263145446777 = 0.07547944784164429 + 2.0 * 6.048391819000244
Epoch 810, val loss: 0.8262386918067932
Epoch 820, training loss: 12.166276931762695 = 0.07216006517410278 + 2.0 * 6.047058582305908
Epoch 820, val loss: 0.8318796157836914
Epoch 830, training loss: 12.160584449768066 = 0.06902462244033813 + 2.0 * 6.045779705047607
Epoch 830, val loss: 0.8376243114471436
Epoch 840, training loss: 12.160292625427246 = 0.06607460230588913 + 2.0 * 6.047109127044678
Epoch 840, val loss: 0.8434116840362549
Epoch 850, training loss: 12.156575202941895 = 0.06329574435949326 + 2.0 * 6.046639919281006
Epoch 850, val loss: 0.8493640422821045
Epoch 860, training loss: 12.147042274475098 = 0.060692813247442245 + 2.0 * 6.043174743652344
Epoch 860, val loss: 0.8553655743598938
Epoch 870, training loss: 12.145575523376465 = 0.058238495141267776 + 2.0 * 6.043668746948242
Epoch 870, val loss: 0.8614321947097778
Epoch 880, training loss: 12.147638320922852 = 0.05591990426182747 + 2.0 * 6.045859336853027
Epoch 880, val loss: 0.8673981428146362
Epoch 890, training loss: 12.133893013000488 = 0.0537264347076416 + 2.0 * 6.040083408355713
Epoch 890, val loss: 0.8734663724899292
Epoch 900, training loss: 12.131099700927734 = 0.05165981128811836 + 2.0 * 6.039720058441162
Epoch 900, val loss: 0.8796283006668091
Epoch 910, training loss: 12.145037651062012 = 0.0497104786336422 + 2.0 * 6.047663688659668
Epoch 910, val loss: 0.8856450915336609
Epoch 920, training loss: 12.127050399780273 = 0.04783568158745766 + 2.0 * 6.039607524871826
Epoch 920, val loss: 0.891694962978363
Epoch 930, training loss: 12.119787216186523 = 0.04608406499028206 + 2.0 * 6.036851406097412
Epoch 930, val loss: 0.8978586792945862
Epoch 940, training loss: 12.116316795349121 = 0.04441075772047043 + 2.0 * 6.035953044891357
Epoch 940, val loss: 0.903971254825592
Epoch 950, training loss: 12.116106033325195 = 0.042819347232580185 + 2.0 * 6.0366435050964355
Epoch 950, val loss: 0.9100989699363708
Epoch 960, training loss: 12.114675521850586 = 0.04131137207150459 + 2.0 * 6.03668212890625
Epoch 960, val loss: 0.9159212708473206
Epoch 970, training loss: 12.115018844604492 = 0.03987226262688637 + 2.0 * 6.037573337554932
Epoch 970, val loss: 0.9220550060272217
Epoch 980, training loss: 12.10700511932373 = 0.038513727486133575 + 2.0 * 6.034245491027832
Epoch 980, val loss: 0.928138256072998
Epoch 990, training loss: 12.103862762451172 = 0.0372207872569561 + 2.0 * 6.033320903778076
Epoch 990, val loss: 0.9340727925300598
Epoch 1000, training loss: 12.104249000549316 = 0.03598383069038391 + 2.0 * 6.034132480621338
Epoch 1000, val loss: 0.9399495124816895
Epoch 1010, training loss: 12.099173545837402 = 0.034801993519067764 + 2.0 * 6.0321855545043945
Epoch 1010, val loss: 0.9458900690078735
Epoch 1020, training loss: 12.102035522460938 = 0.03367755189538002 + 2.0 * 6.034179210662842
Epoch 1020, val loss: 0.951609194278717
Epoch 1030, training loss: 12.095137596130371 = 0.03261076658964157 + 2.0 * 6.03126335144043
Epoch 1030, val loss: 0.9574949145317078
Epoch 1040, training loss: 12.093528747558594 = 0.031587179750204086 + 2.0 * 6.030970573425293
Epoch 1040, val loss: 0.9632247090339661
Epoch 1050, training loss: 12.090764999389648 = 0.030614223331212997 + 2.0 * 6.030075550079346
Epoch 1050, val loss: 0.9689098000526428
Epoch 1060, training loss: 12.088737487792969 = 0.02968156896531582 + 2.0 * 6.0295281410217285
Epoch 1060, val loss: 0.9746206998825073
Epoch 1070, training loss: 12.092710494995117 = 0.028790632262825966 + 2.0 * 6.0319600105285645
Epoch 1070, val loss: 0.9800445437431335
Epoch 1080, training loss: 12.084122657775879 = 0.02793831378221512 + 2.0 * 6.028092384338379
Epoch 1080, val loss: 0.9856882691383362
Epoch 1090, training loss: 12.083809852600098 = 0.02712707780301571 + 2.0 * 6.028341293334961
Epoch 1090, val loss: 0.9912476539611816
Epoch 1100, training loss: 12.078113555908203 = 0.026348482817411423 + 2.0 * 6.025882720947266
Epoch 1100, val loss: 0.996676504611969
Epoch 1110, training loss: 12.076467514038086 = 0.0256038848310709 + 2.0 * 6.0254316329956055
Epoch 1110, val loss: 1.002186894416809
Epoch 1120, training loss: 12.08000659942627 = 0.02488490752875805 + 2.0 * 6.027560710906982
Epoch 1120, val loss: 1.007522463798523
Epoch 1130, training loss: 12.079618453979492 = 0.024202214553952217 + 2.0 * 6.027708053588867
Epoch 1130, val loss: 1.0128425359725952
Epoch 1140, training loss: 12.074424743652344 = 0.02354699932038784 + 2.0 * 6.0254387855529785
Epoch 1140, val loss: 1.0181723833084106
Epoch 1150, training loss: 12.070040702819824 = 0.022918876260519028 + 2.0 * 6.023561000823975
Epoch 1150, val loss: 1.023469090461731
Epoch 1160, training loss: 12.0712308883667 = 0.02231428399682045 + 2.0 * 6.024458408355713
Epoch 1160, val loss: 1.0286352634429932
Epoch 1170, training loss: 12.067537307739258 = 0.021733678877353668 + 2.0 * 6.022902011871338
Epoch 1170, val loss: 1.033606767654419
Epoch 1180, training loss: 12.065587043762207 = 0.02117783948779106 + 2.0 * 6.022204399108887
Epoch 1180, val loss: 1.0389336347579956
Epoch 1190, training loss: 12.06365966796875 = 0.020642085000872612 + 2.0 * 6.021508693695068
Epoch 1190, val loss: 1.0439634323120117
Epoch 1200, training loss: 12.070345878601074 = 0.02012890949845314 + 2.0 * 6.025108337402344
Epoch 1200, val loss: 1.0489075183868408
Epoch 1210, training loss: 12.061610221862793 = 0.019631195813417435 + 2.0 * 6.020989418029785
Epoch 1210, val loss: 1.0538291931152344
Epoch 1220, training loss: 12.058442115783691 = 0.01915295422077179 + 2.0 * 6.019644737243652
Epoch 1220, val loss: 1.0588126182556152
Epoch 1230, training loss: 12.05860424041748 = 0.018692897632718086 + 2.0 * 6.019955635070801
Epoch 1230, val loss: 1.0636537075042725
Epoch 1240, training loss: 12.062559127807617 = 0.018254341557621956 + 2.0 * 6.022152423858643
Epoch 1240, val loss: 1.068127989768982
Epoch 1250, training loss: 12.062176704406738 = 0.017823897302150726 + 2.0 * 6.022176265716553
Epoch 1250, val loss: 1.0729284286499023
Epoch 1260, training loss: 12.055059432983398 = 0.017419718205928802 + 2.0 * 6.018819808959961
Epoch 1260, val loss: 1.0777640342712402
Epoch 1270, training loss: 12.05075454711914 = 0.017024323344230652 + 2.0 * 6.016865253448486
Epoch 1270, val loss: 1.0823196172714233
Epoch 1280, training loss: 12.049739837646484 = 0.016642430797219276 + 2.0 * 6.0165486335754395
Epoch 1280, val loss: 1.0869220495224
Epoch 1290, training loss: 12.058252334594727 = 0.016273152083158493 + 2.0 * 6.020989418029785
Epoch 1290, val loss: 1.0914409160614014
Epoch 1300, training loss: 12.054915428161621 = 0.015915436670184135 + 2.0 * 6.019499778747559
Epoch 1300, val loss: 1.0958020687103271
Epoch 1310, training loss: 12.049311637878418 = 0.015569985844194889 + 2.0 * 6.016870975494385
Epoch 1310, val loss: 1.1003046035766602
Epoch 1320, training loss: 12.044998168945312 = 0.015237479470670223 + 2.0 * 6.014880180358887
Epoch 1320, val loss: 1.1047106981277466
Epoch 1330, training loss: 12.04813003540039 = 0.014917152002453804 + 2.0 * 6.016606330871582
Epoch 1330, val loss: 1.1090978384017944
Epoch 1340, training loss: 12.04978084564209 = 0.014606707729399204 + 2.0 * 6.017587184906006
Epoch 1340, val loss: 1.1132502555847168
Epoch 1350, training loss: 12.041491508483887 = 0.014303529635071754 + 2.0 * 6.013594150543213
Epoch 1350, val loss: 1.1175276041030884
Epoch 1360, training loss: 12.041054725646973 = 0.01401215698570013 + 2.0 * 6.013521194458008
Epoch 1360, val loss: 1.1218671798706055
Epoch 1370, training loss: 12.041009902954102 = 0.013730546459555626 + 2.0 * 6.013639450073242
Epoch 1370, val loss: 1.1260393857955933
Epoch 1380, training loss: 12.045897483825684 = 0.013455912470817566 + 2.0 * 6.016220569610596
Epoch 1380, val loss: 1.1300472021102905
Epoch 1390, training loss: 12.038805961608887 = 0.013190947473049164 + 2.0 * 6.012807369232178
Epoch 1390, val loss: 1.134318470954895
Epoch 1400, training loss: 12.053346633911133 = 0.012935439124703407 + 2.0 * 6.020205497741699
Epoch 1400, val loss: 1.1382241249084473
Epoch 1410, training loss: 12.037520408630371 = 0.012685781344771385 + 2.0 * 6.012417316436768
Epoch 1410, val loss: 1.1422805786132812
Epoch 1420, training loss: 12.034750938415527 = 0.012445121072232723 + 2.0 * 6.011152744293213
Epoch 1420, val loss: 1.1463346481323242
Epoch 1430, training loss: 12.033559799194336 = 0.0122107258066535 + 2.0 * 6.010674476623535
Epoch 1430, val loss: 1.1503633260726929
Epoch 1440, training loss: 12.035980224609375 = 0.011983499862253666 + 2.0 * 6.011998176574707
Epoch 1440, val loss: 1.1542333364486694
Epoch 1450, training loss: 12.036170959472656 = 0.011762154288589954 + 2.0 * 6.012204170227051
Epoch 1450, val loss: 1.1580064296722412
Epoch 1460, training loss: 12.032064437866211 = 0.011546825990080833 + 2.0 * 6.010258674621582
Epoch 1460, val loss: 1.162047028541565
Epoch 1470, training loss: 12.051858901977539 = 0.011343758553266525 + 2.0 * 6.020257472991943
Epoch 1470, val loss: 1.1658239364624023
Epoch 1480, training loss: 12.03342056274414 = 0.011135750450193882 + 2.0 * 6.011142253875732
Epoch 1480, val loss: 1.1693884134292603
Epoch 1490, training loss: 12.02757453918457 = 0.01094313245266676 + 2.0 * 6.008315563201904
Epoch 1490, val loss: 1.1733317375183105
Epoch 1500, training loss: 12.026084899902344 = 0.010752526111900806 + 2.0 * 6.007666110992432
Epoch 1500, val loss: 1.1769932508468628
Epoch 1510, training loss: 12.026108741760254 = 0.010566795244812965 + 2.0 * 6.007771015167236
Epoch 1510, val loss: 1.1806801557540894
Epoch 1520, training loss: 12.048023223876953 = 0.010386638343334198 + 2.0 * 6.018818378448486
Epoch 1520, val loss: 1.1841193437576294
Epoch 1530, training loss: 12.032864570617676 = 0.01020968146622181 + 2.0 * 6.011327266693115
Epoch 1530, val loss: 1.1877305507659912
Epoch 1540, training loss: 12.024192810058594 = 0.010039486922323704 + 2.0 * 6.007076740264893
Epoch 1540, val loss: 1.19138765335083
Epoch 1550, training loss: 12.023210525512695 = 0.009873783215880394 + 2.0 * 6.006668567657471
Epoch 1550, val loss: 1.1949329376220703
Epoch 1560, training loss: 12.023568153381348 = 0.009712176397442818 + 2.0 * 6.006927967071533
Epoch 1560, val loss: 1.1984870433807373
Epoch 1570, training loss: 12.035470962524414 = 0.009554332122206688 + 2.0 * 6.012958526611328
Epoch 1570, val loss: 1.201759696006775
Epoch 1580, training loss: 12.028145790100098 = 0.009400250390172005 + 2.0 * 6.009372711181641
Epoch 1580, val loss: 1.205100655555725
Epoch 1590, training loss: 12.021926879882812 = 0.009251244366168976 + 2.0 * 6.006337642669678
Epoch 1590, val loss: 1.2087020874023438
Epoch 1600, training loss: 12.023344993591309 = 0.009104923345148563 + 2.0 * 6.007120132446289
Epoch 1600, val loss: 1.211980938911438
Epoch 1610, training loss: 12.02186107635498 = 0.0089638726785779 + 2.0 * 6.006448745727539
Epoch 1610, val loss: 1.2152258157730103
Epoch 1620, training loss: 12.022283554077148 = 0.008825012482702732 + 2.0 * 6.0067291259765625
Epoch 1620, val loss: 1.2184991836547852
Epoch 1630, training loss: 12.026850700378418 = 0.00869024358689785 + 2.0 * 6.009080410003662
Epoch 1630, val loss: 1.2217226028442383
Epoch 1640, training loss: 12.018533706665039 = 0.008560207672417164 + 2.0 * 6.004986763000488
Epoch 1640, val loss: 1.2250968217849731
Epoch 1650, training loss: 12.01697826385498 = 0.008431588299572468 + 2.0 * 6.004273414611816
Epoch 1650, val loss: 1.2282954454421997
Epoch 1660, training loss: 12.022379875183105 = 0.008306879550218582 + 2.0 * 6.0070366859436035
Epoch 1660, val loss: 1.2313669919967651
Epoch 1670, training loss: 12.020379066467285 = 0.008183963596820831 + 2.0 * 6.006097316741943
Epoch 1670, val loss: 1.2345538139343262
Epoch 1680, training loss: 12.017897605895996 = 0.008064218796789646 + 2.0 * 6.004916667938232
Epoch 1680, val loss: 1.2375907897949219
Epoch 1690, training loss: 12.013936042785645 = 0.00794809777289629 + 2.0 * 6.002994060516357
Epoch 1690, val loss: 1.2408955097198486
Epoch 1700, training loss: 12.012653350830078 = 0.007834265008568764 + 2.0 * 6.0024094581604
Epoch 1700, val loss: 1.2439215183258057
Epoch 1710, training loss: 12.023022651672363 = 0.007722725160419941 + 2.0 * 6.007649898529053
Epoch 1710, val loss: 1.2468266487121582
Epoch 1720, training loss: 12.018437385559082 = 0.007613573223352432 + 2.0 * 6.0054121017456055
Epoch 1720, val loss: 1.2498610019683838
Epoch 1730, training loss: 12.010869026184082 = 0.007508478593081236 + 2.0 * 6.001680374145508
Epoch 1730, val loss: 1.2529205083847046
Epoch 1740, training loss: 12.009538650512695 = 0.0074058957397937775 + 2.0 * 6.001066207885742
Epoch 1740, val loss: 1.2559903860092163
Epoch 1750, training loss: 12.01417064666748 = 0.007304690312594175 + 2.0 * 6.003432750701904
Epoch 1750, val loss: 1.258834958076477
Epoch 1760, training loss: 12.01328182220459 = 0.0072047594003379345 + 2.0 * 6.00303840637207
Epoch 1760, val loss: 1.2617632150650024
Epoch 1770, training loss: 12.0086088180542 = 0.007107486482709646 + 2.0 * 6.000750541687012
Epoch 1770, val loss: 1.264403223991394
Epoch 1780, training loss: 12.00754165649414 = 0.007013306021690369 + 2.0 * 6.0002641677856445
Epoch 1780, val loss: 1.2673802375793457
Epoch 1790, training loss: 12.006753921508789 = 0.0069210766814649105 + 2.0 * 5.9999165534973145
Epoch 1790, val loss: 1.2702354192733765
Epoch 1800, training loss: 12.006767272949219 = 0.0068303789012134075 + 2.0 * 5.999968528747559
Epoch 1800, val loss: 1.2730352878570557
Epoch 1810, training loss: 12.010614395141602 = 0.006741764489561319 + 2.0 * 6.001936435699463
Epoch 1810, val loss: 1.2757186889648438
Epoch 1820, training loss: 12.016427040100098 = 0.00665608374401927 + 2.0 * 6.004885673522949
Epoch 1820, val loss: 1.2784804105758667
Epoch 1830, training loss: 12.00557804107666 = 0.006570987869054079 + 2.0 * 5.9995036125183105
Epoch 1830, val loss: 1.2809909582138062
Epoch 1840, training loss: 12.00403118133545 = 0.0064882682636380196 + 2.0 * 5.998771667480469
Epoch 1840, val loss: 1.2839950323104858
Epoch 1850, training loss: 12.001733779907227 = 0.006407226901501417 + 2.0 * 5.997663497924805
Epoch 1850, val loss: 1.2866699695587158
Epoch 1860, training loss: 12.001065254211426 = 0.006327670067548752 + 2.0 * 5.997368812561035
Epoch 1860, val loss: 1.2893747091293335
Epoch 1870, training loss: 12.02133560180664 = 0.006251176353543997 + 2.0 * 6.007542133331299
Epoch 1870, val loss: 1.2919560670852661
Epoch 1880, training loss: 12.006869316101074 = 0.006171755958348513 + 2.0 * 6.0003485679626465
Epoch 1880, val loss: 1.2942719459533691
Epoch 1890, training loss: 12.001280784606934 = 0.006097857374697924 + 2.0 * 5.997591495513916
Epoch 1890, val loss: 1.2971948385238647
Epoch 1900, training loss: 12.006502151489258 = 0.006023711524903774 + 2.0 * 6.000239372253418
Epoch 1900, val loss: 1.299609661102295
Epoch 1910, training loss: 12.00210189819336 = 0.005952156148850918 + 2.0 * 5.998075008392334
Epoch 1910, val loss: 1.3020318746566772
Epoch 1920, training loss: 11.997452735900879 = 0.0058817872777581215 + 2.0 * 5.995785236358643
Epoch 1920, val loss: 1.3046445846557617
Epoch 1930, training loss: 11.997590065002441 = 0.005813174415379763 + 2.0 * 5.9958882331848145
Epoch 1930, val loss: 1.3072869777679443
Epoch 1940, training loss: 12.007803916931152 = 0.0057455999776721 + 2.0 * 6.001029014587402
Epoch 1940, val loss: 1.3096132278442383
Epoch 1950, training loss: 12.001311302185059 = 0.005679183639585972 + 2.0 * 5.99781608581543
Epoch 1950, val loss: 1.31200110912323
Epoch 1960, training loss: 11.996814727783203 = 0.0056136371567845345 + 2.0 * 5.995600700378418
Epoch 1960, val loss: 1.314484715461731
Epoch 1970, training loss: 12.000237464904785 = 0.0055504245683550835 + 2.0 * 5.99734354019165
Epoch 1970, val loss: 1.3169546127319336
Epoch 1980, training loss: 11.998584747314453 = 0.005486779380589724 + 2.0 * 5.996549129486084
Epoch 1980, val loss: 1.3191277980804443
Epoch 1990, training loss: 11.996251106262207 = 0.00542403943836689 + 2.0 * 5.995413303375244
Epoch 1990, val loss: 1.3216265439987183
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8074
Overall ASR: 0.5978
Flip ASR: 0.5156/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.714242935180664 = 1.9665961265563965 + 2.0 * 8.373823165893555
Epoch 0, val loss: 1.9689537286758423
Epoch 10, training loss: 18.70018768310547 = 1.9547295570373535 + 2.0 * 8.372729301452637
Epoch 10, val loss: 1.9560461044311523
Epoch 20, training loss: 18.672555923461914 = 1.94000244140625 + 2.0 * 8.366276741027832
Epoch 20, val loss: 1.9388188123703003
Epoch 30, training loss: 18.573305130004883 = 1.9204977750778198 + 2.0 * 8.326403617858887
Epoch 30, val loss: 1.9155858755111694
Epoch 40, training loss: 18.09198760986328 = 1.8993644714355469 + 2.0 * 8.096311569213867
Epoch 40, val loss: 1.891141414642334
Epoch 50, training loss: 17.35747718811035 = 1.877739667892456 + 2.0 * 7.739869117736816
Epoch 50, val loss: 1.8668982982635498
Epoch 60, training loss: 16.55508804321289 = 1.8600775003433228 + 2.0 * 7.347505569458008
Epoch 60, val loss: 1.8484028577804565
Epoch 70, training loss: 15.770707130432129 = 1.8446495532989502 + 2.0 * 6.963028907775879
Epoch 70, val loss: 1.8317992687225342
Epoch 80, training loss: 15.271984100341797 = 1.8312256336212158 + 2.0 * 6.72037935256958
Epoch 80, val loss: 1.8173490762710571
Epoch 90, training loss: 14.924429893493652 = 1.8165912628173828 + 2.0 * 6.553919315338135
Epoch 90, val loss: 1.8020713329315186
Epoch 100, training loss: 14.72238540649414 = 1.800730586051941 + 2.0 * 6.460827350616455
Epoch 100, val loss: 1.7862215042114258
Epoch 110, training loss: 14.58191204071045 = 1.7858353853225708 + 2.0 * 6.398038387298584
Epoch 110, val loss: 1.7714521884918213
Epoch 120, training loss: 14.472658157348633 = 1.7719017267227173 + 2.0 * 6.350378036499023
Epoch 120, val loss: 1.7578462362289429
Epoch 130, training loss: 14.38010311126709 = 1.758235216140747 + 2.0 * 6.310934066772461
Epoch 130, val loss: 1.744645595550537
Epoch 140, training loss: 14.308177947998047 = 1.744071364402771 + 2.0 * 6.282053470611572
Epoch 140, val loss: 1.7314914464950562
Epoch 150, training loss: 14.250200271606445 = 1.729074239730835 + 2.0 * 6.260562896728516
Epoch 150, val loss: 1.7180840969085693
Epoch 160, training loss: 14.196558952331543 = 1.7131140232086182 + 2.0 * 6.241722583770752
Epoch 160, val loss: 1.7042161226272583
Epoch 170, training loss: 14.142276763916016 = 1.6957873106002808 + 2.0 * 6.223244667053223
Epoch 170, val loss: 1.6895381212234497
Epoch 180, training loss: 14.094823837280273 = 1.6765849590301514 + 2.0 * 6.2091193199157715
Epoch 180, val loss: 1.6737020015716553
Epoch 190, training loss: 14.048844337463379 = 1.6551672220230103 + 2.0 * 6.19683837890625
Epoch 190, val loss: 1.6562753915786743
Epoch 200, training loss: 14.000351905822754 = 1.631102442741394 + 2.0 * 6.184624671936035
Epoch 200, val loss: 1.6369105577468872
Epoch 210, training loss: 13.952197074890137 = 1.6040621995925903 + 2.0 * 6.174067497253418
Epoch 210, val loss: 1.6152892112731934
Epoch 220, training loss: 13.906133651733398 = 1.573792815208435 + 2.0 * 6.166170597076416
Epoch 220, val loss: 1.59113347530365
Epoch 230, training loss: 13.856243133544922 = 1.5400806665420532 + 2.0 * 6.1580810546875
Epoch 230, val loss: 1.5643326044082642
Epoch 240, training loss: 13.807112693786621 = 1.5032212734222412 + 2.0 * 6.1519455909729
Epoch 240, val loss: 1.5351355075836182
Epoch 250, training loss: 13.752718925476074 = 1.4631460905075073 + 2.0 * 6.144786357879639
Epoch 250, val loss: 1.5035394430160522
Epoch 260, training loss: 13.702266693115234 = 1.4201552867889404 + 2.0 * 6.141055583953857
Epoch 260, val loss: 1.4698189496994019
Epoch 270, training loss: 13.646276473999023 = 1.3748880624771118 + 2.0 * 6.1356940269470215
Epoch 270, val loss: 1.4345757961273193
Epoch 280, training loss: 13.587976455688477 = 1.3278955221176147 + 2.0 * 6.130040645599365
Epoch 280, val loss: 1.3982751369476318
Epoch 290, training loss: 13.543455123901367 = 1.2798900604248047 + 2.0 * 6.131782531738281
Epoch 290, val loss: 1.3614290952682495
Epoch 300, training loss: 13.47796630859375 = 1.2319508790969849 + 2.0 * 6.123007774353027
Epoch 300, val loss: 1.325348973274231
Epoch 310, training loss: 13.423067092895508 = 1.1847127676010132 + 2.0 * 6.119177341461182
Epoch 310, val loss: 1.2899755239486694
Epoch 320, training loss: 13.368172645568848 = 1.1380938291549683 + 2.0 * 6.115039348602295
Epoch 320, val loss: 1.2554049491882324
Epoch 330, training loss: 13.322614669799805 = 1.0925015211105347 + 2.0 * 6.11505651473999
Epoch 330, val loss: 1.2219789028167725
Epoch 340, training loss: 13.26606559753418 = 1.04856538772583 + 2.0 * 6.108750343322754
Epoch 340, val loss: 1.1902189254760742
Epoch 350, training loss: 13.21740436553955 = 1.006369948387146 + 2.0 * 6.105517387390137
Epoch 350, val loss: 1.159994125366211
Epoch 360, training loss: 13.170565605163574 = 0.965563178062439 + 2.0 * 6.102501392364502
Epoch 360, val loss: 1.131199836730957
Epoch 370, training loss: 13.128510475158691 = 0.9262496829032898 + 2.0 * 6.101130485534668
Epoch 370, val loss: 1.1037883758544922
Epoch 380, training loss: 13.082636833190918 = 0.8886180520057678 + 2.0 * 6.097009181976318
Epoch 380, val loss: 1.0778470039367676
Epoch 390, training loss: 13.040379524230957 = 0.8522853851318359 + 2.0 * 6.0940470695495605
Epoch 390, val loss: 1.053079605102539
Epoch 400, training loss: 13.007177352905273 = 0.8170307874679565 + 2.0 * 6.095073223114014
Epoch 400, val loss: 1.0293514728546143
Epoch 410, training loss: 12.967544555664062 = 0.7830483913421631 + 2.0 * 6.09224796295166
Epoch 410, val loss: 1.0065847635269165
Epoch 420, training loss: 12.926481246948242 = 0.750200629234314 + 2.0 * 6.088140487670898
Epoch 420, val loss: 0.9848432540893555
Epoch 430, training loss: 12.88870906829834 = 0.7184144854545593 + 2.0 * 6.085147380828857
Epoch 430, val loss: 0.9640413522720337
Epoch 440, training loss: 12.855412483215332 = 0.6877186298370361 + 2.0 * 6.0838470458984375
Epoch 440, val loss: 0.9440706968307495
Epoch 450, training loss: 12.820265769958496 = 0.6582868695259094 + 2.0 * 6.080989360809326
Epoch 450, val loss: 0.9251481890678406
Epoch 460, training loss: 12.787161827087402 = 0.6300269365310669 + 2.0 * 6.0785675048828125
Epoch 460, val loss: 0.9073151350021362
Epoch 470, training loss: 12.765316009521484 = 0.6029399633407593 + 2.0 * 6.081188201904297
Epoch 470, val loss: 0.8906087875366211
Epoch 480, training loss: 12.732196807861328 = 0.577369749546051 + 2.0 * 6.077413558959961
Epoch 480, val loss: 0.8751423954963684
Epoch 490, training loss: 12.700811386108398 = 0.5531242489814758 + 2.0 * 6.073843479156494
Epoch 490, val loss: 0.8609648942947388
Epoch 500, training loss: 12.674145698547363 = 0.5301424264907837 + 2.0 * 6.0720014572143555
Epoch 500, val loss: 0.8480811715126038
Epoch 510, training loss: 12.64993667602539 = 0.5084067583084106 + 2.0 * 6.070765018463135
Epoch 510, val loss: 0.8363179564476013
Epoch 520, training loss: 12.625476837158203 = 0.48783671855926514 + 2.0 * 6.068819999694824
Epoch 520, val loss: 0.8257805109024048
Epoch 530, training loss: 12.605472564697266 = 0.46828147768974304 + 2.0 * 6.0685954093933105
Epoch 530, val loss: 0.816292941570282
Epoch 540, training loss: 12.579252243041992 = 0.4496884047985077 + 2.0 * 6.06478214263916
Epoch 540, val loss: 0.8078828454017639
Epoch 550, training loss: 12.565389633178711 = 0.4318440854549408 + 2.0 * 6.066772937774658
Epoch 550, val loss: 0.8003019690513611
Epoch 560, training loss: 12.543161392211914 = 0.41487738490104675 + 2.0 * 6.064142227172852
Epoch 560, val loss: 0.7936605215072632
Epoch 570, training loss: 12.519913673400879 = 0.3984704911708832 + 2.0 * 6.060721397399902
Epoch 570, val loss: 0.7878458499908447
Epoch 580, training loss: 12.501800537109375 = 0.38263851404190063 + 2.0 * 6.0595808029174805
Epoch 580, val loss: 0.7826462388038635
Epoch 590, training loss: 12.485321044921875 = 0.36732780933380127 + 2.0 * 6.058996677398682
Epoch 590, val loss: 0.7780782580375671
Epoch 600, training loss: 12.466964721679688 = 0.35248979926109314 + 2.0 * 6.05723762512207
Epoch 600, val loss: 0.7741077542304993
Epoch 610, training loss: 12.448875427246094 = 0.33806756138801575 + 2.0 * 6.055403709411621
Epoch 610, val loss: 0.7707611918449402
Epoch 620, training loss: 12.437298774719238 = 0.32401949167251587 + 2.0 * 6.056639671325684
Epoch 620, val loss: 0.767889678478241
Epoch 630, training loss: 12.419122695922852 = 0.3103906810283661 + 2.0 * 6.054366111755371
Epoch 630, val loss: 0.7655893564224243
Epoch 640, training loss: 12.404669761657715 = 0.29707375168800354 + 2.0 * 6.053798198699951
Epoch 640, val loss: 0.7635599970817566
Epoch 650, training loss: 12.386856079101562 = 0.2841655910015106 + 2.0 * 6.051345348358154
Epoch 650, val loss: 0.7619983553886414
Epoch 660, training loss: 12.373537063598633 = 0.27153509855270386 + 2.0 * 6.051001071929932
Epoch 660, val loss: 0.7608899474143982
Epoch 670, training loss: 12.35939884185791 = 0.259209543466568 + 2.0 * 6.0500946044921875
Epoch 670, val loss: 0.760254442691803
Epoch 680, training loss: 12.342448234558105 = 0.2471892237663269 + 2.0 * 6.047629356384277
Epoch 680, val loss: 0.7600352168083191
Epoch 690, training loss: 12.328242301940918 = 0.23542460799217224 + 2.0 * 6.046408653259277
Epoch 690, val loss: 0.7601849436759949
Epoch 700, training loss: 12.327805519104004 = 0.22393976151943207 + 2.0 * 6.0519328117370605
Epoch 700, val loss: 0.7609596848487854
Epoch 710, training loss: 12.308096885681152 = 0.2127503752708435 + 2.0 * 6.047673225402832
Epoch 710, val loss: 0.7619665265083313
Epoch 720, training loss: 12.291898727416992 = 0.2018154412508011 + 2.0 * 6.045041561126709
Epoch 720, val loss: 0.7633563876152039
Epoch 730, training loss: 12.275801658630371 = 0.19121192395687103 + 2.0 * 6.042294979095459
Epoch 730, val loss: 0.7651644349098206
Epoch 740, training loss: 12.266757011413574 = 0.1808263063430786 + 2.0 * 6.042965412139893
Epoch 740, val loss: 0.7673104405403137
Epoch 750, training loss: 12.25201416015625 = 0.17073370516300201 + 2.0 * 6.040640354156494
Epoch 750, val loss: 0.770041286945343
Epoch 760, training loss: 12.240524291992188 = 0.16094642877578735 + 2.0 * 6.039788722991943
Epoch 760, val loss: 0.7729628682136536
Epoch 770, training loss: 12.228896141052246 = 0.15133604407310486 + 2.0 * 6.038780212402344
Epoch 770, val loss: 0.7762526869773865
Epoch 780, training loss: 12.228575706481934 = 0.1420743316411972 + 2.0 * 6.043250560760498
Epoch 780, val loss: 0.7800905108451843
Epoch 790, training loss: 12.216705322265625 = 0.1335965394973755 + 2.0 * 6.0415544509887695
Epoch 790, val loss: 0.7841921448707581
Epoch 800, training loss: 12.201438903808594 = 0.1259731948375702 + 2.0 * 6.03773307800293
Epoch 800, val loss: 0.7888535261154175
Epoch 810, training loss: 12.190750122070312 = 0.1189916804432869 + 2.0 * 6.035879135131836
Epoch 810, val loss: 0.7937695980072021
Epoch 820, training loss: 12.183638572692871 = 0.11260921508073807 + 2.0 * 6.035514831542969
Epoch 820, val loss: 0.799257755279541
Epoch 830, training loss: 12.177379608154297 = 0.1066652461886406 + 2.0 * 6.0353569984436035
Epoch 830, val loss: 0.8047677874565125
Epoch 840, training loss: 12.166926383972168 = 0.1011403277516365 + 2.0 * 6.032893180847168
Epoch 840, val loss: 0.8105404376983643
Epoch 850, training loss: 12.170452117919922 = 0.09598872810602188 + 2.0 * 6.037231922149658
Epoch 850, val loss: 0.8162803649902344
Epoch 860, training loss: 12.158992767333984 = 0.09123280644416809 + 2.0 * 6.03387975692749
Epoch 860, val loss: 0.8223690986633301
Epoch 870, training loss: 12.151115417480469 = 0.08677118271589279 + 2.0 * 6.032172203063965
Epoch 870, val loss: 0.828291654586792
Epoch 880, training loss: 12.150437355041504 = 0.08262550830841064 + 2.0 * 6.033905982971191
Epoch 880, val loss: 0.8345397114753723
Epoch 890, training loss: 12.139359474182129 = 0.07875515520572662 + 2.0 * 6.030302047729492
Epoch 890, val loss: 0.8407934308052063
Epoch 900, training loss: 12.136373519897461 = 0.0751146525144577 + 2.0 * 6.030629634857178
Epoch 900, val loss: 0.8469197154045105
Epoch 910, training loss: 12.129938125610352 = 0.07172862440347672 + 2.0 * 6.029104709625244
Epoch 910, val loss: 0.8532990217208862
Epoch 920, training loss: 12.12661075592041 = 0.06854669004678726 + 2.0 * 6.029032230377197
Epoch 920, val loss: 0.8594753742218018
Epoch 930, training loss: 12.118736267089844 = 0.06557271629571915 + 2.0 * 6.026581764221191
Epoch 930, val loss: 0.8656172752380371
Epoch 940, training loss: 12.113898277282715 = 0.0627780482172966 + 2.0 * 6.025559902191162
Epoch 940, val loss: 0.8718782067298889
Epoch 950, training loss: 12.114805221557617 = 0.06013951823115349 + 2.0 * 6.027332782745361
Epoch 950, val loss: 0.8781413435935974
Epoch 960, training loss: 12.10681438446045 = 0.05765174329280853 + 2.0 * 6.024581432342529
Epoch 960, val loss: 0.8843948245048523
Epoch 970, training loss: 12.110296249389648 = 0.05531560629606247 + 2.0 * 6.027490139007568
Epoch 970, val loss: 0.8903760313987732
Epoch 980, training loss: 12.100262641906738 = 0.053127337247133255 + 2.0 * 6.0235676765441895
Epoch 980, val loss: 0.8964260816574097
Epoch 990, training loss: 12.09572982788086 = 0.051055584102869034 + 2.0 * 6.022336959838867
Epoch 990, val loss: 0.9024147987365723
Epoch 1000, training loss: 12.106367111206055 = 0.04909134283661842 + 2.0 * 6.028637886047363
Epoch 1000, val loss: 0.9083315134048462
Epoch 1010, training loss: 12.095437049865723 = 0.04724932834506035 + 2.0 * 6.0240936279296875
Epoch 1010, val loss: 0.9143244624137878
Epoch 1020, training loss: 12.088776588439941 = 0.04548824578523636 + 2.0 * 6.021644115447998
Epoch 1020, val loss: 0.920011043548584
Epoch 1030, training loss: 12.0861234664917 = 0.04382869601249695 + 2.0 * 6.02114725112915
Epoch 1030, val loss: 0.9257596135139465
Epoch 1040, training loss: 12.086371421813965 = 0.04224952310323715 + 2.0 * 6.022060871124268
Epoch 1040, val loss: 0.9315189719200134
Epoch 1050, training loss: 12.079032897949219 = 0.0407586507499218 + 2.0 * 6.019136905670166
Epoch 1050, val loss: 0.9371358156204224
Epoch 1060, training loss: 12.077288627624512 = 0.03934172913432121 + 2.0 * 6.018973350524902
Epoch 1060, val loss: 0.9426959753036499
Epoch 1070, training loss: 12.084449768066406 = 0.03799271956086159 + 2.0 * 6.023228645324707
Epoch 1070, val loss: 0.9482868313789368
Epoch 1080, training loss: 12.072860717773438 = 0.036717794835567474 + 2.0 * 6.01807165145874
Epoch 1080, val loss: 0.9537134170532227
Epoch 1090, training loss: 12.068757057189941 = 0.03550069034099579 + 2.0 * 6.016628265380859
Epoch 1090, val loss: 0.9590186476707458
Epoch 1100, training loss: 12.071320533752441 = 0.03433863818645477 + 2.0 * 6.018490791320801
Epoch 1100, val loss: 0.9643288850784302
Epoch 1110, training loss: 12.066398620605469 = 0.03323523700237274 + 2.0 * 6.0165815353393555
Epoch 1110, val loss: 0.9697725772857666
Epoch 1120, training loss: 12.063079833984375 = 0.03218455985188484 + 2.0 * 6.015447616577148
Epoch 1120, val loss: 0.9748546481132507
Epoch 1130, training loss: 12.075769424438477 = 0.03118412010371685 + 2.0 * 6.022292613983154
Epoch 1130, val loss: 0.9800220727920532
Epoch 1140, training loss: 12.061702728271484 = 0.030228059738874435 + 2.0 * 6.015737533569336
Epoch 1140, val loss: 0.9851503968238831
Epoch 1150, training loss: 12.056907653808594 = 0.029317080974578857 + 2.0 * 6.013795375823975
Epoch 1150, val loss: 0.9900304675102234
Epoch 1160, training loss: 12.065131187438965 = 0.028448285534977913 + 2.0 * 6.018341541290283
Epoch 1160, val loss: 0.9950266480445862
Epoch 1170, training loss: 12.054510116577148 = 0.027605518698692322 + 2.0 * 6.013452529907227
Epoch 1170, val loss: 0.9999597668647766
Epoch 1180, training loss: 12.055238723754883 = 0.0268073920160532 + 2.0 * 6.014215469360352
Epoch 1180, val loss: 1.0047625303268433
Epoch 1190, training loss: 12.053486824035645 = 0.02604232355952263 + 2.0 * 6.0137224197387695
Epoch 1190, val loss: 1.0095770359039307
Epoch 1200, training loss: 12.05267333984375 = 0.02531673014163971 + 2.0 * 6.013678073883057
Epoch 1200, val loss: 1.0142974853515625
Epoch 1210, training loss: 12.0469970703125 = 0.024610232561826706 + 2.0 * 6.01119327545166
Epoch 1210, val loss: 1.0189533233642578
Epoch 1220, training loss: 12.04502010345459 = 0.023937087506055832 + 2.0 * 6.0105414390563965
Epoch 1220, val loss: 1.0235363245010376
Epoch 1230, training loss: 12.051581382751465 = 0.02328789234161377 + 2.0 * 6.01414680480957
Epoch 1230, val loss: 1.0281556844711304
Epoch 1240, training loss: 12.04661750793457 = 0.022672029212117195 + 2.0 * 6.011972904205322
Epoch 1240, val loss: 1.0328401327133179
Epoch 1250, training loss: 12.043134689331055 = 0.022074418142437935 + 2.0 * 6.0105299949646
Epoch 1250, val loss: 1.0371041297912598
Epoch 1260, training loss: 12.045758247375488 = 0.021502485498785973 + 2.0 * 6.012127876281738
Epoch 1260, val loss: 1.0414515733718872
Epoch 1270, training loss: 12.037968635559082 = 0.02095334418118 + 2.0 * 6.00850772857666
Epoch 1270, val loss: 1.0458452701568604
Epoch 1280, training loss: 12.036421775817871 = 0.02042633295059204 + 2.0 * 6.007997512817383
Epoch 1280, val loss: 1.050120234489441
Epoch 1290, training loss: 12.042969703674316 = 0.019916318356990814 + 2.0 * 6.011526584625244
Epoch 1290, val loss: 1.0542888641357422
Epoch 1300, training loss: 12.037738800048828 = 0.019427910447120667 + 2.0 * 6.0091552734375
Epoch 1300, val loss: 1.0586503744125366
Epoch 1310, training loss: 12.037463188171387 = 0.01895746774971485 + 2.0 * 6.009253025054932
Epoch 1310, val loss: 1.0626894235610962
Epoch 1320, training loss: 12.033113479614258 = 0.018503662198781967 + 2.0 * 6.007305145263672
Epoch 1320, val loss: 1.0667437314987183
Epoch 1330, training loss: 12.031556129455566 = 0.018067512661218643 + 2.0 * 6.006744384765625
Epoch 1330, val loss: 1.0707896947860718
Epoch 1340, training loss: 12.031805038452148 = 0.017646200954914093 + 2.0 * 6.007079601287842
Epoch 1340, val loss: 1.074791669845581
Epoch 1350, training loss: 12.030722618103027 = 0.017237434163689613 + 2.0 * 6.006742477416992
Epoch 1350, val loss: 1.0787465572357178
Epoch 1360, training loss: 12.030912399291992 = 0.01684229075908661 + 2.0 * 6.007035255432129
Epoch 1360, val loss: 1.082651972770691
Epoch 1370, training loss: 12.031314849853516 = 0.016466941684484482 + 2.0 * 6.0074238777160645
Epoch 1370, val loss: 1.0865544080734253
Epoch 1380, training loss: 12.03140926361084 = 0.016100162640213966 + 2.0 * 6.007654666900635
Epoch 1380, val loss: 1.0902431011199951
Epoch 1390, training loss: 12.02491569519043 = 0.015746576711535454 + 2.0 * 6.004584789276123
Epoch 1390, val loss: 1.0941411256790161
Epoch 1400, training loss: 12.024335861206055 = 0.015407242812216282 + 2.0 * 6.004464149475098
Epoch 1400, val loss: 1.0977331399917603
Epoch 1410, training loss: 12.025040626525879 = 0.015077550895512104 + 2.0 * 6.004981517791748
Epoch 1410, val loss: 1.1014952659606934
Epoch 1420, training loss: 12.02078914642334 = 0.014756040647625923 + 2.0 * 6.003016471862793
Epoch 1420, val loss: 1.1051409244537354
Epoch 1430, training loss: 12.022684097290039 = 0.014445952139794827 + 2.0 * 6.004118919372559
Epoch 1430, val loss: 1.1087291240692139
Epoch 1440, training loss: 12.022964477539062 = 0.014148649759590626 + 2.0 * 6.00440788269043
Epoch 1440, val loss: 1.112362027168274
Epoch 1450, training loss: 12.025773048400879 = 0.013857840560376644 + 2.0 * 6.00595760345459
Epoch 1450, val loss: 1.1159170866012573
Epoch 1460, training loss: 12.018250465393066 = 0.013574490323662758 + 2.0 * 6.00233793258667
Epoch 1460, val loss: 1.119322657585144
Epoch 1470, training loss: 12.016115188598633 = 0.013303942047059536 + 2.0 * 6.001405715942383
Epoch 1470, val loss: 1.1227562427520752
Epoch 1480, training loss: 12.021703720092773 = 0.013039779849350452 + 2.0 * 6.004332065582275
Epoch 1480, val loss: 1.126080870628357
Epoch 1490, training loss: 12.014067649841309 = 0.012785594910383224 + 2.0 * 6.000640869140625
Epoch 1490, val loss: 1.129582166671753
Epoch 1500, training loss: 12.013971328735352 = 0.012538804672658443 + 2.0 * 6.000716209411621
Epoch 1500, val loss: 1.1328669786453247
Epoch 1510, training loss: 12.017849922180176 = 0.012298397719860077 + 2.0 * 6.0027756690979
Epoch 1510, val loss: 1.1361613273620605
Epoch 1520, training loss: 12.014355659484863 = 0.012064717710018158 + 2.0 * 6.001145362854004
Epoch 1520, val loss: 1.1395853757858276
Epoch 1530, training loss: 12.022298812866211 = 0.011841260828077793 + 2.0 * 6.0052289962768555
Epoch 1530, val loss: 1.1428548097610474
Epoch 1540, training loss: 12.010676383972168 = 0.011621121317148209 + 2.0 * 5.999527454376221
Epoch 1540, val loss: 1.1460376977920532
Epoch 1550, training loss: 12.009100914001465 = 0.011407451704144478 + 2.0 * 5.998846530914307
Epoch 1550, val loss: 1.1490845680236816
Epoch 1560, training loss: 12.010156631469727 = 0.011201760731637478 + 2.0 * 5.999477386474609
Epoch 1560, val loss: 1.152238368988037
Epoch 1570, training loss: 12.012606620788574 = 0.011001355946063995 + 2.0 * 6.000802516937256
Epoch 1570, val loss: 1.155492901802063
Epoch 1580, training loss: 12.009257316589355 = 0.010805543512105942 + 2.0 * 5.999226093292236
Epoch 1580, val loss: 1.1585748195648193
Epoch 1590, training loss: 12.013886451721191 = 0.010617504827678204 + 2.0 * 6.00163459777832
Epoch 1590, val loss: 1.1616448163986206
Epoch 1600, training loss: 12.006601333618164 = 0.010431315749883652 + 2.0 * 5.998085021972656
Epoch 1600, val loss: 1.1646525859832764
Epoch 1610, training loss: 12.006291389465332 = 0.010251645930111408 + 2.0 * 5.998019695281982
Epoch 1610, val loss: 1.1676974296569824
Epoch 1620, training loss: 12.012308120727539 = 0.010078729130327702 + 2.0 * 6.001114845275879
Epoch 1620, val loss: 1.1707093715667725
Epoch 1630, training loss: 12.00908374786377 = 0.009905249811708927 + 2.0 * 5.999589443206787
Epoch 1630, val loss: 1.1735581159591675
Epoch 1640, training loss: 12.003636360168457 = 0.009741636924445629 + 2.0 * 5.996947288513184
Epoch 1640, val loss: 1.1764013767242432
Epoch 1650, training loss: 12.003726959228516 = 0.009580252692103386 + 2.0 * 5.997073173522949
Epoch 1650, val loss: 1.1792134046554565
Epoch 1660, training loss: 12.005938529968262 = 0.009423140436410904 + 2.0 * 5.998257637023926
Epoch 1660, val loss: 1.1820378303527832
Epoch 1670, training loss: 11.999899864196777 = 0.009270651265978813 + 2.0 * 5.995314598083496
Epoch 1670, val loss: 1.1850143671035767
Epoch 1680, training loss: 12.004008293151855 = 0.009121416136622429 + 2.0 * 5.997443675994873
Epoch 1680, val loss: 1.187678337097168
Epoch 1690, training loss: 12.004976272583008 = 0.008974934928119183 + 2.0 * 5.998000621795654
Epoch 1690, val loss: 1.1905368566513062
Epoch 1700, training loss: 12.001651763916016 = 0.008834287524223328 + 2.0 * 5.996408939361572
Epoch 1700, val loss: 1.1934152841567993
Epoch 1710, training loss: 11.99733829498291 = 0.008696464821696281 + 2.0 * 5.994320869445801
Epoch 1710, val loss: 1.1960216760635376
Epoch 1720, training loss: 11.99806022644043 = 0.008561601862311363 + 2.0 * 5.994749546051025
Epoch 1720, val loss: 1.1986429691314697
Epoch 1730, training loss: 12.005608558654785 = 0.008430314250290394 + 2.0 * 5.998589038848877
Epoch 1730, val loss: 1.2013897895812988
Epoch 1740, training loss: 12.000080108642578 = 0.008301140740513802 + 2.0 * 5.995889663696289
Epoch 1740, val loss: 1.2040244340896606
Epoch 1750, training loss: 11.998172760009766 = 0.008176403120160103 + 2.0 * 5.994997978210449
Epoch 1750, val loss: 1.2066618204116821
Epoch 1760, training loss: 12.001405715942383 = 0.008054506033658981 + 2.0 * 5.996675491333008
Epoch 1760, val loss: 1.2091906070709229
Epoch 1770, training loss: 11.996769905090332 = 0.007937006652355194 + 2.0 * 5.994416236877441
Epoch 1770, val loss: 1.211881160736084
Epoch 1780, training loss: 11.997739791870117 = 0.007820622064173222 + 2.0 * 5.994959354400635
Epoch 1780, val loss: 1.214339256286621
Epoch 1790, training loss: 11.995806694030762 = 0.007706017699092627 + 2.0 * 5.9940505027771
Epoch 1790, val loss: 1.2169157266616821
Epoch 1800, training loss: 11.99584674835205 = 0.007596034090965986 + 2.0 * 5.9941253662109375
Epoch 1800, val loss: 1.2194273471832275
Epoch 1810, training loss: 11.992371559143066 = 0.0074878837913274765 + 2.0 * 5.992441654205322
Epoch 1810, val loss: 1.2219353914260864
Epoch 1820, training loss: 11.995850563049316 = 0.0073814671486616135 + 2.0 * 5.994234561920166
Epoch 1820, val loss: 1.2244292497634888
Epoch 1830, training loss: 11.99753189086914 = 0.007277295924723148 + 2.0 * 5.995127201080322
Epoch 1830, val loss: 1.226927638053894
Epoch 1840, training loss: 11.995821952819824 = 0.007176484912633896 + 2.0 * 5.994322776794434
Epoch 1840, val loss: 1.22950279712677
Epoch 1850, training loss: 11.990936279296875 = 0.00707896938547492 + 2.0 * 5.991928577423096
Epoch 1850, val loss: 1.2317168712615967
Epoch 1860, training loss: 11.989154815673828 = 0.006982356775552034 + 2.0 * 5.991086006164551
Epoch 1860, val loss: 1.2341268062591553
Epoch 1870, training loss: 11.995478630065918 = 0.00688741821795702 + 2.0 * 5.994295597076416
Epoch 1870, val loss: 1.2364208698272705
Epoch 1880, training loss: 11.990655899047852 = 0.006794965825974941 + 2.0 * 5.9919304847717285
Epoch 1880, val loss: 1.2389873266220093
Epoch 1890, training loss: 11.991242408752441 = 0.006704231258481741 + 2.0 * 5.992269039154053
Epoch 1890, val loss: 1.2411532402038574
Epoch 1900, training loss: 11.990530967712402 = 0.006616048980504274 + 2.0 * 5.991957664489746
Epoch 1900, val loss: 1.243481159210205
Epoch 1910, training loss: 11.989008903503418 = 0.006530578248202801 + 2.0 * 5.991239070892334
Epoch 1910, val loss: 1.245850682258606
Epoch 1920, training loss: 11.992838859558105 = 0.006446515675634146 + 2.0 * 5.9931960105896
Epoch 1920, val loss: 1.248113751411438
Epoch 1930, training loss: 11.988941192626953 = 0.006363282445818186 + 2.0 * 5.991289138793945
Epoch 1930, val loss: 1.2504181861877441
Epoch 1940, training loss: 11.988113403320312 = 0.0062819551676511765 + 2.0 * 5.990915775299072
Epoch 1940, val loss: 1.252666711807251
Epoch 1950, training loss: 11.989880561828613 = 0.006201912648975849 + 2.0 * 5.991839408874512
Epoch 1950, val loss: 1.254859447479248
Epoch 1960, training loss: 11.989749908447266 = 0.006123531609773636 + 2.0 * 5.9918131828308105
Epoch 1960, val loss: 1.257032871246338
Epoch 1970, training loss: 11.98536491394043 = 0.006048501469194889 + 2.0 * 5.989658355712891
Epoch 1970, val loss: 1.259360432624817
Epoch 1980, training loss: 11.992194175720215 = 0.005974002182483673 + 2.0 * 5.993110179901123
Epoch 1980, val loss: 1.261454463005066
Epoch 1990, training loss: 11.98596477508545 = 0.005900659132748842 + 2.0 * 5.990032196044922
Epoch 1990, val loss: 1.2636020183563232
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8148
Overall ASR: 0.8376
Flip ASR: 0.8044/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.70023536682129 = 1.9524857997894287 + 2.0 * 8.37387466430664
Epoch 0, val loss: 1.9521746635437012
Epoch 10, training loss: 18.686058044433594 = 1.9410489797592163 + 2.0 * 8.372504234313965
Epoch 10, val loss: 1.9394783973693848
Epoch 20, training loss: 18.661911010742188 = 1.9264531135559082 + 2.0 * 8.367729187011719
Epoch 20, val loss: 1.9225834608078003
Epoch 30, training loss: 18.5908203125 = 1.9064770936965942 + 2.0 * 8.342171669006348
Epoch 30, val loss: 1.899454116821289
Epoch 40, training loss: 18.25148582458496 = 1.8834803104400635 + 2.0 * 8.184002876281738
Epoch 40, val loss: 1.8739663362503052
Epoch 50, training loss: 17.254579544067383 = 1.8626484870910645 + 2.0 * 7.695965766906738
Epoch 50, val loss: 1.850903868675232
Epoch 60, training loss: 16.306461334228516 = 1.844288945198059 + 2.0 * 7.231085777282715
Epoch 60, val loss: 1.8320059776306152
Epoch 70, training loss: 15.586150169372559 = 1.82677161693573 + 2.0 * 6.8796892166137695
Epoch 70, val loss: 1.8150032758712769
Epoch 80, training loss: 15.18005657196045 = 1.8134660720825195 + 2.0 * 6.683295249938965
Epoch 80, val loss: 1.8025896549224854
Epoch 90, training loss: 14.928738594055176 = 1.8011637926101685 + 2.0 * 6.563787460327148
Epoch 90, val loss: 1.790989637374878
Epoch 100, training loss: 14.752970695495605 = 1.7877930402755737 + 2.0 * 6.482588768005371
Epoch 100, val loss: 1.7789479494094849
Epoch 110, training loss: 14.623233795166016 = 1.773686170578003 + 2.0 * 6.424773693084717
Epoch 110, val loss: 1.7663661241531372
Epoch 120, training loss: 14.507926940917969 = 1.7593913078308105 + 2.0 * 6.374268054962158
Epoch 120, val loss: 1.7536293268203735
Epoch 130, training loss: 14.409249305725098 = 1.7447572946548462 + 2.0 * 6.332245826721191
Epoch 130, val loss: 1.7407628297805786
Epoch 140, training loss: 14.332328796386719 = 1.7290644645690918 + 2.0 * 6.301631927490234
Epoch 140, val loss: 1.7269889116287231
Epoch 150, training loss: 14.262134552001953 = 1.7117524147033691 + 2.0 * 6.275191307067871
Epoch 150, val loss: 1.7120387554168701
Epoch 160, training loss: 14.199716567993164 = 1.6923590898513794 + 2.0 * 6.253678798675537
Epoch 160, val loss: 1.6956380605697632
Epoch 170, training loss: 14.143091201782227 = 1.6704000234603882 + 2.0 * 6.2363457679748535
Epoch 170, val loss: 1.6774991750717163
Epoch 180, training loss: 14.087506294250488 = 1.645725131034851 + 2.0 * 6.220890522003174
Epoch 180, val loss: 1.657345175743103
Epoch 190, training loss: 14.034502983093262 = 1.6176456212997437 + 2.0 * 6.208428859710693
Epoch 190, val loss: 1.634779691696167
Epoch 200, training loss: 13.985668182373047 = 1.5858793258666992 + 2.0 * 6.199894428253174
Epoch 200, val loss: 1.6095285415649414
Epoch 210, training loss: 13.933526992797852 = 1.5504186153411865 + 2.0 * 6.191554069519043
Epoch 210, val loss: 1.5818406343460083
Epoch 220, training loss: 13.870418548583984 = 1.5120391845703125 + 2.0 * 6.179189682006836
Epoch 220, val loss: 1.5519485473632812
Epoch 230, training loss: 13.81328010559082 = 1.470637559890747 + 2.0 * 6.171321392059326
Epoch 230, val loss: 1.5202078819274902
Epoch 240, training loss: 13.755059242248535 = 1.426845669746399 + 2.0 * 6.164106845855713
Epoch 240, val loss: 1.4872756004333496
Epoch 250, training loss: 13.698165893554688 = 1.3816337585449219 + 2.0 * 6.158266067504883
Epoch 250, val loss: 1.4538298845291138
Epoch 260, training loss: 13.647138595581055 = 1.3361784219741821 + 2.0 * 6.155479907989502
Epoch 260, val loss: 1.420983076095581
Epoch 270, training loss: 13.585041046142578 = 1.2919734716415405 + 2.0 * 6.146533966064453
Epoch 270, val loss: 1.3895190954208374
Epoch 280, training loss: 13.532500267028809 = 1.2491400241851807 + 2.0 * 6.1416802406311035
Epoch 280, val loss: 1.3596835136413574
Epoch 290, training loss: 13.48448657989502 = 1.207883596420288 + 2.0 * 6.138301372528076
Epoch 290, val loss: 1.331514596939087
Epoch 300, training loss: 13.44146728515625 = 1.168271541595459 + 2.0 * 6.136598110198975
Epoch 300, val loss: 1.3049291372299194
Epoch 310, training loss: 13.38952922821045 = 1.1308879852294922 + 2.0 * 6.1293206214904785
Epoch 310, val loss: 1.2797503471374512
Epoch 320, training loss: 13.345300674438477 = 1.0949032306671143 + 2.0 * 6.125198841094971
Epoch 320, val loss: 1.2555879354476929
Epoch 330, training loss: 13.306329727172852 = 1.060009241104126 + 2.0 * 6.123160362243652
Epoch 330, val loss: 1.2319824695587158
Epoch 340, training loss: 13.263498306274414 = 1.0259037017822266 + 2.0 * 6.118797302246094
Epoch 340, val loss: 1.2087894678115845
Epoch 350, training loss: 13.227994918823242 = 0.9927222728729248 + 2.0 * 6.117636203765869
Epoch 350, val loss: 1.185731291770935
Epoch 360, training loss: 13.183512687683105 = 0.9599374532699585 + 2.0 * 6.111787796020508
Epoch 360, val loss: 1.162873387336731
Epoch 370, training loss: 13.143993377685547 = 0.9276485443115234 + 2.0 * 6.108172416687012
Epoch 370, val loss: 1.1400656700134277
Epoch 380, training loss: 13.109424591064453 = 0.8956805467605591 + 2.0 * 6.106872081756592
Epoch 380, val loss: 1.1172642707824707
Epoch 390, training loss: 13.071039199829102 = 0.8639400601387024 + 2.0 * 6.103549480438232
Epoch 390, val loss: 1.0946635007858276
Epoch 400, training loss: 13.031294822692871 = 0.8324849009513855 + 2.0 * 6.099404811859131
Epoch 400, val loss: 1.0720535516738892
Epoch 410, training loss: 13.010031700134277 = 0.8009929060935974 + 2.0 * 6.104519367218018
Epoch 410, val loss: 1.0494507551193237
Epoch 420, training loss: 12.960089683532715 = 0.7700091004371643 + 2.0 * 6.095040321350098
Epoch 420, val loss: 1.0270342826843262
Epoch 430, training loss: 12.922389030456543 = 0.7390769124031067 + 2.0 * 6.09165620803833
Epoch 430, val loss: 1.0049318075180054
Epoch 440, training loss: 12.886845588684082 = 0.7082947492599487 + 2.0 * 6.089275360107422
Epoch 440, val loss: 0.9830676913261414
Epoch 450, training loss: 12.861595153808594 = 0.6776614785194397 + 2.0 * 6.09196662902832
Epoch 450, val loss: 0.9615845680236816
Epoch 460, training loss: 12.818288803100586 = 0.6474549174308777 + 2.0 * 6.085416793823242
Epoch 460, val loss: 0.940736711025238
Epoch 470, training loss: 12.789831161499023 = 0.6177310347557068 + 2.0 * 6.086050033569336
Epoch 470, val loss: 0.9207060933113098
Epoch 480, training loss: 12.759681701660156 = 0.5885691046714783 + 2.0 * 6.085556507110596
Epoch 480, val loss: 0.9015509486198425
Epoch 490, training loss: 12.72509765625 = 0.5603458285331726 + 2.0 * 6.082376003265381
Epoch 490, val loss: 0.8833475112915039
Epoch 500, training loss: 12.68847370147705 = 0.5326891541481018 + 2.0 * 6.077892303466797
Epoch 500, val loss: 0.86640465259552
Epoch 510, training loss: 12.657100677490234 = 0.5058196187019348 + 2.0 * 6.075640678405762
Epoch 510, val loss: 0.8505592346191406
Epoch 520, training loss: 12.62759780883789 = 0.4796140193939209 + 2.0 * 6.073991775512695
Epoch 520, val loss: 0.8358034491539001
Epoch 530, training loss: 12.602511405944824 = 0.45417076349258423 + 2.0 * 6.074170112609863
Epoch 530, val loss: 0.8220977783203125
Epoch 540, training loss: 12.582588195800781 = 0.42951878905296326 + 2.0 * 6.076534748077393
Epoch 540, val loss: 0.80938321352005
Epoch 550, training loss: 12.54848861694336 = 0.406035453081131 + 2.0 * 6.071226596832275
Epoch 550, val loss: 0.7978370785713196
Epoch 560, training loss: 12.531838417053223 = 0.38352036476135254 + 2.0 * 6.074159145355225
Epoch 560, val loss: 0.7873618602752686
Epoch 570, training loss: 12.505208969116211 = 0.3618966341018677 + 2.0 * 6.071656227111816
Epoch 570, val loss: 0.7778295874595642
Epoch 580, training loss: 12.472517967224121 = 0.3414042592048645 + 2.0 * 6.06555700302124
Epoch 580, val loss: 0.769360363483429
Epoch 590, training loss: 12.45168399810791 = 0.32181376218795776 + 2.0 * 6.064935207366943
Epoch 590, val loss: 0.7618172764778137
Epoch 600, training loss: 12.441322326660156 = 0.3030886650085449 + 2.0 * 6.069117069244385
Epoch 600, val loss: 0.7551868557929993
Epoch 610, training loss: 12.408571243286133 = 0.2854366600513458 + 2.0 * 6.061567306518555
Epoch 610, val loss: 0.7493997812271118
Epoch 620, training loss: 12.39201545715332 = 0.26875540614128113 + 2.0 * 6.0616302490234375
Epoch 620, val loss: 0.7445749640464783
Epoch 630, training loss: 12.372278213500977 = 0.2529424726963043 + 2.0 * 6.059668064117432
Epoch 630, val loss: 0.7405665516853333
Epoch 640, training loss: 12.355061531066895 = 0.23806220293045044 + 2.0 * 6.058499813079834
Epoch 640, val loss: 0.7373231053352356
Epoch 650, training loss: 12.34260368347168 = 0.22415965795516968 + 2.0 * 6.059222221374512
Epoch 650, val loss: 0.7348855137825012
Epoch 660, training loss: 12.3294677734375 = 0.21115797758102417 + 2.0 * 6.059154987335205
Epoch 660, val loss: 0.7332301735877991
Epoch 670, training loss: 12.308156967163086 = 0.1990603804588318 + 2.0 * 6.054548263549805
Epoch 670, val loss: 0.7322302460670471
Epoch 680, training loss: 12.298531532287598 = 0.18778890371322632 + 2.0 * 6.055371284484863
Epoch 680, val loss: 0.731978178024292
Epoch 690, training loss: 12.290094375610352 = 0.17726930975914001 + 2.0 * 6.056412696838379
Epoch 690, val loss: 0.7323207855224609
Epoch 700, training loss: 12.273416519165039 = 0.16757027804851532 + 2.0 * 6.052923202514648
Epoch 700, val loss: 0.7331582307815552
Epoch 710, training loss: 12.262762069702148 = 0.15857891738414764 + 2.0 * 6.052091598510742
Epoch 710, val loss: 0.7345501780509949
Epoch 720, training loss: 12.248735427856445 = 0.15023253858089447 + 2.0 * 6.049251556396484
Epoch 720, val loss: 0.7365018725395203
Epoch 730, training loss: 12.238868713378906 = 0.14245545864105225 + 2.0 * 6.048206806182861
Epoch 730, val loss: 0.7388582825660706
Epoch 740, training loss: 12.24340534210205 = 0.13520579040050507 + 2.0 * 6.0540995597839355
Epoch 740, val loss: 0.7415856122970581
Epoch 750, training loss: 12.229467391967773 = 0.12849009037017822 + 2.0 * 6.050488471984863
Epoch 750, val loss: 0.7446187734603882
Epoch 760, training loss: 12.214335441589355 = 0.12226195633411407 + 2.0 * 6.046036720275879
Epoch 760, val loss: 0.7479693293571472
Epoch 770, training loss: 12.215448379516602 = 0.11647549271583557 + 2.0 * 6.0494866371154785
Epoch 770, val loss: 0.7516651749610901
Epoch 780, training loss: 12.20523452758789 = 0.11101745814085007 + 2.0 * 6.0471086502075195
Epoch 780, val loss: 0.7554413676261902
Epoch 790, training loss: 12.192095756530762 = 0.10599610954523087 + 2.0 * 6.0430498123168945
Epoch 790, val loss: 0.7595719695091248
Epoch 800, training loss: 12.183899879455566 = 0.1012641042470932 + 2.0 * 6.041317939758301
Epoch 800, val loss: 0.7639574408531189
Epoch 810, training loss: 12.186967849731445 = 0.09682536125183105 + 2.0 * 6.045071125030518
Epoch 810, val loss: 0.7685446739196777
Epoch 820, training loss: 12.176915168762207 = 0.09264006465673447 + 2.0 * 6.042137622833252
Epoch 820, val loss: 0.7730206251144409
Epoch 830, training loss: 12.167034149169922 = 0.08874069154262543 + 2.0 * 6.039146900177002
Epoch 830, val loss: 0.7778213024139404
Epoch 840, training loss: 12.160420417785645 = 0.08505962789058685 + 2.0 * 6.037680625915527
Epoch 840, val loss: 0.7827208638191223
Epoch 850, training loss: 12.162821769714355 = 0.08158774673938751 + 2.0 * 6.040616989135742
Epoch 850, val loss: 0.7876832485198975
Epoch 860, training loss: 12.16008186340332 = 0.07830722630023956 + 2.0 * 6.040887355804443
Epoch 860, val loss: 0.7926231622695923
Epoch 870, training loss: 12.148058891296387 = 0.07522652298212051 + 2.0 * 6.036416053771973
Epoch 870, val loss: 0.7976559400558472
Epoch 880, training loss: 12.146369934082031 = 0.07232654094696045 + 2.0 * 6.037021636962891
Epoch 880, val loss: 0.8027743697166443
Epoch 890, training loss: 12.137999534606934 = 0.06956393271684647 + 2.0 * 6.034217834472656
Epoch 890, val loss: 0.8078761696815491
Epoch 900, training loss: 12.13382339477539 = 0.06693805754184723 + 2.0 * 6.033442497253418
Epoch 900, val loss: 0.8131087422370911
Epoch 910, training loss: 12.1397705078125 = 0.06444597244262695 + 2.0 * 6.037662506103516
Epoch 910, val loss: 0.818325936794281
Epoch 920, training loss: 12.133379936218262 = 0.06211290508508682 + 2.0 * 6.035633563995361
Epoch 920, val loss: 0.8233599066734314
Epoch 930, training loss: 12.127962112426758 = 0.05987633392214775 + 2.0 * 6.034042835235596
Epoch 930, val loss: 0.8285418748855591
Epoch 940, training loss: 12.118213653564453 = 0.0577632412314415 + 2.0 * 6.0302252769470215
Epoch 940, val loss: 0.8337175846099854
Epoch 950, training loss: 12.116129875183105 = 0.055747684091329575 + 2.0 * 6.030190944671631
Epoch 950, val loss: 0.8389240503311157
Epoch 960, training loss: 12.119627952575684 = 0.05382005497813225 + 2.0 * 6.032904148101807
Epoch 960, val loss: 0.8440658450126648
Epoch 970, training loss: 12.1110258102417 = 0.05199289694428444 + 2.0 * 6.029516220092773
Epoch 970, val loss: 0.8492098450660706
Epoch 980, training loss: 12.108893394470215 = 0.05025210976600647 + 2.0 * 6.02932071685791
Epoch 980, val loss: 0.8542976379394531
Epoch 990, training loss: 12.104202270507812 = 0.0485864132642746 + 2.0 * 6.027807712554932
Epoch 990, val loss: 0.8594462275505066
Epoch 1000, training loss: 12.112385749816895 = 0.04700145870447159 + 2.0 * 6.032691955566406
Epoch 1000, val loss: 0.8644718527793884
Epoch 1010, training loss: 12.099884986877441 = 0.045484498143196106 + 2.0 * 6.027200222015381
Epoch 1010, val loss: 0.8695256114006042
Epoch 1020, training loss: 12.094167709350586 = 0.04403626546263695 + 2.0 * 6.025065898895264
Epoch 1020, val loss: 0.8745847344398499
Epoch 1030, training loss: 12.100591659545898 = 0.042643655091524124 + 2.0 * 6.0289740562438965
Epoch 1030, val loss: 0.8795751929283142
Epoch 1040, training loss: 12.093060493469238 = 0.04132475703954697 + 2.0 * 6.025867938995361
Epoch 1040, val loss: 0.8843119740486145
Epoch 1050, training loss: 12.087926864624023 = 0.0400552935898304 + 2.0 * 6.023935794830322
Epoch 1050, val loss: 0.8892413973808289
Epoch 1060, training loss: 12.084146499633789 = 0.038851939141750336 + 2.0 * 6.022647380828857
Epoch 1060, val loss: 0.8941439390182495
Epoch 1070, training loss: 12.087454795837402 = 0.03768797218799591 + 2.0 * 6.024883270263672
Epoch 1070, val loss: 0.8990339636802673
Epoch 1080, training loss: 12.081014633178711 = 0.036568913608789444 + 2.0 * 6.022222995758057
Epoch 1080, val loss: 0.903676450252533
Epoch 1090, training loss: 12.081168174743652 = 0.03549908474087715 + 2.0 * 6.022834777832031
Epoch 1090, val loss: 0.9084157347679138
Epoch 1100, training loss: 12.076204299926758 = 0.03448062762618065 + 2.0 * 6.020861625671387
Epoch 1100, val loss: 0.9130984544754028
Epoch 1110, training loss: 12.082503318786621 = 0.03349543735384941 + 2.0 * 6.024503707885742
Epoch 1110, val loss: 0.9177441596984863
Epoch 1120, training loss: 12.073710441589355 = 0.0325516052544117 + 2.0 * 6.0205793380737305
Epoch 1120, val loss: 0.9222316741943359
Epoch 1130, training loss: 12.077058792114258 = 0.03164895623922348 + 2.0 * 6.022705078125
Epoch 1130, val loss: 0.9267346858978271
Epoch 1140, training loss: 12.06910228729248 = 0.030779611319303513 + 2.0 * 6.019161224365234
Epoch 1140, val loss: 0.9312378764152527
Epoch 1150, training loss: 12.06711483001709 = 0.02994774840772152 + 2.0 * 6.01858377456665
Epoch 1150, val loss: 0.9356749057769775
Epoch 1160, training loss: 12.074527740478516 = 0.029141997918486595 + 2.0 * 6.022692680358887
Epoch 1160, val loss: 0.9400720596313477
Epoch 1170, training loss: 12.068549156188965 = 0.02837003767490387 + 2.0 * 6.020089626312256
Epoch 1170, val loss: 0.9443639516830444
Epoch 1180, training loss: 12.06144905090332 = 0.027634967118501663 + 2.0 * 6.016907215118408
Epoch 1180, val loss: 0.9486749768257141
Epoch 1190, training loss: 12.059194564819336 = 0.02692272700369358 + 2.0 * 6.0161356925964355
Epoch 1190, val loss: 0.9530436396598816
Epoch 1200, training loss: 12.06515121459961 = 0.026239044964313507 + 2.0 * 6.019455909729004
Epoch 1200, val loss: 0.9573366641998291
Epoch 1210, training loss: 12.058089256286621 = 0.025571290403604507 + 2.0 * 6.01625919342041
Epoch 1210, val loss: 0.96138995885849
Epoch 1220, training loss: 12.057894706726074 = 0.024936096742749214 + 2.0 * 6.0164794921875
Epoch 1220, val loss: 0.9655353426933289
Epoch 1230, training loss: 12.056417465209961 = 0.024323951452970505 + 2.0 * 6.016046524047852
Epoch 1230, val loss: 0.9696404337882996
Epoch 1240, training loss: 12.055723190307617 = 0.023729097098112106 + 2.0 * 6.015996932983398
Epoch 1240, val loss: 0.9736549854278564
Epoch 1250, training loss: 12.053613662719727 = 0.02316000498831272 + 2.0 * 6.0152268409729
Epoch 1250, val loss: 0.9775925278663635
Epoch 1260, training loss: 12.050859451293945 = 0.022610152140259743 + 2.0 * 6.014124870300293
Epoch 1260, val loss: 0.9815403819084167
Epoch 1270, training loss: 12.046792030334473 = 0.022081909701228142 + 2.0 * 6.012354850769043
Epoch 1270, val loss: 0.9854869842529297
Epoch 1280, training loss: 12.044656753540039 = 0.021569184958934784 + 2.0 * 6.0115437507629395
Epoch 1280, val loss: 0.9894586801528931
Epoch 1290, training loss: 12.044207572937012 = 0.02107134275138378 + 2.0 * 6.011568069458008
Epoch 1290, val loss: 0.9933962225914001
Epoch 1300, training loss: 12.068615913391113 = 0.020586566999554634 + 2.0 * 6.024014472961426
Epoch 1300, val loss: 0.9971137046813965
Epoch 1310, training loss: 12.044973373413086 = 0.020128464326262474 + 2.0 * 6.012422561645508
Epoch 1310, val loss: 1.0005954504013062
Epoch 1320, training loss: 12.04044246673584 = 0.01969086565077305 + 2.0 * 6.0103759765625
Epoch 1320, val loss: 1.0044043064117432
Epoch 1330, training loss: 12.03952407836914 = 0.01926136575639248 + 2.0 * 6.010131359100342
Epoch 1330, val loss: 1.0081554651260376
Epoch 1340, training loss: 12.038366317749023 = 0.01884313113987446 + 2.0 * 6.009761810302734
Epoch 1340, val loss: 1.0118253231048584
Epoch 1350, training loss: 12.050881385803223 = 0.018440451472997665 + 2.0 * 6.016220569610596
Epoch 1350, val loss: 1.0153791904449463
Epoch 1360, training loss: 12.039642333984375 = 0.018051330000162125 + 2.0 * 6.010795593261719
Epoch 1360, val loss: 1.0188239812850952
Epoch 1370, training loss: 12.034170150756836 = 0.017672406509518623 + 2.0 * 6.008248805999756
Epoch 1370, val loss: 1.0224406719207764
Epoch 1380, training loss: 12.035297393798828 = 0.017306627705693245 + 2.0 * 6.008995532989502
Epoch 1380, val loss: 1.0259802341461182
Epoch 1390, training loss: 12.03916072845459 = 0.016952676698565483 + 2.0 * 6.011104106903076
Epoch 1390, val loss: 1.0294874906539917
Epoch 1400, training loss: 12.038928985595703 = 0.016612712293863297 + 2.0 * 6.011157989501953
Epoch 1400, val loss: 1.0327727794647217
Epoch 1410, training loss: 12.030892372131348 = 0.01627710834145546 + 2.0 * 6.007307529449463
Epoch 1410, val loss: 1.0361331701278687
Epoch 1420, training loss: 12.030027389526367 = 0.015954628586769104 + 2.0 * 6.007036209106445
Epoch 1420, val loss: 1.0395103693008423
Epoch 1430, training loss: 12.032121658325195 = 0.01564452052116394 + 2.0 * 6.008238792419434
Epoch 1430, val loss: 1.0428575277328491
Epoch 1440, training loss: 12.032172203063965 = 0.015338364988565445 + 2.0 * 6.008417129516602
Epoch 1440, val loss: 1.046113133430481
Epoch 1450, training loss: 12.028236389160156 = 0.015043436549603939 + 2.0 * 6.006596565246582
Epoch 1450, val loss: 1.0493595600128174
Epoch 1460, training loss: 12.028169631958008 = 0.014758275821805 + 2.0 * 6.0067057609558105
Epoch 1460, val loss: 1.0525784492492676
Epoch 1470, training loss: 12.029491424560547 = 0.014481455087661743 + 2.0 * 6.007504940032959
Epoch 1470, val loss: 1.0557622909545898
Epoch 1480, training loss: 12.025638580322266 = 0.014210997149348259 + 2.0 * 6.005713939666748
Epoch 1480, val loss: 1.0588908195495605
Epoch 1490, training loss: 12.024148941040039 = 0.013948721811175346 + 2.0 * 6.005100250244141
Epoch 1490, val loss: 1.0620368719100952
Epoch 1500, training loss: 12.033475875854492 = 0.013695308938622475 + 2.0 * 6.009890079498291
Epoch 1500, val loss: 1.0651326179504395
Epoch 1510, training loss: 12.028935432434082 = 0.013449355959892273 + 2.0 * 6.007742881774902
Epoch 1510, val loss: 1.0680509805679321
Epoch 1520, training loss: 12.02204418182373 = 0.01320641953498125 + 2.0 * 6.004418849945068
Epoch 1520, val loss: 1.0709866285324097
Epoch 1530, training loss: 12.019702911376953 = 0.012975738383829594 + 2.0 * 6.003363609313965
Epoch 1530, val loss: 1.074113368988037
Epoch 1540, training loss: 12.018465042114258 = 0.012748007662594318 + 2.0 * 6.002858638763428
Epoch 1540, val loss: 1.0771647691726685
Epoch 1550, training loss: 12.019464492797852 = 0.0125264348462224 + 2.0 * 6.003468990325928
Epoch 1550, val loss: 1.0801829099655151
Epoch 1560, training loss: 12.034238815307617 = 0.01231253519654274 + 2.0 * 6.010962963104248
Epoch 1560, val loss: 1.0830504894256592
Epoch 1570, training loss: 12.01693344116211 = 0.012098437175154686 + 2.0 * 6.00241756439209
Epoch 1570, val loss: 1.0856906175613403
Epoch 1580, training loss: 12.017355918884277 = 0.011896753683686256 + 2.0 * 6.002729415893555
Epoch 1580, val loss: 1.0886462926864624
Epoch 1590, training loss: 12.026124954223633 = 0.011699633672833443 + 2.0 * 6.0072126388549805
Epoch 1590, val loss: 1.091604471206665
Epoch 1600, training loss: 12.015291213989258 = 0.011505959555506706 + 2.0 * 6.001892566680908
Epoch 1600, val loss: 1.0942752361297607
Epoch 1610, training loss: 12.013245582580566 = 0.011318014934659004 + 2.0 * 6.0009636878967285
Epoch 1610, val loss: 1.0971342325210571
Epoch 1620, training loss: 12.043120384216309 = 0.011133288033306599 + 2.0 * 6.015993595123291
Epoch 1620, val loss: 1.0999858379364014
Epoch 1630, training loss: 12.019166946411133 = 0.010958634316921234 + 2.0 * 6.004104137420654
Epoch 1630, val loss: 1.1023356914520264
Epoch 1640, training loss: 12.011137008666992 = 0.010784019716084003 + 2.0 * 6.000176429748535
Epoch 1640, val loss: 1.1050727367401123
Epoch 1650, training loss: 12.0105619430542 = 0.01061560120433569 + 2.0 * 5.999973297119141
Epoch 1650, val loss: 1.1079375743865967
Epoch 1660, training loss: 12.00930118560791 = 0.010449663735926151 + 2.0 * 5.999425888061523
Epoch 1660, val loss: 1.1106444597244263
Epoch 1670, training loss: 12.013038635253906 = 0.010286775417625904 + 2.0 * 6.001376152038574
Epoch 1670, val loss: 1.1133899688720703
Epoch 1680, training loss: 12.011360168457031 = 0.010127161629498005 + 2.0 * 6.000616550445557
Epoch 1680, val loss: 1.1158078908920288
Epoch 1690, training loss: 12.007960319519043 = 0.009974691085517406 + 2.0 * 5.998992919921875
Epoch 1690, val loss: 1.1183661222457886
Epoch 1700, training loss: 12.010026931762695 = 0.009825694374740124 + 2.0 * 6.000100612640381
Epoch 1700, val loss: 1.121010422706604
Epoch 1710, training loss: 12.01518440246582 = 0.009680218063294888 + 2.0 * 6.002752304077148
Epoch 1710, val loss: 1.1235740184783936
Epoch 1720, training loss: 12.006698608398438 = 0.009535005316138268 + 2.0 * 5.998581886291504
Epoch 1720, val loss: 1.126084327697754
Epoch 1730, training loss: 12.00652027130127 = 0.009395306929945946 + 2.0 * 5.998562335968018
Epoch 1730, val loss: 1.1286667585372925
Epoch 1740, training loss: 12.021242141723633 = 0.009258582256734371 + 2.0 * 6.0059919357299805
Epoch 1740, val loss: 1.1310871839523315
Epoch 1750, training loss: 12.008927345275879 = 0.009126275777816772 + 2.0 * 5.9999003410339355
Epoch 1750, val loss: 1.1335458755493164
Epoch 1760, training loss: 12.004910469055176 = 0.00899516325443983 + 2.0 * 5.997957706451416
Epoch 1760, val loss: 1.136002540588379
Epoch 1770, training loss: 12.012960433959961 = 0.008869532495737076 + 2.0 * 6.002045631408691
Epoch 1770, val loss: 1.1386278867721558
Epoch 1780, training loss: 12.003473281860352 = 0.008741840720176697 + 2.0 * 5.997365951538086
Epoch 1780, val loss: 1.140756607055664
Epoch 1790, training loss: 12.0048189163208 = 0.008621498011052608 + 2.0 * 5.998098850250244
Epoch 1790, val loss: 1.1432106494903564
Epoch 1800, training loss: 12.003515243530273 = 0.008502312004566193 + 2.0 * 5.997506618499756
Epoch 1800, val loss: 1.145638108253479
Epoch 1810, training loss: 12.01236343383789 = 0.008385556749999523 + 2.0 * 6.001988887786865
Epoch 1810, val loss: 1.1481140851974487
Epoch 1820, training loss: 12.000282287597656 = 0.008272640407085419 + 2.0 * 5.996005058288574
Epoch 1820, val loss: 1.1501961946487427
Epoch 1830, training loss: 11.999269485473633 = 0.008161598816514015 + 2.0 * 5.995553970336914
Epoch 1830, val loss: 1.1525914669036865
Epoch 1840, training loss: 12.007852554321289 = 0.00805164035409689 + 2.0 * 5.9999003410339355
Epoch 1840, val loss: 1.1549575328826904
Epoch 1850, training loss: 12.000410079956055 = 0.007945600897073746 + 2.0 * 5.996232032775879
Epoch 1850, val loss: 1.1570844650268555
Epoch 1860, training loss: 11.999709129333496 = 0.007840786129236221 + 2.0 * 5.995934009552002
Epoch 1860, val loss: 1.1593016386032104
Epoch 1870, training loss: 11.998255729675293 = 0.007740509696304798 + 2.0 * 5.995257377624512
Epoch 1870, val loss: 1.1616041660308838
Epoch 1880, training loss: 12.002361297607422 = 0.0076397135853767395 + 2.0 * 5.997360706329346
Epoch 1880, val loss: 1.1638582944869995
Epoch 1890, training loss: 11.998221397399902 = 0.007541288621723652 + 2.0 * 5.995339870452881
Epoch 1890, val loss: 1.1660021543502808
Epoch 1900, training loss: 11.998239517211914 = 0.007446316070854664 + 2.0 * 5.995396614074707
Epoch 1900, val loss: 1.1680879592895508
Epoch 1910, training loss: 12.00057315826416 = 0.007352367974817753 + 2.0 * 5.996610164642334
Epoch 1910, val loss: 1.1703022718429565
Epoch 1920, training loss: 12.000771522521973 = 0.00726031418889761 + 2.0 * 5.996755599975586
Epoch 1920, val loss: 1.1725022792816162
Epoch 1930, training loss: 12.003241539001465 = 0.00716992886736989 + 2.0 * 5.998035907745361
Epoch 1930, val loss: 1.1745896339416504
Epoch 1940, training loss: 11.996376991271973 = 0.007082812953740358 + 2.0 * 5.994647026062012
Epoch 1940, val loss: 1.1765321493148804
Epoch 1950, training loss: 11.993972778320312 = 0.006996945012360811 + 2.0 * 5.99348783493042
Epoch 1950, val loss: 1.1787477731704712
Epoch 1960, training loss: 11.99520492553711 = 0.006912961136549711 + 2.0 * 5.99414587020874
Epoch 1960, val loss: 1.1808987855911255
Epoch 1970, training loss: 12.002004623413086 = 0.006830201484262943 + 2.0 * 5.997587203979492
Epoch 1970, val loss: 1.1829476356506348
Epoch 1980, training loss: 11.99524211883545 = 0.006745688151568174 + 2.0 * 5.994248390197754
Epoch 1980, val loss: 1.1848835945129395
Epoch 1990, training loss: 12.001896858215332 = 0.006667024455964565 + 2.0 * 5.997614860534668
Epoch 1990, val loss: 1.1869198083877563
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.3395
Flip ASR: 0.3467/225 nodes
The final ASR:0.59164, 0.20342, Accuracy:0.81358, 0.00462
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11600])
remove edge: torch.Size([2, 9420])
updated graph: torch.Size([2, 10464])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9705
Flip ASR: 0.9644/225 nodes
The final ASR:0.97663, 0.00460, Accuracy:0.82840, 0.00462
Begin epxeriment: cont_weight: 2 epoch:2000 der1:0.3 der2:0.2 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 18.70045280456543 = 1.9527413845062256 + 2.0 * 8.373855590820312
Epoch 0, val loss: 1.941481113433838
Epoch 10, training loss: 18.6892032623291 = 1.9425755739212036 + 2.0 * 8.373313903808594
Epoch 10, val loss: 1.9320489168167114
Epoch 20, training loss: 18.668535232543945 = 1.930126428604126 + 2.0 * 8.3692045211792
Epoch 20, val loss: 1.92000412940979
Epoch 30, training loss: 18.590805053710938 = 1.9131735563278198 + 2.0 * 8.338815689086914
Epoch 30, val loss: 1.903014063835144
Epoch 40, training loss: 18.17546844482422 = 1.8929282426834106 + 2.0 * 8.14126968383789
Epoch 40, val loss: 1.8831928968429565
Epoch 50, training loss: 17.24871253967285 = 1.8708324432373047 + 2.0 * 7.688940048217773
Epoch 50, val loss: 1.8614994287490845
Epoch 60, training loss: 16.636615753173828 = 1.8522123098373413 + 2.0 * 7.3922014236450195
Epoch 60, val loss: 1.844226360321045
Epoch 70, training loss: 15.818085670471191 = 1.8407809734344482 + 2.0 * 6.988652229309082
Epoch 70, val loss: 1.834700107574463
Epoch 80, training loss: 15.244117736816406 = 1.8329821825027466 + 2.0 * 6.705567836761475
Epoch 80, val loss: 1.8280065059661865
Epoch 90, training loss: 14.960474014282227 = 1.821594476699829 + 2.0 * 6.569439888000488
Epoch 90, val loss: 1.8178269863128662
Epoch 100, training loss: 14.78243637084961 = 1.8081316947937012 + 2.0 * 6.487152576446533
Epoch 100, val loss: 1.8061031103134155
Epoch 110, training loss: 14.656914710998535 = 1.7955511808395386 + 2.0 * 6.4306817054748535
Epoch 110, val loss: 1.7954434156417847
Epoch 120, training loss: 14.54345703125 = 1.7840710878372192 + 2.0 * 6.379693031311035
Epoch 120, val loss: 1.7860316038131714
Epoch 130, training loss: 14.444374084472656 = 1.773098111152649 + 2.0 * 6.335638046264648
Epoch 130, val loss: 1.7772769927978516
Epoch 140, training loss: 14.364240646362305 = 1.7619158029556274 + 2.0 * 6.301162242889404
Epoch 140, val loss: 1.7685613632202148
Epoch 150, training loss: 14.297724723815918 = 1.7496583461761475 + 2.0 * 6.274033069610596
Epoch 150, val loss: 1.7591683864593506
Epoch 160, training loss: 14.236084938049316 = 1.736058235168457 + 2.0 * 6.25001335144043
Epoch 160, val loss: 1.7487733364105225
Epoch 170, training loss: 14.182957649230957 = 1.7207906246185303 + 2.0 * 6.231083393096924
Epoch 170, val loss: 1.7371892929077148
Epoch 180, training loss: 14.135762214660645 = 1.7035703659057617 + 2.0 * 6.216095924377441
Epoch 180, val loss: 1.7241512537002563
Epoch 190, training loss: 14.086772918701172 = 1.6842597723007202 + 2.0 * 6.20125675201416
Epoch 190, val loss: 1.7094969749450684
Epoch 200, training loss: 14.039824485778809 = 1.6625535488128662 + 2.0 * 6.188635349273682
Epoch 200, val loss: 1.6930317878723145
Epoch 210, training loss: 13.994662284851074 = 1.6379951238632202 + 2.0 * 6.178333759307861
Epoch 210, val loss: 1.6744459867477417
Epoch 220, training loss: 13.95221996307373 = 1.6105059385299683 + 2.0 * 6.170856952667236
Epoch 220, val loss: 1.6536409854888916
Epoch 230, training loss: 13.902796745300293 = 1.580122709274292 + 2.0 * 6.161336898803711
Epoch 230, val loss: 1.6306449174880981
Epoch 240, training loss: 13.852314949035645 = 1.5468597412109375 + 2.0 * 6.1527276039123535
Epoch 240, val loss: 1.6055190563201904
Epoch 250, training loss: 13.806194305419922 = 1.51096510887146 + 2.0 * 6.147614479064941
Epoch 250, val loss: 1.5784149169921875
Epoch 260, training loss: 13.753202438354492 = 1.473046898841858 + 2.0 * 6.140077590942383
Epoch 260, val loss: 1.5499249696731567
Epoch 270, training loss: 13.701111793518066 = 1.433606743812561 + 2.0 * 6.133752346038818
Epoch 270, val loss: 1.5203453302383423
Epoch 280, training loss: 13.652434349060059 = 1.3929393291473389 + 2.0 * 6.12974739074707
Epoch 280, val loss: 1.490035057067871
Epoch 290, training loss: 13.606605529785156 = 1.3519271612167358 + 2.0 * 6.1273393630981445
Epoch 290, val loss: 1.4595632553100586
Epoch 300, training loss: 13.551629066467285 = 1.3112293481826782 + 2.0 * 6.120199680328369
Epoch 300, val loss: 1.4293513298034668
Epoch 310, training loss: 13.500946998596191 = 1.2706692218780518 + 2.0 * 6.115139007568359
Epoch 310, val loss: 1.3995742797851562
Epoch 320, training loss: 13.459344863891602 = 1.2304191589355469 + 2.0 * 6.114462852478027
Epoch 320, val loss: 1.3702232837677002
Epoch 330, training loss: 13.411992073059082 = 1.1911014318466187 + 2.0 * 6.110445499420166
Epoch 330, val loss: 1.3413488864898682
Epoch 340, training loss: 13.363502502441406 = 1.1522406339645386 + 2.0 * 6.105630874633789
Epoch 340, val loss: 1.3130496740341187
Epoch 350, training loss: 13.317540168762207 = 1.1138452291488647 + 2.0 * 6.1018476486206055
Epoch 350, val loss: 1.2850693464279175
Epoch 360, training loss: 13.278213500976562 = 1.0756962299346924 + 2.0 * 6.101258754730225
Epoch 360, val loss: 1.2572345733642578
Epoch 370, training loss: 13.234779357910156 = 1.0379688739776611 + 2.0 * 6.098405361175537
Epoch 370, val loss: 1.2296446561813354
Epoch 380, training loss: 13.193136215209961 = 1.000593662261963 + 2.0 * 6.096271514892578
Epoch 380, val loss: 1.202303171157837
Epoch 390, training loss: 13.14808464050293 = 0.9636969566345215 + 2.0 * 6.092194080352783
Epoch 390, val loss: 1.1753971576690674
Epoch 400, training loss: 13.107308387756348 = 0.9271244406700134 + 2.0 * 6.090092182159424
Epoch 400, val loss: 1.148727297782898
Epoch 410, training loss: 13.06796932220459 = 0.8911526203155518 + 2.0 * 6.088408470153809
Epoch 410, val loss: 1.1226260662078857
Epoch 420, training loss: 13.026851654052734 = 0.8561890721321106 + 2.0 * 6.085331439971924
Epoch 420, val loss: 1.0973752737045288
Epoch 430, training loss: 12.98896312713623 = 0.8219185471534729 + 2.0 * 6.083522319793701
Epoch 430, val loss: 1.0729012489318848
Epoch 440, training loss: 12.963115692138672 = 0.7883305549621582 + 2.0 * 6.087392807006836
Epoch 440, val loss: 1.0491307973861694
Epoch 450, training loss: 12.91537094116211 = 0.7559016942977905 + 2.0 * 6.079734802246094
Epoch 450, val loss: 1.026116967201233
Epoch 460, training loss: 12.889341354370117 = 0.7243351936340332 + 2.0 * 6.082503318786621
Epoch 460, val loss: 1.0041310787200928
Epoch 470, training loss: 12.848031044006348 = 0.6938050985336304 + 2.0 * 6.077113151550293
Epoch 470, val loss: 0.9831025004386902
Epoch 480, training loss: 12.812732696533203 = 0.664397120475769 + 2.0 * 6.074167728424072
Epoch 480, val loss: 0.963295578956604
Epoch 490, training loss: 12.784554481506348 = 0.6360836029052734 + 2.0 * 6.074235439300537
Epoch 490, val loss: 0.9445068836212158
Epoch 500, training loss: 12.754287719726562 = 0.6090176105499268 + 2.0 * 6.072635173797607
Epoch 500, val loss: 0.9268755316734314
Epoch 510, training loss: 12.72861099243164 = 0.5834879875183105 + 2.0 * 6.072561264038086
Epoch 510, val loss: 0.9108033180236816
Epoch 520, training loss: 12.696427345275879 = 0.5596439242362976 + 2.0 * 6.068391799926758
Epoch 520, val loss: 0.8960855603218079
Epoch 530, training loss: 12.669784545898438 = 0.537084698677063 + 2.0 * 6.066349983215332
Epoch 530, val loss: 0.8828016519546509
Epoch 540, training loss: 12.644998550415039 = 0.5160409808158875 + 2.0 * 6.064478874206543
Epoch 540, val loss: 0.8708786368370056
Epoch 550, training loss: 12.624396324157715 = 0.49635863304138184 + 2.0 * 6.064018726348877
Epoch 550, val loss: 0.8603984117507935
Epoch 560, training loss: 12.605113983154297 = 0.47791197896003723 + 2.0 * 6.063601016998291
Epoch 560, val loss: 0.8512923717498779
Epoch 570, training loss: 12.583027839660645 = 0.46062594652175903 + 2.0 * 6.061201095581055
Epoch 570, val loss: 0.8433391451835632
Epoch 580, training loss: 12.563512802124023 = 0.44416651129722595 + 2.0 * 6.059673309326172
Epoch 580, val loss: 0.8363541960716248
Epoch 590, training loss: 12.543081283569336 = 0.42855024337768555 + 2.0 * 6.057265281677246
Epoch 590, val loss: 0.8303870558738708
Epoch 600, training loss: 12.523483276367188 = 0.41341298818588257 + 2.0 * 6.05503511428833
Epoch 600, val loss: 0.8250792622566223
Epoch 610, training loss: 12.516716003417969 = 0.3987542986869812 + 2.0 * 6.058980941772461
Epoch 610, val loss: 0.8202891945838928
Epoch 620, training loss: 12.496700286865234 = 0.3843357264995575 + 2.0 * 6.056182384490967
Epoch 620, val loss: 0.81592857837677
Epoch 630, training loss: 12.471551895141602 = 0.37029051780700684 + 2.0 * 6.050630569458008
Epoch 630, val loss: 0.8122076392173767
Epoch 640, training loss: 12.45722484588623 = 0.35638248920440674 + 2.0 * 6.050421237945557
Epoch 640, val loss: 0.8089008331298828
Epoch 650, training loss: 12.446395874023438 = 0.34255191683769226 + 2.0 * 6.051921844482422
Epoch 650, val loss: 0.8058199286460876
Epoch 660, training loss: 12.42770767211914 = 0.3289199471473694 + 2.0 * 6.049393653869629
Epoch 660, val loss: 0.8030188679695129
Epoch 670, training loss: 12.40909194946289 = 0.3154285252094269 + 2.0 * 6.0468316078186035
Epoch 670, val loss: 0.8006270527839661
Epoch 680, training loss: 12.396479606628418 = 0.30209967494010925 + 2.0 * 6.047190189361572
Epoch 680, val loss: 0.7986992001533508
Epoch 690, training loss: 12.375636100769043 = 0.2889866530895233 + 2.0 * 6.043324947357178
Epoch 690, val loss: 0.7970942854881287
Epoch 700, training loss: 12.367743492126465 = 0.27608340978622437 + 2.0 * 6.045830249786377
Epoch 700, val loss: 0.7958723902702332
Epoch 710, training loss: 12.354498863220215 = 0.2634223401546478 + 2.0 * 6.045538425445557
Epoch 710, val loss: 0.794955849647522
Epoch 720, training loss: 12.334630012512207 = 0.25117820501327515 + 2.0 * 6.041726112365723
Epoch 720, val loss: 0.7945496439933777
Epoch 730, training loss: 12.320028305053711 = 0.23925082385540009 + 2.0 * 6.040388584136963
Epoch 730, val loss: 0.7946077585220337
Epoch 740, training loss: 12.310580253601074 = 0.227692112326622 + 2.0 * 6.041444301605225
Epoch 740, val loss: 0.7949133515357971
Epoch 750, training loss: 12.292859077453613 = 0.21652266383171082 + 2.0 * 6.038168430328369
Epoch 750, val loss: 0.7956968545913696
Epoch 760, training loss: 12.283609390258789 = 0.20573778450489044 + 2.0 * 6.038935661315918
Epoch 760, val loss: 0.7969645261764526
Epoch 770, training loss: 12.268655776977539 = 0.19537612795829773 + 2.0 * 6.03663969039917
Epoch 770, val loss: 0.7985228896141052
Epoch 780, training loss: 12.262529373168945 = 0.18540331721305847 + 2.0 * 6.038563251495361
Epoch 780, val loss: 0.8004157543182373
Epoch 790, training loss: 12.246864318847656 = 0.17585597932338715 + 2.0 * 6.035504341125488
Epoch 790, val loss: 0.8025974631309509
Epoch 800, training loss: 12.235173225402832 = 0.16673552989959717 + 2.0 * 6.034218788146973
Epoch 800, val loss: 0.8051524758338928
Epoch 810, training loss: 12.226045608520508 = 0.15805748105049133 + 2.0 * 6.033994197845459
Epoch 810, val loss: 0.8080742359161377
Epoch 820, training loss: 12.220470428466797 = 0.14981766045093536 + 2.0 * 6.0353264808654785
Epoch 820, val loss: 0.8110026717185974
Epoch 830, training loss: 12.208967208862305 = 0.14199310541152954 + 2.0 * 6.033486843109131
Epoch 830, val loss: 0.8143248558044434
Epoch 840, training loss: 12.196060180664062 = 0.13461998105049133 + 2.0 * 6.030720233917236
Epoch 840, val loss: 0.8179352283477783
Epoch 850, training loss: 12.18890380859375 = 0.12764643132686615 + 2.0 * 6.030628681182861
Epoch 850, val loss: 0.8216885924339294
Epoch 860, training loss: 12.186347007751465 = 0.12107909470796585 + 2.0 * 6.0326337814331055
Epoch 860, val loss: 0.825543224811554
Epoch 870, training loss: 12.17628288269043 = 0.11490121483802795 + 2.0 * 6.030690670013428
Epoch 870, val loss: 0.8295271396636963
Epoch 880, training loss: 12.168285369873047 = 0.10912025719881058 + 2.0 * 6.029582500457764
Epoch 880, val loss: 0.8338099718093872
Epoch 890, training loss: 12.158343315124512 = 0.10372506827116013 + 2.0 * 6.027308940887451
Epoch 890, val loss: 0.8381350040435791
Epoch 900, training loss: 12.154016494750977 = 0.09866049885749817 + 2.0 * 6.0276780128479
Epoch 900, val loss: 0.8426429629325867
Epoch 910, training loss: 12.148702621459961 = 0.0939125195145607 + 2.0 * 6.027395248413086
Epoch 910, val loss: 0.8471879363059998
Epoch 920, training loss: 12.141593933105469 = 0.08946151286363602 + 2.0 * 6.026066303253174
Epoch 920, val loss: 0.8518629670143127
Epoch 930, training loss: 12.137625694274902 = 0.08531561493873596 + 2.0 * 6.0261549949646
Epoch 930, val loss: 0.8566575050354004
Epoch 940, training loss: 12.133003234863281 = 0.0814320296049118 + 2.0 * 6.025785446166992
Epoch 940, val loss: 0.8614053726196289
Epoch 950, training loss: 12.131308555603027 = 0.07781337201595306 + 2.0 * 6.026747703552246
Epoch 950, val loss: 0.8662942051887512
Epoch 960, training loss: 12.120816230773926 = 0.07439155876636505 + 2.0 * 6.023212432861328
Epoch 960, val loss: 0.8710706830024719
Epoch 970, training loss: 12.11620044708252 = 0.07120381295681 + 2.0 * 6.02249813079834
Epoch 970, val loss: 0.875935435295105
Epoch 980, training loss: 12.127128601074219 = 0.06820031255483627 + 2.0 * 6.029464244842529
Epoch 980, val loss: 0.8806552886962891
Epoch 990, training loss: 12.10720443725586 = 0.06539930403232574 + 2.0 * 6.020902633666992
Epoch 990, val loss: 0.8853917717933655
Epoch 1000, training loss: 12.1048583984375 = 0.06276050955057144 + 2.0 * 6.0210490226745605
Epoch 1000, val loss: 0.8902845978736877
Epoch 1010, training loss: 12.098971366882324 = 0.060276102274656296 + 2.0 * 6.019347667694092
Epoch 1010, val loss: 0.8950493931770325
Epoch 1020, training loss: 12.102723121643066 = 0.05792536586523056 + 2.0 * 6.022398948669434
Epoch 1020, val loss: 0.8997759819030762
Epoch 1030, training loss: 12.098072052001953 = 0.0557243712246418 + 2.0 * 6.02117395401001
Epoch 1030, val loss: 0.9043241143226624
Epoch 1040, training loss: 12.093192100524902 = 0.05363903194665909 + 2.0 * 6.019776344299316
Epoch 1040, val loss: 0.9090069532394409
Epoch 1050, training loss: 12.103765487670898 = 0.0516686849296093 + 2.0 * 6.026048183441162
Epoch 1050, val loss: 0.91359543800354
Epoch 1060, training loss: 12.088431358337402 = 0.04982564225792885 + 2.0 * 6.019302845001221
Epoch 1060, val loss: 0.9180524349212646
Epoch 1070, training loss: 12.081840515136719 = 0.04805556684732437 + 2.0 * 6.016892433166504
Epoch 1070, val loss: 0.922613799571991
Epoch 1080, training loss: 12.077946662902832 = 0.04638013616204262 + 2.0 * 6.015783309936523
Epoch 1080, val loss: 0.9271146655082703
Epoch 1090, training loss: 12.075918197631836 = 0.04477894306182861 + 2.0 * 6.015569686889648
Epoch 1090, val loss: 0.9315268397331238
Epoch 1100, training loss: 12.08848762512207 = 0.04325515776872635 + 2.0 * 6.022616386413574
Epoch 1100, val loss: 0.9358293414115906
Epoch 1110, training loss: 12.074962615966797 = 0.04182009398937225 + 2.0 * 6.016571044921875
Epoch 1110, val loss: 0.9400346279144287
Epoch 1120, training loss: 12.07265853881836 = 0.04045845568180084 + 2.0 * 6.01609992980957
Epoch 1120, val loss: 0.9444126486778259
Epoch 1130, training loss: 12.069476127624512 = 0.039158765226602554 + 2.0 * 6.015158653259277
Epoch 1130, val loss: 0.9486708045005798
Epoch 1140, training loss: 12.064516067504883 = 0.03790951892733574 + 2.0 * 6.013303279876709
Epoch 1140, val loss: 0.9528090953826904
Epoch 1150, training loss: 12.063456535339355 = 0.03672018647193909 + 2.0 * 6.013368129730225
Epoch 1150, val loss: 0.9569786787033081
Epoch 1160, training loss: 12.066459655761719 = 0.0355839878320694 + 2.0 * 6.015437602996826
Epoch 1160, val loss: 0.9610499739646912
Epoch 1170, training loss: 12.064336776733398 = 0.034505944699048996 + 2.0 * 6.014915466308594
Epoch 1170, val loss: 0.9650819897651672
Epoch 1180, training loss: 12.063458442687988 = 0.033462852239608765 + 2.0 * 6.014997959136963
Epoch 1180, val loss: 0.9690355062484741
Epoch 1190, training loss: 12.057425498962402 = 0.03248317167162895 + 2.0 * 6.0124711990356445
Epoch 1190, val loss: 0.9730544686317444
Epoch 1200, training loss: 12.054765701293945 = 0.031533025205135345 + 2.0 * 6.011616230010986
Epoch 1200, val loss: 0.9769507646560669
Epoch 1210, training loss: 12.052983283996582 = 0.030625518411397934 + 2.0 * 6.011178970336914
Epoch 1210, val loss: 0.9808881878852844
Epoch 1220, training loss: 12.054008483886719 = 0.029752563685178757 + 2.0 * 6.012127876281738
Epoch 1220, val loss: 0.9846540689468384
Epoch 1230, training loss: 12.050745010375977 = 0.028913768008351326 + 2.0 * 6.010915756225586
Epoch 1230, val loss: 0.9884005784988403
Epoch 1240, training loss: 12.056584358215332 = 0.028113756328821182 + 2.0 * 6.014235496520996
Epoch 1240, val loss: 0.9921392202377319
Epoch 1250, training loss: 12.052151679992676 = 0.02734551765024662 + 2.0 * 6.0124030113220215
Epoch 1250, val loss: 0.9956937432289124
Epoch 1260, training loss: 12.045771598815918 = 0.02660888060927391 + 2.0 * 6.009581565856934
Epoch 1260, val loss: 0.999331533908844
Epoch 1270, training loss: 12.048503875732422 = 0.02590233087539673 + 2.0 * 6.011300563812256
Epoch 1270, val loss: 1.0029629468917847
Epoch 1280, training loss: 12.041228294372559 = 0.02521919272840023 + 2.0 * 6.008004665374756
Epoch 1280, val loss: 1.0064195394515991
Epoch 1290, training loss: 12.038880348205566 = 0.024562526494264603 + 2.0 * 6.0071587562561035
Epoch 1290, val loss: 1.0099714994430542
Epoch 1300, training loss: 12.039481163024902 = 0.02393394336104393 + 2.0 * 6.007773399353027
Epoch 1300, val loss: 1.0134543180465698
Epoch 1310, training loss: 12.047301292419434 = 0.023326359689235687 + 2.0 * 6.011987686157227
Epoch 1310, val loss: 1.0168018341064453
Epoch 1320, training loss: 12.039251327514648 = 0.022739721462130547 + 2.0 * 6.008255958557129
Epoch 1320, val loss: 1.0202374458312988
Epoch 1330, training loss: 12.044233322143555 = 0.02217893861234188 + 2.0 * 6.0110273361206055
Epoch 1330, val loss: 1.0235129594802856
Epoch 1340, training loss: 12.036365509033203 = 0.02163655124604702 + 2.0 * 6.007364273071289
Epoch 1340, val loss: 1.0268611907958984
Epoch 1350, training loss: 12.035046577453613 = 0.021115969866514206 + 2.0 * 6.006965160369873
Epoch 1350, val loss: 1.0301514863967896
Epoch 1360, training loss: 12.030010223388672 = 0.020614048466086388 + 2.0 * 6.004698276519775
Epoch 1360, val loss: 1.0334830284118652
Epoch 1370, training loss: 12.030466079711914 = 0.02012825571000576 + 2.0 * 6.005168914794922
Epoch 1370, val loss: 1.0367403030395508
Epoch 1380, training loss: 12.035999298095703 = 0.019661296159029007 + 2.0 * 6.008169174194336
Epoch 1380, val loss: 1.0399177074432373
Epoch 1390, training loss: 12.033123970031738 = 0.019204718992114067 + 2.0 * 6.006959438323975
Epoch 1390, val loss: 1.0429890155792236
Epoch 1400, training loss: 12.028225898742676 = 0.018768535926938057 + 2.0 * 6.0047287940979
Epoch 1400, val loss: 1.0461642742156982
Epoch 1410, training loss: 12.029925346374512 = 0.01834750734269619 + 2.0 * 6.005788803100586
Epoch 1410, val loss: 1.0493993759155273
Epoch 1420, training loss: 12.02708625793457 = 0.017938246950507164 + 2.0 * 6.004573822021484
Epoch 1420, val loss: 1.0522980690002441
Epoch 1430, training loss: 12.027992248535156 = 0.01754170097410679 + 2.0 * 6.00522518157959
Epoch 1430, val loss: 1.055372714996338
Epoch 1440, training loss: 12.0267972946167 = 0.017163287848234177 + 2.0 * 6.004817008972168
Epoch 1440, val loss: 1.0582884550094604
Epoch 1450, training loss: 12.02153491973877 = 0.01679413951933384 + 2.0 * 6.002370357513428
Epoch 1450, val loss: 1.0613311529159546
Epoch 1460, training loss: 12.019969940185547 = 0.016438309103250504 + 2.0 * 6.001765727996826
Epoch 1460, val loss: 1.064328670501709
Epoch 1470, training loss: 12.022074699401855 = 0.016093235462903976 + 2.0 * 6.00299072265625
Epoch 1470, val loss: 1.06731379032135
Epoch 1480, training loss: 12.031075477600098 = 0.01575826294720173 + 2.0 * 6.0076584815979
Epoch 1480, val loss: 1.0699430704116821
Epoch 1490, training loss: 12.021078109741211 = 0.015434949658811092 + 2.0 * 6.002821445465088
Epoch 1490, val loss: 1.072706699371338
Epoch 1500, training loss: 12.016815185546875 = 0.015122612938284874 + 2.0 * 6.0008463859558105
Epoch 1500, val loss: 1.0756717920303345
Epoch 1510, training loss: 12.015059471130371 = 0.014820246025919914 + 2.0 * 6.000119686126709
Epoch 1510, val loss: 1.078546166419983
Epoch 1520, training loss: 12.023579597473145 = 0.014525006525218487 + 2.0 * 6.0045270919799805
Epoch 1520, val loss: 1.0812764167785645
Epoch 1530, training loss: 12.016812324523926 = 0.014238984324038029 + 2.0 * 6.001286506652832
Epoch 1530, val loss: 1.0838929414749146
Epoch 1540, training loss: 12.01651668548584 = 0.013962375931441784 + 2.0 * 6.001276969909668
Epoch 1540, val loss: 1.0865691900253296
Epoch 1550, training loss: 12.014551162719727 = 0.013697066344320774 + 2.0 * 6.00042724609375
Epoch 1550, val loss: 1.0893669128417969
Epoch 1560, training loss: 12.021432876586914 = 0.013435158878564835 + 2.0 * 6.003998756408691
Epoch 1560, val loss: 1.0918662548065186
Epoch 1570, training loss: 12.011977195739746 = 0.013184123672544956 + 2.0 * 5.999396324157715
Epoch 1570, val loss: 1.094496488571167
Epoch 1580, training loss: 12.009748458862305 = 0.012939615175127983 + 2.0 * 5.998404502868652
Epoch 1580, val loss: 1.0971705913543701
Epoch 1590, training loss: 12.008745193481445 = 0.012700743041932583 + 2.0 * 5.998022079467773
Epoch 1590, val loss: 1.0998152494430542
Epoch 1600, training loss: 12.027581214904785 = 0.012469911947846413 + 2.0 * 6.0075554847717285
Epoch 1600, val loss: 1.102375864982605
Epoch 1610, training loss: 12.01540756225586 = 0.012243012897670269 + 2.0 * 6.001582145690918
Epoch 1610, val loss: 1.104553461074829
Epoch 1620, training loss: 12.00683879852295 = 0.012026476673781872 + 2.0 * 5.997406005859375
Epoch 1620, val loss: 1.1072343587875366
Epoch 1630, training loss: 12.012640953063965 = 0.011815661564469337 + 2.0 * 6.000412464141846
Epoch 1630, val loss: 1.109831690788269
Epoch 1640, training loss: 12.005908966064453 = 0.011609148234128952 + 2.0 * 5.99714994430542
Epoch 1640, val loss: 1.1120599508285522
Epoch 1650, training loss: 12.004592895507812 = 0.011406831443309784 + 2.0 * 5.996592998504639
Epoch 1650, val loss: 1.114546775817871
Epoch 1660, training loss: 12.004375457763672 = 0.01121183019131422 + 2.0 * 5.99658203125
Epoch 1660, val loss: 1.1170034408569336
Epoch 1670, training loss: 12.007523536682129 = 0.011019413359463215 + 2.0 * 5.998251914978027
Epoch 1670, val loss: 1.1193739175796509
Epoch 1680, training loss: 12.005887985229492 = 0.010835115797817707 + 2.0 * 5.9975266456604
Epoch 1680, val loss: 1.121633529663086
Epoch 1690, training loss: 12.00279426574707 = 0.010655428282916546 + 2.0 * 5.996069431304932
Epoch 1690, val loss: 1.123989462852478
Epoch 1700, training loss: 12.001517295837402 = 0.010480262339115143 + 2.0 * 5.995518684387207
Epoch 1700, val loss: 1.1263781785964966
Epoch 1710, training loss: 12.007394790649414 = 0.010309913195669651 + 2.0 * 5.998542308807373
Epoch 1710, val loss: 1.1286700963974
Epoch 1720, training loss: 12.003301620483398 = 0.010143420659005642 + 2.0 * 5.996579170227051
Epoch 1720, val loss: 1.1309168338775635
Epoch 1730, training loss: 12.0043306350708 = 0.009981672279536724 + 2.0 * 5.997174263000488
Epoch 1730, val loss: 1.1331106424331665
Epoch 1740, training loss: 12.002569198608398 = 0.009823322296142578 + 2.0 * 5.996372699737549
Epoch 1740, val loss: 1.1353157758712769
Epoch 1750, training loss: 11.999360084533691 = 0.009669075720012188 + 2.0 * 5.994845390319824
Epoch 1750, val loss: 1.137510061264038
Epoch 1760, training loss: 11.99763011932373 = 0.009519749321043491 + 2.0 * 5.994055271148682
Epoch 1760, val loss: 1.139743685722351
Epoch 1770, training loss: 11.996822357177734 = 0.009372825734317303 + 2.0 * 5.993724822998047
Epoch 1770, val loss: 1.1419614553451538
Epoch 1780, training loss: 12.009852409362793 = 0.009229234419763088 + 2.0 * 6.000311374664307
Epoch 1780, val loss: 1.1440060138702393
Epoch 1790, training loss: 11.999735832214355 = 0.00908982940018177 + 2.0 * 5.995323181152344
Epoch 1790, val loss: 1.145981788635254
Epoch 1800, training loss: 12.001691818237305 = 0.008954535238444805 + 2.0 * 5.996368408203125
Epoch 1800, val loss: 1.1481624841690063
Epoch 1810, training loss: 11.997389793395996 = 0.008822211995720863 + 2.0 * 5.994283676147461
Epoch 1810, val loss: 1.1501224040985107
Epoch 1820, training loss: 11.994643211364746 = 0.008692577481269836 + 2.0 * 5.992975234985352
Epoch 1820, val loss: 1.1522719860076904
Epoch 1830, training loss: 11.993539810180664 = 0.00856651272624731 + 2.0 * 5.992486476898193
Epoch 1830, val loss: 1.1543474197387695
Epoch 1840, training loss: 12.001866340637207 = 0.00844382680952549 + 2.0 * 5.996711254119873
Epoch 1840, val loss: 1.1564230918884277
Epoch 1850, training loss: 11.995476722717285 = 0.008320774883031845 + 2.0 * 5.99357795715332
Epoch 1850, val loss: 1.1581107378005981
Epoch 1860, training loss: 11.994741439819336 = 0.008203904144465923 + 2.0 * 5.993268966674805
Epoch 1860, val loss: 1.1600943803787231
Epoch 1870, training loss: 11.992344856262207 = 0.008088277652859688 + 2.0 * 5.992128372192383
Epoch 1870, val loss: 1.162170648574829
Epoch 1880, training loss: 11.995705604553223 = 0.00797566119581461 + 2.0 * 5.993865013122559
Epoch 1880, val loss: 1.164106011390686
Epoch 1890, training loss: 11.992989540100098 = 0.00786505825817585 + 2.0 * 5.992562294006348
Epoch 1890, val loss: 1.1659144163131714
Epoch 1900, training loss: 11.995865821838379 = 0.007757790386676788 + 2.0 * 5.994053840637207
Epoch 1900, val loss: 1.1677075624465942
Epoch 1910, training loss: 11.99360466003418 = 0.007653011009097099 + 2.0 * 5.99297571182251
Epoch 1910, val loss: 1.1696959733963013
Epoch 1920, training loss: 11.990082740783691 = 0.007550928741693497 + 2.0 * 5.991265773773193
Epoch 1920, val loss: 1.1715947389602661
Epoch 1930, training loss: 11.992892265319824 = 0.0074501498602330685 + 2.0 * 5.992721080780029
Epoch 1930, val loss: 1.173445224761963
Epoch 1940, training loss: 11.98996353149414 = 0.0073506999760866165 + 2.0 * 5.991306304931641
Epoch 1940, val loss: 1.1751902103424072
Epoch 1950, training loss: 11.992167472839355 = 0.007254073396325111 + 2.0 * 5.992456912994385
Epoch 1950, val loss: 1.1770566701889038
Epoch 1960, training loss: 11.988804817199707 = 0.007160286419093609 + 2.0 * 5.9908223152160645
Epoch 1960, val loss: 1.1787954568862915
Epoch 1970, training loss: 11.988363265991211 = 0.007068529725074768 + 2.0 * 5.990647315979004
Epoch 1970, val loss: 1.1806012392044067
Epoch 1980, training loss: 11.995378494262695 = 0.006979580502957106 + 2.0 * 5.994199275970459
Epoch 1980, val loss: 1.1824147701263428
Epoch 1990, training loss: 11.988202095031738 = 0.006888313684612513 + 2.0 * 5.990656852722168
Epoch 1990, val loss: 1.1839289665222168
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8037
Overall ASR: 0.6494
Flip ASR: 0.5778/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.706764221191406 = 1.9590051174163818 + 2.0 * 8.373879432678223
Epoch 0, val loss: 1.96356201171875
Epoch 10, training loss: 18.694286346435547 = 1.9480940103530884 + 2.0 * 8.373096466064453
Epoch 10, val loss: 1.951745867729187
Epoch 20, training loss: 18.67120361328125 = 1.9348063468933105 + 2.0 * 8.36819839477539
Epoch 20, val loss: 1.936782717704773
Epoch 30, training loss: 18.602163314819336 = 1.9166823625564575 + 2.0 * 8.342740058898926
Epoch 30, val loss: 1.915776014328003
Epoch 40, training loss: 18.308940887451172 = 1.8942736387252808 + 2.0 * 8.2073335647583
Epoch 40, val loss: 1.8901031017303467
Epoch 50, training loss: 17.483314514160156 = 1.8724141120910645 + 2.0 * 7.805449962615967
Epoch 50, val loss: 1.8650654554367065
Epoch 60, training loss: 16.77142906188965 = 1.8514198064804077 + 2.0 * 7.4600043296813965
Epoch 60, val loss: 1.8413736820220947
Epoch 70, training loss: 15.968005180358887 = 1.831189751625061 + 2.0 * 7.0684075355529785
Epoch 70, val loss: 1.8204175233840942
Epoch 80, training loss: 15.500246047973633 = 1.8130512237548828 + 2.0 * 6.843597412109375
Epoch 80, val loss: 1.8028706312179565
Epoch 90, training loss: 15.225576400756836 = 1.7983871698379517 + 2.0 * 6.713594436645508
Epoch 90, val loss: 1.7886167764663696
Epoch 100, training loss: 14.943081855773926 = 1.7846262454986572 + 2.0 * 6.579227924346924
Epoch 100, val loss: 1.7756930589675903
Epoch 110, training loss: 14.746316909790039 = 1.7719670534133911 + 2.0 * 6.487174987792969
Epoch 110, val loss: 1.7640292644500732
Epoch 120, training loss: 14.60341739654541 = 1.7589167356491089 + 2.0 * 6.422250270843506
Epoch 120, val loss: 1.7518543004989624
Epoch 130, training loss: 14.501213073730469 = 1.7453532218933105 + 2.0 * 6.377930164337158
Epoch 130, val loss: 1.7393646240234375
Epoch 140, training loss: 14.413346290588379 = 1.7309188842773438 + 2.0 * 6.341213703155518
Epoch 140, val loss: 1.7265712022781372
Epoch 150, training loss: 14.33889102935791 = 1.7154273986816406 + 2.0 * 6.311731815338135
Epoch 150, val loss: 1.7133338451385498
Epoch 160, training loss: 14.269242286682129 = 1.6984355449676514 + 2.0 * 6.285403251647949
Epoch 160, val loss: 1.6993169784545898
Epoch 170, training loss: 14.210597038269043 = 1.6793161630630493 + 2.0 * 6.2656402587890625
Epoch 170, val loss: 1.6838871240615845
Epoch 180, training loss: 14.177522659301758 = 1.657676339149475 + 2.0 * 6.259922981262207
Epoch 180, val loss: 1.6665401458740234
Epoch 190, training loss: 14.107930183410645 = 1.6331374645233154 + 2.0 * 6.237396240234375
Epoch 190, val loss: 1.647584080696106
Epoch 200, training loss: 14.05453872680664 = 1.606026530265808 + 2.0 * 6.2242560386657715
Epoch 200, val loss: 1.62651526927948
Epoch 210, training loss: 13.998636245727539 = 1.5754836797714233 + 2.0 * 6.211576461791992
Epoch 210, val loss: 1.6027647256851196
Epoch 220, training loss: 13.94162654876709 = 1.5415891408920288 + 2.0 * 6.200018882751465
Epoch 220, val loss: 1.57655668258667
Epoch 230, training loss: 13.884697914123535 = 1.5048214197158813 + 2.0 * 6.189938068389893
Epoch 230, val loss: 1.5480822324752808
Epoch 240, training loss: 13.824102401733398 = 1.4655441045761108 + 2.0 * 6.179279327392578
Epoch 240, val loss: 1.5178977251052856
Epoch 250, training loss: 13.766929626464844 = 1.4247417449951172 + 2.0 * 6.171093940734863
Epoch 250, val loss: 1.4867156744003296
Epoch 260, training loss: 13.708836555480957 = 1.3835984468460083 + 2.0 * 6.162619113922119
Epoch 260, val loss: 1.4552363157272339
Epoch 270, training loss: 13.651992797851562 = 1.3426928520202637 + 2.0 * 6.1546502113342285
Epoch 270, val loss: 1.4246342182159424
Epoch 280, training loss: 13.601089477539062 = 1.3028160333633423 + 2.0 * 6.149136543273926
Epoch 280, val loss: 1.3950227499008179
Epoch 290, training loss: 13.547332763671875 = 1.2641347646713257 + 2.0 * 6.141599178314209
Epoch 290, val loss: 1.3669124841690063
Epoch 300, training loss: 13.499135971069336 = 1.2267986536026 + 2.0 * 6.136168479919434
Epoch 300, val loss: 1.3400756120681763
Epoch 310, training loss: 13.457229614257812 = 1.190472960472107 + 2.0 * 6.133378505706787
Epoch 310, val loss: 1.3141227960586548
Epoch 320, training loss: 13.411452293395996 = 1.1551473140716553 + 2.0 * 6.128152370452881
Epoch 320, val loss: 1.2890925407409668
Epoch 330, training loss: 13.365771293640137 = 1.1205493211746216 + 2.0 * 6.122611045837402
Epoch 330, val loss: 1.264978051185608
Epoch 340, training loss: 13.323700904846191 = 1.0864709615707397 + 2.0 * 6.11861515045166
Epoch 340, val loss: 1.2410328388214111
Epoch 350, training loss: 13.280014038085938 = 1.05231773853302 + 2.0 * 6.1138482093811035
Epoch 350, val loss: 1.217253565788269
Epoch 360, training loss: 13.246099472045898 = 1.0182753801345825 + 2.0 * 6.113912105560303
Epoch 360, val loss: 1.1932858228683472
Epoch 370, training loss: 13.199377059936523 = 0.9846034049987793 + 2.0 * 6.107387065887451
Epoch 370, val loss: 1.169679045677185
Epoch 380, training loss: 13.159591674804688 = 0.9514268636703491 + 2.0 * 6.1040825843811035
Epoch 380, val loss: 1.146410584449768
Epoch 390, training loss: 13.129585266113281 = 0.9189491271972656 + 2.0 * 6.105318069458008
Epoch 390, val loss: 1.1235026121139526
Epoch 400, training loss: 13.084235191345215 = 0.887351393699646 + 2.0 * 6.098442077636719
Epoch 400, val loss: 1.1013398170471191
Epoch 410, training loss: 13.047138214111328 = 0.8568068742752075 + 2.0 * 6.095165729522705
Epoch 410, val loss: 1.08002507686615
Epoch 420, training loss: 13.032361030578613 = 0.8274605870246887 + 2.0 * 6.102450370788574
Epoch 420, val loss: 1.059587836265564
Epoch 430, training loss: 12.981985092163086 = 0.799248218536377 + 2.0 * 6.091368675231934
Epoch 430, val loss: 1.0404751300811768
Epoch 440, training loss: 12.947504043579102 = 0.7722288966178894 + 2.0 * 6.087637424468994
Epoch 440, val loss: 1.0224359035491943
Epoch 450, training loss: 12.91741943359375 = 0.74607914686203 + 2.0 * 6.085669994354248
Epoch 450, val loss: 1.005248785018921
Epoch 460, training loss: 12.895002365112305 = 0.7207972407341003 + 2.0 * 6.08710241317749
Epoch 460, val loss: 0.9887157082557678
Epoch 470, training loss: 12.862473487854004 = 0.6962926387786865 + 2.0 * 6.083090305328369
Epoch 470, val loss: 0.9731714129447937
Epoch 480, training loss: 12.831559181213379 = 0.6724303960800171 + 2.0 * 6.079564571380615
Epoch 480, val loss: 0.9584113359451294
Epoch 490, training loss: 12.807117462158203 = 0.6490160822868347 + 2.0 * 6.079050540924072
Epoch 490, val loss: 0.9442864060401917
Epoch 500, training loss: 12.78541374206543 = 0.6261324882507324 + 2.0 * 6.079640865325928
Epoch 500, val loss: 0.9305752515792847
Epoch 510, training loss: 12.753288269042969 = 0.6039025187492371 + 2.0 * 6.074692726135254
Epoch 510, val loss: 0.917657196521759
Epoch 520, training loss: 12.727821350097656 = 0.582070529460907 + 2.0 * 6.072875499725342
Epoch 520, val loss: 0.9053751826286316
Epoch 530, training loss: 12.706820487976074 = 0.5607337951660156 + 2.0 * 6.073043346405029
Epoch 530, val loss: 0.8936310410499573
Epoch 540, training loss: 12.68431282043457 = 0.5399273037910461 + 2.0 * 6.072192668914795
Epoch 540, val loss: 0.8825241923332214
Epoch 550, training loss: 12.661261558532715 = 0.5197716951370239 + 2.0 * 6.07074499130249
Epoch 550, val loss: 0.8722955584526062
Epoch 560, training loss: 12.634929656982422 = 0.500181257724762 + 2.0 * 6.067374229431152
Epoch 560, val loss: 0.862835705280304
Epoch 570, training loss: 12.613096237182617 = 0.4811476767063141 + 2.0 * 6.065974235534668
Epoch 570, val loss: 0.8541795015335083
Epoch 580, training loss: 12.596807479858398 = 0.4626505672931671 + 2.0 * 6.067078590393066
Epoch 580, val loss: 0.84614497423172
Epoch 590, training loss: 12.57231330871582 = 0.44480979442596436 + 2.0 * 6.063751697540283
Epoch 590, val loss: 0.8388045430183411
Epoch 600, training loss: 12.553292274475098 = 0.427308589220047 + 2.0 * 6.062991619110107
Epoch 600, val loss: 0.8321518898010254
Epoch 610, training loss: 12.531829833984375 = 0.4103080928325653 + 2.0 * 6.060760974884033
Epoch 610, val loss: 0.8261412978172302
Epoch 620, training loss: 12.515652656555176 = 0.39377155900001526 + 2.0 * 6.060940742492676
Epoch 620, val loss: 0.8207751512527466
Epoch 630, training loss: 12.496464729309082 = 0.3777371048927307 + 2.0 * 6.059363842010498
Epoch 630, val loss: 0.8158137202262878
Epoch 640, training loss: 12.477617263793945 = 0.36208614706993103 + 2.0 * 6.057765483856201
Epoch 640, val loss: 0.8114290237426758
Epoch 650, training loss: 12.457659721374512 = 0.34690549969673157 + 2.0 * 6.055377006530762
Epoch 650, val loss: 0.8074991703033447
Epoch 660, training loss: 12.44180679321289 = 0.3322053849697113 + 2.0 * 6.054800510406494
Epoch 660, val loss: 0.8038892149925232
Epoch 670, training loss: 12.431913375854492 = 0.31796127557754517 + 2.0 * 6.056975841522217
Epoch 670, val loss: 0.8005894422531128
Epoch 680, training loss: 12.41164493560791 = 0.3042930066585541 + 2.0 * 6.053676128387451
Epoch 680, val loss: 0.7977307438850403
Epoch 690, training loss: 12.396742820739746 = 0.2910758852958679 + 2.0 * 6.052833557128906
Epoch 690, val loss: 0.7953716516494751
Epoch 700, training loss: 12.38119888305664 = 0.27837347984313965 + 2.0 * 6.051412582397461
Epoch 700, val loss: 0.7933323979377747
Epoch 710, training loss: 12.365166664123535 = 0.26619547605514526 + 2.0 * 6.049485683441162
Epoch 710, val loss: 0.791795551776886
Epoch 720, training loss: 12.349776268005371 = 0.25450459122657776 + 2.0 * 6.047636032104492
Epoch 720, val loss: 0.7908133864402771
Epoch 730, training loss: 12.336793899536133 = 0.24330465495586395 + 2.0 * 6.0467448234558105
Epoch 730, val loss: 0.7902473211288452
Epoch 740, training loss: 12.350643157958984 = 0.23262456059455872 + 2.0 * 6.059009075164795
Epoch 740, val loss: 0.7900761961936951
Epoch 750, training loss: 12.313098907470703 = 0.22236090898513794 + 2.0 * 6.0453691482543945
Epoch 750, val loss: 0.7903404235839844
Epoch 760, training loss: 12.301487922668457 = 0.21265284717082977 + 2.0 * 6.044417381286621
Epoch 760, val loss: 0.7913118600845337
Epoch 770, training loss: 12.289071083068848 = 0.2033940553665161 + 2.0 * 6.0428385734558105
Epoch 770, val loss: 0.7927320599555969
Epoch 780, training loss: 12.30628776550293 = 0.19458360970020294 + 2.0 * 6.055851936340332
Epoch 780, val loss: 0.7944074273109436
Epoch 790, training loss: 12.272381782531738 = 0.18613597750663757 + 2.0 * 6.0431227684021
Epoch 790, val loss: 0.7967404723167419
Epoch 800, training loss: 12.258461952209473 = 0.1781216412782669 + 2.0 * 6.040170192718506
Epoch 800, val loss: 0.799557089805603
Epoch 810, training loss: 12.249960899353027 = 0.17042089998722076 + 2.0 * 6.039770126342773
Epoch 810, val loss: 0.8027513027191162
Epoch 820, training loss: 12.24429702758789 = 0.16304299235343933 + 2.0 * 6.0406270027160645
Epoch 820, val loss: 0.8063205480575562
Epoch 830, training loss: 12.232183456420898 = 0.15596076846122742 + 2.0 * 6.038111209869385
Epoch 830, val loss: 0.8102375864982605
Epoch 840, training loss: 12.229120254516602 = 0.14913563430309296 + 2.0 * 6.039992332458496
Epoch 840, val loss: 0.8146269917488098
Epoch 850, training loss: 12.223859786987305 = 0.14259348809719086 + 2.0 * 6.040633201599121
Epoch 850, val loss: 0.8191402554512024
Epoch 860, training loss: 12.212292671203613 = 0.13632945716381073 + 2.0 * 6.0379815101623535
Epoch 860, val loss: 0.8239591717720032
Epoch 870, training loss: 12.200724601745605 = 0.13031503558158875 + 2.0 * 6.035204887390137
Epoch 870, val loss: 0.8291928172111511
Epoch 880, training loss: 12.192803382873535 = 0.12460646778345108 + 2.0 * 6.0340986251831055
Epoch 880, val loss: 0.8346143960952759
Epoch 890, training loss: 12.206698417663574 = 0.1192273274064064 + 2.0 * 6.043735504150391
Epoch 890, val loss: 0.840292751789093
Epoch 900, training loss: 12.182531356811523 = 0.11412251740694046 + 2.0 * 6.034204483032227
Epoch 900, val loss: 0.8457651138305664
Epoch 910, training loss: 12.183127403259277 = 0.10929557681083679 + 2.0 * 6.0369157791137695
Epoch 910, val loss: 0.8517534732818604
Epoch 920, training loss: 12.167790412902832 = 0.1046629324555397 + 2.0 * 6.031563758850098
Epoch 920, val loss: 0.8578921556472778
Epoch 930, training loss: 12.161493301391602 = 0.10023806244134903 + 2.0 * 6.030627727508545
Epoch 930, val loss: 0.8642359972000122
Epoch 940, training loss: 12.160502433776855 = 0.09600528329610825 + 2.0 * 6.032248497009277
Epoch 940, val loss: 0.8708131909370422
Epoch 950, training loss: 12.155061721801758 = 0.09191743284463882 + 2.0 * 6.031572341918945
Epoch 950, val loss: 0.8771980404853821
Epoch 960, training loss: 12.149182319641113 = 0.08799254894256592 + 2.0 * 6.030594825744629
Epoch 960, val loss: 0.8838297724723816
Epoch 970, training loss: 12.143352508544922 = 0.08422420918941498 + 2.0 * 6.029564380645752
Epoch 970, val loss: 0.8905612230300903
Epoch 980, training loss: 12.139105796813965 = 0.08067309111356735 + 2.0 * 6.029216289520264
Epoch 980, val loss: 0.8974618911743164
Epoch 990, training loss: 12.133685111999512 = 0.07728410512208939 + 2.0 * 6.028200626373291
Epoch 990, val loss: 0.9042758941650391
Epoch 1000, training loss: 12.125737190246582 = 0.07396962493658066 + 2.0 * 6.025883674621582
Epoch 1000, val loss: 0.9113921523094177
Epoch 1010, training loss: 12.122051239013672 = 0.07079557329416275 + 2.0 * 6.025627613067627
Epoch 1010, val loss: 0.9185880422592163
Epoch 1020, training loss: 12.127859115600586 = 0.06777015328407288 + 2.0 * 6.0300445556640625
Epoch 1020, val loss: 0.9257420897483826
Epoch 1030, training loss: 12.125250816345215 = 0.06487731635570526 + 2.0 * 6.030186653137207
Epoch 1030, val loss: 0.9330056309700012
Epoch 1040, training loss: 12.112412452697754 = 0.06203698366880417 + 2.0 * 6.025187969207764
Epoch 1040, val loss: 0.9404592514038086
Epoch 1050, training loss: 12.106667518615723 = 0.05930579826235771 + 2.0 * 6.023680686950684
Epoch 1050, val loss: 0.9479424953460693
Epoch 1060, training loss: 12.116131782531738 = 0.05676357075572014 + 2.0 * 6.029684066772461
Epoch 1060, val loss: 0.9554229974746704
Epoch 1070, training loss: 12.10300064086914 = 0.05442630499601364 + 2.0 * 6.024287223815918
Epoch 1070, val loss: 0.9626766443252563
Epoch 1080, training loss: 12.098162651062012 = 0.052222840487957 + 2.0 * 6.022969722747803
Epoch 1080, val loss: 0.9703815579414368
Epoch 1090, training loss: 12.103693008422852 = 0.05016358196735382 + 2.0 * 6.026764869689941
Epoch 1090, val loss: 0.9777787923812866
Epoch 1100, training loss: 12.091957092285156 = 0.048238955438137054 + 2.0 * 6.021859169006348
Epoch 1100, val loss: 0.9852056503295898
Epoch 1110, training loss: 12.086265563964844 = 0.04640144854784012 + 2.0 * 6.019932270050049
Epoch 1110, val loss: 0.9927112460136414
Epoch 1120, training loss: 12.091702461242676 = 0.04467267170548439 + 2.0 * 6.023514747619629
Epoch 1120, val loss: 1.0000773668289185
Epoch 1130, training loss: 12.083892822265625 = 0.043044086545705795 + 2.0 * 6.0204243659973145
Epoch 1130, val loss: 1.0071101188659668
Epoch 1140, training loss: 12.08056354522705 = 0.04149650037288666 + 2.0 * 6.019533634185791
Epoch 1140, val loss: 1.0144121646881104
Epoch 1150, training loss: 12.088777542114258 = 0.04004140570759773 + 2.0 * 6.0243682861328125
Epoch 1150, val loss: 1.0214130878448486
Epoch 1160, training loss: 12.077725410461426 = 0.03868462145328522 + 2.0 * 6.019520282745361
Epoch 1160, val loss: 1.0283807516098022
Epoch 1170, training loss: 12.0718994140625 = 0.03737664222717285 + 2.0 * 6.017261505126953
Epoch 1170, val loss: 1.0354164838790894
Epoch 1180, training loss: 12.075298309326172 = 0.03614625334739685 + 2.0 * 6.019576072692871
Epoch 1180, val loss: 1.0422829389572144
Epoch 1190, training loss: 12.070926666259766 = 0.03496960178017616 + 2.0 * 6.017978668212891
Epoch 1190, val loss: 1.0489232540130615
Epoch 1200, training loss: 12.067322731018066 = 0.03385542705655098 + 2.0 * 6.016733646392822
Epoch 1200, val loss: 1.0556373596191406
Epoch 1210, training loss: 12.06499195098877 = 0.03279053047299385 + 2.0 * 6.016100883483887
Epoch 1210, val loss: 1.0623642206192017
Epoch 1220, training loss: 12.080147743225098 = 0.031778302043676376 + 2.0 * 6.024184703826904
Epoch 1220, val loss: 1.0686285495758057
Epoch 1230, training loss: 12.060888290405273 = 0.030826324597001076 + 2.0 * 6.015030860900879
Epoch 1230, val loss: 1.0748738050460815
Epoch 1240, training loss: 12.057999610900879 = 0.029909105971455574 + 2.0 * 6.014045238494873
Epoch 1240, val loss: 1.0813519954681396
Epoch 1250, training loss: 12.056163787841797 = 0.029032524675130844 + 2.0 * 6.013565540313721
Epoch 1250, val loss: 1.087572693824768
Epoch 1260, training loss: 12.071187973022461 = 0.02819858491420746 + 2.0 * 6.0214948654174805
Epoch 1260, val loss: 1.0935178995132446
Epoch 1270, training loss: 12.057765007019043 = 0.027396487072110176 + 2.0 * 6.01518440246582
Epoch 1270, val loss: 1.0996415615081787
Epoch 1280, training loss: 12.05206298828125 = 0.026635410264134407 + 2.0 * 6.01271390914917
Epoch 1280, val loss: 1.1056339740753174
Epoch 1290, training loss: 12.06344223022461 = 0.025912215933203697 + 2.0 * 6.018764972686768
Epoch 1290, val loss: 1.1115515232086182
Epoch 1300, training loss: 12.055495262145996 = 0.02520325407385826 + 2.0 * 6.015145778656006
Epoch 1300, val loss: 1.1170215606689453
Epoch 1310, training loss: 12.0487642288208 = 0.024538930505514145 + 2.0 * 6.012112617492676
Epoch 1310, val loss: 1.1228071451187134
Epoch 1320, training loss: 12.044281959533691 = 0.0238958727568388 + 2.0 * 6.01019287109375
Epoch 1320, val loss: 1.1285803318023682
Epoch 1330, training loss: 12.043842315673828 = 0.0232764333486557 + 2.0 * 6.01028299331665
Epoch 1330, val loss: 1.1342246532440186
Epoch 1340, training loss: 12.065543174743652 = 0.02268112637102604 + 2.0 * 6.021430969238281
Epoch 1340, val loss: 1.1395559310913086
Epoch 1350, training loss: 12.041337013244629 = 0.022112607955932617 + 2.0 * 6.009612083435059
Epoch 1350, val loss: 1.144831657409668
Epoch 1360, training loss: 12.041481018066406 = 0.021566670387983322 + 2.0 * 6.009957313537598
Epoch 1360, val loss: 1.150395154953003
Epoch 1370, training loss: 12.037718772888184 = 0.021040331572294235 + 2.0 * 6.0083394050598145
Epoch 1370, val loss: 1.1557484865188599
Epoch 1380, training loss: 12.046209335327148 = 0.020532840862870216 + 2.0 * 6.012838363647461
Epoch 1380, val loss: 1.1609556674957275
Epoch 1390, training loss: 12.039011001586914 = 0.020043302327394485 + 2.0 * 6.009483814239502
Epoch 1390, val loss: 1.1659746170043945
Epoch 1400, training loss: 12.040515899658203 = 0.019572973251342773 + 2.0 * 6.010471343994141
Epoch 1400, val loss: 1.1711302995681763
Epoch 1410, training loss: 12.038522720336914 = 0.019120756536722183 + 2.0 * 6.009700775146484
Epoch 1410, val loss: 1.176109790802002
Epoch 1420, training loss: 12.037299156188965 = 0.01868770271539688 + 2.0 * 6.009305953979492
Epoch 1420, val loss: 1.1809730529785156
Epoch 1430, training loss: 12.032617568969727 = 0.01827031560242176 + 2.0 * 6.007173538208008
Epoch 1430, val loss: 1.1859747171401978
Epoch 1440, training loss: 12.031383514404297 = 0.017864538356661797 + 2.0 * 6.0067596435546875
Epoch 1440, val loss: 1.1908780336380005
Epoch 1450, training loss: 12.035399436950684 = 0.017473066225647926 + 2.0 * 6.008963108062744
Epoch 1450, val loss: 1.1956692934036255
Epoch 1460, training loss: 12.029006958007812 = 0.017088769003748894 + 2.0 * 6.0059590339660645
Epoch 1460, val loss: 1.200326919555664
Epoch 1470, training loss: 12.03085708618164 = 0.01671936735510826 + 2.0 * 6.007068634033203
Epoch 1470, val loss: 1.2050126791000366
Epoch 1480, training loss: 12.028606414794922 = 0.016365008428692818 + 2.0 * 6.006120681762695
Epoch 1480, val loss: 1.2095814943313599
Epoch 1490, training loss: 12.02633285522461 = 0.016021734103560448 + 2.0 * 6.005155563354492
Epoch 1490, val loss: 1.2142208814620972
Epoch 1500, training loss: 12.025674819946289 = 0.015690235421061516 + 2.0 * 6.004992485046387
Epoch 1500, val loss: 1.2187834978103638
Epoch 1510, training loss: 12.040255546569824 = 0.015368517488241196 + 2.0 * 6.012443542480469
Epoch 1510, val loss: 1.2230027914047241
Epoch 1520, training loss: 12.028108596801758 = 0.015067138709127903 + 2.0 * 6.006520748138428
Epoch 1520, val loss: 1.2272650003433228
Epoch 1530, training loss: 12.023728370666504 = 0.014766649343073368 + 2.0 * 6.004480838775635
Epoch 1530, val loss: 1.2316330671310425
Epoch 1540, training loss: 12.02059268951416 = 0.014475107192993164 + 2.0 * 6.003058910369873
Epoch 1540, val loss: 1.2360566854476929
Epoch 1550, training loss: 12.020923614501953 = 0.01419405173510313 + 2.0 * 6.003364562988281
Epoch 1550, val loss: 1.2404327392578125
Epoch 1560, training loss: 12.02404499053955 = 0.013918714597821236 + 2.0 * 6.005063056945801
Epoch 1560, val loss: 1.2446624040603638
Epoch 1570, training loss: 12.023311614990234 = 0.013653400354087353 + 2.0 * 6.004828929901123
Epoch 1570, val loss: 1.2487211227416992
Epoch 1580, training loss: 12.021819114685059 = 0.013396454975008965 + 2.0 * 6.00421142578125
Epoch 1580, val loss: 1.2527540922164917
Epoch 1590, training loss: 12.02165412902832 = 0.013146582059562206 + 2.0 * 6.00425386428833
Epoch 1590, val loss: 1.2567776441574097
Epoch 1600, training loss: 12.016314506530762 = 0.012906686402857304 + 2.0 * 6.00170373916626
Epoch 1600, val loss: 1.2607735395431519
Epoch 1610, training loss: 12.01484203338623 = 0.012672504410147667 + 2.0 * 6.001084804534912
Epoch 1610, val loss: 1.2648369073867798
Epoch 1620, training loss: 12.028294563293457 = 0.01244273316115141 + 2.0 * 6.007925987243652
Epoch 1620, val loss: 1.2687792778015137
Epoch 1630, training loss: 12.019429206848145 = 0.012223780155181885 + 2.0 * 6.003602504730225
Epoch 1630, val loss: 1.2723158597946167
Epoch 1640, training loss: 12.015402793884277 = 0.012009326368570328 + 2.0 * 6.001696586608887
Epoch 1640, val loss: 1.2761805057525635
Epoch 1650, training loss: 12.011092185974121 = 0.011799265630543232 + 2.0 * 5.9996466636657715
Epoch 1650, val loss: 1.280081033706665
Epoch 1660, training loss: 12.011786460876465 = 0.011595245450735092 + 2.0 * 6.000095844268799
Epoch 1660, val loss: 1.2838938236236572
Epoch 1670, training loss: 12.025107383728027 = 0.011397728696465492 + 2.0 * 6.006855010986328
Epoch 1670, val loss: 1.2875020503997803
Epoch 1680, training loss: 12.017705917358398 = 0.011202903464436531 + 2.0 * 6.003251552581787
Epoch 1680, val loss: 1.2908376455307007
Epoch 1690, training loss: 12.009428977966309 = 0.011016265489161015 + 2.0 * 5.99920654296875
Epoch 1690, val loss: 1.2944645881652832
Epoch 1700, training loss: 12.009111404418945 = 0.010834941640496254 + 2.0 * 5.999138355255127
Epoch 1700, val loss: 1.298219084739685
Epoch 1710, training loss: 12.01706314086914 = 0.010656100697815418 + 2.0 * 6.003203392028809
Epoch 1710, val loss: 1.301756501197815
Epoch 1720, training loss: 12.009882926940918 = 0.0104848463088274 + 2.0 * 5.999699115753174
Epoch 1720, val loss: 1.304903268814087
Epoch 1730, training loss: 12.005985260009766 = 0.010316715575754642 + 2.0 * 5.997834205627441
Epoch 1730, val loss: 1.3084125518798828
Epoch 1740, training loss: 12.008726119995117 = 0.01015223003923893 + 2.0 * 5.999287128448486
Epoch 1740, val loss: 1.3118692636489868
Epoch 1750, training loss: 12.01151180267334 = 0.009992305189371109 + 2.0 * 6.000759601593018
Epoch 1750, val loss: 1.3150947093963623
Epoch 1760, training loss: 12.009870529174805 = 0.009836237877607346 + 2.0 * 6.000017166137695
Epoch 1760, val loss: 1.3184833526611328
Epoch 1770, training loss: 12.006815910339355 = 0.00968489795923233 + 2.0 * 5.998565673828125
Epoch 1770, val loss: 1.3215762376785278
Epoch 1780, training loss: 12.00413703918457 = 0.009537532925605774 + 2.0 * 5.997299671173096
Epoch 1780, val loss: 1.3248629570007324
Epoch 1790, training loss: 12.003716468811035 = 0.009393836371600628 + 2.0 * 5.997161388397217
Epoch 1790, val loss: 1.328038215637207
Epoch 1800, training loss: 12.005297660827637 = 0.009252729825675488 + 2.0 * 5.998022556304932
Epoch 1800, val loss: 1.3311980962753296
Epoch 1810, training loss: 12.003546714782715 = 0.009115105494856834 + 2.0 * 5.997215747833252
Epoch 1810, val loss: 1.3343771696090698
Epoch 1820, training loss: 12.00617790222168 = 0.008982615545392036 + 2.0 * 5.998597621917725
Epoch 1820, val loss: 1.3373528718948364
Epoch 1830, training loss: 12.021836280822754 = 0.008855895139276981 + 2.0 * 6.006490230560303
Epoch 1830, val loss: 1.3402317762374878
Epoch 1840, training loss: 12.004801750183105 = 0.00872254278510809 + 2.0 * 5.998039722442627
Epoch 1840, val loss: 1.343140721321106
Epoch 1850, training loss: 11.998952865600586 = 0.008600268512964249 + 2.0 * 5.995176315307617
Epoch 1850, val loss: 1.346195101737976
Epoch 1860, training loss: 11.997259140014648 = 0.008478764444589615 + 2.0 * 5.99439001083374
Epoch 1860, val loss: 1.3493022918701172
Epoch 1870, training loss: 11.996810913085938 = 0.008359950967133045 + 2.0 * 5.99422550201416
Epoch 1870, val loss: 1.3522326946258545
Epoch 1880, training loss: 12.009462356567383 = 0.008243005722761154 + 2.0 * 6.000609874725342
Epoch 1880, val loss: 1.3550828695297241
Epoch 1890, training loss: 12.01318645477295 = 0.008127355948090553 + 2.0 * 6.002529621124268
Epoch 1890, val loss: 1.357657790184021
Epoch 1900, training loss: 11.999085426330566 = 0.008020651526749134 + 2.0 * 5.995532512664795
Epoch 1900, val loss: 1.3603230714797974
Epoch 1910, training loss: 11.995138168334961 = 0.007911122404038906 + 2.0 * 5.9936137199401855
Epoch 1910, val loss: 1.3633555173873901
Epoch 1920, training loss: 11.994345664978027 = 0.007805444300174713 + 2.0 * 5.993269920349121
Epoch 1920, val loss: 1.3662428855895996
Epoch 1930, training loss: 12.001995086669922 = 0.007702953182160854 + 2.0 * 5.997146129608154
Epoch 1930, val loss: 1.3690497875213623
Epoch 1940, training loss: 11.996044158935547 = 0.007601035758852959 + 2.0 * 5.9942216873168945
Epoch 1940, val loss: 1.3713698387145996
Epoch 1950, training loss: 11.995429992675781 = 0.007500790990889072 + 2.0 * 5.993964672088623
Epoch 1950, val loss: 1.3740761280059814
Epoch 1960, training loss: 11.995344161987305 = 0.007404114119708538 + 2.0 * 5.993969917297363
Epoch 1960, val loss: 1.3766894340515137
Epoch 1970, training loss: 11.9916410446167 = 0.007309679873287678 + 2.0 * 5.992165565490723
Epoch 1970, val loss: 1.379419207572937
Epoch 1980, training loss: 11.992532730102539 = 0.00721623795107007 + 2.0 * 5.9926581382751465
Epoch 1980, val loss: 1.3820451498031616
Epoch 1990, training loss: 12.006265640258789 = 0.00712399510666728 + 2.0 * 5.999570846557617
Epoch 1990, val loss: 1.3845562934875488
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8074
Overall ASR: 0.8745
Flip ASR: 0.8489/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.7039737701416 = 1.9562993049621582 + 2.0 * 8.3738374710083
Epoch 0, val loss: 1.9498281478881836
Epoch 10, training loss: 18.69131088256836 = 1.9451186656951904 + 2.0 * 8.373096466064453
Epoch 10, val loss: 1.9380568265914917
Epoch 20, training loss: 18.666202545166016 = 1.931247591972351 + 2.0 * 8.367477416992188
Epoch 20, val loss: 1.9232866764068604
Epoch 30, training loss: 18.563138961791992 = 1.9127651453018188 + 2.0 * 8.325186729431152
Epoch 30, val loss: 1.9038509130477905
Epoch 40, training loss: 17.886594772338867 = 1.892559289932251 + 2.0 * 7.9970173835754395
Epoch 40, val loss: 1.8835991621017456
Epoch 50, training loss: 16.380769729614258 = 1.8716989755630493 + 2.0 * 7.25453519821167
Epoch 50, val loss: 1.863286018371582
Epoch 60, training loss: 15.635807991027832 = 1.8574906587600708 + 2.0 * 6.889158725738525
Epoch 60, val loss: 1.84929621219635
Epoch 70, training loss: 15.219806671142578 = 1.845199704170227 + 2.0 * 6.68730354309082
Epoch 70, val loss: 1.8370816707611084
Epoch 80, training loss: 14.919268608093262 = 1.833514928817749 + 2.0 * 6.542876720428467
Epoch 80, val loss: 1.8249540328979492
Epoch 90, training loss: 14.71274471282959 = 1.8209182024002075 + 2.0 * 6.445913314819336
Epoch 90, val loss: 1.8119542598724365
Epoch 100, training loss: 14.567655563354492 = 1.807918906211853 + 2.0 * 6.379868507385254
Epoch 100, val loss: 1.7987803220748901
Epoch 110, training loss: 14.464388847351074 = 1.7950150966644287 + 2.0 * 6.334686756134033
Epoch 110, val loss: 1.7857784032821655
Epoch 120, training loss: 14.384600639343262 = 1.782256007194519 + 2.0 * 6.301172256469727
Epoch 120, val loss: 1.7729511260986328
Epoch 130, training loss: 14.31632137298584 = 1.7693926095962524 + 2.0 * 6.273464202880859
Epoch 130, val loss: 1.760132908821106
Epoch 140, training loss: 14.260701179504395 = 1.7560927867889404 + 2.0 * 6.2523040771484375
Epoch 140, val loss: 1.7469637393951416
Epoch 150, training loss: 14.20272445678711 = 1.741926908493042 + 2.0 * 6.230398654937744
Epoch 150, val loss: 1.7334412336349487
Epoch 160, training loss: 14.153515815734863 = 1.7265651226043701 + 2.0 * 6.213475227355957
Epoch 160, val loss: 1.719118356704712
Epoch 170, training loss: 14.1149263381958 = 1.7096384763717651 + 2.0 * 6.202643871307373
Epoch 170, val loss: 1.7037585973739624
Epoch 180, training loss: 14.065404891967773 = 1.6910301446914673 + 2.0 * 6.187187194824219
Epoch 180, val loss: 1.6872535943984985
Epoch 190, training loss: 14.021714210510254 = 1.6702126264572144 + 2.0 * 6.175750732421875
Epoch 190, val loss: 1.6691588163375854
Epoch 200, training loss: 13.9788236618042 = 1.6466518640518188 + 2.0 * 6.166085720062256
Epoch 200, val loss: 1.6489931344985962
Epoch 210, training loss: 13.937371253967285 = 1.6199893951416016 + 2.0 * 6.158690929412842
Epoch 210, val loss: 1.6265122890472412
Epoch 220, training loss: 13.890253067016602 = 1.589937448501587 + 2.0 * 6.150157928466797
Epoch 220, val loss: 1.6013891696929932
Epoch 230, training loss: 13.846552848815918 = 1.5557565689086914 + 2.0 * 6.145398139953613
Epoch 230, val loss: 1.573078989982605
Epoch 240, training loss: 13.793214797973633 = 1.5180310010910034 + 2.0 * 6.13759183883667
Epoch 240, val loss: 1.542193055152893
Epoch 250, training loss: 13.73962116241455 = 1.4766137599945068 + 2.0 * 6.131503582000732
Epoch 250, val loss: 1.5082693099975586
Epoch 260, training loss: 13.684541702270508 = 1.4313857555389404 + 2.0 * 6.126577854156494
Epoch 260, val loss: 1.4716179370880127
Epoch 270, training loss: 13.627922058105469 = 1.3828366994857788 + 2.0 * 6.122542858123779
Epoch 270, val loss: 1.4326398372650146
Epoch 280, training loss: 13.577672958374023 = 1.3320339918136597 + 2.0 * 6.122819423675537
Epoch 280, val loss: 1.3927422761917114
Epoch 290, training loss: 13.508709907531738 = 1.2807337045669556 + 2.0 * 6.113987922668457
Epoch 290, val loss: 1.352640151977539
Epoch 300, training loss: 13.44963550567627 = 1.2287427186965942 + 2.0 * 6.110446453094482
Epoch 300, val loss: 1.3127119541168213
Epoch 310, training loss: 13.390138626098633 = 1.1765692234039307 + 2.0 * 6.106784820556641
Epoch 310, val loss: 1.2732505798339844
Epoch 320, training loss: 13.340307235717773 = 1.1249810457229614 + 2.0 * 6.107663154602051
Epoch 320, val loss: 1.2348618507385254
Epoch 330, training loss: 13.278432846069336 = 1.0749456882476807 + 2.0 * 6.101743698120117
Epoch 330, val loss: 1.1985118389129639
Epoch 340, training loss: 13.221822738647461 = 1.0265214443206787 + 2.0 * 6.097650527954102
Epoch 340, val loss: 1.163983941078186
Epoch 350, training loss: 13.171184539794922 = 0.9796551465988159 + 2.0 * 6.095764636993408
Epoch 350, val loss: 1.1308412551879883
Epoch 360, training loss: 13.129542350769043 = 0.934729814529419 + 2.0 * 6.097406387329102
Epoch 360, val loss: 1.0992786884307861
Epoch 370, training loss: 13.07221508026123 = 0.8920112252235413 + 2.0 * 6.090101718902588
Epoch 370, val loss: 1.0697243213653564
Epoch 380, training loss: 13.024540901184082 = 0.8513015508651733 + 2.0 * 6.086619853973389
Epoch 380, val loss: 1.0418556928634644
Epoch 390, training loss: 12.980073928833008 = 0.8123131394386292 + 2.0 * 6.083880424499512
Epoch 390, val loss: 1.0152547359466553
Epoch 400, training loss: 12.941268920898438 = 0.7748855352401733 + 2.0 * 6.083191871643066
Epoch 400, val loss: 0.9900029897689819
Epoch 410, training loss: 12.908773422241211 = 0.7396582365036011 + 2.0 * 6.08455753326416
Epoch 410, val loss: 0.9660604000091553
Epoch 420, training loss: 12.864047050476074 = 0.7062788009643555 + 2.0 * 6.078884124755859
Epoch 420, val loss: 0.9439501166343689
Epoch 430, training loss: 12.828627586364746 = 0.6745908260345459 + 2.0 * 6.0770182609558105
Epoch 430, val loss: 0.9230968952178955
Epoch 440, training loss: 12.80185604095459 = 0.6446616053581238 + 2.0 * 6.078597068786621
Epoch 440, val loss: 0.9035964012145996
Epoch 450, training loss: 12.761665344238281 = 0.6163339614868164 + 2.0 * 6.072665691375732
Epoch 450, val loss: 0.8855655789375305
Epoch 460, training loss: 12.72890567779541 = 0.5893226861953735 + 2.0 * 6.069791316986084
Epoch 460, val loss: 0.8687618970870972
Epoch 470, training loss: 12.703193664550781 = 0.5634059309959412 + 2.0 * 6.069893836975098
Epoch 470, val loss: 0.8530418872833252
Epoch 480, training loss: 12.684565544128418 = 0.5388896465301514 + 2.0 * 6.072837829589844
Epoch 480, val loss: 0.8384462594985962
Epoch 490, training loss: 12.64720630645752 = 0.5155227184295654 + 2.0 * 6.0658416748046875
Epoch 490, val loss: 0.8252361416816711
Epoch 500, training loss: 12.618975639343262 = 0.4930822253227234 + 2.0 * 6.062946796417236
Epoch 500, val loss: 0.8129850625991821
Epoch 510, training loss: 12.59404182434082 = 0.47138065099716187 + 2.0 * 6.061330795288086
Epoch 510, val loss: 0.8015753030776978
Epoch 520, training loss: 12.584383964538574 = 0.450417160987854 + 2.0 * 6.066983222961426
Epoch 520, val loss: 0.7910553812980652
Epoch 530, training loss: 12.550596237182617 = 0.4302378296852112 + 2.0 * 6.060179233551025
Epoch 530, val loss: 0.7814270853996277
Epoch 540, training loss: 12.529579162597656 = 0.41092926263809204 + 2.0 * 6.059324741363525
Epoch 540, val loss: 0.7727115750312805
Epoch 550, training loss: 12.50511646270752 = 0.3922559916973114 + 2.0 * 6.056430339813232
Epoch 550, val loss: 0.7647717595100403
Epoch 560, training loss: 12.484162330627441 = 0.37413033843040466 + 2.0 * 6.055016040802002
Epoch 560, val loss: 0.7573850154876709
Epoch 570, training loss: 12.473134994506836 = 0.3565429747104645 + 2.0 * 6.058296203613281
Epoch 570, val loss: 0.7505732774734497
Epoch 580, training loss: 12.447360038757324 = 0.33970287442207336 + 2.0 * 6.053828716278076
Epoch 580, val loss: 0.7442914843559265
Epoch 590, training loss: 12.425426483154297 = 0.32348448038101196 + 2.0 * 6.050971031188965
Epoch 590, val loss: 0.7386372685432434
Epoch 600, training loss: 12.407432556152344 = 0.30777135491371155 + 2.0 * 6.049830436706543
Epoch 600, val loss: 0.733341634273529
Epoch 610, training loss: 12.402375221252441 = 0.29256463050842285 + 2.0 * 6.054905414581299
Epoch 610, val loss: 0.7283841967582703
Epoch 620, training loss: 12.382487297058105 = 0.2779766917228699 + 2.0 * 6.052255153656006
Epoch 620, val loss: 0.723906934261322
Epoch 630, training loss: 12.359764099121094 = 0.26396283507347107 + 2.0 * 6.047900676727295
Epoch 630, val loss: 0.7197698354721069
Epoch 640, training loss: 12.341325759887695 = 0.25051984190940857 + 2.0 * 6.045403003692627
Epoch 640, val loss: 0.7159220576286316
Epoch 650, training loss: 12.33066177368164 = 0.23759368062019348 + 2.0 * 6.046534061431885
Epoch 650, val loss: 0.7124751210212708
Epoch 660, training loss: 12.318419456481934 = 0.22519823908805847 + 2.0 * 6.0466108322143555
Epoch 660, val loss: 0.709454357624054
Epoch 670, training loss: 12.303691864013672 = 0.21347111463546753 + 2.0 * 6.04511022567749
Epoch 670, val loss: 0.7068657279014587
Epoch 680, training loss: 12.287004470825195 = 0.202287495136261 + 2.0 * 6.0423583984375
Epoch 680, val loss: 0.7046895623207092
Epoch 690, training loss: 12.271750450134277 = 0.1916862279176712 + 2.0 * 6.040031909942627
Epoch 690, val loss: 0.7030148506164551
Epoch 700, training loss: 12.281961441040039 = 0.18163616955280304 + 2.0 * 6.0501627922058105
Epoch 700, val loss: 0.7016952633857727
Epoch 710, training loss: 12.249439239501953 = 0.17225632071495056 + 2.0 * 6.038591384887695
Epoch 710, val loss: 0.7009307742118835
Epoch 720, training loss: 12.238996505737305 = 0.1634426862001419 + 2.0 * 6.037776947021484
Epoch 720, val loss: 0.7006529569625854
Epoch 730, training loss: 12.228160858154297 = 0.15512658655643463 + 2.0 * 6.036517143249512
Epoch 730, val loss: 0.7007384300231934
Epoch 740, training loss: 12.22804069519043 = 0.14731647074222565 + 2.0 * 6.0403618812561035
Epoch 740, val loss: 0.701141893863678
Epoch 750, training loss: 12.21403980255127 = 0.14007121324539185 + 2.0 * 6.036984443664551
Epoch 750, val loss: 0.7018866539001465
Epoch 760, training loss: 12.201489448547363 = 0.133274644613266 + 2.0 * 6.034107208251953
Epoch 760, val loss: 0.7030691504478455
Epoch 770, training loss: 12.19837474822998 = 0.12688931822776794 + 2.0 * 6.03574275970459
Epoch 770, val loss: 0.7044762969017029
Epoch 780, training loss: 12.18442153930664 = 0.12091872096061707 + 2.0 * 6.03175163269043
Epoch 780, val loss: 0.7061787247657776
Epoch 790, training loss: 12.177845001220703 = 0.11529672890901566 + 2.0 * 6.031274318695068
Epoch 790, val loss: 0.7081416845321655
Epoch 800, training loss: 12.185426712036133 = 0.11001619696617126 + 2.0 * 6.037705421447754
Epoch 800, val loss: 0.7103342413902283
Epoch 810, training loss: 12.168861389160156 = 0.10504859685897827 + 2.0 * 6.031906604766846
Epoch 810, val loss: 0.7126830816268921
Epoch 820, training loss: 12.162662506103516 = 0.10039535909891129 + 2.0 * 6.031133651733398
Epoch 820, val loss: 0.7152594327926636
Epoch 830, training loss: 12.154290199279785 = 0.09600009024143219 + 2.0 * 6.029145240783691
Epoch 830, val loss: 0.7179455757141113
Epoch 840, training loss: 12.164104461669922 = 0.09188741445541382 + 2.0 * 6.036108493804932
Epoch 840, val loss: 0.7208667993545532
Epoch 850, training loss: 12.143839836120605 = 0.08799167722463608 + 2.0 * 6.027924060821533
Epoch 850, val loss: 0.7237446308135986
Epoch 860, training loss: 12.136821746826172 = 0.0843595638871193 + 2.0 * 6.026231288909912
Epoch 860, val loss: 0.7268918752670288
Epoch 870, training loss: 12.130541801452637 = 0.08091144263744354 + 2.0 * 6.024815082550049
Epoch 870, val loss: 0.7300910949707031
Epoch 880, training loss: 12.130036354064941 = 0.07764236629009247 + 2.0 * 6.0261969566345215
Epoch 880, val loss: 0.733383059501648
Epoch 890, training loss: 12.127220153808594 = 0.07458102703094482 + 2.0 * 6.02631950378418
Epoch 890, val loss: 0.7366819381713867
Epoch 900, training loss: 12.11884880065918 = 0.07168214023113251 + 2.0 * 6.02358341217041
Epoch 900, val loss: 0.7401146292686462
Epoch 910, training loss: 12.115307807922363 = 0.06894955039024353 + 2.0 * 6.023179054260254
Epoch 910, val loss: 0.7436285018920898
Epoch 920, training loss: 12.114259719848633 = 0.0663541704416275 + 2.0 * 6.023952960968018
Epoch 920, val loss: 0.747168242931366
Epoch 930, training loss: 12.107553482055664 = 0.06389433145523071 + 2.0 * 6.021829605102539
Epoch 930, val loss: 0.7506922483444214
Epoch 940, training loss: 12.117939949035645 = 0.0615655817091465 + 2.0 * 6.028187274932861
Epoch 940, val loss: 0.7542935609817505
Epoch 950, training loss: 12.101463317871094 = 0.05938924103975296 + 2.0 * 6.0210371017456055
Epoch 950, val loss: 0.7579406499862671
Epoch 960, training loss: 12.096421241760254 = 0.05730919912457466 + 2.0 * 6.019556045532227
Epoch 960, val loss: 0.7616512179374695
Epoch 970, training loss: 12.092734336853027 = 0.05532454699277878 + 2.0 * 6.018704891204834
Epoch 970, val loss: 0.765350878238678
Epoch 980, training loss: 12.090764045715332 = 0.0534297376871109 + 2.0 * 6.018667221069336
Epoch 980, val loss: 0.7690942883491516
Epoch 990, training loss: 12.094938278198242 = 0.051626384258270264 + 2.0 * 6.021656036376953
Epoch 990, val loss: 0.7727945446968079
Epoch 1000, training loss: 12.086684226989746 = 0.049918923527002335 + 2.0 * 6.018382549285889
Epoch 1000, val loss: 0.776537299156189
Epoch 1010, training loss: 12.083176612854004 = 0.048297058790922165 + 2.0 * 6.017439842224121
Epoch 1010, val loss: 0.780335009098053
Epoch 1020, training loss: 12.085733413696289 = 0.046747177839279175 + 2.0 * 6.019493103027344
Epoch 1020, val loss: 0.7840797305107117
Epoch 1030, training loss: 12.076445579528809 = 0.045264832675457 + 2.0 * 6.015590190887451
Epoch 1030, val loss: 0.7878044843673706
Epoch 1040, training loss: 12.074498176574707 = 0.04385293647646904 + 2.0 * 6.015322685241699
Epoch 1040, val loss: 0.7915953397750854
Epoch 1050, training loss: 12.082769393920898 = 0.042494241148233414 + 2.0 * 6.020137786865234
Epoch 1050, val loss: 0.7953215837478638
Epoch 1060, training loss: 12.070673942565918 = 0.041209857910871506 + 2.0 * 6.0147318840026855
Epoch 1060, val loss: 0.7991582155227661
Epoch 1070, training loss: 12.067499160766602 = 0.039970170706510544 + 2.0 * 6.013764381408691
Epoch 1070, val loss: 0.8028775453567505
Epoch 1080, training loss: 12.070796966552734 = 0.03878724202513695 + 2.0 * 6.016005039215088
Epoch 1080, val loss: 0.8066539764404297
Epoch 1090, training loss: 12.063384056091309 = 0.037653952836990356 + 2.0 * 6.01286506652832
Epoch 1090, val loss: 0.8104225993156433
Epoch 1100, training loss: 12.068451881408691 = 0.036567360162734985 + 2.0 * 6.015942096710205
Epoch 1100, val loss: 0.8141090273857117
Epoch 1110, training loss: 12.069449424743652 = 0.0355316624045372 + 2.0 * 6.016958713531494
Epoch 1110, val loss: 0.8179361820220947
Epoch 1120, training loss: 12.057430267333984 = 0.03453986719250679 + 2.0 * 6.011445045471191
Epoch 1120, val loss: 0.821545422077179
Epoch 1130, training loss: 12.057191848754883 = 0.0335879847407341 + 2.0 * 6.011801719665527
Epoch 1130, val loss: 0.8252273201942444
Epoch 1140, training loss: 12.053299903869629 = 0.03266725316643715 + 2.0 * 6.010316371917725
Epoch 1140, val loss: 0.8289299011230469
Epoch 1150, training loss: 12.058948516845703 = 0.031777963042259216 + 2.0 * 6.013585090637207
Epoch 1150, val loss: 0.8325639963150024
Epoch 1160, training loss: 12.049567222595215 = 0.030932143330574036 + 2.0 * 6.009317398071289
Epoch 1160, val loss: 0.8362522125244141
Epoch 1170, training loss: 12.049695014953613 = 0.030117010697722435 + 2.0 * 6.009788990020752
Epoch 1170, val loss: 0.8398816585540771
Epoch 1180, training loss: 12.063233375549316 = 0.02932797558605671 + 2.0 * 6.0169525146484375
Epoch 1180, val loss: 0.8434370160102844
Epoch 1190, training loss: 12.053692817687988 = 0.028578897938132286 + 2.0 * 6.012557029724121
Epoch 1190, val loss: 0.8470704555511475
Epoch 1200, training loss: 12.045087814331055 = 0.027852052822709084 + 2.0 * 6.008617877960205
Epoch 1200, val loss: 0.8505612015724182
Epoch 1210, training loss: 12.050627708435059 = 0.02715490572154522 + 2.0 * 6.0117363929748535
Epoch 1210, val loss: 0.8541252613067627
Epoch 1220, training loss: 12.04162311553955 = 0.02648424543440342 + 2.0 * 6.007569313049316
Epoch 1220, val loss: 0.8576446175575256
Epoch 1230, training loss: 12.03886604309082 = 0.02583344094455242 + 2.0 * 6.006516456604004
Epoch 1230, val loss: 0.8611273765563965
Epoch 1240, training loss: 12.046392440795898 = 0.025203930214047432 + 2.0 * 6.010594367980957
Epoch 1240, val loss: 0.8645807504653931
Epoch 1250, training loss: 12.036952018737793 = 0.024600883945822716 + 2.0 * 6.006175518035889
Epoch 1250, val loss: 0.8680585026741028
Epoch 1260, training loss: 12.034064292907715 = 0.024018920958042145 + 2.0 * 6.0050225257873535
Epoch 1260, val loss: 0.8714953064918518
Epoch 1270, training loss: 12.034272193908691 = 0.023455843329429626 + 2.0 * 6.00540828704834
Epoch 1270, val loss: 0.8749647736549377
Epoch 1280, training loss: 12.042350769042969 = 0.02291208878159523 + 2.0 * 6.009719371795654
Epoch 1280, val loss: 0.8783517479896545
Epoch 1290, training loss: 12.03336238861084 = 0.022386180236935616 + 2.0 * 6.00548791885376
Epoch 1290, val loss: 0.8817597031593323
Epoch 1300, training loss: 12.037728309631348 = 0.021881945431232452 + 2.0 * 6.007923126220703
Epoch 1300, val loss: 0.8851953148841858
Epoch 1310, training loss: 12.028907775878906 = 0.02138802595436573 + 2.0 * 6.003759860992432
Epoch 1310, val loss: 0.888473629951477
Epoch 1320, training loss: 12.029020309448242 = 0.020913973450660706 + 2.0 * 6.004053115844727
Epoch 1320, val loss: 0.8918105959892273
Epoch 1330, training loss: 12.032230377197266 = 0.020456107333302498 + 2.0 * 6.005887031555176
Epoch 1330, val loss: 0.8951687216758728
Epoch 1340, training loss: 12.027363777160645 = 0.020009173080325127 + 2.0 * 6.0036773681640625
Epoch 1340, val loss: 0.8983835577964783
Epoch 1350, training loss: 12.028882026672363 = 0.01957862637937069 + 2.0 * 6.0046515464782715
Epoch 1350, val loss: 0.9016553163528442
Epoch 1360, training loss: 12.02398681640625 = 0.01916111633181572 + 2.0 * 6.002412796020508
Epoch 1360, val loss: 0.904968798160553
Epoch 1370, training loss: 12.036470413208008 = 0.018759528174996376 + 2.0 * 6.00885534286499
Epoch 1370, val loss: 0.9082120060920715
Epoch 1380, training loss: 12.025303840637207 = 0.01836610585451126 + 2.0 * 6.003468990325928
Epoch 1380, val loss: 0.9113301038742065
Epoch 1390, training loss: 12.024188041687012 = 0.01798936165869236 + 2.0 * 6.00309944152832
Epoch 1390, val loss: 0.9145054817199707
Epoch 1400, training loss: 12.019454002380371 = 0.0176226869225502 + 2.0 * 6.00091552734375
Epoch 1400, val loss: 0.9176688194274902
Epoch 1410, training loss: 12.020522117614746 = 0.01726563088595867 + 2.0 * 6.001628398895264
Epoch 1410, val loss: 0.9208080172538757
Epoch 1420, training loss: 12.021869659423828 = 0.01691748946905136 + 2.0 * 6.002476215362549
Epoch 1420, val loss: 0.9238848090171814
Epoch 1430, training loss: 12.024582862854004 = 0.01658080890774727 + 2.0 * 6.004001140594482
Epoch 1430, val loss: 0.9269834756851196
Epoch 1440, training loss: 12.01994800567627 = 0.016256852075457573 + 2.0 * 6.001845359802246
Epoch 1440, val loss: 0.9301162362098694
Epoch 1450, training loss: 12.016053199768066 = 0.015941916033625603 + 2.0 * 6.00005578994751
Epoch 1450, val loss: 0.9331470131874084
Epoch 1460, training loss: 12.016803741455078 = 0.01563524827361107 + 2.0 * 6.000584125518799
Epoch 1460, val loss: 0.9361867904663086
Epoch 1470, training loss: 12.013607025146484 = 0.015336202457547188 + 2.0 * 5.999135494232178
Epoch 1470, val loss: 0.9391761422157288
Epoch 1480, training loss: 12.01342487335205 = 0.01504443772137165 + 2.0 * 5.999190330505371
Epoch 1480, val loss: 0.9421247839927673
Epoch 1490, training loss: 12.031428337097168 = 0.014766104519367218 + 2.0 * 6.008331298828125
Epoch 1490, val loss: 0.9451626539230347
Epoch 1500, training loss: 12.016348838806152 = 0.01448803674429655 + 2.0 * 6.000930309295654
Epoch 1500, val loss: 0.9478817582130432
Epoch 1510, training loss: 12.011163711547852 = 0.014226038008928299 + 2.0 * 5.99846887588501
Epoch 1510, val loss: 0.9508718848228455
Epoch 1520, training loss: 12.009600639343262 = 0.013965463265776634 + 2.0 * 5.997817516326904
Epoch 1520, val loss: 0.9537337422370911
Epoch 1530, training loss: 12.021430015563965 = 0.013714269734919071 + 2.0 * 6.0038580894470215
Epoch 1530, val loss: 0.9566280841827393
Epoch 1540, training loss: 12.012237548828125 = 0.01346882339566946 + 2.0 * 5.99938440322876
Epoch 1540, val loss: 0.9595036506652832
Epoch 1550, training loss: 12.008691787719727 = 0.013231172226369381 + 2.0 * 5.997730255126953
Epoch 1550, val loss: 0.9623063802719116
Epoch 1560, training loss: 12.027373313903809 = 0.01300113182514906 + 2.0 * 6.007185935974121
Epoch 1560, val loss: 0.9651374220848083
Epoch 1570, training loss: 12.00927734375 = 0.012774438597261906 + 2.0 * 5.998251438140869
Epoch 1570, val loss: 0.9678143262863159
Epoch 1580, training loss: 12.003388404846191 = 0.012558119371533394 + 2.0 * 5.995415210723877
Epoch 1580, val loss: 0.9705556035041809
Epoch 1590, training loss: 12.002229690551758 = 0.01234449166804552 + 2.0 * 5.994942665100098
Epoch 1590, val loss: 0.9733484387397766
Epoch 1600, training loss: 12.001839637756348 = 0.012133941054344177 + 2.0 * 5.9948530197143555
Epoch 1600, val loss: 0.976066529750824
Epoch 1610, training loss: 12.016168594360352 = 0.011928495019674301 + 2.0 * 6.002120018005371
Epoch 1610, val loss: 0.9788320064544678
Epoch 1620, training loss: 12.014585494995117 = 0.01173356082290411 + 2.0 * 6.001425743103027
Epoch 1620, val loss: 0.981485903263092
Epoch 1630, training loss: 12.002287864685059 = 0.011539598926901817 + 2.0 * 5.9953742027282715
Epoch 1630, val loss: 0.9840624928474426
Epoch 1640, training loss: 12.007730484008789 = 0.011352770030498505 + 2.0 * 5.9981889724731445
Epoch 1640, val loss: 0.9867337942123413
Epoch 1650, training loss: 11.99926471710205 = 0.011171083897352219 + 2.0 * 5.994046688079834
Epoch 1650, val loss: 0.9893679022789001
Epoch 1660, training loss: 11.998306274414062 = 0.010992802679538727 + 2.0 * 5.993656635284424
Epoch 1660, val loss: 0.9919896721839905
Epoch 1670, training loss: 11.99925708770752 = 0.010817442089319229 + 2.0 * 5.994219779968262
Epoch 1670, val loss: 0.9945884346961975
Epoch 1680, training loss: 12.00992488861084 = 0.010647844523191452 + 2.0 * 5.999638557434082
Epoch 1680, val loss: 0.9971763491630554
Epoch 1690, training loss: 11.99987506866455 = 0.01047925278544426 + 2.0 * 5.9946980476379395
Epoch 1690, val loss: 0.9995670914649963
Epoch 1700, training loss: 11.996851921081543 = 0.01031798217445612 + 2.0 * 5.993267059326172
Epoch 1700, val loss: 1.002120852470398
Epoch 1710, training loss: 12.000410079956055 = 0.010159597732126713 + 2.0 * 5.9951252937316895
Epoch 1710, val loss: 1.0046288967132568
Epoch 1720, training loss: 11.998741149902344 = 0.01000547781586647 + 2.0 * 5.994367599487305
Epoch 1720, val loss: 1.007175326347351
Epoch 1730, training loss: 11.995160102844238 = 0.00985379796475172 + 2.0 * 5.9926533699035645
Epoch 1730, val loss: 1.0095347166061401
Epoch 1740, training loss: 11.99453353881836 = 0.00970651488751173 + 2.0 * 5.992413520812988
Epoch 1740, val loss: 1.0119646787643433
Epoch 1750, training loss: 11.99519157409668 = 0.009562558494508266 + 2.0 * 5.992814540863037
Epoch 1750, val loss: 1.0143934488296509
Epoch 1760, training loss: 12.009267807006836 = 0.009421027265489101 + 2.0 * 5.999923229217529
Epoch 1760, val loss: 1.0167627334594727
Epoch 1770, training loss: 11.996458053588867 = 0.009286349639296532 + 2.0 * 5.99358606338501
Epoch 1770, val loss: 1.0191895961761475
Epoch 1780, training loss: 11.992293357849121 = 0.009153245016932487 + 2.0 * 5.991569995880127
Epoch 1780, val loss: 1.0214948654174805
Epoch 1790, training loss: 11.990966796875 = 0.009022039361298084 + 2.0 * 5.990972518920898
Epoch 1790, val loss: 1.0239181518554688
Epoch 1800, training loss: 11.99265193939209 = 0.00889270007610321 + 2.0 * 5.991879463195801
Epoch 1800, val loss: 1.0262806415557861
Epoch 1810, training loss: 12.00244426727295 = 0.00876798015087843 + 2.0 * 5.996838092803955
Epoch 1810, val loss: 1.0286242961883545
Epoch 1820, training loss: 11.991424560546875 = 0.008644753135740757 + 2.0 * 5.991389751434326
Epoch 1820, val loss: 1.0308232307434082
Epoch 1830, training loss: 11.989836692810059 = 0.008525053039193153 + 2.0 * 5.990655899047852
Epoch 1830, val loss: 1.0330804586410522
Epoch 1840, training loss: 11.991738319396973 = 0.008407886140048504 + 2.0 * 5.991665363311768
Epoch 1840, val loss: 1.0353963375091553
Epoch 1850, training loss: 11.990288734436035 = 0.00829324685037136 + 2.0 * 5.990997791290283
Epoch 1850, val loss: 1.0376389026641846
Epoch 1860, training loss: 11.991205215454102 = 0.008180947974324226 + 2.0 * 5.991512298583984
Epoch 1860, val loss: 1.0398422479629517
Epoch 1870, training loss: 11.99531364440918 = 0.008070954121649265 + 2.0 * 5.993621349334717
Epoch 1870, val loss: 1.0420598983764648
Epoch 1880, training loss: 11.988770484924316 = 0.007961713708937168 + 2.0 * 5.9904046058654785
Epoch 1880, val loss: 1.0441299676895142
Epoch 1890, training loss: 11.987082481384277 = 0.007857509888708591 + 2.0 * 5.989612579345703
Epoch 1890, val loss: 1.0463109016418457
Epoch 1900, training loss: 11.98641586303711 = 0.007753759156912565 + 2.0 * 5.989331245422363
Epoch 1900, val loss: 1.0485291481018066
Epoch 1910, training loss: 11.991546630859375 = 0.0076513709500432014 + 2.0 * 5.991947650909424
Epoch 1910, val loss: 1.0506476163864136
Epoch 1920, training loss: 11.990985870361328 = 0.00755219766870141 + 2.0 * 5.9917168617248535
Epoch 1920, val loss: 1.052783489227295
Epoch 1930, training loss: 11.988733291625977 = 0.007456165738403797 + 2.0 * 5.990638732910156
Epoch 1930, val loss: 1.0549676418304443
Epoch 1940, training loss: 11.985189437866211 = 0.007361779920756817 + 2.0 * 5.9889140129089355
Epoch 1940, val loss: 1.057010293006897
Epoch 1950, training loss: 11.990362167358398 = 0.007268582470715046 + 2.0 * 5.991546630859375
Epoch 1950, val loss: 1.0590155124664307
Epoch 1960, training loss: 11.98518180847168 = 0.007177188526839018 + 2.0 * 5.989002227783203
Epoch 1960, val loss: 1.0611250400543213
Epoch 1970, training loss: 11.983384132385254 = 0.007088283076882362 + 2.0 * 5.988147735595703
Epoch 1970, val loss: 1.063123106956482
Epoch 1980, training loss: 11.98440170288086 = 0.007000369019806385 + 2.0 * 5.988700866699219
Epoch 1980, val loss: 1.06514573097229
Epoch 1990, training loss: 11.986451148986816 = 0.00691404240205884 + 2.0 * 5.9897685050964355
Epoch 1990, val loss: 1.0671724081039429
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8111
Overall ASR: 0.8893
Flip ASR: 0.8667/225 nodes
The final ASR:0.80443, 0.10975, Accuracy:0.80741, 0.00302
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11556])
remove edge: torch.Size([2, 9400])
updated graph: torch.Size([2, 10400])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
The final ASR:0.97786, 0.00000, Accuracy:0.83086, 0.00630
Begin epxeriment: cont_weight: 2 epoch:2000 der1:0.3 der2:0.2 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 18.68686866760254 = 1.9390583038330078 + 2.0 * 8.373905181884766
Epoch 0, val loss: 1.9400080442428589
Epoch 10, training loss: 18.67612648010254 = 1.9290995597839355 + 2.0 * 8.373513221740723
Epoch 10, val loss: 1.9306002855300903
Epoch 20, training loss: 18.6572265625 = 1.9164766073226929 + 2.0 * 8.37037467956543
Epoch 20, val loss: 1.9181697368621826
Epoch 30, training loss: 18.591320037841797 = 1.899043321609497 + 2.0 * 8.346138000488281
Epoch 30, val loss: 1.900680422782898
Epoch 40, training loss: 18.260366439819336 = 1.8778789043426514 + 2.0 * 8.191244125366211
Epoch 40, val loss: 1.8802945613861084
Epoch 50, training loss: 17.433517456054688 = 1.8560051918029785 + 2.0 * 7.788755893707275
Epoch 50, val loss: 1.860290288925171
Epoch 60, training loss: 16.871362686157227 = 1.8369661569595337 + 2.0 * 7.517198085784912
Epoch 60, val loss: 1.8442021608352661
Epoch 70, training loss: 15.973039627075195 = 1.8233213424682617 + 2.0 * 7.074859142303467
Epoch 70, val loss: 1.8323898315429688
Epoch 80, training loss: 15.276958465576172 = 1.8141661882400513 + 2.0 * 6.731396198272705
Epoch 80, val loss: 1.82426917552948
Epoch 90, training loss: 14.989439964294434 = 1.8029558658599854 + 2.0 * 6.593242168426514
Epoch 90, val loss: 1.813511610031128
Epoch 100, training loss: 14.772011756896973 = 1.788923978805542 + 2.0 * 6.491543769836426
Epoch 100, val loss: 1.8008784055709839
Epoch 110, training loss: 14.62167739868164 = 1.7753429412841797 + 2.0 * 6.4231672286987305
Epoch 110, val loss: 1.7887978553771973
Epoch 120, training loss: 14.510751724243164 = 1.7615253925323486 + 2.0 * 6.374613285064697
Epoch 120, val loss: 1.7764194011688232
Epoch 130, training loss: 14.422208786010742 = 1.746762752532959 + 2.0 * 6.3377227783203125
Epoch 130, val loss: 1.763498306274414
Epoch 140, training loss: 14.34825611114502 = 1.7306852340698242 + 2.0 * 6.308785438537598
Epoch 140, val loss: 1.7498114109039307
Epoch 150, training loss: 14.283903121948242 = 1.712823510169983 + 2.0 * 6.285539627075195
Epoch 150, val loss: 1.7348697185516357
Epoch 160, training loss: 14.224162101745605 = 1.6928237676620483 + 2.0 * 6.265669345855713
Epoch 160, val loss: 1.718450665473938
Epoch 170, training loss: 14.167352676391602 = 1.6703314781188965 + 2.0 * 6.248510360717773
Epoch 170, val loss: 1.7001938819885254
Epoch 180, training loss: 14.113956451416016 = 1.6450220346450806 + 2.0 * 6.234467029571533
Epoch 180, val loss: 1.6798542737960815
Epoch 190, training loss: 14.057116508483887 = 1.6166638135910034 + 2.0 * 6.220226287841797
Epoch 190, val loss: 1.657124638557434
Epoch 200, training loss: 14.0016450881958 = 1.5850086212158203 + 2.0 * 6.20831823348999
Epoch 200, val loss: 1.6319955587387085
Epoch 210, training loss: 13.944581985473633 = 1.5502513647079468 + 2.0 * 6.197165489196777
Epoch 210, val loss: 1.6045713424682617
Epoch 220, training loss: 13.896693229675293 = 1.5120679140090942 + 2.0 * 6.192312717437744
Epoch 220, val loss: 1.5747013092041016
Epoch 230, training loss: 13.830888748168945 = 1.471351146697998 + 2.0 * 6.1797685623168945
Epoch 230, val loss: 1.542977213859558
Epoch 240, training loss: 13.770792007446289 = 1.4281795024871826 + 2.0 * 6.171306133270264
Epoch 240, val loss: 1.5095394849777222
Epoch 250, training loss: 13.7101469039917 = 1.3830130100250244 + 2.0 * 6.163567066192627
Epoch 250, val loss: 1.4749701023101807
Epoch 260, training loss: 13.65017318725586 = 1.3366360664367676 + 2.0 * 6.156768798828125
Epoch 260, val loss: 1.439660668373108
Epoch 270, training loss: 13.59156322479248 = 1.2893491983413696 + 2.0 * 6.151106834411621
Epoch 270, val loss: 1.4038619995117188
Epoch 280, training loss: 13.537635803222656 = 1.2418255805969238 + 2.0 * 6.147905349731445
Epoch 280, val loss: 1.368223786354065
Epoch 290, training loss: 13.475824356079102 = 1.1950769424438477 + 2.0 * 6.140373706817627
Epoch 290, val loss: 1.3334060907363892
Epoch 300, training loss: 13.41896915435791 = 1.1494638919830322 + 2.0 * 6.1347527503967285
Epoch 300, val loss: 1.2996445894241333
Epoch 310, training loss: 13.366167068481445 = 1.1052660942077637 + 2.0 * 6.130450248718262
Epoch 310, val loss: 1.2672879695892334
Epoch 320, training loss: 13.315245628356934 = 1.0630371570587158 + 2.0 * 6.126104354858398
Epoch 320, val loss: 1.2365916967391968
Epoch 330, training loss: 13.26655387878418 = 1.0228543281555176 + 2.0 * 6.12185001373291
Epoch 330, val loss: 1.2078487873077393
Epoch 340, training loss: 13.219781875610352 = 0.9847325086593628 + 2.0 * 6.11752462387085
Epoch 340, val loss: 1.1810002326965332
Epoch 350, training loss: 13.18095874786377 = 0.9486251473426819 + 2.0 * 6.116166591644287
Epoch 350, val loss: 1.155997633934021
Epoch 360, training loss: 13.136529922485352 = 0.9147003889083862 + 2.0 * 6.110914707183838
Epoch 360, val loss: 1.1330382823944092
Epoch 370, training loss: 13.097854614257812 = 0.8826432824134827 + 2.0 * 6.107605457305908
Epoch 370, val loss: 1.1118303537368774
Epoch 380, training loss: 13.061481475830078 = 0.8521050214767456 + 2.0 * 6.1046881675720215
Epoch 380, val loss: 1.0921152830123901
Epoch 390, training loss: 13.032917976379395 = 0.823002815246582 + 2.0 * 6.104957580566406
Epoch 390, val loss: 1.0737557411193848
Epoch 400, training loss: 12.994580268859863 = 0.7952720522880554 + 2.0 * 6.099654197692871
Epoch 400, val loss: 1.056632161140442
Epoch 410, training loss: 12.962811470031738 = 0.768707811832428 + 2.0 * 6.097051620483398
Epoch 410, val loss: 1.0405240058898926
Epoch 420, training loss: 12.931219100952148 = 0.7428684830665588 + 2.0 * 6.094175338745117
Epoch 420, val loss: 1.0251301527023315
Epoch 430, training loss: 12.901464462280273 = 0.7175552845001221 + 2.0 * 6.091954708099365
Epoch 430, val loss: 1.0102137327194214
Epoch 440, training loss: 12.872685432434082 = 0.6926115155220032 + 2.0 * 6.090036869049072
Epoch 440, val loss: 0.9955706596374512
Epoch 450, training loss: 12.843254089355469 = 0.6680431962013245 + 2.0 * 6.0876054763793945
Epoch 450, val loss: 0.9811984300613403
Epoch 460, training loss: 12.814495086669922 = 0.6435812711715698 + 2.0 * 6.085456848144531
Epoch 460, val loss: 0.9669599533081055
Epoch 470, training loss: 12.81368350982666 = 0.6191611886024475 + 2.0 * 6.09726095199585
Epoch 470, val loss: 0.9526455402374268
Epoch 480, training loss: 12.760664939880371 = 0.5946339964866638 + 2.0 * 6.083015441894531
Epoch 480, val loss: 0.9382534623146057
Epoch 490, training loss: 12.730669975280762 = 0.570190966129303 + 2.0 * 6.080239295959473
Epoch 490, val loss: 0.9239659905433655
Epoch 500, training loss: 12.701681137084961 = 0.5457201600074768 + 2.0 * 6.0779805183410645
Epoch 500, val loss: 0.9096826910972595
Epoch 510, training loss: 12.673904418945312 = 0.5212545394897461 + 2.0 * 6.076324939727783
Epoch 510, val loss: 0.8954453468322754
Epoch 520, training loss: 12.647537231445312 = 0.4968956410884857 + 2.0 * 6.075320720672607
Epoch 520, val loss: 0.8814533352851868
Epoch 530, training loss: 12.626108169555664 = 0.472932368516922 + 2.0 * 6.076587677001953
Epoch 530, val loss: 0.8676936030387878
Epoch 540, training loss: 12.596419334411621 = 0.44954878091812134 + 2.0 * 6.073435306549072
Epoch 540, val loss: 0.8547236323356628
Epoch 550, training loss: 12.568763732910156 = 0.4267902970314026 + 2.0 * 6.070986747741699
Epoch 550, val loss: 0.8426507115364075
Epoch 560, training loss: 12.542757034301758 = 0.40471893548965454 + 2.0 * 6.069018840789795
Epoch 560, val loss: 0.8314231634140015
Epoch 570, training loss: 12.519776344299316 = 0.3834182322025299 + 2.0 * 6.068179130554199
Epoch 570, val loss: 0.8211312294006348
Epoch 580, training loss: 12.502364158630371 = 0.3629903495311737 + 2.0 * 6.0696868896484375
Epoch 580, val loss: 0.8118715882301331
Epoch 590, training loss: 12.47563648223877 = 0.34362244606018066 + 2.0 * 6.066007137298584
Epoch 590, val loss: 0.8038311004638672
Epoch 600, training loss: 12.45312213897705 = 0.3252548277378082 + 2.0 * 6.063933849334717
Epoch 600, val loss: 0.7969821095466614
Epoch 610, training loss: 12.432846069335938 = 0.30784428119659424 + 2.0 * 6.062500953674316
Epoch 610, val loss: 0.7912548184394836
Epoch 620, training loss: 12.426738739013672 = 0.29142439365386963 + 2.0 * 6.067656993865967
Epoch 620, val loss: 0.7865233421325684
Epoch 630, training loss: 12.403136253356934 = 0.27596357464790344 + 2.0 * 6.063586235046387
Epoch 630, val loss: 0.7829090356826782
Epoch 640, training loss: 12.3806791305542 = 0.2614280879497528 + 2.0 * 6.059625625610352
Epoch 640, val loss: 0.7804314494132996
Epoch 650, training loss: 12.363960266113281 = 0.24772273004055023 + 2.0 * 6.05811882019043
Epoch 650, val loss: 0.7787973880767822
Epoch 660, training loss: 12.354154586791992 = 0.23475928604602814 + 2.0 * 6.05969762802124
Epoch 660, val loss: 0.7779830694198608
Epoch 670, training loss: 12.33563232421875 = 0.22257517278194427 + 2.0 * 6.056528568267822
Epoch 670, val loss: 0.7779493927955627
Epoch 680, training loss: 12.31949234008789 = 0.21099279820919037 + 2.0 * 6.0542497634887695
Epoch 680, val loss: 0.7786685228347778
Epoch 690, training loss: 12.314013481140137 = 0.20005741715431213 + 2.0 * 6.056978225708008
Epoch 690, val loss: 0.7799873948097229
Epoch 700, training loss: 12.303441047668457 = 0.18972913920879364 + 2.0 * 6.056856155395508
Epoch 700, val loss: 0.7816113233566284
Epoch 710, training loss: 12.282251358032227 = 0.18001388013362885 + 2.0 * 6.051118850708008
Epoch 710, val loss: 0.7839371562004089
Epoch 720, training loss: 12.270129203796387 = 0.17082281410694122 + 2.0 * 6.049653053283691
Epoch 720, val loss: 0.7867352366447449
Epoch 730, training loss: 12.2606840133667 = 0.16212721168994904 + 2.0 * 6.049278259277344
Epoch 730, val loss: 0.7898891568183899
Epoch 740, training loss: 12.253988265991211 = 0.15391722321510315 + 2.0 * 6.05003547668457
Epoch 740, val loss: 0.7933245897293091
Epoch 750, training loss: 12.240663528442383 = 0.1461617648601532 + 2.0 * 6.047250747680664
Epoch 750, val loss: 0.79707932472229
Epoch 760, training loss: 12.239838600158691 = 0.13887304067611694 + 2.0 * 6.050482749938965
Epoch 760, val loss: 0.8011185526847839
Epoch 770, training loss: 12.224446296691895 = 0.13198018074035645 + 2.0 * 6.046233177185059
Epoch 770, val loss: 0.8053578734397888
Epoch 780, training loss: 12.225406646728516 = 0.1255238652229309 + 2.0 * 6.049941539764404
Epoch 780, val loss: 0.8098583221435547
Epoch 790, training loss: 12.20911693572998 = 0.11941505968570709 + 2.0 * 6.044850826263428
Epoch 790, val loss: 0.8145105838775635
Epoch 800, training loss: 12.198287963867188 = 0.11368588358163834 + 2.0 * 6.042301177978516
Epoch 800, val loss: 0.8193678259849548
Epoch 810, training loss: 12.196778297424316 = 0.10827238112688065 + 2.0 * 6.044252872467041
Epoch 810, val loss: 0.8243457078933716
Epoch 820, training loss: 12.185422897338867 = 0.10318692773580551 + 2.0 * 6.041118144989014
Epoch 820, val loss: 0.8293505907058716
Epoch 830, training loss: 12.179816246032715 = 0.09839031100273132 + 2.0 * 6.040712833404541
Epoch 830, val loss: 0.8344905376434326
Epoch 840, training loss: 12.174153327941895 = 0.09388750046491623 + 2.0 * 6.040132999420166
Epoch 840, val loss: 0.8397572040557861
Epoch 850, training loss: 12.166747093200684 = 0.08963096886873245 + 2.0 * 6.038558006286621
Epoch 850, val loss: 0.8449112176895142
Epoch 860, training loss: 12.162070274353027 = 0.08562768995761871 + 2.0 * 6.03822135925293
Epoch 860, val loss: 0.8502771258354187
Epoch 870, training loss: 12.154339790344238 = 0.0818440392613411 + 2.0 * 6.036247730255127
Epoch 870, val loss: 0.8555562496185303
Epoch 880, training loss: 12.149134635925293 = 0.0782766342163086 + 2.0 * 6.035429000854492
Epoch 880, val loss: 0.8609538674354553
Epoch 890, training loss: 12.165430068969727 = 0.07489511370658875 + 2.0 * 6.045267581939697
Epoch 890, val loss: 0.866274356842041
Epoch 900, training loss: 12.145730018615723 = 0.07171433418989182 + 2.0 * 6.037007808685303
Epoch 900, val loss: 0.8714912533760071
Epoch 910, training loss: 12.136905670166016 = 0.06869630515575409 + 2.0 * 6.034104824066162
Epoch 910, val loss: 0.876806378364563
Epoch 920, training loss: 12.131287574768066 = 0.06585533171892166 + 2.0 * 6.032716274261475
Epoch 920, val loss: 0.8821600675582886
Epoch 930, training loss: 12.13564395904541 = 0.06315825879573822 + 2.0 * 6.036242961883545
Epoch 930, val loss: 0.8874744772911072
Epoch 940, training loss: 12.127501487731934 = 0.060597941279411316 + 2.0 * 6.033451557159424
Epoch 940, val loss: 0.8925681710243225
Epoch 950, training loss: 12.121129989624023 = 0.05817307159304619 + 2.0 * 6.031478404998779
Epoch 950, val loss: 0.897824764251709
Epoch 960, training loss: 12.117875099182129 = 0.05587451532483101 + 2.0 * 6.031000137329102
Epoch 960, val loss: 0.9029666781425476
Epoch 970, training loss: 12.123509407043457 = 0.05369462072849274 + 2.0 * 6.034907341003418
Epoch 970, val loss: 0.9080395698547363
Epoch 980, training loss: 12.113191604614258 = 0.05162721872329712 + 2.0 * 6.030782222747803
Epoch 980, val loss: 0.9129602313041687
Epoch 990, training loss: 12.109745979309082 = 0.04966210201382637 + 2.0 * 6.0300421714782715
Epoch 990, val loss: 0.9179714918136597
Epoch 1000, training loss: 12.10519790649414 = 0.047797828912734985 + 2.0 * 6.02869987487793
Epoch 1000, val loss: 0.9227812886238098
Epoch 1010, training loss: 12.100290298461914 = 0.046015024185180664 + 2.0 * 6.027137756347656
Epoch 1010, val loss: 0.9276362657546997
Epoch 1020, training loss: 12.100642204284668 = 0.0443238839507103 + 2.0 * 6.028159141540527
Epoch 1020, val loss: 0.932396411895752
Epoch 1030, training loss: 12.097556114196777 = 0.04271572083234787 + 2.0 * 6.0274200439453125
Epoch 1030, val loss: 0.9371311068534851
Epoch 1040, training loss: 12.093388557434082 = 0.041190873831510544 + 2.0 * 6.026098728179932
Epoch 1040, val loss: 0.9417052268981934
Epoch 1050, training loss: 12.089946746826172 = 0.03973827883601189 + 2.0 * 6.02510404586792
Epoch 1050, val loss: 0.9463260173797607
Epoch 1060, training loss: 12.087101936340332 = 0.03835463151335716 + 2.0 * 6.024373531341553
Epoch 1060, val loss: 0.9509262442588806
Epoch 1070, training loss: 12.085165977478027 = 0.03703807666897774 + 2.0 * 6.024064064025879
Epoch 1070, val loss: 0.9554415345191956
Epoch 1080, training loss: 12.10408878326416 = 0.035795778036117554 + 2.0 * 6.034146308898926
Epoch 1080, val loss: 0.9598413705825806
Epoch 1090, training loss: 12.083559036254883 = 0.034593481570482254 + 2.0 * 6.024482727050781
Epoch 1090, val loss: 0.963935911655426
Epoch 1100, training loss: 12.078337669372559 = 0.03346433863043785 + 2.0 * 6.022436618804932
Epoch 1100, val loss: 0.9682445526123047
Epoch 1110, training loss: 12.0761137008667 = 0.03238828852772713 + 2.0 * 6.021862506866455
Epoch 1110, val loss: 0.9724987745285034
Epoch 1120, training loss: 12.072490692138672 = 0.03135846555233002 + 2.0 * 6.020565986633301
Epoch 1120, val loss: 0.9766961932182312
Epoch 1130, training loss: 12.078289031982422 = 0.030374323949217796 + 2.0 * 6.023957252502441
Epoch 1130, val loss: 0.9807947278022766
Epoch 1140, training loss: 12.071782112121582 = 0.029430532827973366 + 2.0 * 6.021175861358643
Epoch 1140, val loss: 0.9847174882888794
Epoch 1150, training loss: 12.07121467590332 = 0.028536779806017876 + 2.0 * 6.021338939666748
Epoch 1150, val loss: 0.9887047410011292
Epoch 1160, training loss: 12.066184997558594 = 0.027681220322847366 + 2.0 * 6.019251823425293
Epoch 1160, val loss: 0.9926888942718506
Epoch 1170, training loss: 12.065184593200684 = 0.026865117251873016 + 2.0 * 6.01915979385376
Epoch 1170, val loss: 0.9966146945953369
Epoch 1180, training loss: 12.073261260986328 = 0.02608639933168888 + 2.0 * 6.023587226867676
Epoch 1180, val loss: 1.0004111528396606
Epoch 1190, training loss: 12.064095497131348 = 0.025328846648335457 + 2.0 * 6.019383430480957
Epoch 1190, val loss: 1.0042595863342285
Epoch 1200, training loss: 12.059863090515137 = 0.024614987894892693 + 2.0 * 6.0176239013671875
Epoch 1200, val loss: 1.008000373840332
Epoch 1210, training loss: 12.06267261505127 = 0.023929234594106674 + 2.0 * 6.019371509552002
Epoch 1210, val loss: 1.011747121810913
Epoch 1220, training loss: 12.05764102935791 = 0.023266451433300972 + 2.0 * 6.017187118530273
Epoch 1220, val loss: 1.0153002738952637
Epoch 1230, training loss: 12.061210632324219 = 0.022633152082562447 + 2.0 * 6.019288539886475
Epoch 1230, val loss: 1.018898606300354
Epoch 1240, training loss: 12.054569244384766 = 0.02203095704317093 + 2.0 * 6.016269207000732
Epoch 1240, val loss: 1.0225955247879028
Epoch 1250, training loss: 12.058880805969238 = 0.02144753187894821 + 2.0 * 6.018716812133789
Epoch 1250, val loss: 1.0260560512542725
Epoch 1260, training loss: 12.052664756774902 = 0.020890405401587486 + 2.0 * 6.015887260437012
Epoch 1260, val loss: 1.0295331478118896
Epoch 1270, training loss: 12.04822826385498 = 0.020354626700282097 + 2.0 * 6.013936996459961
Epoch 1270, val loss: 1.0330746173858643
Epoch 1280, training loss: 12.049860000610352 = 0.019841060042381287 + 2.0 * 6.01500940322876
Epoch 1280, val loss: 1.0365066528320312
Epoch 1290, training loss: 12.055078506469727 = 0.019348690286278725 + 2.0 * 6.01786470413208
Epoch 1290, val loss: 1.039720058441162
Epoch 1300, training loss: 12.046942710876465 = 0.018866488710045815 + 2.0 * 6.0140380859375
Epoch 1300, val loss: 1.0430424213409424
Epoch 1310, training loss: 12.04517650604248 = 0.018409930169582367 + 2.0 * 6.013383388519287
Epoch 1310, val loss: 1.046411395072937
Epoch 1320, training loss: 12.044196128845215 = 0.017970286309719086 + 2.0 * 6.013113021850586
Epoch 1320, val loss: 1.0496662855148315
Epoch 1330, training loss: 12.048418045043945 = 0.017543984577059746 + 2.0 * 6.015437126159668
Epoch 1330, val loss: 1.0528039932250977
Epoch 1340, training loss: 12.043305397033691 = 0.017136303707957268 + 2.0 * 6.013084411621094
Epoch 1340, val loss: 1.0560534000396729
Epoch 1350, training loss: 12.040989875793457 = 0.016741884872317314 + 2.0 * 6.012124061584473
Epoch 1350, val loss: 1.059254765510559
Epoch 1360, training loss: 12.042069435119629 = 0.016361970454454422 + 2.0 * 6.012853622436523
Epoch 1360, val loss: 1.0622615814208984
Epoch 1370, training loss: 12.040541648864746 = 0.015995627269148827 + 2.0 * 6.012272834777832
Epoch 1370, val loss: 1.0653345584869385
Epoch 1380, training loss: 12.039690017700195 = 0.015639137476682663 + 2.0 * 6.012025356292725
Epoch 1380, val loss: 1.068322777748108
Epoch 1390, training loss: 12.03475284576416 = 0.015295948833227158 + 2.0 * 6.00972843170166
Epoch 1390, val loss: 1.0713046789169312
Epoch 1400, training loss: 12.041638374328613 = 0.014967490918934345 + 2.0 * 6.013335227966309
Epoch 1400, val loss: 1.0743277072906494
Epoch 1410, training loss: 12.036532402038574 = 0.014647413045167923 + 2.0 * 6.010942459106445
Epoch 1410, val loss: 1.0770056247711182
Epoch 1420, training loss: 12.036005973815918 = 0.014339995570480824 + 2.0 * 6.010832786560059
Epoch 1420, val loss: 1.0799560546875
Epoch 1430, training loss: 12.029982566833496 = 0.014042962342500687 + 2.0 * 6.007969856262207
Epoch 1430, val loss: 1.0828415155410767
Epoch 1440, training loss: 12.029446601867676 = 0.013754460029304028 + 2.0 * 6.007845878601074
Epoch 1440, val loss: 1.085716724395752
Epoch 1450, training loss: 12.028790473937988 = 0.013474659994244576 + 2.0 * 6.007658004760742
Epoch 1450, val loss: 1.0885857343673706
Epoch 1460, training loss: 12.046006202697754 = 0.01320695411413908 + 2.0 * 6.01639986038208
Epoch 1460, val loss: 1.0913318395614624
Epoch 1470, training loss: 12.037615776062012 = 0.012941922061145306 + 2.0 * 6.012336730957031
Epoch 1470, val loss: 1.0939314365386963
Epoch 1480, training loss: 12.025917053222656 = 0.012685938738286495 + 2.0 * 6.00661563873291
Epoch 1480, val loss: 1.0965534448623657
Epoch 1490, training loss: 12.025753021240234 = 0.012441088445484638 + 2.0 * 6.006656169891357
Epoch 1490, val loss: 1.099305272102356
Epoch 1500, training loss: 12.023911476135254 = 0.012201930396258831 + 2.0 * 6.005854606628418
Epoch 1500, val loss: 1.101973295211792
Epoch 1510, training loss: 12.047534942626953 = 0.011969947256147861 + 2.0 * 6.017782688140869
Epoch 1510, val loss: 1.1045122146606445
Epoch 1520, training loss: 12.03077507019043 = 0.011747959069907665 + 2.0 * 6.0095133781433105
Epoch 1520, val loss: 1.1069940328598022
Epoch 1530, training loss: 12.021904945373535 = 0.01152953039854765 + 2.0 * 6.005187511444092
Epoch 1530, val loss: 1.1096644401550293
Epoch 1540, training loss: 12.020879745483398 = 0.011319519951939583 + 2.0 * 6.004780292510986
Epoch 1540, val loss: 1.1122149229049683
Epoch 1550, training loss: 12.035295486450195 = 0.011116432957351208 + 2.0 * 6.012089729309082
Epoch 1550, val loss: 1.1147428750991821
Epoch 1560, training loss: 12.021990776062012 = 0.010915311984717846 + 2.0 * 6.005537509918213
Epoch 1560, val loss: 1.117044448852539
Epoch 1570, training loss: 12.018716812133789 = 0.010721073485910892 + 2.0 * 6.003997802734375
Epoch 1570, val loss: 1.1195895671844482
Epoch 1580, training loss: 12.016953468322754 = 0.010533536784350872 + 2.0 * 6.003210067749023
Epoch 1580, val loss: 1.1220452785491943
Epoch 1590, training loss: 12.01865005493164 = 0.010351301170885563 + 2.0 * 6.004149436950684
Epoch 1590, val loss: 1.1244213581085205
Epoch 1600, training loss: 12.022538185119629 = 0.010176191106438637 + 2.0 * 6.006180763244629
Epoch 1600, val loss: 1.1265705823898315
Epoch 1610, training loss: 12.016681671142578 = 0.009999752044677734 + 2.0 * 6.003341197967529
Epoch 1610, val loss: 1.1289186477661133
Epoch 1620, training loss: 12.018062591552734 = 0.009831834584474564 + 2.0 * 6.004115581512451
Epoch 1620, val loss: 1.131233811378479
Epoch 1630, training loss: 12.022908210754395 = 0.009670065715909004 + 2.0 * 6.006618976593018
Epoch 1630, val loss: 1.1333333253860474
Epoch 1640, training loss: 12.01626205444336 = 0.009510030038654804 + 2.0 * 6.003376007080078
Epoch 1640, val loss: 1.135701298713684
Epoch 1650, training loss: 12.01294231414795 = 0.009355103597044945 + 2.0 * 6.001793384552002
Epoch 1650, val loss: 1.137925386428833
Epoch 1660, training loss: 12.012079238891602 = 0.009204002097249031 + 2.0 * 6.001437664031982
Epoch 1660, val loss: 1.1400986909866333
Epoch 1670, training loss: 12.030097961425781 = 0.009056701324880123 + 2.0 * 6.0105204582214355
Epoch 1670, val loss: 1.1420233249664307
Epoch 1680, training loss: 12.013031005859375 = 0.008915015496313572 + 2.0 * 6.002058029174805
Epoch 1680, val loss: 1.1442631483078003
Epoch 1690, training loss: 12.010939598083496 = 0.008775119669735432 + 2.0 * 6.001082420349121
Epoch 1690, val loss: 1.146363615989685
Epoch 1700, training loss: 12.01550006866455 = 0.008639606647193432 + 2.0 * 6.003430366516113
Epoch 1700, val loss: 1.1483787298202515
Epoch 1710, training loss: 12.008797645568848 = 0.008508102968335152 + 2.0 * 6.000144958496094
Epoch 1710, val loss: 1.1505851745605469
Epoch 1720, training loss: 12.019328117370605 = 0.008378887549042702 + 2.0 * 6.00547456741333
Epoch 1720, val loss: 1.1525899171829224
Epoch 1730, training loss: 12.009249687194824 = 0.008253426291048527 + 2.0 * 6.000498294830322
Epoch 1730, val loss: 1.1543586254119873
Epoch 1740, training loss: 12.006896018981934 = 0.008131014183163643 + 2.0 * 5.999382495880127
Epoch 1740, val loss: 1.1565077304840088
Epoch 1750, training loss: 12.005072593688965 = 0.008011307567358017 + 2.0 * 5.998530864715576
Epoch 1750, val loss: 1.1584936380386353
Epoch 1760, training loss: 12.010165214538574 = 0.007894916459918022 + 2.0 * 6.001135349273682
Epoch 1760, val loss: 1.1604201793670654
Epoch 1770, training loss: 12.007966041564941 = 0.0077814278192818165 + 2.0 * 6.000092506408691
Epoch 1770, val loss: 1.1622066497802734
Epoch 1780, training loss: 12.003799438476562 = 0.007668682839721441 + 2.0 * 5.99806547164917
Epoch 1780, val loss: 1.1641103029251099
Epoch 1790, training loss: 12.003193855285645 = 0.00755946384742856 + 2.0 * 5.997817039489746
Epoch 1790, val loss: 1.1660568714141846
Epoch 1800, training loss: 12.003987312316895 = 0.007453096564859152 + 2.0 * 5.99826717376709
Epoch 1800, val loss: 1.1679120063781738
Epoch 1810, training loss: 12.013157844543457 = 0.007349728606641293 + 2.0 * 6.002903938293457
Epoch 1810, val loss: 1.169617772102356
Epoch 1820, training loss: 12.008028984069824 = 0.007248993963003159 + 2.0 * 6.00039005279541
Epoch 1820, val loss: 1.1715182065963745
Epoch 1830, training loss: 12.006601333618164 = 0.007149729412049055 + 2.0 * 5.999725818634033
Epoch 1830, val loss: 1.1731555461883545
Epoch 1840, training loss: 12.00805950164795 = 0.00705286581069231 + 2.0 * 6.0005035400390625
Epoch 1840, val loss: 1.1750224828720093
Epoch 1850, training loss: 12.002820014953613 = 0.006958893034607172 + 2.0 * 5.997930526733398
Epoch 1850, val loss: 1.1767151355743408
Epoch 1860, training loss: 12.001550674438477 = 0.006865692790597677 + 2.0 * 5.997342586517334
Epoch 1860, val loss: 1.1784483194351196
Epoch 1870, training loss: 12.003379821777344 = 0.006774566136300564 + 2.0 * 5.998302459716797
Epoch 1870, val loss: 1.1801190376281738
Epoch 1880, training loss: 11.999492645263672 = 0.006685974542051554 + 2.0 * 5.996403217315674
Epoch 1880, val loss: 1.1817911863327026
Epoch 1890, training loss: 12.0029935836792 = 0.006599001120775938 + 2.0 * 5.998197078704834
Epoch 1890, val loss: 1.1833631992340088
Epoch 1900, training loss: 12.004097938537598 = 0.006513900589197874 + 2.0 * 5.9987921714782715
Epoch 1900, val loss: 1.1848688125610352
Epoch 1910, training loss: 12.001026153564453 = 0.006432355381548405 + 2.0 * 5.9972968101501465
Epoch 1910, val loss: 1.1865171194076538
Epoch 1920, training loss: 11.996793746948242 = 0.006351014599204063 + 2.0 * 5.995221138000488
Epoch 1920, val loss: 1.188195824623108
Epoch 1930, training loss: 11.998614311218262 = 0.006272636819630861 + 2.0 * 5.996170997619629
Epoch 1930, val loss: 1.1897532939910889
Epoch 1940, training loss: 12.000105857849121 = 0.006195494439452887 + 2.0 * 5.996955394744873
Epoch 1940, val loss: 1.191184639930725
Epoch 1950, training loss: 11.996764183044434 = 0.006116760428994894 + 2.0 * 5.995323657989502
Epoch 1950, val loss: 1.1927763223648071
Epoch 1960, training loss: 11.99470329284668 = 0.0060427747666835785 + 2.0 * 5.994330406188965
Epoch 1960, val loss: 1.194305658340454
Epoch 1970, training loss: 12.01081371307373 = 0.005969187244772911 + 2.0 * 6.002422332763672
Epoch 1970, val loss: 1.1956679821014404
Epoch 1980, training loss: 12.000508308410645 = 0.005899036768823862 + 2.0 * 5.997304439544678
Epoch 1980, val loss: 1.1970444917678833
Epoch 1990, training loss: 11.996252059936523 = 0.0058281319215893745 + 2.0 * 5.995212078094482
Epoch 1990, val loss: 1.1985666751861572
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8000
Overall ASR: 0.5461
Flip ASR: 0.4533/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.702190399169922 = 1.9543298482894897 + 2.0 * 8.373929977416992
Epoch 0, val loss: 1.9533131122589111
Epoch 10, training loss: 18.691802978515625 = 1.944602608680725 + 2.0 * 8.373600006103516
Epoch 10, val loss: 1.9432393312454224
Epoch 20, training loss: 18.674705505371094 = 1.9326674938201904 + 2.0 * 8.37101936340332
Epoch 20, val loss: 1.9309394359588623
Epoch 30, training loss: 18.618816375732422 = 1.9165692329406738 + 2.0 * 8.351123809814453
Epoch 30, val loss: 1.9145606756210327
Epoch 40, training loss: 18.342069625854492 = 1.8963323831558228 + 2.0 * 8.222868919372559
Epoch 40, val loss: 1.894587516784668
Epoch 50, training loss: 17.36614227294922 = 1.8751168251037598 + 2.0 * 7.745512962341309
Epoch 50, val loss: 1.8742713928222656
Epoch 60, training loss: 16.608091354370117 = 1.856854796409607 + 2.0 * 7.375617980957031
Epoch 60, val loss: 1.8578237295150757
Epoch 70, training loss: 15.782248497009277 = 1.8400065898895264 + 2.0 * 6.971120834350586
Epoch 70, val loss: 1.8418958187103271
Epoch 80, training loss: 15.331302642822266 = 1.82431960105896 + 2.0 * 6.753491401672363
Epoch 80, val loss: 1.827155351638794
Epoch 90, training loss: 15.046103477478027 = 1.8089311122894287 + 2.0 * 6.61858606338501
Epoch 90, val loss: 1.8131084442138672
Epoch 100, training loss: 14.850950241088867 = 1.7933435440063477 + 2.0 * 6.52880334854126
Epoch 100, val loss: 1.7991427183151245
Epoch 110, training loss: 14.730886459350586 = 1.776625633239746 + 2.0 * 6.47713041305542
Epoch 110, val loss: 1.784339427947998
Epoch 120, training loss: 14.636632919311523 = 1.7582507133483887 + 2.0 * 6.439190864562988
Epoch 120, val loss: 1.7681421041488647
Epoch 130, training loss: 14.53974437713623 = 1.738548994064331 + 2.0 * 6.40059757232666
Epoch 130, val loss: 1.7511181831359863
Epoch 140, training loss: 14.450798034667969 = 1.7176953554153442 + 2.0 * 6.366551399230957
Epoch 140, val loss: 1.7334402799606323
Epoch 150, training loss: 14.366506576538086 = 1.694955587387085 + 2.0 * 6.335775375366211
Epoch 150, val loss: 1.7144873142242432
Epoch 160, training loss: 14.289953231811523 = 1.6692473888397217 + 2.0 * 6.310352802276611
Epoch 160, val loss: 1.6933413743972778
Epoch 170, training loss: 14.226901054382324 = 1.6400517225265503 + 2.0 * 6.293424606323242
Epoch 170, val loss: 1.6692874431610107
Epoch 180, training loss: 14.154211044311523 = 1.606768012046814 + 2.0 * 6.273721694946289
Epoch 180, val loss: 1.6420800685882568
Epoch 190, training loss: 14.087185859680176 = 1.5697239637374878 + 2.0 * 6.258730888366699
Epoch 190, val loss: 1.6119476556777954
Epoch 200, training loss: 14.021909713745117 = 1.528174638748169 + 2.0 * 6.246867656707764
Epoch 200, val loss: 1.5781819820404053
Epoch 210, training loss: 13.956698417663574 = 1.482648491859436 + 2.0 * 6.237024784088135
Epoch 210, val loss: 1.5412949323654175
Epoch 220, training loss: 13.888483047485352 = 1.433797001838684 + 2.0 * 6.2273430824279785
Epoch 220, val loss: 1.50197172164917
Epoch 230, training loss: 13.819519996643066 = 1.3822492361068726 + 2.0 * 6.218635559082031
Epoch 230, val loss: 1.460709571838379
Epoch 240, training loss: 13.750162124633789 = 1.3290467262268066 + 2.0 * 6.21055793762207
Epoch 240, val loss: 1.4185823202133179
Epoch 250, training loss: 13.68223762512207 = 1.2755197286605835 + 2.0 * 6.203359127044678
Epoch 250, val loss: 1.3765902519226074
Epoch 260, training loss: 13.620830535888672 = 1.2224851846694946 + 2.0 * 6.199172496795654
Epoch 260, val loss: 1.335452675819397
Epoch 270, training loss: 13.550599098205566 = 1.171196699142456 + 2.0 * 6.189701080322266
Epoch 270, val loss: 1.2959496974945068
Epoch 280, training loss: 13.488334655761719 = 1.1217257976531982 + 2.0 * 6.183304309844971
Epoch 280, val loss: 1.258223295211792
Epoch 290, training loss: 13.433178901672363 = 1.0744022130966187 + 2.0 * 6.179388523101807
Epoch 290, val loss: 1.2223811149597168
Epoch 300, training loss: 13.377032279968262 = 1.0299756526947021 + 2.0 * 6.17352819442749
Epoch 300, val loss: 1.1889909505844116
Epoch 310, training loss: 13.324334144592285 = 0.9882009625434875 + 2.0 * 6.168066501617432
Epoch 310, val loss: 1.1577372550964355
Epoch 320, training loss: 13.27139663696289 = 0.9485275745391846 + 2.0 * 6.161434650421143
Epoch 320, val loss: 1.128172516822815
Epoch 330, training loss: 13.245992660522461 = 0.9107573628425598 + 2.0 * 6.1676177978515625
Epoch 330, val loss: 1.100093126296997
Epoch 340, training loss: 13.184800148010254 = 0.8750308156013489 + 2.0 * 6.1548848152160645
Epoch 340, val loss: 1.073789119720459
Epoch 350, training loss: 13.13744831085205 = 0.8413717746734619 + 2.0 * 6.148038387298584
Epoch 350, val loss: 1.04933762550354
Epoch 360, training loss: 13.0961275100708 = 0.8096779584884644 + 2.0 * 6.143224716186523
Epoch 360, val loss: 1.0267422199249268
Epoch 370, training loss: 13.058259010314941 = 0.7800223231315613 + 2.0 * 6.139118194580078
Epoch 370, val loss: 1.0061283111572266
Epoch 380, training loss: 13.025504112243652 = 0.752630352973938 + 2.0 * 6.136436939239502
Epoch 380, val loss: 0.9877070784568787
Epoch 390, training loss: 12.990877151489258 = 0.7271553874015808 + 2.0 * 6.131860733032227
Epoch 390, val loss: 0.9713671803474426
Epoch 400, training loss: 12.95803165435791 = 0.7032859921455383 + 2.0 * 6.127372741699219
Epoch 400, val loss: 0.9567639231681824
Epoch 410, training loss: 12.933219909667969 = 0.6807713508605957 + 2.0 * 6.126224040985107
Epoch 410, val loss: 0.94371098279953
Epoch 420, training loss: 12.907540321350098 = 0.6594032049179077 + 2.0 * 6.124068737030029
Epoch 420, val loss: 0.9319006204605103
Epoch 430, training loss: 12.874940872192383 = 0.6390854120254517 + 2.0 * 6.117927551269531
Epoch 430, val loss: 0.9214122295379639
Epoch 440, training loss: 12.849365234375 = 0.6193493604660034 + 2.0 * 6.1150078773498535
Epoch 440, val loss: 0.9118595123291016
Epoch 450, training loss: 12.836846351623535 = 0.5999787449836731 + 2.0 * 6.118433952331543
Epoch 450, val loss: 0.9027584195137024
Epoch 460, training loss: 12.80162525177002 = 0.5808274745941162 + 2.0 * 6.110398769378662
Epoch 460, val loss: 0.8941028714179993
Epoch 470, training loss: 12.775458335876465 = 0.5618937015533447 + 2.0 * 6.10678243637085
Epoch 470, val loss: 0.8860569596290588
Epoch 480, training loss: 12.75100326538086 = 0.5429561138153076 + 2.0 * 6.104023456573486
Epoch 480, val loss: 0.8783097863197327
Epoch 490, training loss: 12.744248390197754 = 0.5240530371665955 + 2.0 * 6.110097885131836
Epoch 490, val loss: 0.870976448059082
Epoch 500, training loss: 12.704710960388184 = 0.5050722360610962 + 2.0 * 6.099819183349609
Epoch 500, val loss: 0.8638363480567932
Epoch 510, training loss: 12.682770729064941 = 0.48621392250061035 + 2.0 * 6.098278522491455
Epoch 510, val loss: 0.8572706580162048
Epoch 520, training loss: 12.663191795349121 = 0.4674213230609894 + 2.0 * 6.0978851318359375
Epoch 520, val loss: 0.851112961769104
Epoch 530, training loss: 12.637389183044434 = 0.44877204298973083 + 2.0 * 6.094308376312256
Epoch 530, val loss: 0.8453533053398132
Epoch 540, training loss: 12.613969802856445 = 0.43022334575653076 + 2.0 * 6.0918731689453125
Epoch 540, val loss: 0.8399897813796997
Epoch 550, training loss: 12.590497016906738 = 0.4118622839450836 + 2.0 * 6.089317321777344
Epoch 550, val loss: 0.8350694179534912
Epoch 560, training loss: 12.604976654052734 = 0.3936738669872284 + 2.0 * 6.105651378631592
Epoch 560, val loss: 0.8304482102394104
Epoch 570, training loss: 12.560522079467773 = 0.37581002712249756 + 2.0 * 6.092356204986572
Epoch 570, val loss: 0.826233983039856
Epoch 580, training loss: 12.527363777160645 = 0.35831546783447266 + 2.0 * 6.084524154663086
Epoch 580, val loss: 0.822649359703064
Epoch 590, training loss: 12.507772445678711 = 0.34111449122428894 + 2.0 * 6.083329200744629
Epoch 590, val loss: 0.8193308115005493
Epoch 600, training loss: 12.488785743713379 = 0.3241935968399048 + 2.0 * 6.082295894622803
Epoch 600, val loss: 0.8162879943847656
Epoch 610, training loss: 12.469438552856445 = 0.3076537847518921 + 2.0 * 6.080892562866211
Epoch 610, val loss: 0.8135392069816589
Epoch 620, training loss: 12.45157527923584 = 0.29161056876182556 + 2.0 * 6.079982280731201
Epoch 620, val loss: 0.8112080693244934
Epoch 630, training loss: 12.433330535888672 = 0.27605193853378296 + 2.0 * 6.078639507293701
Epoch 630, val loss: 0.8093396425247192
Epoch 640, training loss: 12.412040710449219 = 0.26105576753616333 + 2.0 * 6.0754923820495605
Epoch 640, val loss: 0.807830274105072
Epoch 650, training loss: 12.398228645324707 = 0.2466275542974472 + 2.0 * 6.07580041885376
Epoch 650, val loss: 0.8068209886550903
Epoch 660, training loss: 12.38754940032959 = 0.23276445269584656 + 2.0 * 6.077392578125
Epoch 660, val loss: 0.8060833215713501
Epoch 670, training loss: 12.367960929870605 = 0.21966500580310822 + 2.0 * 6.074148178100586
Epoch 670, val loss: 0.8060324788093567
Epoch 680, training loss: 12.356792449951172 = 0.20722129940986633 + 2.0 * 6.0747857093811035
Epoch 680, val loss: 0.8063168525695801
Epoch 690, training loss: 12.33399772644043 = 0.19548876583576202 + 2.0 * 6.069254398345947
Epoch 690, val loss: 0.8071107268333435
Epoch 700, training loss: 12.319318771362305 = 0.18445564806461334 + 2.0 * 6.067431449890137
Epoch 700, val loss: 0.8084688782691956
Epoch 710, training loss: 12.308527946472168 = 0.1741097867488861 + 2.0 * 6.067209243774414
Epoch 710, val loss: 0.8102559447288513
Epoch 720, training loss: 12.296926498413086 = 0.16439920663833618 + 2.0 * 6.066263675689697
Epoch 720, val loss: 0.8123595118522644
Epoch 730, training loss: 12.284891128540039 = 0.15531237423419952 + 2.0 * 6.064789295196533
Epoch 730, val loss: 0.8148743510246277
Epoch 740, training loss: 12.271759986877441 = 0.146864116191864 + 2.0 * 6.062448024749756
Epoch 740, val loss: 0.8177909851074219
Epoch 750, training loss: 12.261093139648438 = 0.13900478184223175 + 2.0 * 6.061044216156006
Epoch 750, val loss: 0.821212112903595
Epoch 760, training loss: 12.25320816040039 = 0.131686270236969 + 2.0 * 6.060760974884033
Epoch 760, val loss: 0.8249418139457703
Epoch 770, training loss: 12.244429588317871 = 0.12485427409410477 + 2.0 * 6.059787750244141
Epoch 770, val loss: 0.8288928866386414
Epoch 780, training loss: 12.239657402038574 = 0.11849875003099442 + 2.0 * 6.060579299926758
Epoch 780, val loss: 0.8332681059837341
Epoch 790, training loss: 12.227180480957031 = 0.11259063333272934 + 2.0 * 6.057294845581055
Epoch 790, val loss: 0.8379140496253967
Epoch 800, training loss: 12.217887878417969 = 0.10708700865507126 + 2.0 * 6.055400371551514
Epoch 800, val loss: 0.8427788615226746
Epoch 810, training loss: 12.210240364074707 = 0.10194039344787598 + 2.0 * 6.054150104522705
Epoch 810, val loss: 0.8478927612304688
Epoch 820, training loss: 12.211064338684082 = 0.09712306410074234 + 2.0 * 6.056970596313477
Epoch 820, val loss: 0.8531655669212341
Epoch 830, training loss: 12.199572563171387 = 0.0926114171743393 + 2.0 * 6.053480625152588
Epoch 830, val loss: 0.8588156700134277
Epoch 840, training loss: 12.192898750305176 = 0.0883818045258522 + 2.0 * 6.052258491516113
Epoch 840, val loss: 0.8644036054611206
Epoch 850, training loss: 12.187276840209961 = 0.08441650122404099 + 2.0 * 6.0514302253723145
Epoch 850, val loss: 0.8703716993331909
Epoch 860, training loss: 12.186141014099121 = 0.0806882306933403 + 2.0 * 6.0527262687683105
Epoch 860, val loss: 0.8762423396110535
Epoch 870, training loss: 12.180829048156738 = 0.07717414200305939 + 2.0 * 6.051827430725098
Epoch 870, val loss: 0.8823891878128052
Epoch 880, training loss: 12.17026138305664 = 0.07388177514076233 + 2.0 * 6.048189640045166
Epoch 880, val loss: 0.8885871171951294
Epoch 890, training loss: 12.163063049316406 = 0.07077617198228836 + 2.0 * 6.046143531799316
Epoch 890, val loss: 0.8949757218360901
Epoch 900, training loss: 12.160309791564941 = 0.06784108281135559 + 2.0 * 6.046234130859375
Epoch 900, val loss: 0.9013808369636536
Epoch 910, training loss: 12.161925315856934 = 0.06506049633026123 + 2.0 * 6.048432350158691
Epoch 910, val loss: 0.9077903032302856
Epoch 920, training loss: 12.158841133117676 = 0.06242971494793892 + 2.0 * 6.048205852508545
Epoch 920, val loss: 0.9143333435058594
Epoch 930, training loss: 12.148359298706055 = 0.05995636433362961 + 2.0 * 6.044201374053955
Epoch 930, val loss: 0.9209601283073425
Epoch 940, training loss: 12.14105224609375 = 0.057606689631938934 + 2.0 * 6.041722774505615
Epoch 940, val loss: 0.9275360703468323
Epoch 950, training loss: 12.137286186218262 = 0.055374905467033386 + 2.0 * 6.040955543518066
Epoch 950, val loss: 0.9341811537742615
Epoch 960, training loss: 12.14763355255127 = 0.05324878543615341 + 2.0 * 6.047192573547363
Epoch 960, val loss: 0.9407852292060852
Epoch 970, training loss: 12.145177841186523 = 0.05124107375741005 + 2.0 * 6.046968460083008
Epoch 970, val loss: 0.9473778009414673
Epoch 980, training loss: 12.130088806152344 = 0.04932519793510437 + 2.0 * 6.040381908416748
Epoch 980, val loss: 0.9540020823478699
Epoch 990, training loss: 12.124820709228516 = 0.047515708953142166 + 2.0 * 6.038652420043945
Epoch 990, val loss: 0.9606687426567078
Epoch 1000, training loss: 12.120765686035156 = 0.04579698294401169 + 2.0 * 6.037484169006348
Epoch 1000, val loss: 0.9672333002090454
Epoch 1010, training loss: 12.133512496948242 = 0.044153984636068344 + 2.0 * 6.044679164886475
Epoch 1010, val loss: 0.973831295967102
Epoch 1020, training loss: 12.124618530273438 = 0.04259299859404564 + 2.0 * 6.041012763977051
Epoch 1020, val loss: 0.9802992939949036
Epoch 1030, training loss: 12.113954544067383 = 0.04110250622034073 + 2.0 * 6.036426067352295
Epoch 1030, val loss: 0.9868204593658447
Epoch 1040, training loss: 12.109689712524414 = 0.039689093828201294 + 2.0 * 6.035000324249268
Epoch 1040, val loss: 0.9932856559753418
Epoch 1050, training loss: 12.114222526550293 = 0.03833828866481781 + 2.0 * 6.037941932678223
Epoch 1050, val loss: 0.9996911883354187
Epoch 1060, training loss: 12.104255676269531 = 0.03705289214849472 + 2.0 * 6.0336012840271
Epoch 1060, val loss: 1.0061286687850952
Epoch 1070, training loss: 12.106733322143555 = 0.03582783415913582 + 2.0 * 6.035452842712402
Epoch 1070, val loss: 1.0125819444656372
Epoch 1080, training loss: 12.099949836730957 = 0.03465241938829422 + 2.0 * 6.03264856338501
Epoch 1080, val loss: 1.018957257270813
Epoch 1090, training loss: 12.096797943115234 = 0.033534105867147446 + 2.0 * 6.031631946563721
Epoch 1090, val loss: 1.0252571105957031
Epoch 1100, training loss: 12.094683647155762 = 0.03246831148862839 + 2.0 * 6.0311079025268555
Epoch 1100, val loss: 1.0315494537353516
Epoch 1110, training loss: 12.128735542297363 = 0.03145337104797363 + 2.0 * 6.048641204833984
Epoch 1110, val loss: 1.037742257118225
Epoch 1120, training loss: 12.091972351074219 = 0.03046904131770134 + 2.0 * 6.030751705169678
Epoch 1120, val loss: 1.043715238571167
Epoch 1130, training loss: 12.090777397155762 = 0.029540739953517914 + 2.0 * 6.030618190765381
Epoch 1130, val loss: 1.0498936176300049
Epoch 1140, training loss: 12.086723327636719 = 0.028654223307967186 + 2.0 * 6.029034614562988
Epoch 1140, val loss: 1.0559624433517456
Epoch 1150, training loss: 12.083928108215332 = 0.027804164215922356 + 2.0 * 6.028061866760254
Epoch 1150, val loss: 1.0620059967041016
Epoch 1160, training loss: 12.103631973266602 = 0.026990829035639763 + 2.0 * 6.038320541381836
Epoch 1160, val loss: 1.0680392980575562
Epoch 1170, training loss: 12.089116096496582 = 0.02620008960366249 + 2.0 * 6.031457901000977
Epoch 1170, val loss: 1.0736839771270752
Epoch 1180, training loss: 12.084539413452148 = 0.0254503283649683 + 2.0 * 6.029544353485107
Epoch 1180, val loss: 1.0796946287155151
Epoch 1190, training loss: 12.077889442443848 = 0.0247326772660017 + 2.0 * 6.026578426361084
Epoch 1190, val loss: 1.0855770111083984
Epoch 1200, training loss: 12.076257705688477 = 0.02404366433620453 + 2.0 * 6.026106834411621
Epoch 1200, val loss: 1.0913161039352417
Epoch 1210, training loss: 12.08420467376709 = 0.023381056264042854 + 2.0 * 6.030411720275879
Epoch 1210, val loss: 1.097100019454956
Epoch 1220, training loss: 12.078960418701172 = 0.022744499146938324 + 2.0 * 6.0281081199646
Epoch 1220, val loss: 1.102518081665039
Epoch 1230, training loss: 12.07184886932373 = 0.02213277295231819 + 2.0 * 6.024857997894287
Epoch 1230, val loss: 1.1081477403640747
Epoch 1240, training loss: 12.069242477416992 = 0.021549338474869728 + 2.0 * 6.023846626281738
Epoch 1240, val loss: 1.113653540611267
Epoch 1250, training loss: 12.066495895385742 = 0.020985309034585953 + 2.0 * 6.022755146026611
Epoch 1250, val loss: 1.1190845966339111
Epoch 1260, training loss: 12.068968772888184 = 0.020442552864551544 + 2.0 * 6.02426290512085
Epoch 1260, val loss: 1.1244611740112305
Epoch 1270, training loss: 12.072212219238281 = 0.019921788945794106 + 2.0 * 6.026144981384277
Epoch 1270, val loss: 1.1298191547393799
Epoch 1280, training loss: 12.071709632873535 = 0.019414886832237244 + 2.0 * 6.026147365570068
Epoch 1280, val loss: 1.1350140571594238
Epoch 1290, training loss: 12.06321907043457 = 0.018934039399027824 + 2.0 * 6.02214241027832
Epoch 1290, val loss: 1.1403404474258423
Epoch 1300, training loss: 12.060791015625 = 0.018471699208021164 + 2.0 * 6.021159648895264
Epoch 1300, val loss: 1.1455589532852173
Epoch 1310, training loss: 12.059304237365723 = 0.018023619428277016 + 2.0 * 6.0206403732299805
Epoch 1310, val loss: 1.1507314443588257
Epoch 1320, training loss: 12.065791130065918 = 0.017592236399650574 + 2.0 * 6.024099349975586
Epoch 1320, val loss: 1.1559056043624878
Epoch 1330, training loss: 12.061487197875977 = 0.017175912857055664 + 2.0 * 6.02215576171875
Epoch 1330, val loss: 1.1607508659362793
Epoch 1340, training loss: 12.06041431427002 = 0.016771916300058365 + 2.0 * 6.021821022033691
Epoch 1340, val loss: 1.1658556461334229
Epoch 1350, training loss: 12.055665969848633 = 0.016386020928621292 + 2.0 * 6.01963996887207
Epoch 1350, val loss: 1.1706609725952148
Epoch 1360, training loss: 12.055045127868652 = 0.016011917963624 + 2.0 * 6.019516468048096
Epoch 1360, val loss: 1.1755999326705933
Epoch 1370, training loss: 12.06155776977539 = 0.015650290995836258 + 2.0 * 6.022953510284424
Epoch 1370, val loss: 1.1803680658340454
Epoch 1380, training loss: 12.056476593017578 = 0.015302484855055809 + 2.0 * 6.020586967468262
Epoch 1380, val loss: 1.185308814048767
Epoch 1390, training loss: 12.062156677246094 = 0.014965436421334743 + 2.0 * 6.023595809936523
Epoch 1390, val loss: 1.1899991035461426
Epoch 1400, training loss: 12.050409317016602 = 0.014639264903962612 + 2.0 * 6.017885208129883
Epoch 1400, val loss: 1.1946337223052979
Epoch 1410, training loss: 12.047467231750488 = 0.014325066469609737 + 2.0 * 6.016571044921875
Epoch 1410, val loss: 1.1993355751037598
Epoch 1420, training loss: 12.04627799987793 = 0.014021671377122402 + 2.0 * 6.016128063201904
Epoch 1420, val loss: 1.2039620876312256
Epoch 1430, training loss: 12.051799774169922 = 0.013728823512792587 + 2.0 * 6.019035339355469
Epoch 1430, val loss: 1.208485722541809
Epoch 1440, training loss: 12.046707153320312 = 0.013441154733300209 + 2.0 * 6.016633033752441
Epoch 1440, val loss: 1.2130026817321777
Epoch 1450, training loss: 12.046045303344727 = 0.013165503740310669 + 2.0 * 6.016439914703369
Epoch 1450, val loss: 1.217542290687561
Epoch 1460, training loss: 12.049782752990723 = 0.012899434193968773 + 2.0 * 6.018441677093506
Epoch 1460, val loss: 1.222040057182312
Epoch 1470, training loss: 12.048238754272461 = 0.012640727683901787 + 2.0 * 6.017798900604248
Epoch 1470, val loss: 1.2262303829193115
Epoch 1480, training loss: 12.042706489562988 = 0.012389463372528553 + 2.0 * 6.015158653259277
Epoch 1480, val loss: 1.2305608987808228
Epoch 1490, training loss: 12.040538787841797 = 0.012147192843258381 + 2.0 * 6.014195919036865
Epoch 1490, val loss: 1.234851360321045
Epoch 1500, training loss: 12.041827201843262 = 0.011913487687706947 + 2.0 * 6.014956951141357
Epoch 1500, val loss: 1.239153504371643
Epoch 1510, training loss: 12.047022819519043 = 0.011686260811984539 + 2.0 * 6.0176682472229
Epoch 1510, val loss: 1.2432763576507568
Epoch 1520, training loss: 12.044450759887695 = 0.011461718939244747 + 2.0 * 6.0164947509765625
Epoch 1520, val loss: 1.2474168539047241
Epoch 1530, training loss: 12.038467407226562 = 0.011246978305280209 + 2.0 * 6.013610363006592
Epoch 1530, val loss: 1.2516287565231323
Epoch 1540, training loss: 12.04328727722168 = 0.011039184406399727 + 2.0 * 6.016124248504639
Epoch 1540, val loss: 1.2557320594787598
Epoch 1550, training loss: 12.035675048828125 = 0.010835553519427776 + 2.0 * 6.012419700622559
Epoch 1550, val loss: 1.2596403360366821
Epoch 1560, training loss: 12.035101890563965 = 0.010638995096087456 + 2.0 * 6.012231349945068
Epoch 1560, val loss: 1.2637263536453247
Epoch 1570, training loss: 12.037036895751953 = 0.010449303314089775 + 2.0 * 6.013293743133545
Epoch 1570, val loss: 1.267683744430542
Epoch 1580, training loss: 12.039461135864258 = 0.010264093056321144 + 2.0 * 6.014598369598389
Epoch 1580, val loss: 1.2714614868164062
Epoch 1590, training loss: 12.034273147583008 = 0.010081201791763306 + 2.0 * 6.012095928192139
Epoch 1590, val loss: 1.2753305435180664
Epoch 1600, training loss: 12.031929969787598 = 0.009905917569994926 + 2.0 * 6.011012077331543
Epoch 1600, val loss: 1.2792607545852661
Epoch 1610, training loss: 12.035781860351562 = 0.009736075066030025 + 2.0 * 6.0130228996276855
Epoch 1610, val loss: 1.283077359199524
Epoch 1620, training loss: 12.040144920349121 = 0.009568680077791214 + 2.0 * 6.015288352966309
Epoch 1620, val loss: 1.2868057489395142
Epoch 1630, training loss: 12.031002044677734 = 0.009407250210642815 + 2.0 * 6.010797500610352
Epoch 1630, val loss: 1.2904046773910522
Epoch 1640, training loss: 12.028144836425781 = 0.009250638075172901 + 2.0 * 6.00944709777832
Epoch 1640, val loss: 1.2941886186599731
Epoch 1650, training loss: 12.031042098999023 = 0.009098631329834461 + 2.0 * 6.010971546173096
Epoch 1650, val loss: 1.2977783679962158
Epoch 1660, training loss: 12.029779434204102 = 0.008949736133217812 + 2.0 * 6.010415077209473
Epoch 1660, val loss: 1.3013862371444702
Epoch 1670, training loss: 12.028250694274902 = 0.008804730139672756 + 2.0 * 6.00972318649292
Epoch 1670, val loss: 1.304975152015686
Epoch 1680, training loss: 12.033426284790039 = 0.008664368651807308 + 2.0 * 6.012381076812744
Epoch 1680, val loss: 1.3085428476333618
Epoch 1690, training loss: 12.026337623596191 = 0.008526897989213467 + 2.0 * 6.008905410766602
Epoch 1690, val loss: 1.311902642250061
Epoch 1700, training loss: 12.025416374206543 = 0.008393064141273499 + 2.0 * 6.008511543273926
Epoch 1700, val loss: 1.3154276609420776
Epoch 1710, training loss: 12.022558212280273 = 0.008262117393314838 + 2.0 * 6.007148265838623
Epoch 1710, val loss: 1.3188512325286865
Epoch 1720, training loss: 12.025192260742188 = 0.008135143667459488 + 2.0 * 6.008528709411621
Epoch 1720, val loss: 1.3221780061721802
Epoch 1730, training loss: 12.026993751525879 = 0.008010716177523136 + 2.0 * 6.009491443634033
Epoch 1730, val loss: 1.325581669807434
Epoch 1740, training loss: 12.022350311279297 = 0.007889472879469395 + 2.0 * 6.007230281829834
Epoch 1740, val loss: 1.328993558883667
Epoch 1750, training loss: 12.024176597595215 = 0.0077716633677482605 + 2.0 * 6.00820255279541
Epoch 1750, val loss: 1.3322885036468506
Epoch 1760, training loss: 12.02439022064209 = 0.007656940259039402 + 2.0 * 6.008366584777832
Epoch 1760, val loss: 1.3355053663253784
Epoch 1770, training loss: 12.030158996582031 = 0.007545733358711004 + 2.0 * 6.0113067626953125
Epoch 1770, val loss: 1.3386328220367432
Epoch 1780, training loss: 12.02345085144043 = 0.007434522733092308 + 2.0 * 6.008008003234863
Epoch 1780, val loss: 1.3418538570404053
Epoch 1790, training loss: 12.018826484680176 = 0.007327897008508444 + 2.0 * 6.005749225616455
Epoch 1790, val loss: 1.345075249671936
Epoch 1800, training loss: 12.018179893493652 = 0.007223885972052813 + 2.0 * 6.0054779052734375
Epoch 1800, val loss: 1.3481520414352417
Epoch 1810, training loss: 12.02584457397461 = 0.00712236762046814 + 2.0 * 6.009361267089844
Epoch 1810, val loss: 1.351146936416626
Epoch 1820, training loss: 12.02127456665039 = 0.007022188976407051 + 2.0 * 6.007126331329346
Epoch 1820, val loss: 1.354325532913208
Epoch 1830, training loss: 12.020858764648438 = 0.006925472989678383 + 2.0 * 6.006966590881348
Epoch 1830, val loss: 1.3573410511016846
Epoch 1840, training loss: 12.015939712524414 = 0.0068298825062811375 + 2.0 * 6.004554748535156
Epoch 1840, val loss: 1.3603867292404175
Epoch 1850, training loss: 12.017985343933105 = 0.006736810319125652 + 2.0 * 6.005624294281006
Epoch 1850, val loss: 1.3633371591567993
Epoch 1860, training loss: 12.024658203125 = 0.006646847818046808 + 2.0 * 6.009005546569824
Epoch 1860, val loss: 1.3661887645721436
Epoch 1870, training loss: 12.014946937561035 = 0.006557080894708633 + 2.0 * 6.004194736480713
Epoch 1870, val loss: 1.3690639734268188
Epoch 1880, training loss: 12.013723373413086 = 0.006470076274126768 + 2.0 * 6.003626823425293
Epoch 1880, val loss: 1.3720332384109497
Epoch 1890, training loss: 12.01308536529541 = 0.006385996006429195 + 2.0 * 6.003349781036377
Epoch 1890, val loss: 1.374910831451416
Epoch 1900, training loss: 12.022039413452148 = 0.006302786525338888 + 2.0 * 6.00786828994751
Epoch 1900, val loss: 1.3776323795318604
Epoch 1910, training loss: 12.013574600219727 = 0.006221330724656582 + 2.0 * 6.003676414489746
Epoch 1910, val loss: 1.3805081844329834
Epoch 1920, training loss: 12.012787818908691 = 0.006141933146864176 + 2.0 * 6.003323078155518
Epoch 1920, val loss: 1.3833900690078735
Epoch 1930, training loss: 12.016639709472656 = 0.006064046639949083 + 2.0 * 6.0052876472473145
Epoch 1930, val loss: 1.3860598802566528
Epoch 1940, training loss: 12.01455020904541 = 0.005987599492073059 + 2.0 * 6.004281520843506
Epoch 1940, val loss: 1.3887262344360352
Epoch 1950, training loss: 12.013663291931152 = 0.0059131598100066185 + 2.0 * 6.003875255584717
Epoch 1950, val loss: 1.3914023637771606
Epoch 1960, training loss: 12.009920120239258 = 0.005840820260345936 + 2.0 * 6.002039432525635
Epoch 1960, val loss: 1.3942289352416992
Epoch 1970, training loss: 12.006755828857422 = 0.005769974552094936 + 2.0 * 6.000493049621582
Epoch 1970, val loss: 1.3968490362167358
Epoch 1980, training loss: 12.006233215332031 = 0.0057004657573997974 + 2.0 * 6.0002665519714355
Epoch 1980, val loss: 1.3994956016540527
Epoch 1990, training loss: 12.013741493225098 = 0.005632266402244568 + 2.0 * 6.004054546356201
Epoch 1990, val loss: 1.4019633531570435
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.7269
Flip ASR: 0.6756/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.676311492919922 = 1.9285173416137695 + 2.0 * 8.373896598815918
Epoch 0, val loss: 1.9237720966339111
Epoch 10, training loss: 18.66562271118164 = 1.9185980558395386 + 2.0 * 8.373512268066406
Epoch 10, val loss: 1.9143484830856323
Epoch 20, training loss: 18.647546768188477 = 1.9063671827316284 + 2.0 * 8.370590209960938
Epoch 20, val loss: 1.9024029970169067
Epoch 30, training loss: 18.58668327331543 = 1.889959692955017 + 2.0 * 8.34836196899414
Epoch 30, val loss: 1.8862004280090332
Epoch 40, training loss: 18.27676773071289 = 1.8705220222473145 + 2.0 * 8.203123092651367
Epoch 40, val loss: 1.8678863048553467
Epoch 50, training loss: 17.175085067749023 = 1.85099196434021 + 2.0 * 7.662046909332275
Epoch 50, val loss: 1.8498446941375732
Epoch 60, training loss: 16.327634811401367 = 1.8339537382125854 + 2.0 * 7.246840953826904
Epoch 60, val loss: 1.8346894979476929
Epoch 70, training loss: 15.625861167907715 = 1.8208712339401245 + 2.0 * 6.90249490737915
Epoch 70, val loss: 1.8225531578063965
Epoch 80, training loss: 15.245830535888672 = 1.8078659772872925 + 2.0 * 6.718982219696045
Epoch 80, val loss: 1.8104126453399658
Epoch 90, training loss: 14.988619804382324 = 1.7939603328704834 + 2.0 * 6.597329616546631
Epoch 90, val loss: 1.7973963022232056
Epoch 100, training loss: 14.784952163696289 = 1.7791876792907715 + 2.0 * 6.502882480621338
Epoch 100, val loss: 1.783916711807251
Epoch 110, training loss: 14.634913444519043 = 1.7642509937286377 + 2.0 * 6.435331344604492
Epoch 110, val loss: 1.770532250404358
Epoch 120, training loss: 14.527581214904785 = 1.748454213142395 + 2.0 * 6.38956356048584
Epoch 120, val loss: 1.7566415071487427
Epoch 130, training loss: 14.436929702758789 = 1.7308274507522583 + 2.0 * 6.35305118560791
Epoch 130, val loss: 1.7412670850753784
Epoch 140, training loss: 14.362031936645508 = 1.7107077836990356 + 2.0 * 6.325662136077881
Epoch 140, val loss: 1.7239948511123657
Epoch 150, training loss: 14.293514251708984 = 1.6881383657455444 + 2.0 * 6.302688121795654
Epoch 150, val loss: 1.7047823667526245
Epoch 160, training loss: 14.226632118225098 = 1.6628625392913818 + 2.0 * 6.281884670257568
Epoch 160, val loss: 1.6834310293197632
Epoch 170, training loss: 14.163487434387207 = 1.6342504024505615 + 2.0 * 6.264618396759033
Epoch 170, val loss: 1.659483551979065
Epoch 180, training loss: 14.101072311401367 = 1.6016583442687988 + 2.0 * 6.249706745147705
Epoch 180, val loss: 1.6326195001602173
Epoch 190, training loss: 14.04006290435791 = 1.5649993419647217 + 2.0 * 6.237531661987305
Epoch 190, val loss: 1.6023932695388794
Epoch 200, training loss: 13.97091293334961 = 1.5241706371307373 + 2.0 * 6.2233710289001465
Epoch 200, val loss: 1.5688378810882568
Epoch 210, training loss: 13.904672622680664 = 1.4790141582489014 + 2.0 * 6.212829113006592
Epoch 210, val loss: 1.532027244567871
Epoch 220, training loss: 13.838813781738281 = 1.4296884536743164 + 2.0 * 6.204562664031982
Epoch 220, val loss: 1.4921863079071045
Epoch 230, training loss: 13.769685745239258 = 1.377991795539856 + 2.0 * 6.195847034454346
Epoch 230, val loss: 1.4506340026855469
Epoch 240, training loss: 13.69911003112793 = 1.3246523141860962 + 2.0 * 6.187228679656982
Epoch 240, val loss: 1.4079536199569702
Epoch 250, training loss: 13.629962921142578 = 1.2700713872909546 + 2.0 * 6.179945945739746
Epoch 250, val loss: 1.3645341396331787
Epoch 260, training loss: 13.561752319335938 = 1.215006709098816 + 2.0 * 6.173372745513916
Epoch 260, val loss: 1.3207978010177612
Epoch 270, training loss: 13.505034446716309 = 1.1604938507080078 + 2.0 * 6.17227029800415
Epoch 270, val loss: 1.27774178981781
Epoch 280, training loss: 13.434842109680176 = 1.1081228256225586 + 2.0 * 6.163359642028809
Epoch 280, val loss: 1.236325740814209
Epoch 290, training loss: 13.373895645141602 = 1.0580883026123047 + 2.0 * 6.157903671264648
Epoch 290, val loss: 1.196935772895813
Epoch 300, training loss: 13.317214012145996 = 1.010459542274475 + 2.0 * 6.153377056121826
Epoch 300, val loss: 1.1597118377685547
Epoch 310, training loss: 13.263469696044922 = 0.9654364585876465 + 2.0 * 6.149016380310059
Epoch 310, val loss: 1.124861240386963
Epoch 320, training loss: 13.211607933044434 = 0.9230129718780518 + 2.0 * 6.1442975997924805
Epoch 320, val loss: 1.0923902988433838
Epoch 330, training loss: 13.164061546325684 = 0.8827483057975769 + 2.0 * 6.140656471252441
Epoch 330, val loss: 1.062074899673462
Epoch 340, training loss: 13.119221687316895 = 0.8445436954498291 + 2.0 * 6.137339115142822
Epoch 340, val loss: 1.0338083505630493
Epoch 350, training loss: 13.078808784484863 = 0.8084084391593933 + 2.0 * 6.135200023651123
Epoch 350, val loss: 1.0076388120651245
Epoch 360, training loss: 13.035861015319824 = 0.7739256620407104 + 2.0 * 6.130967617034912
Epoch 360, val loss: 0.9832285642623901
Epoch 370, training loss: 12.996063232421875 = 0.7407896518707275 + 2.0 * 6.127636909484863
Epoch 370, val loss: 0.9603663682937622
Epoch 380, training loss: 12.974262237548828 = 0.7087891697883606 + 2.0 * 6.132736682891846
Epoch 380, val loss: 0.9388870596885681
Epoch 390, training loss: 12.92551326751709 = 0.6781634092330933 + 2.0 * 6.1236748695373535
Epoch 390, val loss: 0.9188899397850037
Epoch 400, training loss: 12.88735580444336 = 0.648348331451416 + 2.0 * 6.119503498077393
Epoch 400, val loss: 0.9001083374023438
Epoch 410, training loss: 12.85438060760498 = 0.6192499995231628 + 2.0 * 6.117565155029297
Epoch 410, val loss: 0.8822936415672302
Epoch 420, training loss: 12.817093849182129 = 0.5907818675041199 + 2.0 * 6.113155841827393
Epoch 420, val loss: 0.8654246926307678
Epoch 430, training loss: 12.78325366973877 = 0.5627778172492981 + 2.0 * 6.110238075256348
Epoch 430, val loss: 0.8493525385856628
Epoch 440, training loss: 12.766632080078125 = 0.5353025197982788 + 2.0 * 6.115664958953857
Epoch 440, val loss: 0.8339946269989014
Epoch 450, training loss: 12.7254638671875 = 0.5083453059196472 + 2.0 * 6.1085591316223145
Epoch 450, val loss: 0.8194618225097656
Epoch 460, training loss: 12.690290451049805 = 0.4820989668369293 + 2.0 * 6.104095935821533
Epoch 460, val loss: 0.8057940602302551
Epoch 470, training loss: 12.658719062805176 = 0.45636776089668274 + 2.0 * 6.101175785064697
Epoch 470, val loss: 0.7928768396377563
Epoch 480, training loss: 12.65066909790039 = 0.43126457929611206 + 2.0 * 6.109702110290527
Epoch 480, val loss: 0.7807167768478394
Epoch 490, training loss: 12.603751182556152 = 0.4069589078426361 + 2.0 * 6.098396301269531
Epoch 490, val loss: 0.7695541381835938
Epoch 500, training loss: 12.57367992401123 = 0.383483350276947 + 2.0 * 6.095098495483398
Epoch 500, val loss: 0.7594795227050781
Epoch 510, training loss: 12.546954154968262 = 0.3608459234237671 + 2.0 * 6.093054294586182
Epoch 510, val loss: 0.7504125237464905
Epoch 520, training loss: 12.521245002746582 = 0.3392080068588257 + 2.0 * 6.0910186767578125
Epoch 520, val loss: 0.7423426508903503
Epoch 530, training loss: 12.49795913696289 = 0.31871846318244934 + 2.0 * 6.089620113372803
Epoch 530, val loss: 0.7354934215545654
Epoch 540, training loss: 12.474150657653809 = 0.2993502914905548 + 2.0 * 6.087399959564209
Epoch 540, val loss: 0.7297852039337158
Epoch 550, training loss: 12.451640129089355 = 0.2810693681240082 + 2.0 * 6.085285186767578
Epoch 550, val loss: 0.7251853942871094
Epoch 560, training loss: 12.437875747680664 = 0.26385557651519775 + 2.0 * 6.087009906768799
Epoch 560, val loss: 0.7215831279754639
Epoch 570, training loss: 12.418560028076172 = 0.24784758687019348 + 2.0 * 6.08535623550415
Epoch 570, val loss: 0.7189066410064697
Epoch 580, training loss: 12.393728256225586 = 0.2329643815755844 + 2.0 * 6.080381870269775
Epoch 580, val loss: 0.7173346877098083
Epoch 590, training loss: 12.375921249389648 = 0.2191234827041626 + 2.0 * 6.078398704528809
Epoch 590, val loss: 0.7167192101478577
Epoch 600, training loss: 12.362212181091309 = 0.2062419354915619 + 2.0 * 6.0779852867126465
Epoch 600, val loss: 0.7169075012207031
Epoch 610, training loss: 12.358478546142578 = 0.1943676620721817 + 2.0 * 6.082055568695068
Epoch 610, val loss: 0.7177396416664124
Epoch 620, training loss: 12.336265563964844 = 0.18332909047603607 + 2.0 * 6.076468467712402
Epoch 620, val loss: 0.7192866802215576
Epoch 630, training loss: 12.319509506225586 = 0.17311735451221466 + 2.0 * 6.073195934295654
Epoch 630, val loss: 0.7215426564216614
Epoch 640, training loss: 12.307466506958008 = 0.16360101103782654 + 2.0 * 6.071932792663574
Epoch 640, val loss: 0.7244006395339966
Epoch 650, training loss: 12.299337387084961 = 0.15473902225494385 + 2.0 * 6.072299003601074
Epoch 650, val loss: 0.7277078628540039
Epoch 660, training loss: 12.285529136657715 = 0.14645683765411377 + 2.0 * 6.069536209106445
Epoch 660, val loss: 0.7315319776535034
Epoch 670, training loss: 12.27364730834961 = 0.1387791633605957 + 2.0 * 6.067434310913086
Epoch 670, val loss: 0.7357724905014038
Epoch 680, training loss: 12.26537036895752 = 0.1316382884979248 + 2.0 * 6.066865921020508
Epoch 680, val loss: 0.7404833436012268
Epoch 690, training loss: 12.254349708557129 = 0.12492729723453522 + 2.0 * 6.064711093902588
Epoch 690, val loss: 0.745546817779541
Epoch 700, training loss: 12.255810737609863 = 0.11865849047899246 + 2.0 * 6.068576335906982
Epoch 700, val loss: 0.7509157061576843
Epoch 710, training loss: 12.24216365814209 = 0.11277878284454346 + 2.0 * 6.064692497253418
Epoch 710, val loss: 0.7565647959709167
Epoch 720, training loss: 12.232989311218262 = 0.10729421675205231 + 2.0 * 6.06284761428833
Epoch 720, val loss: 0.7624515891075134
Epoch 730, training loss: 12.221715927124023 = 0.10215047001838684 + 2.0 * 6.0597825050354
Epoch 730, val loss: 0.768628716468811
Epoch 740, training loss: 12.22263240814209 = 0.09730827063322067 + 2.0 * 6.062662124633789
Epoch 740, val loss: 0.7749630212783813
Epoch 750, training loss: 12.21675968170166 = 0.09278953820466995 + 2.0 * 6.061985015869141
Epoch 750, val loss: 0.7813218832015991
Epoch 760, training loss: 12.201812744140625 = 0.08854787796735764 + 2.0 * 6.0566325187683105
Epoch 760, val loss: 0.7880147695541382
Epoch 770, training loss: 12.195631980895996 = 0.08456406742334366 + 2.0 * 6.0555338859558105
Epoch 770, val loss: 0.7948427200317383
Epoch 780, training loss: 12.19510555267334 = 0.08081850409507751 + 2.0 * 6.057143688201904
Epoch 780, val loss: 0.8018433451652527
Epoch 790, training loss: 12.188549041748047 = 0.07725697010755539 + 2.0 * 6.055645942687988
Epoch 790, val loss: 0.8086445927619934
Epoch 800, training loss: 12.181053161621094 = 0.0739397183060646 + 2.0 * 6.0535569190979
Epoch 800, val loss: 0.8157186508178711
Epoch 810, training loss: 12.173163414001465 = 0.07079613208770752 + 2.0 * 6.051183700561523
Epoch 810, val loss: 0.8228774666786194
Epoch 820, training loss: 12.172367095947266 = 0.06783322989940643 + 2.0 * 6.052267074584961
Epoch 820, val loss: 0.8300685286521912
Epoch 830, training loss: 12.163196563720703 = 0.06503412872552872 + 2.0 * 6.049081325531006
Epoch 830, val loss: 0.8372379541397095
Epoch 840, training loss: 12.170281410217285 = 0.06238449737429619 + 2.0 * 6.053948402404785
Epoch 840, val loss: 0.8444035649299622
Epoch 850, training loss: 12.158495903015137 = 0.05989756062626839 + 2.0 * 6.049299240112305
Epoch 850, val loss: 0.8516591787338257
Epoch 860, training loss: 12.151530265808105 = 0.05752783641219139 + 2.0 * 6.047001361846924
Epoch 860, val loss: 0.8587837219238281
Epoch 870, training loss: 12.146912574768066 = 0.05530045926570892 + 2.0 * 6.045805931091309
Epoch 870, val loss: 0.866104245185852
Epoch 880, training loss: 12.162100791931152 = 0.05317823961377144 + 2.0 * 6.054461479187012
Epoch 880, val loss: 0.8733142018318176
Epoch 890, training loss: 12.14581298828125 = 0.051174066960811615 + 2.0 * 6.047319412231445
Epoch 890, val loss: 0.8801749348640442
Epoch 900, training loss: 12.134712219238281 = 0.04927927628159523 + 2.0 * 6.0427165031433105
Epoch 900, val loss: 0.8872870802879333
Epoch 910, training loss: 12.131543159484863 = 0.0474790558218956 + 2.0 * 6.042032241821289
Epoch 910, val loss: 0.8944270610809326
Epoch 920, training loss: 12.131331443786621 = 0.04576629400253296 + 2.0 * 6.042782783508301
Epoch 920, val loss: 0.9014221429824829
Epoch 930, training loss: 12.131813049316406 = 0.04414796829223633 + 2.0 * 6.043832778930664
Epoch 930, val loss: 0.9083369970321655
Epoch 940, training loss: 12.122337341308594 = 0.04260812699794769 + 2.0 * 6.039864540100098
Epoch 940, val loss: 0.9150861501693726
Epoch 950, training loss: 12.119361877441406 = 0.041151486337184906 + 2.0 * 6.039105415344238
Epoch 950, val loss: 0.9219551086425781
Epoch 960, training loss: 12.11646842956543 = 0.03976261243224144 + 2.0 * 6.038352966308594
Epoch 960, val loss: 0.928859531879425
Epoch 970, training loss: 12.11667537689209 = 0.03843461349606514 + 2.0 * 6.039120197296143
Epoch 970, val loss: 0.9356000423431396
Epoch 980, training loss: 12.11290454864502 = 0.03717317432165146 + 2.0 * 6.03786563873291
Epoch 980, val loss: 0.9421961903572083
Epoch 990, training loss: 12.11624526977539 = 0.03596458211541176 + 2.0 * 6.040140151977539
Epoch 990, val loss: 0.9486409425735474
Epoch 1000, training loss: 12.105498313903809 = 0.03483320772647858 + 2.0 * 6.035332679748535
Epoch 1000, val loss: 0.9552352428436279
Epoch 1010, training loss: 12.105148315429688 = 0.03374829515814781 + 2.0 * 6.035699844360352
Epoch 1010, val loss: 0.9618374109268188
Epoch 1020, training loss: 12.105730056762695 = 0.03270536661148071 + 2.0 * 6.03651237487793
Epoch 1020, val loss: 0.9683164358139038
Epoch 1030, training loss: 12.102496147155762 = 0.03170301765203476 + 2.0 * 6.035396575927734
Epoch 1030, val loss: 0.9746021628379822
Epoch 1040, training loss: 12.097843170166016 = 0.03074939362704754 + 2.0 * 6.0335469245910645
Epoch 1040, val loss: 0.9808443188667297
Epoch 1050, training loss: 12.100974082946777 = 0.029841091483831406 + 2.0 * 6.035566329956055
Epoch 1050, val loss: 0.9871602654457092
Epoch 1060, training loss: 12.09420108795166 = 0.028966838493943214 + 2.0 * 6.032617092132568
Epoch 1060, val loss: 0.9932679533958435
Epoch 1070, training loss: 12.092301368713379 = 0.028132669627666473 + 2.0 * 6.0320844650268555
Epoch 1070, val loss: 0.9993176460266113
Epoch 1080, training loss: 12.089092254638672 = 0.027338555082678795 + 2.0 * 6.030876636505127
Epoch 1080, val loss: 1.0053482055664062
Epoch 1090, training loss: 12.087567329406738 = 0.026571758091449738 + 2.0 * 6.0304975509643555
Epoch 1090, val loss: 1.0113731622695923
Epoch 1100, training loss: 12.098536491394043 = 0.02583487518131733 + 2.0 * 6.036350727081299
Epoch 1100, val loss: 1.0172659158706665
Epoch 1110, training loss: 12.089674949645996 = 0.02513117901980877 + 2.0 * 6.032271862030029
Epoch 1110, val loss: 1.0229408740997314
Epoch 1120, training loss: 12.08330249786377 = 0.024457169696688652 + 2.0 * 6.029422760009766
Epoch 1120, val loss: 1.0287402868270874
Epoch 1130, training loss: 12.080156326293945 = 0.02381051331758499 + 2.0 * 6.028172969818115
Epoch 1130, val loss: 1.0345396995544434
Epoch 1140, training loss: 12.102424621582031 = 0.023188162595033646 + 2.0 * 6.039618015289307
Epoch 1140, val loss: 1.0403468608856201
Epoch 1150, training loss: 12.082541465759277 = 0.022588195279240608 + 2.0 * 6.029976844787598
Epoch 1150, val loss: 1.04550302028656
Epoch 1160, training loss: 12.078770637512207 = 0.02201748639345169 + 2.0 * 6.028376579284668
Epoch 1160, val loss: 1.050876498222351
Epoch 1170, training loss: 12.074625015258789 = 0.02147047407925129 + 2.0 * 6.026577472686768
Epoch 1170, val loss: 1.0563997030258179
Epoch 1180, training loss: 12.071501731872559 = 0.02094123885035515 + 2.0 * 6.025280475616455
Epoch 1180, val loss: 1.0619313716888428
Epoch 1190, training loss: 12.071048736572266 = 0.020426766946911812 + 2.0 * 6.02531099319458
Epoch 1190, val loss: 1.0672721862792969
Epoch 1200, training loss: 12.089168548583984 = 0.01992913708090782 + 2.0 * 6.0346198081970215
Epoch 1200, val loss: 1.072455644607544
Epoch 1210, training loss: 12.06783676147461 = 0.01945650763809681 + 2.0 * 6.0241899490356445
Epoch 1210, val loss: 1.077460765838623
Epoch 1220, training loss: 12.068364143371582 = 0.019003812223672867 + 2.0 * 6.024680137634277
Epoch 1220, val loss: 1.0827101469039917
Epoch 1230, training loss: 12.064950942993164 = 0.01856108568608761 + 2.0 * 6.023194789886475
Epoch 1230, val loss: 1.0878379344940186
Epoch 1240, training loss: 12.063961029052734 = 0.01813516579568386 + 2.0 * 6.022912979125977
Epoch 1240, val loss: 1.0929313898086548
Epoch 1250, training loss: 12.084264755249023 = 0.017722368240356445 + 2.0 * 6.033271312713623
Epoch 1250, val loss: 1.0979156494140625
Epoch 1260, training loss: 12.062417030334473 = 0.017322342842817307 + 2.0 * 6.022547245025635
Epoch 1260, val loss: 1.1026320457458496
Epoch 1270, training loss: 12.06116771697998 = 0.016939176246523857 + 2.0 * 6.022114276885986
Epoch 1270, val loss: 1.1075035333633423
Epoch 1280, training loss: 12.05897045135498 = 0.01656864956021309 + 2.0 * 6.021201133728027
Epoch 1280, val loss: 1.1123915910720825
Epoch 1290, training loss: 12.076196670532227 = 0.016209900379180908 + 2.0 * 6.029993534088135
Epoch 1290, val loss: 1.1171259880065918
Epoch 1300, training loss: 12.06631851196289 = 0.015862591564655304 + 2.0 * 6.025228023529053
Epoch 1300, val loss: 1.1217751502990723
Epoch 1310, training loss: 12.054728507995605 = 0.015526628121733665 + 2.0 * 6.019600868225098
Epoch 1310, val loss: 1.1263370513916016
Epoch 1320, training loss: 12.055403709411621 = 0.015202670358121395 + 2.0 * 6.0201005935668945
Epoch 1320, val loss: 1.1309956312179565
Epoch 1330, training loss: 12.060832977294922 = 0.014887558296322823 + 2.0 * 6.022972583770752
Epoch 1330, val loss: 1.1356357336044312
Epoch 1340, training loss: 12.064783096313477 = 0.01458496693521738 + 2.0 * 6.025099277496338
Epoch 1340, val loss: 1.1399388313293457
Epoch 1350, training loss: 12.055757522583008 = 0.014287343248724937 + 2.0 * 6.020735263824463
Epoch 1350, val loss: 1.1442359685897827
Epoch 1360, training loss: 12.050975799560547 = 0.014004314318299294 + 2.0 * 6.0184855461120605
Epoch 1360, val loss: 1.1486637592315674
Epoch 1370, training loss: 12.048576354980469 = 0.013726789504289627 + 2.0 * 6.017424583435059
Epoch 1370, val loss: 1.1530879735946655
Epoch 1380, training loss: 12.04981517791748 = 0.013455933891236782 + 2.0 * 6.018179416656494
Epoch 1380, val loss: 1.1573829650878906
Epoch 1390, training loss: 12.056682586669922 = 0.013192014768719673 + 2.0 * 6.021745204925537
Epoch 1390, val loss: 1.161555290222168
Epoch 1400, training loss: 12.049787521362305 = 0.012937541119754314 + 2.0 * 6.018424987792969
Epoch 1400, val loss: 1.1655645370483398
Epoch 1410, training loss: 12.046856880187988 = 0.012696020305156708 + 2.0 * 6.017080307006836
Epoch 1410, val loss: 1.1697039604187012
Epoch 1420, training loss: 12.045194625854492 = 0.012457851320505142 + 2.0 * 6.016368389129639
Epoch 1420, val loss: 1.1738985776901245
Epoch 1430, training loss: 12.048076629638672 = 0.012227407656610012 + 2.0 * 6.017924785614014
Epoch 1430, val loss: 1.1780556440353394
Epoch 1440, training loss: 12.043458938598633 = 0.011999499052762985 + 2.0 * 6.015729904174805
Epoch 1440, val loss: 1.1820268630981445
Epoch 1450, training loss: 12.046171188354492 = 0.011778208427131176 + 2.0 * 6.0171966552734375
Epoch 1450, val loss: 1.1859980821609497
Epoch 1460, training loss: 12.049681663513184 = 0.011567644774913788 + 2.0 * 6.019056797027588
Epoch 1460, val loss: 1.189979910850525
Epoch 1470, training loss: 12.041656494140625 = 0.011355264112353325 + 2.0 * 6.015150547027588
Epoch 1470, val loss: 1.1936538219451904
Epoch 1480, training loss: 12.0394287109375 = 0.011156669817864895 + 2.0 * 6.014135837554932
Epoch 1480, val loss: 1.19765043258667
Epoch 1490, training loss: 12.037657737731934 = 0.010959308594465256 + 2.0 * 6.0133490562438965
Epoch 1490, val loss: 1.2015299797058105
Epoch 1500, training loss: 12.039464950561523 = 0.010765830986201763 + 2.0 * 6.014349460601807
Epoch 1500, val loss: 1.2054003477096558
Epoch 1510, training loss: 12.040449142456055 = 0.010577702894806862 + 2.0 * 6.014935493469238
Epoch 1510, val loss: 1.2091491222381592
Epoch 1520, training loss: 12.036897659301758 = 0.010396101512014866 + 2.0 * 6.013250827789307
Epoch 1520, val loss: 1.2127254009246826
Epoch 1530, training loss: 12.037893295288086 = 0.010218726471066475 + 2.0 * 6.0138373374938965
Epoch 1530, val loss: 1.2163878679275513
Epoch 1540, training loss: 12.041402816772461 = 0.010046867653727531 + 2.0 * 6.0156779289245605
Epoch 1540, val loss: 1.2200641632080078
Epoch 1550, training loss: 12.037861824035645 = 0.009879861027002335 + 2.0 * 6.013990879058838
Epoch 1550, val loss: 1.2236263751983643
Epoch 1560, training loss: 12.037711143493652 = 0.009717862121760845 + 2.0 * 6.013996601104736
Epoch 1560, val loss: 1.2270722389221191
Epoch 1570, training loss: 12.034263610839844 = 0.009557586163282394 + 2.0 * 6.01235294342041
Epoch 1570, val loss: 1.2305598258972168
Epoch 1580, training loss: 12.033166885375977 = 0.009401947259902954 + 2.0 * 6.011882305145264
Epoch 1580, val loss: 1.2339777946472168
Epoch 1590, training loss: 12.031757354736328 = 0.009251044131815434 + 2.0 * 6.011253356933594
Epoch 1590, val loss: 1.237449049949646
Epoch 1600, training loss: 12.040639877319336 = 0.009104708209633827 + 2.0 * 6.015767574310303
Epoch 1600, val loss: 1.240907073020935
Epoch 1610, training loss: 12.037432670593262 = 0.008960594423115253 + 2.0 * 6.014235973358154
Epoch 1610, val loss: 1.244170069694519
Epoch 1620, training loss: 12.03038215637207 = 0.00881930161267519 + 2.0 * 6.010781288146973
Epoch 1620, val loss: 1.2473288774490356
Epoch 1630, training loss: 12.029036521911621 = 0.008683355525135994 + 2.0 * 6.010176658630371
Epoch 1630, val loss: 1.25076162815094
Epoch 1640, training loss: 12.045969009399414 = 0.008551020175218582 + 2.0 * 6.018709182739258
Epoch 1640, val loss: 1.2541663646697998
Epoch 1650, training loss: 12.033312797546387 = 0.008420532569289207 + 2.0 * 6.01244592666626
Epoch 1650, val loss: 1.2570750713348389
Epoch 1660, training loss: 12.027828216552734 = 0.008293494582176208 + 2.0 * 6.009767532348633
Epoch 1660, val loss: 1.2602533102035522
Epoch 1670, training loss: 12.026535987854004 = 0.008170640096068382 + 2.0 * 6.009182453155518
Epoch 1670, val loss: 1.2634915113449097
Epoch 1680, training loss: 12.034313201904297 = 0.008051025681197643 + 2.0 * 6.013131141662598
Epoch 1680, val loss: 1.2667181491851807
Epoch 1690, training loss: 12.025253295898438 = 0.00793153140693903 + 2.0 * 6.008660793304443
Epoch 1690, val loss: 1.269707441329956
Epoch 1700, training loss: 12.022527694702148 = 0.007815386168658733 + 2.0 * 6.0073561668396
Epoch 1700, val loss: 1.2727296352386475
Epoch 1710, training loss: 12.025395393371582 = 0.007703803014010191 + 2.0 * 6.008845806121826
Epoch 1710, val loss: 1.2758800983428955
Epoch 1720, training loss: 12.028923034667969 = 0.007592728827148676 + 2.0 * 6.010664939880371
Epoch 1720, val loss: 1.2788798809051514
Epoch 1730, training loss: 12.022541046142578 = 0.007485790643841028 + 2.0 * 6.007527828216553
Epoch 1730, val loss: 1.281784176826477
Epoch 1740, training loss: 12.02293586730957 = 0.00738187413662672 + 2.0 * 6.007777214050293
Epoch 1740, val loss: 1.2848095893859863
Epoch 1750, training loss: 12.036056518554688 = 0.007282155565917492 + 2.0 * 6.014387130737305
Epoch 1750, val loss: 1.2877305746078491
Epoch 1760, training loss: 12.02239990234375 = 0.007177156861871481 + 2.0 * 6.007611274719238
Epoch 1760, val loss: 1.290332555770874
Epoch 1770, training loss: 12.01904010772705 = 0.00708185788244009 + 2.0 * 6.005979061126709
Epoch 1770, val loss: 1.293318510055542
Epoch 1780, training loss: 12.01961612701416 = 0.006986082997173071 + 2.0 * 6.006315231323242
Epoch 1780, val loss: 1.2961196899414062
Epoch 1790, training loss: 12.026229858398438 = 0.00689283199608326 + 2.0 * 6.009668350219727
Epoch 1790, val loss: 1.2989764213562012
Epoch 1800, training loss: 12.023871421813965 = 0.0068002669140696526 + 2.0 * 6.008535385131836
Epoch 1800, val loss: 1.301619529724121
Epoch 1810, training loss: 12.020596504211426 = 0.006712498143315315 + 2.0 * 6.006941795349121
Epoch 1810, val loss: 1.3042912483215332
Epoch 1820, training loss: 12.017072677612305 = 0.00662530492991209 + 2.0 * 6.005223751068115
Epoch 1820, val loss: 1.3069403171539307
Epoch 1830, training loss: 12.015207290649414 = 0.00654053408652544 + 2.0 * 6.00433349609375
Epoch 1830, val loss: 1.3097245693206787
Epoch 1840, training loss: 12.016494750976562 = 0.006456430070102215 + 2.0 * 6.005019187927246
Epoch 1840, val loss: 1.3124313354492188
Epoch 1850, training loss: 12.021778106689453 = 0.006374849937856197 + 2.0 * 6.007701396942139
Epoch 1850, val loss: 1.3151220083236694
Epoch 1860, training loss: 12.020752906799316 = 0.006293420679867268 + 2.0 * 6.007229804992676
Epoch 1860, val loss: 1.3174645900726318
Epoch 1870, training loss: 12.021642684936523 = 0.006214487366378307 + 2.0 * 6.00771427154541
Epoch 1870, val loss: 1.319932460784912
Epoch 1880, training loss: 12.013860702514648 = 0.006138981785625219 + 2.0 * 6.003860950469971
Epoch 1880, val loss: 1.3224427700042725
Epoch 1890, training loss: 12.013389587402344 = 0.0060642678290605545 + 2.0 * 6.003662586212158
Epoch 1890, val loss: 1.3250113725662231
Epoch 1900, training loss: 12.016465187072754 = 0.005990617908537388 + 2.0 * 6.005237102508545
Epoch 1900, val loss: 1.3275723457336426
Epoch 1910, training loss: 12.012020111083984 = 0.005917638074606657 + 2.0 * 6.003051280975342
Epoch 1910, val loss: 1.3299914598464966
Epoch 1920, training loss: 12.016128540039062 = 0.005846783518791199 + 2.0 * 6.005140781402588
Epoch 1920, val loss: 1.3324596881866455
Epoch 1930, training loss: 12.011500358581543 = 0.005778039339929819 + 2.0 * 6.002861022949219
Epoch 1930, val loss: 1.3348687887191772
Epoch 1940, training loss: 12.01339340209961 = 0.005711033940315247 + 2.0 * 6.003841400146484
Epoch 1940, val loss: 1.337233304977417
Epoch 1950, training loss: 12.024724960327148 = 0.005644379183650017 + 2.0 * 6.00954008102417
Epoch 1950, val loss: 1.339563250541687
Epoch 1960, training loss: 12.011810302734375 = 0.00557804387062788 + 2.0 * 6.003116130828857
Epoch 1960, val loss: 1.3417340517044067
Epoch 1970, training loss: 12.008709907531738 = 0.005514256190508604 + 2.0 * 6.001597881317139
Epoch 1970, val loss: 1.3440203666687012
Epoch 1980, training loss: 12.007817268371582 = 0.0054520717822015285 + 2.0 * 6.001182556152344
Epoch 1980, val loss: 1.3463841676712036
Epoch 1990, training loss: 12.014898300170898 = 0.005389799829572439 + 2.0 * 6.004754066467285
Epoch 1990, val loss: 1.348669171333313
=== picking the best model according to the performance on validation ===
target class rate on Vs: 0.8000
accuracy on clean test nodes: 0.7815
Overall ASR: 0.6937
Flip ASR: 0.6489/225 nodes
The final ASR:0.65560, 0.07859, Accuracy:0.80000, 0.01512
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11598])
remove edge: torch.Size([2, 9600])
updated graph: torch.Size([2, 10642])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9889
Flip ASR: 0.9867/225 nodes
The final ASR:0.98032, 0.00627, Accuracy:0.83210, 0.00761
Begin epxeriment: cont_weight: 2 epoch:2000 der1:0.3 der2:0.3 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 18.715456008911133 = 1.9677399396896362 + 2.0 * 8.373858451843262
Epoch 0, val loss: 1.973387598991394
Epoch 10, training loss: 18.70390510559082 = 1.9571439027786255 + 2.0 * 8.373380661010742
Epoch 10, val loss: 1.9629489183425903
Epoch 20, training loss: 18.684823989868164 = 1.944131851196289 + 2.0 * 8.370346069335938
Epoch 20, val loss: 1.9501632452011108
Epoch 30, training loss: 18.624065399169922 = 1.9266659021377563 + 2.0 * 8.348699569702148
Epoch 30, val loss: 1.9331382513046265
Epoch 40, training loss: 18.277374267578125 = 1.905410885810852 + 2.0 * 8.185981750488281
Epoch 40, val loss: 1.9127055406570435
Epoch 50, training loss: 17.0567684173584 = 1.8816025257110596 + 2.0 * 7.587583065032959
Epoch 50, val loss: 1.8898223638534546
Epoch 60, training loss: 16.3277645111084 = 1.863302230834961 + 2.0 * 7.232231140136719
Epoch 60, val loss: 1.8735499382019043
Epoch 70, training loss: 15.478775024414062 = 1.8545254468917847 + 2.0 * 6.812124729156494
Epoch 70, val loss: 1.8649675846099854
Epoch 80, training loss: 15.063228607177734 = 1.8459768295288086 + 2.0 * 6.608625888824463
Epoch 80, val loss: 1.8568928241729736
Epoch 90, training loss: 14.8198823928833 = 1.8357678651809692 + 2.0 * 6.4920573234558105
Epoch 90, val loss: 1.847057580947876
Epoch 100, training loss: 14.666654586791992 = 1.8254592418670654 + 2.0 * 6.420597553253174
Epoch 100, val loss: 1.837001085281372
Epoch 110, training loss: 14.562983512878418 = 1.8159962892532349 + 2.0 * 6.373493671417236
Epoch 110, val loss: 1.8276082277297974
Epoch 120, training loss: 14.475448608398438 = 1.8078653812408447 + 2.0 * 6.333791732788086
Epoch 120, val loss: 1.8193851709365845
Epoch 130, training loss: 14.398691177368164 = 1.8007205724716187 + 2.0 * 6.298985481262207
Epoch 130, val loss: 1.8120654821395874
Epoch 140, training loss: 14.336874008178711 = 1.7940354347229004 + 2.0 * 6.271419525146484
Epoch 140, val loss: 1.80521821975708
Epoch 150, training loss: 14.284075736999512 = 1.7872709035873413 + 2.0 * 6.2484025955200195
Epoch 150, val loss: 1.7985012531280518
Epoch 160, training loss: 14.24040412902832 = 1.7801400423049927 + 2.0 * 6.230132102966309
Epoch 160, val loss: 1.791714072227478
Epoch 170, training loss: 14.20291805267334 = 1.7723939418792725 + 2.0 * 6.215261936187744
Epoch 170, val loss: 1.784645915031433
Epoch 180, training loss: 14.167037010192871 = 1.7638005018234253 + 2.0 * 6.201618194580078
Epoch 180, val loss: 1.7771236896514893
Epoch 190, training loss: 14.133917808532715 = 1.7541052103042603 + 2.0 * 6.189906120300293
Epoch 190, val loss: 1.7688950300216675
Epoch 200, training loss: 14.104403495788574 = 1.7432001829147339 + 2.0 * 6.180601596832275
Epoch 200, val loss: 1.7598052024841309
Epoch 210, training loss: 14.073142051696777 = 1.7307054996490479 + 2.0 * 6.171218395233154
Epoch 210, val loss: 1.749710202217102
Epoch 220, training loss: 14.045125007629395 = 1.716387391090393 + 2.0 * 6.164368629455566
Epoch 220, val loss: 1.7382512092590332
Epoch 230, training loss: 14.012580871582031 = 1.6999799013137817 + 2.0 * 6.1563005447387695
Epoch 230, val loss: 1.7251619100570679
Epoch 240, training loss: 13.980171203613281 = 1.680971622467041 + 2.0 * 6.149600028991699
Epoch 240, val loss: 1.710160493850708
Epoch 250, training loss: 13.947443008422852 = 1.658843994140625 + 2.0 * 6.144299507141113
Epoch 250, val loss: 1.6927664279937744
Epoch 260, training loss: 13.911849975585938 = 1.6331181526184082 + 2.0 * 6.1393656730651855
Epoch 260, val loss: 1.6724777221679688
Epoch 270, training loss: 13.871344566345215 = 1.603543996810913 + 2.0 * 6.133900165557861
Epoch 270, val loss: 1.649239182472229
Epoch 280, training loss: 13.828977584838867 = 1.569908618927002 + 2.0 * 6.1295342445373535
Epoch 280, val loss: 1.6227432489395142
Epoch 290, training loss: 13.782225608825684 = 1.5317447185516357 + 2.0 * 6.125240325927734
Epoch 290, val loss: 1.5925865173339844
Epoch 300, training loss: 13.732425689697266 = 1.4890068769454956 + 2.0 * 6.12170934677124
Epoch 300, val loss: 1.5587542057037354
Epoch 310, training loss: 13.68311882019043 = 1.4426692724227905 + 2.0 * 6.120224952697754
Epoch 310, val loss: 1.5218919515609741
Epoch 320, training loss: 13.624920845031738 = 1.3938729763031006 + 2.0 * 6.115523815155029
Epoch 320, val loss: 1.4831382036209106
Epoch 330, training loss: 13.567010879516602 = 1.3432011604309082 + 2.0 * 6.111904621124268
Epoch 330, val loss: 1.4429748058319092
Epoch 340, training loss: 13.50924301147461 = 1.2918974161148071 + 2.0 * 6.108672618865967
Epoch 340, val loss: 1.402435541152954
Epoch 350, training loss: 13.452922821044922 = 1.2411633729934692 + 2.0 * 6.105879783630371
Epoch 350, val loss: 1.3623340129852295
Epoch 360, training loss: 13.404275894165039 = 1.1916309595108032 + 2.0 * 6.106322288513184
Epoch 360, val loss: 1.3234299421310425
Epoch 370, training loss: 13.34490966796875 = 1.1443077325820923 + 2.0 * 6.1003007888793945
Epoch 370, val loss: 1.2866626977920532
Epoch 380, training loss: 13.29179573059082 = 1.099027156829834 + 2.0 * 6.096384525299072
Epoch 380, val loss: 1.2517094612121582
Epoch 390, training loss: 13.255935668945312 = 1.0557026863098145 + 2.0 * 6.100116729736328
Epoch 390, val loss: 1.2183352708816528
Epoch 400, training loss: 13.198309898376465 = 1.0147578716278076 + 2.0 * 6.091775894165039
Epoch 400, val loss: 1.1871445178985596
Epoch 410, training loss: 13.152154922485352 = 0.9758647084236145 + 2.0 * 6.0881452560424805
Epoch 410, val loss: 1.157640814781189
Epoch 420, training loss: 13.125593185424805 = 0.9384832978248596 + 2.0 * 6.093554973602295
Epoch 420, val loss: 1.129386067390442
Epoch 430, training loss: 13.073440551757812 = 0.903181254863739 + 2.0 * 6.085129737854004
Epoch 430, val loss: 1.1027352809906006
Epoch 440, training loss: 13.030965805053711 = 0.8693463802337646 + 2.0 * 6.080809593200684
Epoch 440, val loss: 1.0772496461868286
Epoch 450, training loss: 12.995023727416992 = 0.836669385433197 + 2.0 * 6.079177379608154
Epoch 450, val loss: 1.0525460243225098
Epoch 460, training loss: 12.96314811706543 = 0.8054185509681702 + 2.0 * 6.078864574432373
Epoch 460, val loss: 1.0289816856384277
Epoch 470, training loss: 12.923891067504883 = 0.7756818532943726 + 2.0 * 6.0741047859191895
Epoch 470, val loss: 1.0066949129104614
Epoch 480, training loss: 12.891700744628906 = 0.7472329139709473 + 2.0 * 6.072234153747559
Epoch 480, val loss: 0.9853090643882751
Epoch 490, training loss: 12.860690116882324 = 0.7200566530227661 + 2.0 * 6.070316791534424
Epoch 490, val loss: 0.9650660753250122
Epoch 500, training loss: 12.830345153808594 = 0.6943994760513306 + 2.0 * 6.067972660064697
Epoch 500, val loss: 0.9460895657539368
Epoch 510, training loss: 12.804112434387207 = 0.6698204874992371 + 2.0 * 6.067145824432373
Epoch 510, val loss: 0.9281182289123535
Epoch 520, training loss: 12.779635429382324 = 0.6462649703025818 + 2.0 * 6.066685199737549
Epoch 520, val loss: 0.9112675189971924
Epoch 530, training loss: 12.749932289123535 = 0.6238633990287781 + 2.0 * 6.063034534454346
Epoch 530, val loss: 0.8954868316650391
Epoch 540, training loss: 12.724773406982422 = 0.6021548509597778 + 2.0 * 6.061309337615967
Epoch 540, val loss: 0.880643367767334
Epoch 550, training loss: 12.706576347351074 = 0.5810900926589966 + 2.0 * 6.062743186950684
Epoch 550, val loss: 0.8666465878486633
Epoch 560, training loss: 12.680305480957031 = 0.5606808662414551 + 2.0 * 6.059812068939209
Epoch 560, val loss: 0.8534573316574097
Epoch 570, training loss: 12.65759563446045 = 0.5409308671951294 + 2.0 * 6.058332443237305
Epoch 570, val loss: 0.8410934209823608
Epoch 580, training loss: 12.650554656982422 = 0.5218951106071472 + 2.0 * 6.064329624176025
Epoch 580, val loss: 0.8294704556465149
Epoch 590, training loss: 12.612749099731445 = 0.5038007497787476 + 2.0 * 6.054474353790283
Epoch 590, val loss: 0.8190565705299377
Epoch 600, training loss: 12.593134880065918 = 0.4862905740737915 + 2.0 * 6.053421974182129
Epoch 600, val loss: 0.8092933297157288
Epoch 610, training loss: 12.572354316711426 = 0.4692748785018921 + 2.0 * 6.051539897918701
Epoch 610, val loss: 0.80023193359375
Epoch 620, training loss: 12.554149627685547 = 0.45272043347358704 + 2.0 * 6.050714492797852
Epoch 620, val loss: 0.7918909788131714
Epoch 630, training loss: 12.537646293640137 = 0.43670499324798584 + 2.0 * 6.05047082901001
Epoch 630, val loss: 0.7841918468475342
Epoch 640, training loss: 12.523693084716797 = 0.42137229442596436 + 2.0 * 6.0511603355407715
Epoch 640, val loss: 0.7772854566574097
Epoch 650, training loss: 12.50252914428711 = 0.4065733850002289 + 2.0 * 6.047977924346924
Epoch 650, val loss: 0.7710765600204468
Epoch 660, training loss: 12.484185218811035 = 0.39224326610565186 + 2.0 * 6.045970916748047
Epoch 660, val loss: 0.765409529209137
Epoch 670, training loss: 12.473191261291504 = 0.3783535361289978 + 2.0 * 6.04741907119751
Epoch 670, val loss: 0.7603276371955872
Epoch 680, training loss: 12.456663131713867 = 0.3650495707988739 + 2.0 * 6.045806884765625
Epoch 680, val loss: 0.7558128833770752
Epoch 690, training loss: 12.444466590881348 = 0.35229477286338806 + 2.0 * 6.046085834503174
Epoch 690, val loss: 0.7518633008003235
Epoch 700, training loss: 12.424896240234375 = 0.34005817770957947 + 2.0 * 6.042418956756592
Epoch 700, val loss: 0.7483853101730347
Epoch 710, training loss: 12.411659240722656 = 0.32827872037887573 + 2.0 * 6.041690349578857
Epoch 710, val loss: 0.7453629374504089
Epoch 720, training loss: 12.398317337036133 = 0.3169160783290863 + 2.0 * 6.040700435638428
Epoch 720, val loss: 0.7427558898925781
Epoch 730, training loss: 12.389996528625488 = 0.3059700131416321 + 2.0 * 6.042013168334961
Epoch 730, val loss: 0.7405073046684265
Epoch 740, training loss: 12.377985000610352 = 0.2954753339290619 + 2.0 * 6.041254997253418
Epoch 740, val loss: 0.7387145161628723
Epoch 750, training loss: 12.362812042236328 = 0.28539153933525085 + 2.0 * 6.038710117340088
Epoch 750, val loss: 0.7372580766677856
Epoch 760, training loss: 12.354846000671387 = 0.27563583850860596 + 2.0 * 6.039605140686035
Epoch 760, val loss: 0.7360749244689941
Epoch 770, training loss: 12.343591690063477 = 0.26618149876594543 + 2.0 * 6.038704872131348
Epoch 770, val loss: 0.7351230978965759
Epoch 780, training loss: 12.331582069396973 = 0.2570141553878784 + 2.0 * 6.037283897399902
Epoch 780, val loss: 0.7344380617141724
Epoch 790, training loss: 12.32086181640625 = 0.24812598526477814 + 2.0 * 6.036367893218994
Epoch 790, val loss: 0.7339754104614258
Epoch 800, training loss: 12.30711841583252 = 0.23944410681724548 + 2.0 * 6.03383731842041
Epoch 800, val loss: 0.7336562871932983
Epoch 810, training loss: 12.297905921936035 = 0.23096317052841187 + 2.0 * 6.033471584320068
Epoch 810, val loss: 0.7335187196731567
Epoch 820, training loss: 12.299744606018066 = 0.22265979647636414 + 2.0 * 6.0385422706604
Epoch 820, val loss: 0.7335067987442017
Epoch 830, training loss: 12.278780937194824 = 0.21457169950008392 + 2.0 * 6.0321044921875
Epoch 830, val loss: 0.7336487770080566
Epoch 840, training loss: 12.285194396972656 = 0.20663250982761383 + 2.0 * 6.039280891418457
Epoch 840, val loss: 0.733814537525177
Epoch 850, training loss: 12.262933731079102 = 0.1989285796880722 + 2.0 * 6.0320024490356445
Epoch 850, val loss: 0.7340461611747742
Epoch 860, training loss: 12.250947952270508 = 0.19137227535247803 + 2.0 * 6.029788017272949
Epoch 860, val loss: 0.7344112396240234
Epoch 870, training loss: 12.242430686950684 = 0.18392357230186462 + 2.0 * 6.0292534828186035
Epoch 870, val loss: 0.734845757484436
Epoch 880, training loss: 12.233651161193848 = 0.17659012973308563 + 2.0 * 6.028530597686768
Epoch 880, val loss: 0.7353754639625549
Epoch 890, training loss: 12.235877990722656 = 0.1694081425666809 + 2.0 * 6.0332350730896
Epoch 890, val loss: 0.7359766960144043
Epoch 900, training loss: 12.22004222869873 = 0.16241177916526794 + 2.0 * 6.028815269470215
Epoch 900, val loss: 0.7365999817848206
Epoch 910, training loss: 12.210917472839355 = 0.15558329224586487 + 2.0 * 6.027667045593262
Epoch 910, val loss: 0.7372615337371826
Epoch 920, training loss: 12.20295238494873 = 0.14893066883087158 + 2.0 * 6.027010917663574
Epoch 920, val loss: 0.7380234599113464
Epoch 930, training loss: 12.202194213867188 = 0.14246705174446106 + 2.0 * 6.029863357543945
Epoch 930, val loss: 0.7389063835144043
Epoch 940, training loss: 12.190556526184082 = 0.1362011730670929 + 2.0 * 6.027177810668945
Epoch 940, val loss: 0.7398199439048767
Epoch 950, training loss: 12.181905746459961 = 0.1302022635936737 + 2.0 * 6.025851726531982
Epoch 950, val loss: 0.7408031225204468
Epoch 960, training loss: 12.177249908447266 = 0.12439105659723282 + 2.0 * 6.026429653167725
Epoch 960, val loss: 0.741965115070343
Epoch 970, training loss: 12.166821479797363 = 0.11884007602930069 + 2.0 * 6.023990631103516
Epoch 970, val loss: 0.7432264685630798
Epoch 980, training loss: 12.15968132019043 = 0.11351306736469269 + 2.0 * 6.0230841636657715
Epoch 980, val loss: 0.7446196675300598
Epoch 990, training loss: 12.15407943725586 = 0.10841736197471619 + 2.0 * 6.022830963134766
Epoch 990, val loss: 0.7461704015731812
Epoch 1000, training loss: 12.164997100830078 = 0.10356028378009796 + 2.0 * 6.0307183265686035
Epoch 1000, val loss: 0.7479115128517151
Epoch 1010, training loss: 12.148195266723633 = 0.09899385273456573 + 2.0 * 6.024600505828857
Epoch 1010, val loss: 0.7497228384017944
Epoch 1020, training loss: 12.135919570922852 = 0.09465941041707993 + 2.0 * 6.0206298828125
Epoch 1020, val loss: 0.7515780329704285
Epoch 1030, training loss: 12.131930351257324 = 0.09053315967321396 + 2.0 * 6.020698547363281
Epoch 1030, val loss: 0.7536540031433105
Epoch 1040, training loss: 12.129772186279297 = 0.08660408109426498 + 2.0 * 6.0215840339660645
Epoch 1040, val loss: 0.7558950781822205
Epoch 1050, training loss: 12.1234130859375 = 0.0828922688961029 + 2.0 * 6.020260334014893
Epoch 1050, val loss: 0.7582945823669434
Epoch 1060, training loss: 12.12717342376709 = 0.07939270883798599 + 2.0 * 6.023890495300293
Epoch 1060, val loss: 0.7607714533805847
Epoch 1070, training loss: 12.118870735168457 = 0.07611490041017532 + 2.0 * 6.021378040313721
Epoch 1070, val loss: 0.7633742094039917
Epoch 1080, training loss: 12.109807968139648 = 0.0730004832148552 + 2.0 * 6.01840353012085
Epoch 1080, val loss: 0.7660666704177856
Epoch 1090, training loss: 12.105365753173828 = 0.07006049156188965 + 2.0 * 6.01765251159668
Epoch 1090, val loss: 0.7689100503921509
Epoch 1100, training loss: 12.10627555847168 = 0.06726592779159546 + 2.0 * 6.019505023956299
Epoch 1100, val loss: 0.7718701362609863
Epoch 1110, training loss: 12.099095344543457 = 0.06462589651346207 + 2.0 * 6.017234802246094
Epoch 1110, val loss: 0.7748796343803406
Epoch 1120, training loss: 12.104683876037598 = 0.06213241443037987 + 2.0 * 6.021275520324707
Epoch 1120, val loss: 0.777933657169342
Epoch 1130, training loss: 12.09359359741211 = 0.05978764593601227 + 2.0 * 6.016902923583984
Epoch 1130, val loss: 0.7810696959495544
Epoch 1140, training loss: 12.08975601196289 = 0.05755993723869324 + 2.0 * 6.0160980224609375
Epoch 1140, val loss: 0.7842138409614563
Epoch 1150, training loss: 12.093474388122559 = 0.05545211583375931 + 2.0 * 6.0190110206604
Epoch 1150, val loss: 0.7874717116355896
Epoch 1160, training loss: 12.085659980773926 = 0.0534525103867054 + 2.0 * 6.016103744506836
Epoch 1160, val loss: 0.7907063961029053
Epoch 1170, training loss: 12.082996368408203 = 0.051566075533628464 + 2.0 * 6.0157151222229
Epoch 1170, val loss: 0.794022798538208
Epoch 1180, training loss: 12.083452224731445 = 0.04977177828550339 + 2.0 * 6.01684045791626
Epoch 1180, val loss: 0.7973341941833496
Epoch 1190, training loss: 12.0754976272583 = 0.04807429760694504 + 2.0 * 6.013711452484131
Epoch 1190, val loss: 0.8007009625434875
Epoch 1200, training loss: 12.071688652038574 = 0.04645910859107971 + 2.0 * 6.012614727020264
Epoch 1200, val loss: 0.8040690422058105
Epoch 1210, training loss: 12.075555801391602 = 0.04491044580936432 + 2.0 * 6.015322685241699
Epoch 1210, val loss: 0.8075157999992371
Epoch 1220, training loss: 12.068743705749512 = 0.043448444455862045 + 2.0 * 6.01264762878418
Epoch 1220, val loss: 0.8109727501869202
Epoch 1230, training loss: 12.066669464111328 = 0.04205890744924545 + 2.0 * 6.01230525970459
Epoch 1230, val loss: 0.8143578171730042
Epoch 1240, training loss: 12.065197944641113 = 0.040734000504016876 + 2.0 * 6.012231826782227
Epoch 1240, val loss: 0.817773699760437
Epoch 1250, training loss: 12.065567016601562 = 0.03946628049015999 + 2.0 * 6.013050556182861
Epoch 1250, val loss: 0.8212211728096008
Epoch 1260, training loss: 12.061515808105469 = 0.038262829184532166 + 2.0 * 6.011626720428467
Epoch 1260, val loss: 0.8246489763259888
Epoch 1270, training loss: 12.064027786254883 = 0.037114668637514114 + 2.0 * 6.013456344604492
Epoch 1270, val loss: 0.82806795835495
Epoch 1280, training loss: 12.059802055358887 = 0.03601762652397156 + 2.0 * 6.011892318725586
Epoch 1280, val loss: 0.8314729332923889
Epoch 1290, training loss: 12.054880142211914 = 0.03496963530778885 + 2.0 * 6.009955406188965
Epoch 1290, val loss: 0.8349058032035828
Epoch 1300, training loss: 12.054227828979492 = 0.033965058624744415 + 2.0 * 6.010131359100342
Epoch 1300, val loss: 0.8383440971374512
Epoch 1310, training loss: 12.055517196655273 = 0.033000554889440536 + 2.0 * 6.011258125305176
Epoch 1310, val loss: 0.8417806029319763
Epoch 1320, training loss: 12.051286697387695 = 0.03208157420158386 + 2.0 * 6.0096025466918945
Epoch 1320, val loss: 0.8451647162437439
Epoch 1330, training loss: 12.048917770385742 = 0.031204471364617348 + 2.0 * 6.008856773376465
Epoch 1330, val loss: 0.8485518097877502
Epoch 1340, training loss: 12.05655288696289 = 0.030354775488376617 + 2.0 * 6.013099193572998
Epoch 1340, val loss: 0.8519085645675659
Epoch 1350, training loss: 12.050521850585938 = 0.02955581620335579 + 2.0 * 6.0104827880859375
Epoch 1350, val loss: 0.8552999496459961
Epoch 1360, training loss: 12.044111251831055 = 0.02878342755138874 + 2.0 * 6.007663726806641
Epoch 1360, val loss: 0.8585379719734192
Epoch 1370, training loss: 12.041131019592285 = 0.028044970706105232 + 2.0 * 6.006543159484863
Epoch 1370, val loss: 0.8618595004081726
Epoch 1380, training loss: 12.042685508728027 = 0.027328219264745712 + 2.0 * 6.007678508758545
Epoch 1380, val loss: 0.8651845455169678
Epoch 1390, training loss: 12.043468475341797 = 0.026639424264431 + 2.0 * 6.0084147453308105
Epoch 1390, val loss: 0.8685092329978943
Epoch 1400, training loss: 12.043850898742676 = 0.02597580850124359 + 2.0 * 6.008937358856201
Epoch 1400, val loss: 0.8717745542526245
Epoch 1410, training loss: 12.04609489440918 = 0.02534433640539646 + 2.0 * 6.010375499725342
Epoch 1410, val loss: 0.8750508427619934
Epoch 1420, training loss: 12.036700248718262 = 0.024733005091547966 + 2.0 * 6.005983829498291
Epoch 1420, val loss: 0.8782147765159607
Epoch 1430, training loss: 12.035201072692871 = 0.024149946868419647 + 2.0 * 6.005525588989258
Epoch 1430, val loss: 0.881438672542572
Epoch 1440, training loss: 12.04251766204834 = 0.02358005754649639 + 2.0 * 6.009469032287598
Epoch 1440, val loss: 0.8846237063407898
Epoch 1450, training loss: 12.032139778137207 = 0.02303537353873253 + 2.0 * 6.004552364349365
Epoch 1450, val loss: 0.887806236743927
Epoch 1460, training loss: 12.02979850769043 = 0.022507475689053535 + 2.0 * 6.003645420074463
Epoch 1460, val loss: 0.890945315361023
Epoch 1470, training loss: 12.035515785217285 = 0.021994533017277718 + 2.0 * 6.006760597229004
Epoch 1470, val loss: 0.8940879106521606
Epoch 1480, training loss: 12.028103828430176 = 0.021503547206521034 + 2.0 * 6.003300189971924
Epoch 1480, val loss: 0.897240161895752
Epoch 1490, training loss: 12.02920913696289 = 0.021031701937317848 + 2.0 * 6.004088878631592
Epoch 1490, val loss: 0.900302529335022
Epoch 1500, training loss: 12.031335830688477 = 0.020572829991579056 + 2.0 * 6.0053815841674805
Epoch 1500, val loss: 0.9033365249633789
Epoch 1510, training loss: 12.028300285339355 = 0.020131930708885193 + 2.0 * 6.00408411026001
Epoch 1510, val loss: 0.9063623547554016
Epoch 1520, training loss: 12.02847671508789 = 0.01970386691391468 + 2.0 * 6.0043864250183105
Epoch 1520, val loss: 0.9094107151031494
Epoch 1530, training loss: 12.023138046264648 = 0.019292229786515236 + 2.0 * 6.001923084259033
Epoch 1530, val loss: 0.9124318957328796
Epoch 1540, training loss: 12.028589248657227 = 0.018891790881752968 + 2.0 * 6.004848957061768
Epoch 1540, val loss: 0.9153800010681152
Epoch 1550, training loss: 12.02214527130127 = 0.018504077568650246 + 2.0 * 6.0018205642700195
Epoch 1550, val loss: 0.918380856513977
Epoch 1560, training loss: 12.020453453063965 = 0.01812811568379402 + 2.0 * 6.001162528991699
Epoch 1560, val loss: 0.9212921261787415
Epoch 1570, training loss: 12.021479606628418 = 0.017765378579497337 + 2.0 * 6.001857280731201
Epoch 1570, val loss: 0.9242663383483887
Epoch 1580, training loss: 12.022863388061523 = 0.017411969602108 + 2.0 * 6.002725601196289
Epoch 1580, val loss: 0.9272169470787048
Epoch 1590, training loss: 12.025390625 = 0.017069445922970772 + 2.0 * 6.004160404205322
Epoch 1590, val loss: 0.930088996887207
Epoch 1600, training loss: 12.018749237060547 = 0.016741452738642693 + 2.0 * 6.001003742218018
Epoch 1600, val loss: 0.9329321980476379
Epoch 1610, training loss: 12.016338348388672 = 0.016425611451268196 + 2.0 * 5.999956130981445
Epoch 1610, val loss: 0.9357309937477112
Epoch 1620, training loss: 12.024367332458496 = 0.01611584611237049 + 2.0 * 6.004125595092773
Epoch 1620, val loss: 0.9385221600532532
Epoch 1630, training loss: 12.014986991882324 = 0.01581846922636032 + 2.0 * 5.999584197998047
Epoch 1630, val loss: 0.9413973689079285
Epoch 1640, training loss: 12.012203216552734 = 0.015526610426604748 + 2.0 * 5.998338222503662
Epoch 1640, val loss: 0.9441697001457214
Epoch 1650, training loss: 12.013050079345703 = 0.015241116285324097 + 2.0 * 5.998904705047607
Epoch 1650, val loss: 0.9469675421714783
Epoch 1660, training loss: 12.019431114196777 = 0.014963686466217041 + 2.0 * 6.002233505249023
Epoch 1660, val loss: 0.9497795701026917
Epoch 1670, training loss: 12.023215293884277 = 0.014694531448185444 + 2.0 * 6.004260540008545
Epoch 1670, val loss: 0.9524728059768677
Epoch 1680, training loss: 12.01276969909668 = 0.01443881168961525 + 2.0 * 5.9991655349731445
Epoch 1680, val loss: 0.9551854133605957
Epoch 1690, training loss: 12.009478569030762 = 0.01418987289071083 + 2.0 * 5.997644424438477
Epoch 1690, val loss: 0.9577714800834656
Epoch 1700, training loss: 12.014501571655273 = 0.013947385363280773 + 2.0 * 6.000277042388916
Epoch 1700, val loss: 0.9604608416557312
Epoch 1710, training loss: 12.006804466247559 = 0.013707577250897884 + 2.0 * 5.996548652648926
Epoch 1710, val loss: 0.9631451368331909
Epoch 1720, training loss: 12.007797241210938 = 0.013475569896399975 + 2.0 * 5.997160911560059
Epoch 1720, val loss: 0.9657901525497437
Epoch 1730, training loss: 12.006295204162598 = 0.013248440809547901 + 2.0 * 5.996523380279541
Epoch 1730, val loss: 0.9684665203094482
Epoch 1740, training loss: 12.02061939239502 = 0.013027490116655827 + 2.0 * 6.003796100616455
Epoch 1740, val loss: 0.9711586236953735
Epoch 1750, training loss: 12.011357307434082 = 0.012813274748623371 + 2.0 * 5.999271869659424
Epoch 1750, val loss: 0.9737017750740051
Epoch 1760, training loss: 12.005966186523438 = 0.012606735341250896 + 2.0 * 5.996679782867432
Epoch 1760, val loss: 0.9761626124382019
Epoch 1770, training loss: 12.00331974029541 = 0.012403905391693115 + 2.0 * 5.995458126068115
Epoch 1770, val loss: 0.97872394323349
Epoch 1780, training loss: 12.008277893066406 = 0.012203402817249298 + 2.0 * 5.998037338256836
Epoch 1780, val loss: 0.9813287854194641
Epoch 1790, training loss: 12.004076957702637 = 0.012010179460048676 + 2.0 * 5.996033191680908
Epoch 1790, val loss: 0.9839147329330444
Epoch 1800, training loss: 12.003437995910645 = 0.011824025772511959 + 2.0 * 5.99580717086792
Epoch 1800, val loss: 0.9863084554672241
Epoch 1810, training loss: 12.004940032958984 = 0.01164265163242817 + 2.0 * 5.996648788452148
Epoch 1810, val loss: 0.9887195825576782
Epoch 1820, training loss: 12.00084400177002 = 0.01146311592310667 + 2.0 * 5.994690418243408
Epoch 1820, val loss: 0.9912211298942566
Epoch 1830, training loss: 12.003874778747559 = 0.011288131587207317 + 2.0 * 5.996293544769287
Epoch 1830, val loss: 0.9937155246734619
Epoch 1840, training loss: 12.00004768371582 = 0.011115810833871365 + 2.0 * 5.9944658279418945
Epoch 1840, val loss: 0.9961400032043457
Epoch 1850, training loss: 12.00656795501709 = 0.010948730632662773 + 2.0 * 5.997809410095215
Epoch 1850, val loss: 0.9985277056694031
Epoch 1860, training loss: 12.00152587890625 = 0.010789791122078896 + 2.0 * 5.995368003845215
Epoch 1860, val loss: 1.000967025756836
Epoch 1870, training loss: 11.999277114868164 = 0.010632135905325413 + 2.0 * 5.994322299957275
Epoch 1870, val loss: 1.003268837928772
Epoch 1880, training loss: 11.996529579162598 = 0.01047808863222599 + 2.0 * 5.993025779724121
Epoch 1880, val loss: 1.005630612373352
Epoch 1890, training loss: 11.996179580688477 = 0.010325430892407894 + 2.0 * 5.992927074432373
Epoch 1890, val loss: 1.0080349445343018
Epoch 1900, training loss: 12.00656795501709 = 0.010175957344472408 + 2.0 * 5.998196125030518
Epoch 1900, val loss: 1.0104498863220215
Epoch 1910, training loss: 11.997918128967285 = 0.010031669400632381 + 2.0 * 5.993943214416504
Epoch 1910, val loss: 1.0127742290496826
Epoch 1920, training loss: 12.000077247619629 = 0.009891572408378124 + 2.0 * 5.995092868804932
Epoch 1920, val loss: 1.015064001083374
Epoch 1930, training loss: 11.995863914489746 = 0.009755035862326622 + 2.0 * 5.993054389953613
Epoch 1930, val loss: 1.0173355340957642
Epoch 1940, training loss: 11.994818687438965 = 0.009620895609259605 + 2.0 * 5.992599010467529
Epoch 1940, val loss: 1.019628882408142
Epoch 1950, training loss: 11.99776840209961 = 0.009488595649600029 + 2.0 * 5.994139671325684
Epoch 1950, val loss: 1.0219075679779053
Epoch 1960, training loss: 11.995715141296387 = 0.00935922097414732 + 2.0 * 5.993177890777588
Epoch 1960, val loss: 1.0241514444351196
Epoch 1970, training loss: 11.990556716918945 = 0.009235109202563763 + 2.0 * 5.990660667419434
Epoch 1970, val loss: 1.0263850688934326
Epoch 1980, training loss: 11.992935180664062 = 0.009111952036619186 + 2.0 * 5.9919114112854
Epoch 1980, val loss: 1.0286598205566406
Epoch 1990, training loss: 12.001029968261719 = 0.008990391157567501 + 2.0 * 5.9960198402404785
Epoch 1990, val loss: 1.0309360027313232
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8111
Overall ASR: 0.5424
Flip ASR: 0.4578/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.695133209228516 = 1.9476112127304077 + 2.0 * 8.373761177062988
Epoch 0, val loss: 1.9455091953277588
Epoch 10, training loss: 18.682270050048828 = 1.9366432428359985 + 2.0 * 8.37281322479248
Epoch 10, val loss: 1.9344923496246338
Epoch 20, training loss: 18.657835006713867 = 1.9231359958648682 + 2.0 * 8.367349624633789
Epoch 20, val loss: 1.920333743095398
Epoch 30, training loss: 18.579011917114258 = 1.9053497314453125 + 2.0 * 8.336831092834473
Epoch 30, val loss: 1.9013603925704956
Epoch 40, training loss: 18.18514060974121 = 1.8859291076660156 + 2.0 * 8.149605751037598
Epoch 40, val loss: 1.8813508749008179
Epoch 50, training loss: 16.801538467407227 = 1.8652344942092896 + 2.0 * 7.468152046203613
Epoch 50, val loss: 1.8599720001220703
Epoch 60, training loss: 15.924839973449707 = 1.849779486656189 + 2.0 * 7.037530422210693
Epoch 60, val loss: 1.845063328742981
Epoch 70, training loss: 15.371269226074219 = 1.8370851278305054 + 2.0 * 6.767092227935791
Epoch 70, val loss: 1.8321776390075684
Epoch 80, training loss: 15.017120361328125 = 1.8274750709533691 + 2.0 * 6.594822883605957
Epoch 80, val loss: 1.8224351406097412
Epoch 90, training loss: 14.784371376037598 = 1.8182920217514038 + 2.0 * 6.483039855957031
Epoch 90, val loss: 1.8126616477966309
Epoch 100, training loss: 14.626447677612305 = 1.8094521760940552 + 2.0 * 6.4084978103637695
Epoch 100, val loss: 1.8030989170074463
Epoch 110, training loss: 14.530839920043945 = 1.8004409074783325 + 2.0 * 6.365199565887451
Epoch 110, val loss: 1.7935009002685547
Epoch 120, training loss: 14.457613945007324 = 1.7913267612457275 + 2.0 * 6.333143711090088
Epoch 120, val loss: 1.7839921712875366
Epoch 130, training loss: 14.39118766784668 = 1.7825819253921509 + 2.0 * 6.30430269241333
Epoch 130, val loss: 1.7747955322265625
Epoch 140, training loss: 14.32983684539795 = 1.7741098403930664 + 2.0 * 6.277863502502441
Epoch 140, val loss: 1.7660624980926514
Epoch 150, training loss: 14.27985954284668 = 1.7654962539672852 + 2.0 * 6.257181644439697
Epoch 150, val loss: 1.7573491334915161
Epoch 160, training loss: 14.2286958694458 = 1.756221890449524 + 2.0 * 6.236237049102783
Epoch 160, val loss: 1.7485665082931519
Epoch 170, training loss: 14.183547973632812 = 1.7460684776306152 + 2.0 * 6.2187395095825195
Epoch 170, val loss: 1.739346981048584
Epoch 180, training loss: 14.14185619354248 = 1.7344729900360107 + 2.0 * 6.203691482543945
Epoch 180, val loss: 1.729336142539978
Epoch 190, training loss: 14.104321479797363 = 1.7212247848510742 + 2.0 * 6.1915483474731445
Epoch 190, val loss: 1.7183692455291748
Epoch 200, training loss: 14.065640449523926 = 1.7063051462173462 + 2.0 * 6.1796674728393555
Epoch 200, val loss: 1.706435203552246
Epoch 210, training loss: 14.027291297912598 = 1.6890641450881958 + 2.0 * 6.169113636016846
Epoch 210, val loss: 1.6928895711898804
Epoch 220, training loss: 13.98879337310791 = 1.668701171875 + 2.0 * 6.160046100616455
Epoch 220, val loss: 1.6770082712173462
Epoch 230, training loss: 13.954753875732422 = 1.6450902223587036 + 2.0 * 6.154831886291504
Epoch 230, val loss: 1.6586397886276245
Epoch 240, training loss: 13.90804672241211 = 1.6175637245178223 + 2.0 * 6.145241737365723
Epoch 240, val loss: 1.6375876665115356
Epoch 250, training loss: 13.863265991210938 = 1.5854439735412598 + 2.0 * 6.138911247253418
Epoch 250, val loss: 1.6128201484680176
Epoch 260, training loss: 13.816224098205566 = 1.5480529069900513 + 2.0 * 6.134085655212402
Epoch 260, val loss: 1.5837314128875732
Epoch 270, training loss: 13.768850326538086 = 1.505717396736145 + 2.0 * 6.131566524505615
Epoch 270, val loss: 1.5512357950210571
Epoch 280, training loss: 13.709617614746094 = 1.4595376253128052 + 2.0 * 6.125040054321289
Epoch 280, val loss: 1.5152181386947632
Epoch 290, training loss: 13.65101146697998 = 1.4095748662948608 + 2.0 * 6.120718479156494
Epoch 290, val loss: 1.4763151407241821
Epoch 300, training loss: 13.590267181396484 = 1.3568700551986694 + 2.0 * 6.116698741912842
Epoch 300, val loss: 1.4353830814361572
Epoch 310, training loss: 13.532054901123047 = 1.3034693002700806 + 2.0 * 6.114292621612549
Epoch 310, val loss: 1.394309401512146
Epoch 320, training loss: 13.474088668823242 = 1.2519512176513672 + 2.0 * 6.1110687255859375
Epoch 320, val loss: 1.354732871055603
Epoch 330, training loss: 13.416775703430176 = 1.2019699811935425 + 2.0 * 6.107402801513672
Epoch 330, val loss: 1.3166084289550781
Epoch 340, training loss: 13.360614776611328 = 1.153363585472107 + 2.0 * 6.103625774383545
Epoch 340, val loss: 1.2799257040023804
Epoch 350, training loss: 13.314449310302734 = 1.106825351715088 + 2.0 * 6.103811740875244
Epoch 350, val loss: 1.2448331117630005
Epoch 360, training loss: 13.26169490814209 = 1.0626589059829712 + 2.0 * 6.099517822265625
Epoch 360, val loss: 1.21212899684906
Epoch 370, training loss: 13.210002899169922 = 1.0199990272521973 + 2.0 * 6.095001697540283
Epoch 370, val loss: 1.1809111833572388
Epoch 380, training loss: 13.16612434387207 = 0.9789316654205322 + 2.0 * 6.093596458435059
Epoch 380, val loss: 1.1509110927581787
Epoch 390, training loss: 13.121628761291504 = 0.9398722052574158 + 2.0 * 6.090878486633301
Epoch 390, val loss: 1.122660517692566
Epoch 400, training loss: 13.076353073120117 = 0.9025331735610962 + 2.0 * 6.086909770965576
Epoch 400, val loss: 1.0960536003112793
Epoch 410, training loss: 13.041728019714355 = 0.8671672940254211 + 2.0 * 6.0872802734375
Epoch 410, val loss: 1.071143388748169
Epoch 420, training loss: 12.998677253723145 = 0.8343164324760437 + 2.0 * 6.082180500030518
Epoch 420, val loss: 1.0483633279800415
Epoch 430, training loss: 12.963042259216309 = 0.8033800721168518 + 2.0 * 6.079831123352051
Epoch 430, val loss: 1.0274083614349365
Epoch 440, training loss: 12.930460929870605 = 0.774046003818512 + 2.0 * 6.078207492828369
Epoch 440, val loss: 1.007966160774231
Epoch 450, training loss: 12.909972190856934 = 0.7465857863426208 + 2.0 * 6.081693172454834
Epoch 450, val loss: 0.9899988174438477
Epoch 460, training loss: 12.87083625793457 = 0.7208178639411926 + 2.0 * 6.075009346008301
Epoch 460, val loss: 0.9740058183670044
Epoch 470, training loss: 12.839781761169434 = 0.6962186098098755 + 2.0 * 6.071781635284424
Epoch 470, val loss: 0.9590187072753906
Epoch 480, training loss: 12.833144187927246 = 0.6725425720214844 + 2.0 * 6.080300807952881
Epoch 480, val loss: 0.9447780847549438
Epoch 490, training loss: 12.790267944335938 = 0.6499204635620117 + 2.0 * 6.070173740386963
Epoch 490, val loss: 0.931751012802124
Epoch 500, training loss: 12.761919975280762 = 0.6281202435493469 + 2.0 * 6.06689977645874
Epoch 500, val loss: 0.9195314049720764
Epoch 510, training loss: 12.735419273376465 = 0.6068712472915649 + 2.0 * 6.064273834228516
Epoch 510, val loss: 0.9077482223510742
Epoch 520, training loss: 12.739017486572266 = 0.5862603187561035 + 2.0 * 6.07637882232666
Epoch 520, val loss: 0.896574854850769
Epoch 530, training loss: 12.692960739135742 = 0.5665147304534912 + 2.0 * 6.063222885131836
Epoch 530, val loss: 0.8864272236824036
Epoch 540, training loss: 12.666400909423828 = 0.5475149154663086 + 2.0 * 6.05944299697876
Epoch 540, val loss: 0.8770313262939453
Epoch 550, training loss: 12.645155906677246 = 0.5290166139602661 + 2.0 * 6.058069705963135
Epoch 550, val loss: 0.8682484030723572
Epoch 560, training loss: 12.630284309387207 = 0.511040449142456 + 2.0 * 6.059621810913086
Epoch 560, val loss: 0.8601946234703064
Epoch 570, training loss: 12.617469787597656 = 0.49376899003982544 + 2.0 * 6.061850547790527
Epoch 570, val loss: 0.852818489074707
Epoch 580, training loss: 12.588034629821777 = 0.4772035479545593 + 2.0 * 6.055415630340576
Epoch 580, val loss: 0.8463492393493652
Epoch 590, training loss: 12.569454193115234 = 0.4611476957798004 + 2.0 * 6.0541534423828125
Epoch 590, val loss: 0.8405898213386536
Epoch 600, training loss: 12.550402641296387 = 0.44563233852386475 + 2.0 * 6.052385330200195
Epoch 600, val loss: 0.8355687856674194
Epoch 610, training loss: 12.540389060974121 = 0.4306444227695465 + 2.0 * 6.054872512817383
Epoch 610, val loss: 0.8313526511192322
Epoch 620, training loss: 12.514257431030273 = 0.4162299335002899 + 2.0 * 6.049013614654541
Epoch 620, val loss: 0.8279275298118591
Epoch 630, training loss: 12.498661994934082 = 0.4022340178489685 + 2.0 * 6.048213958740234
Epoch 630, val loss: 0.8251693248748779
Epoch 640, training loss: 12.497655868530273 = 0.3886849880218506 + 2.0 * 6.054485321044922
Epoch 640, val loss: 0.8230676054954529
Epoch 650, training loss: 12.472956657409668 = 0.3758171796798706 + 2.0 * 6.048569679260254
Epoch 650, val loss: 0.8217445611953735
Epoch 660, training loss: 12.454679489135742 = 0.363294392824173 + 2.0 * 6.045692443847656
Epoch 660, val loss: 0.8210081458091736
Epoch 670, training loss: 12.43832015991211 = 0.3510596454143524 + 2.0 * 6.043630123138428
Epoch 670, val loss: 0.8207598924636841
Epoch 680, training loss: 12.425992965698242 = 0.3391076922416687 + 2.0 * 6.043442726135254
Epoch 680, val loss: 0.8210609555244446
Epoch 690, training loss: 12.412108421325684 = 0.3274727761745453 + 2.0 * 6.042317867279053
Epoch 690, val loss: 0.8217755556106567
Epoch 700, training loss: 12.400494575500488 = 0.31624770164489746 + 2.0 * 6.042123317718506
Epoch 700, val loss: 0.8230000734329224
Epoch 710, training loss: 12.39870548248291 = 0.3052912950515747 + 2.0 * 6.0467071533203125
Epoch 710, val loss: 0.8245721459388733
Epoch 720, training loss: 12.375932693481445 = 0.29469001293182373 + 2.0 * 6.040621280670166
Epoch 720, val loss: 0.826341450214386
Epoch 730, training loss: 12.365203857421875 = 0.28443795442581177 + 2.0 * 6.0403828620910645
Epoch 730, val loss: 0.8284049034118652
Epoch 740, training loss: 12.349641799926758 = 0.27451807260513306 + 2.0 * 6.037561893463135
Epoch 740, val loss: 0.8306164741516113
Epoch 750, training loss: 12.339420318603516 = 0.26498594880104065 + 2.0 * 6.037217140197754
Epoch 750, val loss: 0.8330181837081909
Epoch 760, training loss: 12.338809967041016 = 0.2558072805404663 + 2.0 * 6.041501522064209
Epoch 760, val loss: 0.8355375528335571
Epoch 770, training loss: 12.31810188293457 = 0.24702313542366028 + 2.0 * 6.035539150238037
Epoch 770, val loss: 0.8382088541984558
Epoch 780, training loss: 12.308327674865723 = 0.23855425417423248 + 2.0 * 6.034886837005615
Epoch 780, val loss: 0.8410508036613464
Epoch 790, training loss: 12.316994667053223 = 0.23039449751377106 + 2.0 * 6.043300151824951
Epoch 790, val loss: 0.8439707159996033
Epoch 800, training loss: 12.292288780212402 = 0.22255200147628784 + 2.0 * 6.034868240356445
Epoch 800, val loss: 0.8470239043235779
Epoch 810, training loss: 12.279553413391113 = 0.21504585444927216 + 2.0 * 6.032253742218018
Epoch 810, val loss: 0.8503382802009583
Epoch 820, training loss: 12.27121639251709 = 0.2078060358762741 + 2.0 * 6.031705379486084
Epoch 820, val loss: 0.8537172079086304
Epoch 830, training loss: 12.28886604309082 = 0.20080681145191193 + 2.0 * 6.044029712677002
Epoch 830, val loss: 0.8571376204490662
Epoch 840, training loss: 12.253917694091797 = 0.19420833885669708 + 2.0 * 6.029854774475098
Epoch 840, val loss: 0.8607223629951477
Epoch 850, training loss: 12.248286247253418 = 0.18786121904850006 + 2.0 * 6.03021240234375
Epoch 850, val loss: 0.8644640445709229
Epoch 860, training loss: 12.238007545471191 = 0.18171755969524384 + 2.0 * 6.028144836425781
Epoch 860, val loss: 0.8682370781898499
Epoch 870, training loss: 12.236995697021484 = 0.17579147219657898 + 2.0 * 6.030601978302002
Epoch 870, val loss: 0.8721562027931213
Epoch 880, training loss: 12.226993560791016 = 0.1701212078332901 + 2.0 * 6.028436183929443
Epoch 880, val loss: 0.8761782050132751
Epoch 890, training loss: 12.240478515625 = 0.16469824314117432 + 2.0 * 6.0378899574279785
Epoch 890, val loss: 0.8802039623260498
Epoch 900, training loss: 12.217938423156738 = 0.15956100821495056 + 2.0 * 6.029188632965088
Epoch 900, val loss: 0.8843430876731873
Epoch 910, training loss: 12.206066131591797 = 0.15461622178554535 + 2.0 * 6.0257248878479
Epoch 910, val loss: 0.8885926604270935
Epoch 920, training loss: 12.199603080749512 = 0.14984789490699768 + 2.0 * 6.024877548217773
Epoch 920, val loss: 0.8928795456886292
Epoch 930, training loss: 12.210807800292969 = 0.14528146386146545 + 2.0 * 6.0327630043029785
Epoch 930, val loss: 0.8972040414810181
Epoch 940, training loss: 12.190875053405762 = 0.1408771425485611 + 2.0 * 6.024999141693115
Epoch 940, val loss: 0.9016121029853821
Epoch 950, training loss: 12.182571411132812 = 0.13667890429496765 + 2.0 * 6.022946357727051
Epoch 950, val loss: 0.9061816334724426
Epoch 960, training loss: 12.179762840270996 = 0.13261783123016357 + 2.0 * 6.0235724449157715
Epoch 960, val loss: 0.910740315914154
Epoch 970, training loss: 12.177287101745605 = 0.12872318923473358 + 2.0 * 6.024281978607178
Epoch 970, val loss: 0.915295422077179
Epoch 980, training loss: 12.175135612487793 = 0.12500859797000885 + 2.0 * 6.025063514709473
Epoch 980, val loss: 0.9199565052986145
Epoch 990, training loss: 12.16614055633545 = 0.12144090235233307 + 2.0 * 6.022349834442139
Epoch 990, val loss: 0.9246037006378174
Epoch 1000, training loss: 12.158699035644531 = 0.11799760162830353 + 2.0 * 6.020350933074951
Epoch 1000, val loss: 0.9293049573898315
Epoch 1010, training loss: 12.156638145446777 = 0.11467006802558899 + 2.0 * 6.020984172821045
Epoch 1010, val loss: 0.9340986609458923
Epoch 1020, training loss: 12.15618896484375 = 0.11147043853998184 + 2.0 * 6.022359371185303
Epoch 1020, val loss: 0.9388290643692017
Epoch 1030, training loss: 12.156785011291504 = 0.10841337591409683 + 2.0 * 6.024185657501221
Epoch 1030, val loss: 0.9436125755310059
Epoch 1040, training loss: 12.146541595458984 = 0.10550250858068466 + 2.0 * 6.020519733428955
Epoch 1040, val loss: 0.9484931230545044
Epoch 1050, training loss: 12.138916015625 = 0.10265935957431793 + 2.0 * 6.018128395080566
Epoch 1050, val loss: 0.9533017873764038
Epoch 1060, training loss: 12.135066032409668 = 0.09991094470024109 + 2.0 * 6.017577648162842
Epoch 1060, val loss: 0.9581572413444519
Epoch 1070, training loss: 12.149568557739258 = 0.09725853055715561 + 2.0 * 6.0261549949646
Epoch 1070, val loss: 0.9629701972007751
Epoch 1080, training loss: 12.128571510314941 = 0.09470735490322113 + 2.0 * 6.016932010650635
Epoch 1080, val loss: 0.9677837491035461
Epoch 1090, training loss: 12.125844955444336 = 0.0922531709074974 + 2.0 * 6.016796112060547
Epoch 1090, val loss: 0.9726677536964417
Epoch 1100, training loss: 12.12170124053955 = 0.08987578004598618 + 2.0 * 6.0159125328063965
Epoch 1100, val loss: 0.9775210022926331
Epoch 1110, training loss: 12.152831077575684 = 0.0875789225101471 + 2.0 * 6.032626152038574
Epoch 1110, val loss: 0.9824062585830688
Epoch 1120, training loss: 12.116283416748047 = 0.0853547677397728 + 2.0 * 6.0154643058776855
Epoch 1120, val loss: 0.9870879054069519
Epoch 1130, training loss: 12.113697052001953 = 0.08322502672672272 + 2.0 * 6.015235900878906
Epoch 1130, val loss: 0.9919078946113586
Epoch 1140, training loss: 12.108442306518555 = 0.0811491459608078 + 2.0 * 6.013646602630615
Epoch 1140, val loss: 0.9967089891433716
Epoch 1150, training loss: 12.106489181518555 = 0.07913348078727722 + 2.0 * 6.013678073883057
Epoch 1150, val loss: 1.0016071796417236
Epoch 1160, training loss: 12.119974136352539 = 0.07718703895807266 + 2.0 * 6.021393775939941
Epoch 1160, val loss: 1.00645112991333
Epoch 1170, training loss: 12.104958534240723 = 0.07529155910015106 + 2.0 * 6.014833450317383
Epoch 1170, val loss: 1.0111509561538696
Epoch 1180, training loss: 12.099480628967285 = 0.07346251606941223 + 2.0 * 6.013009071350098
Epoch 1180, val loss: 1.015961766242981
Epoch 1190, training loss: 12.098373413085938 = 0.07168302685022354 + 2.0 * 6.013345241546631
Epoch 1190, val loss: 1.0207653045654297
Epoch 1200, training loss: 12.099859237670898 = 0.06995417177677155 + 2.0 * 6.014952659606934
Epoch 1200, val loss: 1.0255160331726074
Epoch 1210, training loss: 12.098126411437988 = 0.06827395409345627 + 2.0 * 6.014926433563232
Epoch 1210, val loss: 1.0302165746688843
Epoch 1220, training loss: 12.090333938598633 = 0.06663482636213303 + 2.0 * 6.011849403381348
Epoch 1220, val loss: 1.03496253490448
Epoch 1230, training loss: 12.086211204528809 = 0.06503533571958542 + 2.0 * 6.0105881690979
Epoch 1230, val loss: 1.039711833000183
Epoch 1240, training loss: 12.082555770874023 = 0.06346292793750763 + 2.0 * 6.009546279907227
Epoch 1240, val loss: 1.0444425344467163
Epoch 1250, training loss: 12.10289478302002 = 0.06193225458264351 + 2.0 * 6.020481109619141
Epoch 1250, val loss: 1.0491318702697754
Epoch 1260, training loss: 12.091800689697266 = 0.06044836342334747 + 2.0 * 6.015676021575928
Epoch 1260, val loss: 1.0536627769470215
Epoch 1270, training loss: 12.076337814331055 = 0.05902041494846344 + 2.0 * 6.0086588859558105
Epoch 1270, val loss: 1.0583237409591675
Epoch 1280, training loss: 12.07524585723877 = 0.05762345343828201 + 2.0 * 6.008810997009277
Epoch 1280, val loss: 1.0629633665084839
Epoch 1290, training loss: 12.07499885559082 = 0.056253962218761444 + 2.0 * 6.009372234344482
Epoch 1290, val loss: 1.067610502243042
Epoch 1300, training loss: 12.080432891845703 = 0.0549207404255867 + 2.0 * 6.012755870819092
Epoch 1300, val loss: 1.0721769332885742
Epoch 1310, training loss: 12.072113037109375 = 0.05361207574605942 + 2.0 * 6.009250640869141
Epoch 1310, val loss: 1.0766699314117432
Epoch 1320, training loss: 12.069889068603516 = 0.052349839359521866 + 2.0 * 6.008769512176514
Epoch 1320, val loss: 1.0812804698944092
Epoch 1330, training loss: 12.070879936218262 = 0.05110343173146248 + 2.0 * 6.009888172149658
Epoch 1330, val loss: 1.0858094692230225
Epoch 1340, training loss: 12.066604614257812 = 0.04988916590809822 + 2.0 * 6.008357524871826
Epoch 1340, val loss: 1.090445876121521
Epoch 1350, training loss: 12.064422607421875 = 0.048689745366573334 + 2.0 * 6.007866382598877
Epoch 1350, val loss: 1.0949128866195679
Epoch 1360, training loss: 12.060419082641602 = 0.04752356931567192 + 2.0 * 6.006447792053223
Epoch 1360, val loss: 1.0994446277618408
Epoch 1370, training loss: 12.063021659851074 = 0.046371687203645706 + 2.0 * 6.008325099945068
Epoch 1370, val loss: 1.1039859056472778
Epoch 1380, training loss: 12.058735847473145 = 0.04524343088269234 + 2.0 * 6.006746292114258
Epoch 1380, val loss: 1.108407974243164
Epoch 1390, training loss: 12.05639362335205 = 0.04415002092719078 + 2.0 * 6.006121635437012
Epoch 1390, val loss: 1.1129142045974731
Epoch 1400, training loss: 12.054821968078613 = 0.04307619109749794 + 2.0 * 6.00587272644043
Epoch 1400, val loss: 1.1173975467681885
Epoch 1410, training loss: 12.05075740814209 = 0.0420229472219944 + 2.0 * 6.004367351531982
Epoch 1410, val loss: 1.1218255758285522
Epoch 1420, training loss: 12.059870719909668 = 0.04099380224943161 + 2.0 * 6.009438514709473
Epoch 1420, val loss: 1.1263165473937988
Epoch 1430, training loss: 12.048893928527832 = 0.03999374061822891 + 2.0 * 6.00445032119751
Epoch 1430, val loss: 1.1306771039962769
Epoch 1440, training loss: 12.049528121948242 = 0.03901560977101326 + 2.0 * 6.005256175994873
Epoch 1440, val loss: 1.1351367235183716
Epoch 1450, training loss: 12.049345016479492 = 0.03806471824645996 + 2.0 * 6.005640029907227
Epoch 1450, val loss: 1.1394274234771729
Epoch 1460, training loss: 12.043818473815918 = 0.037133507430553436 + 2.0 * 6.003342628479004
Epoch 1460, val loss: 1.143730878829956
Epoch 1470, training loss: 12.041173934936523 = 0.03623354434967041 + 2.0 * 6.002470016479492
Epoch 1470, val loss: 1.1481237411499023
Epoch 1480, training loss: 12.05639362335205 = 0.0353485569357872 + 2.0 * 6.010522365570068
Epoch 1480, val loss: 1.152462363243103
Epoch 1490, training loss: 12.043521881103516 = 0.0344960018992424 + 2.0 * 6.004512786865234
Epoch 1490, val loss: 1.1566202640533447
Epoch 1500, training loss: 12.039196014404297 = 0.033669110387563705 + 2.0 * 6.002763271331787
Epoch 1500, val loss: 1.1609383821487427
Epoch 1510, training loss: 12.043535232543945 = 0.032865822315216064 + 2.0 * 6.005334854125977
Epoch 1510, val loss: 1.1652361154556274
Epoch 1520, training loss: 12.033082008361816 = 0.032081104815006256 + 2.0 * 6.000500679016113
Epoch 1520, val loss: 1.1693580150604248
Epoch 1530, training loss: 12.032733917236328 = 0.03132278844714165 + 2.0 * 6.000705718994141
Epoch 1530, val loss: 1.173634648323059
Epoch 1540, training loss: 12.031477928161621 = 0.030585890635848045 + 2.0 * 6.00044584274292
Epoch 1540, val loss: 1.1779507398605347
Epoch 1550, training loss: 12.046878814697266 = 0.029866158962249756 + 2.0 * 6.0085062980651855
Epoch 1550, val loss: 1.1820929050445557
Epoch 1560, training loss: 12.034577369689941 = 0.029186151921749115 + 2.0 * 6.002695560455322
Epoch 1560, val loss: 1.1862105131149292
Epoch 1570, training loss: 12.028786659240723 = 0.028523581102490425 + 2.0 * 6.000131607055664
Epoch 1570, val loss: 1.1902941465377808
Epoch 1580, training loss: 12.025518417358398 = 0.027879303321242332 + 2.0 * 5.998819351196289
Epoch 1580, val loss: 1.1944859027862549
Epoch 1590, training loss: 12.024556159973145 = 0.02725345455110073 + 2.0 * 5.998651504516602
Epoch 1590, val loss: 1.1986521482467651
Epoch 1600, training loss: 12.040480613708496 = 0.026641933247447014 + 2.0 * 6.0069193840026855
Epoch 1600, val loss: 1.2027276754379272
Epoch 1610, training loss: 12.032768249511719 = 0.026058947667479515 + 2.0 * 6.003354549407959
Epoch 1610, val loss: 1.2067822217941284
Epoch 1620, training loss: 12.024614334106445 = 0.02549460157752037 + 2.0 * 5.9995598793029785
Epoch 1620, val loss: 1.2107641696929932
Epoch 1630, training loss: 12.020406723022461 = 0.02494777739048004 + 2.0 * 5.997729301452637
Epoch 1630, val loss: 1.214871883392334
Epoch 1640, training loss: 12.03892993927002 = 0.024415871128439903 + 2.0 * 6.007256984710693
Epoch 1640, val loss: 1.2188862562179565
Epoch 1650, training loss: 12.025548934936523 = 0.02390342578291893 + 2.0 * 6.0008225440979
Epoch 1650, val loss: 1.222758412361145
Epoch 1660, training loss: 12.020864486694336 = 0.023406624794006348 + 2.0 * 5.9987287521362305
Epoch 1660, val loss: 1.226757287979126
Epoch 1670, training loss: 12.027997970581055 = 0.02292092703282833 + 2.0 * 6.002538681030273
Epoch 1670, val loss: 1.230692982673645
Epoch 1680, training loss: 12.01948356628418 = 0.0224579069763422 + 2.0 * 5.9985127449035645
Epoch 1680, val loss: 1.2346574068069458
Epoch 1690, training loss: 12.017098426818848 = 0.022004161030054092 + 2.0 * 5.997547149658203
Epoch 1690, val loss: 1.23857581615448
Epoch 1700, training loss: 12.014789581298828 = 0.021561292931437492 + 2.0 * 5.9966139793396
Epoch 1700, val loss: 1.2424647808074951
Epoch 1710, training loss: 12.014198303222656 = 0.0211349967867136 + 2.0 * 5.9965314865112305
Epoch 1710, val loss: 1.2464196681976318
Epoch 1720, training loss: 12.01983642578125 = 0.020718244835734367 + 2.0 * 5.999558925628662
Epoch 1720, val loss: 1.2502843141555786
Epoch 1730, training loss: 12.018118858337402 = 0.020313097164034843 + 2.0 * 5.998902797698975
Epoch 1730, val loss: 1.2540467977523804
Epoch 1740, training loss: 12.012877464294434 = 0.01992395706474781 + 2.0 * 5.996476650238037
Epoch 1740, val loss: 1.2578654289245605
Epoch 1750, training loss: 12.009537696838379 = 0.019546477124094963 + 2.0 * 5.994995594024658
Epoch 1750, val loss: 1.2616826295852661
Epoch 1760, training loss: 12.01125717163086 = 0.01917838677763939 + 2.0 * 5.996039390563965
Epoch 1760, val loss: 1.2655072212219238
Epoch 1770, training loss: 12.01816177368164 = 0.018822992220520973 + 2.0 * 5.999669551849365
Epoch 1770, val loss: 1.269265055656433
Epoch 1780, training loss: 12.010438919067383 = 0.018471868708729744 + 2.0 * 5.995983600616455
Epoch 1780, val loss: 1.272881031036377
Epoch 1790, training loss: 12.01215648651123 = 0.01813833974301815 + 2.0 * 5.99700927734375
Epoch 1790, val loss: 1.2766541242599487
Epoch 1800, training loss: 12.009056091308594 = 0.01781350187957287 + 2.0 * 5.995621204376221
Epoch 1800, val loss: 1.2803455591201782
Epoch 1810, training loss: 12.005853652954102 = 0.01749742589890957 + 2.0 * 5.994178295135498
Epoch 1810, val loss: 1.2839816808700562
Epoch 1820, training loss: 12.004149436950684 = 0.01718808151781559 + 2.0 * 5.993480682373047
Epoch 1820, val loss: 1.2876379489898682
Epoch 1830, training loss: 12.013317108154297 = 0.016886992380023003 + 2.0 * 5.998215198516846
Epoch 1830, val loss: 1.2913267612457275
Epoch 1840, training loss: 12.007233619689941 = 0.016598472371697426 + 2.0 * 5.995317459106445
Epoch 1840, val loss: 1.2948949337005615
Epoch 1850, training loss: 12.00188159942627 = 0.016315434128046036 + 2.0 * 5.992783069610596
Epoch 1850, val loss: 1.2984223365783691
Epoch 1860, training loss: 12.006881713867188 = 0.016040582209825516 + 2.0 * 5.995420455932617
Epoch 1860, val loss: 1.3019894361495972
Epoch 1870, training loss: 12.000921249389648 = 0.015773478895425797 + 2.0 * 5.9925737380981445
Epoch 1870, val loss: 1.305538535118103
Epoch 1880, training loss: 12.000533103942871 = 0.015513058751821518 + 2.0 * 5.992509841918945
Epoch 1880, val loss: 1.3090649843215942
Epoch 1890, training loss: 12.001835823059082 = 0.015258685685694218 + 2.0 * 5.993288516998291
Epoch 1890, val loss: 1.3126317262649536
Epoch 1900, training loss: 12.00302505493164 = 0.015008306130766869 + 2.0 * 5.994008541107178
Epoch 1900, val loss: 1.316033124923706
Epoch 1910, training loss: 12.001303672790527 = 0.014765440486371517 + 2.0 * 5.993268966674805
Epoch 1910, val loss: 1.3194329738616943
Epoch 1920, training loss: 12.005638122558594 = 0.014532019384205341 + 2.0 * 5.995553016662598
Epoch 1920, val loss: 1.3228131532669067
Epoch 1930, training loss: 11.998628616333008 = 0.014306548982858658 + 2.0 * 5.992160797119141
Epoch 1930, val loss: 1.3262447118759155
Epoch 1940, training loss: 11.995929718017578 = 0.014081991277635098 + 2.0 * 5.990923881530762
Epoch 1940, val loss: 1.3296284675598145
Epoch 1950, training loss: 12.005181312561035 = 0.013864174485206604 + 2.0 * 5.9956583976745605
Epoch 1950, val loss: 1.3330198526382446
Epoch 1960, training loss: 11.9932861328125 = 0.013649120926856995 + 2.0 * 5.989818572998047
Epoch 1960, val loss: 1.3362547159194946
Epoch 1970, training loss: 11.9918212890625 = 0.013442243449389935 + 2.0 * 5.989189624786377
Epoch 1970, val loss: 1.3396117687225342
Epoch 1980, training loss: 11.993012428283691 = 0.013238072395324707 + 2.0 * 5.989887237548828
Epoch 1980, val loss: 1.3429763317108154
Epoch 1990, training loss: 12.012519836425781 = 0.01303806807845831 + 2.0 * 5.999741077423096
Epoch 1990, val loss: 1.3461633920669556
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7667
Overall ASR: 0.4354
Flip ASR: 0.3511/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.681364059448242 = 1.9336742162704468 + 2.0 * 8.373845100402832
Epoch 0, val loss: 1.9369902610778809
Epoch 10, training loss: 18.670398712158203 = 1.924010157585144 + 2.0 * 8.373194694519043
Epoch 10, val loss: 1.9278753995895386
Epoch 20, training loss: 18.649158477783203 = 1.9108304977416992 + 2.0 * 8.369163513183594
Epoch 20, val loss: 1.9152323007583618
Epoch 30, training loss: 18.57438850402832 = 1.8922666311264038 + 2.0 * 8.341060638427734
Epoch 30, val loss: 1.8974157571792603
Epoch 40, training loss: 18.14442253112793 = 1.8706085681915283 + 2.0 * 8.136906623840332
Epoch 40, val loss: 1.8772270679473877
Epoch 50, training loss: 16.642169952392578 = 1.847758173942566 + 2.0 * 7.397205829620361
Epoch 50, val loss: 1.8559175729751587
Epoch 60, training loss: 16.00049591064453 = 1.8332059383392334 + 2.0 * 7.083644866943359
Epoch 60, val loss: 1.8432302474975586
Epoch 70, training loss: 15.547338485717773 = 1.823635220527649 + 2.0 * 6.861851692199707
Epoch 70, val loss: 1.8337026834487915
Epoch 80, training loss: 15.246757507324219 = 1.8123606443405151 + 2.0 * 6.717198371887207
Epoch 80, val loss: 1.8223345279693604
Epoch 90, training loss: 14.967550277709961 = 1.8020752668380737 + 2.0 * 6.582737445831299
Epoch 90, val loss: 1.8119456768035889
Epoch 100, training loss: 14.779778480529785 = 1.7946168184280396 + 2.0 * 6.492580890655518
Epoch 100, val loss: 1.8038969039916992
Epoch 110, training loss: 14.6231107711792 = 1.7877460718154907 + 2.0 * 6.41768217086792
Epoch 110, val loss: 1.7964012622833252
Epoch 120, training loss: 14.516311645507812 = 1.7802479267120361 + 2.0 * 6.368031978607178
Epoch 120, val loss: 1.788638949394226
Epoch 130, training loss: 14.44037914276123 = 1.7720472812652588 + 2.0 * 6.334166049957275
Epoch 130, val loss: 1.7804547548294067
Epoch 140, training loss: 14.368624687194824 = 1.7632310390472412 + 2.0 * 6.302696704864502
Epoch 140, val loss: 1.771885633468628
Epoch 150, training loss: 14.304810523986816 = 1.7537702322006226 + 2.0 * 6.275520324707031
Epoch 150, val loss: 1.7631584405899048
Epoch 160, training loss: 14.247873306274414 = 1.743226170539856 + 2.0 * 6.252323627471924
Epoch 160, val loss: 1.7540401220321655
Epoch 170, training loss: 14.197482109069824 = 1.7311264276504517 + 2.0 * 6.233177661895752
Epoch 170, val loss: 1.743996500968933
Epoch 180, training loss: 14.153081893920898 = 1.7170357704162598 + 2.0 * 6.21802282333374
Epoch 180, val loss: 1.7326000928878784
Epoch 190, training loss: 14.10448169708252 = 1.7007521390914917 + 2.0 * 6.201864719390869
Epoch 190, val loss: 1.7195876836776733
Epoch 200, training loss: 14.058741569519043 = 1.6818515062332153 + 2.0 * 6.188445091247559
Epoch 200, val loss: 1.7045737504959106
Epoch 210, training loss: 14.014118194580078 = 1.6598533391952515 + 2.0 * 6.177132606506348
Epoch 210, val loss: 1.687190055847168
Epoch 220, training loss: 13.968690872192383 = 1.6346646547317505 + 2.0 * 6.167013168334961
Epoch 220, val loss: 1.667270541191101
Epoch 230, training loss: 13.927966117858887 = 1.6060549020767212 + 2.0 * 6.160955429077148
Epoch 230, val loss: 1.644551396369934
Epoch 240, training loss: 13.874896049499512 = 1.5742266178131104 + 2.0 * 6.15033483505249
Epoch 240, val loss: 1.6192500591278076
Epoch 250, training loss: 13.827302932739258 = 1.5392348766326904 + 2.0 * 6.144033908843994
Epoch 250, val loss: 1.5912028551101685
Epoch 260, training loss: 13.777304649353027 = 1.5014911890029907 + 2.0 * 6.137906551361084
Epoch 260, val loss: 1.5608068704605103
Epoch 270, training loss: 13.736305236816406 = 1.4621615409851074 + 2.0 * 6.1370720863342285
Epoch 270, val loss: 1.5291082859039307
Epoch 280, training loss: 13.681907653808594 = 1.4230085611343384 + 2.0 * 6.129449367523193
Epoch 280, val loss: 1.497376799583435
Epoch 290, training loss: 13.631394386291504 = 1.383974313735962 + 2.0 * 6.1237101554870605
Epoch 290, val loss: 1.4658290147781372
Epoch 300, training loss: 13.583829879760742 = 1.3455923795700073 + 2.0 * 6.119118690490723
Epoch 300, val loss: 1.4349688291549683
Epoch 310, training loss: 13.538291931152344 = 1.3081332445144653 + 2.0 * 6.115079402923584
Epoch 310, val loss: 1.4050939083099365
Epoch 320, training loss: 13.496587753295898 = 1.2721383571624756 + 2.0 * 6.112224578857422
Epoch 320, val loss: 1.3768503665924072
Epoch 330, training loss: 13.456856727600098 = 1.2377642393112183 + 2.0 * 6.109546184539795
Epoch 330, val loss: 1.3501116037368774
Epoch 340, training loss: 13.41412353515625 = 1.204350233078003 + 2.0 * 6.104886531829834
Epoch 340, val loss: 1.3246047496795654
Epoch 350, training loss: 13.373199462890625 = 1.1715906858444214 + 2.0 * 6.100804328918457
Epoch 350, val loss: 1.2996761798858643
Epoch 360, training loss: 13.343582153320312 = 1.1388472318649292 + 2.0 * 6.102367401123047
Epoch 360, val loss: 1.275274395942688
Epoch 370, training loss: 13.295539855957031 = 1.1063188314437866 + 2.0 * 6.094610691070557
Epoch 370, val loss: 1.2509586811065674
Epoch 380, training loss: 13.256603240966797 = 1.073609471321106 + 2.0 * 6.09149694442749
Epoch 380, val loss: 1.2266993522644043
Epoch 390, training loss: 13.21904468536377 = 1.0404698848724365 + 2.0 * 6.089287281036377
Epoch 390, val loss: 1.2020773887634277
Epoch 400, training loss: 13.18523120880127 = 1.0071197748184204 + 2.0 * 6.08905553817749
Epoch 400, val loss: 1.1773688793182373
Epoch 410, training loss: 13.145076751708984 = 0.9740976095199585 + 2.0 * 6.085489749908447
Epoch 410, val loss: 1.1528034210205078
Epoch 420, training loss: 13.106729507446289 = 0.9411023259162903 + 2.0 * 6.082813739776611
Epoch 420, val loss: 1.1283448934555054
Epoch 430, training loss: 13.07345962524414 = 0.9082040786743164 + 2.0 * 6.082627773284912
Epoch 430, val loss: 1.103868842124939
Epoch 440, training loss: 13.03518295288086 = 0.8758761286735535 + 2.0 * 6.079653263092041
Epoch 440, val loss: 1.0798578262329102
Epoch 450, training loss: 12.997808456420898 = 0.8438133001327515 + 2.0 * 6.076997756958008
Epoch 450, val loss: 1.0561442375183105
Epoch 460, training loss: 12.970137596130371 = 0.8120048642158508 + 2.0 * 6.079066276550293
Epoch 460, val loss: 1.032672643661499
Epoch 470, training loss: 12.928359985351562 = 0.7807829976081848 + 2.0 * 6.073788642883301
Epoch 470, val loss: 1.0098642110824585
Epoch 480, training loss: 12.893472671508789 = 0.7499964833259583 + 2.0 * 6.071738243103027
Epoch 480, val loss: 0.9874987602233887
Epoch 490, training loss: 12.877768516540527 = 0.7197702527046204 + 2.0 * 6.078999042510986
Epoch 490, val loss: 0.9657517075538635
Epoch 500, training loss: 12.829520225524902 = 0.6908241510391235 + 2.0 * 6.069347858428955
Epoch 500, val loss: 0.9450380206108093
Epoch 510, training loss: 12.797101020812988 = 0.6628912091255188 + 2.0 * 6.067104816436768
Epoch 510, val loss: 0.9255332946777344
Epoch 520, training loss: 12.77454948425293 = 0.6359764933586121 + 2.0 * 6.069286346435547
Epoch 520, val loss: 0.9069836735725403
Epoch 530, training loss: 12.738533020019531 = 0.6101813316345215 + 2.0 * 6.064176082611084
Epoch 530, val loss: 0.8897301554679871
Epoch 540, training loss: 12.713058471679688 = 0.5854840874671936 + 2.0 * 6.06378698348999
Epoch 540, val loss: 0.8737577795982361
Epoch 550, training loss: 12.692112922668457 = 0.5619821548461914 + 2.0 * 6.065065383911133
Epoch 550, val loss: 0.8591384887695312
Epoch 560, training loss: 12.66137409210205 = 0.5396887063980103 + 2.0 * 6.060842514038086
Epoch 560, val loss: 0.8461002707481384
Epoch 570, training loss: 12.641708374023438 = 0.5182844996452332 + 2.0 * 6.06171178817749
Epoch 570, val loss: 0.8341590762138367
Epoch 580, training loss: 12.615779876708984 = 0.49791568517684937 + 2.0 * 6.058932304382324
Epoch 580, val loss: 0.8234215974807739
Epoch 590, training loss: 12.590352058410645 = 0.4782998859882355 + 2.0 * 6.056025981903076
Epoch 590, val loss: 0.8139685392379761
Epoch 600, training loss: 12.58681583404541 = 0.45948848128318787 + 2.0 * 6.063663482666016
Epoch 600, val loss: 0.80540531873703
Epoch 610, training loss: 12.548421859741211 = 0.44152435660362244 + 2.0 * 6.053448677062988
Epoch 610, val loss: 0.79799485206604
Epoch 620, training loss: 12.529495239257812 = 0.4243042767047882 + 2.0 * 6.052595615386963
Epoch 620, val loss: 0.7916772961616516
Epoch 630, training loss: 12.510431289672852 = 0.40767475962638855 + 2.0 * 6.05137825012207
Epoch 630, val loss: 0.7860605716705322
Epoch 640, training loss: 12.502885818481445 = 0.39157333970069885 + 2.0 * 6.055656433105469
Epoch 640, val loss: 0.7812275886535645
Epoch 650, training loss: 12.47685432434082 = 0.3761707544326782 + 2.0 * 6.050341606140137
Epoch 650, val loss: 0.7770625352859497
Epoch 660, training loss: 12.461136817932129 = 0.3613961935043335 + 2.0 * 6.049870491027832
Epoch 660, val loss: 0.7739579677581787
Epoch 670, training loss: 12.441704750061035 = 0.3470395803451538 + 2.0 * 6.047332763671875
Epoch 670, val loss: 0.7711819410324097
Epoch 680, training loss: 12.425060272216797 = 0.33302226662635803 + 2.0 * 6.046019077301025
Epoch 680, val loss: 0.7688723206520081
Epoch 690, training loss: 12.432432174682617 = 0.31939202547073364 + 2.0 * 6.056519985198975
Epoch 690, val loss: 0.7668647766113281
Epoch 700, training loss: 12.39844799041748 = 0.3062465786933899 + 2.0 * 6.046100616455078
Epoch 700, val loss: 0.7654231786727905
Epoch 710, training loss: 12.385721206665039 = 0.29346606135368347 + 2.0 * 6.046127796173096
Epoch 710, val loss: 0.7644665241241455
Epoch 720, training loss: 12.367476463317871 = 0.28101181983947754 + 2.0 * 6.043232440948486
Epoch 720, val loss: 0.7636746764183044
Epoch 730, training loss: 12.354016304016113 = 0.2688961923122406 + 2.0 * 6.04256010055542
Epoch 730, val loss: 0.7632886171340942
Epoch 740, training loss: 12.343231201171875 = 0.25712478160858154 + 2.0 * 6.043053150177002
Epoch 740, val loss: 0.7630495429039001
Epoch 750, training loss: 12.325702667236328 = 0.2457127571105957 + 2.0 * 6.039995193481445
Epoch 750, val loss: 0.7632385492324829
Epoch 760, training loss: 12.313899993896484 = 0.23466284573078156 + 2.0 * 6.039618492126465
Epoch 760, val loss: 0.7635699510574341
Epoch 770, training loss: 12.32208251953125 = 0.22403427958488464 + 2.0 * 6.0490241050720215
Epoch 770, val loss: 0.7639452815055847
Epoch 780, training loss: 12.297198295593262 = 0.21391531825065613 + 2.0 * 6.041641712188721
Epoch 780, val loss: 0.764803946018219
Epoch 790, training loss: 12.280438423156738 = 0.2042246013879776 + 2.0 * 6.038106918334961
Epoch 790, val loss: 0.7660309672355652
Epoch 800, training loss: 12.268136978149414 = 0.19492846727371216 + 2.0 * 6.036604404449463
Epoch 800, val loss: 0.7672172784805298
Epoch 810, training loss: 12.274767875671387 = 0.18602624535560608 + 2.0 * 6.044370651245117
Epoch 810, val loss: 0.7687534093856812
Epoch 820, training loss: 12.251116752624512 = 0.17759932577610016 + 2.0 * 6.036758899688721
Epoch 820, val loss: 0.7705010771751404
Epoch 830, training loss: 12.240400314331055 = 0.16959425806999207 + 2.0 * 6.035403251647949
Epoch 830, val loss: 0.7727689146995544
Epoch 840, training loss: 12.229659080505371 = 0.16195425391197205 + 2.0 * 6.033852577209473
Epoch 840, val loss: 0.7750063538551331
Epoch 850, training loss: 12.235989570617676 = 0.15468905866146088 + 2.0 * 6.040650367736816
Epoch 850, val loss: 0.7774299383163452
Epoch 860, training loss: 12.224720001220703 = 0.14782090485095978 + 2.0 * 6.038449764251709
Epoch 860, val loss: 0.7799658179283142
Epoch 870, training loss: 12.20662784576416 = 0.1413622945547104 + 2.0 * 6.032632827758789
Epoch 870, val loss: 0.7830445766448975
Epoch 880, training loss: 12.197799682617188 = 0.1352185755968094 + 2.0 * 6.031290531158447
Epoch 880, val loss: 0.786007821559906
Epoch 890, training loss: 12.194080352783203 = 0.129368394613266 + 2.0 * 6.032355785369873
Epoch 890, val loss: 0.7891563773155212
Epoch 900, training loss: 12.185409545898438 = 0.12380780279636383 + 2.0 * 6.030800819396973
Epoch 900, val loss: 0.7923303842544556
Epoch 910, training loss: 12.182229042053223 = 0.11852806806564331 + 2.0 * 6.031850337982178
Epoch 910, val loss: 0.7957192659378052
Epoch 920, training loss: 12.1759614944458 = 0.11355551332235336 + 2.0 * 6.031202793121338
Epoch 920, val loss: 0.7990347743034363
Epoch 930, training loss: 12.165938377380371 = 0.10885719209909439 + 2.0 * 6.02854061126709
Epoch 930, val loss: 0.8027050495147705
Epoch 940, training loss: 12.160175323486328 = 0.10437332093715668 + 2.0 * 6.0279011726379395
Epoch 940, val loss: 0.8062859177589417
Epoch 950, training loss: 12.154603958129883 = 0.1000991016626358 + 2.0 * 6.027252197265625
Epoch 950, val loss: 0.8098978400230408
Epoch 960, training loss: 12.15708065032959 = 0.09603245556354523 + 2.0 * 6.030524253845215
Epoch 960, val loss: 0.8134674429893494
Epoch 970, training loss: 12.146367073059082 = 0.09218429774045944 + 2.0 * 6.0270915031433105
Epoch 970, val loss: 0.8172372579574585
Epoch 980, training loss: 12.139817237854004 = 0.08852437138557434 + 2.0 * 6.025646209716797
Epoch 980, val loss: 0.8210756182670593
Epoch 990, training loss: 12.14115047454834 = 0.08504024893045425 + 2.0 * 6.028055191040039
Epoch 990, val loss: 0.8248865604400635
Epoch 1000, training loss: 12.134142875671387 = 0.08172324299812317 + 2.0 * 6.026209831237793
Epoch 1000, val loss: 0.8285399675369263
Epoch 1010, training loss: 12.125596046447754 = 0.07857260853052139 + 2.0 * 6.02351188659668
Epoch 1010, val loss: 0.8323988914489746
Epoch 1020, training loss: 12.127498626708984 = 0.07556473463773727 + 2.0 * 6.025967121124268
Epoch 1020, val loss: 0.8361364603042603
Epoch 1030, training loss: 12.117866516113281 = 0.07271704077720642 + 2.0 * 6.0225749015808105
Epoch 1030, val loss: 0.839949905872345
Epoch 1040, training loss: 12.11280632019043 = 0.06998962163925171 + 2.0 * 6.021408557891846
Epoch 1040, val loss: 0.8438979387283325
Epoch 1050, training loss: 12.11566162109375 = 0.06738670915365219 + 2.0 * 6.024137496948242
Epoch 1050, val loss: 0.8476325273513794
Epoch 1060, training loss: 12.113521575927734 = 0.06490731239318848 + 2.0 * 6.0243072509765625
Epoch 1060, val loss: 0.8513187766075134
Epoch 1070, training loss: 12.104117393493652 = 0.06255558133125305 + 2.0 * 6.02078104019165
Epoch 1070, val loss: 0.8553060293197632
Epoch 1080, training loss: 12.100971221923828 = 0.0603056475520134 + 2.0 * 6.0203328132629395
Epoch 1080, val loss: 0.8590435981750488
Epoch 1090, training loss: 12.105655670166016 = 0.058153681457042694 + 2.0 * 6.0237507820129395
Epoch 1090, val loss: 0.8627037405967712
Epoch 1100, training loss: 12.098421096801758 = 0.056133415549993515 + 2.0 * 6.021143913269043
Epoch 1100, val loss: 0.8664579391479492
Epoch 1110, training loss: 12.09087085723877 = 0.05419143661856651 + 2.0 * 6.01833963394165
Epoch 1110, val loss: 0.8704530000686646
Epoch 1120, training loss: 12.086838722229004 = 0.05234057083725929 + 2.0 * 6.01724910736084
Epoch 1120, val loss: 0.8741758465766907
Epoch 1130, training loss: 12.084465980529785 = 0.0505591481924057 + 2.0 * 6.016953468322754
Epoch 1130, val loss: 0.8779787421226501
Epoch 1140, training loss: 12.098315238952637 = 0.048859603703022 + 2.0 * 6.024727821350098
Epoch 1140, val loss: 0.8816490173339844
Epoch 1150, training loss: 12.082341194152832 = 0.04723569005727768 + 2.0 * 6.017552852630615
Epoch 1150, val loss: 0.8853550553321838
Epoch 1160, training loss: 12.076377868652344 = 0.04569243639707565 + 2.0 * 6.015342712402344
Epoch 1160, val loss: 0.8891979455947876
Epoch 1170, training loss: 12.07777214050293 = 0.04420578479766846 + 2.0 * 6.016783237457275
Epoch 1170, val loss: 0.8927869200706482
Epoch 1180, training loss: 12.07354736328125 = 0.04279326647520065 + 2.0 * 6.015377044677734
Epoch 1180, val loss: 0.8961897492408752
Epoch 1190, training loss: 12.072053909301758 = 0.04144923761487007 + 2.0 * 6.0153021812438965
Epoch 1190, val loss: 0.8999550938606262
Epoch 1200, training loss: 12.068670272827148 = 0.040163230150938034 + 2.0 * 6.014253616333008
Epoch 1200, val loss: 0.9036121964454651
Epoch 1210, training loss: 12.07323169708252 = 0.038925763219594955 + 2.0 * 6.017152786254883
Epoch 1210, val loss: 0.9070752263069153
Epoch 1220, training loss: 12.065346717834473 = 0.03774544969201088 + 2.0 * 6.013800621032715
Epoch 1220, val loss: 0.910699725151062
Epoch 1230, training loss: 12.063788414001465 = 0.036610960960388184 + 2.0 * 6.013588905334473
Epoch 1230, val loss: 0.9141635894775391
Epoch 1240, training loss: 12.065505981445312 = 0.035527557134628296 + 2.0 * 6.014989376068115
Epoch 1240, val loss: 0.9174957871437073
Epoch 1250, training loss: 12.056928634643555 = 0.03449895232915878 + 2.0 * 6.011214733123779
Epoch 1250, val loss: 0.921218991279602
Epoch 1260, training loss: 12.054734230041504 = 0.03350535035133362 + 2.0 * 6.010614395141602
Epoch 1260, val loss: 0.9246959686279297
Epoch 1270, training loss: 12.05313491821289 = 0.03254684433341026 + 2.0 * 6.010293960571289
Epoch 1270, val loss: 0.9281692504882812
Epoch 1280, training loss: 12.06683349609375 = 0.03162878751754761 + 2.0 * 6.017602443695068
Epoch 1280, val loss: 0.9315694570541382
Epoch 1290, training loss: 12.059802055358887 = 0.03075101226568222 + 2.0 * 6.014525413513184
Epoch 1290, val loss: 0.9345664978027344
Epoch 1300, training loss: 12.048497200012207 = 0.02991805598139763 + 2.0 * 6.009289741516113
Epoch 1300, val loss: 0.9383091330528259
Epoch 1310, training loss: 12.04677677154541 = 0.02911192923784256 + 2.0 * 6.0088324546813965
Epoch 1310, val loss: 0.9416007995605469
Epoch 1320, training loss: 12.046327590942383 = 0.028331609442830086 + 2.0 * 6.008997917175293
Epoch 1320, val loss: 0.9448480010032654
Epoch 1330, training loss: 12.058514595031738 = 0.027582377195358276 + 2.0 * 6.015466213226318
Epoch 1330, val loss: 0.9477763175964355
Epoch 1340, training loss: 12.041892051696777 = 0.02687157317996025 + 2.0 * 6.007510185241699
Epoch 1340, val loss: 0.9512255787849426
Epoch 1350, training loss: 12.04216194152832 = 0.02618548274040222 + 2.0 * 6.007988452911377
Epoch 1350, val loss: 0.9545990228652954
Epoch 1360, training loss: 12.03962230682373 = 0.025519872084259987 + 2.0 * 6.00705099105835
Epoch 1360, val loss: 0.9577745199203491
Epoch 1370, training loss: 12.054028511047363 = 0.02488008141517639 + 2.0 * 6.01457405090332
Epoch 1370, val loss: 0.9609668850898743
Epoch 1380, training loss: 12.044598579406738 = 0.024260228499770164 + 2.0 * 6.01016902923584
Epoch 1380, val loss: 0.9639238119125366
Epoch 1390, training loss: 12.039911270141602 = 0.023672355338931084 + 2.0 * 6.008119583129883
Epoch 1390, val loss: 0.9672695398330688
Epoch 1400, training loss: 12.042197227478027 = 0.023100297898054123 + 2.0 * 6.009548664093018
Epoch 1400, val loss: 0.9702339172363281
Epoch 1410, training loss: 12.033309936523438 = 0.02254914492368698 + 2.0 * 6.005380630493164
Epoch 1410, val loss: 0.9733244776725769
Epoch 1420, training loss: 12.031930923461914 = 0.022018760442733765 + 2.0 * 6.004956245422363
Epoch 1420, val loss: 0.976493239402771
Epoch 1430, training loss: 12.030633926391602 = 0.021502550691366196 + 2.0 * 6.004565715789795
Epoch 1430, val loss: 0.9795605540275574
Epoch 1440, training loss: 12.043306350708008 = 0.02100287936627865 + 2.0 * 6.0111517906188965
Epoch 1440, val loss: 0.9825116395950317
Epoch 1450, training loss: 12.038895606994629 = 0.020525773987174034 + 2.0 * 6.009184837341309
Epoch 1450, val loss: 0.9853335022926331
Epoch 1460, training loss: 12.030388832092285 = 0.02006753720343113 + 2.0 * 6.005160808563232
Epoch 1460, val loss: 0.9886234998703003
Epoch 1470, training loss: 12.02641773223877 = 0.019622240215539932 + 2.0 * 6.0033979415893555
Epoch 1470, val loss: 0.9915920495986938
Epoch 1480, training loss: 12.031651496887207 = 0.019188450649380684 + 2.0 * 6.006231307983398
Epoch 1480, val loss: 0.9944720268249512
Epoch 1490, training loss: 12.024768829345703 = 0.01877244934439659 + 2.0 * 6.002998352050781
Epoch 1490, val loss: 0.9974158406257629
Epoch 1500, training loss: 12.030414581298828 = 0.018371913582086563 + 2.0 * 6.006021499633789
Epoch 1500, val loss: 1.000470519065857
Epoch 1510, training loss: 12.030417442321777 = 0.01798272132873535 + 2.0 * 6.0062174797058105
Epoch 1510, val loss: 1.0031485557556152
Epoch 1520, training loss: 12.021912574768066 = 0.01760908029973507 + 2.0 * 6.002151966094971
Epoch 1520, val loss: 1.006151556968689
Epoch 1530, training loss: 12.02023983001709 = 0.017244813963770866 + 2.0 * 6.001497745513916
Epoch 1530, val loss: 1.0089977979660034
Epoch 1540, training loss: 12.021909713745117 = 0.016891691833734512 + 2.0 * 6.002509117126465
Epoch 1540, val loss: 1.0118821859359741
Epoch 1550, training loss: 12.027680397033691 = 0.016550453379750252 + 2.0 * 6.005565166473389
Epoch 1550, val loss: 1.0144407749176025
Epoch 1560, training loss: 12.02208137512207 = 0.01621812768280506 + 2.0 * 6.002931594848633
Epoch 1560, val loss: 1.0173234939575195
Epoch 1570, training loss: 12.019767761230469 = 0.01589985564351082 + 2.0 * 6.001934051513672
Epoch 1570, val loss: 1.0200307369232178
Epoch 1580, training loss: 12.017019271850586 = 0.015587261877954006 + 2.0 * 6.000716209411621
Epoch 1580, val loss: 1.0227526426315308
Epoch 1590, training loss: 12.020490646362305 = 0.015285330824553967 + 2.0 * 6.002602577209473
Epoch 1590, val loss: 1.025415062904358
Epoch 1600, training loss: 12.014660835266113 = 0.014991777949035168 + 2.0 * 5.9998345375061035
Epoch 1600, val loss: 1.0281316041946411
Epoch 1610, training loss: 12.01673698425293 = 0.014707324095070362 + 2.0 * 6.001014709472656
Epoch 1610, val loss: 1.030915379524231
Epoch 1620, training loss: 12.018898963928223 = 0.014429901726543903 + 2.0 * 6.00223445892334
Epoch 1620, val loss: 1.0334237813949585
Epoch 1630, training loss: 12.015087127685547 = 0.014162600040435791 + 2.0 * 6.000462055206299
Epoch 1630, val loss: 1.0360379219055176
Epoch 1640, training loss: 12.016090393066406 = 0.013904734514653683 + 2.0 * 6.001092910766602
Epoch 1640, val loss: 1.0387513637542725
Epoch 1650, training loss: 12.012104034423828 = 0.013651072978973389 + 2.0 * 5.9992265701293945
Epoch 1650, val loss: 1.0412360429763794
Epoch 1660, training loss: 12.010255813598633 = 0.013404689729213715 + 2.0 * 5.998425483703613
Epoch 1660, val loss: 1.0438719987869263
Epoch 1670, training loss: 12.014286994934082 = 0.01316430326551199 + 2.0 * 6.000561237335205
Epoch 1670, val loss: 1.0462888479232788
Epoch 1680, training loss: 12.011582374572754 = 0.012932254001498222 + 2.0 * 5.999325275421143
Epoch 1680, val loss: 1.0487618446350098
Epoch 1690, training loss: 12.010961532592773 = 0.0127082085236907 + 2.0 * 5.999126434326172
Epoch 1690, val loss: 1.0513947010040283
Epoch 1700, training loss: 12.012404441833496 = 0.01248868741095066 + 2.0 * 5.999958038330078
Epoch 1700, val loss: 1.053737759590149
Epoch 1710, training loss: 12.007412910461426 = 0.01227491907775402 + 2.0 * 5.9975690841674805
Epoch 1710, val loss: 1.0561392307281494
Epoch 1720, training loss: 12.0049467086792 = 0.012068253941833973 + 2.0 * 5.996439456939697
Epoch 1720, val loss: 1.0587040185928345
Epoch 1730, training loss: 12.008354187011719 = 0.011865914799273014 + 2.0 * 5.998244285583496
Epoch 1730, val loss: 1.0611393451690674
Epoch 1740, training loss: 12.0070219039917 = 0.011667105369269848 + 2.0 * 5.997677326202393
Epoch 1740, val loss: 1.0633282661437988
Epoch 1750, training loss: 12.00592041015625 = 0.01147475466132164 + 2.0 * 5.997222900390625
Epoch 1750, val loss: 1.065858006477356
Epoch 1760, training loss: 12.009756088256836 = 0.011288191191852093 + 2.0 * 5.999233722686768
Epoch 1760, val loss: 1.0681867599487305
Epoch 1770, training loss: 12.006036758422852 = 0.011106850579380989 + 2.0 * 5.997465133666992
Epoch 1770, val loss: 1.0704988241195679
Epoch 1780, training loss: 12.0062837600708 = 0.010930086486041546 + 2.0 * 5.997676849365234
Epoch 1780, val loss: 1.0729045867919922
Epoch 1790, training loss: 12.00553035736084 = 0.010756607167422771 + 2.0 * 5.997386932373047
Epoch 1790, val loss: 1.0751436948776245
Epoch 1800, training loss: 12.005699157714844 = 0.010589421726763248 + 2.0 * 5.997554779052734
Epoch 1800, val loss: 1.0776164531707764
Epoch 1810, training loss: 12.0005464553833 = 0.010424705222249031 + 2.0 * 5.995060920715332
Epoch 1810, val loss: 1.0797802209854126
Epoch 1820, training loss: 11.999588966369629 = 0.010264250449836254 + 2.0 * 5.994662284851074
Epoch 1820, val loss: 1.0821118354797363
Epoch 1830, training loss: 11.998526573181152 = 0.010106900706887245 + 2.0 * 5.9942097663879395
Epoch 1830, val loss: 1.084412932395935
Epoch 1840, training loss: 12.002144813537598 = 0.009952766820788383 + 2.0 * 5.996096134185791
Epoch 1840, val loss: 1.086677074432373
Epoch 1850, training loss: 12.001447677612305 = 0.009801703505218029 + 2.0 * 5.995822906494141
Epoch 1850, val loss: 1.088814377784729
Epoch 1860, training loss: 11.998841285705566 = 0.009655426256358624 + 2.0 * 5.994593143463135
Epoch 1860, val loss: 1.091086983680725
Epoch 1870, training loss: 12.013205528259277 = 0.00951530784368515 + 2.0 * 6.001844882965088
Epoch 1870, val loss: 1.0934665203094482
Epoch 1880, training loss: 12.000203132629395 = 0.009375527501106262 + 2.0 * 5.995413780212402
Epoch 1880, val loss: 1.0950796604156494
Epoch 1890, training loss: 11.995542526245117 = 0.009242914617061615 + 2.0 * 5.993149757385254
Epoch 1890, val loss: 1.0976085662841797
Epoch 1900, training loss: 11.993467330932617 = 0.009109629318118095 + 2.0 * 5.992178916931152
Epoch 1900, val loss: 1.09967839717865
Epoch 1910, training loss: 11.992791175842285 = 0.008978654630482197 + 2.0 * 5.99190616607666
Epoch 1910, val loss: 1.101883053779602
Epoch 1920, training loss: 12.013111114501953 = 0.00885035376995802 + 2.0 * 6.002130508422852
Epoch 1920, val loss: 1.1038146018981934
Epoch 1930, training loss: 12.003837585449219 = 0.008726945146918297 + 2.0 * 5.997555255889893
Epoch 1930, val loss: 1.1057010889053345
Epoch 1940, training loss: 11.993579864501953 = 0.008608450181782246 + 2.0 * 5.992485523223877
Epoch 1940, val loss: 1.1080796718597412
Epoch 1950, training loss: 11.991660118103027 = 0.008490074425935745 + 2.0 * 5.9915852546691895
Epoch 1950, val loss: 1.1101409196853638
Epoch 1960, training loss: 11.996726036071777 = 0.008373014628887177 + 2.0 * 5.994176387786865
Epoch 1960, val loss: 1.112267017364502
Epoch 1970, training loss: 11.990880966186523 = 0.008259065449237823 + 2.0 * 5.991311073303223
Epoch 1970, val loss: 1.1141713857650757
Epoch 1980, training loss: 11.991381645202637 = 0.008148412220180035 + 2.0 * 5.991616725921631
Epoch 1980, val loss: 1.1164371967315674
Epoch 1990, training loss: 11.993334770202637 = 0.008038743399083614 + 2.0 * 5.992648124694824
Epoch 1990, val loss: 1.118402123451233
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8074
Overall ASR: 0.8081
Flip ASR: 0.7689/225 nodes
The final ASR:0.59533, 0.15668, Accuracy:0.79506, 0.02014
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11644])
remove edge: torch.Size([2, 9540])
updated graph: torch.Size([2, 10628])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
The final ASR:0.98032, 0.00174, Accuracy:0.83210, 0.00761
Begin epxeriment: cont_weight: 2 epoch:2000 der1:0.3 der2:0.3 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 18.699691772460938 = 1.9522104263305664 + 2.0 * 8.373740196228027
Epoch 0, val loss: 1.9623935222625732
Epoch 10, training loss: 18.686893463134766 = 1.9415875673294067 + 2.0 * 8.372653007507324
Epoch 10, val loss: 1.951461911201477
Epoch 20, training loss: 18.659332275390625 = 1.9281882047653198 + 2.0 * 8.365571975708008
Epoch 20, val loss: 1.9375479221343994
Epoch 30, training loss: 18.547937393188477 = 1.910480260848999 + 2.0 * 8.31872844696045
Epoch 30, val loss: 1.9193081855773926
Epoch 40, training loss: 17.90351104736328 = 1.8910093307495117 + 2.0 * 8.006251335144043
Epoch 40, val loss: 1.9002301692962646
Epoch 50, training loss: 16.421268463134766 = 1.8724517822265625 + 2.0 * 7.274407863616943
Epoch 50, val loss: 1.8830512762069702
Epoch 60, training loss: 15.613676071166992 = 1.8598895072937012 + 2.0 * 6.876893520355225
Epoch 60, val loss: 1.871001124382019
Epoch 70, training loss: 15.147652626037598 = 1.8489378690719604 + 2.0 * 6.649357318878174
Epoch 70, val loss: 1.8601760864257812
Epoch 80, training loss: 14.883535385131836 = 1.839165210723877 + 2.0 * 6.522185325622559
Epoch 80, val loss: 1.8503538370132446
Epoch 90, training loss: 14.70892333984375 = 1.8294185400009155 + 2.0 * 6.439752578735352
Epoch 90, val loss: 1.8405930995941162
Epoch 100, training loss: 14.568260192871094 = 1.8207855224609375 + 2.0 * 6.373737335205078
Epoch 100, val loss: 1.8318575620651245
Epoch 110, training loss: 14.460636138916016 = 1.813103437423706 + 2.0 * 6.323766231536865
Epoch 110, val loss: 1.8239996433258057
Epoch 120, training loss: 14.378928184509277 = 1.8062180280685425 + 2.0 * 6.286355018615723
Epoch 120, val loss: 1.8167133331298828
Epoch 130, training loss: 14.311699867248535 = 1.7996156215667725 + 2.0 * 6.256042003631592
Epoch 130, val loss: 1.809673547744751
Epoch 140, training loss: 14.261102676391602 = 1.7929458618164062 + 2.0 * 6.234078407287598
Epoch 140, val loss: 1.802707314491272
Epoch 150, training loss: 14.213727951049805 = 1.7859272956848145 + 2.0 * 6.213900566101074
Epoch 150, val loss: 1.7957592010498047
Epoch 160, training loss: 14.171737670898438 = 1.7784554958343506 + 2.0 * 6.196640968322754
Epoch 160, val loss: 1.7886532545089722
Epoch 170, training loss: 14.135062217712402 = 1.7702274322509766 + 2.0 * 6.182417392730713
Epoch 170, val loss: 1.7813150882720947
Epoch 180, training loss: 14.105450630187988 = 1.7610098123550415 + 2.0 * 6.172220230102539
Epoch 180, val loss: 1.7734357118606567
Epoch 190, training loss: 14.070234298706055 = 1.75065279006958 + 2.0 * 6.159790992736816
Epoch 190, val loss: 1.7647801637649536
Epoch 200, training loss: 14.039342880249023 = 1.7388052940368652 + 2.0 * 6.1502685546875
Epoch 200, val loss: 1.7551038265228271
Epoch 210, training loss: 14.012554168701172 = 1.7251543998718262 + 2.0 * 6.143700122833252
Epoch 210, val loss: 1.7441684007644653
Epoch 220, training loss: 13.979696273803711 = 1.7094851732254028 + 2.0 * 6.135105609893799
Epoch 220, val loss: 1.7317214012145996
Epoch 230, training loss: 13.947763442993164 = 1.691258430480957 + 2.0 * 6.1282525062561035
Epoch 230, val loss: 1.7172963619232178
Epoch 240, training loss: 13.919188499450684 = 1.669933557510376 + 2.0 * 6.124627590179443
Epoch 240, val loss: 1.700473666191101
Epoch 250, training loss: 13.879778861999512 = 1.6450144052505493 + 2.0 * 6.117382049560547
Epoch 250, val loss: 1.680700421333313
Epoch 260, training loss: 13.839376449584961 = 1.6159321069717407 + 2.0 * 6.111721992492676
Epoch 260, val loss: 1.657576084136963
Epoch 270, training loss: 13.795700073242188 = 1.5819705724716187 + 2.0 * 6.106864929199219
Epoch 270, val loss: 1.6303805112838745
Epoch 280, training loss: 13.753409385681152 = 1.5434542894363403 + 2.0 * 6.104977607727051
Epoch 280, val loss: 1.599623680114746
Epoch 290, training loss: 13.702167510986328 = 1.5013378858566284 + 2.0 * 6.100414752960205
Epoch 290, val loss: 1.5658409595489502
Epoch 300, training loss: 13.647400856018066 = 1.4554643630981445 + 2.0 * 6.095968246459961
Epoch 300, val loss: 1.52904212474823
Epoch 310, training loss: 13.592074394226074 = 1.4065618515014648 + 2.0 * 6.092756271362305
Epoch 310, val loss: 1.4900259971618652
Epoch 320, training loss: 13.53530502319336 = 1.3557299375534058 + 2.0 * 6.089787483215332
Epoch 320, val loss: 1.4497475624084473
Epoch 330, training loss: 13.482142448425293 = 1.3045419454574585 + 2.0 * 6.088800430297852
Epoch 330, val loss: 1.4098865985870361
Epoch 340, training loss: 13.428339958190918 = 1.2552907466888428 + 2.0 * 6.086524486541748
Epoch 340, val loss: 1.371915578842163
Epoch 350, training loss: 13.372572898864746 = 1.2072347402572632 + 2.0 * 6.082669258117676
Epoch 350, val loss: 1.3356324434280396
Epoch 360, training loss: 13.319212913513184 = 1.1605170965194702 + 2.0 * 6.079348087310791
Epoch 360, val loss: 1.300572156906128
Epoch 370, training loss: 13.283536911010742 = 1.1150891780853271 + 2.0 * 6.084223747253418
Epoch 370, val loss: 1.2671921253204346
Epoch 380, training loss: 13.224647521972656 = 1.0720895528793335 + 2.0 * 6.076279163360596
Epoch 380, val loss: 1.235478162765503
Epoch 390, training loss: 13.175800323486328 = 1.0305839776992798 + 2.0 * 6.07260799407959
Epoch 390, val loss: 1.205112338066101
Epoch 400, training loss: 13.146986961364746 = 0.9902384281158447 + 2.0 * 6.07837438583374
Epoch 400, val loss: 1.17599618434906
Epoch 410, training loss: 13.091573715209961 = 0.9518303871154785 + 2.0 * 6.069871425628662
Epoch 410, val loss: 1.1482031345367432
Epoch 420, training loss: 13.047768592834473 = 0.9146735668182373 + 2.0 * 6.066547393798828
Epoch 420, val loss: 1.1216388940811157
Epoch 430, training loss: 13.00612735748291 = 0.8782966732978821 + 2.0 * 6.063915252685547
Epoch 430, val loss: 1.0956966876983643
Epoch 440, training loss: 12.966653823852539 = 0.8424193263053894 + 2.0 * 6.062117099761963
Epoch 440, val loss: 1.0701749324798584
Epoch 450, training loss: 12.978673934936523 = 0.8071543574333191 + 2.0 * 6.08575963973999
Epoch 450, val loss: 1.0452358722686768
Epoch 460, training loss: 12.896773338317871 = 0.773510217666626 + 2.0 * 6.061631679534912
Epoch 460, val loss: 1.0219106674194336
Epoch 470, training loss: 12.860217094421387 = 0.740859866142273 + 2.0 * 6.059678554534912
Epoch 470, val loss: 0.9993419647216797
Epoch 480, training loss: 12.8225679397583 = 0.7087573409080505 + 2.0 * 6.056905269622803
Epoch 480, val loss: 0.9773719906806946
Epoch 490, training loss: 12.787254333496094 = 0.6775010228157043 + 2.0 * 6.054876804351807
Epoch 490, val loss: 0.9562598466873169
Epoch 500, training loss: 12.755130767822266 = 0.6469802856445312 + 2.0 * 6.054075241088867
Epoch 500, val loss: 0.9360167980194092
Epoch 510, training loss: 12.731889724731445 = 0.61793452501297 + 2.0 * 6.05697774887085
Epoch 510, val loss: 0.9170181155204773
Epoch 520, training loss: 12.696012496948242 = 0.5904545783996582 + 2.0 * 6.052778720855713
Epoch 520, val loss: 0.8998622298240662
Epoch 530, training loss: 12.664490699768066 = 0.5643026828765869 + 2.0 * 6.050094127655029
Epoch 530, val loss: 0.8840712308883667
Epoch 540, training loss: 12.636335372924805 = 0.5394788980484009 + 2.0 * 6.048428058624268
Epoch 540, val loss: 0.8697133660316467
Epoch 550, training loss: 12.61168384552002 = 0.5161280035972595 + 2.0 * 6.047778129577637
Epoch 550, val loss: 0.8568731546401978
Epoch 560, training loss: 12.589558601379395 = 0.49437692761421204 + 2.0 * 6.047590732574463
Epoch 560, val loss: 0.8458753228187561
Epoch 570, training loss: 12.564090728759766 = 0.47383105754852295 + 2.0 * 6.045129776000977
Epoch 570, val loss: 0.8362519145011902
Epoch 580, training loss: 12.540756225585938 = 0.4543105363845825 + 2.0 * 6.043222904205322
Epoch 580, val loss: 0.8276752233505249
Epoch 590, training loss: 12.519704818725586 = 0.43568700551986694 + 2.0 * 6.042008876800537
Epoch 590, val loss: 0.8202016949653625
Epoch 600, training loss: 12.500992774963379 = 0.41784584522247314 + 2.0 * 6.041573524475098
Epoch 600, val loss: 0.8136547803878784
Epoch 610, training loss: 12.494514465332031 = 0.4008897542953491 + 2.0 * 6.046812534332275
Epoch 610, val loss: 0.8079065084457397
Epoch 620, training loss: 12.465410232543945 = 0.3847709596157074 + 2.0 * 6.040319442749023
Epoch 620, val loss: 0.8033310174942017
Epoch 630, training loss: 12.445380210876465 = 0.3692288100719452 + 2.0 * 6.038075923919678
Epoch 630, val loss: 0.799303412437439
Epoch 640, training loss: 12.428752899169922 = 0.3541376292705536 + 2.0 * 6.0373077392578125
Epoch 640, val loss: 0.7958049774169922
Epoch 650, training loss: 12.423883438110352 = 0.33951321244239807 + 2.0 * 6.042185306549072
Epoch 650, val loss: 0.7928791046142578
Epoch 660, training loss: 12.399374008178711 = 0.32540982961654663 + 2.0 * 6.03698205947876
Epoch 660, val loss: 0.7906355261802673
Epoch 670, training loss: 12.38071346282959 = 0.3117150068283081 + 2.0 * 6.034499168395996
Epoch 670, val loss: 0.788974404335022
Epoch 680, training loss: 12.364986419677734 = 0.29833507537841797 + 2.0 * 6.033325672149658
Epoch 680, val loss: 0.787661612033844
Epoch 690, training loss: 12.35010051727295 = 0.28525060415267944 + 2.0 * 6.0324249267578125
Epoch 690, val loss: 0.7868452072143555
Epoch 700, training loss: 12.361661911010742 = 0.27249884605407715 + 2.0 * 6.044581413269043
Epoch 700, val loss: 0.7863840460777283
Epoch 710, training loss: 12.328229904174805 = 0.2603144347667694 + 2.0 * 6.0339579582214355
Epoch 710, val loss: 0.7864487767219543
Epoch 720, training loss: 12.308849334716797 = 0.24848072230815887 + 2.0 * 6.030184268951416
Epoch 720, val loss: 0.7871212363243103
Epoch 730, training loss: 12.296014785766602 = 0.23700116574764252 + 2.0 * 6.029506683349609
Epoch 730, val loss: 0.7879323363304138
Epoch 740, training loss: 12.29076099395752 = 0.22582657635211945 + 2.0 * 6.032467365264893
Epoch 740, val loss: 0.7892341613769531
Epoch 750, training loss: 12.281268119812012 = 0.215131014585495 + 2.0 * 6.033068656921387
Epoch 750, val loss: 0.7908530235290527
Epoch 760, training loss: 12.259392738342285 = 0.20483747124671936 + 2.0 * 6.02727746963501
Epoch 760, val loss: 0.7930353879928589
Epoch 770, training loss: 12.248043060302734 = 0.19492681324481964 + 2.0 * 6.026557922363281
Epoch 770, val loss: 0.7954826354980469
Epoch 780, training loss: 12.239258766174316 = 0.18540088832378387 + 2.0 * 6.026928901672363
Epoch 780, val loss: 0.7982598543167114
Epoch 790, training loss: 12.22938060760498 = 0.17629627883434296 + 2.0 * 6.0265421867370605
Epoch 790, val loss: 0.8012210130691528
Epoch 800, training loss: 12.216747283935547 = 0.16769537329673767 + 2.0 * 6.024526119232178
Epoch 800, val loss: 0.8046624064445496
Epoch 810, training loss: 12.207517623901367 = 0.15948478877544403 + 2.0 * 6.024016380310059
Epoch 810, val loss: 0.8083757758140564
Epoch 820, training loss: 12.19798469543457 = 0.15165649354457855 + 2.0 * 6.02316427230835
Epoch 820, val loss: 0.8124012351036072
Epoch 830, training loss: 12.188517570495605 = 0.14419275522232056 + 2.0 * 6.022162437438965
Epoch 830, val loss: 0.8167508244514465
Epoch 840, training loss: 12.185283660888672 = 0.13710752129554749 + 2.0 * 6.024087905883789
Epoch 840, val loss: 0.8213462233543396
Epoch 850, training loss: 12.180547714233398 = 0.13046278059482574 + 2.0 * 6.025042533874512
Epoch 850, val loss: 0.8258669972419739
Epoch 860, training loss: 12.165722846984863 = 0.12421849370002747 + 2.0 * 6.020751953125
Epoch 860, val loss: 0.8309332728385925
Epoch 870, training loss: 12.158312797546387 = 0.11832017451524734 + 2.0 * 6.019996166229248
Epoch 870, val loss: 0.8360515832901001
Epoch 880, training loss: 12.15675163269043 = 0.1127406507730484 + 2.0 * 6.022005558013916
Epoch 880, val loss: 0.8412806391716003
Epoch 890, training loss: 12.148151397705078 = 0.10749395936727524 + 2.0 * 6.020328521728516
Epoch 890, val loss: 0.8466469645500183
Epoch 900, training loss: 12.148043632507324 = 0.10257553309202194 + 2.0 * 6.02273416519165
Epoch 900, val loss: 0.8522467613220215
Epoch 910, training loss: 12.13420581817627 = 0.09793997555971146 + 2.0 * 6.01813268661499
Epoch 910, val loss: 0.8578047752380371
Epoch 920, training loss: 12.127779006958008 = 0.09356940537691116 + 2.0 * 6.017104625701904
Epoch 920, val loss: 0.863601803779602
Epoch 930, training loss: 12.125944137573242 = 0.0894290953874588 + 2.0 * 6.0182576179504395
Epoch 930, val loss: 0.8695023655891418
Epoch 940, training loss: 12.120948791503906 = 0.08553540706634521 + 2.0 * 6.017706871032715
Epoch 940, val loss: 0.8753765821456909
Epoch 950, training loss: 12.112919807434082 = 0.08188202232122421 + 2.0 * 6.015518665313721
Epoch 950, val loss: 0.8815223574638367
Epoch 960, training loss: 12.108777046203613 = 0.07844176888465881 + 2.0 * 6.015167713165283
Epoch 960, val loss: 0.8875882029533386
Epoch 970, training loss: 12.104762077331543 = 0.0751839280128479 + 2.0 * 6.01478910446167
Epoch 970, val loss: 0.8936798572540283
Epoch 980, training loss: 12.11026382446289 = 0.0721110925078392 + 2.0 * 6.019076347351074
Epoch 980, val loss: 0.8997067213058472
Epoch 990, training loss: 12.106158256530762 = 0.06920748203992844 + 2.0 * 6.018475532531738
Epoch 990, val loss: 0.9057937264442444
Epoch 1000, training loss: 12.096335411071777 = 0.06651051342487335 + 2.0 * 6.0149126052856445
Epoch 1000, val loss: 0.9120248556137085
Epoch 1010, training loss: 12.08907413482666 = 0.06394339352846146 + 2.0 * 6.0125651359558105
Epoch 1010, val loss: 0.9181004166603088
Epoch 1020, training loss: 12.085185050964355 = 0.06151457875967026 + 2.0 * 6.011835098266602
Epoch 1020, val loss: 0.9242464303970337
Epoch 1030, training loss: 12.0833158493042 = 0.0592113621532917 + 2.0 * 6.012052059173584
Epoch 1030, val loss: 0.930346667766571
Epoch 1040, training loss: 12.086769104003906 = 0.05702432617545128 + 2.0 * 6.0148725509643555
Epoch 1040, val loss: 0.9364603161811829
Epoch 1050, training loss: 12.094014167785645 = 0.05497577786445618 + 2.0 * 6.019519329071045
Epoch 1050, val loss: 0.9426526427268982
Epoch 1060, training loss: 12.075674057006836 = 0.053044553846120834 + 2.0 * 6.011314868927002
Epoch 1060, val loss: 0.9484592080116272
Epoch 1070, training loss: 12.071502685546875 = 0.05121759697794914 + 2.0 * 6.0101423263549805
Epoch 1070, val loss: 0.954479992389679
Epoch 1080, training loss: 12.066757202148438 = 0.04947430267930031 + 2.0 * 6.008641242980957
Epoch 1080, val loss: 0.9604137539863586
Epoch 1090, training loss: 12.064661979675293 = 0.04781361296772957 + 2.0 * 6.008424282073975
Epoch 1090, val loss: 0.9663453102111816
Epoch 1100, training loss: 12.082347869873047 = 0.046234406530857086 + 2.0 * 6.018056869506836
Epoch 1100, val loss: 0.9721642136573792
Epoch 1110, training loss: 12.063948631286621 = 0.04473995044827461 + 2.0 * 6.009604454040527
Epoch 1110, val loss: 0.9779658317565918
Epoch 1120, training loss: 12.058951377868652 = 0.04332837089896202 + 2.0 * 6.007811546325684
Epoch 1120, val loss: 0.9837790131568909
Epoch 1130, training loss: 12.068370819091797 = 0.04197297617793083 + 2.0 * 6.0131988525390625
Epoch 1130, val loss: 0.9892973899841309
Epoch 1140, training loss: 12.056999206542969 = 0.0407002791762352 + 2.0 * 6.00814962387085
Epoch 1140, val loss: 0.9949536323547363
Epoch 1150, training loss: 12.052419662475586 = 0.03947795554995537 + 2.0 * 6.006470680236816
Epoch 1150, val loss: 1.0005391836166382
Epoch 1160, training loss: 12.057280540466309 = 0.038314253091812134 + 2.0 * 6.009483337402344
Epoch 1160, val loss: 1.0059820413589478
Epoch 1170, training loss: 12.04755687713623 = 0.03719780966639519 + 2.0 * 6.005179405212402
Epoch 1170, val loss: 1.0112800598144531
Epoch 1180, training loss: 12.047191619873047 = 0.03613746166229248 + 2.0 * 6.005527019500732
Epoch 1180, val loss: 1.0167803764343262
Epoch 1190, training loss: 12.049237251281738 = 0.035121165215969086 + 2.0 * 6.007058143615723
Epoch 1190, val loss: 1.0219371318817139
Epoch 1200, training loss: 12.043503761291504 = 0.03414811193943024 + 2.0 * 6.004677772521973
Epoch 1200, val loss: 1.0271623134613037
Epoch 1210, training loss: 12.039854049682617 = 0.033218733966350555 + 2.0 * 6.003317832946777
Epoch 1210, val loss: 1.0324211120605469
Epoch 1220, training loss: 12.046134948730469 = 0.03232217952609062 + 2.0 * 6.006906509399414
Epoch 1220, val loss: 1.0375587940216064
Epoch 1230, training loss: 12.042919158935547 = 0.03146657720208168 + 2.0 * 6.005726337432861
Epoch 1230, val loss: 1.04245126247406
Epoch 1240, training loss: 12.041081428527832 = 0.03065611608326435 + 2.0 * 6.005212783813477
Epoch 1240, val loss: 1.0476303100585938
Epoch 1250, training loss: 12.038142204284668 = 0.02987401932477951 + 2.0 * 6.004134178161621
Epoch 1250, val loss: 1.0524182319641113
Epoch 1260, training loss: 12.03600025177002 = 0.02912871167063713 + 2.0 * 6.0034356117248535
Epoch 1260, val loss: 1.057323694229126
Epoch 1270, training loss: 12.03078842163086 = 0.028407836332917213 + 2.0 * 6.001190185546875
Epoch 1270, val loss: 1.0622320175170898
Epoch 1280, training loss: 12.03041934967041 = 0.027710651978850365 + 2.0 * 6.001354217529297
Epoch 1280, val loss: 1.0670167207717896
Epoch 1290, training loss: 12.03586483001709 = 0.027039214968681335 + 2.0 * 6.004412651062012
Epoch 1290, val loss: 1.071720004081726
Epoch 1300, training loss: 12.026755332946777 = 0.026398399844765663 + 2.0 * 6.000178337097168
Epoch 1300, val loss: 1.0764960050582886
Epoch 1310, training loss: 12.034211158752441 = 0.02577892504632473 + 2.0 * 6.004216194152832
Epoch 1310, val loss: 1.0810792446136475
Epoch 1320, training loss: 12.029623031616211 = 0.025183577090501785 + 2.0 * 6.0022196769714355
Epoch 1320, val loss: 1.0856261253356934
Epoch 1330, training loss: 12.027585983276367 = 0.024620696902275085 + 2.0 * 6.0014824867248535
Epoch 1330, val loss: 1.0902509689331055
Epoch 1340, training loss: 12.025253295898438 = 0.024073006585240364 + 2.0 * 6.0005903244018555
Epoch 1340, val loss: 1.0946038961410522
Epoch 1350, training loss: 12.02046012878418 = 0.023538747802376747 + 2.0 * 5.99846076965332
Epoch 1350, val loss: 1.0990984439849854
Epoch 1360, training loss: 12.018447875976562 = 0.02302256040275097 + 2.0 * 5.9977126121521
Epoch 1360, val loss: 1.1036056280136108
Epoch 1370, training loss: 12.017600059509277 = 0.02252029813826084 + 2.0 * 5.99753999710083
Epoch 1370, val loss: 1.1080013513565063
Epoch 1380, training loss: 12.050158500671387 = 0.022030964493751526 + 2.0 * 6.014063835144043
Epoch 1380, val loss: 1.1122210025787354
Epoch 1390, training loss: 12.019569396972656 = 0.021575478836894035 + 2.0 * 5.998996734619141
Epoch 1390, val loss: 1.1162381172180176
Epoch 1400, training loss: 12.01823902130127 = 0.02113562822341919 + 2.0 * 5.998551845550537
Epoch 1400, val loss: 1.120774745941162
Epoch 1410, training loss: 12.01350212097168 = 0.020704148337244987 + 2.0 * 5.99639892578125
Epoch 1410, val loss: 1.1247870922088623
Epoch 1420, training loss: 12.025455474853516 = 0.020283769816160202 + 2.0 * 6.0025858879089355
Epoch 1420, val loss: 1.1289021968841553
Epoch 1430, training loss: 12.016276359558105 = 0.01987643726170063 + 2.0 * 5.998199939727783
Epoch 1430, val loss: 1.1327483654022217
Epoch 1440, training loss: 12.013679504394531 = 0.019488928839564323 + 2.0 * 5.997095108032227
Epoch 1440, val loss: 1.1370271444320679
Epoch 1450, training loss: 12.009936332702637 = 0.019106736406683922 + 2.0 * 5.995414733886719
Epoch 1450, val loss: 1.1409039497375488
Epoch 1460, training loss: 12.008837699890137 = 0.01873387023806572 + 2.0 * 5.995051860809326
Epoch 1460, val loss: 1.1449437141418457
Epoch 1470, training loss: 12.03402042388916 = 0.018371300771832466 + 2.0 * 6.007824420928955
Epoch 1470, val loss: 1.148681640625
Epoch 1480, training loss: 12.008747100830078 = 0.01802726462483406 + 2.0 * 5.995359897613525
Epoch 1480, val loss: 1.152591586112976
Epoch 1490, training loss: 12.00757122039795 = 0.01769498735666275 + 2.0 * 5.994937896728516
Epoch 1490, val loss: 1.156706690788269
Epoch 1500, training loss: 12.004972457885742 = 0.01736496388912201 + 2.0 * 5.993803977966309
Epoch 1500, val loss: 1.1604526042938232
Epoch 1510, training loss: 12.003954887390137 = 0.017043717205524445 + 2.0 * 5.993455410003662
Epoch 1510, val loss: 1.164300799369812
Epoch 1520, training loss: 12.015660285949707 = 0.01672997511923313 + 2.0 * 5.999464988708496
Epoch 1520, val loss: 1.168030858039856
Epoch 1530, training loss: 12.00588321685791 = 0.016428668051958084 + 2.0 * 5.99472713470459
Epoch 1530, val loss: 1.171842336654663
Epoch 1540, training loss: 12.008381843566895 = 0.01613926701247692 + 2.0 * 5.996121406555176
Epoch 1540, val loss: 1.1755352020263672
Epoch 1550, training loss: 12.006601333618164 = 0.015856223180890083 + 2.0 * 5.995372772216797
Epoch 1550, val loss: 1.1790624856948853
Epoch 1560, training loss: 12.001324653625488 = 0.015579798258841038 + 2.0 * 5.99287223815918
Epoch 1560, val loss: 1.182779312133789
Epoch 1570, training loss: 11.99940013885498 = 0.015312659554183483 + 2.0 * 5.992043972015381
Epoch 1570, val loss: 1.1863949298858643
Epoch 1580, training loss: 11.998641967773438 = 0.015048903413116932 + 2.0 * 5.991796493530273
Epoch 1580, val loss: 1.1899950504302979
Epoch 1590, training loss: 12.009549140930176 = 0.014793282374739647 + 2.0 * 5.997377872467041
Epoch 1590, val loss: 1.1934605836868286
Epoch 1600, training loss: 12.003863334655762 = 0.014544522389769554 + 2.0 * 5.994659423828125
Epoch 1600, val loss: 1.1969302892684937
Epoch 1610, training loss: 11.9994478225708 = 0.01430701743811369 + 2.0 * 5.992570400238037
Epoch 1610, val loss: 1.2006092071533203
Epoch 1620, training loss: 11.995564460754395 = 0.014073085971176624 + 2.0 * 5.990745544433594
Epoch 1620, val loss: 1.2040385007858276
Epoch 1630, training loss: 11.994192123413086 = 0.013841461390256882 + 2.0 * 5.990175247192383
Epoch 1630, val loss: 1.2074729204177856
Epoch 1640, training loss: 12.002002716064453 = 0.013615110889077187 + 2.0 * 5.994194030761719
Epoch 1640, val loss: 1.210847020149231
Epoch 1650, training loss: 12.008014678955078 = 0.013397462666034698 + 2.0 * 5.997308731079102
Epoch 1650, val loss: 1.2141592502593994
Epoch 1660, training loss: 11.997684478759766 = 0.0131936464458704 + 2.0 * 5.992245197296143
Epoch 1660, val loss: 1.2174594402313232
Epoch 1670, training loss: 11.99377727508545 = 0.012992209754884243 + 2.0 * 5.990392684936523
Epoch 1670, val loss: 1.2208354473114014
Epoch 1680, training loss: 11.990699768066406 = 0.012790917418897152 + 2.0 * 5.988954544067383
Epoch 1680, val loss: 1.224055528640747
Epoch 1690, training loss: 11.99034595489502 = 0.012592784129083157 + 2.0 * 5.988876819610596
Epoch 1690, val loss: 1.2274439334869385
Epoch 1700, training loss: 12.000314712524414 = 0.01239881943911314 + 2.0 * 5.993957996368408
Epoch 1700, val loss: 1.2307119369506836
Epoch 1710, training loss: 11.992084503173828 = 0.012210393324494362 + 2.0 * 5.989936828613281
Epoch 1710, val loss: 1.2337088584899902
Epoch 1720, training loss: 11.992208480834961 = 0.01203258614987135 + 2.0 * 5.990087985992432
Epoch 1720, val loss: 1.2371201515197754
Epoch 1730, training loss: 11.989100456237793 = 0.01185537688434124 + 2.0 * 5.988622665405273
Epoch 1730, val loss: 1.2403684854507446
Epoch 1740, training loss: 11.988788604736328 = 0.011679698713123798 + 2.0 * 5.98855447769165
Epoch 1740, val loss: 1.243484377861023
Epoch 1750, training loss: 11.999885559082031 = 0.01150919497013092 + 2.0 * 5.99418830871582
Epoch 1750, val loss: 1.246545672416687
Epoch 1760, training loss: 11.989272117614746 = 0.011339995078742504 + 2.0 * 5.98896598815918
Epoch 1760, val loss: 1.2498515844345093
Epoch 1770, training loss: 11.98702335357666 = 0.011179688386619091 + 2.0 * 5.987921714782715
Epoch 1770, val loss: 1.2529973983764648
Epoch 1780, training loss: 11.992915153503418 = 0.011018828488886356 + 2.0 * 5.99094820022583
Epoch 1780, val loss: 1.2559936046600342
Epoch 1790, training loss: 11.987751007080078 = 0.010864131152629852 + 2.0 * 5.988443374633789
Epoch 1790, val loss: 1.2591955661773682
Epoch 1800, training loss: 11.991165161132812 = 0.010713135823607445 + 2.0 * 5.990225791931152
Epoch 1800, val loss: 1.262312889099121
Epoch 1810, training loss: 11.98814582824707 = 0.010562999173998833 + 2.0 * 5.988791465759277
Epoch 1810, val loss: 1.2651264667510986
Epoch 1820, training loss: 11.98438549041748 = 0.01041919831186533 + 2.0 * 5.986983299255371
Epoch 1820, val loss: 1.2682243585586548
Epoch 1830, training loss: 11.983548164367676 = 0.01027719397097826 + 2.0 * 5.986635684967041
Epoch 1830, val loss: 1.2713443040847778
Epoch 1840, training loss: 11.985076904296875 = 0.010136428289115429 + 2.0 * 5.9874701499938965
Epoch 1840, val loss: 1.2743017673492432
Epoch 1850, training loss: 11.98701000213623 = 0.009999365545809269 + 2.0 * 5.9885053634643555
Epoch 1850, val loss: 1.277212142944336
Epoch 1860, training loss: 11.985259056091309 = 0.009866531938314438 + 2.0 * 5.987696170806885
Epoch 1860, val loss: 1.2801896333694458
Epoch 1870, training loss: 11.986869812011719 = 0.009737369604408741 + 2.0 * 5.9885663986206055
Epoch 1870, val loss: 1.2830755710601807
Epoch 1880, training loss: 11.981382369995117 = 0.009608695283532143 + 2.0 * 5.985887050628662
Epoch 1880, val loss: 1.285950779914856
Epoch 1890, training loss: 11.982141494750977 = 0.00948351714760065 + 2.0 * 5.986329078674316
Epoch 1890, val loss: 1.2888944149017334
Epoch 1900, training loss: 11.982782363891602 = 0.009359865449368954 + 2.0 * 5.986711025238037
Epoch 1900, val loss: 1.2916852235794067
Epoch 1910, training loss: 11.98233413696289 = 0.009240102022886276 + 2.0 * 5.986546993255615
Epoch 1910, val loss: 1.2944868803024292
Epoch 1920, training loss: 11.979402542114258 = 0.009123166091740131 + 2.0 * 5.985139846801758
Epoch 1920, val loss: 1.2972670793533325
Epoch 1930, training loss: 11.983114242553711 = 0.009007470682263374 + 2.0 * 5.987053394317627
Epoch 1930, val loss: 1.3001574277877808
Epoch 1940, training loss: 11.979310035705566 = 0.008893894031643867 + 2.0 * 5.985208034515381
Epoch 1940, val loss: 1.3028327226638794
Epoch 1950, training loss: 11.97686767578125 = 0.008784331381320953 + 2.0 * 5.984041690826416
Epoch 1950, val loss: 1.3057496547698975
Epoch 1960, training loss: 11.981399536132812 = 0.00867591891437769 + 2.0 * 5.986361980438232
Epoch 1960, val loss: 1.3083852529525757
Epoch 1970, training loss: 11.982660293579102 = 0.008568544872105122 + 2.0 * 5.987045764923096
Epoch 1970, val loss: 1.3110642433166504
Epoch 1980, training loss: 11.981040000915527 = 0.00846648309379816 + 2.0 * 5.986286640167236
Epoch 1980, val loss: 1.3136295080184937
Epoch 1990, training loss: 11.976497650146484 = 0.008367234840989113 + 2.0 * 5.984065055847168
Epoch 1990, val loss: 1.316418170928955
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7963
Overall ASR: 0.6531
Flip ASR: 0.5956/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.693286895751953 = 1.9457712173461914 + 2.0 * 8.373758316040039
Epoch 0, val loss: 1.945708155632019
Epoch 10, training loss: 18.68067741394043 = 1.935508370399475 + 2.0 * 8.372584342956543
Epoch 10, val loss: 1.935105800628662
Epoch 20, training loss: 18.653871536254883 = 1.9227572679519653 + 2.0 * 8.365556716918945
Epoch 20, val loss: 1.9211829900741577
Epoch 30, training loss: 18.563724517822266 = 1.905918836593628 + 2.0 * 8.328903198242188
Epoch 30, val loss: 1.9022597074508667
Epoch 40, training loss: 18.08041000366211 = 1.8873695135116577 + 2.0 * 8.09652042388916
Epoch 40, val loss: 1.8814762830734253
Epoch 50, training loss: 16.65542221069336 = 1.8693065643310547 + 2.0 * 7.393057346343994
Epoch 50, val loss: 1.8602421283721924
Epoch 60, training loss: 15.86429214477539 = 1.8525309562683105 + 2.0 * 7.005880355834961
Epoch 60, val loss: 1.8414394855499268
Epoch 70, training loss: 15.371832847595215 = 1.838442087173462 + 2.0 * 6.766695499420166
Epoch 70, val loss: 1.8262524604797363
Epoch 80, training loss: 15.011575698852539 = 1.8266568183898926 + 2.0 * 6.592459201812744
Epoch 80, val loss: 1.8134692907333374
Epoch 90, training loss: 14.770280838012695 = 1.8169200420379639 + 2.0 * 6.476680278778076
Epoch 90, val loss: 1.80295729637146
Epoch 100, training loss: 14.593805313110352 = 1.807241678237915 + 2.0 * 6.393281936645508
Epoch 100, val loss: 1.792357087135315
Epoch 110, training loss: 14.47915267944336 = 1.7978602647781372 + 2.0 * 6.340646266937256
Epoch 110, val loss: 1.7818243503570557
Epoch 120, training loss: 14.396112442016602 = 1.7888526916503906 + 2.0 * 6.3036298751831055
Epoch 120, val loss: 1.7715944051742554
Epoch 130, training loss: 14.324362754821777 = 1.7801158428192139 + 2.0 * 6.272123336791992
Epoch 130, val loss: 1.7618625164031982
Epoch 140, training loss: 14.265165328979492 = 1.7715120315551758 + 2.0 * 6.246826648712158
Epoch 140, val loss: 1.752530813217163
Epoch 150, training loss: 14.213672637939453 = 1.7626749277114868 + 2.0 * 6.225498676300049
Epoch 150, val loss: 1.743425965309143
Epoch 160, training loss: 14.168386459350586 = 1.753238320350647 + 2.0 * 6.207573890686035
Epoch 160, val loss: 1.734283447265625
Epoch 170, training loss: 14.129325866699219 = 1.7428361177444458 + 2.0 * 6.193244934082031
Epoch 170, val loss: 1.724714756011963
Epoch 180, training loss: 14.092023849487305 = 1.731109857559204 + 2.0 * 6.18045711517334
Epoch 180, val loss: 1.7143157720565796
Epoch 190, training loss: 14.055312156677246 = 1.717637062072754 + 2.0 * 6.168837547302246
Epoch 190, val loss: 1.7027610540390015
Epoch 200, training loss: 14.019060134887695 = 1.7017741203308105 + 2.0 * 6.158642768859863
Epoch 200, val loss: 1.6894347667694092
Epoch 210, training loss: 13.990768432617188 = 1.6832023859024048 + 2.0 * 6.153782844543457
Epoch 210, val loss: 1.674106240272522
Epoch 220, training loss: 13.947267532348633 = 1.6621098518371582 + 2.0 * 6.142579078674316
Epoch 220, val loss: 1.6566652059555054
Epoch 230, training loss: 13.906632423400879 = 1.6374861001968384 + 2.0 * 6.134572982788086
Epoch 230, val loss: 1.6365435123443604
Epoch 240, training loss: 13.864900588989258 = 1.608626127243042 + 2.0 * 6.128137111663818
Epoch 240, val loss: 1.612855076789856
Epoch 250, training loss: 13.819540977478027 = 1.574774980545044 + 2.0 * 6.122383117675781
Epoch 250, val loss: 1.5851389169692993
Epoch 260, training loss: 13.76995849609375 = 1.535609483718872 + 2.0 * 6.1171746253967285
Epoch 260, val loss: 1.553207278251648
Epoch 270, training loss: 13.716962814331055 = 1.4913339614868164 + 2.0 * 6.112814426422119
Epoch 270, val loss: 1.5172902345657349
Epoch 280, training loss: 13.666298866271973 = 1.443777322769165 + 2.0 * 6.111260890960693
Epoch 280, val loss: 1.4788004159927368
Epoch 290, training loss: 13.607502937316895 = 1.3950282335281372 + 2.0 * 6.106237411499023
Epoch 290, val loss: 1.4399553537368774
Epoch 300, training loss: 13.548807144165039 = 1.3463190793991089 + 2.0 * 6.10124397277832
Epoch 300, val loss: 1.4012573957443237
Epoch 310, training loss: 13.496535301208496 = 1.2988169193267822 + 2.0 * 6.0988593101501465
Epoch 310, val loss: 1.363592505455017
Epoch 320, training loss: 13.446253776550293 = 1.2538455724716187 + 2.0 * 6.0962042808532715
Epoch 320, val loss: 1.3280761241912842
Epoch 330, training loss: 13.395368576049805 = 1.2114933729171753 + 2.0 * 6.09193754196167
Epoch 330, val loss: 1.2951065301895142
Epoch 340, training loss: 13.350916862487793 = 1.1709144115447998 + 2.0 * 6.090001106262207
Epoch 340, val loss: 1.2638306617736816
Epoch 350, training loss: 13.308673858642578 = 1.1317048072814941 + 2.0 * 6.088484287261963
Epoch 350, val loss: 1.233974575996399
Epoch 360, training loss: 13.263038635253906 = 1.093443512916565 + 2.0 * 6.084797382354736
Epoch 360, val loss: 1.2053022384643555
Epoch 370, training loss: 13.218524932861328 = 1.0554633140563965 + 2.0 * 6.081531047821045
Epoch 370, val loss: 1.177071213722229
Epoch 380, training loss: 13.173165321350098 = 1.016922116279602 + 2.0 * 6.078121662139893
Epoch 380, val loss: 1.1487231254577637
Epoch 390, training loss: 13.131684303283691 = 0.977607011795044 + 2.0 * 6.077038764953613
Epoch 390, val loss: 1.1198915243148804
Epoch 400, training loss: 13.088412284851074 = 0.9380248188972473 + 2.0 * 6.075193881988525
Epoch 400, val loss: 1.0905585289001465
Epoch 410, training loss: 13.044281005859375 = 0.898554801940918 + 2.0 * 6.0728631019592285
Epoch 410, val loss: 1.0616823434829712
Epoch 420, training loss: 12.999109268188477 = 0.8595328330993652 + 2.0 * 6.069787979125977
Epoch 420, val loss: 1.0330294370651245
Epoch 430, training loss: 12.969368934631348 = 0.8213059902191162 + 2.0 * 6.074031352996826
Epoch 430, val loss: 1.0048866271972656
Epoch 440, training loss: 12.923038482666016 = 0.7849160432815552 + 2.0 * 6.069061279296875
Epoch 440, val loss: 0.9784529805183411
Epoch 450, training loss: 12.88171672821045 = 0.7507920861244202 + 2.0 * 6.065462112426758
Epoch 450, val loss: 0.9537816643714905
Epoch 460, training loss: 12.851224899291992 = 0.718580961227417 + 2.0 * 6.066321849822998
Epoch 460, val loss: 0.9306572079658508
Epoch 470, training loss: 12.815521240234375 = 0.6883397102355957 + 2.0 * 6.0635905265808105
Epoch 470, val loss: 0.9095858335494995
Epoch 480, training loss: 12.778451919555664 = 0.6601862907409668 + 2.0 * 6.0591325759887695
Epoch 480, val loss: 0.8902309536933899
Epoch 490, training loss: 12.748735427856445 = 0.6335595846176147 + 2.0 * 6.05758810043335
Epoch 490, val loss: 0.8725019693374634
Epoch 500, training loss: 12.722654342651367 = 0.6084837913513184 + 2.0 * 6.0570855140686035
Epoch 500, val loss: 0.856365978717804
Epoch 510, training loss: 12.694496154785156 = 0.5850472450256348 + 2.0 * 6.054724216461182
Epoch 510, val loss: 0.8418728709220886
Epoch 520, training loss: 12.669257164001465 = 0.5627467632293701 + 2.0 * 6.053255081176758
Epoch 520, val loss: 0.8286223411560059
Epoch 530, training loss: 12.653679847717285 = 0.5413927435874939 + 2.0 * 6.056143760681152
Epoch 530, val loss: 0.8163782358169556
Epoch 540, training loss: 12.625669479370117 = 0.5209851264953613 + 2.0 * 6.052341938018799
Epoch 540, val loss: 0.8052329421043396
Epoch 550, training loss: 12.606840133666992 = 0.501495361328125 + 2.0 * 6.052672386169434
Epoch 550, val loss: 0.7950111031532288
Epoch 560, training loss: 12.58021068572998 = 0.4827759563922882 + 2.0 * 6.048717498779297
Epoch 560, val loss: 0.7857127785682678
Epoch 570, training loss: 12.55785083770752 = 0.46482646465301514 + 2.0 * 6.046512126922607
Epoch 570, val loss: 0.7770864367485046
Epoch 580, training loss: 12.54931926727295 = 0.4474634826183319 + 2.0 * 6.050928115844727
Epoch 580, val loss: 0.7691763043403625
Epoch 590, training loss: 12.518465995788574 = 0.43087008595466614 + 2.0 * 6.043797969818115
Epoch 590, val loss: 0.7619385123252869
Epoch 600, training loss: 12.499252319335938 = 0.4148183763027191 + 2.0 * 6.042216777801514
Epoch 600, val loss: 0.7553558945655823
Epoch 610, training loss: 12.481926918029785 = 0.3992759883403778 + 2.0 * 6.041325569152832
Epoch 610, val loss: 0.7492209672927856
Epoch 620, training loss: 12.476048469543457 = 0.3842654824256897 + 2.0 * 6.045891284942627
Epoch 620, val loss: 0.7437038421630859
Epoch 630, training loss: 12.450013160705566 = 0.36989930272102356 + 2.0 * 6.0400567054748535
Epoch 630, val loss: 0.7387255430221558
Epoch 640, training loss: 12.431923866271973 = 0.3560139238834381 + 2.0 * 6.037954807281494
Epoch 640, val loss: 0.7342163324356079
Epoch 650, training loss: 12.415535926818848 = 0.3424917459487915 + 2.0 * 6.036521911621094
Epoch 650, val loss: 0.7301421761512756
Epoch 660, training loss: 12.438782691955566 = 0.3293769061565399 + 2.0 * 6.0547027587890625
Epoch 660, val loss: 0.7264757752418518
Epoch 670, training loss: 12.398335456848145 = 0.3168022334575653 + 2.0 * 6.040766716003418
Epoch 670, val loss: 0.7232344746589661
Epoch 680, training loss: 12.373624801635742 = 0.30464106798171997 + 2.0 * 6.034492015838623
Epoch 680, val loss: 0.7206186652183533
Epoch 690, training loss: 12.36036205291748 = 0.2927546799182892 + 2.0 * 6.033803462982178
Epoch 690, val loss: 0.7181345224380493
Epoch 700, training loss: 12.349188804626465 = 0.2811986207962036 + 2.0 * 6.033995151519775
Epoch 700, val loss: 0.7161310911178589
Epoch 710, training loss: 12.336752891540527 = 0.27009445428848267 + 2.0 * 6.033329010009766
Epoch 710, val loss: 0.7145915031433105
Epoch 720, training loss: 12.321258544921875 = 0.259310781955719 + 2.0 * 6.0309739112854
Epoch 720, val loss: 0.7133774757385254
Epoch 730, training loss: 12.309054374694824 = 0.24881501495838165 + 2.0 * 6.030119895935059
Epoch 730, val loss: 0.7124865651130676
Epoch 740, training loss: 12.310162544250488 = 0.23866747319698334 + 2.0 * 6.035747528076172
Epoch 740, val loss: 0.7119166254997253
Epoch 750, training loss: 12.289671897888184 = 0.2288968414068222 + 2.0 * 6.0303874015808105
Epoch 750, val loss: 0.7117264270782471
Epoch 760, training loss: 12.27662181854248 = 0.21952097117900848 + 2.0 * 6.028550624847412
Epoch 760, val loss: 0.7118917107582092
Epoch 770, training loss: 12.265600204467773 = 0.21046726405620575 + 2.0 * 6.027566432952881
Epoch 770, val loss: 0.7123390436172485
Epoch 780, training loss: 12.259573936462402 = 0.2017684280872345 + 2.0 * 6.028902530670166
Epoch 780, val loss: 0.7130894660949707
Epoch 790, training loss: 12.246235847473145 = 0.1934831291437149 + 2.0 * 6.026376247406006
Epoch 790, val loss: 0.7141975164413452
Epoch 800, training loss: 12.236778259277344 = 0.1855258047580719 + 2.0 * 6.025626182556152
Epoch 800, val loss: 0.715578019618988
Epoch 810, training loss: 12.233949661254883 = 0.1778869479894638 + 2.0 * 6.028031349182129
Epoch 810, val loss: 0.7171682715415955
Epoch 820, training loss: 12.220083236694336 = 0.17056261003017426 + 2.0 * 6.0247602462768555
Epoch 820, val loss: 0.7191030383110046
Epoch 830, training loss: 12.210330963134766 = 0.16356438398361206 + 2.0 * 6.023383140563965
Epoch 830, val loss: 0.7213010191917419
Epoch 840, training loss: 12.202435493469238 = 0.15686509013175964 + 2.0 * 6.022785186767578
Epoch 840, val loss: 0.7236652970314026
Epoch 850, training loss: 12.20551872253418 = 0.15043652057647705 + 2.0 * 6.027541160583496
Epoch 850, val loss: 0.7262944579124451
Epoch 860, training loss: 12.194727897644043 = 0.14433693885803223 + 2.0 * 6.025195598602295
Epoch 860, val loss: 0.7289813160896301
Epoch 870, training loss: 12.179529190063477 = 0.13851948082447052 + 2.0 * 6.020504951477051
Epoch 870, val loss: 0.7319809794425964
Epoch 880, training loss: 12.173050880432129 = 0.13296610116958618 + 2.0 * 6.020042419433594
Epoch 880, val loss: 0.7351341843605042
Epoch 890, training loss: 12.174240112304688 = 0.1276589035987854 + 2.0 * 6.023290634155273
Epoch 890, val loss: 0.7383853197097778
Epoch 900, training loss: 12.162672996520996 = 0.12261833250522614 + 2.0 * 6.020027160644531
Epoch 900, val loss: 0.7418286800384521
Epoch 910, training loss: 12.157709121704102 = 0.11782442778348923 + 2.0 * 6.019942283630371
Epoch 910, val loss: 0.7453805804252625
Epoch 920, training loss: 12.149679183959961 = 0.11327335983514786 + 2.0 * 6.018202781677246
Epoch 920, val loss: 0.7490285634994507
Epoch 930, training loss: 12.174711227416992 = 0.10893797129392624 + 2.0 * 6.032886505126953
Epoch 930, val loss: 0.7527489066123962
Epoch 940, training loss: 12.142853736877441 = 0.10478904843330383 + 2.0 * 6.0190324783325195
Epoch 940, val loss: 0.756466805934906
Epoch 950, training loss: 12.133328437805176 = 0.10086939483880997 + 2.0 * 6.016229629516602
Epoch 950, val loss: 0.7604467272758484
Epoch 960, training loss: 12.126887321472168 = 0.09711389988660812 + 2.0 * 6.014886856079102
Epoch 960, val loss: 0.7644440531730652
Epoch 970, training loss: 12.121538162231445 = 0.09350429475307465 + 2.0 * 6.014017105102539
Epoch 970, val loss: 0.7685300707817078
Epoch 980, training loss: 12.1170015335083 = 0.09003525227308273 + 2.0 * 6.013483047485352
Epoch 980, val loss: 0.7726951241493225
Epoch 990, training loss: 12.133743286132812 = 0.08671828359365463 + 2.0 * 6.023512363433838
Epoch 990, val loss: 0.7769312858581543
Epoch 1000, training loss: 12.112480163574219 = 0.08357533067464828 + 2.0 * 6.0144524574279785
Epoch 1000, val loss: 0.7810707092285156
Epoch 1010, training loss: 12.114665985107422 = 0.08059470355510712 + 2.0 * 6.017035484313965
Epoch 1010, val loss: 0.7854016423225403
Epoch 1020, training loss: 12.10516357421875 = 0.07777265459299088 + 2.0 * 6.013695240020752
Epoch 1020, val loss: 0.7895491719245911
Epoch 1030, training loss: 12.097493171691895 = 0.07506439089775085 + 2.0 * 6.011214256286621
Epoch 1030, val loss: 0.7937824726104736
Epoch 1040, training loss: 12.095308303833008 = 0.07247354090213776 + 2.0 * 6.011417388916016
Epoch 1040, val loss: 0.7980859875679016
Epoch 1050, training loss: 12.095909118652344 = 0.0699884295463562 + 2.0 * 6.012960433959961
Epoch 1050, val loss: 0.8023567199707031
Epoch 1060, training loss: 12.098788261413574 = 0.06761372834444046 + 2.0 * 6.015587329864502
Epoch 1060, val loss: 0.80669766664505
Epoch 1070, training loss: 12.088729858398438 = 0.06536500155925751 + 2.0 * 6.011682510375977
Epoch 1070, val loss: 0.8108800053596497
Epoch 1080, training loss: 12.083131790161133 = 0.06321282684803009 + 2.0 * 6.009959697723389
Epoch 1080, val loss: 0.8152512311935425
Epoch 1090, training loss: 12.078107833862305 = 0.06115015596151352 + 2.0 * 6.00847864151001
Epoch 1090, val loss: 0.8195130228996277
Epoch 1100, training loss: 12.08890151977539 = 0.059176966547966 + 2.0 * 6.014862060546875
Epoch 1100, val loss: 0.8238038420677185
Epoch 1110, training loss: 12.081494331359863 = 0.05729958415031433 + 2.0 * 6.012097358703613
Epoch 1110, val loss: 0.8279642462730408
Epoch 1120, training loss: 12.070276260375977 = 0.05550546199083328 + 2.0 * 6.00738525390625
Epoch 1120, val loss: 0.8322120904922485
Epoch 1130, training loss: 12.067726135253906 = 0.05378798022866249 + 2.0 * 6.006968975067139
Epoch 1130, val loss: 0.8364323377609253
Epoch 1140, training loss: 12.0784912109375 = 0.05213591456413269 + 2.0 * 6.013177871704102
Epoch 1140, val loss: 0.8406486511230469
Epoch 1150, training loss: 12.069925308227539 = 0.05057667940855026 + 2.0 * 6.009674549102783
Epoch 1150, val loss: 0.8445870280265808
Epoch 1160, training loss: 12.061938285827637 = 0.04907143488526344 + 2.0 * 6.006433486938477
Epoch 1160, val loss: 0.8487918376922607
Epoch 1170, training loss: 12.057149887084961 = 0.047632746398448944 + 2.0 * 6.004758358001709
Epoch 1170, val loss: 0.8528847098350525
Epoch 1180, training loss: 12.05549144744873 = 0.04624541103839874 + 2.0 * 6.004622936248779
Epoch 1180, val loss: 0.8569908142089844
Epoch 1190, training loss: 12.071257591247559 = 0.04490811005234718 + 2.0 * 6.013174533843994
Epoch 1190, val loss: 0.861111581325531
Epoch 1200, training loss: 12.061975479125977 = 0.04363476485013962 + 2.0 * 6.0091705322265625
Epoch 1200, val loss: 0.8649756908416748
Epoch 1210, training loss: 12.05090045928955 = 0.0424174964427948 + 2.0 * 6.004241466522217
Epoch 1210, val loss: 0.8690237402915955
Epoch 1220, training loss: 12.048370361328125 = 0.04124458134174347 + 2.0 * 6.003562927246094
Epoch 1220, val loss: 0.8730155825614929
Epoch 1230, training loss: 12.052823066711426 = 0.04011351242661476 + 2.0 * 6.006354808807373
Epoch 1230, val loss: 0.8769547939300537
Epoch 1240, training loss: 12.045573234558105 = 0.03902992233633995 + 2.0 * 6.003271579742432
Epoch 1240, val loss: 0.8808553814888
Epoch 1250, training loss: 12.043264389038086 = 0.0379902608692646 + 2.0 * 6.002636909484863
Epoch 1250, val loss: 0.8847646117210388
Epoch 1260, training loss: 12.047821044921875 = 0.0369856059551239 + 2.0 * 6.005417823791504
Epoch 1260, val loss: 0.8886058926582336
Epoch 1270, training loss: 12.04188060760498 = 0.036021068692207336 + 2.0 * 6.0029296875
Epoch 1270, val loss: 0.892375111579895
Epoch 1280, training loss: 12.039755821228027 = 0.035093922168016434 + 2.0 * 6.002330780029297
Epoch 1280, val loss: 0.8961870670318604
Epoch 1290, training loss: 12.042805671691895 = 0.03420228138566017 + 2.0 * 6.00430154800415
Epoch 1290, val loss: 0.899908185005188
Epoch 1300, training loss: 12.038336753845215 = 0.033342406153678894 + 2.0 * 6.00249719619751
Epoch 1300, val loss: 0.9035289287567139
Epoch 1310, training loss: 12.033402442932129 = 0.032512880861759186 + 2.0 * 6.0004448890686035
Epoch 1310, val loss: 0.9072048664093018
Epoch 1320, training loss: 12.031599998474121 = 0.03171507269144058 + 2.0 * 5.999942302703857
Epoch 1320, val loss: 0.9108760356903076
Epoch 1330, training loss: 12.03222370147705 = 0.030939416959881783 + 2.0 * 6.0006422996521
Epoch 1330, val loss: 0.9145153760910034
Epoch 1340, training loss: 12.036971092224121 = 0.030190743505954742 + 2.0 * 6.003390312194824
Epoch 1340, val loss: 0.9181033372879028
Epoch 1350, training loss: 12.035662651062012 = 0.029469134286046028 + 2.0 * 6.003096580505371
Epoch 1350, val loss: 0.921696662902832
Epoch 1360, training loss: 12.030705451965332 = 0.0287749283015728 + 2.0 * 6.000965118408203
Epoch 1360, val loss: 0.9251902103424072
Epoch 1370, training loss: 12.029858589172363 = 0.02810983546078205 + 2.0 * 6.0008745193481445
Epoch 1370, val loss: 0.9286532998085022
Epoch 1380, training loss: 12.027998924255371 = 0.027461135759949684 + 2.0 * 6.000268936157227
Epoch 1380, val loss: 0.932105541229248
Epoch 1390, training loss: 12.024410247802734 = 0.02683621644973755 + 2.0 * 5.998786926269531
Epoch 1390, val loss: 0.9355510473251343
Epoch 1400, training loss: 12.021086692810059 = 0.026227416470646858 + 2.0 * 5.997429847717285
Epoch 1400, val loss: 0.9389675855636597
Epoch 1410, training loss: 12.024855613708496 = 0.02563510276377201 + 2.0 * 5.999610424041748
Epoch 1410, val loss: 0.9423599243164062
Epoch 1420, training loss: 12.026078224182129 = 0.025066733360290527 + 2.0 * 6.0005059242248535
Epoch 1420, val loss: 0.9456104636192322
Epoch 1430, training loss: 12.025333404541016 = 0.024527614936232567 + 2.0 * 6.000402927398682
Epoch 1430, val loss: 0.9488059878349304
Epoch 1440, training loss: 12.016132354736328 = 0.024001993238925934 + 2.0 * 5.996065139770508
Epoch 1440, val loss: 0.9521079659461975
Epoch 1450, training loss: 12.016057968139648 = 0.023489663377404213 + 2.0 * 5.996284008026123
Epoch 1450, val loss: 0.9553038477897644
Epoch 1460, training loss: 12.014693260192871 = 0.02298932708799839 + 2.0 * 5.995851993560791
Epoch 1460, val loss: 0.9585691094398499
Epoch 1470, training loss: 12.030266761779785 = 0.022504054009914398 + 2.0 * 6.003881454467773
Epoch 1470, val loss: 0.9617937803268433
Epoch 1480, training loss: 12.020278930664062 = 0.022040383890271187 + 2.0 * 5.999119281768799
Epoch 1480, val loss: 0.9649072885513306
Epoch 1490, training loss: 12.014530181884766 = 0.021590327844023705 + 2.0 * 5.996469974517822
Epoch 1490, val loss: 0.968041181564331
Epoch 1500, training loss: 12.015716552734375 = 0.021153109148144722 + 2.0 * 5.997281551361084
Epoch 1500, val loss: 0.9711238145828247
Epoch 1510, training loss: 12.022850036621094 = 0.020729726180434227 + 2.0 * 6.0010600090026855
Epoch 1510, val loss: 0.9741259813308716
Epoch 1520, training loss: 12.014180183410645 = 0.020324019715189934 + 2.0 * 5.9969282150268555
Epoch 1520, val loss: 0.9771367907524109
Epoch 1530, training loss: 12.00993824005127 = 0.019928641617298126 + 2.0 * 5.995004653930664
Epoch 1530, val loss: 0.9801514744758606
Epoch 1540, training loss: 12.007694244384766 = 0.01954280026257038 + 2.0 * 5.994075775146484
Epoch 1540, val loss: 0.9831641316413879
Epoch 1550, training loss: 12.0128812789917 = 0.01916526071727276 + 2.0 * 5.9968581199646
Epoch 1550, val loss: 0.9861342906951904
Epoch 1560, training loss: 12.006383895874023 = 0.018798567354679108 + 2.0 * 5.993792533874512
Epoch 1560, val loss: 0.9890318512916565
Epoch 1570, training loss: 12.01798152923584 = 0.018444636836647987 + 2.0 * 5.999768257141113
Epoch 1570, val loss: 0.9919781684875488
Epoch 1580, training loss: 12.005414962768555 = 0.018107211217284203 + 2.0 * 5.993653774261475
Epoch 1580, val loss: 0.9947232604026794
Epoch 1590, training loss: 12.00350570678711 = 0.017775487154722214 + 2.0 * 5.992865085601807
Epoch 1590, val loss: 0.9976321458816528
Epoch 1600, training loss: 12.001895904541016 = 0.01745133474469185 + 2.0 * 5.992222309112549
Epoch 1600, val loss: 1.0004299879074097
Epoch 1610, training loss: 12.00078296661377 = 0.01713319681584835 + 2.0 * 5.991825103759766
Epoch 1610, val loss: 1.003300428390503
Epoch 1620, training loss: 12.00157356262207 = 0.016822101548314095 + 2.0 * 5.99237585067749
Epoch 1620, val loss: 1.0061635971069336
Epoch 1630, training loss: 12.015640258789062 = 0.016522103920578957 + 2.0 * 5.999558925628662
Epoch 1630, val loss: 1.0089071989059448
Epoch 1640, training loss: 12.007041931152344 = 0.016233645379543304 + 2.0 * 5.995404243469238
Epoch 1640, val loss: 1.0116077661514282
Epoch 1650, training loss: 12.000872611999512 = 0.015952080488204956 + 2.0 * 5.992460250854492
Epoch 1650, val loss: 1.0143275260925293
Epoch 1660, training loss: 11.998100280761719 = 0.015679335221648216 + 2.0 * 5.991210460662842
Epoch 1660, val loss: 1.0169841051101685
Epoch 1670, training loss: 11.996667861938477 = 0.01541135273873806 + 2.0 * 5.990628242492676
Epoch 1670, val loss: 1.0197019577026367
Epoch 1680, training loss: 11.999584197998047 = 0.015148108825087547 + 2.0 * 5.992218017578125
Epoch 1680, val loss: 1.0224182605743408
Epoch 1690, training loss: 12.002487182617188 = 0.01489289291203022 + 2.0 * 5.993797302246094
Epoch 1690, val loss: 1.0250201225280762
Epoch 1700, training loss: 12.000495910644531 = 0.014645886607468128 + 2.0 * 5.99292516708374
Epoch 1700, val loss: 1.0276304483413696
Epoch 1710, training loss: 11.995577812194824 = 0.014404845423996449 + 2.0 * 5.990586280822754
Epoch 1710, val loss: 1.0302553176879883
Epoch 1720, training loss: 11.9939603805542 = 0.014169427566230297 + 2.0 * 5.989895343780518
Epoch 1720, val loss: 1.0328309535980225
Epoch 1730, training loss: 11.99410629272461 = 0.01393785048276186 + 2.0 * 5.990084171295166
Epoch 1730, val loss: 1.035457968711853
Epoch 1740, training loss: 12.006160736083984 = 0.013711641542613506 + 2.0 * 5.996224403381348
Epoch 1740, val loss: 1.0380401611328125
Epoch 1750, training loss: 11.999900817871094 = 0.013494652695953846 + 2.0 * 5.993203163146973
Epoch 1750, val loss: 1.040436029434204
Epoch 1760, training loss: 12.000327110290527 = 0.01328069344162941 + 2.0 * 5.993523120880127
Epoch 1760, val loss: 1.0429694652557373
Epoch 1770, training loss: 11.992338180541992 = 0.013075926341116428 + 2.0 * 5.989631175994873
Epoch 1770, val loss: 1.0454211235046387
Epoch 1780, training loss: 11.991239547729492 = 0.01287391409277916 + 2.0 * 5.989182949066162
Epoch 1780, val loss: 1.04789137840271
Epoch 1790, training loss: 11.99400520324707 = 0.012674952857196331 + 2.0 * 5.990664958953857
Epoch 1790, val loss: 1.0503268241882324
Epoch 1800, training loss: 11.99573040008545 = 0.012479609809815884 + 2.0 * 5.9916253089904785
Epoch 1800, val loss: 1.0527472496032715
Epoch 1810, training loss: 11.989026069641113 = 0.012291369028389454 + 2.0 * 5.988367557525635
Epoch 1810, val loss: 1.0551518201828003
Epoch 1820, training loss: 11.989815711975098 = 0.012108834460377693 + 2.0 * 5.988853454589844
Epoch 1820, val loss: 1.0575573444366455
Epoch 1830, training loss: 11.988288879394531 = 0.011926415376365185 + 2.0 * 5.988181114196777
Epoch 1830, val loss: 1.0599254369735718
Epoch 1840, training loss: 11.997661590576172 = 0.011748561635613441 + 2.0 * 5.992956638336182
Epoch 1840, val loss: 1.062279462814331
Epoch 1850, training loss: 11.99142074584961 = 0.011574449948966503 + 2.0 * 5.989923000335693
Epoch 1850, val loss: 1.0646106004714966
Epoch 1860, training loss: 11.987211227416992 = 0.011406350880861282 + 2.0 * 5.987902641296387
Epoch 1860, val loss: 1.0669498443603516
Epoch 1870, training loss: 11.989255905151367 = 0.011239294894039631 + 2.0 * 5.98900842666626
Epoch 1870, val loss: 1.0692899227142334
Epoch 1880, training loss: 11.98988151550293 = 0.011076956056058407 + 2.0 * 5.9894022941589355
Epoch 1880, val loss: 1.0715103149414062
Epoch 1890, training loss: 11.9860258102417 = 0.010919750668108463 + 2.0 * 5.987553119659424
Epoch 1890, val loss: 1.0737550258636475
Epoch 1900, training loss: 11.98299789428711 = 0.010765806771814823 + 2.0 * 5.9861159324646
Epoch 1900, val loss: 1.0760353803634644
Epoch 1910, training loss: 11.986795425415039 = 0.010613050311803818 + 2.0 * 5.988090991973877
Epoch 1910, val loss: 1.078269362449646
Epoch 1920, training loss: 11.98892879486084 = 0.010464408434927464 + 2.0 * 5.989232063293457
Epoch 1920, val loss: 1.0804519653320312
Epoch 1930, training loss: 11.985397338867188 = 0.010322121903300285 + 2.0 * 5.987537384033203
Epoch 1930, val loss: 1.0826663970947266
Epoch 1940, training loss: 11.98243236541748 = 0.010178790427744389 + 2.0 * 5.986126899719238
Epoch 1940, val loss: 1.0849034786224365
Epoch 1950, training loss: 11.983704566955566 = 0.010039126500487328 + 2.0 * 5.986832618713379
Epoch 1950, val loss: 1.0870603322982788
Epoch 1960, training loss: 11.984908103942871 = 0.009902161546051502 + 2.0 * 5.9875030517578125
Epoch 1960, val loss: 1.0891882181167603
Epoch 1970, training loss: 11.985581398010254 = 0.009769660420715809 + 2.0 * 5.987905979156494
Epoch 1970, val loss: 1.0914336442947388
Epoch 1980, training loss: 11.980854988098145 = 0.009640105068683624 + 2.0 * 5.985607624053955
Epoch 1980, val loss: 1.0934711694717407
Epoch 1990, training loss: 11.980350494384766 = 0.009514226578176022 + 2.0 * 5.985418319702148
Epoch 1990, val loss: 1.0955393314361572
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.5683
Flip ASR: 0.5022/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.69244384765625 = 1.9450757503509521 + 2.0 * 8.37368392944336
Epoch 0, val loss: 1.9458532333374023
Epoch 10, training loss: 18.679100036621094 = 1.934129238128662 + 2.0 * 8.372485160827637
Epoch 10, val loss: 1.934895396232605
Epoch 20, training loss: 18.649690628051758 = 1.9204208850860596 + 2.0 * 8.36463451385498
Epoch 20, val loss: 1.9205785989761353
Epoch 30, training loss: 18.52875518798828 = 1.9024159908294678 + 2.0 * 8.313169479370117
Epoch 30, val loss: 1.9014562368392944
Epoch 40, training loss: 17.721511840820312 = 1.8837966918945312 + 2.0 * 7.918858051300049
Epoch 40, val loss: 1.8819621801376343
Epoch 50, training loss: 16.349348068237305 = 1.8639695644378662 + 2.0 * 7.24268913269043
Epoch 50, val loss: 1.8619874715805054
Epoch 60, training loss: 15.451602935791016 = 1.8498095273971558 + 2.0 * 6.800896644592285
Epoch 60, val loss: 1.8485186100006104
Epoch 70, training loss: 15.038764953613281 = 1.8395140171051025 + 2.0 * 6.599625587463379
Epoch 70, val loss: 1.8386039733886719
Epoch 80, training loss: 14.856002807617188 = 1.8301047086715698 + 2.0 * 6.512948989868164
Epoch 80, val loss: 1.8288452625274658
Epoch 90, training loss: 14.710869789123535 = 1.8202465772628784 + 2.0 * 6.445311546325684
Epoch 90, val loss: 1.8183666467666626
Epoch 100, training loss: 14.573660850524902 = 1.8113679885864258 + 2.0 * 6.381146430969238
Epoch 100, val loss: 1.8092031478881836
Epoch 110, training loss: 14.465816497802734 = 1.803957462310791 + 2.0 * 6.330929279327393
Epoch 110, val loss: 1.8017938137054443
Epoch 120, training loss: 14.37697696685791 = 1.7970823049545288 + 2.0 * 6.289947509765625
Epoch 120, val loss: 1.7949241399765015
Epoch 130, training loss: 14.301828384399414 = 1.7901177406311035 + 2.0 * 6.255855560302734
Epoch 130, val loss: 1.7880240678787231
Epoch 140, training loss: 14.24074649810791 = 1.7828724384307861 + 2.0 * 6.228937149047852
Epoch 140, val loss: 1.7810235023498535
Epoch 150, training loss: 14.190583229064941 = 1.775100588798523 + 2.0 * 6.2077412605285645
Epoch 150, val loss: 1.7738518714904785
Epoch 160, training loss: 14.145573616027832 = 1.7664660215377808 + 2.0 * 6.189553737640381
Epoch 160, val loss: 1.7661161422729492
Epoch 170, training loss: 14.11119270324707 = 1.7566487789154053 + 2.0 * 6.177271842956543
Epoch 170, val loss: 1.757548451423645
Epoch 180, training loss: 14.072193145751953 = 1.7456021308898926 + 2.0 * 6.163295269012451
Epoch 180, val loss: 1.7480804920196533
Epoch 190, training loss: 14.03830623626709 = 1.7330334186553955 + 2.0 * 6.152636528015137
Epoch 190, val loss: 1.7374293804168701
Epoch 200, training loss: 14.006830215454102 = 1.7186189889907837 + 2.0 * 6.144105434417725
Epoch 200, val loss: 1.7253113985061646
Epoch 210, training loss: 13.972990989685059 = 1.7018892765045166 + 2.0 * 6.1355509757995605
Epoch 210, val loss: 1.711553931236267
Epoch 220, training loss: 13.943384170532227 = 1.6825156211853027 + 2.0 * 6.130434036254883
Epoch 220, val loss: 1.6955435276031494
Epoch 230, training loss: 13.90419864654541 = 1.659997582435608 + 2.0 * 6.122100353240967
Epoch 230, val loss: 1.6772602796554565
Epoch 240, training loss: 13.866321563720703 = 1.633863925933838 + 2.0 * 6.116229057312012
Epoch 240, val loss: 1.6562470197677612
Epoch 250, training loss: 13.82515811920166 = 1.60357666015625 + 2.0 * 6.110790729522705
Epoch 250, val loss: 1.6319504976272583
Epoch 260, training loss: 13.78308391571045 = 1.568713903427124 + 2.0 * 6.107184886932373
Epoch 260, val loss: 1.6040116548538208
Epoch 270, training loss: 13.731693267822266 = 1.5288537740707397 + 2.0 * 6.101419925689697
Epoch 270, val loss: 1.5722037553787231
Epoch 280, training loss: 13.680468559265137 = 1.4836889505386353 + 2.0 * 6.098389625549316
Epoch 280, val loss: 1.5363277196884155
Epoch 290, training loss: 13.626824378967285 = 1.4342461824417114 + 2.0 * 6.096289157867432
Epoch 290, val loss: 1.497373342514038
Epoch 300, training loss: 13.563940048217773 = 1.3817336559295654 + 2.0 * 6.0911030769348145
Epoch 300, val loss: 1.4557852745056152
Epoch 310, training loss: 13.501428604125977 = 1.3264586925506592 + 2.0 * 6.087484836578369
Epoch 310, val loss: 1.4120789766311646
Epoch 320, training loss: 13.439478874206543 = 1.2696949243545532 + 2.0 * 6.0848917961120605
Epoch 320, val loss: 1.3672926425933838
Epoch 330, training loss: 13.378612518310547 = 1.2133351564407349 + 2.0 * 6.082638740539551
Epoch 330, val loss: 1.323293924331665
Epoch 340, training loss: 13.32530689239502 = 1.1594783067703247 + 2.0 * 6.082914352416992
Epoch 340, val loss: 1.281038522720337
Epoch 350, training loss: 13.262593269348145 = 1.1086814403533936 + 2.0 * 6.076955795288086
Epoch 350, val loss: 1.2414355278015137
Epoch 360, training loss: 13.209692001342773 = 1.0609983205795288 + 2.0 * 6.074347019195557
Epoch 360, val loss: 1.2043626308441162
Epoch 370, training loss: 13.171670913696289 = 1.0163044929504395 + 2.0 * 6.077682971954346
Epoch 370, val loss: 1.1699285507202148
Epoch 380, training loss: 13.11518669128418 = 0.9750384092330933 + 2.0 * 6.070074081420898
Epoch 380, val loss: 1.1382588148117065
Epoch 390, training loss: 13.071401596069336 = 0.9364968538284302 + 2.0 * 6.067452430725098
Epoch 390, val loss: 1.1090140342712402
Epoch 400, training loss: 13.034574508666992 = 0.9002218246459961 + 2.0 * 6.067176342010498
Epoch 400, val loss: 1.0817532539367676
Epoch 410, training loss: 12.99321174621582 = 0.8660560250282288 + 2.0 * 6.063577651977539
Epoch 410, val loss: 1.0564571619033813
Epoch 420, training loss: 12.955965042114258 = 0.833504319190979 + 2.0 * 6.061230182647705
Epoch 420, val loss: 1.0325535535812378
Epoch 430, training loss: 12.931241035461426 = 0.8022410273551941 + 2.0 * 6.064499855041504
Epoch 430, val loss: 1.009737491607666
Epoch 440, training loss: 12.890381813049316 = 0.7727604508399963 + 2.0 * 6.058810710906982
Epoch 440, val loss: 0.9881208539009094
Epoch 450, training loss: 12.85561466217041 = 0.74458909034729 + 2.0 * 6.05551290512085
Epoch 450, val loss: 0.967696487903595
Epoch 460, training loss: 12.826229095458984 = 0.7175174951553345 + 2.0 * 6.054355621337891
Epoch 460, val loss: 0.9482260346412659
Epoch 470, training loss: 12.797409057617188 = 0.6917133331298828 + 2.0 * 6.052847862243652
Epoch 470, val loss: 0.9298014044761658
Epoch 480, training loss: 12.771145820617676 = 0.6672418117523193 + 2.0 * 6.051951885223389
Epoch 480, val loss: 0.912610650062561
Epoch 490, training loss: 12.742701530456543 = 0.6438382863998413 + 2.0 * 6.049431800842285
Epoch 490, val loss: 0.8964487910270691
Epoch 500, training loss: 12.723801612854004 = 0.6212589740753174 + 2.0 * 6.051271438598633
Epoch 500, val loss: 0.8811264634132385
Epoch 510, training loss: 12.698831558227539 = 0.5996915698051453 + 2.0 * 6.049570083618164
Epoch 510, val loss: 0.8666532635688782
Epoch 520, training loss: 12.672317504882812 = 0.5789129137992859 + 2.0 * 6.0467023849487305
Epoch 520, val loss: 0.8531570434570312
Epoch 530, training loss: 12.648656845092773 = 0.5587853193283081 + 2.0 * 6.044935703277588
Epoch 530, val loss: 0.8403710722923279
Epoch 540, training loss: 12.62460994720459 = 0.5392110347747803 + 2.0 * 6.042699337005615
Epoch 540, val loss: 0.8283068537712097
Epoch 550, training loss: 12.608976364135742 = 0.5201361775398254 + 2.0 * 6.04442024230957
Epoch 550, val loss: 0.8168795108795166
Epoch 560, training loss: 12.585284233093262 = 0.5015503764152527 + 2.0 * 6.041866779327393
Epoch 560, val loss: 0.8061923980712891
Epoch 570, training loss: 12.564379692077637 = 0.4834921658039093 + 2.0 * 6.0404438972473145
Epoch 570, val loss: 0.7960922122001648
Epoch 580, training loss: 12.543932914733887 = 0.4657883048057556 + 2.0 * 6.039072513580322
Epoch 580, val loss: 0.786609411239624
Epoch 590, training loss: 12.522926330566406 = 0.448404997587204 + 2.0 * 6.03726053237915
Epoch 590, val loss: 0.777621328830719
Epoch 600, training loss: 12.51705551147461 = 0.431341290473938 + 2.0 * 6.0428571701049805
Epoch 600, val loss: 0.7691636681556702
Epoch 610, training loss: 12.486260414123535 = 0.4147483706474304 + 2.0 * 6.0357561111450195
Epoch 610, val loss: 0.761202871799469
Epoch 620, training loss: 12.468215942382812 = 0.3984324336051941 + 2.0 * 6.034891605377197
Epoch 620, val loss: 0.7537961006164551
Epoch 630, training loss: 12.449915885925293 = 0.38237228989601135 + 2.0 * 6.033771991729736
Epoch 630, val loss: 0.7468103766441345
Epoch 640, training loss: 12.431692123413086 = 0.3666783273220062 + 2.0 * 6.032506942749023
Epoch 640, val loss: 0.7402026653289795
Epoch 650, training loss: 12.41584300994873 = 0.35135719180107117 + 2.0 * 6.032242774963379
Epoch 650, val loss: 0.7341495752334595
Epoch 660, training loss: 12.400761604309082 = 0.33633649349212646 + 2.0 * 6.032212734222412
Epoch 660, val loss: 0.7285599708557129
Epoch 670, training loss: 12.382085800170898 = 0.3216770887374878 + 2.0 * 6.0302042961120605
Epoch 670, val loss: 0.7233729362487793
Epoch 680, training loss: 12.368380546569824 = 0.30743440985679626 + 2.0 * 6.030473232269287
Epoch 680, val loss: 0.7186954021453857
Epoch 690, training loss: 12.356098175048828 = 0.29356780648231506 + 2.0 * 6.0312652587890625
Epoch 690, val loss: 0.7145219445228577
Epoch 700, training loss: 12.33597469329834 = 0.2800864279270172 + 2.0 * 6.027944087982178
Epoch 700, val loss: 0.7108486294746399
Epoch 710, training loss: 12.326082229614258 = 0.267080157995224 + 2.0 * 6.029500961303711
Epoch 710, val loss: 0.7076215744018555
Epoch 720, training loss: 12.309680938720703 = 0.25457754731178284 + 2.0 * 6.027551651000977
Epoch 720, val loss: 0.7049674987792969
Epoch 730, training loss: 12.295440673828125 = 0.24256210029125214 + 2.0 * 6.026439189910889
Epoch 730, val loss: 0.7028934359550476
Epoch 740, training loss: 12.281196594238281 = 0.2310372143983841 + 2.0 * 6.025079727172852
Epoch 740, val loss: 0.7012677788734436
Epoch 750, training loss: 12.276413917541504 = 0.22001636028289795 + 2.0 * 6.028198719024658
Epoch 750, val loss: 0.7001693844795227
Epoch 760, training loss: 12.26591968536377 = 0.2095586061477661 + 2.0 * 6.0281805992126465
Epoch 760, val loss: 0.6994895339012146
Epoch 770, training loss: 12.248807907104492 = 0.19971738755702972 + 2.0 * 6.024545192718506
Epoch 770, val loss: 0.6994731426239014
Epoch 780, training loss: 12.234081268310547 = 0.1903689056634903 + 2.0 * 6.021856307983398
Epoch 780, val loss: 0.6999418139457703
Epoch 790, training loss: 12.229238510131836 = 0.18146783113479614 + 2.0 * 6.023885250091553
Epoch 790, val loss: 0.7007774114608765
Epoch 800, training loss: 12.214434623718262 = 0.1730354279279709 + 2.0 * 6.020699501037598
Epoch 800, val loss: 0.7019092440605164
Epoch 810, training loss: 12.211039543151855 = 0.16508585214614868 + 2.0 * 6.022976875305176
Epoch 810, val loss: 0.70353102684021
Epoch 820, training loss: 12.198020935058594 = 0.15756486356258392 + 2.0 * 6.020227909088135
Epoch 820, val loss: 0.7054671049118042
Epoch 830, training loss: 12.199811935424805 = 0.15045686066150665 + 2.0 * 6.024677753448486
Epoch 830, val loss: 0.707789957523346
Epoch 840, training loss: 12.184762954711914 = 0.14376501739025116 + 2.0 * 6.020498752593994
Epoch 840, val loss: 0.7103211283683777
Epoch 850, training loss: 12.173687934875488 = 0.13743776082992554 + 2.0 * 6.018125057220459
Epoch 850, val loss: 0.7131578922271729
Epoch 860, training loss: 12.167658805847168 = 0.13143745064735413 + 2.0 * 6.018110752105713
Epoch 860, val loss: 0.7162616848945618
Epoch 870, training loss: 12.165081977844238 = 0.12576381862163544 + 2.0 * 6.019659042358398
Epoch 870, val loss: 0.7194709777832031
Epoch 880, training loss: 12.152490615844727 = 0.12043368071317673 + 2.0 * 6.01602840423584
Epoch 880, val loss: 0.7229325175285339
Epoch 890, training loss: 12.14527416229248 = 0.11536788195371628 + 2.0 * 6.014953136444092
Epoch 890, val loss: 0.7265854477882385
Epoch 900, training loss: 12.141070365905762 = 0.11055479943752289 + 2.0 * 6.015257835388184
Epoch 900, val loss: 0.7303628921508789
Epoch 910, training loss: 12.139047622680664 = 0.1060042530298233 + 2.0 * 6.016521453857422
Epoch 910, val loss: 0.7342240214347839
Epoch 920, training loss: 12.137147903442383 = 0.10170464962720871 + 2.0 * 6.017721652984619
Epoch 920, val loss: 0.7381988763809204
Epoch 930, training loss: 12.126507759094238 = 0.09766406565904617 + 2.0 * 6.0144219398498535
Epoch 930, val loss: 0.7423396706581116
Epoch 940, training loss: 12.117341041564941 = 0.09382036328315735 + 2.0 * 6.011760234832764
Epoch 940, val loss: 0.7465884685516357
Epoch 950, training loss: 12.11301326751709 = 0.09015477448701859 + 2.0 * 6.011429309844971
Epoch 950, val loss: 0.750877320766449
Epoch 960, training loss: 12.123632431030273 = 0.08667195588350296 + 2.0 * 6.01848030090332
Epoch 960, val loss: 0.755157470703125
Epoch 970, training loss: 12.10751724243164 = 0.0833808109164238 + 2.0 * 6.012068271636963
Epoch 970, val loss: 0.759479284286499
Epoch 980, training loss: 12.103730201721191 = 0.08025184273719788 + 2.0 * 6.011739253997803
Epoch 980, val loss: 0.7639967203140259
Epoch 990, training loss: 12.097528457641602 = 0.07727942615747452 + 2.0 * 6.010124683380127
Epoch 990, val loss: 0.7685512900352478
Epoch 1000, training loss: 12.096834182739258 = 0.07444313913583755 + 2.0 * 6.011195659637451
Epoch 1000, val loss: 0.7731260061264038
Epoch 1010, training loss: 12.08914852142334 = 0.07173405587673187 + 2.0 * 6.008707046508789
Epoch 1010, val loss: 0.7776260375976562
Epoch 1020, training loss: 12.09090805053711 = 0.06915699690580368 + 2.0 * 6.010875701904297
Epoch 1020, val loss: 0.7822915315628052
Epoch 1030, training loss: 12.087400436401367 = 0.06671817600727081 + 2.0 * 6.010341167449951
Epoch 1030, val loss: 0.7868375778198242
Epoch 1040, training loss: 12.082488059997559 = 0.06441491842269897 + 2.0 * 6.009036540985107
Epoch 1040, val loss: 0.7914587259292603
Epoch 1050, training loss: 12.074986457824707 = 0.06220795214176178 + 2.0 * 6.006389141082764
Epoch 1050, val loss: 0.7961921691894531
Epoch 1060, training loss: 12.072850227355957 = 0.06008896604180336 + 2.0 * 6.006380558013916
Epoch 1060, val loss: 0.8007938265800476
Epoch 1070, training loss: 12.084466934204102 = 0.05807057023048401 + 2.0 * 6.013198375701904
Epoch 1070, val loss: 0.8052563071250916
Epoch 1080, training loss: 12.066025733947754 = 0.05614988133311272 + 2.0 * 6.004938125610352
Epoch 1080, val loss: 0.8097894787788391
Epoch 1090, training loss: 12.063750267028809 = 0.054315898567438126 + 2.0 * 6.0047173500061035
Epoch 1090, val loss: 0.8145448565483093
Epoch 1100, training loss: 12.063876152038574 = 0.05255056545138359 + 2.0 * 6.00566291809082
Epoch 1100, val loss: 0.8191663026809692
Epoch 1110, training loss: 12.062152862548828 = 0.05086652562022209 + 2.0 * 6.005643367767334
Epoch 1110, val loss: 0.8235743641853333
Epoch 1120, training loss: 12.055716514587402 = 0.04926416277885437 + 2.0 * 6.003226280212402
Epoch 1120, val loss: 0.8281862139701843
Epoch 1130, training loss: 12.05435848236084 = 0.047723881900310516 + 2.0 * 6.003317356109619
Epoch 1130, val loss: 0.8328220844268799
Epoch 1140, training loss: 12.065577507019043 = 0.04624202102422714 + 2.0 * 6.009667873382568
Epoch 1140, val loss: 0.8373212218284607
Epoch 1150, training loss: 12.053403854370117 = 0.044842321425676346 + 2.0 * 6.0042805671691895
Epoch 1150, val loss: 0.8419029712677002
Epoch 1160, training loss: 12.04629898071289 = 0.0434923991560936 + 2.0 * 6.001403331756592
Epoch 1160, val loss: 0.8465064764022827
Epoch 1170, training loss: 12.046056747436523 = 0.042194806039333344 + 2.0 * 6.001931190490723
Epoch 1170, val loss: 0.851020336151123
Epoch 1180, training loss: 12.055448532104492 = 0.04095401614904404 + 2.0 * 6.007247447967529
Epoch 1180, val loss: 0.8554801940917969
Epoch 1190, training loss: 12.045823097229004 = 0.039767440408468246 + 2.0 * 6.00302791595459
Epoch 1190, val loss: 0.8598567247390747
Epoch 1200, training loss: 12.038907051086426 = 0.0386311374604702 + 2.0 * 6.000137805938721
Epoch 1200, val loss: 0.8643795847892761
Epoch 1210, training loss: 12.04869270324707 = 0.03753885626792908 + 2.0 * 6.005577087402344
Epoch 1210, val loss: 0.8688414692878723
Epoch 1220, training loss: 12.038919448852539 = 0.036495596170425415 + 2.0 * 6.001212120056152
Epoch 1220, val loss: 0.8730390667915344
Epoch 1230, training loss: 12.03422737121582 = 0.03549095615744591 + 2.0 * 5.999368190765381
Epoch 1230, val loss: 0.8774524331092834
Epoch 1240, training loss: 12.033425331115723 = 0.03452245891094208 + 2.0 * 5.999451637268066
Epoch 1240, val loss: 0.8818036317825317
Epoch 1250, training loss: 12.041759490966797 = 0.03359576314687729 + 2.0 * 6.004081726074219
Epoch 1250, val loss: 0.8858931064605713
Epoch 1260, training loss: 12.029071807861328 = 0.03270638734102249 + 2.0 * 5.998182773590088
Epoch 1260, val loss: 0.8901544213294983
Epoch 1270, training loss: 12.02805233001709 = 0.03184949979186058 + 2.0 * 5.998101234436035
Epoch 1270, val loss: 0.8944054245948792
Epoch 1280, training loss: 12.029864311218262 = 0.031024418771266937 + 2.0 * 5.999420166015625
Epoch 1280, val loss: 0.8985787630081177
Epoch 1290, training loss: 12.027544021606445 = 0.030230844393372536 + 2.0 * 5.998656749725342
Epoch 1290, val loss: 0.902561604976654
Epoch 1300, training loss: 12.024211883544922 = 0.029470866546034813 + 2.0 * 5.997370719909668
Epoch 1300, val loss: 0.9065428376197815
Epoch 1310, training loss: 12.02229118347168 = 0.028738338500261307 + 2.0 * 5.996776580810547
Epoch 1310, val loss: 0.9106541872024536
Epoch 1320, training loss: 12.020683288574219 = 0.028025345876812935 + 2.0 * 5.996328830718994
Epoch 1320, val loss: 0.9145998954772949
Epoch 1330, training loss: 12.02858829498291 = 0.02733665704727173 + 2.0 * 6.0006256103515625
Epoch 1330, val loss: 0.918499231338501
Epoch 1340, training loss: 12.036612510681152 = 0.026680167764425278 + 2.0 * 6.0049662590026855
Epoch 1340, val loss: 0.9223455190658569
Epoch 1350, training loss: 12.022281646728516 = 0.02605140581727028 + 2.0 * 5.998115062713623
Epoch 1350, val loss: 0.9260305762290955
Epoch 1360, training loss: 12.015120506286621 = 0.025441763922572136 + 2.0 * 5.994839191436768
Epoch 1360, val loss: 0.9299945831298828
Epoch 1370, training loss: 12.014117240905762 = 0.0248496662825346 + 2.0 * 5.994633674621582
Epoch 1370, val loss: 0.9337905049324036
Epoch 1380, training loss: 12.020903587341309 = 0.024274110794067383 + 2.0 * 5.99831485748291
Epoch 1380, val loss: 0.9374849200248718
Epoch 1390, training loss: 12.014777183532715 = 0.023723307996988297 + 2.0 * 5.9955267906188965
Epoch 1390, val loss: 0.9411110281944275
Epoch 1400, training loss: 12.013127326965332 = 0.02319291979074478 + 2.0 * 5.994966983795166
Epoch 1400, val loss: 0.9448269605636597
Epoch 1410, training loss: 12.009369850158691 = 0.022678080946207047 + 2.0 * 5.993345737457275
Epoch 1410, val loss: 0.9485217928886414
Epoch 1420, training loss: 12.009618759155273 = 0.02217782288789749 + 2.0 * 5.993720531463623
Epoch 1420, val loss: 0.9521706104278564
Epoch 1430, training loss: 12.01333999633789 = 0.021693145856261253 + 2.0 * 5.995823383331299
Epoch 1430, val loss: 0.9557288289070129
Epoch 1440, training loss: 12.01815128326416 = 0.021226616576313972 + 2.0 * 5.998462200164795
Epoch 1440, val loss: 0.9591867923736572
Epoch 1450, training loss: 12.007286071777344 = 0.02077355794608593 + 2.0 * 5.993256092071533
Epoch 1450, val loss: 0.9626588225364685
Epoch 1460, training loss: 12.00723934173584 = 0.02033878117799759 + 2.0 * 5.993450164794922
Epoch 1460, val loss: 0.9662119746208191
Epoch 1470, training loss: 12.009747505187988 = 0.019914809614419937 + 2.0 * 5.9949164390563965
Epoch 1470, val loss: 0.9695872068405151
Epoch 1480, training loss: 12.003664016723633 = 0.019507242366671562 + 2.0 * 5.9920783042907715
Epoch 1480, val loss: 0.9729644656181335
Epoch 1490, training loss: 12.00363826751709 = 0.0191087257117033 + 2.0 * 5.992264747619629
Epoch 1490, val loss: 0.9763821959495544
Epoch 1500, training loss: 12.012555122375488 = 0.018722519278526306 + 2.0 * 5.9969162940979
Epoch 1500, val loss: 0.9795507192611694
Epoch 1510, training loss: 12.001496315002441 = 0.01834946684539318 + 2.0 * 5.991573333740234
Epoch 1510, val loss: 0.9827499985694885
Epoch 1520, training loss: 11.999309539794922 = 0.01799037866294384 + 2.0 * 5.990659713745117
Epoch 1520, val loss: 0.9861162900924683
Epoch 1530, training loss: 12.006182670593262 = 0.017639117315411568 + 2.0 * 5.994271755218506
Epoch 1530, val loss: 0.9892980456352234
Epoch 1540, training loss: 11.996800422668457 = 0.01729709468781948 + 2.0 * 5.989751815795898
Epoch 1540, val loss: 0.9924212694168091
Epoch 1550, training loss: 11.997732162475586 = 0.0169654693454504 + 2.0 * 5.990383148193359
Epoch 1550, val loss: 0.9956498742103577
Epoch 1560, training loss: 12.018806457519531 = 0.01664026826620102 + 2.0 * 6.001082897186279
Epoch 1560, val loss: 0.9987245798110962
Epoch 1570, training loss: 12.004880905151367 = 0.016334719955921173 + 2.0 * 5.9942731857299805
Epoch 1570, val loss: 1.001686930656433
Epoch 1580, training loss: 11.9957914352417 = 0.01603309065103531 + 2.0 * 5.989879131317139
Epoch 1580, val loss: 1.0048285722732544
Epoch 1590, training loss: 11.993239402770996 = 0.01573978178203106 + 2.0 * 5.988749980926514
Epoch 1590, val loss: 1.0079439878463745
Epoch 1600, training loss: 12.000450134277344 = 0.015451439656317234 + 2.0 * 5.992499351501465
Epoch 1600, val loss: 1.0109469890594482
Epoch 1610, training loss: 11.993731498718262 = 0.015172312036156654 + 2.0 * 5.989279747009277
Epoch 1610, val loss: 1.0138849020004272
Epoch 1620, training loss: 11.999929428100586 = 0.014903552830219269 + 2.0 * 5.9925127029418945
Epoch 1620, val loss: 1.0168856382369995
Epoch 1630, training loss: 11.996977806091309 = 0.014642513357102871 + 2.0 * 5.9911675453186035
Epoch 1630, val loss: 1.0196837186813354
Epoch 1640, training loss: 11.990869522094727 = 0.014387880451977253 + 2.0 * 5.988240718841553
Epoch 1640, val loss: 1.0226671695709229
Epoch 1650, training loss: 11.990979194641113 = 0.01413876935839653 + 2.0 * 5.988420009613037
Epoch 1650, val loss: 1.0256292819976807
Epoch 1660, training loss: 11.996438980102539 = 0.013895506039261818 + 2.0 * 5.99127197265625
Epoch 1660, val loss: 1.0284295082092285
Epoch 1670, training loss: 11.991074562072754 = 0.013657458126544952 + 2.0 * 5.98870849609375
Epoch 1670, val loss: 1.031163215637207
Epoch 1680, training loss: 11.991842269897461 = 0.013426593504846096 + 2.0 * 5.989207744598389
Epoch 1680, val loss: 1.033961296081543
Epoch 1690, training loss: 11.988456726074219 = 0.013203335925936699 + 2.0 * 5.987626552581787
Epoch 1690, val loss: 1.0367823839187622
Epoch 1700, training loss: 11.991689682006836 = 0.012984339147806168 + 2.0 * 5.989352703094482
Epoch 1700, val loss: 1.0395170450210571
Epoch 1710, training loss: 11.987899780273438 = 0.012772739864885807 + 2.0 * 5.987563610076904
Epoch 1710, val loss: 1.042064905166626
Epoch 1720, training loss: 11.984743118286133 = 0.012566812336444855 + 2.0 * 5.986088275909424
Epoch 1720, val loss: 1.044814944267273
Epoch 1730, training loss: 11.983906745910645 = 0.012365583330392838 + 2.0 * 5.9857707023620605
Epoch 1730, val loss: 1.047584891319275
Epoch 1740, training loss: 11.985156059265137 = 0.012165750376880169 + 2.0 * 5.986495018005371
Epoch 1740, val loss: 1.0502275228500366
Epoch 1750, training loss: 11.994571685791016 = 0.011971845291554928 + 2.0 * 5.991300106048584
Epoch 1750, val loss: 1.052703857421875
Epoch 1760, training loss: 11.986077308654785 = 0.011786089278757572 + 2.0 * 5.98714542388916
Epoch 1760, val loss: 1.0553661584854126
Epoch 1770, training loss: 11.982465744018555 = 0.011601824313402176 + 2.0 * 5.985432147979736
Epoch 1770, val loss: 1.058043360710144
Epoch 1780, training loss: 11.983132362365723 = 0.011420832015573978 + 2.0 * 5.985855579376221
Epoch 1780, val loss: 1.0606740713119507
Epoch 1790, training loss: 12.000983238220215 = 0.01124559435993433 + 2.0 * 5.994868755340576
Epoch 1790, val loss: 1.0631048679351807
Epoch 1800, training loss: 11.985154151916504 = 0.011078006587922573 + 2.0 * 5.9870381355285645
Epoch 1800, val loss: 1.0653647184371948
Epoch 1810, training loss: 11.980184555053711 = 0.010914802551269531 + 2.0 * 5.984634876251221
Epoch 1810, val loss: 1.0680348873138428
Epoch 1820, training loss: 11.98040771484375 = 0.010752384550869465 + 2.0 * 5.984827518463135
Epoch 1820, val loss: 1.0705903768539429
Epoch 1830, training loss: 11.980737686157227 = 0.010592670179903507 + 2.0 * 5.985072612762451
Epoch 1830, val loss: 1.0730451345443726
Epoch 1840, training loss: 11.987929344177246 = 0.010436794720590115 + 2.0 * 5.988746166229248
Epoch 1840, val loss: 1.075392723083496
Epoch 1850, training loss: 11.983016967773438 = 0.010283904150128365 + 2.0 * 5.9863667488098145
Epoch 1850, val loss: 1.0778371095657349
Epoch 1860, training loss: 11.979949951171875 = 0.010136466473340988 + 2.0 * 5.9849066734313965
Epoch 1860, val loss: 1.0803256034851074
Epoch 1870, training loss: 11.981025695800781 = 0.00999096967279911 + 2.0 * 5.985517501831055
Epoch 1870, val loss: 1.0826083421707153
Epoch 1880, training loss: 11.977598190307617 = 0.009849024005234241 + 2.0 * 5.983874797821045
Epoch 1880, val loss: 1.0849359035491943
Epoch 1890, training loss: 11.98741340637207 = 0.00971181970089674 + 2.0 * 5.9888505935668945
Epoch 1890, val loss: 1.0873422622680664
Epoch 1900, training loss: 11.97758674621582 = 0.009575370699167252 + 2.0 * 5.984005451202393
Epoch 1900, val loss: 1.089553713798523
Epoch 1910, training loss: 11.975287437438965 = 0.009443649090826511 + 2.0 * 5.982922077178955
Epoch 1910, val loss: 1.0919418334960938
Epoch 1920, training loss: 11.978672981262207 = 0.009313134476542473 + 2.0 * 5.984679698944092
Epoch 1920, val loss: 1.0942214727401733
Epoch 1930, training loss: 11.977790832519531 = 0.009185726754367352 + 2.0 * 5.984302520751953
Epoch 1930, val loss: 1.0964049100875854
Epoch 1940, training loss: 11.974618911743164 = 0.009062223136425018 + 2.0 * 5.982778549194336
Epoch 1940, val loss: 1.098732590675354
Epoch 1950, training loss: 11.975570678710938 = 0.0089403185993433 + 2.0 * 5.9833149909973145
Epoch 1950, val loss: 1.1009851694107056
Epoch 1960, training loss: 11.978633880615234 = 0.008820867165923119 + 2.0 * 5.9849066734313965
Epoch 1960, val loss: 1.1032568216323853
Epoch 1970, training loss: 11.976920127868652 = 0.008703728206455708 + 2.0 * 5.984107971191406
Epoch 1970, val loss: 1.1053764820098877
Epoch 1980, training loss: 11.972623825073242 = 0.008590060286223888 + 2.0 * 5.9820170402526855
Epoch 1980, val loss: 1.1075258255004883
Epoch 1990, training loss: 11.973587036132812 = 0.008478493429720402 + 2.0 * 5.9825544357299805
Epoch 1990, val loss: 1.1097638607025146
=== picking the best model according to the performance on validation ===
target class rate on Vs: 0.8000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.7343
Flip ASR: 0.6889/225 nodes
The final ASR:0.65191, 0.06780, Accuracy:0.81358, 0.01259
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11678])
remove edge: torch.Size([2, 9500])
updated graph: torch.Size([2, 10622])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9926
Flip ASR: 0.9911/225 nodes
The final ASR:0.98278, 0.00758, Accuracy:0.83210, 0.00761
Begin epxeriment: cont_weight: 2 epoch:2000 der1:0.3 der2:0.3 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 18.7058162689209 = 1.9582045078277588 + 2.0 * 8.37380599975586
Epoch 0, val loss: 1.9571152925491333
Epoch 10, training loss: 18.694032669067383 = 1.9479405879974365 + 2.0 * 8.373045921325684
Epoch 10, val loss: 1.9475443363189697
Epoch 20, training loss: 18.66994285583496 = 1.9352381229400635 + 2.0 * 8.367352485656738
Epoch 20, val loss: 1.9354099035263062
Epoch 30, training loss: 18.56988525390625 = 1.918407678604126 + 2.0 * 8.325738906860352
Epoch 30, val loss: 1.919210433959961
Epoch 40, training loss: 18.015625 = 1.8987518548965454 + 2.0 * 8.058436393737793
Epoch 40, val loss: 1.9004801511764526
Epoch 50, training loss: 17.111967086791992 = 1.8782310485839844 + 2.0 * 7.616868019104004
Epoch 50, val loss: 1.8819639682769775
Epoch 60, training loss: 16.275630950927734 = 1.865009069442749 + 2.0 * 7.205311298370361
Epoch 60, val loss: 1.8703124523162842
Epoch 70, training loss: 15.432546615600586 = 1.8558566570281982 + 2.0 * 6.788344860076904
Epoch 70, val loss: 1.8619695901870728
Epoch 80, training loss: 14.979730606079102 = 1.848525047302246 + 2.0 * 6.565602779388428
Epoch 80, val loss: 1.8546342849731445
Epoch 90, training loss: 14.764812469482422 = 1.839574933052063 + 2.0 * 6.462618827819824
Epoch 90, val loss: 1.8457911014556885
Epoch 100, training loss: 14.62934684753418 = 1.829613447189331 + 2.0 * 6.399866580963135
Epoch 100, val loss: 1.836330533027649
Epoch 110, training loss: 14.5184965133667 = 1.820687174797058 + 2.0 * 6.348904609680176
Epoch 110, val loss: 1.8278812170028687
Epoch 120, training loss: 14.434577941894531 = 1.8136239051818848 + 2.0 * 6.310477256774902
Epoch 120, val loss: 1.8208825588226318
Epoch 130, training loss: 14.36721420288086 = 1.8075909614562988 + 2.0 * 6.279811382293701
Epoch 130, val loss: 1.8146588802337646
Epoch 140, training loss: 14.315044403076172 = 1.8019262552261353 + 2.0 * 6.256558895111084
Epoch 140, val loss: 1.808705449104309
Epoch 150, training loss: 14.270757675170898 = 1.7963300943374634 + 2.0 * 6.237213611602783
Epoch 150, val loss: 1.8029999732971191
Epoch 160, training loss: 14.23128890991211 = 1.7906357049942017 + 2.0 * 6.2203264236450195
Epoch 160, val loss: 1.7974307537078857
Epoch 170, training loss: 14.196465492248535 = 1.784680724143982 + 2.0 * 6.205892562866211
Epoch 170, val loss: 1.7918782234191895
Epoch 180, training loss: 14.164024353027344 = 1.7783031463623047 + 2.0 * 6.1928606033325195
Epoch 180, val loss: 1.786234974861145
Epoch 190, training loss: 14.133973121643066 = 1.771312952041626 + 2.0 * 6.18133020401001
Epoch 190, val loss: 1.7803106307983398
Epoch 200, training loss: 14.11014461517334 = 1.7635263204574585 + 2.0 * 6.173309326171875
Epoch 200, val loss: 1.7738990783691406
Epoch 210, training loss: 14.077082633972168 = 1.754833459854126 + 2.0 * 6.1611247062683105
Epoch 210, val loss: 1.7669404745101929
Epoch 220, training loss: 14.052207946777344 = 1.7450169324874878 + 2.0 * 6.153595447540283
Epoch 220, val loss: 1.7591825723648071
Epoch 230, training loss: 14.026220321655273 = 1.7338032722473145 + 2.0 * 6.1462082862854
Epoch 230, val loss: 1.7504137754440308
Epoch 240, training loss: 13.997191429138184 = 1.7211207151412964 + 2.0 * 6.138035297393799
Epoch 240, val loss: 1.7405734062194824
Epoch 250, training loss: 13.970479011535645 = 1.7066282033920288 + 2.0 * 6.131925582885742
Epoch 250, val loss: 1.72938871383667
Epoch 260, training loss: 13.941646575927734 = 1.6898926496505737 + 2.0 * 6.1258769035339355
Epoch 260, val loss: 1.7164640426635742
Epoch 270, training loss: 13.913540840148926 = 1.6705007553100586 + 2.0 * 6.121520042419434
Epoch 270, val loss: 1.7014521360397339
Epoch 280, training loss: 13.884828567504883 = 1.6481444835662842 + 2.0 * 6.11834192276001
Epoch 280, val loss: 1.684155821800232
Epoch 290, training loss: 13.846750259399414 = 1.6227749586105347 + 2.0 * 6.111987590789795
Epoch 290, val loss: 1.6643747091293335
Epoch 300, training loss: 13.810471534729004 = 1.5938727855682373 + 2.0 * 6.108299255371094
Epoch 300, val loss: 1.6417561769485474
Epoch 310, training loss: 13.772787094116211 = 1.561328411102295 + 2.0 * 6.105729579925537
Epoch 310, val loss: 1.6161085367202759
Epoch 320, training loss: 13.729519844055176 = 1.5254576206207275 + 2.0 * 6.102031230926514
Epoch 320, val loss: 1.587904691696167
Epoch 330, training loss: 13.683463096618652 = 1.4865490198135376 + 2.0 * 6.098456859588623
Epoch 330, val loss: 1.5571324825286865
Epoch 340, training loss: 13.64184284210205 = 1.4450557231903076 + 2.0 * 6.098393440246582
Epoch 340, val loss: 1.5243616104125977
Epoch 350, training loss: 13.59155559539795 = 1.402384638786316 + 2.0 * 6.094585418701172
Epoch 350, val loss: 1.4906184673309326
Epoch 360, training loss: 13.541353225708008 = 1.3588464260101318 + 2.0 * 6.091253280639648
Epoch 360, val loss: 1.4564225673675537
Epoch 370, training loss: 13.499598503112793 = 1.3151781558990479 + 2.0 * 6.092210292816162
Epoch 370, val loss: 1.4223206043243408
Epoch 380, training loss: 13.447291374206543 = 1.2724609375 + 2.0 * 6.0874152183532715
Epoch 380, val loss: 1.3890767097473145
Epoch 390, training loss: 13.39913558959961 = 1.2309291362762451 + 2.0 * 6.084103107452393
Epoch 390, val loss: 1.3570283651351929
Epoch 400, training loss: 13.357057571411133 = 1.1908118724822998 + 2.0 * 6.083122730255127
Epoch 400, val loss: 1.3261579275131226
Epoch 410, training loss: 13.312418937683105 = 1.1525293588638306 + 2.0 * 6.079944610595703
Epoch 410, val loss: 1.2969000339508057
Epoch 420, training loss: 13.270231246948242 = 1.1159231662750244 + 2.0 * 6.077154159545898
Epoch 420, val loss: 1.2689988613128662
Epoch 430, training loss: 13.241463661193848 = 1.0810989141464233 + 2.0 * 6.0801825523376465
Epoch 430, val loss: 1.2427759170532227
Epoch 440, training loss: 13.201013565063477 = 1.0484801530838013 + 2.0 * 6.076266765594482
Epoch 440, val loss: 1.2183772325515747
Epoch 450, training loss: 13.162918090820312 = 1.0178557634353638 + 2.0 * 6.072531223297119
Epoch 450, val loss: 1.1956583261489868
Epoch 460, training loss: 13.126262664794922 = 0.9888525009155273 + 2.0 * 6.068705081939697
Epoch 460, val loss: 1.1745747327804565
Epoch 470, training loss: 13.096277236938477 = 0.9612613320350647 + 2.0 * 6.067507743835449
Epoch 470, val loss: 1.1547623872756958
Epoch 480, training loss: 13.076400756835938 = 0.9350385665893555 + 2.0 * 6.070681095123291
Epoch 480, val loss: 1.1359785795211792
Epoch 490, training loss: 13.039756774902344 = 0.9101175665855408 + 2.0 * 6.064819812774658
Epoch 490, val loss: 1.118698000907898
Epoch 500, training loss: 13.009455680847168 = 0.8862101435661316 + 2.0 * 6.061622619628906
Epoch 500, val loss: 1.1022993326187134
Epoch 510, training loss: 12.985846519470215 = 0.8630897998809814 + 2.0 * 6.061378479003906
Epoch 510, val loss: 1.08655846118927
Epoch 520, training loss: 12.959809303283691 = 0.8407223224639893 + 2.0 * 6.059543609619141
Epoch 520, val loss: 1.071599006652832
Epoch 530, training loss: 12.937051773071289 = 0.8192785382270813 + 2.0 * 6.058886528015137
Epoch 530, val loss: 1.0575029850006104
Epoch 540, training loss: 12.913139343261719 = 0.7985816597938538 + 2.0 * 6.057278633117676
Epoch 540, val loss: 1.044127106666565
Epoch 550, training loss: 12.892093658447266 = 0.7786406874656677 + 2.0 * 6.056726455688477
Epoch 550, val loss: 1.0314044952392578
Epoch 560, training loss: 12.87128734588623 = 0.7594614028930664 + 2.0 * 6.055912971496582
Epoch 560, val loss: 1.0195224285125732
Epoch 570, training loss: 12.847227096557617 = 0.7409189343452454 + 2.0 * 6.053153991699219
Epoch 570, val loss: 1.0084469318389893
Epoch 580, training loss: 12.82547378540039 = 0.7230303287506104 + 2.0 * 6.05122184753418
Epoch 580, val loss: 0.9978711009025574
Epoch 590, training loss: 12.81731128692627 = 0.7055854201316833 + 2.0 * 6.055862903594971
Epoch 590, val loss: 0.9878228306770325
Epoch 600, training loss: 12.789331436157227 = 0.688637375831604 + 2.0 * 6.050346851348877
Epoch 600, val loss: 0.9784229397773743
Epoch 610, training loss: 12.770883560180664 = 0.6720951795578003 + 2.0 * 6.049394130706787
Epoch 610, val loss: 0.9694918990135193
Epoch 620, training loss: 12.753007888793945 = 0.6558231115341187 + 2.0 * 6.048592567443848
Epoch 620, val loss: 0.9607637524604797
Epoch 630, training loss: 12.73318862915039 = 0.6398301720619202 + 2.0 * 6.0466790199279785
Epoch 630, val loss: 0.9524276256561279
Epoch 640, training loss: 12.715076446533203 = 0.623858630657196 + 2.0 * 6.045608997344971
Epoch 640, val loss: 0.9441702961921692
Epoch 650, training loss: 12.698349952697754 = 0.6079675555229187 + 2.0 * 6.045191287994385
Epoch 650, val loss: 0.936162531375885
Epoch 660, training loss: 12.686623573303223 = 0.5921295881271362 + 2.0 * 6.047246932983398
Epoch 660, val loss: 0.9281451106071472
Epoch 670, training loss: 12.66286563873291 = 0.5761959552764893 + 2.0 * 6.0433349609375
Epoch 670, val loss: 0.9202901721000671
Epoch 680, training loss: 12.644071578979492 = 0.5603057742118835 + 2.0 * 6.0418829917907715
Epoch 680, val loss: 0.9124573469161987
Epoch 690, training loss: 12.63744068145752 = 0.5443199276924133 + 2.0 * 6.046560287475586
Epoch 690, val loss: 0.9043987989425659
Epoch 700, training loss: 12.60614013671875 = 0.528475284576416 + 2.0 * 6.038832664489746
Epoch 700, val loss: 0.8967421650886536
Epoch 710, training loss: 12.590303421020508 = 0.5125843286514282 + 2.0 * 6.0388593673706055
Epoch 710, val loss: 0.8890137076377869
Epoch 720, training loss: 12.572213172912598 = 0.4966289699077606 + 2.0 * 6.037792205810547
Epoch 720, val loss: 0.8813422322273254
Epoch 730, training loss: 12.571052551269531 = 0.4807036817073822 + 2.0 * 6.045174598693848
Epoch 730, val loss: 0.8736813068389893
Epoch 740, training loss: 12.542755126953125 = 0.4650023877620697 + 2.0 * 6.038876533508301
Epoch 740, val loss: 0.8664172887802124
Epoch 750, training loss: 12.520160675048828 = 0.44943767786026 + 2.0 * 6.035361289978027
Epoch 750, val loss: 0.8594484925270081
Epoch 760, training loss: 12.504549026489258 = 0.4338855743408203 + 2.0 * 6.035331726074219
Epoch 760, val loss: 0.8525123000144958
Epoch 770, training loss: 12.49974536895752 = 0.4183369576931 + 2.0 * 6.040704250335693
Epoch 770, val loss: 0.845710039138794
Epoch 780, training loss: 12.470459938049316 = 0.4030049741268158 + 2.0 * 6.033727645874023
Epoch 780, val loss: 0.8393739461898804
Epoch 790, training loss: 12.453871726989746 = 0.38766613602638245 + 2.0 * 6.033102989196777
Epoch 790, val loss: 0.8333702683448792
Epoch 800, training loss: 12.436604499816895 = 0.3722900450229645 + 2.0 * 6.0321574211120605
Epoch 800, val loss: 0.8273839354515076
Epoch 810, training loss: 12.434760093688965 = 0.3568797707557678 + 2.0 * 6.038939952850342
Epoch 810, val loss: 0.8216270804405212
Epoch 820, training loss: 12.41366195678711 = 0.34152865409851074 + 2.0 * 6.03606653213501
Epoch 820, val loss: 0.8161880373954773
Epoch 830, training loss: 12.389076232910156 = 0.3262564539909363 + 2.0 * 6.031409740447998
Epoch 830, val loss: 0.8110266327857971
Epoch 840, training loss: 12.3702392578125 = 0.3111378848552704 + 2.0 * 6.029550552368164
Epoch 840, val loss: 0.8064393401145935
Epoch 850, training loss: 12.356996536254883 = 0.2961975932121277 + 2.0 * 6.030399322509766
Epoch 850, val loss: 0.8020168542861938
Epoch 860, training loss: 12.346381187438965 = 0.2814713418483734 + 2.0 * 6.032454967498779
Epoch 860, val loss: 0.7977893948554993
Epoch 870, training loss: 12.322774887084961 = 0.2672460973262787 + 2.0 * 6.027764320373535
Epoch 870, val loss: 0.7942240834236145
Epoch 880, training loss: 12.309370994567871 = 0.2534646987915039 + 2.0 * 6.027953147888184
Epoch 880, val loss: 0.7910728454589844
Epoch 890, training loss: 12.293147087097168 = 0.24016593396663666 + 2.0 * 6.026490688323975
Epoch 890, val loss: 0.7883086204528809
Epoch 900, training loss: 12.283666610717773 = 0.22741791605949402 + 2.0 * 6.0281243324279785
Epoch 900, val loss: 0.7858923077583313
Epoch 910, training loss: 12.280348777770996 = 0.21539805829524994 + 2.0 * 6.032475471496582
Epoch 910, val loss: 0.7834159135818481
Epoch 920, training loss: 12.259272575378418 = 0.2041393518447876 + 2.0 * 6.027566432952881
Epoch 920, val loss: 0.7819926142692566
Epoch 930, training loss: 12.244154930114746 = 0.19361208379268646 + 2.0 * 6.025271415710449
Epoch 930, val loss: 0.7809171080589294
Epoch 940, training loss: 12.232057571411133 = 0.18368829786777496 + 2.0 * 6.024184703826904
Epoch 940, val loss: 0.7800421118736267
Epoch 950, training loss: 12.22847843170166 = 0.17434857785701752 + 2.0 * 6.027064800262451
Epoch 950, val loss: 0.7794435620307922
Epoch 960, training loss: 12.22578239440918 = 0.16573984920978546 + 2.0 * 6.0300211906433105
Epoch 960, val loss: 0.7789009809494019
Epoch 970, training loss: 12.201894760131836 = 0.1577109694480896 + 2.0 * 6.022091865539551
Epoch 970, val loss: 0.7792701125144958
Epoch 980, training loss: 12.194539070129395 = 0.15021023154258728 + 2.0 * 6.022164344787598
Epoch 980, val loss: 0.7797325253486633
Epoch 990, training loss: 12.18529224395752 = 0.14316360652446747 + 2.0 * 6.021064281463623
Epoch 990, val loss: 0.7803896069526672
Epoch 1000, training loss: 12.177597045898438 = 0.13654999434947968 + 2.0 * 6.020523548126221
Epoch 1000, val loss: 0.7813860177993774
Epoch 1010, training loss: 12.192091941833496 = 0.13033834099769592 + 2.0 * 6.030876636505127
Epoch 1010, val loss: 0.7824141979217529
Epoch 1020, training loss: 12.164717674255371 = 0.12457120418548584 + 2.0 * 6.020073413848877
Epoch 1020, val loss: 0.7838827967643738
Epoch 1030, training loss: 12.162738800048828 = 0.11918630450963974 + 2.0 * 6.02177619934082
Epoch 1030, val loss: 0.7855967283248901
Epoch 1040, training loss: 12.152456283569336 = 0.11411900073289871 + 2.0 * 6.019168853759766
Epoch 1040, val loss: 0.7873595356941223
Epoch 1050, training loss: 12.148176193237305 = 0.10934623330831528 + 2.0 * 6.019414901733398
Epoch 1050, val loss: 0.7894783020019531
Epoch 1060, training loss: 12.156888008117676 = 0.10484106093645096 + 2.0 * 6.0260233879089355
Epoch 1060, val loss: 0.7914461493492126
Epoch 1070, training loss: 12.139267921447754 = 0.10065290331840515 + 2.0 * 6.019307613372803
Epoch 1070, val loss: 0.793886661529541
Epoch 1080, training loss: 12.129485130310059 = 0.09668847918510437 + 2.0 * 6.0163984298706055
Epoch 1080, val loss: 0.7964840531349182
Epoch 1090, training loss: 12.12509536743164 = 0.09293413162231445 + 2.0 * 6.016080379486084
Epoch 1090, val loss: 0.7990873456001282
Epoch 1100, training loss: 12.135651588439941 = 0.08938650041818619 + 2.0 * 6.02313232421875
Epoch 1100, val loss: 0.801774263381958
Epoch 1110, training loss: 12.124576568603516 = 0.08603041619062424 + 2.0 * 6.019273281097412
Epoch 1110, val loss: 0.8045284748077393
Epoch 1120, training loss: 12.11513614654541 = 0.08287149667739868 + 2.0 * 6.016132354736328
Epoch 1120, val loss: 0.8075440526008606
Epoch 1130, training loss: 12.107952117919922 = 0.07986226677894592 + 2.0 * 6.014044761657715
Epoch 1130, val loss: 0.810560941696167
Epoch 1140, training loss: 12.110563278198242 = 0.07700458914041519 + 2.0 * 6.01677942276001
Epoch 1140, val loss: 0.8136529326438904
Epoch 1150, training loss: 12.101569175720215 = 0.07428781688213348 + 2.0 * 6.013640880584717
Epoch 1150, val loss: 0.8165869116783142
Epoch 1160, training loss: 12.099403381347656 = 0.07171563804149628 + 2.0 * 6.013844013214111
Epoch 1160, val loss: 0.8199427723884583
Epoch 1170, training loss: 12.10575008392334 = 0.06926919519901276 + 2.0 * 6.018240451812744
Epoch 1170, val loss: 0.8231322765350342
Epoch 1180, training loss: 12.099284172058105 = 0.06693775206804276 + 2.0 * 6.016173362731934
Epoch 1180, val loss: 0.8262358903884888
Epoch 1190, training loss: 12.088695526123047 = 0.06473200023174286 + 2.0 * 6.011981964111328
Epoch 1190, val loss: 0.8296244740486145
Epoch 1200, training loss: 12.085393905639648 = 0.06262688338756561 + 2.0 * 6.011383533477783
Epoch 1200, val loss: 0.8330020308494568
Epoch 1210, training loss: 12.082036972045898 = 0.060603950172662735 + 2.0 * 6.010716438293457
Epoch 1210, val loss: 0.836380660533905
Epoch 1220, training loss: 12.084879875183105 = 0.05867023393511772 + 2.0 * 6.0131049156188965
Epoch 1220, val loss: 0.8396771550178528
Epoch 1230, training loss: 12.081719398498535 = 0.05682376027107239 + 2.0 * 6.012447834014893
Epoch 1230, val loss: 0.8429943919181824
Epoch 1240, training loss: 12.078032493591309 = 0.05507100746035576 + 2.0 * 6.011480808258057
Epoch 1240, val loss: 0.8463516235351562
Epoch 1250, training loss: 12.073476791381836 = 0.05340270325541496 + 2.0 * 6.010036945343018
Epoch 1250, val loss: 0.8500556945800781
Epoch 1260, training loss: 12.072614669799805 = 0.05179264023900032 + 2.0 * 6.010410785675049
Epoch 1260, val loss: 0.8533756136894226
Epoch 1270, training loss: 12.07081127166748 = 0.050246719270944595 + 2.0 * 6.010282039642334
Epoch 1270, val loss: 0.8567067384719849
Epoch 1280, training loss: 12.071817398071289 = 0.04877445101737976 + 2.0 * 6.011521339416504
Epoch 1280, val loss: 0.8600383996963501
Epoch 1290, training loss: 12.06460189819336 = 0.04736656695604324 + 2.0 * 6.008617877960205
Epoch 1290, val loss: 0.8634923696517944
Epoch 1300, training loss: 12.061176300048828 = 0.04601629823446274 + 2.0 * 6.007579803466797
Epoch 1300, val loss: 0.8670393228530884
Epoch 1310, training loss: 12.064451217651367 = 0.04471322521567345 + 2.0 * 6.00986909866333
Epoch 1310, val loss: 0.8703396916389465
Epoch 1320, training loss: 12.056416511535645 = 0.04345957562327385 + 2.0 * 6.006478309631348
Epoch 1320, val loss: 0.8737040758132935
Epoch 1330, training loss: 12.063553810119629 = 0.04225869104266167 + 2.0 * 6.010647773742676
Epoch 1330, val loss: 0.8770416975021362
Epoch 1340, training loss: 12.059212684631348 = 0.04111219942569733 + 2.0 * 6.009050369262695
Epoch 1340, val loss: 0.8804782629013062
Epoch 1350, training loss: 12.057162284851074 = 0.040006984025239944 + 2.0 * 6.008577823638916
Epoch 1350, val loss: 0.8837170004844666
Epoch 1360, training loss: 12.05137825012207 = 0.038950853049755096 + 2.0 * 6.006213665008545
Epoch 1360, val loss: 0.8872538208961487
Epoch 1370, training loss: 12.04721450805664 = 0.03792738541960716 + 2.0 * 6.004643440246582
Epoch 1370, val loss: 0.8906038403511047
Epoch 1380, training loss: 12.045252799987793 = 0.036937300115823746 + 2.0 * 6.004157543182373
Epoch 1380, val loss: 0.893949568271637
Epoch 1390, training loss: 12.0628080368042 = 0.03598051890730858 + 2.0 * 6.013413906097412
Epoch 1390, val loss: 0.8969568014144897
Epoch 1400, training loss: 12.046310424804688 = 0.03507434204220772 + 2.0 * 6.005618095397949
Epoch 1400, val loss: 0.900225043296814
Epoch 1410, training loss: 12.041194915771484 = 0.0342007540166378 + 2.0 * 6.003497123718262
Epoch 1410, val loss: 0.9037393927574158
Epoch 1420, training loss: 12.039032936096191 = 0.033352453261613846 + 2.0 * 6.002840042114258
Epoch 1420, val loss: 0.9069991707801819
Epoch 1430, training loss: 12.043033599853516 = 0.03253317251801491 + 2.0 * 6.005249977111816
Epoch 1430, val loss: 0.9102016687393188
Epoch 1440, training loss: 12.039205551147461 = 0.031743668019771576 + 2.0 * 6.003730773925781
Epoch 1440, val loss: 0.9131144881248474
Epoch 1450, training loss: 12.036182403564453 = 0.030986227095127106 + 2.0 * 6.002598285675049
Epoch 1450, val loss: 0.9164515137672424
Epoch 1460, training loss: 12.033984184265137 = 0.030255796387791634 + 2.0 * 6.001863956451416
Epoch 1460, val loss: 0.9197995066642761
Epoch 1470, training loss: 12.036324501037598 = 0.029546814039349556 + 2.0 * 6.00338888168335
Epoch 1470, val loss: 0.9228968024253845
Epoch 1480, training loss: 12.033373832702637 = 0.02885989472270012 + 2.0 * 6.002256870269775
Epoch 1480, val loss: 0.925990104675293
Epoch 1490, training loss: 12.037192344665527 = 0.028199028223752975 + 2.0 * 6.0044965744018555
Epoch 1490, val loss: 0.9291055798530579
Epoch 1500, training loss: 12.032859802246094 = 0.02756172977387905 + 2.0 * 6.002648830413818
Epoch 1500, val loss: 0.9321479797363281
Epoch 1510, training loss: 12.030488967895508 = 0.026947977021336555 + 2.0 * 6.001770496368408
Epoch 1510, val loss: 0.9352611303329468
Epoch 1520, training loss: 12.02737045288086 = 0.026354413479566574 + 2.0 * 6.000507831573486
Epoch 1520, val loss: 0.938382625579834
Epoch 1530, training loss: 12.028690338134766 = 0.02577965147793293 + 2.0 * 6.001455307006836
Epoch 1530, val loss: 0.9414147734642029
Epoch 1540, training loss: 12.026938438415527 = 0.025223100557923317 + 2.0 * 6.000857830047607
Epoch 1540, val loss: 0.944430410861969
Epoch 1550, training loss: 12.023032188415527 = 0.024682356044650078 + 2.0 * 5.999175071716309
Epoch 1550, val loss: 0.947441816329956
Epoch 1560, training loss: 12.027268409729004 = 0.024157637730240822 + 2.0 * 6.001555442810059
Epoch 1560, val loss: 0.9503626227378845
Epoch 1570, training loss: 12.026679992675781 = 0.023653559386730194 + 2.0 * 6.0015130043029785
Epoch 1570, val loss: 0.9531325697898865
Epoch 1580, training loss: 12.024864196777344 = 0.023164568468928337 + 2.0 * 6.000849723815918
Epoch 1580, val loss: 0.9560220241546631
Epoch 1590, training loss: 12.017969131469727 = 0.022698523476719856 + 2.0 * 5.997635364532471
Epoch 1590, val loss: 0.9591332077980042
Epoch 1600, training loss: 12.017091751098633 = 0.022238828241825104 + 2.0 * 5.997426509857178
Epoch 1600, val loss: 0.962048351764679
Epoch 1610, training loss: 12.023204803466797 = 0.021793276071548462 + 2.0 * 6.000705718994141
Epoch 1610, val loss: 0.9649325609207153
Epoch 1620, training loss: 12.01703929901123 = 0.021358460187911987 + 2.0 * 5.997840404510498
Epoch 1620, val loss: 0.967486560344696
Epoch 1630, training loss: 12.01772689819336 = 0.0209419596940279 + 2.0 * 5.998392581939697
Epoch 1630, val loss: 0.9705014824867249
Epoch 1640, training loss: 12.016473770141602 = 0.02053903043270111 + 2.0 * 5.99796724319458
Epoch 1640, val loss: 0.9733208417892456
Epoch 1650, training loss: 12.015318870544434 = 0.020142655819654465 + 2.0 * 5.997588157653809
Epoch 1650, val loss: 0.9761297106742859
Epoch 1660, training loss: 12.013758659362793 = 0.019760549068450928 + 2.0 * 5.996999263763428
Epoch 1660, val loss: 0.9788721203804016
Epoch 1670, training loss: 12.01888370513916 = 0.01938587799668312 + 2.0 * 5.999748706817627
Epoch 1670, val loss: 0.9814682602882385
Epoch 1680, training loss: 12.01260757446289 = 0.019025640562176704 + 2.0 * 5.996790885925293
Epoch 1680, val loss: 0.9841561913490295
Epoch 1690, training loss: 12.016630172729492 = 0.018678538501262665 + 2.0 * 5.99897575378418
Epoch 1690, val loss: 0.9870842695236206
Epoch 1700, training loss: 12.007993698120117 = 0.01833636872470379 + 2.0 * 5.994828701019287
Epoch 1700, val loss: 0.9896962642669678
Epoch 1710, training loss: 12.010594367980957 = 0.01800757646560669 + 2.0 * 5.996293544769287
Epoch 1710, val loss: 0.9924749135971069
Epoch 1720, training loss: 12.01108455657959 = 0.017686016857624054 + 2.0 * 5.996699333190918
Epoch 1720, val loss: 0.9950995445251465
Epoch 1730, training loss: 12.005529403686523 = 0.017371321097016335 + 2.0 * 5.994079113006592
Epoch 1730, val loss: 0.9977238178253174
Epoch 1740, training loss: 12.00775146484375 = 0.017066454514861107 + 2.0 * 5.99534273147583
Epoch 1740, val loss: 1.0003890991210938
Epoch 1750, training loss: 12.011588096618652 = 0.016770154237747192 + 2.0 * 5.997408866882324
Epoch 1750, val loss: 1.0029120445251465
Epoch 1760, training loss: 12.007710456848145 = 0.01648101583123207 + 2.0 * 5.995614528656006
Epoch 1760, val loss: 1.0054394006729126
Epoch 1770, training loss: 12.005537033081055 = 0.016199907287955284 + 2.0 * 5.994668483734131
Epoch 1770, val loss: 1.0080320835113525
Epoch 1780, training loss: 12.013631820678711 = 0.015927590429782867 + 2.0 * 5.998852252960205
Epoch 1780, val loss: 1.0105005502700806
Epoch 1790, training loss: 12.002717018127441 = 0.01566139981150627 + 2.0 * 5.993527889251709
Epoch 1790, val loss: 1.0130082368850708
Epoch 1800, training loss: 12.000496864318848 = 0.015402779914438725 + 2.0 * 5.992547035217285
Epoch 1800, val loss: 1.0157372951507568
Epoch 1810, training loss: 12.009357452392578 = 0.015148068778216839 + 2.0 * 5.997104644775391
Epoch 1810, val loss: 1.0180814266204834
Epoch 1820, training loss: 11.998672485351562 = 0.014900827780365944 + 2.0 * 5.991885662078857
Epoch 1820, val loss: 1.0203988552093506
Epoch 1830, training loss: 12.000495910644531 = 0.014662552624940872 + 2.0 * 5.992916584014893
Epoch 1830, val loss: 1.0230664014816284
Epoch 1840, training loss: 12.012917518615723 = 0.014427038840949535 + 2.0 * 5.9992451667785645
Epoch 1840, val loss: 1.0253145694732666
Epoch 1850, training loss: 11.998808860778809 = 0.014200693927705288 + 2.0 * 5.992303848266602
Epoch 1850, val loss: 1.0277595520019531
Epoch 1860, training loss: 11.995795249938965 = 0.013978435657918453 + 2.0 * 5.990908622741699
Epoch 1860, val loss: 1.0302870273590088
Epoch 1870, training loss: 11.998496055603027 = 0.013758310116827488 + 2.0 * 5.992368698120117
Epoch 1870, val loss: 1.0326682329177856
Epoch 1880, training loss: 12.002062797546387 = 0.01354511920362711 + 2.0 * 5.994258880615234
Epoch 1880, val loss: 1.0347529649734497
Epoch 1890, training loss: 11.99820613861084 = 0.013340733014047146 + 2.0 * 5.992432594299316
Epoch 1890, val loss: 1.0372117757797241
Epoch 1900, training loss: 11.996553421020508 = 0.013137761503458023 + 2.0 * 5.991707801818848
Epoch 1900, val loss: 1.0397119522094727
Epoch 1910, training loss: 11.996990203857422 = 0.012941241264343262 + 2.0 * 5.9920244216918945
Epoch 1910, val loss: 1.042027473449707
Epoch 1920, training loss: 11.993261337280273 = 0.012747924774885178 + 2.0 * 5.9902567863464355
Epoch 1920, val loss: 1.0444328784942627
Epoch 1930, training loss: 12.001209259033203 = 0.012560301460325718 + 2.0 * 5.994324684143066
Epoch 1930, val loss: 1.0466958284378052
Epoch 1940, training loss: 11.993586540222168 = 0.012374074198305607 + 2.0 * 5.990606307983398
Epoch 1940, val loss: 1.048758864402771
Epoch 1950, training loss: 11.992643356323242 = 0.012195790186524391 + 2.0 * 5.9902238845825195
Epoch 1950, val loss: 1.0512810945510864
Epoch 1960, training loss: 11.996594429016113 = 0.012020397931337357 + 2.0 * 5.9922871589660645
Epoch 1960, val loss: 1.0534765720367432
Epoch 1970, training loss: 11.992401123046875 = 0.011847121641039848 + 2.0 * 5.99027681350708
Epoch 1970, val loss: 1.0557080507278442
Epoch 1980, training loss: 12.00064754486084 = 0.011680379509925842 + 2.0 * 5.994483470916748
Epoch 1980, val loss: 1.0580106973648071
Epoch 1990, training loss: 11.991666793823242 = 0.011516769416630268 + 2.0 * 5.99007511138916
Epoch 1990, val loss: 1.0599185228347778
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8037
Overall ASR: 0.5793
Flip ASR: 0.5022/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.695634841918945 = 1.9478884935379028 + 2.0 * 8.373872756958008
Epoch 0, val loss: 1.9474414587020874
Epoch 10, training loss: 18.683713912963867 = 1.937017560005188 + 2.0 * 8.373348236083984
Epoch 10, val loss: 1.9366213083267212
Epoch 20, training loss: 18.663175582885742 = 1.9230433702468872 + 2.0 * 8.370065689086914
Epoch 20, val loss: 1.9222339391708374
Epoch 30, training loss: 18.600040435791016 = 1.9038318395614624 + 2.0 * 8.348104476928711
Epoch 30, val loss: 1.9022465944290161
Epoch 40, training loss: 18.272403717041016 = 1.8801251649856567 + 2.0 * 8.196139335632324
Epoch 40, val loss: 1.8780455589294434
Epoch 50, training loss: 16.920989990234375 = 1.853794813156128 + 2.0 * 7.533597469329834
Epoch 50, val loss: 1.8516390323638916
Epoch 60, training loss: 16.153564453125 = 1.836174726486206 + 2.0 * 7.158695220947266
Epoch 60, val loss: 1.8352937698364258
Epoch 70, training loss: 15.545540809631348 = 1.8277884721755981 + 2.0 * 6.8588762283325195
Epoch 70, val loss: 1.8269551992416382
Epoch 80, training loss: 15.169824600219727 = 1.8178606033325195 + 2.0 * 6.6759819984436035
Epoch 80, val loss: 1.8167667388916016
Epoch 90, training loss: 14.975419998168945 = 1.8093823194503784 + 2.0 * 6.583018779754639
Epoch 90, val loss: 1.8079243898391724
Epoch 100, training loss: 14.839444160461426 = 1.8001807928085327 + 2.0 * 6.519631862640381
Epoch 100, val loss: 1.7981905937194824
Epoch 110, training loss: 14.718364715576172 = 1.7924171686172485 + 2.0 * 6.462973594665527
Epoch 110, val loss: 1.7901643514633179
Epoch 120, training loss: 14.598797798156738 = 1.7859562635421753 + 2.0 * 6.406420707702637
Epoch 120, val loss: 1.7837138175964355
Epoch 130, training loss: 14.499979972839355 = 1.779784083366394 + 2.0 * 6.360097885131836
Epoch 130, val loss: 1.7774735689163208
Epoch 140, training loss: 14.420490264892578 = 1.772784948348999 + 2.0 * 6.3238525390625
Epoch 140, val loss: 1.7706283330917358
Epoch 150, training loss: 14.359992980957031 = 1.7647156715393066 + 2.0 * 6.297638416290283
Epoch 150, val loss: 1.763001799583435
Epoch 160, training loss: 14.307168006896973 = 1.7555755376815796 + 2.0 * 6.275796413421631
Epoch 160, val loss: 1.754883050918579
Epoch 170, training loss: 14.262104034423828 = 1.7453171014785767 + 2.0 * 6.258393287658691
Epoch 170, val loss: 1.7460439205169678
Epoch 180, training loss: 14.221973419189453 = 1.733720302581787 + 2.0 * 6.244126796722412
Epoch 180, val loss: 1.7362933158874512
Epoch 190, training loss: 14.18173885345459 = 1.7206518650054932 + 2.0 * 6.230543613433838
Epoch 190, val loss: 1.725412130355835
Epoch 200, training loss: 14.141618728637695 = 1.7055630683898926 + 2.0 * 6.2180280685424805
Epoch 200, val loss: 1.7130026817321777
Epoch 210, training loss: 14.10213851928711 = 1.6880148649215698 + 2.0 * 6.207061767578125
Epoch 210, val loss: 1.6987240314483643
Epoch 220, training loss: 14.061126708984375 = 1.6676850318908691 + 2.0 * 6.196721076965332
Epoch 220, val loss: 1.6821519136428833
Epoch 230, training loss: 14.018866539001465 = 1.644003987312317 + 2.0 * 6.187431335449219
Epoch 230, val loss: 1.6628613471984863
Epoch 240, training loss: 13.982491493225098 = 1.6164175271987915 + 2.0 * 6.183036804199219
Epoch 240, val loss: 1.640332579612732
Epoch 250, training loss: 13.929643630981445 = 1.584710955619812 + 2.0 * 6.172466278076172
Epoch 250, val loss: 1.614603042602539
Epoch 260, training loss: 13.879030227661133 = 1.5487699508666992 + 2.0 * 6.165130138397217
Epoch 260, val loss: 1.5853326320648193
Epoch 270, training loss: 13.832291603088379 = 1.5084750652313232 + 2.0 * 6.161908149719238
Epoch 270, val loss: 1.552648901939392
Epoch 280, training loss: 13.77225112915039 = 1.465234398841858 + 2.0 * 6.153508186340332
Epoch 280, val loss: 1.5176281929016113
Epoch 290, training loss: 13.717422485351562 = 1.4198527336120605 + 2.0 * 6.14878511428833
Epoch 290, val loss: 1.4814660549163818
Epoch 300, training loss: 13.663713455200195 = 1.3733197450637817 + 2.0 * 6.145196914672852
Epoch 300, val loss: 1.444723129272461
Epoch 310, training loss: 13.613587379455566 = 1.3271270990371704 + 2.0 * 6.143229961395264
Epoch 310, val loss: 1.4090304374694824
Epoch 320, training loss: 13.556619644165039 = 1.282362937927246 + 2.0 * 6.1371283531188965
Epoch 320, val loss: 1.3752583265304565
Epoch 330, training loss: 13.504405975341797 = 1.23911452293396 + 2.0 * 6.132645606994629
Epoch 330, val loss: 1.343132495880127
Epoch 340, training loss: 13.45903205871582 = 1.1974782943725586 + 2.0 * 6.130776882171631
Epoch 340, val loss: 1.312709093093872
Epoch 350, training loss: 13.410515785217285 = 1.157549500465393 + 2.0 * 6.126482963562012
Epoch 350, val loss: 1.2840280532836914
Epoch 360, training loss: 13.36143684387207 = 1.1185574531555176 + 2.0 * 6.121439456939697
Epoch 360, val loss: 1.2563639879226685
Epoch 370, training loss: 13.32204532623291 = 1.0803192853927612 + 2.0 * 6.12086296081543
Epoch 370, val loss: 1.22940194606781
Epoch 380, training loss: 13.274099349975586 = 1.0431690216064453 + 2.0 * 6.11546516418457
Epoch 380, val loss: 1.2032583951950073
Epoch 390, training loss: 13.230273246765137 = 1.006416916847229 + 2.0 * 6.1119279861450195
Epoch 390, val loss: 1.1773403882980347
Epoch 400, training loss: 13.187799453735352 = 0.9698400497436523 + 2.0 * 6.10897970199585
Epoch 400, val loss: 1.1514760255813599
Epoch 410, training loss: 13.151992797851562 = 0.9337266087532043 + 2.0 * 6.109133243560791
Epoch 410, val loss: 1.1257925033569336
Epoch 420, training loss: 13.108915328979492 = 0.8985828757286072 + 2.0 * 6.105166435241699
Epoch 420, val loss: 1.100831151008606
Epoch 430, training loss: 13.066793441772461 = 0.8640648126602173 + 2.0 * 6.1013641357421875
Epoch 430, val loss: 1.0762914419174194
Epoch 440, training loss: 13.029372215270996 = 0.8300520777702332 + 2.0 * 6.0996599197387695
Epoch 440, val loss: 1.0520540475845337
Epoch 450, training loss: 12.988107681274414 = 0.796715259552002 + 2.0 * 6.095695972442627
Epoch 450, val loss: 1.028438925743103
Epoch 460, training loss: 12.957292556762695 = 0.764053463935852 + 2.0 * 6.096619606018066
Epoch 460, val loss: 1.005172848701477
Epoch 470, training loss: 12.918252944946289 = 0.7324386835098267 + 2.0 * 6.092906951904297
Epoch 470, val loss: 0.9828910827636719
Epoch 480, training loss: 12.881319046020508 = 0.7020837664604187 + 2.0 * 6.089617729187012
Epoch 480, val loss: 0.9615535140037537
Epoch 490, training loss: 12.857330322265625 = 0.672798752784729 + 2.0 * 6.092265605926514
Epoch 490, val loss: 0.9412609934806824
Epoch 500, training loss: 12.824039459228516 = 0.6448942422866821 + 2.0 * 6.089572429656982
Epoch 500, val loss: 0.9222193956375122
Epoch 510, training loss: 12.787322044372559 = 0.6186330318450928 + 2.0 * 6.084344387054443
Epoch 510, val loss: 0.9045109748840332
Epoch 520, training loss: 12.75410270690918 = 0.593462347984314 + 2.0 * 6.080320358276367
Epoch 520, val loss: 0.8879592418670654
Epoch 530, training loss: 12.740339279174805 = 0.5693888664245605 + 2.0 * 6.085474967956543
Epoch 530, val loss: 0.8724184036254883
Epoch 540, training loss: 12.70170783996582 = 0.5467587113380432 + 2.0 * 6.077474594116211
Epoch 540, val loss: 0.857952892780304
Epoch 550, training loss: 12.68051528930664 = 0.5253963470458984 + 2.0 * 6.077559471130371
Epoch 550, val loss: 0.8451228737831116
Epoch 560, training loss: 12.652413368225098 = 0.5047802329063416 + 2.0 * 6.073816776275635
Epoch 560, val loss: 0.8329356908798218
Epoch 570, training loss: 12.628437042236328 = 0.48477447032928467 + 2.0 * 6.071831226348877
Epoch 570, val loss: 0.8215386867523193
Epoch 580, training loss: 12.612189292907715 = 0.46529048681259155 + 2.0 * 6.073449611663818
Epoch 580, val loss: 0.8109707236289978
Epoch 590, training loss: 12.59406566619873 = 0.44648849964141846 + 2.0 * 6.073788642883301
Epoch 590, val loss: 0.8009071350097656
Epoch 600, training loss: 12.567658424377441 = 0.42823174595832825 + 2.0 * 6.069713115692139
Epoch 600, val loss: 0.7919189929962158
Epoch 610, training loss: 12.545431137084961 = 0.4104650318622589 + 2.0 * 6.067482948303223
Epoch 610, val loss: 0.7832927703857422
Epoch 620, training loss: 12.527149200439453 = 0.39321601390838623 + 2.0 * 6.066966533660889
Epoch 620, val loss: 0.7755612730979919
Epoch 630, training loss: 12.503779411315918 = 0.3762989640235901 + 2.0 * 6.063740253448486
Epoch 630, val loss: 0.7683302760124207
Epoch 640, training loss: 12.4930419921875 = 0.3595768213272095 + 2.0 * 6.066732406616211
Epoch 640, val loss: 0.761570394039154
Epoch 650, training loss: 12.474736213684082 = 0.34330272674560547 + 2.0 * 6.065716743469238
Epoch 650, val loss: 0.7551763653755188
Epoch 660, training loss: 12.451207160949707 = 0.3275027871131897 + 2.0 * 6.061851978302002
Epoch 660, val loss: 0.7497103214263916
Epoch 670, training loss: 12.433598518371582 = 0.31207263469696045 + 2.0 * 6.060762882232666
Epoch 670, val loss: 0.7445682287216187
Epoch 680, training loss: 12.418089866638184 = 0.2970806062221527 + 2.0 * 6.06050443649292
Epoch 680, val loss: 0.7398901581764221
Epoch 690, training loss: 12.40548038482666 = 0.2825770676136017 + 2.0 * 6.061451435089111
Epoch 690, val loss: 0.73562091588974
Epoch 700, training loss: 12.380376815795898 = 0.26873350143432617 + 2.0 * 6.055821418762207
Epoch 700, val loss: 0.7320825457572937
Epoch 710, training loss: 12.366372108459473 = 0.2554071247577667 + 2.0 * 6.055482387542725
Epoch 710, val loss: 0.7289712429046631
Epoch 720, training loss: 12.354864120483398 = 0.24256548285484314 + 2.0 * 6.056149482727051
Epoch 720, val loss: 0.7262797951698303
Epoch 730, training loss: 12.351875305175781 = 0.23036867380142212 + 2.0 * 6.060753345489502
Epoch 730, val loss: 0.7240318655967712
Epoch 740, training loss: 12.324885368347168 = 0.2189619243144989 + 2.0 * 6.052961826324463
Epoch 740, val loss: 0.7225821018218994
Epoch 750, training loss: 12.308938026428223 = 0.20808467268943787 + 2.0 * 6.050426483154297
Epoch 750, val loss: 0.7213491797447205
Epoch 760, training loss: 12.297471046447754 = 0.19773873686790466 + 2.0 * 6.049866199493408
Epoch 760, val loss: 0.7205151319503784
Epoch 770, training loss: 12.32192611694336 = 0.18788674473762512 + 2.0 * 6.067019462585449
Epoch 770, val loss: 0.7200336456298828
Epoch 780, training loss: 12.278057098388672 = 0.17879252135753632 + 2.0 * 6.0496320724487305
Epoch 780, val loss: 0.7198644876480103
Epoch 790, training loss: 12.264034271240234 = 0.17023725807666779 + 2.0 * 6.046898365020752
Epoch 790, val loss: 0.7205639481544495
Epoch 800, training loss: 12.253873825073242 = 0.16213910281658173 + 2.0 * 6.045867443084717
Epoch 800, val loss: 0.7212037444114685
Epoch 810, training loss: 12.248552322387695 = 0.154465451836586 + 2.0 * 6.047043323516846
Epoch 810, val loss: 0.7222042679786682
Epoch 820, training loss: 12.240375518798828 = 0.14727839827537537 + 2.0 * 6.046548366546631
Epoch 820, val loss: 0.7233476042747498
Epoch 830, training loss: 12.232036590576172 = 0.14056353271007538 + 2.0 * 6.045736312866211
Epoch 830, val loss: 0.7250685691833496
Epoch 840, training loss: 12.225380897521973 = 0.134243443608284 + 2.0 * 6.045568943023682
Epoch 840, val loss: 0.7268573045730591
Epoch 850, training loss: 12.215415954589844 = 0.1282818466424942 + 2.0 * 6.043567180633545
Epoch 850, val loss: 0.72905033826828
Epoch 860, training loss: 12.21419620513916 = 0.1226760521531105 + 2.0 * 6.045760154724121
Epoch 860, val loss: 0.7311943769454956
Epoch 870, training loss: 12.200868606567383 = 0.11738372594118118 + 2.0 * 6.041742324829102
Epoch 870, val loss: 0.7336474657058716
Epoch 880, training loss: 12.192845344543457 = 0.11243055015802383 + 2.0 * 6.040207386016846
Epoch 880, val loss: 0.7363699078559875
Epoch 890, training loss: 12.188024520874023 = 0.10772573947906494 + 2.0 * 6.040149211883545
Epoch 890, val loss: 0.7392094135284424
Epoch 900, training loss: 12.181737899780273 = 0.10328925400972366 + 2.0 * 6.039224147796631
Epoch 900, val loss: 0.7420673370361328
Epoch 910, training loss: 12.194751739501953 = 0.09911653399467468 + 2.0 * 6.047817707061768
Epoch 910, val loss: 0.7452403903007507
Epoch 920, training loss: 12.172301292419434 = 0.0952138900756836 + 2.0 * 6.038543701171875
Epoch 920, val loss: 0.7482517957687378
Epoch 930, training loss: 12.165392875671387 = 0.09153299778699875 + 2.0 * 6.036930084228516
Epoch 930, val loss: 0.7516717910766602
Epoch 940, training loss: 12.159504890441895 = 0.08801418542861938 + 2.0 * 6.035745143890381
Epoch 940, val loss: 0.7550022602081299
Epoch 950, training loss: 12.175292015075684 = 0.08466718345880508 + 2.0 * 6.045312404632568
Epoch 950, val loss: 0.7583723068237305
Epoch 960, training loss: 12.154852867126465 = 0.0815509632229805 + 2.0 * 6.036651134490967
Epoch 960, val loss: 0.7618647217750549
Epoch 970, training loss: 12.145445823669434 = 0.07856587320566177 + 2.0 * 6.033440113067627
Epoch 970, val loss: 0.7655980587005615
Epoch 980, training loss: 12.154550552368164 = 0.07572225481271744 + 2.0 * 6.039413928985596
Epoch 980, val loss: 0.7691063284873962
Epoch 990, training loss: 12.14428997039795 = 0.07302798330783844 + 2.0 * 6.03563117980957
Epoch 990, val loss: 0.7726761102676392
Epoch 1000, training loss: 12.143357276916504 = 0.07047799229621887 + 2.0 * 6.036439418792725
Epoch 1000, val loss: 0.7763960957527161
Epoch 1010, training loss: 12.130313873291016 = 0.0680527463555336 + 2.0 * 6.031130790710449
Epoch 1010, val loss: 0.7800827622413635
Epoch 1020, training loss: 12.128483772277832 = 0.06574242562055588 + 2.0 * 6.031370639801025
Epoch 1020, val loss: 0.7838500142097473
Epoch 1030, training loss: 12.150714874267578 = 0.06352343410253525 + 2.0 * 6.043595790863037
Epoch 1030, val loss: 0.7873006463050842
Epoch 1040, training loss: 12.124495506286621 = 0.061445996165275574 + 2.0 * 6.031524658203125
Epoch 1040, val loss: 0.7910617589950562
Epoch 1050, training loss: 12.116301536560059 = 0.059459980577230453 + 2.0 * 6.028420925140381
Epoch 1050, val loss: 0.7950005531311035
Epoch 1060, training loss: 12.114230155944824 = 0.057545095682144165 + 2.0 * 6.0283427238464355
Epoch 1060, val loss: 0.7986641526222229
Epoch 1070, training loss: 12.118504524230957 = 0.05570707842707634 + 2.0 * 6.031398773193359
Epoch 1070, val loss: 0.8023569583892822
Epoch 1080, training loss: 12.11553955078125 = 0.053957123309373856 + 2.0 * 6.030791282653809
Epoch 1080, val loss: 0.8058894872665405
Epoch 1090, training loss: 12.109317779541016 = 0.05229305848479271 + 2.0 * 6.028512477874756
Epoch 1090, val loss: 0.8096711039543152
Epoch 1100, training loss: 12.102465629577637 = 0.05069538578391075 + 2.0 * 6.025885105133057
Epoch 1100, val loss: 0.813426673412323
Epoch 1110, training loss: 12.105132102966309 = 0.049150727689266205 + 2.0 * 6.027990818023682
Epoch 1110, val loss: 0.8169773817062378
Epoch 1120, training loss: 12.100301742553711 = 0.04767531156539917 + 2.0 * 6.026313304901123
Epoch 1120, val loss: 0.8205137848854065
Epoch 1130, training loss: 12.10180950164795 = 0.04626988619565964 + 2.0 * 6.027770042419434
Epoch 1130, val loss: 0.8241089582443237
Epoch 1140, training loss: 12.094799995422363 = 0.044916845858097076 + 2.0 * 6.024941444396973
Epoch 1140, val loss: 0.8277127146720886
Epoch 1150, training loss: 12.09079360961914 = 0.043615538626909256 + 2.0 * 6.023589134216309
Epoch 1150, val loss: 0.831484317779541
Epoch 1160, training loss: 12.091569900512695 = 0.042355988174676895 + 2.0 * 6.024607181549072
Epoch 1160, val loss: 0.8349800109863281
Epoch 1170, training loss: 12.093806266784668 = 0.04114923253655434 + 2.0 * 6.0263285636901855
Epoch 1170, val loss: 0.8384201526641846
Epoch 1180, training loss: 12.086540222167969 = 0.039992302656173706 + 2.0 * 6.023273944854736
Epoch 1180, val loss: 0.8417591452598572
Epoch 1190, training loss: 12.080723762512207 = 0.03888774290680885 + 2.0 * 6.020917892456055
Epoch 1190, val loss: 0.8454597592353821
Epoch 1200, training loss: 12.079928398132324 = 0.03781777620315552 + 2.0 * 6.021055221557617
Epoch 1200, val loss: 0.8490235209465027
Epoch 1210, training loss: 12.0991849899292 = 0.03678400442004204 + 2.0 * 6.031200408935547
Epoch 1210, val loss: 0.8523232340812683
Epoch 1220, training loss: 12.083403587341309 = 0.03579316288232803 + 2.0 * 6.023805141448975
Epoch 1220, val loss: 0.8556634187698364
Epoch 1230, training loss: 12.074376106262207 = 0.03485095500946045 + 2.0 * 6.0197625160217285
Epoch 1230, val loss: 0.8593869209289551
Epoch 1240, training loss: 12.072503089904785 = 0.033931974321603775 + 2.0 * 6.019285678863525
Epoch 1240, val loss: 0.86282879114151
Epoch 1250, training loss: 12.078192710876465 = 0.03304033726453781 + 2.0 * 6.022576332092285
Epoch 1250, val loss: 0.8662882447242737
Epoch 1260, training loss: 12.07065200805664 = 0.032184064388275146 + 2.0 * 6.0192341804504395
Epoch 1260, val loss: 0.8693877458572388
Epoch 1270, training loss: 12.070844650268555 = 0.03136768937110901 + 2.0 * 6.019738674163818
Epoch 1270, val loss: 0.8730264902114868
Epoch 1280, training loss: 12.06961727142334 = 0.030578704550862312 + 2.0 * 6.019519329071045
Epoch 1280, val loss: 0.8763520121574402
Epoch 1290, training loss: 12.067676544189453 = 0.029813922941684723 + 2.0 * 6.0189313888549805
Epoch 1290, val loss: 0.8798019886016846
Epoch 1300, training loss: 12.06462287902832 = 0.029074445366859436 + 2.0 * 6.0177741050720215
Epoch 1300, val loss: 0.8830814361572266
Epoch 1310, training loss: 12.064621925354004 = 0.028359118849039078 + 2.0 * 6.018131256103516
Epoch 1310, val loss: 0.8865191340446472
Epoch 1320, training loss: 12.063998222351074 = 0.027668889611959457 + 2.0 * 6.01816463470459
Epoch 1320, val loss: 0.8898112177848816
Epoch 1330, training loss: 12.059678077697754 = 0.027004217728972435 + 2.0 * 6.016336917877197
Epoch 1330, val loss: 0.8931488990783691
Epoch 1340, training loss: 12.063907623291016 = 0.02636132761836052 + 2.0 * 6.018773078918457
Epoch 1340, val loss: 0.8964855670928955
Epoch 1350, training loss: 12.055126190185547 = 0.02574639394879341 + 2.0 * 6.014689922332764
Epoch 1350, val loss: 0.899799644947052
Epoch 1360, training loss: 12.062018394470215 = 0.025148866698145866 + 2.0 * 6.018435001373291
Epoch 1360, val loss: 0.903143048286438
Epoch 1370, training loss: 12.05484390258789 = 0.024563860148191452 + 2.0 * 6.015140056610107
Epoch 1370, val loss: 0.9062088131904602
Epoch 1380, training loss: 12.056447982788086 = 0.02401210181415081 + 2.0 * 6.0162177085876465
Epoch 1380, val loss: 0.9095750451087952
Epoch 1390, training loss: 12.05031681060791 = 0.023471174761652946 + 2.0 * 6.013422966003418
Epoch 1390, val loss: 0.9126922488212585
Epoch 1400, training loss: 12.048357963562012 = 0.022948186844587326 + 2.0 * 6.012704849243164
Epoch 1400, val loss: 0.9160062670707703
Epoch 1410, training loss: 12.05168342590332 = 0.022435864433646202 + 2.0 * 6.014623641967773
Epoch 1410, val loss: 0.9190911054611206
Epoch 1420, training loss: 12.054377555847168 = 0.021941957995295525 + 2.0 * 6.0162177085876465
Epoch 1420, val loss: 0.9220991730690002
Epoch 1430, training loss: 12.04643726348877 = 0.02147013507783413 + 2.0 * 6.012483596801758
Epoch 1430, val loss: 0.9253454208374023
Epoch 1440, training loss: 12.04476261138916 = 0.021014787256717682 + 2.0 * 6.011873722076416
Epoch 1440, val loss: 0.9285973906517029
Epoch 1450, training loss: 12.048782348632812 = 0.020566711202263832 + 2.0 * 6.014107704162598
Epoch 1450, val loss: 0.9316521883010864
Epoch 1460, training loss: 12.042033195495605 = 0.020135799422860146 + 2.0 * 6.010948657989502
Epoch 1460, val loss: 0.9347163438796997
Epoch 1470, training loss: 12.065485000610352 = 0.019721267744898796 + 2.0 * 6.022881984710693
Epoch 1470, val loss: 0.9377099275588989
Epoch 1480, training loss: 12.04770565032959 = 0.019311407580971718 + 2.0 * 6.01419734954834
Epoch 1480, val loss: 0.9406269788742065
Epoch 1490, training loss: 12.03768539428711 = 0.018931623548269272 + 2.0 * 6.0093770027160645
Epoch 1490, val loss: 0.9439260959625244
Epoch 1500, training loss: 12.036656379699707 = 0.018550440669059753 + 2.0 * 6.009052753448486
Epoch 1500, val loss: 0.9468958377838135
Epoch 1510, training loss: 12.035425186157227 = 0.01817789115011692 + 2.0 * 6.0086236000061035
Epoch 1510, val loss: 0.9499332904815674
Epoch 1520, training loss: 12.050698280334473 = 0.01781761273741722 + 2.0 * 6.016440391540527
Epoch 1520, val loss: 0.9529469609260559
Epoch 1530, training loss: 12.041067123413086 = 0.017467021942138672 + 2.0 * 6.0117998123168945
Epoch 1530, val loss: 0.955390989780426
Epoch 1540, training loss: 12.037747383117676 = 0.017138343304395676 + 2.0 * 6.0103044509887695
Epoch 1540, val loss: 0.9587568640708923
Epoch 1550, training loss: 12.034002304077148 = 0.016811389476060867 + 2.0 * 6.0085954666137695
Epoch 1550, val loss: 0.9615956544876099
Epoch 1560, training loss: 12.04072380065918 = 0.016491549089550972 + 2.0 * 6.012115955352783
Epoch 1560, val loss: 0.9643487930297852
Epoch 1570, training loss: 12.032564163208008 = 0.01618216186761856 + 2.0 * 6.008191108703613
Epoch 1570, val loss: 0.967327892780304
Epoch 1580, training loss: 12.031864166259766 = 0.01588219776749611 + 2.0 * 6.007990837097168
Epoch 1580, val loss: 0.970267653465271
Epoch 1590, training loss: 12.033260345458984 = 0.015588119626045227 + 2.0 * 6.008836269378662
Epoch 1590, val loss: 0.9729862809181213
Epoch 1600, training loss: 12.029480934143066 = 0.015305189415812492 + 2.0 * 6.007087707519531
Epoch 1600, val loss: 0.9758371114730835
Epoch 1610, training loss: 12.03021240234375 = 0.0150285130366683 + 2.0 * 6.007591724395752
Epoch 1610, val loss: 0.9787921905517578
Epoch 1620, training loss: 12.043900489807129 = 0.014754794538021088 + 2.0 * 6.014572620391846
Epoch 1620, val loss: 0.9813879132270813
Epoch 1630, training loss: 12.029635429382324 = 0.014499090611934662 + 2.0 * 6.007568359375
Epoch 1630, val loss: 0.9839985966682434
Epoch 1640, training loss: 12.025218963623047 = 0.014248942956328392 + 2.0 * 6.0054850578308105
Epoch 1640, val loss: 0.9870631694793701
Epoch 1650, training loss: 12.022712707519531 = 0.014000898227095604 + 2.0 * 6.0043559074401855
Epoch 1650, val loss: 0.9897834062576294
Epoch 1660, training loss: 12.022115707397461 = 0.013756295666098595 + 2.0 * 6.00417947769165
Epoch 1660, val loss: 0.9924994707107544
Epoch 1670, training loss: 12.024110794067383 = 0.013517177663743496 + 2.0 * 6.00529670715332
Epoch 1670, val loss: 0.995239794254303
Epoch 1680, training loss: 12.033281326293945 = 0.013287113979458809 + 2.0 * 6.0099968910217285
Epoch 1680, val loss: 0.9975661635398865
Epoch 1690, training loss: 12.024212837219238 = 0.013066217303276062 + 2.0 * 6.005573272705078
Epoch 1690, val loss: 1.0002429485321045
Epoch 1700, training loss: 12.022128105163574 = 0.012855825014412403 + 2.0 * 6.004636287689209
Epoch 1700, val loss: 1.003230094909668
Epoch 1710, training loss: 12.025151252746582 = 0.012644292786717415 + 2.0 * 6.006253242492676
Epoch 1710, val loss: 1.0057628154754639
Epoch 1720, training loss: 12.020927429199219 = 0.01243629027158022 + 2.0 * 6.004245758056641
Epoch 1720, val loss: 1.00804603099823
Epoch 1730, training loss: 12.017818450927734 = 0.012237613089382648 + 2.0 * 6.002790451049805
Epoch 1730, val loss: 1.0108736753463745
Epoch 1740, training loss: 12.017304420471191 = 0.012041478417813778 + 2.0 * 6.002631664276123
Epoch 1740, val loss: 1.013522744178772
Epoch 1750, training loss: 12.016879081726074 = 0.011847727000713348 + 2.0 * 6.00251579284668
Epoch 1750, val loss: 1.0160609483718872
Epoch 1760, training loss: 12.032343864440918 = 0.01166091300547123 + 2.0 * 6.010341644287109
Epoch 1760, val loss: 1.0184295177459717
Epoch 1770, training loss: 12.028257369995117 = 0.011479194276034832 + 2.0 * 6.008388996124268
Epoch 1770, val loss: 1.0208691358566284
Epoch 1780, training loss: 12.014302253723145 = 0.011304859071969986 + 2.0 * 6.001498699188232
Epoch 1780, val loss: 1.023560881614685
Epoch 1790, training loss: 12.01420783996582 = 0.01113300770521164 + 2.0 * 6.001537322998047
Epoch 1790, val loss: 1.0261043310165405
Epoch 1800, training loss: 12.013235092163086 = 0.010962745174765587 + 2.0 * 6.001136302947998
Epoch 1800, val loss: 1.0285680294036865
Epoch 1810, training loss: 12.025970458984375 = 0.010796877555549145 + 2.0 * 6.00758695602417
Epoch 1810, val loss: 1.0310065746307373
Epoch 1820, training loss: 12.021060943603516 = 0.010634935460984707 + 2.0 * 6.005212783813477
Epoch 1820, val loss: 1.0332472324371338
Epoch 1830, training loss: 12.014699935913086 = 0.010480761528015137 + 2.0 * 6.002109527587891
Epoch 1830, val loss: 1.035926103591919
Epoch 1840, training loss: 12.014705657958984 = 0.010327998548746109 + 2.0 * 6.002188682556152
Epoch 1840, val loss: 1.0384521484375
Epoch 1850, training loss: 12.016910552978516 = 0.010176845826208591 + 2.0 * 6.003366947174072
Epoch 1850, val loss: 1.0405504703521729
Epoch 1860, training loss: 12.013591766357422 = 0.01003177184611559 + 2.0 * 6.001780033111572
Epoch 1860, val loss: 1.0430381298065186
Epoch 1870, training loss: 12.02500057220459 = 0.00989109929651022 + 2.0 * 6.007554531097412
Epoch 1870, val loss: 1.0453652143478394
Epoch 1880, training loss: 12.010537147521973 = 0.009747548028826714 + 2.0 * 6.000394821166992
Epoch 1880, val loss: 1.0476157665252686
Epoch 1890, training loss: 12.007489204406738 = 0.009614462964236736 + 2.0 * 5.998937606811523
Epoch 1890, val loss: 1.0502302646636963
Epoch 1900, training loss: 12.006413459777832 = 0.009479112923145294 + 2.0 * 5.998466968536377
Epoch 1900, val loss: 1.052487850189209
Epoch 1910, training loss: 12.013251304626465 = 0.009345169179141521 + 2.0 * 6.001953125
Epoch 1910, val loss: 1.0547693967819214
Epoch 1920, training loss: 12.00921630859375 = 0.009216624312102795 + 2.0 * 6.0
Epoch 1920, val loss: 1.0568697452545166
Epoch 1930, training loss: 12.010246276855469 = 0.009093154221773148 + 2.0 * 6.000576496124268
Epoch 1930, val loss: 1.059187412261963
Epoch 1940, training loss: 12.005785942077637 = 0.008971355855464935 + 2.0 * 5.998407363891602
Epoch 1940, val loss: 1.0617365837097168
Epoch 1950, training loss: 12.008841514587402 = 0.008850890211760998 + 2.0 * 5.999995231628418
Epoch 1950, val loss: 1.0638878345489502
Epoch 1960, training loss: 12.009210586547852 = 0.00873273704200983 + 2.0 * 6.00023889541626
Epoch 1960, val loss: 1.0659948587417603
Epoch 1970, training loss: 12.004293441772461 = 0.008618403226137161 + 2.0 * 5.997837543487549
Epoch 1970, val loss: 1.068490743637085
Epoch 1980, training loss: 12.00500774383545 = 0.0085050193592906 + 2.0 * 5.998251438140869
Epoch 1980, val loss: 1.0707107782363892
Epoch 1990, training loss: 12.017529487609863 = 0.008394809439778328 + 2.0 * 6.0045671463012695
Epoch 1990, val loss: 1.0727366209030151
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.6273
Flip ASR: 0.5778/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 18.706600189208984 = 1.958924412727356 + 2.0 * 8.3738374710083
Epoch 0, val loss: 1.9577538967132568
Epoch 10, training loss: 18.6953125 = 1.9488643407821655 + 2.0 * 8.373224258422852
Epoch 10, val loss: 1.947337031364441
Epoch 20, training loss: 18.674346923828125 = 1.9364064931869507 + 2.0 * 8.368969917297363
Epoch 20, val loss: 1.9343408346176147
Epoch 30, training loss: 18.60124969482422 = 1.919800043106079 + 2.0 * 8.34072494506836
Epoch 30, val loss: 1.9167543649673462
Epoch 40, training loss: 18.249141693115234 = 1.9004087448120117 + 2.0 * 8.174365997314453
Epoch 40, val loss: 1.8966337442398071
Epoch 50, training loss: 17.00040054321289 = 1.8786089420318604 + 2.0 * 7.5608954429626465
Epoch 50, val loss: 1.8736090660095215
Epoch 60, training loss: 16.08987045288086 = 1.861825942993164 + 2.0 * 7.114022731781006
Epoch 60, val loss: 1.8572964668273926
Epoch 70, training loss: 15.52802848815918 = 1.8493781089782715 + 2.0 * 6.839325428009033
Epoch 70, val loss: 1.844623327255249
Epoch 80, training loss: 15.20860767364502 = 1.8368885517120361 + 2.0 * 6.685859680175781
Epoch 80, val loss: 1.8322347402572632
Epoch 90, training loss: 14.958456993103027 = 1.825555443763733 + 2.0 * 6.566450595855713
Epoch 90, val loss: 1.8209474086761475
Epoch 100, training loss: 14.793596267700195 = 1.815222978591919 + 2.0 * 6.489186763763428
Epoch 100, val loss: 1.81040620803833
Epoch 110, training loss: 14.668205261230469 = 1.8056811094284058 + 2.0 * 6.431262016296387
Epoch 110, val loss: 1.8003290891647339
Epoch 120, training loss: 14.56476879119873 = 1.796635389328003 + 2.0 * 6.384066581726074
Epoch 120, val loss: 1.7909104824066162
Epoch 130, training loss: 14.483929634094238 = 1.7880544662475586 + 2.0 * 6.34793758392334
Epoch 130, val loss: 1.7821736335754395
Epoch 140, training loss: 14.416236877441406 = 1.779657006263733 + 2.0 * 6.318289756774902
Epoch 140, val loss: 1.773999571800232
Epoch 150, training loss: 14.35601806640625 = 1.7711713314056396 + 2.0 * 6.292423248291016
Epoch 150, val loss: 1.7661361694335938
Epoch 160, training loss: 14.29903793334961 = 1.762249231338501 + 2.0 * 6.268394470214844
Epoch 160, val loss: 1.7582749128341675
Epoch 170, training loss: 14.249814987182617 = 1.752489686012268 + 2.0 * 6.24866247177124
Epoch 170, val loss: 1.749998688697815
Epoch 180, training loss: 14.210609436035156 = 1.741426944732666 + 2.0 * 6.234591007232666
Epoch 180, val loss: 1.7407809495925903
Epoch 190, training loss: 14.169492721557617 = 1.7286351919174194 + 2.0 * 6.220428943634033
Epoch 190, val loss: 1.73023521900177
Epoch 200, training loss: 14.132205963134766 = 1.7137236595153809 + 2.0 * 6.209240913391113
Epoch 200, val loss: 1.7181153297424316
Epoch 210, training loss: 14.09524154663086 = 1.6962237358093262 + 2.0 * 6.1995086669921875
Epoch 210, val loss: 1.7039828300476074
Epoch 220, training loss: 14.064175605773926 = 1.6755415201187134 + 2.0 * 6.194316864013672
Epoch 220, val loss: 1.687479853630066
Epoch 230, training loss: 14.018878936767578 = 1.6515008211135864 + 2.0 * 6.183689117431641
Epoch 230, val loss: 1.6681922674179077
Epoch 240, training loss: 13.973401069641113 = 1.6231906414031982 + 2.0 * 6.175105094909668
Epoch 240, val loss: 1.6456124782562256
Epoch 250, training loss: 13.929238319396973 = 1.589981198310852 + 2.0 * 6.169628620147705
Epoch 250, val loss: 1.6191835403442383
Epoch 260, training loss: 13.875434875488281 = 1.5514044761657715 + 2.0 * 6.162015438079834
Epoch 260, val loss: 1.5887802839279175
Epoch 270, training loss: 13.818510055541992 = 1.5070897340774536 + 2.0 * 6.155710220336914
Epoch 270, val loss: 1.5538480281829834
Epoch 280, training loss: 13.757637023925781 = 1.4572054147720337 + 2.0 * 6.1502156257629395
Epoch 280, val loss: 1.5145323276519775
Epoch 290, training loss: 13.694758415222168 = 1.4031702280044556 + 2.0 * 6.145793914794922
Epoch 290, val loss: 1.4719822406768799
Epoch 300, training loss: 13.630008697509766 = 1.3471826314926147 + 2.0 * 6.14141321182251
Epoch 300, val loss: 1.4282211065292358
Epoch 310, training loss: 13.562644958496094 = 1.290330410003662 + 2.0 * 6.136157035827637
Epoch 310, val loss: 1.3839201927185059
Epoch 320, training loss: 13.496931076049805 = 1.2337485551834106 + 2.0 * 6.131591320037842
Epoch 320, val loss: 1.3399940729141235
Epoch 330, training loss: 13.448269844055176 = 1.1791621446609497 + 2.0 * 6.134553909301758
Epoch 330, val loss: 1.2986024618148804
Epoch 340, training loss: 13.37999153137207 = 1.128242015838623 + 2.0 * 6.125874996185303
Epoch 340, val loss: 1.2598122358322144
Epoch 350, training loss: 13.319585800170898 = 1.079201102256775 + 2.0 * 6.120192527770996
Epoch 350, val loss: 1.2228814363479614
Epoch 360, training loss: 13.2639741897583 = 1.0317471027374268 + 2.0 * 6.116113662719727
Epoch 360, val loss: 1.1872631311416626
Epoch 370, training loss: 13.210746765136719 = 0.985619843006134 + 2.0 * 6.112563610076904
Epoch 370, val loss: 1.1528490781784058
Epoch 380, training loss: 13.162468910217285 = 0.9411062002182007 + 2.0 * 6.110681533813477
Epoch 380, val loss: 1.1197967529296875
Epoch 390, training loss: 13.113643646240234 = 0.8992002010345459 + 2.0 * 6.107221603393555
Epoch 390, val loss: 1.0884099006652832
Epoch 400, training loss: 13.067422866821289 = 0.8592290282249451 + 2.0 * 6.10409688949585
Epoch 400, val loss: 1.058532953262329
Epoch 410, training loss: 13.022639274597168 = 0.8207660913467407 + 2.0 * 6.100936412811279
Epoch 410, val loss: 1.0299113988876343
Epoch 420, training loss: 13.003229141235352 = 0.7839984893798828 + 2.0 * 6.109615325927734
Epoch 420, val loss: 1.0027402639389038
Epoch 430, training loss: 12.945035934448242 = 0.7500200271606445 + 2.0 * 6.097507953643799
Epoch 430, val loss: 0.9775190353393555
Epoch 440, training loss: 12.906670570373535 = 0.7180739045143127 + 2.0 * 6.094298362731934
Epoch 440, val loss: 0.9541797637939453
Epoch 450, training loss: 12.870957374572754 = 0.6877833008766174 + 2.0 * 6.091587066650391
Epoch 450, val loss: 0.9322162866592407
Epoch 460, training loss: 12.83808422088623 = 0.6591339111328125 + 2.0 * 6.089475154876709
Epoch 460, val loss: 0.9117499589920044
Epoch 470, training loss: 12.806708335876465 = 0.6318668127059937 + 2.0 * 6.08742094039917
Epoch 470, val loss: 0.8928947448730469
Epoch 480, training loss: 12.77617359161377 = 0.6059166789054871 + 2.0 * 6.085128307342529
Epoch 480, val loss: 0.8751537203788757
Epoch 490, training loss: 12.748544692993164 = 0.5810329914093018 + 2.0 * 6.083755970001221
Epoch 490, val loss: 0.8587210178375244
Epoch 500, training loss: 12.717913627624512 = 0.5569174289703369 + 2.0 * 6.080498218536377
Epoch 500, val loss: 0.8431393504142761
Epoch 510, training loss: 12.695198059082031 = 0.5332610011100769 + 2.0 * 6.080968379974365
Epoch 510, val loss: 0.8283575177192688
Epoch 520, training loss: 12.674590110778809 = 0.5101613998413086 + 2.0 * 6.08221435546875
Epoch 520, val loss: 0.8145695328712463
Epoch 530, training loss: 12.639806747436523 = 0.48780617117881775 + 2.0 * 6.076000213623047
Epoch 530, val loss: 0.8015168905258179
Epoch 540, training loss: 12.613876342773438 = 0.46579742431640625 + 2.0 * 6.074039459228516
Epoch 540, val loss: 0.7892476320266724
Epoch 550, training loss: 12.589984893798828 = 0.4440157115459442 + 2.0 * 6.07298469543457
Epoch 550, val loss: 0.7775801420211792
Epoch 560, training loss: 12.568929672241211 = 0.4226342439651489 + 2.0 * 6.073147773742676
Epoch 560, val loss: 0.7665229439735413
Epoch 570, training loss: 12.546259880065918 = 0.4018855392932892 + 2.0 * 6.0721869468688965
Epoch 570, val loss: 0.7562612891197205
Epoch 580, training loss: 12.519220352172852 = 0.3816687762737274 + 2.0 * 6.068775653839111
Epoch 580, val loss: 0.7469087839126587
Epoch 590, training loss: 12.49572467803955 = 0.36198049783706665 + 2.0 * 6.0668721199035645
Epoch 590, val loss: 0.7382321357727051
Epoch 600, training loss: 12.475056648254395 = 0.3427882790565491 + 2.0 * 6.066133975982666
Epoch 600, val loss: 0.7302872538566589
Epoch 610, training loss: 12.4734468460083 = 0.32448679208755493 + 2.0 * 6.074480056762695
Epoch 610, val loss: 0.7228845953941345
Epoch 620, training loss: 12.441285133361816 = 0.307168185710907 + 2.0 * 6.067058563232422
Epoch 620, val loss: 0.7167496681213379
Epoch 630, training loss: 12.414259910583496 = 0.2906412184238434 + 2.0 * 6.061809539794922
Epoch 630, val loss: 0.7114325165748596
Epoch 640, training loss: 12.395670890808105 = 0.27487537264823914 + 2.0 * 6.060397624969482
Epoch 640, val loss: 0.7067217826843262
Epoch 650, training loss: 12.384913444519043 = 0.2599067986011505 + 2.0 * 6.062503337860107
Epoch 650, val loss: 0.7028017640113831
Epoch 660, training loss: 12.370498657226562 = 0.24584966897964478 + 2.0 * 6.062324523925781
Epoch 660, val loss: 0.6994721293449402
Epoch 670, training loss: 12.346281051635742 = 0.23276710510253906 + 2.0 * 6.056756973266602
Epoch 670, val loss: 0.6970861554145813
Epoch 680, training loss: 12.3363676071167 = 0.22047334909439087 + 2.0 * 6.057947158813477
Epoch 680, val loss: 0.6953473091125488
Epoch 690, training loss: 12.320453643798828 = 0.20897042751312256 + 2.0 * 6.055741786956787
Epoch 690, val loss: 0.6940882205963135
Epoch 700, training loss: 12.306281089782715 = 0.19827397167682648 + 2.0 * 6.054003715515137
Epoch 700, val loss: 0.693497896194458
Epoch 710, training loss: 12.29372501373291 = 0.18824197351932526 + 2.0 * 6.052741527557373
Epoch 710, val loss: 0.6933712363243103
Epoch 720, training loss: 12.295945167541504 = 0.1788817197084427 + 2.0 * 6.058531761169434
Epoch 720, val loss: 0.6936305165290833
Epoch 730, training loss: 12.276065826416016 = 0.17021167278289795 + 2.0 * 6.052927017211914
Epoch 730, val loss: 0.6943603754043579
Epoch 740, training loss: 12.270849227905273 = 0.16212834417819977 + 2.0 * 6.054360389709473
Epoch 740, val loss: 0.6956179141998291
Epoch 750, training loss: 12.259947776794434 = 0.1545572280883789 + 2.0 * 6.052695274353027
Epoch 750, val loss: 0.6969015598297119
Epoch 760, training loss: 12.243144035339355 = 0.14753641188144684 + 2.0 * 6.04780387878418
Epoch 760, val loss: 0.6987490057945251
Epoch 770, training loss: 12.233226776123047 = 0.14089785516262054 + 2.0 * 6.046164512634277
Epoch 770, val loss: 0.7008233666419983
Epoch 780, training loss: 12.224906921386719 = 0.13463114202022552 + 2.0 * 6.045137882232666
Epoch 780, val loss: 0.703086256980896
Epoch 790, training loss: 12.230457305908203 = 0.1287471204996109 + 2.0 * 6.0508551597595215
Epoch 790, val loss: 0.7055354714393616
Epoch 800, training loss: 12.214941024780273 = 0.12323085963726044 + 2.0 * 6.0458550453186035
Epoch 800, val loss: 0.7080514430999756
Epoch 810, training loss: 12.203778266906738 = 0.11806274205446243 + 2.0 * 6.042857646942139
Epoch 810, val loss: 0.7109729051589966
Epoch 820, training loss: 12.198172569274902 = 0.11315406113862991 + 2.0 * 6.042509078979492
Epoch 820, val loss: 0.7139754891395569
Epoch 830, training loss: 12.19693660736084 = 0.10850918292999268 + 2.0 * 6.044213771820068
Epoch 830, val loss: 0.7171096205711365
Epoch 840, training loss: 12.187825202941895 = 0.10411755740642548 + 2.0 * 6.041853904724121
Epoch 840, val loss: 0.7203649878501892
Epoch 850, training loss: 12.185717582702637 = 0.09999381750822067 + 2.0 * 6.0428619384765625
Epoch 850, val loss: 0.7237959504127502
Epoch 860, training loss: 12.176501274108887 = 0.09609250724315643 + 2.0 * 6.0402045249938965
Epoch 860, val loss: 0.7272973656654358
Epoch 870, training loss: 12.167062759399414 = 0.09237584471702576 + 2.0 * 6.037343502044678
Epoch 870, val loss: 0.7309159636497498
Epoch 880, training loss: 12.168875694274902 = 0.08883605152368546 + 2.0 * 6.040019989013672
Epoch 880, val loss: 0.7346020340919495
Epoch 890, training loss: 12.164765357971191 = 0.0854812040925026 + 2.0 * 6.039641857147217
Epoch 890, val loss: 0.73836749792099
Epoch 900, training loss: 12.154109954833984 = 0.08232267200946808 + 2.0 * 6.035893440246582
Epoch 900, val loss: 0.7422056794166565
Epoch 910, training loss: 12.147205352783203 = 0.07928959280252457 + 2.0 * 6.0339579582214355
Epoch 910, val loss: 0.746150016784668
Epoch 920, training loss: 12.149256706237793 = 0.0763879120349884 + 2.0 * 6.036434173583984
Epoch 920, val loss: 0.7501193284988403
Epoch 930, training loss: 12.143339157104492 = 0.07365306466817856 + 2.0 * 6.0348429679870605
Epoch 930, val loss: 0.7540222406387329
Epoch 940, training loss: 12.137995719909668 = 0.0710589662194252 + 2.0 * 6.033468246459961
Epoch 940, val loss: 0.758120596408844
Epoch 950, training loss: 12.13198471069336 = 0.06858664751052856 + 2.0 * 6.031699180603027
Epoch 950, val loss: 0.7623003721237183
Epoch 960, training loss: 12.128049850463867 = 0.06621331721544266 + 2.0 * 6.030918121337891
Epoch 960, val loss: 0.7665093541145325
Epoch 970, training loss: 12.137099266052246 = 0.06394136697053909 + 2.0 * 6.036579132080078
Epoch 970, val loss: 0.7707340121269226
Epoch 980, training loss: 12.125371932983398 = 0.0617879293859005 + 2.0 * 6.031792163848877
Epoch 980, val loss: 0.7749252915382385
Epoch 990, training loss: 12.121599197387695 = 0.05974077805876732 + 2.0 * 6.030929088592529
Epoch 990, val loss: 0.7790862321853638
Epoch 1000, training loss: 12.117546081542969 = 0.05778686702251434 + 2.0 * 6.029879570007324
Epoch 1000, val loss: 0.7833387851715088
Epoch 1010, training loss: 12.110587120056152 = 0.0559193380177021 + 2.0 * 6.027333736419678
Epoch 1010, val loss: 0.7876309752464294
Epoch 1020, training loss: 12.108402252197266 = 0.05412251129746437 + 2.0 * 6.027139663696289
Epoch 1020, val loss: 0.7919595241546631
Epoch 1030, training loss: 12.120991706848145 = 0.05239999294281006 + 2.0 * 6.034296035766602
Epoch 1030, val loss: 0.7963083982467651
Epoch 1040, training loss: 12.109029769897461 = 0.050768718123435974 + 2.0 * 6.029130458831787
Epoch 1040, val loss: 0.800413966178894
Epoch 1050, training loss: 12.102375984191895 = 0.04920854791998863 + 2.0 * 6.026583671569824
Epoch 1050, val loss: 0.8047881126403809
Epoch 1060, training loss: 12.109125137329102 = 0.047725312411785126 + 2.0 * 6.030699729919434
Epoch 1060, val loss: 0.8089596033096313
Epoch 1070, training loss: 12.096076965332031 = 0.046297620981931686 + 2.0 * 6.0248894691467285
Epoch 1070, val loss: 0.8131593465805054
Epoch 1080, training loss: 12.093082427978516 = 0.04493354260921478 + 2.0 * 6.024074554443359
Epoch 1080, val loss: 0.817484438419342
Epoch 1090, training loss: 12.090302467346191 = 0.04361971467733383 + 2.0 * 6.023341178894043
Epoch 1090, val loss: 0.8217563629150391
Epoch 1100, training loss: 12.096054077148438 = 0.0423482283949852 + 2.0 * 6.026853084564209
Epoch 1100, val loss: 0.8259926438331604
Epoch 1110, training loss: 12.087016105651855 = 0.041139136999845505 + 2.0 * 6.022938251495361
Epoch 1110, val loss: 0.8301672339439392
Epoch 1120, training loss: 12.084848403930664 = 0.03997459262609482 + 2.0 * 6.02243709564209
Epoch 1120, val loss: 0.8344241380691528
Epoch 1130, training loss: 12.089673042297363 = 0.038856249302625656 + 2.0 * 6.0254082679748535
Epoch 1130, val loss: 0.8385837078094482
Epoch 1140, training loss: 12.082671165466309 = 0.03778383508324623 + 2.0 * 6.022443771362305
Epoch 1140, val loss: 0.842835009098053
Epoch 1150, training loss: 12.086413383483887 = 0.03675529360771179 + 2.0 * 6.024828910827637
Epoch 1150, val loss: 0.8469419479370117
Epoch 1160, training loss: 12.077013969421387 = 0.03577682003378868 + 2.0 * 6.020618438720703
Epoch 1160, val loss: 0.8510887622833252
Epoch 1170, training loss: 12.074625968933105 = 0.03482862561941147 + 2.0 * 6.019898891448975
Epoch 1170, val loss: 0.8552316427230835
Epoch 1180, training loss: 12.07217788696289 = 0.03391493856906891 + 2.0 * 6.019131660461426
Epoch 1180, val loss: 0.859347939491272
Epoch 1190, training loss: 12.081997871398926 = 0.03303627297282219 + 2.0 * 6.024480819702148
Epoch 1190, val loss: 0.863445520401001
Epoch 1200, training loss: 12.074645042419434 = 0.032182395458221436 + 2.0 * 6.021231174468994
Epoch 1200, val loss: 0.867303729057312
Epoch 1210, training loss: 12.069019317626953 = 0.031372830271720886 + 2.0 * 6.018823146820068
Epoch 1210, val loss: 0.8713307976722717
Epoch 1220, training loss: 12.066812515258789 = 0.030584562569856644 + 2.0 * 6.01811408996582
Epoch 1220, val loss: 0.8752596974372864
Epoch 1230, training loss: 12.073176383972168 = 0.02982657589018345 + 2.0 * 6.021675109863281
Epoch 1230, val loss: 0.8791345953941345
Epoch 1240, training loss: 12.067558288574219 = 0.029102185741066933 + 2.0 * 6.019227981567383
Epoch 1240, val loss: 0.8829807043075562
Epoch 1250, training loss: 12.062515258789062 = 0.028401456773281097 + 2.0 * 6.017056941986084
Epoch 1250, val loss: 0.8867830634117126
Epoch 1260, training loss: 12.060094833374023 = 0.027725063264369965 + 2.0 * 6.0161848068237305
Epoch 1260, val loss: 0.8905788064002991
Epoch 1270, training loss: 12.06532096862793 = 0.027067940682172775 + 2.0 * 6.0191264152526855
Epoch 1270, val loss: 0.8943493366241455
Epoch 1280, training loss: 12.060233116149902 = 0.02643323689699173 + 2.0 * 6.016900062561035
Epoch 1280, val loss: 0.8980022668838501
Epoch 1290, training loss: 12.059149742126465 = 0.025827713310718536 + 2.0 * 6.016661167144775
Epoch 1290, val loss: 0.901699960231781
Epoch 1300, training loss: 12.059901237487793 = 0.025243625044822693 + 2.0 * 6.01732873916626
Epoch 1300, val loss: 0.905254065990448
Epoch 1310, training loss: 12.054141998291016 = 0.024673888459801674 + 2.0 * 6.014734268188477
Epoch 1310, val loss: 0.9088607430458069
Epoch 1320, training loss: 12.050281524658203 = 0.024125654250383377 + 2.0 * 6.013077735900879
Epoch 1320, val loss: 0.9124963879585266
Epoch 1330, training loss: 12.05328369140625 = 0.023590417578816414 + 2.0 * 6.0148468017578125
Epoch 1330, val loss: 0.916037917137146
Epoch 1340, training loss: 12.05722427368164 = 0.023071978241205215 + 2.0 * 6.017076015472412
Epoch 1340, val loss: 0.9193750023841858
Epoch 1350, training loss: 12.050865173339844 = 0.022583546116948128 + 2.0 * 6.014140605926514
Epoch 1350, val loss: 0.9228909015655518
Epoch 1360, training loss: 12.045973777770996 = 0.022102659568190575 + 2.0 * 6.011935710906982
Epoch 1360, val loss: 0.9263268709182739
Epoch 1370, training loss: 12.044626235961914 = 0.02163555845618248 + 2.0 * 6.011495113372803
Epoch 1370, val loss: 0.9297253489494324
Epoch 1380, training loss: 12.075013160705566 = 0.021184049546718597 + 2.0 * 6.026914596557617
Epoch 1380, val loss: 0.9329932332038879
Epoch 1390, training loss: 12.047767639160156 = 0.020747490227222443 + 2.0 * 6.013510227203369
Epoch 1390, val loss: 0.9362354278564453
Epoch 1400, training loss: 12.041739463806152 = 0.02032938413321972 + 2.0 * 6.01070499420166
Epoch 1400, val loss: 0.9396319389343262
Epoch 1410, training loss: 12.040650367736816 = 0.019919326528906822 + 2.0 * 6.0103654861450195
Epoch 1410, val loss: 0.9428606033325195
Epoch 1420, training loss: 12.040538787841797 = 0.019517742097377777 + 2.0 * 6.010510444641113
Epoch 1420, val loss: 0.9461784958839417
Epoch 1430, training loss: 12.04935073852539 = 0.019132215529680252 + 2.0 * 6.015109062194824
Epoch 1430, val loss: 0.949203372001648
Epoch 1440, training loss: 12.040549278259277 = 0.01876005157828331 + 2.0 * 6.010894775390625
Epoch 1440, val loss: 0.9522320628166199
Epoch 1450, training loss: 12.0370454788208 = 0.01839684322476387 + 2.0 * 6.009324550628662
Epoch 1450, val loss: 0.9554182291030884
Epoch 1460, training loss: 12.035262107849121 = 0.018043000251054764 + 2.0 * 6.008609771728516
Epoch 1460, val loss: 0.9585228562355042
Epoch 1470, training loss: 12.041210174560547 = 0.017697684466838837 + 2.0 * 6.011756420135498
Epoch 1470, val loss: 0.9616584777832031
Epoch 1480, training loss: 12.035837173461914 = 0.017360303550958633 + 2.0 * 6.009238243103027
Epoch 1480, val loss: 0.964478075504303
Epoch 1490, training loss: 12.03943157196045 = 0.01704055443406105 + 2.0 * 6.011195659637451
Epoch 1490, val loss: 0.9675418138504028
Epoch 1500, training loss: 12.031720161437988 = 0.01672503724694252 + 2.0 * 6.007497787475586
Epoch 1500, val loss: 0.9704660177230835
Epoch 1510, training loss: 12.032320976257324 = 0.016417894512414932 + 2.0 * 6.007951736450195
Epoch 1510, val loss: 0.9734412431716919
Epoch 1520, training loss: 12.039216995239258 = 0.016118697822093964 + 2.0 * 6.01154899597168
Epoch 1520, val loss: 0.9763385057449341
Epoch 1530, training loss: 12.034863471984863 = 0.015827957540750504 + 2.0 * 6.009517669677734
Epoch 1530, val loss: 0.9792078137397766
Epoch 1540, training loss: 12.032403945922852 = 0.01554548554122448 + 2.0 * 6.008429050445557
Epoch 1540, val loss: 0.9820812344551086
Epoch 1550, training loss: 12.027814865112305 = 0.015269544906914234 + 2.0 * 6.006272792816162
Epoch 1550, val loss: 0.9849302172660828
Epoch 1560, training loss: 12.028411865234375 = 0.015000537037849426 + 2.0 * 6.0067057609558105
Epoch 1560, val loss: 0.9877797365188599
Epoch 1570, training loss: 12.037857055664062 = 0.014739475212991238 + 2.0 * 6.011559009552002
Epoch 1570, val loss: 0.9905641674995422
Epoch 1580, training loss: 12.031694412231445 = 0.014489026740193367 + 2.0 * 6.008602619171143
Epoch 1580, val loss: 0.9931541085243225
Epoch 1590, training loss: 12.025690078735352 = 0.01424199715256691 + 2.0 * 6.00572395324707
Epoch 1590, val loss: 0.9958980083465576
Epoch 1600, training loss: 12.02401351928711 = 0.014003281481564045 + 2.0 * 6.0050048828125
Epoch 1600, val loss: 0.9986717104911804
Epoch 1610, training loss: 12.023404121398926 = 0.013766403310000896 + 2.0 * 6.004818916320801
Epoch 1610, val loss: 1.0013847351074219
Epoch 1620, training loss: 12.032752990722656 = 0.013535079546272755 + 2.0 * 6.009608745574951
Epoch 1620, val loss: 1.003983736038208
Epoch 1630, training loss: 12.02448844909668 = 0.013312505558133125 + 2.0 * 6.005588054656982
Epoch 1630, val loss: 1.006646752357483
Epoch 1640, training loss: 12.023405075073242 = 0.013095091097056866 + 2.0 * 6.005155086517334
Epoch 1640, val loss: 1.0093064308166504
Epoch 1650, training loss: 12.02672004699707 = 0.012883597053587437 + 2.0 * 6.006918430328369
Epoch 1650, val loss: 1.0118989944458008
Epoch 1660, training loss: 12.032950401306152 = 0.012677434831857681 + 2.0 * 6.010136604309082
Epoch 1660, val loss: 1.0143762826919556
Epoch 1670, training loss: 12.022884368896484 = 0.012477938085794449 + 2.0 * 6.0052032470703125
Epoch 1670, val loss: 1.0168942213058472
Epoch 1680, training loss: 12.019145965576172 = 0.012282036244869232 + 2.0 * 6.003431797027588
Epoch 1680, val loss: 1.0195201635360718
Epoch 1690, training loss: 12.029400825500488 = 0.012088320218026638 + 2.0 * 6.008656024932861
Epoch 1690, val loss: 1.0219460725784302
Epoch 1700, training loss: 12.017488479614258 = 0.011904475279152393 + 2.0 * 6.002791881561279
Epoch 1700, val loss: 1.0244401693344116
Epoch 1710, training loss: 12.016339302062988 = 0.011720150709152222 + 2.0 * 6.002309799194336
Epoch 1710, val loss: 1.026943325996399
Epoch 1720, training loss: 12.033679962158203 = 0.01154063269495964 + 2.0 * 6.0110697746276855
Epoch 1720, val loss: 1.029308557510376
Epoch 1730, training loss: 12.019319534301758 = 0.011370208114385605 + 2.0 * 6.003974437713623
Epoch 1730, val loss: 1.0317211151123047
Epoch 1740, training loss: 12.015303611755371 = 0.011198487132787704 + 2.0 * 6.0020527839660645
Epoch 1740, val loss: 1.0341943502426147
Epoch 1750, training loss: 12.013676643371582 = 0.01103227399289608 + 2.0 * 6.001322269439697
Epoch 1750, val loss: 1.0366932153701782
Epoch 1760, training loss: 12.023355484008789 = 0.010866962373256683 + 2.0 * 6.00624418258667
Epoch 1760, val loss: 1.039072036743164
Epoch 1770, training loss: 12.015092849731445 = 0.010707690380513668 + 2.0 * 6.002192497253418
Epoch 1770, val loss: 1.04132878780365
Epoch 1780, training loss: 12.01813793182373 = 0.010552231222391129 + 2.0 * 6.003792762756348
Epoch 1780, val loss: 1.043720006942749
Epoch 1790, training loss: 12.012587547302246 = 0.010401690378785133 + 2.0 * 6.001092910766602
Epoch 1790, val loss: 1.0460025072097778
Epoch 1800, training loss: 12.010584831237793 = 0.010253890417516232 + 2.0 * 6.0001654624938965
Epoch 1800, val loss: 1.0483351945877075
Epoch 1810, training loss: 12.01024055480957 = 0.010107888840138912 + 2.0 * 6.00006628036499
Epoch 1810, val loss: 1.0506787300109863
Epoch 1820, training loss: 12.018965721130371 = 0.009963263757526875 + 2.0 * 6.0045013427734375
Epoch 1820, val loss: 1.0529345273971558
Epoch 1830, training loss: 12.011465072631836 = 0.009824458509683609 + 2.0 * 6.000820159912109
Epoch 1830, val loss: 1.055139422416687
Epoch 1840, training loss: 12.01093864440918 = 0.009688231162726879 + 2.0 * 6.000625133514404
Epoch 1840, val loss: 1.057366132736206
Epoch 1850, training loss: 12.011842727661133 = 0.009556123986840248 + 2.0 * 6.001143455505371
Epoch 1850, val loss: 1.0596575736999512
Epoch 1860, training loss: 12.012877464294434 = 0.009426522068679333 + 2.0 * 6.001725673675537
Epoch 1860, val loss: 1.0618308782577515
Epoch 1870, training loss: 12.009742736816406 = 0.009300838224589825 + 2.0 * 6.000220775604248
Epoch 1870, val loss: 1.0639418363571167
Epoch 1880, training loss: 12.006338119506836 = 0.009175264276564121 + 2.0 * 5.998581409454346
Epoch 1880, val loss: 1.0661247968673706
Epoch 1890, training loss: 12.004201889038086 = 0.009053253568708897 + 2.0 * 5.997574329376221
Epoch 1890, val loss: 1.0683667659759521
Epoch 1900, training loss: 12.004124641418457 = 0.008931386284530163 + 2.0 * 5.997596740722656
Epoch 1900, val loss: 1.0705502033233643
Epoch 1910, training loss: 12.023253440856934 = 0.00881359726190567 + 2.0 * 6.0072197914123535
Epoch 1910, val loss: 1.0726759433746338
Epoch 1920, training loss: 12.010784149169922 = 0.008696428500115871 + 2.0 * 6.001043796539307
Epoch 1920, val loss: 1.0746411085128784
Epoch 1930, training loss: 12.007896423339844 = 0.008586541749536991 + 2.0 * 5.999654769897461
Epoch 1930, val loss: 1.0768578052520752
Epoch 1940, training loss: 12.01091194152832 = 0.008475943468511105 + 2.0 * 6.001217842102051
Epoch 1940, val loss: 1.078934907913208
Epoch 1950, training loss: 12.001702308654785 = 0.008367528207600117 + 2.0 * 5.996667385101318
Epoch 1950, val loss: 1.0809308290481567
Epoch 1960, training loss: 12.00593376159668 = 0.008261353708803654 + 2.0 * 5.998836040496826
Epoch 1960, val loss: 1.083048939704895
Epoch 1970, training loss: 12.005204200744629 = 0.00815677922219038 + 2.0 * 5.998523712158203
Epoch 1970, val loss: 1.0851688385009766
Epoch 1980, training loss: 12.00505542755127 = 0.00805503036826849 + 2.0 * 5.998500347137451
Epoch 1980, val loss: 1.0871822834014893
Epoch 1990, training loss: 12.004828453063965 = 0.007954333908855915 + 2.0 * 5.99843692779541
Epoch 1990, val loss: 1.089168906211853
=== picking the best model according to the performance on validation ===
target class rate on Vs: 0.8000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.7306
Flip ASR: 0.6844/225 nodes
The final ASR:0.64576, 0.06313, Accuracy:0.82222, 0.01512
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11586])
remove edge: torch.Size([2, 9558])
updated graph: torch.Size([2, 10588])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
