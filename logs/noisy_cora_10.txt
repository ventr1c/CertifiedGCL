Begin epxeriment: cont_weight: 10 epoch:1000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0001, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11584])
remove edge: torch.Size([2, 9510])
updated graph: torch.Size([2, 10538])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:224: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 87.92494201660156 = 1.9565035104751587 + 10.0 * 8.596843719482422
Epoch 0, val loss: 1.9564464092254639
Epoch 10, training loss: 87.91773223876953 = 1.951182246208191 + 10.0 * 8.596654891967773
Epoch 10, val loss: 1.9509087800979614
Epoch 20, training loss: 87.9056625366211 = 1.9453074932098389 + 10.0 * 8.596035957336426
Epoch 20, val loss: 1.9447168111801147
Epoch 30, training loss: 87.87554168701172 = 1.9384207725524902 + 10.0 * 8.593711853027344
Epoch 30, val loss: 1.9374061822891235
Epoch 40, training loss: 87.77788543701172 = 1.9301494359970093 + 10.0 * 8.584773063659668
Epoch 40, val loss: 1.9286494255065918
Epoch 50, training loss: 87.4604263305664 = 1.9202957153320312 + 10.0 * 8.5540132522583
Epoch 50, val loss: 1.9183011054992676
Epoch 60, training loss: 86.51020812988281 = 1.9089949131011963 + 10.0 * 8.460121154785156
Epoch 60, val loss: 1.9067418575286865
Epoch 70, training loss: 83.83170318603516 = 1.897534728050232 + 10.0 * 8.193416595458984
Epoch 70, val loss: 1.895020842552185
Epoch 80, training loss: 79.27213287353516 = 1.8857251405715942 + 10.0 * 7.738640308380127
Epoch 80, val loss: 1.8833656311035156
Epoch 90, training loss: 77.2481918334961 = 1.8779691457748413 + 10.0 * 7.537022590637207
Epoch 90, val loss: 1.8763779401779175
Epoch 100, training loss: 75.60481262207031 = 1.8726834058761597 + 10.0 * 7.373212814331055
Epoch 100, val loss: 1.8717423677444458
Epoch 110, training loss: 74.16384887695312 = 1.867931604385376 + 10.0 * 7.229591369628906
Epoch 110, val loss: 1.8674230575561523
Epoch 120, training loss: 72.47718811035156 = 1.863568663597107 + 10.0 * 7.061361789703369
Epoch 120, val loss: 1.8635801076889038
Epoch 130, training loss: 70.97281646728516 = 1.8594187498092651 + 10.0 * 6.91133975982666
Epoch 130, val loss: 1.8598190546035767
Epoch 140, training loss: 69.78704833984375 = 1.8554866313934326 + 10.0 * 6.793156147003174
Epoch 140, val loss: 1.8564180135726929
Epoch 150, training loss: 69.0034408569336 = 1.8510754108428955 + 10.0 * 6.715236663818359
Epoch 150, val loss: 1.8524196147918701
Epoch 160, training loss: 68.4887924194336 = 1.845771312713623 + 10.0 * 6.664302349090576
Epoch 160, val loss: 1.847688913345337
Epoch 170, training loss: 68.09181213378906 = 1.8404954671859741 + 10.0 * 6.625132083892822
Epoch 170, val loss: 1.843039631843567
Epoch 180, training loss: 67.76592254638672 = 1.8353219032287598 + 10.0 * 6.593060493469238
Epoch 180, val loss: 1.8385425806045532
Epoch 190, training loss: 67.50724792480469 = 1.830106496810913 + 10.0 * 6.567713737487793
Epoch 190, val loss: 1.833958625793457
Epoch 200, training loss: 67.29385375976562 = 1.8248876333236694 + 10.0 * 6.546896457672119
Epoch 200, val loss: 1.8294494152069092
Epoch 210, training loss: 67.10369110107422 = 1.8198105096817017 + 10.0 * 6.528388023376465
Epoch 210, val loss: 1.825069785118103
Epoch 220, training loss: 66.93144226074219 = 1.814816951751709 + 10.0 * 6.511662483215332
Epoch 220, val loss: 1.8207659721374512
Epoch 230, training loss: 66.79615783691406 = 1.809928059577942 + 10.0 * 6.498622894287109
Epoch 230, val loss: 1.8165156841278076
Epoch 240, training loss: 66.64018249511719 = 1.8050167560577393 + 10.0 * 6.483516693115234
Epoch 240, val loss: 1.8123317956924438
Epoch 250, training loss: 66.49849700927734 = 1.800118088722229 + 10.0 * 6.4698381423950195
Epoch 250, val loss: 1.8081849813461304
Epoch 260, training loss: 66.38560485839844 = 1.7951842546463013 + 10.0 * 6.459042549133301
Epoch 260, val loss: 1.8039779663085938
Epoch 270, training loss: 66.272705078125 = 1.7902092933654785 + 10.0 * 6.448249816894531
Epoch 270, val loss: 1.799733281135559
Epoch 280, training loss: 66.17277526855469 = 1.785157322883606 + 10.0 * 6.438762187957764
Epoch 280, val loss: 1.79542875289917
Epoch 290, training loss: 66.07825469970703 = 1.7800004482269287 + 10.0 * 6.429825782775879
Epoch 290, val loss: 1.7910181283950806
Epoch 300, training loss: 65.994873046875 = 1.774705171585083 + 10.0 * 6.4220170974731445
Epoch 300, val loss: 1.7864782810211182
Epoch 310, training loss: 65.92688751220703 = 1.769221305847168 + 10.0 * 6.415767192840576
Epoch 310, val loss: 1.78179132938385
Epoch 320, training loss: 65.84422302246094 = 1.7635688781738281 + 10.0 * 6.408065319061279
Epoch 320, val loss: 1.7769352197647095
Epoch 330, training loss: 65.80960083007812 = 1.757660150527954 + 10.0 * 6.40519380569458
Epoch 330, val loss: 1.7718971967697144
Epoch 340, training loss: 65.70729064941406 = 1.7515841722488403 + 10.0 * 6.395570278167725
Epoch 340, val loss: 1.7666290998458862
Epoch 350, training loss: 65.64469146728516 = 1.7452627420425415 + 10.0 * 6.389942646026611
Epoch 350, val loss: 1.7611510753631592
Epoch 360, training loss: 65.58560180664062 = 1.7386693954467773 + 10.0 * 6.384693145751953
Epoch 360, val loss: 1.7554296255111694
Epoch 370, training loss: 65.5405044555664 = 1.7317548990249634 + 10.0 * 6.380875110626221
Epoch 370, val loss: 1.7494477033615112
Epoch 380, training loss: 65.47618865966797 = 1.7245838642120361 + 10.0 * 6.375160217285156
Epoch 380, val loss: 1.7431755065917969
Epoch 390, training loss: 65.419677734375 = 1.7170723676681519 + 10.0 * 6.370260715484619
Epoch 390, val loss: 1.7366132736206055
Epoch 400, training loss: 65.3665771484375 = 1.709243655204773 + 10.0 * 6.3657331466674805
Epoch 400, val loss: 1.7297451496124268
Epoch 410, training loss: 65.3930435180664 = 1.7011287212371826 + 10.0 * 6.369191646575928
Epoch 410, val loss: 1.722564697265625
Epoch 420, training loss: 65.2812728881836 = 1.692574381828308 + 10.0 * 6.358870029449463
Epoch 420, val loss: 1.715100646018982
Epoch 430, training loss: 65.22267150878906 = 1.6837197542190552 + 10.0 * 6.3538947105407715
Epoch 430, val loss: 1.7073522806167603
Epoch 440, training loss: 65.17533874511719 = 1.6745587587356567 + 10.0 * 6.350078105926514
Epoch 440, val loss: 1.699292540550232
Epoch 450, training loss: 65.1277084350586 = 1.6650660037994385 + 10.0 * 6.346263885498047
Epoch 450, val loss: 1.690933346748352
Epoch 460, training loss: 65.08293914794922 = 1.6552218198776245 + 10.0 * 6.342772006988525
Epoch 460, val loss: 1.6822702884674072
Epoch 470, training loss: 65.04912567138672 = 1.6450316905975342 + 10.0 * 6.340409278869629
Epoch 470, val loss: 1.6733014583587646
Epoch 480, training loss: 65.02246856689453 = 1.6345536708831787 + 10.0 * 6.338791847229004
Epoch 480, val loss: 1.6640701293945312
Epoch 490, training loss: 64.95980072021484 = 1.6237351894378662 + 10.0 * 6.333606243133545
Epoch 490, val loss: 1.6545606851577759
Epoch 500, training loss: 64.91702270507812 = 1.6126620769500732 + 10.0 * 6.330435752868652
Epoch 500, val loss: 1.644824504852295
Epoch 510, training loss: 64.87657928466797 = 1.60128915309906 + 10.0 * 6.327528953552246
Epoch 510, val loss: 1.634843111038208
Epoch 520, training loss: 64.83843231201172 = 1.589660882949829 + 10.0 * 6.3248772621154785
Epoch 520, val loss: 1.624653935432434
Epoch 530, training loss: 64.84117126464844 = 1.577811360359192 + 10.0 * 6.32633638381958
Epoch 530, val loss: 1.6142492294311523
Epoch 540, training loss: 64.77298736572266 = 1.5656806230545044 + 10.0 * 6.320730686187744
Epoch 540, val loss: 1.6036885976791382
Epoch 550, training loss: 64.7320785522461 = 1.553417682647705 + 10.0 * 6.31786584854126
Epoch 550, val loss: 1.5930582284927368
Epoch 560, training loss: 64.69532012939453 = 1.5410224199295044 + 10.0 * 6.3154296875
Epoch 560, val loss: 1.5823310613632202
Epoch 570, training loss: 64.65945434570312 = 1.5284901857376099 + 10.0 * 6.313096523284912
Epoch 570, val loss: 1.5715153217315674
Epoch 580, training loss: 64.62518310546875 = 1.5158201456069946 + 10.0 * 6.310936450958252
Epoch 580, val loss: 1.560631513595581
Epoch 590, training loss: 64.59219360351562 = 1.5030326843261719 + 10.0 * 6.308916091918945
Epoch 590, val loss: 1.5497207641601562
Epoch 600, training loss: 64.5907211303711 = 1.4901454448699951 + 10.0 * 6.310057640075684
Epoch 600, val loss: 1.5387908220291138
Epoch 610, training loss: 64.53990173339844 = 1.477231740951538 + 10.0 * 6.306266784667969
Epoch 610, val loss: 1.5278884172439575
Epoch 620, training loss: 64.49971008300781 = 1.464263916015625 + 10.0 * 6.303544998168945
Epoch 620, val loss: 1.5170326232910156
Epoch 630, training loss: 64.46597290039062 = 1.4512817859649658 + 10.0 * 6.301469326019287
Epoch 630, val loss: 1.5062587261199951
Epoch 640, training loss: 64.44086456298828 = 1.4382613897323608 + 10.0 * 6.300260543823242
Epoch 640, val loss: 1.4955055713653564
Epoch 650, training loss: 64.42162322998047 = 1.4252383708953857 + 10.0 * 6.299638748168945
Epoch 650, val loss: 1.4848840236663818
Epoch 660, training loss: 64.3746109008789 = 1.412268042564392 + 10.0 * 6.296234607696533
Epoch 660, val loss: 1.4743199348449707
Epoch 670, training loss: 64.34657287597656 = 1.399308681488037 + 10.0 * 6.294726371765137
Epoch 670, val loss: 1.4638906717300415
Epoch 680, training loss: 64.31803131103516 = 1.3863581418991089 + 10.0 * 6.293167591094971
Epoch 680, val loss: 1.453584909439087
Epoch 690, training loss: 64.29037475585938 = 1.373436689376831 + 10.0 * 6.291693687438965
Epoch 690, val loss: 1.4434280395507812
Epoch 700, training loss: 64.28388214111328 = 1.3605302572250366 + 10.0 * 6.292335510253906
Epoch 700, val loss: 1.4334030151367188
Epoch 710, training loss: 64.2376708984375 = 1.3477537631988525 + 10.0 * 6.288991928100586
Epoch 710, val loss: 1.4235690832138062
Epoch 720, training loss: 64.22887420654297 = 1.335003137588501 + 10.0 * 6.289386749267578
Epoch 720, val loss: 1.4138907194137573
Epoch 730, training loss: 64.19622039794922 = 1.3223260641098022 + 10.0 * 6.287389278411865
Epoch 730, val loss: 1.404330849647522
Epoch 740, training loss: 64.15642547607422 = 1.3097087144851685 + 10.0 * 6.284671306610107
Epoch 740, val loss: 1.3949776887893677
Epoch 750, training loss: 64.13135528564453 = 1.297139286994934 + 10.0 * 6.283421516418457
Epoch 750, val loss: 1.3857263326644897
Epoch 760, training loss: 64.10498046875 = 1.2846161127090454 + 10.0 * 6.282036781311035
Epoch 760, val loss: 1.3766212463378906
Epoch 770, training loss: 64.1197738647461 = 1.2721620798110962 + 10.0 * 6.284761428833008
Epoch 770, val loss: 1.3676934242248535
Epoch 780, training loss: 64.07917785644531 = 1.259628176689148 + 10.0 * 6.281955242156982
Epoch 780, val loss: 1.3587360382080078
Epoch 790, training loss: 64.0356216430664 = 1.2472611665725708 + 10.0 * 6.278835773468018
Epoch 790, val loss: 1.3500139713287354
Epoch 800, training loss: 64.00971984863281 = 1.2349308729171753 + 10.0 * 6.2774786949157715
Epoch 800, val loss: 1.3414582014083862
Epoch 810, training loss: 63.985382080078125 = 1.2226120233535767 + 10.0 * 6.2762770652771
Epoch 810, val loss: 1.3329691886901855
Epoch 820, training loss: 64.0055923461914 = 1.210328221321106 + 10.0 * 6.279526233673096
Epoch 820, val loss: 1.3245803117752075
Epoch 830, training loss: 63.9494743347168 = 1.1980723142623901 + 10.0 * 6.275140285491943
Epoch 830, val loss: 1.31623113155365
Epoch 840, training loss: 63.92367172241211 = 1.185822606086731 + 10.0 * 6.27378511428833
Epoch 840, val loss: 1.308016061782837
Epoch 850, training loss: 63.89614486694336 = 1.1736491918563843 + 10.0 * 6.272249698638916
Epoch 850, val loss: 1.299933910369873
Epoch 860, training loss: 63.87229537963867 = 1.1615012884140015 + 10.0 * 6.271079063415527
Epoch 860, val loss: 1.2919420003890991
Epoch 870, training loss: 63.86746597290039 = 1.1493659019470215 + 10.0 * 6.271810054779053
Epoch 870, val loss: 1.2840367555618286
Epoch 880, training loss: 63.863426208496094 = 1.1372586488723755 + 10.0 * 6.272616386413574
Epoch 880, val loss: 1.2761956453323364
Epoch 890, training loss: 63.81388854980469 = 1.1252562999725342 + 10.0 * 6.268863201141357
Epoch 890, val loss: 1.2685089111328125
Epoch 900, training loss: 63.79025650024414 = 1.1132521629333496 + 10.0 * 6.267700672149658
Epoch 900, val loss: 1.2608907222747803
Epoch 910, training loss: 63.76634979248047 = 1.1013317108154297 + 10.0 * 6.2665019035339355
Epoch 910, val loss: 1.2534153461456299
Epoch 920, training loss: 63.745174407958984 = 1.089457392692566 + 10.0 * 6.265571594238281
Epoch 920, val loss: 1.2460498809814453
Epoch 930, training loss: 63.74494934082031 = 1.0775960683822632 + 10.0 * 6.266735553741455
Epoch 930, val loss: 1.2387562990188599
Epoch 940, training loss: 63.719451904296875 = 1.0658897161483765 + 10.0 * 6.265356063842773
Epoch 940, val loss: 1.231719970703125
Epoch 950, training loss: 63.69171142578125 = 1.0541553497314453 + 10.0 * 6.2637553215026855
Epoch 950, val loss: 1.2246180772781372
Epoch 960, training loss: 63.67076110839844 = 1.0425835847854614 + 10.0 * 6.262817859649658
Epoch 960, val loss: 1.2177517414093018
Epoch 970, training loss: 63.657901763916016 = 1.031117558479309 + 10.0 * 6.262678623199463
Epoch 970, val loss: 1.2110427618026733
Epoch 980, training loss: 63.629756927490234 = 1.0197405815124512 + 10.0 * 6.2610015869140625
Epoch 980, val loss: 1.204461932182312
Epoch 990, training loss: 63.61343765258789 = 1.0084606409072876 + 10.0 * 6.260497570037842
Epoch 990, val loss: 1.198061227798462
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.5777777777777778
0.8039008961518187
=== training gcn model ===
Epoch 0, training loss: 87.91840362548828 = 1.9497570991516113 + 10.0 * 8.596864700317383
Epoch 0, val loss: 1.9490679502487183
Epoch 10, training loss: 87.91217041015625 = 1.9448732137680054 + 10.0 * 8.59673023223877
Epoch 10, val loss: 1.9439598321914673
Epoch 20, training loss: 87.90232849121094 = 1.9396214485168457 + 10.0 * 8.596270561218262
Epoch 20, val loss: 1.9384998083114624
Epoch 30, training loss: 87.87859344482422 = 1.9335763454437256 + 10.0 * 8.594501495361328
Epoch 30, val loss: 1.932189702987671
Epoch 40, training loss: 87.80193328857422 = 1.9262728691101074 + 10.0 * 8.587566375732422
Epoch 40, val loss: 1.9245418310165405
Epoch 50, training loss: 87.55258178710938 = 1.9174292087554932 + 10.0 * 8.563515663146973
Epoch 50, val loss: 1.915292501449585
Epoch 60, training loss: 86.8189468383789 = 1.906907558441162 + 10.0 * 8.491204261779785
Epoch 60, val loss: 1.9044461250305176
Epoch 70, training loss: 84.61124420166016 = 1.8962429761886597 + 10.0 * 8.271500587463379
Epoch 70, val loss: 1.8936426639556885
Epoch 80, training loss: 80.90885162353516 = 1.8841564655303955 + 10.0 * 7.902469635009766
Epoch 80, val loss: 1.8818511962890625
Epoch 90, training loss: 78.29402160644531 = 1.8752444982528687 + 10.0 * 7.6418776512146
Epoch 90, val loss: 1.8737791776657104
Epoch 100, training loss: 74.88766479492188 = 1.8692845106124878 + 10.0 * 7.301837921142578
Epoch 100, val loss: 1.8684715032577515
Epoch 110, training loss: 72.340576171875 = 1.8644444942474365 + 10.0 * 7.047613620758057
Epoch 110, val loss: 1.8639869689941406
Epoch 120, training loss: 70.73350524902344 = 1.8603955507278442 + 10.0 * 6.887310981750488
Epoch 120, val loss: 1.8600525856018066
Epoch 130, training loss: 69.9168701171875 = 1.8559186458587646 + 10.0 * 6.806095123291016
Epoch 130, val loss: 1.8556629419326782
Epoch 140, training loss: 69.45634460449219 = 1.8507784605026245 + 10.0 * 6.760556697845459
Epoch 140, val loss: 1.8507702350616455
Epoch 150, training loss: 69.05270385742188 = 1.8453145027160645 + 10.0 * 6.720738887786865
Epoch 150, val loss: 1.845679521560669
Epoch 160, training loss: 68.66741943359375 = 1.8400355577468872 + 10.0 * 6.682738304138184
Epoch 160, val loss: 1.8407914638519287
Epoch 170, training loss: 68.34625244140625 = 1.835102915763855 + 10.0 * 6.651114463806152
Epoch 170, val loss: 1.836247444152832
Epoch 180, training loss: 68.05583953857422 = 1.8303394317626953 + 10.0 * 6.6225504875183105
Epoch 180, val loss: 1.8318707942962646
Epoch 190, training loss: 67.77888488769531 = 1.8256914615631104 + 10.0 * 6.5953192710876465
Epoch 190, val loss: 1.827588438987732
Epoch 200, training loss: 67.52994537353516 = 1.8211065530776978 + 10.0 * 6.5708842277526855
Epoch 200, val loss: 1.8234398365020752
Epoch 210, training loss: 67.31210327148438 = 1.8166557550430298 + 10.0 * 6.549544811248779
Epoch 210, val loss: 1.819334626197815
Epoch 220, training loss: 67.12626647949219 = 1.8122426271438599 + 10.0 * 6.531402587890625
Epoch 220, val loss: 1.8152979612350464
Epoch 230, training loss: 66.97909545898438 = 1.8079135417938232 + 10.0 * 6.517117977142334
Epoch 230, val loss: 1.811252236366272
Epoch 240, training loss: 66.82322692871094 = 1.8035647869110107 + 10.0 * 6.5019659996032715
Epoch 240, val loss: 1.807302713394165
Epoch 250, training loss: 66.6886978149414 = 1.7992801666259766 + 10.0 * 6.488941669464111
Epoch 250, val loss: 1.803355097770691
Epoch 260, training loss: 66.56172180175781 = 1.7950118780136108 + 10.0 * 6.476670742034912
Epoch 260, val loss: 1.7994468212127686
Epoch 270, training loss: 66.44296264648438 = 1.7907350063323975 + 10.0 * 6.4652228355407715
Epoch 270, val loss: 1.7955130338668823
Epoch 280, training loss: 66.33747100830078 = 1.78639817237854 + 10.0 * 6.455106735229492
Epoch 280, val loss: 1.7915048599243164
Epoch 290, training loss: 66.22560119628906 = 1.7820035219192505 + 10.0 * 6.44435977935791
Epoch 290, val loss: 1.787469506263733
Epoch 300, training loss: 66.12478637695312 = 1.777544379234314 + 10.0 * 6.4347243309021
Epoch 300, val loss: 1.7833470106124878
Epoch 310, training loss: 66.03890228271484 = 1.7729365825653076 + 10.0 * 6.426596641540527
Epoch 310, val loss: 1.7791366577148438
Epoch 320, training loss: 65.94972229003906 = 1.7682160139083862 + 10.0 * 6.418150424957275
Epoch 320, val loss: 1.7747447490692139
Epoch 330, training loss: 65.86626434326172 = 1.763292908668518 + 10.0 * 6.410297393798828
Epoch 330, val loss: 1.7702664136886597
Epoch 340, training loss: 65.79029846191406 = 1.7582218647003174 + 10.0 * 6.403207778930664
Epoch 340, val loss: 1.765626311302185
Epoch 350, training loss: 65.72716522216797 = 1.752968192100525 + 10.0 * 6.3974199295043945
Epoch 350, val loss: 1.7608340978622437
Epoch 360, training loss: 65.65309143066406 = 1.7474581003189087 + 10.0 * 6.390563488006592
Epoch 360, val loss: 1.755876064300537
Epoch 370, training loss: 65.59417724609375 = 1.7417322397232056 + 10.0 * 6.385244369506836
Epoch 370, val loss: 1.7507274150848389
Epoch 380, training loss: 65.52338409423828 = 1.7357903718948364 + 10.0 * 6.378758907318115
Epoch 380, val loss: 1.7453734874725342
Epoch 390, training loss: 65.46353149414062 = 1.7295584678649902 + 10.0 * 6.373397350311279
Epoch 390, val loss: 1.7398239374160767
Epoch 400, training loss: 65.40663146972656 = 1.7230792045593262 + 10.0 * 6.368355751037598
Epoch 400, val loss: 1.7340364456176758
Epoch 410, training loss: 65.38172912597656 = 1.716332197189331 + 10.0 * 6.366539478302002
Epoch 410, val loss: 1.7280055284500122
Epoch 420, training loss: 65.30403900146484 = 1.7092177867889404 + 10.0 * 6.3594818115234375
Epoch 420, val loss: 1.7217520475387573
Epoch 430, training loss: 65.25199890136719 = 1.701844573020935 + 10.0 * 6.355015754699707
Epoch 430, val loss: 1.7152661085128784
Epoch 440, training loss: 65.2059555053711 = 1.6941708326339722 + 10.0 * 6.3511786460876465
Epoch 440, val loss: 1.7085232734680176
Epoch 450, training loss: 65.17784118652344 = 1.6862032413482666 + 10.0 * 6.34916353225708
Epoch 450, val loss: 1.7014974355697632
Epoch 460, training loss: 65.1270523071289 = 1.6778900623321533 + 10.0 * 6.344916343688965
Epoch 460, val loss: 1.6942119598388672
Epoch 470, training loss: 65.07705688476562 = 1.6692556142807007 + 10.0 * 6.340779781341553
Epoch 470, val loss: 1.6866612434387207
Epoch 480, training loss: 65.03468322753906 = 1.660292148590088 + 10.0 * 6.337439060211182
Epoch 480, val loss: 1.678859829902649
Epoch 490, training loss: 65.018798828125 = 1.6509907245635986 + 10.0 * 6.336780548095703
Epoch 490, val loss: 1.6707890033721924
Epoch 500, training loss: 64.9593276977539 = 1.6414588689804077 + 10.0 * 6.331787109375
Epoch 500, val loss: 1.6624151468276978
Epoch 510, training loss: 64.91842651367188 = 1.6315463781356812 + 10.0 * 6.32868766784668
Epoch 510, val loss: 1.6538188457489014
Epoch 520, training loss: 64.88153839111328 = 1.6213406324386597 + 10.0 * 6.326020240783691
Epoch 520, val loss: 1.6449769735336304
Epoch 530, training loss: 64.862548828125 = 1.6108404397964478 + 10.0 * 6.325170516967773
Epoch 530, val loss: 1.6358698606491089
Epoch 540, training loss: 64.81571197509766 = 1.6000330448150635 + 10.0 * 6.321567535400391
Epoch 540, val loss: 1.6265512704849243
Epoch 550, training loss: 64.77685546875 = 1.5889523029327393 + 10.0 * 6.318790435791016
Epoch 550, val loss: 1.6170192956924438
Epoch 560, training loss: 64.74491882324219 = 1.5776190757751465 + 10.0 * 6.31673002243042
Epoch 560, val loss: 1.6073105335235596
Epoch 570, training loss: 64.71015930175781 = 1.5660362243652344 + 10.0 * 6.314412593841553
Epoch 570, val loss: 1.5974013805389404
Epoch 580, training loss: 64.68164825439453 = 1.5541818141937256 + 10.0 * 6.312746524810791
Epoch 580, val loss: 1.587323546409607
Epoch 590, training loss: 64.66002655029297 = 1.5420900583267212 + 10.0 * 6.311793804168701
Epoch 590, val loss: 1.5770426988601685
Epoch 600, training loss: 64.61356353759766 = 1.5298422574996948 + 10.0 * 6.3083720207214355
Epoch 600, val loss: 1.5666495561599731
Epoch 610, training loss: 64.58180236816406 = 1.5173643827438354 + 10.0 * 6.30644416809082
Epoch 610, val loss: 1.5561891794204712
Epoch 620, training loss: 64.5508041381836 = 1.5047489404678345 + 10.0 * 6.304605960845947
Epoch 620, val loss: 1.5456188917160034
Epoch 630, training loss: 64.52349853515625 = 1.4919610023498535 + 10.0 * 6.3031535148620605
Epoch 630, val loss: 1.5349377393722534
Epoch 640, training loss: 64.49734497070312 = 1.4789881706237793 + 10.0 * 6.301835536956787
Epoch 640, val loss: 1.524180293083191
Epoch 650, training loss: 64.46002960205078 = 1.4658232927322388 + 10.0 * 6.2994208335876465
Epoch 650, val loss: 1.5133785009384155
Epoch 660, training loss: 64.43733978271484 = 1.4525353908538818 + 10.0 * 6.29848051071167
Epoch 660, val loss: 1.5025427341461182
Epoch 670, training loss: 64.40225219726562 = 1.4392027854919434 + 10.0 * 6.296305179595947
Epoch 670, val loss: 1.4916316270828247
Epoch 680, training loss: 64.37410736083984 = 1.4257049560546875 + 10.0 * 6.294840335845947
Epoch 680, val loss: 1.4807286262512207
Epoch 690, training loss: 64.34732055664062 = 1.4121257066726685 + 10.0 * 6.293519496917725
Epoch 690, val loss: 1.469746708869934
Epoch 700, training loss: 64.32037353515625 = 1.3984251022338867 + 10.0 * 6.2921953201293945
Epoch 700, val loss: 1.4587739706039429
Epoch 710, training loss: 64.29946899414062 = 1.3846185207366943 + 10.0 * 6.291484832763672
Epoch 710, val loss: 1.447800874710083
Epoch 720, training loss: 64.26528930664062 = 1.3708170652389526 + 10.0 * 6.289446830749512
Epoch 720, val loss: 1.4368387460708618
Epoch 730, training loss: 64.23114776611328 = 1.3569409847259521 + 10.0 * 6.287420749664307
Epoch 730, val loss: 1.4259206056594849
Epoch 740, training loss: 64.20397186279297 = 1.3430951833724976 + 10.0 * 6.286087989807129
Epoch 740, val loss: 1.4151033163070679
Epoch 750, training loss: 64.21119689941406 = 1.3291690349578857 + 10.0 * 6.28820276260376
Epoch 750, val loss: 1.404376745223999
Epoch 760, training loss: 64.17001342773438 = 1.3154746294021606 + 10.0 * 6.285453796386719
Epoch 760, val loss: 1.3936192989349365
Epoch 770, training loss: 64.12474060058594 = 1.3016300201416016 + 10.0 * 6.28231143951416
Epoch 770, val loss: 1.3830318450927734
Epoch 780, training loss: 64.10050201416016 = 1.2878899574279785 + 10.0 * 6.281261444091797
Epoch 780, val loss: 1.372557520866394
Epoch 790, training loss: 64.07410430908203 = 1.2742350101470947 + 10.0 * 6.279986381530762
Epoch 790, val loss: 1.362173318862915
Epoch 800, training loss: 64.067138671875 = 1.2605693340301514 + 10.0 * 6.280656814575195
Epoch 800, val loss: 1.3518940210342407
Epoch 810, training loss: 64.04874420166016 = 1.247074842453003 + 10.0 * 6.2801666259765625
Epoch 810, val loss: 1.3417351245880127
Epoch 820, training loss: 64.00492858886719 = 1.2336174249649048 + 10.0 * 6.277131080627441
Epoch 820, val loss: 1.3317224979400635
Epoch 830, training loss: 63.97742462158203 = 1.220290184020996 + 10.0 * 6.2757134437561035
Epoch 830, val loss: 1.321892261505127
Epoch 840, training loss: 63.95391082763672 = 1.2070600986480713 + 10.0 * 6.274684906005859
Epoch 840, val loss: 1.3122066259384155
Epoch 850, training loss: 63.945701599121094 = 1.1939328908920288 + 10.0 * 6.275177001953125
Epoch 850, val loss: 1.3026546239852905
Epoch 860, training loss: 63.914222717285156 = 1.180930733680725 + 10.0 * 6.273329257965088
Epoch 860, val loss: 1.2932393550872803
Epoch 870, training loss: 63.89512252807617 = 1.1680330038070679 + 10.0 * 6.272708892822266
Epoch 870, val loss: 1.2839484214782715
Epoch 880, training loss: 63.87104415893555 = 1.1552504301071167 + 10.0 * 6.271579265594482
Epoch 880, val loss: 1.274819016456604
Epoch 890, training loss: 63.841068267822266 = 1.1425423622131348 + 10.0 * 6.269852638244629
Epoch 890, val loss: 1.2658610343933105
Epoch 900, training loss: 63.827117919921875 = 1.1299757957458496 + 10.0 * 6.26971435546875
Epoch 900, val loss: 1.2570512294769287
Epoch 910, training loss: 63.8057975769043 = 1.1175358295440674 + 10.0 * 6.268826484680176
Epoch 910, val loss: 1.248357892036438
Epoch 920, training loss: 63.788970947265625 = 1.1052778959274292 + 10.0 * 6.268369197845459
Epoch 920, val loss: 1.2397874593734741
Epoch 930, training loss: 63.758785247802734 = 1.0930659770965576 + 10.0 * 6.266571998596191
Epoch 930, val loss: 1.231438159942627
Epoch 940, training loss: 63.73771286010742 = 1.0810478925704956 + 10.0 * 6.265666484832764
Epoch 940, val loss: 1.223201870918274
Epoch 950, training loss: 63.720027923583984 = 1.0691189765930176 + 10.0 * 6.2650909423828125
Epoch 950, val loss: 1.2151381969451904
Epoch 960, training loss: 63.707855224609375 = 1.057344675064087 + 10.0 * 6.265050888061523
Epoch 960, val loss: 1.2071870565414429
Epoch 970, training loss: 63.67988586425781 = 1.045624852180481 + 10.0 * 6.263426303863525
Epoch 970, val loss: 1.1993962526321411
Epoch 980, training loss: 63.6766242980957 = 1.0340656042099 + 10.0 * 6.264256000518799
Epoch 980, val loss: 1.1917356252670288
Epoch 990, training loss: 63.64127731323242 = 1.0226978063583374 + 10.0 * 6.261857986450195
Epoch 990, val loss: 1.184221863746643
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.5407407407407407
0.7986294148655773
=== training gcn model ===
Epoch 0, training loss: 87.92857360839844 = 1.9602361917495728 + 10.0 * 8.596834182739258
Epoch 0, val loss: 1.9579569101333618
Epoch 10, training loss: 87.9215316772461 = 1.9552724361419678 + 10.0 * 8.596626281738281
Epoch 10, val loss: 1.9528404474258423
Epoch 20, training loss: 87.90973663330078 = 1.9498827457427979 + 10.0 * 8.595985412597656
Epoch 20, val loss: 1.9472360610961914
Epoch 30, training loss: 87.88156127929688 = 1.9437594413757324 + 10.0 * 8.593780517578125
Epoch 30, val loss: 1.9408087730407715
Epoch 40, training loss: 87.79296875 = 1.9365696907043457 + 10.0 * 8.585639953613281
Epoch 40, val loss: 1.9332067966461182
Epoch 50, training loss: 87.47727966308594 = 1.9280487298965454 + 10.0 * 8.554923057556152
Epoch 50, val loss: 1.9241713285446167
Epoch 60, training loss: 86.30946350097656 = 1.917937159538269 + 10.0 * 8.439152717590332
Epoch 60, val loss: 1.9134975671768188
Epoch 70, training loss: 82.65287017822266 = 1.9062975645065308 + 10.0 * 8.074657440185547
Epoch 70, val loss: 1.901245355606079
Epoch 80, training loss: 77.66706085205078 = 1.8926948308944702 + 10.0 * 7.577436923980713
Epoch 80, val loss: 1.887166976928711
Epoch 90, training loss: 76.00599670410156 = 1.880694031715393 + 10.0 * 7.412530422210693
Epoch 90, val loss: 1.8756136894226074
Epoch 100, training loss: 74.65323638916016 = 1.8727329969406128 + 10.0 * 7.278050422668457
Epoch 100, val loss: 1.8682690858840942
Epoch 110, training loss: 73.68051147460938 = 1.865620493888855 + 10.0 * 7.181488990783691
Epoch 110, val loss: 1.8613722324371338
Epoch 120, training loss: 73.06910705566406 = 1.8580459356307983 + 10.0 * 7.121105670928955
Epoch 120, val loss: 1.8539843559265137
Epoch 130, training loss: 72.57071685791016 = 1.851003885269165 + 10.0 * 7.071971416473389
Epoch 130, val loss: 1.8472561836242676
Epoch 140, training loss: 71.9383544921875 = 1.8449442386627197 + 10.0 * 7.009340763092041
Epoch 140, val loss: 1.841588020324707
Epoch 150, training loss: 71.1849136352539 = 1.839775562286377 + 10.0 * 6.934514045715332
Epoch 150, val loss: 1.8368836641311646
Epoch 160, training loss: 70.50550842285156 = 1.8354467153549194 + 10.0 * 6.867006301879883
Epoch 160, val loss: 1.832879900932312
Epoch 170, training loss: 69.92170715332031 = 1.8313456773757935 + 10.0 * 6.8090362548828125
Epoch 170, val loss: 1.8291116952896118
Epoch 180, training loss: 69.34406280517578 = 1.827347755432129 + 10.0 * 6.75167179107666
Epoch 180, val loss: 1.8255807161331177
Epoch 190, training loss: 68.89153289794922 = 1.8236074447631836 + 10.0 * 6.70679235458374
Epoch 190, val loss: 1.822353482246399
Epoch 200, training loss: 68.57762908935547 = 1.819899082183838 + 10.0 * 6.675772666931152
Epoch 200, val loss: 1.8191102743148804
Epoch 210, training loss: 68.28813934326172 = 1.8160338401794434 + 10.0 * 6.647211074829102
Epoch 210, val loss: 1.8157076835632324
Epoch 220, training loss: 67.9905776977539 = 1.8123042583465576 + 10.0 * 6.617827892303467
Epoch 220, val loss: 1.8124629259109497
Epoch 230, training loss: 67.70061492919922 = 1.8088443279266357 + 10.0 * 6.589177131652832
Epoch 230, val loss: 1.809524655342102
Epoch 240, training loss: 67.44590759277344 = 1.8055058717727661 + 10.0 * 6.564040660858154
Epoch 240, val loss: 1.8067065477371216
Epoch 250, training loss: 67.23515319824219 = 1.802175760269165 + 10.0 * 6.543297290802002
Epoch 250, val loss: 1.80386483669281
Epoch 260, training loss: 67.04959106445312 = 1.7986482381820679 + 10.0 * 6.525094985961914
Epoch 260, val loss: 1.8009376525878906
Epoch 270, training loss: 66.89434814453125 = 1.795020341873169 + 10.0 * 6.509932994842529
Epoch 270, val loss: 1.7979469299316406
Epoch 280, training loss: 66.75963592529297 = 1.7913622856140137 + 10.0 * 6.496828079223633
Epoch 280, val loss: 1.7949087619781494
Epoch 290, training loss: 66.63813781738281 = 1.7876015901565552 + 10.0 * 6.485053062438965
Epoch 290, val loss: 1.7918035984039307
Epoch 300, training loss: 66.52188873291016 = 1.7837435007095337 + 10.0 * 6.473814487457275
Epoch 300, val loss: 1.78861665725708
Epoch 310, training loss: 66.41751098632812 = 1.7798047065734863 + 10.0 * 6.463770389556885
Epoch 310, val loss: 1.7853513956069946
Epoch 320, training loss: 66.31355285644531 = 1.775755763053894 + 10.0 * 6.453779697418213
Epoch 320, val loss: 1.7820085287094116
Epoch 330, training loss: 66.22187042236328 = 1.7715966701507568 + 10.0 * 6.445026874542236
Epoch 330, val loss: 1.7785890102386475
Epoch 340, training loss: 66.1257095336914 = 1.7672667503356934 + 10.0 * 6.435844421386719
Epoch 340, val loss: 1.7750227451324463
Epoch 350, training loss: 66.0400390625 = 1.762757420539856 + 10.0 * 6.427728652954102
Epoch 350, val loss: 1.7713139057159424
Epoch 360, training loss: 65.95912170410156 = 1.758060097694397 + 10.0 * 6.420106887817383
Epoch 360, val loss: 1.7674552202224731
Epoch 370, training loss: 65.88533782958984 = 1.7531356811523438 + 10.0 * 6.413219928741455
Epoch 370, val loss: 1.7634458541870117
Epoch 380, training loss: 65.81768035888672 = 1.7479560375213623 + 10.0 * 6.406972408294678
Epoch 380, val loss: 1.7592320442199707
Epoch 390, training loss: 65.75190734863281 = 1.7425729036331177 + 10.0 * 6.400932788848877
Epoch 390, val loss: 1.7548575401306152
Epoch 400, training loss: 65.69070434570312 = 1.7369434833526611 + 10.0 * 6.395376205444336
Epoch 400, val loss: 1.7502987384796143
Epoch 410, training loss: 65.63540649414062 = 1.7310336828231812 + 10.0 * 6.390437126159668
Epoch 410, val loss: 1.7455286979675293
Epoch 420, training loss: 65.58000946044922 = 1.7248444557189941 + 10.0 * 6.385516166687012
Epoch 420, val loss: 1.7405445575714111
Epoch 430, training loss: 65.52680206298828 = 1.7183990478515625 + 10.0 * 6.380840301513672
Epoch 430, val loss: 1.735350489616394
Epoch 440, training loss: 65.47457122802734 = 1.711650013923645 + 10.0 * 6.3762922286987305
Epoch 440, val loss: 1.7299314737319946
Epoch 450, training loss: 65.4539794921875 = 1.7045825719833374 + 10.0 * 6.374939918518066
Epoch 450, val loss: 1.7242642641067505
Epoch 460, training loss: 65.38533782958984 = 1.697205662727356 + 10.0 * 6.3688130378723145
Epoch 460, val loss: 1.7183539867401123
Epoch 470, training loss: 65.33374786376953 = 1.689494013786316 + 10.0 * 6.364425182342529
Epoch 470, val loss: 1.7121870517730713
Epoch 480, training loss: 65.28882598876953 = 1.6814467906951904 + 10.0 * 6.3607378005981445
Epoch 480, val loss: 1.7057596445083618
Epoch 490, training loss: 65.25800323486328 = 1.6730451583862305 + 10.0 * 6.358495712280273
Epoch 490, val loss: 1.6990407705307007
Epoch 500, training loss: 65.20887756347656 = 1.6642311811447144 + 10.0 * 6.354464530944824
Epoch 500, val loss: 1.6920413970947266
Epoch 510, training loss: 65.15814208984375 = 1.6550920009613037 + 10.0 * 6.350305080413818
Epoch 510, val loss: 1.6847503185272217
Epoch 520, training loss: 65.11798858642578 = 1.645565390586853 + 10.0 * 6.34724235534668
Epoch 520, val loss: 1.6771665811538696
Epoch 530, training loss: 65.09627532958984 = 1.635653018951416 + 10.0 * 6.346062183380127
Epoch 530, val loss: 1.6692492961883545
Epoch 540, training loss: 65.04647827148438 = 1.6252481937408447 + 10.0 * 6.342122554779053
Epoch 540, val loss: 1.6610093116760254
Epoch 550, training loss: 64.99856567382812 = 1.6145105361938477 + 10.0 * 6.338405609130859
Epoch 550, val loss: 1.6524454355239868
Epoch 560, training loss: 64.96004486083984 = 1.6033707857131958 + 10.0 * 6.335667610168457
Epoch 560, val loss: 1.6435894966125488
Epoch 570, training loss: 64.9328842163086 = 1.5917609930038452 + 10.0 * 6.334112644195557
Epoch 570, val loss: 1.6344008445739746
Epoch 580, training loss: 64.88690185546875 = 1.579738736152649 + 10.0 * 6.330716133117676
Epoch 580, val loss: 1.6248548030853271
Epoch 590, training loss: 64.84857940673828 = 1.5673128366470337 + 10.0 * 6.328126430511475
Epoch 590, val loss: 1.6150264739990234
Epoch 600, training loss: 64.8093490600586 = 1.5543991327285767 + 10.0 * 6.32549524307251
Epoch 600, val loss: 1.604814052581787
Epoch 610, training loss: 64.77396392822266 = 1.5410804748535156 + 10.0 * 6.323288440704346
Epoch 610, val loss: 1.5942842960357666
Epoch 620, training loss: 64.7585220336914 = 1.5273685455322266 + 10.0 * 6.323115348815918
Epoch 620, val loss: 1.5835163593292236
Epoch 630, training loss: 64.70710754394531 = 1.5133452415466309 + 10.0 * 6.319375991821289
Epoch 630, val loss: 1.5724477767944336
Epoch 640, training loss: 64.66841125488281 = 1.4989314079284668 + 10.0 * 6.316947937011719
Epoch 640, val loss: 1.5611858367919922
Epoch 650, training loss: 64.638427734375 = 1.4841692447662354 + 10.0 * 6.315425395965576
Epoch 650, val loss: 1.5496426820755005
Epoch 660, training loss: 64.60018920898438 = 1.4691041707992554 + 10.0 * 6.313108444213867
Epoch 660, val loss: 1.5378897190093994
Epoch 670, training loss: 64.56602478027344 = 1.453730821609497 + 10.0 * 6.311229228973389
Epoch 670, val loss: 1.525930404663086
Epoch 680, training loss: 64.53868103027344 = 1.4380989074707031 + 10.0 * 6.310057640075684
Epoch 680, val loss: 1.5137919187545776
Epoch 690, training loss: 64.49695587158203 = 1.4222062826156616 + 10.0 * 6.3074750900268555
Epoch 690, val loss: 1.5014747381210327
Epoch 700, training loss: 64.4649887084961 = 1.4060983657836914 + 10.0 * 6.305889129638672
Epoch 700, val loss: 1.489059329032898
Epoch 710, training loss: 64.44564819335938 = 1.3898066282272339 + 10.0 * 6.30558443069458
Epoch 710, val loss: 1.4765413999557495
Epoch 720, training loss: 64.405517578125 = 1.3733445405960083 + 10.0 * 6.303216934204102
Epoch 720, val loss: 1.4638780355453491
Epoch 730, training loss: 64.38446807861328 = 1.356810450553894 + 10.0 * 6.302765846252441
Epoch 730, val loss: 1.4512181282043457
Epoch 740, training loss: 64.34175109863281 = 1.340137004852295 + 10.0 * 6.300161361694336
Epoch 740, val loss: 1.4385943412780762
Epoch 750, training loss: 64.30803680419922 = 1.323455810546875 + 10.0 * 6.298458099365234
Epoch 750, val loss: 1.4258946180343628
Epoch 760, training loss: 64.27742004394531 = 1.3067288398742676 + 10.0 * 6.297069072723389
Epoch 760, val loss: 1.413313388824463
Epoch 770, training loss: 64.28378295898438 = 1.28999924659729 + 10.0 * 6.299378395080566
Epoch 770, val loss: 1.4007025957107544
Epoch 780, training loss: 64.21788024902344 = 1.2732760906219482 + 10.0 * 6.294460296630859
Epoch 780, val loss: 1.3881237506866455
Epoch 790, training loss: 64.1882095336914 = 1.2566622495651245 + 10.0 * 6.293154716491699
Epoch 790, val loss: 1.375731348991394
Epoch 800, training loss: 64.1584243774414 = 1.2401083707809448 + 10.0 * 6.291831970214844
Epoch 800, val loss: 1.3634302616119385
Epoch 810, training loss: 64.12849426269531 = 1.2236456871032715 + 10.0 * 6.290484428405762
Epoch 810, val loss: 1.3512585163116455
Epoch 820, training loss: 64.11653900146484 = 1.2072935104370117 + 10.0 * 6.290924549102783
Epoch 820, val loss: 1.3392455577850342
Epoch 830, training loss: 64.10051727294922 = 1.1910126209259033 + 10.0 * 6.290950775146484
Epoch 830, val loss: 1.3271602392196655
Epoch 840, training loss: 64.04884338378906 = 1.1749054193496704 + 10.0 * 6.287393569946289
Epoch 840, val loss: 1.315386414527893
Epoch 850, training loss: 64.01858520507812 = 1.158997654914856 + 10.0 * 6.285958766937256
Epoch 850, val loss: 1.3038380146026611
Epoch 860, training loss: 63.992103576660156 = 1.1432517766952515 + 10.0 * 6.284884929656982
Epoch 860, val loss: 1.2924582958221436
Epoch 870, training loss: 63.99687957763672 = 1.1276304721832275 + 10.0 * 6.286924839019775
Epoch 870, val loss: 1.2812187671661377
Epoch 880, training loss: 63.95763397216797 = 1.1122606992721558 + 10.0 * 6.284537315368652
Epoch 880, val loss: 1.270238995552063
Epoch 890, training loss: 63.91608810424805 = 1.0970284938812256 + 10.0 * 6.2819061279296875
Epoch 890, val loss: 1.2594225406646729
Epoch 900, training loss: 63.897499084472656 = 1.0820411443710327 + 10.0 * 6.281545639038086
Epoch 900, val loss: 1.248801827430725
Epoch 910, training loss: 63.867713928222656 = 1.0672624111175537 + 10.0 * 6.280045509338379
Epoch 910, val loss: 1.2384446859359741
Epoch 920, training loss: 63.84484100341797 = 1.0526732206344604 + 10.0 * 6.279216766357422
Epoch 920, val loss: 1.228338599205017
Epoch 930, training loss: 63.834075927734375 = 1.03831946849823 + 10.0 * 6.279575824737549
Epoch 930, val loss: 1.2183769941329956
Epoch 940, training loss: 63.819252014160156 = 1.0240758657455444 + 10.0 * 6.279517650604248
Epoch 940, val loss: 1.208677887916565
Epoch 950, training loss: 63.77284622192383 = 1.0101691484451294 + 10.0 * 6.2762675285339355
Epoch 950, val loss: 1.1992045640945435
Epoch 960, training loss: 63.74822998046875 = 0.9964819550514221 + 10.0 * 6.275174617767334
Epoch 960, val loss: 1.189948558807373
Epoch 970, training loss: 63.726783752441406 = 0.9830070734024048 + 10.0 * 6.274377822875977
Epoch 970, val loss: 1.180943250656128
Epoch 980, training loss: 63.7098503112793 = 0.9697456359863281 + 10.0 * 6.27401065826416
Epoch 980, val loss: 1.1721597909927368
Epoch 990, training loss: 63.68368911743164 = 0.9566308259963989 + 10.0 * 6.272706031799316
Epoch 990, val loss: 1.163581371307373
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6407407407407407
0.7991565629942015
The final CL Acc:0.58642, 0.04128, The final GNN Acc:0.80056, 0.00237
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13160])
remove edge: torch.Size([2, 7980])
updated graph: torch.Size([2, 10584])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 87.89812469482422 = 1.92955482006073 + 10.0 * 8.596857070922852
Epoch 0, val loss: 1.9152637720108032
Epoch 10, training loss: 87.89193725585938 = 1.9249879121780396 + 10.0 * 8.596694946289062
Epoch 10, val loss: 1.9111590385437012
Epoch 20, training loss: 87.88139343261719 = 1.9200315475463867 + 10.0 * 8.596136093139648
Epoch 20, val loss: 1.9066851139068604
Epoch 30, training loss: 87.854736328125 = 1.9143391847610474 + 10.0 * 8.594039916992188
Epoch 30, val loss: 1.9015328884124756
Epoch 40, training loss: 87.7691650390625 = 1.9076873064041138 + 10.0 * 8.586148262023926
Epoch 40, val loss: 1.89547860622406
Epoch 50, training loss: 87.49622344970703 = 1.8998667001724243 + 10.0 * 8.559636116027832
Epoch 50, val loss: 1.8883087635040283
Epoch 60, training loss: 86.73188018798828 = 1.8907099962234497 + 10.0 * 8.48411750793457
Epoch 60, val loss: 1.8800026178359985
Epoch 70, training loss: 84.7718505859375 = 1.8810724020004272 + 10.0 * 8.289077758789062
Epoch 70, val loss: 1.8712987899780273
Epoch 80, training loss: 80.6973648071289 = 1.8703079223632812 + 10.0 * 7.8827056884765625
Epoch 80, val loss: 1.8616851568222046
Epoch 90, training loss: 77.48670196533203 = 1.859289288520813 + 10.0 * 7.562740802764893
Epoch 90, val loss: 1.852240800857544
Epoch 100, training loss: 75.2308349609375 = 1.8504307270050049 + 10.0 * 7.338040351867676
Epoch 100, val loss: 1.8444207906723022
Epoch 110, training loss: 73.6635513305664 = 1.842979907989502 + 10.0 * 7.1820573806762695
Epoch 110, val loss: 1.8376537561416626
Epoch 120, training loss: 72.33015441894531 = 1.836426854133606 + 10.0 * 7.049373149871826
Epoch 120, val loss: 1.8316359519958496
Epoch 130, training loss: 71.30350494384766 = 1.8310086727142334 + 10.0 * 6.947249412536621
Epoch 130, val loss: 1.8263763189315796
Epoch 140, training loss: 70.47943878173828 = 1.825732946395874 + 10.0 * 6.865371227264404
Epoch 140, val loss: 1.8213930130004883
Epoch 150, training loss: 69.88556671142578 = 1.8204656839370728 + 10.0 * 6.8065104484558105
Epoch 150, val loss: 1.816494107246399
Epoch 160, training loss: 69.4185791015625 = 1.8150957822799683 + 10.0 * 6.760348320007324
Epoch 160, val loss: 1.8116704225540161
Epoch 170, training loss: 69.00100708007812 = 1.8098933696746826 + 10.0 * 6.719111919403076
Epoch 170, val loss: 1.8071576356887817
Epoch 180, training loss: 68.60566711425781 = 1.805071234703064 + 10.0 * 6.680059432983398
Epoch 180, val loss: 1.8031095266342163
Epoch 190, training loss: 68.21099090576172 = 1.800692081451416 + 10.0 * 6.641030311584473
Epoch 190, val loss: 1.7992750406265259
Epoch 200, training loss: 67.84137725830078 = 1.7966022491455078 + 10.0 * 6.604477405548096
Epoch 200, val loss: 1.7956264019012451
Epoch 210, training loss: 67.54059600830078 = 1.792601466178894 + 10.0 * 6.574799060821533
Epoch 210, val loss: 1.7919745445251465
Epoch 220, training loss: 67.26993560791016 = 1.7882786989212036 + 10.0 * 6.548165798187256
Epoch 220, val loss: 1.7881462574005127
Epoch 230, training loss: 67.04896545410156 = 1.7836488485336304 + 10.0 * 6.52653169631958
Epoch 230, val loss: 1.7840089797973633
Epoch 240, training loss: 66.86573028564453 = 1.7787574529647827 + 10.0 * 6.508697509765625
Epoch 240, val loss: 1.7796614170074463
Epoch 250, training loss: 66.70189666748047 = 1.7736413478851318 + 10.0 * 6.492825031280518
Epoch 250, val loss: 1.775120496749878
Epoch 260, training loss: 66.54402923583984 = 1.768316626548767 + 10.0 * 6.477571487426758
Epoch 260, val loss: 1.7704448699951172
Epoch 270, training loss: 66.40476989746094 = 1.7628252506256104 + 10.0 * 6.464194297790527
Epoch 270, val loss: 1.7655826807022095
Epoch 280, training loss: 66.27629852294922 = 1.7571521997451782 + 10.0 * 6.451915264129639
Epoch 280, val loss: 1.7605315446853638
Epoch 290, training loss: 66.17938995361328 = 1.7511956691741943 + 10.0 * 6.442819595336914
Epoch 290, val loss: 1.7552372217178345
Epoch 300, training loss: 66.06351470947266 = 1.7449649572372437 + 10.0 * 6.431854724884033
Epoch 300, val loss: 1.749655842781067
Epoch 310, training loss: 65.9597396850586 = 1.738444209098816 + 10.0 * 6.422130107879639
Epoch 310, val loss: 1.74384605884552
Epoch 320, training loss: 65.86675262451172 = 1.7316036224365234 + 10.0 * 6.413515090942383
Epoch 320, val loss: 1.7377740144729614
Epoch 330, training loss: 65.78314971923828 = 1.7244319915771484 + 10.0 * 6.405871391296387
Epoch 330, val loss: 1.7313958406448364
Epoch 340, training loss: 65.72071075439453 = 1.7168645858764648 + 10.0 * 6.400384902954102
Epoch 340, val loss: 1.7247291803359985
Epoch 350, training loss: 65.65486145019531 = 1.7088613510131836 + 10.0 * 6.3946003913879395
Epoch 350, val loss: 1.7176750898361206
Epoch 360, training loss: 65.5818862915039 = 1.7005659341812134 + 10.0 * 6.388131618499756
Epoch 360, val loss: 1.7103101015090942
Epoch 370, training loss: 65.51953125 = 1.6918386220932007 + 10.0 * 6.3827691078186035
Epoch 370, val loss: 1.7026461362838745
Epoch 380, training loss: 65.46097564697266 = 1.6827131509780884 + 10.0 * 6.377825736999512
Epoch 380, val loss: 1.6945918798446655
Epoch 390, training loss: 65.40557861328125 = 1.6731529235839844 + 10.0 * 6.3732428550720215
Epoch 390, val loss: 1.6861724853515625
Epoch 400, training loss: 65.35240173339844 = 1.6631625890731812 + 10.0 * 6.368924140930176
Epoch 400, val loss: 1.6773587465286255
Epoch 410, training loss: 65.30104064941406 = 1.6527349948883057 + 10.0 * 6.36483097076416
Epoch 410, val loss: 1.6681724786758423
Epoch 420, training loss: 65.25071716308594 = 1.6418626308441162 + 10.0 * 6.360885143280029
Epoch 420, val loss: 1.658599853515625
Epoch 430, training loss: 65.20144653320312 = 1.6305588483810425 + 10.0 * 6.357088565826416
Epoch 430, val loss: 1.648662805557251
Epoch 440, training loss: 65.17465209960938 = 1.6188068389892578 + 10.0 * 6.355584621429443
Epoch 440, val loss: 1.638346552848816
Epoch 450, training loss: 65.11880493164062 = 1.606637716293335 + 10.0 * 6.351216793060303
Epoch 450, val loss: 1.6276198625564575
Epoch 460, training loss: 65.0641860961914 = 1.5941027402877808 + 10.0 * 6.347008228302002
Epoch 460, val loss: 1.6166468858718872
Epoch 470, training loss: 65.02079010009766 = 1.5812019109725952 + 10.0 * 6.343958854675293
Epoch 470, val loss: 1.6053225994110107
Epoch 480, training loss: 64.97505950927734 = 1.5679265260696411 + 10.0 * 6.3407135009765625
Epoch 480, val loss: 1.593693733215332
Epoch 490, training loss: 64.94025421142578 = 1.5543181896209717 + 10.0 * 6.338593482971191
Epoch 490, val loss: 1.5817495584487915
Epoch 500, training loss: 64.90396881103516 = 1.540314793586731 + 10.0 * 6.336365222930908
Epoch 500, val loss: 1.5696121454238892
Epoch 510, training loss: 64.8521728515625 = 1.5261751413345337 + 10.0 * 6.332600116729736
Epoch 510, val loss: 1.557260274887085
Epoch 520, training loss: 64.81160736083984 = 1.5117766857147217 + 10.0 * 6.329982757568359
Epoch 520, val loss: 1.5447499752044678
Epoch 530, training loss: 64.77143859863281 = 1.4971710443496704 + 10.0 * 6.327426910400391
Epoch 530, val loss: 1.5321094989776611
Epoch 540, training loss: 64.73251342773438 = 1.482388973236084 + 10.0 * 6.325012683868408
Epoch 540, val loss: 1.519347906112671
Epoch 550, training loss: 64.69476318359375 = 1.4674474000930786 + 10.0 * 6.322731971740723
Epoch 550, val loss: 1.5065146684646606
Epoch 560, training loss: 64.66399383544922 = 1.4523786306381226 + 10.0 * 6.321161270141602
Epoch 560, val loss: 1.4936058521270752
Epoch 570, training loss: 64.62858581542969 = 1.4372599124908447 + 10.0 * 6.319132328033447
Epoch 570, val loss: 1.4806879758834839
Epoch 580, training loss: 64.58616638183594 = 1.4220919609069824 + 10.0 * 6.316407680511475
Epoch 580, val loss: 1.467820405960083
Epoch 590, training loss: 64.55111694335938 = 1.406934142112732 + 10.0 * 6.314418315887451
Epoch 590, val loss: 1.4549872875213623
Epoch 600, training loss: 64.56367492675781 = 1.391787052154541 + 10.0 * 6.3171892166137695
Epoch 600, val loss: 1.4421747922897339
Epoch 610, training loss: 64.48381042480469 = 1.3766101598739624 + 10.0 * 6.310719966888428
Epoch 610, val loss: 1.4295170307159424
Epoch 620, training loss: 64.45068359375 = 1.3615201711654663 + 10.0 * 6.3089165687561035
Epoch 620, val loss: 1.4170032739639282
Epoch 630, training loss: 64.4164047241211 = 1.3465282917022705 + 10.0 * 6.306987285614014
Epoch 630, val loss: 1.4045889377593994
Epoch 640, training loss: 64.38321685791016 = 1.331595778465271 + 10.0 * 6.305161952972412
Epoch 640, val loss: 1.392251968383789
Epoch 650, training loss: 64.35079956054688 = 1.3166853189468384 + 10.0 * 6.303411483764648
Epoch 650, val loss: 1.3800526857376099
Epoch 660, training loss: 64.3195571899414 = 1.3018465042114258 + 10.0 * 6.3017706871032715
Epoch 660, val loss: 1.3679629564285278
Epoch 670, training loss: 64.30125427246094 = 1.2870659828186035 + 10.0 * 6.301418781280518
Epoch 670, val loss: 1.3559719324111938
Epoch 680, training loss: 64.27838897705078 = 1.2723642587661743 + 10.0 * 6.300602436065674
Epoch 680, val loss: 1.3441168069839478
Epoch 690, training loss: 64.2323226928711 = 1.257757544517517 + 10.0 * 6.29745626449585
Epoch 690, val loss: 1.332456111907959
Epoch 700, training loss: 64.20095825195312 = 1.243216872215271 + 10.0 * 6.295774459838867
Epoch 700, val loss: 1.3209507465362549
Epoch 710, training loss: 64.17072296142578 = 1.2287734746932983 + 10.0 * 6.29419469833374
Epoch 710, val loss: 1.3095139265060425
Epoch 720, training loss: 64.1412124633789 = 1.2143069505691528 + 10.0 * 6.292690753936768
Epoch 720, val loss: 1.2981634140014648
Epoch 730, training loss: 64.1136474609375 = 1.1997923851013184 + 10.0 * 6.291385650634766
Epoch 730, val loss: 1.2868403196334839
Epoch 740, training loss: 64.13212585449219 = 1.1852401494979858 + 10.0 * 6.2946882247924805
Epoch 740, val loss: 1.275623083114624
Epoch 750, training loss: 64.07205200195312 = 1.1709392070770264 + 10.0 * 6.290111064910889
Epoch 750, val loss: 1.264435887336731
Epoch 760, training loss: 64.03520202636719 = 1.1566249132156372 + 10.0 * 6.287858009338379
Epoch 760, val loss: 1.253499150276184
Epoch 770, training loss: 64.0033950805664 = 1.1424165964126587 + 10.0 * 6.286098003387451
Epoch 770, val loss: 1.2427141666412354
Epoch 780, training loss: 63.97737121582031 = 1.1282802820205688 + 10.0 * 6.284909248352051
Epoch 780, val loss: 1.2320201396942139
Epoch 790, training loss: 63.957191467285156 = 1.1142116785049438 + 10.0 * 6.284297943115234
Epoch 790, val loss: 1.2214176654815674
Epoch 800, training loss: 63.93781280517578 = 1.1001513004302979 + 10.0 * 6.283766269683838
Epoch 800, val loss: 1.2109867334365845
Epoch 810, training loss: 63.90095138549805 = 1.0862395763397217 + 10.0 * 6.281471252441406
Epoch 810, val loss: 1.2005754709243774
Epoch 820, training loss: 63.878658294677734 = 1.0723907947540283 + 10.0 * 6.2806267738342285
Epoch 820, val loss: 1.1903069019317627
Epoch 830, training loss: 63.86322784423828 = 1.0586272478103638 + 10.0 * 6.280459880828857
Epoch 830, val loss: 1.1801522970199585
Epoch 840, training loss: 63.82683563232422 = 1.0449331998825073 + 10.0 * 6.2781901359558105
Epoch 840, val loss: 1.1701757907867432
Epoch 850, training loss: 63.807762145996094 = 1.031327724456787 + 10.0 * 6.27764368057251
Epoch 850, val loss: 1.1603295803070068
Epoch 860, training loss: 63.77708435058594 = 1.0178723335266113 + 10.0 * 6.27592134475708
Epoch 860, val loss: 1.1505337953567505
Epoch 870, training loss: 63.76422882080078 = 1.0045074224472046 + 10.0 * 6.27597188949585
Epoch 870, val loss: 1.1408523321151733
Epoch 880, training loss: 63.74248504638672 = 0.9912340641021729 + 10.0 * 6.275125026702881
Epoch 880, val loss: 1.1313154697418213
Epoch 890, training loss: 63.726478576660156 = 0.9780688881874084 + 10.0 * 6.274840831756592
Epoch 890, val loss: 1.121900200843811
Epoch 900, training loss: 63.69426345825195 = 0.9649801254272461 + 10.0 * 6.272928237915039
Epoch 900, val loss: 1.112682819366455
Epoch 910, training loss: 63.66566467285156 = 0.9521096348762512 + 10.0 * 6.271355628967285
Epoch 910, val loss: 1.1035239696502686
Epoch 920, training loss: 63.641292572021484 = 0.9393056035041809 + 10.0 * 6.270198822021484
Epoch 920, val loss: 1.0945463180541992
Epoch 930, training loss: 63.62476348876953 = 0.9266241192817688 + 10.0 * 6.2698140144348145
Epoch 930, val loss: 1.0856648683547974
Epoch 940, training loss: 63.60322952270508 = 0.9140349626541138 + 10.0 * 6.268919467926025
Epoch 940, val loss: 1.0769119262695312
Epoch 950, training loss: 63.61335754394531 = 0.9015625715255737 + 10.0 * 6.271179676055908
Epoch 950, val loss: 1.068312406539917
Epoch 960, training loss: 63.56571960449219 = 0.8893296718597412 + 10.0 * 6.26763916015625
Epoch 960, val loss: 1.0597323179244995
Epoch 970, training loss: 63.541446685791016 = 0.8771786093711853 + 10.0 * 6.266427040100098
Epoch 970, val loss: 1.0514609813690186
Epoch 980, training loss: 63.51732635498047 = 0.8652380108833313 + 10.0 * 6.265208721160889
Epoch 980, val loss: 1.0432565212249756
Epoch 990, training loss: 63.49727249145508 = 0.8534163236618042 + 10.0 * 6.26438570022583
Epoch 990, val loss: 1.0352153778076172
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.674074074074074
0.838165524512388
=== training gcn model ===
Epoch 0, training loss: 87.92848205566406 = 1.960050106048584 + 10.0 * 8.596842765808105
Epoch 0, val loss: 1.9710478782653809
Epoch 10, training loss: 87.9214859008789 = 1.954961895942688 + 10.0 * 8.596652030944824
Epoch 10, val loss: 1.966049075126648
Epoch 20, training loss: 87.90985107421875 = 1.9493504762649536 + 10.0 * 8.596050262451172
Epoch 20, val loss: 1.9603952169418335
Epoch 30, training loss: 87.88221740722656 = 1.9427945613861084 + 10.0 * 8.593942642211914
Epoch 30, val loss: 1.9536354541778564
Epoch 40, training loss: 87.79862976074219 = 1.9349451065063477 + 10.0 * 8.586368560791016
Epoch 40, val loss: 1.9453785419464111
Epoch 50, training loss: 87.53673553466797 = 1.9256283044815063 + 10.0 * 8.561110496520996
Epoch 50, val loss: 1.9355093240737915
Epoch 60, training loss: 86.7856216430664 = 1.9145128726959229 + 10.0 * 8.48711109161377
Epoch 60, val loss: 1.9238145351409912
Epoch 70, training loss: 84.81912231445312 = 1.9022598266601562 + 10.0 * 8.291686058044434
Epoch 70, val loss: 1.9112427234649658
Epoch 80, training loss: 81.183349609375 = 1.8884310722351074 + 10.0 * 7.929491996765137
Epoch 80, val loss: 1.8974132537841797
Epoch 90, training loss: 78.26945495605469 = 1.8757261037826538 + 10.0 * 7.639372825622559
Epoch 90, val loss: 1.8854122161865234
Epoch 100, training loss: 75.83015441894531 = 1.8678799867630005 + 10.0 * 7.3962273597717285
Epoch 100, val loss: 1.8784738779067993
Epoch 110, training loss: 74.0868911743164 = 1.862045407295227 + 10.0 * 7.222484588623047
Epoch 110, val loss: 1.8728840351104736
Epoch 120, training loss: 72.68482208251953 = 1.855486273765564 + 10.0 * 7.0829339027404785
Epoch 120, val loss: 1.8663220405578613
Epoch 130, training loss: 71.56648254394531 = 1.849920630455017 + 10.0 * 6.97165584564209
Epoch 130, val loss: 1.86052668094635
Epoch 140, training loss: 70.66738891601562 = 1.8443018198013306 + 10.0 * 6.8823089599609375
Epoch 140, val loss: 1.8547284603118896
Epoch 150, training loss: 69.91177368164062 = 1.8387963771820068 + 10.0 * 6.807297706604004
Epoch 150, val loss: 1.8491466045379639
Epoch 160, training loss: 69.32472229003906 = 1.8334324359893799 + 10.0 * 6.749128818511963
Epoch 160, val loss: 1.8437817096710205
Epoch 170, training loss: 68.8583755493164 = 1.828133463859558 + 10.0 * 6.703023910522461
Epoch 170, val loss: 1.8385918140411377
Epoch 180, training loss: 68.52062225341797 = 1.8226954936981201 + 10.0 * 6.669793128967285
Epoch 180, val loss: 1.8330947160720825
Epoch 190, training loss: 68.24607849121094 = 1.8170561790466309 + 10.0 * 6.642902374267578
Epoch 190, val loss: 1.827540397644043
Epoch 200, training loss: 68.0106201171875 = 1.811552882194519 + 10.0 * 6.619906425476074
Epoch 200, val loss: 1.8220998048782349
Epoch 210, training loss: 67.80415344238281 = 1.8063099384307861 + 10.0 * 6.599783897399902
Epoch 210, val loss: 1.8168610334396362
Epoch 220, training loss: 67.63262176513672 = 1.801181435585022 + 10.0 * 6.583144664764404
Epoch 220, val loss: 1.8117839097976685
Epoch 230, training loss: 67.43592071533203 = 1.7961523532867432 + 10.0 * 6.563976764678955
Epoch 230, val loss: 1.8067494630813599
Epoch 240, training loss: 67.24810028076172 = 1.7912002801895142 + 10.0 * 6.545690059661865
Epoch 240, val loss: 1.801834225654602
Epoch 250, training loss: 67.07975769042969 = 1.786206841468811 + 10.0 * 6.529354572296143
Epoch 250, val loss: 1.7969756126403809
Epoch 260, training loss: 66.91607666015625 = 1.781103253364563 + 10.0 * 6.513497352600098
Epoch 260, val loss: 1.7920655012130737
Epoch 270, training loss: 66.76982116699219 = 1.7758421897888184 + 10.0 * 6.4993977546691895
Epoch 270, val loss: 1.787086844444275
Epoch 280, training loss: 66.64241790771484 = 1.7703958749771118 + 10.0 * 6.487202167510986
Epoch 280, val loss: 1.7819706201553345
Epoch 290, training loss: 66.50977325439453 = 1.7647041082382202 + 10.0 * 6.474506855010986
Epoch 290, val loss: 1.7766939401626587
Epoch 300, training loss: 66.39505767822266 = 1.758779764175415 + 10.0 * 6.463627815246582
Epoch 300, val loss: 1.7712363004684448
Epoch 310, training loss: 66.3238754272461 = 1.7525832653045654 + 10.0 * 6.457129001617432
Epoch 310, val loss: 1.7655022144317627
Epoch 320, training loss: 66.20726776123047 = 1.746012806892395 + 10.0 * 6.4461259841918945
Epoch 320, val loss: 1.75954008102417
Epoch 330, training loss: 66.10346221923828 = 1.7391653060913086 + 10.0 * 6.436429500579834
Epoch 330, val loss: 1.7533636093139648
Epoch 340, training loss: 66.01436614990234 = 1.7320557832717896 + 10.0 * 6.428231239318848
Epoch 340, val loss: 1.7469258308410645
Epoch 350, training loss: 65.92990112304688 = 1.7246214151382446 + 10.0 * 6.420527458190918
Epoch 350, val loss: 1.7402169704437256
Epoch 360, training loss: 65.84906768798828 = 1.716842532157898 + 10.0 * 6.413222789764404
Epoch 360, val loss: 1.7332271337509155
Epoch 370, training loss: 65.7967529296875 = 1.7086659669876099 + 10.0 * 6.408808708190918
Epoch 370, val loss: 1.7259135246276855
Epoch 380, training loss: 65.70055389404297 = 1.7000761032104492 + 10.0 * 6.400047779083252
Epoch 380, val loss: 1.7182867527008057
Epoch 390, training loss: 65.63214111328125 = 1.691114902496338 + 10.0 * 6.394102573394775
Epoch 390, val loss: 1.71030592918396
Epoch 400, training loss: 65.55943298339844 = 1.6817512512207031 + 10.0 * 6.387768745422363
Epoch 400, val loss: 1.7020113468170166
Epoch 410, training loss: 65.516845703125 = 1.6719589233398438 + 10.0 * 6.384488582611084
Epoch 410, val loss: 1.6933081150054932
Epoch 420, training loss: 65.4343032836914 = 1.661637306213379 + 10.0 * 6.377266883850098
Epoch 420, val loss: 1.6842340230941772
Epoch 430, training loss: 65.37257385253906 = 1.6509374380111694 + 10.0 * 6.37216329574585
Epoch 430, val loss: 1.6747846603393555
Epoch 440, training loss: 65.3115463256836 = 1.639783501625061 + 10.0 * 6.367176055908203
Epoch 440, val loss: 1.66494882106781
Epoch 450, training loss: 65.25450897216797 = 1.628140926361084 + 10.0 * 6.362636566162109
Epoch 450, val loss: 1.65472412109375
Epoch 460, training loss: 65.2518539428711 = 1.616054654121399 + 10.0 * 6.363579750061035
Epoch 460, val loss: 1.6440775394439697
Epoch 470, training loss: 65.16181945800781 = 1.6034514904022217 + 10.0 * 6.355836391448975
Epoch 470, val loss: 1.6330915689468384
Epoch 480, training loss: 65.09965515136719 = 1.5904173851013184 + 10.0 * 6.350923538208008
Epoch 480, val loss: 1.621717929840088
Epoch 490, training loss: 65.0519027709961 = 1.576958179473877 + 10.0 * 6.347494602203369
Epoch 490, val loss: 1.610012173652649
Epoch 500, training loss: 65.00426483154297 = 1.563086748123169 + 10.0 * 6.344117641448975
Epoch 500, val loss: 1.59795343875885
Epoch 510, training loss: 64.9571762084961 = 1.5487629175186157 + 10.0 * 6.340841770172119
Epoch 510, val loss: 1.585522174835205
Epoch 520, training loss: 64.91157531738281 = 1.5340334177017212 + 10.0 * 6.337754249572754
Epoch 520, val loss: 1.572787880897522
Epoch 530, training loss: 64.907470703125 = 1.5188766717910767 + 10.0 * 6.3388590812683105
Epoch 530, val loss: 1.5597225427627563
Epoch 540, training loss: 64.83306121826172 = 1.503402829170227 + 10.0 * 6.332965850830078
Epoch 540, val loss: 1.5463910102844238
Epoch 550, training loss: 64.78509521484375 = 1.4876298904418945 + 10.0 * 6.329746246337891
Epoch 550, val loss: 1.5328518152236938
Epoch 560, training loss: 64.74029541015625 = 1.4715361595153809 + 10.0 * 6.326875686645508
Epoch 560, val loss: 1.5191023349761963
Epoch 570, training loss: 64.7103271484375 = 1.455154299736023 + 10.0 * 6.325517654418945
Epoch 570, val loss: 1.5051525831222534
Epoch 580, training loss: 64.66554260253906 = 1.438620686531067 + 10.0 * 6.322692394256592
Epoch 580, val loss: 1.491025447845459
Epoch 590, training loss: 64.62037658691406 = 1.4217840433120728 + 10.0 * 6.319859504699707
Epoch 590, val loss: 1.4767820835113525
Epoch 600, training loss: 64.57862854003906 = 1.4048503637313843 + 10.0 * 6.31737756729126
Epoch 600, val loss: 1.4624404907226562
Epoch 610, training loss: 64.5402603149414 = 1.3877298831939697 + 10.0 * 6.315252780914307
Epoch 610, val loss: 1.448001503944397
Epoch 620, training loss: 64.52569580078125 = 1.3704513311386108 + 10.0 * 6.315524578094482
Epoch 620, val loss: 1.4334684610366821
Epoch 630, training loss: 64.46917724609375 = 1.3531486988067627 + 10.0 * 6.311602592468262
Epoch 630, val loss: 1.418926477432251
Epoch 640, training loss: 64.4300765991211 = 1.3357921838760376 + 10.0 * 6.309428691864014
Epoch 640, val loss: 1.4043993949890137
Epoch 650, training loss: 64.39396667480469 = 1.3184038400650024 + 10.0 * 6.307556629180908
Epoch 650, val loss: 1.3898824453353882
Epoch 660, training loss: 64.3603286743164 = 1.3009952306747437 + 10.0 * 6.305932998657227
Epoch 660, val loss: 1.3753598928451538
Epoch 670, training loss: 64.34317779541016 = 1.2836556434631348 + 10.0 * 6.305952548980713
Epoch 670, val loss: 1.360908031463623
Epoch 680, training loss: 64.3016128540039 = 1.266248345375061 + 10.0 * 6.303536415100098
Epoch 680, val loss: 1.346490740776062
Epoch 690, training loss: 64.25946044921875 = 1.2490655183792114 + 10.0 * 6.301039218902588
Epoch 690, val loss: 1.3322757482528687
Epoch 700, training loss: 64.2238998413086 = 1.2319273948669434 + 10.0 * 6.299197196960449
Epoch 700, val loss: 1.3181498050689697
Epoch 710, training loss: 64.19096374511719 = 1.2148891687393188 + 10.0 * 6.297607421875
Epoch 710, val loss: 1.3041380643844604
Epoch 720, training loss: 64.175537109375 = 1.1979080438613892 + 10.0 * 6.297762870788574
Epoch 720, val loss: 1.2902491092681885
Epoch 730, training loss: 64.13800048828125 = 1.1811295747756958 + 10.0 * 6.295687198638916
Epoch 730, val loss: 1.2765332460403442
Epoch 740, training loss: 64.10204315185547 = 1.1644562482833862 + 10.0 * 6.293758392333984
Epoch 740, val loss: 1.2630048990249634
Epoch 750, training loss: 64.06743621826172 = 1.1479140520095825 + 10.0 * 6.291952610015869
Epoch 750, val loss: 1.2496131658554077
Epoch 760, training loss: 64.03955841064453 = 1.1315085887908936 + 10.0 * 6.290804862976074
Epoch 760, val loss: 1.2364085912704468
Epoch 770, training loss: 64.03626251220703 = 1.1151944398880005 + 10.0 * 6.292106628417969
Epoch 770, val loss: 1.2232955694198608
Epoch 780, training loss: 63.98212814331055 = 1.098871111869812 + 10.0 * 6.288325786590576
Epoch 780, val loss: 1.2102288007736206
Epoch 790, training loss: 63.95329284667969 = 1.0829280614852905 + 10.0 * 6.287036418914795
Epoch 790, val loss: 1.1976014375686646
Epoch 800, training loss: 63.92490005493164 = 1.0671958923339844 + 10.0 * 6.285770416259766
Epoch 800, val loss: 1.1851927042007446
Epoch 810, training loss: 63.90101623535156 = 1.0517126321792603 + 10.0 * 6.284930229187012
Epoch 810, val loss: 1.1731066703796387
Epoch 820, training loss: 63.882164001464844 = 1.0364426374435425 + 10.0 * 6.284572124481201
Epoch 820, val loss: 1.1612465381622314
Epoch 830, training loss: 63.85003662109375 = 1.0214519500732422 + 10.0 * 6.282858371734619
Epoch 830, val loss: 1.1497633457183838
Epoch 840, training loss: 63.82032012939453 = 1.0067356824874878 + 10.0 * 6.281358242034912
Epoch 840, val loss: 1.1385548114776611
Epoch 850, training loss: 63.79320526123047 = 0.9923261404037476 + 10.0 * 6.280087947845459
Epoch 850, val loss: 1.1277029514312744
Epoch 860, training loss: 63.772830963134766 = 0.9781667590141296 + 10.0 * 6.279466152191162
Epoch 860, val loss: 1.1171786785125732
Epoch 870, training loss: 63.74958419799805 = 0.9643043875694275 + 10.0 * 6.278527736663818
Epoch 870, val loss: 1.1070038080215454
Epoch 880, training loss: 63.72898864746094 = 0.9507209658622742 + 10.0 * 6.27782678604126
Epoch 880, val loss: 1.097074031829834
Epoch 890, training loss: 63.69694900512695 = 0.9374686479568481 + 10.0 * 6.2759480476379395
Epoch 890, val loss: 1.0875838994979858
Epoch 900, training loss: 63.67469024658203 = 0.924480140209198 + 10.0 * 6.275021076202393
Epoch 900, val loss: 1.0784389972686768
Epoch 910, training loss: 63.65791702270508 = 0.911749005317688 + 10.0 * 6.274616718292236
Epoch 910, val loss: 1.0695875883102417
Epoch 920, training loss: 63.645938873291016 = 0.8992879390716553 + 10.0 * 6.274664878845215
Epoch 920, val loss: 1.0610053539276123
Epoch 930, training loss: 63.615848541259766 = 0.887060821056366 + 10.0 * 6.272878646850586
Epoch 930, val loss: 1.0527427196502686
Epoch 940, training loss: 63.588134765625 = 0.8751661777496338 + 10.0 * 6.271296501159668
Epoch 940, val loss: 1.044885277748108
Epoch 950, training loss: 63.566307067871094 = 0.8634888529777527 + 10.0 * 6.270281791687012
Epoch 950, val loss: 1.0373096466064453
Epoch 960, training loss: 63.56687545776367 = 0.8520637154579163 + 10.0 * 6.271481513977051
Epoch 960, val loss: 1.030033826828003
Epoch 970, training loss: 63.538333892822266 = 0.8407862186431885 + 10.0 * 6.269754886627197
Epoch 970, val loss: 1.0228805541992188
Epoch 980, training loss: 63.52452087402344 = 0.8297848105430603 + 10.0 * 6.269473552703857
Epoch 980, val loss: 1.0161371231079102
Epoch 990, training loss: 63.49237060546875 = 0.8190609216690063 + 10.0 * 6.267331123352051
Epoch 990, val loss: 1.009714126586914
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6185185185185186
0.8481813389562468
=== training gcn model ===
Epoch 0, training loss: 87.90631103515625 = 1.9376473426818848 + 10.0 * 8.596866607666016
Epoch 0, val loss: 1.935928463935852
Epoch 10, training loss: 87.90061950683594 = 1.9332221746444702 + 10.0 * 8.596739768981934
Epoch 10, val loss: 1.9318519830703735
Epoch 20, training loss: 87.89163208007812 = 1.9285393953323364 + 10.0 * 8.596308708190918
Epoch 20, val loss: 1.9274826049804688
Epoch 30, training loss: 87.87005615234375 = 1.9232949018478394 + 10.0 * 8.59467601776123
Epoch 30, val loss: 1.9224780797958374
Epoch 40, training loss: 87.8001937866211 = 1.9172284603118896 + 10.0 * 8.588296890258789
Epoch 40, val loss: 1.9165767431259155
Epoch 50, training loss: 87.5721664428711 = 1.909951090812683 + 10.0 * 8.566221237182617
Epoch 50, val loss: 1.909422755241394
Epoch 60, training loss: 86.92955017089844 = 1.900821566581726 + 10.0 * 8.502873420715332
Epoch 60, val loss: 1.9004932641983032
Epoch 70, training loss: 85.30811309814453 = 1.8906904458999634 + 10.0 * 8.341741561889648
Epoch 70, val loss: 1.8907185792922974
Epoch 80, training loss: 82.53926849365234 = 1.8793342113494873 + 10.0 * 8.065993309020996
Epoch 80, val loss: 1.8801429271697998
Epoch 90, training loss: 80.7147216796875 = 1.8677738904953003 + 10.0 * 7.884694576263428
Epoch 90, val loss: 1.8698115348815918
Epoch 100, training loss: 78.52081298828125 = 1.8586547374725342 + 10.0 * 7.666215896606445
Epoch 100, val loss: 1.8613859415054321
Epoch 110, training loss: 75.75221252441406 = 1.8516451120376587 + 10.0 * 7.390056610107422
Epoch 110, val loss: 1.854366660118103
Epoch 120, training loss: 73.47692108154297 = 1.8459049463272095 + 10.0 * 7.163101673126221
Epoch 120, val loss: 1.84805428981781
Epoch 130, training loss: 71.93460083007812 = 1.8406527042388916 + 10.0 * 7.009394645690918
Epoch 130, val loss: 1.8424625396728516
Epoch 140, training loss: 71.08251190185547 = 1.8354302644729614 + 10.0 * 6.924707889556885
Epoch 140, val loss: 1.8370388746261597
Epoch 150, training loss: 70.59149169921875 = 1.8298134803771973 + 10.0 * 6.876168251037598
Epoch 150, val loss: 1.8315670490264893
Epoch 160, training loss: 70.116943359375 = 1.824048638343811 + 10.0 * 6.829288959503174
Epoch 160, val loss: 1.8263015747070312
Epoch 170, training loss: 69.56658935546875 = 1.8187142610549927 + 10.0 * 6.774786949157715
Epoch 170, val loss: 1.8215694427490234
Epoch 180, training loss: 69.06575775146484 = 1.813909888267517 + 10.0 * 6.725184917449951
Epoch 180, val loss: 1.8172436952590942
Epoch 190, training loss: 68.6556167602539 = 1.8091946840286255 + 10.0 * 6.684642314910889
Epoch 190, val loss: 1.8129054307937622
Epoch 200, training loss: 68.26716613769531 = 1.8045917749404907 + 10.0 * 6.6462578773498535
Epoch 200, val loss: 1.8086515665054321
Epoch 210, training loss: 67.91331481933594 = 1.8000009059906006 + 10.0 * 6.611331462860107
Epoch 210, val loss: 1.804474949836731
Epoch 220, training loss: 67.60863494873047 = 1.7954463958740234 + 10.0 * 6.581318378448486
Epoch 220, val loss: 1.8003484010696411
Epoch 230, training loss: 67.34947967529297 = 1.7907606363296509 + 10.0 * 6.555871486663818
Epoch 230, val loss: 1.796128749847412
Epoch 240, training loss: 67.14427185058594 = 1.7858874797821045 + 10.0 * 6.535838603973389
Epoch 240, val loss: 1.7917815446853638
Epoch 250, training loss: 66.9557876586914 = 1.7807382345199585 + 10.0 * 6.517505168914795
Epoch 250, val loss: 1.7872493267059326
Epoch 260, training loss: 66.7769775390625 = 1.7754194736480713 + 10.0 * 6.500155448913574
Epoch 260, val loss: 1.7825547456741333
Epoch 270, training loss: 66.62728118896484 = 1.7699421644210815 + 10.0 * 6.485733509063721
Epoch 270, val loss: 1.7777773141860962
Epoch 280, training loss: 66.49295806884766 = 1.7642529010772705 + 10.0 * 6.472870349884033
Epoch 280, val loss: 1.7728347778320312
Epoch 290, training loss: 66.37032318115234 = 1.7583367824554443 + 10.0 * 6.461198329925537
Epoch 290, val loss: 1.767746090888977
Epoch 300, training loss: 66.2627182006836 = 1.7521514892578125 + 10.0 * 6.451056480407715
Epoch 300, val loss: 1.7624768018722534
Epoch 310, training loss: 66.15367126464844 = 1.745745301246643 + 10.0 * 6.440792560577393
Epoch 310, val loss: 1.7570135593414307
Epoch 320, training loss: 66.05780029296875 = 1.7390226125717163 + 10.0 * 6.431877613067627
Epoch 320, val loss: 1.751314640045166
Epoch 330, training loss: 65.96675109863281 = 1.7320014238357544 + 10.0 * 6.4234747886657715
Epoch 330, val loss: 1.7454077005386353
Epoch 340, training loss: 65.88015747070312 = 1.7247010469436646 + 10.0 * 6.41554594039917
Epoch 340, val loss: 1.7392725944519043
Epoch 350, training loss: 65.81568145751953 = 1.7170765399932861 + 10.0 * 6.409860610961914
Epoch 350, val loss: 1.7328870296478271
Epoch 360, training loss: 65.72802734375 = 1.7090734243392944 + 10.0 * 6.401895523071289
Epoch 360, val loss: 1.7262241840362549
Epoch 370, training loss: 65.65802764892578 = 1.7007262706756592 + 10.0 * 6.395730018615723
Epoch 370, val loss: 1.7192676067352295
Epoch 380, training loss: 65.60948944091797 = 1.6919751167297363 + 10.0 * 6.391751289367676
Epoch 380, val loss: 1.7119914293289185
Epoch 390, training loss: 65.54686737060547 = 1.6828709840774536 + 10.0 * 6.386399745941162
Epoch 390, val loss: 1.7044093608856201
Epoch 400, training loss: 65.47704315185547 = 1.6733487844467163 + 10.0 * 6.380369663238525
Epoch 400, val loss: 1.6965097188949585
Epoch 410, training loss: 65.42017364501953 = 1.6634403467178345 + 10.0 * 6.375673294067383
Epoch 410, val loss: 1.6883076429367065
Epoch 420, training loss: 65.3655014038086 = 1.653153657913208 + 10.0 * 6.371234893798828
Epoch 420, val loss: 1.6798046827316284
Epoch 430, training loss: 65.31208801269531 = 1.6424622535705566 + 10.0 * 6.366962909698486
Epoch 430, val loss: 1.6709845066070557
Epoch 440, training loss: 65.27534484863281 = 1.6313836574554443 + 10.0 * 6.364396095275879
Epoch 440, val loss: 1.6618435382843018
Epoch 450, training loss: 65.22276306152344 = 1.6198077201843262 + 10.0 * 6.36029577255249
Epoch 450, val loss: 1.6523391008377075
Epoch 460, training loss: 65.16036987304688 = 1.6079356670379639 + 10.0 * 6.355243682861328
Epoch 460, val loss: 1.642582893371582
Epoch 470, training loss: 65.112060546875 = 1.5956536531448364 + 10.0 * 6.351640224456787
Epoch 470, val loss: 1.6325339078903198
Epoch 480, training loss: 65.0621566772461 = 1.5829826593399048 + 10.0 * 6.347917079925537
Epoch 480, val loss: 1.622180461883545
Epoch 490, training loss: 65.0205307006836 = 1.5699448585510254 + 10.0 * 6.345058441162109
Epoch 490, val loss: 1.6115442514419556
Epoch 500, training loss: 64.98296356201172 = 1.556519627571106 + 10.0 * 6.342644691467285
Epoch 500, val loss: 1.6006073951721191
Epoch 510, training loss: 64.9291763305664 = 1.542791724205017 + 10.0 * 6.3386383056640625
Epoch 510, val loss: 1.5894616842269897
Epoch 520, training loss: 64.88080596923828 = 1.5287339687347412 + 10.0 * 6.335207462310791
Epoch 520, val loss: 1.5780636072158813
Epoch 530, training loss: 64.83415985107422 = 1.5144169330596924 + 10.0 * 6.331974029541016
Epoch 530, val loss: 1.566480040550232
Epoch 540, training loss: 64.79042053222656 = 1.4997731447219849 + 10.0 * 6.329064846038818
Epoch 540, val loss: 1.554675579071045
Epoch 550, training loss: 64.74961853027344 = 1.4848557710647583 + 10.0 * 6.326476097106934
Epoch 550, val loss: 1.542668104171753
Epoch 560, training loss: 64.71051025390625 = 1.4696800708770752 + 10.0 * 6.324082851409912
Epoch 560, val loss: 1.5304948091506958
Epoch 570, training loss: 64.67324829101562 = 1.4542527198791504 + 10.0 * 6.321899890899658
Epoch 570, val loss: 1.5180976390838623
Epoch 580, training loss: 64.63362884521484 = 1.4387110471725464 + 10.0 * 6.319491386413574
Epoch 580, val loss: 1.505697250366211
Epoch 590, training loss: 64.59219360351562 = 1.4230624437332153 + 10.0 * 6.316912651062012
Epoch 590, val loss: 1.4932676553726196
Epoch 600, training loss: 64.5535888671875 = 1.4072829484939575 + 10.0 * 6.314630508422852
Epoch 600, val loss: 1.4807602167129517
Epoch 610, training loss: 64.51542663574219 = 1.3913779258728027 + 10.0 * 6.312404632568359
Epoch 610, val loss: 1.4681833982467651
Epoch 620, training loss: 64.47909545898438 = 1.3753567934036255 + 10.0 * 6.310373783111572
Epoch 620, val loss: 1.4555588960647583
Epoch 630, training loss: 64.44341278076172 = 1.3592361211776733 + 10.0 * 6.308417797088623
Epoch 630, val loss: 1.4429266452789307
Epoch 640, training loss: 64.40837097167969 = 1.343045949935913 + 10.0 * 6.306532382965088
Epoch 640, val loss: 1.4302918910980225
Epoch 650, training loss: 64.39506530761719 = 1.3267858028411865 + 10.0 * 6.306828022003174
Epoch 650, val loss: 1.4176530838012695
Epoch 660, training loss: 64.34745025634766 = 1.310535192489624 + 10.0 * 6.303691864013672
Epoch 660, val loss: 1.4050543308258057
Epoch 670, training loss: 64.31210327148438 = 1.2943247556686401 + 10.0 * 6.3017778396606445
Epoch 670, val loss: 1.392461895942688
Epoch 680, training loss: 64.27863311767578 = 1.2782071828842163 + 10.0 * 6.300042629241943
Epoch 680, val loss: 1.380042552947998
Epoch 690, training loss: 64.2446517944336 = 1.2621251344680786 + 10.0 * 6.298252582550049
Epoch 690, val loss: 1.3676809072494507
Epoch 700, training loss: 64.21340942382812 = 1.2460954189300537 + 10.0 * 6.296731472015381
Epoch 700, val loss: 1.3554275035858154
Epoch 710, training loss: 64.19401550292969 = 1.2300903797149658 + 10.0 * 6.296392917633057
Epoch 710, val loss: 1.3432197570800781
Epoch 720, training loss: 64.1536865234375 = 1.214191198348999 + 10.0 * 6.293950080871582
Epoch 720, val loss: 1.3311846256256104
Epoch 730, training loss: 64.12776184082031 = 1.1983442306518555 + 10.0 * 6.292942047119141
Epoch 730, val loss: 1.3191637992858887
Epoch 740, training loss: 64.09447479248047 = 1.1826573610305786 + 10.0 * 6.291181564331055
Epoch 740, val loss: 1.307364821434021
Epoch 750, training loss: 64.06465148925781 = 1.1670317649841309 + 10.0 * 6.289762020111084
Epoch 750, val loss: 1.295639991760254
Epoch 760, training loss: 64.0365982055664 = 1.1515014171600342 + 10.0 * 6.288509845733643
Epoch 760, val loss: 1.2840354442596436
Epoch 770, training loss: 64.0199966430664 = 1.136048674583435 + 10.0 * 6.288394451141357
Epoch 770, val loss: 1.2725454568862915
Epoch 780, training loss: 63.99726486206055 = 1.1206765174865723 + 10.0 * 6.28765869140625
Epoch 780, val loss: 1.261032223701477
Epoch 790, training loss: 63.9580192565918 = 1.105420470237732 + 10.0 * 6.285260200500488
Epoch 790, val loss: 1.2498060464859009
Epoch 800, training loss: 63.928714752197266 = 1.0903085470199585 + 10.0 * 6.283840656280518
Epoch 800, val loss: 1.2386034727096558
Epoch 810, training loss: 63.902259826660156 = 1.075347900390625 + 10.0 * 6.28269100189209
Epoch 810, val loss: 1.2276688814163208
Epoch 820, training loss: 63.875732421875 = 1.0604908466339111 + 10.0 * 6.281524181365967
Epoch 820, val loss: 1.2168279886245728
Epoch 830, training loss: 63.849693298339844 = 1.0457345247268677 + 10.0 * 6.280395984649658
Epoch 830, val loss: 1.2061034440994263
Epoch 840, training loss: 63.845436096191406 = 1.0310879945755005 + 10.0 * 6.281434535980225
Epoch 840, val loss: 1.195534586906433
Epoch 850, training loss: 63.81399917602539 = 1.0164815187454224 + 10.0 * 6.279751777648926
Epoch 850, val loss: 1.1849191188812256
Epoch 860, training loss: 63.77561950683594 = 1.0020253658294678 + 10.0 * 6.277359485626221
Epoch 860, val loss: 1.1745089292526245
Epoch 870, training loss: 63.75164794921875 = 0.9877148866653442 + 10.0 * 6.276392936706543
Epoch 870, val loss: 1.164286732673645
Epoch 880, training loss: 63.725852966308594 = 0.9734922647476196 + 10.0 * 6.275236129760742
Epoch 880, val loss: 1.1541433334350586
Epoch 890, training loss: 63.70750045776367 = 0.9593785405158997 + 10.0 * 6.2748122215271
Epoch 890, val loss: 1.1440894603729248
Epoch 900, training loss: 63.7393684387207 = 0.9452897906303406 + 10.0 * 6.279407978057861
Epoch 900, val loss: 1.1341315507888794
Epoch 910, training loss: 63.66294860839844 = 0.931455671787262 + 10.0 * 6.273149013519287
Epoch 910, val loss: 1.1243700981140137
Epoch 920, training loss: 63.64041519165039 = 0.9177713990211487 + 10.0 * 6.27226448059082
Epoch 920, val loss: 1.11483633518219
Epoch 930, training loss: 63.612152099609375 = 0.9042297005653381 + 10.0 * 6.270792484283447
Epoch 930, val loss: 1.105420470237732
Epoch 940, training loss: 63.58903884887695 = 0.8908253908157349 + 10.0 * 6.2698211669921875
Epoch 940, val loss: 1.0961393117904663
Epoch 950, training loss: 63.56689453125 = 0.8775386810302734 + 10.0 * 6.268935203552246
Epoch 950, val loss: 1.0870145559310913
Epoch 960, training loss: 63.545475006103516 = 0.864387035369873 + 10.0 * 6.26810884475708
Epoch 960, val loss: 1.0780123472213745
Epoch 970, training loss: 63.525062561035156 = 0.8513745665550232 + 10.0 * 6.267368793487549
Epoch 970, val loss: 1.069159984588623
Epoch 980, training loss: 63.542728424072266 = 0.8385236263275146 + 10.0 * 6.270420551300049
Epoch 980, val loss: 1.0605005025863647
Epoch 990, training loss: 63.4996452331543 = 0.8257322311401367 + 10.0 * 6.267391204833984
Epoch 990, val loss: 1.0516648292541504
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6592592592592593
0.8365840801265156
The final CL Acc:0.65062, 0.02349, The final GNN Acc:0.84098, 0.00513
 CL Acc:0.64198, 0.00462, The final GNN Acc:0.84150, 0.00066
