Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.3 der2:0 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 104.68872833251953 = 1.09833562374115 + 10.0 * 10.359039306640625
Epoch 0, val loss: 1.098204255104065
Epoch 10, training loss: 104.64061737060547 = 1.087031364440918 + 10.0 * 10.355359077453613
Epoch 10, val loss: 1.0865885019302368
Epoch 20, training loss: 104.02103424072266 = 1.0721137523651123 + 10.0 * 10.294892311096191
Epoch 20, val loss: 1.0716007947921753
Epoch 30, training loss: 98.77935791015625 = 1.0582672357559204 + 10.0 * 9.772109031677246
Epoch 30, val loss: 1.0578950643539429
Epoch 40, training loss: 95.81001281738281 = 1.0458828210830688 + 10.0 * 9.476412773132324
Epoch 40, val loss: 1.04550039768219
Epoch 50, training loss: 94.77788543701172 = 1.0302613973617554 + 10.0 * 9.374762535095215
Epoch 50, val loss: 1.0295387506484985
Epoch 60, training loss: 94.38056182861328 = 1.0136679410934448 + 10.0 * 9.336689949035645
Epoch 60, val loss: 1.0132392644882202
Epoch 70, training loss: 94.03913116455078 = 1.0006561279296875 + 10.0 * 9.303847312927246
Epoch 70, val loss: 1.000859022140503
Epoch 80, training loss: 93.51668548583984 = 0.9925297498703003 + 10.0 * 9.252415657043457
Epoch 80, val loss: 0.9932401776313782
Epoch 90, training loss: 93.0538558959961 = 0.9857154488563538 + 10.0 * 9.20681381225586
Epoch 90, val loss: 0.9863611459732056
Epoch 100, training loss: 92.71610260009766 = 0.9749292135238647 + 10.0 * 9.174117088317871
Epoch 100, val loss: 0.9756114482879639
Epoch 110, training loss: 92.45428466796875 = 0.9619404673576355 + 10.0 * 9.149234771728516
Epoch 110, val loss: 0.9631032347679138
Epoch 120, training loss: 92.3053970336914 = 0.9490363001823425 + 10.0 * 9.135636329650879
Epoch 120, val loss: 0.9505264163017273
Epoch 130, training loss: 92.14456176757812 = 0.9346612095832825 + 10.0 * 9.120989799499512
Epoch 130, val loss: 0.9366151094436646
Epoch 140, training loss: 92.01367950439453 = 0.9183912873268127 + 10.0 * 9.109529495239258
Epoch 140, val loss: 0.9207302927970886
Epoch 150, training loss: 91.90019226074219 = 0.8999854326248169 + 10.0 * 9.100020408630371
Epoch 150, val loss: 0.9030096530914307
Epoch 160, training loss: 91.80508422851562 = 0.8785403370857239 + 10.0 * 9.09265422821045
Epoch 160, val loss: 0.8821340203285217
Epoch 170, training loss: 91.73783111572266 = 0.8529351353645325 + 10.0 * 9.088489532470703
Epoch 170, val loss: 0.8573915362358093
Epoch 180, training loss: 91.66188049316406 = 0.8235915303230286 + 10.0 * 9.083828926086426
Epoch 180, val loss: 0.8290567994117737
Epoch 190, training loss: 91.5947494506836 = 0.7910531759262085 + 10.0 * 9.08036994934082
Epoch 190, val loss: 0.7977805733680725
Epoch 200, training loss: 91.54146575927734 = 0.7556350827217102 + 10.0 * 9.078582763671875
Epoch 200, val loss: 0.7638912200927734
Epoch 210, training loss: 91.46642303466797 = 0.7179080247879028 + 10.0 * 9.074851036071777
Epoch 210, val loss: 0.7280669808387756
Epoch 220, training loss: 91.39952087402344 = 0.6785251498222351 + 10.0 * 9.072099685668945
Epoch 220, val loss: 0.6908810138702393
Epoch 230, training loss: 91.34161376953125 = 0.6385502815246582 + 10.0 * 9.070306777954102
Epoch 230, val loss: 0.6534967422485352
Epoch 240, training loss: 91.2778091430664 = 0.5996361970901489 + 10.0 * 9.067816734313965
Epoch 240, val loss: 0.6173615455627441
Epoch 250, training loss: 91.2089614868164 = 0.5629912614822388 + 10.0 * 9.064597129821777
Epoch 250, val loss: 0.5837812423706055
Epoch 260, training loss: 91.16612243652344 = 0.5294196009635925 + 10.0 * 9.06367015838623
Epoch 260, val loss: 0.5533865094184875
Epoch 270, training loss: 91.12018585205078 = 0.49958303570747375 + 10.0 * 9.062060356140137
Epoch 270, val loss: 0.5268579125404358
Epoch 280, training loss: 91.06331634521484 = 0.473714143037796 + 10.0 * 9.0589599609375
Epoch 280, val loss: 0.5041553378105164
Epoch 290, training loss: 91.02127075195312 = 0.4515150487422943 + 10.0 * 9.056975364685059
Epoch 290, val loss: 0.4850933253765106
Epoch 300, training loss: 90.9974136352539 = 0.43247488141059875 + 10.0 * 9.056493759155273
Epoch 300, val loss: 0.46914806962013245
Epoch 310, training loss: 90.9939193725586 = 0.41641461849212646 + 10.0 * 9.057750701904297
Epoch 310, val loss: 0.45610511302948
Epoch 320, training loss: 90.94200134277344 = 0.40302109718322754 + 10.0 * 9.053897857666016
Epoch 320, val loss: 0.4455994665622711
Epoch 330, training loss: 90.91343688964844 = 0.3917115330696106 + 10.0 * 9.052172660827637
Epoch 330, val loss: 0.4370002746582031
Epoch 340, training loss: 90.8885498046875 = 0.3818702697753906 + 10.0 * 9.050668716430664
Epoch 340, val loss: 0.42986592650413513
Epoch 350, training loss: 90.87591552734375 = 0.37314265966415405 + 10.0 * 9.050276756286621
Epoch 350, val loss: 0.42384371161460876
Epoch 360, training loss: 90.8757095336914 = 0.36543774604797363 + 10.0 * 9.051027297973633
Epoch 360, val loss: 0.41867420077323914
Epoch 370, training loss: 90.83891296386719 = 0.35865122079849243 + 10.0 * 9.048026084899902
Epoch 370, val loss: 0.4143814146518707
Epoch 380, training loss: 90.82556915283203 = 0.35253044962882996 + 10.0 * 9.047304153442383
Epoch 380, val loss: 0.4107150435447693
Epoch 390, training loss: 90.81159210205078 = 0.346969872713089 + 10.0 * 9.046462059020996
Epoch 390, val loss: 0.40761318802833557
Epoch 400, training loss: 90.78546905517578 = 0.34190887212753296 + 10.0 * 9.044355392456055
Epoch 400, val loss: 0.4048137962818146
Epoch 410, training loss: 90.7747573852539 = 0.33722779154777527 + 10.0 * 9.043752670288086
Epoch 410, val loss: 0.4024326801300049
Epoch 420, training loss: 90.79106140136719 = 0.33285558223724365 + 10.0 * 9.045820236206055
Epoch 420, val loss: 0.4003567099571228
Epoch 430, training loss: 90.76266479492188 = 0.3288697302341461 + 10.0 * 9.043378829956055
Epoch 430, val loss: 0.39846071600914
Epoch 440, training loss: 90.73153686523438 = 0.32519930601119995 + 10.0 * 9.040633201599121
Epoch 440, val loss: 0.3968053460121155
Epoch 450, training loss: 90.71981048583984 = 0.3217046856880188 + 10.0 * 9.039811134338379
Epoch 450, val loss: 0.39539432525634766
Epoch 460, training loss: 90.70756530761719 = 0.31832316517829895 + 10.0 * 9.038924217224121
Epoch 460, val loss: 0.3940446972846985
Epoch 470, training loss: 90.72374725341797 = 0.3150782585144043 + 10.0 * 9.04086685180664
Epoch 470, val loss: 0.39285922050476074
Epoch 480, training loss: 90.68633270263672 = 0.3119884729385376 + 10.0 * 9.037434577941895
Epoch 480, val loss: 0.3917448818683624
Epoch 490, training loss: 90.6760025024414 = 0.3090313673019409 + 10.0 * 9.036697387695312
Epoch 490, val loss: 0.3907518982887268
Epoch 500, training loss: 90.70941162109375 = 0.3061816692352295 + 10.0 * 9.040323257446289
Epoch 500, val loss: 0.38994649052619934
Epoch 510, training loss: 90.67243957519531 = 0.3034934103488922 + 10.0 * 9.036894798278809
Epoch 510, val loss: 0.3891172409057617
Epoch 520, training loss: 90.65027618408203 = 0.3009263873100281 + 10.0 * 9.034934997558594
Epoch 520, val loss: 0.38842472434043884
Epoch 530, training loss: 90.6796646118164 = 0.298421174287796 + 10.0 * 9.038124084472656
Epoch 530, val loss: 0.38783565163612366
Epoch 540, training loss: 90.63465881347656 = 0.2960018217563629 + 10.0 * 9.033864974975586
Epoch 540, val loss: 0.3873695731163025
Epoch 550, training loss: 90.62238311767578 = 0.29366225004196167 + 10.0 * 9.032872200012207
Epoch 550, val loss: 0.38690364360809326
Epoch 560, training loss: 90.61212921142578 = 0.2913751006126404 + 10.0 * 9.032075881958008
Epoch 560, val loss: 0.38651567697525024
Epoch 570, training loss: 90.64904022216797 = 0.2891328036785126 + 10.0 * 9.035990715026855
Epoch 570, val loss: 0.3861543536186218
Epoch 580, training loss: 90.60952758789062 = 0.2869691848754883 + 10.0 * 9.032255172729492
Epoch 580, val loss: 0.3858755826950073
Epoch 590, training loss: 90.59086608886719 = 0.28488391637802124 + 10.0 * 9.030598640441895
Epoch 590, val loss: 0.38559097051620483
Epoch 600, training loss: 90.58200073242188 = 0.2828390896320343 + 10.0 * 9.029916763305664
Epoch 600, val loss: 0.38540583848953247
Epoch 610, training loss: 90.57804870605469 = 0.2808160185813904 + 10.0 * 9.029723167419434
Epoch 610, val loss: 0.3852245509624481
Epoch 620, training loss: 90.63455200195312 = 0.2788214087486267 + 10.0 * 9.03557300567627
Epoch 620, val loss: 0.38509809970855713
Epoch 630, training loss: 90.56602478027344 = 0.27689987421035767 + 10.0 * 9.028912544250488
Epoch 630, val loss: 0.3850494921207428
Epoch 640, training loss: 90.55899047851562 = 0.27503830194473267 + 10.0 * 9.02839469909668
Epoch 640, val loss: 0.38498201966285706
Epoch 650, training loss: 90.57696533203125 = 0.2732032239437103 + 10.0 * 9.030376434326172
Epoch 650, val loss: 0.3850623667240143
Epoch 660, training loss: 90.55326843261719 = 0.2713911533355713 + 10.0 * 9.02818775177002
Epoch 660, val loss: 0.38506004214286804
Epoch 670, training loss: 90.54533386230469 = 0.26962271332740784 + 10.0 * 9.027570724487305
Epoch 670, val loss: 0.38511455059051514
Epoch 680, training loss: 90.536865234375 = 0.2678687572479248 + 10.0 * 9.026899337768555
Epoch 680, val loss: 0.3852836489677429
Epoch 690, training loss: 90.53294372558594 = 0.26612913608551025 + 10.0 * 9.026681900024414
Epoch 690, val loss: 0.38544416427612305
Epoch 700, training loss: 90.56951904296875 = 0.2644096910953522 + 10.0 * 9.030510902404785
Epoch 700, val loss: 0.3856840431690216
Epoch 710, training loss: 90.52691650390625 = 0.26272839307785034 + 10.0 * 9.026418685913086
Epoch 710, val loss: 0.38584935665130615
Epoch 720, training loss: 90.53158569335938 = 0.26108264923095703 + 10.0 * 9.027050018310547
Epoch 720, val loss: 0.3860712945461273
Epoch 730, training loss: 90.51362609863281 = 0.2594633102416992 + 10.0 * 9.025416374206543
Epoch 730, val loss: 0.38640424609184265
Epoch 740, training loss: 90.51610565185547 = 0.25785592198371887 + 10.0 * 9.025824546813965
Epoch 740, val loss: 0.3867490291595459
Epoch 750, training loss: 90.52433776855469 = 0.25625959038734436 + 10.0 * 9.02680778503418
Epoch 750, val loss: 0.38710665702819824
Epoch 760, training loss: 90.50910949707031 = 0.2547072768211365 + 10.0 * 9.025440216064453
Epoch 760, val loss: 0.38761991262435913
Epoch 770, training loss: 90.49644470214844 = 0.25315937399864197 + 10.0 * 9.024328231811523
Epoch 770, val loss: 0.38797006011009216
Epoch 780, training loss: 90.50326538085938 = 0.2516368627548218 + 10.0 * 9.025162696838379
Epoch 780, val loss: 0.3884301781654358
Epoch 790, training loss: 90.49407958984375 = 0.25012311339378357 + 10.0 * 9.024395942687988
Epoch 790, val loss: 0.3890920579433441
Epoch 800, training loss: 90.48512268066406 = 0.24864526093006134 + 10.0 * 9.023648262023926
Epoch 800, val loss: 0.38949069380760193
Epoch 810, training loss: 90.47429656982422 = 0.2471769005060196 + 10.0 * 9.022711753845215
Epoch 810, val loss: 0.39011168479919434
Epoch 820, training loss: 90.48319244384766 = 0.24571733176708221 + 10.0 * 9.023747444152832
Epoch 820, val loss: 0.3908214569091797
Epoch 830, training loss: 90.47486877441406 = 0.24427297711372375 + 10.0 * 9.023059844970703
Epoch 830, val loss: 0.39144080877304077
Epoch 840, training loss: 90.47388458251953 = 0.2428506761789322 + 10.0 * 9.023103713989258
Epoch 840, val loss: 0.39202749729156494
Epoch 850, training loss: 90.471435546875 = 0.24145416915416718 + 10.0 * 9.022997856140137
Epoch 850, val loss: 0.3927682936191559
Epoch 860, training loss: 90.46048736572266 = 0.24006609618663788 + 10.0 * 9.022042274475098
Epoch 860, val loss: 0.3933981955051422
Epoch 870, training loss: 90.46595764160156 = 0.23868922889232635 + 10.0 * 9.022727012634277
Epoch 870, val loss: 0.39421942830085754
Epoch 880, training loss: 90.4483642578125 = 0.23732692003250122 + 10.0 * 9.021103858947754
Epoch 880, val loss: 0.395002543926239
Epoch 890, training loss: 90.44440460205078 = 0.235976442694664 + 10.0 * 9.020842552185059
Epoch 890, val loss: 0.3958131670951843
Epoch 900, training loss: 90.49954986572266 = 0.23464062809944153 + 10.0 * 9.026491165161133
Epoch 900, val loss: 0.3967438042163849
Epoch 910, training loss: 90.43858337402344 = 0.23331484198570251 + 10.0 * 9.020526885986328
Epoch 910, val loss: 0.39753660559654236
Epoch 920, training loss: 90.43724060058594 = 0.2320163995027542 + 10.0 * 9.020522117614746
Epoch 920, val loss: 0.39844825863838196
Epoch 930, training loss: 90.42646026611328 = 0.2307228446006775 + 10.0 * 9.019574165344238
Epoch 930, val loss: 0.3993937373161316
Epoch 940, training loss: 90.43350982666016 = 0.22943072021007538 + 10.0 * 9.020407676696777
Epoch 940, val loss: 0.4003733694553375
Epoch 950, training loss: 90.42295837402344 = 0.22814999520778656 + 10.0 * 9.01948070526123
Epoch 950, val loss: 0.4013722836971283
Epoch 960, training loss: 90.42739868164062 = 0.22689244151115417 + 10.0 * 9.020051002502441
Epoch 960, val loss: 0.40237918496131897
Epoch 970, training loss: 90.41500091552734 = 0.22564393281936646 + 10.0 * 9.018935203552246
Epoch 970, val loss: 0.40344834327697754
Epoch 980, training loss: 90.44737243652344 = 0.22439680993556976 + 10.0 * 9.022297859191895
Epoch 980, val loss: 0.40442490577697754
Epoch 990, training loss: 90.41913604736328 = 0.2231641560792923 + 10.0 * 9.019597053527832
Epoch 990, val loss: 0.40569376945495605
Epoch 1000, training loss: 90.41082763671875 = 0.22194071114063263 + 10.0 * 9.018888473510742
Epoch 1000, val loss: 0.4067036509513855
Epoch 1010, training loss: 90.43621063232422 = 0.2207266092300415 + 10.0 * 9.0215482711792
Epoch 1010, val loss: 0.40789830684661865
Epoch 1020, training loss: 90.40492248535156 = 0.21952518820762634 + 10.0 * 9.018539428710938
Epoch 1020, val loss: 0.40910372138023376
Epoch 1030, training loss: 90.39649200439453 = 0.2183307260274887 + 10.0 * 9.017816543579102
Epoch 1030, val loss: 0.4102430045604706
Epoch 1040, training loss: 90.41699981689453 = 0.21714526414871216 + 10.0 * 9.01998519897461
Epoch 1040, val loss: 0.41144272685050964
Epoch 1050, training loss: 90.39673614501953 = 0.21596494317054749 + 10.0 * 9.01807689666748
Epoch 1050, val loss: 0.41276514530181885
Epoch 1060, training loss: 90.4024429321289 = 0.21479710936546326 + 10.0 * 9.01876449584961
Epoch 1060, val loss: 0.41395142674446106
Epoch 1070, training loss: 90.39070129394531 = 0.21363593637943268 + 10.0 * 9.017705917358398
Epoch 1070, val loss: 0.4153028726577759
Epoch 1080, training loss: 90.37835693359375 = 0.21248486638069153 + 10.0 * 9.016587257385254
Epoch 1080, val loss: 0.41654306650161743
Epoch 1090, training loss: 90.37608337402344 = 0.21133750677108765 + 10.0 * 9.016474723815918
Epoch 1090, val loss: 0.4178740978240967
Epoch 1100, training loss: 90.38360595703125 = 0.21019524335861206 + 10.0 * 9.017340660095215
Epoch 1100, val loss: 0.4191782474517822
Epoch 1110, training loss: 90.38584899902344 = 0.20906053483486176 + 10.0 * 9.017679214477539
Epoch 1110, val loss: 0.4205522835254669
Epoch 1120, training loss: 90.36785125732422 = 0.2079308032989502 + 10.0 * 9.015992164611816
Epoch 1120, val loss: 0.42188069224357605
Epoch 1130, training loss: 90.38484954833984 = 0.20681220293045044 + 10.0 * 9.017804145812988
Epoch 1130, val loss: 0.42320695519447327
Epoch 1140, training loss: 90.37144470214844 = 0.2057025283575058 + 10.0 * 9.016573905944824
Epoch 1140, val loss: 0.4246455729007721
Epoch 1150, training loss: 90.3746109008789 = 0.20459893345832825 + 10.0 * 9.017001152038574
Epoch 1150, val loss: 0.42595720291137695
Epoch 1160, training loss: 90.35558319091797 = 0.20350101590156555 + 10.0 * 9.01520824432373
Epoch 1160, val loss: 0.4275202751159668
Epoch 1170, training loss: 90.35322570800781 = 0.20240385830402374 + 10.0 * 9.015082359313965
Epoch 1170, val loss: 0.4289376735687256
Epoch 1180, training loss: 90.37557983398438 = 0.20131148397922516 + 10.0 * 9.017427444458008
Epoch 1180, val loss: 0.4303396940231323
Epoch 1190, training loss: 90.36224365234375 = 0.20022739470005035 + 10.0 * 9.016201972961426
Epoch 1190, val loss: 0.4318123757839203
Epoch 1200, training loss: 90.34808349609375 = 0.19914543628692627 + 10.0 * 9.014894485473633
Epoch 1200, val loss: 0.4333442747592926
Epoch 1210, training loss: 90.358642578125 = 0.19807069003582 + 10.0 * 9.016057014465332
Epoch 1210, val loss: 0.43477872014045715
Epoch 1220, training loss: 90.34394836425781 = 0.19700035452842712 + 10.0 * 9.014695167541504
Epoch 1220, val loss: 0.43627357482910156
Epoch 1230, training loss: 90.33830261230469 = 0.19593457877635956 + 10.0 * 9.014236450195312
Epoch 1230, val loss: 0.4378010332584381
Epoch 1240, training loss: 90.34037780761719 = 0.19487358629703522 + 10.0 * 9.014551162719727
Epoch 1240, val loss: 0.43937382102012634
Epoch 1250, training loss: 90.33618927001953 = 0.1938140094280243 + 10.0 * 9.014237403869629
Epoch 1250, val loss: 0.44095730781555176
Epoch 1260, training loss: 90.34712982177734 = 0.19275614619255066 + 10.0 * 9.015437126159668
Epoch 1260, val loss: 0.44252556562423706
Epoch 1270, training loss: 90.32653045654297 = 0.191702738404274 + 10.0 * 9.013483047485352
Epoch 1270, val loss: 0.4440262019634247
Epoch 1280, training loss: 90.32646942138672 = 0.1906524896621704 + 10.0 * 9.013582229614258
Epoch 1280, val loss: 0.44564539194107056
Epoch 1290, training loss: 90.327880859375 = 0.1896056979894638 + 10.0 * 9.013827323913574
Epoch 1290, val loss: 0.4471970200538635
Epoch 1300, training loss: 90.33061218261719 = 0.18856322765350342 + 10.0 * 9.014204978942871
Epoch 1300, val loss: 0.44875648617744446
Epoch 1310, training loss: 90.31916809082031 = 0.18752001225948334 + 10.0 * 9.013164520263672
Epoch 1310, val loss: 0.4503931403160095
Epoch 1320, training loss: 90.32479095458984 = 0.18648400902748108 + 10.0 * 9.013830184936523
Epoch 1320, val loss: 0.4519844353199005
Epoch 1330, training loss: 90.31803894042969 = 0.18545103073120117 + 10.0 * 9.013258934020996
Epoch 1330, val loss: 0.45369046926498413
Epoch 1340, training loss: 90.31077575683594 = 0.18442147970199585 + 10.0 * 9.012636184692383
Epoch 1340, val loss: 0.4554442763328552
Epoch 1350, training loss: 90.33070373535156 = 0.183393657207489 + 10.0 * 9.014730453491211
Epoch 1350, val loss: 0.4571511745452881
Epoch 1360, training loss: 90.30562591552734 = 0.18237176537513733 + 10.0 * 9.012325286865234
Epoch 1360, val loss: 0.4587033987045288
Epoch 1370, training loss: 90.30076599121094 = 0.18135002255439758 + 10.0 * 9.011941909790039
Epoch 1370, val loss: 0.4605036675930023
Epoch 1380, training loss: 90.29634094238281 = 0.1803298443555832 + 10.0 * 9.011601448059082
Epoch 1380, val loss: 0.4621528685092926
Epoch 1390, training loss: 90.31674194335938 = 0.17930671572685242 + 10.0 * 9.01374340057373
Epoch 1390, val loss: 0.4638946056365967
Epoch 1400, training loss: 90.30120086669922 = 0.1782914251089096 + 10.0 * 9.012290954589844
Epoch 1400, val loss: 0.46566811203956604
Epoch 1410, training loss: 90.2965087890625 = 0.17727479338645935 + 10.0 * 9.011922836303711
Epoch 1410, val loss: 0.46738311648368835
Epoch 1420, training loss: 90.28580474853516 = 0.17626404762268066 + 10.0 * 9.010953903198242
Epoch 1420, val loss: 0.4691658318042755
Epoch 1430, training loss: 90.29232025146484 = 0.1752517968416214 + 10.0 * 9.011706352233887
Epoch 1430, val loss: 0.47096383571624756
Epoch 1440, training loss: 90.29630279541016 = 0.17423957586288452 + 10.0 * 9.012206077575684
Epoch 1440, val loss: 0.472689688205719
Epoch 1450, training loss: 90.29368591308594 = 0.17323338985443115 + 10.0 * 9.012044906616211
Epoch 1450, val loss: 0.4744971692562103
Epoch 1460, training loss: 90.27957153320312 = 0.17222833633422852 + 10.0 * 9.010734558105469
Epoch 1460, val loss: 0.47642236948013306
Epoch 1470, training loss: 90.28109741210938 = 0.1712251752614975 + 10.0 * 9.010987281799316
Epoch 1470, val loss: 0.4782728850841522
Epoch 1480, training loss: 90.29800415039062 = 0.170225590467453 + 10.0 * 9.012777328491211
Epoch 1480, val loss: 0.4800586998462677
Epoch 1490, training loss: 90.27692413330078 = 0.16922584176063538 + 10.0 * 9.010769844055176
Epoch 1490, val loss: 0.48200827836990356
Epoch 1500, training loss: 90.27217864990234 = 0.16823118925094604 + 10.0 * 9.010395050048828
Epoch 1500, val loss: 0.48398157954216003
Epoch 1510, training loss: 90.28146362304688 = 0.16723600029945374 + 10.0 * 9.011423110961914
Epoch 1510, val loss: 0.48588502407073975
Epoch 1520, training loss: 90.26526641845703 = 0.1662420779466629 + 10.0 * 9.009902000427246
Epoch 1520, val loss: 0.48770758509635925
Epoch 1530, training loss: 90.2698745727539 = 0.16524867713451385 + 10.0 * 9.010462760925293
Epoch 1530, val loss: 0.4896797835826874
Epoch 1540, training loss: 90.27617645263672 = 0.16425909101963043 + 10.0 * 9.011191368103027
Epoch 1540, val loss: 0.49184173345565796
Epoch 1550, training loss: 90.26818084716797 = 0.16326592862606049 + 10.0 * 9.010491371154785
Epoch 1550, val loss: 0.49373289942741394
Epoch 1560, training loss: 90.26571655273438 = 0.1622762233018875 + 10.0 * 9.010343551635742
Epoch 1560, val loss: 0.49561402201652527
Epoch 1570, training loss: 90.25227355957031 = 0.1612876057624817 + 10.0 * 9.009099006652832
Epoch 1570, val loss: 0.4978034198284149
Epoch 1580, training loss: 90.25277709960938 = 0.16029784083366394 + 10.0 * 9.009247779846191
Epoch 1580, val loss: 0.4998323321342468
Epoch 1590, training loss: 90.28482818603516 = 0.1593097746372223 + 10.0 * 9.012552261352539
Epoch 1590, val loss: 0.5019211769104004
Epoch 1600, training loss: 90.25909423828125 = 0.15831878781318665 + 10.0 * 9.010077476501465
Epoch 1600, val loss: 0.5040229558944702
Epoch 1610, training loss: 90.2488784790039 = 0.15732723474502563 + 10.0 * 9.0091552734375
Epoch 1610, val loss: 0.506104588508606
Epoch 1620, training loss: 90.24574279785156 = 0.15633797645568848 + 10.0 * 9.008939743041992
Epoch 1620, val loss: 0.5083014369010925
Epoch 1630, training loss: 90.27490997314453 = 0.15534883737564087 + 10.0 * 9.011956214904785
Epoch 1630, val loss: 0.5105153918266296
Epoch 1640, training loss: 90.2459487915039 = 0.15435180068016052 + 10.0 * 9.009160041809082
Epoch 1640, val loss: 0.5125783681869507
Epoch 1650, training loss: 90.23763275146484 = 0.15336035192012787 + 10.0 * 9.008427619934082
Epoch 1650, val loss: 0.5148109793663025
Epoch 1660, training loss: 90.25727844238281 = 0.1523718684911728 + 10.0 * 9.010490417480469
Epoch 1660, val loss: 0.5171242952346802
Epoch 1670, training loss: 90.2376708984375 = 0.1513734757900238 + 10.0 * 9.00862979888916
Epoch 1670, val loss: 0.5191164612770081
Epoch 1680, training loss: 90.23633575439453 = 0.15038233995437622 + 10.0 * 9.00859546661377
Epoch 1680, val loss: 0.5214390754699707
Epoch 1690, training loss: 90.23540496826172 = 0.1493908166885376 + 10.0 * 9.008601188659668
Epoch 1690, val loss: 0.5236150026321411
Epoch 1700, training loss: 90.22909545898438 = 0.14839501678943634 + 10.0 * 9.00806999206543
Epoch 1700, val loss: 0.5259603261947632
Epoch 1710, training loss: 90.22622680664062 = 0.14740172028541565 + 10.0 * 9.007883071899414
Epoch 1710, val loss: 0.5282881855964661
Epoch 1720, training loss: 90.25794982910156 = 0.1464121788740158 + 10.0 * 9.011153221130371
Epoch 1720, val loss: 0.5304967761039734
Epoch 1730, training loss: 90.23167419433594 = 0.14541375637054443 + 10.0 * 9.008625984191895
Epoch 1730, val loss: 0.5328782200813293
Epoch 1740, training loss: 90.21633911132812 = 0.14442332088947296 + 10.0 * 9.00719165802002
Epoch 1740, val loss: 0.5353434681892395
Epoch 1750, training loss: 90.21627807617188 = 0.14343327283859253 + 10.0 * 9.007284164428711
Epoch 1750, val loss: 0.5378145575523376
Epoch 1760, training loss: 90.23493957519531 = 0.14244338870048523 + 10.0 * 9.009249687194824
Epoch 1760, val loss: 0.5402974486351013
Epoch 1770, training loss: 90.21452331542969 = 0.14144673943519592 + 10.0 * 9.007307052612305
Epoch 1770, val loss: 0.542553722858429
Epoch 1780, training loss: 90.20881652832031 = 0.14045238494873047 + 10.0 * 9.006836891174316
Epoch 1780, val loss: 0.5450992584228516
Epoch 1790, training loss: 90.20648193359375 = 0.13945625722408295 + 10.0 * 9.006702423095703
Epoch 1790, val loss: 0.5476147532463074
Epoch 1800, training loss: 90.21827697753906 = 0.13846038281917572 + 10.0 * 9.00798225402832
Epoch 1800, val loss: 0.5502190589904785
Epoch 1810, training loss: 90.21099853515625 = 0.13745778799057007 + 10.0 * 9.007353782653809
Epoch 1810, val loss: 0.5526997447013855
Epoch 1820, training loss: 90.20894622802734 = 0.13645753264427185 + 10.0 * 9.007248878479004
Epoch 1820, val loss: 0.5553470849990845
Epoch 1830, training loss: 90.20250701904297 = 0.13545863330364227 + 10.0 * 9.006704330444336
Epoch 1830, val loss: 0.5578721165657043
Epoch 1840, training loss: 90.21007537841797 = 0.13445886969566345 + 10.0 * 9.007561683654785
Epoch 1840, val loss: 0.5606198310852051
Epoch 1850, training loss: 90.20304870605469 = 0.13345195353031158 + 10.0 * 9.006959915161133
Epoch 1850, val loss: 0.5631288886070251
Epoch 1860, training loss: 90.19953155517578 = 0.1324511021375656 + 10.0 * 9.006708145141602
Epoch 1860, val loss: 0.5656758546829224
Epoch 1870, training loss: 90.19908142089844 = 0.13144434988498688 + 10.0 * 9.006763458251953
Epoch 1870, val loss: 0.5684406161308289
Epoch 1880, training loss: 90.21451568603516 = 0.13043616712093353 + 10.0 * 9.008407592773438
Epoch 1880, val loss: 0.5712442994117737
Epoch 1890, training loss: 90.19145202636719 = 0.12942245602607727 + 10.0 * 9.006202697753906
Epoch 1890, val loss: 0.5740089416503906
Epoch 1900, training loss: 90.18732452392578 = 0.12841802835464478 + 10.0 * 9.005890846252441
Epoch 1900, val loss: 0.5766899585723877
Epoch 1910, training loss: 90.18325805664062 = 0.12740643322467804 + 10.0 * 9.005585670471191
Epoch 1910, val loss: 0.5795111656188965
Epoch 1920, training loss: 90.18045806884766 = 0.1263924539089203 + 10.0 * 9.005406379699707
Epoch 1920, val loss: 0.5824379324913025
Epoch 1930, training loss: 90.17945861816406 = 0.12537319958209991 + 10.0 * 9.00540828704834
Epoch 1930, val loss: 0.5853606462478638
Epoch 1940, training loss: 90.2381820678711 = 0.1243608221411705 + 10.0 * 9.011382102966309
Epoch 1940, val loss: 0.5883239507675171
Epoch 1950, training loss: 90.21109008789062 = 0.12334171682596207 + 10.0 * 9.008774757385254
Epoch 1950, val loss: 0.5909659266471863
Epoch 1960, training loss: 90.17847442626953 = 0.12232141196727753 + 10.0 * 9.005615234375
Epoch 1960, val loss: 0.5939372181892395
Epoch 1970, training loss: 90.1723861694336 = 0.12130627036094666 + 10.0 * 9.005107879638672
Epoch 1970, val loss: 0.5969602465629578
Epoch 1980, training loss: 90.17076110839844 = 0.1202867180109024 + 10.0 * 9.005047798156738
Epoch 1980, val loss: 0.6000384092330933
Epoch 1990, training loss: 90.17108917236328 = 0.11926669627428055 + 10.0 * 9.005182266235352
Epoch 1990, val loss: 0.6030251383781433
Epoch 2000, training loss: 90.19579315185547 = 0.11824630945920944 + 10.0 * 9.0077543258667
Epoch 2000, val loss: 0.6062114238739014
Epoch 2010, training loss: 90.17306518554688 = 0.11721620708703995 + 10.0 * 9.005584716796875
Epoch 2010, val loss: 0.609069287776947
Epoch 2020, training loss: 90.17015838623047 = 0.11619088053703308 + 10.0 * 9.005396842956543
Epoch 2020, val loss: 0.6122645735740662
Epoch 2030, training loss: 90.17959594726562 = 0.1151631772518158 + 10.0 * 9.00644302368164
Epoch 2030, val loss: 0.6154136061668396
Epoch 2040, training loss: 90.1657485961914 = 0.11413610726594925 + 10.0 * 9.00516128540039
Epoch 2040, val loss: 0.6185568571090698
Epoch 2050, training loss: 90.15804290771484 = 0.11310878396034241 + 10.0 * 9.004493713378906
Epoch 2050, val loss: 0.6217399835586548
Epoch 2060, training loss: 90.18024444580078 = 0.1120871901512146 + 10.0 * 9.006815910339355
Epoch 2060, val loss: 0.6252153515815735
Epoch 2070, training loss: 90.15614318847656 = 0.11105195432901382 + 10.0 * 9.004508972167969
Epoch 2070, val loss: 0.6281980276107788
Epoch 2080, training loss: 90.15335845947266 = 0.11002958565950394 + 10.0 * 9.004332542419434
Epoch 2080, val loss: 0.6315963268280029
Epoch 2090, training loss: 90.15076446533203 = 0.10900132358074188 + 10.0 * 9.004176139831543
Epoch 2090, val loss: 0.63477623462677
Epoch 2100, training loss: 90.17507934570312 = 0.10798045992851257 + 10.0 * 9.006710052490234
Epoch 2100, val loss: 0.6383035182952881
Epoch 2110, training loss: 90.14989471435547 = 0.10694326460361481 + 10.0 * 9.004295349121094
Epoch 2110, val loss: 0.6414613127708435
Epoch 2120, training loss: 90.14825439453125 = 0.10592368245124817 + 10.0 * 9.004232406616211
Epoch 2120, val loss: 0.6449599266052246
Epoch 2130, training loss: 90.14351654052734 = 0.10489441454410553 + 10.0 * 9.003862380981445
Epoch 2130, val loss: 0.648357093334198
Epoch 2140, training loss: 90.15164947509766 = 0.10387133806943893 + 10.0 * 9.004777908325195
Epoch 2140, val loss: 0.65206378698349
Epoch 2150, training loss: 90.15052795410156 = 0.10284055769443512 + 10.0 * 9.004768371582031
Epoch 2150, val loss: 0.6554739475250244
Epoch 2160, training loss: 90.14693450927734 = 0.10181663185358047 + 10.0 * 9.004511833190918
Epoch 2160, val loss: 0.6589186787605286
Epoch 2170, training loss: 90.13432312011719 = 0.10079219192266464 + 10.0 * 9.003353118896484
Epoch 2170, val loss: 0.6624565124511719
Epoch 2180, training loss: 90.1370620727539 = 0.09976807236671448 + 10.0 * 9.003728866577148
Epoch 2180, val loss: 0.6661462187767029
Epoch 2190, training loss: 90.14727020263672 = 0.09874501824378967 + 10.0 * 9.004852294921875
Epoch 2190, val loss: 0.6698735952377319
Epoch 2200, training loss: 90.1341552734375 = 0.09771554172039032 + 10.0 * 9.003643989562988
Epoch 2200, val loss: 0.673406183719635
Epoch 2210, training loss: 90.13170623779297 = 0.09668862819671631 + 10.0 * 9.003501892089844
Epoch 2210, val loss: 0.6770265698432922
Epoch 2220, training loss: 90.14796447753906 = 0.09566644579172134 + 10.0 * 9.005229949951172
Epoch 2220, val loss: 0.6807485222816467
Epoch 2230, training loss: 90.14225769042969 = 0.09464386850595474 + 10.0 * 9.004761695861816
Epoch 2230, val loss: 0.6846767067909241
Epoch 2240, training loss: 90.12649536132812 = 0.09361960738897324 + 10.0 * 9.003287315368652
Epoch 2240, val loss: 0.6883094906806946
Epoch 2250, training loss: 90.12860870361328 = 0.09260768443346024 + 10.0 * 9.003600120544434
Epoch 2250, val loss: 0.6922525763511658
Epoch 2260, training loss: 90.13182830810547 = 0.09158939123153687 + 10.0 * 9.004023551940918
Epoch 2260, val loss: 0.6960141658782959
Epoch 2270, training loss: 90.12286376953125 = 0.09057654440402985 + 10.0 * 9.003229141235352
Epoch 2270, val loss: 0.699988603591919
Epoch 2280, training loss: 90.12152099609375 = 0.08956743776798248 + 10.0 * 9.003194808959961
Epoch 2280, val loss: 0.7038904428482056
Epoch 2290, training loss: 90.13353729248047 = 0.0885605737566948 + 10.0 * 9.004497528076172
Epoch 2290, val loss: 0.7078547477722168
Epoch 2300, training loss: 90.12117004394531 = 0.0875524953007698 + 10.0 * 9.003361701965332
Epoch 2300, val loss: 0.7117958664894104
Epoch 2310, training loss: 90.11186981201172 = 0.08654900640249252 + 10.0 * 9.002532005310059
Epoch 2310, val loss: 0.7158930897712708
Epoch 2320, training loss: 90.1097640991211 = 0.08554922044277191 + 10.0 * 9.002421379089355
Epoch 2320, val loss: 0.7199398875236511
Epoch 2330, training loss: 90.11695098876953 = 0.08455551415681839 + 10.0 * 9.003239631652832
Epoch 2330, val loss: 0.7239853143692017
Epoch 2340, training loss: 90.11998748779297 = 0.08356005698442459 + 10.0 * 9.003643035888672
Epoch 2340, val loss: 0.72807377576828
Epoch 2350, training loss: 90.11640930175781 = 0.08257115632295609 + 10.0 * 9.00338363647461
Epoch 2350, val loss: 0.7324342727661133
Epoch 2360, training loss: 90.11412048339844 = 0.0815889984369278 + 10.0 * 9.003252983093262
Epoch 2360, val loss: 0.7364848256111145
Epoch 2370, training loss: 90.1020278930664 = 0.08060773462057114 + 10.0 * 9.002141952514648
Epoch 2370, val loss: 0.74077969789505
Epoch 2380, training loss: 90.10983276367188 = 0.07963699102401733 + 10.0 * 9.003019332885742
Epoch 2380, val loss: 0.7451052665710449
Epoch 2390, training loss: 90.10392761230469 = 0.0786665678024292 + 10.0 * 9.00252628326416
Epoch 2390, val loss: 0.7493770718574524
Epoch 2400, training loss: 90.09859466552734 = 0.07770531624555588 + 10.0 * 9.00208854675293
Epoch 2400, val loss: 0.7535699605941772
Epoch 2410, training loss: 90.09733581542969 = 0.07675011456012726 + 10.0 * 9.002058029174805
Epoch 2410, val loss: 0.7582055926322937
Epoch 2420, training loss: 90.11505126953125 = 0.0758044570684433 + 10.0 * 9.003924369812012
Epoch 2420, val loss: 0.7628108263015747
Epoch 2430, training loss: 90.10357666015625 = 0.07485772669315338 + 10.0 * 9.0028715133667
Epoch 2430, val loss: 0.7667167782783508
Epoch 2440, training loss: 90.0959243774414 = 0.07391774654388428 + 10.0 * 9.00220012664795
Epoch 2440, val loss: 0.771382212638855
Epoch 2450, training loss: 90.0942611694336 = 0.0729864090681076 + 10.0 * 9.002127647399902
Epoch 2450, val loss: 0.7757200598716736
Epoch 2460, training loss: 90.08847045898438 = 0.07205728441476822 + 10.0 * 9.001641273498535
Epoch 2460, val loss: 0.7802731990814209
Epoch 2470, training loss: 90.09172821044922 = 0.07113684713840485 + 10.0 * 9.002058982849121
Epoch 2470, val loss: 0.7848141193389893
Epoch 2480, training loss: 90.0909652709961 = 0.07021874934434891 + 10.0 * 9.002074241638184
Epoch 2480, val loss: 0.7894246578216553
Epoch 2490, training loss: 90.11136627197266 = 0.06931330263614655 + 10.0 * 9.004205703735352
Epoch 2490, val loss: 0.7940648794174194
Epoch 2500, training loss: 90.08775329589844 = 0.06841214746236801 + 10.0 * 9.001934051513672
Epoch 2500, val loss: 0.7983339428901672
Epoch 2510, training loss: 90.07855987548828 = 0.06751567870378494 + 10.0 * 9.001104354858398
Epoch 2510, val loss: 0.8029706478118896
Epoch 2520, training loss: 90.08439636230469 = 0.06663218140602112 + 10.0 * 9.001775741577148
Epoch 2520, val loss: 0.807663083076477
Epoch 2530, training loss: 90.08712005615234 = 0.06575225293636322 + 10.0 * 9.002137184143066
Epoch 2530, val loss: 0.8122133612632751
Epoch 2540, training loss: 90.07890319824219 = 0.06487978994846344 + 10.0 * 9.001401901245117
Epoch 2540, val loss: 0.8167961239814758
Epoch 2550, training loss: 90.09065246582031 = 0.06401822715997696 + 10.0 * 9.002663612365723
Epoch 2550, val loss: 0.8215551972389221
Epoch 2560, training loss: 90.07035064697266 = 0.06316298991441727 + 10.0 * 9.00071907043457
Epoch 2560, val loss: 0.8263322114944458
Epoch 2570, training loss: 90.06705474853516 = 0.06231652945280075 + 10.0 * 9.000473976135254
Epoch 2570, val loss: 0.8310903906822205
Epoch 2580, training loss: 90.07006072998047 = 0.06147794798016548 + 10.0 * 9.000858306884766
Epoch 2580, val loss: 0.8359132409095764
Epoch 2590, training loss: 90.09471893310547 = 0.060658540576696396 + 10.0 * 9.003405570983887
Epoch 2590, val loss: 0.8409883379936218
Epoch 2600, training loss: 90.0713119506836 = 0.059822000563144684 + 10.0 * 9.00114917755127
Epoch 2600, val loss: 0.8451715707778931
Epoch 2610, training loss: 90.07266235351562 = 0.05900878086686134 + 10.0 * 9.001365661621094
Epoch 2610, val loss: 0.8500754237174988
Epoch 2620, training loss: 90.08067321777344 = 0.05820365995168686 + 10.0 * 9.002246856689453
Epoch 2620, val loss: 0.8547481298446655
Epoch 2630, training loss: 90.07003021240234 = 0.05740990862250328 + 10.0 * 9.001261711120605
Epoch 2630, val loss: 0.8595025539398193
Epoch 2640, training loss: 90.06212615966797 = 0.05662314221262932 + 10.0 * 9.000550270080566
Epoch 2640, val loss: 0.8644446134567261
Epoch 2650, training loss: 90.06918334960938 = 0.05584827437996864 + 10.0 * 9.001333236694336
Epoch 2650, val loss: 0.8694401383399963
Epoch 2660, training loss: 90.05872344970703 = 0.05507788062095642 + 10.0 * 9.000364303588867
Epoch 2660, val loss: 0.8743355870246887
Epoch 2670, training loss: 90.06232452392578 = 0.05432261899113655 + 10.0 * 9.000800132751465
Epoch 2670, val loss: 0.8791999816894531
Epoch 2680, training loss: 90.0628662109375 = 0.05357241630554199 + 10.0 * 9.00092887878418
Epoch 2680, val loss: 0.8840304613113403
Epoch 2690, training loss: 90.0556411743164 = 0.05283098667860031 + 10.0 * 9.000280380249023
Epoch 2690, val loss: 0.8890171647071838
Epoch 2700, training loss: 90.05560302734375 = 0.052101004868745804 + 10.0 * 9.000349998474121
Epoch 2700, val loss: 0.8940922617912292
Epoch 2710, training loss: 90.05659484863281 = 0.05137919634580612 + 10.0 * 9.000521659851074
Epoch 2710, val loss: 0.8989943861961365
Epoch 2720, training loss: 90.06356048583984 = 0.05066673830151558 + 10.0 * 9.001289367675781
Epoch 2720, val loss: 0.9037479162216187
Epoch 2730, training loss: 90.05022430419922 = 0.0499621219933033 + 10.0 * 9.000025749206543
Epoch 2730, val loss: 0.9085886478424072
Epoch 2740, training loss: 90.048583984375 = 0.0492686964571476 + 10.0 * 8.999931335449219
Epoch 2740, val loss: 0.9136105179786682
Epoch 2750, training loss: 90.0514144897461 = 0.04858534410595894 + 10.0 * 9.000283241271973
Epoch 2750, val loss: 0.9184987545013428
Epoch 2760, training loss: 90.04830169677734 = 0.047908488661050797 + 10.0 * 9.000040054321289
Epoch 2760, val loss: 0.9233341217041016
Epoch 2770, training loss: 90.05361938476562 = 0.04724264517426491 + 10.0 * 9.000638008117676
Epoch 2770, val loss: 0.9283615350723267
Epoch 2780, training loss: 90.0486068725586 = 0.04658479243516922 + 10.0 * 9.000202178955078
Epoch 2780, val loss: 0.9336112141609192
Epoch 2790, training loss: 90.03838348388672 = 0.045932378619909286 + 10.0 * 8.999245643615723
Epoch 2790, val loss: 0.9383975267410278
Epoch 2800, training loss: 90.04032897949219 = 0.04529105871915817 + 10.0 * 8.999504089355469
Epoch 2800, val loss: 0.9434030055999756
Epoch 2810, training loss: 90.06201934814453 = 0.04466303065419197 + 10.0 * 9.00173568725586
Epoch 2810, val loss: 0.9483953714370728
Epoch 2820, training loss: 90.05274200439453 = 0.04403836280107498 + 10.0 * 9.000870704650879
Epoch 2820, val loss: 0.9531667232513428
Epoch 2830, training loss: 90.03692626953125 = 0.043427396565675735 + 10.0 * 8.999349594116211
Epoch 2830, val loss: 0.9583365321159363
Epoch 2840, training loss: 90.0316162109375 = 0.042820535600185394 + 10.0 * 8.998879432678223
Epoch 2840, val loss: 0.9631096720695496
Epoch 2850, training loss: 90.0293960571289 = 0.04222545772790909 + 10.0 * 8.998716354370117
Epoch 2850, val loss: 0.9680405259132385
Epoch 2860, training loss: 90.04207611083984 = 0.04164564236998558 + 10.0 * 9.000042915344238
Epoch 2860, val loss: 0.972617506980896
Epoch 2870, training loss: 90.03580474853516 = 0.04106011614203453 + 10.0 * 8.99947452545166
Epoch 2870, val loss: 0.977913498878479
Epoch 2880, training loss: 90.03118896484375 = 0.04049070179462433 + 10.0 * 8.999070167541504
Epoch 2880, val loss: 0.9826387763023376
Epoch 2890, training loss: 90.03389739990234 = 0.039931394159793854 + 10.0 * 8.999396324157715
Epoch 2890, val loss: 0.987937331199646
Epoch 2900, training loss: 90.0323257446289 = 0.039378572255373 + 10.0 * 8.999295234680176
Epoch 2900, val loss: 0.9923428893089294
Epoch 2910, training loss: 90.02674865722656 = 0.03883175179362297 + 10.0 * 8.998791694641113
Epoch 2910, val loss: 0.9971303343772888
Epoch 2920, training loss: 90.03343200683594 = 0.038297031074762344 + 10.0 * 8.999513626098633
Epoch 2920, val loss: 1.002201795578003
Epoch 2930, training loss: 90.02825927734375 = 0.03776373714208603 + 10.0 * 8.999049186706543
Epoch 2930, val loss: 1.0070185661315918
Epoch 2940, training loss: 90.03068542480469 = 0.03724246844649315 + 10.0 * 8.999344825744629
Epoch 2940, val loss: 1.0116640329360962
Epoch 2950, training loss: 90.03028106689453 = 0.03673158586025238 + 10.0 * 8.99935531616211
Epoch 2950, val loss: 1.0163452625274658
Epoch 2960, training loss: 90.02061462402344 = 0.03622402995824814 + 10.0 * 8.998438835144043
Epoch 2960, val loss: 1.021404504776001
Epoch 2970, training loss: 90.02909851074219 = 0.035732660442590714 + 10.0 * 8.999336242675781
Epoch 2970, val loss: 1.0261974334716797
Epoch 2980, training loss: 90.02708435058594 = 0.03523937612771988 + 10.0 * 8.999184608459473
Epoch 2980, val loss: 1.0309429168701172
Epoch 2990, training loss: 90.0186538696289 = 0.03475630655884743 + 10.0 * 8.99838924407959
Epoch 2990, val loss: 1.0353151559829712
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8503
Overall ASR: 0.6897
Flip ASR: 0.6126/1554 nodes
./selected_nodes/Pubmed/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.26 GiB already allocated; 897.69 MiB free; 5.53 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.3 der2:0 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 104.69015502929688 = 1.1002486944198608 + 10.0 * 10.358990669250488
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 319, in _train_with_val
    loss.backward()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 6.11 GiB already allocated; 251.69 MiB free; 6.16 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.3 der2:0 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 104.69487762451172 = 1.103918433189392 + 10.0 * 10.359095573425293
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 319, in _train_with_val
    loss.backward()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 6.11 GiB already allocated; 251.69 MiB free; 6.16 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 104.6965560913086 = 1.1053366661071777 + 10.0 * 10.359121322631836
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 319, in _train_with_val
    loss.backward()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 6.11 GiB already allocated; 255.69 MiB free; 6.15 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 104.68168640136719 = 1.0908197164535522 + 10.0 * 10.359086990356445
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 319, in _train_with_val
    loss.backward()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 6.11 GiB already allocated; 255.69 MiB free; 6.15 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 104.69152069091797 = 1.1000527143478394 + 10.0 * 10.359147071838379
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 319, in _train_with_val
    loss.backward()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 6.11 GiB already allocated; 251.69 MiB free; 6.16 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.3 der2:0.2 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 104.6903076171875 = 1.1001161336898804 + 10.0 * 10.35901927947998
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 319, in _train_with_val
    loss.backward()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 6.11 GiB already allocated; 925.69 MiB free; 6.15 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.3 der2:0.2 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 104.69276428222656 = 1.103297233581543 + 10.0 * 10.358946800231934
Epoch 0, val loss: 1.101454734802246
Epoch 10, training loss: 104.6287841796875 = 1.091654896736145 + 10.0 * 10.353713035583496
Epoch 10, val loss: 1.0897622108459473
Epoch 20, training loss: 103.88711547851562 = 1.0770882368087769 + 10.0 * 10.28100299835205
Epoch 20, val loss: 1.0757299661636353
Epoch 30, training loss: 99.62504577636719 = 1.0669389963150024 + 10.0 * 9.855810165405273
Epoch 30, val loss: 1.0660099983215332
Epoch 40, training loss: 96.34313201904297 = 1.0544525384902954 + 10.0 * 9.528867721557617
Epoch 40, val loss: 1.05315101146698
Epoch 50, training loss: 94.92134857177734 = 1.035386323928833 + 10.0 * 9.388596534729004
Epoch 50, val loss: 1.0339126586914062
Epoch 60, training loss: 94.45823669433594 = 1.015770673751831 + 10.0 * 9.344246864318848
Epoch 60, val loss: 1.0147405862808228
Epoch 70, training loss: 93.94073486328125 = 1.0013926029205322 + 10.0 * 9.293933868408203
Epoch 70, val loss: 1.0009170770645142
Epoch 80, training loss: 93.56265258789062 = 0.9924471974372864 + 10.0 * 9.257020950317383
Epoch 80, val loss: 0.9919738173484802
Epoch 90, training loss: 93.3248291015625 = 0.9827589988708496 + 10.0 * 9.234207153320312
Epoch 90, val loss: 0.9818423390388489
Epoch 100, training loss: 92.9979476928711 = 0.9699379801750183 + 10.0 * 9.202800750732422
Epoch 100, val loss: 0.9692609906196594
Epoch 110, training loss: 92.65776062011719 = 0.9592558741569519 + 10.0 * 9.16985034942627
Epoch 110, val loss: 0.9592198729515076
Epoch 120, training loss: 92.4551773071289 = 0.9493308663368225 + 10.0 * 9.15058422088623
Epoch 120, val loss: 0.9494860768318176
Epoch 130, training loss: 92.31706237792969 = 0.9341637492179871 + 10.0 * 9.138289451599121
Epoch 130, val loss: 0.9339399337768555
Epoch 140, training loss: 92.19910430908203 = 0.9151957631111145 + 10.0 * 9.128390312194824
Epoch 140, val loss: 0.9151903390884399
Epoch 150, training loss: 92.10099029541016 = 0.8943930268287659 + 10.0 * 9.120659828186035
Epoch 150, val loss: 0.8948792815208435
Epoch 160, training loss: 92.01536560058594 = 0.8711527585983276 + 10.0 * 9.114420890808105
Epoch 160, val loss: 0.8719639182090759
Epoch 170, training loss: 91.93913269042969 = 0.8445545434951782 + 10.0 * 9.109457969665527
Epoch 170, val loss: 0.8460288643836975
Epoch 180, training loss: 91.85694885253906 = 0.8148180246353149 + 10.0 * 9.104212760925293
Epoch 180, val loss: 0.8172025084495544
Epoch 190, training loss: 91.80308532714844 = 0.7822099924087524 + 10.0 * 9.102087020874023
Epoch 190, val loss: 0.7857645153999329
Epoch 200, training loss: 91.70867156982422 = 0.7474150657653809 + 10.0 * 9.096125602722168
Epoch 200, val loss: 0.75242018699646
Epoch 210, training loss: 91.6308364868164 = 0.711253821849823 + 10.0 * 9.091958999633789
Epoch 210, val loss: 0.7180677056312561
Epoch 220, training loss: 91.55482482910156 = 0.6741761565208435 + 10.0 * 9.088064193725586
Epoch 220, val loss: 0.683275043964386
Epoch 230, training loss: 91.47864532470703 = 0.6371502876281738 + 10.0 * 9.084149360656738
Epoch 230, val loss: 0.6489288210868835
Epoch 240, training loss: 91.40757751464844 = 0.6011592149734497 + 10.0 * 9.080641746520996
Epoch 240, val loss: 0.6160224080085754
Epoch 250, training loss: 91.34727478027344 = 0.5668199062347412 + 10.0 * 9.078045845031738
Epoch 250, val loss: 0.5849811434745789
Epoch 260, training loss: 91.34201049804688 = 0.5347301363945007 + 10.0 * 9.080728530883789
Epoch 260, val loss: 0.5565215349197388
Epoch 270, training loss: 91.25125122070312 = 0.5063228011131287 + 10.0 * 9.074492454528809
Epoch 270, val loss: 0.5318568348884583
Epoch 280, training loss: 91.1952133178711 = 0.48172855377197266 + 10.0 * 9.071348190307617
Epoch 280, val loss: 0.5108639001846313
Epoch 290, training loss: 91.1495590209961 = 0.4603649973869324 + 10.0 * 9.06891918182373
Epoch 290, val loss: 0.49306192994117737
Epoch 300, training loss: 91.1124267578125 = 0.44173291325569153 + 10.0 * 9.067069053649902
Epoch 300, val loss: 0.4779190421104431
Epoch 310, training loss: 91.08797454833984 = 0.4255882799625397 + 10.0 * 9.066238403320312
Epoch 310, val loss: 0.46522897481918335
Epoch 320, training loss: 91.07286071777344 = 0.41205012798309326 + 10.0 * 9.066081047058105
Epoch 320, val loss: 0.45484596490859985
Epoch 330, training loss: 91.03596496582031 = 0.40073391795158386 + 10.0 * 9.063523292541504
Epoch 330, val loss: 0.4464235305786133
Epoch 340, training loss: 91.01150512695312 = 0.39093756675720215 + 10.0 * 9.062056541442871
Epoch 340, val loss: 0.43943166732788086
Epoch 350, training loss: 91.00669860839844 = 0.38225069642066956 + 10.0 * 9.062444686889648
Epoch 350, val loss: 0.433384507894516
Epoch 360, training loss: 90.997314453125 = 0.37451401352882385 + 10.0 * 9.062280654907227
Epoch 360, val loss: 0.42831483483314514
Epoch 370, training loss: 90.96366119384766 = 0.3677596151828766 + 10.0 * 9.059590339660645
Epoch 370, val loss: 0.42397749423980713
Epoch 380, training loss: 90.9439926147461 = 0.36172226071357727 + 10.0 * 9.058226585388184
Epoch 380, val loss: 0.4202567934989929
Epoch 390, training loss: 90.95201873779297 = 0.3562029004096985 + 10.0 * 9.059581756591797
Epoch 390, val loss: 0.41701462864875793
Epoch 400, training loss: 90.9246597290039 = 0.35112160444259644 + 10.0 * 9.057353973388672
Epoch 400, val loss: 0.41413751244544983
Epoch 410, training loss: 90.90179443359375 = 0.34650906920433044 + 10.0 * 9.05552864074707
Epoch 410, val loss: 0.4115835726261139
Epoch 420, training loss: 90.8891372680664 = 0.3422194719314575 + 10.0 * 9.054692268371582
Epoch 420, val loss: 0.40934404730796814
Epoch 430, training loss: 90.88137817382812 = 0.3381763696670532 + 10.0 * 9.054320335388184
Epoch 430, val loss: 0.407288134098053
Epoch 440, training loss: 90.868408203125 = 0.33437731862068176 + 10.0 * 9.0534029006958
Epoch 440, val loss: 0.4054933190345764
Epoch 450, training loss: 90.85662078857422 = 0.33084219694137573 + 10.0 * 9.05257797241211
Epoch 450, val loss: 0.4038221538066864
Epoch 460, training loss: 90.84187316894531 = 0.3274848759174347 + 10.0 * 9.05143928527832
Epoch 460, val loss: 0.4022906720638275
Epoch 470, training loss: 90.84717559814453 = 0.3242572546005249 + 10.0 * 9.052291870117188
Epoch 470, val loss: 0.4009644389152527
Epoch 480, training loss: 90.82867431640625 = 0.3211989104747772 + 10.0 * 9.050747871398926
Epoch 480, val loss: 0.39969736337661743
Epoch 490, training loss: 90.81103515625 = 0.3182920217514038 + 10.0 * 9.049274444580078
Epoch 490, val loss: 0.39848437905311584
Epoch 500, training loss: 90.80284118652344 = 0.3154764771461487 + 10.0 * 9.048736572265625
Epoch 500, val loss: 0.3974069356918335
Epoch 510, training loss: 90.81524658203125 = 0.31273871660232544 + 10.0 * 9.050251007080078
Epoch 510, val loss: 0.3963651955127716
Epoch 520, training loss: 90.7900390625 = 0.3101038634777069 + 10.0 * 9.047993659973145
Epoch 520, val loss: 0.39546963572502136
Epoch 530, training loss: 90.79390716552734 = 0.30756714940071106 + 10.0 * 9.04863452911377
Epoch 530, val loss: 0.3946515917778015
Epoch 540, training loss: 90.7780990600586 = 0.3051413595676422 + 10.0 * 9.047296524047852
Epoch 540, val loss: 0.39386796951293945
Epoch 550, training loss: 90.76287841796875 = 0.3027849495410919 + 10.0 * 9.046009063720703
Epoch 550, val loss: 0.39310842752456665
Epoch 560, training loss: 90.75479888916016 = 0.3004695475101471 + 10.0 * 9.045433044433594
Epoch 560, val loss: 0.39247632026672363
Epoch 570, training loss: 90.79295349121094 = 0.2982019782066345 + 10.0 * 9.049474716186523
Epoch 570, val loss: 0.3918269872665405
Epoch 580, training loss: 90.74929809570312 = 0.2959916889667511 + 10.0 * 9.045331001281738
Epoch 580, val loss: 0.3913571238517761
Epoch 590, training loss: 90.7384262084961 = 0.29386231303215027 + 10.0 * 9.044456481933594
Epoch 590, val loss: 0.3907967805862427
Epoch 600, training loss: 90.72972869873047 = 0.2917661964893341 + 10.0 * 9.04379653930664
Epoch 600, val loss: 0.3903825581073761
Epoch 610, training loss: 90.7561264038086 = 0.28969770669937134 + 10.0 * 9.046643257141113
Epoch 610, val loss: 0.39002108573913574
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 319, in _train_with_val
    loss.backward()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 6.31 GiB already allocated; 377.69 MiB free; 7.34 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.3 der2:0.2 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 104.69032287597656 = 1.099665641784668 + 10.0 * 10.359066009521484
Epoch 0, val loss: 1.0992127656936646
Epoch 10, training loss: 104.65074920654297 = 1.0883926153182983 + 10.0 * 10.35623550415039
Epoch 10, val loss: 1.087580680847168
Epoch 20, training loss: 104.15367126464844 = 1.0734509229660034 + 10.0 * 10.308022499084473
Epoch 20, val loss: 1.072417140007019
Epoch 30, training loss: 99.53282928466797 = 1.0582377910614014 + 10.0 * 9.84745979309082
Epoch 30, val loss: 1.0569721460342407
Epoch 40, training loss: 96.3840103149414 = 1.0425670146942139 + 10.0 * 9.534144401550293
Epoch 40, val loss: 1.0414561033248901
Epoch 50, training loss: 95.13829803466797 = 1.0286771059036255 + 10.0 * 9.410962104797363
Epoch 50, val loss: 1.028009295463562
Epoch 60, training loss: 94.8339614868164 = 1.0157897472381592 + 10.0 * 9.381816864013672
Epoch 60, val loss: 1.0151121616363525
Epoch 70, training loss: 94.54547882080078 = 1.0045390129089355 + 10.0 * 9.354093551635742
Epoch 70, val loss: 1.0041311979293823
Epoch 80, training loss: 94.15066528320312 = 0.9966796040534973 + 10.0 * 9.315398216247559
Epoch 80, val loss: 0.9967879056930542
Epoch 90, training loss: 93.57806396484375 = 0.9891728162765503 + 10.0 * 9.258889198303223
Epoch 90, val loss: 0.989368200302124
Epoch 100, training loss: 93.04757690429688 = 0.9805476665496826 + 10.0 * 9.206703186035156
Epoch 100, val loss: 0.9809349179267883
Epoch 110, training loss: 92.8262939453125 = 0.9696972370147705 + 10.0 * 9.185659408569336
Epoch 110, val loss: 0.970186710357666
Epoch 120, training loss: 92.66057586669922 = 0.9558647871017456 + 10.0 * 9.17047119140625
Epoch 120, val loss: 0.9566957950592041
Epoch 130, training loss: 92.52610778808594 = 0.9400481581687927 + 10.0 * 9.158605575561523
Epoch 130, val loss: 0.9414635896682739
Epoch 140, training loss: 92.41141510009766 = 0.9224952459335327 + 10.0 * 9.148892402648926
Epoch 140, val loss: 0.9245777130126953
Epoch 150, training loss: 92.30213165283203 = 0.9031174182891846 + 10.0 * 9.139902114868164
Epoch 150, val loss: 0.9057440161705017
Epoch 160, training loss: 92.22174072265625 = 0.8811787366867065 + 10.0 * 9.134056091308594
Epoch 160, val loss: 0.8844201564788818
Epoch 170, training loss: 92.14273834228516 = 0.8564562797546387 + 10.0 * 9.128628730773926
Epoch 170, val loss: 0.8607065677642822
Epoch 180, training loss: 92.07621002197266 = 0.8291518092155457 + 10.0 * 9.12470531463623
Epoch 180, val loss: 0.8345862627029419
Epoch 190, training loss: 92.00118255615234 = 0.7993361353874207 + 10.0 * 9.120183944702148
Epoch 190, val loss: 0.8061138391494751
Epoch 200, training loss: 91.92749786376953 = 0.7669243216514587 + 10.0 * 9.116057395935059
Epoch 200, val loss: 0.7752272486686707
Epoch 210, training loss: 91.85295104980469 = 0.7318069934844971 + 10.0 * 9.112114906311035
Epoch 210, val loss: 0.7419341802597046
Epoch 220, training loss: 91.80352020263672 = 0.6944708824157715 + 10.0 * 9.110904693603516
Epoch 220, val loss: 0.7067659497261047
Epoch 230, training loss: 91.71781921386719 = 0.6565819978713989 + 10.0 * 9.106122970581055
Epoch 230, val loss: 0.6714411973953247
Epoch 240, training loss: 91.64456939697266 = 0.6193755865097046 + 10.0 * 9.102519035339355
Epoch 240, val loss: 0.6370236873626709
Epoch 250, training loss: 91.5920639038086 = 0.5832889080047607 + 10.0 * 9.10087776184082
Epoch 250, val loss: 0.6039555072784424
Epoch 260, training loss: 91.52632904052734 = 0.5494641661643982 + 10.0 * 9.097686767578125
Epoch 260, val loss: 0.5735369324684143
Epoch 270, training loss: 91.46511840820312 = 0.5188734531402588 + 10.0 * 9.094624519348145
Epoch 270, val loss: 0.5463045835494995
Epoch 280, training loss: 91.4109115600586 = 0.49140802025794983 + 10.0 * 9.091950416564941
Epoch 280, val loss: 0.5222406983375549
Epoch 290, training loss: 91.38179016113281 = 0.4670739471912384 + 10.0 * 9.091471672058105
Epoch 290, val loss: 0.5013459920883179
Epoch 300, training loss: 91.3363265991211 = 0.44626277685165405 + 10.0 * 9.089006423950195
Epoch 300, val loss: 0.48407435417175293
Epoch 310, training loss: 91.30680847167969 = 0.4286569654941559 + 10.0 * 9.087815284729004
Epoch 310, val loss: 0.4697900116443634
Epoch 320, training loss: 91.25575256347656 = 0.4136846661567688 + 10.0 * 9.084207534790039
Epoch 320, val loss: 0.4579641819000244
Epoch 330, training loss: 91.22147369384766 = 0.40073922276496887 + 10.0 * 9.082073211669922
Epoch 330, val loss: 0.448102742433548
Epoch 340, training loss: 91.20016479492188 = 0.3894679546356201 + 10.0 * 9.081069946289062
Epoch 340, val loss: 0.4397783875465393
Epoch 350, training loss: 91.16582489013672 = 0.37983694672584534 + 10.0 * 9.078598976135254
Epoch 350, val loss: 0.4330870509147644
Epoch 360, training loss: 91.13922119140625 = 0.3714136779308319 + 10.0 * 9.076780319213867
Epoch 360, val loss: 0.4274219870567322
Epoch 370, training loss: 91.14705657958984 = 0.3639141023159027 + 10.0 * 9.078313827514648
Epoch 370, val loss: 0.4225655198097229
Epoch 380, training loss: 91.10749816894531 = 0.3572297990322113 + 10.0 * 9.075026512145996
Epoch 380, val loss: 0.4185396432876587
Epoch 390, training loss: 91.08202362060547 = 0.35131359100341797 + 10.0 * 9.073071479797363
Epoch 390, val loss: 0.41518348455429077
Epoch 400, training loss: 91.06259155273438 = 0.3459318280220032 + 10.0 * 9.07166576385498
Epoch 400, val loss: 0.4122788608074188
Epoch 410, training loss: 91.05165100097656 = 0.34094274044036865 + 10.0 * 9.071070671081543
Epoch 410, val loss: 0.4097639322280884
Epoch 420, training loss: 91.0482177734375 = 0.3363301157951355 + 10.0 * 9.071188926696777
Epoch 420, val loss: 0.407465398311615
Epoch 430, training loss: 91.01917266845703 = 0.3321300745010376 + 10.0 * 9.068704605102539
Epoch 430, val loss: 0.4055960476398468
Epoch 440, training loss: 91.0053939819336 = 0.32820335030555725 + 10.0 * 9.067719459533691
Epoch 440, val loss: 0.4039624333381653
Epoch 450, training loss: 90.99818420410156 = 0.3244655430316925 + 10.0 * 9.06737232208252
Epoch 450, val loss: 0.40247291326522827
Epoch 460, training loss: 90.98704528808594 = 0.3209143877029419 + 10.0 * 9.06661319732666
Epoch 460, val loss: 0.40112707018852234
Epoch 470, training loss: 90.97073364257812 = 0.31757602095603943 + 10.0 * 9.065316200256348
Epoch 470, val loss: 0.40001100301742554
Epoch 480, training loss: 90.96337127685547 = 0.3143722414970398 + 10.0 * 9.064900398254395
Epoch 480, val loss: 0.39902085065841675
Epoch 490, training loss: 90.952392578125 = 0.3112952709197998 + 10.0 * 9.064109802246094
Epoch 490, val loss: 0.3981029987335205
Epoch 500, training loss: 90.94591522216797 = 0.30837252736091614 + 10.0 * 9.063754081726074
Epoch 500, val loss: 0.39730408787727356
Epoch 510, training loss: 90.93460083007812 = 0.305553674697876 + 10.0 * 9.062904357910156
Epoch 510, val loss: 0.39660608768463135
Epoch 520, training loss: 90.92723846435547 = 0.3028317987918854 + 10.0 * 9.062440872192383
Epoch 520, val loss: 0.39595478773117065
Epoch 530, training loss: 90.91738891601562 = 0.30022838711738586 + 10.0 * 9.061716079711914
Epoch 530, val loss: 0.395438015460968
Epoch 540, training loss: 90.90094757080078 = 0.297688752412796 + 10.0 * 9.060325622558594
Epoch 540, val loss: 0.39499661326408386
Epoch 550, training loss: 90.91461944580078 = 0.2952030301094055 + 10.0 * 9.061941146850586
Epoch 550, val loss: 0.3946048319339752
Epoch 560, training loss: 90.89239501953125 = 0.29280906915664673 + 10.0 * 9.059958457946777
Epoch 560, val loss: 0.39424002170562744
Epoch 570, training loss: 90.88252258300781 = 0.29046347737312317 + 10.0 * 9.059206008911133
Epoch 570, val loss: 0.3938952386379242
Epoch 580, training loss: 90.89945983886719 = 0.288185179233551 + 10.0 * 9.061127662658691
Epoch 580, val loss: 0.3936900198459625
Epoch 590, training loss: 90.86947631835938 = 0.2859964668750763 + 10.0 * 9.058347702026367
Epoch 590, val loss: 0.3935229182243347
Epoch 600, training loss: 90.85214233398438 = 0.2838651239871979 + 10.0 * 9.056827545166016
Epoch 600, val loss: 0.3934084177017212
Epoch 610, training loss: 90.841552734375 = 0.2817554175853729 + 10.0 * 9.05597972869873
Epoch 610, val loss: 0.39330118894577026
Epoch 620, training loss: 90.83772277832031 = 0.27966803312301636 + 10.0 * 9.055805206298828
Epoch 620, val loss: 0.39325112104415894
Epoch 630, training loss: 90.829345703125 = 0.2776232957839966 + 10.0 * 9.055171966552734
Epoch 630, val loss: 0.39328280091285706
Epoch 640, training loss: 90.82664489746094 = 0.27564042806625366 + 10.0 * 9.055100440979004
Epoch 640, val loss: 0.39328253269195557
Epoch 650, training loss: 90.81273651123047 = 0.27368366718292236 + 10.0 * 9.053905487060547
Epoch 650, val loss: 0.3934408724308014
Epoch 660, training loss: 90.81934356689453 = 0.2717444896697998 + 10.0 * 9.054759979248047
Epoch 660, val loss: 0.3935600817203522
Epoch 670, training loss: 90.82451629638672 = 0.269854336977005 + 10.0 * 9.055466651916504
Epoch 670, val loss: 0.3937433063983917
Epoch 680, training loss: 90.79085540771484 = 0.268005907535553 + 10.0 * 9.052285194396973
Epoch 680, val loss: 0.39395058155059814
Epoch 690, training loss: 90.78563690185547 = 0.26618269085884094 + 10.0 * 9.051945686340332
Epoch 690, val loss: 0.39426401257514954
Epoch 700, training loss: 90.82258605957031 = 0.264386385679245 + 10.0 * 9.055819511413574
Epoch 700, val loss: 0.39462295174598694
Epoch 710, training loss: 90.77375793457031 = 0.26262086629867554 + 10.0 * 9.051114082336426
Epoch 710, val loss: 0.39488643407821655
Epoch 720, training loss: 90.7688217163086 = 0.2608872950077057 + 10.0 * 9.050793647766113
Epoch 720, val loss: 0.39525964856147766
Epoch 730, training loss: 90.75955963134766 = 0.25916075706481934 + 10.0 * 9.050039291381836
Epoch 730, val loss: 0.39570167660713196
Epoch 740, training loss: 90.81304168701172 = 0.2574545741081238 + 10.0 * 9.055559158325195
Epoch 740, val loss: 0.39613181352615356
Epoch 750, training loss: 90.75051879882812 = 0.25578773021698 + 10.0 * 9.04947280883789
Epoch 750, val loss: 0.39669615030288696
Epoch 760, training loss: 90.74822235107422 = 0.2541486918926239 + 10.0 * 9.049407005310059
Epoch 760, val loss: 0.39718589186668396
Epoch 770, training loss: 90.74617767333984 = 0.25251954793930054 + 10.0 * 9.049365997314453
Epoch 770, val loss: 0.3977518081665039
Epoch 780, training loss: 90.73576354980469 = 0.25091883540153503 + 10.0 * 9.048484802246094
Epoch 780, val loss: 0.3983117640018463
Epoch 790, training loss: 90.72679138183594 = 0.24934309720993042 + 10.0 * 9.047744750976562
Epoch 790, val loss: 0.3989276587963104
Epoch 800, training loss: 90.72575378417969 = 0.24778003990650177 + 10.0 * 9.047797203063965
Epoch 800, val loss: 0.3995937407016754
Epoch 810, training loss: 90.72630310058594 = 0.24623127281665802 + 10.0 * 9.048007011413574
Epoch 810, val loss: 0.40028348565101624
Epoch 820, training loss: 90.71825408935547 = 0.24469970166683197 + 10.0 * 9.047355651855469
Epoch 820, val loss: 0.40109002590179443
Epoch 830, training loss: 90.7412338256836 = 0.2431851029396057 + 10.0 * 9.0498046875
Epoch 830, val loss: 0.4017930030822754
Epoch 840, training loss: 90.7083969116211 = 0.24170631170272827 + 10.0 * 9.046669006347656
Epoch 840, val loss: 0.40261927247047424
Epoch 850, training loss: 90.70319366455078 = 0.24024057388305664 + 10.0 * 9.046295166015625
Epoch 850, val loss: 0.4034551680088043
Epoch 860, training loss: 90.6917953491211 = 0.2387789636850357 + 10.0 * 9.04530143737793
Epoch 860, val loss: 0.4042987525463104
Epoch 870, training loss: 90.68905639648438 = 0.23732280731201172 + 10.0 * 9.045173645019531
Epoch 870, val loss: 0.40514564514160156
Epoch 880, training loss: 90.72682189941406 = 0.23588009178638458 + 10.0 * 9.049094200134277
Epoch 880, val loss: 0.4060961902141571
Epoch 890, training loss: 90.70703887939453 = 0.23446737229824066 + 10.0 * 9.047257423400879
Epoch 890, val loss: 0.40705427527427673
Epoch 900, training loss: 90.68607330322266 = 0.23306827247142792 + 10.0 * 9.045300483703613
Epoch 900, val loss: 0.4079362154006958
Epoch 910, training loss: 90.67166137695312 = 0.23168598115444183 + 10.0 * 9.043996810913086
Epoch 910, val loss: 0.4089168906211853
Epoch 920, training loss: 90.66637420654297 = 0.23030561208724976 + 10.0 * 9.043606758117676
Epoch 920, val loss: 0.4099092483520508
Epoch 930, training loss: 90.67601776123047 = 0.2289314717054367 + 10.0 * 9.044708251953125
Epoch 930, val loss: 0.4109337031841278
Epoch 940, training loss: 90.6653823852539 = 0.2275797426700592 + 10.0 * 9.043780326843262
Epoch 940, val loss: 0.4121151566505432
Epoch 950, training loss: 90.65788269042969 = 0.22624501585960388 + 10.0 * 9.043164253234863
Epoch 950, val loss: 0.4131138026714325
Epoch 960, training loss: 90.65082550048828 = 0.22491376101970673 + 10.0 * 9.042591094970703
Epoch 960, val loss: 0.4142818748950958
Epoch 970, training loss: 90.64492797851562 = 0.22358812391757965 + 10.0 * 9.042134284973145
Epoch 970, val loss: 0.4154351055622101
Epoch 980, training loss: 90.66569519042969 = 0.22226621210575104 + 10.0 * 9.044342994689941
Epoch 980, val loss: 0.41669872403144836
Epoch 990, training loss: 90.68893432617188 = 0.22097831964492798 + 10.0 * 9.046795845031738
Epoch 990, val loss: 0.41783252358436584
Epoch 1000, training loss: 90.64884948730469 = 0.21971078217029572 + 10.0 * 9.042913436889648
Epoch 1000, val loss: 0.41894495487213135
Epoch 1010, training loss: 90.63469696044922 = 0.21844163537025452 + 10.0 * 9.041625022888184
Epoch 1010, val loss: 0.4201942980289459
Epoch 1020, training loss: 90.62738037109375 = 0.21718138456344604 + 10.0 * 9.041020393371582
Epoch 1020, val loss: 0.4214346408843994
Epoch 1030, training loss: 90.62645721435547 = 0.21592430770397186 + 10.0 * 9.04105281829834
Epoch 1030, val loss: 0.4227985739707947
Epoch 1040, training loss: 90.64151000976562 = 0.21467722952365875 + 10.0 * 9.042683601379395
Epoch 1040, val loss: 0.42411714792251587
Epoch 1050, training loss: 90.6158218383789 = 0.21344324946403503 + 10.0 * 9.040238380432129
Epoch 1050, val loss: 0.42538803815841675
Epoch 1060, training loss: 90.62129974365234 = 0.21221502125263214 + 10.0 * 9.040908813476562
Epoch 1060, val loss: 0.4266957938671112
Epoch 1070, training loss: 90.61595916748047 = 0.21099554002285004 + 10.0 * 9.040495872497559
Epoch 1070, val loss: 0.4280834496021271
Epoch 1080, training loss: 90.60771942138672 = 0.2097880095243454 + 10.0 * 9.039793014526367
Epoch 1080, val loss: 0.4295176565647125
Epoch 1090, training loss: 90.60041809082031 = 0.2085743248462677 + 10.0 * 9.0391845703125
Epoch 1090, val loss: 0.430899441242218
Epoch 1100, training loss: 90.62133026123047 = 0.20736877620220184 + 10.0 * 9.041396141052246
Epoch 1100, val loss: 0.4323280155658722
Epoch 1110, training loss: 90.62332916259766 = 0.20618084073066711 + 10.0 * 9.041714668273926
Epoch 1110, val loss: 0.4338722229003906
Epoch 1120, training loss: 90.59090423583984 = 0.2050083428621292 + 10.0 * 9.038589477539062
Epoch 1120, val loss: 0.4352589547634125
Epoch 1130, training loss: 90.58908081054688 = 0.2038339227437973 + 10.0 * 9.038524627685547
Epoch 1130, val loss: 0.436714231967926
Epoch 1140, training loss: 90.58491516113281 = 0.20266397297382355 + 10.0 * 9.038225173950195
Epoch 1140, val loss: 0.4382219910621643
Epoch 1150, training loss: 90.60646057128906 = 0.20149987936019897 + 10.0 * 9.040495872497559
Epoch 1150, val loss: 0.4397601783275604
Epoch 1160, training loss: 90.5923843383789 = 0.20033854246139526 + 10.0 * 9.039204597473145
Epoch 1160, val loss: 0.44131556153297424
Epoch 1170, training loss: 90.57635498046875 = 0.19919098913669586 + 10.0 * 9.03771686553955
Epoch 1170, val loss: 0.44286012649536133
Epoch 1180, training loss: 90.57086181640625 = 0.1980421543121338 + 10.0 * 9.03728199005127
Epoch 1180, val loss: 0.4443992078304291
Epoch 1190, training loss: 90.57366180419922 = 0.19689548015594482 + 10.0 * 9.037676811218262
Epoch 1190, val loss: 0.4460211992263794
Epoch 1200, training loss: 90.5789794921875 = 0.19575734436511993 + 10.0 * 9.038322448730469
Epoch 1200, val loss: 0.4476776421070099
Epoch 1210, training loss: 90.56243896484375 = 0.19462914764881134 + 10.0 * 9.036781311035156
Epoch 1210, val loss: 0.4492250382900238
Epoch 1220, training loss: 90.56089782714844 = 0.19349971413612366 + 10.0 * 9.03674030303955
Epoch 1220, val loss: 0.4508582651615143
Epoch 1230, training loss: 90.55565643310547 = 0.19237762689590454 + 10.0 * 9.036328315734863
Epoch 1230, val loss: 0.45251667499542236
Epoch 1240, training loss: 90.58118438720703 = 0.19125956296920776 + 10.0 * 9.038991928100586
Epoch 1240, val loss: 0.4542269706726074
Epoch 1250, training loss: 90.56364440917969 = 0.19014716148376465 + 10.0 * 9.037349700927734
Epoch 1250, val loss: 0.4559973180294037
Epoch 1260, training loss: 90.55479431152344 = 0.18904373049736023 + 10.0 * 9.036575317382812
Epoch 1260, val loss: 0.45766445994377136
Epoch 1270, training loss: 90.5426254272461 = 0.18794021010398865 + 10.0 * 9.035468101501465
Epoch 1270, val loss: 0.45929503440856934
Epoch 1280, training loss: 90.54176330566406 = 0.1868387907743454 + 10.0 * 9.035492897033691
Epoch 1280, val loss: 0.4610273241996765
Epoch 1290, training loss: 90.5578384399414 = 0.1857426017522812 + 10.0 * 9.037209510803223
Epoch 1290, val loss: 0.46267735958099365
Epoch 1300, training loss: 90.54622650146484 = 0.18465211987495422 + 10.0 * 9.036157608032227
Epoch 1300, val loss: 0.46467265486717224
Epoch 1310, training loss: 90.54462432861328 = 0.18356376886367798 + 10.0 * 9.03610610961914
Epoch 1310, val loss: 0.4663292467594147
Epoch 1320, training loss: 90.53421020507812 = 0.18248704075813293 + 10.0 * 9.035172462463379
Epoch 1320, val loss: 0.46825212240219116
Epoch 1330, training loss: 90.52935028076172 = 0.18140554428100586 + 10.0 * 9.034794807434082
Epoch 1330, val loss: 0.470031201839447
Epoch 1340, training loss: 90.53955078125 = 0.18033097684383392 + 10.0 * 9.035922050476074
Epoch 1340, val loss: 0.47189581394195557
Epoch 1350, training loss: 90.52342224121094 = 0.1792592853307724 + 10.0 * 9.034416198730469
Epoch 1350, val loss: 0.47371169924736023
Epoch 1360, training loss: 90.52106475830078 = 0.1781885325908661 + 10.0 * 9.034287452697754
Epoch 1360, val loss: 0.47564414143562317
Epoch 1370, training loss: 90.52330017089844 = 0.17711828649044037 + 10.0 * 9.034618377685547
Epoch 1370, val loss: 0.4775259494781494
Epoch 1380, training loss: 90.52233123779297 = 0.17605093121528625 + 10.0 * 9.034627914428711
Epoch 1380, val loss: 0.47940194606781006
Epoch 1390, training loss: 90.51698303222656 = 0.17498663067817688 + 10.0 * 9.034199714660645
Epoch 1390, val loss: 0.48145437240600586
Epoch 1400, training loss: 90.50727844238281 = 0.17392337322235107 + 10.0 * 9.03333568572998
Epoch 1400, val loss: 0.48334822058677673
Epoch 1410, training loss: 90.50518035888672 = 0.1728595793247223 + 10.0 * 9.033231735229492
Epoch 1410, val loss: 0.4853247404098511
Epoch 1420, training loss: 90.55509948730469 = 0.1718066930770874 + 10.0 * 9.038329124450684
Epoch 1420, val loss: 0.4874221384525299
Epoch 1430, training loss: 90.51705932617188 = 0.17074453830718994 + 10.0 * 9.034631729125977
Epoch 1430, val loss: 0.4892538785934448
Epoch 1440, training loss: 90.49481201171875 = 0.16969576478004456 + 10.0 * 9.032511711120605
Epoch 1440, val loss: 0.4913419187068939
Epoch 1450, training loss: 90.49392700195312 = 0.1686464250087738 + 10.0 * 9.032527923583984
Epoch 1450, val loss: 0.4934064745903015
Epoch 1460, training loss: 90.48902893066406 = 0.1675918847322464 + 10.0 * 9.032143592834473
Epoch 1460, val loss: 0.4954707622528076
Epoch 1470, training loss: 90.50232696533203 = 0.16654139757156372 + 10.0 * 9.033578872680664
Epoch 1470, val loss: 0.49758124351501465
Epoch 1480, training loss: 90.49166107177734 = 0.16548873484134674 + 10.0 * 9.032617568969727
Epoch 1480, val loss: 0.4998197853565216
Epoch 1490, training loss: 90.49269104003906 = 0.16444452106952667 + 10.0 * 9.032824516296387
Epoch 1490, val loss: 0.5018282532691956
Epoch 1500, training loss: 90.48516845703125 = 0.1633957028388977 + 10.0 * 9.032176971435547
Epoch 1500, val loss: 0.5039600729942322
Epoch 1510, training loss: 90.47740173339844 = 0.16235706210136414 + 10.0 * 9.03150463104248
Epoch 1510, val loss: 0.506233811378479
Epoch 1520, training loss: 90.49553680419922 = 0.1613200604915619 + 10.0 * 9.033421516418457
Epoch 1520, val loss: 0.508521556854248
Epoch 1530, training loss: 90.48165893554688 = 0.16027666628360748 + 10.0 * 9.032137870788574
Epoch 1530, val loss: 0.5104724168777466
Epoch 1540, training loss: 90.47601318359375 = 0.1592467874288559 + 10.0 * 9.031676292419434
Epoch 1540, val loss: 0.5129431486129761
Epoch 1550, training loss: 90.46781921386719 = 0.15820814669132233 + 10.0 * 9.030961036682129
Epoch 1550, val loss: 0.5150419473648071
Epoch 1560, training loss: 90.4626235961914 = 0.15717348456382751 + 10.0 * 9.030545234680176
Epoch 1560, val loss: 0.5174196362495422
Epoch 1570, training loss: 90.46544647216797 = 0.1561369150876999 + 10.0 * 9.03093147277832
Epoch 1570, val loss: 0.5197967290878296
Epoch 1580, training loss: 90.47962951660156 = 0.15510262548923492 + 10.0 * 9.032452583312988
Epoch 1580, val loss: 0.5220876336097717
Epoch 1590, training loss: 90.45696258544922 = 0.15406830608844757 + 10.0 * 9.030289649963379
Epoch 1590, val loss: 0.5244697332382202
Epoch 1600, training loss: 90.46113586425781 = 0.15303383767604828 + 10.0 * 9.030810356140137
Epoch 1600, val loss: 0.526788592338562
Epoch 1610, training loss: 90.46729278564453 = 0.15200106799602509 + 10.0 * 9.031529426574707
Epoch 1610, val loss: 0.5292710065841675
Epoch 1620, training loss: 90.4652328491211 = 0.1509721428155899 + 10.0 * 9.031426429748535
Epoch 1620, val loss: 0.5317284464836121
Epoch 1630, training loss: 90.44999694824219 = 0.14993524551391602 + 10.0 * 9.030006408691406
Epoch 1630, val loss: 0.534135639667511
Epoch 1640, training loss: 90.45514678955078 = 0.14890648424625397 + 10.0 * 9.030624389648438
Epoch 1640, val loss: 0.5366639494895935
Epoch 1650, training loss: 90.446044921875 = 0.14787526428699493 + 10.0 * 9.029817581176758
Epoch 1650, val loss: 0.5392124056816101
Epoch 1660, training loss: 90.44338989257812 = 0.14684951305389404 + 10.0 * 9.029653549194336
Epoch 1660, val loss: 0.5417758822441101
Epoch 1670, training loss: 90.44182586669922 = 0.14582091569900513 + 10.0 * 9.029600143432617
Epoch 1670, val loss: 0.5443212985992432
Epoch 1680, training loss: 90.44519805908203 = 0.14479193091392517 + 10.0 * 9.030040740966797
Epoch 1680, val loss: 0.5469388365745544
Epoch 1690, training loss: 90.4362564086914 = 0.14375454187393188 + 10.0 * 9.029250144958496
Epoch 1690, val loss: 0.54969722032547
Epoch 1700, training loss: 90.43749237060547 = 0.14271990954875946 + 10.0 * 9.0294771194458
Epoch 1700, val loss: 0.5523340106010437
Epoch 1710, training loss: 90.4471664428711 = 0.14168986678123474 + 10.0 * 9.030547142028809
Epoch 1710, val loss: 0.5551254749298096
Epoch 1720, training loss: 90.4357681274414 = 0.1406659036874771 + 10.0 * 9.029510498046875
Epoch 1720, val loss: 0.5579208135604858
Epoch 1730, training loss: 90.4261245727539 = 0.13962839543819427 + 10.0 * 9.028650283813477
Epoch 1730, val loss: 0.5606120228767395
Epoch 1740, training loss: 90.42194366455078 = 0.13859675824642181 + 10.0 * 9.028334617614746
Epoch 1740, val loss: 0.5634545087814331
Epoch 1750, training loss: 90.43551635742188 = 0.13756076991558075 + 10.0 * 9.02979564666748
Epoch 1750, val loss: 0.5663114786148071
Epoch 1760, training loss: 90.41697692871094 = 0.13652339577674866 + 10.0 * 9.028045654296875
Epoch 1760, val loss: 0.5690802335739136
Epoch 1770, training loss: 90.4179916381836 = 0.13548439741134644 + 10.0 * 9.028250694274902
Epoch 1770, val loss: 0.5720142722129822
Epoch 1780, training loss: 90.4190902709961 = 0.1344473958015442 + 10.0 * 9.028464317321777
Epoch 1780, val loss: 0.5748985409736633
Epoch 1790, training loss: 90.4107666015625 = 0.13340851664543152 + 10.0 * 9.027735710144043
Epoch 1790, val loss: 0.5780074000358582
Epoch 1800, training loss: 90.44055938720703 = 0.13237714767456055 + 10.0 * 9.030817985534668
Epoch 1800, val loss: 0.5810825824737549
Epoch 1810, training loss: 90.41658782958984 = 0.13133864104747772 + 10.0 * 9.028524398803711
Epoch 1810, val loss: 0.5837563872337341
Epoch 1820, training loss: 90.4030532836914 = 0.13029637932777405 + 10.0 * 9.027276039123535
Epoch 1820, val loss: 0.5869138240814209
Epoch 1830, training loss: 90.40059661865234 = 0.1292552500963211 + 10.0 * 9.02713394165039
Epoch 1830, val loss: 0.5900018811225891
Epoch 1840, training loss: 90.42411804199219 = 0.1282147765159607 + 10.0 * 9.029590606689453
Epoch 1840, val loss: 0.5930324196815491
Epoch 1850, training loss: 90.406494140625 = 0.12717638909816742 + 10.0 * 9.027932167053223
Epoch 1850, val loss: 0.5961171388626099
Epoch 1860, training loss: 90.39720153808594 = 0.12613140046596527 + 10.0 * 9.027107238769531
Epoch 1860, val loss: 0.5992898344993591
Epoch 1870, training loss: 90.392578125 = 0.1250932514667511 + 10.0 * 9.026748657226562
Epoch 1870, val loss: 0.6024845242500305
Epoch 1880, training loss: 90.39612579345703 = 0.1240495890378952 + 10.0 * 9.02720832824707
Epoch 1880, val loss: 0.6056164503097534
Epoch 1890, training loss: 90.40441131591797 = 0.12300965189933777 + 10.0 * 9.0281400680542
Epoch 1890, val loss: 0.6088209748268127
Epoch 1900, training loss: 90.38907623291016 = 0.12196426093578339 + 10.0 * 9.026711463928223
Epoch 1900, val loss: 0.6121630668640137
Epoch 1910, training loss: 90.3841552734375 = 0.12092410773038864 + 10.0 * 9.026323318481445
Epoch 1910, val loss: 0.6154242157936096
Epoch 1920, training loss: 90.3888931274414 = 0.11988310515880585 + 10.0 * 9.026901245117188
Epoch 1920, val loss: 0.6186025142669678
Epoch 1930, training loss: 90.38496398925781 = 0.1188417300581932 + 10.0 * 9.026612281799316
Epoch 1930, val loss: 0.6219924688339233
Epoch 1940, training loss: 90.37743377685547 = 0.1178031787276268 + 10.0 * 9.025962829589844
Epoch 1940, val loss: 0.6256162524223328
Epoch 1950, training loss: 90.37773132324219 = 0.116757832467556 + 10.0 * 9.026097297668457
Epoch 1950, val loss: 0.628821074962616
Epoch 1960, training loss: 90.39747619628906 = 0.11571730673313141 + 10.0 * 9.028176307678223
Epoch 1960, val loss: 0.6321672797203064
Epoch 1970, training loss: 90.37442779541016 = 0.11467879265546799 + 10.0 * 9.025975227355957
Epoch 1970, val loss: 0.6358953714370728
Epoch 1980, training loss: 90.36830139160156 = 0.11363458633422852 + 10.0 * 9.025466918945312
Epoch 1980, val loss: 0.6393828392028809
Epoch 1990, training loss: 90.3853530883789 = 0.11259671300649643 + 10.0 * 9.027276039123535
Epoch 1990, val loss: 0.642861008644104
Epoch 2000, training loss: 90.36335754394531 = 0.11155537515878677 + 10.0 * 9.025179862976074
Epoch 2000, val loss: 0.6464948058128357
Epoch 2010, training loss: 90.3588638305664 = 0.1105111762881279 + 10.0 * 9.024835586547852
Epoch 2010, val loss: 0.649942934513092
Epoch 2020, training loss: 90.36345672607422 = 0.10947203636169434 + 10.0 * 9.025398254394531
Epoch 2020, val loss: 0.6535578966140747
Epoch 2030, training loss: 90.36775207519531 = 0.10843423753976822 + 10.0 * 9.025931358337402
Epoch 2030, val loss: 0.6572623252868652
Epoch 2040, training loss: 90.35643005371094 = 0.10739452391862869 + 10.0 * 9.024904251098633
Epoch 2040, val loss: 0.6611535549163818
Epoch 2050, training loss: 90.36589813232422 = 0.10636092722415924 + 10.0 * 9.02595329284668
Epoch 2050, val loss: 0.6649032235145569
Epoch 2060, training loss: 90.35614013671875 = 0.10531698912382126 + 10.0 * 9.0250825881958
Epoch 2060, val loss: 0.668414294719696
Epoch 2070, training loss: 90.35551452636719 = 0.10428603738546371 + 10.0 * 9.02512264251709
Epoch 2070, val loss: 0.6723771095275879
Epoch 2080, training loss: 90.34562683105469 = 0.10325369238853455 + 10.0 * 9.024236679077148
Epoch 2080, val loss: 0.6760743260383606
Epoch 2090, training loss: 90.3417739868164 = 0.1022210568189621 + 10.0 * 9.023955345153809
Epoch 2090, val loss: 0.6799361109733582
Epoch 2100, training loss: 90.34121704101562 = 0.10118819028139114 + 10.0 * 9.024003028869629
Epoch 2100, val loss: 0.6837679743766785
Epoch 2110, training loss: 90.36603546142578 = 0.10016123950481415 + 10.0 * 9.02658748626709
Epoch 2110, val loss: 0.6874361634254456
Epoch 2120, training loss: 90.35242462158203 = 0.09913273900747299 + 10.0 * 9.025328636169434
Epoch 2120, val loss: 0.6919689774513245
Epoch 2130, training loss: 90.33946990966797 = 0.09809207916259766 + 10.0 * 9.024137496948242
Epoch 2130, val loss: 0.6956354975700378
Epoch 2140, training loss: 90.33216094970703 = 0.09705799072980881 + 10.0 * 9.023509979248047
Epoch 2140, val loss: 0.699924111366272
Epoch 2150, training loss: 90.33940124511719 = 0.09602723270654678 + 10.0 * 9.024337768554688
Epoch 2150, val loss: 0.7038785219192505
Epoch 2160, training loss: 90.33294677734375 = 0.09499344974756241 + 10.0 * 9.023795127868652
Epoch 2160, val loss: 0.7080411911010742
Epoch 2170, training loss: 90.32843780517578 = 0.09396757185459137 + 10.0 * 9.023447036743164
Epoch 2170, val loss: 0.7121941447257996
Epoch 2180, training loss: 90.35053253173828 = 0.09294887632131577 + 10.0 * 9.025758743286133
Epoch 2180, val loss: 0.7162688374519348
Epoch 2190, training loss: 90.33391571044922 = 0.09192134439945221 + 10.0 * 9.024199485778809
Epoch 2190, val loss: 0.720636248588562
Epoch 2200, training loss: 90.32107543945312 = 0.09089968353509903 + 10.0 * 9.023017883300781
Epoch 2200, val loss: 0.7247970700263977
Epoch 2210, training loss: 90.3175048828125 = 0.08987819403409958 + 10.0 * 9.022762298583984
Epoch 2210, val loss: 0.7292582392692566
Epoch 2220, training loss: 90.31777954101562 = 0.08886092901229858 + 10.0 * 9.022891998291016
Epoch 2220, val loss: 0.7336048483848572
Epoch 2230, training loss: 90.33031463623047 = 0.08784732222557068 + 10.0 * 9.024247169494629
Epoch 2230, val loss: 0.7380200624465942
Epoch 2240, training loss: 90.32596588134766 = 0.08683225512504578 + 10.0 * 9.023913383483887
Epoch 2240, val loss: 0.7425167560577393
Epoch 2250, training loss: 90.31431579589844 = 0.0858178362250328 + 10.0 * 9.022850036621094
Epoch 2250, val loss: 0.7467423677444458
Epoch 2260, training loss: 90.31375885009766 = 0.08481340855360031 + 10.0 * 9.022893905639648
Epoch 2260, val loss: 0.7513219118118286
Epoch 2270, training loss: 90.31056213378906 = 0.08380760252475739 + 10.0 * 9.022675514221191
Epoch 2270, val loss: 0.7559189796447754
Epoch 2280, training loss: 90.30723571777344 = 0.08280836045742035 + 10.0 * 9.022442817687988
Epoch 2280, val loss: 0.7605188488960266
Epoch 2290, training loss: 90.31515502929688 = 0.08181509375572205 + 10.0 * 9.023333549499512
Epoch 2290, val loss: 0.7651236653327942
Epoch 2300, training loss: 90.299560546875 = 0.08082069456577301 + 10.0 * 9.02187442779541
Epoch 2300, val loss: 0.7699528932571411
Epoch 2310, training loss: 90.30062866210938 = 0.07983766496181488 + 10.0 * 9.022079467773438
Epoch 2310, val loss: 0.7747684717178345
Epoch 2320, training loss: 90.3104248046875 = 0.07886894047260284 + 10.0 * 9.023155212402344
Epoch 2320, val loss: 0.7796462178230286
Epoch 2330, training loss: 90.3075180053711 = 0.0778813511133194 + 10.0 * 9.022963523864746
Epoch 2330, val loss: 0.7839557528495789
Epoch 2340, training loss: 90.2922592163086 = 0.0769117996096611 + 10.0 * 9.02153491973877
Epoch 2340, val loss: 0.7889365553855896
Epoch 2350, training loss: 90.30374908447266 = 0.07595067471265793 + 10.0 * 9.02277946472168
Epoch 2350, val loss: 0.7934439182281494
Epoch 2360, training loss: 90.29138946533203 = 0.07499019801616669 + 10.0 * 9.021639823913574
Epoch 2360, val loss: 0.7987367510795593
Epoch 2370, training loss: 90.2883529663086 = 0.07403513789176941 + 10.0 * 9.021431922912598
Epoch 2370, val loss: 0.8035930395126343
Epoch 2380, training loss: 90.2889404296875 = 0.07309448719024658 + 10.0 * 9.021584510803223
Epoch 2380, val loss: 0.8088007569313049
Epoch 2390, training loss: 90.28414154052734 = 0.07215041667222977 + 10.0 * 9.021199226379395
Epoch 2390, val loss: 0.8134236335754395
Epoch 2400, training loss: 90.29383087158203 = 0.07122152298688889 + 10.0 * 9.022260665893555
Epoch 2400, val loss: 0.8181231617927551
Epoch 2410, training loss: 90.28089904785156 = 0.07029104232788086 + 10.0 * 9.021060943603516
Epoch 2410, val loss: 0.8235512375831604
Epoch 2420, training loss: 90.27628326416016 = 0.06936879456043243 + 10.0 * 9.020691871643066
Epoch 2420, val loss: 0.8285103440284729
Epoch 2430, training loss: 90.27499389648438 = 0.06845904886722565 + 10.0 * 9.02065372467041
Epoch 2430, val loss: 0.8336575031280518
Epoch 2440, training loss: 90.28406524658203 = 0.06755925714969635 + 10.0 * 9.021650314331055
Epoch 2440, val loss: 0.8387181162834167
Epoch 2450, training loss: 90.28997802734375 = 0.06666852533817291 + 10.0 * 9.022331237792969
Epoch 2450, val loss: 0.844129741191864
Epoch 2460, training loss: 90.27011108398438 = 0.06578031927347183 + 10.0 * 9.02043342590332
Epoch 2460, val loss: 0.8490433096885681
Epoch 2470, training loss: 90.26526641845703 = 0.06490378081798553 + 10.0 * 9.020036697387695
Epoch 2470, val loss: 0.854361891746521
Epoch 2480, training loss: 90.26439666748047 = 0.0640353336930275 + 10.0 * 9.020036697387695
Epoch 2480, val loss: 0.8595821261405945
Epoch 2490, training loss: 90.28219604492188 = 0.06318335235118866 + 10.0 * 9.02190113067627
Epoch 2490, val loss: 0.8650380969047546
Epoch 2500, training loss: 90.26383972167969 = 0.06233863905072212 + 10.0 * 9.020150184631348
Epoch 2500, val loss: 0.870451033115387
Epoch 2510, training loss: 90.26000213623047 = 0.06148972362279892 + 10.0 * 9.019850730895996
Epoch 2510, val loss: 0.8755882978439331
Epoch 2520, training loss: 90.2599105834961 = 0.06065797433257103 + 10.0 * 9.019925117492676
Epoch 2520, val loss: 0.880893886089325
Epoch 2530, training loss: 90.27593231201172 = 0.059838224202394485 + 10.0 * 9.02160930633545
Epoch 2530, val loss: 0.8860709071159363
Epoch 2540, training loss: 90.26592254638672 = 0.05902760103344917 + 10.0 * 9.020689010620117
Epoch 2540, val loss: 0.8916873335838318
Epoch 2550, training loss: 90.25348663330078 = 0.0582197830080986 + 10.0 * 9.019526481628418
Epoch 2550, val loss: 0.8970955610275269
Epoch 2560, training loss: 90.24937438964844 = 0.05742701143026352 + 10.0 * 9.019194602966309
Epoch 2560, val loss: 0.9027568101882935
Epoch 2570, training loss: 90.25224304199219 = 0.056640032678842545 + 10.0 * 9.019559860229492
Epoch 2570, val loss: 0.9081630706787109
Epoch 2580, training loss: 90.25914764404297 = 0.05586345121264458 + 10.0 * 9.020328521728516
Epoch 2580, val loss: 0.9136290550231934
Epoch 2590, training loss: 90.24788665771484 = 0.05510013550519943 + 10.0 * 9.019278526306152
Epoch 2590, val loss: 0.9190815091133118
Epoch 2600, training loss: 90.24627685546875 = 0.05433940142393112 + 10.0 * 9.019193649291992
Epoch 2600, val loss: 0.9247113466262817
Epoch 2610, training loss: 90.2728271484375 = 0.0535995289683342 + 10.0 * 9.021923065185547
Epoch 2610, val loss: 0.9304093718528748
Epoch 2620, training loss: 90.260986328125 = 0.052862949669361115 + 10.0 * 9.020812034606934
Epoch 2620, val loss: 0.9357844591140747
Epoch 2630, training loss: 90.24549102783203 = 0.052129052579402924 + 10.0 * 9.019335746765137
Epoch 2630, val loss: 0.9414135813713074
Epoch 2640, training loss: 90.2352066040039 = 0.05141216143965721 + 10.0 * 9.018379211425781
Epoch 2640, val loss: 0.9470253586769104
Epoch 2650, training loss: 90.23480987548828 = 0.05070265382528305 + 10.0 * 9.018410682678223
Epoch 2650, val loss: 0.9526758193969727
Epoch 2660, training loss: 90.24694061279297 = 0.050018392503261566 + 10.0 * 9.019692420959473
Epoch 2660, val loss: 0.9589029550552368
Epoch 2670, training loss: 90.23173522949219 = 0.0493151880800724 + 10.0 * 9.018241882324219
Epoch 2670, val loss: 0.9638797640800476
Epoch 2680, training loss: 90.2280502319336 = 0.04863381385803223 + 10.0 * 9.01794147491455
Epoch 2680, val loss: 0.9695217609405518
Epoch 2690, training loss: 90.23084259033203 = 0.04796464741230011 + 10.0 * 9.018287658691406
Epoch 2690, val loss: 0.9750853776931763
Epoch 2700, training loss: 90.24748992919922 = 0.04731238633394241 + 10.0 * 9.020017623901367
Epoch 2700, val loss: 0.9808555245399475
Epoch 2710, training loss: 90.23235321044922 = 0.046656955033540726 + 10.0 * 9.018569946289062
Epoch 2710, val loss: 0.9868419170379639
Epoch 2720, training loss: 90.22651672363281 = 0.04601029306650162 + 10.0 * 9.018050193786621
Epoch 2720, val loss: 0.9924213886260986
Epoch 2730, training loss: 90.22405242919922 = 0.04537392780184746 + 10.0 * 9.017868041992188
Epoch 2730, val loss: 0.9979805946350098
Epoch 2740, training loss: 90.23192596435547 = 0.04474657028913498 + 10.0 * 9.018717765808105
Epoch 2740, val loss: 1.0034431219100952
Epoch 2750, training loss: 90.23390197753906 = 0.04413202777504921 + 10.0 * 9.018977165222168
Epoch 2750, val loss: 1.00933039188385
Epoch 2760, training loss: 90.22528076171875 = 0.04352985695004463 + 10.0 * 9.01817512512207
Epoch 2760, val loss: 1.0151506662368774
Epoch 2770, training loss: 90.21862030029297 = 0.042929768562316895 + 10.0 * 9.017568588256836
Epoch 2770, val loss: 1.0208467245101929
Epoch 2780, training loss: 90.2126693725586 = 0.042337775230407715 + 10.0 * 9.017033576965332
Epoch 2780, val loss: 1.0261701345443726
Epoch 2790, training loss: 90.21550750732422 = 0.04175925254821777 + 10.0 * 9.017374992370605
Epoch 2790, val loss: 1.0317838191986084
Epoch 2800, training loss: 90.23046875 = 0.04119502007961273 + 10.0 * 9.018926620483398
Epoch 2800, val loss: 1.037258267402649
Epoch 2810, training loss: 90.21852111816406 = 0.04063492640852928 + 10.0 * 9.017788887023926
Epoch 2810, val loss: 1.043520450592041
Epoch 2820, training loss: 90.21556091308594 = 0.040072426199913025 + 10.0 * 9.017549514770508
Epoch 2820, val loss: 1.048988938331604
Epoch 2830, training loss: 90.2154541015625 = 0.03952731564640999 + 10.0 * 9.017592430114746
Epoch 2830, val loss: 1.0547713041305542
Epoch 2840, training loss: 90.20977783203125 = 0.03898882865905762 + 10.0 * 9.01707935333252
Epoch 2840, val loss: 1.0604853630065918
Epoch 2850, training loss: 90.20957946777344 = 0.03846513852477074 + 10.0 * 9.017110824584961
Epoch 2850, val loss: 1.066137671470642
Epoch 2860, training loss: 90.21356201171875 = 0.03793647140264511 + 10.0 * 9.017562866210938
Epoch 2860, val loss: 1.0713733434677124
Epoch 2870, training loss: 90.21725463867188 = 0.03742416575551033 + 10.0 * 9.017983436584473
Epoch 2870, val loss: 1.0770851373672485
Epoch 2880, training loss: 90.20023345947266 = 0.03691598027944565 + 10.0 * 9.016331672668457
Epoch 2880, val loss: 1.0830950736999512
Epoch 2890, training loss: 90.1958999633789 = 0.03641843795776367 + 10.0 * 9.015948295593262
Epoch 2890, val loss: 1.088577151298523
Epoch 2900, training loss: 90.20063781738281 = 0.035929907113313675 + 10.0 * 9.016470909118652
Epoch 2900, val loss: 1.093813180923462
Epoch 2910, training loss: 90.22115325927734 = 0.03545524924993515 + 10.0 * 9.018569946289062
Epoch 2910, val loss: 1.0994694232940674
Epoch 2920, training loss: 90.19827270507812 = 0.03497118875384331 + 10.0 * 9.016329765319824
Epoch 2920, val loss: 1.1051414012908936
Epoch 2930, training loss: 90.19245910644531 = 0.03450349345803261 + 10.0 * 9.015795707702637
Epoch 2930, val loss: 1.1106942892074585
Epoch 2940, training loss: 90.1920166015625 = 0.03404439613223076 + 10.0 * 9.01579761505127
Epoch 2940, val loss: 1.1160959005355835
Epoch 2950, training loss: 90.20085906982422 = 0.03359583765268326 + 10.0 * 9.01672649383545
Epoch 2950, val loss: 1.121585726737976
Epoch 2960, training loss: 90.18852996826172 = 0.03314816579222679 + 10.0 * 9.015538215637207
Epoch 2960, val loss: 1.127149224281311
Epoch 2970, training loss: 90.19503021240234 = 0.03270994871854782 + 10.0 * 9.01623249053955
Epoch 2970, val loss: 1.1326117515563965
Epoch 2980, training loss: 90.19802856445312 = 0.03228430077433586 + 10.0 * 9.016573905944824
Epoch 2980, val loss: 1.1377166509628296
Epoch 2990, training loss: 90.19027709960938 = 0.03185534477233887 + 10.0 * 9.01584243774414
Epoch 2990, val loss: 1.1436586380004883
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8468
Overall ASR: 0.6952
Flip ASR: 0.6197/1554 nodes
./selected_nodes/Pubmed/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 104.6974105834961 = 1.1069085597991943 + 10.0 * 10.359049797058105
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 319, in _train_with_val
    loss.backward()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 6.21 GiB already allocated; 471.69 MiB free; 6.43 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.3 der2:0.2 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 104.68159484863281 = 1.0912840366363525 + 10.0 * 10.359030723571777
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 319, in _train_with_val
    loss.backward()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 6.11 GiB already allocated; 753.69 MiB free; 6.16 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.3 der2:0.2 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 104.69115447998047 = 1.1000356674194336 + 10.0 * 10.359111785888672
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 319, in _train_with_val
    loss.backward()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 6.11 GiB already allocated; 757.69 MiB free; 6.15 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.3 der2:0.2 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 104.68701934814453 = 1.0959044694900513 + 10.0 * 10.359111785888672
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 319, in _train_with_val
    loss.backward()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 6.11 GiB already allocated; 753.69 MiB free; 6.16 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.3 der2:0.3 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 104.68318176269531 = 1.093858003616333 + 10.0 * 10.358932495117188
Epoch 0, val loss: 1.0930017232894897
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 319, in _train_with_val
    loss.backward()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 6.21 GiB already allocated; 423.69 MiB free; 7.13 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.3 der2:0.3 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 104.6954345703125 = 1.1057618856430054 + 10.0 * 10.358967781066895
Epoch 0, val loss: 1.1039776802062988
Epoch 10, training loss: 104.63961791992188 = 1.093240737915039 + 10.0 * 10.35463809967041
Epoch 10, val loss: 1.0911589860916138
Epoch 20, training loss: 104.03813934326172 = 1.0761812925338745 + 10.0 * 10.296195983886719
Epoch 20, val loss: 1.0743744373321533
Epoch 30, training loss: 99.53372955322266 = 1.0616557598114014 + 10.0 * 9.847208023071289
Epoch 30, val loss: 1.060145378112793
Epoch 40, training loss: 95.85836029052734 = 1.0467759370803833 + 10.0 * 9.481158256530762
Epoch 40, val loss: 1.0454630851745605
Epoch 50, training loss: 94.6388168334961 = 1.0287792682647705 + 10.0 * 9.361003875732422
Epoch 50, val loss: 1.0275623798370361
Epoch 60, training loss: 94.23635864257812 = 1.0122092962265015 + 10.0 * 9.322415351867676
Epoch 60, val loss: 1.011618971824646
Epoch 70, training loss: 93.74679565429688 = 1.0006389617919922 + 10.0 * 9.274615287780762
Epoch 70, val loss: 1.0006084442138672
Epoch 80, training loss: 93.385009765625 = 0.9909992218017578 + 10.0 * 9.239400863647461
Epoch 80, val loss: 0.9909167885780334
Epoch 90, training loss: 93.02622985839844 = 0.9793016910552979 + 10.0 * 9.204692840576172
Epoch 90, val loss: 0.9791367053985596
Epoch 100, training loss: 92.77405548095703 = 0.9671279788017273 + 10.0 * 9.180692672729492
Epoch 100, val loss: 0.9671050310134888
Epoch 110, training loss: 92.57771301269531 = 0.9544216394424438 + 10.0 * 9.162328720092773
Epoch 110, val loss: 0.9547141790390015
Epoch 120, training loss: 92.42308044433594 = 0.9401448369026184 + 10.0 * 9.148293495178223
Epoch 120, val loss: 0.9408525228500366
Epoch 130, training loss: 92.27135467529297 = 0.9248046875 + 10.0 * 9.134654998779297
Epoch 130, val loss: 0.9259538054466248
Epoch 140, training loss: 92.1385726928711 = 0.9083662629127502 + 10.0 * 9.123021125793457
Epoch 140, val loss: 0.909952700138092
Epoch 150, training loss: 92.05745697021484 = 0.8896909952163696 + 10.0 * 9.116776466369629
Epoch 150, val loss: 0.8916717767715454
Epoch 160, training loss: 91.94207763671875 = 0.867783784866333 + 10.0 * 9.107429504394531
Epoch 160, val loss: 0.8704544901847839
Epoch 170, training loss: 91.86286163330078 = 0.8429673314094543 + 10.0 * 9.10198974609375
Epoch 170, val loss: 0.8462525010108948
Epoch 180, training loss: 91.79766845703125 = 0.8148187398910522 + 10.0 * 9.098284721374512
Epoch 180, val loss: 0.8190488815307617
Epoch 190, training loss: 91.7262191772461 = 0.783696711063385 + 10.0 * 9.09425163269043
Epoch 190, val loss: 0.7890633344650269
Epoch 200, training loss: 91.66624450683594 = 0.7499923706054688 + 10.0 * 9.091625213623047
Epoch 200, val loss: 0.756621778011322
Epoch 210, training loss: 91.60364532470703 = 0.714271068572998 + 10.0 * 9.088937759399414
Epoch 210, val loss: 0.7225574851036072
Epoch 220, training loss: 91.53678894042969 = 0.6773685216903687 + 10.0 * 9.085942268371582
Epoch 220, val loss: 0.6876158714294434
Epoch 230, training loss: 91.47944641113281 = 0.640107274055481 + 10.0 * 9.08393383026123
Epoch 230, val loss: 0.6526327729225159
Epoch 240, training loss: 91.4232406616211 = 0.6035383939743042 + 10.0 * 9.08197021484375
Epoch 240, val loss: 0.6186360716819763
Epoch 250, training loss: 91.3732681274414 = 0.5688980221748352 + 10.0 * 9.080436706542969
Epoch 250, val loss: 0.5868805646896362
Epoch 260, training loss: 91.32954406738281 = 0.5371583700180054 + 10.0 * 9.079238891601562
Epoch 260, val loss: 0.5581546425819397
Epoch 270, training loss: 91.28118896484375 = 0.5084676146507263 + 10.0 * 9.077272415161133
Epoch 270, val loss: 0.5325464606285095
Epoch 280, training loss: 91.238525390625 = 0.48290345072746277 + 10.0 * 9.075562477111816
Epoch 280, val loss: 0.5100467801094055
Epoch 290, training loss: 91.21304321289062 = 0.46042171120643616 + 10.0 * 9.075262069702148
Epoch 290, val loss: 0.4906172752380371
Epoch 300, training loss: 91.166015625 = 0.44100138545036316 + 10.0 * 9.072501182556152
Epoch 300, val loss: 0.47418564558029175
Epoch 310, training loss: 91.14161682128906 = 0.4242418706417084 + 10.0 * 9.071737289428711
Epoch 310, val loss: 0.4603584110736847
Epoch 320, training loss: 91.11268615722656 = 0.40989673137664795 + 10.0 * 9.070279121398926
Epoch 320, val loss: 0.4488523006439209
Epoch 330, training loss: 91.08145141601562 = 0.3976217210292816 + 10.0 * 9.06838321685791
Epoch 330, val loss: 0.4392637610435486
Epoch 340, training loss: 91.06075286865234 = 0.38706183433532715 + 10.0 * 9.06736946105957
Epoch 340, val loss: 0.4313127398490906
Epoch 350, training loss: 91.06245422363281 = 0.3778686225414276 + 10.0 * 9.068458557128906
Epoch 350, val loss: 0.42466339468955994
Epoch 360, training loss: 91.02690124511719 = 0.3698452115058899 + 10.0 * 9.065706253051758
Epoch 360, val loss: 0.41915321350097656
Epoch 370, training loss: 91.00193786621094 = 0.36282873153686523 + 10.0 * 9.063910484313965
Epoch 370, val loss: 0.4144838750362396
Epoch 380, training loss: 90.9871826171875 = 0.3565540015697479 + 10.0 * 9.06306266784668
Epoch 380, val loss: 0.41050398349761963
Epoch 390, training loss: 90.97483825683594 = 0.3508455455303192 + 10.0 * 9.062398910522461
Epoch 390, val loss: 0.40709376335144043
Epoch 400, training loss: 91.00788879394531 = 0.3456210196018219 + 10.0 * 9.066226959228516
Epoch 400, val loss: 0.40405991673469543
Epoch 410, training loss: 90.9587173461914 = 0.3409491181373596 + 10.0 * 9.061777114868164
Epoch 410, val loss: 0.40151292085647583
Epoch 420, training loss: 90.93080139160156 = 0.3366822600364685 + 10.0 * 9.059412002563477
Epoch 420, val loss: 0.39929890632629395
Epoch 430, training loss: 90.91973114013672 = 0.332692414522171 + 10.0 * 9.058703422546387
Epoch 430, val loss: 0.3973695635795593
Epoch 440, training loss: 90.94991302490234 = 0.3289233446121216 + 10.0 * 9.062098503112793
Epoch 440, val loss: 0.3956403136253357
Epoch 450, training loss: 90.9085693359375 = 0.3254231810569763 + 10.0 * 9.058314323425293
Epoch 450, val loss: 0.3940689265727997
Epoch 460, training loss: 90.88716888427734 = 0.3221394419670105 + 10.0 * 9.056503295898438
Epoch 460, val loss: 0.3926647901535034
Epoch 470, training loss: 90.87702178955078 = 0.31900593638420105 + 10.0 * 9.055801391601562
Epoch 470, val loss: 0.39142006635665894
Epoch 480, training loss: 90.86754608154297 = 0.31598153710365295 + 10.0 * 9.055156707763672
Epoch 480, val loss: 0.390301376581192
Epoch 490, training loss: 90.93387603759766 = 0.3130630850791931 + 10.0 * 9.062081336975098
Epoch 490, val loss: 0.3892975151538849
Epoch 500, training loss: 90.86839294433594 = 0.3102969527244568 + 10.0 * 9.05580997467041
Epoch 500, val loss: 0.38842353224754333
Epoch 510, training loss: 90.84908294677734 = 0.3076644241809845 + 10.0 * 9.054141998291016
Epoch 510, val loss: 0.38757696747779846
Epoch 520, training loss: 90.83847045898438 = 0.3051156997680664 + 10.0 * 9.053335189819336
Epoch 520, val loss: 0.3868396580219269
Epoch 530, training loss: 90.86299896240234 = 0.30263304710388184 + 10.0 * 9.056035995483398
Epoch 530, val loss: 0.38614919781684875
Epoch 540, training loss: 90.82646179199219 = 0.30023282766342163 + 10.0 * 9.05262279510498
Epoch 540, val loss: 0.38562965393066406
Epoch 550, training loss: 90.81716918945312 = 0.29790815711021423 + 10.0 * 9.051926612854004
Epoch 550, val loss: 0.38511520624160767
Epoch 560, training loss: 90.84193420410156 = 0.29563984274864197 + 10.0 * 9.0546293258667
Epoch 560, val loss: 0.3846822679042816
Epoch 570, training loss: 90.81586456298828 = 0.2934420704841614 + 10.0 * 9.052242279052734
Epoch 570, val loss: 0.3842424750328064
Epoch 580, training loss: 90.79435729980469 = 0.2913098633289337 + 10.0 * 9.050304412841797
Epoch 580, val loss: 0.3838551938533783
Epoch 590, training loss: 90.79279327392578 = 0.2892175614833832 + 10.0 * 9.050357818603516
Epoch 590, val loss: 0.3836089074611664
Epoch 600, training loss: 90.79358673095703 = 0.2871665060520172 + 10.0 * 9.050642013549805
Epoch 600, val loss: 0.38337939977645874
Epoch 610, training loss: 90.78749084472656 = 0.2851654887199402 + 10.0 * 9.050232887268066
Epoch 610, val loss: 0.38316330313682556
Epoch 620, training loss: 90.77204132080078 = 0.2832169532775879 + 10.0 * 9.048882484436035
Epoch 620, val loss: 0.3830101191997528
Epoch 630, training loss: 90.76544952392578 = 0.2813016474246979 + 10.0 * 9.04841423034668
Epoch 630, val loss: 0.3828796148300171
Epoch 640, training loss: 90.75904083251953 = 0.2794113755226135 + 10.0 * 9.04796314239502
Epoch 640, val loss: 0.3827728033065796
Epoch 650, training loss: 90.78952026367188 = 0.27754586935043335 + 10.0 * 9.05119800567627
Epoch 650, val loss: 0.3826843798160553
Epoch 660, training loss: 90.75984954833984 = 0.275726318359375 + 10.0 * 9.048412322998047
Epoch 660, val loss: 0.38279497623443604
Epoch 670, training loss: 90.74900817871094 = 0.2739390432834625 + 10.0 * 9.047506332397461
Epoch 670, val loss: 0.3828151524066925
Epoch 680, training loss: 90.75579071044922 = 0.2721782624721527 + 10.0 * 9.048360824584961
Epoch 680, val loss: 0.3828504979610443
Epoch 690, training loss: 90.74073028564453 = 0.27044400572776794 + 10.0 * 9.047028541564941
Epoch 690, val loss: 0.38296523690223694
Epoch 700, training loss: 90.73178100585938 = 0.26873648166656494 + 10.0 * 9.046304702758789
Epoch 700, val loss: 0.38309699296951294
Epoch 710, training loss: 90.7365493774414 = 0.267045795917511 + 10.0 * 9.046950340270996
Epoch 710, val loss: 0.3833177089691162
Epoch 720, training loss: 90.71919250488281 = 0.26537781953811646 + 10.0 * 9.045381546020508
Epoch 720, val loss: 0.3835028111934662
Epoch 730, training loss: 90.71992492675781 = 0.2637297511100769 + 10.0 * 9.045619010925293
Epoch 730, val loss: 0.3837254047393799
Epoch 740, training loss: 90.72776794433594 = 0.26210567355155945 + 10.0 * 9.046566009521484
Epoch 740, val loss: 0.3840174973011017
Epoch 750, training loss: 90.71614837646484 = 0.2605062425136566 + 10.0 * 9.045564651489258
Epoch 750, val loss: 0.3843052089214325
Epoch 760, training loss: 90.71430206298828 = 0.2589344084262848 + 10.0 * 9.045536994934082
Epoch 760, val loss: 0.38461193442344666
Epoch 770, training loss: 90.70093536376953 = 0.25737592577934265 + 10.0 * 9.044355392456055
Epoch 770, val loss: 0.3850419521331787
Epoch 780, training loss: 90.69819641113281 = 0.2558339536190033 + 10.0 * 9.044236183166504
Epoch 780, val loss: 0.38538333773612976
Epoch 790, training loss: 90.69490051269531 = 0.25430580973625183 + 10.0 * 9.044059753417969
Epoch 790, val loss: 0.38584956526756287
Epoch 800, training loss: 90.69803619384766 = 0.25279179215431213 + 10.0 * 9.044524192810059
Epoch 800, val loss: 0.3862392008304596
Epoch 810, training loss: 90.6819076538086 = 0.25129860639572144 + 10.0 * 9.043061256408691
Epoch 810, val loss: 0.38678982853889465
Epoch 820, training loss: 90.67911529541016 = 0.24981971085071564 + 10.0 * 9.042929649353027
Epoch 820, val loss: 0.38727107644081116
Epoch 830, training loss: 90.69447326660156 = 0.2483544796705246 + 10.0 * 9.044611930847168
Epoch 830, val loss: 0.3878631293773651
Epoch 840, training loss: 90.68460083007812 = 0.24690888822078705 + 10.0 * 9.043768882751465
Epoch 840, val loss: 0.38838115334510803
Epoch 850, training loss: 90.66989135742188 = 0.245478555560112 + 10.0 * 9.042441368103027
Epoch 850, val loss: 0.38906732201576233
Epoch 860, training loss: 90.66091918945312 = 0.24406307935714722 + 10.0 * 9.041685104370117
Epoch 860, val loss: 0.38963228464126587
Epoch 870, training loss: 90.65753936767578 = 0.242655947804451 + 10.0 * 9.041488647460938
Epoch 870, val loss: 0.3902897536754608
Epoch 880, training loss: 90.65583038330078 = 0.2412531077861786 + 10.0 * 9.041457176208496
Epoch 880, val loss: 0.39098218083381653
Epoch 890, training loss: 90.68457794189453 = 0.23986393213272095 + 10.0 * 9.044471740722656
Epoch 890, val loss: 0.39176324009895325
Epoch 900, training loss: 90.65001678466797 = 0.23849569261074066 + 10.0 * 9.041152000427246
Epoch 900, val loss: 0.39243486523628235
Epoch 910, training loss: 90.64249420166016 = 0.23713654279708862 + 10.0 * 9.040535926818848
Epoch 910, val loss: 0.3932240903377533
Epoch 920, training loss: 90.64186096191406 = 0.2357810139656067 + 10.0 * 9.040608406066895
Epoch 920, val loss: 0.39398908615112305
Epoch 930, training loss: 90.67886352539062 = 0.23443686962127686 + 10.0 * 9.044443130493164
Epoch 930, val loss: 0.39480581879615784
Epoch 940, training loss: 90.63667297363281 = 0.23310916125774384 + 10.0 * 9.040356636047363
Epoch 940, val loss: 0.39568421244621277
Epoch 950, training loss: 90.63260650634766 = 0.23179185390472412 + 10.0 * 9.040081977844238
Epoch 950, val loss: 0.3966110646724701
Epoch 960, training loss: 90.6388168334961 = 0.2304774522781372 + 10.0 * 9.040834426879883
Epoch 960, val loss: 0.3974553346633911
Epoch 970, training loss: 90.62296295166016 = 0.2291739284992218 + 10.0 * 9.039379119873047
Epoch 970, val loss: 0.3983827829360962
Epoch 980, training loss: 90.6344985961914 = 0.2278793305158615 + 10.0 * 9.040661811828613
Epoch 980, val loss: 0.39930853247642517
Epoch 990, training loss: 90.61730194091797 = 0.22659461200237274 + 10.0 * 9.039071083068848
Epoch 990, val loss: 0.4003794491291046
Epoch 1000, training loss: 90.62208557128906 = 0.22531741857528687 + 10.0 * 9.039676666259766
Epoch 1000, val loss: 0.40137025713920593
Epoch 1010, training loss: 90.61517333984375 = 0.2240481674671173 + 10.0 * 9.03911304473877
Epoch 1010, val loss: 0.4024144411087036
Epoch 1020, training loss: 90.614990234375 = 0.22279292345046997 + 10.0 * 9.039219856262207
Epoch 1020, val loss: 0.40346622467041016
Epoch 1030, training loss: 90.606689453125 = 0.22154533863067627 + 10.0 * 9.038515090942383
Epoch 1030, val loss: 0.4045794904232025
Epoch 1040, training loss: 90.59971618652344 = 0.22030213475227356 + 10.0 * 9.037941932678223
Epoch 1040, val loss: 0.40567547082901
Epoch 1050, training loss: 90.60262298583984 = 0.21906253695487976 + 10.0 * 9.038355827331543
Epoch 1050, val loss: 0.4068242013454437
Epoch 1060, training loss: 90.61054992675781 = 0.21783111989498138 + 10.0 * 9.039271354675293
Epoch 1060, val loss: 0.4080217778682709
Epoch 1070, training loss: 90.59078979492188 = 0.21661372482776642 + 10.0 * 9.0374174118042
Epoch 1070, val loss: 0.40914323925971985
Epoch 1080, training loss: 90.59090423583984 = 0.2153998166322708 + 10.0 * 9.03754997253418
Epoch 1080, val loss: 0.41035863757133484
Epoch 1090, training loss: 90.5951919555664 = 0.21418896317481995 + 10.0 * 9.038100242614746
Epoch 1090, val loss: 0.41154175996780396
Epoch 1100, training loss: 90.58329010009766 = 0.21298807859420776 + 10.0 * 9.037030220031738
Epoch 1100, val loss: 0.4128488302230835
Epoch 1110, training loss: 90.5777816772461 = 0.21179386973381042 + 10.0 * 9.036599159240723
Epoch 1110, val loss: 0.41409534215927124
Epoch 1120, training loss: 90.57421875 = 0.21060171723365784 + 10.0 * 9.036361694335938
Epoch 1120, val loss: 0.41539162397384644
Epoch 1130, training loss: 90.57245635986328 = 0.2094149887561798 + 10.0 * 9.036304473876953
Epoch 1130, val loss: 0.41671159863471985
Epoch 1140, training loss: 90.64657592773438 = 0.20823949575424194 + 10.0 * 9.04383373260498
Epoch 1140, val loss: 0.418093740940094
Epoch 1150, training loss: 90.57195281982422 = 0.20707081258296967 + 10.0 * 9.03648853302002
Epoch 1150, val loss: 0.4193747937679291
Epoch 1160, training loss: 90.56414794921875 = 0.20591527223587036 + 10.0 * 9.035822868347168
Epoch 1160, val loss: 0.4206785261631012
Epoch 1170, training loss: 90.56147003173828 = 0.20476111769676208 + 10.0 * 9.03567123413086
Epoch 1170, val loss: 0.42201855778694153
Epoch 1180, training loss: 90.55799865722656 = 0.20361022651195526 + 10.0 * 9.035438537597656
Epoch 1180, val loss: 0.42340287566185
Epoch 1190, training loss: 90.57785034179688 = 0.2024669647216797 + 10.0 * 9.037538528442383
Epoch 1190, val loss: 0.4247840344905853
Epoch 1200, training loss: 90.57135009765625 = 0.20133060216903687 + 10.0 * 9.037001609802246
Epoch 1200, val loss: 0.42632409930229187
Epoch 1210, training loss: 90.55096435546875 = 0.20020096004009247 + 10.0 * 9.035076141357422
Epoch 1210, val loss: 0.4276900887489319
Epoch 1220, training loss: 90.5501480102539 = 0.19907531142234802 + 10.0 * 9.035107612609863
Epoch 1220, val loss: 0.4290846586227417
Epoch 1230, training loss: 90.54481506347656 = 0.19795113801956177 + 10.0 * 9.034686088562012
Epoch 1230, val loss: 0.430536687374115
Epoch 1240, training loss: 90.57402038574219 = 0.19683073461055756 + 10.0 * 9.037718772888184
Epoch 1240, val loss: 0.43204277753829956
Epoch 1250, training loss: 90.54960632324219 = 0.1957160234451294 + 10.0 * 9.035388946533203
Epoch 1250, val loss: 0.43358907103538513
Epoch 1260, training loss: 90.53995513916016 = 0.19460242986679077 + 10.0 * 9.03453540802002
Epoch 1260, val loss: 0.43502718210220337
Epoch 1270, training loss: 90.53497314453125 = 0.19349147379398346 + 10.0 * 9.034148216247559
Epoch 1270, val loss: 0.4365510046482086
Epoch 1280, training loss: 90.56292724609375 = 0.19238238036632538 + 10.0 * 9.037054061889648
Epoch 1280, val loss: 0.43808817863464355
Epoch 1290, training loss: 90.5399398803711 = 0.19128616154193878 + 10.0 * 9.034865379333496
Epoch 1290, val loss: 0.43979233503341675
Epoch 1300, training loss: 90.53099060058594 = 0.19018438458442688 + 10.0 * 9.034080505371094
Epoch 1300, val loss: 0.44124719500541687
Epoch 1310, training loss: 90.52607727050781 = 0.18908798694610596 + 10.0 * 9.033699035644531
Epoch 1310, val loss: 0.44285324215888977
Epoch 1320, training loss: 90.55291748046875 = 0.18799397349357605 + 10.0 * 9.036492347717285
Epoch 1320, val loss: 0.4444975256919861
Epoch 1330, training loss: 90.52754211425781 = 0.18690697848796844 + 10.0 * 9.034063339233398
Epoch 1330, val loss: 0.44606468081474304
Epoch 1340, training loss: 90.51729583740234 = 0.18581703305244446 + 10.0 * 9.033147811889648
Epoch 1340, val loss: 0.4476909935474396
Epoch 1350, training loss: 90.51500701904297 = 0.18472839891910553 + 10.0 * 9.033027648925781
Epoch 1350, val loss: 0.4492819607257843
Epoch 1360, training loss: 90.52790069580078 = 0.18364638090133667 + 10.0 * 9.034425735473633
Epoch 1360, val loss: 0.45101198554039
Epoch 1370, training loss: 90.52411651611328 = 0.1825636923313141 + 10.0 * 9.034154891967773
Epoch 1370, val loss: 0.4528985321521759
Epoch 1380, training loss: 90.51229858398438 = 0.18148651719093323 + 10.0 * 9.0330810546875
Epoch 1380, val loss: 0.4543045163154602
Epoch 1390, training loss: 90.50764465332031 = 0.18041113018989563 + 10.0 * 9.032723426818848
Epoch 1390, val loss: 0.4561871886253357
Epoch 1400, training loss: 90.50474548339844 = 0.1793365180492401 + 10.0 * 9.032541275024414
Epoch 1400, val loss: 0.4577944278717041
Epoch 1410, training loss: 90.51280975341797 = 0.1782660186290741 + 10.0 * 9.033453941345215
Epoch 1410, val loss: 0.45957833528518677
Epoch 1420, training loss: 90.4965591430664 = 0.1771954745054245 + 10.0 * 9.031936645507812
Epoch 1420, val loss: 0.4613969326019287
Epoch 1430, training loss: 90.51040649414062 = 0.1761288195848465 + 10.0 * 9.033427238464355
Epoch 1430, val loss: 0.4631904065608978
Epoch 1440, training loss: 90.50323486328125 = 0.1750631481409073 + 10.0 * 9.032816886901855
Epoch 1440, val loss: 0.4649994671344757
Epoch 1450, training loss: 90.49052429199219 = 0.17399859428405762 + 10.0 * 9.031652450561523
Epoch 1450, val loss: 0.4667234718799591
Epoch 1460, training loss: 90.48703002929688 = 0.17293809354305267 + 10.0 * 9.03140926361084
Epoch 1460, val loss: 0.468585342168808
Epoch 1470, training loss: 90.48233032226562 = 0.1718725562095642 + 10.0 * 9.031045913696289
Epoch 1470, val loss: 0.47041580080986023
Epoch 1480, training loss: 90.48181915283203 = 0.17080989480018616 + 10.0 * 9.03110122680664
Epoch 1480, val loss: 0.47231289744377136
Epoch 1490, training loss: 90.53121948242188 = 0.16975441575050354 + 10.0 * 9.03614616394043
Epoch 1490, val loss: 0.4742719531059265
Epoch 1500, training loss: 90.48832702636719 = 0.168691948056221 + 10.0 * 9.031963348388672
Epoch 1500, val loss: 0.4761511981487274
Epoch 1510, training loss: 90.4783935546875 = 0.1676361858844757 + 10.0 * 9.031076431274414
Epoch 1510, val loss: 0.4780619144439697
Epoch 1520, training loss: 90.47129821777344 = 0.1665792018175125 + 10.0 * 9.030471801757812
Epoch 1520, val loss: 0.48001402616500854
Epoch 1530, training loss: 90.47511291503906 = 0.16552206873893738 + 10.0 * 9.030959129333496
Epoch 1530, val loss: 0.48195403814315796
Epoch 1540, training loss: 90.48983001708984 = 0.16446980834007263 + 10.0 * 9.032536506652832
Epoch 1540, val loss: 0.4839445650577545
Epoch 1550, training loss: 90.46685028076172 = 0.16342328488826752 + 10.0 * 9.030343055725098
Epoch 1550, val loss: 0.486065536737442
Epoch 1560, training loss: 90.46382141113281 = 0.1623734086751938 + 10.0 * 9.030144691467285
Epoch 1560, val loss: 0.4881476163864136
Epoch 1570, training loss: 90.46550750732422 = 0.16131943464279175 + 10.0 * 9.03041934967041
Epoch 1570, val loss: 0.49018916487693787
Epoch 1580, training loss: 90.47687530517578 = 0.1602727621793747 + 10.0 * 9.031660079956055
Epoch 1580, val loss: 0.49235063791275024
Epoch 1590, training loss: 90.46479034423828 = 0.15922211110591888 + 10.0 * 9.030556678771973
Epoch 1590, val loss: 0.494274377822876
Epoch 1600, training loss: 90.45934295654297 = 0.1581731140613556 + 10.0 * 9.03011703491211
Epoch 1600, val loss: 0.496553510427475
Epoch 1610, training loss: 90.46424102783203 = 0.15712769329547882 + 10.0 * 9.03071117401123
Epoch 1610, val loss: 0.49861857295036316
Epoch 1620, training loss: 90.45061492919922 = 0.15607890486717224 + 10.0 * 9.02945327758789
Epoch 1620, val loss: 0.5009276866912842
Epoch 1630, training loss: 90.44670867919922 = 0.15503379702568054 + 10.0 * 9.029167175292969
Epoch 1630, val loss: 0.5030368566513062
Epoch 1640, training loss: 90.4500961303711 = 0.15398916602134705 + 10.0 * 9.029610633850098
Epoch 1640, val loss: 0.5053348541259766
Epoch 1650, training loss: 90.46221160888672 = 0.15294477343559265 + 10.0 * 9.030926704406738
Epoch 1650, val loss: 0.5075430870056152
Epoch 1660, training loss: 90.4430160522461 = 0.15190653502941132 + 10.0 * 9.0291109085083
Epoch 1660, val loss: 0.5099037885665894
Epoch 1670, training loss: 90.43743133544922 = 0.1508605182170868 + 10.0 * 9.028656959533691
Epoch 1670, val loss: 0.5121467113494873
Epoch 1680, training loss: 90.44480895996094 = 0.14982126653194427 + 10.0 * 9.029499053955078
Epoch 1680, val loss: 0.5145611763000488
Epoch 1690, training loss: 90.43606567382812 = 0.14877766370773315 + 10.0 * 9.028728485107422
Epoch 1690, val loss: 0.5168823599815369
Epoch 1700, training loss: 90.43733978271484 = 0.14773383736610413 + 10.0 * 9.028960227966309
Epoch 1700, val loss: 0.5192296504974365
Epoch 1710, training loss: 90.45484924316406 = 0.14670126140117645 + 10.0 * 9.030815124511719
Epoch 1710, val loss: 0.5216999053955078
Epoch 1720, training loss: 90.43513488769531 = 0.1456625759601593 + 10.0 * 9.028947830200195
Epoch 1720, val loss: 0.523999035358429
Epoch 1730, training loss: 90.42491149902344 = 0.14462405443191528 + 10.0 * 9.02802848815918
Epoch 1730, val loss: 0.5265501141548157
Epoch 1740, training loss: 90.42176818847656 = 0.1435876488685608 + 10.0 * 9.02781867980957
Epoch 1740, val loss: 0.5289641618728638
Epoch 1750, training loss: 90.42140197753906 = 0.142554372549057 + 10.0 * 9.027884483337402
Epoch 1750, val loss: 0.5315285921096802
Epoch 1760, training loss: 90.4502182006836 = 0.1415284425020218 + 10.0 * 9.030869483947754
Epoch 1760, val loss: 0.5340887308120728
Epoch 1770, training loss: 90.43379211425781 = 0.14048902690410614 + 10.0 * 9.029330253601074
Epoch 1770, val loss: 0.5365990996360779
Epoch 1780, training loss: 90.41532135009766 = 0.13945414125919342 + 10.0 * 9.027585983276367
Epoch 1780, val loss: 0.5392206311225891
Epoch 1790, training loss: 90.41201782226562 = 0.13842031359672546 + 10.0 * 9.027359962463379
Epoch 1790, val loss: 0.5417612195014954
Epoch 1800, training loss: 90.43024444580078 = 0.13739533722400665 + 10.0 * 9.029284477233887
Epoch 1800, val loss: 0.544418454170227
Epoch 1810, training loss: 90.41297149658203 = 0.13635671138763428 + 10.0 * 9.027661323547363
Epoch 1810, val loss: 0.546924352645874
Epoch 1820, training loss: 90.41266632080078 = 0.13532766699790955 + 10.0 * 9.02773380279541
Epoch 1820, val loss: 0.5496023297309875
Epoch 1830, training loss: 90.40634155273438 = 0.13428997993469238 + 10.0 * 9.027204513549805
Epoch 1830, val loss: 0.5524391531944275
Epoch 1840, training loss: 90.40415954589844 = 0.1332579404115677 + 10.0 * 9.027090072631836
Epoch 1840, val loss: 0.5552310347557068
Epoch 1850, training loss: 90.42150115966797 = 0.1322280764579773 + 10.0 * 9.02892780303955
Epoch 1850, val loss: 0.5579819083213806
Epoch 1860, training loss: 90.39927673339844 = 0.13119326531887054 + 10.0 * 9.02680778503418
Epoch 1860, val loss: 0.5606896877288818
Epoch 1870, training loss: 90.39473724365234 = 0.1301591843366623 + 10.0 * 9.026457786560059
Epoch 1870, val loss: 0.5635267496109009
Epoch 1880, training loss: 90.39936828613281 = 0.12912806868553162 + 10.0 * 9.027024269104004
Epoch 1880, val loss: 0.5663257837295532
Epoch 1890, training loss: 90.39859771728516 = 0.12809273600578308 + 10.0 * 9.027050971984863
Epoch 1890, val loss: 0.5692481398582458
Epoch 1900, training loss: 90.40013122558594 = 0.12706193327903748 + 10.0 * 9.027307510375977
Epoch 1900, val loss: 0.5721897482872009
Epoch 1910, training loss: 90.3912582397461 = 0.126028910279274 + 10.0 * 9.026522636413574
Epoch 1910, val loss: 0.575087308883667
Epoch 1920, training loss: 90.38949584960938 = 0.12500311434268951 + 10.0 * 9.026449203491211
Epoch 1920, val loss: 0.5780286192893982
Epoch 1930, training loss: 90.40130615234375 = 0.12396883964538574 + 10.0 * 9.02773380279541
Epoch 1930, val loss: 0.5810653567314148
Epoch 1940, training loss: 90.3824462890625 = 0.12292823195457458 + 10.0 * 9.025952339172363
Epoch 1940, val loss: 0.5838591456413269
Epoch 1950, training loss: 90.37650299072266 = 0.12189546227455139 + 10.0 * 9.025461196899414
Epoch 1950, val loss: 0.5868815183639526
Epoch 1960, training loss: 90.37344360351562 = 0.12086137384176254 + 10.0 * 9.02525806427002
Epoch 1960, val loss: 0.5898650884628296
Epoch 1970, training loss: 90.37731170654297 = 0.11982943117618561 + 10.0 * 9.025748252868652
Epoch 1970, val loss: 0.5928764343261719
Epoch 1980, training loss: 90.3883285522461 = 0.11879733949899673 + 10.0 * 9.026952743530273
Epoch 1980, val loss: 0.5960444808006287
Epoch 1990, training loss: 90.37251281738281 = 0.11776749789714813 + 10.0 * 9.025474548339844
Epoch 1990, val loss: 0.5992648601531982
Epoch 2000, training loss: 90.36820220947266 = 0.11673516035079956 + 10.0 * 9.025146484375
Epoch 2000, val loss: 0.6022695899009705
Epoch 2010, training loss: 90.3778076171875 = 0.11570998281240463 + 10.0 * 9.026209831237793
Epoch 2010, val loss: 0.605486273765564
Epoch 2020, training loss: 90.36730194091797 = 0.11468523740768433 + 10.0 * 9.025261878967285
Epoch 2020, val loss: 0.6086286306381226
Epoch 2030, training loss: 90.37142944335938 = 0.1136602908372879 + 10.0 * 9.025776863098145
Epoch 2030, val loss: 0.6116958260536194
Epoch 2040, training loss: 90.36611938476562 = 0.11263516545295715 + 10.0 * 9.025348663330078
Epoch 2040, val loss: 0.6149001717567444
Epoch 2050, training loss: 90.35795593261719 = 0.11161624640226364 + 10.0 * 9.024633407592773
Epoch 2050, val loss: 0.618225634098053
Epoch 2060, training loss: 90.35337829589844 = 0.11059479415416718 + 10.0 * 9.02427864074707
Epoch 2060, val loss: 0.6215478777885437
Epoch 2070, training loss: 90.35704803466797 = 0.10957719385623932 + 10.0 * 9.024746894836426
Epoch 2070, val loss: 0.6248853206634521
Epoch 2080, training loss: 90.36619567871094 = 0.1085568368434906 + 10.0 * 9.025763511657715
Epoch 2080, val loss: 0.628066897392273
Epoch 2090, training loss: 90.36175537109375 = 0.10753969848155975 + 10.0 * 9.025422096252441
Epoch 2090, val loss: 0.6311454772949219
Epoch 2100, training loss: 90.34971618652344 = 0.10652659088373184 + 10.0 * 9.02431869506836
Epoch 2100, val loss: 0.6348655223846436
Epoch 2110, training loss: 90.34793853759766 = 0.10550985485315323 + 10.0 * 9.024243354797363
Epoch 2110, val loss: 0.6380762457847595
Epoch 2120, training loss: 90.35540008544922 = 0.10450361669063568 + 10.0 * 9.025090217590332
Epoch 2120, val loss: 0.6415948867797852
Epoch 2130, training loss: 90.357177734375 = 0.10348594933748245 + 10.0 * 9.025369644165039
Epoch 2130, val loss: 0.6450102925300598
Epoch 2140, training loss: 90.34235382080078 = 0.10247616469860077 + 10.0 * 9.023987770080566
Epoch 2140, val loss: 0.6482793092727661
Epoch 2150, training loss: 90.33927917480469 = 0.10146638751029968 + 10.0 * 9.023781776428223
Epoch 2150, val loss: 0.6519532203674316
Epoch 2160, training loss: 90.33741760253906 = 0.10046142339706421 + 10.0 * 9.02369499206543
Epoch 2160, val loss: 0.655432939529419
Epoch 2170, training loss: 90.35022735595703 = 0.09946228563785553 + 10.0 * 9.025075912475586
Epoch 2170, val loss: 0.6591432690620422
Epoch 2180, training loss: 90.34366607666016 = 0.09846271574497223 + 10.0 * 9.024519920349121
Epoch 2180, val loss: 0.6623479127883911
Epoch 2190, training loss: 90.32911682128906 = 0.09745650738477707 + 10.0 * 9.023165702819824
Epoch 2190, val loss: 0.6661074161529541
Epoch 2200, training loss: 90.3249740600586 = 0.09645858407020569 + 10.0 * 9.022851943969727
Epoch 2200, val loss: 0.6696966290473938
Epoch 2210, training loss: 90.32372283935547 = 0.09546162933111191 + 10.0 * 9.022826194763184
Epoch 2210, val loss: 0.6733525991439819
Epoch 2220, training loss: 90.35639953613281 = 0.09447410702705383 + 10.0 * 9.026192665100098
Epoch 2220, val loss: 0.6768987774848938
Epoch 2230, training loss: 90.33155822753906 = 0.09347788244485855 + 10.0 * 9.023808479309082
Epoch 2230, val loss: 0.6808359622955322
Epoch 2240, training loss: 90.32096862792969 = 0.09248077124357224 + 10.0 * 9.022848129272461
Epoch 2240, val loss: 0.6845064759254456
Epoch 2250, training loss: 90.33736419677734 = 0.09149855375289917 + 10.0 * 9.02458667755127
Epoch 2250, val loss: 0.688408374786377
Epoch 2260, training loss: 90.32072448730469 = 0.09051504731178284 + 10.0 * 9.02302074432373
Epoch 2260, val loss: 0.6922653317451477
Epoch 2270, training loss: 90.31157684326172 = 0.08952846378087997 + 10.0 * 9.022204399108887
Epoch 2270, val loss: 0.6958041787147522
Epoch 2280, training loss: 90.31177520751953 = 0.08855624496936798 + 10.0 * 9.022321701049805
Epoch 2280, val loss: 0.6997144818305969
Epoch 2290, training loss: 90.33073425292969 = 0.0875934436917305 + 10.0 * 9.024313926696777
Epoch 2290, val loss: 0.7036864161491394
Epoch 2300, training loss: 90.30912780761719 = 0.08661539107561111 + 10.0 * 9.02225112915039
Epoch 2300, val loss: 0.7074200510978699
Epoch 2310, training loss: 90.3060531616211 = 0.08565126359462738 + 10.0 * 9.022040367126465
Epoch 2310, val loss: 0.7113505601882935
Epoch 2320, training loss: 90.33233642578125 = 0.08469802141189575 + 10.0 * 9.024763107299805
Epoch 2320, val loss: 0.7151094079017639
Epoch 2330, training loss: 90.31461334228516 = 0.08374173939228058 + 10.0 * 9.023087501525879
Epoch 2330, val loss: 0.7193418741226196
Epoch 2340, training loss: 90.29961395263672 = 0.08278588950634003 + 10.0 * 9.021682739257812
Epoch 2340, val loss: 0.7231835722923279
Epoch 2350, training loss: 90.29630279541016 = 0.08184147626161575 + 10.0 * 9.021446228027344
Epoch 2350, val loss: 0.7272580862045288
Epoch 2360, training loss: 90.30984497070312 = 0.08090461790561676 + 10.0 * 9.022893905639648
Epoch 2360, val loss: 0.7312319278717041
Epoch 2370, training loss: 90.30217742919922 = 0.07997056841850281 + 10.0 * 9.022220611572266
Epoch 2370, val loss: 0.7353550791740417
Epoch 2380, training loss: 90.29457092285156 = 0.07904132455587387 + 10.0 * 9.021553039550781
Epoch 2380, val loss: 0.7397176027297974
Epoch 2390, training loss: 90.29666137695312 = 0.07812109589576721 + 10.0 * 9.021854400634766
Epoch 2390, val loss: 0.7437540888786316
Epoch 2400, training loss: 90.29801177978516 = 0.07720238715410233 + 10.0 * 9.02208137512207
Epoch 2400, val loss: 0.7476688623428345
Epoch 2410, training loss: 90.28765106201172 = 0.07628846168518066 + 10.0 * 9.021136283874512
Epoch 2410, val loss: 0.7521750330924988
Epoch 2420, training loss: 90.2859115600586 = 0.07538536190986633 + 10.0 * 9.021052360534668
Epoch 2420, val loss: 0.7562982439994812
Epoch 2430, training loss: 90.29573822021484 = 0.07448802888393402 + 10.0 * 9.022125244140625
Epoch 2430, val loss: 0.7605018615722656
Epoch 2440, training loss: 90.28175354003906 = 0.07359351217746735 + 10.0 * 9.0208158493042
Epoch 2440, val loss: 0.7649375796318054
Epoch 2450, training loss: 90.2802963256836 = 0.07270754128694534 + 10.0 * 9.020758628845215
Epoch 2450, val loss: 0.7691941261291504
Epoch 2460, training loss: 90.28713989257812 = 0.07182829827070236 + 10.0 * 9.021531105041504
Epoch 2460, val loss: 0.7734890580177307
Epoch 2470, training loss: 90.27912139892578 = 0.0709497481584549 + 10.0 * 9.020816802978516
Epoch 2470, val loss: 0.7778996229171753
Epoch 2480, training loss: 90.28267669677734 = 0.07008074223995209 + 10.0 * 9.021259307861328
Epoch 2480, val loss: 0.7822288870811462
Epoch 2490, training loss: 90.2818603515625 = 0.06921803206205368 + 10.0 * 9.02126407623291
Epoch 2490, val loss: 0.7866371273994446
Epoch 2500, training loss: 90.2906494140625 = 0.0683644562959671 + 10.0 * 9.022228240966797
Epoch 2500, val loss: 0.7912825345993042
Epoch 2510, training loss: 90.27410888671875 = 0.06751415133476257 + 10.0 * 9.020659446716309
Epoch 2510, val loss: 0.7952786087989807
Epoch 2520, training loss: 90.2672348022461 = 0.06666859239339828 + 10.0 * 9.02005672454834
Epoch 2520, val loss: 0.7996023893356323
Epoch 2530, training loss: 90.2699203491211 = 0.065841905772686 + 10.0 * 9.020407676696777
Epoch 2530, val loss: 0.8040085434913635
Epoch 2540, training loss: 90.28191375732422 = 0.0650210827589035 + 10.0 * 9.021689414978027
Epoch 2540, val loss: 0.8086423277854919
Epoch 2550, training loss: 90.27313232421875 = 0.06420409679412842 + 10.0 * 9.020893096923828
Epoch 2550, val loss: 0.8129684329032898
Epoch 2560, training loss: 90.26457977294922 = 0.06338990479707718 + 10.0 * 9.020118713378906
Epoch 2560, val loss: 0.8170773386955261
Epoch 2570, training loss: 90.2586441040039 = 0.06258665770292282 + 10.0 * 9.01960563659668
Epoch 2570, val loss: 0.8215330243110657
Epoch 2580, training loss: 90.25741577148438 = 0.061792027205228806 + 10.0 * 9.019562721252441
Epoch 2580, val loss: 0.8261944055557251
Epoch 2590, training loss: 90.27547454833984 = 0.06101454794406891 + 10.0 * 9.021446228027344
Epoch 2590, val loss: 0.8308328986167908
Epoch 2600, training loss: 90.27076721191406 = 0.060235396027565 + 10.0 * 9.021053314208984
Epoch 2600, val loss: 0.8352020978927612
Epoch 2610, training loss: 90.26078033447266 = 0.05945754051208496 + 10.0 * 9.020132064819336
Epoch 2610, val loss: 0.8396441340446472
Epoch 2620, training loss: 90.25701904296875 = 0.05869193375110626 + 10.0 * 9.019832611083984
Epoch 2620, val loss: 0.844162106513977
Epoch 2630, training loss: 90.26031494140625 = 0.057941045612096786 + 10.0 * 9.020237922668457
Epoch 2630, val loss: 0.848473310470581
Epoch 2640, training loss: 90.249267578125 = 0.05719003453850746 + 10.0 * 9.019207954406738
Epoch 2640, val loss: 0.8528071641921997
Epoch 2650, training loss: 90.24512481689453 = 0.0564521849155426 + 10.0 * 9.018867492675781
Epoch 2650, val loss: 0.8575189709663391
Epoch 2660, training loss: 90.24507141113281 = 0.05572452023625374 + 10.0 * 9.01893424987793
Epoch 2660, val loss: 0.8622029423713684
Epoch 2670, training loss: 90.2618408203125 = 0.055011678487062454 + 10.0 * 9.020683288574219
Epoch 2670, val loss: 0.8666531443595886
Epoch 2680, training loss: 90.2487564086914 = 0.05429008603096008 + 10.0 * 9.01944637298584
Epoch 2680, val loss: 0.8703997731208801
Epoch 2690, training loss: 90.24407196044922 = 0.053582727909088135 + 10.0 * 9.019048690795898
Epoch 2690, val loss: 0.8754991292953491
Epoch 2700, training loss: 90.26077270507812 = 0.0528893768787384 + 10.0 * 9.020788192749023
Epoch 2700, val loss: 0.8794819712638855
Epoch 2710, training loss: 90.24191284179688 = 0.052201684564352036 + 10.0 * 9.01897144317627
Epoch 2710, val loss: 0.8843820691108704
Epoch 2720, training loss: 90.23564910888672 = 0.05151505395770073 + 10.0 * 9.018413543701172
Epoch 2720, val loss: 0.8885794281959534
Epoch 2730, training loss: 90.23848724365234 = 0.05084686726331711 + 10.0 * 9.018763542175293
Epoch 2730, val loss: 0.8930991291999817
Epoch 2740, training loss: 90.2457275390625 = 0.05018444359302521 + 10.0 * 9.019554138183594
Epoch 2740, val loss: 0.8974434733390808
Epoch 2750, training loss: 90.24945831298828 = 0.049527522176504135 + 10.0 * 9.01999282836914
Epoch 2750, val loss: 0.9021749496459961
Epoch 2760, training loss: 90.23298645019531 = 0.048876211047172546 + 10.0 * 9.018411636352539
Epoch 2760, val loss: 0.9063152074813843
Epoch 2770, training loss: 90.22737121582031 = 0.048233624547719955 + 10.0 * 9.017913818359375
Epoch 2770, val loss: 0.9108172655105591
Epoch 2780, training loss: 90.23759460449219 = 0.047607749700546265 + 10.0 * 9.018999099731445
Epoch 2780, val loss: 0.9154560565948486
Epoch 2790, training loss: 90.22974395751953 = 0.04698158800601959 + 10.0 * 9.01827621459961
Epoch 2790, val loss: 0.919603705406189
Epoch 2800, training loss: 90.22447967529297 = 0.046364542096853256 + 10.0 * 9.01781177520752
Epoch 2800, val loss: 0.9241455793380737
Epoch 2810, training loss: 90.2325439453125 = 0.04575882852077484 + 10.0 * 9.018678665161133
Epoch 2810, val loss: 0.9283726811408997
Epoch 2820, training loss: 90.22970581054688 = 0.045156653970479965 + 10.0 * 9.018454551696777
Epoch 2820, val loss: 0.9328852891921997
Epoch 2830, training loss: 90.22429656982422 = 0.04456539452075958 + 10.0 * 9.017972946166992
Epoch 2830, val loss: 0.937441885471344
Epoch 2840, training loss: 90.21863555908203 = 0.04398007318377495 + 10.0 * 9.017465591430664
Epoch 2840, val loss: 0.9420621395111084
Epoch 2850, training loss: 90.21504974365234 = 0.043405450880527496 + 10.0 * 9.01716423034668
Epoch 2850, val loss: 0.9462738037109375
Epoch 2860, training loss: 90.22168731689453 = 0.04284246638417244 + 10.0 * 9.017885208129883
Epoch 2860, val loss: 0.950721025466919
Epoch 2870, training loss: 90.22081756591797 = 0.04228251054883003 + 10.0 * 9.017853736877441
Epoch 2870, val loss: 0.9550381898880005
Epoch 2880, training loss: 90.2293472290039 = 0.04173226282000542 + 10.0 * 9.01876163482666
Epoch 2880, val loss: 0.9590997695922852
Epoch 2890, training loss: 90.21510314941406 = 0.04118705913424492 + 10.0 * 9.0173921585083
Epoch 2890, val loss: 0.9639277458190918
Epoch 2900, training loss: 90.21273803710938 = 0.04064733535051346 + 10.0 * 9.01720905303955
Epoch 2900, val loss: 0.9682089686393738
Epoch 2910, training loss: 90.23008728027344 = 0.04012708738446236 + 10.0 * 9.01899528503418
Epoch 2910, val loss: 0.9727885723114014
Epoch 2920, training loss: 90.21572875976562 = 0.039601318538188934 + 10.0 * 9.01761245727539
Epoch 2920, val loss: 0.9771909713745117
Epoch 2930, training loss: 90.20899200439453 = 0.03908707574009895 + 10.0 * 9.016990661621094
Epoch 2930, val loss: 0.9815114736557007
Epoch 2940, training loss: 90.21605682373047 = 0.038583893328905106 + 10.0 * 9.01774787902832
Epoch 2940, val loss: 0.9859424233436584
Epoch 2950, training loss: 90.20545196533203 = 0.038079649209976196 + 10.0 * 9.01673698425293
Epoch 2950, val loss: 0.9900259375572205
Epoch 2960, training loss: 90.20478820800781 = 0.03758755326271057 + 10.0 * 9.016719818115234
Epoch 2960, val loss: 0.9943325519561768
Epoch 2970, training loss: 90.20491027832031 = 0.03710728511214256 + 10.0 * 9.016779899597168
Epoch 2970, val loss: 0.9990062713623047
Epoch 2980, training loss: 90.21971130371094 = 0.036637190729379654 + 10.0 * 9.01830768585205
Epoch 2980, val loss: 1.0031665563583374
Epoch 2990, training loss: 90.20714569091797 = 0.03615902364253998 + 10.0 * 9.017099380493164
Epoch 2990, val loss: 1.007422685623169
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8493
Overall ASR: 0.6917
Flip ASR: 0.6158/1554 nodes
./selected_nodes/Pubmed/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.26 GiB already allocated; 251.69 MiB free; 5.49 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.3 der2:0.3 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 487.69 MiB free; 5.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.3 der2:0.3 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 527.69 MiB free; 5.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.3 der2:0.3 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 487.69 MiB free; 5.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.3 der2:0.3 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 483.69 MiB free; 5.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.3 der2:0.4 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 174, in semi_loss
    refl_sim = f(self.sim(z1, z1))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 4.20 GiB already allocated; 765.69 MiB free; 4.34 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.3 der2:0.4 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 174, in semi_loss
    refl_sim = f(self.sim(z1, z1))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 4.20 GiB already allocated; 765.69 MiB free; 4.34 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.3 der2:0.4 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 174, in semi_loss
    refl_sim = f(self.sim(z1, z1))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 4.20 GiB already allocated; 487.69 MiB free; 4.34 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.3 der2:0.4 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 174, in semi_loss
    refl_sim = f(self.sim(z1, z1))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 4.20 GiB already allocated; 487.69 MiB free; 4.34 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.3 der2:0.4 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 174, in semi_loss
    refl_sim = f(self.sim(z1, z1))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 4.20 GiB already allocated; 487.69 MiB free; 4.34 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.3 der2:0.4 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 211, in loss
    l1 = self.semi_loss(h1, h2)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 3.24 GiB already allocated; 883.69 MiB free; 3.29 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.4 der2:0 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 271.69 MiB free; 5.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.4 der2:0 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 271.69 MiB free; 5.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.4 der2:0 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 271.69 MiB free; 5.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.4 der2:0 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 104.70272827148438 = 1.1113104820251465 + 10.0 * 10.35914134979248
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 319, in _train_with_val
    loss.backward()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 6.11 GiB already allocated; 465.69 MiB free; 6.43 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.4 der2:0 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 104.6896743774414 = 1.098863959312439 + 10.0 * 10.359081268310547
Epoch 0, val loss: 1.0983405113220215
Epoch 10, training loss: 104.6429443359375 = 1.087831735610962 + 10.0 * 10.355511665344238
Epoch 10, val loss: 1.0871169567108154
Epoch 20, training loss: 104.0107650756836 = 1.0736018419265747 + 10.0 * 10.293716430664062
Epoch 20, val loss: 1.0728352069854736
Epoch 30, training loss: 99.79544830322266 = 1.0606416463851929 + 10.0 * 9.873480796813965
Epoch 30, val loss: 1.0599017143249512
Epoch 40, training loss: 96.46804809570312 = 1.0472067594528198 + 10.0 * 9.542084693908691
Epoch 40, val loss: 1.045989751815796
Epoch 50, training loss: 95.29182434082031 = 1.0285850763320923 + 10.0 * 9.426323890686035
Epoch 50, val loss: 1.0272314548492432
Epoch 60, training loss: 94.80694580078125 = 1.0098686218261719 + 10.0 * 9.379708290100098
Epoch 60, val loss: 1.0089308023452759
Epoch 70, training loss: 94.43045806884766 = 0.9929286241531372 + 10.0 * 9.34375286102295
Epoch 70, val loss: 0.9923349022865295
Epoch 80, training loss: 93.87349700927734 = 0.9774258136749268 + 10.0 * 9.289607048034668
Epoch 80, val loss: 0.9772118330001831
Epoch 90, training loss: 93.31339263916016 = 0.9629346132278442 + 10.0 * 9.235045433044434
Epoch 90, val loss: 0.9631285667419434
Epoch 100, training loss: 93.01078796386719 = 0.9461989998817444 + 10.0 * 9.206459045410156
Epoch 100, val loss: 0.9466075897216797
Epoch 110, training loss: 92.83187103271484 = 0.9239941835403442 + 10.0 * 9.190787315368652
Epoch 110, val loss: 0.9248148798942566
Epoch 120, training loss: 92.70082092285156 = 0.8958910703659058 + 10.0 * 9.180493354797363
Epoch 120, val loss: 0.8972100019454956
Epoch 130, training loss: 92.56777954101562 = 0.8634859323501587 + 10.0 * 9.170429229736328
Epoch 130, val loss: 0.8658545017242432
Epoch 140, training loss: 92.44165802001953 = 0.8295831680297852 + 10.0 * 9.16120719909668
Epoch 140, val loss: 0.833330512046814
Epoch 150, training loss: 92.30461883544922 = 0.7941107153892517 + 10.0 * 9.151050567626953
Epoch 150, val loss: 0.8000190258026123
Epoch 160, training loss: 92.19611358642578 = 0.7565631866455078 + 10.0 * 9.14395523071289
Epoch 160, val loss: 0.7649348378181458
Epoch 170, training loss: 92.10646057128906 = 0.7171507477760315 + 10.0 * 9.138931274414062
Epoch 170, val loss: 0.7285727262496948
Epoch 180, training loss: 92.02674102783203 = 0.677370011806488 + 10.0 * 9.134937286376953
Epoch 180, val loss: 0.6924759149551392
Epoch 190, training loss: 91.9448471069336 = 0.6392316818237305 + 10.0 * 9.130561828613281
Epoch 190, val loss: 0.6585586667060852
Epoch 200, training loss: 91.8702163696289 = 0.6039628982543945 + 10.0 * 9.126625061035156
Epoch 200, val loss: 0.6278499364852905
Epoch 210, training loss: 91.7956314086914 = 0.5720579624176025 + 10.0 * 9.122357368469238
Epoch 210, val loss: 0.6007541418075562
Epoch 220, training loss: 91.76446533203125 = 0.5434737205505371 + 10.0 * 9.122098922729492
Epoch 220, val loss: 0.5772397518157959
Epoch 230, training loss: 91.70328521728516 = 0.5186253190040588 + 10.0 * 9.1184663772583
Epoch 230, val loss: 0.5570774674415588
Epoch 240, training loss: 91.6235580444336 = 0.4971456229686737 + 10.0 * 9.112641334533691
Epoch 240, val loss: 0.5401102900505066
Epoch 250, training loss: 91.5782241821289 = 0.4784914553165436 + 10.0 * 9.109972953796387
Epoch 250, val loss: 0.525617241859436
Epoch 260, training loss: 91.53437042236328 = 0.46212539076805115 + 10.0 * 9.107224464416504
Epoch 260, val loss: 0.5134255290031433
Epoch 270, training loss: 91.49857330322266 = 0.44765716791152954 + 10.0 * 9.10509204864502
Epoch 270, val loss: 0.5026901960372925
Epoch 280, training loss: 91.46817016601562 = 0.4348527491092682 + 10.0 * 9.103331565856934
Epoch 280, val loss: 0.4933817386627197
Epoch 290, training loss: 91.40933990478516 = 0.42346450686454773 + 10.0 * 9.098587989807129
Epoch 290, val loss: 0.4852775037288666
Epoch 300, training loss: 91.43685150146484 = 0.4132775664329529 + 10.0 * 9.102357864379883
Epoch 300, val loss: 0.4781854450702667
Epoch 310, training loss: 91.34942626953125 = 0.4040197432041168 + 10.0 * 9.0945405960083
Epoch 310, val loss: 0.47177162766456604
Epoch 320, training loss: 91.30903625488281 = 0.39568349719047546 + 10.0 * 9.09133529663086
Epoch 320, val loss: 0.46603116393089294
Epoch 330, training loss: 91.28565979003906 = 0.3880334496498108 + 10.0 * 9.089762687683105
Epoch 330, val loss: 0.46091538667678833
Epoch 340, training loss: 91.29606628417969 = 0.38093501329421997 + 10.0 * 9.091512680053711
Epoch 340, val loss: 0.4563169777393341
Epoch 350, training loss: 91.23887634277344 = 0.37438729405403137 + 10.0 * 9.086448669433594
Epoch 350, val loss: 0.4521423578262329
Epoch 360, training loss: 91.21633911132812 = 0.3683626353740692 + 10.0 * 9.084797859191895
Epoch 360, val loss: 0.44837483763694763
Epoch 370, training loss: 91.21070861816406 = 0.3627135157585144 + 10.0 * 9.084798812866211
Epoch 370, val loss: 0.4449325501918793
Epoch 380, training loss: 91.18685913085938 = 0.3574223220348358 + 10.0 * 9.0829439163208
Epoch 380, val loss: 0.44173020124435425
Epoch 390, training loss: 91.1598892211914 = 0.35246801376342773 + 10.0 * 9.080741882324219
Epoch 390, val loss: 0.43890380859375
Epoch 400, training loss: 91.16421508789062 = 0.3477577865123749 + 10.0 * 9.081645965576172
Epoch 400, val loss: 0.4363761842250824
Epoch 410, training loss: 91.12824249267578 = 0.3432551324367523 + 10.0 * 9.078498840332031
Epoch 410, val loss: 0.433817982673645
Epoch 420, training loss: 91.11022186279297 = 0.33897149562835693 + 10.0 * 9.07712459564209
Epoch 420, val loss: 0.43142277002334595
Epoch 430, training loss: 91.09309387207031 = 0.3348708748817444 + 10.0 * 9.075822830200195
Epoch 430, val loss: 0.4292820692062378
Epoch 440, training loss: 91.07967376708984 = 0.3309101164340973 + 10.0 * 9.07487678527832
Epoch 440, val loss: 0.42726442217826843
Epoch 450, training loss: 91.06558227539062 = 0.32712408900260925 + 10.0 * 9.073845863342285
Epoch 450, val loss: 0.42529645562171936
Epoch 460, training loss: 91.05316925048828 = 0.32351037859916687 + 10.0 * 9.072965621948242
Epoch 460, val loss: 0.42356085777282715
Epoch 470, training loss: 91.0386962890625 = 0.3200061023235321 + 10.0 * 9.071868896484375
Epoch 470, val loss: 0.4219091832637787
Epoch 480, training loss: 91.08240509033203 = 0.3165842890739441 + 10.0 * 9.076581954956055
Epoch 480, val loss: 0.4203476309776306
Epoch 490, training loss: 91.0353012084961 = 0.31326839327812195 + 10.0 * 9.072202682495117
Epoch 490, val loss: 0.4188382923603058
Epoch 500, training loss: 91.01412963867188 = 0.3100941479206085 + 10.0 * 9.070403099060059
Epoch 500, val loss: 0.41747191548347473
Epoch 510, training loss: 91.00650787353516 = 0.30701205134391785 + 10.0 * 9.06994915008545
Epoch 510, val loss: 0.41625121235847473
Epoch 520, training loss: 90.99191284179688 = 0.3040303885936737 + 10.0 * 9.068788528442383
Epoch 520, val loss: 0.4149802327156067
Epoch 530, training loss: 90.9743881225586 = 0.301138699054718 + 10.0 * 9.0673246383667
Epoch 530, val loss: 0.4138302206993103
Epoch 540, training loss: 90.96398162841797 = 0.2983092665672302 + 10.0 * 9.066567420959473
Epoch 540, val loss: 0.4127825200557709
Epoch 550, training loss: 91.0560302734375 = 0.29553231596946716 + 10.0 * 9.0760498046875
Epoch 550, val loss: 0.4117799401283264
Epoch 560, training loss: 90.97406768798828 = 0.2928345501422882 + 10.0 * 9.068123817443848
Epoch 560, val loss: 0.4109044075012207
Epoch 570, training loss: 90.94470977783203 = 0.2902510166168213 + 10.0 * 9.065445899963379
Epoch 570, val loss: 0.4101037085056305
Epoch 580, training loss: 90.92758178710938 = 0.28773465752601624 + 10.0 * 9.063984870910645
Epoch 580, val loss: 0.40940484404563904
Epoch 590, training loss: 90.91932678222656 = 0.28525397181510925 + 10.0 * 9.063406944274902
Epoch 590, val loss: 0.4087216556072235
Epoch 600, training loss: 90.93177795410156 = 0.28280457854270935 + 10.0 * 9.064897537231445
Epoch 600, val loss: 0.40817978978157043
Epoch 610, training loss: 90.94425201416016 = 0.28039997816085815 + 10.0 * 9.066385269165039
Epoch 610, val loss: 0.40755778551101685
Epoch 620, training loss: 90.90644073486328 = 0.2780798673629761 + 10.0 * 9.062836647033691
Epoch 620, val loss: 0.40704095363616943
Epoch 630, training loss: 90.88934326171875 = 0.275816410779953 + 10.0 * 9.061352729797363
Epoch 630, val loss: 0.40665403008461
Epoch 640, training loss: 90.88408660888672 = 0.2735825181007385 + 10.0 * 9.061050415039062
Epoch 640, val loss: 0.40630367398262024
Epoch 650, training loss: 90.88539123535156 = 0.27138644456863403 + 10.0 * 9.061400413513184
Epoch 650, val loss: 0.406098335981369
Epoch 660, training loss: 90.86395263671875 = 0.26924893260002136 + 10.0 * 9.059470176696777
Epoch 660, val loss: 0.4057508409023285
Epoch 670, training loss: 90.8550033569336 = 0.26716384291648865 + 10.0 * 9.058783531188965
Epoch 670, val loss: 0.40561923384666443
Epoch 680, training loss: 90.84748840332031 = 0.26509958505630493 + 10.0 * 9.058238983154297
Epoch 680, val loss: 0.4054773151874542
Epoch 690, training loss: 90.84027099609375 = 0.26304465532302856 + 10.0 * 9.057722091674805
Epoch 690, val loss: 0.40538379549980164
Epoch 700, training loss: 90.91057586669922 = 0.26100924611091614 + 10.0 * 9.064956665039062
Epoch 700, val loss: 0.40542951226234436
Epoch 710, training loss: 90.82861328125 = 0.2590043544769287 + 10.0 * 9.056961059570312
Epoch 710, val loss: 0.40530189871788025
Epoch 720, training loss: 90.81953430175781 = 0.25705060362815857 + 10.0 * 9.056248664855957
Epoch 720, val loss: 0.40533262491226196
Epoch 730, training loss: 90.81233215332031 = 0.25511467456817627 + 10.0 * 9.0557222366333
Epoch 730, val loss: 0.4054163098335266
Epoch 740, training loss: 90.80601501464844 = 0.25318190455436707 + 10.0 * 9.055283546447754
Epoch 740, val loss: 0.4055180847644806
Epoch 750, training loss: 90.84307861328125 = 0.2512572705745697 + 10.0 * 9.059182167053223
Epoch 750, val loss: 0.40565937757492065
Epoch 760, training loss: 90.81444549560547 = 0.24935081601142883 + 10.0 * 9.056509017944336
Epoch 760, val loss: 0.405841201543808
Epoch 770, training loss: 90.79029083251953 = 0.24747000634670258 + 10.0 * 9.054282188415527
Epoch 770, val loss: 0.4060707092285156
Epoch 780, training loss: 90.78834533691406 = 0.24559681117534637 + 10.0 * 9.054274559020996
Epoch 780, val loss: 0.406340092420578
Epoch 790, training loss: 90.7895736694336 = 0.2437327802181244 + 10.0 * 9.054583549499512
Epoch 790, val loss: 0.40665772557258606
Epoch 800, training loss: 90.7723159790039 = 0.2418862134218216 + 10.0 * 9.0530424118042
Epoch 800, val loss: 0.4069441854953766
Epoch 810, training loss: 90.7673568725586 = 0.24005027115345 + 10.0 * 9.052730560302734
Epoch 810, val loss: 0.4073356091976166
Epoch 820, training loss: 90.76811981201172 = 0.23821291327476501 + 10.0 * 9.052990913391113
Epoch 820, val loss: 0.4077383577823639
Epoch 830, training loss: 90.7586669921875 = 0.23637622594833374 + 10.0 * 9.052228927612305
Epoch 830, val loss: 0.4081181287765503
Epoch 840, training loss: 90.74964141845703 = 0.23454836010932922 + 10.0 * 9.051508903503418
Epoch 840, val loss: 0.40865352749824524
Epoch 850, training loss: 90.75064086914062 = 0.23272687196731567 + 10.0 * 9.051791191101074
Epoch 850, val loss: 0.40911665558815
Epoch 860, training loss: 90.74737548828125 = 0.23090825974941254 + 10.0 * 9.05164623260498
Epoch 860, val loss: 0.4097230136394501
Epoch 870, training loss: 90.73509216308594 = 0.22909383475780487 + 10.0 * 9.050600051879883
Epoch 870, val loss: 0.41028282046318054
Epoch 880, training loss: 90.72928619384766 = 0.22727525234222412 + 10.0 * 9.050201416015625
Epoch 880, val loss: 0.41097813844680786
Epoch 890, training loss: 90.75279998779297 = 0.22545841336250305 + 10.0 * 9.052734375
Epoch 890, val loss: 0.41169601678848267
Epoch 900, training loss: 90.72652435302734 = 0.223638117313385 + 10.0 * 9.050288200378418
Epoch 900, val loss: 0.41241219639778137
Epoch 910, training loss: 90.7238998413086 = 0.22182346880435944 + 10.0 * 9.050207138061523
Epoch 910, val loss: 0.4132841229438782
Epoch 920, training loss: 90.7134017944336 = 0.22000658512115479 + 10.0 * 9.049339294433594
Epoch 920, val loss: 0.4139597415924072
Epoch 930, training loss: 90.72222900390625 = 0.2181931436061859 + 10.0 * 9.050403594970703
Epoch 930, val loss: 0.4149571359157562
Epoch 940, training loss: 90.70012664794922 = 0.2163834273815155 + 10.0 * 9.04837417602539
Epoch 940, val loss: 0.4157480001449585
Epoch 950, training loss: 90.6944808959961 = 0.21457472443580627 + 10.0 * 9.047990798950195
Epoch 950, val loss: 0.41672855615615845
Epoch 960, training loss: 90.68992614746094 = 0.21275638043880463 + 10.0 * 9.047717094421387
Epoch 960, val loss: 0.4176692068576813
Epoch 970, training loss: 90.70952606201172 = 0.21093441545963287 + 10.0 * 9.049859046936035
Epoch 970, val loss: 0.41876766085624695
Epoch 980, training loss: 90.68941497802734 = 0.20910599827766418 + 10.0 * 9.048030853271484
Epoch 980, val loss: 0.41984304785728455
Epoch 990, training loss: 90.68670654296875 = 0.20728179812431335 + 10.0 * 9.047942161560059
Epoch 990, val loss: 0.42098501324653625
Epoch 1000, training loss: 90.6830062866211 = 0.20545153319835663 + 10.0 * 9.047755241394043
Epoch 1000, val loss: 0.42215240001678467
Epoch 1010, training loss: 90.67333984375 = 0.2036140263080597 + 10.0 * 9.046972274780273
Epoch 1010, val loss: 0.4233396649360657
Epoch 1020, training loss: 90.67620849609375 = 0.20177297294139862 + 10.0 * 9.047443389892578
Epoch 1020, val loss: 0.42462414503097534
Epoch 1030, training loss: 90.66400909423828 = 0.19992586970329285 + 10.0 * 9.046407699584961
Epoch 1030, val loss: 0.42596444487571716
Epoch 1040, training loss: 90.65670776367188 = 0.19807757437229156 + 10.0 * 9.045863151550293
Epoch 1040, val loss: 0.42733871936798096
Epoch 1050, training loss: 90.655029296875 = 0.19622470438480377 + 10.0 * 9.045880317687988
Epoch 1050, val loss: 0.42879945039749146
Epoch 1060, training loss: 90.65277862548828 = 0.19436806440353394 + 10.0 * 9.045841217041016
Epoch 1060, val loss: 0.43029317259788513
Epoch 1070, training loss: 90.66607666015625 = 0.19251281023025513 + 10.0 * 9.047356605529785
Epoch 1070, val loss: 0.4318012297153473
Epoch 1080, training loss: 90.64070892333984 = 0.1906593292951584 + 10.0 * 9.045004844665527
Epoch 1080, val loss: 0.43329697847366333
Epoch 1090, training loss: 90.633056640625 = 0.18880631029605865 + 10.0 * 9.044425010681152
Epoch 1090, val loss: 0.43485215306282043
Epoch 1100, training loss: 90.62986755371094 = 0.18694442510604858 + 10.0 * 9.044292449951172
Epoch 1100, val loss: 0.4364446699619293
Epoch 1110, training loss: 90.65372467041016 = 0.18508590757846832 + 10.0 * 9.046863555908203
Epoch 1110, val loss: 0.4381624162197113
Epoch 1120, training loss: 90.6253433227539 = 0.1832166314125061 + 10.0 * 9.044212341308594
Epoch 1120, val loss: 0.43980076909065247
Epoch 1130, training loss: 90.61779022216797 = 0.18135181069374084 + 10.0 * 9.043643951416016
Epoch 1130, val loss: 0.44153985381126404
Epoch 1140, training loss: 90.6204605102539 = 0.17948251962661743 + 10.0 * 9.044097900390625
Epoch 1140, val loss: 0.44335195422172546
Epoch 1150, training loss: 90.61660766601562 = 0.17760907113552094 + 10.0 * 9.043899536132812
Epoch 1150, val loss: 0.44522643089294434
Epoch 1160, training loss: 90.61590576171875 = 0.17573893070220947 + 10.0 * 9.04401683807373
Epoch 1160, val loss: 0.44708511233329773
Epoch 1170, training loss: 90.60519409179688 = 0.17386867105960846 + 10.0 * 9.043132781982422
Epoch 1170, val loss: 0.44905179738998413
Epoch 1180, training loss: 90.60276794433594 = 0.1719939410686493 + 10.0 * 9.04307746887207
Epoch 1180, val loss: 0.4510348439216614
Epoch 1190, training loss: 90.60197448730469 = 0.17011412978172302 + 10.0 * 9.04318618774414
Epoch 1190, val loss: 0.4530205726623535
Epoch 1200, training loss: 90.60025787353516 = 0.1682339757680893 + 10.0 * 9.04320240020752
Epoch 1200, val loss: 0.45511701703071594
Epoch 1210, training loss: 90.59371185302734 = 0.16635164618492126 + 10.0 * 9.042736053466797
Epoch 1210, val loss: 0.45725980401039124
Epoch 1220, training loss: 90.59471130371094 = 0.16447746753692627 + 10.0 * 9.043024063110352
Epoch 1220, val loss: 0.45930126309394836
Epoch 1230, training loss: 90.57843780517578 = 0.1625971645116806 + 10.0 * 9.041584014892578
Epoch 1230, val loss: 0.4614245593547821
Epoch 1240, training loss: 90.59346771240234 = 0.16072572767734528 + 10.0 * 9.04327392578125
Epoch 1240, val loss: 0.46346789598464966
Epoch 1250, training loss: 90.57479858398438 = 0.15885554254055023 + 10.0 * 9.041593551635742
Epoch 1250, val loss: 0.4659719169139862
Epoch 1260, training loss: 90.56929779052734 = 0.1569908857345581 + 10.0 * 9.041231155395508
Epoch 1260, val loss: 0.46799302101135254
Epoch 1270, training loss: 90.56107330322266 = 0.1551242172718048 + 10.0 * 9.040595054626465
Epoch 1270, val loss: 0.47040101885795593
Epoch 1280, training loss: 90.55638885498047 = 0.15325580537319183 + 10.0 * 9.040312767028809
Epoch 1280, val loss: 0.4726870059967041
Epoch 1290, training loss: 90.58734130859375 = 0.15139491856098175 + 10.0 * 9.043594360351562
Epoch 1290, val loss: 0.4751368463039398
Epoch 1300, training loss: 90.57440948486328 = 0.14953696727752686 + 10.0 * 9.042487144470215
Epoch 1300, val loss: 0.47749802470207214
Epoch 1310, training loss: 90.54774475097656 = 0.14768677949905396 + 10.0 * 9.040005683898926
Epoch 1310, val loss: 0.47999435663223267
Epoch 1320, training loss: 90.56890106201172 = 0.14585033059120178 + 10.0 * 9.042304992675781
Epoch 1320, val loss: 0.482636034488678
Epoch 1330, training loss: 90.5425033569336 = 0.14400586485862732 + 10.0 * 9.039850234985352
Epoch 1330, val loss: 0.4851512014865875
Epoch 1340, training loss: 90.53633880615234 = 0.14217443764209747 + 10.0 * 9.039416313171387
Epoch 1340, val loss: 0.48784324526786804
Epoch 1350, training loss: 90.53150177001953 = 0.1403355598449707 + 10.0 * 9.039116859436035
Epoch 1350, val loss: 0.4906120002269745
Epoch 1360, training loss: 90.52934265136719 = 0.13850298523902893 + 10.0 * 9.039083480834961
Epoch 1360, val loss: 0.49343201518058777
Epoch 1370, training loss: 90.55531311035156 = 0.13668200373649597 + 10.0 * 9.041863441467285
Epoch 1370, val loss: 0.49638161063194275
Epoch 1380, training loss: 90.53932189941406 = 0.13485087454319 + 10.0 * 9.040447235107422
Epoch 1380, val loss: 0.4991975724697113
Epoch 1390, training loss: 90.525390625 = 0.13302892446517944 + 10.0 * 9.039236068725586
Epoch 1390, val loss: 0.502061665058136
Epoch 1400, training loss: 90.5177230834961 = 0.1312154084444046 + 10.0 * 9.038650512695312
Epoch 1400, val loss: 0.5050157308578491
Epoch 1410, training loss: 90.51705932617188 = 0.12940779328346252 + 10.0 * 9.038764953613281
Epoch 1410, val loss: 0.5080407857894897
Epoch 1420, training loss: 90.52619934082031 = 0.1276116520166397 + 10.0 * 9.0398588180542
Epoch 1420, val loss: 0.5111962556838989
Epoch 1430, training loss: 90.51293182373047 = 0.1258222758769989 + 10.0 * 9.038710594177246
Epoch 1430, val loss: 0.514316976070404
Epoch 1440, training loss: 90.50196075439453 = 0.12404211610555649 + 10.0 * 9.037792205810547
Epoch 1440, val loss: 0.5174756646156311
Epoch 1450, training loss: 90.50293731689453 = 0.12226882576942444 + 10.0 * 9.038066864013672
Epoch 1450, val loss: 0.5207949280738831
Epoch 1460, training loss: 90.51518249511719 = 0.12050551921129227 + 10.0 * 9.039467811584473
Epoch 1460, val loss: 0.5242162346839905
Epoch 1470, training loss: 90.50430297851562 = 0.11874835193157196 + 10.0 * 9.038555145263672
Epoch 1470, val loss: 0.5276910066604614
Epoch 1480, training loss: 90.49061584472656 = 0.1169971451163292 + 10.0 * 9.037362098693848
Epoch 1480, val loss: 0.5312029123306274
Epoch 1490, training loss: 90.48439025878906 = 0.11524804681539536 + 10.0 * 9.036913871765137
Epoch 1490, val loss: 0.5347386002540588
Epoch 1500, training loss: 90.49383544921875 = 0.11351343244314194 + 10.0 * 9.038032531738281
Epoch 1500, val loss: 0.5384721159934998
Epoch 1510, training loss: 90.47632598876953 = 0.11177633702754974 + 10.0 * 9.036455154418945
Epoch 1510, val loss: 0.5420196652412415
Epoch 1520, training loss: 90.48856353759766 = 0.11005406826734543 + 10.0 * 9.037851333618164
Epoch 1520, val loss: 0.5457444787025452
Epoch 1530, training loss: 90.47504425048828 = 0.10834015905857086 + 10.0 * 9.036670684814453
Epoch 1530, val loss: 0.5497360229492188
Epoch 1540, training loss: 90.46617126464844 = 0.10663939267396927 + 10.0 * 9.035953521728516
Epoch 1540, val loss: 0.553554356098175
Epoch 1550, training loss: 90.46173095703125 = 0.10494732111692429 + 10.0 * 9.035677909851074
Epoch 1550, val loss: 0.5575765371322632
Epoch 1560, training loss: 90.47225952148438 = 0.10327346622943878 + 10.0 * 9.036898612976074
Epoch 1560, val loss: 0.5615794062614441
Epoch 1570, training loss: 90.46135711669922 = 0.1015998125076294 + 10.0 * 9.035975456237793
Epoch 1570, val loss: 0.5657616257667542
Epoch 1580, training loss: 90.45343017578125 = 0.09994173049926758 + 10.0 * 9.035348892211914
Epoch 1580, val loss: 0.5698683857917786
Epoch 1590, training loss: 90.450439453125 = 0.09829766303300858 + 10.0 * 9.0352144241333
Epoch 1590, val loss: 0.5742825269699097
Epoch 1600, training loss: 90.4803237915039 = 0.09668102115392685 + 10.0 * 9.03836441040039
Epoch 1600, val loss: 0.5785914659500122
Epoch 1610, training loss: 90.45423126220703 = 0.09504923969507217 + 10.0 * 9.035918235778809
Epoch 1610, val loss: 0.583044707775116
Epoch 1620, training loss: 90.44734191894531 = 0.09344293177127838 + 10.0 * 9.03538990020752
Epoch 1620, val loss: 0.5875617861747742
Epoch 1630, training loss: 90.4541015625 = 0.09185852110385895 + 10.0 * 9.036224365234375
Epoch 1630, val loss: 0.5922184586524963
Epoch 1640, training loss: 90.43414306640625 = 0.09027505666017532 + 10.0 * 9.03438663482666
Epoch 1640, val loss: 0.5967966914176941
Epoch 1650, training loss: 90.43736267089844 = 0.08871752768754959 + 10.0 * 9.03486442565918
Epoch 1650, val loss: 0.6014919877052307
Epoch 1660, training loss: 90.44804382324219 = 0.08717790246009827 + 10.0 * 9.036086082458496
Epoch 1660, val loss: 0.606360673904419
Epoch 1670, training loss: 90.42794036865234 = 0.0856512039899826 + 10.0 * 9.034229278564453
Epoch 1670, val loss: 0.6113011240959167
Epoch 1680, training loss: 90.42483520507812 = 0.08413972705602646 + 10.0 * 9.034070014953613
Epoch 1680, val loss: 0.6163502931594849
Epoch 1690, training loss: 90.46786499023438 = 0.0826643779873848 + 10.0 * 9.038519859313965
Epoch 1690, val loss: 0.6216042041778564
Epoch 1700, training loss: 90.42618560791016 = 0.08117419481277466 + 10.0 * 9.034501075744629
Epoch 1700, val loss: 0.6261382699012756
Epoch 1710, training loss: 90.41211700439453 = 0.07971619814634323 + 10.0 * 9.033239364624023
Epoch 1710, val loss: 0.6313545107841492
Epoch 1720, training loss: 90.40863037109375 = 0.07827947288751602 + 10.0 * 9.033035278320312
Epoch 1720, val loss: 0.6366137862205505
Epoch 1730, training loss: 90.41190338134766 = 0.07685676217079163 + 10.0 * 9.033504486083984
Epoch 1730, val loss: 0.6418377161026001
Epoch 1740, training loss: 90.42684173583984 = 0.07545071095228195 + 10.0 * 9.035139083862305
Epoch 1740, val loss: 0.6472395658493042
Epoch 1750, training loss: 90.40480041503906 = 0.07405994087457657 + 10.0 * 9.033074378967285
Epoch 1750, val loss: 0.6523140668869019
Epoch 1760, training loss: 90.40426635742188 = 0.07268651574850082 + 10.0 * 9.033158302307129
Epoch 1760, val loss: 0.6578708291053772
Epoch 1770, training loss: 90.41082000732422 = 0.07134223729372025 + 10.0 * 9.033947944641113
Epoch 1770, val loss: 0.6630315184593201
Epoch 1780, training loss: 90.40177154541016 = 0.07000488042831421 + 10.0 * 9.03317642211914
Epoch 1780, val loss: 0.6686536073684692
Epoch 1790, training loss: 90.39386749267578 = 0.0686979666352272 + 10.0 * 9.032517433166504
Epoch 1790, val loss: 0.6742629408836365
Epoch 1800, training loss: 90.3911361694336 = 0.06740740686655045 + 10.0 * 9.032373428344727
Epoch 1800, val loss: 0.680013120174408
Epoch 1810, training loss: 90.41339874267578 = 0.06614693254232407 + 10.0 * 9.034725189208984
Epoch 1810, val loss: 0.6853932738304138
Epoch 1820, training loss: 90.38970947265625 = 0.06489747017621994 + 10.0 * 9.03248119354248
Epoch 1820, val loss: 0.6912459135055542
Epoch 1830, training loss: 90.38299560546875 = 0.06366803497076035 + 10.0 * 9.031932830810547
Epoch 1830, val loss: 0.6968643069267273
Epoch 1840, training loss: 90.3851089477539 = 0.062457647174596786 + 10.0 * 9.032265663146973
Epoch 1840, val loss: 0.7025735378265381
Epoch 1850, training loss: 90.39326477050781 = 0.06127277389168739 + 10.0 * 9.033199310302734
Epoch 1850, val loss: 0.708207368850708
Epoch 1860, training loss: 90.38252258300781 = 0.060107070952653885 + 10.0 * 9.032241821289062
Epoch 1860, val loss: 0.713703989982605
Epoch 1870, training loss: 90.37519073486328 = 0.05895727127790451 + 10.0 * 9.031622886657715
Epoch 1870, val loss: 0.7195755839347839
Epoch 1880, training loss: 90.37476348876953 = 0.057833533734083176 + 10.0 * 9.031693458557129
Epoch 1880, val loss: 0.7254053950309753
Epoch 1890, training loss: 90.38835144042969 = 0.056734971702098846 + 10.0 * 9.033162117004395
Epoch 1890, val loss: 0.7312484383583069
Epoch 1900, training loss: 90.3736801147461 = 0.05565149709582329 + 10.0 * 9.031803131103516
Epoch 1900, val loss: 0.7363712787628174
Epoch 1910, training loss: 90.3638687133789 = 0.054574526846408844 + 10.0 * 9.030929565429688
Epoch 1910, val loss: 0.7424322962760925
Epoch 1920, training loss: 90.35758972167969 = 0.053526587784290314 + 10.0 * 9.03040599822998
Epoch 1920, val loss: 0.747940719127655
Epoch 1930, training loss: 90.3623275756836 = 0.052500348538160324 + 10.0 * 9.030982971191406
Epoch 1930, val loss: 0.753677487373352
Epoch 1940, training loss: 90.37165832519531 = 0.05149216577410698 + 10.0 * 9.03201675415039
Epoch 1940, val loss: 0.7595299482345581
Epoch 1950, training loss: 90.35289001464844 = 0.05049455165863037 + 10.0 * 9.030240058898926
Epoch 1950, val loss: 0.7650616765022278
Epoch 1960, training loss: 90.3652572631836 = 0.04952426999807358 + 10.0 * 9.031573295593262
Epoch 1960, val loss: 0.7708847522735596
Epoch 1970, training loss: 90.35701751708984 = 0.048564471304416656 + 10.0 * 9.030845642089844
Epoch 1970, val loss: 0.7764936089515686
Epoch 1980, training loss: 90.35408782958984 = 0.04762957990169525 + 10.0 * 9.030645370483398
Epoch 1980, val loss: 0.7822356820106506
Epoch 1990, training loss: 90.34580993652344 = 0.04671075940132141 + 10.0 * 9.02991008758545
Epoch 1990, val loss: 0.7875531911849976
Epoch 2000, training loss: 90.33963012695312 = 0.04581143707036972 + 10.0 * 9.02938175201416
Epoch 2000, val loss: 0.7933027148246765
Epoch 2010, training loss: 90.34119415283203 = 0.04493604227900505 + 10.0 * 9.02962589263916
Epoch 2010, val loss: 0.798833966255188
Epoch 2020, training loss: 90.36354064941406 = 0.044080447405576706 + 10.0 * 9.031946182250977
Epoch 2020, val loss: 0.8043166399002075
Epoch 2030, training loss: 90.34097290039062 = 0.04322819784283638 + 10.0 * 9.02977466583252
Epoch 2030, val loss: 0.8102731108665466
Epoch 2040, training loss: 90.36139678955078 = 0.04240700602531433 + 10.0 * 9.031899452209473
Epoch 2040, val loss: 0.8155485391616821
Epoch 2050, training loss: 90.3358154296875 = 0.04159191623330116 + 10.0 * 9.02942180633545
Epoch 2050, val loss: 0.821309506893158
Epoch 2060, training loss: 90.3326644897461 = 0.04079998657107353 + 10.0 * 9.029186248779297
Epoch 2060, val loss: 0.8265535235404968
Epoch 2070, training loss: 90.34822845458984 = 0.04003224894404411 + 10.0 * 9.0308198928833
Epoch 2070, val loss: 0.8324391841888428
Epoch 2080, training loss: 90.32960510253906 = 0.039271049201488495 + 10.0 * 9.029033660888672
Epoch 2080, val loss: 0.8376561999320984
Epoch 2090, training loss: 90.33219146728516 = 0.038531962782144547 + 10.0 * 9.029366493225098
Epoch 2090, val loss: 0.8434924483299255
Epoch 2100, training loss: 90.32035827636719 = 0.03780620917677879 + 10.0 * 9.028255462646484
Epoch 2100, val loss: 0.8487038016319275
Epoch 2110, training loss: 90.3270034790039 = 0.037100087851285934 + 10.0 * 9.028989791870117
Epoch 2110, val loss: 0.8542322516441345
Epoch 2120, training loss: 90.3412094116211 = 0.036413416266441345 + 10.0 * 9.030479431152344
Epoch 2120, val loss: 0.8599664568901062
Epoch 2130, training loss: 90.3206558227539 = 0.035730473697185516 + 10.0 * 9.02849292755127
Epoch 2130, val loss: 0.865199625492096
Epoch 2140, training loss: 90.31941223144531 = 0.035067226737737656 + 10.0 * 9.028434753417969
Epoch 2140, val loss: 0.8704685568809509
Epoch 2150, training loss: 90.33155059814453 = 0.034426603466272354 + 10.0 * 9.029712677001953
Epoch 2150, val loss: 0.8758060336112976
Epoch 2160, training loss: 90.32096862792969 = 0.033792462199926376 + 10.0 * 9.028717994689941
Epoch 2160, val loss: 0.8815538883209229
Epoch 2170, training loss: 90.31163787841797 = 0.03317321464419365 + 10.0 * 9.027846336364746
Epoch 2170, val loss: 0.8865506649017334
Epoch 2180, training loss: 90.31364440917969 = 0.03257064148783684 + 10.0 * 9.028107643127441
Epoch 2180, val loss: 0.892054557800293
Epoch 2190, training loss: 90.3215103149414 = 0.031982649117708206 + 10.0 * 9.028952598571777
Epoch 2190, val loss: 0.8972467184066772
Epoch 2200, training loss: 90.30784606933594 = 0.031399961560964584 + 10.0 * 9.027644157409668
Epoch 2200, val loss: 0.9028927683830261
Epoch 2210, training loss: 90.30741119384766 = 0.03083406388759613 + 10.0 * 9.027658462524414
Epoch 2210, val loss: 0.908062756061554
Epoch 2220, training loss: 90.32283020019531 = 0.03028417006134987 + 10.0 * 9.029254913330078
Epoch 2220, val loss: 0.9133629202842712
Epoch 2230, training loss: 90.30814361572266 = 0.0297410748898983 + 10.0 * 9.027840614318848
Epoch 2230, val loss: 0.91820228099823
Epoch 2240, training loss: 90.29866790771484 = 0.029211129993200302 + 10.0 * 9.026945114135742
Epoch 2240, val loss: 0.9236209988594055
Epoch 2250, training loss: 90.31402587890625 = 0.028698202222585678 + 10.0 * 9.028532028198242
Epoch 2250, val loss: 0.9284762144088745
Epoch 2260, training loss: 90.30074310302734 = 0.028184855356812477 + 10.0 * 9.02725601196289
Epoch 2260, val loss: 0.9343480467796326
Epoch 2270, training loss: 90.29127502441406 = 0.027684271335601807 + 10.0 * 9.026358604431152
Epoch 2270, val loss: 0.9392026662826538
Epoch 2280, training loss: 90.28968811035156 = 0.02719856984913349 + 10.0 * 9.026248931884766
Epoch 2280, val loss: 0.9445351362228394
Epoch 2290, training loss: 90.29917907714844 = 0.02672761306166649 + 10.0 * 9.02724552154541
Epoch 2290, val loss: 0.9497658610343933
Epoch 2300, training loss: 90.29203033447266 = 0.02626543678343296 + 10.0 * 9.026576042175293
Epoch 2300, val loss: 0.9547291398048401
Epoch 2310, training loss: 90.29023742675781 = 0.025809073820710182 + 10.0 * 9.026442527770996
Epoch 2310, val loss: 0.9596844911575317
Epoch 2320, training loss: 90.29772186279297 = 0.02536957710981369 + 10.0 * 9.02723503112793
Epoch 2320, val loss: 0.9642944931983948
Epoch 2330, training loss: 90.28646087646484 = 0.024934135377407074 + 10.0 * 9.026152610778809
Epoch 2330, val loss: 0.9698511362075806
Epoch 2340, training loss: 90.29499053955078 = 0.024512166157364845 + 10.0 * 9.027048110961914
Epoch 2340, val loss: 0.9748656749725342
Epoch 2350, training loss: 90.29127502441406 = 0.024099454283714294 + 10.0 * 9.026717185974121
Epoch 2350, val loss: 0.979546308517456
Epoch 2360, training loss: 90.28419494628906 = 0.02369069494307041 + 10.0 * 9.026050567626953
Epoch 2360, val loss: 0.9843229055404663
Epoch 2370, training loss: 90.27672576904297 = 0.023292301222682 + 10.0 * 9.02534294128418
Epoch 2370, val loss: 0.9888625144958496
Epoch 2380, training loss: 90.27867126464844 = 0.022905047982931137 + 10.0 * 9.0255765914917
Epoch 2380, val loss: 0.9936049580574036
Epoch 2390, training loss: 90.30303192138672 = 0.022529497742652893 + 10.0 * 9.028050422668457
Epoch 2390, val loss: 0.9983878135681152
Epoch 2400, training loss: 90.28463745117188 = 0.022153664380311966 + 10.0 * 9.02624797821045
Epoch 2400, val loss: 1.0028516054153442
Epoch 2410, training loss: 90.27610778808594 = 0.021786537021398544 + 10.0 * 9.025431632995605
Epoch 2410, val loss: 1.0076617002487183
Epoch 2420, training loss: 90.28468322753906 = 0.021434038877487183 + 10.0 * 9.026325225830078
Epoch 2420, val loss: 1.0120614767074585
Epoch 2430, training loss: 90.27208709716797 = 0.021081309765577316 + 10.0 * 9.025100708007812
Epoch 2430, val loss: 1.0166996717453003
Epoch 2440, training loss: 90.27825164794922 = 0.020744867622852325 + 10.0 * 9.025751113891602
Epoch 2440, val loss: 1.0209691524505615
Epoch 2450, training loss: 90.27581024169922 = 0.020408781245350838 + 10.0 * 9.025540351867676
Epoch 2450, val loss: 1.0256770849227905
Epoch 2460, training loss: 90.28826904296875 = 0.020084815099835396 + 10.0 * 9.02681827545166
Epoch 2460, val loss: 1.029467225074768
Epoch 2470, training loss: 90.26786041259766 = 0.019762858748435974 + 10.0 * 9.024809837341309
Epoch 2470, val loss: 1.0342175960540771
Epoch 2480, training loss: 90.2601547241211 = 0.019449014216661453 + 10.0 * 9.024070739746094
Epoch 2480, val loss: 1.0386769771575928
Epoch 2490, training loss: 90.2591323852539 = 0.019144296646118164 + 10.0 * 9.023999214172363
Epoch 2490, val loss: 1.0430502891540527
Epoch 2500, training loss: 90.26175689697266 = 0.01884569600224495 + 10.0 * 9.024291038513184
Epoch 2500, val loss: 1.047722339630127
Epoch 2510, training loss: 90.29299926757812 = 0.018559906631708145 + 10.0 * 9.027443885803223
Epoch 2510, val loss: 1.0524989366531372
Epoch 2520, training loss: 90.27153015136719 = 0.01826973631978035 + 10.0 * 9.025325775146484
Epoch 2520, val loss: 1.0557368993759155
Epoch 2530, training loss: 90.27120208740234 = 0.017989324405789375 + 10.0 * 9.025321006774902
Epoch 2530, val loss: 1.0602513551712036
Epoch 2540, training loss: 90.25405883789062 = 0.017711196094751358 + 10.0 * 9.023634910583496
Epoch 2540, val loss: 1.0642439126968384
Epoch 2550, training loss: 90.25653076171875 = 0.017443083226680756 + 10.0 * 9.023908615112305
Epoch 2550, val loss: 1.0682713985443115
Epoch 2560, training loss: 90.27168273925781 = 0.017182158306241035 + 10.0 * 9.025449752807617
Epoch 2560, val loss: 1.0725094079971313
Epoch 2570, training loss: 90.25108337402344 = 0.016921615228056908 + 10.0 * 9.023416519165039
Epoch 2570, val loss: 1.0765060186386108
Epoch 2580, training loss: 90.25081634521484 = 0.016671154648065567 + 10.0 * 9.023414611816406
Epoch 2580, val loss: 1.0807355642318726
Epoch 2590, training loss: 90.2618637084961 = 0.016428688541054726 + 10.0 * 9.024543762207031
Epoch 2590, val loss: 1.0843772888183594
Epoch 2600, training loss: 90.2490005493164 = 0.01618322916328907 + 10.0 * 9.023282051086426
Epoch 2600, val loss: 1.0886256694793701
Epoch 2610, training loss: 90.25238800048828 = 0.015948472544550896 + 10.0 * 9.02364444732666
Epoch 2610, val loss: 1.0927855968475342
Epoch 2620, training loss: 90.25895690917969 = 0.01572020910680294 + 10.0 * 9.024324417114258
Epoch 2620, val loss: 1.0968371629714966
Epoch 2630, training loss: 90.2511978149414 = 0.015492572449147701 + 10.0 * 9.02357006072998
Epoch 2630, val loss: 1.1012448072433472
Epoch 2640, training loss: 90.25880432128906 = 0.015270000323653221 + 10.0 * 9.024353981018066
Epoch 2640, val loss: 1.1046067476272583
Epoch 2650, training loss: 90.24958801269531 = 0.015054297633469105 + 10.0 * 9.023453712463379
Epoch 2650, val loss: 1.1088849306106567
Epoch 2660, training loss: 90.2409439086914 = 0.014840071089565754 + 10.0 * 9.022610664367676
Epoch 2660, val loss: 1.11277437210083
Epoch 2670, training loss: 90.23969268798828 = 0.014634620398283005 + 10.0 * 9.022505760192871
Epoch 2670, val loss: 1.1165131330490112
Epoch 2680, training loss: 90.26237487792969 = 0.014436045661568642 + 10.0 * 9.02479362487793
Epoch 2680, val loss: 1.1204140186309814
Epoch 2690, training loss: 90.23973846435547 = 0.014231893233954906 + 10.0 * 9.022550582885742
Epoch 2690, val loss: 1.1242932081222534
Epoch 2700, training loss: 90.24569702148438 = 0.01403955090790987 + 10.0 * 9.023165702819824
Epoch 2700, val loss: 1.1277631521224976
Epoch 2710, training loss: 90.23981475830078 = 0.01384633406996727 + 10.0 * 9.02259635925293
Epoch 2710, val loss: 1.131516695022583
Epoch 2720, training loss: 90.24456024169922 = 0.013659698888659477 + 10.0 * 9.023090362548828
Epoch 2720, val loss: 1.13547682762146
Epoch 2730, training loss: 90.24273681640625 = 0.013477805070579052 + 10.0 * 9.02292537689209
Epoch 2730, val loss: 1.138914942741394
Epoch 2740, training loss: 90.23690032958984 = 0.013295459561049938 + 10.0 * 9.022359848022461
Epoch 2740, val loss: 1.1423068046569824
Epoch 2750, training loss: 90.24063110351562 = 0.013121036812663078 + 10.0 * 9.022750854492188
Epoch 2750, val loss: 1.145730972290039
Epoch 2760, training loss: 90.23302459716797 = 0.01294676773250103 + 10.0 * 9.022007942199707
Epoch 2760, val loss: 1.1497430801391602
Epoch 2770, training loss: 90.2332992553711 = 0.012777234427630901 + 10.0 * 9.022051811218262
Epoch 2770, val loss: 1.1533523797988892
Epoch 2780, training loss: 90.24190521240234 = 0.012611757963895798 + 10.0 * 9.022929191589355
Epoch 2780, val loss: 1.1566332578659058
Epoch 2790, training loss: 90.23080444335938 = 0.012446385808289051 + 10.0 * 9.021836280822754
Epoch 2790, val loss: 1.160228967666626
Epoch 2800, training loss: 90.23338317871094 = 0.012287422083318233 + 10.0 * 9.022109031677246
Epoch 2800, val loss: 1.1632699966430664
Epoch 2810, training loss: 90.22555541992188 = 0.012129529379308224 + 10.0 * 9.021342277526855
Epoch 2810, val loss: 1.166809320449829
Epoch 2820, training loss: 90.22769927978516 = 0.011975917965173721 + 10.0 * 9.02157211303711
Epoch 2820, val loss: 1.1702245473861694
Epoch 2830, training loss: 90.26168823242188 = 0.01183015014976263 + 10.0 * 9.024985313415527
Epoch 2830, val loss: 1.1735023260116577
Epoch 2840, training loss: 90.23651885986328 = 0.01168019138276577 + 10.0 * 9.022483825683594
Epoch 2840, val loss: 1.1774718761444092
Epoch 2850, training loss: 90.22286224365234 = 0.011533398181200027 + 10.0 * 9.021132469177246
Epoch 2850, val loss: 1.180014729499817
Epoch 2860, training loss: 90.21693420410156 = 0.011387676000595093 + 10.0 * 9.020554542541504
Epoch 2860, val loss: 1.183530330657959
Epoch 2870, training loss: 90.2156753540039 = 0.011248192749917507 + 10.0 * 9.020442962646484
Epoch 2870, val loss: 1.1866754293441772
Epoch 2880, training loss: 90.24039459228516 = 0.011115401983261108 + 10.0 * 9.022928237915039
Epoch 2880, val loss: 1.1900484561920166
Epoch 2890, training loss: 90.2276382446289 = 0.010979861952364445 + 10.0 * 9.021665573120117
Epoch 2890, val loss: 1.19283127784729
Epoch 2900, training loss: 90.22341918945312 = 0.010846013203263283 + 10.0 * 9.021257400512695
Epoch 2900, val loss: 1.1958773136138916
Epoch 2910, training loss: 90.21243286132812 = 0.01071387529373169 + 10.0 * 9.020172119140625
Epoch 2910, val loss: 1.1991478204727173
Epoch 2920, training loss: 90.21514129638672 = 0.010588054545223713 + 10.0 * 9.020455360412598
Epoch 2920, val loss: 1.2021169662475586
Epoch 2930, training loss: 90.23928833007812 = 0.010467372834682465 + 10.0 * 9.022882461547852
Epoch 2930, val loss: 1.2051774263381958
Epoch 2940, training loss: 90.22106170654297 = 0.010342753492295742 + 10.0 * 9.021071434020996
Epoch 2940, val loss: 1.2087275981903076
Epoch 2950, training loss: 90.21760559082031 = 0.01022274699062109 + 10.0 * 9.02073860168457
Epoch 2950, val loss: 1.2114115953445435
Epoch 2960, training loss: 90.22425079345703 = 0.0101067665964365 + 10.0 * 9.021413803100586
Epoch 2960, val loss: 1.214773416519165
Epoch 2970, training loss: 90.21476745605469 = 0.009990804828703403 + 10.0 * 9.020477294921875
Epoch 2970, val loss: 1.2171087265014648
Epoch 2980, training loss: 90.22367095947266 = 0.009880326688289642 + 10.0 * 9.021379470825195
Epoch 2980, val loss: 1.2204927206039429
Epoch 2990, training loss: 90.21102905273438 = 0.00976807065308094 + 10.0 * 9.020126342773438
Epoch 2990, val loss: 1.2226011753082275
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8493
Overall ASR: 0.6988
Flip ASR: 0.6248/1554 nodes
./selected_nodes/Pubmed/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 104.69342041015625 = 1.1026012897491455 + 10.0 * 10.359082221984863
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 319, in _train_with_val
    loss.backward()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 6.21 GiB already allocated; 417.69 MiB free; 6.45 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.4 der2:0 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 353.69 MiB free; 5.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.4 der2:0.2 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 353.69 MiB free; 5.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.4 der2:0.2 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 353.69 MiB free; 5.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.4 der2:0.2 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 104.68570709228516 = 1.0952452421188354 + 10.0 * 10.35904598236084
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 319, in _train_with_val
    loss.backward()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.18 GiB already allocated; 73.69 MiB free; 6.13 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.4 der2:0.2 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 353.69 MiB free; 5.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.4 der2:0.2 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 353.69 MiB free; 5.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.4 der2:0.2 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 104.69646453857422 = 1.1051127910614014 + 10.0 * 10.359135627746582
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 319, in _train_with_val
    loss.backward()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 6.11 GiB already allocated; 615.69 MiB free; 6.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.4 der2:0.3 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.3, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 104.6962890625 = 1.1064335107803345 + 10.0 * 10.358985900878906
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 319, in _train_with_val
    loss.backward()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 6.10 GiB already allocated; 627.69 MiB free; 6.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.4 der2:0.3 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.3, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 104.68817901611328 = 1.0986230373382568 + 10.0 * 10.358955383300781
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 319, in _train_with_val
    loss.backward()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 6.10 GiB already allocated; 627.69 MiB free; 6.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.4 der2:0.3 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.3, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 104.67752075195312 = 1.0867060422897339 + 10.0 * 10.359081268310547
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 319, in _train_with_val
    loss.backward()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 6.10 GiB already allocated; 625.69 MiB free; 6.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.4 der2:0.3 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 104.69865417480469 = 1.1076853275299072 + 10.0 * 10.35909652709961
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 319, in _train_with_val
    loss.backward()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 6.10 GiB already allocated; 625.69 MiB free; 6.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.4 der2:0.3 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 174, in semi_loss
    refl_sim = f(self.sim(z1, z1))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 4.20 GiB already allocated; 629.69 MiB free; 4.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.4 der2:0.3 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 174, in semi_loss
    refl_sim = f(self.sim(z1, z1))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 4.20 GiB already allocated; 629.69 MiB free; 4.28 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.4 der2:0.4 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.4, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 174, in semi_loss
    refl_sim = f(self.sim(z1, z1))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 4.20 GiB already allocated; 589.69 MiB free; 4.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.4 der2:0.4 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.4, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 174, in semi_loss
    refl_sim = f(self.sim(z1, z1))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 4.20 GiB already allocated; 589.69 MiB free; 4.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.4 der2:0.4 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.4, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 174, in semi_loss
    refl_sim = f(self.sim(z1, z1))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 4.20 GiB already allocated; 589.69 MiB free; 4.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.4 der2:0.4 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 174, in semi_loss
    refl_sim = f(self.sim(z1, z1))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 4.20 GiB already allocated; 589.69 MiB free; 4.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.4 der2:0.4 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 311.69 MiB free; 5.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.4 der2:0.4 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 311.69 MiB free; 5.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.5 der2:0 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 225.69 MiB free; 5.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.5 der2:0 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 227.69 MiB free; 5.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.5 der2:0 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 227.69 MiB free; 5.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.5 der2:0 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 174, in semi_loss
    refl_sim = f(self.sim(z1, z1))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 4.20 GiB already allocated; 655.69 MiB free; 4.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.5 der2:0 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 227.69 MiB free; 5.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.5 der2:0 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 897.69 MiB free; 5.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.5 der2:0.2 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 921.69 MiB free; 5.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.5 der2:0.2 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 921.69 MiB free; 5.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.5 der2:0.2 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 104.6860580444336 = 1.0955840349197388 + 10.0 * 10.359047889709473
Epoch 0, val loss: 1.0954943895339966
Epoch 10, training loss: 104.6471176147461 = 1.0841584205627441 + 10.0 * 10.356295585632324
Epoch 10, val loss: 1.0837517976760864
Epoch 20, training loss: 104.18126678466797 = 1.0690656900405884 + 10.0 * 10.311220169067383
Epoch 20, val loss: 1.068498134613037
Epoch 30, training loss: 98.75025177001953 = 1.0535331964492798 + 10.0 * 9.769671440124512
Epoch 30, val loss: 1.0530011653900146
Epoch 40, training loss: 96.7055435180664 = 1.0412561893463135 + 10.0 * 9.566428184509277
Epoch 40, val loss: 1.0408639907836914
Epoch 50, training loss: 95.40418243408203 = 1.0306216478347778 + 10.0 * 9.437355995178223
Epoch 50, val loss: 1.0301759243011475
Epoch 60, training loss: 94.91218566894531 = 1.019737958908081 + 10.0 * 9.38924503326416
Epoch 60, val loss: 1.0193700790405273
Epoch 70, training loss: 94.34436798095703 = 1.0107089281082153 + 10.0 * 9.333365440368652
Epoch 70, val loss: 1.010643482208252
Epoch 80, training loss: 93.57230377197266 = 1.0043751001358032 + 10.0 * 9.256793022155762
Epoch 80, val loss: 1.0045355558395386
Epoch 90, training loss: 93.15438079833984 = 0.9964544177055359 + 10.0 * 9.215792655944824
Epoch 90, val loss: 0.9965185523033142
Epoch 100, training loss: 92.97975158691406 = 0.9841229319572449 + 10.0 * 9.199563026428223
Epoch 100, val loss: 0.9845104217529297
Epoch 110, training loss: 92.84910583496094 = 0.9700990319252014 + 10.0 * 9.18790054321289
Epoch 110, val loss: 0.9709506630897522
Epoch 120, training loss: 92.72655487060547 = 0.95536208152771 + 10.0 * 9.177119255065918
Epoch 120, val loss: 0.9566295742988586
Epoch 130, training loss: 92.62269592285156 = 0.9393133521080017 + 10.0 * 9.16833782196045
Epoch 130, val loss: 0.9410428404808044
Epoch 140, training loss: 92.53087615966797 = 0.9212651252746582 + 10.0 * 9.160961151123047
Epoch 140, val loss: 0.923572838306427
Epoch 150, training loss: 92.44434356689453 = 0.9009996056556702 + 10.0 * 9.15433406829834
Epoch 150, val loss: 0.9040971398353577
Epoch 160, training loss: 92.3447494506836 = 0.8789061307907104 + 10.0 * 9.146584510803223
Epoch 160, val loss: 0.8828744292259216
Epoch 170, training loss: 92.24695587158203 = 0.8546714186668396 + 10.0 * 9.139228820800781
Epoch 170, val loss: 0.8596081137657166
Epoch 180, training loss: 92.15913391113281 = 0.8275735378265381 + 10.0 * 9.133155822753906
Epoch 180, val loss: 0.833608865737915
Epoch 190, training loss: 92.08486938476562 = 0.7969686985015869 + 10.0 * 9.128789901733398
Epoch 190, val loss: 0.8042765259742737
Epoch 200, training loss: 92.0147476196289 = 0.7629954218864441 + 10.0 * 9.125175476074219
Epoch 200, val loss: 0.7718814611434937
Epoch 210, training loss: 91.94969177246094 = 0.7265185713768005 + 10.0 * 9.12231731414795
Epoch 210, val loss: 0.7372866868972778
Epoch 220, training loss: 91.89417266845703 = 0.6885057091712952 + 10.0 * 9.120566368103027
Epoch 220, val loss: 0.7015640139579773
Epoch 230, training loss: 91.82646942138672 = 0.6503531336784363 + 10.0 * 9.1176118850708
Epoch 230, val loss: 0.6657994985580444
Epoch 240, training loss: 91.76313018798828 = 0.6127985715866089 + 10.0 * 9.115033149719238
Epoch 240, val loss: 0.630882740020752
Epoch 250, training loss: 91.70728302001953 = 0.5765368342399597 + 10.0 * 9.11307430267334
Epoch 250, val loss: 0.5974518656730652
Epoch 260, training loss: 91.65470123291016 = 0.5424464344978333 + 10.0 * 9.111225128173828
Epoch 260, val loss: 0.5663827657699585
Epoch 270, training loss: 91.64631652832031 = 0.5114527344703674 + 10.0 * 9.113486289978027
Epoch 270, val loss: 0.5385122299194336
Epoch 280, training loss: 91.56676483154297 = 0.48448917269706726 + 10.0 * 9.108227729797363
Epoch 280, val loss: 0.5148710608482361
Epoch 290, training loss: 91.5283432006836 = 0.46146008372306824 + 10.0 * 9.106688499450684
Epoch 290, val loss: 0.49489861726760864
Epoch 300, training loss: 91.49698638916016 = 0.4416891634464264 + 10.0 * 9.10552978515625
Epoch 300, val loss: 0.47811102867126465
Epoch 310, training loss: 91.46334075927734 = 0.4247584342956543 + 10.0 * 9.10385799407959
Epoch 310, val loss: 0.46415942907333374
Epoch 320, training loss: 91.4280776977539 = 0.4106076657772064 + 10.0 * 9.101747512817383
Epoch 320, val loss: 0.452840656042099
Epoch 330, training loss: 91.40011596679688 = 0.39848217368125916 + 10.0 * 9.100163459777832
Epoch 330, val loss: 0.44345346093177795
Epoch 340, training loss: 91.37266540527344 = 0.3878340721130371 + 10.0 * 9.098483085632324
Epoch 340, val loss: 0.4354800879955292
Epoch 350, training loss: 91.36971282958984 = 0.3784119188785553 + 10.0 * 9.099130630493164
Epoch 350, val loss: 0.42871594429016113
Epoch 360, training loss: 91.34275817871094 = 0.37017080187797546 + 10.0 * 9.097258567810059
Epoch 360, val loss: 0.42315754294395447
Epoch 370, training loss: 91.30937194824219 = 0.36295610666275024 + 10.0 * 9.09464168548584
Epoch 370, val loss: 0.4183238446712494
Epoch 380, training loss: 91.28884887695312 = 0.3565077483654022 + 10.0 * 9.093234062194824
Epoch 380, val loss: 0.41428065299987793
Epoch 390, training loss: 91.26972961425781 = 0.3505479097366333 + 10.0 * 9.091917991638184
Epoch 390, val loss: 0.4107029139995575
Epoch 400, training loss: 91.28333282470703 = 0.34500181674957275 + 10.0 * 9.093832969665527
Epoch 400, val loss: 0.4074866771697998
Epoch 410, training loss: 91.23565673828125 = 0.3399350643157959 + 10.0 * 9.089571952819824
Epoch 410, val loss: 0.4048837721347809
Epoch 420, training loss: 91.22122955322266 = 0.3352375328540802 + 10.0 * 9.08859920501709
Epoch 420, val loss: 0.4025026261806488
Epoch 430, training loss: 91.23723602294922 = 0.3307833969593048 + 10.0 * 9.090645790100098
Epoch 430, val loss: 0.4003872871398926
Epoch 440, training loss: 91.19734954833984 = 0.3266103267669678 + 10.0 * 9.087074279785156
Epoch 440, val loss: 0.3984229862689972
Epoch 450, training loss: 91.17719268798828 = 0.32271116971969604 + 10.0 * 9.085448265075684
Epoch 450, val loss: 0.396737664937973
Epoch 460, training loss: 91.17037200927734 = 0.31896546483039856 + 10.0 * 9.0851411819458
Epoch 460, val loss: 0.3953108787536621
Epoch 470, training loss: 91.15721130371094 = 0.31537187099456787 + 10.0 * 9.084184646606445
Epoch 470, val loss: 0.39386940002441406
Epoch 480, training loss: 91.13617706298828 = 0.3119451701641083 + 10.0 * 9.082423210144043
Epoch 480, val loss: 0.3926953077316284
Epoch 490, training loss: 91.1229019165039 = 0.3086315095424652 + 10.0 * 9.081426620483398
Epoch 490, val loss: 0.39161187410354614
Epoch 500, training loss: 91.16739654541016 = 0.3054330050945282 + 10.0 * 9.086195945739746
Epoch 500, val loss: 0.39077267050743103
Epoch 510, training loss: 91.11135864257812 = 0.3023764193058014 + 10.0 * 9.08089828491211
Epoch 510, val loss: 0.3898428678512573
Epoch 520, training loss: 91.09497833251953 = 0.2994622588157654 + 10.0 * 9.079551696777344
Epoch 520, val loss: 0.3890983462333679
Epoch 530, training loss: 91.08211517333984 = 0.2966003119945526 + 10.0 * 9.078551292419434
Epoch 530, val loss: 0.38845059275627136
Epoch 540, training loss: 91.09716033935547 = 0.2937922179698944 + 10.0 * 9.080336570739746
Epoch 540, val loss: 0.3877379894256592
Epoch 550, training loss: 91.0759506225586 = 0.29111722111701965 + 10.0 * 9.078483581542969
Epoch 550, val loss: 0.38749197125434875
Epoch 560, training loss: 91.05599975585938 = 0.28854602575302124 + 10.0 * 9.076745986938477
Epoch 560, val loss: 0.3869408965110779
Epoch 570, training loss: 91.04480743408203 = 0.28601813316345215 + 10.0 * 9.075879096984863
Epoch 570, val loss: 0.3866046965122223
Epoch 580, training loss: 91.03573608398438 = 0.28352099657058716 + 10.0 * 9.075221061706543
Epoch 580, val loss: 0.3862822949886322
Epoch 590, training loss: 91.02864837646484 = 0.2810593843460083 + 10.0 * 9.074758529663086
Epoch 590, val loss: 0.38605746626853943
Epoch 600, training loss: 91.03150939941406 = 0.2786548435688019 + 10.0 * 9.075284957885742
Epoch 600, val loss: 0.38600486516952515
Epoch 610, training loss: 91.0148696899414 = 0.2763572931289673 + 10.0 * 9.073850631713867
Epoch 610, val loss: 0.3857085704803467
Epoch 620, training loss: 91.00858306884766 = 0.2741188108921051 + 10.0 * 9.073446273803711
Epoch 620, val loss: 0.38570401072502136
Epoch 630, training loss: 90.99736022949219 = 0.2718964219093323 + 10.0 * 9.072546005249023
Epoch 630, val loss: 0.38572266697883606
Epoch 640, training loss: 90.98995208740234 = 0.2696985602378845 + 10.0 * 9.072025299072266
Epoch 640, val loss: 0.3857943117618561
Epoch 650, training loss: 91.0029296875 = 0.26753056049346924 + 10.0 * 9.073539733886719
Epoch 650, val loss: 0.3860030472278595
Epoch 660, training loss: 90.97921752929688 = 0.2654185891151428 + 10.0 * 9.071379661560059
Epoch 660, val loss: 0.3861314058303833
Epoch 670, training loss: 90.9713134765625 = 0.2633441090583801 + 10.0 * 9.070796966552734
Epoch 670, val loss: 0.3863033652305603
Epoch 680, training loss: 90.9593505859375 = 0.2612922787666321 + 10.0 * 9.069806098937988
Epoch 680, val loss: 0.38653823733329773
Epoch 690, training loss: 90.97420501708984 = 0.2592547535896301 + 10.0 * 9.071495056152344
Epoch 690, val loss: 0.3867815434932709
Epoch 700, training loss: 90.95452880859375 = 0.2572619318962097 + 10.0 * 9.069726943969727
Epoch 700, val loss: 0.3873135447502136
Epoch 710, training loss: 90.94316101074219 = 0.25529542565345764 + 10.0 * 9.06878662109375
Epoch 710, val loss: 0.38764116168022156
Epoch 720, training loss: 90.93883514404297 = 0.2533468008041382 + 10.0 * 9.068548202514648
Epoch 720, val loss: 0.3882363438606262
Epoch 730, training loss: 90.92861938476562 = 0.25142088532447815 + 10.0 * 9.067720413208008
Epoch 730, val loss: 0.3886118233203888
Epoch 740, training loss: 90.92493438720703 = 0.24953919649124146 + 10.0 * 9.06753921508789
Epoch 740, val loss: 0.38920196890830994
Epoch 750, training loss: 90.9150161743164 = 0.2476702481508255 + 10.0 * 9.066734313964844
Epoch 750, val loss: 0.38979101181030273
Epoch 760, training loss: 90.93201446533203 = 0.24582107365131378 + 10.0 * 9.068619728088379
Epoch 760, val loss: 0.39044761657714844
Epoch 770, training loss: 90.90980529785156 = 0.24399827420711517 + 10.0 * 9.066580772399902
Epoch 770, val loss: 0.39101746678352356
Epoch 780, training loss: 90.89481353759766 = 0.24220488965511322 + 10.0 * 9.065260887145996
Epoch 780, val loss: 0.39171165227890015
Epoch 790, training loss: 90.89012908935547 = 0.24042066931724548 + 10.0 * 9.064970970153809
Epoch 790, val loss: 0.392439603805542
Epoch 800, training loss: 90.9138412475586 = 0.23865145444869995 + 10.0 * 9.06751823425293
Epoch 800, val loss: 0.39316707849502563
Epoch 810, training loss: 90.89237976074219 = 0.23692023754119873 + 10.0 * 9.065546035766602
Epoch 810, val loss: 0.39417406916618347
Epoch 820, training loss: 90.8724594116211 = 0.23521246016025543 + 10.0 * 9.063724517822266
Epoch 820, val loss: 0.3950178921222687
Epoch 830, training loss: 90.86824035644531 = 0.23351185023784637 + 10.0 * 9.063472747802734
Epoch 830, val loss: 0.3958554267883301
Epoch 840, training loss: 90.90191650390625 = 0.23183085024356842 + 10.0 * 9.067008018493652
Epoch 840, val loss: 0.39681994915008545
Epoch 850, training loss: 90.85648345947266 = 0.2301807403564453 + 10.0 * 9.062630653381348
Epoch 850, val loss: 0.3979007303714752
Epoch 860, training loss: 90.85176086425781 = 0.2285504937171936 + 10.0 * 9.062320709228516
Epoch 860, val loss: 0.39898696541786194
Epoch 870, training loss: 90.84493255615234 = 0.2269238382577896 + 10.0 * 9.061800956726074
Epoch 870, val loss: 0.39998674392700195
Epoch 880, training loss: 90.84219360351562 = 0.22530795633792877 + 10.0 * 9.061688423156738
Epoch 880, val loss: 0.4011586010456085
Epoch 890, training loss: 90.84420776367188 = 0.22370553016662598 + 10.0 * 9.062049865722656
Epoch 890, val loss: 0.40239837765693665
Epoch 900, training loss: 90.83895111083984 = 0.2221430093050003 + 10.0 * 9.061680793762207
Epoch 900, val loss: 0.40343359112739563
Epoch 910, training loss: 90.82681274414062 = 0.22058402001857758 + 10.0 * 9.060623168945312
Epoch 910, val loss: 0.4046809673309326
Epoch 920, training loss: 90.82157135009766 = 0.2190326303243637 + 10.0 * 9.060254096984863
Epoch 920, val loss: 0.4058416187763214
Epoch 930, training loss: 90.83939361572266 = 0.21749304234981537 + 10.0 * 9.062190055847168
Epoch 930, val loss: 0.40716391801834106
Epoch 940, training loss: 90.81720733642578 = 0.21597479283809662 + 10.0 * 9.060123443603516
Epoch 940, val loss: 0.4084601104259491
Epoch 950, training loss: 90.80718231201172 = 0.21446138620376587 + 10.0 * 9.059271812438965
Epoch 950, val loss: 0.4098191261291504
Epoch 960, training loss: 90.81796264648438 = 0.21296022832393646 + 10.0 * 9.060500144958496
Epoch 960, val loss: 0.41118577122688293
Epoch 970, training loss: 90.80229949951172 = 0.21147796511650085 + 10.0 * 9.05908203125
Epoch 970, val loss: 0.41250675916671753
Epoch 980, training loss: 90.80721282958984 = 0.2099987268447876 + 10.0 * 9.059720993041992
Epoch 980, val loss: 0.4139036238193512
Epoch 990, training loss: 90.78849792480469 = 0.20853964984416962 + 10.0 * 9.057995796203613
Epoch 990, val loss: 0.4154418110847473
Epoch 1000, training loss: 90.78585815429688 = 0.2070859968662262 + 10.0 * 9.057877540588379
Epoch 1000, val loss: 0.41694456338882446
Epoch 1010, training loss: 90.7939453125 = 0.20564793050289154 + 10.0 * 9.058829307556152
Epoch 1010, val loss: 0.41836774349212646
Epoch 1020, training loss: 90.77671813964844 = 0.20421569049358368 + 10.0 * 9.057250022888184
Epoch 1020, val loss: 0.4200727045536041
Epoch 1030, training loss: 90.77302551269531 = 0.20279213786125183 + 10.0 * 9.057024002075195
Epoch 1030, val loss: 0.4215010404586792
Epoch 1040, training loss: 90.77537536621094 = 0.201370507478714 + 10.0 * 9.057400703430176
Epoch 1040, val loss: 0.4232596158981323
Epoch 1050, training loss: 90.77346801757812 = 0.19996261596679688 + 10.0 * 9.057351112365723
Epoch 1050, val loss: 0.42485228180885315
Epoch 1060, training loss: 90.76197052001953 = 0.19856762886047363 + 10.0 * 9.056340217590332
Epoch 1060, val loss: 0.42655229568481445
Epoch 1070, training loss: 90.75762176513672 = 0.19717352092266083 + 10.0 * 9.056044578552246
Epoch 1070, val loss: 0.4280514717102051
Epoch 1080, training loss: 90.7714614868164 = 0.1957867294549942 + 10.0 * 9.057567596435547
Epoch 1080, val loss: 0.42982035875320435
Epoch 1090, training loss: 90.75384521484375 = 0.19441251456737518 + 10.0 * 9.055943489074707
Epoch 1090, val loss: 0.43154066801071167
Epoch 1100, training loss: 90.74314880371094 = 0.19305604696273804 + 10.0 * 9.055009841918945
Epoch 1100, val loss: 0.4332510530948639
Epoch 1110, training loss: 90.73686218261719 = 0.19169345498085022 + 10.0 * 9.054516792297363
Epoch 1110, val loss: 0.43506306409835815
Epoch 1120, training loss: 90.73297119140625 = 0.19033893942832947 + 10.0 * 9.0542631149292
Epoch 1120, val loss: 0.4368942081928253
Epoch 1130, training loss: 90.76313018798828 = 0.18900185823440552 + 10.0 * 9.057413101196289
Epoch 1130, val loss: 0.4389340281486511
Epoch 1140, training loss: 90.72721099853516 = 0.1876579225063324 + 10.0 * 9.053955078125
Epoch 1140, val loss: 0.4406106173992157
Epoch 1150, training loss: 90.72346496582031 = 0.1863389015197754 + 10.0 * 9.053712844848633
Epoch 1150, val loss: 0.4424014091491699
Epoch 1160, training loss: 90.71488952636719 = 0.18502134084701538 + 10.0 * 9.052987098693848
Epoch 1160, val loss: 0.44433528184890747
Epoch 1170, training loss: 90.71666717529297 = 0.18370462954044342 + 10.0 * 9.053296089172363
Epoch 1170, val loss: 0.446381539106369
Epoch 1180, training loss: 90.71914672851562 = 0.18239052593708038 + 10.0 * 9.053675651550293
Epoch 1180, val loss: 0.44840532541275024
Epoch 1190, training loss: 90.70951080322266 = 0.18109264969825745 + 10.0 * 9.052842140197754
Epoch 1190, val loss: 0.4502657353878021
Epoch 1200, training loss: 90.71175384521484 = 0.1798015981912613 + 10.0 * 9.053194999694824
Epoch 1200, val loss: 0.4522729516029358
Epoch 1210, training loss: 90.69876861572266 = 0.17851468920707703 + 10.0 * 9.05202579498291
Epoch 1210, val loss: 0.4543452262878418
Epoch 1220, training loss: 90.69327545166016 = 0.1772303581237793 + 10.0 * 9.051604270935059
Epoch 1220, val loss: 0.45641154050827026
Epoch 1230, training loss: 90.69686889648438 = 0.17594845592975616 + 10.0 * 9.052091598510742
Epoch 1230, val loss: 0.45851442217826843
Epoch 1240, training loss: 90.69032287597656 = 0.17466969788074493 + 10.0 * 9.051565170288086
Epoch 1240, val loss: 0.4606744050979614
Epoch 1250, training loss: 90.68120574951172 = 0.17340300977230072 + 10.0 * 9.050780296325684
Epoch 1250, val loss: 0.4627796709537506
Epoch 1260, training loss: 90.67819213867188 = 0.172138050198555 + 10.0 * 9.050605773925781
Epoch 1260, val loss: 0.4649454951286316
Epoch 1270, training loss: 90.70797729492188 = 0.17087583243846893 + 10.0 * 9.053709983825684
Epoch 1270, val loss: 0.46743717789649963
Epoch 1280, training loss: 90.6924819946289 = 0.16963376104831696 + 10.0 * 9.052285194396973
Epoch 1280, val loss: 0.4692992568016052
Epoch 1290, training loss: 90.6678695678711 = 0.1683846265077591 + 10.0 * 9.049947738647461
Epoch 1290, val loss: 0.47149816155433655
Epoch 1300, training loss: 90.6644287109375 = 0.1671440750360489 + 10.0 * 9.049728393554688
Epoch 1300, val loss: 0.4737759828567505
Epoch 1310, training loss: 90.66062927246094 = 0.1659017950296402 + 10.0 * 9.04947280883789
Epoch 1310, val loss: 0.4760366380214691
Epoch 1320, training loss: 90.6948013305664 = 0.16467027366161346 + 10.0 * 9.05301284790039
Epoch 1320, val loss: 0.47827935218811035
Epoch 1330, training loss: 90.6594009399414 = 0.16342957317829132 + 10.0 * 9.049596786499023
Epoch 1330, val loss: 0.4805757999420166
Epoch 1340, training loss: 90.6481704711914 = 0.1621963083744049 + 10.0 * 9.04859733581543
Epoch 1340, val loss: 0.48297980427742004
Epoch 1350, training loss: 90.64532470703125 = 0.1609688252210617 + 10.0 * 9.04843521118164
Epoch 1350, val loss: 0.4853768050670624
Epoch 1360, training loss: 90.66218566894531 = 0.15974970161914825 + 10.0 * 9.050243377685547
Epoch 1360, val loss: 0.4880226254463196
Epoch 1370, training loss: 90.65126037597656 = 0.1585174798965454 + 10.0 * 9.049274444580078
Epoch 1370, val loss: 0.4899614751338959
Epoch 1380, training loss: 90.63993072509766 = 0.15731114149093628 + 10.0 * 9.048261642456055
Epoch 1380, val loss: 0.49269363284111023
Epoch 1390, training loss: 90.63224792480469 = 0.15609942376613617 + 10.0 * 9.047615051269531
Epoch 1390, val loss: 0.4951055645942688
Epoch 1400, training loss: 90.63591003417969 = 0.15488919615745544 + 10.0 * 9.048101425170898
Epoch 1400, val loss: 0.4975552558898926
Epoch 1410, training loss: 90.63921356201172 = 0.15368786454200745 + 10.0 * 9.048552513122559
Epoch 1410, val loss: 0.5001096725463867
Epoch 1420, training loss: 90.62234497070312 = 0.15248806774616241 + 10.0 * 9.046985626220703
Epoch 1420, val loss: 0.5026702880859375
Epoch 1430, training loss: 90.62300109863281 = 0.15129238367080688 + 10.0 * 9.047170639038086
Epoch 1430, val loss: 0.5053133368492126
Epoch 1440, training loss: 90.64549255371094 = 0.15010111033916473 + 10.0 * 9.049539566040039
Epoch 1440, val loss: 0.5079018473625183
Epoch 1450, training loss: 90.61946868896484 = 0.14891743659973145 + 10.0 * 9.0470552444458
Epoch 1450, val loss: 0.5102887153625488
Epoch 1460, training loss: 90.61023712158203 = 0.14772644639015198 + 10.0 * 9.04625129699707
Epoch 1460, val loss: 0.513048529624939
Epoch 1470, training loss: 90.6208724975586 = 0.14654584228992462 + 10.0 * 9.047432899475098
Epoch 1470, val loss: 0.5156481862068176
Epoch 1480, training loss: 90.60301208496094 = 0.14535754919052124 + 10.0 * 9.04576587677002
Epoch 1480, val loss: 0.5182709693908691
Epoch 1490, training loss: 90.60445404052734 = 0.14418287575244904 + 10.0 * 9.046027183532715
Epoch 1490, val loss: 0.5209973454475403
Epoch 1500, training loss: 90.5953140258789 = 0.14300161600112915 + 10.0 * 9.045230865478516
Epoch 1500, val loss: 0.5236599445343018
Epoch 1510, training loss: 90.59143829345703 = 0.14182424545288086 + 10.0 * 9.044961929321289
Epoch 1510, val loss: 0.5264405608177185
Epoch 1520, training loss: 90.61255645751953 = 0.14065061509609222 + 10.0 * 9.04719066619873
Epoch 1520, val loss: 0.5291765928268433
Epoch 1530, training loss: 90.59727478027344 = 0.13948072493076324 + 10.0 * 9.04577922821045
Epoch 1530, val loss: 0.5319505333900452
Epoch 1540, training loss: 90.58566284179688 = 0.1383025050163269 + 10.0 * 9.0447359085083
Epoch 1540, val loss: 0.5347346067428589
Epoch 1550, training loss: 90.58197021484375 = 0.13713425397872925 + 10.0 * 9.04448413848877
Epoch 1550, val loss: 0.537428081035614
Epoch 1560, training loss: 90.61247253417969 = 0.13597555458545685 + 10.0 * 9.047649383544922
Epoch 1560, val loss: 0.5403956770896912
Epoch 1570, training loss: 90.5746841430664 = 0.13481388986110687 + 10.0 * 9.043987274169922
Epoch 1570, val loss: 0.5433166027069092
Epoch 1580, training loss: 90.57573699951172 = 0.13366296887397766 + 10.0 * 9.044207572937012
Epoch 1580, val loss: 0.5463383197784424
Epoch 1590, training loss: 90.56719970703125 = 0.1325075477361679 + 10.0 * 9.043469429016113
Epoch 1590, val loss: 0.5492538213729858
Epoch 1600, training loss: 90.56562042236328 = 0.13135342299938202 + 10.0 * 9.043426513671875
Epoch 1600, val loss: 0.5520840883255005
Epoch 1610, training loss: 90.61681365966797 = 0.13021717965602875 + 10.0 * 9.048659324645996
Epoch 1610, val loss: 0.5550337433815002
Epoch 1620, training loss: 90.5750961303711 = 0.12905240058898926 + 10.0 * 9.044604301452637
Epoch 1620, val loss: 0.5580993294715881
Epoch 1630, training loss: 90.55921173095703 = 0.12790675461292267 + 10.0 * 9.043130874633789
Epoch 1630, val loss: 0.5610905289649963
Epoch 1640, training loss: 90.55218505859375 = 0.1267612874507904 + 10.0 * 9.042542457580566
Epoch 1640, val loss: 0.563909649848938
Epoch 1650, training loss: 90.54969787597656 = 0.12561434507369995 + 10.0 * 9.042407989501953
Epoch 1650, val loss: 0.5669102668762207
Epoch 1660, training loss: 90.58847045898438 = 0.124477818608284 + 10.0 * 9.046399116516113
Epoch 1660, val loss: 0.5696343779563904
Epoch 1670, training loss: 90.57492065429688 = 0.12333916872739792 + 10.0 * 9.045158386230469
Epoch 1670, val loss: 0.5734402537345886
Epoch 1680, training loss: 90.54149627685547 = 0.12219534814357758 + 10.0 * 9.041930198669434
Epoch 1680, val loss: 0.576269268989563
Epoch 1690, training loss: 90.5396957397461 = 0.12106366455554962 + 10.0 * 9.041863441467285
Epoch 1690, val loss: 0.5791127681732178
Epoch 1700, training loss: 90.53459167480469 = 0.11992724984884262 + 10.0 * 9.04146671295166
Epoch 1700, val loss: 0.5822535753250122
Epoch 1710, training loss: 90.537353515625 = 0.11879083514213562 + 10.0 * 9.04185676574707
Epoch 1710, val loss: 0.5854876041412354
Epoch 1720, training loss: 90.54792022705078 = 0.11765678226947784 + 10.0 * 9.043025970458984
Epoch 1720, val loss: 0.5887283682823181
Epoch 1730, training loss: 90.5270004272461 = 0.11652768403291702 + 10.0 * 9.041047096252441
Epoch 1730, val loss: 0.5918514132499695
Epoch 1740, training loss: 90.52920532226562 = 0.11539875715970993 + 10.0 * 9.041379928588867
Epoch 1740, val loss: 0.5950450301170349
Epoch 1750, training loss: 90.54080200195312 = 0.11427022516727448 + 10.0 * 9.04265308380127
Epoch 1750, val loss: 0.5983471274375916
Epoch 1760, training loss: 90.52406311035156 = 0.11314661055803299 + 10.0 * 9.041091918945312
Epoch 1760, val loss: 0.6015465259552002
Epoch 1770, training loss: 90.51818084716797 = 0.11202649772167206 + 10.0 * 9.04061508178711
Epoch 1770, val loss: 0.604766309261322
Epoch 1780, training loss: 90.5202865600586 = 0.11090699583292007 + 10.0 * 9.040937423706055
Epoch 1780, val loss: 0.6080877780914307
Epoch 1790, training loss: 90.52204895019531 = 0.10978955775499344 + 10.0 * 9.041226387023926
Epoch 1790, val loss: 0.611416757106781
Epoch 1800, training loss: 90.5123062133789 = 0.10866941511631012 + 10.0 * 9.040363311767578
Epoch 1800, val loss: 0.6148518323898315
Epoch 1810, training loss: 90.50662994384766 = 0.10755210369825363 + 10.0 * 9.039907455444336
Epoch 1810, val loss: 0.6181673407554626
Epoch 1820, training loss: 90.5266342163086 = 0.10644077509641647 + 10.0 * 9.042019844055176
Epoch 1820, val loss: 0.6217018961906433
Epoch 1830, training loss: 90.5113754272461 = 0.10533291101455688 + 10.0 * 9.040604591369629
Epoch 1830, val loss: 0.6249699592590332
Epoch 1840, training loss: 90.49993133544922 = 0.10422167927026749 + 10.0 * 9.039570808410645
Epoch 1840, val loss: 0.6284843683242798
Epoch 1850, training loss: 90.49408721923828 = 0.10311760008335114 + 10.0 * 9.03909683227539
Epoch 1850, val loss: 0.6318246722221375
Epoch 1860, training loss: 90.50389099121094 = 0.1020214632153511 + 10.0 * 9.040186882019043
Epoch 1860, val loss: 0.6353164911270142
Epoch 1870, training loss: 90.49493408203125 = 0.1009175255894661 + 10.0 * 9.03940200805664
Epoch 1870, val loss: 0.6389071941375732
Epoch 1880, training loss: 90.48723602294922 = 0.09982271492481232 + 10.0 * 9.038741111755371
Epoch 1880, val loss: 0.6424146294593811
Epoch 1890, training loss: 90.49105072021484 = 0.09873431921005249 + 10.0 * 9.03923225402832
Epoch 1890, val loss: 0.6460158228874207
Epoch 1900, training loss: 90.49586486816406 = 0.09765803813934326 + 10.0 * 9.039820671081543
Epoch 1900, val loss: 0.6495009660720825
Epoch 1910, training loss: 90.48493194580078 = 0.09656748175621033 + 10.0 * 9.038836479187012
Epoch 1910, val loss: 0.6531838774681091
Epoch 1920, training loss: 90.47779846191406 = 0.09549234062433243 + 10.0 * 9.038230895996094
Epoch 1920, val loss: 0.6567903757095337
Epoch 1930, training loss: 90.47467041015625 = 0.09441762417554855 + 10.0 * 9.03802490234375
Epoch 1930, val loss: 0.6603400707244873
Epoch 1940, training loss: 90.5037612915039 = 0.09336057305335999 + 10.0 * 9.041040420532227
Epoch 1940, val loss: 0.6634774804115295
Epoch 1950, training loss: 90.47864532470703 = 0.09228254109621048 + 10.0 * 9.038636207580566
Epoch 1950, val loss: 0.6678619384765625
Epoch 1960, training loss: 90.46648406982422 = 0.0912259966135025 + 10.0 * 9.03752613067627
Epoch 1960, val loss: 0.6714209318161011
Epoch 1970, training loss: 90.46361541748047 = 0.09017288684844971 + 10.0 * 9.037343978881836
Epoch 1970, val loss: 0.6750578880310059
Epoch 1980, training loss: 90.47520446777344 = 0.08912891894578934 + 10.0 * 9.038607597351074
Epoch 1980, val loss: 0.6788936257362366
Epoch 1990, training loss: 90.4598617553711 = 0.08808190375566483 + 10.0 * 9.037178039550781
Epoch 1990, val loss: 0.6827561855316162
Epoch 2000, training loss: 90.46240997314453 = 0.0870463028550148 + 10.0 * 9.03753662109375
Epoch 2000, val loss: 0.6866124868392944
Epoch 2010, training loss: 90.45817565917969 = 0.08602471649646759 + 10.0 * 9.037215232849121
Epoch 2010, val loss: 0.6903181076049805
Epoch 2020, training loss: 90.46292877197266 = 0.0850076898932457 + 10.0 * 9.037792205810547
Epoch 2020, val loss: 0.694361686706543
Epoch 2030, training loss: 90.45481872558594 = 0.08398862183094025 + 10.0 * 9.03708267211914
Epoch 2030, val loss: 0.6985043883323669
Epoch 2040, training loss: 90.44503021240234 = 0.08298104256391525 + 10.0 * 9.036205291748047
Epoch 2040, val loss: 0.7022145390510559
Epoch 2050, training loss: 90.44328308105469 = 0.08198176324367523 + 10.0 * 9.03612995147705
Epoch 2050, val loss: 0.7063775658607483
Epoch 2060, training loss: 90.46771240234375 = 0.08099409192800522 + 10.0 * 9.038671493530273
Epoch 2060, val loss: 0.7103766798973083
Epoch 2070, training loss: 90.4511489868164 = 0.080011747777462 + 10.0 * 9.037114143371582
Epoch 2070, val loss: 0.7141138315200806
Epoch 2080, training loss: 90.4383773803711 = 0.07902555912733078 + 10.0 * 9.035935401916504
Epoch 2080, val loss: 0.7181615829467773
Epoch 2090, training loss: 90.43975830078125 = 0.07806163281202316 + 10.0 * 9.036169052124023
Epoch 2090, val loss: 0.7223461866378784
Epoch 2100, training loss: 90.43995666503906 = 0.07710252702236176 + 10.0 * 9.036285400390625
Epoch 2100, val loss: 0.7265302538871765
Epoch 2110, training loss: 90.43516540527344 = 0.07614345848560333 + 10.0 * 9.03590202331543
Epoch 2110, val loss: 0.7306365370750427
Epoch 2120, training loss: 90.43932342529297 = 0.07520365715026855 + 10.0 * 9.036412239074707
Epoch 2120, val loss: 0.7349042892456055
Epoch 2130, training loss: 90.42447662353516 = 0.07425689697265625 + 10.0 * 9.035021781921387
Epoch 2130, val loss: 0.7387939691543579
Epoch 2140, training loss: 90.4312973022461 = 0.0733334943652153 + 10.0 * 9.035796165466309
Epoch 2140, val loss: 0.7426941990852356
Epoch 2150, training loss: 90.43605041503906 = 0.07241776585578918 + 10.0 * 9.03636360168457
Epoch 2150, val loss: 0.7471953630447388
Epoch 2160, training loss: 90.4234619140625 = 0.07150650769472122 + 10.0 * 9.035195350646973
Epoch 2160, val loss: 0.7511827945709229
Epoch 2170, training loss: 90.41747283935547 = 0.07060053944587708 + 10.0 * 9.034687042236328
Epoch 2170, val loss: 0.7555397748947144
Epoch 2180, training loss: 90.41741943359375 = 0.06971200555562973 + 10.0 * 9.034770965576172
Epoch 2180, val loss: 0.7598392367362976
Epoch 2190, training loss: 90.41399383544922 = 0.0688224509358406 + 10.0 * 9.034517288208008
Epoch 2190, val loss: 0.7642588019371033
Epoch 2200, training loss: 90.4170913696289 = 0.06795451790094376 + 10.0 * 9.034914016723633
Epoch 2200, val loss: 0.7690987586975098
Epoch 2210, training loss: 90.412841796875 = 0.06707322597503662 + 10.0 * 9.034577369689941
Epoch 2210, val loss: 0.7728316187858582
Epoch 2220, training loss: 90.40570831298828 = 0.06621655821800232 + 10.0 * 9.03394889831543
Epoch 2220, val loss: 0.7770314812660217
Epoch 2230, training loss: 90.40386962890625 = 0.06536021083593369 + 10.0 * 9.03385066986084
Epoch 2230, val loss: 0.7813612222671509
Epoch 2240, training loss: 90.42698669433594 = 0.06453178077936172 + 10.0 * 9.036245346069336
Epoch 2240, val loss: 0.785495400428772
Epoch 2250, training loss: 90.4034652709961 = 0.06369567662477493 + 10.0 * 9.033976554870605
Epoch 2250, val loss: 0.7901934385299683
Epoch 2260, training loss: 90.39616394042969 = 0.06286683678627014 + 10.0 * 9.033329963684082
Epoch 2260, val loss: 0.7947223782539368
Epoch 2270, training loss: 90.40239715576172 = 0.062058743089437485 + 10.0 * 9.03403377532959
Epoch 2270, val loss: 0.7988342642784119
Epoch 2280, training loss: 90.40178680419922 = 0.061254940927028656 + 10.0 * 9.034052848815918
Epoch 2280, val loss: 0.8035725355148315
Epoch 2290, training loss: 90.39218139648438 = 0.06045590713620186 + 10.0 * 9.033172607421875
Epoch 2290, val loss: 0.8076878190040588
Epoch 2300, training loss: 90.3849868774414 = 0.05967096611857414 + 10.0 * 9.03253173828125
Epoch 2300, val loss: 0.8123483657836914
Epoch 2310, training loss: 90.38264465332031 = 0.05889409780502319 + 10.0 * 9.03237533569336
Epoch 2310, val loss: 0.8166093230247498
Epoch 2320, training loss: 90.385009765625 = 0.058128722012043 + 10.0 * 9.03268814086914
Epoch 2320, val loss: 0.821372926235199
Epoch 2330, training loss: 90.4081039428711 = 0.05737496167421341 + 10.0 * 9.035073280334473
Epoch 2330, val loss: 0.825789749622345
Epoch 2340, training loss: 90.39602661132812 = 0.05662636458873749 + 10.0 * 9.033940315246582
Epoch 2340, val loss: 0.8300850987434387
Epoch 2350, training loss: 90.37913513183594 = 0.055875517427921295 + 10.0 * 9.032325744628906
Epoch 2350, val loss: 0.8343517184257507
Epoch 2360, training loss: 90.38286590576172 = 0.05514875054359436 + 10.0 * 9.032772064208984
Epoch 2360, val loss: 0.8387958407402039
Epoch 2370, training loss: 90.37726593017578 = 0.05442533269524574 + 10.0 * 9.032283782958984
Epoch 2370, val loss: 0.8433015942573547
Epoch 2380, training loss: 90.37315368652344 = 0.05370962992310524 + 10.0 * 9.031944274902344
Epoch 2380, val loss: 0.8482178449630737
Epoch 2390, training loss: 90.37916564941406 = 0.05300569534301758 + 10.0 * 9.032615661621094
Epoch 2390, val loss: 0.8525720238685608
Epoch 2400, training loss: 90.37234497070312 = 0.052306294441223145 + 10.0 * 9.032003402709961
Epoch 2400, val loss: 0.857039749622345
Epoch 2410, training loss: 90.37724304199219 = 0.05162417143583298 + 10.0 * 9.032562255859375
Epoch 2410, val loss: 0.8615491390228271
Epoch 2420, training loss: 90.36517333984375 = 0.05093550682067871 + 10.0 * 9.031423568725586
Epoch 2420, val loss: 0.8663032054901123
Epoch 2430, training loss: 90.360107421875 = 0.050262242555618286 + 10.0 * 9.030984878540039
Epoch 2430, val loss: 0.8706009387969971
Epoch 2440, training loss: 90.36793518066406 = 0.04960786551237106 + 10.0 * 9.031832695007324
Epoch 2440, val loss: 0.8751798272132874
Epoch 2450, training loss: 90.36502838134766 = 0.048959679901599884 + 10.0 * 9.031606674194336
Epoch 2450, val loss: 0.8799952864646912
Epoch 2460, training loss: 90.35579681396484 = 0.04831537976861 + 10.0 * 9.03074836730957
Epoch 2460, val loss: 0.884188175201416
Epoch 2470, training loss: 90.35897827148438 = 0.04768422618508339 + 10.0 * 9.031129837036133
Epoch 2470, val loss: 0.8887370228767395
Epoch 2480, training loss: 90.36909484863281 = 0.047071050852537155 + 10.0 * 9.032201766967773
Epoch 2480, val loss: 0.8930302262306213
Epoch 2490, training loss: 90.36717987060547 = 0.04645168408751488 + 10.0 * 9.032072067260742
Epoch 2490, val loss: 0.8976791501045227
Epoch 2500, training loss: 90.35009765625 = 0.045840006321668625 + 10.0 * 9.030426025390625
Epoch 2500, val loss: 0.9021326899528503
Epoch 2510, training loss: 90.34730529785156 = 0.04523936286568642 + 10.0 * 9.030206680297852
Epoch 2510, val loss: 0.9065649509429932
Epoch 2520, training loss: 90.34503173828125 = 0.04465290531516075 + 10.0 * 9.030037879943848
Epoch 2520, val loss: 0.9109130501747131
Epoch 2530, training loss: 90.37928771972656 = 0.04408573359251022 + 10.0 * 9.033520698547363
Epoch 2530, val loss: 0.9151026010513306
Epoch 2540, training loss: 90.35208892822266 = 0.04350929707288742 + 10.0 * 9.030858039855957
Epoch 2540, val loss: 0.9200231432914734
Epoch 2550, training loss: 90.34060668945312 = 0.04294394701719284 + 10.0 * 9.029766082763672
Epoch 2550, val loss: 0.9243733286857605
Epoch 2560, training loss: 90.3431396484375 = 0.04239320009946823 + 10.0 * 9.030074119567871
Epoch 2560, val loss: 0.9285624623298645
Epoch 2570, training loss: 90.34526824951172 = 0.041855741292238235 + 10.0 * 9.030341148376465
Epoch 2570, val loss: 0.9326916337013245
Epoch 2580, training loss: 90.3570556640625 = 0.04132023826241493 + 10.0 * 9.031573295593262
Epoch 2580, val loss: 0.9372046589851379
Epoch 2590, training loss: 90.3495864868164 = 0.0407857708632946 + 10.0 * 9.030879974365234
Epoch 2590, val loss: 0.9426207542419434
Epoch 2600, training loss: 90.33353424072266 = 0.04025659337639809 + 10.0 * 9.029327392578125
Epoch 2600, val loss: 0.9464091658592224
Epoch 2610, training loss: 90.33018493652344 = 0.03974504396319389 + 10.0 * 9.029044151306152
Epoch 2610, val loss: 0.9510302543640137
Epoch 2620, training loss: 90.33763885498047 = 0.03924272581934929 + 10.0 * 9.029839515686035
Epoch 2620, val loss: 0.9553877115249634
Epoch 2630, training loss: 90.3333740234375 = 0.0387401208281517 + 10.0 * 9.029462814331055
Epoch 2630, val loss: 0.9598490595817566
Epoch 2640, training loss: 90.32198333740234 = 0.038247279822826385 + 10.0 * 9.028373718261719
Epoch 2640, val loss: 0.964259147644043
Epoch 2650, training loss: 90.32176971435547 = 0.03776494413614273 + 10.0 * 9.028400421142578
Epoch 2650, val loss: 0.9686261415481567
Epoch 2660, training loss: 90.32152557373047 = 0.037290267646312714 + 10.0 * 9.028423309326172
Epoch 2660, val loss: 0.9730991125106812
Epoch 2670, training loss: 90.3379135131836 = 0.03682949021458626 + 10.0 * 9.030108451843262
Epoch 2670, val loss: 0.9772258996963501
Epoch 2680, training loss: 90.31986236572266 = 0.036361753940582275 + 10.0 * 9.028349876403809
Epoch 2680, val loss: 0.9819834232330322
Epoch 2690, training loss: 90.33616638183594 = 0.03591575846076012 + 10.0 * 9.030024528503418
Epoch 2690, val loss: 0.9860179424285889
Epoch 2700, training loss: 90.33487701416016 = 0.03546207398176193 + 10.0 * 9.02994155883789
Epoch 2700, val loss: 0.990502119064331
Epoch 2710, training loss: 90.3165283203125 = 0.03502100706100464 + 10.0 * 9.02815055847168
Epoch 2710, val loss: 0.9948078989982605
Epoch 2720, training loss: 90.3116226196289 = 0.03458559885621071 + 10.0 * 9.027704238891602
Epoch 2720, val loss: 0.9992699027061462
Epoch 2730, training loss: 90.31037139892578 = 0.0341590978205204 + 10.0 * 9.027621269226074
Epoch 2730, val loss: 1.003401279449463
Epoch 2740, training loss: 90.32943725585938 = 0.033753108233213425 + 10.0 * 9.029568672180176
Epoch 2740, val loss: 1.0077931880950928
Epoch 2750, training loss: 90.31858825683594 = 0.03332579880952835 + 10.0 * 9.028526306152344
Epoch 2750, val loss: 1.012392282485962
Epoch 2760, training loss: 90.30997467041016 = 0.032915227115154266 + 10.0 * 9.027706146240234
Epoch 2760, val loss: 1.0166994333267212
Epoch 2770, training loss: 90.30428314208984 = 0.03250974789261818 + 10.0 * 9.027177810668945
Epoch 2770, val loss: 1.021066427230835
Epoch 2780, training loss: 90.30249786376953 = 0.03211446478962898 + 10.0 * 9.02703857421875
Epoch 2780, val loss: 1.025389313697815
Epoch 2790, training loss: 90.32972717285156 = 0.0317317359149456 + 10.0 * 9.029799461364746
Epoch 2790, val loss: 1.0298980474472046
Epoch 2800, training loss: 90.31002044677734 = 0.031356003135442734 + 10.0 * 9.02786636352539
Epoch 2800, val loss: 1.0346400737762451
Epoch 2810, training loss: 90.30635833740234 = 0.030966388061642647 + 10.0 * 9.027539253234863
Epoch 2810, val loss: 1.0386631488800049
Epoch 2820, training loss: 90.30718231201172 = 0.030599484220147133 + 10.0 * 9.027658462524414
Epoch 2820, val loss: 1.043255090713501
Epoch 2830, training loss: 90.29832458496094 = 0.03022482618689537 + 10.0 * 9.026809692382812
Epoch 2830, val loss: 1.0472315549850464
Epoch 2840, training loss: 90.30290985107422 = 0.029865775257349014 + 10.0 * 9.027303695678711
Epoch 2840, val loss: 1.0510398149490356
Epoch 2850, training loss: 90.31089782714844 = 0.02951263077557087 + 10.0 * 9.028139114379883
Epoch 2850, val loss: 1.05568528175354
Epoch 2860, training loss: 90.29236602783203 = 0.02916105091571808 + 10.0 * 9.026320457458496
Epoch 2860, val loss: 1.0596877336502075
Epoch 2870, training loss: 90.29072570800781 = 0.028819788247346878 + 10.0 * 9.026190757751465
Epoch 2870, val loss: 1.0641882419586182
Epoch 2880, training loss: 90.29829406738281 = 0.028482884168624878 + 10.0 * 9.026981353759766
Epoch 2880, val loss: 1.0682809352874756
Epoch 2890, training loss: 90.30171203613281 = 0.028154874220490456 + 10.0 * 9.027356147766113
Epoch 2890, val loss: 1.0717275142669678
Epoch 2900, training loss: 90.28719329833984 = 0.0278214942663908 + 10.0 * 9.0259370803833
Epoch 2900, val loss: 1.0764447450637817
Epoch 2910, training loss: 90.28450012207031 = 0.027499603107571602 + 10.0 * 9.025700569152832
Epoch 2910, val loss: 1.0807257890701294
Epoch 2920, training loss: 90.2820053100586 = 0.027181759476661682 + 10.0 * 9.025482177734375
Epoch 2920, val loss: 1.0845849514007568
Epoch 2930, training loss: 90.29786682128906 = 0.026873700320720673 + 10.0 * 9.027099609375
Epoch 2930, val loss: 1.0886270999908447
Epoch 2940, training loss: 90.29485321044922 = 0.02657301165163517 + 10.0 * 9.026827812194824
Epoch 2940, val loss: 1.0934929847717285
Epoch 2950, training loss: 90.28939819335938 = 0.02626740001142025 + 10.0 * 9.026312828063965
Epoch 2950, val loss: 1.096351981163025
Epoch 2960, training loss: 90.27692413330078 = 0.025964587926864624 + 10.0 * 9.02509593963623
Epoch 2960, val loss: 1.1011443138122559
Epoch 2970, training loss: 90.27620697021484 = 0.02567168138921261 + 10.0 * 9.025053024291992
Epoch 2970, val loss: 1.1051535606384277
Epoch 2980, training loss: 90.28082275390625 = 0.025386352092027664 + 10.0 * 9.025544166564941
Epoch 2980, val loss: 1.1090019941329956
Epoch 2990, training loss: 90.29463958740234 = 0.025111746042966843 + 10.0 * 9.026952743530273
Epoch 2990, val loss: 1.1133031845092773
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8534
Overall ASR: 0.6871
Flip ASR: 0.6094/1554 nodes
./selected_nodes/Pubmed/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 104.6850814819336 = 1.0943955183029175 + 10.0 * 10.359067916870117
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 319, in _train_with_val
    loss.backward()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 6.21 GiB already allocated; 579.69 MiB free; 6.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.5 der2:0.2 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 104.69085693359375 = 1.0997012853622437 + 10.0 * 10.359115600585938
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 319, in _train_with_val
    loss.backward()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 6.10 GiB already allocated; 521.69 MiB free; 6.36 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.5 der2:0.2 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 104.68721008300781 = 1.0961594581604004 + 10.0 * 10.359105110168457
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 319, in _train_with_val
    loss.backward()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 6.10 GiB already allocated; 521.69 MiB free; 6.36 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.5 der2:0.2 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 104.68121337890625 = 1.0898607969284058 + 10.0 * 10.359135627746582
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 319, in _train_with_val
    loss.backward()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 6.10 GiB already allocated; 519.69 MiB free; 6.36 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.5 der2:0.3 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.3, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 104.69673156738281 = 1.1068799495697021 + 10.0 * 10.35898494720459
Epoch 0, val loss: 1.1057147979736328
Epoch 10, training loss: 104.65181732177734 = 1.094202995300293 + 10.0 * 10.355761528015137
Epoch 10, val loss: 1.092824101448059
Epoch 20, training loss: 104.18231201171875 = 1.0771868228912354 + 10.0 * 10.31051254272461
Epoch 20, val loss: 1.0758848190307617
Epoch 30, training loss: 99.03592681884766 = 1.061038851737976 + 10.0 * 9.797489166259766
Epoch 30, val loss: 1.0600597858428955
Epoch 40, training loss: 96.36332702636719 = 1.0472004413604736 + 10.0 * 9.531612396240234
Epoch 40, val loss: 1.0464820861816406
Epoch 50, training loss: 94.97891235351562 = 1.0339497327804565 + 10.0 * 9.394495964050293
Epoch 50, val loss: 1.0333619117736816
Epoch 60, training loss: 94.43744659423828 = 1.021151065826416 + 10.0 * 9.341629981994629
Epoch 60, val loss: 1.0210282802581787
Epoch 70, training loss: 93.68138885498047 = 1.0113435983657837 + 10.0 * 9.267004013061523
Epoch 70, val loss: 1.0117195844650269
Epoch 80, training loss: 93.2260971069336 = 1.0033084154129028 + 10.0 * 9.222278594970703
Epoch 80, val loss: 1.003688097000122
Epoch 90, training loss: 92.96199035644531 = 0.9942246675491333 + 10.0 * 9.196776390075684
Epoch 90, val loss: 0.9946388006210327
Epoch 100, training loss: 92.76275634765625 = 0.9834761619567871 + 10.0 * 9.17792797088623
Epoch 100, val loss: 0.9840498566627502
Epoch 110, training loss: 92.60197448730469 = 0.9716025590896606 + 10.0 * 9.163037300109863
Epoch 110, val loss: 0.9723505973815918
Epoch 120, training loss: 92.4638900756836 = 0.9588845372200012 + 10.0 * 9.150500297546387
Epoch 120, val loss: 0.959864616394043
Epoch 130, training loss: 92.36309814453125 = 0.9442554116249084 + 10.0 * 9.141884803771973
Epoch 130, val loss: 0.945529580116272
Epoch 140, training loss: 92.27665710449219 = 0.9272425770759583 + 10.0 * 9.134941101074219
Epoch 140, val loss: 0.9288167357444763
Epoch 150, training loss: 92.19895935058594 = 0.9077581763267517 + 10.0 * 9.129119873046875
Epoch 150, val loss: 0.9097674489021301
Epoch 160, training loss: 92.13302612304688 = 0.8854750394821167 + 10.0 * 9.124754905700684
Epoch 160, val loss: 0.8879758715629578
Epoch 170, training loss: 92.07675170898438 = 0.8602327704429626 + 10.0 * 9.121652603149414
Epoch 170, val loss: 0.863369882106781
Epoch 180, training loss: 92.00709533691406 = 0.8322352170944214 + 10.0 * 9.117486000061035
Epoch 180, val loss: 0.8361932635307312
Epoch 190, training loss: 91.93760681152344 = 0.8015027642250061 + 10.0 * 9.11361026763916
Epoch 190, val loss: 0.8064391613006592
Epoch 200, training loss: 91.87350463867188 = 0.7677592039108276 + 10.0 * 9.110574722290039
Epoch 200, val loss: 0.7739515900611877
Epoch 210, training loss: 91.83972930908203 = 0.7311068177223206 + 10.0 * 9.110861778259277
Epoch 210, val loss: 0.7389260530471802
Epoch 220, training loss: 91.75814819335938 = 0.6929190158843994 + 10.0 * 9.106523513793945
Epoch 220, val loss: 0.702629029750824
Epoch 230, training loss: 91.69109344482422 = 0.6540706753730774 + 10.0 * 9.103702545166016
Epoch 230, val loss: 0.666134774684906
Epoch 240, training loss: 91.63724517822266 = 0.6153969168663025 + 10.0 * 9.102185249328613
Epoch 240, val loss: 0.6301571130752563
Epoch 250, training loss: 91.5878677368164 = 0.5781358480453491 + 10.0 * 9.100973129272461
Epoch 250, val loss: 0.5959142446517944
Epoch 260, training loss: 91.52677917480469 = 0.5434303283691406 + 10.0 * 9.098334312438965
Epoch 260, val loss: 0.5645002126693726
Epoch 270, training loss: 91.4757080078125 = 0.5118980407714844 + 10.0 * 9.096380233764648
Epoch 270, val loss: 0.5364109873771667
Epoch 280, training loss: 91.47219848632812 = 0.4838150143623352 + 10.0 * 9.098837852478027
Epoch 280, val loss: 0.5118865370750427
Epoch 290, training loss: 91.40263366699219 = 0.4597516655921936 + 10.0 * 9.094287872314453
Epoch 290, val loss: 0.49134939908981323
Epoch 300, training loss: 91.35514831542969 = 0.4393295645713806 + 10.0 * 9.091581344604492
Epoch 300, val loss: 0.4743036925792694
Epoch 310, training loss: 91.3357162475586 = 0.4219041168689728 + 10.0 * 9.091381072998047
Epoch 310, val loss: 0.460144966840744
Epoch 320, training loss: 91.29946899414062 = 0.4071493446826935 + 10.0 * 9.089231491088867
Epoch 320, val loss: 0.4485163688659668
Epoch 330, training loss: 91.27197265625 = 0.3946380317211151 + 10.0 * 9.087733268737793
Epoch 330, val loss: 0.43893173336982727
Epoch 340, training loss: 91.24917602539062 = 0.38384687900543213 + 10.0 * 9.086532592773438
Epoch 340, val loss: 0.430992990732193
Epoch 350, training loss: 91.23015594482422 = 0.37437641620635986 + 10.0 * 9.085577964782715
Epoch 350, val loss: 0.4242688715457916
Epoch 360, training loss: 91.22109985351562 = 0.36602553725242615 + 10.0 * 9.0855073928833
Epoch 360, val loss: 0.41862189769744873
Epoch 370, training loss: 91.1983871459961 = 0.35870760679244995 + 10.0 * 9.083967208862305
Epoch 370, val loss: 0.413855642080307
Epoch 380, training loss: 91.17920684814453 = 0.35217252373695374 + 10.0 * 9.082703590393066
Epoch 380, val loss: 0.40980321168899536
Epoch 390, training loss: 91.1702880859375 = 0.3462102711200714 + 10.0 * 9.08240795135498
Epoch 390, val loss: 0.4063151478767395
Epoch 400, training loss: 91.14839935302734 = 0.34071898460388184 + 10.0 * 9.080767631530762
Epoch 400, val loss: 0.403163343667984
Epoch 410, training loss: 91.15328979492188 = 0.33566033840179443 + 10.0 * 9.081762313842773
Epoch 410, val loss: 0.4004875123500824
Epoch 420, training loss: 91.13075256347656 = 0.33100178837776184 + 10.0 * 9.079975128173828
Epoch 420, val loss: 0.39816921949386597
Epoch 430, training loss: 91.11248016357422 = 0.32669615745544434 + 10.0 * 9.078577995300293
Epoch 430, val loss: 0.3961253762245178
Epoch 440, training loss: 91.10002136230469 = 0.3226196765899658 + 10.0 * 9.077740669250488
Epoch 440, val loss: 0.3942916691303253
Epoch 450, training loss: 91.1025161743164 = 0.3187347650527954 + 10.0 * 9.078378677368164
Epoch 450, val loss: 0.39267054200172424
Epoch 460, training loss: 91.07846069335938 = 0.3150559961795807 + 10.0 * 9.076340675354004
Epoch 460, val loss: 0.391220360994339
Epoch 470, training loss: 91.0817642211914 = 0.3115439713001251 + 10.0 * 9.077021598815918
Epoch 470, val loss: 0.3899468183517456
Epoch 480, training loss: 91.06353759765625 = 0.30819347500801086 + 10.0 * 9.075533866882324
Epoch 480, val loss: 0.3887723684310913
Epoch 490, training loss: 91.04769897460938 = 0.3049793541431427 + 10.0 * 9.074272155761719
Epoch 490, val loss: 0.38778090476989746
Epoch 500, training loss: 91.0609359741211 = 0.30187371373176575 + 10.0 * 9.075906753540039
Epoch 500, val loss: 0.386888325214386
Epoch 510, training loss: 91.03642272949219 = 0.29890376329421997 + 10.0 * 9.073751449584961
Epoch 510, val loss: 0.38613802194595337
Epoch 520, training loss: 91.0220947265625 = 0.296047568321228 + 10.0 * 9.072604179382324
Epoch 520, val loss: 0.3854094445705414
Epoch 530, training loss: 91.01322937011719 = 0.2932718098163605 + 10.0 * 9.071995735168457
Epoch 530, val loss: 0.38477426767349243
Epoch 540, training loss: 91.01729583740234 = 0.29056620597839355 + 10.0 * 9.072672843933105
Epoch 540, val loss: 0.38425764441490173
Epoch 550, training loss: 91.00182342529297 = 0.2879449427127838 + 10.0 * 9.07138729095459
Epoch 550, val loss: 0.3839096426963806
Epoch 560, training loss: 91.02569580078125 = 0.2853972613811493 + 10.0 * 9.074029922485352
Epoch 560, val loss: 0.3835669457912445
Epoch 570, training loss: 90.98836517333984 = 0.2829512655735016 + 10.0 * 9.070541381835938
Epoch 570, val loss: 0.3832288086414337
Epoch 580, training loss: 90.97312927246094 = 0.2805788815021515 + 10.0 * 9.069254875183105
Epoch 580, val loss: 0.3830176591873169
Epoch 590, training loss: 90.96416473388672 = 0.2782480716705322 + 10.0 * 9.068591117858887
Epoch 590, val loss: 0.38290494680404663
Epoch 600, training loss: 90.95726013183594 = 0.2759517431259155 + 10.0 * 9.068130493164062
Epoch 600, val loss: 0.3827807605266571
Epoch 610, training loss: 90.96453857421875 = 0.27369245886802673 + 10.0 * 9.069085121154785
Epoch 610, val loss: 0.3826812207698822
Epoch 620, training loss: 90.97369384765625 = 0.2714991867542267 + 10.0 * 9.070219039916992
Epoch 620, val loss: 0.38290107250213623
Epoch 630, training loss: 90.94426727294922 = 0.2693866491317749 + 10.0 * 9.067487716674805
Epoch 630, val loss: 0.38287970423698425
Epoch 640, training loss: 90.93082427978516 = 0.26730310916900635 + 10.0 * 9.066351890563965
Epoch 640, val loss: 0.38299936056137085
Epoch 650, training loss: 90.9248046875 = 0.2652396261692047 + 10.0 * 9.065957069396973
Epoch 650, val loss: 0.38314321637153625
Epoch 660, training loss: 90.94554138183594 = 0.26320624351501465 + 10.0 * 9.068233489990234
Epoch 660, val loss: 0.38341960310935974
Epoch 670, training loss: 90.91377258300781 = 0.2612256109714508 + 10.0 * 9.065255165100098
Epoch 670, val loss: 0.383654922246933
Epoch 680, training loss: 90.90925598144531 = 0.2592792212963104 + 10.0 * 9.064997673034668
Epoch 680, val loss: 0.3839588761329651
Epoch 690, training loss: 90.90760803222656 = 0.2573532164096832 + 10.0 * 9.065025329589844
Epoch 690, val loss: 0.3843499720096588
Epoch 700, training loss: 90.91017150878906 = 0.2554539442062378 + 10.0 * 9.065471649169922
Epoch 700, val loss: 0.38484638929367065
Epoch 710, training loss: 90.89315795898438 = 0.2535935044288635 + 10.0 * 9.063956260681152
Epoch 710, val loss: 0.3852498531341553
Epoch 720, training loss: 90.88386535644531 = 0.2517535090446472 + 10.0 * 9.063211441040039
Epoch 720, val loss: 0.3857969641685486
Epoch 730, training loss: 90.87706756591797 = 0.24992653727531433 + 10.0 * 9.062714576721191
Epoch 730, val loss: 0.38635575771331787
Epoch 740, training loss: 90.88096618652344 = 0.2481144219636917 + 10.0 * 9.063284873962402
Epoch 740, val loss: 0.38701575994491577
Epoch 750, training loss: 90.88499450683594 = 0.24632617831230164 + 10.0 * 9.063867568969727
Epoch 750, val loss: 0.38760265707969666
Epoch 760, training loss: 90.86067199707031 = 0.24459144473075867 + 10.0 * 9.06160831451416
Epoch 760, val loss: 0.3882356584072113
Epoch 770, training loss: 90.8545150756836 = 0.24286653101444244 + 10.0 * 9.061164855957031
Epoch 770, val loss: 0.3889787197113037
Epoch 780, training loss: 90.85088348388672 = 0.24115124344825745 + 10.0 * 9.060973167419434
Epoch 780, val loss: 0.3897232413291931
Epoch 790, training loss: 90.87386322021484 = 0.2394508570432663 + 10.0 * 9.063441276550293
Epoch 790, val loss: 0.3905792236328125
Epoch 800, training loss: 90.84465026855469 = 0.23778030276298523 + 10.0 * 9.060687065124512
Epoch 800, val loss: 0.39140233397483826
Epoch 810, training loss: 90.83747863769531 = 0.23612743616104126 + 10.0 * 9.060134887695312
Epoch 810, val loss: 0.39228320121765137
Epoch 820, training loss: 90.84098815917969 = 0.2344873994588852 + 10.0 * 9.060649871826172
Epoch 820, val loss: 0.39315181970596313
Epoch 830, training loss: 90.82461547851562 = 0.2328626960515976 + 10.0 * 9.059175491333008
Epoch 830, val loss: 0.3941400349140167
Epoch 840, training loss: 90.82217407226562 = 0.2312474250793457 + 10.0 * 9.05909252166748
Epoch 840, val loss: 0.39515233039855957
Epoch 850, training loss: 90.83805847167969 = 0.22964756190776825 + 10.0 * 9.06084156036377
Epoch 850, val loss: 0.3962166905403137
Epoch 860, training loss: 90.8126220703125 = 0.22807373106479645 + 10.0 * 9.058454513549805
Epoch 860, val loss: 0.39727213978767395
Epoch 870, training loss: 90.81523895263672 = 0.226511612534523 + 10.0 * 9.058873176574707
Epoch 870, val loss: 0.398436963558197
Epoch 880, training loss: 90.81204223632812 = 0.22496150434017181 + 10.0 * 9.058708190917969
Epoch 880, val loss: 0.3995141386985779
Epoch 890, training loss: 90.79704284667969 = 0.22342239320278168 + 10.0 * 9.05736255645752
Epoch 890, val loss: 0.40067338943481445
Epoch 900, training loss: 90.79238891601562 = 0.22189563512802124 + 10.0 * 9.057049751281738
Epoch 900, val loss: 0.40191683173179626
Epoch 910, training loss: 90.79349517822266 = 0.22037038207054138 + 10.0 * 9.057312965393066
Epoch 910, val loss: 0.40314751863479614
Epoch 920, training loss: 90.7944564819336 = 0.21886160969734192 + 10.0 * 9.0575590133667
Epoch 920, val loss: 0.40457218885421753
Epoch 930, training loss: 90.78828430175781 = 0.2173805981874466 + 10.0 * 9.057089805603027
Epoch 930, val loss: 0.4057357907295227
Epoch 940, training loss: 90.77454376220703 = 0.2159058004617691 + 10.0 * 9.055864334106445
Epoch 940, val loss: 0.40704578161239624
Epoch 950, training loss: 90.7725601196289 = 0.21443882584571838 + 10.0 * 9.055811882019043
Epoch 950, val loss: 0.4083728790283203
Epoch 960, training loss: 90.79051971435547 = 0.21298013627529144 + 10.0 * 9.057753562927246
Epoch 960, val loss: 0.4097630977630615
Epoch 970, training loss: 90.77017211914062 = 0.21153604984283447 + 10.0 * 9.055864334106445
Epoch 970, val loss: 0.41126829385757446
Epoch 980, training loss: 90.7606430053711 = 0.21010138094425201 + 10.0 * 9.055054664611816
Epoch 980, val loss: 0.41263124346733093
Epoch 990, training loss: 90.78336334228516 = 0.20867639780044556 + 10.0 * 9.05746841430664
Epoch 990, val loss: 0.4141528010368347
Epoch 1000, training loss: 90.76615142822266 = 0.20726270973682404 + 10.0 * 9.055889129638672
Epoch 1000, val loss: 0.41567525267601013
Epoch 1010, training loss: 90.74706268310547 = 0.2058638632297516 + 10.0 * 9.054120063781738
Epoch 1010, val loss: 0.41707372665405273
Epoch 1020, training loss: 90.74325561523438 = 0.2044682800769806 + 10.0 * 9.053878784179688
Epoch 1020, val loss: 0.41861692070961
Epoch 1030, training loss: 90.74102783203125 = 0.20307756960391998 + 10.0 * 9.053794860839844
Epoch 1030, val loss: 0.42016997933387756
Epoch 1040, training loss: 90.74858856201172 = 0.2016957849264145 + 10.0 * 9.054689407348633
Epoch 1040, val loss: 0.4217648208141327
Epoch 1050, training loss: 90.7355728149414 = 0.20032815635204315 + 10.0 * 9.0535249710083
Epoch 1050, val loss: 0.42337939143180847
Epoch 1060, training loss: 90.73875427246094 = 0.19896717369556427 + 10.0 * 9.05397891998291
Epoch 1060, val loss: 0.42498284578323364
Epoch 1070, training loss: 90.72904968261719 = 0.19761690497398376 + 10.0 * 9.053143501281738
Epoch 1070, val loss: 0.4266871511936188
Epoch 1080, training loss: 90.72032928466797 = 0.19627973437309265 + 10.0 * 9.052404403686523
Epoch 1080, val loss: 0.42824408411979675
Epoch 1090, training loss: 90.71504974365234 = 0.19494511187076569 + 10.0 * 9.052010536193848
Epoch 1090, val loss: 0.42995718121528625
Epoch 1100, training loss: 90.7113037109375 = 0.193612739443779 + 10.0 * 9.051769256591797
Epoch 1100, val loss: 0.43163278698921204
Epoch 1110, training loss: 90.75252532958984 = 0.19229412078857422 + 10.0 * 9.056023597717285
Epoch 1110, val loss: 0.43360504508018494
Epoch 1120, training loss: 90.7049560546875 = 0.190977543592453 + 10.0 * 9.051397323608398
Epoch 1120, val loss: 0.4350276589393616
Epoch 1130, training loss: 90.70357513427734 = 0.1896766573190689 + 10.0 * 9.051389694213867
Epoch 1130, val loss: 0.43673399090766907
Epoch 1140, training loss: 90.69466400146484 = 0.18837405741214752 + 10.0 * 9.050628662109375
Epoch 1140, val loss: 0.4384863078594208
Epoch 1150, training loss: 90.69538879394531 = 0.18707424402236938 + 10.0 * 9.05083179473877
Epoch 1150, val loss: 0.4402947723865509
Epoch 1160, training loss: 90.70870971679688 = 0.18578395247459412 + 10.0 * 9.052292823791504
Epoch 1160, val loss: 0.44220027327537537
Epoch 1170, training loss: 90.68540954589844 = 0.18450628221035004 + 10.0 * 9.050089836120605
Epoch 1170, val loss: 0.44389885663986206
Epoch 1180, training loss: 90.68449401855469 = 0.18323233723640442 + 10.0 * 9.050126075744629
Epoch 1180, val loss: 0.445853054523468
Epoch 1190, training loss: 90.69985961914062 = 0.18196263909339905 + 10.0 * 9.051790237426758
Epoch 1190, val loss: 0.4477006494998932
Epoch 1200, training loss: 90.67858123779297 = 0.18069638311862946 + 10.0 * 9.049788475036621
Epoch 1200, val loss: 0.4495479166507721
Epoch 1210, training loss: 90.67256164550781 = 0.17944279313087463 + 10.0 * 9.049311637878418
Epoch 1210, val loss: 0.45149460434913635
Epoch 1220, training loss: 90.66837310791016 = 0.1781853586435318 + 10.0 * 9.049018859863281
Epoch 1220, val loss: 0.4533994495868683
Epoch 1230, training loss: 90.70038604736328 = 0.1769411563873291 + 10.0 * 9.05234432220459
Epoch 1230, val loss: 0.45542192459106445
Epoch 1240, training loss: 90.66999816894531 = 0.17568956315517426 + 10.0 * 9.049430847167969
Epoch 1240, val loss: 0.45736220479011536
Epoch 1250, training loss: 90.6607666015625 = 0.174460306763649 + 10.0 * 9.048630714416504
Epoch 1250, val loss: 0.45935261249542236
Epoch 1260, training loss: 90.65202331542969 = 0.17323175072669983 + 10.0 * 9.047879219055176
Epoch 1260, val loss: 0.4613232910633087
Epoch 1270, training loss: 90.64923095703125 = 0.17200236022472382 + 10.0 * 9.047722816467285
Epoch 1270, val loss: 0.46335092186927795
Epoch 1280, training loss: 90.67228698730469 = 0.17077253758907318 + 10.0 * 9.050151824951172
Epoch 1280, val loss: 0.4653700888156891
Epoch 1290, training loss: 90.66558074951172 = 0.1695537567138672 + 10.0 * 9.049602508544922
Epoch 1290, val loss: 0.46770185232162476
Epoch 1300, training loss: 90.63892364501953 = 0.1683410108089447 + 10.0 * 9.04705810546875
Epoch 1300, val loss: 0.4696848690509796
Epoch 1310, training loss: 90.6364974975586 = 0.16712789237499237 + 10.0 * 9.046936988830566
Epoch 1310, val loss: 0.4717303216457367
Epoch 1320, training loss: 90.63175964355469 = 0.16591736674308777 + 10.0 * 9.046584129333496
Epoch 1320, val loss: 0.4738873839378357
Epoch 1330, training loss: 90.62814331054688 = 0.1647043526172638 + 10.0 * 9.046343803405762
Epoch 1330, val loss: 0.47611740231513977
Epoch 1340, training loss: 90.64696502685547 = 0.16349667310714722 + 10.0 * 9.048346519470215
Epoch 1340, val loss: 0.4783982038497925
Epoch 1350, training loss: 90.6429443359375 = 0.1622956544160843 + 10.0 * 9.048065185546875
Epoch 1350, val loss: 0.48052364587783813
Epoch 1360, training loss: 90.62173461914062 = 0.1611020565032959 + 10.0 * 9.046063423156738
Epoch 1360, val loss: 0.4827859103679657
Epoch 1370, training loss: 90.61990356445312 = 0.15991446375846863 + 10.0 * 9.045999526977539
Epoch 1370, val loss: 0.48500174283981323
Epoch 1380, training loss: 90.61254119873047 = 0.1587272584438324 + 10.0 * 9.045381546020508
Epoch 1380, val loss: 0.4873138964176178
Epoch 1390, training loss: 90.61107635498047 = 0.157542884349823 + 10.0 * 9.045353889465332
Epoch 1390, val loss: 0.48962023854255676
Epoch 1400, training loss: 90.64434051513672 = 0.15637169778347015 + 10.0 * 9.048796653747559
Epoch 1400, val loss: 0.49208757281303406
Epoch 1410, training loss: 90.60801696777344 = 0.1551830768585205 + 10.0 * 9.045283317565918
Epoch 1410, val loss: 0.49429380893707275
Epoch 1420, training loss: 90.60308074951172 = 0.15401434898376465 + 10.0 * 9.044906616210938
Epoch 1420, val loss: 0.49662864208221436
Epoch 1430, training loss: 90.60074615478516 = 0.15285177528858185 + 10.0 * 9.04478931427002
Epoch 1430, val loss: 0.4990823566913605
Epoch 1440, training loss: 90.6114273071289 = 0.15168854594230652 + 10.0 * 9.045973777770996
Epoch 1440, val loss: 0.5015131831169128
Epoch 1450, training loss: 90.5950927734375 = 0.15052960813045502 + 10.0 * 9.044456481933594
Epoch 1450, val loss: 0.5039147138595581
Epoch 1460, training loss: 90.58828735351562 = 0.14936868846416473 + 10.0 * 9.043891906738281
Epoch 1460, val loss: 0.506483793258667
Epoch 1470, training loss: 90.58899688720703 = 0.148212268948555 + 10.0 * 9.044078826904297
Epoch 1470, val loss: 0.5091131925582886
Epoch 1480, training loss: 90.59879302978516 = 0.1470591127872467 + 10.0 * 9.045173645019531
Epoch 1480, val loss: 0.511633038520813
Epoch 1490, training loss: 90.5828857421875 = 0.14591571688652039 + 10.0 * 9.043697357177734
Epoch 1490, val loss: 0.5141794085502625
Epoch 1500, training loss: 90.57612609863281 = 0.144769087433815 + 10.0 * 9.043135643005371
Epoch 1500, val loss: 0.5166205167770386
Epoch 1510, training loss: 90.57188415527344 = 0.1436300277709961 + 10.0 * 9.042825698852539
Epoch 1510, val loss: 0.5192009210586548
Epoch 1520, training loss: 90.57054138183594 = 0.14249102771282196 + 10.0 * 9.042804718017578
Epoch 1520, val loss: 0.5218697190284729
Epoch 1530, training loss: 90.5971908569336 = 0.14135026931762695 + 10.0 * 9.045583724975586
Epoch 1530, val loss: 0.524467945098877
Epoch 1540, training loss: 90.57110595703125 = 0.14021380245685577 + 10.0 * 9.043088912963867
Epoch 1540, val loss: 0.5271437168121338
Epoch 1550, training loss: 90.5638198852539 = 0.13908520340919495 + 10.0 * 9.042473793029785
Epoch 1550, val loss: 0.5299639701843262
Epoch 1560, training loss: 90.55703735351562 = 0.1379508227109909 + 10.0 * 9.041909217834473
Epoch 1560, val loss: 0.5325796604156494
Epoch 1570, training loss: 90.5603256225586 = 0.13682472705841064 + 10.0 * 9.042349815368652
Epoch 1570, val loss: 0.5352523326873779
Epoch 1580, training loss: 90.56146240234375 = 0.13569532334804535 + 10.0 * 9.042576789855957
Epoch 1580, val loss: 0.5381526947021484
Epoch 1590, training loss: 90.55400848388672 = 0.13458208739757538 + 10.0 * 9.041942596435547
Epoch 1590, val loss: 0.5408114194869995
Epoch 1600, training loss: 90.54652404785156 = 0.13346563279628754 + 10.0 * 9.041305541992188
Epoch 1600, val loss: 0.5437664985656738
Epoch 1610, training loss: 90.54137420654297 = 0.1323516070842743 + 10.0 * 9.040902137756348
Epoch 1610, val loss: 0.546602725982666
Epoch 1620, training loss: 90.53889465332031 = 0.13123738765716553 + 10.0 * 9.040765762329102
Epoch 1620, val loss: 0.549500048160553
Epoch 1630, training loss: 90.57549285888672 = 0.13013090193271637 + 10.0 * 9.044535636901855
Epoch 1630, val loss: 0.5527704358100891
Epoch 1640, training loss: 90.56070709228516 = 0.1290237009525299 + 10.0 * 9.043169021606445
Epoch 1640, val loss: 0.5552067756652832
Epoch 1650, training loss: 90.53524780273438 = 0.12791913747787476 + 10.0 * 9.040732383728027
Epoch 1650, val loss: 0.5580812692642212
Epoch 1660, training loss: 90.52690124511719 = 0.12681570649147034 + 10.0 * 9.040008544921875
Epoch 1660, val loss: 0.5612084865570068
Epoch 1670, training loss: 90.5245132446289 = 0.1257188767194748 + 10.0 * 9.03987979888916
Epoch 1670, val loss: 0.5643629431724548
Epoch 1680, training loss: 90.53850555419922 = 0.12462679296731949 + 10.0 * 9.041387557983398
Epoch 1680, val loss: 0.5675373077392578
Epoch 1690, training loss: 90.53349304199219 = 0.1235371083021164 + 10.0 * 9.040995597839355
Epoch 1690, val loss: 0.570406436920166
Epoch 1700, training loss: 90.51744079589844 = 0.12245599925518036 + 10.0 * 9.039498329162598
Epoch 1700, val loss: 0.5734609365463257
Epoch 1710, training loss: 90.51658630371094 = 0.12137449532747269 + 10.0 * 9.039521217346191
Epoch 1710, val loss: 0.5765228271484375
Epoch 1720, training loss: 90.51093292236328 = 0.12029678374528885 + 10.0 * 9.039063453674316
Epoch 1720, val loss: 0.5796993374824524
Epoch 1730, training loss: 90.51934051513672 = 0.11921893805265427 + 10.0 * 9.04001235961914
Epoch 1730, val loss: 0.5828844308853149
Epoch 1740, training loss: 90.507080078125 = 0.11814286559820175 + 10.0 * 9.038893699645996
Epoch 1740, val loss: 0.5861875414848328
Epoch 1750, training loss: 90.51241302490234 = 0.1170765683054924 + 10.0 * 9.039533615112305
Epoch 1750, val loss: 0.5893054604530334
Epoch 1760, training loss: 90.49984741210938 = 0.11600921303033829 + 10.0 * 9.038383483886719
Epoch 1760, val loss: 0.5924502015113831
Epoch 1770, training loss: 90.49922943115234 = 0.114944688975811 + 10.0 * 9.03842830657959
Epoch 1770, val loss: 0.5957520008087158
Epoch 1780, training loss: 90.52205657958984 = 0.11388901621103287 + 10.0 * 9.040816307067871
Epoch 1780, val loss: 0.5990987420082092
Epoch 1790, training loss: 90.50074005126953 = 0.11281950026750565 + 10.0 * 9.038792610168457
Epoch 1790, val loss: 0.6024409532546997
Epoch 1800, training loss: 90.50845336914062 = 0.1117720752954483 + 10.0 * 9.039668083190918
Epoch 1800, val loss: 0.6059152483940125
Epoch 1810, training loss: 90.48870086669922 = 0.11070434749126434 + 10.0 * 9.037799835205078
Epoch 1810, val loss: 0.6090298891067505
Epoch 1820, training loss: 90.48693084716797 = 0.10965533554553986 + 10.0 * 9.037727355957031
Epoch 1820, val loss: 0.6125046014785767
Epoch 1830, training loss: 90.4978256225586 = 0.10861415416002274 + 10.0 * 9.038921356201172
Epoch 1830, val loss: 0.6160265803337097
Epoch 1840, training loss: 90.48124694824219 = 0.1075618639588356 + 10.0 * 9.037368774414062
Epoch 1840, val loss: 0.6190779209136963
Epoch 1850, training loss: 90.477294921875 = 0.10652182996273041 + 10.0 * 9.037076950073242
Epoch 1850, val loss: 0.6226994395256042
Epoch 1860, training loss: 90.48294830322266 = 0.10548505932092667 + 10.0 * 9.03774642944336
Epoch 1860, val loss: 0.6262018084526062
Epoch 1870, training loss: 90.4740219116211 = 0.10444821417331696 + 10.0 * 9.036957740783691
Epoch 1870, val loss: 0.6295962333679199
Epoch 1880, training loss: 90.4679946899414 = 0.10341782867908478 + 10.0 * 9.036458015441895
Epoch 1880, val loss: 0.633161723613739
Epoch 1890, training loss: 90.46760559082031 = 0.10239074379205704 + 10.0 * 9.036520957946777
Epoch 1890, val loss: 0.636675238609314
Epoch 1900, training loss: 90.48865509033203 = 0.10137058049440384 + 10.0 * 9.038728713989258
Epoch 1900, val loss: 0.64020836353302
Epoch 1910, training loss: 90.46733093261719 = 0.10034032166004181 + 10.0 * 9.036699295043945
Epoch 1910, val loss: 0.6437704563140869
Epoch 1920, training loss: 90.4609603881836 = 0.09932093322277069 + 10.0 * 9.036164283752441
Epoch 1920, val loss: 0.6474512219429016
Epoch 1930, training loss: 90.46793365478516 = 0.09830792248249054 + 10.0 * 9.036962509155273
Epoch 1930, val loss: 0.651150107383728
Epoch 1940, training loss: 90.45671844482422 = 0.0972888171672821 + 10.0 * 9.035943031311035
Epoch 1940, val loss: 0.654744565486908
Epoch 1950, training loss: 90.45254516601562 = 0.09628013521432877 + 10.0 * 9.035626411437988
Epoch 1950, val loss: 0.6583221554756165
Epoch 1960, training loss: 90.44989013671875 = 0.09527435153722763 + 10.0 * 9.03546142578125
Epoch 1960, val loss: 0.6620601415634155
Epoch 1970, training loss: 90.48444366455078 = 0.09428422898054123 + 10.0 * 9.039015769958496
Epoch 1970, val loss: 0.665450930595398
Epoch 1980, training loss: 90.46772766113281 = 0.0932895839214325 + 10.0 * 9.037443161010742
Epoch 1980, val loss: 0.6698004603385925
Epoch 1990, training loss: 90.44174194335938 = 0.09229820966720581 + 10.0 * 9.034944534301758
Epoch 1990, val loss: 0.6734009385108948
Epoch 2000, training loss: 90.44096374511719 = 0.09131718426942825 + 10.0 * 9.034964561462402
Epoch 2000, val loss: 0.6771493554115295
Epoch 2010, training loss: 90.4363021850586 = 0.090341717004776 + 10.0 * 9.03459644317627
Epoch 2010, val loss: 0.6811150312423706
Epoch 2020, training loss: 90.45784759521484 = 0.08937597274780273 + 10.0 * 9.036847114562988
Epoch 2020, val loss: 0.6851889491081238
Epoch 2030, training loss: 90.43733215332031 = 0.08840518444776535 + 10.0 * 9.034893035888672
Epoch 2030, val loss: 0.688716471195221
Epoch 2040, training loss: 90.43334197998047 = 0.087439626455307 + 10.0 * 9.034589767456055
Epoch 2040, val loss: 0.6927791833877563
Epoch 2050, training loss: 90.4435806274414 = 0.08648873120546341 + 10.0 * 9.035709381103516
Epoch 2050, val loss: 0.6967755556106567
Epoch 2060, training loss: 90.42684173583984 = 0.0855347141623497 + 10.0 * 9.034131050109863
Epoch 2060, val loss: 0.7006998062133789
Epoch 2070, training loss: 90.42711639404297 = 0.08459389209747314 + 10.0 * 9.034252166748047
Epoch 2070, val loss: 0.7047852277755737
Epoch 2080, training loss: 90.42191314697266 = 0.08365888148546219 + 10.0 * 9.033825874328613
Epoch 2080, val loss: 0.7087160348892212
Epoch 2090, training loss: 90.41976165771484 = 0.0827283188700676 + 10.0 * 9.033703804016113
Epoch 2090, val loss: 0.7128138542175293
Epoch 2100, training loss: 90.45072937011719 = 0.08181270956993103 + 10.0 * 9.03689193725586
Epoch 2100, val loss: 0.7168471217155457
Epoch 2110, training loss: 90.42448425292969 = 0.08089441806077957 + 10.0 * 9.034358978271484
Epoch 2110, val loss: 0.7206751704216003
Epoch 2120, training loss: 90.41222381591797 = 0.07998014986515045 + 10.0 * 9.033224105834961
Epoch 2120, val loss: 0.724967360496521
Epoch 2130, training loss: 90.4178237915039 = 0.07908265292644501 + 10.0 * 9.033873558044434
Epoch 2130, val loss: 0.7291763424873352
Epoch 2140, training loss: 90.41659545898438 = 0.07818540930747986 + 10.0 * 9.033841133117676
Epoch 2140, val loss: 0.7333450317382812
Epoch 2150, training loss: 90.41120147705078 = 0.07729846239089966 + 10.0 * 9.033390045166016
Epoch 2150, val loss: 0.7376021146774292
Epoch 2160, training loss: 90.40986633300781 = 0.07642032951116562 + 10.0 * 9.033344268798828
Epoch 2160, val loss: 0.7419850826263428
Epoch 2170, training loss: 90.4042739868164 = 0.07554216682910919 + 10.0 * 9.032873153686523
Epoch 2170, val loss: 0.74607253074646
Epoch 2180, training loss: 90.39761352539062 = 0.07466944307088852 + 10.0 * 9.032294273376465
Epoch 2180, val loss: 0.7502215504646301
Epoch 2190, training loss: 90.40645599365234 = 0.07381077855825424 + 10.0 * 9.03326416015625
Epoch 2190, val loss: 0.7545149326324463
Epoch 2200, training loss: 90.4075927734375 = 0.07296249270439148 + 10.0 * 9.033463478088379
Epoch 2200, val loss: 0.7592573761940002
Epoch 2210, training loss: 90.3956069946289 = 0.07210786640644073 + 10.0 * 9.032350540161133
Epoch 2210, val loss: 0.7632677555084229
Epoch 2220, training loss: 90.38990020751953 = 0.07126234471797943 + 10.0 * 9.031864166259766
Epoch 2220, val loss: 0.7677017450332642
Epoch 2230, training loss: 90.38728332519531 = 0.07042605429887772 + 10.0 * 9.031685829162598
Epoch 2230, val loss: 0.7719400525093079
Epoch 2240, training loss: 90.3951187133789 = 0.06960012018680573 + 10.0 * 9.032551765441895
Epoch 2240, val loss: 0.7764104604721069
Epoch 2250, training loss: 90.3825454711914 = 0.06877520680427551 + 10.0 * 9.031376838684082
Epoch 2250, val loss: 0.7806654572486877
Epoch 2260, training loss: 90.40128326416016 = 0.06797048449516296 + 10.0 * 9.033330917358398
Epoch 2260, val loss: 0.7848695516586304
Epoch 2270, training loss: 90.3826675415039 = 0.06715627014636993 + 10.0 * 9.031551361083984
Epoch 2270, val loss: 0.7895667552947998
Epoch 2280, training loss: 90.38069152832031 = 0.06636177003383636 + 10.0 * 9.03143310546875
Epoch 2280, val loss: 0.7937548756599426
Epoch 2290, training loss: 90.37869262695312 = 0.06557216495275497 + 10.0 * 9.031311988830566
Epoch 2290, val loss: 0.7982977628707886
Epoch 2300, training loss: 90.38269805908203 = 0.06479395180940628 + 10.0 * 9.031789779663086
Epoch 2300, val loss: 0.8027917146682739
Epoch 2310, training loss: 90.38507843017578 = 0.06402432173490524 + 10.0 * 9.032105445861816
Epoch 2310, val loss: 0.8074054718017578
Epoch 2320, training loss: 90.37117004394531 = 0.06324818730354309 + 10.0 * 9.030792236328125
Epoch 2320, val loss: 0.811491847038269
Epoch 2330, training loss: 90.36814880371094 = 0.06249699741601944 + 10.0 * 9.03056526184082
Epoch 2330, val loss: 0.8158485293388367
Epoch 2340, training loss: 90.3651351928711 = 0.06174805387854576 + 10.0 * 9.030339241027832
Epoch 2340, val loss: 0.8202293515205383
Epoch 2350, training loss: 90.36360168457031 = 0.061007238924503326 + 10.0 * 9.03026008605957
Epoch 2350, val loss: 0.8248177170753479
Epoch 2360, training loss: 90.39710235595703 = 0.06028364226222038 + 10.0 * 9.033681869506836
Epoch 2360, val loss: 0.8289093375205994
Epoch 2370, training loss: 90.36907196044922 = 0.05955547094345093 + 10.0 * 9.030951499938965
Epoch 2370, val loss: 0.8338984847068787
Epoch 2380, training loss: 90.35881805419922 = 0.05883549153804779 + 10.0 * 9.029997825622559
Epoch 2380, val loss: 0.8382155895233154
Epoch 2390, training loss: 90.35472106933594 = 0.058128103613853455 + 10.0 * 9.029659271240234
Epoch 2390, val loss: 0.8428258895874023
Epoch 2400, training loss: 90.3936538696289 = 0.05744210630655289 + 10.0 * 9.033620834350586
Epoch 2400, val loss: 0.8471420407295227
Epoch 2410, training loss: 90.36719512939453 = 0.05674199387431145 + 10.0 * 9.031045913696289
Epoch 2410, val loss: 0.8517323732376099
Epoch 2420, training loss: 90.3489761352539 = 0.05605245754122734 + 10.0 * 9.029292106628418
Epoch 2420, val loss: 0.8561840057373047
Epoch 2430, training loss: 90.34562683105469 = 0.0553758330643177 + 10.0 * 9.029025077819824
Epoch 2430, val loss: 0.8607463240623474
Epoch 2440, training loss: 90.34478759765625 = 0.054707203060388565 + 10.0 * 9.029007911682129
Epoch 2440, val loss: 0.8653717041015625
Epoch 2450, training loss: 90.38018035888672 = 0.05405527353286743 + 10.0 * 9.032612800598145
Epoch 2450, val loss: 0.8701945543289185
Epoch 2460, training loss: 90.35733795166016 = 0.05340184271335602 + 10.0 * 9.030393600463867
Epoch 2460, val loss: 0.874168336391449
Epoch 2470, training loss: 90.34625244140625 = 0.05274707451462746 + 10.0 * 9.029350280761719
Epoch 2470, val loss: 0.8789143562316895
Epoch 2480, training loss: 90.35054016113281 = 0.05211200565099716 + 10.0 * 9.0298433303833
Epoch 2480, val loss: 0.8831876516342163
Epoch 2490, training loss: 90.34581756591797 = 0.05148467794060707 + 10.0 * 9.029433250427246
Epoch 2490, val loss: 0.8875069618225098
Epoch 2500, training loss: 90.33450317382812 = 0.05085939168930054 + 10.0 * 9.028364181518555
Epoch 2500, val loss: 0.892553448677063
Epoch 2510, training loss: 90.33222198486328 = 0.050245992839336395 + 10.0 * 9.028197288513184
Epoch 2510, val loss: 0.8969638347625732
Epoch 2520, training loss: 90.33135986328125 = 0.04964109882712364 + 10.0 * 9.02817153930664
Epoch 2520, val loss: 0.9013718962669373
Epoch 2530, training loss: 90.37901306152344 = 0.0490582212805748 + 10.0 * 9.032995223999023
Epoch 2530, val loss: 0.905660092830658
Epoch 2540, training loss: 90.3276138305664 = 0.04845348745584488 + 10.0 * 9.027915954589844
Epoch 2540, val loss: 0.9105551838874817
Epoch 2550, training loss: 90.32793426513672 = 0.04787057265639305 + 10.0 * 9.028006553649902
Epoch 2550, val loss: 0.915166437625885
Epoch 2560, training loss: 90.32227325439453 = 0.04729320481419563 + 10.0 * 9.027498245239258
Epoch 2560, val loss: 0.9196149706840515
Epoch 2570, training loss: 90.3246841430664 = 0.046728625893592834 + 10.0 * 9.027795791625977
Epoch 2570, val loss: 0.924278736114502
Epoch 2580, training loss: 90.34709167480469 = 0.04617929831147194 + 10.0 * 9.030091285705566
Epoch 2580, val loss: 0.9289639592170715
Epoch 2590, training loss: 90.32844543457031 = 0.04561443254351616 + 10.0 * 9.02828311920166
Epoch 2590, val loss: 0.9329929351806641
Epoch 2600, training loss: 90.32276153564453 = 0.045065004378557205 + 10.0 * 9.027769088745117
Epoch 2600, val loss: 0.9374210834503174
Epoch 2610, training loss: 90.32006072998047 = 0.044526513665914536 + 10.0 * 9.02755355834961
Epoch 2610, val loss: 0.9418733716011047
Epoch 2620, training loss: 90.31491088867188 = 0.043990641832351685 + 10.0 * 9.027091979980469
Epoch 2620, val loss: 0.9462884664535522
Epoch 2630, training loss: 90.33209228515625 = 0.043470464646816254 + 10.0 * 9.028861999511719
Epoch 2630, val loss: 0.950631856918335
Epoch 2640, training loss: 90.3166732788086 = 0.04294914752244949 + 10.0 * 9.027372360229492
Epoch 2640, val loss: 0.9555637836456299
Epoch 2650, training loss: 90.31089782714844 = 0.0424361415207386 + 10.0 * 9.026845932006836
Epoch 2650, val loss: 0.9596835970878601
Epoch 2660, training loss: 90.31749725341797 = 0.04193572327494621 + 10.0 * 9.027555465698242
Epoch 2660, val loss: 0.9639787673950195
Epoch 2670, training loss: 90.31025695800781 = 0.041436225175857544 + 10.0 * 9.02688217163086
Epoch 2670, val loss: 0.9685249328613281
Epoch 2680, training loss: 90.3169937133789 = 0.04094839468598366 + 10.0 * 9.027605056762695
Epoch 2680, val loss: 0.9726216197013855
Epoch 2690, training loss: 90.30418395996094 = 0.04046293720602989 + 10.0 * 9.026371955871582
Epoch 2690, val loss: 0.9770160913467407
Epoch 2700, training loss: 90.301513671875 = 0.03998700529336929 + 10.0 * 9.026152610778809
Epoch 2700, val loss: 0.9815775156021118
Epoch 2710, training loss: 90.30746459960938 = 0.03952380642294884 + 10.0 * 9.02679443359375
Epoch 2710, val loss: 0.9857944846153259
Epoch 2720, training loss: 90.31214904785156 = 0.03906254470348358 + 10.0 * 9.027308464050293
Epoch 2720, val loss: 0.9902749061584473
Epoch 2730, training loss: 90.30216979980469 = 0.03860108181834221 + 10.0 * 9.02635669708252
Epoch 2730, val loss: 0.9947336316108704
Epoch 2740, training loss: 90.29597473144531 = 0.038150496780872345 + 10.0 * 9.025782585144043
Epoch 2740, val loss: 0.9986793398857117
Epoch 2750, training loss: 90.29413604736328 = 0.03770766407251358 + 10.0 * 9.025643348693848
Epoch 2750, val loss: 1.0028880834579468
Epoch 2760, training loss: 90.31095123291016 = 0.03727857768535614 + 10.0 * 9.02736759185791
Epoch 2760, val loss: 1.0070282220840454
Epoch 2770, training loss: 90.29071044921875 = 0.036837559193372726 + 10.0 * 9.02538776397705
Epoch 2770, val loss: 1.0114785432815552
Epoch 2780, training loss: 90.2899398803711 = 0.036410827189683914 + 10.0 * 9.02535343170166
Epoch 2780, val loss: 1.0158026218414307
Epoch 2790, training loss: 90.30599212646484 = 0.03599729761481285 + 10.0 * 9.026999473571777
Epoch 2790, val loss: 1.0203288793563843
Epoch 2800, training loss: 90.2960205078125 = 0.03558143600821495 + 10.0 * 9.026043891906738
Epoch 2800, val loss: 1.0243669748306274
Epoch 2810, training loss: 90.29315948486328 = 0.035168103873729706 + 10.0 * 9.025798797607422
Epoch 2810, val loss: 1.0281474590301514
Epoch 2820, training loss: 90.28694915771484 = 0.03476488217711449 + 10.0 * 9.02521800994873
Epoch 2820, val loss: 1.032267451286316
Epoch 2830, training loss: 90.28228759765625 = 0.034368325024843216 + 10.0 * 9.024791717529297
Epoch 2830, val loss: 1.0365573167800903
Epoch 2840, training loss: 90.28326416015625 = 0.033977821469306946 + 10.0 * 9.024928092956543
Epoch 2840, val loss: 1.0406017303466797
Epoch 2850, training loss: 90.2960205078125 = 0.03359803557395935 + 10.0 * 9.02624225616455
Epoch 2850, val loss: 1.0442858934402466
Epoch 2860, training loss: 90.2881851196289 = 0.03320903703570366 + 10.0 * 9.025497436523438
Epoch 2860, val loss: 1.0490257740020752
Epoch 2870, training loss: 90.2904052734375 = 0.03283383697271347 + 10.0 * 9.0257568359375
Epoch 2870, val loss: 1.0530455112457275
Epoch 2880, training loss: 90.31298828125 = 0.03247114643454552 + 10.0 * 9.028051376342773
Epoch 2880, val loss: 1.057420253753662
Epoch 2890, training loss: 90.28131866455078 = 0.03209339827299118 + 10.0 * 9.024922370910645
Epoch 2890, val loss: 1.0610514879226685
Epoch 2900, training loss: 90.2720947265625 = 0.031734682619571686 + 10.0 * 9.024035453796387
Epoch 2900, val loss: 1.0649244785308838
Epoch 2910, training loss: 90.27001190185547 = 0.03137890622019768 + 10.0 * 9.023862838745117
Epoch 2910, val loss: 1.0691111087799072
Epoch 2920, training loss: 90.27000427246094 = 0.03103329986333847 + 10.0 * 9.023897171020508
Epoch 2920, val loss: 1.0728312730789185
Epoch 2930, training loss: 90.310791015625 = 0.030717860907316208 + 10.0 * 9.028007507324219
Epoch 2930, val loss: 1.0763425827026367
Epoch 2940, training loss: 90.27765655517578 = 0.03035178780555725 + 10.0 * 9.024730682373047
Epoch 2940, val loss: 1.0813688039779663
Epoch 2950, training loss: 90.26505279541016 = 0.030007490888237953 + 10.0 * 9.023504257202148
Epoch 2950, val loss: 1.085121750831604
Epoch 2960, training loss: 90.27149200439453 = 0.029678987339138985 + 10.0 * 9.024181365966797
Epoch 2960, val loss: 1.0889918804168701
Epoch 2970, training loss: 90.26875305175781 = 0.029350556433200836 + 10.0 * 9.023940086364746
Epoch 2970, val loss: 1.09304678440094
Epoch 2980, training loss: 90.2707290649414 = 0.029024627059698105 + 10.0 * 9.024170875549316
Epoch 2980, val loss: 1.096612572669983
Epoch 2990, training loss: 90.27266693115234 = 0.02870870754122734 + 10.0 * 9.024395942687988
Epoch 2990, val loss: 1.1006358861923218
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8488
Overall ASR: 0.6897
Flip ASR: 0.6126/1554 nodes
./selected_nodes/Pubmed/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 174, in semi_loss
    refl_sim = f(self.sim(z1, z1))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 4.30 GiB already allocated; 851.69 MiB free; 4.45 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.5 der2:0.3 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.3, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 4.23 GiB already allocated; 101.69 MiB free; 5.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.5 der2:0.3 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.3, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 655.69 MiB free; 5.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.5 der2:0.3 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 104.69397735595703 = 1.1032955646514893 + 10.0 * 10.359067916870117
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 319, in _train_with_val
    loss.backward()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 6.10 GiB already allocated; 257.69 MiB free; 6.34 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.5 der2:0.3 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 653.69 MiB free; 5.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.5 der2:0.3 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 655.69 MiB free; 5.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.5 der2:0.4 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.4, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 104.69416809082031 = 1.1046550273895264 + 10.0 * 10.358951568603516
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 319, in _train_with_val
    loss.backward()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 6.10 GiB already allocated; 387.69 MiB free; 6.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.5 der2:0.4 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.4, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 104.6816635131836 = 1.0926913022994995 + 10.0 * 10.35889720916748
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 319, in _train_with_val
    loss.backward()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 6.10 GiB already allocated; 387.69 MiB free; 6.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.5 der2:0.4 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.4, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 104.69047546386719 = 1.1001862287521362 + 10.0 * 10.359028816223145
Epoch 0, val loss: 1.099988579750061
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 319, in _train_with_val
    loss.backward()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 6.21 GiB already allocated; 905.69 MiB free; 6.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.5 der2:0.4 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 104.68958282470703 = 1.0987904071807861 + 10.0 * 10.359079360961914
Epoch 0, val loss: 1.0970051288604736
Epoch 10, training loss: 104.65254211425781 = 1.087279200553894 + 10.0 * 10.356526374816895
Epoch 10, val loss: 1.0854427814483643
Epoch 20, training loss: 104.2003173828125 = 1.0718973875045776 + 10.0 * 10.312841415405273
Epoch 20, val loss: 1.070311427116394
Epoch 30, training loss: 98.89692687988281 = 1.0555444955825806 + 10.0 * 9.784138679504395
Epoch 30, val loss: 1.0543526411056519
Epoch 40, training loss: 96.27685546875 = 1.0364452600479126 + 10.0 * 9.524041175842285
Epoch 40, val loss: 1.0354171991348267
Epoch 50, training loss: 95.4617691040039 = 1.01432204246521 + 10.0 * 9.444745063781738
Epoch 50, val loss: 1.013818383216858
Epoch 60, training loss: 95.14630889892578 = 0.992250919342041 + 10.0 * 9.415406227111816
Epoch 60, val loss: 0.992477536201477
Epoch 70, training loss: 94.58822631835938 = 0.9718455672264099 + 10.0 * 9.361638069152832
Epoch 70, val loss: 0.9730355739593506
Epoch 80, training loss: 94.10530090332031 = 0.953767716884613 + 10.0 * 9.315153121948242
Epoch 80, val loss: 0.955644965171814
Epoch 90, training loss: 93.87328338623047 = 0.9307178258895874 + 10.0 * 9.294256210327148
Epoch 90, val loss: 0.9327125549316406
Epoch 100, training loss: 93.5342788696289 = 0.9008105397224426 + 10.0 * 9.263346672058105
Epoch 100, val loss: 0.9036319851875305
Epoch 110, training loss: 93.23694610595703 = 0.8704648613929749 + 10.0 * 9.236647605895996
Epoch 110, val loss: 0.8742833137512207
Epoch 120, training loss: 93.03666687011719 = 0.8360320925712585 + 10.0 * 9.220064163208008
Epoch 120, val loss: 0.8407483100891113
Epoch 130, training loss: 92.87212371826172 = 0.794788122177124 + 10.0 * 9.207734107971191
Epoch 130, val loss: 0.8015508055686951
Epoch 140, training loss: 92.7419204711914 = 0.7497231364250183 + 10.0 * 9.199219703674316
Epoch 140, val loss: 0.7590023875236511
Epoch 150, training loss: 92.63077545166016 = 0.7035730481147766 + 10.0 * 9.192720413208008
Epoch 150, val loss: 0.7159386873245239
Epoch 160, training loss: 92.53116607666016 = 0.6579832434654236 + 10.0 * 9.187318801879883
Epoch 160, val loss: 0.6741049885749817
Epoch 170, training loss: 92.4552230834961 = 0.6135447025299072 + 10.0 * 9.184167861938477
Epoch 170, val loss: 0.6339864730834961
Epoch 180, training loss: 92.389892578125 = 0.5724326968193054 + 10.0 * 9.181745529174805
Epoch 180, val loss: 0.5976966023445129
Epoch 190, training loss: 92.33311462402344 = 0.5368994474411011 + 10.0 * 9.179621696472168
Epoch 190, val loss: 0.5672066807746887
Epoch 200, training loss: 92.28189086914062 = 0.507399320602417 + 10.0 * 9.177449226379395
Epoch 200, val loss: 0.5426878333091736
Epoch 210, training loss: 92.2382583618164 = 0.4831210970878601 + 10.0 * 9.17551326751709
Epoch 210, val loss: 0.5234082341194153
Epoch 220, training loss: 92.19510650634766 = 0.4631741940975189 + 10.0 * 9.173192977905273
Epoch 220, val loss: 0.5082105994224548
Epoch 230, training loss: 92.16279602050781 = 0.4466511011123657 + 10.0 * 9.171614646911621
Epoch 230, val loss: 0.4963933825492859
Epoch 240, training loss: 92.1128158569336 = 0.43298912048339844 + 10.0 * 9.16798210144043
Epoch 240, val loss: 0.4870465397834778
Epoch 250, training loss: 92.06063842773438 = 0.4214329421520233 + 10.0 * 9.163920402526855
Epoch 250, val loss: 0.47958871722221375
Epoch 260, training loss: 92.00353240966797 = 0.4113534688949585 + 10.0 * 9.159217834472656
Epoch 260, val loss: 0.47333410382270813
Epoch 270, training loss: 91.95906066894531 = 0.4022115468978882 + 10.0 * 9.155684471130371
Epoch 270, val loss: 0.4679555296897888
Epoch 280, training loss: 91.89948272705078 = 0.393832266330719 + 10.0 * 9.150565147399902
Epoch 280, val loss: 0.46306976675987244
Epoch 290, training loss: 91.86260223388672 = 0.38592690229415894 + 10.0 * 9.14766788482666
Epoch 290, val loss: 0.45858433842658997
Epoch 300, training loss: 91.79833984375 = 0.37850862741470337 + 10.0 * 9.141983032226562
Epoch 300, val loss: 0.4544559419155121
Epoch 310, training loss: 91.75418090820312 = 0.3715664744377136 + 10.0 * 9.138261795043945
Epoch 310, val loss: 0.45084163546562195
Epoch 320, training loss: 91.7074203491211 = 0.3649236559867859 + 10.0 * 9.134249687194824
Epoch 320, val loss: 0.4473869502544403
Epoch 330, training loss: 91.73182678222656 = 0.3584774136543274 + 10.0 * 9.137334823608398
Epoch 330, val loss: 0.4439474642276764
Epoch 340, training loss: 91.6640853881836 = 0.35222259163856506 + 10.0 * 9.131185531616211
Epoch 340, val loss: 0.44102224707603455
Epoch 350, training loss: 91.6122817993164 = 0.34645068645477295 + 10.0 * 9.126583099365234
Epoch 350, val loss: 0.43822577595710754
Epoch 360, training loss: 91.57992553710938 = 0.3409211039543152 + 10.0 * 9.123900413513184
Epoch 360, val loss: 0.4354945421218872
Epoch 370, training loss: 91.55538177490234 = 0.3355262279510498 + 10.0 * 9.12198543548584
Epoch 370, val loss: 0.43302062153816223
Epoch 380, training loss: 91.53145599365234 = 0.3302742838859558 + 10.0 * 9.120118141174316
Epoch 380, val loss: 0.4306158423423767
Epoch 390, training loss: 91.5296630859375 = 0.3251649737358093 + 10.0 * 9.120450019836426
Epoch 390, val loss: 0.4284589886665344
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 319, in _train_with_val
    loss.backward()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 6.31 GiB already allocated; 139.69 MiB free; 7.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.5 der2:0.4 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 104.69144439697266 = 1.100746989250183 + 10.0 * 10.35906982421875
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 319, in _train_with_val
    loss.backward()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 6.10 GiB already allocated; 387.69 MiB free; 6.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.5 der2:0.4 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 104.69994354248047 = 1.1087713241577148 + 10.0 * 10.35911750793457
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 319, in _train_with_val
    loss.backward()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 6.10 GiB already allocated; 387.69 MiB free; 6.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.3 der2:0 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 731.69 MiB free; 5.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.3 der2:0 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 5.16 GiB already allocated; 55.69 MiB free; 5.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.3 der2:0 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 212, in loss
    l2 = self.semi_loss(h2, h1)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 174, in semi_loss
    refl_sim = f(self.sim(z1, z1))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 4.20 GiB already allocated; 333.69 MiB free; 4.30 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 211, in loss
    l1 = self.semi_loss(h1, h2)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 3.24 GiB already allocated; 615.69 MiB free; 3.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 211, in loss
    l1 = self.semi_loss(h1, h2)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 3.24 GiB already allocated; 615.69 MiB free; 3.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 211, in loss
    l1 = self.semi_loss(h1, h2)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 3.24 GiB already allocated; 611.69 MiB free; 3.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.3 der2:0.2 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 211, in loss
    l1 = self.semi_loss(h1, h2)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 3.24 GiB already allocated; 615.69 MiB free; 3.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.3 der2:0.2 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 211, in loss
    l1 = self.semi_loss(h1, h2)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 3.24 GiB already allocated; 615.69 MiB free; 3.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.3 der2:0.2 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 211, in loss
    l1 = self.semi_loss(h1, h2)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 3.24 GiB already allocated; 615.69 MiB free; 3.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.3 der2:0.2 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 725, in <module>
    test_model.fit(args, poison_x, poison_edge_index,poison_edge_weights,poison_labels,bkd_tn_nodes,idx_val=idx_val,train_iters=args.cl_num_epochs,cont_iters=args.cl_num_epochs,seen_node_idx=seen_node_idx)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 234, in fit
    self._train_with_val(self.labels, idx_train, idx_val, train_iters)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 309, in _train_with_val
    cont_loss = self.loss(h1[self.seen_node_idx], h2[self.seen_node_idx], batch_size=self.args.cont_batch_size)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 211, in loss
    l1 = self.semi_loss(h1, h2)
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 175, in semi_loss
    between_sim = f(self.sim(z1, z2))
  File "/home/mfl5681/project-contrastive/RobustCL/model.py", line 173, in <lambda>
    f = lambda x: torch.exp(x / self.tau)
RuntimeError: CUDA out of memory. Tried to allocate 950.00 MiB (GPU 0; 47.54 GiB total capacity; 3.24 GiB already allocated; 615.69 MiB free; 3.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.3 der2:0.2 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 104.68106079101562 = 1.09054434299469 + 10.0 * 10.359051704406738
Epoch 0, val loss: 1.089469313621521
Epoch 10, training loss: 104.6309585571289 = 1.0798451900482178 + 10.0 * 10.355112075805664
Epoch 10, val loss: 1.0786513090133667
Epoch 20, training loss: 103.9451675415039 = 1.0662438869476318 + 10.0 * 10.28789234161377
Epoch 20, val loss: 1.0652421712875366
Epoch 30, training loss: 99.21133422851562 = 1.0549640655517578 + 10.0 * 9.815637588500977
Epoch 30, val loss: 1.0540298223495483
Epoch 40, training loss: 96.64036560058594 = 1.0432929992675781 + 10.0 * 9.559706687927246
Epoch 40, val loss: 1.0421428680419922
Epoch 50, training loss: 95.4239730834961 = 1.029201865196228 + 10.0 * 9.43947696685791
Epoch 50, val loss: 1.0281223058700562
Epoch 60, training loss: 94.67131805419922 = 1.012629747390747 + 10.0 * 9.365869522094727
Epoch 60, val loss: 1.011795997619629
Epoch 70, training loss: 94.33120727539062 = 0.9943066239356995 + 10.0 * 9.33368968963623
Epoch 70, val loss: 0.9940499663352966
Epoch 80, training loss: 93.83374786376953 = 0.9779226779937744 + 10.0 * 9.285582542419434
Epoch 80, val loss: 0.9782270193099976
Epoch 90, training loss: 93.43646240234375 = 0.9608832001686096 + 10.0 * 9.247557640075684
Epoch 90, val loss: 0.9614589810371399
Epoch 100, training loss: 93.09097290039062 = 0.9383271336555481 + 10.0 * 9.215265274047852
Epoch 100, val loss: 0.9393699765205383
Epoch 110, training loss: 92.85780334472656 = 0.9115583300590515 + 10.0 * 9.194623947143555
Epoch 110, val loss: 0.9135198593139648
Epoch 120, training loss: 92.64765167236328 = 0.8834784030914307 + 10.0 * 9.176417350769043
Epoch 120, val loss: 0.8866454362869263
Epoch 130, training loss: 92.43675231933594 = 0.8542314171791077 + 10.0 * 9.158251762390137
Epoch 130, val loss: 0.8587762117385864
Epoch 140, training loss: 92.27532958984375 = 0.8220873475074768 + 10.0 * 9.145323753356934
Epoch 140, val loss: 0.828042209148407
Epoch 150, training loss: 92.15494537353516 = 0.7856587767601013 + 10.0 * 9.13692855834961
Epoch 150, val loss: 0.7932567000389099
Epoch 160, training loss: 92.02062225341797 = 0.7457677721977234 + 10.0 * 9.127485275268555
Epoch 160, val loss: 0.7554241418838501
Epoch 170, training loss: 91.91130065917969 = 0.7044030427932739 + 10.0 * 9.120689392089844
Epoch 170, val loss: 0.7163287401199341
Epoch 180, training loss: 91.83821868896484 = 0.6629015207290649 + 10.0 * 9.117531776428223
Epoch 180, val loss: 0.677471935749054
Epoch 190, training loss: 91.73155975341797 = 0.6228846907615662 + 10.0 * 9.110867500305176
Epoch 190, val loss: 0.6402362585067749
Epoch 200, training loss: 91.65606689453125 = 0.5853766798973083 + 10.0 * 9.10706901550293
Epoch 200, val loss: 0.6055746078491211
Epoch 210, training loss: 91.64105224609375 = 0.5510456562042236 + 10.0 * 9.109000205993652
Epoch 210, val loss: 0.5738893151283264
Epoch 220, training loss: 91.53520965576172 = 0.5202272534370422 + 10.0 * 9.1014986038208
Epoch 220, val loss: 0.5461843609809875
Epoch 230, training loss: 91.47627258300781 = 0.49324291944503784 + 10.0 * 9.098302841186523
Epoch 230, val loss: 0.5219321250915527
Epoch 240, training loss: 91.43538665771484 = 0.46968507766723633 + 10.0 * 9.096570014953613
Epoch 240, val loss: 0.5008013248443604
Epoch 250, training loss: 91.3796157836914 = 0.4491065740585327 + 10.0 * 9.093050956726074
Epoch 250, val loss: 0.48263442516326904
Epoch 260, training loss: 91.33958435058594 = 0.431142657995224 + 10.0 * 9.09084415435791
Epoch 260, val loss: 0.46692419052124023
Epoch 270, training loss: 91.33057403564453 = 0.41539686918258667 + 10.0 * 9.091517448425293
Epoch 270, val loss: 0.4533771276473999
Epoch 280, training loss: 91.27363586425781 = 0.401667982339859 + 10.0 * 9.087197303771973
Epoch 280, val loss: 0.44175130128860474
Epoch 290, training loss: 91.23656463623047 = 0.38970884680747986 + 10.0 * 9.084685325622559
Epoch 290, val loss: 0.4317552149295807
Epoch 300, training loss: 91.20751953125 = 0.37914714217185974 + 10.0 * 9.082837104797363
Epoch 300, val loss: 0.4231080114841461
Epoch 310, training loss: 91.24246215820312 = 0.3697411119937897 + 10.0 * 9.087271690368652
Epoch 310, val loss: 0.41551128029823303
Epoch 320, training loss: 91.1815414428711 = 0.36139461398124695 + 10.0 * 9.082014083862305
Epoch 320, val loss: 0.40916934609413147
Epoch 330, training loss: 91.13746643066406 = 0.3540370464324951 + 10.0 * 9.078343391418457
Epoch 330, val loss: 0.40366852283477783
Epoch 340, training loss: 91.11784362792969 = 0.3474249839782715 + 10.0 * 9.077041625976562
Epoch 340, val loss: 0.39886370301246643
Epoch 350, training loss: 91.14070892333984 = 0.3413720428943634 + 10.0 * 9.079934120178223
Epoch 350, val loss: 0.3946477472782135
Epoch 360, training loss: 91.0877456665039 = 0.33579909801483154 + 10.0 * 9.075194358825684
Epoch 360, val loss: 0.39098915457725525
Epoch 370, training loss: 91.06781005859375 = 0.330727756023407 + 10.0 * 9.073708534240723
Epoch 370, val loss: 0.38779330253601074
Epoch 380, training loss: 91.04653930664062 = 0.3260364532470703 + 10.0 * 9.072050094604492
Epoch 380, val loss: 0.384976327419281
Epoch 390, training loss: 91.03260803222656 = 0.32162418961524963 + 10.0 * 9.071098327636719
Epoch 390, val loss: 0.3825090229511261
Epoch 400, training loss: 91.02920532226562 = 0.3174525499343872 + 10.0 * 9.071175575256348
Epoch 400, val loss: 0.3803628981113434
Epoch 410, training loss: 91.0099868774414 = 0.3135654628276825 + 10.0 * 9.069642066955566
Epoch 410, val loss: 0.3784799575805664
Epoch 420, training loss: 90.9903793334961 = 0.3099234998226166 + 10.0 * 9.068045616149902
Epoch 420, val loss: 0.37688401341438293
Epoch 430, training loss: 90.97467803955078 = 0.3064526915550232 + 10.0 * 9.06682300567627
Epoch 430, val loss: 0.37547627091407776
Epoch 440, training loss: 90.98454284667969 = 0.3031258285045624 + 10.0 * 9.06814193725586
Epoch 440, val loss: 0.37429124116897583
Epoch 450, training loss: 90.9656753540039 = 0.29993826150894165 + 10.0 * 9.066573143005371
Epoch 450, val loss: 0.37313026189804077
Epoch 460, training loss: 90.94194793701172 = 0.29691964387893677 + 10.0 * 9.064502716064453
Epoch 460, val loss: 0.37221625447273254
Epoch 470, training loss: 90.9284896850586 = 0.2940398156642914 + 10.0 * 9.063445091247559
Epoch 470, val loss: 0.3714245855808258
Epoch 480, training loss: 90.94999694824219 = 0.2912627160549164 + 10.0 * 9.065874099731445
Epoch 480, val loss: 0.37066078186035156
Epoch 490, training loss: 90.90633392333984 = 0.288583904504776 + 10.0 * 9.061775207519531
Epoch 490, val loss: 0.37020471692085266
Epoch 500, training loss: 90.89997863769531 = 0.2860129177570343 + 10.0 * 9.061396598815918
Epoch 500, val loss: 0.3698432743549347
Epoch 510, training loss: 90.88719940185547 = 0.28351670503616333 + 10.0 * 9.060368537902832
Epoch 510, val loss: 0.36947059631347656
Epoch 520, training loss: 90.95552825927734 = 0.2810957133769989 + 10.0 * 9.067442893981934
Epoch 520, val loss: 0.36938217282295227
Epoch 530, training loss: 90.88525390625 = 0.27874553203582764 + 10.0 * 9.060650825500488
Epoch 530, val loss: 0.3691137433052063
Epoch 540, training loss: 90.87028503417969 = 0.2764979898929596 + 10.0 * 9.059378623962402
Epoch 540, val loss: 0.36885085701942444
Epoch 550, training loss: 90.85617065429688 = 0.2743110656738281 + 10.0 * 9.058185577392578
Epoch 550, val loss: 0.3686615526676178
Epoch 560, training loss: 90.86963653564453 = 0.2721646726131439 + 10.0 * 9.059747695922852
Epoch 560, val loss: 0.368542343378067
Epoch 570, training loss: 90.85746002197266 = 0.2700643539428711 + 10.0 * 9.05873966217041
Epoch 570, val loss: 0.368655800819397
Epoch 580, training loss: 90.8401870727539 = 0.26801446080207825 + 10.0 * 9.057217597961426
Epoch 580, val loss: 0.36870312690734863
Epoch 590, training loss: 90.83041381835938 = 0.2660072445869446 + 10.0 * 9.056440353393555
Epoch 590, val loss: 0.36880892515182495
Epoch 600, training loss: 90.81912231445312 = 0.26403698325157166 + 10.0 * 9.055508613586426
Epoch 600, val loss: 0.368941992521286
Epoch 610, training loss: 90.83255004882812 = 0.26209521293640137 + 10.0 * 9.057045936584473
Epoch 610, val loss: 0.36908021569252014
Epoch 620, training loss: 90.8262939453125 = 0.2601812481880188 + 10.0 * 9.056612014770508
Epoch 620, val loss: 0.36950433254241943
Epoch 630, training loss: 90.79903411865234 = 0.25831061601638794 + 10.0 * 9.054072380065918
Epoch 630, val loss: 0.36962541937828064
Epoch 640, training loss: 90.79691314697266 = 0.25646427273750305 + 10.0 * 9.054044723510742
Epoch 640, val loss: 0.36982935667037964
Epoch 650, training loss: 90.80116271972656 = 0.2546285390853882 + 10.0 * 9.05465316772461
Epoch 650, val loss: 0.3701525628566742
Epoch 660, training loss: 90.79314422607422 = 0.2528179883956909 + 10.0 * 9.054033279418945
Epoch 660, val loss: 0.37072038650512695
Epoch 670, training loss: 90.78396606445312 = 0.2510461211204529 + 10.0 * 9.053292274475098
Epoch 670, val loss: 0.37090495228767395
Epoch 680, training loss: 90.76897430419922 = 0.24929219484329224 + 10.0 * 9.051968574523926
Epoch 680, val loss: 0.37125512957572937
Epoch 690, training loss: 90.76497650146484 = 0.24755078554153442 + 10.0 * 9.051742553710938
Epoch 690, val loss: 0.3717805743217468
Epoch 700, training loss: 90.78129577636719 = 0.24581977725028992 + 10.0 * 9.053547859191895
Epoch 700, val loss: 0.3721548020839691
Epoch 710, training loss: 90.79212951660156 = 0.24411115050315857 + 10.0 * 9.054801940917969
Epoch 710, val loss: 0.372757226228714
Epoch 720, training loss: 90.74756622314453 = 0.24241326749324799 + 10.0 * 9.050515174865723
Epoch 720, val loss: 0.37328922748565674
Epoch 730, training loss: 90.74641418457031 = 0.24073001742362976 + 10.0 * 9.050568580627441
Epoch 730, val loss: 0.3738526403903961
Epoch 740, training loss: 90.76979064941406 = 0.2390591949224472 + 10.0 * 9.053072929382324
Epoch 740, val loss: 0.37456321716308594
Epoch 750, training loss: 90.7328872680664 = 0.23740419745445251 + 10.0 * 9.049548149108887
Epoch 750, val loss: 0.37501955032348633
Epoch 760, training loss: 90.7267837524414 = 0.23576107621192932 + 10.0 * 9.049101829528809
Epoch 760, val loss: 0.3755469024181366
Epoch 770, training loss: 90.72126007080078 = 0.23411697149276733 + 10.0 * 9.048714637756348
Epoch 770, val loss: 0.37639912962913513
Epoch 780, training loss: 90.74124145507812 = 0.23247697949409485 + 10.0 * 9.05087661743164
Epoch 780, val loss: 0.37724658846855164
Epoch 790, training loss: 90.74666595458984 = 0.23085886240005493 + 10.0 * 9.051580429077148
Epoch 790, val loss: 0.3777245581150055
Epoch 800, training loss: 90.71498107910156 = 0.22925029695034027 + 10.0 * 9.04857349395752
Epoch 800, val loss: 0.3784756064414978
Epoch 810, training loss: 90.70243835449219 = 0.22765249013900757 + 10.0 * 9.047478675842285
Epoch 810, val loss: 0.3793472349643707
Epoch 820, training loss: 90.69551849365234 = 0.2260543555021286 + 10.0 * 9.04694652557373
Epoch 820, val loss: 0.3802662789821625
Epoch 830, training loss: 90.70062255859375 = 0.2244577258825302 + 10.0 * 9.047616958618164
Epoch 830, val loss: 0.3812050521373749
Epoch 840, training loss: 90.68971252441406 = 0.22286969423294067 + 10.0 * 9.046684265136719
Epoch 840, val loss: 0.382182240486145
Epoch 850, training loss: 90.6952133178711 = 0.22129324078559875 + 10.0 * 9.047391891479492
Epoch 850, val loss: 0.3831152319908142
Epoch 860, training loss: 90.68365478515625 = 0.2197212278842926 + 10.0 * 9.046393394470215
Epoch 860, val loss: 0.3840964138507843
Epoch 870, training loss: 90.67447662353516 = 0.2181514948606491 + 10.0 * 9.045632362365723
Epoch 870, val loss: 0.3850352168083191
Epoch 880, training loss: 90.70230102539062 = 0.21658270061016083 + 10.0 * 9.048571586608887
Epoch 880, val loss: 0.38599589467048645
Epoch 890, training loss: 90.67523193359375 = 0.2150152027606964 + 10.0 * 9.046022415161133
Epoch 890, val loss: 0.3871942460536957
Epoch 900, training loss: 90.66038513183594 = 0.21344871819019318 + 10.0 * 9.044693946838379
Epoch 900, val loss: 0.38826221227645874
Epoch 910, training loss: 90.65460968017578 = 0.21188098192214966 + 10.0 * 9.044272422790527
Epoch 910, val loss: 0.3894177973270416
Epoch 920, training loss: 90.70634460449219 = 0.2103254646062851 + 10.0 * 9.049601554870605
Epoch 920, val loss: 0.3908885419368744
Epoch 930, training loss: 90.66156768798828 = 0.20876412093639374 + 10.0 * 9.045280456542969
Epoch 930, val loss: 0.39162930846214294
Epoch 940, training loss: 90.64942169189453 = 0.20720823109149933 + 10.0 * 9.044221878051758
Epoch 940, val loss: 0.3927505612373352
Epoch 950, training loss: 90.6395492553711 = 0.20565207302570343 + 10.0 * 9.043390274047852
Epoch 950, val loss: 0.39409956336021423
Epoch 960, training loss: 90.65232849121094 = 0.20409831404685974 + 10.0 * 9.044822692871094
Epoch 960, val loss: 0.3954898715019226
Epoch 970, training loss: 90.6456527709961 = 0.20253914594650269 + 10.0 * 9.0443115234375
Epoch 970, val loss: 0.39672255516052246
Epoch 980, training loss: 90.63191986083984 = 0.20098355412483215 + 10.0 * 9.04309368133545
Epoch 980, val loss: 0.398111492395401
Epoch 990, training loss: 90.62776184082031 = 0.19943061470985413 + 10.0 * 9.04283332824707
Epoch 990, val loss: 0.3993592858314514
Epoch 1000, training loss: 90.62071990966797 = 0.19787108898162842 + 10.0 * 9.042284965515137
Epoch 1000, val loss: 0.4008074700832367
Epoch 1010, training loss: 90.65278625488281 = 0.19631610810756683 + 10.0 * 9.045646667480469
Epoch 1010, val loss: 0.4023471474647522
Epoch 1020, training loss: 90.62470245361328 = 0.19477231800556183 + 10.0 * 9.042993545532227
Epoch 1020, val loss: 0.4036960005760193
Epoch 1030, training loss: 90.60704803466797 = 0.19322572648525238 + 10.0 * 9.041382789611816
Epoch 1030, val loss: 0.4052320718765259
Epoch 1040, training loss: 90.60383605957031 = 0.19167834520339966 + 10.0 * 9.041215896606445
Epoch 1040, val loss: 0.4069323241710663
Epoch 1050, training loss: 90.59749603271484 = 0.19013042747974396 + 10.0 * 9.040736198425293
Epoch 1050, val loss: 0.40846291184425354
Epoch 1060, training loss: 90.59521484375 = 0.18857981264591217 + 10.0 * 9.04066276550293
Epoch 1060, val loss: 0.41008391976356506
Epoch 1070, training loss: 90.65788269042969 = 0.18703845143318176 + 10.0 * 9.047083854675293
Epoch 1070, val loss: 0.4114949703216553
Epoch 1080, training loss: 90.58759307861328 = 0.18547819554805756 + 10.0 * 9.04021167755127
Epoch 1080, val loss: 0.4134048521518707
Epoch 1090, training loss: 90.58753204345703 = 0.18393033742904663 + 10.0 * 9.040360450744629
Epoch 1090, val loss: 0.41525670886039734
Epoch 1100, training loss: 90.58158874511719 = 0.18238188326358795 + 10.0 * 9.039920806884766
Epoch 1100, val loss: 0.4170093536376953
Epoch 1110, training loss: 90.5877914428711 = 0.18083161115646362 + 10.0 * 9.040696144104004
Epoch 1110, val loss: 0.4187082350254059
Epoch 1120, training loss: 90.57996368408203 = 0.17927999794483185 + 10.0 * 9.040067672729492
Epoch 1120, val loss: 0.4207182228565216
Epoch 1130, training loss: 90.57759094238281 = 0.17772860825061798 + 10.0 * 9.039986610412598
Epoch 1130, val loss: 0.422378271818161
Epoch 1140, training loss: 90.56856536865234 = 0.1761789470911026 + 10.0 * 9.039238929748535
Epoch 1140, val loss: 0.424211710691452
Epoch 1150, training loss: 90.56221008300781 = 0.1746271252632141 + 10.0 * 9.038758277893066
Epoch 1150, val loss: 0.4262179732322693
Epoch 1160, training loss: 90.58375549316406 = 0.1730823516845703 + 10.0 * 9.041067123413086
Epoch 1160, val loss: 0.42820605635643005
Epoch 1170, training loss: 90.56947326660156 = 0.17153963446617126 + 10.0 * 9.039793014526367
Epoch 1170, val loss: 0.4302874803543091
Epoch 1180, training loss: 90.557861328125 = 0.16999855637550354 + 10.0 * 9.038785934448242
Epoch 1180, val loss: 0.4323667287826538
Epoch 1190, training loss: 90.548828125 = 0.16845683753490448 + 10.0 * 9.038037300109863
Epoch 1190, val loss: 0.4344562888145447
Epoch 1200, training loss: 90.55223083496094 = 0.16691520810127258 + 10.0 * 9.038531303405762
Epoch 1200, val loss: 0.43667152523994446
Epoch 1210, training loss: 90.55654907226562 = 0.16537775099277496 + 10.0 * 9.039117813110352
Epoch 1210, val loss: 0.4387539327144623
Epoch 1220, training loss: 90.54660034179688 = 0.163848876953125 + 10.0 * 9.038274765014648
Epoch 1220, val loss: 0.4408855438232422
Epoch 1230, training loss: 90.53721618652344 = 0.16231490671634674 + 10.0 * 9.037489891052246
Epoch 1230, val loss: 0.44332003593444824
Epoch 1240, training loss: 90.54783630371094 = 0.16078917682170868 + 10.0 * 9.038704872131348
Epoch 1240, val loss: 0.44579654932022095
Epoch 1250, training loss: 90.53288269042969 = 0.1592632234096527 + 10.0 * 9.037362098693848
Epoch 1250, val loss: 0.4479506015777588
Epoch 1260, training loss: 90.53237915039062 = 0.15773819386959076 + 10.0 * 9.037464141845703
Epoch 1260, val loss: 0.4504092037677765
Epoch 1270, training loss: 90.52730560302734 = 0.1562032848596573 + 10.0 * 9.037110328674316
Epoch 1270, val loss: 0.45290499925613403
Epoch 1280, training loss: 90.52133178710938 = 0.15466690063476562 + 10.0 * 9.036666870117188
Epoch 1280, val loss: 0.4553952217102051
Epoch 1290, training loss: 90.51995849609375 = 0.1531253159046173 + 10.0 * 9.036684036254883
Epoch 1290, val loss: 0.45794475078582764
Epoch 1300, training loss: 90.53172302246094 = 0.15158523619174957 + 10.0 * 9.038013458251953
Epoch 1300, val loss: 0.4604354202747345
Epoch 1310, training loss: 90.5237045288086 = 0.15004707872867584 + 10.0 * 9.037365913391113
Epoch 1310, val loss: 0.46280452609062195
Epoch 1320, training loss: 90.51551818847656 = 0.1485111266374588 + 10.0 * 9.036700248718262
Epoch 1320, val loss: 0.46543312072753906
Epoch 1330, training loss: 90.50605010986328 = 0.14697499573230743 + 10.0 * 9.035907745361328
Epoch 1330, val loss: 0.46802860498428345
Epoch 1340, training loss: 90.51746368408203 = 0.14544351398944855 + 10.0 * 9.037201881408691
Epoch 1340, val loss: 0.47073909640312195
Epoch 1350, training loss: 90.50199890136719 = 0.14391493797302246 + 10.0 * 9.035808563232422
Epoch 1350, val loss: 0.4734042286872864
Epoch 1360, training loss: 90.49795532226562 = 0.14238600432872772 + 10.0 * 9.03555679321289
Epoch 1360, val loss: 0.4762006998062134
Epoch 1370, training loss: 90.49411010742188 = 0.1408585160970688 + 10.0 * 9.035325050354004
Epoch 1370, val loss: 0.47918757796287537
Epoch 1380, training loss: 90.51663208007812 = 0.13934020698070526 + 10.0 * 9.037729263305664
Epoch 1380, val loss: 0.48237621784210205
Epoch 1390, training loss: 90.49781036376953 = 0.13781623542308807 + 10.0 * 9.035999298095703
Epoch 1390, val loss: 0.4852011799812317
Epoch 1400, training loss: 90.48558807373047 = 0.1363016813993454 + 10.0 * 9.034929275512695
Epoch 1400, val loss: 0.4881429672241211
Epoch 1410, training loss: 90.47883605957031 = 0.13478459417819977 + 10.0 * 9.034405708312988
Epoch 1410, val loss: 0.49117568135261536
Epoch 1420, training loss: 90.48432922363281 = 0.13326647877693176 + 10.0 * 9.03510570526123
Epoch 1420, val loss: 0.4941786527633667
Epoch 1430, training loss: 90.47856140136719 = 0.13174554705619812 + 10.0 * 9.03468132019043
Epoch 1430, val loss: 0.49719083309173584
Epoch 1440, training loss: 90.48066711425781 = 0.13021869957447052 + 10.0 * 9.03504467010498
Epoch 1440, val loss: 0.5003014206886292
Epoch 1450, training loss: 90.47589111328125 = 0.12869636714458466 + 10.0 * 9.034719467163086
Epoch 1450, val loss: 0.5036531090736389
Epoch 1460, training loss: 90.47493743896484 = 0.12718547880649567 + 10.0 * 9.034775733947754
Epoch 1460, val loss: 0.5066229104995728
Epoch 1470, training loss: 90.4631576538086 = 0.12565116584300995 + 10.0 * 9.033750534057617
Epoch 1470, val loss: 0.5101782083511353
Epoch 1480, training loss: 90.46439361572266 = 0.12412969768047333 + 10.0 * 9.034026145935059
Epoch 1480, val loss: 0.5134952664375305
Epoch 1490, training loss: 90.45968627929688 = 0.12261881679296494 + 10.0 * 9.033706665039062
Epoch 1490, val loss: 0.5166940689086914
Epoch 1500, training loss: 90.45932006835938 = 0.12109654396772385 + 10.0 * 9.033823013305664
Epoch 1500, val loss: 0.520197331905365
Epoch 1510, training loss: 90.45757293701172 = 0.11958732455968857 + 10.0 * 9.033798217773438
Epoch 1510, val loss: 0.5236778855323792
Epoch 1520, training loss: 90.44416046142578 = 0.11807552725076675 + 10.0 * 9.032608985900879
Epoch 1520, val loss: 0.527182400226593
Epoch 1530, training loss: 90.44085693359375 = 0.11657176911830902 + 10.0 * 9.032428741455078
Epoch 1530, val loss: 0.5305495858192444
Epoch 1540, training loss: 90.44142150878906 = 0.115077443420887 + 10.0 * 9.032634735107422
Epoch 1540, val loss: 0.5338987708091736
Epoch 1550, training loss: 90.4646987915039 = 0.11358729749917984 + 10.0 * 9.035111427307129
Epoch 1550, val loss: 0.5372980237007141
Epoch 1560, training loss: 90.4426498413086 = 0.11209738254547119 + 10.0 * 9.033055305480957
Epoch 1560, val loss: 0.5413331985473633
Epoch 1570, training loss: 90.44247436523438 = 0.11060699075460434 + 10.0 * 9.033186912536621
Epoch 1570, val loss: 0.544806718826294
Epoch 1580, training loss: 90.4312973022461 = 0.10912679880857468 + 10.0 * 9.032217025756836
Epoch 1580, val loss: 0.5479990839958191
Epoch 1590, training loss: 90.43444061279297 = 0.10765720903873444 + 10.0 * 9.032678604125977
Epoch 1590, val loss: 0.5518530011177063
Epoch 1600, training loss: 90.43710327148438 = 0.10619906336069107 + 10.0 * 9.033090591430664
Epoch 1600, val loss: 0.5554770231246948
Epoch 1610, training loss: 90.42040252685547 = 0.10473214089870453 + 10.0 * 9.031567573547363
Epoch 1610, val loss: 0.5593032836914062
Epoch 1620, training loss: 90.42216491699219 = 0.10328364372253418 + 10.0 * 9.031888008117676
Epoch 1620, val loss: 0.563101053237915
Epoch 1630, training loss: 90.4251480102539 = 0.10184171795845032 + 10.0 * 9.032330513000488
Epoch 1630, val loss: 0.5667188763618469
Epoch 1640, training loss: 90.41327667236328 = 0.10039550065994263 + 10.0 * 9.031288146972656
Epoch 1640, val loss: 0.5707181692123413
Epoch 1650, training loss: 90.41421508789062 = 0.09896139800548553 + 10.0 * 9.031525611877441
Epoch 1650, val loss: 0.574571967124939
Epoch 1660, training loss: 90.41695404052734 = 0.0975273847579956 + 10.0 * 9.031942367553711
Epoch 1660, val loss: 0.5785431265830994
Epoch 1670, training loss: 90.42036437988281 = 0.09610846638679504 + 10.0 * 9.032425880432129
Epoch 1670, val loss: 0.5825010538101196
Epoch 1680, training loss: 90.40546417236328 = 0.0946904644370079 + 10.0 * 9.03107738494873
Epoch 1680, val loss: 0.5866032838821411
Epoch 1690, training loss: 90.39952850341797 = 0.09328562766313553 + 10.0 * 9.030624389648438
Epoch 1690, val loss: 0.5904974341392517
Epoch 1700, training loss: 90.39366912841797 = 0.09189322590827942 + 10.0 * 9.030177116394043
Epoch 1700, val loss: 0.5947129726409912
Epoch 1710, training loss: 90.41924285888672 = 0.09052512049674988 + 10.0 * 9.032872200012207
Epoch 1710, val loss: 0.5992056727409363
Epoch 1720, training loss: 90.40299224853516 = 0.08916027098894119 + 10.0 * 9.031383514404297
Epoch 1720, val loss: 0.6030791401863098
Epoch 1730, training loss: 90.39077758789062 = 0.0878010019659996 + 10.0 * 9.030298233032227
Epoch 1730, val loss: 0.6075496673583984
Epoch 1740, training loss: 90.38066101074219 = 0.08645524829626083 + 10.0 * 9.029420852661133
Epoch 1740, val loss: 0.6118279695510864
Epoch 1750, training loss: 90.37924194335938 = 0.08512502908706665 + 10.0 * 9.029411315917969
Epoch 1750, val loss: 0.616064727306366
Epoch 1760, training loss: 90.39022827148438 = 0.08381292968988419 + 10.0 * 9.030641555786133
Epoch 1760, val loss: 0.6204348206520081
Epoch 1770, training loss: 90.37488555908203 = 0.08249275386333466 + 10.0 * 9.029239654541016
Epoch 1770, val loss: 0.625285267829895
Epoch 1780, training loss: 90.37577819824219 = 0.08120395243167877 + 10.0 * 9.029457092285156
Epoch 1780, val loss: 0.629643976688385
Epoch 1790, training loss: 90.38729858398438 = 0.07992438971996307 + 10.0 * 9.030736923217773
Epoch 1790, val loss: 0.6346732378005981
Epoch 1800, training loss: 90.3721923828125 = 0.078651562333107 + 10.0 * 9.029354095458984
Epoch 1800, val loss: 0.6391176581382751
Epoch 1810, training loss: 90.37244415283203 = 0.0774022564291954 + 10.0 * 9.029504776000977
Epoch 1810, val loss: 0.6436288952827454
Epoch 1820, training loss: 90.36329650878906 = 0.07616598904132843 + 10.0 * 9.02871322631836
Epoch 1820, val loss: 0.6486272215843201
Epoch 1830, training loss: 90.36639404296875 = 0.07494741678237915 + 10.0 * 9.029144287109375
Epoch 1830, val loss: 0.6531676054000854
Epoch 1840, training loss: 90.37871551513672 = 0.07376091927289963 + 10.0 * 9.030495643615723
Epoch 1840, val loss: 0.6583897471427917
Epoch 1850, training loss: 90.36491394042969 = 0.07256130874156952 + 10.0 * 9.029234886169434
Epoch 1850, val loss: 0.662947952747345
Epoch 1860, training loss: 90.35684204101562 = 0.07138155400753021 + 10.0 * 9.028546333312988
Epoch 1860, val loss: 0.6678823828697205
Epoch 1870, training loss: 90.35441589355469 = 0.07022886723279953 + 10.0 * 9.02841854095459
Epoch 1870, val loss: 0.6728445291519165
Epoch 1880, training loss: 90.35455322265625 = 0.06907764822244644 + 10.0 * 9.028547286987305
Epoch 1880, val loss: 0.6780047416687012
Epoch 1890, training loss: 90.35867309570312 = 0.0679481029510498 + 10.0 * 9.029072761535645
Epoch 1890, val loss: 0.682791531085968
Epoch 1900, training loss: 90.35071563720703 = 0.0668308287858963 + 10.0 * 9.028388023376465
Epoch 1900, val loss: 0.687727153301239
Epoch 1910, training loss: 90.35285186767578 = 0.06574364006519318 + 10.0 * 9.028711318969727
Epoch 1910, val loss: 0.6923918724060059
Epoch 1920, training loss: 90.34102630615234 = 0.06464386731386185 + 10.0 * 9.02763843536377
Epoch 1920, val loss: 0.6979960799217224
Epoch 1930, training loss: 90.34196472167969 = 0.063572458922863 + 10.0 * 9.027838706970215
Epoch 1930, val loss: 0.7031223773956299
Epoch 1940, training loss: 90.33895874023438 = 0.06251654028892517 + 10.0 * 9.027644157409668
Epoch 1940, val loss: 0.7079858779907227
Epoch 1950, training loss: 90.3434066772461 = 0.06146654859185219 + 10.0 * 9.028193473815918
Epoch 1950, val loss: 0.7129741907119751
Epoch 1960, training loss: 90.35859680175781 = 0.06043865159153938 + 10.0 * 9.029815673828125
Epoch 1960, val loss: 0.7189704775810242
Epoch 1970, training loss: 90.34004211425781 = 0.05941535905003548 + 10.0 * 9.02806282043457
Epoch 1970, val loss: 0.7231330871582031
Epoch 1980, training loss: 90.32815551757812 = 0.0584079883992672 + 10.0 * 9.02697467803955
Epoch 1980, val loss: 0.7283909916877747
Epoch 1990, training loss: 90.32443237304688 = 0.05741319805383682 + 10.0 * 9.026701927185059
Epoch 1990, val loss: 0.7334188222885132
Epoch 2000, training loss: 90.32781982421875 = 0.056438446044921875 + 10.0 * 9.027138710021973
Epoch 2000, val loss: 0.7385146617889404
Epoch 2010, training loss: 90.33981323242188 = 0.055482931435108185 + 10.0 * 9.028432846069336
Epoch 2010, val loss: 0.7433526515960693
Epoch 2020, training loss: 90.33275604248047 = 0.054529447108507156 + 10.0 * 9.027822494506836
Epoch 2020, val loss: 0.7487132549285889
Epoch 2030, training loss: 90.31795501708984 = 0.05359417200088501 + 10.0 * 9.026435852050781
Epoch 2030, val loss: 0.7539030313491821
Epoch 2040, training loss: 90.31757354736328 = 0.05268005654215813 + 10.0 * 9.0264892578125
Epoch 2040, val loss: 0.7586246132850647
Epoch 2050, training loss: 90.32960510253906 = 0.051774848252534866 + 10.0 * 9.027783393859863
Epoch 2050, val loss: 0.7641072869300842
Epoch 2060, training loss: 90.31810760498047 = 0.05088519677519798 + 10.0 * 9.026721954345703
Epoch 2060, val loss: 0.7692430019378662
Epoch 2070, training loss: 90.31916809082031 = 0.05001154541969299 + 10.0 * 9.026915550231934
Epoch 2070, val loss: 0.7743282318115234
Epoch 2080, training loss: 90.30814361572266 = 0.049156904220581055 + 10.0 * 9.025898933410645
Epoch 2080, val loss: 0.779245138168335
Epoch 2090, training loss: 90.30531311035156 = 0.04830808937549591 + 10.0 * 9.025700569152832
Epoch 2090, val loss: 0.7845770716667175
Epoch 2100, training loss: 90.31647491455078 = 0.04747692868113518 + 10.0 * 9.026899337768555
Epoch 2100, val loss: 0.7900173664093018
Epoch 2110, training loss: 90.306884765625 = 0.046657904982566833 + 10.0 * 9.026021957397461
Epoch 2110, val loss: 0.7950537800788879
Epoch 2120, training loss: 90.3000717163086 = 0.04584915190935135 + 10.0 * 9.025422096252441
Epoch 2120, val loss: 0.7998039722442627
Epoch 2130, training loss: 90.30654907226562 = 0.04506154730916023 + 10.0 * 9.026148796081543
Epoch 2130, val loss: 0.8047952651977539
Epoch 2140, training loss: 90.30873107910156 = 0.044296570122241974 + 10.0 * 9.026443481445312
Epoch 2140, val loss: 0.8092498183250427
Epoch 2150, training loss: 90.2991714477539 = 0.043526820838451385 + 10.0 * 9.025564193725586
Epoch 2150, val loss: 0.8145251870155334
Epoch 2160, training loss: 90.29317474365234 = 0.04277863726019859 + 10.0 * 9.025039672851562
Epoch 2160, val loss: 0.8197840452194214
Epoch 2170, training loss: 90.28894805908203 = 0.04204264655709267 + 10.0 * 9.024690628051758
Epoch 2170, val loss: 0.8246927261352539
Epoch 2180, training loss: 90.29320526123047 = 0.041323453187942505 + 10.0 * 9.025188446044922
Epoch 2180, val loss: 0.8296647667884827
Epoch 2190, training loss: 90.29830169677734 = 0.04062190279364586 + 10.0 * 9.025768280029297
Epoch 2190, val loss: 0.8352257013320923
Epoch 2200, training loss: 90.30562591552734 = 0.039935480803251266 + 10.0 * 9.026569366455078
Epoch 2200, val loss: 0.8392990231513977
Epoch 2210, training loss: 90.29035949707031 = 0.0392480306327343 + 10.0 * 9.025111198425293
Epoch 2210, val loss: 0.8446590900421143
Epoch 2220, training loss: 90.29603576660156 = 0.03858208656311035 + 10.0 * 9.025745391845703
Epoch 2220, val loss: 0.8496764302253723
Epoch 2230, training loss: 90.28108978271484 = 0.03792380541563034 + 10.0 * 9.024316787719727
Epoch 2230, val loss: 0.8549732565879822
Epoch 2240, training loss: 90.28778076171875 = 0.03729202598333359 + 10.0 * 9.025049209594727
Epoch 2240, val loss: 0.8596022725105286
Epoch 2250, training loss: 90.29084014892578 = 0.036661095917224884 + 10.0 * 9.025418281555176
Epoch 2250, val loss: 0.8648656606674194
Epoch 2260, training loss: 90.28204345703125 = 0.036044780164957047 + 10.0 * 9.0246000289917
Epoch 2260, val loss: 0.8700107932090759
Epoch 2270, training loss: 90.27252197265625 = 0.035438038408756256 + 10.0 * 9.02370834350586
Epoch 2270, val loss: 0.8751097321510315
Epoch 2280, training loss: 90.2723617553711 = 0.03485001623630524 + 10.0 * 9.023751258850098
Epoch 2280, val loss: 0.8804249167442322
Epoch 2290, training loss: 90.28636932373047 = 0.034277502447366714 + 10.0 * 9.025209426879883
Epoch 2290, val loss: 0.8852608799934387
Epoch 2300, training loss: 90.27490234375 = 0.033709194511175156 + 10.0 * 9.02411937713623
Epoch 2300, val loss: 0.8897730708122253
Epoch 2310, training loss: 90.26832580566406 = 0.03314913064241409 + 10.0 * 9.023517608642578
Epoch 2310, val loss: 0.8948028683662415
Epoch 2320, training loss: 90.26571655273438 = 0.03260485455393791 + 10.0 * 9.023310661315918
Epoch 2320, val loss: 0.8994467258453369
Epoch 2330, training loss: 90.27691650390625 = 0.03207751363515854 + 10.0 * 9.024484634399414
Epoch 2330, val loss: 0.9041356444358826
Epoch 2340, training loss: 90.27125549316406 = 0.03155159950256348 + 10.0 * 9.023969650268555
Epoch 2340, val loss: 0.9089730381965637
Epoch 2350, training loss: 90.26404571533203 = 0.031040048226714134 + 10.0 * 9.023301124572754
Epoch 2350, val loss: 0.9136075973510742
Epoch 2360, training loss: 90.27306365966797 = 0.030538516119122505 + 10.0 * 9.024251937866211
Epoch 2360, val loss: 0.9182154536247253
Epoch 2370, training loss: 90.25855255126953 = 0.030044997110962868 + 10.0 * 9.02285099029541
Epoch 2370, val loss: 0.9229060411453247
Epoch 2380, training loss: 90.25615692138672 = 0.02955813519656658 + 10.0 * 9.022660255432129
Epoch 2380, val loss: 0.927506685256958
Epoch 2390, training loss: 90.25765991210938 = 0.029090676456689835 + 10.0 * 9.022856712341309
Epoch 2390, val loss: 0.9321458339691162
Epoch 2400, training loss: 90.27596282958984 = 0.028633996844291687 + 10.0 * 9.02473258972168
Epoch 2400, val loss: 0.9360979199409485
Epoch 2410, training loss: 90.26614379882812 = 0.028174040839076042 + 10.0 * 9.023797035217285
Epoch 2410, val loss: 0.9416872262954712
Epoch 2420, training loss: 90.2649917602539 = 0.027731694281101227 + 10.0 * 9.023725509643555
Epoch 2420, val loss: 0.9452707767486572
Epoch 2430, training loss: 90.25896453857422 = 0.02729455567896366 + 10.0 * 9.02316665649414
Epoch 2430, val loss: 0.9497165083885193
Epoch 2440, training loss: 90.24935150146484 = 0.026863301172852516 + 10.0 * 9.022249221801758
Epoch 2440, val loss: 0.9548622369766235
Epoch 2450, training loss: 90.24920654296875 = 0.026444606482982635 + 10.0 * 9.022275924682617
Epoch 2450, val loss: 0.9588634967803955
Epoch 2460, training loss: 90.2645263671875 = 0.026039378717541695 + 10.0 * 9.023848533630371
Epoch 2460, val loss: 0.9633405804634094
Epoch 2470, training loss: 90.24859619140625 = 0.025633420795202255 + 10.0 * 9.022295951843262
Epoch 2470, val loss: 0.9669475555419922
Epoch 2480, training loss: 90.24630737304688 = 0.025237319990992546 + 10.0 * 9.022107124328613
Epoch 2480, val loss: 0.9714308977127075
Epoch 2490, training loss: 90.25635528564453 = 0.024850448593497276 + 10.0 * 9.023150444030762
Epoch 2490, val loss: 0.9755885004997253
Epoch 2500, training loss: 90.24524688720703 = 0.024465717375278473 + 10.0 * 9.022077560424805
Epoch 2500, val loss: 0.9801157712936401
Epoch 2510, training loss: 90.24138641357422 = 0.024092255160212517 + 10.0 * 9.021729469299316
Epoch 2510, val loss: 0.9841426014900208
Epoch 2520, training loss: 90.24687957763672 = 0.02373119257390499 + 10.0 * 9.02231502532959
Epoch 2520, val loss: 0.988049328327179
Epoch 2530, training loss: 90.23519134521484 = 0.023367291316390038 + 10.0 * 9.0211820602417
Epoch 2530, val loss: 0.9925894141197205
Epoch 2540, training loss: 90.2376708984375 = 0.023013487458229065 + 10.0 * 9.021466255187988
Epoch 2540, val loss: 0.9973124861717224
Epoch 2550, training loss: 90.25357055664062 = 0.022674839943647385 + 10.0 * 9.023089408874512
Epoch 2550, val loss: 1.001656174659729
Epoch 2560, training loss: 90.25324249267578 = 0.02233968675136566 + 10.0 * 9.023090362548828
Epoch 2560, val loss: 1.0043268203735352
Epoch 2570, training loss: 90.23976135253906 = 0.0220005065202713 + 10.0 * 9.02177619934082
Epoch 2570, val loss: 1.0087981224060059
Epoch 2580, training loss: 90.23145294189453 = 0.021673845127224922 + 10.0 * 9.020977973937988
Epoch 2580, val loss: 1.0124216079711914
Epoch 2590, training loss: 90.2461929321289 = 0.021354837343096733 + 10.0 * 9.022483825683594
Epoch 2590, val loss: 1.0162208080291748
Epoch 2600, training loss: 90.22637939453125 = 0.02103867009282112 + 10.0 * 9.020533561706543
Epoch 2600, val loss: 1.020207405090332
Epoch 2610, training loss: 90.22432708740234 = 0.020726585760712624 + 10.0 * 9.020359992980957
Epoch 2610, val loss: 1.0240533351898193
Epoch 2620, training loss: 90.22600555419922 = 0.020422449335455894 + 10.0 * 9.02055835723877
Epoch 2620, val loss: 1.0280462503433228
Epoch 2630, training loss: 90.25719451904297 = 0.020135123282670975 + 10.0 * 9.023706436157227
Epoch 2630, val loss: 1.031334638595581
Epoch 2640, training loss: 90.23832702636719 = 0.01983710005879402 + 10.0 * 9.021848678588867
Epoch 2640, val loss: 1.0357813835144043
Epoch 2650, training loss: 90.22928619384766 = 0.019549565389752388 + 10.0 * 9.020974159240723
Epoch 2650, val loss: 1.0390639305114746
Epoch 2660, training loss: 90.2244873046875 = 0.019264861941337585 + 10.0 * 9.020522117614746
Epoch 2660, val loss: 1.0429699420928955
Epoch 2670, training loss: 90.22179412841797 = 0.01898983120918274 + 10.0 * 9.020280838012695
Epoch 2670, val loss: 1.046562910079956
Epoch 2680, training loss: 90.22740936279297 = 0.018718380481004715 + 10.0 * 9.020869255065918
Epoch 2680, val loss: 1.0506964921951294
Epoch 2690, training loss: 90.23684692382812 = 0.018457802012562752 + 10.0 * 9.021839141845703
Epoch 2690, val loss: 1.0543043613433838
Epoch 2700, training loss: 90.2183837890625 = 0.01819104328751564 + 10.0 * 9.02001953125
Epoch 2700, val loss: 1.0574510097503662
Epoch 2710, training loss: 90.21588134765625 = 0.01793278381228447 + 10.0 * 9.019795417785645
Epoch 2710, val loss: 1.0607901811599731
Epoch 2720, training loss: 90.21173095703125 = 0.017678920179605484 + 10.0 * 9.019405364990234
Epoch 2720, val loss: 1.0646629333496094
Epoch 2730, training loss: 90.21855926513672 = 0.017434965819120407 + 10.0 * 9.020112991333008
Epoch 2730, val loss: 1.0681816339492798
Epoch 2740, training loss: 90.22511291503906 = 0.017195476219058037 + 10.0 * 9.020792007446289
Epoch 2740, val loss: 1.0714185237884521
Epoch 2750, training loss: 90.21357727050781 = 0.016951892524957657 + 10.0 * 9.019662857055664
Epoch 2750, val loss: 1.075111985206604
Epoch 2760, training loss: 90.21544647216797 = 0.016717012971639633 + 10.0 * 9.019872665405273
Epoch 2760, val loss: 1.078742265701294
Epoch 2770, training loss: 90.21756744384766 = 0.01648743264377117 + 10.0 * 9.020108222961426
Epoch 2770, val loss: 1.0823968648910522
Epoch 2780, training loss: 90.21270751953125 = 0.01625884510576725 + 10.0 * 9.019644737243652
Epoch 2780, val loss: 1.0857341289520264
Epoch 2790, training loss: 90.20960998535156 = 0.016036514192819595 + 10.0 * 9.019357681274414
Epoch 2790, val loss: 1.0891060829162598
Epoch 2800, training loss: 90.2295913696289 = 0.015824174508452415 + 10.0 * 9.021376609802246
Epoch 2800, val loss: 1.092049241065979
Epoch 2810, training loss: 90.20990753173828 = 0.015605110675096512 + 10.0 * 9.019430160522461
Epoch 2810, val loss: 1.0956776142120361
Epoch 2820, training loss: 90.20375061035156 = 0.015393936075270176 + 10.0 * 9.018835067749023
Epoch 2820, val loss: 1.0989902019500732
Epoch 2830, training loss: 90.2148208618164 = 0.015190837904810905 + 10.0 * 9.019963264465332
Epoch 2830, val loss: 1.1024527549743652
Epoch 2840, training loss: 90.20201873779297 = 0.014986597932875156 + 10.0 * 9.01870346069336
Epoch 2840, val loss: 1.1053322553634644
Epoch 2850, training loss: 90.1983413696289 = 0.014786852523684502 + 10.0 * 9.018355369567871
Epoch 2850, val loss: 1.1088087558746338
Epoch 2860, training loss: 90.2091293334961 = 0.014595216140151024 + 10.0 * 9.019453048706055
Epoch 2860, val loss: 1.112035870552063
Epoch 2870, training loss: 90.20335388183594 = 0.014404182322323322 + 10.0 * 9.018895149230957
Epoch 2870, val loss: 1.115012764930725
Epoch 2880, training loss: 90.1945571899414 = 0.014212537556886673 + 10.0 * 9.018033981323242
Epoch 2880, val loss: 1.1181975603103638
Epoch 2890, training loss: 90.19454193115234 = 0.014029029756784439 + 10.0 * 9.018051147460938
Epoch 2890, val loss: 1.1211249828338623
Epoch 2900, training loss: 90.2195053100586 = 0.013863747008144855 + 10.0 * 9.020564079284668
Epoch 2900, val loss: 1.123376727104187
Epoch 2910, training loss: 90.19512939453125 = 0.013674194924533367 + 10.0 * 9.018145561218262
Epoch 2910, val loss: 1.1281152963638306
Epoch 2920, training loss: 90.19315338134766 = 0.013498637825250626 + 10.0 * 9.017965316772461
Epoch 2920, val loss: 1.1305055618286133
Epoch 2930, training loss: 90.19341278076172 = 0.013327735476195812 + 10.0 * 9.0180082321167
Epoch 2930, val loss: 1.1339733600616455
Epoch 2940, training loss: 90.217529296875 = 0.013166314922273159 + 10.0 * 9.02043628692627
Epoch 2940, val loss: 1.1365597248077393
Epoch 2950, training loss: 90.21096801757812 = 0.013001115061342716 + 10.0 * 9.019796371459961
Epoch 2950, val loss: 1.1400738954544067
Epoch 2960, training loss: 90.19457244873047 = 0.012837499380111694 + 10.0 * 9.018173217773438
Epoch 2960, val loss: 1.1424951553344727
Epoch 2970, training loss: 90.18775939941406 = 0.012679199688136578 + 10.0 * 9.017507553100586
Epoch 2970, val loss: 1.1457505226135254
Epoch 2980, training loss: 90.1877670288086 = 0.01252420712262392 + 10.0 * 9.017523765563965
Epoch 2980, val loss: 1.1486378908157349
Epoch 2990, training loss: 90.21031188964844 = 0.012377002276480198 + 10.0 * 9.019793510437012
Epoch 2990, val loss: 1.151887059211731
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8559
Overall ASR: 0.6866
Flip ASR: 0.6088/1554 nodes
./selected_nodes/Pubmed/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 104.67606353759766 = 1.0853997468948364 + 10.0 * 10.359066009521484
Epoch 0, val loss: 1.0851929187774658
Epoch 10, training loss: 104.62995910644531 = 1.0761758089065552 + 10.0 * 10.355378150939941
Epoch 10, val loss: 1.0757977962493896
Epoch 20, training loss: 104.06663513183594 = 1.0646358728408813 + 10.0 * 10.300199508666992
Epoch 20, val loss: 1.064257025718689
Epoch 30, training loss: 100.72952270507812 = 1.0563080310821533 + 10.0 * 9.967321395874023
Epoch 30, val loss: 1.0560085773468018
Epoch 40, training loss: 97.67147827148438 = 1.0448498725891113 + 10.0 * 9.662662506103516
Epoch 40, val loss: 1.0432851314544678
Epoch 50, training loss: 95.6143569946289 = 1.0226824283599854 + 10.0 * 9.45916748046875
Epoch 50, val loss: 1.0204576253890991
Epoch 60, training loss: 94.8899917602539 = 0.999530553817749 + 10.0 * 9.389046669006348
Epoch 60, val loss: 0.9979254007339478
Epoch 70, training loss: 94.49614715576172 = 0.9805678129196167 + 10.0 * 9.351557731628418
Epoch 70, val loss: 0.9795072078704834
Epoch 80, training loss: 93.97877502441406 = 0.9629506468772888 + 10.0 * 9.301582336425781
Epoch 80, val loss: 0.9622365236282349
Epoch 90, training loss: 93.42536926269531 = 0.9459069967269897 + 10.0 * 9.247945785522461
Epoch 90, val loss: 0.9453666806221008
Epoch 100, training loss: 93.13906860351562 = 0.926325261592865 + 10.0 * 9.221274375915527
Epoch 100, val loss: 0.9257396459579468
Epoch 110, training loss: 92.99885559082031 = 0.8974936604499817 + 10.0 * 9.210136413574219
Epoch 110, val loss: 0.8972126245498657
Epoch 120, training loss: 92.82463073730469 = 0.8629277348518372 + 10.0 * 9.19616985321045
Epoch 120, val loss: 0.8642354607582092
Epoch 130, training loss: 92.64934539794922 = 0.829276442527771 + 10.0 * 9.1820068359375
Epoch 130, val loss: 0.8321878910064697
Epoch 140, training loss: 92.50022888183594 = 0.7952809929847717 + 10.0 * 9.17049503326416
Epoch 140, val loss: 0.7997597455978394
Epoch 150, training loss: 92.34562683105469 = 0.7584351897239685 + 10.0 * 9.158719062805176
Epoch 150, val loss: 0.765434741973877
Epoch 160, training loss: 92.21224975585938 = 0.7204720973968506 + 10.0 * 9.149177551269531
Epoch 160, val loss: 0.7300782799720764
Epoch 170, training loss: 92.11598205566406 = 0.6818506121635437 + 10.0 * 9.143412590026855
Epoch 170, val loss: 0.6946160197257996
Epoch 180, training loss: 92.01130676269531 = 0.6438984870910645 + 10.0 * 9.136740684509277
Epoch 180, val loss: 0.6603444218635559
Epoch 190, training loss: 91.91454315185547 = 0.6092654466629028 + 10.0 * 9.13052749633789
Epoch 190, val loss: 0.629522979259491
Epoch 200, training loss: 91.835693359375 = 0.5786116123199463 + 10.0 * 9.125707626342773
Epoch 200, val loss: 0.6026240587234497
Epoch 210, training loss: 91.76533508300781 = 0.5515019297599792 + 10.0 * 9.121383666992188
Epoch 210, val loss: 0.579046368598938
Epoch 220, training loss: 91.70268249511719 = 0.5278050303459167 + 10.0 * 9.117487907409668
Epoch 220, val loss: 0.5587685108184814
Epoch 230, training loss: 91.6484375 = 0.507243812084198 + 10.0 * 9.114119529724121
Epoch 230, val loss: 0.5413833260536194
Epoch 240, training loss: 91.62381744384766 = 0.4891064465045929 + 10.0 * 9.113471031188965
Epoch 240, val loss: 0.5262696146965027
Epoch 250, training loss: 91.56583404541016 = 0.4730468690395355 + 10.0 * 9.109278678894043
Epoch 250, val loss: 0.5130680203437805
Epoch 260, training loss: 91.50901794433594 = 0.4591696560382843 + 10.0 * 9.104985237121582
Epoch 260, val loss: 0.5018641948699951
Epoch 270, training loss: 91.46354675292969 = 0.44688278436660767 + 10.0 * 9.101666450500488
Epoch 270, val loss: 0.4921240508556366
Epoch 280, training loss: 91.42269134521484 = 0.4357243478298187 + 10.0 * 9.0986967086792
Epoch 280, val loss: 0.4834214746952057
Epoch 290, training loss: 91.41849517822266 = 0.4254698157310486 + 10.0 * 9.099302291870117
Epoch 290, val loss: 0.4756092429161072
Epoch 300, training loss: 91.36437225341797 = 0.41605278849601746 + 10.0 * 9.094831466674805
Epoch 300, val loss: 0.4687085449695587
Epoch 310, training loss: 91.31814575195312 = 0.40768715739250183 + 10.0 * 9.091046333312988
Epoch 310, val loss: 0.4626798927783966
Epoch 320, training loss: 91.2906265258789 = 0.3999020457267761 + 10.0 * 9.089072227478027
Epoch 320, val loss: 0.45736441016197205
Epoch 330, training loss: 91.26238250732422 = 0.3925172686576843 + 10.0 * 9.086986541748047
Epoch 330, val loss: 0.45243725180625916
Epoch 340, training loss: 91.27885437011719 = 0.38553792238235474 + 10.0 * 9.08933162689209
Epoch 340, val loss: 0.44794824719429016
Epoch 350, training loss: 91.22246551513672 = 0.3790542781352997 + 10.0 * 9.084341049194336
Epoch 350, val loss: 0.4438988268375397
Epoch 360, training loss: 91.19036865234375 = 0.37309733033180237 + 10.0 * 9.081727027893066
Epoch 360, val loss: 0.44027450680732727
Epoch 370, training loss: 91.17556762695312 = 0.3674176335334778 + 10.0 * 9.080815315246582
Epoch 370, val loss: 0.43701454997062683
Epoch 380, training loss: 91.1536865234375 = 0.3619746267795563 + 10.0 * 9.079171180725098
Epoch 380, val loss: 0.43389108777046204
Epoch 390, training loss: 91.13646697998047 = 0.35683369636535645 + 10.0 * 9.077962875366211
Epoch 390, val loss: 0.4310345947742462
Epoch 400, training loss: 91.126953125 = 0.35194841027259827 + 10.0 * 9.077500343322754
Epoch 400, val loss: 0.4284439980983734
Epoch 410, training loss: 91.11155700683594 = 0.3472454249858856 + 10.0 * 9.076431274414062
Epoch 410, val loss: 0.4259202480316162
Epoch 420, training loss: 91.08345031738281 = 0.3428269028663635 + 10.0 * 9.07406234741211
Epoch 420, val loss: 0.4236721396446228
Epoch 430, training loss: 91.06684875488281 = 0.33862727880477905 + 10.0 * 9.072821617126465
Epoch 430, val loss: 0.42158520221710205
Epoch 440, training loss: 91.05332946777344 = 0.334545761346817 + 10.0 * 9.071878433227539
Epoch 440, val loss: 0.41958585381507874
Epoch 450, training loss: 91.0635986328125 = 0.3305738568305969 + 10.0 * 9.073302268981934
Epoch 450, val loss: 0.4177079200744629
Epoch 460, training loss: 91.03910827636719 = 0.32675644755363464 + 10.0 * 9.071234703063965
Epoch 460, val loss: 0.4159508943557739
Epoch 470, training loss: 91.01793670654297 = 0.32312047481536865 + 10.0 * 9.06948184967041
Epoch 470, val loss: 0.41436901688575745
Epoch 480, training loss: 91.01144409179688 = 0.3195795714855194 + 10.0 * 9.069186210632324
Epoch 480, val loss: 0.41290906071662903
Epoch 490, training loss: 90.9917984008789 = 0.31611087918281555 + 10.0 * 9.0675687789917
Epoch 490, val loss: 0.4114573299884796
Epoch 500, training loss: 90.98229217529297 = 0.3127498924732208 + 10.0 * 9.066953659057617
Epoch 500, val loss: 0.410068154335022
Epoch 510, training loss: 90.97369384765625 = 0.30947262048721313 + 10.0 * 9.066422462463379
Epoch 510, val loss: 0.4088660478591919
Epoch 520, training loss: 90.96537780761719 = 0.30625879764556885 + 10.0 * 9.065912246704102
Epoch 520, val loss: 0.4076630771160126
Epoch 530, training loss: 90.98081970214844 = 0.3031134307384491 + 10.0 * 9.067770004272461
Epoch 530, val loss: 0.4066687226295471
Epoch 540, training loss: 90.95490264892578 = 0.3000672161579132 + 10.0 * 9.065484046936035
Epoch 540, val loss: 0.4055885672569275
Epoch 550, training loss: 90.93189239501953 = 0.29711389541625977 + 10.0 * 9.063478469848633
Epoch 550, val loss: 0.40471264719963074
Epoch 560, training loss: 90.91940307617188 = 0.2942202687263489 + 10.0 * 9.062518119812012
Epoch 560, val loss: 0.4038164019584656
Epoch 570, training loss: 90.92078399658203 = 0.29136309027671814 + 10.0 * 9.062941551208496
Epoch 570, val loss: 0.4030393958091736
Epoch 580, training loss: 90.94200897216797 = 0.2885465621948242 + 10.0 * 9.065346717834473
Epoch 580, val loss: 0.4023672342300415
Epoch 590, training loss: 90.90796661376953 = 0.2858268618583679 + 10.0 * 9.062213897705078
Epoch 590, val loss: 0.40164393186569214
Epoch 600, training loss: 90.88938903808594 = 0.2831972539424896 + 10.0 * 9.060619354248047
Epoch 600, val loss: 0.40114420652389526
Epoch 610, training loss: 90.87757110595703 = 0.2806014120578766 + 10.0 * 9.059697151184082
Epoch 610, val loss: 0.4006819725036621
Epoch 620, training loss: 90.87084197998047 = 0.2780299484729767 + 10.0 * 9.059281349182129
Epoch 620, val loss: 0.4002704620361328
Epoch 630, training loss: 90.88087463378906 = 0.27548936009407043 + 10.0 * 9.060538291931152
Epoch 630, val loss: 0.39999961853027344
Epoch 640, training loss: 90.86614990234375 = 0.2730084955692291 + 10.0 * 9.059313774108887
Epoch 640, val loss: 0.39965957403182983
Epoch 650, training loss: 90.84647369384766 = 0.2705900967121124 + 10.0 * 9.057588577270508
Epoch 650, val loss: 0.39941465854644775
Epoch 660, training loss: 90.8423843383789 = 0.26820382475852966 + 10.0 * 9.057417869567871
Epoch 660, val loss: 0.39916515350341797
Epoch 670, training loss: 90.83829498291016 = 0.26584434509277344 + 10.0 * 9.057245254516602
Epoch 670, val loss: 0.3991730213165283
Epoch 680, training loss: 90.84144592285156 = 0.26353588700294495 + 10.0 * 9.057790756225586
Epoch 680, val loss: 0.39896267652511597
Epoch 690, training loss: 90.82015991210938 = 0.26127123832702637 + 10.0 * 9.055889129638672
Epoch 690, val loss: 0.3991266191005707
Epoch 700, training loss: 90.8128890991211 = 0.25904014706611633 + 10.0 * 9.055384635925293
Epoch 700, val loss: 0.39922767877578735
Epoch 710, training loss: 90.81109619140625 = 0.2568238079547882 + 10.0 * 9.055427551269531
Epoch 710, val loss: 0.3993995487689972
Epoch 720, training loss: 90.80555725097656 = 0.254635214805603 + 10.0 * 9.055091857910156
Epoch 720, val loss: 0.3993609845638275
Epoch 730, training loss: 90.80081939697266 = 0.2524881064891815 + 10.0 * 9.05483341217041
Epoch 730, val loss: 0.39965131878852844
Epoch 740, training loss: 90.78836822509766 = 0.25037720799446106 + 10.0 * 9.053799629211426
Epoch 740, val loss: 0.39993223547935486
Epoch 750, training loss: 90.79483795166016 = 0.24828243255615234 + 10.0 * 9.054655075073242
Epoch 750, val loss: 0.40019044280052185
Epoch 760, training loss: 90.77989959716797 = 0.2462061494588852 + 10.0 * 9.053369522094727
Epoch 760, val loss: 0.40050825476646423
Epoch 770, training loss: 90.78954315185547 = 0.24414964020252228 + 10.0 * 9.054539680480957
Epoch 770, val loss: 0.40089428424835205
Epoch 780, training loss: 90.767822265625 = 0.24211448431015015 + 10.0 * 9.052571296691895
Epoch 780, val loss: 0.4012388586997986
Epoch 790, training loss: 90.75505065917969 = 0.24010078608989716 + 10.0 * 9.051495552062988
Epoch 790, val loss: 0.40165162086486816
Epoch 800, training loss: 90.75182342529297 = 0.23809945583343506 + 10.0 * 9.051372528076172
Epoch 800, val loss: 0.40205588936805725
Epoch 810, training loss: 90.7573013305664 = 0.23610152304172516 + 10.0 * 9.052120208740234
Epoch 810, val loss: 0.40262362360954285
Epoch 820, training loss: 90.74330139160156 = 0.23411761224269867 + 10.0 * 9.050918579101562
Epoch 820, val loss: 0.40322890877723694
Epoch 830, training loss: 90.79244995117188 = 0.23214255273342133 + 10.0 * 9.056031227111816
Epoch 830, val loss: 0.40368103981018066
Epoch 840, training loss: 90.72945404052734 = 0.2301926612854004 + 10.0 * 9.049925804138184
Epoch 840, val loss: 0.4043522775173187
Epoch 850, training loss: 90.7218017578125 = 0.2282654196023941 + 10.0 * 9.04935359954834
Epoch 850, val loss: 0.4049980044364929
Epoch 860, training loss: 90.71644592285156 = 0.22634685039520264 + 10.0 * 9.049009323120117
Epoch 860, val loss: 0.405695378780365
Epoch 870, training loss: 90.70970916748047 = 0.22443021833896637 + 10.0 * 9.048527717590332
Epoch 870, val loss: 0.40640994906425476
Epoch 880, training loss: 90.74544525146484 = 0.22252295911312103 + 10.0 * 9.052291870117188
Epoch 880, val loss: 0.4071110785007477
Epoch 890, training loss: 90.7073745727539 = 0.220627099275589 + 10.0 * 9.048674583435059
Epoch 890, val loss: 0.40800148248672485
Epoch 900, training loss: 90.6994400024414 = 0.218746155500412 + 10.0 * 9.048069953918457
Epoch 900, val loss: 0.4088611900806427
Epoch 910, training loss: 90.69981384277344 = 0.2168729156255722 + 10.0 * 9.048294067382812
Epoch 910, val loss: 0.4096374809741974
Epoch 920, training loss: 90.71989440917969 = 0.21501046419143677 + 10.0 * 9.050488471984863
Epoch 920, val loss: 0.4106414020061493
Epoch 930, training loss: 90.68602752685547 = 0.21315720677375793 + 10.0 * 9.047286987304688
Epoch 930, val loss: 0.4116291105747223
Epoch 940, training loss: 90.67378234863281 = 0.2113160490989685 + 10.0 * 9.046246528625488
Epoch 940, val loss: 0.41269341111183167
Epoch 950, training loss: 90.66925048828125 = 0.209474116563797 + 10.0 * 9.045977592468262
Epoch 950, val loss: 0.4136723577976227
Epoch 960, training loss: 90.67902374267578 = 0.2076338827610016 + 10.0 * 9.047139167785645
Epoch 960, val loss: 0.4147356152534485
Epoch 970, training loss: 90.6915054321289 = 0.20580986142158508 + 10.0 * 9.048569679260254
Epoch 970, val loss: 0.4161315858364105
Epoch 980, training loss: 90.65825653076172 = 0.203992560505867 + 10.0 * 9.045426368713379
Epoch 980, val loss: 0.4171096086502075
Epoch 990, training loss: 90.65302276611328 = 0.20218592882156372 + 10.0 * 9.045083999633789
Epoch 990, val loss: 0.4182630181312561
Epoch 1000, training loss: 90.64762115478516 = 0.20037943124771118 + 10.0 * 9.044724464416504
Epoch 1000, val loss: 0.4194714426994324
Epoch 1010, training loss: 90.64315032958984 = 0.198568657040596 + 10.0 * 9.044458389282227
Epoch 1010, val loss: 0.42076700925827026
Epoch 1020, training loss: 90.69711303710938 = 0.19676551222801208 + 10.0 * 9.05003547668457
Epoch 1020, val loss: 0.4221273958683014
Epoch 1030, training loss: 90.64276885986328 = 0.19496357440948486 + 10.0 * 9.044780731201172
Epoch 1030, val loss: 0.4235507547855377
Epoch 1040, training loss: 90.63570404052734 = 0.19317132234573364 + 10.0 * 9.0442533493042
Epoch 1040, val loss: 0.4248516261577606
Epoch 1050, training loss: 90.62564849853516 = 0.191382497549057 + 10.0 * 9.043426513671875
Epoch 1050, val loss: 0.4262596070766449
Epoch 1060, training loss: 90.63623809814453 = 0.1895948052406311 + 10.0 * 9.04466438293457
Epoch 1060, val loss: 0.42767706513404846
Epoch 1070, training loss: 90.61734771728516 = 0.1878071278333664 + 10.0 * 9.042954444885254
Epoch 1070, val loss: 0.4293390214443207
Epoch 1080, training loss: 90.61936950683594 = 0.1860286146402359 + 10.0 * 9.043334007263184
Epoch 1080, val loss: 0.4307456612586975
Epoch 1090, training loss: 90.61346435546875 = 0.18425138294696808 + 10.0 * 9.04292106628418
Epoch 1090, val loss: 0.4323875606060028
Epoch 1100, training loss: 90.62326049804688 = 0.18247778713703156 + 10.0 * 9.04407787322998
Epoch 1100, val loss: 0.4339127540588379
Epoch 1110, training loss: 90.6084976196289 = 0.1807079166173935 + 10.0 * 9.042778968811035
Epoch 1110, val loss: 0.4358370304107666
Epoch 1120, training loss: 90.60426330566406 = 0.17893292009830475 + 10.0 * 9.042532920837402
Epoch 1120, val loss: 0.43738847970962524
Epoch 1130, training loss: 90.6012191772461 = 0.17716318368911743 + 10.0 * 9.04240608215332
Epoch 1130, val loss: 0.4390479326248169
Epoch 1140, training loss: 90.60831451416016 = 0.17539159953594208 + 10.0 * 9.043292045593262
Epoch 1140, val loss: 0.44068217277526855
Epoch 1150, training loss: 90.58977508544922 = 0.173623725771904 + 10.0 * 9.04161548614502
Epoch 1150, val loss: 0.4425729513168335
Epoch 1160, training loss: 90.58251953125 = 0.1718568652868271 + 10.0 * 9.04106616973877
Epoch 1160, val loss: 0.44433867931365967
Epoch 1170, training loss: 90.57938385009766 = 0.1700831949710846 + 10.0 * 9.040929794311523
Epoch 1170, val loss: 0.4462118446826935
Epoch 1180, training loss: 90.59770202636719 = 0.1683110147714615 + 10.0 * 9.042939186096191
Epoch 1180, val loss: 0.4481842517852783
Epoch 1190, training loss: 90.58748626708984 = 0.1665336638689041 + 10.0 * 9.042095184326172
Epoch 1190, val loss: 0.4498576819896698
Epoch 1200, training loss: 90.57168579101562 = 0.16476105153560638 + 10.0 * 9.040692329406738
Epoch 1200, val loss: 0.45191165804862976
Epoch 1210, training loss: 90.56268310546875 = 0.1629841923713684 + 10.0 * 9.039969444274902
Epoch 1210, val loss: 0.45384132862091064
Epoch 1220, training loss: 90.57211303710938 = 0.16120320558547974 + 10.0 * 9.041090965270996
Epoch 1220, val loss: 0.45602473616600037
Epoch 1230, training loss: 90.56130981445312 = 0.15942031145095825 + 10.0 * 9.040188789367676
Epoch 1230, val loss: 0.45795297622680664
Epoch 1240, training loss: 90.55296325683594 = 0.1576370894908905 + 10.0 * 9.039532661437988
Epoch 1240, val loss: 0.4599238634109497
Epoch 1250, training loss: 90.54731750488281 = 0.15584798157215118 + 10.0 * 9.03914737701416
Epoch 1250, val loss: 0.46201208233833313
Epoch 1260, training loss: 90.54530334472656 = 0.15405356884002686 + 10.0 * 9.039125442504883
Epoch 1260, val loss: 0.4642658233642578
Epoch 1270, training loss: 90.5723648071289 = 0.1522580087184906 + 10.0 * 9.042010307312012
Epoch 1270, val loss: 0.4663953483104706
Epoch 1280, training loss: 90.54014587402344 = 0.15044859051704407 + 10.0 * 9.038969039916992
Epoch 1280, val loss: 0.4688389003276825
Epoch 1290, training loss: 90.53250885009766 = 0.14864130318164825 + 10.0 * 9.038386344909668
Epoch 1290, val loss: 0.47107595205307007
Epoch 1300, training loss: 90.52845001220703 = 0.1468214988708496 + 10.0 * 9.038163185119629
Epoch 1300, val loss: 0.47337114810943604
Epoch 1310, training loss: 90.55237579345703 = 0.14499728381633759 + 10.0 * 9.040738105773926
Epoch 1310, val loss: 0.47594425082206726
Epoch 1320, training loss: 90.544189453125 = 0.14317041635513306 + 10.0 * 9.040102005004883
Epoch 1320, val loss: 0.47835877537727356
Epoch 1330, training loss: 90.52207946777344 = 0.14132745563983917 + 10.0 * 9.03807544708252
Epoch 1330, val loss: 0.4806598424911499
Epoch 1340, training loss: 90.52037048339844 = 0.13948239386081696 + 10.0 * 9.03808879852295
Epoch 1340, val loss: 0.4833677411079407
Epoch 1350, training loss: 90.51790618896484 = 0.13762497901916504 + 10.0 * 9.0380277633667
Epoch 1350, val loss: 0.4859325885772705
Epoch 1360, training loss: 90.50601196289062 = 0.1357521116733551 + 10.0 * 9.037026405334473
Epoch 1360, val loss: 0.4885737895965576
Epoch 1370, training loss: 90.50496673583984 = 0.13388314843177795 + 10.0 * 9.037108421325684
Epoch 1370, val loss: 0.4914465844631195
Epoch 1380, training loss: 90.5171127319336 = 0.13201668858528137 + 10.0 * 9.038509368896484
Epoch 1380, val loss: 0.4942439794540405
Epoch 1390, training loss: 90.49978637695312 = 0.13014689087867737 + 10.0 * 9.03696346282959
Epoch 1390, val loss: 0.4967218041419983
Epoch 1400, training loss: 90.49906921386719 = 0.12827807664871216 + 10.0 * 9.037078857421875
Epoch 1400, val loss: 0.499755859375
Epoch 1410, training loss: 90.49044799804688 = 0.12641774117946625 + 10.0 * 9.036402702331543
Epoch 1410, val loss: 0.5027087926864624
Epoch 1420, training loss: 90.49093627929688 = 0.12456754595041275 + 10.0 * 9.036637306213379
Epoch 1420, val loss: 0.505626380443573
Epoch 1430, training loss: 90.5074691772461 = 0.12273411452770233 + 10.0 * 9.038473129272461
Epoch 1430, val loss: 0.508831799030304
Epoch 1440, training loss: 90.47831726074219 = 0.12089352309703827 + 10.0 * 9.035741806030273
Epoch 1440, val loss: 0.5111500024795532
Epoch 1450, training loss: 90.47076416015625 = 0.11906182765960693 + 10.0 * 9.03516960144043
Epoch 1450, val loss: 0.5143823027610779
Epoch 1460, training loss: 90.4677734375 = 0.11723446100950241 + 10.0 * 9.035054206848145
Epoch 1460, val loss: 0.5175514221191406
Epoch 1470, training loss: 90.4942855834961 = 0.11542545258998871 + 10.0 * 9.037885665893555
Epoch 1470, val loss: 0.5204973220825195
Epoch 1480, training loss: 90.47734832763672 = 0.11362246423959732 + 10.0 * 9.036372184753418
Epoch 1480, val loss: 0.5240487456321716
Epoch 1490, training loss: 90.46355438232422 = 0.11182741820812225 + 10.0 * 9.035173416137695
Epoch 1490, val loss: 0.5269753932952881
Epoch 1500, training loss: 90.45338439941406 = 0.11004900932312012 + 10.0 * 9.034334182739258
Epoch 1500, val loss: 0.5301928520202637
Epoch 1510, training loss: 90.45216369628906 = 0.10827724635601044 + 10.0 * 9.034388542175293
Epoch 1510, val loss: 0.5336301922798157
Epoch 1520, training loss: 90.4665298461914 = 0.10652750730514526 + 10.0 * 9.03600025177002
Epoch 1520, val loss: 0.5368874669075012
Epoch 1530, training loss: 90.45228576660156 = 0.10479462146759033 + 10.0 * 9.034749031066895
Epoch 1530, val loss: 0.5401858687400818
Epoch 1540, training loss: 90.44926452636719 = 0.10307567566633224 + 10.0 * 9.034619331359863
Epoch 1540, val loss: 0.5433840751647949
Epoch 1550, training loss: 90.44244384765625 = 0.10137109458446503 + 10.0 * 9.034107208251953
Epoch 1550, val loss: 0.5467954874038696
Epoch 1560, training loss: 90.43952941894531 = 0.0996825098991394 + 10.0 * 9.033984184265137
Epoch 1560, val loss: 0.5505356192588806
Epoch 1570, training loss: 90.43794250488281 = 0.09801310300827026 + 10.0 * 9.033992767333984
Epoch 1570, val loss: 0.554214358329773
Epoch 1580, training loss: 90.44349670410156 = 0.0963636264204979 + 10.0 * 9.034712791442871
Epoch 1580, val loss: 0.5577180981636047
Epoch 1590, training loss: 90.43245697021484 = 0.09470949321985245 + 10.0 * 9.033774375915527
Epoch 1590, val loss: 0.5607319474220276
Epoch 1600, training loss: 90.42291259765625 = 0.09308063983917236 + 10.0 * 9.032983779907227
Epoch 1600, val loss: 0.5642417669296265
Epoch 1610, training loss: 90.4222412109375 = 0.09147217869758606 + 10.0 * 9.033077239990234
Epoch 1610, val loss: 0.5678622722625732
Epoch 1620, training loss: 90.42826080322266 = 0.08988968282938004 + 10.0 * 9.03383731842041
Epoch 1620, val loss: 0.5715868473052979
Epoch 1630, training loss: 90.42119598388672 = 0.08832506090402603 + 10.0 * 9.033287048339844
Epoch 1630, val loss: 0.5749101638793945
Epoch 1640, training loss: 90.42041015625 = 0.08678359538316727 + 10.0 * 9.03336238861084
Epoch 1640, val loss: 0.5783623456954956
Epoch 1650, training loss: 90.41443634033203 = 0.08525953441858292 + 10.0 * 9.032917976379395
Epoch 1650, val loss: 0.5821610689163208
Epoch 1660, training loss: 90.4076919555664 = 0.08375520259141922 + 10.0 * 9.032393455505371
Epoch 1660, val loss: 0.5857895612716675
Epoch 1670, training loss: 90.40193939208984 = 0.0822739452123642 + 10.0 * 9.031966209411621
Epoch 1670, val loss: 0.5893151760101318
Epoch 1680, training loss: 90.4114761352539 = 0.08081759512424469 + 10.0 * 9.033065795898438
Epoch 1680, val loss: 0.5928574204444885
Epoch 1690, training loss: 90.40022277832031 = 0.07937385886907578 + 10.0 * 9.032084465026855
Epoch 1690, val loss: 0.5964579582214355
Epoch 1700, training loss: 90.39470672607422 = 0.07795120030641556 + 10.0 * 9.031675338745117
Epoch 1700, val loss: 0.6004726886749268
Epoch 1710, training loss: 90.38941192626953 = 0.07654865086078644 + 10.0 * 9.031286239624023
Epoch 1710, val loss: 0.6042763590812683
Epoch 1720, training loss: 90.38975524902344 = 0.0751638412475586 + 10.0 * 9.031458854675293
Epoch 1720, val loss: 0.6082279682159424
Epoch 1730, training loss: 90.41339874267578 = 0.07381908595561981 + 10.0 * 9.033957481384277
Epoch 1730, val loss: 0.6120368242263794
Epoch 1740, training loss: 90.38230895996094 = 0.07246364653110504 + 10.0 * 9.030984878540039
Epoch 1740, val loss: 0.6156660318374634
Epoch 1750, training loss: 90.37989807128906 = 0.07114313542842865 + 10.0 * 9.030875205993652
Epoch 1750, val loss: 0.6194790601730347
Epoch 1760, training loss: 90.37451171875 = 0.06983917206525803 + 10.0 * 9.03046703338623
Epoch 1760, val loss: 0.6232504844665527
Epoch 1770, training loss: 90.38459014892578 = 0.06856455653905869 + 10.0 * 9.03160285949707
Epoch 1770, val loss: 0.6273475289344788
Epoch 1780, training loss: 90.37263488769531 = 0.06730644404888153 + 10.0 * 9.030532836914062
Epoch 1780, val loss: 0.6314093470573425
Epoch 1790, training loss: 90.3699951171875 = 0.06607038527727127 + 10.0 * 9.03039264678955
Epoch 1790, val loss: 0.6349459886550903
Epoch 1800, training loss: 90.37468719482422 = 0.0648614764213562 + 10.0 * 9.03098201751709
Epoch 1800, val loss: 0.6390222311019897
Epoch 1810, training loss: 90.36346435546875 = 0.0636620819568634 + 10.0 * 9.029980659484863
Epoch 1810, val loss: 0.6429390907287598
Epoch 1820, training loss: 90.38117218017578 = 0.062503881752491 + 10.0 * 9.031866073608398
Epoch 1820, val loss: 0.6466473937034607
Epoch 1830, training loss: 90.3653793334961 = 0.0613529309630394 + 10.0 * 9.030402183532715
Epoch 1830, val loss: 0.651311993598938
Epoch 1840, training loss: 90.35608673095703 = 0.06022198498249054 + 10.0 * 9.029586791992188
Epoch 1840, val loss: 0.6547871828079224
Epoch 1850, training loss: 90.35675048828125 = 0.05911506712436676 + 10.0 * 9.029764175415039
Epoch 1850, val loss: 0.6592262983322144
Epoch 1860, training loss: 90.37164306640625 = 0.05803199112415314 + 10.0 * 9.03136157989502
Epoch 1860, val loss: 0.6631999015808105
Epoch 1870, training loss: 90.35128784179688 = 0.0569632351398468 + 10.0 * 9.02943229675293
Epoch 1870, val loss: 0.6671498417854309
Epoch 1880, training loss: 90.34589385986328 = 0.0559128001332283 + 10.0 * 9.028997421264648
Epoch 1880, val loss: 0.6716234683990479
Epoch 1890, training loss: 90.36544799804688 = 0.05489186570048332 + 10.0 * 9.031055450439453
Epoch 1890, val loss: 0.6757009029388428
Epoch 1900, training loss: 90.34418487548828 = 0.05388510599732399 + 10.0 * 9.029029846191406
Epoch 1900, val loss: 0.6788548231124878
Epoch 1910, training loss: 90.33829498291016 = 0.05288852006196976 + 10.0 * 9.02854061126709
Epoch 1910, val loss: 0.6831824779510498
Epoch 1920, training loss: 90.34351348876953 = 0.051925528794527054 + 10.0 * 9.029158592224121
Epoch 1920, val loss: 0.6867280602455139
Epoch 1930, training loss: 90.34402465820312 = 0.05097715184092522 + 10.0 * 9.029304504394531
Epoch 1930, val loss: 0.6904120445251465
Epoch 1940, training loss: 90.3374252319336 = 0.0500447154045105 + 10.0 * 9.028738021850586
Epoch 1940, val loss: 0.6943755745887756
Epoch 1950, training loss: 90.33882141113281 = 0.04913264513015747 + 10.0 * 9.028968811035156
Epoch 1950, val loss: 0.6982544660568237
Epoch 1960, training loss: 90.33431243896484 = 0.04823816940188408 + 10.0 * 9.028607368469238
Epoch 1960, val loss: 0.7026925683021545
Epoch 1970, training loss: 90.32683563232422 = 0.04736074432730675 + 10.0 * 9.027947425842285
Epoch 1970, val loss: 0.7063273191452026
Epoch 1980, training loss: 90.32481384277344 = 0.046502187848091125 + 10.0 * 9.027831077575684
Epoch 1980, val loss: 0.7101786136627197
Epoch 1990, training loss: 90.35843658447266 = 0.04567409306764603 + 10.0 * 9.031275749206543
Epoch 1990, val loss: 0.7137892246246338
Epoch 2000, training loss: 90.32701873779297 = 0.044839441776275635 + 10.0 * 9.028218269348145
Epoch 2000, val loss: 0.7171581387519836
Epoch 2010, training loss: 90.31684112548828 = 0.044030457735061646 + 10.0 * 9.027280807495117
Epoch 2010, val loss: 0.72133868932724
Epoch 2020, training loss: 90.31759643554688 = 0.04323945939540863 + 10.0 * 9.027436256408691
Epoch 2020, val loss: 0.7249694466590881
Epoch 2030, training loss: 90.33924865722656 = 0.04247089475393295 + 10.0 * 9.029677391052246
Epoch 2030, val loss: 0.7288324236869812
Epoch 2040, training loss: 90.31753540039062 = 0.041703980416059494 + 10.0 * 9.027583122253418
Epoch 2040, val loss: 0.7326350212097168
Epoch 2050, training loss: 90.31169128417969 = 0.04095425829291344 + 10.0 * 9.027073860168457
Epoch 2050, val loss: 0.7357456684112549
Epoch 2060, training loss: 90.33480072021484 = 0.0402352400124073 + 10.0 * 9.02945613861084
Epoch 2060, val loss: 0.7388342022895813
Epoch 2070, training loss: 90.30921173095703 = 0.03951285779476166 + 10.0 * 9.026969909667969
Epoch 2070, val loss: 0.7440016865730286
Epoch 2080, training loss: 90.3048324584961 = 0.03880463168025017 + 10.0 * 9.026602745056152
Epoch 2080, val loss: 0.7468001842498779
Epoch 2090, training loss: 90.30732727050781 = 0.03811374679207802 + 10.0 * 9.026921272277832
Epoch 2090, val loss: 0.7509893774986267
Epoch 2100, training loss: 90.3239974975586 = 0.03744523972272873 + 10.0 * 9.028655052185059
Epoch 2100, val loss: 0.754219651222229
Epoch 2110, training loss: 90.30414581298828 = 0.03677680343389511 + 10.0 * 9.026737213134766
Epoch 2110, val loss: 0.7580423951148987
Epoch 2120, training loss: 90.29761505126953 = 0.03612513840198517 + 10.0 * 9.026148796081543
Epoch 2120, val loss: 0.7614889740943909
Epoch 2130, training loss: 90.30149841308594 = 0.035492122173309326 + 10.0 * 9.02660083770752
Epoch 2130, val loss: 0.7651757001876831
Epoch 2140, training loss: 90.30487060546875 = 0.034872401505708694 + 10.0 * 9.026999473571777
Epoch 2140, val loss: 0.7688155770301819
Epoch 2150, training loss: 90.30521392822266 = 0.03426487743854523 + 10.0 * 9.027094841003418
Epoch 2150, val loss: 0.7723572254180908
Epoch 2160, training loss: 90.2999038696289 = 0.03366604447364807 + 10.0 * 9.026623725891113
Epoch 2160, val loss: 0.7757998704910278
Epoch 2170, training loss: 90.28926086425781 = 0.03307938948273659 + 10.0 * 9.025617599487305
Epoch 2170, val loss: 0.779499888420105
Epoch 2180, training loss: 90.29105377197266 = 0.03250811994075775 + 10.0 * 9.025854110717773
Epoch 2180, val loss: 0.7828014492988586
Epoch 2190, training loss: 90.3047866821289 = 0.031954530626535416 + 10.0 * 9.027283668518066
Epoch 2190, val loss: 0.7865716218948364
Epoch 2200, training loss: 90.29690551757812 = 0.03140702471137047 + 10.0 * 9.026549339294434
Epoch 2200, val loss: 0.7900121212005615
Epoch 2210, training loss: 90.28488159179688 = 0.030862055718898773 + 10.0 * 9.025402069091797
Epoch 2210, val loss: 0.7933061718940735
Epoch 2220, training loss: 90.28045654296875 = 0.03033248893916607 + 10.0 * 9.025012016296387
Epoch 2220, val loss: 0.7971780896186829
Epoch 2230, training loss: 90.29074096679688 = 0.029821498319506645 + 10.0 * 9.026091575622559
Epoch 2230, val loss: 0.8002951741218567
Epoch 2240, training loss: 90.28353118896484 = 0.02931344136595726 + 10.0 * 9.025422096252441
Epoch 2240, val loss: 0.8036733865737915
Epoch 2250, training loss: 90.2784194946289 = 0.028819160535931587 + 10.0 * 9.0249605178833
Epoch 2250, val loss: 0.8068416118621826
Epoch 2260, training loss: 90.28315734863281 = 0.02833344228565693 + 10.0 * 9.025482177734375
Epoch 2260, val loss: 0.810513973236084
Epoch 2270, training loss: 90.2746353149414 = 0.027856048196554184 + 10.0 * 9.024678230285645
Epoch 2270, val loss: 0.8140479922294617
Epoch 2280, training loss: 90.27732849121094 = 0.02739156410098076 + 10.0 * 9.024993896484375
Epoch 2280, val loss: 0.8172183632850647
Epoch 2290, training loss: 90.27862548828125 = 0.026940250769257545 + 10.0 * 9.025168418884277
Epoch 2290, val loss: 0.8200626373291016
Epoch 2300, training loss: 90.28817749023438 = 0.02649272419512272 + 10.0 * 9.026168823242188
Epoch 2300, val loss: 0.8229733109474182
Epoch 2310, training loss: 90.27193450927734 = 0.02605399303138256 + 10.0 * 9.024587631225586
Epoch 2310, val loss: 0.8267963528633118
Epoch 2320, training loss: 90.26976013183594 = 0.02562173455953598 + 10.0 * 9.0244140625
Epoch 2320, val loss: 0.82964688539505
Epoch 2330, training loss: 90.26876068115234 = 0.025200016796588898 + 10.0 * 9.0243558883667
Epoch 2330, val loss: 0.832902193069458
Epoch 2340, training loss: 90.26708221435547 = 0.024788018316030502 + 10.0 * 9.024229049682617
Epoch 2340, val loss: 0.8361755609512329
Epoch 2350, training loss: 90.26678466796875 = 0.02438477985560894 + 10.0 * 9.024240493774414
Epoch 2350, val loss: 0.8398199081420898
Epoch 2360, training loss: 90.27876281738281 = 0.023990269750356674 + 10.0 * 9.025477409362793
Epoch 2360, val loss: 0.8427879810333252
Epoch 2370, training loss: 90.26143646240234 = 0.023597432300448418 + 10.0 * 9.023783683776855
Epoch 2370, val loss: 0.8452321290969849
Epoch 2380, training loss: 90.25865936279297 = 0.023213563486933708 + 10.0 * 9.023544311523438
Epoch 2380, val loss: 0.8488457202911377
Epoch 2390, training loss: 90.26939392089844 = 0.022843196988105774 + 10.0 * 9.02465534210205
Epoch 2390, val loss: 0.851250171661377
Epoch 2400, training loss: 90.25733947753906 = 0.022472534328699112 + 10.0 * 9.023486137390137
Epoch 2400, val loss: 0.8547994494438171
Epoch 2410, training loss: 90.26889038085938 = 0.022115439176559448 + 10.0 * 9.024677276611328
Epoch 2410, val loss: 0.8576376438140869
Epoch 2420, training loss: 90.25425720214844 = 0.021758686751127243 + 10.0 * 9.023249626159668
Epoch 2420, val loss: 0.8612020611763
Epoch 2430, training loss: 90.26070404052734 = 0.021415503695607185 + 10.0 * 9.02392864227295
Epoch 2430, val loss: 0.8638839721679688
Epoch 2440, training loss: 90.25386047363281 = 0.021075643599033356 + 10.0 * 9.023279190063477
Epoch 2440, val loss: 0.8665195107460022
Epoch 2450, training loss: 90.24847412109375 = 0.020739760249853134 + 10.0 * 9.022773742675781
Epoch 2450, val loss: 0.8694940209388733
Epoch 2460, training loss: 90.24459838867188 = 0.020414425060153008 + 10.0 * 9.022418022155762
Epoch 2460, val loss: 0.8724280595779419
Epoch 2470, training loss: 90.25357055664062 = 0.020099490880966187 + 10.0 * 9.023347854614258
Epoch 2470, val loss: 0.8747252821922302
Epoch 2480, training loss: 90.253662109375 = 0.019789330661296844 + 10.0 * 9.02338695526123
Epoch 2480, val loss: 0.877790093421936
Epoch 2490, training loss: 90.24083709716797 = 0.01948092132806778 + 10.0 * 9.022135734558105
Epoch 2490, val loss: 0.8809607625007629
Epoch 2500, training loss: 90.240478515625 = 0.019182227551937103 + 10.0 * 9.022130012512207
Epoch 2500, val loss: 0.8842421770095825
Epoch 2510, training loss: 90.26121520996094 = 0.018893608823418617 + 10.0 * 9.024232864379883
Epoch 2510, val loss: 0.8867247104644775
Epoch 2520, training loss: 90.26161193847656 = 0.018609844148159027 + 10.0 * 9.024300575256348
Epoch 2520, val loss: 0.8890514373779297
Epoch 2530, training loss: 90.2432861328125 = 0.018325813114643097 + 10.0 * 9.022496223449707
Epoch 2530, val loss: 0.8922931551933289
Epoch 2540, training loss: 90.23534393310547 = 0.018049145117402077 + 10.0 * 9.021729469299316
Epoch 2540, val loss: 0.8948231935501099
Epoch 2550, training loss: 90.23185729980469 = 0.01777825318276882 + 10.0 * 9.021408081054688
Epoch 2550, val loss: 0.8977879881858826
Epoch 2560, training loss: 90.23260498046875 = 0.01751565933227539 + 10.0 * 9.021509170532227
Epoch 2560, val loss: 0.9005387425422668
Epoch 2570, training loss: 90.27133178710938 = 0.01727152056992054 + 10.0 * 9.025405883789062
Epoch 2570, val loss: 0.9036999344825745
Epoch 2580, training loss: 90.24506378173828 = 0.017011290416121483 + 10.0 * 9.022805213928223
Epoch 2580, val loss: 0.9056305289268494
Epoch 2590, training loss: 90.2348861694336 = 0.01675746962428093 + 10.0 * 9.02181339263916
Epoch 2590, val loss: 0.9085338115692139
Epoch 2600, training loss: 90.24042510986328 = 0.01651463285088539 + 10.0 * 9.022390365600586
Epoch 2600, val loss: 0.9115988612174988
Epoch 2610, training loss: 90.23416137695312 = 0.01627531088888645 + 10.0 * 9.021788597106934
Epoch 2610, val loss: 0.9135965704917908
Epoch 2620, training loss: 90.22831726074219 = 0.016038645058870316 + 10.0 * 9.021227836608887
Epoch 2620, val loss: 0.9160187244415283
Epoch 2630, training loss: 90.22684478759766 = 0.01580864191055298 + 10.0 * 9.021103858947754
Epoch 2630, val loss: 0.91910719871521
Epoch 2640, training loss: 90.23650360107422 = 0.015586079098284245 + 10.0 * 9.02209186553955
Epoch 2640, val loss: 0.9211277961730957
Epoch 2650, training loss: 90.23617553710938 = 0.015366987325251102 + 10.0 * 9.02208137512207
Epoch 2650, val loss: 0.9237521290779114
Epoch 2660, training loss: 90.22480010986328 = 0.015148578211665154 + 10.0 * 9.020964622497559
Epoch 2660, val loss: 0.926344633102417
Epoch 2670, training loss: 90.22232055664062 = 0.014936074614524841 + 10.0 * 9.02073860168457
Epoch 2670, val loss: 0.929004430770874
Epoch 2680, training loss: 90.23025512695312 = 0.014731413684785366 + 10.0 * 9.021552085876465
Epoch 2680, val loss: 0.9316120147705078
Epoch 2690, training loss: 90.21688842773438 = 0.014524098485708237 + 10.0 * 9.020236015319824
Epoch 2690, val loss: 0.9339426755905151
Epoch 2700, training loss: 90.2191162109375 = 0.014325733296573162 + 10.0 * 9.020479202270508
Epoch 2700, val loss: 0.9367465376853943
Epoch 2710, training loss: 90.23448944091797 = 0.014133566990494728 + 10.0 * 9.022035598754883
Epoch 2710, val loss: 0.9392683506011963
Epoch 2720, training loss: 90.2330093383789 = 0.013942157849669456 + 10.0 * 9.021906852722168
Epoch 2720, val loss: 0.9411259293556213
Epoch 2730, training loss: 90.22112274169922 = 0.013748704455792904 + 10.0 * 9.020737648010254
Epoch 2730, val loss: 0.9440774321556091
Epoch 2740, training loss: 90.2110595703125 = 0.013563018292188644 + 10.0 * 9.019749641418457
Epoch 2740, val loss: 0.9464494585990906
Epoch 2750, training loss: 90.21021270751953 = 0.013381143100559711 + 10.0 * 9.019682884216309
Epoch 2750, val loss: 0.9493499398231506
Epoch 2760, training loss: 90.24456787109375 = 0.013211733661592007 + 10.0 * 9.0231351852417
Epoch 2760, val loss: 0.9524502754211426
Epoch 2770, training loss: 90.22119903564453 = 0.013037841767072678 + 10.0 * 9.0208158493042
Epoch 2770, val loss: 0.9530616402626038
Epoch 2780, training loss: 90.21279907226562 = 0.012860267423093319 + 10.0 * 9.019993782043457
Epoch 2780, val loss: 0.9566207528114319
Epoch 2790, training loss: 90.21650695800781 = 0.012694183737039566 + 10.0 * 9.020380973815918
Epoch 2790, val loss: 0.9589203000068665
Epoch 2800, training loss: 90.20890808105469 = 0.012529410421848297 + 10.0 * 9.019638061523438
Epoch 2800, val loss: 0.9610477685928345
Epoch 2810, training loss: 90.22047424316406 = 0.012371828779578209 + 10.0 * 9.0208101272583
Epoch 2810, val loss: 0.9633122682571411
Epoch 2820, training loss: 90.20585632324219 = 0.0122110890224576 + 10.0 * 9.019364356994629
Epoch 2820, val loss: 0.9652172923088074
Epoch 2830, training loss: 90.2076416015625 = 0.012060091830790043 + 10.0 * 9.01955795288086
Epoch 2830, val loss: 0.9673444628715515
Epoch 2840, training loss: 90.24034118652344 = 0.011914445087313652 + 10.0 * 9.022842407226562
Epoch 2840, val loss: 0.9698794484138489
Epoch 2850, training loss: 90.21102905273438 = 0.011762373149394989 + 10.0 * 9.019926071166992
Epoch 2850, val loss: 0.9718311429023743
Epoch 2860, training loss: 90.20040893554688 = 0.011614297516644001 + 10.0 * 9.018879890441895
Epoch 2860, val loss: 0.9741954803466797
Epoch 2870, training loss: 90.20030975341797 = 0.011472439393401146 + 10.0 * 9.01888370513916
Epoch 2870, val loss: 0.9765831828117371
Epoch 2880, training loss: 90.21994018554688 = 0.01133990939706564 + 10.0 * 9.02086067199707
Epoch 2880, val loss: 0.9789949059486389
Epoch 2890, training loss: 90.20494079589844 = 0.011198969557881355 + 10.0 * 9.019373893737793
Epoch 2890, val loss: 0.9799894094467163
Epoch 2900, training loss: 90.2015380859375 = 0.011063081212341785 + 10.0 * 9.019047737121582
Epoch 2900, val loss: 0.9828795194625854
Epoch 2910, training loss: 90.1998519897461 = 0.010932005010545254 + 10.0 * 9.018892288208008
Epoch 2910, val loss: 0.9843748211860657
Epoch 2920, training loss: 90.21814727783203 = 0.010808917693793774 + 10.0 * 9.020733833312988
Epoch 2920, val loss: 0.9867185950279236
Epoch 2930, training loss: 90.19721984863281 = 0.010678006336092949 + 10.0 * 9.018653869628906
Epoch 2930, val loss: 0.9881494045257568
Epoch 2940, training loss: 90.19071960449219 = 0.01055221352726221 + 10.0 * 9.018016815185547
Epoch 2940, val loss: 0.9901975989341736
Epoch 2950, training loss: 90.1901626586914 = 0.010430692695081234 + 10.0 * 9.017972946166992
Epoch 2950, val loss: 0.9923182129859924
Epoch 2960, training loss: 90.20539093017578 = 0.01031846459954977 + 10.0 * 9.01950740814209
Epoch 2960, val loss: 0.9943485856056213
Epoch 2970, training loss: 90.18944549560547 = 0.010195833630859852 + 10.0 * 9.017925262451172
Epoch 2970, val loss: 0.9964218139648438
Epoch 2980, training loss: 90.2000961303711 = 0.010085142217576504 + 10.0 * 9.019001007080078
Epoch 2980, val loss: 0.9981591701507568
Epoch 2990, training loss: 90.19184112548828 = 0.00997055321931839 + 10.0 * 9.018186569213867
Epoch 2990, val loss: 0.9999351501464844
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8519
Overall ASR: 0.7069
Flip ASR: 0.6351/1554 nodes
./selected_nodes/Pubmed/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 104.69049835205078 = 1.100037693977356 + 10.0 * 10.35904598236084
Epoch 0, val loss: 1.0983315706253052
Epoch 10, training loss: 104.64327239990234 = 1.0881965160369873 + 10.0 * 10.355507850646973
Epoch 10, val loss: 1.0864207744598389
Epoch 20, training loss: 104.05975341796875 = 1.0723135471343994 + 10.0 * 10.298744201660156
Epoch 20, val loss: 1.0706608295440674
Epoch 30, training loss: 100.31753540039062 = 1.0561070442199707 + 10.0 * 9.926142692565918
Epoch 30, val loss: 1.0547094345092773
Epoch 40, training loss: 96.94535064697266 = 1.037905216217041 + 10.0 * 9.590744972229004
Epoch 40, val loss: 1.0366634130477905
Epoch 50, training loss: 95.16346740722656 = 1.01809823513031 + 10.0 * 9.41453742980957
Epoch 50, val loss: 1.0168306827545166
Epoch 60, training loss: 94.51758575439453 = 0.9990617632865906 + 10.0 * 9.351852416992188
Epoch 60, val loss: 0.998268187046051
Epoch 70, training loss: 93.99478149414062 = 0.9814161062240601 + 10.0 * 9.301336288452148
Epoch 70, val loss: 0.9809093475341797
Epoch 80, training loss: 93.72767639160156 = 0.9626716375350952 + 10.0 * 9.276500701904297
Epoch 80, val loss: 0.9619075655937195
Epoch 90, training loss: 93.47727966308594 = 0.9391036033630371 + 10.0 * 9.253817558288574
Epoch 90, val loss: 0.9384163618087769
Epoch 100, training loss: 93.1748046875 = 0.9136818051338196 + 10.0 * 9.226112365722656
Epoch 100, val loss: 0.9136376976966858
Epoch 110, training loss: 92.85713958740234 = 0.888182520866394 + 10.0 * 9.196895599365234
Epoch 110, val loss: 0.8889384865760803
Epoch 120, training loss: 92.64151763916016 = 0.859606921672821 + 10.0 * 9.178191184997559
Epoch 120, val loss: 0.8611758947372437
Epoch 130, training loss: 92.42466735839844 = 0.8258940577507019 + 10.0 * 9.159876823425293
Epoch 130, val loss: 0.8284991979598999
Epoch 140, training loss: 92.26712036132812 = 0.7886554598808289 + 10.0 * 9.147846221923828
Epoch 140, val loss: 0.7928001284599304
Epoch 150, training loss: 92.13922119140625 = 0.7488985061645508 + 10.0 * 9.139032363891602
Epoch 150, val loss: 0.7548134922981262
Epoch 160, training loss: 92.00946807861328 = 0.7077226042747498 + 10.0 * 9.13017463684082
Epoch 160, val loss: 0.7162721157073975
Epoch 170, training loss: 91.89767456054688 = 0.6674239635467529 + 10.0 * 9.123024940490723
Epoch 170, val loss: 0.6788697242736816
Epoch 180, training loss: 91.79458618164062 = 0.6295655369758606 + 10.0 * 9.11650276184082
Epoch 180, val loss: 0.6443018317222595
Epoch 190, training loss: 91.74012756347656 = 0.5948440432548523 + 10.0 * 9.11452865600586
Epoch 190, val loss: 0.6134436130523682
Epoch 200, training loss: 91.64311981201172 = 0.5642409324645996 + 10.0 * 9.107888221740723
Epoch 200, val loss: 0.5864611864089966
Epoch 210, training loss: 91.58769989013672 = 0.5374293923377991 + 10.0 * 9.105027198791504
Epoch 210, val loss: 0.5636294484138489
Epoch 220, training loss: 91.54859161376953 = 0.514388918876648 + 10.0 * 9.10342025756836
Epoch 220, val loss: 0.5446504354476929
Epoch 230, training loss: 91.49369049072266 = 0.4949291944503784 + 10.0 * 9.099876403808594
Epoch 230, val loss: 0.5290253758430481
Epoch 240, training loss: 91.4547348022461 = 0.47841957211494446 + 10.0 * 9.097631454467773
Epoch 240, val loss: 0.5163018107414246
Epoch 250, training loss: 91.418701171875 = 0.46422016620635986 + 10.0 * 9.09544849395752
Epoch 250, val loss: 0.5057026743888855
Epoch 260, training loss: 91.40750122070312 = 0.4518042504787445 + 10.0 * 9.095569610595703
Epoch 260, val loss: 0.496888130903244
Epoch 270, training loss: 91.35870361328125 = 0.44090035557746887 + 10.0 * 9.091779708862305
Epoch 270, val loss: 0.4892348647117615
Epoch 280, training loss: 91.33055877685547 = 0.4311758875846863 + 10.0 * 9.089938163757324
Epoch 280, val loss: 0.48262056708335876
Epoch 290, training loss: 91.30772399902344 = 0.4223436117172241 + 10.0 * 9.08853816986084
Epoch 290, val loss: 0.4768887758255005
Epoch 300, training loss: 91.28477478027344 = 0.4142451286315918 + 10.0 * 9.087053298950195
Epoch 300, val loss: 0.4717041254043579
Epoch 310, training loss: 91.255615234375 = 0.40676629543304443 + 10.0 * 9.084884643554688
Epoch 310, val loss: 0.4669668674468994
Epoch 320, training loss: 91.2547836303711 = 0.39975985884666443 + 10.0 * 9.085502624511719
Epoch 320, val loss: 0.4626587927341461
Epoch 330, training loss: 91.2342300415039 = 0.39321017265319824 + 10.0 * 9.084101676940918
Epoch 330, val loss: 0.4586770534515381
Epoch 340, training loss: 91.1949462890625 = 0.3871456980705261 + 10.0 * 9.080780029296875
Epoch 340, val loss: 0.45498257875442505
Epoch 350, training loss: 91.17706298828125 = 0.38143110275268555 + 10.0 * 9.07956314086914
Epoch 350, val loss: 0.4515162408351898
Epoch 360, training loss: 91.17195892333984 = 0.3759678602218628 + 10.0 * 9.079599380493164
Epoch 360, val loss: 0.4482360780239105
Epoch 370, training loss: 91.15939331054688 = 0.37069419026374817 + 10.0 * 9.078869819641113
Epoch 370, val loss: 0.4451378881931305
Epoch 380, training loss: 91.12873077392578 = 0.36570921540260315 + 10.0 * 9.076302528381348
Epoch 380, val loss: 0.442152202129364
Epoch 390, training loss: 91.1175765991211 = 0.36095666885375977 + 10.0 * 9.075662612915039
Epoch 390, val loss: 0.4392871558666229
Epoch 400, training loss: 91.09918975830078 = 0.3564023971557617 + 10.0 * 9.074278831481934
Epoch 400, val loss: 0.436587929725647
Epoch 410, training loss: 91.08470153808594 = 0.3520345091819763 + 10.0 * 9.073266983032227
Epoch 410, val loss: 0.43399783968925476
Epoch 420, training loss: 91.09446716308594 = 0.34782150387763977 + 10.0 * 9.074664115905762
Epoch 420, val loss: 0.43147653341293335
Epoch 430, training loss: 91.06300354003906 = 0.34374263882637024 + 10.0 * 9.07192611694336
Epoch 430, val loss: 0.4292016923427582
Epoch 440, training loss: 91.07901000976562 = 0.33982667326927185 + 10.0 * 9.073918342590332
Epoch 440, val loss: 0.42688044905662537
Epoch 450, training loss: 91.03438568115234 = 0.3360362946987152 + 10.0 * 9.06983470916748
Epoch 450, val loss: 0.42474332451820374
Epoch 460, training loss: 91.0214614868164 = 0.3324030339717865 + 10.0 * 9.0689058303833
Epoch 460, val loss: 0.4226209223270416
Epoch 470, training loss: 91.00847625732422 = 0.32887744903564453 + 10.0 * 9.067959785461426
Epoch 470, val loss: 0.4207039177417755
Epoch 480, training loss: 91.00241088867188 = 0.32542234659194946 + 10.0 * 9.06769847869873
Epoch 480, val loss: 0.41880249977111816
Epoch 490, training loss: 90.99014282226562 = 0.3220977485179901 + 10.0 * 9.066804885864258
Epoch 490, val loss: 0.41705793142318726
Epoch 500, training loss: 90.9740982055664 = 0.3189253807067871 + 10.0 * 9.06551742553711
Epoch 500, val loss: 0.41536861658096313
Epoch 510, training loss: 90.98363494873047 = 0.3158435821533203 + 10.0 * 9.066779136657715
Epoch 510, val loss: 0.41382652521133423
Epoch 520, training loss: 90.97215270996094 = 0.31283438205718994 + 10.0 * 9.06593132019043
Epoch 520, val loss: 0.412361741065979
Epoch 530, training loss: 90.94522857666016 = 0.3099342882633209 + 10.0 * 9.063529014587402
Epoch 530, val loss: 0.41101184487342834
Epoch 540, training loss: 90.93791198730469 = 0.3071194887161255 + 10.0 * 9.063078880310059
Epoch 540, val loss: 0.40980634093284607
Epoch 550, training loss: 90.99281311035156 = 0.3043723702430725 + 10.0 * 9.068843841552734
Epoch 550, val loss: 0.40869346261024475
Epoch 560, training loss: 90.91979217529297 = 0.301690012216568 + 10.0 * 9.061810493469238
Epoch 560, val loss: 0.4077088534832001
Epoch 570, training loss: 90.90927124023438 = 0.29911360144615173 + 10.0 * 9.061016082763672
Epoch 570, val loss: 0.4067467749118805
Epoch 580, training loss: 90.90278625488281 = 0.29659655690193176 + 10.0 * 9.06061840057373
Epoch 580, val loss: 0.40591782331466675
Epoch 590, training loss: 90.94490814208984 = 0.2941311299800873 + 10.0 * 9.065077781677246
Epoch 590, val loss: 0.4050818085670471
Epoch 600, training loss: 90.89366912841797 = 0.2917087972164154 + 10.0 * 9.060195922851562
Epoch 600, val loss: 0.40458691120147705
Epoch 610, training loss: 90.8819351196289 = 0.2893780469894409 + 10.0 * 9.059255599975586
Epoch 610, val loss: 0.4040246903896332
Epoch 620, training loss: 90.87210845947266 = 0.28709498047828674 + 10.0 * 9.058501243591309
Epoch 620, val loss: 0.4035315215587616
Epoch 630, training loss: 90.86481475830078 = 0.2848447859287262 + 10.0 * 9.05799674987793
Epoch 630, val loss: 0.4031435251235962
Epoch 640, training loss: 90.91604614257812 = 0.28262555599212646 + 10.0 * 9.063342094421387
Epoch 640, val loss: 0.40281957387924194
Epoch 650, training loss: 90.85791015625 = 0.28042805194854736 + 10.0 * 9.057748794555664
Epoch 650, val loss: 0.40259870886802673
Epoch 660, training loss: 90.84937286376953 = 0.27829399704933167 + 10.0 * 9.057107925415039
Epoch 660, val loss: 0.40234366059303284
Epoch 670, training loss: 90.84229278564453 = 0.27618923783302307 + 10.0 * 9.056610107421875
Epoch 670, val loss: 0.40219515562057495
Epoch 680, training loss: 90.87114715576172 = 0.2741038203239441 + 10.0 * 9.059704780578613
Epoch 680, val loss: 0.4021315276622772
Epoch 690, training loss: 90.83174896240234 = 0.27203941345214844 + 10.0 * 9.055971145629883
Epoch 690, val loss: 0.4020706117153168
Epoch 700, training loss: 90.82498168945312 = 0.2700138986110687 + 10.0 * 9.055497169494629
Epoch 700, val loss: 0.4020087420940399
Epoch 710, training loss: 90.81587219238281 = 0.26801174879074097 + 10.0 * 9.05478572845459
Epoch 710, val loss: 0.40206244587898254
Epoch 720, training loss: 90.81915283203125 = 0.26602357625961304 + 10.0 * 9.055313110351562
Epoch 720, val loss: 0.40216064453125
Epoch 730, training loss: 90.80724334716797 = 0.2640487849712372 + 10.0 * 9.054319381713867
Epoch 730, val loss: 0.4022972881793976
Epoch 740, training loss: 90.81266784667969 = 0.26209813356399536 + 10.0 * 9.05505657196045
Epoch 740, val loss: 0.40249329805374146
Epoch 750, training loss: 90.80540466308594 = 0.26016539335250854 + 10.0 * 9.054524421691895
Epoch 750, val loss: 0.4027264416217804
Epoch 760, training loss: 90.79664611816406 = 0.2582574188709259 + 10.0 * 9.053838729858398
Epoch 760, val loss: 0.4030489921569824
Epoch 770, training loss: 90.7823715209961 = 0.25636279582977295 + 10.0 * 9.052600860595703
Epoch 770, val loss: 0.4033651649951935
Epoch 780, training loss: 90.77876281738281 = 0.25448137521743774 + 10.0 * 9.052428245544434
Epoch 780, val loss: 0.40368494391441345
Epoch 790, training loss: 90.79071807861328 = 0.2526060938835144 + 10.0 * 9.053811073303223
Epoch 790, val loss: 0.40409350395202637
Epoch 800, training loss: 90.77766418457031 = 0.2507443130016327 + 10.0 * 9.052691459655762
Epoch 800, val loss: 0.4045774042606354
Epoch 810, training loss: 90.7920913696289 = 0.24890674650669098 + 10.0 * 9.05431842803955
Epoch 810, val loss: 0.40507248044013977
Epoch 820, training loss: 90.7611083984375 = 0.24708350002765656 + 10.0 * 9.05140209197998
Epoch 820, val loss: 0.40546852350234985
Epoch 830, training loss: 90.75141143798828 = 0.2452726811170578 + 10.0 * 9.050614356994629
Epoch 830, val loss: 0.405984103679657
Epoch 840, training loss: 90.74735260009766 = 0.243469700217247 + 10.0 * 9.05038833618164
Epoch 840, val loss: 0.4065617620944977
Epoch 850, training loss: 90.7779541015625 = 0.24167078733444214 + 10.0 * 9.053628921508789
Epoch 850, val loss: 0.40713900327682495
Epoch 860, training loss: 90.7574462890625 = 0.2398776262998581 + 10.0 * 9.051756858825684
Epoch 860, val loss: 0.40767183899879456
Epoch 870, training loss: 90.73287200927734 = 0.23810748755931854 + 10.0 * 9.049476623535156
Epoch 870, val loss: 0.40839704871177673
Epoch 880, training loss: 90.72740173339844 = 0.23634187877178192 + 10.0 * 9.049105644226074
Epoch 880, val loss: 0.40907585620880127
Epoch 890, training loss: 90.74486541748047 = 0.23458318412303925 + 10.0 * 9.05102825164795
Epoch 890, val loss: 0.409745454788208
Epoch 900, training loss: 90.75857543945312 = 0.2328277826309204 + 10.0 * 9.05257511138916
Epoch 900, val loss: 0.4105197787284851
Epoch 910, training loss: 90.71843719482422 = 0.23108437657356262 + 10.0 * 9.048734664916992
Epoch 910, val loss: 0.41133812069892883
Epoch 920, training loss: 90.7128677368164 = 0.2293584644794464 + 10.0 * 9.048351287841797
Epoch 920, val loss: 0.4120900630950928
Epoch 930, training loss: 90.70449829101562 = 0.22762265801429749 + 10.0 * 9.047687530517578
Epoch 930, val loss: 0.4128909111022949
Epoch 940, training loss: 90.73103332519531 = 0.22588780522346497 + 10.0 * 9.050514221191406
Epoch 940, val loss: 0.4136146903038025
Epoch 950, training loss: 90.71031188964844 = 0.22415611147880554 + 10.0 * 9.048615455627441
Epoch 950, val loss: 0.41475167870521545
Epoch 960, training loss: 90.69529724121094 = 0.22242553532123566 + 10.0 * 9.047286987304688
Epoch 960, val loss: 0.41551491618156433
Epoch 970, training loss: 90.69245910644531 = 0.2206983119249344 + 10.0 * 9.047176361083984
Epoch 970, val loss: 0.4164659380912781
Epoch 980, training loss: 90.69955444335938 = 0.21896743774414062 + 10.0 * 9.048059463500977
Epoch 980, val loss: 0.4173409044742584
Epoch 990, training loss: 90.68209075927734 = 0.21724393963813782 + 10.0 * 9.046483993530273
Epoch 990, val loss: 0.41839608550071716
Epoch 1000, training loss: 90.6778564453125 = 0.21552248299121857 + 10.0 * 9.046233177185059
Epoch 1000, val loss: 0.4193614423274994
Epoch 1010, training loss: 90.67005157470703 = 0.21379508078098297 + 10.0 * 9.045625686645508
Epoch 1010, val loss: 0.42042815685272217
Epoch 1020, training loss: 90.68397521972656 = 0.2120666652917862 + 10.0 * 9.04719066619873
Epoch 1020, val loss: 0.4216383397579193
Epoch 1030, training loss: 90.67911529541016 = 0.21033553779125214 + 10.0 * 9.04687786102295
Epoch 1030, val loss: 0.42268964648246765
Epoch 1040, training loss: 90.67019653320312 = 0.2086058259010315 + 10.0 * 9.046159744262695
Epoch 1040, val loss: 0.42381852865219116
Epoch 1050, training loss: 90.65591430664062 = 0.20689095556735992 + 10.0 * 9.044901847839355
Epoch 1050, val loss: 0.4250621497631073
Epoch 1060, training loss: 90.6556396484375 = 0.20517264306545258 + 10.0 * 9.04504680633545
Epoch 1060, val loss: 0.4262135326862335
Epoch 1070, training loss: 90.65725708007812 = 0.20345328748226166 + 10.0 * 9.045380592346191
Epoch 1070, val loss: 0.42743945121765137
Epoch 1080, training loss: 90.65020751953125 = 0.20173658430576324 + 10.0 * 9.04484748840332
Epoch 1080, val loss: 0.42862486839294434
Epoch 1090, training loss: 90.6479263305664 = 0.20001833140850067 + 10.0 * 9.044790267944336
Epoch 1090, val loss: 0.42994174361228943
Epoch 1100, training loss: 90.65743255615234 = 0.1983024626970291 + 10.0 * 9.045912742614746
Epoch 1100, val loss: 0.43129220604896545
Epoch 1110, training loss: 90.63455963134766 = 0.19658531248569489 + 10.0 * 9.043797492980957
Epoch 1110, val loss: 0.4326664209365845
Epoch 1120, training loss: 90.62654876708984 = 0.19486604630947113 + 10.0 * 9.043169021606445
Epoch 1120, val loss: 0.43406644463539124
Epoch 1130, training loss: 90.6239013671875 = 0.19314533472061157 + 10.0 * 9.043075561523438
Epoch 1130, val loss: 0.43550363183021545
Epoch 1140, training loss: 90.65703582763672 = 0.19142994284629822 + 10.0 * 9.046560287475586
Epoch 1140, val loss: 0.4369796812534332
Epoch 1150, training loss: 90.62753295898438 = 0.1897129863500595 + 10.0 * 9.043782234191895
Epoch 1150, val loss: 0.4384292662143707
Epoch 1160, training loss: 90.62398529052734 = 0.1880025863647461 + 10.0 * 9.043598175048828
Epoch 1160, val loss: 0.4400590658187866
Epoch 1170, training loss: 90.61693572998047 = 0.1862873136997223 + 10.0 * 9.043065071105957
Epoch 1170, val loss: 0.4416106045246124
Epoch 1180, training loss: 90.60453033447266 = 0.18458017706871033 + 10.0 * 9.04199504852295
Epoch 1180, val loss: 0.4431159794330597
Epoch 1190, training loss: 90.59996032714844 = 0.18287041783332825 + 10.0 * 9.041708946228027
Epoch 1190, val loss: 0.44474664330482483
Epoch 1200, training loss: 90.59832763671875 = 0.181159108877182 + 10.0 * 9.041716575622559
Epoch 1200, val loss: 0.44648200273513794
Epoch 1210, training loss: 90.6452407836914 = 0.17945575714111328 + 10.0 * 9.046578407287598
Epoch 1210, val loss: 0.44806191325187683
Epoch 1220, training loss: 90.59102630615234 = 0.17774435877799988 + 10.0 * 9.041328430175781
Epoch 1220, val loss: 0.4500631093978882
Epoch 1230, training loss: 90.58829498291016 = 0.17604319751262665 + 10.0 * 9.04122543334961
Epoch 1230, val loss: 0.4519060254096985
Epoch 1240, training loss: 90.57939147949219 = 0.1743392050266266 + 10.0 * 9.040505409240723
Epoch 1240, val loss: 0.45375531911849976
Epoch 1250, training loss: 90.57847595214844 = 0.17263434827327728 + 10.0 * 9.040583610534668
Epoch 1250, val loss: 0.45576587319374084
Epoch 1260, training loss: 90.63328552246094 = 0.1709413081407547 + 10.0 * 9.046234130859375
Epoch 1260, val loss: 0.45782995223999023
Epoch 1270, training loss: 90.56977844238281 = 0.16923661530017853 + 10.0 * 9.040054321289062
Epoch 1270, val loss: 0.45981714129447937
Epoch 1280, training loss: 90.57140350341797 = 0.1675407439470291 + 10.0 * 9.040386199951172
Epoch 1280, val loss: 0.46178317070007324
Epoch 1290, training loss: 90.56128692626953 = 0.16584743559360504 + 10.0 * 9.039544105529785
Epoch 1290, val loss: 0.46402400732040405
Epoch 1300, training loss: 90.56446075439453 = 0.16415347158908844 + 10.0 * 9.040030479431152
Epoch 1300, val loss: 0.46639448404312134
Epoch 1310, training loss: 90.57647705078125 = 0.1624607890844345 + 10.0 * 9.041401863098145
Epoch 1310, val loss: 0.4686690866947174
Epoch 1320, training loss: 90.56160736083984 = 0.16076874732971191 + 10.0 * 9.040083885192871
Epoch 1320, val loss: 0.471017986536026
Epoch 1330, training loss: 90.57682800292969 = 0.15909124910831451 + 10.0 * 9.041773796081543
Epoch 1330, val loss: 0.4735606014728546
Epoch 1340, training loss: 90.55524444580078 = 0.15740549564361572 + 10.0 * 9.03978443145752
Epoch 1340, val loss: 0.47581565380096436
Epoch 1350, training loss: 90.5486068725586 = 0.15573103725910187 + 10.0 * 9.039287567138672
Epoch 1350, val loss: 0.47843003273010254
Epoch 1360, training loss: 90.54554748535156 = 0.15405617654323578 + 10.0 * 9.039149284362793
Epoch 1360, val loss: 0.4809021055698395
Epoch 1370, training loss: 90.53709411621094 = 0.15238314867019653 + 10.0 * 9.038471221923828
Epoch 1370, val loss: 0.4836036264896393
Epoch 1380, training loss: 90.54296112060547 = 0.1507173776626587 + 10.0 * 9.039224624633789
Epoch 1380, val loss: 0.4862730801105499
Epoch 1390, training loss: 90.53719329833984 = 0.14905163645744324 + 10.0 * 9.038814544677734
Epoch 1390, val loss: 0.4890455901622772
Epoch 1400, training loss: 90.5366439819336 = 0.1473926603794098 + 10.0 * 9.038925170898438
Epoch 1400, val loss: 0.4917750358581543
Epoch 1410, training loss: 90.5278091430664 = 0.14573664963245392 + 10.0 * 9.038207054138184
Epoch 1410, val loss: 0.49447861313819885
Epoch 1420, training loss: 90.53144836425781 = 0.14408880472183228 + 10.0 * 9.038736343383789
Epoch 1420, val loss: 0.4973151385784149
Epoch 1430, training loss: 90.53082275390625 = 0.14244963228702545 + 10.0 * 9.038837432861328
Epoch 1430, val loss: 0.5004317760467529
Epoch 1440, training loss: 90.51934814453125 = 0.14081653952598572 + 10.0 * 9.037853240966797
Epoch 1440, val loss: 0.5032233595848083
Epoch 1450, training loss: 90.52430725097656 = 0.13918861746788025 + 10.0 * 9.038511276245117
Epoch 1450, val loss: 0.5060157179832458
Epoch 1460, training loss: 90.51316833496094 = 0.1375627964735031 + 10.0 * 9.03756046295166
Epoch 1460, val loss: 0.5092529058456421
Epoch 1470, training loss: 90.50997924804688 = 0.13594362139701843 + 10.0 * 9.03740406036377
Epoch 1470, val loss: 0.5122489333152771
Epoch 1480, training loss: 90.50981903076172 = 0.13432878255844116 + 10.0 * 9.037549018859863
Epoch 1480, val loss: 0.5153518319129944
Epoch 1490, training loss: 90.49960327148438 = 0.1327103227376938 + 10.0 * 9.036688804626465
Epoch 1490, val loss: 0.5183417797088623
Epoch 1500, training loss: 90.52122497558594 = 0.13110342621803284 + 10.0 * 9.03901195526123
Epoch 1500, val loss: 0.5214098691940308
Epoch 1510, training loss: 90.49952697753906 = 0.12949322164058685 + 10.0 * 9.037003517150879
Epoch 1510, val loss: 0.5248033404350281
Epoch 1520, training loss: 90.4929428100586 = 0.127887561917305 + 10.0 * 9.036505699157715
Epoch 1520, val loss: 0.5277190804481506
Epoch 1530, training loss: 90.48990631103516 = 0.1262935847043991 + 10.0 * 9.036360740661621
Epoch 1530, val loss: 0.5311003923416138
Epoch 1540, training loss: 90.50184631347656 = 0.12471500039100647 + 10.0 * 9.037713050842285
Epoch 1540, val loss: 0.5342748165130615
Epoch 1550, training loss: 90.492919921875 = 0.12314170598983765 + 10.0 * 9.036977767944336
Epoch 1550, val loss: 0.537581741809845
Epoch 1560, training loss: 90.47848510742188 = 0.12156996130943298 + 10.0 * 9.03569221496582
Epoch 1560, val loss: 0.5409079790115356
Epoch 1570, training loss: 90.47370910644531 = 0.12000316381454468 + 10.0 * 9.035370826721191
Epoch 1570, val loss: 0.544426679611206
Epoch 1580, training loss: 90.47492218017578 = 0.11844774335622787 + 10.0 * 9.03564739227295
Epoch 1580, val loss: 0.5479974746704102
Epoch 1590, training loss: 90.5073013305664 = 0.11691422760486603 + 10.0 * 9.03903865814209
Epoch 1590, val loss: 0.5516208410263062
Epoch 1600, training loss: 90.47531127929688 = 0.1153624877333641 + 10.0 * 9.035994529724121
Epoch 1600, val loss: 0.554712176322937
Epoch 1610, training loss: 90.4666519165039 = 0.11382818967103958 + 10.0 * 9.035282135009766
Epoch 1610, val loss: 0.5581241250038147
Epoch 1620, training loss: 90.46087646484375 = 0.11230602115392685 + 10.0 * 9.034856796264648
Epoch 1620, val loss: 0.5617485046386719
Epoch 1630, training loss: 90.46937561035156 = 0.11079823970794678 + 10.0 * 9.035857200622559
Epoch 1630, val loss: 0.5653831958770752
Epoch 1640, training loss: 90.46173858642578 = 0.10929528623819351 + 10.0 * 9.03524398803711
Epoch 1640, val loss: 0.5687313079833984
Epoch 1650, training loss: 90.45149993896484 = 0.10779678821563721 + 10.0 * 9.034370422363281
Epoch 1650, val loss: 0.5724390149116516
Epoch 1660, training loss: 90.44822692871094 = 0.1063053160905838 + 10.0 * 9.034192085266113
Epoch 1660, val loss: 0.5759419202804565
Epoch 1670, training loss: 90.46253967285156 = 0.10482911020517349 + 10.0 * 9.035771369934082
Epoch 1670, val loss: 0.5795204043388367
Epoch 1680, training loss: 90.45259094238281 = 0.10335918515920639 + 10.0 * 9.034923553466797
Epoch 1680, val loss: 0.583241879940033
Epoch 1690, training loss: 90.44127655029297 = 0.10189084708690643 + 10.0 * 9.03393840789795
Epoch 1690, val loss: 0.5868862271308899
Epoch 1700, training loss: 90.43965148925781 = 0.10043411701917648 + 10.0 * 9.03392219543457
Epoch 1700, val loss: 0.5905047655105591
Epoch 1710, training loss: 90.46021270751953 = 0.09899822622537613 + 10.0 * 9.036121368408203
Epoch 1710, val loss: 0.5941828489303589
Epoch 1720, training loss: 90.44004821777344 = 0.09756037592887878 + 10.0 * 9.034249305725098
Epoch 1720, val loss: 0.5980867743492126
Epoch 1730, training loss: 90.43560028076172 = 0.09614057093858719 + 10.0 * 9.03394603729248
Epoch 1730, val loss: 0.6017283797264099
Epoch 1740, training loss: 90.42988586425781 = 0.09473138302564621 + 10.0 * 9.033514976501465
Epoch 1740, val loss: 0.605644941329956
Epoch 1750, training loss: 90.43551635742188 = 0.09333671629428864 + 10.0 * 9.034217834472656
Epoch 1750, val loss: 0.6094409823417664
Epoch 1760, training loss: 90.42911529541016 = 0.0919448584318161 + 10.0 * 9.033717155456543
Epoch 1760, val loss: 0.6133021116256714
Epoch 1770, training loss: 90.42424774169922 = 0.0905662477016449 + 10.0 * 9.033368110656738
Epoch 1770, val loss: 0.6170886754989624
Epoch 1780, training loss: 90.42720031738281 = 0.08919960260391235 + 10.0 * 9.03380012512207
Epoch 1780, val loss: 0.6208394765853882
Epoch 1790, training loss: 90.42560577392578 = 0.08784352988004684 + 10.0 * 9.03377628326416
Epoch 1790, val loss: 0.6246943473815918
Epoch 1800, training loss: 90.41722106933594 = 0.08649618178606033 + 10.0 * 9.033072471618652
Epoch 1800, val loss: 0.6284540891647339
Epoch 1810, training loss: 90.40929412841797 = 0.08516216278076172 + 10.0 * 9.032413482666016
Epoch 1810, val loss: 0.6324424147605896
Epoch 1820, training loss: 90.43878173828125 = 0.08385471999645233 + 10.0 * 9.035492897033691
Epoch 1820, val loss: 0.6366080641746521
Epoch 1830, training loss: 90.41301727294922 = 0.08254346996545792 + 10.0 * 9.033047676086426
Epoch 1830, val loss: 0.6404176950454712
Epoch 1840, training loss: 90.40524291992188 = 0.08125074952840805 + 10.0 * 9.03239917755127
Epoch 1840, val loss: 0.644555389881134
Epoch 1850, training loss: 90.39804077148438 = 0.07996593415737152 + 10.0 * 9.031807899475098
Epoch 1850, val loss: 0.64858478307724
Epoch 1860, training loss: 90.39527893066406 = 0.07869654893875122 + 10.0 * 9.031658172607422
Epoch 1860, val loss: 0.6525386571884155
Epoch 1870, training loss: 90.41939544677734 = 0.07745690643787384 + 10.0 * 9.034193992614746
Epoch 1870, val loss: 0.6562904715538025
Epoch 1880, training loss: 90.39907836914062 = 0.07621199637651443 + 10.0 * 9.032286643981934
Epoch 1880, val loss: 0.661177396774292
Epoch 1890, training loss: 90.41048431396484 = 0.07498960942029953 + 10.0 * 9.033549308776855
Epoch 1890, val loss: 0.6649332642555237
Epoch 1900, training loss: 90.40170288085938 = 0.07377547025680542 + 10.0 * 9.032793045043945
Epoch 1900, val loss: 0.669106125831604
Epoch 1910, training loss: 90.3852767944336 = 0.07257530093193054 + 10.0 * 9.031270027160645
Epoch 1910, val loss: 0.6731588840484619
Epoch 1920, training loss: 90.38114166259766 = 0.07139278948307037 + 10.0 * 9.030974388122559
Epoch 1920, val loss: 0.6771884560585022
Epoch 1930, training loss: 90.38087463378906 = 0.07022443413734436 + 10.0 * 9.031064987182617
Epoch 1930, val loss: 0.6814274787902832
Epoch 1940, training loss: 90.42581176757812 = 0.06908919662237167 + 10.0 * 9.035672187805176
Epoch 1940, val loss: 0.6853860020637512
Epoch 1950, training loss: 90.392333984375 = 0.06794697791337967 + 10.0 * 9.032438278198242
Epoch 1950, val loss: 0.689632773399353
Epoch 1960, training loss: 90.37683868408203 = 0.06681313365697861 + 10.0 * 9.03100299835205
Epoch 1960, val loss: 0.6939482092857361
Epoch 1970, training loss: 90.38124084472656 = 0.06569986790418625 + 10.0 * 9.031554222106934
Epoch 1970, val loss: 0.6980463266372681
Epoch 1980, training loss: 90.37457275390625 = 0.0645967572927475 + 10.0 * 9.030997276306152
Epoch 1980, val loss: 0.7023112177848816
Epoch 1990, training loss: 90.36947631835938 = 0.06350891292095184 + 10.0 * 9.030596733093262
Epoch 1990, val loss: 0.7066165208816528
Epoch 2000, training loss: 90.3653793334961 = 0.06243046745657921 + 10.0 * 9.030294418334961
Epoch 2000, val loss: 0.7107617855072021
Epoch 2010, training loss: 90.38077545166016 = 0.061377447098493576 + 10.0 * 9.031939506530762
Epoch 2010, val loss: 0.7151968479156494
Epoch 2020, training loss: 90.36090850830078 = 0.060328301042318344 + 10.0 * 9.030057907104492
Epoch 2020, val loss: 0.7194750308990479
Epoch 2030, training loss: 90.372314453125 = 0.05931169167160988 + 10.0 * 9.03130054473877
Epoch 2030, val loss: 0.7240085601806641
Epoch 2040, training loss: 90.35821533203125 = 0.05828646570444107 + 10.0 * 9.029993057250977
Epoch 2040, val loss: 0.7277438044548035
Epoch 2050, training loss: 90.37078094482422 = 0.05729382485151291 + 10.0 * 9.03134822845459
Epoch 2050, val loss: 0.732106626033783
Epoch 2060, training loss: 90.35897827148438 = 0.056307997554540634 + 10.0 * 9.030267715454102
Epoch 2060, val loss: 0.7366377115249634
Epoch 2070, training loss: 90.35220336914062 = 0.05533395707607269 + 10.0 * 9.02968692779541
Epoch 2070, val loss: 0.7405393719673157
Epoch 2080, training loss: 90.35499572753906 = 0.05438005179166794 + 10.0 * 9.030061721801758
Epoch 2080, val loss: 0.7452094554901123
Epoch 2090, training loss: 90.34817504882812 = 0.05343294143676758 + 10.0 * 9.029474258422852
Epoch 2090, val loss: 0.7492116093635559
Epoch 2100, training loss: 90.34332275390625 = 0.052501849830150604 + 10.0 * 9.029081344604492
Epoch 2100, val loss: 0.7536715269088745
Epoch 2110, training loss: 90.34735107421875 = 0.05159124732017517 + 10.0 * 9.029576301574707
Epoch 2110, val loss: 0.758285403251648
Epoch 2120, training loss: 90.36309051513672 = 0.05069877952337265 + 10.0 * 9.03123950958252
Epoch 2120, val loss: 0.7627685070037842
Epoch 2130, training loss: 90.34612274169922 = 0.049805641174316406 + 10.0 * 9.029631614685059
Epoch 2130, val loss: 0.7665842771530151
Epoch 2140, training loss: 90.33577728271484 = 0.04892909899353981 + 10.0 * 9.028684616088867
Epoch 2140, val loss: 0.7712538242340088
Epoch 2150, training loss: 90.33189392089844 = 0.048068974167108536 + 10.0 * 9.028383255004883
Epoch 2150, val loss: 0.775762677192688
Epoch 2160, training loss: 90.33191680908203 = 0.0472261905670166 + 10.0 * 9.02846908569336
Epoch 2160, val loss: 0.7801878452301025
Epoch 2170, training loss: 90.3577651977539 = 0.046408865600824356 + 10.0 * 9.031135559082031
Epoch 2170, val loss: 0.7846998572349548
Epoch 2180, training loss: 90.33788299560547 = 0.045590974390506744 + 10.0 * 9.029229164123535
Epoch 2180, val loss: 0.7886918187141418
Epoch 2190, training loss: 90.33860778808594 = 0.04479449242353439 + 10.0 * 9.02938175201416
Epoch 2190, val loss: 0.793694794178009
Epoch 2200, training loss: 90.33232879638672 = 0.04400760307908058 + 10.0 * 9.02883243560791
Epoch 2200, val loss: 0.7982415556907654
Epoch 2210, training loss: 90.33643341064453 = 0.04324089363217354 + 10.0 * 9.029318809509277
Epoch 2210, val loss: 0.8025862574577332
Epoch 2220, training loss: 90.33181762695312 = 0.042481567710638046 + 10.0 * 9.02893352508545
Epoch 2220, val loss: 0.8064855337142944
Epoch 2230, training loss: 90.32344818115234 = 0.04173973575234413 + 10.0 * 9.028170585632324
Epoch 2230, val loss: 0.8109896779060364
Epoch 2240, training loss: 90.31818389892578 = 0.04100635647773743 + 10.0 * 9.027717590332031
Epoch 2240, val loss: 0.8158033490180969
Epoch 2250, training loss: 90.3172836303711 = 0.040289171040058136 + 10.0 * 9.02769947052002
Epoch 2250, val loss: 0.820087730884552
Epoch 2260, training loss: 90.32881164550781 = 0.03959158807992935 + 10.0 * 9.028922080993652
Epoch 2260, val loss: 0.8245588541030884
Epoch 2270, training loss: 90.3119125366211 = 0.03889617323875427 + 10.0 * 9.027301788330078
Epoch 2270, val loss: 0.8288284540176392
Epoch 2280, training loss: 90.31616973876953 = 0.038219984620809555 + 10.0 * 9.02779483795166
Epoch 2280, val loss: 0.8332288265228271
Epoch 2290, training loss: 90.32706451416016 = 0.037562914192676544 + 10.0 * 9.028950691223145
Epoch 2290, val loss: 0.8381838798522949
Epoch 2300, training loss: 90.31880950927734 = 0.03690746799111366 + 10.0 * 9.028189659118652
Epoch 2300, val loss: 0.8425828814506531
Epoch 2310, training loss: 90.30870056152344 = 0.03626197203993797 + 10.0 * 9.027243614196777
Epoch 2310, val loss: 0.8464324474334717
Epoch 2320, training loss: 90.32603454589844 = 0.035642582923173904 + 10.0 * 9.02903938293457
Epoch 2320, val loss: 0.8507416844367981
Epoch 2330, training loss: 90.3074951171875 = 0.03502187877893448 + 10.0 * 9.027247428894043
Epoch 2330, val loss: 0.8553580641746521
Epoch 2340, training loss: 90.30339050292969 = 0.034416668117046356 + 10.0 * 9.026897430419922
Epoch 2340, val loss: 0.8594393134117126
Epoch 2350, training loss: 90.2973861694336 = 0.033822983503341675 + 10.0 * 9.02635669708252
Epoch 2350, val loss: 0.8637510538101196
Epoch 2360, training loss: 90.29832458496094 = 0.033242445439100266 + 10.0 * 9.026508331298828
Epoch 2360, val loss: 0.8681102991104126
Epoch 2370, training loss: 90.3228759765625 = 0.03268299996852875 + 10.0 * 9.029019355773926
Epoch 2370, val loss: 0.8720096349716187
Epoch 2380, training loss: 90.31192779541016 = 0.032123468816280365 + 10.0 * 9.02798080444336
Epoch 2380, val loss: 0.876679539680481
Epoch 2390, training loss: 90.30806732177734 = 0.031576916575431824 + 10.0 * 9.02764892578125
Epoch 2390, val loss: 0.8810638189315796
Epoch 2400, training loss: 90.2990951538086 = 0.031041840091347694 + 10.0 * 9.02680492401123
Epoch 2400, val loss: 0.8852108716964722
Epoch 2410, training loss: 90.29231262207031 = 0.030515212565660477 + 10.0 * 9.026179313659668
Epoch 2410, val loss: 0.8888513445854187
Epoch 2420, training loss: 90.29240417480469 = 0.030000798404216766 + 10.0 * 9.026240348815918
Epoch 2420, val loss: 0.8930609226226807
Epoch 2430, training loss: 90.29854583740234 = 0.029501475393772125 + 10.0 * 9.026904106140137
Epoch 2430, val loss: 0.8972302079200745
Epoch 2440, training loss: 90.28642272949219 = 0.029004449024796486 + 10.0 * 9.025741577148438
Epoch 2440, val loss: 0.9013442397117615
Epoch 2450, training loss: 90.2914810180664 = 0.028523044660687447 + 10.0 * 9.02629566192627
Epoch 2450, val loss: 0.9051360487937927
Epoch 2460, training loss: 90.30799865722656 = 0.02806815132498741 + 10.0 * 9.027993202209473
Epoch 2460, val loss: 0.9085714817047119
Epoch 2470, training loss: 90.28968048095703 = 0.02759242244064808 + 10.0 * 9.026208877563477
Epoch 2470, val loss: 0.9134661555290222
Epoch 2480, training loss: 90.27867889404297 = 0.02713632397353649 + 10.0 * 9.025154113769531
Epoch 2480, val loss: 0.9174780249595642
Epoch 2490, training loss: 90.28218841552734 = 0.026693962514400482 + 10.0 * 9.025548934936523
Epoch 2490, val loss: 0.9210901260375977
Epoch 2500, training loss: 90.29312896728516 = 0.026264654472470284 + 10.0 * 9.02668571472168
Epoch 2500, val loss: 0.9253076314926147
Epoch 2510, training loss: 90.28105926513672 = 0.02583874575793743 + 10.0 * 9.025522232055664
Epoch 2510, val loss: 0.9289696216583252
Epoch 2520, training loss: 90.28034973144531 = 0.02542463131248951 + 10.0 * 9.025492668151855
Epoch 2520, val loss: 0.9333270788192749
Epoch 2530, training loss: 90.28681945800781 = 0.025018027052283287 + 10.0 * 9.026180267333984
Epoch 2530, val loss: 0.9365155100822449
Epoch 2540, training loss: 90.27528381347656 = 0.024616936221718788 + 10.0 * 9.025066375732422
Epoch 2540, val loss: 0.9401271343231201
Epoch 2550, training loss: 90.27008819580078 = 0.024221977218985558 + 10.0 * 9.02458667755127
Epoch 2550, val loss: 0.9443095326423645
Epoch 2560, training loss: 90.2724380493164 = 0.023840507492423058 + 10.0 * 9.024859428405762
Epoch 2560, val loss: 0.9483237266540527
Epoch 2570, training loss: 90.29646301269531 = 0.023475389927625656 + 10.0 * 9.027298927307129
Epoch 2570, val loss: 0.9521830081939697
Epoch 2580, training loss: 90.291015625 = 0.02310578152537346 + 10.0 * 9.026790618896484
Epoch 2580, val loss: 0.955221951007843
Epoch 2590, training loss: 90.2717514038086 = 0.02274082973599434 + 10.0 * 9.024900436401367
Epoch 2590, val loss: 0.959151029586792
Epoch 2600, training loss: 90.26447296142578 = 0.0223837997764349 + 10.0 * 9.024209022521973
Epoch 2600, val loss: 0.9625462293624878
Epoch 2610, training loss: 90.26529693603516 = 0.022036980837583542 + 10.0 * 9.02432632446289
Epoch 2610, val loss: 0.966252863407135
Epoch 2620, training loss: 90.27008819580078 = 0.0217021182179451 + 10.0 * 9.0248384475708
Epoch 2620, val loss: 0.9695228934288025
Epoch 2630, training loss: 90.27674102783203 = 0.02137046307325363 + 10.0 * 9.025537490844727
Epoch 2630, val loss: 0.9734494090080261
Epoch 2640, training loss: 90.2625961303711 = 0.021042346954345703 + 10.0 * 9.024155616760254
Epoch 2640, val loss: 0.977713942527771
Epoch 2650, training loss: 90.26222229003906 = 0.020723748952150345 + 10.0 * 9.024149894714355
Epoch 2650, val loss: 0.9806881546974182
Epoch 2660, training loss: 90.25962829589844 = 0.020409664139151573 + 10.0 * 9.023921966552734
Epoch 2660, val loss: 0.9846535921096802
Epoch 2670, training loss: 90.26405334472656 = 0.020108036696910858 + 10.0 * 9.024394035339355
Epoch 2670, val loss: 0.9882550239562988
Epoch 2680, training loss: 90.2623519897461 = 0.01980850286781788 + 10.0 * 9.02425479888916
Epoch 2680, val loss: 0.9915211796760559
Epoch 2690, training loss: 90.25851440429688 = 0.01951678656041622 + 10.0 * 9.023900032043457
Epoch 2690, val loss: 0.9947438836097717
Epoch 2700, training loss: 90.25216674804688 = 0.019226130098104477 + 10.0 * 9.023294448852539
Epoch 2700, val loss: 0.9984905123710632
Epoch 2710, training loss: 90.25293731689453 = 0.01894478127360344 + 10.0 * 9.023399353027344
Epoch 2710, val loss: 1.0016621351242065
Epoch 2720, training loss: 90.27676391601562 = 0.01867740787565708 + 10.0 * 9.025808334350586
Epoch 2720, val loss: 1.0049086809158325
Epoch 2730, training loss: 90.25918579101562 = 0.018405774608254433 + 10.0 * 9.024078369140625
Epoch 2730, val loss: 1.008505940437317
Epoch 2740, training loss: 90.2561264038086 = 0.018137959763407707 + 10.0 * 9.023798942565918
Epoch 2740, val loss: 1.0121893882751465
Epoch 2750, training loss: 90.25254821777344 = 0.017879722639918327 + 10.0 * 9.023466110229492
Epoch 2750, val loss: 1.0155385732650757
Epoch 2760, training loss: 90.25855255126953 = 0.01762363500893116 + 10.0 * 9.024092674255371
Epoch 2760, val loss: 1.0182498693466187
Epoch 2770, training loss: 90.24878692626953 = 0.017372505739331245 + 10.0 * 9.023141860961914
Epoch 2770, val loss: 1.0212385654449463
Epoch 2780, training loss: 90.24674224853516 = 0.017127057537436485 + 10.0 * 9.022961616516113
Epoch 2780, val loss: 1.0246824026107788
Epoch 2790, training loss: 90.25843048095703 = 0.01689283177256584 + 10.0 * 9.024153709411621
Epoch 2790, val loss: 1.0279797315597534
Epoch 2800, training loss: 90.25305938720703 = 0.016657782718539238 + 10.0 * 9.023640632629395
Epoch 2800, val loss: 1.0311005115509033
Epoch 2810, training loss: 90.23885345458984 = 0.016421305015683174 + 10.0 * 9.02224349975586
Epoch 2810, val loss: 1.0338101387023926
Epoch 2820, training loss: 90.23988342285156 = 0.016195198521018028 + 10.0 * 9.022368431091309
Epoch 2820, val loss: 1.0368157625198364
Epoch 2830, training loss: 90.2623062133789 = 0.01598251983523369 + 10.0 * 9.024632453918457
Epoch 2830, val loss: 1.0399744510650635
Epoch 2840, training loss: 90.24490356445312 = 0.015763500705361366 + 10.0 * 9.022913932800293
Epoch 2840, val loss: 1.042923092842102
Epoch 2850, training loss: 90.2401123046875 = 0.01554402057081461 + 10.0 * 9.022457122802734
Epoch 2850, val loss: 1.0456159114837646
Epoch 2860, training loss: 90.23477172851562 = 0.015333209186792374 + 10.0 * 9.021944046020508
Epoch 2860, val loss: 1.048608660697937
Epoch 2870, training loss: 90.25750732421875 = 0.01513280812650919 + 10.0 * 9.024237632751465
Epoch 2870, val loss: 1.0518653392791748
Epoch 2880, training loss: 90.23821258544922 = 0.01492944173514843 + 10.0 * 9.02232837677002
Epoch 2880, val loss: 1.0548642873764038
Epoch 2890, training loss: 90.23535919189453 = 0.014731108210980892 + 10.0 * 9.022062301635742
Epoch 2890, val loss: 1.0577572584152222
Epoch 2900, training loss: 90.22872924804688 = 0.014533430337905884 + 10.0 * 9.021419525146484
Epoch 2900, val loss: 1.060110092163086
Epoch 2910, training loss: 90.232421875 = 0.014345496892929077 + 10.0 * 9.021807670593262
Epoch 2910, val loss: 1.0632661581039429
Epoch 2920, training loss: 90.25244140625 = 0.014169157482683659 + 10.0 * 9.02382755279541
Epoch 2920, val loss: 1.0667259693145752
Epoch 2930, training loss: 90.2405014038086 = 0.01397980097681284 + 10.0 * 9.022652626037598
Epoch 2930, val loss: 1.068541407585144
Epoch 2940, training loss: 90.2326431274414 = 0.013799432665109634 + 10.0 * 9.021883964538574
Epoch 2940, val loss: 1.0709657669067383
Epoch 2950, training loss: 90.23235321044922 = 0.013624878600239754 + 10.0 * 9.021872520446777
Epoch 2950, val loss: 1.0733740329742432
Epoch 2960, training loss: 90.22673797607422 = 0.013451856561005116 + 10.0 * 9.021328926086426
Epoch 2960, val loss: 1.0767658948898315
Epoch 2970, training loss: 90.23971557617188 = 0.013288022950291634 + 10.0 * 9.022642135620117
Epoch 2970, val loss: 1.0790283679962158
Epoch 2980, training loss: 90.22838592529297 = 0.013120774179697037 + 10.0 * 9.021526336669922
Epoch 2980, val loss: 1.0811891555786133
Epoch 2990, training loss: 90.2286605834961 = 0.01295859832316637 + 10.0 * 9.021570205688477
Epoch 2990, val loss: 1.08443021774292
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8458
Overall ASR: 0.6329
Flip ASR: 0.5438/1554 nodes
The final ASR:0.67546, 0.03124, Accuracy:0.85118, 0.00417
#Attach Nodes:5
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 97446])
remove edge: torch.Size([2, 79800])
updated graph: torch.Size([2, 88598])
./selected_nodes/Pubmed/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8503
Overall ASR: 0.6998
Flip ASR: 0.6287/1554 nodes
./selected_nodes/Pubmed/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8519
Overall ASR: 0.7414
Flip ASR: 0.6795/1554 nodes
./selected_nodes/Pubmed/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8498
Overall ASR: 0.7252
Flip ASR: 0.6589/1554 nodes
The final ASR:0.72211, 0.01711, Accuracy:0.85067, 0.00086
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.3 der2:0.2 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 104.68883514404297 = 1.097300410270691 + 10.0 * 10.359153747558594
Epoch 0, val loss: 1.0964713096618652
Epoch 10, training loss: 104.65731811523438 = 1.0862857103347778 + 10.0 * 10.35710334777832
Epoch 10, val loss: 1.0852442979812622
Epoch 20, training loss: 104.25994873046875 = 1.0721664428710938 + 10.0 * 10.318778038024902
Epoch 20, val loss: 1.0708632469177246
Epoch 30, training loss: 101.80608367919922 = 1.0571452379226685 + 10.0 * 10.074893951416016
Epoch 30, val loss: 1.0558675527572632
Epoch 40, training loss: 99.5364990234375 = 1.0409034490585327 + 10.0 * 9.849559783935547
Epoch 40, val loss: 1.0395407676696777
Epoch 50, training loss: 97.02910614013672 = 1.0235708951950073 + 10.0 * 9.600553512573242
Epoch 50, val loss: 1.0219993591308594
Epoch 60, training loss: 95.65210723876953 = 1.0032436847686768 + 10.0 * 9.464886665344238
Epoch 60, val loss: 1.0022791624069214
Epoch 70, training loss: 95.00239562988281 = 0.9809081554412842 + 10.0 * 9.402148246765137
Epoch 70, val loss: 0.9803079962730408
Epoch 80, training loss: 94.70850372314453 = 0.9534268975257874 + 10.0 * 9.375507354736328
Epoch 80, val loss: 0.9529655575752258
Epoch 90, training loss: 94.39472198486328 = 0.922500491142273 + 10.0 * 9.347222328186035
Epoch 90, val loss: 0.9226687550544739
Epoch 100, training loss: 94.0855484008789 = 0.8877983689308167 + 10.0 * 9.319774627685547
Epoch 100, val loss: 0.8890783786773682
Epoch 110, training loss: 93.81324768066406 = 0.8478873372077942 + 10.0 * 9.296536445617676
Epoch 110, val loss: 0.8508512377738953
Epoch 120, training loss: 93.65769958496094 = 0.803714394569397 + 10.0 * 9.285398483276367
Epoch 120, val loss: 0.8088552951812744
Epoch 130, training loss: 93.51715087890625 = 0.7557388544082642 + 10.0 * 9.276141166687012
Epoch 130, val loss: 0.7632225155830383
Epoch 140, training loss: 93.35629272460938 = 0.7052340507507324 + 10.0 * 9.265106201171875
Epoch 140, val loss: 0.7163876295089722
Epoch 150, training loss: 93.15657043457031 = 0.656865656375885 + 10.0 * 9.249970436096191
Epoch 150, val loss: 0.6723122000694275
Epoch 160, training loss: 92.98625946044922 = 0.6137408018112183 + 10.0 * 9.237252235412598
Epoch 160, val loss: 0.6335031390190125
Epoch 170, training loss: 92.75116729736328 = 0.575150728225708 + 10.0 * 9.217601776123047
Epoch 170, val loss: 0.5995978713035583
Epoch 180, training loss: 92.57263946533203 = 0.540631115436554 + 10.0 * 9.203200340270996
Epoch 180, val loss: 0.5695044994354248
Epoch 190, training loss: 92.41114807128906 = 0.5109240412712097 + 10.0 * 9.190022468566895
Epoch 190, val loss: 0.544224202632904
Epoch 200, training loss: 92.33100891113281 = 0.48637083172798157 + 10.0 * 9.184463500976562
Epoch 200, val loss: 0.523877739906311
Epoch 210, training loss: 92.20429992675781 = 0.4660547971725464 + 10.0 * 9.173824310302734
Epoch 210, val loss: 0.5076279640197754
Epoch 220, training loss: 92.11505126953125 = 0.44960150122642517 + 10.0 * 9.166544914245605
Epoch 220, val loss: 0.49500659108161926
Epoch 230, training loss: 92.0362319946289 = 0.43623408675193787 + 10.0 * 9.15999984741211
Epoch 230, val loss: 0.48528215289115906
Epoch 240, training loss: 92.03985595703125 = 0.42503124475479126 + 10.0 * 9.161481857299805
Epoch 240, val loss: 0.47767794132232666
Epoch 250, training loss: 91.91618347167969 = 0.41545307636260986 + 10.0 * 9.150073051452637
Epoch 250, val loss: 0.4713062047958374
Epoch 260, training loss: 91.85528564453125 = 0.4072730541229248 + 10.0 * 9.144801139831543
Epoch 260, val loss: 0.466228723526001
Epoch 270, training loss: 91.79512786865234 = 0.4000280797481537 + 10.0 * 9.139510154724121
Epoch 270, val loss: 0.4621337652206421
Epoch 280, training loss: 91.7479248046875 = 0.39347949624061584 + 10.0 * 9.135444641113281
Epoch 280, val loss: 0.45855334401130676
Epoch 290, training loss: 91.70490264892578 = 0.3875012695789337 + 10.0 * 9.131739616394043
Epoch 290, val loss: 0.4554845094680786
Epoch 300, training loss: 91.73035430908203 = 0.38205084204673767 + 10.0 * 9.134830474853516
Epoch 300, val loss: 0.45289427042007446
Epoch 310, training loss: 91.63835144042969 = 0.3770288825035095 + 10.0 * 9.126132011413574
Epoch 310, val loss: 0.4504307806491852
Epoch 320, training loss: 91.60185241699219 = 0.3724937438964844 + 10.0 * 9.12293529510498
Epoch 320, val loss: 0.44847699999809265
Epoch 330, training loss: 91.57039642333984 = 0.36829304695129395 + 10.0 * 9.120210647583008
Epoch 330, val loss: 0.4467279613018036
Epoch 340, training loss: 91.53985595703125 = 0.36430296301841736 + 10.0 * 9.117555618286133
Epoch 340, val loss: 0.4451112449169159
Epoch 350, training loss: 91.59302520751953 = 0.3604837656021118 + 10.0 * 9.123254776000977
Epoch 350, val loss: 0.4436573088169098
Epoch 360, training loss: 91.49771118164062 = 0.3567715585231781 + 10.0 * 9.114093780517578
Epoch 360, val loss: 0.4421582520008087
Epoch 370, training loss: 91.46781158447266 = 0.3533205986022949 + 10.0 * 9.111449241638184
Epoch 370, val loss: 0.44091325998306274
Epoch 380, training loss: 91.44829559326172 = 0.3500087559223175 + 10.0 * 9.10982894897461
Epoch 380, val loss: 0.43964946269989014
Epoch 390, training loss: 91.42889404296875 = 0.34677210450172424 + 10.0 * 9.1082124710083
Epoch 390, val loss: 0.43848106265068054
Epoch 400, training loss: 91.4185562133789 = 0.34359169006347656 + 10.0 * 9.10749626159668
Epoch 400, val loss: 0.43727102875709534
Epoch 410, training loss: 91.40045928955078 = 0.3404935598373413 + 10.0 * 9.105997085571289
Epoch 410, val loss: 0.43617531657218933
Epoch 420, training loss: 91.3803482055664 = 0.33749809861183167 + 10.0 * 9.104284286499023
Epoch 420, val loss: 0.4350389838218689
Epoch 430, training loss: 91.38543701171875 = 0.3345579206943512 + 10.0 * 9.105088233947754
Epoch 430, val loss: 0.43394705653190613
Epoch 440, training loss: 91.36018371582031 = 0.3316364884376526 + 10.0 * 9.10285472869873
Epoch 440, val loss: 0.43286892771720886
Epoch 450, training loss: 91.32965850830078 = 0.32879379391670227 + 10.0 * 9.100086212158203
Epoch 450, val loss: 0.4317788779735565
Epoch 460, training loss: 91.31562042236328 = 0.3259984850883484 + 10.0 * 9.098962783813477
Epoch 460, val loss: 0.43081215023994446
Epoch 470, training loss: 91.30718231201172 = 0.32322555780410767 + 10.0 * 9.098395347595215
Epoch 470, val loss: 0.42997315526008606
Epoch 480, training loss: 91.2882308959961 = 0.32045915722846985 + 10.0 * 9.096776962280273
Epoch 480, val loss: 0.428905189037323
Epoch 490, training loss: 91.2724838256836 = 0.31773117184638977 + 10.0 * 9.095475196838379
Epoch 490, val loss: 0.4280669689178467
Epoch 500, training loss: 91.25402069091797 = 0.31503933668136597 + 10.0 * 9.093897819519043
Epoch 500, val loss: 0.4272589385509491
Epoch 510, training loss: 91.23906707763672 = 0.3123592436313629 + 10.0 * 9.092670440673828
Epoch 510, val loss: 0.4264267683029175
Epoch 520, training loss: 91.28996276855469 = 0.3096890151500702 + 10.0 * 9.098027229309082
Epoch 520, val loss: 0.42552193999290466
Epoch 530, training loss: 91.24097442626953 = 0.3070005476474762 + 10.0 * 9.09339714050293
Epoch 530, val loss: 0.4248832166194916
Epoch 540, training loss: 91.20173645019531 = 0.3044090270996094 + 10.0 * 9.08973217010498
Epoch 540, val loss: 0.42416325211524963
Epoch 550, training loss: 91.18817901611328 = 0.3018646538257599 + 10.0 * 9.088631629943848
Epoch 550, val loss: 0.42355015873908997
Epoch 560, training loss: 91.17327117919922 = 0.299312561750412 + 10.0 * 9.087396621704102
Epoch 560, val loss: 0.4229796230792999
Epoch 570, training loss: 91.16212463378906 = 0.2967536449432373 + 10.0 * 9.08653736114502
Epoch 570, val loss: 0.42241865396499634
Epoch 580, training loss: 91.14936065673828 = 0.2941778898239136 + 10.0 * 9.085518836975098
Epoch 580, val loss: 0.4218333661556244
Epoch 590, training loss: 91.13658142089844 = 0.29163673520088196 + 10.0 * 9.084494590759277
Epoch 590, val loss: 0.4213246703147888
Epoch 600, training loss: 91.12515258789062 = 0.28916242718696594 + 10.0 * 9.083599090576172
Epoch 600, val loss: 0.42091789841651917
Epoch 610, training loss: 91.11299133300781 = 0.2866915762424469 + 10.0 * 9.082630157470703
Epoch 610, val loss: 0.42057618498802185
Epoch 620, training loss: 91.14888000488281 = 0.284221887588501 + 10.0 * 9.086465835571289
Epoch 620, val loss: 0.42020100355148315
Epoch 630, training loss: 91.09342956542969 = 0.2817422151565552 + 10.0 * 9.081168174743652
Epoch 630, val loss: 0.41993051767349243
Epoch 640, training loss: 91.08406829833984 = 0.279305636882782 + 10.0 * 9.080476760864258
Epoch 640, val loss: 0.41959381103515625
Epoch 650, training loss: 91.07640075683594 = 0.27687758207321167 + 10.0 * 9.079952239990234
Epoch 650, val loss: 0.4193931519985199
Epoch 660, training loss: 91.06118774414062 = 0.2744503915309906 + 10.0 * 9.078673362731934
Epoch 660, val loss: 0.4192121624946594
Epoch 670, training loss: 91.05628967285156 = 0.27204617857933044 + 10.0 * 9.078424453735352
Epoch 670, val loss: 0.4190380573272705
Epoch 680, training loss: 91.03936004638672 = 0.2696666717529297 + 10.0 * 9.076969146728516
Epoch 680, val loss: 0.4189024567604065
Epoch 690, training loss: 91.0762710571289 = 0.2672954797744751 + 10.0 * 9.080897331237793
Epoch 690, val loss: 0.4188207685947418
Epoch 700, training loss: 91.02250671386719 = 0.26491814851760864 + 10.0 * 9.075758934020996
Epoch 700, val loss: 0.4187549650669098
Epoch 710, training loss: 91.01715087890625 = 0.2625710070133209 + 10.0 * 9.075457572937012
Epoch 710, val loss: 0.418744295835495
Epoch 720, training loss: 91.00321960449219 = 0.2602345943450928 + 10.0 * 9.074298858642578
Epoch 720, val loss: 0.4187968075275421
Epoch 730, training loss: 91.0250473022461 = 0.2579071521759033 + 10.0 * 9.076714515686035
Epoch 730, val loss: 0.41887539625167847
Epoch 740, training loss: 91.01636505126953 = 0.25557321310043335 + 10.0 * 9.076079368591309
Epoch 740, val loss: 0.41887548565864563
Epoch 750, training loss: 90.98470306396484 = 0.253256231546402 + 10.0 * 9.073144912719727
Epoch 750, val loss: 0.4190417230129242
Epoch 760, training loss: 90.97317504882812 = 0.2509631812572479 + 10.0 * 9.072221755981445
Epoch 760, val loss: 0.419279545545578
Epoch 770, training loss: 90.9693832397461 = 0.24866896867752075 + 10.0 * 9.072071075439453
Epoch 770, val loss: 0.41954922676086426
Epoch 780, training loss: 90.96626281738281 = 0.24636666476726532 + 10.0 * 9.071989059448242
Epoch 780, val loss: 0.4197765290737152
Epoch 790, training loss: 90.95191955566406 = 0.24406762421131134 + 10.0 * 9.070785522460938
Epoch 790, val loss: 0.4200960397720337
Epoch 800, training loss: 90.9453125 = 0.24177253246307373 + 10.0 * 9.070353507995605
Epoch 800, val loss: 0.4204208254814148
Epoch 810, training loss: 90.93962860107422 = 0.2394745647907257 + 10.0 * 9.070015907287598
Epoch 810, val loss: 0.4209107458591461
Epoch 820, training loss: 90.93563842773438 = 0.23717208206653595 + 10.0 * 9.069846153259277
Epoch 820, val loss: 0.42127907276153564
Epoch 830, training loss: 90.93231201171875 = 0.23487307131290436 + 10.0 * 9.069744110107422
Epoch 830, val loss: 0.421792209148407
Epoch 840, training loss: 90.91819763183594 = 0.2325817495584488 + 10.0 * 9.068561553955078
Epoch 840, val loss: 0.4223005473613739
Epoch 850, training loss: 90.9347152709961 = 0.23029129207134247 + 10.0 * 9.070442199707031
Epoch 850, val loss: 0.4228115975856781
Epoch 860, training loss: 90.90936279296875 = 0.22798480093479156 + 10.0 * 9.068138122558594
Epoch 860, val loss: 0.42345425486564636
Epoch 870, training loss: 90.89704132080078 = 0.2256821095943451 + 10.0 * 9.06713581085205
Epoch 870, val loss: 0.4240301549434662
Epoch 880, training loss: 90.88907623291016 = 0.22336842119693756 + 10.0 * 9.066571235656738
Epoch 880, val loss: 0.4246504008769989
Epoch 890, training loss: 90.9158935546875 = 0.221052348613739 + 10.0 * 9.069483757019043
Epoch 890, val loss: 0.4251781404018402
Epoch 900, training loss: 90.8970718383789 = 0.2187149077653885 + 10.0 * 9.067835807800293
Epoch 900, val loss: 0.4261639416217804
Epoch 910, training loss: 90.87328338623047 = 0.21638721227645874 + 10.0 * 9.065690040588379
Epoch 910, val loss: 0.42683976888656616
Epoch 920, training loss: 90.86615753173828 = 0.2140575349330902 + 10.0 * 9.065210342407227
Epoch 920, val loss: 0.4275960922241211
Epoch 930, training loss: 90.88777923583984 = 0.21172058582305908 + 10.0 * 9.067605972290039
Epoch 930, val loss: 0.4284542500972748
Epoch 940, training loss: 90.86181640625 = 0.20935241878032684 + 10.0 * 9.06524658203125
Epoch 940, val loss: 0.4291869103908539
Epoch 950, training loss: 90.85186004638672 = 0.20698508620262146 + 10.0 * 9.06448745727539
Epoch 950, val loss: 0.4301615059375763
Epoch 960, training loss: 90.84226989746094 = 0.20460760593414307 + 10.0 * 9.063766479492188
Epoch 960, val loss: 0.4309949576854706
Epoch 970, training loss: 90.83299255371094 = 0.20222288370132446 + 10.0 * 9.063076972961426
Epoch 970, val loss: 0.4318750500679016
Epoch 980, training loss: 90.86531829833984 = 0.19984057545661926 + 10.0 * 9.066547393798828
Epoch 980, val loss: 0.4328596591949463
Epoch 990, training loss: 90.83418273925781 = 0.1974445879459381 + 10.0 * 9.063673973083496
Epoch 990, val loss: 0.43387195467948914
Epoch 1000, training loss: 90.81168365478516 = 0.19504867494106293 + 10.0 * 9.061663627624512
Epoch 1000, val loss: 0.43502524495124817
Epoch 1010, training loss: 90.8102035522461 = 0.19264985620975494 + 10.0 * 9.061755180358887
Epoch 1010, val loss: 0.43619221448898315
Epoch 1020, training loss: 90.84221649169922 = 0.19025281071662903 + 10.0 * 9.06519603729248
Epoch 1020, val loss: 0.4375899136066437
Epoch 1030, training loss: 90.79589080810547 = 0.18784283101558685 + 10.0 * 9.06080436706543
Epoch 1030, val loss: 0.4386604130268097
Epoch 1040, training loss: 90.79212188720703 = 0.18544358015060425 + 10.0 * 9.060667991638184
Epoch 1040, val loss: 0.43988779187202454
Epoch 1050, training loss: 90.78340148925781 = 0.18302777409553528 + 10.0 * 9.060037612915039
Epoch 1050, val loss: 0.4412342607975006
Epoch 1060, training loss: 90.7774658203125 = 0.1805994063615799 + 10.0 * 9.059686660766602
Epoch 1060, val loss: 0.4426325857639313
Epoch 1070, training loss: 90.79855346679688 = 0.1781756728887558 + 10.0 * 9.062037467956543
Epoch 1070, val loss: 0.44393157958984375
Epoch 1080, training loss: 90.78559875488281 = 0.17573301494121552 + 10.0 * 9.060986518859863
Epoch 1080, val loss: 0.44562670588493347
Epoch 1090, training loss: 90.76885223388672 = 0.17329680919647217 + 10.0 * 9.059556007385254
Epoch 1090, val loss: 0.44693976640701294
Epoch 1100, training loss: 90.76033782958984 = 0.17085815966129303 + 10.0 * 9.058947563171387
Epoch 1100, val loss: 0.44847142696380615
Epoch 1110, training loss: 90.78178405761719 = 0.16844207048416138 + 10.0 * 9.061334609985352
Epoch 1110, val loss: 0.4499012529850006
Epoch 1120, training loss: 90.75141143798828 = 0.16600871086120605 + 10.0 * 9.058540344238281
Epoch 1120, val loss: 0.45201417803764343
Epoch 1130, training loss: 90.74070739746094 = 0.1635872721672058 + 10.0 * 9.057711601257324
Epoch 1130, val loss: 0.45333272218704224
Epoch 1140, training loss: 90.73384857177734 = 0.1611684411764145 + 10.0 * 9.057268142700195
Epoch 1140, val loss: 0.455247700214386
Epoch 1150, training loss: 90.74346160888672 = 0.15875917673110962 + 10.0 * 9.058469772338867
Epoch 1150, val loss: 0.4569673538208008
Epoch 1160, training loss: 90.72596740722656 = 0.15634366869926453 + 10.0 * 9.056962966918945
Epoch 1160, val loss: 0.45888185501098633
Epoch 1170, training loss: 90.72732543945312 = 0.1539461761713028 + 10.0 * 9.057337760925293
Epoch 1170, val loss: 0.4606463611125946
Epoch 1180, training loss: 90.72034454345703 = 0.15155485272407532 + 10.0 * 9.056879043579102
Epoch 1180, val loss: 0.4626605808734894
Epoch 1190, training loss: 90.7090835571289 = 0.14917774498462677 + 10.0 * 9.055990219116211
Epoch 1190, val loss: 0.46445655822753906
Epoch 1200, training loss: 90.70309448242188 = 0.14680635929107666 + 10.0 * 9.055628776550293
Epoch 1200, val loss: 0.4664858877658844
Epoch 1210, training loss: 90.72473907470703 = 0.14445814490318298 + 10.0 * 9.058028221130371
Epoch 1210, val loss: 0.46841171383857727
Epoch 1220, training loss: 90.70181274414062 = 0.14210671186447144 + 10.0 * 9.055971145629883
Epoch 1220, val loss: 0.47071734070777893
Epoch 1230, training loss: 90.69664764404297 = 0.13978058099746704 + 10.0 * 9.055686950683594
Epoch 1230, val loss: 0.4726557433605194
Epoch 1240, training loss: 90.69837188720703 = 0.13747543096542358 + 10.0 * 9.056089401245117
Epoch 1240, val loss: 0.47487494349479675
Epoch 1250, training loss: 90.68006896972656 = 0.135175421833992 + 10.0 * 9.054489135742188
Epoch 1250, val loss: 0.4776560664176941
Epoch 1260, training loss: 90.67002868652344 = 0.1328919678926468 + 10.0 * 9.05371379852295
Epoch 1260, val loss: 0.47972598671913147
Epoch 1270, training loss: 90.66712951660156 = 0.13062988221645355 + 10.0 * 9.05364990234375
Epoch 1270, val loss: 0.48210182785987854
Epoch 1280, training loss: 90.70486450195312 = 0.1284000128507614 + 10.0 * 9.057645797729492
Epoch 1280, val loss: 0.4845297038555145
Epoch 1290, training loss: 90.68336486816406 = 0.12618251144886017 + 10.0 * 9.055718421936035
Epoch 1290, val loss: 0.4871911108493805
Epoch 1300, training loss: 90.65576171875 = 0.12396419793367386 + 10.0 * 9.053179740905762
Epoch 1300, val loss: 0.4897037446498871
Epoch 1310, training loss: 90.64883422851562 = 0.12178210914134979 + 10.0 * 9.052705764770508
Epoch 1310, val loss: 0.4924256205558777
Epoch 1320, training loss: 90.64654541015625 = 0.11962597072124481 + 10.0 * 9.052691459655762
Epoch 1320, val loss: 0.4950506091117859
Epoch 1330, training loss: 90.66890716552734 = 0.11751698702573776 + 10.0 * 9.055139541625977
Epoch 1330, val loss: 0.49796468019485474
Epoch 1340, training loss: 90.65140533447266 = 0.11540994793176651 + 10.0 * 9.05359935760498
Epoch 1340, val loss: 0.4997917413711548
Epoch 1350, training loss: 90.63968658447266 = 0.11332402378320694 + 10.0 * 9.05263614654541
Epoch 1350, val loss: 0.5029852390289307
Epoch 1360, training loss: 90.6265640258789 = 0.1112651526927948 + 10.0 * 9.051529884338379
Epoch 1360, val loss: 0.5057263970375061
Epoch 1370, training loss: 90.63186645507812 = 0.10924215614795685 + 10.0 * 9.052262306213379
Epoch 1370, val loss: 0.5088741183280945
Epoch 1380, training loss: 90.63396453857422 = 0.10724077373743057 + 10.0 * 9.052672386169434
Epoch 1380, val loss: 0.5116905570030212
Epoch 1390, training loss: 90.61627960205078 = 0.10525134205818176 + 10.0 * 9.051102638244629
Epoch 1390, val loss: 0.5147156715393066
Epoch 1400, training loss: 90.61119079589844 = 0.10329544544219971 + 10.0 * 9.050789833068848
Epoch 1400, val loss: 0.517627477645874
Epoch 1410, training loss: 90.6141128540039 = 0.10137344896793365 + 10.0 * 9.051274299621582
Epoch 1410, val loss: 0.5208751559257507
Epoch 1420, training loss: 90.61052703857422 = 0.09946896880865097 + 10.0 * 9.051105499267578
Epoch 1420, val loss: 0.5239039659500122
Epoch 1430, training loss: 90.60750579833984 = 0.09760187566280365 + 10.0 * 9.050990104675293
Epoch 1430, val loss: 0.5265897512435913
Epoch 1440, training loss: 90.59708404541016 = 0.09574565291404724 + 10.0 * 9.05013370513916
Epoch 1440, val loss: 0.5300853252410889
Epoch 1450, training loss: 90.60458374023438 = 0.09393071383237839 + 10.0 * 9.051065444946289
Epoch 1450, val loss: 0.5333711504936218
Epoch 1460, training loss: 90.58674621582031 = 0.09213293343782425 + 10.0 * 9.049461364746094
Epoch 1460, val loss: 0.5367301106452942
Epoch 1470, training loss: 90.58296203613281 = 0.09036580473184586 + 10.0 * 9.049260139465332
Epoch 1470, val loss: 0.540058970451355
Epoch 1480, training loss: 90.59169006347656 = 0.0886366069316864 + 10.0 * 9.050305366516113
Epoch 1480, val loss: 0.5437882542610168
Epoch 1490, training loss: 90.5830307006836 = 0.08692064881324768 + 10.0 * 9.04961109161377
Epoch 1490, val loss: 0.547177791595459
Epoch 1500, training loss: 90.57498931884766 = 0.08522406220436096 + 10.0 * 9.04897689819336
Epoch 1500, val loss: 0.5503976345062256
Epoch 1510, training loss: 90.57294464111328 = 0.08356660604476929 + 10.0 * 9.048937797546387
Epoch 1510, val loss: 0.5532819628715515
Epoch 1520, training loss: 90.57657623291016 = 0.08193201571702957 + 10.0 * 9.049464225769043
Epoch 1520, val loss: 0.5567930936813354
Epoch 1530, training loss: 90.56389617919922 = 0.08031633496284485 + 10.0 * 9.048357963562012
Epoch 1530, val loss: 0.5607271194458008
Epoch 1540, training loss: 90.55778503417969 = 0.07873488962650299 + 10.0 * 9.047904968261719
Epoch 1540, val loss: 0.5642055869102478
Epoch 1550, training loss: 90.56375122070312 = 0.07718461006879807 + 10.0 * 9.048656463623047
Epoch 1550, val loss: 0.5674493908882141
Epoch 1560, training loss: 90.56072998046875 = 0.07566461712121964 + 10.0 * 9.048505783081055
Epoch 1560, val loss: 0.5715286135673523
Epoch 1570, training loss: 90.55088806152344 = 0.07416140288114548 + 10.0 * 9.047673225402832
Epoch 1570, val loss: 0.5747878551483154
Epoch 1580, training loss: 90.54690551757812 = 0.07269230484962463 + 10.0 * 9.0474214553833
Epoch 1580, val loss: 0.5783114433288574
Epoch 1590, training loss: 90.5557861328125 = 0.07125572115182877 + 10.0 * 9.048452377319336
Epoch 1590, val loss: 0.5819323062896729
Epoch 1600, training loss: 90.5389633178711 = 0.06982943415641785 + 10.0 * 9.046913146972656
Epoch 1600, val loss: 0.5855696797370911
Epoch 1610, training loss: 90.53291320800781 = 0.06843434274196625 + 10.0 * 9.04644775390625
Epoch 1610, val loss: 0.5892478823661804
Epoch 1620, training loss: 90.53146362304688 = 0.06707363575696945 + 10.0 * 9.046439170837402
Epoch 1620, val loss: 0.5930758118629456
Epoch 1630, training loss: 90.56214904785156 = 0.06576330959796906 + 10.0 * 9.049638748168945
Epoch 1630, val loss: 0.5971410274505615
Epoch 1640, training loss: 90.53697204589844 = 0.06442280113697052 + 10.0 * 9.04725456237793
Epoch 1640, val loss: 0.5999633073806763
Epoch 1650, training loss: 90.53169250488281 = 0.06313830614089966 + 10.0 * 9.046854972839355
Epoch 1650, val loss: 0.604174017906189
Epoch 1660, training loss: 90.52752685546875 = 0.06186530366539955 + 10.0 * 9.046566009521484
Epoch 1660, val loss: 0.6076855659484863
Epoch 1670, training loss: 90.51746368408203 = 0.06061793863773346 + 10.0 * 9.045684814453125
Epoch 1670, val loss: 0.6112597584724426
Epoch 1680, training loss: 90.51953887939453 = 0.05940240994095802 + 10.0 * 9.046013832092285
Epoch 1680, val loss: 0.6150463819503784
Epoch 1690, training loss: 90.5162124633789 = 0.05820659548044205 + 10.0 * 9.045801162719727
Epoch 1690, val loss: 0.6186089515686035
Epoch 1700, training loss: 90.51053619384766 = 0.05703520029783249 + 10.0 * 9.045350074768066
Epoch 1700, val loss: 0.6220371127128601
Epoch 1710, training loss: 90.5193862915039 = 0.05589922517538071 + 10.0 * 9.046348571777344
Epoch 1710, val loss: 0.6258599162101746
Epoch 1720, training loss: 90.50715637207031 = 0.054773248732089996 + 10.0 * 9.045238494873047
Epoch 1720, val loss: 0.6303645968437195
Epoch 1730, training loss: 90.50989532470703 = 0.05366598814725876 + 10.0 * 9.045622825622559
Epoch 1730, val loss: 0.634091854095459
Epoch 1740, training loss: 90.49674987792969 = 0.05257944390177727 + 10.0 * 9.044416427612305
Epoch 1740, val loss: 0.637505829334259
Epoch 1750, training loss: 90.50019836425781 = 0.051526278257369995 + 10.0 * 9.044866561889648
Epoch 1750, val loss: 0.6411075592041016
Epoch 1760, training loss: 90.49290466308594 = 0.050485335290431976 + 10.0 * 9.044241905212402
Epoch 1760, val loss: 0.6454331874847412
Epoch 1770, training loss: 90.49456024169922 = 0.049475640058517456 + 10.0 * 9.04450798034668
Epoch 1770, val loss: 0.6493992209434509
Epoch 1780, training loss: 90.51119232177734 = 0.04848155751824379 + 10.0 * 9.046270370483398
Epoch 1780, val loss: 0.6528151631355286
Epoch 1790, training loss: 90.48794555664062 = 0.04749992489814758 + 10.0 * 9.044044494628906
Epoch 1790, val loss: 0.656677782535553
Epoch 1800, training loss: 90.48046112060547 = 0.04654168337583542 + 10.0 * 9.043392181396484
Epoch 1800, val loss: 0.660662055015564
Epoch 1810, training loss: 90.4805908203125 = 0.04560432955622673 + 10.0 * 9.043498992919922
Epoch 1810, val loss: 0.6643929481506348
Epoch 1820, training loss: 90.49737548828125 = 0.044693175703287125 + 10.0 * 9.045268058776855
Epoch 1820, val loss: 0.6682066321372986
Epoch 1830, training loss: 90.47499084472656 = 0.0437934510409832 + 10.0 * 9.043119430541992
Epoch 1830, val loss: 0.6723527312278748
Epoch 1840, training loss: 90.4709701538086 = 0.04291350767016411 + 10.0 * 9.042805671691895
Epoch 1840, val loss: 0.6760649085044861
Epoch 1850, training loss: 90.4744873046875 = 0.04205618053674698 + 10.0 * 9.043243408203125
Epoch 1850, val loss: 0.6797767281532288
Epoch 1860, training loss: 90.4648208618164 = 0.041205886751413345 + 10.0 * 9.04236125946045
Epoch 1860, val loss: 0.6835287809371948
Epoch 1870, training loss: 90.46675872802734 = 0.04038074240088463 + 10.0 * 9.042637825012207
Epoch 1870, val loss: 0.6871128082275391
Epoch 1880, training loss: 90.4675521850586 = 0.03957471624016762 + 10.0 * 9.042798042297363
Epoch 1880, val loss: 0.6911291480064392
Epoch 1890, training loss: 90.47347259521484 = 0.03879440203309059 + 10.0 * 9.04346752166748
Epoch 1890, val loss: 0.6947982907295227
Epoch 1900, training loss: 90.46187591552734 = 0.03801414743065834 + 10.0 * 9.042386054992676
Epoch 1900, val loss: 0.698244035243988
Epoch 1910, training loss: 90.48027801513672 = 0.03730793669819832 + 10.0 * 9.044297218322754
Epoch 1910, val loss: 0.7030642032623291
Epoch 1920, training loss: 90.46476745605469 = 0.036539603024721146 + 10.0 * 9.04282283782959
Epoch 1920, val loss: 0.7050049304962158
Epoch 1930, training loss: 90.45376586914062 = 0.03581378236413002 + 10.0 * 9.04179573059082
Epoch 1930, val loss: 0.7092934250831604
Epoch 1940, training loss: 90.44456481933594 = 0.03510421887040138 + 10.0 * 9.040946006774902
Epoch 1940, val loss: 0.7124627828598022
Epoch 1950, training loss: 90.44258117675781 = 0.03441213071346283 + 10.0 * 9.040817260742188
Epoch 1950, val loss: 0.7160516977310181
Epoch 1960, training loss: 90.45610046386719 = 0.03374496102333069 + 10.0 * 9.042235374450684
Epoch 1960, val loss: 0.7194437980651855
Epoch 1970, training loss: 90.43561553955078 = 0.03307829797267914 + 10.0 * 9.040253639221191
Epoch 1970, val loss: 0.7231428027153015
Epoch 1980, training loss: 90.43855285644531 = 0.03243501856923103 + 10.0 * 9.04061222076416
Epoch 1980, val loss: 0.726405143737793
Epoch 1990, training loss: 90.4733657836914 = 0.031814996153116226 + 10.0 * 9.04415512084961
Epoch 1990, val loss: 0.7299007177352905
Epoch 2000, training loss: 90.44446563720703 = 0.03117828071117401 + 10.0 * 9.041328430175781
Epoch 2000, val loss: 0.7335290312767029
Epoch 2010, training loss: 90.43444061279297 = 0.030567701905965805 + 10.0 * 9.040387153625488
Epoch 2010, val loss: 0.7372269630432129
Epoch 2020, training loss: 90.43055725097656 = 0.02997366152703762 + 10.0 * 9.040058135986328
Epoch 2020, val loss: 0.7406868934631348
Epoch 2030, training loss: 90.42791748046875 = 0.029396841302514076 + 10.0 * 9.039852142333984
Epoch 2030, val loss: 0.7440866827964783
Epoch 2040, training loss: 90.43456268310547 = 0.028834043070673943 + 10.0 * 9.040573120117188
Epoch 2040, val loss: 0.747571587562561
Epoch 2050, training loss: 90.43426513671875 = 0.028277268633246422 + 10.0 * 9.04059886932373
Epoch 2050, val loss: 0.7507636547088623
Epoch 2060, training loss: 90.4234619140625 = 0.02773268148303032 + 10.0 * 9.039572715759277
Epoch 2060, val loss: 0.7544451355934143
Epoch 2070, training loss: 90.42770385742188 = 0.027219047769904137 + 10.0 * 9.040048599243164
Epoch 2070, val loss: 0.7580375075340271
Epoch 2080, training loss: 90.43669128417969 = 0.026695894077420235 + 10.0 * 9.040999412536621
Epoch 2080, val loss: 0.7610718607902527
Epoch 2090, training loss: 90.41751098632812 = 0.026187384501099586 + 10.0 * 9.039133071899414
Epoch 2090, val loss: 0.7636454105377197
Epoch 2100, training loss: 90.4112777709961 = 0.02568996511399746 + 10.0 * 9.038558959960938
Epoch 2100, val loss: 0.7671278119087219
Epoch 2110, training loss: 90.41190338134766 = 0.02520863339304924 + 10.0 * 9.03866958618164
Epoch 2110, val loss: 0.7703769207000732
Epoch 2120, training loss: 90.4378662109375 = 0.024761253967881203 + 10.0 * 9.04131031036377
Epoch 2120, val loss: 0.7729098796844482
Epoch 2130, training loss: 90.41553497314453 = 0.024288661777973175 + 10.0 * 9.039124488830566
Epoch 2130, val loss: 0.7768100500106812
Epoch 2140, training loss: 90.41170501708984 = 0.02384263463318348 + 10.0 * 9.038785934448242
Epoch 2140, val loss: 0.7801156640052795
Epoch 2150, training loss: 90.42425537109375 = 0.023412831127643585 + 10.0 * 9.040083885192871
Epoch 2150, val loss: 0.7835596799850464
Epoch 2160, training loss: 90.41875457763672 = 0.02298898808658123 + 10.0 * 9.039576530456543
Epoch 2160, val loss: 0.7863126993179321
Epoch 2170, training loss: 90.4072265625 = 0.02256709709763527 + 10.0 * 9.03846549987793
Epoch 2170, val loss: 0.7896353602409363
Epoch 2180, training loss: 90.399658203125 = 0.02215936779975891 + 10.0 * 9.037750244140625
Epoch 2180, val loss: 0.7927917242050171
Epoch 2190, training loss: 90.39620971679688 = 0.02175959385931492 + 10.0 * 9.037445068359375
Epoch 2190, val loss: 0.7957649827003479
Epoch 2200, training loss: 90.39715576171875 = 0.021372007206082344 + 10.0 * 9.037578582763672
Epoch 2200, val loss: 0.7989238500595093
Epoch 2210, training loss: 90.40911865234375 = 0.021000416949391365 + 10.0 * 9.038811683654785
Epoch 2210, val loss: 0.8023168444633484
Epoch 2220, training loss: 90.40767669677734 = 0.020628521218895912 + 10.0 * 9.038704872131348
Epoch 2220, val loss: 0.8054324388504028
Epoch 2230, training loss: 90.4146499633789 = 0.02028130739927292 + 10.0 * 9.039437294006348
Epoch 2230, val loss: 0.8086830973625183
Epoch 2240, training loss: 90.39501190185547 = 0.01991286128759384 + 10.0 * 9.03750991821289
Epoch 2240, val loss: 0.8110392093658447
Epoch 2250, training loss: 90.38963317871094 = 0.019570454955101013 + 10.0 * 9.037006378173828
Epoch 2250, val loss: 0.8144885301589966
Epoch 2260, training loss: 90.3938217163086 = 0.019230226054787636 + 10.0 * 9.037458419799805
Epoch 2260, val loss: 0.8175943493843079
Epoch 2270, training loss: 90.38787078857422 = 0.01889665611088276 + 10.0 * 9.036897659301758
Epoch 2270, val loss: 0.8202415704727173
Epoch 2280, training loss: 90.38400268554688 = 0.01857118308544159 + 10.0 * 9.036542892456055
Epoch 2280, val loss: 0.8226329684257507
Epoch 2290, training loss: 90.40760040283203 = 0.01826987974345684 + 10.0 * 9.038932800292969
Epoch 2290, val loss: 0.825426459312439
Epoch 2300, training loss: 90.39433288574219 = 0.017942048609256744 + 10.0 * 9.037638664245605
Epoch 2300, val loss: 0.8289918899536133
Epoch 2310, training loss: 90.38516998291016 = 0.017645692452788353 + 10.0 * 9.036752700805664
Epoch 2310, val loss: 0.8310945630073547
Epoch 2320, training loss: 90.37596130371094 = 0.017342349514365196 + 10.0 * 9.03586196899414
Epoch 2320, val loss: 0.8347197771072388
Epoch 2330, training loss: 90.37519073486328 = 0.017055299133062363 + 10.0 * 9.03581428527832
Epoch 2330, val loss: 0.8371395468711853
Epoch 2340, training loss: 90.39030456542969 = 0.016784152016043663 + 10.0 * 9.037351608276367
Epoch 2340, val loss: 0.8397457003593445
Epoch 2350, training loss: 90.38984680175781 = 0.016521822661161423 + 10.0 * 9.037332534790039
Epoch 2350, val loss: 0.8420639634132385
Epoch 2360, training loss: 90.38333892822266 = 0.01624426245689392 + 10.0 * 9.036709785461426
Epoch 2360, val loss: 0.8453801274299622
Epoch 2370, training loss: 90.37523651123047 = 0.015986375510692596 + 10.0 * 9.035924911499023
Epoch 2370, val loss: 0.84806889295578
Epoch 2380, training loss: 90.37083435058594 = 0.015732476487755775 + 10.0 * 9.035510063171387
Epoch 2380, val loss: 0.8507145643234253
Epoch 2390, training loss: 90.3657455444336 = 0.015481419861316681 + 10.0 * 9.035026550292969
Epoch 2390, val loss: 0.8534113168716431
Epoch 2400, training loss: 90.37145233154297 = 0.015240858308970928 + 10.0 * 9.03562068939209
Epoch 2400, val loss: 0.855984091758728
Epoch 2410, training loss: 90.37442016601562 = 0.015007385052740574 + 10.0 * 9.035941123962402
Epoch 2410, val loss: 0.8585606813430786
Epoch 2420, training loss: 90.37568664550781 = 0.014777516946196556 + 10.0 * 9.036090850830078
Epoch 2420, val loss: 0.8616357445716858
Epoch 2430, training loss: 90.3789291381836 = 0.014553738757967949 + 10.0 * 9.036437034606934
Epoch 2430, val loss: 0.8642722368240356
Epoch 2440, training loss: 90.36283111572266 = 0.01432653609663248 + 10.0 * 9.034850120544434
Epoch 2440, val loss: 0.8667144775390625
Epoch 2450, training loss: 90.3589859008789 = 0.014108582399785519 + 10.0 * 9.0344877243042
Epoch 2450, val loss: 0.8691310882568359
Epoch 2460, training loss: 90.35810089111328 = 0.013897370547056198 + 10.0 * 9.034420013427734
Epoch 2460, val loss: 0.8719200491905212
Epoch 2470, training loss: 90.3760757446289 = 0.013697192072868347 + 10.0 * 9.036237716674805
Epoch 2470, val loss: 0.8745986223220825
Epoch 2480, training loss: 90.35700988769531 = 0.013488195836544037 + 10.0 * 9.03435230255127
Epoch 2480, val loss: 0.8759278655052185
Epoch 2490, training loss: 90.35213470458984 = 0.013286128640174866 + 10.0 * 9.03388500213623
Epoch 2490, val loss: 0.878448486328125
Epoch 2500, training loss: 90.39440155029297 = 0.013113295659422874 + 10.0 * 9.038128852844238
Epoch 2500, val loss: 0.8800884485244751
Epoch 2510, training loss: 90.3614273071289 = 0.012911254540085793 + 10.0 * 9.034852027893066
Epoch 2510, val loss: 0.8840019702911377
Epoch 2520, training loss: 90.35092163085938 = 0.012721565552055836 + 10.0 * 9.033820152282715
Epoch 2520, val loss: 0.8854650259017944
Epoch 2530, training loss: 90.3531265258789 = 0.012538755312561989 + 10.0 * 9.034059524536133
Epoch 2530, val loss: 0.8881031274795532
Epoch 2540, training loss: 90.35225677490234 = 0.01235907431691885 + 10.0 * 9.033989906311035
Epoch 2540, val loss: 0.8905681371688843
Epoch 2550, training loss: 90.3802719116211 = 0.012201846577227116 + 10.0 * 9.0368070602417
Epoch 2550, val loss: 0.893311619758606
Epoch 2560, training loss: 90.3501205444336 = 0.01202055811882019 + 10.0 * 9.033809661865234
Epoch 2560, val loss: 0.8949371576309204
Epoch 2570, training loss: 90.34196472167969 = 0.011847169138491154 + 10.0 * 9.033011436462402
Epoch 2570, val loss: 0.8973199725151062
Epoch 2580, training loss: 90.34452056884766 = 0.011682184413075447 + 10.0 * 9.033284187316895
Epoch 2580, val loss: 0.8998655080795288
Epoch 2590, training loss: 90.36266326904297 = 0.011524145491421223 + 10.0 * 9.035114288330078
Epoch 2590, val loss: 0.9018826484680176
Epoch 2600, training loss: 90.33975219726562 = 0.011364709585905075 + 10.0 * 9.032838821411133
Epoch 2600, val loss: 0.9042550325393677
Epoch 2610, training loss: 90.33495330810547 = 0.011207831092178822 + 10.0 * 9.032374382019043
Epoch 2610, val loss: 0.9058312773704529
Epoch 2620, training loss: 90.3612289428711 = 0.011067735031247139 + 10.0 * 9.035016059875488
Epoch 2620, val loss: 0.908886194229126
Epoch 2630, training loss: 90.3354721069336 = 0.010908572003245354 + 10.0 * 9.032456398010254
Epoch 2630, val loss: 0.910073459148407
Epoch 2640, training loss: 90.3326416015625 = 0.010764828883111477 + 10.0 * 9.032187461853027
Epoch 2640, val loss: 0.9123982787132263
Epoch 2650, training loss: 90.33578491210938 = 0.01062228623777628 + 10.0 * 9.032516479492188
Epoch 2650, val loss: 0.9143913984298706
Epoch 2660, training loss: 90.35637664794922 = 0.010501320473849773 + 10.0 * 9.034587860107422
Epoch 2660, val loss: 0.9170341491699219
Epoch 2670, training loss: 90.33686065673828 = 0.010350966826081276 + 10.0 * 9.0326509475708
Epoch 2670, val loss: 0.9177578687667847
Epoch 2680, training loss: 90.33002471923828 = 0.010218385607004166 + 10.0 * 9.031980514526367
Epoch 2680, val loss: 0.9202480912208557
Epoch 2690, training loss: 90.35269165039062 = 0.010094057768583298 + 10.0 * 9.034259796142578
Epoch 2690, val loss: 0.922617495059967
Epoch 2700, training loss: 90.34898376464844 = 0.009963869117200375 + 10.0 * 9.033902168273926
Epoch 2700, val loss: 0.9230325222015381
Epoch 2710, training loss: 90.33061218261719 = 0.009837579913437366 + 10.0 * 9.03207778930664
Epoch 2710, val loss: 0.926258385181427
Epoch 2720, training loss: 90.32334899902344 = 0.009715771302580833 + 10.0 * 9.031363487243652
Epoch 2720, val loss: 0.9271637797355652
Epoch 2730, training loss: 90.32032775878906 = 0.009594644419848919 + 10.0 * 9.031072616577148
Epoch 2730, val loss: 0.929327130317688
Epoch 2740, training loss: 90.3249740600586 = 0.009480735287070274 + 10.0 * 9.031549453735352
Epoch 2740, val loss: 0.9307299852371216
Epoch 2750, training loss: 90.33979034423828 = 0.009370913729071617 + 10.0 * 9.033041954040527
Epoch 2750, val loss: 0.9326126575469971
Epoch 2760, training loss: 90.32396697998047 = 0.009256250225007534 + 10.0 * 9.031471252441406
Epoch 2760, val loss: 0.9350482225418091
Epoch 2770, training loss: 90.32295989990234 = 0.009147705510258675 + 10.0 * 9.031381607055664
Epoch 2770, val loss: 0.9362629055976868
Epoch 2780, training loss: 90.32645416259766 = 0.00904008001089096 + 10.0 * 9.03174114227295
Epoch 2780, val loss: 0.9383816123008728
Epoch 2790, training loss: 90.32093811035156 = 0.008933908306062222 + 10.0 * 9.031200408935547
Epoch 2790, val loss: 0.9401006102561951
Epoch 2800, training loss: 90.32862091064453 = 0.008833161555230618 + 10.0 * 9.031978607177734
Epoch 2800, val loss: 0.9418004155158997
Epoch 2810, training loss: 90.32547760009766 = 0.008736375719308853 + 10.0 * 9.0316743850708
Epoch 2810, val loss: 0.9440064430236816
Epoch 2820, training loss: 90.31670379638672 = 0.008637568913400173 + 10.0 * 9.030806541442871
Epoch 2820, val loss: 0.9457908272743225
Epoch 2830, training loss: 90.31310272216797 = 0.008539504371583462 + 10.0 * 9.03045654296875
Epoch 2830, val loss: 0.9465258717536926
Epoch 2840, training loss: 90.32582092285156 = 0.008444143459200859 + 10.0 * 9.031737327575684
Epoch 2840, val loss: 0.948721170425415
Epoch 2850, training loss: 90.3129653930664 = 0.008351335301995277 + 10.0 * 9.030461311340332
Epoch 2850, val loss: 0.9498765468597412
Epoch 2860, training loss: 90.31063842773438 = 0.00825701467692852 + 10.0 * 9.030238151550293
Epoch 2860, val loss: 0.951852023601532
Epoch 2870, training loss: 90.31281280517578 = 0.008170274086296558 + 10.0 * 9.030464172363281
Epoch 2870, val loss: 0.9530677199363708
Epoch 2880, training loss: 90.31554412841797 = 0.008083261549472809 + 10.0 * 9.030746459960938
Epoch 2880, val loss: 0.9547024369239807
Epoch 2890, training loss: 90.3180160522461 = 0.00799680594354868 + 10.0 * 9.031002044677734
Epoch 2890, val loss: 0.9567606449127197
Epoch 2900, training loss: 90.30860137939453 = 0.007909497246146202 + 10.0 * 9.030069351196289
Epoch 2900, val loss: 0.9583040475845337
Epoch 2910, training loss: 90.31429290771484 = 0.007831461727619171 + 10.0 * 9.030646324157715
Epoch 2910, val loss: 0.960259735584259
Epoch 2920, training loss: 90.31090545654297 = 0.007745901122689247 + 10.0 * 9.030316352844238
Epoch 2920, val loss: 0.9612862467765808
Epoch 2930, training loss: 90.30669403076172 = 0.007665553130209446 + 10.0 * 9.029902458190918
Epoch 2930, val loss: 0.9627774953842163
Epoch 2940, training loss: 90.30850219726562 = 0.00758954044431448 + 10.0 * 9.030091285705566
Epoch 2940, val loss: 0.9647942185401917
Epoch 2950, training loss: 90.30268859863281 = 0.0075094252824783325 + 10.0 * 9.029518127441406
Epoch 2950, val loss: 0.9662619233131409
Epoch 2960, training loss: 90.3163833618164 = 0.007437973283231258 + 10.0 * 9.03089427947998
Epoch 2960, val loss: 0.968250572681427
Epoch 2970, training loss: 90.30708312988281 = 0.007360681425780058 + 10.0 * 9.029972076416016
Epoch 2970, val loss: 0.9690392017364502
Epoch 2980, training loss: 90.2962875366211 = 0.007284923922270536 + 10.0 * 9.028900146484375
Epoch 2980, val loss: 0.9705681204795837
Epoch 2990, training loss: 90.29251861572266 = 0.007211894728243351 + 10.0 * 9.028531074523926
Epoch 2990, val loss: 0.9720680117607117
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8417
Overall ASR: 0.6968
Flip ASR: 0.6229/1554 nodes
./selected_nodes/Pubmed/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 104.69416809082031 = 1.1030980348587036 + 10.0 * 10.35910701751709
Epoch 0, val loss: 1.10097336769104
Epoch 10, training loss: 104.65621948242188 = 1.0912421941757202 + 10.0 * 10.356497764587402
Epoch 10, val loss: 1.089013934135437
Epoch 20, training loss: 104.1634292602539 = 1.0750542879104614 + 10.0 * 10.308836936950684
Epoch 20, val loss: 1.0729559659957886
Epoch 30, training loss: 100.0460205078125 = 1.0586262941360474 + 10.0 * 9.8987398147583
Epoch 30, val loss: 1.0571084022521973
Epoch 40, training loss: 96.89777374267578 = 1.0417550802230835 + 10.0 * 9.585601806640625
Epoch 40, val loss: 1.0405545234680176
Epoch 50, training loss: 95.68627166748047 = 1.0229874849319458 + 10.0 * 9.466328620910645
Epoch 50, val loss: 1.0217435359954834
Epoch 60, training loss: 94.89285278320312 = 1.0051023960113525 + 10.0 * 9.388774871826172
Epoch 60, val loss: 1.0043456554412842
Epoch 70, training loss: 94.67219543457031 = 0.9869759678840637 + 10.0 * 9.368521690368652
Epoch 70, val loss: 0.9863657355308533
Epoch 80, training loss: 94.3048095703125 = 0.9675654768943787 + 10.0 * 9.333724021911621
Epoch 80, val loss: 0.9672405123710632
Epoch 90, training loss: 93.8234634399414 = 0.9484967589378357 + 10.0 * 9.287496566772461
Epoch 90, val loss: 0.9487824440002441
Epoch 100, training loss: 93.27903747558594 = 0.9294902086257935 + 10.0 * 9.234954833984375
Epoch 100, val loss: 0.9301744699478149
Epoch 110, training loss: 92.97509002685547 = 0.9059072732925415 + 10.0 * 9.206918716430664
Epoch 110, val loss: 0.9067603945732117
Epoch 120, training loss: 92.77935791015625 = 0.8756012320518494 + 10.0 * 9.190375328063965
Epoch 120, val loss: 0.8768903613090515
Epoch 130, training loss: 92.63436889648438 = 0.841008186340332 + 10.0 * 9.179335594177246
Epoch 130, val loss: 0.8431636095046997
Epoch 140, training loss: 92.48619079589844 = 0.803138017654419 + 10.0 * 9.168305397033691
Epoch 140, val loss: 0.8066583275794983
Epoch 150, training loss: 92.34273529052734 = 0.7632364630699158 + 10.0 * 9.157949447631836
Epoch 150, val loss: 0.7686856985092163
Epoch 160, training loss: 92.2171401977539 = 0.722919762134552 + 10.0 * 9.149421691894531
Epoch 160, val loss: 0.730819046497345
Epoch 170, training loss: 92.10127258300781 = 0.6830880641937256 + 10.0 * 9.141818046569824
Epoch 170, val loss: 0.6938396096229553
Epoch 180, training loss: 91.99732971191406 = 0.6448691487312317 + 10.0 * 9.135246276855469
Epoch 180, val loss: 0.6588199138641357
Epoch 190, training loss: 91.94047546386719 = 0.6095868945121765 + 10.0 * 9.133089065551758
Epoch 190, val loss: 0.6268150210380554
Epoch 200, training loss: 91.84673309326172 = 0.5779466032981873 + 10.0 * 9.12687873840332
Epoch 200, val loss: 0.5987266898155212
Epoch 210, training loss: 91.7815933227539 = 0.5498690009117126 + 10.0 * 9.123172760009766
Epoch 210, val loss: 0.5739752650260925
Epoch 220, training loss: 91.73272705078125 = 0.5249075889587402 + 10.0 * 9.120781898498535
Epoch 220, val loss: 0.5523026585578918
Epoch 230, training loss: 91.69258117675781 = 0.5028550028800964 + 10.0 * 9.118972778320312
Epoch 230, val loss: 0.5334960222244263
Epoch 240, training loss: 91.6749038696289 = 0.48353007435798645 + 10.0 * 9.11913776397705
Epoch 240, val loss: 0.5173739790916443
Epoch 250, training loss: 91.63277435302734 = 0.4667607247829437 + 10.0 * 9.116601943969727
Epoch 250, val loss: 0.5037307739257812
Epoch 260, training loss: 91.60003662109375 = 0.452181339263916 + 10.0 * 9.114786148071289
Epoch 260, val loss: 0.4921081066131592
Epoch 270, training loss: 91.56817626953125 = 0.4392997920513153 + 10.0 * 9.112887382507324
Epoch 270, val loss: 0.4821895658969879
Epoch 280, training loss: 91.53898620605469 = 0.42773693799972534 + 10.0 * 9.111124992370605
Epoch 280, val loss: 0.47357162833213806
Epoch 290, training loss: 91.53608703613281 = 0.4172544479370117 + 10.0 * 9.111883163452148
Epoch 290, val loss: 0.46613991260528564
Epoch 300, training loss: 91.50358581542969 = 0.407763808965683 + 10.0 * 9.109582901000977
Epoch 300, val loss: 0.4595155715942383
Epoch 310, training loss: 91.45509338378906 = 0.39926406741142273 + 10.0 * 9.105583190917969
Epoch 310, val loss: 0.45376619696617126
Epoch 320, training loss: 91.41912078857422 = 0.3915175497531891 + 10.0 * 9.102760314941406
Epoch 320, val loss: 0.44891685247421265
Epoch 330, training loss: 91.38935089111328 = 0.38428860902786255 + 10.0 * 9.100506782531738
Epoch 330, val loss: 0.4444670081138611
Epoch 340, training loss: 91.39268493652344 = 0.37748464941978455 + 10.0 * 9.101519584655762
Epoch 340, val loss: 0.4405493140220642
Epoch 350, training loss: 91.33415222167969 = 0.3711545765399933 + 10.0 * 9.09630012512207
Epoch 350, val loss: 0.4369140565395355
Epoch 360, training loss: 91.31143951416016 = 0.36531370878219604 + 10.0 * 9.094613075256348
Epoch 360, val loss: 0.4338797330856323
Epoch 370, training loss: 91.29085540771484 = 0.3598099648952484 + 10.0 * 9.093104362487793
Epoch 370, val loss: 0.43104055523872375
Epoch 380, training loss: 91.2714614868164 = 0.35457196831703186 + 10.0 * 9.091689109802246
Epoch 380, val loss: 0.4285818338394165
Epoch 390, training loss: 91.24800109863281 = 0.34964489936828613 + 10.0 * 9.089835166931152
Epoch 390, val loss: 0.42628222703933716
Epoch 400, training loss: 91.22786712646484 = 0.34500664472579956 + 10.0 * 9.088285446166992
Epoch 400, val loss: 0.42420870065689087
Epoch 410, training loss: 91.22517395019531 = 0.3405841290950775 + 10.0 * 9.088459014892578
Epoch 410, val loss: 0.4223534166812897
Epoch 420, training loss: 91.20466613769531 = 0.3363298177719116 + 10.0 * 9.086833953857422
Epoch 420, val loss: 0.42064276337623596
Epoch 430, training loss: 91.17758178710938 = 0.33235540986061096 + 10.0 * 9.08452320098877
Epoch 430, val loss: 0.4189271032810211
Epoch 440, training loss: 91.15972900390625 = 0.328610897064209 + 10.0 * 9.083111763000488
Epoch 440, val loss: 0.4175797700881958
Epoch 450, training loss: 91.14331817626953 = 0.3249772787094116 + 10.0 * 9.08183479309082
Epoch 450, val loss: 0.4161823093891144
Epoch 460, training loss: 91.12847137451172 = 0.32142409682273865 + 10.0 * 9.080704689025879
Epoch 460, val loss: 0.41493141651153564
Epoch 470, training loss: 91.11992645263672 = 0.3179566562175751 + 10.0 * 9.08019733428955
Epoch 470, val loss: 0.413723886013031
Epoch 480, training loss: 91.11609649658203 = 0.3145677447319031 + 10.0 * 9.08015251159668
Epoch 480, val loss: 0.41261252760887146
Epoch 490, training loss: 91.10265350341797 = 0.3113649785518646 + 10.0 * 9.079129219055176
Epoch 490, val loss: 0.411599725484848
Epoch 500, training loss: 91.08318328857422 = 0.3083087205886841 + 10.0 * 9.077486991882324
Epoch 500, val loss: 0.41068488359451294
Epoch 510, training loss: 91.06835174560547 = 0.3053246736526489 + 10.0 * 9.076302528381348
Epoch 510, val loss: 0.4097965657711029
Epoch 520, training loss: 91.05533599853516 = 0.3023759722709656 + 10.0 * 9.075296401977539
Epoch 520, val loss: 0.4090336263179779
Epoch 530, training loss: 91.04520416259766 = 0.2994721233844757 + 10.0 * 9.074573516845703
Epoch 530, val loss: 0.40832018852233887
Epoch 540, training loss: 91.03370666503906 = 0.2966171205043793 + 10.0 * 9.073709487915039
Epoch 540, val loss: 0.4076317846775055
Epoch 550, training loss: 91.03141784667969 = 0.2938563823699951 + 10.0 * 9.073756217956543
Epoch 550, val loss: 0.4070639908313751
Epoch 560, training loss: 91.01741790771484 = 0.2911909222602844 + 10.0 * 9.072622299194336
Epoch 560, val loss: 0.406559556722641
Epoch 570, training loss: 91.00244903564453 = 0.2885739505290985 + 10.0 * 9.07138729095459
Epoch 570, val loss: 0.4061073958873749
Epoch 580, training loss: 90.99199676513672 = 0.28598785400390625 + 10.0 * 9.070600509643555
Epoch 580, val loss: 0.40572965145111084
Epoch 590, training loss: 90.98463439941406 = 0.2834322154521942 + 10.0 * 9.070119857788086
Epoch 590, val loss: 0.40538474917411804
Epoch 600, training loss: 91.001708984375 = 0.28091704845428467 + 10.0 * 9.0720796585083
Epoch 600, val loss: 0.40513649582862854
Epoch 610, training loss: 90.98075866699219 = 0.27847641706466675 + 10.0 * 9.070228576660156
Epoch 610, val loss: 0.40493157505989075
Epoch 620, training loss: 90.95479583740234 = 0.2761145532131195 + 10.0 * 9.06786823272705
Epoch 620, val loss: 0.40469539165496826
Epoch 630, training loss: 90.94630432128906 = 0.27378126978874207 + 10.0 * 9.067252159118652
Epoch 630, val loss: 0.40459251403808594
Epoch 640, training loss: 90.93803405761719 = 0.2714655101299286 + 10.0 * 9.066656112670898
Epoch 640, val loss: 0.40459156036376953
Epoch 650, training loss: 90.94334411621094 = 0.2691730856895447 + 10.0 * 9.06741714477539
Epoch 650, val loss: 0.40462344884872437
Epoch 660, training loss: 90.92414093017578 = 0.2669305205345154 + 10.0 * 9.06572151184082
Epoch 660, val loss: 0.40465646982192993
Epoch 670, training loss: 90.91006469726562 = 0.2647361755371094 + 10.0 * 9.064533233642578
Epoch 670, val loss: 0.4046701192855835
Epoch 680, training loss: 90.90296173095703 = 0.26256245374679565 + 10.0 * 9.06403923034668
Epoch 680, val loss: 0.404778391122818
Epoch 690, training loss: 90.89234161376953 = 0.26039835810661316 + 10.0 * 9.063194274902344
Epoch 690, val loss: 0.405081182718277
Epoch 700, training loss: 90.88968658447266 = 0.2582756578922272 + 10.0 * 9.063140869140625
Epoch 700, val loss: 0.40523192286491394
Epoch 710, training loss: 90.87899017333984 = 0.2561854422092438 + 10.0 * 9.062280654907227
Epoch 710, val loss: 0.4054633378982544
Epoch 720, training loss: 90.86749267578125 = 0.2541061341762543 + 10.0 * 9.061338424682617
Epoch 720, val loss: 0.405764639377594
Epoch 730, training loss: 90.8805923461914 = 0.25203773379325867 + 10.0 * 9.06285572052002
Epoch 730, val loss: 0.4060996472835541
Epoch 740, training loss: 90.88526916503906 = 0.24998149275779724 + 10.0 * 9.063528060913086
Epoch 740, val loss: 0.40658318996429443
Epoch 750, training loss: 90.8563461303711 = 0.24797417223453522 + 10.0 * 9.060837745666504
Epoch 750, val loss: 0.4068930149078369
Epoch 760, training loss: 90.84272003173828 = 0.2459879368543625 + 10.0 * 9.059673309326172
Epoch 760, val loss: 0.4072949290275574
Epoch 770, training loss: 90.83882904052734 = 0.24400344491004944 + 10.0 * 9.05948257446289
Epoch 770, val loss: 0.40774330496788025
Epoch 780, training loss: 90.82735443115234 = 0.2420225888490677 + 10.0 * 9.058533668518066
Epoch 780, val loss: 0.40827545523643494
Epoch 790, training loss: 90.82691955566406 = 0.240073561668396 + 10.0 * 9.058684349060059
Epoch 790, val loss: 0.40873682498931885
Epoch 800, training loss: 90.81483459472656 = 0.2381388396024704 + 10.0 * 9.057669639587402
Epoch 800, val loss: 0.4092709422111511
Epoch 810, training loss: 90.80963897705078 = 0.2362075299024582 + 10.0 * 9.057343482971191
Epoch 810, val loss: 0.4098946750164032
Epoch 820, training loss: 90.81846618652344 = 0.2342836856842041 + 10.0 * 9.058418273925781
Epoch 820, val loss: 0.4105578362941742
Epoch 830, training loss: 90.79461669921875 = 0.23237378895282745 + 10.0 * 9.05622386932373
Epoch 830, val loss: 0.4111611247062683
Epoch 840, training loss: 90.79496765136719 = 0.23048259317874908 + 10.0 * 9.056447982788086
Epoch 840, val loss: 0.41178905963897705
Epoch 850, training loss: 90.7862319946289 = 0.22859834134578705 + 10.0 * 9.055763244628906
Epoch 850, val loss: 0.4125383794307709
Epoch 860, training loss: 90.78358459472656 = 0.22673210501670837 + 10.0 * 9.055685043334961
Epoch 860, val loss: 0.41326668858528137
Epoch 870, training loss: 90.7745132446289 = 0.22487559914588928 + 10.0 * 9.054964065551758
Epoch 870, val loss: 0.4139708876609802
Epoch 880, training loss: 90.76726531982422 = 0.2230154573917389 + 10.0 * 9.054425239562988
Epoch 880, val loss: 0.41476038098335266
Epoch 890, training loss: 90.77562713623047 = 0.2211506962776184 + 10.0 * 9.055447578430176
Epoch 890, val loss: 0.4156738519668579
Epoch 900, training loss: 90.76791381835938 = 0.21929527819156647 + 10.0 * 9.054862022399902
Epoch 900, val loss: 0.416401207447052
Epoch 910, training loss: 90.75302124023438 = 0.21744786202907562 + 10.0 * 9.053557395935059
Epoch 910, val loss: 0.4172981381416321
Epoch 920, training loss: 90.74921417236328 = 0.21561287343502045 + 10.0 * 9.053359985351562
Epoch 920, val loss: 0.4182112514972687
Epoch 930, training loss: 90.75630187988281 = 0.21377885341644287 + 10.0 * 9.054252624511719
Epoch 930, val loss: 0.419171541929245
Epoch 940, training loss: 90.73765563964844 = 0.21194304525852203 + 10.0 * 9.052571296691895
Epoch 940, val loss: 0.4200494587421417
Epoch 950, training loss: 90.75230407714844 = 0.2101106494665146 + 10.0 * 9.054219245910645
Epoch 950, val loss: 0.42110177874565125
Epoch 960, training loss: 90.73226928710938 = 0.20828257501125336 + 10.0 * 9.052398681640625
Epoch 960, val loss: 0.42198649048805237
Epoch 970, training loss: 90.72371673583984 = 0.20645950734615326 + 10.0 * 9.051725387573242
Epoch 970, val loss: 0.4231114387512207
Epoch 980, training loss: 90.7164535522461 = 0.20463399589061737 + 10.0 * 9.05118179321289
Epoch 980, val loss: 0.42413365840911865
Epoch 990, training loss: 90.72166442871094 = 0.2028079628944397 + 10.0 * 9.051885604858398
Epoch 990, val loss: 0.42532432079315186
Epoch 1000, training loss: 90.71803283691406 = 0.2009839117527008 + 10.0 * 9.051705360412598
Epoch 1000, val loss: 0.4263438284397125
Epoch 1010, training loss: 90.70309448242188 = 0.19917109608650208 + 10.0 * 9.050392150878906
Epoch 1010, val loss: 0.4275316894054413
Epoch 1020, training loss: 90.69929504394531 = 0.19735963642597198 + 10.0 * 9.050193786621094
Epoch 1020, val loss: 0.4287106394767761
Epoch 1030, training loss: 90.69365692138672 = 0.1955432891845703 + 10.0 * 9.049811363220215
Epoch 1030, val loss: 0.4299403131008148
Epoch 1040, training loss: 90.71439361572266 = 0.19372427463531494 + 10.0 * 9.052066802978516
Epoch 1040, val loss: 0.4312448799610138
Epoch 1050, training loss: 90.69477844238281 = 0.19189752638339996 + 10.0 * 9.050288200378418
Epoch 1050, val loss: 0.43242746591567993
Epoch 1060, training loss: 90.68386840820312 = 0.19007959961891174 + 10.0 * 9.049379348754883
Epoch 1060, val loss: 0.43380776047706604
Epoch 1070, training loss: 90.67790985107422 = 0.18825680017471313 + 10.0 * 9.048965454101562
Epoch 1070, val loss: 0.43517300486564636
Epoch 1080, training loss: 90.6871566772461 = 0.18643246591091156 + 10.0 * 9.05007266998291
Epoch 1080, val loss: 0.43660715222358704
Epoch 1090, training loss: 90.68414306640625 = 0.18460676074028015 + 10.0 * 9.04995346069336
Epoch 1090, val loss: 0.43798819184303284
Epoch 1100, training loss: 90.66021728515625 = 0.18277955055236816 + 10.0 * 9.047743797302246
Epoch 1100, val loss: 0.43947112560272217
Epoch 1110, training loss: 90.6574935913086 = 0.18095125257968903 + 10.0 * 9.047654151916504
Epoch 1110, val loss: 0.4410065710544586
Epoch 1120, training loss: 90.6666488647461 = 0.17911508679389954 + 10.0 * 9.04875373840332
Epoch 1120, val loss: 0.44265833497047424
Epoch 1130, training loss: 90.65689849853516 = 0.17728060483932495 + 10.0 * 9.047961235046387
Epoch 1130, val loss: 0.444096177816391
Epoch 1140, training loss: 90.65072631835938 = 0.1754472404718399 + 10.0 * 9.047528266906738
Epoch 1140, val loss: 0.4458908140659332
Epoch 1150, training loss: 90.63947296142578 = 0.17361235618591309 + 10.0 * 9.046586036682129
Epoch 1150, val loss: 0.44755837321281433
Epoch 1160, training loss: 90.63497924804688 = 0.17177000641822815 + 10.0 * 9.046320915222168
Epoch 1160, val loss: 0.44927334785461426
Epoch 1170, training loss: 90.63722229003906 = 0.1699182689189911 + 10.0 * 9.046730041503906
Epoch 1170, val loss: 0.4511550962924957
Epoch 1180, training loss: 90.63026428222656 = 0.16806887090206146 + 10.0 * 9.046219825744629
Epoch 1180, val loss: 0.4529120624065399
Epoch 1190, training loss: 90.62712860107422 = 0.1662253588438034 + 10.0 * 9.046091079711914
Epoch 1190, val loss: 0.45484843850135803
Epoch 1200, training loss: 90.62458038330078 = 0.16438037157058716 + 10.0 * 9.046019554138184
Epoch 1200, val loss: 0.45668885111808777
Epoch 1210, training loss: 90.62693786621094 = 0.16252829134464264 + 10.0 * 9.046441078186035
Epoch 1210, val loss: 0.45866701006889343
Epoch 1220, training loss: 90.61519622802734 = 0.16067110002040863 + 10.0 * 9.045453071594238
Epoch 1220, val loss: 0.46077388525009155
Epoch 1230, training loss: 90.60690307617188 = 0.1588103026151657 + 10.0 * 9.044809341430664
Epoch 1230, val loss: 0.46281346678733826
Epoch 1240, training loss: 90.62528991699219 = 0.15695177018642426 + 10.0 * 9.046833992004395
Epoch 1240, val loss: 0.4649432599544525
Epoch 1250, training loss: 90.61114501953125 = 0.1550872027873993 + 10.0 * 9.045605659484863
Epoch 1250, val loss: 0.46720126271247864
Epoch 1260, training loss: 90.5958023071289 = 0.15322299301624298 + 10.0 * 9.044258117675781
Epoch 1260, val loss: 0.4693886339664459
Epoch 1270, training loss: 90.5890121459961 = 0.15135988593101501 + 10.0 * 9.0437650680542
Epoch 1270, val loss: 0.47165432572364807
Epoch 1280, training loss: 90.59349060058594 = 0.1494910567998886 + 10.0 * 9.044400215148926
Epoch 1280, val loss: 0.47398626804351807
Epoch 1290, training loss: 90.58645629882812 = 0.1476152390241623 + 10.0 * 9.04388427734375
Epoch 1290, val loss: 0.47640520334243774
Epoch 1300, training loss: 90.58999633789062 = 0.1457502543926239 + 10.0 * 9.044424057006836
Epoch 1300, val loss: 0.4787989556789398
Epoch 1310, training loss: 90.5772705078125 = 0.1438785046339035 + 10.0 * 9.043339729309082
Epoch 1310, val loss: 0.481326699256897
Epoch 1320, training loss: 90.57273864746094 = 0.14200039207935333 + 10.0 * 9.043073654174805
Epoch 1320, val loss: 0.4838669002056122
Epoch 1330, training loss: 90.58261108398438 = 0.14012040197849274 + 10.0 * 9.044248580932617
Epoch 1330, val loss: 0.48643946647644043
Epoch 1340, training loss: 90.57161712646484 = 0.1382317841053009 + 10.0 * 9.043338775634766
Epoch 1340, val loss: 0.4889293611049652
Epoch 1350, training loss: 90.56240844726562 = 0.1363483965396881 + 10.0 * 9.042606353759766
Epoch 1350, val loss: 0.49175500869750977
Epoch 1360, training loss: 90.55766296386719 = 0.1344626247882843 + 10.0 * 9.042320251464844
Epoch 1360, val loss: 0.49445250630378723
Epoch 1370, training loss: 90.56454467773438 = 0.132584810256958 + 10.0 * 9.043195724487305
Epoch 1370, val loss: 0.4972275495529175
Epoch 1380, training loss: 90.55535125732422 = 0.13070186972618103 + 10.0 * 9.042465209960938
Epoch 1380, val loss: 0.5002870559692383
Epoch 1390, training loss: 90.54469299316406 = 0.1288180947303772 + 10.0 * 9.041587829589844
Epoch 1390, val loss: 0.5030173659324646
Epoch 1400, training loss: 90.54219818115234 = 0.12693838775157928 + 10.0 * 9.041525840759277
Epoch 1400, val loss: 0.5060484409332275
Epoch 1410, training loss: 90.54586029052734 = 0.12505899369716644 + 10.0 * 9.04207992553711
Epoch 1410, val loss: 0.5091344714164734
Epoch 1420, training loss: 90.53431701660156 = 0.12317846715450287 + 10.0 * 9.04111385345459
Epoch 1420, val loss: 0.5122858881950378
Epoch 1430, training loss: 90.5299301147461 = 0.12130448967218399 + 10.0 * 9.040862083435059
Epoch 1430, val loss: 0.515425443649292
Epoch 1440, training loss: 90.57386779785156 = 0.11945834010839462 + 10.0 * 9.045440673828125
Epoch 1440, val loss: 0.5186278223991394
Epoch 1450, training loss: 90.52232360839844 = 0.11759433150291443 + 10.0 * 9.040472984313965
Epoch 1450, val loss: 0.5220161080360413
Epoch 1460, training loss: 90.52043914794922 = 0.11575916409492493 + 10.0 * 9.040468215942383
Epoch 1460, val loss: 0.5252065658569336
Epoch 1470, training loss: 90.51094818115234 = 0.11393056064844131 + 10.0 * 9.039701461791992
Epoch 1470, val loss: 0.528591513633728
Epoch 1480, training loss: 90.51350402832031 = 0.11211910098791122 + 10.0 * 9.040138244628906
Epoch 1480, val loss: 0.5320017337799072
Epoch 1490, training loss: 90.51972198486328 = 0.11031704396009445 + 10.0 * 9.04094123840332
Epoch 1490, val loss: 0.5356025695800781
Epoch 1500, training loss: 90.50586700439453 = 0.10852298140525818 + 10.0 * 9.039734840393066
Epoch 1500, val loss: 0.5393903255462646
Epoch 1510, training loss: 90.50392150878906 = 0.10674557089805603 + 10.0 * 9.039717674255371
Epoch 1510, val loss: 0.5430788397789001
Epoch 1520, training loss: 90.5027084350586 = 0.10497892647981644 + 10.0 * 9.039772987365723
Epoch 1520, val loss: 0.5465404987335205
Epoch 1530, training loss: 90.4940185546875 = 0.1032223328948021 + 10.0 * 9.039079666137695
Epoch 1530, val loss: 0.5503641366958618
Epoch 1540, training loss: 90.49213409423828 = 0.10148057341575623 + 10.0 * 9.03906536102295
Epoch 1540, val loss: 0.5542329549789429
Epoch 1550, training loss: 90.5033187866211 = 0.09975378215312958 + 10.0 * 9.040356636047363
Epoch 1550, val loss: 0.5581597685813904
Epoch 1560, training loss: 90.49266815185547 = 0.09803904592990875 + 10.0 * 9.03946304321289
Epoch 1560, val loss: 0.5619555711746216
Epoch 1570, training loss: 90.47624206542969 = 0.09633173793554306 + 10.0 * 9.037991523742676
Epoch 1570, val loss: 0.5660403966903687
Epoch 1580, training loss: 90.47283935546875 = 0.09464332461357117 + 10.0 * 9.037819862365723
Epoch 1580, val loss: 0.5702270269393921
Epoch 1590, training loss: 90.47223663330078 = 0.09297160059213638 + 10.0 * 9.03792667388916
Epoch 1590, val loss: 0.5742992162704468
Epoch 1600, training loss: 90.48497009277344 = 0.09132619947195053 + 10.0 * 9.0393648147583
Epoch 1600, val loss: 0.5785186290740967
Epoch 1610, training loss: 90.48435974121094 = 0.08968690037727356 + 10.0 * 9.039467811584473
Epoch 1610, val loss: 0.5823858976364136
Epoch 1620, training loss: 90.47732543945312 = 0.08807601034641266 + 10.0 * 9.038925170898438
Epoch 1620, val loss: 0.5869019627571106
Epoch 1630, training loss: 90.46051025390625 = 0.08647213876247406 + 10.0 * 9.03740406036377
Epoch 1630, val loss: 0.5908887386322021
Epoch 1640, training loss: 90.45577239990234 = 0.0848969966173172 + 10.0 * 9.037087440490723
Epoch 1640, val loss: 0.5952883958816528
Epoch 1650, training loss: 90.45591735839844 = 0.08333799988031387 + 10.0 * 9.03725814819336
Epoch 1650, val loss: 0.599663257598877
Epoch 1660, training loss: 90.45830535888672 = 0.0818016305565834 + 10.0 * 9.037650108337402
Epoch 1660, val loss: 0.6039384603500366
Epoch 1670, training loss: 90.44671630859375 = 0.08027831465005875 + 10.0 * 9.036643981933594
Epoch 1670, val loss: 0.6087418794631958
Epoch 1680, training loss: 90.43934631347656 = 0.07877513021230698 + 10.0 * 9.036057472229004
Epoch 1680, val loss: 0.6132033467292786
Epoch 1690, training loss: 90.44194793701172 = 0.07729922235012054 + 10.0 * 9.03646469116211
Epoch 1690, val loss: 0.617684006690979
Epoch 1700, training loss: 90.45023345947266 = 0.07584267109632492 + 10.0 * 9.037439346313477
Epoch 1700, val loss: 0.622302770614624
Epoch 1710, training loss: 90.4319839477539 = 0.0743955448269844 + 10.0 * 9.035758972167969
Epoch 1710, val loss: 0.6271151304244995
Epoch 1720, training loss: 90.42662048339844 = 0.07297057658433914 + 10.0 * 9.035365104675293
Epoch 1720, val loss: 0.6317082047462463
Epoch 1730, training loss: 90.42344665527344 = 0.0715654194355011 + 10.0 * 9.035188674926758
Epoch 1730, val loss: 0.6364173293113708
Epoch 1740, training loss: 90.44513702392578 = 0.07019613683223724 + 10.0 * 9.037493705749512
Epoch 1740, val loss: 0.641114354133606
Epoch 1750, training loss: 90.4306869506836 = 0.06882962584495544 + 10.0 * 9.036185264587402
Epoch 1750, val loss: 0.6456332206726074
Epoch 1760, training loss: 90.42466735839844 = 0.06749048829078674 + 10.0 * 9.035717964172363
Epoch 1760, val loss: 0.6504489779472351
Epoch 1770, training loss: 90.41934204101562 = 0.0661710873246193 + 10.0 * 9.035317420959473
Epoch 1770, val loss: 0.6552065014839172
Epoch 1780, training loss: 90.4106674194336 = 0.06486698985099792 + 10.0 * 9.03458023071289
Epoch 1780, val loss: 0.6600803136825562
Epoch 1790, training loss: 90.41596221923828 = 0.06359211355447769 + 10.0 * 9.035237312316895
Epoch 1790, val loss: 0.6650925874710083
Epoch 1800, training loss: 90.4332504272461 = 0.062339771538972855 + 10.0 * 9.037091255187988
Epoch 1800, val loss: 0.6696217656135559
Epoch 1810, training loss: 90.40873718261719 = 0.06109454855322838 + 10.0 * 9.034764289855957
Epoch 1810, val loss: 0.6744930744171143
Epoch 1820, training loss: 90.40216827392578 = 0.059876032173633575 + 10.0 * 9.034229278564453
Epoch 1820, val loss: 0.679132878780365
Epoch 1830, training loss: 90.39542388916016 = 0.05867520719766617 + 10.0 * 9.033674240112305
Epoch 1830, val loss: 0.6840150952339172
Epoch 1840, training loss: 90.39373779296875 = 0.057499274611473083 + 10.0 * 9.033623695373535
Epoch 1840, val loss: 0.6888275146484375
Epoch 1850, training loss: 90.43141174316406 = 0.056360531598329544 + 10.0 * 9.037505149841309
Epoch 1850, val loss: 0.6937078833580017
Epoch 1860, training loss: 90.40341186523438 = 0.055213358253240585 + 10.0 * 9.034819602966309
Epoch 1860, val loss: 0.6989645957946777
Epoch 1870, training loss: 90.38519287109375 = 0.054088521748781204 + 10.0 * 9.033110618591309
Epoch 1870, val loss: 0.7036083340644836
Epoch 1880, training loss: 90.3836441040039 = 0.05299082025885582 + 10.0 * 9.033064842224121
Epoch 1880, val loss: 0.7085464000701904
Epoch 1890, training loss: 90.38844299316406 = 0.05191691219806671 + 10.0 * 9.033652305603027
Epoch 1890, val loss: 0.7132672667503357
Epoch 1900, training loss: 90.39188385009766 = 0.0508626326918602 + 10.0 * 9.034101486206055
Epoch 1900, val loss: 0.7181177735328674
Epoch 1910, training loss: 90.3803939819336 = 0.04982075095176697 + 10.0 * 9.03305721282959
Epoch 1910, val loss: 0.7234726548194885
Epoch 1920, training loss: 90.37438201904297 = 0.04880065470933914 + 10.0 * 9.03255844116211
Epoch 1920, val loss: 0.7284968495368958
Epoch 1930, training loss: 90.37226867675781 = 0.047801315784454346 + 10.0 * 9.03244686126709
Epoch 1930, val loss: 0.7332326769828796
Epoch 1940, training loss: 90.38603210449219 = 0.04682685807347298 + 10.0 * 9.033920288085938
Epoch 1940, val loss: 0.738166868686676
Epoch 1950, training loss: 90.37698364257812 = 0.04586784914135933 + 10.0 * 9.033111572265625
Epoch 1950, val loss: 0.7429437041282654
Epoch 1960, training loss: 90.36701202392578 = 0.04492738097906113 + 10.0 * 9.032208442687988
Epoch 1960, val loss: 0.7479415535926819
Epoch 1970, training loss: 90.37020111083984 = 0.04401026666164398 + 10.0 * 9.032618522644043
Epoch 1970, val loss: 0.753506064414978
Epoch 1980, training loss: 90.36489868164062 = 0.04310537502169609 + 10.0 * 9.03217887878418
Epoch 1980, val loss: 0.7579636573791504
Epoch 1990, training loss: 90.3583755493164 = 0.042223185300827026 + 10.0 * 9.031615257263184
Epoch 1990, val loss: 0.7628676295280457
Epoch 2000, training loss: 90.35859680175781 = 0.041360314935445786 + 10.0 * 9.031723976135254
Epoch 2000, val loss: 0.7678326964378357
Epoch 2010, training loss: 90.3680191040039 = 0.04051820933818817 + 10.0 * 9.032750129699707
Epoch 2010, val loss: 0.772400975227356
Epoch 2020, training loss: 90.36438751220703 = 0.039693161845207214 + 10.0 * 9.032468795776367
Epoch 2020, val loss: 0.7774659991264343
Epoch 2030, training loss: 90.3506088256836 = 0.038879334926605225 + 10.0 * 9.031172752380371
Epoch 2030, val loss: 0.7825367450714111
Epoch 2040, training loss: 90.34695434570312 = 0.03808673471212387 + 10.0 * 9.03088665008545
Epoch 2040, val loss: 0.7874423861503601
Epoch 2050, training loss: 90.35770416259766 = 0.03731875121593475 + 10.0 * 9.032038688659668
Epoch 2050, val loss: 0.791914165019989
Epoch 2060, training loss: 90.3429946899414 = 0.036555733531713486 + 10.0 * 9.030644416809082
Epoch 2060, val loss: 0.7968505024909973
Epoch 2070, training loss: 90.33909606933594 = 0.035811882466077805 + 10.0 * 9.030328750610352
Epoch 2070, val loss: 0.8014765381813049
Epoch 2080, training loss: 90.34080505371094 = 0.035089172422885895 + 10.0 * 9.030571937561035
Epoch 2080, val loss: 0.8065119981765747
Epoch 2090, training loss: 90.34864807128906 = 0.03438369184732437 + 10.0 * 9.031426429748535
Epoch 2090, val loss: 0.8110575079917908
Epoch 2100, training loss: 90.34185028076172 = 0.033692073076963425 + 10.0 * 9.030816078186035
Epoch 2100, val loss: 0.8158459067344666
Epoch 2110, training loss: 90.34212493896484 = 0.03302158787846565 + 10.0 * 9.03091049194336
Epoch 2110, val loss: 0.8206520080566406
Epoch 2120, training loss: 90.33610534667969 = 0.03236371651291847 + 10.0 * 9.030374526977539
Epoch 2120, val loss: 0.8247256278991699
Epoch 2130, training loss: 90.33816528320312 = 0.031722381711006165 + 10.0 * 9.030644416809082
Epoch 2130, val loss: 0.8293930292129517
Epoch 2140, training loss: 90.3260269165039 = 0.031092189252376556 + 10.0 * 9.02949333190918
Epoch 2140, val loss: 0.8344506025314331
Epoch 2150, training loss: 90.33560180664062 = 0.030486170202493668 + 10.0 * 9.030511856079102
Epoch 2150, val loss: 0.8393105864524841
Epoch 2160, training loss: 90.32807159423828 = 0.02988688461482525 + 10.0 * 9.029818534851074
Epoch 2160, val loss: 0.8432560563087463
Epoch 2170, training loss: 90.3194351196289 = 0.029298176988959312 + 10.0 * 9.029013633728027
Epoch 2170, val loss: 0.8479575514793396
Epoch 2180, training loss: 90.31837463378906 = 0.028727909550070763 + 10.0 * 9.02896499633789
Epoch 2180, val loss: 0.8520315289497375
Epoch 2190, training loss: 90.3316421508789 = 0.028176825493574142 + 10.0 * 9.030346870422363
Epoch 2190, val loss: 0.8564180135726929
Epoch 2200, training loss: 90.3141098022461 = 0.02762487158179283 + 10.0 * 9.028648376464844
Epoch 2200, val loss: 0.8611652255058289
Epoch 2210, training loss: 90.31838989257812 = 0.027095424011349678 + 10.0 * 9.029129981994629
Epoch 2210, val loss: 0.8657320737838745
Epoch 2220, training loss: 90.31901550292969 = 0.026578163728117943 + 10.0 * 9.029243469238281
Epoch 2220, val loss: 0.8700079917907715
Epoch 2230, training loss: 90.31694793701172 = 0.026076287031173706 + 10.0 * 9.02908706665039
Epoch 2230, val loss: 0.874683141708374
Epoch 2240, training loss: 90.33102416992188 = 0.025589877739548683 + 10.0 * 9.030543327331543
Epoch 2240, val loss: 0.8787460327148438
Epoch 2250, training loss: 90.31281280517578 = 0.025102531537413597 + 10.0 * 9.02877140045166
Epoch 2250, val loss: 0.8830703496932983
Epoch 2260, training loss: 90.30511474609375 = 0.024631232023239136 + 10.0 * 9.028048515319824
Epoch 2260, val loss: 0.8874051570892334
Epoch 2270, training loss: 90.30299377441406 = 0.024171680212020874 + 10.0 * 9.02788257598877
Epoch 2270, val loss: 0.8915578126907349
Epoch 2280, training loss: 90.3326416015625 = 0.023738093674182892 + 10.0 * 9.030890464782715
Epoch 2280, val loss: 0.8958948850631714
Epoch 2290, training loss: 90.32667541503906 = 0.02330189384520054 + 10.0 * 9.0303373336792
Epoch 2290, val loss: 0.9000855684280396
Epoch 2300, training loss: 90.30007934570312 = 0.022861771285533905 + 10.0 * 9.027721405029297
Epoch 2300, val loss: 0.9040219187736511
Epoch 2310, training loss: 90.29610443115234 = 0.022442901507019997 + 10.0 * 9.027365684509277
Epoch 2310, val loss: 0.9081855416297913
Epoch 2320, training loss: 90.29329681396484 = 0.02203226089477539 + 10.0 * 9.02712631225586
Epoch 2320, val loss: 0.9122669696807861
Epoch 2330, training loss: 90.29546356201172 = 0.021634405478835106 + 10.0 * 9.027382850646973
Epoch 2330, val loss: 0.916451632976532
Epoch 2340, training loss: 90.32215881347656 = 0.02125510945916176 + 10.0 * 9.03009033203125
Epoch 2340, val loss: 0.9205381274223328
Epoch 2350, training loss: 90.29864501953125 = 0.020868277177214622 + 10.0 * 9.027777671813965
Epoch 2350, val loss: 0.9242545962333679
Epoch 2360, training loss: 90.29232788085938 = 0.020496884360909462 + 10.0 * 9.027182579040527
Epoch 2360, val loss: 0.9283881187438965
Epoch 2370, training loss: 90.29098510742188 = 0.020135175436735153 + 10.0 * 9.027085304260254
Epoch 2370, val loss: 0.9320804476737976
Epoch 2380, training loss: 90.29291534423828 = 0.019781935960054398 + 10.0 * 9.027313232421875
Epoch 2380, val loss: 0.9355894327163696
Epoch 2390, training loss: 90.29664611816406 = 0.019438860937952995 + 10.0 * 9.02772045135498
Epoch 2390, val loss: 0.9392529129981995
Epoch 2400, training loss: 90.29586791992188 = 0.019103238359093666 + 10.0 * 9.027676582336426
Epoch 2400, val loss: 0.9435865879058838
Epoch 2410, training loss: 90.28522491455078 = 0.018770869821310043 + 10.0 * 9.02664566040039
Epoch 2410, val loss: 0.9470181465148926
Epoch 2420, training loss: 90.2801284790039 = 0.01844763569533825 + 10.0 * 9.026167869567871
Epoch 2420, val loss: 0.9510135650634766
Epoch 2430, training loss: 90.28340148925781 = 0.018136464059352875 + 10.0 * 9.02652645111084
Epoch 2430, val loss: 0.9542729258537292
Epoch 2440, training loss: 90.29701232910156 = 0.017840294167399406 + 10.0 * 9.027917861938477
Epoch 2440, val loss: 0.9579077363014221
Epoch 2450, training loss: 90.28225708007812 = 0.017534174025058746 + 10.0 * 9.026472091674805
Epoch 2450, val loss: 0.9621267914772034
Epoch 2460, training loss: 90.28623962402344 = 0.0172436460852623 + 10.0 * 9.026899337768555
Epoch 2460, val loss: 0.9653854966163635
Epoch 2470, training loss: 90.27909088134766 = 0.016958806663751602 + 10.0 * 9.026212692260742
Epoch 2470, val loss: 0.9688207507133484
Epoch 2480, training loss: 90.28611755371094 = 0.01668388769030571 + 10.0 * 9.02694320678711
Epoch 2480, val loss: 0.9725366234779358
Epoch 2490, training loss: 90.27338409423828 = 0.016408488154411316 + 10.0 * 9.025697708129883
Epoch 2490, val loss: 0.9763361811637878
Epoch 2500, training loss: 90.27169036865234 = 0.016143271699547768 + 10.0 * 9.025554656982422
Epoch 2500, val loss: 0.9798418879508972
Epoch 2510, training loss: 90.28791809082031 = 0.015891265124082565 + 10.0 * 9.027202606201172
Epoch 2510, val loss: 0.9830958247184753
Epoch 2520, training loss: 90.27375030517578 = 0.01563403569161892 + 10.0 * 9.025812149047852
Epoch 2520, val loss: 0.9868931174278259
Epoch 2530, training loss: 90.26822662353516 = 0.015384658239781857 + 10.0 * 9.025284767150879
Epoch 2530, val loss: 0.9900197982788086
Epoch 2540, training loss: 90.2637939453125 = 0.015141590498387814 + 10.0 * 9.02486515045166
Epoch 2540, val loss: 0.9935991764068604
Epoch 2550, training loss: 90.28607177734375 = 0.01491336990147829 + 10.0 * 9.027115821838379
Epoch 2550, val loss: 0.9972034692764282
Epoch 2560, training loss: 90.27017211914062 = 0.014680042862892151 + 10.0 * 9.025548934936523
Epoch 2560, val loss: 0.9997393488883972
Epoch 2570, training loss: 90.26667022705078 = 0.014453803189098835 + 10.0 * 9.025221824645996
Epoch 2570, val loss: 1.0033456087112427
Epoch 2580, training loss: 90.26471710205078 = 0.014235872775316238 + 10.0 * 9.02504825592041
Epoch 2580, val loss: 1.0061171054840088
Epoch 2590, training loss: 90.2612075805664 = 0.014019403606653214 + 10.0 * 9.024718284606934
Epoch 2590, val loss: 1.0093508958816528
Epoch 2600, training loss: 90.26244354248047 = 0.013811921700835228 + 10.0 * 9.024863243103027
Epoch 2600, val loss: 1.0125046968460083
Epoch 2610, training loss: 90.25897979736328 = 0.013606257736682892 + 10.0 * 9.024538040161133
Epoch 2610, val loss: 1.0157252550125122
Epoch 2620, training loss: 90.28422546386719 = 0.013412578031420708 + 10.0 * 9.027081489562988
Epoch 2620, val loss: 1.0188573598861694
Epoch 2630, training loss: 90.26914978027344 = 0.01321626827120781 + 10.0 * 9.025593757629395
Epoch 2630, val loss: 1.0220237970352173
Epoch 2640, training loss: 90.25582885742188 = 0.013020826503634453 + 10.0 * 9.024280548095703
Epoch 2640, val loss: 1.0250951051712036
Epoch 2650, training loss: 90.25949096679688 = 0.012836754322052002 + 10.0 * 9.024664878845215
Epoch 2650, val loss: 1.0278712511062622
Epoch 2660, training loss: 90.26598358154297 = 0.012656454928219318 + 10.0 * 9.0253324508667
Epoch 2660, val loss: 1.0307693481445312
Epoch 2670, training loss: 90.2557601928711 = 0.012476867996156216 + 10.0 * 9.024328231811523
Epoch 2670, val loss: 1.0339727401733398
Epoch 2680, training loss: 90.24830627441406 = 0.012301469221711159 + 10.0 * 9.023600578308105
Epoch 2680, val loss: 1.037166953086853
Epoch 2690, training loss: 90.2480239868164 = 0.012131315656006336 + 10.0 * 9.023589134216309
Epoch 2690, val loss: 1.0399008989334106
Epoch 2700, training loss: 90.26151275634766 = 0.011970379389822483 + 10.0 * 9.024953842163086
Epoch 2700, val loss: 1.0428266525268555
Epoch 2710, training loss: 90.24478912353516 = 0.011804417707026005 + 10.0 * 9.023298263549805
Epoch 2710, val loss: 1.0454033613204956
Epoch 2720, training loss: 90.25284576416016 = 0.011649656109511852 + 10.0 * 9.02411937713623
Epoch 2720, val loss: 1.0477145910263062
Epoch 2730, training loss: 90.24954223632812 = 0.01149256993085146 + 10.0 * 9.023805618286133
Epoch 2730, val loss: 1.050767421722412
Epoch 2740, training loss: 90.24244689941406 = 0.011337962001562119 + 10.0 * 9.023111343383789
Epoch 2740, val loss: 1.053863525390625
Epoch 2750, training loss: 90.24118041992188 = 0.011188746429979801 + 10.0 * 9.022998809814453
Epoch 2750, val loss: 1.0563908815383911
Epoch 2760, training loss: 90.26007843017578 = 0.01104968972504139 + 10.0 * 9.024903297424316
Epoch 2760, val loss: 1.059008002281189
Epoch 2770, training loss: 90.24874114990234 = 0.010905101895332336 + 10.0 * 9.023783683776855
Epoch 2770, val loss: 1.0609815120697021
Epoch 2780, training loss: 90.24101257324219 = 0.010762580670416355 + 10.0 * 9.023024559020996
Epoch 2780, val loss: 1.0640923976898193
Epoch 2790, training loss: 90.24099731445312 = 0.01062515377998352 + 10.0 * 9.02303695678711
Epoch 2790, val loss: 1.0667253732681274
Epoch 2800, training loss: 90.24296569824219 = 0.010493180714547634 + 10.0 * 9.023247718811035
Epoch 2800, val loss: 1.069226861000061
Epoch 2810, training loss: 90.23531341552734 = 0.010360335931181908 + 10.0 * 9.02249526977539
Epoch 2810, val loss: 1.0714741945266724
Epoch 2820, training loss: 90.24862670898438 = 0.010235974565148354 + 10.0 * 9.023838996887207
Epoch 2820, val loss: 1.074234962463379
Epoch 2830, training loss: 90.24386596679688 = 0.010110865347087383 + 10.0 * 9.023375511169434
Epoch 2830, val loss: 1.0773543119430542
Epoch 2840, training loss: 90.23880767822266 = 0.009986150078475475 + 10.0 * 9.022882461547852
Epoch 2840, val loss: 1.0788921117782593
Epoch 2850, training loss: 90.23858642578125 = 0.009867120534181595 + 10.0 * 9.022871971130371
Epoch 2850, val loss: 1.0811227560043335
Epoch 2860, training loss: 90.2300796508789 = 0.00974737573415041 + 10.0 * 9.022032737731934
Epoch 2860, val loss: 1.0836299657821655
Epoch 2870, training loss: 90.22990417480469 = 0.00963253527879715 + 10.0 * 9.022027015686035
Epoch 2870, val loss: 1.0860635042190552
Epoch 2880, training loss: 90.2580337524414 = 0.009528507478535175 + 10.0 * 9.024850845336914
Epoch 2880, val loss: 1.0879297256469727
Epoch 2890, training loss: 90.2412338256836 = 0.009414058178663254 + 10.0 * 9.023181915283203
Epoch 2890, val loss: 1.0906364917755127
Epoch 2900, training loss: 90.240234375 = 0.009305753745138645 + 10.0 * 9.023092269897461
Epoch 2900, val loss: 1.092626690864563
Epoch 2910, training loss: 90.22624969482422 = 0.009197833016514778 + 10.0 * 9.02170467376709
Epoch 2910, val loss: 1.0948936939239502
Epoch 2920, training loss: 90.22431945800781 = 0.009094386361539364 + 10.0 * 9.021522521972656
Epoch 2920, val loss: 1.0975717306137085
Epoch 2930, training loss: 90.2290267944336 = 0.008994615636765957 + 10.0 * 9.022003173828125
Epoch 2930, val loss: 1.0999088287353516
Epoch 2940, training loss: 90.2367172241211 = 0.008897710591554642 + 10.0 * 9.022782325744629
Epoch 2940, val loss: 1.101689338684082
Epoch 2950, training loss: 90.22664642333984 = 0.008798968978226185 + 10.0 * 9.021784782409668
Epoch 2950, val loss: 1.1033551692962646
Epoch 2960, training loss: 90.23566436767578 = 0.008705546148121357 + 10.0 * 9.022695541381836
Epoch 2960, val loss: 1.1057895421981812
Epoch 2970, training loss: 90.22114562988281 = 0.008608429692685604 + 10.0 * 9.02125358581543
Epoch 2970, val loss: 1.1077390909194946
Epoch 2980, training loss: 90.22652435302734 = 0.008517676033079624 + 10.0 * 9.021800994873047
Epoch 2980, val loss: 1.1097744703292847
Epoch 2990, training loss: 90.22946166992188 = 0.00842877198010683 + 10.0 * 9.022103309631348
Epoch 2990, val loss: 1.1120257377624512
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8493
Overall ASR: 0.7140
Flip ASR: 0.6429/1554 nodes
./selected_nodes/Pubmed/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 104.69593048095703 = 1.1048682928085327 + 10.0 * 10.359106063842773
Epoch 0, val loss: 1.1036566495895386
Epoch 10, training loss: 104.65432739257812 = 1.091895580291748 + 10.0 * 10.356243133544922
Epoch 10, val loss: 1.090484380722046
Epoch 20, training loss: 104.13619232177734 = 1.073933720588684 + 10.0 * 10.306225776672363
Epoch 20, val loss: 1.0724608898162842
Epoch 30, training loss: 100.6329345703125 = 1.0557395219802856 + 10.0 * 9.957719802856445
Epoch 30, val loss: 1.0547693967819214
Epoch 40, training loss: 97.41812133789062 = 1.0379565954208374 + 10.0 * 9.638016700744629
Epoch 40, val loss: 1.0375043153762817
Epoch 50, training loss: 95.90040588378906 = 1.0186642408370972 + 10.0 * 9.488174438476562
Epoch 50, val loss: 1.018174171447754
Epoch 60, training loss: 94.99307250976562 = 0.997724175453186 + 10.0 * 9.399534225463867
Epoch 60, val loss: 0.9978992938995361
Epoch 70, training loss: 94.73443603515625 = 0.9746576547622681 + 10.0 * 9.375978469848633
Epoch 70, val loss: 0.9754663109779358
Epoch 80, training loss: 94.2679672241211 = 0.950480043888092 + 10.0 * 9.331748962402344
Epoch 80, val loss: 0.9521601796150208
Epoch 90, training loss: 93.79441833496094 = 0.9260523319244385 + 10.0 * 9.286836624145508
Epoch 90, val loss: 0.9285352826118469
Epoch 100, training loss: 93.4531478881836 = 0.8983525037765503 + 10.0 * 9.25547981262207
Epoch 100, val loss: 0.9015705585479736
Epoch 110, training loss: 93.16338348388672 = 0.8663848638534546 + 10.0 * 9.229700088500977
Epoch 110, val loss: 0.8705872893333435
Epoch 120, training loss: 92.99954986572266 = 0.82973313331604 + 10.0 * 9.216981887817383
Epoch 120, val loss: 0.8352514505386353
Epoch 130, training loss: 92.83145904541016 = 0.7886585593223572 + 10.0 * 9.204279899597168
Epoch 130, val loss: 0.7955269813537598
Epoch 140, training loss: 92.65253448486328 = 0.746785044670105 + 10.0 * 9.190574645996094
Epoch 140, val loss: 0.7557163834571838
Epoch 150, training loss: 92.45889282226562 = 0.706541121006012 + 10.0 * 9.1752347946167
Epoch 150, val loss: 0.7178838849067688
Epoch 160, training loss: 92.31634521484375 = 0.6672546863555908 + 10.0 * 9.164909362792969
Epoch 160, val loss: 0.6812883615493774
Epoch 170, training loss: 92.1935806274414 = 0.6286686658859253 + 10.0 * 9.15649127960205
Epoch 170, val loss: 0.6455516815185547
Epoch 180, training loss: 92.09053802490234 = 0.5925036668777466 + 10.0 * 9.149803161621094
Epoch 180, val loss: 0.6126549243927002
Epoch 190, training loss: 92.00093078613281 = 0.5604631304740906 + 10.0 * 9.144046783447266
Epoch 190, val loss: 0.5841870307922363
Epoch 200, training loss: 91.92938995361328 = 0.5329424738883972 + 10.0 * 9.139644622802734
Epoch 200, val loss: 0.560384213924408
Epoch 210, training loss: 91.8884506225586 = 0.5097135901451111 + 10.0 * 9.137873649597168
Epoch 210, val loss: 0.5409963726997375
Epoch 220, training loss: 91.8237075805664 = 0.4906715154647827 + 10.0 * 9.13330364227295
Epoch 220, val loss: 0.5256443023681641
Epoch 230, training loss: 91.7752914428711 = 0.47513309121131897 + 10.0 * 9.13001537322998
Epoch 230, val loss: 0.5139463543891907
Epoch 240, training loss: 91.73060607910156 = 0.46198251843452454 + 10.0 * 9.126862525939941
Epoch 240, val loss: 0.5045288801193237
Epoch 250, training loss: 91.68539428710938 = 0.45057612657546997 + 10.0 * 9.123481750488281
Epoch 250, val loss: 0.49687185883522034
Epoch 260, training loss: 91.63896942138672 = 0.4405492842197418 + 10.0 * 9.119841575622559
Epoch 260, val loss: 0.49053990840911865
Epoch 270, training loss: 91.59973907470703 = 0.4315231144428253 + 10.0 * 9.1168212890625
Epoch 270, val loss: 0.48515191674232483
Epoch 280, training loss: 91.57920837402344 = 0.4231940507888794 + 10.0 * 9.115601539611816
Epoch 280, val loss: 0.4803597331047058
Epoch 290, training loss: 91.53765869140625 = 0.4156827926635742 + 10.0 * 9.112197875976562
Epoch 290, val loss: 0.4761573076248169
Epoch 300, training loss: 91.49421691894531 = 0.4088688790798187 + 10.0 * 9.108534812927246
Epoch 300, val loss: 0.47255396842956543
Epoch 310, training loss: 91.45526885986328 = 0.4024589955806732 + 10.0 * 9.105280876159668
Epoch 310, val loss: 0.4692538380622864
Epoch 320, training loss: 91.42538452148438 = 0.3963322937488556 + 10.0 * 9.1029052734375
Epoch 320, val loss: 0.466069757938385
Epoch 330, training loss: 91.45065307617188 = 0.39043644070625305 + 10.0 * 9.106021881103516
Epoch 330, val loss: 0.4630378782749176
Epoch 340, training loss: 91.38806915283203 = 0.38473519682884216 + 10.0 * 9.100333213806152
Epoch 340, val loss: 0.46004822850227356
Epoch 350, training loss: 91.35395050048828 = 0.37932682037353516 + 10.0 * 9.09746265411377
Epoch 350, val loss: 0.45731109380722046
Epoch 360, training loss: 91.33084106445312 = 0.3741031289100647 + 10.0 * 9.095674514770508
Epoch 360, val loss: 0.4546276032924652
Epoch 370, training loss: 91.33873748779297 = 0.3689996302127838 + 10.0 * 9.096973419189453
Epoch 370, val loss: 0.45201176404953003
Epoch 380, training loss: 91.29759979248047 = 0.36402270197868347 + 10.0 * 9.093358039855957
Epoch 380, val loss: 0.4494246244430542
Epoch 390, training loss: 91.27449035644531 = 0.3592252731323242 + 10.0 * 9.091526985168457
Epoch 390, val loss: 0.4469802677631378
Epoch 400, training loss: 91.26007080078125 = 0.35454636812210083 + 10.0 * 9.09055233001709
Epoch 400, val loss: 0.4446035623550415
Epoch 410, training loss: 91.2441635131836 = 0.34993287920951843 + 10.0 * 9.089423179626465
Epoch 410, val loss: 0.4421994984149933
Epoch 420, training loss: 91.22727966308594 = 0.34546399116516113 + 10.0 * 9.088181495666504
Epoch 420, val loss: 0.4399673044681549
Epoch 430, training loss: 91.21160125732422 = 0.34115561842918396 + 10.0 * 9.087044715881348
Epoch 430, val loss: 0.4378257095813751
Epoch 440, training loss: 91.1968002319336 = 0.3369215130805969 + 10.0 * 9.08598804473877
Epoch 440, val loss: 0.4357409179210663
Epoch 450, training loss: 91.22284698486328 = 0.3327411711215973 + 10.0 * 9.089010238647461
Epoch 450, val loss: 0.43369200825691223
Epoch 460, training loss: 91.18385314941406 = 0.3286374509334564 + 10.0 * 9.085521697998047
Epoch 460, val loss: 0.4316960275173187
Epoch 470, training loss: 91.16255950927734 = 0.32469403743743896 + 10.0 * 9.083786964416504
Epoch 470, val loss: 0.4298483431339264
Epoch 480, training loss: 91.14559936523438 = 0.32085713744163513 + 10.0 * 9.082474708557129
Epoch 480, val loss: 0.42812880873680115
Epoch 490, training loss: 91.1466293334961 = 0.31708279252052307 + 10.0 * 9.082954406738281
Epoch 490, val loss: 0.4264763593673706
Epoch 500, training loss: 91.123046875 = 0.31336939334869385 + 10.0 * 9.080967903137207
Epoch 500, val loss: 0.42475298047065735
Epoch 510, training loss: 91.11322784423828 = 0.3097480833530426 + 10.0 * 9.080348014831543
Epoch 510, val loss: 0.42326539754867554
Epoch 520, training loss: 91.11003875732422 = 0.3061908185482025 + 10.0 * 9.080385208129883
Epoch 520, val loss: 0.4217568635940552
Epoch 530, training loss: 91.08976745605469 = 0.3027227222919464 + 10.0 * 9.078704833984375
Epoch 530, val loss: 0.42041122913360596
Epoch 540, training loss: 91.07495880126953 = 0.29935604333877563 + 10.0 * 9.077560424804688
Epoch 540, val loss: 0.4191230535507202
Epoch 550, training loss: 91.07891845703125 = 0.2960323989391327 + 10.0 * 9.078288078308105
Epoch 550, val loss: 0.41787952184677124
Epoch 560, training loss: 91.05484771728516 = 0.2927779257297516 + 10.0 * 9.076207160949707
Epoch 560, val loss: 0.41669347882270813
Epoch 570, training loss: 91.0456771850586 = 0.2895945906639099 + 10.0 * 9.075608253479004
Epoch 570, val loss: 0.415610671043396
Epoch 580, training loss: 91.0289306640625 = 0.2864716351032257 + 10.0 * 9.074246406555176
Epoch 580, val loss: 0.4146033823490143
Epoch 590, training loss: 91.0159912109375 = 0.2834031581878662 + 10.0 * 9.073259353637695
Epoch 590, val loss: 0.41365766525268555
Epoch 600, training loss: 91.01597595214844 = 0.28036370873451233 + 10.0 * 9.07356071472168
Epoch 600, val loss: 0.4128093421459198
Epoch 610, training loss: 91.02288818359375 = 0.27735090255737305 + 10.0 * 9.074553489685059
Epoch 610, val loss: 0.4120882451534271
Epoch 620, training loss: 90.98804473876953 = 0.27440327405929565 + 10.0 * 9.07136344909668
Epoch 620, val loss: 0.4113674461841583
Epoch 630, training loss: 90.97789764404297 = 0.2715393900871277 + 10.0 * 9.070635795593262
Epoch 630, val loss: 0.4107927083969116
Epoch 640, training loss: 90.96923828125 = 0.268699049949646 + 10.0 * 9.070054054260254
Epoch 640, val loss: 0.41026800870895386
Epoch 650, training loss: 90.97278594970703 = 0.2658844590187073 + 10.0 * 9.070690155029297
Epoch 650, val loss: 0.4097979962825775
Epoch 660, training loss: 90.96258544921875 = 0.2630883455276489 + 10.0 * 9.06994915008545
Epoch 660, val loss: 0.4095243513584137
Epoch 670, training loss: 90.9548110961914 = 0.2603526711463928 + 10.0 * 9.069445610046387
Epoch 670, val loss: 0.40917497873306274
Epoch 680, training loss: 90.94747161865234 = 0.2576635479927063 + 10.0 * 9.068981170654297
Epoch 680, val loss: 0.4090081751346588
Epoch 690, training loss: 90.934326171875 = 0.25500476360321045 + 10.0 * 9.06793212890625
Epoch 690, val loss: 0.4089454114437103
Epoch 700, training loss: 90.92041015625 = 0.25238704681396484 + 10.0 * 9.066802024841309
Epoch 700, val loss: 0.4088889956474304
Epoch 710, training loss: 90.91362762451172 = 0.24979828298091888 + 10.0 * 9.06638240814209
Epoch 710, val loss: 0.408873051404953
Epoch 720, training loss: 90.92056274414062 = 0.24722370505332947 + 10.0 * 9.067334175109863
Epoch 720, val loss: 0.40894895792007446
Epoch 730, training loss: 90.9020004272461 = 0.24465997517108917 + 10.0 * 9.065733909606934
Epoch 730, val loss: 0.40920644998550415
Epoch 740, training loss: 90.9059066772461 = 0.24214525520801544 + 10.0 * 9.066376686096191
Epoch 740, val loss: 0.4093477725982666
Epoch 750, training loss: 90.88941955566406 = 0.23965393006801605 + 10.0 * 9.064976692199707
Epoch 750, val loss: 0.4097113013267517
Epoch 760, training loss: 90.87848663330078 = 0.2371995449066162 + 10.0 * 9.064128875732422
Epoch 760, val loss: 0.4100671112537384
Epoch 770, training loss: 90.88104248046875 = 0.23476681113243103 + 10.0 * 9.064627647399902
Epoch 770, val loss: 0.4104892313480377
Epoch 780, training loss: 90.88248443603516 = 0.23234328627586365 + 10.0 * 9.065013885498047
Epoch 780, val loss: 0.41101714968681335
Epoch 790, training loss: 90.8621597290039 = 0.2299492061138153 + 10.0 * 9.063220977783203
Epoch 790, val loss: 0.41151881217956543
Epoch 800, training loss: 90.8543930053711 = 0.22758032381534576 + 10.0 * 9.062681198120117
Epoch 800, val loss: 0.41214853525161743
Epoch 810, training loss: 90.84901428222656 = 0.22521772980690002 + 10.0 * 9.062379837036133
Epoch 810, val loss: 0.41278672218322754
Epoch 820, training loss: 90.8716812133789 = 0.22286278009414673 + 10.0 * 9.064882278442383
Epoch 820, val loss: 0.41354772448539734
Epoch 830, training loss: 90.84803009033203 = 0.22051562368869781 + 10.0 * 9.062751770019531
Epoch 830, val loss: 0.4142826497554779
Epoch 840, training loss: 90.83541870117188 = 0.21819698810577393 + 10.0 * 9.061721801757812
Epoch 840, val loss: 0.4150938391685486
Epoch 850, training loss: 90.82494354248047 = 0.2158946990966797 + 10.0 * 9.060904502868652
Epoch 850, val loss: 0.41589635610580444
Epoch 860, training loss: 90.84661865234375 = 0.21360275149345398 + 10.0 * 9.063302040100098
Epoch 860, val loss: 0.41686898469924927
Epoch 870, training loss: 90.8258285522461 = 0.21130937337875366 + 10.0 * 9.06145191192627
Epoch 870, val loss: 0.4178754985332489
Epoch 880, training loss: 90.80886840820312 = 0.20904549956321716 + 10.0 * 9.059982299804688
Epoch 880, val loss: 0.418863445520401
Epoch 890, training loss: 90.80105590820312 = 0.2067847102880478 + 10.0 * 9.059427261352539
Epoch 890, val loss: 0.4199126660823822
Epoch 900, training loss: 90.79781341552734 = 0.20451919734477997 + 10.0 * 9.05932903289795
Epoch 900, val loss: 0.4210844337940216
Epoch 910, training loss: 90.83613586425781 = 0.20225507020950317 + 10.0 * 9.063387870788574
Epoch 910, val loss: 0.4222639203071594
Epoch 920, training loss: 90.78638458251953 = 0.19999948143959045 + 10.0 * 9.058638572692871
Epoch 920, val loss: 0.42346420884132385
Epoch 930, training loss: 90.7827377319336 = 0.1977635771036148 + 10.0 * 9.058497428894043
Epoch 930, val loss: 0.42475566267967224
Epoch 940, training loss: 90.77489471435547 = 0.1955208033323288 + 10.0 * 9.057937622070312
Epoch 940, val loss: 0.4261001944541931
Epoch 950, training loss: 90.79771423339844 = 0.19327405095100403 + 10.0 * 9.060443878173828
Epoch 950, val loss: 0.42757001519203186
Epoch 960, training loss: 90.7805404663086 = 0.1910156011581421 + 10.0 * 9.058952331542969
Epoch 960, val loss: 0.42883527278900146
Epoch 970, training loss: 90.76245880126953 = 0.1887757033109665 + 10.0 * 9.057368278503418
Epoch 970, val loss: 0.43043023347854614
Epoch 980, training loss: 90.75519561767578 = 0.18653234839439392 + 10.0 * 9.056866645812988
Epoch 980, val loss: 0.4319060146808624
Epoch 990, training loss: 90.76337432861328 = 0.18428568542003632 + 10.0 * 9.05790901184082
Epoch 990, val loss: 0.43339186906814575
Epoch 1000, training loss: 90.75420379638672 = 0.18203438818454742 + 10.0 * 9.05721664428711
Epoch 1000, val loss: 0.4353373944759369
Epoch 1010, training loss: 90.74459075927734 = 0.17979708313941956 + 10.0 * 9.056479454040527
Epoch 1010, val loss: 0.4369274079799652
Epoch 1020, training loss: 90.73838806152344 = 0.1775633543729782 + 10.0 * 9.056081771850586
Epoch 1020, val loss: 0.43856239318847656
Epoch 1030, training loss: 90.73238372802734 = 0.1753234565258026 + 10.0 * 9.055706024169922
Epoch 1030, val loss: 0.4403665363788605
Epoch 1040, training loss: 90.74655151367188 = 0.1730775386095047 + 10.0 * 9.057347297668457
Epoch 1040, val loss: 0.44218072295188904
Epoch 1050, training loss: 90.724609375 = 0.1708255410194397 + 10.0 * 9.055378913879395
Epoch 1050, val loss: 0.44423991441726685
Epoch 1060, training loss: 90.71727752685547 = 0.1685773730278015 + 10.0 * 9.054869651794434
Epoch 1060, val loss: 0.44605016708374023
Epoch 1070, training loss: 90.74851989746094 = 0.16633492708206177 + 10.0 * 9.058218002319336
Epoch 1070, val loss: 0.44806039333343506
Epoch 1080, training loss: 90.71892547607422 = 0.16408243775367737 + 10.0 * 9.0554838180542
Epoch 1080, val loss: 0.4501313865184784
Epoch 1090, training loss: 90.7046127319336 = 0.16183923184871674 + 10.0 * 9.054277420043945
Epoch 1090, val loss: 0.4523095488548279
Epoch 1100, training loss: 90.70150756835938 = 0.1595931351184845 + 10.0 * 9.054191589355469
Epoch 1100, val loss: 0.454319030046463
Epoch 1110, training loss: 90.72319793701172 = 0.1573559045791626 + 10.0 * 9.056584358215332
Epoch 1110, val loss: 0.4564872980117798
Epoch 1120, training loss: 90.69474792480469 = 0.1551119089126587 + 10.0 * 9.053963661193848
Epoch 1120, val loss: 0.458939790725708
Epoch 1130, training loss: 90.68353271484375 = 0.15287713706493378 + 10.0 * 9.053065299987793
Epoch 1130, val loss: 0.46112340688705444
Epoch 1140, training loss: 90.68035125732422 = 0.1506451964378357 + 10.0 * 9.052970886230469
Epoch 1140, val loss: 0.4635981023311615
Epoch 1150, training loss: 90.69507598876953 = 0.1484207808971405 + 10.0 * 9.054665565490723
Epoch 1150, val loss: 0.46604615449905396
Epoch 1160, training loss: 90.69076538085938 = 0.14619305729866028 + 10.0 * 9.05445671081543
Epoch 1160, val loss: 0.46816393733024597
Epoch 1170, training loss: 90.67457580566406 = 0.1439794898033142 + 10.0 * 9.053059577941895
Epoch 1170, val loss: 0.4710090756416321
Epoch 1180, training loss: 90.6626968383789 = 0.14177115261554718 + 10.0 * 9.052092552185059
Epoch 1180, val loss: 0.47339487075805664
Epoch 1190, training loss: 90.6567611694336 = 0.13956786692142487 + 10.0 * 9.051719665527344
Epoch 1190, val loss: 0.47604796290397644
Epoch 1200, training loss: 90.6578598022461 = 0.13737010955810547 + 10.0 * 9.05204963684082
Epoch 1200, val loss: 0.4786387085914612
Epoch 1210, training loss: 90.6690902709961 = 0.13518038392066956 + 10.0 * 9.053391456604004
Epoch 1210, val loss: 0.4814976453781128
Epoch 1220, training loss: 90.64637756347656 = 0.13299229741096497 + 10.0 * 9.051338195800781
Epoch 1220, val loss: 0.48430153727531433
Epoch 1230, training loss: 90.64374542236328 = 0.13081958889961243 + 10.0 * 9.051292419433594
Epoch 1230, val loss: 0.4874941110610962
Epoch 1240, training loss: 90.63648223876953 = 0.12864276766777039 + 10.0 * 9.05078411102295
Epoch 1240, val loss: 0.49044010043144226
Epoch 1250, training loss: 90.66172790527344 = 0.12648390233516693 + 10.0 * 9.0535249710083
Epoch 1250, val loss: 0.4935237467288971
Epoch 1260, training loss: 90.64247131347656 = 0.12432828545570374 + 10.0 * 9.051814079284668
Epoch 1260, val loss: 0.4964962303638458
Epoch 1270, training loss: 90.62753295898438 = 0.12219370156526566 + 10.0 * 9.05053424835205
Epoch 1270, val loss: 0.4997282028198242
Epoch 1280, training loss: 90.61894226074219 = 0.12006951868534088 + 10.0 * 9.049886703491211
Epoch 1280, val loss: 0.5028200745582581
Epoch 1290, training loss: 90.6177978515625 = 0.11795596778392792 + 10.0 * 9.049983978271484
Epoch 1290, val loss: 0.5061328411102295
Epoch 1300, training loss: 90.64764404296875 = 0.1158633828163147 + 10.0 * 9.053178787231445
Epoch 1300, val loss: 0.5094479322433472
Epoch 1310, training loss: 90.60746765136719 = 0.11377015709877014 + 10.0 * 9.049369812011719
Epoch 1310, val loss: 0.5126973986625671
Epoch 1320, training loss: 90.60711669921875 = 0.1117057055234909 + 10.0 * 9.049541473388672
Epoch 1320, val loss: 0.516078531742096
Epoch 1330, training loss: 90.5973129272461 = 0.10965055227279663 + 10.0 * 9.048766136169434
Epoch 1330, val loss: 0.5193167924880981
Epoch 1340, training loss: 90.59947967529297 = 0.10761280357837677 + 10.0 * 9.049186706542969
Epoch 1340, val loss: 0.5227092504501343
Epoch 1350, training loss: 90.60550689697266 = 0.10559644550085068 + 10.0 * 9.0499906539917
Epoch 1350, val loss: 0.5263302326202393
Epoch 1360, training loss: 90.59168243408203 = 0.10359714180231094 + 10.0 * 9.048808097839355
Epoch 1360, val loss: 0.5299474000930786
Epoch 1370, training loss: 90.58830261230469 = 0.10161985456943512 + 10.0 * 9.048667907714844
Epoch 1370, val loss: 0.5334376096725464
Epoch 1380, training loss: 90.58737182617188 = 0.09966511279344559 + 10.0 * 9.048770904541016
Epoch 1380, val loss: 0.5370085835456848
Epoch 1390, training loss: 90.58845520019531 = 0.0977298691868782 + 10.0 * 9.049072265625
Epoch 1390, val loss: 0.5407907962799072
Epoch 1400, training loss: 90.57489776611328 = 0.0958152487874031 + 10.0 * 9.047907829284668
Epoch 1400, val loss: 0.5443425178527832
Epoch 1410, training loss: 90.56832122802734 = 0.09392672032117844 + 10.0 * 9.047439575195312
Epoch 1410, val loss: 0.5481432676315308
Epoch 1420, training loss: 90.56309509277344 = 0.09205997735261917 + 10.0 * 9.047103881835938
Epoch 1420, val loss: 0.5519697070121765
Epoch 1430, training loss: 90.56690979003906 = 0.09022011607885361 + 10.0 * 9.047669410705566
Epoch 1430, val loss: 0.5559560060501099
Epoch 1440, training loss: 90.56417083740234 = 0.08840417861938477 + 10.0 * 9.047576904296875
Epoch 1440, val loss: 0.5596672892570496
Epoch 1450, training loss: 90.56458282470703 = 0.08661800622940063 + 10.0 * 9.047796249389648
Epoch 1450, val loss: 0.5635601282119751
Epoch 1460, training loss: 90.54776763916016 = 0.08484504371881485 + 10.0 * 9.046292304992676
Epoch 1460, val loss: 0.567405104637146
Epoch 1470, training loss: 90.54566192626953 = 0.08310283720493317 + 10.0 * 9.046256065368652
Epoch 1470, val loss: 0.5714156627655029
Epoch 1480, training loss: 90.54379272460938 = 0.0813828781247139 + 10.0 * 9.04624080657959
Epoch 1480, val loss: 0.5753312110900879
Epoch 1490, training loss: 90.57587432861328 = 0.07970383018255234 + 10.0 * 9.049616813659668
Epoch 1490, val loss: 0.579538106918335
Epoch 1500, training loss: 90.54013061523438 = 0.07802961766719818 + 10.0 * 9.046210289001465
Epoch 1500, val loss: 0.5837027430534363
Epoch 1510, training loss: 90.53804016113281 = 0.07639167457818985 + 10.0 * 9.046164512634277
Epoch 1510, val loss: 0.5877627730369568
Epoch 1520, training loss: 90.53260040283203 = 0.07477730512619019 + 10.0 * 9.045782089233398
Epoch 1520, val loss: 0.5920054316520691
Epoch 1530, training loss: 90.54206848144531 = 0.07319647818803787 + 10.0 * 9.046887397766113
Epoch 1530, val loss: 0.5960776209831238
Epoch 1540, training loss: 90.53532409667969 = 0.07164503633975983 + 10.0 * 9.046367645263672
Epoch 1540, val loss: 0.6000500917434692
Epoch 1550, training loss: 90.52029418945312 = 0.07011722773313522 + 10.0 * 9.045018196105957
Epoch 1550, val loss: 0.6048124432563782
Epoch 1560, training loss: 90.5153579711914 = 0.06861940771341324 + 10.0 * 9.044673919677734
Epoch 1560, val loss: 0.6088426113128662
Epoch 1570, training loss: 90.5361099243164 = 0.06716056168079376 + 10.0 * 9.046895027160645
Epoch 1570, val loss: 0.6131133437156677
Epoch 1580, training loss: 90.51268768310547 = 0.06571175158023834 + 10.0 * 9.044697761535645
Epoch 1580, val loss: 0.6177268028259277
Epoch 1590, training loss: 90.50870513916016 = 0.06429917365312576 + 10.0 * 9.044440269470215
Epoch 1590, val loss: 0.621888279914856
Epoch 1600, training loss: 90.507568359375 = 0.06291083991527557 + 10.0 * 9.044466018676758
Epoch 1600, val loss: 0.6265073418617249
Epoch 1610, training loss: 90.54033660888672 = 0.06156095489859581 + 10.0 * 9.047877311706543
Epoch 1610, val loss: 0.6305990219116211
Epoch 1620, training loss: 90.50115203857422 = 0.06021837145090103 + 10.0 * 9.044093132019043
Epoch 1620, val loss: 0.6348831653594971
Epoch 1630, training loss: 90.4985580444336 = 0.058910150080919266 + 10.0 * 9.043965339660645
Epoch 1630, val loss: 0.6398786902427673
Epoch 1640, training loss: 90.49105072021484 = 0.05762512981891632 + 10.0 * 9.043342590332031
Epoch 1640, val loss: 0.6442424058914185
Epoch 1650, training loss: 90.49612426757812 = 0.05637287721037865 + 10.0 * 9.043974876403809
Epoch 1650, val loss: 0.6485292315483093
Epoch 1660, training loss: 90.49847412109375 = 0.05514373630285263 + 10.0 * 9.044332504272461
Epoch 1660, val loss: 0.65318363904953
Epoch 1670, training loss: 90.48490142822266 = 0.05393524840474129 + 10.0 * 9.043096542358398
Epoch 1670, val loss: 0.6578842997550964
Epoch 1680, training loss: 90.48567199707031 = 0.052749380469322205 + 10.0 * 9.043292045593262
Epoch 1680, val loss: 0.6623044610023499
Epoch 1690, training loss: 90.48902893066406 = 0.05159491300582886 + 10.0 * 9.043743133544922
Epoch 1690, val loss: 0.6670778393745422
Epoch 1700, training loss: 90.47846984863281 = 0.0504547655582428 + 10.0 * 9.042801856994629
Epoch 1700, val loss: 0.671065628528595
Epoch 1710, training loss: 90.48175048828125 = 0.04934428259730339 + 10.0 * 9.043240547180176
Epoch 1710, val loss: 0.6751303672790527
Epoch 1720, training loss: 90.47872161865234 = 0.04825695976614952 + 10.0 * 9.043046951293945
Epoch 1720, val loss: 0.6799558401107788
Epoch 1730, training loss: 90.4725570678711 = 0.04718859866261482 + 10.0 * 9.042536735534668
Epoch 1730, val loss: 0.6842995285987854
Epoch 1740, training loss: 90.48155212402344 = 0.04615134745836258 + 10.0 * 9.043540000915527
Epoch 1740, val loss: 0.6895580291748047
Epoch 1750, training loss: 90.46891021728516 = 0.04512947425246239 + 10.0 * 9.042378425598145
Epoch 1750, val loss: 0.6936671733856201
Epoch 1760, training loss: 90.4610366821289 = 0.04412836208939552 + 10.0 * 9.041690826416016
Epoch 1760, val loss: 0.6977277994155884
Epoch 1770, training loss: 90.45954895019531 = 0.04315531626343727 + 10.0 * 9.04163932800293
Epoch 1770, val loss: 0.7027198076248169
Epoch 1780, training loss: 90.48410034179688 = 0.042212605476379395 + 10.0 * 9.044188499450684
Epoch 1780, val loss: 0.7072649598121643
Epoch 1790, training loss: 90.48738098144531 = 0.041288234293460846 + 10.0 * 9.044609069824219
Epoch 1790, val loss: 0.711828351020813
Epoch 1800, training loss: 90.45915985107422 = 0.040378130972385406 + 10.0 * 9.041878700256348
Epoch 1800, val loss: 0.7159333825111389
Epoch 1810, training loss: 90.4493408203125 = 0.039493899792432785 + 10.0 * 9.040984153747559
Epoch 1810, val loss: 0.7203536629676819
Epoch 1820, training loss: 90.44691467285156 = 0.03863099589943886 + 10.0 * 9.040828704833984
Epoch 1820, val loss: 0.725018322467804
Epoch 1830, training loss: 90.46556091308594 = 0.03779904916882515 + 10.0 * 9.042776107788086
Epoch 1830, val loss: 0.7292540669441223
Epoch 1840, training loss: 90.44302368164062 = 0.0369691476225853 + 10.0 * 9.040605545043945
Epoch 1840, val loss: 0.7339983582496643
Epoch 1850, training loss: 90.44664764404297 = 0.036171674728393555 + 10.0 * 9.041048049926758
Epoch 1850, val loss: 0.7383198738098145
Epoch 1860, training loss: 90.44571685791016 = 0.03539038822054863 + 10.0 * 9.041032791137695
Epoch 1860, val loss: 0.7428191304206848
Epoch 1870, training loss: 90.44218444824219 = 0.034626346081495285 + 10.0 * 9.040755271911621
Epoch 1870, val loss: 0.7472869753837585
Epoch 1880, training loss: 90.45095825195312 = 0.033886563032865524 + 10.0 * 9.041707038879395
Epoch 1880, val loss: 0.7521538138389587
Epoch 1890, training loss: 90.44557189941406 = 0.03316469117999077 + 10.0 * 9.041240692138672
Epoch 1890, val loss: 0.7570055723190308
Epoch 1900, training loss: 90.43505096435547 = 0.03245139122009277 + 10.0 * 9.040260314941406
Epoch 1900, val loss: 0.7609652876853943
Epoch 1910, training loss: 90.42864227294922 = 0.03176116570830345 + 10.0 * 9.039688110351562
Epoch 1910, val loss: 0.7649036645889282
Epoch 1920, training loss: 90.4245376586914 = 0.031085148453712463 + 10.0 * 9.039345741271973
Epoch 1920, val loss: 0.7696990966796875
Epoch 1930, training loss: 90.43147277832031 = 0.030432919040322304 + 10.0 * 9.040103912353516
Epoch 1930, val loss: 0.7740973830223083
Epoch 1940, training loss: 90.43485260009766 = 0.029796535149216652 + 10.0 * 9.040506362915039
Epoch 1940, val loss: 0.778116762638092
Epoch 1950, training loss: 90.41968536376953 = 0.029171865433454514 + 10.0 * 9.039051055908203
Epoch 1950, val loss: 0.7828396558761597
Epoch 1960, training loss: 90.4304428100586 = 0.02857336960732937 + 10.0 * 9.040186882019043
Epoch 1960, val loss: 0.7865986227989197
Epoch 1970, training loss: 90.41997528076172 = 0.027976157143712044 + 10.0 * 9.039199829101562
Epoch 1970, val loss: 0.7914198040962219
Epoch 1980, training loss: 90.4151840209961 = 0.027399910613894463 + 10.0 * 9.038778305053711
Epoch 1980, val loss: 0.7954327464103699
Epoch 1990, training loss: 90.41819763183594 = 0.02684335596859455 + 10.0 * 9.039135932922363
Epoch 1990, val loss: 0.799618661403656
Epoch 2000, training loss: 90.42695617675781 = 0.026299836114048958 + 10.0 * 9.04006576538086
Epoch 2000, val loss: 0.8034310340881348
Epoch 2010, training loss: 90.41899108886719 = 0.025767657905817032 + 10.0 * 9.039322853088379
Epoch 2010, val loss: 0.8076176643371582
Epoch 2020, training loss: 90.41133880615234 = 0.025248341262340546 + 10.0 * 9.038609504699707
Epoch 2020, val loss: 0.8117734789848328
Epoch 2030, training loss: 90.40528106689453 = 0.02474181167781353 + 10.0 * 9.038053512573242
Epoch 2030, val loss: 0.8159645199775696
Epoch 2040, training loss: 90.42745971679688 = 0.024260131642222404 + 10.0 * 9.040319442749023
Epoch 2040, val loss: 0.8204811811447144
Epoch 2050, training loss: 90.40911865234375 = 0.023775571957230568 + 10.0 * 9.038534164428711
Epoch 2050, val loss: 0.8234044909477234
Epoch 2060, training loss: 90.40341186523438 = 0.02330601029098034 + 10.0 * 9.038010597229004
Epoch 2060, val loss: 0.8277757167816162
Epoch 2070, training loss: 90.40423583984375 = 0.022851843386888504 + 10.0 * 9.038138389587402
Epoch 2070, val loss: 0.8313089609146118
Epoch 2080, training loss: 90.40621185302734 = 0.02240704745054245 + 10.0 * 9.03838062286377
Epoch 2080, val loss: 0.8356380462646484
Epoch 2090, training loss: 90.3987808227539 = 0.02197171002626419 + 10.0 * 9.037680625915527
Epoch 2090, val loss: 0.8395740389823914
Epoch 2100, training loss: 90.41320037841797 = 0.02155456878244877 + 10.0 * 9.039164543151855
Epoch 2100, val loss: 0.8434444069862366
Epoch 2110, training loss: 90.40587615966797 = 0.021141231060028076 + 10.0 * 9.038473129272461
Epoch 2110, val loss: 0.8474940061569214
Epoch 2120, training loss: 90.3926010131836 = 0.02073758654296398 + 10.0 * 9.037186622619629
Epoch 2120, val loss: 0.8511567115783691
Epoch 2130, training loss: 90.388916015625 = 0.020345253869891167 + 10.0 * 9.036856651306152
Epoch 2130, val loss: 0.8549262285232544
Epoch 2140, training loss: 90.39033508300781 = 0.019965896382927895 + 10.0 * 9.037036895751953
Epoch 2140, val loss: 0.8588276505470276
Epoch 2150, training loss: 90.40621185302734 = 0.01960032619535923 + 10.0 * 9.038661003112793
Epoch 2150, val loss: 0.8625280261039734
Epoch 2160, training loss: 90.39575958251953 = 0.019234638661146164 + 10.0 * 9.037652969360352
Epoch 2160, val loss: 0.8666427731513977
Epoch 2170, training loss: 90.38417053222656 = 0.018877411261200905 + 10.0 * 9.036529541015625
Epoch 2170, val loss: 0.8702213764190674
Epoch 2180, training loss: 90.38805389404297 = 0.018533822149038315 + 10.0 * 9.036952018737793
Epoch 2180, val loss: 0.8736666440963745
Epoch 2190, training loss: 90.39818572998047 = 0.018203414976596832 + 10.0 * 9.03799819946289
Epoch 2190, val loss: 0.8778642416000366
Epoch 2200, training loss: 90.38394165039062 = 0.017872119322419167 + 10.0 * 9.036606788635254
Epoch 2200, val loss: 0.8801040649414062
Epoch 2210, training loss: 90.37704467773438 = 0.017550718039274216 + 10.0 * 9.03594970703125
Epoch 2210, val loss: 0.8836719393730164
Epoch 2220, training loss: 90.3765869140625 = 0.0172389168292284 + 10.0 * 9.035934448242188
Epoch 2220, val loss: 0.887175440788269
Epoch 2230, training loss: 90.40296173095703 = 0.016943901777267456 + 10.0 * 9.038601875305176
Epoch 2230, val loss: 0.8902044892311096
Epoch 2240, training loss: 90.406982421875 = 0.016649460420012474 + 10.0 * 9.039033889770508
Epoch 2240, val loss: 0.8942119479179382
Epoch 2250, training loss: 90.3687744140625 = 0.016354547813534737 + 10.0 * 9.035242080688477
Epoch 2250, val loss: 0.8977207541465759
Epoch 2260, training loss: 90.36985778808594 = 0.016071749851107597 + 10.0 * 9.035378456115723
Epoch 2260, val loss: 0.9003377556800842
Epoch 2270, training loss: 90.3638916015625 = 0.015794895589351654 + 10.0 * 9.034810066223145
Epoch 2270, val loss: 0.904060959815979
Epoch 2280, training loss: 90.36263275146484 = 0.01552603580057621 + 10.0 * 9.034710884094238
Epoch 2280, val loss: 0.9073874354362488
Epoch 2290, training loss: 90.36588287353516 = 0.015265543945133686 + 10.0 * 9.035061836242676
Epoch 2290, val loss: 0.9104620218276978
Epoch 2300, training loss: 90.39344787597656 = 0.015018204227089882 + 10.0 * 9.037843704223633
Epoch 2300, val loss: 0.9139837026596069
Epoch 2310, training loss: 90.36615753173828 = 0.014763463288545609 + 10.0 * 9.035139083862305
Epoch 2310, val loss: 0.9170010685920715
Epoch 2320, training loss: 90.36276245117188 = 0.014519820921123028 + 10.0 * 9.03482437133789
Epoch 2320, val loss: 0.919928789138794
Epoch 2330, training loss: 90.35610961914062 = 0.01427984144538641 + 10.0 * 9.03418254852295
Epoch 2330, val loss: 0.9232997298240662
Epoch 2340, training loss: 90.36250305175781 = 0.01404989417642355 + 10.0 * 9.034845352172852
Epoch 2340, val loss: 0.9261172413825989
Epoch 2350, training loss: 90.36771392822266 = 0.013823521323502064 + 10.0 * 9.035388946533203
Epoch 2350, val loss: 0.9294947981834412
Epoch 2360, training loss: 90.35431671142578 = 0.013601048849523067 + 10.0 * 9.03407096862793
Epoch 2360, val loss: 0.932334303855896
Epoch 2370, training loss: 90.36060333251953 = 0.013388780876994133 + 10.0 * 9.034721374511719
Epoch 2370, val loss: 0.9356895089149475
Epoch 2380, training loss: 90.35592651367188 = 0.013176494278013706 + 10.0 * 9.03427505493164
Epoch 2380, val loss: 0.9384058117866516
Epoch 2390, training loss: 90.35750579833984 = 0.012971366755664349 + 10.0 * 9.034453392028809
Epoch 2390, val loss: 0.9414510726928711
Epoch 2400, training loss: 90.36567687988281 = 0.012773649767041206 + 10.0 * 9.035290718078613
Epoch 2400, val loss: 0.9449664354324341
Epoch 2410, training loss: 90.36860656738281 = 0.01258092187345028 + 10.0 * 9.035602569580078
Epoch 2410, val loss: 0.9482702016830444
Epoch 2420, training loss: 90.3499984741211 = 0.012384057976305485 + 10.0 * 9.033761978149414
Epoch 2420, val loss: 0.9495279788970947
Epoch 2430, training loss: 90.34275817871094 = 0.01219498086720705 + 10.0 * 9.033056259155273
Epoch 2430, val loss: 0.9527127742767334
Epoch 2440, training loss: 90.34378814697266 = 0.012013658881187439 + 10.0 * 9.033177375793457
Epoch 2440, val loss: 0.9552527666091919
Epoch 2450, training loss: 90.38094329833984 = 0.011844240128993988 + 10.0 * 9.036910057067871
Epoch 2450, val loss: 0.957789421081543
Epoch 2460, training loss: 90.35388946533203 = 0.011668430641293526 + 10.0 * 9.034222602844238
Epoch 2460, val loss: 0.961275041103363
Epoch 2470, training loss: 90.34355163574219 = 0.011493168771266937 + 10.0 * 9.03320598602295
Epoch 2470, val loss: 0.9634608626365662
Epoch 2480, training loss: 90.34383392333984 = 0.011328211985528469 + 10.0 * 9.03325080871582
Epoch 2480, val loss: 0.9665927886962891
Epoch 2490, training loss: 90.35147857666016 = 0.011167317628860474 + 10.0 * 9.03403091430664
Epoch 2490, val loss: 0.9689126014709473
Epoch 2500, training loss: 90.34165954589844 = 0.011007172986865044 + 10.0 * 9.033064842224121
Epoch 2500, val loss: 0.9711852669715881
Epoch 2510, training loss: 90.34090423583984 = 0.010852322913706303 + 10.0 * 9.033005714416504
Epoch 2510, val loss: 0.9741806983947754
Epoch 2520, training loss: 90.33786010742188 = 0.010701130144298077 + 10.0 * 9.032715797424316
Epoch 2520, val loss: 0.977011501789093
Epoch 2530, training loss: 90.35323333740234 = 0.010557648725807667 + 10.0 * 9.03426742553711
Epoch 2530, val loss: 0.9796757102012634
Epoch 2540, training loss: 90.33228302001953 = 0.010408087633550167 + 10.0 * 9.032187461853027
Epoch 2540, val loss: 0.9808759689331055
Epoch 2550, training loss: 90.3445816040039 = 0.010269127786159515 + 10.0 * 9.033431053161621
Epoch 2550, val loss: 0.983756959438324
Epoch 2560, training loss: 90.33027648925781 = 0.01012857910245657 + 10.0 * 9.032014846801758
Epoch 2560, val loss: 0.9852930903434753
Epoch 2570, training loss: 90.32889556884766 = 0.009993055835366249 + 10.0 * 9.031889915466309
Epoch 2570, val loss: 0.9882969856262207
Epoch 2580, training loss: 90.33342742919922 = 0.00986271072179079 + 10.0 * 9.032356262207031
Epoch 2580, val loss: 0.990604043006897
Epoch 2590, training loss: 90.33565521240234 = 0.009734841994941235 + 10.0 * 9.032591819763184
Epoch 2590, val loss: 0.9931199550628662
Epoch 2600, training loss: 90.32918548583984 = 0.009606405161321163 + 10.0 * 9.031957626342773
Epoch 2600, val loss: 0.9950480461120605
Epoch 2610, training loss: 90.3389663696289 = 0.0094845499843359 + 10.0 * 9.03294849395752
Epoch 2610, val loss: 0.9970368146896362
Epoch 2620, training loss: 90.33644104003906 = 0.009363166987895966 + 10.0 * 9.032708168029785
Epoch 2620, val loss: 0.999170184135437
Epoch 2630, training loss: 90.32022857666016 = 0.009241648949682713 + 10.0 * 9.031099319458008
Epoch 2630, val loss: 1.001855492591858
Epoch 2640, training loss: 90.31787872314453 = 0.00912541151046753 + 10.0 * 9.030875205993652
Epoch 2640, val loss: 1.0036334991455078
Epoch 2650, training loss: 90.31623077392578 = 0.00901069026440382 + 10.0 * 9.030721664428711
Epoch 2650, val loss: 1.0058977603912354
Epoch 2660, training loss: 90.35939025878906 = 0.008907425217330456 + 10.0 * 9.03504753112793
Epoch 2660, val loss: 1.0086809396743774
Epoch 2670, training loss: 90.33644104003906 = 0.008795421570539474 + 10.0 * 9.032764434814453
Epoch 2670, val loss: 1.010834813117981
Epoch 2680, training loss: 90.32044982910156 = 0.008685526438057423 + 10.0 * 9.031176567077637
Epoch 2680, val loss: 1.0123382806777954
Epoch 2690, training loss: 90.31208038330078 = 0.008579195477068424 + 10.0 * 9.030350685119629
Epoch 2690, val loss: 1.0144951343536377
Epoch 2700, training loss: 90.320068359375 = 0.00847938098013401 + 10.0 * 9.031159400939941
Epoch 2700, val loss: 1.0166181325912476
Epoch 2710, training loss: 90.32371520996094 = 0.00837768241763115 + 10.0 * 9.031534194946289
Epoch 2710, val loss: 1.0181267261505127
Epoch 2720, training loss: 90.31085205078125 = 0.008278523571789265 + 10.0 * 9.030257225036621
Epoch 2720, val loss: 1.0202528238296509
Epoch 2730, training loss: 90.30787658691406 = 0.008182032033801079 + 10.0 * 9.029970169067383
Epoch 2730, val loss: 1.0221258401870728
Epoch 2740, training loss: 90.30834197998047 = 0.008087728172540665 + 10.0 * 9.030025482177734
Epoch 2740, val loss: 1.0244672298431396
Epoch 2750, training loss: 90.32933807373047 = 0.007999785244464874 + 10.0 * 9.032133102416992
Epoch 2750, val loss: 1.026525855064392
Epoch 2760, training loss: 90.31015014648438 = 0.007907398045063019 + 10.0 * 9.030224800109863
Epoch 2760, val loss: 1.0277048349380493
Epoch 2770, training loss: 90.31700897216797 = 0.007820075377821922 + 10.0 * 9.030919075012207
Epoch 2770, val loss: 1.029532790184021
Epoch 2780, training loss: 90.31796264648438 = 0.0077347042970359325 + 10.0 * 9.031023025512695
Epoch 2780, val loss: 1.0306187868118286
Epoch 2790, training loss: 90.30706024169922 = 0.007647047750651836 + 10.0 * 9.02994155883789
Epoch 2790, val loss: 1.0333199501037598
Epoch 2800, training loss: 90.30943298339844 = 0.007565766107290983 + 10.0 * 9.030186653137207
Epoch 2800, val loss: 1.0355805158615112
Epoch 2810, training loss: 90.30962371826172 = 0.007483827881515026 + 10.0 * 9.030214309692383
Epoch 2810, val loss: 1.036564826965332
Epoch 2820, training loss: 90.31129455566406 = 0.007404747419059277 + 10.0 * 9.030388832092285
Epoch 2820, val loss: 1.0385171175003052
Epoch 2830, training loss: 90.29854583740234 = 0.007323901634663343 + 10.0 * 9.029122352600098
Epoch 2830, val loss: 1.0407114028930664
Epoch 2840, training loss: 90.29769897460938 = 0.00724665867164731 + 10.0 * 9.029045104980469
Epoch 2840, val loss: 1.0422956943511963
Epoch 2850, training loss: 90.30154418945312 = 0.007172225043177605 + 10.0 * 9.029437065124512
Epoch 2850, val loss: 1.043856143951416
Epoch 2860, training loss: 90.31031036376953 = 0.007100895047187805 + 10.0 * 9.03032112121582
Epoch 2860, val loss: 1.0448968410491943
Epoch 2870, training loss: 90.31031036376953 = 0.007027418352663517 + 10.0 * 9.030328750610352
Epoch 2870, val loss: 1.047093152999878
Epoch 2880, training loss: 90.30115509033203 = 0.006955712102353573 + 10.0 * 9.029419898986816
Epoch 2880, val loss: 1.0493955612182617
Epoch 2890, training loss: 90.29339599609375 = 0.006884464994072914 + 10.0 * 9.028651237487793
Epoch 2890, val loss: 1.0506951808929443
Epoch 2900, training loss: 90.29399871826172 = 0.0068161943927407265 + 10.0 * 9.028718948364258
Epoch 2900, val loss: 1.0520250797271729
Epoch 2910, training loss: 90.30646514892578 = 0.0067513734102249146 + 10.0 * 9.0299711227417
Epoch 2910, val loss: 1.0539705753326416
Epoch 2920, training loss: 90.31610107421875 = 0.006687195505946875 + 10.0 * 9.030941009521484
Epoch 2920, val loss: 1.0562342405319214
Epoch 2930, training loss: 90.29759979248047 = 0.006620049476623535 + 10.0 * 9.029097557067871
Epoch 2930, val loss: 1.0570896863937378
Epoch 2940, training loss: 90.29116821289062 = 0.006555254105478525 + 10.0 * 9.028461456298828
Epoch 2940, val loss: 1.0583854913711548
Epoch 2950, training loss: 90.28809356689453 = 0.006493539549410343 + 10.0 * 9.028160095214844
Epoch 2950, val loss: 1.0597542524337769
Epoch 2960, training loss: 90.31086730957031 = 0.0064365374855697155 + 10.0 * 9.03044319152832
Epoch 2960, val loss: 1.0610897541046143
Epoch 2970, training loss: 90.28469848632812 = 0.0063724578358232975 + 10.0 * 9.027832984924316
Epoch 2970, val loss: 1.0634745359420776
Epoch 2980, training loss: 90.2833023071289 = 0.006313449703156948 + 10.0 * 9.027698516845703
Epoch 2980, val loss: 1.06443190574646
Epoch 2990, training loss: 90.29450225830078 = 0.006257312372326851 + 10.0 * 9.028824806213379
Epoch 2990, val loss: 1.0658320188522339
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8473
Overall ASR: 0.6724
Flip ASR: 0.5920/1554 nodes
The final ASR:0.69439, 0.01706, Accuracy:0.84610, 0.00322
#Attach Nodes:5
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 97556])
remove edge: torch.Size([2, 79582])
updated graph: torch.Size([2, 88490])
./selected_nodes/Pubmed/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8503
Overall ASR: 0.6998
Flip ASR: 0.6287/1554 nodes
./selected_nodes/Pubmed/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8519
Overall ASR: 0.7414
Flip ASR: 0.6795/1554 nodes
./selected_nodes/Pubmed/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8498
Overall ASR: 0.7252
Flip ASR: 0.6589/1554 nodes
The final ASR:0.72211, 0.01711, Accuracy:0.85067, 0.00086
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.3 der2:0.3 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 104.6984634399414 = 1.108312964439392 + 10.0 * 10.359014511108398
Epoch 0, val loss: 1.1074904203414917
Epoch 10, training loss: 104.65074920654297 = 1.094443917274475 + 10.0 * 10.355630874633789
Epoch 10, val loss: 1.09329092502594
Epoch 20, training loss: 104.12652587890625 = 1.0769658088684082 + 10.0 * 10.304956436157227
Epoch 20, val loss: 1.0758423805236816
Epoch 30, training loss: 99.59121704101562 = 1.0634965896606445 + 10.0 * 9.852771759033203
Epoch 30, val loss: 1.0623631477355957
Epoch 40, training loss: 96.57441711425781 = 1.047160029411316 + 10.0 * 9.552725791931152
Epoch 40, val loss: 1.045691728591919
Epoch 50, training loss: 95.51521301269531 = 1.029010534286499 + 10.0 * 9.448620796203613
Epoch 50, val loss: 1.0276609659194946
Epoch 60, training loss: 95.07090759277344 = 1.0107064247131348 + 10.0 * 9.406020164489746
Epoch 60, val loss: 1.009468674659729
Epoch 70, training loss: 94.76669311523438 = 0.992841899394989 + 10.0 * 9.377385139465332
Epoch 70, val loss: 0.9920200109481812
Epoch 80, training loss: 94.39755249023438 = 0.9792831540107727 + 10.0 * 9.341826438903809
Epoch 80, val loss: 0.9790429472923279
Epoch 90, training loss: 93.8020248413086 = 0.968959629535675 + 10.0 * 9.283307075500488
Epoch 90, val loss: 0.9690360426902771
Epoch 100, training loss: 93.2595443725586 = 0.9590593576431274 + 10.0 * 9.230048179626465
Epoch 100, val loss: 0.9594713449478149
Epoch 110, training loss: 92.88569641113281 = 0.9493116140365601 + 10.0 * 9.193638801574707
Epoch 110, val loss: 0.9500247240066528
Epoch 120, training loss: 92.63823699951172 = 0.9384806156158447 + 10.0 * 9.169975280761719
Epoch 120, val loss: 0.9394073486328125
Epoch 130, training loss: 92.45939636230469 = 0.9240095615386963 + 10.0 * 9.153538703918457
Epoch 130, val loss: 0.9250229597091675
Epoch 140, training loss: 92.31294250488281 = 0.9064988493919373 + 10.0 * 9.140644073486328
Epoch 140, val loss: 0.9080525040626526
Epoch 150, training loss: 92.20880889892578 = 0.886394739151001 + 10.0 * 9.132241249084473
Epoch 150, val loss: 0.8885029554367065
Epoch 160, training loss: 92.12333679199219 = 0.86274653673172 + 10.0 * 9.126058578491211
Epoch 160, val loss: 0.8654801845550537
Epoch 170, training loss: 92.06183624267578 = 0.8356473445892334 + 10.0 * 9.122618675231934
Epoch 170, val loss: 0.8392109274864197
Epoch 180, training loss: 91.97943115234375 = 0.805925190448761 + 10.0 * 9.117350578308105
Epoch 180, val loss: 0.8104889392852783
Epoch 190, training loss: 91.90571594238281 = 0.77359539270401 + 10.0 * 9.113211631774902
Epoch 190, val loss: 0.7793986797332764
Epoch 200, training loss: 91.8344497680664 = 0.7386239767074585 + 10.0 * 9.109582901000977
Epoch 200, val loss: 0.7459586262702942
Epoch 210, training loss: 91.76773071289062 = 0.7018619179725647 + 10.0 * 9.106587409973145
Epoch 210, val loss: 0.7109456062316895
Epoch 220, training loss: 91.69586181640625 = 0.6641415357589722 + 10.0 * 9.103172302246094
Epoch 220, val loss: 0.6753244996070862
Epoch 230, training loss: 91.63059997558594 = 0.6262975335121155 + 10.0 * 9.100430488586426
Epoch 230, val loss: 0.6399881839752197
Epoch 240, training loss: 91.57542419433594 = 0.5899211168289185 + 10.0 * 9.098550796508789
Epoch 240, val loss: 0.6063359379768372
Epoch 250, training loss: 91.51172637939453 = 0.5560047626495361 + 10.0 * 9.095571517944336
Epoch 250, val loss: 0.5752747058868408
Epoch 260, training loss: 91.45659637451172 = 0.5247748494148254 + 10.0 * 9.093182563781738
Epoch 260, val loss: 0.5470983982086182
Epoch 270, training loss: 91.45146179199219 = 0.49676743149757385 + 10.0 * 9.09546947479248
Epoch 270, val loss: 0.5221269726753235
Epoch 280, training loss: 91.37396240234375 = 0.47259795665740967 + 10.0 * 9.090136528015137
Epoch 280, val loss: 0.50115966796875
Epoch 290, training loss: 91.32567596435547 = 0.4519861042499542 + 10.0 * 9.087368965148926
Epoch 290, val loss: 0.48345687985420227
Epoch 300, training loss: 91.28475952148438 = 0.4340379238128662 + 10.0 * 9.08507251739502
Epoch 300, val loss: 0.4684813916683197
Epoch 310, training loss: 91.2667236328125 = 0.41833946108818054 + 10.0 * 9.084837913513184
Epoch 310, val loss: 0.4556432366371155
Epoch 320, training loss: 91.2359390258789 = 0.40483900904655457 + 10.0 * 9.083109855651855
Epoch 320, val loss: 0.4449933171272278
Epoch 330, training loss: 91.19330596923828 = 0.3934982419013977 + 10.0 * 9.079980850219727
Epoch 330, val loss: 0.43620774149894714
Epoch 340, training loss: 91.16580200195312 = 0.38372156023979187 + 10.0 * 9.078207969665527
Epoch 340, val loss: 0.4289693236351013
Epoch 350, training loss: 91.14291381835938 = 0.3749740719795227 + 10.0 * 9.076793670654297
Epoch 350, val loss: 0.4227089583873749
Epoch 360, training loss: 91.14292907714844 = 0.3670536279678345 + 10.0 * 9.077588081359863
Epoch 360, val loss: 0.4172031879425049
Epoch 370, training loss: 91.11605072021484 = 0.36004671454429626 + 10.0 * 9.075600624084473
Epoch 370, val loss: 0.4125420153141022
Epoch 380, training loss: 91.0925521850586 = 0.35387349128723145 + 10.0 * 9.073867797851562
Epoch 380, val loss: 0.40870851278305054
Epoch 390, training loss: 91.06478118896484 = 0.34833458065986633 + 10.0 * 9.07164478302002
Epoch 390, val loss: 0.40541955828666687
Epoch 400, training loss: 91.0492935180664 = 0.3431813418865204 + 10.0 * 9.070611953735352
Epoch 400, val loss: 0.40245869755744934
Epoch 410, training loss: 91.03925323486328 = 0.33834248781204224 + 10.0 * 9.070091247558594
Epoch 410, val loss: 0.39984211325645447
Epoch 420, training loss: 91.03375244140625 = 0.33384624123573303 + 10.0 * 9.069990158081055
Epoch 420, val loss: 0.39739498496055603
Epoch 430, training loss: 91.01114654541016 = 0.3297311067581177 + 10.0 * 9.068140983581543
Epoch 430, val loss: 0.3954215943813324
Epoch 440, training loss: 90.99642944335938 = 0.3258945643901825 + 10.0 * 9.067052841186523
Epoch 440, val loss: 0.3936662971973419
Epoch 450, training loss: 90.98772430419922 = 0.3222463130950928 + 10.0 * 9.066548347473145
Epoch 450, val loss: 0.39210283756256104
Epoch 460, training loss: 90.9871597290039 = 0.31876739859580994 + 10.0 * 9.066839218139648
Epoch 460, val loss: 0.3907228708267212
Epoch 470, training loss: 90.97232818603516 = 0.3154718577861786 + 10.0 * 9.065685272216797
Epoch 470, val loss: 0.3894469141960144
Epoch 480, training loss: 90.96083068847656 = 0.3123272657394409 + 10.0 * 9.064850807189941
Epoch 480, val loss: 0.38825029134750366
Epoch 490, training loss: 90.95144653320312 = 0.3093240261077881 + 10.0 * 9.06421184539795
Epoch 490, val loss: 0.38728249073028564
Epoch 500, training loss: 90.93905639648438 = 0.30642709136009216 + 10.0 * 9.063262939453125
Epoch 500, val loss: 0.38631030917167664
Epoch 510, training loss: 90.93671417236328 = 0.30364200472831726 + 10.0 * 9.06330680847168
Epoch 510, val loss: 0.38555634021759033
Epoch 520, training loss: 90.92345428466797 = 0.300986111164093 + 10.0 * 9.062246322631836
Epoch 520, val loss: 0.3847994804382324
Epoch 530, training loss: 90.90873718261719 = 0.2984126806259155 + 10.0 * 9.06103229522705
Epoch 530, val loss: 0.3842262327671051
Epoch 540, training loss: 90.94463348388672 = 0.2959064841270447 + 10.0 * 9.064872741699219
Epoch 540, val loss: 0.383669376373291
Epoch 550, training loss: 90.89594268798828 = 0.29348990321159363 + 10.0 * 9.060245513916016
Epoch 550, val loss: 0.3832217752933502
Epoch 560, training loss: 90.88579559326172 = 0.29115718603134155 + 10.0 * 9.059463500976562
Epoch 560, val loss: 0.3827984035015106
Epoch 570, training loss: 90.87883758544922 = 0.2888641953468323 + 10.0 * 9.05899715423584
Epoch 570, val loss: 0.38248971104621887
Epoch 580, training loss: 90.87680053710938 = 0.28660455346107483 + 10.0 * 9.059019088745117
Epoch 580, val loss: 0.3822422921657562
Epoch 590, training loss: 90.89090728759766 = 0.28440582752227783 + 10.0 * 9.060649871826172
Epoch 590, val loss: 0.38192692399024963
Epoch 600, training loss: 90.8669662475586 = 0.28231409192085266 + 10.0 * 9.058465957641602
Epoch 600, val loss: 0.3817267417907715
Epoch 610, training loss: 90.85514831542969 = 0.28026601672172546 + 10.0 * 9.057488441467285
Epoch 610, val loss: 0.3816721737384796
Epoch 620, training loss: 90.84551239013672 = 0.2782350778579712 + 10.0 * 9.056727409362793
Epoch 620, val loss: 0.3816264569759369
Epoch 630, training loss: 90.8770523071289 = 0.27622440457344055 + 10.0 * 9.060083389282227
Epoch 630, val loss: 0.38157638907432556
Epoch 640, training loss: 90.84042358398438 = 0.27427586913108826 + 10.0 * 9.056614875793457
Epoch 640, val loss: 0.3816366195678711
Epoch 650, training loss: 90.82613372802734 = 0.27236291766166687 + 10.0 * 9.055377006530762
Epoch 650, val loss: 0.3817259669303894
Epoch 660, training loss: 90.81890869140625 = 0.27046579122543335 + 10.0 * 9.054844856262207
Epoch 660, val loss: 0.38184037804603577
Epoch 670, training loss: 90.84201049804688 = 0.2685914933681488 + 10.0 * 9.057341575622559
Epoch 670, val loss: 0.3821270763874054
Epoch 680, training loss: 90.82514953613281 = 0.2667568325996399 + 10.0 * 9.055839538574219
Epoch 680, val loss: 0.3822305202484131
Epoch 690, training loss: 90.80726623535156 = 0.2649756371974945 + 10.0 * 9.054228782653809
Epoch 690, val loss: 0.3823661804199219
Epoch 700, training loss: 90.79719543457031 = 0.2632106840610504 + 10.0 * 9.053398132324219
Epoch 700, val loss: 0.3827056586742401
Epoch 710, training loss: 90.79196166992188 = 0.26145046949386597 + 10.0 * 9.053050994873047
Epoch 710, val loss: 0.3829766809940338
Epoch 720, training loss: 90.80396270751953 = 0.2597101628780365 + 10.0 * 9.054425239562988
Epoch 720, val loss: 0.3833789527416229
Epoch 730, training loss: 90.78341674804688 = 0.2580120861530304 + 10.0 * 9.05254077911377
Epoch 730, val loss: 0.3837086856365204
Epoch 740, training loss: 90.79398345947266 = 0.25634026527404785 + 10.0 * 9.053764343261719
Epoch 740, val loss: 0.3842775523662567
Epoch 750, training loss: 90.77057647705078 = 0.2546931505203247 + 10.0 * 9.05158805847168
Epoch 750, val loss: 0.38468003273010254
Epoch 760, training loss: 90.76367950439453 = 0.2530621290206909 + 10.0 * 9.051061630249023
Epoch 760, val loss: 0.3851681649684906
Epoch 770, training loss: 90.75765228271484 = 0.25144344568252563 + 10.0 * 9.050621032714844
Epoch 770, val loss: 0.38573744893074036
Epoch 780, training loss: 90.7562026977539 = 0.24983258545398712 + 10.0 * 9.050637245178223
Epoch 780, val loss: 0.3862776756286621
Epoch 790, training loss: 90.76187133789062 = 0.24824827909469604 + 10.0 * 9.051362991333008
Epoch 790, val loss: 0.38712456822395325
Epoch 800, training loss: 90.74772644042969 = 0.24671247601509094 + 10.0 * 9.050101280212402
Epoch 800, val loss: 0.3875393569469452
Epoch 810, training loss: 90.73837280273438 = 0.24519528448581696 + 10.0 * 9.049318313598633
Epoch 810, val loss: 0.3882911801338196
Epoch 820, training loss: 90.73353576660156 = 0.24367941915988922 + 10.0 * 9.048985481262207
Epoch 820, val loss: 0.38900646567344666
Epoch 830, training loss: 90.72923278808594 = 0.2421683967113495 + 10.0 * 9.048707008361816
Epoch 830, val loss: 0.38976073265075684
Epoch 840, training loss: 90.7811279296875 = 0.2406749278306961 + 10.0 * 9.054044723510742
Epoch 840, val loss: 0.3906155228614807
Epoch 850, training loss: 90.7297592163086 = 0.2392074018716812 + 10.0 * 9.049055099487305
Epoch 850, val loss: 0.3913489282131195
Epoch 860, training loss: 90.72093963623047 = 0.23776328563690186 + 10.0 * 9.048317909240723
Epoch 860, val loss: 0.392196923494339
Epoch 870, training loss: 90.71568298339844 = 0.236325204372406 + 10.0 * 9.047935485839844
Epoch 870, val loss: 0.39310580492019653
Epoch 880, training loss: 90.7201919555664 = 0.2348935604095459 + 10.0 * 9.048529624938965
Epoch 880, val loss: 0.3939933478832245
Epoch 890, training loss: 90.70389556884766 = 0.23347556591033936 + 10.0 * 9.047041893005371
Epoch 890, val loss: 0.3950069546699524
Epoch 900, training loss: 90.70098114013672 = 0.23207038640975952 + 10.0 * 9.046891212463379
Epoch 900, val loss: 0.3959449827671051
Epoch 910, training loss: 90.72402954101562 = 0.2306719720363617 + 10.0 * 9.049335479736328
Epoch 910, val loss: 0.39690446853637695
Epoch 920, training loss: 90.70391845703125 = 0.2292998880147934 + 10.0 * 9.04746150970459
Epoch 920, val loss: 0.3980787396430969
Epoch 930, training loss: 90.6901626586914 = 0.22793594002723694 + 10.0 * 9.046222686767578
Epoch 930, val loss: 0.39908885955810547
Epoch 940, training loss: 90.68557739257812 = 0.22658133506774902 + 10.0 * 9.045900344848633
Epoch 940, val loss: 0.4002283811569214
Epoch 950, training loss: 90.70974731445312 = 0.22523629665374756 + 10.0 * 9.04845142364502
Epoch 950, val loss: 0.4014156758785248
Epoch 960, training loss: 90.68106079101562 = 0.2239052653312683 + 10.0 * 9.04571533203125
Epoch 960, val loss: 0.40244704484939575
Epoch 970, training loss: 90.6761245727539 = 0.22259019315242767 + 10.0 * 9.045353889465332
Epoch 970, val loss: 0.40368515253067017
Epoch 980, training loss: 90.69312286376953 = 0.22128543257713318 + 10.0 * 9.047183990478516
Epoch 980, val loss: 0.4049376845359802
Epoch 990, training loss: 90.67333221435547 = 0.21999365091323853 + 10.0 * 9.045333862304688
Epoch 990, val loss: 0.40617284178733826
Epoch 1000, training loss: 90.66451263427734 = 0.21871134638786316 + 10.0 * 9.044580459594727
Epoch 1000, val loss: 0.40741807222366333
Epoch 1010, training loss: 90.66216278076172 = 0.217435821890831 + 10.0 * 9.044472694396973
Epoch 1010, val loss: 0.408732146024704
Epoch 1020, training loss: 90.6709213256836 = 0.21616952121257782 + 10.0 * 9.045475006103516
Epoch 1020, val loss: 0.4100511074066162
Epoch 1030, training loss: 90.6691665649414 = 0.214913010597229 + 10.0 * 9.045425415039062
Epoch 1030, val loss: 0.41122323274612427
Epoch 1040, training loss: 90.65254974365234 = 0.21367481350898743 + 10.0 * 9.0438871383667
Epoch 1040, val loss: 0.4127170145511627
Epoch 1050, training loss: 90.64531707763672 = 0.21243885159492493 + 10.0 * 9.04328727722168
Epoch 1050, val loss: 0.4139822721481323
Epoch 1060, training loss: 90.66230773925781 = 0.21121324598789215 + 10.0 * 9.045109748840332
Epoch 1060, val loss: 0.41542088985443115
Epoch 1070, training loss: 90.6493148803711 = 0.21000511944293976 + 10.0 * 9.043931007385254
Epoch 1070, val loss: 0.4168463945388794
Epoch 1080, training loss: 90.63924407958984 = 0.2088010460138321 + 10.0 * 9.043044090270996
Epoch 1080, val loss: 0.4182392954826355
Epoch 1090, training loss: 90.63050842285156 = 0.2076069712638855 + 10.0 * 9.042290687561035
Epoch 1090, val loss: 0.41965827345848083
Epoch 1100, training loss: 90.62987518310547 = 0.20641514658927917 + 10.0 * 9.042346000671387
Epoch 1100, val loss: 0.42111173272132874
Epoch 1110, training loss: 90.6434555053711 = 0.20523105561733246 + 10.0 * 9.043822288513184
Epoch 1110, val loss: 0.42265433073043823
Epoch 1120, training loss: 90.6435775756836 = 0.20405903458595276 + 10.0 * 9.043951988220215
Epoch 1120, val loss: 0.4241240918636322
Epoch 1130, training loss: 90.6259765625 = 0.20289373397827148 + 10.0 * 9.04230785369873
Epoch 1130, val loss: 0.4255768656730652
Epoch 1140, training loss: 90.61270141601562 = 0.20173943042755127 + 10.0 * 9.041096687316895
Epoch 1140, val loss: 0.42707329988479614
Epoch 1150, training loss: 90.61058807373047 = 0.20058849453926086 + 10.0 * 9.040999412536621
Epoch 1150, val loss: 0.4286904036998749
Epoch 1160, training loss: 90.62026977539062 = 0.19944354891777039 + 10.0 * 9.042082786560059
Epoch 1160, val loss: 0.43036597967147827
Epoch 1170, training loss: 90.60566711425781 = 0.19829991459846497 + 10.0 * 9.040736198425293
Epoch 1170, val loss: 0.4318283796310425
Epoch 1180, training loss: 90.62078094482422 = 0.19716952741146088 + 10.0 * 9.04236125946045
Epoch 1180, val loss: 0.4335186779499054
Epoch 1190, training loss: 90.60233306884766 = 0.19604995846748352 + 10.0 * 9.040628433227539
Epoch 1190, val loss: 0.4349913001060486
Epoch 1200, training loss: 90.5967025756836 = 0.19493378698825836 + 10.0 * 9.040177345275879
Epoch 1200, val loss: 0.4367469847202301
Epoch 1210, training loss: 90.59130096435547 = 0.19381991028785706 + 10.0 * 9.039748191833496
Epoch 1210, val loss: 0.43831321597099304
Epoch 1220, training loss: 90.59004211425781 = 0.19270795583724976 + 10.0 * 9.039732933044434
Epoch 1220, val loss: 0.44008511304855347
Epoch 1230, training loss: 90.62271118164062 = 0.1916019767522812 + 10.0 * 9.043110847473145
Epoch 1230, val loss: 0.44184058904647827
Epoch 1240, training loss: 90.58550262451172 = 0.1905103325843811 + 10.0 * 9.039499282836914
Epoch 1240, val loss: 0.4435308575630188
Epoch 1250, training loss: 90.5853500366211 = 0.18942461907863617 + 10.0 * 9.039592742919922
Epoch 1250, val loss: 0.44524312019348145
Epoch 1260, training loss: 90.58182525634766 = 0.18833430111408234 + 10.0 * 9.039349555969238
Epoch 1260, val loss: 0.4470130205154419
Epoch 1270, training loss: 90.58328247070312 = 0.18725037574768066 + 10.0 * 9.039603233337402
Epoch 1270, val loss: 0.44876983761787415
Epoch 1280, training loss: 90.572021484375 = 0.18617480993270874 + 10.0 * 9.03858470916748
Epoch 1280, val loss: 0.45059487223625183
Epoch 1290, training loss: 90.57817077636719 = 0.18510006368160248 + 10.0 * 9.039307594299316
Epoch 1290, val loss: 0.45251011848449707
Epoch 1300, training loss: 90.56721496582031 = 0.18402652442455292 + 10.0 * 9.038318634033203
Epoch 1300, val loss: 0.45418113470077515
Epoch 1310, training loss: 90.56210327148438 = 0.18296130001544952 + 10.0 * 9.037914276123047
Epoch 1310, val loss: 0.45610564947128296
Epoch 1320, training loss: 90.56326293945312 = 0.18189750611782074 + 10.0 * 9.03813648223877
Epoch 1320, val loss: 0.4579331874847412
Epoch 1330, training loss: 90.5702896118164 = 0.18083634972572327 + 10.0 * 9.038945198059082
Epoch 1330, val loss: 0.4597947597503662
Epoch 1340, training loss: 90.56922149658203 = 0.179782897233963 + 10.0 * 9.03894329071045
Epoch 1340, val loss: 0.4616025686264038
Epoch 1350, training loss: 90.55921936035156 = 0.17872989177703857 + 10.0 * 9.038049697875977
Epoch 1350, val loss: 0.4636025130748749
Epoch 1360, training loss: 90.55142211914062 = 0.17768722772598267 + 10.0 * 9.037373542785645
Epoch 1360, val loss: 0.4655439555644989
Epoch 1370, training loss: 90.54713439941406 = 0.176639124751091 + 10.0 * 9.037050247192383
Epoch 1370, val loss: 0.46746116876602173
Epoch 1380, training loss: 90.55962371826172 = 0.1755957305431366 + 10.0 * 9.038402557373047
Epoch 1380, val loss: 0.46951824426651
Epoch 1390, training loss: 90.54877471923828 = 0.1745554655790329 + 10.0 * 9.037422180175781
Epoch 1390, val loss: 0.4714299440383911
Epoch 1400, training loss: 90.54576110839844 = 0.17352451384067535 + 10.0 * 9.037223815917969
Epoch 1400, val loss: 0.4735455811023712
Epoch 1410, training loss: 90.54227447509766 = 0.17249654233455658 + 10.0 * 9.036977767944336
Epoch 1410, val loss: 0.4755101501941681
Epoch 1420, training loss: 90.53215026855469 = 0.1714705377817154 + 10.0 * 9.036067962646484
Epoch 1420, val loss: 0.47764864563941956
Epoch 1430, training loss: 90.53071594238281 = 0.17044469714164734 + 10.0 * 9.036026954650879
Epoch 1430, val loss: 0.47971856594085693
Epoch 1440, training loss: 90.54100036621094 = 0.16942135989665985 + 10.0 * 9.037158012390137
Epoch 1440, val loss: 0.48199090361595154
Epoch 1450, training loss: 90.5271224975586 = 0.16839540004730225 + 10.0 * 9.035872459411621
Epoch 1450, val loss: 0.48410502076148987
Epoch 1460, training loss: 90.53007507324219 = 0.16737450659275055 + 10.0 * 9.036270141601562
Epoch 1460, val loss: 0.4862978756427765
Epoch 1470, training loss: 90.52647399902344 = 0.16635844111442566 + 10.0 * 9.036011695861816
Epoch 1470, val loss: 0.48838284611701965
Epoch 1480, training loss: 90.52523040771484 = 0.16534842550754547 + 10.0 * 9.035987854003906
Epoch 1480, val loss: 0.4906567931175232
Epoch 1490, training loss: 90.5225830078125 = 0.16433939337730408 + 10.0 * 9.0358247756958
Epoch 1490, val loss: 0.49285268783569336
Epoch 1500, training loss: 90.51347351074219 = 0.16333293914794922 + 10.0 * 9.035014152526855
Epoch 1500, val loss: 0.49502670764923096
Epoch 1510, training loss: 90.508544921875 = 0.16232790052890778 + 10.0 * 9.034621238708496
Epoch 1510, val loss: 0.4973202049732208
Epoch 1520, training loss: 90.51139068603516 = 0.16132502257823944 + 10.0 * 9.035006523132324
Epoch 1520, val loss: 0.4997095763683319
Epoch 1530, training loss: 90.5160903930664 = 0.1603211611509323 + 10.0 * 9.035576820373535
Epoch 1530, val loss: 0.5020231604576111
Epoch 1540, training loss: 90.50410461425781 = 0.15931424498558044 + 10.0 * 9.034479141235352
Epoch 1540, val loss: 0.5041995048522949
Epoch 1550, training loss: 90.50949096679688 = 0.15831655263900757 + 10.0 * 9.035117149353027
Epoch 1550, val loss: 0.5066355466842651
Epoch 1560, training loss: 90.50308227539062 = 0.15731613337993622 + 10.0 * 9.034576416015625
Epoch 1560, val loss: 0.5091320872306824
Epoch 1570, training loss: 90.49674224853516 = 0.15631476044654846 + 10.0 * 9.034043312072754
Epoch 1570, val loss: 0.5114356875419617
Epoch 1580, training loss: 90.4997787475586 = 0.15531809628009796 + 10.0 * 9.034445762634277
Epoch 1580, val loss: 0.5138936042785645
Epoch 1590, training loss: 90.49488830566406 = 0.15431836247444153 + 10.0 * 9.034056663513184
Epoch 1590, val loss: 0.5163000226020813
Epoch 1600, training loss: 90.50100708007812 = 0.15332379937171936 + 10.0 * 9.034769058227539
Epoch 1600, val loss: 0.5186742544174194
Epoch 1610, training loss: 90.48763275146484 = 0.15232981741428375 + 10.0 * 9.033530235290527
Epoch 1610, val loss: 0.5212868452072144
Epoch 1620, training loss: 90.49386596679688 = 0.1513383388519287 + 10.0 * 9.034253120422363
Epoch 1620, val loss: 0.523846447467804
Epoch 1630, training loss: 90.48097229003906 = 0.15034866333007812 + 10.0 * 9.033062934875488
Epoch 1630, val loss: 0.5262606143951416
Epoch 1640, training loss: 90.47848510742188 = 0.14935943484306335 + 10.0 * 9.032912254333496
Epoch 1640, val loss: 0.5287954807281494
Epoch 1650, training loss: 90.47977447509766 = 0.1483684480190277 + 10.0 * 9.033140182495117
Epoch 1650, val loss: 0.5314277410507202
Epoch 1660, training loss: 90.48794555664062 = 0.1473810076713562 + 10.0 * 9.034055709838867
Epoch 1660, val loss: 0.5339351296424866
Epoch 1670, training loss: 90.47650909423828 = 0.14638973772525787 + 10.0 * 9.033011436462402
Epoch 1670, val loss: 0.5363969206809998
Epoch 1680, training loss: 90.46803283691406 = 0.145400732755661 + 10.0 * 9.032262802124023
Epoch 1680, val loss: 0.5391706228256226
Epoch 1690, training loss: 90.4676284790039 = 0.14441345632076263 + 10.0 * 9.032320976257324
Epoch 1690, val loss: 0.541718065738678
Epoch 1700, training loss: 90.48797607421875 = 0.14342771470546722 + 10.0 * 9.034455299377441
Epoch 1700, val loss: 0.5443608164787292
Epoch 1710, training loss: 90.4804916381836 = 0.14244817197322845 + 10.0 * 9.033803939819336
Epoch 1710, val loss: 0.5471972823143005
Epoch 1720, training loss: 90.46231842041016 = 0.14146044850349426 + 10.0 * 9.032085418701172
Epoch 1720, val loss: 0.5496716499328613
Epoch 1730, training loss: 90.45543670654297 = 0.14047640562057495 + 10.0 * 9.031496047973633
Epoch 1730, val loss: 0.5524261593818665
Epoch 1740, training loss: 90.4542007446289 = 0.13949395716190338 + 10.0 * 9.03147029876709
Epoch 1740, val loss: 0.5550930500030518
Epoch 1750, training loss: 90.4889907836914 = 0.13851432502269745 + 10.0 * 9.03504753112793
Epoch 1750, val loss: 0.5579853057861328
Epoch 1760, training loss: 90.45457458496094 = 0.1375388205051422 + 10.0 * 9.03170394897461
Epoch 1760, val loss: 0.560880720615387
Epoch 1770, training loss: 90.44586181640625 = 0.1365528106689453 + 10.0 * 9.03093147277832
Epoch 1770, val loss: 0.5636165142059326
Epoch 1780, training loss: 90.45785522460938 = 0.1355748027563095 + 10.0 * 9.032228469848633
Epoch 1780, val loss: 0.5664258599281311
Epoch 1790, training loss: 90.4427490234375 = 0.13459566235542297 + 10.0 * 9.030815124511719
Epoch 1790, val loss: 0.569226086139679
Epoch 1800, training loss: 90.43859100341797 = 0.13361592590808868 + 10.0 * 9.030497550964355
Epoch 1800, val loss: 0.5721264481544495
Epoch 1810, training loss: 90.44074249267578 = 0.13263580203056335 + 10.0 * 9.030810356140137
Epoch 1810, val loss: 0.574870228767395
Epoch 1820, training loss: 90.45113372802734 = 0.13165432214736938 + 10.0 * 9.03194808959961
Epoch 1820, val loss: 0.5777758955955505
Epoch 1830, training loss: 90.44181060791016 = 0.13067986071109772 + 10.0 * 9.031113624572754
Epoch 1830, val loss: 0.5809266567230225
Epoch 1840, training loss: 90.4352035522461 = 0.12969082593917847 + 10.0 * 9.030550956726074
Epoch 1840, val loss: 0.583694338798523
Epoch 1850, training loss: 90.43109130859375 = 0.12870587408542633 + 10.0 * 9.030238151550293
Epoch 1850, val loss: 0.5867471098899841
Epoch 1860, training loss: 90.43070983886719 = 0.12771926820278168 + 10.0 * 9.030299186706543
Epoch 1860, val loss: 0.589702308177948
Epoch 1870, training loss: 90.44367218017578 = 0.12673355638980865 + 10.0 * 9.031694412231445
Epoch 1870, val loss: 0.5927680134773254
Epoch 1880, training loss: 90.43038177490234 = 0.12575207650661469 + 10.0 * 9.030462265014648
Epoch 1880, val loss: 0.5958321690559387
Epoch 1890, training loss: 90.4223861694336 = 0.12476268410682678 + 10.0 * 9.029762268066406
Epoch 1890, val loss: 0.5988717079162598
Epoch 1900, training loss: 90.41624450683594 = 0.12377600371837616 + 10.0 * 9.02924633026123
Epoch 1900, val loss: 0.6021779179573059
Epoch 1910, training loss: 90.41795349121094 = 0.12278441339731216 + 10.0 * 9.029516220092773
Epoch 1910, val loss: 0.6052363514900208
Epoch 1920, training loss: 90.44270324707031 = 0.12179742008447647 + 10.0 * 9.03209114074707
Epoch 1920, val loss: 0.6083964109420776
Epoch 1930, training loss: 90.41165924072266 = 0.12080926448106766 + 10.0 * 9.029085159301758
Epoch 1930, val loss: 0.6113448739051819
Epoch 1940, training loss: 90.41478729248047 = 0.11982134729623795 + 10.0 * 9.029497146606445
Epoch 1940, val loss: 0.6145516037940979
Epoch 1950, training loss: 90.41071319580078 = 0.11883030086755753 + 10.0 * 9.02918815612793
Epoch 1950, val loss: 0.6176683902740479
Epoch 1960, training loss: 90.40199279785156 = 0.11783773452043533 + 10.0 * 9.02841567993164
Epoch 1960, val loss: 0.6210575699806213
Epoch 1970, training loss: 90.40821075439453 = 0.11684682220220566 + 10.0 * 9.029136657714844
Epoch 1970, val loss: 0.6243677139282227
Epoch 1980, training loss: 90.41240692138672 = 0.11585090309381485 + 10.0 * 9.029655456542969
Epoch 1980, val loss: 0.6276339292526245
Epoch 1990, training loss: 90.39591979980469 = 0.11486566811800003 + 10.0 * 9.028104782104492
Epoch 1990, val loss: 0.6310474276542664
Epoch 2000, training loss: 90.39501190185547 = 0.11387676000595093 + 10.0 * 9.02811336517334
Epoch 2000, val loss: 0.6342875957489014
Epoch 2010, training loss: 90.39703369140625 = 0.1128842830657959 + 10.0 * 9.028414726257324
Epoch 2010, val loss: 0.6376472115516663
Epoch 2020, training loss: 90.41355895996094 = 0.11189454793930054 + 10.0 * 9.030166625976562
Epoch 2020, val loss: 0.6411232352256775
Epoch 2030, training loss: 90.40436553955078 = 0.11091110110282898 + 10.0 * 9.029345512390137
Epoch 2030, val loss: 0.6444213390350342
Epoch 2040, training loss: 90.38725280761719 = 0.10991642624139786 + 10.0 * 9.02773380279541
Epoch 2040, val loss: 0.6478128433227539
Epoch 2050, training loss: 90.38702392578125 = 0.10892649739980698 + 10.0 * 9.027810096740723
Epoch 2050, val loss: 0.651282787322998
Epoch 2060, training loss: 90.40864562988281 = 0.10794401913881302 + 10.0 * 9.030070304870605
Epoch 2060, val loss: 0.6548119783401489
Epoch 2070, training loss: 90.3865737915039 = 0.10695978999137878 + 10.0 * 9.027961730957031
Epoch 2070, val loss: 0.6585086584091187
Epoch 2080, training loss: 90.38032531738281 = 0.10597438365221024 + 10.0 * 9.027435302734375
Epoch 2080, val loss: 0.6619377732276917
Epoch 2090, training loss: 90.40126037597656 = 0.10499366372823715 + 10.0 * 9.029626846313477
Epoch 2090, val loss: 0.6655089855194092
Epoch 2100, training loss: 90.37645721435547 = 0.10401744395494461 + 10.0 * 9.027243614196777
Epoch 2100, val loss: 0.669418454170227
Epoch 2110, training loss: 90.37194061279297 = 0.1030365452170372 + 10.0 * 9.026890754699707
Epoch 2110, val loss: 0.6729836463928223
Epoch 2120, training loss: 90.3679428100586 = 0.10205849260091782 + 10.0 * 9.026588439941406
Epoch 2120, val loss: 0.6768260598182678
Epoch 2130, training loss: 90.37257385253906 = 0.10108096152544022 + 10.0 * 9.027149200439453
Epoch 2130, val loss: 0.6806195378303528
Epoch 2140, training loss: 90.37459564208984 = 0.10010163486003876 + 10.0 * 9.027448654174805
Epoch 2140, val loss: 0.6842585206031799
Epoch 2150, training loss: 90.367431640625 = 0.09914244711399078 + 10.0 * 9.02682876586914
Epoch 2150, val loss: 0.6882282495498657
Epoch 2160, training loss: 90.36598205566406 = 0.09817720204591751 + 10.0 * 9.02678108215332
Epoch 2160, val loss: 0.6916243433952332
Epoch 2170, training loss: 90.36894226074219 = 0.09721336513757706 + 10.0 * 9.027173042297363
Epoch 2170, val loss: 0.6953684687614441
Epoch 2180, training loss: 90.35565185546875 = 0.09624671190977097 + 10.0 * 9.025940895080566
Epoch 2180, val loss: 0.6995515823364258
Epoch 2190, training loss: 90.37838745117188 = 0.0952979326248169 + 10.0 * 9.028308868408203
Epoch 2190, val loss: 0.7033650279045105
Epoch 2200, training loss: 90.3583984375 = 0.09433050453662872 + 10.0 * 9.026407241821289
Epoch 2200, val loss: 0.7073447704315186
Epoch 2210, training loss: 90.35430145263672 = 0.09338393062353134 + 10.0 * 9.026091575622559
Epoch 2210, val loss: 0.7113514542579651
Epoch 2220, training loss: 90.3484878540039 = 0.09242922812700272 + 10.0 * 9.025606155395508
Epoch 2220, val loss: 0.715299129486084
Epoch 2230, training loss: 90.3508529663086 = 0.09148373454809189 + 10.0 * 9.0259370803833
Epoch 2230, val loss: 0.7194820046424866
Epoch 2240, training loss: 90.35787200927734 = 0.09053634852170944 + 10.0 * 9.0267333984375
Epoch 2240, val loss: 0.7234629988670349
Epoch 2250, training loss: 90.34525299072266 = 0.08959135413169861 + 10.0 * 9.025566101074219
Epoch 2250, val loss: 0.7273663878440857
Epoch 2260, training loss: 90.34738159179688 = 0.08865202218294144 + 10.0 * 9.025873184204102
Epoch 2260, val loss: 0.7315019369125366
Epoch 2270, training loss: 90.35065460205078 = 0.08771585673093796 + 10.0 * 9.026293754577637
Epoch 2270, val loss: 0.7354660630226135
Epoch 2280, training loss: 90.33917999267578 = 0.08678515255451202 + 10.0 * 9.025239944458008
Epoch 2280, val loss: 0.7397986650466919
Epoch 2290, training loss: 90.34223937988281 = 0.08585832267999649 + 10.0 * 9.02563762664795
Epoch 2290, val loss: 0.7437431216239929
Epoch 2300, training loss: 90.34002685546875 = 0.08493762463331223 + 10.0 * 9.025508880615234
Epoch 2300, val loss: 0.747982919216156
Epoch 2310, training loss: 90.33155059814453 = 0.08401937782764435 + 10.0 * 9.024752616882324
Epoch 2310, val loss: 0.7527041435241699
Epoch 2320, training loss: 90.32789611816406 = 0.08310329169034958 + 10.0 * 9.024478912353516
Epoch 2320, val loss: 0.7566320896148682
Epoch 2330, training loss: 90.35694122314453 = 0.08219984173774719 + 10.0 * 9.027474403381348
Epoch 2330, val loss: 0.7609492540359497
Epoch 2340, training loss: 90.33338165283203 = 0.08130104839801788 + 10.0 * 9.025208473205566
Epoch 2340, val loss: 0.7651647329330444
Epoch 2350, training loss: 90.325927734375 = 0.08039770275354385 + 10.0 * 9.024553298950195
Epoch 2350, val loss: 0.769554853439331
Epoch 2360, training loss: 90.31950378417969 = 0.07949841022491455 + 10.0 * 9.02400016784668
Epoch 2360, val loss: 0.7739998698234558
Epoch 2370, training loss: 90.32804870605469 = 0.07861295342445374 + 10.0 * 9.024943351745605
Epoch 2370, val loss: 0.7784351706504822
Epoch 2380, training loss: 90.32455444335938 = 0.07772448658943176 + 10.0 * 9.024682998657227
Epoch 2380, val loss: 0.7826961278915405
Epoch 2390, training loss: 90.32525634765625 = 0.0768488198518753 + 10.0 * 9.024840354919434
Epoch 2390, val loss: 0.787190318107605
Epoch 2400, training loss: 90.31974792480469 = 0.07598292082548141 + 10.0 * 9.02437686920166
Epoch 2400, val loss: 0.791901707649231
Epoch 2410, training loss: 90.3105239868164 = 0.07511185854673386 + 10.0 * 9.023541450500488
Epoch 2410, val loss: 0.7959611415863037
Epoch 2420, training loss: 90.30967712402344 = 0.0742492601275444 + 10.0 * 9.023542404174805
Epoch 2420, val loss: 0.800387442111969
Epoch 2430, training loss: 90.32809448242188 = 0.07339488714933395 + 10.0 * 9.025469779968262
Epoch 2430, val loss: 0.8048473596572876
Epoch 2440, training loss: 90.31945037841797 = 0.0725485309958458 + 10.0 * 9.024690628051758
Epoch 2440, val loss: 0.8099291920661926
Epoch 2450, training loss: 90.307861328125 = 0.07169496268033981 + 10.0 * 9.023616790771484
Epoch 2450, val loss: 0.8139267563819885
Epoch 2460, training loss: 90.30619812011719 = 0.07085814327001572 + 10.0 * 9.023533821105957
Epoch 2460, val loss: 0.8188502192497253
Epoch 2470, training loss: 90.32320404052734 = 0.07002933323383331 + 10.0 * 9.025317192077637
Epoch 2470, val loss: 0.8233019113540649
Epoch 2480, training loss: 90.3039321899414 = 0.06919316947460175 + 10.0 * 9.023473739624023
Epoch 2480, val loss: 0.8280214667320251
Epoch 2490, training loss: 90.29757690429688 = 0.06837303191423416 + 10.0 * 9.022920608520508
Epoch 2490, val loss: 0.832569420337677
Epoch 2500, training loss: 90.2983169555664 = 0.06755588203668594 + 10.0 * 9.023076057434082
Epoch 2500, val loss: 0.8371867537498474
Epoch 2510, training loss: 90.31085205078125 = 0.06675541400909424 + 10.0 * 9.024409294128418
Epoch 2510, val loss: 0.8419723510742188
Epoch 2520, training loss: 90.29966735839844 = 0.06594400852918625 + 10.0 * 9.023372650146484
Epoch 2520, val loss: 0.8466907143592834
Epoch 2530, training loss: 90.29817199707031 = 0.06515245884656906 + 10.0 * 9.02330207824707
Epoch 2530, val loss: 0.8511464595794678
Epoch 2540, training loss: 90.30166625976562 = 0.06436313688755035 + 10.0 * 9.023730278015137
Epoch 2540, val loss: 0.8558982014656067
Epoch 2550, training loss: 90.29657745361328 = 0.06358787417411804 + 10.0 * 9.023298263549805
Epoch 2550, val loss: 0.8610408306121826
Epoch 2560, training loss: 90.28707885742188 = 0.06281114369630814 + 10.0 * 9.02242660522461
Epoch 2560, val loss: 0.8656084537506104
Epoch 2570, training loss: 90.2825698852539 = 0.06204738840460777 + 10.0 * 9.022051811218262
Epoch 2570, val loss: 0.8704742193222046
Epoch 2580, training loss: 90.30668640136719 = 0.06131526082754135 + 10.0 * 9.024537086486816
Epoch 2580, val loss: 0.8756505846977234
Epoch 2590, training loss: 90.28544616699219 = 0.060544077306985855 + 10.0 * 9.022489547729492
Epoch 2590, val loss: 0.8796447515487671
Epoch 2600, training loss: 90.27957916259766 = 0.059805963188409805 + 10.0 * 9.021977424621582
Epoch 2600, val loss: 0.8848409652709961
Epoch 2610, training loss: 90.27505493164062 = 0.0590658113360405 + 10.0 * 9.021598815917969
Epoch 2610, val loss: 0.8892099857330322
Epoch 2620, training loss: 90.29008483886719 = 0.058342769742012024 + 10.0 * 9.023174285888672
Epoch 2620, val loss: 0.8941797018051147
Epoch 2630, training loss: 90.27112579345703 = 0.05762434005737305 + 10.0 * 9.021349906921387
Epoch 2630, val loss: 0.898824155330658
Epoch 2640, training loss: 90.27153778076172 = 0.05691671743988991 + 10.0 * 9.021462440490723
Epoch 2640, val loss: 0.9036788940429688
Epoch 2650, training loss: 90.27491760253906 = 0.05621901527047157 + 10.0 * 9.021869659423828
Epoch 2650, val loss: 0.9084320664405823
Epoch 2660, training loss: 90.28233337402344 = 0.055526793003082275 + 10.0 * 9.022680282592773
Epoch 2660, val loss: 0.9131963849067688
Epoch 2670, training loss: 90.26868438720703 = 0.05484076216816902 + 10.0 * 9.021384239196777
Epoch 2670, val loss: 0.9184389114379883
Epoch 2680, training loss: 90.27106475830078 = 0.054164595901966095 + 10.0 * 9.021690368652344
Epoch 2680, val loss: 0.9228514432907104
Epoch 2690, training loss: 90.26734924316406 = 0.05349115654826164 + 10.0 * 9.02138614654541
Epoch 2690, val loss: 0.9277335405349731
Epoch 2700, training loss: 90.26850128173828 = 0.05283103510737419 + 10.0 * 9.021566390991211
Epoch 2700, val loss: 0.9326253533363342
Epoch 2710, training loss: 90.26924896240234 = 0.05217808857560158 + 10.0 * 9.021707534790039
Epoch 2710, val loss: 0.9374089241027832
Epoch 2720, training loss: 90.29523468017578 = 0.05155149847269058 + 10.0 * 9.024368286132812
Epoch 2720, val loss: 0.9430160522460938
Epoch 2730, training loss: 90.26547241210938 = 0.0508933961391449 + 10.0 * 9.02145767211914
Epoch 2730, val loss: 0.9467072486877441
Epoch 2740, training loss: 90.25457000732422 = 0.05026628077030182 + 10.0 * 9.020429611206055
Epoch 2740, val loss: 0.9516447186470032
Epoch 2750, training loss: 90.2510986328125 = 0.049639541655778885 + 10.0 * 9.020146369934082
Epoch 2750, val loss: 0.9563888311386108
Epoch 2760, training loss: 90.25088500976562 = 0.04902569204568863 + 10.0 * 9.020185470581055
Epoch 2760, val loss: 0.9609986543655396
Epoch 2770, training loss: 90.27391052246094 = 0.04843026027083397 + 10.0 * 9.022547721862793
Epoch 2770, val loss: 0.9657148122787476
Epoch 2780, training loss: 90.2547378540039 = 0.04782005399465561 + 10.0 * 9.020691871643066
Epoch 2780, val loss: 0.9705814123153687
Epoch 2790, training loss: 90.26339721679688 = 0.047242872416973114 + 10.0 * 9.021615982055664
Epoch 2790, val loss: 0.9749289155006409
Epoch 2800, training loss: 90.24946594238281 = 0.04663796350359917 + 10.0 * 9.020282745361328
Epoch 2800, val loss: 0.9800305962562561
Epoch 2810, training loss: 90.24666595458984 = 0.04605701193213463 + 10.0 * 9.020060539245605
Epoch 2810, val loss: 0.9846011400222778
Epoch 2820, training loss: 90.253662109375 = 0.04548577219247818 + 10.0 * 9.020817756652832
Epoch 2820, val loss: 0.989548921585083
Epoch 2830, training loss: 90.24808502197266 = 0.044921934604644775 + 10.0 * 9.020316123962402
Epoch 2830, val loss: 0.9939444661140442
Epoch 2840, training loss: 90.24696350097656 = 0.04436511546373367 + 10.0 * 9.020259857177734
Epoch 2840, val loss: 0.9985485076904297
Epoch 2850, training loss: 90.25972747802734 = 0.043826643377542496 + 10.0 * 9.021590232849121
Epoch 2850, val loss: 1.0027625560760498
Epoch 2860, training loss: 90.2398452758789 = 0.04327628016471863 + 10.0 * 9.019657135009766
Epoch 2860, val loss: 1.0079957246780396
Epoch 2870, training loss: 90.23446655273438 = 0.04273957386612892 + 10.0 * 9.019172668457031
Epoch 2870, val loss: 1.0121965408325195
Epoch 2880, training loss: 90.23294067382812 = 0.0422099269926548 + 10.0 * 9.019072532653809
Epoch 2880, val loss: 1.0170187950134277
Epoch 2890, training loss: 90.28217315673828 = 0.041704434901475906 + 10.0 * 9.024046897888184
Epoch 2890, val loss: 1.021578311920166
Epoch 2900, training loss: 90.24769592285156 = 0.041184984147548676 + 10.0 * 9.020650863647461
Epoch 2900, val loss: 1.0264124870300293
Epoch 2910, training loss: 90.22816467285156 = 0.04067543148994446 + 10.0 * 9.018749237060547
Epoch 2910, val loss: 1.0306973457336426
Epoch 2920, training loss: 90.22843933105469 = 0.04017731174826622 + 10.0 * 9.018826484680176
Epoch 2920, val loss: 1.0353361368179321
Epoch 2930, training loss: 90.23458099365234 = 0.03969241678714752 + 10.0 * 9.019488334655762
Epoch 2930, val loss: 1.039965033531189
Epoch 2940, training loss: 90.2340316772461 = 0.039207834750413895 + 10.0 * 9.019482612609863
Epoch 2940, val loss: 1.0443787574768066
Epoch 2950, training loss: 90.22789001464844 = 0.0387304313480854 + 10.0 * 9.018916130065918
Epoch 2950, val loss: 1.04893159866333
Epoch 2960, training loss: 90.2267074584961 = 0.038260046392679214 + 10.0 * 9.018844604492188
Epoch 2960, val loss: 1.0535376071929932
Epoch 2970, training loss: 90.23680877685547 = 0.037800341844558716 + 10.0 * 9.01990032196045
Epoch 2970, val loss: 1.0578820705413818
Epoch 2980, training loss: 90.24075317382812 = 0.037340275943279266 + 10.0 * 9.020341873168945
Epoch 2980, val loss: 1.0622344017028809
Epoch 2990, training loss: 90.23094940185547 = 0.036889974027872086 + 10.0 * 9.01940631866455
Epoch 2990, val loss: 1.0667413473129272
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8458
Overall ASR: 0.6937
Flip ASR: 0.6171/1554 nodes
./selected_nodes/Pubmed/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 104.68046569824219 = 1.0906015634536743 + 10.0 * 10.358986854553223
Epoch 0, val loss: 1.0891449451446533
Epoch 10, training loss: 104.6287612915039 = 1.0806503295898438 + 10.0 * 10.35481071472168
Epoch 10, val loss: 1.0790743827819824
Epoch 20, training loss: 104.00602722167969 = 1.0680820941925049 + 10.0 * 10.293794631958008
Epoch 20, val loss: 1.0667940378189087
Epoch 30, training loss: 99.16374206542969 = 1.0572535991668701 + 10.0 * 9.810648918151855
Epoch 30, val loss: 1.056241750717163
Epoch 40, training loss: 96.37298583984375 = 1.044632911682129 + 10.0 * 9.532835006713867
Epoch 40, val loss: 1.0435595512390137
Epoch 50, training loss: 95.22444152832031 = 1.031189203262329 + 10.0 * 9.41932487487793
Epoch 50, val loss: 1.030436635017395
Epoch 60, training loss: 94.69876861572266 = 1.018991470336914 + 10.0 * 9.3679780960083
Epoch 60, val loss: 1.018574595451355
Epoch 70, training loss: 94.22061920166016 = 1.0070710182189941 + 10.0 * 9.321354866027832
Epoch 70, val loss: 1.0069661140441895
Epoch 80, training loss: 93.73451232910156 = 0.9961808919906616 + 10.0 * 9.273833274841309
Epoch 80, val loss: 0.9963136911392212
Epoch 90, training loss: 93.34095764160156 = 0.9836605787277222 + 10.0 * 9.235730171203613
Epoch 90, val loss: 0.9840314984321594
Epoch 100, training loss: 93.07732391357422 = 0.9701128602027893 + 10.0 * 9.210721015930176
Epoch 100, val loss: 0.9710979461669922
Epoch 110, training loss: 92.87045288085938 = 0.9567282795906067 + 10.0 * 9.191372871398926
Epoch 110, val loss: 0.958293616771698
Epoch 120, training loss: 92.65377044677734 = 0.9430770874023438 + 10.0 * 9.171069145202637
Epoch 120, val loss: 0.9449238181114197
Epoch 130, training loss: 92.47354888916016 = 0.9287203550338745 + 10.0 * 9.1544828414917
Epoch 130, val loss: 0.9310460090637207
Epoch 140, training loss: 92.3411865234375 = 0.9115331172943115 + 10.0 * 9.142965316772461
Epoch 140, val loss: 0.9143570065498352
Epoch 150, training loss: 92.22595977783203 = 0.8907404541969299 + 10.0 * 9.133522033691406
Epoch 150, val loss: 0.8943875432014465
Epoch 160, training loss: 92.12256622314453 = 0.8672682642936707 + 10.0 * 9.125529289245605
Epoch 160, val loss: 0.8717832565307617
Epoch 170, training loss: 92.0334701538086 = 0.8412416577339172 + 10.0 * 9.119222640991211
Epoch 170, val loss: 0.8466911315917969
Epoch 180, training loss: 91.93659973144531 = 0.8123216032981873 + 10.0 * 9.112427711486816
Epoch 180, val loss: 0.8189712166786194
Epoch 190, training loss: 91.8646240234375 = 0.7805517911911011 + 10.0 * 9.108407974243164
Epoch 190, val loss: 0.7886032462120056
Epoch 200, training loss: 91.77898406982422 = 0.7463290691375732 + 10.0 * 9.103265762329102
Epoch 200, val loss: 0.7560845613479614
Epoch 210, training loss: 91.7054214477539 = 0.7102403044700623 + 10.0 * 9.099517822265625
Epoch 210, val loss: 0.7220029830932617
Epoch 220, training loss: 91.68004608154297 = 0.672996997833252 + 10.0 * 9.10070514678955
Epoch 220, val loss: 0.6870808005332947
Epoch 230, training loss: 91.57229614257812 = 0.6360657215118408 + 10.0 * 9.093623161315918
Epoch 230, val loss: 0.6527416110038757
Epoch 240, training loss: 91.50919342041016 = 0.6006356477737427 + 10.0 * 9.090855598449707
Epoch 240, val loss: 0.620028555393219
Epoch 250, training loss: 91.4500732421875 = 0.5671901702880859 + 10.0 * 9.088288307189941
Epoch 250, val loss: 0.5894263982772827
Epoch 260, training loss: 91.39493560791016 = 0.5362055897712708 + 10.0 * 9.085872650146484
Epoch 260, val loss: 0.561348021030426
Epoch 270, training loss: 91.34832763671875 = 0.5082943439483643 + 10.0 * 9.084003448486328
Epoch 270, val loss: 0.5363553762435913
Epoch 280, training loss: 91.33009338378906 = 0.48352232575416565 + 10.0 * 9.084657669067383
Epoch 280, val loss: 0.5145391821861267
Epoch 290, training loss: 91.26973724365234 = 0.4620100259780884 + 10.0 * 9.080772399902344
Epoch 290, val loss: 0.49581632018089294
Epoch 300, training loss: 91.22635650634766 = 0.44330814480781555 + 10.0 * 9.0783052444458
Epoch 300, val loss: 0.4798784852027893
Epoch 310, training loss: 91.20684051513672 = 0.42695337533950806 + 10.0 * 9.077988624572754
Epoch 310, val loss: 0.46631526947021484
Epoch 320, training loss: 91.16844940185547 = 0.4127296209335327 + 10.0 * 9.07557201385498
Epoch 320, val loss: 0.4548293948173523
Epoch 330, training loss: 91.15475463867188 = 0.40049535036087036 + 10.0 * 9.07542610168457
Epoch 330, val loss: 0.4452526867389679
Epoch 340, training loss: 91.124755859375 = 0.3898971676826477 + 10.0 * 9.073485374450684
Epoch 340, val loss: 0.43725302815437317
Epoch 350, training loss: 91.0964126586914 = 0.3806105852127075 + 10.0 * 9.07158088684082
Epoch 350, val loss: 0.4305098354816437
Epoch 360, training loss: 91.10311889648438 = 0.3723088502883911 + 10.0 * 9.073081016540527
Epoch 360, val loss: 0.4247109293937683
Epoch 370, training loss: 91.06143951416016 = 0.36494314670562744 + 10.0 * 9.069649696350098
Epoch 370, val loss: 0.41988804936408997
Epoch 380, training loss: 91.04387664794922 = 0.3583657741546631 + 10.0 * 9.068551063537598
Epoch 380, val loss: 0.41573745012283325
Epoch 390, training loss: 91.02678680419922 = 0.3523752689361572 + 10.0 * 9.0674409866333
Epoch 390, val loss: 0.4121479094028473
Epoch 400, training loss: 91.01378631591797 = 0.3468690514564514 + 10.0 * 9.066691398620605
Epoch 400, val loss: 0.40904495120048523
Epoch 410, training loss: 91.00381469726562 = 0.3418494760990143 + 10.0 * 9.06619644165039
Epoch 410, val loss: 0.4063122868537903
Epoch 420, training loss: 90.98377990722656 = 0.33723145723342896 + 10.0 * 9.064654350280762
Epoch 420, val loss: 0.40390995144844055
Epoch 430, training loss: 90.98049926757812 = 0.3329065442085266 + 10.0 * 9.064759254455566
Epoch 430, val loss: 0.40179553627967834
Epoch 440, training loss: 90.97140502929688 = 0.32884225249290466 + 10.0 * 9.06425666809082
Epoch 440, val loss: 0.3998951315879822
Epoch 450, training loss: 90.95165252685547 = 0.32500898838043213 + 10.0 * 9.062664031982422
Epoch 450, val loss: 0.39816558361053467
Epoch 460, training loss: 90.94454193115234 = 0.32138291001319885 + 10.0 * 9.062315940856934
Epoch 460, val loss: 0.39668893814086914
Epoch 470, training loss: 90.93223571777344 = 0.3179355263710022 + 10.0 * 9.061429977416992
Epoch 470, val loss: 0.395297646522522
Epoch 480, training loss: 90.92630767822266 = 0.3146781027317047 + 10.0 * 9.061162948608398
Epoch 480, val loss: 0.39409545063972473
Epoch 490, training loss: 90.90827178955078 = 0.3115691542625427 + 10.0 * 9.059670448303223
Epoch 490, val loss: 0.39300841093063354
Epoch 500, training loss: 90.9066390991211 = 0.30856576561927795 + 10.0 * 9.059807777404785
Epoch 500, val loss: 0.3919987082481384
Epoch 510, training loss: 90.90328979492188 = 0.3056736886501312 + 10.0 * 9.059762001037598
Epoch 510, val loss: 0.39114293456077576
Epoch 520, training loss: 90.88587188720703 = 0.30291077494621277 + 10.0 * 9.058296203613281
Epoch 520, val loss: 0.39031982421875
Epoch 530, training loss: 90.87459564208984 = 0.30025550723075867 + 10.0 * 9.05743408203125
Epoch 530, val loss: 0.3896322250366211
Epoch 540, training loss: 90.86552429199219 = 0.29765719175338745 + 10.0 * 9.05678653717041
Epoch 540, val loss: 0.388995498418808
Epoch 550, training loss: 90.8673324584961 = 0.29513075947761536 + 10.0 * 9.057220458984375
Epoch 550, val loss: 0.3884257376194
Epoch 560, training loss: 90.86024475097656 = 0.29270803928375244 + 10.0 * 9.056753158569336
Epoch 560, val loss: 0.3879387378692627
Epoch 570, training loss: 90.88114166259766 = 0.2903635501861572 + 10.0 * 9.059077262878418
Epoch 570, val loss: 0.3874969482421875
Epoch 580, training loss: 90.83805847167969 = 0.2880880832672119 + 10.0 * 9.054997444152832
Epoch 580, val loss: 0.3871406018733978
Epoch 590, training loss: 90.83001708984375 = 0.28587213158607483 + 10.0 * 9.054414749145508
Epoch 590, val loss: 0.3868187367916107
Epoch 600, training loss: 90.82057189941406 = 0.28368932008743286 + 10.0 * 9.053688049316406
Epoch 600, val loss: 0.3865561783313751
Epoch 610, training loss: 90.8150863647461 = 0.2815343141555786 + 10.0 * 9.05335521697998
Epoch 610, val loss: 0.3863523304462433
Epoch 620, training loss: 90.82667541503906 = 0.279418408870697 + 10.0 * 9.054725646972656
Epoch 620, val loss: 0.3862288296222687
Epoch 630, training loss: 90.813720703125 = 0.27737557888031006 + 10.0 * 9.053634643554688
Epoch 630, val loss: 0.3860727846622467
Epoch 640, training loss: 90.79914093017578 = 0.2753818929195404 + 10.0 * 9.052375793457031
Epoch 640, val loss: 0.3860004246234894
Epoch 650, training loss: 90.79287719726562 = 0.2734079658985138 + 10.0 * 9.051946640014648
Epoch 650, val loss: 0.38597628474235535
Epoch 660, training loss: 90.8028793334961 = 0.27146127820014954 + 10.0 * 9.053141593933105
Epoch 660, val loss: 0.3859841525554657
Epoch 670, training loss: 90.78311157226562 = 0.26956993341445923 + 10.0 * 9.05135440826416
Epoch 670, val loss: 0.38603106141090393
Epoch 680, training loss: 90.77513885498047 = 0.2677153944969177 + 10.0 * 9.050742149353027
Epoch 680, val loss: 0.3861391842365265
Epoch 690, training loss: 90.78147888183594 = 0.26587820053100586 + 10.0 * 9.051560401916504
Epoch 690, val loss: 0.38623887300491333
Epoch 700, training loss: 90.76760864257812 = 0.26407307386398315 + 10.0 * 9.050353050231934
Epoch 700, val loss: 0.38639071583747864
Epoch 710, training loss: 90.75892639160156 = 0.2622959613800049 + 10.0 * 9.049662590026855
Epoch 710, val loss: 0.3866632282733917
Epoch 720, training loss: 90.7607192993164 = 0.2605395019054413 + 10.0 * 9.050018310546875
Epoch 720, val loss: 0.38687485456466675
Epoch 730, training loss: 90.7524642944336 = 0.2588036060333252 + 10.0 * 9.049365997314453
Epoch 730, val loss: 0.38714689016342163
Epoch 740, training loss: 90.74674987792969 = 0.2571078836917877 + 10.0 * 9.04896354675293
Epoch 740, val loss: 0.38756322860717773
Epoch 750, training loss: 90.73469543457031 = 0.25542399287223816 + 10.0 * 9.047926902770996
Epoch 750, val loss: 0.3879101872444153
Epoch 760, training loss: 90.73153686523438 = 0.25374987721443176 + 10.0 * 9.047778129577637
Epoch 760, val loss: 0.38831496238708496
Epoch 770, training loss: 90.77798461914062 = 0.2520977854728699 + 10.0 * 9.05258846282959
Epoch 770, val loss: 0.3887994885444641
Epoch 780, training loss: 90.73934173583984 = 0.2504939138889313 + 10.0 * 9.048884391784668
Epoch 780, val loss: 0.3892705738544464
Epoch 790, training loss: 90.72219848632812 = 0.24891316890716553 + 10.0 * 9.047327995300293
Epoch 790, val loss: 0.3897641897201538
Epoch 800, training loss: 90.7131576538086 = 0.24733959138393402 + 10.0 * 9.046582221984863
Epoch 800, val loss: 0.3902963697910309
Epoch 810, training loss: 90.70773315429688 = 0.24577362835407257 + 10.0 * 9.046195983886719
Epoch 810, val loss: 0.3908666670322418
Epoch 820, training loss: 90.75439453125 = 0.24421866238117218 + 10.0 * 9.051017761230469
Epoch 820, val loss: 0.3913862407207489
Epoch 830, training loss: 90.70270538330078 = 0.24269169569015503 + 10.0 * 9.046001434326172
Epoch 830, val loss: 0.3921487629413605
Epoch 840, training loss: 90.69825744628906 = 0.24118803441524506 + 10.0 * 9.045706748962402
Epoch 840, val loss: 0.3928973078727722
Epoch 850, training loss: 90.69149780273438 = 0.23969128727912903 + 10.0 * 9.045180320739746
Epoch 850, val loss: 0.39363333582878113
Epoch 860, training loss: 90.69845581054688 = 0.23820127546787262 + 10.0 * 9.046025276184082
Epoch 860, val loss: 0.3943987190723419
Epoch 870, training loss: 90.68624114990234 = 0.23672252893447876 + 10.0 * 9.044951438903809
Epoch 870, val loss: 0.3951629400253296
Epoch 880, training loss: 90.68516540527344 = 0.2352691888809204 + 10.0 * 9.044989585876465
Epoch 880, val loss: 0.3960113823413849
Epoch 890, training loss: 90.68246459960938 = 0.2338230162858963 + 10.0 * 9.0448637008667
Epoch 890, val loss: 0.3968502879142761
Epoch 900, training loss: 90.67699432373047 = 0.2323874980211258 + 10.0 * 9.044461250305176
Epoch 900, val loss: 0.39776676893234253
Epoch 910, training loss: 90.67151641845703 = 0.23096224665641785 + 10.0 * 9.044054985046387
Epoch 910, val loss: 0.398701936006546
Epoch 920, training loss: 90.66464233398438 = 0.22954751551151276 + 10.0 * 9.043509483337402
Epoch 920, val loss: 0.39965033531188965
Epoch 930, training loss: 90.65966796875 = 0.22814035415649414 + 10.0 * 9.043152809143066
Epoch 930, val loss: 0.4006902277469635
Epoch 940, training loss: 90.68932342529297 = 0.22674265503883362 + 10.0 * 9.046257972717285
Epoch 940, val loss: 0.4017566740512848
Epoch 950, training loss: 90.6512451171875 = 0.22536106407642365 + 10.0 * 9.042588233947754
Epoch 950, val loss: 0.4028434157371521
Epoch 960, training loss: 90.6500473022461 = 0.22398899495601654 + 10.0 * 9.04260540008545
Epoch 960, val loss: 0.40391531586647034
Epoch 970, training loss: 90.64839935302734 = 0.22262150049209595 + 10.0 * 9.042577743530273
Epoch 970, val loss: 0.40506282448768616
Epoch 980, training loss: 90.64935302734375 = 0.22126586735248566 + 10.0 * 9.042808532714844
Epoch 980, val loss: 0.4062570631504059
Epoch 990, training loss: 90.63824462890625 = 0.21992260217666626 + 10.0 * 9.041831970214844
Epoch 990, val loss: 0.40739867091178894
Epoch 1000, training loss: 90.63870239257812 = 0.21858681738376617 + 10.0 * 9.042011260986328
Epoch 1000, val loss: 0.4086264371871948
Epoch 1010, training loss: 90.6376724243164 = 0.21725700795650482 + 10.0 * 9.042041778564453
Epoch 1010, val loss: 0.40985360741615295
Epoch 1020, training loss: 90.64647674560547 = 0.21594014763832092 + 10.0 * 9.04305362701416
Epoch 1020, val loss: 0.4111481010913849
Epoch 1030, training loss: 90.62833404541016 = 0.214634507894516 + 10.0 * 9.041369438171387
Epoch 1030, val loss: 0.4125145375728607
Epoch 1040, training loss: 90.62943267822266 = 0.21333229541778564 + 10.0 * 9.041609764099121
Epoch 1040, val loss: 0.41383054852485657
Epoch 1050, training loss: 90.63124084472656 = 0.21204222738742828 + 10.0 * 9.041919708251953
Epoch 1050, val loss: 0.4152313768863678
Epoch 1060, training loss: 90.61849212646484 = 0.21076570451259613 + 10.0 * 9.040773391723633
Epoch 1060, val loss: 0.4166058897972107
Epoch 1070, training loss: 90.6131591796875 = 0.20950600504875183 + 10.0 * 9.040365219116211
Epoch 1070, val loss: 0.41803330183029175
Epoch 1080, training loss: 90.6067123413086 = 0.20824187994003296 + 10.0 * 9.039846420288086
Epoch 1080, val loss: 0.41942930221557617
Epoch 1090, training loss: 90.60589599609375 = 0.2069857120513916 + 10.0 * 9.039891242980957
Epoch 1090, val loss: 0.4209050238132477
Epoch 1100, training loss: 90.61689758300781 = 0.20573420822620392 + 10.0 * 9.041116714477539
Epoch 1100, val loss: 0.4224264323711395
Epoch 1110, training loss: 90.6008529663086 = 0.20449383556842804 + 10.0 * 9.039636611938477
Epoch 1110, val loss: 0.4240438938140869
Epoch 1120, training loss: 90.59422302246094 = 0.20325134694576263 + 10.0 * 9.03909683227539
Epoch 1120, val loss: 0.425539493560791
Epoch 1130, training loss: 90.61209869384766 = 0.20201803743839264 + 10.0 * 9.041007995605469
Epoch 1130, val loss: 0.4271705746650696
Epoch 1140, training loss: 90.60104370117188 = 0.20079126954078674 + 10.0 * 9.04002571105957
Epoch 1140, val loss: 0.42873820662498474
Epoch 1150, training loss: 90.5901870727539 = 0.19957681000232697 + 10.0 * 9.039060592651367
Epoch 1150, val loss: 0.430321604013443
Epoch 1160, training loss: 90.59286499023438 = 0.1983623057603836 + 10.0 * 9.039449691772461
Epoch 1160, val loss: 0.43184736371040344
Epoch 1170, training loss: 90.58335876464844 = 0.1971556693315506 + 10.0 * 9.038619995117188
Epoch 1170, val loss: 0.43357694149017334
Epoch 1180, training loss: 90.5760498046875 = 0.1959574669599533 + 10.0 * 9.038008689880371
Epoch 1180, val loss: 0.43529754877090454
Epoch 1190, training loss: 90.57817840576172 = 0.19475781917572021 + 10.0 * 9.038342475891113
Epoch 1190, val loss: 0.43693283200263977
Epoch 1200, training loss: 90.59302520751953 = 0.19356417655944824 + 10.0 * 9.039945602416992
Epoch 1200, val loss: 0.4386425316333771
Epoch 1210, training loss: 90.57587432861328 = 0.19238866865634918 + 10.0 * 9.038348197937012
Epoch 1210, val loss: 0.4405384957790375
Epoch 1220, training loss: 90.5670394897461 = 0.1912073940038681 + 10.0 * 9.037583351135254
Epoch 1220, val loss: 0.44230931997299194
Epoch 1230, training loss: 90.56108093261719 = 0.19003093242645264 + 10.0 * 9.037104606628418
Epoch 1230, val loss: 0.4440919756889343
Epoch 1240, training loss: 90.55879211425781 = 0.18885746598243713 + 10.0 * 9.036993026733398
Epoch 1240, val loss: 0.44595232605934143
Epoch 1250, training loss: 90.58132934570312 = 0.18769335746765137 + 10.0 * 9.039363861083984
Epoch 1250, val loss: 0.44786274433135986
Epoch 1260, training loss: 90.57258605957031 = 0.18653079867362976 + 10.0 * 9.038605690002441
Epoch 1260, val loss: 0.4496725797653198
Epoch 1270, training loss: 90.55036926269531 = 0.18537981808185577 + 10.0 * 9.0364990234375
Epoch 1270, val loss: 0.45155778527259827
Epoch 1280, training loss: 90.54827880859375 = 0.1842324435710907 + 10.0 * 9.036404609680176
Epoch 1280, val loss: 0.4535301625728607
Epoch 1290, training loss: 90.55860137939453 = 0.18308661878108978 + 10.0 * 9.037550926208496
Epoch 1290, val loss: 0.45555660128593445
Epoch 1300, training loss: 90.54071044921875 = 0.18194375932216644 + 10.0 * 9.035876274108887
Epoch 1300, val loss: 0.45733511447906494
Epoch 1310, training loss: 90.54009246826172 = 0.18080641329288483 + 10.0 * 9.035928726196289
Epoch 1310, val loss: 0.45938554406166077
Epoch 1320, training loss: 90.56427001953125 = 0.1796744018793106 + 10.0 * 9.038459777832031
Epoch 1320, val loss: 0.46140149235725403
Epoch 1330, training loss: 90.54324340820312 = 0.1785447895526886 + 10.0 * 9.036470413208008
Epoch 1330, val loss: 0.4633636772632599
Epoch 1340, training loss: 90.53215789794922 = 0.177423894405365 + 10.0 * 9.035473823547363
Epoch 1340, val loss: 0.46537476778030396
Epoch 1350, training loss: 90.52804565429688 = 0.176301509141922 + 10.0 * 9.035174369812012
Epoch 1350, val loss: 0.4674519896507263
Epoch 1360, training loss: 90.54581451416016 = 0.17518514394760132 + 10.0 * 9.037062644958496
Epoch 1360, val loss: 0.46946004033088684
Epoch 1370, training loss: 90.52493286132812 = 0.1740751713514328 + 10.0 * 9.035085678100586
Epoch 1370, val loss: 0.4717361629009247
Epoch 1380, training loss: 90.52143096923828 = 0.17296531796455383 + 10.0 * 9.034846305847168
Epoch 1380, val loss: 0.4737613797187805
Epoch 1390, training loss: 90.51724243164062 = 0.1718541383743286 + 10.0 * 9.034539222717285
Epoch 1390, val loss: 0.47597968578338623
Epoch 1400, training loss: 90.54305267333984 = 0.17074595391750336 + 10.0 * 9.037230491638184
Epoch 1400, val loss: 0.47813865542411804
Epoch 1410, training loss: 90.52589416503906 = 0.16963788866996765 + 10.0 * 9.035625457763672
Epoch 1410, val loss: 0.4803549647331238
Epoch 1420, training loss: 90.51040649414062 = 0.16853706538677216 + 10.0 * 9.034187316894531
Epoch 1420, val loss: 0.482575386762619
Epoch 1430, training loss: 90.50606536865234 = 0.16743209958076477 + 10.0 * 9.033863067626953
Epoch 1430, val loss: 0.4848119914531708
Epoch 1440, training loss: 90.5064926147461 = 0.16632796823978424 + 10.0 * 9.034016609191895
Epoch 1440, val loss: 0.48717397451400757
Epoch 1450, training loss: 90.53081512451172 = 0.1652277708053589 + 10.0 * 9.036558151245117
Epoch 1450, val loss: 0.48948433995246887
Epoch 1460, training loss: 90.49954223632812 = 0.16413500905036926 + 10.0 * 9.033540725708008
Epoch 1460, val loss: 0.491737425327301
Epoch 1470, training loss: 90.5008773803711 = 0.16304391622543335 + 10.0 * 9.033783912658691
Epoch 1470, val loss: 0.49399393796920776
Epoch 1480, training loss: 90.5000228881836 = 0.16195066273212433 + 10.0 * 9.033807754516602
Epoch 1480, val loss: 0.4963848292827606
Epoch 1490, training loss: 90.50456237792969 = 0.1608629822731018 + 10.0 * 9.034369468688965
Epoch 1490, val loss: 0.4987807273864746
Epoch 1500, training loss: 90.50301361083984 = 0.15977837145328522 + 10.0 * 9.034323692321777
Epoch 1500, val loss: 0.5012781620025635
Epoch 1510, training loss: 90.4861831665039 = 0.15870049595832825 + 10.0 * 9.032748222351074
Epoch 1510, val loss: 0.5036170482635498
Epoch 1520, training loss: 90.48348999023438 = 0.15762312710285187 + 10.0 * 9.032587051391602
Epoch 1520, val loss: 0.5060802698135376
Epoch 1530, training loss: 90.48188018798828 = 0.156544491648674 + 10.0 * 9.032533645629883
Epoch 1530, val loss: 0.5085141658782959
Epoch 1540, training loss: 90.49288177490234 = 0.1554698348045349 + 10.0 * 9.033740997314453
Epoch 1540, val loss: 0.5109268426895142
Epoch 1550, training loss: 90.48635864257812 = 0.15438950061798096 + 10.0 * 9.033197402954102
Epoch 1550, val loss: 0.5135717988014221
Epoch 1560, training loss: 90.47782897949219 = 0.153318852186203 + 10.0 * 9.032450675964355
Epoch 1560, val loss: 0.5161003470420837
Epoch 1570, training loss: 90.47305297851562 = 0.15224987268447876 + 10.0 * 9.032079696655273
Epoch 1570, val loss: 0.518784761428833
Epoch 1580, training loss: 90.46981811523438 = 0.15117916464805603 + 10.0 * 9.031864166259766
Epoch 1580, val loss: 0.5213177800178528
Epoch 1590, training loss: 90.47144317626953 = 0.15011103451251984 + 10.0 * 9.032133102416992
Epoch 1590, val loss: 0.5240785479545593
Epoch 1600, training loss: 90.4834213256836 = 0.14904284477233887 + 10.0 * 9.033437728881836
Epoch 1600, val loss: 0.5266947746276855
Epoch 1610, training loss: 90.47472381591797 = 0.14797146618366241 + 10.0 * 9.032674789428711
Epoch 1610, val loss: 0.5292395949363708
Epoch 1620, training loss: 90.46173095703125 = 0.14690588414669037 + 10.0 * 9.031482696533203
Epoch 1620, val loss: 0.5320550203323364
Epoch 1630, training loss: 90.46086120605469 = 0.14584368467330933 + 10.0 * 9.031501770019531
Epoch 1630, val loss: 0.5348122119903564
Epoch 1640, training loss: 90.47207641601562 = 0.14478078484535217 + 10.0 * 9.032729148864746
Epoch 1640, val loss: 0.5375269651412964
Epoch 1650, training loss: 90.4578857421875 = 0.1437229961156845 + 10.0 * 9.031415939331055
Epoch 1650, val loss: 0.5403189063072205
Epoch 1660, training loss: 90.45271301269531 = 0.14265884459018707 + 10.0 * 9.031004905700684
Epoch 1660, val loss: 0.5432016849517822
Epoch 1670, training loss: 90.4701156616211 = 0.14160338044166565 + 10.0 * 9.032851219177246
Epoch 1670, val loss: 0.5461534261703491
Epoch 1680, training loss: 90.45120239257812 = 0.1405400037765503 + 10.0 * 9.031065940856934
Epoch 1680, val loss: 0.5487937927246094
Epoch 1690, training loss: 90.44613647460938 = 0.13947895169258118 + 10.0 * 9.030665397644043
Epoch 1690, val loss: 0.5517429709434509
Epoch 1700, training loss: 90.4483413696289 = 0.13841789960861206 + 10.0 * 9.03099250793457
Epoch 1700, val loss: 0.5545699596405029
Epoch 1710, training loss: 90.44424438476562 = 0.13735516369342804 + 10.0 * 9.030689239501953
Epoch 1710, val loss: 0.5575061440467834
Epoch 1720, training loss: 90.44849395751953 = 0.13629277050495148 + 10.0 * 9.031220436096191
Epoch 1720, val loss: 0.560420572757721
Epoch 1730, training loss: 90.44288635253906 = 0.13523618876934052 + 10.0 * 9.03076457977295
Epoch 1730, val loss: 0.5633847117424011
Epoch 1740, training loss: 90.43569946289062 = 0.1341744065284729 + 10.0 * 9.030152320861816
Epoch 1740, val loss: 0.5663989782333374
Epoch 1750, training loss: 90.44988250732422 = 0.13311930000782013 + 10.0 * 9.031676292419434
Epoch 1750, val loss: 0.5694139003753662
Epoch 1760, training loss: 90.43089294433594 = 0.13205689191818237 + 10.0 * 9.02988338470459
Epoch 1760, val loss: 0.5723613500595093
Epoch 1770, training loss: 90.4315185546875 = 0.13100339472293854 + 10.0 * 9.030051231384277
Epoch 1770, val loss: 0.5754048228263855
Epoch 1780, training loss: 90.42623901367188 = 0.1299445629119873 + 10.0 * 9.029629707336426
Epoch 1780, val loss: 0.5785121321678162
Epoch 1790, training loss: 90.42070770263672 = 0.12888772785663605 + 10.0 * 9.029181480407715
Epoch 1790, val loss: 0.5818309783935547
Epoch 1800, training loss: 90.4258041381836 = 0.12783673405647278 + 10.0 * 9.029796600341797
Epoch 1800, val loss: 0.5851255655288696
Epoch 1810, training loss: 90.42222595214844 = 0.12677818536758423 + 10.0 * 9.029544830322266
Epoch 1810, val loss: 0.5881642699241638
Epoch 1820, training loss: 90.42337799072266 = 0.1257205456495285 + 10.0 * 9.029766082763672
Epoch 1820, val loss: 0.5913859605789185
Epoch 1830, training loss: 90.42254638671875 = 0.12466563284397125 + 10.0 * 9.02978801727295
Epoch 1830, val loss: 0.5947228074073792
Epoch 1840, training loss: 90.4100112915039 = 0.12360893189907074 + 10.0 * 9.028639793395996
Epoch 1840, val loss: 0.5978015065193176
Epoch 1850, training loss: 90.40770721435547 = 0.12255321443080902 + 10.0 * 9.028515815734863
Epoch 1850, val loss: 0.6010199785232544
Epoch 1860, training loss: 90.41509246826172 = 0.12150082737207413 + 10.0 * 9.029359817504883
Epoch 1860, val loss: 0.6045342683792114
Epoch 1870, training loss: 90.40300750732422 = 0.12044225633144379 + 10.0 * 9.0282564163208
Epoch 1870, val loss: 0.6079197525978088
Epoch 1880, training loss: 90.43263244628906 = 0.11939921230077744 + 10.0 * 9.031323432922363
Epoch 1880, val loss: 0.6116377115249634
Epoch 1890, training loss: 90.40681457519531 = 0.11833003908395767 + 10.0 * 9.028848648071289
Epoch 1890, val loss: 0.6142891049385071
Epoch 1900, training loss: 90.39848327636719 = 0.11727918684482574 + 10.0 * 9.028120040893555
Epoch 1900, val loss: 0.6181606650352478
Epoch 1910, training loss: 90.39422607421875 = 0.11621956527233124 + 10.0 * 9.027800559997559
Epoch 1910, val loss: 0.6215066313743591
Epoch 1920, training loss: 90.40995788574219 = 0.11516859382390976 + 10.0 * 9.029479026794434
Epoch 1920, val loss: 0.6250863671302795
Epoch 1930, training loss: 90.3905258178711 = 0.11411076784133911 + 10.0 * 9.027641296386719
Epoch 1930, val loss: 0.6284660696983337
Epoch 1940, training loss: 90.39334869384766 = 0.11305950582027435 + 10.0 * 9.02802848815918
Epoch 1940, val loss: 0.6320382952690125
Epoch 1950, training loss: 90.39694213867188 = 0.11201600730419159 + 10.0 * 9.02849292755127
Epoch 1950, val loss: 0.6356844305992126
Epoch 1960, training loss: 90.3930435180664 = 0.11097037047147751 + 10.0 * 9.028207778930664
Epoch 1960, val loss: 0.6393625140190125
Epoch 1970, training loss: 90.3826675415039 = 0.10992224514484406 + 10.0 * 9.027274131774902
Epoch 1970, val loss: 0.6428590416908264
Epoch 1980, training loss: 90.38033294677734 = 0.10888328403234482 + 10.0 * 9.027144432067871
Epoch 1980, val loss: 0.6467201709747314
Epoch 1990, training loss: 90.38490295410156 = 0.10784376412630081 + 10.0 * 9.027706146240234
Epoch 1990, val loss: 0.6503731608390808
Epoch 2000, training loss: 90.38495635986328 = 0.1068064272403717 + 10.0 * 9.027814865112305
Epoch 2000, val loss: 0.654132604598999
Epoch 2010, training loss: 90.38404083251953 = 0.10576999932527542 + 10.0 * 9.027827262878418
Epoch 2010, val loss: 0.6580740213394165
Epoch 2020, training loss: 90.36951446533203 = 0.10473330318927765 + 10.0 * 9.026477813720703
Epoch 2020, val loss: 0.6618500351905823
Epoch 2030, training loss: 90.36835479736328 = 0.10370250046253204 + 10.0 * 9.02646541595459
Epoch 2030, val loss: 0.6656742095947266
Epoch 2040, training loss: 90.37335968017578 = 0.10267321765422821 + 10.0 * 9.027068138122559
Epoch 2040, val loss: 0.6695505380630493
Epoch 2050, training loss: 90.3643569946289 = 0.10163932293653488 + 10.0 * 9.02627182006836
Epoch 2050, val loss: 0.6735265851020813
Epoch 2060, training loss: 90.36378479003906 = 0.10061319172382355 + 10.0 * 9.02631664276123
Epoch 2060, val loss: 0.6776201128959656
Epoch 2070, training loss: 90.3701400756836 = 0.09958648681640625 + 10.0 * 9.027055740356445
Epoch 2070, val loss: 0.6816479563713074
Epoch 2080, training loss: 90.3690414428711 = 0.0985604077577591 + 10.0 * 9.027048110961914
Epoch 2080, val loss: 0.6855022311210632
Epoch 2090, training loss: 90.3558578491211 = 0.09753578901290894 + 10.0 * 9.025832176208496
Epoch 2090, val loss: 0.6897211074829102
Epoch 2100, training loss: 90.35267639160156 = 0.09651511162519455 + 10.0 * 9.025616645812988
Epoch 2100, val loss: 0.6936743259429932
Epoch 2110, training loss: 90.35346221923828 = 0.09549747407436371 + 10.0 * 9.025796890258789
Epoch 2110, val loss: 0.6980133056640625
Epoch 2120, training loss: 90.35990905761719 = 0.0944824069738388 + 10.0 * 9.026542663574219
Epoch 2120, val loss: 0.7021363973617554
Epoch 2130, training loss: 90.36284637451172 = 0.09346707910299301 + 10.0 * 9.026937484741211
Epoch 2130, val loss: 0.7061051726341248
Epoch 2140, training loss: 90.35093688964844 = 0.09245360642671585 + 10.0 * 9.025848388671875
Epoch 2140, val loss: 0.7104855179786682
Epoch 2150, training loss: 90.34370422363281 = 0.09143923223018646 + 10.0 * 9.025226593017578
Epoch 2150, val loss: 0.7146862149238586
Epoch 2160, training loss: 90.35343170166016 = 0.0904419794678688 + 10.0 * 9.026299476623535
Epoch 2160, val loss: 0.7192942500114441
Epoch 2170, training loss: 90.34404754638672 = 0.0894392803311348 + 10.0 * 9.025461196899414
Epoch 2170, val loss: 0.7236181497573853
Epoch 2180, training loss: 90.33731842041016 = 0.08844456821680069 + 10.0 * 9.024887084960938
Epoch 2180, val loss: 0.7280094623565674
Epoch 2190, training loss: 90.33372497558594 = 0.08745160698890686 + 10.0 * 9.024627685546875
Epoch 2190, val loss: 0.7323364615440369
Epoch 2200, training loss: 90.33673858642578 = 0.0864657536149025 + 10.0 * 9.02502727508545
Epoch 2200, val loss: 0.7367733716964722
Epoch 2210, training loss: 90.33881378173828 = 0.08548387140035629 + 10.0 * 9.0253324508667
Epoch 2210, val loss: 0.7413119673728943
Epoch 2220, training loss: 90.33697509765625 = 0.08451374620199203 + 10.0 * 9.025246620178223
Epoch 2220, val loss: 0.7463486790657043
Epoch 2230, training loss: 90.3430404663086 = 0.0835374966263771 + 10.0 * 9.02595043182373
Epoch 2230, val loss: 0.750771164894104
Epoch 2240, training loss: 90.3297119140625 = 0.08256375044584274 + 10.0 * 9.024714469909668
Epoch 2240, val loss: 0.7551776170730591
Epoch 2250, training loss: 90.32492065429688 = 0.0816020593047142 + 10.0 * 9.024332046508789
Epoch 2250, val loss: 0.7598220109939575
Epoch 2260, training loss: 90.3267593383789 = 0.08064420521259308 + 10.0 * 9.024611473083496
Epoch 2260, val loss: 0.7645576596260071
Epoch 2270, training loss: 90.33191680908203 = 0.07969478517770767 + 10.0 * 9.025221824645996
Epoch 2270, val loss: 0.7694896459579468
Epoch 2280, training loss: 90.3201675415039 = 0.07874429225921631 + 10.0 * 9.024142265319824
Epoch 2280, val loss: 0.7742761373519897
Epoch 2290, training loss: 90.31582641601562 = 0.0777997151017189 + 10.0 * 9.023802757263184
Epoch 2290, val loss: 0.7791325449943542
Epoch 2300, training loss: 90.3189926147461 = 0.07686298340559006 + 10.0 * 9.024212837219238
Epoch 2300, val loss: 0.7839041948318481
Epoch 2310, training loss: 90.31669616699219 = 0.07592836767435074 + 10.0 * 9.024076461791992
Epoch 2310, val loss: 0.7886496186256409
Epoch 2320, training loss: 90.3298568725586 = 0.07500094920396805 + 10.0 * 9.025485038757324
Epoch 2320, val loss: 0.793336033821106
Epoch 2330, training loss: 90.30631256103516 = 0.07407967746257782 + 10.0 * 9.023222923278809
Epoch 2330, val loss: 0.798717200756073
Epoch 2340, training loss: 90.30307006835938 = 0.07316151261329651 + 10.0 * 9.022991180419922
Epoch 2340, val loss: 0.8034689426422119
Epoch 2350, training loss: 90.30119323730469 = 0.07225067168474197 + 10.0 * 9.022893905639648
Epoch 2350, val loss: 0.8085500597953796
Epoch 2360, training loss: 90.30306243896484 = 0.07134661823511124 + 10.0 * 9.023171424865723
Epoch 2360, val loss: 0.8134996294975281
Epoch 2370, training loss: 90.3200912475586 = 0.07044900953769684 + 10.0 * 9.024964332580566
Epoch 2370, val loss: 0.8186091184616089
Epoch 2380, training loss: 90.30476379394531 = 0.06956236064434052 + 10.0 * 9.023519515991211
Epoch 2380, val loss: 0.8241665959358215
Epoch 2390, training loss: 90.30853271484375 = 0.06868086010217667 + 10.0 * 9.023984909057617
Epoch 2390, val loss: 0.8292632699012756
Epoch 2400, training loss: 90.29534912109375 = 0.06779304146766663 + 10.0 * 9.02275562286377
Epoch 2400, val loss: 0.8340807557106018
Epoch 2410, training loss: 90.29405975341797 = 0.0669255331158638 + 10.0 * 9.022713661193848
Epoch 2410, val loss: 0.8393872380256653
Epoch 2420, training loss: 90.29955291748047 = 0.06606627255678177 + 10.0 * 9.023348808288574
Epoch 2420, val loss: 0.844730019569397
Epoch 2430, training loss: 90.30240631103516 = 0.06521452218294144 + 10.0 * 9.02371883392334
Epoch 2430, val loss: 0.8499571084976196
Epoch 2440, training loss: 90.289306640625 = 0.06436413526535034 + 10.0 * 9.022494316101074
Epoch 2440, val loss: 0.8549003601074219
Epoch 2450, training loss: 90.28401947021484 = 0.06352318823337555 + 10.0 * 9.022049903869629
Epoch 2450, val loss: 0.8601749539375305
Epoch 2460, training loss: 90.28150177001953 = 0.06269161403179169 + 10.0 * 9.021881103515625
Epoch 2460, val loss: 0.8655611276626587
Epoch 2470, training loss: 90.29766082763672 = 0.061880286782979965 + 10.0 * 9.023577690124512
Epoch 2470, val loss: 0.8709107041358948
Epoch 2480, training loss: 90.28460693359375 = 0.061058711260557175 + 10.0 * 9.022355079650879
Epoch 2480, val loss: 0.8759204149246216
Epoch 2490, training loss: 90.28671264648438 = 0.060259852558374405 + 10.0 * 9.022645950317383
Epoch 2490, val loss: 0.8814432621002197
Epoch 2500, training loss: 90.28154754638672 = 0.059458114206790924 + 10.0 * 9.022209167480469
Epoch 2500, val loss: 0.8869686722755432
Epoch 2510, training loss: 90.27405548095703 = 0.058671895414590836 + 10.0 * 9.021538734436035
Epoch 2510, val loss: 0.8919848799705505
Epoch 2520, training loss: 90.2859878540039 = 0.057898566126823425 + 10.0 * 9.022809028625488
Epoch 2520, val loss: 0.897284984588623
Epoch 2530, training loss: 90.27229309082031 = 0.05712736025452614 + 10.0 * 9.021516799926758
Epoch 2530, val loss: 0.9032503366470337
Epoch 2540, training loss: 90.27537536621094 = 0.05636760592460632 + 10.0 * 9.02190113067627
Epoch 2540, val loss: 0.9082782864570618
Epoch 2550, training loss: 90.27257537841797 = 0.055611953139305115 + 10.0 * 9.021696090698242
Epoch 2550, val loss: 0.9136561155319214
Epoch 2560, training loss: 90.28683471679688 = 0.054866939783096313 + 10.0 * 9.02319622039795
Epoch 2560, val loss: 0.9184754490852356
Epoch 2570, training loss: 90.27323150634766 = 0.05413249135017395 + 10.0 * 9.021909713745117
Epoch 2570, val loss: 0.924488365650177
Epoch 2580, training loss: 90.2669677734375 = 0.053402431309223175 + 10.0 * 9.021356582641602
Epoch 2580, val loss: 0.9294183254241943
Epoch 2590, training loss: 90.26703643798828 = 0.05268551781773567 + 10.0 * 9.021434783935547
Epoch 2590, val loss: 0.9351434707641602
Epoch 2600, training loss: 90.26713562011719 = 0.05197940766811371 + 10.0 * 9.021515846252441
Epoch 2600, val loss: 0.940491795539856
Epoch 2610, training loss: 90.2608413696289 = 0.05127635598182678 + 10.0 * 9.020956039428711
Epoch 2610, val loss: 0.9459839463233948
Epoch 2620, training loss: 90.25990295410156 = 0.05058249086141586 + 10.0 * 9.0209321975708
Epoch 2620, val loss: 0.9514285922050476
Epoch 2630, training loss: 90.26522064208984 = 0.04990054666996002 + 10.0 * 9.02153205871582
Epoch 2630, val loss: 0.9569664001464844
Epoch 2640, training loss: 90.25941467285156 = 0.04922282323241234 + 10.0 * 9.021018981933594
Epoch 2640, val loss: 0.9621673226356506
Epoch 2650, training loss: 90.25439453125 = 0.0485558807849884 + 10.0 * 9.020584106445312
Epoch 2650, val loss: 0.9674529433250427
Epoch 2660, training loss: 90.25442504882812 = 0.04790135845541954 + 10.0 * 9.020651817321777
Epoch 2660, val loss: 0.9726515412330627
Epoch 2670, training loss: 90.26167297363281 = 0.0472579188644886 + 10.0 * 9.021441459655762
Epoch 2670, val loss: 0.9781634211540222
Epoch 2680, training loss: 90.25038146972656 = 0.04661712795495987 + 10.0 * 9.020376205444336
Epoch 2680, val loss: 0.9841587543487549
Epoch 2690, training loss: 90.26777648925781 = 0.04599341005086899 + 10.0 * 9.022178649902344
Epoch 2690, val loss: 0.9890242218971252
Epoch 2700, training loss: 90.24979400634766 = 0.04536677524447441 + 10.0 * 9.020442962646484
Epoch 2700, val loss: 0.9948813915252686
Epoch 2710, training loss: 90.24372863769531 = 0.044752396643161774 + 10.0 * 9.0198974609375
Epoch 2710, val loss: 0.9998515248298645
Epoch 2720, training loss: 90.23995208740234 = 0.04414648562669754 + 10.0 * 9.019580841064453
Epoch 2720, val loss: 1.005497694015503
Epoch 2730, training loss: 90.24116516113281 = 0.043552156537771225 + 10.0 * 9.01976203918457
Epoch 2730, val loss: 1.0109914541244507
Epoch 2740, training loss: 90.2646484375 = 0.042977795004844666 + 10.0 * 9.022167205810547
Epoch 2740, val loss: 1.0166833400726318
Epoch 2750, training loss: 90.24896240234375 = 0.04238871484994888 + 10.0 * 9.020657539367676
Epoch 2750, val loss: 1.0214804410934448
Epoch 2760, training loss: 90.2386474609375 = 0.041817739605903625 + 10.0 * 9.019682884216309
Epoch 2760, val loss: 1.0270507335662842
Epoch 2770, training loss: 90.24102020263672 = 0.04125300422310829 + 10.0 * 9.019976615905762
Epoch 2770, val loss: 1.0323283672332764
Epoch 2780, training loss: 90.24566650390625 = 0.04070112109184265 + 10.0 * 9.020496368408203
Epoch 2780, val loss: 1.0373557806015015
Epoch 2790, training loss: 90.2338638305664 = 0.04015324264764786 + 10.0 * 9.019371032714844
Epoch 2790, val loss: 1.042994737625122
Epoch 2800, training loss: 90.23080444335938 = 0.03961467742919922 + 10.0 * 9.019119262695312
Epoch 2800, val loss: 1.0482420921325684
Epoch 2810, training loss: 90.22624969482422 = 0.03908407315611839 + 10.0 * 9.018716812133789
Epoch 2810, val loss: 1.0535085201263428
Epoch 2820, training loss: 90.22581481933594 = 0.038562189787626266 + 10.0 * 9.018725395202637
Epoch 2820, val loss: 1.0587282180786133
Epoch 2830, training loss: 90.24164581298828 = 0.03805533051490784 + 10.0 * 9.02035903930664
Epoch 2830, val loss: 1.0644272565841675
Epoch 2840, training loss: 90.23042297363281 = 0.03754408285021782 + 10.0 * 9.019288063049316
Epoch 2840, val loss: 1.0690670013427734
Epoch 2850, training loss: 90.23133850097656 = 0.03704600781202316 + 10.0 * 9.019429206848145
Epoch 2850, val loss: 1.0743775367736816
Epoch 2860, training loss: 90.2317123413086 = 0.03655477240681648 + 10.0 * 9.019515991210938
Epoch 2860, val loss: 1.0792256593704224
Epoch 2870, training loss: 90.22480773925781 = 0.036068402230739594 + 10.0 * 9.01887321472168
Epoch 2870, val loss: 1.0849108695983887
Epoch 2880, training loss: 90.22087097167969 = 0.035590313374996185 + 10.0 * 9.01852798461914
Epoch 2880, val loss: 1.0898914337158203
Epoch 2890, training loss: 90.22188568115234 = 0.03512126952409744 + 10.0 * 9.0186767578125
Epoch 2890, val loss: 1.09520423412323
Epoch 2900, training loss: 90.23770141601562 = 0.03466822952032089 + 10.0 * 9.020303726196289
Epoch 2900, val loss: 1.1006882190704346
Epoch 2910, training loss: 90.22759246826172 = 0.034199826419353485 + 10.0 * 9.019338607788086
Epoch 2910, val loss: 1.1054892539978027
Epoch 2920, training loss: 90.21307373046875 = 0.03374785929918289 + 10.0 * 9.017932891845703
Epoch 2920, val loss: 1.1101242303848267
Epoch 2930, training loss: 90.210693359375 = 0.03330530971288681 + 10.0 * 9.017739295959473
Epoch 2930, val loss: 1.1153042316436768
Epoch 2940, training loss: 90.21047973632812 = 0.032869432121515274 + 10.0 * 9.01776123046875
Epoch 2940, val loss: 1.1203718185424805
Epoch 2950, training loss: 90.2352523803711 = 0.032446276396512985 + 10.0 * 9.020280838012695
Epoch 2950, val loss: 1.1255735158920288
Epoch 2960, training loss: 90.21874237060547 = 0.03201717510819435 + 10.0 * 9.018671989440918
Epoch 2960, val loss: 1.1304662227630615
Epoch 2970, training loss: 90.21653747558594 = 0.03160230070352554 + 10.0 * 9.01849365234375
Epoch 2970, val loss: 1.1354767084121704
Epoch 2980, training loss: 90.21653747558594 = 0.031190237030386925 + 10.0 * 9.018534660339355
Epoch 2980, val loss: 1.1400226354599
Epoch 2990, training loss: 90.21097564697266 = 0.030784837901592255 + 10.0 * 9.01801872253418
Epoch 2990, val loss: 1.14460027217865
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8473
Overall ASR: 0.7099
Flip ASR: 0.6371/1554 nodes
./selected_nodes/Pubmed/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 104.68555450439453 = 1.0954803228378296 + 10.0 * 10.359006881713867
Epoch 0, val loss: 1.0944786071777344
Epoch 10, training loss: 104.6373062133789 = 1.084976077079773 + 10.0 * 10.355233192443848
Epoch 10, val loss: 1.0838443040847778
Epoch 20, training loss: 104.0228500366211 = 1.072241187095642 + 10.0 * 10.295061111450195
Epoch 20, val loss: 1.0713002681732178
Epoch 30, training loss: 99.02888488769531 = 1.0624576807022095 + 10.0 * 9.796643257141113
Epoch 30, val loss: 1.0616356134414673
Epoch 40, training loss: 96.36664581298828 = 1.0517010688781738 + 10.0 * 9.531494140625
Epoch 40, val loss: 1.050693154335022
Epoch 50, training loss: 95.37901306152344 = 1.0403881072998047 + 10.0 * 9.433862686157227
Epoch 50, val loss: 1.039605736732483
Epoch 60, training loss: 94.66462707519531 = 1.0292915105819702 + 10.0 * 9.363533973693848
Epoch 60, val loss: 1.028850793838501
Epoch 70, training loss: 93.9910659790039 = 1.020318865776062 + 10.0 * 9.297075271606445
Epoch 70, val loss: 1.0205731391906738
Epoch 80, training loss: 93.49122619628906 = 1.0149106979370117 + 10.0 * 9.247632026672363
Epoch 80, val loss: 1.0153045654296875
Epoch 90, training loss: 93.12701416015625 = 1.006084680557251 + 10.0 * 9.212092399597168
Epoch 90, val loss: 1.0063598155975342
Epoch 100, training loss: 92.8514175415039 = 0.995459258556366 + 10.0 * 9.185595512390137
Epoch 100, val loss: 0.9962582588195801
Epoch 110, training loss: 92.67124938964844 = 0.985440194606781 + 10.0 * 9.168581008911133
Epoch 110, val loss: 0.9866336584091187
Epoch 120, training loss: 92.53193664550781 = 0.9735754132270813 + 10.0 * 9.15583610534668
Epoch 120, val loss: 0.9750861525535583
Epoch 130, training loss: 92.38227081298828 = 0.9596999883651733 + 10.0 * 9.142256736755371
Epoch 130, val loss: 0.9618314504623413
Epoch 140, training loss: 92.26490783691406 = 0.944963276386261 + 10.0 * 9.131994247436523
Epoch 140, val loss: 0.9478057026863098
Epoch 150, training loss: 92.16695404052734 = 0.9281324744224548 + 10.0 * 9.123882293701172
Epoch 150, val loss: 0.9316545128822327
Epoch 160, training loss: 92.08773040771484 = 0.9083954691886902 + 10.0 * 9.11793327331543
Epoch 160, val loss: 0.9127741456031799
Epoch 170, training loss: 92.01049041748047 = 0.8859554529190063 + 10.0 * 9.11245346069336
Epoch 170, val loss: 0.8913087248802185
Epoch 180, training loss: 91.93669128417969 = 0.860625147819519 + 10.0 * 9.107606887817383
Epoch 180, val loss: 0.867070734500885
Epoch 190, training loss: 91.87212371826172 = 0.832080602645874 + 10.0 * 9.104004859924316
Epoch 190, val loss: 0.8397195339202881
Epoch 200, training loss: 91.79525756835938 = 0.8004968166351318 + 10.0 * 9.099475860595703
Epoch 200, val loss: 0.8095809817314148
Epoch 210, training loss: 91.7349624633789 = 0.7660942673683167 + 10.0 * 9.09688663482666
Epoch 210, val loss: 0.7768190503120422
Epoch 220, training loss: 91.67796325683594 = 0.7298588752746582 + 10.0 * 9.094810485839844
Epoch 220, val loss: 0.7426312565803528
Epoch 230, training loss: 91.61121368408203 = 0.6932007074356079 + 10.0 * 9.091801643371582
Epoch 230, val loss: 0.7081421613693237
Epoch 240, training loss: 91.54940795898438 = 0.6563037037849426 + 10.0 * 9.089310646057129
Epoch 240, val loss: 0.6736176609992981
Epoch 250, training loss: 91.4925765991211 = 0.619892954826355 + 10.0 * 9.087267875671387
Epoch 250, val loss: 0.6397666335105896
Epoch 260, training loss: 91.44154357910156 = 0.5849801301956177 + 10.0 * 9.08565616607666
Epoch 260, val loss: 0.6077086329460144
Epoch 270, training loss: 91.39811706542969 = 0.5529620051383972 + 10.0 * 9.084515571594238
Epoch 270, val loss: 0.5786126852035522
Epoch 280, training loss: 91.34984588623047 = 0.5241150856018066 + 10.0 * 9.082572937011719
Epoch 280, val loss: 0.552703857421875
Epoch 290, training loss: 91.30735778808594 = 0.498110294342041 + 10.0 * 9.080924987792969
Epoch 290, val loss: 0.5297064185142517
Epoch 300, training loss: 91.28937530517578 = 0.47499972581863403 + 10.0 * 9.081438064575195
Epoch 300, val loss: 0.5095517039299011
Epoch 310, training loss: 91.2466049194336 = 0.4550933241844177 + 10.0 * 9.079151153564453
Epoch 310, val loss: 0.49270278215408325
Epoch 320, training loss: 91.20634460449219 = 0.43798646330833435 + 10.0 * 9.076835632324219
Epoch 320, val loss: 0.478565514087677
Epoch 330, training loss: 91.17955780029297 = 0.42298489809036255 + 10.0 * 9.075657844543457
Epoch 330, val loss: 0.46649739146232605
Epoch 340, training loss: 91.21477508544922 = 0.40976783633232117 + 10.0 * 9.080500602722168
Epoch 340, val loss: 0.45616692304611206
Epoch 350, training loss: 91.16209411621094 = 0.39837896823883057 + 10.0 * 9.076372146606445
Epoch 350, val loss: 0.4476453363895416
Epoch 360, training loss: 91.11250305175781 = 0.3887123167514801 + 10.0 * 9.072379112243652
Epoch 360, val loss: 0.44070687890052795
Epoch 370, training loss: 91.09739685058594 = 0.38016781210899353 + 10.0 * 9.071722984313965
Epoch 370, val loss: 0.43486300110816956
Epoch 380, training loss: 91.07909393310547 = 0.3723903000354767 + 10.0 * 9.070670127868652
Epoch 380, val loss: 0.4296587109565735
Epoch 390, training loss: 91.07112884521484 = 0.3653464615345001 + 10.0 * 9.070577621459961
Epoch 390, val loss: 0.4251927137374878
Epoch 400, training loss: 91.05413818359375 = 0.3590549826622009 + 10.0 * 9.06950855255127
Epoch 400, val loss: 0.4212724566459656
Epoch 410, training loss: 91.03126525878906 = 0.3533308207988739 + 10.0 * 9.067792892456055
Epoch 410, val loss: 0.4179043471813202
Epoch 420, training loss: 91.08013153076172 = 0.3480076491832733 + 10.0 * 9.073212623596191
Epoch 420, val loss: 0.4148770272731781
Epoch 430, training loss: 91.01622009277344 = 0.3431212604045868 + 10.0 * 9.067309379577637
Epoch 430, val loss: 0.4122248888015747
Epoch 440, training loss: 91.0002670288086 = 0.3386515974998474 + 10.0 * 9.066161155700684
Epoch 440, val loss: 0.40983524918556213
Epoch 450, training loss: 90.99485778808594 = 0.33442574739456177 + 10.0 * 9.06604290008545
Epoch 450, val loss: 0.4077936112880707
Epoch 460, training loss: 90.9820556640625 = 0.33042141795158386 + 10.0 * 9.065163612365723
Epoch 460, val loss: 0.40581417083740234
Epoch 470, training loss: 90.96444702148438 = 0.32666337490081787 + 10.0 * 9.0637788772583
Epoch 470, val loss: 0.40404653549194336
Epoch 480, training loss: 90.9549331665039 = 0.32308492064476013 + 10.0 * 9.06318473815918
Epoch 480, val loss: 0.4024622142314911
Epoch 490, training loss: 90.94547271728516 = 0.31965047121047974 + 10.0 * 9.062582015991211
Epoch 490, val loss: 0.40094438195228577
Epoch 500, training loss: 90.94696807861328 = 0.31638646125793457 + 10.0 * 9.063058853149414
Epoch 500, val loss: 0.39958974719047546
Epoch 510, training loss: 90.92913818359375 = 0.31327956914901733 + 10.0 * 9.061586380004883
Epoch 510, val loss: 0.3983275294303894
Epoch 520, training loss: 90.91259002685547 = 0.31029075384140015 + 10.0 * 9.060230255126953
Epoch 520, val loss: 0.39720699191093445
Epoch 530, training loss: 90.91281127929688 = 0.30737805366516113 + 10.0 * 9.060543060302734
Epoch 530, val loss: 0.3961194157600403
Epoch 540, training loss: 90.9027099609375 = 0.30456891655921936 + 10.0 * 9.059814453125
Epoch 540, val loss: 0.39525771141052246
Epoch 550, training loss: 90.89254760742188 = 0.30188196897506714 + 10.0 * 9.059066772460938
Epoch 550, val loss: 0.39432293176651
Epoch 560, training loss: 90.87742614746094 = 0.29926609992980957 + 10.0 * 9.057816505432129
Epoch 560, val loss: 0.39359354972839355
Epoch 570, training loss: 90.91986846923828 = 0.29670199751853943 + 10.0 * 9.06231689453125
Epoch 570, val loss: 0.39297592639923096
Epoch 580, training loss: 90.86358642578125 = 0.29424339532852173 + 10.0 * 9.056934356689453
Epoch 580, val loss: 0.3922460675239563
Epoch 590, training loss: 90.85623931884766 = 0.2918834984302521 + 10.0 * 9.056435585021973
Epoch 590, val loss: 0.3916129469871521
Epoch 600, training loss: 90.84844207763672 = 0.2895580232143402 + 10.0 * 9.055888175964355
Epoch 600, val loss: 0.39113616943359375
Epoch 610, training loss: 90.84227752685547 = 0.2872680425643921 + 10.0 * 9.055500984191895
Epoch 610, val loss: 0.39063695073127747
Epoch 620, training loss: 90.89340209960938 = 0.2850249409675598 + 10.0 * 9.060837745666504
Epoch 620, val loss: 0.3902471959590912
Epoch 630, training loss: 90.82862091064453 = 0.2828569710254669 + 10.0 * 9.05457592010498
Epoch 630, val loss: 0.3899250626564026
Epoch 640, training loss: 90.82896423339844 = 0.2807621955871582 + 10.0 * 9.05482006072998
Epoch 640, val loss: 0.38960134983062744
Epoch 650, training loss: 90.82353973388672 = 0.2786979079246521 + 10.0 * 9.054484367370605
Epoch 650, val loss: 0.3894328474998474
Epoch 660, training loss: 90.81415557861328 = 0.27667146921157837 + 10.0 * 9.05374813079834
Epoch 660, val loss: 0.38921627402305603
Epoch 670, training loss: 90.8043441772461 = 0.274671345949173 + 10.0 * 9.052967071533203
Epoch 670, val loss: 0.3891160488128662
Epoch 680, training loss: 90.82469177246094 = 0.27269577980041504 + 10.0 * 9.05519962310791
Epoch 680, val loss: 0.3889865577220917
Epoch 690, training loss: 90.81197357177734 = 0.27077245712280273 + 10.0 * 9.054120063781738
Epoch 690, val loss: 0.38903406262397766
Epoch 700, training loss: 90.78964233398438 = 0.26888900995254517 + 10.0 * 9.052075386047363
Epoch 700, val loss: 0.3889024257659912
Epoch 710, training loss: 90.78189086914062 = 0.2670249342918396 + 10.0 * 9.05148696899414
Epoch 710, val loss: 0.3890168070793152
Epoch 720, training loss: 90.79534149169922 = 0.26517796516418457 + 10.0 * 9.053016662597656
Epoch 720, val loss: 0.38911959528923035
Epoch 730, training loss: 90.80261993408203 = 0.26336905360221863 + 10.0 * 9.053925514221191
Epoch 730, val loss: 0.3890974223613739
Epoch 740, training loss: 90.76640319824219 = 0.26160502433776855 + 10.0 * 9.050479888916016
Epoch 740, val loss: 0.38920894265174866
Epoch 750, training loss: 90.76274871826172 = 0.25986528396606445 + 10.0 * 9.050288200378418
Epoch 750, val loss: 0.3895014822483063
Epoch 760, training loss: 90.75527954101562 = 0.25813165307044983 + 10.0 * 9.049715042114258
Epoch 760, val loss: 0.3896855115890503
Epoch 770, training loss: 90.76631164550781 = 0.2564084231853485 + 10.0 * 9.050990104675293
Epoch 770, val loss: 0.39003849029541016
Epoch 780, training loss: 90.76307678222656 = 0.25471746921539307 + 10.0 * 9.050836563110352
Epoch 780, val loss: 0.39015069603919983
Epoch 790, training loss: 90.74182891845703 = 0.2530623972415924 + 10.0 * 9.048876762390137
Epoch 790, val loss: 0.3904687762260437
Epoch 800, training loss: 90.73808288574219 = 0.25141432881355286 + 10.0 * 9.048666954040527
Epoch 800, val loss: 0.39096370339393616
Epoch 810, training loss: 90.73074340820312 = 0.2497742772102356 + 10.0 * 9.048097610473633
Epoch 810, val loss: 0.3913150429725647
Epoch 820, training loss: 90.76541137695312 = 0.24814186990261078 + 10.0 * 9.051727294921875
Epoch 820, val loss: 0.3917483687400818
Epoch 830, training loss: 90.74891662597656 = 0.24655364453792572 + 10.0 * 9.050236701965332
Epoch 830, val loss: 0.3922128975391388
Epoch 840, training loss: 90.72642517089844 = 0.2449953407049179 + 10.0 * 9.04814338684082
Epoch 840, val loss: 0.39260947704315186
Epoch 850, training loss: 90.7142333984375 = 0.2434379756450653 + 10.0 * 9.047079086303711
Epoch 850, val loss: 0.3932596445083618
Epoch 860, training loss: 90.70812225341797 = 0.24188825488090515 + 10.0 * 9.046623229980469
Epoch 860, val loss: 0.3937687277793884
Epoch 870, training loss: 90.70429992675781 = 0.2403448075056076 + 10.0 * 9.046396255493164
Epoch 870, val loss: 0.3944196105003357
Epoch 880, training loss: 90.74381256103516 = 0.23881089687347412 + 10.0 * 9.050500869750977
Epoch 880, val loss: 0.3950265049934387
Epoch 890, training loss: 90.7186050415039 = 0.23730342090129852 + 10.0 * 9.04813003540039
Epoch 890, val loss: 0.3957838714122772
Epoch 900, training loss: 90.6933364868164 = 0.23582059144973755 + 10.0 * 9.045751571655273
Epoch 900, val loss: 0.39642441272735596
Epoch 910, training loss: 90.692626953125 = 0.2343372255563736 + 10.0 * 9.045828819274902
Epoch 910, val loss: 0.3972000777721405
Epoch 920, training loss: 90.70628356933594 = 0.23286522924900055 + 10.0 * 9.047342300415039
Epoch 920, val loss: 0.397981733083725
Epoch 930, training loss: 90.67909240722656 = 0.23142755031585693 + 10.0 * 9.044766426086426
Epoch 930, val loss: 0.39875656366348267
Epoch 940, training loss: 90.67841339111328 = 0.22999148070812225 + 10.0 * 9.044842720031738
Epoch 940, val loss: 0.39969226717948914
Epoch 950, training loss: 90.67495727539062 = 0.22855669260025024 + 10.0 * 9.04464054107666
Epoch 950, val loss: 0.400582492351532
Epoch 960, training loss: 90.69860076904297 = 0.22713689506053925 + 10.0 * 9.047146797180176
Epoch 960, val loss: 0.40159472823143005
Epoch 970, training loss: 90.67974090576172 = 0.2257307767868042 + 10.0 * 9.045400619506836
Epoch 970, val loss: 0.40236660838127136
Epoch 980, training loss: 90.6635971069336 = 0.2243420034646988 + 10.0 * 9.043925285339355
Epoch 980, val loss: 0.40343958139419556
Epoch 990, training loss: 90.66679382324219 = 0.22296203672885895 + 10.0 * 9.04438304901123
Epoch 990, val loss: 0.40436363220214844
Epoch 1000, training loss: 90.66229248046875 = 0.22158537805080414 + 10.0 * 9.04407024383545
Epoch 1000, val loss: 0.4054410457611084
Epoch 1010, training loss: 90.65142822265625 = 0.22023305296897888 + 10.0 * 9.043119430541992
Epoch 1010, val loss: 0.4064842760562897
Epoch 1020, training loss: 90.64725494384766 = 0.21887445449829102 + 10.0 * 9.042838096618652
Epoch 1020, val loss: 0.4075850248336792
Epoch 1030, training loss: 90.66278839111328 = 0.2175227403640747 + 10.0 * 9.044527053833008
Epoch 1030, val loss: 0.40868785977363586
Epoch 1040, training loss: 90.64576721191406 = 0.21618928015232086 + 10.0 * 9.04295825958252
Epoch 1040, val loss: 0.4099200665950775
Epoch 1050, training loss: 90.64095306396484 = 0.2148643285036087 + 10.0 * 9.042608261108398
Epoch 1050, val loss: 0.4110613465309143
Epoch 1060, training loss: 90.65153503417969 = 0.21354228258132935 + 10.0 * 9.04379940032959
Epoch 1060, val loss: 0.4123572111129761
Epoch 1070, training loss: 90.63001251220703 = 0.2122330665588379 + 10.0 * 9.041777610778809
Epoch 1070, val loss: 0.4133685827255249
Epoch 1080, training loss: 90.62483215332031 = 0.21092921495437622 + 10.0 * 9.041390419006348
Epoch 1080, val loss: 0.41472670435905457
Epoch 1090, training loss: 90.62580108642578 = 0.2096259444952011 + 10.0 * 9.041617393493652
Epoch 1090, val loss: 0.4159316122531891
Epoch 1100, training loss: 90.6368408203125 = 0.2083273082971573 + 10.0 * 9.042851448059082
Epoch 1100, val loss: 0.4173276424407959
Epoch 1110, training loss: 90.61851501464844 = 0.20705024898052216 + 10.0 * 9.041147232055664
Epoch 1110, val loss: 0.4184900224208832
Epoch 1120, training loss: 90.61477661132812 = 0.20577047765254974 + 10.0 * 9.040900230407715
Epoch 1120, val loss: 0.41996973752975464
Epoch 1130, training loss: 90.61504364013672 = 0.20449833571910858 + 10.0 * 9.041054725646973
Epoch 1130, val loss: 0.4213563799858093
Epoch 1140, training loss: 90.61605072021484 = 0.2032279372215271 + 10.0 * 9.041282653808594
Epoch 1140, val loss: 0.42274054884910583
Epoch 1150, training loss: 90.615234375 = 0.20197859406471252 + 10.0 * 9.041325569152832
Epoch 1150, val loss: 0.42406365275382996
Epoch 1160, training loss: 90.60018920898438 = 0.20072786509990692 + 10.0 * 9.039945602416992
Epoch 1160, val loss: 0.4255600571632385
Epoch 1170, training loss: 90.59809112548828 = 0.19949162006378174 + 10.0 * 9.039859771728516
Epoch 1170, val loss: 0.42702746391296387
Epoch 1180, training loss: 90.61361694335938 = 0.19825632870197296 + 10.0 * 9.041536331176758
Epoch 1180, val loss: 0.4285017251968384
Epoch 1190, training loss: 90.59317779541016 = 0.19702979922294617 + 10.0 * 9.0396146774292
Epoch 1190, val loss: 0.4299434423446655
Epoch 1200, training loss: 90.5898666381836 = 0.19581754505634308 + 10.0 * 9.03940486907959
Epoch 1200, val loss: 0.43149325251579285
Epoch 1210, training loss: 90.58434295654297 = 0.1945987492799759 + 10.0 * 9.03897476196289
Epoch 1210, val loss: 0.4329651892185211
Epoch 1220, training loss: 90.58084106445312 = 0.1933864802122116 + 10.0 * 9.038744926452637
Epoch 1220, val loss: 0.43455302715301514
Epoch 1230, training loss: 90.6268310546875 = 0.19217634201049805 + 10.0 * 9.043465614318848
Epoch 1230, val loss: 0.4360485076904297
Epoch 1240, training loss: 90.59335327148438 = 0.19097928702831268 + 10.0 * 9.040237426757812
Epoch 1240, val loss: 0.43785321712493896
Epoch 1250, training loss: 90.57442474365234 = 0.18978750705718994 + 10.0 * 9.038463592529297
Epoch 1250, val loss: 0.43948739767074585
Epoch 1260, training loss: 90.57396697998047 = 0.18860211968421936 + 10.0 * 9.03853702545166
Epoch 1260, val loss: 0.4410393536090851
Epoch 1270, training loss: 90.56880950927734 = 0.18741832673549652 + 10.0 * 9.038139343261719
Epoch 1270, val loss: 0.44273999333381653
Epoch 1280, training loss: 90.57332611083984 = 0.18623848259449005 + 10.0 * 9.038708686828613
Epoch 1280, val loss: 0.4444793164730072
Epoch 1290, training loss: 90.57183837890625 = 0.18506324291229248 + 10.0 * 9.038677215576172
Epoch 1290, val loss: 0.44617024064064026
Epoch 1300, training loss: 90.5689926147461 = 0.18389911949634552 + 10.0 * 9.038509368896484
Epoch 1300, val loss: 0.4479757845401764
Epoch 1310, training loss: 90.5547103881836 = 0.18273600935935974 + 10.0 * 9.03719711303711
Epoch 1310, val loss: 0.4496462643146515
Epoch 1320, training loss: 90.55128479003906 = 0.1815778911113739 + 10.0 * 9.036970138549805
Epoch 1320, val loss: 0.45142021775245667
Epoch 1330, training loss: 90.55204010009766 = 0.1804213523864746 + 10.0 * 9.037161827087402
Epoch 1330, val loss: 0.453188955783844
Epoch 1340, training loss: 90.59502410888672 = 0.17927120625972748 + 10.0 * 9.04157543182373
Epoch 1340, val loss: 0.4550723731517792
Epoch 1350, training loss: 90.5456314086914 = 0.1781165897846222 + 10.0 * 9.036751747131348
Epoch 1350, val loss: 0.4567713439464569
Epoch 1360, training loss: 90.54159545898438 = 0.17696906626224518 + 10.0 * 9.036462783813477
Epoch 1360, val loss: 0.45886340737342834
Epoch 1370, training loss: 90.53683471679688 = 0.17582404613494873 + 10.0 * 9.036100387573242
Epoch 1370, val loss: 0.46056655049324036
Epoch 1380, training loss: 90.53294372558594 = 0.17467676103115082 + 10.0 * 9.035826683044434
Epoch 1380, val loss: 0.46253153681755066
Epoch 1390, training loss: 90.53660583496094 = 0.17353546619415283 + 10.0 * 9.036306381225586
Epoch 1390, val loss: 0.4644840359687805
Epoch 1400, training loss: 90.55360412597656 = 0.17239217460155487 + 10.0 * 9.038121223449707
Epoch 1400, val loss: 0.4665527939796448
Epoch 1410, training loss: 90.53270721435547 = 0.1712646484375 + 10.0 * 9.036144256591797
Epoch 1410, val loss: 0.4682685136795044
Epoch 1420, training loss: 90.52595520019531 = 0.17013396322727203 + 10.0 * 9.035581588745117
Epoch 1420, val loss: 0.47035232186317444
Epoch 1430, training loss: 90.52124786376953 = 0.16900739073753357 + 10.0 * 9.035223960876465
Epoch 1430, val loss: 0.4722869098186493
Epoch 1440, training loss: 90.53298950195312 = 0.16788020730018616 + 10.0 * 9.036511421203613
Epoch 1440, val loss: 0.474278062582016
Epoch 1450, training loss: 90.51773834228516 = 0.16675250232219696 + 10.0 * 9.0350980758667
Epoch 1450, val loss: 0.47637486457824707
Epoch 1460, training loss: 90.5228042602539 = 0.16563118994235992 + 10.0 * 9.035717010498047
Epoch 1460, val loss: 0.47838908433914185
Epoch 1470, training loss: 90.51329803466797 = 0.16451695561408997 + 10.0 * 9.03487777709961
Epoch 1470, val loss: 0.4805211126804352
Epoch 1480, training loss: 90.51298522949219 = 0.16339978575706482 + 10.0 * 9.034958839416504
Epoch 1480, val loss: 0.4825901687145233
Epoch 1490, training loss: 90.51387786865234 = 0.16228549182415009 + 10.0 * 9.03515911102295
Epoch 1490, val loss: 0.48472505807876587
Epoch 1500, training loss: 90.51509094238281 = 0.16117799282073975 + 10.0 * 9.035390853881836
Epoch 1500, val loss: 0.4869315028190613
Epoch 1510, training loss: 90.5032730102539 = 0.16006290912628174 + 10.0 * 9.034320831298828
Epoch 1510, val loss: 0.488959401845932
Epoch 1520, training loss: 90.50016784667969 = 0.15895363688468933 + 10.0 * 9.0341215133667
Epoch 1520, val loss: 0.49125778675079346
Epoch 1530, training loss: 90.50630187988281 = 0.1578509360551834 + 10.0 * 9.034845352172852
Epoch 1530, val loss: 0.4935142397880554
Epoch 1540, training loss: 90.4928207397461 = 0.15674294531345367 + 10.0 * 9.033607482910156
Epoch 1540, val loss: 0.49569112062454224
Epoch 1550, training loss: 90.50909423828125 = 0.1556347757577896 + 10.0 * 9.035346031188965
Epoch 1550, val loss: 0.4980764389038086
Epoch 1560, training loss: 90.4922103881836 = 0.1545289158821106 + 10.0 * 9.033768653869629
Epoch 1560, val loss: 0.5002083778381348
Epoch 1570, training loss: 90.48833465576172 = 0.1534220278263092 + 10.0 * 9.033491134643555
Epoch 1570, val loss: 0.5025651454925537
Epoch 1580, training loss: 90.49292755126953 = 0.15231981873512268 + 10.0 * 9.03406047821045
Epoch 1580, val loss: 0.504951000213623
Epoch 1590, training loss: 90.48662567138672 = 0.1512189656496048 + 10.0 * 9.033540725708008
Epoch 1590, val loss: 0.5073737502098083
Epoch 1600, training loss: 90.47766876220703 = 0.15012136101722717 + 10.0 * 9.032754898071289
Epoch 1600, val loss: 0.5096166133880615
Epoch 1610, training loss: 90.474365234375 = 0.14901311695575714 + 10.0 * 9.032535552978516
Epoch 1610, val loss: 0.5120579600334167
Epoch 1620, training loss: 90.47962951660156 = 0.1479116976261139 + 10.0 * 9.033171653747559
Epoch 1620, val loss: 0.5144925117492676
Epoch 1630, training loss: 90.49156188964844 = 0.14680685102939606 + 10.0 * 9.034475326538086
Epoch 1630, val loss: 0.5169611573219299
Epoch 1640, training loss: 90.46863555908203 = 0.14571651816368103 + 10.0 * 9.032292366027832
Epoch 1640, val loss: 0.5194739103317261
Epoch 1650, training loss: 90.468017578125 = 0.14461936056613922 + 10.0 * 9.032339096069336
Epoch 1650, val loss: 0.521956741809845
Epoch 1660, training loss: 90.46543884277344 = 0.1435229331254959 + 10.0 * 9.032191276550293
Epoch 1660, val loss: 0.5245578289031982
Epoch 1670, training loss: 90.48362731933594 = 0.1424281895160675 + 10.0 * 9.034120559692383
Epoch 1670, val loss: 0.5270430445671082
Epoch 1680, training loss: 90.46282958984375 = 0.14133389294147491 + 10.0 * 9.032149314880371
Epoch 1680, val loss: 0.5296604037284851
Epoch 1690, training loss: 90.45620727539062 = 0.14023323357105255 + 10.0 * 9.031597137451172
Epoch 1690, val loss: 0.5322706699371338
Epoch 1700, training loss: 90.46964263916016 = 0.13913261890411377 + 10.0 * 9.033051490783691
Epoch 1700, val loss: 0.5349376201629639
Epoch 1710, training loss: 90.4538803100586 = 0.13803406059741974 + 10.0 * 9.031584739685059
Epoch 1710, val loss: 0.5375462770462036
Epoch 1720, training loss: 90.45555877685547 = 0.13693749904632568 + 10.0 * 9.031862258911133
Epoch 1720, val loss: 0.5402624011039734
Epoch 1730, training loss: 90.44605255126953 = 0.13584573566913605 + 10.0 * 9.031020164489746
Epoch 1730, val loss: 0.5429185032844543
Epoch 1740, training loss: 90.44720458984375 = 0.13474588096141815 + 10.0 * 9.031246185302734
Epoch 1740, val loss: 0.5457000732421875
Epoch 1750, training loss: 90.45437622070312 = 0.13364732265472412 + 10.0 * 9.032073020935059
Epoch 1750, val loss: 0.548444390296936
Epoch 1760, training loss: 90.43901062011719 = 0.13254520297050476 + 10.0 * 9.030646324157715
Epoch 1760, val loss: 0.5511380434036255
Epoch 1770, training loss: 90.43928527832031 = 0.13144223392009735 + 10.0 * 9.030784606933594
Epoch 1770, val loss: 0.5539232492446899
Epoch 1780, training loss: 90.45899200439453 = 0.1303451806306839 + 10.0 * 9.032864570617676
Epoch 1780, val loss: 0.556710422039032
Epoch 1790, training loss: 90.44548797607422 = 0.12925408780574799 + 10.0 * 9.031622886657715
Epoch 1790, val loss: 0.5597600936889648
Epoch 1800, training loss: 90.4351806640625 = 0.12814153730869293 + 10.0 * 9.0307035446167
Epoch 1800, val loss: 0.5625494718551636
Epoch 1810, training loss: 90.4352798461914 = 0.12704598903656006 + 10.0 * 9.030823707580566
Epoch 1810, val loss: 0.565484881401062
Epoch 1820, training loss: 90.43226623535156 = 0.1259499192237854 + 10.0 * 9.030631065368652
Epoch 1820, val loss: 0.5684352517127991
Epoch 1830, training loss: 90.44266510009766 = 0.12485770881175995 + 10.0 * 9.031781196594238
Epoch 1830, val loss: 0.5717692971229553
Epoch 1840, training loss: 90.42605590820312 = 0.12376422435045242 + 10.0 * 9.030229568481445
Epoch 1840, val loss: 0.5744791626930237
Epoch 1850, training loss: 90.41788482666016 = 0.12266963720321655 + 10.0 * 9.029520988464355
Epoch 1850, val loss: 0.5776166319847107
Epoch 1860, training loss: 90.41361999511719 = 0.12157028913497925 + 10.0 * 9.029205322265625
Epoch 1860, val loss: 0.5807186961174011
Epoch 1870, training loss: 90.41805267333984 = 0.1204749122262001 + 10.0 * 9.029757499694824
Epoch 1870, val loss: 0.5838712453842163
Epoch 1880, training loss: 90.4246597290039 = 0.11937449872493744 + 10.0 * 9.03052806854248
Epoch 1880, val loss: 0.5870048999786377
Epoch 1890, training loss: 90.41197204589844 = 0.11828086525201797 + 10.0 * 9.029369354248047
Epoch 1890, val loss: 0.5901567935943604
Epoch 1900, training loss: 90.42675018310547 = 0.11719153821468353 + 10.0 * 9.03095531463623
Epoch 1900, val loss: 0.5933781266212463
Epoch 1910, training loss: 90.40570831298828 = 0.11609558761119843 + 10.0 * 9.028961181640625
Epoch 1910, val loss: 0.5966201424598694
Epoch 1920, training loss: 90.40323638916016 = 0.11500311642885208 + 10.0 * 9.028822898864746
Epoch 1920, val loss: 0.5998684763908386
Epoch 1930, training loss: 90.41485595703125 = 0.11391746997833252 + 10.0 * 9.030094146728516
Epoch 1930, val loss: 0.6032324433326721
Epoch 1940, training loss: 90.4011459350586 = 0.11282740533351898 + 10.0 * 9.028831481933594
Epoch 1940, val loss: 0.6065035462379456
Epoch 1950, training loss: 90.39717102050781 = 0.11174017190933228 + 10.0 * 9.028543472290039
Epoch 1950, val loss: 0.6099317073822021
Epoch 1960, training loss: 90.39419555664062 = 0.11065643280744553 + 10.0 * 9.028353691101074
Epoch 1960, val loss: 0.6133041381835938
Epoch 1970, training loss: 90.40025329589844 = 0.10956740379333496 + 10.0 * 9.029068946838379
Epoch 1970, val loss: 0.6167027354240417
Epoch 1980, training loss: 90.3919448852539 = 0.10848042368888855 + 10.0 * 9.028346061706543
Epoch 1980, val loss: 0.6201214790344238
Epoch 1990, training loss: 90.3983154296875 = 0.10739752650260925 + 10.0 * 9.029091835021973
Epoch 1990, val loss: 0.6236466765403748
Epoch 2000, training loss: 90.40298461914062 = 0.10632344335317612 + 10.0 * 9.02966594696045
Epoch 2000, val loss: 0.6274504065513611
Epoch 2010, training loss: 90.38414001464844 = 0.10523759573698044 + 10.0 * 9.0278902053833
Epoch 2010, val loss: 0.6305207014083862
Epoch 2020, training loss: 90.3827133178711 = 0.10416468977928162 + 10.0 * 9.027854919433594
Epoch 2020, val loss: 0.6342393159866333
Epoch 2030, training loss: 90.39411926269531 = 0.10309398919343948 + 10.0 * 9.029102325439453
Epoch 2030, val loss: 0.6376022100448608
Epoch 2040, training loss: 90.3818359375 = 0.10201910138130188 + 10.0 * 9.027981758117676
Epoch 2040, val loss: 0.6412541270256042
Epoch 2050, training loss: 90.37445831298828 = 0.10094983875751495 + 10.0 * 9.027350425720215
Epoch 2050, val loss: 0.6450275778770447
Epoch 2060, training loss: 90.37319946289062 = 0.0998842641711235 + 10.0 * 9.027331352233887
Epoch 2060, val loss: 0.6487416625022888
Epoch 2070, training loss: 90.39076232910156 = 0.09882523864507675 + 10.0 * 9.029193878173828
Epoch 2070, val loss: 0.6524550914764404
Epoch 2080, training loss: 90.37146759033203 = 0.09776019304990768 + 10.0 * 9.02737045288086
Epoch 2080, val loss: 0.6560109257698059
Epoch 2090, training loss: 90.36775970458984 = 0.09669925272464752 + 10.0 * 9.027105331420898
Epoch 2090, val loss: 0.6598830223083496
Epoch 2100, training loss: 90.38054656982422 = 0.0956486314535141 + 10.0 * 9.02849006652832
Epoch 2100, val loss: 0.6636441946029663
Epoch 2110, training loss: 90.36872100830078 = 0.09460034221410751 + 10.0 * 9.027412414550781
Epoch 2110, val loss: 0.667479932308197
Epoch 2120, training loss: 90.36015319824219 = 0.09355192631483078 + 10.0 * 9.026659965515137
Epoch 2120, val loss: 0.6712086200714111
Epoch 2130, training loss: 90.36457061767578 = 0.09251054376363754 + 10.0 * 9.027205467224121
Epoch 2130, val loss: 0.6749971508979797
Epoch 2140, training loss: 90.35882568359375 = 0.0914742574095726 + 10.0 * 9.026735305786133
Epoch 2140, val loss: 0.6789796948432922
Epoch 2150, training loss: 90.36898040771484 = 0.09044425934553146 + 10.0 * 9.027853012084961
Epoch 2150, val loss: 0.6830896735191345
Epoch 2160, training loss: 90.35945129394531 = 0.08941106498241425 + 10.0 * 9.02700424194336
Epoch 2160, val loss: 0.6867539286613464
Epoch 2170, training loss: 90.35071563720703 = 0.08838114142417908 + 10.0 * 9.026233673095703
Epoch 2170, val loss: 0.6907705664634705
Epoch 2180, training loss: 90.34538269042969 = 0.0873531699180603 + 10.0 * 9.025802612304688
Epoch 2180, val loss: 0.6946974396705627
Epoch 2190, training loss: 90.3577880859375 = 0.08633521944284439 + 10.0 * 9.027145385742188
Epoch 2190, val loss: 0.6988688111305237
Epoch 2200, training loss: 90.34143829345703 = 0.08531805872917175 + 10.0 * 9.025611877441406
Epoch 2200, val loss: 0.702879011631012
Epoch 2210, training loss: 90.34415435791016 = 0.08431044220924377 + 10.0 * 9.025983810424805
Epoch 2210, val loss: 0.7068639397621155
Epoch 2220, training loss: 90.34262084960938 = 0.08330400288105011 + 10.0 * 9.025931358337402
Epoch 2220, val loss: 0.7109301686286926
Epoch 2230, training loss: 90.33805084228516 = 0.082308329641819 + 10.0 * 9.025574684143066
Epoch 2230, val loss: 0.7151020765304565
Epoch 2240, training loss: 90.35200500488281 = 0.08131961524486542 + 10.0 * 9.027068138122559
Epoch 2240, val loss: 0.7193315625190735
Epoch 2250, training loss: 90.33851623535156 = 0.08032548427581787 + 10.0 * 9.025819778442383
Epoch 2250, val loss: 0.72315514087677
Epoch 2260, training loss: 90.34339904785156 = 0.07934640347957611 + 10.0 * 9.026405334472656
Epoch 2260, val loss: 0.7273698449134827
Epoch 2270, training loss: 90.34014129638672 = 0.07836795598268509 + 10.0 * 9.026177406311035
Epoch 2270, val loss: 0.7316449880599976
Epoch 2280, training loss: 90.32730102539062 = 0.07740044593811035 + 10.0 * 9.02499008178711
Epoch 2280, val loss: 0.7359733581542969
Epoch 2290, training loss: 90.32573699951172 = 0.07643920183181763 + 10.0 * 9.024930000305176
Epoch 2290, val loss: 0.740061342716217
Epoch 2300, training loss: 90.33387756347656 = 0.07548853009939194 + 10.0 * 9.025838851928711
Epoch 2300, val loss: 0.744463324546814
Epoch 2310, training loss: 90.33193969726562 = 0.07453915476799011 + 10.0 * 9.025739669799805
Epoch 2310, val loss: 0.7487087845802307
Epoch 2320, training loss: 90.32723236083984 = 0.07360237091779709 + 10.0 * 9.025362968444824
Epoch 2320, val loss: 0.7526432275772095
Epoch 2330, training loss: 90.31616973876953 = 0.07267273217439651 + 10.0 * 9.0243501663208
Epoch 2330, val loss: 0.75727379322052
Epoch 2340, training loss: 90.31290435791016 = 0.07175183296203613 + 10.0 * 9.024114608764648
Epoch 2340, val loss: 0.7617203593254089
Epoch 2350, training loss: 90.31012725830078 = 0.07083646953105927 + 10.0 * 9.02392864227295
Epoch 2350, val loss: 0.7659400105476379
Epoch 2360, training loss: 90.31190490722656 = 0.0699315220117569 + 10.0 * 9.024197578430176
Epoch 2360, val loss: 0.7702209949493408
Epoch 2370, training loss: 90.33026123046875 = 0.06903406977653503 + 10.0 * 9.026123046875
Epoch 2370, val loss: 0.7745358347892761
Epoch 2380, training loss: 90.32845306396484 = 0.0681438148021698 + 10.0 * 9.026030540466309
Epoch 2380, val loss: 0.7795635461807251
Epoch 2390, training loss: 90.31151580810547 = 0.06725744158029556 + 10.0 * 9.024425506591797
Epoch 2390, val loss: 0.7834523320198059
Epoch 2400, training loss: 90.30510711669922 = 0.06638319790363312 + 10.0 * 9.023872375488281
Epoch 2400, val loss: 0.7882741689682007
Epoch 2410, training loss: 90.30522155761719 = 0.06551998108625412 + 10.0 * 9.023969650268555
Epoch 2410, val loss: 0.7926885485649109
Epoch 2420, training loss: 90.33699035644531 = 0.06469003856182098 + 10.0 * 9.027230262756348
Epoch 2420, val loss: 0.7972482442855835
Epoch 2430, training loss: 90.30608367919922 = 0.06382337957620621 + 10.0 * 9.024226188659668
Epoch 2430, val loss: 0.8014736771583557
Epoch 2440, training loss: 90.29354095458984 = 0.06298588961362839 + 10.0 * 9.023055076599121
Epoch 2440, val loss: 0.8057882785797119
Epoch 2450, training loss: 90.29167175292969 = 0.062161628156900406 + 10.0 * 9.022951126098633
Epoch 2450, val loss: 0.8105541467666626
Epoch 2460, training loss: 90.29149627685547 = 0.06134479492902756 + 10.0 * 9.023015022277832
Epoch 2460, val loss: 0.8150493502616882
Epoch 2470, training loss: 90.31812286376953 = 0.060550473630428314 + 10.0 * 9.025757789611816
Epoch 2470, val loss: 0.819882333278656
Epoch 2480, training loss: 90.29232025146484 = 0.05973493680357933 + 10.0 * 9.023258209228516
Epoch 2480, val loss: 0.8238504528999329
Epoch 2490, training loss: 90.28941345214844 = 0.058942411094903946 + 10.0 * 9.023046493530273
Epoch 2490, val loss: 0.8286153674125671
Epoch 2500, training loss: 90.29573822021484 = 0.058158423751592636 + 10.0 * 9.023757934570312
Epoch 2500, val loss: 0.8330095410346985
Epoch 2510, training loss: 90.28348541259766 = 0.05738622322678566 + 10.0 * 9.02260971069336
Epoch 2510, val loss: 0.8376200199127197
Epoch 2520, training loss: 90.29650115966797 = 0.05662541091442108 + 10.0 * 9.023987770080566
Epoch 2520, val loss: 0.8419826626777649
Epoch 2530, training loss: 90.2919692993164 = 0.05587013065814972 + 10.0 * 9.02361011505127
Epoch 2530, val loss: 0.8463989496231079
Epoch 2540, training loss: 90.28162384033203 = 0.05512237921357155 + 10.0 * 9.022649765014648
Epoch 2540, val loss: 0.8510757684707642
Epoch 2550, training loss: 90.27825164794922 = 0.05438680201768875 + 10.0 * 9.02238655090332
Epoch 2550, val loss: 0.8557177186012268
Epoch 2560, training loss: 90.27604675292969 = 0.05365815758705139 + 10.0 * 9.022238731384277
Epoch 2560, val loss: 0.8601442575454712
Epoch 2570, training loss: 90.28483581542969 = 0.05294281244277954 + 10.0 * 9.023189544677734
Epoch 2570, val loss: 0.8646507859230042
Epoch 2580, training loss: 90.29026794433594 = 0.052243951708078384 + 10.0 * 9.023801803588867
Epoch 2580, val loss: 0.8694542050361633
Epoch 2590, training loss: 90.27374267578125 = 0.05153777077794075 + 10.0 * 9.022220611572266
Epoch 2590, val loss: 0.8737190365791321
Epoch 2600, training loss: 90.27843475341797 = 0.05085467919707298 + 10.0 * 9.022757530212402
Epoch 2600, val loss: 0.8784887790679932
Epoch 2610, training loss: 90.27143096923828 = 0.050167351961135864 + 10.0 * 9.022126197814941
Epoch 2610, val loss: 0.8826352953910828
Epoch 2620, training loss: 90.27770233154297 = 0.04949507862329483 + 10.0 * 9.022821426391602
Epoch 2620, val loss: 0.887255072593689
Epoch 2630, training loss: 90.26535034179688 = 0.048833493143320084 + 10.0 * 9.021651268005371
Epoch 2630, val loss: 0.8918702006340027
Epoch 2640, training loss: 90.26351165771484 = 0.04818068444728851 + 10.0 * 9.021533012390137
Epoch 2640, val loss: 0.8962695002555847
Epoch 2650, training loss: 90.26561737060547 = 0.04753587394952774 + 10.0 * 9.021807670593262
Epoch 2650, val loss: 0.9007331728935242
Epoch 2660, training loss: 90.26998138427734 = 0.046903256326913834 + 10.0 * 9.022307395935059
Epoch 2660, val loss: 0.9053939580917358
Epoch 2670, training loss: 90.26979064941406 = 0.04627685993909836 + 10.0 * 9.022351264953613
Epoch 2670, val loss: 0.9096183776855469
Epoch 2680, training loss: 90.25685119628906 = 0.04565918818116188 + 10.0 * 9.021119117736816
Epoch 2680, val loss: 0.9143482446670532
Epoch 2690, training loss: 90.25322723388672 = 0.04504941403865814 + 10.0 * 9.020817756652832
Epoch 2690, val loss: 0.9189395904541016
Epoch 2700, training loss: 90.25306701660156 = 0.04444647207856178 + 10.0 * 9.020861625671387
Epoch 2700, val loss: 0.9234355688095093
Epoch 2710, training loss: 90.26879119873047 = 0.04386306554079056 + 10.0 * 9.022493362426758
Epoch 2710, val loss: 0.9281406998634338
Epoch 2720, training loss: 90.26515197753906 = 0.04327641800045967 + 10.0 * 9.022188186645508
Epoch 2720, val loss: 0.9321057200431824
Epoch 2730, training loss: 90.26180267333984 = 0.04270171374082565 + 10.0 * 9.021909713745117
Epoch 2730, val loss: 0.9370113611221313
Epoch 2740, training loss: 90.24850463867188 = 0.042139191180467606 + 10.0 * 9.020636558532715
Epoch 2740, val loss: 0.9409772753715515
Epoch 2750, training loss: 90.24498748779297 = 0.04158005118370056 + 10.0 * 9.020340919494629
Epoch 2750, val loss: 0.9453296065330505
Epoch 2760, training loss: 90.24976348876953 = 0.041038382798433304 + 10.0 * 9.020872116088867
Epoch 2760, val loss: 0.9498072266578674
Epoch 2770, training loss: 90.25733947753906 = 0.04050176590681076 + 10.0 * 9.021683692932129
Epoch 2770, val loss: 0.9543851017951965
Epoch 2780, training loss: 90.24724578857422 = 0.03997071832418442 + 10.0 * 9.020727157592773
Epoch 2780, val loss: 0.9590829610824585
Epoch 2790, training loss: 90.24852752685547 = 0.039445240050554276 + 10.0 * 9.02090835571289
Epoch 2790, val loss: 0.9631403684616089
Epoch 2800, training loss: 90.25261688232422 = 0.038931842893362045 + 10.0 * 9.021368026733398
Epoch 2800, val loss: 0.9675437211990356
Epoch 2810, training loss: 90.24378967285156 = 0.03842037543654442 + 10.0 * 9.020536422729492
Epoch 2810, val loss: 0.971788227558136
Epoch 2820, training loss: 90.2456283569336 = 0.03792252764105797 + 10.0 * 9.020770072937012
Epoch 2820, val loss: 0.9767001867294312
Epoch 2830, training loss: 90.24057006835938 = 0.037426453083753586 + 10.0 * 9.02031421661377
Epoch 2830, val loss: 0.9805270433425903
Epoch 2840, training loss: 90.23748016357422 = 0.03694688901305199 + 10.0 * 9.020052909851074
Epoch 2840, val loss: 0.9844686985015869
Epoch 2850, training loss: 90.23696899414062 = 0.036468274891376495 + 10.0 * 9.020050048828125
Epoch 2850, val loss: 0.9891819953918457
Epoch 2860, training loss: 90.24042510986328 = 0.03599752485752106 + 10.0 * 9.020442962646484
Epoch 2860, val loss: 0.9935640096664429
Epoch 2870, training loss: 90.2501449584961 = 0.035545945167541504 + 10.0 * 9.021459579467773
Epoch 2870, val loss: 0.9979068040847778
Epoch 2880, training loss: 90.24139404296875 = 0.03508858382701874 + 10.0 * 9.020630836486816
Epoch 2880, val loss: 1.0021508932113647
Epoch 2890, training loss: 90.2325439453125 = 0.03463806211948395 + 10.0 * 9.019790649414062
Epoch 2890, val loss: 1.006448745727539
Epoch 2900, training loss: 90.22577667236328 = 0.0341971330344677 + 10.0 * 9.019158363342285
Epoch 2900, val loss: 1.0107008218765259
Epoch 2910, training loss: 90.23268127441406 = 0.033766765147447586 + 10.0 * 9.019891738891602
Epoch 2910, val loss: 1.0148584842681885
Epoch 2920, training loss: 90.23164367675781 = 0.03334374353289604 + 10.0 * 9.019830703735352
Epoch 2920, val loss: 1.019325613975525
Epoch 2930, training loss: 90.22506713867188 = 0.032924555242061615 + 10.0 * 9.019214630126953
Epoch 2930, val loss: 1.0232902765274048
Epoch 2940, training loss: 90.22359466552734 = 0.03251416236162186 + 10.0 * 9.019107818603516
Epoch 2940, val loss: 1.0274630784988403
Epoch 2950, training loss: 90.22760772705078 = 0.03210832551121712 + 10.0 * 9.019549369812012
Epoch 2950, val loss: 1.0317784547805786
Epoch 2960, training loss: 90.21839141845703 = 0.03170904517173767 + 10.0 * 9.018668174743652
Epoch 2960, val loss: 1.0361368656158447
Epoch 2970, training loss: 90.23644256591797 = 0.03132963925600052 + 10.0 * 9.020511627197266
Epoch 2970, val loss: 1.040405511856079
Epoch 2980, training loss: 90.22496032714844 = 0.030929183587431908 + 10.0 * 9.019403457641602
Epoch 2980, val loss: 1.0442845821380615
Epoch 2990, training loss: 90.21611022949219 = 0.03055463545024395 + 10.0 * 9.018555641174316
Epoch 2990, val loss: 1.048316478729248
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8447
Overall ASR: 0.7145
Flip ASR: 0.6435/1554 nodes
The final ASR:0.70605, 0.00892, Accuracy:0.84593, 0.00104
#Attach Nodes:5
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 97544])
remove edge: torch.Size([2, 79836])
updated graph: torch.Size([2, 88732])
./selected_nodes/Pubmed/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8503
Overall ASR: 0.6993
Flip ASR: 0.6281/1554 nodes
./selected_nodes/Pubmed/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8519
Overall ASR: 0.7388
Flip ASR: 0.6763/1554 nodes
./selected_nodes/Pubmed/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8493
Overall ASR: 0.7252
Flip ASR: 0.6589/1554 nodes
The final ASR:0.72110, 0.01640, Accuracy:0.85050, 0.00104
Begin epxeriment: cont_weight: 10 epoch:3000 der1:0.3 der2:0.3 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=256, cl_num_layers=2, cl_num_proj_hidden=256, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=10.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=0, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.7, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Pubmed/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 104.68351745605469 = 1.094614028930664 + 10.0 * 10.358890533447266
Epoch 0, val loss: 1.0936306715011597
Epoch 10, training loss: 104.61595153808594 = 1.083156943321228 + 10.0 * 10.353279113769531
Epoch 10, val loss: 1.0818169116973877
Epoch 20, training loss: 103.6856689453125 = 1.068116545677185 + 10.0 * 10.261754989624023
Epoch 20, val loss: 1.0668531656265259
Epoch 30, training loss: 97.43791198730469 = 1.05425226688385 + 10.0 * 9.638365745544434
Epoch 30, val loss: 1.053099274635315
Epoch 40, training loss: 96.0506820678711 = 1.041491985321045 + 10.0 * 9.500919342041016
Epoch 40, val loss: 1.0406825542449951
Epoch 50, training loss: 94.91915893554688 = 1.030535340309143 + 10.0 * 9.388862609863281
Epoch 50, val loss: 1.0303000211715698
Epoch 60, training loss: 93.93317413330078 = 1.022071361541748 + 10.0 * 9.291110038757324
Epoch 60, val loss: 1.0222210884094238
Epoch 70, training loss: 93.52769470214844 = 1.0129834413528442 + 10.0 * 9.251470565795898
Epoch 70, val loss: 1.013230562210083
Epoch 80, training loss: 93.18653869628906 = 1.0038284063339233 + 10.0 * 9.218271255493164
Epoch 80, val loss: 1.004294514656067
Epoch 90, training loss: 92.80706024169922 = 0.9964144229888916 + 10.0 * 9.18106460571289
Epoch 90, val loss: 0.9972994923591614
Epoch 100, training loss: 92.54007720947266 = 0.9899358153343201 + 10.0 * 9.155014038085938
Epoch 100, val loss: 0.990950882434845
Epoch 110, training loss: 92.36573028564453 = 0.9809363484382629 + 10.0 * 9.138479232788086
Epoch 110, val loss: 0.9818977117538452
Epoch 120, training loss: 92.1900634765625 = 0.9699889421463013 + 10.0 * 9.122007369995117
Epoch 120, val loss: 0.9709892272949219
Epoch 130, training loss: 92.07441711425781 = 0.9583548903465271 + 10.0 * 9.11160659790039
Epoch 130, val loss: 0.9596531987190247
Epoch 140, training loss: 91.9738998413086 = 0.9450276494026184 + 10.0 * 9.102887153625488
Epoch 140, val loss: 0.9464417695999146
Epoch 150, training loss: 91.88702392578125 = 0.9290586709976196 + 10.0 * 9.095796585083008
Epoch 150, val loss: 0.930696427822113
Epoch 160, training loss: 91.82875061035156 = 0.9101765155792236 + 10.0 * 9.091856956481934
Epoch 160, val loss: 0.9121401906013489
Epoch 170, training loss: 91.77877044677734 = 0.8881814479827881 + 10.0 * 9.089058876037598
Epoch 170, val loss: 0.890705406665802
Epoch 180, training loss: 91.71495056152344 = 0.8632118105888367 + 10.0 * 9.085173606872559
Epoch 180, val loss: 0.8664420247077942
Epoch 190, training loss: 91.655029296875 = 0.8351642489433289 + 10.0 * 9.081986427307129
Epoch 190, val loss: 0.8391713500022888
Epoch 200, training loss: 91.62492370605469 = 0.8037124872207642 + 10.0 * 9.082120895385742
Epoch 200, val loss: 0.8086729645729065
Epoch 210, training loss: 91.55850982666016 = 0.769231915473938 + 10.0 * 9.078927993774414
Epoch 210, val loss: 0.7754073143005371
Epoch 220, training loss: 91.48411560058594 = 0.7328007221221924 + 10.0 * 9.0751314163208
Epoch 220, val loss: 0.7403936386108398
Epoch 230, training loss: 91.42768859863281 = 0.694996178150177 + 10.0 * 9.07326889038086
Epoch 230, val loss: 0.7041972875595093
Epoch 240, training loss: 91.37296295166016 = 0.6563758254051208 + 10.0 * 9.071659088134766
Epoch 240, val loss: 0.6674165725708008
Epoch 250, training loss: 91.3173828125 = 0.6182109117507935 + 10.0 * 9.069917678833008
Epoch 250, val loss: 0.6312734484672546
Epoch 260, training loss: 91.27678680419922 = 0.5818194150924683 + 10.0 * 9.069497108459473
Epoch 260, val loss: 0.5972334742546082
Epoch 270, training loss: 91.22246551513672 = 0.5482943654060364 + 10.0 * 9.06741714477539
Epoch 270, val loss: 0.5660109519958496
Epoch 280, training loss: 91.17963409423828 = 0.5180458426475525 + 10.0 * 9.06615924835205
Epoch 280, val loss: 0.5381984710693359
Epoch 290, training loss: 91.1465072631836 = 0.4913112223148346 + 10.0 * 9.065519332885742
Epoch 290, val loss: 0.5140520334243774
Epoch 300, training loss: 91.09904479980469 = 0.4681762158870697 + 10.0 * 9.06308650970459
Epoch 300, val loss: 0.49340006709098816
Epoch 310, training loss: 91.06575775146484 = 0.44812706112861633 + 10.0 * 9.061762809753418
Epoch 310, val loss: 0.47585442662239075
Epoch 320, training loss: 91.05599212646484 = 0.43073397874832153 + 10.0 * 9.062525749206543
Epoch 320, val loss: 0.46103280782699585
Epoch 330, training loss: 91.01753997802734 = 0.41584423184394836 + 10.0 * 9.060169219970703
Epoch 330, val loss: 0.44865426421165466
Epoch 340, training loss: 91.00057983398438 = 0.4032370150089264 + 10.0 * 9.059734344482422
Epoch 340, val loss: 0.4384775757789612
Epoch 350, training loss: 90.97251892089844 = 0.39238440990448 + 10.0 * 9.058012962341309
Epoch 350, val loss: 0.42996686697006226
Epoch 360, training loss: 90.97319793701172 = 0.38287246227264404 + 10.0 * 9.059032440185547
Epoch 360, val loss: 0.4227258861064911
Epoch 370, training loss: 90.95576477050781 = 0.3745255172252655 + 10.0 * 9.058123588562012
Epoch 370, val loss: 0.4168052077293396
Epoch 380, training loss: 90.92596435546875 = 0.3672246038913727 + 10.0 * 9.05587387084961
Epoch 380, val loss: 0.41173452138900757
Epoch 390, training loss: 90.91019439697266 = 0.36071595549583435 + 10.0 * 9.054947853088379
Epoch 390, val loss: 0.40741997957229614
Epoch 400, training loss: 90.89773559570312 = 0.35479527711868286 + 10.0 * 9.054293632507324
Epoch 400, val loss: 0.4036712646484375
Epoch 410, training loss: 90.93384552001953 = 0.34936100244522095 + 10.0 * 9.058448791503906
Epoch 410, val loss: 0.400395005941391
Epoch 420, training loss: 90.900146484375 = 0.34442582726478577 + 10.0 * 9.055571556091309
Epoch 420, val loss: 0.3975764214992523
Epoch 430, training loss: 90.8686294555664 = 0.3399434983730316 + 10.0 * 9.052868843078613
Epoch 430, val loss: 0.395079642534256
Epoch 440, training loss: 90.85528564453125 = 0.33578675985336304 + 10.0 * 9.051950454711914
Epoch 440, val loss: 0.3929353654384613
Epoch 450, training loss: 90.84527587890625 = 0.3318515419960022 + 10.0 * 9.051342964172363
Epoch 450, val loss: 0.3909852206707001
Epoch 460, training loss: 90.8808822631836 = 0.32810524106025696 + 10.0 * 9.055277824401855
Epoch 460, val loss: 0.3892159163951874
Epoch 470, training loss: 90.83091735839844 = 0.3246161937713623 + 10.0 * 9.050630569458008
Epoch 470, val loss: 0.38776373863220215
Epoch 480, training loss: 90.83314514160156 = 0.32137444615364075 + 10.0 * 9.051177024841309
Epoch 480, val loss: 0.38638395071029663
Epoch 490, training loss: 90.81210327148438 = 0.31828856468200684 + 10.0 * 9.049381256103516
Epoch 490, val loss: 0.3852219581604004
Epoch 500, training loss: 90.80580139160156 = 0.31529825925827026 + 10.0 * 9.049050331115723
Epoch 500, val loss: 0.38410845398902893
Epoch 510, training loss: 90.7969741821289 = 0.3123927712440491 + 10.0 * 9.048458099365234
Epoch 510, val loss: 0.3831116855144501
Epoch 520, training loss: 90.79762268066406 = 0.30957090854644775 + 10.0 * 9.048805236816406
Epoch 520, val loss: 0.3822246789932251
Epoch 530, training loss: 90.8087387084961 = 0.30684158205986023 + 10.0 * 9.050189971923828
Epoch 530, val loss: 0.3813728392124176
Epoch 540, training loss: 90.78593444824219 = 0.3042549192905426 + 10.0 * 9.048168182373047
Epoch 540, val loss: 0.3806789517402649
Epoch 550, training loss: 90.7750473022461 = 0.3017566502094269 + 10.0 * 9.04732894897461
Epoch 550, val loss: 0.3800416886806488
Epoch 560, training loss: 90.7703857421875 = 0.2993028461933136 + 10.0 * 9.04710865020752
Epoch 560, val loss: 0.3794589638710022
Epoch 570, training loss: 90.7622299194336 = 0.296904981136322 + 10.0 * 9.04653263092041
Epoch 570, val loss: 0.3790126144886017
Epoch 580, training loss: 90.7638931274414 = 0.2945820987224579 + 10.0 * 9.046931266784668
Epoch 580, val loss: 0.37852758169174194
Epoch 590, training loss: 90.75289154052734 = 0.2923174798488617 + 10.0 * 9.046056747436523
Epoch 590, val loss: 0.378156453371048
Epoch 600, training loss: 90.74510955810547 = 0.29009702801704407 + 10.0 * 9.045500755310059
Epoch 600, val loss: 0.3778136670589447
Epoch 610, training loss: 90.74870300292969 = 0.2879162132740021 + 10.0 * 9.0460786819458
Epoch 610, val loss: 0.3774642050266266
Epoch 620, training loss: 90.74838256835938 = 0.28579476475715637 + 10.0 * 9.046258926391602
Epoch 620, val loss: 0.37736162543296814
Epoch 630, training loss: 90.72758483886719 = 0.2837362587451935 + 10.0 * 9.044384956359863
Epoch 630, val loss: 0.3771093785762787
Epoch 640, training loss: 90.72220611572266 = 0.2817172110080719 + 10.0 * 9.044049263000488
Epoch 640, val loss: 0.3769300878047943
Epoch 650, training loss: 90.72483825683594 = 0.27972108125686646 + 10.0 * 9.044511795043945
Epoch 650, val loss: 0.3768337368965149
Epoch 660, training loss: 90.71094512939453 = 0.27775949239730835 + 10.0 * 9.043318748474121
Epoch 660, val loss: 0.37676578760147095
Epoch 670, training loss: 90.71739959716797 = 0.275851309299469 + 10.0 * 9.04415512084961
Epoch 670, val loss: 0.3767416477203369
Epoch 680, training loss: 90.70793914794922 = 0.2739739716053009 + 10.0 * 9.043396949768066
Epoch 680, val loss: 0.37674036622047424
Epoch 690, training loss: 90.71388244628906 = 0.2721240222454071 + 10.0 * 9.04417610168457
Epoch 690, val loss: 0.37684154510498047
Epoch 700, training loss: 90.69808959960938 = 0.2703094780445099 + 10.0 * 9.042778015136719
Epoch 700, val loss: 0.3769269287586212
Epoch 710, training loss: 90.69110107421875 = 0.268520712852478 + 10.0 * 9.042257308959961
Epoch 710, val loss: 0.3770488202571869
Epoch 720, training loss: 90.68531799316406 = 0.26675382256507874 + 10.0 * 9.04185676574707
Epoch 720, val loss: 0.3772467374801636
Epoch 730, training loss: 90.70568084716797 = 0.2650009095668793 + 10.0 * 9.044068336486816
Epoch 730, val loss: 0.3775498569011688
Epoch 740, training loss: 90.69486999511719 = 0.2632763385772705 + 10.0 * 9.043159484863281
Epoch 740, val loss: 0.3775942921638489
Epoch 750, training loss: 90.6740493774414 = 0.26158908009529114 + 10.0 * 9.04124641418457
Epoch 750, val loss: 0.37794968485832214
Epoch 760, training loss: 90.66763305664062 = 0.2599177658557892 + 10.0 * 9.040771484375
Epoch 760, val loss: 0.3782692551612854
Epoch 770, training loss: 90.66315460205078 = 0.25825855135917664 + 10.0 * 9.04049015045166
Epoch 770, val loss: 0.3785388171672821
Epoch 780, training loss: 90.67863464355469 = 0.2566150426864624 + 10.0 * 9.04220199584961
Epoch 780, val loss: 0.37893831729888916
Epoch 790, training loss: 90.67102813720703 = 0.25499972701072693 + 10.0 * 9.041603088378906
Epoch 790, val loss: 0.37938714027404785
Epoch 800, training loss: 90.658447265625 = 0.25340956449508667 + 10.0 * 9.04050350189209
Epoch 800, val loss: 0.3797549307346344
Epoch 810, training loss: 90.65776062011719 = 0.2518332898616791 + 10.0 * 9.040593147277832
Epoch 810, val loss: 0.3802576959133148
Epoch 820, training loss: 90.6495132446289 = 0.25027161836624146 + 10.0 * 9.039923667907715
Epoch 820, val loss: 0.38067853450775146
Epoch 830, training loss: 90.64314270019531 = 0.24873347580432892 + 10.0 * 9.039441108703613
Epoch 830, val loss: 0.38120463490486145
Epoch 840, training loss: 90.6419677734375 = 0.24721606075763702 + 10.0 * 9.039475440979004
Epoch 840, val loss: 0.3817700147628784
Epoch 850, training loss: 90.63450622558594 = 0.24570579826831818 + 10.0 * 9.038880348205566
Epoch 850, val loss: 0.3823390007019043
Epoch 860, training loss: 90.62740325927734 = 0.2442169338464737 + 10.0 * 9.038318634033203
Epoch 860, val loss: 0.382925420999527
Epoch 870, training loss: 90.62289428710938 = 0.2427380532026291 + 10.0 * 9.038015365600586
Epoch 870, val loss: 0.3835749328136444
Epoch 880, training loss: 90.63134765625 = 0.2412610948085785 + 10.0 * 9.039008140563965
Epoch 880, val loss: 0.38421523571014404
Epoch 890, training loss: 90.61710357666016 = 0.23979970812797546 + 10.0 * 9.03773021697998
Epoch 890, val loss: 0.38497495651245117
Epoch 900, training loss: 90.61813354492188 = 0.23835761845111847 + 10.0 * 9.03797721862793
Epoch 900, val loss: 0.38560178875923157
Epoch 910, training loss: 90.6299057006836 = 0.2369273453950882 + 10.0 * 9.039297103881836
Epoch 910, val loss: 0.386356383562088
Epoch 920, training loss: 90.61387634277344 = 0.23551900684833527 + 10.0 * 9.037836074829102
Epoch 920, val loss: 0.3873257339000702
Epoch 930, training loss: 90.61184692382812 = 0.23411628603935242 + 10.0 * 9.037773132324219
Epoch 930, val loss: 0.38799428939819336
Epoch 940, training loss: 90.60617065429688 = 0.2327297478914261 + 10.0 * 9.037343978881836
Epoch 940, val loss: 0.38889947533607483
Epoch 950, training loss: 90.5964126586914 = 0.23135006427764893 + 10.0 * 9.036505699157715
Epoch 950, val loss: 0.38970518112182617
Epoch 960, training loss: 90.59264373779297 = 0.22998447716236115 + 10.0 * 9.03626537322998
Epoch 960, val loss: 0.39061102271080017
Epoch 970, training loss: 90.60623931884766 = 0.228620246052742 + 10.0 * 9.037761688232422
Epoch 970, val loss: 0.39154133200645447
Epoch 980, training loss: 90.60359191894531 = 0.22726957499980927 + 10.0 * 9.03763198852539
Epoch 980, val loss: 0.39245569705963135
Epoch 990, training loss: 90.58680725097656 = 0.22593604028224945 + 10.0 * 9.036087036132812
Epoch 990, val loss: 0.3934696912765503
Epoch 1000, training loss: 90.58645629882812 = 0.22460941970348358 + 10.0 * 9.036184310913086
Epoch 1000, val loss: 0.39441177248954773
Epoch 1010, training loss: 90.5830078125 = 0.22329294681549072 + 10.0 * 9.035971641540527
Epoch 1010, val loss: 0.3954567611217499
Epoch 1020, training loss: 90.57211303710938 = 0.22198882699012756 + 10.0 * 9.035012245178223
Epoch 1020, val loss: 0.3964579999446869
Epoch 1030, training loss: 90.5668716430664 = 0.2206883430480957 + 10.0 * 9.034618377685547
Epoch 1030, val loss: 0.3975660502910614
Epoch 1040, training loss: 90.57449340820312 = 0.21939322352409363 + 10.0 * 9.035510063171387
Epoch 1040, val loss: 0.3987356424331665
Epoch 1050, training loss: 90.5693130493164 = 0.21810410916805267 + 10.0 * 9.035120964050293
Epoch 1050, val loss: 0.3997071087360382
Epoch 1060, training loss: 90.56632995605469 = 0.21683631837368011 + 10.0 * 9.03494930267334
Epoch 1060, val loss: 0.4009518325328827
Epoch 1070, training loss: 90.55447387695312 = 0.21557015180587769 + 10.0 * 9.033890724182129
Epoch 1070, val loss: 0.4020203649997711
Epoch 1080, training loss: 90.55819702148438 = 0.21430696547031403 + 10.0 * 9.034388542175293
Epoch 1080, val loss: 0.40319257974624634
Epoch 1090, training loss: 90.56127166748047 = 0.21305446326732635 + 10.0 * 9.034822463989258
Epoch 1090, val loss: 0.4044751524925232
Epoch 1100, training loss: 90.545654296875 = 0.21180877089500427 + 10.0 * 9.033384323120117
Epoch 1100, val loss: 0.4056103527545929
Epoch 1110, training loss: 90.55043029785156 = 0.21056988835334778 + 10.0 * 9.03398609161377
Epoch 1110, val loss: 0.4067511558532715
Epoch 1120, training loss: 90.54679870605469 = 0.20933414995670319 + 10.0 * 9.033746719360352
Epoch 1120, val loss: 0.40813523530960083
Epoch 1130, training loss: 90.55487060546875 = 0.20811305940151215 + 10.0 * 9.034675598144531
Epoch 1130, val loss: 0.40938425064086914
Epoch 1140, training loss: 90.53401184082031 = 0.20689356327056885 + 10.0 * 9.03271198272705
Epoch 1140, val loss: 0.41075626015663147
Epoch 1150, training loss: 90.533935546875 = 0.20568852126598358 + 10.0 * 9.032824516296387
Epoch 1150, val loss: 0.4119682013988495
Epoch 1160, training loss: 90.52830505371094 = 0.2044844925403595 + 10.0 * 9.032382011413574
Epoch 1160, val loss: 0.41336557269096375
Epoch 1170, training loss: 90.5244369506836 = 0.20327971875667572 + 10.0 * 9.032115936279297
Epoch 1170, val loss: 0.4146994948387146
Epoch 1180, training loss: 90.5566177368164 = 0.20208147168159485 + 10.0 * 9.035453796386719
Epoch 1180, val loss: 0.416087806224823
Epoch 1190, training loss: 90.5411605834961 = 0.20089440047740936 + 10.0 * 9.034026145935059
Epoch 1190, val loss: 0.4175533652305603
Epoch 1200, training loss: 90.52499389648438 = 0.19971327483654022 + 10.0 * 9.032527923583984
Epoch 1200, val loss: 0.41898760199546814
Epoch 1210, training loss: 90.51862335205078 = 0.19853805005550385 + 10.0 * 9.032008171081543
Epoch 1210, val loss: 0.4203576445579529
Epoch 1220, training loss: 90.51387023925781 = 0.19736452400684357 + 10.0 * 9.03165054321289
Epoch 1220, val loss: 0.4218219816684723
Epoch 1230, training loss: 90.50973510742188 = 0.196197047829628 + 10.0 * 9.031353950500488
Epoch 1230, val loss: 0.4233247935771942
Epoch 1240, training loss: 90.52081298828125 = 0.19503381848335266 + 10.0 * 9.032578468322754
Epoch 1240, val loss: 0.42484787106513977
Epoch 1250, training loss: 90.50921630859375 = 0.19386650621891022 + 10.0 * 9.031535148620605
Epoch 1250, val loss: 0.42623353004455566
Epoch 1260, training loss: 90.50434112548828 = 0.19271233677864075 + 10.0 * 9.031163215637207
Epoch 1260, val loss: 0.4277888238430023
Epoch 1270, training loss: 90.49769592285156 = 0.1915624886751175 + 10.0 * 9.030613899230957
Epoch 1270, val loss: 0.42938923835754395
Epoch 1280, training loss: 90.51240539550781 = 0.19041411578655243 + 10.0 * 9.032198905944824
Epoch 1280, val loss: 0.4310287833213806
Epoch 1290, training loss: 90.49642181396484 = 0.1892753690481186 + 10.0 * 9.03071403503418
Epoch 1290, val loss: 0.4325920045375824
Epoch 1300, training loss: 90.51925659179688 = 0.18813838064670563 + 10.0 * 9.033111572265625
Epoch 1300, val loss: 0.4342515468597412
Epoch 1310, training loss: 90.49970245361328 = 0.18700475990772247 + 10.0 * 9.031270027160645
Epoch 1310, val loss: 0.43572601675987244
Epoch 1320, training loss: 90.4837417602539 = 0.18588244915008545 + 10.0 * 9.029786109924316
Epoch 1320, val loss: 0.4374338686466217
Epoch 1330, training loss: 90.4798583984375 = 0.18475712835788727 + 10.0 * 9.029510498046875
Epoch 1330, val loss: 0.43910902738571167
Epoch 1340, training loss: 90.48445129394531 = 0.18363548815250397 + 10.0 * 9.030081748962402
Epoch 1340, val loss: 0.4407048225402832
Epoch 1350, training loss: 90.48745727539062 = 0.1825152486562729 + 10.0 * 9.03049373626709
Epoch 1350, val loss: 0.4424949586391449
Epoch 1360, training loss: 90.47110748291016 = 0.18141157925128937 + 10.0 * 9.028969764709473
Epoch 1360, val loss: 0.4441905915737152
Epoch 1370, training loss: 90.4717788696289 = 0.1803072690963745 + 10.0 * 9.029147148132324
Epoch 1370, val loss: 0.44605106115341187
Epoch 1380, training loss: 90.47391510009766 = 0.17919810116291046 + 10.0 * 9.029471397399902
Epoch 1380, val loss: 0.4477459490299225
Epoch 1390, training loss: 90.47163391113281 = 0.17809061706066132 + 10.0 * 9.029354095458984
Epoch 1390, val loss: 0.4495967924594879
Epoch 1400, training loss: 90.46715545654297 = 0.17700335383415222 + 10.0 * 9.02901554107666
Epoch 1400, val loss: 0.45133426785469055
Epoch 1410, training loss: 90.45978546142578 = 0.1759110540151596 + 10.0 * 9.028387069702148
Epoch 1410, val loss: 0.4531736671924591
Epoch 1420, training loss: 90.45587158203125 = 0.17481838166713715 + 10.0 * 9.028104782104492
Epoch 1420, val loss: 0.45500075817108154
Epoch 1430, training loss: 90.47052764892578 = 0.17373082041740417 + 10.0 * 9.029680252075195
Epoch 1430, val loss: 0.45695751905441284
Epoch 1440, training loss: 90.46273803710938 = 0.17263813316822052 + 10.0 * 9.029009819030762
Epoch 1440, val loss: 0.45876944065093994
Epoch 1450, training loss: 90.45912170410156 = 0.17156469821929932 + 10.0 * 9.028756141662598
Epoch 1450, val loss: 0.460718035697937
Epoch 1460, training loss: 90.45262145996094 = 0.17048102617263794 + 10.0 * 9.028214454650879
Epoch 1460, val loss: 0.4626103341579437
Epoch 1470, training loss: 90.45435333251953 = 0.16940273344516754 + 10.0 * 9.028494834899902
Epoch 1470, val loss: 0.4645470380783081
Epoch 1480, training loss: 90.44287872314453 = 0.168330118060112 + 10.0 * 9.02745532989502
Epoch 1480, val loss: 0.46651020646095276
Epoch 1490, training loss: 90.44544219970703 = 0.1672563999891281 + 10.0 * 9.02781867980957
Epoch 1490, val loss: 0.46852871775627136
Epoch 1500, training loss: 90.45751190185547 = 0.16618111729621887 + 10.0 * 9.029132843017578
Epoch 1500, val loss: 0.4704495966434479
Epoch 1510, training loss: 90.443359375 = 0.16511158645153046 + 10.0 * 9.027824401855469
Epoch 1510, val loss: 0.4725041389465332
Epoch 1520, training loss: 90.4354248046875 = 0.16404937207698822 + 10.0 * 9.027137756347656
Epoch 1520, val loss: 0.47467026114463806
Epoch 1530, training loss: 90.43318176269531 = 0.16298367083072662 + 10.0 * 9.027019500732422
Epoch 1530, val loss: 0.4767344295978546
Epoch 1540, training loss: 90.44891357421875 = 0.16192451119422913 + 10.0 * 9.028698921203613
Epoch 1540, val loss: 0.47889572381973267
Epoch 1550, training loss: 90.43130493164062 = 0.16085709631443024 + 10.0 * 9.027044296264648
Epoch 1550, val loss: 0.48092353343963623
Epoch 1560, training loss: 90.4230728149414 = 0.1597985476255417 + 10.0 * 9.026327133178711
Epoch 1560, val loss: 0.48310935497283936
Epoch 1570, training loss: 90.42020416259766 = 0.15873706340789795 + 10.0 * 9.02614688873291
Epoch 1570, val loss: 0.4853019118309021
Epoch 1580, training loss: 90.43728637695312 = 0.15768355131149292 + 10.0 * 9.027959823608398
Epoch 1580, val loss: 0.4875681400299072
Epoch 1590, training loss: 90.4231185913086 = 0.15661923587322235 + 10.0 * 9.026650428771973
Epoch 1590, val loss: 0.4895758330821991
Epoch 1600, training loss: 90.42903137207031 = 0.15557774901390076 + 10.0 * 9.027345657348633
Epoch 1600, val loss: 0.49193933606147766
Epoch 1610, training loss: 90.41552734375 = 0.15451687574386597 + 10.0 * 9.026101112365723
Epoch 1610, val loss: 0.49417397379875183
Epoch 1620, training loss: 90.41006469726562 = 0.15347127616405487 + 10.0 * 9.025659561157227
Epoch 1620, val loss: 0.4965516924858093
Epoch 1630, training loss: 90.40666198730469 = 0.1524212807416916 + 10.0 * 9.025424003601074
Epoch 1630, val loss: 0.4988739788532257
Epoch 1640, training loss: 90.40888214111328 = 0.15137124061584473 + 10.0 * 9.025751113891602
Epoch 1640, val loss: 0.5012991428375244
Epoch 1650, training loss: 90.42288208007812 = 0.15032295882701874 + 10.0 * 9.02725601196289
Epoch 1650, val loss: 0.5036988854408264
Epoch 1660, training loss: 90.40237426757812 = 0.14927183091640472 + 10.0 * 9.025310516357422
Epoch 1660, val loss: 0.5059531927108765
Epoch 1670, training loss: 90.40837097167969 = 0.14822672307491302 + 10.0 * 9.02601432800293
Epoch 1670, val loss: 0.508428692817688
Epoch 1680, training loss: 90.40390014648438 = 0.14718422293663025 + 10.0 * 9.025671005249023
Epoch 1680, val loss: 0.511001467704773
Epoch 1690, training loss: 90.40516662597656 = 0.14614072442054749 + 10.0 * 9.02590274810791
Epoch 1690, val loss: 0.5132946372032166
Epoch 1700, training loss: 90.39434051513672 = 0.14509959518909454 + 10.0 * 9.024924278259277
Epoch 1700, val loss: 0.5157937407493591
Epoch 1710, training loss: 90.38983917236328 = 0.14405669271945953 + 10.0 * 9.024578094482422
Epoch 1710, val loss: 0.5184268951416016
Epoch 1720, training loss: 90.39586639404297 = 0.14301824569702148 + 10.0 * 9.025284767150879
Epoch 1720, val loss: 0.5209853053092957
Epoch 1730, training loss: 90.3915786743164 = 0.14197328686714172 + 10.0 * 9.0249605178833
Epoch 1730, val loss: 0.5237176418304443
Epoch 1740, training loss: 90.38134002685547 = 0.1409342736005783 + 10.0 * 9.024040222167969
Epoch 1740, val loss: 0.5262904763221741
Epoch 1750, training loss: 90.38026428222656 = 0.13989286124706268 + 10.0 * 9.02403736114502
Epoch 1750, val loss: 0.5290005803108215
Epoch 1760, training loss: 90.39348602294922 = 0.138855442404747 + 10.0 * 9.025463104248047
Epoch 1760, val loss: 0.5317166447639465
Epoch 1770, training loss: 90.38480377197266 = 0.1378219723701477 + 10.0 * 9.024698257446289
Epoch 1770, val loss: 0.5342901945114136
Epoch 1780, training loss: 90.3831787109375 = 0.13678358495235443 + 10.0 * 9.024640083312988
Epoch 1780, val loss: 0.5369344353675842
Epoch 1790, training loss: 90.37413787841797 = 0.13574978709220886 + 10.0 * 9.023838996887207
Epoch 1790, val loss: 0.5397635698318481
Epoch 1800, training loss: 90.37431335449219 = 0.13471797108650208 + 10.0 * 9.023959159851074
Epoch 1800, val loss: 0.5426034927368164
Epoch 1810, training loss: 90.36880493164062 = 0.13368262350559235 + 10.0 * 9.02351188659668
Epoch 1810, val loss: 0.5453765988349915
Epoch 1820, training loss: 90.37165069580078 = 0.13265050947666168 + 10.0 * 9.023900032043457
Epoch 1820, val loss: 0.5482544302940369
Epoch 1830, training loss: 90.38648986816406 = 0.13162027299404144 + 10.0 * 9.025486946105957
Epoch 1830, val loss: 0.5511758923530579
Epoch 1840, training loss: 90.36820220947266 = 0.1305842250585556 + 10.0 * 9.023761749267578
Epoch 1840, val loss: 0.5540317893028259
Epoch 1850, training loss: 90.35882568359375 = 0.12955385446548462 + 10.0 * 9.022927284240723
Epoch 1850, val loss: 0.5568647384643555
Epoch 1860, training loss: 90.35453796386719 = 0.12852339446544647 + 10.0 * 9.022601127624512
Epoch 1860, val loss: 0.5598782896995544
Epoch 1870, training loss: 90.35796356201172 = 0.12749195098876953 + 10.0 * 9.023046493530273
Epoch 1870, val loss: 0.5628561973571777
Epoch 1880, training loss: 90.36647033691406 = 0.1264595091342926 + 10.0 * 9.024001121520996
Epoch 1880, val loss: 0.5658147931098938
Epoch 1890, training loss: 90.35909271240234 = 0.1254335194826126 + 10.0 * 9.02336597442627
Epoch 1890, val loss: 0.5688288807868958
Epoch 1900, training loss: 90.36135864257812 = 0.1243947371840477 + 10.0 * 9.023695945739746
Epoch 1900, val loss: 0.5718408823013306
Epoch 1910, training loss: 90.35675811767578 = 0.12336660176515579 + 10.0 * 9.02333927154541
Epoch 1910, val loss: 0.575027346611023
Epoch 1920, training loss: 90.34747314453125 = 0.12233386188745499 + 10.0 * 9.022513389587402
Epoch 1920, val loss: 0.5780717134475708
Epoch 1930, training loss: 90.34880065917969 = 0.1213073581457138 + 10.0 * 9.022748947143555
Epoch 1930, val loss: 0.5812925696372986
Epoch 1940, training loss: 90.34310150146484 = 0.12028444558382034 + 10.0 * 9.022281646728516
Epoch 1940, val loss: 0.5845253467559814
Epoch 1950, training loss: 90.3478775024414 = 0.11926297843456268 + 10.0 * 9.02286148071289
Epoch 1950, val loss: 0.587554931640625
Epoch 1960, training loss: 90.3409194946289 = 0.11823534220457077 + 10.0 * 9.022268295288086
Epoch 1960, val loss: 0.5907944440841675
Epoch 1970, training loss: 90.33624267578125 = 0.11720827221870422 + 10.0 * 9.021903038024902
Epoch 1970, val loss: 0.5941987633705139
Epoch 1980, training loss: 90.33228302001953 = 0.11618358641862869 + 10.0 * 9.021610260009766
Epoch 1980, val loss: 0.5974329710006714
Epoch 1990, training loss: 90.33757019042969 = 0.11515951156616211 + 10.0 * 9.022241592407227
Epoch 1990, val loss: 0.6007778644561768
Epoch 2000, training loss: 90.33543395996094 = 0.11413706839084625 + 10.0 * 9.022130012512207
Epoch 2000, val loss: 0.6042525172233582
Epoch 2010, training loss: 90.32794189453125 = 0.11310680210590363 + 10.0 * 9.021483421325684
Epoch 2010, val loss: 0.6076340079307556
Epoch 2020, training loss: 90.33006286621094 = 0.1120864748954773 + 10.0 * 9.021798133850098
Epoch 2020, val loss: 0.6110209822654724
Epoch 2030, training loss: 90.33021545410156 = 0.11106625199317932 + 10.0 * 9.0219144821167
Epoch 2030, val loss: 0.6145341992378235
Epoch 2040, training loss: 90.32449340820312 = 0.11003842949867249 + 10.0 * 9.021445274353027
Epoch 2040, val loss: 0.6177461743354797
Epoch 2050, training loss: 90.33318328857422 = 0.10902200639247894 + 10.0 * 9.022416114807129
Epoch 2050, val loss: 0.6214307546615601
Epoch 2060, training loss: 90.3259048461914 = 0.10799671709537506 + 10.0 * 9.021791458129883
Epoch 2060, val loss: 0.6246300339698792
Epoch 2070, training loss: 90.31883239746094 = 0.10697489976882935 + 10.0 * 9.021185874938965
Epoch 2070, val loss: 0.6282333135604858
Epoch 2080, training loss: 90.31546020507812 = 0.10595710575580597 + 10.0 * 9.020950317382812
Epoch 2080, val loss: 0.6317011713981628
Epoch 2090, training loss: 90.32422637939453 = 0.10494508594274521 + 10.0 * 9.021928787231445
Epoch 2090, val loss: 0.6354600787162781
Epoch 2100, training loss: 90.30690002441406 = 0.10392316430807114 + 10.0 * 9.02029800415039
Epoch 2100, val loss: 0.6386774778366089
Epoch 2110, training loss: 90.3026123046875 = 0.10290820151567459 + 10.0 * 9.019970893859863
Epoch 2110, val loss: 0.6424378752708435
Epoch 2120, training loss: 90.32777404785156 = 0.10190669447183609 + 10.0 * 9.022586822509766
Epoch 2120, val loss: 0.6461105346679688
Epoch 2130, training loss: 90.303955078125 = 0.10087798535823822 + 10.0 * 9.020307540893555
Epoch 2130, val loss: 0.6496328711509705
Epoch 2140, training loss: 90.3007583618164 = 0.09987857937812805 + 10.0 * 9.020088195800781
Epoch 2140, val loss: 0.6534103751182556
Epoch 2150, training loss: 90.30298614501953 = 0.09886682033538818 + 10.0 * 9.020411491394043
Epoch 2150, val loss: 0.656936526298523
Epoch 2160, training loss: 90.30133056640625 = 0.09786313027143478 + 10.0 * 9.020346641540527
Epoch 2160, val loss: 0.6607675552368164
Epoch 2170, training loss: 90.29295349121094 = 0.09686116874217987 + 10.0 * 9.019609451293945
Epoch 2170, val loss: 0.6644482016563416
Epoch 2180, training loss: 90.29727935791016 = 0.09585928916931152 + 10.0 * 9.020142555236816
Epoch 2180, val loss: 0.6682718992233276
Epoch 2190, training loss: 90.2992172241211 = 0.09486404061317444 + 10.0 * 9.020435333251953
Epoch 2190, val loss: 0.6722933053970337
Epoch 2200, training loss: 90.28590393066406 = 0.09387201070785522 + 10.0 * 9.019203186035156
Epoch 2200, val loss: 0.6758965849876404
Epoch 2210, training loss: 90.28732299804688 = 0.09288209676742554 + 10.0 * 9.019444465637207
Epoch 2210, val loss: 0.6798154711723328
Epoch 2220, training loss: 90.29443359375 = 0.09190063923597336 + 10.0 * 9.02025318145752
Epoch 2220, val loss: 0.6838163137435913
Epoch 2230, training loss: 90.28543853759766 = 0.09092116355895996 + 10.0 * 9.019452095031738
Epoch 2230, val loss: 0.6872064471244812
Epoch 2240, training loss: 90.30564880371094 = 0.08995175361633301 + 10.0 * 9.021570205688477
Epoch 2240, val loss: 0.6910603046417236
Epoch 2250, training loss: 90.28596496582031 = 0.08896354585886002 + 10.0 * 9.019700050354004
Epoch 2250, val loss: 0.6955350637435913
Epoch 2260, training loss: 90.27452087402344 = 0.08799447864294052 + 10.0 * 9.018651962280273
Epoch 2260, val loss: 0.6994388699531555
Epoch 2270, training loss: 90.26964569091797 = 0.0870252177119255 + 10.0 * 9.018261909484863
Epoch 2270, val loss: 0.7034969925880432
Epoch 2280, training loss: 90.26779174804688 = 0.08606012910604477 + 10.0 * 9.018173217773438
Epoch 2280, val loss: 0.7074941396713257
Epoch 2290, training loss: 90.27775573730469 = 0.08509950339794159 + 10.0 * 9.019266128540039
Epoch 2290, val loss: 0.7115896344184875
Epoch 2300, training loss: 90.28337860107422 = 0.084147147834301 + 10.0 * 9.019923210144043
Epoch 2300, val loss: 0.7160074710845947
Epoch 2310, training loss: 90.26762390136719 = 0.08318851888179779 + 10.0 * 9.01844310760498
Epoch 2310, val loss: 0.7198742032051086
Epoch 2320, training loss: 90.2635269165039 = 0.08224406838417053 + 10.0 * 9.018128395080566
Epoch 2320, val loss: 0.7240791916847229
Epoch 2330, training loss: 90.25801086425781 = 0.08129972964525223 + 10.0 * 9.017671585083008
Epoch 2330, val loss: 0.7283267378807068
Epoch 2340, training loss: 90.25946044921875 = 0.08036122471094131 + 10.0 * 9.01791000366211
Epoch 2340, val loss: 0.7325956225395203
Epoch 2350, training loss: 90.28764343261719 = 0.07944433391094208 + 10.0 * 9.020819664001465
Epoch 2350, val loss: 0.7371488213539124
Epoch 2360, training loss: 90.2730484008789 = 0.07850455492734909 + 10.0 * 9.019454002380371
Epoch 2360, val loss: 0.7409857511520386
Epoch 2370, training loss: 90.25756072998047 = 0.07757773250341415 + 10.0 * 9.017998695373535
Epoch 2370, val loss: 0.7453661561012268
Epoch 2380, training loss: 90.25230407714844 = 0.0766616091132164 + 10.0 * 9.01756477355957
Epoch 2380, val loss: 0.7494069933891296
Epoch 2390, training loss: 90.274658203125 = 0.07576024532318115 + 10.0 * 9.019889831542969
Epoch 2390, val loss: 0.7536487579345703
Epoch 2400, training loss: 90.25935363769531 = 0.07484748214483261 + 10.0 * 9.018450736999512
Epoch 2400, val loss: 0.7582080960273743
Epoch 2410, training loss: 90.24824523925781 = 0.07394910603761673 + 10.0 * 9.01742935180664
Epoch 2410, val loss: 0.7623074650764465
Epoch 2420, training loss: 90.2426528930664 = 0.07305467128753662 + 10.0 * 9.016960144042969
Epoch 2420, val loss: 0.7667873501777649
Epoch 2430, training loss: 90.24185943603516 = 0.07217080146074295 + 10.0 * 9.016968727111816
Epoch 2430, val loss: 0.7711844444274902
Epoch 2440, training loss: 90.2541732788086 = 0.07129918783903122 + 10.0 * 9.018287658691406
Epoch 2440, val loss: 0.7755778431892395
Epoch 2450, training loss: 90.24365234375 = 0.0704171359539032 + 10.0 * 9.01732349395752
Epoch 2450, val loss: 0.7798135280609131
Epoch 2460, training loss: 90.25115203857422 = 0.06955768167972565 + 10.0 * 9.018159866333008
Epoch 2460, val loss: 0.7841082811355591
Epoch 2470, training loss: 90.247314453125 = 0.06869839131832123 + 10.0 * 9.017862319946289
Epoch 2470, val loss: 0.7886248230934143
Epoch 2480, training loss: 90.23648834228516 = 0.06784775853157043 + 10.0 * 9.016863822937012
Epoch 2480, val loss: 0.7932419180870056
Epoch 2490, training loss: 90.23069763183594 = 0.06700127571821213 + 10.0 * 9.016369819641113
Epoch 2490, val loss: 0.7976604700088501
Epoch 2500, training loss: 90.23084259033203 = 0.06616642326116562 + 10.0 * 9.016467094421387
Epoch 2500, val loss: 0.8020101189613342
Epoch 2510, training loss: 90.25692749023438 = 0.06534533202648163 + 10.0 * 9.019158363342285
Epoch 2510, val loss: 0.8064368367195129
Epoch 2520, training loss: 90.23812866210938 = 0.0645231157541275 + 10.0 * 9.01736068725586
Epoch 2520, val loss: 0.8113728165626526
Epoch 2530, training loss: 90.24079895019531 = 0.06370718777179718 + 10.0 * 9.017709732055664
Epoch 2530, val loss: 0.8155977725982666
Epoch 2540, training loss: 90.22418212890625 = 0.06289662420749664 + 10.0 * 9.016128540039062
Epoch 2540, val loss: 0.8200821876525879
Epoch 2550, training loss: 90.22222137451172 = 0.06209767609834671 + 10.0 * 9.016012191772461
Epoch 2550, val loss: 0.8247128129005432
Epoch 2560, training loss: 90.2314453125 = 0.06132412329316139 + 10.0 * 9.017011642456055
Epoch 2560, val loss: 0.8296316266059875
Epoch 2570, training loss: 90.2168960571289 = 0.06052317097783089 + 10.0 * 9.015637397766113
Epoch 2570, val loss: 0.8336511850357056
Epoch 2580, training loss: 90.22023010253906 = 0.05974910408258438 + 10.0 * 9.016048431396484
Epoch 2580, val loss: 0.8384787440299988
Epoch 2590, training loss: 90.23553466796875 = 0.05900176987051964 + 10.0 * 9.017653465270996
Epoch 2590, val loss: 0.8434062600135803
Epoch 2600, training loss: 90.2203140258789 = 0.058225613087415695 + 10.0 * 9.01620864868164
Epoch 2600, val loss: 0.8471395969390869
Epoch 2610, training loss: 90.21513366699219 = 0.05747058242559433 + 10.0 * 9.015766143798828
Epoch 2610, val loss: 0.85201096534729
Epoch 2620, training loss: 90.21438598632812 = 0.05672784894704819 + 10.0 * 9.015766143798828
Epoch 2620, val loss: 0.8566951751708984
Epoch 2630, training loss: 90.21040344238281 = 0.05599018186330795 + 10.0 * 9.015440940856934
Epoch 2630, val loss: 0.8612842559814453
Epoch 2640, training loss: 90.21532440185547 = 0.05525656417012215 + 10.0 * 9.016006469726562
Epoch 2640, val loss: 0.8656796216964722
Epoch 2650, training loss: 90.21484375 = 0.05453789606690407 + 10.0 * 9.016031265258789
Epoch 2650, val loss: 0.8698521256446838
Epoch 2660, training loss: 90.22341918945312 = 0.0538296177983284 + 10.0 * 9.016958236694336
Epoch 2660, val loss: 0.8748350143432617
Epoch 2670, training loss: 90.2076187133789 = 0.053121913224458694 + 10.0 * 9.015449523925781
Epoch 2670, val loss: 0.8792935013771057
Epoch 2680, training loss: 90.20257568359375 = 0.05242852121591568 + 10.0 * 9.0150146484375
Epoch 2680, val loss: 0.8839972615242004
Epoch 2690, training loss: 90.19734191894531 = 0.05174198001623154 + 10.0 * 9.014559745788574
Epoch 2690, val loss: 0.8884811401367188
Epoch 2700, training loss: 90.19451141357422 = 0.05106358230113983 + 10.0 * 9.014345169067383
Epoch 2700, val loss: 0.8931556940078735
Epoch 2710, training loss: 90.2049789428711 = 0.05039794743061066 + 10.0 * 9.015458106994629
Epoch 2710, val loss: 0.8975909352302551
Epoch 2720, training loss: 90.19706726074219 = 0.049733977764844894 + 10.0 * 9.01473331451416
Epoch 2720, val loss: 0.9022824168205261
Epoch 2730, training loss: 90.19282531738281 = 0.04908345639705658 + 10.0 * 9.014374732971191
Epoch 2730, val loss: 0.9068098664283752
Epoch 2740, training loss: 90.19847869873047 = 0.048440612852573395 + 10.0 * 9.01500415802002
Epoch 2740, val loss: 0.9113467931747437
Epoch 2750, training loss: 90.201416015625 = 0.047804512083530426 + 10.0 * 9.015360832214355
Epoch 2750, val loss: 0.9159893989562988
Epoch 2760, training loss: 90.19033813476562 = 0.04717771336436272 + 10.0 * 9.014315605163574
Epoch 2760, val loss: 0.9206190705299377
Epoch 2770, training loss: 90.20532989501953 = 0.04656390845775604 + 10.0 * 9.015876770019531
Epoch 2770, val loss: 0.9253752827644348
Epoch 2780, training loss: 90.18479919433594 = 0.04594534635543823 + 10.0 * 9.013885498046875
Epoch 2780, val loss: 0.9296294450759888
Epoch 2790, training loss: 90.18453979492188 = 0.045340195298194885 + 10.0 * 9.013919830322266
Epoch 2790, val loss: 0.9341639280319214
Epoch 2800, training loss: 90.18224334716797 = 0.04474528133869171 + 10.0 * 9.013750076293945
Epoch 2800, val loss: 0.9386842250823975
Epoch 2810, training loss: 90.19145202636719 = 0.044167909771203995 + 10.0 * 9.014728546142578
Epoch 2810, val loss: 0.9434360265731812
Epoch 2820, training loss: 90.18252563476562 = 0.04358263313770294 + 10.0 * 9.013895034790039
Epoch 2820, val loss: 0.9480080604553223
Epoch 2830, training loss: 90.18647766113281 = 0.04301321133971214 + 10.0 * 9.0143461227417
Epoch 2830, val loss: 0.9522413611412048
Epoch 2840, training loss: 90.1808853149414 = 0.04244723916053772 + 10.0 * 9.013843536376953
Epoch 2840, val loss: 0.9567337036132812
Epoch 2850, training loss: 90.17870330810547 = 0.041900016367435455 + 10.0 * 9.013680458068848
Epoch 2850, val loss: 0.9613008499145508
Epoch 2860, training loss: 90.17737579345703 = 0.04136041924357414 + 10.0 * 9.013601303100586
Epoch 2860, val loss: 0.9662009477615356
Epoch 2870, training loss: 90.1844482421875 = 0.040832486003637314 + 10.0 * 9.014361381530762
Epoch 2870, val loss: 0.9707342982292175
Epoch 2880, training loss: 90.18746185302734 = 0.040290165692567825 + 10.0 * 9.014717102050781
Epoch 2880, val loss: 0.9742838740348816
Epoch 2890, training loss: 90.17517852783203 = 0.03976426273584366 + 10.0 * 9.013541221618652
Epoch 2890, val loss: 0.9794820547103882
Epoch 2900, training loss: 90.17584228515625 = 0.03924635797739029 + 10.0 * 9.013659477233887
Epoch 2900, val loss: 0.9834001660346985
Epoch 2910, training loss: 90.16995239257812 = 0.03874298557639122 + 10.0 * 9.013120651245117
Epoch 2910, val loss: 0.9883080124855042
Epoch 2920, training loss: 90.1792984008789 = 0.0382428802549839 + 10.0 * 9.014104843139648
Epoch 2920, val loss: 0.9922968149185181
Epoch 2930, training loss: 90.16809844970703 = 0.03774291276931763 + 10.0 * 9.013035774230957
Epoch 2930, val loss: 0.9968822598457336
Epoch 2940, training loss: 90.18658447265625 = 0.03726775199174881 + 10.0 * 9.014931678771973
Epoch 2940, val loss: 1.0016229152679443
Epoch 2950, training loss: 90.1678237915039 = 0.036779098212718964 + 10.0 * 9.013104438781738
Epoch 2950, val loss: 1.0053410530090332
Epoch 2960, training loss: 90.16033172607422 = 0.036306578665971756 + 10.0 * 9.012402534484863
Epoch 2960, val loss: 1.009874701499939
Epoch 2970, training loss: 90.15797424316406 = 0.03584202751517296 + 10.0 * 9.012212753295898
Epoch 2970, val loss: 1.0141220092773438
Epoch 2980, training loss: 90.16128540039062 = 0.03538510575890541 + 10.0 * 9.012590408325195
Epoch 2980, val loss: 1.0183974504470825
Epoch 2990, training loss: 90.17542266845703 = 0.034938279539346695 + 10.0 * 9.01404857635498
Epoch 2990, val loss: 1.0228025913238525
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8483
Overall ASR: 0.7018
Flip ASR: 0.6274/1554 nodes
./selected_nodes/Pubmed/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 104.68025207519531 = 1.090893268585205 + 10.0 * 10.358935356140137
Epoch 0, val loss: 1.0901724100112915
Epoch 10, training loss: 104.62016296386719 = 1.0802632570266724 + 10.0 * 10.35399055480957
Epoch 10, val loss: 1.0794734954833984
Epoch 20, training loss: 103.89580535888672 = 1.0671887397766113 + 10.0 * 10.282861709594727
Epoch 20, val loss: 1.0668871402740479
Epoch 30, training loss: 98.7141342163086 = 1.0577237606048584 + 10.0 * 9.765641212463379
Epoch 30, val loss: 1.0575894117355347
Epoch 40, training loss: 95.7258071899414 = 1.0434070825576782 + 10.0 * 9.468240737915039
Epoch 40, val loss: 1.0424327850341797
Epoch 50, training loss: 94.77922058105469 = 1.0232551097869873 + 10.0 * 9.37559700012207
Epoch 50, val loss: 1.022526741027832
Epoch 60, training loss: 94.34978485107422 = 1.0055171251296997 + 10.0 * 9.334426879882812
Epoch 60, val loss: 1.0055193901062012
Epoch 70, training loss: 93.69188690185547 = 0.9927220344543457 + 10.0 * 9.269916534423828
Epoch 70, val loss: 0.9934144616127014
Epoch 80, training loss: 93.23355865478516 = 0.9829773306846619 + 10.0 * 9.225057601928711
Epoch 80, val loss: 0.9836984872817993
Epoch 90, training loss: 92.9904556274414 = 0.9704557061195374 + 10.0 * 9.20199966430664
Epoch 90, val loss: 0.9708908200263977
Epoch 100, training loss: 92.80148315429688 = 0.9535976648330688 + 10.0 * 9.184788703918457
Epoch 100, val loss: 0.9542093276977539
Epoch 110, training loss: 92.6143798828125 = 0.9357730746269226 + 10.0 * 9.16786003112793
Epoch 110, val loss: 0.9369940757751465
Epoch 120, training loss: 92.45218658447266 = 0.9188199639320374 + 10.0 * 9.153336524963379
Epoch 120, val loss: 0.9204686880111694
Epoch 130, training loss: 92.37657165527344 = 0.8991996645927429 + 10.0 * 9.147737503051758
Epoch 130, val loss: 0.9010274410247803
Epoch 140, training loss: 92.27830505371094 = 0.8741602301597595 + 10.0 * 9.140414237976074
Epoch 140, val loss: 0.8762536644935608
Epoch 150, training loss: 92.19548034667969 = 0.8458739519119263 + 10.0 * 9.134961128234863
Epoch 150, val loss: 0.848838746547699
Epoch 160, training loss: 92.1023941040039 = 0.8170310854911804 + 10.0 * 9.128536224365234
Epoch 160, val loss: 0.82108074426651
Epoch 170, training loss: 91.99825286865234 = 0.7874583601951599 + 10.0 * 9.121079444885254
Epoch 170, val loss: 0.7926306128501892
Epoch 180, training loss: 91.88872528076172 = 0.7558639645576477 + 10.0 * 9.113286018371582
Epoch 180, val loss: 0.7621671557426453
Epoch 190, training loss: 91.78960418701172 = 0.7214828133583069 + 10.0 * 9.106812477111816
Epoch 190, val loss: 0.7291218042373657
Epoch 200, training loss: 91.6850814819336 = 0.6847787499427795 + 10.0 * 9.100030899047852
Epoch 200, val loss: 0.6941806077957153
Epoch 210, training loss: 91.61457824707031 = 0.6468995809555054 + 10.0 * 9.096768379211426
Epoch 210, val loss: 0.6583631038665771
Epoch 220, training loss: 91.51954650878906 = 0.6092064380645752 + 10.0 * 9.091033935546875
Epoch 220, val loss: 0.6232131123542786
Epoch 230, training loss: 91.45471954345703 = 0.5728960633277893 + 10.0 * 9.08818244934082
Epoch 230, val loss: 0.5896191596984863
Epoch 240, training loss: 91.38951110839844 = 0.5388662815093994 + 10.0 * 9.085064888000488
Epoch 240, val loss: 0.5586158037185669
Epoch 250, training loss: 91.34610748291016 = 0.5080158114433289 + 10.0 * 9.083808898925781
Epoch 250, val loss: 0.5308822393417358
Epoch 260, training loss: 91.28529357910156 = 0.48081254959106445 + 10.0 * 9.080448150634766
Epoch 260, val loss: 0.5068844556808472
Epoch 270, training loss: 91.23856353759766 = 0.4572468400001526 + 10.0 * 9.078131675720215
Epoch 270, val loss: 0.4865092933177948
Epoch 280, training loss: 91.23204040527344 = 0.4369809925556183 + 10.0 * 9.079505920410156
Epoch 280, val loss: 0.46937957406044006
Epoch 290, training loss: 91.18135833740234 = 0.41995254158973694 + 10.0 * 9.076140403747559
Epoch 290, val loss: 0.4553200304508209
Epoch 300, training loss: 91.14161682128906 = 0.4056070148944855 + 10.0 * 9.073600769042969
Epoch 300, val loss: 0.44385820627212524
Epoch 310, training loss: 91.13423156738281 = 0.393370121717453 + 10.0 * 9.07408618927002
Epoch 310, val loss: 0.4344463050365448
Epoch 320, training loss: 91.0915756225586 = 0.3829135000705719 + 10.0 * 9.070866584777832
Epoch 320, val loss: 0.4267016053199768
Epoch 330, training loss: 91.07337951660156 = 0.3739236295223236 + 10.0 * 9.069945335388184
Epoch 330, val loss: 0.42031583189964294
Epoch 340, training loss: 91.04771423339844 = 0.36607393622398376 + 10.0 * 9.068163871765137
Epoch 340, val loss: 0.4150199294090271
Epoch 350, training loss: 91.02894592285156 = 0.35912325978279114 + 10.0 * 9.06698226928711
Epoch 350, val loss: 0.4105552136898041
Epoch 360, training loss: 91.01846313476562 = 0.3528902232646942 + 10.0 * 9.066556930541992
Epoch 360, val loss: 0.40669119358062744
Epoch 370, training loss: 91.0075454711914 = 0.34728169441223145 + 10.0 * 9.06602668762207
Epoch 370, val loss: 0.4034702777862549
Epoch 380, training loss: 90.98175048828125 = 0.3421887457370758 + 10.0 * 9.063956260681152
Epoch 380, val loss: 0.40066349506378174
Epoch 390, training loss: 90.97624969482422 = 0.33750271797180176 + 10.0 * 9.063875198364258
Epoch 390, val loss: 0.3982020914554596
Epoch 400, training loss: 90.97525024414062 = 0.3331492245197296 + 10.0 * 9.064209938049316
Epoch 400, val loss: 0.3960687518119812
Epoch 410, training loss: 90.95277404785156 = 0.3291204273700714 + 10.0 * 9.062365531921387
Epoch 410, val loss: 0.39424845576286316
Epoch 420, training loss: 90.94194793701172 = 0.32534709572792053 + 10.0 * 9.061659812927246
Epoch 420, val loss: 0.3925863802433014
Epoch 430, training loss: 90.92206573486328 = 0.32179781794548035 + 10.0 * 9.060026168823242
Epoch 430, val loss: 0.3911336660385132
Epoch 440, training loss: 90.91165924072266 = 0.318416029214859 + 10.0 * 9.059324264526367
Epoch 440, val loss: 0.3898504078388214
Epoch 450, training loss: 90.9098892211914 = 0.31519487500190735 + 10.0 * 9.059469223022461
Epoch 450, val loss: 0.38872209191322327
Epoch 460, training loss: 90.89804077148438 = 0.3121378421783447 + 10.0 * 9.058589935302734
Epoch 460, val loss: 0.3876916170120239
Epoch 470, training loss: 90.88330078125 = 0.30922096967697144 + 10.0 * 9.057408332824707
Epoch 470, val loss: 0.3867814540863037
Epoch 480, training loss: 90.87759399414062 = 0.30641597509384155 + 10.0 * 9.057117462158203
Epoch 480, val loss: 0.38599076867103577
Epoch 490, training loss: 90.87760162353516 = 0.3037136495113373 + 10.0 * 9.057389259338379
Epoch 490, val loss: 0.3852027356624603
Epoch 500, training loss: 90.85757446289062 = 0.3011211156845093 + 10.0 * 9.055644989013672
Epoch 500, val loss: 0.38461804389953613
Epoch 510, training loss: 90.84456634521484 = 0.2986075282096863 + 10.0 * 9.054595947265625
Epoch 510, val loss: 0.3840262293815613
Epoch 520, training loss: 90.85574340820312 = 0.29615849256515503 + 10.0 * 9.05595874786377
Epoch 520, val loss: 0.3836161494255066
Epoch 530, training loss: 90.85580444335938 = 0.29378512501716614 + 10.0 * 9.056201934814453
Epoch 530, val loss: 0.3831649124622345
Epoch 540, training loss: 90.82748413085938 = 0.29151207208633423 + 10.0 * 9.053597450256348
Epoch 540, val loss: 0.3827562630176544
Epoch 550, training loss: 90.81373596191406 = 0.2892933785915375 + 10.0 * 9.052444458007812
Epoch 550, val loss: 0.38246214389801025
Epoch 560, training loss: 90.80377197265625 = 0.2871098518371582 + 10.0 * 9.051666259765625
Epoch 560, val loss: 0.3822052776813507
Epoch 570, training loss: 90.85233306884766 = 0.28497061133384705 + 10.0 * 9.05673599243164
Epoch 570, val loss: 0.38202258944511414
Epoch 580, training loss: 90.80491638183594 = 0.2829091548919678 + 10.0 * 9.052201271057129
Epoch 580, val loss: 0.38187333941459656
Epoch 590, training loss: 90.7876968383789 = 0.2809046804904938 + 10.0 * 9.050679206848145
Epoch 590, val loss: 0.38174116611480713
Epoch 600, training loss: 90.77748107910156 = 0.278927743434906 + 10.0 * 9.04985523223877
Epoch 600, val loss: 0.38169243931770325
Epoch 610, training loss: 90.81481170654297 = 0.2769777178764343 + 10.0 * 9.053783416748047
Epoch 610, val loss: 0.38165774941444397
Epoch 620, training loss: 90.7718276977539 = 0.2750779688358307 + 10.0 * 9.049674987792969
Epoch 620, val loss: 0.38165125250816345
Epoch 630, training loss: 90.76204681396484 = 0.2732258439064026 + 10.0 * 9.048882484436035
Epoch 630, val loss: 0.3816741108894348
Epoch 640, training loss: 90.75312805175781 = 0.2713984251022339 + 10.0 * 9.048172950744629
Epoch 640, val loss: 0.38175860047340393
Epoch 650, training loss: 90.76213836669922 = 0.26959228515625 + 10.0 * 9.049254417419434
Epoch 650, val loss: 0.3818460702896118
Epoch 660, training loss: 90.7528305053711 = 0.2678188383579254 + 10.0 * 9.048501014709473
Epoch 660, val loss: 0.3820999264717102
Epoch 670, training loss: 90.73336791992188 = 0.26608163118362427 + 10.0 * 9.046728134155273
Epoch 670, val loss: 0.3822878301143646
Epoch 680, training loss: 90.7263412475586 = 0.26436442136764526 + 10.0 * 9.046197891235352
Epoch 680, val loss: 0.382534384727478
Epoch 690, training loss: 90.7676773071289 = 0.2626588046550751 + 10.0 * 9.050501823425293
Epoch 690, val loss: 0.38286134600639343
Epoch 700, training loss: 90.72236633300781 = 0.26100581884384155 + 10.0 * 9.046135902404785
Epoch 700, val loss: 0.3831253945827484
Epoch 710, training loss: 90.7138442993164 = 0.2593766152858734 + 10.0 * 9.045446395874023
Epoch 710, val loss: 0.38342392444610596
Epoch 720, training loss: 90.70801544189453 = 0.2577575743198395 + 10.0 * 9.045025825500488
Epoch 720, val loss: 0.3837822377681732
Epoch 730, training loss: 90.70584106445312 = 0.2561502456665039 + 10.0 * 9.04496955871582
Epoch 730, val loss: 0.38412824273109436
Epoch 740, training loss: 90.70758819580078 = 0.25456398725509644 + 10.0 * 9.045302391052246
Epoch 740, val loss: 0.38462305068969727
Epoch 750, training loss: 90.70025634765625 = 0.2530082166194916 + 10.0 * 9.04472541809082
Epoch 750, val loss: 0.38503414392471313
Epoch 760, training loss: 90.68753051757812 = 0.2514646053314209 + 10.0 * 9.043606758117676
Epoch 760, val loss: 0.38556361198425293
Epoch 770, training loss: 90.6850814819336 = 0.24993160367012024 + 10.0 * 9.0435152053833
Epoch 770, val loss: 0.3860912024974823
Epoch 780, training loss: 90.70146942138672 = 0.24840860068798065 + 10.0 * 9.045306205749512
Epoch 780, val loss: 0.3866293430328369
Epoch 790, training loss: 90.70628356933594 = 0.2469177544116974 + 10.0 * 9.045936584472656
Epoch 790, val loss: 0.38729798793792725
Epoch 800, training loss: 90.67404174804688 = 0.24543729424476624 + 10.0 * 9.04286003112793
Epoch 800, val loss: 0.3878547251224518
Epoch 810, training loss: 90.66935729980469 = 0.24397622048854828 + 10.0 * 9.0425386428833
Epoch 810, val loss: 0.3884509801864624
Epoch 820, training loss: 90.66255187988281 = 0.24252453446388245 + 10.0 * 9.04200267791748
Epoch 820, val loss: 0.3891231119632721
Epoch 830, training loss: 90.67046356201172 = 0.2410767674446106 + 10.0 * 9.042939186096191
Epoch 830, val loss: 0.3898279368877411
Epoch 840, training loss: 90.6662826538086 = 0.23964382708072662 + 10.0 * 9.04266357421875
Epoch 840, val loss: 0.3906274437904358
Epoch 850, training loss: 90.65418243408203 = 0.23823080956935883 + 10.0 * 9.041595458984375
Epoch 850, val loss: 0.3913846015930176
Epoch 860, training loss: 90.64903259277344 = 0.2368297427892685 + 10.0 * 9.041219711303711
Epoch 860, val loss: 0.39221951365470886
Epoch 870, training loss: 90.65081024169922 = 0.23543408513069153 + 10.0 * 9.041537284851074
Epoch 870, val loss: 0.39301797747612
Epoch 880, training loss: 90.63809204101562 = 0.23405107855796814 + 10.0 * 9.040403366088867
Epoch 880, val loss: 0.39393505454063416
Epoch 890, training loss: 90.63631439208984 = 0.23267990350723267 + 10.0 * 9.040363311767578
Epoch 890, val loss: 0.39480090141296387
Epoch 900, training loss: 90.64567565917969 = 0.23131759464740753 + 10.0 * 9.041436195373535
Epoch 900, val loss: 0.39576420187950134
Epoch 910, training loss: 90.63524627685547 = 0.229964479804039 + 10.0 * 9.040528297424316
Epoch 910, val loss: 0.3967575430870056
Epoch 920, training loss: 90.63329315185547 = 0.22862352430820465 + 10.0 * 9.040467262268066
Epoch 920, val loss: 0.3977346122264862
Epoch 930, training loss: 90.62459564208984 = 0.22729210555553436 + 10.0 * 9.039730072021484
Epoch 930, val loss: 0.39879563450813293
Epoch 940, training loss: 90.64002990722656 = 0.22597451508045197 + 10.0 * 9.04140567779541
Epoch 940, val loss: 0.3998250365257263
Epoch 950, training loss: 90.61856842041016 = 0.22466512024402618 + 10.0 * 9.039390563964844
Epoch 950, val loss: 0.4009561538696289
Epoch 960, training loss: 90.60932922363281 = 0.2233637571334839 + 10.0 * 9.038596153259277
Epoch 960, val loss: 0.4020552337169647
Epoch 970, training loss: 90.60435485839844 = 0.22206391394138336 + 10.0 * 9.038228988647461
Epoch 970, val loss: 0.4032265543937683
Epoch 980, training loss: 90.63849639892578 = 0.22077374160289764 + 10.0 * 9.041772842407227
Epoch 980, val loss: 0.4044647514820099
Epoch 990, training loss: 90.6216049194336 = 0.21949069201946259 + 10.0 * 9.04021167755127
Epoch 990, val loss: 0.4055927097797394
Epoch 1000, training loss: 90.59745025634766 = 0.2182266116142273 + 10.0 * 9.037922859191895
Epoch 1000, val loss: 0.4068814814090729
Epoch 1010, training loss: 90.59102630615234 = 0.2169674187898636 + 10.0 * 9.037405967712402
Epoch 1010, val loss: 0.40805763006210327
Epoch 1020, training loss: 90.5881118774414 = 0.21570934355258942 + 10.0 * 9.037240982055664
Epoch 1020, val loss: 0.40931692719459534
Epoch 1030, training loss: 90.60118865966797 = 0.21445633471012115 + 10.0 * 9.038673400878906
Epoch 1030, val loss: 0.4105475842952728
Epoch 1040, training loss: 90.58692932128906 = 0.21320870518684387 + 10.0 * 9.037371635437012
Epoch 1040, val loss: 0.4120235741138458
Epoch 1050, training loss: 90.59557342529297 = 0.21197392046451569 + 10.0 * 9.038359642028809
Epoch 1050, val loss: 0.41330575942993164
Epoch 1060, training loss: 90.5886459350586 = 0.21074996888637543 + 10.0 * 9.037790298461914
Epoch 1060, val loss: 0.4148221015930176
Epoch 1070, training loss: 90.57470703125 = 0.20953090488910675 + 10.0 * 9.036517143249512
Epoch 1070, val loss: 0.4161083400249481
Epoch 1080, training loss: 90.56840515136719 = 0.20831677317619324 + 10.0 * 9.036008834838867
Epoch 1080, val loss: 0.4175853133201599
Epoch 1090, training loss: 90.58084106445312 = 0.20710588991641998 + 10.0 * 9.037373542785645
Epoch 1090, val loss: 0.4190089702606201
Epoch 1100, training loss: 90.57477569580078 = 0.20590615272521973 + 10.0 * 9.036886215209961
Epoch 1100, val loss: 0.42038607597351074
Epoch 1110, training loss: 90.56551361083984 = 0.2047143280506134 + 10.0 * 9.036080360412598
Epoch 1110, val loss: 0.42204153537750244
Epoch 1120, training loss: 90.55863952636719 = 0.2035275101661682 + 10.0 * 9.035511016845703
Epoch 1120, val loss: 0.4235025644302368
Epoch 1130, training loss: 90.55078125 = 0.20234115421772003 + 10.0 * 9.034844398498535
Epoch 1130, val loss: 0.4250331223011017
Epoch 1140, training loss: 90.54988098144531 = 0.20115973055362701 + 10.0 * 9.034872055053711
Epoch 1140, val loss: 0.4266287088394165
Epoch 1150, training loss: 90.57723999023438 = 0.1999846249818802 + 10.0 * 9.037725448608398
Epoch 1150, val loss: 0.42821621894836426
Epoch 1160, training loss: 90.55577087402344 = 0.1988094300031662 + 10.0 * 9.035696029663086
Epoch 1160, val loss: 0.42980092763900757
Epoch 1170, training loss: 90.54067993164062 = 0.19764630496501923 + 10.0 * 9.034303665161133
Epoch 1170, val loss: 0.43147706985473633
Epoch 1180, training loss: 90.53714752197266 = 0.19648626446723938 + 10.0 * 9.034066200256348
Epoch 1180, val loss: 0.4331014156341553
Epoch 1190, training loss: 90.54780578613281 = 0.19533026218414307 + 10.0 * 9.035247802734375
Epoch 1190, val loss: 0.43476027250289917
Epoch 1200, training loss: 90.53237915039062 = 0.1941775381565094 + 10.0 * 9.033820152282715
Epoch 1200, val loss: 0.43656986951828003
Epoch 1210, training loss: 90.53197479248047 = 0.1930333822965622 + 10.0 * 9.033894538879395
Epoch 1210, val loss: 0.43826618790626526
Epoch 1220, training loss: 90.54244995117188 = 0.1918925940990448 + 10.0 * 9.035055160522461
Epoch 1220, val loss: 0.44008228182792664
Epoch 1230, training loss: 90.52056884765625 = 0.19075627624988556 + 10.0 * 9.032980918884277
Epoch 1230, val loss: 0.44180870056152344
Epoch 1240, training loss: 90.51946258544922 = 0.18962544202804565 + 10.0 * 9.032983779907227
Epoch 1240, val loss: 0.44361141324043274
Epoch 1250, training loss: 90.53242492675781 = 0.1884979009628296 + 10.0 * 9.034392356872559
Epoch 1250, val loss: 0.4454367458820343
Epoch 1260, training loss: 90.5179672241211 = 0.1873771846294403 + 10.0 * 9.033059120178223
Epoch 1260, val loss: 0.44744762778282166
Epoch 1270, training loss: 90.52173614501953 = 0.1862608641386032 + 10.0 * 9.033547401428223
Epoch 1270, val loss: 0.44921183586120605
Epoch 1280, training loss: 90.51490020751953 = 0.18514321744441986 + 10.0 * 9.032976150512695
Epoch 1280, val loss: 0.4510456323623657
Epoch 1290, training loss: 90.51155090332031 = 0.18403014540672302 + 10.0 * 9.03275203704834
Epoch 1290, val loss: 0.4529828429222107
Epoch 1300, training loss: 90.50801849365234 = 0.18292291462421417 + 10.0 * 9.032509803771973
Epoch 1300, val loss: 0.4550930857658386
Epoch 1310, training loss: 90.50216674804688 = 0.18181650340557098 + 10.0 * 9.032034873962402
Epoch 1310, val loss: 0.4569629430770874
Epoch 1320, training loss: 90.50515747070312 = 0.18071559071540833 + 10.0 * 9.03244400024414
Epoch 1320, val loss: 0.4589798152446747
Epoch 1330, training loss: 90.49716186523438 = 0.17961116135120392 + 10.0 * 9.031755447387695
Epoch 1330, val loss: 0.460911363363266
Epoch 1340, training loss: 90.49447631835938 = 0.1785040646791458 + 10.0 * 9.031597137451172
Epoch 1340, val loss: 0.4630173444747925
Epoch 1350, training loss: 90.49738311767578 = 0.17740140855312347 + 10.0 * 9.031998634338379
Epoch 1350, val loss: 0.46514949202537537
Epoch 1360, training loss: 90.4930191040039 = 0.17630401253700256 + 10.0 * 9.031671524047852
Epoch 1360, val loss: 0.46732595562934875
Epoch 1370, training loss: 90.49092864990234 = 0.17520910501480103 + 10.0 * 9.031572341918945
Epoch 1370, val loss: 0.46939876675605774
Epoch 1380, training loss: 90.49279022216797 = 0.17411436140537262 + 10.0 * 9.031867027282715
Epoch 1380, val loss: 0.47141966223716736
Epoch 1390, training loss: 90.47950744628906 = 0.1730315089225769 + 10.0 * 9.030647277832031
Epoch 1390, val loss: 0.47354385256767273
Epoch 1400, training loss: 90.4766616821289 = 0.1719425916671753 + 10.0 * 9.030471801757812
Epoch 1400, val loss: 0.4757521450519562
Epoch 1410, training loss: 90.4937973022461 = 0.1708597093820572 + 10.0 * 9.032293319702148
Epoch 1410, val loss: 0.47807127237319946
Epoch 1420, training loss: 90.47642517089844 = 0.1697712540626526 + 10.0 * 9.030665397644043
Epoch 1420, val loss: 0.480240136384964
Epoch 1430, training loss: 90.47380065917969 = 0.16869008541107178 + 10.0 * 9.030510902404785
Epoch 1430, val loss: 0.48256155848503113
Epoch 1440, training loss: 90.47747039794922 = 0.16761314868927002 + 10.0 * 9.030985832214355
Epoch 1440, val loss: 0.48475074768066406
Epoch 1450, training loss: 90.46200561523438 = 0.1665303260087967 + 10.0 * 9.029547691345215
Epoch 1450, val loss: 0.4869738817214966
Epoch 1460, training loss: 90.46629333496094 = 0.16545210778713226 + 10.0 * 9.030084609985352
Epoch 1460, val loss: 0.48920223116874695
Epoch 1470, training loss: 90.4709243774414 = 0.16438047587871552 + 10.0 * 9.030653953552246
Epoch 1470, val loss: 0.4917028844356537
Epoch 1480, training loss: 90.46110534667969 = 0.16331888735294342 + 10.0 * 9.029778480529785
Epoch 1480, val loss: 0.4938702881336212
Epoch 1490, training loss: 90.44989776611328 = 0.16225656867027283 + 10.0 * 9.028764724731445
Epoch 1490, val loss: 0.49643775820732117
Epoch 1500, training loss: 90.44792175292969 = 0.16119414567947388 + 10.0 * 9.02867317199707
Epoch 1500, val loss: 0.49887987971305847
Epoch 1510, training loss: 90.44425964355469 = 0.1601305454969406 + 10.0 * 9.028412818908691
Epoch 1510, val loss: 0.501288115978241
Epoch 1520, training loss: 90.46546936035156 = 0.15907400846481323 + 10.0 * 9.0306396484375
Epoch 1520, val loss: 0.5035827159881592
Epoch 1530, training loss: 90.45165252685547 = 0.15801556408405304 + 10.0 * 9.029363632202148
Epoch 1530, val loss: 0.506544291973114
Epoch 1540, training loss: 90.4548568725586 = 0.15696735680103302 + 10.0 * 9.029788970947266
Epoch 1540, val loss: 0.5088410973548889
Epoch 1550, training loss: 90.43736267089844 = 0.1559068262577057 + 10.0 * 9.028145790100098
Epoch 1550, val loss: 0.5111892223358154
Epoch 1560, training loss: 90.44608306884766 = 0.15485860407352448 + 10.0 * 9.029122352600098
Epoch 1560, val loss: 0.513802170753479
Epoch 1570, training loss: 90.4353256225586 = 0.15381138026714325 + 10.0 * 9.028151512145996
Epoch 1570, val loss: 0.5164885520935059
Epoch 1580, training loss: 90.4350357055664 = 0.15276579558849335 + 10.0 * 9.028226852416992
Epoch 1580, val loss: 0.5190141201019287
Epoch 1590, training loss: 90.43084716796875 = 0.15172335505485535 + 10.0 * 9.027912139892578
Epoch 1590, val loss: 0.5216256380081177
Epoch 1600, training loss: 90.42304992675781 = 0.15067730844020844 + 10.0 * 9.027236938476562
Epoch 1600, val loss: 0.5243498086929321
Epoch 1610, training loss: 90.4356460571289 = 0.14963842928409576 + 10.0 * 9.028600692749023
Epoch 1610, val loss: 0.5271009802818298
Epoch 1620, training loss: 90.42001342773438 = 0.1485944390296936 + 10.0 * 9.027141571044922
Epoch 1620, val loss: 0.5296592712402344
Epoch 1630, training loss: 90.41661071777344 = 0.14755667746067047 + 10.0 * 9.026905059814453
Epoch 1630, val loss: 0.5324335694313049
Epoch 1640, training loss: 90.41446685791016 = 0.14651422202587128 + 10.0 * 9.026795387268066
Epoch 1640, val loss: 0.5350974798202515
Epoch 1650, training loss: 90.42546081542969 = 0.14547280967235565 + 10.0 * 9.027998924255371
Epoch 1650, val loss: 0.537929117679596
Epoch 1660, training loss: 90.42193603515625 = 0.14442798495292664 + 10.0 * 9.027750968933105
Epoch 1660, val loss: 0.5405597686767578
Epoch 1670, training loss: 90.41260528564453 = 0.14339441061019897 + 10.0 * 9.026921272277832
Epoch 1670, val loss: 0.5434865355491638
Epoch 1680, training loss: 90.40552520751953 = 0.14235159754753113 + 10.0 * 9.026317596435547
Epoch 1680, val loss: 0.546133816242218
Epoch 1690, training loss: 90.40264892578125 = 0.1413108855485916 + 10.0 * 9.02613353729248
Epoch 1690, val loss: 0.5489204525947571
Epoch 1700, training loss: 90.42835235595703 = 0.14028000831604004 + 10.0 * 9.028806686401367
Epoch 1700, val loss: 0.5517141819000244
Epoch 1710, training loss: 90.4117660522461 = 0.13924640417099 + 10.0 * 9.027252197265625
Epoch 1710, val loss: 0.5548705458641052
Epoch 1720, training loss: 90.40027618408203 = 0.1382066160440445 + 10.0 * 9.026206970214844
Epoch 1720, val loss: 0.5575646758079529
Epoch 1730, training loss: 90.39806365966797 = 0.13717219233512878 + 10.0 * 9.026089668273926
Epoch 1730, val loss: 0.5606339573860168
Epoch 1740, training loss: 90.41179656982422 = 0.13614405691623688 + 10.0 * 9.027565002441406
Epoch 1740, val loss: 0.5635941028594971
Epoch 1750, training loss: 90.40015411376953 = 0.13511504232883453 + 10.0 * 9.026503562927246
Epoch 1750, val loss: 0.5664541721343994
Epoch 1760, training loss: 90.388671875 = 0.13408198952674866 + 10.0 * 9.025459289550781
Epoch 1760, val loss: 0.5695310235023499
Epoch 1770, training loss: 90.38347625732422 = 0.13305042684078217 + 10.0 * 9.025042533874512
Epoch 1770, val loss: 0.5725719928741455
Epoch 1780, training loss: 90.38776397705078 = 0.13201986253261566 + 10.0 * 9.025574684143066
Epoch 1780, val loss: 0.5757523775100708
Epoch 1790, training loss: 90.39246368408203 = 0.1309918910264969 + 10.0 * 9.026147842407227
Epoch 1790, val loss: 0.5787503123283386
Epoch 1800, training loss: 90.38346862792969 = 0.12996123731136322 + 10.0 * 9.025350570678711
Epoch 1800, val loss: 0.58167964220047
Epoch 1810, training loss: 90.38908386230469 = 0.12894052267074585 + 10.0 * 9.02601432800293
Epoch 1810, val loss: 0.5846266150474548
Epoch 1820, training loss: 90.38055419921875 = 0.127907395362854 + 10.0 * 9.025264739990234
Epoch 1820, val loss: 0.5881400108337402
Epoch 1830, training loss: 90.37052154541016 = 0.12688317894935608 + 10.0 * 9.02436351776123
Epoch 1830, val loss: 0.5911481380462646
Epoch 1840, training loss: 90.37096405029297 = 0.12586039304733276 + 10.0 * 9.024510383605957
Epoch 1840, val loss: 0.5942019820213318
Epoch 1850, training loss: 90.39926147460938 = 0.12484307587146759 + 10.0 * 9.02744197845459
Epoch 1850, val loss: 0.5973169207572937
Epoch 1860, training loss: 90.37593841552734 = 0.12382671236991882 + 10.0 * 9.025211334228516
Epoch 1860, val loss: 0.6007440686225891
Epoch 1870, training loss: 90.36483764648438 = 0.12279945611953735 + 10.0 * 9.024203300476074
Epoch 1870, val loss: 0.6038696765899658
Epoch 1880, training loss: 90.36994934082031 = 0.12177985906600952 + 10.0 * 9.024816513061523
Epoch 1880, val loss: 0.6073409914970398
Epoch 1890, training loss: 90.36153411865234 = 0.12076263129711151 + 10.0 * 9.024076461791992
Epoch 1890, val loss: 0.6106976866722107
Epoch 1900, training loss: 90.35924530029297 = 0.11974509060382843 + 10.0 * 9.023950576782227
Epoch 1900, val loss: 0.6138582229614258
Epoch 1910, training loss: 90.36080932617188 = 0.11872968077659607 + 10.0 * 9.024208068847656
Epoch 1910, val loss: 0.6170395612716675
Epoch 1920, training loss: 90.36209106445312 = 0.11771347373723984 + 10.0 * 9.02443790435791
Epoch 1920, val loss: 0.6202388405799866
Epoch 1930, training loss: 90.36520385742188 = 0.11670402437448502 + 10.0 * 9.024849891662598
Epoch 1930, val loss: 0.624214768409729
Epoch 1940, training loss: 90.350830078125 = 0.11568424850702286 + 10.0 * 9.023514747619629
Epoch 1940, val loss: 0.627221941947937
Epoch 1950, training loss: 90.347412109375 = 0.11466771364212036 + 10.0 * 9.023274421691895
Epoch 1950, val loss: 0.6304981112480164
Epoch 1960, training loss: 90.35513305664062 = 0.11365640163421631 + 10.0 * 9.024147987365723
Epoch 1960, val loss: 0.6340796947479248
Epoch 1970, training loss: 90.3419189453125 = 0.11264117807149887 + 10.0 * 9.022928237915039
Epoch 1970, val loss: 0.6375126838684082
Epoch 1980, training loss: 90.34465026855469 = 0.11163073033094406 + 10.0 * 9.02330207824707
Epoch 1980, val loss: 0.640813410282135
Epoch 1990, training loss: 90.36116790771484 = 0.11062010377645493 + 10.0 * 9.025054931640625
Epoch 1990, val loss: 0.6442760825157166
Epoch 2000, training loss: 90.34264373779297 = 0.10960138589143753 + 10.0 * 9.023303985595703
Epoch 2000, val loss: 0.6479685306549072
Epoch 2010, training loss: 90.33792877197266 = 0.1085924431681633 + 10.0 * 9.022933959960938
Epoch 2010, val loss: 0.6515576243400574
Epoch 2020, training loss: 90.3395767211914 = 0.10758022964000702 + 10.0 * 9.023199081420898
Epoch 2020, val loss: 0.6552246809005737
Epoch 2030, training loss: 90.33822631835938 = 0.10656807571649551 + 10.0 * 9.023165702819824
Epoch 2030, val loss: 0.6584764122962952
Epoch 2040, training loss: 90.33131408691406 = 0.105560801923275 + 10.0 * 9.022575378417969
Epoch 2040, val loss: 0.6619824767112732
Epoch 2050, training loss: 90.3271713256836 = 0.10455382615327835 + 10.0 * 9.022261619567871
Epoch 2050, val loss: 0.6659047603607178
Epoch 2060, training loss: 90.34149932861328 = 0.10355161875486374 + 10.0 * 9.023794174194336
Epoch 2060, val loss: 0.6693089008331299
Epoch 2070, training loss: 90.33602142333984 = 0.10254616290330887 + 10.0 * 9.023347854614258
Epoch 2070, val loss: 0.6730132102966309
Epoch 2080, training loss: 90.32250213623047 = 0.10154681652784348 + 10.0 * 9.022095680236816
Epoch 2080, val loss: 0.6770843267440796
Epoch 2090, training loss: 90.31689453125 = 0.10054270178079605 + 10.0 * 9.021635055541992
Epoch 2090, val loss: 0.6805257797241211
Epoch 2100, training loss: 90.31462860107422 = 0.09954583644866943 + 10.0 * 9.02150821685791
Epoch 2100, val loss: 0.6845157146453857
Epoch 2110, training loss: 90.3276596069336 = 0.0985550805926323 + 10.0 * 9.022910118103027
Epoch 2110, val loss: 0.6886563301086426
Epoch 2120, training loss: 90.31271362304688 = 0.09755945950746536 + 10.0 * 9.021515846252441
Epoch 2120, val loss: 0.6918548941612244
Epoch 2130, training loss: 90.32018280029297 = 0.09657323360443115 + 10.0 * 9.022360801696777
Epoch 2130, val loss: 0.6960374712944031
Epoch 2140, training loss: 90.30804443359375 = 0.09558366239070892 + 10.0 * 9.021245956420898
Epoch 2140, val loss: 0.6994665861129761
Epoch 2150, training loss: 90.30960083007812 = 0.09459628164768219 + 10.0 * 9.021500587463379
Epoch 2150, val loss: 0.7036681771278381
Epoch 2160, training loss: 90.31707763671875 = 0.09362083673477173 + 10.0 * 9.022345542907715
Epoch 2160, val loss: 0.7078549861907959
Epoch 2170, training loss: 90.30924224853516 = 0.09264153987169266 + 10.0 * 9.021659851074219
Epoch 2170, val loss: 0.7119318246841431
Epoch 2180, training loss: 90.303466796875 = 0.09166605770587921 + 10.0 * 9.021180152893066
Epoch 2180, val loss: 0.7154571413993835
Epoch 2190, training loss: 90.30424499511719 = 0.0906979888677597 + 10.0 * 9.021354675292969
Epoch 2190, val loss: 0.7198359370231628
Epoch 2200, training loss: 90.29661560058594 = 0.08972370624542236 + 10.0 * 9.020689010620117
Epoch 2200, val loss: 0.723897397518158
Epoch 2210, training loss: 90.30574798583984 = 0.088761106133461 + 10.0 * 9.021698951721191
Epoch 2210, val loss: 0.7280963659286499
Epoch 2220, training loss: 90.30403900146484 = 0.08780486136674881 + 10.0 * 9.021623611450195
Epoch 2220, val loss: 0.7317061424255371
Epoch 2230, training loss: 90.29344940185547 = 0.08684969693422318 + 10.0 * 9.020659446716309
Epoch 2230, val loss: 0.7361103892326355
Epoch 2240, training loss: 90.28761291503906 = 0.08589985966682434 + 10.0 * 9.020171165466309
Epoch 2240, val loss: 0.7403045892715454
Epoch 2250, training loss: 90.28666687011719 = 0.08495443314313889 + 10.0 * 9.020171165466309
Epoch 2250, val loss: 0.7445202469825745
Epoch 2260, training loss: 90.2969741821289 = 0.08401887118816376 + 10.0 * 9.021295547485352
Epoch 2260, val loss: 0.7488623857498169
Epoch 2270, training loss: 90.28535461425781 = 0.08308185636997223 + 10.0 * 9.020227432250977
Epoch 2270, val loss: 0.7530837059020996
Epoch 2280, training loss: 90.28936004638672 = 0.08215614408254623 + 10.0 * 9.020720481872559
Epoch 2280, val loss: 0.7575772404670715
Epoch 2290, training loss: 90.28077697753906 = 0.08123432844877243 + 10.0 * 9.019953727722168
Epoch 2290, val loss: 0.7619416117668152
Epoch 2300, training loss: 90.27883911132812 = 0.08031383901834488 + 10.0 * 9.019852638244629
Epoch 2300, val loss: 0.7665038108825684
Epoch 2310, training loss: 90.2919921875 = 0.07940535992383957 + 10.0 * 9.021258354187012
Epoch 2310, val loss: 0.7707833051681519
Epoch 2320, training loss: 90.27642822265625 = 0.07849151641130447 + 10.0 * 9.019793510437012
Epoch 2320, val loss: 0.7747288346290588
Epoch 2330, training loss: 90.27664947509766 = 0.07758990675210953 + 10.0 * 9.019906044006348
Epoch 2330, val loss: 0.7793264985084534
Epoch 2340, training loss: 90.28559875488281 = 0.07669652998447418 + 10.0 * 9.020890235900879
Epoch 2340, val loss: 0.7837973237037659
Epoch 2350, training loss: 90.2844009399414 = 0.07581199705600739 + 10.0 * 9.020858764648438
Epoch 2350, val loss: 0.7890809178352356
Epoch 2360, training loss: 90.26580810546875 = 0.07491188496351242 + 10.0 * 9.019089698791504
Epoch 2360, val loss: 0.7931780219078064
Epoch 2370, training loss: 90.26368713378906 = 0.07402902096509933 + 10.0 * 9.018965721130371
Epoch 2370, val loss: 0.7977045178413391
Epoch 2380, training loss: 90.27433013916016 = 0.07316136360168457 + 10.0 * 9.020116806030273
Epoch 2380, val loss: 0.8020426630973816
Epoch 2390, training loss: 90.25916290283203 = 0.07228416949510574 + 10.0 * 9.018688201904297
Epoch 2390, val loss: 0.8072348237037659
Epoch 2400, training loss: 90.25643920898438 = 0.07141955941915512 + 10.0 * 9.018502235412598
Epoch 2400, val loss: 0.8119305372238159
Epoch 2410, training loss: 90.26970672607422 = 0.07056532055139542 + 10.0 * 9.019914627075195
Epoch 2410, val loss: 0.8168087601661682
Epoch 2420, training loss: 90.25350952148438 = 0.06970866024494171 + 10.0 * 9.018380165100098
Epoch 2420, val loss: 0.8214514851570129
Epoch 2430, training loss: 90.25222778320312 = 0.06886126846075058 + 10.0 * 9.018336296081543
Epoch 2430, val loss: 0.8264157176017761
Epoch 2440, training loss: 90.25680541992188 = 0.06802747398614883 + 10.0 * 9.018877983093262
Epoch 2440, val loss: 0.8312154412269592
Epoch 2450, training loss: 90.26126861572266 = 0.0671917051076889 + 10.0 * 9.019407272338867
Epoch 2450, val loss: 0.8356873393058777
Epoch 2460, training loss: 90.25779724121094 = 0.06637651473283768 + 10.0 * 9.019142150878906
Epoch 2460, val loss: 0.8401974439620972
Epoch 2470, training loss: 90.24603271484375 = 0.06555330008268356 + 10.0 * 9.018048286437988
Epoch 2470, val loss: 0.8459936380386353
Epoch 2480, training loss: 90.24525451660156 = 0.06474410742521286 + 10.0 * 9.018051147460938
Epoch 2480, val loss: 0.8507476449012756
Epoch 2490, training loss: 90.25872039794922 = 0.06394926458597183 + 10.0 * 9.019476890563965
Epoch 2490, val loss: 0.8557299375534058
Epoch 2500, training loss: 90.24620819091797 = 0.06315083801746368 + 10.0 * 9.018305778503418
Epoch 2500, val loss: 0.8601506352424622
Epoch 2510, training loss: 90.24791717529297 = 0.06236777827143669 + 10.0 * 9.0185546875
Epoch 2510, val loss: 0.8657488822937012
Epoch 2520, training loss: 90.24507904052734 = 0.06158718094229698 + 10.0 * 9.018349647521973
Epoch 2520, val loss: 0.8704729080200195
Epoch 2530, training loss: 90.25072479248047 = 0.06081888824701309 + 10.0 * 9.018990516662598
Epoch 2530, val loss: 0.8755868673324585
Epoch 2540, training loss: 90.23690032958984 = 0.06005243957042694 + 10.0 * 9.017684936523438
Epoch 2540, val loss: 0.8804961442947388
Epoch 2550, training loss: 90.23838806152344 = 0.05929910019040108 + 10.0 * 9.017909049987793
Epoch 2550, val loss: 0.8854007124900818
Epoch 2560, training loss: 90.24376678466797 = 0.05855729430913925 + 10.0 * 9.018521308898926
Epoch 2560, val loss: 0.8903444409370422
Epoch 2570, training loss: 90.23503875732422 = 0.0578141063451767 + 10.0 * 9.017722129821777
Epoch 2570, val loss: 0.8950998187065125
Epoch 2580, training loss: 90.22862243652344 = 0.05708310380578041 + 10.0 * 9.0171537399292
Epoch 2580, val loss: 0.900344729423523
Epoch 2590, training loss: 90.23571014404297 = 0.056361183524131775 + 10.0 * 9.017934799194336
Epoch 2590, val loss: 0.9050480127334595
Epoch 2600, training loss: 90.23078155517578 = 0.05564527213573456 + 10.0 * 9.017513275146484
Epoch 2600, val loss: 0.9109556674957275
Epoch 2610, training loss: 90.22835540771484 = 0.05494317784905434 + 10.0 * 9.017340660095215
Epoch 2610, val loss: 0.9160023927688599
Epoch 2620, training loss: 90.22544860839844 = 0.054239869117736816 + 10.0 * 9.017121315002441
Epoch 2620, val loss: 0.9209589958190918
Epoch 2630, training loss: 90.2302474975586 = 0.05355207249522209 + 10.0 * 9.017669677734375
Epoch 2630, val loss: 0.9256243109703064
Epoch 2640, training loss: 90.22683715820312 = 0.05287335067987442 + 10.0 * 9.017396926879883
Epoch 2640, val loss: 0.9302287101745605
Epoch 2650, training loss: 90.21916198730469 = 0.05218973755836487 + 10.0 * 9.01669692993164
Epoch 2650, val loss: 0.9361320734024048
Epoch 2660, training loss: 90.21573638916016 = 0.051520589739084244 + 10.0 * 9.0164213180542
Epoch 2660, val loss: 0.94117271900177
Epoch 2670, training loss: 90.22454071044922 = 0.05086180195212364 + 10.0 * 9.017367362976074
Epoch 2670, val loss: 0.9463298320770264
Epoch 2680, training loss: 90.21995544433594 = 0.05020580440759659 + 10.0 * 9.016974449157715
Epoch 2680, val loss: 0.951759397983551
Epoch 2690, training loss: 90.22093200683594 = 0.049563754349946976 + 10.0 * 9.01713752746582
Epoch 2690, val loss: 0.9568790793418884
Epoch 2700, training loss: 90.2087173461914 = 0.048923175781965256 + 10.0 * 9.015979766845703
Epoch 2700, val loss: 0.9617211222648621
Epoch 2710, training loss: 90.20633697509766 = 0.048293959349393845 + 10.0 * 9.015804290771484
Epoch 2710, val loss: 0.9666698575019836
Epoch 2720, training loss: 90.21773529052734 = 0.04767705872654915 + 10.0 * 9.017005920410156
Epoch 2720, val loss: 0.9718464016914368
Epoch 2730, training loss: 90.20783233642578 = 0.04706436023116112 + 10.0 * 9.016077041625977
Epoch 2730, val loss: 0.9771811962127686
Epoch 2740, training loss: 90.20447540283203 = 0.04646242782473564 + 10.0 * 9.015801429748535
Epoch 2740, val loss: 0.9825688004493713
Epoch 2750, training loss: 90.21662139892578 = 0.04587140306830406 + 10.0 * 9.017075538635254
Epoch 2750, val loss: 0.9874060153961182
Epoch 2760, training loss: 90.19947052001953 = 0.045275039970874786 + 10.0 * 9.015419960021973
Epoch 2760, val loss: 0.9925984740257263
Epoch 2770, training loss: 90.19975280761719 = 0.044694192707538605 + 10.0 * 9.01550579071045
Epoch 2770, val loss: 0.9973748326301575
Epoch 2780, training loss: 90.22444915771484 = 0.044137731194496155 + 10.0 * 9.018031120300293
Epoch 2780, val loss: 1.003501057624817
Epoch 2790, training loss: 90.20536041259766 = 0.04355957359075546 + 10.0 * 9.016180038452148
Epoch 2790, val loss: 1.0066876411437988
Epoch 2800, training loss: 90.19872283935547 = 0.0430045947432518 + 10.0 * 9.015571594238281
Epoch 2800, val loss: 1.012612223625183
Epoch 2810, training loss: 90.19635009765625 = 0.0424557588994503 + 10.0 * 9.015389442443848
Epoch 2810, val loss: 1.017522931098938
Epoch 2820, training loss: 90.2052001953125 = 0.04192633181810379 + 10.0 * 9.016327857971191
Epoch 2820, val loss: 1.0230624675750732
Epoch 2830, training loss: 90.19044494628906 = 0.04138137400150299 + 10.0 * 9.01490592956543
Epoch 2830, val loss: 1.0266603231430054
Epoch 2840, training loss: 90.19121551513672 = 0.040854718536138535 + 10.0 * 9.015035629272461
Epoch 2840, val loss: 1.0318939685821533
Epoch 2850, training loss: 90.20883178710938 = 0.04034152254462242 + 10.0 * 9.01684856414795
Epoch 2850, val loss: 1.0368157625198364
Epoch 2860, training loss: 90.20105743408203 = 0.03983459249138832 + 10.0 * 9.016122817993164
Epoch 2860, val loss: 1.0416258573532104
Epoch 2870, training loss: 90.19404602050781 = 0.03932430222630501 + 10.0 * 9.015472412109375
Epoch 2870, val loss: 1.046492576599121
Epoch 2880, training loss: 90.20637512207031 = 0.038833100348711014 + 10.0 * 9.016754150390625
Epoch 2880, val loss: 1.0512789487838745
Epoch 2890, training loss: 90.18567657470703 = 0.03834538161754608 + 10.0 * 9.01473331451416
Epoch 2890, val loss: 1.0567134618759155
Epoch 2900, training loss: 90.18267822265625 = 0.03785812109708786 + 10.0 * 9.014482498168945
Epoch 2900, val loss: 1.0612239837646484
Epoch 2910, training loss: 90.18936920166016 = 0.03739120066165924 + 10.0 * 9.01519775390625
Epoch 2910, val loss: 1.0666725635528564
Epoch 2920, training loss: 90.18122863769531 = 0.036917708814144135 + 10.0 * 9.01443099975586
Epoch 2920, val loss: 1.070823073387146
Epoch 2930, training loss: 90.1807861328125 = 0.03645625337958336 + 10.0 * 9.014432907104492
Epoch 2930, val loss: 1.0752556324005127
Epoch 2940, training loss: 90.19287109375 = 0.036009080708026886 + 10.0 * 9.01568603515625
Epoch 2940, val loss: 1.080196499824524
Epoch 2950, training loss: 90.18498229980469 = 0.035555172711610794 + 10.0 * 9.01494312286377
Epoch 2950, val loss: 1.0847164392471313
Epoch 2960, training loss: 90.17749786376953 = 0.03511594980955124 + 10.0 * 9.014238357543945
Epoch 2960, val loss: 1.0896769762039185
Epoch 2970, training loss: 90.17630767822266 = 0.034678056836128235 + 10.0 * 9.01416301727295
Epoch 2970, val loss: 1.0942634344100952
Epoch 2980, training loss: 90.1778335571289 = 0.03425385430455208 + 10.0 * 9.014357566833496
Epoch 2980, val loss: 1.098720669746399
Epoch 2990, training loss: 90.17527770996094 = 0.033828653395175934 + 10.0 * 9.014144897460938
Epoch 2990, val loss: 1.1035244464874268
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8503
Overall ASR: 0.6897
Flip ASR: 0.6120/1554 nodes
./selected_nodes/Pubmed/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 104.68083190917969 = 1.0920791625976562 + 10.0 * 10.358875274658203
Epoch 0, val loss: 1.0921192169189453
Epoch 10, training loss: 104.61079406738281 = 1.0810120105743408 + 10.0 * 10.352978706359863
Epoch 10, val loss: 1.080666184425354
Epoch 20, training loss: 103.7635498046875 = 1.0671000480651855 + 10.0 * 10.269644737243652
Epoch 20, val loss: 1.0669596195220947
Epoch 30, training loss: 97.93695831298828 = 1.0567890405654907 + 10.0 * 9.688016891479492
Epoch 30, val loss: 1.0566290616989136
Epoch 40, training loss: 95.53693389892578 = 1.044102430343628 + 10.0 * 9.4492826461792
Epoch 40, val loss: 1.0433944463729858
Epoch 50, training loss: 94.76917266845703 = 1.0266388654708862 + 10.0 * 9.374253273010254
Epoch 50, val loss: 1.0258982181549072
Epoch 60, training loss: 94.2812271118164 = 1.0104596614837646 + 10.0 * 9.32707691192627
Epoch 60, val loss: 1.0103211402893066
Epoch 70, training loss: 93.74907684326172 = 0.9994906783103943 + 10.0 * 9.274958610534668
Epoch 70, val loss: 0.9998677372932434
Epoch 80, training loss: 93.37374114990234 = 0.989298939704895 + 10.0 * 9.238444328308105
Epoch 80, val loss: 0.9896625280380249
Epoch 90, training loss: 93.05464935302734 = 0.9760351777076721 + 10.0 * 9.207860946655273
Epoch 90, val loss: 0.9768536686897278
Epoch 100, training loss: 92.8645248413086 = 0.9622558355331421 + 10.0 * 9.190226554870605
Epoch 100, val loss: 0.9638141393661499
Epoch 110, training loss: 92.62300109863281 = 0.9489719271659851 + 10.0 * 9.167402267456055
Epoch 110, val loss: 0.9513875246047974
Epoch 120, training loss: 92.41629791259766 = 0.9360330104827881 + 10.0 * 9.148026466369629
Epoch 120, val loss: 0.9390127062797546
Epoch 130, training loss: 92.2508544921875 = 0.9203923344612122 + 10.0 * 9.13304615020752
Epoch 130, val loss: 0.9239545464515686
Epoch 140, training loss: 92.13121032714844 = 0.9009036421775818 + 10.0 * 9.123030662536621
Epoch 140, val loss: 0.9051234722137451
Epoch 150, training loss: 92.03421783447266 = 0.8784722685813904 + 10.0 * 9.115574836730957
Epoch 150, val loss: 0.8836333155632019
Epoch 160, training loss: 91.93328094482422 = 0.8539112210273743 + 10.0 * 9.10793685913086
Epoch 160, val loss: 0.8599774837493896
Epoch 170, training loss: 91.85334014892578 = 0.8261610865592957 + 10.0 * 9.102717399597168
Epoch 170, val loss: 0.8334170579910278
Epoch 180, training loss: 91.78297424316406 = 0.7947219014167786 + 10.0 * 9.098825454711914
Epoch 180, val loss: 0.8033103942871094
Epoch 190, training loss: 91.72069549560547 = 0.7600123882293701 + 10.0 * 9.096068382263184
Epoch 190, val loss: 0.7702023386955261
Epoch 200, training loss: 91.64593505859375 = 0.7230184674263 + 10.0 * 9.092291831970215
Epoch 200, val loss: 0.7351967692375183
Epoch 210, training loss: 91.57746124267578 = 0.6845314502716064 + 10.0 * 9.089292526245117
Epoch 210, val loss: 0.6990358829498291
Epoch 220, training loss: 91.51651000976562 = 0.645738959312439 + 10.0 * 9.087077140808105
Epoch 220, val loss: 0.6628944873809814
Epoch 230, training loss: 91.44580841064453 = 0.6080153584480286 + 10.0 * 9.083779335021973
Epoch 230, val loss: 0.628025233745575
Epoch 240, training loss: 91.4030990600586 = 0.5721435546875 + 10.0 * 9.08309555053711
Epoch 240, val loss: 0.5952740907669067
Epoch 250, training loss: 91.34127044677734 = 0.5392692685127258 + 10.0 * 9.0802001953125
Epoch 250, val loss: 0.5656630396842957
Epoch 260, training loss: 91.27919006347656 = 0.5100237131118774 + 10.0 * 9.076916694641113
Epoch 260, val loss: 0.5396439433097839
Epoch 270, training loss: 91.23546600341797 = 0.48428720235824585 + 10.0 * 9.075118064880371
Epoch 270, val loss: 0.5171244740486145
Epoch 280, training loss: 91.2173843383789 = 0.46186187863349915 + 10.0 * 9.075551986694336
Epoch 280, val loss: 0.49797579646110535
Epoch 290, training loss: 91.16606903076172 = 0.4427436888217926 + 10.0 * 9.072332382202148
Epoch 290, val loss: 0.4819025695323944
Epoch 300, training loss: 91.1295166015625 = 0.42645978927612305 + 10.0 * 9.070305824279785
Epoch 300, val loss: 0.4685423672199249
Epoch 310, training loss: 91.10346221923828 = 0.41246622800827026 + 10.0 * 9.069099426269531
Epoch 310, val loss: 0.45742267370224
Epoch 320, training loss: 91.08606719970703 = 0.40034595131874084 + 10.0 * 9.068572044372559
Epoch 320, val loss: 0.4481251537799835
Epoch 330, training loss: 91.07583618164062 = 0.38987958431243896 + 10.0 * 9.068595886230469
Epoch 330, val loss: 0.4404175281524658
Epoch 340, training loss: 91.03253936767578 = 0.3808840811252594 + 10.0 * 9.065165519714355
Epoch 340, val loss: 0.43399885296821594
Epoch 350, training loss: 91.00852966308594 = 0.37297356128692627 + 10.0 * 9.063555717468262
Epoch 350, val loss: 0.42853519320487976
Epoch 360, training loss: 90.99070739746094 = 0.3658439815044403 + 10.0 * 9.06248664855957
Epoch 360, val loss: 0.42385759949684143
Epoch 370, training loss: 90.97570037841797 = 0.3593701720237732 + 10.0 * 9.061633110046387
Epoch 370, val loss: 0.4196859896183014
Epoch 380, training loss: 90.9637222290039 = 0.3535643517971039 + 10.0 * 9.061016082763672
Epoch 380, val loss: 0.41614529490470886
Epoch 390, training loss: 90.94955444335938 = 0.3482809066772461 + 10.0 * 9.060127258300781
Epoch 390, val loss: 0.413091778755188
Epoch 400, training loss: 90.95366668701172 = 0.3433752655982971 + 10.0 * 9.061029434204102
Epoch 400, val loss: 0.41039490699768066
Epoch 410, training loss: 90.94096374511719 = 0.3388233482837677 + 10.0 * 9.060214042663574
Epoch 410, val loss: 0.4079645574092865
Epoch 420, training loss: 90.90721130371094 = 0.33461228013038635 + 10.0 * 9.057260513305664
Epoch 420, val loss: 0.4057863652706146
Epoch 430, training loss: 90.89666748046875 = 0.3306427299976349 + 10.0 * 9.056602478027344
Epoch 430, val loss: 0.40385958552360535
Epoch 440, training loss: 90.91814422607422 = 0.3268633484840393 + 10.0 * 9.059127807617188
Epoch 440, val loss: 0.4020531177520752
Epoch 450, training loss: 90.88227844238281 = 0.32330888509750366 + 10.0 * 9.055896759033203
Epoch 450, val loss: 0.40046000480651855
Epoch 460, training loss: 90.86631774902344 = 0.31996849179267883 + 10.0 * 9.054635047912598
Epoch 460, val loss: 0.3990287780761719
Epoch 470, training loss: 90.85820007324219 = 0.3167726993560791 + 10.0 * 9.054142951965332
Epoch 470, val loss: 0.3977445363998413
Epoch 480, training loss: 90.8683853149414 = 0.31369084119796753 + 10.0 * 9.055469512939453
Epoch 480, val loss: 0.3965977430343628
Epoch 490, training loss: 90.87322998046875 = 0.31073251366615295 + 10.0 * 9.056249618530273
Epoch 490, val loss: 0.39544057846069336
Epoch 500, training loss: 90.86276245117188 = 0.30794039368629456 + 10.0 * 9.055482864379883
Epoch 500, val loss: 0.39442670345306396
Epoch 510, training loss: 90.83470153808594 = 0.3052954375743866 + 10.0 * 9.052940368652344
Epoch 510, val loss: 0.3934648334980011
Epoch 520, training loss: 90.82006072998047 = 0.3027300536632538 + 10.0 * 9.051733016967773
Epoch 520, val loss: 0.3927086591720581
Epoch 530, training loss: 90.80985260009766 = 0.30021414160728455 + 10.0 * 9.050963401794434
Epoch 530, val loss: 0.3919399380683899
Epoch 540, training loss: 90.80154418945312 = 0.2977469265460968 + 10.0 * 9.050379753112793
Epoch 540, val loss: 0.3912617266178131
Epoch 550, training loss: 90.80198669433594 = 0.29533547163009644 + 10.0 * 9.050664901733398
Epoch 550, val loss: 0.3906424641609192
Epoch 560, training loss: 90.81012725830078 = 0.2929949164390564 + 10.0 * 9.051713943481445
Epoch 560, val loss: 0.39011549949645996
Epoch 570, training loss: 90.78640747070312 = 0.29075872898101807 + 10.0 * 9.049565315246582
Epoch 570, val loss: 0.38962969183921814
Epoch 580, training loss: 90.78732299804688 = 0.28858494758605957 + 10.0 * 9.049874305725098
Epoch 580, val loss: 0.38911184668540955
Epoch 590, training loss: 90.7708740234375 = 0.2864561378955841 + 10.0 * 9.048441886901855
Epoch 590, val loss: 0.3888455033302307
Epoch 600, training loss: 90.7665786743164 = 0.28437340259552 + 10.0 * 9.04822063446045
Epoch 600, val loss: 0.3884187340736389
Epoch 610, training loss: 90.77413940429688 = 0.2823329567909241 + 10.0 * 9.04918098449707
Epoch 610, val loss: 0.388187050819397
Epoch 620, training loss: 90.75701141357422 = 0.2803359925746918 + 10.0 * 9.047667503356934
Epoch 620, val loss: 0.38798826932907104
Epoch 630, training loss: 90.74788665771484 = 0.27837949991226196 + 10.0 * 9.046950340270996
Epoch 630, val loss: 0.3877955675125122
Epoch 640, training loss: 90.75296783447266 = 0.27644452452659607 + 10.0 * 9.047652244567871
Epoch 640, val loss: 0.3877002000808716
Epoch 650, training loss: 90.74301147460938 = 0.2745525538921356 + 10.0 * 9.046846389770508
Epoch 650, val loss: 0.38760438561439514
Epoch 660, training loss: 90.73565673828125 = 0.27270740270614624 + 10.0 * 9.046295166015625
Epoch 660, val loss: 0.3875683844089508
Epoch 670, training loss: 90.73246765136719 = 0.27088847756385803 + 10.0 * 9.046157836914062
Epoch 670, val loss: 0.38755759596824646
Epoch 680, training loss: 90.72865295410156 = 0.26909875869750977 + 10.0 * 9.045955657958984
Epoch 680, val loss: 0.38761764764785767
Epoch 690, training loss: 90.71963500976562 = 0.26734668016433716 + 10.0 * 9.045228958129883
Epoch 690, val loss: 0.3876190185546875
Epoch 700, training loss: 90.71502685546875 = 0.2656165361404419 + 10.0 * 9.044940948486328
Epoch 700, val loss: 0.38774728775024414
Epoch 710, training loss: 90.72437286376953 = 0.26390883326530457 + 10.0 * 9.046046257019043
Epoch 710, val loss: 0.3878777027130127
Epoch 720, training loss: 90.716064453125 = 0.26223328709602356 + 10.0 * 9.04538345336914
Epoch 720, val loss: 0.3881971836090088
Epoch 730, training loss: 90.70538330078125 = 0.26058322191238403 + 10.0 * 9.044480323791504
Epoch 730, val loss: 0.38840368390083313
Epoch 740, training loss: 90.6957778930664 = 0.2589523494243622 + 10.0 * 9.043683052062988
Epoch 740, val loss: 0.38863763213157654
Epoch 750, training loss: 90.6910400390625 = 0.2573372721672058 + 10.0 * 9.043370246887207
Epoch 750, val loss: 0.3888746500015259
Epoch 760, training loss: 90.69265747070312 = 0.25573959946632385 + 10.0 * 9.043691635131836
Epoch 760, val loss: 0.3892853260040283
Epoch 770, training loss: 90.69728088378906 = 0.25416508316993713 + 10.0 * 9.0443115234375
Epoch 770, val loss: 0.3898339867591858
Epoch 780, training loss: 90.68772888183594 = 0.2526082992553711 + 10.0 * 9.043512344360352
Epoch 780, val loss: 0.3901285231113434
Epoch 790, training loss: 90.6839599609375 = 0.2510763108730316 + 10.0 * 9.043288230895996
Epoch 790, val loss: 0.39060142636299133
Epoch 800, training loss: 90.67015838623047 = 0.24956580996513367 + 10.0 * 9.042058944702148
Epoch 800, val loss: 0.39101824164390564
Epoch 810, training loss: 90.66541290283203 = 0.24807047843933105 + 10.0 * 9.04173469543457
Epoch 810, val loss: 0.3916063904762268
Epoch 820, training loss: 90.66647338867188 = 0.24658545851707458 + 10.0 * 9.04198932647705
Epoch 820, val loss: 0.3921292722225189
Epoch 830, training loss: 90.65934753417969 = 0.24511705338954926 + 10.0 * 9.041422843933105
Epoch 830, val loss: 0.3927895128726959
Epoch 840, training loss: 90.65091705322266 = 0.2436719536781311 + 10.0 * 9.04072380065918
Epoch 840, val loss: 0.39339688420295715
Epoch 850, training loss: 90.64887237548828 = 0.24223558604717255 + 10.0 * 9.040663719177246
Epoch 850, val loss: 0.3940783441066742
Epoch 860, training loss: 90.69189453125 = 0.24081431329250336 + 10.0 * 9.0451078414917
Epoch 860, val loss: 0.3947579562664032
Epoch 870, training loss: 90.64379119873047 = 0.2394222617149353 + 10.0 * 9.040436744689941
Epoch 870, val loss: 0.3955099284648895
Epoch 880, training loss: 90.63984680175781 = 0.23804673552513123 + 10.0 * 9.040180206298828
Epoch 880, val loss: 0.3962983787059784
Epoch 890, training loss: 90.63236999511719 = 0.23667606711387634 + 10.0 * 9.039568901062012
Epoch 890, val loss: 0.39710697531700134
Epoch 900, training loss: 90.64920043945312 = 0.23531289398670197 + 10.0 * 9.041388511657715
Epoch 900, val loss: 0.3979465067386627
Epoch 910, training loss: 90.63976287841797 = 0.23396803438663483 + 10.0 * 9.040578842163086
Epoch 910, val loss: 0.3986060917377472
Epoch 920, training loss: 90.6281967163086 = 0.2326420545578003 + 10.0 * 9.039555549621582
Epoch 920, val loss: 0.3997129201889038
Epoch 930, training loss: 90.62010192871094 = 0.23132476210594177 + 10.0 * 9.038877487182617
Epoch 930, val loss: 0.4005662202835083
Epoch 940, training loss: 90.61651611328125 = 0.23001210391521454 + 10.0 * 9.038650512695312
Epoch 940, val loss: 0.40144872665405273
Epoch 950, training loss: 90.65644836425781 = 0.2287166565656662 + 10.0 * 9.042773246765137
Epoch 950, val loss: 0.4024121165275574
Epoch 960, training loss: 90.61575317382812 = 0.22743570804595947 + 10.0 * 9.03883171081543
Epoch 960, val loss: 0.40349435806274414
Epoch 970, training loss: 90.61190032958984 = 0.22617162764072418 + 10.0 * 9.038572311401367
Epoch 970, val loss: 0.40457919239997864
Epoch 980, training loss: 90.60318756103516 = 0.22491200268268585 + 10.0 * 9.037827491760254
Epoch 980, val loss: 0.4055633544921875
Epoch 990, training loss: 90.60607147216797 = 0.22365917265415192 + 10.0 * 9.038241386413574
Epoch 990, val loss: 0.4066351354122162
Epoch 1000, training loss: 90.60480499267578 = 0.22241687774658203 + 10.0 * 9.038238525390625
Epoch 1000, val loss: 0.4077857434749603
Epoch 1010, training loss: 90.60526275634766 = 0.2211844027042389 + 10.0 * 9.038408279418945
Epoch 1010, val loss: 0.40895918011665344
Epoch 1020, training loss: 90.5999984741211 = 0.21996212005615234 + 10.0 * 9.038003921508789
Epoch 1020, val loss: 0.410087913274765
Epoch 1030, training loss: 90.60735321044922 = 0.21875019371509552 + 10.0 * 9.038860321044922
Epoch 1030, val loss: 0.4114619493484497
Epoch 1040, training loss: 90.58860778808594 = 0.21754294633865356 + 10.0 * 9.03710651397705
Epoch 1040, val loss: 0.4124429523944855
Epoch 1050, training loss: 90.58110046386719 = 0.21634620428085327 + 10.0 * 9.03647518157959
Epoch 1050, val loss: 0.41369086503982544
Epoch 1060, training loss: 90.57701110839844 = 0.21515238285064697 + 10.0 * 9.036186218261719
Epoch 1060, val loss: 0.4149475693702698
Epoch 1070, training loss: 90.57986450195312 = 0.21396192908287048 + 10.0 * 9.036590576171875
Epoch 1070, val loss: 0.4162817597389221
Epoch 1080, training loss: 90.57976531982422 = 0.2127816528081894 + 10.0 * 9.036698341369629
Epoch 1080, val loss: 0.41753193736076355
Epoch 1090, training loss: 90.57957458496094 = 0.2116202861070633 + 10.0 * 9.036795616149902
Epoch 1090, val loss: 0.41890284419059753
Epoch 1100, training loss: 90.56614685058594 = 0.21045805513858795 + 10.0 * 9.035569190979004
Epoch 1100, val loss: 0.4201804995536804
Epoch 1110, training loss: 90.57262420654297 = 0.20930221676826477 + 10.0 * 9.036332130432129
Epoch 1110, val loss: 0.4215008318424225
Epoch 1120, training loss: 90.57183074951172 = 0.20815418660640717 + 10.0 * 9.036367416381836
Epoch 1120, val loss: 0.4227965474128723
Epoch 1130, training loss: 90.55915069580078 = 0.20701724290847778 + 10.0 * 9.035213470458984
Epoch 1130, val loss: 0.42419564723968506
Epoch 1140, training loss: 90.55402374267578 = 0.20588815212249756 + 10.0 * 9.03481388092041
Epoch 1140, val loss: 0.42553773522377014
Epoch 1150, training loss: 90.55540466308594 = 0.20476257801055908 + 10.0 * 9.035063743591309
Epoch 1150, val loss: 0.4269488751888275
Epoch 1160, training loss: 90.57083129882812 = 0.20364509522914886 + 10.0 * 9.036718368530273
Epoch 1160, val loss: 0.42825695872306824
Epoch 1170, training loss: 90.55622100830078 = 0.20252935588359833 + 10.0 * 9.035368919372559
Epoch 1170, val loss: 0.42994725704193115
Epoch 1180, training loss: 90.56432342529297 = 0.20142188668251038 + 10.0 * 9.036290168762207
Epoch 1180, val loss: 0.4313189685344696
Epoch 1190, training loss: 90.54602813720703 = 0.2003231942653656 + 10.0 * 9.034570693969727
Epoch 1190, val loss: 0.43283313512802124
Epoch 1200, training loss: 90.53831481933594 = 0.19922640919685364 + 10.0 * 9.03390884399414
Epoch 1200, val loss: 0.4342891573905945
Epoch 1210, training loss: 90.54061889648438 = 0.1981329470872879 + 10.0 * 9.034248352050781
Epoch 1210, val loss: 0.43578895926475525
Epoch 1220, training loss: 90.54639434814453 = 0.19704259932041168 + 10.0 * 9.034934997558594
Epoch 1220, val loss: 0.4373089373111725
Epoch 1230, training loss: 90.53872680664062 = 0.19595417380332947 + 10.0 * 9.034276962280273
Epoch 1230, val loss: 0.4388746917247772
Epoch 1240, training loss: 90.53704833984375 = 0.1948719620704651 + 10.0 * 9.034217834472656
Epoch 1240, val loss: 0.44046550989151
Epoch 1250, training loss: 90.5307846069336 = 0.19379101693630219 + 10.0 * 9.033699035644531
Epoch 1250, val loss: 0.4419868290424347
Epoch 1260, training loss: 90.52910614013672 = 0.19271506369113922 + 10.0 * 9.033638954162598
Epoch 1260, val loss: 0.4435691237449646
Epoch 1270, training loss: 90.52279663085938 = 0.19163940846920013 + 10.0 * 9.03311538696289
Epoch 1270, val loss: 0.4452134370803833
Epoch 1280, training loss: 90.550048828125 = 0.1905679553747177 + 10.0 * 9.035947799682617
Epoch 1280, val loss: 0.4468857944011688
Epoch 1290, training loss: 90.52348327636719 = 0.18950286507606506 + 10.0 * 9.033397674560547
Epoch 1290, val loss: 0.4482741057872772
Epoch 1300, training loss: 90.51112365722656 = 0.18843704462051392 + 10.0 * 9.032268524169922
Epoch 1300, val loss: 0.4499393403530121
Epoch 1310, training loss: 90.50733947753906 = 0.18737313151359558 + 10.0 * 9.031996726989746
Epoch 1310, val loss: 0.45158469676971436
Epoch 1320, training loss: 90.5047607421875 = 0.1863056719303131 + 10.0 * 9.031846046447754
Epoch 1320, val loss: 0.4532221853733063
Epoch 1330, training loss: 90.517333984375 = 0.18523772060871124 + 10.0 * 9.033209800720215
Epoch 1330, val loss: 0.4548822343349457
Epoch 1340, training loss: 90.51392364501953 = 0.18417054414749146 + 10.0 * 9.032975196838379
Epoch 1340, val loss: 0.45666632056236267
Epoch 1350, training loss: 90.50279998779297 = 0.18311303853988647 + 10.0 * 9.03196907043457
Epoch 1350, val loss: 0.45839130878448486
Epoch 1360, training loss: 90.4990234375 = 0.18205462396144867 + 10.0 * 9.031697273254395
Epoch 1360, val loss: 0.45998871326446533
Epoch 1370, training loss: 90.49365234375 = 0.1809953898191452 + 10.0 * 9.031266212463379
Epoch 1370, val loss: 0.46170684695243835
Epoch 1380, training loss: 90.50870513916016 = 0.17994143068790436 + 10.0 * 9.032876968383789
Epoch 1380, val loss: 0.46334725618362427
Epoch 1390, training loss: 90.49681091308594 = 0.17887917160987854 + 10.0 * 9.031793594360352
Epoch 1390, val loss: 0.46550828218460083
Epoch 1400, training loss: 90.49662780761719 = 0.1778249591588974 + 10.0 * 9.031880378723145
Epoch 1400, val loss: 0.4669777750968933
Epoch 1410, training loss: 90.49016571044922 = 0.17676550149917603 + 10.0 * 9.031339645385742
Epoch 1410, val loss: 0.4688698649406433
Epoch 1420, training loss: 90.48216247558594 = 0.17570848762989044 + 10.0 * 9.030645370483398
Epoch 1420, val loss: 0.4708264172077179
Epoch 1430, training loss: 90.49608612060547 = 0.17465493083000183 + 10.0 * 9.032143592834473
Epoch 1430, val loss: 0.4727036654949188
Epoch 1440, training loss: 90.47936248779297 = 0.17359013855457306 + 10.0 * 9.030576705932617
Epoch 1440, val loss: 0.4745006263256073
Epoch 1450, training loss: 90.47892761230469 = 0.17253410816192627 + 10.0 * 9.0306396484375
Epoch 1450, val loss: 0.47654104232788086
Epoch 1460, training loss: 90.49024200439453 = 0.17147719860076904 + 10.0 * 9.031876564025879
Epoch 1460, val loss: 0.4784131646156311
Epoch 1470, training loss: 90.47545623779297 = 0.1704261153936386 + 10.0 * 9.030503273010254
Epoch 1470, val loss: 0.48033687472343445
Epoch 1480, training loss: 90.4738540649414 = 0.16936898231506348 + 10.0 * 9.030447959899902
Epoch 1480, val loss: 0.482281893491745
Epoch 1490, training loss: 90.47847747802734 = 0.16830965876579285 + 10.0 * 9.03101634979248
Epoch 1490, val loss: 0.48437005281448364
Epoch 1500, training loss: 90.46443939208984 = 0.16724622249603271 + 10.0 * 9.029719352722168
Epoch 1500, val loss: 0.4862267076969147
Epoch 1510, training loss: 90.46406555175781 = 0.16618216037750244 + 10.0 * 9.02978801727295
Epoch 1510, val loss: 0.488295316696167
Epoch 1520, training loss: 90.49345397949219 = 0.16511870920658112 + 10.0 * 9.03283405303955
Epoch 1520, val loss: 0.49032747745513916
Epoch 1530, training loss: 90.45962524414062 = 0.1640482097864151 + 10.0 * 9.029558181762695
Epoch 1530, val loss: 0.4924464821815491
Epoch 1540, training loss: 90.45181274414062 = 0.16297946870326996 + 10.0 * 9.02888298034668
Epoch 1540, val loss: 0.4945729374885559
Epoch 1550, training loss: 90.4509506225586 = 0.16190636157989502 + 10.0 * 9.028904914855957
Epoch 1550, val loss: 0.4966919422149658
Epoch 1560, training loss: 90.46288299560547 = 0.16083022952079773 + 10.0 * 9.030205726623535
Epoch 1560, val loss: 0.4988798499107361
Epoch 1570, training loss: 90.45011901855469 = 0.15974335372447968 + 10.0 * 9.029037475585938
Epoch 1570, val loss: 0.5009821653366089
Epoch 1580, training loss: 90.44815826416016 = 0.15866419672966003 + 10.0 * 9.028949737548828
Epoch 1580, val loss: 0.503161609172821
Epoch 1590, training loss: 90.45950317382812 = 0.15757571160793304 + 10.0 * 9.030192375183105
Epoch 1590, val loss: 0.505362331867218
Epoch 1600, training loss: 90.43949890136719 = 0.15648679435253143 + 10.0 * 9.028301239013672
Epoch 1600, val loss: 0.5076875686645508
Epoch 1610, training loss: 90.43683624267578 = 0.15539221465587616 + 10.0 * 9.028143882751465
Epoch 1610, val loss: 0.509971559047699
Epoch 1620, training loss: 90.46585083007812 = 0.15429925918579102 + 10.0 * 9.031155586242676
Epoch 1620, val loss: 0.5120870471000671
Epoch 1630, training loss: 90.44185638427734 = 0.15318702161312103 + 10.0 * 9.0288667678833
Epoch 1630, val loss: 0.5146132707595825
Epoch 1640, training loss: 90.43494415283203 = 0.15207570791244507 + 10.0 * 9.028286933898926
Epoch 1640, val loss: 0.5168863534927368
Epoch 1650, training loss: 90.43011474609375 = 0.15095490217208862 + 10.0 * 9.027915954589844
Epoch 1650, val loss: 0.5193246006965637
Epoch 1660, training loss: 90.46565246582031 = 0.14982637763023376 + 10.0 * 9.031582832336426
Epoch 1660, val loss: 0.521781325340271
Epoch 1670, training loss: 90.43585205078125 = 0.148685485124588 + 10.0 * 9.028716087341309
Epoch 1670, val loss: 0.5240826606750488
Epoch 1680, training loss: 90.41967010498047 = 0.14754845201969147 + 10.0 * 9.027212142944336
Epoch 1680, val loss: 0.5266551971435547
Epoch 1690, training loss: 90.41781616210938 = 0.1464066207408905 + 10.0 * 9.027140617370605
Epoch 1690, val loss: 0.5290656685829163
Epoch 1700, training loss: 90.42028045654297 = 0.14525698125362396 + 10.0 * 9.027502059936523
Epoch 1700, val loss: 0.5316514372825623
Epoch 1710, training loss: 90.43863677978516 = 0.14410613477230072 + 10.0 * 9.02945327758789
Epoch 1710, val loss: 0.5341572761535645
Epoch 1720, training loss: 90.42378997802734 = 0.14295199513435364 + 10.0 * 9.028083801269531
Epoch 1720, val loss: 0.536585807800293
Epoch 1730, training loss: 90.41728210449219 = 0.14179843664169312 + 10.0 * 9.027547836303711
Epoch 1730, val loss: 0.5392144322395325
Epoch 1740, training loss: 90.41744232177734 = 0.14064007997512817 + 10.0 * 9.027680397033691
Epoch 1740, val loss: 0.5420922636985779
Epoch 1750, training loss: 90.40580749511719 = 0.13948115706443787 + 10.0 * 9.026632308959961
Epoch 1750, val loss: 0.5447826981544495
Epoch 1760, training loss: 90.41435241699219 = 0.13832376897335052 + 10.0 * 9.027603149414062
Epoch 1760, val loss: 0.547633945941925
Epoch 1770, training loss: 90.4072265625 = 0.13716371357440948 + 10.0 * 9.027006149291992
Epoch 1770, val loss: 0.5502650141716003
Epoch 1780, training loss: 90.4001693725586 = 0.1360015869140625 + 10.0 * 9.026416778564453
Epoch 1780, val loss: 0.5528836846351624
Epoch 1790, training loss: 90.39702606201172 = 0.1348387449979782 + 10.0 * 9.02621841430664
Epoch 1790, val loss: 0.5558426976203918
Epoch 1800, training loss: 90.41764831542969 = 0.13368256390094757 + 10.0 * 9.028396606445312
Epoch 1800, val loss: 0.5587319731712341
Epoch 1810, training loss: 90.39601135253906 = 0.13252127170562744 + 10.0 * 9.026349067687988
Epoch 1810, val loss: 0.5614084601402283
Epoch 1820, training loss: 90.3919448852539 = 0.13136029243469238 + 10.0 * 9.026058197021484
Epoch 1820, val loss: 0.564498245716095
Epoch 1830, training loss: 90.39048767089844 = 0.130201518535614 + 10.0 * 9.026028633117676
Epoch 1830, val loss: 0.5672516226768494
Epoch 1840, training loss: 90.42158508300781 = 0.12904591858386993 + 10.0 * 9.029253959655762
Epoch 1840, val loss: 0.570238471031189
Epoch 1850, training loss: 90.39234924316406 = 0.1278868466615677 + 10.0 * 9.026446342468262
Epoch 1850, val loss: 0.5735486745834351
Epoch 1860, training loss: 90.38236236572266 = 0.1267356425523758 + 10.0 * 9.02556324005127
Epoch 1860, val loss: 0.5764369964599609
Epoch 1870, training loss: 90.40032196044922 = 0.1255834698677063 + 10.0 * 9.027474403381348
Epoch 1870, val loss: 0.5797171592712402
Epoch 1880, training loss: 90.37884521484375 = 0.1244218498468399 + 10.0 * 9.025442123413086
Epoch 1880, val loss: 0.5825059413909912
Epoch 1890, training loss: 90.37336730957031 = 0.12327011674642563 + 10.0 * 9.025010108947754
Epoch 1890, val loss: 0.5858405232429504
Epoch 1900, training loss: 90.37074279785156 = 0.12211355566978455 + 10.0 * 9.024862289428711
Epoch 1900, val loss: 0.5889852046966553
Epoch 1910, training loss: 90.3794174194336 = 0.12096599489450455 + 10.0 * 9.025845527648926
Epoch 1910, val loss: 0.5921732187271118
Epoch 1920, training loss: 90.37260437011719 = 0.11981436610221863 + 10.0 * 9.02527904510498
Epoch 1920, val loss: 0.5952998995780945
Epoch 1930, training loss: 90.38186645507812 = 0.11866972595453262 + 10.0 * 9.02631950378418
Epoch 1930, val loss: 0.5983104109764099
Epoch 1940, training loss: 90.3687973022461 = 0.11753156036138535 + 10.0 * 9.025126457214355
Epoch 1940, val loss: 0.6019293069839478
Epoch 1950, training loss: 90.3863754272461 = 0.11640171706676483 + 10.0 * 9.026997566223145
Epoch 1950, val loss: 0.6049639582633972
Epoch 1960, training loss: 90.36450958251953 = 0.11526485532522202 + 10.0 * 9.024924278259277
Epoch 1960, val loss: 0.608573853969574
Epoch 1970, training loss: 90.35624694824219 = 0.11413928121328354 + 10.0 * 9.024210929870605
Epoch 1970, val loss: 0.6117641925811768
Epoch 1980, training loss: 90.35253143310547 = 0.11300260573625565 + 10.0 * 9.023953437805176
Epoch 1980, val loss: 0.6153315305709839
Epoch 1990, training loss: 90.36188507080078 = 0.11188089102506638 + 10.0 * 9.02500057220459
Epoch 1990, val loss: 0.6186498403549194
Epoch 2000, training loss: 90.35370635986328 = 0.11075656861066818 + 10.0 * 9.02429485321045
Epoch 2000, val loss: 0.6222503781318665
Epoch 2010, training loss: 90.35256958007812 = 0.109639972448349 + 10.0 * 9.024292945861816
Epoch 2010, val loss: 0.6257890462875366
Epoch 2020, training loss: 90.345947265625 = 0.10852986574172974 + 10.0 * 9.023741722106934
Epoch 2020, val loss: 0.6293073892593384
Epoch 2030, training loss: 90.3471450805664 = 0.10741964727640152 + 10.0 * 9.023972511291504
Epoch 2030, val loss: 0.6328369975090027
Epoch 2040, training loss: 90.36427307128906 = 0.10631843656301498 + 10.0 * 9.025795936584473
Epoch 2040, val loss: 0.6365395188331604
Epoch 2050, training loss: 90.35131072998047 = 0.10523349791765213 + 10.0 * 9.02460765838623
Epoch 2050, val loss: 0.6401963829994202
Epoch 2060, training loss: 90.33781433105469 = 0.1041426733136177 + 10.0 * 9.023366928100586
Epoch 2060, val loss: 0.6436957716941833
Epoch 2070, training loss: 90.33680725097656 = 0.10305739939212799 + 10.0 * 9.023374557495117
Epoch 2070, val loss: 0.6473721861839294
Epoch 2080, training loss: 90.35061645507812 = 0.10198717564344406 + 10.0 * 9.024862289428711
Epoch 2080, val loss: 0.651138186454773
Epoch 2090, training loss: 90.33586883544922 = 0.10091531276702881 + 10.0 * 9.0234956741333
Epoch 2090, val loss: 0.6549075841903687
Epoch 2100, training loss: 90.33512115478516 = 0.09985867142677307 + 10.0 * 9.023526191711426
Epoch 2100, val loss: 0.6587393879890442
Epoch 2110, training loss: 90.32674407958984 = 0.0987948328256607 + 10.0 * 9.022794723510742
Epoch 2110, val loss: 0.6625231504440308
Epoch 2120, training loss: 90.3310546875 = 0.09773924946784973 + 10.0 * 9.023331642150879
Epoch 2120, val loss: 0.6663592457771301
Epoch 2130, training loss: 90.3396224975586 = 0.09669596701860428 + 10.0 * 9.024292945861816
Epoch 2130, val loss: 0.6702280640602112
Epoch 2140, training loss: 90.35071563720703 = 0.0956505760550499 + 10.0 * 9.025506973266602
Epoch 2140, val loss: 0.6745649576187134
Epoch 2150, training loss: 90.32025146484375 = 0.09460531920194626 + 10.0 * 9.022564888000488
Epoch 2150, val loss: 0.6778449416160583
