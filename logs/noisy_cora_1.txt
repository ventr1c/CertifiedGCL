Begin epxeriment: cont_weight: 1 epoch:1000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0001, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=1.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11604])
remove edge: torch.Size([2, 9528])
updated graph: torch.Size([2, 10576])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:224: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 10.532079696655273 = 1.9352258443832397 + 1.0 * 8.596854209899902
Epoch 0, val loss: 1.9312628507614136
Epoch 10, training loss: 10.527202606201172 = 1.930410623550415 + 1.0 * 8.596792221069336
Epoch 10, val loss: 1.9265204668045044
Epoch 20, training loss: 10.521871566772461 = 1.9252855777740479 + 1.0 * 8.596586227416992
Epoch 20, val loss: 1.9212359189987183
Epoch 30, training loss: 10.515021324157715 = 1.9192713499069214 + 1.0 * 8.595749855041504
Epoch 30, val loss: 1.9148060083389282
Epoch 40, training loss: 10.503798484802246 = 1.9118465185165405 + 1.0 * 8.591952323913574
Epoch 40, val loss: 1.90658700466156
Epoch 50, training loss: 10.478833198547363 = 1.9024678468704224 + 1.0 * 8.57636547088623
Epoch 50, val loss: 1.895951747894287
Epoch 60, training loss: 10.413044929504395 = 1.8910897970199585 + 1.0 * 8.521955490112305
Epoch 60, val loss: 1.8831143379211426
Epoch 70, training loss: 10.223763465881348 = 1.8786122798919678 + 1.0 * 8.3451509475708
Epoch 70, val loss: 1.8695627450942993
Epoch 80, training loss: 9.950860023498535 = 1.8657206296920776 + 1.0 * 8.085139274597168
Epoch 80, val loss: 1.8562275171279907
Epoch 90, training loss: 9.836105346679688 = 1.8547520637512207 + 1.0 * 7.981353759765625
Epoch 90, val loss: 1.8457632064819336
Epoch 100, training loss: 9.586682319641113 = 1.8457401990890503 + 1.0 * 7.740942478179932
Epoch 100, val loss: 1.8376888036727905
Epoch 110, training loss: 9.216660499572754 = 1.8391350507736206 + 1.0 * 7.377525806427002
Epoch 110, val loss: 1.8319180011749268
Epoch 120, training loss: 8.906200408935547 = 1.8336621522903442 + 1.0 * 7.072537899017334
Epoch 120, val loss: 1.8270728588104248
Epoch 130, training loss: 8.738080024719238 = 1.828631043434143 + 1.0 * 6.909448623657227
Epoch 130, val loss: 1.8224903345108032
Epoch 140, training loss: 8.648416519165039 = 1.8224889039993286 + 1.0 * 6.825927734375
Epoch 140, val loss: 1.816483497619629
Epoch 150, training loss: 8.574468612670898 = 1.815327525138855 + 1.0 * 6.759140968322754
Epoch 150, val loss: 1.8096815347671509
Epoch 160, training loss: 8.516222953796387 = 1.8081574440002441 + 1.0 * 6.708065509796143
Epoch 160, val loss: 1.802817940711975
Epoch 170, training loss: 8.465621948242188 = 1.801235556602478 + 1.0 * 6.664386749267578
Epoch 170, val loss: 1.7962970733642578
Epoch 180, training loss: 8.42341423034668 = 1.7946382761001587 + 1.0 * 6.6287760734558105
Epoch 180, val loss: 1.7902226448059082
Epoch 190, training loss: 8.387081146240234 = 1.788159728050232 + 1.0 * 6.598921775817871
Epoch 190, val loss: 1.7844735383987427
Epoch 200, training loss: 8.355531692504883 = 1.7816089391708374 + 1.0 * 6.573923110961914
Epoch 200, val loss: 1.7788262367248535
Epoch 210, training loss: 8.328128814697266 = 1.774801254272461 + 1.0 * 6.5533270835876465
Epoch 210, val loss: 1.77310049533844
Epoch 220, training loss: 8.302390098571777 = 1.767652988433838 + 1.0 * 6.5347371101379395
Epoch 220, val loss: 1.7671616077423096
Epoch 230, training loss: 8.278690338134766 = 1.760111927986145 + 1.0 * 6.51857852935791
Epoch 230, val loss: 1.760970950126648
Epoch 240, training loss: 8.25734806060791 = 1.7521438598632812 + 1.0 * 6.505204200744629
Epoch 240, val loss: 1.7544382810592651
Epoch 250, training loss: 8.236769676208496 = 1.7436453104019165 + 1.0 * 6.493124485015869
Epoch 250, val loss: 1.7475394010543823
Epoch 260, training loss: 8.21650218963623 = 1.7345315217971802 + 1.0 * 6.48197078704834
Epoch 260, val loss: 1.7401546239852905
Epoch 270, training loss: 8.196806907653809 = 1.724679946899414 + 1.0 * 6.4721269607543945
Epoch 270, val loss: 1.7322050333023071
Epoch 280, training loss: 8.1768159866333 = 1.7140491008758545 + 1.0 * 6.462766647338867
Epoch 280, val loss: 1.7236202955245972
Epoch 290, training loss: 8.16246223449707 = 1.702549934387207 + 1.0 * 6.459911823272705
Epoch 290, val loss: 1.7143892049789429
Epoch 300, training loss: 8.137518882751465 = 1.6904064416885376 + 1.0 * 6.447112560272217
Epoch 300, val loss: 1.7046070098876953
Epoch 310, training loss: 8.116662979125977 = 1.6774317026138306 + 1.0 * 6.4392313957214355
Epoch 310, val loss: 1.694183349609375
Epoch 320, training loss: 8.096247673034668 = 1.6635644435882568 + 1.0 * 6.43268346786499
Epoch 320, val loss: 1.6830356121063232
Epoch 330, training loss: 8.074854850769043 = 1.6487058401107788 + 1.0 * 6.426148891448975
Epoch 330, val loss: 1.67110276222229
Epoch 340, training loss: 8.052946090698242 = 1.632798194885254 + 1.0 * 6.4201483726501465
Epoch 340, val loss: 1.6583445072174072
Epoch 350, training loss: 8.03311538696289 = 1.615804672241211 + 1.0 * 6.41731071472168
Epoch 350, val loss: 1.644761562347412
Epoch 360, training loss: 8.007674217224121 = 1.59800124168396 + 1.0 * 6.409672737121582
Epoch 360, val loss: 1.6305440664291382
Epoch 370, training loss: 7.983945846557617 = 1.5792522430419922 + 1.0 * 6.404693603515625
Epoch 370, val loss: 1.6156420707702637
Epoch 380, training loss: 7.959486961364746 = 1.5596078634262085 + 1.0 * 6.399878978729248
Epoch 380, val loss: 1.6000646352767944
Epoch 390, training loss: 7.936944961547852 = 1.539029836654663 + 1.0 * 6.397915363311768
Epoch 390, val loss: 1.5838266611099243
Epoch 400, training loss: 7.909900188446045 = 1.5179800987243652 + 1.0 * 6.39192008972168
Epoch 400, val loss: 1.5673543214797974
Epoch 410, training loss: 7.884111404418945 = 1.4963526725769043 + 1.0 * 6.387758731842041
Epoch 410, val loss: 1.5505198240280151
Epoch 420, training loss: 7.858104705810547 = 1.4740813970565796 + 1.0 * 6.384023189544678
Epoch 420, val loss: 1.5333747863769531
Epoch 430, training loss: 7.831704616546631 = 1.4512192010879517 + 1.0 * 6.380485534667969
Epoch 430, val loss: 1.5159777402877808
Epoch 440, training loss: 7.805666923522949 = 1.4278110265731812 + 1.0 * 6.3778557777404785
Epoch 440, val loss: 1.4983447790145874
Epoch 450, training loss: 7.780215263366699 = 1.404176950454712 + 1.0 * 6.376038551330566
Epoch 450, val loss: 1.4807368516921997
Epoch 460, training loss: 7.752029895782471 = 1.3805092573165894 + 1.0 * 6.371520519256592
Epoch 460, val loss: 1.4632644653320312
Epoch 470, training loss: 7.725113868713379 = 1.3565919399261475 + 1.0 * 6.368521690368652
Epoch 470, val loss: 1.4457972049713135
Epoch 480, training loss: 7.698427200317383 = 1.3323994874954224 + 1.0 * 6.36602783203125
Epoch 480, val loss: 1.4282969236373901
Epoch 490, training loss: 7.671320915222168 = 1.3079559803009033 + 1.0 * 6.3633646965026855
Epoch 490, val loss: 1.4107269048690796
Epoch 500, training loss: 7.651214599609375 = 1.2833725214004517 + 1.0 * 6.367842197418213
Epoch 500, val loss: 1.393165946006775
Epoch 510, training loss: 7.618435382843018 = 1.2591558694839478 + 1.0 * 6.359279632568359
Epoch 510, val loss: 1.3758820295333862
Epoch 520, training loss: 7.592171669006348 = 1.2350363731384277 + 1.0 * 6.35713529586792
Epoch 520, val loss: 1.3587158918380737
Epoch 530, training loss: 7.5657854080200195 = 1.210951566696167 + 1.0 * 6.354834079742432
Epoch 530, val loss: 1.3416006565093994
Epoch 540, training loss: 7.544423580169678 = 1.1869131326675415 + 1.0 * 6.357510566711426
Epoch 540, val loss: 1.3245441913604736
Epoch 550, training loss: 7.5150837898254395 = 1.1633110046386719 + 1.0 * 6.351772785186768
Epoch 550, val loss: 1.30769944190979
Epoch 560, training loss: 7.489635944366455 = 1.1399273872375488 + 1.0 * 6.349708557128906
Epoch 560, val loss: 1.2910016775131226
Epoch 570, training loss: 7.464225769042969 = 1.116729974746704 + 1.0 * 6.347496032714844
Epoch 570, val loss: 1.274407982826233
Epoch 580, training loss: 7.43930196762085 = 1.0936983823776245 + 1.0 * 6.3456034660339355
Epoch 580, val loss: 1.2579102516174316
Epoch 590, training loss: 7.4153852462768555 = 1.0708552598953247 + 1.0 * 6.34453010559082
Epoch 590, val loss: 1.2414660453796387
Epoch 600, training loss: 7.394690036773682 = 1.048383355140686 + 1.0 * 6.346306800842285
Epoch 600, val loss: 1.2253732681274414
Epoch 610, training loss: 7.367274284362793 = 1.0264064073562622 + 1.0 * 6.34086799621582
Epoch 610, val loss: 1.2094287872314453
Epoch 620, training loss: 7.344363212585449 = 1.0046473741531372 + 1.0 * 6.339715957641602
Epoch 620, val loss: 1.19365656375885
Epoch 630, training loss: 7.324723243713379 = 0.9830607771873474 + 1.0 * 6.341662406921387
Epoch 630, val loss: 1.17795729637146
Epoch 640, training loss: 7.3010358810424805 = 0.9617965221405029 + 1.0 * 6.339239120483398
Epoch 640, val loss: 1.1624780893325806
Epoch 650, training loss: 7.276980876922607 = 0.9408851265907288 + 1.0 * 6.336095809936523
Epoch 650, val loss: 1.1471425294876099
Epoch 660, training loss: 7.253899097442627 = 0.9201168417930603 + 1.0 * 6.333782196044922
Epoch 660, val loss: 1.131890058517456
Epoch 670, training loss: 7.231690406799316 = 0.8994587659835815 + 1.0 * 6.332231521606445
Epoch 670, val loss: 1.1166349649429321
Epoch 680, training loss: 7.20972204208374 = 0.8788862228393555 + 1.0 * 6.330835819244385
Epoch 680, val loss: 1.1013818979263306
Epoch 690, training loss: 7.188584804534912 = 0.8584246635437012 + 1.0 * 6.330160140991211
Epoch 690, val loss: 1.0861462354660034
Epoch 700, training loss: 7.171560287475586 = 0.8382058143615723 + 1.0 * 6.333354473114014
Epoch 700, val loss: 1.0710958242416382
Epoch 710, training loss: 7.147629261016846 = 0.8184377551078796 + 1.0 * 6.3291916847229
Epoch 710, val loss: 1.0562348365783691
Epoch 720, training loss: 7.1249213218688965 = 0.7989120483398438 + 1.0 * 6.326009273529053
Epoch 720, val loss: 1.0415788888931274
Epoch 730, training loss: 7.104383945465088 = 0.7795587778091431 + 1.0 * 6.324825286865234
Epoch 730, val loss: 1.0270904302597046
Epoch 740, training loss: 7.083980083465576 = 0.7603617906570435 + 1.0 * 6.323618412017822
Epoch 740, val loss: 1.0127809047698975
Epoch 750, training loss: 7.067538261413574 = 0.7413623332977295 + 1.0 * 6.326176166534424
Epoch 750, val loss: 0.9986605048179626
Epoch 760, training loss: 7.046131610870361 = 0.7227161526679993 + 1.0 * 6.323415279388428
Epoch 760, val loss: 0.9849101901054382
Epoch 770, training loss: 7.02578067779541 = 0.7044520974159241 + 1.0 * 6.321328639984131
Epoch 770, val loss: 0.9715582728385925
Epoch 780, training loss: 7.005890846252441 = 0.6864109039306641 + 1.0 * 6.319479942321777
Epoch 780, val loss: 0.9585432410240173
Epoch 790, training loss: 6.986708164215088 = 0.6685752272605896 + 1.0 * 6.3181328773498535
Epoch 790, val loss: 0.9458284974098206
Epoch 800, training loss: 6.96832275390625 = 0.6509255170822144 + 1.0 * 6.317397117614746
Epoch 800, val loss: 0.9334460496902466
Epoch 810, training loss: 6.953079700469971 = 0.6335464119911194 + 1.0 * 6.319533348083496
Epoch 810, val loss: 0.9214491844177246
Epoch 820, training loss: 6.933888912200928 = 0.616540789604187 + 1.0 * 6.317348003387451
Epoch 820, val loss: 0.9099931120872498
Epoch 830, training loss: 6.914430141448975 = 0.599856972694397 + 1.0 * 6.314573287963867
Epoch 830, val loss: 0.8989913463592529
Epoch 840, training loss: 6.896547794342041 = 0.5834240913391113 + 1.0 * 6.31312370300293
Epoch 840, val loss: 0.8884115219116211
Epoch 850, training loss: 6.882102966308594 = 0.5672354102134705 + 1.0 * 6.3148674964904785
Epoch 850, val loss: 0.8782724738121033
Epoch 860, training loss: 6.864251613616943 = 0.5513264536857605 + 1.0 * 6.312925338745117
Epoch 860, val loss: 0.8686659932136536
Epoch 870, training loss: 6.846989631652832 = 0.5358178615570068 + 1.0 * 6.311171531677246
Epoch 870, val loss: 0.8595272302627563
Epoch 880, training loss: 6.8297905921936035 = 0.5205464959144592 + 1.0 * 6.309244155883789
Epoch 880, val loss: 0.8509072661399841
Epoch 890, training loss: 6.814105033874512 = 0.5055748224258423 + 1.0 * 6.308530330657959
Epoch 890, val loss: 0.8427629470825195
Epoch 900, training loss: 6.799280166625977 = 0.4909820258617401 + 1.0 * 6.308298110961914
Epoch 900, val loss: 0.8351483345031738
Epoch 910, training loss: 6.783720016479492 = 0.47685572504997253 + 1.0 * 6.306864261627197
Epoch 910, val loss: 0.8281154036521912
Epoch 920, training loss: 6.769251346588135 = 0.4630957245826721 + 1.0 * 6.306155681610107
Epoch 920, val loss: 0.8215950727462769
Epoch 930, training loss: 6.754536151885986 = 0.44963982701301575 + 1.0 * 6.304896354675293
Epoch 930, val loss: 0.8155495524406433
Epoch 940, training loss: 6.740285396575928 = 0.4364883303642273 + 1.0 * 6.303797245025635
Epoch 940, val loss: 0.8099455237388611
Epoch 950, training loss: 6.742197513580322 = 0.42369982600212097 + 1.0 * 6.318497657775879
Epoch 950, val loss: 0.8047957420349121
Epoch 960, training loss: 6.714975357055664 = 0.41135621070861816 + 1.0 * 6.303619384765625
Epoch 960, val loss: 0.8002033233642578
Epoch 970, training loss: 6.702423572540283 = 0.39944711327552795 + 1.0 * 6.302976608276367
Epoch 970, val loss: 0.7960872650146484
Epoch 980, training loss: 6.688899993896484 = 0.3878697454929352 + 1.0 * 6.301030158996582
Epoch 980, val loss: 0.7923813462257385
Epoch 990, training loss: 6.676283359527588 = 0.37660539150238037 + 1.0 * 6.299677848815918
Epoch 990, val loss: 0.7890574336051941
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7740740740740741
0.8139167105956774
=== training gcn model ===
Epoch 0, training loss: 10.53359317779541 = 1.9367377758026123 + 1.0 * 8.596855163574219
Epoch 0, val loss: 1.941996455192566
Epoch 10, training loss: 10.5286865234375 = 1.9319062232971191 + 1.0 * 8.596779823303223
Epoch 10, val loss: 1.9373198747634888
Epoch 20, training loss: 10.523177146911621 = 1.9266380071640015 + 1.0 * 8.596539497375488
Epoch 20, val loss: 1.9321056604385376
Epoch 30, training loss: 10.516166687011719 = 1.9204918146133423 + 1.0 * 8.595674514770508
Epoch 30, val loss: 1.9259661436080933
Epoch 40, training loss: 10.504853248596191 = 1.9129629135131836 + 1.0 * 8.591890335083008
Epoch 40, val loss: 1.9183838367462158
Epoch 50, training loss: 10.47815990447998 = 1.9036028385162354 + 1.0 * 8.574557304382324
Epoch 50, val loss: 1.9088937044143677
Epoch 60, training loss: 10.392317771911621 = 1.8924305438995361 + 1.0 * 8.499887466430664
Epoch 60, val loss: 1.8977020978927612
Epoch 70, training loss: 10.087662696838379 = 1.88006591796875 + 1.0 * 8.207596778869629
Epoch 70, val loss: 1.8854756355285645
Epoch 80, training loss: 9.678879737854004 = 1.8674672842025757 + 1.0 * 7.811412334442139
Epoch 80, val loss: 1.8732917308807373
Epoch 90, training loss: 9.350055694580078 = 1.859503149986267 + 1.0 * 7.4905524253845215
Epoch 90, val loss: 1.8659173250198364
Epoch 100, training loss: 9.058953285217285 = 1.85395085811615 + 1.0 * 7.205002784729004
Epoch 100, val loss: 1.8602912425994873
Epoch 110, training loss: 8.851057052612305 = 1.849128246307373 + 1.0 * 7.001928329467773
Epoch 110, val loss: 1.8555245399475098
Epoch 120, training loss: 8.711419105529785 = 1.8438471555709839 + 1.0 * 6.86757230758667
Epoch 120, val loss: 1.8504254817962646
Epoch 130, training loss: 8.623408317565918 = 1.837935209274292 + 1.0 * 6.785472869873047
Epoch 130, val loss: 1.84480619430542
Epoch 140, training loss: 8.55832290649414 = 1.8317142724990845 + 1.0 * 6.726608753204346
Epoch 140, val loss: 1.8388041257858276
Epoch 150, training loss: 8.505962371826172 = 1.8255000114440918 + 1.0 * 6.680461883544922
Epoch 150, val loss: 1.8327274322509766
Epoch 160, training loss: 8.464898109436035 = 1.8193832635879517 + 1.0 * 6.645514965057373
Epoch 160, val loss: 1.826804757118225
Epoch 170, training loss: 8.427177429199219 = 1.813455581665039 + 1.0 * 6.61372184753418
Epoch 170, val loss: 1.8209768533706665
Epoch 180, training loss: 8.39498233795166 = 1.8075177669525146 + 1.0 * 6.587464809417725
Epoch 180, val loss: 1.8152259588241577
Epoch 190, training loss: 8.365921974182129 = 1.8015092611312866 + 1.0 * 6.564412593841553
Epoch 190, val loss: 1.8094171285629272
Epoch 200, training loss: 8.33999252319336 = 1.7954401969909668 + 1.0 * 6.544551849365234
Epoch 200, val loss: 1.8035533428192139
Epoch 210, training loss: 8.315022468566895 = 1.7892402410507202 + 1.0 * 6.525782108306885
Epoch 210, val loss: 1.797592282295227
Epoch 220, training loss: 8.294403076171875 = 1.7828164100646973 + 1.0 * 6.511586666107178
Epoch 220, val loss: 1.7914894819259644
Epoch 230, training loss: 8.27259635925293 = 1.7761154174804688 + 1.0 * 6.496481418609619
Epoch 230, val loss: 1.7852251529693604
Epoch 240, training loss: 8.251429557800293 = 1.769124150276184 + 1.0 * 6.48230504989624
Epoch 240, val loss: 1.778715968132019
Epoch 250, training loss: 8.231904029846191 = 1.7616857290267944 + 1.0 * 6.470218181610107
Epoch 250, val loss: 1.7719626426696777
Epoch 260, training loss: 8.21279525756836 = 1.7537262439727783 + 1.0 * 6.45906925201416
Epoch 260, val loss: 1.7647967338562012
Epoch 270, training loss: 8.197539329528809 = 1.7450860738754272 + 1.0 * 6.45245361328125
Epoch 270, val loss: 1.7571572065353394
Epoch 280, training loss: 8.175817489624023 = 1.7358211278915405 + 1.0 * 6.439996242523193
Epoch 280, val loss: 1.7490078210830688
Epoch 290, training loss: 8.157509803771973 = 1.725756287574768 + 1.0 * 6.431753635406494
Epoch 290, val loss: 1.7402517795562744
Epoch 300, training loss: 8.138862609863281 = 1.7147691249847412 + 1.0 * 6.424093246459961
Epoch 300, val loss: 1.7308012247085571
Epoch 310, training loss: 8.121475219726562 = 1.7027838230133057 + 1.0 * 6.418691158294678
Epoch 310, val loss: 1.7205984592437744
Epoch 320, training loss: 8.101237297058105 = 1.6898077726364136 + 1.0 * 6.4114298820495605
Epoch 320, val loss: 1.7096261978149414
Epoch 330, training loss: 8.080385208129883 = 1.6756863594055176 + 1.0 * 6.404699325561523
Epoch 330, val loss: 1.697762370109558
Epoch 340, training loss: 8.059536933898926 = 1.6602805852890015 + 1.0 * 6.399256706237793
Epoch 340, val loss: 1.684902548789978
Epoch 350, training loss: 8.04141902923584 = 1.643500566482544 + 1.0 * 6.397918701171875
Epoch 350, val loss: 1.6710243225097656
Epoch 360, training loss: 8.016500473022461 = 1.6256250143051147 + 1.0 * 6.390875816345215
Epoch 360, val loss: 1.6562998294830322
Epoch 370, training loss: 7.992166519165039 = 1.6065561771392822 + 1.0 * 6.385610580444336
Epoch 370, val loss: 1.640687108039856
Epoch 380, training loss: 7.96722412109375 = 1.586190104484558 + 1.0 * 6.381033897399902
Epoch 380, val loss: 1.6240986585617065
Epoch 390, training loss: 7.941583156585693 = 1.564429759979248 + 1.0 * 6.377153396606445
Epoch 390, val loss: 1.6064434051513672
Epoch 400, training loss: 7.91477632522583 = 1.541273593902588 + 1.0 * 6.373502731323242
Epoch 400, val loss: 1.5877493619918823
Epoch 410, training loss: 7.889859676361084 = 1.5169196128845215 + 1.0 * 6.3729400634765625
Epoch 410, val loss: 1.5682564973831177
Epoch 420, training loss: 7.86034631729126 = 1.4918781518936157 + 1.0 * 6.368468284606934
Epoch 420, val loss: 1.5484111309051514
Epoch 430, training loss: 7.830296516418457 = 1.4663314819335938 + 1.0 * 6.363965034484863
Epoch 430, val loss: 1.528346061706543
Epoch 440, training loss: 7.801102638244629 = 1.4402216672897339 + 1.0 * 6.3608808517456055
Epoch 440, val loss: 1.5080773830413818
Epoch 450, training loss: 7.77179479598999 = 1.4136985540390015 + 1.0 * 6.358096122741699
Epoch 450, val loss: 1.4877238273620605
Epoch 460, training loss: 7.746658802032471 = 1.3870849609375 + 1.0 * 6.359573841094971
Epoch 460, val loss: 1.467622995376587
Epoch 470, training loss: 7.713870048522949 = 1.36074697971344 + 1.0 * 6.353123188018799
Epoch 470, val loss: 1.4480499029159546
Epoch 480, training loss: 7.68559455871582 = 1.3346972465515137 + 1.0 * 6.350897312164307
Epoch 480, val loss: 1.428959608078003
Epoch 490, training loss: 7.657260894775391 = 1.308954119682312 + 1.0 * 6.348306655883789
Epoch 490, val loss: 1.4103089570999146
Epoch 500, training loss: 7.630560874938965 = 1.2834845781326294 + 1.0 * 6.347076416015625
Epoch 500, val loss: 1.392111897468567
Epoch 510, training loss: 7.604825019836426 = 1.2584774494171143 + 1.0 * 6.346347808837891
Epoch 510, val loss: 1.3744956254959106
Epoch 520, training loss: 7.576362609863281 = 1.2340028285980225 + 1.0 * 6.342360019683838
Epoch 520, val loss: 1.3574731349945068
Epoch 530, training loss: 7.550100803375244 = 1.2099164724349976 + 1.0 * 6.340184211730957
Epoch 530, val loss: 1.3408418893814087
Epoch 540, training loss: 7.524328231811523 = 1.1860997676849365 + 1.0 * 6.338228225708008
Epoch 540, val loss: 1.3244738578796387
Epoch 550, training loss: 7.504543304443359 = 1.1625092029571533 + 1.0 * 6.342033863067627
Epoch 550, val loss: 1.3083008527755737
Epoch 560, training loss: 7.47442626953125 = 1.1392892599105835 + 1.0 * 6.335136890411377
Epoch 560, val loss: 1.2924448251724243
Epoch 570, training loss: 7.4499969482421875 = 1.1163486242294312 + 1.0 * 6.333648204803467
Epoch 570, val loss: 1.2767817974090576
Epoch 580, training loss: 7.42479133605957 = 1.0935450792312622 + 1.0 * 6.331246376037598
Epoch 580, val loss: 1.2611653804779053
Epoch 590, training loss: 7.4010162353515625 = 1.0708081722259521 + 1.0 * 6.330207824707031
Epoch 590, val loss: 1.2455344200134277
Epoch 600, training loss: 7.379270553588867 = 1.0482637882232666 + 1.0 * 6.33100700378418
Epoch 600, val loss: 1.230059266090393
Epoch 610, training loss: 7.354110240936279 = 1.0261045694351196 + 1.0 * 6.328005790710449
Epoch 610, val loss: 1.2147696018218994
Epoch 620, training loss: 7.329772472381592 = 1.0041594505310059 + 1.0 * 6.325613021850586
Epoch 620, val loss: 1.199600100517273
Epoch 630, training loss: 7.306262969970703 = 0.9823819398880005 + 1.0 * 6.323881149291992
Epoch 630, val loss: 1.1845101118087769
Epoch 640, training loss: 7.283458709716797 = 0.9607876539230347 + 1.0 * 6.322670936584473
Epoch 640, val loss: 1.1695361137390137
Epoch 650, training loss: 7.26187801361084 = 0.9395391941070557 + 1.0 * 6.322339057922363
Epoch 650, val loss: 1.154815912246704
Epoch 660, training loss: 7.239333152770996 = 0.9188588261604309 + 1.0 * 6.320474147796631
Epoch 660, val loss: 1.1405569314956665
Epoch 670, training loss: 7.217290878295898 = 0.8986133933067322 + 1.0 * 6.3186774253845215
Epoch 670, val loss: 1.1266758441925049
Epoch 680, training loss: 7.196016788482666 = 0.8787493705749512 + 1.0 * 6.317267417907715
Epoch 680, val loss: 1.113127589225769
Epoch 690, training loss: 7.177201747894287 = 0.8592581748962402 + 1.0 * 6.317943572998047
Epoch 690, val loss: 1.0999315977096558
Epoch 700, training loss: 7.1573920249938965 = 0.8402588963508606 + 1.0 * 6.317132949829102
Epoch 700, val loss: 1.0872290134429932
Epoch 710, training loss: 7.135715484619141 = 0.8218054175376892 + 1.0 * 6.313910007476807
Epoch 710, val loss: 1.0750553607940674
Epoch 720, training loss: 7.11616325378418 = 0.8037546873092651 + 1.0 * 6.312408447265625
Epoch 720, val loss: 1.0634028911590576
Epoch 730, training loss: 7.097558498382568 = 0.7860882878303528 + 1.0 * 6.311470031738281
Epoch 730, val loss: 1.0521897077560425
Epoch 740, training loss: 7.080615043640137 = 0.7688693404197693 + 1.0 * 6.311745643615723
Epoch 740, val loss: 1.0414615869522095
Epoch 750, training loss: 7.061944484710693 = 0.7521418333053589 + 1.0 * 6.309802532196045
Epoch 750, val loss: 1.0313284397125244
Epoch 760, training loss: 7.044101715087891 = 0.7358298897743225 + 1.0 * 6.308271884918213
Epoch 760, val loss: 1.0216732025146484
Epoch 770, training loss: 7.0267815589904785 = 0.719821572303772 + 1.0 * 6.306960105895996
Epoch 770, val loss: 1.0124834775924683
Epoch 780, training loss: 7.014568328857422 = 0.7041221857070923 + 1.0 * 6.310446262359619
Epoch 780, val loss: 1.0037219524383545
Epoch 790, training loss: 6.99340295791626 = 0.6888036727905273 + 1.0 * 6.304599285125732
Epoch 790, val loss: 0.9954466819763184
Epoch 800, training loss: 6.9780073165893555 = 0.6738102436065674 + 1.0 * 6.304196834564209
Epoch 800, val loss: 0.9875785708427429
Epoch 810, training loss: 6.961496829986572 = 0.6590468287467957 + 1.0 * 6.302450180053711
Epoch 810, val loss: 0.9800887703895569
Epoch 820, training loss: 6.947944164276123 = 0.6444721817970276 + 1.0 * 6.30347204208374
Epoch 820, val loss: 0.9729413986206055
Epoch 830, training loss: 6.9326019287109375 = 0.6300941109657288 + 1.0 * 6.3025078773498535
Epoch 830, val loss: 0.966120183467865
Epoch 840, training loss: 6.916081428527832 = 0.6159595251083374 + 1.0 * 6.300121784210205
Epoch 840, val loss: 0.9596064686775208
Epoch 850, training loss: 6.900727272033691 = 0.601970911026001 + 1.0 * 6.2987565994262695
Epoch 850, val loss: 0.953386664390564
Epoch 860, training loss: 6.890138626098633 = 0.5881130695343018 + 1.0 * 6.302025318145752
Epoch 860, val loss: 0.9474135637283325
Epoch 870, training loss: 6.874302864074707 = 0.5745232105255127 + 1.0 * 6.299779891967773
Epoch 870, val loss: 0.941714882850647
Epoch 880, training loss: 6.857535362243652 = 0.5610808730125427 + 1.0 * 6.296454429626465
Epoch 880, val loss: 0.9363390207290649
Epoch 890, training loss: 6.843083381652832 = 0.5478067994117737 + 1.0 * 6.295276641845703
Epoch 890, val loss: 0.9311906099319458
Epoch 900, training loss: 6.82922887802124 = 0.5346547961235046 + 1.0 * 6.29457426071167
Epoch 900, val loss: 0.92624831199646
Epoch 910, training loss: 6.817168712615967 = 0.5216428637504578 + 1.0 * 6.295526027679443
Epoch 910, val loss: 0.9215609431266785
Epoch 920, training loss: 6.801680564880371 = 0.5088074803352356 + 1.0 * 6.292872905731201
Epoch 920, val loss: 0.9171432256698608
Epoch 930, training loss: 6.788421630859375 = 0.4961354434490204 + 1.0 * 6.292286396026611
Epoch 930, val loss: 0.9129449129104614
Epoch 940, training loss: 6.7769293785095215 = 0.4836113750934601 + 1.0 * 6.293317794799805
Epoch 940, val loss: 0.9089915752410889
Epoch 950, training loss: 6.762335300445557 = 0.4712927043437958 + 1.0 * 6.291042804718018
Epoch 950, val loss: 0.9053048491477966
Epoch 960, training loss: 6.750322341918945 = 0.45914292335510254 + 1.0 * 6.291179180145264
Epoch 960, val loss: 0.9018785357475281
Epoch 970, training loss: 6.73663330078125 = 0.44721555709838867 + 1.0 * 6.289417743682861
Epoch 970, val loss: 0.8986932635307312
Epoch 980, training loss: 6.723998069763184 = 0.4355010390281677 + 1.0 * 6.288496971130371
Epoch 980, val loss: 0.895746111869812
Epoch 990, training loss: 6.711337566375732 = 0.42396581172943115 + 1.0 * 6.287371635437012
Epoch 990, val loss: 0.8930964469909668
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7296296296296296
0.8081180811808119
=== training gcn model ===
Epoch 0, training loss: 10.523533821105957 = 1.9266873598098755 + 1.0 * 8.596846580505371
Epoch 0, val loss: 1.9291495084762573
Epoch 10, training loss: 10.51877212524414 = 1.9219857454299927 + 1.0 * 8.596786499023438
Epoch 10, val loss: 1.9241915941238403
Epoch 20, training loss: 10.513513565063477 = 1.9169249534606934 + 1.0 * 8.596589088439941
Epoch 20, val loss: 1.9187886714935303
Epoch 30, training loss: 10.506868362426758 = 1.9110122919082642 + 1.0 * 8.595855712890625
Epoch 30, val loss: 1.912429690361023
Epoch 40, training loss: 10.49646282196045 = 1.9037573337554932 + 1.0 * 8.592705726623535
Epoch 40, val loss: 1.9045419692993164
Epoch 50, training loss: 10.474536895751953 = 1.8946778774261475 + 1.0 * 8.579858779907227
Epoch 50, val loss: 1.8945040702819824
Epoch 60, training loss: 10.423059463500977 = 1.8835893869400024 + 1.0 * 8.539469718933105
Epoch 60, val loss: 1.8822704553604126
Epoch 70, training loss: 10.290164947509766 = 1.8716044425964355 + 1.0 * 8.418560981750488
Epoch 70, val loss: 1.8695775270462036
Epoch 80, training loss: 9.97972297668457 = 1.859062671661377 + 1.0 * 8.120659828186035
Epoch 80, val loss: 1.8565905094146729
Epoch 90, training loss: 9.734128952026367 = 1.8475373983383179 + 1.0 * 7.88659143447876
Epoch 90, val loss: 1.845301866531372
Epoch 100, training loss: 9.45673656463623 = 1.8383625745773315 + 1.0 * 7.618373870849609
Epoch 100, val loss: 1.8367621898651123
Epoch 110, training loss: 9.200782775878906 = 1.8312777280807495 + 1.0 * 7.369505405426025
Epoch 110, val loss: 1.8301162719726562
Epoch 120, training loss: 8.997060775756836 = 1.8253631591796875 + 1.0 * 7.171698093414307
Epoch 120, val loss: 1.8247164487838745
Epoch 130, training loss: 8.812021255493164 = 1.8201708793640137 + 1.0 * 6.991849899291992
Epoch 130, val loss: 1.8199611902236938
Epoch 140, training loss: 8.682577133178711 = 1.8149170875549316 + 1.0 * 6.867659568786621
Epoch 140, val loss: 1.8150889873504639
Epoch 150, training loss: 8.601537704467773 = 1.8089442253112793 + 1.0 * 6.792593479156494
Epoch 150, val loss: 1.809288501739502
Epoch 160, training loss: 8.540511131286621 = 1.8019734621047974 + 1.0 * 6.738537788391113
Epoch 160, val loss: 1.8026361465454102
Epoch 170, training loss: 8.488187789916992 = 1.7948991060256958 + 1.0 * 6.693288326263428
Epoch 170, val loss: 1.7959556579589844
Epoch 180, training loss: 8.443349838256836 = 1.787929654121399 + 1.0 * 6.655420303344727
Epoch 180, val loss: 1.789548635482788
Epoch 190, training loss: 8.402957916259766 = 1.7810454368591309 + 1.0 * 6.621912956237793
Epoch 190, val loss: 1.783246397972107
Epoch 200, training loss: 8.374357223510742 = 1.7739067077636719 + 1.0 * 6.6004509925842285
Epoch 200, val loss: 1.7766907215118408
Epoch 210, training loss: 8.340689659118652 = 1.7663298845291138 + 1.0 * 6.574359893798828
Epoch 210, val loss: 1.7701318264007568
Epoch 220, training loss: 8.313962936401367 = 1.7584214210510254 + 1.0 * 6.555541515350342
Epoch 220, val loss: 1.763238549232483
Epoch 230, training loss: 8.288342475891113 = 1.7500240802764893 + 1.0 * 6.538318634033203
Epoch 230, val loss: 1.755982518196106
Epoch 240, training loss: 8.263666152954102 = 1.7410216331481934 + 1.0 * 6.52264404296875
Epoch 240, val loss: 1.7483104467391968
Epoch 250, training loss: 8.23923110961914 = 1.7313281297683716 + 1.0 * 6.507903099060059
Epoch 250, val loss: 1.7401765584945679
Epoch 260, training loss: 8.217418670654297 = 1.7208528518676758 + 1.0 * 6.496565341949463
Epoch 260, val loss: 1.7314600944519043
Epoch 270, training loss: 8.192251205444336 = 1.709634780883789 + 1.0 * 6.482616424560547
Epoch 270, val loss: 1.722105860710144
Epoch 280, training loss: 8.168516159057617 = 1.6974632740020752 + 1.0 * 6.471052646636963
Epoch 280, val loss: 1.7119646072387695
Epoch 290, training loss: 8.145038604736328 = 1.6841726303100586 + 1.0 * 6.460866451263428
Epoch 290, val loss: 1.7009254693984985
Epoch 300, training loss: 8.12091064453125 = 1.6696367263793945 + 1.0 * 6.451274394989014
Epoch 300, val loss: 1.6888514757156372
Epoch 310, training loss: 8.098573684692383 = 1.65371572971344 + 1.0 * 6.444858074188232
Epoch 310, val loss: 1.675632357597351
Epoch 320, training loss: 8.07198429107666 = 1.6364203691482544 + 1.0 * 6.435564041137695
Epoch 320, val loss: 1.6613190174102783
Epoch 330, training loss: 8.046060562133789 = 1.6176823377609253 + 1.0 * 6.428378105163574
Epoch 330, val loss: 1.6457993984222412
Epoch 340, training loss: 8.018943786621094 = 1.5974465608596802 + 1.0 * 6.421496868133545
Epoch 340, val loss: 1.6291941404342651
Epoch 350, training loss: 7.991480350494385 = 1.575808048248291 + 1.0 * 6.415672302246094
Epoch 350, val loss: 1.6113547086715698
Epoch 360, training loss: 7.9621100425720215 = 1.5524624586105347 + 1.0 * 6.409647464752197
Epoch 360, val loss: 1.5922048091888428
Epoch 370, training loss: 7.931748390197754 = 1.5273246765136719 + 1.0 * 6.404423713684082
Epoch 370, val loss: 1.5716023445129395
Epoch 380, training loss: 7.900302886962891 = 1.5003693103790283 + 1.0 * 6.399933338165283
Epoch 380, val loss: 1.5495895147323608
Epoch 390, training loss: 7.8695878982543945 = 1.4719173908233643 + 1.0 * 6.397670745849609
Epoch 390, val loss: 1.5265028476715088
Epoch 400, training loss: 7.833830833435059 = 1.4425580501556396 + 1.0 * 6.39127254486084
Epoch 400, val loss: 1.502771258354187
Epoch 410, training loss: 7.799583435058594 = 1.4119904041290283 + 1.0 * 6.3875932693481445
Epoch 410, val loss: 1.4781708717346191
Epoch 420, training loss: 7.764113903045654 = 1.3804270029067993 + 1.0 * 6.3836870193481445
Epoch 420, val loss: 1.4529824256896973
Epoch 430, training loss: 7.7279887199401855 = 1.3479691743850708 + 1.0 * 6.380019664764404
Epoch 430, val loss: 1.4272527694702148
Epoch 440, training loss: 7.692500114440918 = 1.314820408821106 + 1.0 * 6.377679824829102
Epoch 440, val loss: 1.4011856317520142
Epoch 450, training loss: 7.658169269561768 = 1.2821389436721802 + 1.0 * 6.376030445098877
Epoch 450, val loss: 1.3755635023117065
Epoch 460, training loss: 7.621397018432617 = 1.2498432397842407 + 1.0 * 6.371553897857666
Epoch 460, val loss: 1.3505719900131226
Epoch 470, training loss: 7.5859055519104 = 1.2181113958358765 + 1.0 * 6.367794036865234
Epoch 470, val loss: 1.3261972665786743
Epoch 480, training loss: 7.551905155181885 = 1.186867356300354 + 1.0 * 6.36503791809082
Epoch 480, val loss: 1.3024448156356812
Epoch 490, training loss: 7.51836633682251 = 1.1562132835388184 + 1.0 * 6.362153053283691
Epoch 490, val loss: 1.2794502973556519
Epoch 500, training loss: 7.488940238952637 = 1.1264359951019287 + 1.0 * 6.362504482269287
Epoch 500, val loss: 1.2574795484542847
Epoch 510, training loss: 7.455623149871826 = 1.0980291366577148 + 1.0 * 6.357594013214111
Epoch 510, val loss: 1.2367991209030151
Epoch 520, training loss: 7.425607681274414 = 1.070725679397583 + 1.0 * 6.354881763458252
Epoch 520, val loss: 1.2171344757080078
Epoch 530, training loss: 7.398134231567383 = 1.0443652868270874 + 1.0 * 6.353768825531006
Epoch 530, val loss: 1.1984845399856567
Epoch 540, training loss: 7.371519565582275 = 1.0192123651504517 + 1.0 * 6.352307319641113
Epoch 540, val loss: 1.1809489727020264
Epoch 550, training loss: 7.342624664306641 = 0.9950101375579834 + 1.0 * 6.347614288330078
Epoch 550, val loss: 1.164436936378479
Epoch 560, training loss: 7.317500114440918 = 0.9717381000518799 + 1.0 * 6.345762252807617
Epoch 560, val loss: 1.1487433910369873
Epoch 570, training loss: 7.292535781860352 = 0.9492905139923096 + 1.0 * 6.343245029449463
Epoch 570, val loss: 1.1338598728179932
Epoch 580, training loss: 7.268654823303223 = 0.9275161623954773 + 1.0 * 6.34113883972168
Epoch 580, val loss: 1.119753360748291
Epoch 590, training loss: 7.250484943389893 = 0.9064363837242126 + 1.0 * 6.344048500061035
Epoch 590, val loss: 1.1063367128372192
Epoch 600, training loss: 7.223911762237549 = 0.8862772583961487 + 1.0 * 6.337634563446045
Epoch 600, val loss: 1.093673825263977
Epoch 610, training loss: 7.203676223754883 = 0.8667376041412354 + 1.0 * 6.336938381195068
Epoch 610, val loss: 1.0816882848739624
Epoch 620, training loss: 7.181661605834961 = 0.8478602170944214 + 1.0 * 6.33380126953125
Epoch 620, val loss: 1.0703531503677368
Epoch 630, training loss: 7.161422252655029 = 0.8295355439186096 + 1.0 * 6.3318867683410645
Epoch 630, val loss: 1.0595670938491821
Epoch 640, training loss: 7.144735336303711 = 0.8117628693580627 + 1.0 * 6.332972526550293
Epoch 640, val loss: 1.0493414402008057
Epoch 650, training loss: 7.123288154602051 = 0.7945708632469177 + 1.0 * 6.328717231750488
Epoch 650, val loss: 1.0396754741668701
Epoch 660, training loss: 7.104650020599365 = 0.7778695225715637 + 1.0 * 6.326780319213867
Epoch 660, val loss: 1.0304639339447021
Epoch 670, training loss: 7.086503982543945 = 0.7615785598754883 + 1.0 * 6.324925422668457
Epoch 670, val loss: 1.021689534187317
Epoch 680, training loss: 7.075885772705078 = 0.7456260323524475 + 1.0 * 6.330259799957275
Epoch 680, val loss: 1.0133421421051025
Epoch 690, training loss: 7.054560661315918 = 0.73033207654953 + 1.0 * 6.324228763580322
Epoch 690, val loss: 1.0054720640182495
Epoch 700, training loss: 7.0364274978637695 = 0.7154363989830017 + 1.0 * 6.320991039276123
Epoch 700, val loss: 0.9980149865150452
Epoch 710, training loss: 7.020378589630127 = 0.7008822560310364 + 1.0 * 6.319496154785156
Epoch 710, val loss: 0.9909215569496155
Epoch 720, training loss: 7.006590366363525 = 0.6866546869277954 + 1.0 * 6.3199357986450195
Epoch 720, val loss: 0.9842087626457214
Epoch 730, training loss: 6.9904985427856445 = 0.6728042960166931 + 1.0 * 6.317694187164307
Epoch 730, val loss: 0.9778798222541809
Epoch 740, training loss: 6.97457218170166 = 0.6592744588851929 + 1.0 * 6.315297603607178
Epoch 740, val loss: 0.9718884229660034
Epoch 750, training loss: 6.962823390960693 = 0.6460273861885071 + 1.0 * 6.316795825958252
Epoch 750, val loss: 0.9662044048309326
Epoch 760, training loss: 6.946792125701904 = 0.6332014799118042 + 1.0 * 6.3135905265808105
Epoch 760, val loss: 0.9608497619628906
Epoch 770, training loss: 6.932913303375244 = 0.6206199526786804 + 1.0 * 6.312293529510498
Epoch 770, val loss: 0.9558445811271667
Epoch 780, training loss: 6.918140888214111 = 0.608345091342926 + 1.0 * 6.30979585647583
Epoch 780, val loss: 0.9511388540267944
Epoch 790, training loss: 6.904964447021484 = 0.5962635278701782 + 1.0 * 6.308701038360596
Epoch 790, val loss: 0.9467089772224426
Epoch 800, training loss: 6.892212867736816 = 0.584392786026001 + 1.0 * 6.3078203201293945
Epoch 800, val loss: 0.9425175189971924
Epoch 810, training loss: 6.879622459411621 = 0.5727567076683044 + 1.0 * 6.306865692138672
Epoch 810, val loss: 0.9386116862297058
Epoch 820, training loss: 6.870780944824219 = 0.5613906979560852 + 1.0 * 6.309390068054199
Epoch 820, val loss: 0.934955894947052
Epoch 830, training loss: 6.8559675216674805 = 0.5502212047576904 + 1.0 * 6.305746078491211
Epoch 830, val loss: 0.9315295815467834
Epoch 840, training loss: 6.843028545379639 = 0.5392729640007019 + 1.0 * 6.303755760192871
Epoch 840, val loss: 0.9282925128936768
Epoch 850, training loss: 6.830995559692383 = 0.5284507870674133 + 1.0 * 6.302544593811035
Epoch 850, val loss: 0.9252989888191223
Epoch 860, training loss: 6.820443630218506 = 0.5177496075630188 + 1.0 * 6.302693843841553
Epoch 860, val loss: 0.9225012063980103
Epoch 870, training loss: 6.815622329711914 = 0.5072152614593506 + 1.0 * 6.308406829833984
Epoch 870, val loss: 0.9199008345603943
Epoch 880, training loss: 6.7986345291137695 = 0.4969736933708191 + 1.0 * 6.301661014556885
Epoch 880, val loss: 0.9174712300300598
Epoch 890, training loss: 6.786139488220215 = 0.4868592917919159 + 1.0 * 6.299280166625977
Epoch 890, val loss: 0.9151671528816223
Epoch 900, training loss: 6.774716377258301 = 0.47683343291282654 + 1.0 * 6.297883033752441
Epoch 900, val loss: 0.9130083918571472
Epoch 910, training loss: 6.763613700866699 = 0.46687018871307373 + 1.0 * 6.296743392944336
Epoch 910, val loss: 0.9109680652618408
Epoch 920, training loss: 6.75396203994751 = 0.45696666836738586 + 1.0 * 6.296995162963867
Epoch 920, val loss: 0.90903240442276
Epoch 930, training loss: 6.742714881896973 = 0.4471800923347473 + 1.0 * 6.295534610748291
Epoch 930, val loss: 0.907202422618866
Epoch 940, training loss: 6.733282566070557 = 0.43751290440559387 + 1.0 * 6.295769691467285
Epoch 940, val loss: 0.9054155945777893
Epoch 950, training loss: 6.721819877624512 = 0.42791929841041565 + 1.0 * 6.293900489807129
Epoch 950, val loss: 0.903710663318634
Epoch 960, training loss: 6.711054801940918 = 0.41834574937820435 + 1.0 * 6.292708873748779
Epoch 960, val loss: 0.9021013975143433
Epoch 970, training loss: 6.700798034667969 = 0.4087885618209839 + 1.0 * 6.292009353637695
Epoch 970, val loss: 0.9005440473556519
Epoch 980, training loss: 6.6951775550842285 = 0.39924249053001404 + 1.0 * 6.295935153961182
Epoch 980, val loss: 0.8990448117256165
Epoch 990, training loss: 6.683623313903809 = 0.3898441195487976 + 1.0 * 6.293779373168945
Epoch 990, val loss: 0.8975644707679749
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7296296296296296
0.808645229309436
The final CL Acc:0.74444, 0.02095, The final GNN Acc:0.81023, 0.00262
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13238])
remove edge: torch.Size([2, 7918])
updated graph: torch.Size([2, 10600])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 10.557866096496582 = 1.9610309600830078 + 1.0 * 8.596835136413574
Epoch 0, val loss: 1.954634428024292
Epoch 10, training loss: 10.55264663696289 = 1.9559123516082764 + 1.0 * 8.596734046936035
Epoch 10, val loss: 1.9497660398483276
Epoch 20, training loss: 10.546713829040527 = 1.950290322303772 + 1.0 * 8.596423149108887
Epoch 20, val loss: 1.944284439086914
Epoch 30, training loss: 10.538935661315918 = 1.9436490535736084 + 1.0 * 8.59528636932373
Epoch 30, val loss: 1.9376682043075562
Epoch 40, training loss: 10.526199340820312 = 1.9354243278503418 + 1.0 * 8.590775489807129
Epoch 40, val loss: 1.9293382167816162
Epoch 50, training loss: 10.499635696411133 = 1.925005555152893 + 1.0 * 8.574629783630371
Epoch 50, val loss: 1.9186749458312988
Epoch 60, training loss: 10.438594818115234 = 1.9122350215911865 + 1.0 * 8.526359558105469
Epoch 60, val loss: 1.9057759046554565
Epoch 70, training loss: 10.277921676635742 = 1.8981109857559204 + 1.0 * 8.379810333251953
Epoch 70, val loss: 1.8920449018478394
Epoch 80, training loss: 9.933186531066895 = 1.8823966979980469 + 1.0 * 8.050789833068848
Epoch 80, val loss: 1.8766950368881226
Epoch 90, training loss: 9.660799980163574 = 1.8667811155319214 + 1.0 * 7.794018745422363
Epoch 90, val loss: 1.8622099161148071
Epoch 100, training loss: 9.345226287841797 = 1.8552007675170898 + 1.0 * 7.490025043487549
Epoch 100, val loss: 1.8519781827926636
Epoch 110, training loss: 9.068049430847168 = 1.8464477062225342 + 1.0 * 7.221601486206055
Epoch 110, val loss: 1.8442480564117432
Epoch 120, training loss: 8.86624813079834 = 1.8377476930618286 + 1.0 * 7.028500556945801
Epoch 120, val loss: 1.8365336656570435
Epoch 130, training loss: 8.740177154541016 = 1.8287439346313477 + 1.0 * 6.91143274307251
Epoch 130, val loss: 1.8282688856124878
Epoch 140, training loss: 8.6605863571167 = 1.818697214126587 + 1.0 * 6.841888904571533
Epoch 140, val loss: 1.8191910982131958
Epoch 150, training loss: 8.609310150146484 = 1.8080958127975464 + 1.0 * 6.801214694976807
Epoch 150, val loss: 1.8097871541976929
Epoch 160, training loss: 8.571610450744629 = 1.7975494861602783 + 1.0 * 6.7740607261657715
Epoch 160, val loss: 1.8005602359771729
Epoch 170, training loss: 8.533792495727539 = 1.7874550819396973 + 1.0 * 6.746337413787842
Epoch 170, val loss: 1.7918959856033325
Epoch 180, training loss: 8.495892524719238 = 1.7779666185379028 + 1.0 * 6.717925548553467
Epoch 180, val loss: 1.7839564085006714
Epoch 190, training loss: 8.45914363861084 = 1.7690041065216064 + 1.0 * 6.6901397705078125
Epoch 190, val loss: 1.7765295505523682
Epoch 200, training loss: 8.424735069274902 = 1.7601115703582764 + 1.0 * 6.664623260498047
Epoch 200, val loss: 1.7692204713821411
Epoch 210, training loss: 8.39163589477539 = 1.7510623931884766 + 1.0 * 6.640573024749756
Epoch 210, val loss: 1.7617385387420654
Epoch 220, training loss: 8.358095169067383 = 1.741602897644043 + 1.0 * 6.61649227142334
Epoch 220, val loss: 1.7538983821868896
Epoch 230, training loss: 8.325505256652832 = 1.7317506074905396 + 1.0 * 6.593754291534424
Epoch 230, val loss: 1.7456694841384888
Epoch 240, training loss: 8.29443359375 = 1.721364140510559 + 1.0 * 6.573069095611572
Epoch 240, val loss: 1.7370426654815674
Epoch 250, training loss: 8.264636039733887 = 1.7102628946304321 + 1.0 * 6.554372787475586
Epoch 250, val loss: 1.7277814149856567
Epoch 260, training loss: 8.236283302307129 = 1.6981871128082275 + 1.0 * 6.538095951080322
Epoch 260, val loss: 1.7177188396453857
Epoch 270, training loss: 8.208300590515137 = 1.6849567890167236 + 1.0 * 6.523343563079834
Epoch 270, val loss: 1.7066855430603027
Epoch 280, training loss: 8.180448532104492 = 1.67042875289917 + 1.0 * 6.510019302368164
Epoch 280, val loss: 1.6945407390594482
Epoch 290, training loss: 8.151908874511719 = 1.6546012163162231 + 1.0 * 6.497307300567627
Epoch 290, val loss: 1.681372880935669
Epoch 300, training loss: 8.123750686645508 = 1.637516736984253 + 1.0 * 6.486234188079834
Epoch 300, val loss: 1.6671117544174194
Epoch 310, training loss: 8.094744682312012 = 1.618926763534546 + 1.0 * 6.475818157196045
Epoch 310, val loss: 1.6515785455703735
Epoch 320, training loss: 8.06602668762207 = 1.5987014770507812 + 1.0 * 6.467324733734131
Epoch 320, val loss: 1.6346410512924194
Epoch 330, training loss: 8.035721778869629 = 1.5769506692886353 + 1.0 * 6.458770751953125
Epoch 330, val loss: 1.6163684129714966
Epoch 340, training loss: 8.005233764648438 = 1.5535564422607422 + 1.0 * 6.451676845550537
Epoch 340, val loss: 1.596793293952942
Epoch 350, training loss: 7.9734578132629395 = 1.5285415649414062 + 1.0 * 6.444916248321533
Epoch 350, val loss: 1.575791835784912
Epoch 360, training loss: 7.942105770111084 = 1.5019851922988892 + 1.0 * 6.440120697021484
Epoch 360, val loss: 1.5535023212432861
Epoch 370, training loss: 7.908285140991211 = 1.4741337299346924 + 1.0 * 6.434151649475098
Epoch 370, val loss: 1.5303163528442383
Epoch 380, training loss: 7.873822212219238 = 1.4452126026153564 + 1.0 * 6.428609848022461
Epoch 380, val loss: 1.506136417388916
Epoch 390, training loss: 7.83949613571167 = 1.4151678085327148 + 1.0 * 6.424328327178955
Epoch 390, val loss: 1.481157898902893
Epoch 400, training loss: 7.803491592407227 = 1.3843518495559692 + 1.0 * 6.419139862060547
Epoch 400, val loss: 1.4555104970932007
Epoch 410, training loss: 7.769317150115967 = 1.3528355360031128 + 1.0 * 6.4164814949035645
Epoch 410, val loss: 1.4293367862701416
Epoch 420, training loss: 7.732604026794434 = 1.321009635925293 + 1.0 * 6.411594390869141
Epoch 420, val loss: 1.4031213521957397
Epoch 430, training loss: 7.696239471435547 = 1.2889972925186157 + 1.0 * 6.407242298126221
Epoch 430, val loss: 1.3766789436340332
Epoch 440, training loss: 7.659966468811035 = 1.2566707134246826 + 1.0 * 6.403295516967773
Epoch 440, val loss: 1.3500874042510986
Epoch 450, training loss: 7.626856327056885 = 1.2242461442947388 + 1.0 * 6.4026103019714355
Epoch 450, val loss: 1.32345449924469
Epoch 460, training loss: 7.589267730712891 = 1.1923004388809204 + 1.0 * 6.39696741104126
Epoch 460, val loss: 1.2971562147140503
Epoch 470, training loss: 7.553841590881348 = 1.1605678796768188 + 1.0 * 6.393273830413818
Epoch 470, val loss: 1.2711869478225708
Epoch 480, training loss: 7.519284248352051 = 1.1291472911834717 + 1.0 * 6.390137195587158
Epoch 480, val loss: 1.2455099821090698
Epoch 490, training loss: 7.486452579498291 = 1.0983362197875977 + 1.0 * 6.388116359710693
Epoch 490, val loss: 1.220381259918213
Epoch 500, training loss: 7.452930927276611 = 1.068613886833191 + 1.0 * 6.384316921234131
Epoch 500, val loss: 1.1961606740951538
Epoch 510, training loss: 7.420677661895752 = 1.0396243333816528 + 1.0 * 6.381053447723389
Epoch 510, val loss: 1.1726107597351074
Epoch 520, training loss: 7.389218807220459 = 1.0112272500991821 + 1.0 * 6.377991676330566
Epoch 520, val loss: 1.1495742797851562
Epoch 530, training loss: 7.359207630157471 = 0.9834094643592834 + 1.0 * 6.375798225402832
Epoch 530, val loss: 1.1270482540130615
Epoch 540, training loss: 7.333337783813477 = 0.9566572308540344 + 1.0 * 6.376680374145508
Epoch 540, val loss: 1.1053115129470825
Epoch 550, training loss: 7.301900863647461 = 0.9309936761856079 + 1.0 * 6.370907306671143
Epoch 550, val loss: 1.0846283435821533
Epoch 560, training loss: 7.273499488830566 = 0.9061203002929688 + 1.0 * 6.367379188537598
Epoch 560, val loss: 1.0647292137145996
Epoch 570, training loss: 7.246800422668457 = 0.8819819688796997 + 1.0 * 6.364818572998047
Epoch 570, val loss: 1.0454388856887817
Epoch 580, training loss: 7.2206950187683105 = 0.858540952205658 + 1.0 * 6.362154006958008
Epoch 580, val loss: 1.026739478111267
Epoch 590, training loss: 7.195949077606201 = 0.8357807397842407 + 1.0 * 6.36016845703125
Epoch 590, val loss: 1.0086593627929688
Epoch 600, training loss: 7.174059867858887 = 0.8139035701751709 + 1.0 * 6.360156536102295
Epoch 600, val loss: 0.991439938545227
Epoch 610, training loss: 7.14992094039917 = 0.7931352853775024 + 1.0 * 6.356785774230957
Epoch 610, val loss: 0.9751955270767212
Epoch 620, training loss: 7.12659215927124 = 0.7731381058692932 + 1.0 * 6.353454113006592
Epoch 620, val loss: 0.9597880244255066
Epoch 630, training loss: 7.1049981117248535 = 0.7537659406661987 + 1.0 * 6.351232051849365
Epoch 630, val loss: 0.9450072050094604
Epoch 640, training loss: 7.083866596221924 = 0.7349597811698914 + 1.0 * 6.348906993865967
Epoch 640, val loss: 0.930816113948822
Epoch 650, training loss: 7.063807487487793 = 0.716671884059906 + 1.0 * 6.347135543823242
Epoch 650, val loss: 0.9172347784042358
Epoch 660, training loss: 7.045248031616211 = 0.6989900469779968 + 1.0 * 6.346258163452148
Epoch 660, val loss: 0.9043063521385193
Epoch 670, training loss: 7.026947021484375 = 0.6820395588874817 + 1.0 * 6.344907283782959
Epoch 670, val loss: 0.8921738862991333
Epoch 680, training loss: 7.007669448852539 = 0.6655896306037903 + 1.0 * 6.3420796394348145
Epoch 680, val loss: 0.8806993961334229
Epoch 690, training loss: 6.988889217376709 = 0.6495260000228882 + 1.0 * 6.339363098144531
Epoch 690, val loss: 0.8697446584701538
Epoch 700, training loss: 6.971221923828125 = 0.6337730288505554 + 1.0 * 6.337449073791504
Epoch 700, val loss: 0.8592849969863892
Epoch 710, training loss: 6.961881160736084 = 0.6183362603187561 + 1.0 * 6.343544960021973
Epoch 710, val loss: 0.8492894768714905
Epoch 720, training loss: 6.939741611480713 = 0.6033722162246704 + 1.0 * 6.336369514465332
Epoch 720, val loss: 0.8398128747940063
Epoch 730, training loss: 6.9215006828308105 = 0.5887556672096252 + 1.0 * 6.33274507522583
Epoch 730, val loss: 0.8308963775634766
Epoch 740, training loss: 6.910717010498047 = 0.5744539499282837 + 1.0 * 6.336263179779053
Epoch 740, val loss: 0.8224213123321533
Epoch 750, training loss: 6.8938307762146 = 0.5605369806289673 + 1.0 * 6.333293914794922
Epoch 750, val loss: 0.8144177198410034
Epoch 760, training loss: 6.876612663269043 = 0.5470298528671265 + 1.0 * 6.329582691192627
Epoch 760, val loss: 0.8069478869438171
Epoch 770, training loss: 6.860278129577637 = 0.5337862968444824 + 1.0 * 6.326491832733154
Epoch 770, val loss: 0.7999189496040344
Epoch 780, training loss: 6.845183849334717 = 0.5207902193069458 + 1.0 * 6.3243937492370605
Epoch 780, val loss: 0.7932633757591248
Epoch 790, training loss: 6.830812454223633 = 0.5080215930938721 + 1.0 * 6.322790622711182
Epoch 790, val loss: 0.7870084047317505
Epoch 800, training loss: 6.8171281814575195 = 0.49549439549446106 + 1.0 * 6.321633815765381
Epoch 800, val loss: 0.7811635732650757
Epoch 810, training loss: 6.810876369476318 = 0.4832335412502289 + 1.0 * 6.327642917633057
Epoch 810, val loss: 0.7757080793380737
Epoch 820, training loss: 6.7921929359436035 = 0.4712761640548706 + 1.0 * 6.320916652679443
Epoch 820, val loss: 0.7706859111785889
Epoch 830, training loss: 6.778059005737305 = 0.45966392755508423 + 1.0 * 6.318395137786865
Epoch 830, val loss: 0.7661318778991699
Epoch 840, training loss: 6.76495885848999 = 0.44831177592277527 + 1.0 * 6.316647052764893
Epoch 840, val loss: 0.7619292736053467
Epoch 850, training loss: 6.755048751831055 = 0.4372071623802185 + 1.0 * 6.317841529846191
Epoch 850, val loss: 0.7580534219741821
Epoch 860, training loss: 6.740364074707031 = 0.4263552725315094 + 1.0 * 6.314008712768555
Epoch 860, val loss: 0.7545168399810791
Epoch 870, training loss: 6.728705406188965 = 0.4157550036907196 + 1.0 * 6.312950611114502
Epoch 870, val loss: 0.7513203024864197
Epoch 880, training loss: 6.7176432609558105 = 0.4053608179092407 + 1.0 * 6.312282562255859
Epoch 880, val loss: 0.7484034895896912
Epoch 890, training loss: 6.707267761230469 = 0.3951869308948517 + 1.0 * 6.3120808601379395
Epoch 890, val loss: 0.7457497119903564
Epoch 900, training loss: 6.695243835449219 = 0.38525280356407166 + 1.0 * 6.309990882873535
Epoch 900, val loss: 0.7433775067329407
Epoch 910, training loss: 6.684412956237793 = 0.3754637539386749 + 1.0 * 6.308948993682861
Epoch 910, val loss: 0.7412551045417786
Epoch 920, training loss: 6.6773271560668945 = 0.36588600277900696 + 1.0 * 6.311440944671631
Epoch 920, val loss: 0.739298939704895
Epoch 930, training loss: 6.663663387298584 = 0.3564836084842682 + 1.0 * 6.307179927825928
Epoch 930, val loss: 0.7375898361206055
Epoch 940, training loss: 6.6516547203063965 = 0.3472706377506256 + 1.0 * 6.304384231567383
Epoch 940, val loss: 0.7361135482788086
Epoch 950, training loss: 6.642094612121582 = 0.33819231390953064 + 1.0 * 6.3039021492004395
Epoch 950, val loss: 0.7348065972328186
Epoch 960, training loss: 6.64110803604126 = 0.3292659521102905 + 1.0 * 6.31184196472168
Epoch 960, val loss: 0.7336417436599731
Epoch 970, training loss: 6.622467994689941 = 0.320514440536499 + 1.0 * 6.3019537925720215
Epoch 970, val loss: 0.7326549291610718
Epoch 980, training loss: 6.613076210021973 = 0.31192293763160706 + 1.0 * 6.301153182983398
Epoch 980, val loss: 0.7318738102912903
Epoch 990, training loss: 6.603178977966309 = 0.3034539520740509 + 1.0 * 6.29972505569458
Epoch 990, val loss: 0.731245219707489
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8
0.8386926726410122
=== training gcn model ===
Epoch 0, training loss: 10.547405242919922 = 1.9505624771118164 + 1.0 * 8.596842765808105
Epoch 0, val loss: 1.9484575986862183
Epoch 10, training loss: 10.54223346710205 = 1.9454585313796997 + 1.0 * 8.59677505493164
Epoch 10, val loss: 1.9434850215911865
Epoch 20, training loss: 10.53642749786377 = 1.9398744106292725 + 1.0 * 8.596552848815918
Epoch 20, val loss: 1.937936782836914
Epoch 30, training loss: 10.529026985168457 = 1.9332141876220703 + 1.0 * 8.595812797546387
Epoch 30, val loss: 1.93120276927948
Epoch 40, training loss: 10.517809867858887 = 1.9248493909835815 + 1.0 * 8.592960357666016
Epoch 40, val loss: 1.9225990772247314
Epoch 50, training loss: 10.495551109313965 = 1.914014458656311 + 1.0 * 8.581536293029785
Epoch 50, val loss: 1.9113078117370605
Epoch 60, training loss: 10.440637588500977 = 1.9002023935317993 + 1.0 * 8.540434837341309
Epoch 60, val loss: 1.8970284461975098
Epoch 70, training loss: 10.276870727539062 = 1.8847126960754395 + 1.0 * 8.392157554626465
Epoch 70, val loss: 1.881758689880371
Epoch 80, training loss: 9.865806579589844 = 1.868507742881775 + 1.0 * 7.997298717498779
Epoch 80, val loss: 1.8661956787109375
Epoch 90, training loss: 9.466590881347656 = 1.855048418045044 + 1.0 * 7.611542701721191
Epoch 90, val loss: 1.8538463115692139
Epoch 100, training loss: 9.156569480895996 = 1.8456157445907593 + 1.0 * 7.3109540939331055
Epoch 100, val loss: 1.8454533815383911
Epoch 110, training loss: 8.915641784667969 = 1.8385989665985107 + 1.0 * 7.077043056488037
Epoch 110, val loss: 1.8390967845916748
Epoch 120, training loss: 8.751890182495117 = 1.8322468996047974 + 1.0 * 6.919643402099609
Epoch 120, val loss: 1.8333462476730347
Epoch 130, training loss: 8.657781600952148 = 1.8253222703933716 + 1.0 * 6.832459449768066
Epoch 130, val loss: 1.8269078731536865
Epoch 140, training loss: 8.596019744873047 = 1.816915512084961 + 1.0 * 6.779104709625244
Epoch 140, val loss: 1.8191554546356201
Epoch 150, training loss: 8.547465324401855 = 1.8076344728469849 + 1.0 * 6.739830493927002
Epoch 150, val loss: 1.8106826543807983
Epoch 160, training loss: 8.501046180725098 = 1.7984488010406494 + 1.0 * 6.702597141265869
Epoch 160, val loss: 1.8024625778198242
Epoch 170, training loss: 8.453039169311523 = 1.7899247407913208 + 1.0 * 6.663114547729492
Epoch 170, val loss: 1.794971227645874
Epoch 180, training loss: 8.405035018920898 = 1.7820948362350464 + 1.0 * 6.6229400634765625
Epoch 180, val loss: 1.788141131401062
Epoch 190, training loss: 8.361607551574707 = 1.7745380401611328 + 1.0 * 6.587069511413574
Epoch 190, val loss: 1.7815498113632202
Epoch 200, training loss: 8.323987007141113 = 1.766783356666565 + 1.0 * 6.55720329284668
Epoch 200, val loss: 1.7748081684112549
Epoch 210, training loss: 8.292606353759766 = 1.7585108280181885 + 1.0 * 6.534095764160156
Epoch 210, val loss: 1.76766037940979
Epoch 220, training loss: 8.266056060791016 = 1.7495485544204712 + 1.0 * 6.516507148742676
Epoch 220, val loss: 1.7600057125091553
Epoch 230, training loss: 8.241498947143555 = 1.7399230003356934 + 1.0 * 6.5015764236450195
Epoch 230, val loss: 1.7518584728240967
Epoch 240, training loss: 8.218497276306152 = 1.7296133041381836 + 1.0 * 6.488883972167969
Epoch 240, val loss: 1.7431881427764893
Epoch 250, training loss: 8.19581127166748 = 1.7184394598007202 + 1.0 * 6.477372169494629
Epoch 250, val loss: 1.7338371276855469
Epoch 260, training loss: 8.173075675964355 = 1.7062162160873413 + 1.0 * 6.466859340667725
Epoch 260, val loss: 1.7236204147338867
Epoch 270, training loss: 8.151577949523926 = 1.6928160190582275 + 1.0 * 6.458762168884277
Epoch 270, val loss: 1.7124638557434082
Epoch 280, training loss: 8.128045082092285 = 1.678294062614441 + 1.0 * 6.449750900268555
Epoch 280, val loss: 1.7004270553588867
Epoch 290, training loss: 8.104532241821289 = 1.6624417304992676 + 1.0 * 6.4420905113220215
Epoch 290, val loss: 1.687273383140564
Epoch 300, training loss: 8.080167770385742 = 1.6450855731964111 + 1.0 * 6.43508243560791
Epoch 300, val loss: 1.6728904247283936
Epoch 310, training loss: 8.054523468017578 = 1.6260809898376465 + 1.0 * 6.428442478179932
Epoch 310, val loss: 1.6571450233459473
Epoch 320, training loss: 8.02754020690918 = 1.6053662300109863 + 1.0 * 6.422173976898193
Epoch 320, val loss: 1.6399694681167603
Epoch 330, training loss: 8.00249195098877 = 1.5829733610153198 + 1.0 * 6.41951847076416
Epoch 330, val loss: 1.6214779615402222
Epoch 340, training loss: 7.971297264099121 = 1.5595976114273071 + 1.0 * 6.4116997718811035
Epoch 340, val loss: 1.6022957563400269
Epoch 350, training loss: 7.941588878631592 = 1.5350980758666992 + 1.0 * 6.406490802764893
Epoch 350, val loss: 1.582227110862732
Epoch 360, training loss: 7.910991668701172 = 1.5094044208526611 + 1.0 * 6.401587009429932
Epoch 360, val loss: 1.5611886978149414
Epoch 370, training loss: 7.879931449890137 = 1.482582926750183 + 1.0 * 6.397348403930664
Epoch 370, val loss: 1.5392405986785889
Epoch 380, training loss: 7.8497843742370605 = 1.4548360109329224 + 1.0 * 6.394948482513428
Epoch 380, val loss: 1.5167231559753418
Epoch 390, training loss: 7.817084789276123 = 1.4270635843276978 + 1.0 * 6.390021324157715
Epoch 390, val loss: 1.4944658279418945
Epoch 400, training loss: 7.785738945007324 = 1.3992891311645508 + 1.0 * 6.386449813842773
Epoch 400, val loss: 1.4723992347717285
Epoch 410, training loss: 7.754532337188721 = 1.3714590072631836 + 1.0 * 6.383073329925537
Epoch 410, val loss: 1.4504283666610718
Epoch 420, training loss: 7.723836898803711 = 1.3436830043792725 + 1.0 * 6.380153656005859
Epoch 420, val loss: 1.4286909103393555
Epoch 430, training loss: 7.695313930511475 = 1.316362738609314 + 1.0 * 6.378951072692871
Epoch 430, val loss: 1.4076422452926636
Epoch 440, training loss: 7.664443492889404 = 1.289753794670105 + 1.0 * 6.37468957901001
Epoch 440, val loss: 1.387271523475647
Epoch 450, training loss: 7.634932518005371 = 1.2635382413864136 + 1.0 * 6.371394157409668
Epoch 450, val loss: 1.3673651218414307
Epoch 460, training loss: 7.606437683105469 = 1.2376248836517334 + 1.0 * 6.3688130378723145
Epoch 460, val loss: 1.3477686643600464
Epoch 470, training loss: 7.578644752502441 = 1.2121957540512085 + 1.0 * 6.366448879241943
Epoch 470, val loss: 1.3286601305007935
Epoch 480, training loss: 7.5514020919799805 = 1.1874407529830933 + 1.0 * 6.363961219787598
Epoch 480, val loss: 1.31011962890625
Epoch 490, training loss: 7.524418354034424 = 1.1630312204360962 + 1.0 * 6.361387252807617
Epoch 490, val loss: 1.2918254137039185
Epoch 500, training loss: 7.497949600219727 = 1.1388624906539917 + 1.0 * 6.359086990356445
Epoch 500, val loss: 1.2737020254135132
Epoch 510, training loss: 7.471859931945801 = 1.115075707435608 + 1.0 * 6.356784343719482
Epoch 510, val loss: 1.2557833194732666
Epoch 520, training loss: 7.445900917053223 = 1.0916887521743774 + 1.0 * 6.354212284088135
Epoch 520, val loss: 1.2381643056869507
Epoch 530, training loss: 7.420544624328613 = 1.0685781240463257 + 1.0 * 6.351966381072998
Epoch 530, val loss: 1.22066330909729
Epoch 540, training loss: 7.396247386932373 = 1.045779824256897 + 1.0 * 6.350467681884766
Epoch 540, val loss: 1.2033040523529053
Epoch 550, training loss: 7.371435165405273 = 1.023470163345337 + 1.0 * 6.347964763641357
Epoch 550, val loss: 1.1862397193908691
Epoch 560, training loss: 7.346704483032227 = 1.001518964767456 + 1.0 * 6.345185279846191
Epoch 560, val loss: 1.1694533824920654
Epoch 570, training loss: 7.325836658477783 = 0.9798700213432312 + 1.0 * 6.345966815948486
Epoch 570, val loss: 1.1528668403625488
Epoch 580, training loss: 7.300412654876709 = 0.9587846994400024 + 1.0 * 6.341628074645996
Epoch 580, val loss: 1.1366474628448486
Epoch 590, training loss: 7.276734352111816 = 0.9380118250846863 + 1.0 * 6.3387227058410645
Epoch 590, val loss: 1.1207486391067505
Epoch 600, training loss: 7.254389762878418 = 0.9175006151199341 + 1.0 * 6.336889266967773
Epoch 600, val loss: 1.105034589767456
Epoch 610, training loss: 7.232651233673096 = 0.8973298668861389 + 1.0 * 6.335321426391602
Epoch 610, val loss: 1.0895525217056274
Epoch 620, training loss: 7.211251735687256 = 0.8775792717933655 + 1.0 * 6.333672523498535
Epoch 620, val loss: 1.074489712715149
Epoch 630, training loss: 7.190683364868164 = 0.8581807613372803 + 1.0 * 6.332502841949463
Epoch 630, val loss: 1.0597583055496216
Epoch 640, training loss: 7.168343544006348 = 0.8391591906547546 + 1.0 * 6.329184532165527
Epoch 640, val loss: 1.0453979969024658
Epoch 650, training loss: 7.1491217613220215 = 0.8204416036605835 + 1.0 * 6.328680038452148
Epoch 650, val loss: 1.0313900709152222
Epoch 660, training loss: 7.1304030418396 = 0.8022357821464539 + 1.0 * 6.32816743850708
Epoch 660, val loss: 1.0178247690200806
Epoch 670, training loss: 7.1092119216918945 = 0.7844722867012024 + 1.0 * 6.324739456176758
Epoch 670, val loss: 1.0048540830612183
Epoch 680, training loss: 7.089594841003418 = 0.7670885324478149 + 1.0 * 6.322506427764893
Epoch 680, val loss: 0.9922894239425659
Epoch 690, training loss: 7.070770740509033 = 0.7500277757644653 + 1.0 * 6.320743083953857
Epoch 690, val loss: 0.9800878167152405
Epoch 700, training loss: 7.056272983551025 = 0.7333135008811951 + 1.0 * 6.3229594230651855
Epoch 700, val loss: 0.9683040976524353
Epoch 710, training loss: 7.035494327545166 = 0.7170951962471008 + 1.0 * 6.318398952484131
Epoch 710, val loss: 0.9570299983024597
Epoch 720, training loss: 7.01918888092041 = 0.7013102769851685 + 1.0 * 6.317878723144531
Epoch 720, val loss: 0.9463146328926086
Epoch 730, training loss: 7.001242160797119 = 0.6859669089317322 + 1.0 * 6.315275192260742
Epoch 730, val loss: 0.9360920786857605
Epoch 740, training loss: 6.984629154205322 = 0.6709765195846558 + 1.0 * 6.313652515411377
Epoch 740, val loss: 0.9263283610343933
Epoch 750, training loss: 6.9749650955200195 = 0.6563534736633301 + 1.0 * 6.3186116218566895
Epoch 750, val loss: 0.9169741868972778
Epoch 760, training loss: 6.957162857055664 = 0.6422238945960999 + 1.0 * 6.314939022064209
Epoch 760, val loss: 0.9082026481628418
Epoch 770, training loss: 6.939286708831787 = 0.6285843849182129 + 1.0 * 6.310702323913574
Epoch 770, val loss: 0.8999742269515991
Epoch 780, training loss: 6.92391300201416 = 0.61525958776474 + 1.0 * 6.308653354644775
Epoch 780, val loss: 0.8921176791191101
Epoch 790, training loss: 6.909547805786133 = 0.6021955013275146 + 1.0 * 6.307352542877197
Epoch 790, val loss: 0.8846020102500916
Epoch 800, training loss: 6.896063804626465 = 0.5893876552581787 + 1.0 * 6.306675910949707
Epoch 800, val loss: 0.8774188756942749
Epoch 810, training loss: 6.883008003234863 = 0.5768947005271912 + 1.0 * 6.306113243103027
Epoch 810, val loss: 0.8705834150314331
Epoch 820, training loss: 6.868828773498535 = 0.5647401809692383 + 1.0 * 6.304088592529297
Epoch 820, val loss: 0.864141583442688
Epoch 830, training loss: 6.855625152587891 = 0.5528210401535034 + 1.0 * 6.302803993225098
Epoch 830, val loss: 0.8580099940299988
Epoch 840, training loss: 6.845435619354248 = 0.5410958528518677 + 1.0 * 6.30433988571167
Epoch 840, val loss: 0.8521493673324585
Epoch 850, training loss: 6.835018634796143 = 0.5296958088874817 + 1.0 * 6.305322647094727
Epoch 850, val loss: 0.8465495705604553
Epoch 860, training loss: 6.81951904296875 = 0.5185129046440125 + 1.0 * 6.301006317138672
Epoch 860, val loss: 0.8412795066833496
Epoch 870, training loss: 6.806484222412109 = 0.5075018405914307 + 1.0 * 6.2989821434021
Epoch 870, val loss: 0.8362039923667908
Epoch 880, training loss: 6.79441499710083 = 0.49662235379219055 + 1.0 * 6.297792434692383
Epoch 880, val loss: 0.8312975168228149
Epoch 890, training loss: 6.782552242279053 = 0.48585739731788635 + 1.0 * 6.296694755554199
Epoch 890, val loss: 0.8265570402145386
Epoch 900, training loss: 6.777707099914551 = 0.4751947820186615 + 1.0 * 6.302512168884277
Epoch 900, val loss: 0.8219498991966248
Epoch 910, training loss: 6.760674476623535 = 0.4647268056869507 + 1.0 * 6.295947551727295
Epoch 910, val loss: 0.8175000548362732
Epoch 920, training loss: 6.748641014099121 = 0.45439863204956055 + 1.0 * 6.2942423820495605
Epoch 920, val loss: 0.8132187128067017
Epoch 930, training loss: 6.737441062927246 = 0.4441570043563843 + 1.0 * 6.293283939361572
Epoch 930, val loss: 0.8090253472328186
Epoch 940, training loss: 6.731731414794922 = 0.4339931011199951 + 1.0 * 6.297738075256348
Epoch 940, val loss: 0.8048833608627319
Epoch 950, training loss: 6.7170891761779785 = 0.42394310235977173 + 1.0 * 6.293146133422852
Epoch 950, val loss: 0.8008520603179932
Epoch 960, training loss: 6.7053608894348145 = 0.41398927569389343 + 1.0 * 6.291371822357178
Epoch 960, val loss: 0.7969231605529785
Epoch 970, training loss: 6.694286823272705 = 0.4040994346141815 + 1.0 * 6.290187358856201
Epoch 970, val loss: 0.7930378317832947
Epoch 980, training loss: 6.684784412384033 = 0.39426150918006897 + 1.0 * 6.290523052215576
Epoch 980, val loss: 0.789185643196106
Epoch 990, training loss: 6.672850608825684 = 0.38449230790138245 + 1.0 * 6.288358211517334
Epoch 990, val loss: 0.7853772044181824
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.825925925925926
0.8323668950975225
=== training gcn model ===
Epoch 0, training loss: 10.5414457321167 = 1.9445960521697998 + 1.0 * 8.59684944152832
Epoch 0, val loss: 1.9448459148406982
Epoch 10, training loss: 10.536495208740234 = 1.9397166967391968 + 1.0 * 8.596778869628906
Epoch 10, val loss: 1.9403157234191895
Epoch 20, training loss: 10.53097915649414 = 1.9344348907470703 + 1.0 * 8.59654426574707
Epoch 20, val loss: 1.9353011846542358
Epoch 30, training loss: 10.523879051208496 = 1.9282130002975464 + 1.0 * 8.59566593170166
Epoch 30, val loss: 1.9292806386947632
Epoch 40, training loss: 10.512407302856445 = 1.9204351902008057 + 1.0 * 8.591972351074219
Epoch 40, val loss: 1.921645998954773
Epoch 50, training loss: 10.487590789794922 = 1.9104536771774292 + 1.0 * 8.577136993408203
Epoch 50, val loss: 1.9117562770843506
Epoch 60, training loss: 10.424778938293457 = 1.8980839252471924 + 1.0 * 8.526695251464844
Epoch 60, val loss: 1.899632215499878
Epoch 70, training loss: 10.239935874938965 = 1.884438395500183 + 1.0 * 8.355497360229492
Epoch 70, val loss: 1.8866297006607056
Epoch 80, training loss: 9.852075576782227 = 1.8697680234909058 + 1.0 * 7.9823079109191895
Epoch 80, val loss: 1.8727613687515259
Epoch 90, training loss: 9.645817756652832 = 1.857327938079834 + 1.0 * 7.788489818572998
Epoch 90, val loss: 1.8610256910324097
Epoch 100, training loss: 9.325459480285645 = 1.847842812538147 + 1.0 * 7.477616786956787
Epoch 100, val loss: 1.8518978357315063
Epoch 110, training loss: 9.026225090026855 = 1.8406691551208496 + 1.0 * 7.185555934906006
Epoch 110, val loss: 1.8447773456573486
Epoch 120, training loss: 8.806563377380371 = 1.8348286151885986 + 1.0 * 6.971735000610352
Epoch 120, val loss: 1.8389619588851929
Epoch 130, training loss: 8.675572395324707 = 1.828275442123413 + 1.0 * 6.847296714782715
Epoch 130, val loss: 1.8326575756072998
Epoch 140, training loss: 8.606034278869629 = 1.8196258544921875 + 1.0 * 6.786408424377441
Epoch 140, val loss: 1.8246707916259766
Epoch 150, training loss: 8.552684783935547 = 1.809526801109314 + 1.0 * 6.743158340454102
Epoch 150, val loss: 1.8156110048294067
Epoch 160, training loss: 8.499030113220215 = 1.7995437383651733 + 1.0 * 6.699486255645752
Epoch 160, val loss: 1.806744933128357
Epoch 170, training loss: 8.4512357711792 = 1.7902926206588745 + 1.0 * 6.660943508148193
Epoch 170, val loss: 1.798580288887024
Epoch 180, training loss: 8.406301498413086 = 1.7817392349243164 + 1.0 * 6.6245622634887695
Epoch 180, val loss: 1.7908557653427124
Epoch 190, training loss: 8.370156288146973 = 1.7731525897979736 + 1.0 * 6.597003936767578
Epoch 190, val loss: 1.7832238674163818
Epoch 200, training loss: 8.33940315246582 = 1.7642488479614258 + 1.0 * 6.575154781341553
Epoch 200, val loss: 1.77535879611969
Epoch 210, training loss: 8.313529014587402 = 1.754908561706543 + 1.0 * 6.558620452880859
Epoch 210, val loss: 1.7672022581100464
Epoch 220, training loss: 8.286432266235352 = 1.7452104091644287 + 1.0 * 6.541221618652344
Epoch 220, val loss: 1.758745551109314
Epoch 230, training loss: 8.261468887329102 = 1.7349575757980347 + 1.0 * 6.526511192321777
Epoch 230, val loss: 1.749842882156372
Epoch 240, training loss: 8.2391939163208 = 1.7240021228790283 + 1.0 * 6.515192031860352
Epoch 240, val loss: 1.740365743637085
Epoch 250, training loss: 8.21362018585205 = 1.7123141288757324 + 1.0 * 6.501306056976318
Epoch 250, val loss: 1.7303138971328735
Epoch 260, training loss: 8.189422607421875 = 1.6996939182281494 + 1.0 * 6.4897284507751465
Epoch 260, val loss: 1.7195276021957397
Epoch 270, training loss: 8.164640426635742 = 1.685973882675171 + 1.0 * 6.47866678237915
Epoch 270, val loss: 1.7078577280044556
Epoch 280, training loss: 8.139065742492676 = 1.6710004806518555 + 1.0 * 6.46806526184082
Epoch 280, val loss: 1.6951543092727661
Epoch 290, training loss: 8.117399215698242 = 1.654701590538025 + 1.0 * 6.462697982788086
Epoch 290, val loss: 1.68141770362854
Epoch 300, training loss: 8.086261749267578 = 1.6373298168182373 + 1.0 * 6.448931694030762
Epoch 300, val loss: 1.666838526725769
Epoch 310, training loss: 8.058935165405273 = 1.6186935901641846 + 1.0 * 6.440241813659668
Epoch 310, val loss: 1.6512084007263184
Epoch 320, training loss: 8.030621528625488 = 1.5986374616622925 + 1.0 * 6.431983947753906
Epoch 320, val loss: 1.6343631744384766
Epoch 330, training loss: 8.00110149383545 = 1.5770941972732544 + 1.0 * 6.424007415771484
Epoch 330, val loss: 1.6162346601486206
Epoch 340, training loss: 7.972982883453369 = 1.554141640663147 + 1.0 * 6.418841361999512
Epoch 340, val loss: 1.5969692468643188
Epoch 350, training loss: 7.941558361053467 = 1.5303858518600464 + 1.0 * 6.411172389984131
Epoch 350, val loss: 1.577150583267212
Epoch 360, training loss: 7.910023212432861 = 1.505686640739441 + 1.0 * 6.404336452484131
Epoch 360, val loss: 1.5565017461776733
Epoch 370, training loss: 7.878637790679932 = 1.480064034461975 + 1.0 * 6.398573875427246
Epoch 370, val loss: 1.5351160764694214
Epoch 380, training loss: 7.848155975341797 = 1.4536564350128174 + 1.0 * 6.3944993019104
Epoch 380, val loss: 1.5131301879882812
Epoch 390, training loss: 7.816774845123291 = 1.4270092248916626 + 1.0 * 6.389765739440918
Epoch 390, val loss: 1.490990161895752
Epoch 400, training loss: 7.7843451499938965 = 1.4001280069351196 + 1.0 * 6.384217262268066
Epoch 400, val loss: 1.4687588214874268
Epoch 410, training loss: 7.75266170501709 = 1.373035192489624 + 1.0 * 6.379626750946045
Epoch 410, val loss: 1.4464370012283325
Epoch 420, training loss: 7.723331928253174 = 1.3457673788070679 + 1.0 * 6.377564430236816
Epoch 420, val loss: 1.4240778684616089
Epoch 430, training loss: 7.690922737121582 = 1.3187142610549927 + 1.0 * 6.372208595275879
Epoch 430, val loss: 1.4019392728805542
Epoch 440, training loss: 7.660858631134033 = 1.2917526960372925 + 1.0 * 6.369105815887451
Epoch 440, val loss: 1.3799200057983398
Epoch 450, training loss: 7.631172180175781 = 1.26483952999115 + 1.0 * 6.366332530975342
Epoch 450, val loss: 1.3580176830291748
Epoch 460, training loss: 7.601572036743164 = 1.2382303476333618 + 1.0 * 6.363341808319092
Epoch 460, val loss: 1.3364686965942383
Epoch 470, training loss: 7.571577072143555 = 1.2119396924972534 + 1.0 * 6.359637260437012
Epoch 470, val loss: 1.3152058124542236
Epoch 480, training loss: 7.5426812171936035 = 1.1857587099075317 + 1.0 * 6.356922626495361
Epoch 480, val loss: 1.2940566539764404
Epoch 490, training loss: 7.5138139724731445 = 1.1596765518188477 + 1.0 * 6.354137420654297
Epoch 490, val loss: 1.2730841636657715
Epoch 500, training loss: 7.485742568969727 = 1.1338603496551514 + 1.0 * 6.351882457733154
Epoch 500, val loss: 1.2523398399353027
Epoch 510, training loss: 7.460092544555664 = 1.1084506511688232 + 1.0 * 6.351641654968262
Epoch 510, val loss: 1.2319422960281372
Epoch 520, training loss: 7.431623935699463 = 1.0836138725280762 + 1.0 * 6.348010063171387
Epoch 520, val loss: 1.211995005607605
Epoch 530, training loss: 7.403659343719482 = 1.0591448545455933 + 1.0 * 6.3445143699646
Epoch 530, val loss: 1.1923656463623047
Epoch 540, training loss: 7.377178192138672 = 1.0349572896957397 + 1.0 * 6.342220783233643
Epoch 540, val loss: 1.172932744026184
Epoch 550, training loss: 7.350804328918457 = 1.0109663009643555 + 1.0 * 6.339838027954102
Epoch 550, val loss: 1.1535969972610474
Epoch 560, training loss: 7.32624626159668 = 0.9871989488601685 + 1.0 * 6.339047431945801
Epoch 560, val loss: 1.1344250440597534
Epoch 570, training loss: 7.302122116088867 = 0.964042603969574 + 1.0 * 6.338079452514648
Epoch 570, val loss: 1.1157076358795166
Epoch 580, training loss: 7.276287078857422 = 0.9416292309761047 + 1.0 * 6.334657669067383
Epoch 580, val loss: 1.0975276231765747
Epoch 590, training loss: 7.252069473266602 = 0.9196761250495911 + 1.0 * 6.332393169403076
Epoch 590, val loss: 1.0797176361083984
Epoch 600, training loss: 7.22849178314209 = 0.8980646133422852 + 1.0 * 6.330427169799805
Epoch 600, val loss: 1.062163233757019
Epoch 610, training loss: 7.205329895019531 = 0.8767796754837036 + 1.0 * 6.328550338745117
Epoch 610, val loss: 1.0448297262191772
Epoch 620, training loss: 7.1921491622924805 = 0.8558915853500366 + 1.0 * 6.336257457733154
Epoch 620, val loss: 1.0277975797653198
Epoch 630, training loss: 7.1648383140563965 = 0.8358436226844788 + 1.0 * 6.3289947509765625
Epoch 630, val loss: 1.0113924741744995
Epoch 640, training loss: 7.140951633453369 = 0.8165332674980164 + 1.0 * 6.324418544769287
Epoch 640, val loss: 0.9955929517745972
Epoch 650, training loss: 7.120183944702148 = 0.7977930307388306 + 1.0 * 6.322391033172607
Epoch 650, val loss: 0.980338454246521
Epoch 660, training loss: 7.10041618347168 = 0.7795917987823486 + 1.0 * 6.32082462310791
Epoch 660, val loss: 0.9655940532684326
Epoch 670, training loss: 7.081376075744629 = 0.7619359493255615 + 1.0 * 6.319439888000488
Epoch 670, val loss: 0.9513939619064331
Epoch 680, training loss: 7.064628601074219 = 0.7449262142181396 + 1.0 * 6.3197021484375
Epoch 680, val loss: 0.9378369450569153
Epoch 690, training loss: 7.046041488647461 = 0.728693425655365 + 1.0 * 6.317348003387451
Epoch 690, val loss: 0.9250495433807373
Epoch 700, training loss: 7.029173374176025 = 0.713157594203949 + 1.0 * 6.316015720367432
Epoch 700, val loss: 0.9129595160484314
Epoch 710, training loss: 7.012281894683838 = 0.6981130242347717 + 1.0 * 6.314168930053711
Epoch 710, val loss: 0.9014607667922974
Epoch 720, training loss: 6.996250152587891 = 0.6835317015647888 + 1.0 * 6.312718391418457
Epoch 720, val loss: 0.8904847502708435
Epoch 730, training loss: 6.980676651000977 = 0.6693395972251892 + 1.0 * 6.311336994171143
Epoch 730, val loss: 0.8800174593925476
Epoch 740, training loss: 6.9732584953308105 = 0.6555486917495728 + 1.0 * 6.317709922790527
Epoch 740, val loss: 0.8700332045555115
Epoch 750, training loss: 6.952687740325928 = 0.6421942114830017 + 1.0 * 6.310493469238281
Epoch 750, val loss: 0.8606082201004028
Epoch 760, training loss: 6.937110900878906 = 0.629235565662384 + 1.0 * 6.307875156402588
Epoch 760, val loss: 0.8517100811004639
Epoch 770, training loss: 6.922974109649658 = 0.6165291666984558 + 1.0 * 6.306445121765137
Epoch 770, val loss: 0.8432164192199707
Epoch 780, training loss: 6.909298896789551 = 0.6040400266647339 + 1.0 * 6.305258750915527
Epoch 780, val loss: 0.8350697159767151
Epoch 790, training loss: 6.896882057189941 = 0.5917511582374573 + 1.0 * 6.305130958557129
Epoch 790, val loss: 0.8272479176521301
Epoch 800, training loss: 6.88276481628418 = 0.5797035098075867 + 1.0 * 6.303061485290527
Epoch 800, val loss: 0.819801926612854
Epoch 810, training loss: 6.871866226196289 = 0.5679352879524231 + 1.0 * 6.303930759429932
Epoch 810, val loss: 0.8127437233924866
Epoch 820, training loss: 6.8571648597717285 = 0.5563371777534485 + 1.0 * 6.300827503204346
Epoch 820, val loss: 0.8060280680656433
Epoch 830, training loss: 6.850297927856445 = 0.5449244976043701 + 1.0 * 6.305373191833496
Epoch 830, val loss: 0.7996048331260681
Epoch 840, training loss: 6.8331193923950195 = 0.5338266491889954 + 1.0 * 6.29929256439209
Epoch 840, val loss: 0.7935525178909302
Epoch 850, training loss: 6.821455955505371 = 0.5229668617248535 + 1.0 * 6.298489093780518
Epoch 850, val loss: 0.7878646850585938
Epoch 860, training loss: 6.809511661529541 = 0.5122948288917542 + 1.0 * 6.297216892242432
Epoch 860, val loss: 0.7824747562408447
Epoch 870, training loss: 6.79814338684082 = 0.5017977356910706 + 1.0 * 6.2963457107543945
Epoch 870, val loss: 0.7773786783218384
Epoch 880, training loss: 6.789916515350342 = 0.4915008246898651 + 1.0 * 6.298415660858154
Epoch 880, val loss: 0.7726110219955444
Epoch 890, training loss: 6.777441501617432 = 0.48147261142730713 + 1.0 * 6.295969009399414
Epoch 890, val loss: 0.7681525945663452
Epoch 900, training loss: 6.765552043914795 = 0.4716227650642395 + 1.0 * 6.293929100036621
Epoch 900, val loss: 0.7640058994293213
Epoch 910, training loss: 6.756605625152588 = 0.4619618356227875 + 1.0 * 6.294643878936768
Epoch 910, val loss: 0.7601413726806641
Epoch 920, training loss: 6.744195938110352 = 0.45249348878860474 + 1.0 * 6.2917022705078125
Epoch 920, val loss: 0.7565819025039673
Epoch 930, training loss: 6.735103607177734 = 0.44323232769966125 + 1.0 * 6.291871070861816
Epoch 930, val loss: 0.7532960772514343
Epoch 940, training loss: 6.7257890701293945 = 0.4341669976711273 + 1.0 * 6.291622161865234
Epoch 940, val loss: 0.750305712223053
Epoch 950, training loss: 6.714677810668945 = 0.42531818151474 + 1.0 * 6.2893595695495605
Epoch 950, val loss: 0.7475825548171997
Epoch 960, training loss: 6.705222129821777 = 0.41661912202835083 + 1.0 * 6.288602828979492
Epoch 960, val loss: 0.7451134324073792
Epoch 970, training loss: 6.698170185089111 = 0.40805190801620483 + 1.0 * 6.290118217468262
Epoch 970, val loss: 0.7428917288780212
Epoch 980, training loss: 6.687936305999756 = 0.3996991217136383 + 1.0 * 6.28823709487915
Epoch 980, val loss: 0.740924060344696
Epoch 990, training loss: 6.678650856018066 = 0.39149975776672363 + 1.0 * 6.287150859832764
Epoch 990, val loss: 0.7392282485961914
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7666666666666667
0.8360569319978914
The final CL Acc:0.79753, 0.02426, The final GNN Acc:0.83571, 0.00259
, 0.02858, The final GNN Acc:0.84291, 0.00224
