Begin epxeriment: cont_weight: 0.01 epoch:1000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0001, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.01, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11546])
remove edge: torch.Size([2, 9456])
updated graph: torch.Size([2, 10446])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:224: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 2.034752607345581 = 1.9487839937210083 + 0.01 * 8.596861839294434
Epoch 0, val loss: 1.9465709924697876
Epoch 10, training loss: 2.0296504497528076 = 1.943682074546814 + 0.01 * 8.596848487854004
Epoch 10, val loss: 1.941609263420105
Epoch 20, training loss: 2.0242300033569336 = 1.9382619857788086 + 0.01 * 8.59679889678955
Epoch 20, val loss: 1.936330795288086
Epoch 30, training loss: 2.0178494453430176 = 1.9318825006484985 + 0.01 * 8.596705436706543
Epoch 30, val loss: 1.9301621913909912
Epoch 40, training loss: 2.009950637817383 = 1.9239856004714966 + 0.01 * 8.596513748168945
Epoch 40, val loss: 1.9225481748580933
Epoch 50, training loss: 1.9998923540115356 = 1.9139320850372314 + 0.01 * 8.596031188964844
Epoch 50, val loss: 1.91289222240448
Epoch 60, training loss: 1.9869537353515625 = 1.9010088443756104 + 0.01 * 8.59449291229248
Epoch 60, val loss: 1.9005773067474365
Epoch 70, training loss: 1.9704662561416626 = 1.884580373764038 + 0.01 * 8.588592529296875
Epoch 70, val loss: 1.8851641416549683
Epoch 80, training loss: 1.9501737356185913 = 1.864500880241394 + 0.01 * 8.56728458404541
Epoch 80, val loss: 1.8668286800384521
Epoch 90, training loss: 1.9268701076507568 = 1.8417681455612183 + 0.01 * 8.510198593139648
Epoch 90, val loss: 1.8468269109725952
Epoch 100, training loss: 1.9021306037902832 = 1.8186386823654175 + 0.01 * 8.349189758300781
Epoch 100, val loss: 1.827186107635498
Epoch 110, training loss: 1.878865361213684 = 1.7973216772079468 + 0.01 * 8.154370307922363
Epoch 110, val loss: 1.8093023300170898
Epoch 120, training loss: 1.8586457967758179 = 1.7774158716201782 + 0.01 * 8.122991561889648
Epoch 120, val loss: 1.7918338775634766
Epoch 130, training loss: 1.8362364768981934 = 1.7554644346237183 + 0.01 * 8.077199935913086
Epoch 130, val loss: 1.771668791770935
Epoch 140, training loss: 1.8090312480926514 = 1.728881597518921 + 0.01 * 8.014969825744629
Epoch 140, val loss: 1.747374415397644
Epoch 150, training loss: 1.7758458852767944 = 1.6963974237442017 + 0.01 * 7.94484806060791
Epoch 150, val loss: 1.7185189723968506
Epoch 160, training loss: 1.7359764575958252 = 1.6572672128677368 + 0.01 * 7.870929718017578
Epoch 160, val loss: 1.6843994855880737
Epoch 170, training loss: 1.6902185678482056 = 1.6123266220092773 + 0.01 * 7.789199352264404
Epoch 170, val loss: 1.64570951461792
Epoch 180, training loss: 1.6406309604644775 = 1.5638493299484253 + 0.01 * 7.678169250488281
Epoch 180, val loss: 1.6044648885726929
Epoch 190, training loss: 1.5896838903427124 = 1.5145440101623535 + 0.01 * 7.513993263244629
Epoch 190, val loss: 1.5630208253860474
Epoch 200, training loss: 1.54011869430542 = 1.4663889408111572 + 0.01 * 7.372977256774902
Epoch 200, val loss: 1.5232970714569092
Epoch 210, training loss: 1.493366003036499 = 1.4205255508422852 + 0.01 * 7.284046173095703
Epoch 210, val loss: 1.4862558841705322
Epoch 220, training loss: 1.4492262601852417 = 1.3770363330841064 + 0.01 * 7.218993663787842
Epoch 220, val loss: 1.451811671257019
Epoch 230, training loss: 1.407573938369751 = 1.3357279300689697 + 0.01 * 7.184596061706543
Epoch 230, val loss: 1.4202431440353394
Epoch 240, training loss: 1.3679492473602295 = 1.2963027954101562 + 0.01 * 7.164645671844482
Epoch 240, val loss: 1.3911056518554688
Epoch 250, training loss: 1.3298006057739258 = 1.2582969665527344 + 0.01 * 7.150360107421875
Epoch 250, val loss: 1.3638964891433716
Epoch 260, training loss: 1.29249107837677 = 1.221101999282837 + 0.01 * 7.138904571533203
Epoch 260, val loss: 1.337947130203247
Epoch 270, training loss: 1.255397081375122 = 1.1841161251068115 + 0.01 * 7.12809419631958
Epoch 270, val loss: 1.3126394748687744
Epoch 280, training loss: 1.2179498672485352 = 1.1467759609222412 + 0.01 * 7.117384910583496
Epoch 280, val loss: 1.287255883216858
Epoch 290, training loss: 1.179664134979248 = 1.1085985898971558 + 0.01 * 7.106557846069336
Epoch 290, val loss: 1.2613369226455688
Epoch 300, training loss: 1.140251874923706 = 1.0692930221557617 + 0.01 * 7.095881938934326
Epoch 300, val loss: 1.234519124031067
Epoch 310, training loss: 1.0996081829071045 = 1.0287501811981201 + 0.01 * 7.085801124572754
Epoch 310, val loss: 1.206872582435608
Epoch 320, training loss: 1.0578726530075073 = 0.9870858192443848 + 0.01 * 7.078686714172363
Epoch 320, val loss: 1.1783442497253418
Epoch 330, training loss: 1.0153467655181885 = 0.944625198841095 + 0.01 * 7.072159290313721
Epoch 330, val loss: 1.149139642715454
Epoch 340, training loss: 0.972510576248169 = 0.9018452763557434 + 0.01 * 7.066530704498291
Epoch 340, val loss: 1.1196609735488892
Epoch 350, training loss: 0.9299029111862183 = 0.8592866063117981 + 0.01 * 7.061628818511963
Epoch 350, val loss: 1.0903596878051758
Epoch 360, training loss: 0.8880962133407593 = 0.8175202012062073 + 0.01 * 7.057599067687988
Epoch 360, val loss: 1.0617116689682007
Epoch 370, training loss: 0.8476144671440125 = 0.7770751714706421 + 0.01 * 7.053928375244141
Epoch 370, val loss: 1.0341475009918213
Epoch 380, training loss: 0.8088855743408203 = 0.7383761405944824 + 0.01 * 7.0509443283081055
Epoch 380, val loss: 1.0081307888031006
Epoch 390, training loss: 0.7722052335739136 = 0.7017275094985962 + 0.01 * 7.0477705001831055
Epoch 390, val loss: 0.983886182308197
Epoch 400, training loss: 0.7377588152885437 = 0.6673093438148499 + 0.01 * 7.044946193695068
Epoch 400, val loss: 0.961690366268158
Epoch 410, training loss: 0.7055881023406982 = 0.6351627707481384 + 0.01 * 7.042531490325928
Epoch 410, val loss: 0.9416201114654541
Epoch 420, training loss: 0.6756247878074646 = 0.6052331924438477 + 0.01 * 7.039159774780273
Epoch 420, val loss: 0.9236829280853271
Epoch 430, training loss: 0.6477600932121277 = 0.5773919820785522 + 0.01 * 7.036808967590332
Epoch 430, val loss: 0.9077951312065125
Epoch 440, training loss: 0.6217856407165527 = 0.5514481067657471 + 0.01 * 7.033755302429199
Epoch 440, val loss: 0.8938582539558411
Epoch 450, training loss: 0.5974549055099487 = 0.5271527171134949 + 0.01 * 7.030221462249756
Epoch 450, val loss: 0.8816285133361816
Epoch 460, training loss: 0.5745452642440796 = 0.5042762756347656 + 0.01 * 7.026902198791504
Epoch 460, val loss: 0.870927631855011
Epoch 470, training loss: 0.5528420209884644 = 0.4825964570045471 + 0.01 * 7.024555206298828
Epoch 470, val loss: 0.861503541469574
Epoch 480, training loss: 0.5320828557014465 = 0.4618850350379944 + 0.01 * 7.0197834968566895
Epoch 480, val loss: 0.8532131314277649
Epoch 490, training loss: 0.5121159553527832 = 0.4419673979282379 + 0.01 * 7.014858722686768
Epoch 490, val loss: 0.8459126949310303
Epoch 500, training loss: 0.4928952157497406 = 0.4227159023284912 + 0.01 * 7.0179314613342285
Epoch 500, val loss: 0.839470386505127
Epoch 510, training loss: 0.47413069009780884 = 0.4040546119213104 + 0.01 * 7.007607460021973
Epoch 510, val loss: 0.8337888121604919
Epoch 520, training loss: 0.4559476375579834 = 0.3859363794326782 + 0.01 * 7.001125812530518
Epoch 520, val loss: 0.8288142681121826
Epoch 530, training loss: 0.43829959630966187 = 0.36834990978240967 + 0.01 * 6.994967937469482
Epoch 530, val loss: 0.8245460987091064
Epoch 540, training loss: 0.421305775642395 = 0.3513105809688568 + 0.01 * 6.999518871307373
Epoch 540, val loss: 0.8209972381591797
Epoch 550, training loss: 0.4047078490257263 = 0.33483123779296875 + 0.01 * 6.9876627922058105
Epoch 550, val loss: 0.8181709051132202
Epoch 560, training loss: 0.3886936604976654 = 0.31890782713890076 + 0.01 * 6.978582382202148
Epoch 560, val loss: 0.8159781694412231
Epoch 570, training loss: 0.3732629120349884 = 0.30353888869285583 + 0.01 * 6.972402095794678
Epoch 570, val loss: 0.814451277256012
Epoch 580, training loss: 0.35837045311927795 = 0.2887021601200104 + 0.01 * 6.966829776763916
Epoch 580, val loss: 0.813565194606781
Epoch 590, training loss: 0.3439624309539795 = 0.2743605077266693 + 0.01 * 6.9601922035217285
Epoch 590, val loss: 0.8132687211036682
Epoch 600, training loss: 0.3300325274467468 = 0.26047974824905396 + 0.01 * 6.955276966094971
Epoch 600, val loss: 0.8135010600090027
Epoch 610, training loss: 0.31652218103408813 = 0.24702589213848114 + 0.01 * 6.9496283531188965
Epoch 610, val loss: 0.8142191767692566
Epoch 620, training loss: 0.3033874034881592 = 0.2339709848165512 + 0.01 * 6.941641330718994
Epoch 620, val loss: 0.8154328465461731
Epoch 630, training loss: 0.2906597852706909 = 0.22129735350608826 + 0.01 * 6.936243534088135
Epoch 630, val loss: 0.8170923590660095
Epoch 640, training loss: 0.2783665657043457 = 0.20899300277233124 + 0.01 * 6.937356472015381
Epoch 640, val loss: 0.8192064166069031
Epoch 650, training loss: 0.2663619816303253 = 0.19707481563091278 + 0.01 * 6.928715705871582
Epoch 650, val loss: 0.8217142820358276
Epoch 660, training loss: 0.2547380328178406 = 0.18555253744125366 + 0.01 * 6.918551445007324
Epoch 660, val loss: 0.8246382474899292
Epoch 670, training loss: 0.24357053637504578 = 0.17443934082984924 + 0.01 * 6.913118839263916
Epoch 670, val loss: 0.8279035091400146
Epoch 680, training loss: 0.23292185366153717 = 0.1637592762708664 + 0.01 * 6.916257858276367
Epoch 680, val loss: 0.8315672874450684
Epoch 690, training loss: 0.22268791496753693 = 0.15358427166938782 + 0.01 * 6.910364627838135
Epoch 690, val loss: 0.8355990648269653
Epoch 700, training loss: 0.21310463547706604 = 0.14393730461597443 + 0.01 * 6.916733741760254
Epoch 700, val loss: 0.8398730754852295
Epoch 710, training loss: 0.20380806922912598 = 0.13482119143009186 + 0.01 * 6.89868688583374
Epoch 710, val loss: 0.8444997668266296
Epoch 720, training loss: 0.19516639411449432 = 0.12623675167560577 + 0.01 * 6.8929643630981445
Epoch 720, val loss: 0.8494408130645752
Epoch 730, training loss: 0.18706342577934265 = 0.11818037182092667 + 0.01 * 6.888306617736816
Epoch 730, val loss: 0.8546987175941467
Epoch 740, training loss: 0.17948895692825317 = 0.1106472983956337 + 0.01 * 6.884166717529297
Epoch 740, val loss: 0.860212504863739
Epoch 750, training loss: 0.17241951823234558 = 0.1036166176199913 + 0.01 * 6.880289554595947
Epoch 750, val loss: 0.8659599423408508
Epoch 760, training loss: 0.16590173542499542 = 0.09706810116767883 + 0.01 * 6.883363246917725
Epoch 760, val loss: 0.871945321559906
Epoch 770, training loss: 0.1597260981798172 = 0.09098327159881592 + 0.01 * 6.8742828369140625
Epoch 770, val loss: 0.8780714869499207
Epoch 780, training loss: 0.15401476621627808 = 0.08532921969890594 + 0.01 * 6.868555068969727
Epoch 780, val loss: 0.8843840956687927
Epoch 790, training loss: 0.14873307943344116 = 0.08007872104644775 + 0.01 * 6.865437030792236
Epoch 790, val loss: 0.8908323049545288
Epoch 800, training loss: 0.14389446377754211 = 0.07520604878664017 + 0.01 * 6.868842601776123
Epoch 800, val loss: 0.8973855376243591
Epoch 810, training loss: 0.1392749845981598 = 0.07068666815757751 + 0.01 * 6.85883092880249
Epoch 810, val loss: 0.9040510058403015
Epoch 820, training loss: 0.13502371311187744 = 0.06649524718523026 + 0.01 * 6.852847576141357
Epoch 820, val loss: 0.9107579588890076
Epoch 830, training loss: 0.13117384910583496 = 0.0626070573925972 + 0.01 * 6.856680393218994
Epoch 830, val loss: 0.9175432324409485
Epoch 840, training loss: 0.12759774923324585 = 0.059003666043281555 + 0.01 * 6.859407901763916
Epoch 840, val loss: 0.9243543148040771
Epoch 850, training loss: 0.1241026222705841 = 0.055661287158727646 + 0.01 * 6.844134330749512
Epoch 850, val loss: 0.9311766028404236
Epoch 860, training loss: 0.1209324449300766 = 0.05255499854683876 + 0.01 * 6.83774471282959
Epoch 860, val loss: 0.9380280375480652
Epoch 870, training loss: 0.11799335479736328 = 0.04966583102941513 + 0.01 * 6.832752227783203
Epoch 870, val loss: 0.9448807239532471
Epoch 880, training loss: 0.11555972695350647 = 0.04697723314166069 + 0.01 * 6.858249664306641
Epoch 880, val loss: 0.9517315030097961
Epoch 890, training loss: 0.11280777305364609 = 0.044481270015239716 + 0.01 * 6.832650661468506
Epoch 890, val loss: 0.9585596323013306
Epoch 900, training loss: 0.11041662096977234 = 0.0421597957611084 + 0.01 * 6.825682640075684
Epoch 900, val loss: 0.9653525948524475
Epoch 910, training loss: 0.10817551612854004 = 0.03999613970518112 + 0.01 * 6.817937850952148
Epoch 910, val loss: 0.9721004962921143
Epoch 920, training loss: 0.10611855983734131 = 0.03797712177038193 + 0.01 * 6.814144134521484
Epoch 920, val loss: 0.9787970781326294
Epoch 930, training loss: 0.10426612943410873 = 0.03609324246644974 + 0.01 * 6.817288875579834
Epoch 930, val loss: 0.9854322671890259
Epoch 940, training loss: 0.10241779685020447 = 0.034336041659116745 + 0.01 * 6.808175086975098
Epoch 940, val loss: 0.9920047521591187
Epoch 950, training loss: 0.10074546933174133 = 0.03269364312291145 + 0.01 * 6.805183410644531
Epoch 950, val loss: 0.9984942674636841
Epoch 960, training loss: 0.09927969425916672 = 0.031157901510596275 + 0.01 * 6.8121795654296875
Epoch 960, val loss: 1.0049176216125488
Epoch 970, training loss: 0.09771186858415604 = 0.02972252480685711 + 0.01 * 6.798933982849121
Epoch 970, val loss: 1.0112475156784058
Epoch 980, training loss: 0.0963146835565567 = 0.02837802842259407 + 0.01 * 6.793665409088135
Epoch 980, val loss: 1.0175111293792725
Epoch 990, training loss: 0.09501025825738907 = 0.027117110788822174 + 0.01 * 6.7893147468566895
Epoch 990, val loss: 1.0236847400665283
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7407407407407408
0.7991565629942015
=== training gcn model ===
Epoch 0, training loss: 2.0306217670440674 = 1.9446531534194946 + 0.01 * 8.59685230255127
Epoch 0, val loss: 1.9477351903915405
Epoch 10, training loss: 2.0255320072174072 = 1.939563512802124 + 0.01 * 8.596839904785156
Epoch 10, val loss: 1.942448616027832
Epoch 20, training loss: 2.0200555324554443 = 1.9340876340866089 + 0.01 * 8.596790313720703
Epoch 20, val loss: 1.9366954565048218
Epoch 30, training loss: 2.0136404037475586 = 1.9276734590530396 + 0.01 * 8.59669017791748
Epoch 30, val loss: 1.9299505949020386
Epoch 40, training loss: 2.0056874752044678 = 1.9197226762771606 + 0.01 * 8.596487998962402
Epoch 40, val loss: 1.9216033220291138
Epoch 50, training loss: 1.9955101013183594 = 1.9095499515533447 + 0.01 * 8.596020698547363
Epoch 50, val loss: 1.9109551906585693
Epoch 60, training loss: 1.9823973178863525 = 1.8964505195617676 + 0.01 * 8.594676971435547
Epoch 60, val loss: 1.8973668813705444
Epoch 70, training loss: 1.9658349752426147 = 1.8799350261688232 + 0.01 * 8.58998966217041
Epoch 70, val loss: 1.8805184364318848
Epoch 80, training loss: 1.9458520412445068 = 1.8601187467575073 + 0.01 * 8.57332706451416
Epoch 80, val loss: 1.8608866930007935
Epoch 90, training loss: 1.9235645532608032 = 1.8383053541183472 + 0.01 * 8.525925636291504
Epoch 90, val loss: 1.840216040611267
Epoch 100, training loss: 1.9007915258407593 = 1.8169751167297363 + 0.01 * 8.381635665893555
Epoch 100, val loss: 1.821297287940979
Epoch 110, training loss: 1.879225254058838 = 1.797966718673706 + 0.01 * 8.12585163116455
Epoch 110, val loss: 1.8056042194366455
Epoch 120, training loss: 1.8607860803604126 = 1.7800168991088867 + 0.01 * 8.07691764831543
Epoch 120, val loss: 1.7910921573638916
Epoch 130, training loss: 1.8399252891540527 = 1.759904146194458 + 0.01 * 8.00211238861084
Epoch 130, val loss: 1.774524211883545
Epoch 140, training loss: 1.8145500421524048 = 1.73562753200531 + 0.01 * 7.892251968383789
Epoch 140, val loss: 1.7543073892593384
Epoch 150, training loss: 1.783531665802002 = 1.7059683799743652 + 0.01 * 7.756329536437988
Epoch 150, val loss: 1.7294514179229736
Epoch 160, training loss: 1.7459548711776733 = 1.669971227645874 + 0.01 * 7.598366737365723
Epoch 160, val loss: 1.6991842985153198
Epoch 170, training loss: 1.701825499534607 = 1.6277495622634888 + 0.01 * 7.407588481903076
Epoch 170, val loss: 1.6637568473815918
Epoch 180, training loss: 1.6529793739318848 = 1.5807421207427979 + 0.01 * 7.223729133605957
Epoch 180, val loss: 1.624515414237976
Epoch 190, training loss: 1.6030855178833008 = 1.531494379043579 + 0.01 * 7.159117698669434
Epoch 190, val loss: 1.5838663578033447
Epoch 200, training loss: 1.553783655166626 = 1.4825857877731323 + 0.01 * 7.119790554046631
Epoch 200, val loss: 1.5444457530975342
Epoch 210, training loss: 1.5063642263412476 = 1.4354270696640015 + 0.01 * 7.093716144561768
Epoch 210, val loss: 1.5075923204421997
Epoch 220, training loss: 1.4610346555709839 = 1.3902745246887207 + 0.01 * 7.076013565063477
Epoch 220, val loss: 1.4736884832382202
Epoch 230, training loss: 1.4176552295684814 = 1.3470265865325928 + 0.01 * 7.062861919403076
Epoch 230, val loss: 1.442021369934082
Epoch 240, training loss: 1.3758761882781982 = 1.30536687374115 + 0.01 * 7.05093240737915
Epoch 240, val loss: 1.4123328924179077
Epoch 250, training loss: 1.335432767868042 = 1.2650278806686401 + 0.01 * 7.040494918823242
Epoch 250, val loss: 1.3842567205429077
Epoch 260, training loss: 1.2960587739944458 = 1.2257421016693115 + 0.01 * 7.03167200088501
Epoch 260, val loss: 1.3574934005737305
Epoch 270, training loss: 1.2574803829193115 = 1.1872339248657227 + 0.01 * 7.024643898010254
Epoch 270, val loss: 1.3316383361816406
Epoch 280, training loss: 1.2194750308990479 = 1.1492819786071777 + 0.01 * 7.019303321838379
Epoch 280, val loss: 1.3063883781433105
Epoch 290, training loss: 1.1819236278533936 = 1.111769199371338 + 0.01 * 7.015439510345459
Epoch 290, val loss: 1.2815991640090942
Epoch 300, training loss: 1.1447498798370361 = 1.074621558189392 + 0.01 * 7.012826919555664
Epoch 300, val loss: 1.2570353746414185
Epoch 310, training loss: 1.1078860759735107 = 1.0377699136734009 + 0.01 * 7.0116119384765625
Epoch 310, val loss: 1.2325292825698853
Epoch 320, training loss: 1.0713474750518799 = 1.0012444257736206 + 0.01 * 7.010307312011719
Epoch 320, val loss: 1.2081218957901
Epoch 330, training loss: 1.035097599029541 = 0.9650030732154846 + 0.01 * 7.009456157684326
Epoch 330, val loss: 1.1837408542633057
Epoch 340, training loss: 0.9990822076797485 = 0.928994357585907 + 0.01 * 7.008786201477051
Epoch 340, val loss: 1.1592539548873901
Epoch 350, training loss: 0.9632216095924377 = 0.893140435218811 + 0.01 * 7.008118152618408
Epoch 350, val loss: 1.1348037719726562
Epoch 360, training loss: 0.9274876713752747 = 0.8574131727218628 + 0.01 * 7.007448196411133
Epoch 360, val loss: 1.110271692276001
Epoch 370, training loss: 0.8919051289558411 = 0.82183837890625 + 0.01 * 7.006674766540527
Epoch 370, val loss: 1.0858893394470215
Epoch 380, training loss: 0.8566082119941711 = 0.7865462899208069 + 0.01 * 7.006190776824951
Epoch 380, val loss: 1.0617830753326416
Epoch 390, training loss: 0.821785569190979 = 0.7517364025115967 + 0.01 * 7.004915714263916
Epoch 390, val loss: 1.0382133722305298
Epoch 400, training loss: 0.7876719236373901 = 0.717633068561554 + 0.01 * 7.003885746002197
Epoch 400, val loss: 1.0153651237487793
Epoch 410, training loss: 0.7544716596603394 = 0.6844453811645508 + 0.01 * 7.002629280090332
Epoch 410, val loss: 0.9935073256492615
Epoch 420, training loss: 0.722331166267395 = 0.6523205041885376 + 0.01 * 7.001068592071533
Epoch 420, val loss: 0.9728160500526428
Epoch 430, training loss: 0.6913461089134216 = 0.621353268623352 + 0.01 * 6.999283790588379
Epoch 430, val loss: 0.9535847902297974
Epoch 440, training loss: 0.6615387797355652 = 0.5915650725364685 + 0.01 * 6.997371673583984
Epoch 440, val loss: 0.9358960390090942
Epoch 450, training loss: 0.6329034566879272 = 0.5629494786262512 + 0.01 * 6.9953999519348145
Epoch 450, val loss: 0.9196903705596924
Epoch 460, training loss: 0.6054023504257202 = 0.5354730486869812 + 0.01 * 6.992931842803955
Epoch 460, val loss: 0.9051439762115479
Epoch 470, training loss: 0.5790099501609802 = 0.5090907216072083 + 0.01 * 6.991921901702881
Epoch 470, val loss: 0.8921753764152527
Epoch 480, training loss: 0.5536646842956543 = 0.4837868809700012 + 0.01 * 6.987781524658203
Epoch 480, val loss: 0.8807615041732788
Epoch 490, training loss: 0.5293900966644287 = 0.4595438241958618 + 0.01 * 6.9846272468566895
Epoch 490, val loss: 0.8708295822143555
Epoch 500, training loss: 0.5062145590782166 = 0.4363449811935425 + 0.01 * 6.986959934234619
Epoch 500, val loss: 0.8623064756393433
Epoch 510, training loss: 0.48397427797317505 = 0.41418635845184326 + 0.01 * 6.978792667388916
Epoch 510, val loss: 0.8550806045532227
Epoch 520, training loss: 0.462802529335022 = 0.3930510878562927 + 0.01 * 6.975144386291504
Epoch 520, val loss: 0.8490933775901794
Epoch 530, training loss: 0.4426179826259613 = 0.37290194630622864 + 0.01 * 6.971602916717529
Epoch 530, val loss: 0.8442658185958862
Epoch 540, training loss: 0.4233662486076355 = 0.35368379950523376 + 0.01 * 6.9682440757751465
Epoch 540, val loss: 0.8404643535614014
Epoch 550, training loss: 0.40496939420700073 = 0.3353285491466522 + 0.01 * 6.964083194732666
Epoch 550, val loss: 0.837547242641449
Epoch 560, training loss: 0.3873556852340698 = 0.3177492916584015 + 0.01 * 6.960638999938965
Epoch 560, val loss: 0.8354693055152893
Epoch 570, training loss: 0.37040477991104126 = 0.3008483350276947 + 0.01 * 6.955644130706787
Epoch 570, val loss: 0.8341450095176697
Epoch 580, training loss: 0.35405874252319336 = 0.2845516502857208 + 0.01 * 6.950707912445068
Epoch 580, val loss: 0.8334745168685913
Epoch 590, training loss: 0.3383619785308838 = 0.2687973976135254 + 0.01 * 6.956459045410156
Epoch 590, val loss: 0.8333497643470764
Epoch 600, training loss: 0.32299143075942993 = 0.2535466253757477 + 0.01 * 6.944481372833252
Epoch 600, val loss: 0.83377605676651
Epoch 610, training loss: 0.3081609904766083 = 0.23878292739391327 + 0.01 * 6.937806606292725
Epoch 610, val loss: 0.8346904516220093
Epoch 620, training loss: 0.2938564121723175 = 0.22452282905578613 + 0.01 * 6.933359146118164
Epoch 620, val loss: 0.8360590934753418
Epoch 630, training loss: 0.2802969515323639 = 0.21081556379795074 + 0.01 * 6.948139667510986
Epoch 630, val loss: 0.8378415703773499
Epoch 640, training loss: 0.2670333683490753 = 0.19773583114147186 + 0.01 * 6.92975378036499
Epoch 640, val loss: 0.8401828408241272
Epoch 650, training loss: 0.2545190453529358 = 0.18532373011112213 + 0.01 * 6.919530868530273
Epoch 650, val loss: 0.8430130481719971
Epoch 660, training loss: 0.24275250732898712 = 0.17360176146030426 + 0.01 * 6.915074825286865
Epoch 660, val loss: 0.8464081287384033
Epoch 670, training loss: 0.23174239695072174 = 0.16257846355438232 + 0.01 * 6.916393280029297
Epoch 670, val loss: 0.8502973318099976
Epoch 680, training loss: 0.22141695022583008 = 0.1522633582353592 + 0.01 * 6.915360450744629
Epoch 680, val loss: 0.8546736836433411
Epoch 690, training loss: 0.21169182658195496 = 0.14263585209846497 + 0.01 * 6.905597686767578
Epoch 690, val loss: 0.8594828248023987
Epoch 700, training loss: 0.20266544818878174 = 0.13366149365901947 + 0.01 * 6.900396347045898
Epoch 700, val loss: 0.8647527694702148
Epoch 710, training loss: 0.19429250061511993 = 0.12531225383281708 + 0.01 * 6.898025035858154
Epoch 710, val loss: 0.8703935146331787
Epoch 720, training loss: 0.18647268414497375 = 0.11755470186471939 + 0.01 * 6.891798496246338
Epoch 720, val loss: 0.8763474225997925
Epoch 730, training loss: 0.17940574884414673 = 0.11035945266485214 + 0.01 * 6.904629230499268
Epoch 730, val loss: 0.8826318979263306
Epoch 740, training loss: 0.17259109020233154 = 0.10369086265563965 + 0.01 * 6.890021800994873
Epoch 740, val loss: 0.8891146779060364
Epoch 750, training loss: 0.16630573570728302 = 0.09749993681907654 + 0.01 * 6.880579948425293
Epoch 750, val loss: 0.8958341479301453
Epoch 760, training loss: 0.16051769256591797 = 0.09174762666225433 + 0.01 * 6.877007007598877
Epoch 760, val loss: 0.9027068614959717
Epoch 770, training loss: 0.15516018867492676 = 0.08640268445014954 + 0.01 * 6.875750541687012
Epoch 770, val loss: 0.9097084999084473
Epoch 780, training loss: 0.15015828609466553 = 0.08143777400255203 + 0.01 * 6.872051239013672
Epoch 780, val loss: 0.916829526424408
Epoch 790, training loss: 0.1457078456878662 = 0.07682157307863235 + 0.01 * 6.888627529144287
Epoch 790, val loss: 0.9240479469299316
Epoch 800, training loss: 0.14117205142974854 = 0.07253620773553848 + 0.01 * 6.863584041595459
Epoch 800, val loss: 0.9312860369682312
Epoch 810, training loss: 0.13716475665569305 = 0.06854772567749023 + 0.01 * 6.861702919006348
Epoch 810, val loss: 0.9385376572608948
Epoch 820, training loss: 0.13340464234352112 = 0.0648307204246521 + 0.01 * 6.857391357421875
Epoch 820, val loss: 0.9458543062210083
Epoch 830, training loss: 0.12995585799217224 = 0.06136472150683403 + 0.01 * 6.8591132164001465
Epoch 830, val loss: 0.9531384706497192
Epoch 840, training loss: 0.1266411393880844 = 0.058129098266363144 + 0.01 * 6.8512043952941895
Epoch 840, val loss: 0.9603899717330933
Epoch 850, training loss: 0.1236337348818779 = 0.05510879307985306 + 0.01 * 6.852494239807129
Epoch 850, val loss: 0.9676070809364319
Epoch 860, training loss: 0.1207340881228447 = 0.05229048430919647 + 0.01 * 6.844360828399658
Epoch 860, val loss: 0.9747800230979919
Epoch 870, training loss: 0.11804476380348206 = 0.04965510219335556 + 0.01 * 6.838966369628906
Epoch 870, val loss: 0.9819157123565674
Epoch 880, training loss: 0.11551309376955032 = 0.04718771576881409 + 0.01 * 6.83253812789917
Epoch 880, val loss: 0.9889789819717407
Epoch 890, training loss: 0.11321374773979187 = 0.04487916827201843 + 0.01 * 6.833457946777344
Epoch 890, val loss: 0.9959858059883118
Epoch 900, training loss: 0.11099207401275635 = 0.04272124171257019 + 0.01 * 6.827083587646484
Epoch 900, val loss: 1.0028678178787231
Epoch 910, training loss: 0.10893675684928894 = 0.04069768264889717 + 0.01 * 6.823907852172852
Epoch 910, val loss: 1.0096886157989502
Epoch 920, training loss: 0.10700048506259918 = 0.038800306618213654 + 0.01 * 6.820018291473389
Epoch 920, val loss: 1.0164183378219604
Epoch 930, training loss: 0.10520196706056595 = 0.03702101856470108 + 0.01 * 6.8180952072143555
Epoch 930, val loss: 1.0230740308761597
Epoch 940, training loss: 0.10351972281932831 = 0.035349685698747635 + 0.01 * 6.81700325012207
Epoch 940, val loss: 1.0296151638031006
Epoch 950, training loss: 0.10185278207063675 = 0.03377973288297653 + 0.01 * 6.807304859161377
Epoch 950, val loss: 1.0360854864120483
Epoch 960, training loss: 0.10045009851455688 = 0.03230380639433861 + 0.01 * 6.814629077911377
Epoch 960, val loss: 1.0424755811691284
Epoch 970, training loss: 0.09907291829586029 = 0.030916055664420128 + 0.01 * 6.815686225891113
Epoch 970, val loss: 1.0487381219863892
Epoch 980, training loss: 0.09760301560163498 = 0.02960907109081745 + 0.01 * 6.7993950843811035
Epoch 980, val loss: 1.0549453496932983
Epoch 990, training loss: 0.0964498445391655 = 0.02837790548801422 + 0.01 * 6.807194232940674
Epoch 990, val loss: 1.0610448122024536
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7592592592592593
0.8181338956246705
=== training gcn model ===
Epoch 0, training loss: 2.0283913612365723 = 1.9424229860305786 + 0.01 * 8.596842765808105
Epoch 0, val loss: 1.944101333618164
Epoch 10, training loss: 2.023632049560547 = 1.9376637935638428 + 0.01 * 8.59682559967041
Epoch 10, val loss: 1.9392768144607544
Epoch 20, training loss: 2.0185396671295166 = 1.9325718879699707 + 0.01 * 8.596776008605957
Epoch 20, val loss: 1.9340678453445435
Epoch 30, training loss: 2.0126638412475586 = 1.9266971349716187 + 0.01 * 8.596677780151367
Epoch 30, val loss: 1.92807936668396
Epoch 40, training loss: 2.0055341720581055 = 1.9195693731307983 + 0.01 * 8.596482276916504
Epoch 40, val loss: 1.9208964109420776
Epoch 50, training loss: 1.996568202972412 = 1.910607933998108 + 0.01 * 8.59603214263916
Epoch 50, val loss: 1.9119631052017212
Epoch 60, training loss: 1.9851237535476685 = 1.8991762399673462 + 0.01 * 8.594748497009277
Epoch 60, val loss: 1.900709867477417
Epoch 70, training loss: 1.9705884456634521 = 1.8846856355667114 + 0.01 * 8.590278625488281
Epoch 70, val loss: 1.8866527080535889
Epoch 80, training loss: 1.9526036977767944 = 1.8668577671051025 + 0.01 * 8.57459831237793
Epoch 80, val loss: 1.8696447610855103
Epoch 90, training loss: 1.9314519166946411 = 1.8461616039276123 + 0.01 * 8.529027938842773
Epoch 90, val loss: 1.8504936695098877
Epoch 100, training loss: 1.9080662727355957 = 1.824075698852539 + 0.01 * 8.399062156677246
Epoch 100, val loss: 1.8308504819869995
Epoch 110, training loss: 1.8838359117507935 = 1.8023918867111206 + 0.01 * 8.144404411315918
Epoch 110, val loss: 1.812270164489746
Epoch 120, training loss: 1.862002968788147 = 1.7812858819961548 + 0.01 * 8.071703910827637
Epoch 120, val loss: 1.7944042682647705
Epoch 130, training loss: 1.838309407234192 = 1.7587095499038696 + 0.01 * 7.959988117218018
Epoch 130, val loss: 1.7748658657073975
Epoch 140, training loss: 1.8103874921798706 = 1.732337474822998 + 0.01 * 7.804999828338623
Epoch 140, val loss: 1.7518318891525269
Epoch 150, training loss: 1.777099370956421 = 1.700640320777893 + 0.01 * 7.64590311050415
Epoch 150, val loss: 1.7244993448257446
Epoch 160, training loss: 1.7377768754959106 = 1.6626535654067993 + 0.01 * 7.512327194213867
Epoch 160, val loss: 1.6922696828842163
Epoch 170, training loss: 1.692366123199463 = 1.618106722831726 + 0.01 * 7.425942420959473
Epoch 170, val loss: 1.654426097869873
Epoch 180, training loss: 1.6422667503356934 = 1.5684099197387695 + 0.01 * 7.385678768157959
Epoch 180, val loss: 1.6124212741851807
Epoch 190, training loss: 1.588865041732788 = 1.515196442604065 + 0.01 * 7.36685848236084
Epoch 190, val loss: 1.567948818206787
Epoch 200, training loss: 1.5338499546051025 = 1.4603537321090698 + 0.01 * 7.3496222496032715
Epoch 200, val loss: 1.522600531578064
Epoch 210, training loss: 1.4786314964294434 = 1.4052985906600952 + 0.01 * 7.333286762237549
Epoch 210, val loss: 1.4778976440429688
Epoch 220, training loss: 1.4239943027496338 = 1.3507988452911377 + 0.01 * 7.319545745849609
Epoch 220, val loss: 1.4340063333511353
Epoch 230, training loss: 1.3702784776687622 = 1.297200083732605 + 0.01 * 7.307837963104248
Epoch 230, val loss: 1.3916573524475098
Epoch 240, training loss: 1.3176370859146118 = 1.2446775436401367 + 0.01 * 7.2959489822387695
Epoch 240, val loss: 1.3510912656784058
Epoch 250, training loss: 1.2663087844848633 = 1.1934993267059326 + 0.01 * 7.280946254730225
Epoch 250, val loss: 1.3123022317886353
Epoch 260, training loss: 1.2165342569351196 = 1.1439317464828491 + 0.01 * 7.26024866104126
Epoch 260, val loss: 1.2753320932388306
Epoch 270, training loss: 1.1684982776641846 = 1.0961931943893433 + 0.01 * 7.230513095855713
Epoch 270, val loss: 1.2405098676681519
Epoch 280, training loss: 1.122238039970398 = 1.0503334999084473 + 0.01 * 7.1904520988464355
Epoch 280, val loss: 1.2075179815292358
Epoch 290, training loss: 1.0777784585952759 = 1.0063223838806152 + 0.01 * 7.145610809326172
Epoch 290, val loss: 1.1763309240341187
Epoch 300, training loss: 1.035191297531128 = 0.9641104340553284 + 0.01 * 7.108087539672852
Epoch 300, val loss: 1.1468204259872437
Epoch 310, training loss: 0.9944660067558289 = 0.9235907793045044 + 0.01 * 7.087522506713867
Epoch 310, val loss: 1.1188514232635498
Epoch 320, training loss: 0.9553583860397339 = 0.8846321105957031 + 0.01 * 7.072628498077393
Epoch 320, val loss: 1.0924479961395264
Epoch 330, training loss: 0.9177446365356445 = 0.8471340537071228 + 0.01 * 7.0610575675964355
Epoch 330, val loss: 1.067529320716858
Epoch 340, training loss: 0.8815458416938782 = 0.8110326528549194 + 0.01 * 7.051319122314453
Epoch 340, val loss: 1.044188380241394
Epoch 350, training loss: 0.8467175364494324 = 0.7762863039970398 + 0.01 * 7.043122291564941
Epoch 350, val loss: 1.0224127769470215
Epoch 360, training loss: 0.8132588267326355 = 0.742906928062439 + 0.01 * 7.035192012786865
Epoch 360, val loss: 1.002271294593811
Epoch 370, training loss: 0.7811741232872009 = 0.7108991742134094 + 0.01 * 7.02749490737915
Epoch 370, val loss: 0.9838261604309082
Epoch 380, training loss: 0.7504960894584656 = 0.6802796721458435 + 0.01 * 7.021642684936523
Epoch 380, val loss: 0.9669895768165588
Epoch 390, training loss: 0.7212039232254028 = 0.6510597467422485 + 0.01 * 7.014420509338379
Epoch 390, val loss: 0.9518012404441833
Epoch 400, training loss: 0.6932978630065918 = 0.6232369542121887 + 0.01 * 7.006091594696045
Epoch 400, val loss: 0.9382727146148682
Epoch 410, training loss: 0.6667635440826416 = 0.5967702269554138 + 0.01 * 6.999331951141357
Epoch 410, val loss: 0.9262540340423584
Epoch 420, training loss: 0.6415515542030334 = 0.5716122388839722 + 0.01 * 6.993932723999023
Epoch 420, val loss: 0.9157416224479675
Epoch 430, training loss: 0.617607831954956 = 0.5477206110954285 + 0.01 * 6.988723278045654
Epoch 430, val loss: 0.906755805015564
Epoch 440, training loss: 0.5948326587677002 = 0.5250244736671448 + 0.01 * 6.98081636428833
Epoch 440, val loss: 0.8992113471031189
Epoch 450, training loss: 0.5731974840164185 = 0.5034549832344055 + 0.01 * 6.974249839782715
Epoch 450, val loss: 0.8930676579475403
Epoch 460, training loss: 0.5526193380355835 = 0.4829418659210205 + 0.01 * 6.967745304107666
Epoch 460, val loss: 0.8882470726966858
Epoch 470, training loss: 0.5330361723899841 = 0.46342167258262634 + 0.01 * 6.961452484130859
Epoch 470, val loss: 0.8847097158432007
Epoch 480, training loss: 0.5144405961036682 = 0.44482216238975525 + 0.01 * 6.961844444274902
Epoch 480, val loss: 0.8823990225791931
Epoch 490, training loss: 0.4965875744819641 = 0.42707115411758423 + 0.01 * 6.951642036437988
Epoch 490, val loss: 0.8812021017074585
Epoch 500, training loss: 0.47956013679504395 = 0.4100840091705322 + 0.01 * 6.94761323928833
Epoch 500, val loss: 0.881085991859436
Epoch 510, training loss: 0.4632030725479126 = 0.39378267526626587 + 0.01 * 6.942038536071777
Epoch 510, val loss: 0.8819202184677124
Epoch 520, training loss: 0.44744837284088135 = 0.3780772089958191 + 0.01 * 6.937117576599121
Epoch 520, val loss: 0.8836079835891724
Epoch 530, training loss: 0.43219053745269775 = 0.36286717653274536 + 0.01 * 6.932337760925293
Epoch 530, val loss: 0.8860346674919128
Epoch 540, training loss: 0.41735562682151794 = 0.34805983304977417 + 0.01 * 6.929579734802246
Epoch 540, val loss: 0.8892097473144531
Epoch 550, training loss: 0.40283438563346863 = 0.3335718810558319 + 0.01 * 6.926251411437988
Epoch 550, val loss: 0.892998218536377
Epoch 560, training loss: 0.3885519504547119 = 0.31932029128074646 + 0.01 * 6.9231672286987305
Epoch 560, val loss: 0.897344708442688
Epoch 570, training loss: 0.3744167983531952 = 0.30521851778030396 + 0.01 * 6.919827461242676
Epoch 570, val loss: 0.9021448493003845
Epoch 580, training loss: 0.36039745807647705 = 0.2912316620349884 + 0.01 * 6.916579723358154
Epoch 580, val loss: 0.9073783755302429
Epoch 590, training loss: 0.3465002179145813 = 0.27735191583633423 + 0.01 * 6.914829730987549
Epoch 590, val loss: 0.9130145907402039
Epoch 600, training loss: 0.33277443051338196 = 0.26361846923828125 + 0.01 * 6.915596961975098
Epoch 600, val loss: 0.9190680980682373
Epoch 610, training loss: 0.3192046880722046 = 0.2500900328159332 + 0.01 * 6.911466121673584
Epoch 610, val loss: 0.9255027174949646
Epoch 620, training loss: 0.3059408664703369 = 0.23686443269252777 + 0.01 * 6.9076433181762695
Epoch 620, val loss: 0.9324127435684204
Epoch 630, training loss: 0.2930879592895508 = 0.2240307629108429 + 0.01 * 6.905719757080078
Epoch 630, val loss: 0.9397799372673035
Epoch 640, training loss: 0.28070521354675293 = 0.21166881918907166 + 0.01 * 6.903639793395996
Epoch 640, val loss: 0.9476277828216553
Epoch 650, training loss: 0.2688850462436676 = 0.19983473420143127 + 0.01 * 6.905031681060791
Epoch 650, val loss: 0.9560388922691345
Epoch 660, training loss: 0.2576056122779846 = 0.1885758340358734 + 0.01 * 6.902978420257568
Epoch 660, val loss: 0.964993417263031
Epoch 670, training loss: 0.2469349205493927 = 0.1779075413942337 + 0.01 * 6.90273904800415
Epoch 670, val loss: 0.9744817614555359
Epoch 680, training loss: 0.23680350184440613 = 0.16783058643341064 + 0.01 * 6.897292137145996
Epoch 680, val loss: 0.9844415783882141
Epoch 690, training loss: 0.2272709161043167 = 0.15832693874835968 + 0.01 * 6.894398212432861
Epoch 690, val loss: 0.9948687553405762
Epoch 700, training loss: 0.21832019090652466 = 0.14937812089920044 + 0.01 * 6.894207954406738
Epoch 700, val loss: 1.0057427883148193
Epoch 710, training loss: 0.20988982915878296 = 0.1409611999988556 + 0.01 * 6.892863750457764
Epoch 710, val loss: 1.0170161724090576
Epoch 720, training loss: 0.20192591845989227 = 0.13304443657398224 + 0.01 * 6.888148307800293
Epoch 720, val loss: 1.0286054611206055
Epoch 730, training loss: 0.1945004016160965 = 0.12560191750526428 + 0.01 * 6.889848709106445
Epoch 730, val loss: 1.0404940843582153
Epoch 740, training loss: 0.18743625283241272 = 0.11860833317041397 + 0.01 * 6.882791996002197
Epoch 740, val loss: 1.05267333984375
Epoch 750, training loss: 0.18083755671977997 = 0.11203498393297195 + 0.01 * 6.880257606506348
Epoch 750, val loss: 1.065076470375061
Epoch 760, training loss: 0.17465342581272125 = 0.10585598647594452 + 0.01 * 6.879744052886963
Epoch 760, val loss: 1.0776641368865967
Epoch 770, training loss: 0.1688183844089508 = 0.10005371272563934 + 0.01 * 6.876466274261475
Epoch 770, val loss: 1.0904210805892944
Epoch 780, training loss: 0.16334886848926544 = 0.09460519999265671 + 0.01 * 6.8743672370910645
Epoch 780, val loss: 1.103270411491394
Epoch 790, training loss: 0.1581849753856659 = 0.08948691934347153 + 0.01 * 6.869804859161377
Epoch 790, val loss: 1.1162352561950684
Epoch 800, training loss: 0.1534256637096405 = 0.08467857539653778 + 0.01 * 6.874708652496338
Epoch 800, val loss: 1.1292505264282227
Epoch 810, training loss: 0.14883911609649658 = 0.080165795981884 + 0.01 * 6.867332458496094
Epoch 810, val loss: 1.1423077583312988
Epoch 820, training loss: 0.14465028047561646 = 0.0759277492761612 + 0.01 * 6.87225341796875
Epoch 820, val loss: 1.1553465127944946
Epoch 830, training loss: 0.14053940773010254 = 0.07194972783327103 + 0.01 * 6.858968257904053
Epoch 830, val loss: 1.1683892011642456
Epoch 840, training loss: 0.13678090274333954 = 0.06821347773075104 + 0.01 * 6.856742858886719
Epoch 840, val loss: 1.1814112663269043
Epoch 850, training loss: 0.13328686356544495 = 0.06470059603452682 + 0.01 * 6.858626365661621
Epoch 850, val loss: 1.1943472623825073
Epoch 860, training loss: 0.12990963459014893 = 0.06139605864882469 + 0.01 * 6.851357460021973
Epoch 860, val loss: 1.2071760892868042
Epoch 870, training loss: 0.12675398588180542 = 0.058279212564229965 + 0.01 * 6.847477436065674
Epoch 870, val loss: 1.2199547290802002
Epoch 880, training loss: 0.12378188222646713 = 0.05533534288406372 + 0.01 * 6.844654083251953
Epoch 880, val loss: 1.2326041460037231
Epoch 890, training loss: 0.12107974290847778 = 0.05255569517612457 + 0.01 * 6.852404594421387
Epoch 890, val loss: 1.245058298110962
Epoch 900, training loss: 0.11838995665311813 = 0.049936577677726746 + 0.01 * 6.845337867736816
Epoch 900, val loss: 1.2573747634887695
Epoch 910, training loss: 0.11583316326141357 = 0.047467637807130814 + 0.01 * 6.83655309677124
Epoch 910, val loss: 1.2695220708847046
Epoch 920, training loss: 0.1135195791721344 = 0.04514145478606224 + 0.01 * 6.8378119468688965
Epoch 920, val loss: 1.2815138101577759
Epoch 930, training loss: 0.11127118766307831 = 0.04295190051198006 + 0.01 * 6.831928730010986
Epoch 930, val loss: 1.2932792901992798
Epoch 940, training loss: 0.10917991399765015 = 0.040889691561460495 + 0.01 * 6.82902193069458
Epoch 940, val loss: 1.3048988580703735
Epoch 950, training loss: 0.10728287696838379 = 0.03894847258925438 + 0.01 * 6.83344030380249
Epoch 950, val loss: 1.3162949085235596
Epoch 960, training loss: 0.10538263618946075 = 0.037123069167137146 + 0.01 * 6.82595682144165
Epoch 960, val loss: 1.3274809122085571
Epoch 970, training loss: 0.10362371802330017 = 0.03540501371026039 + 0.01 * 6.82187032699585
Epoch 970, val loss: 1.3384590148925781
Epoch 980, training loss: 0.10195405781269073 = 0.033787552267313004 + 0.01 * 6.816650390625
Epoch 980, val loss: 1.3492604494094849
Epoch 990, training loss: 0.10047529637813568 = 0.03226438909769058 + 0.01 * 6.821091175079346
Epoch 990, val loss: 1.3598237037658691
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7333333333333334
0.8149710068529257
The final CL Acc:0.74444, 0.01090, The final GNN Acc:0.81075, 0.00830
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13276])
remove edge: torch.Size([2, 7882])
updated graph: torch.Size([2, 10602])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 2.020371675491333 = 1.9344033002853394 + 0.01 * 8.596848487854004
Epoch 0, val loss: 1.934733510017395
Epoch 10, training loss: 2.015474319458008 = 1.9295060634613037 + 0.01 * 8.59682559967041
Epoch 10, val loss: 1.9302865266799927
Epoch 20, training loss: 2.0099642276763916 = 1.9239964485168457 + 0.01 * 8.59676742553711
Epoch 20, val loss: 1.9251283407211304
Epoch 30, training loss: 2.0032942295074463 = 1.9173275232315063 + 0.01 * 8.596668243408203
Epoch 30, val loss: 1.9187685251235962
Epoch 40, training loss: 1.9949030876159668 = 1.9089382886886597 + 0.01 * 8.59648323059082
Epoch 40, val loss: 1.9107111692428589
Epoch 50, training loss: 1.9840691089630127 = 1.8981084823608398 + 0.01 * 8.596067428588867
Epoch 50, val loss: 1.9003719091415405
Epoch 60, training loss: 1.9700149297714233 = 1.8840662240982056 + 0.01 * 8.594867706298828
Epoch 60, val loss: 1.8871454000473022
Epoch 70, training loss: 1.9521634578704834 = 1.866260051727295 + 0.01 * 8.590339660644531
Epoch 70, val loss: 1.870758056640625
Epoch 80, training loss: 1.9305965900421143 = 1.8448779582977295 + 0.01 * 8.571857452392578
Epoch 80, val loss: 1.8517189025878906
Epoch 90, training loss: 1.906423807144165 = 1.8213176727294922 + 0.01 * 8.510615348815918
Epoch 90, val loss: 1.8316150903701782
Epoch 100, training loss: 1.8810548782348633 = 1.7980200052261353 + 0.01 * 8.303483009338379
Epoch 100, val loss: 1.8123314380645752
Epoch 110, training loss: 1.855921745300293 = 1.775826096534729 + 0.01 * 8.009566307067871
Epoch 110, val loss: 1.793404221534729
Epoch 120, training loss: 1.8317710161209106 = 1.752302646636963 + 0.01 * 7.946839332580566
Epoch 120, val loss: 1.7719528675079346
Epoch 130, training loss: 1.8030720949172974 = 1.7244246006011963 + 0.01 * 7.864749431610107
Epoch 130, val loss: 1.7461371421813965
Epoch 140, training loss: 1.768668293952942 = 1.690848708152771 + 0.01 * 7.7819623947143555
Epoch 140, val loss: 1.7159669399261475
Epoch 150, training loss: 1.7282323837280273 = 1.6511722803115845 + 0.01 * 7.7060089111328125
Epoch 150, val loss: 1.6816285848617554
Epoch 160, training loss: 1.6823906898498535 = 1.606249213218689 + 0.01 * 7.614142894744873
Epoch 160, val loss: 1.6436926126480103
Epoch 170, training loss: 1.6326638460159302 = 1.5578982830047607 + 0.01 * 7.476556777954102
Epoch 170, val loss: 1.603088617324829
Epoch 180, training loss: 1.5818954706192017 = 1.508794903755188 + 0.01 * 7.31005334854126
Epoch 180, val loss: 1.562186360359192
Epoch 190, training loss: 1.5333073139190674 = 1.4609426259994507 + 0.01 * 7.23646354675293
Epoch 190, val loss: 1.5226320028305054
Epoch 200, training loss: 1.487587332725525 = 1.415587306022644 + 0.01 * 7.2000017166137695
Epoch 200, val loss: 1.4855728149414062
Epoch 210, training loss: 1.4447624683380127 = 1.3730741739273071 + 0.01 * 7.168834209442139
Epoch 210, val loss: 1.451050877571106
Epoch 220, training loss: 1.4046287536621094 = 1.3331973552703857 + 0.01 * 7.143142223358154
Epoch 220, val loss: 1.418867826461792
Epoch 230, training loss: 1.3667082786560059 = 1.2955169677734375 + 0.01 * 7.119128227233887
Epoch 230, val loss: 1.3888696432113647
Epoch 240, training loss: 1.33058762550354 = 1.259625792503357 + 0.01 * 7.096181869506836
Epoch 240, val loss: 1.3606618642807007
Epoch 250, training loss: 1.29579758644104 = 1.225037932395935 + 0.01 * 7.075965404510498
Epoch 250, val loss: 1.333661437034607
Epoch 260, training loss: 1.2617467641830444 = 1.1911635398864746 + 0.01 * 7.058317184448242
Epoch 260, val loss: 1.3075426816940308
Epoch 270, training loss: 1.2277555465698242 = 1.1573295593261719 + 0.01 * 7.042604923248291
Epoch 270, val loss: 1.2814828157424927
Epoch 280, training loss: 1.1931642293930054 = 1.1228762865066528 + 0.01 * 7.0287933349609375
Epoch 280, val loss: 1.2548776865005493
Epoch 290, training loss: 1.1574583053588867 = 1.0872856378555298 + 0.01 * 7.01726770401001
Epoch 290, val loss: 1.2272669076919556
Epoch 300, training loss: 1.1202921867370605 = 1.0502060651779175 + 0.01 * 7.008612155914307
Epoch 300, val loss: 1.198476791381836
Epoch 310, training loss: 1.081595778465271 = 1.0115693807601929 + 0.01 * 7.0026397705078125
Epoch 310, val loss: 1.168256402015686
Epoch 320, training loss: 1.0415127277374268 = 0.9715328812599182 + 0.01 * 6.997989177703857
Epoch 320, val loss: 1.1368082761764526
Epoch 330, training loss: 1.0003702640533447 = 0.9304279685020447 + 0.01 * 6.994235515594482
Epoch 330, val loss: 1.1046051979064941
Epoch 340, training loss: 0.9586254358291626 = 0.8887153267860413 + 0.01 * 6.991009712219238
Epoch 340, val loss: 1.07209050655365
Epoch 350, training loss: 0.9167977571487427 = 0.8469159007072449 + 0.01 * 6.988185882568359
Epoch 350, val loss: 1.039704442024231
Epoch 360, training loss: 0.8753681778907776 = 0.8055116534233093 + 0.01 * 6.985651969909668
Epoch 360, val loss: 1.0079963207244873
Epoch 370, training loss: 0.834822952747345 = 0.764988124370575 + 0.01 * 6.9834818840026855
Epoch 370, val loss: 0.9774340391159058
Epoch 380, training loss: 0.7956233620643616 = 0.7258093953132629 + 0.01 * 6.9813947677612305
Epoch 380, val loss: 0.9485436081886292
Epoch 390, training loss: 0.7581716179847717 = 0.6883771419525146 + 0.01 * 6.979447364807129
Epoch 390, val loss: 0.921718955039978
Epoch 400, training loss: 0.7227545976638794 = 0.6529799103736877 + 0.01 * 6.9774675369262695
Epoch 400, val loss: 0.8972816467285156
Epoch 410, training loss: 0.6895352005958557 = 0.6197780966758728 + 0.01 * 6.975709915161133
Epoch 410, val loss: 0.8754485249519348
Epoch 420, training loss: 0.6585197448730469 = 0.5887824296951294 + 0.01 * 6.973731994628906
Epoch 420, val loss: 0.8562673926353455
Epoch 430, training loss: 0.6295968294143677 = 0.559882402420044 + 0.01 * 6.971444606781006
Epoch 430, val loss: 0.8396583199501038
Epoch 440, training loss: 0.6026525497436523 = 0.5329565405845642 + 0.01 * 6.969604015350342
Epoch 440, val loss: 0.8254877328872681
Epoch 450, training loss: 0.577494740486145 = 0.5078258514404297 + 0.01 * 6.96688985824585
Epoch 450, val loss: 0.8135479092597961
Epoch 460, training loss: 0.5539577603340149 = 0.48431169986724854 + 0.01 * 6.964607238769531
Epoch 460, val loss: 0.8036140203475952
Epoch 470, training loss: 0.5318787097930908 = 0.4622475802898407 + 0.01 * 6.963109970092773
Epoch 470, val loss: 0.7954533100128174
Epoch 480, training loss: 0.5110843181610107 = 0.44148752093315125 + 0.01 * 6.959682941436768
Epoch 480, val loss: 0.7888563275337219
Epoch 490, training loss: 0.4914734959602356 = 0.42189931869506836 + 0.01 * 6.9574174880981445
Epoch 490, val loss: 0.78365558385849
Epoch 500, training loss: 0.47289982438087463 = 0.4033566415309906 + 0.01 * 6.954317569732666
Epoch 500, val loss: 0.7796900272369385
Epoch 510, training loss: 0.45526400208473206 = 0.3857540488243103 + 0.01 * 6.950994491577148
Epoch 510, val loss: 0.7768632769584656
Epoch 520, training loss: 0.4384615123271942 = 0.3689793646335602 + 0.01 * 6.948214530944824
Epoch 520, val loss: 0.7750388383865356
Epoch 530, training loss: 0.4223833680152893 = 0.3529343008995056 + 0.01 * 6.944908618927002
Epoch 530, val loss: 0.7741302251815796
Epoch 540, training loss: 0.40693289041519165 = 0.3375193476676941 + 0.01 * 6.941352844238281
Epoch 540, val loss: 0.7740821838378906
Epoch 550, training loss: 0.39201292395591736 = 0.3226377069950104 + 0.01 * 6.937521457672119
Epoch 550, val loss: 0.7747646570205688
Epoch 560, training loss: 0.3775709569454193 = 0.3082117736339569 + 0.01 * 6.935917377471924
Epoch 560, val loss: 0.7760928273200989
Epoch 570, training loss: 0.3634832799434662 = 0.29416677355766296 + 0.01 * 6.931650638580322
Epoch 570, val loss: 0.7779808640480042
Epoch 580, training loss: 0.3496939241886139 = 0.280426561832428 + 0.01 * 6.926737308502197
Epoch 580, val loss: 0.7803666591644287
Epoch 590, training loss: 0.33615565299987793 = 0.2669336199760437 + 0.01 * 6.922201633453369
Epoch 590, val loss: 0.7831907272338867
Epoch 600, training loss: 0.32283973693847656 = 0.2536472976207733 + 0.01 * 6.91924524307251
Epoch 600, val loss: 0.786405622959137
Epoch 610, training loss: 0.30969688296318054 = 0.24055522680282593 + 0.01 * 6.914165496826172
Epoch 610, val loss: 0.7899947762489319
Epoch 620, training loss: 0.29677170515060425 = 0.22766779363155365 + 0.01 * 6.910391330718994
Epoch 620, val loss: 0.7938820719718933
Epoch 630, training loss: 0.28413936495780945 = 0.2150285243988037 + 0.01 * 6.911084175109863
Epoch 630, val loss: 0.7980973720550537
Epoch 640, training loss: 0.27176082134246826 = 0.20271432399749756 + 0.01 * 6.904651641845703
Epoch 640, val loss: 0.8026876449584961
Epoch 650, training loss: 0.25980809330940247 = 0.1907968521118164 + 0.01 * 6.901124477386475
Epoch 650, val loss: 0.807604968547821
Epoch 660, training loss: 0.24833190441131592 = 0.17936038970947266 + 0.01 * 6.897151947021484
Epoch 660, val loss: 0.8129315972328186
Epoch 670, training loss: 0.23742231726646423 = 0.16847115755081177 + 0.01 * 6.895115852355957
Epoch 670, val loss: 0.818708598613739
Epoch 680, training loss: 0.22703315317630768 = 0.1581713855266571 + 0.01 * 6.886177062988281
Epoch 680, val loss: 0.8249350190162659
Epoch 690, training loss: 0.2173190414905548 = 0.14847126603126526 + 0.01 * 6.884777545928955
Epoch 690, val loss: 0.8316237330436707
Epoch 700, training loss: 0.20817843079566956 = 0.1393783688545227 + 0.01 * 6.880006790161133
Epoch 700, val loss: 0.838814914226532
Epoch 710, training loss: 0.1996271312236786 = 0.130879208445549 + 0.01 * 6.874792098999023
Epoch 710, val loss: 0.8464464545249939
Epoch 720, training loss: 0.19167590141296387 = 0.12295264005661011 + 0.01 * 6.872325420379639
Epoch 720, val loss: 0.8544929027557373
Epoch 730, training loss: 0.1842637062072754 = 0.11557478457689285 + 0.01 * 6.868892192840576
Epoch 730, val loss: 0.8629346489906311
Epoch 740, training loss: 0.1774463951587677 = 0.10871340334415436 + 0.01 * 6.8732991218566895
Epoch 740, val loss: 0.8716687560081482
Epoch 750, training loss: 0.17101269960403442 = 0.1023460254073143 + 0.01 * 6.866666793823242
Epoch 750, val loss: 0.8806819319725037
Epoch 760, training loss: 0.1650327444076538 = 0.09643329679965973 + 0.01 * 6.859944820404053
Epoch 760, val loss: 0.8899146914482117
Epoch 770, training loss: 0.15955466032028198 = 0.09094096720218658 + 0.01 * 6.86137056350708
Epoch 770, val loss: 0.8993657827377319
Epoch 780, training loss: 0.15443208813667297 = 0.08583971112966537 + 0.01 * 6.8592376708984375
Epoch 780, val loss: 0.908958911895752
Epoch 790, training loss: 0.14959698915481567 = 0.0810970813035965 + 0.01 * 6.849991321563721
Epoch 790, val loss: 0.9186483025550842
Epoch 800, training loss: 0.14532062411308289 = 0.07668313384056091 + 0.01 * 6.863749980926514
Epoch 800, val loss: 0.9284071326255798
Epoch 810, training loss: 0.1410638391971588 = 0.07257688790559769 + 0.01 * 6.848694801330566
Epoch 810, val loss: 0.9382168650627136
Epoch 820, training loss: 0.1371830552816391 = 0.06875080615282059 + 0.01 * 6.843225002288818
Epoch 820, val loss: 0.9479982852935791
Epoch 830, training loss: 0.13358226418495178 = 0.06518014520406723 + 0.01 * 6.840211391448975
Epoch 830, val loss: 0.9578242301940918
Epoch 840, training loss: 0.13027970492839813 = 0.06184627488255501 + 0.01 * 6.843343257904053
Epoch 840, val loss: 0.9676024317741394
Epoch 850, training loss: 0.12713435292243958 = 0.05873342975974083 + 0.01 * 6.84009313583374
Epoch 850, val loss: 0.9773385524749756
Epoch 860, training loss: 0.12416861951351166 = 0.05582284554839134 + 0.01 * 6.8345770835876465
Epoch 860, val loss: 0.9870118498802185
Epoch 870, training loss: 0.12143426388502121 = 0.05309890955686569 + 0.01 * 6.833535671234131
Epoch 870, val loss: 0.9966196417808533
Epoch 880, training loss: 0.11880842596292496 = 0.05054949223995209 + 0.01 * 6.825893402099609
Epoch 880, val loss: 1.0061559677124023
Epoch 890, training loss: 0.11640346050262451 = 0.04816100373864174 + 0.01 * 6.824245929718018
Epoch 890, val loss: 1.0155956745147705
Epoch 900, training loss: 0.11417917907238007 = 0.04592062160372734 + 0.01 * 6.825855731964111
Epoch 900, val loss: 1.0249298810958862
Epoch 910, training loss: 0.112147256731987 = 0.04381781071424484 + 0.01 * 6.832944393157959
Epoch 910, val loss: 1.0341699123382568
Epoch 920, training loss: 0.1099979430437088 = 0.041843343526124954 + 0.01 * 6.815460681915283
Epoch 920, val loss: 1.0433082580566406
Epoch 930, training loss: 0.10829885303974152 = 0.039986904710531235 + 0.01 * 6.83119535446167
Epoch 930, val loss: 1.0523123741149902
Epoch 940, training loss: 0.10639950633049011 = 0.03824186697602272 + 0.01 * 6.815763473510742
Epoch 940, val loss: 1.061170220375061
Epoch 950, training loss: 0.1046624556183815 = 0.03659862279891968 + 0.01 * 6.8063836097717285
Epoch 950, val loss: 1.06992769241333
Epoch 960, training loss: 0.10322819650173187 = 0.035050082951784134 + 0.01 * 6.817811012268066
Epoch 960, val loss: 1.0785526037216187
Epoch 970, training loss: 0.10164982080459595 = 0.033591415733098984 + 0.01 * 6.805840015411377
Epoch 970, val loss: 1.0870193243026733
Epoch 980, training loss: 0.1001986563205719 = 0.0322151854634285 + 0.01 * 6.798347473144531
Epoch 980, val loss: 1.0953696966171265
Epoch 990, training loss: 0.09887155145406723 = 0.030915988609194756 + 0.01 * 6.795556545257568
Epoch 990, val loss: 1.1035840511322021
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7703703703703704
0.8276225619399051
=== training gcn model ===
Epoch 0, training loss: 2.036724805831909 = 1.9507566690444946 + 0.01 * 8.596803665161133
Epoch 0, val loss: 1.9412518739700317
Epoch 10, training loss: 2.0317444801330566 = 1.9457765817642212 + 0.01 * 8.596778869628906
Epoch 10, val loss: 1.9361913204193115
Epoch 20, training loss: 2.026401996612549 = 1.9404349327087402 + 0.01 * 8.596715927124023
Epoch 20, val loss: 1.930731177330017
Epoch 30, training loss: 2.0202746391296387 = 1.9343087673187256 + 0.01 * 8.59659481048584
Epoch 30, val loss: 1.9244972467422485
Epoch 40, training loss: 2.012857437133789 = 1.9268938302993774 + 0.01 * 8.596349716186523
Epoch 40, val loss: 1.9170057773590088
Epoch 50, training loss: 2.0034549236297607 = 1.9174973964691162 + 0.01 * 8.59575366973877
Epoch 50, val loss: 1.9075895547866821
Epoch 60, training loss: 1.9912158250808716 = 1.9052770137786865 + 0.01 * 8.593886375427246
Epoch 60, val loss: 1.895464539527893
Epoch 70, training loss: 1.975210428237915 = 1.889344334602356 + 0.01 * 8.586614608764648
Epoch 70, val loss: 1.8799397945404053
Epoch 80, training loss: 1.9547232389450073 = 1.8691262006759644 + 0.01 * 8.559706687927246
Epoch 80, val loss: 1.8607276678085327
Epoch 90, training loss: 1.9297394752502441 = 1.8449689149856567 + 0.01 * 8.477051734924316
Epoch 90, val loss: 1.8386938571929932
Epoch 100, training loss: 1.9006597995758057 = 1.818670630455017 + 0.01 * 8.198914527893066
Epoch 100, val loss: 1.8160167932510376
Epoch 110, training loss: 1.873051643371582 = 1.7927441596984863 + 0.01 * 8.030750274658203
Epoch 110, val loss: 1.7950646877288818
Epoch 120, training loss: 1.8474887609481812 = 1.7680469751358032 + 0.01 * 7.944177150726318
Epoch 120, val loss: 1.7756820917129517
Epoch 130, training loss: 1.8199622631072998 = 1.741969108581543 + 0.01 * 7.799318313598633
Epoch 130, val loss: 1.7541484832763672
Epoch 140, training loss: 1.7874137163162231 = 1.7113229036331177 + 0.01 * 7.609086513519287
Epoch 140, val loss: 1.7275969982147217
Epoch 150, training loss: 1.7487189769744873 = 1.6745517253875732 + 0.01 * 7.416721343994141
Epoch 150, val loss: 1.695568323135376
Epoch 160, training loss: 1.7036172151565552 = 1.6311440467834473 + 0.01 * 7.247316837310791
Epoch 160, val loss: 1.6584792137145996
Epoch 170, training loss: 1.6534321308135986 = 1.5818941593170166 + 0.01 * 7.153791904449463
Epoch 170, val loss: 1.617352843284607
Epoch 180, training loss: 1.6008226871490479 = 1.529515266418457 + 0.01 * 7.130738258361816
Epoch 180, val loss: 1.574594259262085
Epoch 190, training loss: 1.5481904745101929 = 1.4771206378936768 + 0.01 * 7.106983184814453
Epoch 190, val loss: 1.5327112674713135
Epoch 200, training loss: 1.4977961778640747 = 1.4268593788146973 + 0.01 * 7.093678951263428
Epoch 200, val loss: 1.4931182861328125
Epoch 210, training loss: 1.4504003524780273 = 1.379562497138977 + 0.01 * 7.083784580230713
Epoch 210, val loss: 1.4565142393112183
Epoch 220, training loss: 1.4058997631072998 = 1.3351467847824097 + 0.01 * 7.075298309326172
Epoch 220, val loss: 1.422499418258667
Epoch 230, training loss: 1.3638982772827148 = 1.2932301759719849 + 0.01 * 7.0668134689331055
Epoch 230, val loss: 1.3907326459884644
Epoch 240, training loss: 1.323923110961914 = 1.2533475160598755 + 0.01 * 7.057558536529541
Epoch 240, val loss: 1.3606187105178833
Epoch 250, training loss: 1.2855255603790283 = 1.2150609493255615 + 0.01 * 7.046457290649414
Epoch 250, val loss: 1.3318970203399658
Epoch 260, training loss: 1.2482938766479492 = 1.1779597997665405 + 0.01 * 7.033407688140869
Epoch 260, val loss: 1.3042405843734741
Epoch 270, training loss: 1.2118926048278809 = 1.1417092084884644 + 0.01 * 7.018344879150391
Epoch 270, val loss: 1.27731192111969
Epoch 280, training loss: 1.1760579347610474 = 1.1060227155685425 + 0.01 * 7.003519535064697
Epoch 280, val loss: 1.2508330345153809
Epoch 290, training loss: 1.1406285762786865 = 1.0707404613494873 + 0.01 * 6.988807678222656
Epoch 290, val loss: 1.2243852615356445
Epoch 300, training loss: 1.105535626411438 = 1.0357661247253418 + 0.01 * 6.976953506469727
Epoch 300, val loss: 1.197935938835144
Epoch 310, training loss: 1.0708268880844116 = 1.0011677742004395 + 0.01 * 6.965906620025635
Epoch 310, val loss: 1.171610951423645
Epoch 320, training loss: 1.0365996360778809 = 0.9670259952545166 + 0.01 * 6.957368850708008
Epoch 320, val loss: 1.1452528238296509
Epoch 330, training loss: 1.0027750730514526 = 0.933297336101532 + 0.01 * 6.947778224945068
Epoch 330, val loss: 1.119064450263977
Epoch 340, training loss: 0.9692611694335938 = 0.8998632431030273 + 0.01 * 6.939795017242432
Epoch 340, val loss: 1.0928798913955688
Epoch 350, training loss: 0.9358675479888916 = 0.8665492534637451 + 0.01 * 6.931830883026123
Epoch 350, val loss: 1.0665861368179321
Epoch 360, training loss: 0.902413010597229 = 0.8331708908081055 + 0.01 * 6.924212455749512
Epoch 360, val loss: 1.0402668714523315
Epoch 370, training loss: 0.8687527775764465 = 0.7995778918266296 + 0.01 * 6.917486667633057
Epoch 370, val loss: 1.0137202739715576
Epoch 380, training loss: 0.8348106145858765 = 0.7657062411308289 + 0.01 * 6.910434722900391
Epoch 380, val loss: 0.9870843887329102
Epoch 390, training loss: 0.800682008266449 = 0.7316344976425171 + 0.01 * 6.904751300811768
Epoch 390, val loss: 0.9604556560516357
Epoch 400, training loss: 0.7665128707885742 = 0.6975312829017639 + 0.01 * 6.898161888122559
Epoch 400, val loss: 0.9342107772827148
Epoch 410, training loss: 0.7325007319450378 = 0.6635789275169373 + 0.01 * 6.892178535461426
Epoch 410, val loss: 0.9085683822631836
Epoch 420, training loss: 0.6989368200302124 = 0.6300261616706848 + 0.01 * 6.891063213348389
Epoch 420, val loss: 0.8839358687400818
Epoch 430, training loss: 0.6659928560256958 = 0.5971347689628601 + 0.01 * 6.885811805725098
Epoch 430, val loss: 0.8605173826217651
Epoch 440, training loss: 0.633888840675354 = 0.5650942325592041 + 0.01 * 6.879459857940674
Epoch 440, val loss: 0.8386151194572449
Epoch 450, training loss: 0.6028662919998169 = 0.534111499786377 + 0.01 * 6.875478267669678
Epoch 450, val loss: 0.8183370232582092
Epoch 460, training loss: 0.5733602643013 = 0.5044159293174744 + 0.01 * 6.894432067871094
Epoch 460, val loss: 0.7998776435852051
Epoch 470, training loss: 0.5449638366699219 = 0.47620299458503723 + 0.01 * 6.8760833740234375
Epoch 470, val loss: 0.7831977605819702
Epoch 480, training loss: 0.5182259678840637 = 0.44955191016197205 + 0.01 * 6.867406845092773
Epoch 480, val loss: 0.7684928178787231
Epoch 490, training loss: 0.4931415319442749 = 0.4244946837425232 + 0.01 * 6.864684581756592
Epoch 490, val loss: 0.7556061148643494
Epoch 500, training loss: 0.469649076461792 = 0.40102824568748474 + 0.01 * 6.86208438873291
Epoch 500, val loss: 0.7445087432861328
Epoch 510, training loss: 0.44770148396492004 = 0.37910276651382446 + 0.01 * 6.859870910644531
Epoch 510, val loss: 0.7351794838905334
Epoch 520, training loss: 0.42721834778785706 = 0.35864201188087463 + 0.01 * 6.857633590698242
Epoch 520, val loss: 0.7275299429893494
Epoch 530, training loss: 0.4082084894180298 = 0.33954116702079773 + 0.01 * 6.866731643676758
Epoch 530, val loss: 0.7214149832725525
Epoch 540, training loss: 0.39023855328559875 = 0.3216894865036011 + 0.01 * 6.85490608215332
Epoch 540, val loss: 0.7167165279388428
Epoch 550, training loss: 0.37347760796546936 = 0.30494335293769836 + 0.01 * 6.853425979614258
Epoch 550, val loss: 0.7132660746574402
Epoch 560, training loss: 0.3576825261116028 = 0.28917112946510315 + 0.01 * 6.851140975952148
Epoch 560, val loss: 0.7109006643295288
Epoch 570, training loss: 0.3428594470024109 = 0.27425751090049744 + 0.01 * 6.860194683074951
Epoch 570, val loss: 0.7095082998275757
Epoch 580, training loss: 0.3285835385322571 = 0.26010778546333313 + 0.01 * 6.84757661819458
Epoch 580, val loss: 0.7089345455169678
Epoch 590, training loss: 0.3151039481163025 = 0.2466249316930771 + 0.01 * 6.847901821136475
Epoch 590, val loss: 0.7090785503387451
Epoch 600, training loss: 0.3021817207336426 = 0.23373231291770935 + 0.01 * 6.844939708709717
Epoch 600, val loss: 0.7098417282104492
Epoch 610, training loss: 0.28981146216392517 = 0.2213699072599411 + 0.01 * 6.844156742095947
Epoch 610, val loss: 0.7110975384712219
Epoch 620, training loss: 0.27793097496032715 = 0.20950208604335785 + 0.01 * 6.8428874015808105
Epoch 620, val loss: 0.7128105163574219
Epoch 630, training loss: 0.2665177583694458 = 0.19809991121292114 + 0.01 * 6.841785907745361
Epoch 630, val loss: 0.7149347066879272
Epoch 640, training loss: 0.2555786669254303 = 0.1871459037065506 + 0.01 * 6.843276023864746
Epoch 640, val loss: 0.7173925638198853
Epoch 650, training loss: 0.24501335620880127 = 0.17663654685020447 + 0.01 * 6.837681293487549
Epoch 650, val loss: 0.7201334834098816
Epoch 660, training loss: 0.23493218421936035 = 0.16656765341758728 + 0.01 * 6.836453914642334
Epoch 660, val loss: 0.7231353521347046
Epoch 670, training loss: 0.22529417276382446 = 0.15693940222263336 + 0.01 * 6.835477828979492
Epoch 670, val loss: 0.7263620495796204
Epoch 680, training loss: 0.21609176695346832 = 0.1477520763874054 + 0.01 * 6.8339691162109375
Epoch 680, val loss: 0.7298161387443542
Epoch 690, training loss: 0.20730198919773102 = 0.13900454342365265 + 0.01 * 6.829744815826416
Epoch 690, val loss: 0.7334744930267334
Epoch 700, training loss: 0.19925352931022644 = 0.13069386780261993 + 0.01 * 6.855965614318848
Epoch 700, val loss: 0.7373251914978027
Epoch 710, training loss: 0.19118094444274902 = 0.1228315457701683 + 0.01 * 6.834939002990723
Epoch 710, val loss: 0.7413191199302673
Epoch 720, training loss: 0.18366670608520508 = 0.11540102958679199 + 0.01 * 6.82656717300415
Epoch 720, val loss: 0.7454773783683777
Epoch 730, training loss: 0.17660728096961975 = 0.10838789492845535 + 0.01 * 6.8219380378723145
Epoch 730, val loss: 0.7497991323471069
Epoch 740, training loss: 0.1699690818786621 = 0.10178163647651672 + 0.01 * 6.818743705749512
Epoch 740, val loss: 0.7542727589607239
Epoch 750, training loss: 0.16379347443580627 = 0.09557037800550461 + 0.01 * 6.822309970855713
Epoch 750, val loss: 0.7588804960250854
Epoch 760, training loss: 0.15789005160331726 = 0.08974180370569229 + 0.01 * 6.814825057983398
Epoch 760, val loss: 0.7636184096336365
Epoch 770, training loss: 0.15238982439041138 = 0.08427875488996506 + 0.01 * 6.811107635498047
Epoch 770, val loss: 0.768495500087738
Epoch 780, training loss: 0.14742255210876465 = 0.07916706055402756 + 0.01 * 6.8255486488342285
Epoch 780, val loss: 0.7734825611114502
Epoch 790, training loss: 0.14252352714538574 = 0.07439851760864258 + 0.01 * 6.812501430511475
Epoch 790, val loss: 0.7785264849662781
Epoch 800, training loss: 0.1379999965429306 = 0.0699504017829895 + 0.01 * 6.804959774017334
Epoch 800, val loss: 0.7836554646492004
Epoch 810, training loss: 0.1338074803352356 = 0.06580352783203125 + 0.01 * 6.8003950119018555
Epoch 810, val loss: 0.7888908386230469
Epoch 820, training loss: 0.13005627691745758 = 0.06194046512246132 + 0.01 * 6.811581134796143
Epoch 820, val loss: 0.7941880822181702
Epoch 830, training loss: 0.1262982189655304 = 0.05834859982132912 + 0.01 * 6.794961929321289
Epoch 830, val loss: 0.7995248436927795
Epoch 840, training loss: 0.1229286789894104 = 0.05500750616192818 + 0.01 * 6.792117595672607
Epoch 840, val loss: 0.804931640625
Epoch 850, training loss: 0.11977717280387878 = 0.05190017446875572 + 0.01 * 6.787700653076172
Epoch 850, val loss: 0.810383677482605
Epoch 860, training loss: 0.11701066046953201 = 0.04901131987571716 + 0.01 * 6.799934387207031
Epoch 860, val loss: 0.8158826231956482
Epoch 870, training loss: 0.11417820304632187 = 0.04632877558469772 + 0.01 * 6.784942626953125
Epoch 870, val loss: 0.8213808536529541
Epoch 880, training loss: 0.1116168424487114 = 0.043835632503032684 + 0.01 * 6.778120994567871
Epoch 880, val loss: 0.826907217502594
Epoch 890, training loss: 0.10942767560482025 = 0.041517473757267 + 0.01 * 6.791020393371582
Epoch 890, val loss: 0.8324567079544067
Epoch 900, training loss: 0.10714996606111526 = 0.03936304152011871 + 0.01 * 6.778692245483398
Epoch 900, val loss: 0.8379955291748047
Epoch 910, training loss: 0.10506315529346466 = 0.03735807538032532 + 0.01 * 6.7705078125
Epoch 910, val loss: 0.8435277938842773
Epoch 920, training loss: 0.10316364467144012 = 0.03549135476350784 + 0.01 * 6.767229080200195
Epoch 920, val loss: 0.8490435481071472
Epoch 930, training loss: 0.10155107080936432 = 0.03375400975346565 + 0.01 * 6.7797064781188965
Epoch 930, val loss: 0.854549765586853
Epoch 940, training loss: 0.09977149963378906 = 0.03213686868548393 + 0.01 * 6.763463973999023
Epoch 940, val loss: 0.8600022196769714
Epoch 950, training loss: 0.09825506061315536 = 0.030628962442278862 + 0.01 * 6.762609958648682
Epoch 950, val loss: 0.8654415607452393
Epoch 960, training loss: 0.09673449397087097 = 0.02922186069190502 + 0.01 * 6.751263618469238
Epoch 960, val loss: 0.870832085609436
Epoch 970, training loss: 0.09551436454057693 = 0.027907148003578186 + 0.01 * 6.760721683502197
Epoch 970, val loss: 0.876183807849884
Epoch 980, training loss: 0.09415091574192047 = 0.026678452268242836 + 0.01 * 6.747246742248535
Epoch 980, val loss: 0.8814867734909058
Epoch 990, training loss: 0.09302452951669693 = 0.02552855759859085 + 0.01 * 6.749597549438477
Epoch 990, val loss: 0.8867442011833191
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7925925925925926
0.8386926726410122
=== training gcn model ===
Epoch 0, training loss: 2.0222363471984863 = 1.9362679719924927 + 0.01 * 8.596845626831055
Epoch 0, val loss: 1.941610336303711
Epoch 10, training loss: 2.0175797939300537 = 1.9316115379333496 + 0.01 * 8.596826553344727
Epoch 10, val loss: 1.9366949796676636
Epoch 20, training loss: 2.012479782104492 = 1.9265121221542358 + 0.01 * 8.596770286560059
Epoch 20, val loss: 1.931309700012207
Epoch 30, training loss: 2.006465435028076 = 1.9204988479614258 + 0.01 * 8.596660614013672
Epoch 30, val loss: 1.924978494644165
Epoch 40, training loss: 1.9990649223327637 = 1.9131006002426147 + 0.01 * 8.59643268585205
Epoch 40, val loss: 1.9171802997589111
Epoch 50, training loss: 1.9896776676177979 = 1.9037188291549683 + 0.01 * 8.595882415771484
Epoch 50, val loss: 1.9072473049163818
Epoch 60, training loss: 1.977628231048584 = 1.891685962677002 + 0.01 * 8.594231605529785
Epoch 60, val loss: 1.8945010900497437
Epoch 70, training loss: 1.9622453451156616 = 1.8763630390167236 + 0.01 * 8.588226318359375
Epoch 70, val loss: 1.8784143924713135
Epoch 80, training loss: 1.9431583881378174 = 1.857489824295044 + 0.01 * 8.566854476928711
Epoch 80, val loss: 1.8589547872543335
Epoch 90, training loss: 1.920668125152588 = 1.8355822563171387 + 0.01 * 8.50859260559082
Epoch 90, val loss: 1.837080478668213
Epoch 100, training loss: 1.8957459926605225 = 1.8122481107711792 + 0.01 * 8.3497896194458
Epoch 100, val loss: 1.8149343729019165
Epoch 110, training loss: 1.8709098100662231 = 1.7894288301467896 + 0.01 * 8.148099899291992
Epoch 110, val loss: 1.7944906949996948
Epoch 120, training loss: 1.8480653762817383 = 1.767233967781067 + 0.01 * 8.083137512207031
Epoch 120, val loss: 1.7754151821136475
Epoch 130, training loss: 1.8227283954620361 = 1.7431665658950806 + 0.01 * 7.956178665161133
Epoch 130, val loss: 1.75477135181427
Epoch 140, training loss: 1.7924467325210571 = 1.7147821187973022 + 0.01 * 7.76646089553833
Epoch 140, val loss: 1.7300299406051636
Epoch 150, training loss: 1.7567594051361084 = 1.68095064163208 + 0.01 * 7.580881595611572
Epoch 150, val loss: 1.7003707885742188
Epoch 160, training loss: 1.7154231071472168 = 1.6410675048828125 + 0.01 * 7.435558319091797
Epoch 160, val loss: 1.6659258604049683
Epoch 170, training loss: 1.668539047241211 = 1.5952239036560059 + 0.01 * 7.331509113311768
Epoch 170, val loss: 1.6268755197525024
Epoch 180, training loss: 1.6167768239974976 = 1.5441325902938843 + 0.01 * 7.264423370361328
Epoch 180, val loss: 1.5838229656219482
Epoch 190, training loss: 1.5619316101074219 = 1.4897034168243408 + 0.01 * 7.2228169441223145
Epoch 190, val loss: 1.5384495258331299
Epoch 200, training loss: 1.5059608221054077 = 1.4340322017669678 + 0.01 * 7.192860126495361
Epoch 200, val loss: 1.492936134338379
Epoch 210, training loss: 1.4500466585159302 = 1.3783667087554932 + 0.01 * 7.168000221252441
Epoch 210, val loss: 1.4477488994598389
Epoch 220, training loss: 1.3949651718139648 = 1.3234766721725464 + 0.01 * 7.1488471031188965
Epoch 220, val loss: 1.4035975933074951
Epoch 230, training loss: 1.341097116470337 = 1.2697490453720093 + 0.01 * 7.13481330871582
Epoch 230, val loss: 1.3606698513031006
Epoch 240, training loss: 1.2888031005859375 = 1.2175601720809937 + 0.01 * 7.124297142028809
Epoch 240, val loss: 1.3191461563110352
Epoch 250, training loss: 1.2386510372161865 = 1.1674972772598267 + 0.01 * 7.115379810333252
Epoch 250, val loss: 1.27969491481781
Epoch 260, training loss: 1.191175937652588 = 1.1201132535934448 + 0.01 * 7.106265068054199
Epoch 260, val loss: 1.2426234483718872
Epoch 270, training loss: 1.1466299295425415 = 1.0756746530532837 + 0.01 * 7.0955328941345215
Epoch 270, val loss: 1.2081527709960938
Epoch 280, training loss: 1.1049518585205078 = 1.034126877784729 + 0.01 * 7.082497596740723
Epoch 280, val loss: 1.1760709285736084
Epoch 290, training loss: 1.0658286809921265 = 0.9951600432395935 + 0.01 * 7.066859245300293
Epoch 290, val loss: 1.146108627319336
Epoch 300, training loss: 1.0287084579467773 = 0.9582188129425049 + 0.01 * 7.048966407775879
Epoch 300, val loss: 1.1177090406417847
Epoch 310, training loss: 0.9929925799369812 = 0.9226627945899963 + 0.01 * 7.032979965209961
Epoch 310, val loss: 1.090232014656067
Epoch 320, training loss: 0.9581208825111389 = 0.8879263997077942 + 0.01 * 7.0194501876831055
Epoch 320, val loss: 1.0633493661880493
Epoch 330, training loss: 0.9236564636230469 = 0.8535804152488708 + 0.01 * 7.007606029510498
Epoch 330, val loss: 1.0365214347839355
Epoch 340, training loss: 0.8893095850944519 = 0.8193350434303284 + 0.01 * 6.99745512008667
Epoch 340, val loss: 1.0096380710601807
Epoch 350, training loss: 0.8549731969833374 = 0.7850717306137085 + 0.01 * 6.990147590637207
Epoch 350, val loss: 0.9825878143310547
Epoch 360, training loss: 0.8206801414489746 = 0.750838577747345 + 0.01 * 6.984157085418701
Epoch 360, val loss: 0.9556892514228821
Epoch 370, training loss: 0.7865662574768066 = 0.7167806625366211 + 0.01 * 6.978561878204346
Epoch 370, val loss: 0.9289987683296204
Epoch 380, training loss: 0.7528598308563232 = 0.6831283569335938 + 0.01 * 6.973146438598633
Epoch 380, val loss: 0.9029764533042908
Epoch 390, training loss: 0.719785749912262 = 0.6500794887542725 + 0.01 * 6.970626354217529
Epoch 390, val loss: 0.8779929876327515
Epoch 400, training loss: 0.6874254941940308 = 0.6177920699119568 + 0.01 * 6.963345050811768
Epoch 400, val loss: 0.8542104959487915
Epoch 410, training loss: 0.655890941619873 = 0.5863130688667297 + 0.01 * 6.957788467407227
Epoch 410, val loss: 0.8318769335746765
Epoch 420, training loss: 0.6251197457313538 = 0.5555989742279053 + 0.01 * 6.9520769119262695
Epoch 420, val loss: 0.8109992742538452
Epoch 430, training loss: 0.5950429439544678 = 0.5255630612373352 + 0.01 * 6.947990417480469
Epoch 430, val loss: 0.7915414571762085
Epoch 440, training loss: 0.5655039548873901 = 0.4960966408252716 + 0.01 * 6.94072961807251
Epoch 440, val loss: 0.7733528017997742
Epoch 450, training loss: 0.5364612340927124 = 0.4671020209789276 + 0.01 * 6.93591833114624
Epoch 450, val loss: 0.7562964558601379
Epoch 460, training loss: 0.507803201675415 = 0.43851396441459656 + 0.01 * 6.928921222686768
Epoch 460, val loss: 0.7403119206428528
Epoch 470, training loss: 0.47956860065460205 = 0.41032907366752625 + 0.01 * 6.923951148986816
Epoch 470, val loss: 0.7252181172370911
Epoch 480, training loss: 0.4518330693244934 = 0.38262268900871277 + 0.01 * 6.921039581298828
Epoch 480, val loss: 0.7109668254852295
Epoch 490, training loss: 0.42471837997436523 = 0.35556384921073914 + 0.01 * 6.915451526641846
Epoch 490, val loss: 0.6976892352104187
Epoch 500, training loss: 0.39845943450927734 = 0.32935887575149536 + 0.01 * 6.910055160522461
Epoch 500, val loss: 0.6852841377258301
Epoch 510, training loss: 0.3732989728450775 = 0.3042280673980713 + 0.01 * 6.907090663909912
Epoch 510, val loss: 0.6739485859870911
Epoch 520, training loss: 0.34940582513809204 = 0.2803860008716583 + 0.01 * 6.901983737945557
Epoch 520, val loss: 0.663754940032959
Epoch 530, training loss: 0.3269919157028198 = 0.25800803303718567 + 0.01 * 6.898388862609863
Epoch 530, val loss: 0.6547805070877075
Epoch 540, training loss: 0.30620861053466797 = 0.23723360896110535 + 0.01 * 6.897500991821289
Epoch 540, val loss: 0.6470580697059631
Epoch 550, training loss: 0.28703635931015015 = 0.2181214988231659 + 0.01 * 6.89148473739624
Epoch 550, val loss: 0.6406793594360352
Epoch 560, training loss: 0.2695649564266205 = 0.20065844058990479 + 0.01 * 6.890650749206543
Epoch 560, val loss: 0.635626494884491
Epoch 570, training loss: 0.2536843419075012 = 0.18480001389980316 + 0.01 * 6.888432502746582
Epoch 570, val loss: 0.6318455338478088
Epoch 580, training loss: 0.23929846286773682 = 0.17044520378112793 + 0.01 * 6.885324954986572
Epoch 580, val loss: 0.6292593479156494
Epoch 590, training loss: 0.22627855837345123 = 0.1574692577123642 + 0.01 * 6.880929946899414
Epoch 590, val loss: 0.6277530193328857
Epoch 600, training loss: 0.21463091671466827 = 0.14573973417282104 + 0.01 * 6.889118671417236
Epoch 600, val loss: 0.6272353529930115
Epoch 610, training loss: 0.20394253730773926 = 0.13513392210006714 + 0.01 * 6.880860805511475
Epoch 610, val loss: 0.6275931000709534
Epoch 620, training loss: 0.19427448511123657 = 0.125519260764122 + 0.01 * 6.875523567199707
Epoch 620, val loss: 0.6287332773208618
Epoch 630, training loss: 0.1854998618364334 = 0.1167798861861229 + 0.01 * 6.871997833251953
Epoch 630, val loss: 0.6305614113807678
Epoch 640, training loss: 0.1775711476802826 = 0.10881926864385605 + 0.01 * 6.875187873840332
Epoch 640, val loss: 0.6329637765884399
Epoch 650, training loss: 0.170238196849823 = 0.10155277699232101 + 0.01 * 6.8685431480407715
Epoch 650, val loss: 0.6358651518821716
Epoch 660, training loss: 0.16358357667922974 = 0.09490344673395157 + 0.01 * 6.868013858795166
Epoch 660, val loss: 0.6391923427581787
Epoch 670, training loss: 0.15744096040725708 = 0.0888061374425888 + 0.01 * 6.863481521606445
Epoch 670, val loss: 0.6429157257080078
Epoch 680, training loss: 0.15182837843894958 = 0.08320218324661255 + 0.01 * 6.862619876861572
Epoch 680, val loss: 0.6469534635543823
Epoch 690, training loss: 0.1466396450996399 = 0.07804153859615326 + 0.01 * 6.859811782836914
Epoch 690, val loss: 0.6512615084648132
Epoch 700, training loss: 0.1419474482536316 = 0.07327287644147873 + 0.01 * 6.867458343505859
Epoch 700, val loss: 0.6557855010032654
Epoch 710, training loss: 0.13745152950286865 = 0.06887923926115036 + 0.01 * 6.8572282791137695
Epoch 710, val loss: 0.6605135202407837
Epoch 720, training loss: 0.13335072994232178 = 0.06482070684432983 + 0.01 * 6.853002548217773
Epoch 720, val loss: 0.6654520630836487
Epoch 730, training loss: 0.1295798271894455 = 0.06106752157211304 + 0.01 * 6.851230621337891
Epoch 730, val loss: 0.670508861541748
Epoch 740, training loss: 0.12608830630779266 = 0.05759377032518387 + 0.01 * 6.849453449249268
Epoch 740, val loss: 0.6756801605224609
Epoch 750, training loss: 0.12283796072006226 = 0.054377708584070206 + 0.01 * 6.8460259437561035
Epoch 750, val loss: 0.6809646487236023
Epoch 760, training loss: 0.11982665956020355 = 0.051396604627370834 + 0.01 * 6.843006134033203
Epoch 760, val loss: 0.686285674571991
Epoch 770, training loss: 0.11705276370048523 = 0.04862895980477333 + 0.01 * 6.842381000518799
Epoch 770, val loss: 0.6916663646697998
Epoch 780, training loss: 0.11447736620903015 = 0.04605929180979729 + 0.01 * 6.841806888580322
Epoch 780, val loss: 0.6970810294151306
Epoch 790, training loss: 0.11199300736188889 = 0.04367022216320038 + 0.01 * 6.832278728485107
Epoch 790, val loss: 0.7025162577629089
Epoch 800, training loss: 0.10984568297863007 = 0.04144594445824623 + 0.01 * 6.839973449707031
Epoch 800, val loss: 0.707962691783905
Epoch 810, training loss: 0.10771149396896362 = 0.039375655353069305 + 0.01 * 6.833583831787109
Epoch 810, val loss: 0.7133881449699402
Epoch 820, training loss: 0.10575421154499054 = 0.03744565695524216 + 0.01 * 6.830855369567871
Epoch 820, val loss: 0.7188323140144348
Epoch 830, training loss: 0.10389994084835052 = 0.03564487397670746 + 0.01 * 6.825506687164307
Epoch 830, val loss: 0.7242463231086731
Epoch 840, training loss: 0.10217797756195068 = 0.03396301716566086 + 0.01 * 6.821496486663818
Epoch 840, val loss: 0.7296253442764282
Epoch 850, training loss: 0.10058172047138214 = 0.03238970786333084 + 0.01 * 6.819201469421387
Epoch 850, val loss: 0.7349898815155029
Epoch 860, training loss: 0.0991140604019165 = 0.03091724030673504 + 0.01 * 6.8196821212768555
Epoch 860, val loss: 0.7403166890144348
Epoch 870, training loss: 0.09763677418231964 = 0.02953771874308586 + 0.01 * 6.809905052185059
Epoch 870, val loss: 0.7455909848213196
Epoch 880, training loss: 0.09643399715423584 = 0.028243497014045715 + 0.01 * 6.819050312042236
Epoch 880, val loss: 0.7508167624473572
Epoch 890, training loss: 0.09507212042808533 = 0.027029331773519516 + 0.01 * 6.804279327392578
Epoch 890, val loss: 0.7560281157493591
Epoch 900, training loss: 0.09401429444551468 = 0.025888361036777496 + 0.01 * 6.812593460083008
Epoch 900, val loss: 0.7611593008041382
Epoch 910, training loss: 0.09282039850950241 = 0.024816008284687996 + 0.01 * 6.800439357757568
Epoch 910, val loss: 0.7662051916122437
Epoch 920, training loss: 0.09173689782619476 = 0.023806437849998474 + 0.01 * 6.793046474456787
Epoch 920, val loss: 0.7712135314941406
Epoch 930, training loss: 0.09074662625789642 = 0.02285618707537651 + 0.01 * 6.789044380187988
Epoch 930, val loss: 0.7761523127555847
Epoch 940, training loss: 0.0898602083325386 = 0.021961281076073647 + 0.01 * 6.78989315032959
Epoch 940, val loss: 0.7810288071632385
Epoch 950, training loss: 0.08914472162723541 = 0.021116532385349274 + 0.01 * 6.802818775177002
Epoch 950, val loss: 0.7858626842498779
Epoch 960, training loss: 0.08808798342943192 = 0.020319579169154167 + 0.01 * 6.776840686798096
Epoch 960, val loss: 0.7906314730644226
Epoch 970, training loss: 0.08758161962032318 = 0.01956631802022457 + 0.01 * 6.801529884338379
Epoch 970, val loss: 0.795324444770813
Epoch 980, training loss: 0.08658384531736374 = 0.018854660913348198 + 0.01 * 6.772918701171875
Epoch 980, val loss: 0.7999705076217651
Epoch 990, training loss: 0.08592496812343597 = 0.01818060129880905 + 0.01 * 6.774436950683594
Epoch 990, val loss: 0.8045765161514282
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8222222222222223
0.8323668950975225
The final CL Acc:0.79506, 0.02124, The final GNN Acc:0.83289, 0.00453
