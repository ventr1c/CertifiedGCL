Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=3, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 97484])
remove edge: torch.Size([2, 79824])
updated graph: torch.Size([2, 88660])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1059.3148193359375 = 1.087012529373169 + 100.0 * 10.582277297973633
Epoch 0, val loss: 1.0865591764450073
Epoch 10, training loss: 1059.2149658203125 = 1.0817759037017822 + 100.0 * 10.581332206726074
Epoch 10, val loss: 1.0813231468200684
Epoch 20, training loss: 1058.33251953125 = 1.075671911239624 + 100.0 * 10.5725679397583
Epoch 20, val loss: 1.075266718864441
Epoch 30, training loss: 1052.178955078125 = 1.0687174797058105 + 100.0 * 10.511102676391602
Epoch 30, val loss: 1.068381905555725
Epoch 40, training loss: 1027.80859375 = 1.0618468523025513 + 100.0 * 10.26746654510498
Epoch 40, val loss: 1.061819076538086
Epoch 50, training loss: 991.2076416015625 = 1.0566638708114624 + 100.0 * 9.901510238647461
Epoch 50, val loss: 1.0567576885223389
Epoch 60, training loss: 956.70361328125 = 1.0513916015625 + 100.0 * 9.556522369384766
Epoch 60, val loss: 1.0513688325881958
Epoch 70, training loss: 939.4857177734375 = 1.0439238548278809 + 100.0 * 9.384417533874512
Epoch 70, val loss: 1.0441259145736694
Epoch 80, training loss: 923.8106079101562 = 1.0364805459976196 + 100.0 * 9.227741241455078
Epoch 80, val loss: 1.0370910167694092
Epoch 90, training loss: 917.377685546875 = 1.0297157764434814 + 100.0 * 9.163479804992676
Epoch 90, val loss: 1.0306681394577026
Epoch 100, training loss: 909.2135009765625 = 1.0236092805862427 + 100.0 * 9.08189868927002
Epoch 100, val loss: 1.024804711341858
Epoch 110, training loss: 895.388916015625 = 1.01823890209198 + 100.0 * 8.943706512451172
Epoch 110, val loss: 1.019684910774231
Epoch 120, training loss: 886.0916748046875 = 1.0142675638198853 + 100.0 * 8.850773811340332
Epoch 120, val loss: 1.0156288146972656
Epoch 130, training loss: 881.1281127929688 = 1.0082612037658691 + 100.0 * 8.801198959350586
Epoch 130, val loss: 1.009617567062378
Epoch 140, training loss: 876.312255859375 = 1.000773310661316 + 100.0 * 8.753114700317383
Epoch 140, val loss: 1.0023695230484009
Epoch 150, training loss: 871.941650390625 = 0.9931761622428894 + 100.0 * 8.709485054016113
Epoch 150, val loss: 0.9950152635574341
Epoch 160, training loss: 869.7451171875 = 0.9854370355606079 + 100.0 * 8.687597274780273
Epoch 160, val loss: 0.9875738620758057
Epoch 170, training loss: 865.9351196289062 = 0.9764127135276794 + 100.0 * 8.64958667755127
Epoch 170, val loss: 0.9787355661392212
Epoch 180, training loss: 863.68115234375 = 0.9662122130393982 + 100.0 * 8.62714958190918
Epoch 180, val loss: 0.9688533544540405
Epoch 190, training loss: 861.5877685546875 = 0.9551196694374084 + 100.0 * 8.60632610321045
Epoch 190, val loss: 0.9581412076950073
Epoch 200, training loss: 859.8308715820312 = 0.9431850910186768 + 100.0 * 8.588876724243164
Epoch 200, val loss: 0.9466701745986938
Epoch 210, training loss: 858.32421875 = 0.9304893016815186 + 100.0 * 8.57393741607666
Epoch 210, val loss: 0.9344916343688965
Epoch 220, training loss: 858.0006713867188 = 0.9170464873313904 + 100.0 * 8.570836067199707
Epoch 220, val loss: 0.9215708374977112
Epoch 230, training loss: 856.25927734375 = 0.9024755358695984 + 100.0 * 8.553567886352539
Epoch 230, val loss: 0.9076564908027649
Epoch 240, training loss: 855.1911010742188 = 0.8875448703765869 + 100.0 * 8.543035507202148
Epoch 240, val loss: 0.893501341342926
Epoch 250, training loss: 854.2978515625 = 0.872366726398468 + 100.0 * 8.534255027770996
Epoch 250, val loss: 0.8791180849075317
Epoch 260, training loss: 853.3854370117188 = 0.8569360375404358 + 100.0 * 8.525284767150879
Epoch 260, val loss: 0.864536464214325
Epoch 270, training loss: 852.5796508789062 = 0.8412912487983704 + 100.0 * 8.517383575439453
Epoch 270, val loss: 0.8498029708862305
Epoch 280, training loss: 851.8153076171875 = 0.8255513310432434 + 100.0 * 8.509897232055664
Epoch 280, val loss: 0.8350537419319153
Epoch 290, training loss: 851.2517700195312 = 0.8098077774047852 + 100.0 * 8.504419326782227
Epoch 290, val loss: 0.8203392624855042
Epoch 300, training loss: 850.5247802734375 = 0.7938638925552368 + 100.0 * 8.497308731079102
Epoch 300, val loss: 0.8054865002632141
Epoch 310, training loss: 849.843505859375 = 0.7780874371528625 + 100.0 * 8.490653991699219
Epoch 310, val loss: 0.7908809781074524
Epoch 320, training loss: 849.2821655273438 = 0.7627465724945068 + 100.0 * 8.485194206237793
Epoch 320, val loss: 0.7766923308372498
Epoch 330, training loss: 849.3682250976562 = 0.7477374076843262 + 100.0 * 8.486205101013184
Epoch 330, val loss: 0.7628152370452881
Epoch 340, training loss: 848.196044921875 = 0.7329289317131042 + 100.0 * 8.474631309509277
Epoch 340, val loss: 0.7493561506271362
Epoch 350, training loss: 847.6884765625 = 0.7187102437019348 + 100.0 * 8.469697952270508
Epoch 350, val loss: 0.7364169955253601
Epoch 360, training loss: 847.23876953125 = 0.7050204277038574 + 100.0 * 8.465337753295898
Epoch 360, val loss: 0.7239775657653809
Epoch 370, training loss: 846.8819580078125 = 0.6917966604232788 + 100.0 * 8.461901664733887
Epoch 370, val loss: 0.7120990753173828
Epoch 380, training loss: 846.371337890625 = 0.6791059970855713 + 100.0 * 8.45692253112793
Epoch 380, val loss: 0.7007901668548584
Epoch 390, training loss: 846.004638671875 = 0.6671847701072693 + 100.0 * 8.453374862670898
Epoch 390, val loss: 0.6902363300323486
Epoch 400, training loss: 846.5801391601562 = 0.6558831334114075 + 100.0 * 8.459242820739746
Epoch 400, val loss: 0.6803374886512756
Epoch 410, training loss: 845.5390625 = 0.6451179385185242 + 100.0 * 8.448939323425293
Epoch 410, val loss: 0.670974612236023
Epoch 420, training loss: 844.9955444335938 = 0.6351874470710754 + 100.0 * 8.443603515625
Epoch 420, val loss: 0.6623873114585876
Epoch 430, training loss: 844.7274780273438 = 0.6260102987289429 + 100.0 * 8.441014289855957
Epoch 430, val loss: 0.6545330286026001
Epoch 440, training loss: 844.4168701171875 = 0.6174730062484741 + 100.0 * 8.437994003295898
Epoch 440, val loss: 0.6474230289459229
Epoch 450, training loss: 844.66259765625 = 0.609484851360321 + 100.0 * 8.440530776977539
Epoch 450, val loss: 0.640821635723114
Epoch 460, training loss: 844.1735229492188 = 0.6019249558448792 + 100.0 * 8.435715675354004
Epoch 460, val loss: 0.6347190141677856
Epoch 470, training loss: 843.6915893554688 = 0.595025897026062 + 100.0 * 8.430965423583984
Epoch 470, val loss: 0.6292344927787781
Epoch 480, training loss: 843.5821533203125 = 0.5886784791946411 + 100.0 * 8.42993450164795
Epoch 480, val loss: 0.6243925094604492
Epoch 490, training loss: 843.54248046875 = 0.5826749801635742 + 100.0 * 8.429597854614258
Epoch 490, val loss: 0.6197658777236938
Epoch 500, training loss: 843.0025024414062 = 0.5770713686943054 + 100.0 * 8.424254417419434
Epoch 500, val loss: 0.615577220916748
Epoch 510, training loss: 842.7437133789062 = 0.5719619393348694 + 100.0 * 8.421717643737793
Epoch 510, val loss: 0.6118447780609131
Epoch 520, training loss: 842.490234375 = 0.5672555565834045 + 100.0 * 8.419229507446289
Epoch 520, val loss: 0.6085618734359741
Epoch 530, training loss: 842.30078125 = 0.5628654956817627 + 100.0 * 8.417379379272461
Epoch 530, val loss: 0.605600118637085
Epoch 540, training loss: 842.8291625976562 = 0.558698832988739 + 100.0 * 8.422704696655273
Epoch 540, val loss: 0.602675199508667
Epoch 550, training loss: 842.3329467773438 = 0.5545595288276672 + 100.0 * 8.417783737182617
Epoch 550, val loss: 0.6002755165100098
Epoch 560, training loss: 841.9207153320312 = 0.5507796406745911 + 100.0 * 8.41369915008545
Epoch 560, val loss: 0.5977071523666382
Epoch 570, training loss: 841.643798828125 = 0.5473020076751709 + 100.0 * 8.410964965820312
Epoch 570, val loss: 0.5956774950027466
Epoch 580, training loss: 841.4525146484375 = 0.5440627932548523 + 100.0 * 8.40908432006836
Epoch 580, val loss: 0.593730628490448
Epoch 590, training loss: 841.4703369140625 = 0.5409862399101257 + 100.0 * 8.409293174743652
Epoch 590, val loss: 0.5919885635375977
Epoch 600, training loss: 841.9756469726562 = 0.5378870368003845 + 100.0 * 8.414377212524414
Epoch 600, val loss: 0.5904877185821533
Epoch 610, training loss: 841.0908813476562 = 0.5349067449569702 + 100.0 * 8.405559539794922
Epoch 610, val loss: 0.588605523109436
Epoch 620, training loss: 840.9048461914062 = 0.5321793556213379 + 100.0 * 8.403726577758789
Epoch 620, val loss: 0.5872388482093811
Epoch 630, training loss: 840.78271484375 = 0.5296592116355896 + 100.0 * 8.402530670166016
Epoch 630, val loss: 0.5859107375144958
Epoch 640, training loss: 840.6298217773438 = 0.5272505283355713 + 100.0 * 8.401025772094727
Epoch 640, val loss: 0.584784984588623
Epoch 650, training loss: 840.8328857421875 = 0.5249339938163757 + 100.0 * 8.40307903289795
Epoch 650, val loss: 0.5836533308029175
Epoch 660, training loss: 841.0426025390625 = 0.5225566625595093 + 100.0 * 8.405200004577637
Epoch 660, val loss: 0.5825098752975464
Epoch 670, training loss: 840.3187866210938 = 0.5202675461769104 + 100.0 * 8.397985458374023
Epoch 670, val loss: 0.5815214514732361
Epoch 680, training loss: 840.1580200195312 = 0.5181738138198853 + 100.0 * 8.396398544311523
Epoch 680, val loss: 0.5806348323822021
Epoch 690, training loss: 840.380126953125 = 0.516183614730835 + 100.0 * 8.398639678955078
Epoch 690, val loss: 0.5799199342727661
Epoch 700, training loss: 839.89501953125 = 0.5141812562942505 + 100.0 * 8.393808364868164
Epoch 700, val loss: 0.5788700580596924
Epoch 710, training loss: 839.8109130859375 = 0.5122835040092468 + 100.0 * 8.392986297607422
Epoch 710, val loss: 0.5780697464942932
Epoch 720, training loss: 839.7542724609375 = 0.5104854106903076 + 100.0 * 8.392437934875488
Epoch 720, val loss: 0.5774749517440796
Epoch 730, training loss: 839.8431396484375 = 0.5087465047836304 + 100.0 * 8.393343925476074
Epoch 730, val loss: 0.5767344832420349
Epoch 740, training loss: 839.9378662109375 = 0.506983757019043 + 100.0 * 8.394309043884277
Epoch 740, val loss: 0.5758107304573059
Epoch 750, training loss: 839.4224243164062 = 0.505248486995697 + 100.0 * 8.389171600341797
Epoch 750, val loss: 0.5753458142280579
Epoch 760, training loss: 839.5113525390625 = 0.5036181211471558 + 100.0 * 8.390077590942383
Epoch 760, val loss: 0.5745264887809753
Epoch 770, training loss: 839.3917236328125 = 0.5019962191581726 + 100.0 * 8.388896942138672
Epoch 770, val loss: 0.5738571882247925
Epoch 780, training loss: 839.1026000976562 = 0.500443696975708 + 100.0 * 8.386021614074707
Epoch 780, val loss: 0.5735292434692383
Epoch 790, training loss: 838.9298706054688 = 0.49895986914634705 + 100.0 * 8.384308815002441
Epoch 790, val loss: 0.572870671749115
Epoch 800, training loss: 839.3060302734375 = 0.49750223755836487 + 100.0 * 8.38808536529541
Epoch 800, val loss: 0.572367787361145
Epoch 810, training loss: 839.2010498046875 = 0.4959540069103241 + 100.0 * 8.38705062866211
Epoch 810, val loss: 0.5719246864318848
Epoch 820, training loss: 838.7276000976562 = 0.49444788694381714 + 100.0 * 8.382331848144531
Epoch 820, val loss: 0.5709927678108215
Epoch 830, training loss: 838.5784301757812 = 0.4930465817451477 + 100.0 * 8.380853652954102
Epoch 830, val loss: 0.5705829858779907
Epoch 840, training loss: 838.3922119140625 = 0.4917351007461548 + 100.0 * 8.37900447845459
Epoch 840, val loss: 0.5701038837432861
Epoch 850, training loss: 838.5994262695312 = 0.4904433786869049 + 100.0 * 8.38109016418457
Epoch 850, val loss: 0.56978440284729
Epoch 860, training loss: 838.6275634765625 = 0.48900967836380005 + 100.0 * 8.381385803222656
Epoch 860, val loss: 0.5689942240715027
Epoch 870, training loss: 838.3177490234375 = 0.4875836968421936 + 100.0 * 8.378301620483398
Epoch 870, val loss: 0.5684179067611694
Epoch 880, training loss: 838.1127319335938 = 0.4862937927246094 + 100.0 * 8.376264572143555
Epoch 880, val loss: 0.568058431148529
Epoch 890, training loss: 837.9600830078125 = 0.4850695729255676 + 100.0 * 8.374750137329102
Epoch 890, val loss: 0.5676029324531555
Epoch 900, training loss: 838.100341796875 = 0.4838644564151764 + 100.0 * 8.376164436340332
Epoch 900, val loss: 0.5672982931137085
Epoch 910, training loss: 837.8679809570312 = 0.4825288951396942 + 100.0 * 8.373854637145996
Epoch 910, val loss: 0.5665879845619202
Epoch 920, training loss: 837.8446044921875 = 0.48119768500328064 + 100.0 * 8.373634338378906
Epoch 920, val loss: 0.5661420822143555
Epoch 930, training loss: 837.7720947265625 = 0.47999805212020874 + 100.0 * 8.372920989990234
Epoch 930, val loss: 0.5655435919761658
Epoch 940, training loss: 837.802001953125 = 0.47885265946388245 + 100.0 * 8.373230934143066
Epoch 940, val loss: 0.5651169419288635
Epoch 950, training loss: 837.5140991210938 = 0.47763851284980774 + 100.0 * 8.37036418914795
Epoch 950, val loss: 0.564879298210144
Epoch 960, training loss: 837.4228515625 = 0.476459801197052 + 100.0 * 8.369463920593262
Epoch 960, val loss: 0.5642914772033691
Epoch 970, training loss: 837.4017944335938 = 0.4753167927265167 + 100.0 * 8.369264602661133
Epoch 970, val loss: 0.5639551877975464
Epoch 980, training loss: 837.6228637695312 = 0.47415590286254883 + 100.0 * 8.37148666381836
Epoch 980, val loss: 0.5634718537330627
Epoch 990, training loss: 837.4207763671875 = 0.4729572832584381 + 100.0 * 8.369478225708008
Epoch 990, val loss: 0.5630541443824768
Epoch 1000, training loss: 837.20361328125 = 0.47178617119789124 + 100.0 * 8.367318153381348
Epoch 1000, val loss: 0.5626558661460876
Epoch 1010, training loss: 837.1741333007812 = 0.4706609845161438 + 100.0 * 8.367034912109375
Epoch 1010, val loss: 0.5622698664665222
Epoch 1020, training loss: 837.5215454101562 = 0.4695015549659729 + 100.0 * 8.37052059173584
Epoch 1020, val loss: 0.5618765950202942
Epoch 1030, training loss: 836.9708251953125 = 0.4682786762714386 + 100.0 * 8.365025520324707
Epoch 1030, val loss: 0.561253547668457
Epoch 1040, training loss: 836.951904296875 = 0.4671303927898407 + 100.0 * 8.364848136901855
Epoch 1040, val loss: 0.5607007145881653
Epoch 1050, training loss: 836.8773803710938 = 0.4660218060016632 + 100.0 * 8.364113807678223
Epoch 1050, val loss: 0.5602510571479797
Epoch 1060, training loss: 837.11767578125 = 0.4649146497249603 + 100.0 * 8.366527557373047
Epoch 1060, val loss: 0.5596799254417419
Epoch 1070, training loss: 836.8782958984375 = 0.46372848749160767 + 100.0 * 8.364145278930664
Epoch 1070, val loss: 0.5593029856681824
Epoch 1080, training loss: 836.8199462890625 = 0.4625369906425476 + 100.0 * 8.363574028015137
Epoch 1080, val loss: 0.5588707327842712
Epoch 1090, training loss: 836.6360473632812 = 0.4613928496837616 + 100.0 * 8.361746788024902
Epoch 1090, val loss: 0.5582573413848877
Epoch 1100, training loss: 836.5740966796875 = 0.46029913425445557 + 100.0 * 8.361137390136719
Epoch 1100, val loss: 0.5578615069389343
Epoch 1110, training loss: 836.7354736328125 = 0.4592069983482361 + 100.0 * 8.362762451171875
Epoch 1110, val loss: 0.5572662353515625
Epoch 1120, training loss: 836.5479736328125 = 0.4580669105052948 + 100.0 * 8.360898971557617
Epoch 1120, val loss: 0.5570722222328186
Epoch 1130, training loss: 837.0889892578125 = 0.45692941546440125 + 100.0 * 8.366320610046387
Epoch 1130, val loss: 0.5564985275268555
Epoch 1140, training loss: 836.5247802734375 = 0.45570108294487 + 100.0 * 8.36069107055664
Epoch 1140, val loss: 0.5561614036560059
Epoch 1150, training loss: 836.2755737304688 = 0.4545481503009796 + 100.0 * 8.358210563659668
Epoch 1150, val loss: 0.5555688738822937
Epoch 1160, training loss: 836.2315063476562 = 0.4534565508365631 + 100.0 * 8.357780456542969
Epoch 1160, val loss: 0.5551488995552063
Epoch 1170, training loss: 836.2657470703125 = 0.45237240195274353 + 100.0 * 8.358133316040039
Epoch 1170, val loss: 0.5549231767654419
Epoch 1180, training loss: 836.9647216796875 = 0.4512009024620056 + 100.0 * 8.365135192871094
Epoch 1180, val loss: 0.5543540716171265
Epoch 1190, training loss: 836.1117553710938 = 0.4499370753765106 + 100.0 * 8.35661792755127
Epoch 1190, val loss: 0.5536391735076904
Epoch 1200, training loss: 836.0562133789062 = 0.44878464937210083 + 100.0 * 8.356074333190918
Epoch 1200, val loss: 0.5532458424568176
Epoch 1210, training loss: 836.0155639648438 = 0.44768473505973816 + 100.0 * 8.35567855834961
Epoch 1210, val loss: 0.5528554916381836
Epoch 1220, training loss: 836.0389404296875 = 0.44659096002578735 + 100.0 * 8.355923652648926
Epoch 1220, val loss: 0.5523963570594788
Epoch 1230, training loss: 836.5173950195312 = 0.44542744755744934 + 100.0 * 8.360719680786133
Epoch 1230, val loss: 0.5518518090248108
Epoch 1240, training loss: 836.0444946289062 = 0.4441990256309509 + 100.0 * 8.356002807617188
Epoch 1240, val loss: 0.5512518286705017
Epoch 1250, training loss: 836.1394653320312 = 0.4430362284183502 + 100.0 * 8.356964111328125
Epoch 1250, val loss: 0.5506084561347961
Epoch 1260, training loss: 835.8450927734375 = 0.441804438829422 + 100.0 * 8.354032516479492
Epoch 1260, val loss: 0.5502499341964722
Epoch 1270, training loss: 835.749755859375 = 0.4406029284000397 + 100.0 * 8.3530912399292
Epoch 1270, val loss: 0.5497799515724182
Epoch 1280, training loss: 835.7150268554688 = 0.43946391344070435 + 100.0 * 8.352755546569824
Epoch 1280, val loss: 0.5492352843284607
Epoch 1290, training loss: 835.7351684570312 = 0.43832507729530334 + 100.0 * 8.352968215942383
Epoch 1290, val loss: 0.5488595962524414
Epoch 1300, training loss: 836.1195068359375 = 0.43714651465415955 + 100.0 * 8.356823921203613
Epoch 1300, val loss: 0.5483276844024658
Epoch 1310, training loss: 835.7330322265625 = 0.43590113520622253 + 100.0 * 8.352971076965332
Epoch 1310, val loss: 0.5478326082229614
Epoch 1320, training loss: 835.678466796875 = 0.4346926510334015 + 100.0 * 8.352437973022461
Epoch 1320, val loss: 0.5472259521484375
Epoch 1330, training loss: 835.5717163085938 = 0.4334850013256073 + 100.0 * 8.3513822555542
Epoch 1330, val loss: 0.546750009059906
Epoch 1340, training loss: 835.6434326171875 = 0.4322739541530609 + 100.0 * 8.35211181640625
Epoch 1340, val loss: 0.5461993217468262
Epoch 1350, training loss: 836.179443359375 = 0.4310263395309448 + 100.0 * 8.357483863830566
Epoch 1350, val loss: 0.5458253622055054
Epoch 1360, training loss: 835.62255859375 = 0.42967936396598816 + 100.0 * 8.3519287109375
Epoch 1360, val loss: 0.5447670221328735
Epoch 1370, training loss: 835.4699096679688 = 0.4284120500087738 + 100.0 * 8.350415229797363
Epoch 1370, val loss: 0.5444912314414978
Epoch 1380, training loss: 835.3650512695312 = 0.42717817425727844 + 100.0 * 8.34937858581543
Epoch 1380, val loss: 0.5437989830970764
Epoch 1390, training loss: 835.3829956054688 = 0.42595210671424866 + 100.0 * 8.349570274353027
Epoch 1390, val loss: 0.5433723330497742
Epoch 1400, training loss: 835.350830078125 = 0.42469245195388794 + 100.0 * 8.349261283874512
Epoch 1400, val loss: 0.5427060723304749
Epoch 1410, training loss: 835.2454833984375 = 0.4234195947647095 + 100.0 * 8.348220825195312
Epoch 1410, val loss: 0.5419368743896484
Epoch 1420, training loss: 836.2806396484375 = 0.4221274256706238 + 100.0 * 8.358585357666016
Epoch 1420, val loss: 0.5411149263381958
Epoch 1430, training loss: 835.4218139648438 = 0.42067936062812805 + 100.0 * 8.350011825561523
Epoch 1430, val loss: 0.54073166847229
Epoch 1440, training loss: 835.123779296875 = 0.4193098843097687 + 100.0 * 8.347044944763184
Epoch 1440, val loss: 0.5400167107582092
Epoch 1450, training loss: 835.0859985351562 = 0.4180106818675995 + 100.0 * 8.3466796875
Epoch 1450, val loss: 0.539289116859436
Epoch 1460, training loss: 835.0494384765625 = 0.4167269468307495 + 100.0 * 8.34632682800293
Epoch 1460, val loss: 0.5387265682220459
Epoch 1470, training loss: 835.4840087890625 = 0.4154186248779297 + 100.0 * 8.350686073303223
Epoch 1470, val loss: 0.538006603717804
Epoch 1480, training loss: 835.169677734375 = 0.41396909952163696 + 100.0 * 8.347557067871094
Epoch 1480, val loss: 0.5374385118484497
Epoch 1490, training loss: 835.1397094726562 = 0.4125325083732605 + 100.0 * 8.347271919250488
Epoch 1490, val loss: 0.5366778373718262
Epoch 1500, training loss: 834.9620971679688 = 0.41115593910217285 + 100.0 * 8.34550952911377
Epoch 1500, val loss: 0.5360840559005737
Epoch 1510, training loss: 835.6406860351562 = 0.4097922742366791 + 100.0 * 8.352309226989746
Epoch 1510, val loss: 0.5357850193977356
Epoch 1520, training loss: 835.0799560546875 = 0.40828701853752136 + 100.0 * 8.34671688079834
Epoch 1520, val loss: 0.5342958569526672
Epoch 1530, training loss: 834.947998046875 = 0.4068635106086731 + 100.0 * 8.34541130065918
Epoch 1530, val loss: 0.5340795516967773
Epoch 1540, training loss: 835.4806518554688 = 0.4054276943206787 + 100.0 * 8.350751876831055
Epoch 1540, val loss: 0.5330895781517029
Epoch 1550, training loss: 834.8211059570312 = 0.4039168059825897 + 100.0 * 8.344171524047852
Epoch 1550, val loss: 0.5325412154197693
Epoch 1560, training loss: 834.787841796875 = 0.40245115756988525 + 100.0 * 8.343853950500488
Epoch 1560, val loss: 0.5316916108131409
Epoch 1570, training loss: 834.7335815429688 = 0.4010048806667328 + 100.0 * 8.3433256149292
Epoch 1570, val loss: 0.5312073826789856
Epoch 1580, training loss: 834.720947265625 = 0.39954036474227905 + 100.0 * 8.34321403503418
Epoch 1580, val loss: 0.530430018901825
Epoch 1590, training loss: 835.0966796875 = 0.3980499505996704 + 100.0 * 8.346985816955566
Epoch 1590, val loss: 0.5295226573944092
Epoch 1600, training loss: 834.9835205078125 = 0.3964250683784485 + 100.0 * 8.345870971679688
Epoch 1600, val loss: 0.5290148854255676
Epoch 1610, training loss: 834.7523803710938 = 0.39481526613235474 + 100.0 * 8.343575477600098
Epoch 1610, val loss: 0.5282504558563232
Epoch 1620, training loss: 834.5641479492188 = 0.39327558875083923 + 100.0 * 8.34170913696289
Epoch 1620, val loss: 0.5275450348854065
Epoch 1630, training loss: 834.6619262695312 = 0.39175352454185486 + 100.0 * 8.34270191192627
Epoch 1630, val loss: 0.5268746018409729
Epoch 1640, training loss: 835.3179931640625 = 0.39012980461120605 + 100.0 * 8.349278450012207
Epoch 1640, val loss: 0.5259166359901428
Epoch 1650, training loss: 834.664794921875 = 0.3884330987930298 + 100.0 * 8.342763900756836
Epoch 1650, val loss: 0.5254935026168823
Epoch 1660, training loss: 834.4700927734375 = 0.38682693243026733 + 100.0 * 8.340832710266113
Epoch 1660, val loss: 0.5246087312698364
Epoch 1670, training loss: 834.4135131835938 = 0.3852585554122925 + 100.0 * 8.340282440185547
Epoch 1670, val loss: 0.5240112543106079
Epoch 1680, training loss: 834.3946533203125 = 0.3836877942085266 + 100.0 * 8.340109825134277
Epoch 1680, val loss: 0.5234302878379822
Epoch 1690, training loss: 834.46630859375 = 0.38209545612335205 + 100.0 * 8.340842247009277
Epoch 1690, val loss: 0.5227676033973694
Epoch 1700, training loss: 834.7613525390625 = 0.38042929768562317 + 100.0 * 8.343809127807617
Epoch 1700, val loss: 0.5219892263412476
Epoch 1710, training loss: 834.509033203125 = 0.3786967098712921 + 100.0 * 8.341302871704102
Epoch 1710, val loss: 0.5210580229759216
Epoch 1720, training loss: 834.3294677734375 = 0.37698882818222046 + 100.0 * 8.339524269104004
Epoch 1720, val loss: 0.5205145478248596
Epoch 1730, training loss: 834.2838745117188 = 0.3753189444541931 + 100.0 * 8.339085578918457
Epoch 1730, val loss: 0.5199685096740723
Epoch 1740, training loss: 834.4573974609375 = 0.3736584782600403 + 100.0 * 8.340837478637695
Epoch 1740, val loss: 0.519151508808136
Epoch 1750, training loss: 834.5474853515625 = 0.37191495299339294 + 100.0 * 8.341755867004395
Epoch 1750, val loss: 0.5184067487716675
Epoch 1760, training loss: 834.276123046875 = 0.3701377213001251 + 100.0 * 8.339059829711914
Epoch 1760, val loss: 0.5179016590118408
Epoch 1770, training loss: 834.2039794921875 = 0.3684128522872925 + 100.0 * 8.338356018066406
Epoch 1770, val loss: 0.5172987580299377
Epoch 1780, training loss: 834.584228515625 = 0.36670681834220886 + 100.0 * 8.342175483703613
Epoch 1780, val loss: 0.5166413187980652
Epoch 1790, training loss: 834.2092895507812 = 0.364912748336792 + 100.0 * 8.338443756103516
Epoch 1790, val loss: 0.5160335898399353
Epoch 1800, training loss: 834.2205810546875 = 0.36314019560813904 + 100.0 * 8.338574409484863
Epoch 1800, val loss: 0.51543128490448
Epoch 1810, training loss: 834.1726684570312 = 0.36138948798179626 + 100.0 * 8.338112831115723
Epoch 1810, val loss: 0.5149531960487366
Epoch 1820, training loss: 834.0964965820312 = 0.35964491963386536 + 100.0 * 8.337368965148926
Epoch 1820, val loss: 0.5144623517990112
Epoch 1830, training loss: 834.4559326171875 = 0.35790714621543884 + 100.0 * 8.340980529785156
Epoch 1830, val loss: 0.514178991317749
Epoch 1840, training loss: 834.0654296875 = 0.35605692863464355 + 100.0 * 8.337093353271484
Epoch 1840, val loss: 0.5128727555274963
Epoch 1850, training loss: 834.1275024414062 = 0.3542580008506775 + 100.0 * 8.337732315063477
Epoch 1850, val loss: 0.512627899646759
Epoch 1860, training loss: 834.5475463867188 = 0.35244637727737427 + 100.0 * 8.341951370239258
Epoch 1860, val loss: 0.5120195746421814
Epoch 1870, training loss: 834.0428466796875 = 0.3505672812461853 + 100.0 * 8.336922645568848
Epoch 1870, val loss: 0.5110524892807007
Epoch 1880, training loss: 833.9003295898438 = 0.3487655222415924 + 100.0 * 8.335515975952148
Epoch 1880, val loss: 0.5104864239692688
Epoch 1890, training loss: 833.9385375976562 = 0.34698954224586487 + 100.0 * 8.335915565490723
Epoch 1890, val loss: 0.5101346969604492
Epoch 1900, training loss: 834.0279541015625 = 0.34519296884536743 + 100.0 * 8.336827278137207
Epoch 1900, val loss: 0.5094636678695679
Epoch 1910, training loss: 834.2786865234375 = 0.34334927797317505 + 100.0 * 8.339353561401367
Epoch 1910, val loss: 0.508573591709137
Epoch 1920, training loss: 834.0060424804688 = 0.3414604067802429 + 100.0 * 8.33664608001709
Epoch 1920, val loss: 0.5083193182945251
Epoch 1930, training loss: 834.0113525390625 = 0.3396211564540863 + 100.0 * 8.33671760559082
Epoch 1930, val loss: 0.5080233812332153
Epoch 1940, training loss: 833.8657836914062 = 0.3377740979194641 + 100.0 * 8.335280418395996
Epoch 1940, val loss: 0.5070354342460632
Epoch 1950, training loss: 834.0816040039062 = 0.33595535159111023 + 100.0 * 8.337456703186035
Epoch 1950, val loss: 0.5068625211715698
Epoch 1960, training loss: 834.0848999023438 = 0.3340808153152466 + 100.0 * 8.337508201599121
Epoch 1960, val loss: 0.506277859210968
Epoch 1970, training loss: 833.8541870117188 = 0.3321772515773773 + 100.0 * 8.335220336914062
Epoch 1970, val loss: 0.505514919757843
Epoch 1980, training loss: 833.7014770507812 = 0.3303135931491852 + 100.0 * 8.333711624145508
Epoch 1980, val loss: 0.5050992369651794
Epoch 1990, training loss: 833.7516479492188 = 0.32847854495048523 + 100.0 * 8.33423137664795
Epoch 1990, val loss: 0.5047098994255066
Epoch 2000, training loss: 834.051513671875 = 0.3266324996948242 + 100.0 * 8.337248802185059
Epoch 2000, val loss: 0.5037558674812317
Epoch 2010, training loss: 833.923583984375 = 0.3247123062610626 + 100.0 * 8.335988998413086
Epoch 2010, val loss: 0.5034666061401367
Epoch 2020, training loss: 833.6099853515625 = 0.32279446721076965 + 100.0 * 8.332871437072754
Epoch 2020, val loss: 0.5033320188522339
Epoch 2030, training loss: 833.6494750976562 = 0.3209352493286133 + 100.0 * 8.333285331726074
Epoch 2030, val loss: 0.5030345320701599
Epoch 2040, training loss: 834.4874267578125 = 0.31908178329467773 + 100.0 * 8.341683387756348
Epoch 2040, val loss: 0.5022552609443665
Epoch 2050, training loss: 833.6849365234375 = 0.31714633107185364 + 100.0 * 8.333678245544434
Epoch 2050, val loss: 0.502058207988739
Epoch 2060, training loss: 833.558349609375 = 0.31527626514434814 + 100.0 * 8.332430839538574
Epoch 2060, val loss: 0.5017480850219727
Epoch 2070, training loss: 833.5050048828125 = 0.3134409189224243 + 100.0 * 8.331915855407715
Epoch 2070, val loss: 0.5012685060501099
Epoch 2080, training loss: 833.5255737304688 = 0.3116181492805481 + 100.0 * 8.332139015197754
Epoch 2080, val loss: 0.5006707310676575
Epoch 2090, training loss: 834.2509765625 = 0.30981698632240295 + 100.0 * 8.339411735534668
Epoch 2090, val loss: 0.49994537234306335
Epoch 2100, training loss: 833.6865234375 = 0.3078922927379608 + 100.0 * 8.333786010742188
Epoch 2100, val loss: 0.5007970333099365
Epoch 2110, training loss: 833.55224609375 = 0.3060081899166107 + 100.0 * 8.332462310791016
Epoch 2110, val loss: 0.4999079406261444
Epoch 2120, training loss: 833.8847045898438 = 0.3042050004005432 + 100.0 * 8.33580493927002
Epoch 2120, val loss: 0.500213086605072
Epoch 2130, training loss: 833.5027465820312 = 0.30229559540748596 + 100.0 * 8.33200454711914
Epoch 2130, val loss: 0.49926847219467163
Epoch 2140, training loss: 833.438720703125 = 0.30043289065361023 + 100.0 * 8.331382751464844
Epoch 2140, val loss: 0.4989987909793854
Epoch 2150, training loss: 833.41552734375 = 0.2986140847206116 + 100.0 * 8.331169128417969
Epoch 2150, val loss: 0.4988953173160553
Epoch 2160, training loss: 833.7161254882812 = 0.29680943489074707 + 100.0 * 8.334193229675293
Epoch 2160, val loss: 0.4980516731739044
Epoch 2170, training loss: 833.3551025390625 = 0.29494595527648926 + 100.0 * 8.330601692199707
Epoch 2170, val loss: 0.4984230399131775
Epoch 2180, training loss: 833.3735961914062 = 0.2931205630302429 + 100.0 * 8.330804824829102
Epoch 2180, val loss: 0.4979497492313385
Epoch 2190, training loss: 833.4274291992188 = 0.29131019115448 + 100.0 * 8.331360816955566
Epoch 2190, val loss: 0.49763166904449463
Epoch 2200, training loss: 833.586669921875 = 0.2894863486289978 + 100.0 * 8.332971572875977
Epoch 2200, val loss: 0.49754518270492554
Epoch 2210, training loss: 834.4977416992188 = 0.28765594959259033 + 100.0 * 8.342101097106934
Epoch 2210, val loss: 0.49685049057006836
Epoch 2220, training loss: 833.564453125 = 0.28573504090309143 + 100.0 * 8.33278751373291
Epoch 2220, val loss: 0.49732786417007446
Epoch 2230, training loss: 833.2764282226562 = 0.2838987708091736 + 100.0 * 8.329925537109375
Epoch 2230, val loss: 0.49712029099464417
Epoch 2240, training loss: 833.19189453125 = 0.2821284830570221 + 100.0 * 8.329097747802734
Epoch 2240, val loss: 0.4969404637813568
Epoch 2250, training loss: 833.1500244140625 = 0.28037118911743164 + 100.0 * 8.328696250915527
Epoch 2250, val loss: 0.4970795512199402
Epoch 2260, training loss: 833.3079223632812 = 0.27862897515296936 + 100.0 * 8.330292701721191
Epoch 2260, val loss: 0.49732154607772827
Epoch 2270, training loss: 833.3819580078125 = 0.27681565284729004 + 100.0 * 8.33105182647705
Epoch 2270, val loss: 0.49690479040145874
Epoch 2280, training loss: 833.1812744140625 = 0.27500033378601074 + 100.0 * 8.329062461853027
Epoch 2280, val loss: 0.4969257414340973
Epoch 2290, training loss: 833.1066284179688 = 0.2732260227203369 + 100.0 * 8.328333854675293
Epoch 2290, val loss: 0.49683308601379395
Epoch 2300, training loss: 833.1635131835938 = 0.2714861035346985 + 100.0 * 8.328920364379883
Epoch 2300, val loss: 0.4972726106643677
Epoch 2310, training loss: 834.4666137695312 = 0.2697571814060211 + 100.0 * 8.341968536376953
Epoch 2310, val loss: 0.4973798096179962
Epoch 2320, training loss: 833.375732421875 = 0.2678737938404083 + 100.0 * 8.33107852935791
Epoch 2320, val loss: 0.49643898010253906
Epoch 2330, training loss: 833.0501708984375 = 0.26610180735588074 + 100.0 * 8.327840805053711
Epoch 2330, val loss: 0.49709850549697876
Epoch 2340, training loss: 832.9956665039062 = 0.2643866240978241 + 100.0 * 8.327312469482422
Epoch 2340, val loss: 0.49688756465911865
Epoch 2350, training loss: 832.9689331054688 = 0.26269808411598206 + 100.0 * 8.327062606811523
Epoch 2350, val loss: 0.49713894724845886
Epoch 2360, training loss: 833.216064453125 = 0.2610125243663788 + 100.0 * 8.329550743103027
Epoch 2360, val loss: 0.49735161662101746
Epoch 2370, training loss: 833.0383911132812 = 0.2592766582965851 + 100.0 * 8.327791213989258
Epoch 2370, val loss: 0.49764686822891235
Epoch 2380, training loss: 832.9595947265625 = 0.25751373171806335 + 100.0 * 8.327020645141602
Epoch 2380, val loss: 0.4974185824394226
Epoch 2390, training loss: 832.969970703125 = 0.25580736994743347 + 100.0 * 8.327141761779785
Epoch 2390, val loss: 0.4976009428501129
Epoch 2400, training loss: 833.2896118164062 = 0.2541227638721466 + 100.0 * 8.330354690551758
Epoch 2400, val loss: 0.497738242149353
Epoch 2410, training loss: 833.11865234375 = 0.2523947060108185 + 100.0 * 8.328662872314453
Epoch 2410, val loss: 0.49805551767349243
Epoch 2420, training loss: 833.0589599609375 = 0.25067898631095886 + 100.0 * 8.328083038330078
Epoch 2420, val loss: 0.4983173608779907
Epoch 2430, training loss: 832.9300537109375 = 0.24897867441177368 + 100.0 * 8.326810836791992
Epoch 2430, val loss: 0.4979844093322754
Epoch 2440, training loss: 832.8870849609375 = 0.2473098784685135 + 100.0 * 8.326397895812988
Epoch 2440, val loss: 0.4984759986400604
Epoch 2450, training loss: 833.0525512695312 = 0.24565379321575165 + 100.0 * 8.328068733215332
Epoch 2450, val loss: 0.49892666935920715
Epoch 2460, training loss: 832.9196166992188 = 0.24396686255931854 + 100.0 * 8.326756477355957
Epoch 2460, val loss: 0.4987187087535858
Epoch 2470, training loss: 833.0546875 = 0.24233300983905792 + 100.0 * 8.328124046325684
Epoch 2470, val loss: 0.49847111105918884
Epoch 2480, training loss: 833.0968627929688 = 0.24063661694526672 + 100.0 * 8.328561782836914
Epoch 2480, val loss: 0.4992959797382355
Epoch 2490, training loss: 832.7699584960938 = 0.23897254467010498 + 100.0 * 8.325309753417969
Epoch 2490, val loss: 0.5001351237297058
Epoch 2500, training loss: 832.9669799804688 = 0.2373519241809845 + 100.0 * 8.327296257019043
Epoch 2500, val loss: 0.5003688335418701
Epoch 2510, training loss: 832.9552612304688 = 0.23570483922958374 + 100.0 * 8.327195167541504
Epoch 2510, val loss: 0.5005736947059631
Epoch 2520, training loss: 832.8001098632812 = 0.23405884206295013 + 100.0 * 8.325660705566406
Epoch 2520, val loss: 0.5015681385993958
Epoch 2530, training loss: 832.7416381835938 = 0.23243513703346252 + 100.0 * 8.325092315673828
Epoch 2530, val loss: 0.5020996332168579
Epoch 2540, training loss: 832.8514404296875 = 0.2308381050825119 + 100.0 * 8.32620620727539
Epoch 2540, val loss: 0.5026803612709045
Epoch 2550, training loss: 833.0584106445312 = 0.22923687100410461 + 100.0 * 8.328291893005371
Epoch 2550, val loss: 0.5033062696456909
Epoch 2560, training loss: 832.7744140625 = 0.22759594023227692 + 100.0 * 8.325468063354492
Epoch 2560, val loss: 0.5029368996620178
Epoch 2570, training loss: 832.8590087890625 = 0.2260134071111679 + 100.0 * 8.326330184936523
Epoch 2570, val loss: 0.5035758018493652
Epoch 2580, training loss: 832.8341674804688 = 0.22443467378616333 + 100.0 * 8.32609748840332
Epoch 2580, val loss: 0.5039721131324768
Epoch 2590, training loss: 832.6853637695312 = 0.22286398708820343 + 100.0 * 8.324625015258789
Epoch 2590, val loss: 0.5042717456817627
Epoch 2600, training loss: 832.6370849609375 = 0.22131581604480743 + 100.0 * 8.32415771484375
Epoch 2600, val loss: 0.5046700835227966
Epoch 2610, training loss: 832.900390625 = 0.21979349851608276 + 100.0 * 8.32680606842041
Epoch 2610, val loss: 0.5050762891769409
Epoch 2620, training loss: 832.5823974609375 = 0.21823009848594666 + 100.0 * 8.323641777038574
Epoch 2620, val loss: 0.5066477656364441
Epoch 2630, training loss: 832.9769287109375 = 0.21679824590682983 + 100.0 * 8.327601432800293
Epoch 2630, val loss: 0.507903516292572
Epoch 2640, training loss: 832.7476196289062 = 0.2151639610528946 + 100.0 * 8.325325012207031
Epoch 2640, val loss: 0.5065096020698547
Epoch 2650, training loss: 832.5513916015625 = 0.21362829208374023 + 100.0 * 8.32337760925293
Epoch 2650, val loss: 0.5077542662620544
Epoch 2660, training loss: 832.45654296875 = 0.2121347337961197 + 100.0 * 8.322443962097168
Epoch 2660, val loss: 0.5084443688392639
Epoch 2670, training loss: 832.5350341796875 = 0.2106553465127945 + 100.0 * 8.323244094848633
Epoch 2670, val loss: 0.5089592337608337
Epoch 2680, training loss: 832.8958740234375 = 0.20918582379817963 + 100.0 * 8.32686710357666
Epoch 2680, val loss: 0.5096294283866882
Epoch 2690, training loss: 832.7100830078125 = 0.20769481360912323 + 100.0 * 8.325023651123047
Epoch 2690, val loss: 0.5092006921768188
Epoch 2700, training loss: 832.48583984375 = 0.20615410804748535 + 100.0 * 8.322796821594238
Epoch 2700, val loss: 0.510779082775116
Epoch 2710, training loss: 832.633544921875 = 0.20468519628047943 + 100.0 * 8.324288368225098
Epoch 2710, val loss: 0.5114240050315857
Epoch 2720, training loss: 832.4647216796875 = 0.2032133936882019 + 100.0 * 8.322615623474121
Epoch 2720, val loss: 0.5119544863700867
Epoch 2730, training loss: 832.6573486328125 = 0.20176152884960175 + 100.0 * 8.324555397033691
Epoch 2730, val loss: 0.5125053524971008
Epoch 2740, training loss: 832.50634765625 = 0.20031201839447021 + 100.0 * 8.323060035705566
Epoch 2740, val loss: 0.5128849148750305
Epoch 2750, training loss: 832.6958618164062 = 0.19886568188667297 + 100.0 * 8.324970245361328
Epoch 2750, val loss: 0.5136398077011108
Epoch 2760, training loss: 832.33544921875 = 0.19739824533462524 + 100.0 * 8.321380615234375
Epoch 2760, val loss: 0.5144048929214478
Epoch 2770, training loss: 832.3212890625 = 0.1959735006093979 + 100.0 * 8.321252822875977
Epoch 2770, val loss: 0.5153657793998718
Epoch 2780, training loss: 832.35693359375 = 0.1945541948080063 + 100.0 * 8.321623802185059
Epoch 2780, val loss: 0.5158400535583496
Epoch 2790, training loss: 833.0148315429688 = 0.19316065311431885 + 100.0 * 8.328216552734375
Epoch 2790, val loss: 0.5167405605316162
Epoch 2800, training loss: 832.8707275390625 = 0.19179821014404297 + 100.0 * 8.326789855957031
Epoch 2800, val loss: 0.5182259678840637
Epoch 2810, training loss: 832.470947265625 = 0.1902787983417511 + 100.0 * 8.322806358337402
Epoch 2810, val loss: 0.5172904133796692
Epoch 2820, training loss: 832.2652587890625 = 0.18889495730400085 + 100.0 * 8.32076358795166
Epoch 2820, val loss: 0.5189127326011658
Epoch 2830, training loss: 832.2160034179688 = 0.1875167191028595 + 100.0 * 8.320284843444824
Epoch 2830, val loss: 0.5193688273429871
Epoch 2840, training loss: 832.6767578125 = 0.1862284541130066 + 100.0 * 8.324905395507812
Epoch 2840, val loss: 0.5209959149360657
Epoch 2850, training loss: 832.2562866210938 = 0.1847863644361496 + 100.0 * 8.320714950561523
Epoch 2850, val loss: 0.5207215547561646
Epoch 2860, training loss: 832.1622314453125 = 0.18340659141540527 + 100.0 * 8.319787979125977
Epoch 2860, val loss: 0.5212430953979492
Epoch 2870, training loss: 832.1787719726562 = 0.18205773830413818 + 100.0 * 8.319967269897461
Epoch 2870, val loss: 0.5221329927444458
Epoch 2880, training loss: 832.7101440429688 = 0.1807395964860916 + 100.0 * 8.325294494628906
Epoch 2880, val loss: 0.5233160257339478
Epoch 2890, training loss: 832.2876586914062 = 0.17938446998596191 + 100.0 * 8.321083068847656
Epoch 2890, val loss: 0.5239753723144531
Epoch 2900, training loss: 832.1581420898438 = 0.1780107021331787 + 100.0 * 8.319801330566406
Epoch 2900, val loss: 0.5242127180099487
Epoch 2910, training loss: 832.1294555664062 = 0.17667871713638306 + 100.0 * 8.319527626037598
Epoch 2910, val loss: 0.5251795649528503
Epoch 2920, training loss: 832.3175048828125 = 0.17536316812038422 + 100.0 * 8.32142162322998
Epoch 2920, val loss: 0.5261530876159668
Epoch 2930, training loss: 832.3867797851562 = 0.17405936121940613 + 100.0 * 8.322127342224121
Epoch 2930, val loss: 0.5273467302322388
Epoch 2940, training loss: 832.2779541015625 = 0.17271502315998077 + 100.0 * 8.321052551269531
Epoch 2940, val loss: 0.5280243754386902
Epoch 2950, training loss: 832.0822143554688 = 0.17137904465198517 + 100.0 * 8.319108009338379
Epoch 2950, val loss: 0.5284321308135986
Epoch 2960, training loss: 832.1249389648438 = 0.170083150267601 + 100.0 * 8.319548606872559
Epoch 2960, val loss: 0.529563844203949
Epoch 2970, training loss: 832.5430297851562 = 0.16883112490177155 + 100.0 * 8.323741912841797
Epoch 2970, val loss: 0.530973494052887
Epoch 2980, training loss: 832.4988403320312 = 0.16748104989528656 + 100.0 * 8.32331371307373
Epoch 2980, val loss: 0.5307357311248779
Epoch 2990, training loss: 832.1548461914062 = 0.1661655455827713 + 100.0 * 8.319887161254883
Epoch 2990, val loss: 0.5316182971000671
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8148148148148148
0.8443092081431574
=== training gcn model ===
Epoch 0, training loss: 1059.316650390625 = 1.0915710926055908 + 100.0 * 10.58225154876709
Epoch 0, val loss: 1.0915873050689697
Epoch 10, training loss: 1059.194580078125 = 1.0856667757034302 + 100.0 * 10.58108901977539
Epoch 10, val loss: 1.0856579542160034
Epoch 20, training loss: 1058.1920166015625 = 1.078601360321045 + 100.0 * 10.571134567260742
Epoch 20, val loss: 1.0785866975784302
Epoch 30, training loss: 1051.919189453125 = 1.0702626705169678 + 100.0 * 10.508488655090332
Epoch 30, val loss: 1.0702600479125977
Epoch 40, training loss: 1031.09619140625 = 1.0622416734695435 + 100.0 * 10.300338745117188
Epoch 40, val loss: 1.0625921487808228
Epoch 50, training loss: 990.8025512695312 = 1.0565894842147827 + 100.0 * 9.897459983825684
Epoch 50, val loss: 1.05696439743042
Epoch 60, training loss: 958.99267578125 = 1.0503357648849487 + 100.0 * 9.579422950744629
Epoch 60, val loss: 1.0504297018051147
Epoch 70, training loss: 946.4825439453125 = 1.0415352582931519 + 100.0 * 9.4544095993042
Epoch 70, val loss: 1.0418356657028198
Epoch 80, training loss: 927.7400512695312 = 1.033785343170166 + 100.0 * 9.267062187194824
Epoch 80, val loss: 1.0346421003341675
Epoch 90, training loss: 916.6614990234375 = 1.0274816751480103 + 100.0 * 9.156340599060059
Epoch 90, val loss: 1.028679370880127
Epoch 100, training loss: 907.5259399414062 = 1.0216445922851562 + 100.0 * 9.065042495727539
Epoch 100, val loss: 1.0231022834777832
Epoch 110, training loss: 898.186279296875 = 1.0166118144989014 + 100.0 * 8.971696853637695
Epoch 110, val loss: 1.0181406736373901
Epoch 120, training loss: 892.6826782226562 = 1.010176181793213 + 100.0 * 8.916725158691406
Epoch 120, val loss: 1.0116289854049683
Epoch 130, training loss: 886.4320068359375 = 1.0022891759872437 + 100.0 * 8.854296684265137
Epoch 130, val loss: 1.0040204524993896
Epoch 140, training loss: 883.8613891601562 = 0.994118869304657 + 100.0 * 8.828672409057617
Epoch 140, val loss: 0.9963048100471497
Epoch 150, training loss: 880.7393798828125 = 0.9854742288589478 + 100.0 * 8.797538757324219
Epoch 150, val loss: 0.9881018996238708
Epoch 160, training loss: 877.0768432617188 = 0.9768450260162354 + 100.0 * 8.76099967956543
Epoch 160, val loss: 0.9799278378486633
Epoch 170, training loss: 873.0760498046875 = 0.96858149766922 + 100.0 * 8.721075057983398
Epoch 170, val loss: 0.9722314476966858
Epoch 180, training loss: 869.7192993164062 = 0.9596351385116577 + 100.0 * 8.687596321105957
Epoch 180, val loss: 0.9637086391448975
Epoch 190, training loss: 867.291259765625 = 0.9490892291069031 + 100.0 * 8.663421630859375
Epoch 190, val loss: 0.9536442756652832
Epoch 200, training loss: 865.07275390625 = 0.9370166659355164 + 100.0 * 8.641357421875
Epoch 200, val loss: 0.942260205745697
Epoch 210, training loss: 863.5794067382812 = 0.9239530563354492 + 100.0 * 8.626554489135742
Epoch 210, val loss: 0.9298473596572876
Epoch 220, training loss: 861.2573852539062 = 0.9100208282470703 + 100.0 * 8.603473663330078
Epoch 220, val loss: 0.9169002771377563
Epoch 230, training loss: 859.5237426757812 = 0.8955457806587219 + 100.0 * 8.586281776428223
Epoch 230, val loss: 0.9033507108688354
Epoch 240, training loss: 858.0418701171875 = 0.8803743720054626 + 100.0 * 8.571615219116211
Epoch 240, val loss: 0.8892146944999695
Epoch 250, training loss: 856.9181518554688 = 0.8642246723175049 + 100.0 * 8.560539245605469
Epoch 250, val loss: 0.8741706609725952
Epoch 260, training loss: 855.6360473632812 = 0.8473212718963623 + 100.0 * 8.547886848449707
Epoch 260, val loss: 0.8585429191589355
Epoch 270, training loss: 854.6272583007812 = 0.830130934715271 + 100.0 * 8.537971496582031
Epoch 270, val loss: 0.8426793813705444
Epoch 280, training loss: 853.7058715820312 = 0.8126453757286072 + 100.0 * 8.528932571411133
Epoch 280, val loss: 0.8266089558601379
Epoch 290, training loss: 853.2886352539062 = 0.7948453426361084 + 100.0 * 8.524937629699707
Epoch 290, val loss: 0.810346782207489
Epoch 300, training loss: 852.0853271484375 = 0.7771397233009338 + 100.0 * 8.513081550598145
Epoch 300, val loss: 0.7942387461662292
Epoch 310, training loss: 851.4515991210938 = 0.7598569393157959 + 100.0 * 8.506917953491211
Epoch 310, val loss: 0.7786625623703003
Epoch 320, training loss: 851.0942993164062 = 0.7428461909294128 + 100.0 * 8.503514289855957
Epoch 320, val loss: 0.7633329629898071
Epoch 330, training loss: 850.138671875 = 0.7262789607048035 + 100.0 * 8.494124412536621
Epoch 330, val loss: 0.7485939860343933
Epoch 340, training loss: 849.4396362304688 = 0.7104076743125916 + 100.0 * 8.487292289733887
Epoch 340, val loss: 0.7345766425132751
Epoch 350, training loss: 849.2608032226562 = 0.6951397657394409 + 100.0 * 8.48565673828125
Epoch 350, val loss: 0.7211733460426331
Epoch 360, training loss: 848.3992919921875 = 0.6804760098457336 + 100.0 * 8.477188110351562
Epoch 360, val loss: 0.7083668112754822
Epoch 370, training loss: 847.9345703125 = 0.6666199564933777 + 100.0 * 8.472679138183594
Epoch 370, val loss: 0.6963841319084167
Epoch 380, training loss: 847.7523193359375 = 0.6535789966583252 + 100.0 * 8.470987319946289
Epoch 380, val loss: 0.6852346658706665
Epoch 390, training loss: 847.2318725585938 = 0.6411494016647339 + 100.0 * 8.465907096862793
Epoch 390, val loss: 0.6745743155479431
Epoch 400, training loss: 846.6995849609375 = 0.6295996308326721 + 100.0 * 8.460700035095215
Epoch 400, val loss: 0.6648246049880981
Epoch 410, training loss: 846.31494140625 = 0.6189286112785339 + 100.0 * 8.45695972442627
Epoch 410, val loss: 0.6558820009231567
Epoch 420, training loss: 846.5460205078125 = 0.6089640259742737 + 100.0 * 8.459370613098145
Epoch 420, val loss: 0.6475995182991028
Epoch 430, training loss: 845.7041015625 = 0.5996285080909729 + 100.0 * 8.451045036315918
Epoch 430, val loss: 0.6398854851722717
Epoch 440, training loss: 845.4736328125 = 0.5910483002662659 + 100.0 * 8.44882583618164
Epoch 440, val loss: 0.6328704357147217
Epoch 450, training loss: 845.1024780273438 = 0.5830939412117004 + 100.0 * 8.445194244384766
Epoch 450, val loss: 0.6264158487319946
Epoch 460, training loss: 845.1185913085938 = 0.5757044553756714 + 100.0 * 8.445428848266602
Epoch 460, val loss: 0.6204813122749329
Epoch 470, training loss: 844.6656494140625 = 0.5688008069992065 + 100.0 * 8.44096851348877
Epoch 470, val loss: 0.6149551272392273
Epoch 480, training loss: 844.172607421875 = 0.5624613165855408 + 100.0 * 8.436101913452148
Epoch 480, val loss: 0.6099635362625122
Epoch 490, training loss: 843.9515991210938 = 0.5566454529762268 + 100.0 * 8.43394947052002
Epoch 490, val loss: 0.6054608821868896
Epoch 500, training loss: 843.7679443359375 = 0.5511494278907776 + 100.0 * 8.432168006896973
Epoch 500, val loss: 0.6012234687805176
Epoch 510, training loss: 843.4721069335938 = 0.5459852814674377 + 100.0 * 8.429261207580566
Epoch 510, val loss: 0.5972768664360046
Epoch 520, training loss: 843.1251220703125 = 0.5412374138832092 + 100.0 * 8.425838470458984
Epoch 520, val loss: 0.5936867594718933
Epoch 530, training loss: 842.96044921875 = 0.5368306636810303 + 100.0 * 8.424236297607422
Epoch 530, val loss: 0.5904322862625122
Epoch 540, training loss: 842.6356201171875 = 0.5326167941093445 + 100.0 * 8.421030044555664
Epoch 540, val loss: 0.5872963070869446
Epoch 550, training loss: 842.5637817382812 = 0.5286762714385986 + 100.0 * 8.420351028442383
Epoch 550, val loss: 0.5844693183898926
Epoch 560, training loss: 842.3006591796875 = 0.525038480758667 + 100.0 * 8.417756080627441
Epoch 560, val loss: 0.5818871855735779
Epoch 570, training loss: 842.009033203125 = 0.5215880274772644 + 100.0 * 8.414874076843262
Epoch 570, val loss: 0.5794975161552429
Epoch 580, training loss: 841.9027099609375 = 0.5183517336845398 + 100.0 * 8.413843154907227
Epoch 580, val loss: 0.5772466659545898
Epoch 590, training loss: 842.1040649414062 = 0.5152465105056763 + 100.0 * 8.415887832641602
Epoch 590, val loss: 0.5750795602798462
Epoch 600, training loss: 841.5156860351562 = 0.5122635364532471 + 100.0 * 8.4100341796875
Epoch 600, val loss: 0.5731233358383179
Epoch 610, training loss: 841.212158203125 = 0.509478747844696 + 100.0 * 8.407027244567871
Epoch 610, val loss: 0.5712526440620422
Epoch 620, training loss: 841.0458374023438 = 0.5068546533584595 + 100.0 * 8.405389785766602
Epoch 620, val loss: 0.5695330500602722
Epoch 630, training loss: 841.1743774414062 = 0.5043436884880066 + 100.0 * 8.406700134277344
Epoch 630, val loss: 0.5678930878639221
Epoch 640, training loss: 841.0940551757812 = 0.501868724822998 + 100.0 * 8.405921936035156
Epoch 640, val loss: 0.5663797855377197
Epoch 650, training loss: 840.5570068359375 = 0.49950870871543884 + 100.0 * 8.400574684143066
Epoch 650, val loss: 0.5648624300956726
Epoch 660, training loss: 840.5059814453125 = 0.4973021447658539 + 100.0 * 8.400086402893066
Epoch 660, val loss: 0.5634560585021973
Epoch 670, training loss: 840.5172729492188 = 0.49515673518180847 + 100.0 * 8.40022087097168
Epoch 670, val loss: 0.5621634125709534
Epoch 680, training loss: 840.25146484375 = 0.49304187297821045 + 100.0 * 8.397583961486816
Epoch 680, val loss: 0.5608257055282593
Epoch 690, training loss: 840.2049560546875 = 0.4910353720188141 + 100.0 * 8.397139549255371
Epoch 690, val loss: 0.5595670938491821
Epoch 700, training loss: 840.1206665039062 = 0.4891093373298645 + 100.0 * 8.396315574645996
Epoch 700, val loss: 0.5584645867347717
Epoch 710, training loss: 839.813720703125 = 0.4872433841228485 + 100.0 * 8.393264770507812
Epoch 710, val loss: 0.5573139786720276
Epoch 720, training loss: 839.633056640625 = 0.4854547381401062 + 100.0 * 8.391475677490234
Epoch 720, val loss: 0.5562358498573303
Epoch 730, training loss: 839.6746826171875 = 0.4837205708026886 + 100.0 * 8.3919095993042
Epoch 730, val loss: 0.5551567077636719
Epoch 740, training loss: 839.8035278320312 = 0.48196762800216675 + 100.0 * 8.39321517944336
Epoch 740, val loss: 0.5541142225265503
Epoch 750, training loss: 839.674072265625 = 0.48019981384277344 + 100.0 * 8.391938209533691
Epoch 750, val loss: 0.5531405210494995
Epoch 760, training loss: 839.1651611328125 = 0.4785386919975281 + 100.0 * 8.386866569519043
Epoch 760, val loss: 0.552118718624115
Epoch 770, training loss: 839.09814453125 = 0.47695842385292053 + 100.0 * 8.386212348937988
Epoch 770, val loss: 0.5512251257896423
Epoch 780, training loss: 839.4398803710938 = 0.4753994643688202 + 100.0 * 8.389644622802734
Epoch 780, val loss: 0.550360918045044
Epoch 790, training loss: 839.222412109375 = 0.4737674295902252 + 100.0 * 8.387486457824707
Epoch 790, val loss: 0.5492872595787048
Epoch 800, training loss: 838.840087890625 = 0.47219541668891907 + 100.0 * 8.383679389953613
Epoch 800, val loss: 0.5484107732772827
Epoch 810, training loss: 838.6986694335938 = 0.4707024395465851 + 100.0 * 8.382279396057129
Epoch 810, val loss: 0.5475398898124695
Epoch 820, training loss: 838.5689086914062 = 0.4692399203777313 + 100.0 * 8.380996704101562
Epoch 820, val loss: 0.5467144846916199
Epoch 830, training loss: 838.5030517578125 = 0.46779292821884155 + 100.0 * 8.380352973937988
Epoch 830, val loss: 0.5458943247795105
Epoch 840, training loss: 839.032958984375 = 0.466350257396698 + 100.0 * 8.385665893554688
Epoch 840, val loss: 0.5451838374137878
Epoch 850, training loss: 838.7435302734375 = 0.46483132243156433 + 100.0 * 8.382786750793457
Epoch 850, val loss: 0.5441452860832214
Epoch 860, training loss: 838.396240234375 = 0.4633590877056122 + 100.0 * 8.379328727722168
Epoch 860, val loss: 0.5432977676391602
Epoch 870, training loss: 838.3336791992188 = 0.4619233012199402 + 100.0 * 8.378717422485352
Epoch 870, val loss: 0.54240483045578
Epoch 880, training loss: 838.0812377929688 = 0.46053072810173035 + 100.0 * 8.37620735168457
Epoch 880, val loss: 0.5415958762168884
Epoch 890, training loss: 838.1090087890625 = 0.45916852355003357 + 100.0 * 8.376498222351074
Epoch 890, val loss: 0.5408178567886353
Epoch 900, training loss: 838.353271484375 = 0.45777276158332825 + 100.0 * 8.378954887390137
Epoch 900, val loss: 0.539859414100647
Epoch 910, training loss: 837.9512329101562 = 0.456338495016098 + 100.0 * 8.374948501586914
Epoch 910, val loss: 0.5390782356262207
Epoch 920, training loss: 837.776123046875 = 0.45494839549064636 + 100.0 * 8.373211860656738
Epoch 920, val loss: 0.5382912755012512
Epoch 930, training loss: 837.791015625 = 0.4536096155643463 + 100.0 * 8.373373985290527
Epoch 930, val loss: 0.5375768542289734
Epoch 940, training loss: 838.0136108398438 = 0.4522542655467987 + 100.0 * 8.37561321258545
Epoch 940, val loss: 0.5366665720939636
Epoch 950, training loss: 837.7288208007812 = 0.4508652985095978 + 100.0 * 8.372779846191406
Epoch 950, val loss: 0.5358308553695679
Epoch 960, training loss: 837.548095703125 = 0.44951823353767395 + 100.0 * 8.370985984802246
Epoch 960, val loss: 0.5350965261459351
Epoch 970, training loss: 837.4976806640625 = 0.44819727540016174 + 100.0 * 8.370494842529297
Epoch 970, val loss: 0.5343515276908875
Epoch 980, training loss: 838.0311889648438 = 0.44688448309898376 + 100.0 * 8.375843048095703
Epoch 980, val loss: 0.533750057220459
Epoch 990, training loss: 837.6299438476562 = 0.4454790949821472 + 100.0 * 8.371844291687012
Epoch 990, val loss: 0.5325093865394592
Epoch 1000, training loss: 837.3129272460938 = 0.444118857383728 + 100.0 * 8.368688583374023
Epoch 1000, val loss: 0.5318475961685181
Epoch 1010, training loss: 837.20458984375 = 0.4428156018257141 + 100.0 * 8.3676176071167
Epoch 1010, val loss: 0.5311458110809326
Epoch 1020, training loss: 837.18896484375 = 0.44153767824172974 + 100.0 * 8.367474555969238
Epoch 1020, val loss: 0.530238687992096
Epoch 1030, training loss: 837.693359375 = 0.44023197889328003 + 100.0 * 8.372530937194824
Epoch 1030, val loss: 0.5296257138252258
Epoch 1040, training loss: 837.0888671875 = 0.438875675201416 + 100.0 * 8.366499900817871
Epoch 1040, val loss: 0.5287054777145386
Epoch 1050, training loss: 837.0457763671875 = 0.43756845593452454 + 100.0 * 8.366082191467285
Epoch 1050, val loss: 0.5280922651290894
Epoch 1060, training loss: 837.3949584960938 = 0.43628618121147156 + 100.0 * 8.369586944580078
Epoch 1060, val loss: 0.5272603034973145
Epoch 1070, training loss: 837.0189819335938 = 0.4349694550037384 + 100.0 * 8.365839958190918
Epoch 1070, val loss: 0.526424765586853
Epoch 1080, training loss: 836.8768310546875 = 0.4336855411529541 + 100.0 * 8.364431381225586
Epoch 1080, val loss: 0.5257299542427063
Epoch 1090, training loss: 837.1094970703125 = 0.4324244260787964 + 100.0 * 8.36677074432373
Epoch 1090, val loss: 0.5248672962188721
Epoch 1100, training loss: 836.7804565429688 = 0.43109267950057983 + 100.0 * 8.363493919372559
Epoch 1100, val loss: 0.5242275595664978
Epoch 1110, training loss: 836.7725219726562 = 0.42978984117507935 + 100.0 * 8.36342716217041
Epoch 1110, val loss: 0.523295521736145
Epoch 1120, training loss: 836.6448974609375 = 0.4285241961479187 + 100.0 * 8.362163543701172
Epoch 1120, val loss: 0.5226181745529175
Epoch 1130, training loss: 836.6304931640625 = 0.42727968096733093 + 100.0 * 8.362031936645508
Epoch 1130, val loss: 0.5218405723571777
Epoch 1140, training loss: 836.8602294921875 = 0.42601847648620605 + 100.0 * 8.364341735839844
Epoch 1140, val loss: 0.5210069417953491
Epoch 1150, training loss: 836.7339477539062 = 0.4247092604637146 + 100.0 * 8.363092422485352
Epoch 1150, val loss: 0.5203065872192383
Epoch 1160, training loss: 836.5094604492188 = 0.4234233498573303 + 100.0 * 8.360860824584961
Epoch 1160, val loss: 0.5197145342826843
Epoch 1170, training loss: 836.3750610351562 = 0.42218101024627686 + 100.0 * 8.359528541564941
Epoch 1170, val loss: 0.518813967704773
Epoch 1180, training loss: 836.3803100585938 = 0.4209565222263336 + 100.0 * 8.359593391418457
Epoch 1180, val loss: 0.5181644558906555
Epoch 1190, training loss: 836.7020874023438 = 0.41971758008003235 + 100.0 * 8.362823486328125
Epoch 1190, val loss: 0.517441987991333
Epoch 1200, training loss: 836.4566650390625 = 0.41843223571777344 + 100.0 * 8.360382080078125
Epoch 1200, val loss: 0.5167807936668396
Epoch 1210, training loss: 836.68017578125 = 0.41718196868896484 + 100.0 * 8.362629890441895
Epoch 1210, val loss: 0.5157460570335388
Epoch 1220, training loss: 836.2979125976562 = 0.4158891439437866 + 100.0 * 8.358819961547852
Epoch 1220, val loss: 0.5154290199279785
Epoch 1230, training loss: 836.18701171875 = 0.41466224193573 + 100.0 * 8.357723236083984
Epoch 1230, val loss: 0.5145511627197266
Epoch 1240, training loss: 836.505615234375 = 0.4134346544742584 + 100.0 * 8.360921859741211
Epoch 1240, val loss: 0.5139291286468506
Epoch 1250, training loss: 836.0765380859375 = 0.41218268871307373 + 100.0 * 8.356643676757812
Epoch 1250, val loss: 0.5132741928100586
Epoch 1260, training loss: 835.99951171875 = 0.41096073389053345 + 100.0 * 8.35588550567627
Epoch 1260, val loss: 0.5125632882118225
Epoch 1270, training loss: 836.2591552734375 = 0.40976083278656006 + 100.0 * 8.35849380493164
Epoch 1270, val loss: 0.5117911696434021
Epoch 1280, training loss: 835.937255859375 = 0.4085003435611725 + 100.0 * 8.355287551879883
Epoch 1280, val loss: 0.5115123391151428
Epoch 1290, training loss: 835.8298950195312 = 0.40727588534355164 + 100.0 * 8.354226112365723
Epoch 1290, val loss: 0.5106593370437622
Epoch 1300, training loss: 836.0072021484375 = 0.4060756266117096 + 100.0 * 8.356011390686035
Epoch 1300, val loss: 0.5101343393325806
Epoch 1310, training loss: 835.8142700195312 = 0.4048444926738739 + 100.0 * 8.354094505310059
Epoch 1310, val loss: 0.5095994472503662
Epoch 1320, training loss: 835.8060302734375 = 0.40362823009490967 + 100.0 * 8.354023933410645
Epoch 1320, val loss: 0.5088650584220886
Epoch 1330, training loss: 835.9066772460938 = 0.40242430567741394 + 100.0 * 8.355042457580566
Epoch 1330, val loss: 0.50820392370224
Epoch 1340, training loss: 835.7440795898438 = 0.4012145698070526 + 100.0 * 8.353428840637207
Epoch 1340, val loss: 0.5077419877052307
Epoch 1350, training loss: 835.8240966796875 = 0.4000236988067627 + 100.0 * 8.354240417480469
Epoch 1350, val loss: 0.5070487856864929
Epoch 1360, training loss: 835.623046875 = 0.3988238275051117 + 100.0 * 8.352242469787598
Epoch 1360, val loss: 0.5065304040908813
Epoch 1370, training loss: 835.7816162109375 = 0.39763614535331726 + 100.0 * 8.353839874267578
Epoch 1370, val loss: 0.5061079859733582
Epoch 1380, training loss: 835.6085815429688 = 0.39643698930740356 + 100.0 * 8.352121353149414
Epoch 1380, val loss: 0.5055098533630371
Epoch 1390, training loss: 835.576416015625 = 0.39524251222610474 + 100.0 * 8.351811408996582
Epoch 1390, val loss: 0.5048414468765259
Epoch 1400, training loss: 835.60302734375 = 0.3940579891204834 + 100.0 * 8.352089881896973
Epoch 1400, val loss: 0.5042417049407959
Epoch 1410, training loss: 835.5874633789062 = 0.39288565516471863 + 100.0 * 8.351945877075195
Epoch 1410, val loss: 0.5036547780036926
Epoch 1420, training loss: 835.5826416015625 = 0.3916802704334259 + 100.0 * 8.351909637451172
Epoch 1420, val loss: 0.503387451171875
Epoch 1430, training loss: 835.416259765625 = 0.3904952108860016 + 100.0 * 8.350257873535156
Epoch 1430, val loss: 0.5029677748680115
Epoch 1440, training loss: 835.5291137695312 = 0.3893449306488037 + 100.0 * 8.351397514343262
Epoch 1440, val loss: 0.5021726489067078
Epoch 1450, training loss: 835.3751220703125 = 0.3881490230560303 + 100.0 * 8.349869728088379
Epoch 1450, val loss: 0.5020492076873779
Epoch 1460, training loss: 835.2556762695312 = 0.38696810603141785 + 100.0 * 8.348687171936035
Epoch 1460, val loss: 0.5013721585273743
Epoch 1470, training loss: 835.3894653320312 = 0.3858114778995514 + 100.0 * 8.35003662109375
Epoch 1470, val loss: 0.5009210109710693
Epoch 1480, training loss: 835.167724609375 = 0.3846440613269806 + 100.0 * 8.347830772399902
Epoch 1480, val loss: 0.5005183815956116
Epoch 1490, training loss: 835.2127075195312 = 0.3834958076477051 + 100.0 * 8.348292350769043
Epoch 1490, val loss: 0.5000560879707336
Epoch 1500, training loss: 835.627197265625 = 0.3823351562023163 + 100.0 * 8.352448463439941
Epoch 1500, val loss: 0.49970006942749023
Epoch 1510, training loss: 835.1882934570312 = 0.38114097714424133 + 100.0 * 8.348071098327637
Epoch 1510, val loss: 0.4990280568599701
Epoch 1520, training loss: 835.072509765625 = 0.3799814283847809 + 100.0 * 8.346924781799316
Epoch 1520, val loss: 0.49876099824905396
Epoch 1530, training loss: 835.2940063476562 = 0.3788391351699829 + 100.0 * 8.349151611328125
Epoch 1530, val loss: 0.49838563799858093
Epoch 1540, training loss: 834.9713745117188 = 0.37767693400382996 + 100.0 * 8.34593677520752
Epoch 1540, val loss: 0.4977865517139435
Epoch 1550, training loss: 835.1248168945312 = 0.3765276372432709 + 100.0 * 8.347482681274414
Epoch 1550, val loss: 0.49737951159477234
Epoch 1560, training loss: 835.0489501953125 = 0.37536415457725525 + 100.0 * 8.346735954284668
Epoch 1560, val loss: 0.4971763491630554
Epoch 1570, training loss: 835.1763305664062 = 0.3742082118988037 + 100.0 * 8.348021507263184
Epoch 1570, val loss: 0.49656572937965393
Epoch 1580, training loss: 834.9918212890625 = 0.3730294108390808 + 100.0 * 8.346187591552734
Epoch 1580, val loss: 0.49620625376701355
Epoch 1590, training loss: 834.80810546875 = 0.37186238169670105 + 100.0 * 8.344362258911133
Epoch 1590, val loss: 0.4959641695022583
Epoch 1600, training loss: 834.7455444335938 = 0.37071549892425537 + 100.0 * 8.343748092651367
Epoch 1600, val loss: 0.4956854581832886
Epoch 1610, training loss: 834.8411865234375 = 0.36957043409347534 + 100.0 * 8.34471607208252
Epoch 1610, val loss: 0.4952712953090668
Epoch 1620, training loss: 834.9861450195312 = 0.36839815974235535 + 100.0 * 8.346177101135254
Epoch 1620, val loss: 0.4949464797973633
Epoch 1630, training loss: 834.9454345703125 = 0.36720892786979675 + 100.0 * 8.345782279968262
Epoch 1630, val loss: 0.4945417046546936
Epoch 1640, training loss: 834.793212890625 = 0.3660143315792084 + 100.0 * 8.344271659851074
Epoch 1640, val loss: 0.4943355917930603
Epoch 1650, training loss: 834.7486572265625 = 0.36484429240226746 + 100.0 * 8.34383773803711
Epoch 1650, val loss: 0.49408766627311707
Epoch 1660, training loss: 835.0034790039062 = 0.36368125677108765 + 100.0 * 8.346397399902344
Epoch 1660, val loss: 0.49411171674728394
Epoch 1670, training loss: 834.5923461914062 = 0.36247214674949646 + 100.0 * 8.34229850769043
Epoch 1670, val loss: 0.4931209981441498
Epoch 1680, training loss: 834.5648803710938 = 0.3612964451313019 + 100.0 * 8.342036247253418
Epoch 1680, val loss: 0.49302443861961365
Epoch 1690, training loss: 834.7088012695312 = 0.3601324260234833 + 100.0 * 8.343486785888672
Epoch 1690, val loss: 0.49260494112968445
Epoch 1700, training loss: 834.485595703125 = 0.35894882678985596 + 100.0 * 8.341266632080078
Epoch 1700, val loss: 0.49248388409614563
Epoch 1710, training loss: 834.4574584960938 = 0.35777127742767334 + 100.0 * 8.340996742248535
Epoch 1710, val loss: 0.4923405945301056
Epoch 1720, training loss: 834.4686889648438 = 0.35660359263420105 + 100.0 * 8.341120719909668
Epoch 1720, val loss: 0.4920913875102997
Epoch 1730, training loss: 834.7938232421875 = 0.35542675852775574 + 100.0 * 8.34438419342041
Epoch 1730, val loss: 0.491775780916214
Epoch 1740, training loss: 834.4260864257812 = 0.35421743988990784 + 100.0 * 8.340719223022461
Epoch 1740, val loss: 0.4913904070854187
Epoch 1750, training loss: 834.6350708007812 = 0.3530128300189972 + 100.0 * 8.342820167541504
Epoch 1750, val loss: 0.49138882756233215
Epoch 1760, training loss: 834.406494140625 = 0.3517957925796509 + 100.0 * 8.340546607971191
Epoch 1760, val loss: 0.49118727445602417
Epoch 1770, training loss: 834.321044921875 = 0.35058173537254333 + 100.0 * 8.339704513549805
Epoch 1770, val loss: 0.4908933937549591
Epoch 1780, training loss: 834.2527465820312 = 0.3493916988372803 + 100.0 * 8.339034080505371
Epoch 1780, val loss: 0.49056482315063477
Epoch 1790, training loss: 834.2954711914062 = 0.348204642534256 + 100.0 * 8.339472770690918
Epoch 1790, val loss: 0.4903429448604584
Epoch 1800, training loss: 834.5319213867188 = 0.34700220823287964 + 100.0 * 8.341849327087402
Epoch 1800, val loss: 0.490120530128479
Epoch 1810, training loss: 834.4337158203125 = 0.3457675874233246 + 100.0 * 8.340879440307617
Epoch 1810, val loss: 0.49037620425224304
Epoch 1820, training loss: 834.3961791992188 = 0.3445122539997101 + 100.0 * 8.340517044067383
Epoch 1820, val loss: 0.4897771179676056
Epoch 1830, training loss: 834.1913452148438 = 0.3432639539241791 + 100.0 * 8.338480949401855
Epoch 1830, val loss: 0.48966458439826965
Epoch 1840, training loss: 834.0736083984375 = 0.3420322835445404 + 100.0 * 8.337315559387207
Epoch 1840, val loss: 0.4895349144935608
Epoch 1850, training loss: 834.2614135742188 = 0.34080639481544495 + 100.0 * 8.339205741882324
Epoch 1850, val loss: 0.4894980490207672
Epoch 1860, training loss: 834.122314453125 = 0.33955109119415283 + 100.0 * 8.337827682495117
Epoch 1860, val loss: 0.4892860949039459
Epoch 1870, training loss: 834.1878662109375 = 0.3382844030857086 + 100.0 * 8.338496208190918
Epoch 1870, val loss: 0.4890531301498413
Epoch 1880, training loss: 834.10546875 = 0.33701643347740173 + 100.0 * 8.337684631347656
Epoch 1880, val loss: 0.48900872468948364
Epoch 1890, training loss: 834.0350952148438 = 0.33573848009109497 + 100.0 * 8.336993217468262
Epoch 1890, val loss: 0.4886765778064728
Epoch 1900, training loss: 834.28125 = 0.33446797728538513 + 100.0 * 8.339468002319336
Epoch 1900, val loss: 0.48843804001808167
Epoch 1910, training loss: 834.3671875 = 0.33315542340278625 + 100.0 * 8.340340614318848
Epoch 1910, val loss: 0.48833420872688293
Epoch 1920, training loss: 834.0014038085938 = 0.3318372368812561 + 100.0 * 8.336695671081543
Epoch 1920, val loss: 0.48805761337280273
Epoch 1930, training loss: 833.8578491210938 = 0.3305489718914032 + 100.0 * 8.335272789001465
Epoch 1930, val loss: 0.4879513084888458
Epoch 1940, training loss: 833.7936401367188 = 0.3292773365974426 + 100.0 * 8.334643363952637
Epoch 1940, val loss: 0.4876837730407715
Epoch 1950, training loss: 833.7664794921875 = 0.32800787687301636 + 100.0 * 8.33438491821289
Epoch 1950, val loss: 0.48759087920188904
Epoch 1960, training loss: 834.00146484375 = 0.3267480432987213 + 100.0 * 8.336747169494629
Epoch 1960, val loss: 0.4870656132698059
Epoch 1970, training loss: 833.9075317382812 = 0.3254161775112152 + 100.0 * 8.335821151733398
Epoch 1970, val loss: 0.48714110255241394
Epoch 1980, training loss: 833.808349609375 = 0.3240737020969391 + 100.0 * 8.334842681884766
Epoch 1980, val loss: 0.48689091205596924
Epoch 1990, training loss: 833.8372802734375 = 0.322761595249176 + 100.0 * 8.335144996643066
Epoch 1990, val loss: 0.4871256947517395
Epoch 2000, training loss: 833.76708984375 = 0.3214631974697113 + 100.0 * 8.334456443786621
Epoch 2000, val loss: 0.4867027997970581
Epoch 2010, training loss: 833.8389892578125 = 0.3201615810394287 + 100.0 * 8.335187911987305
Epoch 2010, val loss: 0.48660746216773987
Epoch 2020, training loss: 833.9773559570312 = 0.31886065006256104 + 100.0 * 8.33658504486084
Epoch 2020, val loss: 0.48682111501693726
Epoch 2030, training loss: 833.6886596679688 = 0.3175221085548401 + 100.0 * 8.333711624145508
Epoch 2030, val loss: 0.48640766739845276
Epoch 2040, training loss: 833.660888671875 = 0.3162158727645874 + 100.0 * 8.333446502685547
Epoch 2040, val loss: 0.48617982864379883
Epoch 2050, training loss: 833.8154907226562 = 0.3149145245552063 + 100.0 * 8.335005760192871
Epoch 2050, val loss: 0.48617419600486755
Epoch 2060, training loss: 833.6618041992188 = 0.3135907053947449 + 100.0 * 8.333481788635254
Epoch 2060, val loss: 0.4861242175102234
Epoch 2070, training loss: 833.9428100585938 = 0.3122663199901581 + 100.0 * 8.336305618286133
Epoch 2070, val loss: 0.48606076836586
Epoch 2080, training loss: 833.5667724609375 = 0.31093236804008484 + 100.0 * 8.332558631896973
Epoch 2080, val loss: 0.4860747456550598
Epoch 2090, training loss: 833.4596557617188 = 0.30961257219314575 + 100.0 * 8.331500053405762
Epoch 2090, val loss: 0.48602449893951416
Epoch 2100, training loss: 833.4644775390625 = 0.3083016872406006 + 100.0 * 8.331562042236328
Epoch 2100, val loss: 0.4859484136104584
Epoch 2110, training loss: 834.3048095703125 = 0.30701717734336853 + 100.0 * 8.339978218078613
Epoch 2110, val loss: 0.48548653721809387
Epoch 2120, training loss: 833.7113037109375 = 0.3056091070175171 + 100.0 * 8.334056854248047
Epoch 2120, val loss: 0.486289918422699
Epoch 2130, training loss: 833.47705078125 = 0.30424949526786804 + 100.0 * 8.331727981567383
Epoch 2130, val loss: 0.4857746362686157
Epoch 2140, training loss: 833.3429565429688 = 0.30289918184280396 + 100.0 * 8.330400466918945
Epoch 2140, val loss: 0.4859100878238678
Epoch 2150, training loss: 833.4136962890625 = 0.30156806111335754 + 100.0 * 8.331121444702148
Epoch 2150, val loss: 0.48569783568382263
Epoch 2160, training loss: 833.8417358398438 = 0.30022677779197693 + 100.0 * 8.33541488647461
Epoch 2160, val loss: 0.48559218645095825
Epoch 2170, training loss: 833.5809326171875 = 0.2988375425338745 + 100.0 * 8.332820892333984
Epoch 2170, val loss: 0.48586171865463257
Epoch 2180, training loss: 833.4043579101562 = 0.2974695563316345 + 100.0 * 8.331068992614746
Epoch 2180, val loss: 0.48561957478523254
Epoch 2190, training loss: 833.4008178710938 = 0.29609736800193787 + 100.0 * 8.331047058105469
Epoch 2190, val loss: 0.48583078384399414
Epoch 2200, training loss: 833.3810424804688 = 0.2947271168231964 + 100.0 * 8.330862998962402
Epoch 2200, val loss: 0.48607856035232544
Epoch 2210, training loss: 833.505126953125 = 0.293345183134079 + 100.0 * 8.332118034362793
Epoch 2210, val loss: 0.4860493242740631
Epoch 2220, training loss: 833.2135620117188 = 0.2919575273990631 + 100.0 * 8.329216003417969
Epoch 2220, val loss: 0.4859826862812042
Epoch 2230, training loss: 833.2046508789062 = 0.2905828654766083 + 100.0 * 8.329140663146973
Epoch 2230, val loss: 0.48602771759033203
Epoch 2240, training loss: 833.1205444335938 = 0.2891983687877655 + 100.0 * 8.328313827514648
Epoch 2240, val loss: 0.48619115352630615
Epoch 2250, training loss: 833.5540771484375 = 0.28783151507377625 + 100.0 * 8.332662582397461
Epoch 2250, val loss: 0.48597124218940735
Epoch 2260, training loss: 833.099853515625 = 0.2863961458206177 + 100.0 * 8.328134536743164
Epoch 2260, val loss: 0.48662838339805603
Epoch 2270, training loss: 833.2197265625 = 0.28498372435569763 + 100.0 * 8.329347610473633
Epoch 2270, val loss: 0.48630428314208984
Epoch 2280, training loss: 833.3347778320312 = 0.2835647165775299 + 100.0 * 8.330512046813965
Epoch 2280, val loss: 0.48666173219680786
Epoch 2290, training loss: 833.2279663085938 = 0.28214433789253235 + 100.0 * 8.329458236694336
Epoch 2290, val loss: 0.48654621839523315
Epoch 2300, training loss: 833.2764892578125 = 0.28073471784591675 + 100.0 * 8.329957008361816
Epoch 2300, val loss: 0.48682597279548645
Epoch 2310, training loss: 833.1034545898438 = 0.27931222319602966 + 100.0 * 8.328241348266602
Epoch 2310, val loss: 0.48697760701179504
Epoch 2320, training loss: 833.0300903320312 = 0.2779011130332947 + 100.0 * 8.327522277832031
Epoch 2320, val loss: 0.4873020350933075
Epoch 2330, training loss: 833.040283203125 = 0.27650031447410583 + 100.0 * 8.327637672424316
Epoch 2330, val loss: 0.48767825961112976
Epoch 2340, training loss: 833.5857543945312 = 0.27508965134620667 + 100.0 * 8.333106994628906
Epoch 2340, val loss: 0.4879211187362671
Epoch 2350, training loss: 833.0396728515625 = 0.27363812923431396 + 100.0 * 8.32766056060791
Epoch 2350, val loss: 0.4875429570674896
Epoch 2360, training loss: 832.896484375 = 0.2722087800502777 + 100.0 * 8.326242446899414
Epoch 2360, val loss: 0.48820167779922485
Epoch 2370, training loss: 832.8516845703125 = 0.2707902789115906 + 100.0 * 8.32580852508545
Epoch 2370, val loss: 0.4883144497871399
Epoch 2380, training loss: 833.1841430664062 = 0.2693914771080017 + 100.0 * 8.329147338867188
Epoch 2380, val loss: 0.48868244886398315
Epoch 2390, training loss: 832.9268188476562 = 0.2679283320903778 + 100.0 * 8.32658863067627
Epoch 2390, val loss: 0.48879584670066833
Epoch 2400, training loss: 832.9161987304688 = 0.26648375391960144 + 100.0 * 8.326497077941895
Epoch 2400, val loss: 0.4888026714324951
Epoch 2410, training loss: 832.9071655273438 = 0.2650442123413086 + 100.0 * 8.326421737670898
Epoch 2410, val loss: 0.48923951387405396
Epoch 2420, training loss: 833.2510375976562 = 0.2636178433895111 + 100.0 * 8.329874038696289
Epoch 2420, val loss: 0.4896438419818878
Epoch 2430, training loss: 833.0195922851562 = 0.262175977230072 + 100.0 * 8.327574729919434
Epoch 2430, val loss: 0.4901730418205261
Epoch 2440, training loss: 832.7857055664062 = 0.2607128918170929 + 100.0 * 8.325249671936035
Epoch 2440, val loss: 0.49025118350982666
Epoch 2450, training loss: 832.6904907226562 = 0.25927266478538513 + 100.0 * 8.324312210083008
Epoch 2450, val loss: 0.49069634079933167
Epoch 2460, training loss: 832.689208984375 = 0.25784197449684143 + 100.0 * 8.32431411743164
Epoch 2460, val loss: 0.49113643169403076
Epoch 2470, training loss: 832.8093872070312 = 0.2564184367656708 + 100.0 * 8.325530052185059
Epoch 2470, val loss: 0.4914770722389221
Epoch 2480, training loss: 833.015380859375 = 0.25497275590896606 + 100.0 * 8.327604293823242
Epoch 2480, val loss: 0.4917812943458557
Epoch 2490, training loss: 833.1563110351562 = 0.25350770354270935 + 100.0 * 8.329028129577637
Epoch 2490, val loss: 0.4924215078353882
Epoch 2500, training loss: 832.738525390625 = 0.25204214453697205 + 100.0 * 8.324865341186523
Epoch 2500, val loss: 0.4930243492126465
Epoch 2510, training loss: 832.6632080078125 = 0.2505805194377899 + 100.0 * 8.324126243591309
Epoch 2510, val loss: 0.49339184165000916
Epoch 2520, training loss: 832.597900390625 = 0.24912713468074799 + 100.0 * 8.323487281799316
Epoch 2520, val loss: 0.49374210834503174
Epoch 2530, training loss: 832.6373901367188 = 0.2476854920387268 + 100.0 * 8.323897361755371
Epoch 2530, val loss: 0.4943770468235016
Epoch 2540, training loss: 833.0519409179688 = 0.24626386165618896 + 100.0 * 8.328056335449219
Epoch 2540, val loss: 0.49519190192222595
Epoch 2550, training loss: 832.7811889648438 = 0.24477127194404602 + 100.0 * 8.325364112854004
Epoch 2550, val loss: 0.4949047267436981
Epoch 2560, training loss: 832.769287109375 = 0.24332235753536224 + 100.0 * 8.3252592086792
Epoch 2560, val loss: 0.4957226514816284
Epoch 2570, training loss: 832.5906372070312 = 0.24186129868030548 + 100.0 * 8.323487281799316
Epoch 2570, val loss: 0.4958607256412506
Epoch 2580, training loss: 832.5390014648438 = 0.2404138445854187 + 100.0 * 8.322985649108887
Epoch 2580, val loss: 0.496457576751709
Epoch 2590, training loss: 832.64208984375 = 0.23900607228279114 + 100.0 * 8.324030876159668
Epoch 2590, val loss: 0.49631980061531067
Epoch 2600, training loss: 832.871337890625 = 0.23755764961242676 + 100.0 * 8.326337814331055
Epoch 2600, val loss: 0.4972928464412689
Epoch 2610, training loss: 832.4417724609375 = 0.23609799146652222 + 100.0 * 8.322056770324707
Epoch 2610, val loss: 0.49798277020454407
Epoch 2620, training loss: 832.4029541015625 = 0.23467537760734558 + 100.0 * 8.321682929992676
Epoch 2620, val loss: 0.4987913966178894
Epoch 2630, training loss: 832.3789672851562 = 0.23327255249023438 + 100.0 * 8.321456909179688
Epoch 2630, val loss: 0.49900856614112854
Epoch 2640, training loss: 832.400146484375 = 0.23187604546546936 + 100.0 * 8.321682929992676
Epoch 2640, val loss: 0.499815434217453
Epoch 2650, training loss: 833.1435546875 = 0.23049190640449524 + 100.0 * 8.329131126403809
Epoch 2650, val loss: 0.5002545118331909
Epoch 2660, training loss: 832.6717529296875 = 0.22903987765312195 + 100.0 * 8.324426651000977
Epoch 2660, val loss: 0.5011335015296936
Epoch 2670, training loss: 832.5833740234375 = 0.22761128842830658 + 100.0 * 8.32355785369873
Epoch 2670, val loss: 0.5016420483589172
Epoch 2680, training loss: 832.4699096679688 = 0.22617588937282562 + 100.0 * 8.322437286376953
Epoch 2680, val loss: 0.5023073554039001
Epoch 2690, training loss: 832.3218994140625 = 0.22475646436214447 + 100.0 * 8.320971488952637
Epoch 2690, val loss: 0.5028720498085022
Epoch 2700, training loss: 832.3151245117188 = 0.22334372997283936 + 100.0 * 8.320918083190918
Epoch 2700, val loss: 0.5035477876663208
Epoch 2710, training loss: 832.6123046875 = 0.22193989157676697 + 100.0 * 8.323904037475586
Epoch 2710, val loss: 0.504329264163971
Epoch 2720, training loss: 832.3521118164062 = 0.22050580382347107 + 100.0 * 8.32131576538086
Epoch 2720, val loss: 0.5050018429756165
Epoch 2730, training loss: 832.8597412109375 = 0.21909141540527344 + 100.0 * 8.326406478881836
Epoch 2730, val loss: 0.5054742693901062
Epoch 2740, training loss: 832.3221435546875 = 0.21763816475868225 + 100.0 * 8.321044921875
Epoch 2740, val loss: 0.5060901045799255
Epoch 2750, training loss: 832.2011108398438 = 0.21620309352874756 + 100.0 * 8.319849014282227
Epoch 2750, val loss: 0.5072067379951477
Epoch 2760, training loss: 832.1659545898438 = 0.21478623151779175 + 100.0 * 8.319511413574219
Epoch 2760, val loss: 0.5076856017112732
Epoch 2770, training loss: 832.1851196289062 = 0.21337735652923584 + 100.0 * 8.319717407226562
Epoch 2770, val loss: 0.5087853074073792
Epoch 2780, training loss: 832.7665405273438 = 0.21202018857002258 + 100.0 * 8.325545310974121
Epoch 2780, val loss: 0.5099729895591736
Epoch 2790, training loss: 832.2687377929688 = 0.21053490042686462 + 100.0 * 8.320582389831543
Epoch 2790, val loss: 0.5099577307701111
Epoch 2800, training loss: 832.2898559570312 = 0.2091107964515686 + 100.0 * 8.320807456970215
Epoch 2800, val loss: 0.5109085440635681
Epoch 2810, training loss: 832.1593017578125 = 0.20769333839416504 + 100.0 * 8.3195161819458
Epoch 2810, val loss: 0.5117484927177429
Epoch 2820, training loss: 832.1024169921875 = 0.20628254115581512 + 100.0 * 8.318961143493652
Epoch 2820, val loss: 0.5125530958175659
Epoch 2830, training loss: 832.2947998046875 = 0.20488280057907104 + 100.0 * 8.32089900970459
Epoch 2830, val loss: 0.5131770372390747
Epoch 2840, training loss: 832.2009887695312 = 0.20346735417842865 + 100.0 * 8.319974899291992
Epoch 2840, val loss: 0.5140929222106934
Epoch 2850, training loss: 832.2522583007812 = 0.20205610990524292 + 100.0 * 8.320502281188965
Epoch 2850, val loss: 0.5155735015869141
Epoch 2860, training loss: 832.4419555664062 = 0.20067435503005981 + 100.0 * 8.322412490844727
Epoch 2860, val loss: 0.5169224739074707
Epoch 2870, training loss: 832.251953125 = 0.19924063980579376 + 100.0 * 8.320527076721191
Epoch 2870, val loss: 0.517086386680603
Epoch 2880, training loss: 832.1724243164062 = 0.19784240424633026 + 100.0 * 8.319746017456055
Epoch 2880, val loss: 0.5179080367088318
Epoch 2890, training loss: 832.1127319335938 = 0.1964321732521057 + 100.0 * 8.31916332244873
Epoch 2890, val loss: 0.5188371539115906
Epoch 2900, training loss: 832.1506958007812 = 0.19506588578224182 + 100.0 * 8.31955623626709
Epoch 2900, val loss: 0.5190643072128296
Epoch 2910, training loss: 832.155029296875 = 0.19366410374641418 + 100.0 * 8.319613456726074
Epoch 2910, val loss: 0.5203492641448975
Epoch 2920, training loss: 832.0123901367188 = 0.1922614425420761 + 100.0 * 8.318201065063477
Epoch 2920, val loss: 0.5220537781715393
Epoch 2930, training loss: 832.0061645507812 = 0.19088055193424225 + 100.0 * 8.318153381347656
Epoch 2930, val loss: 0.5228237509727478
Epoch 2940, training loss: 831.9852294921875 = 0.18950287997722626 + 100.0 * 8.317956924438477
Epoch 2940, val loss: 0.5235011577606201
Epoch 2950, training loss: 832.3487548828125 = 0.1881577968597412 + 100.0 * 8.321605682373047
Epoch 2950, val loss: 0.5241236686706543
Epoch 2960, training loss: 832.0518188476562 = 0.1867615282535553 + 100.0 * 8.318650245666504
Epoch 2960, val loss: 0.5252986550331116
Epoch 2970, training loss: 832.1104736328125 = 0.18539254367351532 + 100.0 * 8.31925106048584
Epoch 2970, val loss: 0.5263292789459229
Epoch 2980, training loss: 832.2369384765625 = 0.18402332067489624 + 100.0 * 8.320528984069824
Epoch 2980, val loss: 0.5275181531906128
Epoch 2990, training loss: 831.9764404296875 = 0.1826498806476593 + 100.0 * 8.317937850952148
Epoch 2990, val loss: 0.528437614440918
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8188736681887366
0.8421357675867566
=== training gcn model ===
Epoch 0, training loss: 1059.334716796875 = 1.1118170022964478 + 100.0 * 10.582228660583496
Epoch 0, val loss: 1.1107851266860962
Epoch 10, training loss: 1059.1951904296875 = 1.104918122291565 + 100.0 * 10.580903053283691
Epoch 10, val loss: 1.103762149810791
Epoch 20, training loss: 1057.989990234375 = 1.0963786840438843 + 100.0 * 10.56893539428711
Epoch 20, val loss: 1.0950475931167603
Epoch 30, training loss: 1049.85205078125 = 1.0855857133865356 + 100.0 * 10.487664222717285
Epoch 30, val loss: 1.0840994119644165
Epoch 40, training loss: 1017.7449340820312 = 1.073883295059204 + 100.0 * 10.166709899902344
Epoch 40, val loss: 1.0725644826889038
Epoch 50, training loss: 977.751953125 = 1.0621066093444824 + 100.0 * 9.766898155212402
Epoch 50, val loss: 1.0611238479614258
Epoch 60, training loss: 952.7100830078125 = 1.0531831979751587 + 100.0 * 9.516569137573242
Epoch 60, val loss: 1.0528563261032104
Epoch 70, training loss: 931.12841796875 = 1.046416163444519 + 100.0 * 9.300820350646973
Epoch 70, val loss: 1.046571135520935
Epoch 80, training loss: 916.8848876953125 = 1.0404645204544067 + 100.0 * 9.15844440460205
Epoch 80, val loss: 1.040850281715393
Epoch 90, training loss: 908.326416015625 = 1.034610390663147 + 100.0 * 9.072917938232422
Epoch 90, val loss: 1.0351881980895996
Epoch 100, training loss: 894.0860595703125 = 1.0302324295043945 + 100.0 * 8.930558204650879
Epoch 100, val loss: 1.0311154127120972
Epoch 110, training loss: 885.9132690429688 = 1.0267986059188843 + 100.0 * 8.848864555358887
Epoch 110, val loss: 1.027812123298645
Epoch 120, training loss: 881.4425048828125 = 1.0216237306594849 + 100.0 * 8.804208755493164
Epoch 120, val loss: 1.0228257179260254
Epoch 130, training loss: 876.7508544921875 = 1.0159449577331543 + 100.0 * 8.757349014282227
Epoch 130, val loss: 1.0175113677978516
Epoch 140, training loss: 871.666748046875 = 1.0107872486114502 + 100.0 * 8.706559181213379
Epoch 140, val loss: 1.012632966041565
Epoch 150, training loss: 867.3953247070312 = 1.0052181482315063 + 100.0 * 8.663901329040527
Epoch 150, val loss: 1.0073195695877075
Epoch 160, training loss: 863.9353637695312 = 0.9988643527030945 + 100.0 * 8.629364967346191
Epoch 160, val loss: 1.0012743473052979
Epoch 170, training loss: 860.9249267578125 = 0.9918434023857117 + 100.0 * 8.59933090209961
Epoch 170, val loss: 0.9946097731590271
Epoch 180, training loss: 858.876708984375 = 0.9837048053741455 + 100.0 * 8.578929901123047
Epoch 180, val loss: 0.9868217706680298
Epoch 190, training loss: 856.9861450195312 = 0.9743733406066895 + 100.0 * 8.560117721557617
Epoch 190, val loss: 0.9778863787651062
Epoch 200, training loss: 855.413330078125 = 0.9640665054321289 + 100.0 * 8.544492721557617
Epoch 200, val loss: 0.9680860638618469
Epoch 210, training loss: 854.0615234375 = 0.952802300453186 + 100.0 * 8.531086921691895
Epoch 210, val loss: 0.9573733806610107
Epoch 220, training loss: 853.459716796875 = 0.9406220316886902 + 100.0 * 8.525191307067871
Epoch 220, val loss: 0.9457616806030273
Epoch 230, training loss: 852.04833984375 = 0.9274749755859375 + 100.0 * 8.511208534240723
Epoch 230, val loss: 0.9333691000938416
Epoch 240, training loss: 851.33203125 = 0.9136070609092712 + 100.0 * 8.504183769226074
Epoch 240, val loss: 0.9201903343200684
Epoch 250, training loss: 850.2435913085938 = 0.899168074131012 + 100.0 * 8.493444442749023
Epoch 250, val loss: 0.906592845916748
Epoch 260, training loss: 849.502197265625 = 0.8843657970428467 + 100.0 * 8.486178398132324
Epoch 260, val loss: 0.8926135897636414
Epoch 270, training loss: 848.793212890625 = 0.8692847490310669 + 100.0 * 8.479239463806152
Epoch 270, val loss: 0.8783219456672668
Epoch 280, training loss: 848.900390625 = 0.8540013432502747 + 100.0 * 8.480463981628418
Epoch 280, val loss: 0.8637934327125549
Epoch 290, training loss: 847.9247436523438 = 0.8384326696395874 + 100.0 * 8.470863342285156
Epoch 290, val loss: 0.8490279912948608
Epoch 300, training loss: 847.18310546875 = 0.8230236172676086 + 100.0 * 8.463601112365723
Epoch 300, val loss: 0.8344457745552063
Epoch 310, training loss: 846.577880859375 = 0.8079116344451904 + 100.0 * 8.4576997756958
Epoch 310, val loss: 0.8200420141220093
Epoch 320, training loss: 846.1004028320312 = 0.7930023074150085 + 100.0 * 8.453073501586914
Epoch 320, val loss: 0.8058294057846069
Epoch 330, training loss: 846.1192626953125 = 0.7782961130142212 + 100.0 * 8.453409194946289
Epoch 330, val loss: 0.7918002605438232
Epoch 340, training loss: 845.5263061523438 = 0.7636786103248596 + 100.0 * 8.447626113891602
Epoch 340, val loss: 0.7776637077331543
Epoch 350, training loss: 845.0833740234375 = 0.7492669224739075 + 100.0 * 8.443341255187988
Epoch 350, val loss: 0.7638978362083435
Epoch 360, training loss: 844.6246337890625 = 0.7351292967796326 + 100.0 * 8.438895225524902
Epoch 360, val loss: 0.750275731086731
Epoch 370, training loss: 844.250244140625 = 0.7212070226669312 + 100.0 * 8.435290336608887
Epoch 370, val loss: 0.736966609954834
Epoch 380, training loss: 843.8626098632812 = 0.7074988484382629 + 100.0 * 8.431550979614258
Epoch 380, val loss: 0.7237133979797363
Epoch 390, training loss: 844.2655639648438 = 0.6938841342926025 + 100.0 * 8.43571662902832
Epoch 390, val loss: 0.7107287049293518
Epoch 400, training loss: 843.255859375 = 0.6804007291793823 + 100.0 * 8.42575454711914
Epoch 400, val loss: 0.6975941061973572
Epoch 410, training loss: 843.60888671875 = 0.6672424674034119 + 100.0 * 8.42941665649414
Epoch 410, val loss: 0.6847401857376099
Epoch 420, training loss: 842.734375 = 0.6542171239852905 + 100.0 * 8.420801162719727
Epoch 420, val loss: 0.6723629236221313
Epoch 430, training loss: 842.4577026367188 = 0.6416592001914978 + 100.0 * 8.418160438537598
Epoch 430, val loss: 0.6604475378990173
Epoch 440, training loss: 842.233154296875 = 0.6295246481895447 + 100.0 * 8.416036605834961
Epoch 440, val loss: 0.6487340331077576
Epoch 450, training loss: 842.0288696289062 = 0.6177089214324951 + 100.0 * 8.414111137390137
Epoch 450, val loss: 0.637575089931488
Epoch 460, training loss: 841.9303588867188 = 0.6063953042030334 + 100.0 * 8.413239479064941
Epoch 460, val loss: 0.6268819570541382
Epoch 470, training loss: 841.5591430664062 = 0.5957577228546143 + 100.0 * 8.40963363647461
Epoch 470, val loss: 0.6169150471687317
Epoch 480, training loss: 841.4160766601562 = 0.5857773423194885 + 100.0 * 8.408303260803223
Epoch 480, val loss: 0.6076348423957825
Epoch 490, training loss: 841.5640258789062 = 0.5763475298881531 + 100.0 * 8.409876823425293
Epoch 490, val loss: 0.5989766716957092
Epoch 500, training loss: 841.447021484375 = 0.5674875378608704 + 100.0 * 8.408795356750488
Epoch 500, val loss: 0.5906608700752258
Epoch 510, training loss: 840.9287719726562 = 0.5592598915100098 + 100.0 * 8.403695106506348
Epoch 510, val loss: 0.5834618806838989
Epoch 520, training loss: 840.627685546875 = 0.5517931580543518 + 100.0 * 8.400758743286133
Epoch 520, val loss: 0.5766962170600891
Epoch 530, training loss: 840.4765014648438 = 0.5449431538581848 + 100.0 * 8.39931583404541
Epoch 530, val loss: 0.570608913898468
Epoch 540, training loss: 840.6220092773438 = 0.5385594367980957 + 100.0 * 8.400834083557129
Epoch 540, val loss: 0.5651211142539978
Epoch 550, training loss: 840.322509765625 = 0.5327009558677673 + 100.0 * 8.397897720336914
Epoch 550, val loss: 0.5600765347480774
Epoch 560, training loss: 839.9475708007812 = 0.5274366140365601 + 100.0 * 8.394201278686523
Epoch 560, val loss: 0.5555269718170166
Epoch 570, training loss: 840.1409301757812 = 0.5226295590400696 + 100.0 * 8.396183013916016
Epoch 570, val loss: 0.551356852054596
Epoch 580, training loss: 839.7481079101562 = 0.5180785655975342 + 100.0 * 8.392300605773926
Epoch 580, val loss: 0.5480083227157593
Epoch 590, training loss: 839.5947875976562 = 0.513994574546814 + 100.0 * 8.39080810546875
Epoch 590, val loss: 0.5446212291717529
Epoch 600, training loss: 839.415283203125 = 0.5102569460868835 + 100.0 * 8.389050483703613
Epoch 600, val loss: 0.5418850183486938
Epoch 610, training loss: 839.9144287109375 = 0.5068012475967407 + 100.0 * 8.394076347351074
Epoch 610, val loss: 0.5392040610313416
Epoch 620, training loss: 839.49853515625 = 0.5034746527671814 + 100.0 * 8.3899507522583
Epoch 620, val loss: 0.5369538068771362
Epoch 630, training loss: 839.1098022460938 = 0.5004643201828003 + 100.0 * 8.386093139648438
Epoch 630, val loss: 0.5347810387611389
Epoch 640, training loss: 838.8882446289062 = 0.4977011978626251 + 100.0 * 8.383905410766602
Epoch 640, val loss: 0.5328423976898193
Epoch 650, training loss: 839.1525268554688 = 0.49515897035598755 + 100.0 * 8.386573791503906
Epoch 650, val loss: 0.5309250950813293
Epoch 660, training loss: 839.1676025390625 = 0.4925387501716614 + 100.0 * 8.386750221252441
Epoch 660, val loss: 0.5297341346740723
Epoch 670, training loss: 838.5659790039062 = 0.49021100997924805 + 100.0 * 8.380757331848145
Epoch 670, val loss: 0.5281912684440613
Epoch 680, training loss: 838.4841918945312 = 0.4880810081958771 + 100.0 * 8.379961013793945
Epoch 680, val loss: 0.5267789959907532
Epoch 690, training loss: 838.3789672851562 = 0.486049085855484 + 100.0 * 8.378929138183594
Epoch 690, val loss: 0.525693416595459
Epoch 700, training loss: 838.5648193359375 = 0.4841044843196869 + 100.0 * 8.380806922912598
Epoch 700, val loss: 0.5247244238853455
Epoch 710, training loss: 838.72216796875 = 0.4821242392063141 + 100.0 * 8.382400512695312
Epoch 710, val loss: 0.5233752727508545
Epoch 720, training loss: 838.176025390625 = 0.4802520275115967 + 100.0 * 8.376957893371582
Epoch 720, val loss: 0.5226799249649048
Epoch 730, training loss: 838.0348510742188 = 0.47858360409736633 + 100.0 * 8.37556266784668
Epoch 730, val loss: 0.5216842889785767
Epoch 740, training loss: 837.94091796875 = 0.4769769608974457 + 100.0 * 8.374639511108398
Epoch 740, val loss: 0.5208533406257629
Epoch 750, training loss: 838.47119140625 = 0.4753939211368561 + 100.0 * 8.379958152770996
Epoch 750, val loss: 0.5200625658035278
Epoch 760, training loss: 837.8785400390625 = 0.4737866520881653 + 100.0 * 8.37404727935791
Epoch 760, val loss: 0.5194805264472961
Epoch 770, training loss: 837.6712036132812 = 0.4723028540611267 + 100.0 * 8.371989250183105
Epoch 770, val loss: 0.5187155604362488
Epoch 780, training loss: 838.0038452148438 = 0.47084397077560425 + 100.0 * 8.375329971313477
Epoch 780, val loss: 0.5181905627250671
Epoch 790, training loss: 837.6659545898438 = 0.469417005777359 + 100.0 * 8.371965408325195
Epoch 790, val loss: 0.5174444317817688
Epoch 800, training loss: 837.6448364257812 = 0.46802228689193726 + 100.0 * 8.3717679977417
Epoch 800, val loss: 0.5169491171836853
Epoch 810, training loss: 837.3861083984375 = 0.4666752815246582 + 100.0 * 8.369194030761719
Epoch 810, val loss: 0.5162510871887207
Epoch 820, training loss: 837.3201293945312 = 0.4653811454772949 + 100.0 * 8.368547439575195
Epoch 820, val loss: 0.5156611204147339
Epoch 830, training loss: 837.2237548828125 = 0.46411624550819397 + 100.0 * 8.367596626281738
Epoch 830, val loss: 0.5152429938316345
Epoch 840, training loss: 838.0626831054688 = 0.4628591537475586 + 100.0 * 8.375998497009277
Epoch 840, val loss: 0.5149235129356384
Epoch 850, training loss: 837.3184814453125 = 0.4615556597709656 + 100.0 * 8.368569374084473
Epoch 850, val loss: 0.5140950083732605
Epoch 860, training loss: 837.04443359375 = 0.4603462219238281 + 100.0 * 8.365840911865234
Epoch 860, val loss: 0.5134519934654236
Epoch 870, training loss: 836.9068603515625 = 0.4591931104660034 + 100.0 * 8.364477157592773
Epoch 870, val loss: 0.5131450295448303
Epoch 880, training loss: 836.8411254882812 = 0.45807507634162903 + 100.0 * 8.36383056640625
Epoch 880, val loss: 0.5127182602882385
Epoch 890, training loss: 837.6099853515625 = 0.4569385051727295 + 100.0 * 8.371530532836914
Epoch 890, val loss: 0.5123483538627625
Epoch 900, training loss: 837.443115234375 = 0.4556865692138672 + 100.0 * 8.369874000549316
Epoch 900, val loss: 0.511694073677063
Epoch 910, training loss: 836.8890380859375 = 0.45456117391586304 + 100.0 * 8.364344596862793
Epoch 910, val loss: 0.5110752582550049
Epoch 920, training loss: 836.6647338867188 = 0.45345062017440796 + 100.0 * 8.362112998962402
Epoch 920, val loss: 0.5107783675193787
Epoch 930, training loss: 836.5626220703125 = 0.45239853858947754 + 100.0 * 8.361102104187012
Epoch 930, val loss: 0.5103541016578674
Epoch 940, training loss: 837.0149536132812 = 0.45135772228240967 + 100.0 * 8.365635871887207
Epoch 940, val loss: 0.5097230076789856
Epoch 950, training loss: 836.478271484375 = 0.45022299885749817 + 100.0 * 8.360280990600586
Epoch 950, val loss: 0.509752094745636
Epoch 960, training loss: 836.3536987304688 = 0.4491885006427765 + 100.0 * 8.359045028686523
Epoch 960, val loss: 0.5091171860694885
Epoch 970, training loss: 836.2903442382812 = 0.4481663107872009 + 100.0 * 8.358421325683594
Epoch 970, val loss: 0.5087761282920837
Epoch 980, training loss: 836.3609008789062 = 0.4471699893474579 + 100.0 * 8.359137535095215
Epoch 980, val loss: 0.508385419845581
Epoch 990, training loss: 836.6607666015625 = 0.44612187147140503 + 100.0 * 8.362146377563477
Epoch 990, val loss: 0.5079379081726074
Epoch 1000, training loss: 836.2125244140625 = 0.4450468420982361 + 100.0 * 8.357674598693848
Epoch 1000, val loss: 0.5076472163200378
Epoch 1010, training loss: 836.4469604492188 = 0.44402554631233215 + 100.0 * 8.360029220581055
Epoch 1010, val loss: 0.5072089433670044
Epoch 1020, training loss: 836.1129760742188 = 0.4430238604545593 + 100.0 * 8.356698989868164
Epoch 1020, val loss: 0.5067343711853027
Epoch 1030, training loss: 836.1100463867188 = 0.442053884267807 + 100.0 * 8.356679916381836
Epoch 1030, val loss: 0.5062680244445801
Epoch 1040, training loss: 836.3019409179688 = 0.4410518407821655 + 100.0 * 8.358609199523926
Epoch 1040, val loss: 0.5058945417404175
Epoch 1050, training loss: 836.030029296875 = 0.44003769755363464 + 100.0 * 8.355899810791016
Epoch 1050, val loss: 0.5057412385940552
Epoch 1060, training loss: 835.8695068359375 = 0.4390846788883209 + 100.0 * 8.354304313659668
Epoch 1060, val loss: 0.5052792429924011
Epoch 1070, training loss: 835.8447265625 = 0.4381366968154907 + 100.0 * 8.354065895080566
Epoch 1070, val loss: 0.5050526857376099
Epoch 1080, training loss: 836.3568725585938 = 0.437173992395401 + 100.0 * 8.359196662902832
Epoch 1080, val loss: 0.5048280358314514
Epoch 1090, training loss: 835.9451904296875 = 0.436179518699646 + 100.0 * 8.355090141296387
Epoch 1090, val loss: 0.5041174292564392
Epoch 1100, training loss: 836.1110229492188 = 0.4352138042449951 + 100.0 * 8.356758117675781
Epoch 1100, val loss: 0.5038573741912842
Epoch 1110, training loss: 835.6608276367188 = 0.4342406988143921 + 100.0 * 8.352265357971191
Epoch 1110, val loss: 0.5035103559494019
Epoch 1120, training loss: 835.5427856445312 = 0.4333202540874481 + 100.0 * 8.351094245910645
Epoch 1120, val loss: 0.5030435919761658
Epoch 1130, training loss: 835.5379638671875 = 0.4324134588241577 + 100.0 * 8.351055145263672
Epoch 1130, val loss: 0.502789318561554
Epoch 1140, training loss: 836.0389404296875 = 0.4315243363380432 + 100.0 * 8.356074333190918
Epoch 1140, val loss: 0.5022158026695251
Epoch 1150, training loss: 835.7520141601562 = 0.43049174547195435 + 100.0 * 8.353215217590332
Epoch 1150, val loss: 0.5022072792053223
Epoch 1160, training loss: 835.565185546875 = 0.4295688271522522 + 100.0 * 8.351356506347656
Epoch 1160, val loss: 0.5016448497772217
Epoch 1170, training loss: 835.394287109375 = 0.42864155769348145 + 100.0 * 8.349656105041504
Epoch 1170, val loss: 0.5014377236366272
Epoch 1180, training loss: 835.377197265625 = 0.4277610778808594 + 100.0 * 8.349494934082031
Epoch 1180, val loss: 0.5010645389556885
Epoch 1190, training loss: 835.7434692382812 = 0.4268471300601959 + 100.0 * 8.353166580200195
Epoch 1190, val loss: 0.5008583664894104
Epoch 1200, training loss: 835.47802734375 = 0.42589789628982544 + 100.0 * 8.350521087646484
Epoch 1200, val loss: 0.5004934072494507
Epoch 1210, training loss: 835.3375854492188 = 0.42495840787887573 + 100.0 * 8.349125862121582
Epoch 1210, val loss: 0.5002197623252869
Epoch 1220, training loss: 835.3916015625 = 0.42403870820999146 + 100.0 * 8.349676132202148
Epoch 1220, val loss: 0.4997991621494293
Epoch 1230, training loss: 835.3372192382812 = 0.4231213927268982 + 100.0 * 8.349141120910645
Epoch 1230, val loss: 0.4994296431541443
Epoch 1240, training loss: 835.1510009765625 = 0.4221949875354767 + 100.0 * 8.347288131713867
Epoch 1240, val loss: 0.4990708827972412
Epoch 1250, training loss: 835.1026611328125 = 0.4212842285633087 + 100.0 * 8.346814155578613
Epoch 1250, val loss: 0.4988638758659363
Epoch 1260, training loss: 835.3184204101562 = 0.42038846015930176 + 100.0 * 8.348979949951172
Epoch 1260, val loss: 0.49832409620285034
Epoch 1270, training loss: 835.1930541992188 = 0.4194377064704895 + 100.0 * 8.347736358642578
Epoch 1270, val loss: 0.4981069564819336
Epoch 1280, training loss: 834.965087890625 = 0.4184778928756714 + 100.0 * 8.345466613769531
Epoch 1280, val loss: 0.4978456497192383
Epoch 1290, training loss: 834.9224243164062 = 0.4175570011138916 + 100.0 * 8.345048904418945
Epoch 1290, val loss: 0.49759697914123535
Epoch 1300, training loss: 835.1261596679688 = 0.41667357087135315 + 100.0 * 8.347094535827637
Epoch 1300, val loss: 0.4972355365753174
Epoch 1310, training loss: 834.9539794921875 = 0.41572895646095276 + 100.0 * 8.345382690429688
Epoch 1310, val loss: 0.496893048286438
Epoch 1320, training loss: 834.8474731445312 = 0.4147986173629761 + 100.0 * 8.344326972961426
Epoch 1320, val loss: 0.49647337198257446
Epoch 1330, training loss: 835.1121826171875 = 0.4138810634613037 + 100.0 * 8.346982955932617
Epoch 1330, val loss: 0.49621739983558655
Epoch 1340, training loss: 834.7755126953125 = 0.4129219055175781 + 100.0 * 8.343626022338867
Epoch 1340, val loss: 0.4959143400192261
Epoch 1350, training loss: 834.739501953125 = 0.41199445724487305 + 100.0 * 8.34327507019043
Epoch 1350, val loss: 0.49571454524993896
Epoch 1360, training loss: 834.6915893554688 = 0.41108325123786926 + 100.0 * 8.342804908752441
Epoch 1360, val loss: 0.4953616261482239
Epoch 1370, training loss: 835.0945434570312 = 0.4101705551147461 + 100.0 * 8.346843719482422
Epoch 1370, val loss: 0.4954002797603607
Epoch 1380, training loss: 834.6812744140625 = 0.40920746326446533 + 100.0 * 8.342720985412598
Epoch 1380, val loss: 0.49461525678634644
Epoch 1390, training loss: 834.59912109375 = 0.40826258063316345 + 100.0 * 8.34190845489502
Epoch 1390, val loss: 0.4944061040878296
Epoch 1400, training loss: 834.6624755859375 = 0.40733519196510315 + 100.0 * 8.342551231384277
Epoch 1400, val loss: 0.4940180778503418
Epoch 1410, training loss: 834.6873168945312 = 0.40639546513557434 + 100.0 * 8.342809677124023
Epoch 1410, val loss: 0.4937959611415863
Epoch 1420, training loss: 834.6297607421875 = 0.4054432809352875 + 100.0 * 8.342243194580078
Epoch 1420, val loss: 0.4934985041618347
Epoch 1430, training loss: 834.6526489257812 = 0.4044923186302185 + 100.0 * 8.34248161315918
Epoch 1430, val loss: 0.49316614866256714
Epoch 1440, training loss: 834.5509033203125 = 0.40353479981422424 + 100.0 * 8.341473579406738
Epoch 1440, val loss: 0.49279627203941345
Epoch 1450, training loss: 834.6658325195312 = 0.40257707238197327 + 100.0 * 8.342632293701172
Epoch 1450, val loss: 0.49241071939468384
Epoch 1460, training loss: 834.49267578125 = 0.4015905559062958 + 100.0 * 8.340910911560059
Epoch 1460, val loss: 0.49209916591644287
Epoch 1470, training loss: 834.4248046875 = 0.40062111616134644 + 100.0 * 8.340241432189941
Epoch 1470, val loss: 0.4917779862880707
Epoch 1480, training loss: 834.4052124023438 = 0.39965471625328064 + 100.0 * 8.340055465698242
Epoch 1480, val loss: 0.4916212558746338
Epoch 1490, training loss: 834.8682861328125 = 0.39868009090423584 + 100.0 * 8.344696044921875
Epoch 1490, val loss: 0.4914683401584625
Epoch 1500, training loss: 834.5916137695312 = 0.39767223596572876 + 100.0 * 8.341939926147461
Epoch 1500, val loss: 0.49068889021873474
Epoch 1510, training loss: 834.2355346679688 = 0.3966478407382965 + 100.0 * 8.338388442993164
Epoch 1510, val loss: 0.490621417760849
Epoch 1520, training loss: 834.2175903320312 = 0.39567211270332336 + 100.0 * 8.338218688964844
Epoch 1520, val loss: 0.49028706550598145
Epoch 1530, training loss: 834.2249755859375 = 0.39469820261001587 + 100.0 * 8.338302612304688
Epoch 1530, val loss: 0.48988106846809387
Epoch 1540, training loss: 834.6076049804688 = 0.39369481801986694 + 100.0 * 8.34213924407959
Epoch 1540, val loss: 0.4895932078361511
Epoch 1550, training loss: 834.2056274414062 = 0.39266523718833923 + 100.0 * 8.338129997253418
Epoch 1550, val loss: 0.4894994795322418
Epoch 1560, training loss: 834.4382934570312 = 0.39163291454315186 + 100.0 * 8.340466499328613
Epoch 1560, val loss: 0.489162802696228
Epoch 1570, training loss: 834.0570068359375 = 0.39058634638786316 + 100.0 * 8.336664199829102
Epoch 1570, val loss: 0.4886062741279602
Epoch 1580, training loss: 834.037109375 = 0.38956233859062195 + 100.0 * 8.336475372314453
Epoch 1580, val loss: 0.488378643989563
Epoch 1590, training loss: 834.0724487304688 = 0.38855642080307007 + 100.0 * 8.336838722229004
Epoch 1590, val loss: 0.48803430795669556
Epoch 1600, training loss: 834.71435546875 = 0.38751277327537537 + 100.0 * 8.343268394470215
Epoch 1600, val loss: 0.4877384305000305
Epoch 1610, training loss: 834.2398681640625 = 0.3864327073097229 + 100.0 * 8.338534355163574
Epoch 1610, val loss: 0.48753881454467773
Epoch 1620, training loss: 833.9801635742188 = 0.38536664843559265 + 100.0 * 8.33594799041748
Epoch 1620, val loss: 0.4871360659599304
Epoch 1630, training loss: 833.8956909179688 = 0.38433635234832764 + 100.0 * 8.335113525390625
Epoch 1630, val loss: 0.486800879240036
Epoch 1640, training loss: 833.9527587890625 = 0.3833004832267761 + 100.0 * 8.335694313049316
Epoch 1640, val loss: 0.48661285638809204
Epoch 1650, training loss: 834.6610717773438 = 0.382226824760437 + 100.0 * 8.342788696289062
Epoch 1650, val loss: 0.4865174889564514
Epoch 1660, training loss: 834.1270751953125 = 0.3810926377773285 + 100.0 * 8.337459564208984
Epoch 1660, val loss: 0.4855247437953949
Epoch 1670, training loss: 833.8992919921875 = 0.3799859881401062 + 100.0 * 8.335192680358887
Epoch 1670, val loss: 0.485411137342453
Epoch 1680, training loss: 833.9825439453125 = 0.37890857458114624 + 100.0 * 8.336036682128906
Epoch 1680, val loss: 0.4849778413772583
Epoch 1690, training loss: 833.8235473632812 = 0.3778035342693329 + 100.0 * 8.334457397460938
Epoch 1690, val loss: 0.48470479249954224
Epoch 1700, training loss: 833.8176879882812 = 0.3766992390155792 + 100.0 * 8.334409713745117
Epoch 1700, val loss: 0.48446887731552124
Epoch 1710, training loss: 834.081787109375 = 0.37559008598327637 + 100.0 * 8.337061882019043
Epoch 1710, val loss: 0.4842115640640259
Epoch 1720, training loss: 833.8749389648438 = 0.37444132566452026 + 100.0 * 8.335004806518555
Epoch 1720, val loss: 0.48356544971466064
Epoch 1730, training loss: 833.8798828125 = 0.3732987940311432 + 100.0 * 8.335065841674805
Epoch 1730, val loss: 0.4833385646343231
Epoch 1740, training loss: 833.654052734375 = 0.3721455931663513 + 100.0 * 8.332818984985352
Epoch 1740, val loss: 0.48288601636886597
Epoch 1750, training loss: 833.7352294921875 = 0.3710087537765503 + 100.0 * 8.33364200592041
Epoch 1750, val loss: 0.4825190305709839
Epoch 1760, training loss: 833.8072509765625 = 0.3698550760746002 + 100.0 * 8.334373474121094
Epoch 1760, val loss: 0.4821817874908447
Epoch 1770, training loss: 833.6828002929688 = 0.3686820864677429 + 100.0 * 8.333141326904297
Epoch 1770, val loss: 0.4819028377532959
Epoch 1780, training loss: 833.8386840820312 = 0.3675062954425812 + 100.0 * 8.334712028503418
Epoch 1780, val loss: 0.4815770983695984
Epoch 1790, training loss: 833.735107421875 = 0.36632096767425537 + 100.0 * 8.333687782287598
Epoch 1790, val loss: 0.4810444414615631
Epoch 1800, training loss: 833.6531982421875 = 0.36512428522109985 + 100.0 * 8.332880973815918
Epoch 1800, val loss: 0.4807177484035492
Epoch 1810, training loss: 833.5805053710938 = 0.36393657326698303 + 100.0 * 8.332165718078613
Epoch 1810, val loss: 0.4804341793060303
Epoch 1820, training loss: 833.6801147460938 = 0.36274096369743347 + 100.0 * 8.333173751831055
Epoch 1820, val loss: 0.4798981547355652
Epoch 1830, training loss: 833.5873413085938 = 0.36151134967803955 + 100.0 * 8.332258224487305
Epoch 1830, val loss: 0.479596346616745
Epoch 1840, training loss: 833.7780151367188 = 0.3602660000324249 + 100.0 * 8.334177017211914
Epoch 1840, val loss: 0.4791449010372162
Epoch 1850, training loss: 833.490966796875 = 0.3590264618396759 + 100.0 * 8.331319808959961
Epoch 1850, val loss: 0.47914326190948486
Epoch 1860, training loss: 833.4016723632812 = 0.35779547691345215 + 100.0 * 8.330438613891602
Epoch 1860, val loss: 0.47885146737098694
Epoch 1870, training loss: 833.4857788085938 = 0.35657623410224915 + 100.0 * 8.331292152404785
Epoch 1870, val loss: 0.4784712493419647
Epoch 1880, training loss: 833.6781616210938 = 0.35532763600349426 + 100.0 * 8.33322811126709
Epoch 1880, val loss: 0.4783594310283661
Epoch 1890, training loss: 833.8453979492188 = 0.35401633381843567 + 100.0 * 8.334914207458496
Epoch 1890, val loss: 0.47797101736068726
Epoch 1900, training loss: 833.3704223632812 = 0.35270801186561584 + 100.0 * 8.330177307128906
Epoch 1900, val loss: 0.47755298018455505
Epoch 1910, training loss: 833.26025390625 = 0.35143670439720154 + 100.0 * 8.32908821105957
Epoch 1910, val loss: 0.47716495394706726
Epoch 1920, training loss: 833.234375 = 0.3501882553100586 + 100.0 * 8.328842163085938
Epoch 1920, val loss: 0.47687065601348877
Epoch 1930, training loss: 833.480224609375 = 0.34893640875816345 + 100.0 * 8.331313133239746
Epoch 1930, val loss: 0.4763684570789337
Epoch 1940, training loss: 833.4454345703125 = 0.34760820865631104 + 100.0 * 8.330978393554688
Epoch 1940, val loss: 0.4761454463005066
Epoch 1950, training loss: 833.5062866210938 = 0.34626662731170654 + 100.0 * 8.331600189208984
Epoch 1950, val loss: 0.4759821593761444
Epoch 1960, training loss: 833.1942749023438 = 0.34494441747665405 + 100.0 * 8.328493118286133
Epoch 1960, val loss: 0.475852906703949
Epoch 1970, training loss: 833.1573486328125 = 0.34364616870880127 + 100.0 * 8.328137397766113
Epoch 1970, val loss: 0.4757334291934967
Epoch 1980, training loss: 833.0838012695312 = 0.3423478901386261 + 100.0 * 8.327414512634277
Epoch 1980, val loss: 0.4752972722053528
Epoch 1990, training loss: 833.0721435546875 = 0.3410368859767914 + 100.0 * 8.327310562133789
Epoch 1990, val loss: 0.4751158356666565
Epoch 2000, training loss: 833.8067016601562 = 0.3397044837474823 + 100.0 * 8.334670066833496
Epoch 2000, val loss: 0.4748263359069824
Epoch 2010, training loss: 833.5289306640625 = 0.33830028772354126 + 100.0 * 8.33190631866455
Epoch 2010, val loss: 0.47440648078918457
Epoch 2020, training loss: 833.2616577148438 = 0.3368867337703705 + 100.0 * 8.32924747467041
Epoch 2020, val loss: 0.4742194414138794
Epoch 2030, training loss: 833.1234130859375 = 0.33551234006881714 + 100.0 * 8.327878952026367
Epoch 2030, val loss: 0.47403064370155334
Epoch 2040, training loss: 833.3292846679688 = 0.3341580331325531 + 100.0 * 8.329951286315918
Epoch 2040, val loss: 0.47399359941482544
Epoch 2050, training loss: 833.0208740234375 = 0.33277279138565063 + 100.0 * 8.326881408691406
Epoch 2050, val loss: 0.4737791419029236
Epoch 2060, training loss: 833.0537719726562 = 0.3314038813114166 + 100.0 * 8.327223777770996
Epoch 2060, val loss: 0.4736191928386688
Epoch 2070, training loss: 833.04736328125 = 0.3300286829471588 + 100.0 * 8.327173233032227
Epoch 2070, val loss: 0.4735041558742523
Epoch 2080, training loss: 833.2257690429688 = 0.32864537835121155 + 100.0 * 8.328970909118652
Epoch 2080, val loss: 0.4733124375343323
Epoch 2090, training loss: 832.95947265625 = 0.3272278904914856 + 100.0 * 8.326322555541992
Epoch 2090, val loss: 0.47268450260162354
Epoch 2100, training loss: 833.1561279296875 = 0.3258376717567444 + 100.0 * 8.328302383422852
Epoch 2100, val loss: 0.47229817509651184
Epoch 2110, training loss: 833.0631103515625 = 0.324421763420105 + 100.0 * 8.327386856079102
Epoch 2110, val loss: 0.47241997718811035
Epoch 2120, training loss: 832.9887084960938 = 0.32302138209342957 + 100.0 * 8.32665729522705
Epoch 2120, val loss: 0.4724324345588684
Epoch 2130, training loss: 832.9876098632812 = 0.3215947151184082 + 100.0 * 8.32666015625
Epoch 2130, val loss: 0.47219038009643555
Epoch 2140, training loss: 832.9563598632812 = 0.3201698362827301 + 100.0 * 8.326361656188965
Epoch 2140, val loss: 0.4719589352607727
Epoch 2150, training loss: 833.1288452148438 = 0.3187479078769684 + 100.0 * 8.32810115814209
Epoch 2150, val loss: 0.4718644917011261
Epoch 2160, training loss: 832.84619140625 = 0.3173009157180786 + 100.0 * 8.325288772583008
Epoch 2160, val loss: 0.4717290699481964
Epoch 2170, training loss: 832.8640747070312 = 0.3158629536628723 + 100.0 * 8.325482368469238
Epoch 2170, val loss: 0.4713221788406372
Epoch 2180, training loss: 832.9652099609375 = 0.31442609429359436 + 100.0 * 8.326507568359375
Epoch 2180, val loss: 0.4712039530277252
Epoch 2190, training loss: 832.9076538085938 = 0.3129650056362152 + 100.0 * 8.325946807861328
Epoch 2190, val loss: 0.4715406596660614
Epoch 2200, training loss: 832.7553100585938 = 0.31150445342063904 + 100.0 * 8.324438095092773
Epoch 2200, val loss: 0.47139525413513184
Epoch 2210, training loss: 833.019287109375 = 0.31005048751831055 + 100.0 * 8.327092170715332
Epoch 2210, val loss: 0.4712681770324707
Epoch 2220, training loss: 832.7457275390625 = 0.30857014656066895 + 100.0 * 8.324371337890625
Epoch 2220, val loss: 0.4713892936706543
Epoch 2230, training loss: 832.6873779296875 = 0.30709531903266907 + 100.0 * 8.323802947998047
Epoch 2230, val loss: 0.47123950719833374
Epoch 2240, training loss: 832.6826782226562 = 0.30562490224838257 + 100.0 * 8.323770523071289
Epoch 2240, val loss: 0.47104761004447937
Epoch 2250, training loss: 833.0831909179688 = 0.3041645884513855 + 100.0 * 8.327790260314941
Epoch 2250, val loss: 0.4707750976085663
Epoch 2260, training loss: 832.91455078125 = 0.30262476205825806 + 100.0 * 8.326119422912598
Epoch 2260, val loss: 0.4712965190410614
Epoch 2270, training loss: 832.6644897460938 = 0.30109670758247375 + 100.0 * 8.323634147644043
Epoch 2270, val loss: 0.4711818993091583
Epoch 2280, training loss: 832.5606079101562 = 0.29960477352142334 + 100.0 * 8.322609901428223
Epoch 2280, val loss: 0.4715483486652374
Epoch 2290, training loss: 832.5933227539062 = 0.29812294244766235 + 100.0 * 8.322952270507812
Epoch 2290, val loss: 0.471520334482193
Epoch 2300, training loss: 833.4550170898438 = 0.2966488301753998 + 100.0 * 8.331583976745605
Epoch 2300, val loss: 0.47124308347702026
Epoch 2310, training loss: 832.7820434570312 = 0.295091837644577 + 100.0 * 8.324869155883789
Epoch 2310, val loss: 0.471615731716156
Epoch 2320, training loss: 832.5739135742188 = 0.29358530044555664 + 100.0 * 8.322803497314453
Epoch 2320, val loss: 0.47180113196372986
Epoch 2330, training loss: 832.4854736328125 = 0.29209572076797485 + 100.0 * 8.32193374633789
Epoch 2330, val loss: 0.4720046818256378
Epoch 2340, training loss: 832.501708984375 = 0.2906097173690796 + 100.0 * 8.322111129760742
Epoch 2340, val loss: 0.4719851613044739
Epoch 2350, training loss: 832.921875 = 0.2891128361225128 + 100.0 * 8.326327323913574
Epoch 2350, val loss: 0.47243890166282654
Epoch 2360, training loss: 832.6353759765625 = 0.2875787913799286 + 100.0 * 8.323477745056152
Epoch 2360, val loss: 0.4726274609565735
Epoch 2370, training loss: 832.4422607421875 = 0.28603410720825195 + 100.0 * 8.321562767028809
Epoch 2370, val loss: 0.4722970724105835
Epoch 2380, training loss: 832.7325439453125 = 0.28453269600868225 + 100.0 * 8.324480056762695
Epoch 2380, val loss: 0.4722627103328705
Epoch 2390, training loss: 832.4608764648438 = 0.2829996645450592 + 100.0 * 8.321778297424316
Epoch 2390, val loss: 0.4730808436870575
Epoch 2400, training loss: 832.3802490234375 = 0.28149184584617615 + 100.0 * 8.320987701416016
Epoch 2400, val loss: 0.4732261896133423
Epoch 2410, training loss: 832.3433837890625 = 0.27998456358909607 + 100.0 * 8.320633888244629
Epoch 2410, val loss: 0.4732378125190735
Epoch 2420, training loss: 832.3795776367188 = 0.27848896384239197 + 100.0 * 8.32101058959961
Epoch 2420, val loss: 0.47354164719581604
Epoch 2430, training loss: 832.9599609375 = 0.27699005603790283 + 100.0 * 8.32682991027832
Epoch 2430, val loss: 0.47407054901123047
Epoch 2440, training loss: 832.5579833984375 = 0.27544689178466797 + 100.0 * 8.32282543182373
Epoch 2440, val loss: 0.4742232859134674
Epoch 2450, training loss: 832.4119873046875 = 0.27390584349632263 + 100.0 * 8.321380615234375
Epoch 2450, val loss: 0.47426488995552063
Epoch 2460, training loss: 832.30078125 = 0.27238941192626953 + 100.0 * 8.320283889770508
Epoch 2460, val loss: 0.4745709002017975
Epoch 2470, training loss: 832.5078125 = 0.27089923620224 + 100.0 * 8.322369575500488
Epoch 2470, val loss: 0.4752673804759979
Epoch 2480, training loss: 832.362548828125 = 0.26938101649284363 + 100.0 * 8.320931434631348
Epoch 2480, val loss: 0.4755095839500427
Epoch 2490, training loss: 832.4396362304688 = 0.26785245537757874 + 100.0 * 8.321718215942383
Epoch 2490, val loss: 0.47553586959838867
Epoch 2500, training loss: 832.3345336914062 = 0.2663327753543854 + 100.0 * 8.32068157196045
Epoch 2500, val loss: 0.47571030259132385
Epoch 2510, training loss: 832.2127075195312 = 0.2648175358772278 + 100.0 * 8.319478988647461
Epoch 2510, val loss: 0.47595345973968506
Epoch 2520, training loss: 832.2802734375 = 0.26331618428230286 + 100.0 * 8.320169448852539
Epoch 2520, val loss: 0.47650206089019775
Epoch 2530, training loss: 832.7157592773438 = 0.2618301212787628 + 100.0 * 8.324539184570312
Epoch 2530, val loss: 0.4773045480251312
Epoch 2540, training loss: 832.3427124023438 = 0.26026451587677 + 100.0 * 8.32082462310791
Epoch 2540, val loss: 0.47695809602737427
Epoch 2550, training loss: 832.1685791015625 = 0.25874418020248413 + 100.0 * 8.319098472595215
Epoch 2550, val loss: 0.47749757766723633
Epoch 2560, training loss: 832.1329956054688 = 0.2572482228279114 + 100.0 * 8.318757057189941
Epoch 2560, val loss: 0.4778905510902405
Epoch 2570, training loss: 832.5838012695312 = 0.25577470660209656 + 100.0 * 8.323280334472656
Epoch 2570, val loss: 0.4788554608821869
Epoch 2580, training loss: 832.3494262695312 = 0.25424662232398987 + 100.0 * 8.320951461791992
Epoch 2580, val loss: 0.4791122376918793
Epoch 2590, training loss: 832.1796875 = 0.25271695852279663 + 100.0 * 8.319269180297852
Epoch 2590, val loss: 0.4791991710662842
Epoch 2600, training loss: 832.061767578125 = 0.2512177526950836 + 100.0 * 8.318105697631836
Epoch 2600, val loss: 0.479458749294281
Epoch 2610, training loss: 832.0210571289062 = 0.24974365532398224 + 100.0 * 8.317712783813477
Epoch 2610, val loss: 0.4800666868686676
Epoch 2620, training loss: 832.14306640625 = 0.24827998876571655 + 100.0 * 8.318947792053223
Epoch 2620, val loss: 0.4804062247276306
Epoch 2630, training loss: 832.313720703125 = 0.24680720269680023 + 100.0 * 8.320669174194336
Epoch 2630, val loss: 0.4810185134410858
Epoch 2640, training loss: 832.3500366210938 = 0.245345801115036 + 100.0 * 8.321046829223633
Epoch 2640, val loss: 0.4819895923137665
Epoch 2650, training loss: 832.0243530273438 = 0.24383977055549622 + 100.0 * 8.317805290222168
Epoch 2650, val loss: 0.4818774461746216
Epoch 2660, training loss: 831.9833984375 = 0.2423851191997528 + 100.0 * 8.317410469055176
Epoch 2660, val loss: 0.48217397928237915
Epoch 2670, training loss: 832.3452758789062 = 0.24096597731113434 + 100.0 * 8.321043014526367
Epoch 2670, val loss: 0.48203131556510925
Epoch 2680, training loss: 832.035400390625 = 0.2394729107618332 + 100.0 * 8.31795883178711
Epoch 2680, val loss: 0.48343557119369507
Epoch 2690, training loss: 831.9826049804688 = 0.238029345870018 + 100.0 * 8.317445755004883
Epoch 2690, val loss: 0.4841381907463074
Epoch 2700, training loss: 831.9749755859375 = 0.2366010993719101 + 100.0 * 8.317383766174316
Epoch 2700, val loss: 0.48421475291252136
Epoch 2710, training loss: 832.0789184570312 = 0.2351795732975006 + 100.0 * 8.318437576293945
Epoch 2710, val loss: 0.48515036702156067
Epoch 2720, training loss: 831.9798583984375 = 0.23375563323497772 + 100.0 * 8.317461013793945
Epoch 2720, val loss: 0.4854387640953064
Epoch 2730, training loss: 831.9937133789062 = 0.2323334664106369 + 100.0 * 8.31761360168457
Epoch 2730, val loss: 0.48628559708595276
Epoch 2740, training loss: 832.1495971679688 = 0.23092053830623627 + 100.0 * 8.31918716430664
Epoch 2740, val loss: 0.48710545897483826
Epoch 2750, training loss: 832.1694946289062 = 0.229487806558609 + 100.0 * 8.3193998336792
Epoch 2750, val loss: 0.48763471841812134
Epoch 2760, training loss: 831.9614868164062 = 0.22806209325790405 + 100.0 * 8.317334175109863
Epoch 2760, val loss: 0.48807767033576965
Epoch 2770, training loss: 831.8576049804688 = 0.2266591340303421 + 100.0 * 8.316308975219727
Epoch 2770, val loss: 0.4888385832309723
Epoch 2780, training loss: 831.9251098632812 = 0.2252681851387024 + 100.0 * 8.316998481750488
Epoch 2780, val loss: 0.48924192786216736
Epoch 2790, training loss: 831.92626953125 = 0.223868265748024 + 100.0 * 8.317024230957031
Epoch 2790, val loss: 0.49002957344055176
Epoch 2800, training loss: 831.8723754882812 = 0.22248095273971558 + 100.0 * 8.316498756408691
Epoch 2800, val loss: 0.49107441306114197
Epoch 2810, training loss: 832.3190307617188 = 0.22110223770141602 + 100.0 * 8.320979118347168
Epoch 2810, val loss: 0.4918923079967499
Epoch 2820, training loss: 831.9092407226562 = 0.21968187391757965 + 100.0 * 8.316895484924316
Epoch 2820, val loss: 0.4915240406990051
Epoch 2830, training loss: 831.8441772460938 = 0.21827563643455505 + 100.0 * 8.316259384155273
Epoch 2830, val loss: 0.4923938512802124
Epoch 2840, training loss: 831.730224609375 = 0.21689189970493317 + 100.0 * 8.315133094787598
Epoch 2840, val loss: 0.49321094155311584
Epoch 2850, training loss: 831.7655639648438 = 0.21552187204360962 + 100.0 * 8.315500259399414
Epoch 2850, val loss: 0.4937749207019806
Epoch 2860, training loss: 832.283447265625 = 0.21415774524211884 + 100.0 * 8.320693016052246
Epoch 2860, val loss: 0.494385689496994
Epoch 2870, training loss: 831.9287109375 = 0.21278057992458344 + 100.0 * 8.317159652709961
Epoch 2870, val loss: 0.4960424304008484
Epoch 2880, training loss: 831.9728393554688 = 0.21139290928840637 + 100.0 * 8.317614555358887
Epoch 2880, val loss: 0.49634671211242676
Epoch 2890, training loss: 831.7889404296875 = 0.2100149244070053 + 100.0 * 8.315789222717285
Epoch 2890, val loss: 0.49671289324760437
Epoch 2900, training loss: 831.879150390625 = 0.2086590975522995 + 100.0 * 8.316704750061035
Epoch 2900, val loss: 0.49707159399986267
Epoch 2910, training loss: 831.6493530273438 = 0.20728906989097595 + 100.0 * 8.314420700073242
Epoch 2910, val loss: 0.49835699796676636
Epoch 2920, training loss: 831.9340209960938 = 0.2059430330991745 + 100.0 * 8.317280769348145
Epoch 2920, val loss: 0.4987689256668091
Epoch 2930, training loss: 831.940673828125 = 0.20460709929466248 + 100.0 * 8.317360877990723
Epoch 2930, val loss: 0.4989112913608551
Epoch 2940, training loss: 831.634033203125 = 0.20322208106517792 + 100.0 * 8.314308166503906
Epoch 2940, val loss: 0.5006057024002075
Epoch 2950, training loss: 831.5689086914062 = 0.20188823342323303 + 100.0 * 8.31367015838623
Epoch 2950, val loss: 0.5014764070510864
Epoch 2960, training loss: 831.58349609375 = 0.20055201649665833 + 100.0 * 8.31382942199707
Epoch 2960, val loss: 0.5020326375961304
Epoch 2970, training loss: 831.85302734375 = 0.1992373764514923 + 100.0 * 8.316537857055664
Epoch 2970, val loss: 0.503054678440094
Epoch 2980, training loss: 831.7947998046875 = 0.19788981974124908 + 100.0 * 8.315969467163086
Epoch 2980, val loss: 0.5034134387969971
Epoch 2990, training loss: 831.6596069335938 = 0.1965426802635193 + 100.0 * 8.314630508422852
Epoch 2990, val loss: 0.5038523077964783
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8249619482496194
0.843439831920597
The final CL Acc:0.81955, 0.00417, The final GNN Acc:0.84329, 0.00089
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 110896])
remove edge: torch.Size([2, 66516])
updated graph: torch.Size([2, 88764])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 1059.3271484375 = 1.10389244556427 + 100.0 * 10.582232475280762
Epoch 0, val loss: 1.1041693687438965
Epoch 10, training loss: 1059.1846923828125 = 1.0969810485839844 + 100.0 * 10.580877304077148
Epoch 10, val loss: 1.0971726179122925
Epoch 20, training loss: 1058.0213623046875 = 1.0888289213180542 + 100.0 * 10.569324493408203
Epoch 20, val loss: 1.088915467262268
Epoch 30, training loss: 1050.2371826171875 = 1.0788569450378418 + 100.0 * 10.491583824157715
Epoch 30, val loss: 1.0787302255630493
Epoch 40, training loss: 1020.8609619140625 = 1.067568063735962 + 100.0 * 10.1979341506958
Epoch 40, val loss: 1.0674227476119995
Epoch 50, training loss: 962.4805297851562 = 1.0562776327133179 + 100.0 * 9.614242553710938
Epoch 50, val loss: 1.0562092065811157
Epoch 60, training loss: 936.4605102539062 = 1.0460169315338135 + 100.0 * 9.354145050048828
Epoch 60, val loss: 1.0459085702896118
Epoch 70, training loss: 924.6776123046875 = 1.0345890522003174 + 100.0 * 9.236430168151855
Epoch 70, val loss: 1.03471040725708
Epoch 80, training loss: 920.7052612304688 = 1.0239228010177612 + 100.0 * 9.196813583374023
Epoch 80, val loss: 1.0244567394256592
Epoch 90, training loss: 917.5068969726562 = 1.014892578125 + 100.0 * 9.16491985321045
Epoch 90, val loss: 1.015859842300415
Epoch 100, training loss: 912.9843139648438 = 1.0070191621780396 + 100.0 * 9.119772911071777
Epoch 100, val loss: 1.00844144821167
Epoch 110, training loss: 905.8763427734375 = 0.9997633099555969 + 100.0 * 9.048766136169434
Epoch 110, val loss: 1.00168776512146
Epoch 120, training loss: 895.6998291015625 = 0.9929457902908325 + 100.0 * 8.94706916809082
Epoch 120, val loss: 0.995428740978241
Epoch 130, training loss: 887.5577392578125 = 0.9866132140159607 + 100.0 * 8.865711212158203
Epoch 130, val loss: 0.989610493183136
Epoch 140, training loss: 880.3855590820312 = 0.9798161387443542 + 100.0 * 8.794057846069336
Epoch 140, val loss: 0.9832739233970642
Epoch 150, training loss: 874.3602294921875 = 0.9724324345588684 + 100.0 * 8.733878135681152
Epoch 150, val loss: 0.9763178825378418
Epoch 160, training loss: 871.0291748046875 = 0.9633975625038147 + 100.0 * 8.700657844543457
Epoch 160, val loss: 0.9675320386886597
Epoch 170, training loss: 867.70458984375 = 0.9525090456008911 + 100.0 * 8.667520523071289
Epoch 170, val loss: 0.957252025604248
Epoch 180, training loss: 864.6360473632812 = 0.9414805173873901 + 100.0 * 8.636945724487305
Epoch 180, val loss: 0.9469615817070007
Epoch 190, training loss: 862.8850708007812 = 0.9302883744239807 + 100.0 * 8.619547843933105
Epoch 190, val loss: 0.9366406798362732
Epoch 200, training loss: 860.5173950195312 = 0.9175456166267395 + 100.0 * 8.595998764038086
Epoch 200, val loss: 0.924572765827179
Epoch 210, training loss: 858.9241943359375 = 0.9030455946922302 + 100.0 * 8.580211639404297
Epoch 210, val loss: 0.9109281301498413
Epoch 220, training loss: 857.4058227539062 = 0.8874252438545227 + 100.0 * 8.565183639526367
Epoch 220, val loss: 0.8962796330451965
Epoch 230, training loss: 856.0695190429688 = 0.8709842562675476 + 100.0 * 8.551985740661621
Epoch 230, val loss: 0.8808705806732178
Epoch 240, training loss: 854.9969482421875 = 0.8537113070487976 + 100.0 * 8.54143238067627
Epoch 240, val loss: 0.8646568655967712
Epoch 250, training loss: 853.669921875 = 0.8358133435249329 + 100.0 * 8.528341293334961
Epoch 250, val loss: 0.8480004072189331
Epoch 260, training loss: 852.5450439453125 = 0.8175391554832458 + 100.0 * 8.517274856567383
Epoch 260, val loss: 0.830890417098999
Epoch 270, training loss: 851.5422973632812 = 0.798818051815033 + 100.0 * 8.507434844970703
Epoch 270, val loss: 0.8134036064147949
Epoch 280, training loss: 850.5625610351562 = 0.7797844409942627 + 100.0 * 8.497827529907227
Epoch 280, val loss: 0.7956868410110474
Epoch 290, training loss: 849.6475830078125 = 0.7608171701431274 + 100.0 * 8.48886775970459
Epoch 290, val loss: 0.7780731916427612
Epoch 300, training loss: 848.966796875 = 0.7419123649597168 + 100.0 * 8.48224925994873
Epoch 300, val loss: 0.7605533599853516
Epoch 310, training loss: 848.117431640625 = 0.7231143116950989 + 100.0 * 8.473942756652832
Epoch 310, val loss: 0.7431907653808594
Epoch 320, training loss: 847.333740234375 = 0.7047173976898193 + 100.0 * 8.466290473937988
Epoch 320, val loss: 0.7262104749679565
Epoch 330, training loss: 846.8671264648438 = 0.6867595911026001 + 100.0 * 8.461803436279297
Epoch 330, val loss: 0.7096712589263916
Epoch 340, training loss: 847.2392578125 = 0.6691290736198425 + 100.0 * 8.46570110321045
Epoch 340, val loss: 0.6936271786689758
Epoch 350, training loss: 845.966064453125 = 0.6520259380340576 + 100.0 * 8.453140258789062
Epoch 350, val loss: 0.6779294013977051
Epoch 360, training loss: 845.3428344726562 = 0.6357606053352356 + 100.0 * 8.447071075439453
Epoch 360, val loss: 0.6631265878677368
Epoch 370, training loss: 844.8873291015625 = 0.6203713417053223 + 100.0 * 8.442669868469238
Epoch 370, val loss: 0.6491554379463196
Epoch 380, training loss: 844.4821166992188 = 0.6057652831077576 + 100.0 * 8.438763618469238
Epoch 380, val loss: 0.6359711289405823
Epoch 390, training loss: 844.1270751953125 = 0.591939389705658 + 100.0 * 8.435351371765137
Epoch 390, val loss: 0.6235590577125549
Epoch 400, training loss: 843.8099975585938 = 0.5789040327072144 + 100.0 * 8.432311058044434
Epoch 400, val loss: 0.611918032169342
Epoch 410, training loss: 843.6408081054688 = 0.5665906667709351 + 100.0 * 8.430742263793945
Epoch 410, val loss: 0.6009047627449036
Epoch 420, training loss: 843.2816772460938 = 0.5549911856651306 + 100.0 * 8.427267074584961
Epoch 420, val loss: 0.5906680226325989
Epoch 430, training loss: 843.0 = 0.544260561466217 + 100.0 * 8.42455768585205
Epoch 430, val loss: 0.5812994837760925
Epoch 440, training loss: 842.8065795898438 = 0.5343809127807617 + 100.0 * 8.422721862792969
Epoch 440, val loss: 0.5727300643920898
Epoch 450, training loss: 842.5073852539062 = 0.5250319242477417 + 100.0 * 8.41982364654541
Epoch 450, val loss: 0.5646703839302063
Epoch 460, training loss: 842.2105102539062 = 0.5163701176643372 + 100.0 * 8.41694164276123
Epoch 460, val loss: 0.5573200583457947
Epoch 470, training loss: 841.9807739257812 = 0.5084208250045776 + 100.0 * 8.41472339630127
Epoch 470, val loss: 0.5506576299667358
Epoch 480, training loss: 841.7151489257812 = 0.5010387301445007 + 100.0 * 8.412140846252441
Epoch 480, val loss: 0.5445176959037781
Epoch 490, training loss: 841.9109497070312 = 0.4941464066505432 + 100.0 * 8.414168357849121
Epoch 490, val loss: 0.5388547778129578
Epoch 500, training loss: 841.391357421875 = 0.48763296008110046 + 100.0 * 8.409037590026855
Epoch 500, val loss: 0.5336063504219055
Epoch 510, training loss: 841.4266967773438 = 0.4816417992115021 + 100.0 * 8.40945053100586
Epoch 510, val loss: 0.5288344621658325
Epoch 520, training loss: 840.997802734375 = 0.4759995937347412 + 100.0 * 8.405218124389648
Epoch 520, val loss: 0.5244033932685852
Epoch 530, training loss: 840.7329711914062 = 0.470829039812088 + 100.0 * 8.402621269226074
Epoch 530, val loss: 0.5204437971115112
Epoch 540, training loss: 840.6094970703125 = 0.46603113412857056 + 100.0 * 8.401434898376465
Epoch 540, val loss: 0.5168915390968323
Epoch 550, training loss: 840.763671875 = 0.46148356795310974 + 100.0 * 8.403021812438965
Epoch 550, val loss: 0.5134968161582947
Epoch 560, training loss: 840.3515625 = 0.4571194350719452 + 100.0 * 8.398944854736328
Epoch 560, val loss: 0.5103505253791809
Epoch 570, training loss: 840.05810546875 = 0.45313772559165955 + 100.0 * 8.396049499511719
Epoch 570, val loss: 0.5074924230575562
Epoch 580, training loss: 839.8727416992188 = 0.4494282007217407 + 100.0 * 8.394233703613281
Epoch 580, val loss: 0.5049300193786621
Epoch 590, training loss: 840.2034912109375 = 0.445922315120697 + 100.0 * 8.397575378417969
Epoch 590, val loss: 0.5024416446685791
Epoch 600, training loss: 839.8378295898438 = 0.4425380229949951 + 100.0 * 8.393952369689941
Epoch 600, val loss: 0.5003200173377991
Epoch 610, training loss: 839.5557250976562 = 0.43935084342956543 + 100.0 * 8.39116382598877
Epoch 610, val loss: 0.4983016550540924
Epoch 620, training loss: 839.32177734375 = 0.43638283014297485 + 100.0 * 8.388854026794434
Epoch 620, val loss: 0.4963896870613098
Epoch 630, training loss: 839.3676147460938 = 0.43359193205833435 + 100.0 * 8.3893404006958
Epoch 630, val loss: 0.49468231201171875
Epoch 640, training loss: 839.4390869140625 = 0.4308621883392334 + 100.0 * 8.390082359313965
Epoch 640, val loss: 0.49297866225242615
Epoch 650, training loss: 838.9153442382812 = 0.42821839451789856 + 100.0 * 8.384871482849121
Epoch 650, val loss: 0.4914455711841583
Epoch 660, training loss: 838.7478637695312 = 0.42579832673072815 + 100.0 * 8.383220672607422
Epoch 660, val loss: 0.4899623394012451
Epoch 670, training loss: 838.5911865234375 = 0.4235340654850006 + 100.0 * 8.38167667388916
Epoch 670, val loss: 0.48877477645874023
Epoch 680, training loss: 838.9407958984375 = 0.4213394224643707 + 100.0 * 8.385194778442383
Epoch 680, val loss: 0.48754310607910156
Epoch 690, training loss: 838.5570678710938 = 0.4191277325153351 + 100.0 * 8.381379127502441
Epoch 690, val loss: 0.48632559180259705
Epoch 700, training loss: 838.2678833007812 = 0.417039155960083 + 100.0 * 8.378508567810059
Epoch 700, val loss: 0.48525309562683105
Epoch 710, training loss: 838.241455078125 = 0.4150758981704712 + 100.0 * 8.378263473510742
Epoch 710, val loss: 0.4843127131462097
Epoch 720, training loss: 837.9751586914062 = 0.4130968153476715 + 100.0 * 8.37562084197998
Epoch 720, val loss: 0.48306891322135925
Epoch 730, training loss: 837.8340454101562 = 0.4112018644809723 + 100.0 * 8.374228477478027
Epoch 730, val loss: 0.48223739862442017
Epoch 740, training loss: 837.6318969726562 = 0.4094216227531433 + 100.0 * 8.372224807739258
Epoch 740, val loss: 0.48124954104423523
Epoch 750, training loss: 837.63427734375 = 0.407699853181839 + 100.0 * 8.372265815734863
Epoch 750, val loss: 0.48046478629112244
Epoch 760, training loss: 837.4569702148438 = 0.4059550166130066 + 100.0 * 8.37051010131836
Epoch 760, val loss: 0.47960758209228516
Epoch 770, training loss: 837.450927734375 = 0.4042228162288666 + 100.0 * 8.370467185974121
Epoch 770, val loss: 0.47869858145713806
Epoch 780, training loss: 837.3081665039062 = 0.40258482098579407 + 100.0 * 8.36905574798584
Epoch 780, val loss: 0.4779037833213806
Epoch 790, training loss: 837.1356811523438 = 0.4009701609611511 + 100.0 * 8.367347717285156
Epoch 790, val loss: 0.47722533345222473
Epoch 800, training loss: 837.1044311523438 = 0.3993857204914093 + 100.0 * 8.367050170898438
Epoch 800, val loss: 0.47643905878067017
Epoch 810, training loss: 836.860595703125 = 0.3978384733200073 + 100.0 * 8.364627838134766
Epoch 810, val loss: 0.47563859820365906
Epoch 820, training loss: 836.7481689453125 = 0.39634090662002563 + 100.0 * 8.363517761230469
Epoch 820, val loss: 0.4749390780925751
Epoch 830, training loss: 837.0565795898438 = 0.3948640525341034 + 100.0 * 8.366617202758789
Epoch 830, val loss: 0.47424736618995667
Epoch 840, training loss: 836.7177124023438 = 0.3933466374874115 + 100.0 * 8.363243103027344
Epoch 840, val loss: 0.47351551055908203
Epoch 850, training loss: 836.4808349609375 = 0.3918763995170593 + 100.0 * 8.360889434814453
Epoch 850, val loss: 0.472648948431015
Epoch 860, training loss: 836.2898559570312 = 0.3905003070831299 + 100.0 * 8.358993530273438
Epoch 860, val loss: 0.4720172584056854
Epoch 870, training loss: 836.2515869140625 = 0.38915932178497314 + 100.0 * 8.358624458312988
Epoch 870, val loss: 0.47133126854896545
Epoch 880, training loss: 836.6409301757812 = 0.3877745270729065 + 100.0 * 8.362531661987305
Epoch 880, val loss: 0.47060975432395935
Epoch 890, training loss: 836.044921875 = 0.38635310530662537 + 100.0 * 8.356585502624512
Epoch 890, val loss: 0.47001487016677856
Epoch 900, training loss: 836.2728271484375 = 0.38503003120422363 + 100.0 * 8.358878135681152
Epoch 900, val loss: 0.4695202708244324
Epoch 910, training loss: 835.9351196289062 = 0.383678674697876 + 100.0 * 8.355514526367188
Epoch 910, val loss: 0.4685695469379425
Epoch 920, training loss: 835.8092651367188 = 0.3823840320110321 + 100.0 * 8.354269027709961
Epoch 920, val loss: 0.46809980273246765
Epoch 930, training loss: 835.676513671875 = 0.3811245262622833 + 100.0 * 8.352953910827637
Epoch 930, val loss: 0.46745139360427856
Epoch 940, training loss: 835.5957641601562 = 0.37989550828933716 + 100.0 * 8.352158546447754
Epoch 940, val loss: 0.466880202293396
Epoch 950, training loss: 836.048828125 = 0.37866994738578796 + 100.0 * 8.356701850891113
Epoch 950, val loss: 0.4663495421409607
Epoch 960, training loss: 836.1320190429688 = 0.37734025716781616 + 100.0 * 8.35754680633545
Epoch 960, val loss: 0.46552735567092896
Epoch 970, training loss: 835.5772094726562 = 0.3760223686695099 + 100.0 * 8.352011680603027
Epoch 970, val loss: 0.464979887008667
Epoch 980, training loss: 835.3457641601562 = 0.3748072683811188 + 100.0 * 8.349709510803223
Epoch 980, val loss: 0.46420371532440186
Epoch 990, training loss: 835.2550659179688 = 0.37364962697029114 + 100.0 * 8.348814010620117
Epoch 990, val loss: 0.46368762850761414
Epoch 1000, training loss: 835.6499633789062 = 0.37249183654785156 + 100.0 * 8.352774620056152
Epoch 1000, val loss: 0.46309229731559753
Epoch 1010, training loss: 835.3018798828125 = 0.3712894320487976 + 100.0 * 8.349306106567383
Epoch 1010, val loss: 0.46245819330215454
Epoch 1020, training loss: 835.126953125 = 0.37010300159454346 + 100.0 * 8.34756851196289
Epoch 1020, val loss: 0.4617919325828552
Epoch 1030, training loss: 835.1356201171875 = 0.368969202041626 + 100.0 * 8.34766674041748
Epoch 1030, val loss: 0.461241215467453
Epoch 1040, training loss: 835.0496826171875 = 0.36778920888900757 + 100.0 * 8.346818923950195
Epoch 1040, val loss: 0.4606730043888092
Epoch 1050, training loss: 834.9505615234375 = 0.3666139245033264 + 100.0 * 8.345839500427246
Epoch 1050, val loss: 0.45991620421409607
Epoch 1060, training loss: 835.2789916992188 = 0.3654707074165344 + 100.0 * 8.349135398864746
Epoch 1060, val loss: 0.4592134654521942
Epoch 1070, training loss: 834.8090209960938 = 0.364338755607605 + 100.0 * 8.344447135925293
Epoch 1070, val loss: 0.4586836099624634
Epoch 1080, training loss: 834.6500854492188 = 0.3632410168647766 + 100.0 * 8.34286880493164
Epoch 1080, val loss: 0.4580404460430145
Epoch 1090, training loss: 834.59326171875 = 0.3621656000614166 + 100.0 * 8.342310905456543
Epoch 1090, val loss: 0.4574586749076843
Epoch 1100, training loss: 836.0568237304688 = 0.36108294129371643 + 100.0 * 8.35695743560791
Epoch 1100, val loss: 0.4570915400981903
Epoch 1110, training loss: 834.6080932617188 = 0.35985130071640015 + 100.0 * 8.342482566833496
Epoch 1110, val loss: 0.45606693625450134
Epoch 1120, training loss: 834.5443725585938 = 0.35873910784721375 + 100.0 * 8.341856002807617
Epoch 1120, val loss: 0.4554238021373749
Epoch 1130, training loss: 834.3232421875 = 0.35769757628440857 + 100.0 * 8.339654922485352
Epoch 1130, val loss: 0.4548965394496918
Epoch 1140, training loss: 834.2659301757812 = 0.3566828966140747 + 100.0 * 8.339092254638672
Epoch 1140, val loss: 0.45434024930000305
Epoch 1150, training loss: 834.1971435546875 = 0.35566580295562744 + 100.0 * 8.338415145874023
Epoch 1150, val loss: 0.4537428617477417
Epoch 1160, training loss: 834.7918701171875 = 0.3546510338783264 + 100.0 * 8.344371795654297
Epoch 1160, val loss: 0.45322439074516296
Epoch 1170, training loss: 834.60986328125 = 0.35348987579345703 + 100.0 * 8.34256362915039
Epoch 1170, val loss: 0.4524815082550049
Epoch 1180, training loss: 834.2076416015625 = 0.35239532589912415 + 100.0 * 8.338552474975586
Epoch 1180, val loss: 0.45190030336380005
Epoch 1190, training loss: 834.030517578125 = 0.3513451814651489 + 100.0 * 8.3367919921875
Epoch 1190, val loss: 0.4512256383895874
Epoch 1200, training loss: 833.9744262695312 = 0.3503321707248688 + 100.0 * 8.336240768432617
Epoch 1200, val loss: 0.4505883753299713
Epoch 1210, training loss: 834.23095703125 = 0.3493199944496155 + 100.0 * 8.33881664276123
Epoch 1210, val loss: 0.4499162435531616
Epoch 1220, training loss: 833.8087768554688 = 0.34824779629707336 + 100.0 * 8.33460521697998
Epoch 1220, val loss: 0.4493929445743561
Epoch 1230, training loss: 833.840087890625 = 0.34721839427948 + 100.0 * 8.334928512573242
Epoch 1230, val loss: 0.4488309919834137
Epoch 1240, training loss: 834.1154174804688 = 0.3462037444114685 + 100.0 * 8.337692260742188
Epoch 1240, val loss: 0.4480161964893341
Epoch 1250, training loss: 834.1696166992188 = 0.3450850546360016 + 100.0 * 8.338245391845703
Epoch 1250, val loss: 0.44757333397865295
Epoch 1260, training loss: 833.6654052734375 = 0.343982458114624 + 100.0 * 8.333213806152344
Epoch 1260, val loss: 0.4468599855899811
Epoch 1270, training loss: 833.629150390625 = 0.34296637773513794 + 100.0 * 8.33286190032959
Epoch 1270, val loss: 0.4462394118309021
Epoch 1280, training loss: 833.5311889648438 = 0.34198400378227234 + 100.0 * 8.331892013549805
Epoch 1280, val loss: 0.44571653008461
Epoch 1290, training loss: 833.632080078125 = 0.34100157022476196 + 100.0 * 8.332910537719727
Epoch 1290, val loss: 0.4451613426208496
Epoch 1300, training loss: 833.6580200195312 = 0.33996108174324036 + 100.0 * 8.33318042755127
Epoch 1300, val loss: 0.44453829526901245
Epoch 1310, training loss: 833.3854370117188 = 0.3389042615890503 + 100.0 * 8.330465316772461
Epoch 1310, val loss: 0.4439701735973358
Epoch 1320, training loss: 833.33447265625 = 0.3378920257091522 + 100.0 * 8.329965591430664
Epoch 1320, val loss: 0.4434332549571991
Epoch 1330, training loss: 833.3494262695312 = 0.3369036912918091 + 100.0 * 8.330124855041504
Epoch 1330, val loss: 0.4429446756839752
Epoch 1340, training loss: 833.9657592773438 = 0.33589717745780945 + 100.0 * 8.336298942565918
Epoch 1340, val loss: 0.4425014555454254
Epoch 1350, training loss: 833.4735107421875 = 0.3348473310470581 + 100.0 * 8.33138656616211
Epoch 1350, val loss: 0.4416450262069702
Epoch 1360, training loss: 833.3158569335938 = 0.333819717168808 + 100.0 * 8.32982063293457
Epoch 1360, val loss: 0.44124531745910645
Epoch 1370, training loss: 833.737548828125 = 0.3328150510787964 + 100.0 * 8.334047317504883
Epoch 1370, val loss: 0.44057872891426086
Epoch 1380, training loss: 833.2646484375 = 0.3317888379096985 + 100.0 * 8.329328536987305
Epoch 1380, val loss: 0.44015440344810486
Epoch 1390, training loss: 833.0921630859375 = 0.33079466223716736 + 100.0 * 8.327613830566406
Epoch 1390, val loss: 0.4396522045135498
Epoch 1400, training loss: 833.0109252929688 = 0.32982301712036133 + 100.0 * 8.326810836791992
Epoch 1400, val loss: 0.4392073154449463
Epoch 1410, training loss: 833.4974365234375 = 0.3288539946079254 + 100.0 * 8.331686019897461
Epoch 1410, val loss: 0.43879246711730957
Epoch 1420, training loss: 833.2221069335938 = 0.3277835547924042 + 100.0 * 8.328943252563477
Epoch 1420, val loss: 0.43813058733940125
Epoch 1430, training loss: 833.004150390625 = 0.32675591111183167 + 100.0 * 8.326773643493652
Epoch 1430, val loss: 0.4376981258392334
Epoch 1440, training loss: 832.9067993164062 = 0.3257579207420349 + 100.0 * 8.325810432434082
Epoch 1440, val loss: 0.4371912181377411
Epoch 1450, training loss: 832.8873291015625 = 0.32479017972946167 + 100.0 * 8.3256254196167
Epoch 1450, val loss: 0.43674275279045105
Epoch 1460, training loss: 833.4302978515625 = 0.32383081316947937 + 100.0 * 8.331064224243164
Epoch 1460, val loss: 0.4361291825771332
Epoch 1470, training loss: 832.8785400390625 = 0.32278165221214294 + 100.0 * 8.325557708740234
Epoch 1470, val loss: 0.43599557876586914
Epoch 1480, training loss: 832.7495727539062 = 0.3218001425266266 + 100.0 * 8.324277877807617
Epoch 1480, val loss: 0.43541979789733887
Epoch 1490, training loss: 832.74462890625 = 0.3208376169204712 + 100.0 * 8.324237823486328
Epoch 1490, val loss: 0.4351003170013428
Epoch 1500, training loss: 833.443359375 = 0.3198769688606262 + 100.0 * 8.3312349319458
Epoch 1500, val loss: 0.4347095489501953
Epoch 1510, training loss: 832.834716796875 = 0.31881430745124817 + 100.0 * 8.325159072875977
Epoch 1510, val loss: 0.4341728091239929
Epoch 1520, training loss: 832.6062622070312 = 0.3178228437900543 + 100.0 * 8.322884559631348
Epoch 1520, val loss: 0.4337700307369232
Epoch 1530, training loss: 832.5617065429688 = 0.31686344742774963 + 100.0 * 8.32244873046875
Epoch 1530, val loss: 0.4334671199321747
Epoch 1540, training loss: 832.5352172851562 = 0.31592556834220886 + 100.0 * 8.322193145751953
Epoch 1540, val loss: 0.4330935478210449
Epoch 1550, training loss: 833.1873168945312 = 0.31498438119888306 + 100.0 * 8.328722953796387
Epoch 1550, val loss: 0.4329794943332672
Epoch 1560, training loss: 832.8986206054688 = 0.31395140290260315 + 100.0 * 8.325846672058105
Epoch 1560, val loss: 0.43234783411026
Epoch 1570, training loss: 832.4255981445312 = 0.3129405677318573 + 100.0 * 8.321126937866211
Epoch 1570, val loss: 0.4319983720779419
Epoch 1580, training loss: 832.3967895507812 = 0.3119940757751465 + 100.0 * 8.320847511291504
Epoch 1580, val loss: 0.4317243695259094
Epoch 1590, training loss: 832.3886108398438 = 0.31108108162879944 + 100.0 * 8.320775032043457
Epoch 1590, val loss: 0.4314310550689697
Epoch 1600, training loss: 833.1234130859375 = 0.3101535439491272 + 100.0 * 8.328132629394531
Epoch 1600, val loss: 0.43117767572402954
Epoch 1610, training loss: 832.5999755859375 = 0.3091592788696289 + 100.0 * 8.322908401489258
Epoch 1610, val loss: 0.4307946562767029
Epoch 1620, training loss: 832.3742065429688 = 0.3082059323787689 + 100.0 * 8.320659637451172
Epoch 1620, val loss: 0.4306090474128723
Epoch 1630, training loss: 832.2813720703125 = 0.3072777986526489 + 100.0 * 8.319741249084473
Epoch 1630, val loss: 0.43032070994377136
Epoch 1640, training loss: 832.5350952148438 = 0.30636003613471985 + 100.0 * 8.322287559509277
Epoch 1640, val loss: 0.4300623834133148
Epoch 1650, training loss: 832.173095703125 = 0.30539748072624207 + 100.0 * 8.318676948547363
Epoch 1650, val loss: 0.429805189371109
Epoch 1660, training loss: 832.1766967773438 = 0.3044579327106476 + 100.0 * 8.31872272491455
Epoch 1660, val loss: 0.4296915531158447
Epoch 1670, training loss: 832.1744384765625 = 0.30353879928588867 + 100.0 * 8.318709373474121
Epoch 1670, val loss: 0.42951375246047974
Epoch 1680, training loss: 832.4364013671875 = 0.3025961220264435 + 100.0 * 8.321337699890137
Epoch 1680, val loss: 0.42926833033561707
Epoch 1690, training loss: 832.0615844726562 = 0.3016214370727539 + 100.0 * 8.317599296569824
Epoch 1690, val loss: 0.4290027320384979
Epoch 1700, training loss: 832.10595703125 = 0.30068594217300415 + 100.0 * 8.318053245544434
Epoch 1700, val loss: 0.42874738574028015
Epoch 1710, training loss: 832.229248046875 = 0.2997521162033081 + 100.0 * 8.319294929504395
Epoch 1710, val loss: 0.42854711413383484
Epoch 1720, training loss: 832.1107177734375 = 0.29883167147636414 + 100.0 * 8.318119049072266
Epoch 1720, val loss: 0.42826658487319946
Epoch 1730, training loss: 832.3626708984375 = 0.2978712022304535 + 100.0 * 8.320648193359375
Epoch 1730, val loss: 0.4280175566673279
Epoch 1740, training loss: 831.9854736328125 = 0.2968994677066803 + 100.0 * 8.316885948181152
Epoch 1740, val loss: 0.4280821681022644
Epoch 1750, training loss: 831.8575439453125 = 0.29596230387687683 + 100.0 * 8.3156156539917
Epoch 1750, val loss: 0.42774760723114014
Epoch 1760, training loss: 831.809326171875 = 0.29505449533462524 + 100.0 * 8.315142631530762
Epoch 1760, val loss: 0.4276960492134094
Epoch 1770, training loss: 831.786376953125 = 0.2941475212574005 + 100.0 * 8.314922332763672
Epoch 1770, val loss: 0.4275195896625519
Epoch 1780, training loss: 831.8273315429688 = 0.2932274341583252 + 100.0 * 8.315340995788574
Epoch 1780, val loss: 0.42737719416618347
Epoch 1790, training loss: 832.6325073242188 = 0.29227113723754883 + 100.0 * 8.323402404785156
Epoch 1790, val loss: 0.427065908908844
Epoch 1800, training loss: 831.8710327148438 = 0.2912495732307434 + 100.0 * 8.315797805786133
Epoch 1800, val loss: 0.4269956648349762
Epoch 1810, training loss: 831.7125854492188 = 0.29027652740478516 + 100.0 * 8.314223289489746
Epoch 1810, val loss: 0.42685946822166443
Epoch 1820, training loss: 831.7013549804688 = 0.28933587670326233 + 100.0 * 8.314120292663574
Epoch 1820, val loss: 0.4266436994075775
Epoch 1830, training loss: 831.6904907226562 = 0.2884135842323303 + 100.0 * 8.314021110534668
Epoch 1830, val loss: 0.42661169171333313
Epoch 1840, training loss: 832.2846069335938 = 0.28748324513435364 + 100.0 * 8.319971084594727
Epoch 1840, val loss: 0.42658689618110657
Epoch 1850, training loss: 832.058837890625 = 0.2864716053009033 + 100.0 * 8.317723274230957
Epoch 1850, val loss: 0.42620494961738586
Epoch 1860, training loss: 831.6870727539062 = 0.2854953110218048 + 100.0 * 8.31401538848877
Epoch 1860, val loss: 0.42611098289489746
Epoch 1870, training loss: 831.5656127929688 = 0.2845434844493866 + 100.0 * 8.312810897827148
Epoch 1870, val loss: 0.4260094463825226
Epoch 1880, training loss: 831.4979858398438 = 0.28360775113105774 + 100.0 * 8.312143325805664
Epoch 1880, val loss: 0.4258947968482971
Epoch 1890, training loss: 831.4989013671875 = 0.28267085552215576 + 100.0 * 8.312162399291992
Epoch 1890, val loss: 0.4258427321910858
Epoch 1900, training loss: 832.1434936523438 = 0.28173530101776123 + 100.0 * 8.318617820739746
Epoch 1900, val loss: 0.4259427785873413
Epoch 1910, training loss: 831.6190185546875 = 0.2807161509990692 + 100.0 * 8.313383102416992
Epoch 1910, val loss: 0.42549556493759155
Epoch 1920, training loss: 831.5421142578125 = 0.2797122299671173 + 100.0 * 8.312623977661133
Epoch 1920, val loss: 0.42544662952423096
Epoch 1930, training loss: 831.6614990234375 = 0.2787489593029022 + 100.0 * 8.313827514648438
Epoch 1930, val loss: 0.4252995252609253
Epoch 1940, training loss: 831.6098022460938 = 0.27777138352394104 + 100.0 * 8.31332015991211
Epoch 1940, val loss: 0.4253627359867096
Epoch 1950, training loss: 831.4495239257812 = 0.27678021788597107 + 100.0 * 8.311727523803711
Epoch 1950, val loss: 0.42529869079589844
Epoch 1960, training loss: 831.36474609375 = 0.27578845620155334 + 100.0 * 8.31088924407959
Epoch 1960, val loss: 0.42510294914245605
Epoch 1970, training loss: 831.3342895507812 = 0.2748193144798279 + 100.0 * 8.31059455871582
Epoch 1970, val loss: 0.4251313805580139
Epoch 1980, training loss: 831.6364135742188 = 0.27384519577026367 + 100.0 * 8.31362533569336
Epoch 1980, val loss: 0.4250989258289337
Epoch 1990, training loss: 831.3021240234375 = 0.2728261649608612 + 100.0 * 8.310293197631836
Epoch 1990, val loss: 0.4249908924102783
Epoch 2000, training loss: 831.3910522460938 = 0.27181899547576904 + 100.0 * 8.311192512512207
Epoch 2000, val loss: 0.42502734065055847
Epoch 2010, training loss: 831.6246948242188 = 0.2708115577697754 + 100.0 * 8.313538551330566
Epoch 2010, val loss: 0.4250790774822235
Epoch 2020, training loss: 831.2400512695312 = 0.26976004242897034 + 100.0 * 8.30970287322998
Epoch 2020, val loss: 0.42482510209083557
Epoch 2030, training loss: 831.1966552734375 = 0.26874658465385437 + 100.0 * 8.309279441833496
Epoch 2030, val loss: 0.42487815022468567
Epoch 2040, training loss: 831.3258056640625 = 0.2677401900291443 + 100.0 * 8.310580253601074
Epoch 2040, val loss: 0.4249911904335022
Epoch 2050, training loss: 831.2566528320312 = 0.2666989266872406 + 100.0 * 8.30989933013916
Epoch 2050, val loss: 0.42493245005607605
Epoch 2060, training loss: 831.1964111328125 = 0.26564401388168335 + 100.0 * 8.309308052062988
Epoch 2060, val loss: 0.4247886538505554
Epoch 2070, training loss: 831.3585205078125 = 0.26462358236312866 + 100.0 * 8.310938835144043
Epoch 2070, val loss: 0.42495855689048767
Epoch 2080, training loss: 831.1922607421875 = 0.2635650634765625 + 100.0 * 8.309287071228027
Epoch 2080, val loss: 0.4248129725456238
Epoch 2090, training loss: 831.2279663085938 = 0.2625100612640381 + 100.0 * 8.309654235839844
Epoch 2090, val loss: 0.42485710978507996
Epoch 2100, training loss: 831.335693359375 = 0.2614622116088867 + 100.0 * 8.310742378234863
Epoch 2100, val loss: 0.42493218183517456
Epoch 2110, training loss: 831.1426391601562 = 0.2603967785835266 + 100.0 * 8.308822631835938
Epoch 2110, val loss: 0.4248223900794983
Epoch 2120, training loss: 831.0174560546875 = 0.25934287905693054 + 100.0 * 8.307580947875977
Epoch 2120, val loss: 0.42480236291885376
Epoch 2130, training loss: 830.9879760742188 = 0.258297324180603 + 100.0 * 8.307296752929688
Epoch 2130, val loss: 0.4247879683971405
Epoch 2140, training loss: 831.0345458984375 = 0.25724878907203674 + 100.0 * 8.307772636413574
Epoch 2140, val loss: 0.42482128739356995
Epoch 2150, training loss: 831.187744140625 = 0.25618603825569153 + 100.0 * 8.30931568145752
Epoch 2150, val loss: 0.4249567985534668
Epoch 2160, training loss: 831.3643798828125 = 0.2551068067550659 + 100.0 * 8.311092376708984
Epoch 2160, val loss: 0.4251011610031128
Epoch 2170, training loss: 830.9590454101562 = 0.2539830207824707 + 100.0 * 8.307050704956055
Epoch 2170, val loss: 0.424928218126297
Epoch 2180, training loss: 830.881103515625 = 0.2529011368751526 + 100.0 * 8.306282043457031
Epoch 2180, val loss: 0.42494451999664307
Epoch 2190, training loss: 830.8388671875 = 0.2518438994884491 + 100.0 * 8.305870056152344
Epoch 2190, val loss: 0.4250198304653168
Epoch 2200, training loss: 830.864501953125 = 0.2507937550544739 + 100.0 * 8.306137084960938
Epoch 2200, val loss: 0.42503875494003296
Epoch 2210, training loss: 831.3363647460938 = 0.24973656237125397 + 100.0 * 8.310866355895996
Epoch 2210, val loss: 0.42503613233566284
Epoch 2220, training loss: 830.9265747070312 = 0.24863462150096893 + 100.0 * 8.306778907775879
Epoch 2220, val loss: 0.4251319468021393
Epoch 2230, training loss: 830.787841796875 = 0.24754422903060913 + 100.0 * 8.305402755737305
Epoch 2230, val loss: 0.42524033784866333
Epoch 2240, training loss: 830.83349609375 = 0.2464776486158371 + 100.0 * 8.305870056152344
Epoch 2240, val loss: 0.4253954291343689
Epoch 2250, training loss: 831.3414916992188 = 0.2454204559326172 + 100.0 * 8.31096076965332
Epoch 2250, val loss: 0.4255666136741638
Epoch 2260, training loss: 830.8926391601562 = 0.24430309236049652 + 100.0 * 8.306483268737793
Epoch 2260, val loss: 0.4253380000591278
Epoch 2270, training loss: 830.7345581054688 = 0.24321383237838745 + 100.0 * 8.304913520812988
Epoch 2270, val loss: 0.4255506694316864
Epoch 2280, training loss: 830.795654296875 = 0.2421528548002243 + 100.0 * 8.305535316467285
Epoch 2280, val loss: 0.4256807267665863
Epoch 2290, training loss: 831.174560546875 = 0.24107860028743744 + 100.0 * 8.309334754943848
Epoch 2290, val loss: 0.42577627301216125
Epoch 2300, training loss: 830.8226318359375 = 0.23994307219982147 + 100.0 * 8.305827140808105
Epoch 2300, val loss: 0.4255608320236206
Epoch 2310, training loss: 830.6991577148438 = 0.23883835971355438 + 100.0 * 8.304603576660156
Epoch 2310, val loss: 0.4257369339466095
Epoch 2320, training loss: 830.6474609375 = 0.23774419724941254 + 100.0 * 8.304097175598145
Epoch 2320, val loss: 0.42580166459083557
Epoch 2330, training loss: 830.6802368164062 = 0.23665201663970947 + 100.0 * 8.304435729980469
Epoch 2330, val loss: 0.4258559048175812
Epoch 2340, training loss: 830.8482055664062 = 0.235552579164505 + 100.0 * 8.306126594543457
Epoch 2340, val loss: 0.4259687662124634
Epoch 2350, training loss: 830.6942138671875 = 0.23443205654621124 + 100.0 * 8.304597854614258
Epoch 2350, val loss: 0.4261186122894287
Epoch 2360, training loss: 830.7843627929688 = 0.23330804705619812 + 100.0 * 8.305510520935059
Epoch 2360, val loss: 0.4261975884437561
Epoch 2370, training loss: 830.7661743164062 = 0.232183039188385 + 100.0 * 8.305339813232422
Epoch 2370, val loss: 0.42619436979293823
Epoch 2380, training loss: 830.5416259765625 = 0.23103822767734528 + 100.0 * 8.303106307983398
Epoch 2380, val loss: 0.4264574348926544
Epoch 2390, training loss: 830.4889526367188 = 0.22991631925106049 + 100.0 * 8.302590370178223
Epoch 2390, val loss: 0.4265832304954529
Epoch 2400, training loss: 830.5843505859375 = 0.2288047969341278 + 100.0 * 8.303555488586426
Epoch 2400, val loss: 0.4268280267715454
Epoch 2410, training loss: 831.2137451171875 = 0.22770336270332336 + 100.0 * 8.309860229492188
Epoch 2410, val loss: 0.4270153343677521
Epoch 2420, training loss: 830.6788330078125 = 0.22650063037872314 + 100.0 * 8.304523468017578
Epoch 2420, val loss: 0.42683544754981995
Epoch 2430, training loss: 830.4837036132812 = 0.22534726560115814 + 100.0 * 8.302583694458008
Epoch 2430, val loss: 0.42707204818725586
Epoch 2440, training loss: 830.4006958007812 = 0.22421793639659882 + 100.0 * 8.301764488220215
Epoch 2440, val loss: 0.42721569538116455
Epoch 2450, training loss: 830.4000244140625 = 0.22309933602809906 + 100.0 * 8.301769256591797
Epoch 2450, val loss: 0.42738696932792664
Epoch 2460, training loss: 830.9334716796875 = 0.22197872400283813 + 100.0 * 8.307114601135254
Epoch 2460, val loss: 0.4274664521217346
Epoch 2470, training loss: 830.443603515625 = 0.22080528736114502 + 100.0 * 8.302227973937988
Epoch 2470, val loss: 0.42780259251594543
Epoch 2480, training loss: 830.569580078125 = 0.219650000333786 + 100.0 * 8.303499221801758
Epoch 2480, val loss: 0.4278484582901001
Epoch 2490, training loss: 830.4298706054688 = 0.21847571432590485 + 100.0 * 8.30211353302002
Epoch 2490, val loss: 0.4280490279197693
Epoch 2500, training loss: 830.3364868164062 = 0.21731126308441162 + 100.0 * 8.301192283630371
Epoch 2500, val loss: 0.4281650483608246
Epoch 2510, training loss: 830.6018676757812 = 0.2161707580089569 + 100.0 * 8.30385684967041
Epoch 2510, val loss: 0.4282337725162506
Epoch 2520, training loss: 830.37255859375 = 0.21498943865299225 + 100.0 * 8.301575660705566
Epoch 2520, val loss: 0.428545206785202
Epoch 2530, training loss: 830.27001953125 = 0.21381151676177979 + 100.0 * 8.300561904907227
Epoch 2530, val loss: 0.4288146495819092
Epoch 2540, training loss: 830.5147094726562 = 0.21265481412410736 + 100.0 * 8.303020477294922
Epoch 2540, val loss: 0.4288959801197052
Epoch 2550, training loss: 830.3577880859375 = 0.211467444896698 + 100.0 * 8.30146312713623
Epoch 2550, val loss: 0.4291508197784424
Epoch 2560, training loss: 830.279541015625 = 0.21028465032577515 + 100.0 * 8.300692558288574
Epoch 2560, val loss: 0.4294520914554596
Epoch 2570, training loss: 830.2339477539062 = 0.20911289751529694 + 100.0 * 8.300248146057129
Epoch 2570, val loss: 0.42963889241218567
Epoch 2580, training loss: 830.7803344726562 = 0.20796072483062744 + 100.0 * 8.305724143981934
Epoch 2580, val loss: 0.4299303889274597
Epoch 2590, training loss: 830.4547729492188 = 0.2067715972661972 + 100.0 * 8.30247974395752
Epoch 2590, val loss: 0.4303072392940521
Epoch 2600, training loss: 830.21826171875 = 0.20557108521461487 + 100.0 * 8.300127029418945
Epoch 2600, val loss: 0.43045493960380554
Epoch 2610, training loss: 830.142822265625 = 0.2043982744216919 + 100.0 * 8.299384117126465
Epoch 2610, val loss: 0.4308065176010132
Epoch 2620, training loss: 830.1843872070312 = 0.20323669910430908 + 100.0 * 8.299811363220215
Epoch 2620, val loss: 0.43114960193634033
Epoch 2630, training loss: 830.5767211914062 = 0.20207743346691132 + 100.0 * 8.303746223449707
Epoch 2630, val loss: 0.4315811097621918
Epoch 2640, training loss: 830.62744140625 = 0.20087818801403046 + 100.0 * 8.304265975952148
Epoch 2640, val loss: 0.4316563308238983
Epoch 2650, training loss: 830.2233276367188 = 0.19965748488903046 + 100.0 * 8.300236701965332
Epoch 2650, val loss: 0.4319671094417572
Epoch 2660, training loss: 830.080078125 = 0.19846662878990173 + 100.0 * 8.298815727233887
Epoch 2660, val loss: 0.43215692043304443
Epoch 2670, training loss: 830.0421142578125 = 0.19729465246200562 + 100.0 * 8.29844856262207
Epoch 2670, val loss: 0.4325955808162689
Epoch 2680, training loss: 830.1492309570312 = 0.19612713158130646 + 100.0 * 8.299530982971191
Epoch 2680, val loss: 0.43299850821495056
Epoch 2690, training loss: 830.2991943359375 = 0.1949404776096344 + 100.0 * 8.301042556762695
Epoch 2690, val loss: 0.4332958161830902
Epoch 2700, training loss: 830.1509399414062 = 0.19373582303524017 + 100.0 * 8.299571990966797
Epoch 2700, val loss: 0.43347594141960144
Epoch 2710, training loss: 830.4012451171875 = 0.1925649791955948 + 100.0 * 8.30208683013916
Epoch 2710, val loss: 0.43410274386405945
Epoch 2720, training loss: 830.0167236328125 = 0.19134196639060974 + 100.0 * 8.298254013061523
Epoch 2720, val loss: 0.4341561794281006
Epoch 2730, training loss: 829.9817504882812 = 0.19014598429203033 + 100.0 * 8.297916412353516
Epoch 2730, val loss: 0.4345381259918213
Epoch 2740, training loss: 829.979248046875 = 0.18896767497062683 + 100.0 * 8.297903060913086
Epoch 2740, val loss: 0.4348895847797394
Epoch 2750, training loss: 830.6387939453125 = 0.1878138929605484 + 100.0 * 8.304510116577148
Epoch 2750, val loss: 0.43534624576568604
Epoch 2760, training loss: 830.0231323242188 = 0.18659014999866486 + 100.0 * 8.298365592956543
Epoch 2760, val loss: 0.43562546372413635
Epoch 2770, training loss: 829.9005126953125 = 0.18539093434810638 + 100.0 * 8.297151565551758
Epoch 2770, val loss: 0.4358466863632202
Epoch 2780, training loss: 829.84912109375 = 0.18421223759651184 + 100.0 * 8.296648979187012
Epoch 2780, val loss: 0.43622493743896484
Epoch 2790, training loss: 829.8253173828125 = 0.18303854763507843 + 100.0 * 8.296422958374023
Epoch 2790, val loss: 0.4365937411785126
Epoch 2800, training loss: 830.14111328125 = 0.18189005553722382 + 100.0 * 8.299592018127441
Epoch 2800, val loss: 0.43685397505760193
Epoch 2810, training loss: 829.874755859375 = 0.18068210780620575 + 100.0 * 8.296940803527832
Epoch 2810, val loss: 0.4374004304409027
Epoch 2820, training loss: 829.8173217773438 = 0.17948877811431885 + 100.0 * 8.296378135681152
Epoch 2820, val loss: 0.43789395689964294
Epoch 2830, training loss: 829.825439453125 = 0.17831556499004364 + 100.0 * 8.296470642089844
Epoch 2830, val loss: 0.4383684992790222
Epoch 2840, training loss: 830.3717651367188 = 0.17715539038181305 + 100.0 * 8.301945686340332
Epoch 2840, val loss: 0.43882858753204346
Epoch 2850, training loss: 829.8661499023438 = 0.17595750093460083 + 100.0 * 8.29690170288086
Epoch 2850, val loss: 0.43894892930984497
Epoch 2860, training loss: 829.9136962890625 = 0.17479348182678223 + 100.0 * 8.297389030456543
Epoch 2860, val loss: 0.43941614031791687
Epoch 2870, training loss: 829.9465942382812 = 0.1736210584640503 + 100.0 * 8.2977294921875
Epoch 2870, val loss: 0.4398866593837738
Epoch 2880, training loss: 829.720703125 = 0.17243939638137817 + 100.0 * 8.295482635498047
Epoch 2880, val loss: 0.4404602348804474
Epoch 2890, training loss: 829.7897338867188 = 0.17128780484199524 + 100.0 * 8.296184539794922
Epoch 2890, val loss: 0.4409107267856598
Epoch 2900, training loss: 829.810546875 = 0.17013636231422424 + 100.0 * 8.296403884887695
Epoch 2900, val loss: 0.4412943124771118
Epoch 2910, training loss: 829.9808959960938 = 0.16899096965789795 + 100.0 * 8.298118591308594
Epoch 2910, val loss: 0.44156500697135925
Epoch 2920, training loss: 829.9475708007812 = 0.16782516241073608 + 100.0 * 8.297797203063965
Epoch 2920, val loss: 0.4423227608203888
Epoch 2930, training loss: 829.8431396484375 = 0.16667503118515015 + 100.0 * 8.296764373779297
Epoch 2930, val loss: 0.4429340362548828
Epoch 2940, training loss: 829.7255249023438 = 0.16550280153751373 + 100.0 * 8.295599937438965
Epoch 2940, val loss: 0.4431793987751007
Epoch 2950, training loss: 829.7379760742188 = 0.16436506807804108 + 100.0 * 8.295736312866211
Epoch 2950, val loss: 0.44369402527809143
Epoch 2960, training loss: 829.7247314453125 = 0.16322270035743713 + 100.0 * 8.295615196228027
Epoch 2960, val loss: 0.4441660940647125
Epoch 2970, training loss: 829.656005859375 = 0.16208592057228088 + 100.0 * 8.294939041137695
Epoch 2970, val loss: 0.44492363929748535
Epoch 2980, training loss: 830.0770874023438 = 0.1609848141670227 + 100.0 * 8.299160957336426
Epoch 2980, val loss: 0.4456769526004791
Epoch 2990, training loss: 829.6024780273438 = 0.15980808436870575 + 100.0 * 8.294426918029785
Epoch 2990, val loss: 0.44586336612701416
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8493150684931506
0.8672027820039123
=== training gcn model ===
Epoch 0, training loss: 1059.3355712890625 = 1.109501838684082 + 100.0 * 10.582260131835938
Epoch 0, val loss: 1.1093865633010864
Epoch 10, training loss: 1059.2275390625 = 1.1025147438049316 + 100.0 * 10.581250190734863
Epoch 10, val loss: 1.1022902727127075
Epoch 20, training loss: 1058.3609619140625 = 1.0941334962844849 + 100.0 * 10.572668075561523
Epoch 20, val loss: 1.0937812328338623
Epoch 30, training loss: 1052.5694580078125 = 1.0839107036590576 + 100.0 * 10.51485538482666
Epoch 30, val loss: 1.0833632946014404
Epoch 40, training loss: 1028.86669921875 = 1.0728970766067505 + 100.0 * 10.277937889099121
Epoch 40, val loss: 1.0724472999572754
Epoch 50, training loss: 974.842529296875 = 1.062574028968811 + 100.0 * 9.737799644470215
Epoch 50, val loss: 1.0619511604309082
Epoch 60, training loss: 958.121826171875 = 1.053131103515625 + 100.0 * 9.570687294006348
Epoch 60, val loss: 1.0525811910629272
Epoch 70, training loss: 949.645751953125 = 1.0448706150054932 + 100.0 * 9.486008644104004
Epoch 70, val loss: 1.0445283651351929
Epoch 80, training loss: 940.1450805664062 = 1.037780523300171 + 100.0 * 9.391073226928711
Epoch 80, val loss: 1.0376216173171997
Epoch 90, training loss: 927.2240600585938 = 1.029799461364746 + 100.0 * 9.261942863464355
Epoch 90, val loss: 1.0297961235046387
Epoch 100, training loss: 911.43603515625 = 1.0211039781570435 + 100.0 * 9.104148864746094
Epoch 100, val loss: 1.0214543342590332
Epoch 110, training loss: 901.385986328125 = 1.0134750604629517 + 100.0 * 9.003725051879883
Epoch 110, val loss: 1.0140517950057983
Epoch 120, training loss: 896.5941162109375 = 1.0057517290115356 + 100.0 * 8.955883979797363
Epoch 120, val loss: 1.0060912370681763
Epoch 130, training loss: 891.5509643554688 = 0.9963397979736328 + 100.0 * 8.905546188354492
Epoch 130, val loss: 0.9966552257537842
Epoch 140, training loss: 884.2454833984375 = 0.9881424903869629 + 100.0 * 8.832572937011719
Epoch 140, val loss: 0.988789439201355
Epoch 150, training loss: 876.745361328125 = 0.9805107116699219 + 100.0 * 8.757648468017578
Epoch 150, val loss: 0.9812644720077515
Epoch 160, training loss: 872.1984252929688 = 0.9705767035484314 + 100.0 * 8.712278366088867
Epoch 160, val loss: 0.9713796973228455
Epoch 170, training loss: 869.2088012695312 = 0.9587368369102478 + 100.0 * 8.682500839233398
Epoch 170, val loss: 0.9597353935241699
Epoch 180, training loss: 866.2213134765625 = 0.9460878968238831 + 100.0 * 8.652751922607422
Epoch 180, val loss: 0.9473703503608704
Epoch 190, training loss: 864.6600952148438 = 0.9331523776054382 + 100.0 * 8.637269020080566
Epoch 190, val loss: 0.9346486926078796
Epoch 200, training loss: 862.3306274414062 = 0.9189271926879883 + 100.0 * 8.614116668701172
Epoch 200, val loss: 0.9206832051277161
Epoch 210, training loss: 860.7090454101562 = 0.9033563137054443 + 100.0 * 8.59805679321289
Epoch 210, val loss: 0.905476450920105
Epoch 220, training loss: 859.323486328125 = 0.8866235017776489 + 100.0 * 8.584368705749512
Epoch 220, val loss: 0.889211118221283
Epoch 230, training loss: 858.0999755859375 = 0.8691550493240356 + 100.0 * 8.572308540344238
Epoch 230, val loss: 0.8723403215408325
Epoch 240, training loss: 856.9638671875 = 0.8511900901794434 + 100.0 * 8.561126708984375
Epoch 240, val loss: 0.8551580309867859
Epoch 250, training loss: 855.8638305664062 = 0.8328564763069153 + 100.0 * 8.550309181213379
Epoch 250, val loss: 0.8375385403633118
Epoch 260, training loss: 854.677490234375 = 0.8143225312232971 + 100.0 * 8.538631439208984
Epoch 260, val loss: 0.8197385668754578
Epoch 270, training loss: 853.6386108398438 = 0.7955353260040283 + 100.0 * 8.528430938720703
Epoch 270, val loss: 0.8017999529838562
Epoch 280, training loss: 853.3174438476562 = 0.7765089273452759 + 100.0 * 8.525409698486328
Epoch 280, val loss: 0.7835690975189209
Epoch 290, training loss: 851.7388916015625 = 0.757375955581665 + 100.0 * 8.509815216064453
Epoch 290, val loss: 0.7654659748077393
Epoch 300, training loss: 850.7655639648438 = 0.738567054271698 + 100.0 * 8.500269889831543
Epoch 300, val loss: 0.7476543188095093
Epoch 310, training loss: 850.4213256835938 = 0.7199234366416931 + 100.0 * 8.497014045715332
Epoch 310, val loss: 0.7300246357917786
Epoch 320, training loss: 849.2498168945312 = 0.7013089656829834 + 100.0 * 8.485485076904297
Epoch 320, val loss: 0.7123720049858093
Epoch 330, training loss: 848.4912109375 = 0.6830689907073975 + 100.0 * 8.478081703186035
Epoch 330, val loss: 0.695256769657135
Epoch 340, training loss: 847.81640625 = 0.6653693914413452 + 100.0 * 8.471510887145996
Epoch 340, val loss: 0.6786863207817078
Epoch 350, training loss: 847.1959228515625 = 0.6483210325241089 + 100.0 * 8.465476036071777
Epoch 350, val loss: 0.6627616286277771
Epoch 360, training loss: 847.6560668945312 = 0.6317117810249329 + 100.0 * 8.470243453979492
Epoch 360, val loss: 0.6471540331840515
Epoch 370, training loss: 846.2559814453125 = 0.6157902479171753 + 100.0 * 8.456401824951172
Epoch 370, val loss: 0.6327244639396667
Epoch 380, training loss: 845.51220703125 = 0.6011191010475159 + 100.0 * 8.449110984802246
Epoch 380, val loss: 0.6191272735595703
Epoch 390, training loss: 845.0112915039062 = 0.5872445106506348 + 100.0 * 8.44424057006836
Epoch 390, val loss: 0.6064608693122864
Epoch 400, training loss: 844.546630859375 = 0.5741558074951172 + 100.0 * 8.439724922180176
Epoch 400, val loss: 0.5945687294006348
Epoch 410, training loss: 844.12646484375 = 0.5619027614593506 + 100.0 * 8.435646057128906
Epoch 410, val loss: 0.583514928817749
Epoch 420, training loss: 843.7492065429688 = 0.5503947138786316 + 100.0 * 8.431987762451172
Epoch 420, val loss: 0.5731929540634155
Epoch 430, training loss: 843.467529296875 = 0.5396900177001953 + 100.0 * 8.429278373718262
Epoch 430, val loss: 0.5636348128318787
Epoch 440, training loss: 843.0311889648438 = 0.5298804044723511 + 100.0 * 8.425012588500977
Epoch 440, val loss: 0.5549826622009277
Epoch 450, training loss: 843.1878051757812 = 0.520809531211853 + 100.0 * 8.42667007446289
Epoch 450, val loss: 0.5469759106636047
Epoch 460, training loss: 842.2505493164062 = 0.5123200416564941 + 100.0 * 8.41738224029541
Epoch 460, val loss: 0.5397697687149048
Epoch 470, training loss: 842.0133666992188 = 0.5045756101608276 + 100.0 * 8.415087699890137
Epoch 470, val loss: 0.5332047939300537
Epoch 480, training loss: 841.8995361328125 = 0.4973750114440918 + 100.0 * 8.414021492004395
Epoch 480, val loss: 0.5269865393638611
Epoch 490, training loss: 841.4551391601562 = 0.49066078662872314 + 100.0 * 8.409645080566406
Epoch 490, val loss: 0.5215709805488586
Epoch 500, training loss: 840.9774780273438 = 0.4845517873764038 + 100.0 * 8.404929161071777
Epoch 500, val loss: 0.5164527297019958
Epoch 510, training loss: 840.8726806640625 = 0.47887375950813293 + 100.0 * 8.403938293457031
Epoch 510, val loss: 0.5117132663726807
Epoch 520, training loss: 840.4742431640625 = 0.4734683632850647 + 100.0 * 8.400008201599121
Epoch 520, val loss: 0.5075405836105347
Epoch 530, training loss: 840.2413330078125 = 0.46849730610847473 + 100.0 * 8.397727966308594
Epoch 530, val loss: 0.503583550453186
Epoch 540, training loss: 840.34814453125 = 0.4638918340206146 + 100.0 * 8.398842811584473
Epoch 540, val loss: 0.49998608231544495
Epoch 550, training loss: 839.8063354492188 = 0.4595198929309845 + 100.0 * 8.393467903137207
Epoch 550, val loss: 0.49673718214035034
Epoch 560, training loss: 839.4890747070312 = 0.4554671049118042 + 100.0 * 8.390336036682129
Epoch 560, val loss: 0.4937537610530853
Epoch 570, training loss: 839.4872436523438 = 0.45165908336639404 + 100.0 * 8.390356063842773
Epoch 570, val loss: 0.49096056818962097
Epoch 580, training loss: 839.5633544921875 = 0.4479914903640747 + 100.0 * 8.391153335571289
Epoch 580, val loss: 0.488318532705307
Epoch 590, training loss: 838.8351440429688 = 0.44454461336135864 + 100.0 * 8.383906364440918
Epoch 590, val loss: 0.48586317896842957
Epoch 600, training loss: 838.672119140625 = 0.44134029746055603 + 100.0 * 8.382308006286621
Epoch 600, val loss: 0.48371291160583496
Epoch 610, training loss: 838.505615234375 = 0.43830764293670654 + 100.0 * 8.3806734085083
Epoch 610, val loss: 0.48166272044181824
Epoch 620, training loss: 839.0762329101562 = 0.43538275361061096 + 100.0 * 8.386408805847168
Epoch 620, val loss: 0.47974053025245667
Epoch 630, training loss: 838.1304321289062 = 0.43255800008773804 + 100.0 * 8.376978874206543
Epoch 630, val loss: 0.477853387594223
Epoch 640, training loss: 838.1243896484375 = 0.42993637919425964 + 100.0 * 8.376944541931152
Epoch 640, val loss: 0.4762347638607025
Epoch 650, training loss: 837.8101196289062 = 0.4274224042892456 + 100.0 * 8.37382698059082
Epoch 650, val loss: 0.47466936707496643
Epoch 660, training loss: 837.6697387695312 = 0.4250277280807495 + 100.0 * 8.37244701385498
Epoch 660, val loss: 0.47336769104003906
Epoch 670, training loss: 837.667724609375 = 0.42272695899009705 + 100.0 * 8.37244987487793
Epoch 670, val loss: 0.4719218909740448
Epoch 680, training loss: 837.5673828125 = 0.4205157458782196 + 100.0 * 8.371468544006348
Epoch 680, val loss: 0.470551073551178
Epoch 690, training loss: 837.5039672851562 = 0.41832804679870605 + 100.0 * 8.370856285095215
Epoch 690, val loss: 0.4694750905036926
Epoch 700, training loss: 837.0738525390625 = 0.4162577986717224 + 100.0 * 8.366576194763184
Epoch 700, val loss: 0.46821659803390503
Epoch 710, training loss: 836.9763793945312 = 0.4143127501010895 + 100.0 * 8.365620613098145
Epoch 710, val loss: 0.4670872390270233
Epoch 720, training loss: 836.7841186523438 = 0.4124274253845215 + 100.0 * 8.363717079162598
Epoch 720, val loss: 0.4661239981651306
Epoch 730, training loss: 837.437255859375 = 0.41058990359306335 + 100.0 * 8.370266914367676
Epoch 730, val loss: 0.4652799367904663
Epoch 740, training loss: 836.6629028320312 = 0.4087744951248169 + 100.0 * 8.362541198730469
Epoch 740, val loss: 0.4641766846179962
Epoch 750, training loss: 836.5532836914062 = 0.4070613384246826 + 100.0 * 8.361462593078613
Epoch 750, val loss: 0.4632421135902405
Epoch 760, training loss: 836.3634033203125 = 0.40540972352027893 + 100.0 * 8.359580039978027
Epoch 760, val loss: 0.46239396929740906
Epoch 770, training loss: 836.4111938476562 = 0.40380603075027466 + 100.0 * 8.360074043273926
Epoch 770, val loss: 0.46168795228004456
Epoch 780, training loss: 836.1965942382812 = 0.4022248089313507 + 100.0 * 8.357943534851074
Epoch 780, val loss: 0.4608730971813202
Epoch 790, training loss: 836.1064453125 = 0.4007013440132141 + 100.0 * 8.357057571411133
Epoch 790, val loss: 0.46010980010032654
Epoch 800, training loss: 836.1456909179688 = 0.39922070503234863 + 100.0 * 8.357464790344238
Epoch 800, val loss: 0.4594455063343048
Epoch 810, training loss: 836.1792602539062 = 0.3977581560611725 + 100.0 * 8.35781478881836
Epoch 810, val loss: 0.4586294889450073
Epoch 820, training loss: 835.832275390625 = 0.3963264226913452 + 100.0 * 8.35435962677002
Epoch 820, val loss: 0.45807862281799316
Epoch 830, training loss: 835.7117309570312 = 0.3949577510356903 + 100.0 * 8.353167533874512
Epoch 830, val loss: 0.45734238624572754
Epoch 840, training loss: 835.6671752929688 = 0.39362505078315735 + 100.0 * 8.35273551940918
Epoch 840, val loss: 0.4567587077617645
Epoch 850, training loss: 835.7791748046875 = 0.3923075795173645 + 100.0 * 8.35386848449707
Epoch 850, val loss: 0.45618394017219543
Epoch 860, training loss: 835.4527587890625 = 0.39100462198257446 + 100.0 * 8.350617408752441
Epoch 860, val loss: 0.45564427971839905
Epoch 870, training loss: 835.3001708984375 = 0.3897534906864166 + 100.0 * 8.349103927612305
Epoch 870, val loss: 0.4549652338027954
Epoch 880, training loss: 835.3942260742188 = 0.38852325081825256 + 100.0 * 8.350056648254395
Epoch 880, val loss: 0.4544290006160736
Epoch 890, training loss: 836.6243286132812 = 0.387285977602005 + 100.0 * 8.362370491027832
Epoch 890, val loss: 0.45392513275146484
Epoch 900, training loss: 835.3079833984375 = 0.3860182464122772 + 100.0 * 8.34921932220459
Epoch 900, val loss: 0.45347410440444946
Epoch 910, training loss: 834.9715576171875 = 0.3848473131656647 + 100.0 * 8.345867156982422
Epoch 910, val loss: 0.45294123888015747
Epoch 920, training loss: 834.8428344726562 = 0.3837164044380188 + 100.0 * 8.34459114074707
Epoch 920, val loss: 0.4524184763431549
Epoch 930, training loss: 834.7659301757812 = 0.38261085748672485 + 100.0 * 8.343832969665527
Epoch 930, val loss: 0.4519481062889099
Epoch 940, training loss: 834.6846313476562 = 0.3815174102783203 + 100.0 * 8.34303092956543
Epoch 940, val loss: 0.45148831605911255
Epoch 950, training loss: 834.6010131835938 = 0.3804330825805664 + 100.0 * 8.342206001281738
Epoch 950, val loss: 0.4510217308998108
Epoch 960, training loss: 834.5601806640625 = 0.37935835123062134 + 100.0 * 8.341808319091797
Epoch 960, val loss: 0.4505573809146881
Epoch 970, training loss: 835.911865234375 = 0.3782603442668915 + 100.0 * 8.35533618927002
Epoch 970, val loss: 0.4501500427722931
Epoch 980, training loss: 834.9091186523438 = 0.37711378931999207 + 100.0 * 8.345319747924805
Epoch 980, val loss: 0.44958704710006714
Epoch 990, training loss: 834.450439453125 = 0.3760654330253601 + 100.0 * 8.340744018554688
Epoch 990, val loss: 0.44908785820007324
Epoch 1000, training loss: 834.287841796875 = 0.3750472962856293 + 100.0 * 8.339127540588379
Epoch 1000, val loss: 0.4487231373786926
Epoch 1010, training loss: 834.213134765625 = 0.37404513359069824 + 100.0 * 8.338391304016113
Epoch 1010, val loss: 0.4482513666152954
Epoch 1020, training loss: 834.142822265625 = 0.37305426597595215 + 100.0 * 8.337697982788086
Epoch 1020, val loss: 0.44782236218452454
Epoch 1030, training loss: 834.0980224609375 = 0.37206172943115234 + 100.0 * 8.337259292602539
Epoch 1030, val loss: 0.4473935067653656
Epoch 1040, training loss: 834.9844360351562 = 0.3710660934448242 + 100.0 * 8.3461332321167
Epoch 1040, val loss: 0.44684094190597534
Epoch 1050, training loss: 834.6602783203125 = 0.3700313866138458 + 100.0 * 8.342902183532715
Epoch 1050, val loss: 0.446569561958313
Epoch 1060, training loss: 833.9816284179688 = 0.3690106272697449 + 100.0 * 8.336126327514648
Epoch 1060, val loss: 0.44608402252197266
Epoch 1070, training loss: 833.912109375 = 0.36803239583969116 + 100.0 * 8.335440635681152
Epoch 1070, val loss: 0.4455987215042114
Epoch 1080, training loss: 833.8229370117188 = 0.3670743405818939 + 100.0 * 8.334558486938477
Epoch 1080, val loss: 0.44517290592193604
Epoch 1090, training loss: 834.1365356445312 = 0.36612194776535034 + 100.0 * 8.337703704833984
Epoch 1090, val loss: 0.44488105177879333
Epoch 1100, training loss: 833.82421875 = 0.3651425838470459 + 100.0 * 8.334590911865234
Epoch 1100, val loss: 0.44419944286346436
Epoch 1110, training loss: 833.7462158203125 = 0.36418452858924866 + 100.0 * 8.333820343017578
Epoch 1110, val loss: 0.4439237713813782
Epoch 1120, training loss: 833.5737915039062 = 0.36324816942214966 + 100.0 * 8.33210563659668
Epoch 1120, val loss: 0.4434179961681366
Epoch 1130, training loss: 833.5796508789062 = 0.36231693625450134 + 100.0 * 8.332173347473145
Epoch 1130, val loss: 0.44299012422561646
Epoch 1140, training loss: 834.3639526367188 = 0.3613680601119995 + 100.0 * 8.340025901794434
Epoch 1140, val loss: 0.4426173269748688
Epoch 1150, training loss: 833.547607421875 = 0.3603978455066681 + 100.0 * 8.33187198638916
Epoch 1150, val loss: 0.44206321239471436
Epoch 1160, training loss: 833.4212036132812 = 0.3594738841056824 + 100.0 * 8.33061695098877
Epoch 1160, val loss: 0.4415983557701111
Epoch 1170, training loss: 833.3335571289062 = 0.35855981707572937 + 100.0 * 8.329750061035156
Epoch 1170, val loss: 0.441220760345459
Epoch 1180, training loss: 833.397216796875 = 0.3576575219631195 + 100.0 * 8.330395698547363
Epoch 1180, val loss: 0.4407724440097809
Epoch 1190, training loss: 833.3711547851562 = 0.35672828555107117 + 100.0 * 8.330143928527832
Epoch 1190, val loss: 0.44032537937164307
Epoch 1200, training loss: 833.1831665039062 = 0.35580042004585266 + 100.0 * 8.32827377319336
Epoch 1200, val loss: 0.43988797068595886
Epoch 1210, training loss: 833.1638793945312 = 0.35489195585250854 + 100.0 * 8.328089714050293
Epoch 1210, val loss: 0.43945494294166565
Epoch 1220, training loss: 833.8236083984375 = 0.35397639870643616 + 100.0 * 8.334695816040039
Epoch 1220, val loss: 0.4390254318714142
Epoch 1230, training loss: 833.2122802734375 = 0.3530486524105072 + 100.0 * 8.328592300415039
Epoch 1230, val loss: 0.4385829269886017
Epoch 1240, training loss: 833.0093383789062 = 0.3521488904953003 + 100.0 * 8.326571464538574
Epoch 1240, val loss: 0.4381808042526245
Epoch 1250, training loss: 832.906005859375 = 0.351255863904953 + 100.0 * 8.325547218322754
Epoch 1250, val loss: 0.4377591013908386
Epoch 1260, training loss: 832.9639282226562 = 0.3503674864768982 + 100.0 * 8.326135635375977
Epoch 1260, val loss: 0.4372909963130951
Epoch 1270, training loss: 833.1103515625 = 0.3494560718536377 + 100.0 * 8.327609062194824
Epoch 1270, val loss: 0.43690839409828186
Epoch 1280, training loss: 832.8513793945312 = 0.34853383898735046 + 100.0 * 8.325028419494629
Epoch 1280, val loss: 0.43659186363220215
Epoch 1290, training loss: 832.8992309570312 = 0.3476318418979645 + 100.0 * 8.325515747070312
Epoch 1290, val loss: 0.43610528111457825
Epoch 1300, training loss: 833.1356811523438 = 0.34671154618263245 + 100.0 * 8.327889442443848
Epoch 1300, val loss: 0.435747355222702
Epoch 1310, training loss: 832.6766357421875 = 0.3457774519920349 + 100.0 * 8.323308944702148
Epoch 1310, val loss: 0.43530696630477905
Epoch 1320, training loss: 832.6500244140625 = 0.3448766767978668 + 100.0 * 8.323051452636719
Epoch 1320, val loss: 0.4348735809326172
Epoch 1330, training loss: 832.622314453125 = 0.3439832329750061 + 100.0 * 8.322783470153809
Epoch 1330, val loss: 0.43449264764785767
Epoch 1340, training loss: 832.748046875 = 0.34309375286102295 + 100.0 * 8.324049949645996
Epoch 1340, val loss: 0.43410420417785645
Epoch 1350, training loss: 832.7970581054688 = 0.3421851694583893 + 100.0 * 8.324548721313477
Epoch 1350, val loss: 0.43371933698654175
Epoch 1360, training loss: 832.6179809570312 = 0.3412768244743347 + 100.0 * 8.32276725769043
Epoch 1360, val loss: 0.43332022428512573
Epoch 1370, training loss: 832.4873046875 = 0.34038347005844116 + 100.0 * 8.3214693069458
Epoch 1370, val loss: 0.43298768997192383
Epoch 1380, training loss: 832.4296875 = 0.33949750661849976 + 100.0 * 8.320901870727539
Epoch 1380, val loss: 0.4326373040676117
Epoch 1390, training loss: 832.7981567382812 = 0.3386085033416748 + 100.0 * 8.32459545135498
Epoch 1390, val loss: 0.4322962760925293
Epoch 1400, training loss: 832.562744140625 = 0.33770284056663513 + 100.0 * 8.322250366210938
Epoch 1400, val loss: 0.43187034130096436
Epoch 1410, training loss: 832.499267578125 = 0.33679622411727905 + 100.0 * 8.321624755859375
Epoch 1410, val loss: 0.431498259305954
Epoch 1420, training loss: 832.3489379882812 = 0.3358892500400543 + 100.0 * 8.320130348205566
Epoch 1420, val loss: 0.4312601387500763
Epoch 1430, training loss: 832.2734985351562 = 0.33500224351882935 + 100.0 * 8.319384574890137
Epoch 1430, val loss: 0.4307982325553894
Epoch 1440, training loss: 832.2177124023438 = 0.3341082036495209 + 100.0 * 8.318836212158203
Epoch 1440, val loss: 0.43051040172576904
Epoch 1450, training loss: 832.6630249023438 = 0.33321380615234375 + 100.0 * 8.323298454284668
Epoch 1450, val loss: 0.43022066354751587
Epoch 1460, training loss: 832.3447265625 = 0.3322875201702118 + 100.0 * 8.320124626159668
Epoch 1460, val loss: 0.429822713136673
Epoch 1470, training loss: 832.3054809570312 = 0.3313766419887543 + 100.0 * 8.319741249084473
Epoch 1470, val loss: 0.4295124113559723
Epoch 1480, training loss: 832.4808349609375 = 0.330449640750885 + 100.0 * 8.321503639221191
Epoch 1480, val loss: 0.4291277229785919
Epoch 1490, training loss: 832.1647338867188 = 0.3295237123966217 + 100.0 * 8.318351745605469
Epoch 1490, val loss: 0.42888814210891724
Epoch 1500, training loss: 832.0345458984375 = 0.3286178708076477 + 100.0 * 8.317059516906738
Epoch 1500, val loss: 0.42857784032821655
Epoch 1510, training loss: 831.9551391601562 = 0.3277190923690796 + 100.0 * 8.316274642944336
Epoch 1510, val loss: 0.42823708057403564
Epoch 1520, training loss: 831.9196166992188 = 0.3268203139305115 + 100.0 * 8.315927505493164
Epoch 1520, val loss: 0.42798975110054016
Epoch 1530, training loss: 832.2620849609375 = 0.32591512799263 + 100.0 * 8.319361686706543
Epoch 1530, val loss: 0.42775848507881165
Epoch 1540, training loss: 832.0571899414062 = 0.3249867558479309 + 100.0 * 8.31732177734375
Epoch 1540, val loss: 0.42728492617607117
Epoch 1550, training loss: 832.0576171875 = 0.3240460455417633 + 100.0 * 8.317336082458496
Epoch 1550, val loss: 0.427156001329422
Epoch 1560, training loss: 831.864013671875 = 0.3231216073036194 + 100.0 * 8.315408706665039
Epoch 1560, val loss: 0.426875501871109
Epoch 1570, training loss: 831.9660034179688 = 0.3222081661224365 + 100.0 * 8.316437721252441
Epoch 1570, val loss: 0.4265696108341217
Epoch 1580, training loss: 832.14404296875 = 0.32126861810684204 + 100.0 * 8.318227767944336
Epoch 1580, val loss: 0.4263343811035156
Epoch 1590, training loss: 831.718505859375 = 0.32031694054603577 + 100.0 * 8.313982009887695
Epoch 1590, val loss: 0.4260348379611969
Epoch 1600, training loss: 831.7045288085938 = 0.3193991482257843 + 100.0 * 8.313851356506348
Epoch 1600, val loss: 0.4257854223251343
Epoch 1610, training loss: 831.6766967773438 = 0.31848666071891785 + 100.0 * 8.313582420349121
Epoch 1610, val loss: 0.4255566895008087
Epoch 1620, training loss: 831.7286376953125 = 0.31757813692092896 + 100.0 * 8.31411075592041
Epoch 1620, val loss: 0.42527252435684204
Epoch 1630, training loss: 832.23583984375 = 0.3166445791721344 + 100.0 * 8.319191932678223
Epoch 1630, val loss: 0.42505693435668945
Epoch 1640, training loss: 831.7938842773438 = 0.3156842291355133 + 100.0 * 8.31478214263916
Epoch 1640, val loss: 0.42490872740745544
Epoch 1650, training loss: 831.632080078125 = 0.3147420883178711 + 100.0 * 8.313173294067383
Epoch 1650, val loss: 0.4245977997779846
Epoch 1660, training loss: 831.6209106445312 = 0.3138066530227661 + 100.0 * 8.313071250915527
Epoch 1660, val loss: 0.42440304160118103
Epoch 1670, training loss: 831.6859741210938 = 0.3128683269023895 + 100.0 * 8.31373119354248
Epoch 1670, val loss: 0.42424276471138
Epoch 1680, training loss: 831.7986450195312 = 0.31192293763160706 + 100.0 * 8.31486701965332
Epoch 1680, val loss: 0.4241129755973816
Epoch 1690, training loss: 831.4400634765625 = 0.31097763776779175 + 100.0 * 8.311290740966797
Epoch 1690, val loss: 0.42384862899780273
Epoch 1700, training loss: 831.5154418945312 = 0.3100475072860718 + 100.0 * 8.312053680419922
Epoch 1700, val loss: 0.42367392778396606
Epoch 1710, training loss: 831.8331909179688 = 0.3091084659099579 + 100.0 * 8.315240859985352
Epoch 1710, val loss: 0.42358458042144775
Epoch 1720, training loss: 831.4808349609375 = 0.3081579804420471 + 100.0 * 8.311726570129395
Epoch 1720, val loss: 0.42332151532173157
Epoch 1730, training loss: 831.66845703125 = 0.30720260739326477 + 100.0 * 8.313612937927246
Epoch 1730, val loss: 0.4232334792613983
Epoch 1740, training loss: 831.3143920898438 = 0.3062414228916168 + 100.0 * 8.310081481933594
Epoch 1740, val loss: 0.42309266328811646
Epoch 1750, training loss: 831.3001098632812 = 0.3052929937839508 + 100.0 * 8.309947967529297
Epoch 1750, val loss: 0.42298033833503723
Epoch 1760, training loss: 831.3089599609375 = 0.30434951186180115 + 100.0 * 8.310046195983887
Epoch 1760, val loss: 0.4228017032146454
Epoch 1770, training loss: 831.7992553710938 = 0.30339616537094116 + 100.0 * 8.314958572387695
Epoch 1770, val loss: 0.42273539304733276
Epoch 1780, training loss: 831.6517944335938 = 0.302418977022171 + 100.0 * 8.313493728637695
Epoch 1780, val loss: 0.42232462763786316
Epoch 1790, training loss: 831.5230102539062 = 0.30142828822135925 + 100.0 * 8.312215805053711
Epoch 1790, val loss: 0.4223734438419342
Epoch 1800, training loss: 831.244140625 = 0.3004462718963623 + 100.0 * 8.309436798095703
Epoch 1800, val loss: 0.4221917390823364
Epoch 1810, training loss: 831.132568359375 = 0.2994852662086487 + 100.0 * 8.308330535888672
Epoch 1810, val loss: 0.42196428775787354
Epoch 1820, training loss: 831.0860595703125 = 0.2985239326953888 + 100.0 * 8.307875633239746
Epoch 1820, val loss: 0.42200595140457153
Epoch 1830, training loss: 831.2501220703125 = 0.29756301641464233 + 100.0 * 8.309525489807129
Epoch 1830, val loss: 0.4219737946987152
Epoch 1840, training loss: 831.2427368164062 = 0.2965810298919678 + 100.0 * 8.30946159362793
Epoch 1840, val loss: 0.42165297269821167
Epoch 1850, training loss: 831.3160400390625 = 0.2955940067768097 + 100.0 * 8.31020450592041
Epoch 1850, val loss: 0.421578049659729
Epoch 1860, training loss: 831.0991821289062 = 0.29458388686180115 + 100.0 * 8.308046340942383
Epoch 1860, val loss: 0.42145365476608276
Epoch 1870, training loss: 831.0344848632812 = 0.2935955226421356 + 100.0 * 8.307409286499023
Epoch 1870, val loss: 0.4214939773082733
Epoch 1880, training loss: 830.9367065429688 = 0.2926180362701416 + 100.0 * 8.306441307067871
Epoch 1880, val loss: 0.42131343483924866
Epoch 1890, training loss: 830.9284057617188 = 0.29164260625839233 + 100.0 * 8.306367874145508
Epoch 1890, val loss: 0.42132461071014404
Epoch 1900, training loss: 831.5596923828125 = 0.29066789150238037 + 100.0 * 8.312690734863281
Epoch 1900, val loss: 0.4211426377296448
Epoch 1910, training loss: 831.1271362304688 = 0.28963711857795715 + 100.0 * 8.308375358581543
Epoch 1910, val loss: 0.4212898910045624
Epoch 1920, training loss: 831.14306640625 = 0.2886326014995575 + 100.0 * 8.308544158935547
Epoch 1920, val loss: 0.4212057590484619
Epoch 1930, training loss: 831.0457763671875 = 0.28760871291160583 + 100.0 * 8.307581901550293
Epoch 1930, val loss: 0.42117801308631897
Epoch 1940, training loss: 830.822509765625 = 0.2865998446941376 + 100.0 * 8.30535888671875
Epoch 1940, val loss: 0.421102911233902
Epoch 1950, training loss: 830.7789306640625 = 0.2855909466743469 + 100.0 * 8.304933547973633
Epoch 1950, val loss: 0.421121746301651
Epoch 1960, training loss: 831.004638671875 = 0.2845829725265503 + 100.0 * 8.30720043182373
Epoch 1960, val loss: 0.4210987985134125
Epoch 1970, training loss: 830.8104858398438 = 0.2835514545440674 + 100.0 * 8.305269241333008
Epoch 1970, val loss: 0.4210776388645172
Epoch 1980, training loss: 830.8209228515625 = 0.28252139687538147 + 100.0 * 8.305383682250977
Epoch 1980, val loss: 0.42102116346359253
Epoch 1990, training loss: 830.90625 = 0.2814767360687256 + 100.0 * 8.30624771118164
Epoch 1990, val loss: 0.4210900366306305
Epoch 2000, training loss: 830.8331298828125 = 0.2804363965988159 + 100.0 * 8.305526733398438
Epoch 2000, val loss: 0.4209862947463989
Epoch 2010, training loss: 830.5908203125 = 0.2793926000595093 + 100.0 * 8.30311393737793
Epoch 2010, val loss: 0.42104220390319824
Epoch 2020, training loss: 830.6112060546875 = 0.2783586382865906 + 100.0 * 8.303328514099121
Epoch 2020, val loss: 0.42098307609558105
Epoch 2030, training loss: 830.8357543945312 = 0.27731865644454956 + 100.0 * 8.305583953857422
Epoch 2030, val loss: 0.42101597785949707
Epoch 2040, training loss: 830.8806762695312 = 0.276256263256073 + 100.0 * 8.306044578552246
Epoch 2040, val loss: 0.42104822397232056
Epoch 2050, training loss: 830.9048461914062 = 0.2751798927783966 + 100.0 * 8.306296348571777
Epoch 2050, val loss: 0.4210127592086792
Epoch 2060, training loss: 830.5501098632812 = 0.2740950882434845 + 100.0 * 8.302760124206543
Epoch 2060, val loss: 0.4209435284137726
Epoch 2070, training loss: 830.4764404296875 = 0.2730295956134796 + 100.0 * 8.302034378051758
Epoch 2070, val loss: 0.42103108763694763
Epoch 2080, training loss: 830.4312744140625 = 0.2719571590423584 + 100.0 * 8.301592826843262
Epoch 2080, val loss: 0.4210217297077179
Epoch 2090, training loss: 830.65283203125 = 0.2708795368671417 + 100.0 * 8.30381965637207
Epoch 2090, val loss: 0.42104777693748474
Epoch 2100, training loss: 830.4969482421875 = 0.26977139711380005 + 100.0 * 8.302271842956543
Epoch 2100, val loss: 0.42095747590065
Epoch 2110, training loss: 830.3671264648438 = 0.2686532735824585 + 100.0 * 8.300984382629395
Epoch 2110, val loss: 0.42116308212280273
Epoch 2120, training loss: 830.4345092773438 = 0.2675466537475586 + 100.0 * 8.30167007446289
Epoch 2120, val loss: 0.4211007356643677
Epoch 2130, training loss: 831.238037109375 = 0.26642706990242004 + 100.0 * 8.30971622467041
Epoch 2130, val loss: 0.4211554229259491
Epoch 2140, training loss: 830.6102905273438 = 0.2652791738510132 + 100.0 * 8.303450584411621
Epoch 2140, val loss: 0.4210706055164337
Epoch 2150, training loss: 830.3600463867188 = 0.2641444504261017 + 100.0 * 8.300958633422852
Epoch 2150, val loss: 0.42120617628097534
Epoch 2160, training loss: 830.2457885742188 = 0.26301881670951843 + 100.0 * 8.299827575683594
Epoch 2160, val loss: 0.42121708393096924
Epoch 2170, training loss: 830.240478515625 = 0.2618900239467621 + 100.0 * 8.299785614013672
Epoch 2170, val loss: 0.4213058054447174
Epoch 2180, training loss: 830.857177734375 = 0.2607516348361969 + 100.0 * 8.305964469909668
Epoch 2180, val loss: 0.421382337808609
Epoch 2190, training loss: 830.454345703125 = 0.2595821022987366 + 100.0 * 8.301947593688965
Epoch 2190, val loss: 0.42119988799095154
Epoch 2200, training loss: 830.29638671875 = 0.2584080696105957 + 100.0 * 8.300379753112793
Epoch 2200, val loss: 0.42132002115249634
Epoch 2210, training loss: 830.323486328125 = 0.25724270939826965 + 100.0 * 8.30066204071045
Epoch 2210, val loss: 0.4212067425251007
Epoch 2220, training loss: 830.4874267578125 = 0.2560485601425171 + 100.0 * 8.302313804626465
Epoch 2220, val loss: 0.4214702546596527
Epoch 2230, training loss: 830.2905883789062 = 0.2548530101776123 + 100.0 * 8.3003568649292
Epoch 2230, val loss: 0.4215109348297119
Epoch 2240, training loss: 830.08837890625 = 0.25366130471229553 + 100.0 * 8.298347473144531
Epoch 2240, val loss: 0.4214746356010437
Epoch 2250, training loss: 830.1149291992188 = 0.2524777352809906 + 100.0 * 8.298624038696289
Epoch 2250, val loss: 0.42162707448005676
Epoch 2260, training loss: 830.524169921875 = 0.2512930631637573 + 100.0 * 8.302728652954102
Epoch 2260, val loss: 0.42178452014923096
Epoch 2270, training loss: 830.0391235351562 = 0.25007855892181396 + 100.0 * 8.297890663146973
Epoch 2270, val loss: 0.4216523766517639
Epoch 2280, training loss: 830.1558227539062 = 0.24887213110923767 + 100.0 * 8.29906940460205
Epoch 2280, val loss: 0.42187944054603577
Epoch 2290, training loss: 830.3574829101562 = 0.24765688180923462 + 100.0 * 8.301097869873047
Epoch 2290, val loss: 0.4220130145549774
Epoch 2300, training loss: 830.1233520507812 = 0.24644233286380768 + 100.0 * 8.298768997192383
Epoch 2300, val loss: 0.4221014678478241
Epoch 2310, training loss: 830.0427856445312 = 0.24522368609905243 + 100.0 * 8.297975540161133
Epoch 2310, val loss: 0.4221748411655426
Epoch 2320, training loss: 829.989501953125 = 0.24401415884494781 + 100.0 * 8.297454833984375
Epoch 2320, val loss: 0.42222869396209717
Epoch 2330, training loss: 829.9496459960938 = 0.24279458820819855 + 100.0 * 8.29706859588623
Epoch 2330, val loss: 0.4224594235420227
Epoch 2340, training loss: 830.1367797851562 = 0.24157099425792694 + 100.0 * 8.298952102661133
Epoch 2340, val loss: 0.4226821959018707
Epoch 2350, training loss: 830.4003295898438 = 0.2403222769498825 + 100.0 * 8.301600456237793
Epoch 2350, val loss: 0.42304763197898865
Epoch 2360, training loss: 830.0079345703125 = 0.239070326089859 + 100.0 * 8.297688484191895
Epoch 2360, val loss: 0.4227713346481323
Epoch 2370, training loss: 829.8645629882812 = 0.2378045618534088 + 100.0 * 8.29626750946045
Epoch 2370, val loss: 0.4232001304626465
Epoch 2380, training loss: 829.8313598632812 = 0.2365572303533554 + 100.0 * 8.295948028564453
Epoch 2380, val loss: 0.4232454299926758
Epoch 2390, training loss: 830.1101684570312 = 0.23529981076717377 + 100.0 * 8.298748970031738
Epoch 2390, val loss: 0.4234490990638733
Epoch 2400, training loss: 830.0135498046875 = 0.23402129113674164 + 100.0 * 8.297795295715332
Epoch 2400, val loss: 0.42349615693092346
Epoch 2410, training loss: 829.8080444335938 = 0.2327290177345276 + 100.0 * 8.295753479003906
Epoch 2410, val loss: 0.4237804710865021
Epoch 2420, training loss: 829.7849731445312 = 0.23145265877246857 + 100.0 * 8.29553508758545
Epoch 2420, val loss: 0.4238705039024353
Epoch 2430, training loss: 829.7332153320312 = 0.2301689088344574 + 100.0 * 8.29503059387207
Epoch 2430, val loss: 0.424254834651947
Epoch 2440, training loss: 829.6822509765625 = 0.22888658940792084 + 100.0 * 8.294533729553223
Epoch 2440, val loss: 0.42439591884613037
Epoch 2450, training loss: 829.8478393554688 = 0.22760970890522003 + 100.0 * 8.296202659606934
Epoch 2450, val loss: 0.4244697093963623
Epoch 2460, training loss: 830.3200073242188 = 0.22630755603313446 + 100.0 * 8.300936698913574
Epoch 2460, val loss: 0.42496955394744873
Epoch 2470, training loss: 829.7139892578125 = 0.22498463094234467 + 100.0 * 8.294890403747559
Epoch 2470, val loss: 0.4248470366001129
Epoch 2480, training loss: 829.71826171875 = 0.22366467118263245 + 100.0 * 8.29494571685791
Epoch 2480, val loss: 0.42545321583747864
Epoch 2490, training loss: 829.8364868164062 = 0.22237488627433777 + 100.0 * 8.296141624450684
Epoch 2490, val loss: 0.42530569434165955
Epoch 2500, training loss: 829.6023559570312 = 0.22105269134044647 + 100.0 * 8.29381275177002
Epoch 2500, val loss: 0.4258359372615814
Epoch 2510, training loss: 829.5712280273438 = 0.21975643932819366 + 100.0 * 8.2935152053833
Epoch 2510, val loss: 0.42601659893989563
Epoch 2520, training loss: 829.5907592773438 = 0.2184445559978485 + 100.0 * 8.293723106384277
Epoch 2520, val loss: 0.42643851041793823
Epoch 2530, training loss: 829.8436279296875 = 0.21713687479496002 + 100.0 * 8.2962646484375
Epoch 2530, val loss: 0.4265584349632263
Epoch 2540, training loss: 829.88720703125 = 0.21579831838607788 + 100.0 * 8.296713829040527
Epoch 2540, val loss: 0.4268367290496826
Epoch 2550, training loss: 829.5498657226562 = 0.21445807814598083 + 100.0 * 8.293354034423828
Epoch 2550, val loss: 0.42706042528152466
Epoch 2560, training loss: 829.5407104492188 = 0.21312634646892548 + 100.0 * 8.293275833129883
Epoch 2560, val loss: 0.42741549015045166
Epoch 2570, training loss: 829.6812133789062 = 0.21179774403572083 + 100.0 * 8.294693946838379
Epoch 2570, val loss: 0.4276701509952545
Epoch 2580, training loss: 829.5697021484375 = 0.21045294404029846 + 100.0 * 8.29359245300293
Epoch 2580, val loss: 0.4279053807258606
Epoch 2590, training loss: 829.5481567382812 = 0.2091144323348999 + 100.0 * 8.293390274047852
Epoch 2590, val loss: 0.42831191420555115
Epoch 2600, training loss: 829.6321411132812 = 0.2077712118625641 + 100.0 * 8.294243812561035
Epoch 2600, val loss: 0.42861488461494446
Epoch 2610, training loss: 829.515380859375 = 0.20643776655197144 + 100.0 * 8.293089866638184
Epoch 2610, val loss: 0.428744912147522
Epoch 2620, training loss: 829.655517578125 = 0.20509901642799377 + 100.0 * 8.294504165649414
Epoch 2620, val loss: 0.4291972815990448
Epoch 2630, training loss: 829.739990234375 = 0.20375323295593262 + 100.0 * 8.29536247253418
Epoch 2630, val loss: 0.4293805956840515
Epoch 2640, training loss: 829.369384765625 = 0.20239245891571045 + 100.0 * 8.291669845581055
Epoch 2640, val loss: 0.43002983927726746
Epoch 2650, training loss: 829.363525390625 = 0.20105038583278656 + 100.0 * 8.291625022888184
Epoch 2650, val loss: 0.4303123652935028
Epoch 2660, training loss: 829.3258056640625 = 0.1997116059064865 + 100.0 * 8.291260719299316
Epoch 2660, val loss: 0.4306132197380066
Epoch 2670, training loss: 829.570556640625 = 0.19837619364261627 + 100.0 * 8.293722152709961
Epoch 2670, val loss: 0.4309678077697754
Epoch 2680, training loss: 829.464599609375 = 0.19701480865478516 + 100.0 * 8.292675971984863
Epoch 2680, val loss: 0.43142884969711304
Epoch 2690, training loss: 829.3301391601562 = 0.1956562101840973 + 100.0 * 8.29134464263916
Epoch 2690, val loss: 0.4319259822368622
Epoch 2700, training loss: 829.3307495117188 = 0.19430525600910187 + 100.0 * 8.291364669799805
Epoch 2700, val loss: 0.4322431683540344
Epoch 2710, training loss: 829.4269409179688 = 0.19295763969421387 + 100.0 * 8.292340278625488
Epoch 2710, val loss: 0.43255797028541565
Epoch 2720, training loss: 829.579833984375 = 0.1915954351425171 + 100.0 * 8.293882369995117
Epoch 2720, val loss: 0.4331050217151642
Epoch 2730, training loss: 829.4713134765625 = 0.19022871553897858 + 100.0 * 8.292810440063477
Epoch 2730, val loss: 0.43357935547828674
Epoch 2740, training loss: 829.4068603515625 = 0.18886925280094147 + 100.0 * 8.292180061340332
Epoch 2740, val loss: 0.4340156614780426
Epoch 2750, training loss: 829.19677734375 = 0.18750688433647156 + 100.0 * 8.290092468261719
Epoch 2750, val loss: 0.4345470666885376
Epoch 2760, training loss: 829.1844482421875 = 0.18615366518497467 + 100.0 * 8.289982795715332
Epoch 2760, val loss: 0.43503305315971375
Epoch 2770, training loss: 829.2997436523438 = 0.1848050057888031 + 100.0 * 8.291149139404297
Epoch 2770, val loss: 0.43565282225608826
Epoch 2780, training loss: 829.3375854492188 = 0.18345172703266144 + 100.0 * 8.29154109954834
Epoch 2780, val loss: 0.43606293201446533
Epoch 2790, training loss: 829.5126953125 = 0.18210072815418243 + 100.0 * 8.293305397033691
Epoch 2790, val loss: 0.43641528487205505
Epoch 2800, training loss: 829.1644287109375 = 0.18073026835918427 + 100.0 * 8.289836883544922
Epoch 2800, val loss: 0.4371510148048401
Epoch 2810, training loss: 829.0778198242188 = 0.17936883866786957 + 100.0 * 8.288984298706055
Epoch 2810, val loss: 0.43761566281318665
Epoch 2820, training loss: 829.0371704101562 = 0.17801307141780853 + 100.0 * 8.288591384887695
Epoch 2820, val loss: 0.43820491433143616
Epoch 2830, training loss: 829.5679931640625 = 0.17667819559574127 + 100.0 * 8.293912887573242
Epoch 2830, val loss: 0.4389219880104065
Epoch 2840, training loss: 829.0982055664062 = 0.1753067970275879 + 100.0 * 8.289229393005371
Epoch 2840, val loss: 0.43926844000816345
Epoch 2850, training loss: 829.0252685546875 = 0.17394474148750305 + 100.0 * 8.28851318359375
Epoch 2850, val loss: 0.4400661885738373
Epoch 2860, training loss: 829.0175170898438 = 0.17259648442268372 + 100.0 * 8.28844928741455
Epoch 2860, val loss: 0.44074389338493347
Epoch 2870, training loss: 829.322021484375 = 0.17126931250095367 + 100.0 * 8.291507720947266
Epoch 2870, val loss: 0.44159117341041565
Epoch 2880, training loss: 829.3623657226562 = 0.16993072628974915 + 100.0 * 8.291924476623535
Epoch 2880, val loss: 0.4418068826198578
Epoch 2890, training loss: 828.9619140625 = 0.1685754805803299 + 100.0 * 8.287933349609375
Epoch 2890, val loss: 0.44254040718078613
Epoch 2900, training loss: 828.9661865234375 = 0.16723740100860596 + 100.0 * 8.287989616394043
Epoch 2900, val loss: 0.4432018995285034
Epoch 2910, training loss: 828.8720703125 = 0.16590768098831177 + 100.0 * 8.28706169128418
Epoch 2910, val loss: 0.4438742697238922
Epoch 2920, training loss: 828.9567260742188 = 0.16459208726882935 + 100.0 * 8.287920951843262
Epoch 2920, val loss: 0.444570392370224
Epoch 2930, training loss: 829.3987426757812 = 0.16327784955501556 + 100.0 * 8.292354583740234
Epoch 2930, val loss: 0.44531121850013733
Epoch 2940, training loss: 829.0242919921875 = 0.16194191575050354 + 100.0 * 8.288623809814453
Epoch 2940, val loss: 0.44603100419044495
Epoch 2950, training loss: 828.8573608398438 = 0.16061528027057648 + 100.0 * 8.286967277526855
Epoch 2950, val loss: 0.4467318058013916
Epoch 2960, training loss: 828.869873046875 = 0.15930785238742828 + 100.0 * 8.287105560302734
Epoch 2960, val loss: 0.4474489986896515
Epoch 2970, training loss: 829.9896850585938 = 0.15801763534545898 + 100.0 * 8.298316955566406
Epoch 2970, val loss: 0.44824060797691345
Epoch 2980, training loss: 829.1360473632812 = 0.15667635202407837 + 100.0 * 8.289793968200684
Epoch 2980, val loss: 0.4491145610809326
Epoch 2990, training loss: 828.8439331054688 = 0.15534855425357819 + 100.0 * 8.286886215209961
Epoch 2990, val loss: 0.4497051239013672
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8396752917300863
0.8667680938926321
=== training gcn model ===
Epoch 0, training loss: 1059.3353271484375 = 1.1102546453475952 + 100.0 * 10.58225154876709
Epoch 0, val loss: 1.1110897064208984
Epoch 10, training loss: 1059.204345703125 = 1.1030867099761963 + 100.0 * 10.581013679504395
Epoch 10, val loss: 1.103811502456665
Epoch 20, training loss: 1058.088623046875 = 1.0945160388946533 + 100.0 * 10.569941520690918
Epoch 20, val loss: 1.0950112342834473
Epoch 30, training loss: 1051.0634765625 = 1.08372163772583 + 100.0 * 10.499796867370605
Epoch 30, val loss: 1.0837996006011963
Epoch 40, training loss: 1026.876220703125 = 1.0707428455352783 + 100.0 * 10.258054733276367
Epoch 40, val loss: 1.0705575942993164
Epoch 50, training loss: 985.970458984375 = 1.0569486618041992 + 100.0 * 9.849135398864746
Epoch 50, val loss: 1.0564277172088623
Epoch 60, training loss: 948.9610595703125 = 1.0459892749786377 + 100.0 * 9.479150772094727
Epoch 60, val loss: 1.0455726385116577
Epoch 70, training loss: 926.48583984375 = 1.035190463066101 + 100.0 * 9.25450611114502
Epoch 70, val loss: 1.0351618528366089
Epoch 80, training loss: 920.9299926757812 = 1.025006651878357 + 100.0 * 9.199049949645996
Epoch 80, val loss: 1.025205373764038
Epoch 90, training loss: 918.1541748046875 = 1.0151571035385132 + 100.0 * 9.171390533447266
Epoch 90, val loss: 1.015554666519165
Epoch 100, training loss: 914.23681640625 = 1.0061066150665283 + 100.0 * 9.132307052612305
Epoch 100, val loss: 1.0066651105880737
Epoch 110, training loss: 909.1439819335938 = 0.9978923201560974 + 100.0 * 9.081460952758789
Epoch 110, val loss: 0.9985970854759216
Epoch 120, training loss: 901.873046875 = 0.9908097386360168 + 100.0 * 9.008822441101074
Epoch 120, val loss: 0.9918090105056763
Epoch 130, training loss: 891.7987060546875 = 0.9857118129730225 + 100.0 * 8.908129692077637
Epoch 130, val loss: 0.9871060252189636
Epoch 140, training loss: 883.7080078125 = 0.9814314246177673 + 100.0 * 8.827265739440918
Epoch 140, val loss: 0.9827261567115784
Epoch 150, training loss: 876.8124389648438 = 0.974393367767334 + 100.0 * 8.758380889892578
Epoch 150, val loss: 0.9754446148872375
Epoch 160, training loss: 872.1444702148438 = 0.9653853178024292 + 100.0 * 8.711791038513184
Epoch 160, val loss: 0.9663967490196228
Epoch 170, training loss: 868.6110229492188 = 0.9555885195732117 + 100.0 * 8.676554679870605
Epoch 170, val loss: 0.9569265246391296
Epoch 180, training loss: 866.2164306640625 = 0.9452430009841919 + 100.0 * 8.652711868286133
Epoch 180, val loss: 0.9468057751655579
Epoch 190, training loss: 864.30810546875 = 0.9334997534751892 + 100.0 * 8.633746147155762
Epoch 190, val loss: 0.9353445172309875
Epoch 200, training loss: 862.4429931640625 = 0.9207401871681213 + 100.0 * 8.615222930908203
Epoch 200, val loss: 0.9229143857955933
Epoch 210, training loss: 861.23291015625 = 0.9074330925941467 + 100.0 * 8.603255271911621
Epoch 210, val loss: 0.9100098013877869
Epoch 220, training loss: 859.8530883789062 = 0.8934856653213501 + 100.0 * 8.589595794677734
Epoch 220, val loss: 0.8964895009994507
Epoch 230, training loss: 858.5634155273438 = 0.8789827227592468 + 100.0 * 8.576844215393066
Epoch 230, val loss: 0.8825039863586426
Epoch 240, training loss: 857.186767578125 = 0.8642486333847046 + 100.0 * 8.563224792480469
Epoch 240, val loss: 0.8683531284332275
Epoch 250, training loss: 856.4009399414062 = 0.8492879271507263 + 100.0 * 8.555516242980957
Epoch 250, val loss: 0.8540264964103699
Epoch 260, training loss: 854.7188720703125 = 0.834079921245575 + 100.0 * 8.538847923278809
Epoch 260, val loss: 0.8393074870109558
Epoch 270, training loss: 853.60546875 = 0.8186541795730591 + 100.0 * 8.527868270874023
Epoch 270, val loss: 0.8245254755020142
Epoch 280, training loss: 853.1250610351562 = 0.802968442440033 + 100.0 * 8.523221015930176
Epoch 280, val loss: 0.8093957901000977
Epoch 290, training loss: 852.1891479492188 = 0.786974310874939 + 100.0 * 8.514021873474121
Epoch 290, val loss: 0.7940542101860046
Epoch 300, training loss: 851.4879150390625 = 0.7710707783699036 + 100.0 * 8.507168769836426
Epoch 300, val loss: 0.7788011431694031
Epoch 310, training loss: 850.7483520507812 = 0.7554603219032288 + 100.0 * 8.499929428100586
Epoch 310, val loss: 0.7638542056083679
Epoch 320, training loss: 850.4146118164062 = 0.7401373982429504 + 100.0 * 8.496745109558105
Epoch 320, val loss: 0.7490869760513306
Epoch 330, training loss: 849.5191040039062 = 0.7249047756195068 + 100.0 * 8.48794174194336
Epoch 330, val loss: 0.7346640825271606
Epoch 340, training loss: 848.62255859375 = 0.7100886106491089 + 100.0 * 8.479125022888184
Epoch 340, val loss: 0.7205976247787476
Epoch 350, training loss: 847.8920288085938 = 0.6956421136856079 + 100.0 * 8.471963882446289
Epoch 350, val loss: 0.7067956328392029
Epoch 360, training loss: 847.4855346679688 = 0.6814800500869751 + 100.0 * 8.468040466308594
Epoch 360, val loss: 0.6932749152183533
Epoch 370, training loss: 846.7814331054688 = 0.6674619317054749 + 100.0 * 8.461139678955078
Epoch 370, val loss: 0.6800661683082581
Epoch 380, training loss: 846.1956176757812 = 0.6539538502693176 + 100.0 * 8.455416679382324
Epoch 380, val loss: 0.6673292517662048
Epoch 390, training loss: 845.8010864257812 = 0.6409942507743835 + 100.0 * 8.451601028442383
Epoch 390, val loss: 0.6551510095596313
Epoch 400, training loss: 845.4736328125 = 0.6284303665161133 + 100.0 * 8.44845199584961
Epoch 400, val loss: 0.6432555317878723
Epoch 410, training loss: 844.7938232421875 = 0.6164426207542419 + 100.0 * 8.441773414611816
Epoch 410, val loss: 0.6321274042129517
Epoch 420, training loss: 844.288330078125 = 0.6051489114761353 + 100.0 * 8.4368314743042
Epoch 420, val loss: 0.6215607523918152
Epoch 430, training loss: 843.9443359375 = 0.5944441556930542 + 100.0 * 8.433499336242676
Epoch 430, val loss: 0.6115303635597229
Epoch 440, training loss: 843.572998046875 = 0.5842586159706116 + 100.0 * 8.429887771606445
Epoch 440, val loss: 0.6022045612335205
Epoch 450, training loss: 843.2813720703125 = 0.5747414231300354 + 100.0 * 8.4270658493042
Epoch 450, val loss: 0.5934640765190125
Epoch 460, training loss: 843.1535034179688 = 0.5658818483352661 + 100.0 * 8.42587661743164
Epoch 460, val loss: 0.5854185223579407
Epoch 470, training loss: 842.8355712890625 = 0.5574702024459839 + 100.0 * 8.422780990600586
Epoch 470, val loss: 0.5778111815452576
Epoch 480, training loss: 842.3499755859375 = 0.5497245788574219 + 100.0 * 8.418002128601074
Epoch 480, val loss: 0.5708841681480408
Epoch 490, training loss: 841.9874877929688 = 0.5425304770469666 + 100.0 * 8.414449691772461
Epoch 490, val loss: 0.5644363164901733
Epoch 500, training loss: 841.7393188476562 = 0.5358051657676697 + 100.0 * 8.41203498840332
Epoch 500, val loss: 0.5584756731987
Epoch 510, training loss: 841.5552368164062 = 0.5294073224067688 + 100.0 * 8.410258293151855
Epoch 510, val loss: 0.5530077815055847
Epoch 520, training loss: 841.30810546875 = 0.5234155654907227 + 100.0 * 8.407846450805664
Epoch 520, val loss: 0.5478674173355103
Epoch 530, training loss: 841.0673217773438 = 0.5179321765899658 + 100.0 * 8.40549373626709
Epoch 530, val loss: 0.5431148409843445
Epoch 540, training loss: 840.8394165039062 = 0.5128105878829956 + 100.0 * 8.403265953063965
Epoch 540, val loss: 0.53875333070755
Epoch 550, training loss: 840.8073120117188 = 0.5079727172851562 + 100.0 * 8.402993202209473
Epoch 550, val loss: 0.5347509384155273
Epoch 560, training loss: 840.7347412109375 = 0.5034105181694031 + 100.0 * 8.402313232421875
Epoch 560, val loss: 0.5310693979263306
Epoch 570, training loss: 840.2703247070312 = 0.4991670846939087 + 100.0 * 8.397711753845215
Epoch 570, val loss: 0.5275275111198425
Epoch 580, training loss: 840.082763671875 = 0.49522456526756287 + 100.0 * 8.395874977111816
Epoch 580, val loss: 0.5243144631385803
Epoch 590, training loss: 839.8953247070312 = 0.4915252923965454 + 100.0 * 8.394038200378418
Epoch 590, val loss: 0.5213994979858398
Epoch 600, training loss: 839.969970703125 = 0.48803386092185974 + 100.0 * 8.394819259643555
Epoch 600, val loss: 0.5186220407485962
Epoch 610, training loss: 839.62158203125 = 0.48470136523246765 + 100.0 * 8.391368865966797
Epoch 610, val loss: 0.516282856464386
Epoch 620, training loss: 839.7390747070312 = 0.48156341910362244 + 100.0 * 8.39257526397705
Epoch 620, val loss: 0.5139414668083191
Epoch 630, training loss: 839.4615478515625 = 0.4785388708114624 + 100.0 * 8.389830589294434
Epoch 630, val loss: 0.5114855170249939
Epoch 640, training loss: 839.0097045898438 = 0.4757175147533417 + 100.0 * 8.385339736938477
Epoch 640, val loss: 0.5094993710517883
Epoch 650, training loss: 838.8244018554688 = 0.47307199239730835 + 100.0 * 8.383513450622559
Epoch 650, val loss: 0.5075574517250061
Epoch 660, training loss: 838.8002319335938 = 0.4705486595630646 + 100.0 * 8.383296966552734
Epoch 660, val loss: 0.5058718919754028
Epoch 670, training loss: 838.70166015625 = 0.4680866599082947 + 100.0 * 8.382335662841797
Epoch 670, val loss: 0.5041859745979309
Epoch 680, training loss: 838.564697265625 = 0.4657154977321625 + 100.0 * 8.380990028381348
Epoch 680, val loss: 0.5023801326751709
Epoch 690, training loss: 838.232177734375 = 0.46346238255500793 + 100.0 * 8.377687454223633
Epoch 690, val loss: 0.5009191036224365
Epoch 700, training loss: 838.1135864257812 = 0.46133434772491455 + 100.0 * 8.376522064208984
Epoch 700, val loss: 0.499554306268692
Epoch 710, training loss: 838.4150390625 = 0.45928508043289185 + 100.0 * 8.379557609558105
Epoch 710, val loss: 0.49821797013282776
Epoch 720, training loss: 838.25634765625 = 0.4572630524635315 + 100.0 * 8.37799072265625
Epoch 720, val loss: 0.4967089593410492
Epoch 730, training loss: 837.7255249023438 = 0.4552636444568634 + 100.0 * 8.372702598571777
Epoch 730, val loss: 0.49560120701789856
Epoch 740, training loss: 837.6142578125 = 0.45338883996009827 + 100.0 * 8.37160873413086
Epoch 740, val loss: 0.49440085887908936
Epoch 750, training loss: 837.5016479492188 = 0.4516034424304962 + 100.0 * 8.370500564575195
Epoch 750, val loss: 0.4932137429714203
Epoch 760, training loss: 837.6153564453125 = 0.4498688876628876 + 100.0 * 8.371654510498047
Epoch 760, val loss: 0.4921315610408783
Epoch 770, training loss: 837.6123046875 = 0.44810980558395386 + 100.0 * 8.371642112731934
Epoch 770, val loss: 0.49126484990119934
Epoch 780, training loss: 837.1295166015625 = 0.4464057683944702 + 100.0 * 8.366830825805664
Epoch 780, val loss: 0.49018192291259766
Epoch 790, training loss: 837.1480712890625 = 0.44478902220726013 + 100.0 * 8.367033004760742
Epoch 790, val loss: 0.48918429017066956
Epoch 800, training loss: 837.3088989257812 = 0.44317686557769775 + 100.0 * 8.368657112121582
Epoch 800, val loss: 0.48825201392173767
Epoch 810, training loss: 836.8102416992188 = 0.44158315658569336 + 100.0 * 8.363686561584473
Epoch 810, val loss: 0.48723530769348145
Epoch 820, training loss: 836.7557983398438 = 0.440074622631073 + 100.0 * 8.363157272338867
Epoch 820, val loss: 0.4862392246723175
Epoch 830, training loss: 836.67431640625 = 0.438604474067688 + 100.0 * 8.362357139587402
Epoch 830, val loss: 0.48551517724990845
Epoch 840, training loss: 836.890869140625 = 0.43713730573654175 + 100.0 * 8.364537239074707
Epoch 840, val loss: 0.4845437705516815
Epoch 850, training loss: 836.483154296875 = 0.43565860390663147 + 100.0 * 8.360474586486816
Epoch 850, val loss: 0.48377352952957153
Epoch 860, training loss: 836.8613891601562 = 0.4342420995235443 + 100.0 * 8.36427116394043
Epoch 860, val loss: 0.4826801121234894
Epoch 870, training loss: 836.3131713867188 = 0.4327991306781769 + 100.0 * 8.358803749084473
Epoch 870, val loss: 0.4821857810020447
Epoch 880, training loss: 836.138916015625 = 0.4314485788345337 + 100.0 * 8.357074737548828
Epoch 880, val loss: 0.4812738597393036
Epoch 890, training loss: 836.1928100585938 = 0.4301378130912781 + 100.0 * 8.357626914978027
Epoch 890, val loss: 0.48060494661331177
Epoch 900, training loss: 835.9832763671875 = 0.42879199981689453 + 100.0 * 8.355545043945312
Epoch 900, val loss: 0.47980135679244995
Epoch 910, training loss: 835.9065551757812 = 0.427476704120636 + 100.0 * 8.354790687561035
Epoch 910, val loss: 0.47913435101509094
Epoch 920, training loss: 835.7798461914062 = 0.4262009561061859 + 100.0 * 8.353536605834961
Epoch 920, val loss: 0.47841939330101013
Epoch 930, training loss: 835.795654296875 = 0.42491766810417175 + 100.0 * 8.353707313537598
Epoch 930, val loss: 0.4777485728263855
Epoch 940, training loss: 835.5454711914062 = 0.4236406981945038 + 100.0 * 8.351218223571777
Epoch 940, val loss: 0.477064311504364
Epoch 950, training loss: 835.5445556640625 = 0.4224022924900055 + 100.0 * 8.351221084594727
Epoch 950, val loss: 0.47628021240234375
Epoch 960, training loss: 835.3260498046875 = 0.42119768261909485 + 100.0 * 8.349048614501953
Epoch 960, val loss: 0.475627064704895
Epoch 970, training loss: 835.3345336914062 = 0.4200162887573242 + 100.0 * 8.34914493560791
Epoch 970, val loss: 0.47502514719963074
Epoch 980, training loss: 835.6669921875 = 0.41879481077194214 + 100.0 * 8.352481842041016
Epoch 980, val loss: 0.47440391778945923
Epoch 990, training loss: 835.1114501953125 = 0.4175645411014557 + 100.0 * 8.346939086914062
Epoch 990, val loss: 0.4735780954360962
Epoch 1000, training loss: 834.978759765625 = 0.4163963198661804 + 100.0 * 8.345623970031738
Epoch 1000, val loss: 0.4728322923183441
Epoch 1010, training loss: 834.8501586914062 = 0.41527682542800903 + 100.0 * 8.344348907470703
Epoch 1010, val loss: 0.4723101556301117
Epoch 1020, training loss: 834.9733276367188 = 0.4141666889190674 + 100.0 * 8.34559154510498
Epoch 1020, val loss: 0.47157788276672363
Epoch 1030, training loss: 834.96875 = 0.41298672556877136 + 100.0 * 8.345558166503906
Epoch 1030, val loss: 0.47110679745674133
Epoch 1040, training loss: 834.6682739257812 = 0.4118136763572693 + 100.0 * 8.342564582824707
Epoch 1040, val loss: 0.47035858035087585
Epoch 1050, training loss: 834.5723876953125 = 0.4106917977333069 + 100.0 * 8.3416166305542
Epoch 1050, val loss: 0.46972277760505676
Epoch 1060, training loss: 834.5164184570312 = 0.40960368514060974 + 100.0 * 8.341068267822266
Epoch 1060, val loss: 0.4692855477333069
Epoch 1070, training loss: 834.9727172851562 = 0.4084974229335785 + 100.0 * 8.34564208984375
Epoch 1070, val loss: 0.4687967300415039
Epoch 1080, training loss: 834.4549560546875 = 0.4073554575443268 + 100.0 * 8.340476036071777
Epoch 1080, val loss: 0.46791872382164
Epoch 1090, training loss: 834.277587890625 = 0.40625452995300293 + 100.0 * 8.338713645935059
Epoch 1090, val loss: 0.46732380986213684
Epoch 1100, training loss: 834.4219970703125 = 0.40518295764923096 + 100.0 * 8.340167999267578
Epoch 1100, val loss: 0.46686500310897827
Epoch 1110, training loss: 834.2223510742188 = 0.4040836691856384 + 100.0 * 8.33818244934082
Epoch 1110, val loss: 0.4661113917827606
Epoch 1120, training loss: 834.2884521484375 = 0.40297454595565796 + 100.0 * 8.338854789733887
Epoch 1120, val loss: 0.46552351117134094
Epoch 1130, training loss: 834.1209106445312 = 0.40190383791923523 + 100.0 * 8.337189674377441
Epoch 1130, val loss: 0.46492889523506165
Epoch 1140, training loss: 833.9557495117188 = 0.4008302092552185 + 100.0 * 8.335549354553223
Epoch 1140, val loss: 0.4644823670387268
Epoch 1150, training loss: 834.0169067382812 = 0.3997848331928253 + 100.0 * 8.33617115020752
Epoch 1150, val loss: 0.46403953433036804
Epoch 1160, training loss: 834.1491088867188 = 0.3987109661102295 + 100.0 * 8.337504386901855
Epoch 1160, val loss: 0.4633293151855469
Epoch 1170, training loss: 833.9451293945312 = 0.39761367440223694 + 100.0 * 8.335474967956543
Epoch 1170, val loss: 0.462903767824173
Epoch 1180, training loss: 834.0462646484375 = 0.39652255177497864 + 100.0 * 8.33649730682373
Epoch 1180, val loss: 0.4622553884983063
Epoch 1190, training loss: 833.8021850585938 = 0.39545679092407227 + 100.0 * 8.334067344665527
Epoch 1190, val loss: 0.4616543650627136
Epoch 1200, training loss: 833.7469482421875 = 0.39440351724624634 + 100.0 * 8.333525657653809
Epoch 1200, val loss: 0.4611341059207916
Epoch 1210, training loss: 833.7518310546875 = 0.39335861802101135 + 100.0 * 8.333584785461426
Epoch 1210, val loss: 0.46067434549331665
Epoch 1220, training loss: 833.6373291015625 = 0.3923129141330719 + 100.0 * 8.332449913024902
Epoch 1220, val loss: 0.46011295914649963
Epoch 1230, training loss: 833.5439453125 = 0.3912745714187622 + 100.0 * 8.331526756286621
Epoch 1230, val loss: 0.45963650941848755
Epoch 1240, training loss: 833.8364868164062 = 0.39023178815841675 + 100.0 * 8.33446216583252
Epoch 1240, val loss: 0.4591650068759918
Epoch 1250, training loss: 833.5498657226562 = 0.38916251063346863 + 100.0 * 8.3316068649292
Epoch 1250, val loss: 0.4584239721298218
Epoch 1260, training loss: 833.43115234375 = 0.38811206817626953 + 100.0 * 8.330430030822754
Epoch 1260, val loss: 0.45793816447257996
Epoch 1270, training loss: 833.4959716796875 = 0.38707679510116577 + 100.0 * 8.33108901977539
Epoch 1270, val loss: 0.457589715719223
Epoch 1280, training loss: 833.4684448242188 = 0.3860321640968323 + 100.0 * 8.33082389831543
Epoch 1280, val loss: 0.45688360929489136
Epoch 1290, training loss: 833.2694091796875 = 0.3849843144416809 + 100.0 * 8.32884407043457
Epoch 1290, val loss: 0.4563637673854828
Epoch 1300, training loss: 833.1946411132812 = 0.3839583396911621 + 100.0 * 8.328106880187988
Epoch 1300, val loss: 0.45593151450157166
Epoch 1310, training loss: 833.375 = 0.38293927907943726 + 100.0 * 8.329920768737793
Epoch 1310, val loss: 0.4554843008518219
Epoch 1320, training loss: 833.4804077148438 = 0.38188526034355164 + 100.0 * 8.330985069274902
Epoch 1320, val loss: 0.45494189858436584
Epoch 1330, training loss: 833.3670043945312 = 0.38082921504974365 + 100.0 * 8.329861640930176
Epoch 1330, val loss: 0.45421910285949707
Epoch 1340, training loss: 833.328857421875 = 0.3797498047351837 + 100.0 * 8.329490661621094
Epoch 1340, val loss: 0.45376864075660706
Epoch 1350, training loss: 833.0789794921875 = 0.3786885440349579 + 100.0 * 8.327003479003906
Epoch 1350, val loss: 0.4533839225769043
Epoch 1360, training loss: 832.9921264648438 = 0.37768498063087463 + 100.0 * 8.326144218444824
Epoch 1360, val loss: 0.45278486609458923
Epoch 1370, training loss: 832.9138793945312 = 0.3766818642616272 + 100.0 * 8.325371742248535
Epoch 1370, val loss: 0.45240655541419983
Epoch 1380, training loss: 832.9535522460938 = 0.37567901611328125 + 100.0 * 8.32577896118164
Epoch 1380, val loss: 0.4519374668598175
Epoch 1390, training loss: 833.1695556640625 = 0.374659925699234 + 100.0 * 8.327949523925781
Epoch 1390, val loss: 0.45134949684143066
Epoch 1400, training loss: 833.2689208984375 = 0.3736306130886078 + 100.0 * 8.32895278930664
Epoch 1400, val loss: 0.45078906416893005
Epoch 1410, training loss: 833.0667724609375 = 0.37253427505493164 + 100.0 * 8.326942443847656
Epoch 1410, val loss: 0.4505082666873932
Epoch 1420, training loss: 832.8950805664062 = 0.3714941740036011 + 100.0 * 8.325235366821289
Epoch 1420, val loss: 0.44983983039855957
Epoch 1430, training loss: 832.7294921875 = 0.370477557182312 + 100.0 * 8.323590278625488
Epoch 1430, val loss: 0.4494817852973938
Epoch 1440, training loss: 832.687255859375 = 0.36948052048683167 + 100.0 * 8.323177337646484
Epoch 1440, val loss: 0.44896212220191956
Epoch 1450, training loss: 832.8563232421875 = 0.36849328875541687 + 100.0 * 8.324878692626953
Epoch 1450, val loss: 0.44845300912857056
Epoch 1460, training loss: 832.7330322265625 = 0.3674570322036743 + 100.0 * 8.32365608215332
Epoch 1460, val loss: 0.4482728838920593
Epoch 1470, training loss: 833.0030517578125 = 0.3664115369319916 + 100.0 * 8.326366424560547
Epoch 1470, val loss: 0.4476556181907654
Epoch 1480, training loss: 832.62841796875 = 0.3653612434864044 + 100.0 * 8.322630882263184
Epoch 1480, val loss: 0.4472423195838928
Epoch 1490, training loss: 832.541748046875 = 0.36435726284980774 + 100.0 * 8.321773529052734
Epoch 1490, val loss: 0.44670093059539795
Epoch 1500, training loss: 832.4786987304688 = 0.3633607029914856 + 100.0 * 8.32115364074707
Epoch 1500, val loss: 0.4464286267757416
Epoch 1510, training loss: 832.6990966796875 = 0.3623656630516052 + 100.0 * 8.32336711883545
Epoch 1510, val loss: 0.44607895612716675
Epoch 1520, training loss: 832.6461791992188 = 0.361320436000824 + 100.0 * 8.322848320007324
Epoch 1520, val loss: 0.4453448951244354
Epoch 1530, training loss: 832.5595092773438 = 0.3602357506752014 + 100.0 * 8.321992874145508
Epoch 1530, val loss: 0.4450676143169403
Epoch 1540, training loss: 832.3488159179688 = 0.35923078656196594 + 100.0 * 8.31989574432373
Epoch 1540, val loss: 0.4445374310016632
Epoch 1550, training loss: 832.3196411132812 = 0.3582468330860138 + 100.0 * 8.319613456726074
Epoch 1550, val loss: 0.44414272904396057
Epoch 1560, training loss: 832.27978515625 = 0.3572571873664856 + 100.0 * 8.319225311279297
Epoch 1560, val loss: 0.44372183084487915
Epoch 1570, training loss: 832.7711791992188 = 0.356282114982605 + 100.0 * 8.324149131774902
Epoch 1570, val loss: 0.4431828558444977
Epoch 1580, training loss: 832.6017456054688 = 0.35517048835754395 + 100.0 * 8.322465896606445
Epoch 1580, val loss: 0.44310852885246277
Epoch 1590, training loss: 832.4031372070312 = 0.35411491990089417 + 100.0 * 8.320489883422852
Epoch 1590, val loss: 0.44233810901641846
Epoch 1600, training loss: 832.1571044921875 = 0.3530718684196472 + 100.0 * 8.318039894104004
Epoch 1600, val loss: 0.4420498013496399
Epoch 1610, training loss: 832.11474609375 = 0.3520697057247162 + 100.0 * 8.317626953125
Epoch 1610, val loss: 0.4416568875312805
Epoch 1620, training loss: 832.0917358398438 = 0.35107457637786865 + 100.0 * 8.31740665435791
Epoch 1620, val loss: 0.4412308931350708
Epoch 1630, training loss: 832.912841796875 = 0.35006099939346313 + 100.0 * 8.325628280639648
Epoch 1630, val loss: 0.44075605273246765
Epoch 1640, training loss: 832.4448852539062 = 0.3489769399166107 + 100.0 * 8.320959091186523
Epoch 1640, val loss: 0.44058501720428467
Epoch 1650, training loss: 832.060546875 = 0.3479243218898773 + 100.0 * 8.317126274108887
Epoch 1650, val loss: 0.4401031732559204
Epoch 1660, training loss: 832.0946655273438 = 0.346902072429657 + 100.0 * 8.317477226257324
Epoch 1660, val loss: 0.4396800696849823
Epoch 1670, training loss: 832.0277099609375 = 0.34589365124702454 + 100.0 * 8.316818237304688
Epoch 1670, val loss: 0.4393959045410156
Epoch 1680, training loss: 832.2554931640625 = 0.34488600492477417 + 100.0 * 8.319106101989746
Epoch 1680, val loss: 0.43921056389808655
Epoch 1690, training loss: 832.0014038085938 = 0.34385234117507935 + 100.0 * 8.316575050354004
Epoch 1690, val loss: 0.4386177957057953
Epoch 1700, training loss: 831.9337768554688 = 0.34283679723739624 + 100.0 * 8.315909385681152
Epoch 1700, val loss: 0.4382553696632385
Epoch 1710, training loss: 831.889892578125 = 0.34182289242744446 + 100.0 * 8.315481185913086
Epoch 1710, val loss: 0.4380383789539337
Epoch 1720, training loss: 831.968505859375 = 0.3408195674419403 + 100.0 * 8.316276550292969
Epoch 1720, val loss: 0.43780285120010376
Epoch 1730, training loss: 831.9344482421875 = 0.3397862911224365 + 100.0 * 8.315946578979492
Epoch 1730, val loss: 0.4373553991317749
Epoch 1740, training loss: 831.9382934570312 = 0.3387477695941925 + 100.0 * 8.315995216369629
Epoch 1740, val loss: 0.43705594539642334
Epoch 1750, training loss: 831.7752075195312 = 0.33772438764572144 + 100.0 * 8.314374923706055
Epoch 1750, val loss: 0.4366525411605835
Epoch 1760, training loss: 831.7205200195312 = 0.3367219567298889 + 100.0 * 8.313838005065918
Epoch 1760, val loss: 0.4364504814147949
Epoch 1770, training loss: 831.9682006835938 = 0.3357241749763489 + 100.0 * 8.316324234008789
Epoch 1770, val loss: 0.4361320436000824
Epoch 1780, training loss: 831.8240966796875 = 0.3346874415874481 + 100.0 * 8.31489372253418
Epoch 1780, val loss: 0.4359046220779419
Epoch 1790, training loss: 831.700439453125 = 0.33364516496658325 + 100.0 * 8.313668251037598
Epoch 1790, val loss: 0.4354073405265808
Epoch 1800, training loss: 831.760986328125 = 0.33262383937835693 + 100.0 * 8.31428337097168
Epoch 1800, val loss: 0.43519142270088196
Epoch 1810, training loss: 831.6348266601562 = 0.33160996437072754 + 100.0 * 8.313032150268555
Epoch 1810, val loss: 0.434841126203537
Epoch 1820, training loss: 831.629638671875 = 0.33060264587402344 + 100.0 * 8.312990188598633
Epoch 1820, val loss: 0.43455979228019714
Epoch 1830, training loss: 831.5751953125 = 0.3295927941799164 + 100.0 * 8.312456130981445
Epoch 1830, val loss: 0.4343230128288269
Epoch 1840, training loss: 831.6591186523438 = 0.32858094573020935 + 100.0 * 8.313305854797363
Epoch 1840, val loss: 0.4340413510799408
Epoch 1850, training loss: 831.9486694335938 = 0.3275739848613739 + 100.0 * 8.316210746765137
Epoch 1850, val loss: 0.4335123896598816
Epoch 1860, training loss: 831.6226196289062 = 0.3264748156070709 + 100.0 * 8.31296157836914
Epoch 1860, val loss: 0.43350115418434143
Epoch 1870, training loss: 831.4461059570312 = 0.3254237771034241 + 100.0 * 8.311206817626953
Epoch 1870, val loss: 0.433002769947052
Epoch 1880, training loss: 831.4100952148438 = 0.3244091868400574 + 100.0 * 8.310856819152832
Epoch 1880, val loss: 0.43294546008110046
Epoch 1890, training loss: 831.3756103515625 = 0.32341188192367554 + 100.0 * 8.310522079467773
Epoch 1890, val loss: 0.4325743317604065
Epoch 1900, training loss: 831.3601684570312 = 0.32240960001945496 + 100.0 * 8.310378074645996
Epoch 1900, val loss: 0.43240925669670105
Epoch 1910, training loss: 831.8633422851562 = 0.32139530777931213 + 100.0 * 8.31541919708252
Epoch 1910, val loss: 0.43219873309135437
Epoch 1920, training loss: 831.7374267578125 = 0.320321261882782 + 100.0 * 8.314170837402344
Epoch 1920, val loss: 0.4318700432777405
Epoch 1930, training loss: 831.4448852539062 = 0.31925055384635925 + 100.0 * 8.311256408691406
Epoch 1930, val loss: 0.4316215515136719
Epoch 1940, training loss: 831.2813720703125 = 0.31821370124816895 + 100.0 * 8.30963134765625
Epoch 1940, val loss: 0.43140342831611633
Epoch 1950, training loss: 831.2319946289062 = 0.3171912729740143 + 100.0 * 8.309147834777832
Epoch 1950, val loss: 0.43115198612213135
Epoch 1960, training loss: 831.41455078125 = 0.31617477536201477 + 100.0 * 8.310983657836914
Epoch 1960, val loss: 0.43084004521369934
Epoch 1970, training loss: 831.310302734375 = 0.315104216337204 + 100.0 * 8.309951782226562
Epoch 1970, val loss: 0.4308371841907501
Epoch 1980, training loss: 831.401611328125 = 0.31402677297592163 + 100.0 * 8.31087589263916
Epoch 1980, val loss: 0.4302241802215576
Epoch 1990, training loss: 831.2453002929688 = 0.31297269463539124 + 100.0 * 8.30932331085205
Epoch 1990, val loss: 0.4300714433193207
Epoch 2000, training loss: 831.1611938476562 = 0.31194061040878296 + 100.0 * 8.308492660522461
Epoch 2000, val loss: 0.42987698316574097
Epoch 2010, training loss: 831.3151245117188 = 0.31091833114624023 + 100.0 * 8.310042381286621
Epoch 2010, val loss: 0.42964521050453186
Epoch 2020, training loss: 831.2340698242188 = 0.3098633289337158 + 100.0 * 8.309242248535156
Epoch 2020, val loss: 0.4294281005859375
Epoch 2030, training loss: 831.2980346679688 = 0.30880722403526306 + 100.0 * 8.309892654418945
Epoch 2030, val loss: 0.4291711747646332
Epoch 2040, training loss: 831.162841796875 = 0.30774664878845215 + 100.0 * 8.308550834655762
Epoch 2040, val loss: 0.428970605134964
Epoch 2050, training loss: 831.0299072265625 = 0.30667856335639954 + 100.0 * 8.307231903076172
Epoch 2050, val loss: 0.4288181960582733
Epoch 2060, training loss: 831.0580444335938 = 0.3056206703186035 + 100.0 * 8.307524681091309
Epoch 2060, val loss: 0.42858636379241943
Epoch 2070, training loss: 831.6209716796875 = 0.3045724332332611 + 100.0 * 8.313163757324219
Epoch 2070, val loss: 0.4281840920448303
Epoch 2080, training loss: 831.337646484375 = 0.30345091223716736 + 100.0 * 8.310341835021973
Epoch 2080, val loss: 0.42816096544265747
Epoch 2090, training loss: 831.153076171875 = 0.3023282587528229 + 100.0 * 8.308507919311523
Epoch 2090, val loss: 0.42799267172813416
Epoch 2100, training loss: 831.0264892578125 = 0.3012367784976959 + 100.0 * 8.307252883911133
Epoch 2100, val loss: 0.4278673827648163
Epoch 2110, training loss: 830.9827270507812 = 0.3001565635204315 + 100.0 * 8.306825637817383
Epoch 2110, val loss: 0.4275586009025574
Epoch 2120, training loss: 831.0721435546875 = 0.2990776300430298 + 100.0 * 8.307730674743652
Epoch 2120, val loss: 0.42737171053886414
Epoch 2130, training loss: 830.8677368164062 = 0.29798468947410583 + 100.0 * 8.305697441101074
Epoch 2130, val loss: 0.4272809624671936
Epoch 2140, training loss: 831.08154296875 = 0.2969112694263458 + 100.0 * 8.307846069335938
Epoch 2140, val loss: 0.4272206127643585
Epoch 2150, training loss: 831.19091796875 = 0.2957930564880371 + 100.0 * 8.308951377868652
Epoch 2150, val loss: 0.42678073048591614
Epoch 2160, training loss: 830.9375 = 0.29468071460723877 + 100.0 * 8.306427955627441
Epoch 2160, val loss: 0.4264764189720154
Epoch 2170, training loss: 830.8148803710938 = 0.29356828331947327 + 100.0 * 8.30521297454834
Epoch 2170, val loss: 0.42640307545661926
Epoch 2180, training loss: 830.7918701171875 = 0.2924879193305969 + 100.0 * 8.304993629455566
Epoch 2180, val loss: 0.42615583539009094
Epoch 2190, training loss: 831.3878173828125 = 0.29140976071357727 + 100.0 * 8.310964584350586
Epoch 2190, val loss: 0.4260084331035614
Epoch 2200, training loss: 830.8714599609375 = 0.2902585566043854 + 100.0 * 8.305811882019043
Epoch 2200, val loss: 0.425752729177475
Epoch 2210, training loss: 830.7385864257812 = 0.28915026783943176 + 100.0 * 8.304494857788086
Epoch 2210, val loss: 0.42561838030815125
Epoch 2220, training loss: 830.908935546875 = 0.28805676102638245 + 100.0 * 8.306208610534668
Epoch 2220, val loss: 0.425497829914093
Epoch 2230, training loss: 830.7449951171875 = 0.28693273663520813 + 100.0 * 8.304580688476562
Epoch 2230, val loss: 0.42525461316108704
Epoch 2240, training loss: 830.6583251953125 = 0.2858090400695801 + 100.0 * 8.303725242614746
Epoch 2240, val loss: 0.4249677360057831
Epoch 2250, training loss: 830.6455688476562 = 0.2846953272819519 + 100.0 * 8.303608894348145
Epoch 2250, val loss: 0.42484116554260254
Epoch 2260, training loss: 831.1483764648438 = 0.2836100161075592 + 100.0 * 8.308647155761719
Epoch 2260, val loss: 0.4244585633277893
Epoch 2270, training loss: 830.9757690429688 = 0.28243401646614075 + 100.0 * 8.306933403015137
Epoch 2270, val loss: 0.42463821172714233
Epoch 2280, training loss: 830.67626953125 = 0.28128373622894287 + 100.0 * 8.303949356079102
Epoch 2280, val loss: 0.4242907762527466
Epoch 2290, training loss: 830.5563354492188 = 0.28015753626823425 + 100.0 * 8.302762031555176
Epoch 2290, val loss: 0.42419344186782837
Epoch 2300, training loss: 830.5364990234375 = 0.2790466547012329 + 100.0 * 8.302574157714844
Epoch 2300, val loss: 0.4241504371166229
Epoch 2310, training loss: 830.6630859375 = 0.2779373824596405 + 100.0 * 8.303851127624512
Epoch 2310, val loss: 0.42401981353759766
Epoch 2320, training loss: 830.7191162109375 = 0.2768024802207947 + 100.0 * 8.304423332214355
Epoch 2320, val loss: 0.4238886833190918
Epoch 2330, training loss: 830.671142578125 = 0.275658518075943 + 100.0 * 8.303955078125
Epoch 2330, val loss: 0.4235502779483795
Epoch 2340, training loss: 830.5707397460938 = 0.27451515197753906 + 100.0 * 8.302962303161621
Epoch 2340, val loss: 0.4233378469944
Epoch 2350, training loss: 830.5433959960938 = 0.27337974309921265 + 100.0 * 8.30270004272461
Epoch 2350, val loss: 0.4231891930103302
Epoch 2360, training loss: 830.5514526367188 = 0.27224037051200867 + 100.0 * 8.3027925491333
Epoch 2360, val loss: 0.42306897044181824
Epoch 2370, training loss: 830.6368408203125 = 0.2710905373096466 + 100.0 * 8.303657531738281
Epoch 2370, val loss: 0.42292213439941406
Epoch 2380, training loss: 830.4500122070312 = 0.2699359059333801 + 100.0 * 8.301800727844238
Epoch 2380, val loss: 0.422893226146698
Epoch 2390, training loss: 830.7477416992188 = 0.26881349086761475 + 100.0 * 8.304789543151855
Epoch 2390, val loss: 0.42265236377716064
Epoch 2400, training loss: 830.440185546875 = 0.2676273584365845 + 100.0 * 8.301725387573242
Epoch 2400, val loss: 0.4226403832435608
Epoch 2410, training loss: 830.47802734375 = 0.2664666771888733 + 100.0 * 8.302115440368652
Epoch 2410, val loss: 0.42250150442123413
Epoch 2420, training loss: 830.4556884765625 = 0.2653275728225708 + 100.0 * 8.30190372467041
Epoch 2420, val loss: 0.42227619886398315
Epoch 2430, training loss: 830.3092041015625 = 0.2641478180885315 + 100.0 * 8.300450325012207
Epoch 2430, val loss: 0.42248475551605225
Epoch 2440, training loss: 830.4323120117188 = 0.2630034387111664 + 100.0 * 8.301692962646484
Epoch 2440, val loss: 0.42246994376182556
Epoch 2450, training loss: 830.452392578125 = 0.26184436678886414 + 100.0 * 8.301905632019043
Epoch 2450, val loss: 0.4222859740257263
Epoch 2460, training loss: 830.3545532226562 = 0.26068249344825745 + 100.0 * 8.300938606262207
Epoch 2460, val loss: 0.4220520257949829
Epoch 2470, training loss: 830.3391723632812 = 0.25951284170150757 + 100.0 * 8.300796508789062
Epoch 2470, val loss: 0.42214515805244446
Epoch 2480, training loss: 830.2415161132812 = 0.25834760069847107 + 100.0 * 8.29983139038086
Epoch 2480, val loss: 0.4220011234283447
Epoch 2490, training loss: 830.7349853515625 = 0.257222443819046 + 100.0 * 8.304778099060059
Epoch 2490, val loss: 0.4216248393058777
Epoch 2500, training loss: 830.455322265625 = 0.2560054659843445 + 100.0 * 8.301993370056152
Epoch 2500, val loss: 0.4221745729446411
Epoch 2510, training loss: 830.4149780273438 = 0.25484153628349304 + 100.0 * 8.30160140991211
Epoch 2510, val loss: 0.42169034481048584
Epoch 2520, training loss: 830.2610473632812 = 0.2536362111568451 + 100.0 * 8.300073623657227
Epoch 2520, val loss: 0.42199450731277466
Epoch 2530, training loss: 830.0985107421875 = 0.2524592876434326 + 100.0 * 8.298460960388184
Epoch 2530, val loss: 0.4218637943267822
Epoch 2540, training loss: 830.1201782226562 = 0.251303106546402 + 100.0 * 8.298688888549805
Epoch 2540, val loss: 0.4220784902572632
Epoch 2550, training loss: 830.32568359375 = 0.2501463294029236 + 100.0 * 8.300755500793457
Epoch 2550, val loss: 0.422120064496994
Epoch 2560, training loss: 830.228271484375 = 0.24895450472831726 + 100.0 * 8.299793243408203
Epoch 2560, val loss: 0.4220364987850189
Epoch 2570, training loss: 830.236083984375 = 0.24777235090732574 + 100.0 * 8.299882888793945
Epoch 2570, val loss: 0.42203661799430847
Epoch 2580, training loss: 830.2029418945312 = 0.24657325446605682 + 100.0 * 8.29956340789795
Epoch 2580, val loss: 0.42208245396614075
Epoch 2590, training loss: 830.3075561523438 = 0.24541136622428894 + 100.0 * 8.300621032714844
Epoch 2590, val loss: 0.422049343585968
Epoch 2600, training loss: 830.0861206054688 = 0.2442028969526291 + 100.0 * 8.298418998718262
Epoch 2600, val loss: 0.4223628044128418
Epoch 2610, training loss: 829.9982299804688 = 0.24301747977733612 + 100.0 * 8.297552108764648
Epoch 2610, val loss: 0.42235109210014343
Epoch 2620, training loss: 829.9849853515625 = 0.2418474704027176 + 100.0 * 8.297431945800781
Epoch 2620, val loss: 0.42235657572746277
Epoch 2630, training loss: 830.0635986328125 = 0.2406768947839737 + 100.0 * 8.298229217529297
Epoch 2630, val loss: 0.4224623143672943
Epoch 2640, training loss: 830.1653442382812 = 0.2394978255033493 + 100.0 * 8.2992582321167
Epoch 2640, val loss: 0.4227439761161804
Epoch 2650, training loss: 830.198974609375 = 0.23831507563591003 + 100.0 * 8.299606323242188
Epoch 2650, val loss: 0.42264363169670105
Epoch 2660, training loss: 830.1416015625 = 0.23714017868041992 + 100.0 * 8.299044609069824
Epoch 2660, val loss: 0.4226161241531372
Epoch 2670, training loss: 829.9195556640625 = 0.2359386682510376 + 100.0 * 8.296835899353027
Epoch 2670, val loss: 0.4230087697505951
Epoch 2680, training loss: 829.9000244140625 = 0.23476861417293549 + 100.0 * 8.296652793884277
Epoch 2680, val loss: 0.42308875918388367
Epoch 2690, training loss: 830.1071166992188 = 0.2336157262325287 + 100.0 * 8.298734664916992
Epoch 2690, val loss: 0.4231981933116913
Epoch 2700, training loss: 829.9173583984375 = 0.23243211209774017 + 100.0 * 8.296849250793457
Epoch 2700, val loss: 0.4231920540332794
Epoch 2710, training loss: 829.9150390625 = 0.23126333951950073 + 100.0 * 8.29683780670166
Epoch 2710, val loss: 0.42337992787361145
Epoch 2720, training loss: 829.9708862304688 = 0.2301015704870224 + 100.0 * 8.297408103942871
Epoch 2720, val loss: 0.42352819442749023
Epoch 2730, training loss: 829.9617309570312 = 0.2289431095123291 + 100.0 * 8.297327995300293
Epoch 2730, val loss: 0.4239245057106018
Epoch 2740, training loss: 829.923583984375 = 0.22777625918388367 + 100.0 * 8.296957969665527
Epoch 2740, val loss: 0.42408525943756104
Epoch 2750, training loss: 829.7559814453125 = 0.2266000658273697 + 100.0 * 8.295293807983398
Epoch 2750, val loss: 0.4240654706954956
Epoch 2760, training loss: 829.8422241210938 = 0.22544816136360168 + 100.0 * 8.296167373657227
Epoch 2760, val loss: 0.4242544174194336
Epoch 2770, training loss: 829.8236694335938 = 0.22428712248802185 + 100.0 * 8.29599380493164
Epoch 2770, val loss: 0.4245016276836395
Epoch 2780, training loss: 829.8134765625 = 0.22312839329242706 + 100.0 * 8.295903205871582
Epoch 2780, val loss: 0.4248303771018982
Epoch 2790, training loss: 829.9335327148438 = 0.22196824848651886 + 100.0 * 8.297115325927734
Epoch 2790, val loss: 0.42495760321617126
Epoch 2800, training loss: 829.9299926757812 = 0.22080571949481964 + 100.0 * 8.297091484069824
Epoch 2800, val loss: 0.4253971576690674
Epoch 2810, training loss: 829.8117065429688 = 0.21963535249233246 + 100.0 * 8.295920372009277
Epoch 2810, val loss: 0.4256419241428375
Epoch 2820, training loss: 829.6475219726562 = 0.2184683233499527 + 100.0 * 8.294290542602539
Epoch 2820, val loss: 0.42558860778808594
Epoch 2830, training loss: 829.612548828125 = 0.21732108294963837 + 100.0 * 8.293951988220215
Epoch 2830, val loss: 0.42582273483276367
Epoch 2840, training loss: 829.783447265625 = 0.21619682013988495 + 100.0 * 8.295672416687012
Epoch 2840, val loss: 0.4260600209236145
Epoch 2850, training loss: 829.9228515625 = 0.21503905951976776 + 100.0 * 8.297078132629395
Epoch 2850, val loss: 0.42671239376068115
Epoch 2860, training loss: 829.711669921875 = 0.21388603746891022 + 100.0 * 8.294978141784668
Epoch 2860, val loss: 0.42699599266052246
Epoch 2870, training loss: 829.5485229492188 = 0.21269486844539642 + 100.0 * 8.293357849121094
Epoch 2870, val loss: 0.42723190784454346
Epoch 2880, training loss: 829.516845703125 = 0.21155261993408203 + 100.0 * 8.293052673339844
Epoch 2880, val loss: 0.4275025725364685
Epoch 2890, training loss: 829.4984130859375 = 0.21041537821292877 + 100.0 * 8.292880058288574
Epoch 2890, val loss: 0.42782697081565857
Epoch 2900, training loss: 829.87353515625 = 0.20929020643234253 + 100.0 * 8.296642303466797
Epoch 2900, val loss: 0.4282565116882324
Epoch 2910, training loss: 829.62744140625 = 0.2081446349620819 + 100.0 * 8.294193267822266
Epoch 2910, val loss: 0.42861878871917725
Epoch 2920, training loss: 829.5778198242188 = 0.20696790516376495 + 100.0 * 8.293708801269531
Epoch 2920, val loss: 0.42869049310684204
Epoch 2930, training loss: 829.4714965820312 = 0.20581819117069244 + 100.0 * 8.292656898498535
Epoch 2930, val loss: 0.4290829300880432
Epoch 2940, training loss: 829.4364013671875 = 0.20467816293239594 + 100.0 * 8.292317390441895
Epoch 2940, val loss: 0.4294750392436981
Epoch 2950, training loss: 829.4443359375 = 0.2035537213087082 + 100.0 * 8.292407989501953
Epoch 2950, val loss: 0.4298570156097412
Epoch 2960, training loss: 829.9910278320312 = 0.20244824886322021 + 100.0 * 8.29788589477539
Epoch 2960, val loss: 0.43015289306640625
Epoch 2970, training loss: 829.744140625 = 0.20128999650478363 + 100.0 * 8.295428276062012
Epoch 2970, val loss: 0.4304460883140564
Epoch 2980, training loss: 829.5663452148438 = 0.20014169812202454 + 100.0 * 8.293662071228027
Epoch 2980, val loss: 0.43113166093826294
Epoch 2990, training loss: 829.5023193359375 = 0.1990157812833786 + 100.0 * 8.2930326461792
Epoch 2990, val loss: 0.43150991201400757
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8462709284627092
0.8666956458740854
The final CL Acc:0.84509, 0.00402, The final GNN Acc:0.86689, 0.00022
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=3, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 97450])
remove edge: torch.Size([2, 79878])
updated graph: torch.Size([2, 88680])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1059.3187255859375 = 1.092340350151062 + 100.0 * 10.582263946533203
Epoch 0, val loss: 1.0926539897918701
Epoch 10, training loss: 1059.223388671875 = 1.0866295099258423 + 100.0 * 10.581367492675781
Epoch 10, val loss: 1.0868791341781616
Epoch 20, training loss: 1058.471923828125 = 1.079717755317688 + 100.0 * 10.573922157287598
Epoch 20, val loss: 1.0798999071121216
Epoch 30, training loss: 1053.1905517578125 = 1.0711687803268433 + 100.0 * 10.521193504333496
Epoch 30, val loss: 1.0712593793869019
Epoch 40, training loss: 1031.4437255859375 = 1.061847448348999 + 100.0 * 10.303817749023438
Epoch 40, val loss: 1.062311053276062
Epoch 50, training loss: 987.876953125 = 1.0549561977386475 + 100.0 * 9.868220329284668
Epoch 50, val loss: 1.0557217597961426
Epoch 60, training loss: 950.6390991210938 = 1.0485793352127075 + 100.0 * 9.495904922485352
Epoch 60, val loss: 1.0491774082183838
Epoch 70, training loss: 927.4290771484375 = 1.0398222208023071 + 100.0 * 9.263893127441406
Epoch 70, val loss: 1.040452003479004
Epoch 80, training loss: 909.1688842773438 = 1.0320570468902588 + 100.0 * 9.081368446350098
Epoch 80, val loss: 1.0330270528793335
Epoch 90, training loss: 897.4360961914062 = 1.0266064405441284 + 100.0 * 8.964095115661621
Epoch 90, val loss: 1.0278109312057495
Epoch 100, training loss: 889.9146118164062 = 1.0219630002975464 + 100.0 * 8.88892650604248
Epoch 100, val loss: 1.0230975151062012
Epoch 110, training loss: 884.56494140625 = 1.016625165939331 + 100.0 * 8.83548355102539
Epoch 110, val loss: 1.017629861831665
Epoch 120, training loss: 880.8505249023438 = 1.0103437900543213 + 100.0 * 8.798401832580566
Epoch 120, val loss: 1.0113935470581055
Epoch 130, training loss: 876.6565551757812 = 1.0036280155181885 + 100.0 * 8.756529808044434
Epoch 130, val loss: 1.0048342943191528
Epoch 140, training loss: 872.664794921875 = 0.9971556067466736 + 100.0 * 8.716676712036133
Epoch 140, val loss: 0.9983444213867188
Epoch 150, training loss: 869.2052001953125 = 0.9902259707450867 + 100.0 * 8.682149887084961
Epoch 150, val loss: 0.9914581179618835
Epoch 160, training loss: 866.4703369140625 = 0.9823530316352844 + 100.0 * 8.654879570007324
Epoch 160, val loss: 0.9836181998252869
Epoch 170, training loss: 863.8187866210938 = 0.9734607338905334 + 100.0 * 8.628453254699707
Epoch 170, val loss: 0.9747483134269714
Epoch 180, training loss: 862.43408203125 = 0.9637744426727295 + 100.0 * 8.614703178405762
Epoch 180, val loss: 0.9652513265609741
Epoch 190, training loss: 860.13232421875 = 0.9531177282333374 + 100.0 * 8.591792106628418
Epoch 190, val loss: 0.9546241760253906
Epoch 200, training loss: 858.28564453125 = 0.941599428653717 + 100.0 * 8.573440551757812
Epoch 200, val loss: 0.943142294883728
Epoch 210, training loss: 856.8292236328125 = 0.9291333556175232 + 100.0 * 8.559000968933105
Epoch 210, val loss: 0.9308252334594727
Epoch 220, training loss: 855.8341064453125 = 0.9156213998794556 + 100.0 * 8.549184799194336
Epoch 220, val loss: 0.9173368811607361
Epoch 230, training loss: 854.4530029296875 = 0.900926947593689 + 100.0 * 8.535520553588867
Epoch 230, val loss: 0.902845561504364
Epoch 240, training loss: 853.3670043945312 = 0.8853464126586914 + 100.0 * 8.524816513061523
Epoch 240, val loss: 0.887528121471405
Epoch 250, training loss: 852.5887451171875 = 0.8689146637916565 + 100.0 * 8.51719856262207
Epoch 250, val loss: 0.8712724447250366
Epoch 260, training loss: 851.60986328125 = 0.8517079949378967 + 100.0 * 8.50758171081543
Epoch 260, val loss: 0.8545046448707581
Epoch 270, training loss: 850.71044921875 = 0.8341422080993652 + 100.0 * 8.498763084411621
Epoch 270, val loss: 0.8373276591300964
Epoch 280, training loss: 850.62353515625 = 0.8162685036659241 + 100.0 * 8.498072624206543
Epoch 280, val loss: 0.8198343515396118
Epoch 290, training loss: 849.3529052734375 = 0.7979221343994141 + 100.0 * 8.485549926757812
Epoch 290, val loss: 0.802241861820221
Epoch 300, training loss: 848.6129150390625 = 0.7798614501953125 + 100.0 * 8.478330612182617
Epoch 300, val loss: 0.7848020792007446
Epoch 310, training loss: 848.849609375 = 0.7620515823364258 + 100.0 * 8.480875968933105
Epoch 310, val loss: 0.767699658870697
Epoch 320, training loss: 847.6033325195312 = 0.7443373799324036 + 100.0 * 8.468589782714844
Epoch 320, val loss: 0.7507718205451965
Epoch 330, training loss: 846.823486328125 = 0.7273353338241577 + 100.0 * 8.46096134185791
Epoch 330, val loss: 0.7346284985542297
Epoch 340, training loss: 846.1907958984375 = 0.7111440896987915 + 100.0 * 8.45479679107666
Epoch 340, val loss: 0.7193140387535095
Epoch 350, training loss: 845.9295654296875 = 0.6955558061599731 + 100.0 * 8.452340126037598
Epoch 350, val loss: 0.7046374082565308
Epoch 360, training loss: 845.5985717773438 = 0.6804968118667603 + 100.0 * 8.449180603027344
Epoch 360, val loss: 0.6906295418739319
Epoch 370, training loss: 844.8814086914062 = 0.6666790246963501 + 100.0 * 8.442147254943848
Epoch 370, val loss: 0.6777433156967163
Epoch 380, training loss: 844.5090942382812 = 0.653820812702179 + 100.0 * 8.438552856445312
Epoch 380, val loss: 0.6658617854118347
Epoch 390, training loss: 844.0934448242188 = 0.641758382320404 + 100.0 * 8.434516906738281
Epoch 390, val loss: 0.654800295829773
Epoch 400, training loss: 843.7235717773438 = 0.6305334568023682 + 100.0 * 8.430930137634277
Epoch 400, val loss: 0.6445861458778381
Epoch 410, training loss: 844.71923828125 = 0.6201310157775879 + 100.0 * 8.440991401672363
Epoch 410, val loss: 0.6351577639579773
Epoch 420, training loss: 843.6235961914062 = 0.6102097630500793 + 100.0 * 8.430133819580078
Epoch 420, val loss: 0.6262641549110413
Epoch 430, training loss: 842.837158203125 = 0.6012256145477295 + 100.0 * 8.422359466552734
Epoch 430, val loss: 0.6183185577392578
Epoch 440, training loss: 842.555419921875 = 0.5930708050727844 + 100.0 * 8.419623374938965
Epoch 440, val loss: 0.6111565232276917
Epoch 450, training loss: 842.2574462890625 = 0.5855513215065002 + 100.0 * 8.416718482971191
Epoch 450, val loss: 0.6046161651611328
Epoch 460, training loss: 842.6035766601562 = 0.5785062313079834 + 100.0 * 8.42025089263916
Epoch 460, val loss: 0.5985649228096008
Epoch 470, training loss: 841.9481201171875 = 0.5719050765037537 + 100.0 * 8.413762092590332
Epoch 470, val loss: 0.5929418802261353
Epoch 480, training loss: 841.535400390625 = 0.5659208297729492 + 100.0 * 8.40969467163086
Epoch 480, val loss: 0.5879471898078918
Epoch 490, training loss: 841.307861328125 = 0.5604271292686462 + 100.0 * 8.407474517822266
Epoch 490, val loss: 0.5834274291992188
Epoch 500, training loss: 841.370361328125 = 0.5553027391433716 + 100.0 * 8.408150672912598
Epoch 500, val loss: 0.579295814037323
Epoch 510, training loss: 841.0787353515625 = 0.5504326820373535 + 100.0 * 8.405282974243164
Epoch 510, val loss: 0.5753774642944336
Epoch 520, training loss: 840.8878784179688 = 0.5459363460540771 + 100.0 * 8.403419494628906
Epoch 520, val loss: 0.5718880891799927
Epoch 530, training loss: 840.532470703125 = 0.541840672492981 + 100.0 * 8.399906158447266
Epoch 530, val loss: 0.5687263011932373
Epoch 540, training loss: 840.4882202148438 = 0.5380129814147949 + 100.0 * 8.39950180053711
Epoch 540, val loss: 0.565849781036377
Epoch 550, training loss: 840.447509765625 = 0.5343170762062073 + 100.0 * 8.399131774902344
Epoch 550, val loss: 0.5631462335586548
Epoch 560, training loss: 840.0513305664062 = 0.5308985114097595 + 100.0 * 8.395204544067383
Epoch 560, val loss: 0.5606532692909241
Epoch 570, training loss: 839.885986328125 = 0.5277619957923889 + 100.0 * 8.393582344055176
Epoch 570, val loss: 0.558413028717041
Epoch 580, training loss: 839.8981323242188 = 0.5248218178749084 + 100.0 * 8.393733024597168
Epoch 580, val loss: 0.556344211101532
Epoch 590, training loss: 839.6205444335938 = 0.5219513177871704 + 100.0 * 8.390985488891602
Epoch 590, val loss: 0.5544129014015198
Epoch 600, training loss: 839.53369140625 = 0.5192697644233704 + 100.0 * 8.390144348144531
Epoch 600, val loss: 0.5525603890419006
Epoch 610, training loss: 839.3819580078125 = 0.5167745351791382 + 100.0 * 8.388651847839355
Epoch 610, val loss: 0.5509329438209534
Epoch 620, training loss: 839.7405395507812 = 0.5143924355506897 + 100.0 * 8.392261505126953
Epoch 620, val loss: 0.5493451952934265
Epoch 630, training loss: 839.2085571289062 = 0.5120701789855957 + 100.0 * 8.386964797973633
Epoch 630, val loss: 0.5478244423866272
Epoch 640, training loss: 838.9971923828125 = 0.5099273920059204 + 100.0 * 8.384872436523438
Epoch 640, val loss: 0.5464878678321838
Epoch 650, training loss: 838.809814453125 = 0.5078936219215393 + 100.0 * 8.38301944732666
Epoch 650, val loss: 0.5451723337173462
Epoch 660, training loss: 838.92529296875 = 0.5059652328491211 + 100.0 * 8.384193420410156
Epoch 660, val loss: 0.543968141078949
Epoch 670, training loss: 839.0117797851562 = 0.5040175914764404 + 100.0 * 8.385077476501465
Epoch 670, val loss: 0.5427494049072266
Epoch 680, training loss: 838.5757446289062 = 0.5021554827690125 + 100.0 * 8.380736351013184
Epoch 680, val loss: 0.5415672659873962
Epoch 690, training loss: 838.4661865234375 = 0.5004496574401855 + 100.0 * 8.379657745361328
Epoch 690, val loss: 0.5405114889144897
Epoch 700, training loss: 838.462158203125 = 0.4988231956958771 + 100.0 * 8.379632949829102
Epoch 700, val loss: 0.5395219922065735
Epoch 710, training loss: 838.5371704101562 = 0.4972038269042969 + 100.0 * 8.380399703979492
Epoch 710, val loss: 0.5385636687278748
Epoch 720, training loss: 838.1519165039062 = 0.49563413858413696 + 100.0 * 8.37656307220459
Epoch 720, val loss: 0.5375986099243164
Epoch 730, training loss: 837.9849243164062 = 0.4941730797290802 + 100.0 * 8.374907493591309
Epoch 730, val loss: 0.5367015600204468
Epoch 740, training loss: 838.3072509765625 = 0.49277105927467346 + 100.0 * 8.378144264221191
Epoch 740, val loss: 0.5358962416648865
Epoch 750, training loss: 838.0796508789062 = 0.4912925958633423 + 100.0 * 8.375884056091309
Epoch 750, val loss: 0.5349968671798706
Epoch 760, training loss: 837.8511962890625 = 0.48989182710647583 + 100.0 * 8.373613357543945
Epoch 760, val loss: 0.5341456532478333
Epoch 770, training loss: 837.6027221679688 = 0.48861750960350037 + 100.0 * 8.371140480041504
Epoch 770, val loss: 0.5334108471870422
Epoch 780, training loss: 837.8721313476562 = 0.48739093542099 + 100.0 * 8.373847961425781
Epoch 780, val loss: 0.5326852202415466
Epoch 790, training loss: 837.49609375 = 0.4860996603965759 + 100.0 * 8.370100021362305
Epoch 790, val loss: 0.5319513082504272
Epoch 800, training loss: 837.3377685546875 = 0.4848790168762207 + 100.0 * 8.368529319763184
Epoch 800, val loss: 0.5311887860298157
Epoch 810, training loss: 837.2213745117188 = 0.4837486743927002 + 100.0 * 8.367376327514648
Epoch 810, val loss: 0.5305594801902771
Epoch 820, training loss: 837.1505126953125 = 0.48265722393989563 + 100.0 * 8.366678237915039
Epoch 820, val loss: 0.5299310684204102
Epoch 830, training loss: 838.148681640625 = 0.48158204555511475 + 100.0 * 8.376670837402344
Epoch 830, val loss: 0.5292448997497559
Epoch 840, training loss: 837.109619140625 = 0.4803621768951416 + 100.0 * 8.366292953491211
Epoch 840, val loss: 0.5286095142364502
Epoch 850, training loss: 837.0559692382812 = 0.47925373911857605 + 100.0 * 8.365767478942871
Epoch 850, val loss: 0.5279356837272644
Epoch 860, training loss: 836.9840087890625 = 0.4782369136810303 + 100.0 * 8.365057945251465
Epoch 860, val loss: 0.5273759961128235
Epoch 870, training loss: 837.3526611328125 = 0.4772195816040039 + 100.0 * 8.368754386901855
Epoch 870, val loss: 0.5268154740333557
Epoch 880, training loss: 836.77294921875 = 0.47618556022644043 + 100.0 * 8.362967491149902
Epoch 880, val loss: 0.5261781215667725
Epoch 890, training loss: 836.6807250976562 = 0.47522228956222534 + 100.0 * 8.362054824829102
Epoch 890, val loss: 0.5256990790367126
Epoch 900, training loss: 836.6248779296875 = 0.4742763936519623 + 100.0 * 8.361505508422852
Epoch 900, val loss: 0.5251579284667969
Epoch 910, training loss: 836.8074340820312 = 0.4733261168003082 + 100.0 * 8.363341331481934
Epoch 910, val loss: 0.5245920419692993
Epoch 920, training loss: 837.0404052734375 = 0.4723418354988098 + 100.0 * 8.365680694580078
Epoch 920, val loss: 0.5240407586097717
Epoch 930, training loss: 836.4680786132812 = 0.4713591933250427 + 100.0 * 8.359967231750488
Epoch 930, val loss: 0.5234854817390442
Epoch 940, training loss: 836.1950073242188 = 0.4704558253288269 + 100.0 * 8.357245445251465
Epoch 940, val loss: 0.5229711532592773
Epoch 950, training loss: 836.1507568359375 = 0.4695967733860016 + 100.0 * 8.3568115234375
Epoch 950, val loss: 0.522520899772644
Epoch 960, training loss: 836.78271484375 = 0.46871834993362427 + 100.0 * 8.363140106201172
Epoch 960, val loss: 0.5220265984535217
Epoch 970, training loss: 836.121337890625 = 0.4677843451499939 + 100.0 * 8.356535911560059
Epoch 970, val loss: 0.5214701294898987
Epoch 980, training loss: 835.938720703125 = 0.4669192135334015 + 100.0 * 8.354718208312988
Epoch 980, val loss: 0.5209779143333435
Epoch 990, training loss: 836.3526611328125 = 0.466055303812027 + 100.0 * 8.358865737915039
Epoch 990, val loss: 0.520496666431427
Epoch 1000, training loss: 835.8150634765625 = 0.4651612639427185 + 100.0 * 8.353499412536621
Epoch 1000, val loss: 0.519990861415863
Epoch 1010, training loss: 835.9508056640625 = 0.4643213450908661 + 100.0 * 8.354865074157715
Epoch 1010, val loss: 0.5195165276527405
Epoch 1020, training loss: 835.875 = 0.4634387195110321 + 100.0 * 8.35411548614502
Epoch 1020, val loss: 0.5190007090568542
Epoch 1030, training loss: 835.6954956054688 = 0.46257999539375305 + 100.0 * 8.35232925415039
Epoch 1030, val loss: 0.5184858441352844
Epoch 1040, training loss: 835.58154296875 = 0.46176785230636597 + 100.0 * 8.351197242736816
Epoch 1040, val loss: 0.5180332660675049
Epoch 1050, training loss: 835.6092529296875 = 0.4609696567058563 + 100.0 * 8.351482391357422
Epoch 1050, val loss: 0.5175765156745911
Epoch 1060, training loss: 835.9678955078125 = 0.4601263999938965 + 100.0 * 8.355077743530273
Epoch 1060, val loss: 0.517091691493988
Epoch 1070, training loss: 835.4461059570312 = 0.45920106768608093 + 100.0 * 8.349868774414062
Epoch 1070, val loss: 0.5166013836860657
Epoch 1080, training loss: 835.3920288085938 = 0.4583527445793152 + 100.0 * 8.349336624145508
Epoch 1080, val loss: 0.5160778760910034
Epoch 1090, training loss: 835.2256469726562 = 0.457545667886734 + 100.0 * 8.347681045532227
Epoch 1090, val loss: 0.5156433582305908
Epoch 1100, training loss: 835.1864624023438 = 0.456765353679657 + 100.0 * 8.347296714782715
Epoch 1100, val loss: 0.5152296423912048
Epoch 1110, training loss: 836.2362670898438 = 0.45594778656959534 + 100.0 * 8.357803344726562
Epoch 1110, val loss: 0.5147064328193665
Epoch 1120, training loss: 835.463623046875 = 0.4550051689147949 + 100.0 * 8.350086212158203
Epoch 1120, val loss: 0.5142104625701904
Epoch 1130, training loss: 835.0369873046875 = 0.45411667227745056 + 100.0 * 8.345829010009766
Epoch 1130, val loss: 0.5136396288871765
Epoch 1140, training loss: 835.4548950195312 = 0.453283429145813 + 100.0 * 8.350015640258789
Epoch 1140, val loss: 0.5131733417510986
Epoch 1150, training loss: 834.8881225585938 = 0.45237404108047485 + 100.0 * 8.34435749053955
Epoch 1150, val loss: 0.5126489996910095
Epoch 1160, training loss: 834.8340454101562 = 0.45150840282440186 + 100.0 * 8.343825340270996
Epoch 1160, val loss: 0.5121045112609863
Epoch 1170, training loss: 834.7980346679688 = 0.4506833255290985 + 100.0 * 8.343473434448242
Epoch 1170, val loss: 0.5116335153579712
Epoch 1180, training loss: 834.7842407226562 = 0.44986721873283386 + 100.0 * 8.343343734741211
Epoch 1180, val loss: 0.5111663937568665
Epoch 1190, training loss: 835.4913940429688 = 0.44901564717292786 + 100.0 * 8.350423812866211
Epoch 1190, val loss: 0.5106722116470337
Epoch 1200, training loss: 835.0775146484375 = 0.44807368516921997 + 100.0 * 8.346294403076172
Epoch 1200, val loss: 0.5102097392082214
Epoch 1210, training loss: 834.8106079101562 = 0.44714248180389404 + 100.0 * 8.343634605407715
Epoch 1210, val loss: 0.509575366973877
Epoch 1220, training loss: 834.6251831054688 = 0.4462452530860901 + 100.0 * 8.341789245605469
Epoch 1220, val loss: 0.509076714515686
Epoch 1230, training loss: 835.0349731445312 = 0.445359468460083 + 100.0 * 8.345895767211914
Epoch 1230, val loss: 0.508594810962677
Epoch 1240, training loss: 834.5081176757812 = 0.4443909525871277 + 100.0 * 8.34063720703125
Epoch 1240, val loss: 0.5079788565635681
Epoch 1250, training loss: 834.4766845703125 = 0.44346311688423157 + 100.0 * 8.34033203125
Epoch 1250, val loss: 0.5074236392974854
Epoch 1260, training loss: 834.5957641601562 = 0.44256114959716797 + 100.0 * 8.341531753540039
Epoch 1260, val loss: 0.5068633556365967
Epoch 1270, training loss: 834.6763916015625 = 0.4416241943836212 + 100.0 * 8.342347145080566
Epoch 1270, val loss: 0.5063474774360657
Epoch 1280, training loss: 834.5513916015625 = 0.44067642092704773 + 100.0 * 8.341107368469238
Epoch 1280, val loss: 0.5057786703109741
Epoch 1290, training loss: 834.8115234375 = 0.4397241771221161 + 100.0 * 8.343718528747559
Epoch 1290, val loss: 0.5052212476730347
Epoch 1300, training loss: 834.4451293945312 = 0.4387355446815491 + 100.0 * 8.34006404876709
Epoch 1300, val loss: 0.504683792591095
Epoch 1310, training loss: 834.4401245117188 = 0.437788188457489 + 100.0 * 8.340023040771484
Epoch 1310, val loss: 0.5041102170944214
Epoch 1320, training loss: 834.3094482421875 = 0.43682387471199036 + 100.0 * 8.338726043701172
Epoch 1320, val loss: 0.5034914016723633
Epoch 1330, training loss: 834.2490234375 = 0.4358745515346527 + 100.0 * 8.33813190460205
Epoch 1330, val loss: 0.5028836727142334
Epoch 1340, training loss: 834.3587646484375 = 0.4349024295806885 + 100.0 * 8.339239120483398
Epoch 1340, val loss: 0.5023195147514343
Epoch 1350, training loss: 834.8135986328125 = 0.4338909983634949 + 100.0 * 8.343796730041504
Epoch 1350, val loss: 0.5017135143280029
Epoch 1360, training loss: 834.1796875 = 0.4328288435935974 + 100.0 * 8.337469100952148
Epoch 1360, val loss: 0.501002848148346
Epoch 1370, training loss: 834.0704956054688 = 0.43181782960891724 + 100.0 * 8.336386680603027
Epoch 1370, val loss: 0.5004750490188599
Epoch 1380, training loss: 833.9872436523438 = 0.4308271110057831 + 100.0 * 8.335563659667969
Epoch 1380, val loss: 0.49987420439720154
Epoch 1390, training loss: 834.4228515625 = 0.42982280254364014 + 100.0 * 8.339930534362793
Epoch 1390, val loss: 0.49924057722091675
Epoch 1400, training loss: 834.1099853515625 = 0.4287387728691101 + 100.0 * 8.336812973022461
Epoch 1400, val loss: 0.498572438955307
Epoch 1410, training loss: 834.2091064453125 = 0.4276505708694458 + 100.0 * 8.337814331054688
Epoch 1410, val loss: 0.4979022741317749
Epoch 1420, training loss: 834.0762329101562 = 0.4265580177307129 + 100.0 * 8.336496353149414
Epoch 1420, val loss: 0.4972732365131378
Epoch 1430, training loss: 833.9981689453125 = 0.42542460560798645 + 100.0 * 8.33572769165039
Epoch 1430, val loss: 0.49652284383773804
Epoch 1440, training loss: 833.751220703125 = 0.4242928922176361 + 100.0 * 8.333269119262695
Epoch 1440, val loss: 0.49585264921188354
Epoch 1450, training loss: 833.7029418945312 = 0.42320963740348816 + 100.0 * 8.332797050476074
Epoch 1450, val loss: 0.495242714881897
Epoch 1460, training loss: 833.674072265625 = 0.422148197889328 + 100.0 * 8.33251953125
Epoch 1460, val loss: 0.49462518095970154
Epoch 1470, training loss: 833.8187866210938 = 0.42106834053993225 + 100.0 * 8.333976745605469
Epoch 1470, val loss: 0.4939868450164795
Epoch 1480, training loss: 834.0382080078125 = 0.4198671579360962 + 100.0 * 8.336183547973633
Epoch 1480, val loss: 0.4932422935962677
Epoch 1490, training loss: 833.6357421875 = 0.41860511898994446 + 100.0 * 8.332171440124512
Epoch 1490, val loss: 0.4923902750015259
Epoch 1500, training loss: 833.6797485351562 = 0.4174204170703888 + 100.0 * 8.332623481750488
Epoch 1500, val loss: 0.4917745590209961
Epoch 1510, training loss: 833.5542602539062 = 0.41628095507621765 + 100.0 * 8.331379890441895
Epoch 1510, val loss: 0.49112021923065186
Epoch 1520, training loss: 833.5184326171875 = 0.4151430130004883 + 100.0 * 8.331032752990723
Epoch 1520, val loss: 0.4904831647872925
Epoch 1530, training loss: 834.0225219726562 = 0.4139898121356964 + 100.0 * 8.336085319519043
Epoch 1530, val loss: 0.4899269938468933
Epoch 1540, training loss: 834.1495361328125 = 0.4127303659915924 + 100.0 * 8.33736801147461
Epoch 1540, val loss: 0.4887940287590027
Epoch 1550, training loss: 833.640380859375 = 0.41142013669013977 + 100.0 * 8.332289695739746
Epoch 1550, val loss: 0.488303542137146
Epoch 1560, training loss: 833.3865966796875 = 0.41016489267349243 + 100.0 * 8.329764366149902
Epoch 1560, val loss: 0.4875275492668152
Epoch 1570, training loss: 833.4022216796875 = 0.4089486002922058 + 100.0 * 8.329933166503906
Epoch 1570, val loss: 0.48678678274154663
Epoch 1580, training loss: 834.1827392578125 = 0.40771743655204773 + 100.0 * 8.337750434875488
Epoch 1580, val loss: 0.48618483543395996
Epoch 1590, training loss: 833.4940185546875 = 0.406353235244751 + 100.0 * 8.330876350402832
Epoch 1590, val loss: 0.48530542850494385
Epoch 1600, training loss: 833.396240234375 = 0.40504202246665955 + 100.0 * 8.329912185668945
Epoch 1600, val loss: 0.48456400632858276
Epoch 1610, training loss: 833.2841186523438 = 0.4037655293941498 + 100.0 * 8.328804016113281
Epoch 1610, val loss: 0.4839017689228058
Epoch 1620, training loss: 833.2373046875 = 0.40248915553092957 + 100.0 * 8.328348159790039
Epoch 1620, val loss: 0.48322373628616333
Epoch 1630, training loss: 833.516357421875 = 0.40120357275009155 + 100.0 * 8.331151962280273
Epoch 1630, val loss: 0.48248282074928284
Epoch 1640, training loss: 833.373046875 = 0.3998088836669922 + 100.0 * 8.329732894897461
Epoch 1640, val loss: 0.48180338740348816
Epoch 1650, training loss: 833.4795532226562 = 0.3984036445617676 + 100.0 * 8.330811500549316
Epoch 1650, val loss: 0.48097464442253113
Epoch 1660, training loss: 833.2421264648438 = 0.39701753854751587 + 100.0 * 8.328451156616211
Epoch 1660, val loss: 0.48027220368385315
Epoch 1670, training loss: 833.1439819335938 = 0.3956810235977173 + 100.0 * 8.327483177185059
Epoch 1670, val loss: 0.47955119609832764
Epoch 1680, training loss: 833.140869140625 = 0.39434614777565 + 100.0 * 8.327465057373047
Epoch 1680, val loss: 0.47886645793914795
Epoch 1690, training loss: 833.34423828125 = 0.3929908871650696 + 100.0 * 8.329512596130371
Epoch 1690, val loss: 0.47816193103790283
Epoch 1700, training loss: 833.2453002929688 = 0.39158448576927185 + 100.0 * 8.328536987304688
Epoch 1700, val loss: 0.4774809777736664
Epoch 1710, training loss: 833.7509155273438 = 0.39014923572540283 + 100.0 * 8.33360767364502
Epoch 1710, val loss: 0.47675320506095886
Epoch 1720, training loss: 833.0909423828125 = 0.38868236541748047 + 100.0 * 8.327022552490234
Epoch 1720, val loss: 0.47594451904296875
Epoch 1730, training loss: 832.9454956054688 = 0.3872809112071991 + 100.0 * 8.325582504272461
Epoch 1730, val loss: 0.47532543540000916
Epoch 1740, training loss: 832.9268188476562 = 0.3859143853187561 + 100.0 * 8.325408935546875
Epoch 1740, val loss: 0.4746381342411041
Epoch 1750, training loss: 832.9713134765625 = 0.3845486640930176 + 100.0 * 8.325867652893066
Epoch 1750, val loss: 0.4740722179412842
Epoch 1760, training loss: 833.6776123046875 = 0.3831396996974945 + 100.0 * 8.332944869995117
Epoch 1760, val loss: 0.47339922189712524
Epoch 1770, training loss: 833.2095947265625 = 0.3816568851470947 + 100.0 * 8.328279495239258
Epoch 1770, val loss: 0.4727069139480591
Epoch 1780, training loss: 833.1298217773438 = 0.380201518535614 + 100.0 * 8.327496528625488
Epoch 1780, val loss: 0.4720677435398102
Epoch 1790, training loss: 833.03369140625 = 0.3787446618080139 + 100.0 * 8.326549530029297
Epoch 1790, val loss: 0.4714266359806061
Epoch 1800, training loss: 832.8012084960938 = 0.3773046135902405 + 100.0 * 8.324238777160645
Epoch 1800, val loss: 0.4708230495452881
Epoch 1810, training loss: 832.8229370117188 = 0.3758973777294159 + 100.0 * 8.324470520019531
Epoch 1810, val loss: 0.47023895382881165
Epoch 1820, training loss: 832.8843994140625 = 0.37448611855506897 + 100.0 * 8.325098991394043
Epoch 1820, val loss: 0.46966472268104553
Epoch 1830, training loss: 833.2138671875 = 0.37304428219795227 + 100.0 * 8.328408241271973
Epoch 1830, val loss: 0.4691489636898041
Epoch 1840, training loss: 833.50244140625 = 0.371559202671051 + 100.0 * 8.331308364868164
Epoch 1840, val loss: 0.4686262905597687
Epoch 1850, training loss: 832.8902587890625 = 0.37001875042915344 + 100.0 * 8.325202941894531
Epoch 1850, val loss: 0.4678459167480469
Epoch 1860, training loss: 832.7557983398438 = 0.3685418665409088 + 100.0 * 8.323872566223145
Epoch 1860, val loss: 0.4673609733581543
Epoch 1870, training loss: 832.6620483398438 = 0.3671216368675232 + 100.0 * 8.322949409484863
Epoch 1870, val loss: 0.4669124484062195
Epoch 1880, training loss: 832.6124267578125 = 0.36572206020355225 + 100.0 * 8.322466850280762
Epoch 1880, val loss: 0.4664612114429474
Epoch 1890, training loss: 832.6074829101562 = 0.3643202781677246 + 100.0 * 8.322431564331055
Epoch 1890, val loss: 0.46604016423225403
Epoch 1900, training loss: 833.4869995117188 = 0.36290842294692993 + 100.0 * 8.3312406539917
Epoch 1900, val loss: 0.46578285098075867
Epoch 1910, training loss: 832.8455200195312 = 0.36139020323753357 + 100.0 * 8.324841499328613
Epoch 1910, val loss: 0.4649134576320648
Epoch 1920, training loss: 832.7365112304688 = 0.3599201738834381 + 100.0 * 8.323765754699707
Epoch 1920, val loss: 0.46472659707069397
Epoch 1930, training loss: 832.7896118164062 = 0.358479380607605 + 100.0 * 8.324311256408691
Epoch 1930, val loss: 0.46412402391433716
Epoch 1940, training loss: 832.6122436523438 = 0.35703492164611816 + 100.0 * 8.322551727294922
Epoch 1940, val loss: 0.4638451933860779
Epoch 1950, training loss: 832.4627685546875 = 0.35561054944992065 + 100.0 * 8.32107162475586
Epoch 1950, val loss: 0.4634409248828888
Epoch 1960, training loss: 832.6484375 = 0.3542100191116333 + 100.0 * 8.322942733764648
Epoch 1960, val loss: 0.4632180333137512
Epoch 1970, training loss: 832.8880615234375 = 0.352760910987854 + 100.0 * 8.325352668762207
Epoch 1970, val loss: 0.46270841360092163
Epoch 1980, training loss: 832.4667358398438 = 0.35127055644989014 + 100.0 * 8.321154594421387
Epoch 1980, val loss: 0.46231937408447266
Epoch 1990, training loss: 832.41943359375 = 0.3498319685459137 + 100.0 * 8.320695877075195
Epoch 1990, val loss: 0.46205684542655945
Epoch 2000, training loss: 832.415771484375 = 0.3484306037425995 + 100.0 * 8.320672988891602
Epoch 2000, val loss: 0.4617711901664734
Epoch 2010, training loss: 833.1663818359375 = 0.3470269441604614 + 100.0 * 8.328193664550781
Epoch 2010, val loss: 0.4615340232849121
Epoch 2020, training loss: 832.4093017578125 = 0.3455308973789215 + 100.0 * 8.320637702941895
Epoch 2020, val loss: 0.4610116481781006
Epoch 2030, training loss: 832.3192749023438 = 0.3440861999988556 + 100.0 * 8.319751739501953
Epoch 2030, val loss: 0.4607630968093872
Epoch 2040, training loss: 832.3988647460938 = 0.34267857670783997 + 100.0 * 8.320562362670898
Epoch 2040, val loss: 0.46059489250183105
Epoch 2050, training loss: 832.71142578125 = 0.3412674367427826 + 100.0 * 8.323701858520508
Epoch 2050, val loss: 0.4602493941783905
Epoch 2060, training loss: 832.3048095703125 = 0.3398256301879883 + 100.0 * 8.319649696350098
Epoch 2060, val loss: 0.4600652754306793
Epoch 2070, training loss: 832.3970336914062 = 0.338412344455719 + 100.0 * 8.320586204528809
Epoch 2070, val loss: 0.4597196877002716
Epoch 2080, training loss: 832.4840698242188 = 0.33697080612182617 + 100.0 * 8.321471214294434
Epoch 2080, val loss: 0.4595281779766083
Epoch 2090, training loss: 832.3977661132812 = 0.3355165123939514 + 100.0 * 8.320622444152832
Epoch 2090, val loss: 0.45937544107437134
Epoch 2100, training loss: 832.5562744140625 = 0.3340839445590973 + 100.0 * 8.322221755981445
Epoch 2100, val loss: 0.4590466320514679
Epoch 2110, training loss: 832.3077392578125 = 0.33261969685554504 + 100.0 * 8.319750785827637
Epoch 2110, val loss: 0.4590545892715454
Epoch 2120, training loss: 832.2357788085938 = 0.33116453886032104 + 100.0 * 8.319046020507812
Epoch 2120, val loss: 0.45874378085136414
Epoch 2130, training loss: 832.1566772460938 = 0.329754501581192 + 100.0 * 8.318268775939941
Epoch 2130, val loss: 0.4586792588233948
Epoch 2140, training loss: 832.6276245117188 = 0.32835453748703003 + 100.0 * 8.322992324829102
Epoch 2140, val loss: 0.45851030945777893
Epoch 2150, training loss: 832.1503295898438 = 0.326880544424057 + 100.0 * 8.31823444366455
Epoch 2150, val loss: 0.4584527313709259
Epoch 2160, training loss: 832.1593627929688 = 0.3254427909851074 + 100.0 * 8.318339347839355
Epoch 2160, val loss: 0.45818203687667847
Epoch 2170, training loss: 832.1053466796875 = 0.3240399658679962 + 100.0 * 8.3178129196167
Epoch 2170, val loss: 0.45819365978240967
Epoch 2180, training loss: 832.1762084960938 = 0.32265210151672363 + 100.0 * 8.318535804748535
Epoch 2180, val loss: 0.45820480585098267
Epoch 2190, training loss: 832.4584350585938 = 0.3212428390979767 + 100.0 * 8.321372032165527
Epoch 2190, val loss: 0.45808839797973633
Epoch 2200, training loss: 832.1846313476562 = 0.3198162615299225 + 100.0 * 8.318648338317871
Epoch 2200, val loss: 0.457886278629303
Epoch 2210, training loss: 832.1452026367188 = 0.31841421127319336 + 100.0 * 8.318267822265625
Epoch 2210, val loss: 0.4579058885574341
Epoch 2220, training loss: 832.3328857421875 = 0.3170210123062134 + 100.0 * 8.320158958435059
Epoch 2220, val loss: 0.45792156457901
Epoch 2230, training loss: 832.0271606445312 = 0.3156100809574127 + 100.0 * 8.317115783691406
Epoch 2230, val loss: 0.4580233693122864
Epoch 2240, training loss: 832.283447265625 = 0.3142275810241699 + 100.0 * 8.319692611694336
Epoch 2240, val loss: 0.4582049548625946
Epoch 2250, training loss: 832.1383666992188 = 0.31281718611717224 + 100.0 * 8.318255424499512
Epoch 2250, val loss: 0.45788177847862244
Epoch 2260, training loss: 831.9274291992188 = 0.311428964138031 + 100.0 * 8.316160202026367
Epoch 2260, val loss: 0.4578058421611786
Epoch 2270, training loss: 832.209228515625 = 0.31007471680641174 + 100.0 * 8.318991661071777
Epoch 2270, val loss: 0.4580787122249603
Epoch 2280, training loss: 832.254638671875 = 0.3086814880371094 + 100.0 * 8.319459915161133
Epoch 2280, val loss: 0.45805084705352783
Epoch 2290, training loss: 831.9349975585938 = 0.3072830140590668 + 100.0 * 8.316276550292969
Epoch 2290, val loss: 0.4579225480556488
Epoch 2300, training loss: 831.84375 = 0.30592477321624756 + 100.0 * 8.315378189086914
Epoch 2300, val loss: 0.45795106887817383
Epoch 2310, training loss: 831.8780517578125 = 0.3045785129070282 + 100.0 * 8.31573486328125
Epoch 2310, val loss: 0.45812365412712097
Epoch 2320, training loss: 832.4724731445312 = 0.3032293915748596 + 100.0 * 8.32169246673584
Epoch 2320, val loss: 0.45826995372772217
Epoch 2330, training loss: 832.0316772460938 = 0.3018389940261841 + 100.0 * 8.317298889160156
Epoch 2330, val loss: 0.4582032263278961
Epoch 2340, training loss: 832.20556640625 = 0.30044791102409363 + 100.0 * 8.319050788879395
Epoch 2340, val loss: 0.4583345949649811
Epoch 2350, training loss: 831.8281860351562 = 0.2990674674510956 + 100.0 * 8.315291404724121
Epoch 2350, val loss: 0.45857393741607666
Epoch 2360, training loss: 831.7421875 = 0.297721803188324 + 100.0 * 8.314444541931152
Epoch 2360, val loss: 0.4586578607559204
Epoch 2370, training loss: 831.72998046875 = 0.29639098048210144 + 100.0 * 8.314335823059082
Epoch 2370, val loss: 0.45881426334381104
Epoch 2380, training loss: 831.8450927734375 = 0.29506155848503113 + 100.0 * 8.315500259399414
Epoch 2380, val loss: 0.45892971754074097
Epoch 2390, training loss: 832.0867309570312 = 0.2937280535697937 + 100.0 * 8.317930221557617
Epoch 2390, val loss: 0.45893266797065735
Epoch 2400, training loss: 832.2737426757812 = 0.29235929250717163 + 100.0 * 8.31981372833252
Epoch 2400, val loss: 0.4592330753803253
Epoch 2410, training loss: 831.8685913085938 = 0.29098576307296753 + 100.0 * 8.315775871276855
Epoch 2410, val loss: 0.4596922993659973
Epoch 2420, training loss: 831.709228515625 = 0.28965333104133606 + 100.0 * 8.31419563293457
Epoch 2420, val loss: 0.45962968468666077
Epoch 2430, training loss: 831.7200317382812 = 0.28833580017089844 + 100.0 * 8.314316749572754
Epoch 2430, val loss: 0.4600518047809601
Epoch 2440, training loss: 831.95361328125 = 0.28702133893966675 + 100.0 * 8.316665649414062
Epoch 2440, val loss: 0.4601001441478729
Epoch 2450, training loss: 831.8858032226562 = 0.2856915593147278 + 100.0 * 8.316000938415527
Epoch 2450, val loss: 0.4607289731502533
Epoch 2460, training loss: 831.6494750976562 = 0.2843487858772278 + 100.0 * 8.313651084899902
Epoch 2460, val loss: 0.4606776535511017
Epoch 2470, training loss: 831.8826293945312 = 0.2830411195755005 + 100.0 * 8.315996170043945
Epoch 2470, val loss: 0.46076756715774536
Epoch 2480, training loss: 831.7272338867188 = 0.281717985868454 + 100.0 * 8.314455032348633
Epoch 2480, val loss: 0.46133577823638916
Epoch 2490, training loss: 831.548583984375 = 0.2804000675678253 + 100.0 * 8.312682151794434
Epoch 2490, val loss: 0.46150991320610046
Epoch 2500, training loss: 831.5139770507812 = 0.27910473942756653 + 100.0 * 8.312348365783691
Epoch 2500, val loss: 0.46177995204925537
Epoch 2510, training loss: 831.5476684570312 = 0.2778193950653076 + 100.0 * 8.312698364257812
Epoch 2510, val loss: 0.4620437026023865
Epoch 2520, training loss: 831.857177734375 = 0.2765273451805115 + 100.0 * 8.31580638885498
Epoch 2520, val loss: 0.46235501766204834
Epoch 2530, training loss: 831.9393920898438 = 0.27521008253097534 + 100.0 * 8.316641807556152
Epoch 2530, val loss: 0.4628194570541382
Epoch 2540, training loss: 831.5591430664062 = 0.273856520652771 + 100.0 * 8.31285285949707
Epoch 2540, val loss: 0.4629814624786377
Epoch 2550, training loss: 831.5165405273438 = 0.27253496646881104 + 100.0 * 8.312439918518066
Epoch 2550, val loss: 0.46343278884887695
Epoch 2560, training loss: 831.4290161132812 = 0.27123191952705383 + 100.0 * 8.311577796936035
Epoch 2560, val loss: 0.4636319577693939
Epoch 2570, training loss: 831.5145263671875 = 0.26994913816452026 + 100.0 * 8.312445640563965
Epoch 2570, val loss: 0.46423640847206116
Epoch 2580, training loss: 831.968505859375 = 0.2686573565006256 + 100.0 * 8.316998481750488
Epoch 2580, val loss: 0.46461403369903564
Epoch 2590, training loss: 831.565673828125 = 0.26733946800231934 + 100.0 * 8.312983512878418
Epoch 2590, val loss: 0.46466943621635437
Epoch 2600, training loss: 831.3809204101562 = 0.2660326361656189 + 100.0 * 8.311148643493652
Epoch 2600, val loss: 0.4652864933013916
Epoch 2610, training loss: 831.3226928710938 = 0.2647421360015869 + 100.0 * 8.310579299926758
Epoch 2610, val loss: 0.4655677378177643
Epoch 2620, training loss: 831.7877807617188 = 0.2634791433811188 + 100.0 * 8.315242767333984
Epoch 2620, val loss: 0.4656807780265808
Epoch 2630, training loss: 831.4024658203125 = 0.2621651887893677 + 100.0 * 8.311403274536133
Epoch 2630, val loss: 0.4669745862483978
Epoch 2640, training loss: 831.5232543945312 = 0.2608467936515808 + 100.0 * 8.312623977661133
Epoch 2640, val loss: 0.4669656753540039
Epoch 2650, training loss: 831.5332641601562 = 0.2595771253108978 + 100.0 * 8.312736511230469
Epoch 2650, val loss: 0.4675201177597046
Epoch 2660, training loss: 832.0582885742188 = 0.25828975439071655 + 100.0 * 8.317999839782715
Epoch 2660, val loss: 0.467621386051178
Epoch 2670, training loss: 831.3077392578125 = 0.2569786310195923 + 100.0 * 8.310507774353027
Epoch 2670, val loss: 0.46831566095352173
Epoch 2680, training loss: 831.2864990234375 = 0.2556995749473572 + 100.0 * 8.310308456420898
Epoch 2680, val loss: 0.46853628754615784
Epoch 2690, training loss: 831.2188110351562 = 0.2544333338737488 + 100.0 * 8.309643745422363
Epoch 2690, val loss: 0.4691591262817383
Epoch 2700, training loss: 831.2109375 = 0.2531762421131134 + 100.0 * 8.309577941894531
Epoch 2700, val loss: 0.46975597739219666
Epoch 2710, training loss: 831.2872924804688 = 0.25191330909729004 + 100.0 * 8.310354232788086
Epoch 2710, val loss: 0.47006756067276
Epoch 2720, training loss: 831.8329467773438 = 0.2506523132324219 + 100.0 * 8.31582260131836
Epoch 2720, val loss: 0.4703826606273651
Epoch 2730, training loss: 831.3804321289062 = 0.24934129416942596 + 100.0 * 8.311310768127441
Epoch 2730, val loss: 0.4709731340408325
Epoch 2740, training loss: 831.633544921875 = 0.2480688840150833 + 100.0 * 8.313855171203613
Epoch 2740, val loss: 0.47136083245277405
Epoch 2750, training loss: 831.2144775390625 = 0.24674595892429352 + 100.0 * 8.309677124023438
Epoch 2750, val loss: 0.4721594750881195
Epoch 2760, training loss: 831.3125610351562 = 0.2454659342765808 + 100.0 * 8.310670852661133
Epoch 2760, val loss: 0.4725596010684967
Epoch 2770, training loss: 831.2904052734375 = 0.24418818950653076 + 100.0 * 8.31046199798584
Epoch 2770, val loss: 0.473040908575058
Epoch 2780, training loss: 831.172119140625 = 0.24290041625499725 + 100.0 * 8.30929183959961
Epoch 2780, val loss: 0.47382426261901855
Epoch 2790, training loss: 831.1497192382812 = 0.24162645637989044 + 100.0 * 8.309081077575684
Epoch 2790, val loss: 0.47455355525016785
Epoch 2800, training loss: 831.5585327148438 = 0.24039481580257416 + 100.0 * 8.313180923461914
Epoch 2800, val loss: 0.4755794405937195
Epoch 2810, training loss: 831.3154296875 = 0.2390739917755127 + 100.0 * 8.310763359069824
Epoch 2810, val loss: 0.4754202365875244
Epoch 2820, training loss: 831.1219482421875 = 0.23778195679187775 + 100.0 * 8.308841705322266
Epoch 2820, val loss: 0.4764024317264557
Epoch 2830, training loss: 831.145263671875 = 0.2365170419216156 + 100.0 * 8.309087753295898
Epoch 2830, val loss: 0.47690561413764954
Epoch 2840, training loss: 831.3294067382812 = 0.23525205254554749 + 100.0 * 8.310941696166992
Epoch 2840, val loss: 0.4773172438144684
Epoch 2850, training loss: 831.2344970703125 = 0.2339952141046524 + 100.0 * 8.310005187988281
Epoch 2850, val loss: 0.4783855378627777
Epoch 2860, training loss: 831.1644897460938 = 0.23272442817687988 + 100.0 * 8.309317588806152
Epoch 2860, val loss: 0.4789590537548065
Epoch 2870, training loss: 831.3644409179688 = 0.23146092891693115 + 100.0 * 8.31132984161377
Epoch 2870, val loss: 0.47941896319389343
Epoch 2880, training loss: 831.2794799804688 = 0.23021017014980316 + 100.0 * 8.310492515563965
Epoch 2880, val loss: 0.47988513112068176
Epoch 2890, training loss: 831.0890502929688 = 0.2289445400238037 + 100.0 * 8.308601379394531
Epoch 2890, val loss: 0.48066532611846924
Epoch 2900, training loss: 831.0087890625 = 0.22770379483699799 + 100.0 * 8.30781078338623
Epoch 2900, val loss: 0.48124468326568604
Epoch 2910, training loss: 830.9624633789062 = 0.22647377848625183 + 100.0 * 8.30735969543457
Epoch 2910, val loss: 0.4819040596485138
Epoch 2920, training loss: 831.1211547851562 = 0.22525234520435333 + 100.0 * 8.308959007263184
Epoch 2920, val loss: 0.48229512572288513
Epoch 2930, training loss: 831.5368041992188 = 0.22404047846794128 + 100.0 * 8.313127517700195
Epoch 2930, val loss: 0.48319679498672485
Epoch 2940, training loss: 831.4171142578125 = 0.22277486324310303 + 100.0 * 8.311943054199219
Epoch 2940, val loss: 0.4842037260532379
Epoch 2950, training loss: 830.9822387695312 = 0.22151723504066467 + 100.0 * 8.307607650756836
Epoch 2950, val loss: 0.48473241925239563
Epoch 2960, training loss: 830.9356689453125 = 0.22029319405555725 + 100.0 * 8.307153701782227
Epoch 2960, val loss: 0.48566606640815735
Epoch 2970, training loss: 830.89697265625 = 0.2190679907798767 + 100.0 * 8.306778907775879
Epoch 2970, val loss: 0.48606929183006287
Epoch 2980, training loss: 830.9427490234375 = 0.21784617006778717 + 100.0 * 8.307249069213867
Epoch 2980, val loss: 0.48680379986763
Epoch 2990, training loss: 831.4249267578125 = 0.21662761270999908 + 100.0 * 8.31208324432373
Epoch 2990, val loss: 0.4875505864620209
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8041603247082698
0.8419184235311165
=== training gcn model ===
Epoch 0, training loss: 1059.3226318359375 = 1.0991266965866089 + 100.0 * 10.582235336303711
Epoch 0, val loss: 1.098114013671875
Epoch 10, training loss: 1059.1873779296875 = 1.0930376052856445 + 100.0 * 10.580944061279297
Epoch 10, val loss: 1.0920095443725586
Epoch 20, training loss: 1058.1231689453125 = 1.0857268571853638 + 100.0 * 10.570374488830566
Epoch 20, val loss: 1.084713339805603
Epoch 30, training loss: 1050.8951416015625 = 1.0768239498138428 + 100.0 * 10.498183250427246
Epoch 30, val loss: 1.0758451223373413
Epoch 40, training loss: 1017.8016357421875 = 1.0664660930633545 + 100.0 * 10.167351722717285
Epoch 40, val loss: 1.0657484531402588
Epoch 50, training loss: 957.896728515625 = 1.0557100772857666 + 100.0 * 9.56840991973877
Epoch 50, val loss: 1.0554530620574951
Epoch 60, training loss: 948.7994995117188 = 1.0475918054580688 + 100.0 * 9.477519035339355
Epoch 60, val loss: 1.0477346181869507
Epoch 70, training loss: 932.8561401367188 = 1.0412640571594238 + 100.0 * 9.318148612976074
Epoch 70, val loss: 1.0415558815002441
Epoch 80, training loss: 920.2557983398438 = 1.0352590084075928 + 100.0 * 9.192205429077148
Epoch 80, val loss: 1.0355459451675415
Epoch 90, training loss: 916.582763671875 = 1.0277447700500488 + 100.0 * 9.155550003051758
Epoch 90, val loss: 1.0280990600585938
Epoch 100, training loss: 909.0845336914062 = 1.0200724601745605 + 100.0 * 9.080644607543945
Epoch 100, val loss: 1.0207164287567139
Epoch 110, training loss: 899.1433715820312 = 1.0138969421386719 + 100.0 * 8.981294631958008
Epoch 110, val loss: 1.0149304866790771
Epoch 120, training loss: 893.2501831054688 = 1.0098235607147217 + 100.0 * 8.922403335571289
Epoch 120, val loss: 1.0109233856201172
Epoch 130, training loss: 886.41455078125 = 1.0051308870315552 + 100.0 * 8.854094505310059
Epoch 130, val loss: 1.006132960319519
Epoch 140, training loss: 878.5506591796875 = 1.0001288652420044 + 100.0 * 8.775505065917969
Epoch 140, val loss: 1.0006999969482422
Epoch 150, training loss: 872.8365478515625 = 0.9949538707733154 + 100.0 * 8.718416213989258
Epoch 150, val loss: 0.9956501722335815
Epoch 160, training loss: 868.82958984375 = 0.9887091517448425 + 100.0 * 8.6784086227417
Epoch 160, val loss: 0.9895475506782532
Epoch 170, training loss: 866.025390625 = 0.9810543060302734 + 100.0 * 8.650443077087402
Epoch 170, val loss: 0.9818161725997925
Epoch 180, training loss: 863.8412475585938 = 0.9724672436714172 + 100.0 * 8.628687858581543
Epoch 180, val loss: 0.9732506275177002
Epoch 190, training loss: 861.6049194335938 = 0.9635684490203857 + 100.0 * 8.606413841247559
Epoch 190, val loss: 0.9644032120704651
Epoch 200, training loss: 859.6930541992188 = 0.9544655084609985 + 100.0 * 8.587386131286621
Epoch 200, val loss: 0.9554047584533691
Epoch 210, training loss: 857.6312255859375 = 0.9449421763420105 + 100.0 * 8.566863059997559
Epoch 210, val loss: 0.9459505081176758
Epoch 220, training loss: 855.9190063476562 = 0.9347701668739319 + 100.0 * 8.549842834472656
Epoch 220, val loss: 0.9358291625976562
Epoch 230, training loss: 854.553955078125 = 0.9237542152404785 + 100.0 * 8.536301612854004
Epoch 230, val loss: 0.9248695969581604
Epoch 240, training loss: 853.1677856445312 = 0.9120430946350098 + 100.0 * 8.522557258605957
Epoch 240, val loss: 0.9132641553878784
Epoch 250, training loss: 852.0336303710938 = 0.8999239802360535 + 100.0 * 8.511337280273438
Epoch 250, val loss: 0.9012026786804199
Epoch 260, training loss: 851.4804077148438 = 0.8873388767242432 + 100.0 * 8.50593090057373
Epoch 260, val loss: 0.8887696862220764
Epoch 270, training loss: 850.5960693359375 = 0.8743027448654175 + 100.0 * 8.497217178344727
Epoch 270, val loss: 0.8758092522621155
Epoch 280, training loss: 849.7171630859375 = 0.8610453009605408 + 100.0 * 8.488561630249023
Epoch 280, val loss: 0.8627105355262756
Epoch 290, training loss: 849.0296020507812 = 0.847714900970459 + 100.0 * 8.481819152832031
Epoch 290, val loss: 0.849563479423523
Epoch 300, training loss: 849.0094604492188 = 0.8342694640159607 + 100.0 * 8.481751441955566
Epoch 300, val loss: 0.8364097476005554
Epoch 310, training loss: 847.8652954101562 = 0.8207919597625732 + 100.0 * 8.470444679260254
Epoch 310, val loss: 0.8230438232421875
Epoch 320, training loss: 847.3402709960938 = 0.8073920607566833 + 100.0 * 8.46532917022705
Epoch 320, val loss: 0.8098405599594116
Epoch 330, training loss: 847.2694702148438 = 0.7940943241119385 + 100.0 * 8.464754104614258
Epoch 330, val loss: 0.7967211008071899
Epoch 340, training loss: 846.5244750976562 = 0.7806755900382996 + 100.0 * 8.457437515258789
Epoch 340, val loss: 0.7836407423019409
Epoch 350, training loss: 845.9560546875 = 0.767425537109375 + 100.0 * 8.451886177062988
Epoch 350, val loss: 0.7707780003547668
Epoch 360, training loss: 845.4680786132812 = 0.7544319033622742 + 100.0 * 8.447135925292969
Epoch 360, val loss: 0.7582120299339294
Epoch 370, training loss: 845.2854614257812 = 0.7416142225265503 + 100.0 * 8.445438385009766
Epoch 370, val loss: 0.7458058595657349
Epoch 380, training loss: 844.78173828125 = 0.7289451360702515 + 100.0 * 8.44052791595459
Epoch 380, val loss: 0.733718991279602
Epoch 390, training loss: 844.3253784179688 = 0.7167580723762512 + 100.0 * 8.436086654663086
Epoch 390, val loss: 0.7220974564552307
Epoch 400, training loss: 843.9818725585938 = 0.7050037384033203 + 100.0 * 8.432768821716309
Epoch 400, val loss: 0.7109377384185791
Epoch 410, training loss: 844.1807250976562 = 0.693661630153656 + 100.0 * 8.434870719909668
Epoch 410, val loss: 0.7003080248832703
Epoch 420, training loss: 843.3956298828125 = 0.6827501654624939 + 100.0 * 8.427128791809082
Epoch 420, val loss: 0.6900081634521484
Epoch 430, training loss: 843.3004150390625 = 0.6723997592926025 + 100.0 * 8.42628002166748
Epoch 430, val loss: 0.6803975701332092
Epoch 440, training loss: 842.8451538085938 = 0.6625873446464539 + 100.0 * 8.421825408935547
Epoch 440, val loss: 0.671432614326477
Epoch 450, training loss: 842.6419677734375 = 0.6533635258674622 + 100.0 * 8.419885635375977
Epoch 450, val loss: 0.6629689931869507
Epoch 460, training loss: 842.304443359375 = 0.6447566747665405 + 100.0 * 8.416596412658691
Epoch 460, val loss: 0.655099630355835
Epoch 470, training loss: 842.2675170898438 = 0.6366778612136841 + 100.0 * 8.416308403015137
Epoch 470, val loss: 0.6478697657585144
Epoch 480, training loss: 841.7236938476562 = 0.6290978789329529 + 100.0 * 8.410945892333984
Epoch 480, val loss: 0.6411688327789307
Epoch 490, training loss: 841.5061645507812 = 0.6221385598182678 + 100.0 * 8.40884017944336
Epoch 490, val loss: 0.6349993944168091
Epoch 500, training loss: 841.6964111328125 = 0.6155609488487244 + 100.0 * 8.410808563232422
Epoch 500, val loss: 0.6292456984519958
Epoch 510, training loss: 841.235595703125 = 0.6094212532043457 + 100.0 * 8.406261444091797
Epoch 510, val loss: 0.6239525079727173
Epoch 520, training loss: 840.8995971679688 = 0.6038046479225159 + 100.0 * 8.402957916259766
Epoch 520, val loss: 0.619146466255188
Epoch 530, training loss: 840.7001342773438 = 0.5986351370811462 + 100.0 * 8.401015281677246
Epoch 530, val loss: 0.6147644519805908
Epoch 540, training loss: 841.2149047851562 = 0.5937930941581726 + 100.0 * 8.406210899353027
Epoch 540, val loss: 0.6108101606369019
Epoch 550, training loss: 840.5306396484375 = 0.5892273187637329 + 100.0 * 8.3994140625
Epoch 550, val loss: 0.6068872213363647
Epoch 560, training loss: 840.3111572265625 = 0.5850237607955933 + 100.0 * 8.397261619567871
Epoch 560, val loss: 0.6034853458404541
Epoch 570, training loss: 840.5712280273438 = 0.5811399817466736 + 100.0 * 8.399901390075684
Epoch 570, val loss: 0.6003093719482422
Epoch 580, training loss: 839.9820556640625 = 0.5774792432785034 + 100.0 * 8.39404582977295
Epoch 580, val loss: 0.5974610447883606
Epoch 590, training loss: 839.8280639648438 = 0.574130654335022 + 100.0 * 8.392539024353027
Epoch 590, val loss: 0.5946545600891113
Epoch 600, training loss: 839.9815063476562 = 0.570962131023407 + 100.0 * 8.394104957580566
Epoch 600, val loss: 0.5922321081161499
Epoch 610, training loss: 839.7666015625 = 0.5679758787155151 + 100.0 * 8.391985893249512
Epoch 610, val loss: 0.5897930860519409
Epoch 620, training loss: 839.541015625 = 0.5651227831840515 + 100.0 * 8.389759063720703
Epoch 620, val loss: 0.5875452160835266
Epoch 630, training loss: 839.261962890625 = 0.5624595880508423 + 100.0 * 8.386995315551758
Epoch 630, val loss: 0.5856236219406128
Epoch 640, training loss: 839.141357421875 = 0.55999755859375 + 100.0 * 8.38581371307373
Epoch 640, val loss: 0.5836441516876221
Epoch 650, training loss: 839.2705688476562 = 0.5576304793357849 + 100.0 * 8.387129783630371
Epoch 650, val loss: 0.5818666815757751
Epoch 660, training loss: 838.8990478515625 = 0.555359423160553 + 100.0 * 8.383437156677246
Epoch 660, val loss: 0.5800780057907104
Epoch 670, training loss: 838.7677001953125 = 0.553214967250824 + 100.0 * 8.382144927978516
Epoch 670, val loss: 0.5784050226211548
Epoch 680, training loss: 839.011474609375 = 0.5511483550071716 + 100.0 * 8.384603500366211
Epoch 680, val loss: 0.5768120288848877
Epoch 690, training loss: 838.7030639648438 = 0.5491114258766174 + 100.0 * 8.381539344787598
Epoch 690, val loss: 0.5754246115684509
Epoch 700, training loss: 838.5940551757812 = 0.5472128987312317 + 100.0 * 8.380468368530273
Epoch 700, val loss: 0.5739169120788574
Epoch 710, training loss: 838.35546875 = 0.545325517654419 + 100.0 * 8.378101348876953
Epoch 710, val loss: 0.5724522471427917
Epoch 720, training loss: 838.174072265625 = 0.5435687303543091 + 100.0 * 8.376304626464844
Epoch 720, val loss: 0.5711485147476196
Epoch 730, training loss: 838.050048828125 = 0.5419045090675354 + 100.0 * 8.375081062316895
Epoch 730, val loss: 0.5698990225791931
Epoch 740, training loss: 838.3473510742188 = 0.5403129458427429 + 100.0 * 8.378070831298828
Epoch 740, val loss: 0.5686519145965576
Epoch 750, training loss: 838.0134887695312 = 0.5386499762535095 + 100.0 * 8.374748229980469
Epoch 750, val loss: 0.5674564242362976
Epoch 760, training loss: 837.7727661132812 = 0.537110447883606 + 100.0 * 8.372356414794922
Epoch 760, val loss: 0.5663074851036072
Epoch 770, training loss: 837.6392822265625 = 0.535647451877594 + 100.0 * 8.371036529541016
Epoch 770, val loss: 0.5652180910110474
Epoch 780, training loss: 838.1817016601562 = 0.5342384576797485 + 100.0 * 8.376474380493164
Epoch 780, val loss: 0.564257800579071
Epoch 790, training loss: 837.696044921875 = 0.5327461361885071 + 100.0 * 8.37163257598877
Epoch 790, val loss: 0.5629757046699524
Epoch 800, training loss: 837.4498901367188 = 0.5313624143600464 + 100.0 * 8.369185447692871
Epoch 800, val loss: 0.561927318572998
Epoch 810, training loss: 837.5364990234375 = 0.5300098061561584 + 100.0 * 8.370064735412598
Epoch 810, val loss: 0.5609982013702393
Epoch 820, training loss: 837.2926635742188 = 0.5286728739738464 + 100.0 * 8.367639541625977
Epoch 820, val loss: 0.5599300861358643
Epoch 830, training loss: 837.2650146484375 = 0.527402937412262 + 100.0 * 8.367376327514648
Epoch 830, val loss: 0.5590094923973083
Epoch 840, training loss: 837.1041870117188 = 0.5261273980140686 + 100.0 * 8.3657808303833
Epoch 840, val loss: 0.5580770373344421
Epoch 850, training loss: 836.9959716796875 = 0.5248925089836121 + 100.0 * 8.364710807800293
Epoch 850, val loss: 0.5571402311325073
Epoch 860, training loss: 837.43701171875 = 0.5236656069755554 + 100.0 * 8.369132995605469
Epoch 860, val loss: 0.5561560392379761
Epoch 870, training loss: 837.3137817382812 = 0.5223389863967896 + 100.0 * 8.367914199829102
Epoch 870, val loss: 0.5553075671195984
Epoch 880, training loss: 836.8089599609375 = 0.5210813879966736 + 100.0 * 8.362878799438477
Epoch 880, val loss: 0.5544047951698303
Epoch 890, training loss: 836.64306640625 = 0.5199170112609863 + 100.0 * 8.361231803894043
Epoch 890, val loss: 0.5535264611244202
Epoch 900, training loss: 836.56396484375 = 0.5187858939170837 + 100.0 * 8.360451698303223
Epoch 900, val loss: 0.5527880191802979
Epoch 910, training loss: 836.604736328125 = 0.5176713466644287 + 100.0 * 8.360870361328125
Epoch 910, val loss: 0.5520195364952087
Epoch 920, training loss: 836.6243286132812 = 0.5164914131164551 + 100.0 * 8.361078262329102
Epoch 920, val loss: 0.5511510968208313
Epoch 930, training loss: 837.0368041992188 = 0.5152859091758728 + 100.0 * 8.365215301513672
Epoch 930, val loss: 0.5501458048820496
Epoch 940, training loss: 836.60400390625 = 0.5140264630317688 + 100.0 * 8.360899925231934
Epoch 940, val loss: 0.549418568611145
Epoch 950, training loss: 836.3006591796875 = 0.5128702521324158 + 100.0 * 8.357877731323242
Epoch 950, val loss: 0.5486012101173401
Epoch 960, training loss: 836.1787109375 = 0.5117827653884888 + 100.0 * 8.356669425964355
Epoch 960, val loss: 0.5478727221488953
Epoch 970, training loss: 836.0679931640625 = 0.5107147693634033 + 100.0 * 8.355572700500488
Epoch 970, val loss: 0.5470969676971436
Epoch 980, training loss: 836.260498046875 = 0.5096445083618164 + 100.0 * 8.357508659362793
Epoch 980, val loss: 0.5462796092033386
Epoch 990, training loss: 836.17529296875 = 0.5084313154220581 + 100.0 * 8.356668472290039
Epoch 990, val loss: 0.5455992817878723
Epoch 1000, training loss: 836.0436401367188 = 0.5072328448295593 + 100.0 * 8.355363845825195
Epoch 1000, val loss: 0.5446478128433228
Epoch 1010, training loss: 836.150146484375 = 0.5061062574386597 + 100.0 * 8.356440544128418
Epoch 1010, val loss: 0.5438476800918579
Epoch 1020, training loss: 835.847900390625 = 0.5049661993980408 + 100.0 * 8.353429794311523
Epoch 1020, val loss: 0.543022632598877
Epoch 1030, training loss: 835.7997436523438 = 0.5038540363311768 + 100.0 * 8.352958679199219
Epoch 1030, val loss: 0.5422515273094177
Epoch 1040, training loss: 836.4129028320312 = 0.502717912197113 + 100.0 * 8.359101295471191
Epoch 1040, val loss: 0.5414440631866455
Epoch 1050, training loss: 835.8359375 = 0.5015183687210083 + 100.0 * 8.353343963623047
Epoch 1050, val loss: 0.5406866073608398
Epoch 1060, training loss: 835.6096801757812 = 0.5003767609596252 + 100.0 * 8.351093292236328
Epoch 1060, val loss: 0.539860188961029
Epoch 1070, training loss: 835.5224609375 = 0.49927622079849243 + 100.0 * 8.350232124328613
Epoch 1070, val loss: 0.5390961170196533
Epoch 1080, training loss: 835.5556030273438 = 0.49816790223121643 + 100.0 * 8.350574493408203
Epoch 1080, val loss: 0.5382618308067322
Epoch 1090, training loss: 835.983154296875 = 0.4969635009765625 + 100.0 * 8.354862213134766
Epoch 1090, val loss: 0.5376061201095581
Epoch 1100, training loss: 835.6107177734375 = 0.4956534504890442 + 100.0 * 8.351150512695312
Epoch 1100, val loss: 0.536319375038147
Epoch 1110, training loss: 835.3026123046875 = 0.49445247650146484 + 100.0 * 8.348081588745117
Epoch 1110, val loss: 0.5356355905532837
Epoch 1120, training loss: 835.2919311523438 = 0.49330076575279236 + 100.0 * 8.347986221313477
Epoch 1120, val loss: 0.5348821878433228
Epoch 1130, training loss: 835.195556640625 = 0.49215179681777954 + 100.0 * 8.347034454345703
Epoch 1130, val loss: 0.5340849161148071
Epoch 1140, training loss: 835.28271484375 = 0.4909873604774475 + 100.0 * 8.347917556762695
Epoch 1140, val loss: 0.5333104133605957
Epoch 1150, training loss: 835.5512084960938 = 0.4897221326828003 + 100.0 * 8.350614547729492
Epoch 1150, val loss: 0.5324538946151733
Epoch 1160, training loss: 835.163330078125 = 0.48841872811317444 + 100.0 * 8.346749305725098
Epoch 1160, val loss: 0.5314574241638184
Epoch 1170, training loss: 835.0903930664062 = 0.48717212677001953 + 100.0 * 8.34603214263916
Epoch 1170, val loss: 0.5306255221366882
Epoch 1180, training loss: 835.1605224609375 = 0.4859393835067749 + 100.0 * 8.346745491027832
Epoch 1180, val loss: 0.529762327671051
Epoch 1190, training loss: 835.0287475585938 = 0.484685480594635 + 100.0 * 8.345440864562988
Epoch 1190, val loss: 0.5289009213447571
Epoch 1200, training loss: 835.142333984375 = 0.4834204316139221 + 100.0 * 8.346589088439941
Epoch 1200, val loss: 0.5280405879020691
Epoch 1210, training loss: 834.9845581054688 = 0.4821358323097229 + 100.0 * 8.345024108886719
Epoch 1210, val loss: 0.5272440910339355
Epoch 1220, training loss: 834.8257446289062 = 0.48086944222450256 + 100.0 * 8.343448638916016
Epoch 1220, val loss: 0.5263421535491943
Epoch 1230, training loss: 834.8206787109375 = 0.4796195924282074 + 100.0 * 8.34341049194336
Epoch 1230, val loss: 0.5254502296447754
Epoch 1240, training loss: 835.5335693359375 = 0.4783106744289398 + 100.0 * 8.350552558898926
Epoch 1240, val loss: 0.5243575572967529
Epoch 1250, training loss: 834.75439453125 = 0.47688135504722595 + 100.0 * 8.342775344848633
Epoch 1250, val loss: 0.5235369801521301
Epoch 1260, training loss: 834.7451171875 = 0.4755261242389679 + 100.0 * 8.342696189880371
Epoch 1260, val loss: 0.522642970085144
Epoch 1270, training loss: 834.649658203125 = 0.47421500086784363 + 100.0 * 8.341753959655762
Epoch 1270, val loss: 0.5217112898826599
Epoch 1280, training loss: 835.343017578125 = 0.47288069128990173 + 100.0 * 8.348701477050781
Epoch 1280, val loss: 0.5207681059837341
Epoch 1290, training loss: 835.0673828125 = 0.4714527428150177 + 100.0 * 8.345959663391113
Epoch 1290, val loss: 0.5197060108184814
Epoch 1300, training loss: 834.5819091796875 = 0.47002121806144714 + 100.0 * 8.341118812561035
Epoch 1300, val loss: 0.51889568567276
Epoch 1310, training loss: 834.5023193359375 = 0.46865329146385193 + 100.0 * 8.340336799621582
Epoch 1310, val loss: 0.5179569125175476
Epoch 1320, training loss: 834.5756225585938 = 0.46730124950408936 + 100.0 * 8.341083526611328
Epoch 1320, val loss: 0.5169892311096191
Epoch 1330, training loss: 834.8226928710938 = 0.4658815860748291 + 100.0 * 8.343567848205566
Epoch 1330, val loss: 0.5159807205200195
Epoch 1340, training loss: 834.5066528320312 = 0.4644000232219696 + 100.0 * 8.340422630310059
Epoch 1340, val loss: 0.5150660276412964
Epoch 1350, training loss: 834.3805541992188 = 0.46298137307167053 + 100.0 * 8.339176177978516
Epoch 1350, val loss: 0.5141434073448181
Epoch 1360, training loss: 834.3551025390625 = 0.46158167719841003 + 100.0 * 8.338934898376465
Epoch 1360, val loss: 0.5132994651794434
Epoch 1370, training loss: 834.572021484375 = 0.460170179605484 + 100.0 * 8.341118812561035
Epoch 1370, val loss: 0.5124978423118591
Epoch 1380, training loss: 834.286865234375 = 0.4586820900440216 + 100.0 * 8.338281631469727
Epoch 1380, val loss: 0.5113183856010437
Epoch 1390, training loss: 834.4209594726562 = 0.45721399784088135 + 100.0 * 8.339637756347656
Epoch 1390, val loss: 0.5103070735931396
Epoch 1400, training loss: 834.2958374023438 = 0.4557235538959503 + 100.0 * 8.338400840759277
Epoch 1400, val loss: 0.5095047354698181
Epoch 1410, training loss: 834.6865234375 = 0.45421499013900757 + 100.0 * 8.342323303222656
Epoch 1410, val loss: 0.508353590965271
Epoch 1420, training loss: 834.2686767578125 = 0.4526773691177368 + 100.0 * 8.338159561157227
Epoch 1420, val loss: 0.5073962807655334
Epoch 1430, training loss: 834.1047973632812 = 0.45117107033729553 + 100.0 * 8.336536407470703
Epoch 1430, val loss: 0.5064706206321716
Epoch 1440, training loss: 834.03857421875 = 0.44971340894699097 + 100.0 * 8.335888862609863
Epoch 1440, val loss: 0.5055939555168152
Epoch 1450, training loss: 834.0626831054688 = 0.4482453465461731 + 100.0 * 8.33614444732666
Epoch 1450, val loss: 0.5046440958976746
Epoch 1460, training loss: 834.6618041992188 = 0.4467290937900543 + 100.0 * 8.342150688171387
Epoch 1460, val loss: 0.5037334561347961
Epoch 1470, training loss: 834.4036865234375 = 0.44512471556663513 + 100.0 * 8.339585304260254
Epoch 1470, val loss: 0.5024555325508118
Epoch 1480, training loss: 833.9475708007812 = 0.4435437321662903 + 100.0 * 8.335040092468262
Epoch 1480, val loss: 0.5016937255859375
Epoch 1490, training loss: 833.8414306640625 = 0.44204047322273254 + 100.0 * 8.333993911743164
Epoch 1490, val loss: 0.5007761716842651
Epoch 1500, training loss: 833.8034057617188 = 0.44055458903312683 + 100.0 * 8.33362865447998
Epoch 1500, val loss: 0.49991583824157715
Epoch 1510, training loss: 834.0941772460938 = 0.4390605688095093 + 100.0 * 8.33655071258545
Epoch 1510, val loss: 0.499139666557312
Epoch 1520, training loss: 833.7634887695312 = 0.43746238946914673 + 100.0 * 8.333260536193848
Epoch 1520, val loss: 0.49793553352355957
Epoch 1530, training loss: 833.8255615234375 = 0.4358873665332794 + 100.0 * 8.33389663696289
Epoch 1530, val loss: 0.4970554709434509
Epoch 1540, training loss: 834.0206298828125 = 0.43434855341911316 + 100.0 * 8.33586311340332
Epoch 1540, val loss: 0.4959578514099121
Epoch 1550, training loss: 833.8343505859375 = 0.432769775390625 + 100.0 * 8.334015846252441
Epoch 1550, val loss: 0.4954063594341278
Epoch 1560, training loss: 834.0073852539062 = 0.43118295073509216 + 100.0 * 8.335762023925781
Epoch 1560, val loss: 0.49442151188850403
Epoch 1570, training loss: 833.6493530273438 = 0.4295772314071655 + 100.0 * 8.332198143005371
Epoch 1570, val loss: 0.49341118335723877
Epoch 1580, training loss: 833.6093139648438 = 0.42801225185394287 + 100.0 * 8.331812858581543
Epoch 1580, val loss: 0.4925607144832611
Epoch 1590, training loss: 833.5595092773438 = 0.42646849155426025 + 100.0 * 8.331330299377441
Epoch 1590, val loss: 0.4917663037776947
Epoch 1600, training loss: 833.58349609375 = 0.4249192476272583 + 100.0 * 8.331585884094238
Epoch 1600, val loss: 0.4910556674003601
Epoch 1610, training loss: 833.9071044921875 = 0.42334386706352234 + 100.0 * 8.334837913513184
Epoch 1610, val loss: 0.49014970660209656
Epoch 1620, training loss: 834.0299682617188 = 0.42170828580856323 + 100.0 * 8.336082458496094
Epoch 1620, val loss: 0.4893447160720825
Epoch 1630, training loss: 833.5792846679688 = 0.42003825306892395 + 100.0 * 8.331592559814453
Epoch 1630, val loss: 0.4886217713356018
Epoch 1640, training loss: 833.4398803710938 = 0.418455570936203 + 100.0 * 8.330214500427246
Epoch 1640, val loss: 0.48774710297584534
Epoch 1650, training loss: 833.6021118164062 = 0.41690194606781006 + 100.0 * 8.331851959228516
Epoch 1650, val loss: 0.4872615933418274
Epoch 1660, training loss: 833.51123046875 = 0.41525256633758545 + 100.0 * 8.330960273742676
Epoch 1660, val loss: 0.4862372577190399
Epoch 1670, training loss: 833.3863525390625 = 0.4136142432689667 + 100.0 * 8.329727172851562
Epoch 1670, val loss: 0.4855878949165344
Epoch 1680, training loss: 833.3133544921875 = 0.41203778982162476 + 100.0 * 8.329012870788574
Epoch 1680, val loss: 0.4847869575023651
Epoch 1690, training loss: 833.2378540039062 = 0.4104900360107422 + 100.0 * 8.32827377319336
Epoch 1690, val loss: 0.48420092463493347
Epoch 1700, training loss: 833.2393798828125 = 0.408954381942749 + 100.0 * 8.328304290771484
Epoch 1700, val loss: 0.48348188400268555
Epoch 1710, training loss: 834.1305541992188 = 0.40738123655319214 + 100.0 * 8.337231636047363
Epoch 1710, val loss: 0.4828628897666931
Epoch 1720, training loss: 833.5790405273438 = 0.40569138526916504 + 100.0 * 8.331733703613281
Epoch 1720, val loss: 0.4821397364139557
Epoch 1730, training loss: 833.2150268554688 = 0.404056191444397 + 100.0 * 8.328109741210938
Epoch 1730, val loss: 0.4813893437385559
Epoch 1740, training loss: 833.151611328125 = 0.40248507261276245 + 100.0 * 8.327491760253906
Epoch 1740, val loss: 0.48078301548957825
Epoch 1750, training loss: 833.09423828125 = 0.4009518325328827 + 100.0 * 8.326932907104492
Epoch 1750, val loss: 0.48027127981185913
Epoch 1760, training loss: 833.4461669921875 = 0.39941513538360596 + 100.0 * 8.330467224121094
Epoch 1760, val loss: 0.47979792952537537
Epoch 1770, training loss: 833.1131591796875 = 0.39781758189201355 + 100.0 * 8.327153205871582
Epoch 1770, val loss: 0.4790356755256653
Epoch 1780, training loss: 833.2196044921875 = 0.3962511122226715 + 100.0 * 8.32823371887207
Epoch 1780, val loss: 0.4783455729484558
Epoch 1790, training loss: 833.203125 = 0.39463743567466736 + 100.0 * 8.328084945678711
Epoch 1790, val loss: 0.47778934240341187
Epoch 1800, training loss: 833.031494140625 = 0.39304909110069275 + 100.0 * 8.326384544372559
Epoch 1800, val loss: 0.47719046473503113
Epoch 1810, training loss: 832.9439086914062 = 0.3915107250213623 + 100.0 * 8.325523376464844
Epoch 1810, val loss: 0.47662127017974854
Epoch 1820, training loss: 832.8973388671875 = 0.39000093936920166 + 100.0 * 8.3250732421875
Epoch 1820, val loss: 0.47617536783218384
Epoch 1830, training loss: 833.1973876953125 = 0.3884927034378052 + 100.0 * 8.328088760375977
Epoch 1830, val loss: 0.4756567180156708
Epoch 1840, training loss: 832.8438720703125 = 0.3868838846683502 + 100.0 * 8.324569702148438
Epoch 1840, val loss: 0.47515085339546204
Epoch 1850, training loss: 832.8461303710938 = 0.38531583547592163 + 100.0 * 8.324607849121094
Epoch 1850, val loss: 0.47445791959762573
Epoch 1860, training loss: 833.193359375 = 0.3837847411632538 + 100.0 * 8.328095436096191
Epoch 1860, val loss: 0.473897248506546
Epoch 1870, training loss: 832.81005859375 = 0.3821832835674286 + 100.0 * 8.324278831481934
Epoch 1870, val loss: 0.4736352860927582
Epoch 1880, training loss: 832.8251953125 = 0.3806449770927429 + 100.0 * 8.324445724487305
Epoch 1880, val loss: 0.47330328822135925
Epoch 1890, training loss: 832.7241821289062 = 0.37913578748703003 + 100.0 * 8.323450088500977
Epoch 1890, val loss: 0.47275519371032715
Epoch 1900, training loss: 832.73486328125 = 0.37763848900794983 + 100.0 * 8.323572158813477
Epoch 1900, val loss: 0.4724445044994354
Epoch 1910, training loss: 833.2514038085938 = 0.3761330246925354 + 100.0 * 8.328752517700195
Epoch 1910, val loss: 0.47236916422843933
Epoch 1920, training loss: 832.9027709960938 = 0.3745557367801666 + 100.0 * 8.325282096862793
Epoch 1920, val loss: 0.4712443947792053
Epoch 1930, training loss: 832.8406982421875 = 0.37302878499031067 + 100.0 * 8.324676513671875
Epoch 1930, val loss: 0.4710724949836731
Epoch 1940, training loss: 832.979736328125 = 0.37149620056152344 + 100.0 * 8.326082229614258
Epoch 1940, val loss: 0.47054874897003174
Epoch 1950, training loss: 832.7034301757812 = 0.3699573874473572 + 100.0 * 8.323334693908691
Epoch 1950, val loss: 0.47036224603652954
Epoch 1960, training loss: 832.69384765625 = 0.36845090985298157 + 100.0 * 8.323253631591797
Epoch 1960, val loss: 0.4699164927005768
Epoch 1970, training loss: 832.7896728515625 = 0.3669290840625763 + 100.0 * 8.324227333068848
Epoch 1970, val loss: 0.46960651874542236
Epoch 1980, training loss: 832.6121215820312 = 0.3654041588306427 + 100.0 * 8.322466850280762
Epoch 1980, val loss: 0.46916666626930237
Epoch 1990, training loss: 832.6348266601562 = 0.36390256881713867 + 100.0 * 8.322709083557129
Epoch 1990, val loss: 0.46879199147224426
Epoch 2000, training loss: 832.7067260742188 = 0.3623870015144348 + 100.0 * 8.323443412780762
Epoch 2000, val loss: 0.468629390001297
Epoch 2010, training loss: 832.4719848632812 = 0.36087894439697266 + 100.0 * 8.321110725402832
Epoch 2010, val loss: 0.4683672785758972
Epoch 2020, training loss: 832.5603637695312 = 0.3593882620334625 + 100.0 * 8.322010040283203
Epoch 2020, val loss: 0.4680924117565155
Epoch 2030, training loss: 832.5228881835938 = 0.3578875660896301 + 100.0 * 8.321649551391602
Epoch 2030, val loss: 0.46787431836128235
Epoch 2040, training loss: 832.6180419921875 = 0.35639050602912903 + 100.0 * 8.322616577148438
Epoch 2040, val loss: 0.4676286280155182
Epoch 2050, training loss: 832.7477416992188 = 0.3548705577850342 + 100.0 * 8.323928833007812
Epoch 2050, val loss: 0.4672873318195343
Epoch 2060, training loss: 832.380126953125 = 0.353311687707901 + 100.0 * 8.320267677307129
Epoch 2060, val loss: 0.46729663014411926
Epoch 2070, training loss: 832.337158203125 = 0.3518206775188446 + 100.0 * 8.319853782653809
Epoch 2070, val loss: 0.4670754075050354
Epoch 2080, training loss: 832.5311279296875 = 0.35035738348960876 + 100.0 * 8.321807861328125
Epoch 2080, val loss: 0.46698105335235596
Epoch 2090, training loss: 832.38330078125 = 0.3488560616970062 + 100.0 * 8.320343971252441
Epoch 2090, val loss: 0.46672186255455017
Epoch 2100, training loss: 832.31396484375 = 0.34737294912338257 + 100.0 * 8.319665908813477
Epoch 2100, val loss: 0.46644097566604614
Epoch 2110, training loss: 832.5344848632812 = 0.3458978235721588 + 100.0 * 8.32188606262207
Epoch 2110, val loss: 0.46626853942871094
Epoch 2120, training loss: 832.3389892578125 = 0.3443639576435089 + 100.0 * 8.3199462890625
Epoch 2120, val loss: 0.46632152795791626
Epoch 2130, training loss: 832.3501586914062 = 0.34285980463027954 + 100.0 * 8.320073127746582
Epoch 2130, val loss: 0.46624529361724854
Epoch 2140, training loss: 832.3917846679688 = 0.3413788974285126 + 100.0 * 8.320504188537598
Epoch 2140, val loss: 0.46579840779304504
Epoch 2150, training loss: 832.1905517578125 = 0.3398546874523163 + 100.0 * 8.318507194519043
Epoch 2150, val loss: 0.4657963514328003
Epoch 2160, training loss: 832.1597290039062 = 0.33834967017173767 + 100.0 * 8.31821346282959
Epoch 2160, val loss: 0.46593791246414185
Epoch 2170, training loss: 832.3438720703125 = 0.3368864059448242 + 100.0 * 8.320069313049316
Epoch 2170, val loss: 0.46560031175613403
Epoch 2180, training loss: 832.288330078125 = 0.3353722095489502 + 100.0 * 8.31952953338623
Epoch 2180, val loss: 0.4657515287399292
Epoch 2190, training loss: 832.2747802734375 = 0.33386680483818054 + 100.0 * 8.319409370422363
Epoch 2190, val loss: 0.46556931734085083
Epoch 2200, training loss: 832.2183837890625 = 0.3323730230331421 + 100.0 * 8.318860054016113
Epoch 2200, val loss: 0.4655663073062897
Epoch 2210, training loss: 832.1591796875 = 0.3308970630168915 + 100.0 * 8.318283081054688
Epoch 2210, val loss: 0.4654538035392761
Epoch 2220, training loss: 832.1024169921875 = 0.3294110596179962 + 100.0 * 8.317729949951172
Epoch 2220, val loss: 0.4654080867767334
Epoch 2230, training loss: 832.0223388671875 = 0.3279346227645874 + 100.0 * 8.316944122314453
Epoch 2230, val loss: 0.4654493033885956
Epoch 2240, training loss: 832.1154174804688 = 0.32646751403808594 + 100.0 * 8.317889213562012
Epoch 2240, val loss: 0.4657033085823059
Epoch 2250, training loss: 832.1880493164062 = 0.3250044882297516 + 100.0 * 8.31863021850586
Epoch 2250, val loss: 0.46567144989967346
Epoch 2260, training loss: 832.4854736328125 = 0.323508083820343 + 100.0 * 8.321619987487793
Epoch 2260, val loss: 0.46582329273223877
Epoch 2270, training loss: 831.9616088867188 = 0.32199370861053467 + 100.0 * 8.31639575958252
Epoch 2270, val loss: 0.4655633866786957
Epoch 2280, training loss: 831.9074096679688 = 0.32052311301231384 + 100.0 * 8.315869331359863
Epoch 2280, val loss: 0.46565860509872437
Epoch 2290, training loss: 832.0372314453125 = 0.3190809488296509 + 100.0 * 8.317181587219238
Epoch 2290, val loss: 0.46579933166503906
Epoch 2300, training loss: 832.1340942382812 = 0.31761443614959717 + 100.0 * 8.318164825439453
Epoch 2300, val loss: 0.4657481014728546
Epoch 2310, training loss: 831.9462280273438 = 0.31615039706230164 + 100.0 * 8.316300392150879
Epoch 2310, val loss: 0.4657174050807953
Epoch 2320, training loss: 831.8082885742188 = 0.3147108554840088 + 100.0 * 8.314935684204102
Epoch 2320, val loss: 0.4659506380558014
Epoch 2330, training loss: 831.97998046875 = 0.31329336762428284 + 100.0 * 8.316666603088379
Epoch 2330, val loss: 0.4660421311855316
Epoch 2340, training loss: 831.8967895507812 = 0.31184762716293335 + 100.0 * 8.315849304199219
Epoch 2340, val loss: 0.4660900831222534
Epoch 2350, training loss: 832.2552490234375 = 0.3104267120361328 + 100.0 * 8.319448471069336
Epoch 2350, val loss: 0.4664161503314972
Epoch 2360, training loss: 831.9373779296875 = 0.3089516758918762 + 100.0 * 8.3162841796875
Epoch 2360, val loss: 0.4663124084472656
Epoch 2370, training loss: 831.8132934570312 = 0.30750715732574463 + 100.0 * 8.315057754516602
Epoch 2370, val loss: 0.4662644565105438
Epoch 2380, training loss: 831.6886596679688 = 0.3061002492904663 + 100.0 * 8.313825607299805
Epoch 2380, val loss: 0.46642664074897766
Epoch 2390, training loss: 831.65576171875 = 0.3047100007534027 + 100.0 * 8.31351089477539
Epoch 2390, val loss: 0.4666571319103241
Epoch 2400, training loss: 832.0014038085938 = 0.30333325266838074 + 100.0 * 8.316980361938477
Epoch 2400, val loss: 0.46653756499290466
Epoch 2410, training loss: 831.7150268554688 = 0.3018723428249359 + 100.0 * 8.314131736755371
Epoch 2410, val loss: 0.4671728014945984
Epoch 2420, training loss: 831.6752319335938 = 0.30045148730278015 + 100.0 * 8.31374740600586
Epoch 2420, val loss: 0.46704772114753723
Epoch 2430, training loss: 831.73828125 = 0.2990569770336151 + 100.0 * 8.31439208984375
Epoch 2430, val loss: 0.4674476385116577
Epoch 2440, training loss: 831.8610229492188 = 0.29767322540283203 + 100.0 * 8.315633773803711
Epoch 2440, val loss: 0.46739476919174194
Epoch 2450, training loss: 831.7373657226562 = 0.29625871777534485 + 100.0 * 8.314411163330078
Epoch 2450, val loss: 0.4682140350341797
Epoch 2460, training loss: 831.626953125 = 0.2948801517486572 + 100.0 * 8.313321113586426
Epoch 2460, val loss: 0.4681529104709625
Epoch 2470, training loss: 831.771240234375 = 0.2935107350349426 + 100.0 * 8.314777374267578
Epoch 2470, val loss: 0.46848317980766296
Epoch 2480, training loss: 832.4669799804688 = 0.2921020984649658 + 100.0 * 8.321748733520508
Epoch 2480, val loss: 0.46861353516578674
Epoch 2490, training loss: 831.6295776367188 = 0.29068371653556824 + 100.0 * 8.31338882446289
Epoch 2490, val loss: 0.46928003430366516
Epoch 2500, training loss: 831.4969482421875 = 0.289305716753006 + 100.0 * 8.312076568603516
Epoch 2500, val loss: 0.4696022868156433
Epoch 2510, training loss: 831.454833984375 = 0.2879544496536255 + 100.0 * 8.311668395996094
Epoch 2510, val loss: 0.46985188126564026
Epoch 2520, training loss: 831.4144287109375 = 0.28660428524017334 + 100.0 * 8.311278343200684
Epoch 2520, val loss: 0.4701808989048004
Epoch 2530, training loss: 831.4212036132812 = 0.28525710105895996 + 100.0 * 8.311359405517578
Epoch 2530, val loss: 0.4704006314277649
Epoch 2540, training loss: 832.0637817382812 = 0.2839227616786957 + 100.0 * 8.317798614501953
Epoch 2540, val loss: 0.4706420600414276
Epoch 2550, training loss: 831.504638671875 = 0.2825000584125519 + 100.0 * 8.31222152709961
Epoch 2550, val loss: 0.4714568257331848
Epoch 2560, training loss: 831.4732666015625 = 0.2811364531517029 + 100.0 * 8.311921119689941
Epoch 2560, val loss: 0.4715769290924072
Epoch 2570, training loss: 831.4752197265625 = 0.2797740399837494 + 100.0 * 8.311954498291016
Epoch 2570, val loss: 0.4720854163169861
Epoch 2580, training loss: 831.7723999023438 = 0.2784627079963684 + 100.0 * 8.314939498901367
Epoch 2580, val loss: 0.47226688265800476
Epoch 2590, training loss: 831.4719848632812 = 0.27705782651901245 + 100.0 * 8.311949729919434
Epoch 2590, val loss: 0.47278985381126404
Epoch 2600, training loss: 831.4617309570312 = 0.27568867802619934 + 100.0 * 8.311860084533691
Epoch 2600, val loss: 0.47372156381607056
Epoch 2610, training loss: 831.3734741210938 = 0.27435973286628723 + 100.0 * 8.310991287231445
Epoch 2610, val loss: 0.47413957118988037
Epoch 2620, training loss: 831.261962890625 = 0.27300336956977844 + 100.0 * 8.309889793395996
Epoch 2620, val loss: 0.4747173488140106
Epoch 2630, training loss: 831.3336791992188 = 0.27167823910713196 + 100.0 * 8.310620307922363
Epoch 2630, val loss: 0.4752555191516876
Epoch 2640, training loss: 831.7533569335938 = 0.27035754919052124 + 100.0 * 8.31482982635498
Epoch 2640, val loss: 0.47582361102104187
Epoch 2650, training loss: 831.3529663085938 = 0.26899856328964233 + 100.0 * 8.310839653015137
Epoch 2650, val loss: 0.4763392210006714
Epoch 2660, training loss: 831.322021484375 = 0.2676507830619812 + 100.0 * 8.31054401397705
Epoch 2660, val loss: 0.4768178164958954
Epoch 2670, training loss: 831.57666015625 = 0.26631277799606323 + 100.0 * 8.313103675842285
Epoch 2670, val loss: 0.4775093197822571
Epoch 2680, training loss: 831.4832153320312 = 0.2649818956851959 + 100.0 * 8.312182426452637
Epoch 2680, val loss: 0.47822248935699463
Epoch 2690, training loss: 831.2323608398438 = 0.26361382007598877 + 100.0 * 8.309687614440918
Epoch 2690, val loss: 0.47881361842155457
Epoch 2700, training loss: 831.1937866210938 = 0.2623034119606018 + 100.0 * 8.309314727783203
Epoch 2700, val loss: 0.47886204719543457
Epoch 2710, training loss: 831.2002563476562 = 0.26098349690437317 + 100.0 * 8.309392929077148
Epoch 2710, val loss: 0.47974663972854614
Epoch 2720, training loss: 831.5657958984375 = 0.2596796751022339 + 100.0 * 8.313060760498047
Epoch 2720, val loss: 0.4802534580230713
Epoch 2730, training loss: 831.27001953125 = 0.25836941599845886 + 100.0 * 8.3101167678833
Epoch 2730, val loss: 0.4806431531906128
Epoch 2740, training loss: 831.2973022460938 = 0.2570470869541168 + 100.0 * 8.310402870178223
Epoch 2740, val loss: 0.48140469193458557
Epoch 2750, training loss: 831.1467895507812 = 0.25573423504829407 + 100.0 * 8.308910369873047
Epoch 2750, val loss: 0.481898695230484
Epoch 2760, training loss: 831.3360595703125 = 0.2544395327568054 + 100.0 * 8.310815811157227
Epoch 2760, val loss: 0.4827415347099304
Epoch 2770, training loss: 831.1430053710938 = 0.25313544273376465 + 100.0 * 8.30889892578125
Epoch 2770, val loss: 0.48337024450302124
Epoch 2780, training loss: 831.1143798828125 = 0.25185057520866394 + 100.0 * 8.308625221252441
Epoch 2780, val loss: 0.48399844765663147
Epoch 2790, training loss: 831.279052734375 = 0.25059083104133606 + 100.0 * 8.310284614562988
Epoch 2790, val loss: 0.4849471151828766
Epoch 2800, training loss: 831.1790161132812 = 0.24929817020893097 + 100.0 * 8.309296607971191
Epoch 2800, val loss: 0.4852368235588074
Epoch 2810, training loss: 831.1401977539062 = 0.24801473319530487 + 100.0 * 8.308921813964844
Epoch 2810, val loss: 0.48529356718063354
Epoch 2820, training loss: 831.27294921875 = 0.246758371591568 + 100.0 * 8.310261726379395
Epoch 2820, val loss: 0.4860847592353821
Epoch 2830, training loss: 831.1339721679688 = 0.24548254907131195 + 100.0 * 8.308884620666504
Epoch 2830, val loss: 0.486796110868454
Epoch 2840, training loss: 830.9802856445312 = 0.24422189593315125 + 100.0 * 8.307360649108887
Epoch 2840, val loss: 0.48760586977005005
Epoch 2850, training loss: 831.3983764648438 = 0.24299806356430054 + 100.0 * 8.311553955078125
Epoch 2850, val loss: 0.4880959987640381
Epoch 2860, training loss: 831.4011840820312 = 0.2417290061712265 + 100.0 * 8.31159496307373
Epoch 2860, val loss: 0.48845693469047546
Epoch 2870, training loss: 831.0266723632812 = 0.24043558537960052 + 100.0 * 8.307862281799316
Epoch 2870, val loss: 0.48957064747810364
Epoch 2880, training loss: 830.9398803710938 = 0.239191934466362 + 100.0 * 8.3070068359375
Epoch 2880, val loss: 0.4903242290019989
Epoch 2890, training loss: 830.8867797851562 = 0.23795990645885468 + 100.0 * 8.306488037109375
Epoch 2890, val loss: 0.49095261096954346
Epoch 2900, training loss: 830.8923950195312 = 0.23674511909484863 + 100.0 * 8.306556701660156
Epoch 2900, val loss: 0.4918908178806305
Epoch 2910, training loss: 831.47607421875 = 0.23558683693408966 + 100.0 * 8.31240463256836
Epoch 2910, val loss: 0.49276697635650635
Epoch 2920, training loss: 831.0345458984375 = 0.23429040610790253 + 100.0 * 8.308002471923828
Epoch 2920, val loss: 0.4931046962738037
Epoch 2930, training loss: 830.876220703125 = 0.23305706679821014 + 100.0 * 8.306431770324707
Epoch 2930, val loss: 0.4938950538635254
Epoch 2940, training loss: 830.8854370117188 = 0.23186840116977692 + 100.0 * 8.306535720825195
Epoch 2940, val loss: 0.4944564402103424
Epoch 2950, training loss: 831.0242309570312 = 0.2306573987007141 + 100.0 * 8.30793571472168
Epoch 2950, val loss: 0.4952024519443512
Epoch 2960, training loss: 831.0184936523438 = 0.2294462025165558 + 100.0 * 8.307890892028809
Epoch 2960, val loss: 0.49644917249679565
Epoch 2970, training loss: 831.092529296875 = 0.22828342020511627 + 100.0 * 8.308642387390137
Epoch 2970, val loss: 0.4974769949913025
Epoch 2980, training loss: 830.9270629882812 = 0.22705581784248352 + 100.0 * 8.307000160217285
Epoch 2980, val loss: 0.49754735827445984
Epoch 2990, training loss: 831.1176147460938 = 0.22584892809391022 + 100.0 * 8.308917999267578
Epoch 2990, val loss: 0.49898844957351685
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8061897513952307
0.8411214953271029
=== training gcn model ===
Epoch 0, training loss: 1059.314697265625 = 1.092875599861145 + 100.0 * 10.582218170166016
Epoch 0, val loss: 1.091835856437683
Epoch 10, training loss: 1059.17041015625 = 1.0875500440597534 + 100.0 * 10.580828666687012
Epoch 10, val loss: 1.0864605903625488
Epoch 20, training loss: 1057.8924560546875 = 1.081178903579712 + 100.0 * 10.56811237335205
Epoch 20, val loss: 1.080104947090149
Epoch 30, training loss: 1050.2562255859375 = 1.0735841989517212 + 100.0 * 10.491826057434082
Epoch 30, val loss: 1.0726407766342163
Epoch 40, training loss: 1025.76806640625 = 1.0667580366134644 + 100.0 * 10.247013092041016
Epoch 40, val loss: 1.066318154335022
Epoch 50, training loss: 995.0785522460938 = 1.0609571933746338 + 100.0 * 9.940176010131836
Epoch 50, val loss: 1.060999870300293
Epoch 60, training loss: 958.920654296875 = 1.0559672117233276 + 100.0 * 9.578646659851074
Epoch 60, val loss: 1.0559948682785034
Epoch 70, training loss: 936.1787109375 = 1.0482571125030518 + 100.0 * 9.351304054260254
Epoch 70, val loss: 1.0483031272888184
Epoch 80, training loss: 918.1637573242188 = 1.0411032438278198 + 100.0 * 9.171226501464844
Epoch 80, val loss: 1.0415493249893188
Epoch 90, training loss: 909.5809936523438 = 1.0355192422866821 + 100.0 * 9.085454940795898
Epoch 90, val loss: 1.036324381828308
Epoch 100, training loss: 894.43115234375 = 1.0310486555099487 + 100.0 * 8.934000968933105
Epoch 100, val loss: 1.0320812463760376
Epoch 110, training loss: 884.1812744140625 = 1.0274503231048584 + 100.0 * 8.831538200378418
Epoch 110, val loss: 1.028557538986206
Epoch 120, training loss: 879.3748168945312 = 1.0227385759353638 + 100.0 * 8.783520698547363
Epoch 120, val loss: 1.023719310760498
Epoch 130, training loss: 875.8435668945312 = 1.0164552927017212 + 100.0 * 8.748270988464355
Epoch 130, val loss: 1.0174846649169922
Epoch 140, training loss: 873.166748046875 = 1.0092811584472656 + 100.0 * 8.721574783325195
Epoch 140, val loss: 1.010454535484314
Epoch 150, training loss: 870.5128173828125 = 1.0018963813781738 + 100.0 * 8.695109367370605
Epoch 150, val loss: 1.0033042430877686
Epoch 160, training loss: 867.66162109375 = 0.9946072101593018 + 100.0 * 8.666669845581055
Epoch 160, val loss: 0.9963393211364746
Epoch 170, training loss: 864.3838500976562 = 0.9870017766952515 + 100.0 * 8.633968353271484
Epoch 170, val loss: 0.9889959096908569
Epoch 180, training loss: 861.392333984375 = 0.9788853526115417 + 100.0 * 8.604134559631348
Epoch 180, val loss: 0.9811749458312988
Epoch 190, training loss: 859.040771484375 = 0.9698744416236877 + 100.0 * 8.580709457397461
Epoch 190, val loss: 0.9724650979042053
Epoch 200, training loss: 856.9090576171875 = 0.9598231911659241 + 100.0 * 8.559492111206055
Epoch 200, val loss: 0.9627671241760254
Epoch 210, training loss: 855.2288208007812 = 0.9488396048545837 + 100.0 * 8.542799949645996
Epoch 210, val loss: 0.9522231221199036
Epoch 220, training loss: 853.7831420898438 = 0.9368548393249512 + 100.0 * 8.528463363647461
Epoch 220, val loss: 0.9407650232315063
Epoch 230, training loss: 852.7473754882812 = 0.9239771366119385 + 100.0 * 8.518234252929688
Epoch 230, val loss: 0.9284921288490295
Epoch 240, training loss: 851.6763305664062 = 0.9104337692260742 + 100.0 * 8.507658958435059
Epoch 240, val loss: 0.9156796932220459
Epoch 250, training loss: 851.0165405273438 = 0.8963156342506409 + 100.0 * 8.501202583312988
Epoch 250, val loss: 0.9023765921592712
Epoch 260, training loss: 850.035888671875 = 0.8817889094352722 + 100.0 * 8.491540908813477
Epoch 260, val loss: 0.888761579990387
Epoch 270, training loss: 849.4971313476562 = 0.8669118285179138 + 100.0 * 8.486302375793457
Epoch 270, val loss: 0.8750170469284058
Epoch 280, training loss: 848.6385498046875 = 0.8519223928451538 + 100.0 * 8.477866172790527
Epoch 280, val loss: 0.861168622970581
Epoch 290, training loss: 848.0146484375 = 0.8369841575622559 + 100.0 * 8.471776962280273
Epoch 290, val loss: 0.8474819660186768
Epoch 300, training loss: 848.0813598632812 = 0.8221262097358704 + 100.0 * 8.4725923538208
Epoch 300, val loss: 0.8339171409606934
Epoch 310, training loss: 846.9360961914062 = 0.8072776198387146 + 100.0 * 8.461288452148438
Epoch 310, val loss: 0.8205763101577759
Epoch 320, training loss: 846.503173828125 = 0.7927558422088623 + 100.0 * 8.457103729248047
Epoch 320, val loss: 0.8076136112213135
Epoch 330, training loss: 846.3684692382812 = 0.7785512208938599 + 100.0 * 8.455899238586426
Epoch 330, val loss: 0.7950050234794617
Epoch 340, training loss: 845.7131958007812 = 0.7645763754844666 + 100.0 * 8.449485778808594
Epoch 340, val loss: 0.7826809287071228
Epoch 350, training loss: 845.2949829101562 = 0.7510536313056946 + 100.0 * 8.445439338684082
Epoch 350, val loss: 0.7708784341812134
Epoch 360, training loss: 845.3397827148438 = 0.7379878163337708 + 100.0 * 8.44601821899414
Epoch 360, val loss: 0.7595151662826538
Epoch 370, training loss: 844.9645385742188 = 0.7250555753707886 + 100.0 * 8.442395210266113
Epoch 370, val loss: 0.7484899163246155
Epoch 380, training loss: 844.2923583984375 = 0.7126979231834412 + 100.0 * 8.435796737670898
Epoch 380, val loss: 0.7379860877990723
Epoch 390, training loss: 843.9058227539062 = 0.7009484767913818 + 100.0 * 8.432048797607422
Epoch 390, val loss: 0.7281560301780701
Epoch 400, training loss: 843.6115112304688 = 0.689713180065155 + 100.0 * 8.429218292236328
Epoch 400, val loss: 0.718858003616333
Epoch 410, training loss: 843.342041015625 = 0.6789702773094177 + 100.0 * 8.426630973815918
Epoch 410, val loss: 0.7100886106491089
Epoch 420, training loss: 843.6898803710938 = 0.6687214374542236 + 100.0 * 8.430212020874023
Epoch 420, val loss: 0.7018940448760986
Epoch 430, training loss: 843.0575561523438 = 0.6588371396064758 + 100.0 * 8.42398738861084
Epoch 430, val loss: 0.6938893795013428
Epoch 440, training loss: 842.65966796875 = 0.6495363116264343 + 100.0 * 8.420101165771484
Epoch 440, val loss: 0.6866517663002014
Epoch 450, training loss: 842.3862915039062 = 0.6409057974815369 + 100.0 * 8.41745376586914
Epoch 450, val loss: 0.6800440549850464
Epoch 460, training loss: 842.1525268554688 = 0.6328054666519165 + 100.0 * 8.415197372436523
Epoch 460, val loss: 0.6739674806594849
Epoch 470, training loss: 842.1253051757812 = 0.6252063512802124 + 100.0 * 8.415000915527344
Epoch 470, val loss: 0.6683856844902039
Epoch 480, training loss: 842.3731689453125 = 0.6179600358009338 + 100.0 * 8.41755199432373
Epoch 480, val loss: 0.6630945801734924
Epoch 490, training loss: 841.7017211914062 = 0.6112159490585327 + 100.0 * 8.410904884338379
Epoch 490, val loss: 0.6583232879638672
Epoch 500, training loss: 841.372802734375 = 0.6050404906272888 + 100.0 * 8.40767765045166
Epoch 500, val loss: 0.654089629650116
Epoch 510, training loss: 841.1802368164062 = 0.5993013381958008 + 100.0 * 8.40580940246582
Epoch 510, val loss: 0.6502489447593689
Epoch 520, training loss: 841.030029296875 = 0.5939488410949707 + 100.0 * 8.4043607711792
Epoch 520, val loss: 0.6467534303665161
Epoch 530, training loss: 842.7112426757812 = 0.5888698697090149 + 100.0 * 8.421223640441895
Epoch 530, val loss: 0.6434983015060425
Epoch 540, training loss: 841.0343627929688 = 0.5838770866394043 + 100.0 * 8.404504776000977
Epoch 540, val loss: 0.6402483582496643
Epoch 550, training loss: 840.710205078125 = 0.5794525146484375 + 100.0 * 8.401307106018066
Epoch 550, val loss: 0.6374862194061279
Epoch 560, training loss: 840.4387817382812 = 0.5754081606864929 + 100.0 * 8.39863395690918
Epoch 560, val loss: 0.6350272297859192
Epoch 570, training loss: 840.2816772460938 = 0.5716276168823242 + 100.0 * 8.397100448608398
Epoch 570, val loss: 0.6327999830245972
Epoch 580, training loss: 840.1253051757812 = 0.5680708885192871 + 100.0 * 8.395572662353516
Epoch 580, val loss: 0.6307427883148193
Epoch 590, training loss: 839.9805297851562 = 0.5647038221359253 + 100.0 * 8.394158363342285
Epoch 590, val loss: 0.6288174390792847
Epoch 600, training loss: 839.8450927734375 = 0.561507523059845 + 100.0 * 8.39283561706543
Epoch 600, val loss: 0.6270055174827576
Epoch 610, training loss: 839.9144897460938 = 0.5584807991981506 + 100.0 * 8.393560409545898
Epoch 610, val loss: 0.6252874135971069
Epoch 620, training loss: 840.4496459960938 = 0.5554306507110596 + 100.0 * 8.398941993713379
Epoch 620, val loss: 0.623504102230072
Epoch 630, training loss: 839.7615356445312 = 0.5525723099708557 + 100.0 * 8.39208984375
Epoch 630, val loss: 0.621935248374939
Epoch 640, training loss: 839.4198608398438 = 0.5499587655067444 + 100.0 * 8.38869857788086
Epoch 640, val loss: 0.6204904317855835
Epoch 650, training loss: 839.2623901367188 = 0.5474851727485657 + 100.0 * 8.3871488571167
Epoch 650, val loss: 0.6191567182540894
Epoch 660, training loss: 839.9717407226562 = 0.5451050400733948 + 100.0 * 8.394266128540039
Epoch 660, val loss: 0.617786705493927
Epoch 670, training loss: 839.4940795898438 = 0.5426600575447083 + 100.0 * 8.389513969421387
Epoch 670, val loss: 0.6165611147880554
Epoch 680, training loss: 839.0852661132812 = 0.5403581261634827 + 100.0 * 8.385449409484863
Epoch 680, val loss: 0.6153122186660767
Epoch 690, training loss: 838.8030395507812 = 0.5382277965545654 + 100.0 * 8.382648468017578
Epoch 690, val loss: 0.6141612529754639
Epoch 700, training loss: 838.7142333984375 = 0.536186933517456 + 100.0 * 8.381780624389648
Epoch 700, val loss: 0.6130784153938293
Epoch 710, training loss: 838.5827026367188 = 0.5342221856117249 + 100.0 * 8.380484580993652
Epoch 710, val loss: 0.6120894551277161
Epoch 720, training loss: 839.4352416992188 = 0.5323025584220886 + 100.0 * 8.389029502868652
Epoch 720, val loss: 0.6111517548561096
Epoch 730, training loss: 839.0220947265625 = 0.5302587151527405 + 100.0 * 8.384918212890625
Epoch 730, val loss: 0.6099685430526733
Epoch 740, training loss: 838.43017578125 = 0.5283651947975159 + 100.0 * 8.37901782989502
Epoch 740, val loss: 0.6089756488800049
Epoch 750, training loss: 838.1659545898438 = 0.5266100168228149 + 100.0 * 8.37639331817627
Epoch 750, val loss: 0.608100950717926
Epoch 760, training loss: 838.1039428710938 = 0.5249184370040894 + 100.0 * 8.3757905960083
Epoch 760, val loss: 0.6072739362716675
Epoch 770, training loss: 838.84912109375 = 0.5232553482055664 + 100.0 * 8.383258819580078
Epoch 770, val loss: 0.6065004467964172
Epoch 780, training loss: 838.3080444335938 = 0.5214932560920715 + 100.0 * 8.3778657913208
Epoch 780, val loss: 0.6054487824440002
Epoch 790, training loss: 837.8651733398438 = 0.5198529958724976 + 100.0 * 8.373453140258789
Epoch 790, val loss: 0.6046586036682129
Epoch 800, training loss: 837.6995849609375 = 0.5182940363883972 + 100.0 * 8.37181282043457
Epoch 800, val loss: 0.6038824319839478
Epoch 810, training loss: 838.168701171875 = 0.5167588591575623 + 100.0 * 8.376519203186035
Epoch 810, val loss: 0.6031116843223572
Epoch 820, training loss: 837.6106567382812 = 0.5151862502098083 + 100.0 * 8.370954513549805
Epoch 820, val loss: 0.6022896766662598
Epoch 830, training loss: 837.4380493164062 = 0.5136943459510803 + 100.0 * 8.369243621826172
Epoch 830, val loss: 0.6015588641166687
Epoch 840, training loss: 838.0753784179688 = 0.5122570395469666 + 100.0 * 8.375631332397461
Epoch 840, val loss: 0.6009313464164734
Epoch 850, training loss: 837.5478515625 = 0.5106604695320129 + 100.0 * 8.37037181854248
Epoch 850, val loss: 0.5998767614364624
Epoch 860, training loss: 837.2236938476562 = 0.509221613407135 + 100.0 * 8.367144584655762
Epoch 860, val loss: 0.5991976261138916
Epoch 870, training loss: 837.0990600585938 = 0.5078315138816833 + 100.0 * 8.365912437438965
Epoch 870, val loss: 0.598486602306366
Epoch 880, training loss: 837.1840209960938 = 0.5064774751663208 + 100.0 * 8.366775512695312
Epoch 880, val loss: 0.5978127121925354
Epoch 890, training loss: 837.0859375 = 0.505092203617096 + 100.0 * 8.365808486938477
Epoch 890, val loss: 0.597072958946228
Epoch 900, training loss: 837.6484985351562 = 0.5037161111831665 + 100.0 * 8.371447563171387
Epoch 900, val loss: 0.5963051915168762
Epoch 910, training loss: 837.1925048828125 = 0.5022466778755188 + 100.0 * 8.366902351379395
Epoch 910, val loss: 0.5955998301506042
Epoch 920, training loss: 836.9020385742188 = 0.5008721947669983 + 100.0 * 8.364011764526367
Epoch 920, val loss: 0.59478360414505
Epoch 930, training loss: 836.652587890625 = 0.4995887577533722 + 100.0 * 8.361530303955078
Epoch 930, val loss: 0.5941798090934753
Epoch 940, training loss: 836.5858764648438 = 0.49833083152770996 + 100.0 * 8.360875129699707
Epoch 940, val loss: 0.5935483574867249
Epoch 950, training loss: 836.5075073242188 = 0.49709346890449524 + 100.0 * 8.36010456085205
Epoch 950, val loss: 0.592914879322052
Epoch 960, training loss: 837.2417602539062 = 0.49583926796913147 + 100.0 * 8.367459297180176
Epoch 960, val loss: 0.5922094583511353
Epoch 970, training loss: 836.777587890625 = 0.4944775700569153 + 100.0 * 8.362831115722656
Epoch 970, val loss: 0.5915302038192749
Epoch 980, training loss: 836.3250122070312 = 0.4931930601596832 + 100.0 * 8.358318328857422
Epoch 980, val loss: 0.5908368229866028
Epoch 990, training loss: 836.2919311523438 = 0.4919721484184265 + 100.0 * 8.357999801635742
Epoch 990, val loss: 0.5901557207107544
Epoch 1000, training loss: 836.5506591796875 = 0.4907645583152771 + 100.0 * 8.36059856414795
Epoch 1000, val loss: 0.5894735455513
Epoch 1010, training loss: 836.4144287109375 = 0.48942825198173523 + 100.0 * 8.35925006866455
Epoch 1010, val loss: 0.5889099836349487
Epoch 1020, training loss: 836.2823486328125 = 0.48809677362442017 + 100.0 * 8.357942581176758
Epoch 1020, val loss: 0.5880107879638672
Epoch 1030, training loss: 836.0941162109375 = 0.48691225051879883 + 100.0 * 8.356071472167969
Epoch 1030, val loss: 0.587394654750824
Epoch 1040, training loss: 835.9540405273438 = 0.4857665002346039 + 100.0 * 8.354682922363281
Epoch 1040, val loss: 0.5868777632713318
Epoch 1050, training loss: 835.8914184570312 = 0.48462536931037903 + 100.0 * 8.3540678024292
Epoch 1050, val loss: 0.5863458514213562
Epoch 1060, training loss: 835.9326782226562 = 0.48347440361976624 + 100.0 * 8.3544921875
Epoch 1060, val loss: 0.5857964158058167
Epoch 1070, training loss: 835.9457397460938 = 0.4822368025779724 + 100.0 * 8.354635238647461
Epoch 1070, val loss: 0.5850026607513428
Epoch 1080, training loss: 836.0558471679688 = 0.48097094893455505 + 100.0 * 8.355749130249023
Epoch 1080, val loss: 0.584412157535553
Epoch 1090, training loss: 835.9229125976562 = 0.47977861762046814 + 100.0 * 8.35443115234375
Epoch 1090, val loss: 0.5837128758430481
Epoch 1100, training loss: 835.7234497070312 = 0.47857484221458435 + 100.0 * 8.352448463439941
Epoch 1100, val loss: 0.5829968452453613
Epoch 1110, training loss: 835.65771484375 = 0.4774048328399658 + 100.0 * 8.351802825927734
Epoch 1110, val loss: 0.5823866724967957
Epoch 1120, training loss: 835.60302734375 = 0.4762689471244812 + 100.0 * 8.35126781463623
Epoch 1120, val loss: 0.5817965865135193
Epoch 1130, training loss: 836.2716674804688 = 0.4751185476779938 + 100.0 * 8.357965469360352
Epoch 1130, val loss: 0.581155002117157
Epoch 1140, training loss: 835.6508178710938 = 0.4738825261592865 + 100.0 * 8.35176944732666
Epoch 1140, val loss: 0.5805732607841492
Epoch 1150, training loss: 835.5021362304688 = 0.47270166873931885 + 100.0 * 8.35029411315918
Epoch 1150, val loss: 0.5799158811569214
Epoch 1160, training loss: 835.3826293945312 = 0.4715675115585327 + 100.0 * 8.34911060333252
Epoch 1160, val loss: 0.5793047547340393
Epoch 1170, training loss: 835.3761596679688 = 0.47045138478279114 + 100.0 * 8.3490571975708
Epoch 1170, val loss: 0.5788009762763977
Epoch 1180, training loss: 835.9439086914062 = 0.46929997205734253 + 100.0 * 8.354745864868164
Epoch 1180, val loss: 0.5782214999198914
Epoch 1190, training loss: 835.6572265625 = 0.4680507183074951 + 100.0 * 8.35189151763916
Epoch 1190, val loss: 0.5774557590484619
Epoch 1200, training loss: 835.2317504882812 = 0.4668617248535156 + 100.0 * 8.347648620605469
Epoch 1200, val loss: 0.5768449306488037
Epoch 1210, training loss: 835.21044921875 = 0.4657137393951416 + 100.0 * 8.347447395324707
Epoch 1210, val loss: 0.5762416124343872
Epoch 1220, training loss: 835.2037353515625 = 0.4645770788192749 + 100.0 * 8.347391128540039
Epoch 1220, val loss: 0.5756450891494751
Epoch 1230, training loss: 835.4625244140625 = 0.46343541145324707 + 100.0 * 8.349990844726562
Epoch 1230, val loss: 0.5751700401306152
Epoch 1240, training loss: 835.2191162109375 = 0.46216920018196106 + 100.0 * 8.347569465637207
Epoch 1240, val loss: 0.5743370056152344
Epoch 1250, training loss: 835.0322265625 = 0.460936963558197 + 100.0 * 8.345712661743164
Epoch 1250, val loss: 0.5737773180007935
Epoch 1260, training loss: 834.98876953125 = 0.4597708284854889 + 100.0 * 8.345290184020996
Epoch 1260, val loss: 0.573080837726593
Epoch 1270, training loss: 834.9239501953125 = 0.4586372673511505 + 100.0 * 8.344653129577637
Epoch 1270, val loss: 0.572625994682312
Epoch 1280, training loss: 834.9119873046875 = 0.4574933350086212 + 100.0 * 8.344544410705566
Epoch 1280, val loss: 0.5720002055168152
Epoch 1290, training loss: 835.8442993164062 = 0.4563177824020386 + 100.0 * 8.353879928588867
Epoch 1290, val loss: 0.5714751482009888
Epoch 1300, training loss: 835.1337890625 = 0.45503437519073486 + 100.0 * 8.346787452697754
Epoch 1300, val loss: 0.5707116723060608
Epoch 1310, training loss: 834.876220703125 = 0.45380401611328125 + 100.0 * 8.344223976135254
Epoch 1310, val loss: 0.5701292157173157
Epoch 1320, training loss: 834.7628173828125 = 0.4526214003562927 + 100.0 * 8.343101501464844
Epoch 1320, val loss: 0.5694987773895264
Epoch 1330, training loss: 835.0346069335938 = 0.4514397084712982 + 100.0 * 8.345831871032715
Epoch 1330, val loss: 0.5688724517822266
Epoch 1340, training loss: 834.6693725585938 = 0.45019233226776123 + 100.0 * 8.342191696166992
Epoch 1340, val loss: 0.5683460235595703
Epoch 1350, training loss: 834.6593017578125 = 0.44897085428237915 + 100.0 * 8.342103004455566
Epoch 1350, val loss: 0.5676811337471008
Epoch 1360, training loss: 834.917236328125 = 0.4477657377719879 + 100.0 * 8.344695091247559
Epoch 1360, val loss: 0.5672080516815186
Epoch 1370, training loss: 835.128173828125 = 0.44647514820098877 + 100.0 * 8.346817016601562
Epoch 1370, val loss: 0.5663707852363586
Epoch 1380, training loss: 834.6927490234375 = 0.44516661763191223 + 100.0 * 8.342475891113281
Epoch 1380, val loss: 0.565766453742981
Epoch 1390, training loss: 834.5435791015625 = 0.44393298029899597 + 100.0 * 8.340996742248535
Epoch 1390, val loss: 0.5651319622993469
Epoch 1400, training loss: 834.4749145507812 = 0.4427244961261749 + 100.0 * 8.34032154083252
Epoch 1400, val loss: 0.5646403431892395
Epoch 1410, training loss: 834.4301147460938 = 0.44150352478027344 + 100.0 * 8.339885711669922
Epoch 1410, val loss: 0.5640102624893188
Epoch 1420, training loss: 834.4769897460938 = 0.4402693808078766 + 100.0 * 8.340367317199707
Epoch 1420, val loss: 0.5634052157402039
Epoch 1430, training loss: 835.8784790039062 = 0.43897393345832825 + 100.0 * 8.354394912719727
Epoch 1430, val loss: 0.5627960562705994
Epoch 1440, training loss: 834.6627197265625 = 0.4375511705875397 + 100.0 * 8.342251777648926
Epoch 1440, val loss: 0.5620046257972717
Epoch 1450, training loss: 834.4165649414062 = 0.43621155619621277 + 100.0 * 8.339803695678711
Epoch 1450, val loss: 0.5613572001457214
Epoch 1460, training loss: 834.3372192382812 = 0.434933066368103 + 100.0 * 8.339022636413574
Epoch 1460, val loss: 0.5606851577758789
Epoch 1470, training loss: 834.2969970703125 = 0.433668315410614 + 100.0 * 8.33863353729248
Epoch 1470, val loss: 0.560111939907074
Epoch 1480, training loss: 834.6614379882812 = 0.43238410353660583 + 100.0 * 8.342290878295898
Epoch 1480, val loss: 0.5593941807746887
Epoch 1490, training loss: 834.2050170898438 = 0.4310162365436554 + 100.0 * 8.337739944458008
Epoch 1490, val loss: 0.5588494539260864
Epoch 1500, training loss: 834.243896484375 = 0.42967960238456726 + 100.0 * 8.338142395019531
Epoch 1500, val loss: 0.558264970779419
Epoch 1510, training loss: 834.275390625 = 0.4283500611782074 + 100.0 * 8.338470458984375
Epoch 1510, val loss: 0.5575517416000366
Epoch 1520, training loss: 834.8043212890625 = 0.4269880950450897 + 100.0 * 8.343772888183594
Epoch 1520, val loss: 0.5570016503334045
Epoch 1530, training loss: 834.2626953125 = 0.42555081844329834 + 100.0 * 8.338371276855469
Epoch 1530, val loss: 0.5562289953231812
Epoch 1540, training loss: 834.070556640625 = 0.42417511343955994 + 100.0 * 8.336463928222656
Epoch 1540, val loss: 0.5557563304901123
Epoch 1550, training loss: 834.0304565429688 = 0.4228196442127228 + 100.0 * 8.336076736450195
Epoch 1550, val loss: 0.5552195310592651
Epoch 1560, training loss: 834.065185546875 = 0.42146340012550354 + 100.0 * 8.336437225341797
Epoch 1560, val loss: 0.5546968579292297
Epoch 1570, training loss: 834.9813842773438 = 0.420064240694046 + 100.0 * 8.345613479614258
Epoch 1570, val loss: 0.5541083812713623
Epoch 1580, training loss: 834.3588256835938 = 0.41853171586990356 + 100.0 * 8.33940315246582
Epoch 1580, val loss: 0.5531772375106812
Epoch 1590, training loss: 834.09033203125 = 0.41704219579696655 + 100.0 * 8.336732864379883
Epoch 1590, val loss: 0.5525420904159546
Epoch 1600, training loss: 833.9466552734375 = 0.4156268537044525 + 100.0 * 8.335309982299805
Epoch 1600, val loss: 0.5520727634429932
Epoch 1610, training loss: 833.9202270507812 = 0.4142262637615204 + 100.0 * 8.335060119628906
Epoch 1610, val loss: 0.5515830516815186
Epoch 1620, training loss: 834.25341796875 = 0.4128146767616272 + 100.0 * 8.33840560913086
Epoch 1620, val loss: 0.5510345101356506
Epoch 1630, training loss: 833.8743286132812 = 0.41130828857421875 + 100.0 * 8.334630012512207
Epoch 1630, val loss: 0.5503132343292236
Epoch 1640, training loss: 833.8665771484375 = 0.40982183814048767 + 100.0 * 8.334567070007324
Epoch 1640, val loss: 0.5496724247932434
Epoch 1650, training loss: 833.8592529296875 = 0.4083656072616577 + 100.0 * 8.334508895874023
Epoch 1650, val loss: 0.5491868853569031
Epoch 1660, training loss: 834.165771484375 = 0.406893789768219 + 100.0 * 8.3375883102417
Epoch 1660, val loss: 0.5486576557159424
Epoch 1670, training loss: 833.7647705078125 = 0.4053477644920349 + 100.0 * 8.33359432220459
Epoch 1670, val loss: 0.547909677028656
Epoch 1680, training loss: 833.7615966796875 = 0.4038352966308594 + 100.0 * 8.333578109741211
Epoch 1680, val loss: 0.5474134087562561
Epoch 1690, training loss: 833.7130126953125 = 0.4023488461971283 + 100.0 * 8.333106994628906
Epoch 1690, val loss: 0.5469701290130615
Epoch 1700, training loss: 834.1220703125 = 0.4008427560329437 + 100.0 * 8.337212562561035
Epoch 1700, val loss: 0.5462815761566162
Epoch 1710, training loss: 834.8548583984375 = 0.39927923679351807 + 100.0 * 8.344555854797363
Epoch 1710, val loss: 0.5453588962554932
Epoch 1720, training loss: 833.8887329101562 = 0.3976008892059326 + 100.0 * 8.334911346435547
Epoch 1720, val loss: 0.5450641512870789
Epoch 1730, training loss: 833.6990966796875 = 0.3960253596305847 + 100.0 * 8.333030700683594
Epoch 1730, val loss: 0.544522225856781
Epoch 1740, training loss: 833.538330078125 = 0.3945096731185913 + 100.0 * 8.331438064575195
Epoch 1740, val loss: 0.5440212488174438
Epoch 1750, training loss: 833.5118408203125 = 0.39300066232681274 + 100.0 * 8.331188201904297
Epoch 1750, val loss: 0.5434771180152893
Epoch 1760, training loss: 833.478271484375 = 0.39147859811782837 + 100.0 * 8.330867767333984
Epoch 1760, val loss: 0.5430013537406921
Epoch 1770, training loss: 833.58935546875 = 0.3899422287940979 + 100.0 * 8.33199405670166
Epoch 1770, val loss: 0.5425822734832764
Epoch 1780, training loss: 834.2196044921875 = 0.38835111260414124 + 100.0 * 8.338312149047852
Epoch 1780, val loss: 0.5420703291893005
Epoch 1790, training loss: 833.6484985351562 = 0.38666731119155884 + 100.0 * 8.332618713378906
Epoch 1790, val loss: 0.5412909388542175
Epoch 1800, training loss: 833.45751953125 = 0.3850683867931366 + 100.0 * 8.330724716186523
Epoch 1800, val loss: 0.5408578515052795
Epoch 1810, training loss: 833.519775390625 = 0.3835025131702423 + 100.0 * 8.3313627243042
Epoch 1810, val loss: 0.5403009057044983
Epoch 1820, training loss: 833.73828125 = 0.3819023072719574 + 100.0 * 8.333563804626465
Epoch 1820, val loss: 0.5398189425468445
Epoch 1830, training loss: 833.456298828125 = 0.38029056787490845 + 100.0 * 8.33076000213623
Epoch 1830, val loss: 0.539358913898468
Epoch 1840, training loss: 833.4188842773438 = 0.3787023425102234 + 100.0 * 8.330401420593262
Epoch 1840, val loss: 0.5388356447219849
Epoch 1850, training loss: 833.6243286132812 = 0.3771345615386963 + 100.0 * 8.33247184753418
Epoch 1850, val loss: 0.5384286642074585
Epoch 1860, training loss: 833.4220581054688 = 0.37551939487457275 + 100.0 * 8.330465316772461
Epoch 1860, val loss: 0.5378875732421875
Epoch 1870, training loss: 833.4744262695312 = 0.37392210960388184 + 100.0 * 8.331005096435547
Epoch 1870, val loss: 0.5374981760978699
Epoch 1880, training loss: 833.2510986328125 = 0.37230435013771057 + 100.0 * 8.328787803649902
Epoch 1880, val loss: 0.5370004773139954
Epoch 1890, training loss: 833.2421875 = 0.37071093916893005 + 100.0 * 8.328714370727539
Epoch 1890, val loss: 0.5365846157073975
Epoch 1900, training loss: 833.4083251953125 = 0.3691279888153076 + 100.0 * 8.330391883850098
Epoch 1900, val loss: 0.5362125039100647
Epoch 1910, training loss: 833.2708740234375 = 0.36749589443206787 + 100.0 * 8.329033851623535
Epoch 1910, val loss: 0.5355949401855469
Epoch 1920, training loss: 833.5060424804688 = 0.36586877703666687 + 100.0 * 8.331401824951172
Epoch 1920, val loss: 0.5351701974868774
Epoch 1930, training loss: 833.68896484375 = 0.3642459809780121 + 100.0 * 8.333247184753418
Epoch 1930, val loss: 0.5348485708236694
Epoch 1940, training loss: 833.1897583007812 = 0.36254173517227173 + 100.0 * 8.328271865844727
Epoch 1940, val loss: 0.5340403914451599
Epoch 1950, training loss: 833.0867919921875 = 0.36093002557754517 + 100.0 * 8.327259063720703
Epoch 1950, val loss: 0.5337180495262146
Epoch 1960, training loss: 833.0195922851562 = 0.3593416213989258 + 100.0 * 8.326602935791016
Epoch 1960, val loss: 0.5333958268165588
Epoch 1970, training loss: 833.0741577148438 = 0.3577602207660675 + 100.0 * 8.327163696289062
Epoch 1970, val loss: 0.5330272316932678
Epoch 1980, training loss: 833.6762084960938 = 0.3561484217643738 + 100.0 * 8.333200454711914
Epoch 1980, val loss: 0.5325261950492859
Epoch 1990, training loss: 833.2677001953125 = 0.3544709384441376 + 100.0 * 8.329132080078125
Epoch 1990, val loss: 0.5321768522262573
Epoch 2000, training loss: 833.0516967773438 = 0.35282382369041443 + 100.0 * 8.326988220214844
Epoch 2000, val loss: 0.5318464636802673
Epoch 2010, training loss: 833.0872192382812 = 0.3512161076068878 + 100.0 * 8.327360153198242
Epoch 2010, val loss: 0.5314889550209045
Epoch 2020, training loss: 834.0252075195312 = 0.34963738918304443 + 100.0 * 8.336755752563477
Epoch 2020, val loss: 0.5307234525680542
Epoch 2030, training loss: 833.2557373046875 = 0.3478977382183075 + 100.0 * 8.329078674316406
Epoch 2030, val loss: 0.5307784676551819
Epoch 2040, training loss: 832.9805297851562 = 0.34625011682510376 + 100.0 * 8.326342582702637
Epoch 2040, val loss: 0.5304304957389832
Epoch 2050, training loss: 832.8921508789062 = 0.34465646743774414 + 100.0 * 8.325474739074707
Epoch 2050, val loss: 0.5301077365875244
Epoch 2060, training loss: 832.8417358398438 = 0.3430754542350769 + 100.0 * 8.324986457824707
Epoch 2060, val loss: 0.5299251079559326
Epoch 2070, training loss: 832.8336791992188 = 0.3414945602416992 + 100.0 * 8.324921607971191
Epoch 2070, val loss: 0.5297107100486755
Epoch 2080, training loss: 833.7631225585938 = 0.3399169147014618 + 100.0 * 8.334232330322266
Epoch 2080, val loss: 0.5295055508613586
Epoch 2090, training loss: 832.9952392578125 = 0.33821868896484375 + 100.0 * 8.326570510864258
Epoch 2090, val loss: 0.5290685296058655
Epoch 2100, training loss: 832.9019165039062 = 0.33658239245414734 + 100.0 * 8.325653076171875
Epoch 2100, val loss: 0.5288154482841492
Epoch 2110, training loss: 832.840087890625 = 0.3349883556365967 + 100.0 * 8.325051307678223
Epoch 2110, val loss: 0.5286077857017517
Epoch 2120, training loss: 833.0751953125 = 0.33340758085250854 + 100.0 * 8.327417373657227
Epoch 2120, val loss: 0.5284161567687988
Epoch 2130, training loss: 832.8707885742188 = 0.33178722858428955 + 100.0 * 8.325389862060547
Epoch 2130, val loss: 0.5282580256462097
Epoch 2140, training loss: 832.8328247070312 = 0.33018141984939575 + 100.0 * 8.325026512145996
Epoch 2140, val loss: 0.5281277298927307
Epoch 2150, training loss: 832.9661254882812 = 0.328611820936203 + 100.0 * 8.326375007629395
Epoch 2150, val loss: 0.5277571082115173
Epoch 2160, training loss: 832.9913330078125 = 0.3269988000392914 + 100.0 * 8.326642990112305
Epoch 2160, val loss: 0.5279243588447571
Epoch 2170, training loss: 832.7869262695312 = 0.32538971304893494 + 100.0 * 8.324615478515625
Epoch 2170, val loss: 0.5280001759529114
Epoch 2180, training loss: 832.684326171875 = 0.3238147795200348 + 100.0 * 8.32360553741455
Epoch 2180, val loss: 0.5280289649963379
Epoch 2190, training loss: 832.6420288085938 = 0.3222610652446747 + 100.0 * 8.323197364807129
Epoch 2190, val loss: 0.5281065702438354
Epoch 2200, training loss: 832.997802734375 = 0.32072800397872925 + 100.0 * 8.326770782470703
Epoch 2200, val loss: 0.5282309055328369
Epoch 2210, training loss: 832.8515625 = 0.31911417841911316 + 100.0 * 8.325325012207031
Epoch 2210, val loss: 0.5279144644737244
Epoch 2220, training loss: 832.6749877929688 = 0.3175208270549774 + 100.0 * 8.323575019836426
Epoch 2220, val loss: 0.5280104875564575
Epoch 2230, training loss: 832.600341796875 = 0.31597185134887695 + 100.0 * 8.322843551635742
Epoch 2230, val loss: 0.5278564095497131
Epoch 2240, training loss: 832.587158203125 = 0.31444570422172546 + 100.0 * 8.32272720336914
Epoch 2240, val loss: 0.527961790561676
Epoch 2250, training loss: 832.7488403320312 = 0.31294476985931396 + 100.0 * 8.324358940124512
Epoch 2250, val loss: 0.527828574180603
Epoch 2260, training loss: 832.8617553710938 = 0.3114074170589447 + 100.0 * 8.3255033493042
Epoch 2260, val loss: 0.527863085269928
Epoch 2270, training loss: 832.8203125 = 0.3098316788673401 + 100.0 * 8.325104713439941
Epoch 2270, val loss: 0.5280919671058655
Epoch 2280, training loss: 832.6884155273438 = 0.30829817056655884 + 100.0 * 8.323801040649414
Epoch 2280, val loss: 0.5283635854721069
Epoch 2290, training loss: 832.6216430664062 = 0.3067898750305176 + 100.0 * 8.323148727416992
Epoch 2290, val loss: 0.5286073088645935
Epoch 2300, training loss: 832.64697265625 = 0.30528002977371216 + 100.0 * 8.323416709899902
Epoch 2300, val loss: 0.5284742712974548
Epoch 2310, training loss: 832.5103759765625 = 0.30376508831977844 + 100.0 * 8.322066307067871
Epoch 2310, val loss: 0.5286062359809875
Epoch 2320, training loss: 832.4658813476562 = 0.3022654950618744 + 100.0 * 8.321636199951172
Epoch 2320, val loss: 0.5287461876869202
Epoch 2330, training loss: 832.7724609375 = 0.3007814586162567 + 100.0 * 8.324716567993164
Epoch 2330, val loss: 0.5287246704101562
Epoch 2340, training loss: 832.9654541015625 = 0.2993181049823761 + 100.0 * 8.326661109924316
Epoch 2340, val loss: 0.5299135446548462
Epoch 2350, training loss: 832.5744018554688 = 0.2976933717727661 + 100.0 * 8.32276725769043
Epoch 2350, val loss: 0.5289410948753357
Epoch 2360, training loss: 832.515625 = 0.2961827516555786 + 100.0 * 8.32219409942627
Epoch 2360, val loss: 0.5294453501701355
Epoch 2370, training loss: 832.4207763671875 = 0.29471203684806824 + 100.0 * 8.321260452270508
Epoch 2370, val loss: 0.5296178460121155
Epoch 2380, training loss: 832.4088134765625 = 0.293255478143692 + 100.0 * 8.321155548095703
Epoch 2380, val loss: 0.5298413038253784
Epoch 2390, training loss: 832.89794921875 = 0.29180648922920227 + 100.0 * 8.326061248779297
Epoch 2390, val loss: 0.5299718976020813
Epoch 2400, training loss: 832.3988037109375 = 0.29030436277389526 + 100.0 * 8.321084976196289
Epoch 2400, val loss: 0.5305338501930237
Epoch 2410, training loss: 832.2904052734375 = 0.2888258099555969 + 100.0 * 8.320015907287598
Epoch 2410, val loss: 0.5303957462310791
Epoch 2420, training loss: 832.4166870117188 = 0.28739187121391296 + 100.0 * 8.321292877197266
Epoch 2420, val loss: 0.5305191874504089
Epoch 2430, training loss: 832.7944946289062 = 0.28596171736717224 + 100.0 * 8.325085639953613
Epoch 2430, val loss: 0.5305911898612976
Epoch 2440, training loss: 832.5906982421875 = 0.28447648882865906 + 100.0 * 8.3230619430542
Epoch 2440, val loss: 0.5316476821899414
Epoch 2450, training loss: 832.3812255859375 = 0.2829888164997101 + 100.0 * 8.320982933044434
Epoch 2450, val loss: 0.531399130821228
Epoch 2460, training loss: 832.697265625 = 0.28159114718437195 + 100.0 * 8.324156761169434
Epoch 2460, val loss: 0.5323326587677002
Epoch 2470, training loss: 832.2803344726562 = 0.2800903916358948 + 100.0 * 8.320002555847168
Epoch 2470, val loss: 0.5321177244186401
Epoch 2480, training loss: 832.3247680664062 = 0.27865368127822876 + 100.0 * 8.32046127319336
Epoch 2480, val loss: 0.5323504209518433
Epoch 2490, training loss: 832.3523559570312 = 0.2772243022918701 + 100.0 * 8.320751190185547
Epoch 2490, val loss: 0.5326051115989685
Epoch 2500, training loss: 832.3736572265625 = 0.27581140398979187 + 100.0 * 8.320978164672852
Epoch 2500, val loss: 0.533251166343689
Epoch 2510, training loss: 832.3568115234375 = 0.27437421679496765 + 100.0 * 8.32082462310791
Epoch 2510, val loss: 0.5334920287132263
Epoch 2520, training loss: 832.22216796875 = 0.2729329466819763 + 100.0 * 8.31949234008789
Epoch 2520, val loss: 0.5333962440490723
Epoch 2530, training loss: 832.197021484375 = 0.27152019739151 + 100.0 * 8.319254875183105
Epoch 2530, val loss: 0.5338663458824158
Epoch 2540, training loss: 832.4874267578125 = 0.27013763785362244 + 100.0 * 8.322173118591309
Epoch 2540, val loss: 0.5346157550811768
Epoch 2550, training loss: 832.4553833007812 = 0.26870524883270264 + 100.0 * 8.321866989135742
Epoch 2550, val loss: 0.5346256494522095
Epoch 2560, training loss: 832.2689819335938 = 0.2672746479511261 + 100.0 * 8.320016860961914
Epoch 2560, val loss: 0.5347356200218201
Epoch 2570, training loss: 832.1807861328125 = 0.2658807933330536 + 100.0 * 8.319149017333984
Epoch 2570, val loss: 0.53476881980896
Epoch 2580, training loss: 832.3776245117188 = 0.2644897699356079 + 100.0 * 8.321131706237793
Epoch 2580, val loss: 0.5352293848991394
Epoch 2590, training loss: 832.1972045898438 = 0.26308560371398926 + 100.0 * 8.319341659545898
Epoch 2590, val loss: 0.5356546640396118
Epoch 2600, training loss: 832.1825561523438 = 0.2616809010505676 + 100.0 * 8.319209098815918
Epoch 2600, val loss: 0.5367816686630249
Epoch 2610, training loss: 832.23974609375 = 0.26028427481651306 + 100.0 * 8.319794654846191
Epoch 2610, val loss: 0.5369727611541748
Epoch 2620, training loss: 832.1932373046875 = 0.2588838040828705 + 100.0 * 8.319343566894531
Epoch 2620, val loss: 0.5373519659042358
Epoch 2630, training loss: 832.2153930664062 = 0.2574872672557831 + 100.0 * 8.319579124450684
Epoch 2630, val loss: 0.5378146767616272
Epoch 2640, training loss: 832.09912109375 = 0.25609949231147766 + 100.0 * 8.318429946899414
Epoch 2640, val loss: 0.5384529829025269
Epoch 2650, training loss: 832.1149291992188 = 0.2547239065170288 + 100.0 * 8.318602561950684
Epoch 2650, val loss: 0.5391131043434143
Epoch 2660, training loss: 832.0695190429688 = 0.2533246576786041 + 100.0 * 8.318161964416504
Epoch 2660, val loss: 0.5394744277000427
Epoch 2670, training loss: 832.021484375 = 0.25192806124687195 + 100.0 * 8.317695617675781
Epoch 2670, val loss: 0.5396849513053894
Epoch 2680, training loss: 832.3085327148438 = 0.25057631731033325 + 100.0 * 8.320579528808594
Epoch 2680, val loss: 0.5397666692733765
Epoch 2690, training loss: 832.375 = 0.24917134642601013 + 100.0 * 8.321258544921875
Epoch 2690, val loss: 0.5407491326332092
Epoch 2700, training loss: 832.0240478515625 = 0.247770756483078 + 100.0 * 8.317763328552246
Epoch 2700, val loss: 0.5416170954704285
Epoch 2710, training loss: 831.8860473632812 = 0.2463950365781784 + 100.0 * 8.316396713256836
Epoch 2710, val loss: 0.5419091582298279
Epoch 2720, training loss: 831.8662109375 = 0.24504055082798004 + 100.0 * 8.316211700439453
Epoch 2720, val loss: 0.5428122878074646
Epoch 2730, training loss: 831.99560546875 = 0.2437281459569931 + 100.0 * 8.317519187927246
Epoch 2730, val loss: 0.5439519286155701
Epoch 2740, training loss: 832.5357666015625 = 0.2423998862504959 + 100.0 * 8.3229341506958
Epoch 2740, val loss: 0.5442266464233398
Epoch 2750, training loss: 832.0084228515625 = 0.24094152450561523 + 100.0 * 8.31767463684082
Epoch 2750, val loss: 0.544487714767456
Epoch 2760, training loss: 831.8640747070312 = 0.23958010971546173 + 100.0 * 8.316245079040527
Epoch 2760, val loss: 0.5449888706207275
Epoch 2770, training loss: 831.9287109375 = 0.2382526844739914 + 100.0 * 8.316904067993164
Epoch 2770, val loss: 0.5459448099136353
Epoch 2780, training loss: 831.9483032226562 = 0.23692594468593597 + 100.0 * 8.317113876342773
Epoch 2780, val loss: 0.5463765263557434
Epoch 2790, training loss: 831.7880249023438 = 0.23559452593326569 + 100.0 * 8.315524101257324
Epoch 2790, val loss: 0.5469412207603455
Epoch 2800, training loss: 832.3392944335938 = 0.23431642353534698 + 100.0 * 8.321049690246582
Epoch 2800, val loss: 0.5470249056816101
Epoch 2810, training loss: 832.3275146484375 = 0.23300497233867645 + 100.0 * 8.320944786071777
Epoch 2810, val loss: 0.5475639700889587
Epoch 2820, training loss: 831.9242553710938 = 0.231622576713562 + 100.0 * 8.316926002502441
Epoch 2820, val loss: 0.5490773916244507
Epoch 2830, training loss: 831.7770385742188 = 0.23032769560813904 + 100.0 * 8.31546688079834
Epoch 2830, val loss: 0.5497224926948547
Epoch 2840, training loss: 831.7127685546875 = 0.22903114557266235 + 100.0 * 8.314837455749512
Epoch 2840, val loss: 0.5503422021865845
Epoch 2850, training loss: 831.6793212890625 = 0.2277485877275467 + 100.0 * 8.314516067504883
Epoch 2850, val loss: 0.551171064376831
Epoch 2860, training loss: 831.9451293945312 = 0.2264871746301651 + 100.0 * 8.31718635559082
Epoch 2860, val loss: 0.5517024993896484
Epoch 2870, training loss: 831.7728881835938 = 0.2251802384853363 + 100.0 * 8.31547737121582
Epoch 2870, val loss: 0.5523987412452698
Epoch 2880, training loss: 832.0110473632812 = 0.22392435371875763 + 100.0 * 8.31787109375
Epoch 2880, val loss: 0.552737295627594
Epoch 2890, training loss: 831.73095703125 = 0.2225920855998993 + 100.0 * 8.315083503723145
Epoch 2890, val loss: 0.5538166761398315
Epoch 2900, training loss: 831.6737060546875 = 0.22130070626735687 + 100.0 * 8.314523696899414
Epoch 2900, val loss: 0.5553122162818909
Epoch 2910, training loss: 831.6419677734375 = 0.220022514462471 + 100.0 * 8.31421947479248
Epoch 2910, val loss: 0.5558909773826599
Epoch 2920, training loss: 832.0916748046875 = 0.2187660038471222 + 100.0 * 8.318729400634766
Epoch 2920, val loss: 0.5561521053314209
Epoch 2930, training loss: 831.732421875 = 0.2174571305513382 + 100.0 * 8.315149307250977
Epoch 2930, val loss: 0.5571770668029785
Epoch 2940, training loss: 831.5780639648438 = 0.21616323292255402 + 100.0 * 8.313618659973145
Epoch 2940, val loss: 0.558178186416626
Epoch 2950, training loss: 831.5631103515625 = 0.21488913893699646 + 100.0 * 8.313482284545898
Epoch 2950, val loss: 0.559211015701294
Epoch 2960, training loss: 831.5382080078125 = 0.21361877024173737 + 100.0 * 8.31324577331543
Epoch 2960, val loss: 0.5597773194313049
Epoch 2970, training loss: 831.6328735351562 = 0.21236726641654968 + 100.0 * 8.314205169677734
Epoch 2970, val loss: 0.5605905055999756
Epoch 2980, training loss: 832.2654418945312 = 0.21112708747386932 + 100.0 * 8.32054328918457
Epoch 2980, val loss: 0.5613414645195007
Epoch 2990, training loss: 831.7221069335938 = 0.209835946559906 + 100.0 * 8.315122604370117
Epoch 2990, val loss: 0.5624115467071533
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.786910197869102
0.8417010794754765
The final CL Acc:0.79909, 0.00865, The final GNN Acc:0.84158, 0.00034
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 110698])
remove edge: torch.Size([2, 66604])
updated graph: torch.Size([2, 88654])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 1059.3291015625 = 1.1037625074386597 + 100.0 * 10.582253456115723
Epoch 0, val loss: 1.1039059162139893
Epoch 10, training loss: 1059.205078125 = 1.0969948768615723 + 100.0 * 10.581080436706543
Epoch 10, val loss: 1.097023367881775
Epoch 20, training loss: 1058.1356201171875 = 1.0888746976852417 + 100.0 * 10.570467948913574
Epoch 20, val loss: 1.0887551307678223
Epoch 30, training loss: 1050.32275390625 = 1.0794752836227417 + 100.0 * 10.492432594299316
Epoch 30, val loss: 1.0791871547698975
Epoch 40, training loss: 1019.4132080078125 = 1.06924307346344 + 100.0 * 10.183440208435059
Epoch 40, val loss: 1.0688444375991821
Epoch 50, training loss: 960.3637084960938 = 1.0593864917755127 + 100.0 * 9.593043327331543
Epoch 50, val loss: 1.0591191053390503
Epoch 60, training loss: 934.7232666015625 = 1.0520780086517334 + 100.0 * 9.336711883544922
Epoch 60, val loss: 1.0516393184661865
Epoch 70, training loss: 919.4358520507812 = 1.0446052551269531 + 100.0 * 9.18391227722168
Epoch 70, val loss: 1.0442665815353394
Epoch 80, training loss: 909.88134765625 = 1.0374897718429565 + 100.0 * 9.088438987731934
Epoch 80, val loss: 1.037327766418457
Epoch 90, training loss: 901.0020751953125 = 1.0312285423278809 + 100.0 * 8.99970817565918
Epoch 90, val loss: 1.031237244606018
Epoch 100, training loss: 890.9806518554688 = 1.0260473489761353 + 100.0 * 8.899545669555664
Epoch 100, val loss: 1.0262067317962646
Epoch 110, training loss: 883.6160888671875 = 1.0212630033493042 + 100.0 * 8.825948715209961
Epoch 110, val loss: 1.0214430093765259
Epoch 120, training loss: 877.2831420898438 = 1.0159608125686646 + 100.0 * 8.76267147064209
Epoch 120, val loss: 1.0161360502243042
Epoch 130, training loss: 872.9508666992188 = 1.0100665092468262 + 100.0 * 8.71940803527832
Epoch 130, val loss: 1.0102306604385376
Epoch 140, training loss: 869.9899291992188 = 1.0036178827285767 + 100.0 * 8.689863204956055
Epoch 140, val loss: 1.0037676095962524
Epoch 150, training loss: 867.2849731445312 = 0.996545135974884 + 100.0 * 8.662884712219238
Epoch 150, val loss: 0.996657133102417
Epoch 160, training loss: 864.8614501953125 = 0.9888203144073486 + 100.0 * 8.638726234436035
Epoch 160, val loss: 0.9888798594474792
Epoch 170, training loss: 863.4237670898438 = 0.9802082777023315 + 100.0 * 8.624435424804688
Epoch 170, val loss: 0.9802733063697815
Epoch 180, training loss: 861.4329223632812 = 0.9706229567527771 + 100.0 * 8.604622840881348
Epoch 180, val loss: 0.9706870913505554
Epoch 190, training loss: 859.8118896484375 = 0.960382878780365 + 100.0 * 8.588515281677246
Epoch 190, val loss: 0.9604929089546204
Epoch 200, training loss: 858.305908203125 = 0.9496538043022156 + 100.0 * 8.573562622070312
Epoch 200, val loss: 0.9497687220573425
Epoch 210, training loss: 856.7509155273438 = 0.9382960796356201 + 100.0 * 8.558126449584961
Epoch 210, val loss: 0.9383945465087891
Epoch 220, training loss: 855.2860717773438 = 0.9262698888778687 + 100.0 * 8.543598175048828
Epoch 220, val loss: 0.9263534545898438
Epoch 230, training loss: 854.1173706054688 = 0.9135895371437073 + 100.0 * 8.532037734985352
Epoch 230, val loss: 0.9135718941688538
Epoch 240, training loss: 852.8152465820312 = 0.8998915553092957 + 100.0 * 8.519153594970703
Epoch 240, val loss: 0.8998908996582031
Epoch 250, training loss: 851.6062622070312 = 0.8857353329658508 + 100.0 * 8.50720500946045
Epoch 250, val loss: 0.8856823444366455
Epoch 260, training loss: 850.5462036132812 = 0.8710952401161194 + 100.0 * 8.496750831604004
Epoch 260, val loss: 0.8710519671440125
Epoch 270, training loss: 849.6297607421875 = 0.8560231328010559 + 100.0 * 8.487737655639648
Epoch 270, val loss: 0.8559489846229553
Epoch 280, training loss: 850.005859375 = 0.8405372500419617 + 100.0 * 8.491653442382812
Epoch 280, val loss: 0.8403741717338562
Epoch 290, training loss: 848.1290893554688 = 0.8243694305419922 + 100.0 * 8.473047256469727
Epoch 290, val loss: 0.8243889808654785
Epoch 300, training loss: 847.3562622070312 = 0.8081229329109192 + 100.0 * 8.465481758117676
Epoch 300, val loss: 0.8081897497177124
Epoch 310, training loss: 846.8082885742188 = 0.7918367981910706 + 100.0 * 8.460165023803711
Epoch 310, val loss: 0.7920321226119995
Epoch 320, training loss: 846.479736328125 = 0.7753956317901611 + 100.0 * 8.457043647766113
Epoch 320, val loss: 0.7758039832115173
Epoch 330, training loss: 845.7664794921875 = 0.759009838104248 + 100.0 * 8.450074195861816
Epoch 330, val loss: 0.7596222758293152
Epoch 340, training loss: 845.1034545898438 = 0.7429311871528625 + 100.0 * 8.443605422973633
Epoch 340, val loss: 0.743842601776123
Epoch 350, training loss: 844.5908813476562 = 0.7272480726242065 + 100.0 * 8.438636779785156
Epoch 350, val loss: 0.7284907698631287
Epoch 360, training loss: 845.09228515625 = 0.7119786143302917 + 100.0 * 8.443802833557129
Epoch 360, val loss: 0.7135564684867859
Epoch 370, training loss: 843.6519165039062 = 0.6970279812812805 + 100.0 * 8.429549217224121
Epoch 370, val loss: 0.6990867853164673
Epoch 380, training loss: 843.2968139648438 = 0.6827854514122009 + 100.0 * 8.426139831542969
Epoch 380, val loss: 0.6853396892547607
Epoch 390, training loss: 842.7736206054688 = 0.6692664623260498 + 100.0 * 8.421043395996094
Epoch 390, val loss: 0.672301709651947
Epoch 400, training loss: 842.5381469726562 = 0.6563761830329895 + 100.0 * 8.418817520141602
Epoch 400, val loss: 0.6599359512329102
Epoch 410, training loss: 842.0286254882812 = 0.6441117525100708 + 100.0 * 8.41384506225586
Epoch 410, val loss: 0.6482725739479065
Epoch 420, training loss: 841.6986083984375 = 0.632623016834259 + 100.0 * 8.410659790039062
Epoch 420, val loss: 0.6373826265335083
Epoch 430, training loss: 841.3272705078125 = 0.6217896342277527 + 100.0 * 8.407054901123047
Epoch 430, val loss: 0.6271567344665527
Epoch 440, training loss: 840.986572265625 = 0.6116401553153992 + 100.0 * 8.403749465942383
Epoch 440, val loss: 0.6176385879516602
Epoch 450, training loss: 840.8194580078125 = 0.6022277474403381 + 100.0 * 8.402172088623047
Epoch 450, val loss: 0.6089110970497131
Epoch 460, training loss: 840.3818969726562 = 0.5933719277381897 + 100.0 * 8.3978853225708
Epoch 460, val loss: 0.6006415486335754
Epoch 470, training loss: 840.1976928710938 = 0.5851306319236755 + 100.0 * 8.396125793457031
Epoch 470, val loss: 0.5931172966957092
Epoch 480, training loss: 839.83740234375 = 0.5775406360626221 + 100.0 * 8.392599105834961
Epoch 480, val loss: 0.5862004160881042
Epoch 490, training loss: 839.562744140625 = 0.5705416202545166 + 100.0 * 8.389922142028809
Epoch 490, val loss: 0.5798771381378174
Epoch 500, training loss: 839.4386596679688 = 0.5640433430671692 + 100.0 * 8.38874626159668
Epoch 500, val loss: 0.5740702748298645
Epoch 510, training loss: 839.3225708007812 = 0.5579298734664917 + 100.0 * 8.387646675109863
Epoch 510, val loss: 0.5685893893241882
Epoch 520, training loss: 839.65234375 = 0.5521763563156128 + 100.0 * 8.39100170135498
Epoch 520, val loss: 0.5635570883750916
Epoch 530, training loss: 838.9313354492188 = 0.5468384623527527 + 100.0 * 8.383845329284668
Epoch 530, val loss: 0.5589395761489868
Epoch 540, training loss: 838.5859985351562 = 0.5419461131095886 + 100.0 * 8.380440711975098
Epoch 540, val loss: 0.5547427535057068
Epoch 550, training loss: 838.2950439453125 = 0.53746497631073 + 100.0 * 8.377575874328613
Epoch 550, val loss: 0.5509712100028992
Epoch 560, training loss: 838.348876953125 = 0.5332974791526794 + 100.0 * 8.378155708312988
Epoch 560, val loss: 0.5475178956985474
Epoch 570, training loss: 838.0240478515625 = 0.5292709469795227 + 100.0 * 8.374947547912598
Epoch 570, val loss: 0.5441678166389465
Epoch 580, training loss: 837.9005126953125 = 0.5255107283592224 + 100.0 * 8.373749732971191
Epoch 580, val loss: 0.5411638021469116
Epoch 590, training loss: 837.6788940429688 = 0.5220402479171753 + 100.0 * 8.37156867980957
Epoch 590, val loss: 0.5383925437927246
Epoch 600, training loss: 837.718017578125 = 0.5188249349594116 + 100.0 * 8.371992111206055
Epoch 600, val loss: 0.5359442830085754
Epoch 610, training loss: 837.8519287109375 = 0.5157027840614319 + 100.0 * 8.37336254119873
Epoch 610, val loss: 0.533405065536499
Epoch 620, training loss: 837.2905883789062 = 0.5127183198928833 + 100.0 * 8.367778778076172
Epoch 620, val loss: 0.5312442183494568
Epoch 630, training loss: 837.0895385742188 = 0.5099790096282959 + 100.0 * 8.365796089172363
Epoch 630, val loss: 0.5292474627494812
Epoch 640, training loss: 836.965087890625 = 0.5074501037597656 + 100.0 * 8.36457633972168
Epoch 640, val loss: 0.5274681448936462
Epoch 650, training loss: 837.2828369140625 = 0.5050373673439026 + 100.0 * 8.367777824401855
Epoch 650, val loss: 0.525751531124115
Epoch 660, training loss: 836.7686767578125 = 0.502650797367096 + 100.0 * 8.36266040802002
Epoch 660, val loss: 0.5241557955741882
Epoch 670, training loss: 836.6365966796875 = 0.5004047751426697 + 100.0 * 8.361361503601074
Epoch 670, val loss: 0.5226410627365112
Epoch 680, training loss: 836.5632934570312 = 0.49832436442375183 + 100.0 * 8.360649108886719
Epoch 680, val loss: 0.5213208198547363
Epoch 690, training loss: 836.6483764648438 = 0.49630630016326904 + 100.0 * 8.361520767211914
Epoch 690, val loss: 0.5200331807136536
Epoch 700, training loss: 836.34765625 = 0.49437475204467773 + 100.0 * 8.358532905578613
Epoch 700, val loss: 0.5188848972320557
Epoch 710, training loss: 836.2296752929688 = 0.49255961179733276 + 100.0 * 8.35737133026123
Epoch 710, val loss: 0.517817497253418
Epoch 720, training loss: 836.4208374023438 = 0.49082452058792114 + 100.0 * 8.359299659729004
Epoch 720, val loss: 0.5168582797050476
Epoch 730, training loss: 836.1581420898438 = 0.4891009032726288 + 100.0 * 8.356690406799316
Epoch 730, val loss: 0.5159449577331543
Epoch 740, training loss: 835.8421630859375 = 0.4874371886253357 + 100.0 * 8.353547096252441
Epoch 740, val loss: 0.5149533152580261
Epoch 750, training loss: 835.7828369140625 = 0.48590323328971863 + 100.0 * 8.3529691696167
Epoch 750, val loss: 0.5141773819923401
Epoch 760, training loss: 836.4537963867188 = 0.4843868911266327 + 100.0 * 8.359694480895996
Epoch 760, val loss: 0.5133771300315857
Epoch 770, training loss: 835.760498046875 = 0.4828754961490631 + 100.0 * 8.352776527404785
Epoch 770, val loss: 0.5127049684524536
Epoch 780, training loss: 835.4771118164062 = 0.4814252555370331 + 100.0 * 8.349956512451172
Epoch 780, val loss: 0.5119443535804749
Epoch 790, training loss: 835.530517578125 = 0.4800852835178375 + 100.0 * 8.350503921508789
Epoch 790, val loss: 0.5113712549209595
Epoch 800, training loss: 835.2753295898438 = 0.47872576117515564 + 100.0 * 8.347966194152832
Epoch 800, val loss: 0.5107001066207886
Epoch 810, training loss: 835.27978515625 = 0.47740429639816284 + 100.0 * 8.348023414611816
Epoch 810, val loss: 0.5101612210273743
Epoch 820, training loss: 835.4090576171875 = 0.476129412651062 + 100.0 * 8.349328994750977
Epoch 820, val loss: 0.5095856189727783
Epoch 830, training loss: 835.1077270507812 = 0.4748477041721344 + 100.0 * 8.346328735351562
Epoch 830, val loss: 0.5091139674186707
Epoch 840, training loss: 835.0987548828125 = 0.47362658381462097 + 100.0 * 8.346251487731934
Epoch 840, val loss: 0.5085837244987488
Epoch 850, training loss: 835.057373046875 = 0.4723297953605652 + 100.0 * 8.345849990844727
Epoch 850, val loss: 0.507966160774231
Epoch 860, training loss: 834.9015502929688 = 0.47104158997535706 + 100.0 * 8.344305038452148
Epoch 860, val loss: 0.5074448585510254
Epoch 870, training loss: 834.7456665039062 = 0.4698573648929596 + 100.0 * 8.342758178710938
Epoch 870, val loss: 0.5069597363471985
Epoch 880, training loss: 834.6204223632812 = 0.46876123547554016 + 100.0 * 8.341516494750977
Epoch 880, val loss: 0.5065851807594299
Epoch 890, training loss: 834.54345703125 = 0.46767839789390564 + 100.0 * 8.340758323669434
Epoch 890, val loss: 0.5061966180801392
Epoch 900, training loss: 834.6411743164062 = 0.46660324931144714 + 100.0 * 8.341745376586914
Epoch 900, val loss: 0.5057245492935181
Epoch 910, training loss: 834.5532836914062 = 0.46542802453041077 + 100.0 * 8.3408784866333
Epoch 910, val loss: 0.5054404139518738
Epoch 920, training loss: 834.5574951171875 = 0.4642261564731598 + 100.0 * 8.340932846069336
Epoch 920, val loss: 0.504792332649231
Epoch 930, training loss: 834.3560791015625 = 0.4631252884864807 + 100.0 * 8.338929176330566
Epoch 930, val loss: 0.504352867603302
Epoch 940, training loss: 834.4588012695312 = 0.4620705246925354 + 100.0 * 8.339966773986816
Epoch 940, val loss: 0.5039991140365601
Epoch 950, training loss: 834.47314453125 = 0.46094590425491333 + 100.0 * 8.34012222290039
Epoch 950, val loss: 0.5035813450813293
Epoch 960, training loss: 834.1989135742188 = 0.4598322808742523 + 100.0 * 8.337390899658203
Epoch 960, val loss: 0.5031649470329285
Epoch 970, training loss: 834.0733032226562 = 0.45876461267471313 + 100.0 * 8.336145401000977
Epoch 970, val loss: 0.5027366280555725
Epoch 980, training loss: 834.0542602539062 = 0.45773881673812866 + 100.0 * 8.335965156555176
Epoch 980, val loss: 0.5024335980415344
Epoch 990, training loss: 834.2962646484375 = 0.456696093082428 + 100.0 * 8.338395118713379
Epoch 990, val loss: 0.5020768642425537
Epoch 1000, training loss: 834.06884765625 = 0.4556029140949249 + 100.0 * 8.336132049560547
Epoch 1000, val loss: 0.5015648007392883
Epoch 1010, training loss: 834.3313598632812 = 0.45451661944389343 + 100.0 * 8.338768005371094
Epoch 1010, val loss: 0.5011439919471741
Epoch 1020, training loss: 833.8916625976562 = 0.4534064829349518 + 100.0 * 8.334382057189941
Epoch 1020, val loss: 0.5006772875785828
Epoch 1030, training loss: 833.7197875976562 = 0.45236945152282715 + 100.0 * 8.332674026489258
Epoch 1030, val loss: 0.5002979636192322
Epoch 1040, training loss: 833.6866455078125 = 0.45136770606040955 + 100.0 * 8.332352638244629
Epoch 1040, val loss: 0.499953955411911
Epoch 1050, training loss: 834.1813354492188 = 0.4503532350063324 + 100.0 * 8.337309837341309
Epoch 1050, val loss: 0.49955645203590393
Epoch 1060, training loss: 834.6841430664062 = 0.44920432567596436 + 100.0 * 8.3423490524292
Epoch 1060, val loss: 0.4990444779396057
Epoch 1070, training loss: 833.7110595703125 = 0.44805628061294556 + 100.0 * 8.332630157470703
Epoch 1070, val loss: 0.4986326992511749
Epoch 1080, training loss: 833.5628051757812 = 0.44700518250465393 + 100.0 * 8.331157684326172
Epoch 1080, val loss: 0.4981772005558014
Epoch 1090, training loss: 833.4345092773438 = 0.4460081160068512 + 100.0 * 8.329885482788086
Epoch 1090, val loss: 0.49780452251434326
Epoch 1100, training loss: 833.3704223632812 = 0.4450286030769348 + 100.0 * 8.329254150390625
Epoch 1100, val loss: 0.4974895715713501
Epoch 1110, training loss: 833.5206909179688 = 0.4440322518348694 + 100.0 * 8.330766677856445
Epoch 1110, val loss: 0.4971042275428772
Epoch 1120, training loss: 833.6160888671875 = 0.44293567538261414 + 100.0 * 8.331731796264648
Epoch 1120, val loss: 0.4966162145137787
Epoch 1130, training loss: 833.2514038085938 = 0.44181105494499207 + 100.0 * 8.328095436096191
Epoch 1130, val loss: 0.4962199628353119
Epoch 1140, training loss: 833.2564086914062 = 0.4407636523246765 + 100.0 * 8.328156471252441
Epoch 1140, val loss: 0.49577951431274414
Epoch 1150, training loss: 833.173095703125 = 0.4397498369216919 + 100.0 * 8.327333450317383
Epoch 1150, val loss: 0.4954526126384735
Epoch 1160, training loss: 833.3707885742188 = 0.4387451410293579 + 100.0 * 8.329320907592773
Epoch 1160, val loss: 0.4950411319732666
Epoch 1170, training loss: 833.2548828125 = 0.4376347064971924 + 100.0 * 8.32817268371582
Epoch 1170, val loss: 0.49466097354888916
Epoch 1180, training loss: 833.0355224609375 = 0.4364994168281555 + 100.0 * 8.325989723205566
Epoch 1180, val loss: 0.4941117465496063
Epoch 1190, training loss: 832.9922485351562 = 0.4354479908943176 + 100.0 * 8.325568199157715
Epoch 1190, val loss: 0.4937140643596649
Epoch 1200, training loss: 832.9432983398438 = 0.43441808223724365 + 100.0 * 8.325088500976562
Epoch 1200, val loss: 0.4933238625526428
Epoch 1210, training loss: 833.2931518554688 = 0.43337687849998474 + 100.0 * 8.328598022460938
Epoch 1210, val loss: 0.49283432960510254
Epoch 1220, training loss: 833.0321044921875 = 0.4322432279586792 + 100.0 * 8.325998306274414
Epoch 1220, val loss: 0.4925652742385864
Epoch 1230, training loss: 832.9718627929688 = 0.4310978353023529 + 100.0 * 8.325407981872559
Epoch 1230, val loss: 0.49193668365478516
Epoch 1240, training loss: 832.8400268554688 = 0.43000900745391846 + 100.0 * 8.324100494384766
Epoch 1240, val loss: 0.4916139245033264
Epoch 1250, training loss: 832.7630615234375 = 0.42895811796188354 + 100.0 * 8.323341369628906
Epoch 1250, val loss: 0.4911557137966156
Epoch 1260, training loss: 832.9891967773438 = 0.42791420221328735 + 100.0 * 8.325613021850586
Epoch 1260, val loss: 0.49082624912261963
Epoch 1270, training loss: 833.0340576171875 = 0.4267576038837433 + 100.0 * 8.326072692871094
Epoch 1270, val loss: 0.4901775121688843
Epoch 1280, training loss: 832.6975708007812 = 0.4255814850330353 + 100.0 * 8.32271957397461
Epoch 1280, val loss: 0.4897780120372772
Epoch 1290, training loss: 832.6345825195312 = 0.42447125911712646 + 100.0 * 8.322100639343262
Epoch 1290, val loss: 0.48926249146461487
Epoch 1300, training loss: 832.5502319335938 = 0.4234071373939514 + 100.0 * 8.321268081665039
Epoch 1300, val loss: 0.4888710081577301
Epoch 1310, training loss: 832.6810302734375 = 0.422332763671875 + 100.0 * 8.322587013244629
Epoch 1310, val loss: 0.4884156584739685
Epoch 1320, training loss: 832.8922119140625 = 0.4211691617965698 + 100.0 * 8.324710845947266
Epoch 1320, val loss: 0.4878898561000824
Epoch 1330, training loss: 832.4849243164062 = 0.4199717938899994 + 100.0 * 8.320649147033691
Epoch 1330, val loss: 0.48740890622138977
Epoch 1340, training loss: 832.3932495117188 = 0.41883784532546997 + 100.0 * 8.319744110107422
Epoch 1340, val loss: 0.4868859052658081
Epoch 1350, training loss: 832.3668823242188 = 0.41775041818618774 + 100.0 * 8.319491386413574
Epoch 1350, val loss: 0.48646944761276245
Epoch 1360, training loss: 832.5213623046875 = 0.41666704416275024 + 100.0 * 8.321046829223633
Epoch 1360, val loss: 0.4860050082206726
Epoch 1370, training loss: 833.0355834960938 = 0.4155016541481018 + 100.0 * 8.326200485229492
Epoch 1370, val loss: 0.48555606603622437
Epoch 1380, training loss: 832.5973510742188 = 0.41424131393432617 + 100.0 * 8.321830749511719
Epoch 1380, val loss: 0.4849400818347931
Epoch 1390, training loss: 832.2846069335938 = 0.41306766867637634 + 100.0 * 8.31871509552002
Epoch 1390, val loss: 0.48440125584602356
Epoch 1400, training loss: 832.1910400390625 = 0.4119540750980377 + 100.0 * 8.317790985107422
Epoch 1400, val loss: 0.48394984006881714
Epoch 1410, training loss: 832.2223510742188 = 0.41085708141326904 + 100.0 * 8.318115234375
Epoch 1410, val loss: 0.4834776818752289
Epoch 1420, training loss: 832.9928588867188 = 0.40970274806022644 + 100.0 * 8.325831413269043
Epoch 1420, val loss: 0.48293960094451904
Epoch 1430, training loss: 832.2754516601562 = 0.4084559679031372 + 100.0 * 8.318670272827148
Epoch 1430, val loss: 0.48240914940834045
Epoch 1440, training loss: 832.0480346679688 = 0.40727582573890686 + 100.0 * 8.316407203674316
Epoch 1440, val loss: 0.4818899631500244
Epoch 1450, training loss: 832.0126953125 = 0.406133770942688 + 100.0 * 8.316065788269043
Epoch 1450, val loss: 0.48140230774879456
Epoch 1460, training loss: 832.0657958984375 = 0.4050000309944153 + 100.0 * 8.316607475280762
Epoch 1460, val loss: 0.4809713363647461
Epoch 1470, training loss: 833.1614990234375 = 0.40380027890205383 + 100.0 * 8.327576637268066
Epoch 1470, val loss: 0.4804017245769501
Epoch 1480, training loss: 832.0939331054688 = 0.40245091915130615 + 100.0 * 8.316914558410645
Epoch 1480, val loss: 0.47971659898757935
Epoch 1490, training loss: 831.978759765625 = 0.40122368931770325 + 100.0 * 8.315774917602539
Epoch 1490, val loss: 0.47912684082984924
Epoch 1500, training loss: 831.8787841796875 = 0.4000546336174011 + 100.0 * 8.314787864685059
Epoch 1500, val loss: 0.4786193370819092
Epoch 1510, training loss: 831.8236083984375 = 0.39890605211257935 + 100.0 * 8.314247131347656
Epoch 1510, val loss: 0.47816383838653564
Epoch 1520, training loss: 831.84765625 = 0.3977481722831726 + 100.0 * 8.314498901367188
Epoch 1520, val loss: 0.4776817560195923
Epoch 1530, training loss: 832.798583984375 = 0.39652761816978455 + 100.0 * 8.324020385742188
Epoch 1530, val loss: 0.4771289527416229
Epoch 1540, training loss: 831.9122314453125 = 0.3951888382434845 + 100.0 * 8.315170288085938
Epoch 1540, val loss: 0.4765095114707947
Epoch 1550, training loss: 831.816650390625 = 0.39391082525253296 + 100.0 * 8.314227104187012
Epoch 1550, val loss: 0.475746750831604
Epoch 1560, training loss: 831.6824951171875 = 0.39269691705703735 + 100.0 * 8.312897682189941
Epoch 1560, val loss: 0.4752487242221832
Epoch 1570, training loss: 831.6799926757812 = 0.3915082514286041 + 100.0 * 8.312885284423828
Epoch 1570, val loss: 0.4747754633426666
Epoch 1580, training loss: 832.9530639648438 = 0.3902806043624878 + 100.0 * 8.325628280639648
Epoch 1580, val loss: 0.47411397099494934
Epoch 1590, training loss: 832.00390625 = 0.3889209032058716 + 100.0 * 8.316149711608887
Epoch 1590, val loss: 0.4734850227832794
Epoch 1600, training loss: 831.74853515625 = 0.3876180052757263 + 100.0 * 8.31360912322998
Epoch 1600, val loss: 0.47286781668663025
Epoch 1610, training loss: 831.5925903320312 = 0.38638103008270264 + 100.0 * 8.31206226348877
Epoch 1610, val loss: 0.47229239344596863
Epoch 1620, training loss: 831.515625 = 0.3851538598537445 + 100.0 * 8.311305046081543
Epoch 1620, val loss: 0.4717094600200653
Epoch 1630, training loss: 831.5206298828125 = 0.38392651081085205 + 100.0 * 8.31136703491211
Epoch 1630, val loss: 0.4711798131465912
Epoch 1640, training loss: 832.3358764648438 = 0.3826490640640259 + 100.0 * 8.31953239440918
Epoch 1640, val loss: 0.4705699384212494
Epoch 1650, training loss: 831.7039184570312 = 0.3812521994113922 + 100.0 * 8.313226699829102
Epoch 1650, val loss: 0.4698323607444763
Epoch 1660, training loss: 831.5367431640625 = 0.3799167573451996 + 100.0 * 8.311568260192871
Epoch 1660, val loss: 0.4691217839717865
Epoch 1670, training loss: 831.363525390625 = 0.3786378800868988 + 100.0 * 8.30984878540039
Epoch 1670, val loss: 0.4684585630893707
Epoch 1680, training loss: 831.3851318359375 = 0.37738725543022156 + 100.0 * 8.310077667236328
Epoch 1680, val loss: 0.46792083978652954
Epoch 1690, training loss: 831.827392578125 = 0.3761211931705475 + 100.0 * 8.314513206481934
Epoch 1690, val loss: 0.46721187233924866
Epoch 1700, training loss: 831.487060546875 = 0.37476322054862976 + 100.0 * 8.31112289428711
Epoch 1700, val loss: 0.46669095754623413
Epoch 1710, training loss: 831.3199462890625 = 0.3734162747859955 + 100.0 * 8.309465408325195
Epoch 1710, val loss: 0.4660034477710724
Epoch 1720, training loss: 831.2747192382812 = 0.37211042642593384 + 100.0 * 8.309025764465332
Epoch 1720, val loss: 0.4654539227485657
Epoch 1730, training loss: 831.5045166015625 = 0.3708157539367676 + 100.0 * 8.3113374710083
Epoch 1730, val loss: 0.46480730175971985
Epoch 1740, training loss: 831.4205322265625 = 0.3694629669189453 + 100.0 * 8.310510635375977
Epoch 1740, val loss: 0.4641740024089813
Epoch 1750, training loss: 831.3139038085938 = 0.36812546849250793 + 100.0 * 8.309457778930664
Epoch 1750, val loss: 0.463514119386673
Epoch 1760, training loss: 831.1178588867188 = 0.36680683493614197 + 100.0 * 8.307510375976562
Epoch 1760, val loss: 0.4629126787185669
Epoch 1770, training loss: 831.2027587890625 = 0.3655073344707489 + 100.0 * 8.308372497558594
Epoch 1770, val loss: 0.46231505274772644
Epoch 1780, training loss: 832.0346069335938 = 0.3641710579395294 + 100.0 * 8.316703796386719
Epoch 1780, val loss: 0.46163442730903625
Epoch 1790, training loss: 831.4525146484375 = 0.36273619532585144 + 100.0 * 8.310897827148438
Epoch 1790, val loss: 0.4608789086341858
Epoch 1800, training loss: 831.1732177734375 = 0.36134862899780273 + 100.0 * 8.30811882019043
Epoch 1800, val loss: 0.46028944849967957
Epoch 1810, training loss: 831.0679321289062 = 0.3600115478038788 + 100.0 * 8.307079315185547
Epoch 1810, val loss: 0.4596199095249176
Epoch 1820, training loss: 831.3892822265625 = 0.35868290066719055 + 100.0 * 8.31030559539795
Epoch 1820, val loss: 0.45902079343795776
Epoch 1830, training loss: 831.078369140625 = 0.35727253556251526 + 100.0 * 8.307210922241211
Epoch 1830, val loss: 0.4583454728126526
Epoch 1840, training loss: 831.0272216796875 = 0.35588183999061584 + 100.0 * 8.306713104248047
Epoch 1840, val loss: 0.4577891528606415
Epoch 1850, training loss: 830.9584350585938 = 0.3545232117176056 + 100.0 * 8.306038856506348
Epoch 1850, val loss: 0.45711493492126465
Epoch 1860, training loss: 831.1633911132812 = 0.3531914949417114 + 100.0 * 8.308101654052734
Epoch 1860, val loss: 0.45661017298698425
Epoch 1870, training loss: 831.0065307617188 = 0.3517857491970062 + 100.0 * 8.306547164916992
Epoch 1870, val loss: 0.4558566212654114
Epoch 1880, training loss: 830.9212646484375 = 0.3503800928592682 + 100.0 * 8.305708885192871
Epoch 1880, val loss: 0.4552001655101776
Epoch 1890, training loss: 830.9270629882812 = 0.34900352358818054 + 100.0 * 8.305780410766602
Epoch 1890, val loss: 0.4545404016971588
Epoch 1900, training loss: 831.1615600585938 = 0.34764018654823303 + 100.0 * 8.308138847351074
Epoch 1900, val loss: 0.453982949256897
Epoch 1910, training loss: 831.0513305664062 = 0.34622278809547424 + 100.0 * 8.307050704956055
Epoch 1910, val loss: 0.45329323410987854
Epoch 1920, training loss: 830.8057250976562 = 0.3448146879673004 + 100.0 * 8.304609298706055
Epoch 1920, val loss: 0.4527149796485901
Epoch 1930, training loss: 830.81494140625 = 0.34344789385795593 + 100.0 * 8.304715156555176
Epoch 1930, val loss: 0.4521440267562866
Epoch 1940, training loss: 830.9378662109375 = 0.3420872688293457 + 100.0 * 8.305957794189453
Epoch 1940, val loss: 0.4515688419342041
Epoch 1950, training loss: 830.890625 = 0.34067580103874207 + 100.0 * 8.305499076843262
Epoch 1950, val loss: 0.45092758536338806
Epoch 1960, training loss: 831.138427734375 = 0.33924993872642517 + 100.0 * 8.307991981506348
Epoch 1960, val loss: 0.4503036141395569
Epoch 1970, training loss: 830.6851806640625 = 0.33781689405441284 + 100.0 * 8.303473472595215
Epoch 1970, val loss: 0.44975927472114563
Epoch 1980, training loss: 830.7083740234375 = 0.33644577860832214 + 100.0 * 8.303719520568848
Epoch 1980, val loss: 0.449266642332077
Epoch 1990, training loss: 830.68408203125 = 0.3350861370563507 + 100.0 * 8.303489685058594
Epoch 1990, val loss: 0.4487258493900299
Epoch 2000, training loss: 830.80078125 = 0.33371853828430176 + 100.0 * 8.304670333862305
Epoch 2000, val loss: 0.44820699095726013
Epoch 2010, training loss: 830.8592529296875 = 0.3323192596435547 + 100.0 * 8.305269241333008
Epoch 2010, val loss: 0.4476589858531952
Epoch 2020, training loss: 830.6159057617188 = 0.3309096097946167 + 100.0 * 8.302849769592285
Epoch 2020, val loss: 0.4471617043018341
Epoch 2030, training loss: 830.8469848632812 = 0.3295195698738098 + 100.0 * 8.305174827575684
Epoch 2030, val loss: 0.44665586948394775
Epoch 2040, training loss: 830.6842651367188 = 0.3281093239784241 + 100.0 * 8.303561210632324
Epoch 2040, val loss: 0.4460192620754242
Epoch 2050, training loss: 830.6800537109375 = 0.32671576738357544 + 100.0 * 8.303533554077148
Epoch 2050, val loss: 0.44558361172676086
Epoch 2060, training loss: 830.8760986328125 = 0.325329065322876 + 100.0 * 8.30550765991211
Epoch 2060, val loss: 0.4452402889728546
Epoch 2070, training loss: 830.7190551757812 = 0.32387933135032654 + 100.0 * 8.30395221710205
Epoch 2070, val loss: 0.44462597370147705
Epoch 2080, training loss: 830.5511474609375 = 0.32244813442230225 + 100.0 * 8.302287101745605
Epoch 2080, val loss: 0.4442121684551239
Epoch 2090, training loss: 830.4334106445312 = 0.32106348872184753 + 100.0 * 8.30112361907959
Epoch 2090, val loss: 0.44372719526290894
Epoch 2100, training loss: 830.395751953125 = 0.3197050094604492 + 100.0 * 8.300760269165039
Epoch 2100, val loss: 0.44331029057502747
Epoch 2110, training loss: 830.829833984375 = 0.3183448910713196 + 100.0 * 8.30511474609375
Epoch 2110, val loss: 0.44287076592445374
Epoch 2120, training loss: 830.395751953125 = 0.3168874979019165 + 100.0 * 8.300788879394531
Epoch 2120, val loss: 0.4423995614051819
Epoch 2130, training loss: 830.370849609375 = 0.3154555857181549 + 100.0 * 8.300554275512695
Epoch 2130, val loss: 0.4420309066772461
Epoch 2140, training loss: 830.3333740234375 = 0.3140644431114197 + 100.0 * 8.300192832946777
Epoch 2140, val loss: 0.44156795740127563
Epoch 2150, training loss: 830.316162109375 = 0.31269675493240356 + 100.0 * 8.300034523010254
Epoch 2150, val loss: 0.4412342309951782
Epoch 2160, training loss: 830.6736450195312 = 0.3113267123699188 + 100.0 * 8.30362319946289
Epoch 2160, val loss: 0.44080406427383423
Epoch 2170, training loss: 830.3309936523438 = 0.3098944127559662 + 100.0 * 8.300210952758789
Epoch 2170, val loss: 0.4404827654361725
Epoch 2180, training loss: 830.4003295898438 = 0.3084721565246582 + 100.0 * 8.300918579101562
Epoch 2180, val loss: 0.4401193857192993
Epoch 2190, training loss: 830.458740234375 = 0.30707746744155884 + 100.0 * 8.30151653289795
Epoch 2190, val loss: 0.4398166835308075
Epoch 2200, training loss: 830.369384765625 = 0.30567026138305664 + 100.0 * 8.300637245178223
Epoch 2200, val loss: 0.4393448233604431
Epoch 2210, training loss: 830.3193359375 = 0.30427229404449463 + 100.0 * 8.300150871276855
Epoch 2210, val loss: 0.43899938464164734
Epoch 2220, training loss: 830.2921142578125 = 0.30288004875183105 + 100.0 * 8.29989242553711
Epoch 2220, val loss: 0.43874800205230713
Epoch 2230, training loss: 830.7010498046875 = 0.30149126052856445 + 100.0 * 8.303995132446289
Epoch 2230, val loss: 0.4385169744491577
Epoch 2240, training loss: 830.3566284179688 = 0.3000403344631195 + 100.0 * 8.300565719604492
Epoch 2240, val loss: 0.43804341554641724
Epoch 2250, training loss: 830.1082763671875 = 0.2986101508140564 + 100.0 * 8.298096656799316
Epoch 2250, val loss: 0.43784672021865845
Epoch 2260, training loss: 830.0949096679688 = 0.2972252070903778 + 100.0 * 8.29797649383545
Epoch 2260, val loss: 0.4375789165496826
Epoch 2270, training loss: 830.1022338867188 = 0.2958536446094513 + 100.0 * 8.298064231872559
Epoch 2270, val loss: 0.43729478120803833
Epoch 2280, training loss: 830.6019897460938 = 0.29448553919792175 + 100.0 * 8.303074836730957
Epoch 2280, val loss: 0.4370236396789551
Epoch 2290, training loss: 830.2657470703125 = 0.2930273413658142 + 100.0 * 8.299727439880371
Epoch 2290, val loss: 0.4366591274738312
Epoch 2300, training loss: 830.2203369140625 = 0.29158955812454224 + 100.0 * 8.299287796020508
Epoch 2300, val loss: 0.43645256757736206
Epoch 2310, training loss: 830.095947265625 = 0.2901674807071686 + 100.0 * 8.298057556152344
Epoch 2310, val loss: 0.43615105748176575
Epoch 2320, training loss: 830.123779296875 = 0.28877606987953186 + 100.0 * 8.29835033416748
Epoch 2320, val loss: 0.4360264539718628
Epoch 2330, training loss: 830.27783203125 = 0.2873747944831848 + 100.0 * 8.299904823303223
Epoch 2330, val loss: 0.43567875027656555
Epoch 2340, training loss: 830.37744140625 = 0.2859501242637634 + 100.0 * 8.300914764404297
Epoch 2340, val loss: 0.43531668186187744
Epoch 2350, training loss: 830.0408325195312 = 0.2845146656036377 + 100.0 * 8.297563552856445
Epoch 2350, val loss: 0.43516308069229126
Epoch 2360, training loss: 829.9242553710938 = 0.28311073780059814 + 100.0 * 8.296411514282227
Epoch 2360, val loss: 0.43500280380249023
Epoch 2370, training loss: 830.0014038085938 = 0.28172701597213745 + 100.0 * 8.297196388244629
Epoch 2370, val loss: 0.4347904324531555
Epoch 2380, training loss: 830.6293334960938 = 0.28033941984176636 + 100.0 * 8.303489685058594
Epoch 2380, val loss: 0.4346553087234497
Epoch 2390, training loss: 830.1100463867188 = 0.27890026569366455 + 100.0 * 8.298311233520508
Epoch 2390, val loss: 0.43433061242103577
Epoch 2400, training loss: 830.4000244140625 = 0.27749720215797424 + 100.0 * 8.301224708557129
Epoch 2400, val loss: 0.43417415022850037
Epoch 2410, training loss: 829.956787109375 = 0.2760674059391022 + 100.0 * 8.296807289123535
Epoch 2410, val loss: 0.4340108036994934
Epoch 2420, training loss: 829.96875 = 0.27468210458755493 + 100.0 * 8.296940803527832
Epoch 2420, val loss: 0.4339359402656555
Epoch 2430, training loss: 829.8134765625 = 0.2733112573623657 + 100.0 * 8.295401573181152
Epoch 2430, val loss: 0.4337826371192932
Epoch 2440, training loss: 829.8077392578125 = 0.2719583213329315 + 100.0 * 8.295357704162598
Epoch 2440, val loss: 0.43365541100502014
Epoch 2450, training loss: 830.0817260742188 = 0.2706097364425659 + 100.0 * 8.298110961914062
Epoch 2450, val loss: 0.43365511298179626
Epoch 2460, training loss: 829.8980102539062 = 0.2692018449306488 + 100.0 * 8.296287536621094
Epoch 2460, val loss: 0.4334767162799835
Epoch 2470, training loss: 829.9539794921875 = 0.26779550313949585 + 100.0 * 8.29686164855957
Epoch 2470, val loss: 0.4333071708679199
Epoch 2480, training loss: 830.0902709960938 = 0.2664084732532501 + 100.0 * 8.298238754272461
Epoch 2480, val loss: 0.4333829879760742
Epoch 2490, training loss: 830.2124633789062 = 0.26501137018203735 + 100.0 * 8.299474716186523
Epoch 2490, val loss: 0.433290034532547
Epoch 2500, training loss: 829.8037719726562 = 0.263601154088974 + 100.0 * 8.295401573181152
Epoch 2500, val loss: 0.4331294894218445
Epoch 2510, training loss: 829.7328491210938 = 0.2622336149215698 + 100.0 * 8.294706344604492
Epoch 2510, val loss: 0.43306392431259155
Epoch 2520, training loss: 829.7035522460938 = 0.26088687777519226 + 100.0 * 8.294426918029785
Epoch 2520, val loss: 0.43310070037841797
Epoch 2530, training loss: 829.771728515625 = 0.2595403790473938 + 100.0 * 8.295122146606445
Epoch 2530, val loss: 0.433089941740036
Epoch 2540, training loss: 830.2047729492188 = 0.25818222761154175 + 100.0 * 8.299466133117676
Epoch 2540, val loss: 0.4331052005290985
Epoch 2550, training loss: 830.1890869140625 = 0.256794273853302 + 100.0 * 8.299323081970215
Epoch 2550, val loss: 0.4330417811870575
Epoch 2560, training loss: 829.781005859375 = 0.2553849220275879 + 100.0 * 8.295256614685059
Epoch 2560, val loss: 0.43303757905960083
Epoch 2570, training loss: 829.71923828125 = 0.25402653217315674 + 100.0 * 8.294651985168457
Epoch 2570, val loss: 0.4330722689628601
Epoch 2580, training loss: 829.7139892578125 = 0.252671480178833 + 100.0 * 8.294612884521484
Epoch 2580, val loss: 0.433111310005188
Epoch 2590, training loss: 829.817626953125 = 0.2513158917427063 + 100.0 * 8.295662879943848
Epoch 2590, val loss: 0.4331079125404358
Epoch 2600, training loss: 829.813720703125 = 0.24994109570980072 + 100.0 * 8.295638084411621
Epoch 2600, val loss: 0.4331055283546448
Epoch 2610, training loss: 829.6785278320312 = 0.24856451153755188 + 100.0 * 8.294300079345703
Epoch 2610, val loss: 0.4331939220428467
Epoch 2620, training loss: 829.662109375 = 0.24719862639904022 + 100.0 * 8.294149398803711
Epoch 2620, val loss: 0.4331986904144287
Epoch 2630, training loss: 829.7348022460938 = 0.24583616852760315 + 100.0 * 8.294889450073242
Epoch 2630, val loss: 0.43323102593421936
Epoch 2640, training loss: 829.5609741210938 = 0.24446991086006165 + 100.0 * 8.29316520690918
Epoch 2640, val loss: 0.4333728849887848
Epoch 2650, training loss: 829.5401611328125 = 0.243122860789299 + 100.0 * 8.292970657348633
Epoch 2650, val loss: 0.43358251452445984
Epoch 2660, training loss: 829.8853759765625 = 0.24178878962993622 + 100.0 * 8.296436309814453
Epoch 2660, val loss: 0.4336758852005005
Epoch 2670, training loss: 830.0415649414062 = 0.24040487408638 + 100.0 * 8.298011779785156
Epoch 2670, val loss: 0.4334109127521515
Epoch 2680, training loss: 829.712158203125 = 0.23899924755096436 + 100.0 * 8.294731140136719
Epoch 2680, val loss: 0.4339486360549927
Epoch 2690, training loss: 829.5676879882812 = 0.23762091994285583 + 100.0 * 8.29330062866211
Epoch 2690, val loss: 0.4339284598827362
Epoch 2700, training loss: 829.4163208007812 = 0.2362845540046692 + 100.0 * 8.291800498962402
Epoch 2700, val loss: 0.43409088253974915
Epoch 2710, training loss: 829.413818359375 = 0.2349608987569809 + 100.0 * 8.291788101196289
Epoch 2710, val loss: 0.4342927932739258
Epoch 2720, training loss: 829.4901733398438 = 0.23364169895648956 + 100.0 * 8.29256534576416
Epoch 2720, val loss: 0.4344725012779236
Epoch 2730, training loss: 830.0029296875 = 0.23230937123298645 + 100.0 * 8.297706604003906
Epoch 2730, val loss: 0.4346989691257477
Epoch 2740, training loss: 829.6981201171875 = 0.23091810941696167 + 100.0 * 8.294672012329102
Epoch 2740, val loss: 0.43473193049430847
Epoch 2750, training loss: 829.5486450195312 = 0.2295617312192917 + 100.0 * 8.293190956115723
Epoch 2750, val loss: 0.43503013253211975
Epoch 2760, training loss: 829.3867797851562 = 0.22821587324142456 + 100.0 * 8.291585922241211
Epoch 2760, val loss: 0.435048907995224
Epoch 2770, training loss: 829.3194580078125 = 0.22689880430698395 + 100.0 * 8.290925025939941
Epoch 2770, val loss: 0.4353986382484436
Epoch 2780, training loss: 829.5205688476562 = 0.22558683156967163 + 100.0 * 8.292949676513672
Epoch 2780, val loss: 0.43567079305648804
Epoch 2790, training loss: 829.8192138671875 = 0.22423847019672394 + 100.0 * 8.295949935913086
Epoch 2790, val loss: 0.4358857572078705
Epoch 2800, training loss: 829.4598388671875 = 0.22286733984947205 + 100.0 * 8.292369842529297
Epoch 2800, val loss: 0.4359932839870453
Epoch 2810, training loss: 829.3387451171875 = 0.2215326428413391 + 100.0 * 8.29117202758789
Epoch 2810, val loss: 0.4363458454608917
Epoch 2820, training loss: 829.2448120117188 = 0.22020688652992249 + 100.0 * 8.29024600982666
Epoch 2820, val loss: 0.436555951833725
Epoch 2830, training loss: 829.4898071289062 = 0.21889746189117432 + 100.0 * 8.292709350585938
Epoch 2830, val loss: 0.43692344427108765
Epoch 2840, training loss: 829.3845825195312 = 0.217535138130188 + 100.0 * 8.291670799255371
Epoch 2840, val loss: 0.4370681643486023
Epoch 2850, training loss: 829.2114868164062 = 0.21616952121257782 + 100.0 * 8.289953231811523
Epoch 2850, val loss: 0.4373410940170288
Epoch 2860, training loss: 829.2003173828125 = 0.2148377001285553 + 100.0 * 8.289855003356934
Epoch 2860, val loss: 0.4375850260257721
Epoch 2870, training loss: 829.2286376953125 = 0.2135271579027176 + 100.0 * 8.290151596069336
Epoch 2870, val loss: 0.437944233417511
Epoch 2880, training loss: 830.0010986328125 = 0.21223357319831848 + 100.0 * 8.29788875579834
Epoch 2880, val loss: 0.43839332461357117
Epoch 2890, training loss: 829.4981079101562 = 0.21087045967578888 + 100.0 * 8.292872428894043
Epoch 2890, val loss: 0.43840348720550537
Epoch 2900, training loss: 829.5298461914062 = 0.20954523980617523 + 100.0 * 8.293203353881836
Epoch 2900, val loss: 0.4388827383518219
Epoch 2910, training loss: 829.348876953125 = 0.20820900797843933 + 100.0 * 8.291406631469727
Epoch 2910, val loss: 0.4392060935497284
Epoch 2920, training loss: 829.1494140625 = 0.20689274370670319 + 100.0 * 8.289424896240234
Epoch 2920, val loss: 0.43977677822113037
Epoch 2930, training loss: 829.20458984375 = 0.20559580624103546 + 100.0 * 8.289990425109863
Epoch 2930, val loss: 0.44014644622802734
Epoch 2940, training loss: 829.3528442382812 = 0.20429950952529907 + 100.0 * 8.291485786437988
Epoch 2940, val loss: 0.44061124324798584
Epoch 2950, training loss: 829.3322143554688 = 0.2029844969511032 + 100.0 * 8.291292190551758
Epoch 2950, val loss: 0.44099658727645874
Epoch 2960, training loss: 829.35302734375 = 0.20168182253837585 + 100.0 * 8.291513442993164
Epoch 2960, val loss: 0.4414321184158325
Epoch 2970, training loss: 829.07666015625 = 0.20036616921424866 + 100.0 * 8.288763046264648
Epoch 2970, val loss: 0.4418606460094452
Epoch 2980, training loss: 829.197265625 = 0.19908547401428223 + 100.0 * 8.289981842041016
Epoch 2980, val loss: 0.4423028528690338
Epoch 2990, training loss: 829.4838256835938 = 0.19780896604061127 + 100.0 * 8.29286003112793
Epoch 2990, val loss: 0.442812979221344
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8224251648909182
0.8678548141708325
=== training gcn model ===
Epoch 0, training loss: 1059.33251953125 = 1.1066510677337646 + 100.0 * 10.582258224487305
Epoch 0, val loss: 1.1075242757797241
Epoch 10, training loss: 1059.2130126953125 = 1.100013256072998 + 100.0 * 10.581130981445312
Epoch 10, val loss: 1.1008174419403076
Epoch 20, training loss: 1058.2469482421875 = 1.0921664237976074 + 100.0 * 10.571547508239746
Epoch 20, val loss: 1.092857003211975
Epoch 30, training loss: 1052.0064697265625 = 1.082432508468628 + 100.0 * 10.509241104125977
Epoch 30, val loss: 1.0829075574874878
Epoch 40, training loss: 1028.2447509765625 = 1.0717109441757202 + 100.0 * 10.27173137664795
Epoch 40, val loss: 1.0722160339355469
Epoch 50, training loss: 974.5250244140625 = 1.0611977577209473 + 100.0 * 9.734638214111328
Epoch 50, val loss: 1.0613371133804321
Epoch 60, training loss: 958.4972534179688 = 1.050776720046997 + 100.0 * 9.574464797973633
Epoch 60, val loss: 1.0507779121398926
Epoch 70, training loss: 951.81787109375 = 1.041067361831665 + 100.0 * 9.507767677307129
Epoch 70, val loss: 1.0410003662109375
Epoch 80, training loss: 937.6593627929688 = 1.0331519842147827 + 100.0 * 9.366262435913086
Epoch 80, val loss: 1.0332083702087402
Epoch 90, training loss: 920.3674926757812 = 1.0264747142791748 + 100.0 * 9.19340991973877
Epoch 90, val loss: 1.0267302989959717
Epoch 100, training loss: 914.1797485351562 = 1.0193265676498413 + 100.0 * 9.131604194641113
Epoch 100, val loss: 1.0196255445480347
Epoch 110, training loss: 908.6923217773438 = 1.0114160776138306 + 100.0 * 9.07680892944336
Epoch 110, val loss: 1.0117260217666626
Epoch 120, training loss: 901.21826171875 = 1.0046690702438354 + 100.0 * 9.00213623046875
Epoch 120, val loss: 1.0050243139266968
Epoch 130, training loss: 893.9833984375 = 1.0000606775283813 + 100.0 * 8.92983341217041
Epoch 130, val loss: 1.0003987550735474
Epoch 140, training loss: 888.5223388671875 = 0.9960930943489075 + 100.0 * 8.875262260437012
Epoch 140, val loss: 0.9961074590682983
Epoch 150, training loss: 883.396240234375 = 0.9900144934654236 + 100.0 * 8.82406234741211
Epoch 150, val loss: 0.9898396730422974
Epoch 160, training loss: 880.7684326171875 = 0.9820852875709534 + 100.0 * 8.797863960266113
Epoch 160, val loss: 0.9817512631416321
Epoch 170, training loss: 877.4896850585938 = 0.9732837677001953 + 100.0 * 8.765164375305176
Epoch 170, val loss: 0.9729743599891663
Epoch 180, training loss: 872.8915405273438 = 0.9660708904266357 + 100.0 * 8.719254493713379
Epoch 180, val loss: 0.9659238457679749
Epoch 190, training loss: 868.3373413085938 = 0.9585347175598145 + 100.0 * 8.673788070678711
Epoch 190, val loss: 0.95815509557724
Epoch 200, training loss: 864.7124633789062 = 0.9486648440361023 + 100.0 * 8.637638092041016
Epoch 200, val loss: 0.9482357501983643
Epoch 210, training loss: 861.819091796875 = 0.9375516176223755 + 100.0 * 8.60881519317627
Epoch 210, val loss: 0.9371335506439209
Epoch 220, training loss: 859.4747924804688 = 0.9258505702018738 + 100.0 * 8.585489273071289
Epoch 220, val loss: 0.9254543781280518
Epoch 230, training loss: 857.4739990234375 = 0.9134175181388855 + 100.0 * 8.565606117248535
Epoch 230, val loss: 0.9130815863609314
Epoch 240, training loss: 855.9566040039062 = 0.8995307683944702 + 100.0 * 8.550570487976074
Epoch 240, val loss: 0.8992086052894592
Epoch 250, training loss: 854.6201782226562 = 0.8847202062606812 + 100.0 * 8.537354469299316
Epoch 250, val loss: 0.8845258951187134
Epoch 260, training loss: 853.6443481445312 = 0.8692602515220642 + 100.0 * 8.527750968933105
Epoch 260, val loss: 0.8692161440849304
Epoch 270, training loss: 852.888916015625 = 0.8528345823287964 + 100.0 * 8.520360946655273
Epoch 270, val loss: 0.8529298901557922
Epoch 280, training loss: 852.26806640625 = 0.8357381820678711 + 100.0 * 8.514323234558105
Epoch 280, val loss: 0.8360801935195923
Epoch 290, training loss: 851.6782836914062 = 0.8183754086494446 + 100.0 * 8.508599281311035
Epoch 290, val loss: 0.8190544843673706
Epoch 300, training loss: 851.0789184570312 = 0.8010623455047607 + 100.0 * 8.502778053283691
Epoch 300, val loss: 0.8021231293678284
Epoch 310, training loss: 850.5559692382812 = 0.7838395833969116 + 100.0 * 8.497721672058105
Epoch 310, val loss: 0.7853343486785889
Epoch 320, training loss: 849.9653930664062 = 0.76682049036026 + 100.0 * 8.491985321044922
Epoch 320, val loss: 0.7687937617301941
Epoch 330, training loss: 849.2249755859375 = 0.7503377795219421 + 100.0 * 8.484745979309082
Epoch 330, val loss: 0.752835750579834
Epoch 340, training loss: 848.9130859375 = 0.7341341972351074 + 100.0 * 8.481789588928223
Epoch 340, val loss: 0.7372215390205383
Epoch 350, training loss: 848.3853759765625 = 0.7180637717247009 + 100.0 * 8.476673126220703
Epoch 350, val loss: 0.7214987277984619
Epoch 360, training loss: 847.74755859375 = 0.7024917602539062 + 100.0 * 8.470450401306152
Epoch 360, val loss: 0.7065882086753845
Epoch 370, training loss: 847.187744140625 = 0.6875563859939575 + 100.0 * 8.465002059936523
Epoch 370, val loss: 0.6922809481620789
Epoch 380, training loss: 846.7998657226562 = 0.6732175946235657 + 100.0 * 8.46126651763916
Epoch 380, val loss: 0.6785702109336853
Epoch 390, training loss: 846.76904296875 = 0.6593886017799377 + 100.0 * 8.46109676361084
Epoch 390, val loss: 0.6655049920082092
Epoch 400, training loss: 845.9392700195312 = 0.6462257504463196 + 100.0 * 8.452930450439453
Epoch 400, val loss: 0.6528566479682922
Epoch 410, training loss: 845.5064697265625 = 0.6338869333267212 + 100.0 * 8.448725700378418
Epoch 410, val loss: 0.6411566734313965
Epoch 420, training loss: 845.5326538085938 = 0.6222257614135742 + 100.0 * 8.449104309082031
Epoch 420, val loss: 0.6300630569458008
Epoch 430, training loss: 844.6129760742188 = 0.6110687851905823 + 100.0 * 8.440018653869629
Epoch 430, val loss: 0.6196334362030029
Epoch 440, training loss: 844.2472534179688 = 0.6008015275001526 + 100.0 * 8.436464309692383
Epoch 440, val loss: 0.610055148601532
Epoch 450, training loss: 844.424560546875 = 0.590985119342804 + 100.0 * 8.438335418701172
Epoch 450, val loss: 0.6007433533668518
Epoch 460, training loss: 843.481201171875 = 0.5816630721092224 + 100.0 * 8.428995132446289
Epoch 460, val loss: 0.5920267105102539
Epoch 470, training loss: 842.9960327148438 = 0.5732084512710571 + 100.0 * 8.42422866821289
Epoch 470, val loss: 0.5841355323791504
Epoch 480, training loss: 842.5869140625 = 0.5652211904525757 + 100.0 * 8.42021656036377
Epoch 480, val loss: 0.5766437649726868
Epoch 490, training loss: 842.179443359375 = 0.5576983094215393 + 100.0 * 8.416217803955078
Epoch 490, val loss: 0.5697110891342163
Epoch 500, training loss: 842.8307495117188 = 0.5505615472793579 + 100.0 * 8.422801971435547
Epoch 500, val loss: 0.563057005405426
Epoch 510, training loss: 841.727783203125 = 0.5435640215873718 + 100.0 * 8.411842346191406
Epoch 510, val loss: 0.5566540360450745
Epoch 520, training loss: 841.2569580078125 = 0.5371549725532532 + 100.0 * 8.407197952270508
Epoch 520, val loss: 0.5509225130081177
Epoch 530, training loss: 840.98388671875 = 0.5311304926872253 + 100.0 * 8.40452766418457
Epoch 530, val loss: 0.5453729629516602
Epoch 540, training loss: 841.295166015625 = 0.5253702998161316 + 100.0 * 8.407697677612305
Epoch 540, val loss: 0.5401439666748047
Epoch 550, training loss: 840.8658447265625 = 0.5197518467903137 + 100.0 * 8.403460502624512
Epoch 550, val loss: 0.5351630449295044
Epoch 560, training loss: 840.33251953125 = 0.5145187377929688 + 100.0 * 8.39818000793457
Epoch 560, val loss: 0.5305526256561279
Epoch 570, training loss: 840.0453491210938 = 0.509643018245697 + 100.0 * 8.395357131958008
Epoch 570, val loss: 0.5261535048484802
Epoch 580, training loss: 840.051025390625 = 0.5049367547035217 + 100.0 * 8.395461082458496
Epoch 580, val loss: 0.5221555829048157
Epoch 590, training loss: 839.607666015625 = 0.5003605484962463 + 100.0 * 8.391073226928711
Epoch 590, val loss: 0.5180482864379883
Epoch 600, training loss: 839.470703125 = 0.49605944752693176 + 100.0 * 8.38974666595459
Epoch 600, val loss: 0.5145314931869507
Epoch 610, training loss: 839.1612548828125 = 0.49207809567451477 + 100.0 * 8.38669204711914
Epoch 610, val loss: 0.5109518766403198
Epoch 620, training loss: 838.8907470703125 = 0.4882587790489197 + 100.0 * 8.384024620056152
Epoch 620, val loss: 0.507810652256012
Epoch 630, training loss: 839.0288696289062 = 0.48461511731147766 + 100.0 * 8.385442733764648
Epoch 630, val loss: 0.5048289895057678
Epoch 640, training loss: 839.0604248046875 = 0.48098352551460266 + 100.0 * 8.385794639587402
Epoch 640, val loss: 0.501542329788208
Epoch 650, training loss: 838.4102172851562 = 0.47751376032829285 + 100.0 * 8.379326820373535
Epoch 650, val loss: 0.4989643394947052
Epoch 660, training loss: 838.1868896484375 = 0.47431397438049316 + 100.0 * 8.37712574005127
Epoch 660, val loss: 0.4963192343711853
Epoch 670, training loss: 838.2820434570312 = 0.47122809290885925 + 100.0 * 8.378108024597168
Epoch 670, val loss: 0.49388837814331055
Epoch 680, training loss: 837.7587280273438 = 0.46825534105300903 + 100.0 * 8.372904777526855
Epoch 680, val loss: 0.4914710521697998
Epoch 690, training loss: 837.6876220703125 = 0.46542978286743164 + 100.0 * 8.372221946716309
Epoch 690, val loss: 0.4891912639141083
Epoch 700, training loss: 837.6821899414062 = 0.4626701772212982 + 100.0 * 8.37219524383545
Epoch 700, val loss: 0.4871039092540741
Epoch 710, training loss: 837.4321899414062 = 0.45997482538223267 + 100.0 * 8.369722366333008
Epoch 710, val loss: 0.48511719703674316
Epoch 720, training loss: 837.5130004882812 = 0.4573722779750824 + 100.0 * 8.370555877685547
Epoch 720, val loss: 0.48303934931755066
Epoch 730, training loss: 837.0245361328125 = 0.45481789112091064 + 100.0 * 8.365696907043457
Epoch 730, val loss: 0.4811122715473175
Epoch 740, training loss: 836.8573608398438 = 0.4524242579936981 + 100.0 * 8.364048957824707
Epoch 740, val loss: 0.47926726937294006
Epoch 750, training loss: 836.8135986328125 = 0.450094610452652 + 100.0 * 8.363635063171387
Epoch 750, val loss: 0.47753188014030457
Epoch 760, training loss: 837.0702514648438 = 0.4478134214878082 + 100.0 * 8.36622428894043
Epoch 760, val loss: 0.4757154583930969
Epoch 770, training loss: 836.6629638671875 = 0.44553253054618835 + 100.0 * 8.362174034118652
Epoch 770, val loss: 0.47417938709259033
Epoch 780, training loss: 836.5509643554688 = 0.4434005916118622 + 100.0 * 8.361075401306152
Epoch 780, val loss: 0.47258174419403076
Epoch 790, training loss: 836.3240356445312 = 0.4412875473499298 + 100.0 * 8.358827590942383
Epoch 790, val loss: 0.4709339439868927
Epoch 800, training loss: 836.2169189453125 = 0.4392511248588562 + 100.0 * 8.357776641845703
Epoch 800, val loss: 0.46954479813575745
Epoch 810, training loss: 836.3214721679688 = 0.4372824728488922 + 100.0 * 8.358841896057129
Epoch 810, val loss: 0.46802183985710144
Epoch 820, training loss: 836.1983642578125 = 0.4353015124797821 + 100.0 * 8.357630729675293
Epoch 820, val loss: 0.46657851338386536
Epoch 830, training loss: 835.9746704101562 = 0.43336185812950134 + 100.0 * 8.355413436889648
Epoch 830, val loss: 0.465361624956131
Epoch 840, training loss: 835.7841796875 = 0.4315541386604309 + 100.0 * 8.35352611541748
Epoch 840, val loss: 0.4639371931552887
Epoch 850, training loss: 835.689208984375 = 0.42976656556129456 + 100.0 * 8.352594375610352
Epoch 850, val loss: 0.462739497423172
Epoch 860, training loss: 836.0918579101562 = 0.4279782474040985 + 100.0 * 8.35663890838623
Epoch 860, val loss: 0.4614349603652954
Epoch 870, training loss: 835.6177368164062 = 0.4262028634548187 + 100.0 * 8.35191535949707
Epoch 870, val loss: 0.4602430760860443
Epoch 880, training loss: 835.6187744140625 = 0.4245063066482544 + 100.0 * 8.351943016052246
Epoch 880, val loss: 0.45896559953689575
Epoch 890, training loss: 835.3873901367188 = 0.4228261411190033 + 100.0 * 8.349645614624023
Epoch 890, val loss: 0.4578697085380554
Epoch 900, training loss: 835.5355834960938 = 0.42120298743247986 + 100.0 * 8.351143836975098
Epoch 900, val loss: 0.4567607045173645
Epoch 910, training loss: 835.1309814453125 = 0.4195970296859741 + 100.0 * 8.347113609313965
Epoch 910, val loss: 0.4555392265319824
Epoch 920, training loss: 835.1005249023438 = 0.4180442988872528 + 100.0 * 8.346824645996094
Epoch 920, val loss: 0.45446163415908813
Epoch 930, training loss: 835.5449829101562 = 0.4165174961090088 + 100.0 * 8.351284980773926
Epoch 930, val loss: 0.4534013271331787
Epoch 940, training loss: 835.1608276367188 = 0.4149136245250702 + 100.0 * 8.347458839416504
Epoch 940, val loss: 0.4522241950035095
Epoch 950, training loss: 834.8914794921875 = 0.4134049415588379 + 100.0 * 8.344780921936035
Epoch 950, val loss: 0.45132094621658325
Epoch 960, training loss: 834.7567138671875 = 0.41194888949394226 + 100.0 * 8.3434476852417
Epoch 960, val loss: 0.45022064447402954
Epoch 970, training loss: 834.702392578125 = 0.41050994396209717 + 100.0 * 8.342918395996094
Epoch 970, val loss: 0.4492608606815338
Epoch 980, training loss: 834.6390380859375 = 0.409090518951416 + 100.0 * 8.342299461364746
Epoch 980, val loss: 0.4483228027820587
Epoch 990, training loss: 835.1219482421875 = 0.4076789915561676 + 100.0 * 8.347143173217773
Epoch 990, val loss: 0.44735386967658997
Epoch 1000, training loss: 835.8915405273438 = 0.4061663746833801 + 100.0 * 8.354853630065918
Epoch 1000, val loss: 0.44650471210479736
Epoch 1010, training loss: 834.80712890625 = 0.4046735465526581 + 100.0 * 8.344024658203125
Epoch 1010, val loss: 0.4454299509525299
Epoch 1020, training loss: 834.5144653320312 = 0.40330731868743896 + 100.0 * 8.341111183166504
Epoch 1020, val loss: 0.44432708621025085
Epoch 1030, training loss: 834.3570556640625 = 0.4020113945007324 + 100.0 * 8.339550018310547
Epoch 1030, val loss: 0.4434733986854553
Epoch 1040, training loss: 834.2694702148438 = 0.4007415771484375 + 100.0 * 8.3386869430542
Epoch 1040, val loss: 0.44264110922813416
Epoch 1050, training loss: 834.2020874023438 = 0.39948171377182007 + 100.0 * 8.33802604675293
Epoch 1050, val loss: 0.44182080030441284
Epoch 1060, training loss: 834.3914184570312 = 0.3982159495353699 + 100.0 * 8.339932441711426
Epoch 1060, val loss: 0.4409857988357544
Epoch 1070, training loss: 834.1398315429688 = 0.3968628942966461 + 100.0 * 8.337430000305176
Epoch 1070, val loss: 0.44013792276382446
Epoch 1080, training loss: 834.1841430664062 = 0.39556071162223816 + 100.0 * 8.337885856628418
Epoch 1080, val loss: 0.4392448663711548
Epoch 1090, training loss: 834.0177612304688 = 0.39432355761528015 + 100.0 * 8.336234092712402
Epoch 1090, val loss: 0.4384908676147461
Epoch 1100, training loss: 833.9161376953125 = 0.3931214511394501 + 100.0 * 8.335229873657227
Epoch 1100, val loss: 0.43773120641708374
Epoch 1110, training loss: 833.8868408203125 = 0.39193469285964966 + 100.0 * 8.334949493408203
Epoch 1110, val loss: 0.4370562434196472
Epoch 1120, training loss: 834.6953735351562 = 0.3907228708267212 + 100.0 * 8.343046188354492
Epoch 1120, val loss: 0.4363929331302643
Epoch 1130, training loss: 833.8704223632812 = 0.38947993516921997 + 100.0 * 8.334809303283691
Epoch 1130, val loss: 0.4355901777744293
Epoch 1140, training loss: 833.8670654296875 = 0.3882960379123688 + 100.0 * 8.334787368774414
Epoch 1140, val loss: 0.434729665517807
Epoch 1150, training loss: 833.9156494140625 = 0.3871215581893921 + 100.0 * 8.335285186767578
Epoch 1150, val loss: 0.4341006875038147
Epoch 1160, training loss: 833.6681518554688 = 0.38597166538238525 + 100.0 * 8.3328218460083
Epoch 1160, val loss: 0.43348464369773865
Epoch 1170, training loss: 833.8829345703125 = 0.38484427332878113 + 100.0 * 8.334980964660645
Epoch 1170, val loss: 0.43277525901794434
Epoch 1180, training loss: 833.6914672851562 = 0.38371747732162476 + 100.0 * 8.333077430725098
Epoch 1180, val loss: 0.43203359842300415
Epoch 1190, training loss: 833.7207641601562 = 0.3826076090335846 + 100.0 * 8.333381652832031
Epoch 1190, val loss: 0.431491881608963
Epoch 1200, training loss: 833.4742431640625 = 0.38150227069854736 + 100.0 * 8.330926895141602
Epoch 1200, val loss: 0.43074673414230347
Epoch 1210, training loss: 833.3478393554688 = 0.38043296337127686 + 100.0 * 8.329673767089844
Epoch 1210, val loss: 0.43013089895248413
Epoch 1220, training loss: 833.9395751953125 = 0.3793856203556061 + 100.0 * 8.335601806640625
Epoch 1220, val loss: 0.42938360571861267
Epoch 1230, training loss: 833.529052734375 = 0.37827035784721375 + 100.0 * 8.331507682800293
Epoch 1230, val loss: 0.42913109064102173
Epoch 1240, training loss: 833.2543334960938 = 0.377218097448349 + 100.0 * 8.328771591186523
Epoch 1240, val loss: 0.42842498421669006
Epoch 1250, training loss: 833.166015625 = 0.3762015998363495 + 100.0 * 8.327898025512695
Epoch 1250, val loss: 0.4278455972671509
Epoch 1260, training loss: 833.535888671875 = 0.37519869208335876 + 100.0 * 8.3316068649292
Epoch 1260, val loss: 0.4273797273635864
Epoch 1270, training loss: 833.9745483398438 = 0.3741167485713959 + 100.0 * 8.336004257202148
Epoch 1270, val loss: 0.42668643593788147
Epoch 1280, training loss: 833.020263671875 = 0.37302786111831665 + 100.0 * 8.326472282409668
Epoch 1280, val loss: 0.42626866698265076
Epoch 1290, training loss: 832.9871826171875 = 0.37203097343444824 + 100.0 * 8.326151847839355
Epoch 1290, val loss: 0.4256691038608551
Epoch 1300, training loss: 832.913330078125 = 0.3710731267929077 + 100.0 * 8.325422286987305
Epoch 1300, val loss: 0.42522546648979187
Epoch 1310, training loss: 832.828125 = 0.3701305389404297 + 100.0 * 8.324580192565918
Epoch 1310, val loss: 0.42482325434684753
Epoch 1320, training loss: 832.7849731445312 = 0.36918625235557556 + 100.0 * 8.32415771484375
Epoch 1320, val loss: 0.42432713508605957
Epoch 1330, training loss: 832.9050903320312 = 0.3682401776313782 + 100.0 * 8.325368881225586
Epoch 1330, val loss: 0.4238503873348236
Epoch 1340, training loss: 833.0073852539062 = 0.3672163188457489 + 100.0 * 8.326401710510254
Epoch 1340, val loss: 0.4233337640762329
Epoch 1350, training loss: 833.1068115234375 = 0.3661879897117615 + 100.0 * 8.32740592956543
Epoch 1350, val loss: 0.4228956401348114
Epoch 1360, training loss: 832.6513061523438 = 0.36521852016448975 + 100.0 * 8.322860717773438
Epoch 1360, val loss: 0.4224845767021179
Epoch 1370, training loss: 832.5678100585938 = 0.36429738998413086 + 100.0 * 8.32203483581543
Epoch 1370, val loss: 0.42211154103279114
Epoch 1380, training loss: 832.86376953125 = 0.36340129375457764 + 100.0 * 8.325003623962402
Epoch 1380, val loss: 0.42179548740386963
Epoch 1390, training loss: 832.5828857421875 = 0.36244145035743713 + 100.0 * 8.32220458984375
Epoch 1390, val loss: 0.4213447570800781
Epoch 1400, training loss: 832.4673461914062 = 0.36151519417762756 + 100.0 * 8.32105827331543
Epoch 1400, val loss: 0.420926034450531
Epoch 1410, training loss: 832.3899536132812 = 0.36062589287757874 + 100.0 * 8.320293426513672
Epoch 1410, val loss: 0.42060375213623047
Epoch 1420, training loss: 832.3875732421875 = 0.3597472906112671 + 100.0 * 8.32027816772461
Epoch 1420, val loss: 0.42025643587112427
Epoch 1430, training loss: 833.0089721679688 = 0.3588750958442688 + 100.0 * 8.32650089263916
Epoch 1430, val loss: 0.41985487937927246
Epoch 1440, training loss: 832.7791137695312 = 0.35791516304016113 + 100.0 * 8.324212074279785
Epoch 1440, val loss: 0.41969236731529236
Epoch 1450, training loss: 832.2388305664062 = 0.3570031225681305 + 100.0 * 8.318818092346191
Epoch 1450, val loss: 0.4192390739917755
Epoch 1460, training loss: 832.231201171875 = 0.3561309278011322 + 100.0 * 8.318750381469727
Epoch 1460, val loss: 0.4190012216567993
Epoch 1470, training loss: 832.3817749023438 = 0.35527053475379944 + 100.0 * 8.32026481628418
Epoch 1470, val loss: 0.41875332593917847
Epoch 1480, training loss: 832.2745361328125 = 0.35438522696495056 + 100.0 * 8.319201469421387
Epoch 1480, val loss: 0.41839808225631714
Epoch 1490, training loss: 832.14111328125 = 0.3535149097442627 + 100.0 * 8.317875862121582
Epoch 1490, val loss: 0.4180464744567871
Epoch 1500, training loss: 832.17822265625 = 0.3526557385921478 + 100.0 * 8.318255424499512
Epoch 1500, val loss: 0.41775426268577576
Epoch 1510, training loss: 832.24365234375 = 0.35176798701286316 + 100.0 * 8.31891918182373
Epoch 1510, val loss: 0.4174491763114929
Epoch 1520, training loss: 832.0990600585938 = 0.3508778214454651 + 100.0 * 8.317481994628906
Epoch 1520, val loss: 0.41718411445617676
Epoch 1530, training loss: 831.9276123046875 = 0.35000964999198914 + 100.0 * 8.315775871276855
Epoch 1530, val loss: 0.4169422686100006
Epoch 1540, training loss: 831.8709716796875 = 0.34917017817497253 + 100.0 * 8.315217971801758
Epoch 1540, val loss: 0.4166642427444458
Epoch 1550, training loss: 831.8433227539062 = 0.3483462929725647 + 100.0 * 8.314949989318848
Epoch 1550, val loss: 0.4164120554924011
Epoch 1560, training loss: 832.2470703125 = 0.3475114107131958 + 100.0 * 8.318995475769043
Epoch 1560, val loss: 0.4161560833454132
Epoch 1570, training loss: 832.5963745117188 = 0.34660351276397705 + 100.0 * 8.322497367858887
Epoch 1570, val loss: 0.4159485101699829
Epoch 1580, training loss: 831.8751831054688 = 0.34565311670303345 + 100.0 * 8.315295219421387
Epoch 1580, val loss: 0.41552823781967163
Epoch 1590, training loss: 831.7664184570312 = 0.3447967767715454 + 100.0 * 8.314216613769531
Epoch 1590, val loss: 0.415295273065567
Epoch 1600, training loss: 831.6616821289062 = 0.34397897124290466 + 100.0 * 8.313177108764648
Epoch 1600, val loss: 0.41516193747520447
Epoch 1610, training loss: 831.6343994140625 = 0.34317031502723694 + 100.0 * 8.312911987304688
Epoch 1610, val loss: 0.4148803949356079
Epoch 1620, training loss: 831.942138671875 = 0.3423558473587036 + 100.0 * 8.315998077392578
Epoch 1620, val loss: 0.41454994678497314
Epoch 1630, training loss: 831.6661376953125 = 0.34147247672080994 + 100.0 * 8.313246726989746
Epoch 1630, val loss: 0.41453248262405396
Epoch 1640, training loss: 831.6021118164062 = 0.34060385823249817 + 100.0 * 8.312615394592285
Epoch 1640, val loss: 0.41417396068573
Epoch 1650, training loss: 831.5258178710938 = 0.3397652208805084 + 100.0 * 8.311860084533691
Epoch 1650, val loss: 0.4139891266822815
Epoch 1660, training loss: 831.63671875 = 0.3389410078525543 + 100.0 * 8.31297779083252
Epoch 1660, val loss: 0.41379424929618835
Epoch 1670, training loss: 831.7186279296875 = 0.33808207511901855 + 100.0 * 8.31380558013916
Epoch 1670, val loss: 0.4135621190071106
Epoch 1680, training loss: 831.5029296875 = 0.3372207283973694 + 100.0 * 8.311656951904297
Epoch 1680, val loss: 0.41318458318710327
Epoch 1690, training loss: 831.5885620117188 = 0.3363814651966095 + 100.0 * 8.312521934509277
Epoch 1690, val loss: 0.41304031014442444
Epoch 1700, training loss: 831.5787353515625 = 0.335528701543808 + 100.0 * 8.312432289123535
Epoch 1700, val loss: 0.4127825200557709
Epoch 1710, training loss: 831.42919921875 = 0.33466705679893494 + 100.0 * 8.310945510864258
Epoch 1710, val loss: 0.41250571608543396
Epoch 1720, training loss: 831.337158203125 = 0.3338232934474945 + 100.0 * 8.310033798217773
Epoch 1720, val loss: 0.4124031960964203
Epoch 1730, training loss: 831.4409790039062 = 0.33299410343170166 + 100.0 * 8.311079978942871
Epoch 1730, val loss: 0.41220372915267944
Epoch 1740, training loss: 831.3549194335938 = 0.33214864134788513 + 100.0 * 8.310227394104004
Epoch 1740, val loss: 0.4119234085083008
Epoch 1750, training loss: 831.8622436523438 = 0.331292062997818 + 100.0 * 8.315309524536133
Epoch 1750, val loss: 0.4115937054157257
Epoch 1760, training loss: 831.33203125 = 0.33040300011634827 + 100.0 * 8.310016632080078
Epoch 1760, val loss: 0.41147783398628235
Epoch 1770, training loss: 831.1798706054688 = 0.32955238223075867 + 100.0 * 8.308503150939941
Epoch 1770, val loss: 0.4111468493938446
Epoch 1780, training loss: 831.1251831054688 = 0.3287122845649719 + 100.0 * 8.307964324951172
Epoch 1780, val loss: 0.41103595495224
Epoch 1790, training loss: 831.1591186523438 = 0.3278740346431732 + 100.0 * 8.30831241607666
Epoch 1790, val loss: 0.410788357257843
Epoch 1800, training loss: 831.7026977539062 = 0.3270036578178406 + 100.0 * 8.313756942749023
Epoch 1800, val loss: 0.4105090796947479
Epoch 1810, training loss: 831.3189086914062 = 0.3260840177536011 + 100.0 * 8.309927940368652
Epoch 1810, val loss: 0.4100896120071411
Epoch 1820, training loss: 831.075927734375 = 0.3251812756061554 + 100.0 * 8.307507514953613
Epoch 1820, val loss: 0.41000622510910034
Epoch 1830, training loss: 831.0463256835938 = 0.3243090510368347 + 100.0 * 8.307220458984375
Epoch 1830, val loss: 0.4097730815410614
Epoch 1840, training loss: 831.0196533203125 = 0.32344695925712585 + 100.0 * 8.306962013244629
Epoch 1840, val loss: 0.40959155559539795
Epoch 1850, training loss: 831.2418212890625 = 0.3225706219673157 + 100.0 * 8.309192657470703
Epoch 1850, val loss: 0.4094243347644806
Epoch 1860, training loss: 831.0906372070312 = 0.3216601312160492 + 100.0 * 8.307689666748047
Epoch 1860, val loss: 0.40906792879104614
Epoch 1870, training loss: 831.01025390625 = 0.32075244188308716 + 100.0 * 8.30689525604248
Epoch 1870, val loss: 0.4088611602783203
Epoch 1880, training loss: 831.2804565429688 = 0.3198636770248413 + 100.0 * 8.309605598449707
Epoch 1880, val loss: 0.4088045656681061
Epoch 1890, training loss: 830.9899291992188 = 0.31891414523124695 + 100.0 * 8.306710243225098
Epoch 1890, val loss: 0.4083423316478729
Epoch 1900, training loss: 830.9512329101562 = 0.3179906904697418 + 100.0 * 8.3063325881958
Epoch 1900, val loss: 0.40827125310897827
Epoch 1910, training loss: 830.806884765625 = 0.31708380579948425 + 100.0 * 8.304898262023926
Epoch 1910, val loss: 0.4079940915107727
Epoch 1920, training loss: 830.7339477539062 = 0.3161970376968384 + 100.0 * 8.304177284240723
Epoch 1920, val loss: 0.4078613221645355
Epoch 1930, training loss: 830.7076416015625 = 0.31530267000198364 + 100.0 * 8.303923606872559
Epoch 1930, val loss: 0.4076859951019287
Epoch 1940, training loss: 831.4662475585938 = 0.3144006133079529 + 100.0 * 8.311518669128418
Epoch 1940, val loss: 0.40750789642333984
Epoch 1950, training loss: 831.3026123046875 = 0.3134080171585083 + 100.0 * 8.309891700744629
Epoch 1950, val loss: 0.4072398245334625
Epoch 1960, training loss: 830.7130737304688 = 0.31241777539253235 + 100.0 * 8.304006576538086
Epoch 1960, val loss: 0.40693801641464233
Epoch 1970, training loss: 830.679443359375 = 0.31149423122406006 + 100.0 * 8.303679466247559
Epoch 1970, val loss: 0.4067784547805786
Epoch 1980, training loss: 830.583740234375 = 0.3105916380882263 + 100.0 * 8.30273151397705
Epoch 1980, val loss: 0.4065607488155365
Epoch 1990, training loss: 830.5685424804688 = 0.3096846044063568 + 100.0 * 8.30258846282959
Epoch 1990, val loss: 0.4063965976238251
Epoch 2000, training loss: 830.987060546875 = 0.3087639808654785 + 100.0 * 8.306782722473145
Epoch 2000, val loss: 0.40630489587783813
Epoch 2010, training loss: 831.0752563476562 = 0.3077895939350128 + 100.0 * 8.307674407958984
Epoch 2010, val loss: 0.4059872627258301
Epoch 2020, training loss: 830.6239624023438 = 0.3067968785762787 + 100.0 * 8.303171157836914
Epoch 2020, val loss: 0.4058179557323456
Epoch 2030, training loss: 830.4994506835938 = 0.305835485458374 + 100.0 * 8.301936149597168
Epoch 2030, val loss: 0.40561267733573914
Epoch 2040, training loss: 830.481201171875 = 0.30488574504852295 + 100.0 * 8.301763534545898
Epoch 2040, val loss: 0.40538209676742554
Epoch 2050, training loss: 830.9505615234375 = 0.3039432764053345 + 100.0 * 8.306466102600098
Epoch 2050, val loss: 0.40515875816345215
Epoch 2060, training loss: 830.5091552734375 = 0.30295050144195557 + 100.0 * 8.302062034606934
Epoch 2060, val loss: 0.4052155911922455
Epoch 2070, training loss: 830.4406127929688 = 0.30198344588279724 + 100.0 * 8.301385879516602
Epoch 2070, val loss: 0.40484336018562317
Epoch 2080, training loss: 830.7744750976562 = 0.30102303624153137 + 100.0 * 8.304734230041504
Epoch 2080, val loss: 0.4048033356666565
Epoch 2090, training loss: 830.3612060546875 = 0.3000122606754303 + 100.0 * 8.300612449645996
Epoch 2090, val loss: 0.40450581908226013
Epoch 2100, training loss: 830.3612060546875 = 0.2990171015262604 + 100.0 * 8.30062198638916
Epoch 2100, val loss: 0.4044969081878662
Epoch 2110, training loss: 830.39501953125 = 0.2980409860610962 + 100.0 * 8.300970077514648
Epoch 2110, val loss: 0.4041840434074402
Epoch 2120, training loss: 830.8899536132812 = 0.29706108570098877 + 100.0 * 8.305929183959961
Epoch 2120, val loss: 0.4040452241897583
Epoch 2130, training loss: 830.377197265625 = 0.2960303723812103 + 100.0 * 8.300811767578125
Epoch 2130, val loss: 0.40410733222961426
Epoch 2140, training loss: 830.2766723632812 = 0.29503098130226135 + 100.0 * 8.299816131591797
Epoch 2140, val loss: 0.4038960337638855
Epoch 2150, training loss: 830.356201171875 = 0.29404133558273315 + 100.0 * 8.300621032714844
Epoch 2150, val loss: 0.40381813049316406
Epoch 2160, training loss: 830.4933471679688 = 0.29302552342414856 + 100.0 * 8.302002906799316
Epoch 2160, val loss: 0.4036596119403839
Epoch 2170, training loss: 830.30126953125 = 0.2919902205467224 + 100.0 * 8.300092697143555
Epoch 2170, val loss: 0.40346047282218933
Epoch 2180, training loss: 830.190185546875 = 0.29097244143486023 + 100.0 * 8.298992156982422
Epoch 2180, val loss: 0.40337687730789185
Epoch 2190, training loss: 830.3490600585938 = 0.28995004296302795 + 100.0 * 8.300590515136719
Epoch 2190, val loss: 0.40328770875930786
Epoch 2200, training loss: 830.4938354492188 = 0.2889106869697571 + 100.0 * 8.302048683166504
Epoch 2200, val loss: 0.4030228555202484
Epoch 2210, training loss: 830.0855712890625 = 0.2878480851650238 + 100.0 * 8.297977447509766
Epoch 2210, val loss: 0.4031301736831665
Epoch 2220, training loss: 830.08056640625 = 0.2868109941482544 + 100.0 * 8.297937393188477
Epoch 2220, val loss: 0.4029889404773712
Epoch 2230, training loss: 830.0436401367188 = 0.2857889235019684 + 100.0 * 8.297578811645508
Epoch 2230, val loss: 0.4028555154800415
Epoch 2240, training loss: 830.0999755859375 = 0.2847651541233063 + 100.0 * 8.298151969909668
Epoch 2240, val loss: 0.4028228223323822
Epoch 2250, training loss: 830.5936889648438 = 0.283723920583725 + 100.0 * 8.303099632263184
Epoch 2250, val loss: 0.4027835726737976
Epoch 2260, training loss: 830.1312255859375 = 0.2826462388038635 + 100.0 * 8.29848575592041
Epoch 2260, val loss: 0.40273842215538025
Epoch 2270, training loss: 829.9524536132812 = 0.28157782554626465 + 100.0 * 8.296709060668945
Epoch 2270, val loss: 0.40249282121658325
Epoch 2280, training loss: 830.1129760742188 = 0.2805384397506714 + 100.0 * 8.298324584960938
Epoch 2280, val loss: 0.40244752168655396
Epoch 2290, training loss: 830.132080078125 = 0.2794688642024994 + 100.0 * 8.2985258102417
Epoch 2290, val loss: 0.4025751054286957
Epoch 2300, training loss: 829.9085693359375 = 0.2783864140510559 + 100.0 * 8.29630184173584
Epoch 2300, val loss: 0.40238937735557556
Epoch 2310, training loss: 829.8424072265625 = 0.2773337960243225 + 100.0 * 8.295650482177734
Epoch 2310, val loss: 0.40235987305641174
Epoch 2320, training loss: 829.8809204101562 = 0.2762899100780487 + 100.0 * 8.296046257019043
Epoch 2320, val loss: 0.40232834219932556
Epoch 2330, training loss: 830.3984375 = 0.27524691820144653 + 100.0 * 8.301231384277344
Epoch 2330, val loss: 0.402211993932724
Epoch 2340, training loss: 830.0230712890625 = 0.2741433382034302 + 100.0 * 8.297489166259766
Epoch 2340, val loss: 0.4024606943130493
Epoch 2350, training loss: 830.0632934570312 = 0.27305957674980164 + 100.0 * 8.29790210723877
Epoch 2350, val loss: 0.4021337032318115
Epoch 2360, training loss: 829.9696044921875 = 0.27197009325027466 + 100.0 * 8.296976089477539
Epoch 2360, val loss: 0.40231597423553467
Epoch 2370, training loss: 829.78564453125 = 0.27088695764541626 + 100.0 * 8.295147895812988
Epoch 2370, val loss: 0.40230438113212585
Epoch 2380, training loss: 829.728271484375 = 0.26981136202812195 + 100.0 * 8.294584274291992
Epoch 2380, val loss: 0.40238481760025024
Epoch 2390, training loss: 829.7750244140625 = 0.2687380909919739 + 100.0 * 8.295063018798828
Epoch 2390, val loss: 0.4023101329803467
Epoch 2400, training loss: 830.1851806640625 = 0.26764976978302 + 100.0 * 8.299175262451172
Epoch 2400, val loss: 0.4023749530315399
Epoch 2410, training loss: 830.3528442382812 = 0.2665320336818695 + 100.0 * 8.300863265991211
Epoch 2410, val loss: 0.4023355543613434
Epoch 2420, training loss: 829.8697509765625 = 0.26539069414138794 + 100.0 * 8.296043395996094
Epoch 2420, val loss: 0.40231212973594666
Epoch 2430, training loss: 829.6459350585938 = 0.2642856240272522 + 100.0 * 8.293816566467285
Epoch 2430, val loss: 0.4023272693157196
Epoch 2440, training loss: 829.5961303710938 = 0.2631929814815521 + 100.0 * 8.293329238891602
Epoch 2440, val loss: 0.40232041478157043
Epoch 2450, training loss: 829.5686645507812 = 0.26210489869117737 + 100.0 * 8.293066024780273
Epoch 2450, val loss: 0.4023216664791107
Epoch 2460, training loss: 829.9239501953125 = 0.2610200345516205 + 100.0 * 8.296628952026367
Epoch 2460, val loss: 0.4022369980812073
Epoch 2470, training loss: 829.62646484375 = 0.25987508893013 + 100.0 * 8.293665885925293
Epoch 2470, val loss: 0.4025336503982544
Epoch 2480, training loss: 829.6788330078125 = 0.2587343454360962 + 100.0 * 8.294200897216797
Epoch 2480, val loss: 0.4022417664527893
Epoch 2490, training loss: 829.5707397460938 = 0.2576095461845398 + 100.0 * 8.293130874633789
Epoch 2490, val loss: 0.40254393219947815
Epoch 2500, training loss: 829.5218505859375 = 0.25649791955947876 + 100.0 * 8.292654037475586
Epoch 2500, val loss: 0.40254756808280945
Epoch 2510, training loss: 830.0176391601562 = 0.25539183616638184 + 100.0 * 8.297622680664062
Epoch 2510, val loss: 0.40262967348098755
Epoch 2520, training loss: 829.930908203125 = 0.25424739718437195 + 100.0 * 8.29676628112793
Epoch 2520, val loss: 0.402460515499115
Epoch 2530, training loss: 829.5079956054688 = 0.25309816002845764 + 100.0 * 8.292549133300781
Epoch 2530, val loss: 0.40272578597068787
Epoch 2540, training loss: 829.5059204101562 = 0.25197476148605347 + 100.0 * 8.292539596557617
Epoch 2540, val loss: 0.4026270806789398
Epoch 2550, training loss: 829.4124145507812 = 0.2508662939071655 + 100.0 * 8.29161548614502
Epoch 2550, val loss: 0.4029146432876587
Epoch 2560, training loss: 829.4883422851562 = 0.24975818395614624 + 100.0 * 8.292386054992676
Epoch 2560, val loss: 0.40302228927612305
Epoch 2570, training loss: 829.8045654296875 = 0.24863824248313904 + 100.0 * 8.29555892944336
Epoch 2570, val loss: 0.4032004177570343
Epoch 2580, training loss: 829.5447387695312 = 0.2474706918001175 + 100.0 * 8.292972564697266
Epoch 2580, val loss: 0.40302151441574097
Epoch 2590, training loss: 829.6289672851562 = 0.2463233768939972 + 100.0 * 8.29382610321045
Epoch 2590, val loss: 0.403329998254776
Epoch 2600, training loss: 829.4526977539062 = 0.2451619803905487 + 100.0 * 8.292075157165527
Epoch 2600, val loss: 0.4034211039543152
Epoch 2610, training loss: 829.3345947265625 = 0.24401313066482544 + 100.0 * 8.290905952453613
Epoch 2610, val loss: 0.4035881459712982
Epoch 2620, training loss: 829.4156494140625 = 0.24287332594394684 + 100.0 * 8.291728019714355
Epoch 2620, val loss: 0.4039076566696167
Epoch 2630, training loss: 829.4308471679688 = 0.24173074960708618 + 100.0 * 8.291891098022461
Epoch 2630, val loss: 0.4041021168231964
Epoch 2640, training loss: 829.6456909179688 = 0.24057628214359283 + 100.0 * 8.294051170349121
Epoch 2640, val loss: 0.4041151702404022
Epoch 2650, training loss: 829.4755859375 = 0.239407017827034 + 100.0 * 8.292362213134766
Epoch 2650, val loss: 0.4042472243309021
Epoch 2660, training loss: 829.3588256835938 = 0.2382424771785736 + 100.0 * 8.291206359863281
Epoch 2660, val loss: 0.4045611619949341
Epoch 2670, training loss: 829.2216186523438 = 0.23708799481391907 + 100.0 * 8.28984546661377
Epoch 2670, val loss: 0.4044957756996155
Epoch 2680, training loss: 829.2088012695312 = 0.235946387052536 + 100.0 * 8.289728164672852
Epoch 2680, val loss: 0.4049203395843506
Epoch 2690, training loss: 829.311279296875 = 0.23481452465057373 + 100.0 * 8.290764808654785
Epoch 2690, val loss: 0.4051711857318878
Epoch 2700, training loss: 829.5916748046875 = 0.2336687296628952 + 100.0 * 8.293580055236816
Epoch 2700, val loss: 0.4053027033805847
Epoch 2710, training loss: 829.3013305664062 = 0.23250757157802582 + 100.0 * 8.290688514709473
Epoch 2710, val loss: 0.4053780436515808
Epoch 2720, training loss: 829.2899780273438 = 0.23135145008563995 + 100.0 * 8.290586471557617
Epoch 2720, val loss: 0.40570855140686035
Epoch 2730, training loss: 829.2448120117188 = 0.23019668459892273 + 100.0 * 8.290145874023438
Epoch 2730, val loss: 0.406087726354599
Epoch 2740, training loss: 829.3013305664062 = 0.22904431819915771 + 100.0 * 8.290722846984863
Epoch 2740, val loss: 0.4061524271965027
Epoch 2750, training loss: 829.1845092773438 = 0.2278970628976822 + 100.0 * 8.289566040039062
Epoch 2750, val loss: 0.40647271275520325
Epoch 2760, training loss: 829.2227172851562 = 0.22675275802612305 + 100.0 * 8.289959907531738
Epoch 2760, val loss: 0.4066939055919647
Epoch 2770, training loss: 829.1403198242188 = 0.22558969259262085 + 100.0 * 8.28914737701416
Epoch 2770, val loss: 0.4069325029850006
Epoch 2780, training loss: 829.0783081054688 = 0.22443123161792755 + 100.0 * 8.288538932800293
Epoch 2780, val loss: 0.40727466344833374
Epoch 2790, training loss: 829.67919921875 = 0.22328701615333557 + 100.0 * 8.294559478759766
Epoch 2790, val loss: 0.4073600769042969
Epoch 2800, training loss: 829.3714599609375 = 0.22209523618221283 + 100.0 * 8.29149341583252
Epoch 2800, val loss: 0.4081505835056305
Epoch 2810, training loss: 829.075439453125 = 0.2209046483039856 + 100.0 * 8.288545608520508
Epoch 2810, val loss: 0.4081452786922455
Epoch 2820, training loss: 828.9835815429688 = 0.2197345793247223 + 100.0 * 8.287638664245605
Epoch 2820, val loss: 0.40867507457733154
Epoch 2830, training loss: 828.9549560546875 = 0.2185787707567215 + 100.0 * 8.28736400604248
Epoch 2830, val loss: 0.4088674783706665
Epoch 2840, training loss: 828.9559936523438 = 0.21742990612983704 + 100.0 * 8.287385940551758
Epoch 2840, val loss: 0.40931692719459534
Epoch 2850, training loss: 829.4957275390625 = 0.21629272401332855 + 100.0 * 8.292794227600098
Epoch 2850, val loss: 0.40978193283081055
Epoch 2860, training loss: 829.0904541015625 = 0.21510054171085358 + 100.0 * 8.288753509521484
Epoch 2860, val loss: 0.4100828170776367
Epoch 2870, training loss: 829.1564331054688 = 0.21392613649368286 + 100.0 * 8.289424896240234
Epoch 2870, val loss: 0.41055598855018616
Epoch 2880, training loss: 829.0191650390625 = 0.21274489164352417 + 100.0 * 8.288064002990723
Epoch 2880, val loss: 0.4109652638435364
Epoch 2890, training loss: 828.9053955078125 = 0.2115725874900818 + 100.0 * 8.286938667297363
Epoch 2890, val loss: 0.4112931787967682
Epoch 2900, training loss: 828.8988037109375 = 0.21040929853916168 + 100.0 * 8.286884307861328
Epoch 2900, val loss: 0.41176554560661316
Epoch 2910, training loss: 829.0671997070312 = 0.2092512995004654 + 100.0 * 8.288579940795898
Epoch 2910, val loss: 0.4122040271759033
Epoch 2920, training loss: 829.1436157226562 = 0.20807097852230072 + 100.0 * 8.289355278015137
Epoch 2920, val loss: 0.4127909541130066
Epoch 2930, training loss: 829.1779174804688 = 0.2068852335214615 + 100.0 * 8.28971004486084
Epoch 2930, val loss: 0.41335904598236084
Epoch 2940, training loss: 828.9830932617188 = 0.20570619404315948 + 100.0 * 8.287774085998535
Epoch 2940, val loss: 0.413774698972702
Epoch 2950, training loss: 828.8298950195312 = 0.20453041791915894 + 100.0 * 8.286253929138184
Epoch 2950, val loss: 0.4143136739730835
Epoch 2960, training loss: 828.8583984375 = 0.20337489247322083 + 100.0 * 8.286550521850586
Epoch 2960, val loss: 0.41493818163871765
Epoch 2970, training loss: 829.1403198242188 = 0.20221617817878723 + 100.0 * 8.28938102722168
Epoch 2970, val loss: 0.4155332148075104
Epoch 2980, training loss: 828.9320068359375 = 0.20104090869426727 + 100.0 * 8.287309646606445
Epoch 2980, val loss: 0.4156889021396637
Epoch 2990, training loss: 829.1878662109375 = 0.19987927377223969 + 100.0 * 8.28987979888916
Epoch 2990, val loss: 0.4164992868900299
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8417047184170472
0.8667680938926321
=== training gcn model ===
Epoch 0, training loss: 1059.311767578125 = 1.0867253541946411 + 100.0 * 10.58225154876709
Epoch 0, val loss: 1.085694670677185
Epoch 10, training loss: 1059.2022705078125 = 1.081960678100586 + 100.0 * 10.58120346069336
Epoch 10, val loss: 1.0809144973754883
Epoch 20, training loss: 1058.3546142578125 = 1.0761370658874512 + 100.0 * 10.572784423828125
Epoch 20, val loss: 1.0750935077667236
Epoch 30, training loss: 1052.3814697265625 = 1.0689409971237183 + 100.0 * 10.5131254196167
Epoch 30, val loss: 1.0679218769073486
Epoch 40, training loss: 1027.47216796875 = 1.0604220628738403 + 100.0 * 10.264117240905762
Epoch 40, val loss: 1.0595669746398926
Epoch 50, training loss: 970.321044921875 = 1.0507192611694336 + 100.0 * 9.692703247070312
Epoch 50, val loss: 1.0499145984649658
Epoch 60, training loss: 952.0614013671875 = 1.0400266647338867 + 100.0 * 9.510213851928711
Epoch 60, val loss: 1.0394753217697144
Epoch 70, training loss: 937.3070068359375 = 1.030409574508667 + 100.0 * 9.36276626586914
Epoch 70, val loss: 1.0301458835601807
Epoch 80, training loss: 919.1822509765625 = 1.0214416980743408 + 100.0 * 9.181608200073242
Epoch 80, val loss: 1.0214499235153198
Epoch 90, training loss: 914.21484375 = 1.0115152597427368 + 100.0 * 9.132033348083496
Epoch 90, val loss: 1.0116581916809082
Epoch 100, training loss: 907.0335693359375 = 1.0014320611953735 + 100.0 * 9.060321807861328
Epoch 100, val loss: 1.0020138025283813
Epoch 110, training loss: 896.5814208984375 = 0.9939509034156799 + 100.0 * 8.9558744430542
Epoch 110, val loss: 0.9951092600822449
Epoch 120, training loss: 886.2839965820312 = 0.9889063239097595 + 100.0 * 8.852951049804688
Epoch 120, val loss: 0.9903822541236877
Epoch 130, training loss: 881.5007934570312 = 0.9834098815917969 + 100.0 * 8.805173873901367
Epoch 130, val loss: 0.9848678708076477
Epoch 140, training loss: 878.3285522460938 = 0.9744359850883484 + 100.0 * 8.773541450500488
Epoch 140, val loss: 0.9758444428443909
Epoch 150, training loss: 873.5260620117188 = 0.964282751083374 + 100.0 * 8.725617408752441
Epoch 150, val loss: 0.9662255644798279
Epoch 160, training loss: 869.6995239257812 = 0.9567692875862122 + 100.0 * 8.687427520751953
Epoch 160, val loss: 0.9591742753982544
Epoch 170, training loss: 866.8101196289062 = 0.9485016465187073 + 100.0 * 8.658616065979004
Epoch 170, val loss: 0.9509493112564087
Epoch 180, training loss: 863.8421630859375 = 0.9381545186042786 + 100.0 * 8.629039764404297
Epoch 180, val loss: 0.9409317374229431
Epoch 190, training loss: 861.3342895507812 = 0.9274724721908569 + 100.0 * 8.6040678024292
Epoch 190, val loss: 0.9306644797325134
Epoch 200, training loss: 859.18408203125 = 0.9163107872009277 + 100.0 * 8.582677841186523
Epoch 200, val loss: 0.9199633002281189
Epoch 210, training loss: 857.5360717773438 = 0.9041210412979126 + 100.0 * 8.566319465637207
Epoch 210, val loss: 0.9082859754562378
Epoch 220, training loss: 855.8046264648438 = 0.8910924792289734 + 100.0 * 8.549135208129883
Epoch 220, val loss: 0.8957863450050354
Epoch 230, training loss: 854.7318115234375 = 0.8775787949562073 + 100.0 * 8.538542747497559
Epoch 230, val loss: 0.8827244639396667
Epoch 240, training loss: 853.3416137695312 = 0.8634194731712341 + 100.0 * 8.524782180786133
Epoch 240, val loss: 0.8691497445106506
Epoch 250, training loss: 852.1656494140625 = 0.8489850759506226 + 100.0 * 8.513166427612305
Epoch 250, val loss: 0.855362594127655
Epoch 260, training loss: 851.3899536132812 = 0.8344228863716125 + 100.0 * 8.505555152893066
Epoch 260, val loss: 0.8413456082344055
Epoch 270, training loss: 850.33056640625 = 0.8195143938064575 + 100.0 * 8.495110511779785
Epoch 270, val loss: 0.8271337747573853
Epoch 280, training loss: 849.3806762695312 = 0.8046925663948059 + 100.0 * 8.485759735107422
Epoch 280, val loss: 0.8130688667297363
Epoch 290, training loss: 848.6192626953125 = 0.7900089621543884 + 100.0 * 8.478292465209961
Epoch 290, val loss: 0.7990543246269226
Epoch 300, training loss: 847.7559814453125 = 0.7752317786216736 + 100.0 * 8.469807624816895
Epoch 300, val loss: 0.785037100315094
Epoch 310, training loss: 847.5629272460938 = 0.7606627941131592 + 100.0 * 8.468022346496582
Epoch 310, val loss: 0.7711858749389648
Epoch 320, training loss: 846.4435424804688 = 0.74598628282547 + 100.0 * 8.456975936889648
Epoch 320, val loss: 0.7573280334472656
Epoch 330, training loss: 845.6336059570312 = 0.7318589091300964 + 100.0 * 8.449017524719238
Epoch 330, val loss: 0.7439512610435486
Epoch 340, training loss: 844.9220581054688 = 0.7180362343788147 + 100.0 * 8.44204044342041
Epoch 340, val loss: 0.7309824228286743
Epoch 350, training loss: 844.4147338867188 = 0.7044988870620728 + 100.0 * 8.437102317810059
Epoch 350, val loss: 0.718299150466919
Epoch 360, training loss: 843.9922485351562 = 0.6911449432373047 + 100.0 * 8.433011054992676
Epoch 360, val loss: 0.7056856751441956
Epoch 370, training loss: 843.5104370117188 = 0.6780495643615723 + 100.0 * 8.428323745727539
Epoch 370, val loss: 0.6934260725975037
Epoch 380, training loss: 843.3944091796875 = 0.6655206084251404 + 100.0 * 8.427289009094238
Epoch 380, val loss: 0.6817120313644409
Epoch 390, training loss: 842.7139282226562 = 0.6533679366111755 + 100.0 * 8.420605659484863
Epoch 390, val loss: 0.6704602837562561
Epoch 400, training loss: 842.1741943359375 = 0.6418148279190063 + 100.0 * 8.415324211120605
Epoch 400, val loss: 0.6596866846084595
Epoch 410, training loss: 841.9815673828125 = 0.6307932138442993 + 100.0 * 8.413507461547852
Epoch 410, val loss: 0.6494455337524414
Epoch 420, training loss: 841.6766967773438 = 0.620083212852478 + 100.0 * 8.410566329956055
Epoch 420, val loss: 0.6395480036735535
Epoch 430, training loss: 841.1889038085938 = 0.6099618077278137 + 100.0 * 8.405789375305176
Epoch 430, val loss: 0.6301730275154114
Epoch 440, training loss: 841.104736328125 = 0.6003727912902832 + 100.0 * 8.405043601989746
Epoch 440, val loss: 0.621289849281311
Epoch 450, training loss: 840.8367919921875 = 0.5912103652954102 + 100.0 * 8.402456283569336
Epoch 450, val loss: 0.6128239631652832
Epoch 460, training loss: 840.3418579101562 = 0.5825067758560181 + 100.0 * 8.39759349822998
Epoch 460, val loss: 0.6049093008041382
Epoch 470, training loss: 840.0740356445312 = 0.5744088292121887 + 100.0 * 8.394996643066406
Epoch 470, val loss: 0.5975210666656494
Epoch 480, training loss: 840.02880859375 = 0.5668211579322815 + 100.0 * 8.394619941711426
Epoch 480, val loss: 0.590631902217865
Epoch 490, training loss: 839.9346923828125 = 0.5595401525497437 + 100.0 * 8.39375114440918
Epoch 490, val loss: 0.5840587019920349
Epoch 500, training loss: 839.48779296875 = 0.5527076125144958 + 100.0 * 8.389350891113281
Epoch 500, val loss: 0.5778540372848511
Epoch 510, training loss: 839.2029418945312 = 0.5464447140693665 + 100.0 * 8.386565208435059
Epoch 510, val loss: 0.5722522735595703
Epoch 520, training loss: 839.2910766601562 = 0.5405563116073608 + 100.0 * 8.387505531311035
Epoch 520, val loss: 0.56699138879776
Epoch 530, training loss: 838.849609375 = 0.534986674785614 + 100.0 * 8.383146286010742
Epoch 530, val loss: 0.5621596574783325
Epoch 540, training loss: 838.77734375 = 0.5298745632171631 + 100.0 * 8.382474899291992
Epoch 540, val loss: 0.5577648878097534
Epoch 550, training loss: 838.6695556640625 = 0.524989128112793 + 100.0 * 8.38144588470459
Epoch 550, val loss: 0.5533790588378906
Epoch 560, training loss: 838.2228393554688 = 0.520402193069458 + 100.0 * 8.37702465057373
Epoch 560, val loss: 0.5494734048843384
Epoch 570, training loss: 837.9339599609375 = 0.5162185430526733 + 100.0 * 8.374176979064941
Epoch 570, val loss: 0.5459432601928711
Epoch 580, training loss: 837.7730712890625 = 0.512317419052124 + 100.0 * 8.372607231140137
Epoch 580, val loss: 0.5426495671272278
Epoch 590, training loss: 838.2095947265625 = 0.508650004863739 + 100.0 * 8.377009391784668
Epoch 590, val loss: 0.5394574999809265
Epoch 600, training loss: 837.5460205078125 = 0.5049811601638794 + 100.0 * 8.370409965515137
Epoch 600, val loss: 0.5365098714828491
Epoch 610, training loss: 837.2935180664062 = 0.5016328692436218 + 100.0 * 8.367918968200684
Epoch 610, val loss: 0.5337992906570435
Epoch 620, training loss: 837.094970703125 = 0.49856632947921753 + 100.0 * 8.36596393585205
Epoch 620, val loss: 0.5313975811004639
Epoch 630, training loss: 837.4005737304688 = 0.49568045139312744 + 100.0 * 8.369049072265625
Epoch 630, val loss: 0.5291348695755005
Epoch 640, training loss: 837.0202026367188 = 0.49285605549812317 + 100.0 * 8.365273475646973
Epoch 640, val loss: 0.526909351348877
Epoch 650, training loss: 836.9132690429688 = 0.4902147948741913 + 100.0 * 8.364230155944824
Epoch 650, val loss: 0.5249419808387756
Epoch 660, training loss: 836.577392578125 = 0.48766404390335083 + 100.0 * 8.360897064208984
Epoch 660, val loss: 0.5228772163391113
Epoch 670, training loss: 836.3446044921875 = 0.4853047728538513 + 100.0 * 8.358592987060547
Epoch 670, val loss: 0.5211819410324097
Epoch 680, training loss: 836.2409057617188 = 0.48309314250946045 + 100.0 * 8.35757827758789
Epoch 680, val loss: 0.5195284485816956
Epoch 690, training loss: 836.28076171875 = 0.48097965121269226 + 100.0 * 8.35799789428711
Epoch 690, val loss: 0.5179545879364014
Epoch 700, training loss: 836.3445434570312 = 0.47888779640197754 + 100.0 * 8.358656883239746
Epoch 700, val loss: 0.5163871645927429
Epoch 710, training loss: 836.0812377929688 = 0.47683754563331604 + 100.0 * 8.356043815612793
Epoch 710, val loss: 0.5150153636932373
Epoch 720, training loss: 835.8080444335938 = 0.47492414712905884 + 100.0 * 8.353331565856934
Epoch 720, val loss: 0.5136425495147705
Epoch 730, training loss: 835.6346435546875 = 0.47314804792404175 + 100.0 * 8.351614952087402
Epoch 730, val loss: 0.512412428855896
Epoch 740, training loss: 835.6484375 = 0.4714720547199249 + 100.0 * 8.35176944732666
Epoch 740, val loss: 0.5112423300743103
Epoch 750, training loss: 835.8453979492188 = 0.46981340646743774 + 100.0 * 8.353755950927734
Epoch 750, val loss: 0.5102427005767822
Epoch 760, training loss: 835.6619873046875 = 0.4681134819984436 + 100.0 * 8.351938247680664
Epoch 760, val loss: 0.5088179111480713
Epoch 770, training loss: 835.180419921875 = 0.46653977036476135 + 100.0 * 8.347138404846191
Epoch 770, val loss: 0.507931649684906
Epoch 780, training loss: 835.010009765625 = 0.4650965929031372 + 100.0 * 8.345449447631836
Epoch 780, val loss: 0.5070202946662903
Epoch 790, training loss: 834.8876953125 = 0.4637267291545868 + 100.0 * 8.344239234924316
Epoch 790, val loss: 0.5061060190200806
Epoch 800, training loss: 835.3331909179688 = 0.4623848497867584 + 100.0 * 8.348708152770996
Epoch 800, val loss: 0.5052599310874939
Epoch 810, training loss: 835.0097045898438 = 0.460962176322937 + 100.0 * 8.345487594604492
Epoch 810, val loss: 0.5044282674789429
Epoch 820, training loss: 834.7047729492188 = 0.45960795879364014 + 100.0 * 8.342452049255371
Epoch 820, val loss: 0.5034672021865845
Epoch 830, training loss: 834.7255249023438 = 0.4583069980144501 + 100.0 * 8.342672348022461
Epoch 830, val loss: 0.502633273601532
Epoch 840, training loss: 834.472900390625 = 0.45705145597457886 + 100.0 * 8.340158462524414
Epoch 840, val loss: 0.5018429756164551
Epoch 850, training loss: 834.3161010742188 = 0.4558640420436859 + 100.0 * 8.338602066040039
Epoch 850, val loss: 0.5012098550796509
Epoch 860, training loss: 834.35107421875 = 0.45470190048217773 + 100.0 * 8.338963508605957
Epoch 860, val loss: 0.5004241466522217
Epoch 870, training loss: 834.7396240234375 = 0.4534403085708618 + 100.0 * 8.342862129211426
Epoch 870, val loss: 0.499686062335968
Epoch 880, training loss: 834.0572509765625 = 0.45216602087020874 + 100.0 * 8.336050987243652
Epoch 880, val loss: 0.49868035316467285
Epoch 890, training loss: 833.94189453125 = 0.45102059841156006 + 100.0 * 8.334908485412598
Epoch 890, val loss: 0.498047798871994
Epoch 900, training loss: 833.8604736328125 = 0.44993856549263 + 100.0 * 8.334105491638184
Epoch 900, val loss: 0.49748343229293823
Epoch 910, training loss: 833.8026733398438 = 0.4488811492919922 + 100.0 * 8.333538055419922
Epoch 910, val loss: 0.4969184398651123
Epoch 920, training loss: 834.4420166015625 = 0.4477986693382263 + 100.0 * 8.33994197845459
Epoch 920, val loss: 0.49643009901046753
Epoch 930, training loss: 833.8121948242188 = 0.4466490149497986 + 100.0 * 8.33365535736084
Epoch 930, val loss: 0.49542236328125
Epoch 940, training loss: 833.5596313476562 = 0.4455627202987671 + 100.0 * 8.331140518188477
Epoch 940, val loss: 0.4948539435863495
Epoch 950, training loss: 833.5235595703125 = 0.44452428817749023 + 100.0 * 8.330790519714355
Epoch 950, val loss: 0.49426817893981934
Epoch 960, training loss: 833.7598876953125 = 0.44345542788505554 + 100.0 * 8.33316421508789
Epoch 960, val loss: 0.4936596751213074
Epoch 970, training loss: 833.4141845703125 = 0.44234248995780945 + 100.0 * 8.329718589782715
Epoch 970, val loss: 0.4928951561450958
Epoch 980, training loss: 833.4310913085938 = 0.44128355383872986 + 100.0 * 8.3298978805542
Epoch 980, val loss: 0.49224600195884705
Epoch 990, training loss: 833.3696899414062 = 0.44024771451950073 + 100.0 * 8.329294204711914
Epoch 990, val loss: 0.4917457401752472
Epoch 1000, training loss: 833.1598510742188 = 0.4392125904560089 + 100.0 * 8.3272066116333
Epoch 1000, val loss: 0.4909976124763489
Epoch 1010, training loss: 833.1422729492188 = 0.4382033944129944 + 100.0 * 8.327040672302246
Epoch 1010, val loss: 0.4904724657535553
Epoch 1020, training loss: 833.1721801757812 = 0.437193900346756 + 100.0 * 8.327349662780762
Epoch 1020, val loss: 0.4898260533809662
Epoch 1030, training loss: 833.1832885742188 = 0.43612009286880493 + 100.0 * 8.327471733093262
Epoch 1030, val loss: 0.4892357289791107
Epoch 1040, training loss: 832.837646484375 = 0.4350424110889435 + 100.0 * 8.324026107788086
Epoch 1040, val loss: 0.4885583519935608
Epoch 1050, training loss: 832.7940063476562 = 0.43404483795166016 + 100.0 * 8.323599815368652
Epoch 1050, val loss: 0.4879743754863739
Epoch 1060, training loss: 832.9100341796875 = 0.43307337164878845 + 100.0 * 8.324769973754883
Epoch 1060, val loss: 0.4874812960624695
Epoch 1070, training loss: 832.7774047851562 = 0.43202710151672363 + 100.0 * 8.323453903198242
Epoch 1070, val loss: 0.48680615425109863
Epoch 1080, training loss: 832.7167358398438 = 0.4309985935688019 + 100.0 * 8.322857856750488
Epoch 1080, val loss: 0.48616042733192444
Epoch 1090, training loss: 832.5798950195312 = 0.4300176799297333 + 100.0 * 8.32149887084961
Epoch 1090, val loss: 0.4855448007583618
Epoch 1100, training loss: 832.5289916992188 = 0.42905500531196594 + 100.0 * 8.320999145507812
Epoch 1100, val loss: 0.48495304584503174
Epoch 1110, training loss: 832.9508056640625 = 0.42806729674339294 + 100.0 * 8.325227737426758
Epoch 1110, val loss: 0.48425546288490295
Epoch 1120, training loss: 832.5646362304688 = 0.4270435869693756 + 100.0 * 8.321375846862793
Epoch 1120, val loss: 0.4838847219944
Epoch 1130, training loss: 832.6409301757812 = 0.42604854702949524 + 100.0 * 8.322149276733398
Epoch 1130, val loss: 0.483009934425354
Epoch 1140, training loss: 832.669189453125 = 0.4249674677848816 + 100.0 * 8.322442054748535
Epoch 1140, val loss: 0.48245304822921753
Epoch 1150, training loss: 832.4300537109375 = 0.42390623688697815 + 100.0 * 8.320061683654785
Epoch 1150, val loss: 0.48186638951301575
Epoch 1160, training loss: 832.1884765625 = 0.42291802167892456 + 100.0 * 8.317655563354492
Epoch 1160, val loss: 0.4812537431716919
Epoch 1170, training loss: 832.11328125 = 0.4219546616077423 + 100.0 * 8.316913604736328
Epoch 1170, val loss: 0.4806517958641052
Epoch 1180, training loss: 832.094482421875 = 0.42099449038505554 + 100.0 * 8.316734313964844
Epoch 1180, val loss: 0.48018768429756165
Epoch 1190, training loss: 832.9548950195312 = 0.4199652075767517 + 100.0 * 8.325348854064941
Epoch 1190, val loss: 0.47957155108451843
Epoch 1200, training loss: 832.0731201171875 = 0.4188542664051056 + 100.0 * 8.316542625427246
Epoch 1200, val loss: 0.4787447154521942
Epoch 1210, training loss: 831.9421997070312 = 0.41782939434051514 + 100.0 * 8.3152437210083
Epoch 1210, val loss: 0.47818723320961
Epoch 1220, training loss: 831.9013061523438 = 0.416843444108963 + 100.0 * 8.314844131469727
Epoch 1220, val loss: 0.47761669754981995
Epoch 1230, training loss: 831.8301391601562 = 0.41587361693382263 + 100.0 * 8.314142227172852
Epoch 1230, val loss: 0.47709858417510986
Epoch 1240, training loss: 832.2923583984375 = 0.4148860275745392 + 100.0 * 8.318775177001953
Epoch 1240, val loss: 0.476618230342865
Epoch 1250, training loss: 831.8289794921875 = 0.41381216049194336 + 100.0 * 8.314151763916016
Epoch 1250, val loss: 0.475740522146225
Epoch 1260, training loss: 831.7792358398438 = 0.4127834737300873 + 100.0 * 8.313664436340332
Epoch 1260, val loss: 0.47537723183631897
Epoch 1270, training loss: 831.89501953125 = 0.41175806522369385 + 100.0 * 8.31483268737793
Epoch 1270, val loss: 0.4745827913284302
Epoch 1280, training loss: 831.6314086914062 = 0.4107278287410736 + 100.0 * 8.312207221984863
Epoch 1280, val loss: 0.47405296564102173
Epoch 1290, training loss: 831.6117553710938 = 0.40972742438316345 + 100.0 * 8.312020301818848
Epoch 1290, val loss: 0.4733808636665344
Epoch 1300, training loss: 831.7601318359375 = 0.40871554613113403 + 100.0 * 8.313514709472656
Epoch 1300, val loss: 0.47276148200035095
Epoch 1310, training loss: 831.6044311523438 = 0.407627671957016 + 100.0 * 8.311967849731445
Epoch 1310, val loss: 0.4720727801322937
Epoch 1320, training loss: 831.54638671875 = 0.4065544605255127 + 100.0 * 8.31139850616455
Epoch 1320, val loss: 0.47146543860435486
Epoch 1330, training loss: 831.4188842773438 = 0.405524343252182 + 100.0 * 8.310133934020996
Epoch 1330, val loss: 0.4709351360797882
Epoch 1340, training loss: 831.3290405273438 = 0.40451425313949585 + 100.0 * 8.309245109558105
Epoch 1340, val loss: 0.47033292055130005
Epoch 1350, training loss: 831.338134765625 = 0.4035138785839081 + 100.0 * 8.309346199035645
Epoch 1350, val loss: 0.469725638628006
Epoch 1360, training loss: 832.2642211914062 = 0.40245291590690613 + 100.0 * 8.318617820739746
Epoch 1360, val loss: 0.4691401422023773
Epoch 1370, training loss: 831.3558349609375 = 0.4012792706489563 + 100.0 * 8.309545516967773
Epoch 1370, val loss: 0.4683459401130676
Epoch 1380, training loss: 831.2675170898438 = 0.40019556879997253 + 100.0 * 8.308672904968262
Epoch 1380, val loss: 0.46756574511528015
Epoch 1390, training loss: 831.1867065429688 = 0.3991517722606659 + 100.0 * 8.307875633239746
Epoch 1390, val loss: 0.4671316146850586
Epoch 1400, training loss: 831.1085205078125 = 0.39813271164894104 + 100.0 * 8.307104110717773
Epoch 1400, val loss: 0.466422975063324
Epoch 1410, training loss: 831.1536865234375 = 0.3971095085144043 + 100.0 * 8.307565689086914
Epoch 1410, val loss: 0.46580854058265686
Epoch 1420, training loss: 831.6524047851562 = 0.3960232436656952 + 100.0 * 8.3125638961792
Epoch 1420, val loss: 0.4652133285999298
Epoch 1430, training loss: 831.1175537109375 = 0.3948422074317932 + 100.0 * 8.30722713470459
Epoch 1430, val loss: 0.4643450677394867
Epoch 1440, training loss: 831.0905151367188 = 0.3937278687953949 + 100.0 * 8.306967735290527
Epoch 1440, val loss: 0.4637818932533264
Epoch 1450, training loss: 830.9495239257812 = 0.3926568329334259 + 100.0 * 8.30556869506836
Epoch 1450, val loss: 0.46307995915412903
Epoch 1460, training loss: 830.9838256835938 = 0.3915986120700836 + 100.0 * 8.305922508239746
Epoch 1460, val loss: 0.46245288848876953
Epoch 1470, training loss: 831.29638671875 = 0.39051270484924316 + 100.0 * 8.309059143066406
Epoch 1470, val loss: 0.46181249618530273
Epoch 1480, training loss: 830.9851684570312 = 0.38938239216804504 + 100.0 * 8.305957794189453
Epoch 1480, val loss: 0.4609829783439636
Epoch 1490, training loss: 830.9285888671875 = 0.38827621936798096 + 100.0 * 8.305402755737305
Epoch 1490, val loss: 0.4605128765106201
Epoch 1500, training loss: 830.9662475585938 = 0.3871506154537201 + 100.0 * 8.305790901184082
Epoch 1500, val loss: 0.4596826434135437
Epoch 1510, training loss: 830.7598876953125 = 0.3860260546207428 + 100.0 * 8.303738594055176
Epoch 1510, val loss: 0.4590036869049072
Epoch 1520, training loss: 830.9332275390625 = 0.3849203884601593 + 100.0 * 8.305482864379883
Epoch 1520, val loss: 0.4583786427974701
Epoch 1530, training loss: 830.984375 = 0.38375431299209595 + 100.0 * 8.30600643157959
Epoch 1530, val loss: 0.4576093852519989
Epoch 1540, training loss: 830.7974243164062 = 0.38258644938468933 + 100.0 * 8.30414867401123
Epoch 1540, val loss: 0.4570005238056183
Epoch 1550, training loss: 830.6614990234375 = 0.3814477324485779 + 100.0 * 8.302800178527832
Epoch 1550, val loss: 0.45626556873321533
Epoch 1560, training loss: 830.755859375 = 0.3803093433380127 + 100.0 * 8.303755760192871
Epoch 1560, val loss: 0.45554280281066895
Epoch 1570, training loss: 830.884033203125 = 0.3791385591030121 + 100.0 * 8.305048942565918
Epoch 1570, val loss: 0.4548919200897217
Epoch 1580, training loss: 830.7144775390625 = 0.37793034315109253 + 100.0 * 8.303365707397461
Epoch 1580, val loss: 0.4539007842540741
Epoch 1590, training loss: 830.7161254882812 = 0.3767354190349579 + 100.0 * 8.303394317626953
Epoch 1590, val loss: 0.45328202843666077
Epoch 1600, training loss: 830.5250854492188 = 0.37554940581321716 + 100.0 * 8.301495552062988
Epoch 1600, val loss: 0.4526370167732239
Epoch 1610, training loss: 830.4767456054688 = 0.3743859529495239 + 100.0 * 8.301023483276367
Epoch 1610, val loss: 0.4520370364189148
Epoch 1620, training loss: 830.7850341796875 = 0.3732195496559143 + 100.0 * 8.304118156433105
Epoch 1620, val loss: 0.45124566555023193
Epoch 1630, training loss: 830.6997680664062 = 0.37194523215293884 + 100.0 * 8.303277969360352
Epoch 1630, val loss: 0.4503331184387207
Epoch 1640, training loss: 830.51220703125 = 0.3706933856010437 + 100.0 * 8.30141544342041
Epoch 1640, val loss: 0.44971194863319397
Epoch 1650, training loss: 830.3932495117188 = 0.36948835849761963 + 100.0 * 8.300237655639648
Epoch 1650, val loss: 0.4489668309688568
Epoch 1660, training loss: 830.3292846679688 = 0.36828941106796265 + 100.0 * 8.299610137939453
Epoch 1660, val loss: 0.44824913144111633
Epoch 1670, training loss: 830.5167846679688 = 0.36708834767341614 + 100.0 * 8.301497459411621
Epoch 1670, val loss: 0.4475201964378357
Epoch 1680, training loss: 830.4002685546875 = 0.3658148944377899 + 100.0 * 8.300344467163086
Epoch 1680, val loss: 0.44668254256248474
Epoch 1690, training loss: 830.2977294921875 = 0.36454641819000244 + 100.0 * 8.299331665039062
Epoch 1690, val loss: 0.4458772540092468
Epoch 1700, training loss: 830.2147827148438 = 0.3633122444152832 + 100.0 * 8.298514366149902
Epoch 1700, val loss: 0.44530150294303894
Epoch 1710, training loss: 830.1876220703125 = 0.3620890974998474 + 100.0 * 8.298255920410156
Epoch 1710, val loss: 0.44457605481147766
Epoch 1720, training loss: 830.2278442382812 = 0.36086514592170715 + 100.0 * 8.298669815063477
Epoch 1720, val loss: 0.4439145624637604
Epoch 1730, training loss: 831.1698608398438 = 0.35961049795150757 + 100.0 * 8.30810260772705
Epoch 1730, val loss: 0.4429187774658203
Epoch 1740, training loss: 830.2057495117188 = 0.358223557472229 + 100.0 * 8.29847526550293
Epoch 1740, val loss: 0.44243955612182617
Epoch 1750, training loss: 830.23681640625 = 0.3569612205028534 + 100.0 * 8.298798561096191
Epoch 1750, val loss: 0.44151046872138977
Epoch 1760, training loss: 830.064208984375 = 0.35572361946105957 + 100.0 * 8.29708480834961
Epoch 1760, val loss: 0.44086581468582153
Epoch 1770, training loss: 830.0621948242188 = 0.35449886322021484 + 100.0 * 8.297077178955078
Epoch 1770, val loss: 0.4402373731136322
Epoch 1780, training loss: 830.4813842773438 = 0.353282630443573 + 100.0 * 8.301280975341797
Epoch 1780, val loss: 0.4394305646419525
Epoch 1790, training loss: 830.0438842773438 = 0.35197627544403076 + 100.0 * 8.296918869018555
Epoch 1790, val loss: 0.43892860412597656
Epoch 1800, training loss: 830.0275268554688 = 0.3507207930088043 + 100.0 * 8.296768188476562
Epoch 1800, val loss: 0.43798357248306274
Epoch 1810, training loss: 829.9764404296875 = 0.3494795262813568 + 100.0 * 8.296269416809082
Epoch 1810, val loss: 0.43755659461021423
Epoch 1820, training loss: 829.934326171875 = 0.34825751185417175 + 100.0 * 8.295860290527344
Epoch 1820, val loss: 0.43685010075569153
Epoch 1830, training loss: 829.9931640625 = 0.34703347086906433 + 100.0 * 8.29646110534668
Epoch 1830, val loss: 0.4363371729850769
Epoch 1840, training loss: 830.399658203125 = 0.3457680940628052 + 100.0 * 8.300539016723633
Epoch 1840, val loss: 0.4353504478931427
Epoch 1850, training loss: 829.9481811523438 = 0.3444797396659851 + 100.0 * 8.296036720275879
Epoch 1850, val loss: 0.4349818527698517
Epoch 1860, training loss: 829.8467407226562 = 0.34323081374168396 + 100.0 * 8.295035362243652
Epoch 1860, val loss: 0.434273898601532
Epoch 1870, training loss: 830.1458129882812 = 0.3420040011405945 + 100.0 * 8.298038482666016
Epoch 1870, val loss: 0.433834969997406
Epoch 1880, training loss: 829.8834228515625 = 0.34071964025497437 + 100.0 * 8.295427322387695
Epoch 1880, val loss: 0.432999849319458
Epoch 1890, training loss: 829.8287963867188 = 0.33947041630744934 + 100.0 * 8.294893264770508
Epoch 1890, val loss: 0.4326077997684479
Epoch 1900, training loss: 829.7723388671875 = 0.3382513225078583 + 100.0 * 8.294341087341309
Epoch 1900, val loss: 0.4320356845855713
Epoch 1910, training loss: 829.7387084960938 = 0.3370665907859802 + 100.0 * 8.29401683807373
Epoch 1910, val loss: 0.43148088455200195
Epoch 1920, training loss: 829.8917846679688 = 0.33588504791259766 + 100.0 * 8.29555892944336
Epoch 1920, val loss: 0.4309549331665039
Epoch 1930, training loss: 829.8427734375 = 0.33463120460510254 + 100.0 * 8.29508113861084
Epoch 1930, val loss: 0.43047505617141724
Epoch 1940, training loss: 829.700439453125 = 0.33339592814445496 + 100.0 * 8.293670654296875
Epoch 1940, val loss: 0.4298771321773529
Epoch 1950, training loss: 829.7063598632812 = 0.33219414949417114 + 100.0 * 8.293741226196289
Epoch 1950, val loss: 0.42951565980911255
Epoch 1960, training loss: 829.6419067382812 = 0.33101725578308105 + 100.0 * 8.293108940124512
Epoch 1960, val loss: 0.42902249097824097
Epoch 1970, training loss: 829.6370849609375 = 0.329857736825943 + 100.0 * 8.293072700500488
Epoch 1970, val loss: 0.4286622107028961
Epoch 1980, training loss: 830.4988403320312 = 0.32868191599845886 + 100.0 * 8.301701545715332
Epoch 1980, val loss: 0.42828571796417236
Epoch 1990, training loss: 830.0034790039062 = 0.3274380564689636 + 100.0 * 8.296760559082031
Epoch 1990, val loss: 0.42740315198898315
Epoch 2000, training loss: 829.7094116210938 = 0.32621249556541443 + 100.0 * 8.293831825256348
Epoch 2000, val loss: 0.4272901117801666
Epoch 2010, training loss: 829.5765991210938 = 0.32502275705337524 + 100.0 * 8.292515754699707
Epoch 2010, val loss: 0.4267832636833191
Epoch 2020, training loss: 829.5502319335938 = 0.3238762319087982 + 100.0 * 8.292263984680176
Epoch 2020, val loss: 0.4264698624610901
Epoch 2030, training loss: 829.5178833007812 = 0.32272928953170776 + 100.0 * 8.291951179504395
Epoch 2030, val loss: 0.42612630128860474
Epoch 2040, training loss: 829.6885375976562 = 0.3215809464454651 + 100.0 * 8.293669700622559
Epoch 2040, val loss: 0.4258708357810974
Epoch 2050, training loss: 829.6021118164062 = 0.3203906714916229 + 100.0 * 8.292817115783691
Epoch 2050, val loss: 0.42536604404449463
Epoch 2060, training loss: 829.5302734375 = 0.319230854511261 + 100.0 * 8.292110443115234
Epoch 2060, val loss: 0.42493993043899536
Epoch 2070, training loss: 829.4415283203125 = 0.3180699050426483 + 100.0 * 8.291234970092773
Epoch 2070, val loss: 0.4247570335865021
Epoch 2080, training loss: 830.1100463867188 = 0.31692227721214294 + 100.0 * 8.297931671142578
Epoch 2080, val loss: 0.42455747723579407
Epoch 2090, training loss: 829.6631469726562 = 0.3157215416431427 + 100.0 * 8.293474197387695
Epoch 2090, val loss: 0.4236651659011841
Epoch 2100, training loss: 829.4346923828125 = 0.31453558802604675 + 100.0 * 8.2912015914917
Epoch 2100, val loss: 0.4235590398311615
Epoch 2110, training loss: 829.3462524414062 = 0.31338053941726685 + 100.0 * 8.290328979492188
Epoch 2110, val loss: 0.4234100580215454
Epoch 2120, training loss: 829.3180541992188 = 0.3122525215148926 + 100.0 * 8.290058135986328
Epoch 2120, val loss: 0.42302027344703674
Epoch 2130, training loss: 829.3515014648438 = 0.311129629611969 + 100.0 * 8.290403366088867
Epoch 2130, val loss: 0.4228850305080414
Epoch 2140, training loss: 829.9913940429688 = 0.30998727679252625 + 100.0 * 8.29681396484375
Epoch 2140, val loss: 0.4226055145263672
Epoch 2150, training loss: 829.6758422851562 = 0.30879515409469604 + 100.0 * 8.293670654296875
Epoch 2150, val loss: 0.421941339969635
Epoch 2160, training loss: 829.44873046875 = 0.30760669708251953 + 100.0 * 8.291411399841309
Epoch 2160, val loss: 0.42201605439186096
Epoch 2170, training loss: 829.2548828125 = 0.3064529597759247 + 100.0 * 8.289484024047852
Epoch 2170, val loss: 0.4216768741607666
Epoch 2180, training loss: 829.2103881835938 = 0.3053266108036041 + 100.0 * 8.289051055908203
Epoch 2180, val loss: 0.42143523693084717
Epoch 2190, training loss: 829.2260131835938 = 0.304208368062973 + 100.0 * 8.289217948913574
Epoch 2190, val loss: 0.42121461033821106
Epoch 2200, training loss: 829.5779418945312 = 0.30308032035827637 + 100.0 * 8.29274845123291
Epoch 2200, val loss: 0.42102646827697754
Epoch 2210, training loss: 829.5822143554688 = 0.30192509293556213 + 100.0 * 8.292802810668945
Epoch 2210, val loss: 0.42061400413513184
Epoch 2220, training loss: 829.2871704101562 = 0.3007463216781616 + 100.0 * 8.289864540100098
Epoch 2220, val loss: 0.42053279280662537
Epoch 2230, training loss: 829.355224609375 = 0.2996111810207367 + 100.0 * 8.290555953979492
Epoch 2230, val loss: 0.4201141595840454
Epoch 2240, training loss: 829.2344970703125 = 0.29846540093421936 + 100.0 * 8.289360046386719
Epoch 2240, val loss: 0.420022577047348
Epoch 2250, training loss: 829.228759765625 = 0.2973380982875824 + 100.0 * 8.289314270019531
Epoch 2250, val loss: 0.41991451382637024
Epoch 2260, training loss: 829.2755126953125 = 0.2962009906768799 + 100.0 * 8.289793014526367
Epoch 2260, val loss: 0.41960257291793823
Epoch 2270, training loss: 829.0978393554688 = 0.2950676679611206 + 100.0 * 8.2880277633667
Epoch 2270, val loss: 0.4194053113460541
Epoch 2280, training loss: 829.0808715820312 = 0.29394057393074036 + 100.0 * 8.287869453430176
Epoch 2280, val loss: 0.41927674412727356
Epoch 2290, training loss: 829.39208984375 = 0.29281842708587646 + 100.0 * 8.290992736816406
Epoch 2290, val loss: 0.4192837178707123
Epoch 2300, training loss: 829.1431274414062 = 0.29164785146713257 + 100.0 * 8.288515090942383
Epoch 2300, val loss: 0.41897326707839966
Epoch 2310, training loss: 829.0240478515625 = 0.2905036211013794 + 100.0 * 8.287335395812988
Epoch 2310, val loss: 0.4188876450061798
Epoch 2320, training loss: 829.1192016601562 = 0.2893733084201813 + 100.0 * 8.288298606872559
Epoch 2320, val loss: 0.41881582140922546
Epoch 2330, training loss: 829.0640258789062 = 0.2882418930530548 + 100.0 * 8.287757873535156
Epoch 2330, val loss: 0.41852420568466187
Epoch 2340, training loss: 828.9798583984375 = 0.28712382912635803 + 100.0 * 8.286927223205566
Epoch 2340, val loss: 0.4183228313922882
Epoch 2350, training loss: 829.0714111328125 = 0.2860221862792969 + 100.0 * 8.287854194641113
Epoch 2350, val loss: 0.4180164337158203
Epoch 2360, training loss: 829.2259521484375 = 0.2848844528198242 + 100.0 * 8.289410591125488
Epoch 2360, val loss: 0.4180549383163452
Epoch 2370, training loss: 829.193359375 = 0.28373298048973083 + 100.0 * 8.289095878601074
Epoch 2370, val loss: 0.4179528057575226
Epoch 2380, training loss: 828.9763793945312 = 0.28258946537971497 + 100.0 * 8.286937713623047
Epoch 2380, val loss: 0.4181712567806244
Epoch 2390, training loss: 828.8732299804688 = 0.2814693748950958 + 100.0 * 8.285917282104492
Epoch 2390, val loss: 0.41767245531082153
Epoch 2400, training loss: 829.0257568359375 = 0.28036606311798096 + 100.0 * 8.287453651428223
Epoch 2400, val loss: 0.4177165925502777
Epoch 2410, training loss: 829.0751342773438 = 0.2792418301105499 + 100.0 * 8.287959098815918
Epoch 2410, val loss: 0.41774073243141174
Epoch 2420, training loss: 828.9273071289062 = 0.2781166732311249 + 100.0 * 8.286491394042969
Epoch 2420, val loss: 0.4175369441509247
Epoch 2430, training loss: 828.8502197265625 = 0.2769964933395386 + 100.0 * 8.28573226928711
Epoch 2430, val loss: 0.4173782467842102
Epoch 2440, training loss: 828.9939575195312 = 0.2758875787258148 + 100.0 * 8.28718090057373
Epoch 2440, val loss: 0.41745293140411377
Epoch 2450, training loss: 828.955078125 = 0.27476534247398376 + 100.0 * 8.286803245544434
Epoch 2450, val loss: 0.41739383339881897
Epoch 2460, training loss: 828.9170532226562 = 0.2736469805240631 + 100.0 * 8.286434173583984
Epoch 2460, val loss: 0.41715648770332336
Epoch 2470, training loss: 828.818115234375 = 0.27252572774887085 + 100.0 * 8.285455703735352
Epoch 2470, val loss: 0.41728976368904114
Epoch 2480, training loss: 828.9994506835938 = 0.27142393589019775 + 100.0 * 8.287280082702637
Epoch 2480, val loss: 0.4173848628997803
Epoch 2490, training loss: 828.897705078125 = 0.27029821276664734 + 100.0 * 8.286273956298828
Epoch 2490, val loss: 0.41709423065185547
Epoch 2500, training loss: 828.7836303710938 = 0.26919320225715637 + 100.0 * 8.285144805908203
Epoch 2500, val loss: 0.41735130548477173
Epoch 2510, training loss: 828.699462890625 = 0.26808682084083557 + 100.0 * 8.284314155578613
Epoch 2510, val loss: 0.4173583984375
Epoch 2520, training loss: 828.7435302734375 = 0.2670046091079712 + 100.0 * 8.284765243530273
Epoch 2520, val loss: 0.4175134599208832
Epoch 2530, training loss: 829.3128662109375 = 0.2659231126308441 + 100.0 * 8.2904691696167
Epoch 2530, val loss: 0.417603075504303
Epoch 2540, training loss: 828.9671020507812 = 0.26480087637901306 + 100.0 * 8.287023544311523
Epoch 2540, val loss: 0.4167124330997467
Epoch 2550, training loss: 828.6824951171875 = 0.26365965604782104 + 100.0 * 8.284188270568848
Epoch 2550, val loss: 0.4172526001930237
Epoch 2560, training loss: 828.6171264648438 = 0.26256823539733887 + 100.0 * 8.28354549407959
Epoch 2560, val loss: 0.4170522689819336
Epoch 2570, training loss: 828.6403198242188 = 0.2614865005016327 + 100.0 * 8.283788681030273
Epoch 2570, val loss: 0.4172641336917877
Epoch 2580, training loss: 829.136962890625 = 0.2604024112224579 + 100.0 * 8.288765907287598
Epoch 2580, val loss: 0.4172501266002655
Epoch 2590, training loss: 828.6897583007812 = 0.25928229093551636 + 100.0 * 8.28430461883545
Epoch 2590, val loss: 0.41728347539901733
Epoch 2600, training loss: 828.7628173828125 = 0.25817936658859253 + 100.0 * 8.285046577453613
Epoch 2600, val loss: 0.4174869656562805
Epoch 2610, training loss: 828.5958251953125 = 0.2570740878582001 + 100.0 * 8.283387184143066
Epoch 2610, val loss: 0.4171940088272095
Epoch 2620, training loss: 828.5556030273438 = 0.25600147247314453 + 100.0 * 8.28299617767334
Epoch 2620, val loss: 0.4170248210430145
Epoch 2630, training loss: 828.619384765625 = 0.25491395592689514 + 100.0 * 8.283644676208496
Epoch 2630, val loss: 0.4172724485397339
Epoch 2640, training loss: 828.7958984375 = 0.25383198261260986 + 100.0 * 8.285420417785645
Epoch 2640, val loss: 0.41725876927375793
Epoch 2650, training loss: 828.6827392578125 = 0.2527245879173279 + 100.0 * 8.284299850463867
Epoch 2650, val loss: 0.41727691888809204
Epoch 2660, training loss: 828.5498657226562 = 0.25162556767463684 + 100.0 * 8.282981872558594
Epoch 2660, val loss: 0.41742342710494995
Epoch 2670, training loss: 828.4896850585938 = 0.2505366802215576 + 100.0 * 8.282391548156738
Epoch 2670, val loss: 0.4173581302165985
Epoch 2680, training loss: 828.4906005859375 = 0.24945759773254395 + 100.0 * 8.282411575317383
Epoch 2680, val loss: 0.4174274206161499
Epoch 2690, training loss: 829.3261108398438 = 0.24839752912521362 + 100.0 * 8.290777206420898
Epoch 2690, val loss: 0.4172961413860321
Epoch 2700, training loss: 828.6737060546875 = 0.24728316068649292 + 100.0 * 8.28426456451416
Epoch 2700, val loss: 0.4177198112010956
Epoch 2710, training loss: 828.4701538085938 = 0.24618060886859894 + 100.0 * 8.28223991394043
Epoch 2710, val loss: 0.4175705313682556
Epoch 2720, training loss: 828.3855590820312 = 0.24509786069393158 + 100.0 * 8.281404495239258
Epoch 2720, val loss: 0.41777268052101135
Epoch 2730, training loss: 828.3676147460938 = 0.24402137100696564 + 100.0 * 8.281235694885254
Epoch 2730, val loss: 0.41766849160194397
Epoch 2740, training loss: 828.8006591796875 = 0.24296987056732178 + 100.0 * 8.285576820373535
Epoch 2740, val loss: 0.4174393117427826
Epoch 2750, training loss: 828.3963623046875 = 0.2418418973684311 + 100.0 * 8.281545639038086
Epoch 2750, val loss: 0.41838783025741577
Epoch 2760, training loss: 828.4788208007812 = 0.24074691534042358 + 100.0 * 8.282381057739258
Epoch 2760, val loss: 0.4180417060852051
Epoch 2770, training loss: 828.4252319335938 = 0.23965464532375336 + 100.0 * 8.281855583190918
Epoch 2770, val loss: 0.4183777868747711
Epoch 2780, training loss: 828.523681640625 = 0.2385895848274231 + 100.0 * 8.282851219177246
Epoch 2780, val loss: 0.4179588258266449
Epoch 2790, training loss: 828.4984741210938 = 0.2374739944934845 + 100.0 * 8.282609939575195
Epoch 2790, val loss: 0.41847679018974304
Epoch 2800, training loss: 828.3357543945312 = 0.2363840937614441 + 100.0 * 8.280993461608887
Epoch 2800, val loss: 0.4188515245914459
Epoch 2810, training loss: 828.3057250976562 = 0.2353043556213379 + 100.0 * 8.280704498291016
Epoch 2810, val loss: 0.4190148115158081
Epoch 2820, training loss: 828.3955688476562 = 0.23424363136291504 + 100.0 * 8.28161334991455
Epoch 2820, val loss: 0.41921937465667725
Epoch 2830, training loss: 828.5853881835938 = 0.2331714779138565 + 100.0 * 8.283522605895996
Epoch 2830, val loss: 0.41934099793434143
Epoch 2840, training loss: 828.481689453125 = 0.23211464285850525 + 100.0 * 8.282495498657227
Epoch 2840, val loss: 0.41887471079826355
Epoch 2850, training loss: 828.2617797851562 = 0.2310050129890442 + 100.0 * 8.28030776977539
Epoch 2850, val loss: 0.419636070728302
Epoch 2860, training loss: 828.2408447265625 = 0.22994081676006317 + 100.0 * 8.280109405517578
Epoch 2860, val loss: 0.41951045393943787
Epoch 2870, training loss: 828.5279541015625 = 0.22889405488967896 + 100.0 * 8.282990455627441
Epoch 2870, val loss: 0.41977059841156006
Epoch 2880, training loss: 828.2015991210938 = 0.22782176733016968 + 100.0 * 8.27973747253418
Epoch 2880, val loss: 0.41984686255455017
Epoch 2890, training loss: 828.2381591796875 = 0.22676165401935577 + 100.0 * 8.28011417388916
Epoch 2890, val loss: 0.42019912600517273
Epoch 2900, training loss: 828.3596801757812 = 0.22570312023162842 + 100.0 * 8.281339645385742
Epoch 2900, val loss: 0.4204965829849243
Epoch 2910, training loss: 828.4677124023438 = 0.22464731335639954 + 100.0 * 8.282430648803711
Epoch 2910, val loss: 0.4209844768047333
Epoch 2920, training loss: 828.2621459960938 = 0.22358064353466034 + 100.0 * 8.280385971069336
Epoch 2920, val loss: 0.4208097755908966
Epoch 2930, training loss: 828.1138916015625 = 0.2225196808576584 + 100.0 * 8.278913497924805
Epoch 2930, val loss: 0.4211081862449646
Epoch 2940, training loss: 828.2653198242188 = 0.22148014605045319 + 100.0 * 8.280438423156738
Epoch 2940, val loss: 0.4214324653148651
Epoch 2950, training loss: 828.2694091796875 = 0.22041793167591095 + 100.0 * 8.280489921569824
Epoch 2950, val loss: 0.421776682138443
Epoch 2960, training loss: 828.2887573242188 = 0.21936607360839844 + 100.0 * 8.280694007873535
Epoch 2960, val loss: 0.4224797189235687
Epoch 2970, training loss: 828.1012573242188 = 0.2182951718568802 + 100.0 * 8.278829574584961
Epoch 2970, val loss: 0.4221528470516205
Epoch 2980, training loss: 828.0309448242188 = 0.21723783016204834 + 100.0 * 8.27813720703125
Epoch 2980, val loss: 0.42224404215812683
Epoch 2990, training loss: 828.0402221679688 = 0.2161896526813507 + 100.0 * 8.278240203857422
Epoch 2990, val loss: 0.42241713404655457
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.839167935058346
0.8669129899297255
The final CL Acc:0.83443, 0.00855, The final GNN Acc:0.86718, 0.00048
Begin epxeriment: cont_weight: 100 epoch:3000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=100.0, cuda=True, dataset='Pubmed', debug=True, defense_mode='none', device_id=3, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 88648])
add edge: torch.Size([2, 97520])
remove edge: torch.Size([2, 79670])
updated graph: torch.Size([2, 88542])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1059.314208984375 = 1.08560049533844 + 100.0 * 10.58228588104248
Epoch 0, val loss: 1.0850439071655273
Epoch 10, training loss: 1059.228759765625 = 1.0811694860458374 + 100.0 * 10.581475257873535
Epoch 10, val loss: 1.0805588960647583
Epoch 20, training loss: 1058.486083984375 = 1.0757791996002197 + 100.0 * 10.574103355407715
Epoch 20, val loss: 1.0750887393951416
Epoch 30, training loss: 1053.18798828125 = 1.0690516233444214 + 100.0 * 10.521188735961914
Epoch 30, val loss: 1.0681923627853394
Epoch 40, training loss: 1033.02294921875 = 1.060822606086731 + 100.0 * 10.319621086120605
Epoch 40, val loss: 1.0598950386047363
Epoch 50, training loss: 1003.4692993164062 = 1.0519531965255737 + 100.0 * 10.024173736572266
Epoch 50, val loss: 1.0507762432098389
Epoch 60, training loss: 973.1076049804688 = 1.043270468711853 + 100.0 * 9.720643043518066
Epoch 60, val loss: 1.0423575639724731
Epoch 70, training loss: 937.8269653320312 = 1.0355502367019653 + 100.0 * 9.367914199829102
Epoch 70, val loss: 1.035368800163269
Epoch 80, training loss: 923.4334716796875 = 1.029531717300415 + 100.0 * 9.224039077758789
Epoch 80, val loss: 1.0297690629959106
Epoch 90, training loss: 918.5142822265625 = 1.0213991403579712 + 100.0 * 9.174928665161133
Epoch 90, val loss: 1.0214155912399292
Epoch 100, training loss: 908.2706298828125 = 1.0139343738555908 + 100.0 * 9.072566986083984
Epoch 100, val loss: 1.014106035232544
Epoch 110, training loss: 896.8778076171875 = 1.0094010829925537 + 100.0 * 8.958683967590332
Epoch 110, val loss: 1.0097594261169434
Epoch 120, training loss: 889.0721435546875 = 1.0046473741531372 + 100.0 * 8.880675315856934
Epoch 120, val loss: 1.0050069093704224
Epoch 130, training loss: 883.6924438476562 = 0.9984680414199829 + 100.0 * 8.826939582824707
Epoch 130, val loss: 0.9987694025039673
Epoch 140, training loss: 879.6284790039062 = 0.9900431036949158 + 100.0 * 8.786384582519531
Epoch 140, val loss: 0.9902680516242981
Epoch 150, training loss: 875.2211303710938 = 0.9811814427375793 + 100.0 * 8.742399215698242
Epoch 150, val loss: 0.9817685484886169
Epoch 160, training loss: 871.7442626953125 = 0.9723387360572815 + 100.0 * 8.707718849182129
Epoch 160, val loss: 0.9731818437576294
Epoch 170, training loss: 868.9893798828125 = 0.962446928024292 + 100.0 * 8.680269241333008
Epoch 170, val loss: 0.9633446931838989
Epoch 180, training loss: 866.3697509765625 = 0.9512672424316406 + 100.0 * 8.654184341430664
Epoch 180, val loss: 0.9523674845695496
Epoch 190, training loss: 864.410888671875 = 0.9393643140792847 + 100.0 * 8.63471508026123
Epoch 190, val loss: 0.9407935738563538
Epoch 200, training loss: 862.417724609375 = 0.9267028570175171 + 100.0 * 8.614910125732422
Epoch 200, val loss: 0.9286137223243713
Epoch 210, training loss: 860.8589477539062 = 0.9131108522415161 + 100.0 * 8.599458694458008
Epoch 210, val loss: 0.9153366088867188
Epoch 220, training loss: 860.3163452148438 = 0.8984271883964539 + 100.0 * 8.594179153442383
Epoch 220, val loss: 0.9010242223739624
Epoch 230, training loss: 858.4796752929688 = 0.8826533555984497 + 100.0 * 8.575970649719238
Epoch 230, val loss: 0.885870099067688
Epoch 240, training loss: 857.4050903320312 = 0.8663623929023743 + 100.0 * 8.565387725830078
Epoch 240, val loss: 0.8702235221862793
Epoch 250, training loss: 856.41796875 = 0.8496689200401306 + 100.0 * 8.555683135986328
Epoch 250, val loss: 0.8542454838752747
Epoch 260, training loss: 856.8650512695312 = 0.8326813578605652 + 100.0 * 8.560323715209961
Epoch 260, val loss: 0.837990403175354
Epoch 270, training loss: 854.6246337890625 = 0.815277099609375 + 100.0 * 8.538093566894531
Epoch 270, val loss: 0.8213379383087158
Epoch 280, training loss: 853.7068481445312 = 0.7981584072113037 + 100.0 * 8.52908706665039
Epoch 280, val loss: 0.8050913214683533
Epoch 290, training loss: 852.8897094726562 = 0.7812736630439758 + 100.0 * 8.521084785461426
Epoch 290, val loss: 0.7891286015510559
Epoch 300, training loss: 852.2644653320312 = 0.764638364315033 + 100.0 * 8.514998435974121
Epoch 300, val loss: 0.7734932899475098
Epoch 310, training loss: 851.8798217773438 = 0.7481668591499329 + 100.0 * 8.511316299438477
Epoch 310, val loss: 0.7578897476196289
Epoch 320, training loss: 851.2228393554688 = 0.73222815990448 + 100.0 * 8.504905700683594
Epoch 320, val loss: 0.7430983185768127
Epoch 330, training loss: 850.6381225585938 = 0.7171689867973328 + 100.0 * 8.4992094039917
Epoch 330, val loss: 0.7291517853736877
Epoch 340, training loss: 850.1808471679688 = 0.7028272747993469 + 100.0 * 8.494780540466309
Epoch 340, val loss: 0.7158961296081543
Epoch 350, training loss: 849.7655029296875 = 0.689235270023346 + 100.0 * 8.490762710571289
Epoch 350, val loss: 0.7034658789634705
Epoch 360, training loss: 850.111572265625 = 0.6764379143714905 + 100.0 * 8.494351387023926
Epoch 360, val loss: 0.6918447017669678
Epoch 370, training loss: 849.3955078125 = 0.6643677353858948 + 100.0 * 8.487311363220215
Epoch 370, val loss: 0.6808146238327026
Epoch 380, training loss: 848.9100952148438 = 0.6531698107719421 + 100.0 * 8.482568740844727
Epoch 380, val loss: 0.6710538864135742
Epoch 390, training loss: 848.4620971679688 = 0.6429571509361267 + 100.0 * 8.478191375732422
Epoch 390, val loss: 0.6618985533714294
Epoch 400, training loss: 848.052001953125 = 0.6334760189056396 + 100.0 * 8.4741849899292
Epoch 400, val loss: 0.6536718606948853
Epoch 410, training loss: 847.7442016601562 = 0.6247197985649109 + 100.0 * 8.471195220947266
Epoch 410, val loss: 0.6462149620056152
Epoch 420, training loss: 847.52685546875 = 0.6164774894714355 + 100.0 * 8.469103813171387
Epoch 420, val loss: 0.638975203037262
Epoch 430, training loss: 847.2613525390625 = 0.6088362336158752 + 100.0 * 8.466525077819824
Epoch 430, val loss: 0.6327454447746277
Epoch 440, training loss: 846.9454345703125 = 0.6020194292068481 + 100.0 * 8.463434219360352
Epoch 440, val loss: 0.627255380153656
Epoch 450, training loss: 846.5765991210938 = 0.5957455635070801 + 100.0 * 8.459808349609375
Epoch 450, val loss: 0.6222978830337524
Epoch 460, training loss: 846.3233032226562 = 0.5899559259414673 + 100.0 * 8.4573335647583
Epoch 460, val loss: 0.6178627014160156
Epoch 470, training loss: 846.1085815429688 = 0.5845057964324951 + 100.0 * 8.455240249633789
Epoch 470, val loss: 0.6136675477027893
Epoch 480, training loss: 845.8582153320312 = 0.5794456601142883 + 100.0 * 8.452787399291992
Epoch 480, val loss: 0.6100773215293884
Epoch 490, training loss: 845.574951171875 = 0.5748702883720398 + 100.0 * 8.450000762939453
Epoch 490, val loss: 0.6069663166999817
Epoch 500, training loss: 845.2711181640625 = 0.5706455707550049 + 100.0 * 8.447005271911621
Epoch 500, val loss: 0.6040409803390503
Epoch 510, training loss: 845.5170288085938 = 0.5666555166244507 + 100.0 * 8.449503898620605
Epoch 510, val loss: 0.601347029209137
Epoch 520, training loss: 844.972900390625 = 0.5628634691238403 + 100.0 * 8.444100379943848
Epoch 520, val loss: 0.5990853905677795
Epoch 530, training loss: 844.5567626953125 = 0.5594388246536255 + 100.0 * 8.439972877502441
Epoch 530, val loss: 0.5970243811607361
Epoch 540, training loss: 844.5166625976562 = 0.5562426447868347 + 100.0 * 8.439604759216309
Epoch 540, val loss: 0.595116913318634
Epoch 550, training loss: 845.06640625 = 0.5530890822410583 + 100.0 * 8.445133209228516
Epoch 550, val loss: 0.5933712720870972
Epoch 560, training loss: 844.0055541992188 = 0.5501487255096436 + 100.0 * 8.434554100036621
Epoch 560, val loss: 0.5919532179832458
Epoch 570, training loss: 843.7860107421875 = 0.5475067496299744 + 100.0 * 8.432385444641113
Epoch 570, val loss: 0.5904359221458435
Epoch 580, training loss: 843.5284423828125 = 0.5450107455253601 + 100.0 * 8.429834365844727
Epoch 580, val loss: 0.5894225835800171
Epoch 590, training loss: 843.601318359375 = 0.5426561236381531 + 100.0 * 8.430586814880371
Epoch 590, val loss: 0.5884565711021423
Epoch 600, training loss: 843.1377563476562 = 0.5403069257736206 + 100.0 * 8.42597484588623
Epoch 600, val loss: 0.5871163010597229
Epoch 610, training loss: 843.0265502929688 = 0.5381270051002502 + 100.0 * 8.424883842468262
Epoch 610, val loss: 0.5864204168319702
Epoch 620, training loss: 842.7913818359375 = 0.5360970497131348 + 100.0 * 8.422553062438965
Epoch 620, val loss: 0.5853931307792664
Epoch 630, training loss: 842.591796875 = 0.5341530442237854 + 100.0 * 8.420576095581055
Epoch 630, val loss: 0.5848148465156555
Epoch 640, training loss: 843.1567993164062 = 0.5322467684745789 + 100.0 * 8.42624568939209
Epoch 640, val loss: 0.5841394066810608
Epoch 650, training loss: 842.4336547851562 = 0.5303614735603333 + 100.0 * 8.41903305053711
Epoch 650, val loss: 0.5834133625030518
Epoch 660, training loss: 842.1841430664062 = 0.5286092758178711 + 100.0 * 8.416555404663086
Epoch 660, val loss: 0.582491397857666
Epoch 670, training loss: 842.3317260742188 = 0.5269158482551575 + 100.0 * 8.418047904968262
Epoch 670, val loss: 0.5818201899528503
Epoch 680, training loss: 841.7787475585938 = 0.5252439379692078 + 100.0 * 8.412534713745117
Epoch 680, val loss: 0.5814117789268494
Epoch 690, training loss: 841.668701171875 = 0.5236684083938599 + 100.0 * 8.411450386047363
Epoch 690, val loss: 0.5810064077377319
Epoch 700, training loss: 841.824462890625 = 0.5221260786056519 + 100.0 * 8.413022994995117
Epoch 700, val loss: 0.5804710388183594
Epoch 710, training loss: 841.4775390625 = 0.5205960869789124 + 100.0 * 8.40956974029541
Epoch 710, val loss: 0.5797456502914429
Epoch 720, training loss: 841.5996704101562 = 0.5190747380256653 + 100.0 * 8.410805702209473
Epoch 720, val loss: 0.579225480556488
Epoch 730, training loss: 841.2164916992188 = 0.5175954103469849 + 100.0 * 8.406989097595215
Epoch 730, val loss: 0.5789702534675598
Epoch 740, training loss: 840.9669189453125 = 0.5161906480789185 + 100.0 * 8.404507637023926
Epoch 740, val loss: 0.5783891677856445
Epoch 750, training loss: 840.9930419921875 = 0.5148111581802368 + 100.0 * 8.40478229522705
Epoch 750, val loss: 0.5780343413352966
Epoch 760, training loss: 841.0510864257812 = 0.5133979320526123 + 100.0 * 8.405376434326172
Epoch 760, val loss: 0.577642023563385
Epoch 770, training loss: 840.652587890625 = 0.5119604468345642 + 100.0 * 8.401406288146973
Epoch 770, val loss: 0.577025294303894
Epoch 780, training loss: 840.5228881835938 = 0.5106044411659241 + 100.0 * 8.40012264251709
Epoch 780, val loss: 0.576491117477417
Epoch 790, training loss: 840.42919921875 = 0.5092933773994446 + 100.0 * 8.399199485778809
Epoch 790, val loss: 0.576044499874115
Epoch 800, training loss: 840.76513671875 = 0.5079924464225769 + 100.0 * 8.402571678161621
Epoch 800, val loss: 0.5754110217094421
Epoch 810, training loss: 841.0235595703125 = 0.5065664052963257 + 100.0 * 8.405169486999512
Epoch 810, val loss: 0.575043797492981
Epoch 820, training loss: 840.2125854492188 = 0.5051806569099426 + 100.0 * 8.397073745727539
Epoch 820, val loss: 0.5747990012168884
Epoch 830, training loss: 840.0448608398438 = 0.5038931369781494 + 100.0 * 8.39540958404541
Epoch 830, val loss: 0.574124813079834
Epoch 840, training loss: 839.9037475585938 = 0.5026248693466187 + 100.0 * 8.394011497497559
Epoch 840, val loss: 0.5738059878349304
Epoch 850, training loss: 839.9696044921875 = 0.5013731718063354 + 100.0 * 8.394682884216309
Epoch 850, val loss: 0.5732342600822449
Epoch 860, training loss: 840.0195922851562 = 0.5000342130661011 + 100.0 * 8.395195007324219
Epoch 860, val loss: 0.572792649269104
Epoch 870, training loss: 840.1024780273438 = 0.49866873025894165 + 100.0 * 8.396038055419922
Epoch 870, val loss: 0.572339653968811
Epoch 880, training loss: 839.6234741210938 = 0.49735110998153687 + 100.0 * 8.391261100769043
Epoch 880, val loss: 0.5717923045158386
Epoch 890, training loss: 839.5016479492188 = 0.49607837200164795 + 100.0 * 8.390055656433105
Epoch 890, val loss: 0.5715157985687256
Epoch 900, training loss: 839.5885009765625 = 0.494826078414917 + 100.0 * 8.390936851501465
Epoch 900, val loss: 0.5711985230445862
Epoch 910, training loss: 839.5054931640625 = 0.49352410435676575 + 100.0 * 8.390119552612305
Epoch 910, val loss: 0.5706378817558289
Epoch 920, training loss: 839.3299560546875 = 0.49224188923835754 + 100.0 * 8.38837718963623
Epoch 920, val loss: 0.5699816942214966
Epoch 930, training loss: 839.2520141601562 = 0.4909774363040924 + 100.0 * 8.38761043548584
Epoch 930, val loss: 0.569633424282074
Epoch 940, training loss: 839.302490234375 = 0.4897140562534332 + 100.0 * 8.388128280639648
Epoch 940, val loss: 0.5690670609474182
Epoch 950, training loss: 839.0485229492188 = 0.4883914887905121 + 100.0 * 8.385601043701172
Epoch 950, val loss: 0.5687544345855713
Epoch 960, training loss: 839.3193359375 = 0.48708662390708923 + 100.0 * 8.388322830200195
Epoch 960, val loss: 0.5683686137199402
Epoch 970, training loss: 839.0369262695312 = 0.485773503780365 + 100.0 * 8.38551139831543
Epoch 970, val loss: 0.5675517916679382
Epoch 980, training loss: 838.8126831054688 = 0.48447251319885254 + 100.0 * 8.383281707763672
Epoch 980, val loss: 0.5671387910842896
Epoch 990, training loss: 839.5971069335938 = 0.48319175839424133 + 100.0 * 8.391139030456543
Epoch 990, val loss: 0.5663779973983765
Epoch 1000, training loss: 838.8695678710938 = 0.4817938506603241 + 100.0 * 8.383877754211426
Epoch 1000, val loss: 0.5663608908653259
Epoch 1010, training loss: 838.5963745117188 = 0.480487197637558 + 100.0 * 8.381158828735352
Epoch 1010, val loss: 0.5655322670936584
Epoch 1020, training loss: 838.458984375 = 0.47920194268226624 + 100.0 * 8.37979793548584
Epoch 1020, val loss: 0.5651189684867859
Epoch 1030, training loss: 838.5801391601562 = 0.47792115807533264 + 100.0 * 8.381022453308105
Epoch 1030, val loss: 0.5646337866783142
Epoch 1040, training loss: 838.3433837890625 = 0.47651609778404236 + 100.0 * 8.378668785095215
Epoch 1040, val loss: 0.564247190952301
Epoch 1050, training loss: 838.3756713867188 = 0.47514092922210693 + 100.0 * 8.379005432128906
Epoch 1050, val loss: 0.5634400248527527
Epoch 1060, training loss: 838.16943359375 = 0.4738014042377472 + 100.0 * 8.37695598602295
Epoch 1060, val loss: 0.5629612803459167
Epoch 1070, training loss: 838.1969604492188 = 0.4724891185760498 + 100.0 * 8.37724494934082
Epoch 1070, val loss: 0.5622928142547607
Epoch 1080, training loss: 838.6460571289062 = 0.4710630178451538 + 100.0 * 8.381750106811523
Epoch 1080, val loss: 0.5617682933807373
Epoch 1090, training loss: 837.97412109375 = 0.46956968307495117 + 100.0 * 8.375045776367188
Epoch 1090, val loss: 0.5611152648925781
Epoch 1100, training loss: 837.9754028320312 = 0.46815887093544006 + 100.0 * 8.375072479248047
Epoch 1100, val loss: 0.5608472228050232
Epoch 1110, training loss: 837.867431640625 = 0.4667922258377075 + 100.0 * 8.374006271362305
Epoch 1110, val loss: 0.5601856708526611
Epoch 1120, training loss: 838.266845703125 = 0.46539556980133057 + 100.0 * 8.37801456451416
Epoch 1120, val loss: 0.5596718788146973
Epoch 1130, training loss: 837.7706298828125 = 0.4639030396938324 + 100.0 * 8.373066902160645
Epoch 1130, val loss: 0.5590044260025024
Epoch 1140, training loss: 837.6651000976562 = 0.46245524287223816 + 100.0 * 8.372026443481445
Epoch 1140, val loss: 0.5583847165107727
Epoch 1150, training loss: 837.6484985351562 = 0.4610171616077423 + 100.0 * 8.371874809265137
Epoch 1150, val loss: 0.5577967762947083
Epoch 1160, training loss: 837.9310302734375 = 0.4595341682434082 + 100.0 * 8.374714851379395
Epoch 1160, val loss: 0.5573164224624634
Epoch 1170, training loss: 837.5938110351562 = 0.4579944312572479 + 100.0 * 8.371357917785645
Epoch 1170, val loss: 0.5566765069961548
Epoch 1180, training loss: 837.4594116210938 = 0.4564917981624603 + 100.0 * 8.37002944946289
Epoch 1180, val loss: 0.555864155292511
Epoch 1190, training loss: 837.4838256835938 = 0.4550001323223114 + 100.0 * 8.370287895202637
Epoch 1190, val loss: 0.5552030205726624
Epoch 1200, training loss: 837.4168090820312 = 0.45344045758247375 + 100.0 * 8.369633674621582
Epoch 1200, val loss: 0.5547324419021606
Epoch 1210, training loss: 837.2612915039062 = 0.45186111330986023 + 100.0 * 8.368094444274902
Epoch 1210, val loss: 0.5541433691978455
Epoch 1220, training loss: 837.6227416992188 = 0.4503178000450134 + 100.0 * 8.371724128723145
Epoch 1220, val loss: 0.553417980670929
Epoch 1230, training loss: 837.818603515625 = 0.4486262798309326 + 100.0 * 8.373700141906738
Epoch 1230, val loss: 0.5531005263328552
Epoch 1240, training loss: 837.2335815429688 = 0.4469531178474426 + 100.0 * 8.367866516113281
Epoch 1240, val loss: 0.5519450902938843
Epoch 1250, training loss: 837.0477905273438 = 0.4453522861003876 + 100.0 * 8.366024017333984
Epoch 1250, val loss: 0.5514392256736755
Epoch 1260, training loss: 836.9733276367188 = 0.4437842071056366 + 100.0 * 8.36529541015625
Epoch 1260, val loss: 0.5508831739425659
Epoch 1270, training loss: 836.900146484375 = 0.44220614433288574 + 100.0 * 8.364579200744629
Epoch 1270, val loss: 0.5502974987030029
Epoch 1280, training loss: 837.1708984375 = 0.4405994117259979 + 100.0 * 8.367302894592285
Epoch 1280, val loss: 0.5497024655342102
Epoch 1290, training loss: 836.9547729492188 = 0.43887001276016235 + 100.0 * 8.365159034729004
Epoch 1290, val loss: 0.5492881536483765
Epoch 1300, training loss: 836.8334350585938 = 0.43714508414268494 + 100.0 * 8.36396312713623
Epoch 1300, val loss: 0.5482385754585266
Epoch 1310, training loss: 836.7636108398438 = 0.4354870617389679 + 100.0 * 8.36328125
Epoch 1310, val loss: 0.5476191639900208
Epoch 1320, training loss: 836.7277221679688 = 0.4338552951812744 + 100.0 * 8.36293888092041
Epoch 1320, val loss: 0.5468617677688599
Epoch 1330, training loss: 837.3558959960938 = 0.43220254778862 + 100.0 * 8.369236946105957
Epoch 1330, val loss: 0.5460513234138489
Epoch 1340, training loss: 836.9467163085938 = 0.43041160702705383 + 100.0 * 8.36516284942627
Epoch 1340, val loss: 0.5461865663528442
Epoch 1350, training loss: 836.6388549804688 = 0.4287038743495941 + 100.0 * 8.362101554870605
Epoch 1350, val loss: 0.5448974370956421
Epoch 1360, training loss: 836.5492553710938 = 0.42701461911201477 + 100.0 * 8.361222267150879
Epoch 1360, val loss: 0.544655442237854
Epoch 1370, training loss: 836.620361328125 = 0.425336092710495 + 100.0 * 8.361949920654297
Epoch 1370, val loss: 0.5439322590827942
Epoch 1380, training loss: 836.7472534179688 = 0.42359253764152527 + 100.0 * 8.363236427307129
Epoch 1380, val loss: 0.5436531901359558
Epoch 1390, training loss: 836.61376953125 = 0.42182210087776184 + 100.0 * 8.361919403076172
Epoch 1390, val loss: 0.5428795218467712
Epoch 1400, training loss: 836.3587646484375 = 0.4200701117515564 + 100.0 * 8.359387397766113
Epoch 1400, val loss: 0.5421571731567383
Epoch 1410, training loss: 836.333740234375 = 0.4183466136455536 + 100.0 * 8.359153747558594
Epoch 1410, val loss: 0.5415138602256775
Epoch 1420, training loss: 836.3712768554688 = 0.41662806272506714 + 100.0 * 8.359546661376953
Epoch 1420, val loss: 0.540790319442749
Epoch 1430, training loss: 836.4010009765625 = 0.41482141613960266 + 100.0 * 8.359862327575684
Epoch 1430, val loss: 0.5403228998184204
Epoch 1440, training loss: 836.2613525390625 = 0.4129984676837921 + 100.0 * 8.35848331451416
Epoch 1440, val loss: 0.5395764708518982
Epoch 1450, training loss: 836.4646606445312 = 0.41121357679367065 + 100.0 * 8.36053466796875
Epoch 1450, val loss: 0.5389506220817566
Epoch 1460, training loss: 836.11572265625 = 0.4094184637069702 + 100.0 * 8.357063293457031
Epoch 1460, val loss: 0.5386315584182739
Epoch 1470, training loss: 836.0661010742188 = 0.4076542258262634 + 100.0 * 8.356584548950195
Epoch 1470, val loss: 0.5383108854293823
Epoch 1480, training loss: 836.2704467773438 = 0.4058941602706909 + 100.0 * 8.35864543914795
Epoch 1480, val loss: 0.5379811525344849
Epoch 1490, training loss: 836.3036499023438 = 0.4040885269641876 + 100.0 * 8.35899543762207
Epoch 1490, val loss: 0.5366663336753845
Epoch 1500, training loss: 836.038330078125 = 0.40222516655921936 + 100.0 * 8.356361389160156
Epoch 1500, val loss: 0.5366452932357788
Epoch 1510, training loss: 836.12353515625 = 0.4004078507423401 + 100.0 * 8.357231140136719
Epoch 1510, val loss: 0.535966157913208
Epoch 1520, training loss: 835.9193725585938 = 0.3986068069934845 + 100.0 * 8.355207443237305
Epoch 1520, val loss: 0.5356685519218445
Epoch 1530, training loss: 835.8287963867188 = 0.3968392312526703 + 100.0 * 8.35431957244873
Epoch 1530, val loss: 0.535187840461731
Epoch 1540, training loss: 836.0499267578125 = 0.39507219195365906 + 100.0 * 8.356548309326172
Epoch 1540, val loss: 0.5350594520568848
Epoch 1550, training loss: 835.9097900390625 = 0.3932575285434723 + 100.0 * 8.355165481567383
Epoch 1550, val loss: 0.5342528820037842
Epoch 1560, training loss: 836.2291870117188 = 0.3914225995540619 + 100.0 * 8.358377456665039
Epoch 1560, val loss: 0.5341725945472717
Epoch 1570, training loss: 835.77880859375 = 0.38957881927490234 + 100.0 * 8.35389232635498
Epoch 1570, val loss: 0.5329355001449585
Epoch 1580, training loss: 835.646728515625 = 0.3877756893634796 + 100.0 * 8.35258960723877
Epoch 1580, val loss: 0.5330662727355957
Epoch 1590, training loss: 835.6035766601562 = 0.38600102066993713 + 100.0 * 8.35217571258545
Epoch 1590, val loss: 0.532509982585907
Epoch 1600, training loss: 835.5799560546875 = 0.38422471284866333 + 100.0 * 8.351957321166992
Epoch 1600, val loss: 0.5320577621459961
Epoch 1610, training loss: 836.3899536132812 = 0.38244491815567017 + 100.0 * 8.360074996948242
Epoch 1610, val loss: 0.5313730835914612
Epoch 1620, training loss: 836.0604858398438 = 0.3805338144302368 + 100.0 * 8.356799125671387
Epoch 1620, val loss: 0.5313900709152222
Epoch 1630, training loss: 835.49169921875 = 0.3786691129207611 + 100.0 * 8.351130485534668
Epoch 1630, val loss: 0.5308188199996948
Epoch 1640, training loss: 835.4436645507812 = 0.37686747312545776 + 100.0 * 8.350667953491211
Epoch 1640, val loss: 0.5304584503173828
Epoch 1650, training loss: 835.4191284179688 = 0.3750899136066437 + 100.0 * 8.350440979003906
Epoch 1650, val loss: 0.5304231643676758
Epoch 1660, training loss: 835.7494506835938 = 0.3733132481575012 + 100.0 * 8.353761672973633
Epoch 1660, val loss: 0.5300428867340088
Epoch 1670, training loss: 835.3433837890625 = 0.371452659368515 + 100.0 * 8.349719047546387
Epoch 1670, val loss: 0.5294064283370972
Epoch 1680, training loss: 835.4263916015625 = 0.36963415145874023 + 100.0 * 8.350567817687988
Epoch 1680, val loss: 0.5289515256881714
Epoch 1690, training loss: 835.452880859375 = 0.36783936619758606 + 100.0 * 8.350850105285645
Epoch 1690, val loss: 0.528594434261322
Epoch 1700, training loss: 835.2283325195312 = 0.3660100996494293 + 100.0 * 8.348623275756836
Epoch 1700, val loss: 0.5289345383644104
Epoch 1710, training loss: 835.2083740234375 = 0.364240825176239 + 100.0 * 8.348441123962402
Epoch 1710, val loss: 0.5289018750190735
Epoch 1720, training loss: 835.6437377929688 = 0.36249008774757385 + 100.0 * 8.352812767028809
Epoch 1720, val loss: 0.5292772650718689
Epoch 1730, training loss: 835.5761108398438 = 0.3606414794921875 + 100.0 * 8.352154731750488
Epoch 1730, val loss: 0.5278750061988831
Epoch 1740, training loss: 835.28076171875 = 0.35879209637641907 + 100.0 * 8.349220275878906
Epoch 1740, val loss: 0.5281080007553101
Epoch 1750, training loss: 835.1141357421875 = 0.35701286792755127 + 100.0 * 8.34757137298584
Epoch 1750, val loss: 0.5278359055519104
Epoch 1760, training loss: 835.1072998046875 = 0.3552716374397278 + 100.0 * 8.347519874572754
Epoch 1760, val loss: 0.5274164080619812
Epoch 1770, training loss: 835.3456420898438 = 0.35352736711502075 + 100.0 * 8.349921226501465
Epoch 1770, val loss: 0.5272071361541748
Epoch 1780, training loss: 835.0931396484375 = 0.35172033309936523 + 100.0 * 8.347414016723633
Epoch 1780, val loss: 0.5275813937187195
Epoch 1790, training loss: 835.2442016601562 = 0.34994226694107056 + 100.0 * 8.348942756652832
Epoch 1790, val loss: 0.5274385809898376
Epoch 1800, training loss: 834.9953002929688 = 0.3481726348400116 + 100.0 * 8.346471786499023
Epoch 1800, val loss: 0.5270025730133057
Epoch 1810, training loss: 834.900390625 = 0.3463980257511139 + 100.0 * 8.345540046691895
Epoch 1810, val loss: 0.5274851322174072
Epoch 1820, training loss: 835.1282958984375 = 0.34465500712394714 + 100.0 * 8.3478364944458
Epoch 1820, val loss: 0.5275872349739075
Epoch 1830, training loss: 834.9729614257812 = 0.3428574800491333 + 100.0 * 8.346301078796387
Epoch 1830, val loss: 0.5270612835884094
Epoch 1840, training loss: 834.9891967773438 = 0.34108108282089233 + 100.0 * 8.346481323242188
Epoch 1840, val loss: 0.5268998146057129
Epoch 1850, training loss: 835.09814453125 = 0.3393212854862213 + 100.0 * 8.347588539123535
Epoch 1850, val loss: 0.5269359350204468
Epoch 1860, training loss: 835.2786254882812 = 0.33754777908325195 + 100.0 * 8.349411010742188
Epoch 1860, val loss: 0.5270770788192749
Epoch 1870, training loss: 834.779541015625 = 0.3357698619365692 + 100.0 * 8.344437599182129
Epoch 1870, val loss: 0.5273041129112244
Epoch 1880, training loss: 834.7149658203125 = 0.334041953086853 + 100.0 * 8.343809127807617
Epoch 1880, val loss: 0.5273084044456482
Epoch 1890, training loss: 834.6656494140625 = 0.33232566714286804 + 100.0 * 8.34333324432373
Epoch 1890, val loss: 0.5275025367736816
Epoch 1900, training loss: 834.9917602539062 = 0.33061927556991577 + 100.0 * 8.346611022949219
Epoch 1900, val loss: 0.5274597406387329
Epoch 1910, training loss: 834.7511596679688 = 0.32886627316474915 + 100.0 * 8.344223022460938
Epoch 1910, val loss: 0.5276210904121399
Epoch 1920, training loss: 834.6909790039062 = 0.3271332085132599 + 100.0 * 8.34363842010498
Epoch 1920, val loss: 0.5274513363838196
Epoch 1930, training loss: 834.8689575195312 = 0.32541000843048096 + 100.0 * 8.34543514251709
Epoch 1930, val loss: 0.528178334236145
Epoch 1940, training loss: 834.770751953125 = 0.3236711025238037 + 100.0 * 8.344470977783203
Epoch 1940, val loss: 0.5284337997436523
Epoch 1950, training loss: 834.6586303710938 = 0.32193729281425476 + 100.0 * 8.343366622924805
Epoch 1950, val loss: 0.5285730957984924
Epoch 1960, training loss: 834.5642700195312 = 0.32020893692970276 + 100.0 * 8.342440605163574
Epoch 1960, val loss: 0.5280983448028564
Epoch 1970, training loss: 834.5079345703125 = 0.3185203969478607 + 100.0 * 8.341894149780273
Epoch 1970, val loss: 0.5284799933433533
Epoch 1980, training loss: 834.7355346679688 = 0.3168400526046753 + 100.0 * 8.344186782836914
Epoch 1980, val loss: 0.5287840962409973
Epoch 1990, training loss: 834.51171875 = 0.31512752175331116 + 100.0 * 8.341965675354004
Epoch 1990, val loss: 0.5285146236419678
Epoch 2000, training loss: 834.7162475585938 = 0.3134879469871521 + 100.0 * 8.344027519226074
Epoch 2000, val loss: 0.5280041098594666
Epoch 2010, training loss: 834.62646484375 = 0.31173989176750183 + 100.0 * 8.343147277832031
Epoch 2010, val loss: 0.5290114879608154
Epoch 2020, training loss: 834.3905639648438 = 0.31005027890205383 + 100.0 * 8.340805053710938
Epoch 2020, val loss: 0.5298019051551819
Epoch 2030, training loss: 834.3826904296875 = 0.3083902597427368 + 100.0 * 8.340743064880371
Epoch 2030, val loss: 0.5299835205078125
Epoch 2040, training loss: 834.8058471679688 = 0.30674347281455994 + 100.0 * 8.344990730285645
Epoch 2040, val loss: 0.5305510759353638
Epoch 2050, training loss: 834.4111328125 = 0.3050435781478882 + 100.0 * 8.341060638427734
Epoch 2050, val loss: 0.5303313732147217
Epoch 2060, training loss: 834.4242553710938 = 0.30338364839553833 + 100.0 * 8.341208457946777
Epoch 2060, val loss: 0.5306674242019653
Epoch 2070, training loss: 834.4104614257812 = 0.30172520875930786 + 100.0 * 8.341087341308594
Epoch 2070, val loss: 0.5311444401741028
Epoch 2080, training loss: 834.3363037109375 = 0.3000777065753937 + 100.0 * 8.340362548828125
Epoch 2080, val loss: 0.5318869352340698
Epoch 2090, training loss: 834.2301635742188 = 0.29841622710227966 + 100.0 * 8.339317321777344
Epoch 2090, val loss: 0.5319957733154297
Epoch 2100, training loss: 834.3182983398438 = 0.2967750132083893 + 100.0 * 8.340215682983398
Epoch 2100, val loss: 0.5323600769042969
Epoch 2110, training loss: 834.4583740234375 = 0.2951231896877289 + 100.0 * 8.341632843017578
Epoch 2110, val loss: 0.5328474044799805
Epoch 2120, training loss: 834.4033203125 = 0.2934519052505493 + 100.0 * 8.34109878540039
Epoch 2120, val loss: 0.5329710841178894
Epoch 2130, training loss: 834.2743530273438 = 0.2918027639389038 + 100.0 * 8.339825630187988
Epoch 2130, val loss: 0.5333510041236877
Epoch 2140, training loss: 834.449462890625 = 0.2901693284511566 + 100.0 * 8.341592788696289
Epoch 2140, val loss: 0.5333970785140991
Epoch 2150, training loss: 834.2437133789062 = 0.288531631231308 + 100.0 * 8.33955192565918
Epoch 2150, val loss: 0.5336710214614868
Epoch 2160, training loss: 834.1188354492188 = 0.2868712544441223 + 100.0 * 8.338319778442383
Epoch 2160, val loss: 0.534927487373352
Epoch 2170, training loss: 834.0079345703125 = 0.2852519452571869 + 100.0 * 8.337226867675781
Epoch 2170, val loss: 0.5352169871330261
Epoch 2180, training loss: 834.0891723632812 = 0.2836453318595886 + 100.0 * 8.338055610656738
Epoch 2180, val loss: 0.5356761813163757
Epoch 2190, training loss: 834.703857421875 = 0.28205910325050354 + 100.0 * 8.344218254089355
Epoch 2190, val loss: 0.5355429649353027
Epoch 2200, training loss: 834.207763671875 = 0.28038254380226135 + 100.0 * 8.339273452758789
Epoch 2200, val loss: 0.5364872217178345
Epoch 2210, training loss: 834.0402221679688 = 0.27875179052352905 + 100.0 * 8.337615013122559
Epoch 2210, val loss: 0.5369554162025452
Epoch 2220, training loss: 834.2364501953125 = 0.27718424797058105 + 100.0 * 8.339592933654785
Epoch 2220, val loss: 0.5368936657905579
Epoch 2230, training loss: 834.0579223632812 = 0.27554118633270264 + 100.0 * 8.337823867797852
Epoch 2230, val loss: 0.5381931662559509
Epoch 2240, training loss: 833.9322509765625 = 0.2739349901676178 + 100.0 * 8.336583137512207
Epoch 2240, val loss: 0.5389552712440491
Epoch 2250, training loss: 833.8507080078125 = 0.27233797311782837 + 100.0 * 8.335783958435059
Epoch 2250, val loss: 0.5395135879516602
Epoch 2260, training loss: 833.8661499023438 = 0.2707536220550537 + 100.0 * 8.335953712463379
Epoch 2260, val loss: 0.5404031872749329
Epoch 2270, training loss: 834.4906616210938 = 0.26922807097435 + 100.0 * 8.342214584350586
Epoch 2270, val loss: 0.5419467687606812
Epoch 2280, training loss: 834.1845092773438 = 0.26755213737487793 + 100.0 * 8.3391695022583
Epoch 2280, val loss: 0.5411977171897888
Epoch 2290, training loss: 833.9199829101562 = 0.2659587860107422 + 100.0 * 8.336540222167969
Epoch 2290, val loss: 0.5421335697174072
Epoch 2300, training loss: 833.8423461914062 = 0.2643885612487793 + 100.0 * 8.335779190063477
Epoch 2300, val loss: 0.5429034233093262
Epoch 2310, training loss: 834.0955200195312 = 0.2628558278083801 + 100.0 * 8.338326454162598
Epoch 2310, val loss: 0.5438104867935181
Epoch 2320, training loss: 833.8843994140625 = 0.26126769185066223 + 100.0 * 8.336231231689453
Epoch 2320, val loss: 0.5436394214630127
Epoch 2330, training loss: 833.8225708007812 = 0.25973203778266907 + 100.0 * 8.335628509521484
Epoch 2330, val loss: 0.5437971949577332
Epoch 2340, training loss: 833.6677856445312 = 0.2581632435321808 + 100.0 * 8.33409595489502
Epoch 2340, val loss: 0.5452170968055725
Epoch 2350, training loss: 833.7239379882812 = 0.25662994384765625 + 100.0 * 8.334672927856445
Epoch 2350, val loss: 0.5459534525871277
Epoch 2360, training loss: 834.0032348632812 = 0.25509339570999146 + 100.0 * 8.337481498718262
Epoch 2360, val loss: 0.5467648506164551
Epoch 2370, training loss: 833.9612426757812 = 0.2535274028778076 + 100.0 * 8.337077140808105
Epoch 2370, val loss: 0.5467990636825562
Epoch 2380, training loss: 833.7295532226562 = 0.25196635723114014 + 100.0 * 8.334775924682617
Epoch 2380, val loss: 0.5474150776863098
Epoch 2390, training loss: 833.6724243164062 = 0.2504081428050995 + 100.0 * 8.334219932556152
Epoch 2390, val loss: 0.5483595132827759
Epoch 2400, training loss: 833.8861083984375 = 0.2489013671875 + 100.0 * 8.336372375488281
Epoch 2400, val loss: 0.5485542416572571
Epoch 2410, training loss: 833.7173461914062 = 0.24731610715389252 + 100.0 * 8.334700584411621
Epoch 2410, val loss: 0.5506891012191772
Epoch 2420, training loss: 833.9888916015625 = 0.24578779935836792 + 100.0 * 8.337430953979492
Epoch 2420, val loss: 0.5512194037437439
Epoch 2430, training loss: 833.6300048828125 = 0.24423934519290924 + 100.0 * 8.333857536315918
Epoch 2430, val loss: 0.551283597946167
Epoch 2440, training loss: 833.5374755859375 = 0.2427022010087967 + 100.0 * 8.332947731018066
Epoch 2440, val loss: 0.552433431148529
Epoch 2450, training loss: 833.6231689453125 = 0.2412259578704834 + 100.0 * 8.333819389343262
Epoch 2450, val loss: 0.552523136138916
Epoch 2460, training loss: 833.6403198242188 = 0.2396930307149887 + 100.0 * 8.334006309509277
Epoch 2460, val loss: 0.5538752675056458
Epoch 2470, training loss: 833.7921142578125 = 0.23820370435714722 + 100.0 * 8.335538864135742
Epoch 2470, val loss: 0.5559210777282715
Epoch 2480, training loss: 833.6819458007812 = 0.23668146133422852 + 100.0 * 8.334452629089355
Epoch 2480, val loss: 0.5560105443000793
Epoch 2490, training loss: 833.500732421875 = 0.23517361283302307 + 100.0 * 8.332655906677246
Epoch 2490, val loss: 0.5567377805709839
Epoch 2500, training loss: 833.4791870117188 = 0.23368559777736664 + 100.0 * 8.332454681396484
Epoch 2500, val loss: 0.5574393272399902
Epoch 2510, training loss: 833.4920654296875 = 0.23220661282539368 + 100.0 * 8.332598686218262
Epoch 2510, val loss: 0.5584415197372437
Epoch 2520, training loss: 833.7307739257812 = 0.2307494431734085 + 100.0 * 8.335000038146973
Epoch 2520, val loss: 0.5598185658454895
Epoch 2530, training loss: 833.3538818359375 = 0.2292356789112091 + 100.0 * 8.331246376037598
Epoch 2530, val loss: 0.5600601434707642
Epoch 2540, training loss: 833.4033203125 = 0.22776329517364502 + 100.0 * 8.331755638122559
Epoch 2540, val loss: 0.5607441067695618
Epoch 2550, training loss: 833.6025390625 = 0.22629870474338531 + 100.0 * 8.333762168884277
Epoch 2550, val loss: 0.5617459416389465
Epoch 2560, training loss: 833.5945434570312 = 0.22481529414653778 + 100.0 * 8.333697319030762
Epoch 2560, val loss: 0.5628865957260132
Epoch 2570, training loss: 833.4635620117188 = 0.22332043945789337 + 100.0 * 8.332402229309082
Epoch 2570, val loss: 0.5631465315818787
Epoch 2580, training loss: 833.5377197265625 = 0.22187624871730804 + 100.0 * 8.333158493041992
Epoch 2580, val loss: 0.5634274482727051
Epoch 2590, training loss: 833.4224853515625 = 0.22036725282669067 + 100.0 * 8.332021713256836
Epoch 2590, val loss: 0.5650440454483032
Epoch 2600, training loss: 833.2666625976562 = 0.21888896822929382 + 100.0 * 8.330477714538574
Epoch 2600, val loss: 0.5660943388938904
Epoch 2610, training loss: 833.3027954101562 = 0.21743877232074738 + 100.0 * 8.330853462219238
Epoch 2610, val loss: 0.5673637390136719
Epoch 2620, training loss: 833.6221923828125 = 0.21602272987365723 + 100.0 * 8.334061622619629
Epoch 2620, val loss: 0.5692797303199768
Epoch 2630, training loss: 833.92919921875 = 0.21453624963760376 + 100.0 * 8.337146759033203
Epoch 2630, val loss: 0.568571925163269
Epoch 2640, training loss: 833.4000854492188 = 0.21305035054683685 + 100.0 * 8.331870079040527
Epoch 2640, val loss: 0.5702175498008728
Epoch 2650, training loss: 833.2578735351562 = 0.21159182488918304 + 100.0 * 8.330462455749512
Epoch 2650, val loss: 0.5706442594528198
Epoch 2660, training loss: 833.1417846679688 = 0.21014414727687836 + 100.0 * 8.329316139221191
Epoch 2660, val loss: 0.5719454884529114
Epoch 2670, training loss: 833.1539916992188 = 0.20872195065021515 + 100.0 * 8.329452514648438
Epoch 2670, val loss: 0.573403537273407
Epoch 2680, training loss: 833.6762084960938 = 0.20741362869739532 + 100.0 * 8.334688186645508
Epoch 2680, val loss: 0.5758947134017944
Epoch 2690, training loss: 833.2904663085938 = 0.2058728188276291 + 100.0 * 8.330845832824707
Epoch 2690, val loss: 0.5739573836326599
Epoch 2700, training loss: 833.2348022460938 = 0.20444679260253906 + 100.0 * 8.330303192138672
Epoch 2700, val loss: 0.5763585567474365
Epoch 2710, training loss: 833.0726928710938 = 0.20303884148597717 + 100.0 * 8.328696250915527
Epoch 2710, val loss: 0.5765141248703003
Epoch 2720, training loss: 833.1045532226562 = 0.20165033638477325 + 100.0 * 8.329029083251953
Epoch 2720, val loss: 0.5777110457420349
Epoch 2730, training loss: 833.7932739257812 = 0.20027916133403778 + 100.0 * 8.335929870605469
Epoch 2730, val loss: 0.5786969661712646
Epoch 2740, training loss: 833.1828002929688 = 0.19886814057826996 + 100.0 * 8.329839706420898
Epoch 2740, val loss: 0.5799155831336975
Epoch 2750, training loss: 833.177978515625 = 0.19748272001743317 + 100.0 * 8.329804420471191
Epoch 2750, val loss: 0.5812587141990662
Epoch 2760, training loss: 833.423828125 = 0.1960996389389038 + 100.0 * 8.332277297973633
Epoch 2760, val loss: 0.5821534395217896
Epoch 2770, training loss: 832.99365234375 = 0.19470937550067902 + 100.0 * 8.32798957824707
Epoch 2770, val loss: 0.5833335518836975
Epoch 2780, training loss: 833.05517578125 = 0.19334100186824799 + 100.0 * 8.328618049621582
Epoch 2780, val loss: 0.5839259028434753
Epoch 2790, training loss: 833.0086669921875 = 0.1919654756784439 + 100.0 * 8.328166961669922
Epoch 2790, val loss: 0.5852563977241516
Epoch 2800, training loss: 833.8263549804688 = 0.19062605500221252 + 100.0 * 8.336357116699219
Epoch 2800, val loss: 0.586705207824707
Epoch 2810, training loss: 833.2463989257812 = 0.18924614787101746 + 100.0 * 8.330571174621582
Epoch 2810, val loss: 0.5875250697135925
Epoch 2820, training loss: 833.0429077148438 = 0.18786951899528503 + 100.0 * 8.328550338745117
Epoch 2820, val loss: 0.5886985063552856
Epoch 2830, training loss: 833.0288696289062 = 0.18652427196502686 + 100.0 * 8.328423500061035
Epoch 2830, val loss: 0.5901387333869934
Epoch 2840, training loss: 833.36328125 = 0.1852218359708786 + 100.0 * 8.331780433654785
Epoch 2840, val loss: 0.5917637944221497
Epoch 2850, training loss: 832.956298828125 = 0.18383896350860596 + 100.0 * 8.32772445678711
Epoch 2850, val loss: 0.5915310978889465
Epoch 2860, training loss: 833.031982421875 = 0.18250355124473572 + 100.0 * 8.328495025634766
Epoch 2860, val loss: 0.5935157537460327
Epoch 2870, training loss: 833.4314575195312 = 0.18119388818740845 + 100.0 * 8.332502365112305
Epoch 2870, val loss: 0.5946061611175537
Epoch 2880, training loss: 833.044677734375 = 0.17983388900756836 + 100.0 * 8.328648567199707
Epoch 2880, val loss: 0.595169723033905
Epoch 2890, training loss: 832.9022216796875 = 0.1785159558057785 + 100.0 * 8.327237129211426
Epoch 2890, val loss: 0.5966935753822327
Epoch 2900, training loss: 832.8113403320312 = 0.17720410227775574 + 100.0 * 8.32634162902832
Epoch 2900, val loss: 0.5971616506576538
Epoch 2910, training loss: 832.829833984375 = 0.17589470744132996 + 100.0 * 8.326539039611816
Epoch 2910, val loss: 0.5985948443412781
Epoch 2920, training loss: 833.1935424804688 = 0.1746535301208496 + 100.0 * 8.330188751220703
Epoch 2920, val loss: 0.5985356569290161
Epoch 2930, training loss: 832.9547119140625 = 0.17329393327236176 + 100.0 * 8.327814102172852
Epoch 2930, val loss: 0.6011509895324707
Epoch 2940, training loss: 832.8043212890625 = 0.171986922621727 + 100.0 * 8.326323509216309
Epoch 2940, val loss: 0.6021926999092102
Epoch 2950, training loss: 832.8494262695312 = 0.17071130871772766 + 100.0 * 8.326786994934082
Epoch 2950, val loss: 0.6040573716163635
Epoch 2960, training loss: 833.2103271484375 = 0.1694493144750595 + 100.0 * 8.330409049987793
Epoch 2960, val loss: 0.6053444147109985
Epoch 2970, training loss: 832.8797607421875 = 0.16814348101615906 + 100.0 * 8.327116012573242
Epoch 2970, val loss: 0.6061817407608032
Epoch 2980, training loss: 833.0492553710938 = 0.16688162088394165 + 100.0 * 8.328824043273926
Epoch 2980, val loss: 0.6077268123626709
Epoch 2990, training loss: 832.8583374023438 = 0.16558992862701416 + 100.0 * 8.326927185058594
Epoch 2990, val loss: 0.608930230140686
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7747336377473364
0.8423531116423967
=== training gcn model ===
Epoch 0, training loss: 1059.324462890625 = 1.0971473455429077 + 100.0 * 10.58227252960205
Epoch 0, val loss: 1.097598910331726
Epoch 10, training loss: 1059.2276611328125 = 1.0910136699676514 + 100.0 * 10.581365585327148
Epoch 10, val loss: 1.0913939476013184
Epoch 20, training loss: 1058.3890380859375 = 1.0839134454727173 + 100.0 * 10.573051452636719
Epoch 20, val loss: 1.0842071771621704
Epoch 30, training loss: 1052.141357421875 = 1.0754618644714355 + 100.0 * 10.510659217834473
Epoch 30, val loss: 1.07554292678833
Epoch 40, training loss: 1025.619140625 = 1.065579891204834 + 100.0 * 10.245535850524902
Epoch 40, val loss: 1.0655231475830078
Epoch 50, training loss: 984.4120483398438 = 1.0558234453201294 + 100.0 * 9.833561897277832
Epoch 50, val loss: 1.0555932521820068
Epoch 60, training loss: 955.7286987304688 = 1.0475960969924927 + 100.0 * 9.5468111038208
Epoch 60, val loss: 1.0473390817642212
Epoch 70, training loss: 941.6609497070312 = 1.0393508672714233 + 100.0 * 9.40621566772461
Epoch 70, val loss: 1.0393320322036743
Epoch 80, training loss: 921.6806640625 = 1.032848834991455 + 100.0 * 9.206478118896484
Epoch 80, val loss: 1.0332106351852417
Epoch 90, training loss: 912.0899658203125 = 1.026863932609558 + 100.0 * 9.110630989074707
Epoch 90, val loss: 1.0274391174316406
Epoch 100, training loss: 898.9187622070312 = 1.0208969116210938 + 100.0 * 8.978979110717773
Epoch 100, val loss: 1.0216342210769653
Epoch 110, training loss: 889.1864013671875 = 1.015011191368103 + 100.0 * 8.8817138671875
Epoch 110, val loss: 1.0157746076583862
Epoch 120, training loss: 883.9061279296875 = 1.007820963859558 + 100.0 * 8.828983306884766
Epoch 120, val loss: 1.0086456537246704
Epoch 130, training loss: 879.7182006835938 = 0.9992503523826599 + 100.0 * 8.787189483642578
Epoch 130, val loss: 1.0002456903457642
Epoch 140, training loss: 876.72216796875 = 0.9894503355026245 + 100.0 * 8.75732707977295
Epoch 140, val loss: 0.990631639957428
Epoch 150, training loss: 874.8308715820312 = 0.9785147905349731 + 100.0 * 8.738523483276367
Epoch 150, val loss: 0.979921281337738
Epoch 160, training loss: 872.4537963867188 = 0.9665237069129944 + 100.0 * 8.714872360229492
Epoch 160, val loss: 0.9682347774505615
Epoch 170, training loss: 869.9234008789062 = 0.9536721110343933 + 100.0 * 8.689697265625
Epoch 170, val loss: 0.9557251334190369
Epoch 180, training loss: 868.0182495117188 = 0.9400407671928406 + 100.0 * 8.670782089233398
Epoch 180, val loss: 0.9424155950546265
Epoch 190, training loss: 865.2507934570312 = 0.9252766370773315 + 100.0 * 8.643255233764648
Epoch 190, val loss: 0.9279891848564148
Epoch 200, training loss: 863.0405883789062 = 0.909193754196167 + 100.0 * 8.62131404876709
Epoch 200, val loss: 0.912333607673645
Epoch 210, training loss: 861.2394409179688 = 0.8918140530586243 + 100.0 * 8.603476524353027
Epoch 210, val loss: 0.8952902555465698
Epoch 220, training loss: 859.7694702148438 = 0.87313312292099 + 100.0 * 8.588963508605957
Epoch 220, val loss: 0.877120316028595
Epoch 230, training loss: 858.8190307617188 = 0.8535901308059692 + 100.0 * 8.579654693603516
Epoch 230, val loss: 0.8581246733665466
Epoch 240, training loss: 857.3531494140625 = 0.8333778977394104 + 100.0 * 8.565197944641113
Epoch 240, val loss: 0.8385247588157654
Epoch 250, training loss: 856.44873046875 = 0.8128610849380493 + 100.0 * 8.556358337402344
Epoch 250, val loss: 0.8187052607536316
Epoch 260, training loss: 855.2900390625 = 0.7922535538673401 + 100.0 * 8.544978141784668
Epoch 260, val loss: 0.7987959384918213
Epoch 270, training loss: 854.38232421875 = 0.7717263698577881 + 100.0 * 8.53610610961914
Epoch 270, val loss: 0.7791282534599304
Epoch 280, training loss: 853.9109497070312 = 0.7514254450798035 + 100.0 * 8.531595230102539
Epoch 280, val loss: 0.7596572041511536
Epoch 290, training loss: 852.8602905273438 = 0.7315688729286194 + 100.0 * 8.521286964416504
Epoch 290, val loss: 0.7407734990119934
Epoch 300, training loss: 852.093017578125 = 0.7124654650688171 + 100.0 * 8.513805389404297
Epoch 300, val loss: 0.7227085828781128
Epoch 310, training loss: 851.634033203125 = 0.6942784190177917 + 100.0 * 8.509397506713867
Epoch 310, val loss: 0.7056384682655334
Epoch 320, training loss: 850.9310302734375 = 0.677078902721405 + 100.0 * 8.50253963470459
Epoch 320, val loss: 0.689544141292572
Epoch 330, training loss: 850.4578857421875 = 0.6610122323036194 + 100.0 * 8.497968673706055
Epoch 330, val loss: 0.6746599674224854
Epoch 340, training loss: 849.817626953125 = 0.646190345287323 + 100.0 * 8.491714477539062
Epoch 340, val loss: 0.6610549092292786
Epoch 350, training loss: 850.3536376953125 = 0.6325706839561462 + 100.0 * 8.497210502624512
Epoch 350, val loss: 0.6486782431602478
Epoch 360, training loss: 849.0264892578125 = 0.620026707649231 + 100.0 * 8.484064102172852
Epoch 360, val loss: 0.6373664736747742
Epoch 370, training loss: 848.22412109375 = 0.6086733341217041 + 100.0 * 8.476154327392578
Epoch 370, val loss: 0.6273457407951355
Epoch 380, training loss: 847.9661865234375 = 0.5984432101249695 + 100.0 * 8.473677635192871
Epoch 380, val loss: 0.6183966398239136
Epoch 390, training loss: 847.72509765625 = 0.5890653729438782 + 100.0 * 8.471360206604004
Epoch 390, val loss: 0.6103535890579224
Epoch 400, training loss: 847.0695190429688 = 0.5805583596229553 + 100.0 * 8.464889526367188
Epoch 400, val loss: 0.6031502485275269
Epoch 410, training loss: 846.5198364257812 = 0.5728829503059387 + 100.0 * 8.45946979522705
Epoch 410, val loss: 0.5967847108840942
Epoch 420, training loss: 846.096923828125 = 0.5659178495407104 + 100.0 * 8.455309867858887
Epoch 420, val loss: 0.5911534428596497
Epoch 430, training loss: 846.92724609375 = 0.5595382452011108 + 100.0 * 8.463677406311035
Epoch 430, val loss: 0.5861639976501465
Epoch 440, training loss: 845.7431640625 = 0.5536152720451355 + 100.0 * 8.451895713806152
Epoch 440, val loss: 0.581586480140686
Epoch 450, training loss: 846.0623168945312 = 0.5482015013694763 + 100.0 * 8.455141067504883
Epoch 450, val loss: 0.5775259733200073
Epoch 460, training loss: 845.1340942382812 = 0.5432074666023254 + 100.0 * 8.445908546447754
Epoch 460, val loss: 0.57383793592453
Epoch 470, training loss: 844.675537109375 = 0.5386139154434204 + 100.0 * 8.44136905670166
Epoch 470, val loss: 0.5706042051315308
Epoch 480, training loss: 844.4221801757812 = 0.5343773365020752 + 100.0 * 8.438878059387207
Epoch 480, val loss: 0.5677047967910767
Epoch 490, training loss: 844.3046875 = 0.5304144024848938 + 100.0 * 8.437743186950684
Epoch 490, val loss: 0.5650837421417236
Epoch 500, training loss: 844.1082763671875 = 0.5266676545143127 + 100.0 * 8.435815811157227
Epoch 500, val loss: 0.5626901984214783
Epoch 510, training loss: 844.0042114257812 = 0.52314293384552 + 100.0 * 8.434810638427734
Epoch 510, val loss: 0.5605397820472717
Epoch 520, training loss: 843.682373046875 = 0.5198324918746948 + 100.0 * 8.431625366210938
Epoch 520, val loss: 0.558559238910675
Epoch 530, training loss: 843.4439697265625 = 0.5166938900947571 + 100.0 * 8.429272651672363
Epoch 530, val loss: 0.5567834377288818
Epoch 540, training loss: 843.0993041992188 = 0.5137344002723694 + 100.0 * 8.42585563659668
Epoch 540, val loss: 0.5551395416259766
Epoch 550, training loss: 843.5308837890625 = 0.5109122395515442 + 100.0 * 8.43019962310791
Epoch 550, val loss: 0.5536637902259827
Epoch 560, training loss: 844.4351196289062 = 0.5081304311752319 + 100.0 * 8.43927001953125
Epoch 560, val loss: 0.5520944595336914
Epoch 570, training loss: 842.883544921875 = 0.5054247975349426 + 100.0 * 8.423781394958496
Epoch 570, val loss: 0.55078125
Epoch 580, training loss: 842.5575561523438 = 0.502911388874054 + 100.0 * 8.420546531677246
Epoch 580, val loss: 0.5496172308921814
Epoch 590, training loss: 842.2391357421875 = 0.500547468662262 + 100.0 * 8.417386054992676
Epoch 590, val loss: 0.548587441444397
Epoch 600, training loss: 842.053955078125 = 0.49829256534576416 + 100.0 * 8.415556907653809
Epoch 600, val loss: 0.547613263130188
Epoch 610, training loss: 841.8875732421875 = 0.49610787630081177 + 100.0 * 8.413914680480957
Epoch 610, val loss: 0.5467169880867004
Epoch 620, training loss: 841.7915649414062 = 0.49399080872535706 + 100.0 * 8.412975311279297
Epoch 620, val loss: 0.5458548069000244
Epoch 630, training loss: 842.0927734375 = 0.4918815493583679 + 100.0 * 8.416008949279785
Epoch 630, val loss: 0.5450654029846191
Epoch 640, training loss: 841.85302734375 = 0.48978912830352783 + 100.0 * 8.4136323928833
Epoch 640, val loss: 0.5441529154777527
Epoch 650, training loss: 841.4327392578125 = 0.487824410200119 + 100.0 * 8.409448623657227
Epoch 650, val loss: 0.5434993505477905
Epoch 660, training loss: 841.1815185546875 = 0.4859497547149658 + 100.0 * 8.40695571899414
Epoch 660, val loss: 0.5428932905197144
Epoch 670, training loss: 842.0831909179688 = 0.48412537574768066 + 100.0 * 8.415990829467773
Epoch 670, val loss: 0.5422881245613098
Epoch 680, training loss: 841.5823364257812 = 0.4822702407836914 + 100.0 * 8.41100025177002
Epoch 680, val loss: 0.5417624115943909
Epoch 690, training loss: 840.854736328125 = 0.4804915189743042 + 100.0 * 8.403742790222168
Epoch 690, val loss: 0.5412416458129883
Epoch 700, training loss: 840.7003173828125 = 0.4787958860397339 + 100.0 * 8.402215003967285
Epoch 700, val loss: 0.540721595287323
Epoch 710, training loss: 840.58935546875 = 0.47715985774993896 + 100.0 * 8.401122093200684
Epoch 710, val loss: 0.5402737855911255
Epoch 720, training loss: 840.8176879882812 = 0.4755217730998993 + 100.0 * 8.403421401977539
Epoch 720, val loss: 0.5399042367935181
Epoch 730, training loss: 840.3555908203125 = 0.47388967871665955 + 100.0 * 8.39881706237793
Epoch 730, val loss: 0.5394311547279358
Epoch 740, training loss: 840.2186279296875 = 0.4723238945007324 + 100.0 * 8.397462844848633
Epoch 740, val loss: 0.5391345620155334
Epoch 750, training loss: 840.025390625 = 0.4707961976528168 + 100.0 * 8.395545959472656
Epoch 750, val loss: 0.5388005375862122
Epoch 760, training loss: 840.436767578125 = 0.4693041741847992 + 100.0 * 8.399674415588379
Epoch 760, val loss: 0.5385610461235046
Epoch 770, training loss: 840.725830078125 = 0.46774905920028687 + 100.0 * 8.402580261230469
Epoch 770, val loss: 0.5380732417106628
Epoch 780, training loss: 839.9013671875 = 0.4661971926689148 + 100.0 * 8.394351959228516
Epoch 780, val loss: 0.5377005934715271
Epoch 790, training loss: 839.604248046875 = 0.46476203203201294 + 100.0 * 8.39139461517334
Epoch 790, val loss: 0.5374892950057983
Epoch 800, training loss: 839.488525390625 = 0.4633752107620239 + 100.0 * 8.390251159667969
Epoch 800, val loss: 0.5373185873031616
Epoch 810, training loss: 839.3907470703125 = 0.4620116949081421 + 100.0 * 8.389286994934082
Epoch 810, val loss: 0.5371327996253967
Epoch 820, training loss: 839.5143432617188 = 0.4606565833091736 + 100.0 * 8.39053726196289
Epoch 820, val loss: 0.5369415879249573
Epoch 830, training loss: 839.439208984375 = 0.4592406153678894 + 100.0 * 8.389800071716309
Epoch 830, val loss: 0.5367573499679565
Epoch 840, training loss: 839.3382568359375 = 0.4578705430030823 + 100.0 * 8.388803482055664
Epoch 840, val loss: 0.5364879369735718
Epoch 850, training loss: 839.3034057617188 = 0.4565236568450928 + 100.0 * 8.388468742370605
Epoch 850, val loss: 0.536287248134613
Epoch 860, training loss: 839.2909545898438 = 0.45516955852508545 + 100.0 * 8.388358116149902
Epoch 860, val loss: 0.5362660884857178
Epoch 870, training loss: 838.971435546875 = 0.4538632333278656 + 100.0 * 8.385175704956055
Epoch 870, val loss: 0.5360525250434875
Epoch 880, training loss: 838.7753295898438 = 0.4525713324546814 + 100.0 * 8.383227348327637
Epoch 880, val loss: 0.535943329334259
Epoch 890, training loss: 838.8779907226562 = 0.45129522681236267 + 100.0 * 8.38426685333252
Epoch 890, val loss: 0.5358796119689941
Epoch 900, training loss: 838.9441528320312 = 0.45001763105392456 + 100.0 * 8.384941101074219
Epoch 900, val loss: 0.5357114672660828
Epoch 910, training loss: 838.8034057617188 = 0.44870126247406006 + 100.0 * 8.383546829223633
Epoch 910, val loss: 0.5356913805007935
Epoch 920, training loss: 838.6116333007812 = 0.4474104642868042 + 100.0 * 8.38164234161377
Epoch 920, val loss: 0.5354300737380981
Epoch 930, training loss: 838.411865234375 = 0.446168452501297 + 100.0 * 8.379656791687012
Epoch 930, val loss: 0.5354361534118652
Epoch 940, training loss: 838.2652587890625 = 0.44494810700416565 + 100.0 * 8.378203392028809
Epoch 940, val loss: 0.5353009700775146
Epoch 950, training loss: 838.6636352539062 = 0.4437360465526581 + 100.0 * 8.38219928741455
Epoch 950, val loss: 0.535210371017456
Epoch 960, training loss: 838.9042358398438 = 0.44245240092277527 + 100.0 * 8.384617805480957
Epoch 960, val loss: 0.5350431799888611
Epoch 970, training loss: 838.3243408203125 = 0.4411793351173401 + 100.0 * 8.37883186340332
Epoch 970, val loss: 0.5350424647331238
Epoch 980, training loss: 838.024169921875 = 0.43994566798210144 + 100.0 * 8.375842094421387
Epoch 980, val loss: 0.5349047183990479
Epoch 990, training loss: 837.9446411132812 = 0.4387398958206177 + 100.0 * 8.375059127807617
Epoch 990, val loss: 0.5348450541496277
Epoch 1000, training loss: 838.8675537109375 = 0.43752941489219666 + 100.0 * 8.384300231933594
Epoch 1000, val loss: 0.5347881317138672
Epoch 1010, training loss: 838.0629272460938 = 0.43629270792007446 + 100.0 * 8.376266479492188
Epoch 1010, val loss: 0.5346783399581909
Epoch 1020, training loss: 837.7427978515625 = 0.4350704252719879 + 100.0 * 8.373077392578125
Epoch 1020, val loss: 0.5345847010612488
Epoch 1030, training loss: 837.580078125 = 0.4338738024234772 + 100.0 * 8.371461868286133
Epoch 1030, val loss: 0.5345587730407715
Epoch 1040, training loss: 837.811279296875 = 0.4326777756214142 + 100.0 * 8.373785972595215
Epoch 1040, val loss: 0.5345749855041504
Epoch 1050, training loss: 837.6781005859375 = 0.4314245283603668 + 100.0 * 8.372467041015625
Epoch 1050, val loss: 0.5343456864356995
Epoch 1060, training loss: 837.4554443359375 = 0.430183470249176 + 100.0 * 8.37025260925293
Epoch 1060, val loss: 0.5342536568641663
Epoch 1070, training loss: 837.374267578125 = 0.4289756417274475 + 100.0 * 8.369453430175781
Epoch 1070, val loss: 0.5341244339942932
Epoch 1080, training loss: 837.36083984375 = 0.42777347564697266 + 100.0 * 8.369330406188965
Epoch 1080, val loss: 0.5340795516967773
Epoch 1090, training loss: 837.9317016601562 = 0.4265574514865875 + 100.0 * 8.375051498413086
Epoch 1090, val loss: 0.5339500308036804
Epoch 1100, training loss: 838.0907592773438 = 0.42529669404029846 + 100.0 * 8.376654624938965
Epoch 1100, val loss: 0.5338551998138428
Epoch 1110, training loss: 837.4150390625 = 0.42401742935180664 + 100.0 * 8.36991024017334
Epoch 1110, val loss: 0.5337811708450317
Epoch 1120, training loss: 837.0611572265625 = 0.42278698086738586 + 100.0 * 8.36638355255127
Epoch 1120, val loss: 0.5336681008338928
Epoch 1130, training loss: 837.0391235351562 = 0.42158231139183044 + 100.0 * 8.366175651550293
Epoch 1130, val loss: 0.5335633158683777
Epoch 1140, training loss: 836.9520263671875 = 0.4203857183456421 + 100.0 * 8.365316390991211
Epoch 1140, val loss: 0.5334917306900024
Epoch 1150, training loss: 836.9813232421875 = 0.41917669773101807 + 100.0 * 8.365621566772461
Epoch 1150, val loss: 0.5334044694900513
Epoch 1160, training loss: 837.9374389648438 = 0.41793522238731384 + 100.0 * 8.375195503234863
Epoch 1160, val loss: 0.5333189964294434
Epoch 1170, training loss: 837.2708740234375 = 0.41664937138557434 + 100.0 * 8.368542671203613
Epoch 1170, val loss: 0.5332000851631165
Epoch 1180, training loss: 836.9337158203125 = 0.41538262367248535 + 100.0 * 8.365182876586914
Epoch 1180, val loss: 0.5329664945602417
Epoch 1190, training loss: 836.79296875 = 0.4141438603401184 + 100.0 * 8.363788604736328
Epoch 1190, val loss: 0.5329102277755737
Epoch 1200, training loss: 837.0449829101562 = 0.4129120409488678 + 100.0 * 8.366320610046387
Epoch 1200, val loss: 0.532715380191803
Epoch 1210, training loss: 836.7117309570312 = 0.4116329550743103 + 100.0 * 8.363000869750977
Epoch 1210, val loss: 0.532650351524353
Epoch 1220, training loss: 836.626708984375 = 0.4103592038154602 + 100.0 * 8.362163543701172
Epoch 1220, val loss: 0.5324853658676147
Epoch 1230, training loss: 836.6740112304688 = 0.4090995192527771 + 100.0 * 8.362648963928223
Epoch 1230, val loss: 0.5323108434677124
Epoch 1240, training loss: 836.8744506835938 = 0.407820463180542 + 100.0 * 8.364665985107422
Epoch 1240, val loss: 0.5321506857872009
Epoch 1250, training loss: 836.623046875 = 0.40651679039001465 + 100.0 * 8.362165451049805
Epoch 1250, val loss: 0.5320947766304016
Epoch 1260, training loss: 837.8582153320312 = 0.40521812438964844 + 100.0 * 8.374529838562012
Epoch 1260, val loss: 0.5319170951843262
Epoch 1270, training loss: 836.8194580078125 = 0.403865784406662 + 100.0 * 8.364155769348145
Epoch 1270, val loss: 0.5316169261932373
Epoch 1280, training loss: 836.49365234375 = 0.402544766664505 + 100.0 * 8.36091136932373
Epoch 1280, val loss: 0.5315065383911133
Epoch 1290, training loss: 836.32275390625 = 0.40124839544296265 + 100.0 * 8.359214782714844
Epoch 1290, val loss: 0.5313646793365479
Epoch 1300, training loss: 836.2445068359375 = 0.399949848651886 + 100.0 * 8.358445167541504
Epoch 1300, val loss: 0.5312677025794983
Epoch 1310, training loss: 836.3342895507812 = 0.39863690733909607 + 100.0 * 8.359356880187988
Epoch 1310, val loss: 0.5310714840888977
Epoch 1320, training loss: 836.60302734375 = 0.3972764015197754 + 100.0 * 8.36205768585205
Epoch 1320, val loss: 0.5308756232261658
Epoch 1330, training loss: 836.202880859375 = 0.3958817720413208 + 100.0 * 8.358070373535156
Epoch 1330, val loss: 0.5307284593582153
Epoch 1340, training loss: 836.2820434570312 = 0.39450374245643616 + 100.0 * 8.358875274658203
Epoch 1340, val loss: 0.5305348038673401
Epoch 1350, training loss: 836.2129516601562 = 0.39313215017318726 + 100.0 * 8.358198165893555
Epoch 1350, val loss: 0.5303380489349365
Epoch 1360, training loss: 836.180908203125 = 0.3917475640773773 + 100.0 * 8.357892036437988
Epoch 1360, val loss: 0.5301322340965271
Epoch 1370, training loss: 836.1119995117188 = 0.39034202694892883 + 100.0 * 8.357216835021973
Epoch 1370, val loss: 0.5299678444862366
Epoch 1380, training loss: 836.4998168945312 = 0.3889230489730835 + 100.0 * 8.361108779907227
Epoch 1380, val loss: 0.5297933220863342
Epoch 1390, training loss: 836.2005004882812 = 0.3874737024307251 + 100.0 * 8.35813045501709
Epoch 1390, val loss: 0.5296450853347778
Epoch 1400, training loss: 836.518798828125 = 0.38601619005203247 + 100.0 * 8.361328125
Epoch 1400, val loss: 0.5293352603912354
Epoch 1410, training loss: 835.9989013671875 = 0.38453662395477295 + 100.0 * 8.356143951416016
Epoch 1410, val loss: 0.5290090441703796
Epoch 1420, training loss: 835.8145141601562 = 0.3830653727054596 + 100.0 * 8.354314804077148
Epoch 1420, val loss: 0.5288165211677551
Epoch 1430, training loss: 835.8701171875 = 0.38160350918769836 + 100.0 * 8.35488510131836
Epoch 1430, val loss: 0.528643012046814
Epoch 1440, training loss: 836.4005737304688 = 0.38011613488197327 + 100.0 * 8.360204696655273
Epoch 1440, val loss: 0.5283913016319275
Epoch 1450, training loss: 835.800537109375 = 0.37857934832572937 + 100.0 * 8.354219436645508
Epoch 1450, val loss: 0.5279443264007568
Epoch 1460, training loss: 835.7503051757812 = 0.3770461976528168 + 100.0 * 8.353732109069824
Epoch 1460, val loss: 0.5277843475341797
Epoch 1470, training loss: 835.9796142578125 = 0.3755098283290863 + 100.0 * 8.356040954589844
Epoch 1470, val loss: 0.5275343656539917
Epoch 1480, training loss: 835.8260498046875 = 0.3739510178565979 + 100.0 * 8.354520797729492
Epoch 1480, val loss: 0.5272008180618286
Epoch 1490, training loss: 835.5825805664062 = 0.37237706780433655 + 100.0 * 8.352102279663086
Epoch 1490, val loss: 0.5270213484764099
Epoch 1500, training loss: 835.7952270507812 = 0.3708130121231079 + 100.0 * 8.354244232177734
Epoch 1500, val loss: 0.5267049670219421
Epoch 1510, training loss: 835.6905517578125 = 0.3692117929458618 + 100.0 * 8.3532133102417
Epoch 1510, val loss: 0.526427149772644
Epoch 1520, training loss: 835.5928344726562 = 0.3675869405269623 + 100.0 * 8.352252006530762
Epoch 1520, val loss: 0.5262117981910706
Epoch 1530, training loss: 835.9805297851562 = 0.3659606873989105 + 100.0 * 8.356145858764648
Epoch 1530, val loss: 0.5259783864021301
Epoch 1540, training loss: 835.4332275390625 = 0.3642977774143219 + 100.0 * 8.350688934326172
Epoch 1540, val loss: 0.525733232498169
Epoch 1550, training loss: 835.5335083007812 = 0.36265262961387634 + 100.0 * 8.35170841217041
Epoch 1550, val loss: 0.5255882740020752
Epoch 1560, training loss: 835.450439453125 = 0.36098742485046387 + 100.0 * 8.350894927978516
Epoch 1560, val loss: 0.5253626108169556
Epoch 1570, training loss: 835.6478271484375 = 0.3593093156814575 + 100.0 * 8.352885246276855
Epoch 1570, val loss: 0.5251672267913818
Epoch 1580, training loss: 835.388916015625 = 0.357597678899765 + 100.0 * 8.350313186645508
Epoch 1580, val loss: 0.5248517394065857
Epoch 1590, training loss: 835.794921875 = 0.3558874726295471 + 100.0 * 8.354390144348145
Epoch 1590, val loss: 0.5247254371643066
Epoch 1600, training loss: 835.5427856445312 = 0.35414114594459534 + 100.0 * 8.351886749267578
Epoch 1600, val loss: 0.5245896577835083
Epoch 1610, training loss: 835.3779296875 = 0.35239607095718384 + 100.0 * 8.350255012512207
Epoch 1610, val loss: 0.5242144465446472
Epoch 1620, training loss: 835.2323608398438 = 0.3506541848182678 + 100.0 * 8.348816871643066
Epoch 1620, val loss: 0.5242118835449219
Epoch 1630, training loss: 835.35986328125 = 0.34891536831855774 + 100.0 * 8.350109100341797
Epoch 1630, val loss: 0.5239924192428589
Epoch 1640, training loss: 835.7172241210938 = 0.3471449911594391 + 100.0 * 8.353700637817383
Epoch 1640, val loss: 0.523786723613739
Epoch 1650, training loss: 835.2633666992188 = 0.3453288674354553 + 100.0 * 8.349180221557617
Epoch 1650, val loss: 0.5235879421234131
Epoch 1660, training loss: 835.1136474609375 = 0.3435423970222473 + 100.0 * 8.347701072692871
Epoch 1660, val loss: 0.523516833782196
Epoch 1670, training loss: 835.0370483398438 = 0.34175169467926025 + 100.0 * 8.346953392028809
Epoch 1670, val loss: 0.5234048962593079
Epoch 1680, training loss: 835.4082641601562 = 0.33995160460472107 + 100.0 * 8.350683212280273
Epoch 1680, val loss: 0.5232459902763367
Epoch 1690, training loss: 835.2454833984375 = 0.3381052017211914 + 100.0 * 8.34907341003418
Epoch 1690, val loss: 0.5231825709342957
Epoch 1700, training loss: 835.1442260742188 = 0.33623793721199036 + 100.0 * 8.348079681396484
Epoch 1700, val loss: 0.5229954719543457
Epoch 1710, training loss: 835.1519165039062 = 0.3343893885612488 + 100.0 * 8.348175048828125
Epoch 1710, val loss: 0.5228918790817261
Epoch 1720, training loss: 835.323486328125 = 0.33253511786460876 + 100.0 * 8.349909782409668
Epoch 1720, val loss: 0.52276211977005
Epoch 1730, training loss: 835.0274047851562 = 0.3306494951248169 + 100.0 * 8.346967697143555
Epoch 1730, val loss: 0.5228719711303711
Epoch 1740, training loss: 834.9431762695312 = 0.32876622676849365 + 100.0 * 8.34614372253418
Epoch 1740, val loss: 0.5229550004005432
Epoch 1750, training loss: 834.7844848632812 = 0.3268830478191376 + 100.0 * 8.344575881958008
Epoch 1750, val loss: 0.5229271054267883
Epoch 1760, training loss: 834.8197631835938 = 0.3249977231025696 + 100.0 * 8.344947814941406
Epoch 1760, val loss: 0.5229822993278503
Epoch 1770, training loss: 835.5478515625 = 0.3230925500392914 + 100.0 * 8.35224723815918
Epoch 1770, val loss: 0.5230809450149536
Epoch 1780, training loss: 834.9934692382812 = 0.3211081624031067 + 100.0 * 8.346723556518555
Epoch 1780, val loss: 0.5232114791870117
Epoch 1790, training loss: 834.6986694335938 = 0.319119393825531 + 100.0 * 8.343795776367188
Epoch 1790, val loss: 0.5231942534446716
Epoch 1800, training loss: 834.6896362304688 = 0.3171527683734894 + 100.0 * 8.343725204467773
Epoch 1800, val loss: 0.5232831835746765
Epoch 1810, training loss: 834.8927612304688 = 0.3151892125606537 + 100.0 * 8.345775604248047
Epoch 1810, val loss: 0.5235655903816223
Epoch 1820, training loss: 834.9024658203125 = 0.31319352984428406 + 100.0 * 8.345892906188965
Epoch 1820, val loss: 0.5235944986343384
Epoch 1830, training loss: 834.71044921875 = 0.31116998195648193 + 100.0 * 8.343993186950684
Epoch 1830, val loss: 0.5235277414321899
Epoch 1840, training loss: 834.6736450195312 = 0.30917105078697205 + 100.0 * 8.343645095825195
Epoch 1840, val loss: 0.5237618088722229
Epoch 1850, training loss: 834.8831176757812 = 0.3071673512458801 + 100.0 * 8.345759391784668
Epoch 1850, val loss: 0.5239675045013428
Epoch 1860, training loss: 834.723388671875 = 0.3051376938819885 + 100.0 * 8.344182968139648
Epoch 1860, val loss: 0.5241708159446716
Epoch 1870, training loss: 834.6878051757812 = 0.3031146824359894 + 100.0 * 8.343847274780273
Epoch 1870, val loss: 0.524517297744751
Epoch 1880, training loss: 834.7241821289062 = 0.30108925700187683 + 100.0 * 8.344230651855469
Epoch 1880, val loss: 0.5246126055717468
Epoch 1890, training loss: 834.7052612304688 = 0.29905086755752563 + 100.0 * 8.344061851501465
Epoch 1890, val loss: 0.5249037742614746
Epoch 1900, training loss: 834.5222778320312 = 0.2970040738582611 + 100.0 * 8.342252731323242
Epoch 1900, val loss: 0.5253745913505554
Epoch 1910, training loss: 834.5816040039062 = 0.2949737012386322 + 100.0 * 8.342865943908691
Epoch 1910, val loss: 0.5255703330039978
Epoch 1920, training loss: 834.8590087890625 = 0.2929345667362213 + 100.0 * 8.345661163330078
Epoch 1920, val loss: 0.5258766412734985
Epoch 1930, training loss: 834.4189453125 = 0.2908588945865631 + 100.0 * 8.341280937194824
Epoch 1930, val loss: 0.5259971618652344
Epoch 1940, training loss: 834.4063110351562 = 0.2888047695159912 + 100.0 * 8.341175079345703
Epoch 1940, val loss: 0.5263339281082153
Epoch 1950, training loss: 834.4696044921875 = 0.2867533564567566 + 100.0 * 8.341828346252441
Epoch 1950, val loss: 0.5267296433448792
Epoch 1960, training loss: 834.670654296875 = 0.28469154238700867 + 100.0 * 8.343859672546387
Epoch 1960, val loss: 0.5271196365356445
Epoch 1970, training loss: 834.86376953125 = 0.28262147307395935 + 100.0 * 8.34581184387207
Epoch 1970, val loss: 0.5278928279876709
Epoch 1980, training loss: 834.2989501953125 = 0.28050655126571655 + 100.0 * 8.340184211730957
Epoch 1980, val loss: 0.5279190540313721
Epoch 1990, training loss: 834.2782592773438 = 0.278439998626709 + 100.0 * 8.339998245239258
Epoch 1990, val loss: 0.5283947587013245
Epoch 2000, training loss: 834.1932373046875 = 0.2763759195804596 + 100.0 * 8.339168548583984
Epoch 2000, val loss: 0.5290175080299377
Epoch 2010, training loss: 834.185546875 = 0.27432212233543396 + 100.0 * 8.339112281799316
Epoch 2010, val loss: 0.529531717300415
Epoch 2020, training loss: 834.3707275390625 = 0.27226385474205017 + 100.0 * 8.340984344482422
Epoch 2020, val loss: 0.5300827026367188
Epoch 2030, training loss: 834.3671875 = 0.270175576210022 + 100.0 * 8.340970039367676
Epoch 2030, val loss: 0.5304625034332275
Epoch 2040, training loss: 835.031982421875 = 0.26807546615600586 + 100.0 * 8.347639083862305
Epoch 2040, val loss: 0.5307471752166748
Epoch 2050, training loss: 834.3955688476562 = 0.26594841480255127 + 100.0 * 8.341296195983887
Epoch 2050, val loss: 0.5315174460411072
Epoch 2060, training loss: 834.16259765625 = 0.26384106278419495 + 100.0 * 8.338987350463867
Epoch 2060, val loss: 0.5318204760551453
Epoch 2070, training loss: 834.0460815429688 = 0.2617722153663635 + 100.0 * 8.33784294128418
Epoch 2070, val loss: 0.532654345035553
Epoch 2080, training loss: 833.9909057617188 = 0.2597150504589081 + 100.0 * 8.337311744689941
Epoch 2080, val loss: 0.5331668257713318
Epoch 2090, training loss: 834.2544555664062 = 0.2576600909233093 + 100.0 * 8.339967727661133
Epoch 2090, val loss: 0.5339322090148926
Epoch 2100, training loss: 834.4346923828125 = 0.25556233525276184 + 100.0 * 8.341791152954102
Epoch 2100, val loss: 0.5344915986061096
Epoch 2110, training loss: 834.0542602539062 = 0.2534545361995697 + 100.0 * 8.338007926940918
Epoch 2110, val loss: 0.5352444052696228
Epoch 2120, training loss: 834.0420532226562 = 0.2513731122016907 + 100.0 * 8.337906837463379
Epoch 2120, val loss: 0.5360243320465088
Epoch 2130, training loss: 834.0841674804688 = 0.24930495023727417 + 100.0 * 8.338348388671875
Epoch 2130, val loss: 0.536770761013031
Epoch 2140, training loss: 833.9404907226562 = 0.24725084006786346 + 100.0 * 8.336932182312012
Epoch 2140, val loss: 0.5375027656555176
Epoch 2150, training loss: 833.8533325195312 = 0.24519513547420502 + 100.0 * 8.336081504821777
Epoch 2150, val loss: 0.5381203293800354
Epoch 2160, training loss: 834.12109375 = 0.2431626319885254 + 100.0 * 8.33877944946289
Epoch 2160, val loss: 0.5389232039451599
Epoch 2170, training loss: 834.0930786132812 = 0.24110062420368195 + 100.0 * 8.338520050048828
Epoch 2170, val loss: 0.5396007895469666
Epoch 2180, training loss: 834.0284423828125 = 0.23903287947177887 + 100.0 * 8.337894439697266
Epoch 2180, val loss: 0.5399394631385803
Epoch 2190, training loss: 834.1746215820312 = 0.23699116706848145 + 100.0 * 8.339376449584961
Epoch 2190, val loss: 0.5407217144966125
Epoch 2200, training loss: 833.8078002929688 = 0.23493903875350952 + 100.0 * 8.335728645324707
Epoch 2200, val loss: 0.5420463681221008
Epoch 2210, training loss: 833.7469482421875 = 0.23291748762130737 + 100.0 * 8.335140228271484
Epoch 2210, val loss: 0.5428831577301025
Epoch 2220, training loss: 834.0045776367188 = 0.23091468214988708 + 100.0 * 8.337737083435059
Epoch 2220, val loss: 0.5438377261161804
Epoch 2230, training loss: 833.7281494140625 = 0.22887490689754486 + 100.0 * 8.334992408752441
Epoch 2230, val loss: 0.5444992780685425
Epoch 2240, training loss: 833.6753540039062 = 0.22685755789279938 + 100.0 * 8.334485054016113
Epoch 2240, val loss: 0.5453658699989319
Epoch 2250, training loss: 833.770263671875 = 0.224863201379776 + 100.0 * 8.335453987121582
Epoch 2250, val loss: 0.5464626550674438
Epoch 2260, training loss: 834.5593872070312 = 0.22287395596504211 + 100.0 * 8.343364715576172
Epoch 2260, val loss: 0.5475015640258789
Epoch 2270, training loss: 833.9371337890625 = 0.22084452211856842 + 100.0 * 8.337162971496582
Epoch 2270, val loss: 0.5479973554611206
Epoch 2280, training loss: 833.6907958984375 = 0.2188340127468109 + 100.0 * 8.33471965789795
Epoch 2280, val loss: 0.5492694973945618
Epoch 2290, training loss: 833.57421875 = 0.216863751411438 + 100.0 * 8.333573341369629
Epoch 2290, val loss: 0.5501109957695007
Epoch 2300, training loss: 833.5680541992188 = 0.21491295099258423 + 100.0 * 8.333531379699707
Epoch 2300, val loss: 0.5512866973876953
Epoch 2310, training loss: 834.27685546875 = 0.21298250555992126 + 100.0 * 8.340638160705566
Epoch 2310, val loss: 0.552435576915741
Epoch 2320, training loss: 833.6683959960938 = 0.21099795401096344 + 100.0 * 8.334573745727539
Epoch 2320, val loss: 0.5533763766288757
Epoch 2330, training loss: 834.0621337890625 = 0.20910732448101044 + 100.0 * 8.338530540466309
Epoch 2330, val loss: 0.5551328063011169
Epoch 2340, training loss: 833.5265502929688 = 0.20710812509059906 + 100.0 * 8.333194732666016
Epoch 2340, val loss: 0.555263340473175
Epoch 2350, training loss: 833.5303955078125 = 0.20520268380641937 + 100.0 * 8.333251953125
Epoch 2350, val loss: 0.5564327836036682
Epoch 2360, training loss: 833.4150390625 = 0.20330588519573212 + 100.0 * 8.332117080688477
Epoch 2360, val loss: 0.5577528476715088
Epoch 2370, training loss: 833.4120483398438 = 0.2014322131872177 + 100.0 * 8.332106590270996
Epoch 2370, val loss: 0.559188187122345
Epoch 2380, training loss: 833.6407470703125 = 0.19957582652568817 + 100.0 * 8.33441162109375
Epoch 2380, val loss: 0.5605610013008118
Epoch 2390, training loss: 834.381591796875 = 0.19768977165222168 + 100.0 * 8.341838836669922
Epoch 2390, val loss: 0.5610418319702148
Epoch 2400, training loss: 833.5035400390625 = 0.19578906893730164 + 100.0 * 8.333077430725098
Epoch 2400, val loss: 0.5626071095466614
Epoch 2410, training loss: 833.3709106445312 = 0.19392141699790955 + 100.0 * 8.331769943237305
Epoch 2410, val loss: 0.5634722709655762
Epoch 2420, training loss: 833.3258666992188 = 0.19209855794906616 + 100.0 * 8.331337928771973
Epoch 2420, val loss: 0.5650643110275269
Epoch 2430, training loss: 833.2985229492188 = 0.19029277563095093 + 100.0 * 8.331082344055176
Epoch 2430, val loss: 0.5660854578018188
Epoch 2440, training loss: 833.9387817382812 = 0.18851438164710999 + 100.0 * 8.337502479553223
Epoch 2440, val loss: 0.567328691482544
Epoch 2450, training loss: 833.3070678710938 = 0.1866818517446518 + 100.0 * 8.33120346069336
Epoch 2450, val loss: 0.5687737464904785
Epoch 2460, training loss: 833.280029296875 = 0.1848832219839096 + 100.0 * 8.330951690673828
Epoch 2460, val loss: 0.5703006982803345
Epoch 2470, training loss: 833.2277221679688 = 0.1830957978963852 + 100.0 * 8.330446243286133
Epoch 2470, val loss: 0.5713513493537903
Epoch 2480, training loss: 833.1869506835938 = 0.18134035170078278 + 100.0 * 8.330056190490723
Epoch 2480, val loss: 0.5729278922080994
Epoch 2490, training loss: 833.2060546875 = 0.17959630489349365 + 100.0 * 8.3302640914917
Epoch 2490, val loss: 0.5742087364196777
Epoch 2500, training loss: 834.805908203125 = 0.17789359390735626 + 100.0 * 8.346280097961426
Epoch 2500, val loss: 0.5757049322128296
Epoch 2510, training loss: 833.782470703125 = 0.1761234700679779 + 100.0 * 8.336063385009766
Epoch 2510, val loss: 0.5768651962280273
Epoch 2520, training loss: 833.3331909179688 = 0.1743442863225937 + 100.0 * 8.331588745117188
Epoch 2520, val loss: 0.578370988368988
Epoch 2530, training loss: 833.1395874023438 = 0.17261961102485657 + 100.0 * 8.329669952392578
Epoch 2530, val loss: 0.5796745419502258
Epoch 2540, training loss: 833.099365234375 = 0.17091919481754303 + 100.0 * 8.32928466796875
Epoch 2540, val loss: 0.5810855031013489
Epoch 2550, training loss: 833.3294677734375 = 0.16925543546676636 + 100.0 * 8.331602096557617
Epoch 2550, val loss: 0.5822250843048096
Epoch 2560, training loss: 833.4146118164062 = 0.16755953431129456 + 100.0 * 8.332470893859863
Epoch 2560, val loss: 0.5839501619338989
Epoch 2570, training loss: 833.1742553710938 = 0.16588708758354187 + 100.0 * 8.330083847045898
Epoch 2570, val loss: 0.5855797529220581
Epoch 2580, training loss: 833.0813598632812 = 0.16422192752361298 + 100.0 * 8.329171180725098
Epoch 2580, val loss: 0.5874688625335693
Epoch 2590, training loss: 833.1022338867188 = 0.16257961094379425 + 100.0 * 8.32939624786377
Epoch 2590, val loss: 0.5885456800460815
Epoch 2600, training loss: 833.3973388671875 = 0.1609552800655365 + 100.0 * 8.332364082336426
Epoch 2600, val loss: 0.5902855396270752
Epoch 2610, training loss: 833.4608154296875 = 0.15932072699069977 + 100.0 * 8.333015441894531
Epoch 2610, val loss: 0.5916928648948669
Epoch 2620, training loss: 833.4078979492188 = 0.15767870843410492 + 100.0 * 8.332502365112305
Epoch 2620, val loss: 0.5936712026596069
Epoch 2630, training loss: 833.0853881835938 = 0.15605218708515167 + 100.0 * 8.329293251037598
Epoch 2630, val loss: 0.5954127907752991
Epoch 2640, training loss: 833.0092163085938 = 0.15445329248905182 + 100.0 * 8.328547477722168
Epoch 2640, val loss: 0.5965278148651123
Epoch 2650, training loss: 833.0151977539062 = 0.15288682281970978 + 100.0 * 8.328622817993164
Epoch 2650, val loss: 0.5983050465583801
Epoch 2660, training loss: 833.2347412109375 = 0.1513364315032959 + 100.0 * 8.33083438873291
Epoch 2660, val loss: 0.5997346043586731
Epoch 2670, training loss: 833.0882568359375 = 0.1497712880373001 + 100.0 * 8.329384803771973
Epoch 2670, val loss: 0.6015275716781616
Epoch 2680, training loss: 832.99951171875 = 0.1482238620519638 + 100.0 * 8.328513145446777
Epoch 2680, val loss: 0.6035516858100891
Epoch 2690, training loss: 833.111328125 = 0.1467006951570511 + 100.0 * 8.329646110534668
Epoch 2690, val loss: 0.6055952906608582
Epoch 2700, training loss: 833.4813842773438 = 0.14521680772304535 + 100.0 * 8.333361625671387
Epoch 2700, val loss: 0.6077300906181335
Epoch 2710, training loss: 832.8955078125 = 0.14366084337234497 + 100.0 * 8.327518463134766
Epoch 2710, val loss: 0.6084639430046082
Epoch 2720, training loss: 832.9085693359375 = 0.1421702653169632 + 100.0 * 8.327664375305176
Epoch 2720, val loss: 0.6104016304016113
Epoch 2730, training loss: 833.1592407226562 = 0.1407289355993271 + 100.0 * 8.330184936523438
Epoch 2730, val loss: 0.6129482388496399
Epoch 2740, training loss: 833.2362670898438 = 0.1392456591129303 + 100.0 * 8.330970764160156
Epoch 2740, val loss: 0.6142582297325134
Epoch 2750, training loss: 832.962158203125 = 0.13778765499591827 + 100.0 * 8.328243255615234
Epoch 2750, val loss: 0.6155530214309692
Epoch 2760, training loss: 832.854248046875 = 0.13635598123073578 + 100.0 * 8.327178955078125
Epoch 2760, val loss: 0.6177954077720642
Epoch 2770, training loss: 832.9163818359375 = 0.13495345413684845 + 100.0 * 8.327814102172852
Epoch 2770, val loss: 0.6197583079338074
Epoch 2780, training loss: 832.9844970703125 = 0.1335466355085373 + 100.0 * 8.328509330749512
Epoch 2780, val loss: 0.621415376663208
Epoch 2790, training loss: 832.9854736328125 = 0.1321500986814499 + 100.0 * 8.328533172607422
Epoch 2790, val loss: 0.6227009296417236
Epoch 2800, training loss: 833.0958862304688 = 0.13077184557914734 + 100.0 * 8.32965087890625
Epoch 2800, val loss: 0.6245729923248291
Epoch 2810, training loss: 832.8893432617188 = 0.12938809394836426 + 100.0 * 8.32759952545166
Epoch 2810, val loss: 0.626640260219574
Epoch 2820, training loss: 832.7857666015625 = 0.1280261129140854 + 100.0 * 8.326577186584473
Epoch 2820, val loss: 0.628332257270813
Epoch 2830, training loss: 832.80615234375 = 0.1266889125108719 + 100.0 * 8.326794624328613
Epoch 2830, val loss: 0.6298365592956543
Epoch 2840, training loss: 833.581298828125 = 0.12541431188583374 + 100.0 * 8.334558486938477
Epoch 2840, val loss: 0.6312219500541687
Epoch 2850, training loss: 832.9698486328125 = 0.12410253286361694 + 100.0 * 8.328457832336426
Epoch 2850, val loss: 0.6347636580467224
Epoch 2860, training loss: 832.8052368164062 = 0.12273555994033813 + 100.0 * 8.326825141906738
Epoch 2860, val loss: 0.6355741024017334
Epoch 2870, training loss: 832.8281860351562 = 0.12144211679697037 + 100.0 * 8.327067375183105
Epoch 2870, val loss: 0.6380569338798523
Epoch 2880, training loss: 832.8609619140625 = 0.12015454471111298 + 100.0 * 8.327407836914062
Epoch 2880, val loss: 0.6396430134773254
Epoch 2890, training loss: 832.825439453125 = 0.11887957155704498 + 100.0 * 8.327065467834473
Epoch 2890, val loss: 0.6411755084991455
Epoch 2900, training loss: 832.8834838867188 = 0.11763036251068115 + 100.0 * 8.327658653259277
Epoch 2900, val loss: 0.6426863074302673
Epoch 2910, training loss: 832.6920166015625 = 0.11636982858181 + 100.0 * 8.325756072998047
Epoch 2910, val loss: 0.6453024744987488
Epoch 2920, training loss: 832.638916015625 = 0.11513777822256088 + 100.0 * 8.325238227844238
Epoch 2920, val loss: 0.6471057534217834
Epoch 2930, training loss: 832.9332885742188 = 0.11393194645643234 + 100.0 * 8.328193664550781
Epoch 2930, val loss: 0.6489716172218323
Epoch 2940, training loss: 832.807861328125 = 0.11270646005868912 + 100.0 * 8.326951026916504
Epoch 2940, val loss: 0.650716245174408
Epoch 2950, training loss: 832.7412109375 = 0.11149715632200241 + 100.0 * 8.32629680633545
Epoch 2950, val loss: 0.6521634459495544
Epoch 2960, training loss: 833.0596313476562 = 0.11030784249305725 + 100.0 * 8.329493522644043
Epoch 2960, val loss: 0.6542433500289917
Epoch 2970, training loss: 832.729248046875 = 0.10911522805690765 + 100.0 * 8.326201438903809
Epoch 2970, val loss: 0.6566898226737976
Epoch 2980, training loss: 832.59130859375 = 0.10792331397533417 + 100.0 * 8.324833869934082
Epoch 2980, val loss: 0.6582894325256348
Epoch 2990, training loss: 832.539794921875 = 0.10676733404397964 + 100.0 * 8.32433032989502
Epoch 2990, val loss: 0.6605215072631836
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7955352612886859
0.8419184235311165
=== training gcn model ===
Epoch 0, training loss: 1059.3177490234375 = 1.0904061794281006 + 100.0 * 10.582273483276367
Epoch 0, val loss: 1.088800311088562
Epoch 10, training loss: 1059.2208251953125 = 1.0848909616470337 + 100.0 * 10.58135986328125
Epoch 10, val loss: 1.0833004713058472
Epoch 20, training loss: 1058.3994140625 = 1.0784857273101807 + 100.0 * 10.573209762573242
Epoch 20, val loss: 1.0769609212875366
Epoch 30, training loss: 1052.7401123046875 = 1.0708798170089722 + 100.0 * 10.516692161560059
Epoch 30, val loss: 1.0694279670715332
Epoch 40, training loss: 1031.32373046875 = 1.062591552734375 + 100.0 * 10.302611351013184
Epoch 40, val loss: 1.061540126800537
Epoch 50, training loss: 993.0745239257812 = 1.0550122261047363 + 100.0 * 9.920195579528809
Epoch 50, val loss: 1.0543160438537598
Epoch 60, training loss: 961.4865112304688 = 1.0465567111968994 + 100.0 * 9.604399681091309
Epoch 60, val loss: 1.045767068862915
Epoch 70, training loss: 942.9710083007812 = 1.0363210439682007 + 100.0 * 9.419346809387207
Epoch 70, val loss: 1.0356953144073486
Epoch 80, training loss: 923.692138671875 = 1.028725504875183 + 100.0 * 9.22663402557373
Epoch 80, val loss: 1.028404951095581
Epoch 90, training loss: 917.3464965820312 = 1.0219660997390747 + 100.0 * 9.16324520111084
Epoch 90, val loss: 1.0217869281768799
Epoch 100, training loss: 907.1825561523438 = 1.0144624710083008 + 100.0 * 9.061680793762207
Epoch 100, val loss: 1.0145485401153564
Epoch 110, training loss: 900.2514038085938 = 1.00692617893219 + 100.0 * 8.99244499206543
Epoch 110, val loss: 1.0070903301239014
Epoch 120, training loss: 897.666015625 = 0.9977053999900818 + 100.0 * 8.966683387756348
Epoch 120, val loss: 0.9977365136146545
Epoch 130, training loss: 892.9112548828125 = 0.9872198700904846 + 100.0 * 8.91923999786377
Epoch 130, val loss: 0.9874629378318787
Epoch 140, training loss: 886.2446899414062 = 0.9782883524894714 + 100.0 * 8.85266399383545
Epoch 140, val loss: 0.9788041710853577
Epoch 150, training loss: 879.6013793945312 = 0.9702690839767456 + 100.0 * 8.786311149597168
Epoch 150, val loss: 0.9707132577896118
Epoch 160, training loss: 875.229736328125 = 0.960353434085846 + 100.0 * 8.742693901062012
Epoch 160, val loss: 0.9606161713600159
Epoch 170, training loss: 871.1925659179688 = 0.9481808543205261 + 100.0 * 8.702444076538086
Epoch 170, val loss: 0.9484279751777649
Epoch 180, training loss: 868.8150024414062 = 0.9342580437660217 + 100.0 * 8.678807258605957
Epoch 180, val loss: 0.9345408082008362
Epoch 190, training loss: 867.046875 = 0.9186609983444214 + 100.0 * 8.661282539367676
Epoch 190, val loss: 0.9191176891326904
Epoch 200, training loss: 865.4021606445312 = 0.9018346071243286 + 100.0 * 8.645003318786621
Epoch 200, val loss: 0.9026928544044495
Epoch 210, training loss: 863.8900146484375 = 0.8843766450881958 + 100.0 * 8.630056381225586
Epoch 210, val loss: 0.8856042623519897
Epoch 220, training loss: 862.2547607421875 = 0.8664835095405579 + 100.0 * 8.613883018493652
Epoch 220, val loss: 0.868197500705719
Epoch 230, training loss: 861.0122680664062 = 0.8481950163841248 + 100.0 * 8.601640701293945
Epoch 230, val loss: 0.8504615426063538
Epoch 240, training loss: 860.37353515625 = 0.829534113407135 + 100.0 * 8.595439910888672
Epoch 240, val loss: 0.8324633836746216
Epoch 250, training loss: 859.1182250976562 = 0.8106389045715332 + 100.0 * 8.583075523376465
Epoch 250, val loss: 0.8142672181129456
Epoch 260, training loss: 858.1095581054688 = 0.7917449474334717 + 100.0 * 8.5731782913208
Epoch 260, val loss: 0.796156644821167
Epoch 270, training loss: 857.2005004882812 = 0.7731109857559204 + 100.0 * 8.564273834228516
Epoch 270, val loss: 0.7783894538879395
Epoch 280, training loss: 856.2978515625 = 0.7549967765808105 + 100.0 * 8.555428504943848
Epoch 280, val loss: 0.7612321972846985
Epoch 290, training loss: 855.5031127929688 = 0.7374258637428284 + 100.0 * 8.547657012939453
Epoch 290, val loss: 0.7446167469024658
Epoch 300, training loss: 854.4327392578125 = 0.7204798460006714 + 100.0 * 8.53712272644043
Epoch 300, val loss: 0.7286633253097534
Epoch 310, training loss: 853.6090087890625 = 0.7042660713195801 + 100.0 * 8.529047012329102
Epoch 310, val loss: 0.7135294079780579
Epoch 320, training loss: 852.8491821289062 = 0.6887285709381104 + 100.0 * 8.521604537963867
Epoch 320, val loss: 0.698963463306427
Epoch 330, training loss: 852.0337524414062 = 0.6738654375076294 + 100.0 * 8.513598442077637
Epoch 330, val loss: 0.6851779222488403
Epoch 340, training loss: 851.3275146484375 = 0.6598879098892212 + 100.0 * 8.506675720214844
Epoch 340, val loss: 0.672239363193512
Epoch 350, training loss: 850.8955078125 = 0.6467691659927368 + 100.0 * 8.502487182617188
Epoch 350, val loss: 0.6601380705833435
Epoch 360, training loss: 850.1129760742188 = 0.6344624161720276 + 100.0 * 8.49478530883789
Epoch 360, val loss: 0.6489225625991821
Epoch 370, training loss: 849.4799194335938 = 0.6231663227081299 + 100.0 * 8.488567352294922
Epoch 370, val loss: 0.6386537551879883
Epoch 380, training loss: 849.2819213867188 = 0.6127842664718628 + 100.0 * 8.48669147491455
Epoch 380, val loss: 0.6292557120323181
Epoch 390, training loss: 848.6953125 = 0.6030561923980713 + 100.0 * 8.48092269897461
Epoch 390, val loss: 0.6206119060516357
Epoch 400, training loss: 848.0869750976562 = 0.5941774845123291 + 100.0 * 8.47492790222168
Epoch 400, val loss: 0.612679123878479
Epoch 410, training loss: 847.6492919921875 = 0.5861270427703857 + 100.0 * 8.47063159942627
Epoch 410, val loss: 0.6055845022201538
Epoch 420, training loss: 847.2167358398438 = 0.5787915587425232 + 100.0 * 8.466379165649414
Epoch 420, val loss: 0.5992237329483032
Epoch 430, training loss: 847.2174682617188 = 0.5720644593238831 + 100.0 * 8.466453552246094
Epoch 430, val loss: 0.5934683084487915
Epoch 440, training loss: 846.7085571289062 = 0.565797746181488 + 100.0 * 8.461427688598633
Epoch 440, val loss: 0.5880905389785767
Epoch 450, training loss: 846.33984375 = 0.5600572824478149 + 100.0 * 8.45779800415039
Epoch 450, val loss: 0.583299458026886
Epoch 460, training loss: 846.203125 = 0.5547950863838196 + 100.0 * 8.456482887268066
Epoch 460, val loss: 0.5789700150489807
Epoch 470, training loss: 845.80224609375 = 0.549940824508667 + 100.0 * 8.452523231506348
Epoch 470, val loss: 0.5750488042831421
Epoch 480, training loss: 845.494140625 = 0.5454999208450317 + 100.0 * 8.44948673248291
Epoch 480, val loss: 0.5715583562850952
Epoch 490, training loss: 845.7102661132812 = 0.5413743853569031 + 100.0 * 8.451688766479492
Epoch 490, val loss: 0.5684303641319275
Epoch 500, training loss: 845.2229614257812 = 0.5375007390975952 + 100.0 * 8.446854591369629
Epoch 500, val loss: 0.565331757068634
Epoch 510, training loss: 844.8617553710938 = 0.5339351892471313 + 100.0 * 8.443278312683105
Epoch 510, val loss: 0.562680184841156
Epoch 520, training loss: 844.7163696289062 = 0.5306670069694519 + 100.0 * 8.44185733795166
Epoch 520, val loss: 0.5603302121162415
Epoch 530, training loss: 844.8200073242188 = 0.5276352167129517 + 100.0 * 8.442923545837402
Epoch 530, val loss: 0.5581154823303223
Epoch 540, training loss: 844.5764770507812 = 0.5247034430503845 + 100.0 * 8.44051742553711
Epoch 540, val loss: 0.5560865998268127
Epoch 550, training loss: 844.28076171875 = 0.5219907164573669 + 100.0 * 8.43758773803711
Epoch 550, val loss: 0.5541546940803528
Epoch 560, training loss: 844.02783203125 = 0.5194903612136841 + 100.0 * 8.435083389282227
Epoch 560, val loss: 0.5524845123291016
Epoch 570, training loss: 843.8999633789062 = 0.5171565413475037 + 100.0 * 8.433828353881836
Epoch 570, val loss: 0.5510122179985046
Epoch 580, training loss: 844.1128540039062 = 0.5149219036102295 + 100.0 * 8.435979843139648
Epoch 580, val loss: 0.5494987964630127
Epoch 590, training loss: 843.701904296875 = 0.5127575993537903 + 100.0 * 8.431891441345215
Epoch 590, val loss: 0.548147976398468
Epoch 600, training loss: 843.3876342773438 = 0.5107685923576355 + 100.0 * 8.4287691116333
Epoch 600, val loss: 0.5467644333839417
Epoch 610, training loss: 843.46533203125 = 0.5088972449302673 + 100.0 * 8.429564476013184
Epoch 610, val loss: 0.5455388426780701
Epoch 620, training loss: 843.40087890625 = 0.5070228576660156 + 100.0 * 8.428938865661621
Epoch 620, val loss: 0.5444076061248779
Epoch 630, training loss: 842.95751953125 = 0.5052550435066223 + 100.0 * 8.424522399902344
Epoch 630, val loss: 0.5433306694030762
Epoch 640, training loss: 843.02734375 = 0.503612220287323 + 100.0 * 8.425237655639648
Epoch 640, val loss: 0.5423711538314819
Epoch 650, training loss: 842.609619140625 = 0.5019994378089905 + 100.0 * 8.421075820922852
Epoch 650, val loss: 0.5413380265235901
Epoch 660, training loss: 842.4969482421875 = 0.500461220741272 + 100.0 * 8.419964790344238
Epoch 660, val loss: 0.5404491424560547
Epoch 670, training loss: 842.3801879882812 = 0.49899953603744507 + 100.0 * 8.418811798095703
Epoch 670, val loss: 0.5395544171333313
Epoch 680, training loss: 842.4744873046875 = 0.49757590889930725 + 100.0 * 8.419769287109375
Epoch 680, val loss: 0.5387340188026428
Epoch 690, training loss: 842.0361938476562 = 0.49613556265830994 + 100.0 * 8.415400505065918
Epoch 690, val loss: 0.537950336933136
Epoch 700, training loss: 841.9601440429688 = 0.49475404620170593 + 100.0 * 8.414653778076172
Epoch 700, val loss: 0.5371001362800598
Epoch 710, training loss: 841.7872924804688 = 0.49346062541007996 + 100.0 * 8.412938117980957
Epoch 710, val loss: 0.5363905429840088
Epoch 720, training loss: 842.1254272460938 = 0.4921838343143463 + 100.0 * 8.416332244873047
Epoch 720, val loss: 0.5356478691101074
Epoch 730, training loss: 841.6625366210938 = 0.4908904433250427 + 100.0 * 8.41171646118164
Epoch 730, val loss: 0.5350885391235352
Epoch 740, training loss: 841.3050537109375 = 0.48966801166534424 + 100.0 * 8.408153533935547
Epoch 740, val loss: 0.5342723727226257
Epoch 750, training loss: 841.1534423828125 = 0.4884890615940094 + 100.0 * 8.406649589538574
Epoch 750, val loss: 0.5336569547653198
Epoch 760, training loss: 841.9068603515625 = 0.4873420298099518 + 100.0 * 8.41419506072998
Epoch 760, val loss: 0.5330147743225098
Epoch 770, training loss: 841.2360229492188 = 0.48604944348335266 + 100.0 * 8.407500267028809
Epoch 770, val loss: 0.5322404503822327
Epoch 780, training loss: 840.7960815429688 = 0.4848543703556061 + 100.0 * 8.403112411499023
Epoch 780, val loss: 0.5316849946975708
Epoch 790, training loss: 840.6580810546875 = 0.4837581217288971 + 100.0 * 8.401742935180664
Epoch 790, val loss: 0.5311353206634521
Epoch 800, training loss: 840.4795532226562 = 0.4826938807964325 + 100.0 * 8.399969100952148
Epoch 800, val loss: 0.5305898785591125
Epoch 810, training loss: 840.361328125 = 0.48164331912994385 + 100.0 * 8.398797035217285
Epoch 810, val loss: 0.5300286412239075
Epoch 820, training loss: 841.0979614257812 = 0.4805607497692108 + 100.0 * 8.406173706054688
Epoch 820, val loss: 0.5294745564460754
Epoch 830, training loss: 840.187744140625 = 0.4794293940067291 + 100.0 * 8.397083282470703
Epoch 830, val loss: 0.5288892984390259
Epoch 840, training loss: 840.0272216796875 = 0.4783516228199005 + 100.0 * 8.395488739013672
Epoch 840, val loss: 0.5283340215682983
Epoch 850, training loss: 839.9711303710938 = 0.47731661796569824 + 100.0 * 8.394938468933105
Epoch 850, val loss: 0.5277693271636963
Epoch 860, training loss: 840.0975341796875 = 0.4762857258319855 + 100.0 * 8.396212577819824
Epoch 860, val loss: 0.5272102355957031
Epoch 870, training loss: 840.285888671875 = 0.47521477937698364 + 100.0 * 8.398106575012207
Epoch 870, val loss: 0.5267549753189087
Epoch 880, training loss: 839.75341796875 = 0.47410404682159424 + 100.0 * 8.392792701721191
Epoch 880, val loss: 0.5261363387107849
Epoch 890, training loss: 839.5119018554688 = 0.4730842113494873 + 100.0 * 8.390388488769531
Epoch 890, val loss: 0.5255498290061951
Epoch 900, training loss: 839.4152221679688 = 0.4720879793167114 + 100.0 * 8.38943099975586
Epoch 900, val loss: 0.5250577330589294
Epoch 910, training loss: 839.857421875 = 0.4710875451564789 + 100.0 * 8.393863677978516
Epoch 910, val loss: 0.5244364738464355
Epoch 920, training loss: 839.3526000976562 = 0.47001200914382935 + 100.0 * 8.388825416564941
Epoch 920, val loss: 0.5241186022758484
Epoch 930, training loss: 839.2160034179688 = 0.4689805805683136 + 100.0 * 8.387470245361328
Epoch 930, val loss: 0.5233483910560608
Epoch 940, training loss: 839.09423828125 = 0.46796631813049316 + 100.0 * 8.386262893676758
Epoch 940, val loss: 0.5229426622390747
Epoch 950, training loss: 839.1707153320312 = 0.4669725298881531 + 100.0 * 8.38703727722168
Epoch 950, val loss: 0.5222668051719666
Epoch 960, training loss: 839.1363525390625 = 0.4659086763858795 + 100.0 * 8.386704444885254
Epoch 960, val loss: 0.5219369530677795
Epoch 970, training loss: 838.9100341796875 = 0.4648345112800598 + 100.0 * 8.384451866149902
Epoch 970, val loss: 0.5211973786354065
Epoch 980, training loss: 838.7940063476562 = 0.46380364894866943 + 100.0 * 8.383301734924316
Epoch 980, val loss: 0.5207912921905518
Epoch 990, training loss: 838.7989501953125 = 0.4627901613712311 + 100.0 * 8.38336181640625
Epoch 990, val loss: 0.5201842188835144
Epoch 1000, training loss: 838.9009399414062 = 0.4617360830307007 + 100.0 * 8.384391784667969
Epoch 1000, val loss: 0.5196423530578613
Epoch 1010, training loss: 838.6361083984375 = 0.4606774151325226 + 100.0 * 8.381753921508789
Epoch 1010, val loss: 0.5190513730049133
Epoch 1020, training loss: 838.50048828125 = 0.459626704454422 + 100.0 * 8.38040828704834
Epoch 1020, val loss: 0.5185146927833557
Epoch 1030, training loss: 838.8250732421875 = 0.4585859477519989 + 100.0 * 8.383665084838867
Epoch 1030, val loss: 0.5179831981658936
Epoch 1040, training loss: 838.3914184570312 = 0.45749950408935547 + 100.0 * 8.379339218139648
Epoch 1040, val loss: 0.5173183083534241
Epoch 1050, training loss: 838.6160888671875 = 0.4564214050769806 + 100.0 * 8.381596565246582
Epoch 1050, val loss: 0.5168604850769043
Epoch 1060, training loss: 838.281494140625 = 0.4553074836730957 + 100.0 * 8.37826156616211
Epoch 1060, val loss: 0.5161046981811523
Epoch 1070, training loss: 838.1997680664062 = 0.4542030990123749 + 100.0 * 8.377455711364746
Epoch 1070, val loss: 0.5156590938568115
Epoch 1080, training loss: 838.0802612304688 = 0.45312851667404175 + 100.0 * 8.37627124786377
Epoch 1080, val loss: 0.5150580406188965
Epoch 1090, training loss: 838.0478515625 = 0.45204877853393555 + 100.0 * 8.375958442687988
Epoch 1090, val loss: 0.5145538449287415
Epoch 1100, training loss: 838.9891967773438 = 0.45095255970954895 + 100.0 * 8.385382652282715
Epoch 1100, val loss: 0.5141029953956604
Epoch 1110, training loss: 838.3159790039062 = 0.44974279403686523 + 100.0 * 8.378662109375
Epoch 1110, val loss: 0.5131163001060486
Epoch 1120, training loss: 838.0525512695312 = 0.44858452677726746 + 100.0 * 8.376039505004883
Epoch 1120, val loss: 0.5127418637275696
Epoch 1130, training loss: 838.0503540039062 = 0.44744208455085754 + 100.0 * 8.376029014587402
Epoch 1130, val loss: 0.512057900428772
Epoch 1140, training loss: 837.9138793945312 = 0.446294367313385 + 100.0 * 8.374675750732422
Epoch 1140, val loss: 0.5113548040390015
Epoch 1150, training loss: 838.1830444335938 = 0.44513025879859924 + 100.0 * 8.377379417419434
Epoch 1150, val loss: 0.5108236074447632
Epoch 1160, training loss: 837.7725219726562 = 0.44391393661499023 + 100.0 * 8.373286247253418
Epoch 1160, val loss: 0.510303258895874
Epoch 1170, training loss: 837.5947265625 = 0.44274047017097473 + 100.0 * 8.371520042419434
Epoch 1170, val loss: 0.5096601247787476
Epoch 1180, training loss: 837.5860595703125 = 0.44156959652900696 + 100.0 * 8.371444702148438
Epoch 1180, val loss: 0.5090715885162354
Epoch 1190, training loss: 837.5272216796875 = 0.4403849244117737 + 100.0 * 8.370868682861328
Epoch 1190, val loss: 0.5084439516067505
Epoch 1200, training loss: 838.0316162109375 = 0.439190536737442 + 100.0 * 8.375924110412598
Epoch 1200, val loss: 0.5076977014541626
Epoch 1210, training loss: 838.1484375 = 0.4378623366355896 + 100.0 * 8.377105712890625
Epoch 1210, val loss: 0.5072859525680542
Epoch 1220, training loss: 837.5103149414062 = 0.43653950095176697 + 100.0 * 8.37073802947998
Epoch 1220, val loss: 0.5065678954124451
Epoch 1230, training loss: 837.3182983398438 = 0.43528103828430176 + 100.0 * 8.368829727172852
Epoch 1230, val loss: 0.5058595538139343
Epoch 1240, training loss: 837.277587890625 = 0.4340432584285736 + 100.0 * 8.368435859680176
Epoch 1240, val loss: 0.5051507949829102
Epoch 1250, training loss: 837.2072143554688 = 0.43279924988746643 + 100.0 * 8.367744445800781
Epoch 1250, val loss: 0.5045995712280273
Epoch 1260, training loss: 837.8773193359375 = 0.43154338002204895 + 100.0 * 8.374458312988281
Epoch 1260, val loss: 0.5039891004562378
Epoch 1270, training loss: 837.7205810546875 = 0.43014395236968994 + 100.0 * 8.372904777526855
Epoch 1270, val loss: 0.5033842921257019
Epoch 1280, training loss: 837.1410522460938 = 0.4287698566913605 + 100.0 * 8.367122650146484
Epoch 1280, val loss: 0.5026407241821289
Epoch 1290, training loss: 837.0752563476562 = 0.42744213342666626 + 100.0 * 8.366477966308594
Epoch 1290, val loss: 0.5019535422325134
Epoch 1300, training loss: 836.9730224609375 = 0.426116943359375 + 100.0 * 8.365468978881836
Epoch 1300, val loss: 0.5013781189918518
Epoch 1310, training loss: 836.9224853515625 = 0.4247760772705078 + 100.0 * 8.36497688293457
Epoch 1310, val loss: 0.5007826685905457
Epoch 1320, training loss: 837.3287963867188 = 0.42341023683547974 + 100.0 * 8.369053840637207
Epoch 1320, val loss: 0.5001345276832581
Epoch 1330, training loss: 837.40478515625 = 0.42195284366607666 + 100.0 * 8.369828224182129
Epoch 1330, val loss: 0.49946174025535583
Epoch 1340, training loss: 836.9002075195312 = 0.4204765856266022 + 100.0 * 8.364797592163086
Epoch 1340, val loss: 0.4986671507358551
Epoch 1350, training loss: 836.7765502929688 = 0.41903868317604065 + 100.0 * 8.363574981689453
Epoch 1350, val loss: 0.4980108439922333
Epoch 1360, training loss: 836.6981201171875 = 0.4176158905029297 + 100.0 * 8.362805366516113
Epoch 1360, val loss: 0.4974035620689392
Epoch 1370, training loss: 836.6763305664062 = 0.4161866009235382 + 100.0 * 8.362601280212402
Epoch 1370, val loss: 0.49676087498664856
Epoch 1380, training loss: 837.6251220703125 = 0.41472020745277405 + 100.0 * 8.372103691101074
Epoch 1380, val loss: 0.4961737394332886
Epoch 1390, training loss: 836.8209838867188 = 0.41314437985420227 + 100.0 * 8.364078521728516
Epoch 1390, val loss: 0.49525997042655945
Epoch 1400, training loss: 836.574951171875 = 0.41161400079727173 + 100.0 * 8.36163330078125
Epoch 1400, val loss: 0.49451446533203125
Epoch 1410, training loss: 836.6611938476562 = 0.4100976884365082 + 100.0 * 8.362510681152344
Epoch 1410, val loss: 0.49396347999572754
Epoch 1420, training loss: 836.8748168945312 = 0.4085264205932617 + 100.0 * 8.364663124084473
Epoch 1420, val loss: 0.49322354793548584
Epoch 1430, training loss: 836.4862060546875 = 0.4069286286830902 + 100.0 * 8.360793113708496
Epoch 1430, val loss: 0.4925130307674408
Epoch 1440, training loss: 836.3477783203125 = 0.4053462743759155 + 100.0 * 8.359424591064453
Epoch 1440, val loss: 0.4918561577796936
Epoch 1450, training loss: 836.2833862304688 = 0.403764545917511 + 100.0 * 8.358796119689941
Epoch 1450, val loss: 0.4911976754665375
Epoch 1460, training loss: 836.2628173828125 = 0.40217629075050354 + 100.0 * 8.358606338500977
Epoch 1460, val loss: 0.490599125623703
Epoch 1470, training loss: 837.1128540039062 = 0.4005618095397949 + 100.0 * 8.367122650146484
Epoch 1470, val loss: 0.4901292026042938
Epoch 1480, training loss: 836.37841796875 = 0.39884984493255615 + 100.0 * 8.359795570373535
Epoch 1480, val loss: 0.48909950256347656
Epoch 1490, training loss: 836.1429443359375 = 0.3971681594848633 + 100.0 * 8.357458114624023
Epoch 1490, val loss: 0.48843854665756226
Epoch 1500, training loss: 836.6736450195312 = 0.39548957347869873 + 100.0 * 8.362781524658203
Epoch 1500, val loss: 0.4878881871700287
Epoch 1510, training loss: 836.0486450195312 = 0.3937397301197052 + 100.0 * 8.356549263000488
Epoch 1510, val loss: 0.48709672689437866
Epoch 1520, training loss: 836.011962890625 = 0.3920179307460785 + 100.0 * 8.356199264526367
Epoch 1520, val loss: 0.48646602034568787
Epoch 1530, training loss: 835.9541015625 = 0.39031240344047546 + 100.0 * 8.355637550354004
Epoch 1530, val loss: 0.4858217239379883
Epoch 1540, training loss: 835.9273071289062 = 0.3886093199253082 + 100.0 * 8.355386734008789
Epoch 1540, val loss: 0.4852430522441864
Epoch 1550, training loss: 835.9485473632812 = 0.38689011335372925 + 100.0 * 8.355616569519043
Epoch 1550, val loss: 0.4846072793006897
Epoch 1560, training loss: 836.66650390625 = 0.38512909412384033 + 100.0 * 8.362813949584961
Epoch 1560, val loss: 0.4839223027229309
Epoch 1570, training loss: 836.1371459960938 = 0.3833056092262268 + 100.0 * 8.357538223266602
Epoch 1570, val loss: 0.4834577143192291
Epoch 1580, training loss: 835.9637451171875 = 0.38149747252464294 + 100.0 * 8.355822563171387
Epoch 1580, val loss: 0.48278528451919556
Epoch 1590, training loss: 835.7677612304688 = 0.3797007203102112 + 100.0 * 8.353880882263184
Epoch 1590, val loss: 0.4821697771549225
Epoch 1600, training loss: 836.010986328125 = 0.37791717052459717 + 100.0 * 8.356330871582031
Epoch 1600, val loss: 0.48159030079841614
Epoch 1610, training loss: 836.0728149414062 = 0.37608784437179565 + 100.0 * 8.356966972351074
Epoch 1610, val loss: 0.4808805584907532
Epoch 1620, training loss: 835.6482543945312 = 0.37421485781669617 + 100.0 * 8.352740287780762
Epoch 1620, val loss: 0.48035237193107605
Epoch 1630, training loss: 835.6126708984375 = 0.37238916754722595 + 100.0 * 8.352402687072754
Epoch 1630, val loss: 0.47969305515289307
Epoch 1640, training loss: 835.560546875 = 0.3705628216266632 + 100.0 * 8.351900100708008
Epoch 1640, val loss: 0.47927165031433105
Epoch 1650, training loss: 835.72705078125 = 0.3687300384044647 + 100.0 * 8.353583335876465
Epoch 1650, val loss: 0.478803426027298
Epoch 1660, training loss: 835.7927856445312 = 0.366839200258255 + 100.0 * 8.354259490966797
Epoch 1660, val loss: 0.4780651330947876
Epoch 1670, training loss: 835.608154296875 = 0.3649160861968994 + 100.0 * 8.352432250976562
Epoch 1670, val loss: 0.4776274263858795
Epoch 1680, training loss: 835.6210327148438 = 0.3630296289920807 + 100.0 * 8.352580070495605
Epoch 1680, val loss: 0.4769536256790161
Epoch 1690, training loss: 835.4495239257812 = 0.36114302277565 + 100.0 * 8.350883483886719
Epoch 1690, val loss: 0.4766404926776886
Epoch 1700, training loss: 835.41748046875 = 0.3592784106731415 + 100.0 * 8.350582122802734
Epoch 1700, val loss: 0.4760623574256897
Epoch 1710, training loss: 835.5039672851562 = 0.3574034869670868 + 100.0 * 8.351465225219727
Epoch 1710, val loss: 0.4756148159503937
Epoch 1720, training loss: 835.3475341796875 = 0.35550400614738464 + 100.0 * 8.349920272827148
Epoch 1720, val loss: 0.4752862751483917
Epoch 1730, training loss: 835.9827880859375 = 0.3536239564418793 + 100.0 * 8.356291770935059
Epoch 1730, val loss: 0.4749515652656555
Epoch 1740, training loss: 835.3767700195312 = 0.35163989663124084 + 100.0 * 8.350251197814941
Epoch 1740, val loss: 0.4744167923927307
Epoch 1750, training loss: 835.2933959960938 = 0.3497149646282196 + 100.0 * 8.34943675994873
Epoch 1750, val loss: 0.4741005003452301
Epoch 1760, training loss: 835.2310791015625 = 0.34780797362327576 + 100.0 * 8.348833084106445
Epoch 1760, val loss: 0.4736451506614685
Epoch 1770, training loss: 835.3658447265625 = 0.3459186255931854 + 100.0 * 8.350198745727539
Epoch 1770, val loss: 0.4734308421611786
Epoch 1780, training loss: 835.234619140625 = 0.3439823389053345 + 100.0 * 8.348906517028809
Epoch 1780, val loss: 0.47295448184013367
Epoch 1790, training loss: 835.209716796875 = 0.34205731749534607 + 100.0 * 8.348676681518555
Epoch 1790, val loss: 0.4725585877895355
Epoch 1800, training loss: 835.2164916992188 = 0.34014037251472473 + 100.0 * 8.348763465881348
Epoch 1800, val loss: 0.47247785329818726
Epoch 1810, training loss: 835.1773681640625 = 0.3382086157798767 + 100.0 * 8.34839153289795
Epoch 1810, val loss: 0.4720647633075714
Epoch 1820, training loss: 835.1958618164062 = 0.33628764748573303 + 100.0 * 8.34859561920166
Epoch 1820, val loss: 0.4715002477169037
Epoch 1830, training loss: 835.2969360351562 = 0.33434879779815674 + 100.0 * 8.349625587463379
Epoch 1830, val loss: 0.47148677706718445
Epoch 1840, training loss: 835.142822265625 = 0.33239561319351196 + 100.0 * 8.348104476928711
Epoch 1840, val loss: 0.47120171785354614
Epoch 1850, training loss: 835.0997924804688 = 0.3304469585418701 + 100.0 * 8.34769344329834
Epoch 1850, val loss: 0.47104302048683167
Epoch 1860, training loss: 835.034423828125 = 0.32850125432014465 + 100.0 * 8.34705924987793
Epoch 1860, val loss: 0.4709145426750183
Epoch 1870, training loss: 834.9240112304688 = 0.3265668451786041 + 100.0 * 8.345974922180176
Epoch 1870, val loss: 0.47059527039527893
Epoch 1880, training loss: 834.8971557617188 = 0.32464683055877686 + 100.0 * 8.345725059509277
Epoch 1880, val loss: 0.4705069065093994
Epoch 1890, training loss: 835.342529296875 = 0.3227252662181854 + 100.0 * 8.350197792053223
Epoch 1890, val loss: 0.4702989459037781
Epoch 1900, training loss: 835.1041870117188 = 0.3207469582557678 + 100.0 * 8.347834587097168
Epoch 1900, val loss: 0.47032514214515686
Epoch 1910, training loss: 834.859130859375 = 0.3187672197818756 + 100.0 * 8.345403671264648
Epoch 1910, val loss: 0.4701210856437683
Epoch 1920, training loss: 834.7769775390625 = 0.316818505525589 + 100.0 * 8.34460163116455
Epoch 1920, val loss: 0.47007519006729126
Epoch 1930, training loss: 834.774169921875 = 0.31488263607025146 + 100.0 * 8.344593048095703
Epoch 1930, val loss: 0.46990588307380676
Epoch 1940, training loss: 835.1336059570312 = 0.3129555284976959 + 100.0 * 8.348206520080566
Epoch 1940, val loss: 0.469799667596817
Epoch 1950, training loss: 834.82958984375 = 0.310971200466156 + 100.0 * 8.345186233520508
Epoch 1950, val loss: 0.4702921509742737
Epoch 1960, training loss: 835.1487426757812 = 0.3090210556983948 + 100.0 * 8.348397254943848
Epoch 1960, val loss: 0.4700031876564026
Epoch 1970, training loss: 834.8778076171875 = 0.3070277273654938 + 100.0 * 8.345707893371582
Epoch 1970, val loss: 0.47003117203712463
Epoch 1980, training loss: 834.70654296875 = 0.3050759732723236 + 100.0 * 8.344015121459961
Epoch 1980, val loss: 0.47009047865867615
Epoch 1990, training loss: 834.6422119140625 = 0.3031497299671173 + 100.0 * 8.343390464782715
Epoch 1990, val loss: 0.470158189535141
Epoch 2000, training loss: 834.7133178710938 = 0.3012318015098572 + 100.0 * 8.344120979309082
Epoch 2000, val loss: 0.4702467918395996
Epoch 2010, training loss: 834.7551879882812 = 0.29930055141448975 + 100.0 * 8.344558715820312
Epoch 2010, val loss: 0.47040480375289917
Epoch 2020, training loss: 834.6290893554688 = 0.297364741563797 + 100.0 * 8.343317031860352
Epoch 2020, val loss: 0.47069889307022095
Epoch 2030, training loss: 834.6830444335938 = 0.2954501807689667 + 100.0 * 8.343875885009766
Epoch 2030, val loss: 0.47069597244262695
Epoch 2040, training loss: 835.2487182617188 = 0.29353681206703186 + 100.0 * 8.349552154541016
Epoch 2040, val loss: 0.47088518738746643
Epoch 2050, training loss: 834.6416015625 = 0.29156258702278137 + 100.0 * 8.343500137329102
Epoch 2050, val loss: 0.470791757106781
Epoch 2060, training loss: 834.5008544921875 = 0.28963586688041687 + 100.0 * 8.34211254119873
Epoch 2060, val loss: 0.47117704153060913
Epoch 2070, training loss: 834.45556640625 = 0.28772932291030884 + 100.0 * 8.341678619384766
Epoch 2070, val loss: 0.47128984332084656
Epoch 2080, training loss: 834.4356689453125 = 0.28581878542900085 + 100.0 * 8.341498374938965
Epoch 2080, val loss: 0.47157537937164307
Epoch 2090, training loss: 834.70263671875 = 0.28391891717910767 + 100.0 * 8.344186782836914
Epoch 2090, val loss: 0.47194716334342957
Epoch 2100, training loss: 834.498291015625 = 0.28196796774864197 + 100.0 * 8.3421630859375
Epoch 2100, val loss: 0.4719005525112152
Epoch 2110, training loss: 834.4522094726562 = 0.28002598881721497 + 100.0 * 8.341721534729004
Epoch 2110, val loss: 0.4720067083835602
Epoch 2120, training loss: 834.3969116210938 = 0.2780974507331848 + 100.0 * 8.341188430786133
Epoch 2120, val loss: 0.472308486700058
Epoch 2130, training loss: 834.7490234375 = 0.27618780732154846 + 100.0 * 8.344728469848633
Epoch 2130, val loss: 0.4723557233810425
Epoch 2140, training loss: 834.4027099609375 = 0.27422723174095154 + 100.0 * 8.34128475189209
Epoch 2140, val loss: 0.4730091691017151
Epoch 2150, training loss: 834.425048828125 = 0.27230212092399597 + 100.0 * 8.341527938842773
Epoch 2150, val loss: 0.47343388199806213
Epoch 2160, training loss: 834.3766479492188 = 0.27037912607192993 + 100.0 * 8.341062545776367
Epoch 2160, val loss: 0.47356659173965454
Epoch 2170, training loss: 834.3757934570312 = 0.26848092675209045 + 100.0 * 8.341073036193848
Epoch 2170, val loss: 0.4741055965423584
Epoch 2180, training loss: 834.4471435546875 = 0.2665727138519287 + 100.0 * 8.341805458068848
Epoch 2180, val loss: 0.47435083985328674
Epoch 2190, training loss: 834.402587890625 = 0.264660120010376 + 100.0 * 8.341379165649414
Epoch 2190, val loss: 0.4743127226829529
Epoch 2200, training loss: 834.3412475585938 = 0.26275959610939026 + 100.0 * 8.340785026550293
Epoch 2200, val loss: 0.47482019662857056
Epoch 2210, training loss: 834.4533081054688 = 0.26087096333503723 + 100.0 * 8.341924667358398
Epoch 2210, val loss: 0.47521230578422546
Epoch 2220, training loss: 834.2228393554688 = 0.2589667737483978 + 100.0 * 8.339638710021973
Epoch 2220, val loss: 0.4755837321281433
Epoch 2230, training loss: 834.2805786132812 = 0.2570864260196686 + 100.0 * 8.340234756469727
Epoch 2230, val loss: 0.4761086702346802
Epoch 2240, training loss: 834.344970703125 = 0.25521254539489746 + 100.0 * 8.340897560119629
Epoch 2240, val loss: 0.4762883484363556
Epoch 2250, training loss: 834.4036254882812 = 0.25332266092300415 + 100.0 * 8.341503143310547
Epoch 2250, val loss: 0.4769136905670166
Epoch 2260, training loss: 834.1865844726562 = 0.2514219582080841 + 100.0 * 8.339351654052734
Epoch 2260, val loss: 0.477218896150589
Epoch 2270, training loss: 834.0972900390625 = 0.2495446652173996 + 100.0 * 8.33847713470459
Epoch 2270, val loss: 0.4778110384941101
Epoch 2280, training loss: 834.3948364257812 = 0.24769584834575653 + 100.0 * 8.341471672058105
Epoch 2280, val loss: 0.47846442461013794
Epoch 2290, training loss: 834.125 = 0.2457987368106842 + 100.0 * 8.338791847229004
Epoch 2290, val loss: 0.47846537828445435
