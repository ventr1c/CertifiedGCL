Begin epxeriment: cont_weight: 50 epoch:1000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0001, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11538])
remove edge: torch.Size([2, 9432])
updated graph: torch.Size([2, 10414])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:224: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 431.80389404296875 = 1.9605913162231445 + 50.0 * 8.5968656539917
Epoch 0, val loss: 1.9682382345199585
Epoch 10, training loss: 431.7904968261719 = 1.9559463262557983 + 50.0 * 8.596691131591797
Epoch 10, val loss: 1.9637415409088135
Epoch 20, training loss: 431.76019287109375 = 1.9507920742034912 + 50.0 * 8.596187591552734
Epoch 20, val loss: 1.95859694480896
Epoch 30, training loss: 431.6704406738281 = 1.9448611736297607 + 50.0 * 8.594511985778809
Epoch 30, val loss: 1.95248281955719
Epoch 40, training loss: 431.3522644042969 = 1.9377236366271973 + 50.0 * 8.58829116821289
Epoch 40, val loss: 1.9449059963226318
Epoch 50, training loss: 430.1731262207031 = 1.9289302825927734 + 50.0 * 8.564884185791016
Epoch 50, val loss: 1.9353594779968262
Epoch 60, training loss: 425.96331787109375 = 1.9183433055877686 + 50.0 * 8.480899810791016
Epoch 60, val loss: 1.9237250089645386
Epoch 70, training loss: 411.4322204589844 = 1.9069305658340454 + 50.0 * 8.190505981445312
Epoch 70, val loss: 1.9111721515655518
Epoch 80, training loss: 376.7086486816406 = 1.8943687677383423 + 50.0 * 7.496285438537598
Epoch 80, val loss: 1.8974952697753906
Epoch 90, training loss: 367.8027038574219 = 1.8855621814727783 + 50.0 * 7.318342685699463
Epoch 90, val loss: 1.888850212097168
Epoch 100, training loss: 357.691650390625 = 1.8804925680160522 + 50.0 * 7.116222858428955
Epoch 100, val loss: 1.8836840391159058
Epoch 110, training loss: 350.6698913574219 = 1.8744826316833496 + 50.0 * 6.975908279418945
Epoch 110, val loss: 1.8772872686386108
Epoch 120, training loss: 346.32568359375 = 1.8686456680297852 + 50.0 * 6.889140605926514
Epoch 120, val loss: 1.8712495565414429
Epoch 130, training loss: 343.6007080078125 = 1.8632079362869263 + 50.0 * 6.834749698638916
Epoch 130, val loss: 1.8655714988708496
Epoch 140, training loss: 340.9990539550781 = 1.8574835062026978 + 50.0 * 6.78283166885376
Epoch 140, val loss: 1.8597614765167236
Epoch 150, training loss: 338.55767822265625 = 1.8522822856903076 + 50.0 * 6.734107494354248
Epoch 150, val loss: 1.8544851541519165
Epoch 160, training loss: 336.50250244140625 = 1.8473773002624512 + 50.0 * 6.6931023597717285
Epoch 160, val loss: 1.8495820760726929
Epoch 170, training loss: 334.707763671875 = 1.8425469398498535 + 50.0 * 6.657304286956787
Epoch 170, val loss: 1.8447473049163818
Epoch 180, training loss: 333.1178894042969 = 1.8379303216934204 + 50.0 * 6.625599384307861
Epoch 180, val loss: 1.8401139974594116
Epoch 190, training loss: 331.9468688964844 = 1.8335702419281006 + 50.0 * 6.602266311645508
Epoch 190, val loss: 1.835695505142212
Epoch 200, training loss: 330.774169921875 = 1.8292832374572754 + 50.0 * 6.578897953033447
Epoch 200, val loss: 1.831472635269165
Epoch 210, training loss: 329.8262634277344 = 1.8251315355300903 + 50.0 * 6.560022354125977
Epoch 210, val loss: 1.8273500204086304
Epoch 220, training loss: 328.9529113769531 = 1.8211469650268555 + 50.0 * 6.542635440826416
Epoch 220, val loss: 1.823376178741455
Epoch 230, training loss: 328.1748352050781 = 1.8172574043273926 + 50.0 * 6.527151584625244
Epoch 230, val loss: 1.8195399045944214
Epoch 240, training loss: 327.4604797363281 = 1.8134567737579346 + 50.0 * 6.512940406799316
Epoch 240, val loss: 1.8157747983932495
Epoch 250, training loss: 326.7952575683594 = 1.8097001314163208 + 50.0 * 6.499711513519287
Epoch 250, val loss: 1.8120628595352173
Epoch 260, training loss: 326.1611633300781 = 1.8059706687927246 + 50.0 * 6.4871039390563965
Epoch 260, val loss: 1.8084285259246826
Epoch 270, training loss: 325.6058349609375 = 1.8022514581680298 + 50.0 * 6.476071834564209
Epoch 270, val loss: 1.804807424545288
Epoch 280, training loss: 325.0235900878906 = 1.798446536064148 + 50.0 * 6.464503288269043
Epoch 280, val loss: 1.80120050907135
Epoch 290, training loss: 324.48565673828125 = 1.7946133613586426 + 50.0 * 6.453820705413818
Epoch 290, val loss: 1.7975753545761108
Epoch 300, training loss: 324.0 = 1.7907170057296753 + 50.0 * 6.444185733795166
Epoch 300, val loss: 1.7939233779907227
Epoch 310, training loss: 323.63092041015625 = 1.7867275476455688 + 50.0 * 6.436883926391602
Epoch 310, val loss: 1.7902063131332397
Epoch 320, training loss: 323.1463317871094 = 1.7825756072998047 + 50.0 * 6.427275657653809
Epoch 320, val loss: 1.7863938808441162
Epoch 330, training loss: 322.7684020996094 = 1.778299331665039 + 50.0 * 6.419802188873291
Epoch 330, val loss: 1.7825056314468384
Epoch 340, training loss: 322.4201965332031 = 1.7738847732543945 + 50.0 * 6.412926197052002
Epoch 340, val loss: 1.7785190343856812
Epoch 350, training loss: 322.156494140625 = 1.769271969795227 + 50.0 * 6.407744407653809
Epoch 350, val loss: 1.7743853330612183
Epoch 360, training loss: 321.8542175292969 = 1.7645182609558105 + 50.0 * 6.401793956756592
Epoch 360, val loss: 1.7701314687728882
Epoch 370, training loss: 321.5474853515625 = 1.7595551013946533 + 50.0 * 6.395758628845215
Epoch 370, val loss: 1.765739917755127
Epoch 380, training loss: 321.2880859375 = 1.7544059753417969 + 50.0 * 6.390674114227295
Epoch 380, val loss: 1.7611974477767944
Epoch 390, training loss: 321.036865234375 = 1.7490410804748535 + 50.0 * 6.385756492614746
Epoch 390, val loss: 1.7565008401870728
Epoch 400, training loss: 320.8038024902344 = 1.7434406280517578 + 50.0 * 6.38120698928833
Epoch 400, val loss: 1.7516326904296875
Epoch 410, training loss: 320.6533203125 = 1.7375996112823486 + 50.0 * 6.37831449508667
Epoch 410, val loss: 1.7465687990188599
Epoch 420, training loss: 320.3924255371094 = 1.7314947843551636 + 50.0 * 6.373218536376953
Epoch 420, val loss: 1.7412946224212646
Epoch 430, training loss: 320.1649169921875 = 1.7251362800598145 + 50.0 * 6.368795871734619
Epoch 430, val loss: 1.7358242273330688
Epoch 440, training loss: 319.9599304199219 = 1.71852707862854 + 50.0 * 6.364828109741211
Epoch 440, val loss: 1.7301610708236694
Epoch 450, training loss: 319.7867736816406 = 1.7116267681121826 + 50.0 * 6.361502647399902
Epoch 450, val loss: 1.7242646217346191
Epoch 460, training loss: 319.5808410644531 = 1.7044625282287598 + 50.0 * 6.357527256011963
Epoch 460, val loss: 1.7181349992752075
Epoch 470, training loss: 319.39434814453125 = 1.697018027305603 + 50.0 * 6.353946685791016
Epoch 470, val loss: 1.7117975950241089
Epoch 480, training loss: 319.2039489746094 = 1.6892908811569214 + 50.0 * 6.350293159484863
Epoch 480, val loss: 1.7052470445632935
Epoch 490, training loss: 319.19134521484375 = 1.681256651878357 + 50.0 * 6.3502020835876465
Epoch 490, val loss: 1.6984704732894897
Epoch 500, training loss: 318.88153076171875 = 1.672966480255127 + 50.0 * 6.344171047210693
Epoch 500, val loss: 1.6914551258087158
Epoch 510, training loss: 318.69708251953125 = 1.6643755435943604 + 50.0 * 6.340654373168945
Epoch 510, val loss: 1.6842375993728638
Epoch 520, training loss: 318.5400695800781 = 1.6555217504501343 + 50.0 * 6.337690830230713
Epoch 520, val loss: 1.6768243312835693
Epoch 530, training loss: 318.5213317871094 = 1.646361231803894 + 50.0 * 6.337499618530273
Epoch 530, val loss: 1.6692047119140625
Epoch 540, training loss: 318.24334716796875 = 1.636988878250122 + 50.0 * 6.332127094268799
Epoch 540, val loss: 1.6613824367523193
Epoch 550, training loss: 318.0863037109375 = 1.6273560523986816 + 50.0 * 6.329179286956787
Epoch 550, val loss: 1.6533880233764648
Epoch 560, training loss: 317.947265625 = 1.6174771785736084 + 50.0 * 6.326595783233643
Epoch 560, val loss: 1.6452363729476929
Epoch 570, training loss: 317.953125 = 1.6073864698410034 + 50.0 * 6.3269147872924805
Epoch 570, val loss: 1.636926531791687
Epoch 580, training loss: 317.73150634765625 = 1.5969620943069458 + 50.0 * 6.322690963745117
Epoch 580, val loss: 1.6284186840057373
Epoch 590, training loss: 317.5649719238281 = 1.5864267349243164 + 50.0 * 6.319571018218994
Epoch 590, val loss: 1.6198209524154663
Epoch 600, training loss: 317.4341125488281 = 1.5757229328155518 + 50.0 * 6.31716775894165
Epoch 600, val loss: 1.6111364364624023
Epoch 610, training loss: 317.3114929199219 = 1.564904808998108 + 50.0 * 6.314931869506836
Epoch 610, val loss: 1.6024281978607178
Epoch 620, training loss: 317.1990966796875 = 1.5538915395736694 + 50.0 * 6.312903881072998
Epoch 620, val loss: 1.5935877561569214
Epoch 630, training loss: 317.22027587890625 = 1.5427356958389282 + 50.0 * 6.31355094909668
Epoch 630, val loss: 1.584657073020935
Epoch 640, training loss: 316.9907531738281 = 1.5314503908157349 + 50.0 * 6.309185981750488
Epoch 640, val loss: 1.5757427215576172
Epoch 650, training loss: 316.89239501953125 = 1.52009916305542 + 50.0 * 6.307445526123047
Epoch 650, val loss: 1.5668197870254517
Epoch 660, training loss: 316.78448486328125 = 1.5087164640426636 + 50.0 * 6.305515289306641
Epoch 660, val loss: 1.5578867197036743
Epoch 670, training loss: 316.6758728027344 = 1.497266173362732 + 50.0 * 6.303572177886963
Epoch 670, val loss: 1.548974633216858
Epoch 680, training loss: 316.57830810546875 = 1.485776424407959 + 50.0 * 6.30185079574585
Epoch 680, val loss: 1.5401030778884888
Epoch 690, training loss: 316.6623840332031 = 1.4742140769958496 + 50.0 * 6.303763389587402
Epoch 690, val loss: 1.5312621593475342
Epoch 700, training loss: 316.471435546875 = 1.4627348184585571 + 50.0 * 6.300173759460449
Epoch 700, val loss: 1.522464632987976
Epoch 710, training loss: 316.3351745605469 = 1.4511624574661255 + 50.0 * 6.297679901123047
Epoch 710, val loss: 1.5137221813201904
Epoch 720, training loss: 316.2186279296875 = 1.4396555423736572 + 50.0 * 6.295579433441162
Epoch 720, val loss: 1.5051484107971191
Epoch 730, training loss: 316.13787841796875 = 1.4281730651855469 + 50.0 * 6.294194221496582
Epoch 730, val loss: 1.4966973066329956
Epoch 740, training loss: 316.10626220703125 = 1.4167139530181885 + 50.0 * 6.293790817260742
Epoch 740, val loss: 1.4882992506027222
Epoch 750, training loss: 315.9683532714844 = 1.4052906036376953 + 50.0 * 6.291260719299316
Epoch 750, val loss: 1.4799758195877075
Epoch 760, training loss: 315.8907470703125 = 1.3939176797866821 + 50.0 * 6.289936542510986
Epoch 760, val loss: 1.4718109369277954
Epoch 770, training loss: 315.9496154785156 = 1.3825483322143555 + 50.0 * 6.291341304779053
Epoch 770, val loss: 1.463639259338379
Epoch 780, training loss: 315.7966003417969 = 1.3713122606277466 + 50.0 * 6.288505554199219
Epoch 780, val loss: 1.4557021856307983
Epoch 790, training loss: 315.66912841796875 = 1.3600844144821167 + 50.0 * 6.2861809730529785
Epoch 790, val loss: 1.447872519493103
Epoch 800, training loss: 315.5820007324219 = 1.3489590883255005 + 50.0 * 6.284660816192627
Epoch 800, val loss: 1.4401452541351318
Epoch 810, training loss: 315.5162658691406 = 1.337868332862854 + 50.0 * 6.283567905426025
Epoch 810, val loss: 1.432541012763977
Epoch 820, training loss: 315.6315002441406 = 1.3268183469772339 + 50.0 * 6.286093711853027
Epoch 820, val loss: 1.42501962184906
Epoch 830, training loss: 315.3778381347656 = 1.3158247470855713 + 50.0 * 6.281239986419678
Epoch 830, val loss: 1.4176056385040283
Epoch 840, training loss: 315.3208312988281 = 1.3049191236495972 + 50.0 * 6.280318260192871
Epoch 840, val loss: 1.410370111465454
Epoch 850, training loss: 315.2809753417969 = 1.2940804958343506 + 50.0 * 6.279737949371338
Epoch 850, val loss: 1.4031767845153809
Epoch 860, training loss: 315.20721435546875 = 1.283294916152954 + 50.0 * 6.278478622436523
Epoch 860, val loss: 1.3961297273635864
Epoch 870, training loss: 315.1425476074219 = 1.272538185119629 + 50.0 * 6.277400016784668
Epoch 870, val loss: 1.3891927003860474
Epoch 880, training loss: 315.0701904296875 = 1.261875033378601 + 50.0 * 6.2761664390563965
Epoch 880, val loss: 1.3823567628860474
Epoch 890, training loss: 314.9974365234375 = 1.2513065338134766 + 50.0 * 6.2749223709106445
Epoch 890, val loss: 1.3756794929504395
Epoch 900, training loss: 314.9817199707031 = 1.2408159971237183 + 50.0 * 6.274817943572998
Epoch 900, val loss: 1.369138240814209
Epoch 910, training loss: 314.9014587402344 = 1.2303202152252197 + 50.0 * 6.273422718048096
Epoch 910, val loss: 1.3625670671463013
Epoch 920, training loss: 314.8686218261719 = 1.2199218273162842 + 50.0 * 6.272974491119385
Epoch 920, val loss: 1.356264591217041
Epoch 930, training loss: 314.7767639160156 = 1.209566593170166 + 50.0 * 6.271344184875488
Epoch 930, val loss: 1.3499493598937988
Epoch 940, training loss: 314.7484130859375 = 1.1993154287338257 + 50.0 * 6.270982265472412
Epoch 940, val loss: 1.3437858819961548
Epoch 950, training loss: 314.8170471191406 = 1.189133882522583 + 50.0 * 6.272558212280273
Epoch 950, val loss: 1.3377301692962646
Epoch 960, training loss: 314.660888671875 = 1.1789932250976562 + 50.0 * 6.2696380615234375
Epoch 960, val loss: 1.3317569494247437
Epoch 970, training loss: 314.5790100097656 = 1.1689454317092896 + 50.0 * 6.2682013511657715
Epoch 970, val loss: 1.3258901834487915
Epoch 980, training loss: 314.5184020996094 = 1.1589797735214233 + 50.0 * 6.267188549041748
Epoch 980, val loss: 1.3201106786727905
Epoch 990, training loss: 314.4727783203125 = 1.1490788459777832 + 50.0 * 6.266473770141602
Epoch 990, val loss: 1.3145281076431274
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.5481481481481482
0.8181338956246705
=== training gcn model ===
Epoch 0, training loss: 431.7822570800781 = 1.9405838251113892 + 50.0 * 8.596833229064941
Epoch 0, val loss: 1.934809923171997
Epoch 10, training loss: 431.7652282714844 = 1.936725378036499 + 50.0 * 8.596570014953613
Epoch 10, val loss: 1.931061029434204
Epoch 20, training loss: 431.723388671875 = 1.9323123693466187 + 50.0 * 8.595821380615234
Epoch 20, val loss: 1.9266667366027832
Epoch 30, training loss: 431.5960388183594 = 1.92709219455719 + 50.0 * 8.593379020690918
Epoch 30, val loss: 1.9213173389434814
Epoch 40, training loss: 431.13958740234375 = 1.9207720756530762 + 50.0 * 8.584376335144043
Epoch 40, val loss: 1.9147262573242188
Epoch 50, training loss: 429.3549499511719 = 1.9130430221557617 + 50.0 * 8.548837661743164
Epoch 50, val loss: 1.9065476655960083
Epoch 60, training loss: 421.7706298828125 = 1.903769612312317 + 50.0 * 8.397336959838867
Epoch 60, val loss: 1.8967572450637817
Epoch 70, training loss: 391.6220703125 = 1.8937934637069702 + 50.0 * 7.794565200805664
Epoch 70, val loss: 1.8866256475448608
Epoch 80, training loss: 364.1957702636719 = 1.8838932514190674 + 50.0 * 7.246237754821777
Epoch 80, val loss: 1.8768552541732788
Epoch 90, training loss: 355.2376403808594 = 1.876954436302185 + 50.0 * 7.067214012145996
Epoch 90, val loss: 1.8703862428665161
Epoch 100, training loss: 350.0444030761719 = 1.870940923690796 + 50.0 * 6.9634690284729
Epoch 100, val loss: 1.8643579483032227
Epoch 110, training loss: 345.65264892578125 = 1.8644511699676514 + 50.0 * 6.8757643699646
Epoch 110, val loss: 1.858193039894104
Epoch 120, training loss: 342.318359375 = 1.8589248657226562 + 50.0 * 6.8091888427734375
Epoch 120, val loss: 1.8529372215270996
Epoch 130, training loss: 339.628173828125 = 1.853251576423645 + 50.0 * 6.75549840927124
Epoch 130, val loss: 1.8476709127426147
Epoch 140, training loss: 337.3330383300781 = 1.8482786417007446 + 50.0 * 6.709695339202881
Epoch 140, val loss: 1.843112587928772
Epoch 150, training loss: 335.32763671875 = 1.84359872341156 + 50.0 * 6.669681072235107
Epoch 150, val loss: 1.8388519287109375
Epoch 160, training loss: 333.5455322265625 = 1.8392516374588013 + 50.0 * 6.634125232696533
Epoch 160, val loss: 1.8348429203033447
Epoch 170, training loss: 331.96697998046875 = 1.8350142240524292 + 50.0 * 6.602639198303223
Epoch 170, val loss: 1.8309156894683838
Epoch 180, training loss: 330.564697265625 = 1.8308730125427246 + 50.0 * 6.574676513671875
Epoch 180, val loss: 1.827053427696228
Epoch 190, training loss: 329.3431091308594 = 1.8268508911132812 + 50.0 * 6.550325393676758
Epoch 190, val loss: 1.8232927322387695
Epoch 200, training loss: 328.2872619628906 = 1.8229132890701294 + 50.0 * 6.529286861419678
Epoch 200, val loss: 1.819604754447937
Epoch 210, training loss: 327.3954162597656 = 1.8189893960952759 + 50.0 * 6.511528015136719
Epoch 210, val loss: 1.8159068822860718
Epoch 220, training loss: 326.58538818359375 = 1.8150473833084106 + 50.0 * 6.4954071044921875
Epoch 220, val loss: 1.8121861219406128
Epoch 230, training loss: 325.86883544921875 = 1.8111155033111572 + 50.0 * 6.481153964996338
Epoch 230, val loss: 1.808485507965088
Epoch 240, training loss: 325.2520446777344 = 1.8071770668029785 + 50.0 * 6.468896865844727
Epoch 240, val loss: 1.804753303527832
Epoch 250, training loss: 324.6492919921875 = 1.803218960762024 + 50.0 * 6.456921577453613
Epoch 250, val loss: 1.8010308742523193
Epoch 260, training loss: 324.11102294921875 = 1.7992446422576904 + 50.0 * 6.446235656738281
Epoch 260, val loss: 1.7973030805587769
Epoch 270, training loss: 323.6307067871094 = 1.7952309846878052 + 50.0 * 6.436709403991699
Epoch 270, val loss: 1.793530821800232
Epoch 280, training loss: 323.1796569824219 = 1.7911417484283447 + 50.0 * 6.427770137786865
Epoch 280, val loss: 1.789700984954834
Epoch 290, training loss: 322.7738342285156 = 1.7869573831558228 + 50.0 * 6.419737815856934
Epoch 290, val loss: 1.7858195304870605
Epoch 300, training loss: 322.40911865234375 = 1.7826660871505737 + 50.0 * 6.412528991699219
Epoch 300, val loss: 1.7818539142608643
Epoch 310, training loss: 322.0888671875 = 1.778252363204956 + 50.0 * 6.406212329864502
Epoch 310, val loss: 1.777767300605774
Epoch 320, training loss: 321.7514343261719 = 1.7736748456954956 + 50.0 * 6.399555206298828
Epoch 320, val loss: 1.773602843284607
Epoch 330, training loss: 321.4533386230469 = 1.7689329385757446 + 50.0 * 6.393688201904297
Epoch 330, val loss: 1.7693192958831787
Epoch 340, training loss: 321.1789245605469 = 1.764007568359375 + 50.0 * 6.388298034667969
Epoch 340, val loss: 1.7648656368255615
Epoch 350, training loss: 320.9114685058594 = 1.7588812112808228 + 50.0 * 6.383051872253418
Epoch 350, val loss: 1.7602827548980713
Epoch 360, training loss: 320.6690979003906 = 1.7535442113876343 + 50.0 * 6.3783111572265625
Epoch 360, val loss: 1.7555370330810547
Epoch 370, training loss: 320.4075927734375 = 1.7479777336120605 + 50.0 * 6.373192310333252
Epoch 370, val loss: 1.7506239414215088
Epoch 380, training loss: 320.1735534667969 = 1.7421823740005493 + 50.0 * 6.368627548217773
Epoch 380, val loss: 1.7455267906188965
Epoch 390, training loss: 320.0030212402344 = 1.7361388206481934 + 50.0 * 6.365337371826172
Epoch 390, val loss: 1.7402369976043701
Epoch 400, training loss: 319.74285888671875 = 1.7298283576965332 + 50.0 * 6.360260486602783
Epoch 400, val loss: 1.7347363233566284
Epoch 410, training loss: 319.5273132324219 = 1.7232273817062378 + 50.0 * 6.356081485748291
Epoch 410, val loss: 1.7290276288986206
Epoch 420, training loss: 319.3139343261719 = 1.7163492441177368 + 50.0 * 6.351952075958252
Epoch 420, val loss: 1.7230899333953857
Epoch 430, training loss: 319.1239318847656 = 1.709168553352356 + 50.0 * 6.348295211791992
Epoch 430, val loss: 1.7169407606124878
Epoch 440, training loss: 318.9655456542969 = 1.7016884088516235 + 50.0 * 6.345277309417725
Epoch 440, val loss: 1.7105159759521484
Epoch 450, training loss: 318.75872802734375 = 1.6938972473144531 + 50.0 * 6.341296195983887
Epoch 450, val loss: 1.7038710117340088
Epoch 460, training loss: 318.5738220214844 = 1.6857993602752686 + 50.0 * 6.3377604484558105
Epoch 460, val loss: 1.6969817876815796
Epoch 470, training loss: 318.4188537597656 = 1.6773943901062012 + 50.0 * 6.334828853607178
Epoch 470, val loss: 1.6898616552352905
Epoch 480, training loss: 318.2785339355469 = 1.6686655282974243 + 50.0 * 6.332197189331055
Epoch 480, val loss: 1.6824917793273926
Epoch 490, training loss: 318.10211181640625 = 1.659611701965332 + 50.0 * 6.328850269317627
Epoch 490, val loss: 1.6748731136322021
Epoch 500, training loss: 317.9396667480469 = 1.6502645015716553 + 50.0 * 6.3257880210876465
Epoch 500, val loss: 1.66705322265625
Epoch 510, training loss: 317.7926025390625 = 1.6406099796295166 + 50.0 * 6.323039531707764
Epoch 510, val loss: 1.6589945554733276
Epoch 520, training loss: 317.6415710449219 = 1.6306308507919312 + 50.0 * 6.320218563079834
Epoch 520, val loss: 1.65072500705719
Epoch 530, training loss: 317.4999694824219 = 1.6203773021697998 + 50.0 * 6.317591667175293
Epoch 530, val loss: 1.6422333717346191
Epoch 540, training loss: 317.3606262207031 = 1.6098453998565674 + 50.0 * 6.31501579284668
Epoch 540, val loss: 1.6335657835006714
Epoch 550, training loss: 317.231201171875 = 1.59905207157135 + 50.0 * 6.312642574310303
Epoch 550, val loss: 1.6247285604476929
Epoch 560, training loss: 317.2493591308594 = 1.5880045890808105 + 50.0 * 6.313226699829102
Epoch 560, val loss: 1.6157200336456299
Epoch 570, training loss: 316.9938659667969 = 1.5766881704330444 + 50.0 * 6.308343410491943
Epoch 570, val loss: 1.606571078300476
Epoch 580, training loss: 316.859619140625 = 1.56515634059906 + 50.0 * 6.305889129638672
Epoch 580, val loss: 1.5973364114761353
Epoch 590, training loss: 316.7551574707031 = 1.5534411668777466 + 50.0 * 6.30403470993042
Epoch 590, val loss: 1.58799147605896
Epoch 600, training loss: 316.6587829589844 = 1.541536569595337 + 50.0 * 6.302344799041748
Epoch 600, val loss: 1.5785552263259888
Epoch 610, training loss: 316.58917236328125 = 1.5294772386550903 + 50.0 * 6.301193714141846
Epoch 610, val loss: 1.5689799785614014
Epoch 620, training loss: 316.4300842285156 = 1.5172460079193115 + 50.0 * 6.298256874084473
Epoch 620, val loss: 1.5594340562820435
Epoch 630, training loss: 316.37310791015625 = 1.5048696994781494 + 50.0 * 6.297364711761475
Epoch 630, val loss: 1.549804449081421
Epoch 640, training loss: 316.2007751464844 = 1.4924075603485107 + 50.0 * 6.294167518615723
Epoch 640, val loss: 1.5401325225830078
Epoch 650, training loss: 316.08282470703125 = 1.4798346757888794 + 50.0 * 6.292059898376465
Epoch 650, val loss: 1.5305280685424805
Epoch 660, training loss: 315.98687744140625 = 1.4671894311904907 + 50.0 * 6.290393829345703
Epoch 660, val loss: 1.520931363105774
Epoch 670, training loss: 315.9342041015625 = 1.4544661045074463 + 50.0 * 6.289594650268555
Epoch 670, val loss: 1.5113574266433716
Epoch 680, training loss: 315.86517333984375 = 1.441655158996582 + 50.0 * 6.28847074508667
Epoch 680, val loss: 1.5018088817596436
Epoch 690, training loss: 315.72528076171875 = 1.4288479089736938 + 50.0 * 6.285929203033447
Epoch 690, val loss: 1.4923003911972046
Epoch 700, training loss: 315.6266784667969 = 1.415993332862854 + 50.0 * 6.284214019775391
Epoch 700, val loss: 1.4829046726226807
Epoch 710, training loss: 315.58831787109375 = 1.4031400680541992 + 50.0 * 6.283703327178955
Epoch 710, val loss: 1.4735264778137207
Epoch 720, training loss: 315.4319763183594 = 1.3902431726455688 + 50.0 * 6.280835151672363
Epoch 720, val loss: 1.4642935991287231
Epoch 730, training loss: 315.34228515625 = 1.3773683309555054 + 50.0 * 6.279298305511475
Epoch 730, val loss: 1.4551140069961548
Epoch 740, training loss: 315.27130126953125 = 1.3645161390304565 + 50.0 * 6.278135776519775
Epoch 740, val loss: 1.4460431337356567
Epoch 750, training loss: 315.2922668457031 = 1.351676344871521 + 50.0 * 6.278811931610107
Epoch 750, val loss: 1.437042236328125
Epoch 760, training loss: 315.1254577636719 = 1.3388479948043823 + 50.0 * 6.275732517242432
Epoch 760, val loss: 1.428171992301941
Epoch 770, training loss: 315.05596923828125 = 1.3260998725891113 + 50.0 * 6.27459716796875
Epoch 770, val loss: 1.4193966388702393
Epoch 780, training loss: 314.974365234375 = 1.3133739233016968 + 50.0 * 6.273219585418701
Epoch 780, val loss: 1.410735011100769
Epoch 790, training loss: 314.8766174316406 = 1.3006811141967773 + 50.0 * 6.271518707275391
Epoch 790, val loss: 1.4022504091262817
Epoch 800, training loss: 314.8473815917969 = 1.2880712747573853 + 50.0 * 6.271185874938965
Epoch 800, val loss: 1.393879771232605
Epoch 810, training loss: 314.7376403808594 = 1.275549292564392 + 50.0 * 6.269241809844971
Epoch 810, val loss: 1.3855589628219604
Epoch 820, training loss: 314.6914978027344 = 1.2630832195281982 + 50.0 * 6.26856803894043
Epoch 820, val loss: 1.3774405717849731
Epoch 830, training loss: 314.6776123046875 = 1.2507076263427734 + 50.0 * 6.268538475036621
Epoch 830, val loss: 1.369418978691101
Epoch 840, training loss: 314.6180725097656 = 1.2383713722229004 + 50.0 * 6.267593860626221
Epoch 840, val loss: 1.3616316318511963
Epoch 850, training loss: 314.4732971191406 = 1.2261806726455688 + 50.0 * 6.264942646026611
Epoch 850, val loss: 1.3539292812347412
Epoch 860, training loss: 314.4198303222656 = 1.2141193151474 + 50.0 * 6.2641143798828125
Epoch 860, val loss: 1.346398949623108
Epoch 870, training loss: 314.34521484375 = 1.2021434307098389 + 50.0 * 6.262861251831055
Epoch 870, val loss: 1.3390675783157349
Epoch 880, training loss: 314.29461669921875 = 1.1903141736984253 + 50.0 * 6.262086391448975
Epoch 880, val loss: 1.3318991661071777
Epoch 890, training loss: 314.3985290527344 = 1.178592324256897 + 50.0 * 6.264398574829102
Epoch 890, val loss: 1.324855089187622
Epoch 900, training loss: 314.191162109375 = 1.1669968366622925 + 50.0 * 6.260483264923096
Epoch 900, val loss: 1.3180103302001953
Epoch 910, training loss: 314.12774658203125 = 1.1555505990982056 + 50.0 * 6.259443759918213
Epoch 910, val loss: 1.311331033706665
Epoch 920, training loss: 314.0596008300781 = 1.144270658493042 + 50.0 * 6.258306980133057
Epoch 920, val loss: 1.304872989654541
Epoch 930, training loss: 314.0320739746094 = 1.1331374645233154 + 50.0 * 6.257978439331055
Epoch 930, val loss: 1.298624873161316
Epoch 940, training loss: 313.9705810546875 = 1.1221599578857422 + 50.0 * 6.2569684982299805
Epoch 940, val loss: 1.2924284934997559
Epoch 950, training loss: 313.94622802734375 = 1.1113061904907227 + 50.0 * 6.2566986083984375
Epoch 950, val loss: 1.2865179777145386
Epoch 960, training loss: 313.86444091796875 = 1.1006619930267334 + 50.0 * 6.255275726318359
Epoch 960, val loss: 1.2807786464691162
Epoch 970, training loss: 313.8419494628906 = 1.0901916027069092 + 50.0 * 6.255035400390625
Epoch 970, val loss: 1.2752349376678467
Epoch 980, training loss: 313.7545166015625 = 1.079835057258606 + 50.0 * 6.253493785858154
Epoch 980, val loss: 1.2699331045150757
Epoch 990, training loss: 313.7195739746094 = 1.069677472114563 + 50.0 * 6.252997875213623
Epoch 990, val loss: 1.2647674083709717
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.5740740740740741
0.8260411175540328
=== training gcn model ===
Epoch 0, training loss: 431.79107666015625 = 1.9509820938110352 + 50.0 * 8.5968017578125
Epoch 0, val loss: 1.9502081871032715
Epoch 10, training loss: 431.7698669433594 = 1.9465761184692383 + 50.0 * 8.596466064453125
Epoch 10, val loss: 1.9459309577941895
Epoch 20, training loss: 431.7135314941406 = 1.9415192604064941 + 50.0 * 8.595439910888672
Epoch 20, val loss: 1.9408578872680664
Epoch 30, training loss: 431.5322570800781 = 1.9355390071868896 + 50.0 * 8.591934204101562
Epoch 30, val loss: 1.934671401977539
Epoch 40, training loss: 430.9056701660156 = 1.9283699989318848 + 50.0 * 8.579545974731445
Epoch 40, val loss: 1.9271184206008911
Epoch 50, training loss: 428.8451232910156 = 1.919805645942688 + 50.0 * 8.538506507873535
Epoch 50, val loss: 1.9180078506469727
Epoch 60, training loss: 422.47088623046875 = 1.9097721576690674 + 50.0 * 8.411222457885742
Epoch 60, val loss: 1.907439112663269
Epoch 70, training loss: 404.0609436035156 = 1.8989214897155762 + 50.0 * 8.043240547180176
Epoch 70, val loss: 1.895975112915039
Epoch 80, training loss: 375.3265380859375 = 1.8877543210983276 + 50.0 * 7.468775749206543
Epoch 80, val loss: 1.8847033977508545
Epoch 90, training loss: 364.89532470703125 = 1.879512071609497 + 50.0 * 7.260315895080566
Epoch 90, val loss: 1.8770027160644531
Epoch 100, training loss: 358.2038269042969 = 1.8738927841186523 + 50.0 * 7.126598358154297
Epoch 100, val loss: 1.871742606163025
Epoch 110, training loss: 355.1366271972656 = 1.8680508136749268 + 50.0 * 7.065371990203857
Epoch 110, val loss: 1.8662011623382568
Epoch 120, training loss: 351.766357421875 = 1.8620407581329346 + 50.0 * 6.998086452484131
Epoch 120, val loss: 1.8605945110321045
Epoch 130, training loss: 348.0964050292969 = 1.856310248374939 + 50.0 * 6.924801826477051
Epoch 130, val loss: 1.8552830219268799
Epoch 140, training loss: 344.74334716796875 = 1.8511813879013062 + 50.0 * 6.857842922210693
Epoch 140, val loss: 1.8504924774169922
Epoch 150, training loss: 341.8869323730469 = 1.846389889717102 + 50.0 * 6.800810813903809
Epoch 150, val loss: 1.8459696769714355
Epoch 160, training loss: 339.0797424316406 = 1.841800332069397 + 50.0 * 6.7447590827941895
Epoch 160, val loss: 1.8416334390640259
Epoch 170, training loss: 336.55096435546875 = 1.837390661239624 + 50.0 * 6.694271087646484
Epoch 170, val loss: 1.8375225067138672
Epoch 180, training loss: 334.39166259765625 = 1.8331407308578491 + 50.0 * 6.65117073059082
Epoch 180, val loss: 1.8335908651351929
Epoch 190, training loss: 332.5272521972656 = 1.829141616821289 + 50.0 * 6.613962173461914
Epoch 190, val loss: 1.8298680782318115
Epoch 200, training loss: 330.9479064941406 = 1.8252564668655396 + 50.0 * 6.58245325088501
Epoch 200, val loss: 1.826332688331604
Epoch 210, training loss: 329.6138916015625 = 1.8214828968048096 + 50.0 * 6.555848598480225
Epoch 210, val loss: 1.822839379310608
Epoch 220, training loss: 328.4488525390625 = 1.8176133632659912 + 50.0 * 6.5326247215271
Epoch 220, val loss: 1.81935453414917
Epoch 230, training loss: 327.4512023925781 = 1.8138227462768555 + 50.0 * 6.512747764587402
Epoch 230, val loss: 1.8158591985702515
Epoch 240, training loss: 326.63946533203125 = 1.8100996017456055 + 50.0 * 6.496587753295898
Epoch 240, val loss: 1.8124217987060547
Epoch 250, training loss: 325.9049072265625 = 1.8063040971755981 + 50.0 * 6.4819722175598145
Epoch 250, val loss: 1.809000015258789
Epoch 260, training loss: 325.2792663574219 = 1.802582025527954 + 50.0 * 6.469533920288086
Epoch 260, val loss: 1.8055773973464966
Epoch 270, training loss: 324.7192077636719 = 1.798839807510376 + 50.0 * 6.458407402038574
Epoch 270, val loss: 1.8021537065505981
Epoch 280, training loss: 324.2059631347656 = 1.795046091079712 + 50.0 * 6.44821834564209
Epoch 280, val loss: 1.7987042665481567
Epoch 290, training loss: 323.7144775390625 = 1.7912019491195679 + 50.0 * 6.438465595245361
Epoch 290, val loss: 1.7952265739440918
Epoch 300, training loss: 323.26702880859375 = 1.7872912883758545 + 50.0 * 6.429594993591309
Epoch 300, val loss: 1.7917176485061646
Epoch 310, training loss: 322.8953857421875 = 1.7832732200622559 + 50.0 * 6.422242641448975
Epoch 310, val loss: 1.788153052330017
Epoch 320, training loss: 322.4756164550781 = 1.7791473865509033 + 50.0 * 6.413929462432861
Epoch 320, val loss: 1.7844603061676025
Epoch 330, training loss: 322.0936279296875 = 1.7748650312423706 + 50.0 * 6.406375408172607
Epoch 330, val loss: 1.780713677406311
Epoch 340, training loss: 321.74169921875 = 1.7704312801361084 + 50.0 * 6.399425506591797
Epoch 340, val loss: 1.776828408241272
Epoch 350, training loss: 321.404052734375 = 1.765850305557251 + 50.0 * 6.392764091491699
Epoch 350, val loss: 1.7728080749511719
Epoch 360, training loss: 321.1102294921875 = 1.7610431909561157 + 50.0 * 6.386983871459961
Epoch 360, val loss: 1.7686448097229004
Epoch 370, training loss: 320.8049621582031 = 1.7560856342315674 + 50.0 * 6.380977630615234
Epoch 370, val loss: 1.7643314599990845
Epoch 380, training loss: 320.5209655761719 = 1.7509227991104126 + 50.0 * 6.375401020050049
Epoch 380, val loss: 1.7598718404769897
Epoch 390, training loss: 320.2747802734375 = 1.745551586151123 + 50.0 * 6.370584487915039
Epoch 390, val loss: 1.7552307844161987
Epoch 400, training loss: 319.9808349609375 = 1.739904761314392 + 50.0 * 6.364818572998047
Epoch 400, val loss: 1.7504172325134277
Epoch 410, training loss: 319.747802734375 = 1.7340571880340576 + 50.0 * 6.360274791717529
Epoch 410, val loss: 1.7453954219818115
Epoch 420, training loss: 319.5602111816406 = 1.727998971939087 + 50.0 * 6.356644153594971
Epoch 420, val loss: 1.74016535282135
Epoch 430, training loss: 319.34027099609375 = 1.7216097116470337 + 50.0 * 6.352373123168945
Epoch 430, val loss: 1.7347314357757568
Epoch 440, training loss: 319.1058349609375 = 1.7150516510009766 + 50.0 * 6.34781551361084
Epoch 440, val loss: 1.7290904521942139
Epoch 450, training loss: 318.91778564453125 = 1.70822012424469 + 50.0 * 6.344191074371338
Epoch 450, val loss: 1.723225474357605
Epoch 460, training loss: 318.7318420410156 = 1.701084017753601 + 50.0 * 6.340615272521973
Epoch 460, val loss: 1.7171380519866943
Epoch 470, training loss: 318.5592041015625 = 1.6937271356582642 + 50.0 * 6.337309837341309
Epoch 470, val loss: 1.7108371257781982
Epoch 480, training loss: 318.40960693359375 = 1.6860928535461426 + 50.0 * 6.334470272064209
Epoch 480, val loss: 1.704323172569275
Epoch 490, training loss: 318.2413330078125 = 1.6781904697418213 + 50.0 * 6.331263065338135
Epoch 490, val loss: 1.6975878477096558
Epoch 500, training loss: 318.081298828125 = 1.6700260639190674 + 50.0 * 6.328225612640381
Epoch 500, val loss: 1.690652847290039
Epoch 510, training loss: 317.9326477050781 = 1.6616233587265015 + 50.0 * 6.325420379638672
Epoch 510, val loss: 1.683503270149231
Epoch 520, training loss: 317.79498291015625 = 1.6529221534729004 + 50.0 * 6.322841167449951
Epoch 520, val loss: 1.6761581897735596
Epoch 530, training loss: 317.6846618652344 = 1.6440222263336182 + 50.0 * 6.320812702178955
Epoch 530, val loss: 1.6686216592788696
Epoch 540, training loss: 317.5147399902344 = 1.634808897972107 + 50.0 * 6.317598819732666
Epoch 540, val loss: 1.6609030961990356
Epoch 550, training loss: 317.3916931152344 = 1.6254019737243652 + 50.0 * 6.315325736999512
Epoch 550, val loss: 1.6530208587646484
Epoch 560, training loss: 317.36444091796875 = 1.6157559156417847 + 50.0 * 6.314973831176758
Epoch 560, val loss: 1.6449915170669556
Epoch 570, training loss: 317.1781005859375 = 1.6060391664505005 + 50.0 * 6.311440944671631
Epoch 570, val loss: 1.6368545293807983
Epoch 580, training loss: 317.01324462890625 = 1.5960291624069214 + 50.0 * 6.308343887329102
Epoch 580, val loss: 1.6285675764083862
Epoch 590, training loss: 316.8968505859375 = 1.5859185457229614 + 50.0 * 6.306219100952148
Epoch 590, val loss: 1.6202142238616943
Epoch 600, training loss: 316.8943176269531 = 1.5756659507751465 + 50.0 * 6.306373596191406
Epoch 600, val loss: 1.6117229461669922
Epoch 610, training loss: 316.7048645019531 = 1.5652451515197754 + 50.0 * 6.302792072296143
Epoch 610, val loss: 1.6032081842422485
Epoch 620, training loss: 316.5856628417969 = 1.5547453165054321 + 50.0 * 6.3006181716918945
Epoch 620, val loss: 1.5946416854858398
Epoch 630, training loss: 316.46417236328125 = 1.5441361665725708 + 50.0 * 6.29840087890625
Epoch 630, val loss: 1.5860475301742554
Epoch 640, training loss: 316.51446533203125 = 1.5334336757659912 + 50.0 * 6.299620151519775
Epoch 640, val loss: 1.5774133205413818
Epoch 650, training loss: 316.2960510253906 = 1.5227885246276855 + 50.0 * 6.295464992523193
Epoch 650, val loss: 1.568846344947815
Epoch 660, training loss: 316.18426513671875 = 1.5120060443878174 + 50.0 * 6.293445587158203
Epoch 660, val loss: 1.56028151512146
Epoch 670, training loss: 316.081787109375 = 1.5012527704238892 + 50.0 * 6.2916107177734375
Epoch 670, val loss: 1.5517734289169312
Epoch 680, training loss: 316.0738525390625 = 1.4904922246932983 + 50.0 * 6.291667461395264
Epoch 680, val loss: 1.5433180332183838
Epoch 690, training loss: 315.9598388671875 = 1.4797276258468628 + 50.0 * 6.289601802825928
Epoch 690, val loss: 1.534959077835083
Epoch 700, training loss: 315.8175964355469 = 1.4689836502075195 + 50.0 * 6.2869720458984375
Epoch 700, val loss: 1.5266464948654175
Epoch 710, training loss: 315.73699951171875 = 1.4582877159118652 + 50.0 * 6.285574436187744
Epoch 710, val loss: 1.5184657573699951
Epoch 720, training loss: 315.8263854980469 = 1.4476754665374756 + 50.0 * 6.287574291229248
Epoch 720, val loss: 1.5103394985198975
Epoch 730, training loss: 315.5928039550781 = 1.436922311782837 + 50.0 * 6.283117771148682
Epoch 730, val loss: 1.5022982358932495
Epoch 740, training loss: 315.49517822265625 = 1.4263317584991455 + 50.0 * 6.281376838684082
Epoch 740, val loss: 1.494399905204773
Epoch 750, training loss: 315.4192199707031 = 1.415812373161316 + 50.0 * 6.280068397521973
Epoch 750, val loss: 1.4865721464157104
Epoch 760, training loss: 315.3426818847656 = 1.4053268432617188 + 50.0 * 6.278747081756592
Epoch 760, val loss: 1.478892207145691
Epoch 770, training loss: 315.41900634765625 = 1.3949538469314575 + 50.0 * 6.280480861663818
Epoch 770, val loss: 1.471329689025879
Epoch 780, training loss: 315.26092529296875 = 1.3844826221466064 + 50.0 * 6.277528762817383
Epoch 780, val loss: 1.463771939277649
Epoch 790, training loss: 315.1719970703125 = 1.3741123676300049 + 50.0 * 6.2759575843811035
Epoch 790, val loss: 1.4563249349594116
Epoch 800, training loss: 315.06036376953125 = 1.3638215065002441 + 50.0 * 6.273930549621582
Epoch 800, val loss: 1.4490530490875244
Epoch 810, training loss: 315.0142822265625 = 1.3535847663879395 + 50.0 * 6.273213863372803
Epoch 810, val loss: 1.4418740272521973
Epoch 820, training loss: 314.94805908203125 = 1.3433434963226318 + 50.0 * 6.272094249725342
Epoch 820, val loss: 1.4347805976867676
Epoch 830, training loss: 315.0216369628906 = 1.3330987691879272 + 50.0 * 6.273770809173584
Epoch 830, val loss: 1.427763819694519
Epoch 840, training loss: 314.84283447265625 = 1.3230465650558472 + 50.0 * 6.270395755767822
Epoch 840, val loss: 1.4208236932754517
Epoch 850, training loss: 314.7537536621094 = 1.3128477334976196 + 50.0 * 6.268818378448486
Epoch 850, val loss: 1.413984775543213
Epoch 860, training loss: 314.6773376464844 = 1.3027915954589844 + 50.0 * 6.267490863800049
Epoch 860, val loss: 1.4072293043136597
Epoch 870, training loss: 314.6152648925781 = 1.2927106618881226 + 50.0 * 6.266450881958008
Epoch 870, val loss: 1.4005558490753174
Epoch 880, training loss: 314.6935119628906 = 1.282700538635254 + 50.0 * 6.268215656280518
Epoch 880, val loss: 1.393959879875183
Epoch 890, training loss: 314.5786437988281 = 1.272618055343628 + 50.0 * 6.266120433807373
Epoch 890, val loss: 1.3874199390411377
Epoch 900, training loss: 314.4679260253906 = 1.2626004219055176 + 50.0 * 6.264106750488281
Epoch 900, val loss: 1.3809702396392822
Epoch 910, training loss: 314.37939453125 = 1.2526360750198364 + 50.0 * 6.262535095214844
Epoch 910, val loss: 1.3745925426483154
Epoch 920, training loss: 314.32989501953125 = 1.2426772117614746 + 50.0 * 6.261744499206543
Epoch 920, val loss: 1.3683040142059326
Epoch 930, training loss: 314.52392578125 = 1.2326725721359253 + 50.0 * 6.265825271606445
Epoch 930, val loss: 1.3620842695236206
Epoch 940, training loss: 314.2381591796875 = 1.2228405475616455 + 50.0 * 6.260306358337402
Epoch 940, val loss: 1.3558543920516968
Epoch 950, training loss: 314.1892395019531 = 1.2129085063934326 + 50.0 * 6.259526252746582
Epoch 950, val loss: 1.3496677875518799
Epoch 960, training loss: 314.1085205078125 = 1.203007459640503 + 50.0 * 6.258110046386719
Epoch 960, val loss: 1.3436498641967773
Epoch 970, training loss: 314.10052490234375 = 1.193143606185913 + 50.0 * 6.258147716522217
Epoch 970, val loss: 1.3376415967941284
Epoch 980, training loss: 314.04736328125 = 1.1832811832427979 + 50.0 * 6.257281303405762
Epoch 980, val loss: 1.3317475318908691
Epoch 990, training loss: 314.00054931640625 = 1.1733888387680054 + 50.0 * 6.256543159484863
Epoch 990, val loss: 1.3258339166641235
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.5370370370370371
0.8191881918819188
The final CL Acc:0.55309, 0.01552, The final GNN Acc:0.82112, 0.00351
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13268])
remove edge: torch.Size([2, 7914])
updated graph: torch.Size([2, 10626])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 431.7826843261719 = 1.9406548738479614 + 50.0 * 8.596840858459473
Epoch 0, val loss: 1.9384692907333374
Epoch 10, training loss: 431.7665100097656 = 1.9368274211883545 + 50.0 * 8.596593856811523
Epoch 10, val loss: 1.9349087476730347
Epoch 20, training loss: 431.72515869140625 = 1.9326571226119995 + 50.0 * 8.595849990844727
Epoch 20, val loss: 1.9309437274932861
Epoch 30, training loss: 431.5938415527344 = 1.9279356002807617 + 50.0 * 8.593317985534668
Epoch 30, val loss: 1.926361322402954
Epoch 40, training loss: 431.1293640136719 = 1.9223734140396118 + 50.0 * 8.584139823913574
Epoch 40, val loss: 1.9208959341049194
Epoch 50, training loss: 429.4919738769531 = 1.9156132936477661 + 50.0 * 8.55152702331543
Epoch 50, val loss: 1.914215087890625
Epoch 60, training loss: 424.00555419921875 = 1.907336711883545 + 50.0 * 8.441964149475098
Epoch 60, val loss: 1.9060393571853638
Epoch 70, training loss: 407.69683837890625 = 1.897992730140686 + 50.0 * 8.11597728729248
Epoch 70, val loss: 1.8970006704330444
Epoch 80, training loss: 383.5710144042969 = 1.8873940706253052 + 50.0 * 7.633672714233398
Epoch 80, val loss: 1.8868374824523926
Epoch 90, training loss: 372.9692687988281 = 1.877888798713684 + 50.0 * 7.42182731628418
Epoch 90, val loss: 1.8777974843978882
Epoch 100, training loss: 365.1781311035156 = 1.8707903623580933 + 50.0 * 7.266147136688232
Epoch 100, val loss: 1.8708436489105225
Epoch 110, training loss: 358.5093688964844 = 1.8646657466888428 + 50.0 * 7.132894515991211
Epoch 110, val loss: 1.8647633790969849
Epoch 120, training loss: 353.7139892578125 = 1.8582086563110352 + 50.0 * 7.037115573883057
Epoch 120, val loss: 1.8586777448654175
Epoch 130, training loss: 350.0383605957031 = 1.8521875143051147 + 50.0 * 6.963723659515381
Epoch 130, val loss: 1.8532909154891968
Epoch 140, training loss: 346.629638671875 = 1.846626877784729 + 50.0 * 6.895660400390625
Epoch 140, val loss: 1.8483948707580566
Epoch 150, training loss: 343.7457275390625 = 1.8413364887237549 + 50.0 * 6.838088035583496
Epoch 150, val loss: 1.8438423871994019
Epoch 160, training loss: 341.1655578613281 = 1.8363370895385742 + 50.0 * 6.786584377288818
Epoch 160, val loss: 1.8394428491592407
Epoch 170, training loss: 338.734619140625 = 1.8314423561096191 + 50.0 * 6.738063335418701
Epoch 170, val loss: 1.8350741863250732
Epoch 180, training loss: 336.6855773925781 = 1.8266575336456299 + 50.0 * 6.697178363800049
Epoch 180, val loss: 1.8307926654815674
Epoch 190, training loss: 334.7467956542969 = 1.8219208717346191 + 50.0 * 6.658497333526611
Epoch 190, val loss: 1.8265870809555054
Epoch 200, training loss: 332.96435546875 = 1.81734037399292 + 50.0 * 6.6229400634765625
Epoch 200, val loss: 1.8225224018096924
Epoch 210, training loss: 331.5447082519531 = 1.8128728866577148 + 50.0 * 6.594636917114258
Epoch 210, val loss: 1.8185925483703613
Epoch 220, training loss: 330.4653015136719 = 1.8083736896514893 + 50.0 * 6.573138236999512
Epoch 220, val loss: 1.8145586252212524
Epoch 230, training loss: 329.6433410644531 = 1.8037424087524414 + 50.0 * 6.55679178237915
Epoch 230, val loss: 1.8103913068771362
Epoch 240, training loss: 328.9580383300781 = 1.7990143299102783 + 50.0 * 6.543180465698242
Epoch 240, val loss: 1.8062101602554321
Epoch 250, training loss: 328.3504638671875 = 1.7943273782730103 + 50.0 * 6.531122207641602
Epoch 250, val loss: 1.8020813465118408
Epoch 260, training loss: 327.83642578125 = 1.789707899093628 + 50.0 * 6.520934581756592
Epoch 260, val loss: 1.7980200052261353
Epoch 270, training loss: 327.30694580078125 = 1.7850854396820068 + 50.0 * 6.51043701171875
Epoch 270, val loss: 1.7939584255218506
Epoch 280, training loss: 326.82769775390625 = 1.7804181575775146 + 50.0 * 6.500945568084717
Epoch 280, val loss: 1.7898614406585693
Epoch 290, training loss: 326.41973876953125 = 1.7756503820419312 + 50.0 * 6.4928812980651855
Epoch 290, val loss: 1.7857403755187988
Epoch 300, training loss: 325.8933410644531 = 1.770804762840271 + 50.0 * 6.482450485229492
Epoch 300, val loss: 1.781477928161621
Epoch 310, training loss: 325.41265869140625 = 1.7658323049545288 + 50.0 * 6.472936153411865
Epoch 310, val loss: 1.7771615982055664
Epoch 320, training loss: 324.9104309082031 = 1.7607474327087402 + 50.0 * 6.462993621826172
Epoch 320, val loss: 1.7727336883544922
Epoch 330, training loss: 324.4176330566406 = 1.75541353225708 + 50.0 * 6.453244686126709
Epoch 330, val loss: 1.7681434154510498
Epoch 340, training loss: 323.9293212890625 = 1.7499340772628784 + 50.0 * 6.443587779998779
Epoch 340, val loss: 1.763397216796875
Epoch 350, training loss: 323.4664306640625 = 1.744181513786316 + 50.0 * 6.434444904327393
Epoch 350, val loss: 1.7584524154663086
Epoch 360, training loss: 323.0257568359375 = 1.7381547689437866 + 50.0 * 6.42575216293335
Epoch 360, val loss: 1.753239393234253
Epoch 370, training loss: 322.7886962890625 = 1.7318048477172852 + 50.0 * 6.421137809753418
Epoch 370, val loss: 1.747756004333496
Epoch 380, training loss: 322.2685241699219 = 1.7250925302505493 + 50.0 * 6.4108686447143555
Epoch 380, val loss: 1.742040753364563
Epoch 390, training loss: 321.9185485839844 = 1.718096375465393 + 50.0 * 6.404008865356445
Epoch 390, val loss: 1.7360162734985352
Epoch 400, training loss: 321.58685302734375 = 1.7107778787612915 + 50.0 * 6.397521495819092
Epoch 400, val loss: 1.7297090291976929
Epoch 410, training loss: 321.2724304199219 = 1.703101634979248 + 50.0 * 6.391386985778809
Epoch 410, val loss: 1.723136067390442
Epoch 420, training loss: 320.97430419921875 = 1.6950883865356445 + 50.0 * 6.385583877563477
Epoch 420, val loss: 1.7162364721298218
Epoch 430, training loss: 320.69012451171875 = 1.6867246627807617 + 50.0 * 6.380067825317383
Epoch 430, val loss: 1.7090345621109009
Epoch 440, training loss: 320.4160461425781 = 1.6779993772506714 + 50.0 * 6.374760627746582
Epoch 440, val loss: 1.7015371322631836
Epoch 450, training loss: 320.2912902832031 = 1.6688740253448486 + 50.0 * 6.372447967529297
Epoch 450, val loss: 1.6937158107757568
Epoch 460, training loss: 319.934326171875 = 1.6594358682632446 + 50.0 * 6.365498065948486
Epoch 460, val loss: 1.6855500936508179
Epoch 470, training loss: 319.6728515625 = 1.6496672630310059 + 50.0 * 6.360464096069336
Epoch 470, val loss: 1.6771292686462402
Epoch 480, training loss: 319.43157958984375 = 1.6395710706710815 + 50.0 * 6.355840682983398
Epoch 480, val loss: 1.6684049367904663
Epoch 490, training loss: 319.28839111328125 = 1.6291236877441406 + 50.0 * 6.353185176849365
Epoch 490, val loss: 1.6594382524490356
Epoch 500, training loss: 319.0171203613281 = 1.6183817386627197 + 50.0 * 6.34797477722168
Epoch 500, val loss: 1.6501346826553345
Epoch 510, training loss: 318.8147277832031 = 1.6073541641235352 + 50.0 * 6.344147205352783
Epoch 510, val loss: 1.640648365020752
Epoch 520, training loss: 318.6119079589844 = 1.596075415611267 + 50.0 * 6.3403167724609375
Epoch 520, val loss: 1.6309438943862915
Epoch 530, training loss: 318.4859924316406 = 1.5845645666122437 + 50.0 * 6.338028430938721
Epoch 530, val loss: 1.6210296154022217
Epoch 540, training loss: 318.24456787109375 = 1.5727899074554443 + 50.0 * 6.333435535430908
Epoch 540, val loss: 1.6109379529953003
Epoch 550, training loss: 318.08282470703125 = 1.5608547925949097 + 50.0 * 6.330439567565918
Epoch 550, val loss: 1.6006971597671509
Epoch 560, training loss: 317.9180908203125 = 1.5487513542175293 + 50.0 * 6.327386856079102
Epoch 560, val loss: 1.590332269668579
Epoch 570, training loss: 317.843505859375 = 1.5364965200424194 + 50.0 * 6.3261399269104
Epoch 570, val loss: 1.5798343420028687
Epoch 580, training loss: 317.650146484375 = 1.5241023302078247 + 50.0 * 6.322520732879639
Epoch 580, val loss: 1.5692723989486694
Epoch 590, training loss: 317.4800109863281 = 1.511631965637207 + 50.0 * 6.319367408752441
Epoch 590, val loss: 1.558656096458435
Epoch 600, training loss: 317.4844665527344 = 1.4991337060928345 + 50.0 * 6.319706439971924
Epoch 600, val loss: 1.5479578971862793
Epoch 610, training loss: 317.21636962890625 = 1.4864542484283447 + 50.0 * 6.314598560333252
Epoch 610, val loss: 1.5372883081436157
Epoch 620, training loss: 317.0823974609375 = 1.47385835647583 + 50.0 * 6.31217098236084
Epoch 620, val loss: 1.526643991470337
Epoch 630, training loss: 316.9397277832031 = 1.4612773656845093 + 50.0 * 6.309569358825684
Epoch 630, val loss: 1.5160205364227295
Epoch 640, training loss: 316.88665771484375 = 1.4486570358276367 + 50.0 * 6.308759689331055
Epoch 640, val loss: 1.5054441690444946
Epoch 650, training loss: 316.7709045410156 = 1.4360793828964233 + 50.0 * 6.306696891784668
Epoch 650, val loss: 1.494828701019287
Epoch 660, training loss: 316.6109924316406 = 1.4235771894454956 + 50.0 * 6.30374813079834
Epoch 660, val loss: 1.484278917312622
Epoch 670, training loss: 316.4765319824219 = 1.411083698272705 + 50.0 * 6.301309108734131
Epoch 670, val loss: 1.4738476276397705
Epoch 680, training loss: 316.3684387207031 = 1.3986732959747314 + 50.0 * 6.2993950843811035
Epoch 680, val loss: 1.4635119438171387
Epoch 690, training loss: 316.3247985839844 = 1.3863025903701782 + 50.0 * 6.298770427703857
Epoch 690, val loss: 1.4532089233398438
Epoch 700, training loss: 316.17822265625 = 1.3740428686141968 + 50.0 * 6.296083450317383
Epoch 700, val loss: 1.442893624305725
Epoch 710, training loss: 316.08612060546875 = 1.361804485321045 + 50.0 * 6.294486045837402
Epoch 710, val loss: 1.4327332973480225
Epoch 720, training loss: 315.9765930175781 = 1.3496699333190918 + 50.0 * 6.292538166046143
Epoch 720, val loss: 1.4226385354995728
Epoch 730, training loss: 315.9055480957031 = 1.3376030921936035 + 50.0 * 6.291358947753906
Epoch 730, val loss: 1.4126235246658325
Epoch 740, training loss: 315.7820739746094 = 1.325624704360962 + 50.0 * 6.28912878036499
Epoch 740, val loss: 1.4027200937271118
Epoch 750, training loss: 315.7000427246094 = 1.3137234449386597 + 50.0 * 6.287726402282715
Epoch 750, val loss: 1.3929412364959717
Epoch 760, training loss: 315.83416748046875 = 1.3018559217453003 + 50.0 * 6.290646076202393
Epoch 760, val loss: 1.3831965923309326
Epoch 770, training loss: 315.5746154785156 = 1.2901408672332764 + 50.0 * 6.285689830780029
Epoch 770, val loss: 1.3735432624816895
Epoch 780, training loss: 315.4586181640625 = 1.278480052947998 + 50.0 * 6.283602714538574
Epoch 780, val loss: 1.363940954208374
Epoch 790, training loss: 315.37225341796875 = 1.2669124603271484 + 50.0 * 6.282106876373291
Epoch 790, val loss: 1.3544279336929321
Epoch 800, training loss: 315.335205078125 = 1.2554136514663696 + 50.0 * 6.2815961837768555
Epoch 800, val loss: 1.3450725078582764
Epoch 810, training loss: 315.2396240234375 = 1.2439910173416138 + 50.0 * 6.27991247177124
Epoch 810, val loss: 1.3356176614761353
Epoch 820, training loss: 315.1775817871094 = 1.2326234579086304 + 50.0 * 6.2788987159729
Epoch 820, val loss: 1.3264179229736328
Epoch 830, training loss: 315.1728210449219 = 1.2213541269302368 + 50.0 * 6.279029846191406
Epoch 830, val loss: 1.3172025680541992
Epoch 840, training loss: 315.05535888671875 = 1.2101079225540161 + 50.0 * 6.276905059814453
Epoch 840, val loss: 1.3080689907073975
Epoch 850, training loss: 314.9517822265625 = 1.1989697217941284 + 50.0 * 6.275055885314941
Epoch 850, val loss: 1.299012303352356
Epoch 860, training loss: 314.8785095214844 = 1.1878862380981445 + 50.0 * 6.273812294006348
Epoch 860, val loss: 1.29005765914917
Epoch 870, training loss: 315.0529479980469 = 1.1768763065338135 + 50.0 * 6.27752161026001
Epoch 870, val loss: 1.2811261415481567
Epoch 880, training loss: 314.7641906738281 = 1.1659104824066162 + 50.0 * 6.271965503692627
Epoch 880, val loss: 1.2722898721694946
Epoch 890, training loss: 314.71044921875 = 1.1549973487854004 + 50.0 * 6.271109104156494
Epoch 890, val loss: 1.2635271549224854
Epoch 900, training loss: 314.6195983886719 = 1.1441699266433716 + 50.0 * 6.2695088386535645
Epoch 900, val loss: 1.2548586130142212
Epoch 910, training loss: 314.5592346191406 = 1.1333942413330078 + 50.0 * 6.268516540527344
Epoch 910, val loss: 1.2462317943572998
Epoch 920, training loss: 314.5054016113281 = 1.1226712465286255 + 50.0 * 6.2676544189453125
Epoch 920, val loss: 1.2377028465270996
Epoch 930, training loss: 314.55108642578125 = 1.1119383573532104 + 50.0 * 6.268783092498779
Epoch 930, val loss: 1.229246973991394
Epoch 940, training loss: 314.4462585449219 = 1.1012952327728271 + 50.0 * 6.266899108886719
Epoch 940, val loss: 1.2207340002059937
Epoch 950, training loss: 314.3529968261719 = 1.0906596183776855 + 50.0 * 6.265246391296387
Epoch 950, val loss: 1.212419033050537
Epoch 960, training loss: 314.2772216796875 = 1.0800981521606445 + 50.0 * 6.263942241668701
Epoch 960, val loss: 1.2040443420410156
Epoch 970, training loss: 314.21258544921875 = 1.0695749521255493 + 50.0 * 6.26285982131958
Epoch 970, val loss: 1.1958513259887695
Epoch 980, training loss: 314.1650390625 = 1.0590977668762207 + 50.0 * 6.262118339538574
Epoch 980, val loss: 1.1876435279846191
Epoch 990, training loss: 314.268798828125 = 1.0486843585968018 + 50.0 * 6.264402389526367
Epoch 990, val loss: 1.1794767379760742
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.5814814814814815
0.838165524512388
=== training gcn model ===
Epoch 0, training loss: 431.7882995605469 = 1.9456369876861572 + 50.0 * 8.596853256225586
Epoch 0, val loss: 1.9619942903518677
Epoch 10, training loss: 431.77313232421875 = 1.9414913654327393 + 50.0 * 8.596632957458496
Epoch 10, val loss: 1.957528829574585
Epoch 20, training loss: 431.73406982421875 = 1.9368808269500732 + 50.0 * 8.595943450927734
Epoch 20, val loss: 1.9525048732757568
Epoch 30, training loss: 431.6044006347656 = 1.9315571784973145 + 50.0 * 8.593457221984863
Epoch 30, val loss: 1.9466993808746338
Epoch 40, training loss: 431.1402893066406 = 1.9252464771270752 + 50.0 * 8.584300994873047
Epoch 40, val loss: 1.9398084878921509
Epoch 50, training loss: 429.61846923828125 = 1.917480230331421 + 50.0 * 8.554019927978516
Epoch 50, val loss: 1.9312822818756104
Epoch 60, training loss: 425.38079833984375 = 1.9082036018371582 + 50.0 * 8.469451904296875
Epoch 60, val loss: 1.9211773872375488
Epoch 70, training loss: 414.69805908203125 = 1.898565411567688 + 50.0 * 8.255990028381348
Epoch 70, val loss: 1.9107496738433838
Epoch 80, training loss: 400.3216857910156 = 1.8881548643112183 + 50.0 * 7.96867036819458
Epoch 80, val loss: 1.8996021747589111
Epoch 90, training loss: 390.3421630859375 = 1.879676103591919 + 50.0 * 7.76924991607666
Epoch 90, val loss: 1.890672206878662
Epoch 100, training loss: 374.7945251464844 = 1.8727221488952637 + 50.0 * 7.458436012268066
Epoch 100, val loss: 1.8833485841751099
Epoch 110, training loss: 359.979248046875 = 1.866073727607727 + 50.0 * 7.1622633934021
Epoch 110, val loss: 1.8765898942947388
Epoch 120, training loss: 349.91900634765625 = 1.8605252504348755 + 50.0 * 6.961169719696045
Epoch 120, val loss: 1.8711836338043213
Epoch 130, training loss: 343.6197814941406 = 1.856204628944397 + 50.0 * 6.835271835327148
Epoch 130, val loss: 1.8668464422225952
Epoch 140, training loss: 339.28204345703125 = 1.8522779941558838 + 50.0 * 6.748595714569092
Epoch 140, val loss: 1.8627090454101562
Epoch 150, training loss: 336.1923522949219 = 1.848073124885559 + 50.0 * 6.686885356903076
Epoch 150, val loss: 1.8582994937896729
Epoch 160, training loss: 333.9716491699219 = 1.8435618877410889 + 50.0 * 6.642561912536621
Epoch 160, val loss: 1.8536182641983032
Epoch 170, training loss: 332.4294738769531 = 1.8391485214233398 + 50.0 * 6.611806869506836
Epoch 170, val loss: 1.849029302597046
Epoch 180, training loss: 331.2593994140625 = 1.8349103927612305 + 50.0 * 6.588490009307861
Epoch 180, val loss: 1.8445276021957397
Epoch 190, training loss: 330.304931640625 = 1.8307172060012817 + 50.0 * 6.569484233856201
Epoch 190, val loss: 1.8401644229888916
Epoch 200, training loss: 329.4571838378906 = 1.8266096115112305 + 50.0 * 6.552611827850342
Epoch 200, val loss: 1.8359228372573853
Epoch 210, training loss: 328.66741943359375 = 1.8226341009140015 + 50.0 * 6.536895751953125
Epoch 210, val loss: 1.8318071365356445
Epoch 220, training loss: 327.9210205078125 = 1.818770170211792 + 50.0 * 6.522045135498047
Epoch 220, val loss: 1.827764868736267
Epoch 230, training loss: 327.1904296875 = 1.8149186372756958 + 50.0 * 6.507510662078857
Epoch 230, val loss: 1.8238085508346558
Epoch 240, training loss: 326.5186462402344 = 1.8111203908920288 + 50.0 * 6.494150638580322
Epoch 240, val loss: 1.8198870420455933
Epoch 250, training loss: 325.89892578125 = 1.8072761297225952 + 50.0 * 6.481832981109619
Epoch 250, val loss: 1.8160136938095093
Epoch 260, training loss: 325.32281494140625 = 1.8034050464630127 + 50.0 * 6.470388412475586
Epoch 260, val loss: 1.8121188879013062
Epoch 270, training loss: 324.8036193847656 = 1.7994495630264282 + 50.0 * 6.460083484649658
Epoch 270, val loss: 1.8082045316696167
Epoch 280, training loss: 324.42388916015625 = 1.7954037189483643 + 50.0 * 6.452569484710693
Epoch 280, val loss: 1.8041929006576538
Epoch 290, training loss: 323.89935302734375 = 1.7911715507507324 + 50.0 * 6.442163467407227
Epoch 290, val loss: 1.8001203536987305
Epoch 300, training loss: 323.4433898925781 = 1.7868609428405762 + 50.0 * 6.433130741119385
Epoch 300, val loss: 1.7959554195404053
Epoch 310, training loss: 323.0169677734375 = 1.7824106216430664 + 50.0 * 6.424691200256348
Epoch 310, val loss: 1.791717529296875
Epoch 320, training loss: 322.6249694824219 = 1.7778127193450928 + 50.0 * 6.416943073272705
Epoch 320, val loss: 1.7873647212982178
Epoch 330, training loss: 322.3356628417969 = 1.7730300426483154 + 50.0 * 6.411252498626709
Epoch 330, val loss: 1.7828876972198486
Epoch 340, training loss: 321.9034423828125 = 1.7680137157440186 + 50.0 * 6.402708530426025
Epoch 340, val loss: 1.7782162427902222
Epoch 350, training loss: 321.57806396484375 = 1.7627995014190674 + 50.0 * 6.396305561065674
Epoch 350, val loss: 1.7734211683273315
Epoch 360, training loss: 321.25640869140625 = 1.757364273071289 + 50.0 * 6.389980792999268
Epoch 360, val loss: 1.7684518098831177
Epoch 370, training loss: 320.9493713378906 = 1.7516889572143555 + 50.0 * 6.38395357131958
Epoch 370, val loss: 1.7633042335510254
Epoch 380, training loss: 320.7804260253906 = 1.7457284927368164 + 50.0 * 6.3806939125061035
Epoch 380, val loss: 1.7579686641693115
Epoch 390, training loss: 320.402587890625 = 1.7395108938217163 + 50.0 * 6.373261451721191
Epoch 390, val loss: 1.7523512840270996
Epoch 400, training loss: 320.1466979980469 = 1.7329906225204468 + 50.0 * 6.368273735046387
Epoch 400, val loss: 1.7465054988861084
Epoch 410, training loss: 319.930908203125 = 1.726173996925354 + 50.0 * 6.3640947341918945
Epoch 410, val loss: 1.7403942346572876
Epoch 420, training loss: 319.6966857910156 = 1.718994379043579 + 50.0 * 6.359553813934326
Epoch 420, val loss: 1.7340360879898071
Epoch 430, training loss: 319.434326171875 = 1.7115105390548706 + 50.0 * 6.354455947875977
Epoch 430, val loss: 1.7273991107940674
Epoch 440, training loss: 319.2392578125 = 1.7036705017089844 + 50.0 * 6.350711822509766
Epoch 440, val loss: 1.7204821109771729
Epoch 450, training loss: 319.0552673339844 = 1.6954655647277832 + 50.0 * 6.347196102142334
Epoch 450, val loss: 1.713257908821106
Epoch 460, training loss: 318.8892822265625 = 1.6868770122528076 + 50.0 * 6.344047546386719
Epoch 460, val loss: 1.7057266235351562
Epoch 470, training loss: 318.8257751464844 = 1.6778746843338013 + 50.0 * 6.342957973480225
Epoch 470, val loss: 1.697899580001831
Epoch 480, training loss: 318.5329284667969 = 1.6685841083526611 + 50.0 * 6.337286949157715
Epoch 480, val loss: 1.6896740198135376
Epoch 490, training loss: 318.38006591796875 = 1.6588716506958008 + 50.0 * 6.334424018859863
Epoch 490, val loss: 1.6811528205871582
Epoch 500, training loss: 318.2195739746094 = 1.6487654447555542 + 50.0 * 6.331416130065918
Epoch 500, val loss: 1.672364354133606
Epoch 510, training loss: 318.072265625 = 1.6383291482925415 + 50.0 * 6.328678607940674
Epoch 510, val loss: 1.6632426977157593
Epoch 520, training loss: 317.9344177246094 = 1.6275144815444946 + 50.0 * 6.326138019561768
Epoch 520, val loss: 1.653843879699707
Epoch 530, training loss: 317.8170166015625 = 1.616344690322876 + 50.0 * 6.324013710021973
Epoch 530, val loss: 1.64414644241333
Epoch 540, training loss: 317.6769714355469 = 1.6048389673233032 + 50.0 * 6.321442604064941
Epoch 540, val loss: 1.634145975112915
Epoch 550, training loss: 317.57537841796875 = 1.5930174589157104 + 50.0 * 6.319647312164307
Epoch 550, val loss: 1.6238899230957031
Epoch 560, training loss: 317.4529113769531 = 1.5808786153793335 + 50.0 * 6.317440509796143
Epoch 560, val loss: 1.6134085655212402
Epoch 570, training loss: 317.3264465332031 = 1.5684031248092651 + 50.0 * 6.315161228179932
Epoch 570, val loss: 1.6027261018753052
Epoch 580, training loss: 317.21234130859375 = 1.555727481842041 + 50.0 * 6.313132286071777
Epoch 580, val loss: 1.5917932987213135
Epoch 590, training loss: 317.0806579589844 = 1.542792558670044 + 50.0 * 6.310757160186768
Epoch 590, val loss: 1.5806437730789185
Epoch 600, training loss: 317.0102844238281 = 1.5296472311019897 + 50.0 * 6.309613227844238
Epoch 600, val loss: 1.569311261177063
Epoch 610, training loss: 316.88525390625 = 1.5161930322647095 + 50.0 * 6.3073811531066895
Epoch 610, val loss: 1.5578802824020386
Epoch 620, training loss: 316.7719421386719 = 1.5026174783706665 + 50.0 * 6.305386066436768
Epoch 620, val loss: 1.5462098121643066
Epoch 630, training loss: 316.67083740234375 = 1.4888362884521484 + 50.0 * 6.303640365600586
Epoch 630, val loss: 1.5344483852386475
Epoch 640, training loss: 316.5593566894531 = 1.4748986959457397 + 50.0 * 6.301689147949219
Epoch 640, val loss: 1.5226027965545654
Epoch 650, training loss: 316.45697021484375 = 1.4608008861541748 + 50.0 * 6.299922943115234
Epoch 650, val loss: 1.5106619596481323
Epoch 660, training loss: 316.56732177734375 = 1.446555256843567 + 50.0 * 6.302414894104004
Epoch 660, val loss: 1.498679280281067
Epoch 670, training loss: 316.2861328125 = 1.4322140216827393 + 50.0 * 6.2970781326293945
Epoch 670, val loss: 1.486510157585144
Epoch 680, training loss: 316.1795349121094 = 1.4177541732788086 + 50.0 * 6.295235633850098
Epoch 680, val loss: 1.47430419921875
Epoch 690, training loss: 316.08154296875 = 1.4031951427459717 + 50.0 * 6.293566703796387
Epoch 690, val loss: 1.462054967880249
Epoch 700, training loss: 316.0789489746094 = 1.3886007070541382 + 50.0 * 6.293806552886963
Epoch 700, val loss: 1.4497274160385132
Epoch 710, training loss: 315.9771423339844 = 1.373785138130188 + 50.0 * 6.292067050933838
Epoch 710, val loss: 1.4374918937683105
Epoch 720, training loss: 315.8380432128906 = 1.3590118885040283 + 50.0 * 6.289580821990967
Epoch 720, val loss: 1.4251086711883545
Epoch 730, training loss: 315.7438659667969 = 1.3441376686096191 + 50.0 * 6.287994384765625
Epoch 730, val loss: 1.4127882719039917
Epoch 740, training loss: 315.6588439941406 = 1.32925283908844 + 50.0 * 6.286591529846191
Epoch 740, val loss: 1.400431752204895
Epoch 750, training loss: 315.60113525390625 = 1.3143138885498047 + 50.0 * 6.285736560821533
Epoch 750, val loss: 1.38813054561615
Epoch 760, training loss: 315.5147399902344 = 1.2992993593215942 + 50.0 * 6.284309387207031
Epoch 760, val loss: 1.375794529914856
Epoch 770, training loss: 315.4565734863281 = 1.2843027114868164 + 50.0 * 6.283445358276367
Epoch 770, val loss: 1.3634952306747437
Epoch 780, training loss: 315.4626159667969 = 1.2693058252334595 + 50.0 * 6.283865928649902
Epoch 780, val loss: 1.351190209388733
Epoch 790, training loss: 315.31109619140625 = 1.2541377544403076 + 50.0 * 6.281138896942139
Epoch 790, val loss: 1.339005947113037
Epoch 800, training loss: 315.22381591796875 = 1.239149570465088 + 50.0 * 6.279693603515625
Epoch 800, val loss: 1.3268108367919922
Epoch 810, training loss: 315.1522216796875 = 1.2240934371948242 + 50.0 * 6.278562545776367
Epoch 810, val loss: 1.3147433996200562
Epoch 820, training loss: 315.1475524902344 = 1.2091021537780762 + 50.0 * 6.278769016265869
Epoch 820, val loss: 1.3027324676513672
Epoch 830, training loss: 315.0130920410156 = 1.1941559314727783 + 50.0 * 6.276378631591797
Epoch 830, val loss: 1.2907352447509766
Epoch 840, training loss: 314.93670654296875 = 1.179233193397522 + 50.0 * 6.275149822235107
Epoch 840, val loss: 1.2788881063461304
Epoch 850, training loss: 314.92974853515625 = 1.1643671989440918 + 50.0 * 6.275307655334473
Epoch 850, val loss: 1.2671990394592285
Epoch 860, training loss: 314.826171875 = 1.149640679359436 + 50.0 * 6.273530960083008
Epoch 860, val loss: 1.2554875612258911
Epoch 870, training loss: 314.7867736816406 = 1.1348868608474731 + 50.0 * 6.273037910461426
Epoch 870, val loss: 1.2440316677093506
Epoch 880, training loss: 314.6887512207031 = 1.120302677154541 + 50.0 * 6.271368980407715
Epoch 880, val loss: 1.2326594591140747
Epoch 890, training loss: 314.6602783203125 = 1.1058340072631836 + 50.0 * 6.27108907699585
Epoch 890, val loss: 1.2213982343673706
Epoch 900, training loss: 314.5823059082031 = 1.091399908065796 + 50.0 * 6.269818305969238
Epoch 900, val loss: 1.2103278636932373
Epoch 910, training loss: 314.53521728515625 = 1.0770864486694336 + 50.0 * 6.269162654876709
Epoch 910, val loss: 1.1994050741195679
Epoch 920, training loss: 314.4591064453125 = 1.062907338142395 + 50.0 * 6.2679243087768555
Epoch 920, val loss: 1.1886157989501953
Epoch 930, training loss: 314.4244079589844 = 1.048869013786316 + 50.0 * 6.267510890960693
Epoch 930, val loss: 1.1780245304107666
Epoch 940, training loss: 314.37689208984375 = 1.034959077835083 + 50.0 * 6.266838073730469
Epoch 940, val loss: 1.1675869226455688
Epoch 950, training loss: 314.3284912109375 = 1.0211395025253296 + 50.0 * 6.266147136688232
Epoch 950, val loss: 1.1573842763900757
Epoch 960, training loss: 314.2636413574219 = 1.0074938535690308 + 50.0 * 6.265122890472412
Epoch 960, val loss: 1.1473370790481567
Epoch 970, training loss: 314.2629699707031 = 0.993977963924408 + 50.0 * 6.265379428863525
Epoch 970, val loss: 1.1375011205673218
Epoch 980, training loss: 314.25677490234375 = 0.9806565046310425 + 50.0 * 6.265522480010986
Epoch 980, val loss: 1.1278856992721558
Epoch 990, training loss: 314.13037109375 = 0.967467725276947 + 50.0 * 6.26325798034668
Epoch 990, val loss: 1.1183454990386963
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.662962962962963
0.8344754876120191
=== training gcn model ===
Epoch 0, training loss: 431.7911071777344 = 1.94728684425354 + 50.0 * 8.59687614440918
Epoch 0, val loss: 1.9463821649551392
Epoch 10, training loss: 431.7787780761719 = 1.9430019855499268 + 50.0 * 8.596715927124023
Epoch 10, val loss: 1.941894769668579
Epoch 20, training loss: 431.7491760253906 = 1.9383193254470825 + 50.0 * 8.596217155456543
Epoch 20, val loss: 1.9369463920593262
Epoch 30, training loss: 431.65509033203125 = 1.9329360723495483 + 50.0 * 8.594443321228027
Epoch 30, val loss: 1.9312058687210083
Epoch 40, training loss: 431.312255859375 = 1.9264482259750366 + 50.0 * 8.587716102600098
Epoch 40, val loss: 1.9242974519729614
Epoch 50, training loss: 430.12286376953125 = 1.918503999710083 + 50.0 * 8.5640869140625
Epoch 50, val loss: 1.9159290790557861
Epoch 60, training loss: 426.5797424316406 = 1.9089183807373047 + 50.0 * 8.493416786193848
Epoch 60, val loss: 1.9060852527618408
Epoch 70, training loss: 415.83984375 = 1.8995823860168457 + 50.0 * 8.278804779052734
Epoch 70, val loss: 1.8968801498413086
Epoch 80, training loss: 390.00775146484375 = 1.8892816305160522 + 50.0 * 7.762369155883789
Epoch 80, val loss: 1.8865467309951782
Epoch 90, training loss: 372.6936950683594 = 1.8790315389633179 + 50.0 * 7.416293144226074
Epoch 90, val loss: 1.87650465965271
Epoch 100, training loss: 360.3357238769531 = 1.8713759183883667 + 50.0 * 7.169287204742432
Epoch 100, val loss: 1.868813395500183
Epoch 110, training loss: 354.15496826171875 = 1.864734172821045 + 50.0 * 7.045804500579834
Epoch 110, val loss: 1.8620189428329468
Epoch 120, training loss: 350.28955078125 = 1.858214259147644 + 50.0 * 6.968626499176025
Epoch 120, val loss: 1.8554749488830566
Epoch 130, training loss: 346.6922912597656 = 1.852548360824585 + 50.0 * 6.896795272827148
Epoch 130, val loss: 1.8499071598052979
Epoch 140, training loss: 343.4432067871094 = 1.8472135066986084 + 50.0 * 6.831920146942139
Epoch 140, val loss: 1.844785213470459
Epoch 150, training loss: 340.6441345214844 = 1.8425105810165405 + 50.0 * 6.776032447814941
Epoch 150, val loss: 1.8403444290161133
Epoch 160, training loss: 338.0022277832031 = 1.8379817008972168 + 50.0 * 6.72328519821167
Epoch 160, val loss: 1.836064100265503
Epoch 170, training loss: 335.7010498046875 = 1.8335336446762085 + 50.0 * 6.6773505210876465
Epoch 170, val loss: 1.8318458795547485
Epoch 180, training loss: 333.93096923828125 = 1.8290828466415405 + 50.0 * 6.642037868499756
Epoch 180, val loss: 1.827633261680603
Epoch 190, training loss: 332.5807189941406 = 1.8247315883636475 + 50.0 * 6.615119457244873
Epoch 190, val loss: 1.8234820365905762
Epoch 200, training loss: 331.4289245605469 = 1.8203483819961548 + 50.0 * 6.592171669006348
Epoch 200, val loss: 1.819459080696106
Epoch 210, training loss: 330.410888671875 = 1.816194772720337 + 50.0 * 6.57189416885376
Epoch 210, val loss: 1.8155691623687744
Epoch 220, training loss: 329.4797668457031 = 1.8121222257614136 + 50.0 * 6.5533528327941895
Epoch 220, val loss: 1.8117948770523071
Epoch 230, training loss: 328.6241760253906 = 1.8081177473068237 + 50.0 * 6.536321640014648
Epoch 230, val loss: 1.8080586194992065
Epoch 240, training loss: 327.92169189453125 = 1.8041728734970093 + 50.0 * 6.522350311279297
Epoch 240, val loss: 1.8043073415756226
Epoch 250, training loss: 327.1479187011719 = 1.8001402616500854 + 50.0 * 6.506955623626709
Epoch 250, val loss: 1.8006350994110107
Epoch 260, training loss: 326.4846496582031 = 1.7961609363555908 + 50.0 * 6.493769645690918
Epoch 260, val loss: 1.7969807386398315
Epoch 270, training loss: 325.8855895996094 = 1.7921127080917358 + 50.0 * 6.481869220733643
Epoch 270, val loss: 1.7932854890823364
Epoch 280, training loss: 325.3512878417969 = 1.7879540920257568 + 50.0 * 6.471266269683838
Epoch 280, val loss: 1.7895475625991821
Epoch 290, training loss: 324.9114685058594 = 1.7837224006652832 + 50.0 * 6.462554931640625
Epoch 290, val loss: 1.7856965065002441
Epoch 300, training loss: 324.4238586425781 = 1.7793219089508057 + 50.0 * 6.452890396118164
Epoch 300, val loss: 1.7818387746810913
Epoch 310, training loss: 323.9924621582031 = 1.7748069763183594 + 50.0 * 6.444353103637695
Epoch 310, val loss: 1.777919888496399
Epoch 320, training loss: 323.60113525390625 = 1.7701826095581055 + 50.0 * 6.436619281768799
Epoch 320, val loss: 1.773932933807373
Epoch 330, training loss: 323.23486328125 = 1.7654109001159668 + 50.0 * 6.429388999938965
Epoch 330, val loss: 1.7698527574539185
Epoch 340, training loss: 322.9414367675781 = 1.760433316230774 + 50.0 * 6.423620223999023
Epoch 340, val loss: 1.7656431198120117
Epoch 350, training loss: 322.5984191894531 = 1.7552953958511353 + 50.0 * 6.4168620109558105
Epoch 350, val loss: 1.7613071203231812
Epoch 360, training loss: 322.2792663574219 = 1.7499735355377197 + 50.0 * 6.410585880279541
Epoch 360, val loss: 1.7568424940109253
Epoch 370, training loss: 321.9844665527344 = 1.7444738149642944 + 50.0 * 6.404799938201904
Epoch 370, val loss: 1.752261996269226
Epoch 380, training loss: 321.6941223144531 = 1.7387508153915405 + 50.0 * 6.399107933044434
Epoch 380, val loss: 1.7475531101226807
Epoch 390, training loss: 321.5748291015625 = 1.732801914215088 + 50.0 * 6.396840572357178
Epoch 390, val loss: 1.7426561117172241
Epoch 400, training loss: 321.188232421875 = 1.726598858833313 + 50.0 * 6.389232635498047
Epoch 400, val loss: 1.7376240491867065
Epoch 410, training loss: 320.9814147949219 = 1.7201777696609497 + 50.0 * 6.385224342346191
Epoch 410, val loss: 1.7324329614639282
Epoch 420, training loss: 320.658447265625 = 1.7134031057357788 + 50.0 * 6.37890100479126
Epoch 420, val loss: 1.7270410060882568
Epoch 430, training loss: 320.4097595214844 = 1.7064623832702637 + 50.0 * 6.374066352844238
Epoch 430, val loss: 1.721483588218689
Epoch 440, training loss: 320.1909484863281 = 1.699200987815857 + 50.0 * 6.369834899902344
Epoch 440, val loss: 1.715751051902771
Epoch 450, training loss: 319.9914855957031 = 1.6916520595550537 + 50.0 * 6.365996837615967
Epoch 450, val loss: 1.7097898721694946
Epoch 460, training loss: 319.76812744140625 = 1.683825969696045 + 50.0 * 6.361685752868652
Epoch 460, val loss: 1.703599452972412
Epoch 470, training loss: 319.6024169921875 = 1.6757152080535889 + 50.0 * 6.35853385925293
Epoch 470, val loss: 1.6971951723098755
Epoch 480, training loss: 319.385498046875 = 1.6672418117523193 + 50.0 * 6.354365348815918
Epoch 480, val loss: 1.690565824508667
Epoch 490, training loss: 319.20263671875 = 1.6584672927856445 + 50.0 * 6.3508830070495605
Epoch 490, val loss: 1.6837701797485352
Epoch 500, training loss: 319.0848388671875 = 1.6494381427764893 + 50.0 * 6.348707675933838
Epoch 500, val loss: 1.6767444610595703
Epoch 510, training loss: 318.9849548339844 = 1.6399275064468384 + 50.0 * 6.346900463104248
Epoch 510, val loss: 1.6694353818893433
Epoch 520, training loss: 318.73150634765625 = 1.6302646398544312 + 50.0 * 6.342024803161621
Epoch 520, val loss: 1.6619731187820435
Epoch 530, training loss: 318.56640625 = 1.6202998161315918 + 50.0 * 6.338922023773193
Epoch 530, val loss: 1.6543207168579102
Epoch 540, training loss: 318.4071960449219 = 1.6100585460662842 + 50.0 * 6.335943222045898
Epoch 540, val loss: 1.6464881896972656
Epoch 550, training loss: 318.255126953125 = 1.5995579957962036 + 50.0 * 6.333111763000488
Epoch 550, val loss: 1.6384916305541992
Epoch 560, training loss: 318.1315612792969 = 1.5888046026229858 + 50.0 * 6.330854892730713
Epoch 560, val loss: 1.6303231716156006
Epoch 570, training loss: 318.06219482421875 = 1.5777586698532104 + 50.0 * 6.329688549041748
Epoch 570, val loss: 1.6219910383224487
Epoch 580, training loss: 317.86065673828125 = 1.5664836168289185 + 50.0 * 6.325883388519287
Epoch 580, val loss: 1.6134984493255615
Epoch 590, training loss: 317.710693359375 = 1.5550110340118408 + 50.0 * 6.323113441467285
Epoch 590, val loss: 1.604920506477356
Epoch 600, training loss: 317.78668212890625 = 1.5434147119522095 + 50.0 * 6.324864864349365
Epoch 600, val loss: 1.5962311029434204
Epoch 610, training loss: 317.4787292480469 = 1.5313931703567505 + 50.0 * 6.318946361541748
Epoch 610, val loss: 1.587376356124878
Epoch 620, training loss: 317.3325500488281 = 1.5193294286727905 + 50.0 * 6.316264629364014
Epoch 620, val loss: 1.5784704685211182
Epoch 630, training loss: 317.21258544921875 = 1.5070934295654297 + 50.0 * 6.314109802246094
Epoch 630, val loss: 1.5695375204086304
Epoch 640, training loss: 317.0886535644531 = 1.4947432279586792 + 50.0 * 6.311878204345703
Epoch 640, val loss: 1.560518741607666
Epoch 650, training loss: 317.0034484863281 = 1.4822744131088257 + 50.0 * 6.310423374176025
Epoch 650, val loss: 1.5514349937438965
Epoch 660, training loss: 316.9063720703125 = 1.4695802927017212 + 50.0 * 6.3087358474731445
Epoch 660, val loss: 1.5423660278320312
Epoch 670, training loss: 316.7568664550781 = 1.4567934274673462 + 50.0 * 6.306001663208008
Epoch 670, val loss: 1.5331531763076782
Epoch 680, training loss: 316.6476745605469 = 1.4439665079116821 + 50.0 * 6.304073810577393
Epoch 680, val loss: 1.5240188837051392
Epoch 690, training loss: 316.5428161621094 = 1.431056022644043 + 50.0 * 6.302235126495361
Epoch 690, val loss: 1.5148862600326538
Epoch 700, training loss: 316.488037109375 = 1.4181288480758667 + 50.0 * 6.301398277282715
Epoch 700, val loss: 1.505805253982544
Epoch 710, training loss: 316.38079833984375 = 1.404967188835144 + 50.0 * 6.299516677856445
Epoch 710, val loss: 1.4965380430221558
Epoch 720, training loss: 316.2645263671875 = 1.3919075727462769 + 50.0 * 6.297452449798584
Epoch 720, val loss: 1.487473964691162
Epoch 730, training loss: 316.15826416015625 = 1.3787707090377808 + 50.0 * 6.295589447021484
Epoch 730, val loss: 1.4784514904022217
Epoch 740, training loss: 316.05804443359375 = 1.365619421005249 + 50.0 * 6.293848514556885
Epoch 740, val loss: 1.4694459438323975
Epoch 750, training loss: 315.9660339355469 = 1.3524911403656006 + 50.0 * 6.292271137237549
Epoch 750, val loss: 1.4605010747909546
Epoch 760, training loss: 315.90789794921875 = 1.3392980098724365 + 50.0 * 6.291371822357178
Epoch 760, val loss: 1.4516322612762451
Epoch 770, training loss: 315.8733215332031 = 1.3260822296142578 + 50.0 * 6.290945053100586
Epoch 770, val loss: 1.442646861076355
Epoch 780, training loss: 315.7292785644531 = 1.312931776046753 + 50.0 * 6.288326740264893
Epoch 780, val loss: 1.4338654279708862
Epoch 790, training loss: 315.635009765625 = 1.299748182296753 + 50.0 * 6.286705493927002
Epoch 790, val loss: 1.4251446723937988
Epoch 800, training loss: 315.55743408203125 = 1.2866451740264893 + 50.0 * 6.2854156494140625
Epoch 800, val loss: 1.4165453910827637
Epoch 810, training loss: 315.5265808105469 = 1.2735353708267212 + 50.0 * 6.285060882568359
Epoch 810, val loss: 1.408016324043274
Epoch 820, training loss: 315.5089111328125 = 1.2604351043701172 + 50.0 * 6.284969329833984
Epoch 820, val loss: 1.399537205696106
Epoch 830, training loss: 315.3466491699219 = 1.2474626302719116 + 50.0 * 6.281983375549316
Epoch 830, val loss: 1.391181230545044
Epoch 840, training loss: 315.2734375 = 1.234527587890625 + 50.0 * 6.280778408050537
Epoch 840, val loss: 1.3829474449157715
Epoch 850, training loss: 315.1972351074219 = 1.22162926197052 + 50.0 * 6.279512405395508
Epoch 850, val loss: 1.374825119972229
Epoch 860, training loss: 315.1304626464844 = 1.2088335752487183 + 50.0 * 6.278432846069336
Epoch 860, val loss: 1.3668267726898193
Epoch 870, training loss: 315.09912109375 = 1.1961132287979126 + 50.0 * 6.278060436248779
Epoch 870, val loss: 1.3590097427368164
Epoch 880, training loss: 315.0826721191406 = 1.183524489402771 + 50.0 * 6.277982711791992
Epoch 880, val loss: 1.3511767387390137
Epoch 890, training loss: 314.9629821777344 = 1.1708202362060547 + 50.0 * 6.275843620300293
Epoch 890, val loss: 1.343498706817627
Epoch 900, training loss: 314.8845520019531 = 1.1583539247512817 + 50.0 * 6.274523735046387
Epoch 900, val loss: 1.3360276222229004
Epoch 910, training loss: 314.8118896484375 = 1.1460084915161133 + 50.0 * 6.273317813873291
Epoch 910, val loss: 1.3286640644073486
Epoch 920, training loss: 314.7577209472656 = 1.1337511539459229 + 50.0 * 6.27247953414917
Epoch 920, val loss: 1.321486234664917
Epoch 930, training loss: 314.7119140625 = 1.121599555015564 + 50.0 * 6.271805763244629
Epoch 930, val loss: 1.3144387006759644
Epoch 940, training loss: 314.7875671386719 = 1.1095119714736938 + 50.0 * 6.273561477661133
Epoch 940, val loss: 1.3074491024017334
Epoch 950, training loss: 314.64605712890625 = 1.0977075099945068 + 50.0 * 6.270966529846191
Epoch 950, val loss: 1.300779938697815
Epoch 960, training loss: 314.5356140136719 = 1.085818886756897 + 50.0 * 6.268995761871338
Epoch 960, val loss: 1.29412841796875
Epoch 970, training loss: 314.46728515625 = 1.0742402076721191 + 50.0 * 6.2678608894348145
Epoch 970, val loss: 1.2877432107925415
Epoch 980, training loss: 314.4154357910156 = 1.0627596378326416 + 50.0 * 6.267053127288818
Epoch 980, val loss: 1.2814865112304688
Epoch 990, training loss: 314.46820068359375 = 1.0514286756515503 + 50.0 * 6.268335819244385
Epoch 990, val loss: 1.2754263877868652
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.5888888888888889
0.8376383763837639
The final CL Acc:0.61111, 0.03679, The final GNN Acc:0.83676, 0.00163
0.00086
