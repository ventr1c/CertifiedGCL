Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11642])
remove edge: torch.Size([2, 9530])
updated graph: torch.Size([2, 10616])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 19.12689781188965 = 1.9331822395324707 + 2.0 * 8.596858024597168
Epoch 0, val loss: 1.9215291738510132
Epoch 10, training loss: 19.116405487060547 = 1.9233289957046509 + 2.0 * 8.596538543701172
Epoch 10, val loss: 1.9120453596115112
Epoch 20, training loss: 19.097808837890625 = 1.9105358123779297 + 2.0 * 8.593636512756348
Epoch 20, val loss: 1.8993209600448608
Epoch 30, training loss: 19.034488677978516 = 1.892472267150879 + 2.0 * 8.571008682250977
Epoch 30, val loss: 1.88128662109375
Epoch 40, training loss: 18.778045654296875 = 1.869661569595337 + 2.0 * 8.454192161560059
Epoch 40, val loss: 1.8598487377166748
Epoch 50, training loss: 17.972599029541016 = 1.84446120262146 + 2.0 * 8.064068794250488
Epoch 50, val loss: 1.837242603302002
Epoch 60, training loss: 17.486974716186523 = 1.8219135999679565 + 2.0 * 7.832530975341797
Epoch 60, val loss: 1.8184574842453003
Epoch 70, training loss: 16.748088836669922 = 1.807494878768921 + 2.0 * 7.470296859741211
Epoch 70, val loss: 1.806525468826294
Epoch 80, training loss: 16.047168731689453 = 1.8000215291976929 + 2.0 * 7.123573303222656
Epoch 80, val loss: 1.8008575439453125
Epoch 90, training loss: 15.57862377166748 = 1.793509602546692 + 2.0 * 6.892557144165039
Epoch 90, val loss: 1.795070767402649
Epoch 100, training loss: 15.424590110778809 = 1.7827320098876953 + 2.0 * 6.820929050445557
Epoch 100, val loss: 1.785590410232544
Epoch 110, training loss: 15.320466995239258 = 1.7697564363479614 + 2.0 * 6.775355339050293
Epoch 110, val loss: 1.7748970985412598
Epoch 120, training loss: 15.213109970092773 = 1.75810968875885 + 2.0 * 6.727499961853027
Epoch 120, val loss: 1.76534104347229
Epoch 130, training loss: 15.111019134521484 = 1.746996521949768 + 2.0 * 6.682011127471924
Epoch 130, val loss: 1.7556819915771484
Epoch 140, training loss: 15.020750045776367 = 1.735069751739502 + 2.0 * 6.642840385437012
Epoch 140, val loss: 1.745345950126648
Epoch 150, training loss: 14.938108444213867 = 1.721756100654602 + 2.0 * 6.608176231384277
Epoch 150, val loss: 1.7344595193862915
Epoch 160, training loss: 14.867232322692871 = 1.7065373659133911 + 2.0 * 6.580347537994385
Epoch 160, val loss: 1.7221934795379639
Epoch 170, training loss: 14.811422348022461 = 1.6889824867248535 + 2.0 * 6.561220169067383
Epoch 170, val loss: 1.7079440355300903
Epoch 180, training loss: 14.751249313354492 = 1.668992519378662 + 2.0 * 6.541128635406494
Epoch 180, val loss: 1.691480278968811
Epoch 190, training loss: 14.696288108825684 = 1.646664023399353 + 2.0 * 6.5248122215271
Epoch 190, val loss: 1.6731805801391602
Epoch 200, training loss: 14.640738487243652 = 1.621996521949768 + 2.0 * 6.509370803833008
Epoch 200, val loss: 1.6530189514160156
Epoch 210, training loss: 14.584238052368164 = 1.594624638557434 + 2.0 * 6.49480676651001
Epoch 210, val loss: 1.6309365034103394
Epoch 220, training loss: 14.532512664794922 = 1.5644936561584473 + 2.0 * 6.484009742736816
Epoch 220, val loss: 1.6068007946014404
Epoch 230, training loss: 14.471333503723145 = 1.5323394536972046 + 2.0 * 6.469497203826904
Epoch 230, val loss: 1.581175684928894
Epoch 240, training loss: 14.413008689880371 = 1.498300552368164 + 2.0 * 6.4573540687561035
Epoch 240, val loss: 1.5544488430023193
Epoch 250, training loss: 14.353151321411133 = 1.4623929262161255 + 2.0 * 6.445379257202148
Epoch 250, val loss: 1.5266671180725098
Epoch 260, training loss: 14.299917221069336 = 1.4250948429107666 + 2.0 * 6.437411308288574
Epoch 260, val loss: 1.4983243942260742
Epoch 270, training loss: 14.247891426086426 = 1.3871231079101562 + 2.0 * 6.430384159088135
Epoch 270, val loss: 1.4700357913970947
Epoch 280, training loss: 14.186388969421387 = 1.349016547203064 + 2.0 * 6.418686389923096
Epoch 280, val loss: 1.44231116771698
Epoch 290, training loss: 14.133892059326172 = 1.3107472658157349 + 2.0 * 6.411572456359863
Epoch 290, val loss: 1.4149366617202759
Epoch 300, training loss: 14.08900260925293 = 1.2724379301071167 + 2.0 * 6.408282279968262
Epoch 300, val loss: 1.387892484664917
Epoch 310, training loss: 14.033195495605469 = 1.234510898590088 + 2.0 * 6.3993425369262695
Epoch 310, val loss: 1.3615859746932983
Epoch 320, training loss: 13.983942031860352 = 1.1970638036727905 + 2.0 * 6.393439292907715
Epoch 320, val loss: 1.3360158205032349
Epoch 330, training loss: 13.943448066711426 = 1.1596592664718628 + 2.0 * 6.391894340515137
Epoch 330, val loss: 1.3105947971343994
Epoch 340, training loss: 13.890362739562988 = 1.1227113008499146 + 2.0 * 6.383825778961182
Epoch 340, val loss: 1.285651683807373
Epoch 350, training loss: 13.845586776733398 = 1.0862314701080322 + 2.0 * 6.379677772521973
Epoch 350, val loss: 1.261381983757019
Epoch 360, training loss: 13.801212310791016 = 1.0499510765075684 + 2.0 * 6.3756303787231445
Epoch 360, val loss: 1.2373696565628052
Epoch 370, training loss: 13.759573936462402 = 1.0140453577041626 + 2.0 * 6.3727641105651855
Epoch 370, val loss: 1.2137106657028198
Epoch 380, training loss: 13.712976455688477 = 0.9786039590835571 + 2.0 * 6.367186069488525
Epoch 380, val loss: 1.1907376050949097
Epoch 390, training loss: 13.670085906982422 = 0.9435596466064453 + 2.0 * 6.363263130187988
Epoch 390, val loss: 1.1682637929916382
Epoch 400, training loss: 13.646448135375977 = 0.9089766144752502 + 2.0 * 6.3687357902526855
Epoch 400, val loss: 1.146433711051941
Epoch 410, training loss: 13.595184326171875 = 0.8752604722976685 + 2.0 * 6.359961986541748
Epoch 410, val loss: 1.1256197690963745
Epoch 420, training loss: 13.551876068115234 = 0.8423392176628113 + 2.0 * 6.3547682762146
Epoch 420, val loss: 1.1057074069976807
Epoch 430, training loss: 13.519964218139648 = 0.8100677728652954 + 2.0 * 6.354948043823242
Epoch 430, val loss: 1.0866482257843018
Epoch 440, training loss: 13.483932495117188 = 0.778649628162384 + 2.0 * 6.352641582489014
Epoch 440, val loss: 1.0685675144195557
Epoch 450, training loss: 13.442298889160156 = 0.7481074929237366 + 2.0 * 6.347095489501953
Epoch 450, val loss: 1.0516595840454102
Epoch 460, training loss: 13.406404495239258 = 0.7184785008430481 + 2.0 * 6.343963146209717
Epoch 460, val loss: 1.0357893705368042
Epoch 470, training loss: 13.376261711120605 = 0.689711332321167 + 2.0 * 6.34327507019043
Epoch 470, val loss: 1.021070957183838
Epoch 480, training loss: 13.350698471069336 = 0.6620320677757263 + 2.0 * 6.344333171844482
Epoch 480, val loss: 1.0076583623886108
Epoch 490, training loss: 13.313104629516602 = 0.635554850101471 + 2.0 * 6.338774681091309
Epoch 490, val loss: 0.9956157803535461
Epoch 500, training loss: 13.285077095031738 = 0.610120952129364 + 2.0 * 6.337478160858154
Epoch 500, val loss: 0.9847968220710754
Epoch 510, training loss: 13.254814147949219 = 0.5857219696044922 + 2.0 * 6.334546089172363
Epoch 510, val loss: 0.975242555141449
Epoch 520, training loss: 13.227588653564453 = 0.5624651908874512 + 2.0 * 6.33256196975708
Epoch 520, val loss: 0.9670730233192444
Epoch 530, training loss: 13.198945999145508 = 0.540044367313385 + 2.0 * 6.329450607299805
Epoch 530, val loss: 0.9599980115890503
Epoch 540, training loss: 13.174042701721191 = 0.5184108018875122 + 2.0 * 6.327816009521484
Epoch 540, val loss: 0.9539599418640137
Epoch 550, training loss: 13.164146423339844 = 0.49749326705932617 + 2.0 * 6.33332633972168
Epoch 550, val loss: 0.949027419090271
Epoch 560, training loss: 13.135294914245605 = 0.47745823860168457 + 2.0 * 6.32891845703125
Epoch 560, val loss: 0.9449220895767212
Epoch 570, training loss: 13.106077194213867 = 0.4581488370895386 + 2.0 * 6.3239641189575195
Epoch 570, val loss: 0.9418879151344299
Epoch 580, training loss: 13.080995559692383 = 0.4393438994884491 + 2.0 * 6.320826053619385
Epoch 580, val loss: 0.9395142793655396
Epoch 590, training loss: 13.059515953063965 = 0.4209137558937073 + 2.0 * 6.319301128387451
Epoch 590, val loss: 0.9377654194831848
Epoch 600, training loss: 13.05449390411377 = 0.40282246470451355 + 2.0 * 6.325835704803467
Epoch 600, val loss: 0.9365813732147217
Epoch 610, training loss: 13.020280838012695 = 0.3852423429489136 + 2.0 * 6.317519187927246
Epoch 610, val loss: 0.9360306262969971
Epoch 620, training loss: 13.005151748657227 = 0.36802640557289124 + 2.0 * 6.3185625076293945
Epoch 620, val loss: 0.9360498785972595
Epoch 630, training loss: 12.979040145874023 = 0.3512504994869232 + 2.0 * 6.313894748687744
Epoch 630, val loss: 0.9364244341850281
Epoch 640, training loss: 12.959077835083008 = 0.3348792493343353 + 2.0 * 6.312099456787109
Epoch 640, val loss: 0.9373573064804077
Epoch 650, training loss: 12.945262908935547 = 0.3189423978328705 + 2.0 * 6.313160419464111
Epoch 650, val loss: 0.9385371208190918
Epoch 660, training loss: 12.921209335327148 = 0.3034920394420624 + 2.0 * 6.308858871459961
Epoch 660, val loss: 0.9402254819869995
Epoch 670, training loss: 12.90428352355957 = 0.2884567975997925 + 2.0 * 6.307913303375244
Epoch 670, val loss: 0.942200243473053
Epoch 680, training loss: 12.900290489196777 = 0.27386847138404846 + 2.0 * 6.313210964202881
Epoch 680, val loss: 0.9444692134857178
Epoch 690, training loss: 12.874844551086426 = 0.2597416043281555 + 2.0 * 6.307551383972168
Epoch 690, val loss: 0.9468790292739868
Epoch 700, training loss: 12.864787101745605 = 0.24618104100227356 + 2.0 * 6.309302806854248
Epoch 700, val loss: 0.949666440486908
Epoch 710, training loss: 12.840291023254395 = 0.23319245874881744 + 2.0 * 6.303549289703369
Epoch 710, val loss: 0.9527390003204346
Epoch 720, training loss: 12.824311256408691 = 0.2207612693309784 + 2.0 * 6.301774978637695
Epoch 720, val loss: 0.9562124609947205
Epoch 730, training loss: 12.809840202331543 = 0.208877295255661 + 2.0 * 6.30048131942749
Epoch 730, val loss: 0.9599441885948181
Epoch 740, training loss: 12.805610656738281 = 0.1975690871477127 + 2.0 * 6.304020881652832
Epoch 740, val loss: 0.9639692306518555
Epoch 750, training loss: 12.78441333770752 = 0.18685568869113922 + 2.0 * 6.298779010772705
Epoch 750, val loss: 0.9683769941329956
Epoch 760, training loss: 12.769847869873047 = 0.1767730861902237 + 2.0 * 6.296537399291992
Epoch 760, val loss: 0.9732015132904053
Epoch 770, training loss: 12.757636070251465 = 0.16725723445415497 + 2.0 * 6.295189380645752
Epoch 770, val loss: 0.9783134460449219
Epoch 780, training loss: 12.756189346313477 = 0.15829616785049438 + 2.0 * 6.298946380615234
Epoch 780, val loss: 0.9838852286338806
Epoch 790, training loss: 12.74710750579834 = 0.14987537264823914 + 2.0 * 6.2986159324646
Epoch 790, val loss: 0.9894366264343262
Epoch 800, training loss: 12.728094100952148 = 0.1420459747314453 + 2.0 * 6.293024063110352
Epoch 800, val loss: 0.9956037998199463
Epoch 810, training loss: 12.716419219970703 = 0.13471275568008423 + 2.0 * 6.290853023529053
Epoch 810, val loss: 1.0019105672836304
Epoch 820, training loss: 12.707422256469727 = 0.12783843278884888 + 2.0 * 6.289792060852051
Epoch 820, val loss: 1.0084571838378906
Epoch 830, training loss: 12.709420204162598 = 0.12138251215219498 + 2.0 * 6.294018745422363
Epoch 830, val loss: 1.0151946544647217
Epoch 840, training loss: 12.699100494384766 = 0.11534690856933594 + 2.0 * 6.291876792907715
Epoch 840, val loss: 1.022062063217163
Epoch 850, training loss: 12.683704376220703 = 0.10969110578298569 + 2.0 * 6.287006855010986
Epoch 850, val loss: 1.0291578769683838
Epoch 860, training loss: 12.676528930664062 = 0.10439035296440125 + 2.0 * 6.286069393157959
Epoch 860, val loss: 1.0364189147949219
Epoch 870, training loss: 12.67833137512207 = 0.09940988570451736 + 2.0 * 6.2894606590271
Epoch 870, val loss: 1.0437251329421997
Epoch 880, training loss: 12.67203140258789 = 0.09475219994783401 + 2.0 * 6.288639545440674
Epoch 880, val loss: 1.051304817199707
Epoch 890, training loss: 12.656970977783203 = 0.09039737284183502 + 2.0 * 6.2832865715026855
Epoch 890, val loss: 1.0590168237686157
Epoch 900, training loss: 12.650154113769531 = 0.08629438281059265 + 2.0 * 6.281929969787598
Epoch 900, val loss: 1.0667223930358887
Epoch 910, training loss: 12.645262718200684 = 0.08242086321115494 + 2.0 * 6.281420707702637
Epoch 910, val loss: 1.0744420289993286
Epoch 920, training loss: 12.659564971923828 = 0.07877036184072495 + 2.0 * 6.2903971672058105
Epoch 920, val loss: 1.0821266174316406
Epoch 930, training loss: 12.635826110839844 = 0.07536341995000839 + 2.0 * 6.280231475830078
Epoch 930, val loss: 1.0901484489440918
Epoch 940, training loss: 12.63664436340332 = 0.07215408235788345 + 2.0 * 6.28224515914917
Epoch 940, val loss: 1.0982273817062378
Epoch 950, training loss: 12.626117706298828 = 0.06912104040384293 + 2.0 * 6.27849817276001
Epoch 950, val loss: 1.1060322523117065
Epoch 960, training loss: 12.624823570251465 = 0.06626077741384506 + 2.0 * 6.2792816162109375
Epoch 960, val loss: 1.1139401197433472
Epoch 970, training loss: 12.617890357971191 = 0.06355989724397659 + 2.0 * 6.277165412902832
Epoch 970, val loss: 1.122130036354065
Epoch 980, training loss: 12.613966941833496 = 0.06099637597799301 + 2.0 * 6.276485443115234
Epoch 980, val loss: 1.1299777030944824
Epoch 990, training loss: 12.610539436340332 = 0.05857691541314125 + 2.0 * 6.275981426239014
Epoch 990, val loss: 1.1379350423812866
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7555555555555555
0.8007380073800738
=== training gcn model ===
Epoch 0, training loss: 19.136184692382812 = 1.9424859285354614 + 2.0 * 8.59684944152832
Epoch 0, val loss: 1.937393307685852
Epoch 10, training loss: 19.12541389465332 = 1.9326483011245728 + 2.0 * 8.596383094787598
Epoch 10, val loss: 1.9279530048370361
Epoch 20, training loss: 19.10468292236328 = 1.9202325344085693 + 2.0 * 8.592225074768066
Epoch 20, val loss: 1.9154380559921265
Epoch 30, training loss: 19.027099609375 = 1.9033992290496826 + 2.0 * 8.561850547790527
Epoch 30, val loss: 1.898092269897461
Epoch 40, training loss: 18.671241760253906 = 1.8828402757644653 + 2.0 * 8.394200325012207
Epoch 40, val loss: 1.8775508403778076
Epoch 50, training loss: 17.642427444458008 = 1.8602569103240967 + 2.0 * 7.891085147857666
Epoch 50, val loss: 1.8557952642440796
Epoch 60, training loss: 16.88714599609375 = 1.8437118530273438 + 2.0 * 7.521716594696045
Epoch 60, val loss: 1.841141939163208
Epoch 70, training loss: 16.207626342773438 = 1.8335853815078735 + 2.0 * 7.187020778656006
Epoch 70, val loss: 1.831985354423523
Epoch 80, training loss: 15.709766387939453 = 1.8238067626953125 + 2.0 * 6.94297981262207
Epoch 80, val loss: 1.8226938247680664
Epoch 90, training loss: 15.456268310546875 = 1.8121302127838135 + 2.0 * 6.82206916809082
Epoch 90, val loss: 1.8116754293441772
Epoch 100, training loss: 15.288594245910645 = 1.7990634441375732 + 2.0 * 6.744765281677246
Epoch 100, val loss: 1.800523042678833
Epoch 110, training loss: 15.160240173339844 = 1.7875971794128418 + 2.0 * 6.686321258544922
Epoch 110, val loss: 1.7911863327026367
Epoch 120, training loss: 15.04438304901123 = 1.7767432928085327 + 2.0 * 6.633820056915283
Epoch 120, val loss: 1.7823688983917236
Epoch 130, training loss: 14.947277069091797 = 1.765419602394104 + 2.0 * 6.590928554534912
Epoch 130, val loss: 1.7729578018188477
Epoch 140, training loss: 14.894073486328125 = 1.753067970275879 + 2.0 * 6.570502758026123
Epoch 140, val loss: 1.7628517150878906
Epoch 150, training loss: 14.812110900878906 = 1.7394728660583496 + 2.0 * 6.536318778991699
Epoch 150, val loss: 1.7517740726470947
Epoch 160, training loss: 14.745442390441895 = 1.7243881225585938 + 2.0 * 6.51052713394165
Epoch 160, val loss: 1.7395308017730713
Epoch 170, training loss: 14.688611030578613 = 1.7074525356292725 + 2.0 * 6.490579128265381
Epoch 170, val loss: 1.7256797552108765
Epoch 180, training loss: 14.63569450378418 = 1.6882151365280151 + 2.0 * 6.4737396240234375
Epoch 180, val loss: 1.70992910861969
Epoch 190, training loss: 14.606643676757812 = 1.666359782218933 + 2.0 * 6.470141887664795
Epoch 190, val loss: 1.6920740604400635
Epoch 200, training loss: 14.539167404174805 = 1.6419100761413574 + 2.0 * 6.4486284255981445
Epoch 200, val loss: 1.6724025011062622
Epoch 210, training loss: 14.48997688293457 = 1.6150497198104858 + 2.0 * 6.437463760375977
Epoch 210, val loss: 1.6507775783538818
Epoch 220, training loss: 14.440339088439941 = 1.5854027271270752 + 2.0 * 6.427468299865723
Epoch 220, val loss: 1.627166748046875
Epoch 230, training loss: 14.391672134399414 = 1.552807331085205 + 2.0 * 6.419432640075684
Epoch 230, val loss: 1.6014288663864136
Epoch 240, training loss: 14.346126556396484 = 1.5178800821304321 + 2.0 * 6.414123058319092
Epoch 240, val loss: 1.5745702981948853
Epoch 250, training loss: 14.288166999816895 = 1.4813743829727173 + 2.0 * 6.403396129608154
Epoch 250, val loss: 1.5469179153442383
Epoch 260, training loss: 14.23554515838623 = 1.443350076675415 + 2.0 * 6.396097660064697
Epoch 260, val loss: 1.5190179347991943
Epoch 270, training loss: 14.194596290588379 = 1.4042432308197021 + 2.0 * 6.395176410675049
Epoch 270, val loss: 1.4912512302398682
Epoch 280, training loss: 14.137323379516602 = 1.3653004169464111 + 2.0 * 6.386011600494385
Epoch 280, val loss: 1.46467924118042
Epoch 290, training loss: 14.087228775024414 = 1.3267278671264648 + 2.0 * 6.380250453948975
Epoch 290, val loss: 1.4394160509109497
Epoch 300, training loss: 14.03698444366455 = 1.2886265516281128 + 2.0 * 6.374178886413574
Epoch 300, val loss: 1.4155080318450928
Epoch 310, training loss: 13.99914264678955 = 1.251217007637024 + 2.0 * 6.373962879180908
Epoch 310, val loss: 1.3930412530899048
Epoch 320, training loss: 13.951967239379883 = 1.2151312828063965 + 2.0 * 6.368417739868164
Epoch 320, val loss: 1.3720396757125854
Epoch 330, training loss: 13.905061721801758 = 1.1799880266189575 + 2.0 * 6.362536907196045
Epoch 330, val loss: 1.3521896600723267
Epoch 340, training loss: 13.861934661865234 = 1.1455445289611816 + 2.0 * 6.358194828033447
Epoch 340, val loss: 1.3333344459533691
Epoch 350, training loss: 13.827808380126953 = 1.111753225326538 + 2.0 * 6.358027458190918
Epoch 350, val loss: 1.315234661102295
Epoch 360, training loss: 13.783791542053223 = 1.0788639783859253 + 2.0 * 6.352463722229004
Epoch 360, val loss: 1.2977131605148315
Epoch 370, training loss: 13.74341869354248 = 1.046600580215454 + 2.0 * 6.348409175872803
Epoch 370, val loss: 1.280772089958191
Epoch 380, training loss: 13.71191120147705 = 1.014855980873108 + 2.0 * 6.348527431488037
Epoch 380, val loss: 1.2643121480941772
Epoch 390, training loss: 13.6697359085083 = 0.9836601614952087 + 2.0 * 6.343038082122803
Epoch 390, val loss: 1.2481818199157715
Epoch 400, training loss: 13.632526397705078 = 0.9530779719352722 + 2.0 * 6.339724063873291
Epoch 400, val loss: 1.2324341535568237
Epoch 410, training loss: 13.59985637664795 = 0.9229099154472351 + 2.0 * 6.338473320007324
Epoch 410, val loss: 1.2171168327331543
Epoch 420, training loss: 13.574079513549805 = 0.893402636051178 + 2.0 * 6.340338230133057
Epoch 420, val loss: 1.2021411657333374
Epoch 430, training loss: 13.533227920532227 = 0.8646878004074097 + 2.0 * 6.334270000457764
Epoch 430, val loss: 1.1875462532043457
Epoch 440, training loss: 13.49553108215332 = 0.8365819454193115 + 2.0 * 6.329474449157715
Epoch 440, val loss: 1.1735368967056274
Epoch 450, training loss: 13.461958885192871 = 0.8089426755905151 + 2.0 * 6.326508045196533
Epoch 450, val loss: 1.1598716974258423
Epoch 460, training loss: 13.451396942138672 = 0.7819477319717407 + 2.0 * 6.334724426269531
Epoch 460, val loss: 1.1466151475906372
Epoch 470, training loss: 13.405021667480469 = 0.755814254283905 + 2.0 * 6.32460355758667
Epoch 470, val loss: 1.1341408491134644
Epoch 480, training loss: 13.371673583984375 = 0.7304114699363708 + 2.0 * 6.32063102722168
Epoch 480, val loss: 1.1225144863128662
Epoch 490, training loss: 13.35836410522461 = 0.7057179808616638 + 2.0 * 6.32632303237915
Epoch 490, val loss: 1.1115813255310059
Epoch 500, training loss: 13.316606521606445 = 0.6816855669021606 + 2.0 * 6.317460536956787
Epoch 500, val loss: 1.1014455556869507
Epoch 510, training loss: 13.287294387817383 = 0.6583512425422668 + 2.0 * 6.31447172164917
Epoch 510, val loss: 1.0922143459320068
Epoch 520, training loss: 13.265997886657715 = 0.6354849934577942 + 2.0 * 6.315256595611572
Epoch 520, val loss: 1.0836498737335205
Epoch 530, training loss: 13.248112678527832 = 0.6132418513298035 + 2.0 * 6.317435264587402
Epoch 530, val loss: 1.076101303100586
Epoch 540, training loss: 13.212108612060547 = 0.5915437936782837 + 2.0 * 6.310282230377197
Epoch 540, val loss: 1.0692015886306763
Epoch 550, training loss: 13.185194969177246 = 0.5703462362289429 + 2.0 * 6.307424545288086
Epoch 550, val loss: 1.0633116960525513
Epoch 560, training loss: 13.162637710571289 = 0.5495526790618896 + 2.0 * 6.30654239654541
Epoch 560, val loss: 1.058214783668518
Epoch 570, training loss: 13.153487205505371 = 0.5292521715164185 + 2.0 * 6.312117576599121
Epoch 570, val loss: 1.0541685819625854
Epoch 580, training loss: 13.123858451843262 = 0.5096144080162048 + 2.0 * 6.307122230529785
Epoch 580, val loss: 1.0509546995162964
Epoch 590, training loss: 13.096878051757812 = 0.4905841052532196 + 2.0 * 6.303146839141846
Epoch 590, val loss: 1.049047827720642
Epoch 600, training loss: 13.074009895324707 = 0.4720971882343292 + 2.0 * 6.3009562492370605
Epoch 600, val loss: 1.0482386350631714
Epoch 610, training loss: 13.056381225585938 = 0.45413243770599365 + 2.0 * 6.301124572753906
Epoch 610, val loss: 1.0486420392990112
Epoch 620, training loss: 13.035205841064453 = 0.43680956959724426 + 2.0 * 6.299198150634766
Epoch 620, val loss: 1.0500693321228027
Epoch 630, training loss: 13.014111518859863 = 0.42017287015914917 + 2.0 * 6.296969413757324
Epoch 630, val loss: 1.0526918172836304
Epoch 640, training loss: 12.994918823242188 = 0.4041391909122467 + 2.0 * 6.295389652252197
Epoch 640, val loss: 1.0564240217208862
Epoch 650, training loss: 12.99413776397705 = 0.3886549472808838 + 2.0 * 6.302741527557373
Epoch 650, val loss: 1.0611894130706787
Epoch 660, training loss: 12.97433853149414 = 0.3738768398761749 + 2.0 * 6.300230979919434
Epoch 660, val loss: 1.066928744316101
Epoch 670, training loss: 12.945060729980469 = 0.359694242477417 + 2.0 * 6.292683124542236
Epoch 670, val loss: 1.0733698606491089
Epoch 680, training loss: 12.927803039550781 = 0.34603872895240784 + 2.0 * 6.290882110595703
Epoch 680, val loss: 1.0806939601898193
Epoch 690, training loss: 12.910964012145996 = 0.3328447639942169 + 2.0 * 6.289059638977051
Epoch 690, val loss: 1.088774561882019
Epoch 700, training loss: 12.900016784667969 = 0.3200506269931793 + 2.0 * 6.28998327255249
Epoch 700, val loss: 1.0974363088607788
Epoch 710, training loss: 12.88492202758789 = 0.30770012736320496 + 2.0 * 6.288610935211182
Epoch 710, val loss: 1.1064872741699219
Epoch 720, training loss: 12.870555877685547 = 0.2957858443260193 + 2.0 * 6.287384986877441
Epoch 720, val loss: 1.116065263748169
Epoch 730, training loss: 12.865389823913574 = 0.2842501103878021 + 2.0 * 6.29056978225708
Epoch 730, val loss: 1.1260896921157837
Epoch 740, training loss: 12.841959953308105 = 0.2731074392795563 + 2.0 * 6.284426212310791
Epoch 740, val loss: 1.1363996267318726
Epoch 750, training loss: 12.829569816589355 = 0.26230964064598083 + 2.0 * 6.283629894256592
Epoch 750, val loss: 1.1470757722854614
Epoch 760, training loss: 12.817999839782715 = 0.25181445479393005 + 2.0 * 6.283092498779297
Epoch 760, val loss: 1.1579617261886597
Epoch 770, training loss: 12.808442115783691 = 0.24166396260261536 + 2.0 * 6.283389091491699
Epoch 770, val loss: 1.169137716293335
Epoch 780, training loss: 12.79465389251709 = 0.23184269666671753 + 2.0 * 6.281405448913574
Epoch 780, val loss: 1.1804295778274536
Epoch 790, training loss: 12.781787872314453 = 0.22236500680446625 + 2.0 * 6.2797112464904785
Epoch 790, val loss: 1.1919050216674805
Epoch 800, training loss: 12.77175521850586 = 0.21325065195560455 + 2.0 * 6.279252052307129
Epoch 800, val loss: 1.2035880088806152
Epoch 810, training loss: 12.77408218383789 = 0.2044437676668167 + 2.0 * 6.28481912612915
Epoch 810, val loss: 1.2153409719467163
Epoch 820, training loss: 12.752612113952637 = 0.1960352212190628 + 2.0 * 6.2782883644104
Epoch 820, val loss: 1.2272684574127197
Epoch 830, training loss: 12.739065170288086 = 0.18794351816177368 + 2.0 * 6.2755608558654785
Epoch 830, val loss: 1.2393134832382202
Epoch 840, training loss: 12.734406471252441 = 0.1801648586988449 + 2.0 * 6.277120590209961
Epoch 840, val loss: 1.2514234781265259
Epoch 850, training loss: 12.724845886230469 = 0.1727156639099121 + 2.0 * 6.276065349578857
Epoch 850, val loss: 1.263600468635559
Epoch 860, training loss: 12.712647438049316 = 0.1655835062265396 + 2.0 * 6.273531913757324
Epoch 860, val loss: 1.275739312171936
Epoch 870, training loss: 12.703869819641113 = 0.15875838696956635 + 2.0 * 6.272555828094482
Epoch 870, val loss: 1.2879834175109863
Epoch 880, training loss: 12.710010528564453 = 0.15221300721168518 + 2.0 * 6.2788987159729
Epoch 880, val loss: 1.3002548217773438
Epoch 890, training loss: 12.693652153015137 = 0.14598096907138824 + 2.0 * 6.2738356590271
Epoch 890, val loss: 1.312430500984192
Epoch 900, training loss: 12.687889099121094 = 0.14002245664596558 + 2.0 * 6.273933410644531
Epoch 900, val loss: 1.324663519859314
Epoch 910, training loss: 12.674042701721191 = 0.13436590135097504 + 2.0 * 6.269838333129883
Epoch 910, val loss: 1.3369394540786743
Epoch 920, training loss: 12.66552734375 = 0.12894316017627716 + 2.0 * 6.26829195022583
Epoch 920, val loss: 1.3492263555526733
Epoch 930, training loss: 12.669631004333496 = 0.12377116084098816 + 2.0 * 6.272930145263672
Epoch 930, val loss: 1.3614404201507568
Epoch 940, training loss: 12.65436840057373 = 0.11882604658603668 + 2.0 * 6.267771244049072
Epoch 940, val loss: 1.3735074996948242
Epoch 950, training loss: 12.647846221923828 = 0.1141330897808075 + 2.0 * 6.266856670379639
Epoch 950, val loss: 1.385654091835022
Epoch 960, training loss: 12.640920639038086 = 0.10965930670499802 + 2.0 * 6.265630722045898
Epoch 960, val loss: 1.3977587223052979
Epoch 970, training loss: 12.661630630493164 = 0.10540276765823364 + 2.0 * 6.278113842010498
Epoch 970, val loss: 1.4098429679870605
Epoch 980, training loss: 12.629281997680664 = 0.10133567452430725 + 2.0 * 6.263973236083984
Epoch 980, val loss: 1.4216821193695068
Epoch 990, training loss: 12.626020431518555 = 0.09748823195695877 + 2.0 * 6.264266014099121
Epoch 990, val loss: 1.4334694147109985
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7518518518518519
0.8165524512387982
=== training gcn model ===
Epoch 0, training loss: 19.135536193847656 = 1.9418283700942993 + 2.0 * 8.596854209899902
Epoch 0, val loss: 1.9420170783996582
Epoch 10, training loss: 19.125242233276367 = 1.93221914768219 + 2.0 * 8.596511840820312
Epoch 10, val loss: 1.9319992065429688
Epoch 20, training loss: 19.10718536376953 = 1.9200795888900757 + 2.0 * 8.593552589416504
Epoch 20, val loss: 1.919402837753296
Epoch 30, training loss: 19.03958511352539 = 1.9033430814743042 + 2.0 * 8.568120956420898
Epoch 30, val loss: 1.9020236730575562
Epoch 40, training loss: 18.678707122802734 = 1.8826158046722412 + 2.0 * 8.398045539855957
Epoch 40, val loss: 1.8812718391418457
Epoch 50, training loss: 17.36765480041504 = 1.8620904684066772 + 2.0 * 7.752782344818115
Epoch 50, val loss: 1.8614169359207153
Epoch 60, training loss: 16.523542404174805 = 1.8460239171981812 + 2.0 * 7.338758945465088
Epoch 60, val loss: 1.8471882343292236
Epoch 70, training loss: 15.9215726852417 = 1.8338948488235474 + 2.0 * 7.043838977813721
Epoch 70, val loss: 1.8356099128723145
Epoch 80, training loss: 15.601600646972656 = 1.8220998048782349 + 2.0 * 6.8897504806518555
Epoch 80, val loss: 1.8244647979736328
Epoch 90, training loss: 15.426653861999512 = 1.8095901012420654 + 2.0 * 6.808531761169434
Epoch 90, val loss: 1.8126115798950195
Epoch 100, training loss: 15.258979797363281 = 1.7978262901306152 + 2.0 * 6.730576992034912
Epoch 100, val loss: 1.8019092082977295
Epoch 110, training loss: 15.137120246887207 = 1.787297248840332 + 2.0 * 6.6749114990234375
Epoch 110, val loss: 1.7925344705581665
Epoch 120, training loss: 15.047144889831543 = 1.7768338918685913 + 2.0 * 6.63515567779541
Epoch 120, val loss: 1.7832087278366089
Epoch 130, training loss: 14.960930824279785 = 1.7660084962844849 + 2.0 * 6.597461223602295
Epoch 130, val loss: 1.7735155820846558
Epoch 140, training loss: 14.895879745483398 = 1.7549057006835938 + 2.0 * 6.570487022399902
Epoch 140, val loss: 1.7637399435043335
Epoch 150, training loss: 14.811071395874023 = 1.7429753541946411 + 2.0 * 6.534048080444336
Epoch 150, val loss: 1.7533456087112427
Epoch 160, training loss: 14.744680404663086 = 1.7298426628112793 + 2.0 * 6.507418632507324
Epoch 160, val loss: 1.7419471740722656
Epoch 170, training loss: 14.688841819763184 = 1.7150678634643555 + 2.0 * 6.486886978149414
Epoch 170, val loss: 1.7292957305908203
Epoch 180, training loss: 14.645724296569824 = 1.698311448097229 + 2.0 * 6.473706245422363
Epoch 180, val loss: 1.715075135231018
Epoch 190, training loss: 14.597220420837402 = 1.6796315908432007 + 2.0 * 6.458794593811035
Epoch 190, val loss: 1.699391484260559
Epoch 200, training loss: 14.551825523376465 = 1.659030795097351 + 2.0 * 6.446397304534912
Epoch 200, val loss: 1.6822367906570435
Epoch 210, training loss: 14.506352424621582 = 1.6362097263336182 + 2.0 * 6.4350714683532715
Epoch 210, val loss: 1.6634668111801147
Epoch 220, training loss: 14.461251258850098 = 1.6110609769821167 + 2.0 * 6.425095081329346
Epoch 220, val loss: 1.6428967714309692
Epoch 230, training loss: 14.435761451721191 = 1.5836896896362305 + 2.0 * 6.4260358810424805
Epoch 230, val loss: 1.6205174922943115
Epoch 240, training loss: 14.37580394744873 = 1.554545521736145 + 2.0 * 6.4106292724609375
Epoch 240, val loss: 1.5971028804779053
Epoch 250, training loss: 14.327510833740234 = 1.523566722869873 + 2.0 * 6.40197229385376
Epoch 250, val loss: 1.5722148418426514
Epoch 260, training loss: 14.279584884643555 = 1.4906888008117676 + 2.0 * 6.394448280334473
Epoch 260, val loss: 1.5462158918380737
Epoch 270, training loss: 14.232988357543945 = 1.456040859222412 + 2.0 * 6.388473987579346
Epoch 270, val loss: 1.5190643072128296
Epoch 280, training loss: 14.195072174072266 = 1.420060634613037 + 2.0 * 6.387506008148193
Epoch 280, val loss: 1.491425633430481
Epoch 290, training loss: 14.143813133239746 = 1.383407711982727 + 2.0 * 6.380202770233154
Epoch 290, val loss: 1.463863492012024
Epoch 300, training loss: 14.093509674072266 = 1.3460917472839355 + 2.0 * 6.373708724975586
Epoch 300, val loss: 1.4365191459655762
Epoch 310, training loss: 14.052974700927734 = 1.308292031288147 + 2.0 * 6.372341156005859
Epoch 310, val loss: 1.409433126449585
Epoch 320, training loss: 14.00571060180664 = 1.270325779914856 + 2.0 * 6.367692470550537
Epoch 320, val loss: 1.382842779159546
Epoch 330, training loss: 13.958589553833008 = 1.232280969619751 + 2.0 * 6.363154411315918
Epoch 330, val loss: 1.3568732738494873
Epoch 340, training loss: 13.912623405456543 = 1.193914532661438 + 2.0 * 6.359354496002197
Epoch 340, val loss: 1.3313876390457153
Epoch 350, training loss: 13.877141952514648 = 1.1553466320037842 + 2.0 * 6.360897541046143
Epoch 350, val loss: 1.3063011169433594
Epoch 360, training loss: 13.826268196105957 = 1.116887092590332 + 2.0 * 6.3546905517578125
Epoch 360, val loss: 1.2817500829696655
Epoch 370, training loss: 13.78028678894043 = 1.0787575244903564 + 2.0 * 6.350764751434326
Epoch 370, val loss: 1.2577338218688965
Epoch 380, training loss: 13.734617233276367 = 1.0405337810516357 + 2.0 * 6.347041606903076
Epoch 380, val loss: 1.2340949773788452
Epoch 390, training loss: 13.690694808959961 = 1.0023213624954224 + 2.0 * 6.344186782836914
Epoch 390, val loss: 1.2107055187225342
Epoch 400, training loss: 13.654339790344238 = 0.9643093347549438 + 2.0 * 6.345015048980713
Epoch 400, val loss: 1.1877626180648804
Epoch 410, training loss: 13.610562324523926 = 0.9270336627960205 + 2.0 * 6.341764450073242
Epoch 410, val loss: 1.1657441854476929
Epoch 420, training loss: 13.565201759338379 = 0.8906726241111755 + 2.0 * 6.337264537811279
Epoch 420, val loss: 1.1444648504257202
Epoch 430, training loss: 13.524259567260742 = 0.8551651239395142 + 2.0 * 6.33454704284668
Epoch 430, val loss: 1.1240147352218628
Epoch 440, training loss: 13.49413776397705 = 0.8206430077552795 + 2.0 * 6.336747169494629
Epoch 440, val loss: 1.1045384407043457
Epoch 450, training loss: 13.460466384887695 = 0.7872596979141235 + 2.0 * 6.336603164672852
Epoch 450, val loss: 1.086254596710205
Epoch 460, training loss: 13.41158390045166 = 0.7555245757102966 + 2.0 * 6.328029632568359
Epoch 460, val loss: 1.069284200668335
Epoch 470, training loss: 13.37575912475586 = 0.7251163721084595 + 2.0 * 6.325321197509766
Epoch 470, val loss: 1.0535237789154053
Epoch 480, training loss: 13.342877388000488 = 0.6959276795387268 + 2.0 * 6.323474884033203
Epoch 480, val loss: 1.0389372110366821
Epoch 490, training loss: 13.317182540893555 = 0.667969822883606 + 2.0 * 6.324606418609619
Epoch 490, val loss: 1.0256539583206177
Epoch 500, training loss: 13.294855117797852 = 0.6413963437080383 + 2.0 * 6.3267292976379395
Epoch 500, val loss: 1.0136265754699707
Epoch 510, training loss: 13.255460739135742 = 0.6162741780281067 + 2.0 * 6.31959342956543
Epoch 510, val loss: 1.0026246309280396
Epoch 520, training loss: 13.225170135498047 = 0.5921968817710876 + 2.0 * 6.316486835479736
Epoch 520, val loss: 0.9929106831550598
Epoch 530, training loss: 13.196686744689941 = 0.5691133737564087 + 2.0 * 6.313786506652832
Epoch 530, val loss: 0.9840500354766846
Epoch 540, training loss: 13.193318367004395 = 0.546839714050293 + 2.0 * 6.323239326477051
Epoch 540, val loss: 0.9762019515037537
Epoch 550, training loss: 13.151220321655273 = 0.5256958603858948 + 2.0 * 6.312762260437012
Epoch 550, val loss: 0.969154417514801
Epoch 560, training loss: 13.126376152038574 = 0.505447506904602 + 2.0 * 6.310464382171631
Epoch 560, val loss: 0.9630144834518433
Epoch 570, training loss: 13.102518081665039 = 0.4859049320220947 + 2.0 * 6.308306694030762
Epoch 570, val loss: 0.9577339291572571
Epoch 580, training loss: 13.079383850097656 = 0.4669457972049713 + 2.0 * 6.306219100952148
Epoch 580, val loss: 0.9530860185623169
Epoch 590, training loss: 13.059515953063965 = 0.44851312041282654 + 2.0 * 6.305501461029053
Epoch 590, val loss: 0.9490895867347717
Epoch 600, training loss: 13.047277450561523 = 0.4306241273880005 + 2.0 * 6.308326721191406
Epoch 600, val loss: 0.9458708167076111
Epoch 610, training loss: 13.023187637329102 = 0.41343748569488525 + 2.0 * 6.304874897003174
Epoch 610, val loss: 0.942934513092041
Epoch 620, training loss: 12.999882698059082 = 0.3967854976654053 + 2.0 * 6.301548480987549
Epoch 620, val loss: 0.9407404065132141
Epoch 630, training loss: 12.980069160461426 = 0.38053470849990845 + 2.0 * 6.299767017364502
Epoch 630, val loss: 0.9391213655471802
Epoch 640, training loss: 12.981287002563477 = 0.36473217606544495 + 2.0 * 6.308277606964111
Epoch 640, val loss: 0.93801349401474
Epoch 650, training loss: 12.945011138916016 = 0.3493410348892212 + 2.0 * 6.297834873199463
Epoch 650, val loss: 0.9374094009399414
Epoch 660, training loss: 12.928556442260742 = 0.3344764709472656 + 2.0 * 6.297039985656738
Epoch 660, val loss: 0.9372830390930176
Epoch 670, training loss: 12.915444374084473 = 0.3200727701187134 + 2.0 * 6.297685623168945
Epoch 670, val loss: 0.9376360177993774
Epoch 680, training loss: 12.892441749572754 = 0.30612221360206604 + 2.0 * 6.2931599617004395
Epoch 680, val loss: 0.9384632110595703
Epoch 690, training loss: 12.87997817993164 = 0.2926531434059143 + 2.0 * 6.2936625480651855
Epoch 690, val loss: 0.9397028088569641
Epoch 700, training loss: 12.864577293395996 = 0.2796669602394104 + 2.0 * 6.292455196380615
Epoch 700, val loss: 0.9413529634475708
Epoch 710, training loss: 12.854869842529297 = 0.2672103941440582 + 2.0 * 6.293829917907715
Epoch 710, val loss: 0.9433782696723938
Epoch 720, training loss: 12.83761215209961 = 0.25536468625068665 + 2.0 * 6.291123867034912
Epoch 720, val loss: 0.9457774758338928
Epoch 730, training loss: 12.819453239440918 = 0.2440260797739029 + 2.0 * 6.287713527679443
Epoch 730, val loss: 0.9486191272735596
Epoch 740, training loss: 12.81583023071289 = 0.23319920897483826 + 2.0 * 6.29131555557251
Epoch 740, val loss: 0.9518114924430847
Epoch 750, training loss: 12.801817893981934 = 0.22285059094429016 + 2.0 * 6.289483547210693
Epoch 750, val loss: 0.9555081725120544
Epoch 760, training loss: 12.784180641174316 = 0.2130935937166214 + 2.0 * 6.285543441772461
Epoch 760, val loss: 0.9592753052711487
Epoch 770, training loss: 12.773460388183594 = 0.20378893613815308 + 2.0 * 6.2848358154296875
Epoch 770, val loss: 0.9635336399078369
Epoch 780, training loss: 12.7702054977417 = 0.19491490721702576 + 2.0 * 6.28764533996582
Epoch 780, val loss: 0.9681129455566406
Epoch 790, training loss: 12.757487297058105 = 0.18652674555778503 + 2.0 * 6.285480499267578
Epoch 790, val loss: 0.9728608131408691
Epoch 800, training loss: 12.743313789367676 = 0.17855004966259003 + 2.0 * 6.282382011413574
Epoch 800, val loss: 0.9778426289558411
Epoch 810, training loss: 12.737091064453125 = 0.17097671329975128 + 2.0 * 6.28305721282959
Epoch 810, val loss: 0.9831071496009827
Epoch 820, training loss: 12.72421646118164 = 0.16378526389598846 + 2.0 * 6.280215740203857
Epoch 820, val loss: 0.9885975122451782
Epoch 830, training loss: 12.718779563903809 = 0.15695808827877045 + 2.0 * 6.280910968780518
Epoch 830, val loss: 0.9942530989646912
Epoch 840, training loss: 12.713242530822754 = 0.15046021342277527 + 2.0 * 6.281391143798828
Epoch 840, val loss: 1.0000708103179932
Epoch 850, training loss: 12.699909210205078 = 0.1443178653717041 + 2.0 * 6.277795791625977
Epoch 850, val loss: 1.0058852434158325
Epoch 860, training loss: 12.693832397460938 = 0.1384815126657486 + 2.0 * 6.277675628662109
Epoch 860, val loss: 1.0118343830108643
Epoch 870, training loss: 12.685188293457031 = 0.1329118013381958 + 2.0 * 6.2761383056640625
Epoch 870, val loss: 1.017961025238037
Epoch 880, training loss: 12.678969383239746 = 0.12760016322135925 + 2.0 * 6.275684833526611
Epoch 880, val loss: 1.024217128753662
Epoch 890, training loss: 12.684283256530762 = 0.12253736704587936 + 2.0 * 6.280872821807861
Epoch 890, val loss: 1.0305180549621582
Epoch 900, training loss: 12.666258811950684 = 0.117745541036129 + 2.0 * 6.274256706237793
Epoch 900, val loss: 1.0367025136947632
Epoch 910, training loss: 12.6598539352417 = 0.11318663507699966 + 2.0 * 6.273333549499512
Epoch 910, val loss: 1.0429537296295166
Epoch 920, training loss: 12.656065940856934 = 0.10882743448019028 + 2.0 * 6.273619174957275
Epoch 920, val loss: 1.0493155717849731
Epoch 930, training loss: 12.65281867980957 = 0.1046653538942337 + 2.0 * 6.274076461791992
Epoch 930, val loss: 1.0557576417922974
Epoch 940, training loss: 12.648317337036133 = 0.10072728246450424 + 2.0 * 6.273795127868652
Epoch 940, val loss: 1.061974048614502
Epoch 950, training loss: 12.636570930480957 = 0.09696023166179657 + 2.0 * 6.269805431365967
Epoch 950, val loss: 1.0683324337005615
Epoch 960, training loss: 12.631424903869629 = 0.09335535019636154 + 2.0 * 6.2690348625183105
Epoch 960, val loss: 1.0748027563095093
Epoch 970, training loss: 12.627656936645508 = 0.08990519493818283 + 2.0 * 6.268876075744629
Epoch 970, val loss: 1.0812348127365112
Epoch 980, training loss: 12.631061553955078 = 0.08660125732421875 + 2.0 * 6.27223014831543
Epoch 980, val loss: 1.087605357170105
Epoch 990, training loss: 12.617740631103516 = 0.08345630019903183 + 2.0 * 6.267142295837402
Epoch 990, val loss: 1.0939146280288696
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7740740740740741
0.8065366367949395
The final CL Acc:0.76049, 0.00972, The final GNN Acc:0.80794, 0.00653
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13278])
remove edge: torch.Size([2, 8008])
updated graph: torch.Size([2, 10730])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 19.141395568847656 = 1.9476889371871948 + 2.0 * 8.596853256225586
Epoch 0, val loss: 1.9411250352859497
Epoch 10, training loss: 19.130226135253906 = 1.9371585845947266 + 2.0 * 8.59653377532959
Epoch 10, val loss: 1.931112289428711
Epoch 20, training loss: 19.111156463623047 = 1.9236249923706055 + 2.0 * 8.593765258789062
Epoch 20, val loss: 1.9179117679595947
Epoch 30, training loss: 19.045257568359375 = 1.904732584953308 + 2.0 * 8.570262908935547
Epoch 30, val loss: 1.8995361328125
Epoch 40, training loss: 18.70871925354004 = 1.8807414770126343 + 2.0 * 8.413989067077637
Epoch 40, val loss: 1.8774299621582031
Epoch 50, training loss: 17.47373390197754 = 1.8558285236358643 + 2.0 * 7.808952808380127
Epoch 50, val loss: 1.855590581893921
Epoch 60, training loss: 16.675216674804688 = 1.8366471529006958 + 2.0 * 7.419284343719482
Epoch 60, val loss: 1.8380639553070068
Epoch 70, training loss: 16.02488136291504 = 1.821526050567627 + 2.0 * 7.101677894592285
Epoch 70, val loss: 1.8233659267425537
Epoch 80, training loss: 15.7416410446167 = 1.8070697784423828 + 2.0 * 6.967285633087158
Epoch 80, val loss: 1.8091516494750977
Epoch 90, training loss: 15.509795188903809 = 1.7906486988067627 + 2.0 * 6.8595733642578125
Epoch 90, val loss: 1.7939743995666504
Epoch 100, training loss: 15.351075172424316 = 1.7758986949920654 + 2.0 * 6.787588119506836
Epoch 100, val loss: 1.7811263799667358
Epoch 110, training loss: 15.196659088134766 = 1.7624423503875732 + 2.0 * 6.717108249664307
Epoch 110, val loss: 1.7694363594055176
Epoch 120, training loss: 15.087045669555664 = 1.748924732208252 + 2.0 * 6.669060707092285
Epoch 120, val loss: 1.757330298423767
Epoch 130, training loss: 14.979700088500977 = 1.7342699766159058 + 2.0 * 6.622714996337891
Epoch 130, val loss: 1.7442677021026611
Epoch 140, training loss: 14.886713027954102 = 1.7176460027694702 + 2.0 * 6.58453369140625
Epoch 140, val loss: 1.729912281036377
Epoch 150, training loss: 14.802968978881836 = 1.6989096403121948 + 2.0 * 6.552029609680176
Epoch 150, val loss: 1.7140923738479614
Epoch 160, training loss: 14.722412109375 = 1.6778197288513184 + 2.0 * 6.52229642868042
Epoch 160, val loss: 1.6961780786514282
Epoch 170, training loss: 14.652130126953125 = 1.6537065505981445 + 2.0 * 6.49921178817749
Epoch 170, val loss: 1.675598382949829
Epoch 180, training loss: 14.584721565246582 = 1.6262433528900146 + 2.0 * 6.479238986968994
Epoch 180, val loss: 1.6522947549819946
Epoch 190, training loss: 14.517059326171875 = 1.5953646898269653 + 2.0 * 6.4608473777771
Epoch 190, val loss: 1.6261674165725708
Epoch 200, training loss: 14.460999488830566 = 1.560935139656067 + 2.0 * 6.4500322341918945
Epoch 200, val loss: 1.5972529649734497
Epoch 210, training loss: 14.387834548950195 = 1.523451566696167 + 2.0 * 6.432191371917725
Epoch 210, val loss: 1.5661286115646362
Epoch 220, training loss: 14.324197769165039 = 1.4829827547073364 + 2.0 * 6.420607566833496
Epoch 220, val loss: 1.532620906829834
Epoch 230, training loss: 14.260626792907715 = 1.4395818710327148 + 2.0 * 6.4105224609375
Epoch 230, val loss: 1.497068166732788
Epoch 240, training loss: 14.199424743652344 = 1.394018530845642 + 2.0 * 6.402703285217285
Epoch 240, val loss: 1.4600746631622314
Epoch 250, training loss: 14.139286041259766 = 1.3477191925048828 + 2.0 * 6.395783424377441
Epoch 250, val loss: 1.422834873199463
Epoch 260, training loss: 14.075235366821289 = 1.3012686967849731 + 2.0 * 6.386983394622803
Epoch 260, val loss: 1.385977864265442
Epoch 270, training loss: 14.031561851501465 = 1.2552299499511719 + 2.0 * 6.3881659507751465
Epoch 270, val loss: 1.3498891592025757
Epoch 280, training loss: 13.962804794311523 = 1.2113655805587769 + 2.0 * 6.3757195472717285
Epoch 280, val loss: 1.3159551620483398
Epoch 290, training loss: 13.906521797180176 = 1.169703483581543 + 2.0 * 6.368409156799316
Epoch 290, val loss: 1.2841510772705078
Epoch 300, training loss: 13.855384826660156 = 1.1301473379135132 + 2.0 * 6.362618923187256
Epoch 300, val loss: 1.2545300722122192
Epoch 310, training loss: 13.806894302368164 = 1.092745065689087 + 2.0 * 6.357074737548828
Epoch 310, val loss: 1.2268842458724976
Epoch 320, training loss: 13.775760650634766 = 1.0575016736984253 + 2.0 * 6.359129428863525
Epoch 320, val loss: 1.2012380361557007
Epoch 330, training loss: 13.722289085388184 = 1.0246597528457642 + 2.0 * 6.348814487457275
Epoch 330, val loss: 1.1775091886520386
Epoch 340, training loss: 13.696322441101074 = 0.9936409592628479 + 2.0 * 6.3513407707214355
Epoch 340, val loss: 1.1553593873977661
Epoch 350, training loss: 13.645856857299805 = 0.9643694758415222 + 2.0 * 6.340743541717529
Epoch 350, val loss: 1.134445071220398
Epoch 360, training loss: 13.609983444213867 = 0.936112105846405 + 2.0 * 6.336935520172119
Epoch 360, val loss: 1.1143319606781006
Epoch 370, training loss: 13.574813842773438 = 0.9083837866783142 + 2.0 * 6.333215236663818
Epoch 370, val loss: 1.0945271253585815
Epoch 380, training loss: 13.543862342834473 = 0.8808178305625916 + 2.0 * 6.331522464752197
Epoch 380, val loss: 1.07478666305542
Epoch 390, training loss: 13.519783020019531 = 0.853441596031189 + 2.0 * 6.3331708908081055
Epoch 390, val loss: 1.0549542903900146
Epoch 400, training loss: 13.479676246643066 = 0.8261443376541138 + 2.0 * 6.326766014099121
Epoch 400, val loss: 1.0351336002349854
Epoch 410, training loss: 13.444258689880371 = 0.7986950278282166 + 2.0 * 6.322782039642334
Epoch 410, val loss: 1.0152746438980103
Epoch 420, training loss: 13.411260604858398 = 0.7709069848060608 + 2.0 * 6.320176601409912
Epoch 420, val loss: 0.9952524900436401
Epoch 430, training loss: 13.381487846374512 = 0.7427828311920166 + 2.0 * 6.319352626800537
Epoch 430, val loss: 0.9750872850418091
Epoch 440, training loss: 13.353267669677734 = 0.7146540880203247 + 2.0 * 6.31930685043335
Epoch 440, val loss: 0.955105721950531
Epoch 450, training loss: 13.315774917602539 = 0.686732292175293 + 2.0 * 6.314521312713623
Epoch 450, val loss: 0.9354515075683594
Epoch 460, training loss: 13.282565116882324 = 0.6590613126754761 + 2.0 * 6.311751842498779
Epoch 460, val loss: 0.9162939190864563
Epoch 470, training loss: 13.252825736999512 = 0.6316947937011719 + 2.0 * 6.31056547164917
Epoch 470, val loss: 0.8976414203643799
Epoch 480, training loss: 13.232154846191406 = 0.6048760414123535 + 2.0 * 6.3136396408081055
Epoch 480, val loss: 0.8796486854553223
Epoch 490, training loss: 13.19428539276123 = 0.5790818929672241 + 2.0 * 6.3076019287109375
Epoch 490, val loss: 0.8627187013626099
Epoch 500, training loss: 13.163420677185059 = 0.55396568775177 + 2.0 * 6.304727554321289
Epoch 500, val loss: 0.8468588590621948
Epoch 510, training loss: 13.135307312011719 = 0.5296894907951355 + 2.0 * 6.30280876159668
Epoch 510, val loss: 0.8320469260215759
Epoch 520, training loss: 13.11780071258545 = 0.5061790347099304 + 2.0 * 6.305810928344727
Epoch 520, val loss: 0.8182473182678223
Epoch 530, training loss: 13.086357116699219 = 0.4834238290786743 + 2.0 * 6.301466464996338
Epoch 530, val loss: 0.8054289817810059
Epoch 540, training loss: 13.060948371887207 = 0.46165528893470764 + 2.0 * 6.299646377563477
Epoch 540, val loss: 0.7938517332077026
Epoch 550, training loss: 13.041836738586426 = 0.44061967730522156 + 2.0 * 6.3006086349487305
Epoch 550, val loss: 0.783193051815033
Epoch 560, training loss: 13.01150894165039 = 0.42045703530311584 + 2.0 * 6.295526027679443
Epoch 560, val loss: 0.773602306842804
Epoch 570, training loss: 12.989691734313965 = 0.4010346233844757 + 2.0 * 6.294328689575195
Epoch 570, val loss: 0.7650495171546936
Epoch 580, training loss: 12.977411270141602 = 0.38234949111938477 + 2.0 * 6.297530651092529
Epoch 580, val loss: 0.7573454976081848
Epoch 590, training loss: 12.948868751525879 = 0.3644811511039734 + 2.0 * 6.29219388961792
Epoch 590, val loss: 0.7504686117172241
Epoch 600, training loss: 12.927385330200195 = 0.3472704291343689 + 2.0 * 6.29005765914917
Epoch 600, val loss: 0.7442920804023743
Epoch 610, training loss: 12.907652854919434 = 0.3307546377182007 + 2.0 * 6.288449287414551
Epoch 610, val loss: 0.7388387322425842
Epoch 620, training loss: 12.900492668151855 = 0.3148839771747589 + 2.0 * 6.29280424118042
Epoch 620, val loss: 0.7340195178985596
Epoch 630, training loss: 12.881569862365723 = 0.2997569739818573 + 2.0 * 6.2909064292907715
Epoch 630, val loss: 0.7298445105552673
Epoch 640, training loss: 12.858330726623535 = 0.2852765619754791 + 2.0 * 6.286527156829834
Epoch 640, val loss: 0.7261650562286377
Epoch 650, training loss: 12.839272499084473 = 0.2715456783771515 + 2.0 * 6.283863544464111
Epoch 650, val loss: 0.7231659293174744
Epoch 660, training loss: 12.823265075683594 = 0.2583549916744232 + 2.0 * 6.282454967498779
Epoch 660, val loss: 0.720634400844574
Epoch 670, training loss: 12.815204620361328 = 0.24573259055614471 + 2.0 * 6.284736156463623
Epoch 670, val loss: 0.7185925245285034
Epoch 680, training loss: 12.799568176269531 = 0.23374131321907043 + 2.0 * 6.2829132080078125
Epoch 680, val loss: 0.7169718146324158
Epoch 690, training loss: 12.780006408691406 = 0.22223512828350067 + 2.0 * 6.278885841369629
Epoch 690, val loss: 0.7157211303710938
Epoch 700, training loss: 12.772940635681152 = 0.21126753091812134 + 2.0 * 6.280836582183838
Epoch 700, val loss: 0.7148951292037964
Epoch 710, training loss: 12.755681991577148 = 0.20086729526519775 + 2.0 * 6.277407169342041
Epoch 710, val loss: 0.714526891708374
Epoch 720, training loss: 12.74556827545166 = 0.19095639884471893 + 2.0 * 6.277306079864502
Epoch 720, val loss: 0.7144868969917297
Epoch 730, training loss: 12.73536491394043 = 0.18155743181705475 + 2.0 * 6.2769036293029785
Epoch 730, val loss: 0.7148205637931824
Epoch 740, training loss: 12.725336074829102 = 0.17260657250881195 + 2.0 * 6.276364803314209
Epoch 740, val loss: 0.7154728770256042
Epoch 750, training loss: 12.714393615722656 = 0.1641532927751541 + 2.0 * 6.275120258331299
Epoch 750, val loss: 0.7164739370346069
Epoch 760, training loss: 12.715126037597656 = 0.1561739295721054 + 2.0 * 6.279476165771484
Epoch 760, val loss: 0.717758297920227
Epoch 770, training loss: 12.692962646484375 = 0.14858046174049377 + 2.0 * 6.272191047668457
Epoch 770, val loss: 0.7192205786705017
Epoch 780, training loss: 12.68189811706543 = 0.14143113791942596 + 2.0 * 6.270233631134033
Epoch 780, val loss: 0.7210782766342163
Epoch 790, training loss: 12.673933029174805 = 0.13464710116386414 + 2.0 * 6.2696428298950195
Epoch 790, val loss: 0.7232235670089722
Epoch 800, training loss: 12.679457664489746 = 0.12820379436016083 + 2.0 * 6.275627136230469
Epoch 800, val loss: 0.7255158424377441
Epoch 810, training loss: 12.66736125946045 = 0.12214954197406769 + 2.0 * 6.272605895996094
Epoch 810, val loss: 0.7280798554420471
Epoch 820, training loss: 12.652613639831543 = 0.11646632105112076 + 2.0 * 6.268073558807373
Epoch 820, val loss: 0.7307832837104797
Epoch 830, training loss: 12.643025398254395 = 0.11108802258968353 + 2.0 * 6.2659687995910645
Epoch 830, val loss: 0.7336389422416687
Epoch 840, training loss: 12.638924598693848 = 0.1059960126876831 + 2.0 * 6.2664642333984375
Epoch 840, val loss: 0.7367032170295715
Epoch 850, training loss: 12.630410194396973 = 0.10118868947029114 + 2.0 * 6.264610767364502
Epoch 850, val loss: 0.7399069666862488
Epoch 860, training loss: 12.626774787902832 = 0.09666156768798828 + 2.0 * 6.265056610107422
Epoch 860, val loss: 0.7431625723838806
Epoch 870, training loss: 12.648460388183594 = 0.09240292757749557 + 2.0 * 6.278028964996338
Epoch 870, val loss: 0.746574342250824
Epoch 880, training loss: 12.614336013793945 = 0.08836568892002106 + 2.0 * 6.2629852294921875
Epoch 880, val loss: 0.7500360608100891
Epoch 890, training loss: 12.609450340270996 = 0.08457919955253601 + 2.0 * 6.262435436248779
Epoch 890, val loss: 0.7537159323692322
Epoch 900, training loss: 12.601954460144043 = 0.0810067281126976 + 2.0 * 6.260473728179932
Epoch 900, val loss: 0.7575632333755493
Epoch 910, training loss: 12.597115516662598 = 0.07760700583457947 + 2.0 * 6.259754180908203
Epoch 910, val loss: 0.7614647746086121
Epoch 920, training loss: 12.608067512512207 = 0.07439445704221725 + 2.0 * 6.266836643218994
Epoch 920, val loss: 0.765514612197876
Epoch 930, training loss: 12.598506927490234 = 0.07134510576725006 + 2.0 * 6.263580799102783
Epoch 930, val loss: 0.7693798542022705
Epoch 940, training loss: 12.583709716796875 = 0.06848637014627457 + 2.0 * 6.2576117515563965
Epoch 940, val loss: 0.7734657526016235
Epoch 950, training loss: 12.580763816833496 = 0.06577671319246292 + 2.0 * 6.257493495941162
Epoch 950, val loss: 0.7776952385902405
Epoch 960, training loss: 12.576268196105957 = 0.06320039927959442 + 2.0 * 6.256534099578857
Epoch 960, val loss: 0.7819899320602417
Epoch 970, training loss: 12.606277465820312 = 0.060765765607357025 + 2.0 * 6.2727556228637695
Epoch 970, val loss: 0.786254346370697
Epoch 980, training loss: 12.571231842041016 = 0.058453429490327835 + 2.0 * 6.256389141082764
Epoch 980, val loss: 0.790404200553894
Epoch 990, training loss: 12.568329811096191 = 0.056277863681316376 + 2.0 * 6.256025791168213
Epoch 990, val loss: 0.7946696877479553
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.788888888888889
0.8371112282551397
=== training gcn model ===
Epoch 0, training loss: 19.127365112304688 = 1.9337198734283447 + 2.0 * 8.596822738647461
Epoch 0, val loss: 1.9383424520492554
Epoch 10, training loss: 19.117094039916992 = 1.924375295639038 + 2.0 * 8.596359252929688
Epoch 10, val loss: 1.9293314218521118
Epoch 20, training loss: 19.096555709838867 = 1.912528395652771 + 2.0 * 8.592013359069824
Epoch 20, val loss: 1.9174786806106567
Epoch 30, training loss: 19.005470275878906 = 1.8965574502944946 + 2.0 * 8.55445671081543
Epoch 30, val loss: 1.9012198448181152
Epoch 40, training loss: 18.393056869506836 = 1.8776562213897705 + 2.0 * 8.257699966430664
Epoch 40, val loss: 1.882521629333496
Epoch 50, training loss: 16.876068115234375 = 1.857399821281433 + 2.0 * 7.509334087371826
Epoch 50, val loss: 1.8627933263778687
Epoch 60, training loss: 16.44552993774414 = 1.841976523399353 + 2.0 * 7.30177640914917
Epoch 60, val loss: 1.847680926322937
Epoch 70, training loss: 16.10472297668457 = 1.828099012374878 + 2.0 * 7.138311862945557
Epoch 70, val loss: 1.8339905738830566
Epoch 80, training loss: 15.85633373260498 = 1.8153148889541626 + 2.0 * 7.020509243011475
Epoch 80, val loss: 1.8213104009628296
Epoch 90, training loss: 15.593923568725586 = 1.8029603958129883 + 2.0 * 6.895481586456299
Epoch 90, val loss: 1.808750867843628
Epoch 100, training loss: 15.344461441040039 = 1.789953589439392 + 2.0 * 6.777254104614258
Epoch 100, val loss: 1.795984148979187
Epoch 110, training loss: 15.175349235534668 = 1.777610182762146 + 2.0 * 6.698869705200195
Epoch 110, val loss: 1.7839224338531494
Epoch 120, training loss: 15.057842254638672 = 1.765512228012085 + 2.0 * 6.646164894104004
Epoch 120, val loss: 1.7719577550888062
Epoch 130, training loss: 14.963773727416992 = 1.7525634765625 + 2.0 * 6.605605125427246
Epoch 130, val loss: 1.7597345113754272
Epoch 140, training loss: 14.884265899658203 = 1.7382757663726807 + 2.0 * 6.572995185852051
Epoch 140, val loss: 1.7466825246810913
Epoch 150, training loss: 14.813387870788574 = 1.722288966178894 + 2.0 * 6.545549392700195
Epoch 150, val loss: 1.7325128316879272
Epoch 160, training loss: 14.740694046020508 = 1.7043981552124023 + 2.0 * 6.518147945404053
Epoch 160, val loss: 1.71701979637146
Epoch 170, training loss: 14.684243202209473 = 1.6842896938323975 + 2.0 * 6.499976634979248
Epoch 170, val loss: 1.6996862888336182
Epoch 180, training loss: 14.622872352600098 = 1.661470651626587 + 2.0 * 6.480700969696045
Epoch 180, val loss: 1.6801364421844482
Epoch 190, training loss: 14.568005561828613 = 1.6352729797363281 + 2.0 * 6.466366291046143
Epoch 190, val loss: 1.6578872203826904
Epoch 200, training loss: 14.523831367492676 = 1.6053611040115356 + 2.0 * 6.459235191345215
Epoch 200, val loss: 1.6325547695159912
Epoch 210, training loss: 14.458776473999023 = 1.571399211883545 + 2.0 * 6.443688869476318
Epoch 210, val loss: 1.6040863990783691
Epoch 220, training loss: 14.403167724609375 = 1.5332303047180176 + 2.0 * 6.434968948364258
Epoch 220, val loss: 1.572203516960144
Epoch 230, training loss: 14.345955848693848 = 1.4907371997833252 + 2.0 * 6.427609443664551
Epoch 230, val loss: 1.5367999076843262
Epoch 240, training loss: 14.281885147094727 = 1.4444609880447388 + 2.0 * 6.418712139129639
Epoch 240, val loss: 1.4983060359954834
Epoch 250, training loss: 14.217345237731934 = 1.395042061805725 + 2.0 * 6.41115140914917
Epoch 250, val loss: 1.4573049545288086
Epoch 260, training loss: 14.154862403869629 = 1.342983603477478 + 2.0 * 6.40593957901001
Epoch 260, val loss: 1.4143147468566895
Epoch 270, training loss: 14.089101791381836 = 1.2897053956985474 + 2.0 * 6.399698257446289
Epoch 270, val loss: 1.3704583644866943
Epoch 280, training loss: 14.024314880371094 = 1.2361335754394531 + 2.0 * 6.39409065246582
Epoch 280, val loss: 1.3266760110855103
Epoch 290, training loss: 13.963260650634766 = 1.1833635568618774 + 2.0 * 6.38994836807251
Epoch 290, val loss: 1.2837618589401245
Epoch 300, training loss: 13.900436401367188 = 1.1321792602539062 + 2.0 * 6.384128570556641
Epoch 300, val loss: 1.2423456907272339
Epoch 310, training loss: 13.840350151062012 = 1.0832687616348267 + 2.0 * 6.378540515899658
Epoch 310, val loss: 1.202783226966858
Epoch 320, training loss: 13.79032039642334 = 1.0370267629623413 + 2.0 * 6.376646995544434
Epoch 320, val loss: 1.1654647588729858
Epoch 330, training loss: 13.735624313354492 = 0.9939538836479187 + 2.0 * 6.370835304260254
Epoch 330, val loss: 1.130730152130127
Epoch 340, training loss: 13.687071800231934 = 0.9535049796104431 + 2.0 * 6.366783618927002
Epoch 340, val loss: 1.0980730056762695
Epoch 350, training loss: 13.641075134277344 = 0.9152745008468628 + 2.0 * 6.362900257110596
Epoch 350, val loss: 1.0672181844711304
Epoch 360, training loss: 13.59853744506836 = 0.8792262077331543 + 2.0 * 6.359655380249023
Epoch 360, val loss: 1.0381805896759033
Epoch 370, training loss: 13.558701515197754 = 0.845329225063324 + 2.0 * 6.356686115264893
Epoch 370, val loss: 1.0110116004943848
Epoch 380, training loss: 13.518672943115234 = 0.8131181001663208 + 2.0 * 6.352777481079102
Epoch 380, val loss: 0.9855391979217529
Epoch 390, training loss: 13.48272705078125 = 0.7824012041091919 + 2.0 * 6.350162982940674
Epoch 390, val loss: 0.9615261554718018
Epoch 400, training loss: 13.455828666687012 = 0.7531293034553528 + 2.0 * 6.351349830627441
Epoch 400, val loss: 0.9391100406646729
Epoch 410, training loss: 13.415925979614258 = 0.7252500057220459 + 2.0 * 6.345337867736816
Epoch 410, val loss: 0.918430745601654
Epoch 420, training loss: 13.382000923156738 = 0.6985705494880676 + 2.0 * 6.341715335845947
Epoch 420, val loss: 0.8992446064949036
Epoch 430, training loss: 13.360137939453125 = 0.6729298830032349 + 2.0 * 6.34360408782959
Epoch 430, val loss: 0.8815040588378906
Epoch 440, training loss: 13.323302268981934 = 0.6484617590904236 + 2.0 * 6.337420463562012
Epoch 440, val loss: 0.8651846051216125
Epoch 450, training loss: 13.294098854064941 = 0.6248155236244202 + 2.0 * 6.334641456604004
Epoch 450, val loss: 0.8501858711242676
Epoch 460, training loss: 13.265615463256836 = 0.6020314693450928 + 2.0 * 6.331791877746582
Epoch 460, val loss: 0.8364230990409851
Epoch 470, training loss: 13.240172386169434 = 0.5799205899238586 + 2.0 * 6.33012580871582
Epoch 470, val loss: 0.8236656188964844
Epoch 480, training loss: 13.215051651000977 = 0.5582106113433838 + 2.0 * 6.328420639038086
Epoch 480, val loss: 0.8117228150367737
Epoch 490, training loss: 13.187381744384766 = 0.536990225315094 + 2.0 * 6.325195789337158
Epoch 490, val loss: 0.8005942702293396
Epoch 500, training loss: 13.163952827453613 = 0.5164238810539246 + 2.0 * 6.323764324188232
Epoch 500, val loss: 0.790305495262146
Epoch 510, training loss: 13.166365623474121 = 0.49650508165359497 + 2.0 * 6.334930419921875
Epoch 510, val loss: 0.7807324528694153
Epoch 520, training loss: 13.115489959716797 = 0.47720518708229065 + 2.0 * 6.3191423416137695
Epoch 520, val loss: 0.7718006372451782
Epoch 530, training loss: 13.094924926757812 = 0.45838552713394165 + 2.0 * 6.318269729614258
Epoch 530, val loss: 0.7634916305541992
Epoch 540, training loss: 13.071310997009277 = 0.43988022208213806 + 2.0 * 6.315715312957764
Epoch 540, val loss: 0.7555944323539734
Epoch 550, training loss: 13.04868221282959 = 0.4215879440307617 + 2.0 * 6.313547134399414
Epoch 550, val loss: 0.7479884028434753
Epoch 560, training loss: 13.02709674835205 = 0.40350136160850525 + 2.0 * 6.311797618865967
Epoch 560, val loss: 0.7406561970710754
Epoch 570, training loss: 13.006360054016113 = 0.3856375515460968 + 2.0 * 6.310361385345459
Epoch 570, val loss: 0.7336227297782898
Epoch 580, training loss: 12.99342155456543 = 0.36804839968681335 + 2.0 * 6.312686443328857
Epoch 580, val loss: 0.7268887758255005
Epoch 590, training loss: 12.970864295959473 = 0.35089248418807983 + 2.0 * 6.309986114501953
Epoch 590, val loss: 0.7204138040542603
Epoch 600, training loss: 12.946538925170898 = 0.33412671089172363 + 2.0 * 6.306206226348877
Epoch 600, val loss: 0.7142267227172852
Epoch 610, training loss: 12.930203437805176 = 0.3177262544631958 + 2.0 * 6.306238651275635
Epoch 610, val loss: 0.7083131670951843
Epoch 620, training loss: 12.91226863861084 = 0.30176541209220886 + 2.0 * 6.305251598358154
Epoch 620, val loss: 0.7026187181472778
Epoch 630, training loss: 12.890822410583496 = 0.28628867864608765 + 2.0 * 6.302267074584961
Epoch 630, val loss: 0.6971896290779114
Epoch 640, training loss: 12.872809410095215 = 0.2712724208831787 + 2.0 * 6.3007683753967285
Epoch 640, val loss: 0.6920568346977234
Epoch 650, training loss: 12.856001853942871 = 0.25670167803764343 + 2.0 * 6.299650192260742
Epoch 650, val loss: 0.6872460842132568
Epoch 660, training loss: 12.85399055480957 = 0.2426108568906784 + 2.0 * 6.305689811706543
Epoch 660, val loss: 0.6827333569526672
Epoch 670, training loss: 12.8327054977417 = 0.22913505136966705 + 2.0 * 6.301784992218018
Epoch 670, val loss: 0.6785697937011719
Epoch 680, training loss: 12.81123161315918 = 0.21625317633152008 + 2.0 * 6.297489166259766
Epoch 680, val loss: 0.6747152805328369
Epoch 690, training loss: 12.79591178894043 = 0.20396757125854492 + 2.0 * 6.295971870422363
Epoch 690, val loss: 0.6712883710861206
Epoch 700, training loss: 12.783900260925293 = 0.19223622977733612 + 2.0 * 6.29583215713501
Epoch 700, val loss: 0.6683239340782166
Epoch 710, training loss: 12.769512176513672 = 0.18111442029476166 + 2.0 * 6.294198989868164
Epoch 710, val loss: 0.6657463312149048
Epoch 720, training loss: 12.754728317260742 = 0.17063133418560028 + 2.0 * 6.292048454284668
Epoch 720, val loss: 0.6635218858718872
Epoch 730, training loss: 12.742772102355957 = 0.16071180999279022 + 2.0 * 6.291029930114746
Epoch 730, val loss: 0.6617633700370789
Epoch 740, training loss: 12.731657028198242 = 0.1513584405183792 + 2.0 * 6.290149211883545
Epoch 740, val loss: 0.6604170203208923
Epoch 750, training loss: 12.730440139770508 = 0.14254343509674072 + 2.0 * 6.293948173522949
Epoch 750, val loss: 0.6594429016113281
Epoch 760, training loss: 12.714677810668945 = 0.13431262969970703 + 2.0 * 6.290182590484619
Epoch 760, val loss: 0.6588223576545715
Epoch 770, training loss: 12.703935623168945 = 0.1266358643770218 + 2.0 * 6.288650035858154
Epoch 770, val loss: 0.6585326194763184
Epoch 780, training loss: 12.693206787109375 = 0.11944136023521423 + 2.0 * 6.2868828773498535
Epoch 780, val loss: 0.658652663230896
Epoch 790, training loss: 12.689786911010742 = 0.11272700130939484 + 2.0 * 6.288529872894287
Epoch 790, val loss: 0.6591259241104126
Epoch 800, training loss: 12.682381629943848 = 0.10646729171276093 + 2.0 * 6.287957191467285
Epoch 800, val loss: 0.6598663330078125
Epoch 810, training loss: 12.668816566467285 = 0.10063110291957855 + 2.0 * 6.284092903137207
Epoch 810, val loss: 0.6608387231826782
Epoch 820, training loss: 12.662254333496094 = 0.09519955515861511 + 2.0 * 6.283527374267578
Epoch 820, val loss: 0.6621338725090027
Epoch 830, training loss: 12.658717155456543 = 0.09013302624225616 + 2.0 * 6.284292221069336
Epoch 830, val loss: 0.6637149453163147
Epoch 840, training loss: 12.648244857788086 = 0.08542168140411377 + 2.0 * 6.281411647796631
Epoch 840, val loss: 0.6654431819915771
Epoch 850, training loss: 12.64477252960205 = 0.08101223409175873 + 2.0 * 6.2818803787231445
Epoch 850, val loss: 0.6673989295959473
Epoch 860, training loss: 12.635746955871582 = 0.0769076943397522 + 2.0 * 6.279419422149658
Epoch 860, val loss: 0.6694973707199097
Epoch 870, training loss: 12.656063079833984 = 0.07306414842605591 + 2.0 * 6.291499614715576
Epoch 870, val loss: 0.6718155741691589
Epoch 880, training loss: 12.635184288024902 = 0.06952556222677231 + 2.0 * 6.282829284667969
Epoch 880, val loss: 0.6741418838500977
Epoch 890, training loss: 12.621469497680664 = 0.06621044874191284 + 2.0 * 6.277629375457764
Epoch 890, val loss: 0.6766352653503418
Epoch 900, training loss: 12.61422061920166 = 0.06311319768428802 + 2.0 * 6.2755537033081055
Epoch 900, val loss: 0.6793040037155151
Epoch 910, training loss: 12.60949420928955 = 0.06020685285329819 + 2.0 * 6.274643898010254
Epoch 910, val loss: 0.6821074485778809
Epoch 920, training loss: 12.604516983032227 = 0.05747474730014801 + 2.0 * 6.2735209465026855
Epoch 920, val loss: 0.6849973797798157
Epoch 930, training loss: 12.601448059082031 = 0.05490757152438164 + 2.0 * 6.273270130157471
Epoch 930, val loss: 0.6879379153251648
Epoch 940, training loss: 12.60327434539795 = 0.05249636247754097 + 2.0 * 6.275389194488525
Epoch 940, val loss: 0.6910157203674316
Epoch 950, training loss: 12.598966598510742 = 0.05024715140461922 + 2.0 * 6.274359703063965
Epoch 950, val loss: 0.6940675973892212
Epoch 960, training loss: 12.591008186340332 = 0.04814634844660759 + 2.0 * 6.271430969238281
Epoch 960, val loss: 0.6970277428627014
Epoch 970, training loss: 12.587175369262695 = 0.046167731285095215 + 2.0 * 6.270503997802734
Epoch 970, val loss: 0.7001887559890747
Epoch 980, training loss: 12.588919639587402 = 0.044298600405454636 + 2.0 * 6.272310733795166
Epoch 980, val loss: 0.7033341526985168
Epoch 990, training loss: 12.597264289855957 = 0.04254217818379402 + 2.0 * 6.277360916137695
Epoch 990, val loss: 0.7065803408622742
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8074074074074075
0.838165524512388
=== training gcn model ===
Epoch 0, training loss: 19.12749481201172 = 1.9338487386703491 + 2.0 * 8.596822738647461
Epoch 0, val loss: 1.9278128147125244
Epoch 10, training loss: 19.116300582885742 = 1.923620343208313 + 2.0 * 8.59634017944336
Epoch 10, val loss: 1.9174373149871826
Epoch 20, training loss: 19.096376419067383 = 1.9109766483306885 + 2.0 * 8.592700004577637
Epoch 20, val loss: 1.9045891761779785
Epoch 30, training loss: 19.026132583618164 = 1.8940775394439697 + 2.0 * 8.566027641296387
Epoch 30, val loss: 1.8874915838241577
Epoch 40, training loss: 18.663589477539062 = 1.8734244108200073 + 2.0 * 8.395082473754883
Epoch 40, val loss: 1.867214560508728
Epoch 50, training loss: 17.085922241210938 = 1.851489543914795 + 2.0 * 7.617216110229492
Epoch 50, val loss: 1.8464992046356201
Epoch 60, training loss: 16.50140953063965 = 1.8342174291610718 + 2.0 * 7.333596229553223
Epoch 60, val loss: 1.8311227560043335
Epoch 70, training loss: 16.07916831970215 = 1.8203061819076538 + 2.0 * 7.129431247711182
Epoch 70, val loss: 1.8176566362380981
Epoch 80, training loss: 15.779491424560547 = 1.8056975603103638 + 2.0 * 6.986896991729736
Epoch 80, val loss: 1.8035483360290527
Epoch 90, training loss: 15.546233177185059 = 1.792325735092163 + 2.0 * 6.876953601837158
Epoch 90, val loss: 1.7907594442367554
Epoch 100, training loss: 15.364378929138184 = 1.7782891988754272 + 2.0 * 6.7930450439453125
Epoch 100, val loss: 1.7778229713439941
Epoch 110, training loss: 15.212634086608887 = 1.76407790184021 + 2.0 * 6.724277973175049
Epoch 110, val loss: 1.765189528465271
Epoch 120, training loss: 15.086210250854492 = 1.7501506805419922 + 2.0 * 6.66802978515625
Epoch 120, val loss: 1.7528811693191528
Epoch 130, training loss: 14.984235763549805 = 1.7361860275268555 + 2.0 * 6.624024868011475
Epoch 130, val loss: 1.740330696105957
Epoch 140, training loss: 14.904855728149414 = 1.720604658126831 + 2.0 * 6.592125415802002
Epoch 140, val loss: 1.7264310121536255
Epoch 150, training loss: 14.834089279174805 = 1.702422022819519 + 2.0 * 6.565833568572998
Epoch 150, val loss: 1.7104946374893188
Epoch 160, training loss: 14.764087677001953 = 1.6815294027328491 + 2.0 * 6.541279315948486
Epoch 160, val loss: 1.6924688816070557
Epoch 170, training loss: 14.69883918762207 = 1.6581097841262817 + 2.0 * 6.520364761352539
Epoch 170, val loss: 1.6722642183303833
Epoch 180, training loss: 14.635940551757812 = 1.631788730621338 + 2.0 * 6.502075672149658
Epoch 180, val loss: 1.6496320962905884
Epoch 190, training loss: 14.575469970703125 = 1.6018991470336914 + 2.0 * 6.486785411834717
Epoch 190, val loss: 1.6241531372070312
Epoch 200, training loss: 14.515140533447266 = 1.5680798292160034 + 2.0 * 6.473530292510986
Epoch 200, val loss: 1.5953470468521118
Epoch 210, training loss: 14.449921607971191 = 1.5302640199661255 + 2.0 * 6.459828853607178
Epoch 210, val loss: 1.5633349418640137
Epoch 220, training loss: 14.383674621582031 = 1.4885408878326416 + 2.0 * 6.447566986083984
Epoch 220, val loss: 1.5282715559005737
Epoch 230, training loss: 14.324662208557129 = 1.4431394338607788 + 2.0 * 6.440761566162109
Epoch 230, val loss: 1.490211844444275
Epoch 240, training loss: 14.249621391296387 = 1.394986867904663 + 2.0 * 6.427317142486572
Epoch 240, val loss: 1.449936032295227
Epoch 250, training loss: 14.182197570800781 = 1.344300389289856 + 2.0 * 6.418948650360107
Epoch 250, val loss: 1.4078956842422485
Epoch 260, training loss: 14.113601684570312 = 1.2916191816329956 + 2.0 * 6.410991191864014
Epoch 260, val loss: 1.3646050691604614
Epoch 270, training loss: 14.050833702087402 = 1.2380741834640503 + 2.0 * 6.406379699707031
Epoch 270, val loss: 1.321075439453125
Epoch 280, training loss: 13.981426239013672 = 1.185324788093567 + 2.0 * 6.398050785064697
Epoch 280, val loss: 1.2786486148834229
Epoch 290, training loss: 13.92907428741455 = 1.1339657306671143 + 2.0 * 6.397554397583008
Epoch 290, val loss: 1.2377792596817017
Epoch 300, training loss: 13.859655380249023 = 1.0846199989318848 + 2.0 * 6.387517929077148
Epoch 300, val loss: 1.1989636421203613
Epoch 310, training loss: 13.800607681274414 = 1.0372087955474854 + 2.0 * 6.381699562072754
Epoch 310, val loss: 1.1621049642562866
Epoch 320, training loss: 13.746047973632812 = 0.9915117025375366 + 2.0 * 6.377268314361572
Epoch 320, val loss: 1.1269105672836304
Epoch 330, training loss: 13.698275566101074 = 0.94754958152771 + 2.0 * 6.375362873077393
Epoch 330, val loss: 1.0934910774230957
Epoch 340, training loss: 13.646162986755371 = 0.9055363535881042 + 2.0 * 6.3703131675720215
Epoch 340, val loss: 1.061767578125
Epoch 350, training loss: 13.594700813293457 = 0.8648260235786438 + 2.0 * 6.3649373054504395
Epoch 350, val loss: 1.0313645601272583
Epoch 360, training loss: 13.570780754089355 = 0.8251753449440002 + 2.0 * 6.372802734375
Epoch 360, val loss: 1.0019638538360596
Epoch 370, training loss: 13.51034927368164 = 0.7867894768714905 + 2.0 * 6.361779689788818
Epoch 370, val loss: 0.9738050699234009
Epoch 380, training loss: 13.459437370300293 = 0.7493933439254761 + 2.0 * 6.355021953582764
Epoch 380, val loss: 0.9466671943664551
Epoch 390, training loss: 13.416028022766113 = 0.7127645015716553 + 2.0 * 6.3516316413879395
Epoch 390, val loss: 0.920328676700592
Epoch 400, training loss: 13.37730598449707 = 0.6768536567687988 + 2.0 * 6.350225925445557
Epoch 400, val loss: 0.8948029279708862
Epoch 410, training loss: 13.341102600097656 = 0.6420284509658813 + 2.0 * 6.349536895751953
Epoch 410, val loss: 0.8703630566596985
Epoch 420, training loss: 13.29411506652832 = 0.6084702014923096 + 2.0 * 6.342822551727295
Epoch 420, val loss: 0.8472582101821899
Epoch 430, training loss: 13.255206108093262 = 0.5759941339492798 + 2.0 * 6.339605808258057
Epoch 430, val loss: 0.8252912759780884
Epoch 440, training loss: 13.225759506225586 = 0.5447046756744385 + 2.0 * 6.340527534484863
Epoch 440, val loss: 0.8045533299446106
Epoch 450, training loss: 13.188542366027832 = 0.5149914622306824 + 2.0 * 6.336775302886963
Epoch 450, val loss: 0.7853206992149353
Epoch 460, training loss: 13.152871131896973 = 0.4866924285888672 + 2.0 * 6.333089351654053
Epoch 460, val loss: 0.7674427032470703
Epoch 470, training loss: 13.133201599121094 = 0.45982691645622253 + 2.0 * 6.3366875648498535
Epoch 470, val loss: 0.7509231567382812
Epoch 480, training loss: 13.0903959274292 = 0.43457120656967163 + 2.0 * 6.327912330627441
Epoch 480, val loss: 0.7359298467636108
Epoch 490, training loss: 13.061996459960938 = 0.41078686714172363 + 2.0 * 6.3256049156188965
Epoch 490, val loss: 0.7223638892173767
Epoch 500, training loss: 13.038392066955566 = 0.3883417546749115 + 2.0 * 6.3250250816345215
Epoch 500, val loss: 0.7099650502204895
Epoch 510, training loss: 13.013084411621094 = 0.3672614097595215 + 2.0 * 6.322911739349365
Epoch 510, val loss: 0.6988512277603149
Epoch 520, training loss: 12.98630142211914 = 0.34747716784477234 + 2.0 * 6.3194122314453125
Epoch 520, val loss: 0.6888340711593628
Epoch 530, training loss: 12.966044425964355 = 0.32881057262420654 + 2.0 * 6.31861686706543
Epoch 530, val loss: 0.6798124313354492
Epoch 540, training loss: 12.946146965026855 = 0.3112982511520386 + 2.0 * 6.317424297332764
Epoch 540, val loss: 0.6716375350952148
Epoch 550, training loss: 12.92616081237793 = 0.2948084771633148 + 2.0 * 6.315676212310791
Epoch 550, val loss: 0.6643913388252258
Epoch 560, training loss: 12.905766487121582 = 0.2792320251464844 + 2.0 * 6.313267230987549
Epoch 560, val loss: 0.6577950119972229
Epoch 570, training loss: 12.892829895019531 = 0.2644316256046295 + 2.0 * 6.314198970794678
Epoch 570, val loss: 0.6517707705497742
Epoch 580, training loss: 12.8839693069458 = 0.25044330954551697 + 2.0 * 6.316762924194336
Epoch 580, val loss: 0.6464428901672363
Epoch 590, training loss: 12.857285499572754 = 0.23719824850559235 + 2.0 * 6.310043811798096
Epoch 590, val loss: 0.6416383385658264
Epoch 600, training loss: 12.839300155639648 = 0.22460532188415527 + 2.0 * 6.307347297668457
Epoch 600, val loss: 0.6372806429862976
Epoch 610, training loss: 12.852307319641113 = 0.21258190274238586 + 2.0 * 6.3198628425598145
Epoch 610, val loss: 0.6333650350570679
Epoch 620, training loss: 12.813766479492188 = 0.2012868970632553 + 2.0 * 6.306239604949951
Epoch 620, val loss: 0.6298887729644775
Epoch 630, training loss: 12.799920082092285 = 0.19059740006923676 + 2.0 * 6.304661273956299
Epoch 630, val loss: 0.6269050240516663
Epoch 640, training loss: 12.785659790039062 = 0.18045732378959656 + 2.0 * 6.302601337432861
Epoch 640, val loss: 0.6242981553077698
Epoch 650, training loss: 12.772522926330566 = 0.1708303838968277 + 2.0 * 6.300846099853516
Epoch 650, val loss: 0.6220725178718567
Epoch 660, training loss: 12.770634651184082 = 0.1617216169834137 + 2.0 * 6.30445671081543
Epoch 660, val loss: 0.6202324628829956
Epoch 670, training loss: 12.763535499572754 = 0.1531815379858017 + 2.0 * 6.305177211761475
Epoch 670, val loss: 0.618787407875061
Epoch 680, training loss: 12.74410343170166 = 0.14519146084785461 + 2.0 * 6.2994561195373535
Epoch 680, val loss: 0.617729663848877
Epoch 690, training loss: 12.731559753417969 = 0.13768836855888367 + 2.0 * 6.296935558319092
Epoch 690, val loss: 0.6170095205307007
Epoch 700, training loss: 12.721491813659668 = 0.130631685256958 + 2.0 * 6.2954301834106445
Epoch 700, val loss: 0.6165945529937744
Epoch 710, training loss: 12.7138671875 = 0.12399410456418991 + 2.0 * 6.294936656951904
Epoch 710, val loss: 0.616494357585907
Epoch 720, training loss: 12.706465721130371 = 0.1177787110209465 + 2.0 * 6.2943434715271
Epoch 720, val loss: 0.6166994571685791
Epoch 730, training loss: 12.699047088623047 = 0.11197805404663086 + 2.0 * 6.293534278869629
Epoch 730, val loss: 0.6171910762786865
Epoch 740, training loss: 12.689264297485352 = 0.10654618591070175 + 2.0 * 6.291358947753906
Epoch 740, val loss: 0.6179285645484924
Epoch 750, training loss: 12.686012268066406 = 0.10144536942243576 + 2.0 * 6.292283535003662
Epoch 750, val loss: 0.6188914179801941
Epoch 760, training loss: 12.684937477111816 = 0.09665839374065399 + 2.0 * 6.294139385223389
Epoch 760, val loss: 0.6201155781745911
Epoch 770, training loss: 12.670886039733887 = 0.0922073945403099 + 2.0 * 6.289339542388916
Epoch 770, val loss: 0.6215276122093201
Epoch 780, training loss: 12.664762496948242 = 0.08803122490644455 + 2.0 * 6.288365840911865
Epoch 780, val loss: 0.6231421828269958
Epoch 790, training loss: 12.6565580368042 = 0.08410289138555527 + 2.0 * 6.286227703094482
Epoch 790, val loss: 0.6249316334724426
Epoch 800, training loss: 12.669641494750977 = 0.08039642870426178 + 2.0 * 6.294622421264648
Epoch 800, val loss: 0.6268468499183655
Epoch 810, training loss: 12.653229713439941 = 0.07694840431213379 + 2.0 * 6.288140773773193
Epoch 810, val loss: 0.6289477348327637
Epoch 820, training loss: 12.641426086425781 = 0.07367662340402603 + 2.0 * 6.28387451171875
Epoch 820, val loss: 0.6311699151992798
Epoch 830, training loss: 12.636539459228516 = 0.07059738039970398 + 2.0 * 6.282970905303955
Epoch 830, val loss: 0.6334561109542847
Epoch 840, training loss: 12.639518737792969 = 0.06767744570970535 + 2.0 * 6.2859206199646
Epoch 840, val loss: 0.6358377933502197
Epoch 850, training loss: 12.632329940795898 = 0.06491725146770477 + 2.0 * 6.283706188201904
Epoch 850, val loss: 0.6383706331253052
Epoch 860, training loss: 12.626161575317383 = 0.06231207028031349 + 2.0 * 6.281924724578857
Epoch 860, val loss: 0.6408595442771912
Epoch 870, training loss: 12.618183135986328 = 0.05984025448560715 + 2.0 * 6.279171466827393
Epoch 870, val loss: 0.6434974670410156
Epoch 880, training loss: 12.618651390075684 = 0.05749409273266792 + 2.0 * 6.28057861328125
Epoch 880, val loss: 0.6461686491966248
Epoch 890, training loss: 12.613966941833496 = 0.05527511611580849 + 2.0 * 6.279345989227295
Epoch 890, val loss: 0.64887535572052
Epoch 900, training loss: 12.608656883239746 = 0.053167521953582764 + 2.0 * 6.277744770050049
Epoch 900, val loss: 0.6516357660293579
Epoch 910, training loss: 12.60974407196045 = 0.051174797117710114 + 2.0 * 6.279284477233887
Epoch 910, val loss: 0.6544461250305176
Epoch 920, training loss: 12.599996566772461 = 0.04927120730280876 + 2.0 * 6.275362491607666
Epoch 920, val loss: 0.6572540402412415
Epoch 930, training loss: 12.595901489257812 = 0.04746663570404053 + 2.0 * 6.27421760559082
Epoch 930, val loss: 0.6601389050483704
Epoch 940, training loss: 12.603694915771484 = 0.045739881694316864 + 2.0 * 6.278977394104004
Epoch 940, val loss: 0.6630241870880127
Epoch 950, training loss: 12.596176147460938 = 0.044108059257268906 + 2.0 * 6.276033878326416
Epoch 950, val loss: 0.6658846735954285
Epoch 960, training loss: 12.591349601745605 = 0.04255091771483421 + 2.0 * 6.274399280548096
Epoch 960, val loss: 0.6688134074211121
Epoch 970, training loss: 12.585960388183594 = 0.041071172803640366 + 2.0 * 6.272444725036621
Epoch 970, val loss: 0.671730637550354
Epoch 980, training loss: 12.580995559692383 = 0.03965859115123749 + 2.0 * 6.2706685066223145
Epoch 980, val loss: 0.6746395230293274
Epoch 990, training loss: 12.578936576843262 = 0.038310907781124115 + 2.0 * 6.270312786102295
Epoch 990, val loss: 0.6775745749473572
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.825925925925926
0.8423827095413812
The final CL Acc:0.80741, 0.01512, The final GNN Acc:0.83922, 0.00228
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11646])
remove edge: torch.Size([2, 9436])
updated graph: torch.Size([2, 10526])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 19.13356590270996 = 1.9398577213287354 + 2.0 * 8.596854209899902
Epoch 0, val loss: 1.9399971961975098
Epoch 10, training loss: 19.123371124267578 = 1.930235743522644 + 2.0 * 8.59656810760498
Epoch 10, val loss: 1.9303231239318848
Epoch 20, training loss: 19.106945037841797 = 1.9184200763702393 + 2.0 * 8.59426212310791
Epoch 20, val loss: 1.9179364442825317
Epoch 30, training loss: 19.0562744140625 = 1.902550458908081 + 2.0 * 8.576862335205078
Epoch 30, val loss: 1.9009252786636353
Epoch 40, training loss: 18.848352432250977 = 1.8829457759857178 + 2.0 * 8.48270320892334
Epoch 40, val loss: 1.8809040784835815
Epoch 50, training loss: 18.0279598236084 = 1.862687349319458 + 2.0 * 8.082635879516602
Epoch 50, val loss: 1.8609787225723267
Epoch 60, training loss: 17.429555892944336 = 1.8453575372695923 + 2.0 * 7.792099475860596
Epoch 60, val loss: 1.8450900316238403
Epoch 70, training loss: 16.439693450927734 = 1.8312358856201172 + 2.0 * 7.304228782653809
Epoch 70, val loss: 1.8321328163146973
Epoch 80, training loss: 15.854334831237793 = 1.8178198337554932 + 2.0 * 7.0182576179504395
Epoch 80, val loss: 1.8198739290237427
Epoch 90, training loss: 15.568387031555176 = 1.805076003074646 + 2.0 * 6.881655693054199
Epoch 90, val loss: 1.8079655170440674
Epoch 100, training loss: 15.364797592163086 = 1.7923153638839722 + 2.0 * 6.786241054534912
Epoch 100, val loss: 1.7961206436157227
Epoch 110, training loss: 15.208471298217773 = 1.7799737453460693 + 2.0 * 6.7142486572265625
Epoch 110, val loss: 1.7842799425125122
Epoch 120, training loss: 15.08041000366211 = 1.7676752805709839 + 2.0 * 6.656367301940918
Epoch 120, val loss: 1.7728440761566162
Epoch 130, training loss: 14.978837013244629 = 1.7551480531692505 + 2.0 * 6.611844539642334
Epoch 130, val loss: 1.7618049383163452
Epoch 140, training loss: 14.891865730285645 = 1.7421504259109497 + 2.0 * 6.574857711791992
Epoch 140, val loss: 1.7503794431686401
Epoch 150, training loss: 14.826117515563965 = 1.7278867959976196 + 2.0 * 6.549115180969238
Epoch 150, val loss: 1.7381211519241333
Epoch 160, training loss: 14.77219009399414 = 1.7117388248443604 + 2.0 * 6.53022575378418
Epoch 160, val loss: 1.7245069742202759
Epoch 170, training loss: 14.715840339660645 = 1.6935056447982788 + 2.0 * 6.511167526245117
Epoch 170, val loss: 1.7092725038528442
Epoch 180, training loss: 14.665351867675781 = 1.67286217212677 + 2.0 * 6.49624490737915
Epoch 180, val loss: 1.6922550201416016
Epoch 190, training loss: 14.613353729248047 = 1.6494978666305542 + 2.0 * 6.481927871704102
Epoch 190, val loss: 1.673115611076355
Epoch 200, training loss: 14.574248313903809 = 1.6229866743087769 + 2.0 * 6.475630760192871
Epoch 200, val loss: 1.6516571044921875
Epoch 210, training loss: 14.512267112731934 = 1.5936450958251953 + 2.0 * 6.459311008453369
Epoch 210, val loss: 1.6277722120285034
Epoch 220, training loss: 14.45928955078125 = 1.5612372159957886 + 2.0 * 6.449026107788086
Epoch 220, val loss: 1.601640224456787
Epoch 230, training loss: 14.405424118041992 = 1.5255067348480225 + 2.0 * 6.439958572387695
Epoch 230, val loss: 1.5730164051055908
Epoch 240, training loss: 14.350017547607422 = 1.4863569736480713 + 2.0 * 6.431830406188965
Epoch 240, val loss: 1.5419106483459473
Epoch 250, training loss: 14.292240142822266 = 1.4439901113510132 + 2.0 * 6.4241251945495605
Epoch 250, val loss: 1.5084619522094727
Epoch 260, training loss: 14.248059272766113 = 1.3989174365997314 + 2.0 * 6.4245710372924805
Epoch 260, val loss: 1.4731441736221313
Epoch 270, training loss: 14.17521858215332 = 1.3522580862045288 + 2.0 * 6.41148042678833
Epoch 270, val loss: 1.4372345209121704
Epoch 280, training loss: 14.116739273071289 = 1.3044867515563965 + 2.0 * 6.406126499176025
Epoch 280, val loss: 1.4004439115524292
Epoch 290, training loss: 14.066747665405273 = 1.2559044361114502 + 2.0 * 6.405421733856201
Epoch 290, val loss: 1.3632432222366333
Epoch 300, training loss: 13.999656677246094 = 1.2071696519851685 + 2.0 * 6.396243572235107
Epoch 300, val loss: 1.3266092538833618
Epoch 310, training loss: 13.942843437194824 = 1.1588735580444336 + 2.0 * 6.391984939575195
Epoch 310, val loss: 1.2904459238052368
Epoch 320, training loss: 13.88691234588623 = 1.111051321029663 + 2.0 * 6.387930393218994
Epoch 320, val loss: 1.2549550533294678
Epoch 330, training loss: 13.831263542175293 = 1.063974142074585 + 2.0 * 6.3836445808410645
Epoch 330, val loss: 1.2202996015548706
Epoch 340, training loss: 13.789234161376953 = 1.0180221796035767 + 2.0 * 6.385605812072754
Epoch 340, val loss: 1.1867915391921997
Epoch 350, training loss: 13.73261547088623 = 0.974033772945404 + 2.0 * 6.37929105758667
Epoch 350, val loss: 1.1549900770187378
Epoch 360, training loss: 13.681462287902832 = 0.932226836681366 + 2.0 * 6.374617576599121
Epoch 360, val loss: 1.1251319646835327
Epoch 370, training loss: 13.632721900939941 = 0.8924165964126587 + 2.0 * 6.370152473449707
Epoch 370, val loss: 1.0969722270965576
Epoch 380, training loss: 13.597797393798828 = 0.8544474840164185 + 2.0 * 6.37167501449585
Epoch 380, val loss: 1.0705899000167847
Epoch 390, training loss: 13.550861358642578 = 0.8188613057136536 + 2.0 * 6.366000175476074
Epoch 390, val loss: 1.0459277629852295
Epoch 400, training loss: 13.50971794128418 = 0.785180926322937 + 2.0 * 6.362268447875977
Epoch 400, val loss: 1.0232611894607544
Epoch 410, training loss: 13.469128608703613 = 0.7531225085258484 + 2.0 * 6.35800313949585
Epoch 410, val loss: 1.0021436214447021
Epoch 420, training loss: 13.438104629516602 = 0.7224356532096863 + 2.0 * 6.357834339141846
Epoch 420, val loss: 0.9823921322822571
Epoch 430, training loss: 13.405401229858398 = 0.6931890249252319 + 2.0 * 6.356106281280518
Epoch 430, val loss: 0.9644200205802917
Epoch 440, training loss: 13.368365287780762 = 0.665468156337738 + 2.0 * 6.3514485359191895
Epoch 440, val loss: 0.9478384256362915
Epoch 450, training loss: 13.334634780883789 = 0.6387500762939453 + 2.0 * 6.347942352294922
Epoch 450, val loss: 0.9326699376106262
Epoch 460, training loss: 13.321564674377441 = 0.6129903197288513 + 2.0 * 6.354287147521973
Epoch 460, val loss: 0.9186604619026184
Epoch 470, training loss: 13.281329154968262 = 0.5884218215942383 + 2.0 * 6.346453666687012
Epoch 470, val loss: 0.9062353372573853
Epoch 480, training loss: 13.247862815856934 = 0.5650261044502258 + 2.0 * 6.341418266296387
Epoch 480, val loss: 0.8950302600860596
Epoch 490, training loss: 13.21873950958252 = 0.5423897504806519 + 2.0 * 6.338174819946289
Epoch 490, val loss: 0.8850167989730835
Epoch 500, training loss: 13.20104694366455 = 0.5205456614494324 + 2.0 * 6.340250492095947
Epoch 500, val loss: 0.8760914206504822
Epoch 510, training loss: 13.170598983764648 = 0.4995262622833252 + 2.0 * 6.335536479949951
Epoch 510, val loss: 0.8684399724006653
Epoch 520, training loss: 13.144197463989258 = 0.47923147678375244 + 2.0 * 6.332482814788818
Epoch 520, val loss: 0.8618202209472656
Epoch 530, training loss: 13.126128196716309 = 0.45956310629844666 + 2.0 * 6.333282470703125
Epoch 530, val loss: 0.8561583161354065
Epoch 540, training loss: 13.096023559570312 = 0.4405468702316284 + 2.0 * 6.327738285064697
Epoch 540, val loss: 0.8514198064804077
Epoch 550, training loss: 13.083089828491211 = 0.422041118144989 + 2.0 * 6.330524444580078
Epoch 550, val loss: 0.8476001024246216
Epoch 560, training loss: 13.070006370544434 = 0.4041871428489685 + 2.0 * 6.33290958404541
Epoch 560, val loss: 0.8447640538215637
Epoch 570, training loss: 13.034423828125 = 0.3871477544307709 + 2.0 * 6.323637962341309
Epoch 570, val loss: 0.8427344560623169
Epoch 580, training loss: 13.010546684265137 = 0.3705914318561554 + 2.0 * 6.319977760314941
Epoch 580, val loss: 0.8414632678031921
Epoch 590, training loss: 12.990622520446777 = 0.354454904794693 + 2.0 * 6.318083763122559
Epoch 590, val loss: 0.8408688306808472
Epoch 600, training loss: 12.976751327514648 = 0.3387596905231476 + 2.0 * 6.318995952606201
Epoch 600, val loss: 0.8409563899040222
Epoch 610, training loss: 12.967143058776855 = 0.3237825632095337 + 2.0 * 6.321680068969727
Epoch 610, val loss: 0.8415946364402771
Epoch 620, training loss: 12.946645736694336 = 0.309438556432724 + 2.0 * 6.318603515625
Epoch 620, val loss: 0.8430044054985046
Epoch 630, training loss: 12.923282623291016 = 0.2956976890563965 + 2.0 * 6.3137922286987305
Epoch 630, val loss: 0.8450201153755188
Epoch 640, training loss: 12.905440330505371 = 0.2825698256492615 + 2.0 * 6.311435222625732
Epoch 640, val loss: 0.8475244641304016
Epoch 650, training loss: 12.89132308959961 = 0.2699587643146515 + 2.0 * 6.31068229675293
Epoch 650, val loss: 0.850556492805481
Epoch 660, training loss: 12.8804292678833 = 0.2579566240310669 + 2.0 * 6.311236381530762
Epoch 660, val loss: 0.8540158271789551
Epoch 670, training loss: 12.863617897033691 = 0.24652576446533203 + 2.0 * 6.30854606628418
Epoch 670, val loss: 0.8579611778259277
Epoch 680, training loss: 12.846980094909668 = 0.23567159473896027 + 2.0 * 6.305654048919678
Epoch 680, val loss: 0.8622888922691345
Epoch 690, training loss: 12.837961196899414 = 0.22535479068756104 + 2.0 * 6.306303024291992
Epoch 690, val loss: 0.8669975399971008
Epoch 700, training loss: 12.823086738586426 = 0.2155015915632248 + 2.0 * 6.303792476654053
Epoch 700, val loss: 0.8720549941062927
Epoch 710, training loss: 12.815183639526367 = 0.20614610612392426 + 2.0 * 6.304518699645996
Epoch 710, val loss: 0.877492368221283
Epoch 720, training loss: 12.801538467407227 = 0.197287455201149 + 2.0 * 6.302125453948975
Epoch 720, val loss: 0.8831853866577148
Epoch 730, training loss: 12.794733047485352 = 0.18882565200328827 + 2.0 * 6.302953720092773
Epoch 730, val loss: 0.8892070651054382
Epoch 740, training loss: 12.781888008117676 = 0.18080584704875946 + 2.0 * 6.300540924072266
Epoch 740, val loss: 0.895393431186676
Epoch 750, training loss: 12.768840789794922 = 0.17317309975624084 + 2.0 * 6.2978339195251465
Epoch 750, val loss: 0.9018584489822388
Epoch 760, training loss: 12.768476486206055 = 0.16593323647975922 + 2.0 * 6.301271438598633
Epoch 760, val loss: 0.9085268378257751
Epoch 770, training loss: 12.756617546081543 = 0.15905939042568207 + 2.0 * 6.298779010772705
Epoch 770, val loss: 0.9152824282646179
Epoch 780, training loss: 12.74002456665039 = 0.15252821147441864 + 2.0 * 6.293748378753662
Epoch 780, val loss: 0.9222669005393982
Epoch 790, training loss: 12.732072830200195 = 0.14629456400871277 + 2.0 * 6.29288911819458
Epoch 790, val loss: 0.9294493794441223
Epoch 800, training loss: 12.7357816696167 = 0.14033734798431396 + 2.0 * 6.297722339630127
Epoch 800, val loss: 0.9366735816001892
Epoch 810, training loss: 12.730978012084961 = 0.13470341265201569 + 2.0 * 6.298137187957764
Epoch 810, val loss: 0.9439569115638733
Epoch 820, training loss: 12.712443351745605 = 0.12940874695777893 + 2.0 * 6.29151725769043
Epoch 820, val loss: 0.9514026641845703
Epoch 830, training loss: 12.70034122467041 = 0.12433996796607971 + 2.0 * 6.288000583648682
Epoch 830, val loss: 0.958994448184967
Epoch 840, training loss: 12.693314552307129 = 0.11948862671852112 + 2.0 * 6.28691291809082
Epoch 840, val loss: 0.966625988483429
Epoch 850, training loss: 12.687607765197754 = 0.11484351009130478 + 2.0 * 6.28638219833374
Epoch 850, val loss: 0.9743537902832031
Epoch 860, training loss: 12.695140838623047 = 0.11042709648609161 + 2.0 * 6.292356967926025
Epoch 860, val loss: 0.9821040630340576
Epoch 870, training loss: 12.682440757751465 = 0.10623537003993988 + 2.0 * 6.288102626800537
Epoch 870, val loss: 0.9898588061332703
Epoch 880, training loss: 12.671940803527832 = 0.10225188732147217 + 2.0 * 6.284844398498535
Epoch 880, val loss: 0.9977779984474182
Epoch 890, training loss: 12.678988456726074 = 0.09842519462108612 + 2.0 * 6.290281772613525
Epoch 890, val loss: 1.0056453943252563
Epoch 900, training loss: 12.671797752380371 = 0.09481778740882874 + 2.0 * 6.288489818572998
Epoch 900, val loss: 1.0133578777313232
Epoch 910, training loss: 12.653895378112793 = 0.09137140959501266 + 2.0 * 6.281261920928955
Epoch 910, val loss: 1.0213083028793335
Epoch 920, training loss: 12.649812698364258 = 0.0880807638168335 + 2.0 * 6.2808661460876465
Epoch 920, val loss: 1.0292478799819946
Epoch 930, training loss: 12.646576881408691 = 0.08493123203516006 + 2.0 * 6.28082275390625
Epoch 930, val loss: 1.0371835231781006
Epoch 940, training loss: 12.642620086669922 = 0.08191481977701187 + 2.0 * 6.280352592468262
Epoch 940, val loss: 1.0450196266174316
Epoch 950, training loss: 12.63781452178955 = 0.07904861122369766 + 2.0 * 6.279383182525635
Epoch 950, val loss: 1.052944540977478
Epoch 960, training loss: 12.630969047546387 = 0.07630715519189835 + 2.0 * 6.2773308753967285
Epoch 960, val loss: 1.0608490705490112
Epoch 970, training loss: 12.636197090148926 = 0.07367973029613495 + 2.0 * 6.281258583068848
Epoch 970, val loss: 1.0687235593795776
Epoch 980, training loss: 12.626327514648438 = 0.07116713374853134 + 2.0 * 6.277580261230469
Epoch 980, val loss: 1.0764678716659546
Epoch 990, training loss: 12.62307357788086 = 0.06877041608095169 + 2.0 * 6.277151584625244
Epoch 990, val loss: 1.0843257904052734
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7555555555555555
0.8075909330521878
=== training gcn model ===
Epoch 0, training loss: 19.141307830810547 = 1.947615623474121 + 2.0 * 8.596845626831055
Epoch 0, val loss: 1.945815920829773
Epoch 10, training loss: 19.130619049072266 = 1.937661051750183 + 2.0 * 8.596479415893555
Epoch 10, val loss: 1.9364956617355347
Epoch 20, training loss: 19.111234664916992 = 1.924916386604309 + 2.0 * 8.593158721923828
Epoch 20, val loss: 1.9241549968719482
Epoch 30, training loss: 19.044933319091797 = 1.9070945978164673 + 2.0 * 8.56891918182373
Epoch 30, val loss: 1.9064977169036865
Epoch 40, training loss: 18.76649284362793 = 1.884846568107605 + 2.0 * 8.440823554992676
Epoch 40, val loss: 1.8850044012069702
Epoch 50, training loss: 17.908994674682617 = 1.860133409500122 + 2.0 * 8.024430274963379
Epoch 50, val loss: 1.8614425659179688
Epoch 60, training loss: 17.459293365478516 = 1.8373737335205078 + 2.0 * 7.810959339141846
Epoch 60, val loss: 1.840948462486267
Epoch 70, training loss: 16.790996551513672 = 1.8207534551620483 + 2.0 * 7.485121250152588
Epoch 70, val loss: 1.8258475065231323
Epoch 80, training loss: 16.013683319091797 = 1.8095927238464355 + 2.0 * 7.10204553604126
Epoch 80, val loss: 1.8152661323547363
Epoch 90, training loss: 15.633910179138184 = 1.8011037111282349 + 2.0 * 6.916403293609619
Epoch 90, val loss: 1.8056598901748657
Epoch 100, training loss: 15.404982566833496 = 1.7905462980270386 + 2.0 * 6.807218074798584
Epoch 100, val loss: 1.794724464416504
Epoch 110, training loss: 15.239217758178711 = 1.7793517112731934 + 2.0 * 6.729933261871338
Epoch 110, val loss: 1.784006118774414
Epoch 120, training loss: 15.100281715393066 = 1.7688267230987549 + 2.0 * 6.665727615356445
Epoch 120, val loss: 1.7740052938461304
Epoch 130, training loss: 14.986466407775879 = 1.7589839696884155 + 2.0 * 6.613741397857666
Epoch 130, val loss: 1.7643463611602783
Epoch 140, training loss: 14.900846481323242 = 1.7484581470489502 + 2.0 * 6.5761942863464355
Epoch 140, val loss: 1.7541776895523071
Epoch 150, training loss: 14.836840629577637 = 1.7362209558486938 + 2.0 * 6.550309658050537
Epoch 150, val loss: 1.7429944276809692
Epoch 160, training loss: 14.779623031616211 = 1.7222075462341309 + 2.0 * 6.528707981109619
Epoch 160, val loss: 1.7307063341140747
Epoch 170, training loss: 14.722620010375977 = 1.7067439556121826 + 2.0 * 6.507937908172607
Epoch 170, val loss: 1.7173835039138794
Epoch 180, training loss: 14.676881790161133 = 1.689673900604248 + 2.0 * 6.493603706359863
Epoch 180, val loss: 1.7028682231903076
Epoch 190, training loss: 14.611865043640137 = 1.670776605606079 + 2.0 * 6.470544338226318
Epoch 190, val loss: 1.6868647336959839
Epoch 200, training loss: 14.559101104736328 = 1.6496273279190063 + 2.0 * 6.454736709594727
Epoch 200, val loss: 1.6690187454223633
Epoch 210, training loss: 14.508798599243164 = 1.6258177757263184 + 2.0 * 6.441490650177002
Epoch 210, val loss: 1.6490708589553833
Epoch 220, training loss: 14.466605186462402 = 1.5991779565811157 + 2.0 * 6.433713436126709
Epoch 220, val loss: 1.6269259452819824
Epoch 230, training loss: 14.411248207092285 = 1.5700849294662476 + 2.0 * 6.420581817626953
Epoch 230, val loss: 1.6029183864593506
Epoch 240, training loss: 14.363515853881836 = 1.5384348630905151 + 2.0 * 6.412540435791016
Epoch 240, val loss: 1.5769522190093994
Epoch 250, training loss: 14.31405258178711 = 1.5041723251342773 + 2.0 * 6.404940128326416
Epoch 250, val loss: 1.5491175651550293
Epoch 260, training loss: 14.271835327148438 = 1.4677103757858276 + 2.0 * 6.40206241607666
Epoch 260, val loss: 1.5197112560272217
Epoch 270, training loss: 14.218667984008789 = 1.4300992488861084 + 2.0 * 6.394284248352051
Epoch 270, val loss: 1.4897395372390747
Epoch 280, training loss: 14.166679382324219 = 1.3919659852981567 + 2.0 * 6.387356758117676
Epoch 280, val loss: 1.4596890211105347
Epoch 290, training loss: 14.117323875427246 = 1.3530181646347046 + 2.0 * 6.382153034210205
Epoch 290, val loss: 1.4296013116836548
Epoch 300, training loss: 14.069438934326172 = 1.3135418891906738 + 2.0 * 6.37794828414917
Epoch 300, val loss: 1.3998327255249023
Epoch 310, training loss: 14.02580451965332 = 1.2749123573303223 + 2.0 * 6.375446319580078
Epoch 310, val loss: 1.371101975440979
Epoch 320, training loss: 13.973880767822266 = 1.2368929386138916 + 2.0 * 6.368494033813477
Epoch 320, val loss: 1.3436071872711182
Epoch 330, training loss: 13.926237106323242 = 1.19963538646698 + 2.0 * 6.363300800323486
Epoch 330, val loss: 1.3172770738601685
Epoch 340, training loss: 13.903326988220215 = 1.1631457805633545 + 2.0 * 6.370090484619141
Epoch 340, val loss: 1.2919756174087524
Epoch 350, training loss: 13.845579147338867 = 1.1283202171325684 + 2.0 * 6.35862922668457
Epoch 350, val loss: 1.2682007551193237
Epoch 360, training loss: 13.80070686340332 = 1.0949056148529053 + 2.0 * 6.352900505065918
Epoch 360, val loss: 1.2457855939865112
Epoch 370, training loss: 13.75822639465332 = 1.0624747276306152 + 2.0 * 6.347875595092773
Epoch 370, val loss: 1.2242876291275024
Epoch 380, training loss: 13.720490455627441 = 1.030957579612732 + 2.0 * 6.344766616821289
Epoch 380, val loss: 1.2034716606140137
Epoch 390, training loss: 13.684494018554688 = 1.0003511905670166 + 2.0 * 6.342071533203125
Epoch 390, val loss: 1.1835076808929443
Epoch 400, training loss: 13.646326065063477 = 0.9708897471427917 + 2.0 * 6.3377180099487305
Epoch 400, val loss: 1.1641201972961426
Epoch 410, training loss: 13.610678672790527 = 0.9419746398925781 + 2.0 * 6.334352016448975
Epoch 410, val loss: 1.1452733278274536
Epoch 420, training loss: 13.576371192932129 = 0.9134253263473511 + 2.0 * 6.331472873687744
Epoch 420, val loss: 1.1266655921936035
Epoch 430, training loss: 13.544154167175293 = 0.8851721286773682 + 2.0 * 6.329491138458252
Epoch 430, val loss: 1.1084010601043701
Epoch 440, training loss: 13.511964797973633 = 0.8576074242591858 + 2.0 * 6.327178478240967
Epoch 440, val loss: 1.0904964208602905
Epoch 450, training loss: 13.478128433227539 = 0.8303489685058594 + 2.0 * 6.32388973236084
Epoch 450, val loss: 1.0728893280029297
Epoch 460, training loss: 13.45378303527832 = 0.803299605846405 + 2.0 * 6.325241565704346
Epoch 460, val loss: 1.055633783340454
Epoch 470, training loss: 13.417550086975098 = 0.7769917249679565 + 2.0 * 6.320279121398926
Epoch 470, val loss: 1.0390539169311523
Epoch 480, training loss: 13.384552955627441 = 0.751221776008606 + 2.0 * 6.3166656494140625
Epoch 480, val loss: 1.023207187652588
Epoch 490, training loss: 13.353411674499512 = 0.7260381579399109 + 2.0 * 6.313686847686768
Epoch 490, val loss: 1.00808584690094
Epoch 500, training loss: 13.35213851928711 = 0.7014855146408081 + 2.0 * 6.325326442718506
Epoch 500, val loss: 0.9938645958900452
Epoch 510, training loss: 13.305463790893555 = 0.6782175898551941 + 2.0 * 6.313622951507568
Epoch 510, val loss: 0.9808381199836731
Epoch 520, training loss: 13.27318286895752 = 0.6558312773704529 + 2.0 * 6.308675765991211
Epoch 520, val loss: 0.969037652015686
Epoch 530, training loss: 13.246281623840332 = 0.634195864200592 + 2.0 * 6.306042671203613
Epoch 530, val loss: 0.9582850933074951
Epoch 540, training loss: 13.232393264770508 = 0.6133198142051697 + 2.0 * 6.309536933898926
Epoch 540, val loss: 0.9486016035079956
Epoch 550, training loss: 13.1998872756958 = 0.5934246182441711 + 2.0 * 6.303231239318848
Epoch 550, val loss: 0.9400860071182251
Epoch 560, training loss: 13.174373626708984 = 0.5742608308792114 + 2.0 * 6.300056457519531
Epoch 560, val loss: 0.9326183199882507
Epoch 570, training loss: 13.168122291564941 = 0.555787980556488 + 2.0 * 6.306167125701904
Epoch 570, val loss: 0.9261101484298706
Epoch 580, training loss: 13.136115074157715 = 0.5380900502204895 + 2.0 * 6.299012660980225
Epoch 580, val loss: 0.9206815958023071
Epoch 590, training loss: 13.113239288330078 = 0.5210003852844238 + 2.0 * 6.296119689941406
Epoch 590, val loss: 0.9161294102668762
Epoch 600, training loss: 13.101303100585938 = 0.5044457316398621 + 2.0 * 6.298428535461426
Epoch 600, val loss: 0.9124237895011902
Epoch 610, training loss: 13.075772285461426 = 0.4884893596172333 + 2.0 * 6.293641567230225
Epoch 610, val loss: 0.9094985127449036
Epoch 620, training loss: 13.056700706481934 = 0.4730111360549927 + 2.0 * 6.291844844818115
Epoch 620, val loss: 0.907374918460846
Epoch 630, training loss: 13.039380073547363 = 0.45809406042099 + 2.0 * 6.290643215179443
Epoch 630, val loss: 0.9059425592422485
Epoch 640, training loss: 13.020005226135254 = 0.4437313973903656 + 2.0 * 6.288136959075928
Epoch 640, val loss: 0.9051782488822937
Epoch 650, training loss: 13.004136085510254 = 0.4297894537448883 + 2.0 * 6.287173271179199
Epoch 650, val loss: 0.9051113128662109
Epoch 660, training loss: 13.004322052001953 = 0.41633573174476624 + 2.0 * 6.29399299621582
Epoch 660, val loss: 0.9056572318077087
Epoch 670, training loss: 12.97370433807373 = 0.4034086763858795 + 2.0 * 6.285147666931152
Epoch 670, val loss: 0.9067709445953369
Epoch 680, training loss: 12.958837509155273 = 0.3909834325313568 + 2.0 * 6.283926963806152
Epoch 680, val loss: 0.9083979725837708
Epoch 690, training loss: 12.944918632507324 = 0.37895911931991577 + 2.0 * 6.282979965209961
Epoch 690, val loss: 0.9105333685874939
Epoch 700, training loss: 12.947020530700684 = 0.3673269748687744 + 2.0 * 6.289846897125244
Epoch 700, val loss: 0.9130774736404419
Epoch 710, training loss: 12.918256759643555 = 0.3562314212322235 + 2.0 * 6.281012535095215
Epoch 710, val loss: 0.9161558747291565
Epoch 720, training loss: 12.905285835266113 = 0.3455049395561218 + 2.0 * 6.279890537261963
Epoch 720, val loss: 0.9195489287376404
Epoch 730, training loss: 12.891640663146973 = 0.33508116006851196 + 2.0 * 6.278279781341553
Epoch 730, val loss: 0.9233545064926147
Epoch 740, training loss: 12.8915433883667 = 0.3249773383140564 + 2.0 * 6.283283233642578
Epoch 740, val loss: 0.9274009466171265
Epoch 750, training loss: 12.866486549377441 = 0.31526947021484375 + 2.0 * 6.275608539581299
Epoch 750, val loss: 0.931837260723114
Epoch 760, training loss: 12.857548713684082 = 0.30579859018325806 + 2.0 * 6.275875091552734
Epoch 760, val loss: 0.9365288615226746
Epoch 770, training loss: 12.851594924926758 = 0.2965225875377655 + 2.0 * 6.277536392211914
Epoch 770, val loss: 0.941402018070221
Epoch 780, training loss: 12.836091041564941 = 0.2874640226364136 + 2.0 * 6.274313449859619
Epoch 780, val loss: 0.9463719725608826
Epoch 790, training loss: 12.828933715820312 = 0.2786198556423187 + 2.0 * 6.2751569747924805
Epoch 790, val loss: 0.9516282677650452
Epoch 800, training loss: 12.815345764160156 = 0.2698758542537689 + 2.0 * 6.272735118865967
Epoch 800, val loss: 0.957003653049469
Epoch 810, training loss: 12.80312442779541 = 0.26122137904167175 + 2.0 * 6.270951747894287
Epoch 810, val loss: 0.9625406861305237
Epoch 820, training loss: 12.801552772521973 = 0.25258669257164 + 2.0 * 6.2744832038879395
Epoch 820, val loss: 0.9680994153022766
Epoch 830, training loss: 12.791342735290527 = 0.24401719868183136 + 2.0 * 6.273662567138672
Epoch 830, val loss: 0.9737721681594849
Epoch 840, training loss: 12.772359848022461 = 0.2354392558336258 + 2.0 * 6.268460273742676
Epoch 840, val loss: 0.9795553684234619
Epoch 850, training loss: 12.763505935668945 = 0.22682669758796692 + 2.0 * 6.26833963394165
Epoch 850, val loss: 0.985434889793396
Epoch 860, training loss: 12.76723861694336 = 0.21816836297512054 + 2.0 * 6.274535179138184
Epoch 860, val loss: 0.9913820624351501
Epoch 870, training loss: 12.754582405090332 = 0.20956802368164062 + 2.0 * 6.272507190704346
Epoch 870, val loss: 0.9974992275238037
Epoch 880, training loss: 12.733182907104492 = 0.20096948742866516 + 2.0 * 6.266106605529785
Epoch 880, val loss: 1.0036933422088623
Epoch 890, training loss: 12.723722457885742 = 0.19244670867919922 + 2.0 * 6.2656378746032715
Epoch 890, val loss: 1.0100631713867188
Epoch 900, training loss: 12.728614807128906 = 0.1840563863515854 + 2.0 * 6.272279262542725
Epoch 900, val loss: 1.0165914297103882
Epoch 910, training loss: 12.708054542541504 = 0.17587853968143463 + 2.0 * 6.266088008880615
Epoch 910, val loss: 1.0234380960464478
Epoch 920, training loss: 12.705976486206055 = 0.16793705523014069 + 2.0 * 6.269019603729248
Epoch 920, val loss: 1.0305081605911255
Epoch 930, training loss: 12.69112777709961 = 0.16028307378292084 + 2.0 * 6.265422344207764
Epoch 930, val loss: 1.0377042293548584
Epoch 940, training loss: 12.678813934326172 = 0.1529725193977356 + 2.0 * 6.26292085647583
Epoch 940, val loss: 1.0452654361724854
Epoch 950, training loss: 12.669495582580566 = 0.14595425128936768 + 2.0 * 6.261770725250244
Epoch 950, val loss: 1.053066372871399
Epoch 960, training loss: 12.670626640319824 = 0.139261394739151 + 2.0 * 6.265682697296143
Epoch 960, val loss: 1.0610203742980957
Epoch 970, training loss: 12.662115097045898 = 0.13295558094978333 + 2.0 * 6.264579772949219
Epoch 970, val loss: 1.0691004991531372
Epoch 980, training loss: 12.647214889526367 = 0.12698811292648315 + 2.0 * 6.26011323928833
Epoch 980, val loss: 1.0774340629577637
Epoch 990, training loss: 12.640181541442871 = 0.12132835388183594 + 2.0 * 6.259426593780518
Epoch 990, val loss: 1.085936427116394
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.737037037037037
0.8170795993674222
=== training gcn model ===
Epoch 0, training loss: 19.142414093017578 = 1.948853611946106 + 2.0 * 8.596779823303223
Epoch 0, val loss: 1.939518690109253
Epoch 10, training loss: 19.130809783935547 = 1.9387775659561157 + 2.0 * 8.596015930175781
Epoch 10, val loss: 1.9300638437271118
Epoch 20, training loss: 19.10487174987793 = 1.9258520603179932 + 2.0 * 8.589509963989258
Epoch 20, val loss: 1.917593240737915
Epoch 30, training loss: 18.99152946472168 = 1.9082622528076172 + 2.0 * 8.541633605957031
Epoch 30, val loss: 1.9006648063659668
Epoch 40, training loss: 18.264053344726562 = 1.8885842561721802 + 2.0 * 8.187734603881836
Epoch 40, val loss: 1.88234281539917
Epoch 50, training loss: 16.871309280395508 = 1.868977427482605 + 2.0 * 7.501165866851807
Epoch 50, val loss: 1.8648736476898193
Epoch 60, training loss: 16.268352508544922 = 1.8549696207046509 + 2.0 * 7.206691265106201
Epoch 60, val loss: 1.8526393175125122
Epoch 70, training loss: 15.858577728271484 = 1.8423961400985718 + 2.0 * 7.008090972900391
Epoch 70, val loss: 1.8412576913833618
Epoch 80, training loss: 15.560585021972656 = 1.8311963081359863 + 2.0 * 6.864694118499756
Epoch 80, val loss: 1.8310126066207886
Epoch 90, training loss: 15.351887702941895 = 1.8202112913131714 + 2.0 * 6.765838146209717
Epoch 90, val loss: 1.8205844163894653
Epoch 100, training loss: 15.21399211883545 = 1.8095124959945679 + 2.0 * 6.702239990234375
Epoch 100, val loss: 1.8104301691055298
Epoch 110, training loss: 15.099639892578125 = 1.799285888671875 + 2.0 * 6.650177001953125
Epoch 110, val loss: 1.8008997440338135
Epoch 120, training loss: 15.000967979431152 = 1.789825201034546 + 2.0 * 6.605571269989014
Epoch 120, val loss: 1.7921473979949951
Epoch 130, training loss: 14.917469024658203 = 1.780504822731018 + 2.0 * 6.568481922149658
Epoch 130, val loss: 1.7833856344223022
Epoch 140, training loss: 14.848554611206055 = 1.7705594301223755 + 2.0 * 6.538997650146484
Epoch 140, val loss: 1.774280071258545
Epoch 150, training loss: 14.792340278625488 = 1.759637713432312 + 2.0 * 6.516351222991943
Epoch 150, val loss: 1.76458740234375
Epoch 160, training loss: 14.74583911895752 = 1.747484803199768 + 2.0 * 6.499176979064941
Epoch 160, val loss: 1.7540221214294434
Epoch 170, training loss: 14.703157424926758 = 1.7338653802871704 + 2.0 * 6.484645843505859
Epoch 170, val loss: 1.7424346208572388
Epoch 180, training loss: 14.66004753112793 = 1.7185702323913574 + 2.0 * 6.470738410949707
Epoch 180, val loss: 1.7295174598693848
Epoch 190, training loss: 14.619087219238281 = 1.701246976852417 + 2.0 * 6.458920001983643
Epoch 190, val loss: 1.7148940563201904
Epoch 200, training loss: 14.581324577331543 = 1.6814144849777222 + 2.0 * 6.449954986572266
Epoch 200, val loss: 1.6982251405715942
Epoch 210, training loss: 14.537574768066406 = 1.6586931943893433 + 2.0 * 6.439440727233887
Epoch 210, val loss: 1.6793543100357056
Epoch 220, training loss: 14.489888191223145 = 1.633051872253418 + 2.0 * 6.428418159484863
Epoch 220, val loss: 1.657876968383789
Epoch 230, training loss: 14.453841209411621 = 1.6038053035736084 + 2.0 * 6.425017833709717
Epoch 230, val loss: 1.6335082054138184
Epoch 240, training loss: 14.390865325927734 = 1.5713059902191162 + 2.0 * 6.4097795486450195
Epoch 240, val loss: 1.6063973903656006
Epoch 250, training loss: 14.339801788330078 = 1.5352833271026611 + 2.0 * 6.402259349822998
Epoch 250, val loss: 1.576385259628296
Epoch 260, training loss: 14.292245864868164 = 1.4958404302597046 + 2.0 * 6.398202896118164
Epoch 260, val loss: 1.5436607599258423
Epoch 270, training loss: 14.22943115234375 = 1.4539300203323364 + 2.0 * 6.387750625610352
Epoch 270, val loss: 1.5090665817260742
Epoch 280, training loss: 14.17194938659668 = 1.4097691774368286 + 2.0 * 6.38109016418457
Epoch 280, val loss: 1.472959280014038
Epoch 290, training loss: 14.115275382995605 = 1.3640755414962769 + 2.0 * 6.3755998611450195
Epoch 290, val loss: 1.4361106157302856
Epoch 300, training loss: 14.058222770690918 = 1.3178365230560303 + 2.0 * 6.370193004608154
Epoch 300, val loss: 1.3992685079574585
Epoch 310, training loss: 14.001665115356445 = 1.2715398073196411 + 2.0 * 6.365062713623047
Epoch 310, val loss: 1.362945556640625
Epoch 320, training loss: 13.948892593383789 = 1.2254372835159302 + 2.0 * 6.361727714538574
Epoch 320, val loss: 1.3276201486587524
Epoch 330, training loss: 13.894176483154297 = 1.180057168006897 + 2.0 * 6.357059478759766
Epoch 330, val loss: 1.2933754920959473
Epoch 340, training loss: 13.838642120361328 = 1.1351114511489868 + 2.0 * 6.351765155792236
Epoch 340, val loss: 1.2602206468582153
Epoch 350, training loss: 13.78977108001709 = 1.0907889604568481 + 2.0 * 6.349491119384766
Epoch 350, val loss: 1.2281574010849
Epoch 360, training loss: 13.738665580749512 = 1.0475519895553589 + 2.0 * 6.345556735992432
Epoch 360, val loss: 1.1973012685775757
Epoch 370, training loss: 13.689724922180176 = 1.005198359489441 + 2.0 * 6.342263221740723
Epoch 370, val loss: 1.1675753593444824
Epoch 380, training loss: 13.646791458129883 = 0.9637230038642883 + 2.0 * 6.34153413772583
Epoch 380, val loss: 1.1389786005020142
Epoch 390, training loss: 13.598031997680664 = 0.9233304858207703 + 2.0 * 6.337350845336914
Epoch 390, val loss: 1.1117123365402222
Epoch 400, training loss: 13.553215980529785 = 0.8842899799346924 + 2.0 * 6.334463119506836
Epoch 400, val loss: 1.0857900381088257
Epoch 410, training loss: 13.518694877624512 = 0.846944272518158 + 2.0 * 6.335875511169434
Epoch 410, val loss: 1.0615756511688232
Epoch 420, training loss: 13.47001838684082 = 0.8116373419761658 + 2.0 * 6.329190731048584
Epoch 420, val loss: 1.039405107498169
Epoch 430, training loss: 13.43210220336914 = 0.7781897187232971 + 2.0 * 6.326956272125244
Epoch 430, val loss: 1.019179344177246
Epoch 440, training loss: 13.398059844970703 = 0.7467230558395386 + 2.0 * 6.3256683349609375
Epoch 440, val loss: 1.00102961063385
Epoch 450, training loss: 13.362369537353516 = 0.717592716217041 + 2.0 * 6.322388648986816
Epoch 450, val loss: 0.9848877191543579
Epoch 460, training loss: 13.330129623413086 = 0.6902549862861633 + 2.0 * 6.319937229156494
Epoch 460, val loss: 0.9707756042480469
Epoch 470, training loss: 13.29935073852539 = 0.6644721031188965 + 2.0 * 6.317439556121826
Epoch 470, val loss: 0.9583474397659302
Epoch 480, training loss: 13.29268741607666 = 0.6401296854019165 + 2.0 * 6.3262786865234375
Epoch 480, val loss: 0.9473731517791748
Epoch 490, training loss: 13.249056816101074 = 0.6174709796905518 + 2.0 * 6.315793037414551
Epoch 490, val loss: 0.9382141828536987
Epoch 500, training loss: 13.222254753112793 = 0.5961759686470032 + 2.0 * 6.313039302825928
Epoch 500, val loss: 0.9303336143493652
Epoch 510, training loss: 13.196512222290039 = 0.5759072303771973 + 2.0 * 6.310302257537842
Epoch 510, val loss: 0.9236846566200256
Epoch 520, training loss: 13.19682502746582 = 0.5565171241760254 + 2.0 * 6.320154190063477
Epoch 520, val loss: 0.9180083870887756
Epoch 530, training loss: 13.152868270874023 = 0.5382747054100037 + 2.0 * 6.3072967529296875
Epoch 530, val loss: 0.9130342602729797
Epoch 540, training loss: 13.132661819458008 = 0.5207727551460266 + 2.0 * 6.305944442749023
Epoch 540, val loss: 0.9090251922607422
Epoch 550, training loss: 13.111054420471191 = 0.5039247870445251 + 2.0 * 6.30356502532959
Epoch 550, val loss: 0.9058032631874084
Epoch 560, training loss: 13.09582805633545 = 0.4877392649650574 + 2.0 * 6.304044246673584
Epoch 560, val loss: 0.9031983613967896
Epoch 570, training loss: 13.079983711242676 = 0.4723108112812042 + 2.0 * 6.303836345672607
Epoch 570, val loss: 0.9011757969856262
Epoch 580, training loss: 13.057419776916504 = 0.4574526250362396 + 2.0 * 6.299983501434326
Epoch 580, val loss: 0.8999460339546204
Epoch 590, training loss: 13.037753105163574 = 0.44311511516571045 + 2.0 * 6.297318935394287
Epoch 590, val loss: 0.8991591930389404
Epoch 600, training loss: 13.031982421875 = 0.4292115271091461 + 2.0 * 6.301385402679443
Epoch 600, val loss: 0.8989452123641968
Epoch 610, training loss: 13.015068054199219 = 0.41586267948150635 + 2.0 * 6.299602508544922
Epoch 610, val loss: 0.8991401791572571
Epoch 620, training loss: 12.994293212890625 = 0.40300223231315613 + 2.0 * 6.295645713806152
Epoch 620, val loss: 0.8998495936393738
Epoch 630, training loss: 12.976892471313477 = 0.3905520439147949 + 2.0 * 6.29317045211792
Epoch 630, val loss: 0.9010385870933533
Epoch 640, training loss: 12.96059799194336 = 0.37849506735801697 + 2.0 * 6.291051387786865
Epoch 640, val loss: 0.9025994539260864
Epoch 650, training loss: 12.961990356445312 = 0.36670106649398804 + 2.0 * 6.29764461517334
Epoch 650, val loss: 0.9046693444252014
Epoch 660, training loss: 12.938855171203613 = 0.3553287982940674 + 2.0 * 6.2917633056640625
Epoch 660, val loss: 0.9069074392318726
Epoch 670, training loss: 12.921857833862305 = 0.3440934717655182 + 2.0 * 6.288882255554199
Epoch 670, val loss: 0.9096102714538574
Epoch 680, training loss: 12.90796947479248 = 0.33307257294654846 + 2.0 * 6.287448406219482
Epoch 680, val loss: 0.912571907043457
Epoch 690, training loss: 12.907517433166504 = 0.32212090492248535 + 2.0 * 6.292698383331299
Epoch 690, val loss: 0.9158751368522644
Epoch 700, training loss: 12.88386344909668 = 0.3113386034965515 + 2.0 * 6.286262512207031
Epoch 700, val loss: 0.9194655418395996
Epoch 710, training loss: 12.86984920501709 = 0.3006373345851898 + 2.0 * 6.284605979919434
Epoch 710, val loss: 0.923362135887146
Epoch 720, training loss: 12.856730461120605 = 0.28999534249305725 + 2.0 * 6.28336763381958
Epoch 720, val loss: 0.9275504350662231
Epoch 730, training loss: 12.84508991241455 = 0.2793707251548767 + 2.0 * 6.282859802246094
Epoch 730, val loss: 0.9320090413093567
Epoch 740, training loss: 12.85012435913086 = 0.2687997817993164 + 2.0 * 6.2906622886657715
Epoch 740, val loss: 0.9366631507873535
Epoch 750, training loss: 12.827194213867188 = 0.2585028111934662 + 2.0 * 6.284345626831055
Epoch 750, val loss: 0.9417135715484619
Epoch 760, training loss: 12.805425643920898 = 0.24830585718154907 + 2.0 * 6.278559684753418
Epoch 760, val loss: 0.9471017122268677
Epoch 770, training loss: 12.794090270996094 = 0.23830144107341766 + 2.0 * 6.277894496917725
Epoch 770, val loss: 0.9528340697288513
Epoch 780, training loss: 12.807446479797363 = 0.22851650416851044 + 2.0 * 6.289464950561523
Epoch 780, val loss: 0.9590648412704468
Epoch 790, training loss: 12.774513244628906 = 0.2190907746553421 + 2.0 * 6.277711391448975
Epoch 790, val loss: 0.9654529690742493
Epoch 800, training loss: 12.763473510742188 = 0.21001854538917542 + 2.0 * 6.276727676391602
Epoch 800, val loss: 0.9723606705665588
Epoch 810, training loss: 12.753172874450684 = 0.20127904415130615 + 2.0 * 6.275947093963623
Epoch 810, val loss: 0.9796191453933716
Epoch 820, training loss: 12.742621421813965 = 0.19289550185203552 + 2.0 * 6.274862766265869
Epoch 820, val loss: 0.9871295094490051
Epoch 830, training loss: 12.73489761352539 = 0.18490949273109436 + 2.0 * 6.274993896484375
Epoch 830, val loss: 0.9949389696121216
Epoch 840, training loss: 12.730578422546387 = 0.17728720605373383 + 2.0 * 6.276645660400391
Epoch 840, val loss: 1.0029690265655518
Epoch 850, training loss: 12.71375560760498 = 0.16999298334121704 + 2.0 * 6.271881103515625
Epoch 850, val loss: 1.011235237121582
Epoch 860, training loss: 12.706310272216797 = 0.16304215788841248 + 2.0 * 6.271634101867676
Epoch 860, val loss: 1.0197311639785767
Epoch 870, training loss: 12.69860553741455 = 0.1563963145017624 + 2.0 * 6.27110481262207
Epoch 870, val loss: 1.0284913778305054
Epoch 880, training loss: 12.702903747558594 = 0.15008611977100372 + 2.0 * 6.276408672332764
Epoch 880, val loss: 1.0373704433441162
Epoch 890, training loss: 12.686251640319824 = 0.14408086240291595 + 2.0 * 6.271085262298584
Epoch 890, val loss: 1.0463120937347412
Epoch 900, training loss: 12.675041198730469 = 0.1383935958147049 + 2.0 * 6.26832389831543
Epoch 900, val loss: 1.0554401874542236
Epoch 910, training loss: 12.668601989746094 = 0.13295520842075348 + 2.0 * 6.267823219299316
Epoch 910, val loss: 1.0647722482681274
Epoch 920, training loss: 12.668651580810547 = 0.12776154279708862 + 2.0 * 6.270444869995117
Epoch 920, val loss: 1.0740991830825806
Epoch 930, training loss: 12.65417194366455 = 0.12283182889223099 + 2.0 * 6.265669822692871
Epoch 930, val loss: 1.083514928817749
Epoch 940, training loss: 12.64865493774414 = 0.1181301474571228 + 2.0 * 6.265262603759766
Epoch 940, val loss: 1.0930978059768677
Epoch 950, training loss: 12.6519193649292 = 0.1136215329170227 + 2.0 * 6.269148826599121
Epoch 950, val loss: 1.1026875972747803
Epoch 960, training loss: 12.63929271697998 = 0.10935888439416885 + 2.0 * 6.26496696472168
Epoch 960, val loss: 1.1122123003005981
Epoch 970, training loss: 12.63305950164795 = 0.10529226064682007 + 2.0 * 6.263883590698242
Epoch 970, val loss: 1.1219669580459595
Epoch 980, training loss: 12.632115364074707 = 0.10142504423856735 + 2.0 * 6.265345096588135
Epoch 980, val loss: 1.131631851196289
Epoch 990, training loss: 12.62338924407959 = 0.09771943837404251 + 2.0 * 6.2628350257873535
Epoch 990, val loss: 1.1412856578826904
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7407407407407408
0.8128624143384291
The final CL Acc:0.74444, 0.00800, The final GNN Acc:0.81251, 0.00388
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13252])
remove edge: torch.Size([2, 7984])
updated graph: torch.Size([2, 10680])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 19.125255584716797 = 1.931570053100586 + 2.0 * 8.596842765808105
Epoch 0, val loss: 1.9343808889389038
Epoch 10, training loss: 19.115690231323242 = 1.9226211309432983 + 2.0 * 8.596534729003906
Epoch 10, val loss: 1.92501962184906
Epoch 20, training loss: 19.099353790283203 = 1.9115784168243408 + 2.0 * 8.593887329101562
Epoch 20, val loss: 1.9133847951889038
Epoch 30, training loss: 19.03765106201172 = 1.8965283632278442 + 2.0 * 8.570561408996582
Epoch 30, val loss: 1.8975673913955688
Epoch 40, training loss: 18.675413131713867 = 1.8770920038223267 + 2.0 * 8.399160385131836
Epoch 40, val loss: 1.877702236175537
Epoch 50, training loss: 17.393722534179688 = 1.8555864095687866 + 2.0 * 7.769068241119385
Epoch 50, val loss: 1.8564614057540894
Epoch 60, training loss: 16.6530704498291 = 1.8376396894454956 + 2.0 * 7.407715320587158
Epoch 60, val loss: 1.8397119045257568
Epoch 70, training loss: 16.092632293701172 = 1.8231786489486694 + 2.0 * 7.134726524353027
Epoch 70, val loss: 1.8258466720581055
Epoch 80, training loss: 15.637017250061035 = 1.8110703229904175 + 2.0 * 6.912973403930664
Epoch 80, val loss: 1.81415855884552
Epoch 90, training loss: 15.36629867553711 = 1.8008503913879395 + 2.0 * 6.782723903656006
Epoch 90, val loss: 1.8042510747909546
Epoch 100, training loss: 15.207243919372559 = 1.7900402545928955 + 2.0 * 6.708601951599121
Epoch 100, val loss: 1.793994426727295
Epoch 110, training loss: 15.060098648071289 = 1.7788907289505005 + 2.0 * 6.640604019165039
Epoch 110, val loss: 1.7838354110717773
Epoch 120, training loss: 14.942144393920898 = 1.768097162246704 + 2.0 * 6.587023735046387
Epoch 120, val loss: 1.774314045906067
Epoch 130, training loss: 14.850725173950195 = 1.7563780546188354 + 2.0 * 6.547173500061035
Epoch 130, val loss: 1.7641854286193848
Epoch 140, training loss: 14.77840805053711 = 1.7432602643966675 + 2.0 * 6.517573833465576
Epoch 140, val loss: 1.7529083490371704
Epoch 150, training loss: 14.715764999389648 = 1.7286641597747803 + 2.0 * 6.4935503005981445
Epoch 150, val loss: 1.7406082153320312
Epoch 160, training loss: 14.652334213256836 = 1.712377905845642 + 2.0 * 6.469978332519531
Epoch 160, val loss: 1.726994514465332
Epoch 170, training loss: 14.600934982299805 = 1.693861722946167 + 2.0 * 6.453536510467529
Epoch 170, val loss: 1.7115330696105957
Epoch 180, training loss: 14.544447898864746 = 1.6726585626602173 + 2.0 * 6.43589448928833
Epoch 180, val loss: 1.6939185857772827
Epoch 190, training loss: 14.494583129882812 = 1.6485028266906738 + 2.0 * 6.423040390014648
Epoch 190, val loss: 1.6738593578338623
Epoch 200, training loss: 14.444129943847656 = 1.620907187461853 + 2.0 * 6.411611557006836
Epoch 200, val loss: 1.6509771347045898
Epoch 210, training loss: 14.402924537658691 = 1.5896282196044922 + 2.0 * 6.4066481590271
Epoch 210, val loss: 1.6251181364059448
Epoch 220, training loss: 14.343435287475586 = 1.5551211833953857 + 2.0 * 6.3941569328308105
Epoch 220, val loss: 1.596539855003357
Epoch 230, training loss: 14.289727210998535 = 1.5171641111373901 + 2.0 * 6.386281490325928
Epoch 230, val loss: 1.565178394317627
Epoch 240, training loss: 14.23282241821289 = 1.4757227897644043 + 2.0 * 6.378550052642822
Epoch 240, val loss: 1.5309498310089111
Epoch 250, training loss: 14.193486213684082 = 1.4312019348144531 + 2.0 * 6.3811421394348145
Epoch 250, val loss: 1.4941943883895874
Epoch 260, training loss: 14.120924949645996 = 1.3850810527801514 + 2.0 * 6.367921829223633
Epoch 260, val loss: 1.4566541910171509
Epoch 270, training loss: 14.06324577331543 = 1.3385030031204224 + 2.0 * 6.362371444702148
Epoch 270, val loss: 1.418690800666809
Epoch 280, training loss: 14.004486083984375 = 1.2915929555892944 + 2.0 * 6.356446743011475
Epoch 280, val loss: 1.3806068897247314
Epoch 290, training loss: 13.947977066040039 = 1.2448407411575317 + 2.0 * 6.351568222045898
Epoch 290, val loss: 1.3430134057998657
Epoch 300, training loss: 13.903148651123047 = 1.199031949043274 + 2.0 * 6.352058410644531
Epoch 300, val loss: 1.3063602447509766
Epoch 310, training loss: 13.843999862670898 = 1.1550105810165405 + 2.0 * 6.344494819641113
Epoch 310, val loss: 1.2716697454452515
Epoch 320, training loss: 13.794733047485352 = 1.113366723060608 + 2.0 * 6.3406829833984375
Epoch 320, val loss: 1.2389146089553833
Epoch 330, training loss: 13.74778938293457 = 1.0737721920013428 + 2.0 * 6.337008476257324
Epoch 330, val loss: 1.2081589698791504
Epoch 340, training loss: 13.70297908782959 = 1.036065936088562 + 2.0 * 6.333456516265869
Epoch 340, val loss: 1.179076075553894
Epoch 350, training loss: 13.678489685058594 = 1.0002878904342651 + 2.0 * 6.3391008377075195
Epoch 350, val loss: 1.1517257690429688
Epoch 360, training loss: 13.6282320022583 = 0.9666823744773865 + 2.0 * 6.330774784088135
Epoch 360, val loss: 1.126278281211853
Epoch 370, training loss: 13.587987899780273 = 0.9348353147506714 + 2.0 * 6.326576232910156
Epoch 370, val loss: 1.1023234128952026
Epoch 380, training loss: 13.548745155334473 = 0.9042249917984009 + 2.0 * 6.322259902954102
Epoch 380, val loss: 1.0794878005981445
Epoch 390, training loss: 13.525561332702637 = 0.8743981719017029 + 2.0 * 6.3255815505981445
Epoch 390, val loss: 1.0575107336044312
Epoch 400, training loss: 13.483919143676758 = 0.8454983234405518 + 2.0 * 6.319210529327393
Epoch 400, val loss: 1.0361870527267456
Epoch 410, training loss: 13.448933601379395 = 0.81711345911026 + 2.0 * 6.3159098625183105
Epoch 410, val loss: 1.0155316591262817
Epoch 420, training loss: 13.41474723815918 = 0.7889640927314758 + 2.0 * 6.312891483306885
Epoch 420, val loss: 0.9953718185424805
Epoch 430, training loss: 13.399579048156738 = 0.7609750628471375 + 2.0 * 6.319302082061768
Epoch 430, val loss: 0.9756854176521301
Epoch 440, training loss: 13.358933448791504 = 0.7335556745529175 + 2.0 * 6.312688827514648
Epoch 440, val loss: 0.9566390514373779
Epoch 450, training loss: 13.323297500610352 = 0.7064308524131775 + 2.0 * 6.308433532714844
Epoch 450, val loss: 0.938321590423584
Epoch 460, training loss: 13.290783882141113 = 0.6795350313186646 + 2.0 * 6.305624485015869
Epoch 460, val loss: 0.9205760359764099
Epoch 470, training loss: 13.262922286987305 = 0.6528230905532837 + 2.0 * 6.305049419403076
Epoch 470, val loss: 0.9035102128982544
Epoch 480, training loss: 13.251187324523926 = 0.6264206171035767 + 2.0 * 6.31238317489624
Epoch 480, val loss: 0.8872985243797302
Epoch 490, training loss: 13.201301574707031 = 0.6007721424102783 + 2.0 * 6.300264835357666
Epoch 490, val loss: 0.8720348477363586
Epoch 500, training loss: 13.172015190124512 = 0.5756639838218689 + 2.0 * 6.298175811767578
Epoch 500, val loss: 0.8577519655227661
Epoch 510, training loss: 13.144452095031738 = 0.5510585308074951 + 2.0 * 6.296696662902832
Epoch 510, val loss: 0.8443674445152283
Epoch 520, training loss: 13.131239891052246 = 0.5270031690597534 + 2.0 * 6.302118301391602
Epoch 520, val loss: 0.8318748474121094
Epoch 530, training loss: 13.095596313476562 = 0.503648042678833 + 2.0 * 6.295974254608154
Epoch 530, val loss: 0.8204007744789124
Epoch 540, training loss: 13.067377090454102 = 0.4811495542526245 + 2.0 * 6.293113708496094
Epoch 540, val loss: 0.8100016117095947
Epoch 550, training loss: 13.042445182800293 = 0.4592708647251129 + 2.0 * 6.2915873527526855
Epoch 550, val loss: 0.8005964159965515
Epoch 560, training loss: 13.018872261047363 = 0.438019335269928 + 2.0 * 6.290426254272461
Epoch 560, val loss: 0.7920820713043213
Epoch 570, training loss: 12.999452590942383 = 0.4175165593624115 + 2.0 * 6.29096794128418
Epoch 570, val loss: 0.784549355506897
Epoch 580, training loss: 12.971601486206055 = 0.3978244364261627 + 2.0 * 6.286888599395752
Epoch 580, val loss: 0.7779860496520996
Epoch 590, training loss: 12.949578285217285 = 0.3788372278213501 + 2.0 * 6.285370349884033
Epoch 590, val loss: 0.7722420692443848
Epoch 600, training loss: 12.933570861816406 = 0.36045122146606445 + 2.0 * 6.286559581756592
Epoch 600, val loss: 0.7672738432884216
Epoch 610, training loss: 12.909109115600586 = 0.34280678629875183 + 2.0 * 6.283151149749756
Epoch 610, val loss: 0.7630152106285095
Epoch 620, training loss: 12.889961242675781 = 0.32585740089416504 + 2.0 * 6.282052040100098
Epoch 620, val loss: 0.7594658732414246
Epoch 630, training loss: 12.873682975769043 = 0.30959585309028625 + 2.0 * 6.28204345703125
Epoch 630, val loss: 0.7565742135047913
Epoch 640, training loss: 12.85384464263916 = 0.2940101623535156 + 2.0 * 6.279917240142822
Epoch 640, val loss: 0.7543506622314453
Epoch 650, training loss: 12.840335845947266 = 0.2792260944843292 + 2.0 * 6.28055477142334
Epoch 650, val loss: 0.752699613571167
Epoch 660, training loss: 12.82367992401123 = 0.26506826281547546 + 2.0 * 6.279305934906006
Epoch 660, val loss: 0.7516436576843262
Epoch 670, training loss: 12.805606842041016 = 0.2516385018825531 + 2.0 * 6.276984214782715
Epoch 670, val loss: 0.7511053085327148
Epoch 680, training loss: 12.797567367553711 = 0.2389138787984848 + 2.0 * 6.279326915740967
Epoch 680, val loss: 0.751058042049408
Epoch 690, training loss: 12.77736759185791 = 0.22685259580612183 + 2.0 * 6.275257587432861
Epoch 690, val loss: 0.7515178322792053
Epoch 700, training loss: 12.766773223876953 = 0.2154790312051773 + 2.0 * 6.275647163391113
Epoch 700, val loss: 0.7524629831314087
Epoch 710, training loss: 12.752464294433594 = 0.20477908849716187 + 2.0 * 6.273842811584473
Epoch 710, val loss: 0.7539390921592712
Epoch 720, training loss: 12.739351272583008 = 0.1947207748889923 + 2.0 * 6.27231502532959
Epoch 720, val loss: 0.7557426691055298
Epoch 730, training loss: 12.729792594909668 = 0.18523408472537994 + 2.0 * 6.272279262542725
Epoch 730, val loss: 0.7580220103263855
Epoch 740, training loss: 12.714625358581543 = 0.17631107568740845 + 2.0 * 6.2691569328308105
Epoch 740, val loss: 0.760686457157135
Epoch 750, training loss: 12.717692375183105 = 0.16788756847381592 + 2.0 * 6.27490234375
Epoch 750, val loss: 0.7637192606925964
Epoch 760, training loss: 12.702864646911621 = 0.16003137826919556 + 2.0 * 6.271416664123535
Epoch 760, val loss: 0.7670424580574036
Epoch 770, training loss: 12.689985275268555 = 0.15267273783683777 + 2.0 * 6.268656253814697
Epoch 770, val loss: 0.7706881165504456
Epoch 780, training loss: 12.676645278930664 = 0.1457551121711731 + 2.0 * 6.265445232391357
Epoch 780, val loss: 0.7745933532714844
Epoch 790, training loss: 12.668450355529785 = 0.13921405375003815 + 2.0 * 6.264617919921875
Epoch 790, val loss: 0.7787668704986572
Epoch 800, training loss: 12.681509971618652 = 0.13304546475410461 + 2.0 * 6.274232387542725
Epoch 800, val loss: 0.7831878662109375
Epoch 810, training loss: 12.65915584564209 = 0.12725487351417542 + 2.0 * 6.265950679779053
Epoch 810, val loss: 0.7876670956611633
Epoch 820, training loss: 12.646397590637207 = 0.12181781232357025 + 2.0 * 6.262290000915527
Epoch 820, val loss: 0.792361319065094
Epoch 830, training loss: 12.639524459838867 = 0.11668406426906586 + 2.0 * 6.261420249938965
Epoch 830, val loss: 0.7972354888916016
Epoch 840, training loss: 12.64991283416748 = 0.11182834208011627 + 2.0 * 6.269042015075684
Epoch 840, val loss: 0.802226185798645
Epoch 850, training loss: 12.639946937561035 = 0.10724323242902756 + 2.0 * 6.266351699829102
Epoch 850, val loss: 0.8073223829269409
Epoch 860, training loss: 12.625286102294922 = 0.10293802618980408 + 2.0 * 6.261174201965332
Epoch 860, val loss: 0.8125055432319641
Epoch 870, training loss: 12.617786407470703 = 0.09886136651039124 + 2.0 * 6.259462356567383
Epoch 870, val loss: 0.8177672028541565
Epoch 880, training loss: 12.611316680908203 = 0.09499365836381912 + 2.0 * 6.258161544799805
Epoch 880, val loss: 0.8231492042541504
Epoch 890, training loss: 12.615790367126465 = 0.09131301939487457 + 2.0 * 6.262238502502441
Epoch 890, val loss: 0.8285591006278992
Epoch 900, training loss: 12.60269832611084 = 0.08784635365009308 + 2.0 * 6.257425785064697
Epoch 900, val loss: 0.833942711353302
Epoch 910, training loss: 12.59601879119873 = 0.08454397320747375 + 2.0 * 6.2557373046875
Epoch 910, val loss: 0.8394063115119934
Epoch 920, training loss: 12.600987434387207 = 0.08140208572149277 + 2.0 * 6.259792804718018
Epoch 920, val loss: 0.8449106812477112
Epoch 930, training loss: 12.593369483947754 = 0.07842449843883514 + 2.0 * 6.257472515106201
Epoch 930, val loss: 0.850436270236969
Epoch 940, training loss: 12.585094451904297 = 0.07559912651777267 + 2.0 * 6.2547478675842285
Epoch 940, val loss: 0.8559823036193848
Epoch 950, training loss: 12.5784273147583 = 0.07290590554475784 + 2.0 * 6.252760887145996
Epoch 950, val loss: 0.8615939617156982
Epoch 960, training loss: 12.579573631286621 = 0.07033482193946838 + 2.0 * 6.254619598388672
Epoch 960, val loss: 0.8672357797622681
Epoch 970, training loss: 12.571784973144531 = 0.0678810104727745 + 2.0 * 6.251952171325684
Epoch 970, val loss: 0.8728612065315247
Epoch 980, training loss: 12.571815490722656 = 0.06554103642702103 + 2.0 * 6.253137111663818
Epoch 980, val loss: 0.8784788250923157
Epoch 990, training loss: 12.568674087524414 = 0.06333278119564056 + 2.0 * 6.252670764923096
Epoch 990, val loss: 0.8840420246124268
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7740740740740741
0.8355297838692674
=== training gcn model ===
Epoch 0, training loss: 19.132104873657227 = 1.938456654548645 + 2.0 * 8.596823692321777
Epoch 0, val loss: 1.9355173110961914
Epoch 10, training loss: 19.121335983276367 = 1.9286240339279175 + 2.0 * 8.596356391906738
Epoch 10, val loss: 1.9262044429779053
Epoch 20, training loss: 19.100616455078125 = 1.9161806106567383 + 2.0 * 8.592217445373535
Epoch 20, val loss: 1.9141159057617188
Epoch 30, training loss: 19.011796951293945 = 1.8992888927459717 + 2.0 * 8.556254386901855
Epoch 30, val loss: 1.897680401802063
Epoch 40, training loss: 18.469226837158203 = 1.8786461353302002 + 2.0 * 8.295289993286133
Epoch 40, val loss: 1.8784115314483643
Epoch 50, training loss: 16.965600967407227 = 1.8572208881378174 + 2.0 * 7.554189682006836
Epoch 50, val loss: 1.858760118484497
Epoch 60, training loss: 16.3007755279541 = 1.8417543172836304 + 2.0 * 7.22951078414917
Epoch 60, val loss: 1.844744324684143
Epoch 70, training loss: 15.885637283325195 = 1.8299843072891235 + 2.0 * 7.027826309204102
Epoch 70, val loss: 1.8334853649139404
Epoch 80, training loss: 15.596110343933105 = 1.8192545175552368 + 2.0 * 6.888427734375
Epoch 80, val loss: 1.823514461517334
Epoch 90, training loss: 15.42004680633545 = 1.8090475797653198 + 2.0 * 6.80549955368042
Epoch 90, val loss: 1.8136460781097412
Epoch 100, training loss: 15.24946403503418 = 1.7976807355880737 + 2.0 * 6.725891590118408
Epoch 100, val loss: 1.8030059337615967
Epoch 110, training loss: 15.107088088989258 = 1.7867413759231567 + 2.0 * 6.660173416137695
Epoch 110, val loss: 1.7930548191070557
Epoch 120, training loss: 14.97894287109375 = 1.7768445014953613 + 2.0 * 6.601049423217773
Epoch 120, val loss: 1.7840373516082764
Epoch 130, training loss: 14.871055603027344 = 1.7667934894561768 + 2.0 * 6.552131175994873
Epoch 130, val loss: 1.7750003337860107
Epoch 140, training loss: 14.792675971984863 = 1.7553131580352783 + 2.0 * 6.518681526184082
Epoch 140, val loss: 1.7648696899414062
Epoch 150, training loss: 14.729371070861816 = 1.7418920993804932 + 2.0 * 6.493739604949951
Epoch 150, val loss: 1.7534373998641968
Epoch 160, training loss: 14.672198295593262 = 1.7266842126846313 + 2.0 * 6.472756862640381
Epoch 160, val loss: 1.7407419681549072
Epoch 170, training loss: 14.620428085327148 = 1.7093772888183594 + 2.0 * 6.4555253982543945
Epoch 170, val loss: 1.7265949249267578
Epoch 180, training loss: 14.575992584228516 = 1.6892666816711426 + 2.0 * 6.443362712860107
Epoch 180, val loss: 1.7104582786560059
Epoch 190, training loss: 14.526578903198242 = 1.6663222312927246 + 2.0 * 6.43012809753418
Epoch 190, val loss: 1.6920143365859985
Epoch 200, training loss: 14.477392196655273 = 1.639900803565979 + 2.0 * 6.418745517730713
Epoch 200, val loss: 1.6708699464797974
Epoch 210, training loss: 14.42950439453125 = 1.6094547510147095 + 2.0 * 6.410024642944336
Epoch 210, val loss: 1.646403193473816
Epoch 220, training loss: 14.377212524414062 = 1.574945330619812 + 2.0 * 6.4011335372924805
Epoch 220, val loss: 1.6185905933380127
Epoch 230, training loss: 14.327157020568848 = 1.5363271236419678 + 2.0 * 6.39541482925415
Epoch 230, val loss: 1.5874489545822144
Epoch 240, training loss: 14.26726245880127 = 1.4940389394760132 + 2.0 * 6.3866119384765625
Epoch 240, val loss: 1.553449034690857
Epoch 250, training loss: 14.208903312683105 = 1.4483386278152466 + 2.0 * 6.380282402038574
Epoch 250, val loss: 1.516748070716858
Epoch 260, training loss: 14.154255867004395 = 1.3999204635620117 + 2.0 * 6.377167701721191
Epoch 260, val loss: 1.4778512716293335
Epoch 270, training loss: 14.086140632629395 = 1.3503206968307495 + 2.0 * 6.367909908294678
Epoch 270, val loss: 1.438200831413269
Epoch 280, training loss: 14.026183128356934 = 1.3003088235855103 + 2.0 * 6.362936973571777
Epoch 280, val loss: 1.398250699043274
Epoch 290, training loss: 13.965890884399414 = 1.2501963376998901 + 2.0 * 6.357847213745117
Epoch 290, val loss: 1.3582289218902588
Epoch 300, training loss: 13.917487144470215 = 1.2008665800094604 + 2.0 * 6.358310222625732
Epoch 300, val loss: 1.3188413381576538
Epoch 310, training loss: 13.852463722229004 = 1.1533715724945068 + 2.0 * 6.349545955657959
Epoch 310, val loss: 1.2811450958251953
Epoch 320, training loss: 13.79958724975586 = 1.1075607538223267 + 2.0 * 6.346013069152832
Epoch 320, val loss: 1.2448219060897827
Epoch 330, training loss: 13.748063087463379 = 1.0633094310760498 + 2.0 * 6.342376708984375
Epoch 330, val loss: 1.2097103595733643
Epoch 340, training loss: 13.700481414794922 = 1.0208348035812378 + 2.0 * 6.339823246002197
Epoch 340, val loss: 1.17626953125
Epoch 350, training loss: 13.652993202209473 = 0.9804765582084656 + 2.0 * 6.336258411407471
Epoch 350, val loss: 1.1443705558776855
Epoch 360, training loss: 13.607344627380371 = 0.9417259693145752 + 2.0 * 6.3328094482421875
Epoch 360, val loss: 1.1138209104537964
Epoch 370, training loss: 13.563569068908691 = 0.9043444991111755 + 2.0 * 6.3296122550964355
Epoch 370, val loss: 1.0844686031341553
Epoch 380, training loss: 13.524044036865234 = 0.8684971332550049 + 2.0 * 6.327773571014404
Epoch 380, val loss: 1.0563305616378784
Epoch 390, training loss: 13.487215995788574 = 0.8345166444778442 + 2.0 * 6.32634973526001
Epoch 390, val loss: 1.029824137687683
Epoch 400, training loss: 13.44615364074707 = 0.801938533782959 + 2.0 * 6.322107315063477
Epoch 400, val loss: 1.0045758485794067
Epoch 410, training loss: 13.410062789916992 = 0.7705824971199036 + 2.0 * 6.319740295410156
Epoch 410, val loss: 0.9803829789161682
Epoch 420, training loss: 13.38254165649414 = 0.7404497861862183 + 2.0 * 6.321045875549316
Epoch 420, val loss: 0.9573838114738464
Epoch 430, training loss: 13.346959114074707 = 0.7116024494171143 + 2.0 * 6.317678451538086
Epoch 430, val loss: 0.9357371926307678
Epoch 440, training loss: 13.311312675476074 = 0.6838461756706238 + 2.0 * 6.313733100891113
Epoch 440, val loss: 0.9153032898902893
Epoch 450, training loss: 13.285215377807617 = 0.6570375561714172 + 2.0 * 6.314088821411133
Epoch 450, val loss: 0.8959479331970215
Epoch 460, training loss: 13.251150131225586 = 0.631115198135376 + 2.0 * 6.3100175857543945
Epoch 460, val loss: 0.8775847554206848
Epoch 470, training loss: 13.235335350036621 = 0.6058489680290222 + 2.0 * 6.3147430419921875
Epoch 470, val loss: 0.8601254224777222
Epoch 480, training loss: 13.197002410888672 = 0.5814773440361023 + 2.0 * 6.307762622833252
Epoch 480, val loss: 0.8434992432594299
Epoch 490, training loss: 13.168464660644531 = 0.5576563477516174 + 2.0 * 6.305404186248779
Epoch 490, val loss: 0.8278724551200867
Epoch 500, training loss: 13.14022445678711 = 0.5342937707901001 + 2.0 * 6.30296516418457
Epoch 500, val loss: 0.8128586411476135
Epoch 510, training loss: 13.115816116333008 = 0.5112366676330566 + 2.0 * 6.302289962768555
Epoch 510, val loss: 0.7984215021133423
Epoch 520, training loss: 13.088022232055664 = 0.48857808113098145 + 2.0 * 6.299722194671631
Epoch 520, val loss: 0.7846624255180359
Epoch 530, training loss: 13.06632137298584 = 0.4664510190486908 + 2.0 * 6.299935340881348
Epoch 530, val loss: 0.7716625332832336
Epoch 540, training loss: 13.041555404663086 = 0.44470691680908203 + 2.0 * 6.298424243927002
Epoch 540, val loss: 0.7593525648117065
Epoch 550, training loss: 13.015830993652344 = 0.4234718680381775 + 2.0 * 6.29617977142334
Epoch 550, val loss: 0.7476781606674194
Epoch 560, training loss: 12.992652893066406 = 0.40284472703933716 + 2.0 * 6.2949042320251465
Epoch 560, val loss: 0.7369247078895569
Epoch 570, training loss: 12.972310066223145 = 0.38272935152053833 + 2.0 * 6.294790267944336
Epoch 570, val loss: 0.7269824147224426
Epoch 580, training loss: 12.948835372924805 = 0.36335986852645874 + 2.0 * 6.29273796081543
Epoch 580, val loss: 0.7179207801818848
Epoch 590, training loss: 12.92745590209961 = 0.3447350561618805 + 2.0 * 6.291360378265381
Epoch 590, val loss: 0.7098721265792847
Epoch 600, training loss: 12.908551216125488 = 0.3268720805644989 + 2.0 * 6.290839672088623
Epoch 600, val loss: 0.7027543783187866
Epoch 610, training loss: 12.889741897583008 = 0.3098185360431671 + 2.0 * 6.289961814880371
Epoch 610, val loss: 0.6964889168739319
Epoch 620, training loss: 12.871013641357422 = 0.2936367392539978 + 2.0 * 6.288688659667969
Epoch 620, val loss: 0.6912776231765747
Epoch 630, training loss: 12.85023307800293 = 0.2782151401042938 + 2.0 * 6.286008834838867
Epoch 630, val loss: 0.6869630813598633
Epoch 640, training loss: 12.847487449645996 = 0.2635287642478943 + 2.0 * 6.2919793128967285
Epoch 640, val loss: 0.6833922863006592
Epoch 650, training loss: 12.819769859313965 = 0.24969956278800964 + 2.0 * 6.285035133361816
Epoch 650, val loss: 0.6806924343109131
Epoch 660, training loss: 12.80102825164795 = 0.23654553294181824 + 2.0 * 6.282241344451904
Epoch 660, val loss: 0.6787557601928711
Epoch 670, training loss: 12.79385757446289 = 0.22411102056503296 + 2.0 * 6.2848734855651855
Epoch 670, val loss: 0.6774559020996094
Epoch 680, training loss: 12.779520034790039 = 0.21236258745193481 + 2.0 * 6.283578872680664
Epoch 680, val loss: 0.6767265200614929
Epoch 690, training loss: 12.760433197021484 = 0.20132741332054138 + 2.0 * 6.279552936553955
Epoch 690, val loss: 0.6767420768737793
Epoch 700, training loss: 12.764008522033691 = 0.19092094898223877 + 2.0 * 6.286543846130371
Epoch 700, val loss: 0.677266538143158
Epoch 710, training loss: 12.741557121276855 = 0.18120084702968597 + 2.0 * 6.280178070068359
Epoch 710, val loss: 0.6782073378562927
Epoch 720, training loss: 12.727254867553711 = 0.1720629334449768 + 2.0 * 6.2775959968566895
Epoch 720, val loss: 0.6797480583190918
Epoch 730, training loss: 12.714471817016602 = 0.16345669329166412 + 2.0 * 6.27550745010376
Epoch 730, val loss: 0.6816586852073669
Epoch 740, training loss: 12.727982521057129 = 0.15537913143634796 + 2.0 * 6.286301612854004
Epoch 740, val loss: 0.6839466094970703
Epoch 750, training loss: 12.704527854919434 = 0.1477796584367752 + 2.0 * 6.278374195098877
Epoch 750, val loss: 0.6864023208618164
Epoch 760, training loss: 12.687479019165039 = 0.1407075971364975 + 2.0 * 6.273385524749756
Epoch 760, val loss: 0.6893779039382935
Epoch 770, training loss: 12.677092552185059 = 0.13404177129268646 + 2.0 * 6.2715253829956055
Epoch 770, val loss: 0.6925820708274841
Epoch 780, training loss: 12.67247486114502 = 0.12774460017681122 + 2.0 * 6.272365093231201
Epoch 780, val loss: 0.6960246562957764
Epoch 790, training loss: 12.669084548950195 = 0.12181885540485382 + 2.0 * 6.273633003234863
Epoch 790, val loss: 0.6996718049049377
Epoch 800, training loss: 12.656196594238281 = 0.1162748709321022 + 2.0 * 6.269960880279541
Epoch 800, val loss: 0.7034862041473389
Epoch 810, training loss: 12.649999618530273 = 0.11105837672948837 + 2.0 * 6.269470691680908
Epoch 810, val loss: 0.7075708508491516
Epoch 820, training loss: 12.649713516235352 = 0.10613347589969635 + 2.0 * 6.271790027618408
Epoch 820, val loss: 0.7117884755134583
Epoch 830, training loss: 12.637413024902344 = 0.10149029642343521 + 2.0 * 6.267961502075195
Epoch 830, val loss: 0.7161620855331421
Epoch 840, training loss: 12.628762245178223 = 0.09710433334112167 + 2.0 * 6.265829086303711
Epoch 840, val loss: 0.7207238078117371
Epoch 850, training loss: 12.643527030944824 = 0.09297099709510803 + 2.0 * 6.275278091430664
Epoch 850, val loss: 0.7254231572151184
Epoch 860, training loss: 12.625733375549316 = 0.08905398845672607 + 2.0 * 6.26833963394165
Epoch 860, val loss: 0.7300509810447693
Epoch 870, training loss: 12.613946914672852 = 0.08538613468408585 + 2.0 * 6.264280319213867
Epoch 870, val loss: 0.7349555492401123
Epoch 880, training loss: 12.61488151550293 = 0.08190888166427612 + 2.0 * 6.266486167907715
Epoch 880, val loss: 0.7399151921272278
Epoch 890, training loss: 12.603426933288574 = 0.07860440760850906 + 2.0 * 6.262411117553711
Epoch 890, val loss: 0.7449045777320862
Epoch 900, training loss: 12.603193283081055 = 0.07547464966773987 + 2.0 * 6.263859272003174
Epoch 900, val loss: 0.7500425577163696
Epoch 910, training loss: 12.601011276245117 = 0.0725153237581253 + 2.0 * 6.264247894287109
Epoch 910, val loss: 0.7552391290664673
Epoch 920, training loss: 12.596809387207031 = 0.0697103962302208 + 2.0 * 6.263549327850342
Epoch 920, val loss: 0.7604565620422363
Epoch 930, training loss: 12.58835506439209 = 0.06703909486532211 + 2.0 * 6.260657787322998
Epoch 930, val loss: 0.7656893730163574
Epoch 940, training loss: 12.583108901977539 = 0.06450933963060379 + 2.0 * 6.2592997550964355
Epoch 940, val loss: 0.7710735201835632
Epoch 950, training loss: 12.584480285644531 = 0.062091924250125885 + 2.0 * 6.261194229125977
Epoch 950, val loss: 0.7764409184455872
Epoch 960, training loss: 12.581924438476562 = 0.05979425832629204 + 2.0 * 6.2610650062561035
Epoch 960, val loss: 0.78178471326828
Epoch 970, training loss: 12.57576847076416 = 0.05762680247426033 + 2.0 * 6.259070873260498
Epoch 970, val loss: 0.7872116565704346
Epoch 980, training loss: 12.57257080078125 = 0.05556400120258331 + 2.0 * 6.258503437042236
Epoch 980, val loss: 0.7926591634750366
Epoch 990, training loss: 12.566631317138672 = 0.053591590374708176 + 2.0 * 6.256519794464111
Epoch 990, val loss: 0.7981353402137756
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.825925925925926
0.838165524512388
=== training gcn model ===
Epoch 0, training loss: 19.13345718383789 = 1.9398173093795776 + 2.0 * 8.596819877624512
Epoch 0, val loss: 1.9375576972961426
Epoch 10, training loss: 19.121641159057617 = 1.9291211366653442 + 2.0 * 8.596260070800781
Epoch 10, val loss: 1.9274970293045044
Epoch 20, training loss: 19.09837532043457 = 1.9156101942062378 + 2.0 * 8.59138298034668
Epoch 20, val loss: 1.9146350622177124
Epoch 30, training loss: 19.007516860961914 = 1.8972855806350708 + 2.0 * 8.555115699768066
Epoch 30, val loss: 1.8971971273422241
Epoch 40, training loss: 18.513490676879883 = 1.8753806352615356 + 2.0 * 8.31905460357666
Epoch 40, val loss: 1.876966118812561
Epoch 50, training loss: 17.12338638305664 = 1.8514809608459473 + 2.0 * 7.635952472686768
Epoch 50, val loss: 1.8555889129638672
Epoch 60, training loss: 16.46724510192871 = 1.8359683752059937 + 2.0 * 7.315638542175293
Epoch 60, val loss: 1.8427505493164062
Epoch 70, training loss: 15.970282554626465 = 1.8250646591186523 + 2.0 * 7.072608947753906
Epoch 70, val loss: 1.8328993320465088
Epoch 80, training loss: 15.655991554260254 = 1.8152216672897339 + 2.0 * 6.920384883880615
Epoch 80, val loss: 1.823094129562378
Epoch 90, training loss: 15.461673736572266 = 1.804322361946106 + 2.0 * 6.828675746917725
Epoch 90, val loss: 1.8124098777770996
Epoch 100, training loss: 15.271238327026367 = 1.794022560119629 + 2.0 * 6.738607883453369
Epoch 100, val loss: 1.8027621507644653
Epoch 110, training loss: 15.093864440917969 = 1.78513765335083 + 2.0 * 6.654363632202148
Epoch 110, val loss: 1.794402003288269
Epoch 120, training loss: 14.981451034545898 = 1.776127815246582 + 2.0 * 6.602661609649658
Epoch 120, val loss: 1.7855483293533325
Epoch 130, training loss: 14.893548011779785 = 1.765539526939392 + 2.0 * 6.564004421234131
Epoch 130, val loss: 1.7754437923431396
Epoch 140, training loss: 14.826910018920898 = 1.7535353899002075 + 2.0 * 6.53668737411499
Epoch 140, val loss: 1.7645260095596313
Epoch 150, training loss: 14.766993522644043 = 1.7402681112289429 + 2.0 * 6.513362884521484
Epoch 150, val loss: 1.752784252166748
Epoch 160, training loss: 14.710094451904297 = 1.7254317998886108 + 2.0 * 6.492331504821777
Epoch 160, val loss: 1.739869236946106
Epoch 170, training loss: 14.658319473266602 = 1.7088110446929932 + 2.0 * 6.474754333496094
Epoch 170, val loss: 1.7256308794021606
Epoch 180, training loss: 14.616207122802734 = 1.6899352073669434 + 2.0 * 6.463135719299316
Epoch 180, val loss: 1.709776759147644
Epoch 190, training loss: 14.563454627990723 = 1.6683852672576904 + 2.0 * 6.447534561157227
Epoch 190, val loss: 1.6919440031051636
Epoch 200, training loss: 14.513526916503906 = 1.6437950134277344 + 2.0 * 6.434865951538086
Epoch 200, val loss: 1.671668529510498
Epoch 210, training loss: 14.46493148803711 = 1.6155527830123901 + 2.0 * 6.424689292907715
Epoch 210, val loss: 1.6483547687530518
Epoch 220, training loss: 14.411839485168457 = 1.5837581157684326 + 2.0 * 6.414040565490723
Epoch 220, val loss: 1.622315764427185
Epoch 230, training loss: 14.359492301940918 = 1.5486327409744263 + 2.0 * 6.405429840087891
Epoch 230, val loss: 1.5935264825820923
Epoch 240, training loss: 14.303256034851074 = 1.5099554061889648 + 2.0 * 6.396650314331055
Epoch 240, val loss: 1.5618946552276611
Epoch 250, training loss: 14.252321243286133 = 1.4682561159133911 + 2.0 * 6.392032623291016
Epoch 250, val loss: 1.528006911277771
Epoch 260, training loss: 14.193634986877441 = 1.4251383543014526 + 2.0 * 6.38424825668335
Epoch 260, val loss: 1.492971658706665
Epoch 270, training loss: 14.135826110839844 = 1.3812263011932373 + 2.0 * 6.377299785614014
Epoch 270, val loss: 1.4573569297790527
Epoch 280, training loss: 14.082731246948242 = 1.3370904922485352 + 2.0 * 6.3728203773498535
Epoch 280, val loss: 1.4218215942382812
Epoch 290, training loss: 14.02905559539795 = 1.2936859130859375 + 2.0 * 6.367684841156006
Epoch 290, val loss: 1.3872281312942505
Epoch 300, training loss: 13.97545337677002 = 1.2516592741012573 + 2.0 * 6.361896991729736
Epoch 300, val loss: 1.3536618947982788
Epoch 310, training loss: 13.925512313842773 = 1.2111666202545166 + 2.0 * 6.357172966003418
Epoch 310, val loss: 1.3213908672332764
Epoch 320, training loss: 13.877334594726562 = 1.1718164682388306 + 2.0 * 6.352758884429932
Epoch 320, val loss: 1.290136694908142
Epoch 330, training loss: 13.830992698669434 = 1.1334995031356812 + 2.0 * 6.3487467765808105
Epoch 330, val loss: 1.2596161365509033
Epoch 340, training loss: 13.78635025024414 = 1.096245288848877 + 2.0 * 6.345052242279053
Epoch 340, val loss: 1.2299984693527222
Epoch 350, training loss: 13.741453170776367 = 1.059463620185852 + 2.0 * 6.340994834899902
Epoch 350, val loss: 1.2007590532302856
Epoch 360, training loss: 13.703008651733398 = 1.0228261947631836 + 2.0 * 6.340091228485107
Epoch 360, val loss: 1.171610951423645
Epoch 370, training loss: 13.65754222869873 = 0.9868210554122925 + 2.0 * 6.335360527038574
Epoch 370, val loss: 1.1427456140518188
Epoch 380, training loss: 13.616424560546875 = 0.9512643814086914 + 2.0 * 6.332580089569092
Epoch 380, val loss: 1.1143529415130615
Epoch 390, training loss: 13.573030471801758 = 0.9159835577011108 + 2.0 * 6.328523635864258
Epoch 390, val loss: 1.0859988927841187
Epoch 400, training loss: 13.532025337219238 = 0.8807846307754517 + 2.0 * 6.325620174407959
Epoch 400, val loss: 1.0577164888381958
Epoch 410, training loss: 13.507230758666992 = 0.8457750082015991 + 2.0 * 6.330728054046631
Epoch 410, val loss: 1.0297679901123047
Epoch 420, training loss: 13.457143783569336 = 0.8116586208343506 + 2.0 * 6.322742462158203
Epoch 420, val loss: 1.002514123916626
Epoch 430, training loss: 13.417054176330566 = 0.7782528400421143 + 2.0 * 6.319400787353516
Epoch 430, val loss: 0.9761016964912415
Epoch 440, training loss: 13.387497901916504 = 0.745532751083374 + 2.0 * 6.320982456207275
Epoch 440, val loss: 0.9506481289863586
Epoch 450, training loss: 13.347307205200195 = 0.7138038277626038 + 2.0 * 6.316751480102539
Epoch 450, val loss: 0.9265025854110718
Epoch 460, training loss: 13.310465812683105 = 0.6832501888275146 + 2.0 * 6.313607692718506
Epoch 460, val loss: 0.9038352966308594
Epoch 470, training loss: 13.277382850646973 = 0.6536716222763062 + 2.0 * 6.311855792999268
Epoch 470, val loss: 0.8826648592948914
Epoch 480, training loss: 13.244328498840332 = 0.6252202391624451 + 2.0 * 6.309554100036621
Epoch 480, val loss: 0.8631182909011841
Epoch 490, training loss: 13.218279838562012 = 0.5979397892951965 + 2.0 * 6.3101701736450195
Epoch 490, val loss: 0.845244824886322
Epoch 500, training loss: 13.184432983398438 = 0.5718294382095337 + 2.0 * 6.306301593780518
Epoch 500, val loss: 0.8289381861686707
Epoch 510, training loss: 13.161026954650879 = 0.5467156171798706 + 2.0 * 6.307155609130859
Epoch 510, val loss: 0.8141447305679321
Epoch 520, training loss: 13.137359619140625 = 0.5225491523742676 + 2.0 * 6.3074049949646
Epoch 520, val loss: 0.8007648587226868
Epoch 530, training loss: 13.106768608093262 = 0.49958476424217224 + 2.0 * 6.303591728210449
Epoch 530, val loss: 0.7886720299720764
Epoch 540, training loss: 13.079681396484375 = 0.4775214195251465 + 2.0 * 6.301079750061035
Epoch 540, val loss: 0.7779024839401245
Epoch 550, training loss: 13.057875633239746 = 0.45635655522346497 + 2.0 * 6.300759315490723
Epoch 550, val loss: 0.7682822942733765
Epoch 560, training loss: 13.034072875976562 = 0.43616756796836853 + 2.0 * 6.298952579498291
Epoch 560, val loss: 0.7598015666007996
Epoch 570, training loss: 13.011263847351074 = 0.4168623983860016 + 2.0 * 6.297200679779053
Epoch 570, val loss: 0.7523176670074463
Epoch 580, training loss: 12.988234519958496 = 0.398364782333374 + 2.0 * 6.2949347496032715
Epoch 580, val loss: 0.7457944750785828
Epoch 590, training loss: 12.988921165466309 = 0.3806425929069519 + 2.0 * 6.304139137268066
Epoch 590, val loss: 0.7401072978973389
Epoch 600, training loss: 12.950587272644043 = 0.36368659138679504 + 2.0 * 6.293450355529785
Epoch 600, val loss: 0.7352446913719177
Epoch 610, training loss: 12.931467056274414 = 0.34756818413734436 + 2.0 * 6.291949272155762
Epoch 610, val loss: 0.7311644554138184
Epoch 620, training loss: 12.912477493286133 = 0.33211806416511536 + 2.0 * 6.29017972946167
Epoch 620, val loss: 0.727874219417572
Epoch 630, training loss: 12.901575088500977 = 0.31725627183914185 + 2.0 * 6.292159557342529
Epoch 630, val loss: 0.7252926230430603
Epoch 640, training loss: 12.888574600219727 = 0.30308693647384644 + 2.0 * 6.292743682861328
Epoch 640, val loss: 0.7233582735061646
Epoch 650, training loss: 12.863933563232422 = 0.2895037829875946 + 2.0 * 6.287214756011963
Epoch 650, val loss: 0.7219052314758301
Epoch 660, training loss: 12.84940242767334 = 0.2765260338783264 + 2.0 * 6.28643798828125
Epoch 660, val loss: 0.721068799495697
Epoch 670, training loss: 12.834251403808594 = 0.2640025019645691 + 2.0 * 6.2851243019104
Epoch 670, val loss: 0.720851719379425
Epoch 680, training loss: 12.826525688171387 = 0.2519688904285431 + 2.0 * 6.287278175354004
Epoch 680, val loss: 0.721121609210968
Epoch 690, training loss: 12.807209968566895 = 0.24046844244003296 + 2.0 * 6.2833709716796875
Epoch 690, val loss: 0.7217356562614441
Epoch 700, training loss: 12.792487144470215 = 0.2294245958328247 + 2.0 * 6.28153133392334
Epoch 700, val loss: 0.7227752804756165
Epoch 710, training loss: 12.795273780822754 = 0.2188180834054947 + 2.0 * 6.2882280349731445
Epoch 710, val loss: 0.7242770195007324
Epoch 720, training loss: 12.771017074584961 = 0.20863503217697144 + 2.0 * 6.281190872192383
Epoch 720, val loss: 0.7260682582855225
Epoch 730, training loss: 12.756136894226074 = 0.19894792139530182 + 2.0 * 6.278594493865967
Epoch 730, val loss: 0.7281069159507751
Epoch 740, training loss: 12.745469093322754 = 0.18963070213794708 + 2.0 * 6.277919292449951
Epoch 740, val loss: 0.7305409908294678
Epoch 750, training loss: 12.741033554077148 = 0.1806936115026474 + 2.0 * 6.28016996383667
Epoch 750, val loss: 0.7332863807678223
Epoch 760, training loss: 12.731014251708984 = 0.1721615344285965 + 2.0 * 6.279426574707031
Epoch 760, val loss: 0.7361621856689453
Epoch 770, training loss: 12.71350383758545 = 0.16405029594898224 + 2.0 * 6.274726867675781
Epoch 770, val loss: 0.7392572164535522
Epoch 780, training loss: 12.703760147094727 = 0.1562907099723816 + 2.0 * 6.2737345695495605
Epoch 780, val loss: 0.7425990700721741
Epoch 790, training loss: 12.71780014038086 = 0.14890040457248688 + 2.0 * 6.284450054168701
Epoch 790, val loss: 0.7461855411529541
Epoch 800, training loss: 12.690886497497559 = 0.14188900589942932 + 2.0 * 6.27449893951416
Epoch 800, val loss: 0.7497466802597046
Epoch 810, training loss: 12.67692756652832 = 0.13525499403476715 + 2.0 * 6.270836353302002
Epoch 810, val loss: 0.753389835357666
Epoch 820, training loss: 12.668283462524414 = 0.12893414497375488 + 2.0 * 6.269674777984619
Epoch 820, val loss: 0.7574008703231812
Epoch 830, training loss: 12.669225692749023 = 0.12293096631765366 + 2.0 * 6.2731475830078125
Epoch 830, val loss: 0.7615486979484558
Epoch 840, training loss: 12.658834457397461 = 0.11722442507743835 + 2.0 * 6.2708048820495605
Epoch 840, val loss: 0.7657374143600464
Epoch 850, training loss: 12.647130012512207 = 0.11182764917612076 + 2.0 * 6.267651081085205
Epoch 850, val loss: 0.7699531316757202
Epoch 860, training loss: 12.640044212341309 = 0.10670303553342819 + 2.0 * 6.2666707038879395
Epoch 860, val loss: 0.7743409276008606
Epoch 870, training loss: 12.64635944366455 = 0.10184486955404282 + 2.0 * 6.272257328033447
Epoch 870, val loss: 0.7788465023040771
Epoch 880, training loss: 12.643619537353516 = 0.0972815528512001 + 2.0 * 6.273169040679932
Epoch 880, val loss: 0.7833133339881897
Epoch 890, training loss: 12.62087631225586 = 0.09295790642499924 + 2.0 * 6.2639594078063965
Epoch 890, val loss: 0.7877881526947021
Epoch 900, training loss: 12.615498542785645 = 0.08886751532554626 + 2.0 * 6.263315677642822
Epoch 900, val loss: 0.7923389673233032
Epoch 910, training loss: 12.608501434326172 = 0.08499549329280853 + 2.0 * 6.261753082275391
Epoch 910, val loss: 0.7970279455184937
Epoch 920, training loss: 12.614363670349121 = 0.08131134510040283 + 2.0 * 6.266526222229004
Epoch 920, val loss: 0.8018007874488831
Epoch 930, training loss: 12.61555290222168 = 0.0778433308005333 + 2.0 * 6.26885461807251
Epoch 930, val loss: 0.8064618706703186
Epoch 940, training loss: 12.597077369689941 = 0.07458667457103729 + 2.0 * 6.261245250701904
Epoch 940, val loss: 0.8109584450721741
Epoch 950, training loss: 12.58935832977295 = 0.07150925695896149 + 2.0 * 6.25892448425293
Epoch 950, val loss: 0.8156272768974304
Epoch 960, training loss: 12.584285736083984 = 0.06858319044113159 + 2.0 * 6.2578511238098145
Epoch 960, val loss: 0.8204441666603088
Epoch 970, training loss: 12.58045482635498 = 0.06580357998609543 + 2.0 * 6.257325649261475
Epoch 970, val loss: 0.8252067565917969
Epoch 980, training loss: 12.608793258666992 = 0.0631612241268158 + 2.0 * 6.272816181182861
Epoch 980, val loss: 0.8299599885940552
Epoch 990, training loss: 12.577486038208008 = 0.060684237629175186 + 2.0 * 6.258400917053223
Epoch 990, val loss: 0.8346558809280396
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8074074074074075
0.8365840801265156
The final CL Acc:0.80247, 0.02145, The final GNN Acc:0.83676, 0.00108
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11576])
remove edge: torch.Size([2, 9442])
updated graph: torch.Size([2, 10462])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 19.140958786010742 = 1.9472448825836182 + 2.0 * 8.596857070922852
Epoch 0, val loss: 1.9446660280227661
Epoch 10, training loss: 19.130226135253906 = 1.9372042417526245 + 2.0 * 8.596510887145996
Epoch 10, val loss: 1.9348764419555664
Epoch 20, training loss: 19.111085891723633 = 1.924569845199585 + 2.0 * 8.593257904052734
Epoch 20, val loss: 1.9219821691513062
Epoch 30, training loss: 19.03564453125 = 1.9076435565948486 + 2.0 * 8.564000129699707
Epoch 30, val loss: 1.9045337438583374
Epoch 40, training loss: 18.611879348754883 = 1.8865835666656494 + 2.0 * 8.362648010253906
Epoch 40, val loss: 1.8834391832351685
Epoch 50, training loss: 17.679677963256836 = 1.8639967441558838 + 2.0 * 7.907840251922607
Epoch 50, val loss: 1.8618005514144897
Epoch 60, training loss: 16.83047103881836 = 1.8473200798034668 + 2.0 * 7.491575717926025
Epoch 60, val loss: 1.847428560256958
Epoch 70, training loss: 16.127307891845703 = 1.8346061706542969 + 2.0 * 7.146351337432861
Epoch 70, val loss: 1.8355189561843872
Epoch 80, training loss: 15.684106826782227 = 1.8226184844970703 + 2.0 * 6.930744171142578
Epoch 80, val loss: 1.8242950439453125
Epoch 90, training loss: 15.414546966552734 = 1.8099271059036255 + 2.0 * 6.802309989929199
Epoch 90, val loss: 1.81235671043396
Epoch 100, training loss: 15.246236801147461 = 1.797695517539978 + 2.0 * 6.724270820617676
Epoch 100, val loss: 1.8011428117752075
Epoch 110, training loss: 15.114480972290039 = 1.78593909740448 + 2.0 * 6.664270877838135
Epoch 110, val loss: 1.7903565168380737
Epoch 120, training loss: 15.012699127197266 = 1.7743546962738037 + 2.0 * 6.619172096252441
Epoch 120, val loss: 1.779798150062561
Epoch 130, training loss: 14.93127155303955 = 1.7622333765029907 + 2.0 * 6.584518909454346
Epoch 130, val loss: 1.7689447402954102
Epoch 140, training loss: 14.861590385437012 = 1.7489961385726929 + 2.0 * 6.556297302246094
Epoch 140, val loss: 1.7570966482162476
Epoch 150, training loss: 14.804758071899414 = 1.7342556715011597 + 2.0 * 6.535251140594482
Epoch 150, val loss: 1.7440515756607056
Epoch 160, training loss: 14.744636535644531 = 1.717970371246338 + 2.0 * 6.513332843780518
Epoch 160, val loss: 1.7298072576522827
Epoch 170, training loss: 14.692026138305664 = 1.699949860572815 + 2.0 * 6.49603796005249
Epoch 170, val loss: 1.7141482830047607
Epoch 180, training loss: 14.635558128356934 = 1.6798862218856812 + 2.0 * 6.4778361320495605
Epoch 180, val loss: 1.6968728303909302
Epoch 190, training loss: 14.5913724899292 = 1.6574552059173584 + 2.0 * 6.466958522796631
Epoch 190, val loss: 1.677669882774353
Epoch 200, training loss: 14.53579044342041 = 1.6324660778045654 + 2.0 * 6.451662063598633
Epoch 200, val loss: 1.6564700603485107
Epoch 210, training loss: 14.484947204589844 = 1.6047801971435547 + 2.0 * 6.4400835037231445
Epoch 210, val loss: 1.63318932056427
Epoch 220, training loss: 14.437204360961914 = 1.5744673013687134 + 2.0 * 6.431368350982666
Epoch 220, val loss: 1.607740879058838
Epoch 230, training loss: 14.382562637329102 = 1.5415902137756348 + 2.0 * 6.420485973358154
Epoch 230, val loss: 1.5805302858352661
Epoch 240, training loss: 14.330167770385742 = 1.5061625242233276 + 2.0 * 6.4120025634765625
Epoch 240, val loss: 1.551543951034546
Epoch 250, training loss: 14.281002044677734 = 1.468369960784912 + 2.0 * 6.40631628036499
Epoch 250, val loss: 1.5210609436035156
Epoch 260, training loss: 14.223916053771973 = 1.4288619756698608 + 2.0 * 6.39752721786499
Epoch 260, val loss: 1.4893888235092163
Epoch 270, training loss: 14.169593811035156 = 1.3876605033874512 + 2.0 * 6.390966892242432
Epoch 270, val loss: 1.4569469690322876
Epoch 280, training loss: 14.121977806091309 = 1.3452308177947998 + 2.0 * 6.388373374938965
Epoch 280, val loss: 1.423964500427246
Epoch 290, training loss: 14.065381050109863 = 1.3023773431777954 + 2.0 * 6.3815016746521
Epoch 290, val loss: 1.3908357620239258
Epoch 300, training loss: 14.012578964233398 = 1.2592207193374634 + 2.0 * 6.376678943634033
Epoch 300, val loss: 1.3580750226974487
Epoch 310, training loss: 13.961483001708984 = 1.2164013385772705 + 2.0 * 6.3725409507751465
Epoch 310, val loss: 1.325722098350525
Epoch 320, training loss: 13.90723705291748 = 1.1740913391113281 + 2.0 * 6.366572856903076
Epoch 320, val loss: 1.2943339347839355
Epoch 330, training loss: 13.862338066101074 = 1.1324872970581055 + 2.0 * 6.364925384521484
Epoch 330, val loss: 1.2639899253845215
Epoch 340, training loss: 13.809618949890137 = 1.0919018983840942 + 2.0 * 6.358858585357666
Epoch 340, val loss: 1.234717845916748
Epoch 350, training loss: 13.765756607055664 = 1.0523924827575684 + 2.0 * 6.356681823730469
Epoch 350, val loss: 1.2068920135498047
Epoch 360, training loss: 13.720735549926758 = 1.0141477584838867 + 2.0 * 6.3532938957214355
Epoch 360, val loss: 1.1805641651153564
Epoch 370, training loss: 13.688660621643066 = 0.9775562286376953 + 2.0 * 6.3555521965026855
Epoch 370, val loss: 1.1560896635055542
Epoch 380, training loss: 13.64053726196289 = 0.9424540996551514 + 2.0 * 6.34904146194458
Epoch 380, val loss: 1.1336947679519653
Epoch 390, training loss: 13.598223686218262 = 0.9089266061782837 + 2.0 * 6.344648361206055
Epoch 390, val loss: 1.1129599809646606
Epoch 400, training loss: 13.563619613647461 = 0.8767795562744141 + 2.0 * 6.343420028686523
Epoch 400, val loss: 1.094082236289978
Epoch 410, training loss: 13.53200912475586 = 0.8459963798522949 + 2.0 * 6.343006134033203
Epoch 410, val loss: 1.0769892930984497
Epoch 420, training loss: 13.49738597869873 = 0.8165559768676758 + 2.0 * 6.340415000915527
Epoch 420, val loss: 1.0616648197174072
Epoch 430, training loss: 13.460331916809082 = 0.7884629368782043 + 2.0 * 6.335934638977051
Epoch 430, val loss: 1.0478371381759644
Epoch 440, training loss: 13.428267478942871 = 0.7612741589546204 + 2.0 * 6.333496570587158
Epoch 440, val loss: 1.0354467630386353
Epoch 450, training loss: 13.405660629272461 = 0.7348859906196594 + 2.0 * 6.335387229919434
Epoch 450, val loss: 1.0242007970809937
Epoch 460, training loss: 13.376252174377441 = 0.709298849105835 + 2.0 * 6.333476543426514
Epoch 460, val loss: 1.013976812362671
Epoch 470, training loss: 13.340282440185547 = 0.6842402219772339 + 2.0 * 6.328021049499512
Epoch 470, val loss: 1.004740834236145
Epoch 480, training loss: 13.311286926269531 = 0.6596178412437439 + 2.0 * 6.32583475112915
Epoch 480, val loss: 0.9962216019630432
Epoch 490, training loss: 13.287991523742676 = 0.6352559328079224 + 2.0 * 6.3263678550720215
Epoch 490, val loss: 0.988303542137146
Epoch 500, training loss: 13.265740394592285 = 0.6110369563102722 + 2.0 * 6.3273515701293945
Epoch 500, val loss: 0.9806779623031616
Epoch 510, training loss: 13.232638359069824 = 0.5869776010513306 + 2.0 * 6.3228302001953125
Epoch 510, val loss: 0.9734536409378052
Epoch 520, training loss: 13.212779998779297 = 0.5629720091819763 + 2.0 * 6.324903964996338
Epoch 520, val loss: 0.966545045375824
Epoch 530, training loss: 13.176201820373535 = 0.5389499068260193 + 2.0 * 6.3186259269714355
Epoch 530, val loss: 0.9599055647850037
Epoch 540, training loss: 13.149823188781738 = 0.5149049758911133 + 2.0 * 6.3174591064453125
Epoch 540, val loss: 0.9534170627593994
Epoch 550, training loss: 13.138748168945312 = 0.49083569645881653 + 2.0 * 6.32395601272583
Epoch 550, val loss: 0.9470372200012207
Epoch 560, training loss: 13.099197387695312 = 0.4670124650001526 + 2.0 * 6.316092491149902
Epoch 560, val loss: 0.9407822489738464
Epoch 570, training loss: 13.070152282714844 = 0.4432826042175293 + 2.0 * 6.313434600830078
Epoch 570, val loss: 0.9349891543388367
Epoch 580, training loss: 13.046920776367188 = 0.4198625683784485 + 2.0 * 6.313529014587402
Epoch 580, val loss: 0.9295084476470947
Epoch 590, training loss: 13.022222518920898 = 0.3968457579612732 + 2.0 * 6.31268835067749
Epoch 590, val loss: 0.9244845509529114
Epoch 600, training loss: 12.995773315429688 = 0.37443113327026367 + 2.0 * 6.310670852661133
Epoch 600, val loss: 0.9199389219284058
Epoch 610, training loss: 12.971562385559082 = 0.3527224361896515 + 2.0 * 6.309420108795166
Epoch 610, val loss: 0.9161218404769897
Epoch 620, training loss: 12.951253890991211 = 0.33189329504966736 + 2.0 * 6.309680461883545
Epoch 620, val loss: 0.9129272699356079
Epoch 630, training loss: 12.927678108215332 = 0.31210777163505554 + 2.0 * 6.3077850341796875
Epoch 630, val loss: 0.9105960130691528
Epoch 640, training loss: 12.903538703918457 = 0.2933618426322937 + 2.0 * 6.305088520050049
Epoch 640, val loss: 0.909277617931366
Epoch 650, training loss: 12.886034965515137 = 0.2756967842578888 + 2.0 * 6.305169105529785
Epoch 650, val loss: 0.908935010433197
Epoch 660, training loss: 12.866357803344727 = 0.2591439485549927 + 2.0 * 6.303606986999512
Epoch 660, val loss: 0.9093248844146729
Epoch 670, training loss: 12.85490894317627 = 0.24371859431266785 + 2.0 * 6.305595397949219
Epoch 670, val loss: 0.9106364250183105
Epoch 680, training loss: 12.831459045410156 = 0.22940906882286072 + 2.0 * 6.301024913787842
Epoch 680, val loss: 0.9127529859542847
Epoch 690, training loss: 12.814925193786621 = 0.21606741845607758 + 2.0 * 6.299428939819336
Epoch 690, val loss: 0.9156432747840881
Epoch 700, training loss: 12.81166934967041 = 0.20364771783351898 + 2.0 * 6.30401086807251
Epoch 700, val loss: 0.9192115068435669
Epoch 710, training loss: 12.791878700256348 = 0.19209668040275574 + 2.0 * 6.299890995025635
Epoch 710, val loss: 0.9233260154724121
Epoch 720, training loss: 12.776466369628906 = 0.18140137195587158 + 2.0 * 6.297532558441162
Epoch 720, val loss: 0.9279559254646301
Epoch 730, training loss: 12.77167797088623 = 0.17146611213684082 + 2.0 * 6.300106048583984
Epoch 730, val loss: 0.9329633712768555
Epoch 740, training loss: 12.751314163208008 = 0.16220973432064056 + 2.0 * 6.294552326202393
Epoch 740, val loss: 0.9383519291877747
Epoch 750, training loss: 12.738458633422852 = 0.1536235213279724 + 2.0 * 6.292417526245117
Epoch 750, val loss: 0.9441238641738892
Epoch 760, training loss: 12.728841781616211 = 0.14559103548526764 + 2.0 * 6.291625499725342
Epoch 760, val loss: 0.9501882791519165
Epoch 770, training loss: 12.73978042602539 = 0.13808555901050568 + 2.0 * 6.30084753036499
Epoch 770, val loss: 0.956392765045166
Epoch 780, training loss: 12.71021842956543 = 0.13105760514736176 + 2.0 * 6.289580345153809
Epoch 780, val loss: 0.9626474976539612
Epoch 790, training loss: 12.711042404174805 = 0.12451225519180298 + 2.0 * 6.293264865875244
Epoch 790, val loss: 0.9690746068954468
Epoch 800, training loss: 12.693504333496094 = 0.11840762197971344 + 2.0 * 6.287548542022705
Epoch 800, val loss: 0.9756366610527039
Epoch 810, training loss: 12.689247131347656 = 0.11268636584281921 + 2.0 * 6.288280487060547
Epoch 810, val loss: 0.9823307991027832
Epoch 820, training loss: 12.681854248046875 = 0.10730921477079391 + 2.0 * 6.2872724533081055
Epoch 820, val loss: 0.9890605807304382
Epoch 830, training loss: 12.675381660461426 = 0.102260060608387 + 2.0 * 6.286561012268066
Epoch 830, val loss: 0.9957695007324219
Epoch 840, training loss: 12.673065185546875 = 0.0975339412689209 + 2.0 * 6.2877655029296875
Epoch 840, val loss: 1.0024290084838867
Epoch 850, training loss: 12.661620140075684 = 0.09309639781713486 + 2.0 * 6.284261703491211
Epoch 850, val loss: 1.009110689163208
Epoch 860, training loss: 12.653084754943848 = 0.08892587572336197 + 2.0 * 6.282079219818115
Epoch 860, val loss: 1.0158400535583496
Epoch 870, training loss: 12.659891128540039 = 0.08500026166439056 + 2.0 * 6.287445545196533
Epoch 870, val loss: 1.0226085186004639
Epoch 880, training loss: 12.64470386505127 = 0.08127738535404205 + 2.0 * 6.281713008880615
Epoch 880, val loss: 1.0291593074798584
Epoch 890, training loss: 12.638678550720215 = 0.07779118418693542 + 2.0 * 6.2804436683654785
Epoch 890, val loss: 1.0358057022094727
Epoch 900, training loss: 12.632928848266602 = 0.0744914636015892 + 2.0 * 6.279218673706055
Epoch 900, val loss: 1.0424367189407349
Epoch 910, training loss: 12.639656066894531 = 0.07138101756572723 + 2.0 * 6.284137725830078
Epoch 910, val loss: 1.0490491390228271
Epoch 920, training loss: 12.628060340881348 = 0.06844261288642883 + 2.0 * 6.27980899810791
Epoch 920, val loss: 1.0555070638656616
Epoch 930, training loss: 12.626357078552246 = 0.06566401571035385 + 2.0 * 6.280346393585205
Epoch 930, val loss: 1.061976432800293
Epoch 940, training loss: 12.615551948547363 = 0.06303413212299347 + 2.0 * 6.276258945465088
Epoch 940, val loss: 1.0683766603469849
Epoch 950, training loss: 12.610457420349121 = 0.06054256856441498 + 2.0 * 6.274957656860352
Epoch 950, val loss: 1.0748004913330078
Epoch 960, training loss: 12.626505851745605 = 0.05817927047610283 + 2.0 * 6.284163475036621
Epoch 960, val loss: 1.0811331272125244
Epoch 970, training loss: 12.612217903137207 = 0.0559479184448719 + 2.0 * 6.278134822845459
Epoch 970, val loss: 1.0872758626937866
Epoch 980, training loss: 12.600433349609375 = 0.053833141922950745 + 2.0 * 6.2733001708984375
Epoch 980, val loss: 1.0934433937072754
Epoch 990, training loss: 12.60134220123291 = 0.05183517932891846 + 2.0 * 6.274753570556641
Epoch 990, val loss: 1.0996383428573608
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7666666666666667
0.8033737480231946
=== training gcn model ===
Epoch 0, training loss: 19.142547607421875 = 1.9488035440444946 + 2.0 * 8.596872329711914
Epoch 0, val loss: 1.9489470720291138
Epoch 10, training loss: 19.130931854248047 = 1.9375722408294678 + 2.0 * 8.5966796875
Epoch 10, val loss: 1.9372525215148926
Epoch 20, training loss: 19.113550186157227 = 1.9237576723098755 + 2.0 * 8.59489631652832
Epoch 20, val loss: 1.9226646423339844
Epoch 30, training loss: 19.063364028930664 = 1.904800534248352 + 2.0 * 8.5792818069458
Epoch 30, val loss: 1.9027886390686035
Epoch 40, training loss: 18.857851028442383 = 1.8805968761444092 + 2.0 * 8.488627433776855
Epoch 40, val loss: 1.8786113262176514
Epoch 50, training loss: 18.026779174804688 = 1.8557133674621582 + 2.0 * 8.085533142089844
Epoch 50, val loss: 1.8551374673843384
Epoch 60, training loss: 17.258930206298828 = 1.8355497121810913 + 2.0 * 7.711690425872803
Epoch 60, val loss: 1.8374961614608765
Epoch 70, training loss: 16.406410217285156 = 1.8229331970214844 + 2.0 * 7.291738033294678
Epoch 70, val loss: 1.82666015625
Epoch 80, training loss: 15.906347274780273 = 1.8115330934524536 + 2.0 * 7.047407150268555
Epoch 80, val loss: 1.8167065382003784
Epoch 90, training loss: 15.595352172851562 = 1.8007737398147583 + 2.0 * 6.897289276123047
Epoch 90, val loss: 1.8068625926971436
Epoch 100, training loss: 15.393659591674805 = 1.7873129844665527 + 2.0 * 6.803173542022705
Epoch 100, val loss: 1.7947129011154175
Epoch 110, training loss: 15.233528137207031 = 1.7736293077468872 + 2.0 * 6.729949474334717
Epoch 110, val loss: 1.7828409671783447
Epoch 120, training loss: 15.116264343261719 = 1.760617733001709 + 2.0 * 6.677823066711426
Epoch 120, val loss: 1.77169930934906
Epoch 130, training loss: 15.022790908813477 = 1.7470741271972656 + 2.0 * 6.6378583908081055
Epoch 130, val loss: 1.760118007659912
Epoch 140, training loss: 14.936135292053223 = 1.732115626335144 + 2.0 * 6.6020097732543945
Epoch 140, val loss: 1.7473366260528564
Epoch 150, training loss: 14.85715389251709 = 1.7156282663345337 + 2.0 * 6.570762634277344
Epoch 150, val loss: 1.7334303855895996
Epoch 160, training loss: 14.785419464111328 = 1.6972858905792236 + 2.0 * 6.544066905975342
Epoch 160, val loss: 1.7180062532424927
Epoch 170, training loss: 14.724369049072266 = 1.6763989925384521 + 2.0 * 6.523984909057617
Epoch 170, val loss: 1.7004523277282715
Epoch 180, training loss: 14.660505294799805 = 1.65253484249115 + 2.0 * 6.503985404968262
Epoch 180, val loss: 1.680540919303894
Epoch 190, training loss: 14.59947395324707 = 1.6254692077636719 + 2.0 * 6.487002372741699
Epoch 190, val loss: 1.6577818393707275
Epoch 200, training loss: 14.543046951293945 = 1.594968318939209 + 2.0 * 6.474039554595947
Epoch 200, val loss: 1.6322139501571655
Epoch 210, training loss: 14.500239372253418 = 1.560774803161621 + 2.0 * 6.469732284545898
Epoch 210, val loss: 1.6037421226501465
Epoch 220, training loss: 14.434120178222656 = 1.5236988067626953 + 2.0 * 6.4552106857299805
Epoch 220, val loss: 1.5732802152633667
Epoch 230, training loss: 14.37686538696289 = 1.4840190410614014 + 2.0 * 6.446423053741455
Epoch 230, val loss: 1.5410412549972534
Epoch 240, training loss: 14.31861686706543 = 1.4418717622756958 + 2.0 * 6.438372611999512
Epoch 240, val loss: 1.507347822189331
Epoch 250, training loss: 14.260702133178711 = 1.397746205329895 + 2.0 * 6.431478023529053
Epoch 250, val loss: 1.4728186130523682
Epoch 260, training loss: 14.209108352661133 = 1.352895736694336 + 2.0 * 6.428106307983398
Epoch 260, val loss: 1.4387487173080444
Epoch 270, training loss: 14.14702033996582 = 1.308253526687622 + 2.0 * 6.419383525848389
Epoch 270, val loss: 1.4056365489959717
Epoch 280, training loss: 14.087919235229492 = 1.264127492904663 + 2.0 * 6.411895751953125
Epoch 280, val loss: 1.3734705448150635
Epoch 290, training loss: 14.036552429199219 = 1.2203820943832397 + 2.0 * 6.408085346221924
Epoch 290, val loss: 1.3424092531204224
Epoch 300, training loss: 13.983404159545898 = 1.1778836250305176 + 2.0 * 6.4027605056762695
Epoch 300, val loss: 1.3131088018417358
Epoch 310, training loss: 13.935274124145508 = 1.1371468305587769 + 2.0 * 6.399063587188721
Epoch 310, val loss: 1.2855067253112793
Epoch 320, training loss: 13.882871627807617 = 1.0975710153579712 + 2.0 * 6.392650127410889
Epoch 320, val loss: 1.2592628002166748
Epoch 330, training loss: 13.835634231567383 = 1.0590660572052002 + 2.0 * 6.388284206390381
Epoch 330, val loss: 1.2342418432235718
Epoch 340, training loss: 13.793663024902344 = 1.0216069221496582 + 2.0 * 6.386027812957764
Epoch 340, val loss: 1.2104945182800293
Epoch 350, training loss: 13.748175621032715 = 0.9854042530059814 + 2.0 * 6.381385803222656
Epoch 350, val loss: 1.1878817081451416
Epoch 360, training loss: 13.702266693115234 = 0.9505325555801392 + 2.0 * 6.375866889953613
Epoch 360, val loss: 1.1665382385253906
Epoch 370, training loss: 13.665735244750977 = 0.9166674613952637 + 2.0 * 6.3745341300964355
Epoch 370, val loss: 1.1463916301727295
Epoch 380, training loss: 13.622090339660645 = 0.8841972947120667 + 2.0 * 6.368946552276611
Epoch 380, val loss: 1.1271306276321411
Epoch 390, training loss: 13.582340240478516 = 0.8528385758399963 + 2.0 * 6.364750862121582
Epoch 390, val loss: 1.1091736555099487
Epoch 400, training loss: 13.546686172485352 = 0.8225301504135132 + 2.0 * 6.3620781898498535
Epoch 400, val loss: 1.092301607131958
Epoch 410, training loss: 13.510890007019043 = 0.7934360504150391 + 2.0 * 6.358726978302002
Epoch 410, val loss: 1.0763858556747437
Epoch 420, training loss: 13.472151756286621 = 0.7655318975448608 + 2.0 * 6.3533101081848145
Epoch 420, val loss: 1.0618822574615479
Epoch 430, training loss: 13.440156936645508 = 0.7387900352478027 + 2.0 * 6.350683689117432
Epoch 430, val loss: 1.0485620498657227
Epoch 440, training loss: 13.416488647460938 = 0.7131467461585999 + 2.0 * 6.351670742034912
Epoch 440, val loss: 1.03632390499115
Epoch 450, training loss: 13.385826110839844 = 0.6887054443359375 + 2.0 * 6.348560333251953
Epoch 450, val loss: 1.0255616903305054
Epoch 460, training loss: 13.351360321044922 = 0.6654355525970459 + 2.0 * 6.342962265014648
Epoch 460, val loss: 1.0162303447723389
Epoch 470, training loss: 13.325783729553223 = 0.6431860327720642 + 2.0 * 6.341299057006836
Epoch 470, val loss: 1.0081427097320557
Epoch 480, training loss: 13.296772956848145 = 0.6218165755271912 + 2.0 * 6.337478160858154
Epoch 480, val loss: 1.0013290643692017
Epoch 490, training loss: 13.275436401367188 = 0.6013460159301758 + 2.0 * 6.337045192718506
Epoch 490, val loss: 0.9954802393913269
Epoch 500, training loss: 13.24718189239502 = 0.5817569494247437 + 2.0 * 6.332712650299072
Epoch 500, val loss: 0.991020143032074
Epoch 510, training loss: 13.223733901977539 = 0.5628491640090942 + 2.0 * 6.330442428588867
Epoch 510, val loss: 0.9874227046966553
Epoch 520, training loss: 13.222344398498535 = 0.5445497632026672 + 2.0 * 6.338897228240967
Epoch 520, val loss: 0.9847732186317444
Epoch 530, training loss: 13.182229042053223 = 0.5269572138786316 + 2.0 * 6.327635765075684
Epoch 530, val loss: 0.9829052686691284
Epoch 540, training loss: 13.159148216247559 = 0.5098539590835571 + 2.0 * 6.324646949768066
Epoch 540, val loss: 0.9819138050079346
Epoch 550, training loss: 13.139225006103516 = 0.4931950867176056 + 2.0 * 6.323014736175537
Epoch 550, val loss: 0.9814853072166443
Epoch 560, training loss: 13.143295288085938 = 0.4769081771373749 + 2.0 * 6.333193778991699
Epoch 560, val loss: 0.981667160987854
Epoch 570, training loss: 13.105391502380371 = 0.46120375394821167 + 2.0 * 6.322093963623047
Epoch 570, val loss: 0.9823618531227112
Epoch 580, training loss: 13.083704948425293 = 0.4459218680858612 + 2.0 * 6.318891525268555
Epoch 580, val loss: 0.98375403881073
Epoch 590, training loss: 13.063800811767578 = 0.4310043454170227 + 2.0 * 6.3163981437683105
Epoch 590, val loss: 0.9855818152427673
Epoch 600, training loss: 13.045207977294922 = 0.4163636565208435 + 2.0 * 6.314422130584717
Epoch 600, val loss: 0.9879307746887207
Epoch 610, training loss: 13.046857833862305 = 0.401979923248291 + 2.0 * 6.322438716888428
Epoch 610, val loss: 0.9907557964324951
Epoch 620, training loss: 13.017450332641602 = 0.3880451023578644 + 2.0 * 6.31470251083374
Epoch 620, val loss: 0.9937741756439209
Epoch 630, training loss: 12.996166229248047 = 0.37441208958625793 + 2.0 * 6.310876846313477
Epoch 630, val loss: 0.9972805380821228
Epoch 640, training loss: 12.979248046875 = 0.36105766892433167 + 2.0 * 6.30909538269043
Epoch 640, val loss: 1.0011911392211914
Epoch 650, training loss: 12.976968765258789 = 0.3479359745979309 + 2.0 * 6.314516544342041
Epoch 650, val loss: 1.00546133518219
Epoch 660, training loss: 12.952424049377441 = 0.3351350724697113 + 2.0 * 6.3086442947387695
Epoch 660, val loss: 1.0100187063217163
Epoch 670, training loss: 12.933370590209961 = 0.3225593864917755 + 2.0 * 6.305405616760254
Epoch 670, val loss: 1.014973521232605
Epoch 680, training loss: 12.921504974365234 = 0.31022676825523376 + 2.0 * 6.305639266967773
Epoch 680, val loss: 1.0201513767242432
Epoch 690, training loss: 12.908052444458008 = 0.29815873503685 + 2.0 * 6.3049468994140625
Epoch 690, val loss: 1.0255388021469116
Epoch 700, training loss: 12.890203475952148 = 0.2863246202468872 + 2.0 * 6.301939487457275
Epoch 700, val loss: 1.0312672853469849
Epoch 710, training loss: 12.877063751220703 = 0.27478882670402527 + 2.0 * 6.301137447357178
Epoch 710, val loss: 1.0373250246047974
Epoch 720, training loss: 12.870988845825195 = 0.2635211646556854 + 2.0 * 6.303733825683594
Epoch 720, val loss: 1.0435850620269775
Epoch 730, training loss: 12.849320411682129 = 0.252606600522995 + 2.0 * 6.298357009887695
Epoch 730, val loss: 1.050119161605835
Epoch 740, training loss: 12.838094711303711 = 0.2419734001159668 + 2.0 * 6.298060417175293
Epoch 740, val loss: 1.056870698928833
Epoch 750, training loss: 12.82760238647461 = 0.2316826581954956 + 2.0 * 6.297959804534912
Epoch 750, val loss: 1.0638995170593262
Epoch 760, training loss: 12.813469886779785 = 0.22175724804401398 + 2.0 * 6.295856475830078
Epoch 760, val loss: 1.071035385131836
Epoch 770, training loss: 12.802265167236328 = 0.21215605735778809 + 2.0 * 6.2950544357299805
Epoch 770, val loss: 1.0785073041915894
Epoch 780, training loss: 12.791091918945312 = 0.20293571054935455 + 2.0 * 6.2940778732299805
Epoch 780, val loss: 1.0861400365829468
Epoch 790, training loss: 12.779535293579102 = 0.19408701360225677 + 2.0 * 6.292724132537842
Epoch 790, val loss: 1.0939595699310303
Epoch 800, training loss: 12.771170616149902 = 0.18556790053844452 + 2.0 * 6.292801380157471
Epoch 800, val loss: 1.101967692375183
Epoch 810, training loss: 12.756680488586426 = 0.1774371713399887 + 2.0 * 6.289621829986572
Epoch 810, val loss: 1.1100739240646362
Epoch 820, training loss: 12.7465238571167 = 0.16963307559490204 + 2.0 * 6.288445472717285
Epoch 820, val loss: 1.118301272392273
Epoch 830, training loss: 12.747136116027832 = 0.16217140853405 + 2.0 * 6.292482376098633
Epoch 830, val loss: 1.1267191171646118
Epoch 840, training loss: 12.735408782958984 = 0.1550685465335846 + 2.0 * 6.290170192718506
Epoch 840, val loss: 1.1352030038833618
Epoch 850, training loss: 12.719773292541504 = 0.14827373623847961 + 2.0 * 6.285749912261963
Epoch 850, val loss: 1.1436517238616943
Epoch 860, training loss: 12.710395812988281 = 0.14180006086826324 + 2.0 * 6.284297943115234
Epoch 860, val loss: 1.1524035930633545
Epoch 870, training loss: 12.700814247131348 = 0.13560917973518372 + 2.0 * 6.282602310180664
Epoch 870, val loss: 1.1613194942474365
Epoch 880, training loss: 12.701449394226074 = 0.12968875467777252 + 2.0 * 6.285880088806152
Epoch 880, val loss: 1.170326828956604
Epoch 890, training loss: 12.691482543945312 = 0.12405763566493988 + 2.0 * 6.283712387084961
Epoch 890, val loss: 1.1792511940002441
Epoch 900, training loss: 12.683745384216309 = 0.11870347708463669 + 2.0 * 6.282520771026611
Epoch 900, val loss: 1.1882637739181519
Epoch 910, training loss: 12.678725242614746 = 0.11360213160514832 + 2.0 * 6.282561779022217
Epoch 910, val loss: 1.1973904371261597
Epoch 920, training loss: 12.677644729614258 = 0.10876067727804184 + 2.0 * 6.284441947937012
Epoch 920, val loss: 1.206410527229309
Epoch 930, training loss: 12.662970542907715 = 0.10414798557758331 + 2.0 * 6.279411315917969
Epoch 930, val loss: 1.2154338359832764
Epoch 940, training loss: 12.653726577758789 = 0.09977530688047409 + 2.0 * 6.276975631713867
Epoch 940, val loss: 1.2245913743972778
Epoch 950, training loss: 12.648359298706055 = 0.0956050306558609 + 2.0 * 6.276377201080322
Epoch 950, val loss: 1.2337615489959717
Epoch 960, training loss: 12.66456127166748 = 0.09164181351661682 + 2.0 * 6.286459922790527
Epoch 960, val loss: 1.2428776025772095
Epoch 970, training loss: 12.637384414672852 = 0.08785270899534225 + 2.0 * 6.274765968322754
Epoch 970, val loss: 1.2517942190170288
Epoch 980, training loss: 12.635781288146973 = 0.08426150679588318 + 2.0 * 6.275759696960449
Epoch 980, val loss: 1.260819673538208
Epoch 990, training loss: 12.639114379882812 = 0.08084701746702194 + 2.0 * 6.2791337966918945
Epoch 990, val loss: 1.2697921991348267
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7333333333333334
0.804955192409067
=== training gcn model ===
Epoch 0, training loss: 19.11627769470215 = 1.9225672483444214 + 2.0 * 8.596855163574219
Epoch 0, val loss: 1.9185824394226074
Epoch 10, training loss: 19.107149124145508 = 1.9139972925186157 + 2.0 * 8.596575736999512
Epoch 10, val loss: 1.9101004600524902
Epoch 20, training loss: 19.091575622558594 = 1.9032020568847656 + 2.0 * 8.594186782836914
Epoch 20, val loss: 1.8994715213775635
Epoch 30, training loss: 19.037691116333008 = 1.8882801532745361 + 2.0 * 8.574705123901367
Epoch 30, val loss: 1.8850181102752686
Epoch 40, training loss: 18.790754318237305 = 1.8690792322158813 + 2.0 * 8.460837364196777
Epoch 40, val loss: 1.8670365810394287
Epoch 50, training loss: 17.95099449157715 = 1.8479079008102417 + 2.0 * 8.051543235778809
Epoch 50, val loss: 1.8473960161209106
Epoch 60, training loss: 17.452774047851562 = 1.8279329538345337 + 2.0 * 7.81242036819458
Epoch 60, val loss: 1.8290292024612427
Epoch 70, training loss: 16.690567016601562 = 1.8134894371032715 + 2.0 * 7.438538551330566
Epoch 70, val loss: 1.8156789541244507
Epoch 80, training loss: 15.984594345092773 = 1.804534673690796 + 2.0 * 7.090029716491699
Epoch 80, val loss: 1.8074568510055542
Epoch 90, training loss: 15.577620506286621 = 1.7943328619003296 + 2.0 * 6.89164400100708
Epoch 90, val loss: 1.7972830533981323
Epoch 100, training loss: 15.357439041137695 = 1.780574917793274 + 2.0 * 6.7884321212768555
Epoch 100, val loss: 1.7847756147384644
Epoch 110, training loss: 15.21210765838623 = 1.766075849533081 + 2.0 * 6.723015785217285
Epoch 110, val loss: 1.7719866037368774
Epoch 120, training loss: 15.080114364624023 = 1.751516580581665 + 2.0 * 6.664299011230469
Epoch 120, val loss: 1.7589094638824463
Epoch 130, training loss: 14.981709480285645 = 1.7357451915740967 + 2.0 * 6.622982025146484
Epoch 130, val loss: 1.7444331645965576
Epoch 140, training loss: 14.901679039001465 = 1.7176933288574219 + 2.0 * 6.5919928550720215
Epoch 140, val loss: 1.7280622720718384
Epoch 150, training loss: 14.828654289245605 = 1.6970880031585693 + 2.0 * 6.5657830238342285
Epoch 150, val loss: 1.709558129310608
Epoch 160, training loss: 14.76516056060791 = 1.6732804775238037 + 2.0 * 6.545939922332764
Epoch 160, val loss: 1.6883100271224976
Epoch 170, training loss: 14.71679401397705 = 1.6459009647369385 + 2.0 * 6.535446643829346
Epoch 170, val loss: 1.6641663312911987
Epoch 180, training loss: 14.649880409240723 = 1.6157453060150146 + 2.0 * 6.5170674324035645
Epoch 180, val loss: 1.6380178928375244
Epoch 190, training loss: 14.592958450317383 = 1.5827877521514893 + 2.0 * 6.505085468292236
Epoch 190, val loss: 1.6099441051483154
Epoch 200, training loss: 14.532353401184082 = 1.5469255447387695 + 2.0 * 6.492713928222656
Epoch 200, val loss: 1.579890489578247
Epoch 210, training loss: 14.469685554504395 = 1.5083674192428589 + 2.0 * 6.480659008026123
Epoch 210, val loss: 1.548410415649414
Epoch 220, training loss: 14.406533241271973 = 1.467673420906067 + 2.0 * 6.469429969787598
Epoch 220, val loss: 1.5161385536193848
Epoch 230, training loss: 14.34878158569336 = 1.425915241241455 + 2.0 * 6.461433410644531
Epoch 230, val loss: 1.4842171669006348
Epoch 240, training loss: 14.285966873168945 = 1.3847848176956177 + 2.0 * 6.450591087341309
Epoch 240, val loss: 1.4541146755218506
Epoch 250, training loss: 14.227413177490234 = 1.3444561958312988 + 2.0 * 6.441478252410889
Epoch 250, val loss: 1.425992727279663
Epoch 260, training loss: 14.170653343200684 = 1.3052772283554077 + 2.0 * 6.432688236236572
Epoch 260, val loss: 1.4000712633132935
Epoch 270, training loss: 14.118585586547852 = 1.2675198316574097 + 2.0 * 6.425532817840576
Epoch 270, val loss: 1.3763011693954468
Epoch 280, training loss: 14.067249298095703 = 1.2308435440063477 + 2.0 * 6.418202877044678
Epoch 280, val loss: 1.3541574478149414
Epoch 290, training loss: 14.017766952514648 = 1.1951498985290527 + 2.0 * 6.411308765411377
Epoch 290, val loss: 1.333508014678955
Epoch 300, training loss: 13.974492073059082 = 1.1604117155075073 + 2.0 * 6.407040119171143
Epoch 300, val loss: 1.3138160705566406
Epoch 310, training loss: 13.925247192382812 = 1.126216173171997 + 2.0 * 6.399515628814697
Epoch 310, val loss: 1.2947421073913574
Epoch 320, training loss: 13.879838943481445 = 1.0921578407287598 + 2.0 * 6.393840312957764
Epoch 320, val loss: 1.2759374380111694
Epoch 330, training loss: 13.844091415405273 = 1.05818510055542 + 2.0 * 6.392952919006348
Epoch 330, val loss: 1.2573095560073853
Epoch 340, training loss: 13.79822826385498 = 1.0245589017868042 + 2.0 * 6.386834621429443
Epoch 340, val loss: 1.238817572593689
Epoch 350, training loss: 13.758723258972168 = 0.9911778569221497 + 2.0 * 6.383772850036621
Epoch 350, val loss: 1.220647931098938
Epoch 360, training loss: 13.713253021240234 = 0.9582908749580383 + 2.0 * 6.377480983734131
Epoch 360, val loss: 1.2027113437652588
Epoch 370, training loss: 13.672955513000488 = 0.9257398247718811 + 2.0 * 6.373607635498047
Epoch 370, val loss: 1.1852511167526245
Epoch 380, training loss: 13.635441780090332 = 0.8935948014259338 + 2.0 * 6.3709235191345215
Epoch 380, val loss: 1.16815984249115
Epoch 390, training loss: 13.597861289978027 = 0.8621101975440979 + 2.0 * 6.367875576019287
Epoch 390, val loss: 1.1515905857086182
Epoch 400, training loss: 13.560850143432617 = 0.8313916921615601 + 2.0 * 6.364729404449463
Epoch 400, val loss: 1.1357680559158325
Epoch 410, training loss: 13.523433685302734 = 0.8013706803321838 + 2.0 * 6.361031532287598
Epoch 410, val loss: 1.1206674575805664
Epoch 420, training loss: 13.498631477355957 = 0.7720063328742981 + 2.0 * 6.363312721252441
Epoch 420, val loss: 1.1062642335891724
Epoch 430, training loss: 13.456798553466797 = 0.7433131337165833 + 2.0 * 6.356742858886719
Epoch 430, val loss: 1.0924898386001587
Epoch 440, training loss: 13.421290397644043 = 0.7152710556983948 + 2.0 * 6.3530097007751465
Epoch 440, val loss: 1.0794576406478882
Epoch 450, training loss: 13.401556968688965 = 0.6877533793449402 + 2.0 * 6.3569016456604
Epoch 450, val loss: 1.0671272277832031
Epoch 460, training loss: 13.359752655029297 = 0.6610016226768494 + 2.0 * 6.3493757247924805
Epoch 460, val loss: 1.055356502532959
Epoch 470, training loss: 13.328049659729004 = 0.6348435282707214 + 2.0 * 6.346602916717529
Epoch 470, val loss: 1.0445128679275513
Epoch 480, training loss: 13.29686450958252 = 0.6092343926429749 + 2.0 * 6.343814849853516
Epoch 480, val loss: 1.0342453718185425
Epoch 490, training loss: 13.28106689453125 = 0.5840590000152588 + 2.0 * 6.348504066467285
Epoch 490, val loss: 1.0245273113250732
Epoch 500, training loss: 13.24030876159668 = 0.5595571994781494 + 2.0 * 6.340375900268555
Epoch 500, val loss: 1.015610694885254
Epoch 510, training loss: 13.211915016174316 = 0.535536527633667 + 2.0 * 6.338189125061035
Epoch 510, val loss: 1.0071443319320679
Epoch 520, training loss: 13.184029579162598 = 0.511959969997406 + 2.0 * 6.336034774780273
Epoch 520, val loss: 0.9994234442710876
Epoch 530, training loss: 13.158645629882812 = 0.48888686299324036 + 2.0 * 6.334879398345947
Epoch 530, val loss: 0.9922993779182434
Epoch 540, training loss: 13.132584571838379 = 0.46633535623550415 + 2.0 * 6.33312463760376
Epoch 540, val loss: 0.9857900142669678
Epoch 550, training loss: 13.106704711914062 = 0.44432497024536133 + 2.0 * 6.3311896324157715
Epoch 550, val loss: 0.9798381924629211
Epoch 560, training loss: 13.080591201782227 = 0.4229131042957306 + 2.0 * 6.32883882522583
Epoch 560, val loss: 0.9744250178337097
Epoch 570, training loss: 13.058948516845703 = 0.40205812454223633 + 2.0 * 6.328444957733154
Epoch 570, val loss: 0.9696959257125854
Epoch 580, training loss: 13.040092468261719 = 0.38189518451690674 + 2.0 * 6.329098701477051
Epoch 580, val loss: 0.965731143951416
Epoch 590, training loss: 13.011407852172852 = 0.36242374777793884 + 2.0 * 6.32449197769165
Epoch 590, val loss: 0.9620777368545532
Epoch 600, training loss: 12.987642288208008 = 0.34364765882492065 + 2.0 * 6.321997165679932
Epoch 600, val loss: 0.9591916799545288
Epoch 610, training loss: 12.966741561889648 = 0.3255840837955475 + 2.0 * 6.320578575134277
Epoch 610, val loss: 0.9568057656288147
Epoch 620, training loss: 12.965359687805176 = 0.3082062005996704 + 2.0 * 6.328576564788818
Epoch 620, val loss: 0.9548079967498779
Epoch 630, training loss: 12.926898956298828 = 0.2917267978191376 + 2.0 * 6.3175859451293945
Epoch 630, val loss: 0.9537509679794312
Epoch 640, training loss: 12.907859802246094 = 0.2759735882282257 + 2.0 * 6.315943241119385
Epoch 640, val loss: 0.953305184841156
Epoch 650, training loss: 12.888051986694336 = 0.26098060607910156 + 2.0 * 6.313535690307617
Epoch 650, val loss: 0.9533509612083435
Epoch 660, training loss: 12.871808052062988 = 0.24669238924980164 + 2.0 * 6.312557697296143
Epoch 660, val loss: 0.9539631009101868
Epoch 670, training loss: 12.859772682189941 = 0.2331325113773346 + 2.0 * 6.313320159912109
Epoch 670, val loss: 0.9551395177841187
Epoch 680, training loss: 12.843268394470215 = 0.22038616240024567 + 2.0 * 6.311440944671631
Epoch 680, val loss: 0.9568281173706055
Epoch 690, training loss: 12.824993133544922 = 0.2083798199892044 + 2.0 * 6.308306694030762
Epoch 690, val loss: 0.9590734243392944
Epoch 700, training loss: 12.81455135345459 = 0.1970583200454712 + 2.0 * 6.308746337890625
Epoch 700, val loss: 0.9619277119636536
Epoch 710, training loss: 12.800963401794434 = 0.18640346825122833 + 2.0 * 6.30728006362915
Epoch 710, val loss: 0.965303897857666
Epoch 720, training loss: 12.788911819458008 = 0.17640389502048492 + 2.0 * 6.306253910064697
Epoch 720, val loss: 0.9689958095550537
Epoch 730, training loss: 12.77597427368164 = 0.16704636812210083 + 2.0 * 6.304463863372803
Epoch 730, val loss: 0.9732471704483032
Epoch 740, training loss: 12.763406753540039 = 0.1582786738872528 + 2.0 * 6.3025641441345215
Epoch 740, val loss: 0.9778130650520325
Epoch 750, training loss: 12.76020622253418 = 0.15003380179405212 + 2.0 * 6.305086135864258
Epoch 750, val loss: 0.982749879360199
Epoch 760, training loss: 12.747591018676758 = 0.14230439066886902 + 2.0 * 6.302643299102783
Epoch 760, val loss: 0.9881023168563843
Epoch 770, training loss: 12.735779762268066 = 0.13508349657058716 + 2.0 * 6.300348281860352
Epoch 770, val loss: 0.993718147277832
Epoch 780, training loss: 12.725156784057617 = 0.12828834354877472 + 2.0 * 6.298434257507324
Epoch 780, val loss: 0.9996790885925293
Epoch 790, training loss: 12.720840454101562 = 0.12190937250852585 + 2.0 * 6.299465656280518
Epoch 790, val loss: 1.0059324502944946
Epoch 800, training loss: 12.715209007263184 = 0.11590878665447235 + 2.0 * 6.299650192260742
Epoch 800, val loss: 1.012113094329834
Epoch 810, training loss: 12.702851295471191 = 0.11027491837739944 + 2.0 * 6.296288013458252
Epoch 810, val loss: 1.0187007188796997
Epoch 820, training loss: 12.693452835083008 = 0.10498712211847305 + 2.0 * 6.2942328453063965
Epoch 820, val loss: 1.0254714488983154
Epoch 830, training loss: 12.68653392791748 = 0.10000131279230118 + 2.0 * 6.293266296386719
Epoch 830, val loss: 1.0322946310043335
Epoch 840, training loss: 12.701089859008789 = 0.09529488533735275 + 2.0 * 6.3028974533081055
Epoch 840, val loss: 1.039245843887329
Epoch 850, training loss: 12.676738739013672 = 0.09088772535324097 + 2.0 * 6.2929253578186035
Epoch 850, val loss: 1.0462052822113037
Epoch 860, training loss: 12.673371315002441 = 0.08673762530088425 + 2.0 * 6.293316841125488
Epoch 860, val loss: 1.0532135963439941
Epoch 870, training loss: 12.665277481079102 = 0.0828312560915947 + 2.0 * 6.291223049163818
Epoch 870, val loss: 1.06032133102417
Epoch 880, training loss: 12.658596992492676 = 0.07916233688592911 + 2.0 * 6.289717197418213
Epoch 880, val loss: 1.067651391029358
Epoch 890, training loss: 12.651344299316406 = 0.07569529861211777 + 2.0 * 6.287824630737305
Epoch 890, val loss: 1.0748528242111206
Epoch 900, training loss: 12.655845642089844 = 0.07242538034915924 + 2.0 * 6.291709899902344
Epoch 900, val loss: 1.0820411443710327
Epoch 910, training loss: 12.651061058044434 = 0.06933388859033585 + 2.0 * 6.290863513946533
Epoch 910, val loss: 1.0894321203231812
Epoch 920, training loss: 12.642998695373535 = 0.06642948091030121 + 2.0 * 6.288284778594971
Epoch 920, val loss: 1.0966570377349854
Epoch 930, training loss: 12.634546279907227 = 0.06368429958820343 + 2.0 * 6.285430908203125
Epoch 930, val loss: 1.10383939743042
Epoch 940, training loss: 12.629396438598633 = 0.06109790503978729 + 2.0 * 6.284149169921875
Epoch 940, val loss: 1.1110458374023438
Epoch 950, training loss: 12.628073692321777 = 0.05865516886115074 + 2.0 * 6.284709453582764
Epoch 950, val loss: 1.1183339357376099
Epoch 960, training loss: 12.629948616027832 = 0.0563451386988163 + 2.0 * 6.286801815032959
Epoch 960, val loss: 1.1252734661102295
Epoch 970, training loss: 12.619527816772461 = 0.0541575662791729 + 2.0 * 6.282685279846191
Epoch 970, val loss: 1.1324117183685303
Epoch 980, training loss: 12.613908767700195 = 0.05209864303469658 + 2.0 * 6.280905246734619
Epoch 980, val loss: 1.1394648551940918
Epoch 990, training loss: 12.609029769897461 = 0.05015105754137039 + 2.0 * 6.279439449310303
Epoch 990, val loss: 1.1465047597885132
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7740740740740741
0.8107538218239325
The final CL Acc:0.75802, 0.01772, The final GNN Acc:0.80636, 0.00317
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13166])
remove edge: torch.Size([2, 7786])
updated graph: torch.Size([2, 10396])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 19.139320373535156 = 1.9455995559692383 + 2.0 * 8.5968599319458
Epoch 0, val loss: 1.9361276626586914
Epoch 10, training loss: 19.127117156982422 = 1.9341012239456177 + 2.0 * 8.596508026123047
Epoch 10, val loss: 1.9250563383102417
Epoch 20, training loss: 19.105426788330078 = 1.9194523096084595 + 2.0 * 8.592987060546875
Epoch 20, val loss: 1.9106115102767944
Epoch 30, training loss: 19.033756256103516 = 1.8992507457733154 + 2.0 * 8.567253112792969
Epoch 30, val loss: 1.8906203508377075
Epoch 40, training loss: 18.757402420043945 = 1.873555302619934 + 2.0 * 8.441923141479492
Epoch 40, val loss: 1.8661162853240967
Epoch 50, training loss: 17.983858108520508 = 1.8439055681228638 + 2.0 * 8.069975852966309
Epoch 50, val loss: 1.8387383222579956
Epoch 60, training loss: 17.623308181762695 = 1.817188024520874 + 2.0 * 7.903059959411621
Epoch 60, val loss: 1.8159124851226807
Epoch 70, training loss: 16.940410614013672 = 1.798386573791504 + 2.0 * 7.571012020111084
Epoch 70, val loss: 1.8001691102981567
Epoch 80, training loss: 16.18468475341797 = 1.7881189584732056 + 2.0 * 7.1982831954956055
Epoch 80, val loss: 1.7922965288162231
Epoch 90, training loss: 15.918167114257812 = 1.7789703607559204 + 2.0 * 7.069598197937012
Epoch 90, val loss: 1.7839349508285522
Epoch 100, training loss: 15.69027042388916 = 1.7649056911468506 + 2.0 * 6.962682247161865
Epoch 100, val loss: 1.7719420194625854
Epoch 110, training loss: 15.463830947875977 = 1.7513161897659302 + 2.0 * 6.856257438659668
Epoch 110, val loss: 1.761046290397644
Epoch 120, training loss: 15.302456855773926 = 1.7371160984039307 + 2.0 * 6.782670497894287
Epoch 120, val loss: 1.7494981288909912
Epoch 130, training loss: 15.165779113769531 = 1.7208763360977173 + 2.0 * 6.722451210021973
Epoch 130, val loss: 1.7361458539962769
Epoch 140, training loss: 15.054088592529297 = 1.7031856775283813 + 2.0 * 6.675451278686523
Epoch 140, val loss: 1.7216699123382568
Epoch 150, training loss: 14.95981502532959 = 1.6832271814346313 + 2.0 * 6.638293743133545
Epoch 150, val loss: 1.7053505182266235
Epoch 160, training loss: 14.882085800170898 = 1.660646915435791 + 2.0 * 6.610719203948975
Epoch 160, val loss: 1.6868234872817993
Epoch 170, training loss: 14.811579704284668 = 1.635339379310608 + 2.0 * 6.588119983673096
Epoch 170, val loss: 1.6660577058792114
Epoch 180, training loss: 14.741881370544434 = 1.6075975894927979 + 2.0 * 6.567142009735107
Epoch 180, val loss: 1.6434446573257446
Epoch 190, training loss: 14.66913890838623 = 1.5777270793914795 + 2.0 * 6.545705795288086
Epoch 190, val loss: 1.619211196899414
Epoch 200, training loss: 14.600818634033203 = 1.5455772876739502 + 2.0 * 6.527620792388916
Epoch 200, val loss: 1.5932512283325195
Epoch 210, training loss: 14.532257080078125 = 1.5114750862121582 + 2.0 * 6.510390758514404
Epoch 210, val loss: 1.565694808959961
Epoch 220, training loss: 14.465079307556152 = 1.475502848625183 + 2.0 * 6.49478816986084
Epoch 220, val loss: 1.5365960597991943
Epoch 230, training loss: 14.401241302490234 = 1.4376964569091797 + 2.0 * 6.481772422790527
Epoch 230, val loss: 1.5060654878616333
Epoch 240, training loss: 14.337835311889648 = 1.3986836671829224 + 2.0 * 6.469575881958008
Epoch 240, val loss: 1.4746408462524414
Epoch 250, training loss: 14.275328636169434 = 1.3590145111083984 + 2.0 * 6.458157062530518
Epoch 250, val loss: 1.4428460597991943
Epoch 260, training loss: 14.216056823730469 = 1.3186010122299194 + 2.0 * 6.448728084564209
Epoch 260, val loss: 1.4107040166854858
Epoch 270, training loss: 14.16001033782959 = 1.2775613069534302 + 2.0 * 6.441224575042725
Epoch 270, val loss: 1.3783191442489624
Epoch 280, training loss: 14.10139274597168 = 1.2365422248840332 + 2.0 * 6.432425022125244
Epoch 280, val loss: 1.3461662530899048
Epoch 290, training loss: 14.049877166748047 = 1.1960538625717163 + 2.0 * 6.4269118309021
Epoch 290, val loss: 1.3145917654037476
Epoch 300, training loss: 13.993770599365234 = 1.1557871103286743 + 2.0 * 6.418991565704346
Epoch 300, val loss: 1.283543586730957
Epoch 310, training loss: 13.942816734313965 = 1.1158026456832886 + 2.0 * 6.413506984710693
Epoch 310, val loss: 1.2528635263442993
Epoch 320, training loss: 13.902749061584473 = 1.0762022733688354 + 2.0 * 6.413273334503174
Epoch 320, val loss: 1.2227932214736938
Epoch 330, training loss: 13.845063209533691 = 1.037406325340271 + 2.0 * 6.4038286209106445
Epoch 330, val loss: 1.1935566663742065
Epoch 340, training loss: 13.797929763793945 = 0.9993264675140381 + 2.0 * 6.399301528930664
Epoch 340, val loss: 1.165183186531067
Epoch 350, training loss: 13.751609802246094 = 0.9617252945899963 + 2.0 * 6.394942283630371
Epoch 350, val loss: 1.1374447345733643
Epoch 360, training loss: 13.719043731689453 = 0.9246468544006348 + 2.0 * 6.39719820022583
Epoch 360, val loss: 1.1103205680847168
Epoch 370, training loss: 13.664456367492676 = 0.8884674310684204 + 2.0 * 6.387994289398193
Epoch 370, val loss: 1.0839653015136719
Epoch 380, training loss: 13.620830535888672 = 0.8530942797660828 + 2.0 * 6.383868217468262
Epoch 380, val loss: 1.0583624839782715
Epoch 390, training loss: 13.582610130310059 = 0.8184449076652527 + 2.0 * 6.382082462310791
Epoch 390, val loss: 1.0336583852767944
Epoch 400, training loss: 13.539213180541992 = 0.7847558856010437 + 2.0 * 6.377228736877441
Epoch 400, val loss: 1.0097688436508179
Epoch 410, training loss: 13.499882698059082 = 0.7521738409996033 + 2.0 * 6.373854637145996
Epoch 410, val loss: 0.9871480464935303
Epoch 420, training loss: 13.469921112060547 = 0.720680832862854 + 2.0 * 6.374619960784912
Epoch 420, val loss: 0.9656651020050049
Epoch 430, training loss: 13.426772117614746 = 0.690528154373169 + 2.0 * 6.368122100830078
Epoch 430, val loss: 0.94542396068573
Epoch 440, training loss: 13.38915729522705 = 0.6616806387901306 + 2.0 * 6.363738536834717
Epoch 440, val loss: 0.9267188906669617
Epoch 450, training loss: 13.35471248626709 = 0.6340391635894775 + 2.0 * 6.360336780548096
Epoch 450, val loss: 0.9094091057777405
Epoch 460, training loss: 13.336980819702148 = 0.6075975894927979 + 2.0 * 6.364691734313965
Epoch 460, val loss: 0.8934226036071777
Epoch 470, training loss: 13.293167114257812 = 0.582595705986023 + 2.0 * 6.35528564453125
Epoch 470, val loss: 0.8787791728973389
Epoch 480, training loss: 13.261433601379395 = 0.5587417483329773 + 2.0 * 6.351346015930176
Epoch 480, val loss: 0.8655173778533936
Epoch 490, training loss: 13.233169555664062 = 0.5359454154968262 + 2.0 * 6.348612308502197
Epoch 490, val loss: 0.8534190654754639
Epoch 500, training loss: 13.20949935913086 = 0.5141273140907288 + 2.0 * 6.347685813903809
Epoch 500, val loss: 0.8425715565681458
Epoch 510, training loss: 13.183194160461426 = 0.4933021068572998 + 2.0 * 6.344945907592773
Epoch 510, val loss: 0.8327000141143799
Epoch 520, training loss: 13.162541389465332 = 0.473319411277771 + 2.0 * 6.344611167907715
Epoch 520, val loss: 0.8239349126815796
Epoch 530, training loss: 13.132935523986816 = 0.45421257615089417 + 2.0 * 6.339361667633057
Epoch 530, val loss: 0.8160168528556824
Epoch 540, training loss: 13.114657402038574 = 0.43577662110328674 + 2.0 * 6.33944034576416
Epoch 540, val loss: 0.8088923096656799
Epoch 550, training loss: 13.089219093322754 = 0.41796261072158813 + 2.0 * 6.335628032684326
Epoch 550, val loss: 0.8024135231971741
Epoch 560, training loss: 13.077216148376465 = 0.40071165561676025 + 2.0 * 6.338252067565918
Epoch 560, val loss: 0.796467125415802
Epoch 570, training loss: 13.04748821258545 = 0.3840167224407196 + 2.0 * 6.331735610961914
Epoch 570, val loss: 0.7910659313201904
Epoch 580, training loss: 13.025175094604492 = 0.3677237033843994 + 2.0 * 6.328725814819336
Epoch 580, val loss: 0.7861698269844055
Epoch 590, training loss: 13.004327774047852 = 0.35176146030426025 + 2.0 * 6.326282978057861
Epoch 590, val loss: 0.7815960645675659
Epoch 600, training loss: 12.99072551727295 = 0.3360879719257355 + 2.0 * 6.3273186683654785
Epoch 600, val loss: 0.7773619890213013
Epoch 610, training loss: 12.978967666625977 = 0.32074612379074097 + 2.0 * 6.329110622406006
Epoch 610, val loss: 0.7733822464942932
Epoch 620, training loss: 12.948956489562988 = 0.3058567941188812 + 2.0 * 6.321549892425537
Epoch 620, val loss: 0.7699007987976074
Epoch 630, training loss: 12.930560111999512 = 0.29126104712486267 + 2.0 * 6.319649696350098
Epoch 630, val loss: 0.7666588425636292
Epoch 640, training loss: 12.935553550720215 = 0.2769412696361542 + 2.0 * 6.329306125640869
Epoch 640, val loss: 0.7637026906013489
Epoch 650, training loss: 12.899442672729492 = 0.2630905210971832 + 2.0 * 6.31817626953125
Epoch 650, val loss: 0.7611098885536194
Epoch 660, training loss: 12.881726264953613 = 0.24964095652103424 + 2.0 * 6.316042423248291
Epoch 660, val loss: 0.7590088248252869
Epoch 670, training loss: 12.862870216369629 = 0.23660244047641754 + 2.0 * 6.313133716583252
Epoch 670, val loss: 0.7572060823440552
Epoch 680, training loss: 12.858031272888184 = 0.22403594851493835 + 2.0 * 6.316997528076172
Epoch 680, val loss: 0.7559375166893005
Epoch 690, training loss: 12.839757919311523 = 0.21205370128154755 + 2.0 * 6.313852310180664
Epoch 690, val loss: 0.7547929883003235
Epoch 700, training loss: 12.819153785705566 = 0.20068228244781494 + 2.0 * 6.309235572814941
Epoch 700, val loss: 0.754365861415863
Epoch 710, training loss: 12.805404663085938 = 0.18988241255283356 + 2.0 * 6.307761192321777
Epoch 710, val loss: 0.7542219161987305
Epoch 720, training loss: 12.79412841796875 = 0.179629847407341 + 2.0 * 6.307249069213867
Epoch 720, val loss: 0.7545126676559448
Epoch 730, training loss: 12.781218528747559 = 0.16996270418167114 + 2.0 * 6.305627822875977
Epoch 730, val loss: 0.7551373839378357
Epoch 740, training loss: 12.770529747009277 = 0.16091245412826538 + 2.0 * 6.304808616638184
Epoch 740, val loss: 0.7563174366950989
Epoch 750, training loss: 12.758674621582031 = 0.15239503979682922 + 2.0 * 6.303139686584473
Epoch 750, val loss: 0.7577545642852783
Epoch 760, training loss: 12.76451301574707 = 0.14438356459140778 + 2.0 * 6.310064792633057
Epoch 760, val loss: 0.7594878077507019
Epoch 770, training loss: 12.73702621459961 = 0.1369035243988037 + 2.0 * 6.300061225891113
Epoch 770, val loss: 0.7615518569946289
Epoch 780, training loss: 12.727757453918457 = 0.12988772988319397 + 2.0 * 6.2989349365234375
Epoch 780, val loss: 0.7640155553817749
Epoch 790, training loss: 12.72275447845459 = 0.12330170720815659 + 2.0 * 6.299726486206055
Epoch 790, val loss: 0.766608476638794
Epoch 800, training loss: 12.711137771606445 = 0.1171315535902977 + 2.0 * 6.297003269195557
Epoch 800, val loss: 0.769355833530426
Epoch 810, training loss: 12.70096206665039 = 0.11136095970869064 + 2.0 * 6.294800758361816
Epoch 810, val loss: 0.7724162340164185
Epoch 820, training loss: 12.69339656829834 = 0.10595081746578217 + 2.0 * 6.293723106384277
Epoch 820, val loss: 0.775628387928009
Epoch 830, training loss: 12.702797889709473 = 0.1008603647351265 + 2.0 * 6.300968647003174
Epoch 830, val loss: 0.7789996862411499
Epoch 840, training loss: 12.68172550201416 = 0.0960894376039505 + 2.0 * 6.292818069458008
Epoch 840, val loss: 0.7823795676231384
Epoch 850, training loss: 12.676813125610352 = 0.09163301438093185 + 2.0 * 6.292590141296387
Epoch 850, val loss: 0.7861039042472839
Epoch 860, training loss: 12.667482376098633 = 0.08742737770080566 + 2.0 * 6.290027618408203
Epoch 860, val loss: 0.7898382544517517
Epoch 870, training loss: 12.660233497619629 = 0.08345893025398254 + 2.0 * 6.288387298583984
Epoch 870, val loss: 0.7937094569206238
Epoch 880, training loss: 12.659902572631836 = 0.07971744239330292 + 2.0 * 6.290092468261719
Epoch 880, val loss: 0.7977205514907837
Epoch 890, training loss: 12.656207084655762 = 0.07619300484657288 + 2.0 * 6.2900071144104
Epoch 890, val loss: 0.801724374294281
Epoch 900, training loss: 12.648423194885254 = 0.07287809252738953 + 2.0 * 6.2877726554870605
Epoch 900, val loss: 0.8058803081512451
Epoch 910, training loss: 12.641746520996094 = 0.06975574791431427 + 2.0 * 6.2859954833984375
Epoch 910, val loss: 0.8100560307502747
Epoch 920, training loss: 12.64512825012207 = 0.06681111454963684 + 2.0 * 6.289158344268799
Epoch 920, val loss: 0.8142090439796448
Epoch 930, training loss: 12.632074356079102 = 0.06403227895498276 + 2.0 * 6.284020900726318
Epoch 930, val loss: 0.8185657262802124
Epoch 940, training loss: 12.625439643859863 = 0.06140271574258804 + 2.0 * 6.282018661499023
Epoch 940, val loss: 0.8229262828826904
Epoch 950, training loss: 12.629836082458496 = 0.05890948325395584 + 2.0 * 6.285463333129883
Epoch 950, val loss: 0.8273255228996277
Epoch 960, training loss: 12.63215160369873 = 0.056556336581707 + 2.0 * 6.287797451019287
Epoch 960, val loss: 0.8316242694854736
Epoch 970, training loss: 12.614142417907715 = 0.05434079095721245 + 2.0 * 6.279901027679443
Epoch 970, val loss: 0.836101233959198
Epoch 980, training loss: 12.60966682434082 = 0.05224103853106499 + 2.0 * 6.278712749481201
Epoch 980, val loss: 0.84058678150177
Epoch 990, training loss: 12.605545997619629 = 0.05024688318371773 + 2.0 * 6.277649402618408
Epoch 990, val loss: 0.8450165390968323
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7925925925925926
0.8408012651555088
=== training gcn model ===
Epoch 0, training loss: 19.149837493896484 = 1.9560794830322266 + 2.0 * 8.596879005432129
Epoch 0, val loss: 1.9658359289169312
Epoch 10, training loss: 19.13872718811035 = 1.9454401731491089 + 2.0 * 8.596643447875977
Epoch 10, val loss: 1.9545321464538574
Epoch 20, training loss: 19.120895385742188 = 1.9325456619262695 + 2.0 * 8.5941743850708
Epoch 20, val loss: 1.9405535459518433
Epoch 30, training loss: 19.0587215423584 = 1.9153425693511963 + 2.0 * 8.57168960571289
Epoch 30, val loss: 1.921869158744812
Epoch 40, training loss: 18.73369598388672 = 1.8931965827941895 + 2.0 * 8.420249938964844
Epoch 40, val loss: 1.8981260061264038
Epoch 50, training loss: 17.92392921447754 = 1.8677104711532593 + 2.0 * 8.028109550476074
Epoch 50, val loss: 1.8719589710235596
Epoch 60, training loss: 17.492141723632812 = 1.8466289043426514 + 2.0 * 7.822756767272949
Epoch 60, val loss: 1.8511395454406738
Epoch 70, training loss: 16.550031661987305 = 1.8301974534988403 + 2.0 * 7.359917163848877
Epoch 70, val loss: 1.8350070714950562
Epoch 80, training loss: 15.859082221984863 = 1.8176041841506958 + 2.0 * 7.0207390785217285
Epoch 80, val loss: 1.8224434852600098
Epoch 90, training loss: 15.556756019592285 = 1.8033807277679443 + 2.0 * 6.876687526702881
Epoch 90, val loss: 1.8084443807601929
Epoch 100, training loss: 15.349477767944336 = 1.787522792816162 + 2.0 * 6.780977249145508
Epoch 100, val loss: 1.7931737899780273
Epoch 110, training loss: 15.230133056640625 = 1.7715535163879395 + 2.0 * 6.729290008544922
Epoch 110, val loss: 1.7781691551208496
Epoch 120, training loss: 15.106241226196289 = 1.7553167343139648 + 2.0 * 6.675462245941162
Epoch 120, val loss: 1.7628813982009888
Epoch 130, training loss: 15.003267288208008 = 1.7384753227233887 + 2.0 * 6.632396221160889
Epoch 130, val loss: 1.7470632791519165
Epoch 140, training loss: 14.907891273498535 = 1.7203341722488403 + 2.0 * 6.593778610229492
Epoch 140, val loss: 1.7302089929580688
Epoch 150, training loss: 14.826332092285156 = 1.6998549699783325 + 2.0 * 6.563238620758057
Epoch 150, val loss: 1.7114806175231934
Epoch 160, training loss: 14.758852005004883 = 1.676474928855896 + 2.0 * 6.541188716888428
Epoch 160, val loss: 1.6904653310775757
Epoch 170, training loss: 14.689595222473145 = 1.650551676750183 + 2.0 * 6.519521713256836
Epoch 170, val loss: 1.6673225164413452
Epoch 180, training loss: 14.626306533813477 = 1.6216055154800415 + 2.0 * 6.502350330352783
Epoch 180, val loss: 1.6416730880737305
Epoch 190, training loss: 14.571775436401367 = 1.5896632671356201 + 2.0 * 6.491055965423584
Epoch 190, val loss: 1.6136205196380615
Epoch 200, training loss: 14.50803279876709 = 1.5546395778656006 + 2.0 * 6.476696491241455
Epoch 200, val loss: 1.5831680297851562
Epoch 210, training loss: 14.447348594665527 = 1.5166301727294922 + 2.0 * 6.465359210968018
Epoch 210, val loss: 1.5504651069641113
Epoch 220, training loss: 14.389719009399414 = 1.4758507013320923 + 2.0 * 6.456933975219727
Epoch 220, val loss: 1.515838623046875
Epoch 230, training loss: 14.327731132507324 = 1.4330215454101562 + 2.0 * 6.447354793548584
Epoch 230, val loss: 1.4799377918243408
Epoch 240, training loss: 14.261574745178223 = 1.388900876045227 + 2.0 * 6.436336994171143
Epoch 240, val loss: 1.443526029586792
Epoch 250, training loss: 14.197528839111328 = 1.3437671661376953 + 2.0 * 6.426880836486816
Epoch 250, val loss: 1.4069116115570068
Epoch 260, training loss: 14.145805358886719 = 1.2984414100646973 + 2.0 * 6.423681735992432
Epoch 260, val loss: 1.370855450630188
Epoch 270, training loss: 14.076374053955078 = 1.254256248474121 + 2.0 * 6.4110589027404785
Epoch 270, val loss: 1.3364932537078857
Epoch 280, training loss: 14.016454696655273 = 1.2115635871887207 + 2.0 * 6.402445316314697
Epoch 280, val loss: 1.3038437366485596
Epoch 290, training loss: 13.969655990600586 = 1.1706324815750122 + 2.0 * 6.399511814117432
Epoch 290, val loss: 1.2731643915176392
Epoch 300, training loss: 13.911860466003418 = 1.1317914724349976 + 2.0 * 6.3900346755981445
Epoch 300, val loss: 1.2444849014282227
Epoch 310, training loss: 13.86336612701416 = 1.0951318740844727 + 2.0 * 6.384117126464844
Epoch 310, val loss: 1.2179222106933594
Epoch 320, training loss: 13.823208808898926 = 1.0604420900344849 + 2.0 * 6.381383419036865
Epoch 320, val loss: 1.193189024925232
Epoch 330, training loss: 13.775835990905762 = 1.0278714895248413 + 2.0 * 6.3739824295043945
Epoch 330, val loss: 1.1703298091888428
Epoch 340, training loss: 13.735958099365234 = 0.9970209002494812 + 2.0 * 6.369468688964844
Epoch 340, val loss: 1.148850917816162
Epoch 350, training loss: 13.704054832458496 = 0.9674877524375916 + 2.0 * 6.368283748626709
Epoch 350, val loss: 1.1284644603729248
Epoch 360, training loss: 13.666651725769043 = 0.9389612078666687 + 2.0 * 6.363845348358154
Epoch 360, val loss: 1.1087414026260376
Epoch 370, training loss: 13.633345603942871 = 0.9111807346343994 + 2.0 * 6.361082553863525
Epoch 370, val loss: 1.0895642042160034
Epoch 380, training loss: 13.594114303588867 = 0.883759081363678 + 2.0 * 6.355177402496338
Epoch 380, val loss: 1.0705132484436035
Epoch 390, training loss: 13.563821792602539 = 0.8565368056297302 + 2.0 * 6.353642463684082
Epoch 390, val loss: 1.051547884941101
Epoch 400, training loss: 13.52649211883545 = 0.8293828964233398 + 2.0 * 6.348554611206055
Epoch 400, val loss: 1.0325491428375244
Epoch 410, training loss: 13.504363059997559 = 0.8022394776344299 + 2.0 * 6.351061820983887
Epoch 410, val loss: 1.013418436050415
Epoch 420, training loss: 13.467618942260742 = 0.775221586227417 + 2.0 * 6.346198558807373
Epoch 420, val loss: 0.9942207336425781
Epoch 430, training loss: 13.4321928024292 = 0.7484148740768433 + 2.0 * 6.341888904571533
Epoch 430, val loss: 0.9751984477043152
Epoch 440, training loss: 13.399487495422363 = 0.7218528389930725 + 2.0 * 6.338817119598389
Epoch 440, val loss: 0.9564347863197327
Epoch 450, training loss: 13.368623733520508 = 0.6956233382225037 + 2.0 * 6.33650016784668
Epoch 450, val loss: 0.9379171133041382
Epoch 460, training loss: 13.358308792114258 = 0.6697874069213867 + 2.0 * 6.3442606925964355
Epoch 460, val loss: 0.9198733568191528
Epoch 470, training loss: 13.31208324432373 = 0.6446241736412048 + 2.0 * 6.3337297439575195
Epoch 470, val loss: 0.902458667755127
Epoch 480, training loss: 13.2792387008667 = 0.6200419068336487 + 2.0 * 6.329598426818848
Epoch 480, val loss: 0.8857346773147583
Epoch 490, training loss: 13.250515937805176 = 0.5958847403526306 + 2.0 * 6.327315807342529
Epoch 490, val loss: 0.8696188926696777
Epoch 500, training loss: 13.222333908081055 = 0.5720818042755127 + 2.0 * 6.3251261711120605
Epoch 500, val loss: 0.8540797233581543
Epoch 510, training loss: 13.195751190185547 = 0.5485997200012207 + 2.0 * 6.323575973510742
Epoch 510, val loss: 0.8391326665878296
Epoch 520, training loss: 13.174342155456543 = 0.5255013704299927 + 2.0 * 6.32442045211792
Epoch 520, val loss: 0.8248533010482788
Epoch 530, training loss: 13.145556449890137 = 0.5028888583183289 + 2.0 * 6.321333885192871
Epoch 530, val loss: 0.811288058757782
Epoch 540, training loss: 13.120087623596191 = 0.4807063043117523 + 2.0 * 6.319690704345703
Epoch 540, val loss: 0.7985115051269531
Epoch 550, training loss: 13.093870162963867 = 0.458813339471817 + 2.0 * 6.317528247833252
Epoch 550, val loss: 0.7864080667495728
Epoch 560, training loss: 13.076666831970215 = 0.4372074604034424 + 2.0 * 6.319729804992676
Epoch 560, val loss: 0.7749858498573303
Epoch 570, training loss: 13.043545722961426 = 0.4158848226070404 + 2.0 * 6.313830375671387
Epoch 570, val loss: 0.7642438411712646
Epoch 580, training loss: 13.020977973937988 = 0.3949061632156372 + 2.0 * 6.31303596496582
Epoch 580, val loss: 0.754241406917572
Epoch 590, training loss: 13.012651443481445 = 0.37435269355773926 + 2.0 * 6.319149494171143
Epoch 590, val loss: 0.7449988722801208
Epoch 600, training loss: 12.97696304321289 = 0.35422030091285706 + 2.0 * 6.311371326446533
Epoch 600, val loss: 0.7365915179252625
Epoch 610, training loss: 12.951602935791016 = 0.33468180894851685 + 2.0 * 6.308460712432861
Epoch 610, val loss: 0.7290706634521484
Epoch 620, training loss: 12.939821243286133 = 0.31571850180625916 + 2.0 * 6.312051296234131
Epoch 620, val loss: 0.7224235534667969
Epoch 630, training loss: 12.916566848754883 = 0.29749003052711487 + 2.0 * 6.3095383644104
Epoch 630, val loss: 0.7167361974716187
Epoch 640, training loss: 12.892205238342285 = 0.2800601124763489 + 2.0 * 6.30607271194458
Epoch 640, val loss: 0.7119513750076294
Epoch 650, training loss: 12.8699312210083 = 0.2634734809398651 + 2.0 * 6.303228855133057
Epoch 650, val loss: 0.7081727385520935
Epoch 660, training loss: 12.851762771606445 = 0.24771657586097717 + 2.0 * 6.302022933959961
Epoch 660, val loss: 0.7051963210105896
Epoch 670, training loss: 12.859244346618652 = 0.232819601893425 + 2.0 * 6.3132123947143555
Epoch 670, val loss: 0.703097939491272
Epoch 680, training loss: 12.820024490356445 = 0.2189023494720459 + 2.0 * 6.30056095123291
Epoch 680, val loss: 0.7018730044364929
Epoch 690, training loss: 12.805317878723145 = 0.20590941607952118 + 2.0 * 6.299704074859619
Epoch 690, val loss: 0.7015671133995056
Epoch 700, training loss: 12.791004180908203 = 0.19377760589122772 + 2.0 * 6.29861307144165
Epoch 700, val loss: 0.7020148634910583
Epoch 710, training loss: 12.790719985961914 = 0.18249060213565826 + 2.0 * 6.304114818572998
Epoch 710, val loss: 0.7031569480895996
Epoch 720, training loss: 12.770345687866211 = 0.17196007072925568 + 2.0 * 6.299192905426025
Epoch 720, val loss: 0.7047061324119568
Epoch 730, training loss: 12.754866600036621 = 0.16222766041755676 + 2.0 * 6.296319484710693
Epoch 730, val loss: 0.7070581912994385
Epoch 740, training loss: 12.74182415008545 = 0.15316757559776306 + 2.0 * 6.294328212738037
Epoch 740, val loss: 0.7098172307014465
Epoch 750, training loss: 12.748085975646973 = 0.14477264881134033 + 2.0 * 6.301656723022461
Epoch 750, val loss: 0.7130638957023621
Epoch 760, training loss: 12.723695755004883 = 0.13694769144058228 + 2.0 * 6.293374061584473
Epoch 760, val loss: 0.7165747880935669
Epoch 770, training loss: 12.710989952087402 = 0.12968945503234863 + 2.0 * 6.290650367736816
Epoch 770, val loss: 0.7205139994621277
Epoch 780, training loss: 12.720345497131348 = 0.12293729931116104 + 2.0 * 6.298704147338867
Epoch 780, val loss: 0.7247755527496338
Epoch 790, training loss: 12.70803165435791 = 0.11662443727254868 + 2.0 * 6.295703411102295
Epoch 790, val loss: 0.7290103435516357
Epoch 800, training loss: 12.688722610473633 = 0.11078039556741714 + 2.0 * 6.288970947265625
Epoch 800, val loss: 0.7337536811828613
Epoch 810, training loss: 12.680312156677246 = 0.10532225668430328 + 2.0 * 6.287495136260986
Epoch 810, val loss: 0.7386200428009033
Epoch 820, training loss: 12.672429084777832 = 0.1002197340130806 + 2.0 * 6.286104679107666
Epoch 820, val loss: 0.7436202168464661
Epoch 830, training loss: 12.666380882263184 = 0.09542831033468246 + 2.0 * 6.285476207733154
Epoch 830, val loss: 0.7487577199935913
Epoch 840, training loss: 12.681180000305176 = 0.09092394262552261 + 2.0 * 6.295127868652344
Epoch 840, val loss: 0.7538948059082031
Epoch 850, training loss: 12.6566801071167 = 0.08671937137842178 + 2.0 * 6.284980297088623
Epoch 850, val loss: 0.7591626644134521
Epoch 860, training loss: 12.649872779846191 = 0.0827798992395401 + 2.0 * 6.283546447753906
Epoch 860, val loss: 0.7645934820175171
Epoch 870, training loss: 12.644861221313477 = 0.07907909899950027 + 2.0 * 6.282891273498535
Epoch 870, val loss: 0.7700154781341553
Epoch 880, training loss: 12.651827812194824 = 0.07560287415981293 + 2.0 * 6.288112640380859
Epoch 880, val loss: 0.7754899263381958
Epoch 890, training loss: 12.635879516601562 = 0.07230672985315323 + 2.0 * 6.2817864418029785
Epoch 890, val loss: 0.7807809114456177
Epoch 900, training loss: 12.628989219665527 = 0.0692269504070282 + 2.0 * 6.279881000518799
Epoch 900, val loss: 0.7863736748695374
Epoch 910, training loss: 12.635321617126465 = 0.06631901115179062 + 2.0 * 6.284501075744629
Epoch 910, val loss: 0.7918494343757629
Epoch 920, training loss: 12.628959655761719 = 0.06358054280281067 + 2.0 * 6.282689571380615
Epoch 920, val loss: 0.7972557544708252
Epoch 930, training loss: 12.615797996520996 = 0.06099512800574303 + 2.0 * 6.277401447296143
Epoch 930, val loss: 0.8027153611183167
Epoch 940, training loss: 12.612102508544922 = 0.058560196310281754 + 2.0 * 6.276771068572998
Epoch 940, val loss: 0.8081746101379395
Epoch 950, training loss: 12.61475658416748 = 0.05625789612531662 + 2.0 * 6.27924919128418
Epoch 950, val loss: 0.813535213470459
Epoch 960, training loss: 12.603753089904785 = 0.054080791771411896 + 2.0 * 6.27483606338501
Epoch 960, val loss: 0.8189679384231567
Epoch 970, training loss: 12.601455688476562 = 0.05201909318566322 + 2.0 * 6.274718284606934
Epoch 970, val loss: 0.8243294358253479
Epoch 980, training loss: 12.604835510253906 = 0.05006781592965126 + 2.0 * 6.277383804321289
Epoch 980, val loss: 0.8296001553535461
Epoch 990, training loss: 12.593942642211914 = 0.04823059216141701 + 2.0 * 6.27285623550415
Epoch 990, val loss: 0.8350425958633423
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7962962962962963
0.8355297838692674
=== training gcn model ===
Epoch 0, training loss: 19.160409927368164 = 1.966722846031189 + 2.0 * 8.596843719482422
Epoch 0, val loss: 1.9640616178512573
Epoch 10, training loss: 19.14847183227539 = 1.9553574323654175 + 2.0 * 8.5965576171875
Epoch 10, val loss: 1.9531222581863403
Epoch 20, training loss: 19.129743576049805 = 1.9419491291046143 + 2.0 * 8.593896865844727
Epoch 20, val loss: 1.9399081468582153
Epoch 30, training loss: 19.068790435791016 = 1.924688458442688 + 2.0 * 8.572051048278809
Epoch 30, val loss: 1.9226359128952026
Epoch 40, training loss: 18.7789363861084 = 1.9033769369125366 + 2.0 * 8.437779426574707
Epoch 40, val loss: 1.9016814231872559
Epoch 50, training loss: 17.791133880615234 = 1.8798466920852661 + 2.0 * 7.955643653869629
Epoch 50, val loss: 1.8794431686401367
Epoch 60, training loss: 17.076892852783203 = 1.8590353727340698 + 2.0 * 7.60892915725708
Epoch 60, val loss: 1.8601124286651611
Epoch 70, training loss: 16.342384338378906 = 1.8421114683151245 + 2.0 * 7.250136852264404
Epoch 70, val loss: 1.8436492681503296
Epoch 80, training loss: 15.85930347442627 = 1.8268786668777466 + 2.0 * 7.016212463378906
Epoch 80, val loss: 1.8288792371749878
Epoch 90, training loss: 15.6201753616333 = 1.8120492696762085 + 2.0 * 6.9040632247924805
Epoch 90, val loss: 1.815040111541748
Epoch 100, training loss: 15.462440490722656 = 1.797330617904663 + 2.0 * 6.832554817199707
Epoch 100, val loss: 1.8011702299118042
Epoch 110, training loss: 15.307632446289062 = 1.7826097011566162 + 2.0 * 6.762511253356934
Epoch 110, val loss: 1.7868446111679077
Epoch 120, training loss: 15.176277160644531 = 1.768680453300476 + 2.0 * 6.703798294067383
Epoch 120, val loss: 1.7736618518829346
Epoch 130, training loss: 15.091951370239258 = 1.7542715072631836 + 2.0 * 6.668839931488037
Epoch 130, val loss: 1.7601540088653564
Epoch 140, training loss: 15.00322151184082 = 1.7379484176635742 + 2.0 * 6.632636547088623
Epoch 140, val loss: 1.7454322576522827
Epoch 150, training loss: 14.929506301879883 = 1.7205393314361572 + 2.0 * 6.604483604431152
Epoch 150, val loss: 1.7296266555786133
Epoch 160, training loss: 14.865925788879395 = 1.7014347314834595 + 2.0 * 6.582245349884033
Epoch 160, val loss: 1.7124440670013428
Epoch 170, training loss: 14.811417579650879 = 1.680059552192688 + 2.0 * 6.56567907333374
Epoch 170, val loss: 1.693401575088501
Epoch 180, training loss: 14.73059368133545 = 1.6565231084823608 + 2.0 * 6.5370354652404785
Epoch 180, val loss: 1.672613263130188
Epoch 190, training loss: 14.661478996276855 = 1.6304655075073242 + 2.0 * 6.515506744384766
Epoch 190, val loss: 1.6499289274215698
Epoch 200, training loss: 14.60554313659668 = 1.6011518239974976 + 2.0 * 6.502195835113525
Epoch 200, val loss: 1.624631404876709
Epoch 210, training loss: 14.527358055114746 = 1.5681753158569336 + 2.0 * 6.479591369628906
Epoch 210, val loss: 1.5966589450836182
Epoch 220, training loss: 14.458015441894531 = 1.5316171646118164 + 2.0 * 6.463199138641357
Epoch 220, val loss: 1.565911054611206
Epoch 230, training loss: 14.390345573425293 = 1.4912967681884766 + 2.0 * 6.449524402618408
Epoch 230, val loss: 1.532063603401184
Epoch 240, training loss: 14.324274063110352 = 1.4471455812454224 + 2.0 * 6.438564300537109
Epoch 240, val loss: 1.495604395866394
Epoch 250, training loss: 14.260015487670898 = 1.3999814987182617 + 2.0 * 6.430016994476318
Epoch 250, val loss: 1.4571254253387451
Epoch 260, training loss: 14.194598197937012 = 1.3501386642456055 + 2.0 * 6.422229766845703
Epoch 260, val loss: 1.4168933629989624
Epoch 270, training loss: 14.131559371948242 = 1.2981089353561401 + 2.0 * 6.416725158691406
Epoch 270, val loss: 1.3753937482833862
Epoch 280, training loss: 14.063536643981934 = 1.245038628578186 + 2.0 * 6.4092488288879395
Epoch 280, val loss: 1.3336577415466309
Epoch 290, training loss: 13.997527122497559 = 1.19167959690094 + 2.0 * 6.402923583984375
Epoch 290, val loss: 1.29214346408844
Epoch 300, training loss: 13.932297706604004 = 1.1387521028518677 + 2.0 * 6.396772861480713
Epoch 300, val loss: 1.2513896226882935
Epoch 310, training loss: 13.87310791015625 = 1.087261438369751 + 2.0 * 6.392923355102539
Epoch 310, val loss: 1.2120667695999146
Epoch 320, training loss: 13.816818237304688 = 1.0383644104003906 + 2.0 * 6.389226913452148
Epoch 320, val loss: 1.17496919631958
Epoch 330, training loss: 13.754694938659668 = 0.9921286106109619 + 2.0 * 6.381283283233643
Epoch 330, val loss: 1.140379786491394
Epoch 340, training loss: 13.697549819946289 = 0.94835364818573 + 2.0 * 6.374598026275635
Epoch 340, val loss: 1.1080166101455688
Epoch 350, training loss: 13.644952774047852 = 0.9067977070808411 + 2.0 * 6.369077682495117
Epoch 350, val loss: 1.0776556730270386
Epoch 360, training loss: 13.610920906066895 = 0.8674871921539307 + 2.0 * 6.3717169761657715
Epoch 360, val loss: 1.0493935346603394
Epoch 370, training loss: 13.554718017578125 = 0.8307414054870605 + 2.0 * 6.361988067626953
Epoch 370, val loss: 1.0235064029693604
Epoch 380, training loss: 13.509971618652344 = 0.7961578369140625 + 2.0 * 6.356906890869141
Epoch 380, val loss: 0.9997022747993469
Epoch 390, training loss: 13.468993186950684 = 0.7634867429733276 + 2.0 * 6.352753162384033
Epoch 390, val loss: 0.9777346253395081
Epoch 400, training loss: 13.42913818359375 = 0.7326118350028992 + 2.0 * 6.348263263702393
Epoch 400, val loss: 0.9576026201248169
Epoch 410, training loss: 13.392168998718262 = 0.7033539414405823 + 2.0 * 6.344407558441162
Epoch 410, val loss: 0.9391486048698425
Epoch 420, training loss: 13.36347770690918 = 0.6755099892616272 + 2.0 * 6.3439836502075195
Epoch 420, val loss: 0.9221245050430298
Epoch 430, training loss: 13.327020645141602 = 0.6491087079048157 + 2.0 * 6.338955879211426
Epoch 430, val loss: 0.9065789580345154
Epoch 440, training loss: 13.292487144470215 = 0.6239745020866394 + 2.0 * 6.334256172180176
Epoch 440, val loss: 0.8923048377037048
Epoch 450, training loss: 13.265425682067871 = 0.5998172163963318 + 2.0 * 6.332804203033447
Epoch 450, val loss: 0.8791987895965576
Epoch 460, training loss: 13.246201515197754 = 0.5767467617988586 + 2.0 * 6.3347272872924805
Epoch 460, val loss: 0.8671179413795471
Epoch 470, training loss: 13.208462715148926 = 0.5548442006111145 + 2.0 * 6.326809406280518
Epoch 470, val loss: 0.856279730796814
Epoch 480, training loss: 13.179676055908203 = 0.5337733030319214 + 2.0 * 6.322951316833496
Epoch 480, val loss: 0.8463172912597656
Epoch 490, training loss: 13.172338485717773 = 0.513439953327179 + 2.0 * 6.32944917678833
Epoch 490, val loss: 0.8370891213417053
Epoch 500, training loss: 13.137457847595215 = 0.4937841296195984 + 2.0 * 6.321836948394775
Epoch 500, val loss: 0.8285973072052002
Epoch 510, training loss: 13.111137390136719 = 0.47485029697418213 + 2.0 * 6.318143367767334
Epoch 510, val loss: 0.8207667469978333
Epoch 520, training loss: 13.088675498962402 = 0.4564271867275238 + 2.0 * 6.316123962402344
Epoch 520, val loss: 0.813542366027832
Epoch 530, training loss: 13.062767028808594 = 0.43844765424728394 + 2.0 * 6.312159538269043
Epoch 530, val loss: 0.8067113757133484
Epoch 540, training loss: 13.044723510742188 = 0.42080119252204895 + 2.0 * 6.3119611740112305
Epoch 540, val loss: 0.8002632856369019
Epoch 550, training loss: 13.022720336914062 = 0.4034096300601959 + 2.0 * 6.30965518951416
Epoch 550, val loss: 0.7941693067550659
Epoch 560, training loss: 13.011709213256836 = 0.38623329997062683 + 2.0 * 6.312737941741943
Epoch 560, val loss: 0.7884076237678528
Epoch 570, training loss: 12.98922348022461 = 0.36942246556282043 + 2.0 * 6.309900283813477
Epoch 570, val loss: 0.7827118635177612
Epoch 580, training loss: 12.962502479553223 = 0.3527999818325043 + 2.0 * 6.304851055145264
Epoch 580, val loss: 0.7773313522338867
Epoch 590, training loss: 12.939606666564941 = 0.3363974988460541 + 2.0 * 6.301604747772217
Epoch 590, val loss: 0.7721518278121948
Epoch 600, training loss: 12.920392036437988 = 0.3201826214790344 + 2.0 * 6.30010461807251
Epoch 600, val loss: 0.7670755386352539
Epoch 610, training loss: 12.917470932006836 = 0.30422449111938477 + 2.0 * 6.3066229820251465
Epoch 610, val loss: 0.7620630264282227
Epoch 620, training loss: 12.891242980957031 = 0.28852853178977966 + 2.0 * 6.301357269287109
Epoch 620, val loss: 0.7573380470275879
Epoch 630, training loss: 12.869056701660156 = 0.27335089445114136 + 2.0 * 6.297852993011475
Epoch 630, val loss: 0.7528430223464966
Epoch 640, training loss: 12.857931137084961 = 0.258546382188797 + 2.0 * 6.299692153930664
Epoch 640, val loss: 0.7484531402587891
Epoch 650, training loss: 12.834450721740723 = 0.2442777305841446 + 2.0 * 6.29508638381958
Epoch 650, val loss: 0.7444840669631958
Epoch 660, training loss: 12.816302299499512 = 0.23050473630428314 + 2.0 * 6.292898654937744
Epoch 660, val loss: 0.7407678961753845
Epoch 670, training loss: 12.80233097076416 = 0.21728746592998505 + 2.0 * 6.292521953582764
Epoch 670, val loss: 0.737440824508667
Epoch 680, training loss: 12.79102897644043 = 0.20467405021190643 + 2.0 * 6.293177604675293
Epoch 680, val loss: 0.7345215678215027
Epoch 690, training loss: 12.770934104919434 = 0.19273731112480164 + 2.0 * 6.289098262786865
Epoch 690, val loss: 0.7319915890693665
Epoch 700, training loss: 12.75747299194336 = 0.18148061633110046 + 2.0 * 6.287996292114258
Epoch 700, val loss: 0.7299460768699646
Epoch 710, training loss: 12.744681358337402 = 0.1708446741104126 + 2.0 * 6.2869181632995605
Epoch 710, val loss: 0.7282134294509888
Epoch 720, training loss: 12.74852466583252 = 0.16081133484840393 + 2.0 * 6.293856620788574
Epoch 720, val loss: 0.727103054523468
Epoch 730, training loss: 12.733138084411621 = 0.15150220692157745 + 2.0 * 6.290817737579346
Epoch 730, val loss: 0.7261304259300232
Epoch 740, training loss: 12.715072631835938 = 0.14279794692993164 + 2.0 * 6.286137104034424
Epoch 740, val loss: 0.7260251045227051
Epoch 750, training loss: 12.700942993164062 = 0.13467173278331757 + 2.0 * 6.283135414123535
Epoch 750, val loss: 0.7261864542961121
Epoch 760, training loss: 12.689549446105957 = 0.12706905603408813 + 2.0 * 6.281239986419678
Epoch 760, val loss: 0.7267743349075317
Epoch 770, training loss: 12.680425643920898 = 0.11996854841709137 + 2.0 * 6.280228614807129
Epoch 770, val loss: 0.7278053760528564
Epoch 780, training loss: 12.676502227783203 = 0.11332958936691284 + 2.0 * 6.281586170196533
Epoch 780, val loss: 0.7292109727859497
Epoch 790, training loss: 12.673055648803711 = 0.10714131593704224 + 2.0 * 6.282957077026367
Epoch 790, val loss: 0.7305988073348999
Epoch 800, training loss: 12.657928466796875 = 0.1014002338051796 + 2.0 * 6.278264045715332
Epoch 800, val loss: 0.7327194213867188
Epoch 810, training loss: 12.649287223815918 = 0.09606047719717026 + 2.0 * 6.276613235473633
Epoch 810, val loss: 0.7350240349769592
Epoch 820, training loss: 12.645869255065918 = 0.09106956422328949 + 2.0 * 6.277400016784668
Epoch 820, val loss: 0.7375561594963074
Epoch 830, training loss: 12.638422012329102 = 0.08640050888061523 + 2.0 * 6.276010990142822
Epoch 830, val loss: 0.7402383685112
Epoch 840, training loss: 12.636088371276855 = 0.08204003423452377 + 2.0 * 6.277024269104004
Epoch 840, val loss: 0.7431368827819824
Epoch 850, training loss: 12.626555442810059 = 0.07796692848205566 + 2.0 * 6.274294376373291
Epoch 850, val loss: 0.7462850213050842
Epoch 860, training loss: 12.62069034576416 = 0.07417331635951996 + 2.0 * 6.273258686065674
Epoch 860, val loss: 0.7495383620262146
Epoch 870, training loss: 12.613670349121094 = 0.07061824947595596 + 2.0 * 6.271525859832764
Epoch 870, val loss: 0.7530149817466736
Epoch 880, training loss: 12.608222007751465 = 0.06728463619947433 + 2.0 * 6.270468711853027
Epoch 880, val loss: 0.7565587759017944
Epoch 890, training loss: 12.619646072387695 = 0.0641593262553215 + 2.0 * 6.277743339538574
Epoch 890, val loss: 0.7602199912071228
Epoch 900, training loss: 12.60948371887207 = 0.06123529002070427 + 2.0 * 6.2741241455078125
Epoch 900, val loss: 0.7639679312705994
Epoch 910, training loss: 12.597294807434082 = 0.058483585715293884 + 2.0 * 6.269405841827393
Epoch 910, val loss: 0.7678923010826111
Epoch 920, training loss: 12.591226577758789 = 0.05591495335102081 + 2.0 * 6.267655849456787
Epoch 920, val loss: 0.7718417048454285
Epoch 930, training loss: 12.587637901306152 = 0.05350181832909584 + 2.0 * 6.267067909240723
Epoch 930, val loss: 0.7757507562637329
Epoch 940, training loss: 12.592826843261719 = 0.05122217535972595 + 2.0 * 6.2708024978637695
Epoch 940, val loss: 0.7798039317131042
Epoch 950, training loss: 12.579644203186035 = 0.04909760504961014 + 2.0 * 6.265273094177246
Epoch 950, val loss: 0.7838743925094604
Epoch 960, training loss: 12.576753616333008 = 0.047084830701351166 + 2.0 * 6.264834403991699
Epoch 960, val loss: 0.7880532145500183
Epoch 970, training loss: 12.580928802490234 = 0.04518617317080498 + 2.0 * 6.267871379852295
Epoch 970, val loss: 0.7921836376190186
Epoch 980, training loss: 12.574061393737793 = 0.043411556631326675 + 2.0 * 6.26532506942749
Epoch 980, val loss: 0.7961702942848206
Epoch 990, training loss: 12.573647499084473 = 0.04171931371092796 + 2.0 * 6.265964031219482
Epoch 990, val loss: 0.800367534160614
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7925925925925926
0.8360569319978914
The final CL Acc:0.79383, 0.00175, The final GNN Acc:0.83746, 0.00237
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.3 der2:0.2 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11602])
remove edge: torch.Size([2, 9394])
updated graph: torch.Size([2, 10440])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 19.142074584960938 = 1.9484256505966187 + 2.0 * 8.596824645996094
Epoch 0, val loss: 1.9480665922164917
Epoch 10, training loss: 19.130722045898438 = 1.9378671646118164 + 2.0 * 8.596426963806152
Epoch 10, val loss: 1.9371685981750488
Epoch 20, training loss: 19.111328125 = 1.9246686697006226 + 2.0 * 8.593329429626465
Epoch 20, val loss: 1.9232144355773926
Epoch 30, training loss: 19.04997444152832 = 1.9065616130828857 + 2.0 * 8.571706771850586
Epoch 30, val loss: 1.9041273593902588
Epoch 40, training loss: 18.770185470581055 = 1.8845757246017456 + 2.0 * 8.442805290222168
Epoch 40, val loss: 1.8821601867675781
Epoch 50, training loss: 17.5958251953125 = 1.861934781074524 + 2.0 * 7.866944789886475
Epoch 50, val loss: 1.8599966764450073
Epoch 60, training loss: 16.82770347595215 = 1.84526526927948 + 2.0 * 7.491219520568848
Epoch 60, val loss: 1.8447246551513672
Epoch 70, training loss: 16.338542938232422 = 1.8327480554580688 + 2.0 * 7.2528977394104
Epoch 70, val loss: 1.833034873008728
Epoch 80, training loss: 16.023218154907227 = 1.8208673000335693 + 2.0 * 7.101175785064697
Epoch 80, val loss: 1.8221062421798706
Epoch 90, training loss: 15.758295059204102 = 1.8096697330474854 + 2.0 * 6.974312782287598
Epoch 90, val loss: 1.8115291595458984
Epoch 100, training loss: 15.508888244628906 = 1.799749493598938 + 2.0 * 6.854569435119629
Epoch 100, val loss: 1.8021392822265625
Epoch 110, training loss: 15.317856788635254 = 1.7911399602890015 + 2.0 * 6.7633585929870605
Epoch 110, val loss: 1.793755054473877
Epoch 120, training loss: 15.175640106201172 = 1.7820669412612915 + 2.0 * 6.696786403656006
Epoch 120, val loss: 1.7850736379623413
Epoch 130, training loss: 15.073492050170898 = 1.772256851196289 + 2.0 * 6.650617599487305
Epoch 130, val loss: 1.7757586240768433
Epoch 140, training loss: 14.992718696594238 = 1.761647343635559 + 2.0 * 6.615535736083984
Epoch 140, val loss: 1.765934944152832
Epoch 150, training loss: 14.922586441040039 = 1.7503739595413208 + 2.0 * 6.586106300354004
Epoch 150, val loss: 1.7557649612426758
Epoch 160, training loss: 14.854777336120605 = 1.7382696866989136 + 2.0 * 6.558253765106201
Epoch 160, val loss: 1.7451138496398926
Epoch 170, training loss: 14.792679786682129 = 1.724817156791687 + 2.0 * 6.533931255340576
Epoch 170, val loss: 1.73350989818573
Epoch 180, training loss: 14.73853874206543 = 1.7095669507980347 + 2.0 * 6.514485836029053
Epoch 180, val loss: 1.7205554246902466
Epoch 190, training loss: 14.697297096252441 = 1.6922805309295654 + 2.0 * 6.502508163452148
Epoch 190, val loss: 1.7059133052825928
Epoch 200, training loss: 14.64069938659668 = 1.672836184501648 + 2.0 * 6.483931541442871
Epoch 200, val loss: 1.6895065307617188
Epoch 210, training loss: 14.593778610229492 = 1.6509428024291992 + 2.0 * 6.4714179039001465
Epoch 210, val loss: 1.6709563732147217
Epoch 220, training loss: 14.54638671875 = 1.6262696981430054 + 2.0 * 6.460058689117432
Epoch 220, val loss: 1.6501731872558594
Epoch 230, training loss: 14.498156547546387 = 1.598609209060669 + 2.0 * 6.449773788452148
Epoch 230, val loss: 1.626927375793457
Epoch 240, training loss: 14.452411651611328 = 1.5679659843444824 + 2.0 * 6.442223072052002
Epoch 240, val loss: 1.6013003587722778
Epoch 250, training loss: 14.402713775634766 = 1.534767746925354 + 2.0 * 6.4339728355407715
Epoch 250, val loss: 1.5735868215560913
Epoch 260, training loss: 14.35161018371582 = 1.4989584684371948 + 2.0 * 6.426325798034668
Epoch 260, val loss: 1.5438387393951416
Epoch 270, training loss: 14.296466827392578 = 1.4608521461486816 + 2.0 * 6.417807102203369
Epoch 270, val loss: 1.512246012687683
Epoch 280, training loss: 14.244325637817383 = 1.4207490682601929 + 2.0 * 6.411788463592529
Epoch 280, val loss: 1.4791338443756104
Epoch 290, training loss: 14.193283081054688 = 1.379237413406372 + 2.0 * 6.407022953033447
Epoch 290, val loss: 1.445224404335022
Epoch 300, training loss: 14.138465881347656 = 1.337132215499878 + 2.0 * 6.4006667137146
Epoch 300, val loss: 1.4110461473464966
Epoch 310, training loss: 14.085366249084473 = 1.2946135997772217 + 2.0 * 6.395376205444336
Epoch 310, val loss: 1.376799464225769
Epoch 320, training loss: 14.043509483337402 = 1.25247061252594 + 2.0 * 6.395519256591797
Epoch 320, val loss: 1.3432108163833618
Epoch 330, training loss: 13.982625007629395 = 1.211261510848999 + 2.0 * 6.385681629180908
Epoch 330, val loss: 1.3108383417129517
Epoch 340, training loss: 13.932906150817871 = 1.1707611083984375 + 2.0 * 6.381072521209717
Epoch 340, val loss: 1.2793723344802856
Epoch 350, training loss: 13.885956764221191 = 1.1308938264846802 + 2.0 * 6.3775315284729
Epoch 350, val loss: 1.2488348484039307
Epoch 360, training loss: 13.847865104675293 = 1.0924183130264282 + 2.0 * 6.377723217010498
Epoch 360, val loss: 1.2197654247283936
Epoch 370, training loss: 13.79569149017334 = 1.0551731586456299 + 2.0 * 6.3702592849731445
Epoch 370, val loss: 1.1921309232711792
Epoch 380, training loss: 13.74959659576416 = 1.0188077688217163 + 2.0 * 6.365394592285156
Epoch 380, val loss: 1.1653354167938232
Epoch 390, training loss: 13.710637092590332 = 0.9831511378288269 + 2.0 * 6.363742828369141
Epoch 390, val loss: 1.139327049255371
Epoch 400, training loss: 13.672971725463867 = 0.9482502937316895 + 2.0 * 6.362360954284668
Epoch 400, val loss: 1.1143064498901367
Epoch 410, training loss: 13.626724243164062 = 0.9141945838928223 + 2.0 * 6.356265068054199
Epoch 410, val loss: 1.0901024341583252
Epoch 420, training loss: 13.587456703186035 = 0.8808361291885376 + 2.0 * 6.3533101081848145
Epoch 420, val loss: 1.0666981935501099
Epoch 430, training loss: 13.549307823181152 = 0.8479223847389221 + 2.0 * 6.3506927490234375
Epoch 430, val loss: 1.043871521949768
Epoch 440, training loss: 13.514498710632324 = 0.8156291246414185 + 2.0 * 6.349434852600098
Epoch 440, val loss: 1.0217041969299316
Epoch 450, training loss: 13.476757049560547 = 0.7842043042182922 + 2.0 * 6.34627628326416
Epoch 450, val loss: 1.0005465745925903
Epoch 460, training loss: 13.438146591186523 = 0.7538099884986877 + 2.0 * 6.34216833114624
Epoch 460, val loss: 0.9802820086479187
Epoch 470, training loss: 13.407129287719727 = 0.7243173122406006 + 2.0 * 6.341405868530273
Epoch 470, val loss: 0.9610125422477722
Epoch 480, training loss: 13.374215126037598 = 0.695874810218811 + 2.0 * 6.339169979095459
Epoch 480, val loss: 0.9428516626358032
Epoch 490, training loss: 13.342659950256348 = 0.6687049865722656 + 2.0 * 6.336977481842041
Epoch 490, val loss: 0.9260727763175964
Epoch 500, training loss: 13.309867858886719 = 0.6426101922988892 + 2.0 * 6.3336286544799805
Epoch 500, val loss: 0.910519003868103
Epoch 510, training loss: 13.299211502075195 = 0.6175274848937988 + 2.0 * 6.340841770172119
Epoch 510, val loss: 0.8960685729980469
Epoch 520, training loss: 13.253058433532715 = 0.5937593579292297 + 2.0 * 6.329649448394775
Epoch 520, val loss: 0.882856011390686
Epoch 530, training loss: 13.226804733276367 = 0.571126401424408 + 2.0 * 6.327839374542236
Epoch 530, val loss: 0.8709713816642761
Epoch 540, training loss: 13.202031135559082 = 0.5494886040687561 + 2.0 * 6.326271057128906
Epoch 540, val loss: 0.860103964805603
Epoch 550, training loss: 13.194332122802734 = 0.5288223624229431 + 2.0 * 6.332755088806152
Epoch 550, val loss: 0.8502102494239807
Epoch 560, training loss: 13.16252326965332 = 0.5091437697410583 + 2.0 * 6.326689720153809
Epoch 560, val loss: 0.8413036465644836
Epoch 570, training loss: 13.134078979492188 = 0.4904288053512573 + 2.0 * 6.32182502746582
Epoch 570, val loss: 0.8333497047424316
Epoch 580, training loss: 13.11181926727295 = 0.4724031984806061 + 2.0 * 6.319707870483398
Epoch 580, val loss: 0.8261316418647766
Epoch 590, training loss: 13.094315528869629 = 0.4549656808376312 + 2.0 * 6.319674968719482
Epoch 590, val loss: 0.8195574879646301
Epoch 600, training loss: 13.075798988342285 = 0.43811535835266113 + 2.0 * 6.318841934204102
Epoch 600, val loss: 0.813532292842865
Epoch 610, training loss: 13.052330017089844 = 0.4218057096004486 + 2.0 * 6.315262317657471
Epoch 610, val loss: 0.8081200122833252
Epoch 620, training loss: 13.03559684753418 = 0.40591493248939514 + 2.0 * 6.314840793609619
Epoch 620, val loss: 0.8031782507896423
Epoch 630, training loss: 13.016656875610352 = 0.3904019892215729 + 2.0 * 6.313127517700195
Epoch 630, val loss: 0.7984445691108704
Epoch 640, training loss: 12.999629020690918 = 0.3753410279750824 + 2.0 * 6.312143802642822
Epoch 640, val loss: 0.7943214178085327
Epoch 650, training loss: 12.98206901550293 = 0.3605601489543915 + 2.0 * 6.310754299163818
Epoch 650, val loss: 0.7904810905456543
Epoch 660, training loss: 12.963656425476074 = 0.3459765315055847 + 2.0 * 6.308839797973633
Epoch 660, val loss: 0.7868170738220215
Epoch 670, training loss: 12.946085929870605 = 0.331561416387558 + 2.0 * 6.307262420654297
Epoch 670, val loss: 0.7834815979003906
Epoch 680, training loss: 12.956807136535645 = 0.317312091588974 + 2.0 * 6.319747447967529
Epoch 680, val loss: 0.78029465675354
Epoch 690, training loss: 12.913723945617676 = 0.30343443155288696 + 2.0 * 6.305144786834717
Epoch 690, val loss: 0.777350902557373
Epoch 700, training loss: 12.897598266601562 = 0.2898716926574707 + 2.0 * 6.303863525390625
Epoch 700, val loss: 0.7748085260391235
Epoch 710, training loss: 12.881989479064941 = 0.27660492062568665 + 2.0 * 6.302692413330078
Epoch 710, val loss: 0.7724356055259705
Epoch 720, training loss: 12.872057914733887 = 0.26361051201820374 + 2.0 * 6.304223537445068
Epoch 720, val loss: 0.7703352570533752
Epoch 730, training loss: 12.86170482635498 = 0.2510242164134979 + 2.0 * 6.30534029006958
Epoch 730, val loss: 0.7685592770576477
Epoch 740, training loss: 12.838773727416992 = 0.23882029950618744 + 2.0 * 6.299976825714111
Epoch 740, val loss: 0.7670955061912537
Epoch 750, training loss: 12.824638366699219 = 0.2270386815071106 + 2.0 * 6.298799991607666
Epoch 750, val loss: 0.7659169435501099
Epoch 760, training loss: 12.81071949005127 = 0.21564960479736328 + 2.0 * 6.297534942626953
Epoch 760, val loss: 0.765010416507721
Epoch 770, training loss: 12.799094200134277 = 0.20467785000801086 + 2.0 * 6.297208309173584
Epoch 770, val loss: 0.7645158767700195
Epoch 780, training loss: 12.788579940795898 = 0.1941739171743393 + 2.0 * 6.297203063964844
Epoch 780, val loss: 0.7641538381576538
Epoch 790, training loss: 12.7774658203125 = 0.18419474363327026 + 2.0 * 6.296635627746582
Epoch 790, val loss: 0.764376699924469
Epoch 800, training loss: 12.762494087219238 = 0.17472447454929352 + 2.0 * 6.293884754180908
Epoch 800, val loss: 0.7648478746414185
Epoch 810, training loss: 12.752299308776855 = 0.16572463512420654 + 2.0 * 6.29328727722168
Epoch 810, val loss: 0.7656515836715698
Epoch 820, training loss: 12.751203536987305 = 0.15717695653438568 + 2.0 * 6.297013282775879
Epoch 820, val loss: 0.7667582035064697
Epoch 830, training loss: 12.735889434814453 = 0.14907310903072357 + 2.0 * 6.293408393859863
Epoch 830, val loss: 0.7683721780776978
Epoch 840, training loss: 12.726863861083984 = 0.1414555311203003 + 2.0 * 6.292704105377197
Epoch 840, val loss: 0.7702181339263916
Epoch 850, training loss: 12.712370872497559 = 0.1342541128396988 + 2.0 * 6.289058208465576
Epoch 850, val loss: 0.7723832130432129
Epoch 860, training loss: 12.712302207946777 = 0.12746669352054596 + 2.0 * 6.292417526245117
Epoch 860, val loss: 0.7747852206230164
Epoch 870, training loss: 12.702569961547852 = 0.12108727544546127 + 2.0 * 6.290741443634033
Epoch 870, val loss: 0.7776358127593994
Epoch 880, training loss: 12.689589500427246 = 0.11510299146175385 + 2.0 * 6.287243366241455
Epoch 880, val loss: 0.780746579170227
Epoch 890, training loss: 12.680910110473633 = 0.10947197675704956 + 2.0 * 6.28571891784668
Epoch 890, val loss: 0.7840688228607178
Epoch 900, training loss: 12.681828498840332 = 0.10416766256093979 + 2.0 * 6.288830280303955
Epoch 900, val loss: 0.7876786589622498
Epoch 910, training loss: 12.675288200378418 = 0.0991712212562561 + 2.0 * 6.288058280944824
Epoch 910, val loss: 0.7913102507591248
Epoch 920, training loss: 12.664243698120117 = 0.09452232718467712 + 2.0 * 6.284860610961914
Epoch 920, val loss: 0.795562744140625
Epoch 930, training loss: 12.654376029968262 = 0.09012261778116226 + 2.0 * 6.2821269035339355
Epoch 930, val loss: 0.7997943758964539
Epoch 940, training loss: 12.648722648620605 = 0.08598018437623978 + 2.0 * 6.281371116638184
Epoch 940, val loss: 0.8042203783988953
Epoch 950, training loss: 12.658510208129883 = 0.08206550776958466 + 2.0 * 6.288222312927246
Epoch 950, val loss: 0.8088106513023376
Epoch 960, training loss: 12.666433334350586 = 0.07841800898313522 + 2.0 * 6.294007778167725
Epoch 960, val loss: 0.81345534324646
Epoch 970, training loss: 12.638300895690918 = 0.07499022781848907 + 2.0 * 6.281655311584473
Epoch 970, val loss: 0.8182653784751892
Epoch 980, training loss: 12.628643035888672 = 0.07177035510540009 + 2.0 * 6.278436183929443
Epoch 980, val loss: 0.8232045769691467
Epoch 990, training loss: 12.624369621276855 = 0.068721704185009 + 2.0 * 6.2778239250183105
Epoch 990, val loss: 0.8280849456787109
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7666666666666667
0.8028465998945704
=== training gcn model ===
Epoch 0, training loss: 19.12822151184082 = 1.9345707893371582 + 2.0 * 8.59682559967041
Epoch 0, val loss: 1.931736946105957
Epoch 10, training loss: 19.117610931396484 = 1.924545168876648 + 2.0 * 8.596532821655273
Epoch 10, val loss: 1.9222404956817627
Epoch 20, training loss: 19.10015869140625 = 1.912094235420227 + 2.0 * 8.594032287597656
Epoch 20, val loss: 1.9098619222640991
Epoch 30, training loss: 19.041027069091797 = 1.894760012626648 + 2.0 * 8.57313346862793
Epoch 30, val loss: 1.892277479171753
Epoch 40, training loss: 18.76327896118164 = 1.8729844093322754 + 2.0 * 8.445147514343262
Epoch 40, val loss: 1.8712247610092163
Epoch 50, training loss: 17.906862258911133 = 1.8497138023376465 + 2.0 * 8.028573989868164
Epoch 50, val loss: 1.8496568202972412
Epoch 60, training loss: 17.24233627319336 = 1.831491231918335 + 2.0 * 7.705422878265381
Epoch 60, val loss: 1.8343080282211304
Epoch 70, training loss: 16.359577178955078 = 1.8203679323196411 + 2.0 * 7.269604682922363
Epoch 70, val loss: 1.8246291875839233
Epoch 80, training loss: 15.868446350097656 = 1.8134907484054565 + 2.0 * 7.027477741241455
Epoch 80, val loss: 1.8185760974884033
Epoch 90, training loss: 15.5403413772583 = 1.8053462505340576 + 2.0 * 6.867497444152832
Epoch 90, val loss: 1.810666799545288
Epoch 100, training loss: 15.308030128479004 = 1.7960879802703857 + 2.0 * 6.7559709548950195
Epoch 100, val loss: 1.8020853996276855
Epoch 110, training loss: 15.177539825439453 = 1.7871079444885254 + 2.0 * 6.695215702056885
Epoch 110, val loss: 1.7939939498901367
Epoch 120, training loss: 15.076871871948242 = 1.777167797088623 + 2.0 * 6.649852275848389
Epoch 120, val loss: 1.7851687669754028
Epoch 130, training loss: 14.991720199584961 = 1.7660841941833496 + 2.0 * 6.612818241119385
Epoch 130, val loss: 1.7753314971923828
Epoch 140, training loss: 14.92102336883545 = 1.754347801208496 + 2.0 * 6.583337783813477
Epoch 140, val loss: 1.7649699449539185
Epoch 150, training loss: 14.85066032409668 = 1.7415566444396973 + 2.0 * 6.554551601409912
Epoch 150, val loss: 1.7539657354354858
Epoch 160, training loss: 14.790724754333496 = 1.726990818977356 + 2.0 * 6.531867027282715
Epoch 160, val loss: 1.7416648864746094
Epoch 170, training loss: 14.732748031616211 = 1.7104994058609009 + 2.0 * 6.511124134063721
Epoch 170, val loss: 1.7279644012451172
Epoch 180, training loss: 14.676430702209473 = 1.6919060945510864 + 2.0 * 6.492262363433838
Epoch 180, val loss: 1.712478518486023
Epoch 190, training loss: 14.62701416015625 = 1.6708272695541382 + 2.0 * 6.47809362411499
Epoch 190, val loss: 1.6950173377990723
Epoch 200, training loss: 14.574419975280762 = 1.647202730178833 + 2.0 * 6.463608741760254
Epoch 200, val loss: 1.6754904985427856
Epoch 210, training loss: 14.524680137634277 = 1.62075936794281 + 2.0 * 6.451960563659668
Epoch 210, val loss: 1.653612494468689
Epoch 220, training loss: 14.486080169677734 = 1.5914502143859863 + 2.0 * 6.447315216064453
Epoch 220, val loss: 1.6292779445648193
Epoch 230, training loss: 14.429192543029785 = 1.559536099433899 + 2.0 * 6.434828281402588
Epoch 230, val loss: 1.60321843624115
Epoch 240, training loss: 14.378844261169434 = 1.5253229141235352 + 2.0 * 6.426760673522949
Epoch 240, val loss: 1.5751780271530151
Epoch 250, training loss: 14.327579498291016 = 1.4887428283691406 + 2.0 * 6.4194183349609375
Epoch 250, val loss: 1.5453977584838867
Epoch 260, training loss: 14.275948524475098 = 1.450001835823059 + 2.0 * 6.412973403930664
Epoch 260, val loss: 1.5140739679336548
Epoch 270, training loss: 14.22531509399414 = 1.4098092317581177 + 2.0 * 6.407752990722656
Epoch 270, val loss: 1.4820404052734375
Epoch 280, training loss: 14.178487777709961 = 1.3694758415222168 + 2.0 * 6.404505729675293
Epoch 280, val loss: 1.4500218629837036
Epoch 290, training loss: 14.124444961547852 = 1.3288921117782593 + 2.0 * 6.3977766036987305
Epoch 290, val loss: 1.418066143989563
Epoch 300, training loss: 14.071743965148926 = 1.288236379623413 + 2.0 * 6.391753673553467
Epoch 300, val loss: 1.386437177658081
Epoch 310, training loss: 14.024958610534668 = 1.247653603553772 + 2.0 * 6.388652324676514
Epoch 310, val loss: 1.3551697731018066
Epoch 320, training loss: 13.977752685546875 = 1.2080203294754028 + 2.0 * 6.384866237640381
Epoch 320, val loss: 1.3246381282806396
Epoch 330, training loss: 13.926384925842285 = 1.1690664291381836 + 2.0 * 6.378659248352051
Epoch 330, val loss: 1.2951140403747559
Epoch 340, training loss: 13.884172439575195 = 1.1307927370071411 + 2.0 * 6.376689910888672
Epoch 340, val loss: 1.266411304473877
Epoch 350, training loss: 13.837573051452637 = 1.0933196544647217 + 2.0 * 6.372126579284668
Epoch 350, val loss: 1.238491415977478
Epoch 360, training loss: 13.789490699768066 = 1.056545376777649 + 2.0 * 6.3664727210998535
Epoch 360, val loss: 1.211571455001831
Epoch 370, training loss: 13.748652458190918 = 1.0202710628509521 + 2.0 * 6.364190578460693
Epoch 370, val loss: 1.1854721307754517
Epoch 380, training loss: 13.712366104125977 = 0.9848320484161377 + 2.0 * 6.363767147064209
Epoch 380, val loss: 1.160248041152954
Epoch 390, training loss: 13.663021087646484 = 0.9502562284469604 + 2.0 * 6.356382369995117
Epoch 390, val loss: 1.1361874341964722
Epoch 400, training loss: 13.623387336730957 = 0.916417121887207 + 2.0 * 6.353485107421875
Epoch 400, val loss: 1.1131755113601685
Epoch 410, training loss: 13.587212562561035 = 0.8833497166633606 + 2.0 * 6.351931571960449
Epoch 410, val loss: 1.0909682512283325
Epoch 420, training loss: 13.550724983215332 = 0.851230263710022 + 2.0 * 6.349747180938721
Epoch 420, val loss: 1.0700607299804688
Epoch 430, training loss: 13.510152816772461 = 0.8203309178352356 + 2.0 * 6.344911098480225
Epoch 430, val loss: 1.0503571033477783
Epoch 440, training loss: 13.477577209472656 = 0.7904868125915527 + 2.0 * 6.343544960021973
Epoch 440, val loss: 1.0318000316619873
Epoch 450, training loss: 13.441696166992188 = 0.7618514895439148 + 2.0 * 6.3399224281311035
Epoch 450, val loss: 1.0144853591918945
Epoch 460, training loss: 13.413982391357422 = 0.7344880104064941 + 2.0 * 6.339746952056885
Epoch 460, val loss: 0.9983863830566406
Epoch 470, training loss: 13.386728286743164 = 0.708458423614502 + 2.0 * 6.33913516998291
Epoch 470, val loss: 0.9838549494743347
Epoch 480, training loss: 13.352836608886719 = 0.6838741898536682 + 2.0 * 6.334481239318848
Epoch 480, val loss: 0.970461905002594
Epoch 490, training loss: 13.322230339050293 = 0.6602689027786255 + 2.0 * 6.3309807777404785
Epoch 490, val loss: 0.9583505988121033
Epoch 500, training loss: 13.294631958007812 = 0.6375224590301514 + 2.0 * 6.328554630279541
Epoch 500, val loss: 0.9473786354064941
Epoch 510, training loss: 13.285587310791016 = 0.6156237125396729 + 2.0 * 6.334981918334961
Epoch 510, val loss: 0.9372714757919312
Epoch 520, training loss: 13.244226455688477 = 0.5946813821792603 + 2.0 * 6.324772357940674
Epoch 520, val loss: 0.9284305572509766
Epoch 530, training loss: 13.220532417297363 = 0.5745910406112671 + 2.0 * 6.322970867156982
Epoch 530, val loss: 0.9206123352050781
Epoch 540, training loss: 13.202675819396973 = 0.5552226305007935 + 2.0 * 6.323726654052734
Epoch 540, val loss: 0.9136775732040405
Epoch 550, training loss: 13.178749084472656 = 0.536567211151123 + 2.0 * 6.321091175079346
Epoch 550, val loss: 0.9075953364372253
Epoch 560, training loss: 13.156877517700195 = 0.5186188817024231 + 2.0 * 6.319129467010498
Epoch 560, val loss: 0.9023934006690979
Epoch 570, training loss: 13.138875007629395 = 0.5012967586517334 + 2.0 * 6.318789005279541
Epoch 570, val loss: 0.8978891968727112
Epoch 580, training loss: 13.11994743347168 = 0.4846402406692505 + 2.0 * 6.317653656005859
Epoch 580, val loss: 0.8942205309867859
Epoch 590, training loss: 13.097077369689941 = 0.4685881435871124 + 2.0 * 6.314244747161865
Epoch 590, val loss: 0.8913201093673706
Epoch 600, training loss: 13.077108383178711 = 0.45313870906829834 + 2.0 * 6.311985015869141
Epoch 600, val loss: 0.8890029788017273
Epoch 610, training loss: 13.058930397033691 = 0.43819698691368103 + 2.0 * 6.310366630554199
Epoch 610, val loss: 0.8873150944709778
Epoch 620, training loss: 13.052321434020996 = 0.4237496256828308 + 2.0 * 6.314285755157471
Epoch 620, val loss: 0.8863018751144409
Epoch 630, training loss: 13.034663200378418 = 0.4098784625530243 + 2.0 * 6.312392234802246
Epoch 630, val loss: 0.8857660889625549
Epoch 640, training loss: 13.009729385375977 = 0.39651522040367126 + 2.0 * 6.306607246398926
Epoch 640, val loss: 0.8857017159461975
Epoch 650, training loss: 12.992722511291504 = 0.3835757374763489 + 2.0 * 6.3045735359191895
Epoch 650, val loss: 0.8861706852912903
Epoch 660, training loss: 12.979323387145996 = 0.3710363209247589 + 2.0 * 6.30414342880249
Epoch 660, val loss: 0.8870881199836731
Epoch 670, training loss: 12.965805053710938 = 0.3589346408843994 + 2.0 * 6.303435325622559
Epoch 670, val loss: 0.8883695006370544
Epoch 680, training loss: 12.949555397033691 = 0.3472660779953003 + 2.0 * 6.301144599914551
Epoch 680, val loss: 0.8899925351142883
Epoch 690, training loss: 12.935593605041504 = 0.3359440565109253 + 2.0 * 6.2998247146606445
Epoch 690, val loss: 0.8919768333435059
Epoch 700, training loss: 12.92690658569336 = 0.3249603509902954 + 2.0 * 6.300972938537598
Epoch 700, val loss: 0.8942780494689941
Epoch 710, training loss: 12.922016143798828 = 0.3143317699432373 + 2.0 * 6.303842067718506
Epoch 710, val loss: 0.8968949913978577
Epoch 720, training loss: 12.898534774780273 = 0.3040075898170471 + 2.0 * 6.2972636222839355
Epoch 720, val loss: 0.899615466594696
Epoch 730, training loss: 12.885862350463867 = 0.29400625824928284 + 2.0 * 6.295928001403809
Epoch 730, val loss: 0.9026113152503967
Epoch 740, training loss: 12.871732711791992 = 0.2842213809490204 + 2.0 * 6.293755531311035
Epoch 740, val loss: 0.9058606028556824
Epoch 750, training loss: 12.862871170043945 = 0.2746514678001404 + 2.0 * 6.29410982131958
Epoch 750, val loss: 0.9093849062919617
Epoch 760, training loss: 12.852582931518555 = 0.26534536480903625 + 2.0 * 6.293618679046631
Epoch 760, val loss: 0.9130134582519531
Epoch 770, training loss: 12.841259002685547 = 0.25625067949295044 + 2.0 * 6.29250431060791
Epoch 770, val loss: 0.9168842434883118
Epoch 780, training loss: 12.82719612121582 = 0.24737179279327393 + 2.0 * 6.289912223815918
Epoch 780, val loss: 0.920913577079773
Epoch 790, training loss: 12.822749137878418 = 0.23864395916461945 + 2.0 * 6.292052745819092
Epoch 790, val loss: 0.9252268671989441
Epoch 800, training loss: 12.80665397644043 = 0.23011845350265503 + 2.0 * 6.288267612457275
Epoch 800, val loss: 0.9297652840614319
Epoch 810, training loss: 12.795692443847656 = 0.22174257040023804 + 2.0 * 6.286974906921387
Epoch 810, val loss: 0.9344668388366699
Epoch 820, training loss: 12.787556648254395 = 0.2134997844696045 + 2.0 * 6.2870283126831055
Epoch 820, val loss: 0.9394938349723816
Epoch 830, training loss: 12.785934448242188 = 0.20542839169502258 + 2.0 * 6.290253162384033
Epoch 830, val loss: 0.9446913003921509
Epoch 840, training loss: 12.768309593200684 = 0.1974870264530182 + 2.0 * 6.285411357879639
Epoch 840, val loss: 0.9500443339347839
Epoch 850, training loss: 12.759546279907227 = 0.1897040605545044 + 2.0 * 6.284921169281006
Epoch 850, val loss: 0.9555858373641968
Epoch 860, training loss: 12.750287055969238 = 0.18207308650016785 + 2.0 * 6.284107208251953
Epoch 860, val loss: 0.9612978100776672
Epoch 870, training loss: 12.741508483886719 = 0.17459307610988617 + 2.0 * 6.2834577560424805
Epoch 870, val loss: 0.967231035232544
Epoch 880, training loss: 12.738118171691895 = 0.1672317236661911 + 2.0 * 6.285443305969238
Epoch 880, val loss: 0.9733702540397644
Epoch 890, training loss: 12.725773811340332 = 0.1600317656993866 + 2.0 * 6.282871246337891
Epoch 890, val loss: 0.9797074198722839
Epoch 900, training loss: 12.714679718017578 = 0.15295346081256866 + 2.0 * 6.280863285064697
Epoch 900, val loss: 0.9861957430839539
Epoch 910, training loss: 12.706901550292969 = 0.14599649608135223 + 2.0 * 6.280452728271484
Epoch 910, val loss: 0.9929437637329102
Epoch 920, training loss: 12.69887924194336 = 0.13923421502113342 + 2.0 * 6.27982234954834
Epoch 920, val loss: 0.9998605251312256
Epoch 930, training loss: 12.689762115478516 = 0.1326771378517151 + 2.0 * 6.278542518615723
Epoch 930, val loss: 1.0068856477737427
Epoch 940, training loss: 12.685370445251465 = 0.12635965645313263 + 2.0 * 6.279505252838135
Epoch 940, val loss: 1.0141268968582153
Epoch 950, training loss: 12.676315307617188 = 0.12031009048223495 + 2.0 * 6.278002738952637
Epoch 950, val loss: 1.0214439630508423
Epoch 960, training loss: 12.670001029968262 = 0.11453429609537125 + 2.0 * 6.277733325958252
Epoch 960, val loss: 1.0289157629013062
Epoch 970, training loss: 12.66208553314209 = 0.10902971029281616 + 2.0 * 6.2765278816223145
Epoch 970, val loss: 1.0363538265228271
Epoch 980, training loss: 12.656330108642578 = 0.10381700098514557 + 2.0 * 6.276256561279297
Epoch 980, val loss: 1.0439321994781494
Epoch 990, training loss: 12.646702766418457 = 0.09885650128126144 + 2.0 * 6.273922920227051
Epoch 990, val loss: 1.0515217781066895
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6814814814814815
0.8102266736953084
=== training gcn model ===
Epoch 0, training loss: 19.132266998291016 = 1.9385488033294678 + 2.0 * 8.596858978271484
Epoch 0, val loss: 1.931593656539917
Epoch 10, training loss: 19.122329711914062 = 1.9289747476577759 + 2.0 * 8.596677780151367
Epoch 10, val loss: 1.9230629205703735
Epoch 20, training loss: 19.107526779174805 = 1.9171440601348877 + 2.0 * 8.59519100189209
Epoch 20, val loss: 1.912429690361023
Epoch 30, training loss: 19.064756393432617 = 1.900587558746338 + 2.0 * 8.582084655761719
Epoch 30, val loss: 1.8974376916885376
Epoch 40, training loss: 18.890045166015625 = 1.8783501386642456 + 2.0 * 8.505847930908203
Epoch 40, val loss: 1.877679467201233
Epoch 50, training loss: 18.184494018554688 = 1.853918194770813 + 2.0 * 8.165287971496582
Epoch 50, val loss: 1.8559478521347046
Epoch 60, training loss: 17.528850555419922 = 1.8308206796646118 + 2.0 * 7.849014759063721
Epoch 60, val loss: 1.8356558084487915
Epoch 70, training loss: 16.684003829956055 = 1.8156222105026245 + 2.0 * 7.43419075012207
Epoch 70, val loss: 1.8217418193817139
Epoch 80, training loss: 16.163854598999023 = 1.804639220237732 + 2.0 * 7.179607391357422
Epoch 80, val loss: 1.811357021331787
Epoch 90, training loss: 15.814445495605469 = 1.793810486793518 + 2.0 * 7.010317325592041
Epoch 90, val loss: 1.8011113405227661
Epoch 100, training loss: 15.554906845092773 = 1.7826321125030518 + 2.0 * 6.88613748550415
Epoch 100, val loss: 1.791355848312378
Epoch 110, training loss: 15.366205215454102 = 1.7713477611541748 + 2.0 * 6.797428607940674
Epoch 110, val loss: 1.7811987400054932
Epoch 120, training loss: 15.21984577178955 = 1.7601462602615356 + 2.0 * 6.729849815368652
Epoch 120, val loss: 1.7704582214355469
Epoch 130, training loss: 15.108694076538086 = 1.7483327388763428 + 2.0 * 6.680180549621582
Epoch 130, val loss: 1.7589867115020752
Epoch 140, training loss: 15.02003288269043 = 1.7350976467132568 + 2.0 * 6.642467498779297
Epoch 140, val loss: 1.746778964996338
Epoch 150, training loss: 14.942024230957031 = 1.720233678817749 + 2.0 * 6.610895156860352
Epoch 150, val loss: 1.733648657798767
Epoch 160, training loss: 14.883760452270508 = 1.7034621238708496 + 2.0 * 6.59014892578125
Epoch 160, val loss: 1.7191441059112549
Epoch 170, training loss: 14.815892219543457 = 1.6845266819000244 + 2.0 * 6.565682888031006
Epoch 170, val loss: 1.7030091285705566
Epoch 180, training loss: 14.75747299194336 = 1.6633472442626953 + 2.0 * 6.547062873840332
Epoch 180, val loss: 1.6848987340927124
Epoch 190, training loss: 14.708952903747559 = 1.639640212059021 + 2.0 * 6.534656524658203
Epoch 190, val loss: 1.6647403240203857
Epoch 200, training loss: 14.651927947998047 = 1.6136553287506104 + 2.0 * 6.519136428833008
Epoch 200, val loss: 1.642507791519165
Epoch 210, training loss: 14.597262382507324 = 1.5852497816085815 + 2.0 * 6.506006240844727
Epoch 210, val loss: 1.6182218790054321
Epoch 220, training loss: 14.542891502380371 = 1.5543395280838013 + 2.0 * 6.49427604675293
Epoch 220, val loss: 1.5918197631835938
Epoch 230, training loss: 14.496953010559082 = 1.521145224571228 + 2.0 * 6.487904071807861
Epoch 230, val loss: 1.563573956489563
Epoch 240, training loss: 14.437355041503906 = 1.486706256866455 + 2.0 * 6.4753241539001465
Epoch 240, val loss: 1.5342578887939453
Epoch 250, training loss: 14.37890338897705 = 1.4514734745025635 + 2.0 * 6.463715076446533
Epoch 250, val loss: 1.5042465925216675
Epoch 260, training loss: 14.322697639465332 = 1.4155977964401245 + 2.0 * 6.453549861907959
Epoch 260, val loss: 1.4738911390304565
Epoch 270, training loss: 14.288392066955566 = 1.3791887760162354 + 2.0 * 6.454601764678955
Epoch 270, val loss: 1.443318247795105
Epoch 280, training loss: 14.223505020141602 = 1.3435168266296387 + 2.0 * 6.439993858337402
Epoch 280, val loss: 1.413605809211731
Epoch 290, training loss: 14.169961929321289 = 1.3086336851119995 + 2.0 * 6.4306640625
Epoch 290, val loss: 1.384913682937622
Epoch 300, training loss: 14.119147300720215 = 1.2740823030471802 + 2.0 * 6.422532558441162
Epoch 300, val loss: 1.3567099571228027
Epoch 310, training loss: 14.074320793151855 = 1.2397727966308594 + 2.0 * 6.417273998260498
Epoch 310, val loss: 1.3291549682617188
Epoch 320, training loss: 14.032002449035645 = 1.2061506509780884 + 2.0 * 6.412925720214844
Epoch 320, val loss: 1.302276372909546
Epoch 330, training loss: 13.984182357788086 = 1.172824501991272 + 2.0 * 6.405678749084473
Epoch 330, val loss: 1.2763267755508423
Epoch 340, training loss: 13.940220832824707 = 1.1397466659545898 + 2.0 * 6.400237083435059
Epoch 340, val loss: 1.2507539987564087
Epoch 350, training loss: 13.896246910095215 = 1.1065702438354492 + 2.0 * 6.394838333129883
Epoch 350, val loss: 1.2255221605300903
Epoch 360, training loss: 13.854279518127441 = 1.0731322765350342 + 2.0 * 6.390573501586914
Epoch 360, val loss: 1.2004417181015015
Epoch 370, training loss: 13.813948631286621 = 1.0394185781478882 + 2.0 * 6.387265205383301
Epoch 370, val loss: 1.175526738166809
Epoch 380, training loss: 13.77377700805664 = 1.0057168006896973 + 2.0 * 6.384029865264893
Epoch 380, val loss: 1.150857925415039
Epoch 390, training loss: 13.731620788574219 = 0.9717485308647156 + 2.0 * 6.379936218261719
Epoch 390, val loss: 1.1263095140457153
Epoch 400, training loss: 13.68972396850586 = 0.9374260902404785 + 2.0 * 6.376148700714111
Epoch 400, val loss: 1.1017931699752808
Epoch 410, training loss: 13.665115356445312 = 0.9027681946754456 + 2.0 * 6.381173610687256
Epoch 410, val loss: 1.0774893760681152
Epoch 420, training loss: 13.611045837402344 = 0.8685544729232788 + 2.0 * 6.371245861053467
Epoch 420, val loss: 1.0535173416137695
Epoch 430, training loss: 13.569442749023438 = 0.834583580493927 + 2.0 * 6.367429733276367
Epoch 430, val loss: 1.0302066802978516
Epoch 440, training loss: 13.53028392791748 = 0.8008499145507812 + 2.0 * 6.36471700668335
Epoch 440, val loss: 1.0074121952056885
Epoch 450, training loss: 13.49380874633789 = 0.767581045627594 + 2.0 * 6.363113880157471
Epoch 450, val loss: 0.9852690696716309
Epoch 460, training loss: 13.463598251342773 = 0.7350350618362427 + 2.0 * 6.36428165435791
Epoch 460, val loss: 0.9644390940666199
Epoch 470, training loss: 13.421629905700684 = 0.7039148807525635 + 2.0 * 6.35885763168335
Epoch 470, val loss: 0.9446960091590881
Epoch 480, training loss: 13.385011672973633 = 0.6740223169326782 + 2.0 * 6.355494499206543
Epoch 480, val loss: 0.9263450503349304
Epoch 490, training loss: 13.364030838012695 = 0.6453191637992859 + 2.0 * 6.359355926513672
Epoch 490, val loss: 0.9094571471214294
Epoch 500, training loss: 13.324073791503906 = 0.6182289719581604 + 2.0 * 6.352922439575195
Epoch 500, val loss: 0.894209086894989
Epoch 510, training loss: 13.291698455810547 = 0.592589795589447 + 2.0 * 6.349554538726807
Epoch 510, val loss: 0.880407452583313
Epoch 520, training loss: 13.261420249938965 = 0.5682070255279541 + 2.0 * 6.346606731414795
Epoch 520, val loss: 0.8680514693260193
Epoch 530, training loss: 13.233604431152344 = 0.5449733138084412 + 2.0 * 6.344315528869629
Epoch 530, val loss: 0.8568919897079468
Epoch 540, training loss: 13.207512855529785 = 0.5227528214454651 + 2.0 * 6.342380046844482
Epoch 540, val loss: 0.8469418883323669
Epoch 550, training loss: 13.184566497802734 = 0.5015819072723389 + 2.0 * 6.341492176055908
Epoch 550, val loss: 0.8380603194236755
Epoch 560, training loss: 13.16050910949707 = 0.48160043358802795 + 2.0 * 6.339454174041748
Epoch 560, val loss: 0.8301611542701721
Epoch 570, training loss: 13.13758373260498 = 0.4624827206134796 + 2.0 * 6.337550640106201
Epoch 570, val loss: 0.8232893943786621
Epoch 580, training loss: 13.117095947265625 = 0.4440355896949768 + 2.0 * 6.3365302085876465
Epoch 580, val loss: 0.8171831965446472
Epoch 590, training loss: 13.09450626373291 = 0.42618823051452637 + 2.0 * 6.334158897399902
Epoch 590, val loss: 0.811703085899353
Epoch 600, training loss: 13.072927474975586 = 0.40896642208099365 + 2.0 * 6.3319807052612305
Epoch 600, val loss: 0.8067754507064819
Epoch 610, training loss: 13.053176879882812 = 0.3922217786312103 + 2.0 * 6.330477714538574
Epoch 610, val loss: 0.8024329543113708
Epoch 620, training loss: 13.038344383239746 = 0.3758857548236847 + 2.0 * 6.331229209899902
Epoch 620, val loss: 0.7984537482261658
Epoch 630, training loss: 13.016716957092285 = 0.3600919544696808 + 2.0 * 6.328312397003174
Epoch 630, val loss: 0.7949238419532776
Epoch 640, training loss: 12.995335578918457 = 0.3446997404098511 + 2.0 * 6.325317859649658
Epoch 640, val loss: 0.7918269634246826
Epoch 650, training loss: 12.980243682861328 = 0.3297116756439209 + 2.0 * 6.325265884399414
Epoch 650, val loss: 0.7891520857810974
Epoch 660, training loss: 12.962242126464844 = 0.3151461184024811 + 2.0 * 6.323547840118408
Epoch 660, val loss: 0.7868687510490417
Epoch 670, training loss: 12.944048881530762 = 0.3010585904121399 + 2.0 * 6.321495056152344
Epoch 670, val loss: 0.7848310470581055
Epoch 680, training loss: 12.92563247680664 = 0.2874079942703247 + 2.0 * 6.319112300872803
Epoch 680, val loss: 0.7832438945770264
Epoch 690, training loss: 12.9227933883667 = 0.2742196321487427 + 2.0 * 6.324286937713623
Epoch 690, val loss: 0.7819197177886963
Epoch 700, training loss: 12.896419525146484 = 0.26165854930877686 + 2.0 * 6.317380428314209
Epoch 700, val loss: 0.7811406850814819
Epoch 710, training loss: 12.879475593566895 = 0.2494991421699524 + 2.0 * 6.314988136291504
Epoch 710, val loss: 0.7806174159049988
Epoch 720, training loss: 12.868552207946777 = 0.23788641393184662 + 2.0 * 6.315332889556885
Epoch 720, val loss: 0.7805082201957703
Epoch 730, training loss: 12.85308837890625 = 0.22679826617240906 + 2.0 * 6.313145160675049
Epoch 730, val loss: 0.7806261777877808
Epoch 740, training loss: 12.84797477722168 = 0.21628335118293762 + 2.0 * 6.315845489501953
Epoch 740, val loss: 0.781187117099762
Epoch 750, training loss: 12.826865196228027 = 0.2062884420156479 + 2.0 * 6.310288429260254
Epoch 750, val loss: 0.7821970582008362
Epoch 760, training loss: 12.812383651733398 = 0.19678835570812225 + 2.0 * 6.307797431945801
Epoch 760, val loss: 0.7834696173667908
Epoch 770, training loss: 12.802443504333496 = 0.18776236474514008 + 2.0 * 6.307340621948242
Epoch 770, val loss: 0.785096287727356
Epoch 780, training loss: 12.815849304199219 = 0.17921438813209534 + 2.0 * 6.318317413330078
Epoch 780, val loss: 0.7871140241622925
Epoch 790, training loss: 12.787511825561523 = 0.17104357481002808 + 2.0 * 6.308234214782715
Epoch 790, val loss: 0.7891899347305298
Epoch 800, training loss: 12.77077579498291 = 0.16340890526771545 + 2.0 * 6.303683280944824
Epoch 800, val loss: 0.7917307615280151
Epoch 810, training loss: 12.761542320251465 = 0.15616881847381592 + 2.0 * 6.30268669128418
Epoch 810, val loss: 0.7945806384086609
Epoch 820, training loss: 12.751173973083496 = 0.1492736041545868 + 2.0 * 6.300950050354004
Epoch 820, val loss: 0.7976340055465698
Epoch 830, training loss: 12.742338180541992 = 0.1427282989025116 + 2.0 * 6.299805164337158
Epoch 830, val loss: 0.800872266292572
Epoch 840, training loss: 12.768017768859863 = 0.13649456202983856 + 2.0 * 6.315761566162109
Epoch 840, val loss: 0.8042315244674683
Epoch 850, training loss: 12.733846664428711 = 0.1306282877922058 + 2.0 * 6.301609039306641
Epoch 850, val loss: 0.8078600168228149
Epoch 860, training loss: 12.72126293182373 = 0.1250755488872528 + 2.0 * 6.298093795776367
Epoch 860, val loss: 0.8116393089294434
Epoch 870, training loss: 12.712180137634277 = 0.11981388926506042 + 2.0 * 6.296183109283447
Epoch 870, val loss: 0.8155829906463623
Epoch 880, training loss: 12.713438987731934 = 0.11481685191392899 + 2.0 * 6.29931116104126
Epoch 880, val loss: 0.8198301196098328
Epoch 890, training loss: 12.70190715789795 = 0.11007144302129745 + 2.0 * 6.295917987823486
Epoch 890, val loss: 0.8240182995796204
Epoch 900, training loss: 12.695220947265625 = 0.10557359457015991 + 2.0 * 6.29482364654541
Epoch 900, val loss: 0.8285987377166748
Epoch 910, training loss: 12.688187599182129 = 0.10130640864372253 + 2.0 * 6.293440818786621
Epoch 910, val loss: 0.8331590294837952
Epoch 920, training loss: 12.686485290527344 = 0.09725780040025711 + 2.0 * 6.294613838195801
Epoch 920, val loss: 0.8378478288650513
Epoch 930, training loss: 12.67927074432373 = 0.09338688850402832 + 2.0 * 6.292942047119141
Epoch 930, val loss: 0.842780351638794
Epoch 940, training loss: 12.669687271118164 = 0.08971898257732391 + 2.0 * 6.289984226226807
Epoch 940, val loss: 0.8475944995880127
Epoch 950, training loss: 12.663921356201172 = 0.08622033149003983 + 2.0 * 6.2888503074646
Epoch 950, val loss: 0.8527641296386719
Epoch 960, training loss: 12.674612998962402 = 0.08288848400115967 + 2.0 * 6.295862197875977
Epoch 960, val loss: 0.8578742742538452
Epoch 970, training loss: 12.659083366394043 = 0.07972846180200577 + 2.0 * 6.289677619934082
Epoch 970, val loss: 0.8629152178764343
Epoch 980, training loss: 12.652275085449219 = 0.07670720666646957 + 2.0 * 6.287784099578857
Epoch 980, val loss: 0.8681752681732178
Epoch 990, training loss: 12.650306701660156 = 0.07383265346288681 + 2.0 * 6.28823709487915
Epoch 990, val loss: 0.8733981251716614
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7000000000000001
0.812335266209805
The final CL Acc:0.71605, 0.03658, The final GNN Acc:0.80847, 0.00407
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13280])
remove edge: torch.Size([2, 7988])
updated graph: torch.Size([2, 10712])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 19.155399322509766 = 1.9617068767547607 + 2.0 * 8.596846580505371
Epoch 0, val loss: 1.9518836736679077
Epoch 10, training loss: 19.144271850585938 = 1.951155662536621 + 2.0 * 8.596558570861816
Epoch 10, val loss: 1.9416004419326782
Epoch 20, training loss: 19.126811981201172 = 1.9384503364562988 + 2.0 * 8.594181060791016
Epoch 20, val loss: 1.9285866022109985
Epoch 30, training loss: 19.07377052307129 = 1.9210680723190308 + 2.0 * 8.576351165771484
Epoch 30, val loss: 1.910237431526184
Epoch 40, training loss: 18.842554092407227 = 1.8980801105499268 + 2.0 * 8.472236633300781
Epoch 40, val loss: 1.8868224620819092
Epoch 50, training loss: 17.82463836669922 = 1.8717576265335083 + 2.0 * 7.976439952850342
Epoch 50, val loss: 1.8612556457519531
Epoch 60, training loss: 16.884933471679688 = 1.8489398956298828 + 2.0 * 7.517996311187744
Epoch 60, val loss: 1.841286063194275
Epoch 70, training loss: 16.106645584106445 = 1.8332972526550293 + 2.0 * 7.136674404144287
Epoch 70, val loss: 1.8275443315505981
Epoch 80, training loss: 15.766406059265137 = 1.8176376819610596 + 2.0 * 6.974384307861328
Epoch 80, val loss: 1.8134222030639648
Epoch 90, training loss: 15.642486572265625 = 1.8007926940917969 + 2.0 * 6.920846939086914
Epoch 90, val loss: 1.7983217239379883
Epoch 100, training loss: 15.514904975891113 = 1.7825020551681519 + 2.0 * 6.866201400756836
Epoch 100, val loss: 1.7830049991607666
Epoch 110, training loss: 15.37723159790039 = 1.7664624452590942 + 2.0 * 6.805384635925293
Epoch 110, val loss: 1.7703311443328857
Epoch 120, training loss: 15.240795135498047 = 1.7525690793991089 + 2.0 * 6.744112968444824
Epoch 120, val loss: 1.7594070434570312
Epoch 130, training loss: 15.13996696472168 = 1.7382698059082031 + 2.0 * 6.700848579406738
Epoch 130, val loss: 1.747697114944458
Epoch 140, training loss: 15.054877281188965 = 1.7219610214233398 + 2.0 * 6.6664581298828125
Epoch 140, val loss: 1.734268069267273
Epoch 150, training loss: 14.975159645080566 = 1.703857183456421 + 2.0 * 6.635651111602783
Epoch 150, val loss: 1.719416856765747
Epoch 160, training loss: 14.897638320922852 = 1.68422269821167 + 2.0 * 6.606707572937012
Epoch 160, val loss: 1.7034718990325928
Epoch 170, training loss: 14.831819534301758 = 1.6627224683761597 + 2.0 * 6.584548473358154
Epoch 170, val loss: 1.6861629486083984
Epoch 180, training loss: 14.767434120178223 = 1.638519048690796 + 2.0 * 6.564457416534424
Epoch 180, val loss: 1.6665375232696533
Epoch 190, training loss: 14.705016136169434 = 1.6112912893295288 + 2.0 * 6.546862602233887
Epoch 190, val loss: 1.6444154977798462
Epoch 200, training loss: 14.63988208770752 = 1.5811039209365845 + 2.0 * 6.529388904571533
Epoch 200, val loss: 1.619588851928711
Epoch 210, training loss: 14.574885368347168 = 1.5477440357208252 + 2.0 * 6.513570785522461
Epoch 210, val loss: 1.5921510457992554
Epoch 220, training loss: 14.516098976135254 = 1.5114405155181885 + 2.0 * 6.502329349517822
Epoch 220, val loss: 1.5621992349624634
Epoch 230, training loss: 14.454381942749023 = 1.472944736480713 + 2.0 * 6.490718841552734
Epoch 230, val loss: 1.5306384563446045
Epoch 240, training loss: 14.394430160522461 = 1.4322938919067383 + 2.0 * 6.481068134307861
Epoch 240, val loss: 1.497353434562683
Epoch 250, training loss: 14.332515716552734 = 1.389662265777588 + 2.0 * 6.471426963806152
Epoch 250, val loss: 1.4625970125198364
Epoch 260, training loss: 14.269147872924805 = 1.3453315496444702 + 2.0 * 6.461908340454102
Epoch 260, val loss: 1.4267500638961792
Epoch 270, training loss: 14.205724716186523 = 1.2999049425125122 + 2.0 * 6.45290994644165
Epoch 270, val loss: 1.3903186321258545
Epoch 280, training loss: 14.14453411102295 = 1.254471778869629 + 2.0 * 6.44503116607666
Epoch 280, val loss: 1.3539292812347412
Epoch 290, training loss: 14.080865859985352 = 1.2091670036315918 + 2.0 * 6.435849666595459
Epoch 290, val loss: 1.3178198337554932
Epoch 300, training loss: 14.018749237060547 = 1.163789987564087 + 2.0 * 6.4274797439575195
Epoch 300, val loss: 1.2818222045898438
Epoch 310, training loss: 13.964905738830566 = 1.1185637712478638 + 2.0 * 6.423171043395996
Epoch 310, val loss: 1.2458311319351196
Epoch 320, training loss: 13.910388946533203 = 1.074124813079834 + 2.0 * 6.418132305145264
Epoch 320, val loss: 1.2107484340667725
Epoch 330, training loss: 13.850052833557129 = 1.0313856601715088 + 2.0 * 6.4093337059021
Epoch 330, val loss: 1.1771639585494995
Epoch 340, training loss: 13.795495986938477 = 0.9898228049278259 + 2.0 * 6.402836799621582
Epoch 340, val loss: 1.1449452638626099
Epoch 350, training loss: 13.744305610656738 = 0.949609100818634 + 2.0 * 6.397348403930664
Epoch 350, val loss: 1.1139971017837524
Epoch 360, training loss: 13.69698429107666 = 0.9110342860221863 + 2.0 * 6.392974853515625
Epoch 360, val loss: 1.0847094058990479
Epoch 370, training loss: 13.668292999267578 = 0.8746086359024048 + 2.0 * 6.396842002868652
Epoch 370, val loss: 1.057507872581482
Epoch 380, training loss: 13.612654685974121 = 0.8404696583747864 + 2.0 * 6.386092662811279
Epoch 380, val loss: 1.0325928926467896
Epoch 390, training loss: 13.567575454711914 = 0.8082743883132935 + 2.0 * 6.379650592803955
Epoch 390, val loss: 1.0098680257797241
Epoch 400, training loss: 13.529067993164062 = 0.7777727246284485 + 2.0 * 6.37564754486084
Epoch 400, val loss: 0.9889606833457947
Epoch 410, training loss: 13.493500709533691 = 0.7488523125648499 + 2.0 * 6.372323989868164
Epoch 410, val loss: 0.9701109528541565
Epoch 420, training loss: 13.458879470825195 = 0.721591591835022 + 2.0 * 6.368643760681152
Epoch 420, val loss: 0.9532410502433777
Epoch 430, training loss: 13.42481517791748 = 0.6958985924720764 + 2.0 * 6.364458084106445
Epoch 430, val loss: 0.9382471442222595
Epoch 440, training loss: 13.396369934082031 = 0.6714959740638733 + 2.0 * 6.362436771392822
Epoch 440, val loss: 0.9249399900436401
Epoch 450, training loss: 13.371030807495117 = 0.6482117772102356 + 2.0 * 6.361409664154053
Epoch 450, val loss: 0.913017988204956
Epoch 460, training loss: 13.343182563781738 = 0.625858724117279 + 2.0 * 6.358662128448486
Epoch 460, val loss: 0.9024447798728943
Epoch 470, training loss: 13.311483383178711 = 0.6042520999908447 + 2.0 * 6.353615760803223
Epoch 470, val loss: 0.8928514719009399
Epoch 480, training loss: 13.286436080932617 = 0.583254337310791 + 2.0 * 6.351591110229492
Epoch 480, val loss: 0.884244441986084
Epoch 490, training loss: 13.266721725463867 = 0.5628525018692017 + 2.0 * 6.351934432983398
Epoch 490, val loss: 0.8764045238494873
Epoch 500, training loss: 13.236001968383789 = 0.5430580377578735 + 2.0 * 6.346471786499023
Epoch 500, val loss: 0.8694812059402466
Epoch 510, training loss: 13.214543342590332 = 0.5236680507659912 + 2.0 * 6.345437526702881
Epoch 510, val loss: 0.8631628751754761
Epoch 520, training loss: 13.189240455627441 = 0.5047264695167542 + 2.0 * 6.342257022857666
Epoch 520, val loss: 0.8573500514030457
Epoch 530, training loss: 13.166165351867676 = 0.48626676201820374 + 2.0 * 6.339949131011963
Epoch 530, val loss: 0.8522573709487915
Epoch 540, training loss: 13.166937828063965 = 0.4682021737098694 + 2.0 * 6.349367618560791
Epoch 540, val loss: 0.8477887511253357
Epoch 550, training loss: 13.126997947692871 = 0.4506615698337555 + 2.0 * 6.338168144226074
Epoch 550, val loss: 0.8436576724052429
Epoch 560, training loss: 13.103799819946289 = 0.43360987305641174 + 2.0 * 6.335094928741455
Epoch 560, val loss: 0.8402318954467773
Epoch 570, training loss: 13.083325386047363 = 0.4169064164161682 + 2.0 * 6.33320951461792
Epoch 570, val loss: 0.8373041749000549
Epoch 580, training loss: 13.07565975189209 = 0.4005824625492096 + 2.0 * 6.337538719177246
Epoch 580, val loss: 0.8348500728607178
Epoch 590, training loss: 13.054152488708496 = 0.3846496641635895 + 2.0 * 6.334751605987549
Epoch 590, val loss: 0.8328819274902344
Epoch 600, training loss: 13.027656555175781 = 0.369202196598053 + 2.0 * 6.329226970672607
Epoch 600, val loss: 0.8315115571022034
Epoch 610, training loss: 13.010638236999512 = 0.3541366159915924 + 2.0 * 6.328250885009766
Epoch 610, val loss: 0.8306216597557068
Epoch 620, training loss: 12.993858337402344 = 0.33948463201522827 + 2.0 * 6.3271870613098145
Epoch 620, val loss: 0.83017498254776
Epoch 630, training loss: 12.980320930480957 = 0.3252643942832947 + 2.0 * 6.327528476715088
Epoch 630, val loss: 0.830290675163269
Epoch 640, training loss: 12.957945823669434 = 0.3114074170589447 + 2.0 * 6.323269367218018
Epoch 640, val loss: 0.8307437300682068
Epoch 650, training loss: 12.941605567932129 = 0.2980172634124756 + 2.0 * 6.321794033050537
Epoch 650, val loss: 0.8316390514373779
Epoch 660, training loss: 12.923322677612305 = 0.2849370539188385 + 2.0 * 6.319192886352539
Epoch 660, val loss: 0.8328794836997986
Epoch 670, training loss: 12.926285743713379 = 0.2722553014755249 + 2.0 * 6.327015399932861
Epoch 670, val loss: 0.834436297416687
Epoch 680, training loss: 12.892693519592285 = 0.2600528597831726 + 2.0 * 6.316320419311523
Epoch 680, val loss: 0.8363434672355652
Epoch 690, training loss: 12.880522727966309 = 0.24832333624362946 + 2.0 * 6.316099643707275
Epoch 690, val loss: 0.8387245535850525
Epoch 700, training loss: 12.867751121520996 = 0.23700955510139465 + 2.0 * 6.315370559692383
Epoch 700, val loss: 0.8412740230560303
Epoch 710, training loss: 12.856904029846191 = 0.22620631754398346 + 2.0 * 6.3153486251831055
Epoch 710, val loss: 0.8443496823310852
Epoch 720, training loss: 12.836297035217285 = 0.21580342948436737 + 2.0 * 6.31024694442749
Epoch 720, val loss: 0.847752034664154
Epoch 730, training loss: 12.824664115905762 = 0.20581017434597015 + 2.0 * 6.309426784515381
Epoch 730, val loss: 0.851420521736145
Epoch 740, training loss: 12.817481994628906 = 0.19620421528816223 + 2.0 * 6.310638904571533
Epoch 740, val loss: 0.8554408550262451
Epoch 750, training loss: 12.80566692352295 = 0.18704646825790405 + 2.0 * 6.309310436248779
Epoch 750, val loss: 0.8595272898674011
Epoch 760, training loss: 12.790154457092285 = 0.1783221960067749 + 2.0 * 6.3059163093566895
Epoch 760, val loss: 0.8640875220298767
Epoch 770, training loss: 12.788987159729004 = 0.16999971866607666 + 2.0 * 6.309493541717529
Epoch 770, val loss: 0.8687840700149536
Epoch 780, training loss: 12.775373458862305 = 0.16213065385818481 + 2.0 * 6.306621551513672
Epoch 780, val loss: 0.8736488223075867
Epoch 790, training loss: 12.76170825958252 = 0.1546436995267868 + 2.0 * 6.303532123565674
Epoch 790, val loss: 0.8787609338760376
Epoch 800, training loss: 12.749404907226562 = 0.14756420254707336 + 2.0 * 6.300920486450195
Epoch 800, val loss: 0.8841482996940613
Epoch 810, training loss: 12.747350692749023 = 0.14082559943199158 + 2.0 * 6.303262710571289
Epoch 810, val loss: 0.8896801471710205
Epoch 820, training loss: 12.745871543884277 = 0.1344476044178009 + 2.0 * 6.30571174621582
Epoch 820, val loss: 0.8952490091323853
Epoch 830, training loss: 12.728236198425293 = 0.128439798951149 + 2.0 * 6.299898147583008
Epoch 830, val loss: 0.9009090662002563
Epoch 840, training loss: 12.716833114624023 = 0.12276149541139603 + 2.0 * 6.2970356941223145
Epoch 840, val loss: 0.9068683385848999
Epoch 850, training loss: 12.70840835571289 = 0.11738205701112747 + 2.0 * 6.295513153076172
Epoch 850, val loss: 0.9128538370132446
Epoch 860, training loss: 12.709975242614746 = 0.11227412521839142 + 2.0 * 6.2988505363464355
Epoch 860, val loss: 0.9188826680183411
Epoch 870, training loss: 12.69345474243164 = 0.10742433369159698 + 2.0 * 6.293015003204346
Epoch 870, val loss: 0.9248510003089905
Epoch 880, training loss: 12.687908172607422 = 0.10284294933080673 + 2.0 * 6.292532444000244
Epoch 880, val loss: 0.9310176372528076
Epoch 890, training loss: 12.698535919189453 = 0.098507359623909 + 2.0 * 6.300014495849609
Epoch 890, val loss: 0.9371418356895447
Epoch 900, training loss: 12.677467346191406 = 0.09440894424915314 + 2.0 * 6.291529178619385
Epoch 900, val loss: 0.943164050579071
Epoch 910, training loss: 12.670084953308105 = 0.09052085876464844 + 2.0 * 6.2897820472717285
Epoch 910, val loss: 0.9493772387504578
Epoch 920, training loss: 12.664434432983398 = 0.08683771640062332 + 2.0 * 6.2887983322143555
Epoch 920, val loss: 0.9554948806762695
Epoch 930, training loss: 12.672307014465332 = 0.08333276957273483 + 2.0 * 6.294486999511719
Epoch 930, val loss: 0.9616295695304871
Epoch 940, training loss: 12.662737846374512 = 0.08000556379556656 + 2.0 * 6.291366100311279
Epoch 940, val loss: 0.9676868319511414
Epoch 950, training loss: 12.669150352478027 = 0.07685729116201401 + 2.0 * 6.296146392822266
Epoch 950, val loss: 0.9737008810043335
Epoch 960, training loss: 12.65215015411377 = 0.07391001284122467 + 2.0 * 6.289120197296143
Epoch 960, val loss: 0.9797499179840088
Epoch 970, training loss: 12.644143104553223 = 0.07109513878822327 + 2.0 * 6.286523818969727
Epoch 970, val loss: 0.985824465751648
Epoch 980, training loss: 12.636483192443848 = 0.06843340396881104 + 2.0 * 6.284024715423584
Epoch 980, val loss: 0.991889238357544
Epoch 990, training loss: 12.63113021850586 = 0.06588625907897949 + 2.0 * 6.28262186050415
Epoch 990, val loss: 0.9978762865066528
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7666666666666667
0.8397469688982605
=== training gcn model ===
Epoch 0, training loss: 19.134681701660156 = 1.941061019897461 + 2.0 * 8.596810340881348
Epoch 0, val loss: 1.9333899021148682
Epoch 10, training loss: 19.122909545898438 = 1.9303754568099976 + 2.0 * 8.596266746520996
Epoch 10, val loss: 1.9235270023345947
Epoch 20, training loss: 19.100765228271484 = 1.9171463251113892 + 2.0 * 8.591809272766113
Epoch 20, val loss: 1.911016583442688
Epoch 30, training loss: 19.015094757080078 = 1.8996695280075073 + 2.0 * 8.55771255493164
Epoch 30, val loss: 1.8943581581115723
Epoch 40, training loss: 18.565954208374023 = 1.878327488899231 + 2.0 * 8.343812942504883
Epoch 40, val loss: 1.8745677471160889
Epoch 50, training loss: 17.208892822265625 = 1.8553836345672607 + 2.0 * 7.676754951477051
Epoch 50, val loss: 1.8535507917404175
Epoch 60, training loss: 16.58523941040039 = 1.8377288579940796 + 2.0 * 7.373754978179932
Epoch 60, val loss: 1.8379510641098022
Epoch 70, training loss: 16.115196228027344 = 1.8234280347824097 + 2.0 * 7.145884037017822
Epoch 70, val loss: 1.8244528770446777
Epoch 80, training loss: 15.77013874053955 = 1.8092451095581055 + 2.0 * 6.980446815490723
Epoch 80, val loss: 1.811537504196167
Epoch 90, training loss: 15.518939971923828 = 1.7972511053085327 + 2.0 * 6.860844612121582
Epoch 90, val loss: 1.800955057144165
Epoch 100, training loss: 15.346151351928711 = 1.7846007347106934 + 2.0 * 6.78077507019043
Epoch 100, val loss: 1.7900645732879639
Epoch 110, training loss: 15.217161178588867 = 1.7708511352539062 + 2.0 * 6.7231550216674805
Epoch 110, val loss: 1.7778416872024536
Epoch 120, training loss: 15.116939544677734 = 1.7564769983291626 + 2.0 * 6.680231094360352
Epoch 120, val loss: 1.7647953033447266
Epoch 130, training loss: 15.030594825744629 = 1.7413092851638794 + 2.0 * 6.6446428298950195
Epoch 130, val loss: 1.7510530948638916
Epoch 140, training loss: 14.938591003417969 = 1.7248718738555908 + 2.0 * 6.6068596839904785
Epoch 140, val loss: 1.7365325689315796
Epoch 150, training loss: 14.8607177734375 = 1.7066113948822021 + 2.0 * 6.577053070068359
Epoch 150, val loss: 1.7205778360366821
Epoch 160, training loss: 14.792364120483398 = 1.6856145858764648 + 2.0 * 6.553374767303467
Epoch 160, val loss: 1.7025123834609985
Epoch 170, training loss: 14.733394622802734 = 1.6617112159729004 + 2.0 * 6.535841464996338
Epoch 170, val loss: 1.6820324659347534
Epoch 180, training loss: 14.676349639892578 = 1.6349728107452393 + 2.0 * 6.520688533782959
Epoch 180, val loss: 1.6593849658966064
Epoch 190, training loss: 14.618160247802734 = 1.6050755977630615 + 2.0 * 6.506542205810547
Epoch 190, val loss: 1.634260892868042
Epoch 200, training loss: 14.559181213378906 = 1.5717369318008423 + 2.0 * 6.493721961975098
Epoch 200, val loss: 1.6063024997711182
Epoch 210, training loss: 14.5100679397583 = 1.5347630977630615 + 2.0 * 6.48765230178833
Epoch 210, val loss: 1.5755283832550049
Epoch 220, training loss: 14.442328453063965 = 1.4952417612075806 + 2.0 * 6.473543167114258
Epoch 220, val loss: 1.5424388647079468
Epoch 230, training loss: 14.378595352172852 = 1.4534835815429688 + 2.0 * 6.462555885314941
Epoch 230, val loss: 1.5077741146087646
Epoch 240, training loss: 14.318718910217285 = 1.4098374843597412 + 2.0 * 6.454440593719482
Epoch 240, val loss: 1.4716612100601196
Epoch 250, training loss: 14.258049964904785 = 1.3653523921966553 + 2.0 * 6.446348667144775
Epoch 250, val loss: 1.4350005388259888
Epoch 260, training loss: 14.195846557617188 = 1.3206303119659424 + 2.0 * 6.437608242034912
Epoch 260, val loss: 1.3984209299087524
Epoch 270, training loss: 14.13595962524414 = 1.275925636291504 + 2.0 * 6.430016994476318
Epoch 270, val loss: 1.362005591392517
Epoch 280, training loss: 14.083236694335938 = 1.2318462133407593 + 2.0 * 6.425695419311523
Epoch 280, val loss: 1.3262776136398315
Epoch 290, training loss: 14.034070014953613 = 1.1889493465423584 + 2.0 * 6.422560214996338
Epoch 290, val loss: 1.2917059659957886
Epoch 300, training loss: 13.974291801452637 = 1.147482991218567 + 2.0 * 6.41340446472168
Epoch 300, val loss: 1.2586214542388916
Epoch 310, training loss: 13.923595428466797 = 1.1074000597000122 + 2.0 * 6.408097743988037
Epoch 310, val loss: 1.2266813516616821
Epoch 320, training loss: 13.874144554138184 = 1.0683554410934448 + 2.0 * 6.402894496917725
Epoch 320, val loss: 1.1957000494003296
Epoch 330, training loss: 13.843881607055664 = 1.0302106142044067 + 2.0 * 6.406835556030273
Epoch 330, val loss: 1.1656535863876343
Epoch 340, training loss: 13.792098999023438 = 0.9936676025390625 + 2.0 * 6.3992156982421875
Epoch 340, val loss: 1.1368237733840942
Epoch 350, training loss: 13.73967170715332 = 0.9583418965339661 + 2.0 * 6.390665054321289
Epoch 350, val loss: 1.109165906906128
Epoch 360, training loss: 13.697938919067383 = 0.9238317608833313 + 2.0 * 6.387053489685059
Epoch 360, val loss: 1.0822925567626953
Epoch 370, training loss: 13.656014442443848 = 0.8898974061012268 + 2.0 * 6.383058547973633
Epoch 370, val loss: 1.0559148788452148
Epoch 380, training loss: 13.624650955200195 = 0.856525719165802 + 2.0 * 6.384062767028809
Epoch 380, val loss: 1.0300052165985107
Epoch 390, training loss: 13.579487800598145 = 0.8239242434501648 + 2.0 * 6.377781867980957
Epoch 390, val loss: 1.0049564838409424
Epoch 400, training loss: 13.53847599029541 = 0.7921313047409058 + 2.0 * 6.373172283172607
Epoch 400, val loss: 0.9806259870529175
Epoch 410, training loss: 13.501981735229492 = 0.7609536051750183 + 2.0 * 6.370513916015625
Epoch 410, val loss: 0.9569031596183777
Epoch 420, training loss: 13.46729850769043 = 0.7304855585098267 + 2.0 * 6.368406295776367
Epoch 420, val loss: 0.9338777661323547
Epoch 430, training loss: 13.428322792053223 = 0.7008602619171143 + 2.0 * 6.363731384277344
Epoch 430, val loss: 0.9118036031723022
Epoch 440, training loss: 13.395947456359863 = 0.6721368432044983 + 2.0 * 6.361905097961426
Epoch 440, val loss: 0.8906373977661133
Epoch 450, training loss: 13.365510940551758 = 0.6443375945091248 + 2.0 * 6.360586643218994
Epoch 450, val loss: 0.8705881834030151
Epoch 460, training loss: 13.329639434814453 = 0.61761474609375 + 2.0 * 6.356012344360352
Epoch 460, val loss: 0.8516596555709839
Epoch 470, training loss: 13.305672645568848 = 0.5918196439743042 + 2.0 * 6.356926441192627
Epoch 470, val loss: 0.833850622177124
Epoch 480, training loss: 13.269538879394531 = 0.5670579671859741 + 2.0 * 6.351240634918213
Epoch 480, val loss: 0.8170773386955261
Epoch 490, training loss: 13.244826316833496 = 0.5431353449821472 + 2.0 * 6.3508453369140625
Epoch 490, val loss: 0.8014640808105469
Epoch 500, training loss: 13.214093208312988 = 0.5200768709182739 + 2.0 * 6.347008228302002
Epoch 500, val loss: 0.7868232131004333
Epoch 510, training loss: 13.186552047729492 = 0.49761849641799927 + 2.0 * 6.344466686248779
Epoch 510, val loss: 0.7730018496513367
Epoch 520, training loss: 13.167398452758789 = 0.47571268677711487 + 2.0 * 6.3458428382873535
Epoch 520, val loss: 0.7600390911102295
Epoch 530, training loss: 13.140804290771484 = 0.4544580280780792 + 2.0 * 6.343173027038574
Epoch 530, val loss: 0.7477272748947144
Epoch 540, training loss: 13.111918449401855 = 0.4336913526058197 + 2.0 * 6.339113712310791
Epoch 540, val loss: 0.7362197041511536
Epoch 550, training loss: 13.086783409118652 = 0.4133128225803375 + 2.0 * 6.336735248565674
Epoch 550, val loss: 0.7252791523933411
Epoch 560, training loss: 13.07150936126709 = 0.39335161447525024 + 2.0 * 6.339078903198242
Epoch 560, val loss: 0.7149401903152466
Epoch 570, training loss: 13.043145179748535 = 0.3738396465778351 + 2.0 * 6.334652900695801
Epoch 570, val loss: 0.7053199410438538
Epoch 580, training loss: 13.020456314086914 = 0.3549686670303345 + 2.0 * 6.3327436447143555
Epoch 580, val loss: 0.6962718963623047
Epoch 590, training loss: 12.99901008605957 = 0.33661097288131714 + 2.0 * 6.331199645996094
Epoch 590, val loss: 0.6879562735557556
Epoch 600, training loss: 12.979564666748047 = 0.3190561830997467 + 2.0 * 6.330254077911377
Epoch 600, val loss: 0.6802607774734497
Epoch 610, training loss: 12.952950477600098 = 0.3021806478500366 + 2.0 * 6.325385093688965
Epoch 610, val loss: 0.6733367443084717
Epoch 620, training loss: 12.935591697692871 = 0.2861069142818451 + 2.0 * 6.324742317199707
Epoch 620, val loss: 0.6670777201652527
Epoch 630, training loss: 12.921602249145508 = 0.2707992196083069 + 2.0 * 6.325401306152344
Epoch 630, val loss: 0.661519467830658
Epoch 640, training loss: 12.900168418884277 = 0.25626635551452637 + 2.0 * 6.321950912475586
Epoch 640, val loss: 0.6567025780677795
Epoch 650, training loss: 12.893322944641113 = 0.24249406158924103 + 2.0 * 6.325414657592773
Epoch 650, val loss: 0.6525518894195557
Epoch 660, training loss: 12.872872352600098 = 0.22959557175636292 + 2.0 * 6.321638584136963
Epoch 660, val loss: 0.6489689946174622
Epoch 670, training loss: 12.849912643432617 = 0.21739459037780762 + 2.0 * 6.316258907318115
Epoch 670, val loss: 0.646087646484375
Epoch 680, training loss: 12.837779998779297 = 0.2058994174003601 + 2.0 * 6.3159403800964355
Epoch 680, val loss: 0.643818736076355
Epoch 690, training loss: 12.824166297912598 = 0.1951218992471695 + 2.0 * 6.3145222663879395
Epoch 690, val loss: 0.6421279907226562
Epoch 700, training loss: 12.817100524902344 = 0.18500204384326935 + 2.0 * 6.316049098968506
Epoch 700, val loss: 0.6409916877746582
Epoch 710, training loss: 12.797720909118652 = 0.17551466822624207 + 2.0 * 6.311103343963623
Epoch 710, val loss: 0.6403539776802063
Epoch 720, training loss: 12.789365768432617 = 0.16658858954906464 + 2.0 * 6.3113884925842285
Epoch 720, val loss: 0.6402273774147034
Epoch 730, training loss: 12.776899337768555 = 0.15821418166160583 + 2.0 * 6.309342384338379
Epoch 730, val loss: 0.6405954360961914
Epoch 740, training loss: 12.769706726074219 = 0.15037034451961517 + 2.0 * 6.309668064117432
Epoch 740, val loss: 0.6413089036941528
Epoch 750, training loss: 12.756839752197266 = 0.1430169939994812 + 2.0 * 6.306911468505859
Epoch 750, val loss: 0.6424668431282043
Epoch 760, training loss: 12.746681213378906 = 0.13608497381210327 + 2.0 * 6.305298328399658
Epoch 760, val loss: 0.6439704895019531
Epoch 770, training loss: 12.754192352294922 = 0.12957382202148438 + 2.0 * 6.312309265136719
Epoch 770, val loss: 0.6458021402359009
Epoch 780, training loss: 12.728382110595703 = 0.12344726920127869 + 2.0 * 6.302467346191406
Epoch 780, val loss: 0.6478778719902039
Epoch 790, training loss: 12.721923828125 = 0.11769330501556396 + 2.0 * 6.302115440368652
Epoch 790, val loss: 0.6502331495285034
Epoch 800, training loss: 12.713129043579102 = 0.1122676432132721 + 2.0 * 6.300430774688721
Epoch 800, val loss: 0.6528545022010803
Epoch 810, training loss: 12.726855278015137 = 0.10713428258895874 + 2.0 * 6.309860706329346
Epoch 810, val loss: 0.6557013988494873
Epoch 820, training loss: 12.701807022094727 = 0.10233849287033081 + 2.0 * 6.299734115600586
Epoch 820, val loss: 0.6587003469467163
Epoch 830, training loss: 12.695497512817383 = 0.09779832512140274 + 2.0 * 6.298849582672119
Epoch 830, val loss: 0.6618720889091492
Epoch 840, training loss: 12.685250282287598 = 0.09350281953811646 + 2.0 * 6.295873641967773
Epoch 840, val loss: 0.665263295173645
Epoch 850, training loss: 12.686896324157715 = 0.08945079892873764 + 2.0 * 6.298722743988037
Epoch 850, val loss: 0.6687889099121094
Epoch 860, training loss: 12.686738014221191 = 0.08562491834163666 + 2.0 * 6.300556659698486
Epoch 860, val loss: 0.6724476218223572
Epoch 870, training loss: 12.671051025390625 = 0.08200517296791077 + 2.0 * 6.294522762298584
Epoch 870, val loss: 0.676122784614563
Epoch 880, training loss: 12.664545059204102 = 0.07860688865184784 + 2.0 * 6.292969226837158
Epoch 880, val loss: 0.6799569129943848
Epoch 890, training loss: 12.657401084899902 = 0.0753697082400322 + 2.0 * 6.291015625
Epoch 890, val loss: 0.6838974356651306
Epoch 900, training loss: 12.652078628540039 = 0.07230428606271744 + 2.0 * 6.289886951446533
Epoch 900, val loss: 0.6879584789276123
Epoch 910, training loss: 12.676041603088379 = 0.06939554959535599 + 2.0 * 6.303322792053223
Epoch 910, val loss: 0.6920596957206726
Epoch 920, training loss: 12.642290115356445 = 0.06663838028907776 + 2.0 * 6.287826061248779
Epoch 920, val loss: 0.6961783766746521
Epoch 930, training loss: 12.64082145690918 = 0.06403520703315735 + 2.0 * 6.288393020629883
Epoch 930, val loss: 0.700363278388977
Epoch 940, training loss: 12.63501262664795 = 0.061562541872262955 + 2.0 * 6.286725044250488
Epoch 940, val loss: 0.7046477794647217
Epoch 950, training loss: 12.650702476501465 = 0.059215009212493896 + 2.0 * 6.295743942260742
Epoch 950, val loss: 0.7089856863021851
Epoch 960, training loss: 12.62916374206543 = 0.05699227750301361 + 2.0 * 6.286085605621338
Epoch 960, val loss: 0.7131885886192322
Epoch 970, training loss: 12.6232328414917 = 0.05488527566194534 + 2.0 * 6.284173965454102
Epoch 970, val loss: 0.7174636721611023
Epoch 980, training loss: 12.619166374206543 = 0.052884142845869064 + 2.0 * 6.283141136169434
Epoch 980, val loss: 0.7218217849731445
Epoch 990, training loss: 12.631087303161621 = 0.05097179487347603 + 2.0 * 6.29005765914917
Epoch 990, val loss: 0.7261890769004822
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.825925925925926
0.8397469688982605
=== training gcn model ===
Epoch 0, training loss: 19.134855270385742 = 1.941178798675537 + 2.0 * 8.596837997436523
Epoch 0, val loss: 1.9403941631317139
Epoch 10, training loss: 19.123830795288086 = 1.9308470487594604 + 2.0 * 8.596491813659668
Epoch 10, val loss: 1.9296667575836182
Epoch 20, training loss: 19.106136322021484 = 1.9184035062789917 + 2.0 * 8.593866348266602
Epoch 20, val loss: 1.9166300296783447
Epoch 30, training loss: 19.050548553466797 = 1.901992678642273 + 2.0 * 8.574277877807617
Epoch 30, val loss: 1.8996695280075073
Epoch 40, training loss: 18.793006896972656 = 1.8814589977264404 + 2.0 * 8.455774307250977
Epoch 40, val loss: 1.8793681859970093
Epoch 50, training loss: 17.706331253051758 = 1.8586655855178833 + 2.0 * 7.923832416534424
Epoch 50, val loss: 1.857291579246521
Epoch 60, training loss: 16.77164649963379 = 1.8395549058914185 + 2.0 * 7.466045379638672
Epoch 60, val loss: 1.8399384021759033
Epoch 70, training loss: 16.07513427734375 = 1.8272693157196045 + 2.0 * 7.123932361602783
Epoch 70, val loss: 1.8281996250152588
Epoch 80, training loss: 15.77003288269043 = 1.8129065036773682 + 2.0 * 6.97856330871582
Epoch 80, val loss: 1.8139774799346924
Epoch 90, training loss: 15.528929710388184 = 1.7968918085098267 + 2.0 * 6.866018772125244
Epoch 90, val loss: 1.7987873554229736
Epoch 100, training loss: 15.350786209106445 = 1.7811721563339233 + 2.0 * 6.784807205200195
Epoch 100, val loss: 1.7843940258026123
Epoch 110, training loss: 15.22612190246582 = 1.7666494846343994 + 2.0 * 6.729736328125
Epoch 110, val loss: 1.7710386514663696
Epoch 120, training loss: 15.1148099899292 = 1.7525694370269775 + 2.0 * 6.6811203956604
Epoch 120, val loss: 1.7577478885650635
Epoch 130, training loss: 15.034594535827637 = 1.737729787826538 + 2.0 * 6.64843225479126
Epoch 130, val loss: 1.7441693544387817
Epoch 140, training loss: 14.968557357788086 = 1.7212145328521729 + 2.0 * 6.623671531677246
Epoch 140, val loss: 1.729320764541626
Epoch 150, training loss: 14.9031982421875 = 1.7028173208236694 + 2.0 * 6.60019063949585
Epoch 150, val loss: 1.7127647399902344
Epoch 160, training loss: 14.836650848388672 = 1.6825456619262695 + 2.0 * 6.577052593231201
Epoch 160, val loss: 1.6946637630462646
Epoch 170, training loss: 14.769302368164062 = 1.659982442855835 + 2.0 * 6.554659843444824
Epoch 170, val loss: 1.6747406721115112
Epoch 180, training loss: 14.709348678588867 = 1.6344046592712402 + 2.0 * 6.537472248077393
Epoch 180, val loss: 1.652367115020752
Epoch 190, training loss: 14.640332221984863 = 1.6055047512054443 + 2.0 * 6.51741361618042
Epoch 190, val loss: 1.6272345781326294
Epoch 200, training loss: 14.572471618652344 = 1.5731253623962402 + 2.0 * 6.499673366546631
Epoch 200, val loss: 1.5990495681762695
Epoch 210, training loss: 14.5034761428833 = 1.5368025302886963 + 2.0 * 6.483336925506592
Epoch 210, val loss: 1.5674558877944946
Epoch 220, training loss: 14.439268112182617 = 1.4965797662734985 + 2.0 * 6.471343994140625
Epoch 220, val loss: 1.5324623584747314
Epoch 230, training loss: 14.371025085449219 = 1.4531257152557373 + 2.0 * 6.458949565887451
Epoch 230, val loss: 1.4950180053710938
Epoch 240, training loss: 14.304361343383789 = 1.4069538116455078 + 2.0 * 6.448703765869141
Epoch 240, val loss: 1.4553356170654297
Epoch 250, training loss: 14.237792015075684 = 1.358499526977539 + 2.0 * 6.439646244049072
Epoch 250, val loss: 1.414210557937622
Epoch 260, training loss: 14.172220230102539 = 1.308837890625 + 2.0 * 6.4316911697387695
Epoch 260, val loss: 1.3726166486740112
Epoch 270, training loss: 14.108630180358887 = 1.2593272924423218 + 2.0 * 6.424651622772217
Epoch 270, val loss: 1.3315047025680542
Epoch 280, training loss: 14.04562759399414 = 1.2102370262145996 + 2.0 * 6.417695045471191
Epoch 280, val loss: 1.2909719944000244
Epoch 290, training loss: 13.984529495239258 = 1.1619091033935547 + 2.0 * 6.411310195922852
Epoch 290, val loss: 1.2514979839324951
Epoch 300, training loss: 13.928945541381836 = 1.1152170896530151 + 2.0 * 6.406864166259766
Epoch 300, val loss: 1.2137876749038696
Epoch 310, training loss: 13.871114730834961 = 1.0707173347473145 + 2.0 * 6.400198459625244
Epoch 310, val loss: 1.1780756711959839
Epoch 320, training loss: 13.815048217773438 = 1.0280624628067017 + 2.0 * 6.393492698669434
Epoch 320, val loss: 1.1440117359161377
Epoch 330, training loss: 13.766304016113281 = 0.9871137738227844 + 2.0 * 6.389595031738281
Epoch 330, val loss: 1.1115925312042236
Epoch 340, training loss: 13.716200828552246 = 0.9483184814453125 + 2.0 * 6.383941173553467
Epoch 340, val loss: 1.0812965631484985
Epoch 350, training loss: 13.672616958618164 = 0.9119688272476196 + 2.0 * 6.380323886871338
Epoch 350, val loss: 1.0528048276901245
Epoch 360, training loss: 13.624204635620117 = 0.8771736025810242 + 2.0 * 6.373515605926514
Epoch 360, val loss: 1.0259039402008057
Epoch 370, training loss: 13.583084106445312 = 0.8437498807907104 + 2.0 * 6.369667053222656
Epoch 370, val loss: 1.000180959701538
Epoch 380, training loss: 13.546658515930176 = 0.8116496801376343 + 2.0 * 6.367504596710205
Epoch 380, val loss: 0.975760817527771
Epoch 390, training loss: 13.504825592041016 = 0.7810083031654358 + 2.0 * 6.361908435821533
Epoch 390, val loss: 0.9527375102043152
Epoch 400, training loss: 13.467544555664062 = 0.7514793276786804 + 2.0 * 6.358032703399658
Epoch 400, val loss: 0.9308805465698242
Epoch 410, training loss: 13.431987762451172 = 0.7228645086288452 + 2.0 * 6.354561805725098
Epoch 410, val loss: 0.9100632071495056
Epoch 420, training loss: 13.401093482971191 = 0.6950563192367554 + 2.0 * 6.353018760681152
Epoch 420, val loss: 0.8901838660240173
Epoch 430, training loss: 13.371794700622559 = 0.6683533191680908 + 2.0 * 6.351720809936523
Epoch 430, val loss: 0.8714038133621216
Epoch 440, training loss: 13.340184211730957 = 0.6427074670791626 + 2.0 * 6.348738193511963
Epoch 440, val loss: 0.8538045883178711
Epoch 450, training loss: 13.305766105651855 = 0.6178543567657471 + 2.0 * 6.343955993652344
Epoch 450, val loss: 0.8372711539268494
Epoch 460, training loss: 13.27431869506836 = 0.5935463905334473 + 2.0 * 6.340385913848877
Epoch 460, val loss: 0.8214223384857178
Epoch 470, training loss: 13.2464017868042 = 0.5697668194770813 + 2.0 * 6.338317394256592
Epoch 470, val loss: 0.8063119649887085
Epoch 480, training loss: 13.218744277954102 = 0.5465419888496399 + 2.0 * 6.336101055145264
Epoch 480, val loss: 0.7919026613235474
Epoch 490, training loss: 13.192218780517578 = 0.5240136384963989 + 2.0 * 6.334102630615234
Epoch 490, val loss: 0.7783371806144714
Epoch 500, training loss: 13.165806770324707 = 0.5020044445991516 + 2.0 * 6.3319010734558105
Epoch 500, val loss: 0.765507698059082
Epoch 510, training loss: 13.141080856323242 = 0.4803650677204132 + 2.0 * 6.330358028411865
Epoch 510, val loss: 0.7531821727752686
Epoch 520, training loss: 13.116217613220215 = 0.4590887129306793 + 2.0 * 6.328564643859863
Epoch 520, val loss: 0.741456925868988
Epoch 530, training loss: 13.0891695022583 = 0.4383901357650757 + 2.0 * 6.325389862060547
Epoch 530, val loss: 0.7302014231681824
Epoch 540, training loss: 13.06549072265625 = 0.4179908335208893 + 2.0 * 6.323750019073486
Epoch 540, val loss: 0.7194436192512512
Epoch 550, training loss: 13.046113014221191 = 0.39783474802970886 + 2.0 * 6.32413911819458
Epoch 550, val loss: 0.7090209126472473
Epoch 560, training loss: 13.023618698120117 = 0.37802839279174805 + 2.0 * 6.3227949142456055
Epoch 560, val loss: 0.6990581750869751
Epoch 570, training loss: 12.996232986450195 = 0.35864073038101196 + 2.0 * 6.318796157836914
Epoch 570, val loss: 0.6896345019340515
Epoch 580, training loss: 12.973587036132812 = 0.3396930694580078 + 2.0 * 6.316946983337402
Epoch 580, val loss: 0.6807655096054077
Epoch 590, training loss: 12.958855628967285 = 0.32122841477394104 + 2.0 * 6.318813800811768
Epoch 590, val loss: 0.6724411845207214
Epoch 600, training loss: 12.93256664276123 = 0.30344653129577637 + 2.0 * 6.3145599365234375
Epoch 600, val loss: 0.6648960709571838
Epoch 610, training loss: 12.911767959594727 = 0.2863870859146118 + 2.0 * 6.312690258026123
Epoch 610, val loss: 0.6581230163574219
Epoch 620, training loss: 12.892033576965332 = 0.270092248916626 + 2.0 * 6.310970783233643
Epoch 620, val loss: 0.6520761847496033
Epoch 630, training loss: 12.881159782409668 = 0.2546311318874359 + 2.0 * 6.3132643699646
Epoch 630, val loss: 0.6468512415885925
Epoch 640, training loss: 12.861225128173828 = 0.2401430755853653 + 2.0 * 6.310541152954102
Epoch 640, val loss: 0.6424634456634521
Epoch 650, training loss: 12.841568946838379 = 0.22654733061790466 + 2.0 * 6.307510852813721
Epoch 650, val loss: 0.6388776302337646
Epoch 660, training loss: 12.825348854064941 = 0.21373532712459564 + 2.0 * 6.305806636810303
Epoch 660, val loss: 0.6360254287719727
Epoch 670, training loss: 12.830894470214844 = 0.20176029205322266 + 2.0 * 6.3145670890808105
Epoch 670, val loss: 0.6337977051734924
Epoch 680, training loss: 12.801980018615723 = 0.1905890703201294 + 2.0 * 6.305695533752441
Epoch 680, val loss: 0.6321658492088318
Epoch 690, training loss: 12.784565925598145 = 0.18022209405899048 + 2.0 * 6.30217170715332
Epoch 690, val loss: 0.6312459111213684
Epoch 700, training loss: 12.771512985229492 = 0.17048728466033936 + 2.0 * 6.300512790679932
Epoch 700, val loss: 0.6307316422462463
Epoch 710, training loss: 12.760299682617188 = 0.16136136651039124 + 2.0 * 6.299468994140625
Epoch 710, val loss: 0.6307248473167419
Epoch 720, training loss: 12.751306533813477 = 0.1528000682592392 + 2.0 * 6.299253463745117
Epoch 720, val loss: 0.6311759352684021
Epoch 730, training loss: 12.749216079711914 = 0.14482104778289795 + 2.0 * 6.302197456359863
Epoch 730, val loss: 0.6319633722305298
Epoch 740, training loss: 12.738323211669922 = 0.13737349212169647 + 2.0 * 6.300474643707275
Epoch 740, val loss: 0.6330564022064209
Epoch 750, training loss: 12.724251747131348 = 0.13050740957260132 + 2.0 * 6.296872138977051
Epoch 750, val loss: 0.6345852613449097
Epoch 760, training loss: 12.713134765625 = 0.12403132021427155 + 2.0 * 6.294551849365234
Epoch 760, val loss: 0.6362534165382385
Epoch 770, training loss: 12.70433235168457 = 0.11792749166488647 + 2.0 * 6.2932024002075195
Epoch 770, val loss: 0.6381412148475647
Epoch 780, training loss: 12.698864936828613 = 0.11218032240867615 + 2.0 * 6.293342113494873
Epoch 780, val loss: 0.6402849555015564
Epoch 790, training loss: 12.690960884094238 = 0.10679080337285995 + 2.0 * 6.29208517074585
Epoch 790, val loss: 0.6426464915275574
Epoch 800, training loss: 12.686779975891113 = 0.10177039355039597 + 2.0 * 6.292504787445068
Epoch 800, val loss: 0.6452754735946655
Epoch 810, training loss: 12.676580429077148 = 0.09703656286001205 + 2.0 * 6.289772033691406
Epoch 810, val loss: 0.647960901260376
Epoch 820, training loss: 12.67107105255127 = 0.09256934374570847 + 2.0 * 6.28925085067749
Epoch 820, val loss: 0.6507642269134521
Epoch 830, training loss: 12.678295135498047 = 0.08835013955831528 + 2.0 * 6.2949724197387695
Epoch 830, val loss: 0.6537762880325317
Epoch 840, training loss: 12.659016609191895 = 0.0844310075044632 + 2.0 * 6.287292957305908
Epoch 840, val loss: 0.6569099426269531
Epoch 850, training loss: 12.6631498336792 = 0.08073544502258301 + 2.0 * 6.291207313537598
Epoch 850, val loss: 0.6602309346199036
Epoch 860, training loss: 12.64894962310791 = 0.07722827047109604 + 2.0 * 6.285860538482666
Epoch 860, val loss: 0.6633387804031372
Epoch 870, training loss: 12.64365005493164 = 0.07393744587898254 + 2.0 * 6.28485631942749
Epoch 870, val loss: 0.6668036580085754
Epoch 880, training loss: 12.641555786132812 = 0.0708174929022789 + 2.0 * 6.285368919372559
Epoch 880, val loss: 0.6702114343643188
Epoch 890, training loss: 12.633235931396484 = 0.0678793415427208 + 2.0 * 6.282678127288818
Epoch 890, val loss: 0.6737402677536011
Epoch 900, training loss: 12.633673667907715 = 0.06511417776346207 + 2.0 * 6.284279823303223
Epoch 900, val loss: 0.6773431301116943
Epoch 910, training loss: 12.626426696777344 = 0.06249969080090523 + 2.0 * 6.281963348388672
Epoch 910, val loss: 0.6809565424919128
Epoch 920, training loss: 12.623051643371582 = 0.06001869961619377 + 2.0 * 6.2815165519714355
Epoch 920, val loss: 0.6846373081207275
Epoch 930, training loss: 12.629997253417969 = 0.05767274647951126 + 2.0 * 6.286162376403809
Epoch 930, val loss: 0.6882059574127197
Epoch 940, training loss: 12.615555763244629 = 0.05547056719660759 + 2.0 * 6.28004264831543
Epoch 940, val loss: 0.6921399235725403
Epoch 950, training loss: 12.610347747802734 = 0.05336688458919525 + 2.0 * 6.2784905433654785
Epoch 950, val loss: 0.6958943605422974
Epoch 960, training loss: 12.61405086517334 = 0.051372066140174866 + 2.0 * 6.281339168548584
Epoch 960, val loss: 0.6996223330497742
Epoch 970, training loss: 12.60730266571045 = 0.04947405308485031 + 2.0 * 6.278914451599121
Epoch 970, val loss: 0.7034644484519958
Epoch 980, training loss: 12.601089477539062 = 0.04767344892024994 + 2.0 * 6.276708126068115
Epoch 980, val loss: 0.707268238067627
Epoch 990, training loss: 12.598299026489258 = 0.04596333205699921 + 2.0 * 6.276167869567871
Epoch 990, val loss: 0.7110982537269592
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8185185185185185
0.8371112282551397
The final CL Acc:0.80370, 0.02636, The final GNN Acc:0.83887, 0.00124
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.3 der2:0.2 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11574])
remove edge: torch.Size([2, 9496])
updated graph: torch.Size([2, 10514])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 19.12562370300293 = 1.9319779872894287 + 2.0 * 8.596822738647461
Epoch 0, val loss: 1.923024296760559
Epoch 10, training loss: 19.114879608154297 = 1.9218614101409912 + 2.0 * 8.596508979797363
Epoch 10, val loss: 1.9134609699249268
Epoch 20, training loss: 19.0975399017334 = 1.9092711210250854 + 2.0 * 8.594134330749512
Epoch 20, val loss: 1.901228666305542
Epoch 30, training loss: 19.041284561157227 = 1.89199697971344 + 2.0 * 8.574644088745117
Epoch 30, val loss: 1.8843404054641724
Epoch 40, training loss: 18.718692779541016 = 1.8713798522949219 + 2.0 * 8.423656463623047
Epoch 40, val loss: 1.8654451370239258
Epoch 50, training loss: 17.46164321899414 = 1.8509631156921387 + 2.0 * 7.80534029006958
Epoch 50, val loss: 1.8476982116699219
Epoch 60, training loss: 16.660171508789062 = 1.8353872299194336 + 2.0 * 7.4123921394348145
Epoch 60, val loss: 1.8347395658493042
Epoch 70, training loss: 16.072847366333008 = 1.8247143030166626 + 2.0 * 7.124066352844238
Epoch 70, val loss: 1.825076937675476
Epoch 80, training loss: 15.77891731262207 = 1.8144526481628418 + 2.0 * 6.982232570648193
Epoch 80, val loss: 1.8151237964630127
Epoch 90, training loss: 15.572356224060059 = 1.804065465927124 + 2.0 * 6.884145259857178
Epoch 90, val loss: 1.8052246570587158
Epoch 100, training loss: 15.359939575195312 = 1.7951455116271973 + 2.0 * 6.782397270202637
Epoch 100, val loss: 1.7973523139953613
Epoch 110, training loss: 15.19773006439209 = 1.7869914770126343 + 2.0 * 6.705369472503662
Epoch 110, val loss: 1.7902708053588867
Epoch 120, training loss: 15.091835021972656 = 1.7783429622650146 + 2.0 * 6.656745910644531
Epoch 120, val loss: 1.7825225591659546
Epoch 130, training loss: 14.996173858642578 = 1.769528865814209 + 2.0 * 6.613322734832764
Epoch 130, val loss: 1.7746754884719849
Epoch 140, training loss: 14.915604591369629 = 1.7604258060455322 + 2.0 * 6.577589511871338
Epoch 140, val loss: 1.7668019533157349
Epoch 150, training loss: 14.841529846191406 = 1.75040864944458 + 2.0 * 6.545560836791992
Epoch 150, val loss: 1.7584569454193115
Epoch 160, training loss: 14.77716064453125 = 1.7392605543136597 + 2.0 * 6.51894998550415
Epoch 160, val loss: 1.7494714260101318
Epoch 170, training loss: 14.719924926757812 = 1.7267060279846191 + 2.0 * 6.496609210968018
Epoch 170, val loss: 1.7394083738327026
Epoch 180, training loss: 14.667067527770996 = 1.7127339839935303 + 2.0 * 6.477166652679443
Epoch 180, val loss: 1.7282410860061646
Epoch 190, training loss: 14.618950843811035 = 1.6969289779663086 + 2.0 * 6.461010932922363
Epoch 190, val loss: 1.7156705856323242
Epoch 200, training loss: 14.578483581542969 = 1.678996205329895 + 2.0 * 6.449743747711182
Epoch 200, val loss: 1.7014954090118408
Epoch 210, training loss: 14.531959533691406 = 1.6589181423187256 + 2.0 * 6.436520576477051
Epoch 210, val loss: 1.685645580291748
Epoch 220, training loss: 14.485849380493164 = 1.6365803480148315 + 2.0 * 6.4246344566345215
Epoch 220, val loss: 1.6679916381835938
Epoch 230, training loss: 14.442479133605957 = 1.6117256879806519 + 2.0 * 6.415376663208008
Epoch 230, val loss: 1.6483991146087646
Epoch 240, training loss: 14.398920059204102 = 1.5843000411987305 + 2.0 * 6.4073100090026855
Epoch 240, val loss: 1.6267790794372559
Epoch 250, training loss: 14.355774879455566 = 1.554370641708374 + 2.0 * 6.400701999664307
Epoch 250, val loss: 1.603288173675537
Epoch 260, training loss: 14.312906265258789 = 1.5224244594573975 + 2.0 * 6.395240783691406
Epoch 260, val loss: 1.5782159566879272
Epoch 270, training loss: 14.264610290527344 = 1.4885916709899902 + 2.0 * 6.388009071350098
Epoch 270, val loss: 1.551845669746399
Epoch 280, training loss: 14.24088191986084 = 1.4531327486038208 + 2.0 * 6.393874645233154
Epoch 280, val loss: 1.5243932008743286
Epoch 290, training loss: 14.177264213562012 = 1.4167158603668213 + 2.0 * 6.380274295806885
Epoch 290, val loss: 1.4964709281921387
Epoch 300, training loss: 14.124560356140137 = 1.3795661926269531 + 2.0 * 6.372497081756592
Epoch 300, val loss: 1.468164324760437
Epoch 310, training loss: 14.078691482543945 = 1.3417659997940063 + 2.0 * 6.368462562561035
Epoch 310, val loss: 1.4397146701812744
Epoch 320, training loss: 14.033266067504883 = 1.3036240339279175 + 2.0 * 6.364820957183838
Epoch 320, val loss: 1.4113166332244873
Epoch 330, training loss: 13.988691329956055 = 1.2657135725021362 + 2.0 * 6.3614888191223145
Epoch 330, val loss: 1.3833810091018677
Epoch 340, training loss: 13.942426681518555 = 1.2278053760528564 + 2.0 * 6.357310771942139
Epoch 340, val loss: 1.355724573135376
Epoch 350, training loss: 13.896617889404297 = 1.1897319555282593 + 2.0 * 6.353443145751953
Epoch 350, val loss: 1.328239917755127
Epoch 360, training loss: 13.873605728149414 = 1.1515092849731445 + 2.0 * 6.361048221588135
Epoch 360, val loss: 1.300842046737671
Epoch 370, training loss: 13.815031051635742 = 1.113698959350586 + 2.0 * 6.350666046142578
Epoch 370, val loss: 1.2737053632736206
Epoch 380, training loss: 13.765678405761719 = 1.0762284994125366 + 2.0 * 6.344725131988525
Epoch 380, val loss: 1.2469885349273682
Epoch 390, training loss: 13.722494125366211 = 1.0389115810394287 + 2.0 * 6.341791152954102
Epoch 390, val loss: 1.2204468250274658
Epoch 400, training loss: 13.687589645385742 = 1.0018037557601929 + 2.0 * 6.342893123626709
Epoch 400, val loss: 1.19422447681427
Epoch 410, training loss: 13.644927978515625 = 0.9653104543685913 + 2.0 * 6.339808940887451
Epoch 410, val loss: 1.1685268878936768
Epoch 420, training loss: 13.601147651672363 = 0.9297544360160828 + 2.0 * 6.335696697235107
Epoch 420, val loss: 1.1437698602676392
Epoch 430, training loss: 13.561307907104492 = 0.8949758410453796 + 2.0 * 6.333166122436523
Epoch 430, val loss: 1.1197458505630493
Epoch 440, training loss: 13.52773666381836 = 0.8610706329345703 + 2.0 * 6.3333330154418945
Epoch 440, val loss: 1.0966556072235107
Epoch 450, training loss: 13.48898696899414 = 0.8282681107521057 + 2.0 * 6.33035945892334
Epoch 450, val loss: 1.0747400522232056
Epoch 460, training loss: 13.449708938598633 = 0.7967391014099121 + 2.0 * 6.326484680175781
Epoch 460, val loss: 1.054233193397522
Epoch 470, training loss: 13.415822982788086 = 0.7663237452507019 + 2.0 * 6.32474946975708
Epoch 470, val loss: 1.0350170135498047
Epoch 480, training loss: 13.384641647338867 = 0.7369999289512634 + 2.0 * 6.323821067810059
Epoch 480, val loss: 1.0172452926635742
Epoch 490, training loss: 13.352849960327148 = 0.7088950872421265 + 2.0 * 6.321977615356445
Epoch 490, val loss: 1.0009347200393677
Epoch 500, training loss: 13.32522201538086 = 0.6817900538444519 + 2.0 * 6.321715831756592
Epoch 500, val loss: 0.9860798120498657
Epoch 510, training loss: 13.293782234191895 = 0.6556528210639954 + 2.0 * 6.319064617156982
Epoch 510, val loss: 0.9725122451782227
Epoch 520, training loss: 13.26095199584961 = 0.6303675174713135 + 2.0 * 6.3152923583984375
Epoch 520, val loss: 0.9602184295654297
Epoch 530, training loss: 13.240303039550781 = 0.6057592034339905 + 2.0 * 6.317271709442139
Epoch 530, val loss: 0.9491546750068665
Epoch 540, training loss: 13.224068641662598 = 0.5818585157394409 + 2.0 * 6.321105003356934
Epoch 540, val loss: 0.9392167329788208
Epoch 550, training loss: 13.18189811706543 = 0.558876633644104 + 2.0 * 6.3115105628967285
Epoch 550, val loss: 0.9303390979766846
Epoch 560, training loss: 13.154096603393555 = 0.536500096321106 + 2.0 * 6.308798313140869
Epoch 560, val loss: 0.9224929809570312
Epoch 570, training loss: 13.128615379333496 = 0.5146352648735046 + 2.0 * 6.306990146636963
Epoch 570, val loss: 0.915493905544281
Epoch 580, training loss: 13.12199592590332 = 0.493263840675354 + 2.0 * 6.314365863800049
Epoch 580, val loss: 0.9092178344726562
Epoch 590, training loss: 13.081635475158691 = 0.4726003408432007 + 2.0 * 6.30451774597168
Epoch 590, val loss: 0.9037237167358398
Epoch 600, training loss: 13.059529304504395 = 0.4525661766529083 + 2.0 * 6.303481578826904
Epoch 600, val loss: 0.8989750742912292
Epoch 610, training loss: 13.036540031433105 = 0.4330187737941742 + 2.0 * 6.301760673522949
Epoch 610, val loss: 0.8948561549186707
Epoch 620, training loss: 13.036849975585938 = 0.4139975607395172 + 2.0 * 6.311426162719727
Epoch 620, val loss: 0.8913608193397522
Epoch 630, training loss: 13.004790306091309 = 0.39571014046669006 + 2.0 * 6.304540157318115
Epoch 630, val loss: 0.8882924914360046
Epoch 640, training loss: 12.977590560913086 = 0.3781636655330658 + 2.0 * 6.299713611602783
Epoch 640, val loss: 0.8858846426010132
Epoch 650, training loss: 12.954421043395996 = 0.3611350953578949 + 2.0 * 6.296642780303955
Epoch 650, val loss: 0.8840584754943848
Epoch 660, training loss: 12.935042381286621 = 0.3446093797683716 + 2.0 * 6.2952165603637695
Epoch 660, val loss: 0.8827372789382935
Epoch 670, training loss: 12.920089721679688 = 0.32862114906311035 + 2.0 * 6.295734405517578
Epoch 670, val loss: 0.8818336725234985
Epoch 680, training loss: 12.898646354675293 = 0.313225656747818 + 2.0 * 6.292710304260254
Epoch 680, val loss: 0.8813096284866333
Epoch 690, training loss: 12.885034561157227 = 0.29846930503845215 + 2.0 * 6.293282508850098
Epoch 690, val loss: 0.8812289834022522
Epoch 700, training loss: 12.86937141418457 = 0.28428590297698975 + 2.0 * 6.292542934417725
Epoch 700, val loss: 0.8816104531288147
Epoch 710, training loss: 12.850235939025879 = 0.27068865299224854 + 2.0 * 6.289773464202881
Epoch 710, val loss: 0.8823437094688416
Epoch 720, training loss: 12.836345672607422 = 0.2576492130756378 + 2.0 * 6.289348125457764
Epoch 720, val loss: 0.883479118347168
Epoch 730, training loss: 12.83329963684082 = 0.24518093466758728 + 2.0 * 6.2940592765808105
Epoch 730, val loss: 0.8849478960037231
Epoch 740, training loss: 12.813859939575195 = 0.23335571587085724 + 2.0 * 6.290252208709717
Epoch 740, val loss: 0.8866296410560608
Epoch 750, training loss: 12.792964935302734 = 0.2220950722694397 + 2.0 * 6.285434722900391
Epoch 750, val loss: 0.8887325525283813
Epoch 760, training loss: 12.780324935913086 = 0.21133698523044586 + 2.0 * 6.284493923187256
Epoch 760, val loss: 0.891228437423706
Epoch 770, training loss: 12.768892288208008 = 0.20105910301208496 + 2.0 * 6.283916473388672
Epoch 770, val loss: 0.8940913081169128
Epoch 780, training loss: 12.76251220703125 = 0.19129353761672974 + 2.0 * 6.285609245300293
Epoch 780, val loss: 0.8971529603004456
Epoch 790, training loss: 12.752737045288086 = 0.18208269774913788 + 2.0 * 6.285326957702637
Epoch 790, val loss: 0.9004826545715332
Epoch 800, training loss: 12.734749794006348 = 0.1733551025390625 + 2.0 * 6.280697345733643
Epoch 800, val loss: 0.904131293296814
Epoch 810, training loss: 12.72487735748291 = 0.16504976153373718 + 2.0 * 6.279913902282715
Epoch 810, val loss: 0.9081501364707947
Epoch 820, training loss: 12.749072074890137 = 0.15715080499649048 + 2.0 * 6.295960426330566
Epoch 820, val loss: 0.9124734401702881
Epoch 830, training loss: 12.705679893493652 = 0.14971718192100525 + 2.0 * 6.277981281280518
Epoch 830, val loss: 0.9167900085449219
Epoch 840, training loss: 12.698819160461426 = 0.14269830286502838 + 2.0 * 6.278060436248779
Epoch 840, val loss: 0.9214081764221191
Epoch 850, training loss: 12.68966293334961 = 0.1360314041376114 + 2.0 * 6.276815891265869
Epoch 850, val loss: 0.9263119697570801
Epoch 860, training loss: 12.684915542602539 = 0.12970951199531555 + 2.0 * 6.2776031494140625
Epoch 860, val loss: 0.9314565658569336
Epoch 870, training loss: 12.674429893493652 = 0.12372085452079773 + 2.0 * 6.275354385375977
Epoch 870, val loss: 0.9366557002067566
Epoch 880, training loss: 12.669153213500977 = 0.11807893216609955 + 2.0 * 6.275537014007568
Epoch 880, val loss: 0.942016065120697
Epoch 890, training loss: 12.665227890014648 = 0.11271798610687256 + 2.0 * 6.276255130767822
Epoch 890, val loss: 0.9475312829017639
Epoch 900, training loss: 12.65242862701416 = 0.10766740143299103 + 2.0 * 6.272380828857422
Epoch 900, val loss: 0.9532374739646912
Epoch 910, training loss: 12.652742385864258 = 0.10288179665803909 + 2.0 * 6.274930477142334
Epoch 910, val loss: 0.9590657353401184
Epoch 920, training loss: 12.644054412841797 = 0.09835030138492584 + 2.0 * 6.272851943969727
Epoch 920, val loss: 0.9648585319519043
Epoch 930, training loss: 12.636360168457031 = 0.09407168626785278 + 2.0 * 6.271144390106201
Epoch 930, val loss: 0.9708524346351624
Epoch 940, training loss: 12.634246826171875 = 0.09001162648200989 + 2.0 * 6.272117614746094
Epoch 940, val loss: 0.9768843054771423
Epoch 950, training loss: 12.625268936157227 = 0.08616054058074951 + 2.0 * 6.269554138183594
Epoch 950, val loss: 0.9829775094985962
Epoch 960, training loss: 12.622594833374023 = 0.08251363039016724 + 2.0 * 6.270040512084961
Epoch 960, val loss: 0.9890812635421753
Epoch 970, training loss: 12.613011360168457 = 0.07906998693943024 + 2.0 * 6.266970634460449
Epoch 970, val loss: 0.9952868223190308
Epoch 980, training loss: 12.610895156860352 = 0.07579141855239868 + 2.0 * 6.267551898956299
Epoch 980, val loss: 1.0015430450439453
Epoch 990, training loss: 12.614274978637695 = 0.07268571853637695 + 2.0 * 6.270794868469238
Epoch 990, val loss: 1.0077444314956665
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7592592592592593
0.8081180811808119
=== training gcn model ===
Epoch 0, training loss: 19.152650833129883 = 1.9589879512786865 + 2.0 * 8.596831321716309
Epoch 0, val loss: 1.9614946842193604
Epoch 10, training loss: 19.14105987548828 = 1.9482306241989136 + 2.0 * 8.596414566040039
Epoch 10, val loss: 1.9510959386825562
Epoch 20, training loss: 19.120073318481445 = 1.9342154264450073 + 2.0 * 8.592928886413574
Epoch 20, val loss: 1.9370654821395874
Epoch 30, training loss: 19.051197052001953 = 1.9142613410949707 + 2.0 * 8.56846809387207
Epoch 30, val loss: 1.9167715311050415
Epoch 40, training loss: 18.75191879272461 = 1.8895093202590942 + 2.0 * 8.431204795837402
Epoch 40, val loss: 1.892774224281311
Epoch 50, training loss: 17.857803344726562 = 1.8635847568511963 + 2.0 * 7.9971089363098145
Epoch 50, val loss: 1.868402123451233
Epoch 60, training loss: 16.996061325073242 = 1.8447914123535156 + 2.0 * 7.575634956359863
Epoch 60, val loss: 1.8520246744155884
Epoch 70, training loss: 16.25185775756836 = 1.8311705589294434 + 2.0 * 7.210343837738037
Epoch 70, val loss: 1.8392516374588013
Epoch 80, training loss: 15.8334321975708 = 1.8193522691726685 + 2.0 * 7.007040023803711
Epoch 80, val loss: 1.827109456062317
Epoch 90, training loss: 15.566924095153809 = 1.8072988986968994 + 2.0 * 6.879812717437744
Epoch 90, val loss: 1.8149025440216064
Epoch 100, training loss: 15.409168243408203 = 1.795774221420288 + 2.0 * 6.806696891784668
Epoch 100, val loss: 1.8033778667449951
Epoch 110, training loss: 15.280582427978516 = 1.7836809158325195 + 2.0 * 6.748450756072998
Epoch 110, val loss: 1.791464924812317
Epoch 120, training loss: 15.166276931762695 = 1.7720521688461304 + 2.0 * 6.697112560272217
Epoch 120, val loss: 1.7799592018127441
Epoch 130, training loss: 15.068967819213867 = 1.7610347270965576 + 2.0 * 6.653966426849365
Epoch 130, val loss: 1.7690125703811646
Epoch 140, training loss: 14.976722717285156 = 1.7494453191757202 + 2.0 * 6.613638877868652
Epoch 140, val loss: 1.7578034400939941
Epoch 150, training loss: 14.900230407714844 = 1.7366423606872559 + 2.0 * 6.581793785095215
Epoch 150, val loss: 1.746045470237732
Epoch 160, training loss: 14.829258918762207 = 1.722370982170105 + 2.0 * 6.553443908691406
Epoch 160, val loss: 1.7332433462142944
Epoch 170, training loss: 14.7662353515625 = 1.7066984176635742 + 2.0 * 6.529768466949463
Epoch 170, val loss: 1.7193728685379028
Epoch 180, training loss: 14.708654403686523 = 1.6892180442810059 + 2.0 * 6.50971794128418
Epoch 180, val loss: 1.7040077447891235
Epoch 190, training loss: 14.660266876220703 = 1.6695077419281006 + 2.0 * 6.495379447937012
Epoch 190, val loss: 1.6868579387664795
Epoch 200, training loss: 14.605741500854492 = 1.6476542949676514 + 2.0 * 6.479043483734131
Epoch 200, val loss: 1.667977213859558
Epoch 210, training loss: 14.552094459533691 = 1.6236108541488647 + 2.0 * 6.464241981506348
Epoch 210, val loss: 1.6473095417022705
Epoch 220, training loss: 14.501787185668945 = 1.596945881843567 + 2.0 * 6.452420711517334
Epoch 220, val loss: 1.624525785446167
Epoch 230, training loss: 14.452996253967285 = 1.5674960613250732 + 2.0 * 6.442749977111816
Epoch 230, val loss: 1.5994700193405151
Epoch 240, training loss: 14.398058891296387 = 1.5352355241775513 + 2.0 * 6.4314117431640625
Epoch 240, val loss: 1.5721780061721802
Epoch 250, training loss: 14.351164817810059 = 1.500243067741394 + 2.0 * 6.4254608154296875
Epoch 250, val loss: 1.5428441762924194
Epoch 260, training loss: 14.294354438781738 = 1.4628655910491943 + 2.0 * 6.415744304656982
Epoch 260, val loss: 1.5117952823638916
Epoch 270, training loss: 14.240123748779297 = 1.42333984375 + 2.0 * 6.408391952514648
Epoch 270, val loss: 1.4793511629104614
Epoch 280, training loss: 14.187127113342285 = 1.381912350654602 + 2.0 * 6.402607440948486
Epoch 280, val loss: 1.4458415508270264
Epoch 290, training loss: 14.132206916809082 = 1.3392326831817627 + 2.0 * 6.396487236022949
Epoch 290, val loss: 1.4119192361831665
Epoch 300, training loss: 14.080802917480469 = 1.2959152460098267 + 2.0 * 6.392443656921387
Epoch 300, val loss: 1.3780778646469116
Epoch 310, training loss: 14.030670166015625 = 1.2528157234191895 + 2.0 * 6.388926982879639
Epoch 310, val loss: 1.3451533317565918
Epoch 320, training loss: 13.977797508239746 = 1.2107511758804321 + 2.0 * 6.383522987365723
Epoch 320, val loss: 1.3136943578720093
Epoch 330, training loss: 13.925288200378418 = 1.1694514751434326 + 2.0 * 6.377918243408203
Epoch 330, val loss: 1.2834665775299072
Epoch 340, training loss: 13.875171661376953 = 1.1289974451065063 + 2.0 * 6.373086929321289
Epoch 340, val loss: 1.254384994506836
Epoch 350, training loss: 13.82769775390625 = 1.0894670486450195 + 2.0 * 6.369115352630615
Epoch 350, val loss: 1.2265479564666748
Epoch 360, training loss: 13.798330307006836 = 1.050950050354004 + 2.0 * 6.373690128326416
Epoch 360, val loss: 1.2000023126602173
Epoch 370, training loss: 13.74364185333252 = 1.0141748189926147 + 2.0 * 6.364733695983887
Epoch 370, val loss: 1.1747967004776
Epoch 380, training loss: 13.69941234588623 = 0.9789056181907654 + 2.0 * 6.36025333404541
Epoch 380, val loss: 1.1510287523269653
Epoch 390, training loss: 13.65676498413086 = 0.9448503851890564 + 2.0 * 6.355957508087158
Epoch 390, val loss: 1.1283785104751587
Epoch 400, training loss: 13.620231628417969 = 0.9120524525642395 + 2.0 * 6.354089736938477
Epoch 400, val loss: 1.106793999671936
Epoch 410, training loss: 13.58223819732666 = 0.8808934688568115 + 2.0 * 6.350672245025635
Epoch 410, val loss: 1.0865141153335571
Epoch 420, training loss: 13.546060562133789 = 0.8510194420814514 + 2.0 * 6.347520351409912
Epoch 420, val loss: 1.0673726797103882
Epoch 430, training loss: 13.517988204956055 = 0.822247326374054 + 2.0 * 6.347870349884033
Epoch 430, val loss: 1.0491787195205688
Epoch 440, training loss: 13.480698585510254 = 0.7945519685745239 + 2.0 * 6.34307336807251
Epoch 440, val loss: 1.0320584774017334
Epoch 450, training loss: 13.447555541992188 = 0.7678076028823853 + 2.0 * 6.339873790740967
Epoch 450, val loss: 1.0158144235610962
Epoch 460, training loss: 13.415884971618652 = 0.7418667078018188 + 2.0 * 6.337008953094482
Epoch 460, val loss: 1.0005162954330444
Epoch 470, training loss: 13.401890754699707 = 0.7168349027633667 + 2.0 * 6.342527866363525
Epoch 470, val loss: 0.986284613609314
Epoch 480, training loss: 13.358621597290039 = 0.692842960357666 + 2.0 * 6.332889556884766
Epoch 480, val loss: 0.9729453921318054
Epoch 490, training loss: 13.330120086669922 = 0.6695486307144165 + 2.0 * 6.330285549163818
Epoch 490, val loss: 0.9606820344924927
Epoch 500, training loss: 13.302995681762695 = 0.6467273831367493 + 2.0 * 6.328134059906006
Epoch 500, val loss: 0.9491398334503174
Epoch 510, training loss: 13.282354354858398 = 0.6243163347244263 + 2.0 * 6.329019069671631
Epoch 510, val loss: 0.9383231997489929
Epoch 520, training loss: 13.260574340820312 = 0.6025363802909851 + 2.0 * 6.329019069671631
Epoch 520, val loss: 0.9281734228134155
Epoch 530, training loss: 13.227928161621094 = 0.5814270973205566 + 2.0 * 6.323250770568848
Epoch 530, val loss: 0.9189664125442505
Epoch 540, training loss: 13.203654289245605 = 0.5607212781906128 + 2.0 * 6.321466445922852
Epoch 540, val loss: 0.9103894829750061
Epoch 550, training loss: 13.177909851074219 = 0.5403121709823608 + 2.0 * 6.318799018859863
Epoch 550, val loss: 0.9023845791816711
Epoch 560, training loss: 13.153777122497559 = 0.5201765894889832 + 2.0 * 6.316800117492676
Epoch 560, val loss: 0.894982099533081
Epoch 570, training loss: 13.131303787231445 = 0.5003199577331543 + 2.0 * 6.315491676330566
Epoch 570, val loss: 0.8881552815437317
Epoch 580, training loss: 13.112272262573242 = 0.4808388650417328 + 2.0 * 6.315716743469238
Epoch 580, val loss: 0.8819485306739807
Epoch 590, training loss: 13.087379455566406 = 0.4618922472000122 + 2.0 * 6.312743663787842
Epoch 590, val loss: 0.8763953447341919
Epoch 600, training loss: 13.066895484924316 = 0.44338130950927734 + 2.0 * 6.3117570877075195
Epoch 600, val loss: 0.8714441061019897
Epoch 610, training loss: 13.04833984375 = 0.42520079016685486 + 2.0 * 6.311569690704346
Epoch 610, val loss: 0.8669938445091248
Epoch 620, training loss: 13.02407169342041 = 0.4074501395225525 + 2.0 * 6.3083109855651855
Epoch 620, val loss: 0.863068699836731
Epoch 630, training loss: 13.0120267868042 = 0.3901788592338562 + 2.0 * 6.310924053192139
Epoch 630, val loss: 0.8596475720405579
Epoch 640, training loss: 12.985403060913086 = 0.37335073947906494 + 2.0 * 6.306025981903076
Epoch 640, val loss: 0.8568117618560791
Epoch 650, training loss: 12.967284202575684 = 0.3570249080657959 + 2.0 * 6.305129528045654
Epoch 650, val loss: 0.8544355630874634
Epoch 660, training loss: 12.947681427001953 = 0.3411065936088562 + 2.0 * 6.303287506103516
Epoch 660, val loss: 0.8526270985603333
Epoch 670, training loss: 12.942540168762207 = 0.32565951347351074 + 2.0 * 6.308440208435059
Epoch 670, val loss: 0.8512501120567322
Epoch 680, training loss: 12.918357849121094 = 0.3107718527317047 + 2.0 * 6.303792953491211
Epoch 680, val loss: 0.8504490256309509
Epoch 690, training loss: 12.89683723449707 = 0.29640838503837585 + 2.0 * 6.3002142906188965
Epoch 690, val loss: 0.8500030636787415
Epoch 700, training loss: 12.884710311889648 = 0.28255924582481384 + 2.0 * 6.301075458526611
Epoch 700, val loss: 0.8500807285308838
Epoch 710, training loss: 12.866796493530273 = 0.2692560851573944 + 2.0 * 6.298770427703857
Epoch 710, val loss: 0.8506802320480347
Epoch 720, training loss: 12.853693008422852 = 0.2565620541572571 + 2.0 * 6.29856538772583
Epoch 720, val loss: 0.8515224456787109
Epoch 730, training loss: 12.835308074951172 = 0.2443748116493225 + 2.0 * 6.295466423034668
Epoch 730, val loss: 0.8529012799263
Epoch 740, training loss: 12.84095287322998 = 0.23268313705921173 + 2.0 * 6.304134845733643
Epoch 740, val loss: 0.8546741008758545
Epoch 750, training loss: 12.811572074890137 = 0.22159221768379211 + 2.0 * 6.294990062713623
Epoch 750, val loss: 0.8567000031471252
Epoch 760, training loss: 12.798462867736816 = 0.21099047362804413 + 2.0 * 6.293735980987549
Epoch 760, val loss: 0.859067976474762
Epoch 770, training loss: 12.796733856201172 = 0.2008475512266159 + 2.0 * 6.297943115234375
Epoch 770, val loss: 0.8618356585502625
Epoch 780, training loss: 12.778372764587402 = 0.19122447073459625 + 2.0 * 6.293574333190918
Epoch 780, val loss: 0.8648490905761719
Epoch 790, training loss: 12.76603889465332 = 0.18206879496574402 + 2.0 * 6.291985034942627
Epoch 790, val loss: 0.8681479096412659
Epoch 800, training loss: 12.75670051574707 = 0.17336849868297577 + 2.0 * 6.291666030883789
Epoch 800, val loss: 0.8716827630996704
Epoch 810, training loss: 12.741345405578613 = 0.16510748863220215 + 2.0 * 6.288118839263916
Epoch 810, val loss: 0.8754525780677795
Epoch 820, training loss: 12.735142707824707 = 0.1572403609752655 + 2.0 * 6.288951396942139
Epoch 820, val loss: 0.8793609738349915
Epoch 830, training loss: 12.72713565826416 = 0.14977820217609406 + 2.0 * 6.2886786460876465
Epoch 830, val loss: 0.8836007714271545
Epoch 840, training loss: 12.731247901916504 = 0.14271186292171478 + 2.0 * 6.2942681312561035
Epoch 840, val loss: 0.8879356384277344
Epoch 850, training loss: 12.70789909362793 = 0.1360468864440918 + 2.0 * 6.28592586517334
Epoch 850, val loss: 0.8925482630729675
Epoch 860, training loss: 12.699015617370605 = 0.12975040078163147 + 2.0 * 6.284632682800293
Epoch 860, val loss: 0.8971474170684814
Epoch 870, training loss: 12.690542221069336 = 0.12375574558973312 + 2.0 * 6.283393383026123
Epoch 870, val loss: 0.9020004272460938
Epoch 880, training loss: 12.692634582519531 = 0.11807218194007874 + 2.0 * 6.287281036376953
Epoch 880, val loss: 0.9069132804870605
Epoch 890, training loss: 12.685633659362793 = 0.1126929521560669 + 2.0 * 6.286470413208008
Epoch 890, val loss: 0.9119651317596436
Epoch 900, training loss: 12.675195693969727 = 0.1076265200972557 + 2.0 * 6.28378438949585
Epoch 900, val loss: 0.917048990726471
Epoch 910, training loss: 12.66535472869873 = 0.10284052789211273 + 2.0 * 6.281257152557373
Epoch 910, val loss: 0.9222326874732971
Epoch 920, training loss: 12.661450386047363 = 0.09830499440431595 + 2.0 * 6.2815728187561035
Epoch 920, val loss: 0.9275211095809937
Epoch 930, training loss: 12.653613090515137 = 0.09400340914726257 + 2.0 * 6.279804706573486
Epoch 930, val loss: 0.9328215718269348
Epoch 940, training loss: 12.648877143859863 = 0.08992847055196762 + 2.0 * 6.279474258422852
Epoch 940, val loss: 0.9382327795028687
Epoch 950, training loss: 12.648369789123535 = 0.08606881648302078 + 2.0 * 6.2811503410339355
Epoch 950, val loss: 0.9437600374221802
Epoch 960, training loss: 12.635004997253418 = 0.08242812007665634 + 2.0 * 6.2762885093688965
Epoch 960, val loss: 0.9491815567016602
Epoch 970, training loss: 12.63123893737793 = 0.07897786796092987 + 2.0 * 6.276130676269531
Epoch 970, val loss: 0.9547269344329834
Epoch 980, training loss: 12.625667572021484 = 0.07569944858551025 + 2.0 * 6.274983882904053
Epoch 980, val loss: 0.9603029489517212
Epoch 990, training loss: 12.644169807434082 = 0.07258124649524689 + 2.0 * 6.285794258117676
Epoch 990, val loss: 0.9660705327987671
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7296296296296296
0.8065366367949395
=== training gcn model ===
Epoch 0, training loss: 19.137680053710938 = 1.943986415863037 + 2.0 * 8.596846580505371
Epoch 0, val loss: 1.9454989433288574
Epoch 10, training loss: 19.127138137817383 = 1.9340317249298096 + 2.0 * 8.596552848815918
Epoch 10, val loss: 1.9349335432052612
Epoch 20, training loss: 19.110275268554688 = 1.9219032526016235 + 2.0 * 8.594185829162598
Epoch 20, val loss: 1.9219154119491577
Epoch 30, training loss: 19.05693817138672 = 1.9054781198501587 + 2.0 * 8.575730323791504
Epoch 30, val loss: 1.9043461084365845
Epoch 40, training loss: 18.792858123779297 = 1.8851226568222046 + 2.0 * 8.45386791229248
Epoch 40, val loss: 1.8834830522537231
Epoch 50, training loss: 17.537673950195312 = 1.8637168407440186 + 2.0 * 7.836978912353516
Epoch 50, val loss: 1.8622221946716309
Epoch 60, training loss: 16.89853858947754 = 1.8462977409362793 + 2.0 * 7.526120185852051
Epoch 60, val loss: 1.8461620807647705
Epoch 70, training loss: 16.372323989868164 = 1.8329914808273315 + 2.0 * 7.26966667175293
Epoch 70, val loss: 1.8336474895477295
Epoch 80, training loss: 15.942314147949219 = 1.820271372795105 + 2.0 * 7.061021327972412
Epoch 80, val loss: 1.8215148448944092
Epoch 90, training loss: 15.633540153503418 = 1.808143973350525 + 2.0 * 6.912698268890381
Epoch 90, val loss: 1.810167908668518
Epoch 100, training loss: 15.426901817321777 = 1.796263575553894 + 2.0 * 6.815319061279297
Epoch 100, val loss: 1.7987598180770874
Epoch 110, training loss: 15.275959968566895 = 1.7852885723114014 + 2.0 * 6.745335578918457
Epoch 110, val loss: 1.7882766723632812
Epoch 120, training loss: 15.16492748260498 = 1.7752397060394287 + 2.0 * 6.694843769073486
Epoch 120, val loss: 1.7784430980682373
Epoch 130, training loss: 15.071852684020996 = 1.7650593519210815 + 2.0 * 6.6533966064453125
Epoch 130, val loss: 1.7686071395874023
Epoch 140, training loss: 14.973760604858398 = 1.754284381866455 + 2.0 * 6.609737873077393
Epoch 140, val loss: 1.7586702108383179
Epoch 150, training loss: 14.895495414733887 = 1.742808222770691 + 2.0 * 6.576343536376953
Epoch 150, val loss: 1.7484322786331177
Epoch 160, training loss: 14.827045440673828 = 1.7299901247024536 + 2.0 * 6.548527717590332
Epoch 160, val loss: 1.7373559474945068
Epoch 170, training loss: 14.76613998413086 = 1.7153555154800415 + 2.0 * 6.525392055511475
Epoch 170, val loss: 1.7248724699020386
Epoch 180, training loss: 14.719742774963379 = 1.6987642049789429 + 2.0 * 6.510489463806152
Epoch 180, val loss: 1.7109298706054688
Epoch 190, training loss: 14.660990715026855 = 1.680130124092102 + 2.0 * 6.4904303550720215
Epoch 190, val loss: 1.6952420473098755
Epoch 200, training loss: 14.611169815063477 = 1.6592663526535034 + 2.0 * 6.475951671600342
Epoch 200, val loss: 1.677754282951355
Epoch 210, training loss: 14.561737060546875 = 1.635913372039795 + 2.0 * 6.462912082672119
Epoch 210, val loss: 1.6582591533660889
Epoch 220, training loss: 14.512495040893555 = 1.6098830699920654 + 2.0 * 6.451305866241455
Epoch 220, val loss: 1.6366524696350098
Epoch 230, training loss: 14.478714942932129 = 1.5811777114868164 + 2.0 * 6.448768615722656
Epoch 230, val loss: 1.6127415895462036
Epoch 240, training loss: 14.415518760681152 = 1.549957513809204 + 2.0 * 6.432780742645264
Epoch 240, val loss: 1.5871165990829468
Epoch 250, training loss: 14.362752914428711 = 1.5164083242416382 + 2.0 * 6.423172473907471
Epoch 250, val loss: 1.5597660541534424
Epoch 260, training loss: 14.310331344604492 = 1.480501413345337 + 2.0 * 6.414915084838867
Epoch 260, val loss: 1.5307196378707886
Epoch 270, training loss: 14.257238388061523 = 1.44248628616333 + 2.0 * 6.407375812530518
Epoch 270, val loss: 1.5003994703292847
Epoch 280, training loss: 14.20693302154541 = 1.4030078649520874 + 2.0 * 6.401962757110596
Epoch 280, val loss: 1.469433069229126
Epoch 290, training loss: 14.156715393066406 = 1.3630270957946777 + 2.0 * 6.396843910217285
Epoch 290, val loss: 1.4386142492294312
Epoch 300, training loss: 14.100699424743652 = 1.3227183818817139 + 2.0 * 6.38899040222168
Epoch 300, val loss: 1.4080387353897095
Epoch 310, training loss: 14.050901412963867 = 1.282233476638794 + 2.0 * 6.384334087371826
Epoch 310, val loss: 1.3778939247131348
Epoch 320, training loss: 14.005167961120605 = 1.2421084642410278 + 2.0 * 6.381529808044434
Epoch 320, val loss: 1.3484257459640503
Epoch 330, training loss: 13.957860946655273 = 1.2030224800109863 + 2.0 * 6.3774189949035645
Epoch 330, val loss: 1.3201465606689453
Epoch 340, training loss: 13.906801223754883 = 1.1648591756820679 + 2.0 * 6.370971202850342
Epoch 340, val loss: 1.2929579019546509
Epoch 350, training loss: 13.865204811096191 = 1.1274664402008057 + 2.0 * 6.368869304656982
Epoch 350, val loss: 1.2665393352508545
Epoch 360, training loss: 13.821858406066895 = 1.0910871028900146 + 2.0 * 6.36538553237915
Epoch 360, val loss: 1.2414087057113647
Epoch 370, training loss: 13.775399208068848 = 1.055763840675354 + 2.0 * 6.3598175048828125
Epoch 370, val loss: 1.2168506383895874
Epoch 380, training loss: 13.739250183105469 = 1.0212453603744507 + 2.0 * 6.359002590179443
Epoch 380, val loss: 1.1931754350662231
Epoch 390, training loss: 13.696797370910645 = 0.9876576066017151 + 2.0 * 6.354569911956787
Epoch 390, val loss: 1.1703543663024902
Epoch 400, training loss: 13.656791687011719 = 0.9550489187240601 + 2.0 * 6.350871562957764
Epoch 400, val loss: 1.148301362991333
Epoch 410, training loss: 13.621796607971191 = 0.9231795072555542 + 2.0 * 6.349308490753174
Epoch 410, val loss: 1.1268409490585327
Epoch 420, training loss: 13.583914756774902 = 0.8920875191688538 + 2.0 * 6.345913410186768
Epoch 420, val loss: 1.1061279773712158
Epoch 430, training loss: 13.54692268371582 = 0.8618883490562439 + 2.0 * 6.342517375946045
Epoch 430, val loss: 1.085891604423523
Epoch 440, training loss: 13.51246166229248 = 0.8322733640670776 + 2.0 * 6.340094089508057
Epoch 440, val loss: 1.0665746927261353
Epoch 450, training loss: 13.485987663269043 = 0.8033521771430969 + 2.0 * 6.341317653656006
Epoch 450, val loss: 1.04755699634552
Epoch 460, training loss: 13.448938369750977 = 0.7752724885940552 + 2.0 * 6.3368330001831055
Epoch 460, val loss: 1.0292727947235107
Epoch 470, training loss: 13.415179252624512 = 0.7479679584503174 + 2.0 * 6.333605766296387
Epoch 470, val loss: 1.0117243528366089
Epoch 480, training loss: 13.393902778625488 = 0.7212842702865601 + 2.0 * 6.336309432983398
Epoch 480, val loss: 0.9948691129684448
Epoch 490, training loss: 13.360697746276855 = 0.6952866911888123 + 2.0 * 6.332705497741699
Epoch 490, val loss: 0.9789214134216309
Epoch 500, training loss: 13.325358390808105 = 0.6701403856277466 + 2.0 * 6.327609062194824
Epoch 500, val loss: 0.9636119604110718
Epoch 510, training loss: 13.295483589172363 = 0.645621657371521 + 2.0 * 6.3249311447143555
Epoch 510, val loss: 0.9492353796958923
Epoch 520, training loss: 13.267236709594727 = 0.621586799621582 + 2.0 * 6.322824954986572
Epoch 520, val loss: 0.9353899359703064
Epoch 530, training loss: 13.244372367858887 = 0.5981480479240417 + 2.0 * 6.3231120109558105
Epoch 530, val loss: 0.9222248792648315
Epoch 540, training loss: 13.214741706848145 = 0.5753856897354126 + 2.0 * 6.319677829742432
Epoch 540, val loss: 0.9100033044815063
Epoch 550, training loss: 13.18860912322998 = 0.5531365871429443 + 2.0 * 6.3177361488342285
Epoch 550, val loss: 0.8984355330467224
Epoch 560, training loss: 13.165602684020996 = 0.5312299728393555 + 2.0 * 6.31718635559082
Epoch 560, val loss: 0.887459933757782
Epoch 570, training loss: 13.147902488708496 = 0.5097877979278564 + 2.0 * 6.319057464599609
Epoch 570, val loss: 0.8771971464157104
Epoch 580, training loss: 13.117260932922363 = 0.48890170454978943 + 2.0 * 6.314179420471191
Epoch 580, val loss: 0.8675561547279358
Epoch 590, training loss: 13.091100692749023 = 0.4684115946292877 + 2.0 * 6.311344623565674
Epoch 590, val loss: 0.8587297797203064
Epoch 600, training loss: 13.068107604980469 = 0.44821226596832275 + 2.0 * 6.309947490692139
Epoch 600, val loss: 0.8504385352134705
Epoch 610, training loss: 13.067148208618164 = 0.42834708094596863 + 2.0 * 6.319400787353516
Epoch 610, val loss: 0.8427743315696716
Epoch 620, training loss: 13.028014183044434 = 0.40904808044433594 + 2.0 * 6.309483051300049
Epoch 620, val loss: 0.8357279300689697
Epoch 630, training loss: 13.003558158874512 = 0.3902164399623871 + 2.0 * 6.306670665740967
Epoch 630, val loss: 0.8294088840484619
Epoch 640, training loss: 12.982646942138672 = 0.37186694145202637 + 2.0 * 6.305389881134033
Epoch 640, val loss: 0.8236874938011169
Epoch 650, training loss: 12.968990325927734 = 0.3540014326572418 + 2.0 * 6.307494640350342
Epoch 650, val loss: 0.8186681270599365
Epoch 660, training loss: 12.951911926269531 = 0.3367469012737274 + 2.0 * 6.307582378387451
Epoch 660, val loss: 0.8142674565315247
Epoch 670, training loss: 12.923381805419922 = 0.320144921541214 + 2.0 * 6.301618576049805
Epoch 670, val loss: 0.8105284571647644
Epoch 680, training loss: 12.907381057739258 = 0.3041635751724243 + 2.0 * 6.301608562469482
Epoch 680, val loss: 0.8075556755065918
Epoch 690, training loss: 12.892792701721191 = 0.2888346016407013 + 2.0 * 6.301979064941406
Epoch 690, val loss: 0.8052834272384644
Epoch 700, training loss: 12.874344825744629 = 0.27417024970054626 + 2.0 * 6.3000874519348145
Epoch 700, val loss: 0.80369633436203
Epoch 710, training loss: 12.857770919799805 = 0.260262668132782 + 2.0 * 6.2987542152404785
Epoch 710, val loss: 0.8025917410850525
Epoch 720, training loss: 12.839426040649414 = 0.24700984358787537 + 2.0 * 6.296207904815674
Epoch 720, val loss: 0.8021982312202454
Epoch 730, training loss: 12.825638771057129 = 0.23438678681850433 + 2.0 * 6.295626163482666
Epoch 730, val loss: 0.8022419810295105
Epoch 740, training loss: 12.817939758300781 = 0.2224673479795456 + 2.0 * 6.297736167907715
Epoch 740, val loss: 0.8028709292411804
Epoch 750, training loss: 12.800867080688477 = 0.21120525896549225 + 2.0 * 6.294830799102783
Epoch 750, val loss: 0.8039565086364746
Epoch 760, training loss: 12.783563613891602 = 0.2006090134382248 + 2.0 * 6.291477203369141
Epoch 760, val loss: 0.8055591583251953
Epoch 770, training loss: 12.774964332580566 = 0.19056835770606995 + 2.0 * 6.292198181152344
Epoch 770, val loss: 0.8075040578842163
Epoch 780, training loss: 12.762798309326172 = 0.1811133325099945 + 2.0 * 6.290842533111572
Epoch 780, val loss: 0.8098703622817993
Epoch 790, training loss: 12.751428604125977 = 0.17222531139850616 + 2.0 * 6.289601802825928
Epoch 790, val loss: 0.8125726580619812
Epoch 800, training loss: 12.740220069885254 = 0.16386672854423523 + 2.0 * 6.288176536560059
Epoch 800, val loss: 0.8155755400657654
Epoch 810, training loss: 12.735258102416992 = 0.15596772730350494 + 2.0 * 6.289645195007324
Epoch 810, val loss: 0.8188852071762085
Epoch 820, training loss: 12.726677894592285 = 0.14854440093040466 + 2.0 * 6.289066791534424
Epoch 820, val loss: 0.8224455714225769
Epoch 830, training loss: 12.714066505432129 = 0.1415444016456604 + 2.0 * 6.286261081695557
Epoch 830, val loss: 0.8261699676513672
Epoch 840, training loss: 12.704813957214355 = 0.13495603203773499 + 2.0 * 6.284928798675537
Epoch 840, val loss: 0.8301459550857544
Epoch 850, training loss: 12.701679229736328 = 0.1287360042333603 + 2.0 * 6.286471843719482
Epoch 850, val loss: 0.83424311876297
Epoch 860, training loss: 12.68856143951416 = 0.12288083136081696 + 2.0 * 6.282840251922607
Epoch 860, val loss: 0.8385685086250305
Epoch 870, training loss: 12.687600135803223 = 0.11735393851995468 + 2.0 * 6.285122871398926
Epoch 870, val loss: 0.8429411053657532
Epoch 880, training loss: 12.673209190368652 = 0.11214042454957962 + 2.0 * 6.280534267425537
Epoch 880, val loss: 0.8473899960517883
Epoch 890, training loss: 12.666850090026855 = 0.10720274597406387 + 2.0 * 6.2798237800598145
Epoch 890, val loss: 0.8519508838653564
Epoch 900, training loss: 12.664121627807617 = 0.10251247137784958 + 2.0 * 6.280804634094238
Epoch 900, val loss: 0.8566364049911499
Epoch 910, training loss: 12.656793594360352 = 0.09807831794023514 + 2.0 * 6.279357433319092
Epoch 910, val loss: 0.8613752126693726
Epoch 920, training loss: 12.661650657653809 = 0.09390352666378021 + 2.0 * 6.283873558044434
Epoch 920, val loss: 0.8660705089569092
Epoch 930, training loss: 12.644010543823242 = 0.08996962010860443 + 2.0 * 6.277020454406738
Epoch 930, val loss: 0.8707993030548096
Epoch 940, training loss: 12.63854694366455 = 0.0862339586019516 + 2.0 * 6.276156425476074
Epoch 940, val loss: 0.8755045533180237
Epoch 950, training loss: 12.63148021697998 = 0.08269664645195007 + 2.0 * 6.2743916511535645
Epoch 950, val loss: 0.8802937269210815
Epoch 960, training loss: 12.626784324645996 = 0.07932943105697632 + 2.0 * 6.2737274169921875
Epoch 960, val loss: 0.8851499557495117
Epoch 970, training loss: 12.631624221801758 = 0.07612352073192596 + 2.0 * 6.277750492095947
Epoch 970, val loss: 0.8900683522224426
Epoch 980, training loss: 12.628978729248047 = 0.07308020442724228 + 2.0 * 6.277949333190918
Epoch 980, val loss: 0.8948491811752319
Epoch 990, training loss: 12.6226167678833 = 0.07019954919815063 + 2.0 * 6.276208400726318
Epoch 990, val loss: 0.8996621370315552
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7407407407407408
0.8060094886663153
The final CL Acc:0.74321, 0.01222, The final GNN Acc:0.80689, 0.00090
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13184])
remove edge: torch.Size([2, 7790])
updated graph: torch.Size([2, 10418])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 19.132404327392578 = 1.9388432502746582 + 2.0 * 8.596780776977539
Epoch 0, val loss: 1.9369395971298218
Epoch 10, training loss: 19.120832443237305 = 1.9284900426864624 + 2.0 * 8.596171379089355
Epoch 10, val loss: 1.9266258478164673
Epoch 20, training loss: 19.097923278808594 = 1.9157764911651611 + 2.0 * 8.591073036193848
Epoch 20, val loss: 1.9138988256454468
Epoch 30, training loss: 19.005128860473633 = 1.8988840579986572 + 2.0 * 8.553122520446777
Epoch 30, val loss: 1.8972082138061523
Epoch 40, training loss: 18.463411331176758 = 1.8795150518417358 + 2.0 * 8.291948318481445
Epoch 40, val loss: 1.8787891864776611
Epoch 50, training loss: 17.20256805419922 = 1.8599590063095093 + 2.0 * 7.671304225921631
Epoch 50, val loss: 1.8604512214660645
Epoch 60, training loss: 16.347454071044922 = 1.8453055620193481 + 2.0 * 7.251073837280273
Epoch 60, val loss: 1.8465204238891602
Epoch 70, training loss: 15.98698616027832 = 1.8334622383117676 + 2.0 * 7.076761722564697
Epoch 70, val loss: 1.8344236612319946
Epoch 80, training loss: 15.750894546508789 = 1.8207976818084717 + 2.0 * 6.965048313140869
Epoch 80, val loss: 1.8223068714141846
Epoch 90, training loss: 15.479101181030273 = 1.8080617189407349 + 2.0 * 6.835519790649414
Epoch 90, val loss: 1.8108221292495728
Epoch 100, training loss: 15.28995418548584 = 1.796644926071167 + 2.0 * 6.746654510498047
Epoch 100, val loss: 1.8008345365524292
Epoch 110, training loss: 15.140220642089844 = 1.7861024141311646 + 2.0 * 6.677059173583984
Epoch 110, val loss: 1.7907979488372803
Epoch 120, training loss: 15.021029472351074 = 1.7746788263320923 + 2.0 * 6.623175144195557
Epoch 120, val loss: 1.7797236442565918
Epoch 130, training loss: 14.927939414978027 = 1.7624722719192505 + 2.0 * 6.582733631134033
Epoch 130, val loss: 1.7683765888214111
Epoch 140, training loss: 14.850297927856445 = 1.7495895624160767 + 2.0 * 6.55035400390625
Epoch 140, val loss: 1.7570135593414307
Epoch 150, training loss: 14.785947799682617 = 1.7360697984695435 + 2.0 * 6.524939060211182
Epoch 150, val loss: 1.745143175125122
Epoch 160, training loss: 14.723226547241211 = 1.7213232517242432 + 2.0 * 6.500951766967773
Epoch 160, val loss: 1.7324665784835815
Epoch 170, training loss: 14.66826343536377 = 1.70480477809906 + 2.0 * 6.481729507446289
Epoch 170, val loss: 1.7184396982192993
Epoch 180, training loss: 14.621964454650879 = 1.6859270334243774 + 2.0 * 6.468018531799316
Epoch 180, val loss: 1.7025034427642822
Epoch 190, training loss: 14.569580078125 = 1.6642825603485107 + 2.0 * 6.452648639678955
Epoch 190, val loss: 1.684301495552063
Epoch 200, training loss: 14.522187232971191 = 1.6394927501678467 + 2.0 * 6.441347122192383
Epoch 200, val loss: 1.663521647453308
Epoch 210, training loss: 14.473620414733887 = 1.611129879951477 + 2.0 * 6.43124532699585
Epoch 210, val loss: 1.6399221420288086
Epoch 220, training loss: 14.424186706542969 = 1.5790810585021973 + 2.0 * 6.422553062438965
Epoch 220, val loss: 1.6132677793502808
Epoch 230, training loss: 14.368626594543457 = 1.5433262586593628 + 2.0 * 6.412650108337402
Epoch 230, val loss: 1.5835955142974854
Epoch 240, training loss: 14.312653541564941 = 1.5035326480865479 + 2.0 * 6.404560565948486
Epoch 240, val loss: 1.5505812168121338
Epoch 250, training loss: 14.255098342895508 = 1.4594779014587402 + 2.0 * 6.397810459136963
Epoch 250, val loss: 1.5142995119094849
Epoch 260, training loss: 14.195111274719238 = 1.411836862564087 + 2.0 * 6.391637325286865
Epoch 260, val loss: 1.4751780033111572
Epoch 270, training loss: 14.136219024658203 = 1.3617750406265259 + 2.0 * 6.387221813201904
Epoch 270, val loss: 1.4343234300613403
Epoch 280, training loss: 14.071624755859375 = 1.310168743133545 + 2.0 * 6.380727767944336
Epoch 280, val loss: 1.3926022052764893
Epoch 290, training loss: 14.011879920959473 = 1.2581642866134644 + 2.0 * 6.376857757568359
Epoch 290, val loss: 1.3506841659545898
Epoch 300, training loss: 13.951107025146484 = 1.2070057392120361 + 2.0 * 6.372050762176514
Epoch 300, val loss: 1.3095393180847168
Epoch 310, training loss: 13.894730567932129 = 1.1575719118118286 + 2.0 * 6.368579387664795
Epoch 310, val loss: 1.2702884674072266
Epoch 320, training loss: 13.837777137756348 = 1.1106932163238525 + 2.0 * 6.363542079925537
Epoch 320, val loss: 1.2335182428359985
Epoch 330, training loss: 13.795167922973633 = 1.0662662982940674 + 2.0 * 6.364450931549072
Epoch 330, val loss: 1.199070930480957
Epoch 340, training loss: 13.737458229064941 = 1.0249640941619873 + 2.0 * 6.3562469482421875
Epoch 340, val loss: 1.167079210281372
Epoch 350, training loss: 13.691555976867676 = 0.9860493540763855 + 2.0 * 6.352753162384033
Epoch 350, val loss: 1.1376200914382935
Epoch 360, training loss: 13.648574829101562 = 0.9491280317306519 + 2.0 * 6.3497233390808105
Epoch 360, val loss: 1.1098963022232056
Epoch 370, training loss: 13.61290454864502 = 0.9139053225517273 + 2.0 * 6.349499702453613
Epoch 370, val loss: 1.0837318897247314
Epoch 380, training loss: 13.570571899414062 = 0.8805133700370789 + 2.0 * 6.345029354095459
Epoch 380, val loss: 1.059054970741272
Epoch 390, training loss: 13.529365539550781 = 0.8484310507774353 + 2.0 * 6.34046745300293
Epoch 390, val loss: 1.03561270236969
Epoch 400, training loss: 13.50070571899414 = 0.8173192143440247 + 2.0 * 6.34169340133667
Epoch 400, val loss: 1.013062596321106
Epoch 410, training loss: 13.4623384475708 = 0.7869618535041809 + 2.0 * 6.337688446044922
Epoch 410, val loss: 0.9915447235107422
Epoch 420, training loss: 13.42345905303955 = 0.7575557231903076 + 2.0 * 6.332951545715332
Epoch 420, val loss: 0.9707954525947571
Epoch 430, training loss: 13.394256591796875 = 0.7287865281105042 + 2.0 * 6.332735061645508
Epoch 430, val loss: 0.9508503079414368
Epoch 440, training loss: 13.360840797424316 = 0.7007103562355042 + 2.0 * 6.3300652503967285
Epoch 440, val loss: 0.9316190481185913
Epoch 450, training loss: 13.32760238647461 = 0.6732858419418335 + 2.0 * 6.327158451080322
Epoch 450, val loss: 0.9133120775222778
Epoch 460, training loss: 13.293795585632324 = 0.6464588046073914 + 2.0 * 6.323668479919434
Epoch 460, val loss: 0.8958117961883545
Epoch 470, training loss: 13.280231475830078 = 0.6200953125953674 + 2.0 * 6.330068111419678
Epoch 470, val loss: 0.8792124390602112
Epoch 480, training loss: 13.234128952026367 = 0.5946900248527527 + 2.0 * 6.319719314575195
Epoch 480, val loss: 0.8634816408157349
Epoch 490, training loss: 13.205093383789062 = 0.5697999596595764 + 2.0 * 6.317646503448486
Epoch 490, val loss: 0.848586916923523
Epoch 500, training loss: 13.179553985595703 = 0.5453453660011292 + 2.0 * 6.317104339599609
Epoch 500, val loss: 0.8344457149505615
Epoch 510, training loss: 13.150289535522461 = 0.5214724540710449 + 2.0 * 6.314408779144287
Epoch 510, val loss: 0.8211296200752258
Epoch 520, training loss: 13.121472358703613 = 0.4980708062648773 + 2.0 * 6.311700820922852
Epoch 520, val loss: 0.8085098266601562
Epoch 530, training loss: 13.094541549682617 = 0.4750567674636841 + 2.0 * 6.309742450714111
Epoch 530, val loss: 0.7964539527893066
Epoch 540, training loss: 13.06830883026123 = 0.4522383511066437 + 2.0 * 6.308035373687744
Epoch 540, val loss: 0.7850183844566345
Epoch 550, training loss: 13.062906265258789 = 0.42976143956184387 + 2.0 * 6.316572189331055
Epoch 550, val loss: 0.7742088437080383
Epoch 560, training loss: 13.01911735534668 = 0.4082004129886627 + 2.0 * 6.3054585456848145
Epoch 560, val loss: 0.7640633583068848
Epoch 570, training loss: 12.995409965515137 = 0.38712048530578613 + 2.0 * 6.304144859313965
Epoch 570, val loss: 0.7545814514160156
Epoch 580, training loss: 12.969979286193848 = 0.36649906635284424 + 2.0 * 6.3017401695251465
Epoch 580, val loss: 0.7457470893859863
Epoch 590, training loss: 12.958149909973145 = 0.3464899957180023 + 2.0 * 6.305830001831055
Epoch 590, val loss: 0.7374961972236633
Epoch 600, training loss: 12.932171821594238 = 0.3271592855453491 + 2.0 * 6.302506446838379
Epoch 600, val loss: 0.730011522769928
Epoch 610, training loss: 12.904590606689453 = 0.3088644742965698 + 2.0 * 6.297863006591797
Epoch 610, val loss: 0.7231444716453552
Epoch 620, training loss: 12.883594512939453 = 0.2913042902946472 + 2.0 * 6.296144962310791
Epoch 620, val loss: 0.7170419692993164
Epoch 630, training loss: 12.864473342895508 = 0.27452322840690613 + 2.0 * 6.294975280761719
Epoch 630, val loss: 0.7116528153419495
Epoch 640, training loss: 12.87372875213623 = 0.25856536626815796 + 2.0 * 6.307581901550293
Epoch 640, val loss: 0.7070059776306152
Epoch 650, training loss: 12.834961891174316 = 0.24367597699165344 + 2.0 * 6.295642852783203
Epoch 650, val loss: 0.7030709981918335
Epoch 660, training loss: 12.816178321838379 = 0.22973889112472534 + 2.0 * 6.293219566345215
Epoch 660, val loss: 0.6998789310455322
Epoch 670, training loss: 12.798346519470215 = 0.2166101038455963 + 2.0 * 6.290868282318115
Epoch 670, val loss: 0.6974846720695496
Epoch 680, training loss: 12.783199310302734 = 0.20426549017429352 + 2.0 * 6.289466857910156
Epoch 680, val loss: 0.6957589983940125
Epoch 690, training loss: 12.77440071105957 = 0.19267311692237854 + 2.0 * 6.290863990783691
Epoch 690, val loss: 0.6946633458137512
Epoch 700, training loss: 12.761858940124512 = 0.18188445270061493 + 2.0 * 6.289987087249756
Epoch 700, val loss: 0.6940844655036926
Epoch 710, training loss: 12.74686336517334 = 0.17188331484794617 + 2.0 * 6.287489891052246
Epoch 710, val loss: 0.6941403746604919
Epoch 720, training loss: 12.732926368713379 = 0.16252823173999786 + 2.0 * 6.285199165344238
Epoch 720, val loss: 0.6947796940803528
Epoch 730, training loss: 12.723946571350098 = 0.15380604565143585 + 2.0 * 6.285070419311523
Epoch 730, val loss: 0.6958772540092468
Epoch 740, training loss: 12.715930938720703 = 0.14566545188426971 + 2.0 * 6.285132884979248
Epoch 740, val loss: 0.6973703503608704
Epoch 750, training loss: 12.7075777053833 = 0.13811199367046356 + 2.0 * 6.284732818603516
Epoch 750, val loss: 0.6991974115371704
Epoch 760, training loss: 12.701220512390137 = 0.13107708096504211 + 2.0 * 6.285071849822998
Epoch 760, val loss: 0.70143723487854
Epoch 770, training loss: 12.68731689453125 = 0.12451125681400299 + 2.0 * 6.281402587890625
Epoch 770, val loss: 0.7039880752563477
Epoch 780, training loss: 12.677844047546387 = 0.11838249117136002 + 2.0 * 6.279730796813965
Epoch 780, val loss: 0.7068573832511902
Epoch 790, training loss: 12.676322937011719 = 0.11262612044811249 + 2.0 * 6.281848430633545
Epoch 790, val loss: 0.7100282907485962
Epoch 800, training loss: 12.664332389831543 = 0.1072378158569336 + 2.0 * 6.278547286987305
Epoch 800, val loss: 0.7133215665817261
Epoch 810, training loss: 12.662781715393066 = 0.10218872129917145 + 2.0 * 6.280296325683594
Epoch 810, val loss: 0.7168213129043579
Epoch 820, training loss: 12.650702476501465 = 0.09747578203678131 + 2.0 * 6.276613235473633
Epoch 820, val loss: 0.7205343246459961
Epoch 830, training loss: 12.64786148071289 = 0.09303747117519379 + 2.0 * 6.277411937713623
Epoch 830, val loss: 0.7244412302970886
Epoch 840, training loss: 12.639459609985352 = 0.08885221183300018 + 2.0 * 6.275303840637207
Epoch 840, val loss: 0.7283713817596436
Epoch 850, training loss: 12.643327713012695 = 0.08493024110794067 + 2.0 * 6.27919864654541
Epoch 850, val loss: 0.7324441075325012
Epoch 860, training loss: 12.626808166503906 = 0.08127468079328537 + 2.0 * 6.272766590118408
Epoch 860, val loss: 0.7365939617156982
Epoch 870, training loss: 12.622182846069336 = 0.0778118371963501 + 2.0 * 6.272185325622559
Epoch 870, val loss: 0.740849494934082
Epoch 880, training loss: 12.622636795043945 = 0.07453097403049469 + 2.0 * 6.27405309677124
Epoch 880, val loss: 0.7452347278594971
Epoch 890, training loss: 12.61797046661377 = 0.07144387066364288 + 2.0 * 6.273263454437256
Epoch 890, val loss: 0.7495566010475159
Epoch 900, training loss: 12.61301326751709 = 0.06853573769330978 + 2.0 * 6.272238731384277
Epoch 900, val loss: 0.7538898587226868
Epoch 910, training loss: 12.605110168457031 = 0.0658135935664177 + 2.0 * 6.26964807510376
Epoch 910, val loss: 0.7583657503128052
Epoch 920, training loss: 12.599760055541992 = 0.06322763860225677 + 2.0 * 6.268266201019287
Epoch 920, val loss: 0.7628920674324036
Epoch 930, training loss: 12.606025695800781 = 0.06078542396426201 + 2.0 * 6.27262020111084
Epoch 930, val loss: 0.7674403190612793
Epoch 940, training loss: 12.603520393371582 = 0.05844908207654953 + 2.0 * 6.272535800933838
Epoch 940, val loss: 0.7718360424041748
Epoch 950, training loss: 12.590648651123047 = 0.056263018399477005 + 2.0 * 6.267192840576172
Epoch 950, val loss: 0.7762957811355591
Epoch 960, training loss: 12.5835542678833 = 0.05418290197849274 + 2.0 * 6.26468563079834
Epoch 960, val loss: 0.7808844447135925
Epoch 970, training loss: 12.581022262573242 = 0.052203722298145294 + 2.0 * 6.264409065246582
Epoch 970, val loss: 0.7854693531990051
Epoch 980, training loss: 12.592999458312988 = 0.050316665321588516 + 2.0 * 6.271341323852539
Epoch 980, val loss: 0.7899558544158936
Epoch 990, training loss: 12.573419570922852 = 0.04854254797101021 + 2.0 * 6.2624382972717285
Epoch 990, val loss: 0.7943944931030273
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8111111111111111
0.8350026357406432
=== training gcn model ===
Epoch 0, training loss: 19.13434600830078 = 1.9407099485397339 + 2.0 * 8.596817970275879
Epoch 0, val loss: 1.9397153854370117
Epoch 10, training loss: 19.123825073242188 = 1.9309443235397339 + 2.0 * 8.596440315246582
Epoch 10, val loss: 1.9298940896987915
Epoch 20, training loss: 19.105724334716797 = 1.918892741203308 + 2.0 * 8.593416213989258
Epoch 20, val loss: 1.9179316759109497
Epoch 30, training loss: 19.044414520263672 = 1.9023300409317017 + 2.0 * 8.57104206085205
Epoch 30, val loss: 1.901764988899231
Epoch 40, training loss: 18.76655387878418 = 1.881259799003601 + 2.0 * 8.442646980285645
Epoch 40, val loss: 1.8816616535186768
Epoch 50, training loss: 17.75098991394043 = 1.8572019338607788 + 2.0 * 7.94689416885376
Epoch 50, val loss: 1.859011173248291
Epoch 60, training loss: 16.876178741455078 = 1.8350751399993896 + 2.0 * 7.520551681518555
Epoch 60, val loss: 1.8386813402175903
Epoch 70, training loss: 16.230783462524414 = 1.8201202154159546 + 2.0 * 7.205331802368164
Epoch 70, val loss: 1.823318600654602
Epoch 80, training loss: 15.860611915588379 = 1.8052902221679688 + 2.0 * 7.027660846710205
Epoch 80, val loss: 1.8084527254104614
Epoch 90, training loss: 15.5828857421875 = 1.7910921573638916 + 2.0 * 6.895896911621094
Epoch 90, val loss: 1.7950440645217896
Epoch 100, training loss: 15.379605293273926 = 1.777295708656311 + 2.0 * 6.801154613494873
Epoch 100, val loss: 1.7826908826828003
Epoch 110, training loss: 15.202325820922852 = 1.7638283967971802 + 2.0 * 6.7192487716674805
Epoch 110, val loss: 1.770753026008606
Epoch 120, training loss: 15.084197998046875 = 1.7504383325576782 + 2.0 * 6.666879653930664
Epoch 120, val loss: 1.7586166858673096
Epoch 130, training loss: 14.990815162658691 = 1.7351865768432617 + 2.0 * 6.627814292907715
Epoch 130, val loss: 1.7448259592056274
Epoch 140, training loss: 14.909469604492188 = 1.7178266048431396 + 2.0 * 6.595821380615234
Epoch 140, val loss: 1.7296417951583862
Epoch 150, training loss: 14.829835891723633 = 1.698630452156067 + 2.0 * 6.565602779388428
Epoch 150, val loss: 1.7131143808364868
Epoch 160, training loss: 14.756739616394043 = 1.677268624305725 + 2.0 * 6.539735317230225
Epoch 160, val loss: 1.6949788331985474
Epoch 170, training loss: 14.677739143371582 = 1.6531133651733398 + 2.0 * 6.512312889099121
Epoch 170, val loss: 1.6745034456253052
Epoch 180, training loss: 14.607421875 = 1.6255123615264893 + 2.0 * 6.490954875946045
Epoch 180, val loss: 1.6511662006378174
Epoch 190, training loss: 14.545074462890625 = 1.5939760208129883 + 2.0 * 6.475549221038818
Epoch 190, val loss: 1.6247800588607788
Epoch 200, training loss: 14.4803466796875 = 1.5586427450180054 + 2.0 * 6.460852146148682
Epoch 200, val loss: 1.5950552225112915
Epoch 210, training loss: 14.417901039123535 = 1.5194299221038818 + 2.0 * 6.449235439300537
Epoch 210, val loss: 1.5619868040084839
Epoch 220, training loss: 14.355269432067871 = 1.476505994796753 + 2.0 * 6.4393815994262695
Epoch 220, val loss: 1.5258883237838745
Epoch 230, training loss: 14.297805786132812 = 1.430993914604187 + 2.0 * 6.433405876159668
Epoch 230, val loss: 1.4878953695297241
Epoch 240, training loss: 14.231566429138184 = 1.3844550848007202 + 2.0 * 6.423555850982666
Epoch 240, val loss: 1.4490967988967896
Epoch 250, training loss: 14.168172836303711 = 1.3372329473495483 + 2.0 * 6.415470123291016
Epoch 250, val loss: 1.4100642204284668
Epoch 260, training loss: 14.107980728149414 = 1.2897638082504272 + 2.0 * 6.409108638763428
Epoch 260, val loss: 1.3712133169174194
Epoch 270, training loss: 14.051097869873047 = 1.2430894374847412 + 2.0 * 6.404004096984863
Epoch 270, val loss: 1.3338032960891724
Epoch 280, training loss: 13.990483283996582 = 1.1979386806488037 + 2.0 * 6.3962721824646
Epoch 280, val loss: 1.297743320465088
Epoch 290, training loss: 13.934179306030273 = 1.1536628007888794 + 2.0 * 6.390258312225342
Epoch 290, val loss: 1.2629038095474243
Epoch 300, training loss: 13.878998756408691 = 1.1100913286209106 + 2.0 * 6.384453773498535
Epoch 300, val loss: 1.2289870977401733
Epoch 310, training loss: 13.833507537841797 = 1.0671027898788452 + 2.0 * 6.38320255279541
Epoch 310, val loss: 1.1959569454193115
Epoch 320, training loss: 13.775650978088379 = 1.025297999382019 + 2.0 * 6.375176429748535
Epoch 320, val loss: 1.1639072895050049
Epoch 330, training loss: 13.730599403381348 = 0.9844817519187927 + 2.0 * 6.373058795928955
Epoch 330, val loss: 1.1328771114349365
Epoch 340, training loss: 13.677868843078613 = 0.9444839358329773 + 2.0 * 6.366692543029785
Epoch 340, val loss: 1.1028355360031128
Epoch 350, training loss: 13.635892868041992 = 0.9054827690124512 + 2.0 * 6.36520528793335
Epoch 350, val loss: 1.0735433101654053
Epoch 360, training loss: 13.583696365356445 = 0.8678174018859863 + 2.0 * 6.357939720153809
Epoch 360, val loss: 1.0453059673309326
Epoch 370, training loss: 13.544086456298828 = 0.8315025568008423 + 2.0 * 6.356291770935059
Epoch 370, val loss: 1.0183742046356201
Epoch 380, training loss: 13.503467559814453 = 0.7969841957092285 + 2.0 * 6.353241443634033
Epoch 380, val loss: 0.9928606748580933
Epoch 390, training loss: 13.459606170654297 = 0.7640624046325684 + 2.0 * 6.347771644592285
Epoch 390, val loss: 0.9690005779266357
Epoch 400, training loss: 13.422628402709961 = 0.7327558398246765 + 2.0 * 6.344936370849609
Epoch 400, val loss: 0.9466607570648193
Epoch 410, training loss: 13.402326583862305 = 0.7028905749320984 + 2.0 * 6.34971809387207
Epoch 410, val loss: 0.9257606267929077
Epoch 420, training loss: 13.357234001159668 = 0.6746214628219604 + 2.0 * 6.341306209564209
Epoch 420, val loss: 0.906516432762146
Epoch 430, training loss: 13.32065200805664 = 0.6477344036102295 + 2.0 * 6.336458683013916
Epoch 430, val loss: 0.8888550400733948
Epoch 440, training loss: 13.312137603759766 = 0.6219121813774109 + 2.0 * 6.3451128005981445
Epoch 440, val loss: 0.8725658655166626
Epoch 450, training loss: 13.266786575317383 = 0.5975648760795593 + 2.0 * 6.334610939025879
Epoch 450, val loss: 0.8576070070266724
Epoch 460, training loss: 13.232298851013184 = 0.5742574334144592 + 2.0 * 6.3290205001831055
Epoch 460, val loss: 0.8440579175949097
Epoch 470, training loss: 13.204610824584961 = 0.5518127679824829 + 2.0 * 6.326398849487305
Epoch 470, val loss: 0.8315432667732239
Epoch 480, training loss: 13.181448936462402 = 0.5303182005882263 + 2.0 * 6.325565338134766
Epoch 480, val loss: 0.8200379014015198
Epoch 490, training loss: 13.155376434326172 = 0.5098678469657898 + 2.0 * 6.322754383087158
Epoch 490, val loss: 0.809766948223114
Epoch 500, training loss: 13.13000202178955 = 0.49025988578796387 + 2.0 * 6.319870948791504
Epoch 500, val loss: 0.800486147403717
Epoch 510, training loss: 13.10757827758789 = 0.47134116291999817 + 2.0 * 6.318118572235107
Epoch 510, val loss: 0.7919915914535522
Epoch 520, training loss: 13.087042808532715 = 0.45314136147499084 + 2.0 * 6.316950798034668
Epoch 520, val loss: 0.7841880917549133
Epoch 530, training loss: 13.06501579284668 = 0.43565940856933594 + 2.0 * 6.314678192138672
Epoch 530, val loss: 0.7772051692008972
Epoch 540, training loss: 13.04288101196289 = 0.4187445640563965 + 2.0 * 6.312067985534668
Epoch 540, val loss: 0.7709383964538574
Epoch 550, training loss: 13.030219078063965 = 0.402336448431015 + 2.0 * 6.313941478729248
Epoch 550, val loss: 0.7651455998420715
Epoch 560, training loss: 13.013032913208008 = 0.3863973617553711 + 2.0 * 6.313317775726318
Epoch 560, val loss: 0.7599446773529053
Epoch 570, training loss: 12.987268447875977 = 0.3708913326263428 + 2.0 * 6.308188438415527
Epoch 570, val loss: 0.7552667260169983
Epoch 580, training loss: 12.967498779296875 = 0.355843186378479 + 2.0 * 6.305827617645264
Epoch 580, val loss: 0.7510350942611694
Epoch 590, training loss: 12.955461502075195 = 0.34109580516815186 + 2.0 * 6.307182788848877
Epoch 590, val loss: 0.7471547722816467
Epoch 600, training loss: 12.936365127563477 = 0.3268262445926666 + 2.0 * 6.304769515991211
Epoch 600, val loss: 0.7436480522155762
Epoch 610, training loss: 12.915643692016602 = 0.31294485926628113 + 2.0 * 6.301349639892578
Epoch 610, val loss: 0.7406381964683533
Epoch 620, training loss: 12.898271560668945 = 0.29944702982902527 + 2.0 * 6.299412250518799
Epoch 620, val loss: 0.7380061149597168
Epoch 630, training loss: 12.900901794433594 = 0.286374568939209 + 2.0 * 6.3072638511657715
Epoch 630, val loss: 0.7357684373855591
Epoch 640, training loss: 12.870396614074707 = 0.2738429307937622 + 2.0 * 6.298276901245117
Epoch 640, val loss: 0.7339584231376648
Epoch 650, training loss: 12.854146957397461 = 0.2618224024772644 + 2.0 * 6.296162128448486
Epoch 650, val loss: 0.7326415777206421
Epoch 660, training loss: 12.83899974822998 = 0.2502455413341522 + 2.0 * 6.294377326965332
Epoch 660, val loss: 0.7316286563873291
Epoch 670, training loss: 12.83089828491211 = 0.23910167813301086 + 2.0 * 6.2958984375
Epoch 670, val loss: 0.7309861779212952
Epoch 680, training loss: 12.816462516784668 = 0.22842776775360107 + 2.0 * 6.294017314910889
Epoch 680, val loss: 0.7306472659111023
Epoch 690, training loss: 12.802390098571777 = 0.21829009056091309 + 2.0 * 6.292049884796143
Epoch 690, val loss: 0.7308977246284485
Epoch 700, training loss: 12.788398742675781 = 0.2085743397474289 + 2.0 * 6.289912223815918
Epoch 700, val loss: 0.7314059138298035
Epoch 710, training loss: 12.78514289855957 = 0.1992892026901245 + 2.0 * 6.292926788330078
Epoch 710, val loss: 0.7321918606758118
Epoch 720, training loss: 12.775938034057617 = 0.19048833847045898 + 2.0 * 6.292724609375
Epoch 720, val loss: 0.7333971858024597
Epoch 730, training loss: 12.75687313079834 = 0.18213598430156708 + 2.0 * 6.2873687744140625
Epoch 730, val loss: 0.7350204586982727
Epoch 740, training loss: 12.754451751708984 = 0.1741924285888672 + 2.0 * 6.290129661560059
Epoch 740, val loss: 0.7368866801261902
Epoch 750, training loss: 12.744377136230469 = 0.16667892038822174 + 2.0 * 6.288848876953125
Epoch 750, val loss: 0.7390826344490051
Epoch 760, training loss: 12.728962898254395 = 0.15954814851284027 + 2.0 * 6.284707546234131
Epoch 760, val loss: 0.7416041493415833
Epoch 770, training loss: 12.724003791809082 = 0.15277934074401855 + 2.0 * 6.285612106323242
Epoch 770, val loss: 0.744405210018158
Epoch 780, training loss: 12.713262557983398 = 0.1463487148284912 + 2.0 * 6.283456802368164
Epoch 780, val loss: 0.7473539113998413
Epoch 790, training loss: 12.703694343566895 = 0.14024990797042847 + 2.0 * 6.281722068786621
Epoch 790, val loss: 0.7506904006004333
Epoch 800, training loss: 12.696000099182129 = 0.13443723320960999 + 2.0 * 6.280781269073486
Epoch 800, val loss: 0.7541789412498474
Epoch 810, training loss: 12.689425468444824 = 0.12888690829277039 + 2.0 * 6.280269145965576
Epoch 810, val loss: 0.7578578591346741
Epoch 820, training loss: 12.690826416015625 = 0.12361235171556473 + 2.0 * 6.283607006072998
Epoch 820, val loss: 0.7616168260574341
Epoch 830, training loss: 12.68114948272705 = 0.11862113326787949 + 2.0 * 6.281264305114746
Epoch 830, val loss: 0.7657361626625061
Epoch 840, training loss: 12.668794631958008 = 0.11387377232313156 + 2.0 * 6.27746057510376
Epoch 840, val loss: 0.7699435353279114
Epoch 850, training loss: 12.662834167480469 = 0.10933570563793182 + 2.0 * 6.276749134063721
Epoch 850, val loss: 0.7742385268211365
Epoch 860, training loss: 12.67480754852295 = 0.10499724000692368 + 2.0 * 6.284904956817627
Epoch 860, val loss: 0.778659999370575
Epoch 870, training loss: 12.655016899108887 = 0.10084033757448196 + 2.0 * 6.277088165283203
Epoch 870, val loss: 0.7831509113311768
Epoch 880, training loss: 12.6449556350708 = 0.09688417613506317 + 2.0 * 6.274035930633545
Epoch 880, val loss: 0.7878421545028687
Epoch 890, training loss: 12.640039443969727 = 0.0930914357304573 + 2.0 * 6.273474216461182
Epoch 890, val loss: 0.7925247550010681
Epoch 900, training loss: 12.64696216583252 = 0.08945945650339127 + 2.0 * 6.278751373291016
Epoch 900, val loss: 0.7972511649131775
Epoch 910, training loss: 12.644540786743164 = 0.08599410951137543 + 2.0 * 6.279273509979248
Epoch 910, val loss: 0.8019507527351379
Epoch 920, training loss: 12.627986907958984 = 0.082712821662426 + 2.0 * 6.272636890411377
Epoch 920, val loss: 0.8068994879722595
Epoch 930, training loss: 12.62083625793457 = 0.0795716941356659 + 2.0 * 6.270632266998291
Epoch 930, val loss: 0.8118056654930115
Epoch 940, training loss: 12.61892318725586 = 0.07655124366283417 + 2.0 * 6.271185874938965
Epoch 940, val loss: 0.8167496919631958
Epoch 950, training loss: 12.614616394042969 = 0.07367077469825745 + 2.0 * 6.270473003387451
Epoch 950, val loss: 0.8216331601142883
Epoch 960, training loss: 12.611594200134277 = 0.07091347128152847 + 2.0 * 6.270340442657471
Epoch 960, val loss: 0.826797366142273
Epoch 970, training loss: 12.611827850341797 = 0.06829008460044861 + 2.0 * 6.271769046783447
Epoch 970, val loss: 0.8318172693252563
Epoch 980, training loss: 12.600288391113281 = 0.06578195840120316 + 2.0 * 6.267253398895264
Epoch 980, val loss: 0.8367677927017212
Epoch 990, training loss: 12.598634719848633 = 0.06338528543710709 + 2.0 * 6.267624855041504
Epoch 990, val loss: 0.8419158458709717
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8037037037037037
0.8360569319978914
=== training gcn model ===
Epoch 0, training loss: 19.15303611755371 = 1.9594342708587646 + 2.0 * 8.596800804138184
Epoch 0, val loss: 1.957014560699463
Epoch 10, training loss: 19.141496658325195 = 1.9488338232040405 + 2.0 * 8.596331596374512
Epoch 10, val loss: 1.9466618299484253
Epoch 20, training loss: 19.120508193969727 = 1.9358681440353394 + 2.0 * 8.592320442199707
Epoch 20, val loss: 1.933572769165039
Epoch 30, training loss: 19.045997619628906 = 1.9185841083526611 + 2.0 * 8.563706398010254
Epoch 30, val loss: 1.9157990217208862
Epoch 40, training loss: 18.693700790405273 = 1.8973568677902222 + 2.0 * 8.398172378540039
Epoch 40, val loss: 1.8948248624801636
Epoch 50, training loss: 17.752517700195312 = 1.8735827207565308 + 2.0 * 7.939467430114746
Epoch 50, val loss: 1.871647596359253
Epoch 60, training loss: 16.897254943847656 = 1.8524727821350098 + 2.0 * 7.522391319274902
Epoch 60, val loss: 1.8519221544265747
Epoch 70, training loss: 16.061552047729492 = 1.8363734483718872 + 2.0 * 7.112588882446289
Epoch 70, val loss: 1.8368414640426636
Epoch 80, training loss: 15.729912757873535 = 1.8213289976119995 + 2.0 * 6.954291820526123
Epoch 80, val loss: 1.8225983381271362
Epoch 90, training loss: 15.502973556518555 = 1.8055272102355957 + 2.0 * 6.848723411560059
Epoch 90, val loss: 1.8070954084396362
Epoch 100, training loss: 15.307706832885742 = 1.791774034500122 + 2.0 * 6.7579665184021
Epoch 100, val loss: 1.7937618494033813
Epoch 110, training loss: 15.165743827819824 = 1.7802155017852783 + 2.0 * 6.6927642822265625
Epoch 110, val loss: 1.7823681831359863
Epoch 120, training loss: 15.048381805419922 = 1.7687042951583862 + 2.0 * 6.639838695526123
Epoch 120, val loss: 1.771324872970581
Epoch 130, training loss: 14.962347030639648 = 1.7569726705551147 + 2.0 * 6.602687358856201
Epoch 130, val loss: 1.760354995727539
Epoch 140, training loss: 14.889608383178711 = 1.7443252801895142 + 2.0 * 6.572641372680664
Epoch 140, val loss: 1.7486038208007812
Epoch 150, training loss: 14.827278137207031 = 1.730136513710022 + 2.0 * 6.54857063293457
Epoch 150, val loss: 1.7354774475097656
Epoch 160, training loss: 14.767151832580566 = 1.714120626449585 + 2.0 * 6.526515483856201
Epoch 160, val loss: 1.720741868019104
Epoch 170, training loss: 14.71440601348877 = 1.6961935758590698 + 2.0 * 6.509106159210205
Epoch 170, val loss: 1.7043673992156982
Epoch 180, training loss: 14.662019729614258 = 1.675862193107605 + 2.0 * 6.493078708648682
Epoch 180, val loss: 1.6859326362609863
Epoch 190, training loss: 14.606161117553711 = 1.652937889099121 + 2.0 * 6.476611614227295
Epoch 190, val loss: 1.665417194366455
Epoch 200, training loss: 14.561513900756836 = 1.6270700693130493 + 2.0 * 6.467221736907959
Epoch 200, val loss: 1.642595648765564
Epoch 210, training loss: 14.499963760375977 = 1.598435640335083 + 2.0 * 6.450764179229736
Epoch 210, val loss: 1.617356300354004
Epoch 220, training loss: 14.446186065673828 = 1.5666829347610474 + 2.0 * 6.439751625061035
Epoch 220, val loss: 1.5896893739700317
Epoch 230, training loss: 14.391983032226562 = 1.531404972076416 + 2.0 * 6.430288791656494
Epoch 230, val loss: 1.5592795610427856
Epoch 240, training loss: 14.336564064025879 = 1.4924957752227783 + 2.0 * 6.42203426361084
Epoch 240, val loss: 1.526131510734558
Epoch 250, training loss: 14.282476425170898 = 1.4504363536834717 + 2.0 * 6.416019916534424
Epoch 250, val loss: 1.490558385848999
Epoch 260, training loss: 14.221999168395996 = 1.4055534601211548 + 2.0 * 6.408222675323486
Epoch 260, val loss: 1.4533029794692993
Epoch 270, training loss: 14.160696983337402 = 1.3581132888793945 + 2.0 * 6.401291847229004
Epoch 270, val loss: 1.414386510848999
Epoch 280, training loss: 14.107673645019531 = 1.308803677558899 + 2.0 * 6.399435043334961
Epoch 280, val loss: 1.3744438886642456
Epoch 290, training loss: 14.041528701782227 = 1.259058952331543 + 2.0 * 6.391234874725342
Epoch 290, val loss: 1.334707498550415
Epoch 300, training loss: 13.981060028076172 = 1.2094289064407349 + 2.0 * 6.385815620422363
Epoch 300, val loss: 1.2954305410385132
Epoch 310, training loss: 13.920428276062012 = 1.1599791049957275 + 2.0 * 6.380224704742432
Epoch 310, val loss: 1.2566910982131958
Epoch 320, training loss: 13.883505821228027 = 1.1111537218093872 + 2.0 * 6.386176109313965
Epoch 320, val loss: 1.218861699104309
Epoch 330, training loss: 13.81025505065918 = 1.0645298957824707 + 2.0 * 6.372862339019775
Epoch 330, val loss: 1.1825751066207886
Epoch 340, training loss: 13.757610321044922 = 1.0198185443878174 + 2.0 * 6.368896007537842
Epoch 340, val loss: 1.1481356620788574
Epoch 350, training loss: 13.705663681030273 = 0.976764440536499 + 2.0 * 6.364449501037598
Epoch 350, val loss: 1.1153151988983154
Epoch 360, training loss: 13.657251358032227 = 0.9354296326637268 + 2.0 * 6.360910892486572
Epoch 360, val loss: 1.0840259790420532
Epoch 370, training loss: 13.610875129699707 = 0.8956906795501709 + 2.0 * 6.3575921058654785
Epoch 370, val loss: 1.0543015003204346
Epoch 380, training loss: 13.566740989685059 = 0.8577927947044373 + 2.0 * 6.354474067687988
Epoch 380, val loss: 1.0263456106185913
Epoch 390, training loss: 13.527027130126953 = 0.822087287902832 + 2.0 * 6.3524699211120605
Epoch 390, val loss: 1.0003306865692139
Epoch 400, training loss: 13.487224578857422 = 0.7881476879119873 + 2.0 * 6.349538326263428
Epoch 400, val loss: 0.9761158227920532
Epoch 410, training loss: 13.46469783782959 = 0.7557758092880249 + 2.0 * 6.354461193084717
Epoch 410, val loss: 0.953567624092102
Epoch 420, training loss: 13.414002418518066 = 0.725024402141571 + 2.0 * 6.344489097595215
Epoch 420, val loss: 0.9326848387718201
Epoch 430, training loss: 13.379237174987793 = 0.6955466866493225 + 2.0 * 6.3418450355529785
Epoch 430, val loss: 0.9132126569747925
Epoch 440, training loss: 13.34506607055664 = 0.6669629812240601 + 2.0 * 6.339051723480225
Epoch 440, val loss: 0.8949195146560669
Epoch 450, training loss: 13.312071800231934 = 0.638988733291626 + 2.0 * 6.336541652679443
Epoch 450, val loss: 0.8774940967559814
Epoch 460, training loss: 13.296525001525879 = 0.6115076541900635 + 2.0 * 6.342508792877197
Epoch 460, val loss: 0.8608033657073975
Epoch 470, training loss: 13.253998756408691 = 0.5845935344696045 + 2.0 * 6.334702491760254
Epoch 470, val loss: 0.8450556397438049
Epoch 480, training loss: 13.220619201660156 = 0.5581986308097839 + 2.0 * 6.331210136413574
Epoch 480, val loss: 0.8299634456634521
Epoch 490, training loss: 13.19286823272705 = 0.5320580005645752 + 2.0 * 6.330405235290527
Epoch 490, val loss: 0.8154752254486084
Epoch 500, training loss: 13.160741806030273 = 0.5061740279197693 + 2.0 * 6.32728385925293
Epoch 500, val loss: 0.801726222038269
Epoch 510, training loss: 13.132749557495117 = 0.48072072863578796 + 2.0 * 6.326014518737793
Epoch 510, val loss: 0.7883810997009277
Epoch 520, training loss: 13.102849006652832 = 0.45552515983581543 + 2.0 * 6.323661804199219
Epoch 520, val loss: 0.7760047912597656
Epoch 530, training loss: 13.075753211975098 = 0.43082547187805176 + 2.0 * 6.3224639892578125
Epoch 530, val loss: 0.7643245458602905
Epoch 540, training loss: 13.046109199523926 = 0.4067888855934143 + 2.0 * 6.319660186767578
Epoch 540, val loss: 0.7534708976745605
Epoch 550, training loss: 13.020325660705566 = 0.3834484815597534 + 2.0 * 6.318438529968262
Epoch 550, val loss: 0.7435497045516968
Epoch 560, training loss: 13.00024127960205 = 0.36085426807403564 + 2.0 * 6.319693565368652
Epoch 560, val loss: 0.7346361875534058
Epoch 570, training loss: 12.979238510131836 = 0.3392740488052368 + 2.0 * 6.319982051849365
Epoch 570, val loss: 0.7264595627784729
Epoch 580, training loss: 12.948795318603516 = 0.3187015652656555 + 2.0 * 6.315046787261963
Epoch 580, val loss: 0.7195149064064026
Epoch 590, training loss: 12.924178123474121 = 0.29921436309814453 + 2.0 * 6.312481880187988
Epoch 590, val loss: 0.7134867906570435
Epoch 600, training loss: 12.901915550231934 = 0.2807091474533081 + 2.0 * 6.310603141784668
Epoch 600, val loss: 0.7085045576095581
Epoch 610, training loss: 12.898028373718262 = 0.2632538378238678 + 2.0 * 6.317387104034424
Epoch 610, val loss: 0.7044200897216797
Epoch 620, training loss: 12.863716125488281 = 0.24693310260772705 + 2.0 * 6.308391571044922
Epoch 620, val loss: 0.7012478709220886
Epoch 630, training loss: 12.843171119689941 = 0.2317131757736206 + 2.0 * 6.305728912353516
Epoch 630, val loss: 0.6989318132400513
Epoch 640, training loss: 12.828923225402832 = 0.21747411787509918 + 2.0 * 6.305724620819092
Epoch 640, val loss: 0.697523295879364
Epoch 650, training loss: 12.818195343017578 = 0.2041686773300171 + 2.0 * 6.307013511657715
Epoch 650, val loss: 0.6969773173332214
Epoch 660, training loss: 12.796496391296387 = 0.19189105927944183 + 2.0 * 6.302302837371826
Epoch 660, val loss: 0.6970426440238953
Epoch 670, training loss: 12.782862663269043 = 0.18051816523075104 + 2.0 * 6.301172256469727
Epoch 670, val loss: 0.6978084444999695
Epoch 680, training loss: 12.767997741699219 = 0.16993501782417297 + 2.0 * 6.2990312576293945
Epoch 680, val loss: 0.6992145776748657
Epoch 690, training loss: 12.75566291809082 = 0.16008955240249634 + 2.0 * 6.297786712646484
Epoch 690, val loss: 0.7011550068855286
Epoch 700, training loss: 12.75752067565918 = 0.15093399584293365 + 2.0 * 6.303293228149414
Epoch 700, val loss: 0.7036125063896179
Epoch 710, training loss: 12.748984336853027 = 0.14245390892028809 + 2.0 * 6.30326509475708
Epoch 710, val loss: 0.7064076662063599
Epoch 720, training loss: 12.72457218170166 = 0.1346202790737152 + 2.0 * 6.294975757598877
Epoch 720, val loss: 0.7094728350639343
Epoch 730, training loss: 12.71384048461914 = 0.12736395001411438 + 2.0 * 6.293238162994385
Epoch 730, val loss: 0.7129272818565369
Epoch 740, training loss: 12.716261863708496 = 0.12061150372028351 + 2.0 * 6.297825336456299
Epoch 740, val loss: 0.7167884111404419
Epoch 750, training loss: 12.70011043548584 = 0.11429663747549057 + 2.0 * 6.292906761169434
Epoch 750, val loss: 0.7208279371261597
Epoch 760, training loss: 12.688295364379883 = 0.10844382643699646 + 2.0 * 6.289925575256348
Epoch 760, val loss: 0.7251042723655701
Epoch 770, training loss: 12.684433937072754 = 0.10294779390096664 + 2.0 * 6.290742874145508
Epoch 770, val loss: 0.7296566367149353
Epoch 780, training loss: 12.672475814819336 = 0.09783332049846649 + 2.0 * 6.287321090698242
Epoch 780, val loss: 0.7342443466186523
Epoch 790, training loss: 12.667593002319336 = 0.09306313097476959 + 2.0 * 6.287264823913574
Epoch 790, val loss: 0.7390229105949402
Epoch 800, training loss: 12.658743858337402 = 0.08859336376190186 + 2.0 * 6.2850751876831055
Epoch 800, val loss: 0.743865430355072
Epoch 810, training loss: 12.66519832611084 = 0.08439355343580246 + 2.0 * 6.290402412414551
Epoch 810, val loss: 0.7488707304000854
Epoch 820, training loss: 12.649016380310059 = 0.08047610521316528 + 2.0 * 6.284270286560059
Epoch 820, val loss: 0.7539248466491699
Epoch 830, training loss: 12.64159107208252 = 0.07681364566087723 + 2.0 * 6.282388687133789
Epoch 830, val loss: 0.759058952331543
Epoch 840, training loss: 12.633923530578613 = 0.07336531579494476 + 2.0 * 6.280279159545898
Epoch 840, val loss: 0.764244556427002
Epoch 850, training loss: 12.635530471801758 = 0.07011949270963669 + 2.0 * 6.282705307006836
Epoch 850, val loss: 0.7695852518081665
Epoch 860, training loss: 12.62743854522705 = 0.06707087904214859 + 2.0 * 6.280183792114258
Epoch 860, val loss: 0.7749519944190979
Epoch 870, training loss: 12.632196426391602 = 0.06421639025211334 + 2.0 * 6.283989906311035
Epoch 870, val loss: 0.7802553176879883
Epoch 880, training loss: 12.616944313049316 = 0.061538998037576675 + 2.0 * 6.277702808380127
Epoch 880, val loss: 0.7855567336082458
Epoch 890, training loss: 12.610956192016602 = 0.05902106314897537 + 2.0 * 6.275967597961426
Epoch 890, val loss: 0.7909049987792969
Epoch 900, training loss: 12.605485916137695 = 0.05664244294166565 + 2.0 * 6.274421691894531
Epoch 900, val loss: 0.7964115142822266
Epoch 910, training loss: 12.603585243225098 = 0.054385166615247726 + 2.0 * 6.274600028991699
Epoch 910, val loss: 0.8019056916236877
Epoch 920, training loss: 12.599502563476562 = 0.052261389791965485 + 2.0 * 6.27362060546875
Epoch 920, val loss: 0.807413637638092
Epoch 930, training loss: 12.600016593933105 = 0.05025949329137802 + 2.0 * 6.27487850189209
Epoch 930, val loss: 0.8127708435058594
Epoch 940, training loss: 12.59064769744873 = 0.04837379232048988 + 2.0 * 6.27113676071167
Epoch 940, val loss: 0.818139910697937
Epoch 950, training loss: 12.592389106750488 = 0.046583034098148346 + 2.0 * 6.272902965545654
Epoch 950, val loss: 0.8235331177711487
Epoch 960, training loss: 12.590999603271484 = 0.044889673590660095 + 2.0 * 6.273055076599121
Epoch 960, val loss: 0.8289068937301636
Epoch 970, training loss: 12.580987930297852 = 0.0432916060090065 + 2.0 * 6.268847942352295
Epoch 970, val loss: 0.8342487215995789
Epoch 980, training loss: 12.578069686889648 = 0.04177693650126457 + 2.0 * 6.268146514892578
Epoch 980, val loss: 0.839464008808136
Epoch 990, training loss: 12.574283599853516 = 0.04033970087766647 + 2.0 * 6.266972064971924
Epoch 990, val loss: 0.8447954654693604
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8333333333333334
0.8386926726410122
The final CL Acc:0.81605, 0.01259, The final GNN Acc:0.83658, 0.00155
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.3 der2:0.2 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11624])
remove edge: torch.Size([2, 9576])
updated graph: torch.Size([2, 10644])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 19.145261764526367 = 1.9515234231948853 + 2.0 * 8.596869468688965
Epoch 0, val loss: 1.9435558319091797
Epoch 10, training loss: 19.134716033935547 = 1.9415104389190674 + 2.0 * 8.596602439880371
Epoch 10, val loss: 1.9334043264389038
Epoch 20, training loss: 19.1173038482666 = 1.9290485382080078 + 2.0 * 8.594127655029297
Epoch 20, val loss: 1.92067551612854
Epoch 30, training loss: 19.059873580932617 = 1.9120161533355713 + 2.0 * 8.573928833007812
Epoch 30, val loss: 1.9034401178359985
Epoch 40, training loss: 18.805965423583984 = 1.8909133672714233 + 2.0 * 8.457526206970215
Epoch 40, val loss: 1.8831545114517212
Epoch 50, training loss: 18.04635238647461 = 1.8697267770767212 + 2.0 * 8.088313102722168
Epoch 50, val loss: 1.8634997606277466
Epoch 60, training loss: 17.28457260131836 = 1.8517744541168213 + 2.0 * 7.7163987159729
Epoch 60, val loss: 1.847632646560669
Epoch 70, training loss: 16.289981842041016 = 1.8384230136871338 + 2.0 * 7.225779056549072
Epoch 70, val loss: 1.835943341255188
Epoch 80, training loss: 15.726510047912598 = 1.8270314931869507 + 2.0 * 6.949739456176758
Epoch 80, val loss: 1.825923204421997
Epoch 90, training loss: 15.456210136413574 = 1.8138593435287476 + 2.0 * 6.821175575256348
Epoch 90, val loss: 1.814572811126709
Epoch 100, training loss: 15.272114753723145 = 1.7990837097167969 + 2.0 * 6.736515522003174
Epoch 100, val loss: 1.802592158317566
Epoch 110, training loss: 15.137984275817871 = 1.7848962545394897 + 2.0 * 6.676544189453125
Epoch 110, val loss: 1.7911685705184937
Epoch 120, training loss: 15.045096397399902 = 1.7708895206451416 + 2.0 * 6.63710355758667
Epoch 120, val loss: 1.7794909477233887
Epoch 130, training loss: 14.964273452758789 = 1.7560772895812988 + 2.0 * 6.604098320007324
Epoch 130, val loss: 1.7669445276260376
Epoch 140, training loss: 14.902177810668945 = 1.739973545074463 + 2.0 * 6.58110237121582
Epoch 140, val loss: 1.7532953023910522
Epoch 150, training loss: 14.834826469421387 = 1.722273588180542 + 2.0 * 6.556276321411133
Epoch 150, val loss: 1.7385132312774658
Epoch 160, training loss: 14.774213790893555 = 1.7026793956756592 + 2.0 * 6.535767078399658
Epoch 160, val loss: 1.7221651077270508
Epoch 170, training loss: 14.720863342285156 = 1.6809327602386475 + 2.0 * 6.519965171813965
Epoch 170, val loss: 1.704177975654602
Epoch 180, training loss: 14.660778045654297 = 1.656699299812317 + 2.0 * 6.502039432525635
Epoch 180, val loss: 1.6843472719192505
Epoch 190, training loss: 14.604785919189453 = 1.6298463344573975 + 2.0 * 6.487469673156738
Epoch 190, val loss: 1.662463665008545
Epoch 200, training loss: 14.56271743774414 = 1.6002345085144043 + 2.0 * 6.481241703033447
Epoch 200, val loss: 1.6386040449142456
Epoch 210, training loss: 14.497764587402344 = 1.5683375597000122 + 2.0 * 6.4647135734558105
Epoch 210, val loss: 1.6130856275558472
Epoch 220, training loss: 14.440875053405762 = 1.5342814922332764 + 2.0 * 6.453296661376953
Epoch 220, val loss: 1.5860698223114014
Epoch 230, training loss: 14.388275146484375 = 1.4983168840408325 + 2.0 * 6.444979190826416
Epoch 230, val loss: 1.5579754114151
Epoch 240, training loss: 14.333118438720703 = 1.4609795808792114 + 2.0 * 6.436069488525391
Epoch 240, val loss: 1.5292892456054688
Epoch 250, training loss: 14.280503273010254 = 1.4228143692016602 + 2.0 * 6.428844451904297
Epoch 250, val loss: 1.500273585319519
Epoch 260, training loss: 14.230997085571289 = 1.3846157789230347 + 2.0 * 6.423190593719482
Epoch 260, val loss: 1.471833348274231
Epoch 270, training loss: 14.175577163696289 = 1.346553921699524 + 2.0 * 6.414511680603027
Epoch 270, val loss: 1.443837285041809
Epoch 280, training loss: 14.124555587768555 = 1.3086917400360107 + 2.0 * 6.407931804656982
Epoch 280, val loss: 1.4164854288101196
Epoch 290, training loss: 14.076428413391113 = 1.2711020708084106 + 2.0 * 6.402663230895996
Epoch 290, val loss: 1.3896204233169556
Epoch 300, training loss: 14.043355941772461 = 1.2338385581970215 + 2.0 * 6.404758453369141
Epoch 300, val loss: 1.363216757774353
Epoch 310, training loss: 13.988931655883789 = 1.197160243988037 + 2.0 * 6.395885467529297
Epoch 310, val loss: 1.3371388912200928
Epoch 320, training loss: 13.938514709472656 = 1.1608130931854248 + 2.0 * 6.388850688934326
Epoch 320, val loss: 1.3115061521530151
Epoch 330, training loss: 13.893623352050781 = 1.1244359016418457 + 2.0 * 6.384593963623047
Epoch 330, val loss: 1.2858370542526245
Epoch 340, training loss: 13.849150657653809 = 1.0879889726638794 + 2.0 * 6.380580902099609
Epoch 340, val loss: 1.2601150274276733
Epoch 350, training loss: 13.835466384887695 = 1.0515756607055664 + 2.0 * 6.3919453620910645
Epoch 350, val loss: 1.23419988155365
Epoch 360, training loss: 13.771648406982422 = 1.0155667066574097 + 2.0 * 6.378040790557861
Epoch 360, val loss: 1.2084656953811646
Epoch 370, training loss: 13.72158145904541 = 0.9799827337265015 + 2.0 * 6.370799541473389
Epoch 370, val loss: 1.183059811592102
Epoch 380, training loss: 13.679348945617676 = 0.9450914263725281 + 2.0 * 6.367128849029541
Epoch 380, val loss: 1.1583104133605957
Epoch 390, training loss: 13.639805793762207 = 0.9110047221183777 + 2.0 * 6.364400386810303
Epoch 390, val loss: 1.1342535018920898
Epoch 400, training loss: 13.606673240661621 = 0.8781435489654541 + 2.0 * 6.364264965057373
Epoch 400, val loss: 1.1112486124038696
Epoch 410, training loss: 13.565366744995117 = 0.8467731475830078 + 2.0 * 6.359296798706055
Epoch 410, val loss: 1.089522361755371
Epoch 420, training loss: 13.529251098632812 = 0.8168783187866211 + 2.0 * 6.356186389923096
Epoch 420, val loss: 1.0691059827804565
Epoch 430, training loss: 13.508119583129883 = 0.7884466648101807 + 2.0 * 6.359836578369141
Epoch 430, val loss: 1.049913763999939
Epoch 440, training loss: 13.465744972229004 = 0.7616280317306519 + 2.0 * 6.352058410644531
Epoch 440, val loss: 1.0321590900421143
Epoch 450, training loss: 13.434326171875 = 0.7362356185913086 + 2.0 * 6.349045276641846
Epoch 450, val loss: 1.0156571865081787
Epoch 460, training loss: 13.40461540222168 = 0.712059497833252 + 2.0 * 6.346277713775635
Epoch 460, val loss: 1.0002732276916504
Epoch 470, training loss: 13.390583038330078 = 0.6888329386711121 + 2.0 * 6.350874900817871
Epoch 470, val loss: 0.9856170415878296
Epoch 480, training loss: 13.35595417022705 = 0.6667175889015198 + 2.0 * 6.344618320465088
Epoch 480, val loss: 0.9719454050064087
Epoch 490, training loss: 13.326079368591309 = 0.6454257369041443 + 2.0 * 6.34032678604126
Epoch 490, val loss: 0.9589831233024597
Epoch 500, training loss: 13.301399230957031 = 0.6248955130577087 + 2.0 * 6.338252067565918
Epoch 500, val loss: 0.9469007849693298
Epoch 510, training loss: 13.277709007263184 = 0.6049782633781433 + 2.0 * 6.336365222930908
Epoch 510, val loss: 0.9353237748146057
Epoch 520, training loss: 13.26755142211914 = 0.585668683052063 + 2.0 * 6.340941429138184
Epoch 520, val loss: 0.9244366884231567
Epoch 530, training loss: 13.23808765411377 = 0.5669183731079102 + 2.0 * 6.33558464050293
Epoch 530, val loss: 0.914134681224823
Epoch 540, training loss: 13.213152885437012 = 0.5488170981407166 + 2.0 * 6.332168102264404
Epoch 540, val loss: 0.9045846462249756
Epoch 550, training loss: 13.191061973571777 = 0.5313211679458618 + 2.0 * 6.329870223999023
Epoch 550, val loss: 0.8956582546234131
Epoch 560, training loss: 13.169862747192383 = 0.5143488645553589 + 2.0 * 6.327756881713867
Epoch 560, val loss: 0.8873744606971741
Epoch 570, training loss: 13.153100967407227 = 0.4978404641151428 + 2.0 * 6.327630043029785
Epoch 570, val loss: 0.8796921372413635
Epoch 580, training loss: 13.134649276733398 = 0.4817783534526825 + 2.0 * 6.326435565948486
Epoch 580, val loss: 0.8725019097328186
Epoch 590, training loss: 13.115860939025879 = 0.46618902683258057 + 2.0 * 6.324835777282715
Epoch 590, val loss: 0.8659543991088867
Epoch 600, training loss: 13.095027923583984 = 0.45093244314193726 + 2.0 * 6.322047710418701
Epoch 600, val loss: 0.859906792640686
Epoch 610, training loss: 13.08721923828125 = 0.43593788146972656 + 2.0 * 6.325640678405762
Epoch 610, val loss: 0.8542017340660095
Epoch 620, training loss: 13.061968803405762 = 0.4210698902606964 + 2.0 * 6.320449352264404
Epoch 620, val loss: 0.8487918972969055
Epoch 630, training loss: 13.0429105758667 = 0.4064328372478485 + 2.0 * 6.318238735198975
Epoch 630, val loss: 0.8438386917114258
Epoch 640, training loss: 13.033513069152832 = 0.39185819029808044 + 2.0 * 6.320827484130859
Epoch 640, val loss: 0.8390319347381592
Epoch 650, training loss: 13.009921073913574 = 0.3773273527622223 + 2.0 * 6.3162970542907715
Epoch 650, val loss: 0.834420919418335
Epoch 660, training loss: 12.9917573928833 = 0.36287713050842285 + 2.0 * 6.3144402503967285
Epoch 660, val loss: 0.8300821781158447
Epoch 670, training loss: 12.975971221923828 = 0.3484698235988617 + 2.0 * 6.313750743865967
Epoch 670, val loss: 0.8258130550384521
Epoch 680, training loss: 12.961466789245605 = 0.33418193459510803 + 2.0 * 6.313642501831055
Epoch 680, val loss: 0.8217697143554688
Epoch 690, training loss: 12.944242477416992 = 0.3199811279773712 + 2.0 * 6.312130451202393
Epoch 690, val loss: 0.8179082274436951
Epoch 700, training loss: 12.926541328430176 = 0.30597832798957825 + 2.0 * 6.310281276702881
Epoch 700, val loss: 0.8142777681350708
Epoch 710, training loss: 12.914083480834961 = 0.29222291707992554 + 2.0 * 6.310930252075195
Epoch 710, val loss: 0.8108078241348267
Epoch 720, training loss: 12.892855644226074 = 0.2787037193775177 + 2.0 * 6.3070759773254395
Epoch 720, val loss: 0.8075827360153198
Epoch 730, training loss: 12.879395484924316 = 0.2655133008956909 + 2.0 * 6.306941032409668
Epoch 730, val loss: 0.8046168088912964
Epoch 740, training loss: 12.884668350219727 = 0.2526909112930298 + 2.0 * 6.315988540649414
Epoch 740, val loss: 0.8018732070922852
Epoch 750, training loss: 12.855489730834961 = 0.24020794034004211 + 2.0 * 6.30764102935791
Epoch 750, val loss: 0.7991411089897156
Epoch 760, training loss: 12.837550163269043 = 0.22825650870800018 + 2.0 * 6.304646968841553
Epoch 760, val loss: 0.7970127463340759
Epoch 770, training loss: 12.819921493530273 = 0.21671162545681 + 2.0 * 6.301604747772217
Epoch 770, val loss: 0.7949354648590088
Epoch 780, training loss: 12.80573844909668 = 0.2055961787700653 + 2.0 * 6.3000712394714355
Epoch 780, val loss: 0.7931399345397949
Epoch 790, training loss: 12.793683052062988 = 0.19491465389728546 + 2.0 * 6.299384117126465
Epoch 790, val loss: 0.7917313575744629
Epoch 800, training loss: 12.802071571350098 = 0.18467935919761658 + 2.0 * 6.308696269989014
Epoch 800, val loss: 0.7907079458236694
Epoch 810, training loss: 12.78195571899414 = 0.17494063079357147 + 2.0 * 6.303507328033447
Epoch 810, val loss: 0.7896521091461182
Epoch 820, training loss: 12.764154434204102 = 0.1657402366399765 + 2.0 * 6.2992072105407715
Epoch 820, val loss: 0.789251446723938
Epoch 830, training loss: 12.74815845489502 = 0.15700845420360565 + 2.0 * 6.295575141906738
Epoch 830, val loss: 0.7890718579292297
Epoch 840, training loss: 12.742063522338867 = 0.14877240359783173 + 2.0 * 6.296645641326904
Epoch 840, val loss: 0.7891973257064819
Epoch 850, training loss: 12.739990234375 = 0.1410328596830368 + 2.0 * 6.299478530883789
Epoch 850, val loss: 0.7897615432739258
Epoch 860, training loss: 12.721224784851074 = 0.1337270885705948 + 2.0 * 6.29374885559082
Epoch 860, val loss: 0.7905011773109436
Epoch 870, training loss: 12.711654663085938 = 0.12688210606575012 + 2.0 * 6.292386054992676
Epoch 870, val loss: 0.7917790412902832
Epoch 880, training loss: 12.704669952392578 = 0.1204448714852333 + 2.0 * 6.292112350463867
Epoch 880, val loss: 0.7932676076889038
Epoch 890, training loss: 12.698201179504395 = 0.11437748372554779 + 2.0 * 6.291912078857422
Epoch 890, val loss: 0.7949541211128235
Epoch 900, training loss: 12.697352409362793 = 0.10867419093847275 + 2.0 * 6.294339179992676
Epoch 900, val loss: 0.796825110912323
Epoch 910, training loss: 12.684019088745117 = 0.10334678739309311 + 2.0 * 6.2903361320495605
Epoch 910, val loss: 0.7992135882377625
Epoch 920, training loss: 12.676849365234375 = 0.09834364801645279 + 2.0 * 6.289252758026123
Epoch 920, val loss: 0.801633358001709
Epoch 930, training loss: 12.673064231872559 = 0.09364645928144455 + 2.0 * 6.289709091186523
Epoch 930, val loss: 0.804241418838501
Epoch 940, training loss: 12.662884712219238 = 0.0892261192202568 + 2.0 * 6.286829471588135
Epoch 940, val loss: 0.8070030212402344
Epoch 950, training loss: 12.657135009765625 = 0.08507458865642548 + 2.0 * 6.286030292510986
Epoch 950, val loss: 0.8101398348808289
Epoch 960, training loss: 12.652573585510254 = 0.08117396384477615 + 2.0 * 6.285699844360352
Epoch 960, val loss: 0.8133416175842285
Epoch 970, training loss: 12.661748886108398 = 0.07750782370567322 + 2.0 * 6.292120456695557
Epoch 970, val loss: 0.816611647605896
Epoch 980, training loss: 12.649459838867188 = 0.074036605656147 + 2.0 * 6.2877116203308105
Epoch 980, val loss: 0.8198529481887817
Epoch 990, training loss: 12.643084526062012 = 0.07080212235450745 + 2.0 * 6.286141395568848
Epoch 990, val loss: 0.8234630227088928
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7481481481481482
0.816025303110174
=== training gcn model ===
Epoch 0, training loss: 19.145977020263672 = 1.9522287845611572 + 2.0 * 8.596874237060547
Epoch 0, val loss: 1.9511178731918335
Epoch 10, training loss: 19.134977340698242 = 1.9415326118469238 + 2.0 * 8.596722602844238
Epoch 10, val loss: 1.9406304359436035
Epoch 20, training loss: 19.119354248046875 = 1.9283523559570312 + 2.0 * 8.595500946044922
Epoch 20, val loss: 1.927800178527832
Epoch 30, training loss: 19.080427169799805 = 1.910154938697815 + 2.0 * 8.585136413574219
Epoch 30, val loss: 1.9103431701660156
Epoch 40, training loss: 18.93541717529297 = 1.8862380981445312 + 2.0 * 8.524589538574219
Epoch 40, val loss: 1.8882609605789185
Epoch 50, training loss: 18.223180770874023 = 1.8616740703582764 + 2.0 * 8.180753707885742
Epoch 50, val loss: 1.8661667108535767
Epoch 60, training loss: 17.48558235168457 = 1.840604543685913 + 2.0 * 7.822489261627197
Epoch 60, val loss: 1.8475422859191895
Epoch 70, training loss: 16.69872283935547 = 1.8253082036972046 + 2.0 * 7.436707019805908
Epoch 70, val loss: 1.8328064680099487
Epoch 80, training loss: 16.154319763183594 = 1.8122761249542236 + 2.0 * 7.171021461486816
Epoch 80, val loss: 1.8202415704727173
Epoch 90, training loss: 15.696586608886719 = 1.8001294136047363 + 2.0 * 6.94822883605957
Epoch 90, val loss: 1.8089457750320435
Epoch 100, training loss: 15.487411499023438 = 1.7870155572891235 + 2.0 * 6.850197792053223
Epoch 100, val loss: 1.797042965888977
Epoch 110, training loss: 15.340664863586426 = 1.772302508354187 + 2.0 * 6.784181118011475
Epoch 110, val loss: 1.7836533784866333
Epoch 120, training loss: 15.21501350402832 = 1.7574716806411743 + 2.0 * 6.728770732879639
Epoch 120, val loss: 1.7699151039123535
Epoch 130, training loss: 15.10919189453125 = 1.7420424222946167 + 2.0 * 6.683574676513672
Epoch 130, val loss: 1.7556401491165161
Epoch 140, training loss: 15.02183723449707 = 1.7251191139221191 + 2.0 * 6.648359298706055
Epoch 140, val loss: 1.7404025793075562
Epoch 150, training loss: 14.942543029785156 = 1.7061436176300049 + 2.0 * 6.618199825286865
Epoch 150, val loss: 1.7234965562820435
Epoch 160, training loss: 14.86796760559082 = 1.6847227811813354 + 2.0 * 6.591622352600098
Epoch 160, val loss: 1.7047126293182373
Epoch 170, training loss: 14.80080795288086 = 1.6605843305587769 + 2.0 * 6.5701117515563965
Epoch 170, val loss: 1.6836234331130981
Epoch 180, training loss: 14.733428001403809 = 1.6334384679794312 + 2.0 * 6.549994945526123
Epoch 180, val loss: 1.6600117683410645
Epoch 190, training loss: 14.668642044067383 = 1.6028761863708496 + 2.0 * 6.5328826904296875
Epoch 190, val loss: 1.633577823638916
Epoch 200, training loss: 14.605411529541016 = 1.568790078163147 + 2.0 * 6.518310546875
Epoch 200, val loss: 1.6042680740356445
Epoch 210, training loss: 14.546269416809082 = 1.531288504600525 + 2.0 * 6.507490634918213
Epoch 210, val loss: 1.5723029375076294
Epoch 220, training loss: 14.480676651000977 = 1.4909828901290894 + 2.0 * 6.494846820831299
Epoch 220, val loss: 1.5382483005523682
Epoch 230, training loss: 14.417746543884277 = 1.4482473134994507 + 2.0 * 6.484749794006348
Epoch 230, val loss: 1.5026538372039795
Epoch 240, training loss: 14.35550594329834 = 1.4043776988983154 + 2.0 * 6.475564002990723
Epoch 240, val loss: 1.4665948152542114
Epoch 250, training loss: 14.29343032836914 = 1.3600821495056152 + 2.0 * 6.466674327850342
Epoch 250, val loss: 1.4307613372802734
Epoch 260, training loss: 14.233709335327148 = 1.3159171342849731 + 2.0 * 6.458896160125732
Epoch 260, val loss: 1.3955047130584717
Epoch 270, training loss: 14.183488845825195 = 1.2723805904388428 + 2.0 * 6.455554008483887
Epoch 270, val loss: 1.3613568544387817
Epoch 280, training loss: 14.122612953186035 = 1.2301923036575317 + 2.0 * 6.4462103843688965
Epoch 280, val loss: 1.3284721374511719
Epoch 290, training loss: 14.066469192504883 = 1.188948154449463 + 2.0 * 6.438760757446289
Epoch 290, val loss: 1.2969367504119873
Epoch 300, training loss: 14.013832092285156 = 1.1486414670944214 + 2.0 * 6.432595252990723
Epoch 300, val loss: 1.2665270566940308
Epoch 310, training loss: 13.968059539794922 = 1.1091002225875854 + 2.0 * 6.429479598999023
Epoch 310, val loss: 1.2369403839111328
Epoch 320, training loss: 13.918396949768066 = 1.0704755783081055 + 2.0 * 6.4239606857299805
Epoch 320, val loss: 1.2085316181182861
Epoch 330, training loss: 13.871960639953613 = 1.0330225229263306 + 2.0 * 6.419468879699707
Epoch 330, val loss: 1.1811060905456543
Epoch 340, training loss: 13.824074745178223 = 0.9963655471801758 + 2.0 * 6.413854598999023
Epoch 340, val loss: 1.1545636653900146
Epoch 350, training loss: 13.779459953308105 = 0.9602662324905396 + 2.0 * 6.409596920013428
Epoch 350, val loss: 1.128761649131775
Epoch 360, training loss: 13.745132446289062 = 0.9248623251914978 + 2.0 * 6.410135269165039
Epoch 360, val loss: 1.103579044342041
Epoch 370, training loss: 13.695355415344238 = 0.8904089331626892 + 2.0 * 6.402473449707031
Epoch 370, val loss: 1.079486608505249
Epoch 380, training loss: 13.655229568481445 = 0.8571106195449829 + 2.0 * 6.399059295654297
Epoch 380, val loss: 1.056306004524231
Epoch 390, training loss: 13.615564346313477 = 0.824730396270752 + 2.0 * 6.395416736602783
Epoch 390, val loss: 1.0341320037841797
Epoch 400, training loss: 13.590842247009277 = 0.7933472394943237 + 2.0 * 6.398747444152832
Epoch 400, val loss: 1.0127729177474976
Epoch 410, training loss: 13.543343544006348 = 0.7629283666610718 + 2.0 * 6.390207767486572
Epoch 410, val loss: 0.9927626848220825
Epoch 420, training loss: 13.506481170654297 = 0.7337217926979065 + 2.0 * 6.386379718780518
Epoch 420, val loss: 0.9738770723342896
Epoch 430, training loss: 13.470252990722656 = 0.7055332660675049 + 2.0 * 6.382359981536865
Epoch 430, val loss: 0.9560778737068176
Epoch 440, training loss: 13.44107437133789 = 0.6782513856887817 + 2.0 * 6.381411552429199
Epoch 440, val loss: 0.9393945336341858
Epoch 450, training loss: 13.407872200012207 = 0.6519986391067505 + 2.0 * 6.377936840057373
Epoch 450, val loss: 0.9239628314971924
Epoch 460, training loss: 13.373641967773438 = 0.6266831755638123 + 2.0 * 6.37347936630249
Epoch 460, val loss: 0.9097377061843872
Epoch 470, training loss: 13.355966567993164 = 0.6022148132324219 + 2.0 * 6.376875877380371
Epoch 470, val loss: 0.896619439125061
Epoch 480, training loss: 13.315008163452148 = 0.5786269307136536 + 2.0 * 6.368190765380859
Epoch 480, val loss: 0.8848130702972412
Epoch 490, training loss: 13.28866195678711 = 0.556000292301178 + 2.0 * 6.366330623626709
Epoch 490, val loss: 0.8742130994796753
Epoch 500, training loss: 13.260786056518555 = 0.534076988697052 + 2.0 * 6.363354682922363
Epoch 500, val loss: 0.864692747592926
Epoch 510, training loss: 13.240764617919922 = 0.5127941370010376 + 2.0 * 6.363985061645508
Epoch 510, val loss: 0.8561493754386902
Epoch 520, training loss: 13.214625358581543 = 0.49202239513397217 + 2.0 * 6.361301422119141
Epoch 520, val loss: 0.8486738204956055
Epoch 530, training loss: 13.18453311920166 = 0.47197964787483215 + 2.0 * 6.356276512145996
Epoch 530, val loss: 0.8421010375022888
Epoch 540, training loss: 13.16089916229248 = 0.45238929986953735 + 2.0 * 6.354254722595215
Epoch 540, val loss: 0.8364478349685669
Epoch 550, training loss: 13.152141571044922 = 0.4331916570663452 + 2.0 * 6.359475135803223
Epoch 550, val loss: 0.8315844535827637
Epoch 560, training loss: 13.11605167388916 = 0.41455960273742676 + 2.0 * 6.350746154785156
Epoch 560, val loss: 0.8274799585342407
Epoch 570, training loss: 13.093853950500488 = 0.3963126838207245 + 2.0 * 6.348770618438721
Epoch 570, val loss: 0.8241428732872009
Epoch 580, training loss: 13.070794105529785 = 0.37849241495132446 + 2.0 * 6.346150875091553
Epoch 580, val loss: 0.8214117288589478
Epoch 590, training loss: 13.060760498046875 = 0.36110320687294006 + 2.0 * 6.349828720092773
Epoch 590, val loss: 0.8192598819732666
Epoch 600, training loss: 13.033758163452148 = 0.34416934847831726 + 2.0 * 6.344794273376465
Epoch 600, val loss: 0.8177185654640198
Epoch 610, training loss: 13.011717796325684 = 0.32782161235809326 + 2.0 * 6.34194803237915
Epoch 610, val loss: 0.8167330622673035
Epoch 620, training loss: 12.991390228271484 = 0.31202322244644165 + 2.0 * 6.339683532714844
Epoch 620, val loss: 0.8163297176361084
Epoch 630, training loss: 12.973644256591797 = 0.29675331711769104 + 2.0 * 6.338445663452148
Epoch 630, val loss: 0.8164247274398804
Epoch 640, training loss: 12.96815013885498 = 0.282047837972641 + 2.0 * 6.343050956726074
Epoch 640, val loss: 0.8169893622398376
Epoch 650, training loss: 12.93951416015625 = 0.26787251234054565 + 2.0 * 6.33582067489624
Epoch 650, val loss: 0.8179859519004822
Epoch 660, training loss: 12.923233985900879 = 0.25436508655548096 + 2.0 * 6.334434509277344
Epoch 660, val loss: 0.8194156885147095
Epoch 670, training loss: 12.906746864318848 = 0.2414468675851822 + 2.0 * 6.332650184631348
Epoch 670, val loss: 0.8213006854057312
Epoch 680, training loss: 12.901846885681152 = 0.22911520302295685 + 2.0 * 6.336365699768066
Epoch 680, val loss: 0.823513925075531
Epoch 690, training loss: 12.881726264953613 = 0.21735283732414246 + 2.0 * 6.332186698913574
Epoch 690, val loss: 0.8260812759399414
Epoch 700, training loss: 12.867255210876465 = 0.2062911093235016 + 2.0 * 6.330482006072998
Epoch 700, val loss: 0.8288838267326355
Epoch 710, training loss: 12.852646827697754 = 0.19579674303531647 + 2.0 * 6.32842493057251
Epoch 710, val loss: 0.8320373296737671
Epoch 720, training loss: 12.838022232055664 = 0.18591168522834778 + 2.0 * 6.32605504989624
Epoch 720, val loss: 0.8355178833007812
Epoch 730, training loss: 12.829388618469238 = 0.1765402853488922 + 2.0 * 6.3264241218566895
Epoch 730, val loss: 0.8392423391342163
Epoch 740, training loss: 12.816258430480957 = 0.1676880419254303 + 2.0 * 6.32428503036499
Epoch 740, val loss: 0.843110203742981
Epoch 750, training loss: 12.814863204956055 = 0.15933561325073242 + 2.0 * 6.327763557434082
Epoch 750, val loss: 0.8472198843955994
Epoch 760, training loss: 12.794955253601074 = 0.1515180468559265 + 2.0 * 6.321718692779541
Epoch 760, val loss: 0.8514384627342224
Epoch 770, training loss: 12.783895492553711 = 0.14414072036743164 + 2.0 * 6.319877624511719
Epoch 770, val loss: 0.8558400869369507
Epoch 780, training loss: 12.777158737182617 = 0.13719744980335236 + 2.0 * 6.319980621337891
Epoch 780, val loss: 0.8603551983833313
Epoch 790, training loss: 12.772457122802734 = 0.13063916563987732 + 2.0 * 6.320909023284912
Epoch 790, val loss: 0.8648754358291626
Epoch 800, training loss: 12.757448196411133 = 0.12444283068180084 + 2.0 * 6.316502571105957
Epoch 800, val loss: 0.8694614768028259
Epoch 810, training loss: 12.748937606811523 = 0.11862657964229584 + 2.0 * 6.315155506134033
Epoch 810, val loss: 0.8742045760154724
Epoch 820, training loss: 12.74556827545166 = 0.11312451958656311 + 2.0 * 6.316221714019775
Epoch 820, val loss: 0.8789663910865784
Epoch 830, training loss: 12.737865447998047 = 0.10794953256845474 + 2.0 * 6.314958095550537
Epoch 830, val loss: 0.8837931156158447
Epoch 840, training loss: 12.73358154296875 = 0.10305024683475494 + 2.0 * 6.315265655517578
Epoch 840, val loss: 0.8885645270347595
Epoch 850, training loss: 12.722054481506348 = 0.09845923632383347 + 2.0 * 6.311797618865967
Epoch 850, val loss: 0.8934818506240845
Epoch 860, training loss: 12.719432830810547 = 0.09411714971065521 + 2.0 * 6.312657833099365
Epoch 860, val loss: 0.89842689037323
Epoch 870, training loss: 12.709282875061035 = 0.09000272303819656 + 2.0 * 6.309639930725098
Epoch 870, val loss: 0.90333491563797
Epoch 880, training loss: 12.70172119140625 = 0.08611462265253067 + 2.0 * 6.307803153991699
Epoch 880, val loss: 0.9082589745521545
Epoch 890, training loss: 12.708552360534668 = 0.08242916315793991 + 2.0 * 6.313061714172363
Epoch 890, val loss: 0.9131703972816467
Epoch 900, training loss: 12.709221839904785 = 0.07897797226905823 + 2.0 * 6.315122127532959
Epoch 900, val loss: 0.9179978966712952
Epoch 910, training loss: 12.691646575927734 = 0.07568833976984024 + 2.0 * 6.307979106903076
Epoch 910, val loss: 0.9227633476257324
Epoch 920, training loss: 12.681906700134277 = 0.07260112464427948 + 2.0 * 6.304652690887451
Epoch 920, val loss: 0.9276344180107117
Epoch 930, training loss: 12.675600051879883 = 0.06967517733573914 + 2.0 * 6.302962303161621
Epoch 930, val loss: 0.9324904680252075
Epoch 940, training loss: 12.671043395996094 = 0.06688765436410904 + 2.0 * 6.302077770233154
Epoch 940, val loss: 0.9373054504394531
Epoch 950, training loss: 12.666316986083984 = 0.0642387643456459 + 2.0 * 6.301039218902588
Epoch 950, val loss: 0.9421308040618896
Epoch 960, training loss: 12.700675964355469 = 0.061722226440906525 + 2.0 * 6.319477081298828
Epoch 960, val loss: 0.9470682144165039
Epoch 970, training loss: 12.659442901611328 = 0.059320516884326935 + 2.0 * 6.300061225891113
Epoch 970, val loss: 0.9514920711517334
Epoch 980, training loss: 12.654850006103516 = 0.057060953229665756 + 2.0 * 6.29889440536499
Epoch 980, val loss: 0.9560521841049194
Epoch 990, training loss: 12.650445938110352 = 0.05492289364337921 + 2.0 * 6.2977614402771
Epoch 990, val loss: 0.9606921076774597
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7407407407407408
0.827095413811281
=== training gcn model ===
Epoch 0, training loss: 19.143226623535156 = 1.9495068788528442 + 2.0 * 8.5968599319458
Epoch 0, val loss: 1.9580340385437012
Epoch 10, training loss: 19.13217544555664 = 1.9389914274215698 + 2.0 * 8.59659194946289
Epoch 10, val loss: 1.9476605653762817
Epoch 20, training loss: 19.1146297454834 = 1.9259445667266846 + 2.0 * 8.594342231750488
Epoch 20, val loss: 1.9342601299285889
Epoch 30, training loss: 19.059053421020508 = 1.908302903175354 + 2.0 * 8.5753755569458
Epoch 30, val loss: 1.9157660007476807
Epoch 40, training loss: 18.76285743713379 = 1.8869463205337524 + 2.0 * 8.437955856323242
Epoch 40, val loss: 1.8940327167510986
Epoch 50, training loss: 17.679243087768555 = 1.8653019666671753 + 2.0 * 7.906970500946045
Epoch 50, val loss: 1.872064232826233
Epoch 60, training loss: 16.868539810180664 = 1.8497097492218018 + 2.0 * 7.509415149688721
Epoch 60, val loss: 1.8576194047927856
Epoch 70, training loss: 16.273231506347656 = 1.8377248048782349 + 2.0 * 7.217752933502197
Epoch 70, val loss: 1.8460379838943481
Epoch 80, training loss: 15.777853012084961 = 1.8260416984558105 + 2.0 * 6.975905895233154
Epoch 80, val loss: 1.8345872163772583
Epoch 90, training loss: 15.54606819152832 = 1.813927173614502 + 2.0 * 6.866070747375488
Epoch 90, val loss: 1.822486162185669
Epoch 100, training loss: 15.38058853149414 = 1.8015401363372803 + 2.0 * 6.789524078369141
Epoch 100, val loss: 1.810469388961792
Epoch 110, training loss: 15.2378568649292 = 1.7894057035446167 + 2.0 * 6.7242255210876465
Epoch 110, val loss: 1.798904299736023
Epoch 120, training loss: 15.136810302734375 = 1.7776002883911133 + 2.0 * 6.679605007171631
Epoch 120, val loss: 1.7877275943756104
Epoch 130, training loss: 15.0459623336792 = 1.7650995254516602 + 2.0 * 6.6404314041137695
Epoch 130, val loss: 1.7761386632919312
Epoch 140, training loss: 14.971237182617188 = 1.751501202583313 + 2.0 * 6.609868049621582
Epoch 140, val loss: 1.7635480165481567
Epoch 150, training loss: 14.903280258178711 = 1.7365409135818481 + 2.0 * 6.583369731903076
Epoch 150, val loss: 1.7498754262924194
Epoch 160, training loss: 14.83895206451416 = 1.7198735475540161 + 2.0 * 6.559539318084717
Epoch 160, val loss: 1.734895944595337
Epoch 170, training loss: 14.782785415649414 = 1.7011172771453857 + 2.0 * 6.540833950042725
Epoch 170, val loss: 1.7182704210281372
Epoch 180, training loss: 14.726519584655762 = 1.679933786392212 + 2.0 * 6.5232930183410645
Epoch 180, val loss: 1.69968843460083
Epoch 190, training loss: 14.673038482666016 = 1.6560721397399902 + 2.0 * 6.508482933044434
Epoch 190, val loss: 1.678902268409729
Epoch 200, training loss: 14.618632316589355 = 1.6292381286621094 + 2.0 * 6.494697093963623
Epoch 200, val loss: 1.6557250022888184
Epoch 210, training loss: 14.57170295715332 = 1.5994433164596558 + 2.0 * 6.4861297607421875
Epoch 210, val loss: 1.6301934719085693
Epoch 220, training loss: 14.510944366455078 = 1.5667977333068848 + 2.0 * 6.472073554992676
Epoch 220, val loss: 1.6024256944656372
Epoch 230, training loss: 14.45233154296875 = 1.5311758518218994 + 2.0 * 6.460577964782715
Epoch 230, val loss: 1.5723539590835571
Epoch 240, training loss: 14.4110689163208 = 1.4927681684494019 + 2.0 * 6.459150314331055
Epoch 240, val loss: 1.540208101272583
Epoch 250, training loss: 14.341465950012207 = 1.4522950649261475 + 2.0 * 6.44458532333374
Epoch 250, val loss: 1.5067903995513916
Epoch 260, training loss: 14.28459358215332 = 1.410332441329956 + 2.0 * 6.437130451202393
Epoch 260, val loss: 1.4722398519515991
Epoch 270, training loss: 14.226006507873535 = 1.3670074939727783 + 2.0 * 6.429499626159668
Epoch 270, val loss: 1.4367954730987549
Epoch 280, training loss: 14.187814712524414 = 1.3225971460342407 + 2.0 * 6.432608604431152
Epoch 280, val loss: 1.4008915424346924
Epoch 290, training loss: 14.11526107788086 = 1.27803373336792 + 2.0 * 6.418613433837891
Epoch 290, val loss: 1.3650671243667603
Epoch 300, training loss: 14.0572509765625 = 1.2333213090896606 + 2.0 * 6.4119648933410645
Epoch 300, val loss: 1.3294522762298584
Epoch 310, training loss: 14.01242446899414 = 1.1888105869293213 + 2.0 * 6.411807060241699
Epoch 310, val loss: 1.2943042516708374
Epoch 320, training loss: 13.951545715332031 = 1.1448094844818115 + 2.0 * 6.40336799621582
Epoch 320, val loss: 1.2598106861114502
Epoch 330, training loss: 13.896562576293945 = 1.101300597190857 + 2.0 * 6.3976311683654785
Epoch 330, val loss: 1.2259608507156372
Epoch 340, training loss: 13.86275863647461 = 1.0582414865493774 + 2.0 * 6.402258396148682
Epoch 340, val loss: 1.1927878856658936
Epoch 350, training loss: 13.800058364868164 = 1.0165810585021973 + 2.0 * 6.391738414764404
Epoch 350, val loss: 1.160764217376709
Epoch 360, training loss: 13.74726676940918 = 0.9757398366928101 + 2.0 * 6.385763645172119
Epoch 360, val loss: 1.1296417713165283
Epoch 370, training loss: 13.709769248962402 = 0.9359732866287231 + 2.0 * 6.386898040771484
Epoch 370, val loss: 1.0994070768356323
Epoch 380, training loss: 13.65832233428955 = 0.8974665999412537 + 2.0 * 6.380427837371826
Epoch 380, val loss: 1.070523738861084
Epoch 390, training loss: 13.610397338867188 = 0.8605149388313293 + 2.0 * 6.374941349029541
Epoch 390, val loss: 1.042816162109375
Epoch 400, training loss: 13.569075584411621 = 0.8248546719551086 + 2.0 * 6.372110366821289
Epoch 400, val loss: 1.0162484645843506
Epoch 410, training loss: 13.526872634887695 = 0.7905053496360779 + 2.0 * 6.368183612823486
Epoch 410, val loss: 0.9909298419952393
Epoch 420, training loss: 13.500171661376953 = 0.7576377987861633 + 2.0 * 6.371266841888428
Epoch 420, val loss: 0.9668238759040833
Epoch 430, training loss: 13.454030990600586 = 0.726294994354248 + 2.0 * 6.363868236541748
Epoch 430, val loss: 0.9439561367034912
Epoch 440, training loss: 13.414716720581055 = 0.6960331201553345 + 2.0 * 6.359341621398926
Epoch 440, val loss: 0.922319769859314
Epoch 450, training loss: 13.379573822021484 = 0.6668963432312012 + 2.0 * 6.3563385009765625
Epoch 450, val loss: 0.9017128348350525
Epoch 460, training loss: 13.346570014953613 = 0.6388117671012878 + 2.0 * 6.353878974914551
Epoch 460, val loss: 0.8821191787719727
Epoch 470, training loss: 13.32377815246582 = 0.6117658615112305 + 2.0 * 6.356006145477295
Epoch 470, val loss: 0.8635997772216797
Epoch 480, training loss: 13.28576946258545 = 0.5858207941055298 + 2.0 * 6.349974155426025
Epoch 480, val loss: 0.8460520505905151
Epoch 490, training loss: 13.2529878616333 = 0.5607529282569885 + 2.0 * 6.3461174964904785
Epoch 490, val loss: 0.8295428156852722
Epoch 500, training loss: 13.227749824523926 = 0.5365374088287354 + 2.0 * 6.345606327056885
Epoch 500, val loss: 0.8139356970787048
Epoch 510, training loss: 13.20749282836914 = 0.5132630467414856 + 2.0 * 6.3471150398254395
Epoch 510, val loss: 0.799277126789093
Epoch 520, training loss: 13.173929214477539 = 0.49087783694267273 + 2.0 * 6.341525554656982
Epoch 520, val loss: 0.7856289744377136
Epoch 530, training loss: 13.14554500579834 = 0.4693635404109955 + 2.0 * 6.338090896606445
Epoch 530, val loss: 0.7728723883628845
Epoch 540, training loss: 13.130049705505371 = 0.4485655128955841 + 2.0 * 6.340742111206055
Epoch 540, val loss: 0.7610061764717102
Epoch 550, training loss: 13.107072830200195 = 0.42869386076927185 + 2.0 * 6.339189529418945
Epoch 550, val loss: 0.7498500943183899
Epoch 560, training loss: 13.073615074157715 = 0.40949589014053345 + 2.0 * 6.332059383392334
Epoch 560, val loss: 0.7396420240402222
Epoch 570, training loss: 13.052687644958496 = 0.3910515010356903 + 2.0 * 6.330818176269531
Epoch 570, val loss: 0.730263352394104
Epoch 580, training loss: 13.059406280517578 = 0.3733028471469879 + 2.0 * 6.343051910400391
Epoch 580, val loss: 0.7216441035270691
Epoch 590, training loss: 13.01231861114502 = 0.35638511180877686 + 2.0 * 6.327966690063477
Epoch 590, val loss: 0.7136613130569458
Epoch 600, training loss: 12.992849349975586 = 0.34012824296951294 + 2.0 * 6.326360702514648
Epoch 600, val loss: 0.7064405083656311
Epoch 610, training loss: 12.972482681274414 = 0.32449257373809814 + 2.0 * 6.323995113372803
Epoch 610, val loss: 0.6999397873878479
Epoch 620, training loss: 12.954761505126953 = 0.30946165323257446 + 2.0 * 6.322649955749512
Epoch 620, val loss: 0.6940032243728638
Epoch 630, training loss: 12.951565742492676 = 0.29498931765556335 + 2.0 * 6.3282880783081055
Epoch 630, val loss: 0.6886712908744812
Epoch 640, training loss: 12.926881790161133 = 0.2811930775642395 + 2.0 * 6.322844505310059
Epoch 640, val loss: 0.683870792388916
Epoch 650, training loss: 12.907289505004883 = 0.2679671347141266 + 2.0 * 6.3196611404418945
Epoch 650, val loss: 0.6796494722366333
Epoch 660, training loss: 12.888460159301758 = 0.25526976585388184 + 2.0 * 6.316595077514648
Epoch 660, val loss: 0.675977885723114
Epoch 670, training loss: 12.880260467529297 = 0.24307361245155334 + 2.0 * 6.318593502044678
Epoch 670, val loss: 0.6728259325027466
Epoch 680, training loss: 12.874300956726074 = 0.23142613470554352 + 2.0 * 6.321437358856201
Epoch 680, val loss: 0.6701944470405579
Epoch 690, training loss: 12.85123348236084 = 0.22035883367061615 + 2.0 * 6.315437316894531
Epoch 690, val loss: 0.6679072976112366
Epoch 700, training loss: 12.834680557250977 = 0.20976726710796356 + 2.0 * 6.3124566078186035
Epoch 700, val loss: 0.6661830544471741
Epoch 710, training loss: 12.821470260620117 = 0.19965234398841858 + 2.0 * 6.310908794403076
Epoch 710, val loss: 0.664897620677948
Epoch 720, training loss: 12.826131820678711 = 0.1899641752243042 + 2.0 * 6.318083763122559
Epoch 720, val loss: 0.6639912128448486
Epoch 730, training loss: 12.817716598510742 = 0.18081656098365784 + 2.0 * 6.318449974060059
Epoch 730, val loss: 0.6634110808372498
Epoch 740, training loss: 12.792409896850586 = 0.17208640277385712 + 2.0 * 6.310161590576172
Epoch 740, val loss: 0.6631874442100525
Epoch 750, training loss: 12.776328086853027 = 0.16382871568202972 + 2.0 * 6.306249618530273
Epoch 750, val loss: 0.6633729338645935
Epoch 760, training loss: 12.766779899597168 = 0.15596574544906616 + 2.0 * 6.3054070472717285
Epoch 760, val loss: 0.663933515548706
Epoch 770, training loss: 12.762643814086914 = 0.14849279820919037 + 2.0 * 6.307075500488281
Epoch 770, val loss: 0.6648228764533997
Epoch 780, training loss: 12.750972747802734 = 0.1414082646369934 + 2.0 * 6.304782390594482
Epoch 780, val loss: 0.6660630106925964
Epoch 790, training loss: 12.73945426940918 = 0.13469702005386353 + 2.0 * 6.3023786544799805
Epoch 790, val loss: 0.6674806475639343
Epoch 800, training loss: 12.734146118164062 = 0.12835927307605743 + 2.0 * 6.30289363861084
Epoch 800, val loss: 0.669235110282898
Epoch 810, training loss: 12.727109909057617 = 0.12237125635147095 + 2.0 * 6.302369117736816
Epoch 810, val loss: 0.6711046695709229
Epoch 820, training loss: 12.719902038574219 = 0.11669568717479706 + 2.0 * 6.301603317260742
Epoch 820, val loss: 0.6732223033905029
Epoch 830, training loss: 12.711137771606445 = 0.1113205999135971 + 2.0 * 6.299908638000488
Epoch 830, val loss: 0.6754757165908813
Epoch 840, training loss: 12.705050468444824 = 0.10625491291284561 + 2.0 * 6.299397945404053
Epoch 840, val loss: 0.6779552102088928
Epoch 850, training loss: 12.701375007629395 = 0.10146304965019226 + 2.0 * 6.29995584487915
Epoch 850, val loss: 0.6806076169013977
Epoch 860, training loss: 12.692432403564453 = 0.09691824018955231 + 2.0 * 6.297757148742676
Epoch 860, val loss: 0.6833894848823547
Epoch 870, training loss: 12.682150840759277 = 0.09263865649700165 + 2.0 * 6.294755935668945
Epoch 870, val loss: 0.6863085031509399
Epoch 880, training loss: 12.680242538452148 = 0.08857538551092148 + 2.0 * 6.295833587646484
Epoch 880, val loss: 0.6893364787101746
Epoch 890, training loss: 12.682452201843262 = 0.08473033457994461 + 2.0 * 6.298861026763916
Epoch 890, val loss: 0.6924816966056824
Epoch 900, training loss: 12.665593147277832 = 0.08110464364290237 + 2.0 * 6.2922444343566895
Epoch 900, val loss: 0.6956279873847961
Epoch 910, training loss: 12.660760879516602 = 0.07767383009195328 + 2.0 * 6.291543483734131
Epoch 910, val loss: 0.6988765597343445
Epoch 920, training loss: 12.690322875976562 = 0.07443740218877792 + 2.0 * 6.307942867279053
Epoch 920, val loss: 0.7022057175636292
Epoch 930, training loss: 12.66001033782959 = 0.0713377296924591 + 2.0 * 6.294336318969727
Epoch 930, val loss: 0.7055301070213318
Epoch 940, training loss: 12.647547721862793 = 0.06844531744718552 + 2.0 * 6.289551258087158
Epoch 940, val loss: 0.7088620066642761
Epoch 950, training loss: 12.641779899597168 = 0.06569749861955643 + 2.0 * 6.288041114807129
Epoch 950, val loss: 0.7123333811759949
Epoch 960, training loss: 12.637502670288086 = 0.06307844072580338 + 2.0 * 6.287211894989014
Epoch 960, val loss: 0.7158237099647522
Epoch 970, training loss: 12.665449142456055 = 0.060598812997341156 + 2.0 * 6.302425384521484
Epoch 970, val loss: 0.7192940711975098
Epoch 980, training loss: 12.63670825958252 = 0.0582256056368351 + 2.0 * 6.289241313934326
Epoch 980, val loss: 0.7227658629417419
Epoch 990, training loss: 12.626973152160645 = 0.056004688143730164 + 2.0 * 6.285484313964844
Epoch 990, val loss: 0.726250946521759
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7518518518518519
0.8065366367949395
The final CL Acc:0.74691, 0.00462, The final GNN Acc:0.81655, 0.00840
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13272])
remove edge: torch.Size([2, 7994])
updated graph: torch.Size([2, 10710])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 19.120540618896484 = 1.9268277883529663 + 2.0 * 8.596856117248535
Epoch 0, val loss: 1.9230364561080933
Epoch 10, training loss: 19.110225677490234 = 1.9170844554901123 + 2.0 * 8.59657096862793
Epoch 10, val loss: 1.9127774238586426
Epoch 20, training loss: 19.092702865600586 = 1.9051538705825806 + 2.0 * 8.593774795532227
Epoch 20, val loss: 1.9001182317733765
Epoch 30, training loss: 19.032623291015625 = 1.8892229795455933 + 2.0 * 8.571700096130371
Epoch 30, val loss: 1.8833051919937134
Epoch 40, training loss: 18.75450897216797 = 1.8697816133499146 + 2.0 * 8.442363739013672
Epoch 40, val loss: 1.8638150691986084
Epoch 50, training loss: 18.07487678527832 = 1.8490699529647827 + 2.0 * 8.112903594970703
Epoch 50, val loss: 1.8442432880401611
Epoch 60, training loss: 17.568283081054688 = 1.8317073583602905 + 2.0 * 7.868288040161133
Epoch 60, val loss: 1.8294365406036377
Epoch 70, training loss: 16.7125186920166 = 1.8180087804794312 + 2.0 * 7.447254657745361
Epoch 70, val loss: 1.817101001739502
Epoch 80, training loss: 16.046951293945312 = 1.8089938163757324 + 2.0 * 7.118978977203369
Epoch 80, val loss: 1.8087503910064697
Epoch 90, training loss: 15.645923614501953 = 1.7972049713134766 + 2.0 * 6.924359321594238
Epoch 90, val loss: 1.7979224920272827
Epoch 100, training loss: 15.444941520690918 = 1.7828785181045532 + 2.0 * 6.831031322479248
Epoch 100, val loss: 1.7850934267044067
Epoch 110, training loss: 15.296865463256836 = 1.7688461542129517 + 2.0 * 6.764009475708008
Epoch 110, val loss: 1.7725194692611694
Epoch 120, training loss: 15.168912887573242 = 1.7551168203353882 + 2.0 * 6.706898212432861
Epoch 120, val loss: 1.759961724281311
Epoch 130, training loss: 15.064016342163086 = 1.74031400680542 + 2.0 * 6.661850929260254
Epoch 130, val loss: 1.7463375329971313
Epoch 140, training loss: 14.975337028503418 = 1.7235071659088135 + 2.0 * 6.625915050506592
Epoch 140, val loss: 1.7312546968460083
Epoch 150, training loss: 14.9002046585083 = 1.7047051191329956 + 2.0 * 6.597749710083008
Epoch 150, val loss: 1.7144560813903809
Epoch 160, training loss: 14.820755958557129 = 1.6836646795272827 + 2.0 * 6.568545818328857
Epoch 160, val loss: 1.6958582401275635
Epoch 170, training loss: 14.762426376342773 = 1.6599466800689697 + 2.0 * 6.551239967346191
Epoch 170, val loss: 1.67495596408844
Epoch 180, training loss: 14.681575775146484 = 1.633104920387268 + 2.0 * 6.524235248565674
Epoch 180, val loss: 1.651549220085144
Epoch 190, training loss: 14.618054389953613 = 1.6029348373413086 + 2.0 * 6.507559776306152
Epoch 190, val loss: 1.6252951622009277
Epoch 200, training loss: 14.55573558807373 = 1.569010615348816 + 2.0 * 6.4933624267578125
Epoch 200, val loss: 1.5958163738250732
Epoch 210, training loss: 14.507563591003418 = 1.5309611558914185 + 2.0 * 6.4883012771606445
Epoch 210, val loss: 1.5627409219741821
Epoch 220, training loss: 14.431848526000977 = 1.4886943101882935 + 2.0 * 6.471577167510986
Epoch 220, val loss: 1.5263211727142334
Epoch 230, training loss: 14.364640235900879 = 1.4433472156524658 + 2.0 * 6.460646629333496
Epoch 230, val loss: 1.4874045848846436
Epoch 240, training loss: 14.297876358032227 = 1.3946269750595093 + 2.0 * 6.451624870300293
Epoch 240, val loss: 1.4458619356155396
Epoch 250, training loss: 14.230375289916992 = 1.3431291580200195 + 2.0 * 6.443623065948486
Epoch 250, val loss: 1.402300238609314
Epoch 260, training loss: 14.172049522399902 = 1.2899787425994873 + 2.0 * 6.441035270690918
Epoch 260, val loss: 1.3577253818511963
Epoch 270, training loss: 14.094658851623535 = 1.2368133068084717 + 2.0 * 6.428922653198242
Epoch 270, val loss: 1.3135181665420532
Epoch 280, training loss: 14.027976036071777 = 1.1837819814682007 + 2.0 * 6.422097206115723
Epoch 280, val loss: 1.2697333097457886
Epoch 290, training loss: 13.963122367858887 = 1.1312482357025146 + 2.0 * 6.4159369468688965
Epoch 290, val loss: 1.2265597581863403
Epoch 300, training loss: 13.900497436523438 = 1.0800522565841675 + 2.0 * 6.41022253036499
Epoch 300, val loss: 1.18462336063385
Epoch 310, training loss: 13.843671798706055 = 1.0310555696487427 + 2.0 * 6.406308174133301
Epoch 310, val loss: 1.1445584297180176
Epoch 320, training loss: 13.785371780395508 = 0.9841666221618652 + 2.0 * 6.400602340698242
Epoch 320, val loss: 1.106526494026184
Epoch 330, training loss: 13.729822158813477 = 0.939400315284729 + 2.0 * 6.3952107429504395
Epoch 330, val loss: 1.070473313331604
Epoch 340, training loss: 13.69540023803711 = 0.8969845771789551 + 2.0 * 6.399207592010498
Epoch 340, val loss: 1.0367096662521362
Epoch 350, training loss: 13.636956214904785 = 0.8574755787849426 + 2.0 * 6.389740467071533
Epoch 350, val loss: 1.0052517652511597
Epoch 360, training loss: 13.585159301757812 = 0.820391833782196 + 2.0 * 6.382383823394775
Epoch 360, val loss: 0.9761955142021179
Epoch 370, training loss: 13.547857284545898 = 0.7853302955627441 + 2.0 * 6.381263732910156
Epoch 370, val loss: 0.9493009448051453
Epoch 380, training loss: 13.512321472167969 = 0.752348005771637 + 2.0 * 6.379986763000488
Epoch 380, val loss: 0.9245467782020569
Epoch 390, training loss: 13.463890075683594 = 0.7210244536399841 + 2.0 * 6.371432781219482
Epoch 390, val loss: 0.9015065431594849
Epoch 400, training loss: 13.42818832397461 = 0.6910873055458069 + 2.0 * 6.3685503005981445
Epoch 400, val loss: 0.8801901340484619
Epoch 410, training loss: 13.393165588378906 = 0.6623808145523071 + 2.0 * 6.365392208099365
Epoch 410, val loss: 0.8602933287620544
Epoch 420, training loss: 13.360345840454102 = 0.6347989439964294 + 2.0 * 6.362773418426514
Epoch 420, val loss: 0.8418112397193909
Epoch 430, training loss: 13.32513427734375 = 0.6081258654594421 + 2.0 * 6.358504295349121
Epoch 430, val loss: 0.8245928883552551
Epoch 440, training loss: 13.292768478393555 = 0.5821375250816345 + 2.0 * 6.355315685272217
Epoch 440, val loss: 0.8085129857063293
Epoch 450, training loss: 13.297930717468262 = 0.5569410920143127 + 2.0 * 6.370494842529297
Epoch 450, val loss: 0.7935234308242798
Epoch 460, training loss: 13.235847473144531 = 0.532656729221344 + 2.0 * 6.351595401763916
Epoch 460, val loss: 0.7795830965042114
Epoch 470, training loss: 13.20765209197998 = 0.5092681050300598 + 2.0 * 6.349192142486572
Epoch 470, val loss: 0.7670812010765076
Epoch 480, training loss: 13.1763334274292 = 0.4866326153278351 + 2.0 * 6.344850540161133
Epoch 480, val loss: 0.7556337714195251
Epoch 490, training loss: 13.149784088134766 = 0.46468132734298706 + 2.0 * 6.342551231384277
Epoch 490, val loss: 0.7451706528663635
Epoch 500, training loss: 13.13305377960205 = 0.44341951608657837 + 2.0 * 6.344817161560059
Epoch 500, val loss: 0.735755205154419
Epoch 510, training loss: 13.111913681030273 = 0.42301639914512634 + 2.0 * 6.344448566436768
Epoch 510, val loss: 0.727258026599884
Epoch 520, training loss: 13.080256462097168 = 0.4034923315048218 + 2.0 * 6.338382244110107
Epoch 520, val loss: 0.7198547720909119
Epoch 530, training loss: 13.062609672546387 = 0.384765088558197 + 2.0 * 6.338922500610352
Epoch 530, val loss: 0.7134191989898682
Epoch 540, training loss: 13.032487869262695 = 0.3669079542160034 + 2.0 * 6.332789897918701
Epoch 540, val loss: 0.7078877687454224
Epoch 550, training loss: 13.012042045593262 = 0.3497568964958191 + 2.0 * 6.331142425537109
Epoch 550, val loss: 0.703266978263855
Epoch 560, training loss: 12.989663124084473 = 0.33330237865448 + 2.0 * 6.328180313110352
Epoch 560, val loss: 0.6994706988334656
Epoch 570, training loss: 12.979763984680176 = 0.3175775110721588 + 2.0 * 6.3310933113098145
Epoch 570, val loss: 0.6964293718338013
Epoch 580, training loss: 12.962102890014648 = 0.302564412355423 + 2.0 * 6.329769134521484
Epoch 580, val loss: 0.6941567659378052
Epoch 590, training loss: 12.935175895690918 = 0.2882910370826721 + 2.0 * 6.323442459106445
Epoch 590, val loss: 0.6925862431526184
Epoch 600, training loss: 12.918378829956055 = 0.2746700942516327 + 2.0 * 6.321854591369629
Epoch 600, val loss: 0.6917438507080078
Epoch 610, training loss: 12.901957511901855 = 0.26160281896591187 + 2.0 * 6.3201775550842285
Epoch 610, val loss: 0.6916073560714722
Epoch 620, training loss: 12.89639949798584 = 0.24910449981689453 + 2.0 * 6.323647499084473
Epoch 620, val loss: 0.6920550465583801
Epoch 630, training loss: 12.876450538635254 = 0.23723380267620087 + 2.0 * 6.319608211517334
Epoch 630, val loss: 0.6929973363876343
Epoch 640, training loss: 12.857583045959473 = 0.2258533239364624 + 2.0 * 6.3158650398254395
Epoch 640, val loss: 0.6945324540138245
Epoch 650, training loss: 12.845418930053711 = 0.21509188413619995 + 2.0 * 6.315163612365723
Epoch 650, val loss: 0.696468710899353
Epoch 660, training loss: 12.830952644348145 = 0.20480696856975555 + 2.0 * 6.313072681427002
Epoch 660, val loss: 0.6989312171936035
Epoch 670, training loss: 12.816475868225098 = 0.19499622285366058 + 2.0 * 6.310739994049072
Epoch 670, val loss: 0.7018169164657593
Epoch 680, training loss: 12.814814567565918 = 0.18562732636928558 + 2.0 * 6.31459379196167
Epoch 680, val loss: 0.7051084637641907
Epoch 690, training loss: 12.800642013549805 = 0.17675267159938812 + 2.0 * 6.311944484710693
Epoch 690, val loss: 0.7085281610488892
Epoch 700, training loss: 12.786526679992676 = 0.1683138906955719 + 2.0 * 6.309106349945068
Epoch 700, val loss: 0.7123919725418091
Epoch 710, training loss: 12.781930923461914 = 0.1603095829486847 + 2.0 * 6.310810565948486
Epoch 710, val loss: 0.7165101170539856
Epoch 720, training loss: 12.76281452178955 = 0.1527523398399353 + 2.0 * 6.3050312995910645
Epoch 720, val loss: 0.7207675576210022
Epoch 730, training loss: 12.754049301147461 = 0.14555831253528595 + 2.0 * 6.304245471954346
Epoch 730, val loss: 0.7253050208091736
Epoch 740, training loss: 12.743786811828613 = 0.13871212303638458 + 2.0 * 6.302537441253662
Epoch 740, val loss: 0.7300795912742615
Epoch 750, training loss: 12.740999221801758 = 0.13221794366836548 + 2.0 * 6.3043904304504395
Epoch 750, val loss: 0.7349885702133179
Epoch 760, training loss: 12.732545852661133 = 0.12608514726161957 + 2.0 * 6.303230285644531
Epoch 760, val loss: 0.7400440573692322
Epoch 770, training loss: 12.726703643798828 = 0.12027407437562943 + 2.0 * 6.3032145500183105
Epoch 770, val loss: 0.7452504634857178
Epoch 780, training loss: 12.710735321044922 = 0.11480117589235306 + 2.0 * 6.297966957092285
Epoch 780, val loss: 0.7504938244819641
Epoch 790, training loss: 12.70755672454834 = 0.1096094623208046 + 2.0 * 6.298973560333252
Epoch 790, val loss: 0.7558778524398804
Epoch 800, training loss: 12.704964637756348 = 0.10468538850545883 + 2.0 * 6.300139427185059
Epoch 800, val loss: 0.7614119648933411
Epoch 810, training loss: 12.695298194885254 = 0.10000623017549515 + 2.0 * 6.2976460456848145
Epoch 810, val loss: 0.7669383883476257
Epoch 820, training loss: 12.685881614685059 = 0.09561720490455627 + 2.0 * 6.295132160186768
Epoch 820, val loss: 0.7724729180335999
Epoch 830, training loss: 12.678860664367676 = 0.0914512351155281 + 2.0 * 6.293704509735107
Epoch 830, val loss: 0.7781367897987366
Epoch 840, training loss: 12.683258056640625 = 0.08751296997070312 + 2.0 * 6.297872543334961
Epoch 840, val loss: 0.783827543258667
Epoch 850, training loss: 12.675148010253906 = 0.0837676078081131 + 2.0 * 6.295690059661865
Epoch 850, val loss: 0.7895976901054382
Epoch 860, training loss: 12.660418510437012 = 0.0802529826760292 + 2.0 * 6.290082931518555
Epoch 860, val loss: 0.7952225804328918
Epoch 870, training loss: 12.655614852905273 = 0.07691541314125061 + 2.0 * 6.289349555969238
Epoch 870, val loss: 0.8009584546089172
Epoch 880, training loss: 12.664158821105957 = 0.07376344501972198 + 2.0 * 6.295197486877441
Epoch 880, val loss: 0.806668758392334
Epoch 890, training loss: 12.647668838500977 = 0.07075311988592148 + 2.0 * 6.288457870483398
Epoch 890, val loss: 0.8124641180038452
Epoch 900, training loss: 12.639785766601562 = 0.06791628152132034 + 2.0 * 6.285934925079346
Epoch 900, val loss: 0.8182035088539124
Epoch 910, training loss: 12.63794231414795 = 0.06522370129823685 + 2.0 * 6.2863593101501465
Epoch 910, val loss: 0.8240042328834534
Epoch 920, training loss: 12.653580665588379 = 0.06265554577112198 + 2.0 * 6.295462608337402
Epoch 920, val loss: 0.8297484517097473
Epoch 930, training loss: 12.630393981933594 = 0.06024773418903351 + 2.0 * 6.285073280334473
Epoch 930, val loss: 0.8353092074394226
Epoch 940, training loss: 12.626503944396973 = 0.05794385448098183 + 2.0 * 6.284279823303223
Epoch 940, val loss: 0.840959370136261
Epoch 950, training loss: 12.624375343322754 = 0.055768486112356186 + 2.0 * 6.284303665161133
Epoch 950, val loss: 0.8466507792472839
Epoch 960, training loss: 12.619592666625977 = 0.05369740352034569 + 2.0 * 6.282947540283203
Epoch 960, val loss: 0.8523080945014954
Epoch 970, training loss: 12.613840103149414 = 0.051727525889873505 + 2.0 * 6.2810564041137695
Epoch 970, val loss: 0.8578835725784302
Epoch 980, training loss: 12.609966278076172 = 0.04985390231013298 + 2.0 * 6.280055999755859
Epoch 980, val loss: 0.8635173439979553
Epoch 990, training loss: 12.61147689819336 = 0.04806565120816231 + 2.0 * 6.281705856323242
Epoch 990, val loss: 0.8691408038139343
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8185185185185185
0.8376383763837639
=== training gcn model ===
Epoch 0, training loss: 19.13015365600586 = 1.9363884925842285 + 2.0 * 8.596882820129395
Epoch 0, val loss: 1.9252680540084839
Epoch 10, training loss: 19.119773864746094 = 1.9264358282089233 + 2.0 * 8.59666919708252
Epoch 10, val loss: 1.9157724380493164
Epoch 20, training loss: 19.103309631347656 = 1.9141087532043457 + 2.0 * 8.594600677490234
Epoch 20, val loss: 1.903585433959961
Epoch 30, training loss: 19.052080154418945 = 1.897432565689087 + 2.0 * 8.577323913574219
Epoch 30, val loss: 1.8867541551589966
Epoch 40, training loss: 18.834135055541992 = 1.875821590423584 + 2.0 * 8.479156494140625
Epoch 40, val loss: 1.865647554397583
Epoch 50, training loss: 18.13685417175293 = 1.8517664670944214 + 2.0 * 8.14254379272461
Epoch 50, val loss: 1.843383550643921
Epoch 60, training loss: 17.804723739624023 = 1.829490065574646 + 2.0 * 7.987617015838623
Epoch 60, val loss: 1.8244526386260986
Epoch 70, training loss: 17.103248596191406 = 1.81101393699646 + 2.0 * 7.646117210388184
Epoch 70, val loss: 1.8087667226791382
Epoch 80, training loss: 16.293840408325195 = 1.7985306978225708 + 2.0 * 7.247654438018799
Epoch 80, val loss: 1.7981218099594116
Epoch 90, training loss: 15.774465560913086 = 1.786712646484375 + 2.0 * 6.9938764572143555
Epoch 90, val loss: 1.787339210510254
Epoch 100, training loss: 15.502330780029297 = 1.7716761827468872 + 2.0 * 6.86532735824585
Epoch 100, val loss: 1.7741754055023193
Epoch 110, training loss: 15.314950942993164 = 1.7564855813980103 + 2.0 * 6.779232501983643
Epoch 110, val loss: 1.761202335357666
Epoch 120, training loss: 15.178335189819336 = 1.7413697242736816 + 2.0 * 6.718482494354248
Epoch 120, val loss: 1.7483779191970825
Epoch 130, training loss: 15.072550773620605 = 1.7242833375930786 + 2.0 * 6.674133777618408
Epoch 130, val loss: 1.733793020248413
Epoch 140, training loss: 14.995048522949219 = 1.7050009965896606 + 2.0 * 6.645023822784424
Epoch 140, val loss: 1.717790961265564
Epoch 150, training loss: 14.921246528625488 = 1.683616042137146 + 2.0 * 6.6188154220581055
Epoch 150, val loss: 1.7001960277557373
Epoch 160, training loss: 14.85676383972168 = 1.6598385572433472 + 2.0 * 6.5984625816345215
Epoch 160, val loss: 1.681026816368103
Epoch 170, training loss: 14.795010566711426 = 1.633184552192688 + 2.0 * 6.580913066864014
Epoch 170, val loss: 1.6597403287887573
Epoch 180, training loss: 14.752697944641113 = 1.6033284664154053 + 2.0 * 6.5746846199035645
Epoch 180, val loss: 1.6361291408538818
Epoch 190, training loss: 14.675500869750977 = 1.5707684755325317 + 2.0 * 6.552366256713867
Epoch 190, val loss: 1.6105378866195679
Epoch 200, training loss: 14.603815078735352 = 1.5353567600250244 + 2.0 * 6.534229278564453
Epoch 200, val loss: 1.5829139947891235
Epoch 210, training loss: 14.537575721740723 = 1.496894121170044 + 2.0 * 6.520340919494629
Epoch 210, val loss: 1.5530962944030762
Epoch 220, training loss: 14.47173023223877 = 1.4553529024124146 + 2.0 * 6.508188724517822
Epoch 220, val loss: 1.5213165283203125
Epoch 230, training loss: 14.414422035217285 = 1.4113010168075562 + 2.0 * 6.501560688018799
Epoch 230, val loss: 1.4880481958389282
Epoch 240, training loss: 14.343202590942383 = 1.3659734725952148 + 2.0 * 6.488614559173584
Epoch 240, val loss: 1.4543246030807495
Epoch 250, training loss: 14.279114723205566 = 1.31991708278656 + 2.0 * 6.4795989990234375
Epoch 250, val loss: 1.420582890510559
Epoch 260, training loss: 14.217150688171387 = 1.2735350131988525 + 2.0 * 6.471807956695557
Epoch 260, val loss: 1.3871591091156006
Epoch 270, training loss: 14.15645694732666 = 1.2275745868682861 + 2.0 * 6.464441299438477
Epoch 270, val loss: 1.3546348810195923
Epoch 280, training loss: 14.104939460754395 = 1.1828395128250122 + 2.0 * 6.461050033569336
Epoch 280, val loss: 1.3237016201019287
Epoch 290, training loss: 14.045413970947266 = 1.1406887769699097 + 2.0 * 6.452362537384033
Epoch 290, val loss: 1.2951093912124634
Epoch 300, training loss: 13.992910385131836 = 1.1012468338012695 + 2.0 * 6.445831775665283
Epoch 300, val loss: 1.2689719200134277
Epoch 310, training loss: 13.942817687988281 = 1.0642173290252686 + 2.0 * 6.439300060272217
Epoch 310, val loss: 1.2449871301651
Epoch 320, training loss: 13.89875316619873 = 1.0295841693878174 + 2.0 * 6.434584617614746
Epoch 320, val loss: 1.2230561971664429
Epoch 330, training loss: 13.84949779510498 = 0.9973298907279968 + 2.0 * 6.426084041595459
Epoch 330, val loss: 1.203131914138794
Epoch 340, training loss: 13.80732536315918 = 0.9668174982070923 + 2.0 * 6.420253753662109
Epoch 340, val loss: 1.1847506761550903
Epoch 350, training loss: 13.771517753601074 = 0.9376140236854553 + 2.0 * 6.416951656341553
Epoch 350, val loss: 1.1675093173980713
Epoch 360, training loss: 13.73343276977539 = 0.9097614288330078 + 2.0 * 6.411835670471191
Epoch 360, val loss: 1.1512806415557861
Epoch 370, training loss: 13.69382381439209 = 0.8828459978103638 + 2.0 * 6.405488967895508
Epoch 370, val loss: 1.1360576152801514
Epoch 380, training loss: 13.65555191040039 = 0.8565341830253601 + 2.0 * 6.399508953094482
Epoch 380, val loss: 1.1214185953140259
Epoch 390, training loss: 13.623088836669922 = 0.8304888606071472 + 2.0 * 6.396299839019775
Epoch 390, val loss: 1.1071561574935913
Epoch 400, training loss: 13.589292526245117 = 0.8046241998672485 + 2.0 * 6.392333984375
Epoch 400, val loss: 1.09316885471344
Epoch 410, training loss: 13.555200576782227 = 0.7790303230285645 + 2.0 * 6.388084888458252
Epoch 410, val loss: 1.0795989036560059
Epoch 420, training loss: 13.520756721496582 = 0.753473162651062 + 2.0 * 6.383641719818115
Epoch 420, val loss: 1.0662199258804321
Epoch 430, training loss: 13.50063419342041 = 0.7279757261276245 + 2.0 * 6.386329174041748
Epoch 430, val loss: 1.0530107021331787
Epoch 440, training loss: 13.458623886108398 = 0.7027334570884705 + 2.0 * 6.377945423126221
Epoch 440, val loss: 1.0400639772415161
Epoch 450, training loss: 13.432123184204102 = 0.6776790022850037 + 2.0 * 6.377222061157227
Epoch 450, val loss: 1.0274494886398315
Epoch 460, training loss: 13.396306037902832 = 0.652907133102417 + 2.0 * 6.371699333190918
Epoch 460, val loss: 1.0151182413101196
Epoch 470, training loss: 13.36649227142334 = 0.6285715103149414 + 2.0 * 6.368960380554199
Epoch 470, val loss: 1.0031909942626953
Epoch 480, training loss: 13.335989952087402 = 0.6045379042625427 + 2.0 * 6.365725994110107
Epoch 480, val loss: 0.9916415810585022
Epoch 490, training loss: 13.325860023498535 = 0.5808659195899963 + 2.0 * 6.372497081756592
Epoch 490, val loss: 0.9805032014846802
Epoch 500, training loss: 13.28148078918457 = 0.5577863454818726 + 2.0 * 6.361847400665283
Epoch 500, val loss: 0.9697574377059937
Epoch 510, training loss: 13.27058219909668 = 0.5351914763450623 + 2.0 * 6.367695331573486
Epoch 510, val loss: 0.9595324397087097
Epoch 520, training loss: 13.232138633728027 = 0.5132614374160767 + 2.0 * 6.359438419342041
Epoch 520, val loss: 0.9498445987701416
Epoch 530, training loss: 13.202154159545898 = 0.49181994795799255 + 2.0 * 6.355166912078857
Epoch 530, val loss: 0.940700888633728
Epoch 540, training loss: 13.176294326782227 = 0.47081565856933594 + 2.0 * 6.352739334106445
Epoch 540, val loss: 0.9319924116134644
Epoch 550, training loss: 13.174680709838867 = 0.4502789378166199 + 2.0 * 6.362200736999512
Epoch 550, val loss: 0.9236667156219482
Epoch 560, training loss: 13.129340171813965 = 0.43019941449165344 + 2.0 * 6.349570274353027
Epoch 560, val loss: 0.9156265258789062
Epoch 570, training loss: 13.10491943359375 = 0.41072922945022583 + 2.0 * 6.347095012664795
Epoch 570, val loss: 0.9080302715301514
Epoch 580, training loss: 13.082009315490723 = 0.39168792963027954 + 2.0 * 6.345160484313965
Epoch 580, val loss: 0.9008105397224426
Epoch 590, training loss: 13.062984466552734 = 0.37307801842689514 + 2.0 * 6.3449530601501465
Epoch 590, val loss: 0.893976628780365
Epoch 600, training loss: 13.038891792297363 = 0.35498178005218506 + 2.0 * 6.341955184936523
Epoch 600, val loss: 0.8875173330307007
Epoch 610, training loss: 13.0224609375 = 0.33745381236076355 + 2.0 * 6.342503547668457
Epoch 610, val loss: 0.8815290927886963
Epoch 620, training loss: 12.998686790466309 = 0.3205137550830841 + 2.0 * 6.339086532592773
Epoch 620, val loss: 0.8760181665420532
Epoch 630, training loss: 12.992035865783691 = 0.30406439304351807 + 2.0 * 6.343985557556152
Epoch 630, val loss: 0.8709118366241455
Epoch 640, training loss: 12.96010971069336 = 0.2882311940193176 + 2.0 * 6.335939407348633
Epoch 640, val loss: 0.8662885427474976
Epoch 650, training loss: 12.942037582397461 = 0.2730158865451813 + 2.0 * 6.334510803222656
Epoch 650, val loss: 0.8622260689735413
Epoch 660, training loss: 12.928214073181152 = 0.2583676278591156 + 2.0 * 6.334923267364502
Epoch 660, val loss: 0.8585824370384216
Epoch 670, training loss: 12.91476821899414 = 0.24437762796878815 + 2.0 * 6.335195064544678
Epoch 670, val loss: 0.8553940653800964
Epoch 680, training loss: 12.890052795410156 = 0.2310868352651596 + 2.0 * 6.3294830322265625
Epoch 680, val loss: 0.8527630567550659
Epoch 690, training loss: 12.875382423400879 = 0.21845118701457977 + 2.0 * 6.328465461730957
Epoch 690, val loss: 0.8506160378456116
Epoch 700, training loss: 12.869441986083984 = 0.2064356654882431 + 2.0 * 6.331503391265869
Epoch 700, val loss: 0.8489599823951721
Epoch 710, training loss: 12.868045806884766 = 0.19505460560321808 + 2.0 * 6.336495399475098
Epoch 710, val loss: 0.8475144505500793
Epoch 720, training loss: 12.836854934692383 = 0.1843065321445465 + 2.0 * 6.326274394989014
Epoch 720, val loss: 0.8464531898498535
Epoch 730, training loss: 12.81961441040039 = 0.1742105484008789 + 2.0 * 6.322701930999756
Epoch 730, val loss: 0.8458182215690613
Epoch 740, training loss: 12.807979583740234 = 0.1646760255098343 + 2.0 * 6.321651935577393
Epoch 740, val loss: 0.845501184463501
Epoch 750, training loss: 12.812402725219727 = 0.1556960940361023 + 2.0 * 6.328353404998779
Epoch 750, val loss: 0.8455075621604919
Epoch 760, training loss: 12.791375160217285 = 0.14719752967357635 + 2.0 * 6.322088718414307
Epoch 760, val loss: 0.8456657528877258
Epoch 770, training loss: 12.777244567871094 = 0.13926543295383453 + 2.0 * 6.3189897537231445
Epoch 770, val loss: 0.8462831974029541
Epoch 780, training loss: 12.777719497680664 = 0.13179470598697662 + 2.0 * 6.322962284088135
Epoch 780, val loss: 0.8471286296844482
Epoch 790, training loss: 12.759352684020996 = 0.12478593736886978 + 2.0 * 6.3172831535339355
Epoch 790, val loss: 0.8482027053833008
Epoch 800, training loss: 12.750737190246582 = 0.11821623146533966 + 2.0 * 6.31626033782959
Epoch 800, val loss: 0.8496269583702087
Epoch 810, training loss: 12.742804527282715 = 0.11206589639186859 + 2.0 * 6.315369129180908
Epoch 810, val loss: 0.8512466549873352
Epoch 820, training loss: 12.729726791381836 = 0.10629356652498245 + 2.0 * 6.311716556549072
Epoch 820, val loss: 0.8531347513198853
Epoch 830, training loss: 12.723029136657715 = 0.10087986290454865 + 2.0 * 6.311074733734131
Epoch 830, val loss: 0.855276882648468
Epoch 840, training loss: 12.73173999786377 = 0.09579507261514664 + 2.0 * 6.317972660064697
Epoch 840, val loss: 0.8575987815856934
Epoch 850, training loss: 12.716852188110352 = 0.09104656428098679 + 2.0 * 6.312902927398682
Epoch 850, val loss: 0.8601945638656616
Epoch 860, training loss: 12.702632904052734 = 0.08659014105796814 + 2.0 * 6.308021545410156
Epoch 860, val loss: 0.8629725575447083
Epoch 870, training loss: 12.694635391235352 = 0.08241961151361465 + 2.0 * 6.306107997894287
Epoch 870, val loss: 0.8659484386444092
Epoch 880, training loss: 12.69229793548584 = 0.07849638909101486 + 2.0 * 6.306900978088379
Epoch 880, val loss: 0.8690774440765381
Epoch 890, training loss: 12.690378189086914 = 0.07480305433273315 + 2.0 * 6.3077874183654785
Epoch 890, val loss: 0.8722884058952332
Epoch 900, training loss: 12.682217597961426 = 0.0713425949215889 + 2.0 * 6.3054375648498535
Epoch 900, val loss: 0.8756625056266785
Epoch 910, training loss: 12.681294441223145 = 0.06809132546186447 + 2.0 * 6.306601524353027
Epoch 910, val loss: 0.879104495048523
Epoch 920, training loss: 12.669107437133789 = 0.06505254656076431 + 2.0 * 6.302027225494385
Epoch 920, val loss: 0.882762610912323
Epoch 930, training loss: 12.664227485656738 = 0.062188878655433655 + 2.0 * 6.301019191741943
Epoch 930, val loss: 0.8864988088607788
Epoch 940, training loss: 12.67908000946045 = 0.0595005564391613 + 2.0 * 6.309789657592773
Epoch 940, val loss: 0.8903363943099976
Epoch 950, training loss: 12.660672187805176 = 0.056944895535707474 + 2.0 * 6.301863670349121
Epoch 950, val loss: 0.8940263390541077
Epoch 960, training loss: 12.65182113647461 = 0.05455752834677696 + 2.0 * 6.29863166809082
Epoch 960, val loss: 0.8980709314346313
Epoch 970, training loss: 12.645947456359863 = 0.05230463296175003 + 2.0 * 6.296821594238281
Epoch 970, val loss: 0.9020522832870483
Epoch 980, training loss: 12.657814025878906 = 0.05017474293708801 + 2.0 * 6.30381965637207
Epoch 980, val loss: 0.9061284065246582
Epoch 990, training loss: 12.642520904541016 = 0.04816211760044098 + 2.0 * 6.297179222106934
Epoch 990, val loss: 0.9101605415344238
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7851851851851852
0.8371112282551397
=== training gcn model ===
Epoch 0, training loss: 19.130882263183594 = 1.9371252059936523 + 2.0 * 8.596879005432129
Epoch 0, val loss: 1.9400385618209839
Epoch 10, training loss: 19.121007919311523 = 1.927622675895691 + 2.0 * 8.59669303894043
Epoch 10, val loss: 1.9301692247390747
Epoch 20, training loss: 19.105457305908203 = 1.915316104888916 + 2.0 * 8.595070838928223
Epoch 20, val loss: 1.91726815700531
Epoch 30, training loss: 19.059968948364258 = 1.8979418277740479 + 2.0 * 8.581013679504395
Epoch 30, val loss: 1.8991613388061523
Epoch 40, training loss: 18.866945266723633 = 1.8745505809783936 + 2.0 * 8.496197700500488
Epoch 40, val loss: 1.8754630088806152
Epoch 50, training loss: 18.08894157409668 = 1.848399043083191 + 2.0 * 8.120271682739258
Epoch 50, val loss: 1.8502168655395508
Epoch 60, training loss: 17.497804641723633 = 1.8253886699676514 + 2.0 * 7.836208343505859
Epoch 60, val loss: 1.8296332359313965
Epoch 70, training loss: 16.488454818725586 = 1.809287190437317 + 2.0 * 7.339583873748779
Epoch 70, val loss: 1.8152085542678833
Epoch 80, training loss: 15.826473236083984 = 1.7980365753173828 + 2.0 * 7.014218330383301
Epoch 80, val loss: 1.804667353630066
Epoch 90, training loss: 15.595601081848145 = 1.7841920852661133 + 2.0 * 6.905704498291016
Epoch 90, val loss: 1.7914892435073853
Epoch 100, training loss: 15.423029899597168 = 1.7664270401000977 + 2.0 * 6.828301429748535
Epoch 100, val loss: 1.7757049798965454
Epoch 110, training loss: 15.294682502746582 = 1.7491719722747803 + 2.0 * 6.772755146026611
Epoch 110, val loss: 1.7606277465820312
Epoch 120, training loss: 15.179563522338867 = 1.7327810525894165 + 2.0 * 6.723391056060791
Epoch 120, val loss: 1.7462327480316162
Epoch 130, training loss: 15.072874069213867 = 1.7158969640731812 + 2.0 * 6.678488731384277
Epoch 130, val loss: 1.7313446998596191
Epoch 140, training loss: 14.982321739196777 = 1.697310447692871 + 2.0 * 6.642505645751953
Epoch 140, val loss: 1.7152270078659058
Epoch 150, training loss: 14.888265609741211 = 1.6765954494476318 + 2.0 * 6.6058349609375
Epoch 150, val loss: 1.69777512550354
Epoch 160, training loss: 14.809823989868164 = 1.6531327962875366 + 2.0 * 6.578345775604248
Epoch 160, val loss: 1.6783338785171509
Epoch 170, training loss: 14.743206024169922 = 1.6264294385910034 + 2.0 * 6.5583882331848145
Epoch 170, val loss: 1.6563491821289062
Epoch 180, training loss: 14.67860221862793 = 1.59630286693573 + 2.0 * 6.541149616241455
Epoch 180, val loss: 1.6316626071929932
Epoch 190, training loss: 14.616249084472656 = 1.5626413822174072 + 2.0 * 6.526803970336914
Epoch 190, val loss: 1.604185700416565
Epoch 200, training loss: 14.5552396774292 = 1.525553584098816 + 2.0 * 6.514842987060547
Epoch 200, val loss: 1.574162244796753
Epoch 210, training loss: 14.48961067199707 = 1.485490322113037 + 2.0 * 6.5020599365234375
Epoch 210, val loss: 1.54209566116333
Epoch 220, training loss: 14.421404838562012 = 1.442832112312317 + 2.0 * 6.489286422729492
Epoch 220, val loss: 1.5082533359527588
Epoch 230, training loss: 14.365306854248047 = 1.3977408409118652 + 2.0 * 6.483782768249512
Epoch 230, val loss: 1.4727799892425537
Epoch 240, training loss: 14.290139198303223 = 1.3513363599777222 + 2.0 * 6.4694013595581055
Epoch 240, val loss: 1.4365473985671997
Epoch 250, training loss: 14.220682144165039 = 1.304046392440796 + 2.0 * 6.458317756652832
Epoch 250, val loss: 1.3998366594314575
Epoch 260, training loss: 14.154854774475098 = 1.256183385848999 + 2.0 * 6.44933557510376
Epoch 260, val loss: 1.3630367517471313
Epoch 270, training loss: 14.110740661621094 = 1.208495020866394 + 2.0 * 6.451122760772705
Epoch 270, val loss: 1.3267133235931396
Epoch 280, training loss: 14.03476619720459 = 1.1623190641403198 + 2.0 * 6.43622350692749
Epoch 280, val loss: 1.2920267581939697
Epoch 290, training loss: 13.976481437683105 = 1.1178743839263916 + 2.0 * 6.4293036460876465
Epoch 290, val loss: 1.2590974569320679
Epoch 300, training loss: 13.919448852539062 = 1.0748668909072876 + 2.0 * 6.422290802001953
Epoch 300, val loss: 1.2277928590774536
Epoch 310, training loss: 13.869279861450195 = 1.033470869064331 + 2.0 * 6.417904376983643
Epoch 310, val loss: 1.1980981826782227
Epoch 320, training loss: 13.82663631439209 = 0.993779718875885 + 2.0 * 6.416428089141846
Epoch 320, val loss: 1.170218825340271
Epoch 330, training loss: 13.7687406539917 = 0.9559581279754639 + 2.0 * 6.406391143798828
Epoch 330, val loss: 1.1441396474838257
Epoch 340, training loss: 13.719808578491211 = 0.9195732474327087 + 2.0 * 6.400117874145508
Epoch 340, val loss: 1.1195639371871948
Epoch 350, training loss: 13.674137115478516 = 0.8843991160392761 + 2.0 * 6.394868850708008
Epoch 350, val loss: 1.0963294506072998
Epoch 360, training loss: 13.635113716125488 = 0.8503268361091614 + 2.0 * 6.392393589019775
Epoch 360, val loss: 1.0743507146835327
Epoch 370, training loss: 13.595561027526855 = 0.8175505995750427 + 2.0 * 6.389005184173584
Epoch 370, val loss: 1.0537551641464233
Epoch 380, training loss: 13.554469108581543 = 0.7862682938575745 + 2.0 * 6.384100437164307
Epoch 380, val loss: 1.0346630811691284
Epoch 390, training loss: 13.516570091247559 = 0.7565276622772217 + 2.0 * 6.380021095275879
Epoch 390, val loss: 1.0171189308166504
Epoch 400, training loss: 13.477705001831055 = 0.7280367016792297 + 2.0 * 6.374834060668945
Epoch 400, val loss: 1.0009727478027344
Epoch 410, training loss: 13.443436622619629 = 0.700629472732544 + 2.0 * 6.371403694152832
Epoch 410, val loss: 0.9860968589782715
Epoch 420, training loss: 13.422014236450195 = 0.6742927432060242 + 2.0 * 6.373860836029053
Epoch 420, val loss: 0.972405195236206
Epoch 430, training loss: 13.383772850036621 = 0.6490715146064758 + 2.0 * 6.3673505783081055
Epoch 430, val loss: 0.9599884152412415
Epoch 440, training loss: 13.350396156311035 = 0.6248294711112976 + 2.0 * 6.362783432006836
Epoch 440, val loss: 0.948693037033081
Epoch 450, training loss: 13.320320129394531 = 0.6013393998146057 + 2.0 * 6.359490394592285
Epoch 450, val loss: 0.9383131265640259
Epoch 460, training loss: 13.295735359191895 = 0.5785939693450928 + 2.0 * 6.358570575714111
Epoch 460, val loss: 0.9287746548652649
Epoch 470, training loss: 13.282423973083496 = 0.556676983833313 + 2.0 * 6.362873554229736
Epoch 470, val loss: 0.9201237559318542
Epoch 480, training loss: 13.242330551147461 = 0.535504162311554 + 2.0 * 6.353413105010986
Epoch 480, val loss: 0.912257194519043
Epoch 490, training loss: 13.214107513427734 = 0.5150143504142761 + 2.0 * 6.349546432495117
Epoch 490, val loss: 0.9051086902618408
Epoch 500, training loss: 13.187545776367188 = 0.495049387216568 + 2.0 * 6.346248149871826
Epoch 500, val loss: 0.8986209034919739
Epoch 510, training loss: 13.168173789978027 = 0.4755789339542389 + 2.0 * 6.346297264099121
Epoch 510, val loss: 0.8927753567695618
Epoch 520, training loss: 13.16855525970459 = 0.4566400349140167 + 2.0 * 6.355957508087158
Epoch 520, val loss: 0.8874553442001343
Epoch 530, training loss: 13.122785568237305 = 0.4382232427597046 + 2.0 * 6.342281341552734
Epoch 530, val loss: 0.8826379179954529
Epoch 540, training loss: 13.097818374633789 = 0.42032334208488464 + 2.0 * 6.338747501373291
Epoch 540, val loss: 0.8784139752388
Epoch 550, training loss: 13.074220657348633 = 0.402799129486084 + 2.0 * 6.3357110023498535
Epoch 550, val loss: 0.8746731877326965
Epoch 560, training loss: 13.05237102508545 = 0.3856097459793091 + 2.0 * 6.333380699157715
Epoch 560, val loss: 0.8715168237686157
Epoch 570, training loss: 13.038566589355469 = 0.36879369616508484 + 2.0 * 6.33488655090332
Epoch 570, val loss: 0.8687005639076233
Epoch 580, training loss: 13.013373374938965 = 0.3524630069732666 + 2.0 * 6.330455303192139
Epoch 580, val loss: 0.866497278213501
Epoch 590, training loss: 12.992652893066406 = 0.33663514256477356 + 2.0 * 6.328008651733398
Epoch 590, val loss: 0.8648930191993713
Epoch 600, training loss: 12.972790718078613 = 0.32123488187789917 + 2.0 * 6.325778007507324
Epoch 600, val loss: 0.8638231754302979
Epoch 610, training loss: 12.958671569824219 = 0.3062739074230194 + 2.0 * 6.326199054718018
Epoch 610, val loss: 0.8633594512939453
Epoch 620, training loss: 12.954078674316406 = 0.29184216260910034 + 2.0 * 6.331118106842041
Epoch 620, val loss: 0.863209068775177
Epoch 630, training loss: 12.925644874572754 = 0.27799174189567566 + 2.0 * 6.323826789855957
Epoch 630, val loss: 0.8637580871582031
Epoch 640, training loss: 12.906113624572754 = 0.2646695375442505 + 2.0 * 6.3207221031188965
Epoch 640, val loss: 0.8648276329040527
Epoch 650, training loss: 12.902278900146484 = 0.25184908509254456 + 2.0 * 6.325214862823486
Epoch 650, val loss: 0.8663030862808228
Epoch 660, training loss: 12.872961044311523 = 0.23952308297157288 + 2.0 * 6.316719055175781
Epoch 660, val loss: 0.8683202862739563
Epoch 670, training loss: 12.857608795166016 = 0.2277451604604721 + 2.0 * 6.314931869506836
Epoch 670, val loss: 0.8708219528198242
Epoch 680, training loss: 12.843145370483398 = 0.21645858883857727 + 2.0 * 6.313343524932861
Epoch 680, val loss: 0.8737471699714661
Epoch 690, training loss: 12.830913543701172 = 0.20565658807754517 + 2.0 * 6.312628269195557
Epoch 690, val loss: 0.8770575523376465
Epoch 700, training loss: 12.830953598022461 = 0.19534564018249512 + 2.0 * 6.317803859710693
Epoch 700, val loss: 0.8806239366531372
Epoch 710, training loss: 12.814062118530273 = 0.18549077212810516 + 2.0 * 6.314285755157471
Epoch 710, val loss: 0.8845640420913696
Epoch 720, training loss: 12.798004150390625 = 0.1761896312236786 + 2.0 * 6.310907363891602
Epoch 720, val loss: 0.8887996077537537
Epoch 730, training loss: 12.788147926330566 = 0.16733719408512115 + 2.0 * 6.310405254364014
Epoch 730, val loss: 0.8932785987854004
Epoch 740, training loss: 12.773097038269043 = 0.15893608331680298 + 2.0 * 6.307080268859863
Epoch 740, val loss: 0.8981070518493652
Epoch 750, training loss: 12.76094913482666 = 0.1509738266468048 + 2.0 * 6.30498743057251
Epoch 750, val loss: 0.9031455516815186
Epoch 760, training loss: 12.75777530670166 = 0.14341019093990326 + 2.0 * 6.307182788848877
Epoch 760, val loss: 0.9084838628768921
Epoch 770, training loss: 12.754066467285156 = 0.13624322414398193 + 2.0 * 6.3089118003845215
Epoch 770, val loss: 0.9136877059936523
Epoch 780, training loss: 12.734051704406738 = 0.12949402630329132 + 2.0 * 6.302278995513916
Epoch 780, val loss: 0.9192912578582764
Epoch 790, training loss: 12.724225997924805 = 0.12312514334917068 + 2.0 * 6.30055046081543
Epoch 790, val loss: 0.9250760078430176
Epoch 800, training loss: 12.7227144241333 = 0.11709290742874146 + 2.0 * 6.3028106689453125
Epoch 800, val loss: 0.9310133457183838
Epoch 810, training loss: 12.713343620300293 = 0.11141671240329742 + 2.0 * 6.300963401794434
Epoch 810, val loss: 0.936898946762085
Epoch 820, training loss: 12.70265007019043 = 0.10606750845909119 + 2.0 * 6.298291206359863
Epoch 820, val loss: 0.9429534077644348
Epoch 830, training loss: 12.694501876831055 = 0.10103242099285126 + 2.0 * 6.296734809875488
Epoch 830, val loss: 0.9491292238235474
Epoch 840, training loss: 12.689204216003418 = 0.0962822288274765 + 2.0 * 6.29646110534668
Epoch 840, val loss: 0.9553889632225037
Epoch 850, training loss: 12.69063663482666 = 0.09180046617984772 + 2.0 * 6.299417972564697
Epoch 850, val loss: 0.9616219997406006
Epoch 860, training loss: 12.677556037902832 = 0.08760271221399307 + 2.0 * 6.294976711273193
Epoch 860, val loss: 0.9679189324378967
Epoch 870, training loss: 12.673035621643066 = 0.08367276191711426 + 2.0 * 6.294681549072266
Epoch 870, val loss: 0.9740697741508484
Epoch 880, training loss: 12.676956176757812 = 0.07997123897075653 + 2.0 * 6.298492431640625
Epoch 880, val loss: 0.9803457856178284
Epoch 890, training loss: 12.661664962768555 = 0.07648767530918121 + 2.0 * 6.292588710784912
Epoch 890, val loss: 0.986577570438385
Epoch 900, training loss: 12.654963493347168 = 0.07320506125688553 + 2.0 * 6.290879249572754
Epoch 900, val loss: 0.9929095506668091
Epoch 910, training loss: 12.653687477111816 = 0.07011035829782486 + 2.0 * 6.291788578033447
Epoch 910, val loss: 0.9991423487663269
Epoch 920, training loss: 12.645173072814941 = 0.06718926131725311 + 2.0 * 6.288991928100586
Epoch 920, val loss: 1.0052669048309326
Epoch 930, training loss: 12.645111083984375 = 0.06444451957941055 + 2.0 * 6.290333271026611
Epoch 930, val loss: 1.0113134384155273
Epoch 940, training loss: 12.6363525390625 = 0.06185542047023773 + 2.0 * 6.287248611450195
Epoch 940, val loss: 1.01746666431427
Epoch 950, training loss: 12.631224632263184 = 0.059404969215393066 + 2.0 * 6.285909652709961
Epoch 950, val loss: 1.0235486030578613
Epoch 960, training loss: 12.632991790771484 = 0.05708004906773567 + 2.0 * 6.2879557609558105
Epoch 960, val loss: 1.0296601057052612
Epoch 970, training loss: 12.62893295288086 = 0.05488480255007744 + 2.0 * 6.287024021148682
Epoch 970, val loss: 1.0354337692260742
Epoch 980, training loss: 12.622774124145508 = 0.05280827730894089 + 2.0 * 6.284983158111572
Epoch 980, val loss: 1.0412676334381104
Epoch 990, training loss: 12.618890762329102 = 0.05085105076432228 + 2.0 * 6.284019947052002
Epoch 990, val loss: 1.0471125841140747
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7777777777777778
0.8413284132841329
The final CL Acc:0.79383, 0.01772, The final GNN Acc:0.83869, 0.00188
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.3 der2:0.3 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11588])
remove edge: torch.Size([2, 9480])
updated graph: torch.Size([2, 10512])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 19.152490615844727 = 1.9587980508804321 + 2.0 * 8.596846580505371
Epoch 0, val loss: 1.9587734937667847
Epoch 10, training loss: 19.14065933227539 = 1.9474725723266602 + 2.0 * 8.596592903137207
Epoch 10, val loss: 1.947877049446106
Epoch 20, training loss: 19.122350692749023 = 1.9335063695907593 + 2.0 * 8.594422340393066
Epoch 20, val loss: 1.9340752363204956
Epoch 30, training loss: 19.066829681396484 = 1.9143093824386597 + 2.0 * 8.576260566711426
Epoch 30, val loss: 1.9148554801940918
Epoch 40, training loss: 18.825355529785156 = 1.89078950881958 + 2.0 * 8.467283248901367
Epoch 40, val loss: 1.8924866914749146
Epoch 50, training loss: 18.137727737426758 = 1.8674012422561646 + 2.0 * 8.135163307189941
Epoch 50, val loss: 1.8710888624191284
Epoch 60, training loss: 17.389549255371094 = 1.8503862619400024 + 2.0 * 7.7695817947387695
Epoch 60, val loss: 1.8564722537994385
Epoch 70, training loss: 16.308197021484375 = 1.8389850854873657 + 2.0 * 7.23460578918457
Epoch 70, val loss: 1.8461065292358398
Epoch 80, training loss: 15.777963638305664 = 1.830239176750183 + 2.0 * 6.973862171173096
Epoch 80, val loss: 1.8374797105789185
Epoch 90, training loss: 15.535865783691406 = 1.8193318843841553 + 2.0 * 6.858266830444336
Epoch 90, val loss: 1.8271844387054443
Epoch 100, training loss: 15.360551834106445 = 1.8069671392440796 + 2.0 * 6.776792526245117
Epoch 100, val loss: 1.8163642883300781
Epoch 110, training loss: 15.216259002685547 = 1.7954384088516235 + 2.0 * 6.710410118103027
Epoch 110, val loss: 1.8065383434295654
Epoch 120, training loss: 15.107892036437988 = 1.7848484516143799 + 2.0 * 6.661521911621094
Epoch 120, val loss: 1.7971856594085693
Epoch 130, training loss: 15.02274227142334 = 1.774213433265686 + 2.0 * 6.624264240264893
Epoch 130, val loss: 1.7874988317489624
Epoch 140, training loss: 14.954789161682129 = 1.7628740072250366 + 2.0 * 6.5959577560424805
Epoch 140, val loss: 1.777305245399475
Epoch 150, training loss: 14.904373168945312 = 1.7505731582641602 + 2.0 * 6.576900005340576
Epoch 150, val loss: 1.7665654420852661
Epoch 160, training loss: 14.84001636505127 = 1.737223505973816 + 2.0 * 6.551396369934082
Epoch 160, val loss: 1.755153775215149
Epoch 170, training loss: 14.788930892944336 = 1.722625732421875 + 2.0 * 6.5331525802612305
Epoch 170, val loss: 1.7427732944488525
Epoch 180, training loss: 14.737900733947754 = 1.7063426971435547 + 2.0 * 6.5157790184021
Epoch 180, val loss: 1.729017972946167
Epoch 190, training loss: 14.691516876220703 = 1.688059687614441 + 2.0 * 6.501728534698486
Epoch 190, val loss: 1.7136973142623901
Epoch 200, training loss: 14.646570205688477 = 1.6675721406936646 + 2.0 * 6.489499092102051
Epoch 200, val loss: 1.696516752243042
Epoch 210, training loss: 14.592494010925293 = 1.6446455717086792 + 2.0 * 6.473924160003662
Epoch 210, val loss: 1.6773138046264648
Epoch 220, training loss: 14.543583869934082 = 1.6189861297607422 + 2.0 * 6.46229887008667
Epoch 220, val loss: 1.6557337045669556
Epoch 230, training loss: 14.494973182678223 = 1.590347409248352 + 2.0 * 6.45231294631958
Epoch 230, val loss: 1.6316993236541748
Epoch 240, training loss: 14.444328308105469 = 1.5587639808654785 + 2.0 * 6.442782402038574
Epoch 240, val loss: 1.6051703691482544
Epoch 250, training loss: 14.391460418701172 = 1.524162769317627 + 2.0 * 6.433649063110352
Epoch 250, val loss: 1.5762237310409546
Epoch 260, training loss: 14.352840423583984 = 1.486719012260437 + 2.0 * 6.433060646057129
Epoch 260, val loss: 1.545172929763794
Epoch 270, training loss: 14.291145324707031 = 1.4476674795150757 + 2.0 * 6.421739101409912
Epoch 270, val loss: 1.5131573677062988
Epoch 280, training loss: 14.232808113098145 = 1.4072540998458862 + 2.0 * 6.412776947021484
Epoch 280, val loss: 1.4803788661956787
Epoch 290, training loss: 14.1793212890625 = 1.3658086061477661 + 2.0 * 6.406756401062012
Epoch 290, val loss: 1.4470666646957397
Epoch 300, training loss: 14.131169319152832 = 1.323888897895813 + 2.0 * 6.403640270233154
Epoch 300, val loss: 1.4137685298919678
Epoch 310, training loss: 14.074562072753906 = 1.282110333442688 + 2.0 * 6.396225929260254
Epoch 310, val loss: 1.3809934854507446
Epoch 320, training loss: 14.022466659545898 = 1.2406954765319824 + 2.0 * 6.390885829925537
Epoch 320, val loss: 1.3489197492599487
Epoch 330, training loss: 13.985002517700195 = 1.1999478340148926 + 2.0 * 6.3925275802612305
Epoch 330, val loss: 1.3178024291992188
Epoch 340, training loss: 13.931495666503906 = 1.1603597402572632 + 2.0 * 6.385568141937256
Epoch 340, val loss: 1.2880172729492188
Epoch 350, training loss: 13.879730224609375 = 1.121719241142273 + 2.0 * 6.379005432128906
Epoch 350, val loss: 1.2592644691467285
Epoch 360, training loss: 13.835325241088867 = 1.0837509632110596 + 2.0 * 6.375787258148193
Epoch 360, val loss: 1.2313920259475708
Epoch 370, training loss: 13.793115615844727 = 1.0465829372406006 + 2.0 * 6.373266220092773
Epoch 370, val loss: 1.2043272256851196
Epoch 380, training loss: 13.745657920837402 = 1.0103250741958618 + 2.0 * 6.367666244506836
Epoch 380, val loss: 1.1783310174942017
Epoch 390, training loss: 13.705487251281738 = 0.974784791469574 + 2.0 * 6.36535120010376
Epoch 390, val loss: 1.1531769037246704
Epoch 400, training loss: 13.6690092086792 = 0.9400205612182617 + 2.0 * 6.364494323730469
Epoch 400, val loss: 1.1289094686508179
Epoch 410, training loss: 13.624473571777344 = 0.9061936736106873 + 2.0 * 6.359139919281006
Epoch 410, val loss: 1.1055649518966675
Epoch 420, training loss: 13.586575508117676 = 0.8731974363327026 + 2.0 * 6.356688976287842
Epoch 420, val loss: 1.0831457376480103
Epoch 430, training loss: 13.548539161682129 = 0.8410661220550537 + 2.0 * 6.353736400604248
Epoch 430, val loss: 1.061761736869812
Epoch 440, training loss: 13.514571189880371 = 0.8097845911979675 + 2.0 * 6.35239315032959
Epoch 440, val loss: 1.0413044691085815
Epoch 450, training loss: 13.49867057800293 = 0.7796531915664673 + 2.0 * 6.359508514404297
Epoch 450, val loss: 1.022019863128662
Epoch 460, training loss: 13.448996543884277 = 0.7510555386543274 + 2.0 * 6.348970413208008
Epoch 460, val loss: 1.0039597749710083
Epoch 470, training loss: 13.415899276733398 = 0.7236087322235107 + 2.0 * 6.346145153045654
Epoch 470, val loss: 0.9873677492141724
Epoch 480, training loss: 13.38355827331543 = 0.697147786617279 + 2.0 * 6.343205451965332
Epoch 480, val loss: 0.9717296361923218
Epoch 490, training loss: 13.35377025604248 = 0.6716033220291138 + 2.0 * 6.341083526611328
Epoch 490, val loss: 0.9570232033729553
Epoch 500, training loss: 13.34265422821045 = 0.6468936800956726 + 2.0 * 6.3478803634643555
Epoch 500, val loss: 0.9432067275047302
Epoch 510, training loss: 13.297802925109863 = 0.6232682466506958 + 2.0 * 6.3372673988342285
Epoch 510, val loss: 0.9303246736526489
Epoch 520, training loss: 13.273126602172852 = 0.6005251407623291 + 2.0 * 6.336300849914551
Epoch 520, val loss: 0.9183301329612732
Epoch 530, training loss: 13.248781204223633 = 0.5784962773323059 + 2.0 * 6.335142612457275
Epoch 530, val loss: 0.9070496559143066
Epoch 540, training loss: 13.223210334777832 = 0.5572298765182495 + 2.0 * 6.3329901695251465
Epoch 540, val loss: 0.8965392112731934
Epoch 550, training loss: 13.200396537780762 = 0.5367293953895569 + 2.0 * 6.331833362579346
Epoch 550, val loss: 0.8866968750953674
Epoch 560, training loss: 13.173763275146484 = 0.5168355107307434 + 2.0 * 6.328464031219482
Epoch 560, val loss: 0.8775128722190857
Epoch 570, training loss: 13.16058349609375 = 0.49749624729156494 + 2.0 * 6.331543445587158
Epoch 570, val loss: 0.8689074516296387
Epoch 580, training loss: 13.139016151428223 = 0.4786646068096161 + 2.0 * 6.330175876617432
Epoch 580, val loss: 0.8607712984085083
Epoch 590, training loss: 13.111401557922363 = 0.4604383111000061 + 2.0 * 6.325481414794922
Epoch 590, val loss: 0.8530908226966858
Epoch 600, training loss: 13.087749481201172 = 0.44264885783195496 + 2.0 * 6.322550296783447
Epoch 600, val loss: 0.8459369540214539
Epoch 610, training loss: 13.07625961303711 = 0.42523491382598877 + 2.0 * 6.325512409210205
Epoch 610, val loss: 0.8391347527503967
Epoch 620, training loss: 13.061074256896973 = 0.408265620470047 + 2.0 * 6.326404094696045
Epoch 620, val loss: 0.8329274654388428
Epoch 630, training loss: 13.040833473205566 = 0.3917628526687622 + 2.0 * 6.324535369873047
Epoch 630, val loss: 0.8270903825759888
Epoch 640, training loss: 13.011483192443848 = 0.3757660984992981 + 2.0 * 6.317858695983887
Epoch 640, val loss: 0.8215664625167847
Epoch 650, training loss: 12.992172241210938 = 0.36016973853111267 + 2.0 * 6.3160014152526855
Epoch 650, val loss: 0.81646728515625
Epoch 660, training loss: 12.973196029663086 = 0.34489426016807556 + 2.0 * 6.314150810241699
Epoch 660, val loss: 0.8118481040000916
Epoch 670, training loss: 12.956727027893066 = 0.32995739579200745 + 2.0 * 6.313385009765625
Epoch 670, val loss: 0.8075634837150574
Epoch 680, training loss: 12.956748008728027 = 0.31538519263267517 + 2.0 * 6.320681571960449
Epoch 680, val loss: 0.8035745620727539
Epoch 690, training loss: 12.925153732299805 = 0.30122146010398865 + 2.0 * 6.3119659423828125
Epoch 690, val loss: 0.8000015616416931
Epoch 700, training loss: 12.90717601776123 = 0.28751397132873535 + 2.0 * 6.309831142425537
Epoch 700, val loss: 0.7968738079071045
Epoch 710, training loss: 12.89058780670166 = 0.27416494488716125 + 2.0 * 6.308211326599121
Epoch 710, val loss: 0.7941470742225647
Epoch 720, training loss: 12.880329132080078 = 0.2611628472805023 + 2.0 * 6.3095831871032715
Epoch 720, val loss: 0.7918440699577332
Epoch 730, training loss: 12.859820365905762 = 0.2485111504793167 + 2.0 * 6.305654525756836
Epoch 730, val loss: 0.7898624539375305
Epoch 740, training loss: 12.845829010009766 = 0.23630160093307495 + 2.0 * 6.3047637939453125
Epoch 740, val loss: 0.788321316242218
Epoch 750, training loss: 12.845282554626465 = 0.2245064228773117 + 2.0 * 6.310388088226318
Epoch 750, val loss: 0.7872117757797241
Epoch 760, training loss: 12.82004451751709 = 0.21323561668395996 + 2.0 * 6.303404331207275
Epoch 760, val loss: 0.7864593267440796
Epoch 770, training loss: 12.80676555633545 = 0.2024078369140625 + 2.0 * 6.302178859710693
Epoch 770, val loss: 0.7862144708633423
Epoch 780, training loss: 12.793051719665527 = 0.1920517534017563 + 2.0 * 6.30049991607666
Epoch 780, val loss: 0.7864981293678284
Epoch 790, training loss: 12.792791366577148 = 0.1821441948413849 + 2.0 * 6.305323600769043
Epoch 790, val loss: 0.7872330546379089
Epoch 800, training loss: 12.79391860961914 = 0.17281019687652588 + 2.0 * 6.310554027557373
Epoch 800, val loss: 0.7884035110473633
Epoch 810, training loss: 12.763764381408691 = 0.16400501132011414 + 2.0 * 6.299879550933838
Epoch 810, val loss: 0.7899746894836426
Epoch 820, training loss: 12.748591423034668 = 0.15571866929531097 + 2.0 * 6.296436309814453
Epoch 820, val loss: 0.792011022567749
Epoch 830, training loss: 12.738181114196777 = 0.14789563417434692 + 2.0 * 6.295142650604248
Epoch 830, val loss: 0.794498860836029
Epoch 840, training loss: 12.730255126953125 = 0.1405184268951416 + 2.0 * 6.294868469238281
Epoch 840, val loss: 0.7973417639732361
Epoch 850, training loss: 12.737101554870605 = 0.13357770442962646 + 2.0 * 6.301762104034424
Epoch 850, val loss: 0.8004350662231445
Epoch 860, training loss: 12.712507247924805 = 0.12705597281455994 + 2.0 * 6.292725563049316
Epoch 860, val loss: 0.8038398623466492
Epoch 870, training loss: 12.706157684326172 = 0.12094821035861969 + 2.0 * 6.292604923248291
Epoch 870, val loss: 0.807643473148346
Epoch 880, training loss: 12.697429656982422 = 0.11520709842443466 + 2.0 * 6.291111469268799
Epoch 880, val loss: 0.8116983771324158
Epoch 890, training loss: 12.698902130126953 = 0.10978269577026367 + 2.0 * 6.294559478759766
Epoch 890, val loss: 0.8159939050674438
Epoch 900, training loss: 12.692432403564453 = 0.10470713675022125 + 2.0 * 6.293862819671631
Epoch 900, val loss: 0.820379376411438
Epoch 910, training loss: 12.681159973144531 = 0.09993050247430801 + 2.0 * 6.290614604949951
Epoch 910, val loss: 0.8251271843910217
Epoch 920, training loss: 12.67371654510498 = 0.09544781595468521 + 2.0 * 6.289134502410889
Epoch 920, val loss: 0.8299031853675842
Epoch 930, training loss: 12.666217803955078 = 0.09121016412973404 + 2.0 * 6.287503719329834
Epoch 930, val loss: 0.8348309993743896
Epoch 940, training loss: 12.658459663391113 = 0.08721721172332764 + 2.0 * 6.285621166229248
Epoch 940, val loss: 0.8399214744567871
Epoch 950, training loss: 12.657050132751465 = 0.08344622701406479 + 2.0 * 6.286801815032959
Epoch 950, val loss: 0.8450593948364258
Epoch 960, training loss: 12.657078742980957 = 0.07988181710243225 + 2.0 * 6.288598537445068
Epoch 960, val loss: 0.8501948118209839
Epoch 970, training loss: 12.660654067993164 = 0.0765080600976944 + 2.0 * 6.292072772979736
Epoch 970, val loss: 0.8553444147109985
Epoch 980, training loss: 12.642858505249023 = 0.0733526200056076 + 2.0 * 6.28475284576416
Epoch 980, val loss: 0.8606210947036743
Epoch 990, training loss: 12.632118225097656 = 0.07035697996616364 + 2.0 * 6.280880451202393
Epoch 990, val loss: 0.8660081028938293
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7703703703703704
0.8144438587243016
=== training gcn model ===
Epoch 0, training loss: 19.128137588500977 = 1.9344573020935059 + 2.0 * 8.596839904785156
Epoch 0, val loss: 1.932402491569519
Epoch 10, training loss: 19.118106842041016 = 1.9250012636184692 + 2.0 * 8.596552848815918
Epoch 10, val loss: 1.922760248184204
Epoch 20, training loss: 19.102489471435547 = 1.9135805368423462 + 2.0 * 8.594454765319824
Epoch 20, val loss: 1.9109994173049927
Epoch 30, training loss: 19.05550193786621 = 1.8982141017913818 + 2.0 * 8.578643798828125
Epoch 30, val loss: 1.895140528678894
Epoch 40, training loss: 18.852304458618164 = 1.8789788484573364 + 2.0 * 8.486662864685059
Epoch 40, val loss: 1.8760075569152832
Epoch 50, training loss: 17.918384552001953 = 1.8591761589050293 + 2.0 * 8.029603958129883
Epoch 50, val loss: 1.8569649457931519
Epoch 60, training loss: 17.05038070678711 = 1.8422001600265503 + 2.0 * 7.604090213775635
Epoch 60, val loss: 1.8415745496749878
Epoch 70, training loss: 16.394380569458008 = 1.830435872077942 + 2.0 * 7.281972408294678
Epoch 70, val loss: 1.8302977085113525
Epoch 80, training loss: 15.96752643585205 = 1.8182157278060913 + 2.0 * 7.074655532836914
Epoch 80, val loss: 1.8186368942260742
Epoch 90, training loss: 15.651155471801758 = 1.8048498630523682 + 2.0 * 6.923152923583984
Epoch 90, val loss: 1.8062864542007446
Epoch 100, training loss: 15.462325096130371 = 1.7910182476043701 + 2.0 * 6.835653305053711
Epoch 100, val loss: 1.7936995029449463
Epoch 110, training loss: 15.341628074645996 = 1.776782751083374 + 2.0 * 6.7824225425720215
Epoch 110, val loss: 1.7810473442077637
Epoch 120, training loss: 15.223259925842285 = 1.7628346681594849 + 2.0 * 6.730212688446045
Epoch 120, val loss: 1.768770694732666
Epoch 130, training loss: 15.125242233276367 = 1.7489984035491943 + 2.0 * 6.688121795654297
Epoch 130, val loss: 1.7564926147460938
Epoch 140, training loss: 15.049290657043457 = 1.7341835498809814 + 2.0 * 6.657553672790527
Epoch 140, val loss: 1.743214726448059
Epoch 150, training loss: 14.97482967376709 = 1.7174444198608398 + 2.0 * 6.628692626953125
Epoch 150, val loss: 1.7286784648895264
Epoch 160, training loss: 14.913818359375 = 1.6986891031265259 + 2.0 * 6.607564449310303
Epoch 160, val loss: 1.7121758460998535
Epoch 170, training loss: 14.85206413269043 = 1.6774808168411255 + 2.0 * 6.587291717529297
Epoch 170, val loss: 1.6936604976654053
Epoch 180, training loss: 14.790712356567383 = 1.6537024974822998 + 2.0 * 6.568504810333252
Epoch 180, val loss: 1.6729246377944946
Epoch 190, training loss: 14.730319023132324 = 1.6270278692245483 + 2.0 * 6.551645755767822
Epoch 190, val loss: 1.6501359939575195
Epoch 200, training loss: 14.664114952087402 = 1.5975102186203003 + 2.0 * 6.533302307128906
Epoch 200, val loss: 1.6248037815093994
Epoch 210, training loss: 14.597583770751953 = 1.5647140741348267 + 2.0 * 6.516434669494629
Epoch 210, val loss: 1.5967661142349243
Epoch 220, training loss: 14.53683090209961 = 1.5283259153366089 + 2.0 * 6.5042524337768555
Epoch 220, val loss: 1.5658009052276611
Epoch 230, training loss: 14.483766555786133 = 1.488454818725586 + 2.0 * 6.497655868530273
Epoch 230, val loss: 1.532249093055725
Epoch 240, training loss: 14.402952194213867 = 1.4461981058120728 + 2.0 * 6.478376865386963
Epoch 240, val loss: 1.4965579509735107
Epoch 250, training loss: 14.336071014404297 = 1.4013988971710205 + 2.0 * 6.467336177825928
Epoch 250, val loss: 1.4591554403305054
Epoch 260, training loss: 14.270536422729492 = 1.354385256767273 + 2.0 * 6.458075523376465
Epoch 260, val loss: 1.4201595783233643
Epoch 270, training loss: 14.204607009887695 = 1.3055661916732788 + 2.0 * 6.449520587921143
Epoch 270, val loss: 1.3798457384109497
Epoch 280, training loss: 14.142489433288574 = 1.2556270360946655 + 2.0 * 6.443431377410889
Epoch 280, val loss: 1.3387995958328247
Epoch 290, training loss: 14.080232620239258 = 1.2057898044586182 + 2.0 * 6.437221527099609
Epoch 290, val loss: 1.2987306118011475
Epoch 300, training loss: 14.013920783996582 = 1.1571645736694336 + 2.0 * 6.428378105163574
Epoch 300, val loss: 1.259979486465454
Epoch 310, training loss: 13.954994201660156 = 1.1098228693008423 + 2.0 * 6.422585487365723
Epoch 310, val loss: 1.2228599786758423
Epoch 320, training loss: 13.907581329345703 = 1.0640549659729004 + 2.0 * 6.421762943267822
Epoch 320, val loss: 1.1876089572906494
Epoch 330, training loss: 13.849485397338867 = 1.0202556848526 + 2.0 * 6.414614677429199
Epoch 330, val loss: 1.1547733545303345
Epoch 340, training loss: 13.795319557189941 = 0.978691041469574 + 2.0 * 6.408314228057861
Epoch 340, val loss: 1.1243966817855835
Epoch 350, training loss: 13.746682167053223 = 0.9389282464981079 + 2.0 * 6.403876781463623
Epoch 350, val loss: 1.0961135625839233
Epoch 360, training loss: 13.702545166015625 = 0.9008433818817139 + 2.0 * 6.400850772857666
Epoch 360, val loss: 1.069810390472412
Epoch 370, training loss: 13.655284881591797 = 0.8643893599510193 + 2.0 * 6.395447731018066
Epoch 370, val loss: 1.0452741384506226
Epoch 380, training loss: 13.611610412597656 = 0.829014003276825 + 2.0 * 6.391298294067383
Epoch 380, val loss: 1.0223013162612915
Epoch 390, training loss: 13.571746826171875 = 0.794609785079956 + 2.0 * 6.38856840133667
Epoch 390, val loss: 1.0005747079849243
Epoch 400, training loss: 13.543339729309082 = 0.761371374130249 + 2.0 * 6.390984058380127
Epoch 400, val loss: 0.9802144169807434
Epoch 410, training loss: 13.489896774291992 = 0.7294527888298035 + 2.0 * 6.380221843719482
Epoch 410, val loss: 0.9611749649047852
Epoch 420, training loss: 13.452116012573242 = 0.6984310746192932 + 2.0 * 6.376842498779297
Epoch 420, val loss: 0.9432225823402405
Epoch 430, training loss: 13.417119979858398 = 0.668279767036438 + 2.0 * 6.374420166015625
Epoch 430, val loss: 0.9262887239456177
Epoch 440, training loss: 13.387473106384277 = 0.6390030980110168 + 2.0 * 6.374235153198242
Epoch 440, val loss: 0.9103331565856934
Epoch 450, training loss: 13.3499116897583 = 0.6109439134597778 + 2.0 * 6.369483947753906
Epoch 450, val loss: 0.895539402961731
Epoch 460, training loss: 13.316510200500488 = 0.5837673544883728 + 2.0 * 6.3663716316223145
Epoch 460, val loss: 0.8817163705825806
Epoch 470, training loss: 13.290771484375 = 0.5575642585754395 + 2.0 * 6.366603374481201
Epoch 470, val loss: 0.8688399195671082
Epoch 480, training loss: 13.255693435668945 = 0.5322552919387817 + 2.0 * 6.361719131469727
Epoch 480, val loss: 0.8569156527519226
Epoch 490, training loss: 13.224794387817383 = 0.5079319477081299 + 2.0 * 6.358431339263916
Epoch 490, val loss: 0.8458332419395447
Epoch 500, training loss: 13.197486877441406 = 0.48437967896461487 + 2.0 * 6.356553554534912
Epoch 500, val loss: 0.835694432258606
Epoch 510, training loss: 13.175179481506348 = 0.46165207028388977 + 2.0 * 6.35676383972168
Epoch 510, val loss: 0.8262373805046082
Epoch 520, training loss: 13.144620895385742 = 0.4398579001426697 + 2.0 * 6.352381706237793
Epoch 520, val loss: 0.8177128434181213
Epoch 530, training loss: 13.117575645446777 = 0.4188627600669861 + 2.0 * 6.349356651306152
Epoch 530, val loss: 0.8099893927574158
Epoch 540, training loss: 13.1008939743042 = 0.3986005187034607 + 2.0 * 6.351146697998047
Epoch 540, val loss: 0.8029932379722595
Epoch 550, training loss: 13.070764541625977 = 0.3790004551410675 + 2.0 * 6.345881938934326
Epoch 550, val loss: 0.7968022227287292
Epoch 560, training loss: 13.045894622802734 = 0.36022070050239563 + 2.0 * 6.342836856842041
Epoch 560, val loss: 0.7913944721221924
Epoch 570, training loss: 13.028432846069336 = 0.3421475887298584 + 2.0 * 6.343142509460449
Epoch 570, val loss: 0.7866902947425842
Epoch 580, training loss: 13.006414413452148 = 0.3248579502105713 + 2.0 * 6.340778350830078
Epoch 580, val loss: 0.7827621698379517
Epoch 590, training loss: 12.981074333190918 = 0.3083229959011078 + 2.0 * 6.336375713348389
Epoch 590, val loss: 0.7795810103416443
Epoch 600, training loss: 12.9672269821167 = 0.2924787402153015 + 2.0 * 6.337374210357666
Epoch 600, val loss: 0.7770904898643494
Epoch 610, training loss: 12.962108612060547 = 0.27744755148887634 + 2.0 * 6.342330455780029
Epoch 610, val loss: 0.7752256393432617
Epoch 620, training loss: 12.931719779968262 = 0.26329493522644043 + 2.0 * 6.334212303161621
Epoch 620, val loss: 0.774315357208252
Epoch 630, training loss: 12.911075592041016 = 0.24987533688545227 + 2.0 * 6.330600261688232
Epoch 630, val loss: 0.7740038633346558
Epoch 640, training loss: 12.894416809082031 = 0.23714163899421692 + 2.0 * 6.328637599945068
Epoch 640, val loss: 0.7742493748664856
Epoch 650, training loss: 12.884654998779297 = 0.22508008778095245 + 2.0 * 6.329787254333496
Epoch 650, val loss: 0.7750760316848755
Epoch 660, training loss: 12.865957260131836 = 0.2136741429567337 + 2.0 * 6.326141357421875
Epoch 660, val loss: 0.7763639688491821
Epoch 670, training loss: 12.855303764343262 = 0.2029537409543991 + 2.0 * 6.326175212860107
Epoch 670, val loss: 0.7782219052314758
Epoch 680, training loss: 12.84298324584961 = 0.19282077252864838 + 2.0 * 6.3250813484191895
Epoch 680, val loss: 0.780516505241394
Epoch 690, training loss: 12.829190254211426 = 0.18327458202838898 + 2.0 * 6.322957992553711
Epoch 690, val loss: 0.7832167744636536
Epoch 700, training loss: 12.817069053649902 = 0.17424577474594116 + 2.0 * 6.321411609649658
Epoch 700, val loss: 0.786206841468811
Epoch 710, training loss: 12.804283142089844 = 0.16574248671531677 + 2.0 * 6.319270133972168
Epoch 710, val loss: 0.7897035479545593
Epoch 720, training loss: 12.80379581451416 = 0.15772444009780884 + 2.0 * 6.323035717010498
Epoch 720, val loss: 0.7933692932128906
Epoch 730, training loss: 12.78550910949707 = 0.15013259649276733 + 2.0 * 6.317688465118408
Epoch 730, val loss: 0.7972751259803772
Epoch 740, training loss: 12.774991989135742 = 0.14303219318389893 + 2.0 * 6.315979957580566
Epoch 740, val loss: 0.8015297055244446
Epoch 750, training loss: 12.763559341430664 = 0.1363009810447693 + 2.0 * 6.313629150390625
Epoch 750, val loss: 0.8059203624725342
Epoch 760, training loss: 12.75627613067627 = 0.12993597984313965 + 2.0 * 6.313169956207275
Epoch 760, val loss: 0.8105491995811462
Epoch 770, training loss: 12.758024215698242 = 0.12393316626548767 + 2.0 * 6.31704568862915
Epoch 770, val loss: 0.8153403401374817
Epoch 780, training loss: 12.740835189819336 = 0.11824413388967514 + 2.0 * 6.311295509338379
Epoch 780, val loss: 0.8201918005943298
Epoch 790, training loss: 12.734509468078613 = 0.11290398985147476 + 2.0 * 6.310802936553955
Epoch 790, val loss: 0.8252722024917603
Epoch 800, training loss: 12.725777626037598 = 0.10786944627761841 + 2.0 * 6.308954238891602
Epoch 800, val loss: 0.8303543329238892
Epoch 810, training loss: 12.716622352600098 = 0.10310781747102737 + 2.0 * 6.30675745010376
Epoch 810, val loss: 0.8356519937515259
Epoch 820, training loss: 12.710334777832031 = 0.09860444068908691 + 2.0 * 6.305865287780762
Epoch 820, val loss: 0.8410086631774902
Epoch 830, training loss: 12.72463321685791 = 0.09435641765594482 + 2.0 * 6.315138339996338
Epoch 830, val loss: 0.8464065194129944
Epoch 840, training loss: 12.709087371826172 = 0.09032826125621796 + 2.0 * 6.309379577636719
Epoch 840, val loss: 0.8519418239593506
Epoch 850, training loss: 12.693669319152832 = 0.08653614670038223 + 2.0 * 6.3035664558410645
Epoch 850, val loss: 0.8574945330619812
Epoch 860, training loss: 12.68735122680664 = 0.08295105397701263 + 2.0 * 6.3022003173828125
Epoch 860, val loss: 0.8630648851394653
Epoch 870, training loss: 12.696183204650879 = 0.07954887300729752 + 2.0 * 6.308317184448242
Epoch 870, val loss: 0.8686833381652832
Epoch 880, training loss: 12.680757522583008 = 0.07635777443647385 + 2.0 * 6.302199840545654
Epoch 880, val loss: 0.8746410012245178
Epoch 890, training loss: 12.67443561553955 = 0.07330520451068878 + 2.0 * 6.300565242767334
Epoch 890, val loss: 0.8803471922874451
Epoch 900, training loss: 12.677776336669922 = 0.07043515145778656 + 2.0 * 6.303670406341553
Epoch 900, val loss: 0.8861005306243896
Epoch 910, training loss: 12.668981552124023 = 0.06770617514848709 + 2.0 * 6.300637722015381
Epoch 910, val loss: 0.8918904066085815
Epoch 920, training loss: 12.657861709594727 = 0.06512908637523651 + 2.0 * 6.296366214752197
Epoch 920, val loss: 0.8977237939834595
Epoch 930, training loss: 12.654633522033691 = 0.06268389523029327 + 2.0 * 6.2959747314453125
Epoch 930, val loss: 0.903587818145752
Epoch 940, training loss: 12.663386344909668 = 0.06036504730582237 + 2.0 * 6.301510810852051
Epoch 940, val loss: 0.9093753099441528
Epoch 950, training loss: 12.65424633026123 = 0.05815128609538078 + 2.0 * 6.2980475425720215
Epoch 950, val loss: 0.91521817445755
Epoch 960, training loss: 12.645820617675781 = 0.056054964661598206 + 2.0 * 6.294882774353027
Epoch 960, val loss: 0.9209972620010376
Epoch 970, training loss: 12.641175270080566 = 0.05406658723950386 + 2.0 * 6.293554306030273
Epoch 970, val loss: 0.9267816543579102
Epoch 980, training loss: 12.63630485534668 = 0.052176255732774734 + 2.0 * 6.292064189910889
Epoch 980, val loss: 0.9326218366622925
Epoch 990, training loss: 12.647631645202637 = 0.05038587003946304 + 2.0 * 6.298623085021973
Epoch 990, val loss: 0.9383717775344849
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7666666666666667
0.8207696362677913
=== training gcn model ===
Epoch 0, training loss: 19.139631271362305 = 1.945933222770691 + 2.0 * 8.59684944152832
Epoch 0, val loss: 1.941857933998108
Epoch 10, training loss: 19.128713607788086 = 1.9355229139328003 + 2.0 * 8.596595764160156
Epoch 10, val loss: 1.9325342178344727
Epoch 20, training loss: 19.111167907714844 = 1.922250509262085 + 2.0 * 8.59445858001709
Epoch 20, val loss: 1.9201816320419312
Epoch 30, training loss: 19.05780029296875 = 1.9034459590911865 + 2.0 * 8.577177047729492
Epoch 30, val loss: 1.902510166168213
Epoch 40, training loss: 18.823728561401367 = 1.879276990890503 + 2.0 * 8.4722261428833
Epoch 40, val loss: 1.8808543682098389
Epoch 50, training loss: 18.124692916870117 = 1.853833556175232 + 2.0 * 8.135429382324219
Epoch 50, val loss: 1.8592016696929932
Epoch 60, training loss: 17.484098434448242 = 1.8353149890899658 + 2.0 * 7.824391841888428
Epoch 60, val loss: 1.8439643383026123
Epoch 70, training loss: 16.452356338500977 = 1.8252240419387817 + 2.0 * 7.313565731048584
Epoch 70, val loss: 1.8348993062973022
Epoch 80, training loss: 15.899374008178711 = 1.8171350955963135 + 2.0 * 7.041119575500488
Epoch 80, val loss: 1.826936960220337
Epoch 90, training loss: 15.642864227294922 = 1.8049393892288208 + 2.0 * 6.918962478637695
Epoch 90, val loss: 1.8160861730575562
Epoch 100, training loss: 15.460479736328125 = 1.7918448448181152 + 2.0 * 6.834317684173584
Epoch 100, val loss: 1.8046339750289917
Epoch 110, training loss: 15.332032203674316 = 1.7794429063796997 + 2.0 * 6.776294708251953
Epoch 110, val loss: 1.793772578239441
Epoch 120, training loss: 15.224574089050293 = 1.767945408821106 + 2.0 * 6.728314399719238
Epoch 120, val loss: 1.7835464477539062
Epoch 130, training loss: 15.133111000061035 = 1.756700873374939 + 2.0 * 6.688205242156982
Epoch 130, val loss: 1.7735424041748047
Epoch 140, training loss: 15.038386344909668 = 1.7445467710494995 + 2.0 * 6.6469197273254395
Epoch 140, val loss: 1.7630081176757812
Epoch 150, training loss: 14.953554153442383 = 1.7314635515213013 + 2.0 * 6.6110453605651855
Epoch 150, val loss: 1.7518655061721802
Epoch 160, training loss: 14.881543159484863 = 1.7170379161834717 + 2.0 * 6.582252502441406
Epoch 160, val loss: 1.7397538423538208
Epoch 170, training loss: 14.81399154663086 = 1.7010588645935059 + 2.0 * 6.556466102600098
Epoch 170, val loss: 1.726454734802246
Epoch 180, training loss: 14.753718376159668 = 1.6831865310668945 + 2.0 * 6.535265922546387
Epoch 180, val loss: 1.711810827255249
Epoch 190, training loss: 14.704935073852539 = 1.6632373332977295 + 2.0 * 6.520848751068115
Epoch 190, val loss: 1.6953248977661133
Epoch 200, training loss: 14.646101951599121 = 1.641243815422058 + 2.0 * 6.502429008483887
Epoch 200, val loss: 1.6772764921188354
Epoch 210, training loss: 14.591890335083008 = 1.6169487237930298 + 2.0 * 6.487470626831055
Epoch 210, val loss: 1.65733003616333
Epoch 220, training loss: 14.550500869750977 = 1.5903024673461914 + 2.0 * 6.480099201202393
Epoch 220, val loss: 1.6354280710220337
Epoch 230, training loss: 14.490196228027344 = 1.561611533164978 + 2.0 * 6.464292526245117
Epoch 230, val loss: 1.6116852760314941
Epoch 240, training loss: 14.439498901367188 = 1.5308862924575806 + 2.0 * 6.454306125640869
Epoch 240, val loss: 1.5862867832183838
Epoch 250, training loss: 14.39169979095459 = 1.4984523057937622 + 2.0 * 6.446623802185059
Epoch 250, val loss: 1.5595276355743408
Epoch 260, training loss: 14.33712100982666 = 1.4647184610366821 + 2.0 * 6.436201095581055
Epoch 260, val loss: 1.531644344329834
Epoch 270, training loss: 14.307619094848633 = 1.4296716451644897 + 2.0 * 6.438973903656006
Epoch 270, val loss: 1.5028754472732544
Epoch 280, training loss: 14.241539001464844 = 1.3942517042160034 + 2.0 * 6.423643589019775
Epoch 280, val loss: 1.4736202955245972
Epoch 290, training loss: 14.189337730407715 = 1.3583743572235107 + 2.0 * 6.4154815673828125
Epoch 290, val loss: 1.444252610206604
Epoch 300, training loss: 14.139713287353516 = 1.3219618797302246 + 2.0 * 6.408875942230225
Epoch 300, val loss: 1.4146969318389893
Epoch 310, training loss: 14.104289054870605 = 1.2851840257644653 + 2.0 * 6.409552574157715
Epoch 310, val loss: 1.3849787712097168
Epoch 320, training loss: 14.047996520996094 = 1.2484049797058105 + 2.0 * 6.399796009063721
Epoch 320, val loss: 1.3553993701934814
Epoch 330, training loss: 14.000068664550781 = 1.2113553285598755 + 2.0 * 6.394356727600098
Epoch 330, val loss: 1.3259475231170654
Epoch 340, training loss: 13.953617095947266 = 1.1740243434906006 + 2.0 * 6.389796257019043
Epoch 340, val loss: 1.2963733673095703
Epoch 350, training loss: 13.907661437988281 = 1.1365430355072021 + 2.0 * 6.38555908203125
Epoch 350, val loss: 1.2667205333709717
Epoch 360, training loss: 13.862651824951172 = 1.0986140966415405 + 2.0 * 6.38201904296875
Epoch 360, val loss: 1.2369356155395508
Epoch 370, training loss: 13.816939353942871 = 1.0604543685913086 + 2.0 * 6.378242492675781
Epoch 370, val loss: 1.2067159414291382
Epoch 380, training loss: 13.772512435913086 = 1.022379994392395 + 2.0 * 6.37506628036499
Epoch 380, val loss: 1.1768912076950073
Epoch 390, training loss: 13.72716236114502 = 0.9843180179595947 + 2.0 * 6.371422290802002
Epoch 390, val loss: 1.1470617055892944
Epoch 400, training loss: 13.687420845031738 = 0.9464170932769775 + 2.0 * 6.37050199508667
Epoch 400, val loss: 1.1172583103179932
Epoch 410, training loss: 13.6481351852417 = 0.9089658260345459 + 2.0 * 6.369584560394287
Epoch 410, val loss: 1.0880800485610962
Epoch 420, training loss: 13.599323272705078 = 0.8725714683532715 + 2.0 * 6.363376140594482
Epoch 420, val loss: 1.0597553253173828
Epoch 430, training loss: 13.556952476501465 = 0.8371855020523071 + 2.0 * 6.3598833084106445
Epoch 430, val loss: 1.0324655771255493
Epoch 440, training loss: 13.5355806350708 = 0.8031839728355408 + 2.0 * 6.366198539733887
Epoch 440, val loss: 1.0061893463134766
Epoch 450, training loss: 13.482133865356445 = 0.77079838514328 + 2.0 * 6.355667591094971
Epoch 450, val loss: 0.9816837906837463
Epoch 460, training loss: 13.445820808410645 = 0.7403451800346375 + 2.0 * 6.352737903594971
Epoch 460, val loss: 0.9592933058738708
Epoch 470, training loss: 13.412457466125488 = 0.711536169052124 + 2.0 * 6.350460529327393
Epoch 470, val loss: 0.938459038734436
Epoch 480, training loss: 13.38355827331543 = 0.6843647956848145 + 2.0 * 6.3495965003967285
Epoch 480, val loss: 0.9190329909324646
Epoch 490, training loss: 13.349637031555176 = 0.6588429808616638 + 2.0 * 6.345396995544434
Epoch 490, val loss: 0.9014286994934082
Epoch 500, training loss: 13.324457168579102 = 0.6347250938415527 + 2.0 * 6.3448662757873535
Epoch 500, val loss: 0.8853388428688049
Epoch 510, training loss: 13.296751022338867 = 0.6120080351829529 + 2.0 * 6.342371463775635
Epoch 510, val loss: 0.8704788088798523
Epoch 520, training loss: 13.270318984985352 = 0.5905853509902954 + 2.0 * 6.339866638183594
Epoch 520, val loss: 0.8571963906288147
Epoch 530, training loss: 13.243834495544434 = 0.5701782703399658 + 2.0 * 6.336828231811523
Epoch 530, val loss: 0.8450695872306824
Epoch 540, training loss: 13.229496955871582 = 0.5506753325462341 + 2.0 * 6.339410781860352
Epoch 540, val loss: 0.8338342905044556
Epoch 550, training loss: 13.203923225402832 = 0.5320923328399658 + 2.0 * 6.335915565490723
Epoch 550, val loss: 0.8232102394104004
Epoch 560, training loss: 13.177810668945312 = 0.514176070690155 + 2.0 * 6.331817150115967
Epoch 560, val loss: 0.8138014674186707
Epoch 570, training loss: 13.158927917480469 = 0.496891051530838 + 2.0 * 6.331018447875977
Epoch 570, val loss: 0.804858386516571
Epoch 580, training loss: 13.140202522277832 = 0.4800661504268646 + 2.0 * 6.330068111419678
Epoch 580, val loss: 0.7961041331291199
Epoch 590, training loss: 13.118583679199219 = 0.4637593924999237 + 2.0 * 6.327412128448486
Epoch 590, val loss: 0.7880228757858276
Epoch 600, training loss: 13.110875129699707 = 0.447738379240036 + 2.0 * 6.331568241119385
Epoch 600, val loss: 0.7802734971046448
Epoch 610, training loss: 13.082763671875 = 0.43198150396347046 + 2.0 * 6.3253912925720215
Epoch 610, val loss: 0.7728410959243774
Epoch 620, training loss: 13.060588836669922 = 0.4164421558380127 + 2.0 * 6.322073459625244
Epoch 620, val loss: 0.765794038772583
Epoch 630, training loss: 13.041523933410645 = 0.4010370671749115 + 2.0 * 6.3202433586120605
Epoch 630, val loss: 0.7589378356933594
Epoch 640, training loss: 13.043055534362793 = 0.3856791853904724 + 2.0 * 6.328688144683838
Epoch 640, val loss: 0.7520102262496948
Epoch 650, training loss: 13.009151458740234 = 0.37051185965538025 + 2.0 * 6.319319725036621
Epoch 650, val loss: 0.7454203963279724
Epoch 660, training loss: 12.988537788391113 = 0.3554544746875763 + 2.0 * 6.31654167175293
Epoch 660, val loss: 0.73929762840271
Epoch 670, training loss: 12.978283882141113 = 0.34052836894989014 + 2.0 * 6.318877696990967
Epoch 670, val loss: 0.7332726716995239
Epoch 680, training loss: 12.954947471618652 = 0.3258395493030548 + 2.0 * 6.314553737640381
Epoch 680, val loss: 0.727593183517456
Epoch 690, training loss: 12.935371398925781 = 0.31130051612854004 + 2.0 * 6.31203556060791
Epoch 690, val loss: 0.7223045229911804
Epoch 700, training loss: 12.920424461364746 = 0.2970341444015503 + 2.0 * 6.311695098876953
Epoch 700, val loss: 0.7173026204109192
Epoch 710, training loss: 12.907800674438477 = 0.2830931544303894 + 2.0 * 6.312353610992432
Epoch 710, val loss: 0.7126079201698303
Epoch 720, training loss: 12.898041725158691 = 0.2696087658405304 + 2.0 * 6.314216613769531
Epoch 720, val loss: 0.7085143327713013
Epoch 730, training loss: 12.872930526733398 = 0.25660866498947144 + 2.0 * 6.308160781860352
Epoch 730, val loss: 0.7048549056053162
Epoch 740, training loss: 12.857532501220703 = 0.24410122632980347 + 2.0 * 6.306715488433838
Epoch 740, val loss: 0.7016792893409729
Epoch 750, training loss: 12.851706504821777 = 0.2321145087480545 + 2.0 * 6.30979585647583
Epoch 750, val loss: 0.6988304853439331
Epoch 760, training loss: 12.829020500183105 = 0.2207455188035965 + 2.0 * 6.304137706756592
Epoch 760, val loss: 0.6965945363044739
Epoch 770, training loss: 12.814754486083984 = 0.20988473296165466 + 2.0 * 6.302434921264648
Epoch 770, val loss: 0.6948781609535217
Epoch 780, training loss: 12.809408187866211 = 0.19958217442035675 + 2.0 * 6.30491304397583
Epoch 780, val loss: 0.6934733390808105
Epoch 790, training loss: 12.792957305908203 = 0.18978741765022278 + 2.0 * 6.301584720611572
Epoch 790, val loss: 0.6923882961273193
Epoch 800, training loss: 12.780717849731445 = 0.18056921660900116 + 2.0 * 6.300074100494385
Epoch 800, val loss: 0.6920234560966492
Epoch 810, training loss: 12.769705772399902 = 0.1718217134475708 + 2.0 * 6.2989420890808105
Epoch 810, val loss: 0.6919583082199097
Epoch 820, training loss: 12.773900985717773 = 0.16355988383293152 + 2.0 * 6.30517053604126
Epoch 820, val loss: 0.6922419667243958
Epoch 830, training loss: 12.757214546203613 = 0.15578106045722961 + 2.0 * 6.300716876983643
Epoch 830, val loss: 0.6929066777229309
Epoch 840, training loss: 12.745466232299805 = 0.1484280526638031 + 2.0 * 6.298519134521484
Epoch 840, val loss: 0.6939082145690918
Epoch 850, training loss: 12.735840797424316 = 0.14151710271835327 + 2.0 * 6.297162055969238
Epoch 850, val loss: 0.695303201675415
Epoch 860, training loss: 12.723392486572266 = 0.13498617708683014 + 2.0 * 6.294203281402588
Epoch 860, val loss: 0.6970307230949402
Epoch 870, training loss: 12.714883804321289 = 0.12882103025913239 + 2.0 * 6.293031215667725
Epoch 870, val loss: 0.6990888714790344
Epoch 880, training loss: 12.718671798706055 = 0.12300146371126175 + 2.0 * 6.297835350036621
Epoch 880, val loss: 0.7012994885444641
Epoch 890, training loss: 12.702824592590332 = 0.11747272312641144 + 2.0 * 6.292675971984863
Epoch 890, val loss: 0.7038634419441223
Epoch 900, training loss: 12.698782920837402 = 0.11227426677942276 + 2.0 * 6.293254375457764
Epoch 900, val loss: 0.7065867185592651
Epoch 910, training loss: 12.688130378723145 = 0.10735458880662918 + 2.0 * 6.290388107299805
Epoch 910, val loss: 0.7094651460647583
Epoch 920, training loss: 12.681092262268066 = 0.10271911323070526 + 2.0 * 6.289186477661133
Epoch 920, val loss: 0.7126242518424988
Epoch 930, training loss: 12.67381477355957 = 0.09832315146923065 + 2.0 * 6.287745952606201
Epoch 930, val loss: 0.7160399556159973
Epoch 940, training loss: 12.67766284942627 = 0.09415716677904129 + 2.0 * 6.291752815246582
Epoch 940, val loss: 0.7194817066192627
Epoch 950, training loss: 12.67171573638916 = 0.09022379666566849 + 2.0 * 6.290745735168457
Epoch 950, val loss: 0.722969114780426
Epoch 960, training loss: 12.664470672607422 = 0.08649618178606033 + 2.0 * 6.288987159729004
Epoch 960, val loss: 0.7267476916313171
Epoch 970, training loss: 12.651021957397461 = 0.08298896253108978 + 2.0 * 6.2840166091918945
Epoch 970, val loss: 0.7306197881698608
Epoch 980, training loss: 12.6474609375 = 0.07966142147779465 + 2.0 * 6.283899784088135
Epoch 980, val loss: 0.7346607446670532
Epoch 990, training loss: 12.644364356994629 = 0.07649911940097809 + 2.0 * 6.283932685852051
Epoch 990, val loss: 0.7386992573738098
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8
0.8176067474960464
The final CL Acc:0.77901, 0.01492, The final GNN Acc:0.81761, 0.00258
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13126])
remove edge: torch.Size([2, 7992])
updated graph: torch.Size([2, 10562])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 19.133258819580078 = 1.9395722150802612 + 2.0 * 8.596843719482422
Epoch 0, val loss: 1.9259151220321655
Epoch 10, training loss: 19.12226676940918 = 1.9291112422943115 + 2.0 * 8.596577644348145
Epoch 10, val loss: 1.9160624742507935
Epoch 20, training loss: 19.105403900146484 = 1.9165582656860352 + 2.0 * 8.594422340393066
Epoch 20, val loss: 1.9038007259368896
Epoch 30, training loss: 19.056377410888672 = 1.8997405767440796 + 2.0 * 8.57831859588623
Epoch 30, val loss: 1.8870141506195068
Epoch 40, training loss: 18.84927749633789 = 1.8778425455093384 + 2.0 * 8.4857177734375
Epoch 40, val loss: 1.8657512664794922
Epoch 50, training loss: 17.817670822143555 = 1.8538081645965576 + 2.0 * 7.981931209564209
Epoch 50, val loss: 1.8430284261703491
Epoch 60, training loss: 16.83544921875 = 1.834082007408142 + 2.0 * 7.500683784484863
Epoch 60, val loss: 1.8257735967636108
Epoch 70, training loss: 16.196487426757812 = 1.8202859163284302 + 2.0 * 7.188100814819336
Epoch 70, val loss: 1.8127801418304443
Epoch 80, training loss: 15.852932929992676 = 1.8079663515090942 + 2.0 * 7.0224833488464355
Epoch 80, val loss: 1.8017390966415405
Epoch 90, training loss: 15.657498359680176 = 1.7961362600326538 + 2.0 * 6.930681228637695
Epoch 90, val loss: 1.7908706665039062
Epoch 100, training loss: 15.489782333374023 = 1.7832592725753784 + 2.0 * 6.853261470794678
Epoch 100, val loss: 1.7796474695205688
Epoch 110, training loss: 15.317279815673828 = 1.771743655204773 + 2.0 * 6.772768020629883
Epoch 110, val loss: 1.770312786102295
Epoch 120, training loss: 15.172819137573242 = 1.7619704008102417 + 2.0 * 6.7054243087768555
Epoch 120, val loss: 1.7624680995941162
Epoch 130, training loss: 15.063371658325195 = 1.7510311603546143 + 2.0 * 6.65617036819458
Epoch 130, val loss: 1.7532042264938354
Epoch 140, training loss: 14.9784517288208 = 1.7377241849899292 + 2.0 * 6.620363712310791
Epoch 140, val loss: 1.7416021823883057
Epoch 150, training loss: 14.908501625061035 = 1.7223924398422241 + 2.0 * 6.59305477142334
Epoch 150, val loss: 1.7281370162963867
Epoch 160, training loss: 14.847769737243652 = 1.7053769826889038 + 2.0 * 6.571196556091309
Epoch 160, val loss: 1.713249921798706
Epoch 170, training loss: 14.785789489746094 = 1.686421275138855 + 2.0 * 6.549684047698975
Epoch 170, val loss: 1.6968013048171997
Epoch 180, training loss: 14.727609634399414 = 1.6650052070617676 + 2.0 * 6.531301975250244
Epoch 180, val loss: 1.6784570217132568
Epoch 190, training loss: 14.673469543457031 = 1.640653133392334 + 2.0 * 6.5164079666137695
Epoch 190, val loss: 1.657690405845642
Epoch 200, training loss: 14.61353874206543 = 1.6131248474121094 + 2.0 * 6.50020694732666
Epoch 200, val loss: 1.6343992948532104
Epoch 210, training loss: 14.557034492492676 = 1.5821831226348877 + 2.0 * 6.487425804138184
Epoch 210, val loss: 1.6082121133804321
Epoch 220, training loss: 14.49837875366211 = 1.5472853183746338 + 2.0 * 6.475546836853027
Epoch 220, val loss: 1.578626275062561
Epoch 230, training loss: 14.441526412963867 = 1.5082814693450928 + 2.0 * 6.466622352600098
Epoch 230, val loss: 1.545716643333435
Epoch 240, training loss: 14.376689910888672 = 1.4657282829284668 + 2.0 * 6.455481052398682
Epoch 240, val loss: 1.5101886987686157
Epoch 250, training loss: 14.315530776977539 = 1.4200022220611572 + 2.0 * 6.4477643966674805
Epoch 250, val loss: 1.4722888469696045
Epoch 260, training loss: 14.250000953674316 = 1.3719751834869385 + 2.0 * 6.4390130043029785
Epoch 260, val loss: 1.433133602142334
Epoch 270, training loss: 14.186084747314453 = 1.322340488433838 + 2.0 * 6.4318718910217285
Epoch 270, val loss: 1.393117070198059
Epoch 280, training loss: 14.122713088989258 = 1.2721257209777832 + 2.0 * 6.425293445587158
Epoch 280, val loss: 1.3535934686660767
Epoch 290, training loss: 14.059871673583984 = 1.2225240468978882 + 2.0 * 6.418673992156982
Epoch 290, val loss: 1.3151897192001343
Epoch 300, training loss: 13.99874210357666 = 1.173779845237732 + 2.0 * 6.412481307983398
Epoch 300, val loss: 1.2780627012252808
Epoch 310, training loss: 13.941385269165039 = 1.1261969804763794 + 2.0 * 6.407594203948975
Epoch 310, val loss: 1.242557406425476
Epoch 320, training loss: 13.895069122314453 = 1.080476999282837 + 2.0 * 6.407296180725098
Epoch 320, val loss: 1.2089262008666992
Epoch 330, training loss: 13.832952499389648 = 1.0372577905654907 + 2.0 * 6.3978471755981445
Epoch 330, val loss: 1.1774308681488037
Epoch 340, training loss: 13.77953052520752 = 0.9958617091178894 + 2.0 * 6.391834259033203
Epoch 340, val loss: 1.1477279663085938
Epoch 350, training loss: 13.731945991516113 = 0.955967366695404 + 2.0 * 6.387989521026611
Epoch 350, val loss: 1.1193711757659912
Epoch 360, training loss: 13.68665885925293 = 0.917521059513092 + 2.0 * 6.384568691253662
Epoch 360, val loss: 1.092278242111206
Epoch 370, training loss: 13.638623237609863 = 0.8805280327796936 + 2.0 * 6.379047393798828
Epoch 370, val loss: 1.0665459632873535
Epoch 380, training loss: 13.604705810546875 = 0.8446242809295654 + 2.0 * 6.380040645599365
Epoch 380, val loss: 1.041825771331787
Epoch 390, training loss: 13.555685043334961 = 0.8102235198020935 + 2.0 * 6.372730731964111
Epoch 390, val loss: 1.0181878805160522
Epoch 400, training loss: 13.515016555786133 = 0.7769847512245178 + 2.0 * 6.369015693664551
Epoch 400, val loss: 0.995850145816803
Epoch 410, training loss: 13.474872589111328 = 0.7448224425315857 + 2.0 * 6.365025043487549
Epoch 410, val loss: 0.9746583104133606
Epoch 420, training loss: 13.446146965026855 = 0.7138903141021729 + 2.0 * 6.366128444671631
Epoch 420, val loss: 0.9545785188674927
Epoch 430, training loss: 13.404053688049316 = 0.6846393942832947 + 2.0 * 6.359707355499268
Epoch 430, val loss: 0.936051070690155
Epoch 440, training loss: 13.370155334472656 = 0.6567655801773071 + 2.0 * 6.35669469833374
Epoch 440, val loss: 0.91890949010849
Epoch 450, training loss: 13.345719337463379 = 0.6301321387290955 + 2.0 * 6.357793807983398
Epoch 450, val loss: 0.9029895067214966
Epoch 460, training loss: 13.310042381286621 = 0.6048844456672668 + 2.0 * 6.352579116821289
Epoch 460, val loss: 0.8884820938110352
Epoch 470, training loss: 13.280179977416992 = 0.5809298753738403 + 2.0 * 6.349625110626221
Epoch 470, val loss: 0.875206708908081
Epoch 480, training loss: 13.259611129760742 = 0.5580557584762573 + 2.0 * 6.350777626037598
Epoch 480, val loss: 0.8630872964859009
Epoch 490, training loss: 13.234905242919922 = 0.536457896232605 + 2.0 * 6.349223613739014
Epoch 490, val loss: 0.8519905805587769
Epoch 500, training loss: 13.203784942626953 = 0.5158562660217285 + 2.0 * 6.343964576721191
Epoch 500, val loss: 0.8419922590255737
Epoch 510, training loss: 13.177059173583984 = 0.4961102306842804 + 2.0 * 6.340474605560303
Epoch 510, val loss: 0.8329435586929321
Epoch 520, training loss: 13.164986610412598 = 0.47713103890419006 + 2.0 * 6.34392786026001
Epoch 520, val loss: 0.8246226906776428
Epoch 530, training loss: 13.134398460388184 = 0.45887264609336853 + 2.0 * 6.337762832641602
Epoch 530, val loss: 0.8171511888504028
Epoch 540, training loss: 13.112631797790527 = 0.4413334131240845 + 2.0 * 6.335649013519287
Epoch 540, val loss: 0.8103315830230713
Epoch 550, training loss: 13.091517448425293 = 0.42440351843833923 + 2.0 * 6.33355712890625
Epoch 550, val loss: 0.8041463494300842
Epoch 560, training loss: 13.072322845458984 = 0.4080605208873749 + 2.0 * 6.332131385803223
Epoch 560, val loss: 0.7985209226608276
Epoch 570, training loss: 13.052471160888672 = 0.39220529794692993 + 2.0 * 6.330132961273193
Epoch 570, val loss: 0.7933326959609985
Epoch 580, training loss: 13.041333198547363 = 0.37675777077674866 + 2.0 * 6.332287788391113
Epoch 580, val loss: 0.7885602116584778
Epoch 590, training loss: 13.022412300109863 = 0.3617115318775177 + 2.0 * 6.330350399017334
Epoch 590, val loss: 0.7842268943786621
Epoch 600, training loss: 12.999496459960938 = 0.34719371795654297 + 2.0 * 6.326151371002197
Epoch 600, val loss: 0.7801744341850281
Epoch 610, training loss: 12.983436584472656 = 0.33303168416023254 + 2.0 * 6.325202465057373
Epoch 610, val loss: 0.7765248417854309
Epoch 620, training loss: 12.965728759765625 = 0.31926923990249634 + 2.0 * 6.323229789733887
Epoch 620, val loss: 0.7731055021286011
Epoch 630, training loss: 12.950592994689941 = 0.3058784008026123 + 2.0 * 6.322357177734375
Epoch 630, val loss: 0.770072877407074
Epoch 640, training loss: 12.934343338012695 = 0.2928805351257324 + 2.0 * 6.3207316398620605
Epoch 640, val loss: 0.7673364281654358
Epoch 650, training loss: 12.92017936706543 = 0.2802354395389557 + 2.0 * 6.319972038269043
Epoch 650, val loss: 0.7649593949317932
Epoch 660, training loss: 12.903779029846191 = 0.2680034935474396 + 2.0 * 6.317887783050537
Epoch 660, val loss: 0.7629376649856567
Epoch 670, training loss: 12.897027015686035 = 0.25617119669914246 + 2.0 * 6.320427894592285
Epoch 670, val loss: 0.7613235712051392
Epoch 680, training loss: 12.882040977478027 = 0.24479031562805176 + 2.0 * 6.318625450134277
Epoch 680, val loss: 0.7600637078285217
Epoch 690, training loss: 12.863913536071777 = 0.23387591540813446 + 2.0 * 6.315018653869629
Epoch 690, val loss: 0.7591879367828369
Epoch 700, training loss: 12.850346565246582 = 0.2234288603067398 + 2.0 * 6.3134589195251465
Epoch 700, val loss: 0.7587393522262573
Epoch 710, training loss: 12.840405464172363 = 0.21339833736419678 + 2.0 * 6.313503742218018
Epoch 710, val loss: 0.7587648630142212
Epoch 720, training loss: 12.826761245727539 = 0.20385265350341797 + 2.0 * 6.3114542961120605
Epoch 720, val loss: 0.7591329216957092
Epoch 730, training loss: 12.816590309143066 = 0.1947856843471527 + 2.0 * 6.310902118682861
Epoch 730, val loss: 0.7598730325698853
Epoch 740, training loss: 12.805739402770996 = 0.18617862462997437 + 2.0 * 6.309780597686768
Epoch 740, val loss: 0.7609301805496216
Epoch 750, training loss: 12.794352531433105 = 0.17797298729419708 + 2.0 * 6.308189868927002
Epoch 750, val loss: 0.762373149394989
Epoch 760, training loss: 12.786853790283203 = 0.1701831966638565 + 2.0 * 6.308335304260254
Epoch 760, val loss: 0.7641287446022034
Epoch 770, training loss: 12.777799606323242 = 0.16277556121349335 + 2.0 * 6.307511806488037
Epoch 770, val loss: 0.7662217020988464
Epoch 780, training loss: 12.767678260803223 = 0.15580151975154877 + 2.0 * 6.305938243865967
Epoch 780, val loss: 0.7684999704360962
Epoch 790, training loss: 12.756818771362305 = 0.14917197823524475 + 2.0 * 6.303823471069336
Epoch 790, val loss: 0.7710675001144409
Epoch 800, training loss: 12.75246524810791 = 0.1428651362657547 + 2.0 * 6.304800033569336
Epoch 800, val loss: 0.7738881707191467
Epoch 810, training loss: 12.763442039489746 = 0.13687433302402496 + 2.0 * 6.313283920288086
Epoch 810, val loss: 0.7768869400024414
Epoch 820, training loss: 12.73398494720459 = 0.13123708963394165 + 2.0 * 6.3013739585876465
Epoch 820, val loss: 0.779970645904541
Epoch 830, training loss: 12.725547790527344 = 0.12588201463222504 + 2.0 * 6.299832820892334
Epoch 830, val loss: 0.7832011580467224
Epoch 840, training loss: 12.718052864074707 = 0.12076909840106964 + 2.0 * 6.298641681671143
Epoch 840, val loss: 0.7866550087928772
Epoch 850, training loss: 12.716084480285645 = 0.11588927358388901 + 2.0 * 6.300097465515137
Epoch 850, val loss: 0.7902783155441284
Epoch 860, training loss: 12.706892013549805 = 0.11125080287456512 + 2.0 * 6.297820568084717
Epoch 860, val loss: 0.7940131425857544
Epoch 870, training loss: 12.702800750732422 = 0.10684661567211151 + 2.0 * 6.297976970672607
Epoch 870, val loss: 0.7978019714355469
Epoch 880, training loss: 12.693115234375 = 0.1026606336236 + 2.0 * 6.295227527618408
Epoch 880, val loss: 0.8016791939735413
Epoch 890, training loss: 12.69033145904541 = 0.09864717721939087 + 2.0 * 6.295842170715332
Epoch 890, val loss: 0.8056573867797852
Epoch 900, training loss: 12.68393325805664 = 0.09483989328145981 + 2.0 * 6.294546604156494
Epoch 900, val loss: 0.8097153306007385
Epoch 910, training loss: 12.681639671325684 = 0.09122910350561142 + 2.0 * 6.295205116271973
Epoch 910, val loss: 0.8136914968490601
Epoch 920, training loss: 12.672929763793945 = 0.08779434859752655 + 2.0 * 6.292567729949951
Epoch 920, val loss: 0.8177332282066345
Epoch 930, training loss: 12.670013427734375 = 0.08450913429260254 + 2.0 * 6.292752265930176
Epoch 930, val loss: 0.8218922019004822
Epoch 940, training loss: 12.66278076171875 = 0.0813671126961708 + 2.0 * 6.290706634521484
Epoch 940, val loss: 0.8260561227798462
Epoch 950, training loss: 12.65871524810791 = 0.07837746292352676 + 2.0 * 6.290168762207031
Epoch 950, val loss: 0.830176830291748
Epoch 960, training loss: 12.657739639282227 = 0.07551062852144241 + 2.0 * 6.291114330291748
Epoch 960, val loss: 0.8343282341957092
Epoch 970, training loss: 12.651124000549316 = 0.07277967035770416 + 2.0 * 6.289172172546387
Epoch 970, val loss: 0.8385950326919556
Epoch 980, training loss: 12.64897632598877 = 0.0701841339468956 + 2.0 * 6.289396286010742
Epoch 980, val loss: 0.8427072167396545
Epoch 990, training loss: 12.64284610748291 = 0.0676930770277977 + 2.0 * 6.287576675415039
Epoch 990, val loss: 0.8469259142875671
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8074074074074075
0.8360569319978914
=== training gcn model ===
Epoch 0, training loss: 19.166051864624023 = 1.972407579421997 + 2.0 * 8.596821784973145
Epoch 0, val loss: 1.972306489944458
Epoch 10, training loss: 19.153247833251953 = 1.9605826139450073 + 2.0 * 8.596332550048828
Epoch 10, val loss: 1.9612268209457397
Epoch 20, training loss: 19.129444122314453 = 1.9457660913467407 + 2.0 * 8.591838836669922
Epoch 20, val loss: 1.9468605518341064
Epoch 30, training loss: 19.049577713012695 = 1.925782561302185 + 2.0 * 8.561897277832031
Epoch 30, val loss: 1.9273629188537598
Epoch 40, training loss: 18.72097396850586 = 1.9040517807006836 + 2.0 * 8.408461570739746
Epoch 40, val loss: 1.9074243307113647
Epoch 50, training loss: 18.119173049926758 = 1.8835622072219849 + 2.0 * 8.117805480957031
Epoch 50, val loss: 1.8886942863464355
Epoch 60, training loss: 17.537511825561523 = 1.8658303022384644 + 2.0 * 7.835840702056885
Epoch 60, val loss: 1.87279212474823
Epoch 70, training loss: 16.55733299255371 = 1.8502219915390015 + 2.0 * 7.353555202484131
Epoch 70, val loss: 1.8579272031784058
Epoch 80, training loss: 15.980998039245605 = 1.8351733684539795 + 2.0 * 7.072912216186523
Epoch 80, val loss: 1.8438557386398315
Epoch 90, training loss: 15.663143157958984 = 1.8203281164169312 + 2.0 * 6.921407699584961
Epoch 90, val loss: 1.8298207521438599
Epoch 100, training loss: 15.462503433227539 = 1.8046772480010986 + 2.0 * 6.82891321182251
Epoch 100, val loss: 1.8151508569717407
Epoch 110, training loss: 15.307977676391602 = 1.7890703678131104 + 2.0 * 6.759453773498535
Epoch 110, val loss: 1.8006138801574707
Epoch 120, training loss: 15.17074966430664 = 1.7751322984695435 + 2.0 * 6.697808742523193
Epoch 120, val loss: 1.7872151136398315
Epoch 130, training loss: 15.062609672546387 = 1.7616082429885864 + 2.0 * 6.650500774383545
Epoch 130, val loss: 1.7740662097930908
Epoch 140, training loss: 14.974800109863281 = 1.7468476295471191 + 2.0 * 6.61397647857666
Epoch 140, val loss: 1.7599310874938965
Epoch 150, training loss: 14.90073299407959 = 1.7303584814071655 + 2.0 * 6.5851874351501465
Epoch 150, val loss: 1.7445248365402222
Epoch 160, training loss: 14.827491760253906 = 1.7121976613998413 + 2.0 * 6.557647228240967
Epoch 160, val loss: 1.7277867794036865
Epoch 170, training loss: 14.767470359802246 = 1.6918914318084717 + 2.0 * 6.537789344787598
Epoch 170, val loss: 1.7092595100402832
Epoch 180, training loss: 14.701505661010742 = 1.6691129207611084 + 2.0 * 6.516196250915527
Epoch 180, val loss: 1.6886917352676392
Epoch 190, training loss: 14.64095401763916 = 1.6435422897338867 + 2.0 * 6.498705863952637
Epoch 190, val loss: 1.6657315492630005
Epoch 200, training loss: 14.581358909606934 = 1.6149863004684448 + 2.0 * 6.4831862449646
Epoch 200, val loss: 1.640365481376648
Epoch 210, training loss: 14.52114200592041 = 1.5837005376815796 + 2.0 * 6.46872091293335
Epoch 210, val loss: 1.6127828359603882
Epoch 220, training loss: 14.462642669677734 = 1.5498992204666138 + 2.0 * 6.456371784210205
Epoch 220, val loss: 1.5832452774047852
Epoch 230, training loss: 14.403491973876953 = 1.5140544176101685 + 2.0 * 6.444718837738037
Epoch 230, val loss: 1.5523004531860352
Epoch 240, training loss: 14.347272872924805 = 1.476436972618103 + 2.0 * 6.435418128967285
Epoch 240, val loss: 1.5201911926269531
Epoch 250, training loss: 14.304603576660156 = 1.4376643896102905 + 2.0 * 6.433469772338867
Epoch 250, val loss: 1.4877171516418457
Epoch 260, training loss: 14.236010551452637 = 1.398935317993164 + 2.0 * 6.418537616729736
Epoch 260, val loss: 1.4555951356887817
Epoch 270, training loss: 14.186046600341797 = 1.3604178428649902 + 2.0 * 6.412814617156982
Epoch 270, val loss: 1.4243218898773193
Epoch 280, training loss: 14.136086463928223 = 1.322691798210144 + 2.0 * 6.4066972732543945
Epoch 280, val loss: 1.394202709197998
Epoch 290, training loss: 14.083122253417969 = 1.2861762046813965 + 2.0 * 6.398473262786865
Epoch 290, val loss: 1.3656299114227295
Epoch 300, training loss: 14.040310859680176 = 1.250756025314331 + 2.0 * 6.394777297973633
Epoch 300, val loss: 1.3383294343948364
Epoch 310, training loss: 13.990487098693848 = 1.2165453433990479 + 2.0 * 6.3869709968566895
Epoch 310, val loss: 1.3123284578323364
Epoch 320, training loss: 13.946834564208984 = 1.1833164691925049 + 2.0 * 6.381759166717529
Epoch 320, val loss: 1.2872326374053955
Epoch 330, training loss: 13.908415794372559 = 1.1509549617767334 + 2.0 * 6.378730297088623
Epoch 330, val loss: 1.2630668878555298
Epoch 340, training loss: 13.866111755371094 = 1.1195505857467651 + 2.0 * 6.3732805252075195
Epoch 340, val loss: 1.2396403551101685
Epoch 350, training loss: 13.82426929473877 = 1.088627576828003 + 2.0 * 6.367820739746094
Epoch 350, val loss: 1.2165838479995728
Epoch 360, training loss: 13.784208297729492 = 1.057929277420044 + 2.0 * 6.363139629364014
Epoch 360, val loss: 1.1937538385391235
Epoch 370, training loss: 13.763678550720215 = 1.027459740638733 + 2.0 * 6.368109226226807
Epoch 370, val loss: 1.1711519956588745
Epoch 380, training loss: 13.710522651672363 = 0.9975519180297852 + 2.0 * 6.356485366821289
Epoch 380, val loss: 1.1488646268844604
Epoch 390, training loss: 13.674428939819336 = 0.9680895805358887 + 2.0 * 6.3531694412231445
Epoch 390, val loss: 1.12703537940979
Epoch 400, training loss: 13.641288757324219 = 0.9391224980354309 + 2.0 * 6.351083278656006
Epoch 400, val loss: 1.105482578277588
Epoch 410, training loss: 13.604982376098633 = 0.910696268081665 + 2.0 * 6.347143173217773
Epoch 410, val loss: 1.0844172239303589
Epoch 420, training loss: 13.571039199829102 = 0.8827226161956787 + 2.0 * 6.344158172607422
Epoch 420, val loss: 1.0638331174850464
Epoch 430, training loss: 13.541818618774414 = 0.8552347421646118 + 2.0 * 6.343291759490967
Epoch 430, val loss: 1.0438083410263062
Epoch 440, training loss: 13.518935203552246 = 0.828409731388092 + 2.0 * 6.34526252746582
Epoch 440, val loss: 1.024173378944397
Epoch 450, training loss: 13.475883483886719 = 0.8019588589668274 + 2.0 * 6.3369622230529785
Epoch 450, val loss: 1.005250096321106
Epoch 460, training loss: 13.44382381439209 = 0.7762214541435242 + 2.0 * 6.33380126953125
Epoch 460, val loss: 0.9869652390480042
Epoch 470, training loss: 13.413485527038574 = 0.7509471774101257 + 2.0 * 6.331269264221191
Epoch 470, val loss: 0.969271183013916
Epoch 480, training loss: 13.393821716308594 = 0.7261065244674683 + 2.0 * 6.333857536315918
Epoch 480, val loss: 0.9522424936294556
Epoch 490, training loss: 13.365813255310059 = 0.7018574476242065 + 2.0 * 6.331977844238281
Epoch 490, val loss: 0.9360215067863464
Epoch 500, training loss: 13.331413269042969 = 0.6783468127250671 + 2.0 * 6.326533317565918
Epoch 500, val loss: 0.9208062887191772
Epoch 510, training loss: 13.302804946899414 = 0.6555304527282715 + 2.0 * 6.32363748550415
Epoch 510, val loss: 0.9065222144126892
Epoch 520, training loss: 13.280460357666016 = 0.633342444896698 + 2.0 * 6.323558807373047
Epoch 520, val loss: 0.8931917548179626
Epoch 530, training loss: 13.25629997253418 = 0.6118612885475159 + 2.0 * 6.322219371795654
Epoch 530, val loss: 0.8808371424674988
Epoch 540, training loss: 13.231884002685547 = 0.5912078619003296 + 2.0 * 6.320338249206543
Epoch 540, val loss: 0.8695273995399475
Epoch 550, training loss: 13.205484390258789 = 0.5712583065032959 + 2.0 * 6.317112922668457
Epoch 550, val loss: 0.8591955900192261
Epoch 560, training loss: 13.182173728942871 = 0.551879346370697 + 2.0 * 6.315147399902344
Epoch 560, val loss: 0.8497961759567261
Epoch 570, training loss: 13.163726806640625 = 0.5331230759620667 + 2.0 * 6.315301895141602
Epoch 570, val loss: 0.8411789536476135
Epoch 580, training loss: 13.140315055847168 = 0.5150191783905029 + 2.0 * 6.312647819519043
Epoch 580, val loss: 0.8333908915519714
Epoch 590, training loss: 13.120097160339355 = 0.49738794565200806 + 2.0 * 6.311354637145996
Epoch 590, val loss: 0.82623291015625
Epoch 600, training loss: 13.102008819580078 = 0.4801180064678192 + 2.0 * 6.310945510864258
Epoch 600, val loss: 0.8195946216583252
Epoch 610, training loss: 13.08071231842041 = 0.46322599053382874 + 2.0 * 6.308743000030518
Epoch 610, val loss: 0.8134583830833435
Epoch 620, training loss: 13.060700416564941 = 0.44671523571014404 + 2.0 * 6.306992530822754
Epoch 620, val loss: 0.8077738285064697
Epoch 630, training loss: 13.049999237060547 = 0.43050694465637207 + 2.0 * 6.309746265411377
Epoch 630, val loss: 0.8024606704711914
Epoch 640, training loss: 13.025588989257812 = 0.4145275354385376 + 2.0 * 6.305530548095703
Epoch 640, val loss: 0.7974671721458435
Epoch 650, training loss: 13.013754844665527 = 0.39887842535972595 + 2.0 * 6.307438373565674
Epoch 650, val loss: 0.7928268313407898
Epoch 660, training loss: 12.989233016967773 = 0.3835330009460449 + 2.0 * 6.302849769592285
Epoch 660, val loss: 0.7885286808013916
Epoch 670, training loss: 12.971490859985352 = 0.3684636652469635 + 2.0 * 6.301513671875
Epoch 670, val loss: 0.7844937443733215
Epoch 680, training loss: 12.960626602172852 = 0.3536572754383087 + 2.0 * 6.3034844398498535
Epoch 680, val loss: 0.7807708382606506
Epoch 690, training loss: 12.940092086791992 = 0.33913296461105347 + 2.0 * 6.300479412078857
Epoch 690, val loss: 0.777238667011261
Epoch 700, training loss: 12.926676750183105 = 0.3249701261520386 + 2.0 * 6.300853252410889
Epoch 700, val loss: 0.7740365266799927
Epoch 710, training loss: 12.904414176940918 = 0.3111584782600403 + 2.0 * 6.296627998352051
Epoch 710, val loss: 0.7710915207862854
Epoch 720, training loss: 12.893616676330566 = 0.2976932227611542 + 2.0 * 6.297961711883545
Epoch 720, val loss: 0.7684054374694824
Epoch 730, training loss: 12.87954044342041 = 0.2845436632633209 + 2.0 * 6.2974982261657715
Epoch 730, val loss: 0.7659536004066467
Epoch 740, training loss: 12.859904289245605 = 0.2718263268470764 + 2.0 * 6.294038772583008
Epoch 740, val loss: 0.7637993693351746
Epoch 750, training loss: 12.84480094909668 = 0.2594875693321228 + 2.0 * 6.292656898498535
Epoch 750, val loss: 0.7619045972824097
Epoch 760, training loss: 12.833556175231934 = 0.2475445717573166 + 2.0 * 6.29300594329834
Epoch 760, val loss: 0.7602110505104065
Epoch 770, training loss: 12.819451332092285 = 0.236061230301857 + 2.0 * 6.2916951179504395
Epoch 770, val loss: 0.7588979005813599
Epoch 780, training loss: 12.811944961547852 = 0.22502262890338898 + 2.0 * 6.293461322784424
Epoch 780, val loss: 0.7578496932983398
Epoch 790, training loss: 12.790765762329102 = 0.21446675062179565 + 2.0 * 6.288149356842041
Epoch 790, val loss: 0.7571815848350525
Epoch 800, training loss: 12.784093856811523 = 0.204336479306221 + 2.0 * 6.289878845214844
Epoch 800, val loss: 0.7568665742874146
Epoch 810, training loss: 12.768656730651855 = 0.1946689784526825 + 2.0 * 6.286993980407715
Epoch 810, val loss: 0.756744921207428
Epoch 820, training loss: 12.761573791503906 = 0.18549051880836487 + 2.0 * 6.288041591644287
Epoch 820, val loss: 0.7569785714149475
Epoch 830, training loss: 12.745442390441895 = 0.17675690352916718 + 2.0 * 6.2843427658081055
Epoch 830, val loss: 0.7574918270111084
Epoch 840, training loss: 12.75045394897461 = 0.16846910119056702 + 2.0 * 6.290992259979248
Epoch 840, val loss: 0.7583638429641724
Epoch 850, training loss: 12.733509063720703 = 0.1605449616909027 + 2.0 * 6.286481857299805
Epoch 850, val loss: 0.7592530250549316
Epoch 860, training loss: 12.719327926635742 = 0.15310166776180267 + 2.0 * 6.2831130027771
Epoch 860, val loss: 0.7606425285339355
Epoch 870, training loss: 12.708910942077637 = 0.14601296186447144 + 2.0 * 6.281448841094971
Epoch 870, val loss: 0.7621979117393494
Epoch 880, training loss: 12.716622352600098 = 0.13928115367889404 + 2.0 * 6.288670539855957
Epoch 880, val loss: 0.7639090418815613
Epoch 890, training loss: 12.695791244506836 = 0.13292789459228516 + 2.0 * 6.281431674957275
Epoch 890, val loss: 0.7659238576889038
Epoch 900, training loss: 12.68991470336914 = 0.12689049541950226 + 2.0 * 6.281512260437012
Epoch 900, val loss: 0.7681190967559814
Epoch 910, training loss: 12.679549217224121 = 0.1211806908249855 + 2.0 * 6.279184341430664
Epoch 910, val loss: 0.7704761028289795
Epoch 920, training loss: 12.671392440795898 = 0.11577850580215454 + 2.0 * 6.277806758880615
Epoch 920, val loss: 0.7730895280838013
Epoch 930, training loss: 12.665843963623047 = 0.11065763980150223 + 2.0 * 6.27759313583374
Epoch 930, val loss: 0.7758474349975586
Epoch 940, training loss: 12.672158241271973 = 0.10579084604978561 + 2.0 * 6.283183574676514
Epoch 940, val loss: 0.7786798477172852
Epoch 950, training loss: 12.655050277709961 = 0.1012035384774208 + 2.0 * 6.276923179626465
Epoch 950, val loss: 0.78179532289505
Epoch 960, training loss: 12.646926879882812 = 0.09684371203184128 + 2.0 * 6.275041580200195
Epoch 960, val loss: 0.7849920988082886
Epoch 970, training loss: 12.677879333496094 = 0.09272115677595139 + 2.0 * 6.292579174041748
Epoch 970, val loss: 0.7882373929023743
Epoch 980, training loss: 12.636971473693848 = 0.08880116045475006 + 2.0 * 6.27408504486084
Epoch 980, val loss: 0.7915865182876587
Epoch 990, training loss: 12.630715370178223 = 0.08510835468769073 + 2.0 * 6.27280330657959
Epoch 990, val loss: 0.7951444983482361
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7851851851851852
0.8413284132841329
=== training gcn model ===
Epoch 0, training loss: 19.141286849975586 = 1.9476386308670044 + 2.0 * 8.596823692321777
Epoch 0, val loss: 1.944214105606079
Epoch 10, training loss: 19.1304874420166 = 1.937674641609192 + 2.0 * 8.596405982971191
Epoch 10, val loss: 1.9341155290603638
Epoch 20, training loss: 19.111303329467773 = 1.9254087209701538 + 2.0 * 8.592947006225586
Epoch 20, val loss: 1.9215962886810303
Epoch 30, training loss: 19.04734992980957 = 1.9089648723602295 + 2.0 * 8.569192886352539
Epoch 30, val loss: 1.9050616025924683
Epoch 40, training loss: 18.765832901000977 = 1.8890869617462158 + 2.0 * 8.438372611999512
Epoch 40, val loss: 1.8861514329910278
Epoch 50, training loss: 17.931655883789062 = 1.8668526411056519 + 2.0 * 8.032402038574219
Epoch 50, val loss: 1.865073323249817
Epoch 60, training loss: 17.18509292602539 = 1.847419023513794 + 2.0 * 7.668837070465088
Epoch 60, val loss: 1.8477323055267334
Epoch 70, training loss: 16.358854293823242 = 1.8338358402252197 + 2.0 * 7.262508869171143
Epoch 70, val loss: 1.8360432386398315
Epoch 80, training loss: 15.836371421813965 = 1.8228386640548706 + 2.0 * 7.006766319274902
Epoch 80, val loss: 1.8262125253677368
Epoch 90, training loss: 15.584074020385742 = 1.8090966939926147 + 2.0 * 6.887488842010498
Epoch 90, val loss: 1.8133878707885742
Epoch 100, training loss: 15.413290023803711 = 1.7939884662628174 + 2.0 * 6.809650897979736
Epoch 100, val loss: 1.7998732328414917
Epoch 110, training loss: 15.281218528747559 = 1.7790883779525757 + 2.0 * 6.751065254211426
Epoch 110, val loss: 1.7869422435760498
Epoch 120, training loss: 15.163017272949219 = 1.7652008533477783 + 2.0 * 6.69890832901001
Epoch 120, val loss: 1.7745071649551392
Epoch 130, training loss: 15.066515922546387 = 1.7508602142333984 + 2.0 * 6.657827854156494
Epoch 130, val loss: 1.7614648342132568
Epoch 140, training loss: 14.978730201721191 = 1.7352875471115112 + 2.0 * 6.621721267700195
Epoch 140, val loss: 1.7476105690002441
Epoch 150, training loss: 14.902791976928711 = 1.7181086540222168 + 2.0 * 6.592341423034668
Epoch 150, val loss: 1.7326045036315918
Epoch 160, training loss: 14.83476734161377 = 1.698715329170227 + 2.0 * 6.568026065826416
Epoch 160, val loss: 1.7157262563705444
Epoch 170, training loss: 14.77687931060791 = 1.6769561767578125 + 2.0 * 6.549961566925049
Epoch 170, val loss: 1.6967906951904297
Epoch 180, training loss: 14.709293365478516 = 1.6526583433151245 + 2.0 * 6.528317451477051
Epoch 180, val loss: 1.6760690212249756
Epoch 190, training loss: 14.648236274719238 = 1.625540852546692 + 2.0 * 6.511347770690918
Epoch 190, val loss: 1.6531099081039429
Epoch 200, training loss: 14.58606243133545 = 1.5956699848175049 + 2.0 * 6.495196342468262
Epoch 200, val loss: 1.627963662147522
Epoch 210, training loss: 14.523425102233887 = 1.5628976821899414 + 2.0 * 6.480263710021973
Epoch 210, val loss: 1.6006829738616943
Epoch 220, training loss: 14.459890365600586 = 1.5275347232818604 + 2.0 * 6.466177940368652
Epoch 220, val loss: 1.5714961290359497
Epoch 230, training loss: 14.397351264953613 = 1.4897634983062744 + 2.0 * 6.453794002532959
Epoch 230, val loss: 1.5403730869293213
Epoch 240, training loss: 14.341609954833984 = 1.4498286247253418 + 2.0 * 6.4458909034729
Epoch 240, val loss: 1.5079457759857178
Epoch 250, training loss: 14.279618263244629 = 1.4094133377075195 + 2.0 * 6.435102462768555
Epoch 250, val loss: 1.4753857851028442
Epoch 260, training loss: 14.218694686889648 = 1.368125557899475 + 2.0 * 6.425284385681152
Epoch 260, val loss: 1.4425549507141113
Epoch 270, training loss: 14.16319465637207 = 1.32633376121521 + 2.0 * 6.418430328369141
Epoch 270, val loss: 1.4096957445144653
Epoch 280, training loss: 14.10236930847168 = 1.2843470573425293 + 2.0 * 6.409010887145996
Epoch 280, val loss: 1.3769447803497314
Epoch 290, training loss: 14.051055908203125 = 1.2428226470947266 + 2.0 * 6.404116630554199
Epoch 290, val loss: 1.3451826572418213
Epoch 300, training loss: 13.993566513061523 = 1.2023879289627075 + 2.0 * 6.395589351654053
Epoch 300, val loss: 1.3146337270736694
Epoch 310, training loss: 13.940129280090332 = 1.162990927696228 + 2.0 * 6.388569355010986
Epoch 310, val loss: 1.2853463888168335
Epoch 320, training loss: 13.90454387664795 = 1.1251987218856812 + 2.0 * 6.389672756195068
Epoch 320, val loss: 1.257564902305603
Epoch 330, training loss: 13.848525047302246 = 1.0894004106521606 + 2.0 * 6.3795623779296875
Epoch 330, val loss: 1.2315675020217896
Epoch 340, training loss: 13.803925514221191 = 1.0552674531936646 + 2.0 * 6.374329090118408
Epoch 340, val loss: 1.2070754766464233
Epoch 350, training loss: 13.760574340820312 = 1.0226025581359863 + 2.0 * 6.368986129760742
Epoch 350, val loss: 1.183768391609192
Epoch 360, training loss: 13.720596313476562 = 0.9911648035049438 + 2.0 * 6.364715576171875
Epoch 360, val loss: 1.1612393856048584
Epoch 370, training loss: 13.71013069152832 = 0.9607833623886108 + 2.0 * 6.374673843383789
Epoch 370, val loss: 1.1395848989486694
Epoch 380, training loss: 13.649735450744629 = 0.9319760799407959 + 2.0 * 6.358879566192627
Epoch 380, val loss: 1.118894338607788
Epoch 390, training loss: 13.614934921264648 = 0.9039775133132935 + 2.0 * 6.355478763580322
Epoch 390, val loss: 1.0988959074020386
Epoch 400, training loss: 13.57982349395752 = 0.8763718008995056 + 2.0 * 6.351726055145264
Epoch 400, val loss: 1.078961730003357
Epoch 410, training loss: 13.545591354370117 = 0.8489441275596619 + 2.0 * 6.348323822021484
Epoch 410, val loss: 1.0590133666992188
Epoch 420, training loss: 13.513729095458984 = 0.821506679058075 + 2.0 * 6.346111297607422
Epoch 420, val loss: 1.0389554500579834
Epoch 430, training loss: 13.481201171875 = 0.7940070033073425 + 2.0 * 6.343596935272217
Epoch 430, val loss: 1.0190068483352661
Epoch 440, training loss: 13.447356224060059 = 0.7665965557098389 + 2.0 * 6.34037971496582
Epoch 440, val loss: 0.9989683032035828
Epoch 450, training loss: 13.416739463806152 = 0.7390136122703552 + 2.0 * 6.338862895965576
Epoch 450, val loss: 0.9789422154426575
Epoch 460, training loss: 13.385469436645508 = 0.7114044427871704 + 2.0 * 6.337032318115234
Epoch 460, val loss: 0.9590528607368469
Epoch 470, training loss: 13.35252857208252 = 0.6840021014213562 + 2.0 * 6.334263324737549
Epoch 470, val loss: 0.939457893371582
Epoch 480, training loss: 13.320430755615234 = 0.6566945314407349 + 2.0 * 6.3318681716918945
Epoch 480, val loss: 0.9204486608505249
Epoch 490, training loss: 13.299286842346191 = 0.6296100616455078 + 2.0 * 6.334838390350342
Epoch 490, val loss: 0.901939332485199
Epoch 500, training loss: 13.259074211120605 = 0.6028182506561279 + 2.0 * 6.328127861022949
Epoch 500, val loss: 0.8840008974075317
Epoch 510, training loss: 13.227570533752441 = 0.5764134526252747 + 2.0 * 6.325578689575195
Epoch 510, val loss: 0.866994321346283
Epoch 520, training loss: 13.198298454284668 = 0.550389289855957 + 2.0 * 6.3239545822143555
Epoch 520, val loss: 0.8508157134056091
Epoch 530, training loss: 13.183591842651367 = 0.524811863899231 + 2.0 * 6.329390048980713
Epoch 530, val loss: 0.8356470465660095
Epoch 540, training loss: 13.141847610473633 = 0.49995550513267517 + 2.0 * 6.320946216583252
Epoch 540, val loss: 0.8217349648475647
Epoch 550, training loss: 13.113431930541992 = 0.47583436965942383 + 2.0 * 6.318799018859863
Epoch 550, val loss: 0.8091862797737122
Epoch 560, training loss: 13.087044715881348 = 0.4523019790649414 + 2.0 * 6.317371368408203
Epoch 560, val loss: 0.7979461550712585
Epoch 570, training loss: 13.065011978149414 = 0.42935943603515625 + 2.0 * 6.317826271057129
Epoch 570, val loss: 0.7879294157028198
Epoch 580, training loss: 13.049298286437988 = 0.4071613550186157 + 2.0 * 6.321068286895752
Epoch 580, val loss: 0.7792778015136719
Epoch 590, training loss: 13.014976501464844 = 0.38589218258857727 + 2.0 * 6.314542293548584
Epoch 590, val loss: 0.7718998193740845
Epoch 600, training loss: 12.98900318145752 = 0.3655703067779541 + 2.0 * 6.311716556549072
Epoch 600, val loss: 0.7659333348274231
Epoch 610, training loss: 12.966341972351074 = 0.3460647761821747 + 2.0 * 6.310138702392578
Epoch 610, val loss: 0.7612544894218445
Epoch 620, training loss: 12.963168144226074 = 0.32743966579437256 + 2.0 * 6.317864418029785
Epoch 620, val loss: 0.7578201293945312
Epoch 630, training loss: 12.924741744995117 = 0.3098943531513214 + 2.0 * 6.3074235916137695
Epoch 630, val loss: 0.7555480003356934
Epoch 640, training loss: 12.906614303588867 = 0.29326704144477844 + 2.0 * 6.306673526763916
Epoch 640, val loss: 0.7544640302658081
Epoch 650, training loss: 12.88734245300293 = 0.27749642729759216 + 2.0 * 6.304923057556152
Epoch 650, val loss: 0.7543357014656067
Epoch 660, training loss: 12.88780689239502 = 0.2626173198223114 + 2.0 * 6.312594890594482
Epoch 660, val loss: 0.7551252841949463
Epoch 670, training loss: 12.854907035827637 = 0.2487097680568695 + 2.0 * 6.303098678588867
Epoch 670, val loss: 0.7566702365875244
Epoch 680, training loss: 12.837440490722656 = 0.23559819161891937 + 2.0 * 6.3009209632873535
Epoch 680, val loss: 0.7590876817703247
Epoch 690, training loss: 12.8244047164917 = 0.22324791550636292 + 2.0 * 6.300578594207764
Epoch 690, val loss: 0.7620876431465149
Epoch 700, training loss: 12.812240600585938 = 0.2116425633430481 + 2.0 * 6.300299167633057
Epoch 700, val loss: 0.7656847834587097
Epoch 710, training loss: 12.798592567443848 = 0.20074789226055145 + 2.0 * 6.298922538757324
Epoch 710, val loss: 0.7696495056152344
Epoch 720, training loss: 12.78388786315918 = 0.19051727652549744 + 2.0 * 6.296685218811035
Epoch 720, val loss: 0.7741338014602661
Epoch 730, training loss: 12.77370548248291 = 0.1808987706899643 + 2.0 * 6.296403408050537
Epoch 730, val loss: 0.7788852453231812
Epoch 740, training loss: 12.766364097595215 = 0.17184212803840637 + 2.0 * 6.297260761260986
Epoch 740, val loss: 0.7840083837509155
Epoch 750, training loss: 12.750657081604004 = 0.16338442265987396 + 2.0 * 6.293636322021484
Epoch 750, val loss: 0.7894149422645569
Epoch 760, training loss: 12.740676879882812 = 0.15541449189186096 + 2.0 * 6.292631149291992
Epoch 760, val loss: 0.7950412631034851
Epoch 770, training loss: 12.730053901672363 = 0.14792075753211975 + 2.0 * 6.291066646575928
Epoch 770, val loss: 0.8008536696434021
Epoch 780, training loss: 12.73289680480957 = 0.14085806906223297 + 2.0 * 6.296019554138184
Epoch 780, val loss: 0.8068863749504089
Epoch 790, training loss: 12.710700035095215 = 0.13419659435749054 + 2.0 * 6.288251876831055
Epoch 790, val loss: 0.8130866289138794
Epoch 800, training loss: 12.703104019165039 = 0.1279255896806717 + 2.0 * 6.287589073181152
Epoch 800, val loss: 0.8194323778152466
Epoch 810, training loss: 12.696476936340332 = 0.12200171500444412 + 2.0 * 6.287237644195557
Epoch 810, val loss: 0.8258535861968994
Epoch 820, training loss: 12.693264961242676 = 0.11639829725027084 + 2.0 * 6.28843355178833
Epoch 820, val loss: 0.832379937171936
Epoch 830, training loss: 12.687336921691895 = 0.11115328222513199 + 2.0 * 6.288091659545898
Epoch 830, val loss: 0.8389127850532532
Epoch 840, training loss: 12.673308372497559 = 0.10620971769094467 + 2.0 * 6.2835493087768555
Epoch 840, val loss: 0.8455923795700073
Epoch 850, training loss: 12.667288780212402 = 0.10153478384017944 + 2.0 * 6.282876968383789
Epoch 850, val loss: 0.8522670269012451
Epoch 860, training loss: 12.675387382507324 = 0.09710130095481873 + 2.0 * 6.289143085479736
Epoch 860, val loss: 0.8590354323387146
Epoch 870, training loss: 12.655841827392578 = 0.09291817992925644 + 2.0 * 6.281461715698242
Epoch 870, val loss: 0.8657776713371277
Epoch 880, training loss: 12.648740768432617 = 0.0889642983675003 + 2.0 * 6.279888153076172
Epoch 880, val loss: 0.8725700974464417
Epoch 890, training loss: 12.648913383483887 = 0.08522190898656845 + 2.0 * 6.281845569610596
Epoch 890, val loss: 0.8794172406196594
Epoch 900, training loss: 12.639941215515137 = 0.08166944235563278 + 2.0 * 6.279135704040527
Epoch 900, val loss: 0.8861672878265381
Epoch 910, training loss: 12.634946823120117 = 0.07832498848438263 + 2.0 * 6.278310775756836
Epoch 910, val loss: 0.8930907249450684
Epoch 920, training loss: 12.629129409790039 = 0.0751504898071289 + 2.0 * 6.276989459991455
Epoch 920, val loss: 0.8998247981071472
Epoch 930, training loss: 12.626138687133789 = 0.07214963436126709 + 2.0 * 6.276994705200195
Epoch 930, val loss: 0.9066334366798401
Epoch 940, training loss: 12.62143325805664 = 0.06928914040327072 + 2.0 * 6.276072025299072
Epoch 940, val loss: 0.9134451746940613
Epoch 950, training loss: 12.61530876159668 = 0.0665961280465126 + 2.0 * 6.274356365203857
Epoch 950, val loss: 0.9202653169631958
Epoch 960, training loss: 12.613805770874023 = 0.0640338733792305 + 2.0 * 6.274886131286621
Epoch 960, val loss: 0.9270373582839966
Epoch 970, training loss: 12.611320495605469 = 0.06160188093781471 + 2.0 * 6.274859428405762
Epoch 970, val loss: 0.9337558150291443
Epoch 980, training loss: 12.603281021118164 = 0.05929882824420929 + 2.0 * 6.27199125289917
Epoch 980, val loss: 0.9405202269554138
Epoch 990, training loss: 12.60472297668457 = 0.05711381882429123 + 2.0 * 6.273804664611816
Epoch 990, val loss: 0.94722980260849
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7962962962962963
0.8334211913547708
The final CL Acc:0.79630, 0.00907, The final GNN Acc:0.83694, 0.00329
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.3 der2:0.3 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11598])
remove edge: torch.Size([2, 9386])
updated graph: torch.Size([2, 10428])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 19.134536743164062 = 1.9409154653549194 + 2.0 * 8.596810340881348
Epoch 0, val loss: 1.9436322450637817
Epoch 10, training loss: 19.123855590820312 = 1.9310835599899292 + 2.0 * 8.596385955810547
Epoch 10, val loss: 1.9335306882858276
Epoch 20, training loss: 19.10586929321289 = 1.9190795421600342 + 2.0 * 8.593395233154297
Epoch 20, val loss: 1.920985460281372
Epoch 30, training loss: 19.045780181884766 = 1.9031214714050293 + 2.0 * 8.571329116821289
Epoch 30, val loss: 1.9043924808502197
Epoch 40, training loss: 18.765933990478516 = 1.8844724893569946 + 2.0 * 8.440731048583984
Epoch 40, val loss: 1.8859639167785645
Epoch 50, training loss: 17.76041030883789 = 1.8662391901016235 + 2.0 * 7.947085380554199
Epoch 50, val loss: 1.8684816360473633
Epoch 60, training loss: 16.745628356933594 = 1.851757287979126 + 2.0 * 7.446935176849365
Epoch 60, val loss: 1.8553848266601562
Epoch 70, training loss: 16.069843292236328 = 1.8399404287338257 + 2.0 * 7.1149516105651855
Epoch 70, val loss: 1.8437955379486084
Epoch 80, training loss: 15.658432006835938 = 1.8297992944717407 + 2.0 * 6.914316177368164
Epoch 80, val loss: 1.8339200019836426
Epoch 90, training loss: 15.425933837890625 = 1.8195676803588867 + 2.0 * 6.803183078765869
Epoch 90, val loss: 1.8241039514541626
Epoch 100, training loss: 15.277154922485352 = 1.8093889951705933 + 2.0 * 6.733882904052734
Epoch 100, val loss: 1.8144112825393677
Epoch 110, training loss: 15.157973289489746 = 1.7993155717849731 + 2.0 * 6.679328918457031
Epoch 110, val loss: 1.8049336671829224
Epoch 120, training loss: 15.06115436553955 = 1.7896744012832642 + 2.0 * 6.635739803314209
Epoch 120, val loss: 1.7958080768585205
Epoch 130, training loss: 14.981488227844238 = 1.7801870107650757 + 2.0 * 6.600650787353516
Epoch 130, val loss: 1.786792516708374
Epoch 140, training loss: 14.920169830322266 = 1.7703721523284912 + 2.0 * 6.574898719787598
Epoch 140, val loss: 1.77761709690094
Epoch 150, training loss: 14.861064910888672 = 1.7597624063491821 + 2.0 * 6.5506510734558105
Epoch 150, val loss: 1.768104076385498
Epoch 160, training loss: 14.809889793395996 = 1.7480933666229248 + 2.0 * 6.530898094177246
Epoch 160, val loss: 1.7580337524414062
Epoch 170, training loss: 14.763132095336914 = 1.7350903749465942 + 2.0 * 6.514020919799805
Epoch 170, val loss: 1.7471024990081787
Epoch 180, training loss: 14.71546745300293 = 1.7204594612121582 + 2.0 * 6.497504234313965
Epoch 180, val loss: 1.7350703477859497
Epoch 190, training loss: 14.670677185058594 = 1.7038836479187012 + 2.0 * 6.483397006988525
Epoch 190, val loss: 1.7216664552688599
Epoch 200, training loss: 14.628228187561035 = 1.6850134134292603 + 2.0 * 6.471607208251953
Epoch 200, val loss: 1.7066147327423096
Epoch 210, training loss: 14.581110954284668 = 1.6635407209396362 + 2.0 * 6.458785057067871
Epoch 210, val loss: 1.6896241903305054
Epoch 220, training loss: 14.534762382507324 = 1.6388427019119263 + 2.0 * 6.447959899902344
Epoch 220, val loss: 1.6702094078063965
Epoch 230, training loss: 14.487899780273438 = 1.6109063625335693 + 2.0 * 6.4384965896606445
Epoch 230, val loss: 1.6481831073760986
Epoch 240, training loss: 14.440948486328125 = 1.5792207717895508 + 2.0 * 6.430863857269287
Epoch 240, val loss: 1.6232985258102417
Epoch 250, training loss: 14.390993118286133 = 1.5440027713775635 + 2.0 * 6.423495292663574
Epoch 250, val loss: 1.5957361459732056
Epoch 260, training loss: 14.334943771362305 = 1.5052698850631714 + 2.0 * 6.414836883544922
Epoch 260, val loss: 1.5657074451446533
Epoch 270, training loss: 14.279596328735352 = 1.4630826711654663 + 2.0 * 6.408257007598877
Epoch 270, val loss: 1.5331964492797852
Epoch 280, training loss: 14.233925819396973 = 1.4181108474731445 + 2.0 * 6.407907485961914
Epoch 280, val loss: 1.498894214630127
Epoch 290, training loss: 14.166072845458984 = 1.371777892112732 + 2.0 * 6.3971476554870605
Epoch 290, val loss: 1.4639290571212769
Epoch 300, training loss: 14.109790802001953 = 1.3246288299560547 + 2.0 * 6.392580986022949
Epoch 300, val loss: 1.4289478063583374
Epoch 310, training loss: 14.06082534790039 = 1.2772674560546875 + 2.0 * 6.391778945922852
Epoch 310, val loss: 1.3944644927978516
Epoch 320, training loss: 13.997823715209961 = 1.230782389640808 + 2.0 * 6.383520603179932
Epoch 320, val loss: 1.3613890409469604
Epoch 330, training loss: 13.944108963012695 = 1.1853704452514648 + 2.0 * 6.379369258880615
Epoch 330, val loss: 1.3298050165176392
Epoch 340, training loss: 13.892803192138672 = 1.1410130262374878 + 2.0 * 6.375895023345947
Epoch 340, val loss: 1.2996488809585571
Epoch 350, training loss: 13.845358848571777 = 1.097922444343567 + 2.0 * 6.37371826171875
Epoch 350, val loss: 1.2710682153701782
Epoch 360, training loss: 13.797443389892578 = 1.0560457706451416 + 2.0 * 6.370698928833008
Epoch 360, val loss: 1.244002342224121
Epoch 370, training loss: 13.75347900390625 = 1.0156975984573364 + 2.0 * 6.368890762329102
Epoch 370, val loss: 1.2180664539337158
Epoch 380, training loss: 13.702753067016602 = 0.9764357209205627 + 2.0 * 6.363158702850342
Epoch 380, val loss: 1.1934045553207397
Epoch 390, training loss: 13.657296180725098 = 0.9380782842636108 + 2.0 * 6.359609127044678
Epoch 390, val loss: 1.169610619544983
Epoch 400, training loss: 13.616113662719727 = 0.9005272388458252 + 2.0 * 6.35779333114624
Epoch 400, val loss: 1.1464654207229614
Epoch 410, training loss: 13.577919006347656 = 0.8642393350601196 + 2.0 * 6.356839656829834
Epoch 410, val loss: 1.124456763267517
Epoch 420, training loss: 13.533703804016113 = 0.8291065096855164 + 2.0 * 6.352298736572266
Epoch 420, val loss: 1.1036455631256104
Epoch 430, training loss: 13.495780944824219 = 0.7950848937034607 + 2.0 * 6.350347995758057
Epoch 430, val loss: 1.083805799484253
Epoch 440, training loss: 13.46357536315918 = 0.7623230218887329 + 2.0 * 6.350625991821289
Epoch 440, val loss: 1.0651648044586182
Epoch 450, training loss: 13.42148494720459 = 0.73097825050354 + 2.0 * 6.3452534675598145
Epoch 450, val loss: 1.04804265499115
Epoch 460, training loss: 13.385504722595215 = 0.7008569240570068 + 2.0 * 6.3423237800598145
Epoch 460, val loss: 1.032203197479248
Epoch 470, training loss: 13.356081008911133 = 0.6719911694526672 + 2.0 * 6.342044830322266
Epoch 470, val loss: 1.0177103281021118
Epoch 480, training loss: 13.321545600891113 = 0.6446741819381714 + 2.0 * 6.338435649871826
Epoch 480, val loss: 1.0050212144851685
Epoch 490, training loss: 13.28999137878418 = 0.6185694932937622 + 2.0 * 6.3357110023498535
Epoch 490, val loss: 0.9938240051269531
Epoch 500, training loss: 13.264630317687988 = 0.5936183333396912 + 2.0 * 6.335505962371826
Epoch 500, val loss: 0.9839996099472046
Epoch 510, training loss: 13.242612838745117 = 0.5698984265327454 + 2.0 * 6.336357116699219
Epoch 510, val loss: 0.9756482243537903
Epoch 520, training loss: 13.211009979248047 = 0.547271192073822 + 2.0 * 6.331869602203369
Epoch 520, val loss: 0.9688591957092285
Epoch 530, training loss: 13.18255615234375 = 0.5256780385971069 + 2.0 * 6.328439235687256
Epoch 530, val loss: 0.9632090330123901
Epoch 540, training loss: 13.156669616699219 = 0.5048767924308777 + 2.0 * 6.325896263122559
Epoch 540, val loss: 0.9587299227714539
Epoch 550, training loss: 13.157310485839844 = 0.48481127619743347 + 2.0 * 6.336249828338623
Epoch 550, val loss: 0.9551652669906616
Epoch 560, training loss: 13.11148738861084 = 0.4654269516468048 + 2.0 * 6.3230299949646
Epoch 560, val loss: 0.9524833559989929
Epoch 570, training loss: 13.088367462158203 = 0.44667771458625793 + 2.0 * 6.320844650268555
Epoch 570, val loss: 0.9507139921188354
Epoch 580, training loss: 13.066761016845703 = 0.42831599712371826 + 2.0 * 6.319222450256348
Epoch 580, val loss: 0.9493789076805115
Epoch 590, training loss: 13.061882972717285 = 0.41028645634651184 + 2.0 * 6.325798034667969
Epoch 590, val loss: 0.9485905170440674
Epoch 600, training loss: 13.031938552856445 = 0.3927449882030487 + 2.0 * 6.319596767425537
Epoch 600, val loss: 0.9485037326812744
Epoch 610, training loss: 13.006266593933105 = 0.3755069375038147 + 2.0 * 6.315379619598389
Epoch 610, val loss: 0.9489744901657104
Epoch 620, training loss: 12.984930992126465 = 0.3584902286529541 + 2.0 * 6.313220500946045
Epoch 620, val loss: 0.949763298034668
Epoch 630, training loss: 12.989356994628906 = 0.3417014479637146 + 2.0 * 6.323827743530273
Epoch 630, val loss: 0.9510031342506409
Epoch 640, training loss: 12.951372146606445 = 0.3254280090332031 + 2.0 * 6.312972068786621
Epoch 640, val loss: 0.952651858329773
Epoch 650, training loss: 12.931548118591309 = 0.3095424771308899 + 2.0 * 6.311002731323242
Epoch 650, val loss: 0.955024003982544
Epoch 660, training loss: 12.911710739135742 = 0.2940746247768402 + 2.0 * 6.3088178634643555
Epoch 660, val loss: 0.9575984477996826
Epoch 670, training loss: 12.902884483337402 = 0.27904629707336426 + 2.0 * 6.311919212341309
Epoch 670, val loss: 0.9606078863143921
Epoch 680, training loss: 12.884724617004395 = 0.2645716071128845 + 2.0 * 6.310076713562012
Epoch 680, val loss: 0.9642581343650818
Epoch 690, training loss: 12.863360404968262 = 0.2507340610027313 + 2.0 * 6.3063130378723145
Epoch 690, val loss: 0.9683700203895569
Epoch 700, training loss: 12.846335411071777 = 0.23751486837863922 + 2.0 * 6.304410457611084
Epoch 700, val loss: 0.9729756712913513
Epoch 710, training loss: 12.834259033203125 = 0.22488106787204742 + 2.0 * 6.304688930511475
Epoch 710, val loss: 0.9779515266418457
Epoch 720, training loss: 12.818513870239258 = 0.21288800239562988 + 2.0 * 6.3028130531311035
Epoch 720, val loss: 0.9834206104278564
Epoch 730, training loss: 12.80348014831543 = 0.20152521133422852 + 2.0 * 6.3009772300720215
Epoch 730, val loss: 0.9893324971199036
Epoch 740, training loss: 12.795642852783203 = 0.19079475104808807 + 2.0 * 6.30242395401001
Epoch 740, val loss: 0.9956731796264648
Epoch 750, training loss: 12.77946949005127 = 0.18070851266384125 + 2.0 * 6.299380302429199
Epoch 750, val loss: 1.0022988319396973
Epoch 760, training loss: 12.769041061401367 = 0.1711743324995041 + 2.0 * 6.298933506011963
Epoch 760, val loss: 1.0093079805374146
Epoch 770, training loss: 12.770110130310059 = 0.16225922107696533 + 2.0 * 6.303925514221191
Epoch 770, val loss: 1.0163345336914062
Epoch 780, training loss: 12.74731731414795 = 0.15394040942192078 + 2.0 * 6.296688556671143
Epoch 780, val loss: 1.024044394493103
Epoch 790, training loss: 12.73552131652832 = 0.1461726874113083 + 2.0 * 6.294674396514893
Epoch 790, val loss: 1.0318450927734375
Epoch 800, training loss: 12.726009368896484 = 0.1388632357120514 + 2.0 * 6.293572902679443
Epoch 800, val loss: 1.0397437810897827
Epoch 810, training loss: 12.723382949829102 = 0.13198880851268768 + 2.0 * 6.295697212219238
Epoch 810, val loss: 1.047908067703247
Epoch 820, training loss: 12.710522651672363 = 0.12557826936244965 + 2.0 * 6.2924723625183105
Epoch 820, val loss: 1.0563099384307861
Epoch 830, training loss: 12.703317642211914 = 0.11955558508634567 + 2.0 * 6.291881084442139
Epoch 830, val loss: 1.064804196357727
Epoch 840, training loss: 12.70479965209961 = 0.11392609030008316 + 2.0 * 6.295436859130859
Epoch 840, val loss: 1.0732876062393188
Epoch 850, training loss: 12.688608169555664 = 0.10864249616861343 + 2.0 * 6.289982795715332
Epoch 850, val loss: 1.0820525884628296
Epoch 860, training loss: 12.680876731872559 = 0.10368816554546356 + 2.0 * 6.2885942459106445
Epoch 860, val loss: 1.090713620185852
Epoch 870, training loss: 12.689483642578125 = 0.09902837872505188 + 2.0 * 6.295227527618408
Epoch 870, val loss: 1.0992918014526367
Epoch 880, training loss: 12.668835639953613 = 0.09468627721071243 + 2.0 * 6.287074565887451
Epoch 880, val loss: 1.107992172241211
Epoch 890, training loss: 12.664398193359375 = 0.09060109406709671 + 2.0 * 6.286898612976074
Epoch 890, val loss: 1.1166839599609375
Epoch 900, training loss: 12.655461311340332 = 0.08674004673957825 + 2.0 * 6.284360408782959
Epoch 900, val loss: 1.1253186464309692
Epoch 910, training loss: 12.65918254852295 = 0.08309181779623032 + 2.0 * 6.288045406341553
Epoch 910, val loss: 1.133848786354065
Epoch 920, training loss: 12.652006149291992 = 0.07967203855514526 + 2.0 * 6.286167144775391
Epoch 920, val loss: 1.1421868801116943
Epoch 930, training loss: 12.642914772033691 = 0.07643962651491165 + 2.0 * 6.283237457275391
Epoch 930, val loss: 1.1506860256195068
Epoch 940, training loss: 12.635662078857422 = 0.07338953763246536 + 2.0 * 6.2811360359191895
Epoch 940, val loss: 1.1590907573699951
Epoch 950, training loss: 12.638154029846191 = 0.07049572467803955 + 2.0 * 6.283829212188721
Epoch 950, val loss: 1.1671311855316162
Epoch 960, training loss: 12.634352684020996 = 0.06776028871536255 + 2.0 * 6.28329610824585
Epoch 960, val loss: 1.1754136085510254
Epoch 970, training loss: 12.627288818359375 = 0.06518780440092087 + 2.0 * 6.281050682067871
Epoch 970, val loss: 1.1833734512329102
Epoch 980, training loss: 12.621495246887207 = 0.06275203078985214 + 2.0 * 6.279371738433838
Epoch 980, val loss: 1.1914600133895874
Epoch 990, training loss: 12.619190216064453 = 0.06043786182999611 + 2.0 * 6.279376029968262
Epoch 990, val loss: 1.1992111206054688
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7148148148148148
0.8075909330521878
=== training gcn model ===
Epoch 0, training loss: 19.133710861206055 = 1.9401129484176636 + 2.0 * 8.59679889678955
Epoch 0, val loss: 1.9438869953155518
Epoch 10, training loss: 19.123092651367188 = 1.9306843280792236 + 2.0 * 8.596203804016113
Epoch 10, val loss: 1.934788465499878
Epoch 20, training loss: 19.10134506225586 = 1.9186468124389648 + 2.0 * 8.591349601745605
Epoch 20, val loss: 1.9228260517120361
Epoch 30, training loss: 19.01868438720703 = 1.9020603895187378 + 2.0 * 8.55831241607666
Epoch 30, val loss: 1.9061682224273682
Epoch 40, training loss: 18.613842010498047 = 1.8829162120819092 + 2.0 * 8.365463256835938
Epoch 40, val loss: 1.8876268863677979
Epoch 50, training loss: 17.88503646850586 = 1.8620727062225342 + 2.0 * 8.011482238769531
Epoch 50, val loss: 1.8679717779159546
Epoch 60, training loss: 17.271413803100586 = 1.8451952934265137 + 2.0 * 7.713109493255615
Epoch 60, val loss: 1.852721095085144
Epoch 70, training loss: 16.382831573486328 = 1.8354395627975464 + 2.0 * 7.273696422576904
Epoch 70, val loss: 1.8443068265914917
Epoch 80, training loss: 15.730203628540039 = 1.8290834426879883 + 2.0 * 6.950560092926025
Epoch 80, val loss: 1.837941288948059
Epoch 90, training loss: 15.433487892150879 = 1.817927360534668 + 2.0 * 6.8077802658081055
Epoch 90, val loss: 1.8270142078399658
Epoch 100, training loss: 15.262866020202637 = 1.8055000305175781 + 2.0 * 6.728682994842529
Epoch 100, val loss: 1.8153965473175049
Epoch 110, training loss: 15.154983520507812 = 1.794527292251587 + 2.0 * 6.680228233337402
Epoch 110, val loss: 1.8052794933319092
Epoch 120, training loss: 15.060196876525879 = 1.7843945026397705 + 2.0 * 6.637901306152344
Epoch 120, val loss: 1.7959420680999756
Epoch 130, training loss: 14.983623504638672 = 1.7743264436721802 + 2.0 * 6.604648590087891
Epoch 130, val loss: 1.7865103483200073
Epoch 140, training loss: 14.914259910583496 = 1.763816237449646 + 2.0 * 6.575222015380859
Epoch 140, val loss: 1.776890516281128
Epoch 150, training loss: 14.852502822875977 = 1.7526122331619263 + 2.0 * 6.54994535446167
Epoch 150, val loss: 1.7669336795806885
Epoch 160, training loss: 14.801069259643555 = 1.740429162979126 + 2.0 * 6.530320167541504
Epoch 160, val loss: 1.7564853429794312
Epoch 170, training loss: 14.742735862731934 = 1.7269618511199951 + 2.0 * 6.50788688659668
Epoch 170, val loss: 1.745164155960083
Epoch 180, training loss: 14.694169998168945 = 1.7119140625 + 2.0 * 6.491127967834473
Epoch 180, val loss: 1.7328040599822998
Epoch 190, training loss: 14.64787769317627 = 1.6949596405029297 + 2.0 * 6.47645902633667
Epoch 190, val loss: 1.7189687490463257
Epoch 200, training loss: 14.600341796875 = 1.6757845878601074 + 2.0 * 6.462278842926025
Epoch 200, val loss: 1.7035378217697144
Epoch 210, training loss: 14.557812690734863 = 1.65414297580719 + 2.0 * 6.451834678649902
Epoch 210, val loss: 1.6862519979476929
Epoch 220, training loss: 14.508997917175293 = 1.6298481225967407 + 2.0 * 6.439574718475342
Epoch 220, val loss: 1.6668727397918701
Epoch 230, training loss: 14.463830947875977 = 1.602837324142456 + 2.0 * 6.430496692657471
Epoch 230, val loss: 1.6453202962875366
Epoch 240, training loss: 14.416274070739746 = 1.573071837425232 + 2.0 * 6.421601295471191
Epoch 240, val loss: 1.6215876340866089
Epoch 250, training loss: 14.374289512634277 = 1.5404540300369263 + 2.0 * 6.41691780090332
Epoch 250, val loss: 1.5958070755004883
Epoch 260, training loss: 14.322122573852539 = 1.5053489208221436 + 2.0 * 6.408386707305908
Epoch 260, val loss: 1.567858099937439
Epoch 270, training loss: 14.27286434173584 = 1.467885971069336 + 2.0 * 6.402489185333252
Epoch 270, val loss: 1.5383611917495728
Epoch 280, training loss: 14.220527648925781 = 1.4285435676574707 + 2.0 * 6.395991802215576
Epoch 280, val loss: 1.5079044103622437
Epoch 290, training loss: 14.169297218322754 = 1.387969970703125 + 2.0 * 6.3906636238098145
Epoch 290, val loss: 1.47670316696167
Epoch 300, training loss: 14.117181777954102 = 1.3467867374420166 + 2.0 * 6.385197639465332
Epoch 300, val loss: 1.445389747619629
Epoch 310, training loss: 14.070688247680664 = 1.3051810264587402 + 2.0 * 6.382753372192383
Epoch 310, val loss: 1.4144010543823242
Epoch 320, training loss: 14.022177696228027 = 1.263756513595581 + 2.0 * 6.379210472106934
Epoch 320, val loss: 1.3844457864761353
Epoch 330, training loss: 13.968294143676758 = 1.2232247591018677 + 2.0 * 6.37253475189209
Epoch 330, val loss: 1.3552191257476807
Epoch 340, training loss: 13.919914245605469 = 1.183213472366333 + 2.0 * 6.368350505828857
Epoch 340, val loss: 1.3269875049591064
Epoch 350, training loss: 13.87865924835205 = 1.1437395811080933 + 2.0 * 6.367459774017334
Epoch 350, val loss: 1.2995506525039673
Epoch 360, training loss: 13.832928657531738 = 1.105065107345581 + 2.0 * 6.363931655883789
Epoch 360, val loss: 1.2730767726898193
Epoch 370, training loss: 13.785923957824707 = 1.067387580871582 + 2.0 * 6.3592681884765625
Epoch 370, val loss: 1.2473196983337402
Epoch 380, training loss: 13.752142906188965 = 1.0306575298309326 + 2.0 * 6.360742568969727
Epoch 380, val loss: 1.2225513458251953
Epoch 390, training loss: 13.701998710632324 = 0.9951269030570984 + 2.0 * 6.35343599319458
Epoch 390, val loss: 1.1986767053604126
Epoch 400, training loss: 13.658775329589844 = 0.9605352878570557 + 2.0 * 6.349120140075684
Epoch 400, val loss: 1.175750970840454
Epoch 410, training loss: 13.629063606262207 = 0.9269201159477234 + 2.0 * 6.351071834564209
Epoch 410, val loss: 1.1535391807556152
Epoch 420, training loss: 13.59040641784668 = 0.8943985104560852 + 2.0 * 6.34800386428833
Epoch 420, val loss: 1.13228440284729
Epoch 430, training loss: 13.548516273498535 = 0.8633068799972534 + 2.0 * 6.342604637145996
Epoch 430, val loss: 1.1121748685836792
Epoch 440, training loss: 13.511171340942383 = 0.8332277536392212 + 2.0 * 6.3389716148376465
Epoch 440, val loss: 1.0931566953659058
Epoch 450, training loss: 13.476316452026367 = 0.8041452169418335 + 2.0 * 6.336085796356201
Epoch 450, val loss: 1.0751537084579468
Epoch 460, training loss: 13.47490406036377 = 0.7762254476547241 + 2.0 * 6.349339485168457
Epoch 460, val loss: 1.0582748651504517
Epoch 470, training loss: 13.419100761413574 = 0.7499038577079773 + 2.0 * 6.334598541259766
Epoch 470, val loss: 1.0430593490600586
Epoch 480, training loss: 13.385345458984375 = 0.724938154220581 + 2.0 * 6.330203533172607
Epoch 480, val loss: 1.0293247699737549
Epoch 490, training loss: 13.358365058898926 = 0.7011156678199768 + 2.0 * 6.328624725341797
Epoch 490, val loss: 1.0168888568878174
Epoch 500, training loss: 13.33359146118164 = 0.6784557700157166 + 2.0 * 6.327568054199219
Epoch 500, val loss: 1.0057398080825806
Epoch 510, training loss: 13.305991172790527 = 0.6569882035255432 + 2.0 * 6.3245015144348145
Epoch 510, val loss: 0.9960852861404419
Epoch 520, training loss: 13.29437255859375 = 0.6365346908569336 + 2.0 * 6.328918933868408
Epoch 520, val loss: 0.9877015352249146
Epoch 530, training loss: 13.259610176086426 = 0.6169866919517517 + 2.0 * 6.321311950683594
Epoch 530, val loss: 0.980392336845398
Epoch 540, training loss: 13.235133171081543 = 0.5983220934867859 + 2.0 * 6.318405628204346
Epoch 540, val loss: 0.9743972420692444
Epoch 550, training loss: 13.227106094360352 = 0.5803883075714111 + 2.0 * 6.32335901260376
Epoch 550, val loss: 0.9694370031356812
Epoch 560, training loss: 13.198022842407227 = 0.5632310509681702 + 2.0 * 6.3173956871032715
Epoch 560, val loss: 0.9653717279434204
Epoch 570, training loss: 13.17849349975586 = 0.5466818809509277 + 2.0 * 6.315905570983887
Epoch 570, val loss: 0.9622682929039001
Epoch 580, training loss: 13.154735565185547 = 0.5307166576385498 + 2.0 * 6.312009334564209
Epoch 580, val loss: 0.9599468111991882
Epoch 590, training loss: 13.139573097229004 = 0.5152283906936646 + 2.0 * 6.3121724128723145
Epoch 590, val loss: 0.9583591222763062
Epoch 600, training loss: 13.12070083618164 = 0.5001240968704224 + 2.0 * 6.310288429260254
Epoch 600, val loss: 0.9574041366577148
Epoch 610, training loss: 13.10163402557373 = 0.4854990839958191 + 2.0 * 6.308067321777344
Epoch 610, val loss: 0.9571823477745056
Epoch 620, training loss: 13.097574234008789 = 0.4712069034576416 + 2.0 * 6.313183784484863
Epoch 620, val loss: 0.9574905037879944
Epoch 630, training loss: 13.070026397705078 = 0.4572935402393341 + 2.0 * 6.306366443634033
Epoch 630, val loss: 0.9583680033683777
Epoch 640, training loss: 13.05067253112793 = 0.4436327815055847 + 2.0 * 6.3035197257995605
Epoch 640, val loss: 0.9597113728523254
Epoch 650, training loss: 13.035920143127441 = 0.4301760196685791 + 2.0 * 6.302872180938721
Epoch 650, val loss: 0.9615601897239685
Epoch 660, training loss: 13.030264854431152 = 0.4169072210788727 + 2.0 * 6.306678771972656
Epoch 660, val loss: 0.963908314704895
Epoch 670, training loss: 13.018864631652832 = 0.4040187895298004 + 2.0 * 6.307423114776611
Epoch 670, val loss: 0.9666039347648621
Epoch 680, training loss: 12.994070053100586 = 0.39132753014564514 + 2.0 * 6.301371097564697
Epoch 680, val loss: 0.9698141813278198
Epoch 690, training loss: 12.974675178527832 = 0.3788398206233978 + 2.0 * 6.29791784286499
Epoch 690, val loss: 0.9734159111976624
Epoch 700, training loss: 12.959880828857422 = 0.3665209412574768 + 2.0 * 6.296679973602295
Epoch 700, val loss: 0.9775074124336243
Epoch 710, training loss: 12.957633972167969 = 0.3543985188007355 + 2.0 * 6.301617622375488
Epoch 710, val loss: 0.9820833802223206
Epoch 720, training loss: 12.936007499694824 = 0.3424516022205353 + 2.0 * 6.296777725219727
Epoch 720, val loss: 0.9870054125785828
Epoch 730, training loss: 12.920398712158203 = 0.33078333735466003 + 2.0 * 6.2948079109191895
Epoch 730, val loss: 0.9925180077552795
Epoch 740, training loss: 12.909358024597168 = 0.3193270266056061 + 2.0 * 6.295015335083008
Epoch 740, val loss: 0.9982770085334778
Epoch 750, training loss: 12.897701263427734 = 0.3082011342048645 + 2.0 * 6.294750213623047
Epoch 750, val loss: 1.004572868347168
Epoch 760, training loss: 12.879910469055176 = 0.29726773500442505 + 2.0 * 6.291321277618408
Epoch 760, val loss: 1.011252522468567
Epoch 770, training loss: 12.86981201171875 = 0.2866066098213196 + 2.0 * 6.291602611541748
Epoch 770, val loss: 1.0184099674224854
Epoch 780, training loss: 12.852812767028809 = 0.2762086093425751 + 2.0 * 6.288301944732666
Epoch 780, val loss: 1.0260071754455566
Epoch 790, training loss: 12.847342491149902 = 0.2660529911518097 + 2.0 * 6.290644645690918
Epoch 790, val loss: 1.0340030193328857
Epoch 800, training loss: 12.837210655212402 = 0.25614356994628906 + 2.0 * 6.290533542633057
Epoch 800, val loss: 1.0422686338424683
Epoch 810, training loss: 12.83639144897461 = 0.24654747545719147 + 2.0 * 6.294921875
Epoch 810, val loss: 1.0509456396102905
Epoch 820, training loss: 12.807790756225586 = 0.23725000023841858 + 2.0 * 6.2852702140808105
Epoch 820, val loss: 1.059936285018921
Epoch 830, training loss: 12.796821594238281 = 0.22820259630680084 + 2.0 * 6.284309387207031
Epoch 830, val loss: 1.0693550109863281
Epoch 840, training loss: 12.788171768188477 = 0.2193710058927536 + 2.0 * 6.284400463104248
Epoch 840, val loss: 1.0791324377059937
Epoch 850, training loss: 12.774852752685547 = 0.21078386902809143 + 2.0 * 6.282034397125244
Epoch 850, val loss: 1.0889956951141357
Epoch 860, training loss: 12.771008491516113 = 0.20247788727283478 + 2.0 * 6.284265518188477
Epoch 860, val loss: 1.0993348360061646
Epoch 870, training loss: 12.75439453125 = 0.19438715279102325 + 2.0 * 6.280003547668457
Epoch 870, val loss: 1.1097158193588257
Epoch 880, training loss: 12.745339393615723 = 0.18652670085430145 + 2.0 * 6.279406547546387
Epoch 880, val loss: 1.1203420162200928
Epoch 890, training loss: 12.763251304626465 = 0.1788950264453888 + 2.0 * 6.292178153991699
Epoch 890, val loss: 1.1311789751052856
Epoch 900, training loss: 12.73571491241455 = 0.17152205109596252 + 2.0 * 6.2820963859558105
Epoch 900, val loss: 1.1420246362686157
Epoch 910, training loss: 12.720274925231934 = 0.16440968215465546 + 2.0 * 6.277932643890381
Epoch 910, val loss: 1.153106927871704
Epoch 920, training loss: 12.712196350097656 = 0.1575084626674652 + 2.0 * 6.27734375
Epoch 920, val loss: 1.1643003225326538
Epoch 930, training loss: 12.714700698852539 = 0.15085487067699432 + 2.0 * 6.281922817230225
Epoch 930, val loss: 1.1755930185317993
Epoch 940, training loss: 12.697388648986816 = 0.1444675773382187 + 2.0 * 6.276460647583008
Epoch 940, val loss: 1.186910629272461
Epoch 950, training loss: 12.69694709777832 = 0.13833291828632355 + 2.0 * 6.279306888580322
Epoch 950, val loss: 1.1982477903366089
Epoch 960, training loss: 12.682109832763672 = 0.13246749341487885 + 2.0 * 6.2748212814331055
Epoch 960, val loss: 1.2096977233886719
Epoch 970, training loss: 12.674158096313477 = 0.12680138647556305 + 2.0 * 6.273678302764893
Epoch 970, val loss: 1.221005916595459
Epoch 980, training loss: 12.684359550476074 = 0.12139105796813965 + 2.0 * 6.281484127044678
Epoch 980, val loss: 1.2324169874191284
Epoch 990, training loss: 12.668386459350586 = 0.11621520668268204 + 2.0 * 6.27608585357666
Epoch 990, val loss: 1.243768334388733
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.737037037037037
0.8118081180811808
=== training gcn model ===
Epoch 0, training loss: 19.148637771606445 = 1.9549696445465088 + 2.0 * 8.596834182739258
Epoch 0, val loss: 1.957489252090454
Epoch 10, training loss: 19.137142181396484 = 1.9441405534744263 + 2.0 * 8.596500396728516
Epoch 10, val loss: 1.9460560083389282
Epoch 20, training loss: 19.118234634399414 = 1.9305015802383423 + 2.0 * 8.593866348266602
Epoch 20, val loss: 1.9315820932388306
Epoch 30, training loss: 19.058834075927734 = 1.9118207693099976 + 2.0 * 8.573506355285645
Epoch 30, val loss: 1.9118891954421997
Epoch 40, training loss: 18.79883575439453 = 1.8895519971847534 + 2.0 * 8.454642295837402
Epoch 40, val loss: 1.8897753953933716
Epoch 50, training loss: 17.9813289642334 = 1.8671162128448486 + 2.0 * 8.057106018066406
Epoch 50, val loss: 1.868767499923706
Epoch 60, training loss: 17.159637451171875 = 1.8484991788864136 + 2.0 * 7.655569076538086
Epoch 60, val loss: 1.8527686595916748
Epoch 70, training loss: 16.397003173828125 = 1.8356986045837402 + 2.0 * 7.280652046203613
Epoch 70, val loss: 1.841921329498291
Epoch 80, training loss: 15.961175918579102 = 1.825350284576416 + 2.0 * 7.067912578582764
Epoch 80, val loss: 1.8326107263565063
Epoch 90, training loss: 15.621706008911133 = 1.814719319343567 + 2.0 * 6.903493404388428
Epoch 90, val loss: 1.823063611984253
Epoch 100, training loss: 15.39277172088623 = 1.8041343688964844 + 2.0 * 6.794318675994873
Epoch 100, val loss: 1.8136507272720337
Epoch 110, training loss: 15.215340614318848 = 1.7933446168899536 + 2.0 * 6.710998058319092
Epoch 110, val loss: 1.8041681051254272
Epoch 120, training loss: 15.082980155944824 = 1.7827532291412354 + 2.0 * 6.650113582611084
Epoch 120, val loss: 1.7948267459869385
Epoch 130, training loss: 14.988212585449219 = 1.771851897239685 + 2.0 * 6.608180522918701
Epoch 130, val loss: 1.7854050397872925
Epoch 140, training loss: 14.913494110107422 = 1.759930968284607 + 2.0 * 6.576781749725342
Epoch 140, val loss: 1.7754026651382446
Epoch 150, training loss: 14.85328197479248 = 1.7467576265335083 + 2.0 * 6.553262233734131
Epoch 150, val loss: 1.7646251916885376
Epoch 160, training loss: 14.809901237487793 = 1.7321299314498901 + 2.0 * 6.538885593414307
Epoch 160, val loss: 1.7529301643371582
Epoch 170, training loss: 14.754472732543945 = 1.7160992622375488 + 2.0 * 6.519186496734619
Epoch 170, val loss: 1.7401596307754517
Epoch 180, training loss: 14.705829620361328 = 1.6980725526809692 + 2.0 * 6.503878593444824
Epoch 180, val loss: 1.7261513471603394
Epoch 190, training loss: 14.658782005310059 = 1.6776398420333862 + 2.0 * 6.490571022033691
Epoch 190, val loss: 1.7104928493499756
Epoch 200, training loss: 14.616074562072754 = 1.6546038389205933 + 2.0 * 6.4807353019714355
Epoch 200, val loss: 1.6929806470870972
Epoch 210, training loss: 14.563013076782227 = 1.6289105415344238 + 2.0 * 6.4670515060424805
Epoch 210, val loss: 1.6733654737472534
Epoch 220, training loss: 14.514073371887207 = 1.5999536514282227 + 2.0 * 6.457059860229492
Epoch 220, val loss: 1.651313066482544
Epoch 230, training loss: 14.462008476257324 = 1.5679271221160889 + 2.0 * 6.447040557861328
Epoch 230, val loss: 1.6269316673278809
Epoch 240, training loss: 14.406761169433594 = 1.5331696271896362 + 2.0 * 6.436795711517334
Epoch 240, val loss: 1.6004745960235596
Epoch 250, training loss: 14.35212516784668 = 1.495883584022522 + 2.0 * 6.4281206130981445
Epoch 250, val loss: 1.5722005367279053
Epoch 260, training loss: 14.295660972595215 = 1.4565086364746094 + 2.0 * 6.419576168060303
Epoch 260, val loss: 1.542441964149475
Epoch 270, training loss: 14.249138832092285 = 1.415662407875061 + 2.0 * 6.416738033294678
Epoch 270, val loss: 1.5120611190795898
Epoch 280, training loss: 14.187070846557617 = 1.3740127086639404 + 2.0 * 6.406528949737549
Epoch 280, val loss: 1.481450080871582
Epoch 290, training loss: 14.130970001220703 = 1.3319356441497803 + 2.0 * 6.399517059326172
Epoch 290, val loss: 1.4510177373886108
Epoch 300, training loss: 14.08959674835205 = 1.2900315523147583 + 2.0 * 6.399782657623291
Epoch 300, val loss: 1.420960783958435
Epoch 310, training loss: 14.031375885009766 = 1.2495083808898926 + 2.0 * 6.390933513641357
Epoch 310, val loss: 1.3925596475601196
Epoch 320, training loss: 13.979447364807129 = 1.2106157541275024 + 2.0 * 6.384415626525879
Epoch 320, val loss: 1.366122841835022
Epoch 330, training loss: 13.9324369430542 = 1.1733434200286865 + 2.0 * 6.379546642303467
Epoch 330, val loss: 1.3414117097854614
Epoch 340, training loss: 13.887969970703125 = 1.1376291513442993 + 2.0 * 6.3751702308654785
Epoch 340, val loss: 1.3183313608169556
Epoch 350, training loss: 13.8537015914917 = 1.1035805940628052 + 2.0 * 6.375060558319092
Epoch 350, val loss: 1.2971312999725342
Epoch 360, training loss: 13.807048797607422 = 1.0718492269515991 + 2.0 * 6.367599964141846
Epoch 360, val loss: 1.2778044939041138
Epoch 370, training loss: 13.770281791687012 = 1.041759729385376 + 2.0 * 6.364261150360107
Epoch 370, val loss: 1.2601332664489746
Epoch 380, training loss: 13.733833312988281 = 1.0130351781845093 + 2.0 * 6.36039924621582
Epoch 380, val loss: 1.2436959743499756
Epoch 390, training loss: 13.705588340759277 = 0.9853731989860535 + 2.0 * 6.360107421875
Epoch 390, val loss: 1.2282058000564575
Epoch 400, training loss: 13.672179222106934 = 0.9587923288345337 + 2.0 * 6.356693267822266
Epoch 400, val loss: 1.2135981321334839
Epoch 410, training loss: 13.635159492492676 = 0.9330308437347412 + 2.0 * 6.351064205169678
Epoch 410, val loss: 1.199597954750061
Epoch 420, training loss: 13.604874610900879 = 0.9076624512672424 + 2.0 * 6.348606109619141
Epoch 420, val loss: 1.1859304904937744
Epoch 430, training loss: 13.57599925994873 = 0.882475733757019 + 2.0 * 6.346761703491211
Epoch 430, val loss: 1.1723828315734863
Epoch 440, training loss: 13.543669700622559 = 0.8573205471038818 + 2.0 * 6.343174457550049
Epoch 440, val loss: 1.1589831113815308
Epoch 450, training loss: 13.524996757507324 = 0.8321380615234375 + 2.0 * 6.346429347991943
Epoch 450, val loss: 1.14556086063385
Epoch 460, training loss: 13.48516845703125 = 0.8069011569023132 + 2.0 * 6.3391337394714355
Epoch 460, val loss: 1.131940484046936
Epoch 470, training loss: 13.455389976501465 = 0.7815700769424438 + 2.0 * 6.336909770965576
Epoch 470, val loss: 1.1183791160583496
Epoch 480, training loss: 13.42565631866455 = 0.7561985850334167 + 2.0 * 6.334728717803955
Epoch 480, val loss: 1.104995846748352
Epoch 490, training loss: 13.398468971252441 = 0.7308352589607239 + 2.0 * 6.333817005157471
Epoch 490, val loss: 1.0915695428848267
Epoch 500, training loss: 13.370722770690918 = 0.7057138085365295 + 2.0 * 6.3325042724609375
Epoch 500, val loss: 1.078458547592163
Epoch 510, training loss: 13.337933540344238 = 0.6808215975761414 + 2.0 * 6.328556060791016
Epoch 510, val loss: 1.0657752752304077
Epoch 520, training loss: 13.315674781799316 = 0.6563147902488708 + 2.0 * 6.3296799659729
Epoch 520, val loss: 1.0536357164382935
Epoch 530, training loss: 13.285069465637207 = 0.6322097778320312 + 2.0 * 6.326429843902588
Epoch 530, val loss: 1.0420784950256348
Epoch 540, training loss: 13.25879955291748 = 0.6086875796318054 + 2.0 * 6.325056076049805
Epoch 540, val loss: 1.0310680866241455
Epoch 550, training loss: 13.230477333068848 = 0.5857179164886475 + 2.0 * 6.3223795890808105
Epoch 550, val loss: 1.020904302597046
Epoch 560, training loss: 13.206478118896484 = 0.5634792447090149 + 2.0 * 6.321499347686768
Epoch 560, val loss: 1.0115910768508911
Epoch 570, training loss: 13.186308860778809 = 0.5417721271514893 + 2.0 * 6.322268486022949
Epoch 570, val loss: 1.0030081272125244
Epoch 580, training loss: 13.158802032470703 = 0.5208716988563538 + 2.0 * 6.318964958190918
Epoch 580, val loss: 0.9950657486915588
Epoch 590, training loss: 13.133216857910156 = 0.5006038546562195 + 2.0 * 6.3163065910339355
Epoch 590, val loss: 0.9878854155540466
Epoch 600, training loss: 13.115946769714355 = 0.4810260832309723 + 2.0 * 6.317460536956787
Epoch 600, val loss: 0.9814261198043823
Epoch 610, training loss: 13.093876838684082 = 0.46214669942855835 + 2.0 * 6.3158650398254395
Epoch 610, val loss: 0.9755327105522156
Epoch 620, training loss: 13.06728744506836 = 0.4439663887023926 + 2.0 * 6.311660289764404
Epoch 620, val loss: 0.9704350233078003
Epoch 630, training loss: 13.047813415527344 = 0.4263989329338074 + 2.0 * 6.310707092285156
Epoch 630, val loss: 0.9659523367881775
Epoch 640, training loss: 13.039542198181152 = 0.40939226746559143 + 2.0 * 6.315074920654297
Epoch 640, val loss: 0.9619105458259583
Epoch 650, training loss: 13.00829792022705 = 0.39295679330825806 + 2.0 * 6.307670593261719
Epoch 650, val loss: 0.9585678577423096
Epoch 660, training loss: 13.004205703735352 = 0.3770946264266968 + 2.0 * 6.313555717468262
Epoch 660, val loss: 0.9556782841682434
Epoch 670, training loss: 12.975095748901367 = 0.3618723750114441 + 2.0 * 6.30661153793335
Epoch 670, val loss: 0.9532643556594849
Epoch 680, training loss: 12.956169128417969 = 0.34712329506874084 + 2.0 * 6.30452299118042
Epoch 680, val loss: 0.9512847661972046
Epoch 690, training loss: 12.940112113952637 = 0.33289197087287903 + 2.0 * 6.303609848022461
Epoch 690, val loss: 0.9499239325523376
Epoch 700, training loss: 12.93242073059082 = 0.3191947042942047 + 2.0 * 6.306612968444824
Epoch 700, val loss: 0.9488785266876221
Epoch 710, training loss: 12.90964126586914 = 0.3060515820980072 + 2.0 * 6.30179500579834
Epoch 710, val loss: 0.9483212232589722
Epoch 720, training loss: 12.893622398376465 = 0.29345786571502686 + 2.0 * 6.300082206726074
Epoch 720, val loss: 0.9481603503227234
Epoch 730, training loss: 12.87838363647461 = 0.28128334879875183 + 2.0 * 6.298550128936768
Epoch 730, val loss: 0.9483814239501953
Epoch 740, training loss: 12.86390209197998 = 0.2695125639438629 + 2.0 * 6.297194957733154
Epoch 740, val loss: 0.9489855766296387
Epoch 750, training loss: 12.870820999145508 = 0.25816646218299866 + 2.0 * 6.3063273429870605
Epoch 750, val loss: 0.9497838020324707
Epoch 760, training loss: 12.844009399414062 = 0.24725592136383057 + 2.0 * 6.298376560211182
Epoch 760, val loss: 0.9512001276016235
Epoch 770, training loss: 12.826436042785645 = 0.23678289353847504 + 2.0 * 6.294826507568359
Epoch 770, val loss: 0.9527132511138916
Epoch 780, training loss: 12.815185546875 = 0.226680725812912 + 2.0 * 6.294252395629883
Epoch 780, val loss: 0.9546981453895569
Epoch 790, training loss: 12.807816505432129 = 0.21693900227546692 + 2.0 * 6.295438766479492
Epoch 790, val loss: 0.95683753490448
Epoch 800, training loss: 12.794281005859375 = 0.20759499073028564 + 2.0 * 6.2933430671691895
Epoch 800, val loss: 0.9593470692634583
Epoch 810, training loss: 12.784131050109863 = 0.1986134946346283 + 2.0 * 6.292758941650391
Epoch 810, val loss: 0.9620424509048462
Epoch 820, training loss: 12.769779205322266 = 0.18996132910251617 + 2.0 * 6.2899088859558105
Epoch 820, val loss: 0.9650711417198181
Epoch 830, training loss: 12.75994873046875 = 0.18164227902889252 + 2.0 * 6.289153099060059
Epoch 830, val loss: 0.9683037400245667
Epoch 840, training loss: 12.75471305847168 = 0.17362523078918457 + 2.0 * 6.290544033050537
Epoch 840, val loss: 0.9716649055480957
Epoch 850, training loss: 12.749053001403809 = 0.16593746840953827 + 2.0 * 6.291557788848877
Epoch 850, val loss: 0.9751543998718262
Epoch 860, training loss: 12.733110427856445 = 0.15857098996639252 + 2.0 * 6.287269592285156
Epoch 860, val loss: 0.9787943363189697
Epoch 870, training loss: 12.722261428833008 = 0.1514931321144104 + 2.0 * 6.285384178161621
Epoch 870, val loss: 0.9825530052185059
Epoch 880, training loss: 12.728519439697266 = 0.14469505846500397 + 2.0 * 6.291912078857422
Epoch 880, val loss: 0.9863116145133972
Epoch 890, training loss: 12.719927787780762 = 0.13822253048419952 + 2.0 * 6.2908525466918945
Epoch 890, val loss: 0.9902960658073425
Epoch 900, training loss: 12.702515602111816 = 0.1320503205060959 + 2.0 * 6.2852325439453125
Epoch 900, val loss: 0.9945697784423828
Epoch 910, training loss: 12.690251350402832 = 0.1261489987373352 + 2.0 * 6.282051086425781
Epoch 910, val loss: 0.9987061619758606
Epoch 920, training loss: 12.681516647338867 = 0.12050163000822067 + 2.0 * 6.280507564544678
Epoch 920, val loss: 1.0030269622802734
Epoch 930, training loss: 12.674927711486816 = 0.11509839445352554 + 2.0 * 6.279914855957031
Epoch 930, val loss: 1.007327675819397
Epoch 940, training loss: 12.687026977539062 = 0.10995515435934067 + 2.0 * 6.288536071777344
Epoch 940, val loss: 1.0116537809371948
Epoch 950, training loss: 12.67519474029541 = 0.10502269864082336 + 2.0 * 6.285086154937744
Epoch 950, val loss: 1.0159448385238647
Epoch 960, training loss: 12.660259246826172 = 0.10037364065647125 + 2.0 * 6.279942989349365
Epoch 960, val loss: 1.0204886198043823
Epoch 970, training loss: 12.650138854980469 = 0.09596607089042664 + 2.0 * 6.27708625793457
Epoch 970, val loss: 1.0249089002609253
Epoch 980, training loss: 12.644606590270996 = 0.0917629599571228 + 2.0 * 6.276422023773193
Epoch 980, val loss: 1.0295029878616333
Epoch 990, training loss: 12.646273612976074 = 0.08776675909757614 + 2.0 * 6.2792534828186035
Epoch 990, val loss: 1.0341230630874634
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7592592592592593
0.8102266736953084
The final CL Acc:0.73704, 0.01814, The final GNN Acc:0.80988, 0.00174
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13246])
remove edge: torch.Size([2, 7902])
updated graph: torch.Size([2, 10592])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 19.135534286499023 = 1.9419082403182983 + 2.0 * 8.596813201904297
Epoch 0, val loss: 1.9474467039108276
Epoch 10, training loss: 19.124723434448242 = 1.932161808013916 + 2.0 * 8.596281051635742
Epoch 10, val loss: 1.9370908737182617
Epoch 20, training loss: 19.104307174682617 = 1.92027747631073 + 2.0 * 8.592015266418457
Epoch 20, val loss: 1.9240695238113403
Epoch 30, training loss: 19.02593994140625 = 1.9046190977096558 + 2.0 * 8.560660362243652
Epoch 30, val loss: 1.9068026542663574
Epoch 40, training loss: 18.60781478881836 = 1.8858553171157837 + 2.0 * 8.360980033874512
Epoch 40, val loss: 1.8865129947662354
Epoch 50, training loss: 17.178415298461914 = 1.8652228116989136 + 2.0 * 7.6565961837768555
Epoch 50, val loss: 1.8646585941314697
Epoch 60, training loss: 16.485719680786133 = 1.8513514995574951 + 2.0 * 7.3171844482421875
Epoch 60, val loss: 1.8515208959579468
Epoch 70, training loss: 16.0830020904541 = 1.8398258686065674 + 2.0 * 7.121587753295898
Epoch 70, val loss: 1.8398295640945435
Epoch 80, training loss: 15.798662185668945 = 1.8290704488754272 + 2.0 * 6.984796047210693
Epoch 80, val loss: 1.8293570280075073
Epoch 90, training loss: 15.510063171386719 = 1.8190683126449585 + 2.0 * 6.8454976081848145
Epoch 90, val loss: 1.8198689222335815
Epoch 100, training loss: 15.305220603942871 = 1.8090260028839111 + 2.0 * 6.7480974197387695
Epoch 100, val loss: 1.8105653524398804
Epoch 110, training loss: 15.135271072387695 = 1.7994256019592285 + 2.0 * 6.6679229736328125
Epoch 110, val loss: 1.8018641471862793
Epoch 120, training loss: 15.023771286010742 = 1.7898492813110352 + 2.0 * 6.6169610023498535
Epoch 120, val loss: 1.7931928634643555
Epoch 130, training loss: 14.937758445739746 = 1.7800921201705933 + 2.0 * 6.578833103179932
Epoch 130, val loss: 1.784348487854004
Epoch 140, training loss: 14.869800567626953 = 1.7701224088668823 + 2.0 * 6.549839019775391
Epoch 140, val loss: 1.775416374206543
Epoch 150, training loss: 14.808979988098145 = 1.7594845294952393 + 2.0 * 6.524747848510742
Epoch 150, val loss: 1.7659423351287842
Epoch 160, training loss: 14.755195617675781 = 1.747977614402771 + 2.0 * 6.5036091804504395
Epoch 160, val loss: 1.7558131217956543
Epoch 170, training loss: 14.71879768371582 = 1.7353817224502563 + 2.0 * 6.491707801818848
Epoch 170, val loss: 1.7448731660842896
Epoch 180, training loss: 14.664348602294922 = 1.7213718891143799 + 2.0 * 6.4714884757995605
Epoch 180, val loss: 1.7330154180526733
Epoch 190, training loss: 14.619693756103516 = 1.7057342529296875 + 2.0 * 6.456979751586914
Epoch 190, val loss: 1.719956398010254
Epoch 200, training loss: 14.577910423278809 = 1.6881195306777954 + 2.0 * 6.444895267486572
Epoch 200, val loss: 1.7053558826446533
Epoch 210, training loss: 14.536337852478027 = 1.6681019067764282 + 2.0 * 6.434117794036865
Epoch 210, val loss: 1.6888560056686401
Epoch 220, training loss: 14.495338439941406 = 1.6453062295913696 + 2.0 * 6.425015926361084
Epoch 220, val loss: 1.6701035499572754
Epoch 230, training loss: 14.458120346069336 = 1.6194335222244263 + 2.0 * 6.4193434715271
Epoch 230, val loss: 1.6488687992095947
Epoch 240, training loss: 14.412337303161621 = 1.590754747390747 + 2.0 * 6.410791397094727
Epoch 240, val loss: 1.6253107786178589
Epoch 250, training loss: 14.365409851074219 = 1.558833122253418 + 2.0 * 6.4032883644104
Epoch 250, val loss: 1.5990110635757446
Epoch 260, training loss: 14.316587448120117 = 1.5231446027755737 + 2.0 * 6.396721363067627
Epoch 260, val loss: 1.5696271657943726
Epoch 270, training loss: 14.265417098999023 = 1.4833706617355347 + 2.0 * 6.3910231590271
Epoch 270, val loss: 1.536870002746582
Epoch 280, training loss: 14.213113784790039 = 1.439622163772583 + 2.0 * 6.386745929718018
Epoch 280, val loss: 1.50071120262146
Epoch 290, training loss: 14.158035278320312 = 1.392612099647522 + 2.0 * 6.382711410522461
Epoch 290, val loss: 1.462157130241394
Epoch 300, training loss: 14.10014820098877 = 1.343911051750183 + 2.0 * 6.378118515014648
Epoch 300, val loss: 1.422248363494873
Epoch 310, training loss: 14.039846420288086 = 1.2933517694473267 + 2.0 * 6.373247146606445
Epoch 310, val loss: 1.3810644149780273
Epoch 320, training loss: 13.979567527770996 = 1.2416397333145142 + 2.0 * 6.368963718414307
Epoch 320, val loss: 1.33902907371521
Epoch 330, training loss: 13.919681549072266 = 1.1895480155944824 + 2.0 * 6.365067005157471
Epoch 330, val loss: 1.2967162132263184
Epoch 340, training loss: 13.862207412719727 = 1.1376585960388184 + 2.0 * 6.362274169921875
Epoch 340, val loss: 1.2545182704925537
Epoch 350, training loss: 13.80707836151123 = 1.0865517854690552 + 2.0 * 6.360263347625732
Epoch 350, val loss: 1.2131229639053345
Epoch 360, training loss: 13.747275352478027 = 1.037169337272644 + 2.0 * 6.355052947998047
Epoch 360, val loss: 1.1729745864868164
Epoch 370, training loss: 13.700305938720703 = 0.989874005317688 + 2.0 * 6.355216026306152
Epoch 370, val loss: 1.1344871520996094
Epoch 380, training loss: 13.642166137695312 = 0.945057213306427 + 2.0 * 6.348554611206055
Epoch 380, val loss: 1.0980600118637085
Epoch 390, training loss: 13.593382835388184 = 0.9029054641723633 + 2.0 * 6.34523868560791
Epoch 390, val loss: 1.0638792514801025
Epoch 400, training loss: 13.554490089416504 = 0.8636007905006409 + 2.0 * 6.345444679260254
Epoch 400, val loss: 1.0322390794754028
Epoch 410, training loss: 13.5088472366333 = 0.8279657959938049 + 2.0 * 6.34044075012207
Epoch 410, val loss: 1.003622055053711
Epoch 420, training loss: 13.470521926879883 = 0.7951541543006897 + 2.0 * 6.33768367767334
Epoch 420, val loss: 0.9778004884719849
Epoch 430, training loss: 13.432044982910156 = 0.7646917104721069 + 2.0 * 6.333676815032959
Epoch 430, val loss: 0.9543027281761169
Epoch 440, training loss: 13.398517608642578 = 0.7362749576568604 + 2.0 * 6.331121444702148
Epoch 440, val loss: 0.9328675270080566
Epoch 450, training loss: 13.392781257629395 = 0.709611713886261 + 2.0 * 6.3415846824646
Epoch 450, val loss: 0.9134036302566528
Epoch 460, training loss: 13.34149169921875 = 0.6848584413528442 + 2.0 * 6.328316688537598
Epoch 460, val loss: 0.8959755301475525
Epoch 470, training loss: 13.311245918273926 = 0.6614142656326294 + 2.0 * 6.324915885925293
Epoch 470, val loss: 0.8800926208496094
Epoch 480, training loss: 13.283686637878418 = 0.6388265490531921 + 2.0 * 6.32243013381958
Epoch 480, val loss: 0.865397036075592
Epoch 490, training loss: 13.257620811462402 = 0.6168603301048279 + 2.0 * 6.320380210876465
Epoch 490, val loss: 0.8517580032348633
Epoch 500, training loss: 13.240518569946289 = 0.5954550504684448 + 2.0 * 6.322531700134277
Epoch 500, val loss: 0.8390628695487976
Epoch 510, training loss: 13.211164474487305 = 0.5746667981147766 + 2.0 * 6.318248748779297
Epoch 510, val loss: 0.8272528648376465
Epoch 520, training loss: 13.183064460754395 = 0.5542495250701904 + 2.0 * 6.3144073486328125
Epoch 520, val loss: 0.816183865070343
Epoch 530, training loss: 13.165743827819824 = 0.5341771841049194 + 2.0 * 6.315783500671387
Epoch 530, val loss: 0.8057965040206909
Epoch 540, training loss: 13.142438888549805 = 0.5145887136459351 + 2.0 * 6.313925266265869
Epoch 540, val loss: 0.7961185574531555
Epoch 550, training loss: 13.11605167388916 = 0.495523065328598 + 2.0 * 6.3102641105651855
Epoch 550, val loss: 0.7871867418289185
Epoch 560, training loss: 13.093396186828613 = 0.47691839933395386 + 2.0 * 6.308238983154297
Epoch 560, val loss: 0.7789443135261536
Epoch 570, training loss: 13.083587646484375 = 0.4588662087917328 + 2.0 * 6.312360763549805
Epoch 570, val loss: 0.7713937759399414
Epoch 580, training loss: 13.051952362060547 = 0.44136112928390503 + 2.0 * 6.305295467376709
Epoch 580, val loss: 0.7647237181663513
Epoch 590, training loss: 13.033787727355957 = 0.42457205057144165 + 2.0 * 6.30460786819458
Epoch 590, val loss: 0.7587225437164307
Epoch 600, training loss: 13.012834548950195 = 0.4083310067653656 + 2.0 * 6.302251815795898
Epoch 600, val loss: 0.7534080743789673
Epoch 610, training loss: 13.008040428161621 = 0.39262163639068604 + 2.0 * 6.307709217071533
Epoch 610, val loss: 0.7487356066703796
Epoch 620, training loss: 12.97815227508545 = 0.37758374214172363 + 2.0 * 6.300284385681152
Epoch 620, val loss: 0.7445830702781677
Epoch 630, training loss: 12.960395812988281 = 0.36307981610298157 + 2.0 * 6.2986578941345215
Epoch 630, val loss: 0.7409795522689819
Epoch 640, training loss: 12.950533866882324 = 0.3490128219127655 + 2.0 * 6.300760746002197
Epoch 640, val loss: 0.737949788570404
Epoch 650, training loss: 12.934502601623535 = 0.33557313680648804 + 2.0 * 6.299464702606201
Epoch 650, val loss: 0.7352630496025085
Epoch 660, training loss: 12.91280460357666 = 0.32251814007759094 + 2.0 * 6.295143127441406
Epoch 660, val loss: 0.7330584526062012
Epoch 670, training loss: 12.897400856018066 = 0.30992934107780457 + 2.0 * 6.293735980987549
Epoch 670, val loss: 0.7312122583389282
Epoch 680, training loss: 12.882328033447266 = 0.2977136969566345 + 2.0 * 6.292307376861572
Epoch 680, val loss: 0.7296718955039978
Epoch 690, training loss: 12.891377449035645 = 0.28581032156944275 + 2.0 * 6.302783489227295
Epoch 690, val loss: 0.7285211682319641
Epoch 700, training loss: 12.856619834899902 = 0.2743632197380066 + 2.0 * 6.291128158569336
Epoch 700, val loss: 0.7275580763816833
Epoch 710, training loss: 12.841717720031738 = 0.26328301429748535 + 2.0 * 6.289217472076416
Epoch 710, val loss: 0.7268661260604858
Epoch 720, training loss: 12.828174591064453 = 0.25251075625419617 + 2.0 * 6.287831783294678
Epoch 720, val loss: 0.7264548540115356
Epoch 730, training loss: 12.832667350769043 = 0.24203753471374512 + 2.0 * 6.295314788818359
Epoch 730, val loss: 0.7263047695159912
Epoch 740, training loss: 12.807259559631348 = 0.23192201554775238 + 2.0 * 6.287668704986572
Epoch 740, val loss: 0.7263498306274414
Epoch 750, training loss: 12.798916816711426 = 0.22210533916950226 + 2.0 * 6.288405895233154
Epoch 750, val loss: 0.7266148924827576
Epoch 760, training loss: 12.783958435058594 = 0.2126893252134323 + 2.0 * 6.285634517669678
Epoch 760, val loss: 0.7270675301551819
Epoch 770, training loss: 12.773107528686523 = 0.2035539150238037 + 2.0 * 6.28477668762207
Epoch 770, val loss: 0.7277841567993164
Epoch 780, training loss: 12.758889198303223 = 0.19476939737796783 + 2.0 * 6.282059669494629
Epoch 780, val loss: 0.72869473695755
Epoch 790, training loss: 12.749095916748047 = 0.1862683892250061 + 2.0 * 6.281413555145264
Epoch 790, val loss: 0.7298554182052612
Epoch 800, training loss: 12.756364822387695 = 0.1780480295419693 + 2.0 * 6.289158344268799
Epoch 800, val loss: 0.7313096523284912
Epoch 810, training loss: 12.732646942138672 = 0.17020076513290405 + 2.0 * 6.281223297119141
Epoch 810, val loss: 0.7327951788902283
Epoch 820, training loss: 12.71986198425293 = 0.16263480484485626 + 2.0 * 6.278613567352295
Epoch 820, val loss: 0.7344903945922852
Epoch 830, training loss: 12.712545394897461 = 0.15537233650684357 + 2.0 * 6.278586387634277
Epoch 830, val loss: 0.7363829612731934
Epoch 840, training loss: 12.709179878234863 = 0.14839695394039154 + 2.0 * 6.280391693115234
Epoch 840, val loss: 0.7384796738624573
Epoch 850, training loss: 12.69564437866211 = 0.1417417824268341 + 2.0 * 6.276951313018799
Epoch 850, val loss: 0.7406902313232422
Epoch 860, training loss: 12.698466300964355 = 0.13537520170211792 + 2.0 * 6.281545639038086
Epoch 860, val loss: 0.7429934740066528
Epoch 870, training loss: 12.682676315307617 = 0.12925225496292114 + 2.0 * 6.276711940765381
Epoch 870, val loss: 0.7455886006355286
Epoch 880, training loss: 12.67108154296875 = 0.12346584349870682 + 2.0 * 6.273808002471924
Epoch 880, val loss: 0.7481261491775513
Epoch 890, training loss: 12.663607597351074 = 0.11792252212762833 + 2.0 * 6.2728424072265625
Epoch 890, val loss: 0.750862717628479
Epoch 900, training loss: 12.67264175415039 = 0.11261438578367233 + 2.0 * 6.280013561248779
Epoch 900, val loss: 0.7537316679954529
Epoch 910, training loss: 12.656120300292969 = 0.10759530961513519 + 2.0 * 6.274262428283691
Epoch 910, val loss: 0.7566124796867371
Epoch 920, training loss: 12.645468711853027 = 0.10283200442790985 + 2.0 * 6.271318435668945
Epoch 920, val loss: 0.7595561742782593
Epoch 930, training loss: 12.638087272644043 = 0.09830086678266525 + 2.0 * 6.269893169403076
Epoch 930, val loss: 0.7626476883888245
Epoch 940, training loss: 12.640181541442871 = 0.09399587661027908 + 2.0 * 6.273092746734619
Epoch 940, val loss: 0.7658006548881531
Epoch 950, training loss: 12.627464294433594 = 0.08988500386476517 + 2.0 * 6.268789768218994
Epoch 950, val loss: 0.7690620422363281
Epoch 960, training loss: 12.624079704284668 = 0.08601325750350952 + 2.0 * 6.269033432006836
Epoch 960, val loss: 0.7722324132919312
Epoch 970, training loss: 12.625441551208496 = 0.08232057839632034 + 2.0 * 6.2715606689453125
Epoch 970, val loss: 0.7755609750747681
Epoch 980, training loss: 12.613210678100586 = 0.07884181290864944 + 2.0 * 6.267184257507324
Epoch 980, val loss: 0.778893232345581
Epoch 990, training loss: 12.60672378540039 = 0.07552660256624222 + 2.0 * 6.265598773956299
Epoch 990, val loss: 0.7822206616401672
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7851851851851852
0.8423827095413812
=== training gcn model ===
Epoch 0, training loss: 19.126699447631836 = 1.9330779314041138 + 2.0 * 8.596810340881348
Epoch 0, val loss: 1.9349526166915894
Epoch 10, training loss: 19.11618995666504 = 1.9234871864318848 + 2.0 * 8.596351623535156
Epoch 10, val loss: 1.9254584312438965
Epoch 20, training loss: 19.096946716308594 = 1.9109495878219604 + 2.0 * 8.592998504638672
Epoch 20, val loss: 1.9122949838638306
Epoch 30, training loss: 19.033796310424805 = 1.8929743766784668 + 2.0 * 8.57041072845459
Epoch 30, val loss: 1.8930418491363525
Epoch 40, training loss: 18.76766014099121 = 1.8707362413406372 + 2.0 * 8.448461532592773
Epoch 40, val loss: 1.8704893589019775
Epoch 50, training loss: 17.575773239135742 = 1.8481910228729248 + 2.0 * 7.863790988922119
Epoch 50, val loss: 1.8482441902160645
Epoch 60, training loss: 16.840749740600586 = 1.8286919593811035 + 2.0 * 7.506028652191162
Epoch 60, val loss: 1.8307527303695679
Epoch 70, training loss: 16.35967254638672 = 1.8134316205978394 + 2.0 * 7.273120880126953
Epoch 70, val loss: 1.8163173198699951
Epoch 80, training loss: 15.869783401489258 = 1.7984700202941895 + 2.0 * 7.035656452178955
Epoch 80, val loss: 1.8015210628509521
Epoch 90, training loss: 15.570928573608398 = 1.7863943576812744 + 2.0 * 6.892267227172852
Epoch 90, val loss: 1.7894433736801147
Epoch 100, training loss: 15.392474174499512 = 1.7742196321487427 + 2.0 * 6.809127330780029
Epoch 100, val loss: 1.7775635719299316
Epoch 110, training loss: 15.235135078430176 = 1.7616852521896362 + 2.0 * 6.736724853515625
Epoch 110, val loss: 1.76569664478302
Epoch 120, training loss: 15.099148750305176 = 1.7490581274032593 + 2.0 * 6.675045490264893
Epoch 120, val loss: 1.7535220384597778
Epoch 130, training loss: 14.99510383605957 = 1.73580002784729 + 2.0 * 6.62965202331543
Epoch 130, val loss: 1.7408063411712646
Epoch 140, training loss: 14.903420448303223 = 1.7210638523101807 + 2.0 * 6.5911784172058105
Epoch 140, val loss: 1.7273170948028564
Epoch 150, training loss: 14.823412895202637 = 1.7042278051376343 + 2.0 * 6.5595927238464355
Epoch 150, val loss: 1.71230947971344
Epoch 160, training loss: 14.752395629882812 = 1.685004472732544 + 2.0 * 6.533695697784424
Epoch 160, val loss: 1.6955372095108032
Epoch 170, training loss: 14.68715763092041 = 1.6630659103393555 + 2.0 * 6.512045860290527
Epoch 170, val loss: 1.6766149997711182
Epoch 180, training loss: 14.62526798248291 = 1.6381016969680786 + 2.0 * 6.4935832023620605
Epoch 180, val loss: 1.6552072763442993
Epoch 190, training loss: 14.570034980773926 = 1.6097468137741089 + 2.0 * 6.480144023895264
Epoch 190, val loss: 1.6311653852462769
Epoch 200, training loss: 14.51164436340332 = 1.5782585144042969 + 2.0 * 6.466692924499512
Epoch 200, val loss: 1.6049206256866455
Epoch 210, training loss: 14.454273223876953 = 1.543949842453003 + 2.0 * 6.4551615715026855
Epoch 210, val loss: 1.5766632556915283
Epoch 220, training loss: 14.396222114562988 = 1.5068204402923584 + 2.0 * 6.444700717926025
Epoch 220, val loss: 1.5466805696487427
Epoch 230, training loss: 14.340810775756836 = 1.4672338962554932 + 2.0 * 6.436788558959961
Epoch 230, val loss: 1.5154120922088623
Epoch 240, training loss: 14.284292221069336 = 1.4261358976364136 + 2.0 * 6.429078102111816
Epoch 240, val loss: 1.4837309122085571
Epoch 250, training loss: 14.228529930114746 = 1.3843438625335693 + 2.0 * 6.422092914581299
Epoch 250, val loss: 1.4523836374282837
Epoch 260, training loss: 14.169300079345703 = 1.3424526453018188 + 2.0 * 6.413423538208008
Epoch 260, val loss: 1.4217028617858887
Epoch 270, training loss: 14.114192962646484 = 1.3005855083465576 + 2.0 * 6.406803607940674
Epoch 270, val loss: 1.391743540763855
Epoch 280, training loss: 14.068130493164062 = 1.2590727806091309 + 2.0 * 6.404529094696045
Epoch 280, val loss: 1.3628662824630737
Epoch 290, training loss: 14.009772300720215 = 1.2185794115066528 + 2.0 * 6.395596504211426
Epoch 290, val loss: 1.3353360891342163
Epoch 300, training loss: 13.95915699005127 = 1.1791536808013916 + 2.0 * 6.3900017738342285
Epoch 300, val loss: 1.3088080883026123
Epoch 310, training loss: 13.911247253417969 = 1.1405880451202393 + 2.0 * 6.385329723358154
Epoch 310, val loss: 1.2831019163131714
Epoch 320, training loss: 13.8724946975708 = 1.1033300161361694 + 2.0 * 6.38458251953125
Epoch 320, val loss: 1.2583128213882446
Epoch 330, training loss: 13.81953239440918 = 1.0673110485076904 + 2.0 * 6.376110553741455
Epoch 330, val loss: 1.2345634698867798
Epoch 340, training loss: 13.775388717651367 = 1.0322211980819702 + 2.0 * 6.371583938598633
Epoch 340, val loss: 1.2116272449493408
Epoch 350, training loss: 13.732376098632812 = 0.9979462623596191 + 2.0 * 6.367214679718018
Epoch 350, val loss: 1.1891694068908691
Epoch 360, training loss: 13.709519386291504 = 0.9643483757972717 + 2.0 * 6.372585296630859
Epoch 360, val loss: 1.1671950817108154
Epoch 370, training loss: 13.657805442810059 = 0.931723415851593 + 2.0 * 6.363040924072266
Epoch 370, val loss: 1.145693302154541
Epoch 380, training loss: 13.613666534423828 = 0.8996663689613342 + 2.0 * 6.35699987411499
Epoch 380, val loss: 1.1246250867843628
Epoch 390, training loss: 13.576202392578125 = 0.8678202629089355 + 2.0 * 6.354191303253174
Epoch 390, val loss: 1.1036008596420288
Epoch 400, training loss: 13.536417007446289 = 0.8360408544540405 + 2.0 * 6.350188255310059
Epoch 400, val loss: 1.0824867486953735
Epoch 410, training loss: 13.508028984069824 = 0.8043400645256042 + 2.0 * 6.351844310760498
Epoch 410, val loss: 1.0612186193466187
Epoch 420, training loss: 13.463749885559082 = 0.7727419137954712 + 2.0 * 6.345503807067871
Epoch 420, val loss: 1.0400692224502563
Epoch 430, training loss: 13.426379203796387 = 0.7413833737373352 + 2.0 * 6.342497825622559
Epoch 430, val loss: 1.0188207626342773
Epoch 440, training loss: 13.395208358764648 = 0.7101819515228271 + 2.0 * 6.342513084411621
Epoch 440, val loss: 0.9977104067802429
Epoch 450, training loss: 13.357333183288574 = 0.6795445084571838 + 2.0 * 6.338894367218018
Epoch 450, val loss: 0.9768414497375488
Epoch 460, training loss: 13.319554328918457 = 0.6494985818862915 + 2.0 * 6.335027694702148
Epoch 460, val loss: 0.9564684629440308
Epoch 470, training loss: 13.292146682739258 = 0.6200435161590576 + 2.0 * 6.3360514640808105
Epoch 470, val loss: 0.9367645978927612
Epoch 480, training loss: 13.258520126342773 = 0.5917100310325623 + 2.0 * 6.333405017852783
Epoch 480, val loss: 0.9178345203399658
Epoch 490, training loss: 13.221972465515137 = 0.5643118619918823 + 2.0 * 6.328830242156982
Epoch 490, val loss: 0.8999977111816406
Epoch 500, training loss: 13.190710067749023 = 0.5378733277320862 + 2.0 * 6.326418399810791
Epoch 500, val loss: 0.8831681609153748
Epoch 510, training loss: 13.16952896118164 = 0.5123071670532227 + 2.0 * 6.328610897064209
Epoch 510, val loss: 0.867408037185669
Epoch 520, training loss: 13.140061378479004 = 0.4879935681819916 + 2.0 * 6.326034069061279
Epoch 520, val loss: 0.8529109954833984
Epoch 530, training loss: 13.105975151062012 = 0.46474984288215637 + 2.0 * 6.32061243057251
Epoch 530, val loss: 0.8397027850151062
Epoch 540, training loss: 13.080562591552734 = 0.4424279034137726 + 2.0 * 6.319067478179932
Epoch 540, val loss: 0.827642560005188
Epoch 550, training loss: 13.055207252502441 = 0.420905202627182 + 2.0 * 6.317151069641113
Epoch 550, val loss: 0.8165571093559265
Epoch 560, training loss: 13.033456802368164 = 0.4001080095767975 + 2.0 * 6.31667423248291
Epoch 560, val loss: 0.8064461946487427
Epoch 570, training loss: 13.014458656311035 = 0.3801198899745941 + 2.0 * 6.317169189453125
Epoch 570, val loss: 0.7973381876945496
Epoch 580, training loss: 12.986384391784668 = 0.3610677719116211 + 2.0 * 6.312658309936523
Epoch 580, val loss: 0.7892247438430786
Epoch 590, training loss: 12.96837329864502 = 0.3426811397075653 + 2.0 * 6.3128461837768555
Epoch 590, val loss: 0.7819844484329224
Epoch 600, training loss: 12.950329780578613 = 0.32493895292282104 + 2.0 * 6.312695503234863
Epoch 600, val loss: 0.7756011486053467
Epoch 610, training loss: 12.927879333496094 = 0.30799737572669983 + 2.0 * 6.309940814971924
Epoch 610, val loss: 0.7700760364532471
Epoch 620, training loss: 12.906501770019531 = 0.29160988330841064 + 2.0 * 6.307446002960205
Epoch 620, val loss: 0.7653813362121582
Epoch 630, training loss: 12.889131546020508 = 0.27583760023117065 + 2.0 * 6.306646823883057
Epoch 630, val loss: 0.7614791989326477
Epoch 640, training loss: 12.87070083618164 = 0.260709285736084 + 2.0 * 6.304996013641357
Epoch 640, val loss: 0.7583184838294983
Epoch 650, training loss: 12.853672981262207 = 0.2463003545999527 + 2.0 * 6.303686141967773
Epoch 650, val loss: 0.7559871077537537
Epoch 660, training loss: 12.863397598266602 = 0.23253516852855682 + 2.0 * 6.315431118011475
Epoch 660, val loss: 0.7544138431549072
Epoch 670, training loss: 12.82264232635498 = 0.2196744829416275 + 2.0 * 6.301484107971191
Epoch 670, val loss: 0.753624439239502
Epoch 680, training loss: 12.808012962341309 = 0.20752131938934326 + 2.0 * 6.300245761871338
Epoch 680, val loss: 0.7535673379898071
Epoch 690, training loss: 12.792985916137695 = 0.19604846835136414 + 2.0 * 6.298468589782715
Epoch 690, val loss: 0.754154622554779
Epoch 700, training loss: 12.77912425994873 = 0.18522998690605164 + 2.0 * 6.296947002410889
Epoch 700, val loss: 0.7554084062576294
Epoch 710, training loss: 12.781501770019531 = 0.17507301270961761 + 2.0 * 6.3032145500183105
Epoch 710, val loss: 0.7573128342628479
Epoch 720, training loss: 12.764491081237793 = 0.1656034141778946 + 2.0 * 6.29944372177124
Epoch 720, val loss: 0.7597364783287048
Epoch 730, training loss: 12.7471284866333 = 0.1568455547094345 + 2.0 * 6.295141696929932
Epoch 730, val loss: 0.7627459764480591
Epoch 740, training loss: 12.735785484313965 = 0.1486731469631195 + 2.0 * 6.293556213378906
Epoch 740, val loss: 0.7661787271499634
Epoch 750, training loss: 12.73330307006836 = 0.14103879034519196 + 2.0 * 6.2961320877075195
Epoch 750, val loss: 0.7700231075286865
Epoch 760, training loss: 12.717245101928711 = 0.13392435014247894 + 2.0 * 6.291660308837891
Epoch 760, val loss: 0.7742726802825928
Epoch 770, training loss: 12.708073616027832 = 0.12728795409202576 + 2.0 * 6.290392875671387
Epoch 770, val loss: 0.7789437174797058
Epoch 780, training loss: 12.708113670349121 = 0.12109441310167313 + 2.0 * 6.293509483337402
Epoch 780, val loss: 0.78378826379776
Epoch 790, training loss: 12.694591522216797 = 0.11534926295280457 + 2.0 * 6.289621353149414
Epoch 790, val loss: 0.7889639139175415
Epoch 800, training loss: 12.684979438781738 = 0.10996663570404053 + 2.0 * 6.287506580352783
Epoch 800, val loss: 0.794303834438324
Epoch 810, training loss: 12.686283111572266 = 0.1049298346042633 + 2.0 * 6.290676593780518
Epoch 810, val loss: 0.7998632788658142
Epoch 820, training loss: 12.674430847167969 = 0.10021407902240753 + 2.0 * 6.287108421325684
Epoch 820, val loss: 0.8055042028427124
Epoch 830, training loss: 12.665460586547852 = 0.09580573439598083 + 2.0 * 6.28482723236084
Epoch 830, val loss: 0.8113793730735779
Epoch 840, training loss: 12.663029670715332 = 0.09164676815271378 + 2.0 * 6.285691261291504
Epoch 840, val loss: 0.8172640800476074
Epoch 850, training loss: 12.652477264404297 = 0.08775435388088226 + 2.0 * 6.2823615074157715
Epoch 850, val loss: 0.8233178853988647
Epoch 860, training loss: 12.648187637329102 = 0.08408966660499573 + 2.0 * 6.282049179077148
Epoch 860, val loss: 0.829459011554718
Epoch 870, training loss: 12.64914608001709 = 0.08063221722841263 + 2.0 * 6.284256935119629
Epoch 870, val loss: 0.8356895446777344
Epoch 880, training loss: 12.63952922821045 = 0.07738003879785538 + 2.0 * 6.281074523925781
Epoch 880, val loss: 0.8419145941734314
Epoch 890, training loss: 12.638151168823242 = 0.07430536299943924 + 2.0 * 6.281922817230225
Epoch 890, val loss: 0.8481953740119934
Epoch 900, training loss: 12.626458168029785 = 0.0714012086391449 + 2.0 * 6.277528285980225
Epoch 900, val loss: 0.8544907569885254
Epoch 910, training loss: 12.625741958618164 = 0.06864813715219498 + 2.0 * 6.2785468101501465
Epoch 910, val loss: 0.8607684969902039
Epoch 920, training loss: 12.629073143005371 = 0.06604024767875671 + 2.0 * 6.2815165519714355
Epoch 920, val loss: 0.8670205473899841
Epoch 930, training loss: 12.616043090820312 = 0.06358376890420914 + 2.0 * 6.2762298583984375
Epoch 930, val loss: 0.8733556866645813
Epoch 940, training loss: 12.612651824951172 = 0.06124215945601463 + 2.0 * 6.275704860687256
Epoch 940, val loss: 0.8796025514602661
Epoch 950, training loss: 12.615859031677246 = 0.05902254953980446 + 2.0 * 6.278418064117432
Epoch 950, val loss: 0.8857700824737549
Epoch 960, training loss: 12.606502532958984 = 0.05693082511425018 + 2.0 * 6.274785995483398
Epoch 960, val loss: 0.8920500874519348
Epoch 970, training loss: 12.599714279174805 = 0.05493774265050888 + 2.0 * 6.272388458251953
Epoch 970, val loss: 0.8981797695159912
Epoch 980, training loss: 12.596551895141602 = 0.053034860640764236 + 2.0 * 6.271758556365967
Epoch 980, val loss: 0.9042774438858032
Epoch 990, training loss: 12.613423347473145 = 0.05121523141860962 + 2.0 * 6.28110408782959
Epoch 990, val loss: 0.910366415977478
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8111111111111111
0.8386926726410122
=== training gcn model ===
Epoch 0, training loss: 19.149810791015625 = 1.9561527967453003 + 2.0 * 8.596829414367676
Epoch 0, val loss: 1.9495054483413696
Epoch 10, training loss: 19.138912200927734 = 1.9459117650985718 + 2.0 * 8.596500396728516
Epoch 10, val loss: 1.9400813579559326
Epoch 20, training loss: 19.12126922607422 = 1.9336059093475342 + 2.0 * 8.593832015991211
Epoch 20, val loss: 1.9283415079116821
Epoch 30, training loss: 19.062583923339844 = 1.9169765710830688 + 2.0 * 8.572803497314453
Epoch 30, val loss: 1.9121453762054443
Epoch 40, training loss: 18.765670776367188 = 1.8956124782562256 + 2.0 * 8.435029029846191
Epoch 40, val loss: 1.8915802240371704
Epoch 50, training loss: 17.5537166595459 = 1.8719582557678223 + 2.0 * 7.840879440307617
Epoch 50, val loss: 1.8689770698547363
Epoch 60, training loss: 16.94011688232422 = 1.8530588150024414 + 2.0 * 7.543529510498047
Epoch 60, val loss: 1.8516294956207275
Epoch 70, training loss: 16.526063919067383 = 1.8383290767669678 + 2.0 * 7.343867301940918
Epoch 70, val loss: 1.8372516632080078
Epoch 80, training loss: 16.057268142700195 = 1.8245806694030762 + 2.0 * 7.1163434982299805
Epoch 80, val loss: 1.8239706754684448
Epoch 90, training loss: 15.716263771057129 = 1.811254858970642 + 2.0 * 6.952504634857178
Epoch 90, val loss: 1.8115005493164062
Epoch 100, training loss: 15.437185287475586 = 1.7978284358978271 + 2.0 * 6.81967830657959
Epoch 100, val loss: 1.7990390062332153
Epoch 110, training loss: 15.25765609741211 = 1.783488392829895 + 2.0 * 6.737083911895752
Epoch 110, val loss: 1.7860349416732788
Epoch 120, training loss: 15.147058486938477 = 1.7685846090316772 + 2.0 * 6.689237117767334
Epoch 120, val loss: 1.772585391998291
Epoch 130, training loss: 15.053033828735352 = 1.7538762092590332 + 2.0 * 6.64957857131958
Epoch 130, val loss: 1.7592391967773438
Epoch 140, training loss: 14.97329330444336 = 1.739019751548767 + 2.0 * 6.6171369552612305
Epoch 140, val loss: 1.74556565284729
Epoch 150, training loss: 14.903668403625488 = 1.723136067390442 + 2.0 * 6.590266227722168
Epoch 150, val loss: 1.7308201789855957
Epoch 160, training loss: 14.83632755279541 = 1.7056481838226318 + 2.0 * 6.5653395652771
Epoch 160, val loss: 1.7147581577301025
Epoch 170, training loss: 14.776476860046387 = 1.6858834028244019 + 2.0 * 6.545296669006348
Epoch 170, val loss: 1.6970102787017822
Epoch 180, training loss: 14.717279434204102 = 1.6635905504226685 + 2.0 * 6.526844501495361
Epoch 180, val loss: 1.6771711111068726
Epoch 190, training loss: 14.656402587890625 = 1.6385245323181152 + 2.0 * 6.508938789367676
Epoch 190, val loss: 1.6548229455947876
Epoch 200, training loss: 14.59712028503418 = 1.6103248596191406 + 2.0 * 6.4933977127075195
Epoch 200, val loss: 1.629678726196289
Epoch 210, training loss: 14.538575172424316 = 1.5786844491958618 + 2.0 * 6.479945182800293
Epoch 210, val loss: 1.6014846563339233
Epoch 220, training loss: 14.48348331451416 = 1.5434751510620117 + 2.0 * 6.470004081726074
Epoch 220, val loss: 1.5703192949295044
Epoch 230, training loss: 14.42152214050293 = 1.5053133964538574 + 2.0 * 6.458104133605957
Epoch 230, val loss: 1.5366705656051636
Epoch 240, training loss: 14.357823371887207 = 1.4641139507293701 + 2.0 * 6.446854591369629
Epoch 240, val loss: 1.5005966424942017
Epoch 250, training loss: 14.299579620361328 = 1.4203581809997559 + 2.0 * 6.439610958099365
Epoch 250, val loss: 1.4627565145492554
Epoch 260, training loss: 14.238911628723145 = 1.375139832496643 + 2.0 * 6.431885719299316
Epoch 260, val loss: 1.424689769744873
Epoch 270, training loss: 14.17455005645752 = 1.3294037580490112 + 2.0 * 6.422573089599609
Epoch 270, val loss: 1.3866199254989624
Epoch 280, training loss: 14.11390495300293 = 1.2831428050994873 + 2.0 * 6.415380954742432
Epoch 280, val loss: 1.348717212677002
Epoch 290, training loss: 14.063648223876953 = 1.236735463142395 + 2.0 * 6.413456439971924
Epoch 290, val loss: 1.311280369758606
Epoch 300, training loss: 14.007369041442871 = 1.1909312009811401 + 2.0 * 6.408218860626221
Epoch 300, val loss: 1.2750287055969238
Epoch 310, training loss: 13.943549156188965 = 1.1463018655776978 + 2.0 * 6.398623466491699
Epoch 310, val loss: 1.2399406433105469
Epoch 320, training loss: 13.887085914611816 = 1.1025009155273438 + 2.0 * 6.392292499542236
Epoch 320, val loss: 1.2058684825897217
Epoch 330, training loss: 13.834978103637695 = 1.059356451034546 + 2.0 * 6.387810707092285
Epoch 330, val loss: 1.1727348566055298
Epoch 340, training loss: 13.787590026855469 = 1.017359972000122 + 2.0 * 6.385115146636963
Epoch 340, val loss: 1.1407244205474854
Epoch 350, training loss: 13.74063777923584 = 0.9769002795219421 + 2.0 * 6.381868839263916
Epoch 350, val loss: 1.1102230548858643
Epoch 360, training loss: 13.687768936157227 = 0.9381483793258667 + 2.0 * 6.374810218811035
Epoch 360, val loss: 1.0813016891479492
Epoch 370, training loss: 13.64217758178711 = 0.9008417725563049 + 2.0 * 6.370667934417725
Epoch 370, val loss: 1.0538923740386963
Epoch 380, training loss: 13.60526180267334 = 0.8651142716407776 + 2.0 * 6.3700737953186035
Epoch 380, val loss: 1.0279110670089722
Epoch 390, training loss: 13.556310653686523 = 0.8309292793273926 + 2.0 * 6.3626909255981445
Epoch 390, val loss: 1.0035450458526611
Epoch 400, training loss: 13.524759292602539 = 0.7982668876647949 + 2.0 * 6.363246440887451
Epoch 400, val loss: 0.9807551503181458
Epoch 410, training loss: 13.4803466796875 = 0.7673730850219727 + 2.0 * 6.356486797332764
Epoch 410, val loss: 0.9595592617988586
Epoch 420, training loss: 13.452279090881348 = 0.7379055023193359 + 2.0 * 6.357186794281006
Epoch 420, val loss: 0.9397604465484619
Epoch 430, training loss: 13.413605690002441 = 0.7100015878677368 + 2.0 * 6.351801872253418
Epoch 430, val loss: 0.9215494394302368
Epoch 440, training loss: 13.378307342529297 = 0.6834622621536255 + 2.0 * 6.3474225997924805
Epoch 440, val loss: 0.9047636389732361
Epoch 450, training loss: 13.349740028381348 = 0.6581730246543884 + 2.0 * 6.345783710479736
Epoch 450, val loss: 0.8893111348152161
Epoch 460, training loss: 13.332159996032715 = 0.6341143846511841 + 2.0 * 6.34902286529541
Epoch 460, val loss: 0.8750967979431152
Epoch 470, training loss: 13.294410705566406 = 0.6114447116851807 + 2.0 * 6.341483116149902
Epoch 470, val loss: 0.8622169494628906
Epoch 480, training loss: 13.265216827392578 = 0.589769184589386 + 2.0 * 6.337723731994629
Epoch 480, val loss: 0.850399911403656
Epoch 490, training loss: 13.247344017028809 = 0.5689738392829895 + 2.0 * 6.3391852378845215
Epoch 490, val loss: 0.8395863175392151
Epoch 500, training loss: 13.220710754394531 = 0.5491100549697876 + 2.0 * 6.3358001708984375
Epoch 500, val loss: 0.8298236727714539
Epoch 510, training loss: 13.192225456237793 = 0.5300179719924927 + 2.0 * 6.331103801727295
Epoch 510, val loss: 0.8209718465805054
Epoch 520, training loss: 13.173613548278809 = 0.5115625858306885 + 2.0 * 6.33102560043335
Epoch 520, val loss: 0.812872052192688
Epoch 530, training loss: 13.15074634552002 = 0.4936944842338562 + 2.0 * 6.328526020050049
Epoch 530, val loss: 0.8056456446647644
Epoch 540, training loss: 13.129728317260742 = 0.4765712320804596 + 2.0 * 6.326578617095947
Epoch 540, val loss: 0.7991182208061218
Epoch 550, training loss: 13.108280181884766 = 0.45982563495635986 + 2.0 * 6.324227333068848
Epoch 550, val loss: 0.7933546900749207
Epoch 560, training loss: 13.088322639465332 = 0.44346269965171814 + 2.0 * 6.32243013381958
Epoch 560, val loss: 0.7881698608398438
Epoch 570, training loss: 13.074884414672852 = 0.42753803730010986 + 2.0 * 6.323673248291016
Epoch 570, val loss: 0.7836125493049622
Epoch 580, training loss: 13.05462646484375 = 0.4121250510215759 + 2.0 * 6.321250915527344
Epoch 580, val loss: 0.7797427773475647
Epoch 590, training loss: 13.034801483154297 = 0.3970658481121063 + 2.0 * 6.3188676834106445
Epoch 590, val loss: 0.776525616645813
Epoch 600, training loss: 13.021906852722168 = 0.3823228180408478 + 2.0 * 6.319791793823242
Epoch 600, val loss: 0.7738621234893799
Epoch 610, training loss: 13.001913070678711 = 0.3679538369178772 + 2.0 * 6.31697940826416
Epoch 610, val loss: 0.7717865705490112
Epoch 620, training loss: 12.98337459564209 = 0.35395553708076477 + 2.0 * 6.314709663391113
Epoch 620, val loss: 0.7703607082366943
Epoch 630, training loss: 12.971061706542969 = 0.34026020765304565 + 2.0 * 6.31540060043335
Epoch 630, val loss: 0.769440233707428
Epoch 640, training loss: 12.95651626586914 = 0.32707709074020386 + 2.0 * 6.3147196769714355
Epoch 640, val loss: 0.7690867781639099
Epoch 650, training loss: 12.93454647064209 = 0.3142477571964264 + 2.0 * 6.310149192810059
Epoch 650, val loss: 0.7692984342575073
Epoch 660, training loss: 12.920561790466309 = 0.3018096387386322 + 2.0 * 6.309376239776611
Epoch 660, val loss: 0.7700756788253784
Epoch 670, training loss: 12.91724967956543 = 0.28975045680999756 + 2.0 * 6.31374979019165
Epoch 670, val loss: 0.7713660597801208
Epoch 680, training loss: 12.899688720703125 = 0.2781711220741272 + 2.0 * 6.310758590698242
Epoch 680, val loss: 0.7731887102127075
Epoch 690, training loss: 12.877902030944824 = 0.2669929265975952 + 2.0 * 6.305454730987549
Epoch 690, val loss: 0.7755374312400818
Epoch 700, training loss: 12.866068840026855 = 0.2561948895454407 + 2.0 * 6.30493688583374
Epoch 700, val loss: 0.7782943844795227
Epoch 710, training loss: 12.860495567321777 = 0.24579277634620667 + 2.0 * 6.307351589202881
Epoch 710, val loss: 0.781470537185669
Epoch 720, training loss: 12.841815948486328 = 0.2358628660440445 + 2.0 * 6.302976608276367
Epoch 720, val loss: 0.7850589156150818
Epoch 730, training loss: 12.82941722869873 = 0.226291224360466 + 2.0 * 6.301562786102295
Epoch 730, val loss: 0.7891623973846436
Epoch 740, training loss: 12.816364288330078 = 0.217077374458313 + 2.0 * 6.299643516540527
Epoch 740, val loss: 0.7935380935668945
Epoch 750, training loss: 12.813977241516113 = 0.20824897289276123 + 2.0 * 6.302864074707031
Epoch 750, val loss: 0.7981252074241638
Epoch 760, training loss: 12.80117416381836 = 0.19987478852272034 + 2.0 * 6.300649642944336
Epoch 760, val loss: 0.8031137585639954
Epoch 770, training loss: 12.787604331970215 = 0.19185955822467804 + 2.0 * 6.297872543334961
Epoch 770, val loss: 0.8085159063339233
Epoch 780, training loss: 12.776762008666992 = 0.18415160477161407 + 2.0 * 6.296305179595947
Epoch 780, val loss: 0.8140720129013062
Epoch 790, training loss: 12.77545166015625 = 0.17677965760231018 + 2.0 * 6.299335956573486
Epoch 790, val loss: 0.8197701573371887
Epoch 800, training loss: 12.75907039642334 = 0.16973555088043213 + 2.0 * 6.2946672439575195
Epoch 800, val loss: 0.8256983160972595
Epoch 810, training loss: 12.749229431152344 = 0.16299232840538025 + 2.0 * 6.293118476867676
Epoch 810, val loss: 0.8320101499557495
Epoch 820, training loss: 12.747859954833984 = 0.1565249115228653 + 2.0 * 6.29566764831543
Epoch 820, val loss: 0.838319718837738
Epoch 830, training loss: 12.733816146850586 = 0.15034621953964233 + 2.0 * 6.2917351722717285
Epoch 830, val loss: 0.8447180390357971
Epoch 840, training loss: 12.726494789123535 = 0.1444292813539505 + 2.0 * 6.291032791137695
Epoch 840, val loss: 0.8513679504394531
Epoch 850, training loss: 12.72833251953125 = 0.13876895606517792 + 2.0 * 6.294781684875488
Epoch 850, val loss: 0.858070433139801
Epoch 860, training loss: 12.713513374328613 = 0.1333862841129303 + 2.0 * 6.290063381195068
Epoch 860, val loss: 0.8650676012039185
Epoch 870, training loss: 12.707880973815918 = 0.1282106190919876 + 2.0 * 6.289834976196289
Epoch 870, val loss: 0.8720331788063049
Epoch 880, training loss: 12.699435234069824 = 0.12326828390359879 + 2.0 * 6.288083553314209
Epoch 880, val loss: 0.8789080381393433
Epoch 890, training loss: 12.690704345703125 = 0.11854778975248337 + 2.0 * 6.286078453063965
Epoch 890, val loss: 0.8860322833061218
Epoch 900, training loss: 12.690629959106445 = 0.11401917785406113 + 2.0 * 6.288305282592773
Epoch 900, val loss: 0.8931503295898438
Epoch 910, training loss: 12.684536933898926 = 0.10971588641405106 + 2.0 * 6.287410736083984
Epoch 910, val loss: 0.9001912474632263
Epoch 920, training loss: 12.673678398132324 = 0.10561638325452805 + 2.0 * 6.284030914306641
Epoch 920, val loss: 0.9073570966720581
Epoch 930, training loss: 12.66816520690918 = 0.10168813914060593 + 2.0 * 6.283238410949707
Epoch 930, val loss: 0.914593517780304
Epoch 940, training loss: 12.661564826965332 = 0.09792549908161163 + 2.0 * 6.281819820404053
Epoch 940, val loss: 0.921863317489624
Epoch 950, training loss: 12.676350593566895 = 0.09433306008577347 + 2.0 * 6.291008949279785
Epoch 950, val loss: 0.9289904236793518
Epoch 960, training loss: 12.651801109313965 = 0.09088481962680817 + 2.0 * 6.280457973480225
Epoch 960, val loss: 0.936127245426178
Epoch 970, training loss: 12.648397445678711 = 0.08759851008653641 + 2.0 * 6.280399322509766
Epoch 970, val loss: 0.9433631896972656
Epoch 980, training loss: 12.645394325256348 = 0.08446245640516281 + 2.0 * 6.280466079711914
Epoch 980, val loss: 0.9506986141204834
Epoch 990, training loss: 12.64102554321289 = 0.0814574733376503 + 2.0 * 6.279784202575684
Epoch 990, val loss: 0.957697331905365
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.788888888888889
0.8376383763837639
The final CL Acc:0.79506, 0.01145, The final GNN Acc:0.83957, 0.00203
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.3 der2:0.3 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11662])
remove edge: torch.Size([2, 9492])
updated graph: torch.Size([2, 10598])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 19.140460968017578 = 1.946744680404663 + 2.0 * 8.596858024597168
Epoch 0, val loss: 1.949194312095642
Epoch 10, training loss: 19.12982749938965 = 1.9366965293884277 + 2.0 * 8.596565246582031
Epoch 10, val loss: 1.9385312795639038
Epoch 20, training loss: 19.112503051757812 = 1.9243617057800293 + 2.0 * 8.594070434570312
Epoch 20, val loss: 1.9252210855484009
Epoch 30, training loss: 19.0504150390625 = 1.9079614877700806 + 2.0 * 8.571227073669434
Epoch 30, val loss: 1.9077413082122803
Epoch 40, training loss: 18.68083381652832 = 1.8885105848312378 + 2.0 * 8.396162033081055
Epoch 40, val loss: 1.8879902362823486
Epoch 50, training loss: 17.388452529907227 = 1.8690470457077026 + 2.0 * 7.759702682495117
Epoch 50, val loss: 1.8689594268798828
Epoch 60, training loss: 16.490833282470703 = 1.854921817779541 + 2.0 * 7.31795597076416
Epoch 60, val loss: 1.8558714389801025
Epoch 70, training loss: 15.935837745666504 = 1.842332124710083 + 2.0 * 7.0467529296875
Epoch 70, val loss: 1.843354344367981
Epoch 80, training loss: 15.578993797302246 = 1.8303617238998413 + 2.0 * 6.874316215515137
Epoch 80, val loss: 1.8312573432922363
Epoch 90, training loss: 15.353109359741211 = 1.8175441026687622 + 2.0 * 6.767782688140869
Epoch 90, val loss: 1.8187092542648315
Epoch 100, training loss: 15.214217185974121 = 1.8043057918548584 + 2.0 * 6.704955577850342
Epoch 100, val loss: 1.8060060739517212
Epoch 110, training loss: 15.103250503540039 = 1.7915326356887817 + 2.0 * 6.655858993530273
Epoch 110, val loss: 1.793837308883667
Epoch 120, training loss: 15.01062297821045 = 1.7790285348892212 + 2.0 * 6.61579704284668
Epoch 120, val loss: 1.7821424007415771
Epoch 130, training loss: 14.933721542358398 = 1.766180396080017 + 2.0 * 6.583770751953125
Epoch 130, val loss: 1.7703146934509277
Epoch 140, training loss: 14.868514060974121 = 1.752544641494751 + 2.0 * 6.557984828948975
Epoch 140, val loss: 1.758068323135376
Epoch 150, training loss: 14.810731887817383 = 1.7377002239227295 + 2.0 * 6.536515712738037
Epoch 150, val loss: 1.744999885559082
Epoch 160, training loss: 14.76313591003418 = 1.721238136291504 + 2.0 * 6.520948886871338
Epoch 160, val loss: 1.7307943105697632
Epoch 170, training loss: 14.718483924865723 = 1.7028586864471436 + 2.0 * 6.5078125
Epoch 170, val loss: 1.7152352333068848
Epoch 180, training loss: 14.667607307434082 = 1.6823985576629639 + 2.0 * 6.4926042556762695
Epoch 180, val loss: 1.6980210542678833
Epoch 190, training loss: 14.619729995727539 = 1.659421443939209 + 2.0 * 6.480154037475586
Epoch 190, val loss: 1.678879737854004
Epoch 200, training loss: 14.572376251220703 = 1.6337237358093262 + 2.0 * 6.469326019287109
Epoch 200, val loss: 1.657515048980713
Epoch 210, training loss: 14.53394889831543 = 1.6050490140914917 + 2.0 * 6.464449882507324
Epoch 210, val loss: 1.63374662399292
Epoch 220, training loss: 14.478243827819824 = 1.5733429193496704 + 2.0 * 6.452450275421143
Epoch 220, val loss: 1.6074934005737305
Epoch 230, training loss: 14.430150032043457 = 1.5385901927947998 + 2.0 * 6.445779800415039
Epoch 230, val loss: 1.578773021697998
Epoch 240, training loss: 14.371809005737305 = 1.5011085271835327 + 2.0 * 6.43535041809082
Epoch 240, val loss: 1.5477423667907715
Epoch 250, training loss: 14.318156242370605 = 1.460861086845398 + 2.0 * 6.428647518157959
Epoch 250, val loss: 1.5144305229187012
Epoch 260, training loss: 14.262715339660645 = 1.4179847240447998 + 2.0 * 6.422365188598633
Epoch 260, val loss: 1.4790492057800293
Epoch 270, training loss: 14.206986427307129 = 1.3728753328323364 + 2.0 * 6.417055606842041
Epoch 270, val loss: 1.4417866468429565
Epoch 280, training loss: 14.155760765075684 = 1.3261024951934814 + 2.0 * 6.414829254150391
Epoch 280, val loss: 1.4031847715377808
Epoch 290, training loss: 14.103285789489746 = 1.2791168689727783 + 2.0 * 6.412084579467773
Epoch 290, val loss: 1.364576816558838
Epoch 300, training loss: 14.042365074157715 = 1.2327241897583008 + 2.0 * 6.404820442199707
Epoch 300, val loss: 1.3265315294265747
Epoch 310, training loss: 13.985210418701172 = 1.187239170074463 + 2.0 * 6.398985385894775
Epoch 310, val loss: 1.2895570993423462
Epoch 320, training loss: 13.933096885681152 = 1.1428662538528442 + 2.0 * 6.395115375518799
Epoch 320, val loss: 1.2536600828170776
Epoch 330, training loss: 13.882217407226562 = 1.0998339653015137 + 2.0 * 6.3911919593811035
Epoch 330, val loss: 1.2191051244735718
Epoch 340, training loss: 13.86644172668457 = 1.0584664344787598 + 2.0 * 6.403987407684326
Epoch 340, val loss: 1.1861807107925415
Epoch 350, training loss: 13.79777717590332 = 1.0194745063781738 + 2.0 * 6.389151573181152
Epoch 350, val loss: 1.1553406715393066
Epoch 360, training loss: 13.746638298034668 = 0.9826290011405945 + 2.0 * 6.382004737854004
Epoch 360, val loss: 1.1266565322875977
Epoch 370, training loss: 13.703673362731934 = 0.9475229382514954 + 2.0 * 6.378075122833252
Epoch 370, val loss: 1.0997390747070312
Epoch 380, training loss: 13.663310050964355 = 0.9139307737350464 + 2.0 * 6.37468957901001
Epoch 380, val loss: 1.0743889808654785
Epoch 390, training loss: 13.637123107910156 = 0.8817188143730164 + 2.0 * 6.377702236175537
Epoch 390, val loss: 1.050482153892517
Epoch 400, training loss: 13.590911865234375 = 0.8510504364967346 + 2.0 * 6.369930744171143
Epoch 400, val loss: 1.0282039642333984
Epoch 410, training loss: 13.55859375 = 0.8218708038330078 + 2.0 * 6.368361473083496
Epoch 410, val loss: 1.007564663887024
Epoch 420, training loss: 13.521611213684082 = 0.7937837839126587 + 2.0 * 6.363913536071777
Epoch 420, val loss: 0.9882895946502686
Epoch 430, training loss: 13.489153861999512 = 0.7666184306144714 + 2.0 * 6.361267566680908
Epoch 430, val loss: 0.9700905084609985
Epoch 440, training loss: 13.480060577392578 = 0.7402394413948059 + 2.0 * 6.369910717010498
Epoch 440, val loss: 0.9529015421867371
Epoch 450, training loss: 13.432433128356934 = 0.714747965335846 + 2.0 * 6.358842372894287
Epoch 450, val loss: 0.9367372393608093
Epoch 460, training loss: 13.39989185333252 = 0.6899721026420593 + 2.0 * 6.354959964752197
Epoch 460, val loss: 0.9215905666351318
Epoch 470, training loss: 13.371114730834961 = 0.6657546758651733 + 2.0 * 6.352680206298828
Epoch 470, val loss: 0.9072330594062805
Epoch 480, training loss: 13.341571807861328 = 0.6420851349830627 + 2.0 * 6.349743366241455
Epoch 480, val loss: 0.8936203718185425
Epoch 490, training loss: 13.331978797912598 = 0.6188830733299255 + 2.0 * 6.356547832489014
Epoch 490, val loss: 0.8807693719863892
Epoch 500, training loss: 13.295138359069824 = 0.5963670611381531 + 2.0 * 6.349385738372803
Epoch 500, val loss: 0.8685864806175232
Epoch 510, training loss: 13.263870239257812 = 0.5742784738540649 + 2.0 * 6.3447957038879395
Epoch 510, val loss: 0.8572489619255066
Epoch 520, training loss: 13.23611831665039 = 0.5525714755058289 + 2.0 * 6.341773509979248
Epoch 520, val loss: 0.8465210199356079
Epoch 530, training loss: 13.210929870605469 = 0.5312052965164185 + 2.0 * 6.33986234664917
Epoch 530, val loss: 0.8363255262374878
Epoch 540, training loss: 13.195944786071777 = 0.5101916193962097 + 2.0 * 6.342876434326172
Epoch 540, val loss: 0.826722264289856
Epoch 550, training loss: 13.175168991088867 = 0.4896605312824249 + 2.0 * 6.342754364013672
Epoch 550, val loss: 0.8175591826438904
Epoch 560, training loss: 13.140628814697266 = 0.46966826915740967 + 2.0 * 6.335480213165283
Epoch 560, val loss: 0.8091161847114563
Epoch 570, training loss: 13.11708927154541 = 0.45019224286079407 + 2.0 * 6.33344841003418
Epoch 570, val loss: 0.8012577891349792
Epoch 580, training loss: 13.100822448730469 = 0.431173175573349 + 2.0 * 6.334824562072754
Epoch 580, val loss: 0.7939968705177307
Epoch 590, training loss: 13.085848808288574 = 0.4127710163593292 + 2.0 * 6.336538791656494
Epoch 590, val loss: 0.7870684266090393
Epoch 600, training loss: 13.053976058959961 = 0.3949740529060364 + 2.0 * 6.329501152038574
Epoch 600, val loss: 0.780867338180542
Epoch 610, training loss: 13.033160209655762 = 0.37778714299201965 + 2.0 * 6.327686309814453
Epoch 610, val loss: 0.7752017974853516
Epoch 620, training loss: 13.014595985412598 = 0.3611648380756378 + 2.0 * 6.326715469360352
Epoch 620, val loss: 0.7700266242027283
Epoch 630, training loss: 12.99947738647461 = 0.34511712193489075 + 2.0 * 6.327179908752441
Epoch 630, val loss: 0.7653017044067383
Epoch 640, training loss: 12.977242469787598 = 0.3296770453453064 + 2.0 * 6.323782920837402
Epoch 640, val loss: 0.7611386179924011
Epoch 650, training loss: 12.960796356201172 = 0.3147865831851959 + 2.0 * 6.323004722595215
Epoch 650, val loss: 0.7574852705001831
Epoch 660, training loss: 12.947619438171387 = 0.3005274832248688 + 2.0 * 6.323545932769775
Epoch 660, val loss: 0.7540289163589478
Epoch 670, training loss: 12.926819801330566 = 0.2868531048297882 + 2.0 * 6.31998348236084
Epoch 670, val loss: 0.7511852979660034
Epoch 680, training loss: 12.910362243652344 = 0.27378442883491516 + 2.0 * 6.318288803100586
Epoch 680, val loss: 0.7486353516578674
Epoch 690, training loss: 12.893572807312012 = 0.26122695207595825 + 2.0 * 6.316173076629639
Epoch 690, val loss: 0.746557354927063
Epoch 700, training loss: 12.882893562316895 = 0.24916376173496246 + 2.0 * 6.316864967346191
Epoch 700, val loss: 0.7447524666786194
Epoch 710, training loss: 12.870447158813477 = 0.2376192808151245 + 2.0 * 6.316413879394531
Epoch 710, val loss: 0.7430605292320251
Epoch 720, training loss: 12.853604316711426 = 0.22662004828453064 + 2.0 * 6.313492298126221
Epoch 720, val loss: 0.7418698668479919
Epoch 730, training loss: 12.840751647949219 = 0.2161301225423813 + 2.0 * 6.312310695648193
Epoch 730, val loss: 0.7409745454788208
Epoch 740, training loss: 12.82633113861084 = 0.2061208337545395 + 2.0 * 6.310105323791504
Epoch 740, val loss: 0.7404499053955078
Epoch 750, training loss: 12.850923538208008 = 0.19660410284996033 + 2.0 * 6.327159881591797
Epoch 750, val loss: 0.740113377571106
Epoch 760, training loss: 12.805045127868652 = 0.18753460049629211 + 2.0 * 6.308755397796631
Epoch 760, val loss: 0.7402159571647644
Epoch 770, training loss: 12.793827056884766 = 0.17897430062294006 + 2.0 * 6.307426452636719
Epoch 770, val loss: 0.7405775785446167
Epoch 780, training loss: 12.782942771911621 = 0.17084325850009918 + 2.0 * 6.306049823760986
Epoch 780, val loss: 0.7411998510360718
Epoch 790, training loss: 12.775428771972656 = 0.16310618817806244 + 2.0 * 6.306161403656006
Epoch 790, val loss: 0.7421122193336487
Epoch 800, training loss: 12.765619277954102 = 0.15575732290744781 + 2.0 * 6.304931163787842
Epoch 800, val loss: 0.7430926561355591
Epoch 810, training loss: 12.760709762573242 = 0.14883041381835938 + 2.0 * 6.305939674377441
Epoch 810, val loss: 0.7445269823074341
Epoch 820, training loss: 12.746198654174805 = 0.1422601044178009 + 2.0 * 6.301969051361084
Epoch 820, val loss: 0.7460851073265076
Epoch 830, training loss: 12.737464904785156 = 0.13601361215114594 + 2.0 * 6.30072546005249
Epoch 830, val loss: 0.7479314208030701
Epoch 840, training loss: 12.735947608947754 = 0.13007865846157074 + 2.0 * 6.302934646606445
Epoch 840, val loss: 0.7500194907188416
Epoch 850, training loss: 12.728839874267578 = 0.12448230385780334 + 2.0 * 6.302178859710693
Epoch 850, val loss: 0.7520502209663391
Epoch 860, training loss: 12.717752456665039 = 0.11916409432888031 + 2.0 * 6.2992939949035645
Epoch 860, val loss: 0.7545236945152283
Epoch 870, training loss: 12.709875106811523 = 0.1141507625579834 + 2.0 * 6.2978620529174805
Epoch 870, val loss: 0.7570136785507202
Epoch 880, training loss: 12.709233283996582 = 0.10939065366983414 + 2.0 * 6.29992151260376
Epoch 880, val loss: 0.7598006725311279
Epoch 890, training loss: 12.69566535949707 = 0.10487303137779236 + 2.0 * 6.295396327972412
Epoch 890, val loss: 0.7627249956130981
Epoch 900, training loss: 12.693646430969238 = 0.10057979822158813 + 2.0 * 6.296533107757568
Epoch 900, val loss: 0.7658995389938354
Epoch 910, training loss: 12.69456958770752 = 0.0965166762471199 + 2.0 * 6.2990264892578125
Epoch 910, val loss: 0.7690634727478027
Epoch 920, training loss: 12.680875778198242 = 0.0926649272441864 + 2.0 * 6.294105529785156
Epoch 920, val loss: 0.7724308967590332
Epoch 930, training loss: 12.678720474243164 = 0.08900099992752075 + 2.0 * 6.294859886169434
Epoch 930, val loss: 0.775938868522644
Epoch 940, training loss: 12.669525146484375 = 0.08552314341068268 + 2.0 * 6.292000770568848
Epoch 940, val loss: 0.7795620560646057
Epoch 950, training loss: 12.665168762207031 = 0.08221995085477829 + 2.0 * 6.291474342346191
Epoch 950, val loss: 0.7833666801452637
Epoch 960, training loss: 12.658862113952637 = 0.0790843516588211 + 2.0 * 6.289888858795166
Epoch 960, val loss: 0.7871890664100647
Epoch 970, training loss: 12.673172950744629 = 0.07609952241182327 + 2.0 * 6.298536777496338
Epoch 970, val loss: 0.7910943627357483
Epoch 980, training loss: 12.65662956237793 = 0.07325445860624313 + 2.0 * 6.291687488555908
Epoch 980, val loss: 0.7952145338058472
Epoch 990, training loss: 12.64659309387207 = 0.07054959982633591 + 2.0 * 6.288021564483643
Epoch 990, val loss: 0.799230694770813
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7703703703703704
0.8023194517659463
=== training gcn model ===
Epoch 0, training loss: 19.12296485900879 = 1.9292858839035034 + 2.0 * 8.596839904785156
Epoch 0, val loss: 1.9309107065200806
Epoch 10, training loss: 19.113033294677734 = 1.9201459884643555 + 2.0 * 8.596444129943848
Epoch 10, val loss: 1.9221625328063965
Epoch 20, training loss: 19.09498405456543 = 1.908754825592041 + 2.0 * 8.593114852905273
Epoch 20, val loss: 1.910841703414917
Epoch 30, training loss: 19.033214569091797 = 1.8932589292526245 + 2.0 * 8.569977760314941
Epoch 30, val loss: 1.895265817642212
Epoch 40, training loss: 18.760631561279297 = 1.874914526939392 + 2.0 * 8.442858695983887
Epoch 40, val loss: 1.8779656887054443
Epoch 50, training loss: 17.838930130004883 = 1.8550022840499878 + 2.0 * 7.991963863372803
Epoch 50, val loss: 1.8596060276031494
Epoch 60, training loss: 16.985979080200195 = 1.839099645614624 + 2.0 * 7.573439598083496
Epoch 60, val loss: 1.8462110757827759
Epoch 70, training loss: 16.274263381958008 = 1.8274767398834229 + 2.0 * 7.223393440246582
Epoch 70, val loss: 1.8356595039367676
Epoch 80, training loss: 15.777165412902832 = 1.8161884546279907 + 2.0 * 6.980488300323486
Epoch 80, val loss: 1.8245662450790405
Epoch 90, training loss: 15.542132377624512 = 1.8038808107376099 + 2.0 * 6.869125843048096
Epoch 90, val loss: 1.8126856088638306
Epoch 100, training loss: 15.383511543273926 = 1.789791464805603 + 2.0 * 6.796860218048096
Epoch 100, val loss: 1.7996811866760254
Epoch 110, training loss: 15.260676383972168 = 1.7757271528244019 + 2.0 * 6.742474555969238
Epoch 110, val loss: 1.7865948677062988
Epoch 120, training loss: 15.153318405151367 = 1.7620874643325806 + 2.0 * 6.695615291595459
Epoch 120, val loss: 1.7734298706054688
Epoch 130, training loss: 15.054649353027344 = 1.7479108572006226 + 2.0 * 6.653369426727295
Epoch 130, val loss: 1.759461522102356
Epoch 140, training loss: 14.956565856933594 = 1.7322227954864502 + 2.0 * 6.612171649932861
Epoch 140, val loss: 1.7444900274276733
Epoch 150, training loss: 14.87362003326416 = 1.7147595882415771 + 2.0 * 6.579430103302002
Epoch 150, val loss: 1.7283663749694824
Epoch 160, training loss: 14.808768272399902 = 1.6948589086532593 + 2.0 * 6.556954860687256
Epoch 160, val loss: 1.710371494293213
Epoch 170, training loss: 14.740111351013184 = 1.6723755598068237 + 2.0 * 6.533867835998535
Epoch 170, val loss: 1.690313458442688
Epoch 180, training loss: 14.678533554077148 = 1.6472296714782715 + 2.0 * 6.515651702880859
Epoch 180, val loss: 1.6680548191070557
Epoch 190, training loss: 14.621969223022461 = 1.6193037033081055 + 2.0 * 6.501332759857178
Epoch 190, val loss: 1.643505573272705
Epoch 200, training loss: 14.56938362121582 = 1.5888049602508545 + 2.0 * 6.490289211273193
Epoch 200, val loss: 1.6168951988220215
Epoch 210, training loss: 14.510332107543945 = 1.5560672283172607 + 2.0 * 6.477132320404053
Epoch 210, val loss: 1.5886645317077637
Epoch 220, training loss: 14.455819129943848 = 1.5212037563323975 + 2.0 * 6.4673075675964355
Epoch 220, val loss: 1.5589174032211304
Epoch 230, training loss: 14.40211296081543 = 1.4847445487976074 + 2.0 * 6.45868444442749
Epoch 230, val loss: 1.5282214879989624
Epoch 240, training loss: 14.347492218017578 = 1.447404384613037 + 2.0 * 6.450043678283691
Epoch 240, val loss: 1.4974020719528198
Epoch 250, training loss: 14.290974617004395 = 1.409541368484497 + 2.0 * 6.440716743469238
Epoch 250, val loss: 1.466909408569336
Epoch 260, training loss: 14.241016387939453 = 1.3716473579406738 + 2.0 * 6.434684753417969
Epoch 260, val loss: 1.4370163679122925
Epoch 270, training loss: 14.191650390625 = 1.3344860076904297 + 2.0 * 6.428582191467285
Epoch 270, val loss: 1.408456802368164
Epoch 280, training loss: 14.143159866333008 = 1.2984527349472046 + 2.0 * 6.422353744506836
Epoch 280, val loss: 1.3815045356750488
Epoch 290, training loss: 14.094603538513184 = 1.2633424997329712 + 2.0 * 6.415630340576172
Epoch 290, val loss: 1.3557733297348022
Epoch 300, training loss: 14.048096656799316 = 1.229047417640686 + 2.0 * 6.409524440765381
Epoch 300, val loss: 1.3311305046081543
Epoch 310, training loss: 14.025029182434082 = 1.1956546306610107 + 2.0 * 6.414687156677246
Epoch 310, val loss: 1.3074405193328857
Epoch 320, training loss: 13.968470573425293 = 1.1633461713790894 + 2.0 * 6.402562141418457
Epoch 320, val loss: 1.2850219011306763
Epoch 330, training loss: 13.923903465270996 = 1.1320024728775024 + 2.0 * 6.3959503173828125
Epoch 330, val loss: 1.2634830474853516
Epoch 340, training loss: 13.884831428527832 = 1.1014225482940674 + 2.0 * 6.391704559326172
Epoch 340, val loss: 1.2424087524414062
Epoch 350, training loss: 13.861577033996582 = 1.0714603662490845 + 2.0 * 6.3950581550598145
Epoch 350, val loss: 1.2220208644866943
Epoch 360, training loss: 13.813789367675781 = 1.0422688722610474 + 2.0 * 6.385760307312012
Epoch 360, val loss: 1.2022342681884766
Epoch 370, training loss: 13.775874137878418 = 1.0138314962387085 + 2.0 * 6.381021499633789
Epoch 370, val loss: 1.183014988899231
Epoch 380, training loss: 13.741997718811035 = 0.9858589172363281 + 2.0 * 6.3780694007873535
Epoch 380, val loss: 1.1642396450042725
Epoch 390, training loss: 13.707234382629395 = 0.9582546353340149 + 2.0 * 6.374489784240723
Epoch 390, val loss: 1.1458805799484253
Epoch 400, training loss: 13.675497055053711 = 0.9309502243995667 + 2.0 * 6.3722734451293945
Epoch 400, val loss: 1.1278328895568848
Epoch 410, training loss: 13.646137237548828 = 0.9039821624755859 + 2.0 * 6.371077537536621
Epoch 410, val loss: 1.11006760597229
Epoch 420, training loss: 13.615321159362793 = 0.8774086833000183 + 2.0 * 6.368956089019775
Epoch 420, val loss: 1.0927485227584839
Epoch 430, training loss: 13.577535629272461 = 0.85113525390625 + 2.0 * 6.3632001876831055
Epoch 430, val loss: 1.0758912563323975
Epoch 440, training loss: 13.547736167907715 = 0.8252876996994019 + 2.0 * 6.361224174499512
Epoch 440, val loss: 1.0594007968902588
Epoch 450, training loss: 13.524481773376465 = 0.7999045252799988 + 2.0 * 6.362288475036621
Epoch 450, val loss: 1.0434892177581787
Epoch 460, training loss: 13.488622665405273 = 0.7752184867858887 + 2.0 * 6.356701850891113
Epoch 460, val loss: 1.0282407999038696
Epoch 470, training loss: 13.45770263671875 = 0.7510581612586975 + 2.0 * 6.3533220291137695
Epoch 470, val loss: 1.0136382579803467
Epoch 480, training loss: 13.443113327026367 = 0.7275375127792358 + 2.0 * 6.3577880859375
Epoch 480, val loss: 0.9997351765632629
Epoch 490, training loss: 13.406262397766113 = 0.7047780752182007 + 2.0 * 6.350742340087891
Epoch 490, val loss: 0.9867509603500366
Epoch 500, training loss: 13.37668514251709 = 0.6828663945198059 + 2.0 * 6.346909523010254
Epoch 500, val loss: 0.9745752215385437
Epoch 510, training loss: 13.353238105773926 = 0.6616359353065491 + 2.0 * 6.345800876617432
Epoch 510, val loss: 0.9633010029792786
Epoch 520, training loss: 13.334846496582031 = 0.641158401966095 + 2.0 * 6.34684419631958
Epoch 520, val loss: 0.9529730677604675
Epoch 530, training loss: 13.309167861938477 = 0.6214776635169983 + 2.0 * 6.343844890594482
Epoch 530, val loss: 0.9435223937034607
Epoch 540, training loss: 13.281316757202148 = 0.6024980545043945 + 2.0 * 6.339409351348877
Epoch 540, val loss: 0.9349291920661926
Epoch 550, training loss: 13.259207725524902 = 0.5841659307479858 + 2.0 * 6.337521076202393
Epoch 550, val loss: 0.9271128177642822
Epoch 560, training loss: 13.241711616516113 = 0.5663946866989136 + 2.0 * 6.337658405303955
Epoch 560, val loss: 0.9200584888458252
Epoch 570, training loss: 13.231342315673828 = 0.549275815486908 + 2.0 * 6.341033458709717
Epoch 570, val loss: 0.9136662483215332
Epoch 580, training loss: 13.196859359741211 = 0.5327140092849731 + 2.0 * 6.332072734832764
Epoch 580, val loss: 0.9080899357795715
Epoch 590, training loss: 13.177388191223145 = 0.5166747570037842 + 2.0 * 6.330356597900391
Epoch 590, val loss: 0.9030907154083252
Epoch 600, training loss: 13.1632080078125 = 0.501031219959259 + 2.0 * 6.331088542938232
Epoch 600, val loss: 0.8987017869949341
Epoch 610, training loss: 13.157259941101074 = 0.48587566614151 + 2.0 * 6.335691928863525
Epoch 610, val loss: 0.8948437571525574
Epoch 620, training loss: 13.125041961669922 = 0.47110268473625183 + 2.0 * 6.326969623565674
Epoch 620, val loss: 0.8915252089500427
Epoch 630, training loss: 13.10338020324707 = 0.4566192328929901 + 2.0 * 6.323380470275879
Epoch 630, val loss: 0.8885148167610168
Epoch 640, training loss: 13.085827827453613 = 0.4423588812351227 + 2.0 * 6.321734428405762
Epoch 640, val loss: 0.8859410881996155
Epoch 650, training loss: 13.069926261901855 = 0.4282858967781067 + 2.0 * 6.320820331573486
Epoch 650, val loss: 0.8836909532546997
Epoch 660, training loss: 13.058427810668945 = 0.4144729971885681 + 2.0 * 6.321977615356445
Epoch 660, val loss: 0.8818995952606201
Epoch 670, training loss: 13.038948059082031 = 0.4010007083415985 + 2.0 * 6.318973541259766
Epoch 670, val loss: 0.8803775310516357
Epoch 680, training loss: 13.020916938781738 = 0.3877686858177185 + 2.0 * 6.3165740966796875
Epoch 680, val loss: 0.8791601061820984
Epoch 690, training loss: 13.005769729614258 = 0.3747178316116333 + 2.0 * 6.315526008605957
Epoch 690, val loss: 0.8782570362091064
Epoch 700, training loss: 12.995688438415527 = 0.36188197135925293 + 2.0 * 6.316903114318848
Epoch 700, val loss: 0.8776732087135315
Epoch 710, training loss: 12.976561546325684 = 0.34933459758758545 + 2.0 * 6.313613414764404
Epoch 710, val loss: 0.8773103356361389
Epoch 720, training loss: 12.960674285888672 = 0.33698704838752747 + 2.0 * 6.311843395233154
Epoch 720, val loss: 0.8771705031394958
Epoch 730, training loss: 12.957616806030273 = 0.3248535692691803 + 2.0 * 6.316381454467773
Epoch 730, val loss: 0.8773374557495117
Epoch 740, training loss: 12.936250686645508 = 0.31295543909072876 + 2.0 * 6.311647415161133
Epoch 740, val loss: 0.8777375817298889
Epoch 750, training loss: 12.919710159301758 = 0.3013097941875458 + 2.0 * 6.309200286865234
Epoch 750, val loss: 0.8783620595932007
Epoch 760, training loss: 12.91007137298584 = 0.28991368412971497 + 2.0 * 6.3100786209106445
Epoch 760, val loss: 0.8792292475700378
Epoch 770, training loss: 12.895915985107422 = 0.2787664532661438 + 2.0 * 6.308574676513672
Epoch 770, val loss: 0.8803342580795288
Epoch 780, training loss: 12.882999420166016 = 0.2678607404232025 + 2.0 * 6.30756950378418
Epoch 780, val loss: 0.8817329406738281
Epoch 790, training loss: 12.865203857421875 = 0.25722217559814453 + 2.0 * 6.303990840911865
Epoch 790, val loss: 0.8834156394004822
Epoch 800, training loss: 12.853437423706055 = 0.24684806168079376 + 2.0 * 6.303294658660889
Epoch 800, val loss: 0.8853754997253418
Epoch 810, training loss: 12.851529121398926 = 0.23674465715885162 + 2.0 * 6.307392120361328
Epoch 810, val loss: 0.8875773549079895
Epoch 820, training loss: 12.835814476013184 = 0.2269287258386612 + 2.0 * 6.304442882537842
Epoch 820, val loss: 0.890113890171051
Epoch 830, training loss: 12.817931175231934 = 0.21746554970741272 + 2.0 * 6.300232887268066
Epoch 830, val loss: 0.8927976489067078
Epoch 840, training loss: 12.806061744689941 = 0.20827874541282654 + 2.0 * 6.298891544342041
Epoch 840, val loss: 0.8958196640014648
Epoch 850, training loss: 12.809115409851074 = 0.1993916779756546 + 2.0 * 6.304862022399902
Epoch 850, val loss: 0.8990445733070374
Epoch 860, training loss: 12.790464401245117 = 0.1908203512430191 + 2.0 * 6.299821853637695
Epoch 860, val loss: 0.9024577736854553
Epoch 870, training loss: 12.782180786132812 = 0.18259871006011963 + 2.0 * 6.299790859222412
Epoch 870, val loss: 0.9059833288192749
Epoch 880, training loss: 12.765337944030762 = 0.17470364272594452 + 2.0 * 6.29531717300415
Epoch 880, val loss: 0.9096242189407349
Epoch 890, training loss: 12.75595760345459 = 0.16711954772472382 + 2.0 * 6.294418811798096
Epoch 890, val loss: 0.91343754529953
Epoch 900, training loss: 12.747943878173828 = 0.1598391979932785 + 2.0 * 6.2940521240234375
Epoch 900, val loss: 0.9174065589904785
Epoch 910, training loss: 12.74710464477539 = 0.15287838876247406 + 2.0 * 6.297112941741943
Epoch 910, val loss: 0.9214690327644348
Epoch 920, training loss: 12.731375694274902 = 0.14622552692890167 + 2.0 * 6.292574882507324
Epoch 920, val loss: 0.9256982803344727
Epoch 930, training loss: 12.72752571105957 = 0.1398804634809494 + 2.0 * 6.293822765350342
Epoch 930, val loss: 0.929986298084259
Epoch 940, training loss: 12.716800689697266 = 0.13381770253181458 + 2.0 * 6.291491508483887
Epoch 940, val loss: 0.9344131350517273
Epoch 950, training loss: 12.711990356445312 = 0.12802644073963165 + 2.0 * 6.291982173919678
Epoch 950, val loss: 0.9389249086380005
Epoch 960, training loss: 12.702000617980957 = 0.12249989807605743 + 2.0 * 6.289750576019287
Epoch 960, val loss: 0.9434186220169067
Epoch 970, training loss: 12.694536209106445 = 0.11723373830318451 + 2.0 * 6.288651466369629
Epoch 970, val loss: 0.9479800462722778
Epoch 980, training loss: 12.697877883911133 = 0.11221329867839813 + 2.0 * 6.292832374572754
Epoch 980, val loss: 0.9525818824768066
Epoch 990, training loss: 12.683589935302734 = 0.1074347123503685 + 2.0 * 6.2880778312683105
Epoch 990, val loss: 0.9572858810424805
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7666666666666667
0.797575118608329
=== training gcn model ===
Epoch 0, training loss: 19.126707077026367 = 1.9330077171325684 + 2.0 * 8.59684944152832
Epoch 0, val loss: 1.9335254430770874
Epoch 10, training loss: 19.116519927978516 = 1.9234899282455444 + 2.0 * 8.596514701843262
Epoch 10, val loss: 1.9231183528900146
Epoch 20, training loss: 19.09884262084961 = 1.911590814590454 + 2.0 * 8.593626022338867
Epoch 20, val loss: 1.909982681274414
Epoch 30, training loss: 19.033000946044922 = 1.8951977491378784 + 2.0 * 8.568902015686035
Epoch 30, val loss: 1.8920279741287231
Epoch 40, training loss: 18.648035049438477 = 1.875372052192688 + 2.0 * 8.386331558227539
Epoch 40, val loss: 1.871669888496399
Epoch 50, training loss: 17.74920082092285 = 1.855068564414978 + 2.0 * 7.947065830230713
Epoch 50, val loss: 1.8522217273712158
Epoch 60, training loss: 16.88780975341797 = 1.8418655395507812 + 2.0 * 7.5229716300964355
Epoch 60, val loss: 1.8403728008270264
Epoch 70, training loss: 16.238529205322266 = 1.8323711156845093 + 2.0 * 7.203078746795654
Epoch 70, val loss: 1.8312252759933472
Epoch 80, training loss: 15.883841514587402 = 1.822235107421875 + 2.0 * 7.030803203582764
Epoch 80, val loss: 1.8211424350738525
Epoch 90, training loss: 15.558794975280762 = 1.8103708028793335 + 2.0 * 6.874212265014648
Epoch 90, val loss: 1.8099169731140137
Epoch 100, training loss: 15.370000839233398 = 1.7988630533218384 + 2.0 * 6.785568714141846
Epoch 100, val loss: 1.7992801666259766
Epoch 110, training loss: 15.216652870178223 = 1.7875556945800781 + 2.0 * 6.714548587799072
Epoch 110, val loss: 1.7886989116668701
Epoch 120, training loss: 15.104681968688965 = 1.7758499383926392 + 2.0 * 6.6644158363342285
Epoch 120, val loss: 1.7778784036636353
Epoch 130, training loss: 15.011529922485352 = 1.7631703615188599 + 2.0 * 6.624179840087891
Epoch 130, val loss: 1.7662042379379272
Epoch 140, training loss: 14.933577537536621 = 1.7492034435272217 + 2.0 * 6.59218692779541
Epoch 140, val loss: 1.7535241842269897
Epoch 150, training loss: 14.866687774658203 = 1.7336864471435547 + 2.0 * 6.566500663757324
Epoch 150, val loss: 1.739650845527649
Epoch 160, training loss: 14.806516647338867 = 1.7163292169570923 + 2.0 * 6.545093536376953
Epoch 160, val loss: 1.724272608757019
Epoch 170, training loss: 14.752849578857422 = 1.6967805624008179 + 2.0 * 6.528034687042236
Epoch 170, val loss: 1.7069878578186035
Epoch 180, training loss: 14.696550369262695 = 1.6747708320617676 + 2.0 * 6.510890007019043
Epoch 180, val loss: 1.6875487565994263
Epoch 190, training loss: 14.64184284210205 = 1.6500290632247925 + 2.0 * 6.495906829833984
Epoch 190, val loss: 1.6657217741012573
Epoch 200, training loss: 14.598259925842285 = 1.6223007440567017 + 2.0 * 6.487979412078857
Epoch 200, val loss: 1.641304850578308
Epoch 210, training loss: 14.538154602050781 = 1.591687798500061 + 2.0 * 6.473233222961426
Epoch 210, val loss: 1.6143728494644165
Epoch 220, training loss: 14.484308242797852 = 1.558287501335144 + 2.0 * 6.463010311126709
Epoch 220, val loss: 1.5850164890289307
Epoch 230, training loss: 14.435028076171875 = 1.5222587585449219 + 2.0 * 6.456384658813477
Epoch 230, val loss: 1.5534828901290894
Epoch 240, training loss: 14.379779815673828 = 1.484192967414856 + 2.0 * 6.447793483734131
Epoch 240, val loss: 1.5205342769622803
Epoch 250, training loss: 14.323868751525879 = 1.4445562362670898 + 2.0 * 6.4396562576293945
Epoch 250, val loss: 1.48637056350708
Epoch 260, training loss: 14.265060424804688 = 1.4038710594177246 + 2.0 * 6.4305949211120605
Epoch 260, val loss: 1.4517061710357666
Epoch 270, training loss: 14.213885307312012 = 1.3623920679092407 + 2.0 * 6.425746440887451
Epoch 270, val loss: 1.416911244392395
Epoch 280, training loss: 14.16501522064209 = 1.3208295106887817 + 2.0 * 6.422092914581299
Epoch 280, val loss: 1.3825258016586304
Epoch 290, training loss: 14.105525016784668 = 1.2798811197280884 + 2.0 * 6.4128217697143555
Epoch 290, val loss: 1.3493133783340454
Epoch 300, training loss: 14.051567077636719 = 1.2397727966308594 + 2.0 * 6.40589714050293
Epoch 300, val loss: 1.3172873258590698
Epoch 310, training loss: 14.001799583435059 = 1.2001537084579468 + 2.0 * 6.40082311630249
Epoch 310, val loss: 1.2861970663070679
Epoch 320, training loss: 13.952855110168457 = 1.1608705520629883 + 2.0 * 6.395992279052734
Epoch 320, val loss: 1.255798101425171
Epoch 330, training loss: 13.906364440917969 = 1.121983289718628 + 2.0 * 6.392190456390381
Epoch 330, val loss: 1.2260710000991821
Epoch 340, training loss: 13.85817813873291 = 1.0837888717651367 + 2.0 * 6.387194633483887
Epoch 340, val loss: 1.1972192525863647
Epoch 350, training loss: 13.812047958374023 = 1.0460009574890137 + 2.0 * 6.383023738861084
Epoch 350, val loss: 1.1690014600753784
Epoch 360, training loss: 13.783084869384766 = 1.0085999965667725 + 2.0 * 6.387242317199707
Epoch 360, val loss: 1.1412302255630493
Epoch 370, training loss: 13.727360725402832 = 0.971836268901825 + 2.0 * 6.377762317657471
Epoch 370, val loss: 1.1141740083694458
Epoch 380, training loss: 13.68129825592041 = 0.9357467293739319 + 2.0 * 6.372775554656982
Epoch 380, val loss: 1.0878311395645142
Epoch 390, training loss: 13.641589164733887 = 0.900387167930603 + 2.0 * 6.370601177215576
Epoch 390, val loss: 1.0621850490570068
Epoch 400, training loss: 13.611163139343262 = 0.8660306930541992 + 2.0 * 6.372566223144531
Epoch 400, val loss: 1.0374056100845337
Epoch 410, training loss: 13.564847946166992 = 0.8329424858093262 + 2.0 * 6.365952491760254
Epoch 410, val loss: 1.0137542486190796
Epoch 420, training loss: 13.52517318725586 = 0.801068902015686 + 2.0 * 6.362051963806152
Epoch 420, val loss: 0.9912102818489075
Epoch 430, training loss: 13.48962688446045 = 0.7703580260276794 + 2.0 * 6.3596343994140625
Epoch 430, val loss: 0.9697772264480591
Epoch 440, training loss: 13.458039283752441 = 0.7408692240715027 + 2.0 * 6.358584880828857
Epoch 440, val loss: 0.9496138095855713
Epoch 450, training loss: 13.425277709960938 = 0.7127038240432739 + 2.0 * 6.356287002563477
Epoch 450, val loss: 0.9306010007858276
Epoch 460, training loss: 13.389358520507812 = 0.6858039498329163 + 2.0 * 6.351777076721191
Epoch 460, val loss: 0.9128783345222473
Epoch 470, training loss: 13.363922119140625 = 0.660002589225769 + 2.0 * 6.351959705352783
Epoch 470, val loss: 0.8962732553482056
Epoch 480, training loss: 13.335278511047363 = 0.635293185710907 + 2.0 * 6.349992752075195
Epoch 480, val loss: 0.8809322118759155
Epoch 490, training loss: 13.303975105285645 = 0.6117182970046997 + 2.0 * 6.346128463745117
Epoch 490, val loss: 0.8665430545806885
Epoch 500, training loss: 13.27808666229248 = 0.5891092419624329 + 2.0 * 6.344488620758057
Epoch 500, val loss: 0.8532712459564209
Epoch 510, training loss: 13.26614761352539 = 0.5673220157623291 + 2.0 * 6.34941291809082
Epoch 510, val loss: 0.8408982157707214
Epoch 520, training loss: 13.23006820678711 = 0.5462756752967834 + 2.0 * 6.341896057128906
Epoch 520, val loss: 0.8295931220054626
Epoch 530, training loss: 13.202702522277832 = 0.5259752869606018 + 2.0 * 6.3383636474609375
Epoch 530, val loss: 0.818889856338501
Epoch 540, training loss: 13.181248664855957 = 0.506232500076294 + 2.0 * 6.337508201599121
Epoch 540, val loss: 0.8089298605918884
Epoch 550, training loss: 13.158080101013184 = 0.4869818091392517 + 2.0 * 6.335549354553223
Epoch 550, val loss: 0.799443244934082
Epoch 560, training loss: 13.138736724853516 = 0.4682154357433319 + 2.0 * 6.33526086807251
Epoch 560, val loss: 0.7904837131500244
Epoch 570, training loss: 13.115260124206543 = 0.44987374544143677 + 2.0 * 6.332693099975586
Epoch 570, val loss: 0.7820674777030945
Epoch 580, training loss: 13.090808868408203 = 0.43187856674194336 + 2.0 * 6.329464912414551
Epoch 580, val loss: 0.7740839719772339
Epoch 590, training loss: 13.077005386352539 = 0.4141853451728821 + 2.0 * 6.331409931182861
Epoch 590, val loss: 0.7664231657981873
Epoch 600, training loss: 13.064430236816406 = 0.3967246115207672 + 2.0 * 6.333852767944336
Epoch 600, val loss: 0.7595147490501404
Epoch 610, training loss: 13.033153533935547 = 0.379675954580307 + 2.0 * 6.3267388343811035
Epoch 610, val loss: 0.7527035474777222
Epoch 620, training loss: 13.012102127075195 = 0.36295753717422485 + 2.0 * 6.3245720863342285
Epoch 620, val loss: 0.746502697467804
Epoch 630, training loss: 12.995999336242676 = 0.34650498628616333 + 2.0 * 6.324747085571289
Epoch 630, val loss: 0.7407828569412231
Epoch 640, training loss: 12.974287986755371 = 0.330389142036438 + 2.0 * 6.321949481964111
Epoch 640, val loss: 0.7354772090911865
Epoch 650, training loss: 12.955382347106934 = 0.3146391808986664 + 2.0 * 6.320371627807617
Epoch 650, val loss: 0.7305840849876404
Epoch 660, training loss: 12.946081161499023 = 0.299359530210495 + 2.0 * 6.323360919952393
Epoch 660, val loss: 0.7262662649154663
Epoch 670, training loss: 12.921613693237305 = 0.2845713794231415 + 2.0 * 6.318521022796631
Epoch 670, val loss: 0.7224194407463074
Epoch 680, training loss: 12.903528213500977 = 0.2703290283679962 + 2.0 * 6.316599369049072
Epoch 680, val loss: 0.7192575931549072
Epoch 690, training loss: 12.888579368591309 = 0.2566131055355072 + 2.0 * 6.315983295440674
Epoch 690, val loss: 0.7165641784667969
Epoch 700, training loss: 12.871472358703613 = 0.24345654249191284 + 2.0 * 6.314007759094238
Epoch 700, val loss: 0.7144584059715271
Epoch 710, training loss: 12.85840892791748 = 0.2309264987707138 + 2.0 * 6.313741207122803
Epoch 710, val loss: 0.7128416299819946
Epoch 720, training loss: 12.848333358764648 = 0.2190004587173462 + 2.0 * 6.314666271209717
Epoch 720, val loss: 0.7118330001831055
Epoch 730, training loss: 12.831741333007812 = 0.20769646763801575 + 2.0 * 6.3120222091674805
Epoch 730, val loss: 0.7115039825439453
Epoch 740, training loss: 12.817292213439941 = 0.19700489938259125 + 2.0 * 6.31014347076416
Epoch 740, val loss: 0.7114713788032532
Epoch 750, training loss: 12.803204536437988 = 0.18691600859165192 + 2.0 * 6.3081440925598145
Epoch 750, val loss: 0.7120174169540405
Epoch 760, training loss: 12.813282012939453 = 0.17736802995204926 + 2.0 * 6.317956924438477
Epoch 760, val loss: 0.7130902409553528
Epoch 770, training loss: 12.782492637634277 = 0.1684054583311081 + 2.0 * 6.307043552398682
Epoch 770, val loss: 0.7142921090126038
Epoch 780, training loss: 12.769121170043945 = 0.15995903313159943 + 2.0 * 6.304581165313721
Epoch 780, val loss: 0.7161111831665039
Epoch 790, training loss: 12.759838104248047 = 0.1520329713821411 + 2.0 * 6.303902626037598
Epoch 790, val loss: 0.7183742523193359
Epoch 800, training loss: 12.769598007202148 = 0.1445770412683487 + 2.0 * 6.3125104904174805
Epoch 800, val loss: 0.7208144664764404
Epoch 810, training loss: 12.742996215820312 = 0.13753464818000793 + 2.0 * 6.302730560302734
Epoch 810, val loss: 0.7236732244491577
Epoch 820, training loss: 12.736149787902832 = 0.1309424489736557 + 2.0 * 6.302603721618652
Epoch 820, val loss: 0.726847767829895
Epoch 830, training loss: 12.727044105529785 = 0.12472742795944214 + 2.0 * 6.301158428192139
Epoch 830, val loss: 0.7301580905914307
Epoch 840, training loss: 12.717345237731934 = 0.11886272579431534 + 2.0 * 6.299241065979004
Epoch 840, val loss: 0.733819305896759
Epoch 850, training loss: 12.717275619506836 = 0.11334898322820663 + 2.0 * 6.3019633293151855
Epoch 850, val loss: 0.7375937104225159
Epoch 860, training loss: 12.70310115814209 = 0.10813743621110916 + 2.0 * 6.297482013702393
Epoch 860, val loss: 0.7416111826896667
Epoch 870, training loss: 12.705979347229004 = 0.10322613269090652 + 2.0 * 6.301376819610596
Epoch 870, val loss: 0.745811939239502
Epoch 880, training loss: 12.690025329589844 = 0.09860396385192871 + 2.0 * 6.295710563659668
Epoch 880, val loss: 0.7499752044677734
Epoch 890, training loss: 12.682071685791016 = 0.09422498196363449 + 2.0 * 6.293923377990723
Epoch 890, val loss: 0.7544324398040771
Epoch 900, training loss: 12.67807674407959 = 0.09009795635938644 + 2.0 * 6.293989181518555
Epoch 900, val loss: 0.7589148879051208
Epoch 910, training loss: 12.684272766113281 = 0.08619002997875214 + 2.0 * 6.299041271209717
Epoch 910, val loss: 0.7635700106620789
Epoch 920, training loss: 12.66866397857666 = 0.08250399678945541 + 2.0 * 6.293079853057861
Epoch 920, val loss: 0.7681778073310852
Epoch 930, training loss: 12.660054206848145 = 0.07902871072292328 + 2.0 * 6.290512561798096
Epoch 930, val loss: 0.7729578018188477
Epoch 940, training loss: 12.657944679260254 = 0.07573646306991577 + 2.0 * 6.291104316711426
Epoch 940, val loss: 0.7777746915817261
Epoch 950, training loss: 12.661091804504395 = 0.07261553406715393 + 2.0 * 6.294238090515137
Epoch 950, val loss: 0.7827172875404358
Epoch 960, training loss: 12.647354125976562 = 0.06966442614793777 + 2.0 * 6.288845062255859
Epoch 960, val loss: 0.7875925302505493
Epoch 970, training loss: 12.644505500793457 = 0.06686868518590927 + 2.0 * 6.288818359375
Epoch 970, val loss: 0.7925653457641602
Epoch 980, training loss: 12.645322799682617 = 0.06422296166419983 + 2.0 * 6.2905497550964355
Epoch 980, val loss: 0.7975910305976868
Epoch 990, training loss: 12.638425827026367 = 0.06172467768192291 + 2.0 * 6.288350582122803
Epoch 990, val loss: 0.8024743795394897
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7666666666666667
0.8060094886663153
The final CL Acc:0.76790, 0.00175, The final GNN Acc:0.80197, 0.00345
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13150])
remove edge: torch.Size([2, 7994])
updated graph: torch.Size([2, 10588])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 19.128694534301758 = 1.9350249767303467 + 2.0 * 8.596835136413574
Epoch 0, val loss: 1.9308983087539673
Epoch 10, training loss: 19.117998123168945 = 1.9250280857086182 + 2.0 * 8.596485137939453
Epoch 10, val loss: 1.921228289604187
Epoch 20, training loss: 19.10031509399414 = 1.9128258228302002 + 2.0 * 8.593744277954102
Epoch 20, val loss: 1.9090311527252197
Epoch 30, training loss: 19.0389404296875 = 1.8966031074523926 + 2.0 * 8.571168899536133
Epoch 30, val loss: 1.892431378364563
Epoch 40, training loss: 18.680585861206055 = 1.8770619630813599 + 2.0 * 8.401762008666992
Epoch 40, val loss: 1.8727959394454956
Epoch 50, training loss: 17.429208755493164 = 1.8566778898239136 + 2.0 * 7.7862653732299805
Epoch 50, val loss: 1.853143572807312
Epoch 60, training loss: 16.523595809936523 = 1.8414452075958252 + 2.0 * 7.341075420379639
Epoch 60, val loss: 1.8390710353851318
Epoch 70, training loss: 15.959781646728516 = 1.8286110162734985 + 2.0 * 7.065585136413574
Epoch 70, val loss: 1.8261860609054565
Epoch 80, training loss: 15.617626190185547 = 1.8153623342514038 + 2.0 * 6.901132106781006
Epoch 80, val loss: 1.8131409883499146
Epoch 90, training loss: 15.436720848083496 = 1.8006832599639893 + 2.0 * 6.818018913269043
Epoch 90, val loss: 1.7994812726974487
Epoch 100, training loss: 15.291054725646973 = 1.785801649093628 + 2.0 * 6.752626419067383
Epoch 100, val loss: 1.78614342212677
Epoch 110, training loss: 15.171307563781738 = 1.7718600034713745 + 2.0 * 6.699723720550537
Epoch 110, val loss: 1.7737725973129272
Epoch 120, training loss: 15.06386661529541 = 1.7580540180206299 + 2.0 * 6.65290641784668
Epoch 120, val loss: 1.761484146118164
Epoch 130, training loss: 14.972884178161621 = 1.7434022426605225 + 2.0 * 6.61474084854126
Epoch 130, val loss: 1.7484800815582275
Epoch 140, training loss: 14.897960662841797 = 1.7272875308990479 + 2.0 * 6.585336685180664
Epoch 140, val loss: 1.73428213596344
Epoch 150, training loss: 14.832409858703613 = 1.7093799114227295 + 2.0 * 6.561514854431152
Epoch 150, val loss: 1.718554139137268
Epoch 160, training loss: 14.772631645202637 = 1.6892973184585571 + 2.0 * 6.5416669845581055
Epoch 160, val loss: 1.7010290622711182
Epoch 170, training loss: 14.718881607055664 = 1.6665884256362915 + 2.0 * 6.526146411895752
Epoch 170, val loss: 1.6813085079193115
Epoch 180, training loss: 14.660170555114746 = 1.6412218809127808 + 2.0 * 6.509474277496338
Epoch 180, val loss: 1.6593925952911377
Epoch 190, training loss: 14.611104965209961 = 1.6129777431488037 + 2.0 * 6.499063491821289
Epoch 190, val loss: 1.635104775428772
Epoch 200, training loss: 14.550140380859375 = 1.5819790363311768 + 2.0 * 6.484080791473389
Epoch 200, val loss: 1.6084879636764526
Epoch 210, training loss: 14.49312686920166 = 1.547898530960083 + 2.0 * 6.472614288330078
Epoch 210, val loss: 1.5795108079910278
Epoch 220, training loss: 14.43763256072998 = 1.5107754468917847 + 2.0 * 6.463428497314453
Epoch 220, val loss: 1.548364281654358
Epoch 230, training loss: 14.37415599822998 = 1.4710967540740967 + 2.0 * 6.451529502868652
Epoch 230, val loss: 1.515269160270691
Epoch 240, training loss: 14.314334869384766 = 1.4288628101348877 + 2.0 * 6.4427361488342285
Epoch 240, val loss: 1.4804472923278809
Epoch 250, training loss: 14.253535270690918 = 1.3844506740570068 + 2.0 * 6.434542179107666
Epoch 250, val loss: 1.4443359375
Epoch 260, training loss: 14.191638946533203 = 1.3385729789733887 + 2.0 * 6.426532745361328
Epoch 260, val loss: 1.407571792602539
Epoch 270, training loss: 14.130857467651367 = 1.2914093732833862 + 2.0 * 6.419723987579346
Epoch 270, val loss: 1.3702738285064697
Epoch 280, training loss: 14.072076797485352 = 1.243383526802063 + 2.0 * 6.414346694946289
Epoch 280, val loss: 1.3328441381454468
Epoch 290, training loss: 14.011751174926758 = 1.1950684785842896 + 2.0 * 6.408341407775879
Epoch 290, val loss: 1.295517086982727
Epoch 300, training loss: 13.951922416687012 = 1.1469029188156128 + 2.0 * 6.402509689331055
Epoch 300, val loss: 1.2586579322814941
Epoch 310, training loss: 13.907225608825684 = 1.099209189414978 + 2.0 * 6.404008388519287
Epoch 310, val loss: 1.2225046157836914
Epoch 320, training loss: 13.84131145477295 = 1.0525318384170532 + 2.0 * 6.394389629364014
Epoch 320, val loss: 1.187298059463501
Epoch 330, training loss: 13.786380767822266 = 1.0071040391921997 + 2.0 * 6.389638423919678
Epoch 330, val loss: 1.1531864404678345
Epoch 340, training loss: 13.733222007751465 = 0.9631490707397461 + 2.0 * 6.385036468505859
Epoch 340, val loss: 1.1205307245254517
Epoch 350, training loss: 13.686131477355957 = 0.9210445880889893 + 2.0 * 6.382543563842773
Epoch 350, val loss: 1.0892996788024902
Epoch 360, training loss: 13.63786506652832 = 0.8808363676071167 + 2.0 * 6.378514289855957
Epoch 360, val loss: 1.059924602508545
Epoch 370, training loss: 13.593043327331543 = 0.8426915407180786 + 2.0 * 6.375175952911377
Epoch 370, val loss: 1.032353162765503
Epoch 380, training loss: 13.548687934875488 = 0.8065386414527893 + 2.0 * 6.371074676513672
Epoch 380, val loss: 1.0065828561782837
Epoch 390, training loss: 13.510066986083984 = 0.772158682346344 + 2.0 * 6.368954181671143
Epoch 390, val loss: 0.9826280474662781
Epoch 400, training loss: 13.476515769958496 = 0.7396770715713501 + 2.0 * 6.368419170379639
Epoch 400, val loss: 0.9603671431541443
Epoch 410, training loss: 13.434418678283691 = 0.7090451121330261 + 2.0 * 6.362686634063721
Epoch 410, val loss: 0.9399336576461792
Epoch 420, training loss: 13.402399063110352 = 0.6798987984657288 + 2.0 * 6.361249923706055
Epoch 420, val loss: 0.9211437106132507
Epoch 430, training loss: 13.364850044250488 = 0.6520949602127075 + 2.0 * 6.356377601623535
Epoch 430, val loss: 0.9036474227905273
Epoch 440, training loss: 13.337186813354492 = 0.6253615021705627 + 2.0 * 6.355912685394287
Epoch 440, val loss: 0.8875176310539246
Epoch 450, training loss: 13.307833671569824 = 0.599634051322937 + 2.0 * 6.354099750518799
Epoch 450, val loss: 0.8726059794425964
Epoch 460, training loss: 13.274096488952637 = 0.5748887658119202 + 2.0 * 6.349603652954102
Epoch 460, val loss: 0.8586952090263367
Epoch 470, training loss: 13.247285842895508 = 0.5508633255958557 + 2.0 * 6.348211288452148
Epoch 470, val loss: 0.8458267450332642
Epoch 480, training loss: 13.216256141662598 = 0.5276124477386475 + 2.0 * 6.3443217277526855
Epoch 480, val loss: 0.8338576555252075
Epoch 490, training loss: 13.199420928955078 = 0.5049576163291931 + 2.0 * 6.347231864929199
Epoch 490, val loss: 0.8228067755699158
Epoch 500, training loss: 13.168377876281738 = 0.48304182291030884 + 2.0 * 6.342668056488037
Epoch 500, val loss: 0.8124379515647888
Epoch 510, training loss: 13.138129234313965 = 0.4617335796356201 + 2.0 * 6.338197708129883
Epoch 510, val loss: 0.8028357028961182
Epoch 520, training loss: 13.112037658691406 = 0.44095364212989807 + 2.0 * 6.33554220199585
Epoch 520, val loss: 0.7939794659614563
Epoch 530, training loss: 13.101884841918945 = 0.42071181535720825 + 2.0 * 6.3405866622924805
Epoch 530, val loss: 0.7856963276863098
Epoch 540, training loss: 13.072269439697266 = 0.4009658098220825 + 2.0 * 6.335651874542236
Epoch 540, val loss: 0.7782814502716064
Epoch 550, training loss: 13.04551887512207 = 0.3819260597229004 + 2.0 * 6.331796646118164
Epoch 550, val loss: 0.771292507648468
Epoch 560, training loss: 13.023370742797852 = 0.36347225308418274 + 2.0 * 6.329949378967285
Epoch 560, val loss: 0.7649785876274109
Epoch 570, training loss: 12.999147415161133 = 0.34559866786003113 + 2.0 * 6.326774597167969
Epoch 570, val loss: 0.7592194676399231
Epoch 580, training loss: 12.978842735290527 = 0.3283396363258362 + 2.0 * 6.325251579284668
Epoch 580, val loss: 0.754031777381897
Epoch 590, training loss: 12.971056938171387 = 0.31172260642051697 + 2.0 * 6.329667091369629
Epoch 590, val loss: 0.7495575547218323
Epoch 600, training loss: 12.9419527053833 = 0.29576268792152405 + 2.0 * 6.323094844818115
Epoch 600, val loss: 0.7456248998641968
Epoch 610, training loss: 12.922280311584473 = 0.28053030371665955 + 2.0 * 6.32087516784668
Epoch 610, val loss: 0.7423135638237
Epoch 620, training loss: 12.906216621398926 = 0.2659670114517212 + 2.0 * 6.320124626159668
Epoch 620, val loss: 0.7396653294563293
Epoch 630, training loss: 12.893989562988281 = 0.252066969871521 + 2.0 * 6.3209614753723145
Epoch 630, val loss: 0.7376434206962585
Epoch 640, training loss: 12.88176155090332 = 0.23889583349227905 + 2.0 * 6.321433067321777
Epoch 640, val loss: 0.7362991571426392
Epoch 650, training loss: 12.858453750610352 = 0.2264174222946167 + 2.0 * 6.316018104553223
Epoch 650, val loss: 0.7354645133018494
Epoch 660, training loss: 12.84323501586914 = 0.21463879942893982 + 2.0 * 6.314298152923584
Epoch 660, val loss: 0.7352409362792969
Epoch 670, training loss: 12.83316421508789 = 0.20349815487861633 + 2.0 * 6.314833164215088
Epoch 670, val loss: 0.7357785701751709
Epoch 680, training loss: 12.816047668457031 = 0.19300983846187592 + 2.0 * 6.311519145965576
Epoch 680, val loss: 0.7365793585777283
Epoch 690, training loss: 12.806690216064453 = 0.18311357498168945 + 2.0 * 6.311788082122803
Epoch 690, val loss: 0.738113522529602
Epoch 700, training loss: 12.79810905456543 = 0.1738014817237854 + 2.0 * 6.3121538162231445
Epoch 700, val loss: 0.7400914430618286
Epoch 710, training loss: 12.791650772094727 = 0.16507406532764435 + 2.0 * 6.31328821182251
Epoch 710, val loss: 0.7423937320709229
Epoch 720, training loss: 12.770416259765625 = 0.15687236189842224 + 2.0 * 6.306771755218506
Epoch 720, val loss: 0.7451541423797607
Epoch 730, training loss: 12.759654998779297 = 0.14918126165866852 + 2.0 * 6.30523681640625
Epoch 730, val loss: 0.748343288898468
Epoch 740, training loss: 12.749281883239746 = 0.1419389545917511 + 2.0 * 6.303671360015869
Epoch 740, val loss: 0.7518808245658875
Epoch 750, training loss: 12.755416870117188 = 0.13510547578334808 + 2.0 * 6.310155868530273
Epoch 750, val loss: 0.7557165622711182
Epoch 760, training loss: 12.747016906738281 = 0.12871389091014862 + 2.0 * 6.309151649475098
Epoch 760, val loss: 0.7599166035652161
Epoch 770, training loss: 12.72617244720459 = 0.1227264553308487 + 2.0 * 6.301723003387451
Epoch 770, val loss: 0.7641268968582153
Epoch 780, training loss: 12.715919494628906 = 0.11708897352218628 + 2.0 * 6.299415111541748
Epoch 780, val loss: 0.7687202095985413
Epoch 790, training loss: 12.708979606628418 = 0.11177291721105576 + 2.0 * 6.298603534698486
Epoch 790, val loss: 0.7734769582748413
Epoch 800, training loss: 12.709894180297852 = 0.10675745457410812 + 2.0 * 6.301568508148193
Epoch 800, val loss: 0.7784493565559387
Epoch 810, training loss: 12.705517768859863 = 0.10201658308506012 + 2.0 * 6.301750659942627
Epoch 810, val loss: 0.7834685444831848
Epoch 820, training loss: 12.692501068115234 = 0.09756015986204147 + 2.0 * 6.297470569610596
Epoch 820, val loss: 0.7886533141136169
Epoch 830, training loss: 12.682536125183105 = 0.09334998577833176 + 2.0 * 6.29459285736084
Epoch 830, val loss: 0.7939991354942322
Epoch 840, training loss: 12.678786277770996 = 0.0893728956580162 + 2.0 * 6.29470682144165
Epoch 840, val loss: 0.799394428730011
Epoch 850, training loss: 12.678810119628906 = 0.08560867607593536 + 2.0 * 6.296600818634033
Epoch 850, val loss: 0.8049549460411072
Epoch 860, training loss: 12.675169944763184 = 0.08205482363700867 + 2.0 * 6.296557426452637
Epoch 860, val loss: 0.8103946447372437
Epoch 870, training loss: 12.664947509765625 = 0.07868855446577072 + 2.0 * 6.2931294441223145
Epoch 870, val loss: 0.8158748745918274
Epoch 880, training loss: 12.655810356140137 = 0.07550448179244995 + 2.0 * 6.2901530265808105
Epoch 880, val loss: 0.8213803172111511
Epoch 890, training loss: 12.6527681350708 = 0.07248169928789139 + 2.0 * 6.290143013000488
Epoch 890, val loss: 0.8269610404968262
Epoch 900, training loss: 12.652499198913574 = 0.06961178779602051 + 2.0 * 6.291443824768066
Epoch 900, val loss: 0.8325653076171875
Epoch 910, training loss: 12.647032737731934 = 0.06688325107097626 + 2.0 * 6.290074825286865
Epoch 910, val loss: 0.8381564021110535
Epoch 920, training loss: 12.645888328552246 = 0.06429874897003174 + 2.0 * 6.290794849395752
Epoch 920, val loss: 0.8437352180480957
Epoch 930, training loss: 12.6379976272583 = 0.06184910982847214 + 2.0 * 6.288074493408203
Epoch 930, val loss: 0.8492001295089722
Epoch 940, training loss: 12.631054878234863 = 0.05952627956867218 + 2.0 * 6.285764217376709
Epoch 940, val loss: 0.8545809388160706
Epoch 950, training loss: 12.625893592834473 = 0.0573173463344574 + 2.0 * 6.284287929534912
Epoch 950, val loss: 0.8602056503295898
Epoch 960, training loss: 12.62517261505127 = 0.0552121177315712 + 2.0 * 6.284980297088623
Epoch 960, val loss: 0.8656558394432068
Epoch 970, training loss: 12.625645637512207 = 0.05321648344397545 + 2.0 * 6.286214351654053
Epoch 970, val loss: 0.8710811734199524
Epoch 980, training loss: 12.621688842773438 = 0.05130791664123535 + 2.0 * 6.285190582275391
Epoch 980, val loss: 0.8764015436172485
Epoch 990, training loss: 12.613424301147461 = 0.04950642213225365 + 2.0 * 6.281959056854248
Epoch 990, val loss: 0.8816648721694946
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7962962962962963
0.8365840801265156
=== training gcn model ===
Epoch 0, training loss: 19.126785278320312 = 1.933099627494812 + 2.0 * 8.596842765808105
Epoch 0, val loss: 1.9301362037658691
Epoch 10, training loss: 19.116806030273438 = 1.9237171411514282 + 2.0 * 8.59654426574707
Epoch 10, val loss: 1.9211007356643677
Epoch 20, training loss: 19.100053787231445 = 1.9122682809829712 + 2.0 * 8.593893051147461
Epoch 20, val loss: 1.909713625907898
Epoch 30, training loss: 19.03953742980957 = 1.8969792127609253 + 2.0 * 8.571279525756836
Epoch 30, val loss: 1.89422607421875
Epoch 40, training loss: 18.702672958374023 = 1.8783191442489624 + 2.0 * 8.412177085876465
Epoch 40, val loss: 1.8759735822677612
Epoch 50, training loss: 17.80074119567871 = 1.8575788736343384 + 2.0 * 7.97158145904541
Epoch 50, val loss: 1.855795979499817
Epoch 60, training loss: 16.90604591369629 = 1.8395754098892212 + 2.0 * 7.533235549926758
Epoch 60, val loss: 1.8397091627120972
Epoch 70, training loss: 16.157363891601562 = 1.8270403146743774 + 2.0 * 7.165162086486816
Epoch 70, val loss: 1.82817542552948
Epoch 80, training loss: 15.766231536865234 = 1.8148754835128784 + 2.0 * 6.975677967071533
Epoch 80, val loss: 1.8172353506088257
Epoch 90, training loss: 15.426426887512207 = 1.8021745681762695 + 2.0 * 6.812126159667969
Epoch 90, val loss: 1.806313395500183
Epoch 100, training loss: 15.23103141784668 = 1.7882061004638672 + 2.0 * 6.721412658691406
Epoch 100, val loss: 1.7939180135726929
Epoch 110, training loss: 15.100408554077148 = 1.7730406522750854 + 2.0 * 6.663683891296387
Epoch 110, val loss: 1.7803876399993896
Epoch 120, training loss: 14.997758865356445 = 1.757568359375 + 2.0 * 6.620095252990723
Epoch 120, val loss: 1.7664555311203003
Epoch 130, training loss: 14.912574768066406 = 1.7412441968917847 + 2.0 * 6.585665225982666
Epoch 130, val loss: 1.75166654586792
Epoch 140, training loss: 14.834124565124512 = 1.7236549854278564 + 2.0 * 6.555234909057617
Epoch 140, val loss: 1.735879898071289
Epoch 150, training loss: 14.768409729003906 = 1.7044005393981934 + 2.0 * 6.532004356384277
Epoch 150, val loss: 1.7188215255737305
Epoch 160, training loss: 14.709881782531738 = 1.6829164028167725 + 2.0 * 6.513482570648193
Epoch 160, val loss: 1.7000070810317993
Epoch 170, training loss: 14.654869079589844 = 1.6590561866760254 + 2.0 * 6.497906684875488
Epoch 170, val loss: 1.6793063879013062
Epoch 180, training loss: 14.59964656829834 = 1.632590413093567 + 2.0 * 6.483528137207031
Epoch 180, val loss: 1.6564431190490723
Epoch 190, training loss: 14.546774864196777 = 1.6032048463821411 + 2.0 * 6.471785068511963
Epoch 190, val loss: 1.6311631202697754
Epoch 200, training loss: 14.491180419921875 = 1.5709288120269775 + 2.0 * 6.460125923156738
Epoch 200, val loss: 1.6035354137420654
Epoch 210, training loss: 14.435433387756348 = 1.5356639623641968 + 2.0 * 6.44988489151001
Epoch 210, val loss: 1.573441743850708
Epoch 220, training loss: 14.386347770690918 = 1.4972779750823975 + 2.0 * 6.444534778594971
Epoch 220, val loss: 1.5408532619476318
Epoch 230, training loss: 14.324930191040039 = 1.456098198890686 + 2.0 * 6.434415817260742
Epoch 230, val loss: 1.5061306953430176
Epoch 240, training loss: 14.265460968017578 = 1.4126973152160645 + 2.0 * 6.426382064819336
Epoch 240, val loss: 1.4696635007858276
Epoch 250, training loss: 14.215054512023926 = 1.367127537727356 + 2.0 * 6.42396354675293
Epoch 250, val loss: 1.431532859802246
Epoch 260, training loss: 14.148390769958496 = 1.320338249206543 + 2.0 * 6.414026260375977
Epoch 260, val loss: 1.3928003311157227
Epoch 270, training loss: 14.089673042297363 = 1.272650122642517 + 2.0 * 6.408511638641357
Epoch 270, val loss: 1.3536126613616943
Epoch 280, training loss: 14.036017417907715 = 1.2245330810546875 + 2.0 * 6.405742168426514
Epoch 280, val loss: 1.3146281242370605
Epoch 290, training loss: 13.97695255279541 = 1.1769558191299438 + 2.0 * 6.399998188018799
Epoch 290, val loss: 1.2764984369277954
Epoch 300, training loss: 13.919434547424316 = 1.130458950996399 + 2.0 * 6.3944878578186035
Epoch 300, val loss: 1.2397847175598145
Epoch 310, training loss: 13.864754676818848 = 1.0852439403533936 + 2.0 * 6.3897552490234375
Epoch 310, val loss: 1.2044943571090698
Epoch 320, training loss: 13.818541526794434 = 1.041357159614563 + 2.0 * 6.38859224319458
Epoch 320, val loss: 1.1707477569580078
Epoch 330, training loss: 13.761469841003418 = 0.9992604851722717 + 2.0 * 6.381104469299316
Epoch 330, val loss: 1.1387677192687988
Epoch 340, training loss: 13.712839126586914 = 0.9591403007507324 + 2.0 * 6.37684965133667
Epoch 340, val loss: 1.1085903644561768
Epoch 350, training loss: 13.667794227600098 = 0.9206106662750244 + 2.0 * 6.373591899871826
Epoch 350, val loss: 1.0800209045410156
Epoch 360, training loss: 13.623348236083984 = 0.8839156031608582 + 2.0 * 6.369716167449951
Epoch 360, val loss: 1.0533524751663208
Epoch 370, training loss: 13.584007263183594 = 0.8492426872253418 + 2.0 * 6.367382049560547
Epoch 370, val loss: 1.028491497039795
Epoch 380, training loss: 13.543213844299316 = 0.8163403272628784 + 2.0 * 6.363436698913574
Epoch 380, val loss: 1.0055269002914429
Epoch 390, training loss: 13.507161140441895 = 0.7852141261100769 + 2.0 * 6.360973358154297
Epoch 390, val loss: 0.9843251705169678
Epoch 400, training loss: 13.472332000732422 = 0.7555201053619385 + 2.0 * 6.358406066894531
Epoch 400, val loss: 0.9648422598838806
Epoch 410, training loss: 13.446455955505371 = 0.7274096608161926 + 2.0 * 6.359523296356201
Epoch 410, val loss: 0.9470930099487305
Epoch 420, training loss: 13.408578872680664 = 0.7007890343666077 + 2.0 * 6.3538947105407715
Epoch 420, val loss: 0.9310305118560791
Epoch 430, training loss: 13.37553596496582 = 0.6753764152526855 + 2.0 * 6.350079536437988
Epoch 430, val loss: 0.9165276885032654
Epoch 440, training loss: 13.34605884552002 = 0.6509848237037659 + 2.0 * 6.347537040710449
Epoch 440, val loss: 0.9033954739570618
Epoch 450, training loss: 13.322588920593262 = 0.6275045275688171 + 2.0 * 6.3475422859191895
Epoch 450, val loss: 0.8915001749992371
Epoch 460, training loss: 13.296238899230957 = 0.6050061583518982 + 2.0 * 6.345616340637207
Epoch 460, val loss: 0.8808857202529907
Epoch 470, training loss: 13.264806747436523 = 0.5833286046981812 + 2.0 * 6.3407392501831055
Epoch 470, val loss: 0.8714035749435425
Epoch 480, training loss: 13.238724708557129 = 0.562303364276886 + 2.0 * 6.338210582733154
Epoch 480, val loss: 0.8629439473152161
Epoch 490, training loss: 13.216702461242676 = 0.5418746471405029 + 2.0 * 6.337413787841797
Epoch 490, val loss: 0.8554518818855286
Epoch 500, training loss: 13.195588111877441 = 0.5220515727996826 + 2.0 * 6.33676815032959
Epoch 500, val loss: 0.8488646149635315
Epoch 510, training loss: 13.174478530883789 = 0.5027273893356323 + 2.0 * 6.335875511169434
Epoch 510, val loss: 0.8431347012519836
Epoch 520, training loss: 13.146624565124512 = 0.48392122983932495 + 2.0 * 6.3313517570495605
Epoch 520, val loss: 0.8382748365402222
Epoch 530, training loss: 13.123043060302734 = 0.46548905968666077 + 2.0 * 6.328776836395264
Epoch 530, val loss: 0.8341742753982544
Epoch 540, training loss: 13.11490535736084 = 0.4473671317100525 + 2.0 * 6.33376932144165
Epoch 540, val loss: 0.830836832523346
Epoch 550, training loss: 13.085042953491211 = 0.42963558435440063 + 2.0 * 6.327703475952148
Epoch 550, val loss: 0.8281012177467346
Epoch 560, training loss: 13.060222625732422 = 0.41221508383750916 + 2.0 * 6.32400369644165
Epoch 560, val loss: 0.8260756134986877
Epoch 570, training loss: 13.043254852294922 = 0.395038366317749 + 2.0 * 6.324108123779297
Epoch 570, val loss: 0.8247125744819641
Epoch 580, training loss: 13.026447296142578 = 0.37822091579437256 + 2.0 * 6.324113368988037
Epoch 580, val loss: 0.8239354491233826
Epoch 590, training loss: 12.99927806854248 = 0.3616520166397095 + 2.0 * 6.318812847137451
Epoch 590, val loss: 0.8238174915313721
Epoch 600, training loss: 12.979351043701172 = 0.3453138470649719 + 2.0 * 6.317018508911133
Epoch 600, val loss: 0.8242568373680115
Epoch 610, training loss: 12.984850883483887 = 0.32918959856033325 + 2.0 * 6.327830791473389
Epoch 610, val loss: 0.8252633213996887
Epoch 620, training loss: 12.945433616638184 = 0.3133859932422638 + 2.0 * 6.316023826599121
Epoch 620, val loss: 0.8267514109611511
Epoch 630, training loss: 12.92387866973877 = 0.29796087741851807 + 2.0 * 6.312958717346191
Epoch 630, val loss: 0.8288108110427856
Epoch 640, training loss: 12.907613754272461 = 0.28289124369621277 + 2.0 * 6.312361240386963
Epoch 640, val loss: 0.8314273357391357
Epoch 650, training loss: 12.894618034362793 = 0.26822057366371155 + 2.0 * 6.313198566436768
Epoch 650, val loss: 0.8346037864685059
Epoch 660, training loss: 12.872138977050781 = 0.25400516390800476 + 2.0 * 6.3090667724609375
Epoch 660, val loss: 0.8383839130401611
Epoch 670, training loss: 12.866362571716309 = 0.24032606184482574 + 2.0 * 6.313018321990967
Epoch 670, val loss: 0.8426633477210999
Epoch 680, training loss: 12.845130920410156 = 0.2272505760192871 + 2.0 * 6.3089399337768555
Epoch 680, val loss: 0.8475390076637268
Epoch 690, training loss: 12.823464393615723 = 0.21478474140167236 + 2.0 * 6.30433988571167
Epoch 690, val loss: 0.852902352809906
Epoch 700, training loss: 12.811556816101074 = 0.2029128223657608 + 2.0 * 6.304321765899658
Epoch 700, val loss: 0.8587508797645569
Epoch 710, training loss: 12.80618667602539 = 0.19166511297225952 + 2.0 * 6.307260990142822
Epoch 710, val loss: 0.8650253415107727
Epoch 720, training loss: 12.789793968200684 = 0.18105365335941315 + 2.0 * 6.304369926452637
Epoch 720, val loss: 0.8716751933097839
Epoch 730, training loss: 12.772170066833496 = 0.17108047008514404 + 2.0 * 6.300544738769531
Epoch 730, val loss: 0.8786771297454834
Epoch 740, training loss: 12.761027336120605 = 0.16170819103717804 + 2.0 * 6.299659729003906
Epoch 740, val loss: 0.8859650492668152
Epoch 750, training loss: 12.762207984924316 = 0.15290065109729767 + 2.0 * 6.304653644561768
Epoch 750, val loss: 0.8935009241104126
Epoch 760, training loss: 12.742305755615234 = 0.14465132355690002 + 2.0 * 6.298827171325684
Epoch 760, val loss: 0.9012194871902466
Epoch 770, training loss: 12.728055953979492 = 0.13692855834960938 + 2.0 * 6.295563697814941
Epoch 770, val loss: 0.909203052520752
Epoch 780, training loss: 12.720094680786133 = 0.1297028660774231 + 2.0 * 6.295196056365967
Epoch 780, val loss: 0.9173851609230042
Epoch 790, training loss: 12.716748237609863 = 0.12292995303869247 + 2.0 * 6.296909332275391
Epoch 790, val loss: 0.925632894039154
Epoch 800, training loss: 12.708271980285645 = 0.11661651730537415 + 2.0 * 6.295827865600586
Epoch 800, val loss: 0.9341272115707397
Epoch 810, training loss: 12.69544506072998 = 0.11072935909032822 + 2.0 * 6.292357921600342
Epoch 810, val loss: 0.942570686340332
Epoch 820, training loss: 12.684503555297852 = 0.10521019250154495 + 2.0 * 6.289646625518799
Epoch 820, val loss: 0.9511585235595703
Epoch 830, training loss: 12.685199737548828 = 0.10003571212291718 + 2.0 * 6.292582035064697
Epoch 830, val loss: 0.9599218368530273
Epoch 840, training loss: 12.67996883392334 = 0.09518489986658096 + 2.0 * 6.292391777038574
Epoch 840, val loss: 0.9685305953025818
Epoch 850, training loss: 12.668973922729492 = 0.0906655564904213 + 2.0 * 6.289154052734375
Epoch 850, val loss: 0.9773039817810059
Epoch 860, training loss: 12.659172058105469 = 0.08643011003732681 + 2.0 * 6.286370754241943
Epoch 860, val loss: 0.9860784411430359
Epoch 870, training loss: 12.662138938903809 = 0.08244619518518448 + 2.0 * 6.289846420288086
Epoch 870, val loss: 0.9947602152824402
Epoch 880, training loss: 12.650100708007812 = 0.07872436940670013 + 2.0 * 6.285688400268555
Epoch 880, val loss: 1.0035215616226196
Epoch 890, training loss: 12.648128509521484 = 0.07521763443946838 + 2.0 * 6.2864556312561035
Epoch 890, val loss: 1.0122214555740356
Epoch 900, training loss: 12.643195152282715 = 0.071930430829525 + 2.0 * 6.285632133483887
Epoch 900, val loss: 1.0208947658538818
Epoch 910, training loss: 12.636066436767578 = 0.06882829964160919 + 2.0 * 6.283618927001953
Epoch 910, val loss: 1.029512643814087
Epoch 920, training loss: 12.629172325134277 = 0.06591753661632538 + 2.0 * 6.281627178192139
Epoch 920, val loss: 1.0381218194961548
Epoch 930, training loss: 12.624239921569824 = 0.0631660521030426 + 2.0 * 6.280537128448486
Epoch 930, val loss: 1.0467143058776855
Epoch 940, training loss: 12.635176658630371 = 0.06056927517056465 + 2.0 * 6.287303924560547
Epoch 940, val loss: 1.055221438407898
Epoch 950, training loss: 12.6207914352417 = 0.05812785029411316 + 2.0 * 6.281332015991211
Epoch 950, val loss: 1.0635008811950684
Epoch 960, training loss: 12.614442825317383 = 0.05582873895764351 + 2.0 * 6.279306888580322
Epoch 960, val loss: 1.071943759918213
Epoch 970, training loss: 12.608857154846191 = 0.053653281182050705 + 2.0 * 6.277601718902588
Epoch 970, val loss: 1.0802056789398193
Epoch 980, training loss: 12.60779857635498 = 0.05159123241901398 + 2.0 * 6.278103828430176
Epoch 980, val loss: 1.0883574485778809
Epoch 990, training loss: 12.616312026977539 = 0.04965148866176605 + 2.0 * 6.28333044052124
Epoch 990, val loss: 1.0965100526809692
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7592592592592593
0.8365840801265156
=== training gcn model ===
Epoch 0, training loss: 19.148168563842773 = 1.954477071762085 + 2.0 * 8.596845626831055
Epoch 0, val loss: 1.9484033584594727
Epoch 10, training loss: 19.136499404907227 = 1.943374514579773 + 2.0 * 8.596562385559082
Epoch 10, val loss: 1.9365935325622559
Epoch 20, training loss: 19.117923736572266 = 1.9296159744262695 + 2.0 * 8.59415340423584
Epoch 20, val loss: 1.9217249155044556
Epoch 30, training loss: 19.058862686157227 = 1.910946249961853 + 2.0 * 8.573958396911621
Epoch 30, val loss: 1.9014978408813477
Epoch 40, training loss: 18.776308059692383 = 1.8876984119415283 + 2.0 * 8.444304466247559
Epoch 40, val loss: 1.8776729106903076
Epoch 50, training loss: 17.9947566986084 = 1.8638883829116821 + 2.0 * 8.065434455871582
Epoch 50, val loss: 1.8546143770217896
Epoch 60, training loss: 17.007232666015625 = 1.8462580442428589 + 2.0 * 7.5804877281188965
Epoch 60, val loss: 1.8391995429992676
Epoch 70, training loss: 16.077844619750977 = 1.8337119817733765 + 2.0 * 7.122066020965576
Epoch 70, val loss: 1.8279179334640503
Epoch 80, training loss: 15.67259693145752 = 1.8218834400177002 + 2.0 * 6.925356864929199
Epoch 80, val loss: 1.8167859315872192
Epoch 90, training loss: 15.44608211517334 = 1.8067649602890015 + 2.0 * 6.8196587562561035
Epoch 90, val loss: 1.8027538061141968
Epoch 100, training loss: 15.262821197509766 = 1.790987253189087 + 2.0 * 6.735917091369629
Epoch 100, val loss: 1.7885116338729858
Epoch 110, training loss: 15.12485122680664 = 1.7756158113479614 + 2.0 * 6.674617767333984
Epoch 110, val loss: 1.7751033306121826
Epoch 120, training loss: 15.016744613647461 = 1.7605738639831543 + 2.0 * 6.628085136413574
Epoch 120, val loss: 1.7621967792510986
Epoch 130, training loss: 14.935807228088379 = 1.745599627494812 + 2.0 * 6.595103740692139
Epoch 130, val loss: 1.7491767406463623
Epoch 140, training loss: 14.86214542388916 = 1.7296134233474731 + 2.0 * 6.566266059875488
Epoch 140, val loss: 1.7353312969207764
Epoch 150, training loss: 14.8023681640625 = 1.7122483253479004 + 2.0 * 6.545059680938721
Epoch 150, val loss: 1.7203896045684814
Epoch 160, training loss: 14.735952377319336 = 1.6932986974716187 + 2.0 * 6.521327018737793
Epoch 160, val loss: 1.703984260559082
Epoch 170, training loss: 14.67633056640625 = 1.6724913120269775 + 2.0 * 6.501919746398926
Epoch 170, val loss: 1.6860508918762207
Epoch 180, training loss: 14.618911743164062 = 1.6494780778884888 + 2.0 * 6.484716892242432
Epoch 180, val loss: 1.6663326025009155
Epoch 190, training loss: 14.571017265319824 = 1.62396240234375 + 2.0 * 6.473527431488037
Epoch 190, val loss: 1.6445456743240356
Epoch 200, training loss: 14.512207984924316 = 1.5959588289260864 + 2.0 * 6.45812463760376
Epoch 200, val loss: 1.6205863952636719
Epoch 210, training loss: 14.45984172821045 = 1.565561056137085 + 2.0 * 6.447140216827393
Epoch 210, val loss: 1.5947284698486328
Epoch 220, training loss: 14.40545654296875 = 1.5324883460998535 + 2.0 * 6.436483860015869
Epoch 220, val loss: 1.5666487216949463
Epoch 230, training loss: 14.362611770629883 = 1.4966588020324707 + 2.0 * 6.432976722717285
Epoch 230, val loss: 1.5364995002746582
Epoch 240, training loss: 14.302179336547852 = 1.4587409496307373 + 2.0 * 6.421719074249268
Epoch 240, val loss: 1.5045191049575806
Epoch 250, training loss: 14.246652603149414 = 1.4185738563537598 + 2.0 * 6.414039134979248
Epoch 250, val loss: 1.4708709716796875
Epoch 260, training loss: 14.200695037841797 = 1.3765742778778076 + 2.0 * 6.412060260772705
Epoch 260, val loss: 1.435761570930481
Epoch 270, training loss: 14.139464378356934 = 1.333203673362732 + 2.0 * 6.403130531311035
Epoch 270, val loss: 1.39976167678833
Epoch 280, training loss: 14.08227825164795 = 1.2885997295379639 + 2.0 * 6.396839141845703
Epoch 280, val loss: 1.3628385066986084
Epoch 290, training loss: 14.028292655944824 = 1.2429754734039307 + 2.0 * 6.392658710479736
Epoch 290, val loss: 1.3252660036087036
Epoch 300, training loss: 13.9742431640625 = 1.1969863176345825 + 2.0 * 6.3886284828186035
Epoch 300, val loss: 1.2874001264572144
Epoch 310, training loss: 13.919775009155273 = 1.1510710716247559 + 2.0 * 6.38435173034668
Epoch 310, val loss: 1.2497200965881348
Epoch 320, training loss: 13.866397857666016 = 1.1053825616836548 + 2.0 * 6.380507469177246
Epoch 320, val loss: 1.2124831676483154
Epoch 330, training loss: 13.831255912780762 = 1.0605077743530273 + 2.0 * 6.385374069213867
Epoch 330, val loss: 1.1759098768234253
Epoch 340, training loss: 13.768518447875977 = 1.0171691179275513 + 2.0 * 6.375674724578857
Epoch 340, val loss: 1.1406892538070679
Epoch 350, training loss: 13.717752456665039 = 0.9755816459655762 + 2.0 * 6.371085166931152
Epoch 350, val loss: 1.1073330640792847
Epoch 360, training loss: 13.670764923095703 = 0.9359717965126038 + 2.0 * 6.367396354675293
Epoch 360, val loss: 1.075904130935669
Epoch 370, training loss: 13.630191802978516 = 0.8984769582748413 + 2.0 * 6.3658576011657715
Epoch 370, val loss: 1.0467392206192017
Epoch 380, training loss: 13.595321655273438 = 0.8634571433067322 + 2.0 * 6.365932464599609
Epoch 380, val loss: 1.0197550058364868
Epoch 390, training loss: 13.550519943237305 = 0.8307280540466309 + 2.0 * 6.359896183013916
Epoch 390, val loss: 0.9954120516777039
Epoch 400, training loss: 13.513620376586914 = 0.8001245856285095 + 2.0 * 6.356748104095459
Epoch 400, val loss: 0.9731625914573669
Epoch 410, training loss: 13.479997634887695 = 0.7715486288070679 + 2.0 * 6.354224681854248
Epoch 410, val loss: 0.9529930949211121
Epoch 420, training loss: 13.448040008544922 = 0.7448465824127197 + 2.0 * 6.351596832275391
Epoch 420, val loss: 0.9344900846481323
Epoch 430, training loss: 13.417357444763184 = 0.7195168733596802 + 2.0 * 6.3489203453063965
Epoch 430, val loss: 0.9174327254295349
Epoch 440, training loss: 13.396798133850098 = 0.6953095197677612 + 2.0 * 6.350744247436523
Epoch 440, val loss: 0.9016026258468628
Epoch 450, training loss: 13.361791610717773 = 0.6719490885734558 + 2.0 * 6.344921112060547
Epoch 450, val loss: 0.8866510987281799
Epoch 460, training loss: 13.332586288452148 = 0.649265468120575 + 2.0 * 6.341660499572754
Epoch 460, val loss: 0.8725643157958984
Epoch 470, training loss: 13.313535690307617 = 0.6269937753677368 + 2.0 * 6.343270778656006
Epoch 470, val loss: 0.8590795993804932
Epoch 480, training loss: 13.281455993652344 = 0.6052213907241821 + 2.0 * 6.3381171226501465
Epoch 480, val loss: 0.8461884260177612
Epoch 490, training loss: 13.25424861907959 = 0.5836929082870483 + 2.0 * 6.335278034210205
Epoch 490, val loss: 0.833727240562439
Epoch 500, training loss: 13.228473663330078 = 0.5622738003730774 + 2.0 * 6.333099842071533
Epoch 500, val loss: 0.8217049241065979
Epoch 510, training loss: 13.203537940979004 = 0.5408896207809448 + 2.0 * 6.331324100494385
Epoch 510, val loss: 0.810032069683075
Epoch 520, training loss: 13.198447227478027 = 0.5195557475090027 + 2.0 * 6.3394455909729
Epoch 520, val loss: 0.7986981272697449
Epoch 530, training loss: 13.154684066772461 = 0.4983716309070587 + 2.0 * 6.328155994415283
Epoch 530, val loss: 0.7875375747680664
Epoch 540, training loss: 13.13198471069336 = 0.47751492261886597 + 2.0 * 6.327234745025635
Epoch 540, val loss: 0.7768145799636841
Epoch 550, training loss: 13.10995101928711 = 0.4569152593612671 + 2.0 * 6.3265180587768555
Epoch 550, val loss: 0.7665494680404663
Epoch 560, training loss: 13.083659172058105 = 0.43658894300460815 + 2.0 * 6.323534965515137
Epoch 560, val loss: 0.7568361759185791
Epoch 570, training loss: 13.063589096069336 = 0.41662898659706116 + 2.0 * 6.323480129241943
Epoch 570, val loss: 0.7475692629814148
Epoch 580, training loss: 13.038519859313965 = 0.3971319794654846 + 2.0 * 6.3206939697265625
Epoch 580, val loss: 0.7387759685516357
Epoch 590, training loss: 13.014694213867188 = 0.3782368004322052 + 2.0 * 6.318228721618652
Epoch 590, val loss: 0.7307467460632324
Epoch 600, training loss: 12.993773460388184 = 0.359928160905838 + 2.0 * 6.316922664642334
Epoch 600, val loss: 0.7233244180679321
Epoch 610, training loss: 12.977471351623535 = 0.3422386050224304 + 2.0 * 6.3176164627075195
Epoch 610, val loss: 0.7166938781738281
Epoch 620, training loss: 12.960701942443848 = 0.3252212703227997 + 2.0 * 6.317740440368652
Epoch 620, val loss: 0.7107142210006714
Epoch 630, training loss: 12.934759140014648 = 0.30902099609375 + 2.0 * 6.312869071960449
Epoch 630, val loss: 0.7055222392082214
Epoch 640, training loss: 12.916291236877441 = 0.29350516200065613 + 2.0 * 6.3113932609558105
Epoch 640, val loss: 0.701093316078186
Epoch 650, training loss: 12.920687675476074 = 0.2786796987056732 + 2.0 * 6.3210039138793945
Epoch 650, val loss: 0.6973403096199036
Epoch 660, training loss: 12.887276649475098 = 0.26442059874534607 + 2.0 * 6.311428070068359
Epoch 660, val loss: 0.6943768262863159
Epoch 670, training loss: 12.866517066955566 = 0.250916451215744 + 2.0 * 6.30780029296875
Epoch 670, val loss: 0.6919699311256409
Epoch 680, training loss: 12.859169960021973 = 0.23803620040416718 + 2.0 * 6.3105669021606445
Epoch 680, val loss: 0.6902580857276917
Epoch 690, training loss: 12.839414596557617 = 0.22574414312839508 + 2.0 * 6.306835174560547
Epoch 690, val loss: 0.68918377161026
Epoch 700, training loss: 12.823018074035645 = 0.21410183608531952 + 2.0 * 6.304458141326904
Epoch 700, val loss: 0.6886456608772278
Epoch 710, training loss: 12.809804916381836 = 0.2030431628227234 + 2.0 * 6.303380966186523
Epoch 710, val loss: 0.688701331615448
Epoch 720, training loss: 12.803314208984375 = 0.19257467985153198 + 2.0 * 6.305369853973389
Epoch 720, val loss: 0.6891816854476929
Epoch 730, training loss: 12.78665542602539 = 0.18266704678535461 + 2.0 * 6.301994323730469
Epoch 730, val loss: 0.6901204586029053
Epoch 740, training loss: 12.772541046142578 = 0.173310324549675 + 2.0 * 6.299615383148193
Epoch 740, val loss: 0.691529393196106
Epoch 750, training loss: 12.765612602233887 = 0.16448557376861572 + 2.0 * 6.300563335418701
Epoch 750, val loss: 0.6933183670043945
Epoch 760, training loss: 12.751996994018555 = 0.15617036819458008 + 2.0 * 6.297913551330566
Epoch 760, val loss: 0.6953849792480469
Epoch 770, training loss: 12.745278358459473 = 0.1483553797006607 + 2.0 * 6.298461437225342
Epoch 770, val loss: 0.6977090835571289
Epoch 780, training loss: 12.738486289978027 = 0.14094287157058716 + 2.0 * 6.298771858215332
Epoch 780, val loss: 0.7004737257957458
Epoch 790, training loss: 12.723410606384277 = 0.13400836288928986 + 2.0 * 6.294701099395752
Epoch 790, val loss: 0.7034658193588257
Epoch 800, training loss: 12.718052864074707 = 0.12744580209255219 + 2.0 * 6.2953033447265625
Epoch 800, val loss: 0.7067940831184387
Epoch 810, training loss: 12.708632469177246 = 0.12127836793661118 + 2.0 * 6.293676853179932
Epoch 810, val loss: 0.7101789116859436
Epoch 820, training loss: 12.700688362121582 = 0.11548265814781189 + 2.0 * 6.292603015899658
Epoch 820, val loss: 0.7138151526451111
Epoch 830, training loss: 12.690923690795898 = 0.11002009361982346 + 2.0 * 6.290452003479004
Epoch 830, val loss: 0.7175804376602173
Epoch 840, training loss: 12.689850807189941 = 0.10486596077680588 + 2.0 * 6.292492389678955
Epoch 840, val loss: 0.7215771675109863
Epoch 850, training loss: 12.67763614654541 = 0.10001127421855927 + 2.0 * 6.288812637329102
Epoch 850, val loss: 0.7256224751472473
Epoch 860, training loss: 12.674965858459473 = 0.09542737156152725 + 2.0 * 6.289769172668457
Epoch 860, val loss: 0.7296590209007263
Epoch 870, training loss: 12.669486045837402 = 0.09110929816961288 + 2.0 * 6.289188385009766
Epoch 870, val loss: 0.7339022159576416
Epoch 880, training loss: 12.660207748413086 = 0.08704490214586258 + 2.0 * 6.286581516265869
Epoch 880, val loss: 0.738115668296814
Epoch 890, training loss: 12.653706550598145 = 0.08321276307106018 + 2.0 * 6.285246849060059
Epoch 890, val loss: 0.7424294948577881
Epoch 900, training loss: 12.647126197814941 = 0.07958275824785233 + 2.0 * 6.283771514892578
Epoch 900, val loss: 0.7467943429946899
Epoch 910, training loss: 12.641209602355957 = 0.07614485919475555 + 2.0 * 6.282532215118408
Epoch 910, val loss: 0.751220166683197
Epoch 920, training loss: 12.654706001281738 = 0.07289288192987442 + 2.0 * 6.2909064292907715
Epoch 920, val loss: 0.7557685375213623
Epoch 930, training loss: 12.638651847839355 = 0.0698050782084465 + 2.0 * 6.284423351287842
Epoch 930, val loss: 0.760037899017334
Epoch 940, training loss: 12.633292198181152 = 0.06690071523189545 + 2.0 * 6.283195972442627
Epoch 940, val loss: 0.764445960521698
Epoch 950, training loss: 12.629142761230469 = 0.06414500623941422 + 2.0 * 6.282498836517334
Epoch 950, val loss: 0.7689290642738342
Epoch 960, training loss: 12.619551658630371 = 0.061528272926807404 + 2.0 * 6.2790117263793945
Epoch 960, val loss: 0.7732862234115601
Epoch 970, training loss: 12.614442825317383 = 0.05906882882118225 + 2.0 * 6.277687072753906
Epoch 970, val loss: 0.7777379155158997
Epoch 980, training loss: 12.611040115356445 = 0.056725502014160156 + 2.0 * 6.277157306671143
Epoch 980, val loss: 0.7822110652923584
Epoch 990, training loss: 12.62572193145752 = 0.054506588727235794 + 2.0 * 6.285607814788818
Epoch 990, val loss: 0.7866308093070984
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8037037037037037
0.8386926726410122
The final CL Acc:0.78642, 0.01944, The final GNN Acc:0.83729, 0.00099
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.3 der2:0.4 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11622])
remove edge: torch.Size([2, 9502])
updated graph: torch.Size([2, 10568])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 19.13028335571289 = 1.9366085529327393 + 2.0 * 8.596837043762207
Epoch 0, val loss: 1.9295498132705688
Epoch 10, training loss: 19.11985206604004 = 1.9267795085906982 + 2.0 * 8.596536636352539
Epoch 10, val loss: 1.9207022190093994
Epoch 20, training loss: 19.103118896484375 = 1.9143187999725342 + 2.0 * 8.594400405883789
Epoch 20, val loss: 1.9092482328414917
Epoch 30, training loss: 19.05284309387207 = 1.897051453590393 + 2.0 * 8.577896118164062
Epoch 30, val loss: 1.8933618068695068
Epoch 40, training loss: 18.835567474365234 = 1.8750094175338745 + 2.0 * 8.480278968811035
Epoch 40, val loss: 1.8737761974334717
Epoch 50, training loss: 18.051164627075195 = 1.8522820472717285 + 2.0 * 8.099441528320312
Epoch 50, val loss: 1.8535383939743042
Epoch 60, training loss: 17.404714584350586 = 1.833258032798767 + 2.0 * 7.7857279777526855
Epoch 60, val loss: 1.8363933563232422
Epoch 70, training loss: 16.473712921142578 = 1.8207309246063232 + 2.0 * 7.326491355895996
Epoch 70, val loss: 1.8254820108413696
Epoch 80, training loss: 15.824398040771484 = 1.813245177268982 + 2.0 * 7.0055766105651855
Epoch 80, val loss: 1.8183550834655762
Epoch 90, training loss: 15.548538208007812 = 1.8016622066497803 + 2.0 * 6.873437881469727
Epoch 90, val loss: 1.8074023723602295
Epoch 100, training loss: 15.35810661315918 = 1.7883782386779785 + 2.0 * 6.7848639488220215
Epoch 100, val loss: 1.7958791255950928
Epoch 110, training loss: 15.221735954284668 = 1.7759732007980347 + 2.0 * 6.722881317138672
Epoch 110, val loss: 1.7848564386367798
Epoch 120, training loss: 15.118441581726074 = 1.7639132738113403 + 2.0 * 6.677264213562012
Epoch 120, val loss: 1.7734832763671875
Epoch 130, training loss: 15.041872024536133 = 1.7511963844299316 + 2.0 * 6.6453375816345215
Epoch 130, val loss: 1.7612886428833008
Epoch 140, training loss: 14.9635591506958 = 1.737201452255249 + 2.0 * 6.613178730010986
Epoch 140, val loss: 1.7482503652572632
Epoch 150, training loss: 14.896934509277344 = 1.7218214273452759 + 2.0 * 6.5875563621521
Epoch 150, val loss: 1.7343019247055054
Epoch 160, training loss: 14.842561721801758 = 1.704648733139038 + 2.0 * 6.56895637512207
Epoch 160, val loss: 1.7190569639205933
Epoch 170, training loss: 14.780473709106445 = 1.6855785846710205 + 2.0 * 6.547447681427002
Epoch 170, val loss: 1.7022887468338013
Epoch 180, training loss: 14.724664688110352 = 1.6643484830856323 + 2.0 * 6.530158042907715
Epoch 180, val loss: 1.6837878227233887
Epoch 190, training loss: 14.669343948364258 = 1.6405283212661743 + 2.0 * 6.514407634735107
Epoch 190, val loss: 1.6632145643234253
Epoch 200, training loss: 14.617192268371582 = 1.6140106916427612 + 2.0 * 6.501590728759766
Epoch 200, val loss: 1.6404473781585693
Epoch 210, training loss: 14.568074226379395 = 1.5851303339004517 + 2.0 * 6.491471767425537
Epoch 210, val loss: 1.6159776449203491
Epoch 220, training loss: 14.513469696044922 = 1.5538145303726196 + 2.0 * 6.479827404022217
Epoch 220, val loss: 1.5898445844650269
Epoch 230, training loss: 14.460376739501953 = 1.5202170610427856 + 2.0 * 6.4700798988342285
Epoch 230, val loss: 1.5622413158416748
Epoch 240, training loss: 14.423176765441895 = 1.4846875667572021 + 2.0 * 6.469244480133057
Epoch 240, val loss: 1.5337203741073608
Epoch 250, training loss: 14.358743667602539 = 1.448194980621338 + 2.0 * 6.4552741050720215
Epoch 250, val loss: 1.5052249431610107
Epoch 260, training loss: 14.305051803588867 = 1.4109725952148438 + 2.0 * 6.447039604187012
Epoch 260, val loss: 1.4769304990768433
Epoch 270, training loss: 14.254432678222656 = 1.3732213973999023 + 2.0 * 6.440605640411377
Epoch 270, val loss: 1.4489351511001587
Epoch 280, training loss: 14.203496932983398 = 1.3356316089630127 + 2.0 * 6.433932781219482
Epoch 280, val loss: 1.421991229057312
Epoch 290, training loss: 14.155649185180664 = 1.2984371185302734 + 2.0 * 6.428606033325195
Epoch 290, val loss: 1.396119236946106
Epoch 300, training loss: 14.113995552062988 = 1.2617138624191284 + 2.0 * 6.426140785217285
Epoch 300, val loss: 1.3712421655654907
Epoch 310, training loss: 14.065186500549316 = 1.2256033420562744 + 2.0 * 6.4197916984558105
Epoch 310, val loss: 1.3472403287887573
Epoch 320, training loss: 14.016901016235352 = 1.190140724182129 + 2.0 * 6.413380146026611
Epoch 320, val loss: 1.3239778280258179
Epoch 330, training loss: 13.972355842590332 = 1.1550700664520264 + 2.0 * 6.408642768859863
Epoch 330, val loss: 1.3011726140975952
Epoch 340, training loss: 13.928950309753418 = 1.1202995777130127 + 2.0 * 6.404325485229492
Epoch 340, val loss: 1.2786329984664917
Epoch 350, training loss: 13.8869047164917 = 1.0859622955322266 + 2.0 * 6.400471210479736
Epoch 350, val loss: 1.256446123123169
Epoch 360, training loss: 13.846923828125 = 1.0522589683532715 + 2.0 * 6.397332191467285
Epoch 360, val loss: 1.2344582080841064
Epoch 370, training loss: 13.802544593811035 = 1.0191049575805664 + 2.0 * 6.391719818115234
Epoch 370, val loss: 1.2127528190612793
Epoch 380, training loss: 13.768586158752441 = 0.9863325357437134 + 2.0 * 6.39112663269043
Epoch 380, val loss: 1.1911065578460693
Epoch 390, training loss: 13.73023509979248 = 0.9539549946784973 + 2.0 * 6.3881402015686035
Epoch 390, val loss: 1.169666051864624
Epoch 400, training loss: 13.685722351074219 = 0.9221056699752808 + 2.0 * 6.381808280944824
Epoch 400, val loss: 1.148366093635559
Epoch 410, training loss: 13.650492668151855 = 0.8904616236686707 + 2.0 * 6.3800153732299805
Epoch 410, val loss: 1.127210021018982
Epoch 420, training loss: 13.614809036254883 = 0.8592396378517151 + 2.0 * 6.377784729003906
Epoch 420, val loss: 1.1062275171279907
Epoch 430, training loss: 13.57320785522461 = 0.8284598588943481 + 2.0 * 6.372374057769775
Epoch 430, val loss: 1.085504174232483
Epoch 440, training loss: 13.536849021911621 = 0.7980800867080688 + 2.0 * 6.369384288787842
Epoch 440, val loss: 1.0650928020477295
Epoch 450, training loss: 13.51658821105957 = 0.7680908441543579 + 2.0 * 6.374248504638672
Epoch 450, val loss: 1.045145869255066
Epoch 460, training loss: 13.468584060668945 = 0.738899827003479 + 2.0 * 6.364841938018799
Epoch 460, val loss: 1.0257627964019775
Epoch 470, training loss: 13.434673309326172 = 0.7105280160903931 + 2.0 * 6.362072467803955
Epoch 470, val loss: 1.0073224306106567
Epoch 480, training loss: 13.401979446411133 = 0.6829006671905518 + 2.0 * 6.35953950881958
Epoch 480, val loss: 0.989773690700531
Epoch 490, training loss: 13.380617141723633 = 0.6560719609260559 + 2.0 * 6.3622727394104
Epoch 490, val loss: 0.9731553792953491
Epoch 500, training loss: 13.339237213134766 = 0.6303081512451172 + 2.0 * 6.354464530944824
Epoch 500, val loss: 0.9577702283859253
Epoch 510, training loss: 13.312137603759766 = 0.6055481433868408 + 2.0 * 6.353294849395752
Epoch 510, val loss: 0.9436677694320679
Epoch 520, training loss: 13.281472206115723 = 0.5816446542739868 + 2.0 * 6.349913597106934
Epoch 520, val loss: 0.9308592081069946
Epoch 530, training loss: 13.254095077514648 = 0.5585094094276428 + 2.0 * 6.347792625427246
Epoch 530, val loss: 0.9192367792129517
Epoch 540, training loss: 13.238083839416504 = 0.5360918045043945 + 2.0 * 6.350996017456055
Epoch 540, val loss: 0.9087296724319458
Epoch 550, training loss: 13.208056449890137 = 0.5144094228744507 + 2.0 * 6.346823692321777
Epoch 550, val loss: 0.8993651866912842
Epoch 560, training loss: 13.179227828979492 = 0.4934299886226654 + 2.0 * 6.342898845672607
Epoch 560, val loss: 0.8911575675010681
Epoch 570, training loss: 13.156671524047852 = 0.473006010055542 + 2.0 * 6.341832637786865
Epoch 570, val loss: 0.8839378356933594
Epoch 580, training loss: 13.133277893066406 = 0.4530683159828186 + 2.0 * 6.340104579925537
Epoch 580, val loss: 0.8776394724845886
Epoch 590, training loss: 13.107812881469727 = 0.433573454618454 + 2.0 * 6.3371195793151855
Epoch 590, val loss: 0.8721999526023865
Epoch 600, training loss: 13.08549976348877 = 0.41453608870506287 + 2.0 * 6.335481643676758
Epoch 600, val loss: 0.8676623106002808
Epoch 610, training loss: 13.065731048583984 = 0.39588773250579834 + 2.0 * 6.334921836853027
Epoch 610, val loss: 0.8638923168182373
Epoch 620, training loss: 13.042563438415527 = 0.3776693344116211 + 2.0 * 6.332447052001953
Epoch 620, val loss: 0.8609007000923157
Epoch 630, training loss: 13.025521278381348 = 0.3599051535129547 + 2.0 * 6.332808017730713
Epoch 630, val loss: 0.8586398363113403
Epoch 640, training loss: 13.000486373901367 = 0.3425893187522888 + 2.0 * 6.328948497772217
Epoch 640, val loss: 0.8570738434791565
Epoch 650, training loss: 12.98703384399414 = 0.3258250951766968 + 2.0 * 6.330604553222656
Epoch 650, val loss: 0.8562027812004089
Epoch 660, training loss: 12.963780403137207 = 0.3096119463443756 + 2.0 * 6.327084064483643
Epoch 660, val loss: 0.8559556007385254
Epoch 670, training loss: 12.94233512878418 = 0.2940423786640167 + 2.0 * 6.324146270751953
Epoch 670, val loss: 0.8564198017120361
Epoch 680, training loss: 12.923251152038574 = 0.27904921770095825 + 2.0 * 6.32210111618042
Epoch 680, val loss: 0.8574573397636414
Epoch 690, training loss: 12.910021781921387 = 0.26463785767555237 + 2.0 * 6.322691917419434
Epoch 690, val loss: 0.8591431379318237
Epoch 700, training loss: 12.90161418914795 = 0.25091832876205444 + 2.0 * 6.325347900390625
Epoch 700, val loss: 0.8612532019615173
Epoch 710, training loss: 12.876781463623047 = 0.23788756132125854 + 2.0 * 6.319447040557861
Epoch 710, val loss: 0.863971471786499
Epoch 720, training loss: 12.858992576599121 = 0.22550806403160095 + 2.0 * 6.316742420196533
Epoch 720, val loss: 0.8671880960464478
Epoch 730, training loss: 12.844367027282715 = 0.21372894942760468 + 2.0 * 6.315319061279297
Epoch 730, val loss: 0.8708523511886597
Epoch 740, training loss: 12.853074073791504 = 0.20254258811473846 + 2.0 * 6.325265884399414
Epoch 740, val loss: 0.8749288320541382
Epoch 750, training loss: 12.820247650146484 = 0.19204513728618622 + 2.0 * 6.314101219177246
Epoch 750, val loss: 0.8793528079986572
Epoch 760, training loss: 12.806807518005371 = 0.18212218582630157 + 2.0 * 6.312342643737793
Epoch 760, val loss: 0.8841394782066345
Epoch 770, training loss: 12.801414489746094 = 0.17276223003864288 + 2.0 * 6.314326286315918
Epoch 770, val loss: 0.8892530202865601
Epoch 780, training loss: 12.783056259155273 = 0.1639343798160553 + 2.0 * 6.309560775756836
Epoch 780, val loss: 0.8946008086204529
Epoch 790, training loss: 12.773470878601074 = 0.15562885999679565 + 2.0 * 6.308920860290527
Epoch 790, val loss: 0.9002740979194641
Epoch 800, training loss: 12.780807495117188 = 0.1477992683649063 + 2.0 * 6.316504001617432
Epoch 800, val loss: 0.9061667919158936
Epoch 810, training loss: 12.755559921264648 = 0.14043235778808594 + 2.0 * 6.307563781738281
Epoch 810, val loss: 0.9122377634048462
Epoch 820, training loss: 12.74332046508789 = 0.133514404296875 + 2.0 * 6.304903030395508
Epoch 820, val loss: 0.9185405969619751
Epoch 830, training loss: 12.743901252746582 = 0.12698808312416077 + 2.0 * 6.3084564208984375
Epoch 830, val loss: 0.9249739050865173
Epoch 840, training loss: 12.731363296508789 = 0.12086272239685059 + 2.0 * 6.30525016784668
Epoch 840, val loss: 0.9315597414970398
Epoch 850, training loss: 12.722594261169434 = 0.11508935689926147 + 2.0 * 6.303752422332764
Epoch 850, val loss: 0.9383190870285034
Epoch 860, training loss: 12.717801094055176 = 0.10966961830854416 + 2.0 * 6.304065704345703
Epoch 860, val loss: 0.9452071189880371
Epoch 870, training loss: 12.706063270568848 = 0.10456544160842896 + 2.0 * 6.300748825073242
Epoch 870, val loss: 0.9522730112075806
Epoch 880, training loss: 12.700928688049316 = 0.09975045919418335 + 2.0 * 6.300589084625244
Epoch 880, val loss: 0.9593564867973328
Epoch 890, training loss: 12.699888229370117 = 0.09521765261888504 + 2.0 * 6.302335262298584
Epoch 890, val loss: 0.966488778591156
Epoch 900, training loss: 12.68713665008545 = 0.09094005078077316 + 2.0 * 6.298098087310791
Epoch 900, val loss: 0.9737494587898254
Epoch 910, training loss: 12.680085182189941 = 0.08691685646772385 + 2.0 * 6.296584129333496
Epoch 910, val loss: 0.981036365032196
Epoch 920, training loss: 12.680001258850098 = 0.08311691135168076 + 2.0 * 6.298442363739014
Epoch 920, val loss: 0.9884246587753296
Epoch 930, training loss: 12.677098274230957 = 0.07952550053596497 + 2.0 * 6.298786163330078
Epoch 930, val loss: 0.9957312941551208
Epoch 940, training loss: 12.669661521911621 = 0.07614561915397644 + 2.0 * 6.29675817489624
Epoch 940, val loss: 1.0031248331069946
Epoch 950, training loss: 12.661478042602539 = 0.07295917719602585 + 2.0 * 6.294259548187256
Epoch 950, val loss: 1.0104069709777832
Epoch 960, training loss: 12.654815673828125 = 0.06995826959609985 + 2.0 * 6.292428493499756
Epoch 960, val loss: 1.0177417993545532
Epoch 970, training loss: 12.649043083190918 = 0.06711290031671524 + 2.0 * 6.2909650802612305
Epoch 970, val loss: 1.0251301527023315
Epoch 980, training loss: 12.647185325622559 = 0.06441844254732132 + 2.0 * 6.291383266448975
Epoch 980, val loss: 1.0324726104736328
Epoch 990, training loss: 12.645953178405762 = 0.061873726546764374 + 2.0 * 6.29203987121582
Epoch 990, val loss: 1.039726734161377
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7555555555555555
0.8170795993674222
=== training gcn model ===
Epoch 0, training loss: 19.15477180480957 = 1.961210012435913 + 2.0 * 8.596780776977539
Epoch 0, val loss: 1.9568418264389038
Epoch 10, training loss: 19.143163681030273 = 1.950615644454956 + 2.0 * 8.596274375915527
Epoch 10, val loss: 1.9467549324035645
Epoch 20, training loss: 19.122167587280273 = 1.9376399517059326 + 2.0 * 8.592264175415039
Epoch 20, val loss: 1.9337486028671265
Epoch 30, training loss: 19.04277801513672 = 1.9201745986938477 + 2.0 * 8.561301231384277
Epoch 30, val loss: 1.9160993099212646
Epoch 40, training loss: 18.48927879333496 = 1.9006140232086182 + 2.0 * 8.294332504272461
Epoch 40, val loss: 1.8974026441574097
Epoch 50, training loss: 17.098047256469727 = 1.881250023841858 + 2.0 * 7.608398914337158
Epoch 50, val loss: 1.8792383670806885
Epoch 60, training loss: 16.40867805480957 = 1.8674559593200684 + 2.0 * 7.270610809326172
Epoch 60, val loss: 1.8665125370025635
Epoch 70, training loss: 15.888980865478516 = 1.8535393476486206 + 2.0 * 7.017720699310303
Epoch 70, val loss: 1.8535722494125366
Epoch 80, training loss: 15.617870330810547 = 1.840802550315857 + 2.0 * 6.888534069061279
Epoch 80, val loss: 1.841766595840454
Epoch 90, training loss: 15.415590286254883 = 1.8275809288024902 + 2.0 * 6.794004917144775
Epoch 90, val loss: 1.830065131187439
Epoch 100, training loss: 15.275045394897461 = 1.815096378326416 + 2.0 * 6.729974746704102
Epoch 100, val loss: 1.8190523386001587
Epoch 110, training loss: 15.165876388549805 = 1.8035037517547607 + 2.0 * 6.681186199188232
Epoch 110, val loss: 1.8085315227508545
Epoch 120, training loss: 15.07331657409668 = 1.7924541234970093 + 2.0 * 6.6404314041137695
Epoch 120, val loss: 1.7982693910598755
Epoch 130, training loss: 14.998549461364746 = 1.7814414501190186 + 2.0 * 6.608553886413574
Epoch 130, val loss: 1.787825584411621
Epoch 140, training loss: 14.932979583740234 = 1.7699010372161865 + 2.0 * 6.581539154052734
Epoch 140, val loss: 1.7768291234970093
Epoch 150, training loss: 14.874762535095215 = 1.7574385404586792 + 2.0 * 6.558661937713623
Epoch 150, val loss: 1.7649999856948853
Epoch 160, training loss: 14.823348045349121 = 1.743744969367981 + 2.0 * 6.539801597595215
Epoch 160, val loss: 1.7521414756774902
Epoch 170, training loss: 14.77420425415039 = 1.728571891784668 + 2.0 * 6.522816181182861
Epoch 170, val loss: 1.7379945516586304
Epoch 180, training loss: 14.726423263549805 = 1.7115992307662964 + 2.0 * 6.507411956787109
Epoch 180, val loss: 1.7223968505859375
Epoch 190, training loss: 14.679265975952148 = 1.6924848556518555 + 2.0 * 6.4933905601501465
Epoch 190, val loss: 1.7049463987350464
Epoch 200, training loss: 14.636642456054688 = 1.67081880569458 + 2.0 * 6.482911586761475
Epoch 200, val loss: 1.6852748394012451
Epoch 210, training loss: 14.588467597961426 = 1.6464589834213257 + 2.0 * 6.471004486083984
Epoch 210, val loss: 1.663260579109192
Epoch 220, training loss: 14.542993545532227 = 1.6192950010299683 + 2.0 * 6.461849212646484
Epoch 220, val loss: 1.6388260126113892
Epoch 230, training loss: 14.493988990783691 = 1.5890552997589111 + 2.0 * 6.45246696472168
Epoch 230, val loss: 1.611730694770813
Epoch 240, training loss: 14.444982528686523 = 1.5554654598236084 + 2.0 * 6.444758415222168
Epoch 240, val loss: 1.581884503364563
Epoch 250, training loss: 14.39645004272461 = 1.5186543464660645 + 2.0 * 6.438897609710693
Epoch 250, val loss: 1.549498438835144
Epoch 260, training loss: 14.340946197509766 = 1.4790154695510864 + 2.0 * 6.430965423583984
Epoch 260, val loss: 1.5149917602539062
Epoch 270, training loss: 14.286870956420898 = 1.4367718696594238 + 2.0 * 6.425049781799316
Epoch 270, val loss: 1.478561282157898
Epoch 280, training loss: 14.234501838684082 = 1.392592191696167 + 2.0 * 6.420954704284668
Epoch 280, val loss: 1.440688133239746
Epoch 290, training loss: 14.17609691619873 = 1.3472316265106201 + 2.0 * 6.414432525634766
Epoch 290, val loss: 1.4023069143295288
Epoch 300, training loss: 14.118849754333496 = 1.3012362718582153 + 2.0 * 6.408806800842285
Epoch 300, val loss: 1.3637059926986694
Epoch 310, training loss: 14.064148902893066 = 1.2550562620162964 + 2.0 * 6.40454626083374
Epoch 310, val loss: 1.3253657817840576
Epoch 320, training loss: 14.009016036987305 = 1.2093011140823364 + 2.0 * 6.399857521057129
Epoch 320, val loss: 1.2877589464187622
Epoch 330, training loss: 13.959567070007324 = 1.1645680665969849 + 2.0 * 6.3974995613098145
Epoch 330, val loss: 1.2513844966888428
Epoch 340, training loss: 13.903990745544434 = 1.1211111545562744 + 2.0 * 6.391439914703369
Epoch 340, val loss: 1.2165993452072144
Epoch 350, training loss: 13.861486434936523 = 1.078995704650879 + 2.0 * 6.391245365142822
Epoch 350, val loss: 1.1833906173706055
Epoch 360, training loss: 13.809956550598145 = 1.0386078357696533 + 2.0 * 6.385674476623535
Epoch 360, val loss: 1.1521764993667603
Epoch 370, training loss: 13.765874862670898 = 1.0000247955322266 + 2.0 * 6.382925033569336
Epoch 370, val loss: 1.1228867769241333
Epoch 380, training loss: 13.716347694396973 = 0.9631568789482117 + 2.0 * 6.376595497131348
Epoch 380, val loss: 1.0955427885055542
Epoch 390, training loss: 13.67523193359375 = 0.9278852939605713 + 2.0 * 6.373673439025879
Epoch 390, val loss: 1.0699325799942017
Epoch 400, training loss: 13.64331340789795 = 0.8941792845726013 + 2.0 * 6.374567031860352
Epoch 400, val loss: 1.0461525917053223
Epoch 410, training loss: 13.600139617919922 = 0.8621892333030701 + 2.0 * 6.3689751625061035
Epoch 410, val loss: 1.0241212844848633
Epoch 420, training loss: 13.55901050567627 = 0.8317076563835144 + 2.0 * 6.363651275634766
Epoch 420, val loss: 1.0038429498672485
Epoch 430, training loss: 13.525803565979004 = 0.8025528192520142 + 2.0 * 6.3616251945495605
Epoch 430, val loss: 0.9850380420684814
Epoch 440, training loss: 13.49042797088623 = 0.774675726890564 + 2.0 * 6.357876300811768
Epoch 440, val loss: 0.9677908420562744
Epoch 450, training loss: 13.463966369628906 = 0.7480812668800354 + 2.0 * 6.357942581176758
Epoch 450, val loss: 0.951871395111084
Epoch 460, training loss: 13.426329612731934 = 0.7226353287696838 + 2.0 * 6.351847171783447
Epoch 460, val loss: 0.9372568726539612
Epoch 470, training loss: 13.401143074035645 = 0.69819575548172 + 2.0 * 6.351473808288574
Epoch 470, val loss: 0.9238199591636658
Epoch 480, training loss: 13.382052421569824 = 0.6746975779533386 + 2.0 * 6.353677272796631
Epoch 480, val loss: 0.9115974307060242
Epoch 490, training loss: 13.34409236907959 = 0.6523012518882751 + 2.0 * 6.345895767211914
Epoch 490, val loss: 0.9004607200622559
Epoch 500, training loss: 13.317452430725098 = 0.6306915283203125 + 2.0 * 6.343380451202393
Epoch 500, val loss: 0.8902619481086731
Epoch 510, training loss: 13.290882110595703 = 0.6097501516342163 + 2.0 * 6.340566158294678
Epoch 510, val loss: 0.8809747099876404
Epoch 520, training loss: 13.269932746887207 = 0.5894463658332825 + 2.0 * 6.340243339538574
Epoch 520, val loss: 0.8725159764289856
Epoch 530, training loss: 13.242801666259766 = 0.5698825716972351 + 2.0 * 6.336459636688232
Epoch 530, val loss: 0.8648830056190491
Epoch 540, training loss: 13.220004081726074 = 0.5508783459663391 + 2.0 * 6.3345627784729
Epoch 540, val loss: 0.8579780459403992
Epoch 550, training loss: 13.197115898132324 = 0.5323066711425781 + 2.0 * 6.332404613494873
Epoch 550, val loss: 0.8517388105392456
Epoch 560, training loss: 13.208725929260254 = 0.5141494274139404 + 2.0 * 6.347288131713867
Epoch 560, val loss: 0.8460365533828735
Epoch 570, training loss: 13.162354469299316 = 0.49648410081863403 + 2.0 * 6.332935333251953
Epoch 570, val loss: 0.8410865664482117
Epoch 580, training loss: 13.138528823852539 = 0.47926604747772217 + 2.0 * 6.329631328582764
Epoch 580, val loss: 0.8366626501083374
Epoch 590, training loss: 13.114141464233398 = 0.4623516798019409 + 2.0 * 6.325894832611084
Epoch 590, val loss: 0.8328016996383667
Epoch 600, training loss: 13.093856811523438 = 0.445694237947464 + 2.0 * 6.3240814208984375
Epoch 600, val loss: 0.8294446468353271
Epoch 610, training loss: 13.10376262664795 = 0.42928677797317505 + 2.0 * 6.33723783493042
Epoch 610, val loss: 0.8265925645828247
Epoch 620, training loss: 13.057526588439941 = 0.4132525622844696 + 2.0 * 6.322136878967285
Epoch 620, val loss: 0.8242135047912598
Epoch 630, training loss: 13.037832260131836 = 0.3975696563720703 + 2.0 * 6.320131301879883
Epoch 630, val loss: 0.8223049640655518
Epoch 640, training loss: 13.01941204071045 = 0.3821396231651306 + 2.0 * 6.318636417388916
Epoch 640, val loss: 0.8208796977996826
Epoch 650, training loss: 13.00171947479248 = 0.36695995926856995 + 2.0 * 6.317379951477051
Epoch 650, val loss: 0.8198997974395752
Epoch 660, training loss: 12.994723320007324 = 0.3520878851413727 + 2.0 * 6.321317672729492
Epoch 660, val loss: 0.819368302822113
Epoch 670, training loss: 12.973193168640137 = 0.3375999629497528 + 2.0 * 6.31779670715332
Epoch 670, val loss: 0.8192645311355591
Epoch 680, training loss: 12.952099800109863 = 0.32346558570861816 + 2.0 * 6.314317226409912
Epoch 680, val loss: 0.8195542097091675
Epoch 690, training loss: 12.936903953552246 = 0.30968090891838074 + 2.0 * 6.3136115074157715
Epoch 690, val loss: 0.8202763199806213
Epoch 700, training loss: 12.919477462768555 = 0.2962946593761444 + 2.0 * 6.311591625213623
Epoch 700, val loss: 0.8214519023895264
Epoch 710, training loss: 12.9074068069458 = 0.2833152413368225 + 2.0 * 6.312045574188232
Epoch 710, val loss: 0.823087751865387
Epoch 720, training loss: 12.894558906555176 = 0.27081063389778137 + 2.0 * 6.311873912811279
Epoch 720, val loss: 0.8250497579574585
Epoch 730, training loss: 12.877435684204102 = 0.25878599286079407 + 2.0 * 6.309324741363525
Epoch 730, val loss: 0.827538251876831
Epoch 740, training loss: 12.860879898071289 = 0.2472241073846817 + 2.0 * 6.306828022003174
Epoch 740, val loss: 0.8304160833358765
Epoch 750, training loss: 12.84731674194336 = 0.23609299957752228 + 2.0 * 6.305612087249756
Epoch 750, val loss: 0.8336586356163025
Epoch 760, training loss: 12.856585502624512 = 0.2254059910774231 + 2.0 * 6.315589904785156
Epoch 760, val loss: 0.8373530507087708
Epoch 770, training loss: 12.826515197753906 = 0.21526575088500977 + 2.0 * 6.305624485015869
Epoch 770, val loss: 0.8413814306259155
Epoch 780, training loss: 12.810567855834961 = 0.20563310384750366 + 2.0 * 6.302467346191406
Epoch 780, val loss: 0.845815122127533
Epoch 790, training loss: 12.798601150512695 = 0.19643625617027283 + 2.0 * 6.301082611083984
Epoch 790, val loss: 0.8506669998168945
Epoch 800, training loss: 12.78862476348877 = 0.1876557618379593 + 2.0 * 6.300484657287598
Epoch 800, val loss: 0.855905294418335
Epoch 810, training loss: 12.795624732971191 = 0.1792781502008438 + 2.0 * 6.308173179626465
Epoch 810, val loss: 0.8614722490310669
Epoch 820, training loss: 12.769304275512695 = 0.1713014543056488 + 2.0 * 6.299001216888428
Epoch 820, val loss: 0.8673139810562134
Epoch 830, training loss: 12.778609275817871 = 0.16373975574970245 + 2.0 * 6.307434558868408
Epoch 830, val loss: 0.8734756708145142
Epoch 840, training loss: 12.754973411560059 = 0.1565658152103424 + 2.0 * 6.299203872680664
Epoch 840, val loss: 0.8799580335617065
Epoch 850, training loss: 12.743639945983887 = 0.14973759651184082 + 2.0 * 6.2969512939453125
Epoch 850, val loss: 0.8867455124855042
Epoch 860, training loss: 12.733354568481445 = 0.14322024583816528 + 2.0 * 6.295067310333252
Epoch 860, val loss: 0.8937250375747681
Epoch 870, training loss: 12.730731010437012 = 0.13700436055660248 + 2.0 * 6.296863555908203
Epoch 870, val loss: 0.9009644389152527
Epoch 880, training loss: 12.725913047790527 = 0.13113351166248322 + 2.0 * 6.297389984130859
Epoch 880, val loss: 0.9082708358764648
Epoch 890, training loss: 12.711714744567871 = 0.12556074559688568 + 2.0 * 6.293076992034912
Epoch 890, val loss: 0.9159252643585205
Epoch 900, training loss: 12.705489158630371 = 0.12028242647647858 + 2.0 * 6.292603492736816
Epoch 900, val loss: 0.9237185716629028
Epoch 910, training loss: 12.697183609008789 = 0.11527324467897415 + 2.0 * 6.290955066680908
Epoch 910, val loss: 0.9316388368606567
Epoch 920, training loss: 12.706145286560059 = 0.11050909757614136 + 2.0 * 6.297818183898926
Epoch 920, val loss: 0.9396841526031494
Epoch 930, training loss: 12.693016052246094 = 0.10602036118507385 + 2.0 * 6.2934980392456055
Epoch 930, val loss: 0.9479256868362427
Epoch 940, training loss: 12.678260803222656 = 0.10177819430828094 + 2.0 * 6.288241386413574
Epoch 940, val loss: 0.9562660455703735
Epoch 950, training loss: 12.675402641296387 = 0.09774378687143326 + 2.0 * 6.288829326629639
Epoch 950, val loss: 0.96468585729599
Epoch 960, training loss: 12.671011924743652 = 0.09391079097986221 + 2.0 * 6.28855037689209
Epoch 960, val loss: 0.9731361865997314
Epoch 970, training loss: 12.66618537902832 = 0.09026875346899033 + 2.0 * 6.287958145141602
Epoch 970, val loss: 0.9816242456436157
Epoch 980, training loss: 12.658346176147461 = 0.08680848777294159 + 2.0 * 6.285768985748291
Epoch 980, val loss: 0.9902061223983765
Epoch 990, training loss: 12.66341495513916 = 0.0835256427526474 + 2.0 * 6.289944648742676
Epoch 990, val loss: 0.9987310171127319
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7518518518518519
0.8033737480231946
=== training gcn model ===
Epoch 0, training loss: 19.145496368408203 = 1.951786756515503 + 2.0 * 8.596855163574219
Epoch 0, val loss: 1.9555261135101318
Epoch 10, training loss: 19.135244369506836 = 1.9420924186706543 + 2.0 * 8.596575736999512
Epoch 10, val loss: 1.945159912109375
Epoch 20, training loss: 19.11855697631836 = 1.9301977157592773 + 2.0 * 8.5941801071167
Epoch 20, val loss: 1.9324002265930176
Epoch 30, training loss: 19.059776306152344 = 1.9138946533203125 + 2.0 * 8.572940826416016
Epoch 30, val loss: 1.9147754907608032
Epoch 40, training loss: 18.69430160522461 = 1.8938357830047607 + 2.0 * 8.400233268737793
Epoch 40, val loss: 1.8936538696289062
Epoch 50, training loss: 17.646638870239258 = 1.8747103214263916 + 2.0 * 7.885964393615723
Epoch 50, val loss: 1.8747516870498657
Epoch 60, training loss: 16.557331085205078 = 1.861450433731079 + 2.0 * 7.347940444946289
Epoch 60, val loss: 1.8619242906570435
Epoch 70, training loss: 16.06849479675293 = 1.8499619960784912 + 2.0 * 7.109266757965088
Epoch 70, val loss: 1.8503015041351318
Epoch 80, training loss: 15.720261573791504 = 1.8373944759368896 + 2.0 * 6.941433429718018
Epoch 80, val loss: 1.837592601776123
Epoch 90, training loss: 15.489167213439941 = 1.824947476387024 + 2.0 * 6.8321099281311035
Epoch 90, val loss: 1.8252683877944946
Epoch 100, training loss: 15.326873779296875 = 1.8127875328063965 + 2.0 * 6.75704288482666
Epoch 100, val loss: 1.81338369846344
Epoch 110, training loss: 15.196623802185059 = 1.8014296293258667 + 2.0 * 6.697597026824951
Epoch 110, val loss: 1.8023242950439453
Epoch 120, training loss: 15.096709251403809 = 1.7907497882843018 + 2.0 * 6.652979850769043
Epoch 120, val loss: 1.791894793510437
Epoch 130, training loss: 15.013509750366211 = 1.779922604560852 + 2.0 * 6.616793632507324
Epoch 130, val loss: 1.7814489603042603
Epoch 140, training loss: 14.953717231750488 = 1.7686798572540283 + 2.0 * 6.5925188064575195
Epoch 140, val loss: 1.7708580493927002
Epoch 150, training loss: 14.889459609985352 = 1.756898283958435 + 2.0 * 6.566280841827393
Epoch 150, val loss: 1.760055661201477
Epoch 160, training loss: 14.835745811462402 = 1.744246482849121 + 2.0 * 6.545749664306641
Epoch 160, val loss: 1.7488282918930054
Epoch 170, training loss: 14.794888496398926 = 1.7303316593170166 + 2.0 * 6.532278537750244
Epoch 170, val loss: 1.7367686033248901
Epoch 180, training loss: 14.739593505859375 = 1.7147225141525269 + 2.0 * 6.512435436248779
Epoch 180, val loss: 1.723567008972168
Epoch 190, training loss: 14.695457458496094 = 1.6971888542175293 + 2.0 * 6.499134540557861
Epoch 190, val loss: 1.7088818550109863
Epoch 200, training loss: 14.64940071105957 = 1.6773762702941895 + 2.0 * 6.486011981964111
Epoch 200, val loss: 1.6924517154693604
Epoch 210, training loss: 14.604698181152344 = 1.655219316482544 + 2.0 * 6.4747395515441895
Epoch 210, val loss: 1.6742823123931885
Epoch 220, training loss: 14.559173583984375 = 1.6303447484970093 + 2.0 * 6.464414596557617
Epoch 220, val loss: 1.6540125608444214
Epoch 230, training loss: 14.516451835632324 = 1.6026667356491089 + 2.0 * 6.456892490386963
Epoch 230, val loss: 1.6316691637039185
Epoch 240, training loss: 14.46727466583252 = 1.5722814798355103 + 2.0 * 6.44749641418457
Epoch 240, val loss: 1.6072707176208496
Epoch 250, training loss: 14.416570663452148 = 1.5389140844345093 + 2.0 * 6.438828468322754
Epoch 250, val loss: 1.5806121826171875
Epoch 260, training loss: 14.375138282775879 = 1.5023047924041748 + 2.0 * 6.4364166259765625
Epoch 260, val loss: 1.5515799522399902
Epoch 270, training loss: 14.312294006347656 = 1.4626423120498657 + 2.0 * 6.424825668334961
Epoch 270, val loss: 1.5203146934509277
Epoch 280, training loss: 14.257875442504883 = 1.4198517799377441 + 2.0 * 6.41901159286499
Epoch 280, val loss: 1.4867157936096191
Epoch 290, training loss: 14.201905250549316 = 1.3740965127944946 + 2.0 * 6.413904190063477
Epoch 290, val loss: 1.4509862661361694
Epoch 300, training loss: 14.146949768066406 = 1.325989842414856 + 2.0 * 6.41048002243042
Epoch 300, val loss: 1.4138340950012207
Epoch 310, training loss: 14.088361740112305 = 1.276772379875183 + 2.0 * 6.405794620513916
Epoch 310, val loss: 1.375943899154663
Epoch 320, training loss: 14.027355194091797 = 1.2267138957977295 + 2.0 * 6.400320529937744
Epoch 320, val loss: 1.3377419710159302
Epoch 330, training loss: 13.974913597106934 = 1.1764403581619263 + 2.0 * 6.399236679077148
Epoch 330, val loss: 1.2998682260513306
Epoch 340, training loss: 13.911821365356445 = 1.1272248029708862 + 2.0 * 6.392298221588135
Epoch 340, val loss: 1.2629879713058472
Epoch 350, training loss: 13.855363845825195 = 1.0791343450546265 + 2.0 * 6.388114929199219
Epoch 350, val loss: 1.2275571823120117
Epoch 360, training loss: 13.804730415344238 = 1.032677412033081 + 2.0 * 6.386026382446289
Epoch 360, val loss: 1.1940126419067383
Epoch 370, training loss: 13.752321243286133 = 0.9884695410728455 + 2.0 * 6.3819260597229
Epoch 370, val loss: 1.1628828048706055
Epoch 380, training loss: 13.703964233398438 = 0.9472321271896362 + 2.0 * 6.378365993499756
Epoch 380, val loss: 1.1342304944992065
Epoch 390, training loss: 13.6658353805542 = 0.9083548188209534 + 2.0 * 6.378740310668945
Epoch 390, val loss: 1.1081477403640747
Epoch 400, training loss: 13.61947250366211 = 0.8721380829811096 + 2.0 * 6.373667240142822
Epoch 400, val loss: 1.0844801664352417
Epoch 410, training loss: 13.576095581054688 = 0.8384362459182739 + 2.0 * 6.368829727172852
Epoch 410, val loss: 1.0631760358810425
Epoch 420, training loss: 13.547428131103516 = 0.807012140750885 + 2.0 * 6.370207786560059
Epoch 420, val loss: 1.043915033340454
Epoch 430, training loss: 13.510625839233398 = 0.7776495814323425 + 2.0 * 6.366487979888916
Epoch 430, val loss: 1.0266975164413452
Epoch 440, training loss: 13.470612525939941 = 0.7503455281257629 + 2.0 * 6.360133647918701
Epoch 440, val loss: 1.0112330913543701
Epoch 450, training loss: 13.439974784851074 = 0.7246949672698975 + 2.0 * 6.357639789581299
Epoch 450, val loss: 0.9974435567855835
Epoch 460, training loss: 13.414231300354004 = 0.700527012348175 + 2.0 * 6.356852054595947
Epoch 460, val loss: 0.98515784740448
Epoch 470, training loss: 13.38414192199707 = 0.6777470707893372 + 2.0 * 6.3531975746154785
Epoch 470, val loss: 0.9741275310516357
Epoch 480, training loss: 13.357754707336426 = 0.6560705900192261 + 2.0 * 6.350841999053955
Epoch 480, val loss: 0.9642559885978699
Epoch 490, training loss: 13.338766098022461 = 0.635373055934906 + 2.0 * 6.351696491241455
Epoch 490, val loss: 0.9554058909416199
Epoch 500, training loss: 13.30866813659668 = 0.6156290173530579 + 2.0 * 6.346519470214844
Epoch 500, val loss: 0.9475273489952087
Epoch 510, training loss: 13.281037330627441 = 0.5966271162033081 + 2.0 * 6.342205047607422
Epoch 510, val loss: 0.9405662417411804
Epoch 520, training loss: 13.258005142211914 = 0.5781418085098267 + 2.0 * 6.339931488037109
Epoch 520, val loss: 0.9343527555465698
Epoch 530, training loss: 13.266961097717285 = 0.5600355267524719 + 2.0 * 6.3534626960754395
Epoch 530, val loss: 0.9288197159767151
Epoch 540, training loss: 13.226316452026367 = 0.5426274538040161 + 2.0 * 6.34184455871582
Epoch 540, val loss: 0.9240162968635559
Epoch 550, training loss: 13.194960594177246 = 0.5255119204521179 + 2.0 * 6.334724426269531
Epoch 550, val loss: 0.9196890592575073
Epoch 560, training loss: 13.171951293945312 = 0.5086116790771484 + 2.0 * 6.331669807434082
Epoch 560, val loss: 0.916082501411438
Epoch 570, training loss: 13.150675773620605 = 0.4918440878391266 + 2.0 * 6.329415798187256
Epoch 570, val loss: 0.9128116965293884
Epoch 580, training loss: 13.136775970458984 = 0.4751366972923279 + 2.0 * 6.330819606781006
Epoch 580, val loss: 0.9099757075309753
Epoch 590, training loss: 13.1240816116333 = 0.45854446291923523 + 2.0 * 6.332768440246582
Epoch 590, val loss: 0.9077295064926147
Epoch 600, training loss: 13.091951370239258 = 0.44217073917388916 + 2.0 * 6.32489013671875
Epoch 600, val loss: 0.9056865572929382
Epoch 610, training loss: 13.084724426269531 = 0.4258430600166321 + 2.0 * 6.329440593719482
Epoch 610, val loss: 0.9041655659675598
Epoch 620, training loss: 13.059328079223633 = 0.40974244475364685 + 2.0 * 6.324792861938477
Epoch 620, val loss: 0.9034355878829956
Epoch 630, training loss: 13.034886360168457 = 0.39378225803375244 + 2.0 * 6.320551872253418
Epoch 630, val loss: 0.9028482437133789
Epoch 640, training loss: 13.014296531677246 = 0.3779996931552887 + 2.0 * 6.318148612976074
Epoch 640, val loss: 0.9028798937797546
Epoch 650, training loss: 12.999724388122559 = 0.3624512255191803 + 2.0 * 6.318636417388916
Epoch 650, val loss: 0.9034225940704346
Epoch 660, training loss: 12.980245590209961 = 0.34719860553741455 + 2.0 * 6.316523551940918
Epoch 660, val loss: 0.9043791890144348
Epoch 670, training loss: 12.964710235595703 = 0.3324490189552307 + 2.0 * 6.316130638122559
Epoch 670, val loss: 0.9057676792144775
Epoch 680, training loss: 12.944870948791504 = 0.31806254386901855 + 2.0 * 6.313404083251953
Epoch 680, val loss: 0.9078569412231445
Epoch 690, training loss: 12.950827598571777 = 0.30407634377479553 + 2.0 * 6.323375701904297
Epoch 690, val loss: 0.9103865027427673
Epoch 700, training loss: 12.917301177978516 = 0.2906852960586548 + 2.0 * 6.313307762145996
Epoch 700, val loss: 0.9134127497673035
Epoch 710, training loss: 12.897302627563477 = 0.27779054641723633 + 2.0 * 6.309756278991699
Epoch 710, val loss: 0.9170666337013245
Epoch 720, training loss: 12.881335258483887 = 0.2654086649417877 + 2.0 * 6.3079633712768555
Epoch 720, val loss: 0.9212049841880798
Epoch 730, training loss: 12.882009506225586 = 0.2535393536090851 + 2.0 * 6.314235210418701
Epoch 730, val loss: 0.9258028268814087
Epoch 740, training loss: 12.861109733581543 = 0.24220244586467743 + 2.0 * 6.30945348739624
Epoch 740, val loss: 0.9308901429176331
Epoch 750, training loss: 12.845220565795898 = 0.23141509294509888 + 2.0 * 6.306902885437012
Epoch 750, val loss: 0.9363903999328613
Epoch 760, training loss: 12.833558082580566 = 0.2211374193429947 + 2.0 * 6.306210517883301
Epoch 760, val loss: 0.9423568248748779
Epoch 770, training loss: 12.818937301635742 = 0.2113688588142395 + 2.0 * 6.303784370422363
Epoch 770, val loss: 0.9485942721366882
Epoch 780, training loss: 12.80608081817627 = 0.20210035145282745 + 2.0 * 6.301990032196045
Epoch 780, val loss: 0.9553648829460144
Epoch 790, training loss: 12.805416107177734 = 0.19324827194213867 + 2.0 * 6.306083679199219
Epoch 790, val loss: 0.9623823165893555
Epoch 800, training loss: 12.787826538085938 = 0.18490435183048248 + 2.0 * 6.301461219787598
Epoch 800, val loss: 0.969780683517456
Epoch 810, training loss: 12.77804946899414 = 0.17694813013076782 + 2.0 * 6.30055046081543
Epoch 810, val loss: 0.9771820902824402
Epoch 820, training loss: 12.768282890319824 = 0.16943329572677612 + 2.0 * 6.299424648284912
Epoch 820, val loss: 0.9849992394447327
Epoch 830, training loss: 12.762048721313477 = 0.1622588038444519 + 2.0 * 6.2998948097229
Epoch 830, val loss: 0.9929913282394409
Epoch 840, training loss: 12.752461433410645 = 0.15546110272407532 + 2.0 * 6.298500061035156
Epoch 840, val loss: 1.0012173652648926
Epoch 850, training loss: 12.739592552185059 = 0.14898593723773956 + 2.0 * 6.2953033447265625
Epoch 850, val loss: 1.0094799995422363
Epoch 860, training loss: 12.749876022338867 = 0.14285054802894592 + 2.0 * 6.3035125732421875
Epoch 860, val loss: 1.0179635286331177
Epoch 870, training loss: 12.72846508026123 = 0.13699118793010712 + 2.0 * 6.295736789703369
Epoch 870, val loss: 1.0263011455535889
Epoch 880, training loss: 12.714553833007812 = 0.13143891096115112 + 2.0 * 6.291557312011719
Epoch 880, val loss: 1.0351587533950806
Epoch 890, training loss: 12.70649528503418 = 0.12614163756370544 + 2.0 * 6.290176868438721
Epoch 890, val loss: 1.04405677318573
Epoch 900, training loss: 12.717423439025879 = 0.12107711285352707 + 2.0 * 6.298172950744629
Epoch 900, val loss: 1.053078293800354
Epoch 910, training loss: 12.709218978881836 = 0.11626675724983215 + 2.0 * 6.296475887298584
Epoch 910, val loss: 1.0619816780090332
Epoch 920, training loss: 12.689229965209961 = 0.11169317364692688 + 2.0 * 6.288768291473389
Epoch 920, val loss: 1.0709819793701172
Epoch 930, training loss: 12.681843757629395 = 0.10733692348003387 + 2.0 * 6.287253379821777
Epoch 930, val loss: 1.0802884101867676
Epoch 940, training loss: 12.675979614257812 = 0.10316991806030273 + 2.0 * 6.286405086517334
Epoch 940, val loss: 1.089594841003418
Epoch 950, training loss: 12.705370903015137 = 0.09921012073755264 + 2.0 * 6.3030805587768555
Epoch 950, val loss: 1.0990346670150757
Epoch 960, training loss: 12.665006637573242 = 0.09539493918418884 + 2.0 * 6.284805774688721
Epoch 960, val loss: 1.1081491708755493
Epoch 970, training loss: 12.661955833435059 = 0.09178663790225983 + 2.0 * 6.2850847244262695
Epoch 970, val loss: 1.1174253225326538
Epoch 980, training loss: 12.65512466430664 = 0.08834357559680939 + 2.0 * 6.283390522003174
Epoch 980, val loss: 1.1270694732666016
Epoch 990, training loss: 12.652178764343262 = 0.0850372314453125 + 2.0 * 6.283570766448975
Epoch 990, val loss: 1.1366076469421387
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7074074074074075
0.8070637849235636
The final CL Acc:0.73827, 0.02188, The final GNN Acc:0.80917, 0.00579
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13216])
remove edge: torch.Size([2, 7896])
updated graph: torch.Size([2, 10556])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 19.127077102661133 = 1.9334633350372314 + 2.0 * 8.596806526184082
Epoch 0, val loss: 1.9214462041854858
Epoch 10, training loss: 19.115676879882812 = 1.9228143692016602 + 2.0 * 8.596431732177734
Epoch 10, val loss: 1.9112236499786377
Epoch 20, training loss: 19.096832275390625 = 1.9096128940582275 + 2.0 * 8.593609809875488
Epoch 20, val loss: 1.8981664180755615
Epoch 30, training loss: 19.03797721862793 = 1.8916207551956177 + 2.0 * 8.5731782913208
Epoch 30, val loss: 1.8800665140151978
Epoch 40, training loss: 18.782638549804688 = 1.8697516918182373 + 2.0 * 8.456443786621094
Epoch 40, val loss: 1.858886480331421
Epoch 50, training loss: 17.716768264770508 = 1.8481107950210571 + 2.0 * 7.934329032897949
Epoch 50, val loss: 1.838598370552063
Epoch 60, training loss: 16.86398696899414 = 1.8298777341842651 + 2.0 * 7.517054557800293
Epoch 60, val loss: 1.8222042322158813
Epoch 70, training loss: 16.33128547668457 = 1.8148157596588135 + 2.0 * 7.258234977722168
Epoch 70, val loss: 1.8086894750595093
Epoch 80, training loss: 15.976164817810059 = 1.800416111946106 + 2.0 * 7.087874412536621
Epoch 80, val loss: 1.7958686351776123
Epoch 90, training loss: 15.719169616699219 = 1.7873821258544922 + 2.0 * 6.965893745422363
Epoch 90, val loss: 1.7845426797866821
Epoch 100, training loss: 15.508654594421387 = 1.7751365900039673 + 2.0 * 6.866758823394775
Epoch 100, val loss: 1.7739570140838623
Epoch 110, training loss: 15.346159934997559 = 1.7630974054336548 + 2.0 * 6.791531085968018
Epoch 110, val loss: 1.763853669166565
Epoch 120, training loss: 15.245736122131348 = 1.7498193979263306 + 2.0 * 6.747958183288574
Epoch 120, val loss: 1.7526521682739258
Epoch 130, training loss: 15.152961730957031 = 1.734601378440857 + 2.0 * 6.7091803550720215
Epoch 130, val loss: 1.7395724058151245
Epoch 140, training loss: 15.058900833129883 = 1.7183971405029297 + 2.0 * 6.670251846313477
Epoch 140, val loss: 1.7254196405410767
Epoch 150, training loss: 14.96754264831543 = 1.700868010520935 + 2.0 * 6.633337497711182
Epoch 150, val loss: 1.7100058794021606
Epoch 160, training loss: 14.883559226989746 = 1.6810779571533203 + 2.0 * 6.601240634918213
Epoch 160, val loss: 1.6926225423812866
Epoch 170, training loss: 14.805933952331543 = 1.6584055423736572 + 2.0 * 6.573764324188232
Epoch 170, val loss: 1.6727070808410645
Epoch 180, training loss: 14.737198829650879 = 1.6323260068893433 + 2.0 * 6.552436351776123
Epoch 180, val loss: 1.6498051881790161
Epoch 190, training loss: 14.6663818359375 = 1.6029225587844849 + 2.0 * 6.531729698181152
Epoch 190, val loss: 1.6241976022720337
Epoch 200, training loss: 14.598949432373047 = 1.5701279640197754 + 2.0 * 6.514410972595215
Epoch 200, val loss: 1.595739483833313
Epoch 210, training loss: 14.534086227416992 = 1.5337868928909302 + 2.0 * 6.500149726867676
Epoch 210, val loss: 1.564393162727356
Epoch 220, training loss: 14.46890926361084 = 1.4945443868637085 + 2.0 * 6.4871826171875
Epoch 220, val loss: 1.5308412313461304
Epoch 230, training loss: 14.40304183959961 = 1.4532219171524048 + 2.0 * 6.474909782409668
Epoch 230, val loss: 1.4955430030822754
Epoch 240, training loss: 14.335902214050293 = 1.4099515676498413 + 2.0 * 6.46297550201416
Epoch 240, val loss: 1.4587544202804565
Epoch 250, training loss: 14.270559310913086 = 1.3649731874465942 + 2.0 * 6.452793121337891
Epoch 250, val loss: 1.4208143949508667
Epoch 260, training loss: 14.21393871307373 = 1.3188602924346924 + 2.0 * 6.447539329528809
Epoch 260, val loss: 1.382128119468689
Epoch 270, training loss: 14.144899368286133 = 1.2732899188995361 + 2.0 * 6.435804843902588
Epoch 270, val loss: 1.3442504405975342
Epoch 280, training loss: 14.085821151733398 = 1.2284079790115356 + 2.0 * 6.428706645965576
Epoch 280, val loss: 1.30728280544281
Epoch 290, training loss: 14.031628608703613 = 1.1849733591079712 + 2.0 * 6.423327445983887
Epoch 290, val loss: 1.271884560585022
Epoch 300, training loss: 13.973017692565918 = 1.1431931257247925 + 2.0 * 6.414912223815918
Epoch 300, val loss: 1.2382798194885254
Epoch 310, training loss: 13.919318199157715 = 1.102815866470337 + 2.0 * 6.4082512855529785
Epoch 310, val loss: 1.2061556577682495
Epoch 320, training loss: 13.86838150024414 = 1.0636988878250122 + 2.0 * 6.402341365814209
Epoch 320, val loss: 1.17549467086792
Epoch 330, training loss: 13.836201667785645 = 1.0260034799575806 + 2.0 * 6.405098915100098
Epoch 330, val loss: 1.1464905738830566
Epoch 340, training loss: 13.779289245605469 = 0.9902589917182922 + 2.0 * 6.394515037536621
Epoch 340, val loss: 1.1192392110824585
Epoch 350, training loss: 13.733333587646484 = 0.9555583000183105 + 2.0 * 6.388887882232666
Epoch 350, val loss: 1.0932358503341675
Epoch 360, training loss: 13.693244934082031 = 0.9217039942741394 + 2.0 * 6.385770320892334
Epoch 360, val loss: 1.0680781602859497
Epoch 370, training loss: 13.653583526611328 = 0.8888497948646545 + 2.0 * 6.38236665725708
Epoch 370, val loss: 1.0438176393508911
Epoch 380, training loss: 13.610429763793945 = 0.856505811214447 + 2.0 * 6.376962184906006
Epoch 380, val loss: 1.020358920097351
Epoch 390, training loss: 13.572234153747559 = 0.824614942073822 + 2.0 * 6.373809814453125
Epoch 390, val loss: 0.9974102973937988
Epoch 400, training loss: 13.53573989868164 = 0.793197751045227 + 2.0 * 6.371271133422852
Epoch 400, val loss: 0.974938154220581
Epoch 410, training loss: 13.498029708862305 = 0.762458324432373 + 2.0 * 6.367785453796387
Epoch 410, val loss: 0.9531542658805847
Epoch 420, training loss: 13.461383819580078 = 0.7322903275489807 + 2.0 * 6.364546775817871
Epoch 420, val loss: 0.9319598078727722
Epoch 430, training loss: 13.42613697052002 = 0.7026934027671814 + 2.0 * 6.361721992492676
Epoch 430, val loss: 0.9112495183944702
Epoch 440, training loss: 13.392641067504883 = 0.673851728439331 + 2.0 * 6.359394550323486
Epoch 440, val loss: 0.8913388848304749
Epoch 450, training loss: 13.359840393066406 = 0.6462069749832153 + 2.0 * 6.35681676864624
Epoch 450, val loss: 0.8723652958869934
Epoch 460, training loss: 13.32844352722168 = 0.6195428371429443 + 2.0 * 6.354450225830078
Epoch 460, val loss: 0.8543703556060791
Epoch 470, training loss: 13.299068450927734 = 0.5939567685127258 + 2.0 * 6.352555751800537
Epoch 470, val loss: 0.8372987508773804
Epoch 480, training loss: 13.270575523376465 = 0.5695233941078186 + 2.0 * 6.350525856018066
Epoch 480, val loss: 0.8214489221572876
Epoch 490, training loss: 13.241218566894531 = 0.5461690425872803 + 2.0 * 6.347524642944336
Epoch 490, val loss: 0.8066561818122864
Epoch 500, training loss: 13.213415145874023 = 0.5238410830497742 + 2.0 * 6.344787120819092
Epoch 500, val loss: 0.7930126786231995
Epoch 510, training loss: 13.193589210510254 = 0.502593457698822 + 2.0 * 6.345498085021973
Epoch 510, val loss: 0.7804290056228638
Epoch 520, training loss: 13.163387298583984 = 0.4822635352611542 + 2.0 * 6.340561866760254
Epoch 520, val loss: 0.7688547372817993
Epoch 530, training loss: 13.141236305236816 = 0.46274980902671814 + 2.0 * 6.339243412017822
Epoch 530, val loss: 0.7581895589828491
Epoch 540, training loss: 13.120451927185059 = 0.4438875913619995 + 2.0 * 6.338282108306885
Epoch 540, val loss: 0.7482948899269104
Epoch 550, training loss: 13.103620529174805 = 0.4255870282649994 + 2.0 * 6.339016914367676
Epoch 550, val loss: 0.7390037178993225
Epoch 560, training loss: 13.075949668884277 = 0.4078902006149292 + 2.0 * 6.334029674530029
Epoch 560, val loss: 0.7304292917251587
Epoch 570, training loss: 13.05599594116211 = 0.3906128704547882 + 2.0 * 6.332691669464111
Epoch 570, val loss: 0.7223839163780212
Epoch 580, training loss: 13.036259651184082 = 0.3736766278743744 + 2.0 * 6.331291675567627
Epoch 580, val loss: 0.7147966027259827
Epoch 590, training loss: 13.015244483947754 = 0.3571542501449585 + 2.0 * 6.329045295715332
Epoch 590, val loss: 0.707648754119873
Epoch 600, training loss: 12.99731731414795 = 0.3409687280654907 + 2.0 * 6.328174114227295
Epoch 600, val loss: 0.7010174989700317
Epoch 610, training loss: 12.974884986877441 = 0.3252561390399933 + 2.0 * 6.324814319610596
Epoch 610, val loss: 0.6948146820068359
Epoch 620, training loss: 12.956192016601562 = 0.30994170904159546 + 2.0 * 6.32312536239624
Epoch 620, val loss: 0.6891185641288757
Epoch 630, training loss: 12.951111793518066 = 0.2951596677303314 + 2.0 * 6.327976226806641
Epoch 630, val loss: 0.6839989423751831
Epoch 640, training loss: 12.92138385772705 = 0.2809440493583679 + 2.0 * 6.320219993591309
Epoch 640, val loss: 0.6795333623886108
Epoch 650, training loss: 12.90994930267334 = 0.2673453092575073 + 2.0 * 6.3213019371032715
Epoch 650, val loss: 0.6757350564002991
Epoch 660, training loss: 12.893501281738281 = 0.2544320523738861 + 2.0 * 6.319534778594971
Epoch 660, val loss: 0.6725001335144043
Epoch 670, training loss: 12.875130653381348 = 0.24212826788425446 + 2.0 * 6.316501140594482
Epoch 670, val loss: 0.6699119210243225
Epoch 680, training loss: 12.86008358001709 = 0.2303985059261322 + 2.0 * 6.314842700958252
Epoch 680, val loss: 0.6679649949073792
Epoch 690, training loss: 12.86505126953125 = 0.21921075880527496 + 2.0 * 6.322920322418213
Epoch 690, val loss: 0.6666058897972107
Epoch 700, training loss: 12.837271690368652 = 0.2087022215127945 + 2.0 * 6.314284801483154
Epoch 700, val loss: 0.6658129692077637
Epoch 710, training loss: 12.82185173034668 = 0.1987246721982956 + 2.0 * 6.311563491821289
Epoch 710, val loss: 0.6655932664871216
Epoch 720, training loss: 12.809419631958008 = 0.18924608826637268 + 2.0 * 6.310086727142334
Epoch 720, val loss: 0.665935218334198
Epoch 730, training loss: 12.805049896240234 = 0.1802322417497635 + 2.0 * 6.312408924102783
Epoch 730, val loss: 0.6667622923851013
Epoch 740, training loss: 12.790307998657227 = 0.17171551287174225 + 2.0 * 6.309296131134033
Epoch 740, val loss: 0.6680446863174438
Epoch 750, training loss: 12.778560638427734 = 0.1636362224817276 + 2.0 * 6.307462215423584
Epoch 750, val loss: 0.6698033809661865
Epoch 760, training loss: 12.776504516601562 = 0.1559837907552719 + 2.0 * 6.31026029586792
Epoch 760, val loss: 0.6719689965248108
Epoch 770, training loss: 12.760676383972168 = 0.1487625241279602 + 2.0 * 6.305956840515137
Epoch 770, val loss: 0.6745083928108215
Epoch 780, training loss: 12.752633094787598 = 0.1418962925672531 + 2.0 * 6.305368423461914
Epoch 780, val loss: 0.6772825121879578
Epoch 790, training loss: 12.74073600769043 = 0.13541311025619507 + 2.0 * 6.302661418914795
Epoch 790, val loss: 0.6803922653198242
Epoch 800, training loss: 12.730674743652344 = 0.12926389276981354 + 2.0 * 6.300705432891846
Epoch 800, val loss: 0.6837592720985413
Epoch 810, training loss: 12.729808807373047 = 0.12342415750026703 + 2.0 * 6.303192138671875
Epoch 810, val loss: 0.6874191761016846
Epoch 820, training loss: 12.72087287902832 = 0.11791662871837616 + 2.0 * 6.301477909088135
Epoch 820, val loss: 0.6913018822669983
Epoch 830, training loss: 12.708613395690918 = 0.11268483102321625 + 2.0 * 6.297964096069336
Epoch 830, val loss: 0.6953727602958679
Epoch 840, training loss: 12.71044635772705 = 0.1077374592423439 + 2.0 * 6.30135440826416
Epoch 840, val loss: 0.6996914744377136
Epoch 850, training loss: 12.696831703186035 = 0.10303616523742676 + 2.0 * 6.296897888183594
Epoch 850, val loss: 0.7040879130363464
Epoch 860, training loss: 12.689868927001953 = 0.09860078990459442 + 2.0 * 6.2956342697143555
Epoch 860, val loss: 0.7086645364761353
Epoch 870, training loss: 12.696016311645508 = 0.09438641369342804 + 2.0 * 6.300815105438232
Epoch 870, val loss: 0.7133931517601013
Epoch 880, training loss: 12.685410499572754 = 0.09040292352437973 + 2.0 * 6.29750394821167
Epoch 880, val loss: 0.7181047201156616
Epoch 890, training loss: 12.672639846801758 = 0.08664267510175705 + 2.0 * 6.292998790740967
Epoch 890, val loss: 0.7229423522949219
Epoch 900, training loss: 12.672226905822754 = 0.08307188004255295 + 2.0 * 6.294577598571777
Epoch 900, val loss: 0.727920651435852
Epoch 910, training loss: 12.661267280578613 = 0.07968553900718689 + 2.0 * 6.290791034698486
Epoch 910, val loss: 0.7329158186912537
Epoch 920, training loss: 12.659082412719727 = 0.07647009193897247 + 2.0 * 6.291306018829346
Epoch 920, val loss: 0.7379830479621887
Epoch 930, training loss: 12.656505584716797 = 0.07342629134654999 + 2.0 * 6.291539669036865
Epoch 930, val loss: 0.7430439591407776
Epoch 940, training loss: 12.647268295288086 = 0.07053671032190323 + 2.0 * 6.288365840911865
Epoch 940, val loss: 0.7480796575546265
Epoch 950, training loss: 12.643476486206055 = 0.06780195236206055 + 2.0 * 6.287837028503418
Epoch 950, val loss: 0.753219485282898
Epoch 960, training loss: 12.65633487701416 = 0.06520415842533112 + 2.0 * 6.295565128326416
Epoch 960, val loss: 0.758362352848053
Epoch 970, training loss: 12.634241104125977 = 0.06273765861988068 + 2.0 * 6.285751819610596
Epoch 970, val loss: 0.7634336352348328
Epoch 980, training loss: 12.630897521972656 = 0.060401324182748795 + 2.0 * 6.285248279571533
Epoch 980, val loss: 0.768587052822113
Epoch 990, training loss: 12.627488136291504 = 0.058177292346954346 + 2.0 * 6.284655570983887
Epoch 990, val loss: 0.7738340497016907
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7962962962962963
0.8376383763837639
=== training gcn model ===
Epoch 0, training loss: 19.157705307006836 = 1.9640203714370728 + 2.0 * 8.596842765808105
Epoch 0, val loss: 1.96257483959198
Epoch 10, training loss: 19.146499633789062 = 1.9533411264419556 + 2.0 * 8.596579551696777
Epoch 10, val loss: 1.951904535293579
Epoch 20, training loss: 19.129676818847656 = 1.9400521516799927 + 2.0 * 8.594812393188477
Epoch 20, val loss: 1.9383161067962646
Epoch 30, training loss: 19.085800170898438 = 1.921539068222046 + 2.0 * 8.582130432128906
Epoch 30, val loss: 1.919209599494934
Epoch 40, training loss: 18.916000366210938 = 1.8964954614639282 + 2.0 * 8.50975227355957
Epoch 40, val loss: 1.8942385911941528
Epoch 50, training loss: 17.987159729003906 = 1.8691339492797852 + 2.0 * 8.059013366699219
Epoch 50, val loss: 1.8676337003707886
Epoch 60, training loss: 17.012155532836914 = 1.8451828956604004 + 2.0 * 7.583486557006836
Epoch 60, val loss: 1.8468490839004517
Epoch 70, training loss: 16.35694122314453 = 1.827675461769104 + 2.0 * 7.2646331787109375
Epoch 70, val loss: 1.8314276933670044
Epoch 80, training loss: 15.951169967651367 = 1.8111625909805298 + 2.0 * 7.070003509521484
Epoch 80, val loss: 1.8167790174484253
Epoch 90, training loss: 15.756256103515625 = 1.796804428100586 + 2.0 * 6.9797258377075195
Epoch 90, val loss: 1.8039251565933228
Epoch 100, training loss: 15.584572792053223 = 1.7815138101577759 + 2.0 * 6.901529312133789
Epoch 100, val loss: 1.790498971939087
Epoch 110, training loss: 15.399215698242188 = 1.7673556804656982 + 2.0 * 6.815929889678955
Epoch 110, val loss: 1.778063416481018
Epoch 120, training loss: 15.249662399291992 = 1.7546916007995605 + 2.0 * 6.747485637664795
Epoch 120, val loss: 1.7667049169540405
Epoch 130, training loss: 15.124908447265625 = 1.7417752742767334 + 2.0 * 6.691566467285156
Epoch 130, val loss: 1.7552170753479004
Epoch 140, training loss: 15.025562286376953 = 1.7269480228424072 + 2.0 * 6.6493072509765625
Epoch 140, val loss: 1.7426027059555054
Epoch 150, training loss: 14.939353942871094 = 1.7100424766540527 + 2.0 * 6.6146559715271
Epoch 150, val loss: 1.728442668914795
Epoch 160, training loss: 14.864002227783203 = 1.6909229755401611 + 2.0 * 6.5865397453308105
Epoch 160, val loss: 1.7127041816711426
Epoch 170, training loss: 14.798843383789062 = 1.6691231727600098 + 2.0 * 6.564859867095947
Epoch 170, val loss: 1.694737434387207
Epoch 180, training loss: 14.73312759399414 = 1.6445353031158447 + 2.0 * 6.5442962646484375
Epoch 180, val loss: 1.674527645111084
Epoch 190, training loss: 14.672708511352539 = 1.6167806386947632 + 2.0 * 6.527964115142822
Epoch 190, val loss: 1.6518446207046509
Epoch 200, training loss: 14.60975456237793 = 1.5858149528503418 + 2.0 * 6.511969566345215
Epoch 200, val loss: 1.6266655921936035
Epoch 210, training loss: 14.54971694946289 = 1.551605224609375 + 2.0 * 6.499055862426758
Epoch 210, val loss: 1.598982810974121
Epoch 220, training loss: 14.492387771606445 = 1.514129877090454 + 2.0 * 6.489129066467285
Epoch 220, val loss: 1.5686169862747192
Epoch 230, training loss: 14.42741584777832 = 1.4738736152648926 + 2.0 * 6.476771354675293
Epoch 230, val loss: 1.5363361835479736
Epoch 240, training loss: 14.365124702453613 = 1.4312703609466553 + 2.0 * 6.4669270515441895
Epoch 240, val loss: 1.502255916595459
Epoch 250, training loss: 14.307674407958984 = 1.3865891695022583 + 2.0 * 6.460542678833008
Epoch 250, val loss: 1.4668231010437012
Epoch 260, training loss: 14.241544723510742 = 1.3410354852676392 + 2.0 * 6.450254440307617
Epoch 260, val loss: 1.4308468103408813
Epoch 270, training loss: 14.182023048400879 = 1.295128583908081 + 2.0 * 6.443447113037109
Epoch 270, val loss: 1.3948882818222046
Epoch 280, training loss: 14.119858741760254 = 1.2494745254516602 + 2.0 * 6.435192108154297
Epoch 280, val loss: 1.3593734502792358
Epoch 290, training loss: 14.064189910888672 = 1.2045159339904785 + 2.0 * 6.429837226867676
Epoch 290, val loss: 1.3247843980789185
Epoch 300, training loss: 14.007125854492188 = 1.160836935043335 + 2.0 * 6.423144340515137
Epoch 300, val loss: 1.2912365198135376
Epoch 310, training loss: 13.956664085388184 = 1.1182348728179932 + 2.0 * 6.419214725494385
Epoch 310, val loss: 1.2587072849273682
Epoch 320, training loss: 13.902888298034668 = 1.0770207643508911 + 2.0 * 6.412933826446533
Epoch 320, val loss: 1.2272652387619019
Epoch 330, training loss: 13.852272033691406 = 1.0369362831115723 + 2.0 * 6.407667636871338
Epoch 330, val loss: 1.1968787908554077
Epoch 340, training loss: 13.815390586853027 = 0.9979790449142456 + 2.0 * 6.408705711364746
Epoch 340, val loss: 1.1673367023468018
Epoch 350, training loss: 13.760905265808105 = 0.9604417681694031 + 2.0 * 6.400231838226318
Epoch 350, val loss: 1.139286756515503
Epoch 360, training loss: 13.713133811950684 = 0.92436683177948 + 2.0 * 6.394383430480957
Epoch 360, val loss: 1.1128257513046265
Epoch 370, training loss: 13.669675827026367 = 0.8892983198165894 + 2.0 * 6.390188694000244
Epoch 370, val loss: 1.0873396396636963
Epoch 380, training loss: 13.63299560546875 = 0.8553316593170166 + 2.0 * 6.388832092285156
Epoch 380, val loss: 1.0630197525024414
Epoch 390, training loss: 13.589066505432129 = 0.82274329662323 + 2.0 * 6.383161544799805
Epoch 390, val loss: 1.0402172803878784
Epoch 400, training loss: 13.551567077636719 = 0.7913891077041626 + 2.0 * 6.380088806152344
Epoch 400, val loss: 1.0189545154571533
Epoch 410, training loss: 13.51480770111084 = 0.7611091732978821 + 2.0 * 6.376849174499512
Epoch 410, val loss: 0.9989246726036072
Epoch 420, training loss: 13.483573913574219 = 0.7319726943969727 + 2.0 * 6.375800609588623
Epoch 420, val loss: 0.9803104996681213
Epoch 430, training loss: 13.447324752807617 = 0.7041242122650146 + 2.0 * 6.371600151062012
Epoch 430, val loss: 0.9631996154785156
Epoch 440, training loss: 13.411667823791504 = 0.6773880124092102 + 2.0 * 6.36713981628418
Epoch 440, val loss: 0.9474807977676392
Epoch 450, training loss: 13.392173767089844 = 0.6516795754432678 + 2.0 * 6.370246887207031
Epoch 450, val loss: 0.9329168796539307
Epoch 460, training loss: 13.351483345031738 = 0.6270449161529541 + 2.0 * 6.362219333648682
Epoch 460, val loss: 0.9197095036506653
Epoch 470, training loss: 13.32246208190918 = 0.6035136580467224 + 2.0 * 6.359474182128906
Epoch 470, val loss: 0.9077466726303101
Epoch 480, training loss: 13.294745445251465 = 0.580763041973114 + 2.0 * 6.356991291046143
Epoch 480, val loss: 0.896676242351532
Epoch 490, training loss: 13.271187782287598 = 0.5586769580841064 + 2.0 * 6.356255531311035
Epoch 490, val loss: 0.8863882422447205
Epoch 500, training loss: 13.241868019104004 = 0.5372661352157593 + 2.0 * 6.352301120758057
Epoch 500, val loss: 0.8767951726913452
Epoch 510, training loss: 13.219724655151367 = 0.5165626406669617 + 2.0 * 6.35158109664917
Epoch 510, val loss: 0.8680185079574585
Epoch 520, training loss: 13.193522453308105 = 0.49639198184013367 + 2.0 * 6.348565101623535
Epoch 520, val loss: 0.8598094582557678
Epoch 530, training loss: 13.168685913085938 = 0.476824015378952 + 2.0 * 6.345931053161621
Epoch 530, val loss: 0.8522495031356812
Epoch 540, training loss: 13.1459379196167 = 0.45767733454704285 + 2.0 * 6.344130516052246
Epoch 540, val loss: 0.8452377319335938
Epoch 550, training loss: 13.122411727905273 = 0.43900707364082336 + 2.0 * 6.341702461242676
Epoch 550, val loss: 0.8386439085006714
Epoch 560, training loss: 13.101945877075195 = 0.4208085536956787 + 2.0 * 6.340568542480469
Epoch 560, val loss: 0.8326181173324585
Epoch 570, training loss: 13.09375 = 0.40299057960510254 + 2.0 * 6.345379829406738
Epoch 570, val loss: 0.8270894289016724
Epoch 580, training loss: 13.063472747802734 = 0.38575559854507446 + 2.0 * 6.338858604431152
Epoch 580, val loss: 0.8219784498214722
Epoch 590, training loss: 13.038875579833984 = 0.3690006136894226 + 2.0 * 6.334937572479248
Epoch 590, val loss: 0.8175139427185059
Epoch 600, training loss: 13.01794147491455 = 0.35265353322029114 + 2.0 * 6.332643985748291
Epoch 600, val loss: 0.8135753273963928
Epoch 610, training loss: 13.004206657409668 = 0.3366867005825043 + 2.0 * 6.333759784698486
Epoch 610, val loss: 0.8100708723068237
Epoch 620, training loss: 12.983878135681152 = 0.32124143838882446 + 2.0 * 6.331318378448486
Epoch 620, val loss: 0.8070763349533081
Epoch 630, training loss: 12.965106010437012 = 0.3062482178211212 + 2.0 * 6.329428672790527
Epoch 630, val loss: 0.80450838804245
Epoch 640, training loss: 12.9443998336792 = 0.29170456528663635 + 2.0 * 6.326347827911377
Epoch 640, val loss: 0.8024871349334717
Epoch 650, training loss: 12.935175895690918 = 0.27754876017570496 + 2.0 * 6.328813552856445
Epoch 650, val loss: 0.8009907603263855
Epoch 660, training loss: 12.922164916992188 = 0.2639676034450531 + 2.0 * 6.329098701477051
Epoch 660, val loss: 0.7997515201568604
Epoch 670, training loss: 12.896349906921387 = 0.2508578896522522 + 2.0 * 6.3227458000183105
Epoch 670, val loss: 0.7990627884864807
Epoch 680, training loss: 12.880990028381348 = 0.23822002112865448 + 2.0 * 6.321384906768799
Epoch 680, val loss: 0.7988881468772888
Epoch 690, training loss: 12.866231918334961 = 0.22601161897182465 + 2.0 * 6.320110321044922
Epoch 690, val loss: 0.7992204427719116
Epoch 700, training loss: 12.866719245910645 = 0.21423131227493286 + 2.0 * 6.326243877410889
Epoch 700, val loss: 0.7999211549758911
Epoch 710, training loss: 12.83764934539795 = 0.20299281179904938 + 2.0 * 6.317328453063965
Epoch 710, val loss: 0.80106520652771
Epoch 720, training loss: 12.825956344604492 = 0.19221711158752441 + 2.0 * 6.316869735717773
Epoch 720, val loss: 0.8026701211929321
Epoch 730, training loss: 12.821537017822266 = 0.18187877535820007 + 2.0 * 6.319828987121582
Epoch 730, val loss: 0.8047643303871155
Epoch 740, training loss: 12.809669494628906 = 0.17207695543766022 + 2.0 * 6.318796157836914
Epoch 740, val loss: 0.8071460127830505
Epoch 750, training loss: 12.789165496826172 = 0.16276639699935913 + 2.0 * 6.313199520111084
Epoch 750, val loss: 0.8099230527877808
Epoch 760, training loss: 12.77678108215332 = 0.1539527028799057 + 2.0 * 6.3114142417907715
Epoch 760, val loss: 0.8131350874900818
Epoch 770, training loss: 12.76634407043457 = 0.14558446407318115 + 2.0 * 6.310379981994629
Epoch 770, val loss: 0.8167553544044495
Epoch 780, training loss: 12.771403312683105 = 0.13767088949680328 + 2.0 * 6.316866397857666
Epoch 780, val loss: 0.8206231594085693
Epoch 790, training loss: 12.750509262084961 = 0.13023363053798676 + 2.0 * 6.310137748718262
Epoch 790, val loss: 0.8248937129974365
Epoch 800, training loss: 12.7394380569458 = 0.12326819449663162 + 2.0 * 6.308084964752197
Epoch 800, val loss: 0.8293133974075317
Epoch 810, training loss: 12.728584289550781 = 0.11673137545585632 + 2.0 * 6.305926322937012
Epoch 810, val loss: 0.8341505527496338
Epoch 820, training loss: 12.732881546020508 = 0.11058346927165985 + 2.0 * 6.3111491203308105
Epoch 820, val loss: 0.8392360806465149
Epoch 830, training loss: 12.725762367248535 = 0.10487912595272064 + 2.0 * 6.310441493988037
Epoch 830, val loss: 0.8444015979766846
Epoch 840, training loss: 12.708952903747559 = 0.09953752905130386 + 2.0 * 6.3047075271606445
Epoch 840, val loss: 0.8497706651687622
Epoch 850, training loss: 12.69888973236084 = 0.09454387426376343 + 2.0 * 6.302173137664795
Epoch 850, val loss: 0.8554590940475464
Epoch 860, training loss: 12.692476272583008 = 0.08987556397914886 + 2.0 * 6.301300525665283
Epoch 860, val loss: 0.8613952994346619
Epoch 870, training loss: 12.69019889831543 = 0.08549483865499496 + 2.0 * 6.302351951599121
Epoch 870, val loss: 0.8674362301826477
Epoch 880, training loss: 12.68530559539795 = 0.08139478415250778 + 2.0 * 6.301955223083496
Epoch 880, val loss: 0.8734498023986816
Epoch 890, training loss: 12.678238868713379 = 0.07757370918989182 + 2.0 * 6.300332546234131
Epoch 890, val loss: 0.8795621991157532
Epoch 900, training loss: 12.672810554504395 = 0.07399608194828033 + 2.0 * 6.299407005310059
Epoch 900, val loss: 0.8857697248458862
Epoch 910, training loss: 12.665005683898926 = 0.07063338160514832 + 2.0 * 6.297186374664307
Epoch 910, val loss: 0.8920167088508606
Epoch 920, training loss: 12.669093132019043 = 0.06747866421937943 + 2.0 * 6.300806999206543
Epoch 920, val loss: 0.8982952833175659
Epoch 930, training loss: 12.655648231506348 = 0.06451403349637985 + 2.0 * 6.295567035675049
Epoch 930, val loss: 0.9045605659484863
Epoch 940, training loss: 12.651615142822266 = 0.061728041619062424 + 2.0 * 6.294943332672119
Epoch 940, val loss: 0.910825252532959
Epoch 950, training loss: 12.653475761413574 = 0.05910560488700867 + 2.0 * 6.297184944152832
Epoch 950, val loss: 0.9170694947242737
Epoch 960, training loss: 12.644002914428711 = 0.056643757969141006 + 2.0 * 6.293679714202881
Epoch 960, val loss: 0.923308789730072
Epoch 970, training loss: 12.641121864318848 = 0.05432060733437538 + 2.0 * 6.293400764465332
Epoch 970, val loss: 0.929494321346283
Epoch 980, training loss: 12.639226913452148 = 0.052129633724689484 + 2.0 * 6.293548583984375
Epoch 980, val loss: 0.9356870055198669
Epoch 990, training loss: 12.6314115524292 = 0.05005588009953499 + 2.0 * 6.290678024291992
Epoch 990, val loss: 0.9418141841888428
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8037037037037037
0.8418555614127571
=== training gcn model ===
Epoch 0, training loss: 19.142120361328125 = 1.9485002756118774 + 2.0 * 8.596810340881348
Epoch 0, val loss: 1.9604103565216064
Epoch 10, training loss: 19.131576538085938 = 1.9388316869735718 + 2.0 * 8.596372604370117
Epoch 10, val loss: 1.9505473375320435
Epoch 20, training loss: 19.112205505371094 = 1.9269713163375854 + 2.0 * 8.59261703491211
Epoch 20, val loss: 1.9383058547973633
Epoch 30, training loss: 19.034303665161133 = 1.910959243774414 + 2.0 * 8.56167221069336
Epoch 30, val loss: 1.9218472242355347
Epoch 40, training loss: 18.551429748535156 = 1.8923527002334595 + 2.0 * 8.329538345336914
Epoch 40, val loss: 1.9032131433486938
Epoch 50, training loss: 17.3115291595459 = 1.874088168144226 + 2.0 * 7.718720436096191
Epoch 50, val loss: 1.8854899406433105
Epoch 60, training loss: 16.397489547729492 = 1.8605211973190308 + 2.0 * 7.268484592437744
Epoch 60, val loss: 1.872113823890686
Epoch 70, training loss: 15.917332649230957 = 1.8490973711013794 + 2.0 * 7.034117698669434
Epoch 70, val loss: 1.8606740236282349
Epoch 80, training loss: 15.636961936950684 = 1.836920142173767 + 2.0 * 6.900021076202393
Epoch 80, val loss: 1.848591923713684
Epoch 90, training loss: 15.431427955627441 = 1.8232964277267456 + 2.0 * 6.804065704345703
Epoch 90, val loss: 1.835500717163086
Epoch 100, training loss: 15.280787467956543 = 1.809523582458496 + 2.0 * 6.735631942749023
Epoch 100, val loss: 1.8224900960922241
Epoch 110, training loss: 15.16793441772461 = 1.796183466911316 + 2.0 * 6.685875415802002
Epoch 110, val loss: 1.8099464178085327
Epoch 120, training loss: 15.064252853393555 = 1.7830947637557983 + 2.0 * 6.6405792236328125
Epoch 120, val loss: 1.7972052097320557
Epoch 130, training loss: 14.982549667358398 = 1.7694607973098755 + 2.0 * 6.606544494628906
Epoch 130, val loss: 1.7840478420257568
Epoch 140, training loss: 14.92161750793457 = 1.7549153566360474 + 2.0 * 6.583351135253906
Epoch 140, val loss: 1.770159125328064
Epoch 150, training loss: 14.845974922180176 = 1.739095687866211 + 2.0 * 6.553439617156982
Epoch 150, val loss: 1.755428433418274
Epoch 160, training loss: 14.787630081176758 = 1.7217180728912354 + 2.0 * 6.532956123352051
Epoch 160, val loss: 1.7395081520080566
Epoch 170, training loss: 14.73153305053711 = 1.7023015022277832 + 2.0 * 6.514616012573242
Epoch 170, val loss: 1.7220354080200195
Epoch 180, training loss: 14.679893493652344 = 1.6805232763290405 + 2.0 * 6.499685287475586
Epoch 180, val loss: 1.7027122974395752
Epoch 190, training loss: 14.619590759277344 = 1.656024694442749 + 2.0 * 6.481782913208008
Epoch 190, val loss: 1.6811834573745728
Epoch 200, training loss: 14.565396308898926 = 1.6285067796707153 + 2.0 * 6.46844482421875
Epoch 200, val loss: 1.6573100090026855
Epoch 210, training loss: 14.509562492370605 = 1.5976307392120361 + 2.0 * 6.455965995788574
Epoch 210, val loss: 1.6306699514389038
Epoch 220, training loss: 14.468908309936523 = 1.5633080005645752 + 2.0 * 6.452800273895264
Epoch 220, val loss: 1.6012661457061768
Epoch 230, training loss: 14.402052879333496 = 1.525957465171814 + 2.0 * 6.438047885894775
Epoch 230, val loss: 1.5697137117385864
Epoch 240, training loss: 14.341257095336914 = 1.4859240055084229 + 2.0 * 6.427666664123535
Epoch 240, val loss: 1.5362451076507568
Epoch 250, training loss: 14.286752700805664 = 1.4436047077178955 + 2.0 * 6.421574115753174
Epoch 250, val loss: 1.5012894868850708
Epoch 260, training loss: 14.22765064239502 = 1.4001575708389282 + 2.0 * 6.413746356964111
Epoch 260, val loss: 1.4657936096191406
Epoch 270, training loss: 14.17129135131836 = 1.3563053607940674 + 2.0 * 6.4074931144714355
Epoch 270, val loss: 1.430458664894104
Epoch 280, training loss: 14.113882064819336 = 1.3123316764831543 + 2.0 * 6.40077543258667
Epoch 280, val loss: 1.3953771591186523
Epoch 290, training loss: 14.059335708618164 = 1.268502116203308 + 2.0 * 6.395416736602783
Epoch 290, val loss: 1.3607419729232788
Epoch 300, training loss: 14.011561393737793 = 1.2254087924957275 + 2.0 * 6.393076419830322
Epoch 300, val loss: 1.3270092010498047
Epoch 310, training loss: 13.959097862243652 = 1.1835743188858032 + 2.0 * 6.38776159286499
Epoch 310, val loss: 1.2946093082427979
Epoch 320, training loss: 13.907987594604492 = 1.1424897909164429 + 2.0 * 6.382749080657959
Epoch 320, val loss: 1.262998104095459
Epoch 330, training loss: 13.857505798339844 = 1.1019185781478882 + 2.0 * 6.377793788909912
Epoch 330, val loss: 1.231884479522705
Epoch 340, training loss: 13.812578201293945 = 1.0616635084152222 + 2.0 * 6.375457286834717
Epoch 340, val loss: 1.2011165618896484
Epoch 350, training loss: 13.772221565246582 = 1.0218602418899536 + 2.0 * 6.375180721282959
Epoch 350, val loss: 1.1710548400878906
Epoch 360, training loss: 13.719696044921875 = 0.9830693602561951 + 2.0 * 6.368313312530518
Epoch 360, val loss: 1.141622543334961
Epoch 370, training loss: 13.674385070800781 = 0.9448334574699402 + 2.0 * 6.364775657653809
Epoch 370, val loss: 1.112866759300232
Epoch 380, training loss: 13.630145072937012 = 0.9072555303573608 + 2.0 * 6.36144495010376
Epoch 380, val loss: 1.0848705768585205
Epoch 390, training loss: 13.608845710754395 = 0.8705068826675415 + 2.0 * 6.369169235229492
Epoch 390, val loss: 1.0578917264938354
Epoch 400, training loss: 13.549285888671875 = 0.8354976773262024 + 2.0 * 6.356894016265869
Epoch 400, val loss: 1.0324442386627197
Epoch 410, training loss: 13.50952434539795 = 0.8019837737083435 + 2.0 * 6.3537702560424805
Epoch 410, val loss: 1.008616328239441
Epoch 420, training loss: 13.471470832824707 = 0.7698090672492981 + 2.0 * 6.350831031799316
Epoch 420, val loss: 0.9862831234931946
Epoch 430, training loss: 13.434847831726074 = 0.7389153838157654 + 2.0 * 6.347966194152832
Epoch 430, val loss: 0.9654843807220459
Epoch 440, training loss: 13.400274276733398 = 0.7092651128768921 + 2.0 * 6.3455047607421875
Epoch 440, val loss: 0.9461669325828552
Epoch 450, training loss: 13.381410598754883 = 0.6808830499649048 + 2.0 * 6.350263595581055
Epoch 450, val loss: 0.9283114671707153
Epoch 460, training loss: 13.34272575378418 = 0.6540689468383789 + 2.0 * 6.3443284034729
Epoch 460, val loss: 0.9122312068939209
Epoch 470, training loss: 13.308087348937988 = 0.6284841895103455 + 2.0 * 6.339801788330078
Epoch 470, val loss: 0.8975961804389954
Epoch 480, training loss: 13.276815414428711 = 0.6039383411407471 + 2.0 * 6.3364386558532715
Epoch 480, val loss: 0.884256899356842
Epoch 490, training loss: 13.25584602355957 = 0.5803879499435425 + 2.0 * 6.337728977203369
Epoch 490, val loss: 0.8720405697822571
Epoch 500, training loss: 13.22530460357666 = 0.55806964635849 + 2.0 * 6.333617687225342
Epoch 500, val loss: 0.8611242771148682
Epoch 510, training loss: 13.199728965759277 = 0.5367427468299866 + 2.0 * 6.331492900848389
Epoch 510, val loss: 0.8513882756233215
Epoch 520, training loss: 13.172290802001953 = 0.5162004232406616 + 2.0 * 6.32804536819458
Epoch 520, val loss: 0.8426065444946289
Epoch 530, training loss: 13.159503936767578 = 0.496381938457489 + 2.0 * 6.331561088562012
Epoch 530, val loss: 0.8347189426422119
Epoch 540, training loss: 13.133420944213867 = 0.47744813561439514 + 2.0 * 6.327986240386963
Epoch 540, val loss: 0.8276509046554565
Epoch 550, training loss: 13.10407543182373 = 0.4591881036758423 + 2.0 * 6.32244348526001
Epoch 550, val loss: 0.8214783072471619
Epoch 560, training loss: 13.101693153381348 = 0.4416183531284332 + 2.0 * 6.330037593841553
Epoch 560, val loss: 0.8160772919654846
Epoch 570, training loss: 13.068853378295898 = 0.4246625602245331 + 2.0 * 6.3220953941345215
Epoch 570, val loss: 0.8113779425621033
Epoch 580, training loss: 13.042817115783691 = 0.4083941578865051 + 2.0 * 6.317211627960205
Epoch 580, val loss: 0.807418704032898
Epoch 590, training loss: 13.024672508239746 = 0.392560213804245 + 2.0 * 6.316056251525879
Epoch 590, val loss: 0.8040646314620972
Epoch 600, training loss: 13.012892723083496 = 0.37716394662857056 + 2.0 * 6.317864418029785
Epoch 600, val loss: 0.8012641072273254
Epoch 610, training loss: 12.988110542297363 = 0.362225204706192 + 2.0 * 6.3129425048828125
Epoch 610, val loss: 0.7990313172340393
Epoch 620, training loss: 12.984128952026367 = 0.3476915955543518 + 2.0 * 6.31821870803833
Epoch 620, val loss: 0.7972930073738098
Epoch 630, training loss: 12.955187797546387 = 0.3335952162742615 + 2.0 * 6.31079626083374
Epoch 630, val loss: 0.7961471676826477
Epoch 640, training loss: 12.93591022491455 = 0.3198092579841614 + 2.0 * 6.308050632476807
Epoch 640, val loss: 0.7955402731895447
Epoch 650, training loss: 12.921370506286621 = 0.30636686086654663 + 2.0 * 6.307501792907715
Epoch 650, val loss: 0.7954232096672058
Epoch 660, training loss: 12.909217834472656 = 0.29325973987579346 + 2.0 * 6.307979106903076
Epoch 660, val loss: 0.7957948446273804
Epoch 670, training loss: 12.889373779296875 = 0.28050726652145386 + 2.0 * 6.304433345794678
Epoch 670, val loss: 0.7967138290405273
Epoch 680, training loss: 12.874045372009277 = 0.268118292093277 + 2.0 * 6.302963733673096
Epoch 680, val loss: 0.7981231212615967
Epoch 690, training loss: 12.880311012268066 = 0.25612425804138184 + 2.0 * 6.312093257904053
Epoch 690, val loss: 0.7999921441078186
Epoch 700, training loss: 12.845185279846191 = 0.24448907375335693 + 2.0 * 6.300348281860352
Epoch 700, val loss: 0.8024304509162903
Epoch 710, training loss: 12.831672668457031 = 0.2332637459039688 + 2.0 * 6.299204349517822
Epoch 710, val loss: 0.8053706288337708
Epoch 720, training loss: 12.8206205368042 = 0.22243092954158783 + 2.0 * 6.2990946769714355
Epoch 720, val loss: 0.8087419271469116
Epoch 730, training loss: 12.808770179748535 = 0.21203742921352386 + 2.0 * 6.298366546630859
Epoch 730, val loss: 0.8125534653663635
Epoch 740, training loss: 12.799283981323242 = 0.20210883021354675 + 2.0 * 6.298587799072266
Epoch 740, val loss: 0.8167856931686401
Epoch 750, training loss: 12.784053802490234 = 0.19258548319339752 + 2.0 * 6.29573392868042
Epoch 750, val loss: 0.8214731812477112
Epoch 760, training loss: 12.770180702209473 = 0.18351322412490845 + 2.0 * 6.293333530426025
Epoch 760, val loss: 0.8265678882598877
Epoch 770, training loss: 12.760433197021484 = 0.1748378574848175 + 2.0 * 6.292797565460205
Epoch 770, val loss: 0.8320558667182922
Epoch 780, training loss: 12.753138542175293 = 0.16655394434928894 + 2.0 * 6.29329252243042
Epoch 780, val loss: 0.8378683924674988
Epoch 790, training loss: 12.742894172668457 = 0.1586928516626358 + 2.0 * 6.292100429534912
Epoch 790, val loss: 0.8439534902572632
Epoch 800, training loss: 12.74715805053711 = 0.15125684440135956 + 2.0 * 6.297950744628906
Epoch 800, val loss: 0.8503684997558594
Epoch 810, training loss: 12.727048873901367 = 0.14416714012622833 + 2.0 * 6.291440963745117
Epoch 810, val loss: 0.8569232225418091
Epoch 820, training loss: 12.713774681091309 = 0.13752248883247375 + 2.0 * 6.288125991821289
Epoch 820, val loss: 0.8637551069259644
Epoch 830, training loss: 12.70343017578125 = 0.13120236992835999 + 2.0 * 6.286113739013672
Epoch 830, val loss: 0.8707888722419739
Epoch 840, training loss: 12.694563865661621 = 0.1252186894416809 + 2.0 * 6.284672737121582
Epoch 840, val loss: 0.878018856048584
Epoch 850, training loss: 12.69123649597168 = 0.11955355852842331 + 2.0 * 6.285841464996338
Epoch 850, val loss: 0.8853643536567688
Epoch 860, training loss: 12.686895370483398 = 0.11420446634292603 + 2.0 * 6.286345481872559
Epoch 860, val loss: 0.8927383422851562
Epoch 870, training loss: 12.680131912231445 = 0.10919339954853058 + 2.0 * 6.285469055175781
Epoch 870, val loss: 0.9002137184143066
Epoch 880, training loss: 12.668757438659668 = 0.10446705669164658 + 2.0 * 6.282145023345947
Epoch 880, val loss: 0.9077163934707642
Epoch 890, training loss: 12.66182804107666 = 0.10000281035900116 + 2.0 * 6.280912399291992
Epoch 890, val loss: 0.9152723550796509
Epoch 900, training loss: 12.679503440856934 = 0.09580012410879135 + 2.0 * 6.29185152053833
Epoch 900, val loss: 0.9228522181510925
Epoch 910, training loss: 12.654862403869629 = 0.09180761128664017 + 2.0 * 6.281527519226074
Epoch 910, val loss: 0.9303062558174133
Epoch 920, training loss: 12.644883155822754 = 0.08807000517845154 + 2.0 * 6.278406620025635
Epoch 920, val loss: 0.9377797842025757
Epoch 930, training loss: 12.63974380493164 = 0.08452078700065613 + 2.0 * 6.27761173248291
Epoch 930, val loss: 0.9452244639396667
Epoch 940, training loss: 12.643362998962402 = 0.08115234225988388 + 2.0 * 6.2811055183410645
Epoch 940, val loss: 0.9526943564414978
Epoch 950, training loss: 12.63480281829834 = 0.07798683643341064 + 2.0 * 6.278408050537109
Epoch 950, val loss: 0.9599233865737915
Epoch 960, training loss: 12.628229141235352 = 0.07496757805347443 + 2.0 * 6.276630878448486
Epoch 960, val loss: 0.9671315550804138
Epoch 970, training loss: 12.620250701904297 = 0.07212064415216446 + 2.0 * 6.274065017700195
Epoch 970, val loss: 0.9743208289146423
Epoch 980, training loss: 12.62062931060791 = 0.06940411776304245 + 2.0 * 6.275612831115723
Epoch 980, val loss: 0.9814140200614929
Epoch 990, training loss: 12.61345386505127 = 0.0668254867196083 + 2.0 * 6.273313999176025
Epoch 990, val loss: 0.9884116649627686
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7666666666666667
0.8434370057986295
The final CL Acc:0.78889, 0.01600, The final GNN Acc:0.84098, 0.00245
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.3 der2:0.4 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11626])
remove edge: torch.Size([2, 9494])
updated graph: torch.Size([2, 10564])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 19.13690948486328 = 1.9432686567306519 + 2.0 * 8.596820831298828
Epoch 0, val loss: 1.9372748136520386
Epoch 10, training loss: 19.126453399658203 = 1.9335312843322754 + 2.0 * 8.596461296081543
Epoch 10, val loss: 1.9275686740875244
Epoch 20, training loss: 19.109235763549805 = 1.9212777614593506 + 2.0 * 8.593978881835938
Epoch 20, val loss: 1.9152872562408447
Epoch 30, training loss: 19.05418586730957 = 1.9044452905654907 + 2.0 * 8.574870109558105
Epoch 30, val loss: 1.898610234260559
Epoch 40, training loss: 18.753767013549805 = 1.8844443559646606 + 2.0 * 8.434660911560059
Epoch 40, val loss: 1.8800950050354004
Epoch 50, training loss: 17.332962036132812 = 1.8647239208221436 + 2.0 * 7.734119415283203
Epoch 50, val loss: 1.8625212907791138
Epoch 60, training loss: 16.57362174987793 = 1.8493266105651855 + 2.0 * 7.362147331237793
Epoch 60, val loss: 1.8489148616790771
Epoch 70, training loss: 15.995548248291016 = 1.8368266820907593 + 2.0 * 7.0793609619140625
Epoch 70, val loss: 1.8372197151184082
Epoch 80, training loss: 15.640389442443848 = 1.825282096862793 + 2.0 * 6.907553672790527
Epoch 80, val loss: 1.827304720878601
Epoch 90, training loss: 15.414199829101562 = 1.8143829107284546 + 2.0 * 6.799908638000488
Epoch 90, val loss: 1.8180387020111084
Epoch 100, training loss: 15.255973815917969 = 1.803611159324646 + 2.0 * 6.726181507110596
Epoch 100, val loss: 1.8089784383773804
Epoch 110, training loss: 15.135171890258789 = 1.7928333282470703 + 2.0 * 6.671169281005859
Epoch 110, val loss: 1.8000432252883911
Epoch 120, training loss: 15.048754692077637 = 1.7826389074325562 + 2.0 * 6.633058071136475
Epoch 120, val loss: 1.7915092706680298
Epoch 130, training loss: 14.963661193847656 = 1.7726855278015137 + 2.0 * 6.59548807144165
Epoch 130, val loss: 1.7830370664596558
Epoch 140, training loss: 14.895853996276855 = 1.7624863386154175 + 2.0 * 6.566683769226074
Epoch 140, val loss: 1.774181842803955
Epoch 150, training loss: 14.837403297424316 = 1.751495361328125 + 2.0 * 6.542953968048096
Epoch 150, val loss: 1.764552116394043
Epoch 160, training loss: 14.786500930786133 = 1.7393711805343628 + 2.0 * 6.52356481552124
Epoch 160, val loss: 1.754023551940918
Epoch 170, training loss: 14.738506317138672 = 1.7257344722747803 + 2.0 * 6.506385803222656
Epoch 170, val loss: 1.742333173751831
Epoch 180, training loss: 14.695650100708008 = 1.7103428840637207 + 2.0 * 6.4926533699035645
Epoch 180, val loss: 1.7292988300323486
Epoch 190, training loss: 14.65196418762207 = 1.692971110343933 + 2.0 * 6.479496479034424
Epoch 190, val loss: 1.7146804332733154
Epoch 200, training loss: 14.606955528259277 = 1.673223614692688 + 2.0 * 6.4668660163879395
Epoch 200, val loss: 1.698097586631775
Epoch 210, training loss: 14.565411567687988 = 1.6506165266036987 + 2.0 * 6.4573974609375
Epoch 210, val loss: 1.6790857315063477
Epoch 220, training loss: 14.518417358398438 = 1.6249885559082031 + 2.0 * 6.446714401245117
Epoch 220, val loss: 1.6577523946762085
Epoch 230, training loss: 14.473176002502441 = 1.5962475538253784 + 2.0 * 6.438464164733887
Epoch 230, val loss: 1.633984088897705
Epoch 240, training loss: 14.423288345336914 = 1.5640610456466675 + 2.0 * 6.4296135902404785
Epoch 240, val loss: 1.60750412940979
Epoch 250, training loss: 14.387475967407227 = 1.5283970832824707 + 2.0 * 6.429539680480957
Epoch 250, val loss: 1.5784499645233154
Epoch 260, training loss: 14.324844360351562 = 1.4899195432662964 + 2.0 * 6.417462348937988
Epoch 260, val loss: 1.5476659536361694
Epoch 270, training loss: 14.267885208129883 = 1.4489986896514893 + 2.0 * 6.409443378448486
Epoch 270, val loss: 1.5153393745422363
Epoch 280, training loss: 14.212855339050293 = 1.4058314561843872 + 2.0 * 6.403512001037598
Epoch 280, val loss: 1.4817209243774414
Epoch 290, training loss: 14.165578842163086 = 1.361693263053894 + 2.0 * 6.401942729949951
Epoch 290, val loss: 1.4483009576797485
Epoch 300, training loss: 14.107460021972656 = 1.3178151845932007 + 2.0 * 6.394822597503662
Epoch 300, val loss: 1.4157689809799194
Epoch 310, training loss: 14.053945541381836 = 1.274909496307373 + 2.0 * 6.3895182609558105
Epoch 310, val loss: 1.3847185373306274
Epoch 320, training loss: 14.005014419555664 = 1.2330602407455444 + 2.0 * 6.385977268218994
Epoch 320, val loss: 1.3551870584487915
Epoch 330, training loss: 13.9542236328125 = 1.1927094459533691 + 2.0 * 6.3807573318481445
Epoch 330, val loss: 1.32749342918396
Epoch 340, training loss: 13.908455848693848 = 1.1538286209106445 + 2.0 * 6.377313613891602
Epoch 340, val loss: 1.3013992309570312
Epoch 350, training loss: 13.865349769592285 = 1.1164146661758423 + 2.0 * 6.374467372894287
Epoch 350, val loss: 1.2766252756118774
Epoch 360, training loss: 13.829845428466797 = 1.0803736448287964 + 2.0 * 6.3747358322143555
Epoch 360, val loss: 1.2532671689987183
Epoch 370, training loss: 13.779461860656738 = 1.0459247827529907 + 2.0 * 6.3667683601379395
Epoch 370, val loss: 1.231441855430603
Epoch 380, training loss: 13.746674537658691 = 1.0126687288284302 + 2.0 * 6.367002964019775
Epoch 380, val loss: 1.2107882499694824
Epoch 390, training loss: 13.703786849975586 = 0.9807923436164856 + 2.0 * 6.361497402191162
Epoch 390, val loss: 1.191272258758545
Epoch 400, training loss: 13.665861129760742 = 0.9496904611587524 + 2.0 * 6.3580851554870605
Epoch 400, val loss: 1.172898292541504
Epoch 410, training loss: 13.639057159423828 = 0.9194254875183105 + 2.0 * 6.359816074371338
Epoch 410, val loss: 1.1552597284317017
Epoch 420, training loss: 13.595741271972656 = 0.8899168372154236 + 2.0 * 6.352912425994873
Epoch 420, val loss: 1.1385300159454346
Epoch 430, training loss: 13.562212944030762 = 0.8610931634902954 + 2.0 * 6.350559711456299
Epoch 430, val loss: 1.1227656602859497
Epoch 440, training loss: 13.53133487701416 = 0.8328691720962524 + 2.0 * 6.3492326736450195
Epoch 440, val loss: 1.1078535318374634
Epoch 450, training loss: 13.496838569641113 = 0.8052592873573303 + 2.0 * 6.345789432525635
Epoch 450, val loss: 1.0936473608016968
Epoch 460, training loss: 13.469611167907715 = 0.7780126929283142 + 2.0 * 6.345799446105957
Epoch 460, val loss: 1.0801657438278198
Epoch 470, training loss: 13.433127403259277 = 0.7513032555580139 + 2.0 * 6.340911865234375
Epoch 470, val loss: 1.0674434900283813
Epoch 480, training loss: 13.401769638061523 = 0.7251744270324707 + 2.0 * 6.3382978439331055
Epoch 480, val loss: 1.0555016994476318
Epoch 490, training loss: 13.372979164123535 = 0.6994768977165222 + 2.0 * 6.3367509841918945
Epoch 490, val loss: 1.0442864894866943
Epoch 500, training loss: 13.34628677368164 = 0.6743370294570923 + 2.0 * 6.33597469329834
Epoch 500, val loss: 1.0339109897613525
Epoch 510, training loss: 13.325820922851562 = 0.6499264240264893 + 2.0 * 6.337947368621826
Epoch 510, val loss: 1.024449110031128
Epoch 520, training loss: 13.288829803466797 = 0.6261901259422302 + 2.0 * 6.331319808959961
Epoch 520, val loss: 1.0160696506500244
Epoch 530, training loss: 13.26098918914795 = 0.6033055782318115 + 2.0 * 6.328841686248779
Epoch 530, val loss: 1.008673906326294
Epoch 540, training loss: 13.244124412536621 = 0.5810142159461975 + 2.0 * 6.331554889678955
Epoch 540, val loss: 1.0022618770599365
Epoch 550, training loss: 13.215766906738281 = 0.5596054792404175 + 2.0 * 6.328080654144287
Epoch 550, val loss: 0.9968278408050537
Epoch 560, training loss: 13.187211990356445 = 0.5389538407325745 + 2.0 * 6.324129104614258
Epoch 560, val loss: 0.9924322962760925
Epoch 570, training loss: 13.171856880187988 = 0.5189634561538696 + 2.0 * 6.326446533203125
Epoch 570, val loss: 0.988996148109436
Epoch 580, training loss: 13.144553184509277 = 0.4997396171092987 + 2.0 * 6.322406768798828
Epoch 580, val loss: 0.9864204525947571
Epoch 590, training loss: 13.119576454162598 = 0.4810290038585663 + 2.0 * 6.319273948669434
Epoch 590, val loss: 0.9846799373626709
Epoch 600, training loss: 13.11271858215332 = 0.4629344940185547 + 2.0 * 6.324892044067383
Epoch 600, val loss: 0.9835721850395203
Epoch 610, training loss: 13.080069541931152 = 0.44529205560684204 + 2.0 * 6.317388534545898
Epoch 610, val loss: 0.9832140803337097
Epoch 620, training loss: 13.055723190307617 = 0.4282713234424591 + 2.0 * 6.31372594833374
Epoch 620, val loss: 0.9834859371185303
Epoch 630, training loss: 13.038004875183105 = 0.41168367862701416 + 2.0 * 6.313160419464111
Epoch 630, val loss: 0.9843065142631531
Epoch 640, training loss: 13.021371841430664 = 0.3955099880695343 + 2.0 * 6.312931060791016
Epoch 640, val loss: 0.9856911897659302
Epoch 650, training loss: 13.004535675048828 = 0.3798392713069916 + 2.0 * 6.312348365783691
Epoch 650, val loss: 0.9875341057777405
Epoch 660, training loss: 12.985657691955566 = 0.3645469546318054 + 2.0 * 6.310555458068848
Epoch 660, val loss: 0.9897609949111938
Epoch 670, training loss: 12.967750549316406 = 0.3497341275215149 + 2.0 * 6.3090081214904785
Epoch 670, val loss: 0.9924852252006531
Epoch 680, training loss: 12.950315475463867 = 0.33532050251960754 + 2.0 * 6.307497501373291
Epoch 680, val loss: 0.9955096244812012
Epoch 690, training loss: 12.935307502746582 = 0.3213166892528534 + 2.0 * 6.306995391845703
Epoch 690, val loss: 0.9990114569664001
Epoch 700, training loss: 12.915298461914062 = 0.30772656202316284 + 2.0 * 6.303785800933838
Epoch 700, val loss: 1.0027549266815186
Epoch 710, training loss: 12.914262771606445 = 0.29460063576698303 + 2.0 * 6.309831142425537
Epoch 710, val loss: 1.0068488121032715
Epoch 720, training loss: 12.888347625732422 = 0.2818562090396881 + 2.0 * 6.303245544433594
Epoch 720, val loss: 1.0111230611801147
Epoch 730, training loss: 12.872031211853027 = 0.26963695883750916 + 2.0 * 6.301197052001953
Epoch 730, val loss: 1.0156848430633545
Epoch 740, training loss: 12.862603187561035 = 0.2578398287296295 + 2.0 * 6.30238151550293
Epoch 740, val loss: 1.0205250978469849
Epoch 750, training loss: 12.843158721923828 = 0.24646419286727905 + 2.0 * 6.298347473144531
Epoch 750, val loss: 1.0257015228271484
Epoch 760, training loss: 12.835368156433105 = 0.23553061485290527 + 2.0 * 6.2999186515808105
Epoch 760, val loss: 1.0310444831848145
Epoch 770, training loss: 12.8207426071167 = 0.22505642473697662 + 2.0 * 6.297842979431152
Epoch 770, val loss: 1.036726713180542
Epoch 780, training loss: 12.807805061340332 = 0.21499831974506378 + 2.0 * 6.296403408050537
Epoch 780, val loss: 1.0425453186035156
Epoch 790, training loss: 12.798822402954102 = 0.2053842395544052 + 2.0 * 6.296719074249268
Epoch 790, val loss: 1.0485135316848755
Epoch 800, training loss: 12.783976554870605 = 0.19619783759117126 + 2.0 * 6.29388952255249
Epoch 800, val loss: 1.054877519607544
Epoch 810, training loss: 12.771865844726562 = 0.1874276101589203 + 2.0 * 6.292219161987305
Epoch 810, val loss: 1.0613771677017212
Epoch 820, training loss: 12.76133918762207 = 0.17902937531471252 + 2.0 * 6.291154861450195
Epoch 820, val loss: 1.068153738975525
Epoch 830, training loss: 12.76767349243164 = 0.17099866271018982 + 2.0 * 6.298337459564209
Epoch 830, val loss: 1.0750036239624023
Epoch 840, training loss: 12.742959976196289 = 0.16338399052619934 + 2.0 * 6.289787769317627
Epoch 840, val loss: 1.082266092300415
Epoch 850, training loss: 12.735607147216797 = 0.15615105628967285 + 2.0 * 6.289728164672852
Epoch 850, val loss: 1.0896844863891602
Epoch 860, training loss: 12.728804588317871 = 0.14925727248191833 + 2.0 * 6.289773464202881
Epoch 860, val loss: 1.0970362424850464
Epoch 870, training loss: 12.71738338470459 = 0.1427077353000641 + 2.0 * 6.287337779998779
Epoch 870, val loss: 1.1048448085784912
Epoch 880, training loss: 12.711767196655273 = 0.13648641109466553 + 2.0 * 6.287640571594238
Epoch 880, val loss: 1.1124730110168457
Epoch 890, training loss: 12.701642990112305 = 0.1305694878101349 + 2.0 * 6.285536766052246
Epoch 890, val loss: 1.1204802989959717
Epoch 900, training loss: 12.706755638122559 = 0.1249404326081276 + 2.0 * 6.290907382965088
Epoch 900, val loss: 1.1285316944122314
Epoch 910, training loss: 12.690245628356934 = 0.11955346167087555 + 2.0 * 6.285346031188965
Epoch 910, val loss: 1.1364678144454956
Epoch 920, training loss: 12.683680534362793 = 0.11447322368621826 + 2.0 * 6.284603595733643
Epoch 920, val loss: 1.144744634628296
Epoch 930, training loss: 12.683934211730957 = 0.10962613672018051 + 2.0 * 6.287154197692871
Epoch 930, val loss: 1.1528124809265137
Epoch 940, training loss: 12.668706893920898 = 0.10505528002977371 + 2.0 * 6.281826019287109
Epoch 940, val loss: 1.1611120700836182
Epoch 950, training loss: 12.663420677185059 = 0.10071290284395218 + 2.0 * 6.281353950500488
Epoch 950, val loss: 1.1694564819335938
Epoch 960, training loss: 12.655444145202637 = 0.09656240046024323 + 2.0 * 6.279440879821777
Epoch 960, val loss: 1.177797794342041
Epoch 970, training loss: 12.652382850646973 = 0.09260674566030502 + 2.0 * 6.279888153076172
Epoch 970, val loss: 1.186249852180481
Epoch 980, training loss: 12.646337509155273 = 0.08884862065315247 + 2.0 * 6.278744220733643
Epoch 980, val loss: 1.1946219205856323
Epoch 990, training loss: 12.64834213256836 = 0.0852845162153244 + 2.0 * 6.281528949737549
Epoch 990, val loss: 1.2032771110534668
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7074074074074075
0.8128624143384291
=== training gcn model ===
Epoch 0, training loss: 19.14253807067871 = 1.9488674402236938 + 2.0 * 8.596835136413574
Epoch 0, val loss: 1.9541938304901123
Epoch 10, training loss: 19.131319046020508 = 1.938348650932312 + 2.0 * 8.596485137939453
Epoch 10, val loss: 1.9438015222549438
Epoch 20, training loss: 19.11278533935547 = 1.9248838424682617 + 2.0 * 8.593950271606445
Epoch 20, val loss: 1.9302221536636353
Epoch 30, training loss: 19.054346084594727 = 1.9060008525848389 + 2.0 * 8.574172973632812
Epoch 30, val loss: 1.9111744165420532
Epoch 40, training loss: 18.731868743896484 = 1.8828394412994385 + 2.0 * 8.424514770507812
Epoch 40, val loss: 1.8890703916549683
Epoch 50, training loss: 17.616268157958984 = 1.8608200550079346 + 2.0 * 7.8777241706848145
Epoch 50, val loss: 1.8694230318069458
Epoch 60, training loss: 16.594650268554688 = 1.8485958576202393 + 2.0 * 7.373027324676514
Epoch 60, val loss: 1.8584537506103516
Epoch 70, training loss: 15.910984992980957 = 1.8387495279312134 + 2.0 * 7.0361175537109375
Epoch 70, val loss: 1.8487080335617065
Epoch 80, training loss: 15.538272857666016 = 1.8289027214050293 + 2.0 * 6.854684829711914
Epoch 80, val loss: 1.838465929031372
Epoch 90, training loss: 15.323261260986328 = 1.8171628713607788 + 2.0 * 6.753049373626709
Epoch 90, val loss: 1.8268029689788818
Epoch 100, training loss: 15.174339294433594 = 1.8049975633621216 + 2.0 * 6.684670925140381
Epoch 100, val loss: 1.814713954925537
Epoch 110, training loss: 15.06518840789795 = 1.79338800907135 + 2.0 * 6.635900020599365
Epoch 110, val loss: 1.8029558658599854
Epoch 120, training loss: 14.985376358032227 = 1.7823631763458252 + 2.0 * 6.60150671005249
Epoch 120, val loss: 1.7915581464767456
Epoch 130, training loss: 14.91518783569336 = 1.7713401317596436 + 2.0 * 6.571923732757568
Epoch 130, val loss: 1.7804803848266602
Epoch 140, training loss: 14.854532241821289 = 1.75984525680542 + 2.0 * 6.547343730926514
Epoch 140, val loss: 1.7694591283798218
Epoch 150, training loss: 14.799429893493652 = 1.7473396062850952 + 2.0 * 6.526045322418213
Epoch 150, val loss: 1.7580825090408325
Epoch 160, training loss: 14.75544548034668 = 1.733442783355713 + 2.0 * 6.5110015869140625
Epoch 160, val loss: 1.745862603187561
Epoch 170, training loss: 14.705122947692871 = 1.7178858518600464 + 2.0 * 6.493618488311768
Epoch 170, val loss: 1.7325183153152466
Epoch 180, training loss: 14.657319068908691 = 1.7003110647201538 + 2.0 * 6.478504180908203
Epoch 180, val loss: 1.7175571918487549
Epoch 190, training loss: 14.613763809204102 = 1.6802915334701538 + 2.0 * 6.466736316680908
Epoch 190, val loss: 1.7007304430007935
Epoch 200, training loss: 14.573057174682617 = 1.6578096151351929 + 2.0 * 6.4576239585876465
Epoch 200, val loss: 1.6819790601730347
Epoch 210, training loss: 14.527828216552734 = 1.6327643394470215 + 2.0 * 6.4475321769714355
Epoch 210, val loss: 1.6613062620162964
Epoch 220, training loss: 14.48022747039795 = 1.6051727533340454 + 2.0 * 6.437527179718018
Epoch 220, val loss: 1.638735055923462
Epoch 230, training loss: 14.441844940185547 = 1.5748286247253418 + 2.0 * 6.433508396148682
Epoch 230, val loss: 1.6141963005065918
Epoch 240, training loss: 14.387166976928711 = 1.5422452688217163 + 2.0 * 6.422461032867432
Epoch 240, val loss: 1.588180661201477
Epoch 250, training loss: 14.339160919189453 = 1.5077824592590332 + 2.0 * 6.415689468383789
Epoch 250, val loss: 1.5609835386276245
Epoch 260, training loss: 14.29599380493164 = 1.4717897176742554 + 2.0 * 6.412102222442627
Epoch 260, val loss: 1.5329548120498657
Epoch 270, training loss: 14.245278358459473 = 1.4351154565811157 + 2.0 * 6.405081272125244
Epoch 270, val loss: 1.5050865411758423
Epoch 280, training loss: 14.196051597595215 = 1.398191213607788 + 2.0 * 6.398930072784424
Epoch 280, val loss: 1.4775941371917725
Epoch 290, training loss: 14.158136367797852 = 1.361763834953308 + 2.0 * 6.398186206817627
Epoch 290, val loss: 1.4513190984725952
Epoch 300, training loss: 14.103863716125488 = 1.3264665603637695 + 2.0 * 6.388698577880859
Epoch 300, val loss: 1.426466464996338
Epoch 310, training loss: 14.061022758483887 = 1.2921384572982788 + 2.0 * 6.384442329406738
Epoch 310, val loss: 1.402845025062561
Epoch 320, training loss: 14.0183744430542 = 1.2587007284164429 + 2.0 * 6.3798370361328125
Epoch 320, val loss: 1.3801047801971436
Epoch 330, training loss: 13.984033584594727 = 1.2259469032287598 + 2.0 * 6.379043102264404
Epoch 330, val loss: 1.3580433130264282
Epoch 340, training loss: 13.945229530334473 = 1.1941825151443481 + 2.0 * 6.375523567199707
Epoch 340, val loss: 1.3366448879241943
Epoch 350, training loss: 13.906694412231445 = 1.1630293130874634 + 2.0 * 6.371832370758057
Epoch 350, val loss: 1.3157395124435425
Epoch 360, training loss: 13.865333557128906 = 1.1323771476745605 + 2.0 * 6.366478443145752
Epoch 360, val loss: 1.295046329498291
Epoch 370, training loss: 13.826025009155273 = 1.1019330024719238 + 2.0 * 6.362045764923096
Epoch 370, val loss: 1.2742061614990234
Epoch 380, training loss: 13.791585922241211 = 1.0713671445846558 + 2.0 * 6.360109329223633
Epoch 380, val loss: 1.2532037496566772
Epoch 390, training loss: 13.75187873840332 = 1.0407660007476807 + 2.0 * 6.355556488037109
Epoch 390, val loss: 1.2321141958236694
Epoch 400, training loss: 13.715811729431152 = 1.0101557970046997 + 2.0 * 6.352828025817871
Epoch 400, val loss: 1.2107810974121094
Epoch 410, training loss: 13.68051528930664 = 0.9793855547904968 + 2.0 * 6.350564956665039
Epoch 410, val loss: 1.189361572265625
Epoch 420, training loss: 13.64785385131836 = 0.9485639333724976 + 2.0 * 6.349645137786865
Epoch 420, val loss: 1.167867660522461
Epoch 430, training loss: 13.613409042358398 = 0.918056845664978 + 2.0 * 6.3476762771606445
Epoch 430, val loss: 1.1466827392578125
Epoch 440, training loss: 13.575400352478027 = 0.8879144787788391 + 2.0 * 6.343742847442627
Epoch 440, val loss: 1.1258114576339722
Epoch 450, training loss: 13.53991413116455 = 0.8581812977790833 + 2.0 * 6.340866565704346
Epoch 450, val loss: 1.1053885221481323
Epoch 460, training loss: 13.513336181640625 = 0.8288978934288025 + 2.0 * 6.342219352722168
Epoch 460, val loss: 1.0857996940612793
Epoch 470, training loss: 13.48756217956543 = 0.8006278276443481 + 2.0 * 6.3434672355651855
Epoch 470, val loss: 1.066941261291504
Epoch 480, training loss: 13.445290565490723 = 0.77326899766922 + 2.0 * 6.336010932922363
Epoch 480, val loss: 1.0493311882019043
Epoch 490, training loss: 13.411903381347656 = 0.7466723918914795 + 2.0 * 6.332615375518799
Epoch 490, val loss: 1.032860279083252
Epoch 500, training loss: 13.382418632507324 = 0.7208373546600342 + 2.0 * 6.3307905197143555
Epoch 500, val loss: 1.0174108743667603
Epoch 510, training loss: 13.355356216430664 = 0.6957685351371765 + 2.0 * 6.329793930053711
Epoch 510, val loss: 1.0032016038894653
Epoch 520, training loss: 13.326292037963867 = 0.67162024974823 + 2.0 * 6.327335834503174
Epoch 520, val loss: 0.9901471138000488
Epoch 530, training loss: 13.298358917236328 = 0.6480939388275146 + 2.0 * 6.325132369995117
Epoch 530, val loss: 0.9783787727355957
Epoch 540, training loss: 13.27274227142334 = 0.6250327825546265 + 2.0 * 6.323854923248291
Epoch 540, val loss: 0.9676616787910461
Epoch 550, training loss: 13.25584888458252 = 0.6024989485740662 + 2.0 * 6.326674938201904
Epoch 550, val loss: 0.9580722451210022
Epoch 560, training loss: 13.231042861938477 = 0.5806041359901428 + 2.0 * 6.32521915435791
Epoch 560, val loss: 0.9496020078659058
Epoch 570, training loss: 13.199919700622559 = 0.5592178106307983 + 2.0 * 6.3203511238098145
Epoch 570, val loss: 0.9422286152839661
Epoch 580, training loss: 13.174562454223633 = 0.5382601022720337 + 2.0 * 6.318150997161865
Epoch 580, val loss: 0.9358536005020142
Epoch 590, training loss: 13.157605171203613 = 0.5177252888679504 + 2.0 * 6.319940090179443
Epoch 590, val loss: 0.930428683757782
Epoch 600, training loss: 13.12813663482666 = 0.49771517515182495 + 2.0 * 6.315210819244385
Epoch 600, val loss: 0.9258787035942078
Epoch 610, training loss: 13.1055326461792 = 0.478158563375473 + 2.0 * 6.313686847686768
Epoch 610, val loss: 0.9223222136497498
Epoch 620, training loss: 13.093323707580566 = 0.45904210209846497 + 2.0 * 6.317140579223633
Epoch 620, val loss: 0.9194917678833008
Epoch 630, training loss: 13.064496994018555 = 0.44039979577064514 + 2.0 * 6.312048435211182
Epoch 630, val loss: 0.9174073934555054
Epoch 640, training loss: 13.041755676269531 = 0.42219460010528564 + 2.0 * 6.309780597686768
Epoch 640, val loss: 0.9160199165344238
Epoch 650, training loss: 13.033904075622559 = 0.40444695949554443 + 2.0 * 6.314728736877441
Epoch 650, val loss: 0.9152692556381226
Epoch 660, training loss: 13.0216703414917 = 0.3872985243797302 + 2.0 * 6.317185878753662
Epoch 660, val loss: 0.9151244759559631
Epoch 670, training loss: 12.990189552307129 = 0.37066856026649475 + 2.0 * 6.309760570526123
Epoch 670, val loss: 0.9155796766281128
Epoch 680, training loss: 12.966642379760742 = 0.35460200905799866 + 2.0 * 6.306020259857178
Epoch 680, val loss: 0.9164997339248657
Epoch 690, training loss: 12.947067260742188 = 0.33898162841796875 + 2.0 * 6.304042816162109
Epoch 690, val loss: 0.9178586602210999
Epoch 700, training loss: 12.929312705993652 = 0.32382914423942566 + 2.0 * 6.302742004394531
Epoch 700, val loss: 0.919739305973053
Epoch 710, training loss: 12.924016952514648 = 0.3091670274734497 + 2.0 * 6.307425022125244
Epoch 710, val loss: 0.9220041632652283
Epoch 720, training loss: 12.914216995239258 = 0.29520490765571594 + 2.0 * 6.309505939483643
Epoch 720, val loss: 0.9245641827583313
Epoch 730, training loss: 12.887035369873047 = 0.28186747431755066 + 2.0 * 6.302584171295166
Epoch 730, val loss: 0.9277800917625427
Epoch 740, training loss: 12.86849308013916 = 0.2691046893596649 + 2.0 * 6.299694061279297
Epoch 740, val loss: 0.9314179420471191
Epoch 750, training loss: 12.864775657653809 = 0.25688114762306213 + 2.0 * 6.303947448730469
Epoch 750, val loss: 0.93541020154953
Epoch 760, training loss: 12.842602729797363 = 0.24530920386314392 + 2.0 * 6.298646926879883
Epoch 760, val loss: 0.9397446513175964
Epoch 770, training loss: 12.82768726348877 = 0.2342810481786728 + 2.0 * 6.296703338623047
Epoch 770, val loss: 0.9446998834609985
Epoch 780, training loss: 12.813551902770996 = 0.2238064408302307 + 2.0 * 6.294872760772705
Epoch 780, val loss: 0.9499303698539734
Epoch 790, training loss: 12.821056365966797 = 0.2138335406780243 + 2.0 * 6.3036112785339355
Epoch 790, val loss: 0.9556395411491394
Epoch 800, training loss: 12.798498153686523 = 0.20446303486824036 + 2.0 * 6.297017574310303
Epoch 800, val loss: 0.9615686535835266
Epoch 810, training loss: 12.78225040435791 = 0.195583313703537 + 2.0 * 6.293333530426025
Epoch 810, val loss: 0.9680538773536682
Epoch 820, training loss: 12.770297050476074 = 0.1871640384197235 + 2.0 * 6.291566371917725
Epoch 820, val loss: 0.9748579263687134
Epoch 830, training loss: 12.776397705078125 = 0.17917399108409882 + 2.0 * 6.298611640930176
Epoch 830, val loss: 0.9820565581321716
Epoch 840, training loss: 12.77363109588623 = 0.1716262400150299 + 2.0 * 6.301002502441406
Epoch 840, val loss: 0.9892999529838562
Epoch 850, training loss: 12.74742317199707 = 0.16450399160385132 + 2.0 * 6.291459560394287
Epoch 850, val loss: 0.9968845844268799
Epoch 860, training loss: 12.734051704406738 = 0.15777266025543213 + 2.0 * 6.288139343261719
Epoch 860, val loss: 1.0048907995224
Epoch 870, training loss: 12.726480484008789 = 0.15135230123996735 + 2.0 * 6.287564277648926
Epoch 870, val loss: 1.0130820274353027
Epoch 880, training loss: 12.723382949829102 = 0.14523936808109283 + 2.0 * 6.289071559906006
Epoch 880, val loss: 1.0214934349060059
Epoch 890, training loss: 12.712641716003418 = 0.1394382119178772 + 2.0 * 6.286601543426514
Epoch 890, val loss: 1.0298995971679688
Epoch 900, training loss: 12.70516300201416 = 0.1339259296655655 + 2.0 * 6.285618305206299
Epoch 900, val loss: 1.038584589958191
Epoch 910, training loss: 12.699202537536621 = 0.1286953091621399 + 2.0 * 6.285253524780273
Epoch 910, val loss: 1.0474340915679932
Epoch 920, training loss: 12.707595825195312 = 0.12371779978275299 + 2.0 * 6.291938781738281
Epoch 920, val loss: 1.0562928915023804
Epoch 930, training loss: 12.689061164855957 = 0.11897967755794525 + 2.0 * 6.285040855407715
Epoch 930, val loss: 1.0651321411132812
Epoch 940, training loss: 12.680109024047852 = 0.11449585109949112 + 2.0 * 6.282806396484375
Epoch 940, val loss: 1.074182152748108
Epoch 950, training loss: 12.673508644104004 = 0.11021534353494644 + 2.0 * 6.281646728515625
Epoch 950, val loss: 1.08324134349823
Epoch 960, training loss: 12.6786470413208 = 0.10612931847572327 + 2.0 * 6.286258697509766
Epoch 960, val loss: 1.0923583507537842
Epoch 970, training loss: 12.672828674316406 = 0.10225103050470352 + 2.0 * 6.2852888107299805
Epoch 970, val loss: 1.1013027429580688
Epoch 980, training loss: 12.657707214355469 = 0.0985649973154068 + 2.0 * 6.279571056365967
Epoch 980, val loss: 1.1104705333709717
Epoch 990, training loss: 12.656368255615234 = 0.09504693001508713 + 2.0 * 6.280660629272461
Epoch 990, val loss: 1.119625449180603
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7740740740740741
0.8065366367949395
=== training gcn model ===
Epoch 0, training loss: 19.129825592041016 = 1.936217188835144 + 2.0 * 8.59680461883545
Epoch 0, val loss: 1.9450801610946655
Epoch 10, training loss: 19.119775772094727 = 1.9270881414413452 + 2.0 * 8.596343994140625
Epoch 10, val loss: 1.9353359937667847
Epoch 20, training loss: 19.101980209350586 = 1.9157414436340332 + 2.0 * 8.593119621276855
Epoch 20, val loss: 1.9230778217315674
Epoch 30, training loss: 19.04054832458496 = 1.9003655910491943 + 2.0 * 8.570091247558594
Epoch 30, val loss: 1.9065418243408203
Epoch 40, training loss: 18.720129013061523 = 1.881827712059021 + 2.0 * 8.419150352478027
Epoch 40, val loss: 1.8875503540039062
Epoch 50, training loss: 17.5904541015625 = 1.8612487316131592 + 2.0 * 7.864602565765381
Epoch 50, val loss: 1.867305874824524
Epoch 60, training loss: 16.794801712036133 = 1.8447234630584717 + 2.0 * 7.475039005279541
Epoch 60, val loss: 1.8520874977111816
Epoch 70, training loss: 16.094715118408203 = 1.836040735244751 + 2.0 * 7.129337310791016
Epoch 70, val loss: 1.8440381288528442
Epoch 80, training loss: 15.806462287902832 = 1.8251827955245972 + 2.0 * 6.990639686584473
Epoch 80, val loss: 1.8338521718978882
Epoch 90, training loss: 15.592842102050781 = 1.8119399547576904 + 2.0 * 6.890450954437256
Epoch 90, val loss: 1.8217737674713135
Epoch 100, training loss: 15.408527374267578 = 1.8011609315872192 + 2.0 * 6.803683280944824
Epoch 100, val loss: 1.811644196510315
Epoch 110, training loss: 15.28048038482666 = 1.7915630340576172 + 2.0 * 6.7444586753845215
Epoch 110, val loss: 1.802121639251709
Epoch 120, training loss: 15.169784545898438 = 1.7814112901687622 + 2.0 * 6.694186687469482
Epoch 120, val loss: 1.7921068668365479
Epoch 130, training loss: 15.081184387207031 = 1.770560383796692 + 2.0 * 6.6553120613098145
Epoch 130, val loss: 1.7817543745040894
Epoch 140, training loss: 15.004480361938477 = 1.759115219116211 + 2.0 * 6.622682571411133
Epoch 140, val loss: 1.7710294723510742
Epoch 150, training loss: 14.936452865600586 = 1.746779203414917 + 2.0 * 6.594836711883545
Epoch 150, val loss: 1.7596044540405273
Epoch 160, training loss: 14.87645435333252 = 1.7329730987548828 + 2.0 * 6.571740627288818
Epoch 160, val loss: 1.747193455696106
Epoch 170, training loss: 14.815183639526367 = 1.7173577547073364 + 2.0 * 6.54891300201416
Epoch 170, val loss: 1.733330488204956
Epoch 180, training loss: 14.75776481628418 = 1.6995692253112793 + 2.0 * 6.529097557067871
Epoch 180, val loss: 1.7177743911743164
Epoch 190, training loss: 14.707042694091797 = 1.6791772842407227 + 2.0 * 6.513932704925537
Epoch 190, val loss: 1.700096845626831
Epoch 200, training loss: 14.652002334594727 = 1.6558945178985596 + 2.0 * 6.498054027557373
Epoch 200, val loss: 1.679969310760498
Epoch 210, training loss: 14.60042953491211 = 1.6294896602630615 + 2.0 * 6.485469818115234
Epoch 210, val loss: 1.657167673110962
Epoch 220, training loss: 14.55610466003418 = 1.599623441696167 + 2.0 * 6.478240489959717
Epoch 220, val loss: 1.6314777135849
Epoch 230, training loss: 14.49585247039795 = 1.5664156675338745 + 2.0 * 6.464718341827393
Epoch 230, val loss: 1.6030385494232178
Epoch 240, training loss: 14.44059944152832 = 1.5297973155975342 + 2.0 * 6.4554009437561035
Epoch 240, val loss: 1.5717086791992188
Epoch 250, training loss: 14.384052276611328 = 1.489661693572998 + 2.0 * 6.447195529937744
Epoch 250, val loss: 1.5376306772232056
Epoch 260, training loss: 14.32797908782959 = 1.4466179609298706 + 2.0 * 6.440680503845215
Epoch 260, val loss: 1.5015379190444946
Epoch 270, training loss: 14.269025802612305 = 1.4017608165740967 + 2.0 * 6.4336323738098145
Epoch 270, val loss: 1.4644521474838257
Epoch 280, training loss: 14.210387229919434 = 1.3555737733840942 + 2.0 * 6.4274067878723145
Epoch 280, val loss: 1.4268490076065063
Epoch 290, training loss: 14.153299331665039 = 1.3090307712554932 + 2.0 * 6.4221343994140625
Epoch 290, val loss: 1.389648675918579
Epoch 300, training loss: 14.094904899597168 = 1.2630707025527954 + 2.0 * 6.415916919708252
Epoch 300, val loss: 1.3536829948425293
Epoch 310, training loss: 14.03803539276123 = 1.2181326150894165 + 2.0 * 6.409951210021973
Epoch 310, val loss: 1.319206953048706
Epoch 320, training loss: 13.986576080322266 = 1.174350619316101 + 2.0 * 6.4061126708984375
Epoch 320, val loss: 1.2861377000808716
Epoch 330, training loss: 13.933074951171875 = 1.1321640014648438 + 2.0 * 6.400455474853516
Epoch 330, val loss: 1.2548812627792358
Epoch 340, training loss: 13.883941650390625 = 1.0916476249694824 + 2.0 * 6.396146774291992
Epoch 340, val loss: 1.2253631353378296
Epoch 350, training loss: 13.84034538269043 = 1.0528382062911987 + 2.0 * 6.393753528594971
Epoch 350, val loss: 1.1976121664047241
Epoch 360, training loss: 13.790261268615723 = 1.0157897472381592 + 2.0 * 6.387235641479492
Epoch 360, val loss: 1.171509861946106
Epoch 370, training loss: 13.746826171875 = 0.9802911877632141 + 2.0 * 6.383267402648926
Epoch 370, val loss: 1.1468970775604248
Epoch 380, training loss: 13.70760726928711 = 0.9459555745124817 + 2.0 * 6.380825996398926
Epoch 380, val loss: 1.123637080192566
Epoch 390, training loss: 13.66672420501709 = 0.9126871228218079 + 2.0 * 6.377018451690674
Epoch 390, val loss: 1.101630687713623
Epoch 400, training loss: 13.631434440612793 = 0.8805725574493408 + 2.0 * 6.375431060791016
Epoch 400, val loss: 1.080836296081543
Epoch 410, training loss: 13.591151237487793 = 0.8495122790336609 + 2.0 * 6.370819568634033
Epoch 410, val loss: 1.0610575675964355
Epoch 420, training loss: 13.55188274383545 = 0.8190688490867615 + 2.0 * 6.3664069175720215
Epoch 420, val loss: 1.0422545671463013
Epoch 430, training loss: 13.520458221435547 = 0.7892014384269714 + 2.0 * 6.365628242492676
Epoch 430, val loss: 1.0241706371307373
Epoch 440, training loss: 13.490013122558594 = 0.7599906325340271 + 2.0 * 6.365011215209961
Epoch 440, val loss: 1.0069202184677124
Epoch 450, training loss: 13.44962215423584 = 0.7316063046455383 + 2.0 * 6.359007835388184
Epoch 450, val loss: 0.9906194806098938
Epoch 460, training loss: 13.414325714111328 = 0.7037705183029175 + 2.0 * 6.3552775382995605
Epoch 460, val loss: 0.9753583073616028
Epoch 470, training loss: 13.382984161376953 = 0.6764349937438965 + 2.0 * 6.353274822235107
Epoch 470, val loss: 0.9608485698699951
Epoch 480, training loss: 13.350576400756836 = 0.6496499180793762 + 2.0 * 6.350463390350342
Epoch 480, val loss: 0.9472708702087402
Epoch 490, training loss: 13.334105491638184 = 0.6235494613647461 + 2.0 * 6.355278015136719
Epoch 490, val loss: 0.9347013235092163
Epoch 500, training loss: 13.291547775268555 = 0.5981422662734985 + 2.0 * 6.346702575683594
Epoch 500, val loss: 0.9231374263763428
Epoch 510, training loss: 13.26062297821045 = 0.5733950734138489 + 2.0 * 6.343614101409912
Epoch 510, val loss: 0.9126390814781189
Epoch 520, training loss: 13.246278762817383 = 0.5492773652076721 + 2.0 * 6.348500728607178
Epoch 520, val loss: 0.9031887054443359
Epoch 530, training loss: 13.207093238830566 = 0.5260038375854492 + 2.0 * 6.340544700622559
Epoch 530, val loss: 0.8949065208435059
Epoch 540, training loss: 13.180721282958984 = 0.5034569501876831 + 2.0 * 6.338632106781006
Epoch 540, val loss: 0.8877396583557129
Epoch 550, training loss: 13.158258438110352 = 0.48159512877464294 + 2.0 * 6.338331699371338
Epoch 550, val loss: 0.8816152215003967
Epoch 560, training loss: 13.12992000579834 = 0.46052154898643494 + 2.0 * 6.3346991539001465
Epoch 560, val loss: 0.8765055537223816
Epoch 570, training loss: 13.110960006713867 = 0.4401616156101227 + 2.0 * 6.335399150848389
Epoch 570, val loss: 0.8724271655082703
Epoch 580, training loss: 13.084539413452148 = 0.42072951793670654 + 2.0 * 6.331904888153076
Epoch 580, val loss: 0.8693156242370605
Epoch 590, training loss: 13.06015682220459 = 0.40201202034950256 + 2.0 * 6.32907247543335
Epoch 590, val loss: 0.8670888543128967
Epoch 600, training loss: 13.046010971069336 = 0.3841327130794525 + 2.0 * 6.330939292907715
Epoch 600, val loss: 0.8657298684120178
Epoch 610, training loss: 13.018948554992676 = 0.36704856157302856 + 2.0 * 6.3259501457214355
Epoch 610, val loss: 0.8651118874549866
Epoch 620, training loss: 12.998895645141602 = 0.3507499694824219 + 2.0 * 6.32407283782959
Epoch 620, val loss: 0.8653469085693359
Epoch 630, training loss: 12.997723579406738 = 0.33516567945480347 + 2.0 * 6.3312788009643555
Epoch 630, val loss: 0.8661983013153076
Epoch 640, training loss: 12.961931228637695 = 0.32034027576446533 + 2.0 * 6.32079553604126
Epoch 640, val loss: 0.867509126663208
Epoch 650, training loss: 12.947641372680664 = 0.3061997890472412 + 2.0 * 6.320720672607422
Epoch 650, val loss: 0.8694435954093933
Epoch 660, training loss: 12.931598663330078 = 0.29270216822624207 + 2.0 * 6.319448471069336
Epoch 660, val loss: 0.871758759021759
Epoch 670, training loss: 12.919578552246094 = 0.27984514832496643 + 2.0 * 6.31986665725708
Epoch 670, val loss: 0.874511182308197
Epoch 680, training loss: 12.899582862854004 = 0.2676107585430145 + 2.0 * 6.315986156463623
Epoch 680, val loss: 0.8774962425231934
Epoch 690, training loss: 12.883386611938477 = 0.2559202313423157 + 2.0 * 6.313733100891113
Epoch 690, val loss: 0.8808323740959167
Epoch 700, training loss: 12.877589225769043 = 0.24474067986011505 + 2.0 * 6.316424369812012
Epoch 700, val loss: 0.8843887448310852
Epoch 710, training loss: 12.859354019165039 = 0.23407426476478577 + 2.0 * 6.3126397132873535
Epoch 710, val loss: 0.8881255388259888
Epoch 720, training loss: 12.844943046569824 = 0.2238720953464508 + 2.0 * 6.310535430908203
Epoch 720, val loss: 0.892204225063324
Epoch 730, training loss: 12.84296989440918 = 0.21413885056972504 + 2.0 * 6.314415454864502
Epoch 730, val loss: 0.8962751030921936
Epoch 740, training loss: 12.82832145690918 = 0.20483505725860596 + 2.0 * 6.311743259429932
Epoch 740, val loss: 0.9003986120223999
Epoch 750, training loss: 12.810137748718262 = 0.1960311084985733 + 2.0 * 6.307053089141846
Epoch 750, val loss: 0.9048911333084106
Epoch 760, training loss: 12.798946380615234 = 0.18762390315532684 + 2.0 * 6.305661201477051
Epoch 760, val loss: 0.9093532562255859
Epoch 770, training loss: 12.794681549072266 = 0.17957431077957153 + 2.0 * 6.307553768157959
Epoch 770, val loss: 0.9139649271965027
Epoch 780, training loss: 12.782022476196289 = 0.17188981175422668 + 2.0 * 6.305066108703613
Epoch 780, val loss: 0.9185905456542969
Epoch 790, training loss: 12.768386840820312 = 0.16458994150161743 + 2.0 * 6.30189847946167
Epoch 790, val loss: 0.9233576655387878
Epoch 800, training loss: 12.759652137756348 = 0.15761646628379822 + 2.0 * 6.301017761230469
Epoch 800, val loss: 0.9282000660896301
Epoch 810, training loss: 12.750574111938477 = 0.15097935497760773 + 2.0 * 6.299797534942627
Epoch 810, val loss: 0.9330586194992065
Epoch 820, training loss: 12.743769645690918 = 0.14468996226787567 + 2.0 * 6.299540042877197
Epoch 820, val loss: 0.9380043148994446
Epoch 830, training loss: 12.735086441040039 = 0.138702392578125 + 2.0 * 6.298192024230957
Epoch 830, val loss: 0.9430454969406128
Epoch 840, training loss: 12.726927757263184 = 0.132985457777977 + 2.0 * 6.296971321105957
Epoch 840, val loss: 0.9481215476989746
Epoch 850, training loss: 12.723673820495605 = 0.12754715979099274 + 2.0 * 6.298063278198242
Epoch 850, val loss: 0.953131914138794
Epoch 860, training loss: 12.721219062805176 = 0.12240508943796158 + 2.0 * 6.299407005310059
Epoch 860, val loss: 0.9582986235618591
Epoch 870, training loss: 12.709104537963867 = 0.11752991378307343 + 2.0 * 6.295787334442139
Epoch 870, val loss: 0.9634220600128174
Epoch 880, training loss: 12.69931411743164 = 0.11288061738014221 + 2.0 * 6.293216705322266
Epoch 880, val loss: 0.9685043692588806
Epoch 890, training loss: 12.695173263549805 = 0.10845419764518738 + 2.0 * 6.293359756469727
Epoch 890, val loss: 0.9736346006393433
Epoch 900, training loss: 12.697088241577148 = 0.10423719137907028 + 2.0 * 6.2964253425598145
Epoch 900, val loss: 0.9787536859512329
Epoch 910, training loss: 12.68519115447998 = 0.10024343430995941 + 2.0 * 6.292473793029785
Epoch 910, val loss: 0.9840590953826904
Epoch 920, training loss: 12.681282043457031 = 0.09643852710723877 + 2.0 * 6.292421817779541
Epoch 920, val loss: 0.9891858100891113
Epoch 930, training loss: 12.672310829162598 = 0.09282860159873962 + 2.0 * 6.289741039276123
Epoch 930, val loss: 0.9943045377731323
Epoch 940, training loss: 12.664321899414062 = 0.08939050137996674 + 2.0 * 6.287465572357178
Epoch 940, val loss: 0.9994502067565918
Epoch 950, training loss: 12.659977912902832 = 0.0861138105392456 + 2.0 * 6.286931991577148
Epoch 950, val loss: 1.0045344829559326
Epoch 960, training loss: 12.661179542541504 = 0.08298883587121964 + 2.0 * 6.289095401763916
Epoch 960, val loss: 1.0096420049667358
Epoch 970, training loss: 12.651725769042969 = 0.08001009374856949 + 2.0 * 6.285857677459717
Epoch 970, val loss: 1.014581561088562
Epoch 980, training loss: 12.64801025390625 = 0.0771733969449997 + 2.0 * 6.285418510437012
Epoch 980, val loss: 1.0197213888168335
Epoch 990, training loss: 12.644854545593262 = 0.07445922493934631 + 2.0 * 6.285197734832764
Epoch 990, val loss: 1.0246270895004272
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7555555555555555
0.8054823405376912
The final CL Acc:0.74568, 0.02810, The final GNN Acc:0.80829, 0.00326
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13244])
remove edge: torch.Size([2, 7916])
updated graph: torch.Size([2, 10604])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 19.118417739868164 = 1.9247791767120361 + 2.0 * 8.596818923950195
Epoch 0, val loss: 1.919805645942688
Epoch 10, training loss: 19.108112335205078 = 1.9153892993927002 + 2.0 * 8.59636116027832
Epoch 10, val loss: 1.9103554487228394
Epoch 20, training loss: 19.089252471923828 = 1.9036493301391602 + 2.0 * 8.592802047729492
Epoch 20, val loss: 1.898682713508606
Epoch 30, training loss: 19.01806640625 = 1.8876601457595825 + 2.0 * 8.565202713012695
Epoch 30, val loss: 1.8830856084823608
Epoch 40, training loss: 18.60232925415039 = 1.8679686784744263 + 2.0 * 8.367179870605469
Epoch 40, val loss: 1.8642587661743164
Epoch 50, training loss: 17.687885284423828 = 1.8456822633743286 + 2.0 * 7.9211015701293945
Epoch 50, val loss: 1.842679500579834
Epoch 60, training loss: 17.06893539428711 = 1.8275635242462158 + 2.0 * 7.620685577392578
Epoch 60, val loss: 1.8267195224761963
Epoch 70, training loss: 16.541637420654297 = 1.8159196376800537 + 2.0 * 7.36285924911499
Epoch 70, val loss: 1.8170833587646484
Epoch 80, training loss: 16.037485122680664 = 1.8048388957977295 + 2.0 * 7.116323471069336
Epoch 80, val loss: 1.8071447610855103
Epoch 90, training loss: 15.640303611755371 = 1.7946442365646362 + 2.0 * 6.922829627990723
Epoch 90, val loss: 1.7979927062988281
Epoch 100, training loss: 15.419173240661621 = 1.784300684928894 + 2.0 * 6.817436218261719
Epoch 100, val loss: 1.788880467414856
Epoch 110, training loss: 15.255627632141113 = 1.7724195718765259 + 2.0 * 6.741603851318359
Epoch 110, val loss: 1.7789454460144043
Epoch 120, training loss: 15.14080810546875 = 1.7604937553405762 + 2.0 * 6.690157413482666
Epoch 120, val loss: 1.7686352729797363
Epoch 130, training loss: 15.044742584228516 = 1.7479283809661865 + 2.0 * 6.648406982421875
Epoch 130, val loss: 1.757402777671814
Epoch 140, training loss: 14.989471435546875 = 1.7337944507598877 + 2.0 * 6.627838611602783
Epoch 140, val loss: 1.7448461055755615
Epoch 150, training loss: 14.897011756896973 = 1.7180774211883545 + 2.0 * 6.5894670486450195
Epoch 150, val loss: 1.7309986352920532
Epoch 160, training loss: 14.83260726928711 = 1.7005656957626343 + 2.0 * 6.566020965576172
Epoch 160, val loss: 1.7158172130584717
Epoch 170, training loss: 14.77027416229248 = 1.6807855367660522 + 2.0 * 6.544744491577148
Epoch 170, val loss: 1.698831558227539
Epoch 180, training loss: 14.708587646484375 = 1.6583290100097656 + 2.0 * 6.525129318237305
Epoch 180, val loss: 1.6797254085540771
Epoch 190, training loss: 14.650215148925781 = 1.632741093635559 + 2.0 * 6.508737087249756
Epoch 190, val loss: 1.6579004526138306
Epoch 200, training loss: 14.60257339477539 = 1.6036410331726074 + 2.0 * 6.4994659423828125
Epoch 200, val loss: 1.6329951286315918
Epoch 210, training loss: 14.536770820617676 = 1.5711108446121216 + 2.0 * 6.482830047607422
Epoch 210, val loss: 1.6052430868148804
Epoch 220, training loss: 14.478857040405273 = 1.5351028442382812 + 2.0 * 6.471877098083496
Epoch 220, val loss: 1.5745478868484497
Epoch 230, training loss: 14.422855377197266 = 1.4957730770111084 + 2.0 * 6.463541030883789
Epoch 230, val loss: 1.5410568714141846
Epoch 240, training loss: 14.359981536865234 = 1.4537359476089478 + 2.0 * 6.453122615814209
Epoch 240, val loss: 1.5053293704986572
Epoch 250, training loss: 14.299298286437988 = 1.4093540906906128 + 2.0 * 6.444972038269043
Epoch 250, val loss: 1.4676777124404907
Epoch 260, training loss: 14.238842010498047 = 1.3634554147720337 + 2.0 * 6.437693119049072
Epoch 260, val loss: 1.4291850328445435
Epoch 270, training loss: 14.175033569335938 = 1.316930890083313 + 2.0 * 6.429051399230957
Epoch 270, val loss: 1.3903297185897827
Epoch 280, training loss: 14.114036560058594 = 1.2699543237686157 + 2.0 * 6.422040939331055
Epoch 280, val loss: 1.3513818979263306
Epoch 290, training loss: 14.057456016540527 = 1.2231947183609009 + 2.0 * 6.417130470275879
Epoch 290, val loss: 1.3129087686538696
Epoch 300, training loss: 13.998950004577637 = 1.177599310874939 + 2.0 * 6.410675525665283
Epoch 300, val loss: 1.275970458984375
Epoch 310, training loss: 13.942330360412598 = 1.1332260370254517 + 2.0 * 6.404551982879639
Epoch 310, val loss: 1.240349531173706
Epoch 320, training loss: 13.888395309448242 = 1.0899296998977661 + 2.0 * 6.399232864379883
Epoch 320, val loss: 1.206162691116333
Epoch 330, training loss: 13.849615097045898 = 1.048081398010254 + 2.0 * 6.400766849517822
Epoch 330, val loss: 1.1734944581985474
Epoch 340, training loss: 13.79098129272461 = 1.0082646608352661 + 2.0 * 6.391358375549316
Epoch 340, val loss: 1.1429922580718994
Epoch 350, training loss: 13.74050521850586 = 0.9699877500534058 + 2.0 * 6.385258674621582
Epoch 350, val loss: 1.1141324043273926
Epoch 360, training loss: 13.694929122924805 = 0.9328990578651428 + 2.0 * 6.381014823913574
Epoch 360, val loss: 1.0865312814712524
Epoch 370, training loss: 13.666720390319824 = 0.8968446254730225 + 2.0 * 6.384937763214111
Epoch 370, val loss: 1.0602384805679321
Epoch 380, training loss: 13.614566802978516 = 0.8622385263442993 + 2.0 * 6.376163959503174
Epoch 380, val loss: 1.035254716873169
Epoch 390, training loss: 13.571358680725098 = 0.8287099003791809 + 2.0 * 6.37132453918457
Epoch 390, val loss: 1.0114974975585938
Epoch 400, training loss: 13.539871215820312 = 0.7960679531097412 + 2.0 * 6.371901512145996
Epoch 400, val loss: 0.9886131882667542
Epoch 410, training loss: 13.491889953613281 = 0.7644273638725281 + 2.0 * 6.363731384277344
Epoch 410, val loss: 0.9667301774024963
Epoch 420, training loss: 13.454312324523926 = 0.7335445284843445 + 2.0 * 6.360383987426758
Epoch 420, val loss: 0.945614755153656
Epoch 430, training loss: 13.42369270324707 = 0.7033528685569763 + 2.0 * 6.360169887542725
Epoch 430, val loss: 0.925158679485321
Epoch 440, training loss: 13.38919448852539 = 0.674105703830719 + 2.0 * 6.357544422149658
Epoch 440, val loss: 0.9054856896400452
Epoch 450, training loss: 13.350319862365723 = 0.6457110047340393 + 2.0 * 6.352304458618164
Epoch 450, val loss: 0.8867457509040833
Epoch 460, training loss: 13.324383735656738 = 0.6181485056877136 + 2.0 * 6.3531174659729
Epoch 460, val loss: 0.8688315749168396
Epoch 470, training loss: 13.286791801452637 = 0.5914767980575562 + 2.0 * 6.347657680511475
Epoch 470, val loss: 0.8517863750457764
Epoch 480, training loss: 13.260926246643066 = 0.5657002329826355 + 2.0 * 6.3476128578186035
Epoch 480, val loss: 0.8355046510696411
Epoch 490, training loss: 13.225790023803711 = 0.5408369898796082 + 2.0 * 6.3424763679504395
Epoch 490, val loss: 0.8201364874839783
Epoch 500, training loss: 13.20034408569336 = 0.5168652534484863 + 2.0 * 6.341739177703857
Epoch 500, val loss: 0.805657148361206
Epoch 510, training loss: 13.171196937561035 = 0.49383822083473206 + 2.0 * 6.338679313659668
Epoch 510, val loss: 0.7921086549758911
Epoch 520, training loss: 13.144269943237305 = 0.4718463718891144 + 2.0 * 6.336211681365967
Epoch 520, val loss: 0.779457688331604
Epoch 530, training loss: 13.116537094116211 = 0.45082831382751465 + 2.0 * 6.332854270935059
Epoch 530, val loss: 0.7677826881408691
Epoch 540, training loss: 13.10503101348877 = 0.4307415187358856 + 2.0 * 6.33714485168457
Epoch 540, val loss: 0.7570205330848694
Epoch 550, training loss: 13.074045181274414 = 0.411778062582016 + 2.0 * 6.3311333656311035
Epoch 550, val loss: 0.7472125291824341
Epoch 560, training loss: 13.04787540435791 = 0.3938505947589874 + 2.0 * 6.327012538909912
Epoch 560, val loss: 0.7383898496627808
Epoch 570, training loss: 13.025818824768066 = 0.3768683969974518 + 2.0 * 6.324475288391113
Epoch 570, val loss: 0.7304577231407166
Epoch 580, training loss: 13.009881973266602 = 0.3607715964317322 + 2.0 * 6.324555397033691
Epoch 580, val loss: 0.7233031988143921
Epoch 590, training loss: 12.990878105163574 = 0.34558314085006714 + 2.0 * 6.322647571563721
Epoch 590, val loss: 0.7169431447982788
Epoch 600, training loss: 12.981584548950195 = 0.33124053478240967 + 2.0 * 6.325171947479248
Epoch 600, val loss: 0.7113912105560303
Epoch 610, training loss: 12.96190357208252 = 0.3177034854888916 + 2.0 * 6.3221001625061035
Epoch 610, val loss: 0.7065658569335938
Epoch 620, training loss: 12.936993598937988 = 0.304905503988266 + 2.0 * 6.316043853759766
Epoch 620, val loss: 0.7024106979370117
Epoch 630, training loss: 12.921494483947754 = 0.29268839955329895 + 2.0 * 6.314403057098389
Epoch 630, val loss: 0.6987707614898682
Epoch 640, training loss: 12.910602569580078 = 0.2809557020664215 + 2.0 * 6.314823627471924
Epoch 640, val loss: 0.695676326751709
Epoch 650, training loss: 12.895279884338379 = 0.26966592669487 + 2.0 * 6.312807083129883
Epoch 650, val loss: 0.6930354237556458
Epoch 660, training loss: 12.883131980895996 = 0.25880104303359985 + 2.0 * 6.312165260314941
Epoch 660, val loss: 0.6908528804779053
Epoch 670, training loss: 12.867338180541992 = 0.24830441176891327 + 2.0 * 6.309516906738281
Epoch 670, val loss: 0.6890759468078613
Epoch 680, training loss: 12.852407455444336 = 0.2381037324666977 + 2.0 * 6.307151794433594
Epoch 680, val loss: 0.6876634359359741
Epoch 690, training loss: 12.840435028076172 = 0.2281550168991089 + 2.0 * 6.306139945983887
Epoch 690, val loss: 0.6865208148956299
Epoch 700, training loss: 12.841066360473633 = 0.21841230988502502 + 2.0 * 6.31132698059082
Epoch 700, val loss: 0.6856541633605957
Epoch 710, training loss: 12.8246488571167 = 0.20894476771354675 + 2.0 * 6.307852268218994
Epoch 710, val loss: 0.6850648522377014
Epoch 720, training loss: 12.805628776550293 = 0.19970978796482086 + 2.0 * 6.302959442138672
Epoch 720, val loss: 0.6846922636032104
Epoch 730, training loss: 12.792729377746582 = 0.19074369966983795 + 2.0 * 6.300992965698242
Epoch 730, val loss: 0.6845365762710571
Epoch 740, training loss: 12.78441333770752 = 0.18201160430908203 + 2.0 * 6.301200866699219
Epoch 740, val loss: 0.6846181750297546
Epoch 750, training loss: 12.773636817932129 = 0.1735878884792328 + 2.0 * 6.300024509429932
Epoch 750, val loss: 0.6849147081375122
Epoch 760, training loss: 12.767271995544434 = 0.16547824442386627 + 2.0 * 6.300896644592285
Epoch 760, val loss: 0.6854031682014465
Epoch 770, training loss: 12.752016067504883 = 0.15770015120506287 + 2.0 * 6.2971577644348145
Epoch 770, val loss: 0.6860858798027039
Epoch 780, training loss: 12.743006706237793 = 0.15025211870670319 + 2.0 * 6.296377182006836
Epoch 780, val loss: 0.6870328783988953
Epoch 790, training loss: 12.737919807434082 = 0.14312876760959625 + 2.0 * 6.297395706176758
Epoch 790, val loss: 0.688163697719574
Epoch 800, training loss: 12.735339164733887 = 0.13635078072547913 + 2.0 * 6.29949426651001
Epoch 800, val loss: 0.6895202398300171
Epoch 810, training loss: 12.729673385620117 = 0.12992608547210693 + 2.0 * 6.2998738288879395
Epoch 810, val loss: 0.6910151243209839
Epoch 820, training loss: 12.710042953491211 = 0.12382727116346359 + 2.0 * 6.293107986450195
Epoch 820, val loss: 0.6926847696304321
Epoch 830, training loss: 12.699813842773438 = 0.11805702745914459 + 2.0 * 6.2908782958984375
Epoch 830, val loss: 0.694489061832428
Epoch 840, training loss: 12.693380355834961 = 0.1125730499625206 + 2.0 * 6.290403842926025
Epoch 840, val loss: 0.6964966654777527
Epoch 850, training loss: 12.692462921142578 = 0.10737993568181992 + 2.0 * 6.29254150390625
Epoch 850, val loss: 0.6986232399940491
Epoch 860, training loss: 12.68114185333252 = 0.10247206687927246 + 2.0 * 6.289334774017334
Epoch 860, val loss: 0.7008969187736511
Epoch 870, training loss: 12.676584243774414 = 0.09782839566469193 + 2.0 * 6.289377689361572
Epoch 870, val loss: 0.7032658457756042
Epoch 880, training loss: 12.669150352478027 = 0.09344291687011719 + 2.0 * 6.287853717803955
Epoch 880, val loss: 0.7057216167449951
Epoch 890, training loss: 12.66203498840332 = 0.08930458128452301 + 2.0 * 6.286365032196045
Epoch 890, val loss: 0.7083155512809753
Epoch 900, training loss: 12.662311553955078 = 0.08538708835840225 + 2.0 * 6.288462162017822
Epoch 900, val loss: 0.7110180854797363
Epoch 910, training loss: 12.649625778198242 = 0.08168768882751465 + 2.0 * 6.283968925476074
Epoch 910, val loss: 0.7137919068336487
Epoch 920, training loss: 12.649869918823242 = 0.0781984031200409 + 2.0 * 6.2858357429504395
Epoch 920, val loss: 0.7166274785995483
Epoch 930, training loss: 12.63885498046875 = 0.07490716874599457 + 2.0 * 6.281973838806152
Epoch 930, val loss: 0.7195072770118713
Epoch 940, training loss: 12.63575553894043 = 0.0717904344201088 + 2.0 * 6.281982421875
Epoch 940, val loss: 0.7224946618080139
Epoch 950, training loss: 12.63697624206543 = 0.06885377317667007 + 2.0 * 6.284061431884766
Epoch 950, val loss: 0.7254941463470459
Epoch 960, training loss: 12.62700080871582 = 0.06608237326145172 + 2.0 * 6.280459403991699
Epoch 960, val loss: 0.7285754680633545
Epoch 970, training loss: 12.621895790100098 = 0.06345262378454208 + 2.0 * 6.279221534729004
Epoch 970, val loss: 0.7317131161689758
Epoch 980, training loss: 12.626856803894043 = 0.060964155942201614 + 2.0 * 6.2829461097717285
Epoch 980, val loss: 0.7349230051040649
Epoch 990, training loss: 12.622117042541504 = 0.05861341953277588 + 2.0 * 6.28175163269043
Epoch 990, val loss: 0.7380703687667847
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.788888888888889
0.8402741170268846
=== training gcn model ===
Epoch 0, training loss: 19.144163131713867 = 1.9505125284194946 + 2.0 * 8.59682559967041
Epoch 0, val loss: 1.9515857696533203
Epoch 10, training loss: 19.132722854614258 = 1.939799427986145 + 2.0 * 8.596461296081543
Epoch 10, val loss: 1.940489411354065
Epoch 20, training loss: 19.11393928527832 = 1.926156759262085 + 2.0 * 8.593891143798828
Epoch 20, val loss: 1.9263606071472168
Epoch 30, training loss: 19.05636215209961 = 1.9073102474212646 + 2.0 * 8.574525833129883
Epoch 30, val loss: 1.9070384502410889
Epoch 40, training loss: 18.756473541259766 = 1.8840527534484863 + 2.0 * 8.436210632324219
Epoch 40, val loss: 1.8843501806259155
Epoch 50, training loss: 17.57878875732422 = 1.8589439392089844 + 2.0 * 7.859922885894775
Epoch 50, val loss: 1.8607182502746582
Epoch 60, training loss: 16.74273109436035 = 1.8386483192443848 + 2.0 * 7.4520416259765625
Epoch 60, val loss: 1.8425225019454956
Epoch 70, training loss: 16.23549461364746 = 1.8242307901382446 + 2.0 * 7.205631732940674
Epoch 70, val loss: 1.8289415836334229
Epoch 80, training loss: 15.874773025512695 = 1.8095265626907349 + 2.0 * 7.032623291015625
Epoch 80, val loss: 1.8152549266815186
Epoch 90, training loss: 15.617171287536621 = 1.7962702512741089 + 2.0 * 6.910450458526611
Epoch 90, val loss: 1.8031296730041504
Epoch 100, training loss: 15.419276237487793 = 1.782515525817871 + 2.0 * 6.818380355834961
Epoch 100, val loss: 1.7903823852539062
Epoch 110, training loss: 15.275678634643555 = 1.7694048881530762 + 2.0 * 6.75313663482666
Epoch 110, val loss: 1.7783616781234741
Epoch 120, training loss: 15.153593063354492 = 1.7570288181304932 + 2.0 * 6.698282241821289
Epoch 120, val loss: 1.7670440673828125
Epoch 130, training loss: 15.045928955078125 = 1.7442857027053833 + 2.0 * 6.650821685791016
Epoch 130, val loss: 1.7549960613250732
Epoch 140, training loss: 14.954977989196777 = 1.730249047279358 + 2.0 * 6.612364292144775
Epoch 140, val loss: 1.7419320344924927
Epoch 150, training loss: 14.884339332580566 = 1.7147318124771118 + 2.0 * 6.584803581237793
Epoch 150, val loss: 1.727819561958313
Epoch 160, training loss: 14.807600021362305 = 1.6975255012512207 + 2.0 * 6.555037021636963
Epoch 160, val loss: 1.7125619649887085
Epoch 170, training loss: 14.73892593383789 = 1.6784014701843262 + 2.0 * 6.530261993408203
Epoch 170, val loss: 1.6959317922592163
Epoch 180, training loss: 14.679455757141113 = 1.6570682525634766 + 2.0 * 6.511193752288818
Epoch 180, val loss: 1.6775872707366943
Epoch 190, training loss: 14.616805076599121 = 1.6334655284881592 + 2.0 * 6.491669654846191
Epoch 190, val loss: 1.6574089527130127
Epoch 200, training loss: 14.559287071228027 = 1.6074329614639282 + 2.0 * 6.475926876068115
Epoch 200, val loss: 1.6352757215499878
Epoch 210, training loss: 14.501521110534668 = 1.5789077281951904 + 2.0 * 6.461306571960449
Epoch 210, val loss: 1.6112006902694702
Epoch 220, training loss: 14.446345329284668 = 1.5477463006973267 + 2.0 * 6.449299335479736
Epoch 220, val loss: 1.5851703882217407
Epoch 230, training loss: 14.392215728759766 = 1.5142072439193726 + 2.0 * 6.439004421234131
Epoch 230, val loss: 1.5573569536209106
Epoch 240, training loss: 14.336605072021484 = 1.4787650108337402 + 2.0 * 6.428919792175293
Epoch 240, val loss: 1.5283043384552002
Epoch 250, training loss: 14.28162670135498 = 1.441590428352356 + 2.0 * 6.420018196105957
Epoch 250, val loss: 1.498154878616333
Epoch 260, training loss: 14.237055778503418 = 1.4028116464614868 + 2.0 * 6.417121887207031
Epoch 260, val loss: 1.4670552015304565
Epoch 270, training loss: 14.178814888000488 = 1.3633334636688232 + 2.0 * 6.407740592956543
Epoch 270, val loss: 1.4356625080108643
Epoch 280, training loss: 14.121528625488281 = 1.3234220743179321 + 2.0 * 6.39905309677124
Epoch 280, val loss: 1.404226303100586
Epoch 290, training loss: 14.069408416748047 = 1.2833136320114136 + 2.0 * 6.393047332763672
Epoch 290, val loss: 1.3728971481323242
Epoch 300, training loss: 14.028114318847656 = 1.2433769702911377 + 2.0 * 6.392368793487549
Epoch 300, val loss: 1.34201979637146
Epoch 310, training loss: 13.973530769348145 = 1.2044800519943237 + 2.0 * 6.384525299072266
Epoch 310, val loss: 1.3122235536575317
Epoch 320, training loss: 13.923845291137695 = 1.1668704748153687 + 2.0 * 6.378487586975098
Epoch 320, val loss: 1.2836954593658447
Epoch 330, training loss: 13.885790824890137 = 1.1307194232940674 + 2.0 * 6.377535820007324
Epoch 330, val loss: 1.2563831806182861
Epoch 340, training loss: 13.84760570526123 = 1.0965160131454468 + 2.0 * 6.375545024871826
Epoch 340, val loss: 1.2305742502212524
Epoch 350, training loss: 13.795312881469727 = 1.0644919872283936 + 2.0 * 6.365410327911377
Epoch 350, val loss: 1.2067590951919556
Epoch 360, training loss: 13.756431579589844 = 1.0343987941741943 + 2.0 * 6.361016273498535
Epoch 360, val loss: 1.1846097707748413
Epoch 370, training loss: 13.719786643981934 = 1.0059696435928345 + 2.0 * 6.356908321380615
Epoch 370, val loss: 1.1637049913406372
Epoch 380, training loss: 13.686732292175293 = 0.9789011478424072 + 2.0 * 6.353915691375732
Epoch 380, val loss: 1.1438777446746826
Epoch 390, training loss: 13.660920143127441 = 0.953119695186615 + 2.0 * 6.35390043258667
Epoch 390, val loss: 1.125084400177002
Epoch 400, training loss: 13.625908851623535 = 0.9285065531730652 + 2.0 * 6.348701000213623
Epoch 400, val loss: 1.1071513891220093
Epoch 410, training loss: 13.59339714050293 = 0.9045720100402832 + 2.0 * 6.344412803649902
Epoch 410, val loss: 1.0898159742355347
Epoch 420, training loss: 13.566523551940918 = 0.8809079527854919 + 2.0 * 6.342807769775391
Epoch 420, val loss: 1.0725798606872559
Epoch 430, training loss: 13.533590316772461 = 0.8572985529899597 + 2.0 * 6.338145732879639
Epoch 430, val loss: 1.0553209781646729
Epoch 440, training loss: 13.506223678588867 = 0.8334699273109436 + 2.0 * 6.336376667022705
Epoch 440, val loss: 1.037981390953064
Epoch 450, training loss: 13.478883743286133 = 0.8093351125717163 + 2.0 * 6.334774494171143
Epoch 450, val loss: 1.0203555822372437
Epoch 460, training loss: 13.449380874633789 = 0.7849603891372681 + 2.0 * 6.332210063934326
Epoch 460, val loss: 1.002663254737854
Epoch 470, training loss: 13.419113159179688 = 0.7602347135543823 + 2.0 * 6.329439163208008
Epoch 470, val loss: 0.9847776293754578
Epoch 480, training loss: 13.38896656036377 = 0.7350938320159912 + 2.0 * 6.3269362449646
Epoch 480, val loss: 0.9667627215385437
Epoch 490, training loss: 13.374153137207031 = 0.7097068428993225 + 2.0 * 6.332222938537598
Epoch 490, val loss: 0.9488133192062378
Epoch 500, training loss: 13.334527015686035 = 0.6846180558204651 + 2.0 * 6.324954509735107
Epoch 500, val loss: 0.9312540292739868
Epoch 510, training loss: 13.305585861206055 = 0.6597758531570435 + 2.0 * 6.32290506362915
Epoch 510, val loss: 0.9145025014877319
Epoch 520, training loss: 13.274419784545898 = 0.6352386474609375 + 2.0 * 6.3195905685424805
Epoch 520, val loss: 0.8983073234558105
Epoch 530, training loss: 13.246681213378906 = 0.6110588312149048 + 2.0 * 6.317811012268066
Epoch 530, val loss: 0.8828611373901367
Epoch 540, training loss: 13.2421293258667 = 0.5873736143112183 + 2.0 * 6.327377796173096
Epoch 540, val loss: 0.868229329586029
Epoch 550, training loss: 13.194106101989746 = 0.5644031167030334 + 2.0 * 6.3148512840271
Epoch 550, val loss: 0.8546037077903748
Epoch 560, training loss: 13.168316841125488 = 0.5421876311302185 + 2.0 * 6.3130645751953125
Epoch 560, val loss: 0.842155933380127
Epoch 570, training loss: 13.143945693969727 = 0.5206078886985779 + 2.0 * 6.311668872833252
Epoch 570, val loss: 0.8305726051330566
Epoch 580, training loss: 13.12041187286377 = 0.49964991211891174 + 2.0 * 6.310380935668945
Epoch 580, val loss: 0.819875180721283
Epoch 590, training loss: 13.120575904846191 = 0.479290634393692 + 2.0 * 6.320642471313477
Epoch 590, val loss: 0.8099996447563171
Epoch 600, training loss: 13.083102226257324 = 0.45980748534202576 + 2.0 * 6.311647415161133
Epoch 600, val loss: 0.8008637428283691
Epoch 610, training loss: 13.056855201721191 = 0.44106969237327576 + 2.0 * 6.307892799377441
Epoch 610, val loss: 0.7926509380340576
Epoch 620, training loss: 13.034496307373047 = 0.42295414209365845 + 2.0 * 6.3057708740234375
Epoch 620, val loss: 0.7851008176803589
Epoch 630, training loss: 13.020395278930664 = 0.4054639935493469 + 2.0 * 6.307465553283691
Epoch 630, val loss: 0.7782118916511536
Epoch 640, training loss: 13.00330638885498 = 0.38861897587776184 + 2.0 * 6.307343482971191
Epoch 640, val loss: 0.7718937993049622
Epoch 650, training loss: 12.978537559509277 = 0.3724997043609619 + 2.0 * 6.303019046783447
Epoch 650, val loss: 0.7663422226905823
Epoch 660, training loss: 12.958383560180664 = 0.35703450441360474 + 2.0 * 6.3006744384765625
Epoch 660, val loss: 0.7614478468894958
Epoch 670, training loss: 12.941695213317871 = 0.34220948815345764 + 2.0 * 6.299742698669434
Epoch 670, val loss: 0.7571439743041992
Epoch 680, training loss: 12.933026313781738 = 0.32804903388023376 + 2.0 * 6.302488803863525
Epoch 680, val loss: 0.7534225583076477
Epoch 690, training loss: 12.921911239624023 = 0.31460484862327576 + 2.0 * 6.303653240203857
Epoch 690, val loss: 0.7502373456954956
Epoch 700, training loss: 12.896953582763672 = 0.30179619789123535 + 2.0 * 6.297578811645508
Epoch 700, val loss: 0.7476528882980347
Epoch 710, training loss: 12.880284309387207 = 0.2896013855934143 + 2.0 * 6.295341491699219
Epoch 710, val loss: 0.7457146644592285
Epoch 720, training loss: 12.865784645080566 = 0.27790015935897827 + 2.0 * 6.293942451477051
Epoch 720, val loss: 0.7441883683204651
Epoch 730, training loss: 12.858805656433105 = 0.2666562795639038 + 2.0 * 6.296074867248535
Epoch 730, val loss: 0.7431458830833435
Epoch 740, training loss: 12.854275703430176 = 0.25588110089302063 + 2.0 * 6.299197196960449
Epoch 740, val loss: 0.7423753142356873
Epoch 750, training loss: 12.831098556518555 = 0.2455766201019287 + 2.0 * 6.292760848999023
Epoch 750, val loss: 0.7421787977218628
Epoch 760, training loss: 12.816522598266602 = 0.23562538623809814 + 2.0 * 6.2904486656188965
Epoch 760, val loss: 0.742301344871521
Epoch 770, training loss: 12.814977645874023 = 0.22595591843128204 + 2.0 * 6.294510841369629
Epoch 770, val loss: 0.7426878213882446
Epoch 780, training loss: 12.798165321350098 = 0.21660621464252472 + 2.0 * 6.2907795906066895
Epoch 780, val loss: 0.7433843612670898
Epoch 790, training loss: 12.784637451171875 = 0.20746824145317078 + 2.0 * 6.2885847091674805
Epoch 790, val loss: 0.7442138195037842
Epoch 800, training loss: 12.77220344543457 = 0.19856181740760803 + 2.0 * 6.286820888519287
Epoch 800, val loss: 0.7453212141990662
Epoch 810, training loss: 12.768058776855469 = 0.18981821835041046 + 2.0 * 6.289120197296143
Epoch 810, val loss: 0.7466525435447693
Epoch 820, training loss: 12.763823509216309 = 0.18130391836166382 + 2.0 * 6.291259765625
Epoch 820, val loss: 0.7480084300041199
Epoch 830, training loss: 12.743289947509766 = 0.173018679022789 + 2.0 * 6.285135746002197
Epoch 830, val loss: 0.7497538924217224
Epoch 840, training loss: 12.731148719787598 = 0.16494448482990265 + 2.0 * 6.283102035522461
Epoch 840, val loss: 0.7516782879829407
Epoch 850, training loss: 12.726259231567383 = 0.1570824533700943 + 2.0 * 6.28458833694458
Epoch 850, val loss: 0.7538604736328125
Epoch 860, training loss: 12.715282440185547 = 0.1494736522436142 + 2.0 * 6.282904624938965
Epoch 860, val loss: 0.7560828924179077
Epoch 870, training loss: 12.706155776977539 = 0.14216727018356323 + 2.0 * 6.281994342803955
Epoch 870, val loss: 0.7587671279907227
Epoch 880, training loss: 12.704190254211426 = 0.13515187799930573 + 2.0 * 6.284519195556641
Epoch 880, val loss: 0.7616968750953674
Epoch 890, training loss: 12.694448471069336 = 0.12844131886959076 + 2.0 * 6.283003807067871
Epoch 890, val loss: 0.7646411061286926
Epoch 900, training loss: 12.682599067687988 = 0.12211070209741592 + 2.0 * 6.28024435043335
Epoch 900, val loss: 0.7679428458213806
Epoch 910, training loss: 12.672303199768066 = 0.11608266085386276 + 2.0 * 6.278110504150391
Epoch 910, val loss: 0.771530032157898
Epoch 920, training loss: 12.667482376098633 = 0.11035632342100143 + 2.0 * 6.278563022613525
Epoch 920, val loss: 0.7753216028213501
Epoch 930, training loss: 12.66529369354248 = 0.10494887828826904 + 2.0 * 6.280172348022461
Epoch 930, val loss: 0.7791427969932556
Epoch 940, training loss: 12.65446949005127 = 0.0998709574341774 + 2.0 * 6.277299404144287
Epoch 940, val loss: 0.7831924557685852
Epoch 950, training loss: 12.647066116333008 = 0.09509928524494171 + 2.0 * 6.2759833335876465
Epoch 950, val loss: 0.7873959541320801
Epoch 960, training loss: 12.640148162841797 = 0.09058224409818649 + 2.0 * 6.274783134460449
Epoch 960, val loss: 0.7917749285697937
Epoch 970, training loss: 12.640454292297363 = 0.08632662147283554 + 2.0 * 6.277063846588135
Epoch 970, val loss: 0.7962955236434937
Epoch 980, training loss: 12.634300231933594 = 0.0823221504688263 + 2.0 * 6.275989055633545
Epoch 980, val loss: 0.8006888031959534
Epoch 990, training loss: 12.626236915588379 = 0.07855063676834106 + 2.0 * 6.273843288421631
Epoch 990, val loss: 0.805237352848053
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7666666666666667
0.8402741170268846
=== training gcn model ===
Epoch 0, training loss: 19.13816261291504 = 1.944539189338684 + 2.0 * 8.596811294555664
Epoch 0, val loss: 1.9367862939834595
Epoch 10, training loss: 19.127315521240234 = 1.9346246719360352 + 2.0 * 8.596344947814941
Epoch 10, val loss: 1.927491545677185
Epoch 20, training loss: 19.108104705810547 = 1.9224271774291992 + 2.0 * 8.592838287353516
Epoch 20, val loss: 1.9155807495117188
Epoch 30, training loss: 19.04111671447754 = 1.9063472747802734 + 2.0 * 8.567384719848633
Epoch 30, val loss: 1.8995842933654785
Epoch 40, training loss: 18.68396759033203 = 1.8873555660247803 + 2.0 * 8.398305892944336
Epoch 40, val loss: 1.8813949823379517
Epoch 50, training loss: 17.279401779174805 = 1.8683897256851196 + 2.0 * 7.705505847930908
Epoch 50, val loss: 1.8640925884246826
Epoch 60, training loss: 16.455936431884766 = 1.8527556657791138 + 2.0 * 7.301590442657471
Epoch 60, val loss: 1.8501888513565063
Epoch 70, training loss: 15.914505958557129 = 1.8377147912979126 + 2.0 * 7.038395404815674
Epoch 70, val loss: 1.8363122940063477
Epoch 80, training loss: 15.690899848937988 = 1.8245145082473755 + 2.0 * 6.933192729949951
Epoch 80, val loss: 1.823928713798523
Epoch 90, training loss: 15.508209228515625 = 1.811336874961853 + 2.0 * 6.84843635559082
Epoch 90, val loss: 1.8119322061538696
Epoch 100, training loss: 15.347732543945312 = 1.7991297245025635 + 2.0 * 6.774301528930664
Epoch 100, val loss: 1.8011603355407715
Epoch 110, training loss: 15.213471412658691 = 1.788033127784729 + 2.0 * 6.712718963623047
Epoch 110, val loss: 1.7913484573364258
Epoch 120, training loss: 15.102834701538086 = 1.777328610420227 + 2.0 * 6.662753105163574
Epoch 120, val loss: 1.781590223312378
Epoch 130, training loss: 15.01193618774414 = 1.7662583589553833 + 2.0 * 6.622838973999023
Epoch 130, val loss: 1.771375298500061
Epoch 140, training loss: 14.942028999328613 = 1.75453782081604 + 2.0 * 6.593745708465576
Epoch 140, val loss: 1.760696530342102
Epoch 150, training loss: 14.868634223937988 = 1.7421427965164185 + 2.0 * 6.56324577331543
Epoch 150, val loss: 1.7494933605194092
Epoch 160, training loss: 14.80573844909668 = 1.7285605669021606 + 2.0 * 6.538589000701904
Epoch 160, val loss: 1.737488865852356
Epoch 170, training loss: 14.753511428833008 = 1.7133203744888306 + 2.0 * 6.520095348358154
Epoch 170, val loss: 1.724226713180542
Epoch 180, training loss: 14.695748329162598 = 1.6962448358535767 + 2.0 * 6.499751567840576
Epoch 180, val loss: 1.7094541788101196
Epoch 190, training loss: 14.644392013549805 = 1.676810622215271 + 2.0 * 6.483790874481201
Epoch 190, val loss: 1.6927530765533447
Epoch 200, training loss: 14.598479270935059 = 1.6545897722244263 + 2.0 * 6.471944808959961
Epoch 200, val loss: 1.673691749572754
Epoch 210, training loss: 14.54603385925293 = 1.6293190717697144 + 2.0 * 6.458357334136963
Epoch 210, val loss: 1.6520615816116333
Epoch 220, training loss: 14.494864463806152 = 1.6005500555038452 + 2.0 * 6.447157382965088
Epoch 220, val loss: 1.627565860748291
Epoch 230, training loss: 14.444673538208008 = 1.5678950548171997 + 2.0 * 6.438389301300049
Epoch 230, val loss: 1.5997951030731201
Epoch 240, training loss: 14.391692161560059 = 1.5312557220458984 + 2.0 * 6.43021821975708
Epoch 240, val loss: 1.5689387321472168
Epoch 250, training loss: 14.334917068481445 = 1.4906480312347412 + 2.0 * 6.4221343994140625
Epoch 250, val loss: 1.534968376159668
Epoch 260, training loss: 14.277850151062012 = 1.4463245868682861 + 2.0 * 6.415762901306152
Epoch 260, val loss: 1.4982423782348633
Epoch 270, training loss: 14.217778205871582 = 1.3991203308105469 + 2.0 * 6.409328937530518
Epoch 270, val loss: 1.4594453573226929
Epoch 280, training loss: 14.159655570983887 = 1.349550485610962 + 2.0 * 6.405052661895752
Epoch 280, val loss: 1.4192615747451782
Epoch 290, training loss: 14.09687328338623 = 1.2986332178115845 + 2.0 * 6.399119853973389
Epoch 290, val loss: 1.378718614578247
Epoch 300, training loss: 14.035676002502441 = 1.2475980520248413 + 2.0 * 6.394039154052734
Epoch 300, val loss: 1.3383809328079224
Epoch 310, training loss: 13.975790023803711 = 1.1970926523208618 + 2.0 * 6.38934850692749
Epoch 310, val loss: 1.2990119457244873
Epoch 320, training loss: 13.916483879089355 = 1.1478016376495361 + 2.0 * 6.384341239929199
Epoch 320, val loss: 1.2609093189239502
Epoch 330, training loss: 13.867098808288574 = 1.0998666286468506 + 2.0 * 6.383615970611572
Epoch 330, val loss: 1.2243781089782715
Epoch 340, training loss: 13.810285568237305 = 1.054234266281128 + 2.0 * 6.378025531768799
Epoch 340, val loss: 1.1895835399627686
Epoch 350, training loss: 13.756932258605957 = 1.0104748010635376 + 2.0 * 6.373228549957275
Epoch 350, val loss: 1.1562962532043457
Epoch 360, training loss: 13.709099769592285 = 0.9682906270027161 + 2.0 * 6.3704047203063965
Epoch 360, val loss: 1.1242860555648804
Epoch 370, training loss: 13.657997131347656 = 0.9276765584945679 + 2.0 * 6.3651604652404785
Epoch 370, val loss: 1.0935014486312866
Epoch 380, training loss: 13.613506317138672 = 0.8888197541236877 + 2.0 * 6.3623433113098145
Epoch 380, val loss: 1.064054250717163
Epoch 390, training loss: 13.576908111572266 = 0.851833701133728 + 2.0 * 6.362537384033203
Epoch 390, val loss: 1.036076545715332
Epoch 400, training loss: 13.529431343078613 = 0.8166099190711975 + 2.0 * 6.356410503387451
Epoch 400, val loss: 1.0096635818481445
Epoch 410, training loss: 13.489654541015625 = 0.7831168174743652 + 2.0 * 6.353268623352051
Epoch 410, val loss: 0.9847348928451538
Epoch 420, training loss: 13.457427978515625 = 0.7512484788894653 + 2.0 * 6.353089809417725
Epoch 420, val loss: 0.9614115357398987
Epoch 430, training loss: 13.42743968963623 = 0.7211418747901917 + 2.0 * 6.353148937225342
Epoch 430, val loss: 0.9396784901618958
Epoch 440, training loss: 13.38432788848877 = 0.6927689909934998 + 2.0 * 6.3457794189453125
Epoch 440, val loss: 0.9197895526885986
Epoch 450, training loss: 13.350140571594238 = 0.6657646298408508 + 2.0 * 6.342187881469727
Epoch 450, val loss: 0.9014302492141724
Epoch 460, training loss: 13.332857131958008 = 0.6398819088935852 + 2.0 * 6.346487522125244
Epoch 460, val loss: 0.8844545483589172
Epoch 470, training loss: 13.29345989227295 = 0.6152334213256836 + 2.0 * 6.339113235473633
Epoch 470, val loss: 0.8688445091247559
Epoch 480, training loss: 13.267186164855957 = 0.591635525226593 + 2.0 * 6.337775230407715
Epoch 480, val loss: 0.8546287417411804
Epoch 490, training loss: 13.237818717956543 = 0.5689178705215454 + 2.0 * 6.3344502449035645
Epoch 490, val loss: 0.8415337204933167
Epoch 500, training loss: 13.213605880737305 = 0.5469644665718079 + 2.0 * 6.333320617675781
Epoch 500, val loss: 0.8295244574546814
Epoch 510, training loss: 13.186079025268555 = 0.5256321430206299 + 2.0 * 6.330223560333252
Epoch 510, val loss: 0.8184961676597595
Epoch 520, training loss: 13.177618026733398 = 0.5049118995666504 + 2.0 * 6.336352825164795
Epoch 520, val loss: 0.8084210157394409
Epoch 530, training loss: 13.14222526550293 = 0.4850594699382782 + 2.0 * 6.328582763671875
Epoch 530, val loss: 0.7992567420005798
Epoch 540, training loss: 13.115986824035645 = 0.46575814485549927 + 2.0 * 6.3251142501831055
Epoch 540, val loss: 0.7909660339355469
Epoch 550, training loss: 13.091534614562988 = 0.4468984007835388 + 2.0 * 6.322318077087402
Epoch 550, val loss: 0.7834164500236511
Epoch 560, training loss: 13.070980072021484 = 0.4284238815307617 + 2.0 * 6.321278095245361
Epoch 560, val loss: 0.7765551805496216
Epoch 570, training loss: 13.050204277038574 = 0.41041672229766846 + 2.0 * 6.319893836975098
Epoch 570, val loss: 0.7703297138214111
Epoch 580, training loss: 13.028310775756836 = 0.3929678201675415 + 2.0 * 6.317671298980713
Epoch 580, val loss: 0.7648447751998901
Epoch 590, training loss: 13.00828742980957 = 0.37597572803497314 + 2.0 * 6.316155910491943
Epoch 590, val loss: 0.7599830031394958
Epoch 600, training loss: 13.011064529418945 = 0.35939860343933105 + 2.0 * 6.325832843780518
Epoch 600, val loss: 0.7556455135345459
Epoch 610, training loss: 12.973013877868652 = 0.3433285653591156 + 2.0 * 6.314842700958252
Epoch 610, val loss: 0.7519109845161438
Epoch 620, training loss: 12.9520902633667 = 0.32774993777275085 + 2.0 * 6.312170028686523
Epoch 620, val loss: 0.7487763166427612
Epoch 630, training loss: 12.9367094039917 = 0.3125723600387573 + 2.0 * 6.312068462371826
Epoch 630, val loss: 0.7460832595825195
Epoch 640, training loss: 12.917869567871094 = 0.2978978753089905 + 2.0 * 6.309985637664795
Epoch 640, val loss: 0.743746817111969
Epoch 650, training loss: 12.903727531433105 = 0.2837303578853607 + 2.0 * 6.309998512268066
Epoch 650, val loss: 0.7419401407241821
Epoch 660, training loss: 12.884526252746582 = 0.270084947347641 + 2.0 * 6.307220458984375
Epoch 660, val loss: 0.7405480742454529
Epoch 670, training loss: 12.870349884033203 = 0.25689154863357544 + 2.0 * 6.306729316711426
Epoch 670, val loss: 0.7395565509796143
Epoch 680, training loss: 12.856074333190918 = 0.2441752552986145 + 2.0 * 6.305949687957764
Epoch 680, val loss: 0.73890221118927
Epoch 690, training loss: 12.842413902282715 = 0.2320234626531601 + 2.0 * 6.305195331573486
Epoch 690, val loss: 0.7386805415153503
Epoch 700, training loss: 12.831257820129395 = 0.22038309276103973 + 2.0 * 6.3054375648498535
Epoch 700, val loss: 0.7388374209403992
Epoch 710, training loss: 12.8132963180542 = 0.20925337076187134 + 2.0 * 6.302021503448486
Epoch 710, val loss: 0.7393375039100647
Epoch 720, training loss: 12.799727439880371 = 0.19863483309745789 + 2.0 * 6.300546169281006
Epoch 720, val loss: 0.740209698677063
Epoch 730, training loss: 12.800925254821777 = 0.18850089609622955 + 2.0 * 6.306211948394775
Epoch 730, val loss: 0.7414109110832214
Epoch 740, training loss: 12.789878845214844 = 0.17887966334819794 + 2.0 * 6.30549955368042
Epoch 740, val loss: 0.742811918258667
Epoch 750, training loss: 12.769250869750977 = 0.16984090209007263 + 2.0 * 6.2997050285339355
Epoch 750, val loss: 0.7445509433746338
Epoch 760, training loss: 12.75551986694336 = 0.16127744317054749 + 2.0 * 6.297121047973633
Epoch 760, val loss: 0.7466099858283997
Epoch 770, training loss: 12.743260383605957 = 0.1531715989112854 + 2.0 * 6.295044422149658
Epoch 770, val loss: 0.7488824725151062
Epoch 780, training loss: 12.736063003540039 = 0.1454765647649765 + 2.0 * 6.29529333114624
Epoch 780, val loss: 0.7514186501502991
Epoch 790, training loss: 12.729077339172363 = 0.13818618655204773 + 2.0 * 6.295445442199707
Epoch 790, val loss: 0.7540566921234131
Epoch 800, training loss: 12.720252990722656 = 0.13134627044200897 + 2.0 * 6.294453144073486
Epoch 800, val loss: 0.7569822072982788
Epoch 810, training loss: 12.708196640014648 = 0.12490424513816833 + 2.0 * 6.2916460037231445
Epoch 810, val loss: 0.7600221037864685
Epoch 820, training loss: 12.702401161193848 = 0.11882707476615906 + 2.0 * 6.291787147521973
Epoch 820, val loss: 0.7632195949554443
Epoch 830, training loss: 12.700699806213379 = 0.11308973282575607 + 2.0 * 6.293805122375488
Epoch 830, val loss: 0.7665911912918091
Epoch 840, training loss: 12.691232681274414 = 0.10768965631723404 + 2.0 * 6.291771411895752
Epoch 840, val loss: 0.7700098752975464
Epoch 850, training loss: 12.679695129394531 = 0.10264990478754044 + 2.0 * 6.288522720336914
Epoch 850, val loss: 0.7736628651618958
Epoch 860, training loss: 12.672922134399414 = 0.09789741039276123 + 2.0 * 6.287512302398682
Epoch 860, val loss: 0.7773249745368958
Epoch 870, training loss: 12.666254043579102 = 0.09341397881507874 + 2.0 * 6.286419868469238
Epoch 870, val loss: 0.7811458110809326
Epoch 880, training loss: 12.669596672058105 = 0.08918670564889908 + 2.0 * 6.290205001831055
Epoch 880, val loss: 0.7850407958030701
Epoch 890, training loss: 12.66749095916748 = 0.08521449565887451 + 2.0 * 6.291138172149658
Epoch 890, val loss: 0.7888864278793335
Epoch 900, training loss: 12.65218734741211 = 0.08149845898151398 + 2.0 * 6.28534460067749
Epoch 900, val loss: 0.7927758693695068
Epoch 910, training loss: 12.644399642944336 = 0.07800498604774475 + 2.0 * 6.283197402954102
Epoch 910, val loss: 0.796781063079834
Epoch 920, training loss: 12.648846626281738 = 0.07470325380563736 + 2.0 * 6.287071704864502
Epoch 920, val loss: 0.8007497191429138
Epoch 930, training loss: 12.635862350463867 = 0.07159706205129623 + 2.0 * 6.282132625579834
Epoch 930, val loss: 0.8047623038291931
Epoch 940, training loss: 12.630615234375 = 0.06866858154535294 + 2.0 * 6.280973434448242
Epoch 940, val loss: 0.8087849020957947
Epoch 950, training loss: 12.625030517578125 = 0.06590145081281662 + 2.0 * 6.279564380645752
Epoch 950, val loss: 0.8128329515457153
Epoch 960, training loss: 12.622694969177246 = 0.0632755383849144 + 2.0 * 6.279709815979004
Epoch 960, val loss: 0.8168630599975586
Epoch 970, training loss: 12.626811027526855 = 0.06079203635454178 + 2.0 * 6.2830095291137695
Epoch 970, val loss: 0.8208550214767456
Epoch 980, training loss: 12.61391544342041 = 0.05845561623573303 + 2.0 * 6.2777299880981445
Epoch 980, val loss: 0.8248592615127563
Epoch 990, training loss: 12.615306854248047 = 0.0562489777803421 + 2.0 * 6.279529094696045
Epoch 990, val loss: 0.8288165330886841
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8111111111111111
0.849235635213495
The final CL Acc:0.78889, 0.01814, The final GNN Acc:0.84326, 0.00422
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.3 der2:0.4 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11640])
remove edge: torch.Size([2, 9490])
updated graph: torch.Size([2, 10574])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 19.164566040039062 = 1.9708577394485474 + 2.0 * 8.596854209899902
Epoch 0, val loss: 1.971531629562378
Epoch 10, training loss: 19.153213500976562 = 1.960036039352417 + 2.0 * 8.596589088439941
Epoch 10, val loss: 1.9608633518218994
Epoch 20, training loss: 19.135183334350586 = 1.9463181495666504 + 2.0 * 8.594432830810547
Epoch 20, val loss: 1.9473601579666138
Epoch 30, training loss: 19.084102630615234 = 1.9270198345184326 + 2.0 * 8.57854175567627
Epoch 30, val loss: 1.9282289743423462
Epoch 40, training loss: 18.907743453979492 = 1.9023586511611938 + 2.0 * 8.502692222595215
Epoch 40, val loss: 1.904783844947815
Epoch 50, training loss: 18.336196899414062 = 1.8768091201782227 + 2.0 * 8.229693412780762
Epoch 50, val loss: 1.8809900283813477
Epoch 60, training loss: 18.005821228027344 = 1.853004813194275 + 2.0 * 8.076408386230469
Epoch 60, val loss: 1.8593096733093262
Epoch 70, training loss: 17.48462677001953 = 1.8312640190124512 + 2.0 * 7.826681613922119
Epoch 70, val loss: 1.8391625881195068
Epoch 80, training loss: 16.58209228515625 = 1.8164173364639282 + 2.0 * 7.382837772369385
Epoch 80, val loss: 1.8255817890167236
Epoch 90, training loss: 15.938694953918457 = 1.8076488971710205 + 2.0 * 7.065523147583008
Epoch 90, val loss: 1.8167980909347534
Epoch 100, training loss: 15.671789169311523 = 1.7937670946121216 + 2.0 * 6.939011096954346
Epoch 100, val loss: 1.8033684492111206
Epoch 110, training loss: 15.478472709655762 = 1.7773847579956055 + 2.0 * 6.850543975830078
Epoch 110, val loss: 1.7883573770523071
Epoch 120, training loss: 15.329954147338867 = 1.7628298997879028 + 2.0 * 6.783562183380127
Epoch 120, val loss: 1.774598240852356
Epoch 130, training loss: 15.208858489990234 = 1.7490296363830566 + 2.0 * 6.729914665222168
Epoch 130, val loss: 1.761129379272461
Epoch 140, training loss: 15.100849151611328 = 1.734791874885559 + 2.0 * 6.683028697967529
Epoch 140, val loss: 1.7476859092712402
Epoch 150, training loss: 15.006511688232422 = 1.7198294401168823 + 2.0 * 6.643341064453125
Epoch 150, val loss: 1.7342357635498047
Epoch 160, training loss: 14.924978256225586 = 1.7035139799118042 + 2.0 * 6.610732078552246
Epoch 160, val loss: 1.7200244665145874
Epoch 170, training loss: 14.853134155273438 = 1.6857539415359497 + 2.0 * 6.583690166473389
Epoch 170, val loss: 1.704738974571228
Epoch 180, training loss: 14.797030448913574 = 1.6662797927856445 + 2.0 * 6.565375328063965
Epoch 180, val loss: 1.688002347946167
Epoch 190, training loss: 14.733322143554688 = 1.6447826623916626 + 2.0 * 6.544269561767578
Epoch 190, val loss: 1.6695610284805298
Epoch 200, training loss: 14.675735473632812 = 1.6212314367294312 + 2.0 * 6.527252197265625
Epoch 200, val loss: 1.6494048833847046
Epoch 210, training loss: 14.624881744384766 = 1.5955781936645508 + 2.0 * 6.514651775360107
Epoch 210, val loss: 1.6275969743728638
Epoch 220, training loss: 14.57101058959961 = 1.568278193473816 + 2.0 * 6.501366138458252
Epoch 220, val loss: 1.6043493747711182
Epoch 230, training loss: 14.520238876342773 = 1.5393404960632324 + 2.0 * 6.490448951721191
Epoch 230, val loss: 1.5798596143722534
Epoch 240, training loss: 14.472393035888672 = 1.509043574333191 + 2.0 * 6.481674671173096
Epoch 240, val loss: 1.5544918775558472
Epoch 250, training loss: 14.419767379760742 = 1.477892518043518 + 2.0 * 6.470937252044678
Epoch 250, val loss: 1.5286462306976318
Epoch 260, training loss: 14.372082710266113 = 1.446065068244934 + 2.0 * 6.463008880615234
Epoch 260, val loss: 1.5026692152023315
Epoch 270, training loss: 14.325037956237793 = 1.413822889328003 + 2.0 * 6.4556074142456055
Epoch 270, val loss: 1.476761817932129
Epoch 280, training loss: 14.277310371398926 = 1.381608247756958 + 2.0 * 6.447851181030273
Epoch 280, val loss: 1.4513732194900513
Epoch 290, training loss: 14.232080459594727 = 1.3492655754089355 + 2.0 * 6.441407680511475
Epoch 290, val loss: 1.4263427257537842
Epoch 300, training loss: 14.187548637390137 = 1.3168331384658813 + 2.0 * 6.435357570648193
Epoch 300, val loss: 1.4017066955566406
Epoch 310, training loss: 14.149592399597168 = 1.2844480276107788 + 2.0 * 6.432572364807129
Epoch 310, val loss: 1.3776443004608154
Epoch 320, training loss: 14.10577392578125 = 1.2524558305740356 + 2.0 * 6.426659107208252
Epoch 320, val loss: 1.3544725179672241
Epoch 330, training loss: 14.060354232788086 = 1.2205828428268433 + 2.0 * 6.419885635375977
Epoch 330, val loss: 1.3319050073623657
Epoch 340, training loss: 14.019850730895996 = 1.1886389255523682 + 2.0 * 6.4156060218811035
Epoch 340, val loss: 1.3097364902496338
Epoch 350, training loss: 13.980794906616211 = 1.1568071842193604 + 2.0 * 6.411993980407715
Epoch 350, val loss: 1.2881720066070557
Epoch 360, training loss: 13.940281867980957 = 1.1252278089523315 + 2.0 * 6.407526969909668
Epoch 360, val loss: 1.2673249244689941
Epoch 370, training loss: 13.903081893920898 = 1.0940673351287842 + 2.0 * 6.404507160186768
Epoch 370, val loss: 1.2472115755081177
Epoch 380, training loss: 13.868041038513184 = 1.0633561611175537 + 2.0 * 6.402342319488525
Epoch 380, val loss: 1.2279876470565796
Epoch 390, training loss: 13.825199127197266 = 1.0333731174468994 + 2.0 * 6.395913124084473
Epoch 390, val loss: 1.2097197771072388
Epoch 400, training loss: 13.787313461303711 = 1.003887414932251 + 2.0 * 6.3917131423950195
Epoch 400, val loss: 1.1922279596328735
Epoch 410, training loss: 13.756195068359375 = 0.974812924861908 + 2.0 * 6.39069128036499
Epoch 410, val loss: 1.1754837036132812
Epoch 420, training loss: 13.723122596740723 = 0.9464395642280579 + 2.0 * 6.388341426849365
Epoch 420, val loss: 1.159328579902649
Epoch 430, training loss: 13.683999061584473 = 0.9186890125274658 + 2.0 * 6.382655143737793
Epoch 430, val loss: 1.1439528465270996
Epoch 440, training loss: 13.649639129638672 = 0.8914883732795715 + 2.0 * 6.379075527191162
Epoch 440, val loss: 1.1292352676391602
Epoch 450, training loss: 13.619003295898438 = 0.8648850917816162 + 2.0 * 6.377058982849121
Epoch 450, val loss: 1.1150636672973633
Epoch 460, training loss: 13.592145919799805 = 0.8389630317687988 + 2.0 * 6.376591682434082
Epoch 460, val loss: 1.1015878915786743
Epoch 470, training loss: 13.556037902832031 = 0.8138779401779175 + 2.0 * 6.371079921722412
Epoch 470, val loss: 1.0888663530349731
Epoch 480, training loss: 13.526058197021484 = 0.7895016670227051 + 2.0 * 6.368278503417969
Epoch 480, val loss: 1.0767756700515747
Epoch 490, training loss: 13.49636459350586 = 0.7658035755157471 + 2.0 * 6.365280628204346
Epoch 490, val loss: 1.0655494928359985
Epoch 500, training loss: 13.474782943725586 = 0.7428157329559326 + 2.0 * 6.365983486175537
Epoch 500, val loss: 1.0548853874206543
Epoch 510, training loss: 13.449094772338867 = 0.7206371426582336 + 2.0 * 6.36422872543335
Epoch 510, val loss: 1.0452359914779663
Epoch 520, training loss: 13.4180269241333 = 0.6993830800056458 + 2.0 * 6.3593220710754395
Epoch 520, val loss: 1.0362411737442017
Epoch 530, training loss: 13.395047187805176 = 0.6788943409919739 + 2.0 * 6.358076572418213
Epoch 530, val loss: 1.0280643701553345
Epoch 540, training loss: 13.36892032623291 = 0.6591570377349854 + 2.0 * 6.354881763458252
Epoch 540, val loss: 1.02068030834198
Epoch 550, training loss: 13.344250679016113 = 0.6402156352996826 + 2.0 * 6.352017402648926
Epoch 550, val loss: 1.0142155885696411
Epoch 560, training loss: 13.330817222595215 = 0.6218441128730774 + 2.0 * 6.354486465454102
Epoch 560, val loss: 1.0084519386291504
Epoch 570, training loss: 13.308565139770508 = 0.6041907072067261 + 2.0 * 6.352187156677246
Epoch 570, val loss: 1.0035377740859985
Epoch 580, training loss: 13.277620315551758 = 0.5870654582977295 + 2.0 * 6.345277309417725
Epoch 580, val loss: 0.9991684556007385
Epoch 590, training loss: 13.257500648498535 = 0.5704368948936462 + 2.0 * 6.343532085418701
Epoch 590, val loss: 0.9954162836074829
Epoch 600, training loss: 13.237947463989258 = 0.5541980862617493 + 2.0 * 6.341874599456787
Epoch 600, val loss: 0.9922208189964294
Epoch 610, training loss: 13.221769332885742 = 0.538307249546051 + 2.0 * 6.341731071472168
Epoch 610, val loss: 0.9894660115242004
Epoch 620, training loss: 13.204300880432129 = 0.5228654146194458 + 2.0 * 6.340717792510986
Epoch 620, val loss: 0.9871914386749268
Epoch 630, training loss: 13.18275260925293 = 0.5077174305915833 + 2.0 * 6.337517738342285
Epoch 630, val loss: 0.9852122068405151
Epoch 640, training loss: 13.162811279296875 = 0.49275028705596924 + 2.0 * 6.335030555725098
Epoch 640, val loss: 0.9835116863250732
Epoch 650, training loss: 13.153803825378418 = 0.4778931438922882 + 2.0 * 6.337955474853516
Epoch 650, val loss: 0.9819897413253784
Epoch 660, training loss: 13.135923385620117 = 0.4631668031215668 + 2.0 * 6.33637809753418
Epoch 660, val loss: 0.9808661341667175
Epoch 670, training loss: 13.10981273651123 = 0.4485391676425934 + 2.0 * 6.330636978149414
Epoch 670, val loss: 0.9796856045722961
Epoch 680, training loss: 13.093029022216797 = 0.4340490698814392 + 2.0 * 6.3294901847839355
Epoch 680, val loss: 0.978528618812561
Epoch 690, training loss: 13.074830055236816 = 0.41955259442329407 + 2.0 * 6.327638626098633
Epoch 690, val loss: 0.9775676727294922
Epoch 700, training loss: 13.056983947753906 = 0.40508052706718445 + 2.0 * 6.32595157623291
Epoch 700, val loss: 0.976727306842804
Epoch 710, training loss: 13.060967445373535 = 0.3906138837337494 + 2.0 * 6.335176944732666
Epoch 710, val loss: 0.9760535359382629
Epoch 720, training loss: 13.029115676879883 = 0.3762371242046356 + 2.0 * 6.326439380645752
Epoch 720, val loss: 0.9752174615859985
Epoch 730, training loss: 13.006061553955078 = 0.36199647188186646 + 2.0 * 6.322032451629639
Epoch 730, val loss: 0.9746889472007751
Epoch 740, training loss: 12.990694999694824 = 0.3478628098964691 + 2.0 * 6.321415901184082
Epoch 740, val loss: 0.9743238687515259
Epoch 750, training loss: 12.98353385925293 = 0.33392152190208435 + 2.0 * 6.324806213378906
Epoch 750, val loss: 0.974005937576294
Epoch 760, training loss: 12.958614349365234 = 0.32028263807296753 + 2.0 * 6.3191657066345215
Epoch 760, val loss: 0.9740341901779175
Epoch 770, training loss: 12.950202941894531 = 0.3068866431713104 + 2.0 * 6.321658134460449
Epoch 770, val loss: 0.9741286635398865
Epoch 780, training loss: 12.926907539367676 = 0.2938074767589569 + 2.0 * 6.316550254821777
Epoch 780, val loss: 0.9746711850166321
Epoch 790, training loss: 12.91244888305664 = 0.2810095548629761 + 2.0 * 6.3157196044921875
Epoch 790, val loss: 0.9755313396453857
Epoch 800, training loss: 12.897449493408203 = 0.2685604393482208 + 2.0 * 6.314444541931152
Epoch 800, val loss: 0.9766557216644287
Epoch 810, training loss: 12.900341033935547 = 0.2564566731452942 + 2.0 * 6.321942329406738
Epoch 810, val loss: 0.9783428907394409
Epoch 820, training loss: 12.872541427612305 = 0.24479514360427856 + 2.0 * 6.313873291015625
Epoch 820, val loss: 0.9802253842353821
Epoch 830, training loss: 12.862531661987305 = 0.23352478444576263 + 2.0 * 6.3145036697387695
Epoch 830, val loss: 0.9823733568191528
Epoch 840, training loss: 12.844635009765625 = 0.22269608080387115 + 2.0 * 6.310969352722168
Epoch 840, val loss: 0.9849409461021423
Epoch 850, training loss: 12.831437110900879 = 0.2122925966978073 + 2.0 * 6.309572219848633
Epoch 850, val loss: 0.9879340529441833
Epoch 860, training loss: 12.826412200927734 = 0.20231565833091736 + 2.0 * 6.312048435211182
Epoch 860, val loss: 0.9910834431648254
Epoch 870, training loss: 12.81993579864502 = 0.19275455176830292 + 2.0 * 6.3135905265808105
Epoch 870, val loss: 0.9945754408836365
Epoch 880, training loss: 12.80116081237793 = 0.18367110192775726 + 2.0 * 6.30874490737915
Epoch 880, val loss: 0.9982877373695374
Epoch 890, training loss: 12.785357475280762 = 0.17504073679447174 + 2.0 * 6.3051581382751465
Epoch 890, val loss: 1.0023289918899536
Epoch 900, training loss: 12.77519702911377 = 0.166800394654274 + 2.0 * 6.304198265075684
Epoch 900, val loss: 1.0065593719482422
Epoch 910, training loss: 12.788732528686523 = 0.1589699685573578 + 2.0 * 6.314881324768066
Epoch 910, val loss: 1.0110160112380981
Epoch 920, training loss: 12.769326210021973 = 0.1516002118587494 + 2.0 * 6.308863162994385
Epoch 920, val loss: 1.0158697366714478
Epoch 930, training loss: 12.751436233520508 = 0.1445988565683365 + 2.0 * 6.3034186363220215
Epoch 930, val loss: 1.020561933517456
Epoch 940, training loss: 12.739089965820312 = 0.13797935843467712 + 2.0 * 6.300555229187012
Epoch 940, val loss: 1.025521993637085
Epoch 950, training loss: 12.746376991271973 = 0.1316986382007599 + 2.0 * 6.307339191436768
Epoch 950, val loss: 1.0308164358139038
Epoch 960, training loss: 12.732067108154297 = 0.125745952129364 + 2.0 * 6.303160667419434
Epoch 960, val loss: 1.0358154773712158
Epoch 970, training loss: 12.718664169311523 = 0.120107501745224 + 2.0 * 6.299278259277344
Epoch 970, val loss: 1.041178584098816
Epoch 980, training loss: 12.709043502807617 = 0.1147630512714386 + 2.0 * 6.297140121459961
Epoch 980, val loss: 1.046509861946106
Epoch 990, training loss: 12.706620216369629 = 0.10969987511634827 + 2.0 * 6.298460006713867
Epoch 990, val loss: 1.0520310401916504
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7222222222222222
0.8033737480231946
=== training gcn model ===
Epoch 0, training loss: 19.135448455810547 = 1.9417142868041992 + 2.0 * 8.596866607666016
Epoch 0, val loss: 1.941094994544983
Epoch 10, training loss: 19.125471115112305 = 1.9321448802947998 + 2.0 * 8.596663475036621
Epoch 10, val loss: 1.9310671091079712
Epoch 20, training loss: 19.1103572845459 = 1.9202641248703003 + 2.0 * 8.595046997070312
Epoch 20, val loss: 1.9185758829116821
Epoch 30, training loss: 19.06797981262207 = 1.9037535190582275 + 2.0 * 8.582113265991211
Epoch 30, val loss: 1.9012261629104614
Epoch 40, training loss: 18.89349365234375 = 1.8823000192642212 + 2.0 * 8.505597114562988
Epoch 40, val loss: 1.8792929649353027
Epoch 50, training loss: 18.26427459716797 = 1.8594765663146973 + 2.0 * 8.202399253845215
Epoch 50, val loss: 1.8569196462631226
Epoch 60, training loss: 17.680078506469727 = 1.8395838737487793 + 2.0 * 7.920247554779053
Epoch 60, val loss: 1.8386262655258179
Epoch 70, training loss: 16.615760803222656 = 1.8254621028900146 + 2.0 * 7.3951497077941895
Epoch 70, val loss: 1.825587511062622
Epoch 80, training loss: 15.953962326049805 = 1.8154306411743164 + 2.0 * 7.069265842437744
Epoch 80, val loss: 1.8153026103973389
Epoch 90, training loss: 15.66481876373291 = 1.8029775619506836 + 2.0 * 6.930920600891113
Epoch 90, val loss: 1.802819848060608
Epoch 100, training loss: 15.462120056152344 = 1.7883062362670898 + 2.0 * 6.836906909942627
Epoch 100, val loss: 1.7886476516723633
Epoch 110, training loss: 15.32655143737793 = 1.7738059759140015 + 2.0 * 6.776372909545898
Epoch 110, val loss: 1.7747722864151
Epoch 120, training loss: 15.211725234985352 = 1.7597202062606812 + 2.0 * 6.7260026931762695
Epoch 120, val loss: 1.7611149549484253
Epoch 130, training loss: 15.119491577148438 = 1.7449465990066528 + 2.0 * 6.687272548675537
Epoch 130, val loss: 1.7468632459640503
Epoch 140, training loss: 15.036043167114258 = 1.7286162376403809 + 2.0 * 6.653713226318359
Epoch 140, val loss: 1.7314610481262207
Epoch 150, training loss: 14.961845397949219 = 1.7103623151779175 + 2.0 * 6.625741481781006
Epoch 150, val loss: 1.7145472764968872
Epoch 160, training loss: 14.891722679138184 = 1.6898077726364136 + 2.0 * 6.60095739364624
Epoch 160, val loss: 1.6957056522369385
Epoch 170, training loss: 14.82598876953125 = 1.6666314601898193 + 2.0 * 6.579678535461426
Epoch 170, val loss: 1.6745001077651978
Epoch 180, training loss: 14.764668464660645 = 1.6406136751174927 + 2.0 * 6.562027454376221
Epoch 180, val loss: 1.6508580446243286
Epoch 190, training loss: 14.70116901397705 = 1.611522912979126 + 2.0 * 6.544823169708252
Epoch 190, val loss: 1.6246039867401123
Epoch 200, training loss: 14.63984489440918 = 1.579226016998291 + 2.0 * 6.530309200286865
Epoch 200, val loss: 1.595631718635559
Epoch 210, training loss: 14.582746505737305 = 1.5434138774871826 + 2.0 * 6.5196661949157715
Epoch 210, val loss: 1.5638056993484497
Epoch 220, training loss: 14.522662162780762 = 1.5042475461959839 + 2.0 * 6.509207248687744
Epoch 220, val loss: 1.5293444395065308
Epoch 230, training loss: 14.457460403442383 = 1.4619871377944946 + 2.0 * 6.49773645401001
Epoch 230, val loss: 1.4926164150238037
Epoch 240, training loss: 14.395001411437988 = 1.4168585538864136 + 2.0 * 6.489071369171143
Epoch 240, val loss: 1.4538993835449219
Epoch 250, training loss: 14.330790519714355 = 1.3693263530731201 + 2.0 * 6.480731964111328
Epoch 250, val loss: 1.413821816444397
Epoch 260, training loss: 14.26528549194336 = 1.3203164339065552 + 2.0 * 6.472484588623047
Epoch 260, val loss: 1.373369574546814
Epoch 270, training loss: 14.201104164123535 = 1.2705353498458862 + 2.0 * 6.46528434753418
Epoch 270, val loss: 1.3329904079437256
Epoch 280, training loss: 14.141515731811523 = 1.2206404209136963 + 2.0 * 6.460437774658203
Epoch 280, val loss: 1.2934826612472534
Epoch 290, training loss: 14.077282905578613 = 1.1715114116668701 + 2.0 * 6.452885627746582
Epoch 290, val loss: 1.2554831504821777
Epoch 300, training loss: 14.016160011291504 = 1.1237338781356812 + 2.0 * 6.446213245391846
Epoch 300, val loss: 1.219481110572815
Epoch 310, training loss: 13.958675384521484 = 1.0773253440856934 + 2.0 * 6.440674781799316
Epoch 310, val loss: 1.1855132579803467
Epoch 320, training loss: 13.904401779174805 = 1.0327532291412354 + 2.0 * 6.435824394226074
Epoch 320, val loss: 1.15388822555542
Epoch 330, training loss: 13.85251235961914 = 0.9904245734214783 + 2.0 * 6.431044101715088
Epoch 330, val loss: 1.1247942447662354
Epoch 340, training loss: 13.808182716369629 = 0.9502789378166199 + 2.0 * 6.428951740264893
Epoch 340, val loss: 1.0981653928756714
Epoch 350, training loss: 13.755833625793457 = 0.9123782515525818 + 2.0 * 6.421727657318115
Epoch 350, val loss: 1.0738000869750977
Epoch 360, training loss: 13.713430404663086 = 0.8765695095062256 + 2.0 * 6.418430328369141
Epoch 360, val loss: 1.0518336296081543
Epoch 370, training loss: 13.666312217712402 = 0.8428049683570862 + 2.0 * 6.4117536544799805
Epoch 370, val loss: 1.0319337844848633
Epoch 380, training loss: 13.626179695129395 = 0.8108446002006531 + 2.0 * 6.407667636871338
Epoch 380, val loss: 1.0140360593795776
Epoch 390, training loss: 13.591047286987305 = 0.7807157635688782 + 2.0 * 6.405165672302246
Epoch 390, val loss: 0.9980764985084534
Epoch 400, training loss: 13.550052642822266 = 0.752228319644928 + 2.0 * 6.398911952972412
Epoch 400, val loss: 0.9836599826812744
Epoch 410, training loss: 13.515501976013184 = 0.7251664400100708 + 2.0 * 6.395167827606201
Epoch 410, val loss: 0.9708231687545776
Epoch 420, training loss: 13.48654556274414 = 0.6993740797042847 + 2.0 * 6.393585681915283
Epoch 420, val loss: 0.9593462944030762
Epoch 430, training loss: 13.453514099121094 = 0.674790620803833 + 2.0 * 6.38936185836792
Epoch 430, val loss: 0.9491920471191406
Epoch 440, training loss: 13.421516418457031 = 0.6512442827224731 + 2.0 * 6.385136127471924
Epoch 440, val loss: 0.9401412010192871
Epoch 450, training loss: 13.392510414123535 = 0.6287084221839905 + 2.0 * 6.381900787353516
Epoch 450, val loss: 0.9321907162666321
Epoch 460, training loss: 13.361928939819336 = 0.6069296002388 + 2.0 * 6.377499580383301
Epoch 460, val loss: 0.9251697659492493
Epoch 470, training loss: 13.342916488647461 = 0.5858801603317261 + 2.0 * 6.378518104553223
Epoch 470, val loss: 0.9189537763595581
Epoch 480, training loss: 13.312210083007812 = 0.565609872341156 + 2.0 * 6.373300075531006
Epoch 480, val loss: 0.9135705232620239
Epoch 490, training loss: 13.284421920776367 = 0.545991837978363 + 2.0 * 6.36921501159668
Epoch 490, val loss: 0.9089760184288025
Epoch 500, training loss: 13.271738052368164 = 0.526927649974823 + 2.0 * 6.372405052185059
Epoch 500, val loss: 0.9050048589706421
Epoch 510, training loss: 13.243819236755371 = 0.5084900259971619 + 2.0 * 6.367664813995361
Epoch 510, val loss: 0.9017362594604492
Epoch 520, training loss: 13.213516235351562 = 0.49063217639923096 + 2.0 * 6.3614420890808105
Epoch 520, val loss: 0.8990980386734009
Epoch 530, training loss: 13.192079544067383 = 0.47328662872314453 + 2.0 * 6.359396457672119
Epoch 530, val loss: 0.897038459777832
Epoch 540, training loss: 13.17396354675293 = 0.456491082906723 + 2.0 * 6.358736038208008
Epoch 540, val loss: 0.8955387473106384
Epoch 550, training loss: 13.147591590881348 = 0.44019606709480286 + 2.0 * 6.353697776794434
Epoch 550, val loss: 0.8946405649185181
Epoch 560, training loss: 13.13441276550293 = 0.42439746856689453 + 2.0 * 6.355007648468018
Epoch 560, val loss: 0.8943341374397278
Epoch 570, training loss: 13.112882614135742 = 0.40907636284828186 + 2.0 * 6.351902961730957
Epoch 570, val loss: 0.8944836258888245
Epoch 580, training loss: 13.090457916259766 = 0.3942391574382782 + 2.0 * 6.348109245300293
Epoch 580, val loss: 0.8951916098594666
Epoch 590, training loss: 13.085464477539062 = 0.379826158285141 + 2.0 * 6.352818965911865
Epoch 590, val loss: 0.896430492401123
Epoch 600, training loss: 13.058942794799805 = 0.36589014530181885 + 2.0 * 6.346526145935059
Epoch 600, val loss: 0.8980291485786438
Epoch 610, training loss: 13.03529167175293 = 0.35236406326293945 + 2.0 * 6.341463565826416
Epoch 610, val loss: 0.9001642465591431
Epoch 620, training loss: 13.01937198638916 = 0.3392200171947479 + 2.0 * 6.340075969696045
Epoch 620, val loss: 0.9026910662651062
Epoch 630, training loss: 13.009668350219727 = 0.32645854353904724 + 2.0 * 6.341604709625244
Epoch 630, val loss: 0.9055617451667786
Epoch 640, training loss: 12.991965293884277 = 0.31411585211753845 + 2.0 * 6.338924884796143
Epoch 640, val loss: 0.9087704420089722
Epoch 650, training loss: 12.971951484680176 = 0.30215397477149963 + 2.0 * 6.334898948669434
Epoch 650, val loss: 0.9123919010162354
Epoch 660, training loss: 12.956908226013184 = 0.2905353903770447 + 2.0 * 6.333186626434326
Epoch 660, val loss: 0.9163603186607361
Epoch 670, training loss: 12.946945190429688 = 0.2792544364929199 + 2.0 * 6.333845615386963
Epoch 670, val loss: 0.9205508232116699
Epoch 680, training loss: 12.934186935424805 = 0.2683456242084503 + 2.0 * 6.332920551300049
Epoch 680, val loss: 0.9249237775802612
Epoch 690, training loss: 12.916417121887207 = 0.25780561566352844 + 2.0 * 6.329305648803711
Epoch 690, val loss: 0.9296519756317139
Epoch 700, training loss: 12.900059700012207 = 0.2475835531949997 + 2.0 * 6.32623815536499
Epoch 700, val loss: 0.9346618056297302
Epoch 710, training loss: 12.89651107788086 = 0.23764869570732117 + 2.0 * 6.329431056976318
Epoch 710, val loss: 0.9398666620254517
Epoch 720, training loss: 12.88586139678955 = 0.22805775701999664 + 2.0 * 6.328901767730713
Epoch 720, val loss: 0.9452990889549255
Epoch 730, training loss: 12.863114356994629 = 0.21876774728298187 + 2.0 * 6.322173118591309
Epoch 730, val loss: 0.9508727788925171
Epoch 740, training loss: 12.851818084716797 = 0.20979399979114532 + 2.0 * 6.321012020111084
Epoch 740, val loss: 0.9566336274147034
Epoch 750, training loss: 12.856755256652832 = 0.20113183557987213 + 2.0 * 6.3278117179870605
Epoch 750, val loss: 0.9626322388648987
Epoch 760, training loss: 12.84294319152832 = 0.1928061991930008 + 2.0 * 6.325068473815918
Epoch 760, val loss: 0.9687789082527161
Epoch 770, training loss: 12.820243835449219 = 0.18481417000293732 + 2.0 * 6.317714691162109
Epoch 770, val loss: 0.9750569462776184
Epoch 780, training loss: 12.808754920959473 = 0.1771521121263504 + 2.0 * 6.315801620483398
Epoch 780, val loss: 0.9815196990966797
Epoch 790, training loss: 12.797890663146973 = 0.1697811335325241 + 2.0 * 6.3140549659729
Epoch 790, val loss: 0.9882024526596069
Epoch 800, training loss: 12.793012619018555 = 0.1626928597688675 + 2.0 * 6.315159797668457
Epoch 800, val loss: 0.9950562715530396
Epoch 810, training loss: 12.782665252685547 = 0.15591397881507874 + 2.0 * 6.313375473022461
Epoch 810, val loss: 1.0020487308502197
Epoch 820, training loss: 12.784607887268066 = 0.14942814409732819 + 2.0 * 6.31758975982666
Epoch 820, val loss: 1.0092095136642456
Epoch 830, training loss: 12.76560115814209 = 0.1432550847530365 + 2.0 * 6.311172962188721
Epoch 830, val loss: 1.0164542198181152
Epoch 840, training loss: 12.75879955291748 = 0.1373514086008072 + 2.0 * 6.310724258422852
Epoch 840, val loss: 1.0239273309707642
Epoch 850, training loss: 12.748625755310059 = 0.13171333074569702 + 2.0 * 6.3084564208984375
Epoch 850, val loss: 1.0314199924468994
Epoch 860, training loss: 12.743370056152344 = 0.12632794678211212 + 2.0 * 6.308521270751953
Epoch 860, val loss: 1.0390907526016235
Epoch 870, training loss: 12.735079765319824 = 0.12118469923734665 + 2.0 * 6.306947708129883
Epoch 870, val loss: 1.0468851327896118
Epoch 880, training loss: 12.732048988342285 = 0.11627347767353058 + 2.0 * 6.307887554168701
Epoch 880, val loss: 1.0546940565109253
Epoch 890, training loss: 12.731258392333984 = 0.11159877479076385 + 2.0 * 6.3098297119140625
Epoch 890, val loss: 1.0626624822616577
Epoch 900, training loss: 12.723235130310059 = 0.10715305805206299 + 2.0 * 6.308041095733643
Epoch 900, val loss: 1.070622205734253
Epoch 910, training loss: 12.706790924072266 = 0.10291649401187897 + 2.0 * 6.301937103271484
Epoch 910, val loss: 1.0786244869232178
Epoch 920, training loss: 12.701436996459961 = 0.09887697547674179 + 2.0 * 6.3012800216674805
Epoch 920, val loss: 1.0867352485656738
Epoch 930, training loss: 12.713016510009766 = 0.09502743929624557 + 2.0 * 6.308994770050049
Epoch 930, val loss: 1.0949430465698242
Epoch 940, training loss: 12.696846961975098 = 0.09136102348566055 + 2.0 * 6.302742958068848
Epoch 940, val loss: 1.1030324697494507
Epoch 950, training loss: 12.684953689575195 = 0.08785947412252426 + 2.0 * 6.298547267913818
Epoch 950, val loss: 1.1111507415771484
Epoch 960, training loss: 12.680027961730957 = 0.08452663570642471 + 2.0 * 6.297750473022461
Epoch 960, val loss: 1.1193897724151611
Epoch 970, training loss: 12.686742782592773 = 0.08134181797504425 + 2.0 * 6.302700519561768
Epoch 970, val loss: 1.1276260614395142
Epoch 980, training loss: 12.682708740234375 = 0.07831050455570221 + 2.0 * 6.302198886871338
Epoch 980, val loss: 1.1360164880752563
Epoch 990, training loss: 12.665976524353027 = 0.07540328055620193 + 2.0 * 6.295286655426025
Epoch 990, val loss: 1.143998146057129
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7333333333333334
0.808645229309436
=== training gcn model ===
Epoch 0, training loss: 19.139385223388672 = 1.9456721544265747 + 2.0 * 8.596856117248535
Epoch 0, val loss: 1.9422519207000732
Epoch 10, training loss: 19.128747940063477 = 1.9356052875518799 + 2.0 * 8.59657096862793
Epoch 10, val loss: 1.9324512481689453
Epoch 20, training loss: 19.111093521118164 = 1.9231891632080078 + 2.0 * 8.593952178955078
Epoch 20, val loss: 1.919627070426941
Epoch 30, training loss: 19.053382873535156 = 1.906509280204773 + 2.0 * 8.573436737060547
Epoch 30, val loss: 1.9019497632980347
Epoch 40, training loss: 18.756465911865234 = 1.8871632814407349 + 2.0 * 8.434651374816895
Epoch 40, val loss: 1.882383108139038
Epoch 50, training loss: 18.074106216430664 = 1.8686716556549072 + 2.0 * 8.102717399597168
Epoch 50, val loss: 1.8645613193511963
Epoch 60, training loss: 17.091014862060547 = 1.8541971445083618 + 2.0 * 7.618409156799316
Epoch 60, val loss: 1.8511059284210205
Epoch 70, training loss: 16.246143341064453 = 1.8428137302398682 + 2.0 * 7.201664924621582
Epoch 70, val loss: 1.8401144742965698
Epoch 80, training loss: 15.851476669311523 = 1.831888198852539 + 2.0 * 7.009794235229492
Epoch 80, val loss: 1.8292816877365112
Epoch 90, training loss: 15.565298080444336 = 1.8198484182357788 + 2.0 * 6.872725009918213
Epoch 90, val loss: 1.8176145553588867
Epoch 100, training loss: 15.367481231689453 = 1.8078391551971436 + 2.0 * 6.779820919036865
Epoch 100, val loss: 1.8064032793045044
Epoch 110, training loss: 15.237273216247559 = 1.7965465784072876 + 2.0 * 6.720363140106201
Epoch 110, val loss: 1.7958048582077026
Epoch 120, training loss: 15.13353443145752 = 1.7855786085128784 + 2.0 * 6.673977851867676
Epoch 120, val loss: 1.7854067087173462
Epoch 130, training loss: 15.047444343566895 = 1.7743617296218872 + 2.0 * 6.636541366577148
Epoch 130, val loss: 1.7749593257904053
Epoch 140, training loss: 14.987849235534668 = 1.7625770568847656 + 2.0 * 6.612636089324951
Epoch 140, val loss: 1.7640591859817505
Epoch 150, training loss: 14.915390014648438 = 1.7498688697814941 + 2.0 * 6.582760810852051
Epoch 150, val loss: 1.7524733543395996
Epoch 160, training loss: 14.858221054077148 = 1.7360239028930664 + 2.0 * 6.561098575592041
Epoch 160, val loss: 1.739915370941162
Epoch 170, training loss: 14.80802059173584 = 1.720712423324585 + 2.0 * 6.543653964996338
Epoch 170, val loss: 1.7261749505996704
Epoch 180, training loss: 14.759946823120117 = 1.7035517692565918 + 2.0 * 6.528197765350342
Epoch 180, val loss: 1.7109696865081787
Epoch 190, training loss: 14.710775375366211 = 1.6844162940979004 + 2.0 * 6.513179779052734
Epoch 190, val loss: 1.6941763162612915
Epoch 200, training loss: 14.661357879638672 = 1.6629019975662231 + 2.0 * 6.499228000640869
Epoch 200, val loss: 1.6754140853881836
Epoch 210, training loss: 14.61281967163086 = 1.638533115386963 + 2.0 * 6.487143039703369
Epoch 210, val loss: 1.6543362140655518
Epoch 220, training loss: 14.566076278686523 = 1.6108529567718506 + 2.0 * 6.477611541748047
Epoch 220, val loss: 1.6306509971618652
Epoch 230, training loss: 14.531002044677734 = 1.5798823833465576 + 2.0 * 6.475559711456299
Epoch 230, val loss: 1.6043339967727661
Epoch 240, training loss: 14.466752052307129 = 1.5454596281051636 + 2.0 * 6.460646152496338
Epoch 240, val loss: 1.575195074081421
Epoch 250, training loss: 14.40993881225586 = 1.507692575454712 + 2.0 * 6.451123237609863
Epoch 250, val loss: 1.543479084968567
Epoch 260, training loss: 14.352622985839844 = 1.4659620523452759 + 2.0 * 6.44333028793335
Epoch 260, val loss: 1.5088194608688354
Epoch 270, training loss: 14.299538612365723 = 1.4208205938339233 + 2.0 * 6.439359188079834
Epoch 270, val loss: 1.4717798233032227
Epoch 280, training loss: 14.23789119720459 = 1.3732569217681885 + 2.0 * 6.43231725692749
Epoch 280, val loss: 1.4332079887390137
Epoch 290, training loss: 14.172514915466309 = 1.3234424591064453 + 2.0 * 6.424536228179932
Epoch 290, val loss: 1.393424391746521
Epoch 300, training loss: 14.111618041992188 = 1.271782398223877 + 2.0 * 6.419917583465576
Epoch 300, val loss: 1.35287344455719
Epoch 310, training loss: 14.047419548034668 = 1.2193324565887451 + 2.0 * 6.414043426513672
Epoch 310, val loss: 1.312185287475586
Epoch 320, training loss: 13.98878002166748 = 1.1669384241104126 + 2.0 * 6.4109206199646
Epoch 320, val loss: 1.2725427150726318
Epoch 330, training loss: 13.930822372436523 = 1.1153016090393066 + 2.0 * 6.407760143280029
Epoch 330, val loss: 1.234089970588684
Epoch 340, training loss: 13.874581336975098 = 1.0652323961257935 + 2.0 * 6.404674530029297
Epoch 340, val loss: 1.1977465152740479
Epoch 350, training loss: 13.809858322143555 = 1.0172771215438843 + 2.0 * 6.3962907791137695
Epoch 350, val loss: 1.1640352010726929
Epoch 360, training loss: 13.760910987854004 = 0.9718950390815735 + 2.0 * 6.394507884979248
Epoch 360, val loss: 1.1331729888916016
Epoch 370, training loss: 13.707950592041016 = 0.9296290874481201 + 2.0 * 6.389160633087158
Epoch 370, val loss: 1.1055105924606323
Epoch 380, training loss: 13.66595458984375 = 0.8903657793998718 + 2.0 * 6.387794494628906
Epoch 380, val loss: 1.0810539722442627
Epoch 390, training loss: 13.619321823120117 = 0.854151725769043 + 2.0 * 6.382585048675537
Epoch 390, val loss: 1.05938720703125
Epoch 400, training loss: 13.581564903259277 = 0.8205286264419556 + 2.0 * 6.380517959594727
Epoch 400, val loss: 1.0406855344772339
Epoch 410, training loss: 13.541104316711426 = 0.7893534302711487 + 2.0 * 6.375875473022461
Epoch 410, val loss: 1.0244221687316895
Epoch 420, training loss: 13.506634712219238 = 0.7601801156997681 + 2.0 * 6.373227119445801
Epoch 420, val loss: 1.0103353261947632
Epoch 430, training loss: 13.474045753479004 = 0.7327383756637573 + 2.0 * 6.3706536293029785
Epoch 430, val loss: 0.99793940782547
Epoch 440, training loss: 13.456270217895508 = 0.7067708373069763 + 2.0 * 6.374749660491943
Epoch 440, val loss: 0.9872646927833557
Epoch 450, training loss: 13.411065101623535 = 0.6823183298110962 + 2.0 * 6.364373207092285
Epoch 450, val loss: 0.9780104160308838
Epoch 460, training loss: 13.379247665405273 = 0.6588726043701172 + 2.0 * 6.360187530517578
Epoch 460, val loss: 0.9699150919914246
Epoch 470, training loss: 13.351250648498535 = 0.6362716555595398 + 2.0 * 6.357489585876465
Epoch 470, val loss: 0.9629708528518677
Epoch 480, training loss: 13.347505569458008 = 0.614362359046936 + 2.0 * 6.366571426391602
Epoch 480, val loss: 0.9568584561347961
Epoch 490, training loss: 13.30630111694336 = 0.5931346416473389 + 2.0 * 6.356583118438721
Epoch 490, val loss: 0.9515281319618225
Epoch 500, training loss: 13.275734901428223 = 0.5725781917572021 + 2.0 * 6.351578235626221
Epoch 500, val loss: 0.9469561576843262
Epoch 510, training loss: 13.26543140411377 = 0.5525254011154175 + 2.0 * 6.356452941894531
Epoch 510, val loss: 0.9431298971176147
Epoch 520, training loss: 13.232646942138672 = 0.5331146717071533 + 2.0 * 6.349766254425049
Epoch 520, val loss: 0.9402007460594177
Epoch 530, training loss: 13.202109336853027 = 0.5142145156860352 + 2.0 * 6.343947410583496
Epoch 530, val loss: 0.9376454949378967
Epoch 540, training loss: 13.181034088134766 = 0.49586838483810425 + 2.0 * 6.342582702636719
Epoch 540, val loss: 0.935906708240509
Epoch 550, training loss: 13.161262512207031 = 0.4780479371547699 + 2.0 * 6.341607093811035
Epoch 550, val loss: 0.9348716735839844
Epoch 560, training loss: 13.141151428222656 = 0.4608096480369568 + 2.0 * 6.340170860290527
Epoch 560, val loss: 0.9344365000724792
Epoch 570, training loss: 13.120689392089844 = 0.44420960545539856 + 2.0 * 6.338239669799805
Epoch 570, val loss: 0.9348260164260864
Epoch 580, training loss: 13.098634719848633 = 0.42811858654022217 + 2.0 * 6.3352580070495605
Epoch 580, val loss: 0.9359418153762817
Epoch 590, training loss: 13.08433723449707 = 0.4126391112804413 + 2.0 * 6.335849285125732
Epoch 590, val loss: 0.9376954436302185
Epoch 600, training loss: 13.061808586120605 = 0.3977404534816742 + 2.0 * 6.332034111022949
Epoch 600, val loss: 0.9401021599769592
Epoch 610, training loss: 13.043298721313477 = 0.3834092617034912 + 2.0 * 6.329944610595703
Epoch 610, val loss: 0.9431032538414001
Epoch 620, training loss: 13.033575057983398 = 0.36959049105644226 + 2.0 * 6.331992149353027
Epoch 620, val loss: 0.9466288685798645
Epoch 630, training loss: 13.013163566589355 = 0.3562241494655609 + 2.0 * 6.328469753265381
Epoch 630, val loss: 0.9507101774215698
Epoch 640, training loss: 12.995565414428711 = 0.34338974952697754 + 2.0 * 6.326087951660156
Epoch 640, val loss: 0.9552527666091919
Epoch 650, training loss: 12.979187965393066 = 0.3309730887413025 + 2.0 * 6.324107646942139
Epoch 650, val loss: 0.9603407979011536
Epoch 660, training loss: 12.980904579162598 = 0.31896620988845825 + 2.0 * 6.330969333648682
Epoch 660, val loss: 0.9657862782478333
Epoch 670, training loss: 12.950491905212402 = 0.30741629004478455 + 2.0 * 6.321537971496582
Epoch 670, val loss: 0.9714833498001099
Epoch 680, training loss: 12.935842514038086 = 0.29627081751823425 + 2.0 * 6.319786071777344
Epoch 680, val loss: 0.9776196479797363
Epoch 690, training loss: 12.92663288116455 = 0.2854562997817993 + 2.0 * 6.320588111877441
Epoch 690, val loss: 0.9840968251228333
Epoch 700, training loss: 12.913778305053711 = 0.27496790885925293 + 2.0 * 6.3194050788879395
Epoch 700, val loss: 0.9907791614532471
Epoch 710, training loss: 12.897275924682617 = 0.26480114459991455 + 2.0 * 6.316237449645996
Epoch 710, val loss: 0.9978592395782471
Epoch 720, training loss: 12.889042854309082 = 0.2549711763858795 + 2.0 * 6.317035675048828
Epoch 720, val loss: 1.0050538778305054
Epoch 730, training loss: 12.876419067382812 = 0.2454289048910141 + 2.0 * 6.315495014190674
Epoch 730, val loss: 1.0125038623809814
Epoch 740, training loss: 12.862101554870605 = 0.2361784130334854 + 2.0 * 6.312961578369141
Epoch 740, val loss: 1.0201499462127686
Epoch 750, training loss: 12.849906921386719 = 0.22720840573310852 + 2.0 * 6.311349391937256
Epoch 750, val loss: 1.027949333190918
Epoch 760, training loss: 12.853079795837402 = 0.2184770554304123 + 2.0 * 6.317301273345947
Epoch 760, val loss: 1.0360803604125977
Epoch 770, training loss: 12.83360481262207 = 0.21005606651306152 + 2.0 * 6.311774253845215
Epoch 770, val loss: 1.0439987182617188
Epoch 780, training loss: 12.822589874267578 = 0.20186395943164825 + 2.0 * 6.310362815856934
Epoch 780, val loss: 1.052451729774475
Epoch 790, training loss: 12.811978340148926 = 0.19395452737808228 + 2.0 * 6.309011936187744
Epoch 790, val loss: 1.0608024597167969
Epoch 800, training loss: 12.801426887512207 = 0.18629872798919678 + 2.0 * 6.3075642585754395
Epoch 800, val loss: 1.0694102048873901
Epoch 810, training loss: 12.78858470916748 = 0.17892448604106903 + 2.0 * 6.304830074310303
Epoch 810, val loss: 1.0779727697372437
Epoch 820, training loss: 12.778998374938965 = 0.17178969085216522 + 2.0 * 6.3036041259765625
Epoch 820, val loss: 1.0868562459945679
Epoch 830, training loss: 12.783219337463379 = 0.164908766746521 + 2.0 * 6.309155464172363
Epoch 830, val loss: 1.0957601070404053
Epoch 840, training loss: 12.769828796386719 = 0.1582702249288559 + 2.0 * 6.305779457092285
Epoch 840, val loss: 1.1050100326538086
Epoch 850, training loss: 12.757004737854004 = 0.15191428363323212 + 2.0 * 6.302545070648193
Epoch 850, val loss: 1.1141194105148315
Epoch 860, training loss: 12.747314453125 = 0.14583554863929749 + 2.0 * 6.300739288330078
Epoch 860, val loss: 1.1234475374221802
Epoch 870, training loss: 12.737358093261719 = 0.13998550176620483 + 2.0 * 6.298686504364014
Epoch 870, val loss: 1.1329063177108765
Epoch 880, training loss: 12.74913215637207 = 0.13436223566532135 + 2.0 * 6.307384967803955
Epoch 880, val loss: 1.142655372619629
Epoch 890, training loss: 12.733242988586426 = 0.12901368737220764 + 2.0 * 6.302114486694336
Epoch 890, val loss: 1.1521614789962769
Epoch 900, training loss: 12.716249465942383 = 0.12390090525150299 + 2.0 * 6.296174049377441
Epoch 900, val loss: 1.1617224216461182
Epoch 910, training loss: 12.708985328674316 = 0.11902007460594177 + 2.0 * 6.294982433319092
Epoch 910, val loss: 1.171539306640625
Epoch 920, training loss: 12.719137191772461 = 0.11437108367681503 + 2.0 * 6.302382946014404
Epoch 920, val loss: 1.1811939477920532
Epoch 930, training loss: 12.70327091217041 = 0.10987553745508194 + 2.0 * 6.296697616577148
Epoch 930, val loss: 1.191367506980896
Epoch 940, training loss: 12.694574356079102 = 0.10564738512039185 + 2.0 * 6.294463634490967
Epoch 940, val loss: 1.200937032699585
Epoch 950, training loss: 12.688361167907715 = 0.10158815234899521 + 2.0 * 6.293386459350586
Epoch 950, val loss: 1.210923433303833
Epoch 960, training loss: 12.683549880981445 = 0.09772797673940659 + 2.0 * 6.292911052703857
Epoch 960, val loss: 1.2206379175186157
Epoch 970, training loss: 12.673608779907227 = 0.09403999894857407 + 2.0 * 6.2897844314575195
Epoch 970, val loss: 1.23049795627594
Epoch 980, training loss: 12.672928810119629 = 0.09052510559558868 + 2.0 * 6.291202068328857
Epoch 980, val loss: 1.2403814792633057
Epoch 990, training loss: 12.680485725402832 = 0.08716846257448196 + 2.0 * 6.296658515930176
Epoch 990, val loss: 1.2503222227096558
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7222222222222222
0.8044280442804429
The final CL Acc:0.72593, 0.00524, The final GNN Acc:0.80548, 0.00228
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13306])
remove edge: torch.Size([2, 7936])
updated graph: torch.Size([2, 10686])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 19.13848114013672 = 1.9448165893554688 + 2.0 * 8.596832275390625
Epoch 0, val loss: 1.9337594509124756
Epoch 10, training loss: 19.127445220947266 = 1.9344909191131592 + 2.0 * 8.596477508544922
Epoch 10, val loss: 1.92376708984375
Epoch 20, training loss: 19.10860252380371 = 1.9221954345703125 + 2.0 * 8.5932035446167
Epoch 20, val loss: 1.9115769863128662
Epoch 30, training loss: 19.041454315185547 = 1.906180739402771 + 2.0 * 8.567636489868164
Epoch 30, val loss: 1.895666480064392
Epoch 40, training loss: 18.72067642211914 = 1.8879501819610596 + 2.0 * 8.416362762451172
Epoch 40, val loss: 1.8785021305084229
Epoch 50, training loss: 17.95562744140625 = 1.869773030281067 + 2.0 * 8.042926788330078
Epoch 50, val loss: 1.862056016921997
Epoch 60, training loss: 16.88398551940918 = 1.8542602062225342 + 2.0 * 7.514862537384033
Epoch 60, val loss: 1.848244309425354
Epoch 70, training loss: 16.099266052246094 = 1.8431297540664673 + 2.0 * 7.128067970275879
Epoch 70, val loss: 1.8376710414886475
Epoch 80, training loss: 15.829245567321777 = 1.8316458463668823 + 2.0 * 6.998799800872803
Epoch 80, val loss: 1.8268312215805054
Epoch 90, training loss: 15.627720832824707 = 1.8173866271972656 + 2.0 * 6.905167102813721
Epoch 90, val loss: 1.8143278360366821
Epoch 100, training loss: 15.463942527770996 = 1.8025782108306885 + 2.0 * 6.830682277679443
Epoch 100, val loss: 1.801667332649231
Epoch 110, training loss: 15.332735061645508 = 1.788338303565979 + 2.0 * 6.77219820022583
Epoch 110, val loss: 1.7896450757980347
Epoch 120, training loss: 15.231201171875 = 1.7739421129226685 + 2.0 * 6.7286295890808105
Epoch 120, val loss: 1.7774258852005005
Epoch 130, training loss: 15.143548011779785 = 1.759539008140564 + 2.0 * 6.692004680633545
Epoch 130, val loss: 1.7650065422058105
Epoch 140, training loss: 15.06456184387207 = 1.7443218231201172 + 2.0 * 6.660120010375977
Epoch 140, val loss: 1.7518539428710938
Epoch 150, training loss: 14.996249198913574 = 1.7275362014770508 + 2.0 * 6.634356498718262
Epoch 150, val loss: 1.737229347229004
Epoch 160, training loss: 14.925188064575195 = 1.7091526985168457 + 2.0 * 6.608017444610596
Epoch 160, val loss: 1.7212474346160889
Epoch 170, training loss: 14.858642578125 = 1.6885716915130615 + 2.0 * 6.58503532409668
Epoch 170, val loss: 1.70353102684021
Epoch 180, training loss: 14.8068208694458 = 1.6651338338851929 + 2.0 * 6.570843696594238
Epoch 180, val loss: 1.6835012435913086
Epoch 190, training loss: 14.740509033203125 = 1.6391632556915283 + 2.0 * 6.550673007965088
Epoch 190, val loss: 1.6612515449523926
Epoch 200, training loss: 14.679186820983887 = 1.6102854013442993 + 2.0 * 6.534450531005859
Epoch 200, val loss: 1.6366041898727417
Epoch 210, training loss: 14.61766529083252 = 1.577915072441101 + 2.0 * 6.5198750495910645
Epoch 210, val loss: 1.6090147495269775
Epoch 220, training loss: 14.55881118774414 = 1.5417470932006836 + 2.0 * 6.5085320472717285
Epoch 220, val loss: 1.578311562538147
Epoch 230, training loss: 14.498929977416992 = 1.5028047561645508 + 2.0 * 6.498062610626221
Epoch 230, val loss: 1.5451569557189941
Epoch 240, training loss: 14.430906295776367 = 1.4615997076034546 + 2.0 * 6.484653472900391
Epoch 240, val loss: 1.5099142789840698
Epoch 250, training loss: 14.363153457641602 = 1.417450189590454 + 2.0 * 6.472851753234863
Epoch 250, val loss: 1.4725968837738037
Epoch 260, training loss: 14.297944068908691 = 1.3710378408432007 + 2.0 * 6.46345329284668
Epoch 260, val loss: 1.43319571018219
Epoch 270, training loss: 14.239429473876953 = 1.3227769136428833 + 2.0 * 6.45832633972168
Epoch 270, val loss: 1.3928583860397339
Epoch 280, training loss: 14.168813705444336 = 1.2739659547805786 + 2.0 * 6.447423934936523
Epoch 280, val loss: 1.3518364429473877
Epoch 290, training loss: 14.102083206176758 = 1.2246567010879517 + 2.0 * 6.438713073730469
Epoch 290, val loss: 1.310573935508728
Epoch 300, training loss: 14.04782485961914 = 1.1753708124160767 + 2.0 * 6.436226844787598
Epoch 300, val loss: 1.2694734334945679
Epoch 310, training loss: 13.979300498962402 = 1.1268504858016968 + 2.0 * 6.426225185394287
Epoch 310, val loss: 1.2293590307235718
Epoch 320, training loss: 13.918501853942871 = 1.0795786380767822 + 2.0 * 6.419461727142334
Epoch 320, val loss: 1.1904875040054321
Epoch 330, training loss: 13.870993614196777 = 1.0337092876434326 + 2.0 * 6.418642044067383
Epoch 330, val loss: 1.1531717777252197
Epoch 340, training loss: 13.809513092041016 = 0.989891529083252 + 2.0 * 6.409810543060303
Epoch 340, val loss: 1.117615818977356
Epoch 350, training loss: 13.756531715393066 = 0.947965681552887 + 2.0 * 6.404283046722412
Epoch 350, val loss: 1.084136724472046
Epoch 360, training loss: 13.706502914428711 = 0.9080849289894104 + 2.0 * 6.399209022521973
Epoch 360, val loss: 1.0525482892990112
Epoch 370, training loss: 13.658016204833984 = 0.8698482513427734 + 2.0 * 6.3940839767456055
Epoch 370, val loss: 1.0226821899414062
Epoch 380, training loss: 13.621075630187988 = 0.8331759572029114 + 2.0 * 6.39394998550415
Epoch 380, val loss: 0.9945809841156006
Epoch 390, training loss: 13.576273918151855 = 0.798420250415802 + 2.0 * 6.388926982879639
Epoch 390, val loss: 0.9681338667869568
Epoch 400, training loss: 13.528797149658203 = 0.7651730179786682 + 2.0 * 6.38181209564209
Epoch 400, val loss: 0.9435434341430664
Epoch 410, training loss: 13.493324279785156 = 0.7335386872291565 + 2.0 * 6.379892826080322
Epoch 410, val loss: 0.9205812811851501
Epoch 420, training loss: 13.452905654907227 = 0.7032853960990906 + 2.0 * 6.374810218811035
Epoch 420, val loss: 0.8992266058921814
Epoch 430, training loss: 13.41989803314209 = 0.6744769215583801 + 2.0 * 6.372710704803467
Epoch 430, val loss: 0.8794480562210083
Epoch 440, training loss: 13.388323783874512 = 0.6469067931175232 + 2.0 * 6.370708465576172
Epoch 440, val loss: 0.8612467050552368
Epoch 450, training loss: 13.355940818786621 = 0.6206510066986084 + 2.0 * 6.367644786834717
Epoch 450, val loss: 0.8443695902824402
Epoch 460, training loss: 13.321784019470215 = 0.5955973863601685 + 2.0 * 6.363093376159668
Epoch 460, val loss: 0.8289007544517517
Epoch 470, training loss: 13.28996467590332 = 0.5714194178581238 + 2.0 * 6.359272480010986
Epoch 470, val loss: 0.8146101832389832
Epoch 480, training loss: 13.270358085632324 = 0.5481623411178589 + 2.0 * 6.361097812652588
Epoch 480, val loss: 0.8013787269592285
Epoch 490, training loss: 13.241271018981934 = 0.5259057879447937 + 2.0 * 6.357682704925537
Epoch 490, val loss: 0.7891463041305542
Epoch 500, training loss: 13.206295013427734 = 0.5044140219688416 + 2.0 * 6.350940704345703
Epoch 500, val loss: 0.7780242562294006
Epoch 510, training loss: 13.182112693786621 = 0.4837122857570648 + 2.0 * 6.349200248718262
Epoch 510, val loss: 0.76787269115448
Epoch 520, training loss: 13.173698425292969 = 0.46376529335975647 + 2.0 * 6.354966640472412
Epoch 520, val loss: 0.7585629820823669
Epoch 530, training loss: 13.134078979492188 = 0.44438299536705017 + 2.0 * 6.344848155975342
Epoch 530, val loss: 0.7500756978988647
Epoch 540, training loss: 13.111920356750488 = 0.42580682039260864 + 2.0 * 6.343056678771973
Epoch 540, val loss: 0.7425835728645325
Epoch 550, training loss: 13.087958335876465 = 0.40782663226127625 + 2.0 * 6.340065956115723
Epoch 550, val loss: 0.735900342464447
Epoch 560, training loss: 13.072907447814941 = 0.39036643505096436 + 2.0 * 6.341270446777344
Epoch 560, val loss: 0.7299363613128662
Epoch 570, training loss: 13.05555248260498 = 0.37348562479019165 + 2.0 * 6.341033458709717
Epoch 570, val loss: 0.7246174812316895
Epoch 580, training loss: 13.024818420410156 = 0.35719314217567444 + 2.0 * 6.333812713623047
Epoch 580, val loss: 0.7201054096221924
Epoch 590, training loss: 13.005857467651367 = 0.3413999080657959 + 2.0 * 6.332228660583496
Epoch 590, val loss: 0.7162599563598633
Epoch 600, training loss: 13.012078285217285 = 0.32615920901298523 + 2.0 * 6.342959403991699
Epoch 600, val loss: 0.7130638957023621
Epoch 610, training loss: 12.970429420471191 = 0.31122586131095886 + 2.0 * 6.329601764678955
Epoch 610, val loss: 0.7103242874145508
Epoch 620, training loss: 12.950447082519531 = 0.296935498714447 + 2.0 * 6.326756000518799
Epoch 620, val loss: 0.708379864692688
Epoch 630, training loss: 12.932819366455078 = 0.2831241488456726 + 2.0 * 6.32484769821167
Epoch 630, val loss: 0.7069718241691589
Epoch 640, training loss: 12.916849136352539 = 0.26972758769989014 + 2.0 * 6.32356071472168
Epoch 640, val loss: 0.7060869336128235
Epoch 650, training loss: 12.912500381469727 = 0.25681450963020325 + 2.0 * 6.327842712402344
Epoch 650, val loss: 0.7056464552879333
Epoch 660, training loss: 12.891594886779785 = 0.2443765550851822 + 2.0 * 6.323609352111816
Epoch 660, val loss: 0.7057685852050781
Epoch 670, training loss: 12.871371269226074 = 0.23247309029102325 + 2.0 * 6.319448947906494
Epoch 670, val loss: 0.7064799666404724
Epoch 680, training loss: 12.856579780578613 = 0.22105856239795685 + 2.0 * 6.317760467529297
Epoch 680, val loss: 0.7076787948608398
Epoch 690, training loss: 12.85245132446289 = 0.21007040143013 + 2.0 * 6.321190357208252
Epoch 690, val loss: 0.7093229293823242
Epoch 700, training loss: 12.832318305969238 = 0.19962403178215027 + 2.0 * 6.316347122192383
Epoch 700, val loss: 0.7113593220710754
Epoch 710, training loss: 12.81860637664795 = 0.1896294206380844 + 2.0 * 6.314488410949707
Epoch 710, val loss: 0.7138593196868896
Epoch 720, training loss: 12.80868148803711 = 0.18012182414531708 + 2.0 * 6.314280033111572
Epoch 720, val loss: 0.7167243361473083
Epoch 730, training loss: 12.794961929321289 = 0.1710783988237381 + 2.0 * 6.311941623687744
Epoch 730, val loss: 0.7198359370231628
Epoch 740, training loss: 12.781781196594238 = 0.16248813271522522 + 2.0 * 6.3096466064453125
Epoch 740, val loss: 0.7234156131744385
Epoch 750, training loss: 12.77755355834961 = 0.1543685644865036 + 2.0 * 6.3115925788879395
Epoch 750, val loss: 0.7273078560829163
Epoch 760, training loss: 12.761999130249023 = 0.1466498076915741 + 2.0 * 6.307674884796143
Epoch 760, val loss: 0.7313183546066284
Epoch 770, training loss: 12.753265380859375 = 0.1393977254629135 + 2.0 * 6.306933879852295
Epoch 770, val loss: 0.7356934547424316
Epoch 780, training loss: 12.757874488830566 = 0.132563516497612 + 2.0 * 6.312655448913574
Epoch 780, val loss: 0.7401419281959534
Epoch 790, training loss: 12.73619556427002 = 0.1260964572429657 + 2.0 * 6.305049419403076
Epoch 790, val loss: 0.7448627352714539
Epoch 800, training loss: 12.724481582641602 = 0.12003874778747559 + 2.0 * 6.302221298217773
Epoch 800, val loss: 0.7497748732566833
Epoch 810, training loss: 12.717018127441406 = 0.1143365278840065 + 2.0 * 6.301340579986572
Epoch 810, val loss: 0.7548628449440002
Epoch 820, training loss: 12.709867477416992 = 0.10895858705043793 + 2.0 * 6.300454616546631
Epoch 820, val loss: 0.7600530385971069
Epoch 830, training loss: 12.714713096618652 = 0.10388888418674469 + 2.0 * 6.305412292480469
Epoch 830, val loss: 0.765284538269043
Epoch 840, training loss: 12.701926231384277 = 0.0991140827536583 + 2.0 * 6.301405906677246
Epoch 840, val loss: 0.7706435322761536
Epoch 850, training loss: 12.690074920654297 = 0.09464357793331146 + 2.0 * 6.297715663909912
Epoch 850, val loss: 0.7761203050613403
Epoch 860, training loss: 12.6834135055542 = 0.09044544398784637 + 2.0 * 6.296483993530273
Epoch 860, val loss: 0.7816609144210815
Epoch 870, training loss: 12.69091510772705 = 0.08650203794240952 + 2.0 * 6.302206516265869
Epoch 870, val loss: 0.7871384620666504
Epoch 880, training loss: 12.671730995178223 = 0.08273971825838089 + 2.0 * 6.294495582580566
Epoch 880, val loss: 0.7926398515701294
Epoch 890, training loss: 12.668184280395508 = 0.07922331988811493 + 2.0 * 6.294480323791504
Epoch 890, val loss: 0.7982744574546814
Epoch 900, training loss: 12.662529945373535 = 0.07593227177858353 + 2.0 * 6.293298721313477
Epoch 900, val loss: 0.8039323687553406
Epoch 910, training loss: 12.66452693939209 = 0.0728021189570427 + 2.0 * 6.295862197875977
Epoch 910, val loss: 0.8095540404319763
Epoch 920, training loss: 12.65227222442627 = 0.0698636919260025 + 2.0 * 6.291204452514648
Epoch 920, val loss: 0.8151944875717163
Epoch 930, training loss: 12.64965534210205 = 0.06708279997110367 + 2.0 * 6.291286468505859
Epoch 930, val loss: 0.8208494186401367
Epoch 940, training loss: 12.646955490112305 = 0.06444253772497177 + 2.0 * 6.291256427764893
Epoch 940, val loss: 0.8263938426971436
Epoch 950, training loss: 12.637389183044434 = 0.06196776032447815 + 2.0 * 6.287710666656494
Epoch 950, val loss: 0.8320453763008118
Epoch 960, training loss: 12.635037422180176 = 0.059622786939144135 + 2.0 * 6.287707328796387
Epoch 960, val loss: 0.837661862373352
Epoch 970, training loss: 12.64317798614502 = 0.05739695951342583 + 2.0 * 6.292890548706055
Epoch 970, val loss: 0.8432212471961975
Epoch 980, training loss: 12.6362943649292 = 0.05526455491781235 + 2.0 * 6.290514945983887
Epoch 980, val loss: 0.8486111760139465
Epoch 990, training loss: 12.624677658081055 = 0.053289324045181274 + 2.0 * 6.285694122314453
Epoch 990, val loss: 0.8541079759597778
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.788888888888889
0.8397469688982605
=== training gcn model ===
Epoch 0, training loss: 19.155006408691406 = 1.9613319635391235 + 2.0 * 8.596837043762207
Epoch 0, val loss: 1.962203860282898
Epoch 10, training loss: 19.143558502197266 = 1.950535774230957 + 2.0 * 8.596511840820312
Epoch 10, val loss: 1.9520392417907715
Epoch 20, training loss: 19.12481689453125 = 1.9368590116500854 + 2.0 * 8.593978881835938
Epoch 20, val loss: 1.9387365579605103
Epoch 30, training loss: 19.067121505737305 = 1.91768217086792 + 2.0 * 8.574719429016113
Epoch 30, val loss: 1.9198393821716309
Epoch 40, training loss: 18.770824432373047 = 1.8930208683013916 + 2.0 * 8.438901901245117
Epoch 40, val loss: 1.8961325883865356
Epoch 50, training loss: 17.78615951538086 = 1.864487648010254 + 2.0 * 7.960836410522461
Epoch 50, val loss: 1.8688629865646362
Epoch 60, training loss: 17.28716468811035 = 1.8394136428833008 + 2.0 * 7.723875522613525
Epoch 60, val loss: 1.8465303182601929
Epoch 70, training loss: 16.6756591796875 = 1.8231791257858276 + 2.0 * 7.42624044418335
Epoch 70, val loss: 1.8319077491760254
Epoch 80, training loss: 16.15637969970703 = 1.8108750581741333 + 2.0 * 7.1727519035339355
Epoch 80, val loss: 1.8199163675308228
Epoch 90, training loss: 15.807168960571289 = 1.7972906827926636 + 2.0 * 7.004939079284668
Epoch 90, val loss: 1.8073698282241821
Epoch 100, training loss: 15.581209182739258 = 1.782515048980713 + 2.0 * 6.899346828460693
Epoch 100, val loss: 1.7944611310958862
Epoch 110, training loss: 15.391683578491211 = 1.7668943405151367 + 2.0 * 6.812394618988037
Epoch 110, val loss: 1.780922532081604
Epoch 120, training loss: 15.238719940185547 = 1.7520138025283813 + 2.0 * 6.743352890014648
Epoch 120, val loss: 1.767992377281189
Epoch 130, training loss: 15.122034072875977 = 1.736726999282837 + 2.0 * 6.692653656005859
Epoch 130, val loss: 1.754593849182129
Epoch 140, training loss: 15.031305313110352 = 1.719665288925171 + 2.0 * 6.655819892883301
Epoch 140, val loss: 1.7400116920471191
Epoch 150, training loss: 14.953730583190918 = 1.700485348701477 + 2.0 * 6.626622676849365
Epoch 150, val loss: 1.72391939163208
Epoch 160, training loss: 14.886800765991211 = 1.6789708137512207 + 2.0 * 6.603915214538574
Epoch 160, val loss: 1.7061291933059692
Epoch 170, training loss: 14.823556900024414 = 1.6547298431396484 + 2.0 * 6.584413528442383
Epoch 170, val loss: 1.6863951683044434
Epoch 180, training loss: 14.762657165527344 = 1.627522587776184 + 2.0 * 6.567567348480225
Epoch 180, val loss: 1.6643590927124023
Epoch 190, training loss: 14.702605247497559 = 1.5970934629440308 + 2.0 * 6.552755832672119
Epoch 190, val loss: 1.6397225856781006
Epoch 200, training loss: 14.646793365478516 = 1.563157081604004 + 2.0 * 6.541818141937256
Epoch 200, val loss: 1.6122974157333374
Epoch 210, training loss: 14.582468032836914 = 1.5261563062667847 + 2.0 * 6.52815580368042
Epoch 210, val loss: 1.582261562347412
Epoch 220, training loss: 14.522833824157715 = 1.4863125085830688 + 2.0 * 6.518260478973389
Epoch 220, val loss: 1.54998779296875
Epoch 230, training loss: 14.461291313171387 = 1.4445511102676392 + 2.0 * 6.5083699226379395
Epoch 230, val loss: 1.5161198377609253
Epoch 240, training loss: 14.392930030822754 = 1.4014564752578735 + 2.0 * 6.495736598968506
Epoch 240, val loss: 1.4812262058258057
Epoch 250, training loss: 14.327200889587402 = 1.3569859266281128 + 2.0 * 6.485107421875
Epoch 250, val loss: 1.4453259706497192
Epoch 260, training loss: 14.262799263000488 = 1.3114354610443115 + 2.0 * 6.475681781768799
Epoch 260, val loss: 1.4086673259735107
Epoch 270, training loss: 14.200807571411133 = 1.2654659748077393 + 2.0 * 6.467670917510986
Epoch 270, val loss: 1.3719762563705444
Epoch 280, training loss: 14.142254829406738 = 1.2202727794647217 + 2.0 * 6.460990905761719
Epoch 280, val loss: 1.3360016345977783
Epoch 290, training loss: 14.078214645385742 = 1.1754705905914307 + 2.0 * 6.451372146606445
Epoch 290, val loss: 1.3007962703704834
Epoch 300, training loss: 14.023881912231445 = 1.1311657428741455 + 2.0 * 6.4463582038879395
Epoch 300, val loss: 1.2662675380706787
Epoch 310, training loss: 13.965560913085938 = 1.0879849195480347 + 2.0 * 6.438787937164307
Epoch 310, val loss: 1.2328740358352661
Epoch 320, training loss: 13.909685134887695 = 1.045900821685791 + 2.0 * 6.431892395019531
Epoch 320, val loss: 1.2007182836532593
Epoch 330, training loss: 13.856406211853027 = 1.0047836303710938 + 2.0 * 6.425811290740967
Epoch 330, val loss: 1.1694953441619873
Epoch 340, training loss: 13.804813385009766 = 0.9645523428916931 + 2.0 * 6.420130729675293
Epoch 340, val loss: 1.139376163482666
Epoch 350, training loss: 13.755966186523438 = 0.9254564046859741 + 2.0 * 6.415255069732666
Epoch 350, val loss: 1.1104402542114258
Epoch 360, training loss: 13.706254959106445 = 0.8873156309127808 + 2.0 * 6.4094696044921875
Epoch 360, val loss: 1.0824655294418335
Epoch 370, training loss: 13.66914176940918 = 0.8500784039497375 + 2.0 * 6.409531593322754
Epoch 370, val loss: 1.0553098917007446
Epoch 380, training loss: 13.616209983825684 = 0.8141018748283386 + 2.0 * 6.4010539054870605
Epoch 380, val loss: 1.0294145345687866
Epoch 390, training loss: 13.572535514831543 = 0.7796795964241028 + 2.0 * 6.396428108215332
Epoch 390, val loss: 1.0049080848693848
Epoch 400, training loss: 13.528700828552246 = 0.7466421127319336 + 2.0 * 6.391029357910156
Epoch 400, val loss: 0.9816442131996155
Epoch 410, training loss: 13.488129615783691 = 0.7149472832679749 + 2.0 * 6.386590957641602
Epoch 410, val loss: 0.9596115946769714
Epoch 420, training loss: 13.45226764678955 = 0.6846209168434143 + 2.0 * 6.383823394775391
Epoch 420, val loss: 0.9389290809631348
Epoch 430, training loss: 13.424925804138184 = 0.6560014486312866 + 2.0 * 6.384462356567383
Epoch 430, val loss: 0.9197125434875488
Epoch 440, training loss: 13.383983612060547 = 0.6291681528091431 + 2.0 * 6.377407550811768
Epoch 440, val loss: 0.9023730754852295
Epoch 450, training loss: 13.34821891784668 = 0.6039027571678162 + 2.0 * 6.372158050537109
Epoch 450, val loss: 0.8863797783851624
Epoch 460, training loss: 13.318503379821777 = 0.5799573063850403 + 2.0 * 6.3692731857299805
Epoch 460, val loss: 0.871918797492981
Epoch 470, training loss: 13.303544044494629 = 0.5571721792221069 + 2.0 * 6.373186111450195
Epoch 470, val loss: 0.8588687181472778
Epoch 480, training loss: 13.262537956237793 = 0.5356148481369019 + 2.0 * 6.363461494445801
Epoch 480, val loss: 0.8469877243041992
Epoch 490, training loss: 13.242105484008789 = 0.5150725841522217 + 2.0 * 6.363516330718994
Epoch 490, val loss: 0.8363361358642578
Epoch 500, training loss: 13.217768669128418 = 0.49557942152023315 + 2.0 * 6.3610944747924805
Epoch 500, val loss: 0.8267467617988586
Epoch 510, training loss: 13.188492774963379 = 0.47683998942375183 + 2.0 * 6.355826377868652
Epoch 510, val loss: 0.8182362914085388
Epoch 520, training loss: 13.164176940917969 = 0.4587990343570709 + 2.0 * 6.352688789367676
Epoch 520, val loss: 0.8105599880218506
Epoch 530, training loss: 13.144180297851562 = 0.44129785895347595 + 2.0 * 6.351441383361816
Epoch 530, val loss: 0.8036242723464966
Epoch 540, training loss: 13.12359619140625 = 0.4243142306804657 + 2.0 * 6.349640846252441
Epoch 540, val loss: 0.7973099946975708
Epoch 550, training loss: 13.10038948059082 = 0.40782633423805237 + 2.0 * 6.3462815284729
Epoch 550, val loss: 0.7917255163192749
Epoch 560, training loss: 13.080336570739746 = 0.3917245864868164 + 2.0 * 6.344305992126465
Epoch 560, val loss: 0.786723256111145
Epoch 570, training loss: 13.066956520080566 = 0.3759690523147583 + 2.0 * 6.345493793487549
Epoch 570, val loss: 0.7823059558868408
Epoch 580, training loss: 13.047429084777832 = 0.3605412244796753 + 2.0 * 6.343443870544434
Epoch 580, val loss: 0.7785054445266724
Epoch 590, training loss: 13.025174140930176 = 0.3454870283603668 + 2.0 * 6.33984375
Epoch 590, val loss: 0.7752047777175903
Epoch 600, training loss: 13.007159233093262 = 0.3307347595691681 + 2.0 * 6.338212013244629
Epoch 600, val loss: 0.7724395394325256
Epoch 610, training loss: 12.98788833618164 = 0.31628525257110596 + 2.0 * 6.335801601409912
Epoch 610, val loss: 0.7702925205230713
Epoch 620, training loss: 12.969733238220215 = 0.30219730734825134 + 2.0 * 6.333767890930176
Epoch 620, val loss: 0.7686939239501953
Epoch 630, training loss: 12.973681449890137 = 0.288446843624115 + 2.0 * 6.342617511749268
Epoch 630, val loss: 0.7675387859344482
Epoch 640, training loss: 12.936521530151367 = 0.27522510290145874 + 2.0 * 6.330648422241211
Epoch 640, val loss: 0.7671465873718262
Epoch 650, training loss: 12.92103385925293 = 0.2624027132987976 + 2.0 * 6.329315662384033
Epoch 650, val loss: 0.7672704458236694
Epoch 660, training loss: 12.901885986328125 = 0.250043123960495 + 2.0 * 6.325921535491943
Epoch 660, val loss: 0.7679330706596375
Epoch 670, training loss: 12.888131141662598 = 0.23814044892787933 + 2.0 * 6.324995517730713
Epoch 670, val loss: 0.7691457867622375
Epoch 680, training loss: 12.880906105041504 = 0.22672395408153534 + 2.0 * 6.327091217041016
Epoch 680, val loss: 0.7707919478416443
Epoch 690, training loss: 12.863978385925293 = 0.21590963006019592 + 2.0 * 6.324034214019775
Epoch 690, val loss: 0.773147463798523
Epoch 700, training loss: 12.848004341125488 = 0.20561686158180237 + 2.0 * 6.321193695068359
Epoch 700, val loss: 0.7759865522384644
Epoch 710, training loss: 12.834403038024902 = 0.1958179622888565 + 2.0 * 6.3192925453186035
Epoch 710, val loss: 0.7791423797607422
Epoch 720, training loss: 12.84011459350586 = 0.18649309873580933 + 2.0 * 6.326810836791992
Epoch 720, val loss: 0.782671332359314
Epoch 730, training loss: 12.814845085144043 = 0.17769677937030792 + 2.0 * 6.318573951721191
Epoch 730, val loss: 0.7865822315216064
Epoch 740, training loss: 12.80193042755127 = 0.16935649514198303 + 2.0 * 6.316287040710449
Epoch 740, val loss: 0.7908341288566589
Epoch 750, training loss: 12.790851593017578 = 0.1614573746919632 + 2.0 * 6.314697265625
Epoch 750, val loss: 0.7953780293464661
Epoch 760, training loss: 12.783044815063477 = 0.1539885252714157 + 2.0 * 6.314527988433838
Epoch 760, val loss: 0.8001652956008911
Epoch 770, training loss: 12.772046089172363 = 0.14692527055740356 + 2.0 * 6.312560558319092
Epoch 770, val loss: 0.8052915930747986
Epoch 780, training loss: 12.766051292419434 = 0.14026018977165222 + 2.0 * 6.312895774841309
Epoch 780, val loss: 0.8106509447097778
Epoch 790, training loss: 12.754302978515625 = 0.13393127918243408 + 2.0 * 6.31018590927124
Epoch 790, val loss: 0.8161484599113464
Epoch 800, training loss: 12.743768692016602 = 0.12797150015830994 + 2.0 * 6.30789852142334
Epoch 800, val loss: 0.8219706416130066
Epoch 810, training loss: 12.751155853271484 = 0.12232191115617752 + 2.0 * 6.314416885375977
Epoch 810, val loss: 0.8279580473899841
Epoch 820, training loss: 12.741034507751465 = 0.1169666275382042 + 2.0 * 6.3120341300964355
Epoch 820, val loss: 0.8337072134017944
Epoch 830, training loss: 12.722896575927734 = 0.11192014813423157 + 2.0 * 6.305488109588623
Epoch 830, val loss: 0.8401580452919006
Epoch 840, training loss: 12.713050842285156 = 0.10715053230524063 + 2.0 * 6.302950382232666
Epoch 840, val loss: 0.8465385437011719
Epoch 850, training loss: 12.706814765930176 = 0.10262132436037064 + 2.0 * 6.302096843719482
Epoch 850, val loss: 0.8530451059341431
Epoch 860, training loss: 12.71546745300293 = 0.09832781553268433 + 2.0 * 6.30856990814209
Epoch 860, val loss: 0.8596317768096924
Epoch 870, training loss: 12.703019142150879 = 0.09421803802251816 + 2.0 * 6.304400444030762
Epoch 870, val loss: 0.8660438060760498
Epoch 880, training loss: 12.68994140625 = 0.09036490321159363 + 2.0 * 6.299788475036621
Epoch 880, val loss: 0.8730515837669373
Epoch 890, training loss: 12.68504810333252 = 0.08670219779014587 + 2.0 * 6.299172878265381
Epoch 890, val loss: 0.8798542022705078
Epoch 900, training loss: 12.68690013885498 = 0.08321742713451385 + 2.0 * 6.3018412590026855
Epoch 900, val loss: 0.8866125345230103
Epoch 910, training loss: 12.676590919494629 = 0.07990416139364243 + 2.0 * 6.298343181610107
Epoch 910, val loss: 0.8935834169387817
Epoch 920, training loss: 12.674843788146973 = 0.07676962018013 + 2.0 * 6.299036979675293
Epoch 920, val loss: 0.900504469871521
Epoch 930, training loss: 12.664572715759277 = 0.07378841936588287 + 2.0 * 6.295392036437988
Epoch 930, val loss: 0.9076288342475891
Epoch 940, training loss: 12.661216735839844 = 0.07095616310834885 + 2.0 * 6.295130252838135
Epoch 940, val loss: 0.9146865010261536
Epoch 950, training loss: 12.659247398376465 = 0.06825302541255951 + 2.0 * 6.295497417449951
Epoch 950, val loss: 0.9216800332069397
Epoch 960, training loss: 12.660106658935547 = 0.0656781867146492 + 2.0 * 6.297214031219482
Epoch 960, val loss: 0.9286485910415649
Epoch 970, training loss: 12.6488037109375 = 0.06323999911546707 + 2.0 * 6.292781829833984
Epoch 970, val loss: 0.9357858300209045
Epoch 980, training loss: 12.644427299499512 = 0.06090742349624634 + 2.0 * 6.291759967803955
Epoch 980, val loss: 0.9428293108940125
Epoch 990, training loss: 12.640997886657715 = 0.05869410187005997 + 2.0 * 6.291152000427246
Epoch 990, val loss: 0.9498639106750488
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7666666666666667
0.8386926726410122
=== training gcn model ===
Epoch 0, training loss: 19.132911682128906 = 1.9392268657684326 + 2.0 * 8.596842765808105
Epoch 0, val loss: 1.9446913003921509
Epoch 10, training loss: 19.12271499633789 = 1.9295732975006104 + 2.0 * 8.59657096862793
Epoch 10, val loss: 1.9343637228012085
Epoch 20, training loss: 19.10639190673828 = 1.9178940057754517 + 2.0 * 8.59424877166748
Epoch 20, val loss: 1.9219543933868408
Epoch 30, training loss: 19.05590057373047 = 1.9022176265716553 + 2.0 * 8.576841354370117
Epoch 30, val loss: 1.9054651260375977
Epoch 40, training loss: 18.83411407470703 = 1.8830592632293701 + 2.0 * 8.4755277633667
Epoch 40, val loss: 1.88607656955719
Epoch 50, training loss: 18.019229888916016 = 1.8637993335723877 + 2.0 * 8.077714920043945
Epoch 50, val loss: 1.8670905828475952
Epoch 60, training loss: 17.038240432739258 = 1.8477083444595337 + 2.0 * 7.595265865325928
Epoch 60, val loss: 1.851684331893921
Epoch 70, training loss: 16.17093849182129 = 1.8343255519866943 + 2.0 * 7.168306350708008
Epoch 70, val loss: 1.838410496711731
Epoch 80, training loss: 15.83456039428711 = 1.8209810256958008 + 2.0 * 7.006789684295654
Epoch 80, val loss: 1.824786901473999
Epoch 90, training loss: 15.606040954589844 = 1.8055882453918457 + 2.0 * 6.90022611618042
Epoch 90, val loss: 1.8094029426574707
Epoch 100, training loss: 15.400245666503906 = 1.7882511615753174 + 2.0 * 6.805997371673584
Epoch 100, val loss: 1.793143630027771
Epoch 110, training loss: 15.243114471435547 = 1.7709131240844727 + 2.0 * 6.736100673675537
Epoch 110, val loss: 1.7770875692367554
Epoch 120, training loss: 15.115008354187012 = 1.7530107498168945 + 2.0 * 6.680998802185059
Epoch 120, val loss: 1.760412573814392
Epoch 130, training loss: 15.015508651733398 = 1.7332552671432495 + 2.0 * 6.64112663269043
Epoch 130, val loss: 1.7422001361846924
Epoch 140, training loss: 14.932560920715332 = 1.7109259366989136 + 2.0 * 6.6108174324035645
Epoch 140, val loss: 1.7217495441436768
Epoch 150, training loss: 14.856285095214844 = 1.6859276294708252 + 2.0 * 6.585178852081299
Epoch 150, val loss: 1.6990208625793457
Epoch 160, training loss: 14.783790588378906 = 1.658271312713623 + 2.0 * 6.5627593994140625
Epoch 160, val loss: 1.6739931106567383
Epoch 170, training loss: 14.71748161315918 = 1.627232313156128 + 2.0 * 6.545124530792236
Epoch 170, val loss: 1.6464049816131592
Epoch 180, training loss: 14.653696060180664 = 1.5928982496261597 + 2.0 * 6.530398845672607
Epoch 180, val loss: 1.6163007020950317
Epoch 190, training loss: 14.582910537719727 = 1.5551141500473022 + 2.0 * 6.5138983726501465
Epoch 190, val loss: 1.583457350730896
Epoch 200, training loss: 14.517196655273438 = 1.513517141342163 + 2.0 * 6.501839637756348
Epoch 200, val loss: 1.5475279092788696
Epoch 210, training loss: 14.450122833251953 = 1.4679845571517944 + 2.0 * 6.491069316864014
Epoch 210, val loss: 1.5083870887756348
Epoch 220, training loss: 14.384190559387207 = 1.4190587997436523 + 2.0 * 6.482565879821777
Epoch 220, val loss: 1.4668526649475098
Epoch 230, training loss: 14.313369750976562 = 1.367296576499939 + 2.0 * 6.473036766052246
Epoch 230, val loss: 1.4231749773025513
Epoch 240, training loss: 14.245721817016602 = 1.3139041662216187 + 2.0 * 6.465909004211426
Epoch 240, val loss: 1.3785456418991089
Epoch 250, training loss: 14.176311492919922 = 1.2598909139633179 + 2.0 * 6.458210468292236
Epoch 250, val loss: 1.3336652517318726
Epoch 260, training loss: 14.108040809631348 = 1.206649661064148 + 2.0 * 6.450695514678955
Epoch 260, val loss: 1.2897731065750122
Epoch 270, training loss: 14.04660415649414 = 1.1543153524398804 + 2.0 * 6.4461445808410645
Epoch 270, val loss: 1.2468677759170532
Epoch 280, training loss: 13.981264114379883 = 1.1038917303085327 + 2.0 * 6.438686370849609
Epoch 280, val loss: 1.2055740356445312
Epoch 290, training loss: 13.919090270996094 = 1.0557717084884644 + 2.0 * 6.43165922164917
Epoch 290, val loss: 1.1666264533996582
Epoch 300, training loss: 13.862979888916016 = 1.0098779201507568 + 2.0 * 6.42655086517334
Epoch 300, val loss: 1.1298190355300903
Epoch 310, training loss: 13.812567710876465 = 0.9663511514663696 + 2.0 * 6.423108100891113
Epoch 310, val loss: 1.0951273441314697
Epoch 320, training loss: 13.758583068847656 = 0.9253741502761841 + 2.0 * 6.416604518890381
Epoch 320, val loss: 1.0626336336135864
Epoch 330, training loss: 13.707460403442383 = 0.8866775035858154 + 2.0 * 6.410391330718994
Epoch 330, val loss: 1.0322052240371704
Epoch 340, training loss: 13.672635078430176 = 0.8500087261199951 + 2.0 * 6.411313056945801
Epoch 340, val loss: 1.0036259889602661
Epoch 350, training loss: 13.620412826538086 = 0.8157166838645935 + 2.0 * 6.402348041534424
Epoch 350, val loss: 0.9772772789001465
Epoch 360, training loss: 13.58050537109375 = 0.7834214568138123 + 2.0 * 6.3985419273376465
Epoch 360, val loss: 0.9527838826179504
Epoch 370, training loss: 13.540348052978516 = 0.7529181838035583 + 2.0 * 6.393714904785156
Epoch 370, val loss: 0.9300206303596497
Epoch 380, training loss: 13.504110336303711 = 0.7238835692405701 + 2.0 * 6.390113353729248
Epoch 380, val loss: 0.9088708162307739
Epoch 390, training loss: 13.474645614624023 = 0.6962937712669373 + 2.0 * 6.389175891876221
Epoch 390, val loss: 0.8892369866371155
Epoch 400, training loss: 13.438942909240723 = 0.6700219511985779 + 2.0 * 6.38446044921875
Epoch 400, val loss: 0.8712121248245239
Epoch 410, training loss: 13.405139923095703 = 0.6449711918830872 + 2.0 * 6.38008451461792
Epoch 410, val loss: 0.8545182943344116
Epoch 420, training loss: 13.379271507263184 = 0.6208732724189758 + 2.0 * 6.379199028015137
Epoch 420, val loss: 0.8390988707542419
Epoch 430, training loss: 13.348265647888184 = 0.5976172089576721 + 2.0 * 6.375324249267578
Epoch 430, val loss: 0.8248588442802429
Epoch 440, training loss: 13.320768356323242 = 0.5753542184829712 + 2.0 * 6.372706890106201
Epoch 440, val loss: 0.8118318915367126
Epoch 450, training loss: 13.290468215942383 = 0.5538164377212524 + 2.0 * 6.368325710296631
Epoch 450, val loss: 0.7998788952827454
Epoch 460, training loss: 13.274044036865234 = 0.5331049561500549 + 2.0 * 6.370469570159912
Epoch 460, val loss: 0.7889211773872375
Epoch 470, training loss: 13.242095947265625 = 0.5131720304489136 + 2.0 * 6.364461898803711
Epoch 470, val loss: 0.7789644598960876
Epoch 480, training loss: 13.216171264648438 = 0.49401015043258667 + 2.0 * 6.361080646514893
Epoch 480, val loss: 0.7700384259223938
Epoch 490, training loss: 13.193708419799805 = 0.4755406379699707 + 2.0 * 6.359084129333496
Epoch 490, val loss: 0.7619621157646179
Epoch 500, training loss: 13.169783592224121 = 0.4577208161354065 + 2.0 * 6.35603141784668
Epoch 500, val loss: 0.7546498775482178
Epoch 510, training loss: 13.150713920593262 = 0.4404833912849426 + 2.0 * 6.3551154136657715
Epoch 510, val loss: 0.7479645013809204
Epoch 520, training loss: 13.129218101501465 = 0.42388206720352173 + 2.0 * 6.352667808532715
Epoch 520, val loss: 0.7420456409454346
Epoch 530, training loss: 13.10661792755127 = 0.4077773690223694 + 2.0 * 6.349420070648193
Epoch 530, val loss: 0.736610472202301
Epoch 540, training loss: 13.09005069732666 = 0.3920752704143524 + 2.0 * 6.348987579345703
Epoch 540, val loss: 0.7316001057624817
Epoch 550, training loss: 13.080297470092773 = 0.3768460154533386 + 2.0 * 6.3517255783081055
Epoch 550, val loss: 0.7271301746368408
Epoch 560, training loss: 13.049914360046387 = 0.3620150685310364 + 2.0 * 6.343949794769287
Epoch 560, val loss: 0.7230151891708374
Epoch 570, training loss: 13.029361724853516 = 0.34754520654678345 + 2.0 * 6.340908050537109
Epoch 570, val loss: 0.7192102670669556
Epoch 580, training loss: 13.010025978088379 = 0.33340564370155334 + 2.0 * 6.338310241699219
Epoch 580, val loss: 0.7157725691795349
Epoch 590, training loss: 13.011483192443848 = 0.3195900022983551 + 2.0 * 6.345946788787842
Epoch 590, val loss: 0.7126652598381042
Epoch 600, training loss: 12.98397445678711 = 0.30609896779060364 + 2.0 * 6.338937759399414
Epoch 600, val loss: 0.7098101377487183
Epoch 610, training loss: 12.960769653320312 = 0.2930484116077423 + 2.0 * 6.333860397338867
Epoch 610, val loss: 0.7072661519050598
Epoch 620, training loss: 12.94295597076416 = 0.28039243817329407 + 2.0 * 6.331281661987305
Epoch 620, val loss: 0.7052057385444641
Epoch 630, training loss: 12.928730964660645 = 0.2680734694004059 + 2.0 * 6.330328941345215
Epoch 630, val loss: 0.7034081816673279
Epoch 640, training loss: 12.922651290893555 = 0.2561590373516083 + 2.0 * 6.333246231079102
Epoch 640, val loss: 0.7019604444503784
Epoch 650, training loss: 12.898016929626465 = 0.24463459849357605 + 2.0 * 6.326691150665283
Epoch 650, val loss: 0.7007883191108704
Epoch 660, training loss: 12.883913040161133 = 0.23359020054340363 + 2.0 * 6.325161457061768
Epoch 660, val loss: 0.7000399231910706
Epoch 670, training loss: 12.883735656738281 = 0.22299964725971222 + 2.0 * 6.3303680419921875
Epoch 670, val loss: 0.6998022198677063
Epoch 680, training loss: 12.858075141906738 = 0.21283984184265137 + 2.0 * 6.322617530822754
Epoch 680, val loss: 0.6998037099838257
Epoch 690, training loss: 12.843764305114746 = 0.20314393937587738 + 2.0 * 6.320310115814209
Epoch 690, val loss: 0.700221598148346
Epoch 700, training loss: 12.839662551879883 = 0.19388771057128906 + 2.0 * 6.322887420654297
Epoch 700, val loss: 0.7009541392326355
Epoch 710, training loss: 12.824952125549316 = 0.18511787056922913 + 2.0 * 6.31991720199585
Epoch 710, val loss: 0.7023209929466248
Epoch 720, training loss: 12.810569763183594 = 0.1767522096633911 + 2.0 * 6.316908836364746
Epoch 720, val loss: 0.703758955001831
Epoch 730, training loss: 12.798423767089844 = 0.16882820427417755 + 2.0 * 6.314797878265381
Epoch 730, val loss: 0.7056699991226196
Epoch 740, training loss: 12.815537452697754 = 0.1613076627254486 + 2.0 * 6.327115058898926
Epoch 740, val loss: 0.7078901529312134
Epoch 750, training loss: 12.77985668182373 = 0.15415486693382263 + 2.0 * 6.3128509521484375
Epoch 750, val loss: 0.710423469543457
Epoch 760, training loss: 12.771685600280762 = 0.14738623797893524 + 2.0 * 6.312149524688721
Epoch 760, val loss: 0.7131605744361877
Epoch 770, training loss: 12.759732246398926 = 0.14097164571285248 + 2.0 * 6.309380531311035
Epoch 770, val loss: 0.7161856889724731
Epoch 780, training loss: 12.782686233520508 = 0.13487087190151215 + 2.0 * 6.323907852172852
Epoch 780, val loss: 0.7193731069564819
Epoch 790, training loss: 12.745262145996094 = 0.12911465764045715 + 2.0 * 6.3080735206604
Epoch 790, val loss: 0.7230339646339417
Epoch 800, training loss: 12.737143516540527 = 0.12365429103374481 + 2.0 * 6.306744575500488
Epoch 800, val loss: 0.7266638875007629
Epoch 810, training loss: 12.73011302947998 = 0.11847127228975296 + 2.0 * 6.305820941925049
Epoch 810, val loss: 0.7305213212966919
Epoch 820, training loss: 12.739082336425781 = 0.11356452107429504 + 2.0 * 6.312758922576904
Epoch 820, val loss: 0.7345331311225891
Epoch 830, training loss: 12.715523719787598 = 0.1088678166270256 + 2.0 * 6.303328037261963
Epoch 830, val loss: 0.738621175289154
Epoch 840, training loss: 12.707801818847656 = 0.10443399101495743 + 2.0 * 6.3016839027404785
Epoch 840, val loss: 0.7428638339042664
Epoch 850, training loss: 12.701175689697266 = 0.10022499412298203 + 2.0 * 6.300475120544434
Epoch 850, val loss: 0.7472915649414062
Epoch 860, training loss: 12.701139450073242 = 0.09621187299489975 + 2.0 * 6.302464008331299
Epoch 860, val loss: 0.7517880201339722
Epoch 870, training loss: 12.705158233642578 = 0.09239190816879272 + 2.0 * 6.30638313293457
Epoch 870, val loss: 0.7565058469772339
Epoch 880, training loss: 12.688549995422363 = 0.08877991884946823 + 2.0 * 6.299885272979736
Epoch 880, val loss: 0.7611936926841736
Epoch 890, training loss: 12.679110527038574 = 0.08534890413284302 + 2.0 * 6.296880722045898
Epoch 890, val loss: 0.7658118009567261
Epoch 900, training loss: 12.678038597106934 = 0.08208730071783066 + 2.0 * 6.297975540161133
Epoch 900, val loss: 0.7705804705619812
Epoch 910, training loss: 12.672943115234375 = 0.07897905260324478 + 2.0 * 6.2969818115234375
Epoch 910, val loss: 0.775506317615509
Epoch 920, training loss: 12.677530288696289 = 0.07602448016405106 + 2.0 * 6.300753116607666
Epoch 920, val loss: 0.7805160284042358
Epoch 930, training loss: 12.664322853088379 = 0.07320281118154526 + 2.0 * 6.295559883117676
Epoch 930, val loss: 0.7854726314544678
Epoch 940, training loss: 12.662718772888184 = 0.07052711397409439 + 2.0 * 6.296095848083496
Epoch 940, val loss: 0.7904992699623108
Epoch 950, training loss: 12.651582717895508 = 0.06797725707292557 + 2.0 * 6.291802883148193
Epoch 950, val loss: 0.7955884337425232
Epoch 960, training loss: 12.650321960449219 = 0.06554608047008514 + 2.0 * 6.292387962341309
Epoch 960, val loss: 0.800642728805542
Epoch 970, training loss: 12.654800415039062 = 0.06322259455919266 + 2.0 * 6.295788764953613
Epoch 970, val loss: 0.8057364821434021
Epoch 980, training loss: 12.641505241394043 = 0.06101803481578827 + 2.0 * 6.290243625640869
Epoch 980, val loss: 0.8110342621803284
Epoch 990, training loss: 12.636455535888672 = 0.05890975147485733 + 2.0 * 6.288773059844971
Epoch 990, val loss: 0.8161582350730896
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8037037037037037
0.8365840801265156
The final CL Acc:0.78642, 0.01522, The final GNN Acc:0.83834, 0.00131
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.4 der2:0 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11644])
remove edge: torch.Size([2, 9486])
updated graph: torch.Size([2, 10574])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 19.149471282958984 = 1.9557607173919678 + 2.0 * 8.596855163574219
Epoch 0, val loss: 1.9509968757629395
Epoch 10, training loss: 19.13807487487793 = 1.9447420835494995 + 2.0 * 8.59666633605957
Epoch 10, val loss: 1.939910888671875
Epoch 20, training loss: 19.121809005737305 = 1.9314144849777222 + 2.0 * 8.595197677612305
Epoch 20, val loss: 1.9265801906585693
Epoch 30, training loss: 19.07880973815918 = 1.9133334159851074 + 2.0 * 8.582737922668457
Epoch 30, val loss: 1.9088051319122314
Epoch 40, training loss: 18.875160217285156 = 1.8895788192749023 + 2.0 * 8.492791175842285
Epoch 40, val loss: 1.886356234550476
Epoch 50, training loss: 17.88275146484375 = 1.8629801273345947 + 2.0 * 8.009885787963867
Epoch 50, val loss: 1.8621221780776978
Epoch 60, training loss: 16.905147552490234 = 1.8424899578094482 + 2.0 * 7.531329154968262
Epoch 60, val loss: 1.843921422958374
Epoch 70, training loss: 16.089582443237305 = 1.8295867443084717 + 2.0 * 7.129998207092285
Epoch 70, val loss: 1.831047773361206
Epoch 80, training loss: 15.749429702758789 = 1.8173093795776367 + 2.0 * 6.966060161590576
Epoch 80, val loss: 1.818802833557129
Epoch 90, training loss: 15.543536186218262 = 1.8023574352264404 + 2.0 * 6.870589256286621
Epoch 90, val loss: 1.804721474647522
Epoch 100, training loss: 15.364270210266113 = 1.7869069576263428 + 2.0 * 6.788681507110596
Epoch 100, val loss: 1.7906794548034668
Epoch 110, training loss: 15.219069480895996 = 1.7734516859054565 + 2.0 * 6.722808837890625
Epoch 110, val loss: 1.7784318923950195
Epoch 120, training loss: 15.103099822998047 = 1.761068344116211 + 2.0 * 6.671015739440918
Epoch 120, val loss: 1.7666939496994019
Epoch 130, training loss: 15.007193565368652 = 1.7481820583343506 + 2.0 * 6.629505634307861
Epoch 130, val loss: 1.7545729875564575
Epoch 140, training loss: 14.921910285949707 = 1.7343612909317017 + 2.0 * 6.593774318695068
Epoch 140, val loss: 1.7418323755264282
Epoch 150, training loss: 14.850202560424805 = 1.7193883657455444 + 2.0 * 6.5654072761535645
Epoch 150, val loss: 1.7280328273773193
Epoch 160, training loss: 14.790290832519531 = 1.7028080224990845 + 2.0 * 6.543741226196289
Epoch 160, val loss: 1.7128016948699951
Epoch 170, training loss: 14.72884464263916 = 1.6844371557235718 + 2.0 * 6.5222039222717285
Epoch 170, val loss: 1.6960169076919556
Epoch 180, training loss: 14.674532890319824 = 1.664025902748108 + 2.0 * 6.505253314971924
Epoch 180, val loss: 1.677573561668396
Epoch 190, training loss: 14.622833251953125 = 1.6414145231246948 + 2.0 * 6.49070930480957
Epoch 190, val loss: 1.6572787761688232
Epoch 200, training loss: 14.567708969116211 = 1.6164547204971313 + 2.0 * 6.4756269454956055
Epoch 200, val loss: 1.6351414918899536
Epoch 210, training loss: 14.525175094604492 = 1.5891433954238892 + 2.0 * 6.468015670776367
Epoch 210, val loss: 1.6110585927963257
Epoch 220, training loss: 14.466750144958496 = 1.5597244501113892 + 2.0 * 6.453512668609619
Epoch 220, val loss: 1.5853869915008545
Epoch 230, training loss: 14.414259910583496 = 1.5282613039016724 + 2.0 * 6.442999362945557
Epoch 230, val loss: 1.5581655502319336
Epoch 240, training loss: 14.363956451416016 = 1.4947733879089355 + 2.0 * 6.434591770172119
Epoch 240, val loss: 1.5296533107757568
Epoch 250, training loss: 14.312775611877441 = 1.4595826864242554 + 2.0 * 6.426596641540527
Epoch 250, val loss: 1.500208854675293
Epoch 260, training loss: 14.26309585571289 = 1.4231419563293457 + 2.0 * 6.419976711273193
Epoch 260, val loss: 1.4702461957931519
Epoch 270, training loss: 14.211731910705566 = 1.3854506015777588 + 2.0 * 6.413140773773193
Epoch 270, val loss: 1.4399033784866333
Epoch 280, training loss: 14.174924850463867 = 1.3467293977737427 + 2.0 * 6.414097785949707
Epoch 280, val loss: 1.4094221591949463
Epoch 290, training loss: 14.1111421585083 = 1.3077476024627686 + 2.0 * 6.401697158813477
Epoch 290, val loss: 1.3792825937271118
Epoch 300, training loss: 14.062872886657715 = 1.2683378458023071 + 2.0 * 6.3972673416137695
Epoch 300, val loss: 1.3494899272918701
Epoch 310, training loss: 14.011653900146484 = 1.2283968925476074 + 2.0 * 6.391628265380859
Epoch 310, val loss: 1.319836974143982
Epoch 320, training loss: 13.980228424072266 = 1.1880998611450195 + 2.0 * 6.396064281463623
Epoch 320, val loss: 1.290522575378418
Epoch 330, training loss: 13.918639183044434 = 1.1481447219848633 + 2.0 * 6.385247230529785
Epoch 330, val loss: 1.2618727684020996
Epoch 340, training loss: 13.867106437683105 = 1.1083338260650635 + 2.0 * 6.3793864250183105
Epoch 340, val loss: 1.2336763143539429
Epoch 350, training loss: 13.819598197937012 = 1.068668007850647 + 2.0 * 6.375464916229248
Epoch 350, val loss: 1.20596182346344
Epoch 360, training loss: 13.773542404174805 = 1.0292489528656006 + 2.0 * 6.3721466064453125
Epoch 360, val loss: 1.1787608861923218
Epoch 370, training loss: 13.735107421875 = 0.9905169010162354 + 2.0 * 6.372295379638672
Epoch 370, val loss: 1.1522938013076782
Epoch 380, training loss: 13.688345909118652 = 0.9530585408210754 + 2.0 * 6.3676438331604
Epoch 380, val loss: 1.1270184516906738
Epoch 390, training loss: 13.642199516296387 = 0.9168691039085388 + 2.0 * 6.362665176391602
Epoch 390, val loss: 1.102885365486145
Epoch 400, training loss: 13.614094734191895 = 0.8819965720176697 + 2.0 * 6.366049289703369
Epoch 400, val loss: 1.0799617767333984
Epoch 410, training loss: 13.564905166625977 = 0.8491035103797913 + 2.0 * 6.357900619506836
Epoch 410, val loss: 1.0587605237960815
Epoch 420, training loss: 13.529159545898438 = 0.8179900646209717 + 2.0 * 6.355584621429443
Epoch 420, val loss: 1.0391697883605957
Epoch 430, training loss: 13.490619659423828 = 0.7885472178459167 + 2.0 * 6.351036071777344
Epoch 430, val loss: 1.0210505723953247
Epoch 440, training loss: 13.457233428955078 = 0.7604943513870239 + 2.0 * 6.348369598388672
Epoch 440, val loss: 1.0043866634368896
Epoch 450, training loss: 13.433436393737793 = 0.7337870001792908 + 2.0 * 6.349824905395508
Epoch 450, val loss: 0.9889689087867737
Epoch 460, training loss: 13.397591590881348 = 0.7084099054336548 + 2.0 * 6.344590663909912
Epoch 460, val loss: 0.9748285412788391
Epoch 470, training loss: 13.381896018981934 = 0.6841114163398743 + 2.0 * 6.3488922119140625
Epoch 470, val loss: 0.9619138240814209
Epoch 480, training loss: 13.344501495361328 = 0.6611804366111755 + 2.0 * 6.341660499572754
Epoch 480, val loss: 0.949992299079895
Epoch 490, training loss: 13.31403636932373 = 0.6390315294265747 + 2.0 * 6.337502479553223
Epoch 490, val loss: 0.9391239881515503
Epoch 500, training loss: 13.286593437194824 = 0.6174449920654297 + 2.0 * 6.334574222564697
Epoch 500, val loss: 0.9290926456451416
Epoch 510, training loss: 13.260908126831055 = 0.5962570309638977 + 2.0 * 6.332325458526611
Epoch 510, val loss: 0.9195996522903442
Epoch 520, training loss: 13.238439559936523 = 0.5753300786018372 + 2.0 * 6.331554889678955
Epoch 520, val loss: 0.910656750202179
Epoch 530, training loss: 13.216309547424316 = 0.554722785949707 + 2.0 * 6.330793380737305
Epoch 530, val loss: 0.9023258686065674
Epoch 540, training loss: 13.188438415527344 = 0.5345603823661804 + 2.0 * 6.326939105987549
Epoch 540, val loss: 0.8945536613464355
Epoch 550, training loss: 13.165358543395996 = 0.5145871639251709 + 2.0 * 6.325385570526123
Epoch 550, val loss: 0.8872836232185364
Epoch 560, training loss: 13.14498519897461 = 0.49470117688179016 + 2.0 * 6.325141906738281
Epoch 560, val loss: 0.880511462688446
Epoch 570, training loss: 13.119255065917969 = 0.47496819496154785 + 2.0 * 6.3221435546875
Epoch 570, val loss: 0.874298632144928
Epoch 580, training loss: 13.106104850769043 = 0.4553942382335663 + 2.0 * 6.325355529785156
Epoch 580, val loss: 0.8686122894287109
Epoch 590, training loss: 13.074667930603027 = 0.4360884428024292 + 2.0 * 6.319289684295654
Epoch 590, val loss: 0.863502025604248
Epoch 600, training loss: 13.052968978881836 = 0.4169811010360718 + 2.0 * 6.317994117736816
Epoch 600, val loss: 0.8590354919433594
Epoch 610, training loss: 13.04227066040039 = 0.398068904876709 + 2.0 * 6.32210111618042
Epoch 610, val loss: 0.855175793170929
Epoch 620, training loss: 13.012995719909668 = 0.37961792945861816 + 2.0 * 6.3166890144348145
Epoch 620, val loss: 0.8519887328147888
Epoch 630, training loss: 12.995856285095215 = 0.3616304099559784 + 2.0 * 6.317112922668457
Epoch 630, val loss: 0.8495063781738281
Epoch 640, training loss: 12.98544692993164 = 0.34427404403686523 + 2.0 * 6.320586681365967
Epoch 640, val loss: 0.8477320075035095
Epoch 650, training loss: 12.953853607177734 = 0.32758092880249023 + 2.0 * 6.313136100769043
Epoch 650, val loss: 0.8467181921005249
Epoch 660, training loss: 12.930723190307617 = 0.31153419613838196 + 2.0 * 6.309594631195068
Epoch 660, val loss: 0.8463148474693298
Epoch 670, training loss: 12.912405014038086 = 0.29608336091041565 + 2.0 * 6.308160781860352
Epoch 670, val loss: 0.8465239405632019
Epoch 680, training loss: 12.907249450683594 = 0.28124290704727173 + 2.0 * 6.313003063201904
Epoch 680, val loss: 0.8474113941192627
Epoch 690, training loss: 12.896676063537598 = 0.2671833038330078 + 2.0 * 6.314746379852295
Epoch 690, val loss: 0.8487539291381836
Epoch 700, training loss: 12.86290454864502 = 0.2538503408432007 + 2.0 * 6.304527282714844
Epoch 700, val loss: 0.8506106734275818
Epoch 710, training loss: 12.849332809448242 = 0.2411840558052063 + 2.0 * 6.304074287414551
Epoch 710, val loss: 0.853056013584137
Epoch 720, training loss: 12.834397315979004 = 0.22913922369480133 + 2.0 * 6.302628993988037
Epoch 720, val loss: 0.8559035062789917
Epoch 730, training loss: 12.832064628601074 = 0.21770890057086945 + 2.0 * 6.307178020477295
Epoch 730, val loss: 0.8591145277023315
Epoch 740, training loss: 12.809837341308594 = 0.20695503056049347 + 2.0 * 6.301441192626953
Epoch 740, val loss: 0.8627035021781921
Epoch 750, training loss: 12.795902252197266 = 0.19679324328899384 + 2.0 * 6.299554347991943
Epoch 750, val loss: 0.8665992617607117
Epoch 760, training loss: 12.797652244567871 = 0.18716715276241302 + 2.0 * 6.305242538452148
Epoch 760, val loss: 0.8708674907684326
Epoch 770, training loss: 12.785603523254395 = 0.17808309197425842 + 2.0 * 6.303760051727295
Epoch 770, val loss: 0.8752686381340027
Epoch 780, training loss: 12.763340950012207 = 0.1695881485939026 + 2.0 * 6.296876430511475
Epoch 780, val loss: 0.880134105682373
Epoch 790, training loss: 12.750899314880371 = 0.161537304520607 + 2.0 * 6.294681072235107
Epoch 790, val loss: 0.8852032423019409
Epoch 800, training loss: 12.742080688476562 = 0.15390177071094513 + 2.0 * 6.294089317321777
Epoch 800, val loss: 0.8905548453330994
Epoch 810, training loss: 12.74820613861084 = 0.14667581021785736 + 2.0 * 6.300765037536621
Epoch 810, val loss: 0.8960573673248291
Epoch 820, training loss: 12.735904693603516 = 0.1398690640926361 + 2.0 * 6.298017978668213
Epoch 820, val loss: 0.9016645550727844
Epoch 830, training loss: 12.719919204711914 = 0.1334429830312729 + 2.0 * 6.293238162994385
Epoch 830, val loss: 0.9074143767356873
Epoch 840, training loss: 12.70935344696045 = 0.1273869425058365 + 2.0 * 6.290983200073242
Epoch 840, val loss: 0.91334068775177
Epoch 850, training loss: 12.701891899108887 = 0.12165582925081253 + 2.0 * 6.290118217468262
Epoch 850, val loss: 0.9193843603134155
Epoch 860, training loss: 12.700979232788086 = 0.11622494459152222 + 2.0 * 6.29237699508667
Epoch 860, val loss: 0.9254825115203857
Epoch 870, training loss: 12.68544864654541 = 0.11110301315784454 + 2.0 * 6.287172794342041
Epoch 870, val loss: 0.9317256212234497
Epoch 880, training loss: 12.681310653686523 = 0.10625531524419785 + 2.0 * 6.287527561187744
Epoch 880, val loss: 0.937987744808197
Epoch 890, training loss: 12.683701515197754 = 0.10165265947580338 + 2.0 * 6.291024208068848
Epoch 890, val loss: 0.9442996382713318
Epoch 900, training loss: 12.679445266723633 = 0.09730984270572662 + 2.0 * 6.291067600250244
Epoch 900, val loss: 0.9503606557846069
Epoch 910, training loss: 12.663281440734863 = 0.09322231262922287 + 2.0 * 6.285029411315918
Epoch 910, val loss: 0.9567736387252808
Epoch 920, training loss: 12.656994819641113 = 0.08934221416711807 + 2.0 * 6.2838263511657715
Epoch 920, val loss: 0.9631538391113281
Epoch 930, training loss: 12.65938663482666 = 0.08566149324178696 + 2.0 * 6.286862373352051
Epoch 930, val loss: 0.9693423509597778
Epoch 940, training loss: 12.659242630004883 = 0.08217547833919525 + 2.0 * 6.288533687591553
Epoch 940, val loss: 0.9756560325622559
Epoch 950, training loss: 12.641390800476074 = 0.07888813316822052 + 2.0 * 6.281251430511475
Epoch 950, val loss: 0.981999397277832
Epoch 960, training loss: 12.635322570800781 = 0.0757681131362915 + 2.0 * 6.2797770500183105
Epoch 960, val loss: 0.9882431626319885
Epoch 970, training loss: 12.631731986999512 = 0.07280091196298599 + 2.0 * 6.279465675354004
Epoch 970, val loss: 0.994499921798706
Epoch 980, training loss: 12.630794525146484 = 0.06997661292552948 + 2.0 * 6.28040885925293
Epoch 980, val loss: 1.0007323026657104
Epoch 990, training loss: 12.629631996154785 = 0.06730268895626068 + 2.0 * 6.281164646148682
Epoch 990, val loss: 1.0068598985671997
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7444444444444445
0.808645229309436
=== training gcn model ===
Epoch 0, training loss: 19.13794708251953 = 1.944267988204956 + 2.0 * 8.596839904785156
Epoch 0, val loss: 1.9386667013168335
Epoch 10, training loss: 19.12705421447754 = 1.9339834451675415 + 2.0 * 8.596535682678223
Epoch 10, val loss: 1.9293773174285889
Epoch 20, training loss: 19.108633041381836 = 1.9209582805633545 + 2.0 * 8.59383773803711
Epoch 20, val loss: 1.9174214601516724
Epoch 30, training loss: 19.041881561279297 = 1.9029288291931152 + 2.0 * 8.569476127624512
Epoch 30, val loss: 1.9010834693908691
Epoch 40, training loss: 18.69200897216797 = 1.8808951377868652 + 2.0 * 8.405556678771973
Epoch 40, val loss: 1.8819162845611572
Epoch 50, training loss: 17.70956039428711 = 1.8576207160949707 + 2.0 * 7.925970077514648
Epoch 50, val loss: 1.8619853258132935
Epoch 60, training loss: 16.75855255126953 = 1.8421555757522583 + 2.0 * 7.458198070526123
Epoch 60, val loss: 1.8488523960113525
Epoch 70, training loss: 15.949030876159668 = 1.832813024520874 + 2.0 * 7.058108806610107
Epoch 70, val loss: 1.8398609161376953
Epoch 80, training loss: 15.604859352111816 = 1.8240737915039062 + 2.0 * 6.890392780303955
Epoch 80, val loss: 1.8310744762420654
Epoch 90, training loss: 15.389473915100098 = 1.8118916749954224 + 2.0 * 6.788791179656982
Epoch 90, val loss: 1.8197071552276611
Epoch 100, training loss: 15.228951454162598 = 1.7988746166229248 + 2.0 * 6.715038299560547
Epoch 100, val loss: 1.8082070350646973
Epoch 110, training loss: 15.11310863494873 = 1.7871845960617065 + 2.0 * 6.662961959838867
Epoch 110, val loss: 1.7974112033843994
Epoch 120, training loss: 15.012511253356934 = 1.775875210762024 + 2.0 * 6.6183180809021
Epoch 120, val loss: 1.7864960432052612
Epoch 130, training loss: 14.933143615722656 = 1.7639836072921753 + 2.0 * 6.584579944610596
Epoch 130, val loss: 1.7749505043029785
Epoch 140, training loss: 14.859511375427246 = 1.750848650932312 + 2.0 * 6.554331302642822
Epoch 140, val loss: 1.7626726627349854
Epoch 150, training loss: 14.797100067138672 = 1.7362452745437622 + 2.0 * 6.5304274559021
Epoch 150, val loss: 1.7493901252746582
Epoch 160, training loss: 14.754640579223633 = 1.7198106050491333 + 2.0 * 6.5174150466918945
Epoch 160, val loss: 1.7347811460494995
Epoch 170, training loss: 14.693398475646973 = 1.7012836933135986 + 2.0 * 6.496057510375977
Epoch 170, val loss: 1.718658447265625
Epoch 180, training loss: 14.640291213989258 = 1.6804505586624146 + 2.0 * 6.479920387268066
Epoch 180, val loss: 1.7007193565368652
Epoch 190, training loss: 14.591127395629883 = 1.656904935836792 + 2.0 * 6.467111110687256
Epoch 190, val loss: 1.6806952953338623
Epoch 200, training loss: 14.552698135375977 = 1.6307276487350464 + 2.0 * 6.46098518371582
Epoch 200, val loss: 1.6588150262832642
Epoch 210, training loss: 14.496317863464355 = 1.602333903312683 + 2.0 * 6.446991920471191
Epoch 210, val loss: 1.635402798652649
Epoch 220, training loss: 14.444892883300781 = 1.5714259147644043 + 2.0 * 6.436733245849609
Epoch 220, val loss: 1.610229253768921
Epoch 230, training loss: 14.397777557373047 = 1.5385973453521729 + 2.0 * 6.429590225219727
Epoch 230, val loss: 1.5841035842895508
Epoch 240, training loss: 14.346506118774414 = 1.5044389963150024 + 2.0 * 6.4210333824157715
Epoch 240, val loss: 1.557577133178711
Epoch 250, training loss: 14.296695709228516 = 1.4696645736694336 + 2.0 * 6.413515567779541
Epoch 250, val loss: 1.5311458110809326
Epoch 260, training loss: 14.252843856811523 = 1.4351375102996826 + 2.0 * 6.408853054046631
Epoch 260, val loss: 1.505741834640503
Epoch 270, training loss: 14.200587272644043 = 1.4013903141021729 + 2.0 * 6.399598598480225
Epoch 270, val loss: 1.4817967414855957
Epoch 280, training loss: 14.155816078186035 = 1.3686667680740356 + 2.0 * 6.3935747146606445
Epoch 280, val loss: 1.4594693183898926
Epoch 290, training loss: 14.113590240478516 = 1.3370599746704102 + 2.0 * 6.388265132904053
Epoch 290, val loss: 1.4386433362960815
Epoch 300, training loss: 14.077502250671387 = 1.3067106008529663 + 2.0 * 6.3853960037231445
Epoch 300, val loss: 1.4192510843276978
Epoch 310, training loss: 14.04179859161377 = 1.2780519723892212 + 2.0 * 6.38187313079834
Epoch 310, val loss: 1.40142023563385
Epoch 320, training loss: 13.998930931091309 = 1.2503217458724976 + 2.0 * 6.37430477142334
Epoch 320, val loss: 1.3846172094345093
Epoch 330, training loss: 13.964717864990234 = 1.2231266498565674 + 2.0 * 6.370795726776123
Epoch 330, val loss: 1.3682188987731934
Epoch 340, training loss: 13.946313858032227 = 1.1961617469787598 + 2.0 * 6.3750762939453125
Epoch 340, val loss: 1.351751685142517
Epoch 350, training loss: 13.896247863769531 = 1.1691409349441528 + 2.0 * 6.363553524017334
Epoch 350, val loss: 1.3351829051971436
Epoch 360, training loss: 13.863541603088379 = 1.1418447494506836 + 2.0 * 6.360848426818848
Epoch 360, val loss: 1.318203330039978
Epoch 370, training loss: 13.82793140411377 = 1.1139459609985352 + 2.0 * 6.356992721557617
Epoch 370, val loss: 1.300482988357544
Epoch 380, training loss: 13.796427726745605 = 1.08530592918396 + 2.0 * 6.355560779571533
Epoch 380, val loss: 1.2819856405258179
Epoch 390, training loss: 13.757405281066895 = 1.0559486150741577 + 2.0 * 6.350728511810303
Epoch 390, val loss: 1.2627575397491455
Epoch 400, training loss: 13.7217435836792 = 1.0257104635238647 + 2.0 * 6.348016738891602
Epoch 400, val loss: 1.242723822593689
Epoch 410, training loss: 13.703554153442383 = 0.9947493076324463 + 2.0 * 6.354402542114258
Epoch 410, val loss: 1.222169280052185
Epoch 420, training loss: 13.649311065673828 = 0.9636796712875366 + 2.0 * 6.34281587600708
Epoch 420, val loss: 1.2012664079666138
Epoch 430, training loss: 13.613940238952637 = 0.9323195815086365 + 2.0 * 6.340810298919678
Epoch 430, val loss: 1.180464267730713
Epoch 440, training loss: 13.57784366607666 = 0.9008017182350159 + 2.0 * 6.3385210037231445
Epoch 440, val loss: 1.1597732305526733
Epoch 450, training loss: 13.543087005615234 = 0.8693481087684631 + 2.0 * 6.336869239807129
Epoch 450, val loss: 1.139349341392517
Epoch 460, training loss: 13.515684127807617 = 0.838261604309082 + 2.0 * 6.338711261749268
Epoch 460, val loss: 1.1194701194763184
Epoch 470, training loss: 13.473971366882324 = 0.8077108263969421 + 2.0 * 6.333130359649658
Epoch 470, val loss: 1.1001813411712646
Epoch 480, training loss: 13.443676948547363 = 0.7779814600944519 + 2.0 * 6.332847595214844
Epoch 480, val loss: 1.0820475816726685
Epoch 490, training loss: 13.412576675415039 = 0.7490255832672119 + 2.0 * 6.331775665283203
Epoch 490, val loss: 1.0650267601013184
Epoch 500, training loss: 13.374588966369629 = 0.7212504744529724 + 2.0 * 6.326669216156006
Epoch 500, val loss: 1.0490895509719849
Epoch 510, training loss: 13.342453956604004 = 0.6943889260292053 + 2.0 * 6.324032306671143
Epoch 510, val loss: 1.0344617366790771
Epoch 520, training loss: 13.313279151916504 = 0.6684624552726746 + 2.0 * 6.322408199310303
Epoch 520, val loss: 1.021125078201294
Epoch 530, training loss: 13.301566123962402 = 0.6435150504112244 + 2.0 * 6.329025745391846
Epoch 530, val loss: 1.0088982582092285
Epoch 540, training loss: 13.261688232421875 = 0.6197569370269775 + 2.0 * 6.320965766906738
Epoch 540, val loss: 0.9980805516242981
Epoch 550, training loss: 13.23239517211914 = 0.5970611572265625 + 2.0 * 6.317667007446289
Epoch 550, val loss: 0.9886363744735718
Epoch 560, training loss: 13.209526062011719 = 0.5753254890441895 + 2.0 * 6.317100524902344
Epoch 560, val loss: 0.9804736375808716
Epoch 570, training loss: 13.189714431762695 = 0.5545797348022461 + 2.0 * 6.317567348480225
Epoch 570, val loss: 0.9733096361160278
Epoch 580, training loss: 13.164372444152832 = 0.5346910953521729 + 2.0 * 6.314840793609619
Epoch 580, val loss: 0.9673234820365906
Epoch 590, training loss: 13.138904571533203 = 0.5156832337379456 + 2.0 * 6.311610698699951
Epoch 590, val loss: 0.9624356031417847
Epoch 600, training loss: 13.118517875671387 = 0.4974832236766815 + 2.0 * 6.310517311096191
Epoch 600, val loss: 0.9584868550300598
Epoch 610, training loss: 13.10245132446289 = 0.4799844026565552 + 2.0 * 6.3112335205078125
Epoch 610, val loss: 0.9553248882293701
Epoch 620, training loss: 13.080094337463379 = 0.4631403684616089 + 2.0 * 6.30847692489624
Epoch 620, val loss: 0.953117311000824
Epoch 630, training loss: 13.05998420715332 = 0.44700896739959717 + 2.0 * 6.306487560272217
Epoch 630, val loss: 0.9514370560646057
Epoch 640, training loss: 13.045089721679688 = 0.4314562976360321 + 2.0 * 6.306816577911377
Epoch 640, val loss: 0.9505267143249512
Epoch 650, training loss: 13.035097122192383 = 0.4165020287036896 + 2.0 * 6.309297561645508
Epoch 650, val loss: 0.9501795768737793
Epoch 660, training loss: 13.005745887756348 = 0.40216130018234253 + 2.0 * 6.301792144775391
Epoch 660, val loss: 0.9503386616706848
Epoch 670, training loss: 12.99441146850586 = 0.38831719756126404 + 2.0 * 6.303047180175781
Epoch 670, val loss: 0.9510423541069031
Epoch 680, training loss: 12.973834991455078 = 0.37492963671684265 + 2.0 * 6.299452781677246
Epoch 680, val loss: 0.9521346688270569
Epoch 690, training loss: 12.964203834533691 = 0.3620010316371918 + 2.0 * 6.301101207733154
Epoch 690, val loss: 0.9535822868347168
Epoch 700, training loss: 12.94871711730957 = 0.34954628348350525 + 2.0 * 6.299585342407227
Epoch 700, val loss: 0.9555708169937134
Epoch 710, training loss: 12.93387222290039 = 0.3374794125556946 + 2.0 * 6.298196315765381
Epoch 710, val loss: 0.9576800465583801
Epoch 720, training loss: 12.91780948638916 = 0.32580482959747314 + 2.0 * 6.296002388000488
Epoch 720, val loss: 0.9602274298667908
Epoch 730, training loss: 12.901959419250488 = 0.31450772285461426 + 2.0 * 6.293725967407227
Epoch 730, val loss: 0.9630582332611084
Epoch 740, training loss: 12.887595176696777 = 0.303533136844635 + 2.0 * 6.2920308113098145
Epoch 740, val loss: 0.9661982655525208
Epoch 750, training loss: 12.877811431884766 = 0.2928329408168793 + 2.0 * 6.292489051818848
Epoch 750, val loss: 0.9695632457733154
Epoch 760, training loss: 12.873186111450195 = 0.2824409604072571 + 2.0 * 6.295372486114502
Epoch 760, val loss: 0.9730987548828125
Epoch 770, training loss: 12.856688499450684 = 0.2723518908023834 + 2.0 * 6.292168140411377
Epoch 770, val loss: 0.9767539501190186
Epoch 780, training loss: 12.840453147888184 = 0.2626034915447235 + 2.0 * 6.288924694061279
Epoch 780, val loss: 0.9808115363121033
Epoch 790, training loss: 12.8280029296875 = 0.25311630964279175 + 2.0 * 6.287443161010742
Epoch 790, val loss: 0.9850296378135681
Epoch 800, training loss: 12.822036743164062 = 0.24384906888008118 + 2.0 * 6.289093971252441
Epoch 800, val loss: 0.9895299673080444
Epoch 810, training loss: 12.813624382019043 = 0.23481589555740356 + 2.0 * 6.289404392242432
Epoch 810, val loss: 0.9939947128295898
Epoch 820, training loss: 12.800232887268066 = 0.2260552942752838 + 2.0 * 6.287088871002197
Epoch 820, val loss: 0.9987582564353943
Epoch 830, training loss: 12.785446166992188 = 0.21754442155361176 + 2.0 * 6.2839508056640625
Epoch 830, val loss: 1.0036267042160034
Epoch 840, training loss: 12.776872634887695 = 0.20925366878509521 + 2.0 * 6.283809661865234
Epoch 840, val loss: 1.008712649345398
Epoch 850, training loss: 12.783827781677246 = 0.20118069648742676 + 2.0 * 6.291323661804199
Epoch 850, val loss: 1.0138696432113647
Epoch 860, training loss: 12.757957458496094 = 0.1933211088180542 + 2.0 * 6.282318115234375
Epoch 860, val loss: 1.0191079378128052
Epoch 870, training loss: 12.748623847961426 = 0.18573039770126343 + 2.0 * 6.281446933746338
Epoch 870, val loss: 1.0246429443359375
Epoch 880, training loss: 12.74004077911377 = 0.1783682256937027 + 2.0 * 6.28083610534668
Epoch 880, val loss: 1.0302894115447998
Epoch 890, training loss: 12.740782737731934 = 0.17121796309947968 + 2.0 * 6.284782409667969
Epoch 890, val loss: 1.0359591245651245
Epoch 900, training loss: 12.7242431640625 = 0.16427938640117645 + 2.0 * 6.279982089996338
Epoch 900, val loss: 1.0417430400848389
Epoch 910, training loss: 12.714529991149902 = 0.1575937122106552 + 2.0 * 6.278468132019043
Epoch 910, val loss: 1.0476181507110596
Epoch 920, training loss: 12.722439765930176 = 0.1510976105928421 + 2.0 * 6.285671234130859
Epoch 920, val loss: 1.0535002946853638
Epoch 930, training loss: 12.702840805053711 = 0.144865021109581 + 2.0 * 6.278987884521484
Epoch 930, val loss: 1.0594719648361206
Epoch 940, training loss: 12.69189167022705 = 0.1388438194990158 + 2.0 * 6.276524066925049
Epoch 940, val loss: 1.0655455589294434
Epoch 950, training loss: 12.691903114318848 = 0.1330469250679016 + 2.0 * 6.279428005218506
Epoch 950, val loss: 1.0716801881790161
Epoch 960, training loss: 12.67673110961914 = 0.12746761739253998 + 2.0 * 6.274631977081299
Epoch 960, val loss: 1.077903389930725
Epoch 970, training loss: 12.670719146728516 = 0.1221156120300293 + 2.0 * 6.274301528930664
Epoch 970, val loss: 1.0841645002365112
Epoch 980, training loss: 12.669805526733398 = 0.11697159707546234 + 2.0 * 6.276416778564453
Epoch 980, val loss: 1.0904837846755981
Epoch 990, training loss: 12.658636093139648 = 0.11203814297914505 + 2.0 * 6.273298740386963
Epoch 990, val loss: 1.0967320203781128
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7185185185185186
0.8176067474960464
=== training gcn model ===
Epoch 0, training loss: 19.133398056030273 = 1.9397644996643066 + 2.0 * 8.596817016601562
Epoch 0, val loss: 1.9455924034118652
Epoch 10, training loss: 19.12285041809082 = 1.9300589561462402 + 2.0 * 8.596395492553711
Epoch 10, val loss: 1.9353693723678589
Epoch 20, training loss: 19.103511810302734 = 1.9179829359054565 + 2.0 * 8.592764854431152
Epoch 20, val loss: 1.922491192817688
Epoch 30, training loss: 19.035112380981445 = 1.901166319847107 + 2.0 * 8.566972732543945
Epoch 30, val loss: 1.9044514894485474
Epoch 40, training loss: 18.73846435546875 = 1.8802921772003174 + 2.0 * 8.429085731506348
Epoch 40, val loss: 1.8830453157424927
Epoch 50, training loss: 17.964262008666992 = 1.856688141822815 + 2.0 * 8.053787231445312
Epoch 50, val loss: 1.8592772483825684
Epoch 60, training loss: 17.297279357910156 = 1.8363709449768066 + 2.0 * 7.730453968048096
Epoch 60, val loss: 1.8405680656433105
Epoch 70, training loss: 16.384014129638672 = 1.8239622116088867 + 2.0 * 7.280025959014893
Epoch 70, val loss: 1.8293782472610474
Epoch 80, training loss: 15.890597343444824 = 1.8151617050170898 + 2.0 * 7.037717819213867
Epoch 80, val loss: 1.8209936618804932
Epoch 90, training loss: 15.643256187438965 = 1.8048832416534424 + 2.0 * 6.919186592102051
Epoch 90, val loss: 1.8105628490447998
Epoch 100, training loss: 15.44143295288086 = 1.7933413982391357 + 2.0 * 6.824045658111572
Epoch 100, val loss: 1.7993438243865967
Epoch 110, training loss: 15.244519233703613 = 1.7833417654037476 + 2.0 * 6.730588912963867
Epoch 110, val loss: 1.7900038957595825
Epoch 120, training loss: 15.09733772277832 = 1.7743840217590332 + 2.0 * 6.6614766120910645
Epoch 120, val loss: 1.781841516494751
Epoch 130, training loss: 15.007270812988281 = 1.764564037322998 + 2.0 * 6.6213531494140625
Epoch 130, val loss: 1.773177146911621
Epoch 140, training loss: 14.945058822631836 = 1.7529879808425903 + 2.0 * 6.596035480499268
Epoch 140, val loss: 1.7632415294647217
Epoch 150, training loss: 14.879810333251953 = 1.7399441003799438 + 2.0 * 6.56993293762207
Epoch 150, val loss: 1.7523852586746216
Epoch 160, training loss: 14.819597244262695 = 1.725770354270935 + 2.0 * 6.5469136238098145
Epoch 160, val loss: 1.7407619953155518
Epoch 170, training loss: 14.765671730041504 = 1.7100155353546143 + 2.0 * 6.527828216552734
Epoch 170, val loss: 1.7279090881347656
Epoch 180, training loss: 14.721752166748047 = 1.6922554969787598 + 2.0 * 6.514748573303223
Epoch 180, val loss: 1.713499903678894
Epoch 190, training loss: 14.66795539855957 = 1.6721904277801514 + 2.0 * 6.49788236618042
Epoch 190, val loss: 1.6972556114196777
Epoch 200, training loss: 14.616786003112793 = 1.6495157480239868 + 2.0 * 6.483634948730469
Epoch 200, val loss: 1.6790175437927246
Epoch 210, training loss: 14.567320823669434 = 1.6238549947738647 + 2.0 * 6.471733093261719
Epoch 210, val loss: 1.658430814743042
Epoch 220, training loss: 14.539557456970215 = 1.5948859453201294 + 2.0 * 6.4723358154296875
Epoch 220, val loss: 1.6352301836013794
Epoch 230, training loss: 14.470062255859375 = 1.5630625486373901 + 2.0 * 6.453499794006348
Epoch 230, val loss: 1.609789252281189
Epoch 240, training loss: 14.414874076843262 = 1.5283607244491577 + 2.0 * 6.443256855010986
Epoch 240, val loss: 1.5821936130523682
Epoch 250, training loss: 14.358115196228027 = 1.4904687404632568 + 2.0 * 6.433823108673096
Epoch 250, val loss: 1.5521831512451172
Epoch 260, training loss: 14.300138473510742 = 1.44962477684021 + 2.0 * 6.425256729125977
Epoch 260, val loss: 1.5199146270751953
Epoch 270, training loss: 14.25855541229248 = 1.4061905145645142 + 2.0 * 6.426182270050049
Epoch 270, val loss: 1.4858219623565674
Epoch 280, training loss: 14.18342113494873 = 1.3614190816879272 + 2.0 * 6.411001205444336
Epoch 280, val loss: 1.4508907794952393
Epoch 290, training loss: 14.126778602600098 = 1.3156436681747437 + 2.0 * 6.405567646026611
Epoch 290, val loss: 1.4153887033462524
Epoch 300, training loss: 14.071281433105469 = 1.2692787647247314 + 2.0 * 6.401001453399658
Epoch 300, val loss: 1.3798402547836304
Epoch 310, training loss: 14.011682510375977 = 1.2229853868484497 + 2.0 * 6.394348621368408
Epoch 310, val loss: 1.3448342084884644
Epoch 320, training loss: 13.96196460723877 = 1.1774526834487915 + 2.0 * 6.392255783081055
Epoch 320, val loss: 1.3110077381134033
Epoch 330, training loss: 13.905220985412598 = 1.1335610151290894 + 2.0 * 6.385829925537109
Epoch 330, val loss: 1.278834581375122
Epoch 340, training loss: 13.85244083404541 = 1.0910683870315552 + 2.0 * 6.380686283111572
Epoch 340, val loss: 1.2483513355255127
Epoch 350, training loss: 13.804985046386719 = 1.0500043630599976 + 2.0 * 6.377490520477295
Epoch 350, val loss: 1.2195357084274292
Epoch 360, training loss: 13.762041091918945 = 1.0107665061950684 + 2.0 * 6.375637531280518
Epoch 360, val loss: 1.1925654411315918
Epoch 370, training loss: 13.714086532592773 = 0.9735409617424011 + 2.0 * 6.370272636413574
Epoch 370, val loss: 1.1675333976745605
Epoch 380, training loss: 13.66990852355957 = 0.9379141330718994 + 2.0 * 6.365997314453125
Epoch 380, val loss: 1.1441234350204468
Epoch 390, training loss: 13.628422737121582 = 0.9036083221435547 + 2.0 * 6.362407207489014
Epoch 390, val loss: 1.1219935417175293
Epoch 400, training loss: 13.59566593170166 = 0.8706672191619873 + 2.0 * 6.362499237060547
Epoch 400, val loss: 1.1013109683990479
Epoch 410, training loss: 13.551972389221191 = 0.8395093083381653 + 2.0 * 6.356231689453125
Epoch 410, val loss: 1.0821112394332886
Epoch 420, training loss: 13.515562057495117 = 0.8097253441810608 + 2.0 * 6.3529181480407715
Epoch 420, val loss: 1.0643250942230225
Epoch 430, training loss: 13.48128890991211 = 0.7811129689216614 + 2.0 * 6.350088119506836
Epoch 430, val loss: 1.0476562976837158
Epoch 440, training loss: 13.465736389160156 = 0.7537510395050049 + 2.0 * 6.355992794036865
Epoch 440, val loss: 1.0321708917617798
Epoch 450, training loss: 13.426961898803711 = 0.7278317213058472 + 2.0 * 6.349565029144287
Epoch 450, val loss: 1.0180412530899048
Epoch 460, training loss: 13.389618873596191 = 0.7033767700195312 + 2.0 * 6.34312105178833
Epoch 460, val loss: 1.0054620504379272
Epoch 470, training loss: 13.358983993530273 = 0.6800026893615723 + 2.0 * 6.3394904136657715
Epoch 470, val loss: 0.9940121173858643
Epoch 480, training loss: 13.334831237792969 = 0.657619833946228 + 2.0 * 6.338605880737305
Epoch 480, val loss: 0.9837298393249512
Epoch 490, training loss: 13.307010650634766 = 0.6363399624824524 + 2.0 * 6.3353352546691895
Epoch 490, val loss: 0.9747669100761414
Epoch 500, training loss: 13.282876014709473 = 0.6159951686859131 + 2.0 * 6.33344030380249
Epoch 500, val loss: 0.9668785929679871
Epoch 510, training loss: 13.258768081665039 = 0.5966444611549377 + 2.0 * 6.331061840057373
Epoch 510, val loss: 0.9600479602813721
Epoch 520, training loss: 13.238235473632812 = 0.5783741474151611 + 2.0 * 6.329930782318115
Epoch 520, val loss: 0.9543963670730591
Epoch 530, training loss: 13.214603424072266 = 0.5609481334686279 + 2.0 * 6.326827526092529
Epoch 530, val loss: 0.949961245059967
Epoch 540, training loss: 13.191661834716797 = 0.5442118048667908 + 2.0 * 6.32372522354126
Epoch 540, val loss: 0.9462165236473083
Epoch 550, training loss: 13.171272277832031 = 0.5280624628067017 + 2.0 * 6.3216047286987305
Epoch 550, val loss: 0.943353533744812
Epoch 560, training loss: 13.165326118469238 = 0.5124490261077881 + 2.0 * 6.3264384269714355
Epoch 560, val loss: 0.9413366913795471
Epoch 570, training loss: 13.138789176940918 = 0.4975185990333557 + 2.0 * 6.3206353187561035
Epoch 570, val loss: 0.9400518536567688
Epoch 580, training loss: 13.116558074951172 = 0.4830858111381531 + 2.0 * 6.316736221313477
Epoch 580, val loss: 0.9394131898880005
Epoch 590, training loss: 13.114612579345703 = 0.46905070543289185 + 2.0 * 6.322781085968018
Epoch 590, val loss: 0.93953937292099
Epoch 600, training loss: 13.087782859802246 = 0.4554162919521332 + 2.0 * 6.316183090209961
Epoch 600, val loss: 0.9398508071899414
Epoch 610, training loss: 13.066884994506836 = 0.4421883821487427 + 2.0 * 6.312348365783691
Epoch 610, val loss: 0.9410265684127808
Epoch 620, training loss: 13.049110412597656 = 0.4291446805000305 + 2.0 * 6.309982776641846
Epoch 620, val loss: 0.9424626231193542
Epoch 630, training loss: 13.032459259033203 = 0.41620877385139465 + 2.0 * 6.308125019073486
Epoch 630, val loss: 0.944180428981781
Epoch 640, training loss: 13.01856517791748 = 0.4033253788948059 + 2.0 * 6.307620048522949
Epoch 640, val loss: 0.946261465549469
Epoch 650, training loss: 13.004560470581055 = 0.3904976546764374 + 2.0 * 6.307031631469727
Epoch 650, val loss: 0.9485412836074829
Epoch 660, training loss: 12.9910249710083 = 0.3777492046356201 + 2.0 * 6.306637763977051
Epoch 660, val loss: 0.9510912895202637
Epoch 670, training loss: 12.97130012512207 = 0.3649956285953522 + 2.0 * 6.303152084350586
Epoch 670, val loss: 0.9538964033126831
Epoch 680, training loss: 12.963830947875977 = 0.35220471024513245 + 2.0 * 6.305813312530518
Epoch 680, val loss: 0.9567825794219971
Epoch 690, training loss: 12.942760467529297 = 0.3394901156425476 + 2.0 * 6.301635265350342
Epoch 690, val loss: 0.9601953625679016
Epoch 700, training loss: 12.92699146270752 = 0.3267876207828522 + 2.0 * 6.3001017570495605
Epoch 700, val loss: 0.9636594653129578
Epoch 710, training loss: 12.911626815795898 = 0.31411975622177124 + 2.0 * 6.29875373840332
Epoch 710, val loss: 0.9675567746162415
Epoch 720, training loss: 12.912116050720215 = 0.30154117941856384 + 2.0 * 6.3052873611450195
Epoch 720, val loss: 0.9715731143951416
Epoch 730, training loss: 12.884782791137695 = 0.28911226987838745 + 2.0 * 6.297835350036621
Epoch 730, val loss: 0.9757782220840454
Epoch 740, training loss: 12.86854362487793 = 0.27689874172210693 + 2.0 * 6.295822620391846
Epoch 740, val loss: 0.9805095791816711
Epoch 750, training loss: 12.85414981842041 = 0.2648402750492096 + 2.0 * 6.294654846191406
Epoch 750, val loss: 0.9853630065917969
Epoch 760, training loss: 12.863832473754883 = 0.25304409861564636 + 2.0 * 6.305394172668457
Epoch 760, val loss: 0.9905498623847961
Epoch 770, training loss: 12.834329605102539 = 0.24152925610542297 + 2.0 * 6.29640007019043
Epoch 770, val loss: 0.9957109093666077
Epoch 780, training loss: 12.814632415771484 = 0.2303960919380188 + 2.0 * 6.292118072509766
Epoch 780, val loss: 1.0015453100204468
Epoch 790, training loss: 12.800381660461426 = 0.21956785023212433 + 2.0 * 6.290406703948975
Epoch 790, val loss: 1.007340431213379
Epoch 800, training loss: 12.788993835449219 = 0.20906706154346466 + 2.0 * 6.289963245391846
Epoch 800, val loss: 1.0134345293045044
Epoch 810, training loss: 12.791707992553711 = 0.19893968105316162 + 2.0 * 6.296384334564209
Epoch 810, val loss: 1.0196950435638428
Epoch 820, training loss: 12.769502639770508 = 0.18923181295394897 + 2.0 * 6.290135383605957
Epoch 820, val loss: 1.0261125564575195
Epoch 830, training loss: 12.756855964660645 = 0.17994050681591034 + 2.0 * 6.288457870483398
Epoch 830, val loss: 1.0326052904129028
Epoch 840, training loss: 12.746295928955078 = 0.17104674875736237 + 2.0 * 6.287624359130859
Epoch 840, val loss: 1.0392557382583618
Epoch 850, training loss: 12.742352485656738 = 0.16256316006183624 + 2.0 * 6.2898945808410645
Epoch 850, val loss: 1.045965313911438
Epoch 860, training loss: 12.726086616516113 = 0.15450964868068695 + 2.0 * 6.285788536071777
Epoch 860, val loss: 1.0531522035598755
Epoch 870, training loss: 12.716171264648438 = 0.14682139456272125 + 2.0 * 6.284675121307373
Epoch 870, val loss: 1.0600868463516235
Epoch 880, training loss: 12.713281631469727 = 0.13951759040355682 + 2.0 * 6.286881923675537
Epoch 880, val loss: 1.066912055015564
Epoch 890, training loss: 12.696443557739258 = 0.1326172798871994 + 2.0 * 6.2819132804870605
Epoch 890, val loss: 1.0740618705749512
Epoch 900, training loss: 12.70193099975586 = 0.12607987225055695 + 2.0 * 6.287925720214844
Epoch 900, val loss: 1.0812792778015137
Epoch 910, training loss: 12.69284439086914 = 0.11988823860883713 + 2.0 * 6.286478042602539
Epoch 910, val loss: 1.087737798690796
Epoch 920, training loss: 12.677474975585938 = 0.11408034712076187 + 2.0 * 6.2816972732543945
Epoch 920, val loss: 1.0949105024337769
Epoch 930, training loss: 12.668533325195312 = 0.10859297960996628 + 2.0 * 6.279970169067383
Epoch 930, val loss: 1.101962924003601
Epoch 940, training loss: 12.660120010375977 = 0.1033940240740776 + 2.0 * 6.278363227844238
Epoch 940, val loss: 1.1089282035827637
Epoch 950, training loss: 12.659625053405762 = 0.09847506135702133 + 2.0 * 6.280574798583984
Epoch 950, val loss: 1.116020917892456
Epoch 960, training loss: 12.647960662841797 = 0.09382698684930801 + 2.0 * 6.277066707611084
Epoch 960, val loss: 1.1225405931472778
Epoch 970, training loss: 12.64434814453125 = 0.089455246925354 + 2.0 * 6.277446269989014
Epoch 970, val loss: 1.1295714378356934
Epoch 980, training loss: 12.637542724609375 = 0.08532210439443588 + 2.0 * 6.2761101722717285
Epoch 980, val loss: 1.1363078355789185
Epoch 990, training loss: 12.631061553955078 = 0.0814167857170105 + 2.0 * 6.274822235107422
Epoch 990, val loss: 1.143168568611145
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7000000000000001
0.8002108592514497
The final CL Acc:0.72099, 0.01823, The final GNN Acc:0.80882, 0.00710
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13282])
remove edge: torch.Size([2, 7916])
updated graph: torch.Size([2, 10642])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 19.147541046142578 = 1.953831672668457 + 2.0 * 8.596855163574219
Epoch 0, val loss: 1.9467271566390991
Epoch 10, training loss: 19.136356353759766 = 1.9432282447814941 + 2.0 * 8.596564292907715
Epoch 10, val loss: 1.9361032247543335
Epoch 20, training loss: 19.117549896240234 = 1.9298763275146484 + 2.0 * 8.593836784362793
Epoch 20, val loss: 1.9227566719055176
Epoch 30, training loss: 19.0521297454834 = 1.911159873008728 + 2.0 * 8.57048511505127
Epoch 30, val loss: 1.9043430089950562
Epoch 40, training loss: 18.7238712310791 = 1.8871651887893677 + 2.0 * 8.418353080749512
Epoch 40, val loss: 1.8817691802978516
Epoch 50, training loss: 17.687458038330078 = 1.8606371879577637 + 2.0 * 7.913410186767578
Epoch 50, val loss: 1.8576750755310059
Epoch 60, training loss: 16.898406982421875 = 1.8391122817993164 + 2.0 * 7.5296478271484375
Epoch 60, val loss: 1.8385186195373535
Epoch 70, training loss: 16.280847549438477 = 1.825092077255249 + 2.0 * 7.227878093719482
Epoch 70, val loss: 1.8252735137939453
Epoch 80, training loss: 16.050704956054688 = 1.810779333114624 + 2.0 * 7.119962692260742
Epoch 80, val loss: 1.8120537996292114
Epoch 90, training loss: 15.844359397888184 = 1.7937602996826172 + 2.0 * 7.025299549102783
Epoch 90, val loss: 1.7973819971084595
Epoch 100, training loss: 15.610754013061523 = 1.7776808738708496 + 2.0 * 6.916536808013916
Epoch 100, val loss: 1.7842894792556763
Epoch 110, training loss: 15.455927848815918 = 1.7638181447982788 + 2.0 * 6.846055030822754
Epoch 110, val loss: 1.772308349609375
Epoch 120, training loss: 15.294140815734863 = 1.750733494758606 + 2.0 * 6.771703720092773
Epoch 120, val loss: 1.7605648040771484
Epoch 130, training loss: 15.133354187011719 = 1.7385891675949097 + 2.0 * 6.69738245010376
Epoch 130, val loss: 1.7495440244674683
Epoch 140, training loss: 15.013997077941895 = 1.7251107692718506 + 2.0 * 6.644443035125732
Epoch 140, val loss: 1.7376958131790161
Epoch 150, training loss: 14.923378944396973 = 1.7090693712234497 + 2.0 * 6.607154846191406
Epoch 150, val loss: 1.7236741781234741
Epoch 160, training loss: 14.841850280761719 = 1.6909087896347046 + 2.0 * 6.575470924377441
Epoch 160, val loss: 1.7075278759002686
Epoch 170, training loss: 14.769474029541016 = 1.670613408088684 + 2.0 * 6.5494303703308105
Epoch 170, val loss: 1.6893372535705566
Epoch 180, training loss: 14.71207332611084 = 1.6475584506988525 + 2.0 * 6.532257556915283
Epoch 180, val loss: 1.668858528137207
Epoch 190, training loss: 14.647738456726074 = 1.6218518018722534 + 2.0 * 6.512943267822266
Epoch 190, val loss: 1.6464375257492065
Epoch 200, training loss: 14.590070724487305 = 1.5933966636657715 + 2.0 * 6.4983367919921875
Epoch 200, val loss: 1.6214840412139893
Epoch 210, training loss: 14.53260612487793 = 1.5618739128112793 + 2.0 * 6.485365867614746
Epoch 210, val loss: 1.5938600301742554
Epoch 220, training loss: 14.474349021911621 = 1.5272287130355835 + 2.0 * 6.473560333251953
Epoch 220, val loss: 1.5635583400726318
Epoch 230, training loss: 14.426870346069336 = 1.490023136138916 + 2.0 * 6.468423366546631
Epoch 230, val loss: 1.5312029123306274
Epoch 240, training loss: 14.359021186828613 = 1.4512083530426025 + 2.0 * 6.453906536102295
Epoch 240, val loss: 1.4977772235870361
Epoch 250, training loss: 14.295316696166992 = 1.4109781980514526 + 2.0 * 6.442169189453125
Epoch 250, val loss: 1.4632947444915771
Epoch 260, training loss: 14.24232292175293 = 1.3697113990783691 + 2.0 * 6.436305999755859
Epoch 260, val loss: 1.42848801612854
Epoch 270, training loss: 14.17650032043457 = 1.3285002708435059 + 2.0 * 6.423999786376953
Epoch 270, val loss: 1.3941864967346191
Epoch 280, training loss: 14.118577003479004 = 1.287439227104187 + 2.0 * 6.415568828582764
Epoch 280, val loss: 1.3603743314743042
Epoch 290, training loss: 14.06232738494873 = 1.2464524507522583 + 2.0 * 6.407937526702881
Epoch 290, val loss: 1.3270246982574463
Epoch 300, training loss: 14.017916679382324 = 1.2061229944229126 + 2.0 * 6.4058966636657715
Epoch 300, val loss: 1.294498085975647
Epoch 310, training loss: 13.956758499145508 = 1.1666004657745361 + 2.0 * 6.395079135894775
Epoch 310, val loss: 1.2630701065063477
Epoch 320, training loss: 13.90571117401123 = 1.1274127960205078 + 2.0 * 6.389149188995361
Epoch 320, val loss: 1.2320283651351929
Epoch 330, training loss: 13.855438232421875 = 1.0882329940795898 + 2.0 * 6.383602619171143
Epoch 330, val loss: 1.2010935544967651
Epoch 340, training loss: 13.823116302490234 = 1.0491012334823608 + 2.0 * 6.387007713317871
Epoch 340, val loss: 1.1702690124511719
Epoch 350, training loss: 13.762901306152344 = 1.0102949142456055 + 2.0 * 6.376303195953369
Epoch 350, val loss: 1.1398518085479736
Epoch 360, training loss: 13.712491035461426 = 0.9718905687332153 + 2.0 * 6.37030029296875
Epoch 360, val loss: 1.1099592447280884
Epoch 370, training loss: 13.66695785522461 = 0.9338608384132385 + 2.0 * 6.366548538208008
Epoch 370, val loss: 1.0804964303970337
Epoch 380, training loss: 13.62098503112793 = 0.8964406847953796 + 2.0 * 6.362272262573242
Epoch 380, val loss: 1.051742434501648
Epoch 390, training loss: 13.594463348388672 = 0.8599497675895691 + 2.0 * 6.3672566413879395
Epoch 390, val loss: 1.0240713357925415
Epoch 400, training loss: 13.538311004638672 = 0.8252902030944824 + 2.0 * 6.356510162353516
Epoch 400, val loss: 0.997881293296814
Epoch 410, training loss: 13.496683120727539 = 0.7922438979148865 + 2.0 * 6.352219581604004
Epoch 410, val loss: 0.9734330773353577
Epoch 420, training loss: 13.46938419342041 = 0.7607092261314392 + 2.0 * 6.354337692260742
Epoch 420, val loss: 0.9506700038909912
Epoch 430, training loss: 13.422171592712402 = 0.7310072183609009 + 2.0 * 6.345582008361816
Epoch 430, val loss: 0.9295896291732788
Epoch 440, training loss: 13.385993957519531 = 0.7030043005943298 + 2.0 * 6.341495037078857
Epoch 440, val loss: 0.9103745222091675
Epoch 450, training loss: 13.354373931884766 = 0.6765375137329102 + 2.0 * 6.338918209075928
Epoch 450, val loss: 0.8928694725036621
Epoch 460, training loss: 13.335935592651367 = 0.6515110731124878 + 2.0 * 6.342212200164795
Epoch 460, val loss: 0.876967191696167
Epoch 470, training loss: 13.294655799865723 = 0.6280114054679871 + 2.0 * 6.333322048187256
Epoch 470, val loss: 0.8627099990844727
Epoch 480, training loss: 13.267572402954102 = 0.6058474183082581 + 2.0 * 6.330862522125244
Epoch 480, val loss: 0.8498471975326538
Epoch 490, training loss: 13.247541427612305 = 0.5848203897476196 + 2.0 * 6.331360340118408
Epoch 490, val loss: 0.8382539749145508
Epoch 500, training loss: 13.227272033691406 = 0.564866304397583 + 2.0 * 6.331202983856201
Epoch 500, val loss: 0.8280249834060669
Epoch 510, training loss: 13.196295738220215 = 0.5459778308868408 + 2.0 * 6.325159072875977
Epoch 510, val loss: 0.8189381957054138
Epoch 520, training loss: 13.171554565429688 = 0.5279179215431213 + 2.0 * 6.3218183517456055
Epoch 520, val loss: 0.810818076133728
Epoch 530, training loss: 13.14924430847168 = 0.5105295777320862 + 2.0 * 6.319357395172119
Epoch 530, val loss: 0.8035647869110107
Epoch 540, training loss: 13.174348831176758 = 0.4937172830104828 + 2.0 * 6.340315818786621
Epoch 540, val loss: 0.7971883416175842
Epoch 550, training loss: 13.123177528381348 = 0.47768905758857727 + 2.0 * 6.322744369506836
Epoch 550, val loss: 0.791530430316925
Epoch 560, training loss: 13.092583656311035 = 0.46220874786376953 + 2.0 * 6.315187454223633
Epoch 560, val loss: 0.786604642868042
Epoch 570, training loss: 13.072128295898438 = 0.44713667035102844 + 2.0 * 6.312495708465576
Epoch 570, val loss: 0.7823214530944824
Epoch 580, training loss: 13.054112434387207 = 0.43240728974342346 + 2.0 * 6.310852527618408
Epoch 580, val loss: 0.7786434888839722
Epoch 590, training loss: 13.043008804321289 = 0.4179803729057312 + 2.0 * 6.312514305114746
Epoch 590, val loss: 0.775517463684082
Epoch 600, training loss: 13.031503677368164 = 0.40395665168762207 + 2.0 * 6.3137736320495605
Epoch 600, val loss: 0.7728912830352783
Epoch 610, training loss: 13.005207061767578 = 0.39038941264152527 + 2.0 * 6.307408809661865
Epoch 610, val loss: 0.7709075212478638
Epoch 620, training loss: 12.989091873168945 = 0.37711843848228455 + 2.0 * 6.3059868812561035
Epoch 620, val loss: 0.7694304585456848
Epoch 630, training loss: 12.972528457641602 = 0.3641152083873749 + 2.0 * 6.304206848144531
Epoch 630, val loss: 0.7684738039970398
Epoch 640, training loss: 12.973409652709961 = 0.3513842225074768 + 2.0 * 6.3110127449035645
Epoch 640, val loss: 0.7680181264877319
Epoch 650, training loss: 12.946662902832031 = 0.33890578150749207 + 2.0 * 6.3038787841796875
Epoch 650, val loss: 0.7681466341018677
Epoch 660, training loss: 12.927398681640625 = 0.3267762362957001 + 2.0 * 6.300311088562012
Epoch 660, val loss: 0.768751323223114
Epoch 670, training loss: 12.914350509643555 = 0.31489798426628113 + 2.0 * 6.299726486206055
Epoch 670, val loss: 0.7698477506637573
Epoch 680, training loss: 12.902202606201172 = 0.3032990097999573 + 2.0 * 6.29945182800293
Epoch 680, val loss: 0.7714430093765259
Epoch 690, training loss: 12.890059471130371 = 0.2920282781124115 + 2.0 * 6.299015522003174
Epoch 690, val loss: 0.77339768409729
Epoch 700, training loss: 12.877412796020508 = 0.2809945344924927 + 2.0 * 6.298209190368652
Epoch 700, val loss: 0.7757319808006287
Epoch 710, training loss: 12.862340927124023 = 0.27018922567367554 + 2.0 * 6.296075820922852
Epoch 710, val loss: 0.778444230556488
Epoch 720, training loss: 12.850473403930664 = 0.2595856785774231 + 2.0 * 6.295444011688232
Epoch 720, val loss: 0.781570553779602
Epoch 730, training loss: 12.834349632263184 = 0.24919918179512024 + 2.0 * 6.292575359344482
Epoch 730, val loss: 0.7849834561347961
Epoch 740, training loss: 12.837282180786133 = 0.23898018896579742 + 2.0 * 6.2991509437561035
Epoch 740, val loss: 0.7886731028556824
Epoch 750, training loss: 12.819491386413574 = 0.22896938025951385 + 2.0 * 6.295260906219482
Epoch 750, val loss: 0.792719304561615
Epoch 760, training loss: 12.798945426940918 = 0.2191770225763321 + 2.0 * 6.289884090423584
Epoch 760, val loss: 0.7969762682914734
Epoch 770, training loss: 12.800225257873535 = 0.20959961414337158 + 2.0 * 6.295312881469727
Epoch 770, val loss: 0.8014893531799316
Epoch 780, training loss: 12.781490325927734 = 0.20020709931850433 + 2.0 * 6.290641784667969
Epoch 780, val loss: 0.8063062429428101
Epoch 790, training loss: 12.767203330993652 = 0.19110357761383057 + 2.0 * 6.288049697875977
Epoch 790, val loss: 0.8113139867782593
Epoch 800, training loss: 12.7555513381958 = 0.18219538033008575 + 2.0 * 6.286677837371826
Epoch 800, val loss: 0.8165208697319031
Epoch 810, training loss: 12.7601318359375 = 0.173585906624794 + 2.0 * 6.293272972106934
Epoch 810, val loss: 0.8220152854919434
Epoch 820, training loss: 12.738199234008789 = 0.16530181467533112 + 2.0 * 6.2864484786987305
Epoch 820, val loss: 0.8275812864303589
Epoch 830, training loss: 12.729009628295898 = 0.1573731005191803 + 2.0 * 6.285818099975586
Epoch 830, val loss: 0.8333445191383362
Epoch 840, training loss: 12.714982032775879 = 0.14975634217262268 + 2.0 * 6.2826128005981445
Epoch 840, val loss: 0.8392451405525208
Epoch 850, training loss: 12.710445404052734 = 0.14247578382492065 + 2.0 * 6.283984661102295
Epoch 850, val loss: 0.8453466296195984
Epoch 860, training loss: 12.702313423156738 = 0.1355699896812439 + 2.0 * 6.283371925354004
Epoch 860, val loss: 0.851514995098114
Epoch 870, training loss: 12.69682788848877 = 0.12901724874973297 + 2.0 * 6.283905506134033
Epoch 870, val loss: 0.857756495475769
Epoch 880, training loss: 12.68247127532959 = 0.12282491475343704 + 2.0 * 6.279823303222656
Epoch 880, val loss: 0.864107072353363
Epoch 890, training loss: 12.676336288452148 = 0.1169491782784462 + 2.0 * 6.279693603515625
Epoch 890, val loss: 0.870552122592926
Epoch 900, training loss: 12.683256149291992 = 0.11139509081840515 + 2.0 * 6.285930633544922
Epoch 900, val loss: 0.8770189881324768
Epoch 910, training loss: 12.666988372802734 = 0.1061987355351448 + 2.0 * 6.280395030975342
Epoch 910, val loss: 0.8835044503211975
Epoch 920, training loss: 12.662285804748535 = 0.10126043111085892 + 2.0 * 6.280512809753418
Epoch 920, val loss: 0.8900326490402222
Epoch 930, training loss: 12.651485443115234 = 0.09663377702236176 + 2.0 * 6.277425765991211
Epoch 930, val loss: 0.8966234922409058
Epoch 940, training loss: 12.644142150878906 = 0.09225411713123322 + 2.0 * 6.275944232940674
Epoch 940, val loss: 0.9032374024391174
Epoch 950, training loss: 12.637763977050781 = 0.08812417089939117 + 2.0 * 6.274819850921631
Epoch 950, val loss: 0.9098342061042786
Epoch 960, training loss: 12.640348434448242 = 0.08423685282468796 + 2.0 * 6.278055667877197
Epoch 960, val loss: 0.9165486693382263
Epoch 970, training loss: 12.630644798278809 = 0.08056055009365082 + 2.0 * 6.2750420570373535
Epoch 970, val loss: 0.923096776008606
Epoch 980, training loss: 12.627581596374512 = 0.07711358368396759 + 2.0 * 6.275234222412109
Epoch 980, val loss: 0.9296982884407043
Epoch 990, training loss: 12.624652862548828 = 0.0738629549741745 + 2.0 * 6.275394916534424
Epoch 990, val loss: 0.9362071752548218
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7740740740740741
0.8376383763837639
=== training gcn model ===
Epoch 0, training loss: 19.14076042175293 = 1.9470235109329224 + 2.0 * 8.596868515014648
Epoch 0, val loss: 1.9433408975601196
Epoch 10, training loss: 19.13015365600586 = 1.9368946552276611 + 2.0 * 8.59662914276123
Epoch 10, val loss: 1.933924674987793
Epoch 20, training loss: 19.1130313873291 = 1.9242279529571533 + 2.0 * 8.594401359558105
Epoch 20, val loss: 1.9217185974121094
Epoch 30, training loss: 19.059616088867188 = 1.9066698551177979 + 2.0 * 8.576473236083984
Epoch 30, val loss: 1.9044784307479858
Epoch 40, training loss: 18.854007720947266 = 1.8837443590164185 + 2.0 * 8.48513126373291
Epoch 40, val loss: 1.8824338912963867
Epoch 50, training loss: 18.205562591552734 = 1.8583667278289795 + 2.0 * 8.173598289489746
Epoch 50, val loss: 1.8587170839309692
Epoch 60, training loss: 17.819686889648438 = 1.8339858055114746 + 2.0 * 7.9928507804870605
Epoch 60, val loss: 1.837159276008606
Epoch 70, training loss: 17.10883331298828 = 1.8139921426773071 + 2.0 * 7.647420406341553
Epoch 70, val loss: 1.819378137588501
Epoch 80, training loss: 16.249414443969727 = 1.80143141746521 + 2.0 * 7.223991870880127
Epoch 80, val loss: 1.8080852031707764
Epoch 90, training loss: 15.798184394836426 = 1.7907012701034546 + 2.0 * 7.00374174118042
Epoch 90, val loss: 1.7975014448165894
Epoch 100, training loss: 15.521740913391113 = 1.776327133178711 + 2.0 * 6.872706890106201
Epoch 100, val loss: 1.784246563911438
Epoch 110, training loss: 15.322436332702637 = 1.761834740638733 + 2.0 * 6.780300617218018
Epoch 110, val loss: 1.7710480690002441
Epoch 120, training loss: 15.188121795654297 = 1.7480686902999878 + 2.0 * 6.72002649307251
Epoch 120, val loss: 1.7578548192977905
Epoch 130, training loss: 15.089054107666016 = 1.7331204414367676 + 2.0 * 6.677966594696045
Epoch 130, val loss: 1.743330717086792
Epoch 140, training loss: 14.996196746826172 = 1.716379165649414 + 2.0 * 6.639908790588379
Epoch 140, val loss: 1.7274760007858276
Epoch 150, training loss: 14.903743743896484 = 1.6987520456314087 + 2.0 * 6.6024956703186035
Epoch 150, val loss: 1.7111117839813232
Epoch 160, training loss: 14.824308395385742 = 1.6793941259384155 + 2.0 * 6.572457313537598
Epoch 160, val loss: 1.693658471107483
Epoch 170, training loss: 14.758543014526367 = 1.6572874784469604 + 2.0 * 6.550627708435059
Epoch 170, val loss: 1.6741957664489746
Epoch 180, training loss: 14.699066162109375 = 1.6324201822280884 + 2.0 * 6.533322811126709
Epoch 180, val loss: 1.6522976160049438
Epoch 190, training loss: 14.641111373901367 = 1.6047031879425049 + 2.0 * 6.518204212188721
Epoch 190, val loss: 1.6279932260513306
Epoch 200, training loss: 14.582841873168945 = 1.5742769241333008 + 2.0 * 6.504282474517822
Epoch 200, val loss: 1.6014028787612915
Epoch 210, training loss: 14.527502059936523 = 1.5411033630371094 + 2.0 * 6.493199348449707
Epoch 210, val loss: 1.572624921798706
Epoch 220, training loss: 14.469266891479492 = 1.50574791431427 + 2.0 * 6.481759548187256
Epoch 220, val loss: 1.5422507524490356
Epoch 230, training loss: 14.412432670593262 = 1.4686142206192017 + 2.0 * 6.471909046173096
Epoch 230, val loss: 1.51076340675354
Epoch 240, training loss: 14.354342460632324 = 1.4297701120376587 + 2.0 * 6.462285995483398
Epoch 240, val loss: 1.4781769514083862
Epoch 250, training loss: 14.299936294555664 = 1.389557957649231 + 2.0 * 6.455189228057861
Epoch 250, val loss: 1.4448100328445435
Epoch 260, training loss: 14.244325637817383 = 1.3489508628845215 + 2.0 * 6.44768762588501
Epoch 260, val loss: 1.4116131067276
Epoch 270, training loss: 14.189238548278809 = 1.3086464405059814 + 2.0 * 6.440296173095703
Epoch 270, val loss: 1.3790385723114014
Epoch 280, training loss: 14.132621765136719 = 1.2685061693191528 + 2.0 * 6.432057857513428
Epoch 280, val loss: 1.3470492362976074
Epoch 290, training loss: 14.083089828491211 = 1.2286571264266968 + 2.0 * 6.427216529846191
Epoch 290, val loss: 1.3157380819320679
Epoch 300, training loss: 14.032678604125977 = 1.1896530389785767 + 2.0 * 6.421512603759766
Epoch 300, val loss: 1.2853180170059204
Epoch 310, training loss: 13.981429100036621 = 1.1516859531402588 + 2.0 * 6.414871692657471
Epoch 310, val loss: 1.256124496459961
Epoch 320, training loss: 13.9345121383667 = 1.1145308017730713 + 2.0 * 6.4099907875061035
Epoch 320, val loss: 1.2279267311096191
Epoch 330, training loss: 13.897555351257324 = 1.0782331228256226 + 2.0 * 6.409661293029785
Epoch 330, val loss: 1.2005889415740967
Epoch 340, training loss: 13.842524528503418 = 1.0431039333343506 + 2.0 * 6.399710178375244
Epoch 340, val loss: 1.1742228269577026
Epoch 350, training loss: 13.799590110778809 = 1.0088695287704468 + 2.0 * 6.395360469818115
Epoch 350, val loss: 1.1488524675369263
Epoch 360, training loss: 13.756736755371094 = 0.9754012823104858 + 2.0 * 6.390667915344238
Epoch 360, val loss: 1.1242539882659912
Epoch 370, training loss: 13.727254867553711 = 0.9427996277809143 + 2.0 * 6.392227649688721
Epoch 370, val loss: 1.1006191968917847
Epoch 380, training loss: 13.678179740905762 = 0.9111743569374084 + 2.0 * 6.38350248336792
Epoch 380, val loss: 1.0778508186340332
Epoch 390, training loss: 13.641523361206055 = 0.8802936673164368 + 2.0 * 6.380614757537842
Epoch 390, val loss: 1.0559260845184326
Epoch 400, training loss: 13.600264549255371 = 0.8499240279197693 + 2.0 * 6.3751702308654785
Epoch 400, val loss: 1.0346251726150513
Epoch 410, training loss: 13.565142631530762 = 0.8199577927589417 + 2.0 * 6.372592449188232
Epoch 410, val loss: 1.0137969255447388
Epoch 420, training loss: 13.535896301269531 = 0.790307879447937 + 2.0 * 6.372794151306152
Epoch 420, val loss: 0.9935784339904785
Epoch 430, training loss: 13.493236541748047 = 0.7612144947052002 + 2.0 * 6.366011142730713
Epoch 430, val loss: 0.9739205837249756
Epoch 440, training loss: 13.456321716308594 = 0.732367217540741 + 2.0 * 6.3619771003723145
Epoch 440, val loss: 0.954780638217926
Epoch 450, training loss: 13.437736511230469 = 0.7037656903266907 + 2.0 * 6.366985321044922
Epoch 450, val loss: 0.9360591173171997
Epoch 460, training loss: 13.390510559082031 = 0.6757111549377441 + 2.0 * 6.357399940490723
Epoch 460, val loss: 0.9180788993835449
Epoch 470, training loss: 13.356026649475098 = 0.648085355758667 + 2.0 * 6.353970527648926
Epoch 470, val loss: 0.9008505940437317
Epoch 480, training loss: 13.322354316711426 = 0.6209123134613037 + 2.0 * 6.3507208824157715
Epoch 480, val loss: 0.8842669725418091
Epoch 490, training loss: 13.295737266540527 = 0.5941341519355774 + 2.0 * 6.350801467895508
Epoch 490, val loss: 0.8684028387069702
Epoch 500, training loss: 13.270527839660645 = 0.5680212378501892 + 2.0 * 6.351253509521484
Epoch 500, val loss: 0.8533045649528503
Epoch 510, training loss: 13.234787940979004 = 0.54261314868927 + 2.0 * 6.346087455749512
Epoch 510, val loss: 0.8391115665435791
Epoch 520, training loss: 13.201915740966797 = 0.5178722739219666 + 2.0 * 6.342021942138672
Epoch 520, val loss: 0.8257462978363037
Epoch 530, training loss: 13.173120498657227 = 0.49366238713264465 + 2.0 * 6.339728832244873
Epoch 530, val loss: 0.8131710886955261
Epoch 540, training loss: 13.176291465759277 = 0.4700005352497101 + 2.0 * 6.353145599365234
Epoch 540, val loss: 0.8012597560882568
Epoch 550, training loss: 13.13831615447998 = 0.4472058415412903 + 2.0 * 6.345555305480957
Epoch 550, val loss: 0.7899156212806702
Epoch 560, training loss: 13.09540843963623 = 0.42530855536460876 + 2.0 * 6.335050106048584
Epoch 560, val loss: 0.7795243859291077
Epoch 570, training loss: 13.070477485656738 = 0.4040340483188629 + 2.0 * 6.333221912384033
Epoch 570, val loss: 0.7698870301246643
Epoch 580, training loss: 13.044919967651367 = 0.3833675682544708 + 2.0 * 6.330776214599609
Epoch 580, val loss: 0.7609138488769531
Epoch 590, training loss: 13.021589279174805 = 0.3633328080177307 + 2.0 * 6.329128265380859
Epoch 590, val loss: 0.7525674700737
Epoch 600, training loss: 12.998983383178711 = 0.3439708948135376 + 2.0 * 6.327506065368652
Epoch 600, val loss: 0.7448453903198242
Epoch 610, training loss: 12.994390487670898 = 0.3253597915172577 + 2.0 * 6.334515571594238
Epoch 610, val loss: 0.7377346158027649
Epoch 620, training loss: 12.965402603149414 = 0.30765536427497864 + 2.0 * 6.328873634338379
Epoch 620, val loss: 0.7312544584274292
Epoch 630, training loss: 12.941612243652344 = 0.29092177748680115 + 2.0 * 6.325345039367676
Epoch 630, val loss: 0.7256240248680115
Epoch 640, training loss: 12.919560432434082 = 0.27505654096603394 + 2.0 * 6.322251796722412
Epoch 640, val loss: 0.720767617225647
Epoch 650, training loss: 12.910619735717773 = 0.2600367069244385 + 2.0 * 6.325291633605957
Epoch 650, val loss: 0.7165713906288147
Epoch 660, training loss: 12.893407821655273 = 0.24595724046230316 + 2.0 * 6.32372522354126
Epoch 660, val loss: 0.7130599617958069
Epoch 670, training loss: 12.870588302612305 = 0.2327595055103302 + 2.0 * 6.318914413452148
Epoch 670, val loss: 0.7103555202484131
Epoch 680, training loss: 12.854784965515137 = 0.220377117395401 + 2.0 * 6.317203998565674
Epoch 680, val loss: 0.7083197832107544
Epoch 690, training loss: 12.851203918457031 = 0.20877981185913086 + 2.0 * 6.321212291717529
Epoch 690, val loss: 0.706881046295166
Epoch 700, training loss: 12.825325012207031 = 0.1979697197675705 + 2.0 * 6.313677787780762
Epoch 700, val loss: 0.7061094641685486
Epoch 710, training loss: 12.81394100189209 = 0.1878518909215927 + 2.0 * 6.313044548034668
Epoch 710, val loss: 0.705991268157959
Epoch 720, training loss: 12.810129165649414 = 0.1783752739429474 + 2.0 * 6.3158769607543945
Epoch 720, val loss: 0.7063205242156982
Epoch 730, training loss: 12.796712875366211 = 0.16951918601989746 + 2.0 * 6.313596725463867
Epoch 730, val loss: 0.7070881724357605
Epoch 740, training loss: 12.777291297912598 = 0.16124612092971802 + 2.0 * 6.308022499084473
Epoch 740, val loss: 0.7083359360694885
Epoch 750, training loss: 12.768762588500977 = 0.15349173545837402 + 2.0 * 6.307635307312012
Epoch 750, val loss: 0.709958016872406
Epoch 760, training loss: 12.770419120788574 = 0.14621181786060333 + 2.0 * 6.312103748321533
Epoch 760, val loss: 0.7119191884994507
Epoch 770, training loss: 12.764902114868164 = 0.13936372101306915 + 2.0 * 6.312769412994385
Epoch 770, val loss: 0.7140727043151855
Epoch 780, training loss: 12.740467071533203 = 0.13298776745796204 + 2.0 * 6.303739547729492
Epoch 780, val loss: 0.716609001159668
Epoch 790, training loss: 12.732390403747559 = 0.1269885003566742 + 2.0 * 6.302700996398926
Epoch 790, val loss: 0.719376802444458
Epoch 800, training loss: 12.724653244018555 = 0.12132249027490616 + 2.0 * 6.301665306091309
Epoch 800, val loss: 0.7223802208900452
Epoch 810, training loss: 12.729962348937988 = 0.11597652733325958 + 2.0 * 6.306993007659912
Epoch 810, val loss: 0.7255772352218628
Epoch 820, training loss: 12.712754249572754 = 0.11094105243682861 + 2.0 * 6.300906658172607
Epoch 820, val loss: 0.7289868593215942
Epoch 830, training loss: 12.710367202758789 = 0.10618510097265244 + 2.0 * 6.302091121673584
Epoch 830, val loss: 0.7325487732887268
Epoch 840, training loss: 12.701159477233887 = 0.10170174390077591 + 2.0 * 6.299728870391846
Epoch 840, val loss: 0.7360804080963135
Epoch 850, training loss: 12.69094181060791 = 0.09746675938367844 + 2.0 * 6.2967376708984375
Epoch 850, val loss: 0.7399009466171265
Epoch 860, training loss: 12.68757152557373 = 0.0934612974524498 + 2.0 * 6.297055244445801
Epoch 860, val loss: 0.7438035607337952
Epoch 870, training loss: 12.677709579467773 = 0.08966661989688873 + 2.0 * 6.2940216064453125
Epoch 870, val loss: 0.7476921081542969
Epoch 880, training loss: 12.678221702575684 = 0.08607280254364014 + 2.0 * 6.296074390411377
Epoch 880, val loss: 0.7517716884613037
Epoch 890, training loss: 12.67146110534668 = 0.08267547935247421 + 2.0 * 6.2943925857543945
Epoch 890, val loss: 0.7557916641235352
Epoch 900, training loss: 12.664717674255371 = 0.07944644242525101 + 2.0 * 6.292635440826416
Epoch 900, val loss: 0.7598997354507446
Epoch 910, training loss: 12.65720272064209 = 0.07639490067958832 + 2.0 * 6.290403842926025
Epoch 910, val loss: 0.7640941143035889
Epoch 920, training loss: 12.65509033203125 = 0.07348703593015671 + 2.0 * 6.290801525115967
Epoch 920, val loss: 0.7683035731315613
Epoch 930, training loss: 12.653072357177734 = 0.07071822881698608 + 2.0 * 6.291177272796631
Epoch 930, val loss: 0.7724946737289429
Epoch 940, training loss: 12.648110389709473 = 0.06809048354625702 + 2.0 * 6.29000997543335
Epoch 940, val loss: 0.7767590284347534
Epoch 950, training loss: 12.639290809631348 = 0.06559443473815918 + 2.0 * 6.286848068237305
Epoch 950, val loss: 0.7810661792755127
Epoch 960, training loss: 12.637836456298828 = 0.06321697682142258 + 2.0 * 6.287309646606445
Epoch 960, val loss: 0.7853623628616333
Epoch 970, training loss: 12.63268756866455 = 0.06095319241285324 + 2.0 * 6.285867214202881
Epoch 970, val loss: 0.7895946502685547
Epoch 980, training loss: 12.629551887512207 = 0.058798208832740784 + 2.0 * 6.285377025604248
Epoch 980, val loss: 0.7938900589942932
Epoch 990, training loss: 12.624258041381836 = 0.056747984141111374 + 2.0 * 6.283754825592041
Epoch 990, val loss: 0.7981915473937988
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8037037037037037
0.8434370057986295
=== training gcn model ===
Epoch 0, training loss: 19.16129493713379 = 1.9675899744033813 + 2.0 * 8.59685230255127
Epoch 0, val loss: 1.978993535041809
Epoch 10, training loss: 19.149717330932617 = 1.9566028118133545 + 2.0 * 8.5965576171875
Epoch 10, val loss: 1.967407464981079
Epoch 20, training loss: 19.130949020385742 = 1.9430103302001953 + 2.0 * 8.593969345092773
Epoch 20, val loss: 1.952783226966858
Epoch 30, training loss: 19.070825576782227 = 1.9243412017822266 + 2.0 * 8.5732421875
Epoch 30, val loss: 1.9325200319290161
Epoch 40, training loss: 18.78352928161621 = 1.9007270336151123 + 2.0 * 8.441401481628418
Epoch 40, val loss: 1.9078859090805054
Epoch 50, training loss: 17.80792999267578 = 1.874898910522461 + 2.0 * 7.96651554107666
Epoch 50, val loss: 1.881460428237915
Epoch 60, training loss: 17.298208236694336 = 1.8511619567871094 + 2.0 * 7.723523139953613
Epoch 60, val loss: 1.857930064201355
Epoch 70, training loss: 16.70232391357422 = 1.8336104154586792 + 2.0 * 7.434356689453125
Epoch 70, val loss: 1.84062659740448
Epoch 80, training loss: 16.311174392700195 = 1.8180986642837524 + 2.0 * 7.246537685394287
Epoch 80, val loss: 1.824690341949463
Epoch 90, training loss: 15.983925819396973 = 1.8009233474731445 + 2.0 * 7.091501235961914
Epoch 90, val loss: 1.8080514669418335
Epoch 100, training loss: 15.726347923278809 = 1.785461187362671 + 2.0 * 6.970443248748779
Epoch 100, val loss: 1.793373942375183
Epoch 110, training loss: 15.481873512268066 = 1.7720530033111572 + 2.0 * 6.854910373687744
Epoch 110, val loss: 1.7798831462860107
Epoch 120, training loss: 15.314619064331055 = 1.7582066059112549 + 2.0 * 6.7782063484191895
Epoch 120, val loss: 1.765800952911377
Epoch 130, training loss: 15.182999610900879 = 1.743007779121399 + 2.0 * 6.719995975494385
Epoch 130, val loss: 1.7512719631195068
Epoch 140, training loss: 15.063720703125 = 1.727372169494629 + 2.0 * 6.6681742668151855
Epoch 140, val loss: 1.7365349531173706
Epoch 150, training loss: 14.969507217407227 = 1.7103896141052246 + 2.0 * 6.62955904006958
Epoch 150, val loss: 1.721010446548462
Epoch 160, training loss: 14.89202880859375 = 1.6911664009094238 + 2.0 * 6.600431442260742
Epoch 160, val loss: 1.7039153575897217
Epoch 170, training loss: 14.809657096862793 = 1.669875144958496 + 2.0 * 6.569890975952148
Epoch 170, val loss: 1.6852976083755493
Epoch 180, training loss: 14.744787216186523 = 1.6461873054504395 + 2.0 * 6.549300193786621
Epoch 180, val loss: 1.6648105382919312
Epoch 190, training loss: 14.674530982971191 = 1.6197110414505005 + 2.0 * 6.52741003036499
Epoch 190, val loss: 1.6421083211898804
Epoch 200, training loss: 14.608926773071289 = 1.5904698371887207 + 2.0 * 6.509228706359863
Epoch 200, val loss: 1.6172772645950317
Epoch 210, training loss: 14.543998718261719 = 1.558464527130127 + 2.0 * 6.492767333984375
Epoch 210, val loss: 1.590420126914978
Epoch 220, training loss: 14.491745948791504 = 1.5237191915512085 + 2.0 * 6.484013557434082
Epoch 220, val loss: 1.5618085861206055
Epoch 230, training loss: 14.419645309448242 = 1.4872719049453735 + 2.0 * 6.4661865234375
Epoch 230, val loss: 1.5320781469345093
Epoch 240, training loss: 14.359874725341797 = 1.4496870040893555 + 2.0 * 6.455093860626221
Epoch 240, val loss: 1.502070665359497
Epoch 250, training loss: 14.309316635131836 = 1.4113088846206665 + 2.0 * 6.44900369644165
Epoch 250, val loss: 1.4720206260681152
Epoch 260, training loss: 14.245587348937988 = 1.373010277748108 + 2.0 * 6.436288356781006
Epoch 260, val loss: 1.442517638206482
Epoch 270, training loss: 14.18766975402832 = 1.334962010383606 + 2.0 * 6.426353931427002
Epoch 270, val loss: 1.4138010740280151
Epoch 280, training loss: 14.143062591552734 = 1.2971583604812622 + 2.0 * 6.422952175140381
Epoch 280, val loss: 1.3858354091644287
Epoch 290, training loss: 14.083806037902832 = 1.2601152658462524 + 2.0 * 6.4118452072143555
Epoch 290, val loss: 1.3583862781524658
Epoch 300, training loss: 14.03482723236084 = 1.223848581314087 + 2.0 * 6.405489444732666
Epoch 300, val loss: 1.331602931022644
Epoch 310, training loss: 13.985838890075684 = 1.188012719154358 + 2.0 * 6.3989129066467285
Epoch 310, val loss: 1.305220603942871
Epoch 320, training loss: 13.951223373413086 = 1.1524767875671387 + 2.0 * 6.399373531341553
Epoch 320, val loss: 1.2787694931030273
Epoch 330, training loss: 13.912288665771484 = 1.1173644065856934 + 2.0 * 6.397462368011475
Epoch 330, val loss: 1.2525309324264526
Epoch 340, training loss: 13.851136207580566 = 1.0830508470535278 + 2.0 * 6.384042739868164
Epoch 340, val loss: 1.2266643047332764
Epoch 350, training loss: 13.81035041809082 = 1.0492054224014282 + 2.0 * 6.380572319030762
Epoch 350, val loss: 1.2009254693984985
Epoch 360, training loss: 13.766596794128418 = 1.0157331228256226 + 2.0 * 6.375432014465332
Epoch 360, val loss: 1.175182819366455
Epoch 370, training loss: 13.72644329071045 = 0.9826964735984802 + 2.0 * 6.371873378753662
Epoch 370, val loss: 1.1496145725250244
Epoch 380, training loss: 13.696675300598145 = 0.9501500725746155 + 2.0 * 6.373262405395508
Epoch 380, val loss: 1.124286413192749
Epoch 390, training loss: 13.65009593963623 = 0.918165385723114 + 2.0 * 6.365965366363525
Epoch 390, val loss: 1.0992567539215088
Epoch 400, training loss: 13.625679016113281 = 0.8869221210479736 + 2.0 * 6.369378566741943
Epoch 400, val loss: 1.0746599435806274
Epoch 410, training loss: 13.573235511779785 = 0.8563310503959656 + 2.0 * 6.358452320098877
Epoch 410, val loss: 1.0507086515426636
Epoch 420, training loss: 13.537515640258789 = 0.8265408277511597 + 2.0 * 6.35548734664917
Epoch 420, val loss: 1.0273302793502808
Epoch 430, training loss: 13.501595497131348 = 0.7974289655685425 + 2.0 * 6.352083206176758
Epoch 430, val loss: 1.0045089721679688
Epoch 440, training loss: 13.467752456665039 = 0.7689334750175476 + 2.0 * 6.349409580230713
Epoch 440, val loss: 0.9823028445243835
Epoch 450, training loss: 13.457420349121094 = 0.7412000298500061 + 2.0 * 6.358109951019287
Epoch 450, val loss: 0.9608083963394165
Epoch 460, training loss: 13.406487464904785 = 0.714298665523529 + 2.0 * 6.346094608306885
Epoch 460, val loss: 0.9403344988822937
Epoch 470, training loss: 13.371513366699219 = 0.6883845329284668 + 2.0 * 6.341564655303955
Epoch 470, val loss: 0.9209402799606323
Epoch 480, training loss: 13.340438842773438 = 0.6631721258163452 + 2.0 * 6.3386335372924805
Epoch 480, val loss: 0.9025699496269226
Epoch 490, training loss: 13.315354347229004 = 0.6386042833328247 + 2.0 * 6.338375091552734
Epoch 490, val loss: 0.8850943446159363
Epoch 500, training loss: 13.288118362426758 = 0.6147044897079468 + 2.0 * 6.33670711517334
Epoch 500, val loss: 0.8684061169624329
Epoch 510, training loss: 13.2542724609375 = 0.5915861129760742 + 2.0 * 6.331343173980713
Epoch 510, val loss: 0.8528591990470886
Epoch 520, training loss: 13.228975296020508 = 0.5690474510192871 + 2.0 * 6.3299641609191895
Epoch 520, val loss: 0.8382924795150757
Epoch 530, training loss: 13.210531234741211 = 0.5471121668815613 + 2.0 * 6.331709384918213
Epoch 530, val loss: 0.8245842456817627
Epoch 540, training loss: 13.178278923034668 = 0.5257997512817383 + 2.0 * 6.326239585876465
Epoch 540, val loss: 0.8118385076522827
Epoch 550, training loss: 13.151230812072754 = 0.5051509141921997 + 2.0 * 6.323040008544922
Epoch 550, val loss: 0.8000724911689758
Epoch 560, training loss: 13.126812934875488 = 0.4850458800792694 + 2.0 * 6.320883750915527
Epoch 560, val loss: 0.7890976667404175
Epoch 570, training loss: 13.108983039855957 = 0.4654584527015686 + 2.0 * 6.3217620849609375
Epoch 570, val loss: 0.778853714466095
Epoch 580, training loss: 13.094758987426758 = 0.4463258683681488 + 2.0 * 6.324216365814209
Epoch 580, val loss: 0.7694094777107239
Epoch 590, training loss: 13.060759544372559 = 0.4279487431049347 + 2.0 * 6.316405296325684
Epoch 590, val loss: 0.7607361674308777
Epoch 600, training loss: 13.037014961242676 = 0.41007640957832336 + 2.0 * 6.313469409942627
Epoch 600, val loss: 0.7528566718101501
Epoch 610, training loss: 13.016606330871582 = 0.392656147480011 + 2.0 * 6.311975002288818
Epoch 610, val loss: 0.7456070780754089
Epoch 620, training loss: 13.013805389404297 = 0.37567800283432007 + 2.0 * 6.319063663482666
Epoch 620, val loss: 0.7390888929367065
Epoch 630, training loss: 12.979608535766602 = 0.35910290479660034 + 2.0 * 6.310252666473389
Epoch 630, val loss: 0.7330095767974854
Epoch 640, training loss: 12.959491729736328 = 0.3430546820163727 + 2.0 * 6.308218479156494
Epoch 640, val loss: 0.7276976108551025
Epoch 650, training loss: 12.940130233764648 = 0.3274087607860565 + 2.0 * 6.306360721588135
Epoch 650, val loss: 0.7230699062347412
Epoch 660, training loss: 12.923809051513672 = 0.31213507056236267 + 2.0 * 6.305837154388428
Epoch 660, val loss: 0.7189314961433411
Epoch 670, training loss: 12.906201362609863 = 0.2973022758960724 + 2.0 * 6.304449558258057
Epoch 670, val loss: 0.7153208255767822
Epoch 680, training loss: 12.886119842529297 = 0.28284814953804016 + 2.0 * 6.3016357421875
Epoch 680, val loss: 0.7122960090637207
Epoch 690, training loss: 12.873279571533203 = 0.2687786817550659 + 2.0 * 6.302250385284424
Epoch 690, val loss: 0.7097666263580322
Epoch 700, training loss: 12.854202270507812 = 0.2550632059574127 + 2.0 * 6.299569606781006
Epoch 700, val loss: 0.7076794505119324
Epoch 710, training loss: 12.839540481567383 = 0.24187594652175903 + 2.0 * 6.298832416534424
Epoch 710, val loss: 0.7060800194740295
Epoch 720, training loss: 12.8218994140625 = 0.22914272546768188 + 2.0 * 6.296378135681152
Epoch 720, val loss: 0.7050849199295044
Epoch 730, training loss: 12.806938171386719 = 0.21690572798252106 + 2.0 * 6.295016288757324
Epoch 730, val loss: 0.7045372724533081
Epoch 740, training loss: 12.823538780212402 = 0.20521168410778046 + 2.0 * 6.309163570404053
Epoch 740, val loss: 0.7044726610183716
Epoch 750, training loss: 12.781428337097168 = 0.19410939514636993 + 2.0 * 6.293659687042236
Epoch 750, val loss: 0.7045153975486755
Epoch 760, training loss: 12.76684856414795 = 0.1836487054824829 + 2.0 * 6.291599750518799
Epoch 760, val loss: 0.7051072120666504
Epoch 770, training loss: 12.755195617675781 = 0.17377467453479767 + 2.0 * 6.29071044921875
Epoch 770, val loss: 0.7062764763832092
Epoch 780, training loss: 12.75259780883789 = 0.1644938886165619 + 2.0 * 6.2940521240234375
Epoch 780, val loss: 0.7076697945594788
Epoch 790, training loss: 12.736411094665527 = 0.15573202073574066 + 2.0 * 6.290339469909668
Epoch 790, val loss: 0.7094525098800659
Epoch 800, training loss: 12.724335670471191 = 0.1476069539785385 + 2.0 * 6.288364410400391
Epoch 800, val loss: 0.7114294767379761
Epoch 810, training loss: 12.712052345275879 = 0.1399778574705124 + 2.0 * 6.286037445068359
Epoch 810, val loss: 0.7137542963027954
Epoch 820, training loss: 12.703269004821777 = 0.13285115361213684 + 2.0 * 6.285208702087402
Epoch 820, val loss: 0.7162628173828125
Epoch 830, training loss: 12.703136444091797 = 0.12615929543972015 + 2.0 * 6.288488388061523
Epoch 830, val loss: 0.7190113067626953
Epoch 840, training loss: 12.690793991088867 = 0.11992128938436508 + 2.0 * 6.285436153411865
Epoch 840, val loss: 0.7217597961425781
Epoch 850, training loss: 12.678621292114258 = 0.11411099880933762 + 2.0 * 6.282255172729492
Epoch 850, val loss: 0.7247347235679626
Epoch 860, training loss: 12.673375129699707 = 0.10866548120975494 + 2.0 * 6.282354831695557
Epoch 860, val loss: 0.7278235554695129
Epoch 870, training loss: 12.666808128356934 = 0.10354980826377869 + 2.0 * 6.2816290855407715
Epoch 870, val loss: 0.7311629056930542
Epoch 880, training loss: 12.658090591430664 = 0.09874744713306427 + 2.0 * 6.279671669006348
Epoch 880, val loss: 0.7345872521400452
Epoch 890, training loss: 12.660768508911133 = 0.09423808753490448 + 2.0 * 6.283265113830566
Epoch 890, val loss: 0.7380257844924927
Epoch 900, training loss: 12.649092674255371 = 0.08999764919281006 + 2.0 * 6.279547691345215
Epoch 900, val loss: 0.7414751052856445
Epoch 910, training loss: 12.639782905578613 = 0.086027592420578 + 2.0 * 6.2768778800964355
Epoch 910, val loss: 0.7449561357498169
Epoch 920, training loss: 12.632678985595703 = 0.08227776736021042 + 2.0 * 6.275200843811035
Epoch 920, val loss: 0.7486693263053894
Epoch 930, training loss: 12.629544258117676 = 0.07873723655939102 + 2.0 * 6.2754034996032715
Epoch 930, val loss: 0.7523713707923889
Epoch 940, training loss: 12.629544258117676 = 0.07537905871868134 + 2.0 * 6.277082443237305
Epoch 940, val loss: 0.7560681104660034
Epoch 950, training loss: 12.617504119873047 = 0.07221349328756332 + 2.0 * 6.272645473480225
Epoch 950, val loss: 0.7597519755363464
Epoch 960, training loss: 12.613870620727539 = 0.0692240372300148 + 2.0 * 6.272323131561279
Epoch 960, val loss: 0.7635024189949036
Epoch 970, training loss: 12.608583450317383 = 0.06639678031206131 + 2.0 * 6.271093368530273
Epoch 970, val loss: 0.7672466039657593
Epoch 980, training loss: 12.629888534545898 = 0.06369877606630325 + 2.0 * 6.283094882965088
Epoch 980, val loss: 0.7709894180297852
Epoch 990, training loss: 12.612419128417969 = 0.06117922067642212 + 2.0 * 6.275619983673096
Epoch 990, val loss: 0.7747654914855957
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7962962962962963
0.8365840801265156
The final CL Acc:0.79136, 0.01259, The final GNN Acc:0.83922, 0.00301
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.4 der2:0 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11556])
remove edge: torch.Size([2, 9460])
updated graph: torch.Size([2, 10460])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 19.152437210083008 = 1.958818793296814 + 2.0 * 8.596809387207031
Epoch 0, val loss: 1.9584431648254395
Epoch 10, training loss: 19.140737533569336 = 1.9480172395706177 + 2.0 * 8.596360206604004
Epoch 10, val loss: 1.9471584558486938
Epoch 20, training loss: 19.119230270385742 = 1.9345043897628784 + 2.0 * 8.592363357543945
Epoch 20, val loss: 1.9326680898666382
Epoch 30, training loss: 19.03692626953125 = 1.9159727096557617 + 2.0 * 8.560477256774902
Epoch 30, val loss: 1.912988305091858
Epoch 40, training loss: 18.607067108154297 = 1.8939719200134277 + 2.0 * 8.356547355651855
Epoch 40, val loss: 1.890832543373108
Epoch 50, training loss: 17.567907333374023 = 1.8709028959274292 + 2.0 * 7.8485026359558105
Epoch 50, val loss: 1.8677806854248047
Epoch 60, training loss: 16.694650650024414 = 1.8558725118637085 + 2.0 * 7.419389247894287
Epoch 60, val loss: 1.8539766073226929
Epoch 70, training loss: 16.062744140625 = 1.8451814651489258 + 2.0 * 7.108781814575195
Epoch 70, val loss: 1.844104290008545
Epoch 80, training loss: 15.593347549438477 = 1.8354334831237793 + 2.0 * 6.878957271575928
Epoch 80, val loss: 1.8351631164550781
Epoch 90, training loss: 15.348079681396484 = 1.8238376379013062 + 2.0 * 6.762121200561523
Epoch 90, val loss: 1.824938416481018
Epoch 100, training loss: 15.165498733520508 = 1.8121337890625 + 2.0 * 6.676682472229004
Epoch 100, val loss: 1.8149172067642212
Epoch 110, training loss: 15.045510292053223 = 1.8014105558395386 + 2.0 * 6.622049808502197
Epoch 110, val loss: 1.8058503866195679
Epoch 120, training loss: 14.963066101074219 = 1.791061282157898 + 2.0 * 6.586002349853516
Epoch 120, val loss: 1.797012448310852
Epoch 130, training loss: 14.885360717773438 = 1.780358910560608 + 2.0 * 6.5525007247924805
Epoch 130, val loss: 1.787980079650879
Epoch 140, training loss: 14.82418441772461 = 1.7690521478652954 + 2.0 * 6.527565956115723
Epoch 140, val loss: 1.778635859489441
Epoch 150, training loss: 14.775208473205566 = 1.7568223476409912 + 2.0 * 6.509192943572998
Epoch 150, val loss: 1.7687032222747803
Epoch 160, training loss: 14.728156089782715 = 1.7434790134429932 + 2.0 * 6.49233865737915
Epoch 160, val loss: 1.7579004764556885
Epoch 170, training loss: 14.683565139770508 = 1.728756308555603 + 2.0 * 6.477404594421387
Epoch 170, val loss: 1.7460278272628784
Epoch 180, training loss: 14.65855884552002 = 1.712306261062622 + 2.0 * 6.473126411437988
Epoch 180, val loss: 1.7327951192855835
Epoch 190, training loss: 14.600584030151367 = 1.694085955619812 + 2.0 * 6.453248977661133
Epoch 190, val loss: 1.718093991279602
Epoch 200, training loss: 14.560660362243652 = 1.6739500761032104 + 2.0 * 6.443355083465576
Epoch 200, val loss: 1.7018799781799316
Epoch 210, training loss: 14.519827842712402 = 1.6516485214233398 + 2.0 * 6.434089660644531
Epoch 210, val loss: 1.6839669942855835
Epoch 220, training loss: 14.47708511352539 = 1.6272639036178589 + 2.0 * 6.424910545349121
Epoch 220, val loss: 1.664368748664856
Epoch 230, training loss: 14.432146072387695 = 1.6010180711746216 + 2.0 * 6.415564060211182
Epoch 230, val loss: 1.6433624029159546
Epoch 240, training loss: 14.3882417678833 = 1.5728110074996948 + 2.0 * 6.407715320587158
Epoch 240, val loss: 1.6207714080810547
Epoch 250, training loss: 14.358787536621094 = 1.5428487062454224 + 2.0 * 6.4079694747924805
Epoch 250, val loss: 1.596888542175293
Epoch 260, training loss: 14.304571151733398 = 1.5117450952529907 + 2.0 * 6.3964128494262695
Epoch 260, val loss: 1.572145700454712
Epoch 270, training loss: 14.25813102722168 = 1.4799448251724243 + 2.0 * 6.389092922210693
Epoch 270, val loss: 1.5470770597457886
Epoch 280, training loss: 14.213438987731934 = 1.4475404024124146 + 2.0 * 6.382949352264404
Epoch 280, val loss: 1.5216624736785889
Epoch 290, training loss: 14.17282772064209 = 1.4147804975509644 + 2.0 * 6.379023551940918
Epoch 290, val loss: 1.4960505962371826
Epoch 300, training loss: 14.142881393432617 = 1.3821555376052856 + 2.0 * 6.3803629875183105
Epoch 300, val loss: 1.4709086418151855
Epoch 310, training loss: 14.088447570800781 = 1.3500727415084839 + 2.0 * 6.369187355041504
Epoch 310, val loss: 1.4461983442306519
Epoch 320, training loss: 14.04837417602539 = 1.318171501159668 + 2.0 * 6.365101337432861
Epoch 320, val loss: 1.4219846725463867
Epoch 330, training loss: 14.009686470031738 = 1.2861268520355225 + 2.0 * 6.361779689788818
Epoch 330, val loss: 1.3977619409561157
Epoch 340, training loss: 13.977962493896484 = 1.2544922828674316 + 2.0 * 6.361734867095947
Epoch 340, val loss: 1.3739477396011353
Epoch 350, training loss: 13.933003425598145 = 1.2230759859085083 + 2.0 * 6.354963779449463
Epoch 350, val loss: 1.350632905960083
Epoch 360, training loss: 13.892948150634766 = 1.1917405128479004 + 2.0 * 6.350604057312012
Epoch 360, val loss: 1.3274885416030884
Epoch 370, training loss: 13.854991912841797 = 1.160410761833191 + 2.0 * 6.347290515899658
Epoch 370, val loss: 1.3045910596847534
Epoch 380, training loss: 13.829325675964355 = 1.1289881467819214 + 2.0 * 6.350168704986572
Epoch 380, val loss: 1.2816468477249146
Epoch 390, training loss: 13.786479949951172 = 1.0974195003509521 + 2.0 * 6.34453010559082
Epoch 390, val loss: 1.25872802734375
Epoch 400, training loss: 13.746744155883789 = 1.0661076307296753 + 2.0 * 6.340318202972412
Epoch 400, val loss: 1.2365833520889282
Epoch 410, training loss: 13.70796012878418 = 1.034945011138916 + 2.0 * 6.336507320404053
Epoch 410, val loss: 1.2146565914154053
Epoch 420, training loss: 13.670071601867676 = 1.0038602352142334 + 2.0 * 6.333105564117432
Epoch 420, val loss: 1.1931184530258179
Epoch 430, training loss: 13.637458801269531 = 0.9729201197624207 + 2.0 * 6.332269191741943
Epoch 430, val loss: 1.1719292402267456
Epoch 440, training loss: 13.609546661376953 = 0.9422409534454346 + 2.0 * 6.333652973175049
Epoch 440, val loss: 1.1513751745224
Epoch 450, training loss: 13.56610107421875 = 0.9122232794761658 + 2.0 * 6.326939105987549
Epoch 450, val loss: 1.1315337419509888
Epoch 460, training loss: 13.530842781066895 = 0.8827588558197021 + 2.0 * 6.324041843414307
Epoch 460, val loss: 1.1126419305801392
Epoch 470, training loss: 13.498501777648926 = 0.8538382053375244 + 2.0 * 6.32233190536499
Epoch 470, val loss: 1.0944149494171143
Epoch 480, training loss: 13.487099647521973 = 0.8255729079246521 + 2.0 * 6.330763339996338
Epoch 480, val loss: 1.0770210027694702
Epoch 490, training loss: 13.444287300109863 = 0.7982446551322937 + 2.0 * 6.323021411895752
Epoch 490, val loss: 1.0607651472091675
Epoch 500, training loss: 13.408980369567871 = 0.7718174457550049 + 2.0 * 6.318581581115723
Epoch 500, val loss: 1.0457628965377808
Epoch 510, training loss: 13.376118659973145 = 0.7462615966796875 + 2.0 * 6.3149285316467285
Epoch 510, val loss: 1.0316051244735718
Epoch 520, training loss: 13.350135803222656 = 0.7214576601982117 + 2.0 * 6.3143391609191895
Epoch 520, val loss: 1.018517017364502
Epoch 530, training loss: 13.321531295776367 = 0.6974485516548157 + 2.0 * 6.312041282653809
Epoch 530, val loss: 1.006469488143921
Epoch 540, training loss: 13.297584533691406 = 0.6742348670959473 + 2.0 * 6.31167459487915
Epoch 540, val loss: 0.9955340623855591
Epoch 550, training loss: 13.27614688873291 = 0.6518745422363281 + 2.0 * 6.312136173248291
Epoch 550, val loss: 0.9854847192764282
Epoch 560, training loss: 13.243800163269043 = 0.6303141713142395 + 2.0 * 6.306743144989014
Epoch 560, val loss: 0.9766087532043457
Epoch 570, training loss: 13.221211433410645 = 0.6095054745674133 + 2.0 * 6.305852890014648
Epoch 570, val loss: 0.9688101410865784
Epoch 580, training loss: 13.200963973999023 = 0.589358925819397 + 2.0 * 6.305802345275879
Epoch 580, val loss: 0.9618317484855652
Epoch 590, training loss: 13.178305625915527 = 0.5699173212051392 + 2.0 * 6.30419397354126
Epoch 590, val loss: 0.9558753371238708
Epoch 600, training loss: 13.157047271728516 = 0.5510035753250122 + 2.0 * 6.3030219078063965
Epoch 600, val loss: 0.9506997466087341
Epoch 610, training loss: 13.133170127868652 = 0.5325477123260498 + 2.0 * 6.300311088562012
Epoch 610, val loss: 0.9461275339126587
Epoch 620, training loss: 13.111845970153809 = 0.5145326852798462 + 2.0 * 6.298656463623047
Epoch 620, val loss: 0.9423770308494568
Epoch 630, training loss: 13.09940242767334 = 0.49687108397483826 + 2.0 * 6.301265716552734
Epoch 630, val loss: 0.9392597675323486
Epoch 640, training loss: 13.074374198913574 = 0.4795151352882385 + 2.0 * 6.29742956161499
Epoch 640, val loss: 0.9364652037620544
Epoch 650, training loss: 13.052847862243652 = 0.46249935030937195 + 2.0 * 6.2951741218566895
Epoch 650, val loss: 0.9343862533569336
Epoch 660, training loss: 13.034476280212402 = 0.44569456577301025 + 2.0 * 6.294390678405762
Epoch 660, val loss: 0.9325369000434875
Epoch 670, training loss: 13.025312423706055 = 0.42915964126586914 + 2.0 * 6.298076629638672
Epoch 670, val loss: 0.9310527443885803
Epoch 680, training loss: 12.99914264678955 = 0.41289761662483215 + 2.0 * 6.293122291564941
Epoch 680, val loss: 0.9298754930496216
Epoch 690, training loss: 12.97876262664795 = 0.3969824016094208 + 2.0 * 6.290890216827393
Epoch 690, val loss: 0.9292501211166382
Epoch 700, training loss: 12.961853981018066 = 0.38131430745124817 + 2.0 * 6.29026985168457
Epoch 700, val loss: 0.9287877082824707
Epoch 710, training loss: 12.948148727416992 = 0.3659362494945526 + 2.0 * 6.291106224060059
Epoch 710, val loss: 0.928419291973114
Epoch 720, training loss: 12.930532455444336 = 0.35099154710769653 + 2.0 * 6.289770603179932
Epoch 720, val loss: 0.9286234974861145
Epoch 730, training loss: 12.909881591796875 = 0.3364199995994568 + 2.0 * 6.286730766296387
Epoch 730, val loss: 0.92901611328125
Epoch 740, training loss: 12.892993927001953 = 0.32221120595932007 + 2.0 * 6.285391330718994
Epoch 740, val loss: 0.9297376275062561
Epoch 750, training loss: 12.891280174255371 = 0.3083900213241577 + 2.0 * 6.291445255279541
Epoch 750, val loss: 0.930804967880249
Epoch 760, training loss: 12.86514949798584 = 0.2950795292854309 + 2.0 * 6.285035133361816
Epoch 760, val loss: 0.932227373123169
Epoch 770, training loss: 12.84799861907959 = 0.2822248637676239 + 2.0 * 6.282886981964111
Epoch 770, val loss: 0.9340914487838745
Epoch 780, training loss: 12.836585998535156 = 0.26979056000709534 + 2.0 * 6.283397674560547
Epoch 780, val loss: 0.9363079071044922
Epoch 790, training loss: 12.820688247680664 = 0.25783589482307434 + 2.0 * 6.281425952911377
Epoch 790, val loss: 0.9388881325721741
Epoch 800, training loss: 12.817292213439941 = 0.24632881581783295 + 2.0 * 6.285481929779053
Epoch 800, val loss: 0.9419188499450684
Epoch 810, training loss: 12.796079635620117 = 0.23530788719654083 + 2.0 * 6.280385971069336
Epoch 810, val loss: 0.945196807384491
Epoch 820, training loss: 12.781072616577148 = 0.224749356508255 + 2.0 * 6.278161525726318
Epoch 820, val loss: 0.9491389393806458
Epoch 830, training loss: 12.769447326660156 = 0.214610293507576 + 2.0 * 6.277418613433838
Epoch 830, val loss: 0.9534348249435425
Epoch 840, training loss: 12.770109176635742 = 0.20488403737545013 + 2.0 * 6.2826128005981445
Epoch 840, val loss: 0.9581241011619568
Epoch 850, training loss: 12.756256103515625 = 0.1955512911081314 + 2.0 * 6.280352592468262
Epoch 850, val loss: 0.963057816028595
Epoch 860, training loss: 12.738430976867676 = 0.18666104972362518 + 2.0 * 6.275885105133057
Epoch 860, val loss: 0.9686039686203003
Epoch 870, training loss: 12.72927474975586 = 0.1781499683856964 + 2.0 * 6.275562286376953
Epoch 870, val loss: 0.9742887020111084
Epoch 880, training loss: 12.72136402130127 = 0.1700117588043213 + 2.0 * 6.275676250457764
Epoch 880, val loss: 0.9804107546806335
Epoch 890, training loss: 12.711648941040039 = 0.1622486561536789 + 2.0 * 6.274700164794922
Epoch 890, val loss: 0.9869353175163269
Epoch 900, training loss: 12.698236465454102 = 0.1548190712928772 + 2.0 * 6.2717084884643555
Epoch 900, val loss: 0.9935898184776306
Epoch 910, training loss: 12.693572998046875 = 0.14773043990135193 + 2.0 * 6.272921085357666
Epoch 910, val loss: 1.0005797147750854
Epoch 920, training loss: 12.689560890197754 = 0.1409911960363388 + 2.0 * 6.274284839630127
Epoch 920, val loss: 1.0077848434448242
Epoch 930, training loss: 12.677529335021973 = 0.13455402851104736 + 2.0 * 6.271487712860107
Epoch 930, val loss: 1.0151710510253906
Epoch 940, training loss: 12.675968170166016 = 0.12845362722873688 + 2.0 * 6.273757457733154
Epoch 940, val loss: 1.0227962732315063
Epoch 950, training loss: 12.659404754638672 = 0.12266842275857925 + 2.0 * 6.268368244171143
Epoch 950, val loss: 1.0306060314178467
Epoch 960, training loss: 12.651853561401367 = 0.11717231571674347 + 2.0 * 6.267340660095215
Epoch 960, val loss: 1.0386779308319092
Epoch 970, training loss: 12.646574020385742 = 0.1119377464056015 + 2.0 * 6.267318248748779
Epoch 970, val loss: 1.046913743019104
Epoch 980, training loss: 12.646073341369629 = 0.10696523636579514 + 2.0 * 6.269554138183594
Epoch 980, val loss: 1.0552849769592285
Epoch 990, training loss: 12.647347450256348 = 0.10225052386522293 + 2.0 * 6.272548675537109
Epoch 990, val loss: 1.0635751485824585
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7592592592592593
0.8028465998945704
=== training gcn model ===
Epoch 0, training loss: 19.147859573364258 = 1.9541974067687988 + 2.0 * 8.596831321716309
Epoch 0, val loss: 1.9495779275894165
Epoch 10, training loss: 19.13691520690918 = 1.9439092874526978 + 2.0 * 8.596503257751465
Epoch 10, val loss: 1.9396352767944336
Epoch 20, training loss: 19.11880874633789 = 1.9311604499816895 + 2.0 * 8.59382438659668
Epoch 20, val loss: 1.9268652200698853
Epoch 30, training loss: 19.06261444091797 = 1.9135539531707764 + 2.0 * 8.574530601501465
Epoch 30, val loss: 1.9089360237121582
Epoch 40, training loss: 18.825576782226562 = 1.8911159038543701 + 2.0 * 8.467230796813965
Epoch 40, val loss: 1.8868416547775269
Epoch 50, training loss: 18.039472579956055 = 1.8660991191864014 + 2.0 * 8.086687088012695
Epoch 50, val loss: 1.863066554069519
Epoch 60, training loss: 17.478872299194336 = 1.8432011604309082 + 2.0 * 7.817835807800293
Epoch 60, val loss: 1.8425407409667969
Epoch 70, training loss: 16.609134674072266 = 1.828002691268921 + 2.0 * 7.390565872192383
Epoch 70, val loss: 1.828262209892273
Epoch 80, training loss: 16.0346622467041 = 1.8173420429229736 + 2.0 * 7.1086602210998535
Epoch 80, val loss: 1.8178377151489258
Epoch 90, training loss: 15.698352813720703 = 1.8064382076263428 + 2.0 * 6.945957183837891
Epoch 90, val loss: 1.8073172569274902
Epoch 100, training loss: 15.433023452758789 = 1.7950737476348877 + 2.0 * 6.81897497177124
Epoch 100, val loss: 1.7969985008239746
Epoch 110, training loss: 15.281662940979004 = 1.78452467918396 + 2.0 * 6.748569011688232
Epoch 110, val loss: 1.786868691444397
Epoch 120, training loss: 15.14819049835205 = 1.774535059928894 + 2.0 * 6.686827659606934
Epoch 120, val loss: 1.7767671346664429
Epoch 130, training loss: 15.04963207244873 = 1.764655351638794 + 2.0 * 6.642488479614258
Epoch 130, val loss: 1.7668673992156982
Epoch 140, training loss: 14.967756271362305 = 1.7539329528808594 + 2.0 * 6.606911659240723
Epoch 140, val loss: 1.7566267251968384
Epoch 150, training loss: 14.90042495727539 = 1.7419570684432983 + 2.0 * 6.5792341232299805
Epoch 150, val loss: 1.7456459999084473
Epoch 160, training loss: 14.845071792602539 = 1.728614330291748 + 2.0 * 6.558228492736816
Epoch 160, val loss: 1.7334814071655273
Epoch 170, training loss: 14.78927230834961 = 1.7135677337646484 + 2.0 * 6.5378522872924805
Epoch 170, val loss: 1.7197866439819336
Epoch 180, training loss: 14.7371826171875 = 1.696616768836975 + 2.0 * 6.520282745361328
Epoch 180, val loss: 1.7043187618255615
Epoch 190, training loss: 14.686864852905273 = 1.677368402481079 + 2.0 * 6.504748344421387
Epoch 190, val loss: 1.6868301630020142
Epoch 200, training loss: 14.646446228027344 = 1.6555284261703491 + 2.0 * 6.495459079742432
Epoch 200, val loss: 1.6671775579452515
Epoch 210, training loss: 14.588521003723145 = 1.630884051322937 + 2.0 * 6.478818416595459
Epoch 210, val loss: 1.6451854705810547
Epoch 220, training loss: 14.538331985473633 = 1.6032148599624634 + 2.0 * 6.46755838394165
Epoch 220, val loss: 1.620546579360962
Epoch 230, training loss: 14.499921798706055 = 1.5721104145050049 + 2.0 * 6.4639058113098145
Epoch 230, val loss: 1.5931735038757324
Epoch 240, training loss: 14.436752319335938 = 1.5379873514175415 + 2.0 * 6.449382305145264
Epoch 240, val loss: 1.5634315013885498
Epoch 250, training loss: 14.380806922912598 = 1.50069260597229 + 2.0 * 6.440057277679443
Epoch 250, val loss: 1.5313189029693604
Epoch 260, training loss: 14.326201438903809 = 1.4601507186889648 + 2.0 * 6.433025360107422
Epoch 260, val loss: 1.4970362186431885
Epoch 270, training loss: 14.273029327392578 = 1.4171528816223145 + 2.0 * 6.427938461303711
Epoch 270, val loss: 1.4612468481063843
Epoch 280, training loss: 14.20804214477539 = 1.3723244667053223 + 2.0 * 6.417859077453613
Epoch 280, val loss: 1.4246082305908203
Epoch 290, training loss: 14.14818286895752 = 1.3258540630340576 + 2.0 * 6.411164283752441
Epoch 290, val loss: 1.387288212776184
Epoch 300, training loss: 14.093805313110352 = 1.2784078121185303 + 2.0 * 6.407698631286621
Epoch 300, val loss: 1.349816918373108
Epoch 310, training loss: 14.031917572021484 = 1.2311453819274902 + 2.0 * 6.400386333465576
Epoch 310, val loss: 1.313056468963623
Epoch 320, training loss: 13.97185230255127 = 1.1840523481369019 + 2.0 * 6.393899917602539
Epoch 320, val loss: 1.2770814895629883
Epoch 330, training loss: 13.918718338012695 = 1.1374459266662598 + 2.0 * 6.390636444091797
Epoch 330, val loss: 1.2419706583023071
Epoch 340, training loss: 13.863153457641602 = 1.0924885272979736 + 2.0 * 6.3853325843811035
Epoch 340, val loss: 1.2085683345794678
Epoch 350, training loss: 13.806642532348633 = 1.0494592189788818 + 2.0 * 6.378591537475586
Epoch 350, val loss: 1.1771531105041504
Epoch 360, training loss: 13.770139694213867 = 1.0082682371139526 + 2.0 * 6.3809356689453125
Epoch 360, val loss: 1.1475759744644165
Epoch 370, training loss: 13.713943481445312 = 0.9694275856018066 + 2.0 * 6.372257709503174
Epoch 370, val loss: 1.120199203491211
Epoch 380, training loss: 13.668084144592285 = 0.9329484701156616 + 2.0 * 6.367568016052246
Epoch 380, val loss: 1.0950241088867188
Epoch 390, training loss: 13.624654769897461 = 0.8985192775726318 + 2.0 * 6.363067626953125
Epoch 390, val loss: 1.0717130899429321
Epoch 400, training loss: 13.58428955078125 = 0.8659722208976746 + 2.0 * 6.359158515930176
Epoch 400, val loss: 1.0502829551696777
Epoch 410, training loss: 13.570272445678711 = 0.8352921605110168 + 2.0 * 6.367490291595459
Epoch 410, val loss: 1.0307621955871582
Epoch 420, training loss: 13.515702247619629 = 0.8067706227302551 + 2.0 * 6.354465961456299
Epoch 420, val loss: 1.0132544040679932
Epoch 430, training loss: 13.48270320892334 = 0.780036985874176 + 2.0 * 6.351333141326904
Epoch 430, val loss: 0.9974719882011414
Epoch 440, training loss: 13.448073387145996 = 0.7546034455299377 + 2.0 * 6.346735000610352
Epoch 440, val loss: 0.9831023216247559
Epoch 450, training loss: 13.447663307189941 = 0.7302727103233337 + 2.0 * 6.3586955070495605
Epoch 450, val loss: 0.9699589610099792
Epoch 460, training loss: 13.405637741088867 = 0.7072311639785767 + 2.0 * 6.349203109741211
Epoch 460, val loss: 0.9581289887428284
Epoch 470, training loss: 13.36408805847168 = 0.6853125095367432 + 2.0 * 6.339387893676758
Epoch 470, val loss: 0.9474729895591736
Epoch 480, training loss: 13.33868408203125 = 0.664110004901886 + 2.0 * 6.337286949157715
Epoch 480, val loss: 0.9378352165222168
Epoch 490, training loss: 13.311619758605957 = 0.6434791684150696 + 2.0 * 6.334070205688477
Epoch 490, val loss: 0.9292200803756714
Epoch 500, training loss: 13.289528846740723 = 0.6233463287353516 + 2.0 * 6.3330912590026855
Epoch 500, val loss: 0.921478271484375
Epoch 510, training loss: 13.265789031982422 = 0.6038615107536316 + 2.0 * 6.330963611602783
Epoch 510, val loss: 0.91451096534729
Epoch 520, training loss: 13.240761756896973 = 0.5849902629852295 + 2.0 * 6.327885627746582
Epoch 520, val loss: 0.9085379242897034
Epoch 530, training loss: 13.219727516174316 = 0.5665498971939087 + 2.0 * 6.3265886306762695
Epoch 530, val loss: 0.9033031463623047
Epoch 540, training loss: 13.202987670898438 = 0.5484482645988464 + 2.0 * 6.327269554138184
Epoch 540, val loss: 0.8987941145896912
Epoch 550, training loss: 13.191095352172852 = 0.530921995639801 + 2.0 * 6.330086708068848
Epoch 550, val loss: 0.8951526880264282
Epoch 560, training loss: 13.15884780883789 = 0.5138956308364868 + 2.0 * 6.322475910186768
Epoch 560, val loss: 0.8922629356384277
Epoch 570, training loss: 13.135002136230469 = 0.49731552600860596 + 2.0 * 6.318843364715576
Epoch 570, val loss: 0.8900458216667175
Epoch 580, training loss: 13.117241859436035 = 0.48107993602752686 + 2.0 * 6.318080902099609
Epoch 580, val loss: 0.8885907530784607
Epoch 590, training loss: 13.099159240722656 = 0.4652714729309082 + 2.0 * 6.316943645477295
Epoch 590, val loss: 0.8876131176948547
Epoch 600, training loss: 13.076102256774902 = 0.44988957047462463 + 2.0 * 6.313106536865234
Epoch 600, val loss: 0.8873559832572937
Epoch 610, training loss: 13.05910587310791 = 0.43489110469818115 + 2.0 * 6.312107563018799
Epoch 610, val loss: 0.887721836566925
Epoch 620, training loss: 13.046653747558594 = 0.4202762544155121 + 2.0 * 6.313188552856445
Epoch 620, val loss: 0.8887424468994141
Epoch 630, training loss: 13.025835037231445 = 0.40613898634910583 + 2.0 * 6.309847831726074
Epoch 630, val loss: 0.8902394771575928
Epoch 640, training loss: 13.008565902709961 = 0.39235371351242065 + 2.0 * 6.308105945587158
Epoch 640, val loss: 0.8923197388648987
Epoch 650, training loss: 13.008660316467285 = 0.378852516412735 + 2.0 * 6.314903736114502
Epoch 650, val loss: 0.8950914740562439
Epoch 660, training loss: 12.981581687927246 = 0.36575913429260254 + 2.0 * 6.307911396026611
Epoch 660, val loss: 0.8983864784240723
Epoch 670, training loss: 12.962027549743652 = 0.3528802692890167 + 2.0 * 6.3045735359191895
Epoch 670, val loss: 0.9021873474121094
Epoch 680, training loss: 12.954793930053711 = 0.3402927815914154 + 2.0 * 6.307250499725342
Epoch 680, val loss: 0.9067201018333435
Epoch 690, training loss: 12.938538551330566 = 0.32801809906959534 + 2.0 * 6.305260181427002
Epoch 690, val loss: 0.9115483164787292
Epoch 700, training loss: 12.916259765625 = 0.3160606920719147 + 2.0 * 6.3000993728637695
Epoch 700, val loss: 0.9169415235519409
Epoch 710, training loss: 12.901666641235352 = 0.30436837673187256 + 2.0 * 6.298649311065674
Epoch 710, val loss: 0.9228987097740173
Epoch 720, training loss: 12.915437698364258 = 0.29292744398117065 + 2.0 * 6.311254978179932
Epoch 720, val loss: 0.929218053817749
Epoch 730, training loss: 12.884063720703125 = 0.2819228768348694 + 2.0 * 6.301070213317871
Epoch 730, val loss: 0.9361038208007812
Epoch 740, training loss: 12.862093925476074 = 0.2711855471134186 + 2.0 * 6.295454025268555
Epoch 740, val loss: 0.9434124827384949
Epoch 750, training loss: 12.849250793457031 = 0.26072996854782104 + 2.0 * 6.294260501861572
Epoch 750, val loss: 0.9513633251190186
Epoch 760, training loss: 12.869192123413086 = 0.2505769431591034 + 2.0 * 6.30930757522583
Epoch 760, val loss: 0.9595925807952881
Epoch 770, training loss: 12.824603080749512 = 0.2408236861228943 + 2.0 * 6.291889667510986
Epoch 770, val loss: 0.9682533740997314
Epoch 780, training loss: 12.814583778381348 = 0.23139876127243042 + 2.0 * 6.291592597961426
Epoch 780, val loss: 0.9771798849105835
Epoch 790, training loss: 12.803842544555664 = 0.2222587615251541 + 2.0 * 6.290791988372803
Epoch 790, val loss: 0.9866566061973572
Epoch 800, training loss: 12.805554389953613 = 0.2134046107530594 + 2.0 * 6.296074867248535
Epoch 800, val loss: 0.9964239001274109
Epoch 810, training loss: 12.790613174438477 = 0.20487216114997864 + 2.0 * 6.29287052154541
Epoch 810, val loss: 1.006377935409546
Epoch 820, training loss: 12.774604797363281 = 0.19664224982261658 + 2.0 * 6.2889814376831055
Epoch 820, val loss: 1.01652991771698
Epoch 830, training loss: 12.761832237243652 = 0.18869870901107788 + 2.0 * 6.286566734313965
Epoch 830, val loss: 1.0271235704421997
Epoch 840, training loss: 12.760255813598633 = 0.18103234469890594 + 2.0 * 6.28961181640625
Epoch 840, val loss: 1.0378612279891968
Epoch 850, training loss: 12.751154899597168 = 0.1736733764410019 + 2.0 * 6.288740634918213
Epoch 850, val loss: 1.0488203763961792
Epoch 860, training loss: 12.73862075805664 = 0.16663280129432678 + 2.0 * 6.285994052886963
Epoch 860, val loss: 1.059748888015747
Epoch 870, training loss: 12.72702407836914 = 0.1598631590604782 + 2.0 * 6.283580303192139
Epoch 870, val loss: 1.0709972381591797
Epoch 880, training loss: 12.717669486999512 = 0.15334872901439667 + 2.0 * 6.28216028213501
Epoch 880, val loss: 1.0823593139648438
Epoch 890, training loss: 12.725776672363281 = 0.14708943665027618 + 2.0 * 6.28934383392334
Epoch 890, val loss: 1.0937879085540771
Epoch 900, training loss: 12.713631629943848 = 0.1411256194114685 + 2.0 * 6.286252975463867
Epoch 900, val loss: 1.105262041091919
Epoch 910, training loss: 12.700077056884766 = 0.13543398678302765 + 2.0 * 6.282321453094482
Epoch 910, val loss: 1.1165961027145386
Epoch 920, training loss: 12.70638370513916 = 0.1299891620874405 + 2.0 * 6.288197040557861
Epoch 920, val loss: 1.1281795501708984
Epoch 930, training loss: 12.686756134033203 = 0.1248161643743515 + 2.0 * 6.280970096588135
Epoch 930, val loss: 1.139672875404358
Epoch 940, training loss: 12.67644214630127 = 0.11985072493553162 + 2.0 * 6.278295516967773
Epoch 940, val loss: 1.1511296033859253
Epoch 950, training loss: 12.668438911437988 = 0.11510474979877472 + 2.0 * 6.27666711807251
Epoch 950, val loss: 1.1626981496810913
Epoch 960, training loss: 12.66999340057373 = 0.11056093126535416 + 2.0 * 6.2797160148620605
Epoch 960, val loss: 1.1741514205932617
Epoch 970, training loss: 12.663095474243164 = 0.10622841119766235 + 2.0 * 6.278433322906494
Epoch 970, val loss: 1.1854616403579712
Epoch 980, training loss: 12.65737247467041 = 0.10211633890867233 + 2.0 * 6.277627944946289
Epoch 980, val loss: 1.1966837644577026
Epoch 990, training loss: 12.64931869506836 = 0.09819396585226059 + 2.0 * 6.275562286376953
Epoch 990, val loss: 1.2079391479492188
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6888888888888889
0.7970479704797049
=== training gcn model ===
Epoch 0, training loss: 19.13906478881836 = 1.945383071899414 + 2.0 * 8.596840858459473
Epoch 0, val loss: 1.9397623538970947
Epoch 10, training loss: 19.128633499145508 = 1.935556173324585 + 2.0 * 8.596538543701172
Epoch 10, val loss: 1.9293586015701294
Epoch 20, training loss: 19.111658096313477 = 1.9235541820526123 + 2.0 * 8.5940523147583
Epoch 20, val loss: 1.9166467189788818
Epoch 30, training loss: 19.059284210205078 = 1.9071626663208008 + 2.0 * 8.576061248779297
Epoch 30, val loss: 1.8993802070617676
Epoch 40, training loss: 18.83776092529297 = 1.8867110013961792 + 2.0 * 8.47552490234375
Epoch 40, val loss: 1.8789888620376587
Epoch 50, training loss: 18.022750854492188 = 1.8646219968795776 + 2.0 * 8.07906436920166
Epoch 50, val loss: 1.8575998544692993
Epoch 60, training loss: 17.413185119628906 = 1.8447002172470093 + 2.0 * 7.784242153167725
Epoch 60, val loss: 1.8403606414794922
Epoch 70, training loss: 16.526826858520508 = 1.8297593593597412 + 2.0 * 7.348533630371094
Epoch 70, val loss: 1.8273833990097046
Epoch 80, training loss: 15.961033821105957 = 1.8175013065338135 + 2.0 * 7.071766376495361
Epoch 80, val loss: 1.8163779973983765
Epoch 90, training loss: 15.666799545288086 = 1.80672025680542 + 2.0 * 6.930039882659912
Epoch 90, val loss: 1.8065365552902222
Epoch 100, training loss: 15.503714561462402 = 1.795578956604004 + 2.0 * 6.854067802429199
Epoch 100, val loss: 1.795973777770996
Epoch 110, training loss: 15.381336212158203 = 1.7840511798858643 + 2.0 * 6.798642635345459
Epoch 110, val loss: 1.7852922677993774
Epoch 120, training loss: 15.268213272094727 = 1.7734341621398926 + 2.0 * 6.747389793395996
Epoch 120, val loss: 1.7757600545883179
Epoch 130, training loss: 15.155098915100098 = 1.763569712638855 + 2.0 * 6.695764541625977
Epoch 130, val loss: 1.7667739391326904
Epoch 140, training loss: 15.063138008117676 = 1.753253698348999 + 2.0 * 6.654942035675049
Epoch 140, val loss: 1.7571496963500977
Epoch 150, training loss: 14.969746589660645 = 1.74148690700531 + 2.0 * 6.614130020141602
Epoch 150, val loss: 1.7462797164916992
Epoch 160, training loss: 14.89238452911377 = 1.7285881042480469 + 2.0 * 6.581898212432861
Epoch 160, val loss: 1.7345291376113892
Epoch 170, training loss: 14.818355560302734 = 1.7141835689544678 + 2.0 * 6.552085876464844
Epoch 170, val loss: 1.7217884063720703
Epoch 180, training loss: 14.757182121276855 = 1.6980088949203491 + 2.0 * 6.5295867919921875
Epoch 180, val loss: 1.707391381263733
Epoch 190, training loss: 14.701688766479492 = 1.6796832084655762 + 2.0 * 6.511003017425537
Epoch 190, val loss: 1.6912181377410889
Epoch 200, training loss: 14.656733512878418 = 1.6589003801345825 + 2.0 * 6.4989166259765625
Epoch 200, val loss: 1.67295503616333
Epoch 210, training loss: 14.6023588180542 = 1.6356258392333984 + 2.0 * 6.4833664894104
Epoch 210, val loss: 1.652754783630371
Epoch 220, training loss: 14.5514554977417 = 1.6096802949905396 + 2.0 * 6.470887660980225
Epoch 220, val loss: 1.630130410194397
Epoch 230, training loss: 14.499897956848145 = 1.5806537866592407 + 2.0 * 6.459621906280518
Epoch 230, val loss: 1.6049913167953491
Epoch 240, training loss: 14.451774597167969 = 1.548363208770752 + 2.0 * 6.451705455780029
Epoch 240, val loss: 1.577155351638794
Epoch 250, training loss: 14.394283294677734 = 1.5128934383392334 + 2.0 * 6.440694808959961
Epoch 250, val loss: 1.5469611883163452
Epoch 260, training loss: 14.342046737670898 = 1.4745429754257202 + 2.0 * 6.433752059936523
Epoch 260, val loss: 1.5144990682601929
Epoch 270, training loss: 14.283829689025879 = 1.4330202341079712 + 2.0 * 6.4254045486450195
Epoch 270, val loss: 1.4797065258026123
Epoch 280, training loss: 14.225308418273926 = 1.388601303100586 + 2.0 * 6.41835355758667
Epoch 280, val loss: 1.4429523944854736
Epoch 290, training loss: 14.179072380065918 = 1.342042088508606 + 2.0 * 6.418515205383301
Epoch 290, val loss: 1.4048445224761963
Epoch 300, training loss: 14.10661506652832 = 1.2944915294647217 + 2.0 * 6.40606164932251
Epoch 300, val loss: 1.3665461540222168
Epoch 310, training loss: 14.04548454284668 = 1.2461373805999756 + 2.0 * 6.3996734619140625
Epoch 310, val loss: 1.328154444694519
Epoch 320, training loss: 13.985308647155762 = 1.1973196268081665 + 2.0 * 6.393994331359863
Epoch 320, val loss: 1.2900031805038452
Epoch 330, training loss: 13.945541381835938 = 1.1486735343933105 + 2.0 * 6.398433685302734
Epoch 330, val loss: 1.2525025606155396
Epoch 340, training loss: 13.878226280212402 = 1.1013822555541992 + 2.0 * 6.388422012329102
Epoch 340, val loss: 1.2167549133300781
Epoch 350, training loss: 13.816482543945312 = 1.0559821128845215 + 2.0 * 6.380250453948975
Epoch 350, val loss: 1.183037281036377
Epoch 360, training loss: 13.765459060668945 = 1.0122716426849365 + 2.0 * 6.376593589782715
Epoch 360, val loss: 1.1511749029159546
Epoch 370, training loss: 13.72005558013916 = 0.9702889919281006 + 2.0 * 6.37488317489624
Epoch 370, val loss: 1.121308445930481
Epoch 380, training loss: 13.676835060119629 = 0.9306284785270691 + 2.0 * 6.373103141784668
Epoch 380, val loss: 1.0933395624160767
Epoch 390, training loss: 13.626806259155273 = 0.8933154344558716 + 2.0 * 6.366745471954346
Epoch 390, val loss: 1.0679668188095093
Epoch 400, training loss: 13.583919525146484 = 0.8579747676849365 + 2.0 * 6.362972259521484
Epoch 400, val loss: 1.0444884300231934
Epoch 410, training loss: 13.557393074035645 = 0.8244083523750305 + 2.0 * 6.36649227142334
Epoch 410, val loss: 1.0228300094604492
Epoch 420, training loss: 13.50716495513916 = 0.7928206324577332 + 2.0 * 6.357172012329102
Epoch 420, val loss: 1.003019094467163
Epoch 430, training loss: 13.47458553314209 = 0.7629359364509583 + 2.0 * 6.355824947357178
Epoch 430, val loss: 0.9850136041641235
Epoch 440, training loss: 13.436355590820312 = 0.7345415949821472 + 2.0 * 6.350906848907471
Epoch 440, val loss: 0.9685543775558472
Epoch 450, training loss: 13.408804893493652 = 0.7075265645980835 + 2.0 * 6.350639343261719
Epoch 450, val loss: 0.9535353183746338
Epoch 460, training loss: 13.377459526062012 = 0.6818701028823853 + 2.0 * 6.347794532775879
Epoch 460, val loss: 0.9397621154785156
Epoch 470, training loss: 13.343751907348633 = 0.6573906540870667 + 2.0 * 6.3431806564331055
Epoch 470, val loss: 0.9273583889007568
Epoch 480, training loss: 13.319087982177734 = 0.6339293718338013 + 2.0 * 6.342579364776611
Epoch 480, val loss: 0.9160998463630676
Epoch 490, training loss: 13.2953462600708 = 0.611485481262207 + 2.0 * 6.341930389404297
Epoch 490, val loss: 0.9059154987335205
Epoch 500, training loss: 13.263030052185059 = 0.5901387929916382 + 2.0 * 6.3364458084106445
Epoch 500, val loss: 0.8968538045883179
Epoch 510, training loss: 13.237588882446289 = 0.5695610046386719 + 2.0 * 6.334013938903809
Epoch 510, val loss: 0.8886246085166931
Epoch 520, training loss: 13.212343215942383 = 0.5496277809143066 + 2.0 * 6.331357479095459
Epoch 520, val loss: 0.8812621831893921
Epoch 530, training loss: 13.210719108581543 = 0.5303590297698975 + 2.0 * 6.340179920196533
Epoch 530, val loss: 0.8747596740722656
Epoch 540, training loss: 13.171102523803711 = 0.5119617581367493 + 2.0 * 6.329570293426514
Epoch 540, val loss: 0.8689846992492676
Epoch 550, training loss: 13.15001392364502 = 0.4942426085472107 + 2.0 * 6.327885627746582
Epoch 550, val loss: 0.8641588687896729
Epoch 560, training loss: 13.124175071716309 = 0.47714903950691223 + 2.0 * 6.323513031005859
Epoch 560, val loss: 0.8600077629089355
Epoch 570, training loss: 13.107839584350586 = 0.4605751037597656 + 2.0 * 6.32363224029541
Epoch 570, val loss: 0.8564642071723938
Epoch 580, training loss: 13.094094276428223 = 0.44451114535331726 + 2.0 * 6.324791431427002
Epoch 580, val loss: 0.853572428226471
Epoch 590, training loss: 13.078179359436035 = 0.42904379963874817 + 2.0 * 6.324567794799805
Epoch 590, val loss: 0.8512330055236816
Epoch 600, training loss: 13.050989151000977 = 0.4141753613948822 + 2.0 * 6.31840705871582
Epoch 600, val loss: 0.8494290113449097
Epoch 610, training loss: 13.030596733093262 = 0.39977630972862244 + 2.0 * 6.315410137176514
Epoch 610, val loss: 0.8482589721679688
Epoch 620, training loss: 13.014581680297852 = 0.3857846260070801 + 2.0 * 6.314398288726807
Epoch 620, val loss: 0.8475260138511658
Epoch 630, training loss: 13.00100326538086 = 0.37215518951416016 + 2.0 * 6.31442403793335
Epoch 630, val loss: 0.8472645282745361
Epoch 640, training loss: 12.981931686401367 = 0.3589634895324707 + 2.0 * 6.311484336853027
Epoch 640, val loss: 0.847331166267395
Epoch 650, training loss: 12.972063064575195 = 0.34620359539985657 + 2.0 * 6.312929630279541
Epoch 650, val loss: 0.8479539752006531
Epoch 660, training loss: 12.951977729797363 = 0.3338642716407776 + 2.0 * 6.309056758880615
Epoch 660, val loss: 0.8488421440124512
Epoch 670, training loss: 12.936712265014648 = 0.3218227028846741 + 2.0 * 6.3074445724487305
Epoch 670, val loss: 0.8501204252243042
Epoch 680, training loss: 12.934013366699219 = 0.3101281225681305 + 2.0 * 6.3119425773620605
Epoch 680, val loss: 0.8516733050346375
Epoch 690, training loss: 12.907754898071289 = 0.2987985610961914 + 2.0 * 6.304478168487549
Epoch 690, val loss: 0.8535287976264954
Epoch 700, training loss: 12.920389175415039 = 0.28780341148376465 + 2.0 * 6.316292762756348
Epoch 700, val loss: 0.8557132482528687
Epoch 710, training loss: 12.88615894317627 = 0.27724137902259827 + 2.0 * 6.3044586181640625
Epoch 710, val loss: 0.8581017851829529
Epoch 720, training loss: 12.869823455810547 = 0.2670224606990814 + 2.0 * 6.301400661468506
Epoch 720, val loss: 0.860893726348877
Epoch 730, training loss: 12.857515335083008 = 0.257092148065567 + 2.0 * 6.300211429595947
Epoch 730, val loss: 0.8639109134674072
Epoch 740, training loss: 12.857059478759766 = 0.24744504690170288 + 2.0 * 6.304807186126709
Epoch 740, val loss: 0.867170512676239
Epoch 750, training loss: 12.845952033996582 = 0.2380993515253067 + 2.0 * 6.303926467895508
Epoch 750, val loss: 0.8707126975059509
Epoch 760, training loss: 12.82167911529541 = 0.22914662957191467 + 2.0 * 6.296266078948975
Epoch 760, val loss: 0.8745979070663452
Epoch 770, training loss: 12.812362670898438 = 0.22046756744384766 + 2.0 * 6.295947551727295
Epoch 770, val loss: 0.8785883188247681
Epoch 780, training loss: 12.804878234863281 = 0.21204830706119537 + 2.0 * 6.296414852142334
Epoch 780, val loss: 0.8829274773597717
Epoch 790, training loss: 12.799324989318848 = 0.20392754673957825 + 2.0 * 6.297698497772217
Epoch 790, val loss: 0.8873006701469421
Epoch 800, training loss: 12.781983375549316 = 0.19616355001926422 + 2.0 * 6.292910099029541
Epoch 800, val loss: 0.892142653465271
Epoch 810, training loss: 12.771408081054688 = 0.18867050111293793 + 2.0 * 6.2913689613342285
Epoch 810, val loss: 0.8971067667007446
Epoch 820, training loss: 12.780467987060547 = 0.18141795694828033 + 2.0 * 6.299524784088135
Epoch 820, val loss: 0.9022424817085266
Epoch 830, training loss: 12.762279510498047 = 0.17450454831123352 + 2.0 * 6.293887615203857
Epoch 830, val loss: 0.9075214862823486
Epoch 840, training loss: 12.746893882751465 = 0.16781553626060486 + 2.0 * 6.289539337158203
Epoch 840, val loss: 0.91313636302948
Epoch 850, training loss: 12.737273216247559 = 0.16139908134937286 + 2.0 * 6.287937164306641
Epoch 850, val loss: 0.9188176989555359
Epoch 860, training loss: 12.748932838439941 = 0.1552000194787979 + 2.0 * 6.296866416931152
Epoch 860, val loss: 0.9247163534164429
Epoch 870, training loss: 12.73095989227295 = 0.14928673207759857 + 2.0 * 6.290836811065674
Epoch 870, val loss: 0.9306662082672119
Epoch 880, training loss: 12.718955039978027 = 0.14362065494060516 + 2.0 * 6.287667274475098
Epoch 880, val loss: 0.9368798732757568
Epoch 890, training loss: 12.707769393920898 = 0.13818241655826569 + 2.0 * 6.284793376922607
Epoch 890, val loss: 0.9430302381515503
Epoch 900, training loss: 12.700003623962402 = 0.13296851515769958 + 2.0 * 6.283517360687256
Epoch 900, val loss: 0.949501097202301
Epoch 910, training loss: 12.692991256713867 = 0.12795980274677277 + 2.0 * 6.282515525817871
Epoch 910, val loss: 0.955954909324646
Epoch 920, training loss: 12.707548141479492 = 0.12313318252563477 + 2.0 * 6.292207717895508
Epoch 920, val loss: 0.9625086188316345
Epoch 930, training loss: 12.68966007232666 = 0.11855695396661758 + 2.0 * 6.28555154800415
Epoch 930, val loss: 0.9690744876861572
Epoch 940, training loss: 12.676395416259766 = 0.11416421830654144 + 2.0 * 6.281115531921387
Epoch 940, val loss: 0.975788950920105
Epoch 950, training loss: 12.669211387634277 = 0.10995424538850784 + 2.0 * 6.279628753662109
Epoch 950, val loss: 0.9825976490974426
Epoch 960, training loss: 12.662944793701172 = 0.10590340942144394 + 2.0 * 6.278520584106445
Epoch 960, val loss: 0.9894925951957703
Epoch 970, training loss: 12.689249038696289 = 0.10204122215509415 + 2.0 * 6.293603897094727
Epoch 970, val loss: 0.9963391423225403
Epoch 980, training loss: 12.655213356018066 = 0.09833015501499176 + 2.0 * 6.278441429138184
Epoch 980, val loss: 1.0032923221588135
Epoch 990, training loss: 12.647838592529297 = 0.09481105953454971 + 2.0 * 6.276513576507568
Epoch 990, val loss: 1.0103387832641602
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7074074074074075
0.7928307854507117
The final CL Acc:0.71852, 0.02978, The final GNN Acc:0.79758, 0.00411
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13202])
remove edge: torch.Size([2, 8040])
updated graph: torch.Size([2, 10686])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 19.15127944946289 = 1.957640528678894 + 2.0 * 8.596819877624512
Epoch 0, val loss: 1.962446928024292
Epoch 10, training loss: 19.139976501464844 = 1.9471604824066162 + 2.0 * 8.596407890319824
Epoch 10, val loss: 1.9515416622161865
Epoch 20, training loss: 19.120208740234375 = 1.9340827465057373 + 2.0 * 8.593063354492188
Epoch 20, val loss: 1.9377481937408447
Epoch 30, training loss: 19.052156448364258 = 1.916094183921814 + 2.0 * 8.568031311035156
Epoch 30, val loss: 1.918595790863037
Epoch 40, training loss: 18.690305709838867 = 1.8939028978347778 + 2.0 * 8.398200988769531
Epoch 40, val loss: 1.8957325220108032
Epoch 50, training loss: 17.330944061279297 = 1.8697102069854736 + 2.0 * 7.730617046356201
Epoch 50, val loss: 1.8710625171661377
Epoch 60, training loss: 16.54285430908203 = 1.8517296314239502 + 2.0 * 7.34556245803833
Epoch 60, val loss: 1.8544721603393555
Epoch 70, training loss: 15.929492950439453 = 1.8383854627609253 + 2.0 * 7.045553684234619
Epoch 70, val loss: 1.8410577774047852
Epoch 80, training loss: 15.574227333068848 = 1.8253757953643799 + 2.0 * 6.874425888061523
Epoch 80, val loss: 1.8281464576721191
Epoch 90, training loss: 15.403695106506348 = 1.8121364116668701 + 2.0 * 6.795779228210449
Epoch 90, val loss: 1.8147622346878052
Epoch 100, training loss: 15.274450302124023 = 1.7992584705352783 + 2.0 * 6.737596035003662
Epoch 100, val loss: 1.8021812438964844
Epoch 110, training loss: 15.142485618591309 = 1.7874897718429565 + 2.0 * 6.677497863769531
Epoch 110, val loss: 1.79104483127594
Epoch 120, training loss: 15.029356002807617 = 1.7767043113708496 + 2.0 * 6.626325607299805
Epoch 120, val loss: 1.7808127403259277
Epoch 130, training loss: 14.939613342285156 = 1.7659480571746826 + 2.0 * 6.586832523345947
Epoch 130, val loss: 1.7705365419387817
Epoch 140, training loss: 14.866631507873535 = 1.7549599409103394 + 2.0 * 6.555835723876953
Epoch 140, val loss: 1.7602593898773193
Epoch 150, training loss: 14.805521011352539 = 1.7432175874710083 + 2.0 * 6.53115177154541
Epoch 150, val loss: 1.7496542930603027
Epoch 160, training loss: 14.75224781036377 = 1.7303297519683838 + 2.0 * 6.510959148406982
Epoch 160, val loss: 1.7383549213409424
Epoch 170, training loss: 14.702885627746582 = 1.7160297632217407 + 2.0 * 6.493427753448486
Epoch 170, val loss: 1.7261099815368652
Epoch 180, training loss: 14.655987739562988 = 1.6999940872192383 + 2.0 * 6.477996826171875
Epoch 180, val loss: 1.7125388383865356
Epoch 190, training loss: 14.6152982711792 = 1.6819789409637451 + 2.0 * 6.4666595458984375
Epoch 190, val loss: 1.6974174976348877
Epoch 200, training loss: 14.569379806518555 = 1.6618324518203735 + 2.0 * 6.453773498535156
Epoch 200, val loss: 1.6806071996688843
Epoch 210, training loss: 14.525124549865723 = 1.6390522718429565 + 2.0 * 6.443036079406738
Epoch 210, val loss: 1.6617027521133423
Epoch 220, training loss: 14.48039436340332 = 1.6132395267486572 + 2.0 * 6.433577537536621
Epoch 220, val loss: 1.6402398347854614
Epoch 230, training loss: 14.433972358703613 = 1.5842593908309937 + 2.0 * 6.424856662750244
Epoch 230, val loss: 1.616326093673706
Epoch 240, training loss: 14.387846946716309 = 1.5518081188201904 + 2.0 * 6.4180192947387695
Epoch 240, val loss: 1.5895699262619019
Epoch 250, training loss: 14.336200714111328 = 1.5159775018692017 + 2.0 * 6.410111427307129
Epoch 250, val loss: 1.560182809829712
Epoch 260, training loss: 14.281157493591309 = 1.4765880107879639 + 2.0 * 6.402284622192383
Epoch 260, val loss: 1.5280063152313232
Epoch 270, training loss: 14.238119125366211 = 1.4335527420043945 + 2.0 * 6.402283191680908
Epoch 270, val loss: 1.4930319786071777
Epoch 280, training loss: 14.171948432922363 = 1.3879116773605347 + 2.0 * 6.3920183181762695
Epoch 280, val loss: 1.4562047719955444
Epoch 290, training loss: 14.110807418823242 = 1.3398325443267822 + 2.0 * 6.3854875564575195
Epoch 290, val loss: 1.4177131652832031
Epoch 300, training loss: 14.057554244995117 = 1.2895442247390747 + 2.0 * 6.384005069732666
Epoch 300, val loss: 1.3777107000350952
Epoch 310, training loss: 13.990049362182617 = 1.2379783391952515 + 2.0 * 6.376035690307617
Epoch 310, val loss: 1.336852788925171
Epoch 320, training loss: 13.929559707641602 = 1.1857165098190308 + 2.0 * 6.371921539306641
Epoch 320, val loss: 1.2956621646881104
Epoch 330, training loss: 13.873858451843262 = 1.133921504020691 + 2.0 * 6.369968414306641
Epoch 330, val loss: 1.2548184394836426
Epoch 340, training loss: 13.812453269958496 = 1.0831950902938843 + 2.0 * 6.36462926864624
Epoch 340, val loss: 1.2150474786758423
Epoch 350, training loss: 13.759166717529297 = 1.0335360765457153 + 2.0 * 6.3628153800964355
Epoch 350, val loss: 1.1762559413909912
Epoch 360, training loss: 13.706015586853027 = 0.9858809113502502 + 2.0 * 6.360067367553711
Epoch 360, val loss: 1.1388514041900635
Epoch 370, training loss: 13.650558471679688 = 0.9403751492500305 + 2.0 * 6.355091571807861
Epoch 370, val loss: 1.1034053564071655
Epoch 380, training loss: 13.60540771484375 = 0.8971657752990723 + 2.0 * 6.35412073135376
Epoch 380, val loss: 1.0698891878128052
Epoch 390, training loss: 13.555062294006348 = 0.8565715551376343 + 2.0 * 6.349245548248291
Epoch 390, val loss: 1.0385444164276123
Epoch 400, training loss: 13.51539421081543 = 0.8184172511100769 + 2.0 * 6.3484883308410645
Epoch 400, val loss: 1.0093597173690796
Epoch 410, training loss: 13.47593879699707 = 0.7830410003662109 + 2.0 * 6.34644889831543
Epoch 410, val loss: 0.9825763702392578
Epoch 420, training loss: 13.433343887329102 = 0.750244677066803 + 2.0 * 6.341549396514893
Epoch 420, val loss: 0.958228349685669
Epoch 430, training loss: 13.396310806274414 = 0.7194309830665588 + 2.0 * 6.33843994140625
Epoch 430, val loss: 0.9358065128326416
Epoch 440, training loss: 13.365361213684082 = 0.6902077794075012 + 2.0 * 6.337576866149902
Epoch 440, val loss: 0.9150886535644531
Epoch 450, training loss: 13.346447944641113 = 0.6628116369247437 + 2.0 * 6.341818332672119
Epoch 450, val loss: 0.8958855271339417
Epoch 460, training loss: 13.301607131958008 = 0.6368997693061829 + 2.0 * 6.332353591918945
Epoch 460, val loss: 0.8784506916999817
Epoch 470, training loss: 13.271757125854492 = 0.6120597720146179 + 2.0 * 6.329848766326904
Epoch 470, val loss: 0.8622165322303772
Epoch 480, training loss: 13.242011070251465 = 0.5879870653152466 + 2.0 * 6.327012062072754
Epoch 480, val loss: 0.8469443321228027
Epoch 490, training loss: 13.23401165008545 = 0.5646266341209412 + 2.0 * 6.334692478179932
Epoch 490, val loss: 0.8325012922286987
Epoch 500, training loss: 13.198451042175293 = 0.5421053767204285 + 2.0 * 6.32817268371582
Epoch 500, val loss: 0.8189676403999329
Epoch 510, training loss: 13.164166450500488 = 0.5203421711921692 + 2.0 * 6.3219122886657715
Epoch 510, val loss: 0.8062005043029785
Epoch 520, training loss: 13.138227462768555 = 0.4991461932659149 + 2.0 * 6.319540500640869
Epoch 520, val loss: 0.7941747903823853
Epoch 530, training loss: 13.12057113647461 = 0.4784579277038574 + 2.0 * 6.321056842803955
Epoch 530, val loss: 0.7828186750411987
Epoch 540, training loss: 13.09766674041748 = 0.4582982361316681 + 2.0 * 6.319684028625488
Epoch 540, val loss: 0.7721969485282898
Epoch 550, training loss: 13.068537712097168 = 0.4387907385826111 + 2.0 * 6.314873695373535
Epoch 550, val loss: 0.762154221534729
Epoch 560, training loss: 13.047138214111328 = 0.4198133051395416 + 2.0 * 6.313662528991699
Epoch 560, val loss: 0.752655565738678
Epoch 570, training loss: 13.028822898864746 = 0.40132012963294983 + 2.0 * 6.313751220703125
Epoch 570, val loss: 0.7437574863433838
Epoch 580, training loss: 13.00408935546875 = 0.3833533525466919 + 2.0 * 6.310368061065674
Epoch 580, val loss: 0.7353948950767517
Epoch 590, training loss: 12.981334686279297 = 0.36583784222602844 + 2.0 * 6.307748317718506
Epoch 590, val loss: 0.7274996638298035
Epoch 600, training loss: 12.973725318908691 = 0.34874972701072693 + 2.0 * 6.312487602233887
Epoch 600, val loss: 0.7200913429260254
Epoch 610, training loss: 12.944328308105469 = 0.33221524953842163 + 2.0 * 6.306056499481201
Epoch 610, val loss: 0.7131341695785522
Epoch 620, training loss: 12.923972129821777 = 0.3161669373512268 + 2.0 * 6.303902626037598
Epoch 620, val loss: 0.7065075039863586
Epoch 630, training loss: 12.904783248901367 = 0.30053019523620605 + 2.0 * 6.302126407623291
Epoch 630, val loss: 0.700299859046936
Epoch 640, training loss: 12.909685134887695 = 0.2852764427661896 + 2.0 * 6.312204360961914
Epoch 640, val loss: 0.6945773363113403
Epoch 650, training loss: 12.86829662322998 = 0.27067387104034424 + 2.0 * 6.298811435699463
Epoch 650, val loss: 0.6891176104545593
Epoch 660, training loss: 12.852293968200684 = 0.2565000355243683 + 2.0 * 6.297896862030029
Epoch 660, val loss: 0.683938205242157
Epoch 670, training loss: 12.839508056640625 = 0.24279245734214783 + 2.0 * 6.298357963562012
Epoch 670, val loss: 0.6792160868644714
Epoch 680, training loss: 12.826643943786621 = 0.22965110838413239 + 2.0 * 6.298496246337891
Epoch 680, val loss: 0.6751909255981445
Epoch 690, training loss: 12.805709838867188 = 0.21712443232536316 + 2.0 * 6.29429292678833
Epoch 690, val loss: 0.6712334752082825
Epoch 700, training loss: 12.799766540527344 = 0.20516949892044067 + 2.0 * 6.297298431396484
Epoch 700, val loss: 0.6677775382995605
Epoch 710, training loss: 12.778580665588379 = 0.19383078813552856 + 2.0 * 6.292375087738037
Epoch 710, val loss: 0.6649168729782104
Epoch 720, training loss: 12.763995170593262 = 0.1830640584230423 + 2.0 * 6.290465354919434
Epoch 720, val loss: 0.662499725818634
Epoch 730, training loss: 12.773818969726562 = 0.1728752702474594 + 2.0 * 6.300471782684326
Epoch 730, val loss: 0.6606968641281128
Epoch 740, training loss: 12.74242877960205 = 0.16343039274215698 + 2.0 * 6.289499282836914
Epoch 740, val loss: 0.6593681573867798
Epoch 750, training loss: 12.730159759521484 = 0.1545364111661911 + 2.0 * 6.287811756134033
Epoch 750, val loss: 0.6583447456359863
Epoch 760, training loss: 12.72432804107666 = 0.14619195461273193 + 2.0 * 6.289068222045898
Epoch 760, val loss: 0.6579366326332092
Epoch 770, training loss: 12.711263656616211 = 0.13841980695724487 + 2.0 * 6.286421775817871
Epoch 770, val loss: 0.6581819653511047
Epoch 780, training loss: 12.700997352600098 = 0.1311488300561905 + 2.0 * 6.284924030303955
Epoch 780, val loss: 0.6585822701454163
Epoch 790, training loss: 12.690279006958008 = 0.12434040009975433 + 2.0 * 6.2829694747924805
Epoch 790, val loss: 0.6594554781913757
Epoch 800, training loss: 12.7063570022583 = 0.1179979220032692 + 2.0 * 6.294179439544678
Epoch 800, val loss: 0.6608582139015198
Epoch 810, training loss: 12.67866325378418 = 0.11208150535821915 + 2.0 * 6.283290863037109
Epoch 810, val loss: 0.6626001000404358
Epoch 820, training loss: 12.666762351989746 = 0.10657905787229538 + 2.0 * 6.280091762542725
Epoch 820, val loss: 0.6643399000167847
Epoch 830, training loss: 12.660415649414062 = 0.10142405331134796 + 2.0 * 6.279495716094971
Epoch 830, val loss: 0.6665915846824646
Epoch 840, training loss: 12.6685209274292 = 0.09658927470445633 + 2.0 * 6.285965919494629
Epoch 840, val loss: 0.6692929863929749
Epoch 850, training loss: 12.650684356689453 = 0.09208078682422638 + 2.0 * 6.279301643371582
Epoch 850, val loss: 0.6720941066741943
Epoch 860, training loss: 12.640814781188965 = 0.08783825486898422 + 2.0 * 6.276488304138184
Epoch 860, val loss: 0.6750361323356628
Epoch 870, training loss: 12.637406349182129 = 0.08384711295366287 + 2.0 * 6.276779651641846
Epoch 870, val loss: 0.6783007383346558
Epoch 880, training loss: 12.634271621704102 = 0.0801120474934578 + 2.0 * 6.2770795822143555
Epoch 880, val loss: 0.6818230152130127
Epoch 890, training loss: 12.631590843200684 = 0.07662650942802429 + 2.0 * 6.277482032775879
Epoch 890, val loss: 0.6853630542755127
Epoch 900, training loss: 12.61918830871582 = 0.07334372401237488 + 2.0 * 6.272922515869141
Epoch 900, val loss: 0.6888256072998047
Epoch 910, training loss: 12.617962837219238 = 0.07023783773183823 + 2.0 * 6.273862361907959
Epoch 910, val loss: 0.6926853656768799
Epoch 920, training loss: 12.616650581359863 = 0.06729515641927719 + 2.0 * 6.274677753448486
Epoch 920, val loss: 0.6967874765396118
Epoch 930, training loss: 12.611749649047852 = 0.06453908979892731 + 2.0 * 6.2736053466796875
Epoch 930, val loss: 0.7008605003356934
Epoch 940, training loss: 12.603096961975098 = 0.06194460019469261 + 2.0 * 6.270576000213623
Epoch 940, val loss: 0.70488440990448
Epoch 950, training loss: 12.599662780761719 = 0.059472329914569855 + 2.0 * 6.270095348358154
Epoch 950, val loss: 0.709045946598053
Epoch 960, training loss: 12.602252006530762 = 0.05713450536131859 + 2.0 * 6.272558689117432
Epoch 960, val loss: 0.7133725881576538
Epoch 970, training loss: 12.598613739013672 = 0.05493291839957237 + 2.0 * 6.271840572357178
Epoch 970, val loss: 0.7177539467811584
Epoch 980, training loss: 12.588438034057617 = 0.05283757299184799 + 2.0 * 6.267800331115723
Epoch 980, val loss: 0.722038984298706
Epoch 990, training loss: 12.584217071533203 = 0.050860997289419174 + 2.0 * 6.2666778564453125
Epoch 990, val loss: 0.7263255715370178
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8148148148148149
0.8376383763837639
=== training gcn model ===
Epoch 0, training loss: 19.139196395874023 = 1.9455947875976562 + 2.0 * 8.596800804138184
Epoch 0, val loss: 1.9363667964935303
Epoch 10, training loss: 19.1276912689209 = 1.9351369142532349 + 2.0 * 8.596277236938477
Epoch 10, val loss: 1.9266014099121094
Epoch 20, training loss: 19.105642318725586 = 1.9221211671829224 + 2.0 * 8.591760635375977
Epoch 20, val loss: 1.9140923023223877
Epoch 30, training loss: 19.018552780151367 = 1.9047260284423828 + 2.0 * 8.556913375854492
Epoch 30, val loss: 1.8973171710968018
Epoch 40, training loss: 18.514610290527344 = 1.8844118118286133 + 2.0 * 8.315098762512207
Epoch 40, val loss: 1.8785059452056885
Epoch 50, training loss: 17.107017517089844 = 1.8627218008041382 + 2.0 * 7.622147560119629
Epoch 50, val loss: 1.8588804006576538
Epoch 60, training loss: 16.4020938873291 = 1.8462046384811401 + 2.0 * 7.277944564819336
Epoch 60, val loss: 1.8446985483169556
Epoch 70, training loss: 15.936790466308594 = 1.8322358131408691 + 2.0 * 7.052277088165283
Epoch 70, val loss: 1.83194899559021
Epoch 80, training loss: 15.668978691101074 = 1.8176010847091675 + 2.0 * 6.925688743591309
Epoch 80, val loss: 1.81852388381958
Epoch 90, training loss: 15.457559585571289 = 1.8033512830734253 + 2.0 * 6.827104091644287
Epoch 90, val loss: 1.8054986000061035
Epoch 100, training loss: 15.28452205657959 = 1.7918788194656372 + 2.0 * 6.746321678161621
Epoch 100, val loss: 1.7948393821716309
Epoch 110, training loss: 15.13809871673584 = 1.7816054821014404 + 2.0 * 6.67824649810791
Epoch 110, val loss: 1.7851364612579346
Epoch 120, training loss: 15.020319938659668 = 1.770572304725647 + 2.0 * 6.624873638153076
Epoch 120, val loss: 1.7748652696609497
Epoch 130, training loss: 14.929886817932129 = 1.7585372924804688 + 2.0 * 6.58567476272583
Epoch 130, val loss: 1.7640711069107056
Epoch 140, training loss: 14.853381156921387 = 1.7455683946609497 + 2.0 * 6.553906440734863
Epoch 140, val loss: 1.7525608539581299
Epoch 150, training loss: 14.785393714904785 = 1.7311983108520508 + 2.0 * 6.527097702026367
Epoch 150, val loss: 1.740041732788086
Epoch 160, training loss: 14.729947090148926 = 1.715084433555603 + 2.0 * 6.507431507110596
Epoch 160, val loss: 1.7261403799057007
Epoch 170, training loss: 14.671760559082031 = 1.6970329284667969 + 2.0 * 6.487363815307617
Epoch 170, val loss: 1.710717797279358
Epoch 180, training loss: 14.61867904663086 = 1.6766310930252075 + 2.0 * 6.471024036407471
Epoch 180, val loss: 1.6935205459594727
Epoch 190, training loss: 14.5658597946167 = 1.6536182165145874 + 2.0 * 6.45612096786499
Epoch 190, val loss: 1.674279808998108
Epoch 200, training loss: 14.51557445526123 = 1.6277004480361938 + 2.0 * 6.443936824798584
Epoch 200, val loss: 1.6527589559555054
Epoch 210, training loss: 14.464594841003418 = 1.5984634160995483 + 2.0 * 6.433065891265869
Epoch 210, val loss: 1.6286271810531616
Epoch 220, training loss: 14.415432929992676 = 1.565839409828186 + 2.0 * 6.4247965812683105
Epoch 220, val loss: 1.60189950466156
Epoch 230, training loss: 14.358128547668457 = 1.530011773109436 + 2.0 * 6.414058208465576
Epoch 230, val loss: 1.5726028680801392
Epoch 240, training loss: 14.305054664611816 = 1.4908647537231445 + 2.0 * 6.407094955444336
Epoch 240, val loss: 1.5408227443695068
Epoch 250, training loss: 14.256635665893555 = 1.4493460655212402 + 2.0 * 6.403644561767578
Epoch 250, val loss: 1.5072671175003052
Epoch 260, training loss: 14.192991256713867 = 1.4060057401657104 + 2.0 * 6.393492698669434
Epoch 260, val loss: 1.4724984169006348
Epoch 270, training loss: 14.136713027954102 = 1.3613038063049316 + 2.0 * 6.387704372406006
Epoch 270, val loss: 1.4368057250976562
Epoch 280, training loss: 14.08003044128418 = 1.3155219554901123 + 2.0 * 6.382254123687744
Epoch 280, val loss: 1.4005800485610962
Epoch 290, training loss: 14.023838996887207 = 1.269269347190857 + 2.0 * 6.377285003662109
Epoch 290, val loss: 1.3642710447311401
Epoch 300, training loss: 13.98972225189209 = 1.223373293876648 + 2.0 * 6.383174419403076
Epoch 300, val loss: 1.3287068605422974
Epoch 310, training loss: 13.919327735900879 = 1.1796528100967407 + 2.0 * 6.369837284088135
Epoch 310, val loss: 1.2950383424758911
Epoch 320, training loss: 13.868897438049316 = 1.1374658346176147 + 2.0 * 6.365715980529785
Epoch 320, val loss: 1.2631171941757202
Epoch 330, training loss: 13.822558403015137 = 1.0969547033309937 + 2.0 * 6.362802028656006
Epoch 330, val loss: 1.2329015731811523
Epoch 340, training loss: 13.780439376831055 = 1.058371901512146 + 2.0 * 6.361033916473389
Epoch 340, val loss: 1.2043681144714355
Epoch 350, training loss: 13.7295560836792 = 1.0214723348617554 + 2.0 * 6.354042053222656
Epoch 350, val loss: 1.1773242950439453
Epoch 360, training loss: 13.704659461975098 = 0.9860826134681702 + 2.0 * 6.359288215637207
Epoch 360, val loss: 1.1514676809310913
Epoch 370, training loss: 13.648101806640625 = 0.9521408677101135 + 2.0 * 6.347980499267578
Epoch 370, val loss: 1.1268914937973022
Epoch 380, training loss: 13.608109474182129 = 0.9192245602607727 + 2.0 * 6.344442367553711
Epoch 380, val loss: 1.1031233072280884
Epoch 390, training loss: 13.569464683532715 = 0.8869292736053467 + 2.0 * 6.3412675857543945
Epoch 390, val loss: 1.079839825630188
Epoch 400, training loss: 13.536046028137207 = 0.8550432920455933 + 2.0 * 6.340501308441162
Epoch 400, val loss: 1.0568970441818237
Epoch 410, training loss: 13.503453254699707 = 0.8236852288246155 + 2.0 * 6.339883804321289
Epoch 410, val loss: 1.034595251083374
Epoch 420, training loss: 13.464015007019043 = 0.7931472659111023 + 2.0 * 6.3354339599609375
Epoch 420, val loss: 1.0129097700119019
Epoch 430, training loss: 13.426227569580078 = 0.7631377577781677 + 2.0 * 6.331544876098633
Epoch 430, val loss: 0.9919461011886597
Epoch 440, training loss: 13.395439147949219 = 0.733635663986206 + 2.0 * 6.330901622772217
Epoch 440, val loss: 0.9717244505882263
Epoch 450, training loss: 13.370505332946777 = 0.7049975991249084 + 2.0 * 6.332753658294678
Epoch 450, val loss: 0.9524434208869934
Epoch 460, training loss: 13.334037780761719 = 0.6773334741592407 + 2.0 * 6.328351974487305
Epoch 460, val loss: 0.9346168041229248
Epoch 470, training loss: 13.299158096313477 = 0.6506748795509338 + 2.0 * 6.324241638183594
Epoch 470, val loss: 0.9179592132568359
Epoch 480, training loss: 13.266839981079102 = 0.624568521976471 + 2.0 * 6.321135520935059
Epoch 480, val loss: 0.902536153793335
Epoch 490, training loss: 13.240823745727539 = 0.599030077457428 + 2.0 * 6.320896625518799
Epoch 490, val loss: 0.8882297277450562
Epoch 500, training loss: 13.215357780456543 = 0.5741750597953796 + 2.0 * 6.320591449737549
Epoch 500, val loss: 0.8749696016311646
Epoch 510, training loss: 13.18879508972168 = 0.5500247478485107 + 2.0 * 6.319385051727295
Epoch 510, val loss: 0.8628700971603394
Epoch 520, training loss: 13.156551361083984 = 0.5263781547546387 + 2.0 * 6.315086364746094
Epoch 520, val loss: 0.8518279194831848
Epoch 530, training loss: 13.13379192352295 = 0.5031836628913879 + 2.0 * 6.315304279327393
Epoch 530, val loss: 0.8416391015052795
Epoch 540, training loss: 13.102041244506836 = 0.48053818941116333 + 2.0 * 6.310751438140869
Epoch 540, val loss: 0.8322219252586365
Epoch 550, training loss: 13.080185890197754 = 0.45836320519447327 + 2.0 * 6.310911178588867
Epoch 550, val loss: 0.8236603140830994
Epoch 560, training loss: 13.071292877197266 = 0.4367375075817108 + 2.0 * 6.317277908325195
Epoch 560, val loss: 0.8159406185150146
Epoch 570, training loss: 13.030434608459473 = 0.4158696234226227 + 2.0 * 6.307282447814941
Epoch 570, val loss: 0.8090643286705017
Epoch 580, training loss: 13.0086030960083 = 0.3957372307777405 + 2.0 * 6.306432723999023
Epoch 580, val loss: 0.8030843138694763
Epoch 590, training loss: 12.985198020935059 = 0.37620407342910767 + 2.0 * 6.304496765136719
Epoch 590, val loss: 0.7979540824890137
Epoch 600, training loss: 12.971528053283691 = 0.35733142495155334 + 2.0 * 6.307098388671875
Epoch 600, val loss: 0.7936893105506897
Epoch 610, training loss: 12.952792167663574 = 0.33946624398231506 + 2.0 * 6.3066630363464355
Epoch 610, val loss: 0.7901387810707092
Epoch 620, training loss: 12.92442512512207 = 0.32240548729896545 + 2.0 * 6.301009654998779
Epoch 620, val loss: 0.7876567840576172
Epoch 630, training loss: 12.90595531463623 = 0.3062278628349304 + 2.0 * 6.299863815307617
Epoch 630, val loss: 0.7860627770423889
Epoch 640, training loss: 12.911331176757812 = 0.29085585474967957 + 2.0 * 6.310237884521484
Epoch 640, val loss: 0.7852174639701843
Epoch 650, training loss: 12.87485122680664 = 0.27640053629875183 + 2.0 * 6.299225330352783
Epoch 650, val loss: 0.7850564122200012
Epoch 660, training loss: 12.857405662536621 = 0.26276490092277527 + 2.0 * 6.297320365905762
Epoch 660, val loss: 0.7857472896575928
Epoch 670, training loss: 12.839488983154297 = 0.24988777935504913 + 2.0 * 6.294800758361816
Epoch 670, val loss: 0.7870998978614807
Epoch 680, training loss: 12.832573890686035 = 0.23768745362758636 + 2.0 * 6.297443389892578
Epoch 680, val loss: 0.78901207447052
Epoch 690, training loss: 12.816170692443848 = 0.22625558078289032 + 2.0 * 6.294957637786865
Epoch 690, val loss: 0.7913621068000793
Epoch 700, training loss: 12.801797866821289 = 0.21550004184246063 + 2.0 * 6.293148994445801
Epoch 700, val loss: 0.7942675352096558
Epoch 710, training loss: 12.786368370056152 = 0.205392524600029 + 2.0 * 6.290487766265869
Epoch 710, val loss: 0.7976344227790833
Epoch 720, training loss: 12.77820873260498 = 0.19580240547657013 + 2.0 * 6.291203022003174
Epoch 720, val loss: 0.8013685345649719
Epoch 730, training loss: 12.774083137512207 = 0.18678103387355804 + 2.0 * 6.293651103973389
Epoch 730, val loss: 0.8052989840507507
Epoch 740, training loss: 12.754717826843262 = 0.17831707000732422 + 2.0 * 6.288200378417969
Epoch 740, val loss: 0.8095722198486328
Epoch 750, training loss: 12.74478816986084 = 0.1703386753797531 + 2.0 * 6.287224769592285
Epoch 750, val loss: 0.8142240047454834
Epoch 760, training loss: 12.735152244567871 = 0.1627732664346695 + 2.0 * 6.286189556121826
Epoch 760, val loss: 0.8191356658935547
Epoch 770, training loss: 12.731884002685547 = 0.1556025892496109 + 2.0 * 6.288140773773193
Epoch 770, val loss: 0.8242294788360596
Epoch 780, training loss: 12.740409851074219 = 0.14882420003414154 + 2.0 * 6.295793056488037
Epoch 780, val loss: 0.8295335173606873
Epoch 790, training loss: 12.711958885192871 = 0.14240364730358124 + 2.0 * 6.284777641296387
Epoch 790, val loss: 0.8349118828773499
Epoch 800, training loss: 12.70154094696045 = 0.1363699585199356 + 2.0 * 6.282585620880127
Epoch 800, val loss: 0.8404962420463562
Epoch 810, training loss: 12.691472053527832 = 0.13062052428722382 + 2.0 * 6.280425548553467
Epoch 810, val loss: 0.8462982177734375
Epoch 820, training loss: 12.688061714172363 = 0.12513627111911774 + 2.0 * 6.281462669372559
Epoch 820, val loss: 0.852228581905365
Epoch 830, training loss: 12.681390762329102 = 0.11992662400007248 + 2.0 * 6.280732154846191
Epoch 830, val loss: 0.8581461310386658
Epoch 840, training loss: 12.678637504577637 = 0.11500372737646103 + 2.0 * 6.2818169593811035
Epoch 840, val loss: 0.8641510009765625
Epoch 850, training loss: 12.66486644744873 = 0.11033345758914948 + 2.0 * 6.277266502380371
Epoch 850, val loss: 0.8703732490539551
Epoch 860, training loss: 12.665863990783691 = 0.1058654636144638 + 2.0 * 6.279999256134033
Epoch 860, val loss: 0.8766515851020813
Epoch 870, training loss: 12.654449462890625 = 0.10162422060966492 + 2.0 * 6.276412487030029
Epoch 870, val loss: 0.8828596472740173
Epoch 880, training loss: 12.647989273071289 = 0.09759923070669174 + 2.0 * 6.275195121765137
Epoch 880, val loss: 0.8891972899436951
Epoch 890, training loss: 12.642362594604492 = 0.09376965463161469 + 2.0 * 6.274296283721924
Epoch 890, val loss: 0.8956347703933716
Epoch 900, training loss: 12.637743949890137 = 0.09010772407054901 + 2.0 * 6.273818016052246
Epoch 900, val loss: 0.9021422266960144
Epoch 910, training loss: 12.646137237548828 = 0.08661773800849915 + 2.0 * 6.279759883880615
Epoch 910, val loss: 0.9086459875106812
Epoch 920, training loss: 12.630284309387207 = 0.08329687267541885 + 2.0 * 6.273493766784668
Epoch 920, val loss: 0.9150902032852173
Epoch 930, training loss: 12.627269744873047 = 0.08014097064733505 + 2.0 * 6.273564338684082
Epoch 930, val loss: 0.9216006994247437
Epoch 940, training loss: 12.621886253356934 = 0.07713540643453598 + 2.0 * 6.272375583648682
Epoch 940, val loss: 0.9280768632888794
Epoch 950, training loss: 12.629968643188477 = 0.0742783397436142 + 2.0 * 6.27784538269043
Epoch 950, val loss: 0.9344808459281921
Epoch 960, training loss: 12.614786148071289 = 0.07153929769992828 + 2.0 * 6.271623611450195
Epoch 960, val loss: 0.9409424662590027
Epoch 970, training loss: 12.60802173614502 = 0.06894823908805847 + 2.0 * 6.269536972045898
Epoch 970, val loss: 0.9474281072616577
Epoch 980, training loss: 12.609257698059082 = 0.06647078692913055 + 2.0 * 6.271393299102783
Epoch 980, val loss: 0.9539427757263184
Epoch 990, training loss: 12.604897499084473 = 0.0641099214553833 + 2.0 * 6.2703938484191895
Epoch 990, val loss: 0.960375189781189
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8074074074074075
0.8444913020558777
=== training gcn model ===
Epoch 0, training loss: 19.129350662231445 = 1.9356982707977295 + 2.0 * 8.596826553344727
Epoch 0, val loss: 1.9345463514328003
Epoch 10, training loss: 19.118471145629883 = 1.925665020942688 + 2.0 * 8.596403121948242
Epoch 10, val loss: 1.9239696264266968
Epoch 20, training loss: 19.09869384765625 = 1.9133262634277344 + 2.0 * 8.592683792114258
Epoch 20, val loss: 1.9107071161270142
Epoch 30, training loss: 19.02247428894043 = 1.897158145904541 + 2.0 * 8.562658309936523
Epoch 30, val loss: 1.8932658433914185
Epoch 40, training loss: 18.599905014038086 = 1.8781511783599854 + 2.0 * 8.36087703704834
Epoch 40, val loss: 1.8735277652740479
Epoch 50, training loss: 17.486019134521484 = 1.8591464757919312 + 2.0 * 7.813436031341553
Epoch 50, val loss: 1.8547618389129639
Epoch 60, training loss: 16.501705169677734 = 1.8456536531448364 + 2.0 * 7.3280253410339355
Epoch 60, val loss: 1.8428508043289185
Epoch 70, training loss: 15.893430709838867 = 1.8357771635055542 + 2.0 * 7.028826713562012
Epoch 70, val loss: 1.833670973777771
Epoch 80, training loss: 15.600590705871582 = 1.8258715867996216 + 2.0 * 6.887359619140625
Epoch 80, val loss: 1.8242031335830688
Epoch 90, training loss: 15.397923469543457 = 1.812944769859314 + 2.0 * 6.792489528656006
Epoch 90, val loss: 1.8121616840362549
Epoch 100, training loss: 15.237417221069336 = 1.8003251552581787 + 2.0 * 6.718545913696289
Epoch 100, val loss: 1.8007888793945312
Epoch 110, training loss: 15.094955444335938 = 1.7896878719329834 + 2.0 * 6.6526336669921875
Epoch 110, val loss: 1.7910352945327759
Epoch 120, training loss: 14.995475769042969 = 1.779312252998352 + 2.0 * 6.608081817626953
Epoch 120, val loss: 1.7812035083770752
Epoch 130, training loss: 14.925114631652832 = 1.7675435543060303 + 2.0 * 6.578785419464111
Epoch 130, val loss: 1.770181655883789
Epoch 140, training loss: 14.861248970031738 = 1.7544708251953125 + 2.0 * 6.553389072418213
Epoch 140, val loss: 1.758072853088379
Epoch 150, training loss: 14.80146312713623 = 1.7403154373168945 + 2.0 * 6.530573844909668
Epoch 150, val loss: 1.7451094388961792
Epoch 160, training loss: 14.748417854309082 = 1.7246277332305908 + 2.0 * 6.511895179748535
Epoch 160, val loss: 1.730969786643982
Epoch 170, training loss: 14.6990327835083 = 1.7072412967681885 + 2.0 * 6.495895862579346
Epoch 170, val loss: 1.7154487371444702
Epoch 180, training loss: 14.644771575927734 = 1.6876078844070435 + 2.0 * 6.47858190536499
Epoch 180, val loss: 1.6980704069137573
Epoch 190, training loss: 14.594934463500977 = 1.6652207374572754 + 2.0 * 6.4648566246032715
Epoch 190, val loss: 1.6783472299575806
Epoch 200, training loss: 14.55138874053955 = 1.6395748853683472 + 2.0 * 6.455906867980957
Epoch 200, val loss: 1.6558701992034912
Epoch 210, training loss: 14.499213218688965 = 1.6107298135757446 + 2.0 * 6.444241523742676
Epoch 210, val loss: 1.6305793523788452
Epoch 220, training loss: 14.44478988647461 = 1.578288197517395 + 2.0 * 6.433250904083252
Epoch 220, val loss: 1.6022632122039795
Epoch 230, training loss: 14.390985488891602 = 1.541375756263733 + 2.0 * 6.4248046875
Epoch 230, val loss: 1.5702804327011108
Epoch 240, training loss: 14.33322525024414 = 1.5005186796188354 + 2.0 * 6.416353225708008
Epoch 240, val loss: 1.534899115562439
Epoch 250, training loss: 14.28441333770752 = 1.4554378986358643 + 2.0 * 6.414487838745117
Epoch 250, val loss: 1.4962692260742188
Epoch 260, training loss: 14.218146324157715 = 1.4083325862884521 + 2.0 * 6.404906749725342
Epoch 260, val loss: 1.4557241201400757
Epoch 270, training loss: 14.152727127075195 = 1.3593260049819946 + 2.0 * 6.396700382232666
Epoch 270, val loss: 1.413628101348877
Epoch 280, training loss: 14.087799072265625 = 1.308681607246399 + 2.0 * 6.389558792114258
Epoch 280, val loss: 1.3704495429992676
Epoch 290, training loss: 14.02505111694336 = 1.2573652267456055 + 2.0 * 6.383842945098877
Epoch 290, val loss: 1.3268641233444214
Epoch 300, training loss: 13.972299575805664 = 1.2066731452941895 + 2.0 * 6.382812976837158
Epoch 300, val loss: 1.2840604782104492
Epoch 310, training loss: 13.90829086303711 = 1.1579948663711548 + 2.0 * 6.375147819519043
Epoch 310, val loss: 1.2433593273162842
Epoch 320, training loss: 13.850872039794922 = 1.111708402633667 + 2.0 * 6.369581699371338
Epoch 320, val loss: 1.2049630880355835
Epoch 330, training loss: 13.801535606384277 = 1.067745566368103 + 2.0 * 6.3668951988220215
Epoch 330, val loss: 1.1688451766967773
Epoch 340, training loss: 13.753777503967285 = 1.0263313055038452 + 2.0 * 6.363723278045654
Epoch 340, val loss: 1.13533616065979
Epoch 350, training loss: 13.703786849975586 = 0.9874979257583618 + 2.0 * 6.358144283294678
Epoch 350, val loss: 1.1039468050003052
Epoch 360, training loss: 13.661470413208008 = 0.9506441950798035 + 2.0 * 6.35541296005249
Epoch 360, val loss: 1.0743918418884277
Epoch 370, training loss: 13.622478485107422 = 0.9153515696525574 + 2.0 * 6.35356330871582
Epoch 370, val loss: 1.046461582183838
Epoch 380, training loss: 13.582263946533203 = 0.8819078207015991 + 2.0 * 6.350178241729736
Epoch 380, val loss: 1.0198849439620972
Epoch 390, training loss: 13.538836479187012 = 0.8494030237197876 + 2.0 * 6.344716548919678
Epoch 390, val loss: 0.9944861531257629
Epoch 400, training loss: 13.501340866088867 = 0.8177769184112549 + 2.0 * 6.341782093048096
Epoch 400, val loss: 0.969784677028656
Epoch 410, training loss: 13.475091934204102 = 0.7867411375045776 + 2.0 * 6.344175338745117
Epoch 410, val loss: 0.9456602931022644
Epoch 420, training loss: 13.43152904510498 = 0.7566055059432983 + 2.0 * 6.337461948394775
Epoch 420, val loss: 0.9223245978355408
Epoch 430, training loss: 13.397027969360352 = 0.7270064949989319 + 2.0 * 6.335010528564453
Epoch 430, val loss: 0.8997246623039246
Epoch 440, training loss: 13.36800479888916 = 0.6980471014976501 + 2.0 * 6.334979057312012
Epoch 440, val loss: 0.8777996301651001
Epoch 450, training loss: 13.328149795532227 = 0.6700466275215149 + 2.0 * 6.329051494598389
Epoch 450, val loss: 0.8568297028541565
Epoch 460, training loss: 13.298271179199219 = 0.6425981521606445 + 2.0 * 6.327836513519287
Epoch 460, val loss: 0.8366230726242065
Epoch 470, training loss: 13.265721321105957 = 0.6157177090644836 + 2.0 * 6.3250017166137695
Epoch 470, val loss: 0.8172053694725037
Epoch 480, training loss: 13.237526893615723 = 0.5895450115203857 + 2.0 * 6.323990821838379
Epoch 480, val loss: 0.7986357808113098
Epoch 490, training loss: 13.206482887268066 = 0.5639516711235046 + 2.0 * 6.321265697479248
Epoch 490, val loss: 0.7810337543487549
Epoch 500, training loss: 13.1745023727417 = 0.5390645861625671 + 2.0 * 6.317718982696533
Epoch 500, val loss: 0.7641908526420593
Epoch 510, training loss: 13.15957260131836 = 0.5146058797836304 + 2.0 * 6.322483539581299
Epoch 510, val loss: 0.7481582164764404
Epoch 520, training loss: 13.118955612182617 = 0.49068430066108704 + 2.0 * 6.314135551452637
Epoch 520, val loss: 0.7329266667366028
Epoch 530, training loss: 13.1024808883667 = 0.46735748648643494 + 2.0 * 6.317561626434326
Epoch 530, val loss: 0.7186448574066162
Epoch 540, training loss: 13.071016311645508 = 0.4448412358760834 + 2.0 * 6.313087463378906
Epoch 540, val loss: 0.7051295638084412
Epoch 550, training loss: 13.03897476196289 = 0.42287781834602356 + 2.0 * 6.308048248291016
Epoch 550, val loss: 0.6925976872444153
Epoch 560, training loss: 13.021119117736816 = 0.40155211091041565 + 2.0 * 6.309783458709717
Epoch 560, val loss: 0.6809240579605103
Epoch 570, training loss: 12.992159843444824 = 0.3808346092700958 + 2.0 * 6.305662631988525
Epoch 570, val loss: 0.6700793504714966
Epoch 580, training loss: 12.968647956848145 = 0.3609919846057892 + 2.0 * 6.30382776260376
Epoch 580, val loss: 0.6601700186729431
Epoch 590, training loss: 12.945940017700195 = 0.3418271541595459 + 2.0 * 6.302056312561035
Epoch 590, val loss: 0.651210606098175
Epoch 600, training loss: 12.950972557067871 = 0.32356610894203186 + 2.0 * 6.3137030601501465
Epoch 600, val loss: 0.6430475115776062
Epoch 610, training loss: 12.905904769897461 = 0.3060814440250397 + 2.0 * 6.2999114990234375
Epoch 610, val loss: 0.6358626484870911
Epoch 620, training loss: 12.885154724121094 = 0.28963127732276917 + 2.0 * 6.297761917114258
Epoch 620, val loss: 0.6296678185462952
Epoch 630, training loss: 12.866209030151367 = 0.27401503920555115 + 2.0 * 6.2960968017578125
Epoch 630, val loss: 0.6242965459823608
Epoch 640, training loss: 12.866029739379883 = 0.25922030210494995 + 2.0 * 6.303404808044434
Epoch 640, val loss: 0.6197492480278015
Epoch 650, training loss: 12.83170223236084 = 0.24534468352794647 + 2.0 * 6.293178558349609
Epoch 650, val loss: 0.6159413456916809
Epoch 660, training loss: 12.816556930541992 = 0.23229306936264038 + 2.0 * 6.2921319007873535
Epoch 660, val loss: 0.612975001335144
Epoch 670, training loss: 12.828729629516602 = 0.22005489468574524 + 2.0 * 6.304337501525879
Epoch 670, val loss: 0.6107179522514343
Epoch 680, training loss: 12.790165901184082 = 0.2085106372833252 + 2.0 * 6.290827751159668
Epoch 680, val loss: 0.6090759634971619
Epoch 690, training loss: 12.774727821350098 = 0.19782105088233948 + 2.0 * 6.288453578948975
Epoch 690, val loss: 0.6080949902534485
Epoch 700, training loss: 12.761799812316895 = 0.1877719610929489 + 2.0 * 6.287014007568359
Epoch 700, val loss: 0.6077304482460022
Epoch 710, training loss: 12.749982833862305 = 0.17832107841968536 + 2.0 * 6.285830974578857
Epoch 710, val loss: 0.6079034209251404
Epoch 720, training loss: 12.760244369506836 = 0.16944874823093414 + 2.0 * 6.295397758483887
Epoch 720, val loss: 0.6085251569747925
Epoch 730, training loss: 12.730683326721191 = 0.16117450594902039 + 2.0 * 6.284754276275635
Epoch 730, val loss: 0.609484851360321
Epoch 740, training loss: 12.72695255279541 = 0.15342238545417786 + 2.0 * 6.286765098571777
Epoch 740, val loss: 0.6108692288398743
Epoch 750, training loss: 12.712727546691895 = 0.1461968719959259 + 2.0 * 6.283265113830566
Epoch 750, val loss: 0.6125519871711731
Epoch 760, training loss: 12.702454566955566 = 0.1393769085407257 + 2.0 * 6.281538963317871
Epoch 760, val loss: 0.6145434975624084
Epoch 770, training loss: 12.706450462341309 = 0.13295431435108185 + 2.0 * 6.286747932434082
Epoch 770, val loss: 0.6167847514152527
Epoch 780, training loss: 12.689472198486328 = 0.12696397304534912 + 2.0 * 6.281254291534424
Epoch 780, val loss: 0.6192115545272827
Epoch 790, training loss: 12.67908000946045 = 0.12127846479415894 + 2.0 * 6.278900623321533
Epoch 790, val loss: 0.6218293905258179
Epoch 800, training loss: 12.681849479675293 = 0.11592459678649902 + 2.0 * 6.282962322235107
Epoch 800, val loss: 0.6246697902679443
Epoch 810, training loss: 12.665663719177246 = 0.11090191453695297 + 2.0 * 6.27738094329834
Epoch 810, val loss: 0.6275389194488525
Epoch 820, training loss: 12.656906127929688 = 0.10612780600786209 + 2.0 * 6.275389194488525
Epoch 820, val loss: 0.6306676268577576
Epoch 830, training loss: 12.64995002746582 = 0.101622074842453 + 2.0 * 6.274164199829102
Epoch 830, val loss: 0.633880078792572
Epoch 840, training loss: 12.652748107910156 = 0.09733955562114716 + 2.0 * 6.277704238891602
Epoch 840, val loss: 0.6372055411338806
Epoch 850, training loss: 12.65137004852295 = 0.0933132916688919 + 2.0 * 6.279028415679932
Epoch 850, val loss: 0.6405754089355469
Epoch 860, training loss: 12.634973526000977 = 0.08952388167381287 + 2.0 * 6.272724628448486
Epoch 860, val loss: 0.6439237594604492
Epoch 870, training loss: 12.628385543823242 = 0.08594819903373718 + 2.0 * 6.271218776702881
Epoch 870, val loss: 0.6473696827888489
Epoch 880, training loss: 12.623194694519043 = 0.08254189044237137 + 2.0 * 6.270326614379883
Epoch 880, val loss: 0.6509610414505005
Epoch 890, training loss: 12.618110656738281 = 0.07930181920528412 + 2.0 * 6.269404411315918
Epoch 890, val loss: 0.6546081304550171
Epoch 900, training loss: 12.639674186706543 = 0.07620859891176224 + 2.0 * 6.281732559204102
Epoch 900, val loss: 0.6583428382873535
Epoch 910, training loss: 12.609930992126465 = 0.07330074161291122 + 2.0 * 6.268315315246582
Epoch 910, val loss: 0.6619468927383423
Epoch 920, training loss: 12.607978820800781 = 0.07053636759519577 + 2.0 * 6.268721103668213
Epoch 920, val loss: 0.6656067371368408
Epoch 930, training loss: 12.604049682617188 = 0.06789829581975937 + 2.0 * 6.268075466156006
Epoch 930, val loss: 0.6694093942642212
Epoch 940, training loss: 12.598922729492188 = 0.06539301574230194 + 2.0 * 6.2667646408081055
Epoch 940, val loss: 0.6732039451599121
Epoch 950, training loss: 12.600866317749023 = 0.06301306188106537 + 2.0 * 6.268926620483398
Epoch 950, val loss: 0.6769658923149109
Epoch 960, training loss: 12.59192943572998 = 0.06075569614768028 + 2.0 * 6.265586853027344
Epoch 960, val loss: 0.6807239055633545
Epoch 970, training loss: 12.586777687072754 = 0.05859995633363724 + 2.0 * 6.2640886306762695
Epoch 970, val loss: 0.6845197677612305
Epoch 980, training loss: 12.583672523498535 = 0.05655096471309662 + 2.0 * 6.263560771942139
Epoch 980, val loss: 0.6883327960968018
Epoch 990, training loss: 12.58340835571289 = 0.054586175829172134 + 2.0 * 6.264410972595215
Epoch 990, val loss: 0.6922339797019958
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8
0.8339483394833949
The final CL Acc:0.80741, 0.00605, The final GNN Acc:0.83869, 0.00437
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.4 der2:0 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11610])
remove edge: torch.Size([2, 9534])
updated graph: torch.Size([2, 10588])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 19.13497543334961 = 1.9412834644317627 + 2.0 * 8.596845626831055
Epoch 0, val loss: 1.9369059801101685
Epoch 10, training loss: 19.12471580505371 = 1.931540846824646 + 2.0 * 8.596587181091309
Epoch 10, val loss: 1.926939845085144
Epoch 20, training loss: 19.10867691040039 = 1.919816493988037 + 2.0 * 8.594429969787598
Epoch 20, val loss: 1.9150358438491821
Epoch 30, training loss: 19.05540657043457 = 1.9041935205459595 + 2.0 * 8.575606346130371
Epoch 30, val loss: 1.8992226123809814
Epoch 40, training loss: 18.793596267700195 = 1.8848106861114502 + 2.0 * 8.454392433166504
Epoch 40, val loss: 1.8801469802856445
Epoch 50, training loss: 17.9033145904541 = 1.8642375469207764 + 2.0 * 8.019538879394531
Epoch 50, val loss: 1.8605194091796875
Epoch 60, training loss: 17.069629669189453 = 1.8469276428222656 + 2.0 * 7.611351013183594
Epoch 60, val loss: 1.8445097208023071
Epoch 70, training loss: 16.372764587402344 = 1.833423376083374 + 2.0 * 7.2696709632873535
Epoch 70, val loss: 1.830989956855774
Epoch 80, training loss: 15.855877876281738 = 1.8195788860321045 + 2.0 * 7.018149375915527
Epoch 80, val loss: 1.8175545930862427
Epoch 90, training loss: 15.60185718536377 = 1.805785894393921 + 2.0 * 6.898035526275635
Epoch 90, val loss: 1.8043168783187866
Epoch 100, training loss: 15.406618118286133 = 1.7918587923049927 + 2.0 * 6.807379722595215
Epoch 100, val loss: 1.7915830612182617
Epoch 110, training loss: 15.255023002624512 = 1.7773853540420532 + 2.0 * 6.738818645477295
Epoch 110, val loss: 1.7784273624420166
Epoch 120, training loss: 15.143836975097656 = 1.762539029121399 + 2.0 * 6.690649032592773
Epoch 120, val loss: 1.7646418809890747
Epoch 130, training loss: 15.041786193847656 = 1.7467491626739502 + 2.0 * 6.647518634796143
Epoch 130, val loss: 1.7499866485595703
Epoch 140, training loss: 14.96369457244873 = 1.7294436693191528 + 2.0 * 6.617125511169434
Epoch 140, val loss: 1.733837604522705
Epoch 150, training loss: 14.894681930541992 = 1.709951639175415 + 2.0 * 6.592365264892578
Epoch 150, val loss: 1.7159276008605957
Epoch 160, training loss: 14.829154968261719 = 1.6881556510925293 + 2.0 * 6.570499897003174
Epoch 160, val loss: 1.6959383487701416
Epoch 170, training loss: 14.767126083374023 = 1.663818597793579 + 2.0 * 6.551653861999512
Epoch 170, val loss: 1.6737109422683716
Epoch 180, training loss: 14.705177307128906 = 1.6369662284851074 + 2.0 * 6.5341057777404785
Epoch 180, val loss: 1.6494390964508057
Epoch 190, training loss: 14.639044761657715 = 1.6072901487350464 + 2.0 * 6.5158772468566895
Epoch 190, val loss: 1.622626543045044
Epoch 200, training loss: 14.573760032653809 = 1.5744596719741821 + 2.0 * 6.499650001525879
Epoch 200, val loss: 1.5930800437927246
Epoch 210, training loss: 14.508134841918945 = 1.5385708808898926 + 2.0 * 6.484781742095947
Epoch 210, val loss: 1.5611070394515991
Epoch 220, training loss: 14.496671676635742 = 1.4998712539672852 + 2.0 * 6.4984002113342285
Epoch 220, val loss: 1.5269519090652466
Epoch 230, training loss: 14.387225151062012 = 1.4591971635818481 + 2.0 * 6.464014053344727
Epoch 230, val loss: 1.491902232170105
Epoch 240, training loss: 14.326537132263184 = 1.417317271232605 + 2.0 * 6.4546098709106445
Epoch 240, val loss: 1.456049919128418
Epoch 250, training loss: 14.265495300292969 = 1.3740613460540771 + 2.0 * 6.445716857910156
Epoch 250, val loss: 1.41972017288208
Epoch 260, training loss: 14.205039024353027 = 1.3300578594207764 + 2.0 * 6.437490463256836
Epoch 260, val loss: 1.383402705192566
Epoch 270, training loss: 14.145803451538086 = 1.2856422662734985 + 2.0 * 6.430080413818359
Epoch 270, val loss: 1.3474173545837402
Epoch 280, training loss: 14.09334945678711 = 1.2412015199661255 + 2.0 * 6.426074028015137
Epoch 280, val loss: 1.3120273351669312
Epoch 290, training loss: 14.033721923828125 = 1.1973987817764282 + 2.0 * 6.418161392211914
Epoch 290, val loss: 1.2779675722122192
Epoch 300, training loss: 13.98069953918457 = 1.1546118259429932 + 2.0 * 6.413043975830078
Epoch 300, val loss: 1.2451635599136353
Epoch 310, training loss: 13.925247192382812 = 1.112436056137085 + 2.0 * 6.406405448913574
Epoch 310, val loss: 1.2133232355117798
Epoch 320, training loss: 13.873212814331055 = 1.0710371732711792 + 2.0 * 6.401087760925293
Epoch 320, val loss: 1.1824077367782593
Epoch 330, training loss: 13.826842308044434 = 1.0304286479949951 + 2.0 * 6.39820671081543
Epoch 330, val loss: 1.1524614095687866
Epoch 340, training loss: 13.785322189331055 = 0.9914073348045349 + 2.0 * 6.3969573974609375
Epoch 340, val loss: 1.1238179206848145
Epoch 350, training loss: 13.73461627960205 = 0.9539461135864258 + 2.0 * 6.3903350830078125
Epoch 350, val loss: 1.096840739250183
Epoch 360, training loss: 13.687092781066895 = 0.9179725050926208 + 2.0 * 6.3845601081848145
Epoch 360, val loss: 1.0712329149246216
Epoch 370, training loss: 13.643701553344727 = 0.8833274245262146 + 2.0 * 6.380187034606934
Epoch 370, val loss: 1.0468939542770386
Epoch 380, training loss: 13.603200912475586 = 0.8500033617019653 + 2.0 * 6.376598834991455
Epoch 380, val loss: 1.023720622062683
Epoch 390, training loss: 13.56511116027832 = 0.8179255723953247 + 2.0 * 6.373592853546143
Epoch 390, val loss: 1.0017330646514893
Epoch 400, training loss: 13.529342651367188 = 0.7871317863464355 + 2.0 * 6.371105670928955
Epoch 400, val loss: 0.9810054302215576
Epoch 410, training loss: 13.494206428527832 = 0.7577800154685974 + 2.0 * 6.368213176727295
Epoch 410, val loss: 0.9614626169204712
Epoch 420, training loss: 13.459023475646973 = 0.729553759098053 + 2.0 * 6.364734649658203
Epoch 420, val loss: 0.9430966377258301
Epoch 430, training loss: 13.425591468811035 = 0.702225923538208 + 2.0 * 6.361682891845703
Epoch 430, val loss: 0.9256948828697205
Epoch 440, training loss: 13.39993667602539 = 0.6755857467651367 + 2.0 * 6.362175464630127
Epoch 440, val loss: 0.9091098308563232
Epoch 450, training loss: 13.369257926940918 = 0.6498215198516846 + 2.0 * 6.359718322753906
Epoch 450, val loss: 0.8931333422660828
Epoch 460, training loss: 13.334609985351562 = 0.6247435808181763 + 2.0 * 6.354933261871338
Epoch 460, val loss: 0.8781872391700745
Epoch 470, training loss: 13.302509307861328 = 0.6003193259239197 + 2.0 * 6.351095199584961
Epoch 470, val loss: 0.8640620112419128
Epoch 480, training loss: 13.278233528137207 = 0.576400101184845 + 2.0 * 6.350916862487793
Epoch 480, val loss: 0.8506744503974915
Epoch 490, training loss: 13.260271072387695 = 0.5530925393104553 + 2.0 * 6.353589057922363
Epoch 490, val loss: 0.8380694389343262
Epoch 500, training loss: 13.224958419799805 = 0.5304856896400452 + 2.0 * 6.347236156463623
Epoch 500, val loss: 0.8262988328933716
Epoch 510, training loss: 13.194222450256348 = 0.508456289768219 + 2.0 * 6.342883110046387
Epoch 510, val loss: 0.8154200315475464
Epoch 520, training loss: 13.168107986450195 = 0.4869319200515747 + 2.0 * 6.340588092803955
Epoch 520, val loss: 0.8053436875343323
Epoch 530, training loss: 13.155550003051758 = 0.4659532606601715 + 2.0 * 6.344798564910889
Epoch 530, val loss: 0.7960012555122375
Epoch 540, training loss: 13.130449295043945 = 0.4454364478588104 + 2.0 * 6.342506408691406
Epoch 540, val loss: 0.7875511646270752
Epoch 550, training loss: 13.096057891845703 = 0.4257180392742157 + 2.0 * 6.335169792175293
Epoch 550, val loss: 0.7798334360122681
Epoch 560, training loss: 13.074739456176758 = 0.40657055377960205 + 2.0 * 6.334084510803223
Epoch 560, val loss: 0.7728696465492249
Epoch 570, training loss: 13.064343452453613 = 0.38793128728866577 + 2.0 * 6.3382062911987305
Epoch 570, val loss: 0.7666426301002502
Epoch 580, training loss: 13.036914825439453 = 0.37000441551208496 + 2.0 * 6.3334550857543945
Epoch 580, val loss: 0.7609151005744934
Epoch 590, training loss: 13.011334419250488 = 0.35267284512519836 + 2.0 * 6.329330921173096
Epoch 590, val loss: 0.7560660243034363
Epoch 600, training loss: 12.989855766296387 = 0.3359917104244232 + 2.0 * 6.326931953430176
Epoch 600, val loss: 0.7518015503883362
Epoch 610, training loss: 12.985076904296875 = 0.3198733329772949 + 2.0 * 6.332601547241211
Epoch 610, val loss: 0.7480830550193787
Epoch 620, training loss: 12.951869010925293 = 0.30438268184661865 + 2.0 * 6.3237433433532715
Epoch 620, val loss: 0.7449167966842651
Epoch 630, training loss: 12.934325218200684 = 0.2895314395427704 + 2.0 * 6.322396755218506
Epoch 630, val loss: 0.7423116564750671
Epoch 640, training loss: 12.92807674407959 = 0.2752830982208252 + 2.0 * 6.326396942138672
Epoch 640, val loss: 0.7402870655059814
Epoch 650, training loss: 12.912487030029297 = 0.2616901695728302 + 2.0 * 6.3253984451293945
Epoch 650, val loss: 0.7386506199836731
Epoch 660, training loss: 12.886188507080078 = 0.24877895414829254 + 2.0 * 6.318704605102539
Epoch 660, val loss: 0.7375546097755432
Epoch 670, training loss: 12.868919372558594 = 0.2364758551120758 + 2.0 * 6.316221714019775
Epoch 670, val loss: 0.7370458245277405
Epoch 680, training loss: 12.854070663452148 = 0.22476433217525482 + 2.0 * 6.314653396606445
Epoch 680, val loss: 0.7369735836982727
Epoch 690, training loss: 12.839385986328125 = 0.2135923206806183 + 2.0 * 6.312896728515625
Epoch 690, val loss: 0.7374038696289062
Epoch 700, training loss: 12.868868827819824 = 0.20293912291526794 + 2.0 * 6.332964897155762
Epoch 700, val loss: 0.7383102178573608
Epoch 710, training loss: 12.818307876586914 = 0.19294941425323486 + 2.0 * 6.312679290771484
Epoch 710, val loss: 0.7394804358482361
Epoch 720, training loss: 12.804937362670898 = 0.18354244530200958 + 2.0 * 6.310697555541992
Epoch 720, val loss: 0.7411240339279175
Epoch 730, training loss: 12.792351722717285 = 0.1746426224708557 + 2.0 * 6.308854579925537
Epoch 730, val loss: 0.7431977391242981
Epoch 740, training loss: 12.780165672302246 = 0.16620944440364838 + 2.0 * 6.306978225708008
Epoch 740, val loss: 0.7455939650535583
Epoch 750, training loss: 12.768671989440918 = 0.15820223093032837 + 2.0 * 6.305234909057617
Epoch 750, val loss: 0.7483177185058594
Epoch 760, training loss: 12.803155899047852 = 0.15064547955989838 + 2.0 * 6.3262553215026855
Epoch 760, val loss: 0.7513794302940369
Epoch 770, training loss: 12.752776145935059 = 0.14347030222415924 + 2.0 * 6.304652690887451
Epoch 770, val loss: 0.7545254826545715
Epoch 780, training loss: 12.740368843078613 = 0.13674627244472504 + 2.0 * 6.301811218261719
Epoch 780, val loss: 0.7580049633979797
Epoch 790, training loss: 12.731555938720703 = 0.13040195405483246 + 2.0 * 6.300577163696289
Epoch 790, val loss: 0.7617180347442627
Epoch 800, training loss: 12.723697662353516 = 0.12438560277223587 + 2.0 * 6.299655914306641
Epoch 800, val loss: 0.765677273273468
Epoch 810, training loss: 12.718257904052734 = 0.11865945160388947 + 2.0 * 6.29979944229126
Epoch 810, val loss: 0.7698985934257507
Epoch 820, training loss: 12.711074829101562 = 0.11323285102844238 + 2.0 * 6.29892110824585
Epoch 820, val loss: 0.7742494940757751
Epoch 830, training loss: 12.716546058654785 = 0.10810455679893494 + 2.0 * 6.304220676422119
Epoch 830, val loss: 0.7787066698074341
Epoch 840, training loss: 12.699214935302734 = 0.10326370596885681 + 2.0 * 6.297975540161133
Epoch 840, val loss: 0.7832439541816711
Epoch 850, training loss: 12.689911842346191 = 0.09869751334190369 + 2.0 * 6.295607089996338
Epoch 850, val loss: 0.7879656553268433
Epoch 860, training loss: 12.682310104370117 = 0.09437749534845352 + 2.0 * 6.293966293334961
Epoch 860, val loss: 0.7927846908569336
Epoch 870, training loss: 12.69243049621582 = 0.09026774764060974 + 2.0 * 6.30108118057251
Epoch 870, val loss: 0.7976614236831665
Epoch 880, training loss: 12.672846794128418 = 0.0864110141992569 + 2.0 * 6.293217658996582
Epoch 880, val loss: 0.8026149272918701
Epoch 890, training loss: 12.663683891296387 = 0.08273477107286453 + 2.0 * 6.2904744148254395
Epoch 890, val loss: 0.8076375126838684
Epoch 900, training loss: 12.658360481262207 = 0.0792718157172203 + 2.0 * 6.289544105529785
Epoch 900, val loss: 0.812740683555603
Epoch 910, training loss: 12.65591049194336 = 0.07599449157714844 + 2.0 * 6.2899580001831055
Epoch 910, val loss: 0.8179054260253906
Epoch 920, training loss: 12.650446891784668 = 0.07288199663162231 + 2.0 * 6.288782596588135
Epoch 920, val loss: 0.8231136202812195
Epoch 930, training loss: 12.647611618041992 = 0.06994229555130005 + 2.0 * 6.288834571838379
Epoch 930, val loss: 0.8282791972160339
Epoch 940, training loss: 12.638728141784668 = 0.06717177480459213 + 2.0 * 6.285778045654297
Epoch 940, val loss: 0.8335152864456177
Epoch 950, training loss: 12.646923065185547 = 0.06453907489776611 + 2.0 * 6.291192054748535
Epoch 950, val loss: 0.838738203048706
Epoch 960, training loss: 12.634397506713867 = 0.06205497682094574 + 2.0 * 6.2861714363098145
Epoch 960, val loss: 0.8439570069313049
Epoch 970, training loss: 12.631068229675293 = 0.05969182029366493 + 2.0 * 6.285688400268555
Epoch 970, val loss: 0.8491630554199219
Epoch 980, training loss: 12.623434066772461 = 0.057469215244054794 + 2.0 * 6.282982349395752
Epoch 980, val loss: 0.854405403137207
Epoch 990, training loss: 12.618497848510742 = 0.055346596986055374 + 2.0 * 6.281575679779053
Epoch 990, val loss: 0.859612762928009
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7666666666666667
0.8039008961518187
=== training gcn model ===
Epoch 0, training loss: 19.138931274414062 = 1.9452486038208008 + 2.0 * 8.596840858459473
Epoch 0, val loss: 1.940721035003662
Epoch 10, training loss: 19.12874412536621 = 1.9357050657272339 + 2.0 * 8.596519470214844
Epoch 10, val loss: 1.9318492412567139
Epoch 20, training loss: 19.111141204833984 = 1.9236608743667603 + 2.0 * 8.593740463256836
Epoch 20, val loss: 1.9200938940048218
Epoch 30, training loss: 19.048187255859375 = 1.9068727493286133 + 2.0 * 8.570656776428223
Epoch 30, val loss: 1.90335214138031
Epoch 40, training loss: 18.69806671142578 = 1.885301947593689 + 2.0 * 8.40638256072998
Epoch 40, val loss: 1.882110834121704
Epoch 50, training loss: 17.691556930541992 = 1.8622097969055176 + 2.0 * 7.914673805236816
Epoch 50, val loss: 1.8595484495162964
Epoch 60, training loss: 16.8969783782959 = 1.8457133769989014 + 2.0 * 7.525632858276367
Epoch 60, val loss: 1.8445698022842407
Epoch 70, training loss: 16.31068229675293 = 1.8327816724777222 + 2.0 * 7.238950252532959
Epoch 70, val loss: 1.832030177116394
Epoch 80, training loss: 15.828660011291504 = 1.8206464052200317 + 2.0 * 7.004006862640381
Epoch 80, val loss: 1.8205890655517578
Epoch 90, training loss: 15.630531311035156 = 1.8070193529129028 + 2.0 * 6.9117560386657715
Epoch 90, val loss: 1.8075698614120483
Epoch 100, training loss: 15.44107437133789 = 1.7934887409210205 + 2.0 * 6.823792934417725
Epoch 100, val loss: 1.795257806777954
Epoch 110, training loss: 15.274965286254883 = 1.7813315391540527 + 2.0 * 6.746817111968994
Epoch 110, val loss: 1.7841228246688843
Epoch 120, training loss: 15.1528959274292 = 1.769045114517212 + 2.0 * 6.691925525665283
Epoch 120, val loss: 1.772756576538086
Epoch 130, training loss: 15.057637214660645 = 1.7552785873413086 + 2.0 * 6.651179313659668
Epoch 130, val loss: 1.7601534128189087
Epoch 140, training loss: 14.974516868591309 = 1.7401392459869385 + 2.0 * 6.617188930511475
Epoch 140, val loss: 1.7463476657867432
Epoch 150, training loss: 14.900434494018555 = 1.7235915660858154 + 2.0 * 6.58842134475708
Epoch 150, val loss: 1.7314382791519165
Epoch 160, training loss: 14.827766418457031 = 1.7051422595977783 + 2.0 * 6.561312198638916
Epoch 160, val loss: 1.7148950099945068
Epoch 170, training loss: 14.758142471313477 = 1.684481143951416 + 2.0 * 6.536830902099609
Epoch 170, val loss: 1.6964235305786133
Epoch 180, training loss: 14.705525398254395 = 1.6612718105316162 + 2.0 * 6.5221266746521
Epoch 180, val loss: 1.6758410930633545
Epoch 190, training loss: 14.636124610900879 = 1.6353213787078857 + 2.0 * 6.500401496887207
Epoch 190, val loss: 1.6529746055603027
Epoch 200, training loss: 14.576803207397461 = 1.606360673904419 + 2.0 * 6.4852213859558105
Epoch 200, val loss: 1.6276350021362305
Epoch 210, training loss: 14.51758861541748 = 1.5744028091430664 + 2.0 * 6.471592903137207
Epoch 210, val loss: 1.6000158786773682
Epoch 220, training loss: 14.459431648254395 = 1.5398403406143188 + 2.0 * 6.4597954750061035
Epoch 220, val loss: 1.5705796480178833
Epoch 230, training loss: 14.400678634643555 = 1.5028451681137085 + 2.0 * 6.448916912078857
Epoch 230, val loss: 1.5394974946975708
Epoch 240, training loss: 14.342467308044434 = 1.4636907577514648 + 2.0 * 6.439388275146484
Epoch 240, val loss: 1.5071194171905518
Epoch 250, training loss: 14.290868759155273 = 1.423117756843567 + 2.0 * 6.433875560760498
Epoch 250, val loss: 1.474144458770752
Epoch 260, training loss: 14.230831146240234 = 1.3820791244506836 + 2.0 * 6.424376010894775
Epoch 260, val loss: 1.441469430923462
Epoch 270, training loss: 14.173628807067871 = 1.340964436531067 + 2.0 * 6.416332244873047
Epoch 270, val loss: 1.4092626571655273
Epoch 280, training loss: 14.118669509887695 = 1.2998547554016113 + 2.0 * 6.409407615661621
Epoch 280, val loss: 1.377536416053772
Epoch 290, training loss: 14.068029403686523 = 1.2590562105178833 + 2.0 * 6.404486656188965
Epoch 290, val loss: 1.3465081453323364
Epoch 300, training loss: 14.025941848754883 = 1.2187820672988892 + 2.0 * 6.4035797119140625
Epoch 300, val loss: 1.3164572715759277
Epoch 310, training loss: 13.967291831970215 = 1.1795822381973267 + 2.0 * 6.39385461807251
Epoch 310, val loss: 1.287514567375183
Epoch 320, training loss: 13.917929649353027 = 1.1409847736358643 + 2.0 * 6.388472557067871
Epoch 320, val loss: 1.2594244480133057
Epoch 330, training loss: 13.870197296142578 = 1.1027573347091675 + 2.0 * 6.3837199211120605
Epoch 330, val loss: 1.231972575187683
Epoch 340, training loss: 13.823226928710938 = 1.0649571418762207 + 2.0 * 6.379134654998779
Epoch 340, val loss: 1.2052162885665894
Epoch 350, training loss: 13.80828857421875 = 1.0276448726654053 + 2.0 * 6.390321731567383
Epoch 350, val loss: 1.1792181730270386
Epoch 360, training loss: 13.743836402893066 = 0.9913744330406189 + 2.0 * 6.3762311935424805
Epoch 360, val loss: 1.1542119979858398
Epoch 370, training loss: 13.692397117614746 = 0.9562281966209412 + 2.0 * 6.36808443069458
Epoch 370, val loss: 1.1304142475128174
Epoch 380, training loss: 13.65324592590332 = 0.9221401810646057 + 2.0 * 6.36555290222168
Epoch 380, val loss: 1.107718825340271
Epoch 390, training loss: 13.61367416381836 = 0.8893188834190369 + 2.0 * 6.362177848815918
Epoch 390, val loss: 1.0863854885101318
Epoch 400, training loss: 13.57544231414795 = 0.8579640984535217 + 2.0 * 6.358738899230957
Epoch 400, val loss: 1.0664665699005127
Epoch 410, training loss: 13.53896427154541 = 0.828007161617279 + 2.0 * 6.355478763580322
Epoch 410, val loss: 1.047900915145874
Epoch 420, training loss: 13.522397994995117 = 0.7994217276573181 + 2.0 * 6.361488342285156
Epoch 420, val loss: 1.0306802988052368
Epoch 430, training loss: 13.476176261901855 = 0.7723082304000854 + 2.0 * 6.35193395614624
Epoch 430, val loss: 1.0149441957473755
Epoch 440, training loss: 13.440373420715332 = 0.7465552091598511 + 2.0 * 6.346909046173096
Epoch 440, val loss: 1.0006071329116821
Epoch 450, training loss: 13.409942626953125 = 0.7218941450119019 + 2.0 * 6.344024181365967
Epoch 450, val loss: 0.9874025583267212
Epoch 460, training loss: 13.382417678833008 = 0.6982215046882629 + 2.0 * 6.342098236083984
Epoch 460, val loss: 0.9752039909362793
Epoch 470, training loss: 13.354446411132812 = 0.6754813194274902 + 2.0 * 6.339482307434082
Epoch 470, val loss: 0.9640453457832336
Epoch 480, training loss: 13.336487770080566 = 0.6534850001335144 + 2.0 * 6.341501235961914
Epoch 480, val loss: 0.9536875486373901
Epoch 490, training loss: 13.311153411865234 = 0.6323230266571045 + 2.0 * 6.339415073394775
Epoch 490, val loss: 0.9441424608230591
Epoch 500, training loss: 13.278493881225586 = 0.6117345094680786 + 2.0 * 6.333379745483398
Epoch 500, val loss: 0.9353955984115601
Epoch 510, training loss: 13.251073837280273 = 0.5915899872779846 + 2.0 * 6.329741954803467
Epoch 510, val loss: 0.9272670149803162
Epoch 520, training loss: 13.227681159973145 = 0.5717567801475525 + 2.0 * 6.327962398529053
Epoch 520, val loss: 0.9197061061859131
Epoch 530, training loss: 13.204262733459473 = 0.5522509813308716 + 2.0 * 6.326005935668945
Epoch 530, val loss: 0.9126163721084595
Epoch 540, training loss: 13.184840202331543 = 0.5331088900566101 + 2.0 * 6.325865745544434
Epoch 540, val loss: 0.9061102271080017
Epoch 550, training loss: 13.159050941467285 = 0.5142285227775574 + 2.0 * 6.322411060333252
Epoch 550, val loss: 0.9000652432441711
Epoch 560, training loss: 13.150298118591309 = 0.4956393837928772 + 2.0 * 6.327329158782959
Epoch 560, val loss: 0.8945102691650391
Epoch 570, training loss: 13.11585521697998 = 0.4773738980293274 + 2.0 * 6.319240570068359
Epoch 570, val loss: 0.8895058035850525
Epoch 580, training loss: 13.094003677368164 = 0.4594214856624603 + 2.0 * 6.317291259765625
Epoch 580, val loss: 0.884920060634613
Epoch 590, training loss: 13.080523490905762 = 0.4416545033454895 + 2.0 * 6.319434642791748
Epoch 590, val loss: 0.8806746602058411
Epoch 600, training loss: 13.060710906982422 = 0.42418527603149414 + 2.0 * 6.318262577056885
Epoch 600, val loss: 0.876892626285553
Epoch 610, training loss: 13.034801483154297 = 0.4071131944656372 + 2.0 * 6.313844203948975
Epoch 610, val loss: 0.8736828565597534
Epoch 620, training loss: 13.015700340270996 = 0.3903862237930298 + 2.0 * 6.312656879425049
Epoch 620, val loss: 0.8708953261375427
Epoch 630, training loss: 12.995011329650879 = 0.37400898337364197 + 2.0 * 6.3105010986328125
Epoch 630, val loss: 0.8686324954032898
Epoch 640, training loss: 13.001112937927246 = 0.35801294445991516 + 2.0 * 6.321549892425537
Epoch 640, val loss: 0.8668530583381653
Epoch 650, training loss: 12.959756851196289 = 0.34260794520378113 + 2.0 * 6.308574676513672
Epoch 650, val loss: 0.8656384944915771
Epoch 660, training loss: 12.944533348083496 = 0.3276750445365906 + 2.0 * 6.30842924118042
Epoch 660, val loss: 0.8649353384971619
Epoch 670, training loss: 12.924186706542969 = 0.3131902813911438 + 2.0 * 6.305498123168945
Epoch 670, val loss: 0.8648027181625366
Epoch 680, training loss: 12.917162895202637 = 0.299193799495697 + 2.0 * 6.308984756469727
Epoch 680, val loss: 0.8651546239852905
Epoch 690, training loss: 12.893630027770996 = 0.28576645255088806 + 2.0 * 6.303931713104248
Epoch 690, val loss: 0.8660125136375427
Epoch 700, training loss: 12.882561683654785 = 0.27284619212150574 + 2.0 * 6.3048577308654785
Epoch 700, val loss: 0.867367684841156
Epoch 710, training loss: 12.864191055297852 = 0.2604649066925049 + 2.0 * 6.301863193511963
Epoch 710, val loss: 0.8692752122879028
Epoch 720, training loss: 12.852189064025879 = 0.24860766530036926 + 2.0 * 6.301790714263916
Epoch 720, val loss: 0.8716800808906555
Epoch 730, training loss: 12.836886405944824 = 0.2372675985097885 + 2.0 * 6.299809455871582
Epoch 730, val loss: 0.8746102452278137
Epoch 740, training loss: 12.832585334777832 = 0.22644956409931183 + 2.0 * 6.303067684173584
Epoch 740, val loss: 0.8780199289321899
Epoch 750, training loss: 12.818290710449219 = 0.21610577404499054 + 2.0 * 6.301092624664307
Epoch 750, val loss: 0.8818634748458862
Epoch 760, training loss: 12.801355361938477 = 0.20628806948661804 + 2.0 * 6.2975335121154785
Epoch 760, val loss: 0.8862599730491638
Epoch 770, training loss: 12.787274360656738 = 0.1969178318977356 + 2.0 * 6.295178413391113
Epoch 770, val loss: 0.8910422325134277
Epoch 780, training loss: 12.782161712646484 = 0.18798504769802094 + 2.0 * 6.297088146209717
Epoch 780, val loss: 0.8962472677230835
Epoch 790, training loss: 12.769929885864258 = 0.17949101328849792 + 2.0 * 6.295219421386719
Epoch 790, val loss: 0.90183424949646
Epoch 800, training loss: 12.764410972595215 = 0.1714121699333191 + 2.0 * 6.296499252319336
Epoch 800, val loss: 0.9076878428459167
Epoch 810, training loss: 12.747895240783691 = 0.16377004981040955 + 2.0 * 6.292062759399414
Epoch 810, val loss: 0.913887083530426
Epoch 820, training loss: 12.739151954650879 = 0.15649926662445068 + 2.0 * 6.291326522827148
Epoch 820, val loss: 0.9204750657081604
Epoch 830, training loss: 12.737723350524902 = 0.14956536889076233 + 2.0 * 6.294078826904297
Epoch 830, val loss: 0.9272335171699524
Epoch 840, training loss: 12.732060432434082 = 0.14295819401741028 + 2.0 * 6.294550895690918
Epoch 840, val loss: 0.9342395663261414
Epoch 850, training loss: 12.72038745880127 = 0.13670645654201508 + 2.0 * 6.291840553283691
Epoch 850, val loss: 0.9415343403816223
Epoch 860, training loss: 12.70925235748291 = 0.13075436651706696 + 2.0 * 6.289248943328857
Epoch 860, val loss: 0.9489404559135437
Epoch 870, training loss: 12.703451156616211 = 0.12510539591312408 + 2.0 * 6.289172649383545
Epoch 870, val loss: 0.9565075039863586
Epoch 880, training loss: 12.691088676452637 = 0.11975064873695374 + 2.0 * 6.285668849945068
Epoch 880, val loss: 0.9642167687416077
Epoch 890, training loss: 12.684210777282715 = 0.11463220417499542 + 2.0 * 6.284789085388184
Epoch 890, val loss: 0.9721229672431946
Epoch 900, training loss: 12.68969440460205 = 0.109746053814888 + 2.0 * 6.289974212646484
Epoch 900, val loss: 0.9801546335220337
Epoch 910, training loss: 12.674062728881836 = 0.10506103932857513 + 2.0 * 6.284501075744629
Epoch 910, val loss: 0.98813396692276
Epoch 920, training loss: 12.671730995178223 = 0.10061958432197571 + 2.0 * 6.285555839538574
Epoch 920, val loss: 0.9963185787200928
Epoch 930, training loss: 12.664785385131836 = 0.09637721627950668 + 2.0 * 6.284204006195068
Epoch 930, val loss: 1.0045263767242432
Epoch 940, training loss: 12.658405303955078 = 0.09236130863428116 + 2.0 * 6.283021926879883
Epoch 940, val loss: 1.0127904415130615
Epoch 950, training loss: 12.650778770446777 = 0.08853640407323837 + 2.0 * 6.281121253967285
Epoch 950, val loss: 1.0210731029510498
Epoch 960, training loss: 12.64554214477539 = 0.08489689230918884 + 2.0 * 6.280322551727295
Epoch 960, val loss: 1.0294126272201538
Epoch 970, training loss: 12.653141021728516 = 0.08142954111099243 + 2.0 * 6.285855770111084
Epoch 970, val loss: 1.0376964807510376
Epoch 980, training loss: 12.641504287719727 = 0.07814811915159225 + 2.0 * 6.281678199768066
Epoch 980, val loss: 1.0460588932037354
Epoch 990, training loss: 12.633600234985352 = 0.07502331584692001 + 2.0 * 6.279288291931152
Epoch 990, val loss: 1.0544049739837646
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.762962962962963
0.8144438587243016
=== training gcn model ===
Epoch 0, training loss: 19.144548416137695 = 1.9508099555969238 + 2.0 * 8.596869468688965
Epoch 0, val loss: 1.9490865468978882
Epoch 10, training loss: 19.133941650390625 = 1.9407758712768555 + 2.0 * 8.596582412719727
Epoch 10, val loss: 1.938461184501648
Epoch 20, training loss: 19.115798950195312 = 1.928439736366272 + 2.0 * 8.593679428100586
Epoch 20, val loss: 1.9252185821533203
Epoch 30, training loss: 19.051082611083984 = 1.9116666316986084 + 2.0 * 8.569707870483398
Epoch 30, val loss: 1.907348871231079
Epoch 40, training loss: 18.717411041259766 = 1.8907145261764526 + 2.0 * 8.413348197937012
Epoch 40, val loss: 1.8861801624298096
Epoch 50, training loss: 18.086692810058594 = 1.867861270904541 + 2.0 * 8.109416007995605
Epoch 50, val loss: 1.863847255706787
Epoch 60, training loss: 17.620380401611328 = 1.8492445945739746 + 2.0 * 7.885568141937256
Epoch 60, val loss: 1.8465901613235474
Epoch 70, training loss: 16.5720272064209 = 1.8350727558135986 + 2.0 * 7.368476867675781
Epoch 70, val loss: 1.8337712287902832
Epoch 80, training loss: 15.883835792541504 = 1.8242356777191162 + 2.0 * 7.029799938201904
Epoch 80, val loss: 1.823835015296936
Epoch 90, training loss: 15.578916549682617 = 1.8110660314559937 + 2.0 * 6.883925437927246
Epoch 90, val loss: 1.8115766048431396
Epoch 100, training loss: 15.376654624938965 = 1.7969286441802979 + 2.0 * 6.789863109588623
Epoch 100, val loss: 1.7985773086547852
Epoch 110, training loss: 15.233858108520508 = 1.7831697463989258 + 2.0 * 6.725344181060791
Epoch 110, val loss: 1.7858318090438843
Epoch 120, training loss: 15.115328788757324 = 1.7693605422973633 + 2.0 * 6.6729841232299805
Epoch 120, val loss: 1.7735120058059692
Epoch 130, training loss: 15.015490531921387 = 1.7551323175430298 + 2.0 * 6.630178928375244
Epoch 130, val loss: 1.7609196901321411
Epoch 140, training loss: 14.936322212219238 = 1.7400296926498413 + 2.0 * 6.598146438598633
Epoch 140, val loss: 1.7476462125778198
Epoch 150, training loss: 14.865118980407715 = 1.7234724760055542 + 2.0 * 6.5708231925964355
Epoch 150, val loss: 1.7329506874084473
Epoch 160, training loss: 14.804727554321289 = 1.7050727605819702 + 2.0 * 6.549827575683594
Epoch 160, val loss: 1.7167617082595825
Epoch 170, training loss: 14.739680290222168 = 1.6846257448196411 + 2.0 * 6.527527332305908
Epoch 170, val loss: 1.6988754272460938
Epoch 180, training loss: 14.676080703735352 = 1.6618672609329224 + 2.0 * 6.507106781005859
Epoch 180, val loss: 1.67911696434021
Epoch 190, training loss: 14.624916076660156 = 1.6363872289657593 + 2.0 * 6.494264602661133
Epoch 190, val loss: 1.6572587490081787
Epoch 200, training loss: 14.563361167907715 = 1.608173131942749 + 2.0 * 6.477593898773193
Epoch 200, val loss: 1.6332060098648071
Epoch 210, training loss: 14.50605297088623 = 1.5770397186279297 + 2.0 * 6.46450662612915
Epoch 210, val loss: 1.6068423986434937
Epoch 220, training loss: 14.448513984680176 = 1.5427920818328857 + 2.0 * 6.4528608322143555
Epoch 220, val loss: 1.5780397653579712
Epoch 230, training loss: 14.397656440734863 = 1.505395770072937 + 2.0 * 6.446130275726318
Epoch 230, val loss: 1.5469481945037842
Epoch 240, training loss: 14.335617065429688 = 1.4650713205337524 + 2.0 * 6.435272693634033
Epoch 240, val loss: 1.5137313604354858
Epoch 250, training loss: 14.278352737426758 = 1.4219317436218262 + 2.0 * 6.428210258483887
Epoch 250, val loss: 1.478602647781372
Epoch 260, training loss: 14.219104766845703 = 1.3761305809020996 + 2.0 * 6.421486854553223
Epoch 260, val loss: 1.441679835319519
Epoch 270, training loss: 14.163394927978516 = 1.3282310962677002 + 2.0 * 6.417582035064697
Epoch 270, val loss: 1.4034653902053833
Epoch 280, training loss: 14.101310729980469 = 1.2793022394180298 + 2.0 * 6.411004066467285
Epoch 280, val loss: 1.3647629022598267
Epoch 290, training loss: 14.039676666259766 = 1.2299882173538208 + 2.0 * 6.404844284057617
Epoch 290, val loss: 1.326299786567688
Epoch 300, training loss: 13.984268188476562 = 1.1807520389556885 + 2.0 * 6.401758193969727
Epoch 300, val loss: 1.2882856130599976
Epoch 310, training loss: 13.927549362182617 = 1.132441520690918 + 2.0 * 6.39755392074585
Epoch 310, val loss: 1.2514559030532837
Epoch 320, training loss: 13.868399620056152 = 1.0854994058609009 + 2.0 * 6.391449928283691
Epoch 320, val loss: 1.2162103652954102
Epoch 330, training loss: 13.813725471496582 = 1.040230393409729 + 2.0 * 6.386747360229492
Epoch 330, val loss: 1.1827272176742554
Epoch 340, training loss: 13.771597862243652 = 0.9970346689224243 + 2.0 * 6.38728141784668
Epoch 340, val loss: 1.1515311002731323
Epoch 350, training loss: 13.722603797912598 = 0.9564440250396729 + 2.0 * 6.383080005645752
Epoch 350, val loss: 1.1229052543640137
Epoch 360, training loss: 13.670587539672852 = 0.9183691740036011 + 2.0 * 6.3761091232299805
Epoch 360, val loss: 1.0966380834579468
Epoch 370, training loss: 13.626309394836426 = 0.88259357213974 + 2.0 * 6.3718581199646
Epoch 370, val loss: 1.072629690170288
Epoch 380, training loss: 13.601497650146484 = 0.8489843606948853 + 2.0 * 6.376256465911865
Epoch 380, val loss: 1.0507409572601318
Epoch 390, training loss: 13.548794746398926 = 0.8175876140594482 + 2.0 * 6.365603446960449
Epoch 390, val loss: 1.0309721231460571
Epoch 400, training loss: 13.514217376708984 = 0.7882180213928223 + 2.0 * 6.36299991607666
Epoch 400, val loss: 1.013086199760437
Epoch 410, training loss: 13.480623245239258 = 0.7605023384094238 + 2.0 * 6.360060214996338
Epoch 410, val loss: 0.9968659281730652
Epoch 420, training loss: 13.455242156982422 = 0.7342514991760254 + 2.0 * 6.360495567321777
Epoch 420, val loss: 0.9821353554725647
Epoch 430, training loss: 13.42251968383789 = 0.7093364000320435 + 2.0 * 6.356591701507568
Epoch 430, val loss: 0.9685425758361816
Epoch 440, training loss: 13.39431095123291 = 0.6857283711433411 + 2.0 * 6.3542914390563965
Epoch 440, val loss: 0.9562554359436035
Epoch 450, training loss: 13.361529350280762 = 0.6631507277488708 + 2.0 * 6.349189281463623
Epoch 450, val loss: 0.9449253082275391
Epoch 460, training loss: 13.340042114257812 = 0.6415008306503296 + 2.0 * 6.349270820617676
Epoch 460, val loss: 0.9346437454223633
Epoch 470, training loss: 13.31241226196289 = 0.6207183003425598 + 2.0 * 6.345847129821777
Epoch 470, val loss: 0.9253065586090088
Epoch 480, training loss: 13.295598030090332 = 0.6007099151611328 + 2.0 * 6.3474440574646
Epoch 480, val loss: 0.9166828393936157
Epoch 490, training loss: 13.262765884399414 = 0.5814298391342163 + 2.0 * 6.340668201446533
Epoch 490, val loss: 0.9089427590370178
Epoch 500, training loss: 13.238972663879395 = 0.5628109574317932 + 2.0 * 6.338080883026123
Epoch 500, val loss: 0.9018902778625488
Epoch 510, training loss: 13.233308792114258 = 0.5447425246238708 + 2.0 * 6.344283103942871
Epoch 510, val loss: 0.895534098148346
Epoch 520, training loss: 13.199431419372559 = 0.527191698551178 + 2.0 * 6.336119651794434
Epoch 520, val loss: 0.889585018157959
Epoch 530, training loss: 13.176557540893555 = 0.5102040767669678 + 2.0 * 6.333176612854004
Epoch 530, val loss: 0.8843365907669067
Epoch 540, training loss: 13.153613090515137 = 0.4936515986919403 + 2.0 * 6.329980850219727
Epoch 540, val loss: 0.8795444965362549
Epoch 550, training loss: 13.137588500976562 = 0.4774229824542999 + 2.0 * 6.330082893371582
Epoch 550, val loss: 0.8751214146614075
Epoch 560, training loss: 13.11866283416748 = 0.4615480303764343 + 2.0 * 6.32855749130249
Epoch 560, val loss: 0.8711377382278442
Epoch 570, training loss: 13.09688663482666 = 0.4459694027900696 + 2.0 * 6.325458526611328
Epoch 570, val loss: 0.8673723936080933
Epoch 580, training loss: 13.077290534973145 = 0.43064504861831665 + 2.0 * 6.323322772979736
Epoch 580, val loss: 0.8640041947364807
Epoch 590, training loss: 13.06983757019043 = 0.4155479371547699 + 2.0 * 6.327144622802734
Epoch 590, val loss: 0.860873281955719
Epoch 600, training loss: 13.040279388427734 = 0.40066108107566833 + 2.0 * 6.3198089599609375
Epoch 600, val loss: 0.8579731583595276
Epoch 610, training loss: 13.024333000183105 = 0.38597607612609863 + 2.0 * 6.319178581237793
Epoch 610, val loss: 0.8553704619407654
Epoch 620, training loss: 13.01247787475586 = 0.37147635221481323 + 2.0 * 6.32050085067749
Epoch 620, val loss: 0.8529680967330933
Epoch 630, training loss: 12.99405574798584 = 0.35721832513809204 + 2.0 * 6.318418502807617
Epoch 630, val loss: 0.8508564233779907
Epoch 640, training loss: 12.970803260803223 = 0.3431667387485504 + 2.0 * 6.313818454742432
Epoch 640, val loss: 0.8489995002746582
Epoch 650, training loss: 12.955869674682617 = 0.3293209671974182 + 2.0 * 6.313274383544922
Epoch 650, val loss: 0.8474666476249695
Epoch 660, training loss: 12.9556303024292 = 0.3157171905040741 + 2.0 * 6.3199567794799805
Epoch 660, val loss: 0.8462111353874207
Epoch 670, training loss: 12.92343807220459 = 0.30235832929611206 + 2.0 * 6.310539722442627
Epoch 670, val loss: 0.845189094543457
Epoch 680, training loss: 12.908332824707031 = 0.2892951965332031 + 2.0 * 6.309518814086914
Epoch 680, val loss: 0.8444693684577942
Epoch 690, training loss: 12.891014099121094 = 0.2765483856201172 + 2.0 * 6.307232856750488
Epoch 690, val loss: 0.8441410660743713
Epoch 700, training loss: 12.878318786621094 = 0.26409631967544556 + 2.0 * 6.3071112632751465
Epoch 700, val loss: 0.8441565632820129
Epoch 710, training loss: 12.873004913330078 = 0.25201940536499023 + 2.0 * 6.310492515563965
Epoch 710, val loss: 0.8444895148277283
Epoch 720, training loss: 12.853996276855469 = 0.2403746098279953 + 2.0 * 6.3068108558654785
Epoch 720, val loss: 0.8450307846069336
Epoch 730, training loss: 12.837623596191406 = 0.22916483879089355 + 2.0 * 6.304229259490967
Epoch 730, val loss: 0.8460554480552673
Epoch 740, training loss: 12.82259750366211 = 0.21836695075035095 + 2.0 * 6.302115440368652
Epoch 740, val loss: 0.8474922180175781
Epoch 750, training loss: 12.815823554992676 = 0.20796769857406616 + 2.0 * 6.303927898406982
Epoch 750, val loss: 0.8492811918258667
Epoch 760, training loss: 12.810441017150879 = 0.19799423217773438 + 2.0 * 6.306223392486572
Epoch 760, val loss: 0.8514505624771118
Epoch 770, training loss: 12.785415649414062 = 0.18848466873168945 + 2.0 * 6.298465728759766
Epoch 770, val loss: 0.8538773059844971
Epoch 780, training loss: 12.774935722351074 = 0.1794200986623764 + 2.0 * 6.297757625579834
Epoch 780, val loss: 0.8566041588783264
Epoch 790, training loss: 12.767889976501465 = 0.1707793027162552 + 2.0 * 6.298555374145508
Epoch 790, val loss: 0.8596996068954468
Epoch 800, training loss: 12.753803253173828 = 0.1625630408525467 + 2.0 * 6.295619964599609
Epoch 800, val loss: 0.8631759881973267
Epoch 810, training loss: 12.749540328979492 = 0.15477122366428375 + 2.0 * 6.297384738922119
Epoch 810, val loss: 0.8669416904449463
Epoch 820, training loss: 12.734898567199707 = 0.14738668501377106 + 2.0 * 6.293756008148193
Epoch 820, val loss: 0.8710381388664246
Epoch 830, training loss: 12.725653648376465 = 0.1403878927230835 + 2.0 * 6.292633056640625
Epoch 830, val loss: 0.8754561543464661
Epoch 840, training loss: 12.719078063964844 = 0.13374070823192596 + 2.0 * 6.29266881942749
Epoch 840, val loss: 0.8801095485687256
Epoch 850, training loss: 12.709081649780273 = 0.12745214998722076 + 2.0 * 6.2908148765563965
Epoch 850, val loss: 0.8850305080413818
Epoch 860, training loss: 12.706665992736816 = 0.12153597176074982 + 2.0 * 6.292564868927002
Epoch 860, val loss: 0.8900657296180725
Epoch 870, training loss: 12.696518898010254 = 0.1159721240401268 + 2.0 * 6.290273189544678
Epoch 870, val loss: 0.8953737020492554
Epoch 880, training loss: 12.687017440795898 = 0.11071612685918808 + 2.0 * 6.288150787353516
Epoch 880, val loss: 0.9008844494819641
Epoch 890, training loss: 12.679736137390137 = 0.10573818534612656 + 2.0 * 6.286998748779297
Epoch 890, val loss: 0.9066784977912903
Epoch 900, training loss: 12.685343742370605 = 0.10103902220726013 + 2.0 * 6.292152404785156
Epoch 900, val loss: 0.9126976132392883
Epoch 910, training loss: 12.686614990234375 = 0.09656334668397903 + 2.0 * 6.295025825500488
Epoch 910, val loss: 0.9184530377388
Epoch 920, training loss: 12.665305137634277 = 0.09236901998519897 + 2.0 * 6.286468029022217
Epoch 920, val loss: 0.9246498346328735
Epoch 930, training loss: 12.657185554504395 = 0.08841460943222046 + 2.0 * 6.284385681152344
Epoch 930, val loss: 0.9308412075042725
Epoch 940, training loss: 12.652673721313477 = 0.0846654549241066 + 2.0 * 6.284004211425781
Epoch 940, val loss: 0.9372172951698303
Epoch 950, training loss: 12.653996467590332 = 0.08111347258090973 + 2.0 * 6.286441326141357
Epoch 950, val loss: 0.9436595439910889
Epoch 960, training loss: 12.64292049407959 = 0.07775504142045975 + 2.0 * 6.282582759857178
Epoch 960, val loss: 0.9503354430198669
Epoch 970, training loss: 12.653373718261719 = 0.07458071410655975 + 2.0 * 6.289396286010742
Epoch 970, val loss: 0.9570525884628296
Epoch 980, training loss: 12.638646125793457 = 0.0715557187795639 + 2.0 * 6.283545017242432
Epoch 980, val loss: 0.9632335305213928
Epoch 990, training loss: 12.631353378295898 = 0.06872369349002838 + 2.0 * 6.281314849853516
Epoch 990, val loss: 0.9702775478363037
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7185185185185186
0.8170795993674222
The final CL Acc:0.74938, 0.02188, The final GNN Acc:0.81181, 0.00569
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13256])
remove edge: torch.Size([2, 7906])
updated graph: torch.Size([2, 10606])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 19.131336212158203 = 1.9376258850097656 + 2.0 * 8.596855163574219
Epoch 0, val loss: 1.9433538913726807
Epoch 10, training loss: 19.1201171875 = 1.9271624088287354 + 2.0 * 8.596477508544922
Epoch 10, val loss: 1.932215929031372
Epoch 20, training loss: 19.099374771118164 = 1.9136868715286255 + 2.0 * 8.592844009399414
Epoch 20, val loss: 1.917481541633606
Epoch 30, training loss: 19.02826499938965 = 1.8949028253555298 + 2.0 * 8.566680908203125
Epoch 30, val loss: 1.8967375755310059
Epoch 40, training loss: 18.74008560180664 = 1.8721866607666016 + 2.0 * 8.43394947052002
Epoch 40, val loss: 1.8729679584503174
Epoch 50, training loss: 18.013198852539062 = 1.848034381866455 + 2.0 * 8.082582473754883
Epoch 50, val loss: 1.848615288734436
Epoch 60, training loss: 17.609142303466797 = 1.8256945610046387 + 2.0 * 7.891724109649658
Epoch 60, val loss: 1.8279180526733398
Epoch 70, training loss: 16.984375 = 1.807997703552246 + 2.0 * 7.588189125061035
Epoch 70, val loss: 1.8123167753219604
Epoch 80, training loss: 16.302766799926758 = 1.7988168001174927 + 2.0 * 7.251974582672119
Epoch 80, val loss: 1.8045639991760254
Epoch 90, training loss: 15.81814193725586 = 1.7910085916519165 + 2.0 * 7.013566493988037
Epoch 90, val loss: 1.7961448431015015
Epoch 100, training loss: 15.481565475463867 = 1.7787495851516724 + 2.0 * 6.851408004760742
Epoch 100, val loss: 1.7841392755508423
Epoch 110, training loss: 15.278810501098633 = 1.7655127048492432 + 2.0 * 6.756649017333984
Epoch 110, val loss: 1.7715767621994019
Epoch 120, training loss: 15.143010139465332 = 1.7519704103469849 + 2.0 * 6.695519924163818
Epoch 120, val loss: 1.7589035034179688
Epoch 130, training loss: 15.025415420532227 = 1.737709403038025 + 2.0 * 6.643853187561035
Epoch 130, val loss: 1.7457003593444824
Epoch 140, training loss: 14.927661895751953 = 1.7217144966125488 + 2.0 * 6.602973937988281
Epoch 140, val loss: 1.7312051057815552
Epoch 150, training loss: 14.85036849975586 = 1.7035555839538574 + 2.0 * 6.57340669631958
Epoch 150, val loss: 1.7150431871414185
Epoch 160, training loss: 14.779276847839355 = 1.6833661794662476 + 2.0 * 6.547955513000488
Epoch 160, val loss: 1.697327733039856
Epoch 170, training loss: 14.717479705810547 = 1.660889744758606 + 2.0 * 6.528295040130615
Epoch 170, val loss: 1.6776762008666992
Epoch 180, training loss: 14.659192085266113 = 1.635824203491211 + 2.0 * 6.511683940887451
Epoch 180, val loss: 1.6558449268341064
Epoch 190, training loss: 14.602799415588379 = 1.608099341392517 + 2.0 * 6.497350215911865
Epoch 190, val loss: 1.6317895650863647
Epoch 200, training loss: 14.54935359954834 = 1.5781371593475342 + 2.0 * 6.485608100891113
Epoch 200, val loss: 1.6059335470199585
Epoch 210, training loss: 14.490500450134277 = 1.5457839965820312 + 2.0 * 6.472358226776123
Epoch 210, val loss: 1.5781091451644897
Epoch 220, training loss: 14.436928749084473 = 1.511130452156067 + 2.0 * 6.462899208068848
Epoch 220, val loss: 1.5484938621520996
Epoch 230, training loss: 14.376313209533691 = 1.4749563932418823 + 2.0 * 6.45067834854126
Epoch 230, val loss: 1.5178817510604858
Epoch 240, training loss: 14.32793140411377 = 1.437080979347229 + 2.0 * 6.445425033569336
Epoch 240, val loss: 1.4862498044967651
Epoch 250, training loss: 14.264957427978516 = 1.3983440399169922 + 2.0 * 6.433306694030762
Epoch 250, val loss: 1.4542884826660156
Epoch 260, training loss: 14.207324981689453 = 1.3589136600494385 + 2.0 * 6.424205780029297
Epoch 260, val loss: 1.4221792221069336
Epoch 270, training loss: 14.159343719482422 = 1.3188552856445312 + 2.0 * 6.420244216918945
Epoch 270, val loss: 1.3901281356811523
Epoch 280, training loss: 14.098405838012695 = 1.2787539958953857 + 2.0 * 6.409825801849365
Epoch 280, val loss: 1.3584493398666382
Epoch 290, training loss: 14.04589557647705 = 1.2387008666992188 + 2.0 * 6.403597354888916
Epoch 290, val loss: 1.3271520137786865
Epoch 300, training loss: 14.008041381835938 = 1.1987346410751343 + 2.0 * 6.404653549194336
Epoch 300, val loss: 1.2962983846664429
Epoch 310, training loss: 13.948600769042969 = 1.1594338417053223 + 2.0 * 6.394583225250244
Epoch 310, val loss: 1.2663042545318604
Epoch 320, training loss: 13.898086547851562 = 1.1208726167678833 + 2.0 * 6.388607025146484
Epoch 320, val loss: 1.237186074256897
Epoch 330, training loss: 13.851676940917969 = 1.0829423666000366 + 2.0 * 6.3843674659729
Epoch 330, val loss: 1.2087361812591553
Epoch 340, training loss: 13.819522857666016 = 1.0458258390426636 + 2.0 * 6.386848449707031
Epoch 340, val loss: 1.1810648441314697
Epoch 350, training loss: 13.769527435302734 = 1.0099380016326904 + 2.0 * 6.379794597625732
Epoch 350, val loss: 1.1543283462524414
Epoch 360, training loss: 13.724132537841797 = 0.9751350283622742 + 2.0 * 6.3744988441467285
Epoch 360, val loss: 1.1286022663116455
Epoch 370, training loss: 13.683216094970703 = 0.9411938786506653 + 2.0 * 6.371011257171631
Epoch 370, val loss: 1.1036615371704102
Epoch 380, training loss: 13.651784896850586 = 0.9080830812454224 + 2.0 * 6.371850967407227
Epoch 380, val loss: 1.0794121026992798
Epoch 390, training loss: 13.608931541442871 = 0.8759574294090271 + 2.0 * 6.3664870262146
Epoch 390, val loss: 1.056011438369751
Epoch 400, training loss: 13.570685386657715 = 0.844943642616272 + 2.0 * 6.362870693206787
Epoch 400, val loss: 1.0334651470184326
Epoch 410, training loss: 13.534488677978516 = 0.8147449493408203 + 2.0 * 6.359871864318848
Epoch 410, val loss: 1.0116710662841797
Epoch 420, training loss: 13.49935531616211 = 0.785243034362793 + 2.0 * 6.357056140899658
Epoch 420, val loss: 0.9905745983123779
Epoch 430, training loss: 13.466768264770508 = 0.7564300894737244 + 2.0 * 6.355169296264648
Epoch 430, val loss: 0.9701631665229797
Epoch 440, training loss: 13.440389633178711 = 0.7284903526306152 + 2.0 * 6.355949878692627
Epoch 440, val loss: 0.9505528211593628
Epoch 450, training loss: 13.403793334960938 = 0.7016435861587524 + 2.0 * 6.351074695587158
Epoch 450, val loss: 0.9321272969245911
Epoch 460, training loss: 13.374550819396973 = 0.67578125 + 2.0 * 6.349384784698486
Epoch 460, val loss: 0.9147135019302368
Epoch 470, training loss: 13.343975067138672 = 0.6509914994239807 + 2.0 * 6.346491813659668
Epoch 470, val loss: 0.8983912467956543
Epoch 480, training loss: 13.316385269165039 = 0.627336323261261 + 2.0 * 6.344524383544922
Epoch 480, val loss: 0.883210301399231
Epoch 490, training loss: 13.287352561950684 = 0.6046507358551025 + 2.0 * 6.34135103225708
Epoch 490, val loss: 0.8691967129707336
Epoch 500, training loss: 13.263818740844727 = 0.5828679800033569 + 2.0 * 6.340475559234619
Epoch 500, val loss: 0.8560465574264526
Epoch 510, training loss: 13.240301132202148 = 0.5619235634803772 + 2.0 * 6.339188575744629
Epoch 510, val loss: 0.843660295009613
Epoch 520, training loss: 13.209465026855469 = 0.5418131947517395 + 2.0 * 6.333826065063477
Epoch 520, val loss: 0.8322396278381348
Epoch 530, training loss: 13.187667846679688 = 0.5222491025924683 + 2.0 * 6.332709312438965
Epoch 530, val loss: 0.8214229941368103
Epoch 540, training loss: 13.176664352416992 = 0.503071665763855 + 2.0 * 6.336796283721924
Epoch 540, val loss: 0.8109797835350037
Epoch 550, training loss: 13.142646789550781 = 0.48424232006073 + 2.0 * 6.329202175140381
Epoch 550, val loss: 0.8010265231132507
Epoch 560, training loss: 13.1177978515625 = 0.4656554162502289 + 2.0 * 6.326071262359619
Epoch 560, val loss: 0.7913901805877686
Epoch 570, training loss: 13.104249954223633 = 0.44707345962524414 + 2.0 * 6.328588008880615
Epoch 570, val loss: 0.7820300459861755
Epoch 580, training loss: 13.079178810119629 = 0.42848697304725647 + 2.0 * 6.325345993041992
Epoch 580, val loss: 0.7727470397949219
Epoch 590, training loss: 13.050790786743164 = 0.4098338186740875 + 2.0 * 6.320478439331055
Epoch 590, val loss: 0.7638510465621948
Epoch 600, training loss: 13.029147148132324 = 0.39107513427734375 + 2.0 * 6.31903600692749
Epoch 600, val loss: 0.7550697326660156
Epoch 610, training loss: 13.020503044128418 = 0.37230241298675537 + 2.0 * 6.324100494384766
Epoch 610, val loss: 0.7464368939399719
Epoch 620, training loss: 12.991657257080078 = 0.35371309518814087 + 2.0 * 6.318972110748291
Epoch 620, val loss: 0.7382170557975769
Epoch 630, training loss: 12.967196464538574 = 0.3353215754032135 + 2.0 * 6.315937519073486
Epoch 630, val loss: 0.7303810119628906
Epoch 640, training loss: 12.955047607421875 = 0.31721314787864685 + 2.0 * 6.318917274475098
Epoch 640, val loss: 0.722834587097168
Epoch 650, training loss: 12.92506217956543 = 0.29963919520378113 + 2.0 * 6.312711715698242
Epoch 650, val loss: 0.7158217430114746
Epoch 660, training loss: 12.903883934020996 = 0.2825915515422821 + 2.0 * 6.310646057128906
Epoch 660, val loss: 0.7093537449836731
Epoch 670, training loss: 12.894304275512695 = 0.2661682665348053 + 2.0 * 6.314067840576172
Epoch 670, val loss: 0.7033910155296326
Epoch 680, training loss: 12.870302200317383 = 0.2505471706390381 + 2.0 * 6.309877395629883
Epoch 680, val loss: 0.6981707811355591
Epoch 690, training loss: 12.848158836364746 = 0.23567414283752441 + 2.0 * 6.3062424659729
Epoch 690, val loss: 0.6935826539993286
Epoch 700, training loss: 12.833418846130371 = 0.2215760052204132 + 2.0 * 6.30592155456543
Epoch 700, val loss: 0.6895837187767029
Epoch 710, training loss: 12.827735900878906 = 0.208266019821167 + 2.0 * 6.30973482131958
Epoch 710, val loss: 0.6862511038780212
Epoch 720, training loss: 12.814871788024902 = 0.19580696523189545 + 2.0 * 6.309532642364502
Epoch 720, val loss: 0.6836987733840942
Epoch 730, training loss: 12.789481163024902 = 0.18418018519878387 + 2.0 * 6.302650451660156
Epoch 730, val loss: 0.6818292140960693
Epoch 740, training loss: 12.775551795959473 = 0.17335177958011627 + 2.0 * 6.30109977722168
Epoch 740, val loss: 0.6805822253227234
Epoch 750, training loss: 12.772189140319824 = 0.16326618194580078 + 2.0 * 6.304461479187012
Epoch 750, val loss: 0.6799137592315674
Epoch 760, training loss: 12.76023006439209 = 0.15396232903003693 + 2.0 * 6.303133964538574
Epoch 760, val loss: 0.6797492504119873
Epoch 770, training loss: 12.740979194641113 = 0.14533528685569763 + 2.0 * 6.297821998596191
Epoch 770, val loss: 0.6801335215568542
Epoch 780, training loss: 12.733770370483398 = 0.13734059035778046 + 2.0 * 6.298214912414551
Epoch 780, val loss: 0.6809174418449402
Epoch 790, training loss: 12.722981452941895 = 0.12994670867919922 + 2.0 * 6.296517372131348
Epoch 790, val loss: 0.6821256875991821
Epoch 800, training loss: 12.711827278137207 = 0.12308312952518463 + 2.0 * 6.294372081756592
Epoch 800, val loss: 0.6837720274925232
Epoch 810, training loss: 12.704416275024414 = 0.11670573055744171 + 2.0 * 6.2938551902771
Epoch 810, val loss: 0.6857258081436157
Epoch 820, training loss: 12.706271171569824 = 0.11077579110860825 + 2.0 * 6.297747611999512
Epoch 820, val loss: 0.6879581809043884
Epoch 830, training loss: 12.691116333007812 = 0.10527724027633667 + 2.0 * 6.292919635772705
Epoch 830, val loss: 0.6905035972595215
Epoch 840, training loss: 12.705686569213867 = 0.10014378279447556 + 2.0 * 6.30277156829834
Epoch 840, val loss: 0.6932606101036072
Epoch 850, training loss: 12.680290222167969 = 0.09538000822067261 + 2.0 * 6.292455196380615
Epoch 850, val loss: 0.696262538433075
Epoch 860, training loss: 12.671767234802246 = 0.09095407277345657 + 2.0 * 6.290406703948975
Epoch 860, val loss: 0.699444591999054
Epoch 870, training loss: 12.661384582519531 = 0.08680000901222229 + 2.0 * 6.28729248046875
Epoch 870, val loss: 0.7027433514595032
Epoch 880, training loss: 12.656062126159668 = 0.08289363235235214 + 2.0 * 6.286584377288818
Epoch 880, val loss: 0.7062047719955444
Epoch 890, training loss: 12.672338485717773 = 0.07921754568815231 + 2.0 * 6.296560287475586
Epoch 890, val loss: 0.7097800374031067
Epoch 900, training loss: 12.650471687316895 = 0.07577928900718689 + 2.0 * 6.287346363067627
Epoch 900, val loss: 0.7135015726089478
Epoch 910, training loss: 12.643266677856445 = 0.072554811835289 + 2.0 * 6.285356044769287
Epoch 910, val loss: 0.7172823548316956
Epoch 920, training loss: 12.638362884521484 = 0.06951583921909332 + 2.0 * 6.284423351287842
Epoch 920, val loss: 0.7210865020751953
Epoch 930, training loss: 12.64013671875 = 0.06665186583995819 + 2.0 * 6.286742210388184
Epoch 930, val loss: 0.7249955534934998
Epoch 940, training loss: 12.629490852355957 = 0.06394749134778976 + 2.0 * 6.282771587371826
Epoch 940, val loss: 0.7289597392082214
Epoch 950, training loss: 12.624919891357422 = 0.06140455976128578 + 2.0 * 6.281757831573486
Epoch 950, val loss: 0.7329609990119934
Epoch 960, training loss: 12.619596481323242 = 0.05900714918971062 + 2.0 * 6.280294895172119
Epoch 960, val loss: 0.7369704842567444
Epoch 970, training loss: 12.61503791809082 = 0.05672525241971016 + 2.0 * 6.27915620803833
Epoch 970, val loss: 0.7410333752632141
Epoch 980, training loss: 12.625907897949219 = 0.05456291139125824 + 2.0 * 6.285672664642334
Epoch 980, val loss: 0.7451321482658386
Epoch 990, training loss: 12.620552062988281 = 0.05252503976225853 + 2.0 * 6.284013748168945
Epoch 990, val loss: 0.7492682337760925
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8111111111111111
0.8402741170268846
=== training gcn model ===
Epoch 0, training loss: 19.13743782043457 = 1.9437177181243896 + 2.0 * 8.5968599319458
Epoch 0, val loss: 1.9374470710754395
Epoch 10, training loss: 19.126903533935547 = 1.93374764919281 + 2.0 * 8.596577644348145
Epoch 10, val loss: 1.9274897575378418
Epoch 20, training loss: 19.110450744628906 = 1.9219882488250732 + 2.0 * 8.594231605529785
Epoch 20, val loss: 1.9156509637832642
Epoch 30, training loss: 19.053617477416992 = 1.9066766500473022 + 2.0 * 8.573470115661621
Epoch 30, val loss: 1.9003313779830933
Epoch 40, training loss: 18.684797286987305 = 1.8877497911453247 + 2.0 * 8.398523330688477
Epoch 40, val loss: 1.8817428350448608
Epoch 50, training loss: 17.102840423583984 = 1.866711974143982 + 2.0 * 7.6180644035339355
Epoch 50, val loss: 1.8620470762252808
Epoch 60, training loss: 16.344106674194336 = 1.85164475440979 + 2.0 * 7.246230602264404
Epoch 60, val loss: 1.8483506441116333
Epoch 70, training loss: 15.866954803466797 = 1.8379297256469727 + 2.0 * 7.014512538909912
Epoch 70, val loss: 1.8355612754821777
Epoch 80, training loss: 15.60550308227539 = 1.8245142698287964 + 2.0 * 6.890494346618652
Epoch 80, val loss: 1.8231711387634277
Epoch 90, training loss: 15.441503524780273 = 1.810295581817627 + 2.0 * 6.815604209899902
Epoch 90, val loss: 1.8104294538497925
Epoch 100, training loss: 15.33056926727295 = 1.7958158254623413 + 2.0 * 6.767376899719238
Epoch 100, val loss: 1.7976633310317993
Epoch 110, training loss: 15.221907615661621 = 1.7812260389328003 + 2.0 * 6.720340728759766
Epoch 110, val loss: 1.7849044799804688
Epoch 120, training loss: 15.118574142456055 = 1.766867756843567 + 2.0 * 6.675853252410889
Epoch 120, val loss: 1.7721110582351685
Epoch 130, training loss: 15.019447326660156 = 1.7518811225891113 + 2.0 * 6.633783340454102
Epoch 130, val loss: 1.7588310241699219
Epoch 140, training loss: 14.935224533081055 = 1.7356284856796265 + 2.0 * 6.599798202514648
Epoch 140, val loss: 1.7447019815444946
Epoch 150, training loss: 14.858570098876953 = 1.7177555561065674 + 2.0 * 6.570407390594482
Epoch 150, val loss: 1.7294203042984009
Epoch 160, training loss: 14.788613319396973 = 1.6977421045303345 + 2.0 * 6.545435428619385
Epoch 160, val loss: 1.7124933004379272
Epoch 170, training loss: 14.721684455871582 = 1.6750954389572144 + 2.0 * 6.523294448852539
Epoch 170, val loss: 1.693696141242981
Epoch 180, training loss: 14.662711143493652 = 1.6494148969650269 + 2.0 * 6.506648063659668
Epoch 180, val loss: 1.6724960803985596
Epoch 190, training loss: 14.599271774291992 = 1.6206234693527222 + 2.0 * 6.48932409286499
Epoch 190, val loss: 1.6487091779708862
Epoch 200, training loss: 14.535881042480469 = 1.58841073513031 + 2.0 * 6.473735332489014
Epoch 200, val loss: 1.622288703918457
Epoch 210, training loss: 14.475014686584473 = 1.552511215209961 + 2.0 * 6.461251735687256
Epoch 210, val loss: 1.592885971069336
Epoch 220, training loss: 14.421183586120605 = 1.5130956172943115 + 2.0 * 6.454043865203857
Epoch 220, val loss: 1.560800552368164
Epoch 230, training loss: 14.352920532226562 = 1.4707210063934326 + 2.0 * 6.441099643707275
Epoch 230, val loss: 1.526458978652954
Epoch 240, training loss: 14.288795471191406 = 1.4253993034362793 + 2.0 * 6.431698322296143
Epoch 240, val loss: 1.489949107170105
Epoch 250, training loss: 14.243308067321777 = 1.3777713775634766 + 2.0 * 6.43276834487915
Epoch 250, val loss: 1.4517898559570312
Epoch 260, training loss: 14.165037155151367 = 1.3290623426437378 + 2.0 * 6.41798734664917
Epoch 260, val loss: 1.4132492542266846
Epoch 270, training loss: 14.100761413574219 = 1.2796483039855957 + 2.0 * 6.410556316375732
Epoch 270, val loss: 1.3744986057281494
Epoch 280, training loss: 14.037347793579102 = 1.2296714782714844 + 2.0 * 6.403838157653809
Epoch 280, val loss: 1.3357263803482056
Epoch 290, training loss: 13.987881660461426 = 1.1797659397125244 + 2.0 * 6.40405797958374
Epoch 290, val loss: 1.2974897623062134
Epoch 300, training loss: 13.920464515686035 = 1.1311311721801758 + 2.0 * 6.39466667175293
Epoch 300, val loss: 1.2606433629989624
Epoch 310, training loss: 13.859539031982422 = 1.0837396383285522 + 2.0 * 6.387899875640869
Epoch 310, val loss: 1.2252463102340698
Epoch 320, training loss: 13.80272388458252 = 1.0375001430511475 + 2.0 * 6.3826117515563965
Epoch 320, val loss: 1.1911101341247559
Epoch 330, training loss: 13.759482383728027 = 0.9925029277801514 + 2.0 * 6.383489608764648
Epoch 330, val loss: 1.1583306789398193
Epoch 340, training loss: 13.704161643981934 = 0.9492920637130737 + 2.0 * 6.377434730529785
Epoch 340, val loss: 1.1272071599960327
Epoch 350, training loss: 13.652115821838379 = 0.9082470536231995 + 2.0 * 6.371934413909912
Epoch 350, val loss: 1.097948670387268
Epoch 360, training loss: 13.60280704498291 = 0.8691036105155945 + 2.0 * 6.366851806640625
Epoch 360, val loss: 1.0707038640975952
Epoch 370, training loss: 13.569209098815918 = 0.8318478465080261 + 2.0 * 6.368680477142334
Epoch 370, val loss: 1.04520583152771
Epoch 380, training loss: 13.518634796142578 = 0.7966390252113342 + 2.0 * 6.360997676849365
Epoch 380, val loss: 1.0216851234436035
Epoch 390, training loss: 13.476398468017578 = 0.7634645700454712 + 2.0 * 6.356466770172119
Epoch 390, val loss: 0.9999603033065796
Epoch 400, training loss: 13.438750267028809 = 0.7319661974906921 + 2.0 * 6.353392124176025
Epoch 400, val loss: 0.9800022840499878
Epoch 410, training loss: 13.416735649108887 = 0.7020889520645142 + 2.0 * 6.357323169708252
Epoch 410, val loss: 0.9614958763122559
Epoch 420, training loss: 13.374255180358887 = 0.673905611038208 + 2.0 * 6.350174903869629
Epoch 420, val loss: 0.9445211291313171
Epoch 430, training loss: 13.348556518554688 = 0.64713054895401 + 2.0 * 6.350712776184082
Epoch 430, val loss: 0.9289267063140869
Epoch 440, training loss: 13.309309959411621 = 0.6216946244239807 + 2.0 * 6.343807697296143
Epoch 440, val loss: 0.9144706726074219
Epoch 450, training loss: 13.276522636413574 = 0.597248375415802 + 2.0 * 6.339637279510498
Epoch 450, val loss: 0.9010802507400513
Epoch 460, training loss: 13.247777938842773 = 0.5736773610115051 + 2.0 * 6.337050437927246
Epoch 460, val loss: 0.8885847330093384
Epoch 470, training loss: 13.23501968383789 = 0.5509185194969177 + 2.0 * 6.342050552368164
Epoch 470, val loss: 0.8768352270126343
Epoch 480, training loss: 13.198122024536133 = 0.5290347337722778 + 2.0 * 6.334543704986572
Epoch 480, val loss: 0.8658902645111084
Epoch 490, training loss: 13.17241096496582 = 0.5079259872436523 + 2.0 * 6.332242488861084
Epoch 490, val loss: 0.8556857109069824
Epoch 500, training loss: 13.145831108093262 = 0.48753252625465393 + 2.0 * 6.32914924621582
Epoch 500, val loss: 0.8462120890617371
Epoch 510, training loss: 13.121070861816406 = 0.4678886830806732 + 2.0 * 6.3265910148620605
Epoch 510, val loss: 0.8374171257019043
Epoch 520, training loss: 13.1040620803833 = 0.4488680958747864 + 2.0 * 6.327597141265869
Epoch 520, val loss: 0.8293160796165466
Epoch 530, training loss: 13.083033561706543 = 0.4305431842803955 + 2.0 * 6.326245307922363
Epoch 530, val loss: 0.8217652440071106
Epoch 540, training loss: 13.05595874786377 = 0.4128960072994232 + 2.0 * 6.321531295776367
Epoch 540, val loss: 0.8148367404937744
Epoch 550, training loss: 13.039729118347168 = 0.3958474397659302 + 2.0 * 6.321940898895264
Epoch 550, val loss: 0.8084951043128967
Epoch 560, training loss: 13.020552635192871 = 0.37937647104263306 + 2.0 * 6.320588111877441
Epoch 560, val loss: 0.8026416897773743
Epoch 570, training loss: 12.999133110046387 = 0.3634733259677887 + 2.0 * 6.3178300857543945
Epoch 570, val loss: 0.7973186373710632
Epoch 580, training loss: 12.976181983947754 = 0.34814268350601196 + 2.0 * 6.314019680023193
Epoch 580, val loss: 0.7923595309257507
Epoch 590, training loss: 12.966184616088867 = 0.3333037495613098 + 2.0 * 6.316440582275391
Epoch 590, val loss: 0.7878221869468689
Epoch 600, training loss: 12.94416618347168 = 0.3189677298069 + 2.0 * 6.312599182128906
Epoch 600, val loss: 0.7836454510688782
Epoch 610, training loss: 12.926692008972168 = 0.305136114358902 + 2.0 * 6.3107781410217285
Epoch 610, val loss: 0.7798962593078613
Epoch 620, training loss: 12.9114990234375 = 0.29177191853523254 + 2.0 * 6.309863567352295
Epoch 620, val loss: 0.77653568983078
Epoch 630, training loss: 12.892608642578125 = 0.2788679301738739 + 2.0 * 6.306870460510254
Epoch 630, val loss: 0.7735603451728821
Epoch 640, training loss: 12.88070011138916 = 0.26652175188064575 + 2.0 * 6.307089328765869
Epoch 640, val loss: 0.7708523869514465
Epoch 650, training loss: 12.865184783935547 = 0.25463780760765076 + 2.0 * 6.305273532867432
Epoch 650, val loss: 0.7685596942901611
Epoch 660, training loss: 12.850456237792969 = 0.24318228662014008 + 2.0 * 6.3036370277404785
Epoch 660, val loss: 0.7666310667991638
Epoch 670, training loss: 12.844917297363281 = 0.2321772575378418 + 2.0 * 6.306370258331299
Epoch 670, val loss: 0.7650755047798157
Epoch 680, training loss: 12.825778007507324 = 0.22167621552944183 + 2.0 * 6.302051067352295
Epoch 680, val loss: 0.7636874914169312
Epoch 690, training loss: 12.810995101928711 = 0.21165797114372253 + 2.0 * 6.299668788909912
Epoch 690, val loss: 0.7627296447753906
Epoch 700, training loss: 12.798744201660156 = 0.20202279090881348 + 2.0 * 6.298360824584961
Epoch 700, val loss: 0.7621110677719116
Epoch 710, training loss: 12.790529251098633 = 0.19275933504104614 + 2.0 * 6.298884868621826
Epoch 710, val loss: 0.761796236038208
Epoch 720, training loss: 12.79519271850586 = 0.18389947712421417 + 2.0 * 6.3056464195251465
Epoch 720, val loss: 0.761891782283783
Epoch 730, training loss: 12.76943588256836 = 0.175472691655159 + 2.0 * 6.2969818115234375
Epoch 730, val loss: 0.7621182203292847
Epoch 740, training loss: 12.758435249328613 = 0.16745968163013458 + 2.0 * 6.295487880706787
Epoch 740, val loss: 0.7627211809158325
Epoch 750, training loss: 12.745840072631836 = 0.15981020033359528 + 2.0 * 6.293015003204346
Epoch 750, val loss: 0.7636653780937195
Epoch 760, training loss: 12.737324714660645 = 0.15249945223331451 + 2.0 * 6.292412757873535
Epoch 760, val loss: 0.7649106383323669
Epoch 770, training loss: 12.741040229797363 = 0.1455395370721817 + 2.0 * 6.297750473022461
Epoch 770, val loss: 0.7664086818695068
Epoch 780, training loss: 12.728392601013184 = 0.13896584510803223 + 2.0 * 6.294713497161865
Epoch 780, val loss: 0.7680608034133911
Epoch 790, training loss: 12.712865829467773 = 0.13274593651294708 + 2.0 * 6.290060043334961
Epoch 790, val loss: 0.7699673771858215
Epoch 800, training loss: 12.703500747680664 = 0.12682510912418365 + 2.0 * 6.288337707519531
Epoch 800, val loss: 0.7721618413925171
Epoch 810, training loss: 12.705288887023926 = 0.12118524312973022 + 2.0 * 6.292051792144775
Epoch 810, val loss: 0.7746009230613708
Epoch 820, training loss: 12.692506790161133 = 0.1158272922039032 + 2.0 * 6.288339614868164
Epoch 820, val loss: 0.7771591544151306
Epoch 830, training loss: 12.685483932495117 = 0.11074994504451752 + 2.0 * 6.28736686706543
Epoch 830, val loss: 0.7799376845359802
Epoch 840, training loss: 12.68229866027832 = 0.10593123733997345 + 2.0 * 6.288183689117432
Epoch 840, val loss: 0.7828775644302368
Epoch 850, training loss: 12.671706199645996 = 0.10134761035442352 + 2.0 * 6.285179138183594
Epoch 850, val loss: 0.7860288619995117
Epoch 860, training loss: 12.664034843444824 = 0.09700892865657806 + 2.0 * 6.283513069152832
Epoch 860, val loss: 0.7892665266990662
Epoch 870, training loss: 12.65869426727295 = 0.09289155900478363 + 2.0 * 6.282901287078857
Epoch 870, val loss: 0.792659342288971
Epoch 880, training loss: 12.658358573913574 = 0.08898314088582993 + 2.0 * 6.284687519073486
Epoch 880, val loss: 0.7961999177932739
Epoch 890, training loss: 12.64675521850586 = 0.08527971804141998 + 2.0 * 6.28073787689209
Epoch 890, val loss: 0.7998084425926208
Epoch 900, training loss: 12.660050392150879 = 0.08176117390394211 + 2.0 * 6.289144515991211
Epoch 900, val loss: 0.80356764793396
Epoch 910, training loss: 12.645923614501953 = 0.07843048870563507 + 2.0 * 6.283746719360352
Epoch 910, val loss: 0.8072818517684937
Epoch 920, training loss: 12.631486892700195 = 0.07528217136859894 + 2.0 * 6.278102397918701
Epoch 920, val loss: 0.8111293911933899
Epoch 930, training loss: 12.627582550048828 = 0.07229258865118027 + 2.0 * 6.277645111083984
Epoch 930, val loss: 0.815091609954834
Epoch 940, training loss: 12.627376556396484 = 0.06945205479860306 + 2.0 * 6.278962135314941
Epoch 940, val loss: 0.819115161895752
Epoch 950, training loss: 12.618895530700684 = 0.0667436495423317 + 2.0 * 6.276075839996338
Epoch 950, val loss: 0.8232041597366333
Epoch 960, training loss: 12.62109088897705 = 0.06418774276971817 + 2.0 * 6.278451442718506
Epoch 960, val loss: 0.8272421360015869
Epoch 970, training loss: 12.612679481506348 = 0.061757270246744156 + 2.0 * 6.275461196899414
Epoch 970, val loss: 0.8313891887664795
Epoch 980, training loss: 12.618943214416504 = 0.05944602191448212 + 2.0 * 6.279748439788818
Epoch 980, val loss: 0.8356108069419861
Epoch 990, training loss: 12.605047225952148 = 0.05724926292896271 + 2.0 * 6.273899078369141
Epoch 990, val loss: 0.8397930860519409
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8111111111111111
0.845018450184502
=== training gcn model ===
Epoch 0, training loss: 19.152759552001953 = 1.9590574502944946 + 2.0 * 8.596851348876953
Epoch 0, val loss: 1.9715569019317627
Epoch 10, training loss: 19.141529083251953 = 1.9485985040664673 + 2.0 * 8.596465110778809
Epoch 10, val loss: 1.9608640670776367
Epoch 20, training loss: 19.121057510375977 = 1.9358291625976562 + 2.0 * 8.59261417388916
Epoch 20, val loss: 1.947648525238037
Epoch 30, training loss: 19.041645050048828 = 1.918891191482544 + 2.0 * 8.561376571655273
Epoch 30, val loss: 1.930314540863037
Epoch 40, training loss: 18.596332550048828 = 1.8987339735031128 + 2.0 * 8.348799705505371
Epoch 40, val loss: 1.9105260372161865
Epoch 50, training loss: 17.472286224365234 = 1.8779841661453247 + 2.0 * 7.7971510887146
Epoch 50, val loss: 1.8909869194030762
Epoch 60, training loss: 16.690513610839844 = 1.8621820211410522 + 2.0 * 7.41416597366333
Epoch 60, val loss: 1.876068115234375
Epoch 70, training loss: 16.148313522338867 = 1.8478388786315918 + 2.0 * 7.150237560272217
Epoch 70, val loss: 1.8616429567337036
Epoch 80, training loss: 15.759072303771973 = 1.8332897424697876 + 2.0 * 6.962891101837158
Epoch 80, val loss: 1.8467495441436768
Epoch 90, training loss: 15.527602195739746 = 1.8174629211425781 + 2.0 * 6.855069637298584
Epoch 90, val loss: 1.8305805921554565
Epoch 100, training loss: 15.365592002868652 = 1.8000456094741821 + 2.0 * 6.782773017883301
Epoch 100, val loss: 1.8132505416870117
Epoch 110, training loss: 15.238253593444824 = 1.7828913927078247 + 2.0 * 6.7276811599731445
Epoch 110, val loss: 1.7961827516555786
Epoch 120, training loss: 15.126996040344238 = 1.766577124595642 + 2.0 * 6.680209636688232
Epoch 120, val loss: 1.7797859907150269
Epoch 130, training loss: 15.037054061889648 = 1.7501715421676636 + 2.0 * 6.643441200256348
Epoch 130, val loss: 1.7634756565093994
Epoch 140, training loss: 14.950713157653809 = 1.7330909967422485 + 2.0 * 6.608810901641846
Epoch 140, val loss: 1.746803879737854
Epoch 150, training loss: 14.870843887329102 = 1.7149593830108643 + 2.0 * 6.577942371368408
Epoch 150, val loss: 1.7296251058578491
Epoch 160, training loss: 14.799245834350586 = 1.6953449249267578 + 2.0 * 6.551950454711914
Epoch 160, val loss: 1.7115395069122314
Epoch 170, training loss: 14.734916687011719 = 1.6737384796142578 + 2.0 * 6.5305891036987305
Epoch 170, val loss: 1.6919167041778564
Epoch 180, training loss: 14.680899620056152 = 1.6495931148529053 + 2.0 * 6.515653133392334
Epoch 180, val loss: 1.6702725887298584
Epoch 190, training loss: 14.623313903808594 = 1.622881531715393 + 2.0 * 6.500216007232666
Epoch 190, val loss: 1.6465660333633423
Epoch 200, training loss: 14.56348991394043 = 1.5935910940170288 + 2.0 * 6.484949588775635
Epoch 200, val loss: 1.620631456375122
Epoch 210, training loss: 14.507164001464844 = 1.5614418983459473 + 2.0 * 6.472860813140869
Epoch 210, val loss: 1.5923516750335693
Epoch 220, training loss: 14.462099075317383 = 1.5262895822525024 + 2.0 * 6.467904567718506
Epoch 220, val loss: 1.5616153478622437
Epoch 230, training loss: 14.394332885742188 = 1.4885432720184326 + 2.0 * 6.452894687652588
Epoch 230, val loss: 1.528954029083252
Epoch 240, training loss: 14.334650993347168 = 1.448269009590149 + 2.0 * 6.443191051483154
Epoch 240, val loss: 1.4943645000457764
Epoch 250, training loss: 14.276779174804688 = 1.4054718017578125 + 2.0 * 6.4356536865234375
Epoch 250, val loss: 1.4578821659088135
Epoch 260, training loss: 14.222379684448242 = 1.3606572151184082 + 2.0 * 6.430861473083496
Epoch 260, val loss: 1.4199273586273193
Epoch 270, training loss: 14.153817176818848 = 1.3145774602890015 + 2.0 * 6.419620037078857
Epoch 270, val loss: 1.3814325332641602
Epoch 280, training loss: 14.09317684173584 = 1.2677973508834839 + 2.0 * 6.412689685821533
Epoch 280, val loss: 1.3426105976104736
Epoch 290, training loss: 14.04034423828125 = 1.2214525938034058 + 2.0 * 6.409445762634277
Epoch 290, val loss: 1.304878830909729
Epoch 300, training loss: 13.98105239868164 = 1.1767195463180542 + 2.0 * 6.402166366577148
Epoch 300, val loss: 1.2688236236572266
Epoch 310, training loss: 13.925183296203613 = 1.1334925889968872 + 2.0 * 6.395845413208008
Epoch 310, val loss: 1.2344639301300049
Epoch 320, training loss: 13.87203311920166 = 1.0917445421218872 + 2.0 * 6.390144348144531
Epoch 320, val loss: 1.2016925811767578
Epoch 330, training loss: 13.846589088439941 = 1.051666021347046 + 2.0 * 6.397461414337158
Epoch 330, val loss: 1.170499324798584
Epoch 340, training loss: 13.786516189575195 = 1.0134968757629395 + 2.0 * 6.386509418487549
Epoch 340, val loss: 1.1413540840148926
Epoch 350, training loss: 13.732471466064453 = 0.9771403670310974 + 2.0 * 6.3776655197143555
Epoch 350, val loss: 1.1137105226516724
Epoch 360, training loss: 13.690469741821289 = 0.9420366883277893 + 2.0 * 6.374216556549072
Epoch 360, val loss: 1.087067723274231
Epoch 370, training loss: 13.649347305297852 = 0.9079052209854126 + 2.0 * 6.370720863342285
Epoch 370, val loss: 1.0611577033996582
Epoch 380, training loss: 13.612788200378418 = 0.8747085332870483 + 2.0 * 6.369040012359619
Epoch 380, val loss: 1.0359452962875366
Epoch 390, training loss: 13.570277214050293 = 0.842553973197937 + 2.0 * 6.363861560821533
Epoch 390, val loss: 1.0115467309951782
Epoch 400, training loss: 13.5328369140625 = 0.8113840222358704 + 2.0 * 6.360726356506348
Epoch 400, val loss: 0.9878852963447571
Epoch 410, training loss: 13.4970121383667 = 0.7810496091842651 + 2.0 * 6.357981204986572
Epoch 410, val loss: 0.9649155139923096
Epoch 420, training loss: 13.469212532043457 = 0.7516076564788818 + 2.0 * 6.358802318572998
Epoch 420, val loss: 0.9427525401115417
Epoch 430, training loss: 13.43332576751709 = 0.7232578992843628 + 2.0 * 6.355033874511719
Epoch 430, val loss: 0.9217451810836792
Epoch 440, training loss: 13.395750045776367 = 0.6958187818527222 + 2.0 * 6.349965572357178
Epoch 440, val loss: 0.901878833770752
Epoch 450, training loss: 13.363129615783691 = 0.6691763997077942 + 2.0 * 6.3469767570495605
Epoch 450, val loss: 0.8830454349517822
Epoch 460, training loss: 13.34505844116211 = 0.6432111859321594 + 2.0 * 6.350923538208008
Epoch 460, val loss: 0.8652893304824829
Epoch 470, training loss: 13.302546501159668 = 0.6180241107940674 + 2.0 * 6.34226131439209
Epoch 470, val loss: 0.8485422134399414
Epoch 480, training loss: 13.282282829284668 = 0.5935066938400269 + 2.0 * 6.344388008117676
Epoch 480, val loss: 0.8329252600669861
Epoch 490, training loss: 13.24737548828125 = 0.5694555044174194 + 2.0 * 6.33896017074585
Epoch 490, val loss: 0.8183817267417908
Epoch 500, training loss: 13.219595909118652 = 0.5459731817245483 + 2.0 * 6.336811542510986
Epoch 500, val loss: 0.8047010898590088
Epoch 510, training loss: 13.19518756866455 = 0.5229162573814392 + 2.0 * 6.3361358642578125
Epoch 510, val loss: 0.7919245362281799
Epoch 520, training loss: 13.178793907165527 = 0.5003241300582886 + 2.0 * 6.339234828948975
Epoch 520, val loss: 0.7799177169799805
Epoch 530, training loss: 13.142683029174805 = 0.47840607166290283 + 2.0 * 6.332138538360596
Epoch 530, val loss: 0.7688309550285339
Epoch 540, training loss: 13.1129732131958 = 0.45697566866874695 + 2.0 * 6.327998638153076
Epoch 540, val loss: 0.758623480796814
Epoch 550, training loss: 13.08997917175293 = 0.43608933687210083 + 2.0 * 6.326944828033447
Epoch 550, val loss: 0.7491738200187683
Epoch 560, training loss: 13.075061798095703 = 0.4158059358596802 + 2.0 * 6.329627990722656
Epoch 560, val loss: 0.7404594421386719
Epoch 570, training loss: 13.051874160766602 = 0.39634937047958374 + 2.0 * 6.327762603759766
Epoch 570, val loss: 0.7326256632804871
Epoch 580, training loss: 13.022821426391602 = 0.37765368819236755 + 2.0 * 6.3225836753845215
Epoch 580, val loss: 0.7256948947906494
Epoch 590, training loss: 12.998625755310059 = 0.3596627116203308 + 2.0 * 6.319481372833252
Epoch 590, val loss: 0.719535768032074
Epoch 600, training loss: 12.984931945800781 = 0.34235936403274536 + 2.0 * 6.321286201477051
Epoch 600, val loss: 0.714072048664093
Epoch 610, training loss: 12.960004806518555 = 0.3258889615535736 + 2.0 * 6.317058086395264
Epoch 610, val loss: 0.7092815041542053
Epoch 620, training loss: 12.941488265991211 = 0.3101414442062378 + 2.0 * 6.315673351287842
Epoch 620, val loss: 0.7053619623184204
Epoch 630, training loss: 12.9225435256958 = 0.2951371967792511 + 2.0 * 6.3137030601501465
Epoch 630, val loss: 0.7020763754844666
Epoch 640, training loss: 12.907483100891113 = 0.280808687210083 + 2.0 * 6.313337326049805
Epoch 640, val loss: 0.6993970274925232
Epoch 650, training loss: 12.895503044128418 = 0.2672351896762848 + 2.0 * 6.314134120941162
Epoch 650, val loss: 0.6971877217292786
Epoch 660, training loss: 12.872729301452637 = 0.2543421983718872 + 2.0 * 6.3091936111450195
Epoch 660, val loss: 0.6955716013908386
Epoch 670, training loss: 12.859875679016113 = 0.2421310395002365 + 2.0 * 6.308872222900391
Epoch 670, val loss: 0.6945736408233643
Epoch 680, training loss: 12.845671653747559 = 0.23053225874900818 + 2.0 * 6.30756950378418
Epoch 680, val loss: 0.6939752101898193
Epoch 690, training loss: 12.84201717376709 = 0.21950989961624146 + 2.0 * 6.311253547668457
Epoch 690, val loss: 0.6937324404716492
Epoch 700, training loss: 12.820943832397461 = 0.20912809669971466 + 2.0 * 6.305907726287842
Epoch 700, val loss: 0.6939577460289001
Epoch 710, training loss: 12.804997444152832 = 0.199215367436409 + 2.0 * 6.302891254425049
Epoch 710, val loss: 0.694546639919281
Epoch 720, training loss: 12.806406021118164 = 0.1898336261510849 + 2.0 * 6.308286190032959
Epoch 720, val loss: 0.6954343914985657
Epoch 730, training loss: 12.786786079406738 = 0.18098004162311554 + 2.0 * 6.302903175354004
Epoch 730, val loss: 0.6966948509216309
Epoch 740, training loss: 12.773429870605469 = 0.17259597778320312 + 2.0 * 6.300416946411133
Epoch 740, val loss: 0.6982263922691345
Epoch 750, training loss: 12.761821746826172 = 0.16467396914958954 + 2.0 * 6.298573970794678
Epoch 750, val loss: 0.7000353932380676
Epoch 760, training loss: 12.769103050231934 = 0.15719756484031677 + 2.0 * 6.305952548980713
Epoch 760, val loss: 0.7020564675331116
Epoch 770, training loss: 12.748778343200684 = 0.1500461995601654 + 2.0 * 6.299365997314453
Epoch 770, val loss: 0.7041469812393188
Epoch 780, training loss: 12.732821464538574 = 0.14337089657783508 + 2.0 * 6.29472541809082
Epoch 780, val loss: 0.7066503763198853
Epoch 790, training loss: 12.723433494567871 = 0.13702166080474854 + 2.0 * 6.293205738067627
Epoch 790, val loss: 0.7093129754066467
Epoch 800, training loss: 12.715980529785156 = 0.1310010403394699 + 2.0 * 6.292489528656006
Epoch 800, val loss: 0.7121317982673645
Epoch 810, training loss: 12.728378295898438 = 0.12527823448181152 + 2.0 * 6.301549911499023
Epoch 810, val loss: 0.7150633931159973
Epoch 820, training loss: 12.707879066467285 = 0.11989208310842514 + 2.0 * 6.2939934730529785
Epoch 820, val loss: 0.7180843949317932
Epoch 830, training loss: 12.696979522705078 = 0.11481809616088867 + 2.0 * 6.291080474853516
Epoch 830, val loss: 0.7213401794433594
Epoch 840, training loss: 12.687833786010742 = 0.11001461744308472 + 2.0 * 6.288909435272217
Epoch 840, val loss: 0.72477787733078
Epoch 850, training loss: 12.68067455291748 = 0.10546135157346725 + 2.0 * 6.287606716156006
Epoch 850, val loss: 0.7282840013504028
Epoch 860, training loss: 12.692667961120605 = 0.10114531964063644 + 2.0 * 6.2957611083984375
Epoch 860, val loss: 0.7318940162658691
Epoch 870, training loss: 12.680187225341797 = 0.09702666848897934 + 2.0 * 6.2915802001953125
Epoch 870, val loss: 0.7352457046508789
Epoch 880, training loss: 12.670507431030273 = 0.09316188097000122 + 2.0 * 6.288672924041748
Epoch 880, val loss: 0.7390226125717163
Epoch 890, training loss: 12.659209251403809 = 0.08949561417102814 + 2.0 * 6.284856796264648
Epoch 890, val loss: 0.7428830862045288
Epoch 900, training loss: 12.652288436889648 = 0.0860178992152214 + 2.0 * 6.283135414123535
Epoch 900, val loss: 0.7467661499977112
Epoch 910, training loss: 12.64744758605957 = 0.08269535005092621 + 2.0 * 6.282376289367676
Epoch 910, val loss: 0.7507105469703674
Epoch 920, training loss: 12.645724296569824 = 0.07953179627656937 + 2.0 * 6.2830963134765625
Epoch 920, val loss: 0.754730761051178
Epoch 930, training loss: 12.645042419433594 = 0.07652149349451065 + 2.0 * 6.284260272979736
Epoch 930, val loss: 0.7586586475372314
Epoch 940, training loss: 12.639228820800781 = 0.07365483045578003 + 2.0 * 6.282786846160889
Epoch 940, val loss: 0.7627089619636536
Epoch 950, training loss: 12.631301879882812 = 0.07093097269535065 + 2.0 * 6.280185222625732
Epoch 950, val loss: 0.7668896913528442
Epoch 960, training loss: 12.626198768615723 = 0.06835239380598068 + 2.0 * 6.278923034667969
Epoch 960, val loss: 0.7710663676261902
Epoch 970, training loss: 12.630194664001465 = 0.06588971614837646 + 2.0 * 6.2821526527404785
Epoch 970, val loss: 0.7752916812896729
Epoch 980, training loss: 12.624003410339355 = 0.06352873146533966 + 2.0 * 6.280237197875977
Epoch 980, val loss: 0.7793513536453247
Epoch 990, training loss: 12.61967658996582 = 0.06128799915313721 + 2.0 * 6.279194355010986
Epoch 990, val loss: 0.7834908366203308
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8148148148148149
0.8397469688982605
The final CL Acc:0.81235, 0.00175, The final GNN Acc:0.84168, 0.00237
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.4 der2:0.2 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11676])
remove edge: torch.Size([2, 9464])
updated graph: torch.Size([2, 10584])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 19.139328002929688 = 1.9456251859664917 + 2.0 * 8.596851348876953
Epoch 0, val loss: 1.9511009454727173
Epoch 10, training loss: 19.129331588745117 = 1.9362208843231201 + 2.0 * 8.596555709838867
Epoch 10, val loss: 1.9412312507629395
Epoch 20, training loss: 19.113000869750977 = 1.9244210720062256 + 2.0 * 8.594289779663086
Epoch 20, val loss: 1.9287599325180054
Epoch 30, training loss: 19.062294006347656 = 1.9079465866088867 + 2.0 * 8.577173233032227
Epoch 30, val loss: 1.911442756652832
Epoch 40, training loss: 18.81159782409668 = 1.8869632482528687 + 2.0 * 8.46231746673584
Epoch 40, val loss: 1.8903456926345825
Epoch 50, training loss: 17.830028533935547 = 1.863715648651123 + 2.0 * 7.983156681060791
Epoch 50, val loss: 1.8671932220458984
Epoch 60, training loss: 17.283565521240234 = 1.8420498371124268 + 2.0 * 7.720757961273193
Epoch 60, val loss: 1.8454910516738892
Epoch 70, training loss: 16.769784927368164 = 1.8262211084365845 + 2.0 * 7.4717817306518555
Epoch 70, val loss: 1.8296856880187988
Epoch 80, training loss: 16.228471755981445 = 1.8145390748977661 + 2.0 * 7.206966400146484
Epoch 80, val loss: 1.8180180788040161
Epoch 90, training loss: 15.832056045532227 = 1.8044381141662598 + 2.0 * 7.013808727264404
Epoch 90, val loss: 1.8078038692474365
Epoch 100, training loss: 15.620809555053711 = 1.792041301727295 + 2.0 * 6.914383888244629
Epoch 100, val loss: 1.7948994636535645
Epoch 110, training loss: 15.417747497558594 = 1.7797250747680664 + 2.0 * 6.819011211395264
Epoch 110, val loss: 1.781843662261963
Epoch 120, training loss: 15.272733688354492 = 1.7685017585754395 + 2.0 * 6.7521162033081055
Epoch 120, val loss: 1.7700719833374023
Epoch 130, training loss: 15.163189888000488 = 1.756550669670105 + 2.0 * 6.703319549560547
Epoch 130, val loss: 1.7587310075759888
Epoch 140, training loss: 15.062482833862305 = 1.743497610092163 + 2.0 * 6.659492492675781
Epoch 140, val loss: 1.7467542886734009
Epoch 150, training loss: 14.980497360229492 = 1.7293457984924316 + 2.0 * 6.625575542449951
Epoch 150, val loss: 1.7342272996902466
Epoch 160, training loss: 14.910847663879395 = 1.7132999897003174 + 2.0 * 6.598773956298828
Epoch 160, val loss: 1.7199443578720093
Epoch 170, training loss: 14.854126930236816 = 1.6947189569473267 + 2.0 * 6.5797038078308105
Epoch 170, val loss: 1.7034904956817627
Epoch 180, training loss: 14.788936614990234 = 1.6734340190887451 + 2.0 * 6.557751178741455
Epoch 180, val loss: 1.6848678588867188
Epoch 190, training loss: 14.730134963989258 = 1.6491930484771729 + 2.0 * 6.540471076965332
Epoch 190, val loss: 1.6638941764831543
Epoch 200, training loss: 14.692330360412598 = 1.6213314533233643 + 2.0 * 6.535499572753906
Epoch 200, val loss: 1.6400940418243408
Epoch 210, training loss: 14.617094039916992 = 1.5903549194335938 + 2.0 * 6.513369560241699
Epoch 210, val loss: 1.6135687828063965
Epoch 220, training loss: 14.554408073425293 = 1.5558089017868042 + 2.0 * 6.4992995262146
Epoch 220, val loss: 1.5842177867889404
Epoch 230, training loss: 14.493123054504395 = 1.5176602602005005 + 2.0 * 6.487731456756592
Epoch 230, val loss: 1.5520737171173096
Epoch 240, training loss: 14.430668830871582 = 1.4759982824325562 + 2.0 * 6.477335453033447
Epoch 240, val loss: 1.517188310623169
Epoch 250, training loss: 14.366768836975098 = 1.431357502937317 + 2.0 * 6.467705726623535
Epoch 250, val loss: 1.480098009109497
Epoch 260, training loss: 14.309292793273926 = 1.384749174118042 + 2.0 * 6.462271690368652
Epoch 260, val loss: 1.4419068098068237
Epoch 270, training loss: 14.245268821716309 = 1.3384746313095093 + 2.0 * 6.453397274017334
Epoch 270, val loss: 1.4043304920196533
Epoch 280, training loss: 14.1845121383667 = 1.292817234992981 + 2.0 * 6.445847511291504
Epoch 280, val loss: 1.3677157163619995
Epoch 290, training loss: 14.123712539672852 = 1.2480641603469849 + 2.0 * 6.437824249267578
Epoch 290, val loss: 1.332287073135376
Epoch 300, training loss: 14.067137718200684 = 1.2044507265090942 + 2.0 * 6.4313435554504395
Epoch 300, val loss: 1.2982779741287231
Epoch 310, training loss: 14.04619312286377 = 1.1623239517211914 + 2.0 * 6.441934585571289
Epoch 310, val loss: 1.2659955024719238
Epoch 320, training loss: 13.965989112854004 = 1.12291419506073 + 2.0 * 6.421537399291992
Epoch 320, val loss: 1.2363255023956299
Epoch 330, training loss: 13.917871475219727 = 1.0857940912246704 + 2.0 * 6.416038513183594
Epoch 330, val loss: 1.2089990377426147
Epoch 340, training loss: 13.872209548950195 = 1.0502731800079346 + 2.0 * 6.41096830368042
Epoch 340, val loss: 1.1832250356674194
Epoch 350, training loss: 13.827713966369629 = 1.0160247087478638 + 2.0 * 6.405844688415527
Epoch 350, val loss: 1.1589021682739258
Epoch 360, training loss: 13.784833908081055 = 0.9828894138336182 + 2.0 * 6.400972366333008
Epoch 360, val loss: 1.1357674598693848
Epoch 370, training loss: 13.743887901306152 = 0.9506959915161133 + 2.0 * 6.3965959548950195
Epoch 370, val loss: 1.1136621236801147
Epoch 380, training loss: 13.722368240356445 = 0.9194648265838623 + 2.0 * 6.401451587677002
Epoch 380, val loss: 1.092743992805481
Epoch 390, training loss: 13.668844223022461 = 0.8895766735076904 + 2.0 * 6.389633655548096
Epoch 390, val loss: 1.0728012323379517
Epoch 400, training loss: 13.630643844604492 = 0.8604754209518433 + 2.0 * 6.38508415222168
Epoch 400, val loss: 1.0539268255233765
Epoch 410, training loss: 13.59374713897705 = 0.8318927884101868 + 2.0 * 6.380927085876465
Epoch 410, val loss: 1.0356173515319824
Epoch 420, training loss: 13.55876350402832 = 0.803756058216095 + 2.0 * 6.377503871917725
Epoch 420, val loss: 1.017884373664856
Epoch 430, training loss: 13.527758598327637 = 0.776020884513855 + 2.0 * 6.375868797302246
Epoch 430, val loss: 1.0006152391433716
Epoch 440, training loss: 13.506596565246582 = 0.7488793134689331 + 2.0 * 6.37885856628418
Epoch 440, val loss: 0.9840456247329712
Epoch 450, training loss: 13.463638305664062 = 0.7225589752197266 + 2.0 * 6.370539665222168
Epoch 450, val loss: 0.9683018326759338
Epoch 460, training loss: 13.429601669311523 = 0.6968159675598145 + 2.0 * 6.366392612457275
Epoch 460, val loss: 0.9532902240753174
Epoch 470, training loss: 13.407485961914062 = 0.6716666221618652 + 2.0 * 6.3679094314575195
Epoch 470, val loss: 0.9389180541038513
Epoch 480, training loss: 13.369900703430176 = 0.6472217440605164 + 2.0 * 6.361339569091797
Epoch 480, val loss: 0.9253842234611511
Epoch 490, training loss: 13.339995384216309 = 0.6233692765235901 + 2.0 * 6.358313083648682
Epoch 490, val loss: 0.9125898480415344
Epoch 500, training loss: 13.321188926696777 = 0.600114107131958 + 2.0 * 6.360537528991699
Epoch 500, val loss: 0.9005457162857056
Epoch 510, training loss: 13.29367446899414 = 0.5774713158607483 + 2.0 * 6.3581013679504395
Epoch 510, val loss: 0.8892476558685303
Epoch 520, training loss: 13.261053085327148 = 0.5554872751235962 + 2.0 * 6.352782726287842
Epoch 520, val loss: 0.8788212537765503
Epoch 530, training loss: 13.234515190124512 = 0.5338975787162781 + 2.0 * 6.350308895111084
Epoch 530, val loss: 0.8689263463020325
Epoch 540, training loss: 13.208982467651367 = 0.5126473903656006 + 2.0 * 6.348167419433594
Epoch 540, val loss: 0.8596378564834595
Epoch 550, training loss: 13.196020126342773 = 0.49175772070884705 + 2.0 * 6.352131366729736
Epoch 550, val loss: 0.8508337736129761
Epoch 560, training loss: 13.164549827575684 = 0.47136667370796204 + 2.0 * 6.346591472625732
Epoch 560, val loss: 0.8427358269691467
Epoch 570, training loss: 13.137360572814941 = 0.45138847827911377 + 2.0 * 6.342986106872559
Epoch 570, val loss: 0.8351263999938965
Epoch 580, training loss: 13.112393379211426 = 0.43170875310897827 + 2.0 * 6.3403425216674805
Epoch 580, val loss: 0.8280789852142334
Epoch 590, training loss: 13.103839874267578 = 0.4123428761959076 + 2.0 * 6.345748424530029
Epoch 590, val loss: 0.821491003036499
Epoch 600, training loss: 13.070034980773926 = 0.3934979736804962 + 2.0 * 6.338268280029297
Epoch 600, val loss: 0.8155223727226257
Epoch 610, training loss: 13.046977996826172 = 0.3751530647277832 + 2.0 * 6.335912227630615
Epoch 610, val loss: 0.810116708278656
Epoch 620, training loss: 13.026077270507812 = 0.35724911093711853 + 2.0 * 6.334414005279541
Epoch 620, val loss: 0.8052479028701782
Epoch 630, training loss: 13.023961067199707 = 0.33983471989631653 + 2.0 * 6.342062950134277
Epoch 630, val loss: 0.8009622097015381
Epoch 640, training loss: 12.987274169921875 = 0.3230329751968384 + 2.0 * 6.332120418548584
Epoch 640, val loss: 0.7971341609954834
Epoch 650, training loss: 12.9666748046875 = 0.3068705201148987 + 2.0 * 6.329902172088623
Epoch 650, val loss: 0.7940327525138855
Epoch 660, training loss: 12.947220802307129 = 0.29126816987991333 + 2.0 * 6.327976226806641
Epoch 660, val loss: 0.7914788126945496
Epoch 670, training loss: 12.949946403503418 = 0.27628302574157715 + 2.0 * 6.336831569671631
Epoch 670, val loss: 0.789514422416687
Epoch 680, training loss: 12.915061950683594 = 0.26196587085723877 + 2.0 * 6.326548099517822
Epoch 680, val loss: 0.7878944873809814
Epoch 690, training loss: 12.89737319946289 = 0.24834951758384705 + 2.0 * 6.324512004852295
Epoch 690, val loss: 0.7868886590003967
Epoch 700, training loss: 12.89085578918457 = 0.2353968620300293 + 2.0 * 6.32772970199585
Epoch 700, val loss: 0.7863993644714355
Epoch 710, training loss: 12.868959426879883 = 0.22308449447155 + 2.0 * 6.322937488555908
Epoch 710, val loss: 0.7862950563430786
Epoch 720, training loss: 12.855768203735352 = 0.21141669154167175 + 2.0 * 6.322175979614258
Epoch 720, val loss: 0.7867763042449951
Epoch 730, training loss: 12.837538719177246 = 0.20034857094287872 + 2.0 * 6.318594932556152
Epoch 730, val loss: 0.7874584794044495
Epoch 740, training loss: 12.824982643127441 = 0.18988381326198578 + 2.0 * 6.317549228668213
Epoch 740, val loss: 0.7887471914291382
Epoch 750, training loss: 12.824101448059082 = 0.17999285459518433 + 2.0 * 6.322054386138916
Epoch 750, val loss: 0.7902816534042358
Epoch 760, training loss: 12.804689407348633 = 0.1706741601228714 + 2.0 * 6.317007541656494
Epoch 760, val loss: 0.7922779321670532
Epoch 770, training loss: 12.793116569519043 = 0.1619049310684204 + 2.0 * 6.315605640411377
Epoch 770, val loss: 0.7945435643196106
Epoch 780, training loss: 12.801406860351562 = 0.15364623069763184 + 2.0 * 6.323880195617676
Epoch 780, val loss: 0.7970384955406189
Epoch 790, training loss: 12.77492904663086 = 0.14594094455242157 + 2.0 * 6.3144941329956055
Epoch 790, val loss: 0.8000353574752808
Epoch 800, training loss: 12.759263038635254 = 0.13869734108448029 + 2.0 * 6.3102827072143555
Epoch 800, val loss: 0.8032727241516113
Epoch 810, training loss: 12.751627922058105 = 0.13186585903167725 + 2.0 * 6.309881210327148
Epoch 810, val loss: 0.8066193461418152
Epoch 820, training loss: 12.751791954040527 = 0.12543898820877075 + 2.0 * 6.31317663192749
Epoch 820, val loss: 0.8102700710296631
Epoch 830, training loss: 12.740001678466797 = 0.11938933283090591 + 2.0 * 6.310306072235107
Epoch 830, val loss: 0.8141749501228333
Epoch 840, training loss: 12.728255271911621 = 0.11370669305324554 + 2.0 * 6.307274341583252
Epoch 840, val loss: 0.8181284666061401
Epoch 850, training loss: 12.719096183776855 = 0.1083560362458229 + 2.0 * 6.305369853973389
Epoch 850, val loss: 0.8222714066505432
Epoch 860, training loss: 12.719557762145996 = 0.10330935567617416 + 2.0 * 6.30812406539917
Epoch 860, val loss: 0.8265065550804138
Epoch 870, training loss: 12.705334663391113 = 0.09855503588914871 + 2.0 * 6.303390026092529
Epoch 870, val loss: 0.8310854434967041
Epoch 880, training loss: 12.704913139343262 = 0.09408437460660934 + 2.0 * 6.305414199829102
Epoch 880, val loss: 0.8357363939285278
Epoch 890, training loss: 12.69703483581543 = 0.08986696600914001 + 2.0 * 6.303584098815918
Epoch 890, val loss: 0.8404127955436707
Epoch 900, training loss: 12.689437866210938 = 0.08591390401124954 + 2.0 * 6.301762104034424
Epoch 900, val loss: 0.8453052639961243
Epoch 910, training loss: 12.680105209350586 = 0.08217282593250275 + 2.0 * 6.298966407775879
Epoch 910, val loss: 0.8501491546630859
Epoch 920, training loss: 12.679171562194824 = 0.07863929867744446 + 2.0 * 6.300266265869141
Epoch 920, val loss: 0.8552048802375793
Epoch 930, training loss: 12.674101829528809 = 0.07529697567224503 + 2.0 * 6.299402236938477
Epoch 930, val loss: 0.859923779964447
Epoch 940, training loss: 12.670246124267578 = 0.07215868681669235 + 2.0 * 6.299043655395508
Epoch 940, val loss: 0.8651210069656372
Epoch 950, training loss: 12.660064697265625 = 0.06918705999851227 + 2.0 * 6.295438766479492
Epoch 950, val loss: 0.8701530694961548
Epoch 960, training loss: 12.656682968139648 = 0.06636925041675568 + 2.0 * 6.295156955718994
Epoch 960, val loss: 0.8752489686012268
Epoch 970, training loss: 12.668421745300293 = 0.06369802355766296 + 2.0 * 6.302361965179443
Epoch 970, val loss: 0.8802551627159119
Epoch 980, training loss: 12.648069381713867 = 0.0611727312207222 + 2.0 * 6.293448448181152
Epoch 980, val loss: 0.885410726070404
Epoch 990, training loss: 12.644010543823242 = 0.05878268554806709 + 2.0 * 6.292613983154297
Epoch 990, val loss: 0.8906002044677734
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7777777777777778
0.812335266209805
=== training gcn model ===
Epoch 0, training loss: 19.136695861816406 = 1.9429631233215332 + 2.0 * 8.596866607666016
Epoch 0, val loss: 1.9355297088623047
Epoch 10, training loss: 19.125316619873047 = 1.9320389032363892 + 2.0 * 8.596638679504395
Epoch 10, val loss: 1.9245933294296265
Epoch 20, training loss: 19.107847213745117 = 1.918172001838684 + 2.0 * 8.594837188720703
Epoch 20, val loss: 1.9108117818832397
Epoch 30, training loss: 19.059377670288086 = 1.8986750841140747 + 2.0 * 8.580350875854492
Epoch 30, val loss: 1.8918012380599976
Epoch 40, training loss: 18.870201110839844 = 1.8734997510910034 + 2.0 * 8.498351097106934
Epoch 40, val loss: 1.8684806823730469
Epoch 50, training loss: 18.12358856201172 = 1.8477709293365479 + 2.0 * 8.137908935546875
Epoch 50, val loss: 1.8451128005981445
Epoch 60, training loss: 17.58894157409668 = 1.8260245323181152 + 2.0 * 7.881458759307861
Epoch 60, val loss: 1.8256738185882568
Epoch 70, training loss: 16.895362854003906 = 1.811224102973938 + 2.0 * 7.542069435119629
Epoch 70, val loss: 1.813555121421814
Epoch 80, training loss: 16.190420150756836 = 1.8039878606796265 + 2.0 * 7.193215847015381
Epoch 80, val loss: 1.8074512481689453
Epoch 90, training loss: 15.694215774536133 = 1.7946653366088867 + 2.0 * 6.949775218963623
Epoch 90, val loss: 1.798869013786316
Epoch 100, training loss: 15.41830062866211 = 1.7826849222183228 + 2.0 * 6.817807674407959
Epoch 100, val loss: 1.7884101867675781
Epoch 110, training loss: 15.235947608947754 = 1.77090585231781 + 2.0 * 6.732521057128906
Epoch 110, val loss: 1.778303623199463
Epoch 120, training loss: 15.105450630187988 = 1.759324073791504 + 2.0 * 6.673063278198242
Epoch 120, val loss: 1.7679179906845093
Epoch 130, training loss: 15.007516860961914 = 1.746579885482788 + 2.0 * 6.630468368530273
Epoch 130, val loss: 1.7561591863632202
Epoch 140, training loss: 14.923694610595703 = 1.7328174114227295 + 2.0 * 6.595438480377197
Epoch 140, val loss: 1.74345064163208
Epoch 150, training loss: 14.858404159545898 = 1.7180206775665283 + 2.0 * 6.570191860198975
Epoch 150, val loss: 1.7297943830490112
Epoch 160, training loss: 14.788957595825195 = 1.7017550468444824 + 2.0 * 6.543601036071777
Epoch 160, val loss: 1.7150819301605225
Epoch 170, training loss: 14.729276657104492 = 1.683819055557251 + 2.0 * 6.52272891998291
Epoch 170, val loss: 1.6990023851394653
Epoch 180, training loss: 14.679929733276367 = 1.663884162902832 + 2.0 * 6.508022785186768
Epoch 180, val loss: 1.6813298463821411
Epoch 190, training loss: 14.625054359436035 = 1.6420780420303345 + 2.0 * 6.491487979888916
Epoch 190, val loss: 1.662164330482483
Epoch 200, training loss: 14.572392463684082 = 1.618318796157837 + 2.0 * 6.477036952972412
Epoch 200, val loss: 1.6414220333099365
Epoch 210, training loss: 14.530055046081543 = 1.59249746799469 + 2.0 * 6.468778610229492
Epoch 210, val loss: 1.6191072463989258
Epoch 220, training loss: 14.47610092163086 = 1.5649062395095825 + 2.0 * 6.455597400665283
Epoch 220, val loss: 1.5954889059066772
Epoch 230, training loss: 14.426413536071777 = 1.5358707904815674 + 2.0 * 6.4452714920043945
Epoch 230, val loss: 1.5707800388336182
Epoch 240, training loss: 14.381821632385254 = 1.5054081678390503 + 2.0 * 6.438206672668457
Epoch 240, val loss: 1.5452940464019775
Epoch 250, training loss: 14.333090782165527 = 1.4741402864456177 + 2.0 * 6.4294753074646
Epoch 250, val loss: 1.5195462703704834
Epoch 260, training loss: 14.284260749816895 = 1.4422893524169922 + 2.0 * 6.420985698699951
Epoch 260, val loss: 1.4938205480575562
Epoch 270, training loss: 14.238353729248047 = 1.4099583625793457 + 2.0 * 6.41419792175293
Epoch 270, val loss: 1.468307614326477
Epoch 280, training loss: 14.198993682861328 = 1.3775907754898071 + 2.0 * 6.410701274871826
Epoch 280, val loss: 1.4433724880218506
Epoch 290, training loss: 14.157167434692383 = 1.3456692695617676 + 2.0 * 6.405749320983887
Epoch 290, val loss: 1.4193315505981445
Epoch 300, training loss: 14.111571311950684 = 1.3140923976898193 + 2.0 * 6.398739337921143
Epoch 300, val loss: 1.3962690830230713
Epoch 310, training loss: 14.068926811218262 = 1.2826515436172485 + 2.0 * 6.393137454986572
Epoch 310, val loss: 1.373834490776062
Epoch 320, training loss: 14.038822174072266 = 1.2512385845184326 + 2.0 * 6.393791675567627
Epoch 320, val loss: 1.35187828540802
Epoch 330, training loss: 13.992189407348633 = 1.220089077949524 + 2.0 * 6.386050224304199
Epoch 330, val loss: 1.330419659614563
Epoch 340, training loss: 13.95444107055664 = 1.1890355348587036 + 2.0 * 6.382702827453613
Epoch 340, val loss: 1.3094446659088135
Epoch 350, training loss: 13.915853500366211 = 1.1579976081848145 + 2.0 * 6.378927707672119
Epoch 350, val loss: 1.2887520790100098
Epoch 360, training loss: 13.877957344055176 = 1.1269716024398804 + 2.0 * 6.375493049621582
Epoch 360, val loss: 1.268245816230774
Epoch 370, training loss: 13.83816909790039 = 1.096073865890503 + 2.0 * 6.371047496795654
Epoch 370, val loss: 1.2479082345962524
Epoch 380, training loss: 13.805089950561523 = 1.0652352571487427 + 2.0 * 6.369927406311035
Epoch 380, val loss: 1.2277363538742065
Epoch 390, training loss: 13.76467227935791 = 1.0346235036849976 + 2.0 * 6.365024566650391
Epoch 390, val loss: 1.2077062129974365
Epoch 400, training loss: 13.726875305175781 = 1.0043635368347168 + 2.0 * 6.361255645751953
Epoch 400, val loss: 1.1879643201828003
Epoch 410, training loss: 13.692832946777344 = 0.9745557904243469 + 2.0 * 6.359138488769531
Epoch 410, val loss: 1.1686339378356934
Epoch 420, training loss: 13.661048889160156 = 0.9454354047775269 + 2.0 * 6.35780668258667
Epoch 420, val loss: 1.149867057800293
Epoch 430, training loss: 13.625646591186523 = 0.9172080159187317 + 2.0 * 6.354219436645508
Epoch 430, val loss: 1.1315518617630005
Epoch 440, training loss: 13.593914985656738 = 0.8899090886116028 + 2.0 * 6.35200309753418
Epoch 440, val loss: 1.1139302253723145
Epoch 450, training loss: 13.564451217651367 = 0.8634133338928223 + 2.0 * 6.350518703460693
Epoch 450, val loss: 1.0968726873397827
Epoch 460, training loss: 13.530183792114258 = 0.8378027677536011 + 2.0 * 6.346190452575684
Epoch 460, val loss: 1.0804049968719482
Epoch 470, training loss: 13.506537437438965 = 0.812982976436615 + 2.0 * 6.346777439117432
Epoch 470, val loss: 1.064622402191162
Epoch 480, training loss: 13.485062599182129 = 0.7891300320625305 + 2.0 * 6.347966194152832
Epoch 480, val loss: 1.049475908279419
Epoch 490, training loss: 13.447144508361816 = 0.7663188576698303 + 2.0 * 6.340412616729736
Epoch 490, val loss: 1.0353201627731323
Epoch 500, training loss: 13.419865608215332 = 0.744347333908081 + 2.0 * 6.337759017944336
Epoch 500, val loss: 1.0219675302505493
Epoch 510, training loss: 13.39855670928955 = 0.7231448888778687 + 2.0 * 6.337706089019775
Epoch 510, val loss: 1.0094883441925049
Epoch 520, training loss: 13.373723030090332 = 0.7027485966682434 + 2.0 * 6.335487365722656
Epoch 520, val loss: 0.997685968875885
Epoch 530, training loss: 13.3488187789917 = 0.6830081939697266 + 2.0 * 6.332905292510986
Epoch 530, val loss: 0.9867474436759949
Epoch 540, training loss: 13.324639320373535 = 0.6638637781143188 + 2.0 * 6.330387592315674
Epoch 540, val loss: 0.9765089154243469
Epoch 550, training loss: 13.311393737792969 = 0.645121157169342 + 2.0 * 6.333136081695557
Epoch 550, val loss: 0.9669407606124878
Epoch 560, training loss: 13.2821626663208 = 0.6268500089645386 + 2.0 * 6.327656269073486
Epoch 560, val loss: 0.9580010175704956
Epoch 570, training loss: 13.267768859863281 = 0.6089122295379639 + 2.0 * 6.329428195953369
Epoch 570, val loss: 0.9497878551483154
Epoch 580, training loss: 13.240791320800781 = 0.5912472605705261 + 2.0 * 6.324771881103516
Epoch 580, val loss: 0.9421490430831909
Epoch 590, training loss: 13.218671798706055 = 0.5738787651062012 + 2.0 * 6.322396278381348
Epoch 590, val loss: 0.9351893067359924
Epoch 600, training loss: 13.197851181030273 = 0.5566661953926086 + 2.0 * 6.320592403411865
Epoch 600, val loss: 0.9287593960762024
Epoch 610, training loss: 13.186603546142578 = 0.5396242737770081 + 2.0 * 6.323489665985107
Epoch 610, val loss: 0.9228785037994385
Epoch 620, training loss: 13.163640022277832 = 0.522716224193573 + 2.0 * 6.320461750030518
Epoch 620, val loss: 0.9174591898918152
Epoch 630, training loss: 13.142863273620605 = 0.505966305732727 + 2.0 * 6.318448543548584
Epoch 630, val loss: 0.9125959873199463
Epoch 640, training loss: 13.122690200805664 = 0.4893762171268463 + 2.0 * 6.316657066345215
Epoch 640, val loss: 0.9081640839576721
Epoch 650, training loss: 13.104353904724121 = 0.4728371202945709 + 2.0 * 6.315758228302002
Epoch 650, val loss: 0.9039360880851746
Epoch 660, training loss: 13.08332633972168 = 0.4564405083656311 + 2.0 * 6.313442707061768
Epoch 660, val loss: 0.9002863168716431
Epoch 670, training loss: 13.063396453857422 = 0.4400904178619385 + 2.0 * 6.311653137207031
Epoch 670, val loss: 0.8969342112541199
Epoch 680, training loss: 13.04495620727539 = 0.4238392412662506 + 2.0 * 6.310558319091797
Epoch 680, val loss: 0.8938606977462769
Epoch 690, training loss: 13.030789375305176 = 0.40769776701927185 + 2.0 * 6.3115458488464355
Epoch 690, val loss: 0.891062319278717
Epoch 700, training loss: 13.00735092163086 = 0.3917528986930847 + 2.0 * 6.307798862457275
Epoch 700, val loss: 0.8886581659317017
Epoch 710, training loss: 12.990960121154785 = 0.3760063946247101 + 2.0 * 6.307476997375488
Epoch 710, val loss: 0.8865984678268433
Epoch 720, training loss: 12.970480918884277 = 0.3604927062988281 + 2.0 * 6.304994106292725
Epoch 720, val loss: 0.884908139705658
Epoch 730, training loss: 12.96935749053955 = 0.3452034592628479 + 2.0 * 6.312077045440674
Epoch 730, val loss: 0.8835463523864746
Epoch 740, training loss: 12.945005416870117 = 0.3303704261779785 + 2.0 * 6.307317733764648
Epoch 740, val loss: 0.8825371265411377
Epoch 750, training loss: 12.918617248535156 = 0.3158205449581146 + 2.0 * 6.301398277282715
Epoch 750, val loss: 0.8819125890731812
Epoch 760, training loss: 12.90369987487793 = 0.3016841411590576 + 2.0 * 6.3010077476501465
Epoch 760, val loss: 0.8816262483596802
Epoch 770, training loss: 12.895718574523926 = 0.28795313835144043 + 2.0 * 6.303882598876953
Epoch 770, val loss: 0.8817129135131836
Epoch 780, training loss: 12.881267547607422 = 0.2746366858482361 + 2.0 * 6.30331563949585
Epoch 780, val loss: 0.8821820616722107
Epoch 790, training loss: 12.859478950500488 = 0.26186102628707886 + 2.0 * 6.298809051513672
Epoch 790, val loss: 0.8830180764198303
Epoch 800, training loss: 12.842530250549316 = 0.24957819283008575 + 2.0 * 6.296475887298584
Epoch 800, val loss: 0.8842208385467529
Epoch 810, training loss: 12.83014965057373 = 0.23772494494915009 + 2.0 * 6.296212196350098
Epoch 810, val loss: 0.8856120109558105
Epoch 820, training loss: 12.824131965637207 = 0.22642408311367035 + 2.0 * 6.298853874206543
Epoch 820, val loss: 0.8873074054718018
Epoch 830, training loss: 12.807900428771973 = 0.2156406044960022 + 2.0 * 6.2961297035217285
Epoch 830, val loss: 0.8894284963607788
Epoch 840, training loss: 12.790669441223145 = 0.20537881553173065 + 2.0 * 6.292645454406738
Epoch 840, val loss: 0.8916749358177185
Epoch 850, training loss: 12.779707908630371 = 0.19560258090496063 + 2.0 * 6.292052745819092
Epoch 850, val loss: 0.8942117094993591
Epoch 860, training loss: 12.783931732177734 = 0.18628962337970734 + 2.0 * 6.298820972442627
Epoch 860, val loss: 0.8970344662666321
Epoch 870, training loss: 12.773050308227539 = 0.17746004462242126 + 2.0 * 6.297795295715332
Epoch 870, val loss: 0.9000157117843628
Epoch 880, training loss: 12.74992561340332 = 0.16911840438842773 + 2.0 * 6.290403366088867
Epoch 880, val loss: 0.9032469987869263
Epoch 890, training loss: 12.743110656738281 = 0.16119235754013062 + 2.0 * 6.290959358215332
Epoch 890, val loss: 0.9066160917282104
Epoch 900, training loss: 12.728919982910156 = 0.1536686271429062 + 2.0 * 6.287625789642334
Epoch 900, val loss: 0.9101327061653137
Epoch 910, training loss: 12.720134735107422 = 0.14652560651302338 + 2.0 * 6.286804676055908
Epoch 910, val loss: 0.9138514399528503
Epoch 920, training loss: 12.717622756958008 = 0.1397508978843689 + 2.0 * 6.288936138153076
Epoch 920, val loss: 0.917643666267395
Epoch 930, training loss: 12.711350440979004 = 0.13335087895393372 + 2.0 * 6.288999557495117
Epoch 930, val loss: 0.921661913394928
Epoch 940, training loss: 12.704696655273438 = 0.1272994428873062 + 2.0 * 6.288698673248291
Epoch 940, val loss: 0.925504207611084
Epoch 950, training loss: 12.689743041992188 = 0.12160449475049973 + 2.0 * 6.284069061279297
Epoch 950, val loss: 0.9297497868537903
Epoch 960, training loss: 12.6829195022583 = 0.11620833724737167 + 2.0 * 6.283355712890625
Epoch 960, val loss: 0.9338303208351135
Epoch 970, training loss: 12.682384490966797 = 0.11110608279705048 + 2.0 * 6.28563928604126
Epoch 970, val loss: 0.9381629228591919
Epoch 980, training loss: 12.667354583740234 = 0.10624776035547256 + 2.0 * 6.280553340911865
Epoch 980, val loss: 0.9425573348999023
Epoch 990, training loss: 12.66148567199707 = 0.10166428238153458 + 2.0 * 6.279910564422607
Epoch 990, val loss: 0.9470810294151306
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7296296296296296
0.8128624143384291
=== training gcn model ===
Epoch 0, training loss: 19.155031204223633 = 1.9613770246505737 + 2.0 * 8.596827507019043
Epoch 0, val loss: 1.9684052467346191
Epoch 10, training loss: 19.14354133605957 = 1.9507449865341187 + 2.0 * 8.59639835357666
Epoch 10, val loss: 1.9574532508850098
Epoch 20, training loss: 19.123563766479492 = 1.9374514818191528 + 2.0 * 8.593055725097656
Epoch 20, val loss: 1.9431923627853394
Epoch 30, training loss: 19.05816078186035 = 1.9195533990859985 + 2.0 * 8.569303512573242
Epoch 30, val loss: 1.9235668182373047
Epoch 40, training loss: 18.74448585510254 = 1.8984140157699585 + 2.0 * 8.423035621643066
Epoch 40, val loss: 1.901377558708191
Epoch 50, training loss: 17.754722595214844 = 1.8755879402160645 + 2.0 * 7.9395670890808105
Epoch 50, val loss: 1.8774195909500122
Epoch 60, training loss: 16.91408348083496 = 1.8585857152938843 + 2.0 * 7.527749061584473
Epoch 60, val loss: 1.8612200021743774
Epoch 70, training loss: 16.216106414794922 = 1.8454405069351196 + 2.0 * 7.185333251953125
Epoch 70, val loss: 1.8490735292434692
Epoch 80, training loss: 15.882638931274414 = 1.833322286605835 + 2.0 * 7.024658203125
Epoch 80, val loss: 1.8374792337417603
Epoch 90, training loss: 15.648836135864258 = 1.8195900917053223 + 2.0 * 6.914623260498047
Epoch 90, val loss: 1.8241784572601318
Epoch 100, training loss: 15.453160285949707 = 1.806491494178772 + 2.0 * 6.823334217071533
Epoch 100, val loss: 1.8115843534469604
Epoch 110, training loss: 15.315614700317383 = 1.7948718070983887 + 2.0 * 6.760371208190918
Epoch 110, val loss: 1.8005857467651367
Epoch 120, training loss: 15.205818176269531 = 1.7841113805770874 + 2.0 * 6.710853576660156
Epoch 120, val loss: 1.790508508682251
Epoch 130, training loss: 15.117692947387695 = 1.7733206748962402 + 2.0 * 6.672186374664307
Epoch 130, val loss: 1.7803847789764404
Epoch 140, training loss: 15.044510841369629 = 1.7617664337158203 + 2.0 * 6.641372203826904
Epoch 140, val loss: 1.7695839405059814
Epoch 150, training loss: 14.974806785583496 = 1.7492929697036743 + 2.0 * 6.612756729125977
Epoch 150, val loss: 1.7580727338790894
Epoch 160, training loss: 14.908801078796387 = 1.7358728647232056 + 2.0 * 6.586463928222656
Epoch 160, val loss: 1.745984673500061
Epoch 170, training loss: 14.847564697265625 = 1.7212296724319458 + 2.0 * 6.563167572021484
Epoch 170, val loss: 1.7329872846603394
Epoch 180, training loss: 14.790021896362305 = 1.7049816846847534 + 2.0 * 6.542520046234131
Epoch 180, val loss: 1.7188509702682495
Epoch 190, training loss: 14.736577987670898 = 1.6868549585342407 + 2.0 * 6.5248613357543945
Epoch 190, val loss: 1.7031997442245483
Epoch 200, training loss: 14.684650421142578 = 1.666581630706787 + 2.0 * 6.509034156799316
Epoch 200, val loss: 1.685905933380127
Epoch 210, training loss: 14.640190124511719 = 1.6440000534057617 + 2.0 * 6.4980950355529785
Epoch 210, val loss: 1.6667757034301758
Epoch 220, training loss: 14.581989288330078 = 1.619143009185791 + 2.0 * 6.481423377990723
Epoch 220, val loss: 1.6458276510238647
Epoch 230, training loss: 14.52855110168457 = 1.5914539098739624 + 2.0 * 6.468548774719238
Epoch 230, val loss: 1.6225385665893555
Epoch 240, training loss: 14.475838661193848 = 1.5610584020614624 + 2.0 * 6.457390308380127
Epoch 240, val loss: 1.5971755981445312
Epoch 250, training loss: 14.431769371032715 = 1.527387022972107 + 2.0 * 6.452191352844238
Epoch 250, val loss: 1.5693886280059814
Epoch 260, training loss: 14.376165390014648 = 1.4911965131759644 + 2.0 * 6.442484378814697
Epoch 260, val loss: 1.539665699005127
Epoch 270, training loss: 14.317229270935059 = 1.4525378942489624 + 2.0 * 6.432345867156982
Epoch 270, val loss: 1.508256435394287
Epoch 280, training loss: 14.263087272644043 = 1.4113863706588745 + 2.0 * 6.4258503913879395
Epoch 280, val loss: 1.4750583171844482
Epoch 290, training loss: 14.206887245178223 = 1.367865800857544 + 2.0 * 6.419510841369629
Epoch 290, val loss: 1.440242052078247
Epoch 300, training loss: 14.15210247039795 = 1.322177767753601 + 2.0 * 6.414962291717529
Epoch 300, val loss: 1.4039543867111206
Epoch 310, training loss: 14.103604316711426 = 1.275418996810913 + 2.0 * 6.414092540740967
Epoch 310, val loss: 1.3673310279846191
Epoch 320, training loss: 14.0383882522583 = 1.2286664247512817 + 2.0 * 6.404860973358154
Epoch 320, val loss: 1.3309180736541748
Epoch 330, training loss: 13.983638763427734 = 1.1819031238555908 + 2.0 * 6.400867938995361
Epoch 330, val loss: 1.2949107885360718
Epoch 340, training loss: 13.929155349731445 = 1.1354268789291382 + 2.0 * 6.396864414215088
Epoch 340, val loss: 1.2594131231307983
Epoch 350, training loss: 13.877286911010742 = 1.0899605751037598 + 2.0 * 6.39366340637207
Epoch 350, val loss: 1.225106120109558
Epoch 360, training loss: 13.827322006225586 = 1.0461362600326538 + 2.0 * 6.3905930519104
Epoch 360, val loss: 1.1925560235977173
Epoch 370, training loss: 13.774893760681152 = 1.0041465759277344 + 2.0 * 6.385373592376709
Epoch 370, val loss: 1.1615833044052124
Epoch 380, training loss: 13.730853080749512 = 0.963713526725769 + 2.0 * 6.383569717407227
Epoch 380, val loss: 1.1321996450424194
Epoch 390, training loss: 13.680529594421387 = 0.9248945713043213 + 2.0 * 6.377817630767822
Epoch 390, val loss: 1.1044950485229492
Epoch 400, training loss: 13.649276733398438 = 0.887580156326294 + 2.0 * 6.380848407745361
Epoch 400, val loss: 1.0782501697540283
Epoch 410, training loss: 13.600184440612793 = 0.8522969484329224 + 2.0 * 6.37394380569458
Epoch 410, val loss: 1.053697109222412
Epoch 420, training loss: 13.557879447937012 = 0.8185858130455017 + 2.0 * 6.369647026062012
Epoch 420, val loss: 1.0307685136795044
Epoch 430, training loss: 13.522139549255371 = 0.7862331867218018 + 2.0 * 6.367953300476074
Epoch 430, val loss: 1.0091419219970703
Epoch 440, training loss: 13.485518455505371 = 0.755418062210083 + 2.0 * 6.365050315856934
Epoch 440, val loss: 0.9887806177139282
Epoch 450, training loss: 13.4468994140625 = 0.7259536981582642 + 2.0 * 6.360472679138184
Epoch 450, val loss: 0.9698523879051208
Epoch 460, training loss: 13.420528411865234 = 0.6976246237754822 + 2.0 * 6.361452102661133
Epoch 460, val loss: 0.9519633054733276
Epoch 470, training loss: 13.383744239807129 = 0.6704920530319214 + 2.0 * 6.356626033782959
Epoch 470, val loss: 0.935232937335968
Epoch 480, training loss: 13.374104499816895 = 0.6444121599197388 + 2.0 * 6.364846229553223
Epoch 480, val loss: 0.9195507764816284
Epoch 490, training loss: 13.322669982910156 = 0.6195266842842102 + 2.0 * 6.351571559906006
Epoch 490, val loss: 0.9049032926559448
Epoch 500, training loss: 13.291167259216309 = 0.5955521464347839 + 2.0 * 6.34780740737915
Epoch 500, val loss: 0.8913164138793945
Epoch 510, training loss: 13.262714385986328 = 0.5722905993461609 + 2.0 * 6.345211982727051
Epoch 510, val loss: 0.8785645365715027
Epoch 520, training loss: 13.237449645996094 = 0.5496474504470825 + 2.0 * 6.34390115737915
Epoch 520, val loss: 0.8665327429771423
Epoch 530, training loss: 13.221412658691406 = 0.5277624130249023 + 2.0 * 6.346825122833252
Epoch 530, val loss: 0.8552984595298767
Epoch 540, training loss: 13.188355445861816 = 0.5066221952438354 + 2.0 * 6.340866565704346
Epoch 540, val loss: 0.8450217843055725
Epoch 550, training loss: 13.160602569580078 = 0.48603907227516174 + 2.0 * 6.337281703948975
Epoch 550, val loss: 0.8354271650314331
Epoch 560, training loss: 13.136528015136719 = 0.46591418981552124 + 2.0 * 6.3353071212768555
Epoch 560, val loss: 0.8264878988265991
Epoch 570, training loss: 13.12130355834961 = 0.4463129937648773 + 2.0 * 6.33749532699585
Epoch 570, val loss: 0.8182691931724548
Epoch 580, training loss: 13.095847129821777 = 0.4274563193321228 + 2.0 * 6.334195613861084
Epoch 580, val loss: 0.8109472393989563
Epoch 590, training loss: 13.069676399230957 = 0.4090938866138458 + 2.0 * 6.330291271209717
Epoch 590, val loss: 0.8043182492256165
Epoch 600, training loss: 13.046667098999023 = 0.39119279384613037 + 2.0 * 6.327737331390381
Epoch 600, val loss: 0.7983247637748718
Epoch 610, training loss: 13.026246070861816 = 0.3737066984176636 + 2.0 * 6.326269626617432
Epoch 610, val loss: 0.793005645275116
Epoch 620, training loss: 13.008877754211426 = 0.3567410707473755 + 2.0 * 6.32606840133667
Epoch 620, val loss: 0.7882204651832581
Epoch 630, training loss: 12.988545417785645 = 0.34044530987739563 + 2.0 * 6.324049949645996
Epoch 630, val loss: 0.784107506275177
Epoch 640, training loss: 12.967395782470703 = 0.32470202445983887 + 2.0 * 6.321346759796143
Epoch 640, val loss: 0.7806205749511719
Epoch 650, training loss: 12.949122428894043 = 0.30948570370674133 + 2.0 * 6.319818496704102
Epoch 650, val loss: 0.7777325510978699
Epoch 660, training loss: 12.952130317687988 = 0.29480165243148804 + 2.0 * 6.328664302825928
Epoch 660, val loss: 0.7753735780715942
Epoch 670, training loss: 12.914777755737305 = 0.28079953789711 + 2.0 * 6.316988945007324
Epoch 670, val loss: 0.7735547423362732
Epoch 680, training loss: 12.899297714233398 = 0.26735153794288635 + 2.0 * 6.315973281860352
Epoch 680, val loss: 0.7722591757774353
Epoch 690, training loss: 12.885334968566895 = 0.2544547915458679 + 2.0 * 6.3154401779174805
Epoch 690, val loss: 0.7714171409606934
Epoch 700, training loss: 12.872126579284668 = 0.2420923411846161 + 2.0 * 6.315017223358154
Epoch 700, val loss: 0.7710217237472534
Epoch 710, training loss: 12.85727596282959 = 0.23030447959899902 + 2.0 * 6.313485622406006
Epoch 710, val loss: 0.771058201789856
Epoch 720, training loss: 12.839834213256836 = 0.21903938055038452 + 2.0 * 6.310397624969482
Epoch 720, val loss: 0.7714489698410034
Epoch 730, training loss: 12.834254264831543 = 0.20826341211795807 + 2.0 * 6.312995433807373
Epoch 730, val loss: 0.7721461653709412
Epoch 740, training loss: 12.82455825805664 = 0.19801343977451324 + 2.0 * 6.313272476196289
Epoch 740, val loss: 0.7731378078460693
Epoch 750, training loss: 12.806194305419922 = 0.18832023441791534 + 2.0 * 6.308937072753906
Epoch 750, val loss: 0.7743721604347229
Epoch 760, training loss: 12.78966236114502 = 0.17905639111995697 + 2.0 * 6.30530309677124
Epoch 760, val loss: 0.7758730053901672
Epoch 770, training loss: 12.779440879821777 = 0.17022500932216644 + 2.0 * 6.30460786819458
Epoch 770, val loss: 0.7775962352752686
Epoch 780, training loss: 12.786954879760742 = 0.16180849075317383 + 2.0 * 6.312573432922363
Epoch 780, val loss: 0.7794848680496216
Epoch 790, training loss: 12.758157730102539 = 0.1539066880941391 + 2.0 * 6.302125453948975
Epoch 790, val loss: 0.7816325426101685
Epoch 800, training loss: 12.747657775878906 = 0.14637821912765503 + 2.0 * 6.300639629364014
Epoch 800, val loss: 0.7839577198028564
Epoch 810, training loss: 12.740239143371582 = 0.13923099637031555 + 2.0 * 6.300504207611084
Epoch 810, val loss: 0.7864212989807129
Epoch 820, training loss: 12.740249633789062 = 0.1324819177389145 + 2.0 * 6.303884029388428
Epoch 820, val loss: 0.7890816926956177
Epoch 830, training loss: 12.728379249572754 = 0.1261173039674759 + 2.0 * 6.301130771636963
Epoch 830, val loss: 0.7917881011962891
Epoch 840, training loss: 12.713592529296875 = 0.12013141810894012 + 2.0 * 6.2967305183410645
Epoch 840, val loss: 0.7947136163711548
Epoch 850, training loss: 12.705212593078613 = 0.11445386707782745 + 2.0 * 6.295379161834717
Epoch 850, val loss: 0.7977952361106873
Epoch 860, training loss: 12.721972465515137 = 0.1090724989771843 + 2.0 * 6.306449890136719
Epoch 860, val loss: 0.8009691834449768
Epoch 870, training loss: 12.69159984588623 = 0.1040380522608757 + 2.0 * 6.29378080368042
Epoch 870, val loss: 0.8042219877243042
Epoch 880, training loss: 12.685776710510254 = 0.0992790088057518 + 2.0 * 6.293248653411865
Epoch 880, val loss: 0.8075529932975769
Epoch 890, training loss: 12.678789138793945 = 0.09477924555540085 + 2.0 * 6.2920050621032715
Epoch 890, val loss: 0.8110186457633972
Epoch 900, training loss: 12.693589210510254 = 0.09052590280771255 + 2.0 * 6.301531791687012
Epoch 900, val loss: 0.8144957423210144
Epoch 910, training loss: 12.672379493713379 = 0.08654865622520447 + 2.0 * 6.292915344238281
Epoch 910, val loss: 0.8181170225143433
Epoch 920, training loss: 12.661110877990723 = 0.0827917531132698 + 2.0 * 6.289159774780273
Epoch 920, val loss: 0.8217529654502869
Epoch 930, training loss: 12.660453796386719 = 0.07925000786781311 + 2.0 * 6.29060173034668
Epoch 930, val loss: 0.8254857659339905
Epoch 940, training loss: 12.655694961547852 = 0.07589716464281082 + 2.0 * 6.289898872375488
Epoch 940, val loss: 0.8291740417480469
Epoch 950, training loss: 12.646126747131348 = 0.07274653762578964 + 2.0 * 6.2866902351379395
Epoch 950, val loss: 0.8329512476921082
Epoch 960, training loss: 12.642083168029785 = 0.06976891309022903 + 2.0 * 6.286157131195068
Epoch 960, val loss: 0.8367509841918945
Epoch 970, training loss: 12.648138046264648 = 0.06695693731307983 + 2.0 * 6.290590763092041
Epoch 970, val loss: 0.8406054973602295
Epoch 980, training loss: 12.635030746459961 = 0.06430057436227798 + 2.0 * 6.285365104675293
Epoch 980, val loss: 0.844391942024231
Epoch 990, training loss: 12.62949275970459 = 0.061782654374837875 + 2.0 * 6.283854961395264
Epoch 990, val loss: 0.8482632040977478
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7740740740740741
0.8176067474960464
The final CL Acc:0.76049, 0.02188, The final GNN Acc:0.81427, 0.00237
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13140])
remove edge: torch.Size([2, 7996])
updated graph: torch.Size([2, 10580])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 19.157541275024414 = 1.9638845920562744 + 2.0 * 8.59682846069336
Epoch 0, val loss: 1.9698415994644165
Epoch 10, training loss: 19.14624786376953 = 1.953421950340271 + 2.0 * 8.596412658691406
Epoch 10, val loss: 1.9589089155197144
Epoch 20, training loss: 19.12594985961914 = 1.9400569200515747 + 2.0 * 8.59294605255127
Epoch 20, val loss: 1.9447914361953735
Epoch 30, training loss: 19.058807373046875 = 1.921291708946228 + 2.0 * 8.568758010864258
Epoch 30, val loss: 1.9249212741851807
Epoch 40, training loss: 18.776939392089844 = 1.898606538772583 + 2.0 * 8.439166069030762
Epoch 40, val loss: 1.9020466804504395
Epoch 50, training loss: 17.69692611694336 = 1.8759150505065918 + 2.0 * 7.910505294799805
Epoch 50, val loss: 1.8792786598205566
Epoch 60, training loss: 16.92985725402832 = 1.8560736179351807 + 2.0 * 7.536891460418701
Epoch 60, val loss: 1.8597437143325806
Epoch 70, training loss: 16.3861083984375 = 1.8397579193115234 + 2.0 * 7.27317476272583
Epoch 70, val loss: 1.8439807891845703
Epoch 80, training loss: 15.970552444458008 = 1.8232054710388184 + 2.0 * 7.073673248291016
Epoch 80, val loss: 1.828545331954956
Epoch 90, training loss: 15.684805870056152 = 1.8074690103530884 + 2.0 * 6.938668251037598
Epoch 90, val loss: 1.8140119314193726
Epoch 100, training loss: 15.468692779541016 = 1.7922545671463013 + 2.0 * 6.838219165802002
Epoch 100, val loss: 1.7998433113098145
Epoch 110, training loss: 15.312530517578125 = 1.7781219482421875 + 2.0 * 6.767204284667969
Epoch 110, val loss: 1.786399245262146
Epoch 120, training loss: 15.214897155761719 = 1.7649005651474 + 2.0 * 6.724998474121094
Epoch 120, val loss: 1.773213267326355
Epoch 130, training loss: 15.126007080078125 = 1.7505980730056763 + 2.0 * 6.687704563140869
Epoch 130, val loss: 1.7593977451324463
Epoch 140, training loss: 15.039363861083984 = 1.7358689308166504 + 2.0 * 6.651747226715088
Epoch 140, val loss: 1.7456865310668945
Epoch 150, training loss: 14.95926284790039 = 1.7205312252044678 + 2.0 * 6.619365692138672
Epoch 150, val loss: 1.731779932975769
Epoch 160, training loss: 14.88619613647461 = 1.703598141670227 + 2.0 * 6.591299057006836
Epoch 160, val loss: 1.7164690494537354
Epoch 170, training loss: 14.820263862609863 = 1.6844621896743774 + 2.0 * 6.567900657653809
Epoch 170, val loss: 1.6996599435806274
Epoch 180, training loss: 14.763521194458008 = 1.6626687049865723 + 2.0 * 6.550426483154297
Epoch 180, val loss: 1.6805737018585205
Epoch 190, training loss: 14.70357608795166 = 1.6379916667938232 + 2.0 * 6.532792091369629
Epoch 190, val loss: 1.6590185165405273
Epoch 200, training loss: 14.644742965698242 = 1.610230565071106 + 2.0 * 6.517256259918213
Epoch 200, val loss: 1.6348291635513306
Epoch 210, training loss: 14.586974143981934 = 1.5790717601776123 + 2.0 * 6.503951072692871
Epoch 210, val loss: 1.6076453924179077
Epoch 220, training loss: 14.538127899169922 = 1.5445846319198608 + 2.0 * 6.496771812438965
Epoch 220, val loss: 1.5777453184127808
Epoch 230, training loss: 14.473499298095703 = 1.5072252750396729 + 2.0 * 6.483137130737305
Epoch 230, val loss: 1.54552161693573
Epoch 240, training loss: 14.411733627319336 = 1.4671374559402466 + 2.0 * 6.4722981452941895
Epoch 240, val loss: 1.5110468864440918
Epoch 250, training loss: 14.350784301757812 = 1.4243308305740356 + 2.0 * 6.463226795196533
Epoch 250, val loss: 1.4743928909301758
Epoch 260, training loss: 14.289852142333984 = 1.379163384437561 + 2.0 * 6.455344200134277
Epoch 260, val loss: 1.435832142829895
Epoch 270, training loss: 14.234190940856934 = 1.3323876857757568 + 2.0 * 6.450901508331299
Epoch 270, val loss: 1.3962599039077759
Epoch 280, training loss: 14.169976234436035 = 1.2852413654327393 + 2.0 * 6.4423675537109375
Epoch 280, val loss: 1.3565555810928345
Epoch 290, training loss: 14.115413665771484 = 1.2380973100662231 + 2.0 * 6.438658237457275
Epoch 290, val loss: 1.3174071311950684
Epoch 300, training loss: 14.052628517150879 = 1.1919357776641846 + 2.0 * 6.430346488952637
Epoch 300, val loss: 1.2794578075408936
Epoch 310, training loss: 13.9979829788208 = 1.146789789199829 + 2.0 * 6.425596714019775
Epoch 310, val loss: 1.242722749710083
Epoch 320, training loss: 13.944366455078125 = 1.1031743288040161 + 2.0 * 6.420596122741699
Epoch 320, val loss: 1.2078245878219604
Epoch 330, training loss: 13.892577171325684 = 1.061522364616394 + 2.0 * 6.41552734375
Epoch 330, val loss: 1.174721360206604
Epoch 340, training loss: 13.843236923217773 = 1.0213674306869507 + 2.0 * 6.410934925079346
Epoch 340, val loss: 1.143331527709961
Epoch 350, training loss: 13.803457260131836 = 0.9827591180801392 + 2.0 * 6.410348892211914
Epoch 350, val loss: 1.1135934591293335
Epoch 360, training loss: 13.752467155456543 = 0.9459972977638245 + 2.0 * 6.403234958648682
Epoch 360, val loss: 1.0853936672210693
Epoch 370, training loss: 13.707839012145996 = 0.9103838801383972 + 2.0 * 6.3987274169921875
Epoch 370, val loss: 1.0584266185760498
Epoch 380, training loss: 13.671647071838379 = 0.8755290508270264 + 2.0 * 6.398058891296387
Epoch 380, val loss: 1.0322177410125732
Epoch 390, training loss: 13.627453804016113 = 0.8413906097412109 + 2.0 * 6.393031597137451
Epoch 390, val loss: 1.0067987442016602
Epoch 400, training loss: 13.61010456085205 = 0.8078755736351013 + 2.0 * 6.401114463806152
Epoch 400, val loss: 0.9822025895118713
Epoch 410, training loss: 13.548969268798828 = 0.7756258249282837 + 2.0 * 6.386671543121338
Epoch 410, val loss: 0.9584559798240662
Epoch 420, training loss: 13.509208679199219 = 0.7439436316490173 + 2.0 * 6.382632732391357
Epoch 420, val loss: 0.9355230331420898
Epoch 430, training loss: 13.469799995422363 = 0.712600588798523 + 2.0 * 6.378599643707275
Epoch 430, val loss: 0.913065493106842
Epoch 440, training loss: 13.433624267578125 = 0.6817294955253601 + 2.0 * 6.37594747543335
Epoch 440, val loss: 0.89121013879776
Epoch 450, training loss: 13.405689239501953 = 0.6515424251556396 + 2.0 * 6.377073287963867
Epoch 450, val loss: 0.8703257441520691
Epoch 460, training loss: 13.364114761352539 = 0.6224380731582642 + 2.0 * 6.370838165283203
Epoch 460, val loss: 0.8505272269248962
Epoch 470, training loss: 13.329106330871582 = 0.5941957235336304 + 2.0 * 6.36745548248291
Epoch 470, val loss: 0.8317683339118958
Epoch 480, training loss: 13.30288314819336 = 0.5669389963150024 + 2.0 * 6.367971897125244
Epoch 480, val loss: 0.8141281604766846
Epoch 490, training loss: 13.267244338989258 = 0.5408983826637268 + 2.0 * 6.363173007965088
Epoch 490, val loss: 0.7978517413139343
Epoch 500, training loss: 13.235092163085938 = 0.5159158706665039 + 2.0 * 6.359588146209717
Epoch 500, val loss: 0.7828119397163391
Epoch 510, training loss: 13.204154014587402 = 0.49179720878601074 + 2.0 * 6.356178283691406
Epoch 510, val loss: 0.7687950730323792
Epoch 520, training loss: 13.177456855773926 = 0.4684235751628876 + 2.0 * 6.354516506195068
Epoch 520, val loss: 0.7559075355529785
Epoch 530, training loss: 13.17303466796875 = 0.4462099075317383 + 2.0 * 6.363412380218506
Epoch 530, val loss: 0.7440975904464722
Epoch 540, training loss: 13.126060485839844 = 0.4248935878276825 + 2.0 * 6.350583553314209
Epoch 540, val loss: 0.7335330843925476
Epoch 550, training loss: 13.097476959228516 = 0.4045358896255493 + 2.0 * 6.346470355987549
Epoch 550, val loss: 0.7239306569099426
Epoch 560, training loss: 13.075339317321777 = 0.3848889172077179 + 2.0 * 6.3452253341674805
Epoch 560, val loss: 0.7151174545288086
Epoch 570, training loss: 13.060397148132324 = 0.36606040596961975 + 2.0 * 6.347168445587158
Epoch 570, val loss: 0.7071138620376587
Epoch 580, training loss: 13.03302001953125 = 0.34814706444740295 + 2.0 * 6.34243631362915
Epoch 580, val loss: 0.7000269293785095
Epoch 590, training loss: 13.008054733276367 = 0.33097347617149353 + 2.0 * 6.338540554046631
Epoch 590, val loss: 0.693730354309082
Epoch 600, training loss: 12.985755920410156 = 0.3144749701023102 + 2.0 * 6.3356404304504395
Epoch 600, val loss: 0.6880903840065002
Epoch 610, training loss: 12.986635208129883 = 0.2987059950828552 + 2.0 * 6.343964576721191
Epoch 610, val loss: 0.6831092834472656
Epoch 620, training loss: 12.952597618103027 = 0.2837288975715637 + 2.0 * 6.334434509277344
Epoch 620, val loss: 0.6787977814674377
Epoch 630, training loss: 12.928778648376465 = 0.26951414346694946 + 2.0 * 6.32963228225708
Epoch 630, val loss: 0.6751921772956848
Epoch 640, training loss: 12.910926818847656 = 0.2559494078159332 + 2.0 * 6.327488899230957
Epoch 640, val loss: 0.6721278429031372
Epoch 650, training loss: 12.911333084106445 = 0.2430509626865387 + 2.0 * 6.334141254425049
Epoch 650, val loss: 0.6696329712867737
Epoch 660, training loss: 12.881082534790039 = 0.23087160289287567 + 2.0 * 6.325105667114258
Epoch 660, val loss: 0.6676787734031677
Epoch 670, training loss: 12.865781784057617 = 0.21939559280872345 + 2.0 * 6.323193073272705
Epoch 670, val loss: 0.6662697792053223
Epoch 680, training loss: 12.851532936096191 = 0.2085103988647461 + 2.0 * 6.321511268615723
Epoch 680, val loss: 0.665368914604187
Epoch 690, training loss: 12.838912010192871 = 0.1982358694076538 + 2.0 * 6.320338249206543
Epoch 690, val loss: 0.6648712158203125
Epoch 700, training loss: 12.834625244140625 = 0.18856777250766754 + 2.0 * 6.323028564453125
Epoch 700, val loss: 0.6648359894752502
Epoch 710, training loss: 12.813224792480469 = 0.17953301966190338 + 2.0 * 6.316845893859863
Epoch 710, val loss: 0.6652462482452393
Epoch 720, training loss: 12.800202369689941 = 0.17097961902618408 + 2.0 * 6.314611434936523
Epoch 720, val loss: 0.6659632325172424
Epoch 730, training loss: 12.791271209716797 = 0.16288436949253082 + 2.0 * 6.314193248748779
Epoch 730, val loss: 0.6670714020729065
Epoch 740, training loss: 12.78557300567627 = 0.15523016452789307 + 2.0 * 6.315171241760254
Epoch 740, val loss: 0.6684677600860596
Epoch 750, training loss: 12.775473594665527 = 0.14805075526237488 + 2.0 * 6.313711643218994
Epoch 750, val loss: 0.6701663136482239
Epoch 760, training loss: 12.76121997833252 = 0.14126840233802795 + 2.0 * 6.309975624084473
Epoch 760, val loss: 0.6721230149269104
Epoch 770, training loss: 12.751416206359863 = 0.1348646581172943 + 2.0 * 6.3082756996154785
Epoch 770, val loss: 0.674322247505188
Epoch 780, training loss: 12.761443138122559 = 0.12880440056324005 + 2.0 * 6.316319465637207
Epoch 780, val loss: 0.6766939163208008
Epoch 790, training loss: 12.738092422485352 = 0.12313544005155563 + 2.0 * 6.307478427886963
Epoch 790, val loss: 0.6793293356895447
Epoch 800, training loss: 12.729717254638672 = 0.11777757853269577 + 2.0 * 6.305969715118408
Epoch 800, val loss: 0.6821458339691162
Epoch 810, training loss: 12.720759391784668 = 0.11269879341125488 + 2.0 * 6.304030418395996
Epoch 810, val loss: 0.6851041316986084
Epoch 820, training loss: 12.722947120666504 = 0.1078864336013794 + 2.0 * 6.307530403137207
Epoch 820, val loss: 0.6881992816925049
Epoch 830, training loss: 12.710550308227539 = 0.1033504456281662 + 2.0 * 6.303599834442139
Epoch 830, val loss: 0.6914175152778625
Epoch 840, training loss: 12.704668045043945 = 0.0990377739071846 + 2.0 * 6.302814960479736
Epoch 840, val loss: 0.6947915554046631
Epoch 850, training loss: 12.706244468688965 = 0.09495195746421814 + 2.0 * 6.3056464195251465
Epoch 850, val loss: 0.698211133480072
Epoch 860, training loss: 12.69006633758545 = 0.09112051129341125 + 2.0 * 6.299472808837891
Epoch 860, val loss: 0.7017836570739746
Epoch 870, training loss: 12.681411743164062 = 0.08745855838060379 + 2.0 * 6.296976566314697
Epoch 870, val loss: 0.7053821682929993
Epoch 880, training loss: 12.676078796386719 = 0.083990678191185 + 2.0 * 6.296043872833252
Epoch 880, val loss: 0.7091027498245239
Epoch 890, training loss: 12.6788330078125 = 0.08068393170833588 + 2.0 * 6.299074649810791
Epoch 890, val loss: 0.7128941416740417
Epoch 900, training loss: 12.669854164123535 = 0.07754985988140106 + 2.0 * 6.296152114868164
Epoch 900, val loss: 0.7167205214500427
Epoch 910, training loss: 12.66600227355957 = 0.07459073513746262 + 2.0 * 6.295705795288086
Epoch 910, val loss: 0.7205885648727417
Epoch 920, training loss: 12.662924766540527 = 0.07176870107650757 + 2.0 * 6.2955780029296875
Epoch 920, val loss: 0.7244959473609924
Epoch 930, training loss: 12.65962028503418 = 0.0690867230296135 + 2.0 * 6.295266628265381
Epoch 930, val loss: 0.7284284830093384
Epoch 940, training loss: 12.649580001831055 = 0.06655387580394745 + 2.0 * 6.291512966156006
Epoch 940, val loss: 0.7324000000953674
Epoch 950, training loss: 12.64512825012207 = 0.06412727385759354 + 2.0 * 6.290500640869141
Epoch 950, val loss: 0.7363717555999756
Epoch 960, training loss: 12.643784523010254 = 0.061819687485694885 + 2.0 * 6.290982246398926
Epoch 960, val loss: 0.7403995990753174
Epoch 970, training loss: 12.643945693969727 = 0.059619925916194916 + 2.0 * 6.292162895202637
Epoch 970, val loss: 0.7444142699241638
Epoch 980, training loss: 12.633378028869629 = 0.0575384646654129 + 2.0 * 6.287919998168945
Epoch 980, val loss: 0.7484222650527954
Epoch 990, training loss: 12.628323554992676 = 0.05555948615074158 + 2.0 * 6.28638219833374
Epoch 990, val loss: 0.7524377703666687
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7925925925925926
0.8397469688982605
=== training gcn model ===
Epoch 0, training loss: 19.156335830688477 = 1.962689995765686 + 2.0 * 8.596822738647461
Epoch 0, val loss: 1.9713139533996582
Epoch 10, training loss: 19.14352035522461 = 1.950721263885498 + 2.0 * 8.596399307250977
Epoch 10, val loss: 1.9585808515548706
Epoch 20, training loss: 19.121074676513672 = 1.9354482889175415 + 2.0 * 8.592813491821289
Epoch 20, val loss: 1.9420602321624756
Epoch 30, training loss: 19.049238204956055 = 1.914227843284607 + 2.0 * 8.5675048828125
Epoch 30, val loss: 1.9191445112228394
Epoch 40, training loss: 18.758203506469727 = 1.888377070426941 + 2.0 * 8.434913635253906
Epoch 40, val loss: 1.892646074295044
Epoch 50, training loss: 17.789976119995117 = 1.8617936372756958 + 2.0 * 7.964090824127197
Epoch 50, val loss: 1.8663489818572998
Epoch 60, training loss: 16.835723876953125 = 1.842463493347168 + 2.0 * 7.49662971496582
Epoch 60, val loss: 1.8485859632492065
Epoch 70, training loss: 16.087671279907227 = 1.828937292098999 + 2.0 * 7.129366874694824
Epoch 70, val loss: 1.8347675800323486
Epoch 80, training loss: 15.76794147491455 = 1.8158009052276611 + 2.0 * 6.976070404052734
Epoch 80, val loss: 1.8216952085494995
Epoch 90, training loss: 15.507637023925781 = 1.80160391330719 + 2.0 * 6.853016376495361
Epoch 90, val loss: 1.80799400806427
Epoch 100, training loss: 15.335450172424316 = 1.7881609201431274 + 2.0 * 6.77364444732666
Epoch 100, val loss: 1.7953466176986694
Epoch 110, training loss: 15.184244155883789 = 1.7750308513641357 + 2.0 * 6.704606533050537
Epoch 110, val loss: 1.7831668853759766
Epoch 120, training loss: 15.072233200073242 = 1.7620152235031128 + 2.0 * 6.65510892868042
Epoch 120, val loss: 1.771000623703003
Epoch 130, training loss: 14.973346710205078 = 1.7485700845718384 + 2.0 * 6.6123881340026855
Epoch 130, val loss: 1.7586121559143066
Epoch 140, training loss: 14.896612167358398 = 1.7340915203094482 + 2.0 * 6.5812602043151855
Epoch 140, val loss: 1.7454307079315186
Epoch 150, training loss: 14.82798957824707 = 1.71807062625885 + 2.0 * 6.554959297180176
Epoch 150, val loss: 1.7311675548553467
Epoch 160, training loss: 14.76773738861084 = 1.7003015279769897 + 2.0 * 6.533718109130859
Epoch 160, val loss: 1.7155216932296753
Epoch 170, training loss: 14.712745666503906 = 1.680799961090088 + 2.0 * 6.515973091125488
Epoch 170, val loss: 1.6985646486282349
Epoch 180, training loss: 14.65328598022461 = 1.6593222618103027 + 2.0 * 6.496982097625732
Epoch 180, val loss: 1.6800731420516968
Epoch 190, training loss: 14.5983304977417 = 1.6356201171875 + 2.0 * 6.4813551902771
Epoch 190, val loss: 1.6596492528915405
Epoch 200, training loss: 14.556002616882324 = 1.6095565557479858 + 2.0 * 6.4732232093811035
Epoch 200, val loss: 1.6373683214187622
Epoch 210, training loss: 14.494808197021484 = 1.581268310546875 + 2.0 * 6.456769943237305
Epoch 210, val loss: 1.6131463050842285
Epoch 220, training loss: 14.445218086242676 = 1.5507789850234985 + 2.0 * 6.447219371795654
Epoch 220, val loss: 1.5871691703796387
Epoch 230, training loss: 14.392345428466797 = 1.5182512998580933 + 2.0 * 6.437047004699707
Epoch 230, val loss: 1.5596325397491455
Epoch 240, training loss: 14.341259956359863 = 1.4837278127670288 + 2.0 * 6.428766250610352
Epoch 240, val loss: 1.5305202007293701
Epoch 250, training loss: 14.29069995880127 = 1.4475284814834595 + 2.0 * 6.421585559844971
Epoch 250, val loss: 1.5002427101135254
Epoch 260, training loss: 14.244384765625 = 1.4101022481918335 + 2.0 * 6.417141437530518
Epoch 260, val loss: 1.4693740606307983
Epoch 270, training loss: 14.190169334411621 = 1.372124433517456 + 2.0 * 6.409022331237793
Epoch 270, val loss: 1.4384346008300781
Epoch 280, training loss: 14.139148712158203 = 1.3336838483810425 + 2.0 * 6.4027323722839355
Epoch 280, val loss: 1.4074749946594238
Epoch 290, training loss: 14.091513633728027 = 1.2947895526885986 + 2.0 * 6.398362159729004
Epoch 290, val loss: 1.3765056133270264
Epoch 300, training loss: 14.043906211853027 = 1.255698323249817 + 2.0 * 6.39410400390625
Epoch 300, val loss: 1.3456202745437622
Epoch 310, training loss: 13.99219799041748 = 1.2166763544082642 + 2.0 * 6.387760639190674
Epoch 310, val loss: 1.3151042461395264
Epoch 320, training loss: 13.944450378417969 = 1.177487850189209 + 2.0 * 6.383481502532959
Epoch 320, val loss: 1.2847282886505127
Epoch 330, training loss: 13.898539543151855 = 1.138196587562561 + 2.0 * 6.380171298980713
Epoch 330, val loss: 1.2544548511505127
Epoch 340, training loss: 13.851546287536621 = 1.0991371870040894 + 2.0 * 6.376204490661621
Epoch 340, val loss: 1.2244162559509277
Epoch 350, training loss: 13.803646087646484 = 1.060197114944458 + 2.0 * 6.371724605560303
Epoch 350, val loss: 1.1946178674697876
Epoch 360, training loss: 13.762130737304688 = 1.0212211608886719 + 2.0 * 6.370454788208008
Epoch 360, val loss: 1.1648035049438477
Epoch 370, training loss: 13.7140474319458 = 0.9824666380882263 + 2.0 * 6.365790367126465
Epoch 370, val loss: 1.135150671005249
Epoch 380, training loss: 13.667254447937012 = 0.9439778923988342 + 2.0 * 6.361638069152832
Epoch 380, val loss: 1.1057673692703247
Epoch 390, training loss: 13.626426696777344 = 0.906105637550354 + 2.0 * 6.3601603507995605
Epoch 390, val loss: 1.076884388923645
Epoch 400, training loss: 13.579795837402344 = 0.8691504001617432 + 2.0 * 6.35532283782959
Epoch 400, val loss: 1.048818826675415
Epoch 410, training loss: 13.537590980529785 = 0.833252489566803 + 2.0 * 6.352169036865234
Epoch 410, val loss: 1.0216717720031738
Epoch 420, training loss: 13.511494636535645 = 0.7985510230064392 + 2.0 * 6.356472015380859
Epoch 420, val loss: 0.995457649230957
Epoch 430, training loss: 13.463123321533203 = 0.7653505802154541 + 2.0 * 6.348886489868164
Epoch 430, val loss: 0.9706176519393921
Epoch 440, training loss: 13.425508499145508 = 0.7336239814758301 + 2.0 * 6.345942497253418
Epoch 440, val loss: 0.9473320841789246
Epoch 450, training loss: 13.388301849365234 = 0.7031823396682739 + 2.0 * 6.342559814453125
Epoch 450, val loss: 0.9251548647880554
Epoch 460, training loss: 13.354142189025879 = 0.6738230586051941 + 2.0 * 6.3401594161987305
Epoch 460, val loss: 0.9041754007339478
Epoch 470, training loss: 13.318318367004395 = 0.645404577255249 + 2.0 * 6.336456775665283
Epoch 470, val loss: 0.8842442035675049
Epoch 480, training loss: 13.290139198303223 = 0.6178752183914185 + 2.0 * 6.336132049560547
Epoch 480, val loss: 0.8652337193489075
Epoch 490, training loss: 13.255208015441895 = 0.5914202928543091 + 2.0 * 6.3318939208984375
Epoch 490, val loss: 0.8473713994026184
Epoch 500, training loss: 13.2273530960083 = 0.5656392574310303 + 2.0 * 6.330856800079346
Epoch 500, val loss: 0.8306276798248291
Epoch 510, training loss: 13.197490692138672 = 0.5403523445129395 + 2.0 * 6.328568935394287
Epoch 510, val loss: 0.8146493434906006
Epoch 520, training loss: 13.17734432220459 = 0.5155555605888367 + 2.0 * 6.330894470214844
Epoch 520, val loss: 0.7995166182518005
Epoch 530, training loss: 13.145380020141602 = 0.49143245816230774 + 2.0 * 6.326973915100098
Epoch 530, val loss: 0.7854448556900024
Epoch 540, training loss: 13.112627029418945 = 0.467899888753891 + 2.0 * 6.322363376617432
Epoch 540, val loss: 0.7723322510719299
Epoch 550, training loss: 13.088004112243652 = 0.4448550343513489 + 2.0 * 6.321574687957764
Epoch 550, val loss: 0.7599993944168091
Epoch 560, training loss: 13.070112228393555 = 0.4224680960178375 + 2.0 * 6.323822021484375
Epoch 560, val loss: 0.7485160827636719
Epoch 570, training loss: 13.039668083190918 = 0.4008210003376007 + 2.0 * 6.319423675537109
Epoch 570, val loss: 0.7380560636520386
Epoch 580, training loss: 13.011279106140137 = 0.3798343241214752 + 2.0 * 6.315722465515137
Epoch 580, val loss: 0.7284318208694458
Epoch 590, training loss: 12.98812484741211 = 0.35951027274131775 + 2.0 * 6.31430721282959
Epoch 590, val loss: 0.719538688659668
Epoch 600, training loss: 12.968125343322754 = 0.3399023115634918 + 2.0 * 6.314111709594727
Epoch 600, val loss: 0.7114564180374146
Epoch 610, training loss: 12.951613426208496 = 0.3211565613746643 + 2.0 * 6.315228462219238
Epoch 610, val loss: 0.7042219638824463
Epoch 620, training loss: 12.92530345916748 = 0.3033616244792938 + 2.0 * 6.310970783233643
Epoch 620, val loss: 0.697752058506012
Epoch 630, training loss: 12.914070129394531 = 0.2864336371421814 + 2.0 * 6.313818454742432
Epoch 630, val loss: 0.6920878291130066
Epoch 640, training loss: 12.887828826904297 = 0.2703867554664612 + 2.0 * 6.30872106552124
Epoch 640, val loss: 0.6871874332427979
Epoch 650, training loss: 12.875896453857422 = 0.25526872277259827 + 2.0 * 6.310313701629639
Epoch 650, val loss: 0.6831201910972595
Epoch 660, training loss: 12.853081703186035 = 0.24111653864383698 + 2.0 * 6.30598258972168
Epoch 660, val loss: 0.6797543168067932
Epoch 670, training loss: 12.83852767944336 = 0.22779595851898193 + 2.0 * 6.305366039276123
Epoch 670, val loss: 0.6771631836891174
Epoch 680, training loss: 12.819979667663574 = 0.215362548828125 + 2.0 * 6.302308559417725
Epoch 680, val loss: 0.675225555896759
Epoch 690, training loss: 12.807384490966797 = 0.2037278711795807 + 2.0 * 6.301828384399414
Epoch 690, val loss: 0.6739935874938965
Epoch 700, training loss: 12.794000625610352 = 0.19285355508327484 + 2.0 * 6.300573348999023
Epoch 700, val loss: 0.6733349561691284
Epoch 710, training loss: 12.786044120788574 = 0.18267636001110077 + 2.0 * 6.3016839027404785
Epoch 710, val loss: 0.6732010245323181
Epoch 720, training loss: 12.786234855651855 = 0.1731587052345276 + 2.0 * 6.306538105010986
Epoch 720, val loss: 0.6735599637031555
Epoch 730, training loss: 12.762749671936035 = 0.16438011825084686 + 2.0 * 6.299184799194336
Epoch 730, val loss: 0.6743601560592651
Epoch 740, training loss: 12.748275756835938 = 0.15613515675067902 + 2.0 * 6.296070098876953
Epoch 740, val loss: 0.6756574511528015
Epoch 750, training loss: 12.73800277709961 = 0.1484176367521286 + 2.0 * 6.294792652130127
Epoch 750, val loss: 0.6773172616958618
Epoch 760, training loss: 12.73228645324707 = 0.14116384088993073 + 2.0 * 6.29556131362915
Epoch 760, val loss: 0.6793174743652344
Epoch 770, training loss: 12.72585678100586 = 0.13437442481517792 + 2.0 * 6.295741081237793
Epoch 770, val loss: 0.6815659403800964
Epoch 780, training loss: 12.713826179504395 = 0.12806054949760437 + 2.0 * 6.292882919311523
Epoch 780, val loss: 0.6841268539428711
Epoch 790, training loss: 12.704435348510742 = 0.1221272274851799 + 2.0 * 6.291153907775879
Epoch 790, val loss: 0.6869896054267883
Epoch 800, training loss: 12.698689460754395 = 0.1165391206741333 + 2.0 * 6.291075229644775
Epoch 800, val loss: 0.690111517906189
Epoch 810, training loss: 12.693851470947266 = 0.1112784594297409 + 2.0 * 6.291286468505859
Epoch 810, val loss: 0.6933638453483582
Epoch 820, training loss: 12.683001518249512 = 0.10631705075502396 + 2.0 * 6.288341999053955
Epoch 820, val loss: 0.696840226650238
Epoch 830, training loss: 12.682106971740723 = 0.10165999829769135 + 2.0 * 6.290223598480225
Epoch 830, val loss: 0.7005124092102051
Epoch 840, training loss: 12.669954299926758 = 0.09727514535188675 + 2.0 * 6.28633975982666
Epoch 840, val loss: 0.704254150390625
Epoch 850, training loss: 12.663637161254883 = 0.09312944114208221 + 2.0 * 6.285254001617432
Epoch 850, val loss: 0.708198606967926
Epoch 860, training loss: 12.663640022277832 = 0.08920151740312576 + 2.0 * 6.287219047546387
Epoch 860, val loss: 0.7122542262077332
Epoch 870, training loss: 12.653619766235352 = 0.08551014959812164 + 2.0 * 6.284054756164551
Epoch 870, val loss: 0.7163699865341187
Epoch 880, training loss: 12.657618522644043 = 0.08200346678495407 + 2.0 * 6.287807464599609
Epoch 880, val loss: 0.7205607295036316
Epoch 890, training loss: 12.64345645904541 = 0.07872360199689865 + 2.0 * 6.2823662757873535
Epoch 890, val loss: 0.7248303294181824
Epoch 900, training loss: 12.63963794708252 = 0.07560374587774277 + 2.0 * 6.282017230987549
Epoch 900, val loss: 0.7291831970214844
Epoch 910, training loss: 12.63350772857666 = 0.07264510542154312 + 2.0 * 6.280431270599365
Epoch 910, val loss: 0.7336443066596985
Epoch 920, training loss: 12.647065162658691 = 0.06983477622270584 + 2.0 * 6.2886152267456055
Epoch 920, val loss: 0.7381198406219482
Epoch 930, training loss: 12.631391525268555 = 0.06716316193342209 + 2.0 * 6.282114028930664
Epoch 930, val loss: 0.7425481677055359
Epoch 940, training loss: 12.62181282043457 = 0.0646432414650917 + 2.0 * 6.278584957122803
Epoch 940, val loss: 0.7471263408660889
Epoch 950, training loss: 12.622292518615723 = 0.062239401042461395 + 2.0 * 6.280026435852051
Epoch 950, val loss: 0.7517141699790955
Epoch 960, training loss: 12.615781784057617 = 0.05995713546872139 + 2.0 * 6.277912139892578
Epoch 960, val loss: 0.7562947869300842
Epoch 970, training loss: 12.6211576461792 = 0.057783935219049454 + 2.0 * 6.281686782836914
Epoch 970, val loss: 0.7608449459075928
Epoch 980, training loss: 12.610109329223633 = 0.05573918670415878 + 2.0 * 6.277184963226318
Epoch 980, val loss: 0.765447199344635
Epoch 990, training loss: 12.603200912475586 = 0.053774941712617874 + 2.0 * 6.274713039398193
Epoch 990, val loss: 0.7700578570365906
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8111111111111111
0.8397469688982605
=== training gcn model ===
Epoch 0, training loss: 19.146413803100586 = 1.9527267217636108 + 2.0 * 8.596843719482422
Epoch 0, val loss: 1.9494836330413818
Epoch 10, training loss: 19.136167526245117 = 1.942977786064148 + 2.0 * 8.59659481048584
Epoch 10, val loss: 1.9403371810913086
Epoch 20, training loss: 19.120567321777344 = 1.9311548471450806 + 2.0 * 8.594706535339355
Epoch 20, val loss: 1.928828477859497
Epoch 30, training loss: 19.07210350036621 = 1.9148623943328857 + 2.0 * 8.578620910644531
Epoch 30, val loss: 1.9125994443893433
Epoch 40, training loss: 18.815601348876953 = 1.8933554887771606 + 2.0 * 8.461122512817383
Epoch 40, val loss: 1.8915480375289917
Epoch 50, training loss: 17.550769805908203 = 1.8696086406707764 + 2.0 * 7.840580463409424
Epoch 50, val loss: 1.8686208724975586
Epoch 60, training loss: 16.688446044921875 = 1.8487954139709473 + 2.0 * 7.419825077056885
Epoch 60, val loss: 1.849222183227539
Epoch 70, training loss: 16.032855987548828 = 1.8336974382400513 + 2.0 * 7.099578857421875
Epoch 70, val loss: 1.8345646858215332
Epoch 80, training loss: 15.660138130187988 = 1.819232702255249 + 2.0 * 6.92045259475708
Epoch 80, val loss: 1.8205231428146362
Epoch 90, training loss: 15.456958770751953 = 1.8048937320709229 + 2.0 * 6.826032638549805
Epoch 90, val loss: 1.8063706159591675
Epoch 100, training loss: 15.327987670898438 = 1.7892909049987793 + 2.0 * 6.76934814453125
Epoch 100, val loss: 1.7917805910110474
Epoch 110, training loss: 15.203476905822754 = 1.7737095355987549 + 2.0 * 6.714883804321289
Epoch 110, val loss: 1.7776391506195068
Epoch 120, training loss: 15.10494327545166 = 1.7583287954330444 + 2.0 * 6.673307418823242
Epoch 120, val loss: 1.7636064291000366
Epoch 130, training loss: 15.018485069274902 = 1.7424135208129883 + 2.0 * 6.638035774230957
Epoch 130, val loss: 1.7491103410720825
Epoch 140, training loss: 14.95035171508789 = 1.7255523204803467 + 2.0 * 6.612399578094482
Epoch 140, val loss: 1.7341021299362183
Epoch 150, training loss: 14.881302833557129 = 1.7068723440170288 + 2.0 * 6.587215423583984
Epoch 150, val loss: 1.7177215814590454
Epoch 160, training loss: 14.818842887878418 = 1.685716986656189 + 2.0 * 6.566563129425049
Epoch 160, val loss: 1.6996177434921265
Epoch 170, training loss: 14.757682800292969 = 1.6616564989089966 + 2.0 * 6.548013210296631
Epoch 170, val loss: 1.6792089939117432
Epoch 180, training loss: 14.698213577270508 = 1.6345562934875488 + 2.0 * 6.5318284034729
Epoch 180, val loss: 1.6563503742218018
Epoch 190, training loss: 14.638471603393555 = 1.6041682958602905 + 2.0 * 6.517151832580566
Epoch 190, val loss: 1.6308232545852661
Epoch 200, training loss: 14.57540512084961 = 1.5703740119934082 + 2.0 * 6.50251579284668
Epoch 200, val loss: 1.6023904085159302
Epoch 210, training loss: 14.514047622680664 = 1.5331497192382812 + 2.0 * 6.490448951721191
Epoch 210, val loss: 1.5711009502410889
Epoch 220, training loss: 14.447027206420898 = 1.4927903413772583 + 2.0 * 6.477118492126465
Epoch 220, val loss: 1.5375127792358398
Epoch 230, training loss: 14.378111839294434 = 1.4493074417114258 + 2.0 * 6.464402198791504
Epoch 230, val loss: 1.5014408826828003
Epoch 240, training loss: 14.318954467773438 = 1.4033184051513672 + 2.0 * 6.457818031311035
Epoch 240, val loss: 1.4632564783096313
Epoch 250, training loss: 14.242473602294922 = 1.3559597730636597 + 2.0 * 6.443256855010986
Epoch 250, val loss: 1.4242717027664185
Epoch 260, training loss: 14.1734619140625 = 1.307425856590271 + 2.0 * 6.433018207550049
Epoch 260, val loss: 1.3845622539520264
Epoch 270, training loss: 14.10961627960205 = 1.2580087184906006 + 2.0 * 6.4258036613464355
Epoch 270, val loss: 1.3443771600723267
Epoch 280, training loss: 14.046425819396973 = 1.2089823484420776 + 2.0 * 6.418721675872803
Epoch 280, val loss: 1.3044558763504028
Epoch 290, training loss: 13.9794921875 = 1.1606024503707886 + 2.0 * 6.409444808959961
Epoch 290, val loss: 1.2654846906661987
Epoch 300, training loss: 13.928813934326172 = 1.1132121086120605 + 2.0 * 6.407800674438477
Epoch 300, val loss: 1.2275491952896118
Epoch 310, training loss: 13.859108924865723 = 1.0676841735839844 + 2.0 * 6.395712375640869
Epoch 310, val loss: 1.1909931898117065
Epoch 320, training loss: 13.804915428161621 = 1.0238734483718872 + 2.0 * 6.390521049499512
Epoch 320, val loss: 1.1561492681503296
Epoch 330, training loss: 13.750320434570312 = 0.9816775918006897 + 2.0 * 6.384321212768555
Epoch 330, val loss: 1.1225937604904175
Epoch 340, training loss: 13.71849250793457 = 0.9411232471466064 + 2.0 * 6.3886847496032715
Epoch 340, val loss: 1.0902131795883179
Epoch 350, training loss: 13.659860610961914 = 0.9027523398399353 + 2.0 * 6.378554344177246
Epoch 350, val loss: 1.0597734451293945
Epoch 360, training loss: 13.610816955566406 = 0.8665184378623962 + 2.0 * 6.372149467468262
Epoch 360, val loss: 1.0311717987060547
Epoch 370, training loss: 13.570474624633789 = 0.8321700692176819 + 2.0 * 6.369152069091797
Epoch 370, val loss: 1.0041719675064087
Epoch 380, training loss: 13.540745735168457 = 0.8000184297561646 + 2.0 * 6.370363712310791
Epoch 380, val loss: 0.9786659479141235
Epoch 390, training loss: 13.491983413696289 = 0.7698422074317932 + 2.0 * 6.36107063293457
Epoch 390, val loss: 0.9553117752075195
Epoch 400, training loss: 13.455995559692383 = 0.7414088249206543 + 2.0 * 6.357293128967285
Epoch 400, val loss: 0.9335177540779114
Epoch 410, training loss: 13.422942161560059 = 0.7143746614456177 + 2.0 * 6.354283809661865
Epoch 410, val loss: 0.9130919575691223
Epoch 420, training loss: 13.39188003540039 = 0.6885542869567871 + 2.0 * 6.351663112640381
Epoch 420, val loss: 0.893793523311615
Epoch 430, training loss: 13.362170219421387 = 0.6638491153717041 + 2.0 * 6.349160671234131
Epoch 430, val loss: 0.8757562041282654
Epoch 440, training loss: 13.336917877197266 = 0.6399638056755066 + 2.0 * 6.348476886749268
Epoch 440, val loss: 0.8586932420730591
Epoch 450, training loss: 13.30567741394043 = 0.6168013215065002 + 2.0 * 6.344438076019287
Epoch 450, val loss: 0.842306911945343
Epoch 460, training loss: 13.277360916137695 = 0.5939617156982422 + 2.0 * 6.341699600219727
Epoch 460, val loss: 0.826718270778656
Epoch 470, training loss: 13.255922317504883 = 0.5715451836585999 + 2.0 * 6.342188358306885
Epoch 470, val loss: 0.8116394281387329
Epoch 480, training loss: 13.226006507873535 = 0.5493246912956238 + 2.0 * 6.338340759277344
Epoch 480, val loss: 0.7972375154495239
Epoch 490, training loss: 13.19874095916748 = 0.5274284482002258 + 2.0 * 6.33565616607666
Epoch 490, val loss: 0.7833267450332642
Epoch 500, training loss: 13.169278144836426 = 0.5057100653648376 + 2.0 * 6.331784248352051
Epoch 500, val loss: 0.769996702671051
Epoch 510, training loss: 13.144145011901855 = 0.48422855138778687 + 2.0 * 6.329958438873291
Epoch 510, val loss: 0.7573064565658569
Epoch 520, training loss: 13.119638442993164 = 0.46296262741088867 + 2.0 * 6.328338146209717
Epoch 520, val loss: 0.7453310489654541
Epoch 530, training loss: 13.105749130249023 = 0.44193220138549805 + 2.0 * 6.331908702850342
Epoch 530, val loss: 0.7339340448379517
Epoch 540, training loss: 13.072196960449219 = 0.42145752906799316 + 2.0 * 6.325369834899902
Epoch 540, val loss: 0.7233306169509888
Epoch 550, training loss: 13.04908561706543 = 0.40141600370407104 + 2.0 * 6.3238348960876465
Epoch 550, val loss: 0.7134888172149658
Epoch 560, training loss: 13.0262451171875 = 0.381851464509964 + 2.0 * 6.322196960449219
Epoch 560, val loss: 0.7043667435646057
Epoch 570, training loss: 13.003454208374023 = 0.3628346920013428 + 2.0 * 6.320309638977051
Epoch 570, val loss: 0.6960356831550598
Epoch 580, training loss: 12.981745719909668 = 0.3444689214229584 + 2.0 * 6.318638324737549
Epoch 580, val loss: 0.6885603666305542
Epoch 590, training loss: 12.962496757507324 = 0.32684603333473206 + 2.0 * 6.3178253173828125
Epoch 590, val loss: 0.6818066835403442
Epoch 600, training loss: 12.940404891967773 = 0.3099617660045624 + 2.0 * 6.315221786499023
Epoch 600, val loss: 0.6759551763534546
Epoch 610, training loss: 12.92507266998291 = 0.2938438355922699 + 2.0 * 6.315614223480225
Epoch 610, val loss: 0.6708418726921082
Epoch 620, training loss: 12.904712677001953 = 0.278569757938385 + 2.0 * 6.313071250915527
Epoch 620, val loss: 0.6664776802062988
Epoch 630, training loss: 12.893684387207031 = 0.26406750082969666 + 2.0 * 6.314808368682861
Epoch 630, val loss: 0.662947416305542
Epoch 640, training loss: 12.878496170043945 = 0.2504502534866333 + 2.0 * 6.314023017883301
Epoch 640, val loss: 0.6599626541137695
Epoch 650, training loss: 12.854418754577637 = 0.23758035898208618 + 2.0 * 6.308419227600098
Epoch 650, val loss: 0.6576366424560547
Epoch 660, training loss: 12.839254379272461 = 0.22540183365345 + 2.0 * 6.306926250457764
Epoch 660, val loss: 0.6558429002761841
Epoch 670, training loss: 12.824263572692871 = 0.21387822926044464 + 2.0 * 6.305192470550537
Epoch 670, val loss: 0.6545016169548035
Epoch 680, training loss: 12.814946174621582 = 0.20295868813991547 + 2.0 * 6.305993556976318
Epoch 680, val loss: 0.6536505818367004
Epoch 690, training loss: 12.825432777404785 = 0.1927039921283722 + 2.0 * 6.316364288330078
Epoch 690, val loss: 0.6531878113746643
Epoch 700, training loss: 12.795750617980957 = 0.18313869833946228 + 2.0 * 6.306305885314941
Epoch 700, val loss: 0.653080940246582
Epoch 710, training loss: 12.778457641601562 = 0.17414499819278717 + 2.0 * 6.302156448364258
Epoch 710, val loss: 0.6534270644187927
Epoch 720, training loss: 12.765364646911621 = 0.16563661396503448 + 2.0 * 6.299863815307617
Epoch 720, val loss: 0.6541111469268799
Epoch 730, training loss: 12.78532600402832 = 0.15759868919849396 + 2.0 * 6.313863754272461
Epoch 730, val loss: 0.6550016403198242
Epoch 740, training loss: 12.748047828674316 = 0.1499975174665451 + 2.0 * 6.299025058746338
Epoch 740, val loss: 0.6562516689300537
Epoch 750, training loss: 12.737773895263672 = 0.14286942780017853 + 2.0 * 6.297452449798584
Epoch 750, val loss: 0.6578344106674194
Epoch 760, training loss: 12.726242065429688 = 0.1361413300037384 + 2.0 * 6.295050144195557
Epoch 760, val loss: 0.6596654653549194
Epoch 770, training loss: 12.71705150604248 = 0.12974923849105835 + 2.0 * 6.293651103973389
Epoch 770, val loss: 0.661750316619873
Epoch 780, training loss: 12.715490341186523 = 0.12370118498802185 + 2.0 * 6.295894622802734
Epoch 780, val loss: 0.6641022562980652
Epoch 790, training loss: 12.715949058532715 = 0.11799325048923492 + 2.0 * 6.298977851867676
Epoch 790, val loss: 0.6666348576545715
Epoch 800, training loss: 12.6948823928833 = 0.11263328790664673 + 2.0 * 6.29112434387207
Epoch 800, val loss: 0.6693789958953857
Epoch 810, training loss: 12.687299728393555 = 0.10757212340831757 + 2.0 * 6.289863586425781
Epoch 810, val loss: 0.6723940372467041
Epoch 820, training loss: 12.681166648864746 = 0.10277695953845978 + 2.0 * 6.2891950607299805
Epoch 820, val loss: 0.6755725741386414
Epoch 830, training loss: 12.700260162353516 = 0.09825364500284195 + 2.0 * 6.301003456115723
Epoch 830, val loss: 0.6788977980613708
Epoch 840, training loss: 12.67279052734375 = 0.09396760910749435 + 2.0 * 6.289411544799805
Epoch 840, val loss: 0.682357132434845
Epoch 850, training loss: 12.664287567138672 = 0.0899534821510315 + 2.0 * 6.287167072296143
Epoch 850, val loss: 0.685990035533905
Epoch 860, training loss: 12.662972450256348 = 0.08615532517433167 + 2.0 * 6.2884087562561035
Epoch 860, val loss: 0.689774751663208
Epoch 870, training loss: 12.653523445129395 = 0.0825677290558815 + 2.0 * 6.285477638244629
Epoch 870, val loss: 0.6936210989952087
Epoch 880, training loss: 12.64847469329834 = 0.07917647808790207 + 2.0 * 6.284648895263672
Epoch 880, val loss: 0.6975304484367371
Epoch 890, training loss: 12.643219947814941 = 0.07596955448389053 + 2.0 * 6.28362512588501
Epoch 890, val loss: 0.701606273651123
Epoch 900, training loss: 12.647868156433105 = 0.07293083518743515 + 2.0 * 6.287468433380127
Epoch 900, val loss: 0.7057293057441711
Epoch 910, training loss: 12.636713027954102 = 0.07005763798952103 + 2.0 * 6.283327579498291
Epoch 910, val loss: 0.7098535895347595
Epoch 920, training loss: 12.62839412689209 = 0.06732844561338425 + 2.0 * 6.2805328369140625
Epoch 920, val loss: 0.7141228318214417
Epoch 930, training loss: 12.627187728881836 = 0.06474258750677109 + 2.0 * 6.281222343444824
Epoch 930, val loss: 0.7184439897537231
Epoch 940, training loss: 12.628536224365234 = 0.06228911876678467 + 2.0 * 6.28312349319458
Epoch 940, val loss: 0.7227516770362854
Epoch 950, training loss: 12.618743896484375 = 0.05997733399271965 + 2.0 * 6.279383182525635
Epoch 950, val loss: 0.7270587086677551
Epoch 960, training loss: 12.613654136657715 = 0.05777513608336449 + 2.0 * 6.277939319610596
Epoch 960, val loss: 0.7314777374267578
Epoch 970, training loss: 12.60867977142334 = 0.05568050593137741 + 2.0 * 6.2764997482299805
Epoch 970, val loss: 0.7359166145324707
Epoch 980, training loss: 12.625199317932129 = 0.05368748679757118 + 2.0 * 6.2857561111450195
Epoch 980, val loss: 0.7403387427330017
Epoch 990, training loss: 12.605319023132324 = 0.05179731920361519 + 2.0 * 6.276761054992676
Epoch 990, val loss: 0.7446811199188232
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8148148148148149
0.8376383763837639
The final CL Acc:0.80617, 0.00972, The final GNN Acc:0.83904, 0.00099
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.4 der2:0.2 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11636])
remove edge: torch.Size([2, 9494])
updated graph: torch.Size([2, 10574])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 19.145000457763672 = 1.951289176940918 + 2.0 * 8.596855163574219
Epoch 0, val loss: 1.9509724378585815
Epoch 10, training loss: 19.134353637695312 = 1.941072702407837 + 2.0 * 8.596640586853027
Epoch 10, val loss: 1.9409797191619873
Epoch 20, training loss: 19.11861228942871 = 1.9285274744033813 + 2.0 * 8.59504222869873
Epoch 20, val loss: 1.9281948804855347
Epoch 30, training loss: 19.07554054260254 = 1.9114773273468018 + 2.0 * 8.58203125
Epoch 30, val loss: 1.9104387760162354
Epoch 40, training loss: 18.890653610229492 = 1.8896223306655884 + 2.0 * 8.500515937805176
Epoch 40, val loss: 1.888201117515564
Epoch 50, training loss: 17.954221725463867 = 1.8673361539840698 + 2.0 * 8.043442726135254
Epoch 50, val loss: 1.8662102222442627
Epoch 60, training loss: 16.817752838134766 = 1.8482781648635864 + 2.0 * 7.484736919403076
Epoch 60, val loss: 1.8478357791900635
Epoch 70, training loss: 16.136472702026367 = 1.8348124027252197 + 2.0 * 7.150829792022705
Epoch 70, val loss: 1.834298849105835
Epoch 80, training loss: 15.83751392364502 = 1.8216121196746826 + 2.0 * 7.007950782775879
Epoch 80, val loss: 1.821334719657898
Epoch 90, training loss: 15.6637544631958 = 1.807893991470337 + 2.0 * 6.9279303550720215
Epoch 90, val loss: 1.807878017425537
Epoch 100, training loss: 15.481338500976562 = 1.793858289718628 + 2.0 * 6.843739986419678
Epoch 100, val loss: 1.794676423072815
Epoch 110, training loss: 15.340142250061035 = 1.7815488576889038 + 2.0 * 6.779296875
Epoch 110, val loss: 1.7829840183258057
Epoch 120, training loss: 15.214064598083496 = 1.770189642906189 + 2.0 * 6.721937656402588
Epoch 120, val loss: 1.771977424621582
Epoch 130, training loss: 15.106791496276855 = 1.758676290512085 + 2.0 * 6.674057483673096
Epoch 130, val loss: 1.761103868484497
Epoch 140, training loss: 15.023049354553223 = 1.7462478876113892 + 2.0 * 6.638400554656982
Epoch 140, val loss: 1.7495372295379639
Epoch 150, training loss: 14.951845169067383 = 1.732458233833313 + 2.0 * 6.60969352722168
Epoch 150, val loss: 1.7371602058410645
Epoch 160, training loss: 14.88820743560791 = 1.717244029045105 + 2.0 * 6.585481643676758
Epoch 160, val loss: 1.72369384765625
Epoch 170, training loss: 14.827617645263672 = 1.7006096839904785 + 2.0 * 6.563503742218018
Epoch 170, val loss: 1.7089121341705322
Epoch 180, training loss: 14.770210266113281 = 1.6820807456970215 + 2.0 * 6.544064998626709
Epoch 180, val loss: 1.692610502243042
Epoch 190, training loss: 14.716352462768555 = 1.661256194114685 + 2.0 * 6.527548313140869
Epoch 190, val loss: 1.6742839813232422
Epoch 200, training loss: 14.66748046875 = 1.6376913785934448 + 2.0 * 6.514894485473633
Epoch 200, val loss: 1.6537176370620728
Epoch 210, training loss: 14.61876392364502 = 1.6113842725753784 + 2.0 * 6.503689765930176
Epoch 210, val loss: 1.6308372020721436
Epoch 220, training loss: 14.569010734558105 = 1.5820657014846802 + 2.0 * 6.493472576141357
Epoch 220, val loss: 1.6054505109786987
Epoch 230, training loss: 14.51682186126709 = 1.5494776964187622 + 2.0 * 6.483672142028809
Epoch 230, val loss: 1.577329397201538
Epoch 240, training loss: 14.463512420654297 = 1.513466238975525 + 2.0 * 6.47502326965332
Epoch 240, val loss: 1.546403408050537
Epoch 250, training loss: 14.41334056854248 = 1.4742310047149658 + 2.0 * 6.469554901123047
Epoch 250, val loss: 1.5130672454833984
Epoch 260, training loss: 14.351461410522461 = 1.4324774742126465 + 2.0 * 6.459492206573486
Epoch 260, val loss: 1.4777867794036865
Epoch 270, training loss: 14.293098449707031 = 1.3883028030395508 + 2.0 * 6.45239782333374
Epoch 270, val loss: 1.4407267570495605
Epoch 280, training loss: 14.233205795288086 = 1.3419519662857056 + 2.0 * 6.445626735687256
Epoch 280, val loss: 1.4023176431655884
Epoch 290, training loss: 14.176092147827148 = 1.2943564653396606 + 2.0 * 6.440867900848389
Epoch 290, val loss: 1.3633191585540771
Epoch 300, training loss: 14.113608360290527 = 1.246484398841858 + 2.0 * 6.4335618019104
Epoch 300, val loss: 1.3247298002243042
Epoch 310, training loss: 14.052436828613281 = 1.1990903615951538 + 2.0 * 6.426673412322998
Epoch 310, val loss: 1.2869120836257935
Epoch 320, training loss: 13.99361801147461 = 1.1524577140808105 + 2.0 * 6.4205803871154785
Epoch 320, val loss: 1.2504745721817017
Epoch 330, training loss: 13.940616607666016 = 1.1071022748947144 + 2.0 * 6.416757106781006
Epoch 330, val loss: 1.2155994176864624
Epoch 340, training loss: 13.891019821166992 = 1.0636377334594727 + 2.0 * 6.41369104385376
Epoch 340, val loss: 1.1830148696899414
Epoch 350, training loss: 13.835298538208008 = 1.0221738815307617 + 2.0 * 6.406562328338623
Epoch 350, val loss: 1.1523360013961792
Epoch 360, training loss: 13.784794807434082 = 0.9821423292160034 + 2.0 * 6.4013261795043945
Epoch 360, val loss: 1.123289942741394
Epoch 370, training loss: 13.743457794189453 = 0.9433155059814453 + 2.0 * 6.400071144104004
Epoch 370, val loss: 1.0956954956054688
Epoch 380, training loss: 13.698545455932617 = 0.9059147238731384 + 2.0 * 6.396315574645996
Epoch 380, val loss: 1.069509744644165
Epoch 390, training loss: 13.651562690734863 = 0.869596540927887 + 2.0 * 6.3909831047058105
Epoch 390, val loss: 1.044734001159668
Epoch 400, training loss: 13.609418869018555 = 0.8340021967887878 + 2.0 * 6.3877081871032715
Epoch 400, val loss: 1.0207785367965698
Epoch 410, training loss: 13.57297134399414 = 0.7990885376930237 + 2.0 * 6.386941432952881
Epoch 410, val loss: 0.9977412223815918
Epoch 420, training loss: 13.527948379516602 = 0.7651572823524475 + 2.0 * 6.38139533996582
Epoch 420, val loss: 0.9756892323493958
Epoch 430, training loss: 13.486842155456543 = 0.73204505443573 + 2.0 * 6.377398490905762
Epoch 430, val loss: 0.9547443389892578
Epoch 440, training loss: 13.449251174926758 = 0.6999234557151794 + 2.0 * 6.374663829803467
Epoch 440, val loss: 0.9348586201667786
Epoch 450, training loss: 13.410100936889648 = 0.6685487031936646 + 2.0 * 6.370776176452637
Epoch 450, val loss: 0.9160993099212646
Epoch 460, training loss: 13.380903244018555 = 0.6380615830421448 + 2.0 * 6.371420860290527
Epoch 460, val loss: 0.8983731865882874
Epoch 470, training loss: 13.341588020324707 = 0.6087102293968201 + 2.0 * 6.366438865661621
Epoch 470, val loss: 0.8819668889045715
Epoch 480, training loss: 13.307202339172363 = 0.580662727355957 + 2.0 * 6.363269805908203
Epoch 480, val loss: 0.8668994903564453
Epoch 490, training loss: 13.278099060058594 = 0.5536099672317505 + 2.0 * 6.362244606018066
Epoch 490, val loss: 0.8530304431915283
Epoch 500, training loss: 13.247445106506348 = 0.5277802348136902 + 2.0 * 6.359832286834717
Epoch 500, val loss: 0.8402010798454285
Epoch 510, training loss: 13.21399211883545 = 0.5030336976051331 + 2.0 * 6.3554792404174805
Epoch 510, val loss: 0.8286711573600769
Epoch 520, training loss: 13.187188148498535 = 0.47932353615760803 + 2.0 * 6.3539323806762695
Epoch 520, val loss: 0.818166196346283
Epoch 530, training loss: 13.159893989562988 = 0.45654112100601196 + 2.0 * 6.3516764640808105
Epoch 530, val loss: 0.8085627555847168
Epoch 540, training loss: 13.13451862335205 = 0.4347583055496216 + 2.0 * 6.349880218505859
Epoch 540, val loss: 0.7997881174087524
Epoch 550, training loss: 13.11054801940918 = 0.41401124000549316 + 2.0 * 6.348268508911133
Epoch 550, val loss: 0.7918837666511536
Epoch 560, training loss: 13.085694313049316 = 0.39419418573379517 + 2.0 * 6.345749855041504
Epoch 560, val loss: 0.784746527671814
Epoch 570, training loss: 13.065410614013672 = 0.3751780390739441 + 2.0 * 6.345116138458252
Epoch 570, val loss: 0.7782447338104248
Epoch 580, training loss: 13.041983604431152 = 0.3569362163543701 + 2.0 * 6.342523574829102
Epoch 580, val loss: 0.7723360657691956
Epoch 590, training loss: 13.021309852600098 = 0.3394941985607147 + 2.0 * 6.340908050537109
Epoch 590, val loss: 0.7670108079910278
Epoch 600, training loss: 13.002605438232422 = 0.32276085019111633 + 2.0 * 6.3399224281311035
Epoch 600, val loss: 0.7622016072273254
Epoch 610, training loss: 12.981770515441895 = 0.30673089623451233 + 2.0 * 6.337519645690918
Epoch 610, val loss: 0.7580103278160095
Epoch 620, training loss: 12.961487770080566 = 0.2914275825023651 + 2.0 * 6.3350300788879395
Epoch 620, val loss: 0.754186749458313
Epoch 630, training loss: 12.957939147949219 = 0.27677157521247864 + 2.0 * 6.340583801269531
Epoch 630, val loss: 0.7509355545043945
Epoch 640, training loss: 12.928498268127441 = 0.2628316581249237 + 2.0 * 6.332833290100098
Epoch 640, val loss: 0.7481141090393066
Epoch 650, training loss: 12.910353660583496 = 0.2495405375957489 + 2.0 * 6.330406665802002
Epoch 650, val loss: 0.74574875831604
Epoch 660, training loss: 12.89380931854248 = 0.23685814440250397 + 2.0 * 6.328475475311279
Epoch 660, val loss: 0.7438980937004089
Epoch 670, training loss: 12.888415336608887 = 0.22478526830673218 + 2.0 * 6.331815242767334
Epoch 670, val loss: 0.742488443851471
Epoch 680, training loss: 12.870148658752441 = 0.213374063372612 + 2.0 * 6.328387260437012
Epoch 680, val loss: 0.7413647174835205
Epoch 690, training loss: 12.852068901062012 = 0.20256543159484863 + 2.0 * 6.324751853942871
Epoch 690, val loss: 0.7407616972923279
Epoch 700, training loss: 12.836777687072754 = 0.1922958642244339 + 2.0 * 6.322240829467773
Epoch 700, val loss: 0.7406910061836243
Epoch 710, training loss: 12.855140686035156 = 0.18253616988658905 + 2.0 * 6.336302280426025
Epoch 710, val loss: 0.7409908175468445
Epoch 720, training loss: 12.819098472595215 = 0.17340697348117828 + 2.0 * 6.322845935821533
Epoch 720, val loss: 0.7416731119155884
Epoch 730, training loss: 12.805254936218262 = 0.16482661664485931 + 2.0 * 6.32021427154541
Epoch 730, val loss: 0.7426033616065979
Epoch 740, training loss: 12.792083740234375 = 0.15672950446605682 + 2.0 * 6.317677021026611
Epoch 740, val loss: 0.7441108226776123
Epoch 750, training loss: 12.78064250946045 = 0.14906679093837738 + 2.0 * 6.3157877922058105
Epoch 750, val loss: 0.7459689378738403
Epoch 760, training loss: 12.772488594055176 = 0.14181755483150482 + 2.0 * 6.315335750579834
Epoch 760, val loss: 0.7481342554092407
Epoch 770, training loss: 12.765002250671387 = 0.13496951758861542 + 2.0 * 6.315016269683838
Epoch 770, val loss: 0.7505387663841248
Epoch 780, training loss: 12.756959915161133 = 0.12852412462234497 + 2.0 * 6.314218044281006
Epoch 780, val loss: 0.7532652616500854
Epoch 790, training loss: 12.744525909423828 = 0.12245555967092514 + 2.0 * 6.31103515625
Epoch 790, val loss: 0.7562471032142639
Epoch 800, training loss: 12.739055633544922 = 0.11672747135162354 + 2.0 * 6.311163902282715
Epoch 800, val loss: 0.7595295310020447
Epoch 810, training loss: 12.732563972473145 = 0.11132369190454483 + 2.0 * 6.310620307922363
Epoch 810, val loss: 0.7630798816680908
Epoch 820, training loss: 12.723730087280273 = 0.10624314844608307 + 2.0 * 6.308743476867676
Epoch 820, val loss: 0.7667744755744934
Epoch 830, training loss: 12.714191436767578 = 0.10143538564443588 + 2.0 * 6.30637788772583
Epoch 830, val loss: 0.7706727981567383
Epoch 840, training loss: 12.725382804870605 = 0.09689560532569885 + 2.0 * 6.314243793487549
Epoch 840, val loss: 0.7747636437416077
Epoch 850, training loss: 12.709392547607422 = 0.09263230115175247 + 2.0 * 6.308380126953125
Epoch 850, val loss: 0.7789821624755859
Epoch 860, training loss: 12.696006774902344 = 0.08862865716218948 + 2.0 * 6.303689002990723
Epoch 860, val loss: 0.7831693887710571
Epoch 870, training loss: 12.688096046447754 = 0.08484097570180893 + 2.0 * 6.3016276359558105
Epoch 870, val loss: 0.787596583366394
Epoch 880, training loss: 12.689629554748535 = 0.08126462250947952 + 2.0 * 6.304182529449463
Epoch 880, val loss: 0.7921501994132996
Epoch 890, training loss: 12.67973518371582 = 0.07786791771650314 + 2.0 * 6.300933837890625
Epoch 890, val loss: 0.7966901063919067
Epoch 900, training loss: 12.676334381103516 = 0.07468408346176147 + 2.0 * 6.300825119018555
Epoch 900, val loss: 0.8012855648994446
Epoch 910, training loss: 12.66816520690918 = 0.07165970653295517 + 2.0 * 6.298252582550049
Epoch 910, val loss: 0.8059892654418945
Epoch 920, training loss: 12.679788589477539 = 0.06880345940589905 + 2.0 * 6.305492401123047
Epoch 920, val loss: 0.8107751607894897
Epoch 930, training loss: 12.663802146911621 = 0.06610126793384552 + 2.0 * 6.2988505363464355
Epoch 930, val loss: 0.8154735565185547
Epoch 940, training loss: 12.654094696044922 = 0.06354347616434097 + 2.0 * 6.295275688171387
Epoch 940, val loss: 0.8202595114707947
Epoch 950, training loss: 12.649210929870605 = 0.06111883372068405 + 2.0 * 6.294045925140381
Epoch 950, val loss: 0.825092613697052
Epoch 960, training loss: 12.661971092224121 = 0.058815956115722656 + 2.0 * 6.301577568054199
Epoch 960, val loss: 0.8299290537834167
Epoch 970, training loss: 12.654825210571289 = 0.05663299188017845 + 2.0 * 6.29909610748291
Epoch 970, val loss: 0.8347556591033936
Epoch 980, training loss: 12.643033981323242 = 0.05456879734992981 + 2.0 * 6.294232368469238
Epoch 980, val loss: 0.8394240736961365
Epoch 990, training loss: 12.635132789611816 = 0.05261686444282532 + 2.0 * 6.291257858276367
Epoch 990, val loss: 0.8442175388336182
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7851851851851852
0.8075909330521878
=== training gcn model ===
Epoch 0, training loss: 19.146303176879883 = 1.9526182413101196 + 2.0 * 8.596842765808105
Epoch 0, val loss: 1.9585644006729126
Epoch 10, training loss: 19.13485336303711 = 1.941773533821106 + 2.0 * 8.596539497375488
Epoch 10, val loss: 1.947114109992981
Epoch 20, training loss: 19.116519927978516 = 1.9283037185668945 + 2.0 * 8.594107627868652
Epoch 20, val loss: 1.9327207803726196
Epoch 30, training loss: 19.06120491027832 = 1.9099398851394653 + 2.0 * 8.575632095336914
Epoch 30, val loss: 1.9131535291671753
Epoch 40, training loss: 18.811201095581055 = 1.887536644935608 + 2.0 * 8.461832046508789
Epoch 40, val loss: 1.8906785249710083
Epoch 50, training loss: 17.951627731323242 = 1.8648728132247925 + 2.0 * 8.043377876281738
Epoch 50, val loss: 1.8687585592269897
Epoch 60, training loss: 17.166345596313477 = 1.8482095003128052 + 2.0 * 7.659067630767822
Epoch 60, val loss: 1.8532383441925049
Epoch 70, training loss: 16.54339599609375 = 1.8368090391159058 + 2.0 * 7.353293418884277
Epoch 70, val loss: 1.8427132368087769
Epoch 80, training loss: 15.929620742797852 = 1.8290457725524902 + 2.0 * 7.050287246704102
Epoch 80, val loss: 1.8354225158691406
Epoch 90, training loss: 15.581206321716309 = 1.819726586341858 + 2.0 * 6.880739688873291
Epoch 90, val loss: 1.8266183137893677
Epoch 100, training loss: 15.3814115524292 = 1.8088613748550415 + 2.0 * 6.7862749099731445
Epoch 100, val loss: 1.8159167766571045
Epoch 110, training loss: 15.23109245300293 = 1.798662781715393 + 2.0 * 6.716214656829834
Epoch 110, val loss: 1.8057442903518677
Epoch 120, training loss: 15.127992630004883 = 1.7894760370254517 + 2.0 * 6.669258117675781
Epoch 120, val loss: 1.7961034774780273
Epoch 130, training loss: 15.040314674377441 = 1.780419111251831 + 2.0 * 6.629947662353516
Epoch 130, val loss: 1.7867622375488281
Epoch 140, training loss: 14.962667465209961 = 1.7710384130477905 + 2.0 * 6.5958147048950195
Epoch 140, val loss: 1.777513861656189
Epoch 150, training loss: 14.898345947265625 = 1.7611714601516724 + 2.0 * 6.568587303161621
Epoch 150, val loss: 1.7681313753128052
Epoch 160, training loss: 14.839835166931152 = 1.7505367994308472 + 2.0 * 6.544649124145508
Epoch 160, val loss: 1.7582343816757202
Epoch 170, training loss: 14.783470153808594 = 1.738730549812317 + 2.0 * 6.522369861602783
Epoch 170, val loss: 1.7474209070205688
Epoch 180, training loss: 14.73302936553955 = 1.725474238395691 + 2.0 * 6.503777503967285
Epoch 180, val loss: 1.7354273796081543
Epoch 190, training loss: 14.690204620361328 = 1.7105293273925781 + 2.0 * 6.489837646484375
Epoch 190, val loss: 1.721956729888916
Epoch 200, training loss: 14.642855644226074 = 1.6937379837036133 + 2.0 * 6.4745588302612305
Epoch 200, val loss: 1.7069039344787598
Epoch 210, training loss: 14.599834442138672 = 1.67478346824646 + 2.0 * 6.462525367736816
Epoch 210, val loss: 1.6900194883346558
Epoch 220, training loss: 14.559818267822266 = 1.6532955169677734 + 2.0 * 6.453261375427246
Epoch 220, val loss: 1.6709215641021729
Epoch 230, training loss: 14.514742851257324 = 1.62917160987854 + 2.0 * 6.442785739898682
Epoch 230, val loss: 1.64955735206604
Epoch 240, training loss: 14.471692085266113 = 1.6021159887313843 + 2.0 * 6.434788227081299
Epoch 240, val loss: 1.625817060470581
Epoch 250, training loss: 14.423916816711426 = 1.571897268295288 + 2.0 * 6.426009654998779
Epoch 250, val loss: 1.5995171070098877
Epoch 260, training loss: 14.379575729370117 = 1.5383951663970947 + 2.0 * 6.420590400695801
Epoch 260, val loss: 1.570595145225525
Epoch 270, training loss: 14.3334379196167 = 1.5017082691192627 + 2.0 * 6.415864944458008
Epoch 270, val loss: 1.539444923400879
Epoch 280, training loss: 14.27931022644043 = 1.4624048471450806 + 2.0 * 6.40845251083374
Epoch 280, val loss: 1.5065237283706665
Epoch 290, training loss: 14.225618362426758 = 1.4207227230072021 + 2.0 * 6.402447700500488
Epoch 290, val loss: 1.472061038017273
Epoch 300, training loss: 14.174474716186523 = 1.3770697116851807 + 2.0 * 6.398702621459961
Epoch 300, val loss: 1.4366052150726318
Epoch 310, training loss: 14.122696876525879 = 1.332356333732605 + 2.0 * 6.395170211791992
Epoch 310, val loss: 1.4012556076049805
Epoch 320, training loss: 14.064922332763672 = 1.2874979972839355 + 2.0 * 6.388711929321289
Epoch 320, val loss: 1.366313099861145
Epoch 330, training loss: 14.012173652648926 = 1.2427202463150024 + 2.0 * 6.384726524353027
Epoch 330, val loss: 1.332229733467102
Epoch 340, training loss: 13.966383934020996 = 1.1983582973480225 + 2.0 * 6.384012699127197
Epoch 340, val loss: 1.2993693351745605
Epoch 350, training loss: 13.912240028381348 = 1.1551309823989868 + 2.0 * 6.378554344177246
Epoch 350, val loss: 1.2680792808532715
Epoch 360, training loss: 13.861703872680664 = 1.1130937337875366 + 2.0 * 6.374305248260498
Epoch 360, val loss: 1.238527774810791
Epoch 370, training loss: 13.812976837158203 = 1.0720696449279785 + 2.0 * 6.370453834533691
Epoch 370, val loss: 1.21043062210083
Epoch 380, training loss: 13.778691291809082 = 1.0320711135864258 + 2.0 * 6.373310089111328
Epoch 380, val loss: 1.1838769912719727
Epoch 390, training loss: 13.722111701965332 = 0.9935872554779053 + 2.0 * 6.364262104034424
Epoch 390, val loss: 1.158787727355957
Epoch 400, training loss: 13.679895401000977 = 0.9562208652496338 + 2.0 * 6.361837387084961
Epoch 400, val loss: 1.1351675987243652
Epoch 410, training loss: 13.648005485534668 = 0.9199575185775757 + 2.0 * 6.3640241622924805
Epoch 410, val loss: 1.112736463546753
Epoch 420, training loss: 13.598306655883789 = 0.8848219513893127 + 2.0 * 6.3567423820495605
Epoch 420, val loss: 1.091444730758667
Epoch 430, training loss: 13.558764457702637 = 0.8507201075553894 + 2.0 * 6.354022026062012
Epoch 430, val loss: 1.0711596012115479
Epoch 440, training loss: 13.520325660705566 = 0.8176590204238892 + 2.0 * 6.351333141326904
Epoch 440, val loss: 1.0519112348556519
Epoch 450, training loss: 13.485261917114258 = 0.7857726812362671 + 2.0 * 6.34974479675293
Epoch 450, val loss: 1.0336135625839233
Epoch 460, training loss: 13.447648048400879 = 0.7548283934593201 + 2.0 * 6.346409797668457
Epoch 460, val loss: 1.0162982940673828
Epoch 470, training loss: 13.413254737854004 = 0.7247194647789001 + 2.0 * 6.344267845153809
Epoch 470, val loss: 0.9997962117195129
Epoch 480, training loss: 13.386927604675293 = 0.6956876516342163 + 2.0 * 6.345620155334473
Epoch 480, val loss: 0.9842182397842407
Epoch 490, training loss: 13.357099533081055 = 0.6681039929389954 + 2.0 * 6.3444976806640625
Epoch 490, val loss: 0.9696497321128845
Epoch 500, training loss: 13.319173812866211 = 0.6416648030281067 + 2.0 * 6.338754653930664
Epoch 500, val loss: 0.9560883641242981
Epoch 510, training loss: 13.288825035095215 = 0.6160801649093628 + 2.0 * 6.336372375488281
Epoch 510, val loss: 0.9434654116630554
Epoch 520, training loss: 13.259757041931152 = 0.5912697911262512 + 2.0 * 6.3342437744140625
Epoch 520, val loss: 0.9315987229347229
Epoch 530, training loss: 13.264776229858398 = 0.5671787261962891 + 2.0 * 6.348798751831055
Epoch 530, val loss: 0.9203939437866211
Epoch 540, training loss: 13.214156150817871 = 0.5441120266914368 + 2.0 * 6.33502197265625
Epoch 540, val loss: 0.9099600911140442
Epoch 550, training loss: 13.185669898986816 = 0.5218963027000427 + 2.0 * 6.3318867683410645
Epoch 550, val loss: 0.900333046913147
Epoch 560, training loss: 13.156216621398926 = 0.5003536939620972 + 2.0 * 6.3279314041137695
Epoch 560, val loss: 0.8913506865501404
Epoch 570, training loss: 13.131841659545898 = 0.47933676838874817 + 2.0 * 6.326252460479736
Epoch 570, val loss: 0.8829016089439392
Epoch 580, training loss: 13.118189811706543 = 0.45879003405570984 + 2.0 * 6.329699993133545
Epoch 580, val loss: 0.8749642372131348
Epoch 590, training loss: 13.09759521484375 = 0.4389956593513489 + 2.0 * 6.3292999267578125
Epoch 590, val loss: 0.8673312067985535
Epoch 600, training loss: 13.066167831420898 = 0.4197809398174286 + 2.0 * 6.323193550109863
Epoch 600, val loss: 0.860310971736908
Epoch 610, training loss: 13.043083190917969 = 0.40112006664276123 + 2.0 * 6.320981502532959
Epoch 610, val loss: 0.8537498116493225
Epoch 620, training loss: 13.02182674407959 = 0.3829159140586853 + 2.0 * 6.319455623626709
Epoch 620, val loss: 0.8476905226707458
Epoch 630, training loss: 13.006431579589844 = 0.36515194177627563 + 2.0 * 6.320639610290527
Epoch 630, val loss: 0.8420681953430176
Epoch 640, training loss: 12.991713523864746 = 0.3479640483856201 + 2.0 * 6.321874618530273
Epoch 640, val loss: 0.8368896842002869
Epoch 650, training loss: 12.963881492614746 = 0.3313726782798767 + 2.0 * 6.316254615783691
Epoch 650, val loss: 0.8321368098258972
Epoch 660, training loss: 12.949499130249023 = 0.31536048650741577 + 2.0 * 6.3170695304870605
Epoch 660, val loss: 0.8279266953468323
Epoch 670, training loss: 12.928790092468262 = 0.29995936155319214 + 2.0 * 6.314415454864502
Epoch 670, val loss: 0.8244128227233887
Epoch 680, training loss: 12.912432670593262 = 0.2852204740047455 + 2.0 * 6.313606262207031
Epoch 680, val loss: 0.821343183517456
Epoch 690, training loss: 12.89455795288086 = 0.27106973528862 + 2.0 * 6.311744213104248
Epoch 690, val loss: 0.8187561631202698
Epoch 700, training loss: 12.89072322845459 = 0.2575379014015198 + 2.0 * 6.316592693328857
Epoch 700, val loss: 0.8166242241859436
Epoch 710, training loss: 12.866146087646484 = 0.24469496309757233 + 2.0 * 6.310725688934326
Epoch 710, val loss: 0.8147276639938354
Epoch 720, training loss: 12.85544204711914 = 0.23249314725399017 + 2.0 * 6.311474323272705
Epoch 720, val loss: 0.8134168386459351
Epoch 730, training loss: 12.835232734680176 = 0.22093458473682404 + 2.0 * 6.3071489334106445
Epoch 730, val loss: 0.8127046823501587
Epoch 740, training loss: 12.824679374694824 = 0.2099589705467224 + 2.0 * 6.3073601722717285
Epoch 740, val loss: 0.8122977018356323
Epoch 750, training loss: 12.822183609008789 = 0.19954414665699005 + 2.0 * 6.311319828033447
Epoch 750, val loss: 0.8122776746749878
Epoch 760, training loss: 12.807316780090332 = 0.18977467715740204 + 2.0 * 6.308771133422852
Epoch 760, val loss: 0.812522292137146
Epoch 770, training loss: 12.787049293518066 = 0.18052223324775696 + 2.0 * 6.3032636642456055
Epoch 770, val loss: 0.813104510307312
Epoch 780, training loss: 12.775211334228516 = 0.17178845405578613 + 2.0 * 6.301711559295654
Epoch 780, val loss: 0.8141255974769592
Epoch 790, training loss: 12.770920753479004 = 0.16351647675037384 + 2.0 * 6.303702354431152
Epoch 790, val loss: 0.8153266906738281
Epoch 800, training loss: 12.76213264465332 = 0.15573978424072266 + 2.0 * 6.303196430206299
Epoch 800, val loss: 0.8169941902160645
Epoch 810, training loss: 12.749116897583008 = 0.14839984476566315 + 2.0 * 6.300358295440674
Epoch 810, val loss: 0.8188005089759827
Epoch 820, training loss: 12.737573623657227 = 0.14148405194282532 + 2.0 * 6.298044681549072
Epoch 820, val loss: 0.8209477663040161
Epoch 830, training loss: 12.738692283630371 = 0.13493746519088745 + 2.0 * 6.301877498626709
Epoch 830, val loss: 0.8233733177185059
Epoch 840, training loss: 12.731574058532715 = 0.12877999246120453 + 2.0 * 6.30139684677124
Epoch 840, val loss: 0.8258103132247925
Epoch 850, training loss: 12.714757919311523 = 0.1229749545454979 + 2.0 * 6.295891284942627
Epoch 850, val loss: 0.828636109828949
Epoch 860, training loss: 12.713479042053223 = 0.11750203371047974 + 2.0 * 6.297988414764404
Epoch 860, val loss: 0.8315896391868591
Epoch 870, training loss: 12.701445579528809 = 0.11234424263238907 + 2.0 * 6.294550895690918
Epoch 870, val loss: 0.8347234725952148
Epoch 880, training loss: 12.694755554199219 = 0.10747671872377396 + 2.0 * 6.293639183044434
Epoch 880, val loss: 0.8378427624702454
Epoch 890, training loss: 12.687041282653809 = 0.10286416858434677 + 2.0 * 6.292088508605957
Epoch 890, val loss: 0.8412677049636841
Epoch 900, training loss: 12.681571006774902 = 0.09848297387361526 + 2.0 * 6.291543960571289
Epoch 900, val loss: 0.8449142575263977
Epoch 910, training loss: 12.684260368347168 = 0.09433765709400177 + 2.0 * 6.294961452484131
Epoch 910, val loss: 0.8484658002853394
Epoch 920, training loss: 12.668787956237793 = 0.0904393419623375 + 2.0 * 6.2891740798950195
Epoch 920, val loss: 0.8521095514297485
Epoch 930, training loss: 12.666093826293945 = 0.08674994856119156 + 2.0 * 6.289671897888184
Epoch 930, val loss: 0.8557301163673401
Epoch 940, training loss: 12.664605140686035 = 0.08324594795703888 + 2.0 * 6.290679454803467
Epoch 940, val loss: 0.8597919344902039
Epoch 950, training loss: 12.665480613708496 = 0.07993558049201965 + 2.0 * 6.29277229309082
Epoch 950, val loss: 0.8635141849517822
Epoch 960, training loss: 12.650800704956055 = 0.07680663466453552 + 2.0 * 6.286996841430664
Epoch 960, val loss: 0.8674358129501343
Epoch 970, training loss: 12.645477294921875 = 0.07384086400270462 + 2.0 * 6.285818099975586
Epoch 970, val loss: 0.8714867830276489
Epoch 980, training loss: 12.641622543334961 = 0.07101984322071075 + 2.0 * 6.285301208496094
Epoch 980, val loss: 0.8755543231964111
Epoch 990, training loss: 12.646932601928711 = 0.06834018975496292 + 2.0 * 6.2892961502075195
Epoch 990, val loss: 0.8796011209487915
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7555555555555555
0.8096995255666843
=== training gcn model ===
Epoch 0, training loss: 19.139066696166992 = 1.9454102516174316 + 2.0 * 8.59682846069336
Epoch 0, val loss: 1.946882724761963
Epoch 10, training loss: 19.12812042236328 = 1.9352447986602783 + 2.0 * 8.596437454223633
Epoch 10, val loss: 1.936106562614441
Epoch 20, training loss: 19.10890007019043 = 1.9225836992263794 + 2.0 * 8.593157768249512
Epoch 20, val loss: 1.9225972890853882
Epoch 30, training loss: 19.039457321166992 = 1.9055415391921997 + 2.0 * 8.566957473754883
Epoch 30, val loss: 1.9046204090118408
Epoch 40, training loss: 18.67377281188965 = 1.885999083518982 + 2.0 * 8.39388656616211
Epoch 40, val loss: 1.885306715965271
Epoch 50, training loss: 17.571884155273438 = 1.8659238815307617 + 2.0 * 7.852980136871338
Epoch 50, val loss: 1.8656747341156006
Epoch 60, training loss: 16.48209571838379 = 1.8513778448104858 + 2.0 * 7.315358638763428
Epoch 60, val loss: 1.8530880212783813
Epoch 70, training loss: 15.918220520019531 = 1.8413161039352417 + 2.0 * 7.0384521484375
Epoch 70, val loss: 1.8432302474975586
Epoch 80, training loss: 15.677203178405762 = 1.830809473991394 + 2.0 * 6.923196792602539
Epoch 80, val loss: 1.8330503702163696
Epoch 90, training loss: 15.468585014343262 = 1.8192949295043945 + 2.0 * 6.824645042419434
Epoch 90, val loss: 1.8223985433578491
Epoch 100, training loss: 15.295278549194336 = 1.8090758323669434 + 2.0 * 6.743101119995117
Epoch 100, val loss: 1.8134493827819824
Epoch 110, training loss: 15.151230812072754 = 1.8002455234527588 + 2.0 * 6.675492763519287
Epoch 110, val loss: 1.8056637048721313
Epoch 120, training loss: 15.038243293762207 = 1.7918293476104736 + 2.0 * 6.623207092285156
Epoch 120, val loss: 1.7979532480239868
Epoch 130, training loss: 14.960025787353516 = 1.782954454421997 + 2.0 * 6.588535785675049
Epoch 130, val loss: 1.7897111177444458
Epoch 140, training loss: 14.887483596801758 = 1.7733798027038574 + 2.0 * 6.557051658630371
Epoch 140, val loss: 1.781040072441101
Epoch 150, training loss: 14.828706741333008 = 1.7632708549499512 + 2.0 * 6.532717704772949
Epoch 150, val loss: 1.771896481513977
Epoch 160, training loss: 14.78738021850586 = 1.752307415008545 + 2.0 * 6.517536163330078
Epoch 160, val loss: 1.7620110511779785
Epoch 170, training loss: 14.731915473937988 = 1.740343451499939 + 2.0 * 6.495786190032959
Epoch 170, val loss: 1.7513707876205444
Epoch 180, training loss: 14.687139511108398 = 1.7272295951843262 + 2.0 * 6.479955196380615
Epoch 180, val loss: 1.7398614883422852
Epoch 190, training loss: 14.645951271057129 = 1.712598443031311 + 2.0 * 6.466676235198975
Epoch 190, val loss: 1.727116584777832
Epoch 200, training loss: 14.607312202453613 = 1.6961406469345093 + 2.0 * 6.455585956573486
Epoch 200, val loss: 1.7129185199737549
Epoch 210, training loss: 14.57255744934082 = 1.6776680946350098 + 2.0 * 6.447444438934326
Epoch 210, val loss: 1.696933388710022
Epoch 220, training loss: 14.530186653137207 = 1.6570245027542114 + 2.0 * 6.436581134796143
Epoch 220, val loss: 1.6792532205581665
Epoch 230, training loss: 14.488611221313477 = 1.633949875831604 + 2.0 * 6.427330493927002
Epoch 230, val loss: 1.659533143043518
Epoch 240, training loss: 14.452714920043945 = 1.6080915927886963 + 2.0 * 6.422311782836914
Epoch 240, val loss: 1.6375356912612915
Epoch 250, training loss: 14.406437873840332 = 1.5794411897659302 + 2.0 * 6.413498401641846
Epoch 250, val loss: 1.6133618354797363
Epoch 260, training loss: 14.362992286682129 = 1.5480550527572632 + 2.0 * 6.407468795776367
Epoch 260, val loss: 1.5870577096939087
Epoch 270, training loss: 14.314709663391113 = 1.5136687755584717 + 2.0 * 6.400520324707031
Epoch 270, val loss: 1.5585330724716187
Epoch 280, training loss: 14.266626358032227 = 1.4762043952941895 + 2.0 * 6.3952107429504395
Epoch 280, val loss: 1.5276916027069092
Epoch 290, training loss: 14.22443675994873 = 1.4357706308364868 + 2.0 * 6.3943328857421875
Epoch 290, val loss: 1.4948006868362427
Epoch 300, training loss: 14.165863037109375 = 1.3929537534713745 + 2.0 * 6.3864545822143555
Epoch 300, val loss: 1.4603831768035889
Epoch 310, training loss: 14.11288833618164 = 1.3481357097625732 + 2.0 * 6.382376194000244
Epoch 310, val loss: 1.4248919486999512
Epoch 320, training loss: 14.056828498840332 = 1.3016071319580078 + 2.0 * 6.377610683441162
Epoch 320, val loss: 1.3886215686798096
Epoch 330, training loss: 14.002946853637695 = 1.253832459449768 + 2.0 * 6.374557018280029
Epoch 330, val loss: 1.3519784212112427
Epoch 340, training loss: 13.955988883972168 = 1.2056629657745361 + 2.0 * 6.3751630783081055
Epoch 340, val loss: 1.3156987428665161
Epoch 350, training loss: 13.894889831542969 = 1.157834529876709 + 2.0 * 6.368527412414551
Epoch 350, val loss: 1.2803336381912231
Epoch 360, training loss: 13.84189224243164 = 1.1105960607528687 + 2.0 * 6.36564826965332
Epoch 360, val loss: 1.2461297512054443
Epoch 370, training loss: 13.794333457946777 = 1.0644500255584717 + 2.0 * 6.364941596984863
Epoch 370, val loss: 1.2134464979171753
Epoch 380, training loss: 13.737419128417969 = 1.0199177265167236 + 2.0 * 6.358750820159912
Epoch 380, val loss: 1.1824519634246826
Epoch 390, training loss: 13.687538146972656 = 0.9767276644706726 + 2.0 * 6.355405330657959
Epoch 390, val loss: 1.1529512405395508
Epoch 400, training loss: 13.652262687683105 = 0.9348686337471008 + 2.0 * 6.358696937561035
Epoch 400, val loss: 1.1248277425765991
Epoch 410, training loss: 13.59563159942627 = 0.8947052359580994 + 2.0 * 6.350463390350342
Epoch 410, val loss: 1.0981807708740234
Epoch 420, training loss: 13.559661865234375 = 0.8560270071029663 + 2.0 * 6.351817607879639
Epoch 420, val loss: 1.0729955434799194
Epoch 430, training loss: 13.512927055358887 = 0.8190155029296875 + 2.0 * 6.3469557762146
Epoch 430, val loss: 1.0490840673446655
Epoch 440, training loss: 13.47069263458252 = 0.7834234833717346 + 2.0 * 6.343634605407715
Epoch 440, val loss: 1.0265324115753174
Epoch 450, training loss: 13.434279441833496 = 0.7492014169692993 + 2.0 * 6.342538833618164
Epoch 450, val loss: 1.0051926374435425
Epoch 460, training loss: 13.396515846252441 = 0.7165442705154419 + 2.0 * 6.3399858474731445
Epoch 460, val loss: 0.9850543141365051
Epoch 470, training loss: 13.361833572387695 = 0.6854079365730286 + 2.0 * 6.338212966918945
Epoch 470, val loss: 0.9662789702415466
Epoch 480, training loss: 13.330323219299316 = 0.6557573080062866 + 2.0 * 6.337283134460449
Epoch 480, val loss: 0.9487626552581787
Epoch 490, training loss: 13.295429229736328 = 0.6276426315307617 + 2.0 * 6.333893299102783
Epoch 490, val loss: 0.9325123429298401
Epoch 500, training loss: 13.264409065246582 = 0.60086590051651 + 2.0 * 6.331771373748779
Epoch 500, val loss: 0.9175757765769958
Epoch 510, training loss: 13.240011215209961 = 0.5753800272941589 + 2.0 * 6.332315444946289
Epoch 510, val loss: 0.9037419557571411
Epoch 520, training loss: 13.208539009094238 = 0.5511878132820129 + 2.0 * 6.328675746917725
Epoch 520, val loss: 0.8910526037216187
Epoch 530, training loss: 13.185517311096191 = 0.5281182527542114 + 2.0 * 6.328699588775635
Epoch 530, val loss: 0.8795475959777832
Epoch 540, training loss: 13.156283378601074 = 0.506170392036438 + 2.0 * 6.325056552886963
Epoch 540, val loss: 0.8690752387046814
Epoch 550, training loss: 13.128974914550781 = 0.4852195084095001 + 2.0 * 6.321877479553223
Epoch 550, val loss: 0.8596360087394714
Epoch 560, training loss: 13.111681938171387 = 0.46514618396759033 + 2.0 * 6.323267936706543
Epoch 560, val loss: 0.8511034846305847
Epoch 570, training loss: 13.091187477111816 = 0.4459556043148041 + 2.0 * 6.322616100311279
Epoch 570, val loss: 0.8433732390403748
Epoch 580, training loss: 13.066825866699219 = 0.42756712436676025 + 2.0 * 6.319629192352295
Epoch 580, val loss: 0.8364773392677307
Epoch 590, training loss: 13.041476249694824 = 0.40982839465141296 + 2.0 * 6.315824031829834
Epoch 590, val loss: 0.830366313457489
Epoch 600, training loss: 13.022170066833496 = 0.39274469017982483 + 2.0 * 6.3147125244140625
Epoch 600, val loss: 0.8248612880706787
Epoch 610, training loss: 13.01112174987793 = 0.3762668967247009 + 2.0 * 6.317427635192871
Epoch 610, val loss: 0.8198997378349304
Epoch 620, training loss: 12.982847213745117 = 0.3603402376174927 + 2.0 * 6.311253547668457
Epoch 620, val loss: 0.8155509233474731
Epoch 630, training loss: 12.976333618164062 = 0.34501194953918457 + 2.0 * 6.3156609535217285
Epoch 630, val loss: 0.8116670250892639
Epoch 640, training loss: 12.948382377624512 = 0.33019694685935974 + 2.0 * 6.3090925216674805
Epoch 640, val loss: 0.80826336145401
Epoch 650, training loss: 12.929642677307129 = 0.31587666273117065 + 2.0 * 6.306882858276367
Epoch 650, val loss: 0.8052853941917419
Epoch 660, training loss: 12.91451358795166 = 0.30199187994003296 + 2.0 * 6.30626106262207
Epoch 660, val loss: 0.8027030229568481
Epoch 670, training loss: 12.900675773620605 = 0.28854161500930786 + 2.0 * 6.306066989898682
Epoch 670, val loss: 0.8004963994026184
Epoch 680, training loss: 12.890170097351074 = 0.27561354637145996 + 2.0 * 6.307278156280518
Epoch 680, val loss: 0.7985408306121826
Epoch 690, training loss: 12.866592407226562 = 0.26316961646080017 + 2.0 * 6.301711559295654
Epoch 690, val loss: 0.7969954609870911
Epoch 700, training loss: 12.857150077819824 = 0.2511654198169708 + 2.0 * 6.302992343902588
Epoch 700, val loss: 0.795863926410675
Epoch 710, training loss: 12.8424711227417 = 0.23958267271518707 + 2.0 * 6.301444053649902
Epoch 710, val loss: 0.7950485944747925
Epoch 720, training loss: 12.827256202697754 = 0.2285127341747284 + 2.0 * 6.299371719360352
Epoch 720, val loss: 0.7945483326911926
Epoch 730, training loss: 12.8129301071167 = 0.21785753965377808 + 2.0 * 6.297536373138428
Epoch 730, val loss: 0.7943611145019531
Epoch 740, training loss: 12.800639152526855 = 0.20764879882335663 + 2.0 * 6.296494960784912
Epoch 740, val loss: 0.7945281267166138
Epoch 750, training loss: 12.808465003967285 = 0.19784796237945557 + 2.0 * 6.3053083419799805
Epoch 750, val loss: 0.7950006723403931
Epoch 760, training loss: 12.776973724365234 = 0.18851183354854584 + 2.0 * 6.294230937957764
Epoch 760, val loss: 0.7957523465156555
Epoch 770, training loss: 12.767666816711426 = 0.17964355647563934 + 2.0 * 6.29401159286499
Epoch 770, val loss: 0.7967621684074402
Epoch 780, training loss: 12.76640796661377 = 0.17118532955646515 + 2.0 * 6.297611236572266
Epoch 780, val loss: 0.7980230450630188
Epoch 790, training loss: 12.748248100280762 = 0.16316477954387665 + 2.0 * 6.29254150390625
Epoch 790, val loss: 0.799627959728241
Epoch 800, training loss: 12.736309051513672 = 0.15554043650627136 + 2.0 * 6.290384292602539
Epoch 800, val loss: 0.8013522624969482
Epoch 810, training loss: 12.725850105285645 = 0.14828766882419586 + 2.0 * 6.28878116607666
Epoch 810, val loss: 0.8033943176269531
Epoch 820, training loss: 12.718121528625488 = 0.1413954496383667 + 2.0 * 6.288362979888916
Epoch 820, val loss: 0.8056793212890625
Epoch 830, training loss: 12.712972640991211 = 0.13486343622207642 + 2.0 * 6.2890543937683105
Epoch 830, val loss: 0.8082144260406494
Epoch 840, training loss: 12.70902156829834 = 0.12869513034820557 + 2.0 * 6.290163040161133
Epoch 840, val loss: 0.8109043836593628
Epoch 850, training loss: 12.694136619567871 = 0.12286736816167831 + 2.0 * 6.285634517669678
Epoch 850, val loss: 0.8137521743774414
Epoch 860, training loss: 12.684425354003906 = 0.11734288930892944 + 2.0 * 6.283541202545166
Epoch 860, val loss: 0.8168177008628845
Epoch 870, training loss: 12.688536643981934 = 0.11209437251091003 + 2.0 * 6.28822135925293
Epoch 870, val loss: 0.8200230598449707
Epoch 880, training loss: 12.689231872558594 = 0.10714320838451385 + 2.0 * 6.291044235229492
Epoch 880, val loss: 0.8235217332839966
Epoch 890, training loss: 12.664705276489258 = 0.10249100625514984 + 2.0 * 6.281106948852539
Epoch 890, val loss: 0.8268861174583435
Epoch 900, training loss: 12.660079956054688 = 0.09808515012264252 + 2.0 * 6.280997276306152
Epoch 900, val loss: 0.8304849863052368
Epoch 910, training loss: 12.655017852783203 = 0.09390871971845627 + 2.0 * 6.28055477142334
Epoch 910, val loss: 0.8342800736427307
Epoch 920, training loss: 12.657833099365234 = 0.08994702994823456 + 2.0 * 6.283943176269531
Epoch 920, val loss: 0.8381607532501221
Epoch 930, training loss: 12.64692211151123 = 0.08620248734951019 + 2.0 * 6.280359745025635
Epoch 930, val loss: 0.8420224189758301
Epoch 940, training loss: 12.638252258300781 = 0.08266688883304596 + 2.0 * 6.277792453765869
Epoch 940, val loss: 0.8460230231285095
Epoch 950, training loss: 12.634710311889648 = 0.07933010160923004 + 2.0 * 6.2776899337768555
Epoch 950, val loss: 0.8501186966896057
Epoch 960, training loss: 12.629817008972168 = 0.07615687698125839 + 2.0 * 6.276830196380615
Epoch 960, val loss: 0.8542396426200867
Epoch 970, training loss: 12.633319854736328 = 0.07314828038215637 + 2.0 * 6.280085563659668
Epoch 970, val loss: 0.8584481477737427
Epoch 980, training loss: 12.618866920471191 = 0.07031901180744171 + 2.0 * 6.274273872375488
Epoch 980, val loss: 0.8626837134361267
Epoch 990, training loss: 12.616233825683594 = 0.067633718252182 + 2.0 * 6.2743000984191895
Epoch 990, val loss: 0.8669495582580566
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.762962962962963
0.8044280442804429
The final CL Acc:0.76790, 0.01259, The final GNN Acc:0.80724, 0.00217
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13096])
remove edge: torch.Size([2, 7850])
updated graph: torch.Size([2, 10390])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 19.12294578552246 = 1.929247260093689 + 2.0 * 8.59684944152832
Epoch 0, val loss: 1.9333288669586182
Epoch 10, training loss: 19.113285064697266 = 1.9200897216796875 + 2.0 * 8.596597671508789
Epoch 10, val loss: 1.9237178564071655
Epoch 20, training loss: 19.097944259643555 = 1.9087775945663452 + 2.0 * 8.594583511352539
Epoch 20, val loss: 1.9117716550827026
Epoch 30, training loss: 19.051498413085938 = 1.8931196928024292 + 2.0 * 8.57918930053711
Epoch 30, val loss: 1.8952546119689941
Epoch 40, training loss: 18.838529586791992 = 1.8725582361221313 + 2.0 * 8.482985496520996
Epoch 40, val loss: 1.8741012811660767
Epoch 50, training loss: 17.937694549560547 = 1.8493708372116089 + 2.0 * 8.044161796569824
Epoch 50, val loss: 1.8506983518600464
Epoch 60, training loss: 17.18243980407715 = 1.8297594785690308 + 2.0 * 7.676340579986572
Epoch 60, val loss: 1.8313519954681396
Epoch 70, training loss: 16.34687042236328 = 1.8165712356567383 + 2.0 * 7.26515007019043
Epoch 70, val loss: 1.8182557821273804
Epoch 80, training loss: 15.927956581115723 = 1.8072384595870972 + 2.0 * 7.060359001159668
Epoch 80, val loss: 1.8087520599365234
Epoch 90, training loss: 15.67576789855957 = 1.793982744216919 + 2.0 * 6.940892696380615
Epoch 90, val loss: 1.7959067821502686
Epoch 100, training loss: 15.465632438659668 = 1.7797422409057617 + 2.0 * 6.842945098876953
Epoch 100, val loss: 1.783028244972229
Epoch 110, training loss: 15.298954010009766 = 1.76685631275177 + 2.0 * 6.766048908233643
Epoch 110, val loss: 1.7711585760116577
Epoch 120, training loss: 15.15030288696289 = 1.75312077999115 + 2.0 * 6.698591232299805
Epoch 120, val loss: 1.7584315538406372
Epoch 130, training loss: 15.033662796020508 = 1.7383151054382324 + 2.0 * 6.647674083709717
Epoch 130, val loss: 1.7443130016326904
Epoch 140, training loss: 14.938267707824707 = 1.721500039100647 + 2.0 * 6.608383655548096
Epoch 140, val loss: 1.7282651662826538
Epoch 150, training loss: 14.856310844421387 = 1.7020000219345093 + 2.0 * 6.577155590057373
Epoch 150, val loss: 1.7095166444778442
Epoch 160, training loss: 14.789340019226074 = 1.6796597242355347 + 2.0 * 6.554840087890625
Epoch 160, val loss: 1.6882978677749634
Epoch 170, training loss: 14.725598335266113 = 1.6541593074798584 + 2.0 * 6.535719394683838
Epoch 170, val loss: 1.6643372774124146
Epoch 180, training loss: 14.665863990783691 = 1.6251572370529175 + 2.0 * 6.520353317260742
Epoch 180, val loss: 1.6374667882919312
Epoch 190, training loss: 14.606171607971191 = 1.59246826171875 + 2.0 * 6.506851673126221
Epoch 190, val loss: 1.6075509786605835
Epoch 200, training loss: 14.544011116027832 = 1.5561797618865967 + 2.0 * 6.493915557861328
Epoch 200, val loss: 1.5746097564697266
Epoch 210, training loss: 14.49311351776123 = 1.5163699388504028 + 2.0 * 6.488371849060059
Epoch 210, val loss: 1.5387587547302246
Epoch 220, training loss: 14.416793823242188 = 1.4740556478500366 + 2.0 * 6.47136926651001
Epoch 220, val loss: 1.5010433197021484
Epoch 230, training loss: 14.352468490600586 = 1.429546594619751 + 2.0 * 6.461461067199707
Epoch 230, val loss: 1.4618257284164429
Epoch 240, training loss: 14.297353744506836 = 1.383429765701294 + 2.0 * 6.4569621086120605
Epoch 240, val loss: 1.4216870069503784
Epoch 250, training loss: 14.226768493652344 = 1.336922526359558 + 2.0 * 6.444922924041748
Epoch 250, val loss: 1.3819398880004883
Epoch 260, training loss: 14.162237167358398 = 1.2906737327575684 + 2.0 * 6.435781955718994
Epoch 260, val loss: 1.3428778648376465
Epoch 270, training loss: 14.107479095458984 = 1.2450261116027832 + 2.0 * 6.43122673034668
Epoch 270, val loss: 1.3049802780151367
Epoch 280, training loss: 14.044478416442871 = 1.2006423473358154 + 2.0 * 6.421917915344238
Epoch 280, val loss: 1.2689950466156006
Epoch 290, training loss: 13.989367485046387 = 1.1577602624893188 + 2.0 * 6.4158034324646
Epoch 290, val loss: 1.2346731424331665
Epoch 300, training loss: 13.93526840209961 = 1.1161575317382812 + 2.0 * 6.409555435180664
Epoch 300, val loss: 1.2019647359848022
Epoch 310, training loss: 13.883729934692383 = 1.075757384300232 + 2.0 * 6.40398645401001
Epoch 310, val loss: 1.1708173751831055
Epoch 320, training loss: 13.849167823791504 = 1.036436915397644 + 2.0 * 6.406365394592285
Epoch 320, val loss: 1.1412235498428345
Epoch 330, training loss: 13.789814949035645 = 0.9989200830459595 + 2.0 * 6.395447254180908
Epoch 330, val loss: 1.1131867170333862
Epoch 340, training loss: 13.741805076599121 = 0.9624877572059631 + 2.0 * 6.389658451080322
Epoch 340, val loss: 1.0865358114242554
Epoch 350, training loss: 13.697626113891602 = 0.9268485307693481 + 2.0 * 6.3853888511657715
Epoch 350, val loss: 1.0608824491500854
Epoch 360, training loss: 13.657386779785156 = 0.8920449614524841 + 2.0 * 6.382670879364014
Epoch 360, val loss: 1.0361946821212769
Epoch 370, training loss: 13.613204002380371 = 0.8585070967674255 + 2.0 * 6.37734842300415
Epoch 370, val loss: 1.012782096862793
Epoch 380, training loss: 13.573564529418945 = 0.8258100748062134 + 2.0 * 6.373877048492432
Epoch 380, val loss: 0.99025958776474
Epoch 390, training loss: 13.54515266418457 = 0.7939753532409668 + 2.0 * 6.375588893890381
Epoch 390, val loss: 0.9686222672462463
Epoch 400, training loss: 13.498559951782227 = 0.7631821632385254 + 2.0 * 6.36768913269043
Epoch 400, val loss: 0.9481499791145325
Epoch 410, training loss: 13.462457656860352 = 0.7334690690040588 + 2.0 * 6.364494323730469
Epoch 410, val loss: 0.9286913275718689
Epoch 420, training loss: 13.426909446716309 = 0.7047281265258789 + 2.0 * 6.361090660095215
Epoch 420, val loss: 0.9102327227592468
Epoch 430, training loss: 13.399361610412598 = 0.6769838929176331 + 2.0 * 6.361188888549805
Epoch 430, val loss: 0.8928951621055603
Epoch 440, training loss: 13.36347484588623 = 0.6505132913589478 + 2.0 * 6.356480598449707
Epoch 440, val loss: 0.8766664266586304
Epoch 450, training loss: 13.330206871032715 = 0.6249510645866394 + 2.0 * 6.352627754211426
Epoch 450, val loss: 0.8616045713424683
Epoch 460, training loss: 13.300227165222168 = 0.6002560257911682 + 2.0 * 6.349985599517822
Epoch 460, val loss: 0.8474189043045044
Epoch 470, training loss: 13.283674240112305 = 0.5763586759567261 + 2.0 * 6.3536577224731445
Epoch 470, val loss: 0.8342230319976807
Epoch 480, training loss: 13.24452018737793 = 0.5534311532974243 + 2.0 * 6.345544338226318
Epoch 480, val loss: 0.8219006061553955
Epoch 490, training loss: 13.218843460083008 = 0.5311828851699829 + 2.0 * 6.343830108642578
Epoch 490, val loss: 0.8104181885719299
Epoch 500, training loss: 13.190948486328125 = 0.5094531178474426 + 2.0 * 6.340747833251953
Epoch 500, val loss: 0.799616813659668
Epoch 510, training loss: 13.172052383422852 = 0.4881454110145569 + 2.0 * 6.341953277587891
Epoch 510, val loss: 0.7893645167350769
Epoch 520, training loss: 13.149805068969727 = 0.4673081338405609 + 2.0 * 6.341248512268066
Epoch 520, val loss: 0.7799791693687439
Epoch 530, training loss: 13.116476058959961 = 0.44718611240386963 + 2.0 * 6.334644794464111
Epoch 530, val loss: 0.7710227966308594
Epoch 540, training loss: 13.093145370483398 = 0.4274141490459442 + 2.0 * 6.3328657150268555
Epoch 540, val loss: 0.7627276182174683
Epoch 550, training loss: 13.08346939086914 = 0.4080428183078766 + 2.0 * 6.337713241577148
Epoch 550, val loss: 0.7550365328788757
Epoch 560, training loss: 13.053573608398438 = 0.38919591903686523 + 2.0 * 6.332189083099365
Epoch 560, val loss: 0.7479662895202637
Epoch 570, training loss: 13.027301788330078 = 0.37086567282676697 + 2.0 * 6.32821798324585
Epoch 570, val loss: 0.7414981126785278
Epoch 580, training loss: 13.004085540771484 = 0.35303300619125366 + 2.0 * 6.325526237487793
Epoch 580, val loss: 0.7356815934181213
Epoch 590, training loss: 13.013925552368164 = 0.3357909917831421 + 2.0 * 6.339067459106445
Epoch 590, val loss: 0.7304984331130981
Epoch 600, training loss: 12.970510482788086 = 0.3192952573299408 + 2.0 * 6.325607776641846
Epoch 600, val loss: 0.7261109352111816
Epoch 610, training loss: 12.950488090515137 = 0.30356159806251526 + 2.0 * 6.323463439941406
Epoch 610, val loss: 0.7224043011665344
Epoch 620, training loss: 12.928762435913086 = 0.288584440946579 + 2.0 * 6.320088863372803
Epoch 620, val loss: 0.7193957567214966
Epoch 630, training loss: 12.911092758178711 = 0.27437302470207214 + 2.0 * 6.318359851837158
Epoch 630, val loss: 0.7170825004577637
Epoch 640, training loss: 12.895127296447754 = 0.2608529329299927 + 2.0 * 6.317137241363525
Epoch 640, val loss: 0.7154424786567688
Epoch 650, training loss: 12.891205787658691 = 0.24808914959430695 + 2.0 * 6.321558475494385
Epoch 650, val loss: 0.7143867611885071
Epoch 660, training loss: 12.874686241149902 = 0.23612122237682343 + 2.0 * 6.319282531738281
Epoch 660, val loss: 0.713842511177063
Epoch 670, training loss: 12.85007095336914 = 0.2249210774898529 + 2.0 * 6.312574863433838
Epoch 670, val loss: 0.7138543128967285
Epoch 680, training loss: 12.835870742797852 = 0.21435047686100006 + 2.0 * 6.310760021209717
Epoch 680, val loss: 0.7144084572792053
Epoch 690, training loss: 12.82346248626709 = 0.20437419414520264 + 2.0 * 6.309544086456299
Epoch 690, val loss: 0.7153634428977966
Epoch 700, training loss: 12.832944869995117 = 0.1949922889471054 + 2.0 * 6.318976402282715
Epoch 700, val loss: 0.7166852951049805
Epoch 710, training loss: 12.806517601013184 = 0.1862202286720276 + 2.0 * 6.3101487159729
Epoch 710, val loss: 0.718371570110321
Epoch 720, training loss: 12.789371490478516 = 0.17800647020339966 + 2.0 * 6.30568265914917
Epoch 720, val loss: 0.7203954458236694
Epoch 730, training loss: 12.779304504394531 = 0.17027288675308228 + 2.0 * 6.304515838623047
Epoch 730, val loss: 0.7227829694747925
Epoch 740, training loss: 12.787797927856445 = 0.16295839846134186 + 2.0 * 6.312419891357422
Epoch 740, val loss: 0.7254410982131958
Epoch 750, training loss: 12.768800735473633 = 0.15613698959350586 + 2.0 * 6.306331634521484
Epoch 750, val loss: 0.72816002368927
Epoch 760, training loss: 12.75419807434082 = 0.14970819652080536 + 2.0 * 6.302245140075684
Epoch 760, val loss: 0.7311612963676453
Epoch 770, training loss: 12.744834899902344 = 0.14362449944019318 + 2.0 * 6.300605297088623
Epoch 770, val loss: 0.7344236373901367
Epoch 780, training loss: 12.742234230041504 = 0.1378818303346634 + 2.0 * 6.302175998687744
Epoch 780, val loss: 0.7378109693527222
Epoch 790, training loss: 12.728168487548828 = 0.13244277238845825 + 2.0 * 6.297863006591797
Epoch 790, val loss: 0.7413514256477356
Epoch 800, training loss: 12.720626831054688 = 0.12728796899318695 + 2.0 * 6.2966694831848145
Epoch 800, val loss: 0.7450218796730042
Epoch 810, training loss: 12.728032112121582 = 0.12238357216119766 + 2.0 * 6.3028244972229
Epoch 810, val loss: 0.7488858103752136
Epoch 820, training loss: 12.715239524841309 = 0.11775819212198257 + 2.0 * 6.298740863800049
Epoch 820, val loss: 0.7526693940162659
Epoch 830, training loss: 12.70676040649414 = 0.11339569836854935 + 2.0 * 6.296682357788086
Epoch 830, val loss: 0.7566045522689819
Epoch 840, training loss: 12.696837425231934 = 0.10925628989934921 + 2.0 * 6.293790340423584
Epoch 840, val loss: 0.7606438994407654
Epoch 850, training loss: 12.689314842224121 = 0.10533066093921661 + 2.0 * 6.2919921875
Epoch 850, val loss: 0.7647773027420044
Epoch 860, training loss: 12.683453559875488 = 0.10158202797174454 + 2.0 * 6.29093599319458
Epoch 860, val loss: 0.7689933180809021
Epoch 870, training loss: 12.688627243041992 = 0.09801390022039413 + 2.0 * 6.29530668258667
Epoch 870, val loss: 0.773229718208313
Epoch 880, training loss: 12.683526039123535 = 0.09461729973554611 + 2.0 * 6.294454574584961
Epoch 880, val loss: 0.7775055170059204
Epoch 890, training loss: 12.67136001586914 = 0.09141397476196289 + 2.0 * 6.289973258972168
Epoch 890, val loss: 0.7817308902740479
Epoch 900, training loss: 12.661945343017578 = 0.08835874497890472 + 2.0 * 6.286793231964111
Epoch 900, val loss: 0.786051332950592
Epoch 910, training loss: 12.664344787597656 = 0.08543521165847778 + 2.0 * 6.289454936981201
Epoch 910, val loss: 0.7904409766197205
Epoch 920, training loss: 12.65406322479248 = 0.08265819400548935 + 2.0 * 6.285702705383301
Epoch 920, val loss: 0.7947593927383423
Epoch 930, training loss: 12.651032447814941 = 0.07999289035797119 + 2.0 * 6.285519599914551
Epoch 930, val loss: 0.7991334199905396
Epoch 940, training loss: 12.645713806152344 = 0.07745267450809479 + 2.0 * 6.284130573272705
Epoch 940, val loss: 0.8035489320755005
Epoch 950, training loss: 12.646181106567383 = 0.07500042766332626 + 2.0 * 6.285590171813965
Epoch 950, val loss: 0.8080094456672668
Epoch 960, training loss: 12.644733428955078 = 0.07266412675380707 + 2.0 * 6.28603458404541
Epoch 960, val loss: 0.8123974800109863
Epoch 970, training loss: 12.638971328735352 = 0.07043392211198807 + 2.0 * 6.284268856048584
Epoch 970, val loss: 0.81669020652771
Epoch 980, training loss: 12.6297607421875 = 0.06829901784658432 + 2.0 * 6.280730724334717
Epoch 980, val loss: 0.8211421966552734
Epoch 990, training loss: 12.627242088317871 = 0.06625039875507355 + 2.0 * 6.280495643615723
Epoch 990, val loss: 0.8255993723869324
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8111111111111111
0.8371112282551397
=== training gcn model ===
Epoch 0, training loss: 19.15591812133789 = 1.9622888565063477 + 2.0 * 8.59681510925293
Epoch 0, val loss: 1.9627113342285156
Epoch 10, training loss: 19.144323348999023 = 1.951375126838684 + 2.0 * 8.596473693847656
Epoch 10, val loss: 1.9511723518371582
Epoch 20, training loss: 19.125198364257812 = 1.9377886056900024 + 2.0 * 8.593705177307129
Epoch 20, val loss: 1.9364503622055054
Epoch 30, training loss: 19.065298080444336 = 1.9189296960830688 + 2.0 * 8.5731840133667
Epoch 30, val loss: 1.9158433675765991
Epoch 40, training loss: 18.815946578979492 = 1.8957422971725464 + 2.0 * 8.460102081298828
Epoch 40, val loss: 1.8917070627212524
Epoch 50, training loss: 18.04437828063965 = 1.871949315071106 + 2.0 * 8.086214065551758
Epoch 50, val loss: 1.8671178817749023
Epoch 60, training loss: 17.056909561157227 = 1.8532330989837646 + 2.0 * 7.6018385887146
Epoch 60, val loss: 1.8484219312667847
Epoch 70, training loss: 15.998245239257812 = 1.8387101888656616 + 2.0 * 7.07976770401001
Epoch 70, val loss: 1.8338303565979004
Epoch 80, training loss: 15.639644622802734 = 1.8251744508743286 + 2.0 * 6.907235145568848
Epoch 80, val loss: 1.820451259613037
Epoch 90, training loss: 15.411652565002441 = 1.81029212474823 + 2.0 * 6.800680160522461
Epoch 90, val loss: 1.8055751323699951
Epoch 100, training loss: 15.261983871459961 = 1.7951157093048096 + 2.0 * 6.733434200286865
Epoch 100, val loss: 1.790537714958191
Epoch 110, training loss: 15.15058708190918 = 1.7804635763168335 + 2.0 * 6.685061931610107
Epoch 110, val loss: 1.7766423225402832
Epoch 120, training loss: 15.056868553161621 = 1.767045021057129 + 2.0 * 6.644911766052246
Epoch 120, val loss: 1.764094352722168
Epoch 130, training loss: 14.978857040405273 = 1.754267930984497 + 2.0 * 6.612294673919678
Epoch 130, val loss: 1.7522342205047607
Epoch 140, training loss: 14.902235984802246 = 1.7410038709640503 + 2.0 * 6.580615997314453
Epoch 140, val loss: 1.7401231527328491
Epoch 150, training loss: 14.831379890441895 = 1.726531982421875 + 2.0 * 6.55242395401001
Epoch 150, val loss: 1.7268333435058594
Epoch 160, training loss: 14.767406463623047 = 1.7105461359024048 + 2.0 * 6.528429985046387
Epoch 160, val loss: 1.7123271226882935
Epoch 170, training loss: 14.710295677185059 = 1.6931401491165161 + 2.0 * 6.508577823638916
Epoch 170, val loss: 1.6966068744659424
Epoch 180, training loss: 14.656457901000977 = 1.6741573810577393 + 2.0 * 6.491150379180908
Epoch 180, val loss: 1.6794792413711548
Epoch 190, training loss: 14.603876113891602 = 1.6530916690826416 + 2.0 * 6.4753923416137695
Epoch 190, val loss: 1.6605815887451172
Epoch 200, training loss: 14.554618835449219 = 1.629618525505066 + 2.0 * 6.462500095367432
Epoch 200, val loss: 1.6395853757858276
Epoch 210, training loss: 14.50644588470459 = 1.6034295558929443 + 2.0 * 6.451508045196533
Epoch 210, val loss: 1.6162773370742798
Epoch 220, training loss: 14.457344055175781 = 1.574247121810913 + 2.0 * 6.4415483474731445
Epoch 220, val loss: 1.5905122756958008
Epoch 230, training loss: 14.408689498901367 = 1.542331337928772 + 2.0 * 6.433178901672363
Epoch 230, val loss: 1.5625320672988892
Epoch 240, training loss: 14.358198165893555 = 1.508213996887207 + 2.0 * 6.424992084503174
Epoch 240, val loss: 1.533235788345337
Epoch 250, training loss: 14.308378219604492 = 1.4723588228225708 + 2.0 * 6.4180097579956055
Epoch 250, val loss: 1.502747654914856
Epoch 260, training loss: 14.255889892578125 = 1.4342093467712402 + 2.0 * 6.410840034484863
Epoch 260, val loss: 1.4709500074386597
Epoch 270, training loss: 14.203201293945312 = 1.3939714431762695 + 2.0 * 6.4046149253845215
Epoch 270, val loss: 1.437995433807373
Epoch 280, training loss: 14.158336639404297 = 1.3516669273376465 + 2.0 * 6.403334617614746
Epoch 280, val loss: 1.4040898084640503
Epoch 290, training loss: 14.098939895629883 = 1.3086494207382202 + 2.0 * 6.395145416259766
Epoch 290, val loss: 1.3701016902923584
Epoch 300, training loss: 14.042644500732422 = 1.2642028331756592 + 2.0 * 6.389220714569092
Epoch 300, val loss: 1.3356764316558838
Epoch 310, training loss: 13.988177299499512 = 1.2185686826705933 + 2.0 * 6.3848042488098145
Epoch 310, val loss: 1.3008216619491577
Epoch 320, training loss: 13.932480812072754 = 1.171749234199524 + 2.0 * 6.38036584854126
Epoch 320, val loss: 1.2654043436050415
Epoch 330, training loss: 13.880370140075684 = 1.1245521306991577 + 2.0 * 6.377909183502197
Epoch 330, val loss: 1.229996681213379
Epoch 340, training loss: 13.825100898742676 = 1.0778677463531494 + 2.0 * 6.373616695404053
Epoch 340, val loss: 1.1949787139892578
Epoch 350, training loss: 13.770842552185059 = 1.0318946838378906 + 2.0 * 6.369473934173584
Epoch 350, val loss: 1.1605074405670166
Epoch 360, training loss: 13.72358512878418 = 0.9869886636734009 + 2.0 * 6.368298053741455
Epoch 360, val loss: 1.1266491413116455
Epoch 370, training loss: 13.672924041748047 = 0.9436230659484863 + 2.0 * 6.364650726318359
Epoch 370, val loss: 1.0942180156707764
Epoch 380, training loss: 13.625917434692383 = 0.9026986956596375 + 2.0 * 6.36160945892334
Epoch 380, val loss: 1.0634640455245972
Epoch 390, training loss: 13.578958511352539 = 0.8644118309020996 + 2.0 * 6.357273101806641
Epoch 390, val loss: 1.0347412824630737
Epoch 400, training loss: 13.537120819091797 = 0.82877117395401 + 2.0 * 6.354174613952637
Epoch 400, val loss: 1.008375883102417
Epoch 410, training loss: 13.500581741333008 = 0.7959569096565247 + 2.0 * 6.3523125648498535
Epoch 410, val loss: 0.9842883348464966
Epoch 420, training loss: 13.46164608001709 = 0.7657764554023743 + 2.0 * 6.347934722900391
Epoch 420, val loss: 0.9625560641288757
Epoch 430, training loss: 13.428309440612793 = 0.7379713654518127 + 2.0 * 6.3451690673828125
Epoch 430, val loss: 0.9428961873054504
Epoch 440, training loss: 13.40456485748291 = 0.7122819423675537 + 2.0 * 6.346141338348389
Epoch 440, val loss: 0.9250871539115906
Epoch 450, training loss: 13.37414264678955 = 0.6886579394340515 + 2.0 * 6.342742443084717
Epoch 450, val loss: 0.9091038107872009
Epoch 460, training loss: 13.34241008758545 = 0.6666544675827026 + 2.0 * 6.3378777503967285
Epoch 460, val loss: 0.8944671154022217
Epoch 470, training loss: 13.315577507019043 = 0.6457140445709229 + 2.0 * 6.33493185043335
Epoch 470, val loss: 0.8808890581130981
Epoch 480, training loss: 13.289732933044434 = 0.625515341758728 + 2.0 * 6.332108974456787
Epoch 480, val loss: 0.8680613040924072
Epoch 490, training loss: 13.278959274291992 = 0.6059220433235168 + 2.0 * 6.33651876449585
Epoch 490, val loss: 0.8557661175727844
Epoch 500, training loss: 13.248750686645508 = 0.5867977738380432 + 2.0 * 6.330976486206055
Epoch 500, val loss: 0.844012439250946
Epoch 510, training loss: 13.220356941223145 = 0.5680005550384521 + 2.0 * 6.326178073883057
Epoch 510, val loss: 0.8326572179794312
Epoch 520, training loss: 13.19753646850586 = 0.5492470860481262 + 2.0 * 6.3241448402404785
Epoch 520, val loss: 0.821479082107544
Epoch 530, training loss: 13.18322467803955 = 0.5305373668670654 + 2.0 * 6.326343536376953
Epoch 530, val loss: 0.8104503154754639
Epoch 540, training loss: 13.15353012084961 = 0.5119901299476624 + 2.0 * 6.320769786834717
Epoch 540, val loss: 0.7996379733085632
Epoch 550, training loss: 13.129438400268555 = 0.4933237135410309 + 2.0 * 6.318057537078857
Epoch 550, val loss: 0.7889279127120972
Epoch 560, training loss: 13.107603073120117 = 0.474508672952652 + 2.0 * 6.316547393798828
Epoch 560, val loss: 0.7782764434814453
Epoch 570, training loss: 13.088438034057617 = 0.45555806159973145 + 2.0 * 6.316440105438232
Epoch 570, val loss: 0.7677432298660278
Epoch 580, training loss: 13.062250137329102 = 0.4365352690219879 + 2.0 * 6.312857627868652
Epoch 580, val loss: 0.7573862671852112
Epoch 590, training loss: 13.041072845458984 = 0.417394757270813 + 2.0 * 6.3118391036987305
Epoch 590, val loss: 0.7471765875816345
Epoch 600, training loss: 13.018009185791016 = 0.3981935679912567 + 2.0 * 6.309907913208008
Epoch 600, val loss: 0.7371999025344849
Epoch 610, training loss: 12.99824333190918 = 0.3790493309497833 + 2.0 * 6.309597015380859
Epoch 610, val loss: 0.7275156378746033
Epoch 620, training loss: 12.978670120239258 = 0.35994860529899597 + 2.0 * 6.309360980987549
Epoch 620, val loss: 0.7182437777519226
Epoch 630, training loss: 12.967166900634766 = 0.34126341342926025 + 2.0 * 6.312951564788818
Epoch 630, val loss: 0.7094681262969971
Epoch 640, training loss: 12.936620712280273 = 0.32297638058662415 + 2.0 * 6.306822299957275
Epoch 640, val loss: 0.7013016939163208
Epoch 650, training loss: 12.912184715270996 = 0.30511459708213806 + 2.0 * 6.303534984588623
Epoch 650, val loss: 0.6937239766120911
Epoch 660, training loss: 12.891003608703613 = 0.2877551317214966 + 2.0 * 6.301624298095703
Epoch 660, val loss: 0.6868287324905396
Epoch 670, training loss: 12.892727851867676 = 0.2709554433822632 + 2.0 * 6.310886383056641
Epoch 670, val loss: 0.6806461811065674
Epoch 680, training loss: 12.855146408081055 = 0.2551144063472748 + 2.0 * 6.300015926361084
Epoch 680, val loss: 0.6753469705581665
Epoch 690, training loss: 12.837508201599121 = 0.2400200515985489 + 2.0 * 6.298744201660156
Epoch 690, val loss: 0.6708462238311768
Epoch 700, training loss: 12.819470405578613 = 0.22572654485702515 + 2.0 * 6.296872138977051
Epoch 700, val loss: 0.6671581864356995
Epoch 710, training loss: 12.805675506591797 = 0.2123393714427948 + 2.0 * 6.29666805267334
Epoch 710, val loss: 0.6642708778381348
Epoch 720, training loss: 12.792534828186035 = 0.19987909495830536 + 2.0 * 6.296328067779541
Epoch 720, val loss: 0.6621223092079163
Epoch 730, training loss: 12.775968551635742 = 0.1882457435131073 + 2.0 * 6.293861389160156
Epoch 730, val loss: 0.6606476902961731
Epoch 740, training loss: 12.762972831726074 = 0.1773647665977478 + 2.0 * 6.29280424118042
Epoch 740, val loss: 0.6598510146141052
Epoch 750, training loss: 12.751692771911621 = 0.16725455224514008 + 2.0 * 6.292219161987305
Epoch 750, val loss: 0.6596397161483765
Epoch 760, training loss: 12.738801002502441 = 0.15786361694335938 + 2.0 * 6.290468692779541
Epoch 760, val loss: 0.659906804561615
Epoch 770, training loss: 12.734108924865723 = 0.1491239219903946 + 2.0 * 6.292492389678955
Epoch 770, val loss: 0.6606341004371643
Epoch 780, training loss: 12.717117309570312 = 0.14104631543159485 + 2.0 * 6.2880353927612305
Epoch 780, val loss: 0.6618040204048157
Epoch 790, training loss: 12.707816123962402 = 0.13348735868930817 + 2.0 * 6.287164211273193
Epoch 790, val loss: 0.6633455753326416
Epoch 800, training loss: 12.711867332458496 = 0.12643218040466309 + 2.0 * 6.292717456817627
Epoch 800, val loss: 0.665233850479126
Epoch 810, training loss: 12.693337440490723 = 0.11993784457445145 + 2.0 * 6.2866997718811035
Epoch 810, val loss: 0.667456865310669
Epoch 820, training loss: 12.683084487915039 = 0.11386427283287048 + 2.0 * 6.284610271453857
Epoch 820, val loss: 0.6698741912841797
Epoch 830, training loss: 12.675663948059082 = 0.10818293690681458 + 2.0 * 6.283740520477295
Epoch 830, val loss: 0.6725777983665466
Epoch 840, training loss: 12.67109203338623 = 0.10288292914628983 + 2.0 * 6.284104347229004
Epoch 840, val loss: 0.6755157113075256
Epoch 850, training loss: 12.659918785095215 = 0.09794710576534271 + 2.0 * 6.2809858322143555
Epoch 850, val loss: 0.6785851120948792
Epoch 860, training loss: 12.653254508972168 = 0.09331382066011429 + 2.0 * 6.279970169067383
Epoch 860, val loss: 0.6818099617958069
Epoch 870, training loss: 12.655049324035645 = 0.0889679342508316 + 2.0 * 6.283040523529053
Epoch 870, val loss: 0.6851489543914795
Epoch 880, training loss: 12.654006958007812 = 0.08491838723421097 + 2.0 * 6.284544467926025
Epoch 880, val loss: 0.6886783242225647
Epoch 890, training loss: 12.637713432312012 = 0.08110488951206207 + 2.0 * 6.278304100036621
Epoch 890, val loss: 0.6921764612197876
Epoch 900, training loss: 12.632014274597168 = 0.07754359394311905 + 2.0 * 6.277235507965088
Epoch 900, val loss: 0.6957772374153137
Epoch 910, training loss: 12.626361846923828 = 0.07418687641620636 + 2.0 * 6.276087284088135
Epoch 910, val loss: 0.6994966268539429
Epoch 920, training loss: 12.628658294677734 = 0.07102542370557785 + 2.0 * 6.278816223144531
Epoch 920, val loss: 0.7033219337463379
Epoch 930, training loss: 12.630631446838379 = 0.06803295761346817 + 2.0 * 6.281299114227295
Epoch 930, val loss: 0.7070831656455994
Epoch 940, training loss: 12.612602233886719 = 0.06526288390159607 + 2.0 * 6.273669719696045
Epoch 940, val loss: 0.7108486890792847
Epoch 950, training loss: 12.60838508605957 = 0.06264431774616241 + 2.0 * 6.2728705406188965
Epoch 950, val loss: 0.7146735787391663
Epoch 960, training loss: 12.60986328125 = 0.06016135960817337 + 2.0 * 6.274850845336914
Epoch 960, val loss: 0.7185849547386169
Epoch 970, training loss: 12.601490020751953 = 0.05781177431344986 + 2.0 * 6.271839141845703
Epoch 970, val loss: 0.7224922776222229
Epoch 980, training loss: 12.598909378051758 = 0.05558539927005768 + 2.0 * 6.271661758422852
Epoch 980, val loss: 0.7263805866241455
Epoch 990, training loss: 12.596275329589844 = 0.05348321422934532 + 2.0 * 6.271396160125732
Epoch 990, val loss: 0.7302947640419006
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8111111111111111
0.8334211913547708
=== training gcn model ===
Epoch 0, training loss: 19.13414192199707 = 1.9404966831207275 + 2.0 * 8.596822738647461
Epoch 0, val loss: 1.946243166923523
Epoch 10, training loss: 19.12261962890625 = 1.9297971725463867 + 2.0 * 8.596410751342773
Epoch 10, val loss: 1.934997320175171
Epoch 20, training loss: 19.102642059326172 = 1.9164018630981445 + 2.0 * 8.593120574951172
Epoch 20, val loss: 1.9208595752716064
Epoch 30, training loss: 19.040321350097656 = 1.8982901573181152 + 2.0 * 8.571015357971191
Epoch 30, val loss: 1.9019818305969238
Epoch 40, training loss: 18.781869888305664 = 1.8764805793762207 + 2.0 * 8.4526948928833
Epoch 40, val loss: 1.8805413246154785
Epoch 50, training loss: 18.001445770263672 = 1.8527599573135376 + 2.0 * 8.074342727661133
Epoch 50, val loss: 1.8575340509414673
Epoch 60, training loss: 17.355287551879883 = 1.831951379776001 + 2.0 * 7.7616682052612305
Epoch 60, val loss: 1.8386677503585815
Epoch 70, training loss: 16.577791213989258 = 1.8176932334899902 + 2.0 * 7.380048751831055
Epoch 70, val loss: 1.8263643980026245
Epoch 80, training loss: 15.990387916564941 = 1.809134840965271 + 2.0 * 7.0906267166137695
Epoch 80, val loss: 1.8183451890945435
Epoch 90, training loss: 15.703614234924316 = 1.7985235452651978 + 2.0 * 6.952545166015625
Epoch 90, val loss: 1.808060646057129
Epoch 100, training loss: 15.46771240234375 = 1.7850711345672607 + 2.0 * 6.841320514678955
Epoch 100, val loss: 1.7956359386444092
Epoch 110, training loss: 15.285178184509277 = 1.7735460996627808 + 2.0 * 6.7558159828186035
Epoch 110, val loss: 1.7849457263946533
Epoch 120, training loss: 15.158583641052246 = 1.7622438669204712 + 2.0 * 6.698169708251953
Epoch 120, val loss: 1.774269938468933
Epoch 130, training loss: 15.055533409118652 = 1.7500072717666626 + 2.0 * 6.6527628898620605
Epoch 130, val loss: 1.7629187107086182
Epoch 140, training loss: 14.97121810913086 = 1.737032175064087 + 2.0 * 6.617093086242676
Epoch 140, val loss: 1.7513116598129272
Epoch 150, training loss: 14.895423889160156 = 1.7228593826293945 + 2.0 * 6.586282253265381
Epoch 150, val loss: 1.7389708757400513
Epoch 160, training loss: 14.828046798706055 = 1.706855297088623 + 2.0 * 6.560595989227295
Epoch 160, val loss: 1.7253752946853638
Epoch 170, training loss: 14.761005401611328 = 1.6886385679244995 + 2.0 * 6.5361833572387695
Epoch 170, val loss: 1.7101497650146484
Epoch 180, training loss: 14.706246376037598 = 1.6679970026016235 + 2.0 * 6.519124507904053
Epoch 180, val loss: 1.6929235458374023
Epoch 190, training loss: 14.64219856262207 = 1.6447515487670898 + 2.0 * 6.49872350692749
Epoch 190, val loss: 1.673769474029541
Epoch 200, training loss: 14.583614349365234 = 1.618680477142334 + 2.0 * 6.482466697692871
Epoch 200, val loss: 1.6522114276885986
Epoch 210, training loss: 14.540245056152344 = 1.5891499519348145 + 2.0 * 6.475547790527344
Epoch 210, val loss: 1.6277832984924316
Epoch 220, training loss: 14.475626945495605 = 1.5566471815109253 + 2.0 * 6.459489822387695
Epoch 220, val loss: 1.6009975671768188
Epoch 230, training loss: 14.41850757598877 = 1.521262288093567 + 2.0 * 6.448622703552246
Epoch 230, val loss: 1.5719168186187744
Epoch 240, training loss: 14.361328125 = 1.482986569404602 + 2.0 * 6.439170837402344
Epoch 240, val loss: 1.5405309200286865
Epoch 250, training loss: 14.305279731750488 = 1.442270278930664 + 2.0 * 6.431504726409912
Epoch 250, val loss: 1.5073429346084595
Epoch 260, training loss: 14.248212814331055 = 1.4000972509384155 + 2.0 * 6.424057960510254
Epoch 260, val loss: 1.4732493162155151
Epoch 270, training loss: 14.193310737609863 = 1.3568488359451294 + 2.0 * 6.418231010437012
Epoch 270, val loss: 1.4387116432189941
Epoch 280, training loss: 14.133073806762695 = 1.3132760524749756 + 2.0 * 6.40989875793457
Epoch 280, val loss: 1.4039171934127808
Epoch 290, training loss: 14.074505805969238 = 1.2695071697235107 + 2.0 * 6.402499198913574
Epoch 290, val loss: 1.3694835901260376
Epoch 300, training loss: 14.029300689697266 = 1.2261103391647339 + 2.0 * 6.401595115661621
Epoch 300, val loss: 1.335656762123108
Epoch 310, training loss: 13.968674659729004 = 1.184160590171814 + 2.0 * 6.392257213592529
Epoch 310, val loss: 1.3031774759292603
Epoch 320, training loss: 13.918939590454102 = 1.1436543464660645 + 2.0 * 6.3876423835754395
Epoch 320, val loss: 1.2721341848373413
Epoch 330, training loss: 13.870229721069336 = 1.1046664714813232 + 2.0 * 6.382781505584717
Epoch 330, val loss: 1.2425416707992554
Epoch 340, training loss: 13.824702262878418 = 1.0673811435699463 + 2.0 * 6.378660678863525
Epoch 340, val loss: 1.2145243883132935
Epoch 350, training loss: 13.788527488708496 = 1.0320707559585571 + 2.0 * 6.378228187561035
Epoch 350, val loss: 1.1881563663482666
Epoch 360, training loss: 13.74556827545166 = 0.9990829229354858 + 2.0 * 6.3732428550720215
Epoch 360, val loss: 1.1638470888137817
Epoch 370, training loss: 13.702354431152344 = 0.9679362177848816 + 2.0 * 6.367208957672119
Epoch 370, val loss: 1.1410640478134155
Epoch 380, training loss: 13.665966987609863 = 0.9379765391349792 + 2.0 * 6.36399507522583
Epoch 380, val loss: 1.1192766427993774
Epoch 390, training loss: 13.636590957641602 = 0.9090563058853149 + 2.0 * 6.363767147064209
Epoch 390, val loss: 1.0983151197433472
Epoch 400, training loss: 13.595512390136719 = 0.8810903429985046 + 2.0 * 6.357211112976074
Epoch 400, val loss: 1.0783183574676514
Epoch 410, training loss: 13.570117950439453 = 0.8538554310798645 + 2.0 * 6.358131408691406
Epoch 410, val loss: 1.0589991807937622
Epoch 420, training loss: 13.534539222717285 = 0.8272958397865295 + 2.0 * 6.353621482849121
Epoch 420, val loss: 1.0401172637939453
Epoch 430, training loss: 13.497745513916016 = 0.8013975620269775 + 2.0 * 6.348174095153809
Epoch 430, val loss: 1.0218491554260254
Epoch 440, training loss: 13.471782684326172 = 0.775800347328186 + 2.0 * 6.347990989685059
Epoch 440, val loss: 1.0039840936660767
Epoch 450, training loss: 13.441527366638184 = 0.750832736492157 + 2.0 * 6.3453474044799805
Epoch 450, val loss: 0.9864292740821838
Epoch 460, training loss: 13.408309936523438 = 0.7264432907104492 + 2.0 * 6.340933322906494
Epoch 460, val loss: 0.9696078896522522
Epoch 470, training loss: 13.387410163879395 = 0.7025231122970581 + 2.0 * 6.342443466186523
Epoch 470, val loss: 0.9532641172409058
Epoch 480, training loss: 13.35335636138916 = 0.6792116165161133 + 2.0 * 6.337072372436523
Epoch 480, val loss: 0.937497079372406
Epoch 490, training loss: 13.32282543182373 = 0.656642496585846 + 2.0 * 6.3330912590026855
Epoch 490, val loss: 0.9225808382034302
Epoch 500, training loss: 13.296140670776367 = 0.6346023678779602 + 2.0 * 6.330769062042236
Epoch 500, val loss: 0.9082106947898865
Epoch 510, training loss: 13.299275398254395 = 0.6130450963973999 + 2.0 * 6.343115329742432
Epoch 510, val loss: 0.8944815397262573
Epoch 520, training loss: 13.247556686401367 = 0.5922970175743103 + 2.0 * 6.327630043029785
Epoch 520, val loss: 0.881561815738678
Epoch 530, training loss: 13.221714973449707 = 0.5722051858901978 + 2.0 * 6.32475471496582
Epoch 530, val loss: 0.8695975542068481
Epoch 540, training loss: 13.207716941833496 = 0.5526056289672852 + 2.0 * 6.3275556564331055
Epoch 540, val loss: 0.858161211013794
Epoch 550, training loss: 13.175722122192383 = 0.5336822867393494 + 2.0 * 6.321020126342773
Epoch 550, val loss: 0.8475291728973389
Epoch 560, training loss: 13.15422248840332 = 0.5151898860931396 + 2.0 * 6.319516181945801
Epoch 560, val loss: 0.8375142812728882
Epoch 570, training loss: 13.137188911437988 = 0.49702849984169006 + 2.0 * 6.320080280303955
Epoch 570, val loss: 0.8281079530715942
Epoch 580, training loss: 13.119953155517578 = 0.47933295369148254 + 2.0 * 6.320310115814209
Epoch 580, val loss: 0.8190737962722778
Epoch 590, training loss: 13.093185424804688 = 0.4620867371559143 + 2.0 * 6.315549373626709
Epoch 590, val loss: 0.8108131289482117
Epoch 600, training loss: 13.070316314697266 = 0.44516780972480774 + 2.0 * 6.31257438659668
Epoch 600, val loss: 0.8029395341873169
Epoch 610, training loss: 13.058579444885254 = 0.4285392761230469 + 2.0 * 6.3150200843811035
Epoch 610, val loss: 0.795528769493103
Epoch 620, training loss: 13.03959846496582 = 0.4122222661972046 + 2.0 * 6.313688278198242
Epoch 620, val loss: 0.7884652614593506
Epoch 630, training loss: 13.0147123336792 = 0.39629945158958435 + 2.0 * 6.309206485748291
Epoch 630, val loss: 0.7820069193840027
Epoch 640, training loss: 13.001768112182617 = 0.38063958287239075 + 2.0 * 6.310564041137695
Epoch 640, val loss: 0.7759491801261902
Epoch 650, training loss: 12.979059219360352 = 0.3653600215911865 + 2.0 * 6.306849479675293
Epoch 650, val loss: 0.7702940702438354
Epoch 660, training loss: 12.961329460144043 = 0.350333571434021 + 2.0 * 6.305498123168945
Epoch 660, val loss: 0.7651944756507874
Epoch 670, training loss: 12.952184677124023 = 0.3356524109840393 + 2.0 * 6.3082661628723145
Epoch 670, val loss: 0.7605005502700806
Epoch 680, training loss: 12.927708625793457 = 0.32138368487358093 + 2.0 * 6.303162574768066
Epoch 680, val loss: 0.7562897801399231
Epoch 690, training loss: 12.910894393920898 = 0.3074912130832672 + 2.0 * 6.301701545715332
Epoch 690, val loss: 0.7526251077651978
Epoch 700, training loss: 12.914813995361328 = 0.2939947545528412 + 2.0 * 6.3104095458984375
Epoch 700, val loss: 0.7493242621421814
Epoch 710, training loss: 12.881782531738281 = 0.2810570299625397 + 2.0 * 6.300362586975098
Epoch 710, val loss: 0.7463474273681641
Epoch 720, training loss: 12.865335464477539 = 0.26855364441871643 + 2.0 * 6.298390865325928
Epoch 720, val loss: 0.7439723014831543
Epoch 730, training loss: 12.848800659179688 = 0.2564619481563568 + 2.0 * 6.296169281005859
Epoch 730, val loss: 0.7419471740722656
Epoch 740, training loss: 12.8418607711792 = 0.24481816589832306 + 2.0 * 6.298521518707275
Epoch 740, val loss: 0.740344226360321
Epoch 750, training loss: 12.834813117980957 = 0.23369784653186798 + 2.0 * 6.300557613372803
Epoch 750, val loss: 0.7391003370285034
Epoch 760, training loss: 12.809589385986328 = 0.22310400009155273 + 2.0 * 6.293242931365967
Epoch 760, val loss: 0.7382393479347229
Epoch 770, training loss: 12.797656059265137 = 0.2129594385623932 + 2.0 * 6.292348384857178
Epoch 770, val loss: 0.7377405762672424
Epoch 780, training loss: 12.801366806030273 = 0.20323729515075684 + 2.0 * 6.299064636230469
Epoch 780, val loss: 0.737565279006958
Epoch 790, training loss: 12.781100273132324 = 0.19400565326213837 + 2.0 * 6.2935471534729
Epoch 790, val loss: 0.7376534938812256
Epoch 800, training loss: 12.765860557556152 = 0.18517647683620453 + 2.0 * 6.290341854095459
Epoch 800, val loss: 0.7381227612495422
Epoch 810, training loss: 12.760150909423828 = 0.17674927413463593 + 2.0 * 6.291700839996338
Epoch 810, val loss: 0.7388136386871338
Epoch 820, training loss: 12.746040344238281 = 0.16874460875988007 + 2.0 * 6.288647651672363
Epoch 820, val loss: 0.7397164106369019
Epoch 830, training loss: 12.743685722351074 = 0.16111862659454346 + 2.0 * 6.29128360748291
Epoch 830, val loss: 0.7408782243728638
Epoch 840, training loss: 12.726166725158691 = 0.15385949611663818 + 2.0 * 6.286153793334961
Epoch 840, val loss: 0.7422745227813721
Epoch 850, training loss: 12.715413093566895 = 0.14695879817008972 + 2.0 * 6.28422737121582
Epoch 850, val loss: 0.7437922954559326
Epoch 860, training loss: 12.711406707763672 = 0.14038746058940887 + 2.0 * 6.2855095863342285
Epoch 860, val loss: 0.7455294132232666
Epoch 870, training loss: 12.71071720123291 = 0.13412146270275116 + 2.0 * 6.288297653198242
Epoch 870, val loss: 0.7474149465560913
Epoch 880, training loss: 12.695846557617188 = 0.12823475897312164 + 2.0 * 6.283805847167969
Epoch 880, val loss: 0.7493864297866821
Epoch 890, training loss: 12.685199737548828 = 0.12260417640209198 + 2.0 * 6.28129768371582
Epoch 890, val loss: 0.7515709400177002
Epoch 900, training loss: 12.676987648010254 = 0.1172589585185051 + 2.0 * 6.279864311218262
Epoch 900, val loss: 0.7538756132125854
Epoch 910, training loss: 12.67064094543457 = 0.11216229200363159 + 2.0 * 6.279239177703857
Epoch 910, val loss: 0.7563394904136658
Epoch 920, training loss: 12.675390243530273 = 0.1073170006275177 + 2.0 * 6.284036636352539
Epoch 920, val loss: 0.7588863372802734
Epoch 930, training loss: 12.674375534057617 = 0.1027098298072815 + 2.0 * 6.28583288192749
Epoch 930, val loss: 0.7615550756454468
Epoch 940, training loss: 12.652864456176758 = 0.09834729880094528 + 2.0 * 6.277258396148682
Epoch 940, val loss: 0.7642154097557068
Epoch 950, training loss: 12.647477149963379 = 0.09423252195119858 + 2.0 * 6.276622295379639
Epoch 950, val loss: 0.7669770121574402
Epoch 960, training loss: 12.640521049499512 = 0.09031551331281662 + 2.0 * 6.275102615356445
Epoch 960, val loss: 0.7698925137519836
Epoch 970, training loss: 12.65520191192627 = 0.08658169209957123 + 2.0 * 6.284310340881348
Epoch 970, val loss: 0.7727954387664795
Epoch 980, training loss: 12.637273788452148 = 0.08304785192012787 + 2.0 * 6.27711296081543
Epoch 980, val loss: 0.7757899165153503
Epoch 990, training loss: 12.626358032226562 = 0.07968523353338242 + 2.0 * 6.273336410522461
Epoch 990, val loss: 0.7788915634155273
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8222222222222223
0.838165524512388
The final CL Acc:0.81481, 0.00524, The final GNN Acc:0.83623, 0.00203
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.4 der2:0.2 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11604])
remove edge: torch.Size([2, 9558])
updated graph: torch.Size([2, 10606])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 19.132604598999023 = 1.9388607740402222 + 2.0 * 8.596872329711914
Epoch 0, val loss: 1.9361692667007446
Epoch 10, training loss: 19.122787475585938 = 1.9294345378875732 + 2.0 * 8.59667682647705
Epoch 10, val loss: 1.9268105030059814
Epoch 20, training loss: 19.10853385925293 = 1.9183083772659302 + 2.0 * 8.595112800598145
Epoch 20, val loss: 1.9157185554504395
Epoch 30, training loss: 19.06926918029785 = 1.903594732284546 + 2.0 * 8.582837104797363
Epoch 30, val loss: 1.900862455368042
Epoch 40, training loss: 18.89875030517578 = 1.884817361831665 + 2.0 * 8.506966590881348
Epoch 40, val loss: 1.8821983337402344
Epoch 50, training loss: 18.132596969604492 = 1.864471435546875 + 2.0 * 8.134062767028809
Epoch 50, val loss: 1.8624516725540161
Epoch 60, training loss: 17.13153076171875 = 1.8476958274841309 + 2.0 * 7.6419172286987305
Epoch 60, val loss: 1.846716046333313
Epoch 70, training loss: 16.253135681152344 = 1.834387183189392 + 2.0 * 7.20937442779541
Epoch 70, val loss: 1.8341926336288452
Epoch 80, training loss: 15.830429077148438 = 1.8211743831634521 + 2.0 * 7.004627227783203
Epoch 80, val loss: 1.8216627836227417
Epoch 90, training loss: 15.56249713897705 = 1.8074861764907837 + 2.0 * 6.877505302429199
Epoch 90, val loss: 1.8086321353912354
Epoch 100, training loss: 15.406871795654297 = 1.7935044765472412 + 2.0 * 6.806683540344238
Epoch 100, val loss: 1.795653223991394
Epoch 110, training loss: 15.28801155090332 = 1.7793142795562744 + 2.0 * 6.7543487548828125
Epoch 110, val loss: 1.782343864440918
Epoch 120, training loss: 15.199629783630371 = 1.7648845911026 + 2.0 * 6.717372417449951
Epoch 120, val loss: 1.7689212560653687
Epoch 130, training loss: 15.126376152038574 = 1.7500382661819458 + 2.0 * 6.688169002532959
Epoch 130, val loss: 1.7553266286849976
Epoch 140, training loss: 15.056729316711426 = 1.7342828512191772 + 2.0 * 6.661223411560059
Epoch 140, val loss: 1.7411315441131592
Epoch 150, training loss: 14.993210792541504 = 1.716933250427246 + 2.0 * 6.638138771057129
Epoch 150, val loss: 1.7257908582687378
Epoch 160, training loss: 14.926457405090332 = 1.6975526809692383 + 2.0 * 6.614452362060547
Epoch 160, val loss: 1.7090363502502441
Epoch 170, training loss: 14.860593795776367 = 1.6756788492202759 + 2.0 * 6.592457294464111
Epoch 170, val loss: 1.690339207649231
Epoch 180, training loss: 14.796351432800293 = 1.6507165431976318 + 2.0 * 6.572817325592041
Epoch 180, val loss: 1.6692252159118652
Epoch 190, training loss: 14.733184814453125 = 1.6226476430892944 + 2.0 * 6.55526876449585
Epoch 190, val loss: 1.6453269720077515
Epoch 200, training loss: 14.671344757080078 = 1.5912575721740723 + 2.0 * 6.540043354034424
Epoch 200, val loss: 1.61881685256958
Epoch 210, training loss: 14.60503101348877 = 1.5560892820358276 + 2.0 * 6.524470806121826
Epoch 210, val loss: 1.5894501209259033
Epoch 220, training loss: 14.545923233032227 = 1.5175317525863647 + 2.0 * 6.514195919036865
Epoch 220, val loss: 1.557108998298645
Epoch 230, training loss: 14.480615615844727 = 1.4755884408950806 + 2.0 * 6.502513408660889
Epoch 230, val loss: 1.5223402976989746
Epoch 240, training loss: 14.415071487426758 = 1.4304171800613403 + 2.0 * 6.4923272132873535
Epoch 240, val loss: 1.4853090047836304
Epoch 250, training loss: 14.350621223449707 = 1.3822650909423828 + 2.0 * 6.484178066253662
Epoch 250, val loss: 1.4463566541671753
Epoch 260, training loss: 14.287367820739746 = 1.3322917222976685 + 2.0 * 6.477538108825684
Epoch 260, val loss: 1.4065639972686768
Epoch 270, training loss: 14.219564437866211 = 1.2817848920822144 + 2.0 * 6.4688897132873535
Epoch 270, val loss: 1.3667715787887573
Epoch 280, training loss: 14.153007507324219 = 1.2309662103652954 + 2.0 * 6.461020469665527
Epoch 280, val loss: 1.3274531364440918
Epoch 290, training loss: 14.093537330627441 = 1.1807643175125122 + 2.0 * 6.456386566162109
Epoch 290, val loss: 1.2895663976669312
Epoch 300, training loss: 14.031054496765137 = 1.1323292255401611 + 2.0 * 6.449362754821777
Epoch 300, val loss: 1.2536829710006714
Epoch 310, training loss: 13.969067573547363 = 1.0852890014648438 + 2.0 * 6.44188928604126
Epoch 310, val loss: 1.2194761037826538
Epoch 320, training loss: 13.910595893859863 = 1.0393480062484741 + 2.0 * 6.435624122619629
Epoch 320, val loss: 1.1868170499801636
Epoch 330, training loss: 13.854244232177734 = 0.9944158792495728 + 2.0 * 6.4299139976501465
Epoch 330, val loss: 1.1554969549179077
Epoch 340, training loss: 13.803637504577637 = 0.9506498575210571 + 2.0 * 6.4264936447143555
Epoch 340, val loss: 1.1255592107772827
Epoch 350, training loss: 13.758800506591797 = 0.908582866191864 + 2.0 * 6.425108909606934
Epoch 350, val loss: 1.0974773168563843
Epoch 360, training loss: 13.70454216003418 = 0.8683913946151733 + 2.0 * 6.4180755615234375
Epoch 360, val loss: 1.0713011026382446
Epoch 370, training loss: 13.65441608428955 = 0.8298808932304382 + 2.0 * 6.412267684936523
Epoch 370, val loss: 1.0470155477523804
Epoch 380, training loss: 13.609404563903809 = 0.7930619716644287 + 2.0 * 6.4081711769104
Epoch 380, val loss: 1.0245012044906616
Epoch 390, training loss: 13.567047119140625 = 0.7579188346862793 + 2.0 * 6.404564380645752
Epoch 390, val loss: 1.003779649734497
Epoch 400, training loss: 13.523398399353027 = 0.7245084047317505 + 2.0 * 6.399445056915283
Epoch 400, val loss: 0.9847649335861206
Epoch 410, training loss: 13.487732887268066 = 0.6927455067634583 + 2.0 * 6.397493839263916
Epoch 410, val loss: 0.9677110314369202
Epoch 420, training loss: 13.45083999633789 = 0.6627441644668579 + 2.0 * 6.394047737121582
Epoch 420, val loss: 0.9523054361343384
Epoch 430, training loss: 13.414320945739746 = 0.634221613407135 + 2.0 * 6.390049457550049
Epoch 430, val loss: 0.9387762546539307
Epoch 440, training loss: 13.38017749786377 = 0.606972873210907 + 2.0 * 6.386602401733398
Epoch 440, val loss: 0.9267820715904236
Epoch 450, training loss: 13.362262725830078 = 0.580965518951416 + 2.0 * 6.39064884185791
Epoch 450, val loss: 0.9162288308143616
Epoch 460, training loss: 13.321288108825684 = 0.5561681985855103 + 2.0 * 6.382559776306152
Epoch 460, val loss: 0.906867265701294
Epoch 470, training loss: 13.291084289550781 = 0.5324902534484863 + 2.0 * 6.379297256469727
Epoch 470, val loss: 0.8990715742111206
Epoch 480, training loss: 13.261565208435059 = 0.5096614360809326 + 2.0 * 6.375951766967773
Epoch 480, val loss: 0.8922827243804932
Epoch 490, training loss: 13.234882354736328 = 0.48751312494277954 + 2.0 * 6.373684406280518
Epoch 490, val loss: 0.8863760828971863
Epoch 500, training loss: 13.221107482910156 = 0.4659932553768158 + 2.0 * 6.377557277679443
Epoch 500, val loss: 0.8812512159347534
Epoch 510, training loss: 13.182051658630371 = 0.44524189829826355 + 2.0 * 6.368404865264893
Epoch 510, val loss: 0.8770558834075928
Epoch 520, training loss: 13.159582138061523 = 0.4250746965408325 + 2.0 * 6.36725378036499
Epoch 520, val loss: 0.8737351298332214
Epoch 530, training loss: 13.138644218444824 = 0.40537479519844055 + 2.0 * 6.366634845733643
Epoch 530, val loss: 0.8708047866821289
Epoch 540, training loss: 13.113584518432617 = 0.386216402053833 + 2.0 * 6.363684177398682
Epoch 540, val loss: 0.8685226440429688
Epoch 550, training loss: 13.089147567749023 = 0.3675808608531952 + 2.0 * 6.360783576965332
Epoch 550, val loss: 0.8668362498283386
Epoch 560, training loss: 13.06966781616211 = 0.3494340479373932 + 2.0 * 6.360116958618164
Epoch 560, val loss: 0.865691602230072
Epoch 570, training loss: 13.05027961730957 = 0.33183398842811584 + 2.0 * 6.359222888946533
Epoch 570, val loss: 0.8650041818618774
Epoch 580, training loss: 13.024986267089844 = 0.3149375021457672 + 2.0 * 6.355024337768555
Epoch 580, val loss: 0.8649515509605408
Epoch 590, training loss: 13.004203796386719 = 0.29862135648727417 + 2.0 * 6.3527913093566895
Epoch 590, val loss: 0.865540087223053
Epoch 600, training loss: 12.99410629272461 = 0.28288084268569946 + 2.0 * 6.355612754821777
Epoch 600, val loss: 0.8666520714759827
Epoch 610, training loss: 12.973978996276855 = 0.2677139341831207 + 2.0 * 6.353132724761963
Epoch 610, val loss: 0.8679901957511902
Epoch 620, training loss: 12.949315071105957 = 0.25329554080963135 + 2.0 * 6.3480095863342285
Epoch 620, val loss: 0.8703364133834839
Epoch 630, training loss: 12.930766105651855 = 0.23946243524551392 + 2.0 * 6.345651626586914
Epoch 630, val loss: 0.8729454874992371
Epoch 640, training loss: 12.913641929626465 = 0.22622796893119812 + 2.0 * 6.343707084655762
Epoch 640, val loss: 0.8760886788368225
Epoch 650, training loss: 12.90198040008545 = 0.21363331377506256 + 2.0 * 6.344173431396484
Epoch 650, val loss: 0.8796881437301636
Epoch 660, training loss: 12.902604103088379 = 0.20168527960777283 + 2.0 * 6.350459575653076
Epoch 660, val loss: 0.8831550478935242
Epoch 670, training loss: 12.877433776855469 = 0.19053997099399567 + 2.0 * 6.343446731567383
Epoch 670, val loss: 0.8875845074653625
Epoch 680, training loss: 12.85672378540039 = 0.17998789250850677 + 2.0 * 6.338367938995361
Epoch 680, val loss: 0.8924537897109985
Epoch 690, training loss: 12.841992378234863 = 0.1700689047574997 + 2.0 * 6.335961818695068
Epoch 690, val loss: 0.89761883020401
Epoch 700, training loss: 12.82877254486084 = 0.16068509221076965 + 2.0 * 6.334043502807617
Epoch 700, val loss: 0.9029114842414856
Epoch 710, training loss: 12.824363708496094 = 0.15183717012405396 + 2.0 * 6.336263179779053
Epoch 710, val loss: 0.9085516929626465
Epoch 720, training loss: 12.80502700805664 = 0.14356134831905365 + 2.0 * 6.330732822418213
Epoch 720, val loss: 0.9144458174705505
Epoch 730, training loss: 12.794894218444824 = 0.13579323887825012 + 2.0 * 6.329550266265869
Epoch 730, val loss: 0.9206528663635254
Epoch 740, training loss: 12.800968170166016 = 0.12854216992855072 + 2.0 * 6.336213111877441
Epoch 740, val loss: 0.9270188808441162
Epoch 750, training loss: 12.782580375671387 = 0.12170284986495972 + 2.0 * 6.330438613891602
Epoch 750, val loss: 0.9332907795906067
Epoch 760, training loss: 12.766650199890137 = 0.11535965651273727 + 2.0 * 6.325645446777344
Epoch 760, val loss: 0.9400652050971985
Epoch 770, training loss: 12.760005950927734 = 0.10941392928361893 + 2.0 * 6.325295925140381
Epoch 770, val loss: 0.9467872977256775
Epoch 780, training loss: 12.759811401367188 = 0.10383092612028122 + 2.0 * 6.3279900550842285
Epoch 780, val loss: 0.9533659815788269
Epoch 790, training loss: 12.741071701049805 = 0.09863460808992386 + 2.0 * 6.321218490600586
Epoch 790, val loss: 0.9603899717330933
Epoch 800, training loss: 12.734353065490723 = 0.09376578778028488 + 2.0 * 6.320293426513672
Epoch 800, val loss: 0.9674021601676941
Epoch 810, training loss: 12.741726875305176 = 0.08921676129102707 + 2.0 * 6.326254844665527
Epoch 810, val loss: 0.9744159579277039
Epoch 820, training loss: 12.727837562561035 = 0.08493940532207489 + 2.0 * 6.321449279785156
Epoch 820, val loss: 0.9811642169952393
Epoch 830, training loss: 12.716047286987305 = 0.08093761652708054 + 2.0 * 6.317554950714111
Epoch 830, val loss: 0.9882740378379822
Epoch 840, training loss: 12.71031665802002 = 0.0771958976984024 + 2.0 * 6.3165602684021
Epoch 840, val loss: 0.9953114986419678
Epoch 850, training loss: 12.713876724243164 = 0.07367019355297089 + 2.0 * 6.320103168487549
Epoch 850, val loss: 1.0022673606872559
Epoch 860, training loss: 12.701849937438965 = 0.07038528472185135 + 2.0 * 6.315732479095459
Epoch 860, val loss: 1.009268879890442
Epoch 870, training loss: 12.693418502807617 = 0.06728217750787735 + 2.0 * 6.313068389892578
Epoch 870, val loss: 1.0163289308547974
Epoch 880, training loss: 12.690804481506348 = 0.0643690899014473 + 2.0 * 6.313217639923096
Epoch 880, val loss: 1.0232598781585693
Epoch 890, training loss: 12.691652297973633 = 0.06163071468472481 + 2.0 * 6.315011024475098
Epoch 890, val loss: 1.0300406217575073
Epoch 900, training loss: 12.678362846374512 = 0.0590677410364151 + 2.0 * 6.309647560119629
Epoch 900, val loss: 1.0370672941207886
Epoch 910, training loss: 12.671706199645996 = 0.056645240634679794 + 2.0 * 6.307530403137207
Epoch 910, val loss: 1.043944239616394
Epoch 920, training loss: 12.688471794128418 = 0.05436262860894203 + 2.0 * 6.317054748535156
Epoch 920, val loss: 1.0506935119628906
Epoch 930, training loss: 12.66545295715332 = 0.05221008136868477 + 2.0 * 6.306621551513672
Epoch 930, val loss: 1.0571755170822144
Epoch 940, training loss: 12.66197681427002 = 0.05018577352166176 + 2.0 * 6.3058953285217285
Epoch 940, val loss: 1.0638633966445923
Epoch 950, training loss: 12.66954517364502 = 0.04827835038304329 + 2.0 * 6.310633182525635
Epoch 950, val loss: 1.0704540014266968
Epoch 960, training loss: 12.658613204956055 = 0.046466317027807236 + 2.0 * 6.3060736656188965
Epoch 960, val loss: 1.0766013860702515
Epoch 970, training loss: 12.649868965148926 = 0.04475374519824982 + 2.0 * 6.302557468414307
Epoch 970, val loss: 1.0831679105758667
Epoch 980, training loss: 12.645259857177734 = 0.043134503066539764 + 2.0 * 6.30106258392334
Epoch 980, val loss: 1.0893287658691406
Epoch 990, training loss: 12.655760765075684 = 0.041594650596380234 + 2.0 * 6.3070831298828125
Epoch 990, val loss: 1.0953898429870605
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7444444444444445
0.7843964153927254
=== training gcn model ===
Epoch 0, training loss: 19.1450138092041 = 1.9512808322906494 + 2.0 * 8.596866607666016
Epoch 0, val loss: 1.9503408670425415
Epoch 10, training loss: 19.13533592224121 = 1.942000150680542 + 2.0 * 8.596668243408203
Epoch 10, val loss: 1.9413375854492188
Epoch 20, training loss: 19.120473861694336 = 1.9304864406585693 + 2.0 * 8.594993591308594
Epoch 20, val loss: 1.9295992851257324
Epoch 30, training loss: 19.074527740478516 = 1.9146778583526611 + 2.0 * 8.579924583435059
Epoch 30, val loss: 1.9129494428634644
Epoch 40, training loss: 18.849876403808594 = 1.894112229347229 + 2.0 * 8.477882385253906
Epoch 40, val loss: 1.8918641805648804
Epoch 50, training loss: 18.092641830444336 = 1.8722869157791138 + 2.0 * 8.110177040100098
Epoch 50, val loss: 1.870598316192627
Epoch 60, training loss: 17.323955535888672 = 1.8551225662231445 + 2.0 * 7.7344160079956055
Epoch 60, val loss: 1.8546624183654785
Epoch 70, training loss: 16.47001075744629 = 1.8420720100402832 + 2.0 * 7.313969135284424
Epoch 70, val loss: 1.8422224521636963
Epoch 80, training loss: 15.928494453430176 = 1.830553412437439 + 2.0 * 7.048970699310303
Epoch 80, val loss: 1.8307139873504639
Epoch 90, training loss: 15.677266120910645 = 1.8160136938095093 + 2.0 * 6.930626392364502
Epoch 90, val loss: 1.8164488077163696
Epoch 100, training loss: 15.49081802368164 = 1.799898624420166 + 2.0 * 6.845459938049316
Epoch 100, val loss: 1.801201581954956
Epoch 110, training loss: 15.359033584594727 = 1.785041332244873 + 2.0 * 6.786995887756348
Epoch 110, val loss: 1.7871583700180054
Epoch 120, training loss: 15.251864433288574 = 1.7710257768630981 + 2.0 * 6.740419387817383
Epoch 120, val loss: 1.773647665977478
Epoch 130, training loss: 15.15998649597168 = 1.756482720375061 + 2.0 * 6.701751708984375
Epoch 130, val loss: 1.7594937086105347
Epoch 140, training loss: 15.070980072021484 = 1.7408417463302612 + 2.0 * 6.665069103240967
Epoch 140, val loss: 1.744461178779602
Epoch 150, training loss: 14.994268417358398 = 1.7239760160446167 + 2.0 * 6.635146141052246
Epoch 150, val loss: 1.7285369634628296
Epoch 160, training loss: 14.915702819824219 = 1.7054944038391113 + 2.0 * 6.605104446411133
Epoch 160, val loss: 1.7113230228424072
Epoch 170, training loss: 14.844457626342773 = 1.6850800514221191 + 2.0 * 6.579688549041748
Epoch 170, val loss: 1.6924229860305786
Epoch 180, training loss: 14.776678085327148 = 1.6623430252075195 + 2.0 * 6.5571675300598145
Epoch 180, val loss: 1.6716066598892212
Epoch 190, training loss: 14.720047950744629 = 1.6372345685958862 + 2.0 * 6.541406631469727
Epoch 190, val loss: 1.64883291721344
Epoch 200, training loss: 14.649956703186035 = 1.6097278594970703 + 2.0 * 6.520114421844482
Epoch 200, val loss: 1.6241445541381836
Epoch 210, training loss: 14.594310760498047 = 1.5796003341674805 + 2.0 * 6.507355213165283
Epoch 210, val loss: 1.597464919090271
Epoch 220, training loss: 14.531356811523438 = 1.5469192266464233 + 2.0 * 6.492218971252441
Epoch 220, val loss: 1.5690281391143799
Epoch 230, training loss: 14.473321914672852 = 1.5117861032485962 + 2.0 * 6.480767726898193
Epoch 230, val loss: 1.5390160083770752
Epoch 240, training loss: 14.424179077148438 = 1.474397897720337 + 2.0 * 6.47489070892334
Epoch 240, val loss: 1.5076297521591187
Epoch 250, training loss: 14.363378524780273 = 1.435213327407837 + 2.0 * 6.464082717895508
Epoch 250, val loss: 1.4755631685256958
Epoch 260, training loss: 14.301770210266113 = 1.3946613073349 + 2.0 * 6.453554630279541
Epoch 260, val loss: 1.443028450012207
Epoch 270, training loss: 14.24545669555664 = 1.35292387008667 + 2.0 * 6.4462666511535645
Epoch 270, val loss: 1.4102861881256104
Epoch 280, training loss: 14.189735412597656 = 1.3102881908416748 + 2.0 * 6.439723491668701
Epoch 280, val loss: 1.3777607679367065
Epoch 290, training loss: 14.140801429748535 = 1.2671269178390503 + 2.0 * 6.436837196350098
Epoch 290, val loss: 1.3456461429595947
Epoch 300, training loss: 14.083136558532715 = 1.2239494323730469 + 2.0 * 6.429593563079834
Epoch 300, val loss: 1.314073920249939
Epoch 310, training loss: 14.02289867401123 = 1.1808643341064453 + 2.0 * 6.421017169952393
Epoch 310, val loss: 1.2833393812179565
Epoch 320, training loss: 13.969856262207031 = 1.1379402875900269 + 2.0 * 6.415957927703857
Epoch 320, val loss: 1.2534252405166626
Epoch 330, training loss: 13.921339988708496 = 1.0952099561691284 + 2.0 * 6.413064956665039
Epoch 330, val loss: 1.2242372035980225
Epoch 340, training loss: 13.867353439331055 = 1.0530883073806763 + 2.0 * 6.407132625579834
Epoch 340, val loss: 1.195888638496399
Epoch 350, training loss: 13.818831443786621 = 1.011778712272644 + 2.0 * 6.403526306152344
Epoch 350, val loss: 1.168840765953064
Epoch 360, training loss: 13.769201278686523 = 0.97161865234375 + 2.0 * 6.398791313171387
Epoch 360, val loss: 1.1427996158599854
Epoch 370, training loss: 13.721298217773438 = 0.9326282739639282 + 2.0 * 6.39433479309082
Epoch 370, val loss: 1.118253231048584
Epoch 380, training loss: 13.699479103088379 = 0.8951486945152283 + 2.0 * 6.402165412902832
Epoch 380, val loss: 1.095298409461975
Epoch 390, training loss: 13.641082763671875 = 0.859642505645752 + 2.0 * 6.390719890594482
Epoch 390, val loss: 1.0738935470581055
Epoch 400, training loss: 13.594558715820312 = 0.8259744048118591 + 2.0 * 6.384292125701904
Epoch 400, val loss: 1.0543980598449707
Epoch 410, training loss: 13.556042671203613 = 0.793920636177063 + 2.0 * 6.38106107711792
Epoch 410, val loss: 1.0364183187484741
Epoch 420, training loss: 13.519082069396973 = 0.763355016708374 + 2.0 * 6.37786340713501
Epoch 420, val loss: 1.0199569463729858
Epoch 430, training loss: 13.504493713378906 = 0.7341873049736023 + 2.0 * 6.385153293609619
Epoch 430, val loss: 1.004748821258545
Epoch 440, training loss: 13.453531265258789 = 0.7064914703369141 + 2.0 * 6.3735198974609375
Epoch 440, val loss: 0.9910298585891724
Epoch 450, training loss: 13.42601203918457 = 0.6802223920822144 + 2.0 * 6.372894763946533
Epoch 450, val loss: 0.9785326719284058
Epoch 460, training loss: 13.391067504882812 = 0.6551292538642883 + 2.0 * 6.367969036102295
Epoch 460, val loss: 0.9672762155532837
Epoch 470, training loss: 13.365371704101562 = 0.631001889705658 + 2.0 * 6.367185115814209
Epoch 470, val loss: 0.9569993615150452
Epoch 480, training loss: 13.342352867126465 = 0.6078742146492004 + 2.0 * 6.367239475250244
Epoch 480, val loss: 0.9476526975631714
Epoch 490, training loss: 13.3065767288208 = 0.5856977105140686 + 2.0 * 6.360439300537109
Epoch 490, val loss: 0.9392200112342834
Epoch 500, training loss: 13.281051635742188 = 0.5643329620361328 + 2.0 * 6.358359336853027
Epoch 500, val loss: 0.93160480260849
Epoch 510, training loss: 13.271455764770508 = 0.543645441532135 + 2.0 * 6.36390495300293
Epoch 510, val loss: 0.9247296452522278
Epoch 520, training loss: 13.238825798034668 = 0.5235596299171448 + 2.0 * 6.357633113861084
Epoch 520, val loss: 0.9181711077690125
Epoch 530, training loss: 13.207860946655273 = 0.5041247606277466 + 2.0 * 6.351868152618408
Epoch 530, val loss: 0.9123677611351013
Epoch 540, training loss: 13.186047554016113 = 0.4851064085960388 + 2.0 * 6.350470542907715
Epoch 540, val loss: 0.9071152806282043
Epoch 550, training loss: 13.167728424072266 = 0.4664769768714905 + 2.0 * 6.350625514984131
Epoch 550, val loss: 0.9021419286727905
Epoch 560, training loss: 13.143925666809082 = 0.44830867648124695 + 2.0 * 6.347808361053467
Epoch 560, val loss: 0.8977820873260498
Epoch 570, training loss: 13.120548248291016 = 0.43048709630966187 + 2.0 * 6.345030784606934
Epoch 570, val loss: 0.8936751484870911
Epoch 580, training loss: 13.102994918823242 = 0.4130205810070038 + 2.0 * 6.344987392425537
Epoch 580, val loss: 0.8901054859161377
Epoch 590, training loss: 13.080840110778809 = 0.39588406682014465 + 2.0 * 6.342477798461914
Epoch 590, val loss: 0.8868829607963562
Epoch 600, training loss: 13.06074333190918 = 0.37913307547569275 + 2.0 * 6.3408050537109375
Epoch 600, val loss: 0.884017288684845
Epoch 610, training loss: 13.040465354919434 = 0.3627554178237915 + 2.0 * 6.338854789733887
Epoch 610, val loss: 0.8815113306045532
Epoch 620, training loss: 13.025979995727539 = 0.3468177020549774 + 2.0 * 6.33958101272583
Epoch 620, val loss: 0.8795625567436218
Epoch 630, training loss: 13.003782272338867 = 0.3313048481941223 + 2.0 * 6.336238861083984
Epoch 630, val loss: 0.8779222369194031
Epoch 640, training loss: 12.987524032592773 = 0.31625884771347046 + 2.0 * 6.335632801055908
Epoch 640, val loss: 0.8769065141677856
Epoch 650, training loss: 12.974143028259277 = 0.3016742467880249 + 2.0 * 6.3362345695495605
Epoch 650, val loss: 0.876386284828186
Epoch 660, training loss: 12.953557968139648 = 0.2875669002532959 + 2.0 * 6.332995414733887
Epoch 660, val loss: 0.8762758374214172
Epoch 670, training loss: 12.933979034423828 = 0.27402734756469727 + 2.0 * 6.329975605010986
Epoch 670, val loss: 0.876836359500885
Epoch 680, training loss: 12.919428825378418 = 0.2609860599040985 + 2.0 * 6.329221248626709
Epoch 680, val loss: 0.877881646156311
Epoch 690, training loss: 12.91517448425293 = 0.2484893500804901 + 2.0 * 6.333342552185059
Epoch 690, val loss: 0.8794314861297607
Epoch 700, training loss: 12.891656875610352 = 0.23659580945968628 + 2.0 * 6.327530384063721
Epoch 700, val loss: 0.8813804984092712
Epoch 710, training loss: 12.874523162841797 = 0.22522343695163727 + 2.0 * 6.324649810791016
Epoch 710, val loss: 0.8837901949882507
Epoch 720, training loss: 12.863986015319824 = 0.21438059210777283 + 2.0 * 6.324802875518799
Epoch 720, val loss: 0.8867139220237732
Epoch 730, training loss: 12.846874237060547 = 0.20400075614452362 + 2.0 * 6.321436882019043
Epoch 730, val loss: 0.8899432420730591
Epoch 740, training loss: 12.836384773254395 = 0.1941371113061905 + 2.0 * 6.3211236000061035
Epoch 740, val loss: 0.8936920762062073
Epoch 750, training loss: 12.828519821166992 = 0.18475408852100372 + 2.0 * 6.321882724761963
Epoch 750, val loss: 0.8978245258331299
Epoch 760, training loss: 12.819193840026855 = 0.1758110225200653 + 2.0 * 6.321691513061523
Epoch 760, val loss: 0.9023302793502808
Epoch 770, training loss: 12.8026123046875 = 0.16731898486614227 + 2.0 * 6.317646503448486
Epoch 770, val loss: 0.9072162508964539
Epoch 780, training loss: 12.792096138000488 = 0.1592605859041214 + 2.0 * 6.316417694091797
Epoch 780, val loss: 0.9124779105186462
Epoch 790, training loss: 12.785006523132324 = 0.15163438022136688 + 2.0 * 6.316686153411865
Epoch 790, val loss: 0.9180352687835693
Epoch 800, training loss: 12.776358604431152 = 0.14436787366867065 + 2.0 * 6.315995216369629
Epoch 800, val loss: 0.9238458871841431
Epoch 810, training loss: 12.764655113220215 = 0.13751225173473358 + 2.0 * 6.313571453094482
Epoch 810, val loss: 0.9299271106719971
Epoch 820, training loss: 12.753073692321777 = 0.1309976428747177 + 2.0 * 6.311038017272949
Epoch 820, val loss: 0.9363549947738647
Epoch 830, training loss: 12.743167877197266 = 0.1248406395316124 + 2.0 * 6.309163570404053
Epoch 830, val loss: 0.9431024193763733
Epoch 840, training loss: 12.761056900024414 = 0.1189904734492302 + 2.0 * 6.321033000946045
Epoch 840, val loss: 0.9500705003738403
Epoch 850, training loss: 12.737001419067383 = 0.11349481344223022 + 2.0 * 6.311753273010254
Epoch 850, val loss: 0.9570767879486084
Epoch 860, training loss: 12.72195053100586 = 0.10831212252378464 + 2.0 * 6.306819438934326
Epoch 860, val loss: 0.9642955660820007
Epoch 870, training loss: 12.713638305664062 = 0.10340956598520279 + 2.0 * 6.305114269256592
Epoch 870, val loss: 0.9718279838562012
Epoch 880, training loss: 12.70569896697998 = 0.09876039624214172 + 2.0 * 6.303469181060791
Epoch 880, val loss: 0.9795107245445251
Epoch 890, training loss: 12.711727142333984 = 0.09434425830841064 + 2.0 * 6.308691501617432
Epoch 890, val loss: 0.9873443841934204
Epoch 900, training loss: 12.710440635681152 = 0.09016872197389603 + 2.0 * 6.310135841369629
Epoch 900, val loss: 0.9953272938728333
Epoch 910, training loss: 12.691437721252441 = 0.0862501710653305 + 2.0 * 6.30259370803833
Epoch 910, val loss: 1.0032507181167603
Epoch 920, training loss: 12.686887741088867 = 0.08253907412290573 + 2.0 * 6.3021745681762695
Epoch 920, val loss: 1.0113945007324219
Epoch 930, training loss: 12.677984237670898 = 0.07903335988521576 + 2.0 * 6.29947566986084
Epoch 930, val loss: 1.0195837020874023
Epoch 940, training loss: 12.67882251739502 = 0.07571758329868317 + 2.0 * 6.3015522956848145
Epoch 940, val loss: 1.0278069972991943
Epoch 950, training loss: 12.668782234191895 = 0.07256916165351868 + 2.0 * 6.298106670379639
Epoch 950, val loss: 1.0360387563705444
Epoch 960, training loss: 12.663701057434082 = 0.06959592550992966 + 2.0 * 6.297052383422852
Epoch 960, val loss: 1.0443679094314575
Epoch 970, training loss: 12.670948028564453 = 0.06679291278123856 + 2.0 * 6.302077770233154
Epoch 970, val loss: 1.052680253982544
Epoch 980, training loss: 12.655719757080078 = 0.06411932408809662 + 2.0 * 6.29580020904541
Epoch 980, val loss: 1.0609074831008911
Epoch 990, training loss: 12.648799896240234 = 0.06160745769739151 + 2.0 * 6.293596267700195
Epoch 990, val loss: 1.069291353225708
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7703703703703704
0.797575118608329
=== training gcn model ===
Epoch 0, training loss: 19.130958557128906 = 1.9372795820236206 + 2.0 * 8.596839904785156
Epoch 0, val loss: 1.9385297298431396
Epoch 10, training loss: 19.120309829711914 = 1.9272923469543457 + 2.0 * 8.596508979797363
Epoch 10, val loss: 1.9284803867340088
Epoch 20, training loss: 19.102148056030273 = 1.9149781465530396 + 2.0 * 8.593585014343262
Epoch 20, val loss: 1.9161648750305176
Epoch 30, training loss: 19.0323543548584 = 1.89842689037323 + 2.0 * 8.566964149475098
Epoch 30, val loss: 1.8998967409133911
Epoch 40, training loss: 18.59344482421875 = 1.8785455226898193 + 2.0 * 8.357449531555176
Epoch 40, val loss: 1.8807144165039062
Epoch 50, training loss: 17.615663528442383 = 1.8567579984664917 + 2.0 * 7.879452705383301
Epoch 50, val loss: 1.8601019382476807
Epoch 60, training loss: 16.661739349365234 = 1.8416982889175415 + 2.0 * 7.410020351409912
Epoch 60, val loss: 1.8461449146270752
Epoch 70, training loss: 16.119081497192383 = 1.8309839963912964 + 2.0 * 7.144048690795898
Epoch 70, val loss: 1.8349593877792358
Epoch 80, training loss: 15.778624534606934 = 1.8205093145370483 + 2.0 * 6.979057788848877
Epoch 80, val loss: 1.8245989084243774
Epoch 90, training loss: 15.54870891571045 = 1.8082122802734375 + 2.0 * 6.870248317718506
Epoch 90, val loss: 1.813518762588501
Epoch 100, training loss: 15.39263916015625 = 1.7963272333145142 + 2.0 * 6.798155784606934
Epoch 100, val loss: 1.8029519319534302
Epoch 110, training loss: 15.285587310791016 = 1.784515380859375 + 2.0 * 6.75053596496582
Epoch 110, val loss: 1.7921518087387085
Epoch 120, training loss: 15.18830680847168 = 1.7720309495925903 + 2.0 * 6.7081379890441895
Epoch 120, val loss: 1.7805969715118408
Epoch 130, training loss: 15.09435749053955 = 1.758995771408081 + 2.0 * 6.667680740356445
Epoch 130, val loss: 1.768569827079773
Epoch 140, training loss: 15.017339706420898 = 1.745093822479248 + 2.0 * 6.636122703552246
Epoch 140, val loss: 1.756052851676941
Epoch 150, training loss: 14.940171241760254 = 1.7298353910446167 + 2.0 * 6.605167865753174
Epoch 150, val loss: 1.7426379919052124
Epoch 160, training loss: 14.87095832824707 = 1.7127264738082886 + 2.0 * 6.579115867614746
Epoch 160, val loss: 1.7279466390609741
Epoch 170, training loss: 14.813939094543457 = 1.6933740377426147 + 2.0 * 6.5602827072143555
Epoch 170, val loss: 1.7115012407302856
Epoch 180, training loss: 14.752955436706543 = 1.6715255975723267 + 2.0 * 6.540714740753174
Epoch 180, val loss: 1.6931978464126587
Epoch 190, training loss: 14.69067096710205 = 1.6470681428909302 + 2.0 * 6.521801471710205
Epoch 190, val loss: 1.6728591918945312
Epoch 200, training loss: 14.64112663269043 = 1.6196624040603638 + 2.0 * 6.510732173919678
Epoch 200, val loss: 1.6503187417984009
Epoch 210, training loss: 14.577962875366211 = 1.5896000862121582 + 2.0 * 6.4941816329956055
Epoch 210, val loss: 1.6258163452148438
Epoch 220, training loss: 14.520181655883789 = 1.5567890405654907 + 2.0 * 6.481696128845215
Epoch 220, val loss: 1.5994129180908203
Epoch 230, training loss: 14.462525367736816 = 1.5212517976760864 + 2.0 * 6.47063684463501
Epoch 230, val loss: 1.571186900138855
Epoch 240, training loss: 14.40629768371582 = 1.483187198638916 + 2.0 * 6.461555480957031
Epoch 240, val loss: 1.5414297580718994
Epoch 250, training loss: 14.351903915405273 = 1.4436612129211426 + 2.0 * 6.4541215896606445
Epoch 250, val loss: 1.510825276374817
Epoch 260, training loss: 14.29097843170166 = 1.4027197360992432 + 2.0 * 6.444129467010498
Epoch 260, val loss: 1.4797917604446411
Epoch 270, training loss: 14.232263565063477 = 1.3606127500534058 + 2.0 * 6.435825347900391
Epoch 270, val loss: 1.448304295539856
Epoch 280, training loss: 14.175291061401367 = 1.3174545764923096 + 2.0 * 6.428918361663818
Epoch 280, val loss: 1.4165984392166138
Epoch 290, training loss: 14.120306015014648 = 1.2735276222229004 + 2.0 * 6.423389434814453
Epoch 290, val loss: 1.3848191499710083
Epoch 300, training loss: 14.069307327270508 = 1.2293477058410645 + 2.0 * 6.419979572296143
Epoch 300, val loss: 1.353171467781067
Epoch 310, training loss: 14.010953903198242 = 1.185099720954895 + 2.0 * 6.412927150726318
Epoch 310, val loss: 1.321785807609558
Epoch 320, training loss: 13.954620361328125 = 1.1406407356262207 + 2.0 * 6.406990051269531
Epoch 320, val loss: 1.290507435798645
Epoch 330, training loss: 13.90174674987793 = 1.0960346460342407 + 2.0 * 6.40285587310791
Epoch 330, val loss: 1.2593644857406616
Epoch 340, training loss: 13.846725463867188 = 1.0514663457870483 + 2.0 * 6.397629737854004
Epoch 340, val loss: 1.2285256385803223
Epoch 350, training loss: 13.801907539367676 = 1.0073797702789307 + 2.0 * 6.397264003753662
Epoch 350, val loss: 1.198096752166748
Epoch 360, training loss: 13.744649887084961 = 0.9638674259185791 + 2.0 * 6.3903913497924805
Epoch 360, val loss: 1.1684781312942505
Epoch 370, training loss: 13.692495346069336 = 0.9213282465934753 + 2.0 * 6.385583400726318
Epoch 370, val loss: 1.139837622642517
Epoch 380, training loss: 13.65334415435791 = 0.8800948262214661 + 2.0 * 6.386624813079834
Epoch 380, val loss: 1.1126233339309692
Epoch 390, training loss: 13.59756851196289 = 0.8404421806335449 + 2.0 * 6.378563404083252
Epoch 390, val loss: 1.0867846012115479
Epoch 400, training loss: 13.553516387939453 = 0.8027059435844421 + 2.0 * 6.375405311584473
Epoch 400, val loss: 1.0626840591430664
Epoch 410, training loss: 13.513633728027344 = 0.7667229771614075 + 2.0 * 6.37345552444458
Epoch 410, val loss: 1.040297269821167
Epoch 420, training loss: 13.478724479675293 = 0.7325230240821838 + 2.0 * 6.373100757598877
Epoch 420, val loss: 1.019620418548584
Epoch 430, training loss: 13.433663368225098 = 0.7002319097518921 + 2.0 * 6.366715908050537
Epoch 430, val loss: 1.0005991458892822
Epoch 440, training loss: 13.394633293151855 = 0.6697186231613159 + 2.0 * 6.362457275390625
Epoch 440, val loss: 0.9834848046302795
Epoch 450, training loss: 13.361114501953125 = 0.640760064125061 + 2.0 * 6.360177040100098
Epoch 450, val loss: 0.9679021835327148
Epoch 460, training loss: 13.3372802734375 = 0.6133432388305664 + 2.0 * 6.361968517303467
Epoch 460, val loss: 0.9537814855575562
Epoch 470, training loss: 13.298563003540039 = 0.5876241326332092 + 2.0 * 6.355469226837158
Epoch 470, val loss: 0.9410891532897949
Epoch 480, training loss: 13.266517639160156 = 0.5631792545318604 + 2.0 * 6.3516693115234375
Epoch 480, val loss: 0.9298271536827087
Epoch 490, training loss: 13.254851341247559 = 0.5399125218391418 + 2.0 * 6.35746955871582
Epoch 490, val loss: 0.9197036027908325
Epoch 500, training loss: 13.212031364440918 = 0.5178896188735962 + 2.0 * 6.347070693969727
Epoch 500, val loss: 0.9108521938323975
Epoch 510, training loss: 13.185491561889648 = 0.4969048500061035 + 2.0 * 6.344293594360352
Epoch 510, val loss: 0.9029110074043274
Epoch 520, training loss: 13.159306526184082 = 0.47678086161613464 + 2.0 * 6.3412628173828125
Epoch 520, val loss: 0.8959371447563171
Epoch 530, training loss: 13.150946617126465 = 0.45742279291152954 + 2.0 * 6.346761703491211
Epoch 530, val loss: 0.8897007703781128
Epoch 540, training loss: 13.115913391113281 = 0.43882569670677185 + 2.0 * 6.338543891906738
Epoch 540, val loss: 0.8842769861221313
Epoch 550, training loss: 13.093791961669922 = 0.42098814249038696 + 2.0 * 6.33640193939209
Epoch 550, val loss: 0.8796009421348572
Epoch 560, training loss: 13.070290565490723 = 0.40377652645111084 + 2.0 * 6.33325719833374
Epoch 560, val loss: 0.8755607604980469
Epoch 570, training loss: 13.057222366333008 = 0.3870435357093811 + 2.0 * 6.335089206695557
Epoch 570, val loss: 0.8721027970314026
Epoch 580, training loss: 13.040325164794922 = 0.3707970678806305 + 2.0 * 6.334764003753662
Epoch 580, val loss: 0.8691068887710571
Epoch 590, training loss: 13.009960174560547 = 0.3550657331943512 + 2.0 * 6.327447414398193
Epoch 590, val loss: 0.8667519092559814
Epoch 600, training loss: 12.99120807647705 = 0.33973899483680725 + 2.0 * 6.325734615325928
Epoch 600, val loss: 0.8648765087127686
Epoch 610, training loss: 12.97393798828125 = 0.32479128241539 + 2.0 * 6.324573516845703
Epoch 610, val loss: 0.8634427189826965
Epoch 620, training loss: 12.960006713867188 = 0.310192734003067 + 2.0 * 6.324906826019287
Epoch 620, val loss: 0.8623216152191162
Epoch 630, training loss: 12.937917709350586 = 0.295943021774292 + 2.0 * 6.320987224578857
Epoch 630, val loss: 0.8616619110107422
Epoch 640, training loss: 12.923640251159668 = 0.2821420431137085 + 2.0 * 6.320749282836914
Epoch 640, val loss: 0.8614830374717712
Epoch 650, training loss: 12.90372085571289 = 0.26867401599884033 + 2.0 * 6.31752347946167
Epoch 650, val loss: 0.8615557551383972
Epoch 660, training loss: 12.891397476196289 = 0.25564396381378174 + 2.0 * 6.317876815795898
Epoch 660, val loss: 0.8621630072593689
Epoch 670, training loss: 12.87439250946045 = 0.24301999807357788 + 2.0 * 6.315686225891113
Epoch 670, val loss: 0.8631048798561096
Epoch 680, training loss: 12.863880157470703 = 0.2308426946401596 + 2.0 * 6.316518783569336
Epoch 680, val loss: 0.8644953966140747
Epoch 690, training loss: 12.843518257141113 = 0.21916791796684265 + 2.0 * 6.312175273895264
Epoch 690, val loss: 0.866244375705719
Epoch 700, training loss: 12.829504013061523 = 0.20797660946846008 + 2.0 * 6.310763835906982
Epoch 700, val loss: 0.8683260679244995
Epoch 710, training loss: 12.819711685180664 = 0.19728241860866547 + 2.0 * 6.311214447021484
Epoch 710, val loss: 0.8708506226539612
Epoch 720, training loss: 12.80627727508545 = 0.18707652390003204 + 2.0 * 6.309600353240967
Epoch 720, val loss: 0.8735866546630859
Epoch 730, training loss: 12.790751457214355 = 0.17735114693641663 + 2.0 * 6.306700229644775
Epoch 730, val loss: 0.8767720460891724
Epoch 740, training loss: 12.782326698303223 = 0.16813620924949646 + 2.0 * 6.307095050811768
Epoch 740, val loss: 0.8803165555000305
Epoch 750, training loss: 12.770824432373047 = 0.15940707921981812 + 2.0 * 6.305708885192871
Epoch 750, val loss: 0.8842556476593018
Epoch 760, training loss: 12.760181427001953 = 0.1511288583278656 + 2.0 * 6.304526329040527
Epoch 760, val loss: 0.8885063529014587
Epoch 770, training loss: 12.745576858520508 = 0.14332744479179382 + 2.0 * 6.301124572753906
Epoch 770, val loss: 0.8930026292800903
Epoch 780, training loss: 12.736784934997559 = 0.1359586864709854 + 2.0 * 6.300413131713867
Epoch 780, val loss: 0.8978381156921387
Epoch 790, training loss: 12.736211776733398 = 0.12900322675704956 + 2.0 * 6.3036041259765625
Epoch 790, val loss: 0.9029688835144043
Epoch 800, training loss: 12.720264434814453 = 0.12246637791395187 + 2.0 * 6.298899173736572
Epoch 800, val loss: 0.9084799885749817
Epoch 810, training loss: 12.71499252319336 = 0.11629148572683334 + 2.0 * 6.299350738525391
Epoch 810, val loss: 0.9140552282333374
Epoch 820, training loss: 12.704181671142578 = 0.11048514395952225 + 2.0 * 6.296848297119141
Epoch 820, val loss: 0.9199807047843933
Epoch 830, training loss: 12.70684814453125 = 0.10501029342412949 + 2.0 * 6.300919055938721
Epoch 830, val loss: 0.9260478019714355
Epoch 840, training loss: 12.690167427062988 = 0.0998624786734581 + 2.0 * 6.29515266418457
Epoch 840, val loss: 0.9323210716247559
Epoch 850, training loss: 12.681913375854492 = 0.09502709656953812 + 2.0 * 6.293443202972412
Epoch 850, val loss: 0.9387471675872803
Epoch 860, training loss: 12.676243782043457 = 0.09047197550535202 + 2.0 * 6.292885780334473
Epoch 860, val loss: 0.9452808499336243
Epoch 870, training loss: 12.679195404052734 = 0.08619965612888336 + 2.0 * 6.296497821807861
Epoch 870, val loss: 0.9519307613372803
Epoch 880, training loss: 12.664349555969238 = 0.0821656882762909 + 2.0 * 6.2910919189453125
Epoch 880, val loss: 0.9586384892463684
Epoch 890, training loss: 12.657697677612305 = 0.07838167250156403 + 2.0 * 6.289658069610596
Epoch 890, val loss: 0.9654507040977478
Epoch 900, training loss: 12.658723831176758 = 0.07482028007507324 + 2.0 * 6.291951656341553
Epoch 900, val loss: 0.9723365902900696
Epoch 910, training loss: 12.650123596191406 = 0.0714896097779274 + 2.0 * 6.2893171310424805
Epoch 910, val loss: 0.9792645573616028
Epoch 920, training loss: 12.644465446472168 = 0.06833182275295258 + 2.0 * 6.288066864013672
Epoch 920, val loss: 0.9862253665924072
Epoch 930, training loss: 12.638700485229492 = 0.06537626683712006 + 2.0 * 6.2866621017456055
Epoch 930, val loss: 0.9931995272636414
Epoch 940, training loss: 12.633435249328613 = 0.06258956342935562 + 2.0 * 6.2854228019714355
Epoch 940, val loss: 1.0002456903457642
Epoch 950, training loss: 12.63978099822998 = 0.05995646491646767 + 2.0 * 6.289912223815918
Epoch 950, val loss: 1.007195234298706
Epoch 960, training loss: 12.62296199798584 = 0.057490136474370956 + 2.0 * 6.282735824584961
Epoch 960, val loss: 1.0142669677734375
Epoch 970, training loss: 12.6185884475708 = 0.055159490555524826 + 2.0 * 6.28171443939209
Epoch 970, val loss: 1.0212501287460327
Epoch 980, training loss: 12.615074157714844 = 0.052955903112888336 + 2.0 * 6.281059265136719
Epoch 980, val loss: 1.0282305479049683
Epoch 990, training loss: 12.63468074798584 = 0.05087311565876007 + 2.0 * 6.291903972625732
Epoch 990, val loss: 1.0352154970169067
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7740740740740741
0.7970479704797049
The final CL Acc:0.76296, 0.01318, The final GNN Acc:0.79301, 0.00609
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13136])
remove edge: torch.Size([2, 7898])
updated graph: torch.Size([2, 10478])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 19.155385971069336 = 1.9616769552230835 + 2.0 * 8.596854209899902
Epoch 0, val loss: 1.9558175802230835
Epoch 10, training loss: 19.14437484741211 = 1.9512118101119995 + 2.0 * 8.59658145904541
Epoch 10, val loss: 1.945267915725708
Epoch 20, training loss: 19.126928329467773 = 1.9383658170700073 + 2.0 * 8.594281196594238
Epoch 20, val loss: 1.932404637336731
Epoch 30, training loss: 19.069644927978516 = 1.9208959341049194 + 2.0 * 8.574374198913574
Epoch 30, val loss: 1.9150508642196655
Epoch 40, training loss: 18.764272689819336 = 1.8987771272659302 + 2.0 * 8.432747840881348
Epoch 40, val loss: 1.8936898708343506
Epoch 50, training loss: 17.368162155151367 = 1.8754063844680786 + 2.0 * 7.746377944946289
Epoch 50, val loss: 1.8715627193450928
Epoch 60, training loss: 16.573558807373047 = 1.8575068712234497 + 2.0 * 7.358026027679443
Epoch 60, val loss: 1.8555083274841309
Epoch 70, training loss: 16.12112808227539 = 1.841763973236084 + 2.0 * 7.139682292938232
Epoch 70, val loss: 1.8408198356628418
Epoch 80, training loss: 15.836899757385254 = 1.8250877857208252 + 2.0 * 7.005906105041504
Epoch 80, val loss: 1.8251376152038574
Epoch 90, training loss: 15.608285903930664 = 1.8083288669586182 + 2.0 * 6.8999786376953125
Epoch 90, val loss: 1.810263752937317
Epoch 100, training loss: 15.438907623291016 = 1.7924036979675293 + 2.0 * 6.823252201080322
Epoch 100, val loss: 1.796448826789856
Epoch 110, training loss: 15.28797435760498 = 1.776275634765625 + 2.0 * 6.755849361419678
Epoch 110, val loss: 1.782475471496582
Epoch 120, training loss: 15.167794227600098 = 1.7596269845962524 + 2.0 * 6.704083442687988
Epoch 120, val loss: 1.7680326700210571
Epoch 130, training loss: 15.061436653137207 = 1.7427068948745728 + 2.0 * 6.659364700317383
Epoch 130, val loss: 1.7530256509780884
Epoch 140, training loss: 14.974516868591309 = 1.7245142459869385 + 2.0 * 6.625001430511475
Epoch 140, val loss: 1.7367422580718994
Epoch 150, training loss: 14.898107528686523 = 1.7044605016708374 + 2.0 * 6.596823692321777
Epoch 150, val loss: 1.7189083099365234
Epoch 160, training loss: 14.828243255615234 = 1.6825929880142212 + 2.0 * 6.572824954986572
Epoch 160, val loss: 1.6995865106582642
Epoch 170, training loss: 14.760866165161133 = 1.658385157585144 + 2.0 * 6.55124044418335
Epoch 170, val loss: 1.6783510446548462
Epoch 180, training loss: 14.7027587890625 = 1.631531834602356 + 2.0 * 6.535613536834717
Epoch 180, val loss: 1.655042290687561
Epoch 190, training loss: 14.634941101074219 = 1.6021366119384766 + 2.0 * 6.516402244567871
Epoch 190, val loss: 1.6297858953475952
Epoch 200, training loss: 14.576615333557129 = 1.5699306726455688 + 2.0 * 6.503342151641846
Epoch 200, val loss: 1.6021751165390015
Epoch 210, training loss: 14.513004302978516 = 1.5348597764968872 + 2.0 * 6.489072322845459
Epoch 210, val loss: 1.572251558303833
Epoch 220, training loss: 14.452362060546875 = 1.4969109296798706 + 2.0 * 6.477725505828857
Epoch 220, val loss: 1.5401605367660522
Epoch 230, training loss: 14.400104522705078 = 1.456671118736267 + 2.0 * 6.47171688079834
Epoch 230, val loss: 1.5064960718154907
Epoch 240, training loss: 14.334731101989746 = 1.4148647785186768 + 2.0 * 6.459933280944824
Epoch 240, val loss: 1.4719501733779907
Epoch 250, training loss: 14.273908615112305 = 1.3718514442443848 + 2.0 * 6.451028823852539
Epoch 250, val loss: 1.4369251728057861
Epoch 260, training loss: 14.215063095092773 = 1.3281714916229248 + 2.0 * 6.443445682525635
Epoch 260, val loss: 1.4019145965576172
Epoch 270, training loss: 14.162202835083008 = 1.2841591835021973 + 2.0 * 6.439021587371826
Epoch 270, val loss: 1.3671890497207642
Epoch 280, training loss: 14.101455688476562 = 1.240434169769287 + 2.0 * 6.430510520935059
Epoch 280, val loss: 1.333025336265564
Epoch 290, training loss: 14.044623374938965 = 1.1969777345657349 + 2.0 * 6.42382287979126
Epoch 290, val loss: 1.2996207475662231
Epoch 300, training loss: 13.997079849243164 = 1.1540122032165527 + 2.0 * 6.421533584594727
Epoch 300, val loss: 1.2669280767440796
Epoch 310, training loss: 13.940448760986328 = 1.1120930910110474 + 2.0 * 6.414177894592285
Epoch 310, val loss: 1.2351515293121338
Epoch 320, training loss: 13.90086555480957 = 1.0708458423614502 + 2.0 * 6.41500997543335
Epoch 320, val loss: 1.2043108940124512
Epoch 330, training loss: 13.84153938293457 = 1.0308717489242554 + 2.0 * 6.405333995819092
Epoch 330, val loss: 1.1744529008865356
Epoch 340, training loss: 13.79267406463623 = 0.9918072819709778 + 2.0 * 6.400433540344238
Epoch 340, val loss: 1.1455034017562866
Epoch 350, training loss: 13.748741149902344 = 0.9534586071968079 + 2.0 * 6.397641181945801
Epoch 350, val loss: 1.1172635555267334
Epoch 360, training loss: 13.706428527832031 = 0.9160191416740417 + 2.0 * 6.395204544067383
Epoch 360, val loss: 1.0897789001464844
Epoch 370, training loss: 13.657975196838379 = 0.8794675469398499 + 2.0 * 6.389253616333008
Epoch 370, val loss: 1.0632236003875732
Epoch 380, training loss: 13.617670059204102 = 0.8437740206718445 + 2.0 * 6.386948108673096
Epoch 380, val loss: 1.0374078750610352
Epoch 390, training loss: 13.573553085327148 = 0.8089384436607361 + 2.0 * 6.382307529449463
Epoch 390, val loss: 1.012399673461914
Epoch 400, training loss: 13.537132263183594 = 0.7749752402305603 + 2.0 * 6.381078720092773
Epoch 400, val loss: 0.9880805015563965
Epoch 410, training loss: 13.495197296142578 = 0.7423007488250732 + 2.0 * 6.376448154449463
Epoch 410, val loss: 0.9649174213409424
Epoch 420, training loss: 13.456071853637695 = 0.7106559872627258 + 2.0 * 6.372707843780518
Epoch 420, val loss: 0.9428439736366272
Epoch 430, training loss: 13.430318832397461 = 0.6800578236579895 + 2.0 * 6.375130653381348
Epoch 430, val loss: 0.9217809438705444
Epoch 440, training loss: 13.388870239257812 = 0.6510870456695557 + 2.0 * 6.368891716003418
Epoch 440, val loss: 0.9020695090293884
Epoch 450, training loss: 13.353504180908203 = 0.6235461831092834 + 2.0 * 6.364978790283203
Epoch 450, val loss: 0.8838509917259216
Epoch 460, training loss: 13.31984806060791 = 0.5972724556922913 + 2.0 * 6.361287593841553
Epoch 460, val loss: 0.8669956922531128
Epoch 470, training loss: 13.294854164123535 = 0.5721379518508911 + 2.0 * 6.361358165740967
Epoch 470, val loss: 0.8513714075088501
Epoch 480, training loss: 13.274262428283691 = 0.548214852809906 + 2.0 * 6.36302375793457
Epoch 480, val loss: 0.8368375301361084
Epoch 490, training loss: 13.236592292785645 = 0.5254086852073669 + 2.0 * 6.355591773986816
Epoch 490, val loss: 0.8235913515090942
Epoch 500, training loss: 13.207792282104492 = 0.503596842288971 + 2.0 * 6.352097511291504
Epoch 500, val loss: 0.8114009499549866
Epoch 510, training loss: 13.18359088897705 = 0.4825400412082672 + 2.0 * 6.350525379180908
Epoch 510, val loss: 0.8000669479370117
Epoch 520, training loss: 13.154998779296875 = 0.46225157380104065 + 2.0 * 6.346373558044434
Epoch 520, val loss: 0.7893944978713989
Epoch 530, training loss: 13.133665084838867 = 0.44262826442718506 + 2.0 * 6.345518589019775
Epoch 530, val loss: 0.7795808911323547
Epoch 540, training loss: 13.109210968017578 = 0.4235715866088867 + 2.0 * 6.342819690704346
Epoch 540, val loss: 0.7702530026435852
Epoch 550, training loss: 13.10019302368164 = 0.40501171350479126 + 2.0 * 6.347590446472168
Epoch 550, val loss: 0.7615517973899841
Epoch 560, training loss: 13.067312240600586 = 0.3869624435901642 + 2.0 * 6.340174674987793
Epoch 560, val loss: 0.7534036040306091
Epoch 570, training loss: 13.04965591430664 = 0.3694721758365631 + 2.0 * 6.340091705322266
Epoch 570, val loss: 0.7457410097122192
Epoch 580, training loss: 13.024797439575195 = 0.3523961305618286 + 2.0 * 6.336200714111328
Epoch 580, val loss: 0.7386301159858704
Epoch 590, training loss: 13.000896453857422 = 0.3358953297138214 + 2.0 * 6.332500457763672
Epoch 590, val loss: 0.732099175453186
Epoch 600, training loss: 12.980279922485352 = 0.31982117891311646 + 2.0 * 6.33022928237915
Epoch 600, val loss: 0.7260648608207703
Epoch 610, training loss: 12.997299194335938 = 0.3043086528778076 + 2.0 * 6.346495151519775
Epoch 610, val loss: 0.720517098903656
Epoch 620, training loss: 12.951857566833496 = 0.2893645465373993 + 2.0 * 6.331246376037598
Epoch 620, val loss: 0.7155901193618774
Epoch 630, training loss: 12.9278564453125 = 0.27508386969566345 + 2.0 * 6.326386451721191
Epoch 630, val loss: 0.7112095952033997
Epoch 640, training loss: 12.908116340637207 = 0.26139482855796814 + 2.0 * 6.323360919952393
Epoch 640, val loss: 0.7074297070503235
Epoch 650, training loss: 12.895211219787598 = 0.24826548993587494 + 2.0 * 6.32347297668457
Epoch 650, val loss: 0.7041335701942444
Epoch 660, training loss: 12.8784761428833 = 0.23575147986412048 + 2.0 * 6.321362495422363
Epoch 660, val loss: 0.7011542320251465
Epoch 670, training loss: 12.86327838897705 = 0.2238912731409073 + 2.0 * 6.319693565368652
Epoch 670, val loss: 0.6987996697425842
Epoch 680, training loss: 12.84720230102539 = 0.21261009573936462 + 2.0 * 6.317296028137207
Epoch 680, val loss: 0.6969124674797058
Epoch 690, training loss: 12.850561141967773 = 0.2019294947385788 + 2.0 * 6.324316024780273
Epoch 690, val loss: 0.6955127716064453
Epoch 700, training loss: 12.827186584472656 = 0.19179514050483704 + 2.0 * 6.317695617675781
Epoch 700, val loss: 0.694365918636322
Epoch 710, training loss: 12.809412002563477 = 0.18225140869617462 + 2.0 * 6.313580513000488
Epoch 710, val loss: 0.6937224268913269
Epoch 720, training loss: 12.800202369689941 = 0.1732378453016281 + 2.0 * 6.313482284545898
Epoch 720, val loss: 0.693474292755127
Epoch 730, training loss: 12.786934852600098 = 0.16471275687217712 + 2.0 * 6.311110973358154
Epoch 730, val loss: 0.693609356880188
Epoch 740, training loss: 12.776461601257324 = 0.15667563676834106 + 2.0 * 6.3098931312561035
Epoch 740, val loss: 0.6939578652381897
Epoch 750, training loss: 12.76589584350586 = 0.1490689218044281 + 2.0 * 6.308413505554199
Epoch 750, val loss: 0.6946988701820374
Epoch 760, training loss: 12.769207954406738 = 0.14188827574253082 + 2.0 * 6.31365966796875
Epoch 760, val loss: 0.6957301497459412
Epoch 770, training loss: 12.751762390136719 = 0.13511481881141663 + 2.0 * 6.308323860168457
Epoch 770, val loss: 0.6969872117042542
Epoch 780, training loss: 12.737683296203613 = 0.12870970368385315 + 2.0 * 6.3044867515563965
Epoch 780, val loss: 0.6984819173812866
Epoch 790, training loss: 12.736652374267578 = 0.12266477197408676 + 2.0 * 6.3069939613342285
Epoch 790, val loss: 0.7002701163291931
Epoch 800, training loss: 12.72193717956543 = 0.11694997549057007 + 2.0 * 6.302493572235107
Epoch 800, val loss: 0.7022757530212402
Epoch 810, training loss: 12.7141695022583 = 0.1115679070353508 + 2.0 * 6.301301002502441
Epoch 810, val loss: 0.7044552564620972
Epoch 820, training loss: 12.71423053741455 = 0.10647109150886536 + 2.0 * 6.303879737854004
Epoch 820, val loss: 0.7068993449211121
Epoch 830, training loss: 12.706408500671387 = 0.10166234523057938 + 2.0 * 6.302372932434082
Epoch 830, val loss: 0.7094579339027405
Epoch 840, training loss: 12.692183494567871 = 0.09712998569011688 + 2.0 * 6.297526836395264
Epoch 840, val loss: 0.7121802568435669
Epoch 850, training loss: 12.689105987548828 = 0.0928393080830574 + 2.0 * 6.298133373260498
Epoch 850, val loss: 0.7150532007217407
Epoch 860, training loss: 12.680974960327148 = 0.08877777308225632 + 2.0 * 6.296098709106445
Epoch 860, val loss: 0.7182005047798157
Epoch 870, training loss: 12.67921257019043 = 0.08494944125413895 + 2.0 * 6.297131538391113
Epoch 870, val loss: 0.7212182283401489
Epoch 880, training loss: 12.667884826660156 = 0.08133379369974136 + 2.0 * 6.293275356292725
Epoch 880, val loss: 0.7244853973388672
Epoch 890, training loss: 12.66482925415039 = 0.07789719104766846 + 2.0 * 6.293466091156006
Epoch 890, val loss: 0.727959930896759
Epoch 900, training loss: 12.666190147399902 = 0.07463566958904266 + 2.0 * 6.295777320861816
Epoch 900, val loss: 0.7315144538879395
Epoch 910, training loss: 12.655229568481445 = 0.07156562805175781 + 2.0 * 6.291831970214844
Epoch 910, val loss: 0.7350493669509888
Epoch 920, training loss: 12.64956283569336 = 0.068666972219944 + 2.0 * 6.29044771194458
Epoch 920, val loss: 0.7386255860328674
Epoch 930, training loss: 12.643365859985352 = 0.06590408831834793 + 2.0 * 6.288731098175049
Epoch 930, val loss: 0.7424485683441162
Epoch 940, training loss: 12.646059036254883 = 0.06328170746564865 + 2.0 * 6.291388511657715
Epoch 940, val loss: 0.7463037371635437
Epoch 950, training loss: 12.643346786499023 = 0.06079490855336189 + 2.0 * 6.291275978088379
Epoch 950, val loss: 0.7502896785736084
Epoch 960, training loss: 12.635321617126465 = 0.05844280496239662 + 2.0 * 6.2884392738342285
Epoch 960, val loss: 0.7540952563285828
Epoch 970, training loss: 12.628314018249512 = 0.05621041730046272 + 2.0 * 6.2860517501831055
Epoch 970, val loss: 0.7580482959747314
Epoch 980, training loss: 12.623164176940918 = 0.054091133177280426 + 2.0 * 6.284536361694336
Epoch 980, val loss: 0.7620787024497986
Epoch 990, training loss: 12.627008438110352 = 0.052074138075113297 + 2.0 * 6.287467002868652
Epoch 990, val loss: 0.7661828398704529
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.788888888888889
0.8386926726410122
=== training gcn model ===
Epoch 0, training loss: 19.144681930541992 = 1.9509721994400024 + 2.0 * 8.596855163574219
Epoch 0, val loss: 1.9500513076782227
Epoch 10, training loss: 19.13353157043457 = 1.9403712749481201 + 2.0 * 8.596580505371094
Epoch 10, val loss: 1.9397605657577515
Epoch 20, training loss: 19.115915298461914 = 1.927457571029663 + 2.0 * 8.594228744506836
Epoch 20, val loss: 1.9267919063568115
Epoch 30, training loss: 19.06005859375 = 1.910050868988037 + 2.0 * 8.575003623962402
Epoch 30, val loss: 1.9089174270629883
Epoch 40, training loss: 18.78879165649414 = 1.8881317377090454 + 2.0 * 8.450329780578613
Epoch 40, val loss: 1.886949896812439
Epoch 50, training loss: 17.89400291442871 = 1.865568995475769 + 2.0 * 8.014217376708984
Epoch 50, val loss: 1.865325927734375
Epoch 60, training loss: 16.92042350769043 = 1.847379446029663 + 2.0 * 7.536521911621094
Epoch 60, val loss: 1.8483099937438965
Epoch 70, training loss: 16.18791389465332 = 1.8326215744018555 + 2.0 * 7.177646160125732
Epoch 70, val loss: 1.834240198135376
Epoch 80, training loss: 15.893527030944824 = 1.818082571029663 + 2.0 * 7.037722110748291
Epoch 80, val loss: 1.820173978805542
Epoch 90, training loss: 15.590065002441406 = 1.8023123741149902 + 2.0 * 6.893876075744629
Epoch 90, val loss: 1.8050974607467651
Epoch 100, training loss: 15.366655349731445 = 1.7874722480773926 + 2.0 * 6.7895917892456055
Epoch 100, val loss: 1.7908198833465576
Epoch 110, training loss: 15.190852165222168 = 1.773526906967163 + 2.0 * 6.708662509918213
Epoch 110, val loss: 1.7775673866271973
Epoch 120, training loss: 15.06396198272705 = 1.7589561939239502 + 2.0 * 6.65250301361084
Epoch 120, val loss: 1.7639888525009155
Epoch 130, training loss: 14.960372924804688 = 1.7431750297546387 + 2.0 * 6.608598709106445
Epoch 130, val loss: 1.7498106956481934
Epoch 140, training loss: 14.87504768371582 = 1.725996494293213 + 2.0 * 6.574525833129883
Epoch 140, val loss: 1.7345991134643555
Epoch 150, training loss: 14.806598663330078 = 1.7071826457977295 + 2.0 * 6.549707889556885
Epoch 150, val loss: 1.7181316614151
Epoch 160, training loss: 14.735687255859375 = 1.6862906217575073 + 2.0 * 6.524698257446289
Epoch 160, val loss: 1.7000994682312012
Epoch 170, training loss: 14.673046112060547 = 1.6628825664520264 + 2.0 * 6.505081653594971
Epoch 170, val loss: 1.6800884008407593
Epoch 180, training loss: 14.620944023132324 = 1.636535882949829 + 2.0 * 6.492204189300537
Epoch 180, val loss: 1.6576300859451294
Epoch 190, training loss: 14.5578031539917 = 1.6070165634155273 + 2.0 * 6.475393295288086
Epoch 190, val loss: 1.6327414512634277
Epoch 200, training loss: 14.497865676879883 = 1.574297547340393 + 2.0 * 6.4617838859558105
Epoch 200, val loss: 1.6050838232040405
Epoch 210, training loss: 14.438952445983887 = 1.538068413734436 + 2.0 * 6.450441837310791
Epoch 210, val loss: 1.5746698379516602
Epoch 220, training loss: 14.389159202575684 = 1.498407006263733 + 2.0 * 6.445375919342041
Epoch 220, val loss: 1.541496992111206
Epoch 230, training loss: 14.32140064239502 = 1.4557826519012451 + 2.0 * 6.432808876037598
Epoch 230, val loss: 1.5061657428741455
Epoch 240, training loss: 14.260418891906738 = 1.4107609987258911 + 2.0 * 6.424829006195068
Epoch 240, val loss: 1.4691087007522583
Epoch 250, training loss: 14.203121185302734 = 1.3636963367462158 + 2.0 * 6.419712543487549
Epoch 250, val loss: 1.4307870864868164
Epoch 260, training loss: 14.140361785888672 = 1.3157285451889038 + 2.0 * 6.412316799163818
Epoch 260, val loss: 1.392217755317688
Epoch 270, training loss: 14.078697204589844 = 1.2675472497940063 + 2.0 * 6.405574798583984
Epoch 270, val loss: 1.3539600372314453
Epoch 280, training loss: 14.025442123413086 = 1.2198567390441895 + 2.0 * 6.402792453765869
Epoch 280, val loss: 1.316688895225525
Epoch 290, training loss: 13.968199729919434 = 1.1735259294509888 + 2.0 * 6.397336959838867
Epoch 290, val loss: 1.2813502550125122
Epoch 300, training loss: 13.91186809539795 = 1.1292133331298828 + 2.0 * 6.391327381134033
Epoch 300, val loss: 1.2481577396392822
Epoch 310, training loss: 13.859827041625977 = 1.0868679285049438 + 2.0 * 6.386479377746582
Epoch 310, val loss: 1.2172428369522095
Epoch 320, training loss: 13.828733444213867 = 1.0465716123580933 + 2.0 * 6.391080856323242
Epoch 320, val loss: 1.1885900497436523
Epoch 330, training loss: 13.766637802124023 = 1.0086702108383179 + 2.0 * 6.378983974456787
Epoch 330, val loss: 1.1623624563217163
Epoch 340, training loss: 13.723746299743652 = 0.9730024933815002 + 2.0 * 6.375371932983398
Epoch 340, val loss: 1.1383318901062012
Epoch 350, training loss: 13.683281898498535 = 0.9392058849334717 + 2.0 * 6.372037887573242
Epoch 350, val loss: 1.1162617206573486
Epoch 360, training loss: 13.650618553161621 = 0.9070650935173035 + 2.0 * 6.371776580810547
Epoch 360, val loss: 1.0958747863769531
Epoch 370, training loss: 13.612703323364258 = 0.8766322731971741 + 2.0 * 6.368035316467285
Epoch 370, val loss: 1.0769439935684204
Epoch 380, training loss: 13.57540225982666 = 0.8478532433509827 + 2.0 * 6.363774299621582
Epoch 380, val loss: 1.0595779418945312
Epoch 390, training loss: 13.542978286743164 = 0.8205010294914246 + 2.0 * 6.361238479614258
Epoch 390, val loss: 1.0435256958007812
Epoch 400, training loss: 13.517660140991211 = 0.7943939566612244 + 2.0 * 6.36163330078125
Epoch 400, val loss: 1.0286396741867065
Epoch 410, training loss: 13.486125946044922 = 0.769505500793457 + 2.0 * 6.358310222625732
Epoch 410, val loss: 1.014917016029358
Epoch 420, training loss: 13.4531888961792 = 0.7458732724189758 + 2.0 * 6.3536577224731445
Epoch 420, val loss: 1.0022993087768555
Epoch 430, training loss: 13.425910949707031 = 0.7231964468955994 + 2.0 * 6.351357460021973
Epoch 430, val loss: 0.9907868504524231
Epoch 440, training loss: 13.408008575439453 = 0.7013928294181824 + 2.0 * 6.353307723999023
Epoch 440, val loss: 0.9802218675613403
Epoch 450, training loss: 13.374618530273438 = 0.6803745627403259 + 2.0 * 6.3471221923828125
Epoch 450, val loss: 0.9705294966697693
Epoch 460, training loss: 13.349446296691895 = 0.6600656509399414 + 2.0 * 6.344690322875977
Epoch 460, val loss: 0.9616236686706543
Epoch 470, training loss: 13.325030326843262 = 0.6402972936630249 + 2.0 * 6.342366695404053
Epoch 470, val loss: 0.953369140625
Epoch 480, training loss: 13.305998802185059 = 0.6209189295768738 + 2.0 * 6.3425397872924805
Epoch 480, val loss: 0.9456605911254883
Epoch 490, training loss: 13.287543296813965 = 0.601976752281189 + 2.0 * 6.342783451080322
Epoch 490, val loss: 0.9385256767272949
Epoch 500, training loss: 13.259634017944336 = 0.5834805369377136 + 2.0 * 6.338076591491699
Epoch 500, val loss: 0.931835949420929
Epoch 510, training loss: 13.236059188842773 = 0.5653786659240723 + 2.0 * 6.3353400230407715
Epoch 510, val loss: 0.9256569147109985
Epoch 520, training loss: 13.21374797821045 = 0.5475996732711792 + 2.0 * 6.33307409286499
Epoch 520, val loss: 0.9199188351631165
Epoch 530, training loss: 13.193069458007812 = 0.5300779938697815 + 2.0 * 6.331495761871338
Epoch 530, val loss: 0.9146302342414856
Epoch 540, training loss: 13.172675132751465 = 0.5128424763679504 + 2.0 * 6.329916477203369
Epoch 540, val loss: 0.909650981426239
Epoch 550, training loss: 13.154102325439453 = 0.4959721267223358 + 2.0 * 6.329065322875977
Epoch 550, val loss: 0.9050319194793701
Epoch 560, training loss: 13.14797306060791 = 0.47937023639678955 + 2.0 * 6.334301471710205
Epoch 560, val loss: 0.9009301662445068
Epoch 570, training loss: 13.118544578552246 = 0.46319127082824707 + 2.0 * 6.327676773071289
Epoch 570, val loss: 0.8969584703445435
Epoch 580, training loss: 13.095220565795898 = 0.44727662205696106 + 2.0 * 6.323971748352051
Epoch 580, val loss: 0.8934852480888367
Epoch 590, training loss: 13.075431823730469 = 0.43159011006355286 + 2.0 * 6.321920871734619
Epoch 590, val loss: 0.8904569745063782
Epoch 600, training loss: 13.059242248535156 = 0.4160955846309662 + 2.0 * 6.321573257446289
Epoch 600, val loss: 0.8877492547035217
Epoch 610, training loss: 13.041953086853027 = 0.40082621574401855 + 2.0 * 6.320563316345215
Epoch 610, val loss: 0.8850725293159485
Epoch 620, training loss: 13.023397445678711 = 0.3858426511287689 + 2.0 * 6.318777561187744
Epoch 620, val loss: 0.8828855752944946
Epoch 630, training loss: 13.004707336425781 = 0.37109285593032837 + 2.0 * 6.316807270050049
Epoch 630, val loss: 0.8808812499046326
Epoch 640, training loss: 12.985718727111816 = 0.3565499782562256 + 2.0 * 6.314584255218506
Epoch 640, val loss: 0.8791900873184204
Epoch 650, training loss: 13.001456260681152 = 0.342234343290329 + 2.0 * 6.329610824584961
Epoch 650, val loss: 0.877702534198761
Epoch 660, training loss: 12.9541597366333 = 0.3281494677066803 + 2.0 * 6.313004970550537
Epoch 660, val loss: 0.8765174150466919
Epoch 670, training loss: 12.937644004821777 = 0.3144395053386688 + 2.0 * 6.3116021156311035
Epoch 670, val loss: 0.8754880428314209
Epoch 680, training loss: 12.920939445495605 = 0.3010219633579254 + 2.0 * 6.3099589347839355
Epoch 680, val loss: 0.8749059438705444
Epoch 690, training loss: 12.904424667358398 = 0.2878575921058655 + 2.0 * 6.30828332901001
Epoch 690, val loss: 0.8745800256729126
Epoch 700, training loss: 12.90877628326416 = 0.2749945819377899 + 2.0 * 6.316890716552734
Epoch 700, val loss: 0.8743752241134644
Epoch 710, training loss: 12.885005950927734 = 0.26250430941581726 + 2.0 * 6.311250686645508
Epoch 710, val loss: 0.8744781613349915
Epoch 720, training loss: 12.861977577209473 = 0.2504735291004181 + 2.0 * 6.305751800537109
Epoch 720, val loss: 0.8748605251312256
Epoch 730, training loss: 12.849142074584961 = 0.23882576823234558 + 2.0 * 6.3051581382751465
Epoch 730, val loss: 0.8755583167076111
Epoch 740, training loss: 12.834875106811523 = 0.2275564968585968 + 2.0 * 6.303659439086914
Epoch 740, val loss: 0.876591682434082
Epoch 750, training loss: 12.822395324707031 = 0.21669328212738037 + 2.0 * 6.30285120010376
Epoch 750, val loss: 0.878021776676178
Epoch 760, training loss: 12.812725067138672 = 0.20625311136245728 + 2.0 * 6.30323600769043
Epoch 760, val loss: 0.8795937895774841
Epoch 770, training loss: 12.79808521270752 = 0.19627204537391663 + 2.0 * 6.300906658172607
Epoch 770, val loss: 0.8814584016799927
Epoch 780, training loss: 12.7857084274292 = 0.1867058426141739 + 2.0 * 6.299501419067383
Epoch 780, val loss: 0.8837352395057678
Epoch 790, training loss: 12.791849136352539 = 0.17758718132972717 + 2.0 * 6.307130813598633
Epoch 790, val loss: 0.8862789869308472
Epoch 800, training loss: 12.764031410217285 = 0.16888552904129028 + 2.0 * 6.297573089599609
Epoch 800, val loss: 0.8892083168029785
Epoch 810, training loss: 12.752276420593262 = 0.16064204275608063 + 2.0 * 6.2958173751831055
Epoch 810, val loss: 0.8924700617790222
Epoch 820, training loss: 12.742263793945312 = 0.152801975607872 + 2.0 * 6.294731140136719
Epoch 820, val loss: 0.8961472511291504
Epoch 830, training loss: 12.755857467651367 = 0.1453423649072647 + 2.0 * 6.305257320404053
Epoch 830, val loss: 0.9001187086105347
Epoch 840, training loss: 12.726319313049316 = 0.13827738165855408 + 2.0 * 6.294021129608154
Epoch 840, val loss: 0.9041022658348083
Epoch 850, training loss: 12.716960906982422 = 0.13161084055900574 + 2.0 * 6.292675018310547
Epoch 850, val loss: 0.9084809422492981
Epoch 860, training loss: 12.707598686218262 = 0.1252938061952591 + 2.0 * 6.291152477264404
Epoch 860, val loss: 0.9132161736488342
Epoch 870, training loss: 12.711231231689453 = 0.11930755525827408 + 2.0 * 6.295961856842041
Epoch 870, val loss: 0.9181198477745056
Epoch 880, training loss: 12.702524185180664 = 0.11363282799720764 + 2.0 * 6.294445514678955
Epoch 880, val loss: 0.923209011554718
Epoch 890, training loss: 12.685966491699219 = 0.10827329754829407 + 2.0 * 6.288846492767334
Epoch 890, val loss: 0.9285553097724915
Epoch 900, training loss: 12.679814338684082 = 0.10320691019296646 + 2.0 * 6.288303852081299
Epoch 900, val loss: 0.9341058731079102
Epoch 910, training loss: 12.680843353271484 = 0.0984174981713295 + 2.0 * 6.291213035583496
Epoch 910, val loss: 0.9398320317268372
Epoch 920, training loss: 12.669496536254883 = 0.09390443563461304 + 2.0 * 6.2877960205078125
Epoch 920, val loss: 0.9453932046890259
Epoch 930, training loss: 12.667169570922852 = 0.08963041752576828 + 2.0 * 6.288769721984863
Epoch 930, val loss: 0.9514509439468384
Epoch 940, training loss: 12.658935546875 = 0.08559667319059372 + 2.0 * 6.2866692543029785
Epoch 940, val loss: 0.9574921727180481
Epoch 950, training loss: 12.64926528930664 = 0.08177757263183594 + 2.0 * 6.283743858337402
Epoch 950, val loss: 0.9636507630348206
Epoch 960, training loss: 12.644305229187012 = 0.07817479968070984 + 2.0 * 6.283065319061279
Epoch 960, val loss: 0.969810426235199
Epoch 970, training loss: 12.657931327819824 = 0.07476314157247543 + 2.0 * 6.291584014892578
Epoch 970, val loss: 0.9761612415313721
Epoch 980, training loss: 12.638531684875488 = 0.0715460479259491 + 2.0 * 6.2834930419921875
Epoch 980, val loss: 0.9819890856742859
Epoch 990, training loss: 12.630671501159668 = 0.0685107633471489 + 2.0 * 6.28108024597168
Epoch 990, val loss: 0.9883585572242737
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7851851851851852
0.8355297838692674
=== training gcn model ===
Epoch 0, training loss: 19.1405029296875 = 1.9467748403549194 + 2.0 * 8.596863746643066
Epoch 0, val loss: 1.9377832412719727
Epoch 10, training loss: 19.13037109375 = 1.9370884895324707 + 2.0 * 8.596641540527344
Epoch 10, val loss: 1.928755283355713
Epoch 20, training loss: 19.115015029907227 = 1.9254448413848877 + 2.0 * 8.5947847366333
Epoch 20, val loss: 1.9176043272018433
Epoch 30, training loss: 19.068283081054688 = 1.9097983837127686 + 2.0 * 8.579242706298828
Epoch 30, val loss: 1.9024136066436768
Epoch 40, training loss: 18.83785057067871 = 1.8898355960845947 + 2.0 * 8.474007606506348
Epoch 40, val loss: 1.883242130279541
Epoch 50, training loss: 17.90068244934082 = 1.8668495416641235 + 2.0 * 8.016916275024414
Epoch 50, val loss: 1.8613579273223877
Epoch 60, training loss: 17.189708709716797 = 1.8468594551086426 + 2.0 * 7.671424865722656
Epoch 60, val loss: 1.8436505794525146
Epoch 70, training loss: 16.76731300354004 = 1.8324319124221802 + 2.0 * 7.467440128326416
Epoch 70, val loss: 1.8310704231262207
Epoch 80, training loss: 16.238554000854492 = 1.8186134099960327 + 2.0 * 7.209969997406006
Epoch 80, val loss: 1.8188880681991577
Epoch 90, training loss: 15.778863906860352 = 1.8052048683166504 + 2.0 * 6.98682975769043
Epoch 90, val loss: 1.806725263595581
Epoch 100, training loss: 15.534846305847168 = 1.7902822494506836 + 2.0 * 6.872282028198242
Epoch 100, val loss: 1.79355788230896
Epoch 110, training loss: 15.378581047058105 = 1.7740317583084106 + 2.0 * 6.802274703979492
Epoch 110, val loss: 1.779631495475769
Epoch 120, training loss: 15.249691009521484 = 1.7573368549346924 + 2.0 * 6.7461771965026855
Epoch 120, val loss: 1.765458106994629
Epoch 130, training loss: 15.144512176513672 = 1.739877700805664 + 2.0 * 6.702317237854004
Epoch 130, val loss: 1.7501471042633057
Epoch 140, training loss: 15.056087493896484 = 1.7203119993209839 + 2.0 * 6.6678876876831055
Epoch 140, val loss: 1.732850193977356
Epoch 150, training loss: 14.970016479492188 = 1.6983425617218018 + 2.0 * 6.635837078094482
Epoch 150, val loss: 1.7133489847183228
Epoch 160, training loss: 14.897611618041992 = 1.6734466552734375 + 2.0 * 6.612082481384277
Epoch 160, val loss: 1.6912999153137207
Epoch 170, training loss: 14.822360038757324 = 1.6451005935668945 + 2.0 * 6.588629722595215
Epoch 170, val loss: 1.6665256023406982
Epoch 180, training loss: 14.751216888427734 = 1.6131024360656738 + 2.0 * 6.569057464599609
Epoch 180, val loss: 1.6386481523513794
Epoch 190, training loss: 14.689573287963867 = 1.5775583982467651 + 2.0 * 6.556007385253906
Epoch 190, val loss: 1.607831597328186
Epoch 200, training loss: 14.612504959106445 = 1.5387957096099854 + 2.0 * 6.5368547439575195
Epoch 200, val loss: 1.5745632648468018
Epoch 210, training loss: 14.54353141784668 = 1.4972503185272217 + 2.0 * 6.5231404304504395
Epoch 210, val loss: 1.5391703844070435
Epoch 220, training loss: 14.480757713317871 = 1.4536443948745728 + 2.0 * 6.513556480407715
Epoch 220, val loss: 1.5026476383209229
Epoch 230, training loss: 14.406023025512695 = 1.4096760749816895 + 2.0 * 6.498173713684082
Epoch 230, val loss: 1.466232419013977
Epoch 240, training loss: 14.341913223266602 = 1.365729570388794 + 2.0 * 6.488091945648193
Epoch 240, val loss: 1.4306142330169678
Epoch 250, training loss: 14.280190467834473 = 1.3224064111709595 + 2.0 * 6.478891849517822
Epoch 250, val loss: 1.3962854146957397
Epoch 260, training loss: 14.219914436340332 = 1.2805110216140747 + 2.0 * 6.469701766967773
Epoch 260, val loss: 1.3635165691375732
Epoch 270, training loss: 14.160972595214844 = 1.2397687435150146 + 2.0 * 6.460601806640625
Epoch 270, val loss: 1.3320608139038086
Epoch 280, training loss: 14.106134414672852 = 1.1998275518417358 + 2.0 * 6.453153610229492
Epoch 280, val loss: 1.3016465902328491
Epoch 290, training loss: 14.059347152709961 = 1.160468339920044 + 2.0 * 6.449439525604248
Epoch 290, val loss: 1.2719995975494385
Epoch 300, training loss: 14.003965377807617 = 1.121748447418213 + 2.0 * 6.441108226776123
Epoch 300, val loss: 1.2430814504623413
Epoch 310, training loss: 13.952716827392578 = 1.0834903717041016 + 2.0 * 6.434613227844238
Epoch 310, val loss: 1.2148425579071045
Epoch 320, training loss: 13.917718887329102 = 1.045352578163147 + 2.0 * 6.436182975769043
Epoch 320, val loss: 1.1870044469833374
Epoch 330, training loss: 13.853940963745117 = 1.0076662302017212 + 2.0 * 6.423137187957764
Epoch 330, val loss: 1.1595185995101929
Epoch 340, training loss: 13.805688858032227 = 0.970122218132019 + 2.0 * 6.417783260345459
Epoch 340, val loss: 1.1324830055236816
Epoch 350, training loss: 13.758895874023438 = 0.9327513575553894 + 2.0 * 6.413072109222412
Epoch 350, val loss: 1.1060113906860352
Epoch 360, training loss: 13.714897155761719 = 0.8959181904792786 + 2.0 * 6.409489631652832
Epoch 360, val loss: 1.0802891254425049
Epoch 370, training loss: 13.667950630187988 = 0.8599341511726379 + 2.0 * 6.404008388519287
Epoch 370, val loss: 1.0554471015930176
Epoch 380, training loss: 13.622451782226562 = 0.8248788118362427 + 2.0 * 6.398786544799805
Epoch 380, val loss: 1.0317137241363525
Epoch 390, training loss: 13.59888744354248 = 0.790929913520813 + 2.0 * 6.4039788246154785
Epoch 390, val loss: 1.0092121362686157
Epoch 400, training loss: 13.547865867614746 = 0.7584375739097595 + 2.0 * 6.39471435546875
Epoch 400, val loss: 0.988274335861206
Epoch 410, training loss: 13.503094673156738 = 0.7276412844657898 + 2.0 * 6.387726783752441
Epoch 410, val loss: 0.9689109325408936
Epoch 420, training loss: 13.463883399963379 = 0.6983556747436523 + 2.0 * 6.382763862609863
Epoch 420, val loss: 0.9513993859291077
Epoch 430, training loss: 13.438772201538086 = 0.670593798160553 + 2.0 * 6.38408899307251
Epoch 430, val loss: 0.9356066584587097
Epoch 440, training loss: 13.402543067932129 = 0.6445836424827576 + 2.0 * 6.378979682922363
Epoch 440, val loss: 0.9213590025901794
Epoch 450, training loss: 13.367435455322266 = 0.6200270652770996 + 2.0 * 6.373704433441162
Epoch 450, val loss: 0.9089518785476685
Epoch 460, training loss: 13.336507797241211 = 0.5969364047050476 + 2.0 * 6.369785785675049
Epoch 460, val loss: 0.8980104923248291
Epoch 470, training loss: 13.311225891113281 = 0.5750365257263184 + 2.0 * 6.3680949211120605
Epoch 470, val loss: 0.8884865641593933
Epoch 480, training loss: 13.293206214904785 = 0.554333508014679 + 2.0 * 6.369436264038086
Epoch 480, val loss: 0.8801278471946716
Epoch 490, training loss: 13.25665283203125 = 0.5348367691040039 + 2.0 * 6.360908031463623
Epoch 490, val loss: 0.8727819323539734
Epoch 500, training loss: 13.233814239501953 = 0.5161972045898438 + 2.0 * 6.358808517456055
Epoch 500, val loss: 0.8663313388824463
Epoch 510, training loss: 13.209737777709961 = 0.4982091188430786 + 2.0 * 6.355764389038086
Epoch 510, val loss: 0.8606046438217163
Epoch 520, training loss: 13.189705848693848 = 0.48074403405189514 + 2.0 * 6.354480743408203
Epoch 520, val loss: 0.8554700016975403
Epoch 530, training loss: 13.172798156738281 = 0.46379929780960083 + 2.0 * 6.354499340057373
Epoch 530, val loss: 0.8506180644035339
Epoch 540, training loss: 13.14416790008545 = 0.44716283679008484 + 2.0 * 6.3485026359558105
Epoch 540, val loss: 0.8462247848510742
Epoch 550, training loss: 13.1235933303833 = 0.4307124614715576 + 2.0 * 6.346440315246582
Epoch 550, val loss: 0.8421157598495483
Epoch 560, training loss: 13.121526718139648 = 0.4143703877925873 + 2.0 * 6.353578090667725
Epoch 560, val loss: 0.838045060634613
Epoch 570, training loss: 13.083491325378418 = 0.3981977105140686 + 2.0 * 6.342646598815918
Epoch 570, val loss: 0.8342267870903015
Epoch 580, training loss: 13.063176155090332 = 0.3821354806423187 + 2.0 * 6.34052038192749
Epoch 580, val loss: 0.8305124044418335
Epoch 590, training loss: 13.042631149291992 = 0.3661983609199524 + 2.0 * 6.338216304779053
Epoch 590, val loss: 0.8272102475166321
Epoch 600, training loss: 13.03784465789795 = 0.3503299653530121 + 2.0 * 6.343757152557373
Epoch 600, val loss: 0.8242347240447998
Epoch 610, training loss: 13.01154613494873 = 0.3347257971763611 + 2.0 * 6.338410377502441
Epoch 610, val loss: 0.8214773535728455
Epoch 620, training loss: 12.986379623413086 = 0.3194020092487335 + 2.0 * 6.333488941192627
Epoch 620, val loss: 0.8190662264823914
Epoch 630, training loss: 12.968130111694336 = 0.3044110834598541 + 2.0 * 6.331859588623047
Epoch 630, val loss: 0.8171553015708923
Epoch 640, training loss: 12.971096992492676 = 0.2898723781108856 + 2.0 * 6.340612411499023
Epoch 640, val loss: 0.8157703876495361
Epoch 650, training loss: 12.940261840820312 = 0.27574652433395386 + 2.0 * 6.3322577476501465
Epoch 650, val loss: 0.8148676156997681
Epoch 660, training loss: 12.915281295776367 = 0.2622864842414856 + 2.0 * 6.326497554779053
Epoch 660, val loss: 0.8142999410629272
Epoch 670, training loss: 12.897902488708496 = 0.24934421479701996 + 2.0 * 6.324279308319092
Epoch 670, val loss: 0.8142792582511902
Epoch 680, training loss: 12.88382625579834 = 0.23697280883789062 + 2.0 * 6.323426723480225
Epoch 680, val loss: 0.8147065043449402
Epoch 690, training loss: 12.88338565826416 = 0.22514905035495758 + 2.0 * 6.329118251800537
Epoch 690, val loss: 0.8153803944587708
Epoch 700, training loss: 12.865730285644531 = 0.21396003663539886 + 2.0 * 6.32588529586792
Epoch 700, val loss: 0.816457211971283
Epoch 710, training loss: 12.842066764831543 = 0.20340053737163544 + 2.0 * 6.319333076477051
Epoch 710, val loss: 0.8178381323814392
Epoch 720, training loss: 12.83028507232666 = 0.19339653849601746 + 2.0 * 6.31844425201416
Epoch 720, val loss: 0.8194915652275085
Epoch 730, training loss: 12.826920509338379 = 0.18392738699913025 + 2.0 * 6.321496486663818
Epoch 730, val loss: 0.8215307593345642
Epoch 740, training loss: 12.809070587158203 = 0.17497161030769348 + 2.0 * 6.317049503326416
Epoch 740, val loss: 0.8238617777824402
Epoch 750, training loss: 12.807538032531738 = 0.1665264219045639 + 2.0 * 6.320505619049072
Epoch 750, val loss: 0.8262269496917725
Epoch 760, training loss: 12.786320686340332 = 0.15849712491035461 + 2.0 * 6.3139119148254395
Epoch 760, val loss: 0.8288502097129822
Epoch 770, training loss: 12.77285099029541 = 0.1509491503238678 + 2.0 * 6.310950756072998
Epoch 770, val loss: 0.831703782081604
Epoch 780, training loss: 12.779461860656738 = 0.1437905877828598 + 2.0 * 6.317835807800293
Epoch 780, val loss: 0.8349012732505798
Epoch 790, training loss: 12.754186630249023 = 0.1370076835155487 + 2.0 * 6.308589458465576
Epoch 790, val loss: 0.8380486965179443
Epoch 800, training loss: 12.745019912719727 = 0.1305949091911316 + 2.0 * 6.3072123527526855
Epoch 800, val loss: 0.8414604663848877
Epoch 810, training loss: 12.737870216369629 = 0.12451833486557007 + 2.0 * 6.306675910949707
Epoch 810, val loss: 0.8451102375984192
Epoch 820, training loss: 12.745800971984863 = 0.11875856667757034 + 2.0 * 6.313521385192871
Epoch 820, val loss: 0.8489232659339905
Epoch 830, training loss: 12.722335815429688 = 0.11327338218688965 + 2.0 * 6.304531097412109
Epoch 830, val loss: 0.8527618646621704
Epoch 840, training loss: 12.714287757873535 = 0.10810898244380951 + 2.0 * 6.303089618682861
Epoch 840, val loss: 0.8567343354225159
Epoch 850, training loss: 12.706320762634277 = 0.10321567952632904 + 2.0 * 6.301552772521973
Epoch 850, val loss: 0.8611065149307251
Epoch 860, training loss: 12.716209411621094 = 0.0985572338104248 + 2.0 * 6.308825969696045
Epoch 860, val loss: 0.8655443787574768
Epoch 870, training loss: 12.697807312011719 = 0.0941532701253891 + 2.0 * 6.3018269538879395
Epoch 870, val loss: 0.8697752952575684
Epoch 880, training loss: 12.6868314743042 = 0.08999039232730865 + 2.0 * 6.298420429229736
Epoch 880, val loss: 0.8743008971214294
Epoch 890, training loss: 12.680241584777832 = 0.08604463189840317 + 2.0 * 6.297098636627197
Epoch 890, val loss: 0.879062831401825
Epoch 900, training loss: 12.698901176452637 = 0.08229928463697433 + 2.0 * 6.308300971984863
Epoch 900, val loss: 0.8838104605674744
Epoch 910, training loss: 12.677775382995605 = 0.0787699744105339 + 2.0 * 6.299502849578857
Epoch 910, val loss: 0.8885828256607056
Epoch 920, training loss: 12.665245056152344 = 0.07541254162788391 + 2.0 * 6.294916152954102
Epoch 920, val loss: 0.8934711217880249
Epoch 930, training loss: 12.65798282623291 = 0.0722491666674614 + 2.0 * 6.2928667068481445
Epoch 930, val loss: 0.898502767086029
Epoch 940, training loss: 12.665258407592773 = 0.06923911720514297 + 2.0 * 6.298009872436523
Epoch 940, val loss: 0.9036413431167603
Epoch 950, training loss: 12.65438175201416 = 0.0663929209113121 + 2.0 * 6.293994426727295
Epoch 950, val loss: 0.9087042212486267
Epoch 960, training loss: 12.646683692932129 = 0.0636918693780899 + 2.0 * 6.2914958000183105
Epoch 960, val loss: 0.9137871265411377
Epoch 970, training loss: 12.655296325683594 = 0.06115158647298813 + 2.0 * 6.297072410583496
Epoch 970, val loss: 0.9189736843109131
Epoch 980, training loss: 12.64213752746582 = 0.058715466409921646 + 2.0 * 6.29171085357666
Epoch 980, val loss: 0.9241642951965332
Epoch 990, training loss: 12.633299827575684 = 0.05642987787723541 + 2.0 * 6.288434982299805
Epoch 990, val loss: 0.9294149279594421
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7962962962962963
0.838165524512388
The final CL Acc:0.79012, 0.00462, The final GNN Acc:0.83746, 0.00138
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.4 der2:0.3 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11574])
remove edge: torch.Size([2, 9478])
updated graph: torch.Size([2, 10496])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 19.131942749023438 = 1.9382376670837402 + 2.0 * 8.59685230255127
Epoch 0, val loss: 1.935435175895691
Epoch 10, training loss: 19.121007919311523 = 1.9278557300567627 + 2.0 * 8.596575736999512
Epoch 10, val loss: 1.925191044807434
Epoch 20, training loss: 19.10366439819336 = 1.9146615266799927 + 2.0 * 8.594501495361328
Epoch 20, val loss: 1.912459135055542
Epoch 30, training loss: 19.052288055419922 = 1.8964704275131226 + 2.0 * 8.577908515930176
Epoch 30, val loss: 1.8951873779296875
Epoch 40, training loss: 18.81338882446289 = 1.8740016222000122 + 2.0 * 8.469693183898926
Epoch 40, val loss: 1.8747308254241943
Epoch 50, training loss: 18.10405731201172 = 1.851108431816101 + 2.0 * 8.126474380493164
Epoch 50, val loss: 1.8544305562973022
Epoch 60, training loss: 17.30637550354004 = 1.8335202932357788 + 2.0 * 7.736427307128906
Epoch 60, val loss: 1.8385870456695557
Epoch 70, training loss: 16.356962203979492 = 1.8237134218215942 + 2.0 * 7.266624450683594
Epoch 70, val loss: 1.8287346363067627
Epoch 80, training loss: 15.878225326538086 = 1.8161698579788208 + 2.0 * 7.031027793884277
Epoch 80, val loss: 1.8202630281448364
Epoch 90, training loss: 15.591747283935547 = 1.8048691749572754 + 2.0 * 6.893439292907715
Epoch 90, val loss: 1.8095107078552246
Epoch 100, training loss: 15.398128509521484 = 1.793215274810791 + 2.0 * 6.802456378936768
Epoch 100, val loss: 1.7990394830703735
Epoch 110, training loss: 15.254996299743652 = 1.782902717590332 + 2.0 * 6.73604679107666
Epoch 110, val loss: 1.7893821001052856
Epoch 120, training loss: 15.147168159484863 = 1.7729613780975342 + 2.0 * 6.687103271484375
Epoch 120, val loss: 1.7796896696090698
Epoch 130, training loss: 15.058028221130371 = 1.7624139785766602 + 2.0 * 6.6478071212768555
Epoch 130, val loss: 1.7695579528808594
Epoch 140, training loss: 14.979896545410156 = 1.7510335445404053 + 2.0 * 6.614431381225586
Epoch 140, val loss: 1.759053349494934
Epoch 150, training loss: 14.92047119140625 = 1.7387735843658447 + 2.0 * 6.590848922729492
Epoch 150, val loss: 1.7479019165039062
Epoch 160, training loss: 14.854341506958008 = 1.725233554840088 + 2.0 * 6.564553737640381
Epoch 160, val loss: 1.7358704805374146
Epoch 170, training loss: 14.798589706420898 = 1.7103302478790283 + 2.0 * 6.544129848480225
Epoch 170, val loss: 1.722651720046997
Epoch 180, training loss: 14.747323036193848 = 1.6935689449310303 + 2.0 * 6.526876926422119
Epoch 180, val loss: 1.7079236507415771
Epoch 190, training loss: 14.7046480178833 = 1.6748148202896118 + 2.0 * 6.51491641998291
Epoch 190, val loss: 1.6915208101272583
Epoch 200, training loss: 14.64953899383545 = 1.6539793014526367 + 2.0 * 6.497779846191406
Epoch 200, val loss: 1.6733952760696411
Epoch 210, training loss: 14.604757308959961 = 1.6308894157409668 + 2.0 * 6.486933708190918
Epoch 210, val loss: 1.6533684730529785
Epoch 220, training loss: 14.555045127868652 = 1.6053816080093384 + 2.0 * 6.474831581115723
Epoch 220, val loss: 1.6312624216079712
Epoch 230, training loss: 14.522317886352539 = 1.5773597955703735 + 2.0 * 6.472478866577148
Epoch 230, val loss: 1.6070778369903564
Epoch 240, training loss: 14.465533256530762 = 1.5470134019851685 + 2.0 * 6.459259986877441
Epoch 240, val loss: 1.5810571908950806
Epoch 250, training loss: 14.410083770751953 = 1.514552354812622 + 2.0 * 6.447765827178955
Epoch 250, val loss: 1.553308367729187
Epoch 260, training loss: 14.360584259033203 = 1.480023741722107 + 2.0 * 6.440280437469482
Epoch 260, val loss: 1.5240074396133423
Epoch 270, training loss: 14.335716247558594 = 1.4436020851135254 + 2.0 * 6.446056842803955
Epoch 270, val loss: 1.4932479858398438
Epoch 280, training loss: 14.264375686645508 = 1.40601646900177 + 2.0 * 6.429179668426514
Epoch 280, val loss: 1.4621371030807495
Epoch 290, training loss: 14.211467742919922 = 1.3677819967269897 + 2.0 * 6.4218430519104
Epoch 290, val loss: 1.4305365085601807
Epoch 300, training loss: 14.160625457763672 = 1.32875394821167 + 2.0 * 6.41593599319458
Epoch 300, val loss: 1.3987021446228027
Epoch 310, training loss: 14.109662055969238 = 1.2890675067901611 + 2.0 * 6.410297393798828
Epoch 310, val loss: 1.3667868375778198
Epoch 320, training loss: 14.063519477844238 = 1.2486835718154907 + 2.0 * 6.4074177742004395
Epoch 320, val loss: 1.3347752094268799
Epoch 330, training loss: 14.019129753112793 = 1.208185076713562 + 2.0 * 6.405472278594971
Epoch 330, val loss: 1.3029038906097412
Epoch 340, training loss: 13.963438034057617 = 1.167569637298584 + 2.0 * 6.3979339599609375
Epoch 340, val loss: 1.271519660949707
Epoch 350, training loss: 13.914305686950684 = 1.1269558668136597 + 2.0 * 6.393674850463867
Epoch 350, val loss: 1.240586280822754
Epoch 360, training loss: 13.864018440246582 = 1.0866068601608276 + 2.0 * 6.388705730438232
Epoch 360, val loss: 1.2102261781692505
Epoch 370, training loss: 13.817997932434082 = 1.0465211868286133 + 2.0 * 6.385738372802734
Epoch 370, val loss: 1.1805312633514404
Epoch 380, training loss: 13.78002643585205 = 1.0068548917770386 + 2.0 * 6.386585712432861
Epoch 380, val loss: 1.151442050933838
Epoch 390, training loss: 13.724960327148438 = 0.9678177833557129 + 2.0 * 6.378571510314941
Epoch 390, val loss: 1.1232129335403442
Epoch 400, training loss: 13.681093215942383 = 0.9297879934310913 + 2.0 * 6.37565279006958
Epoch 400, val loss: 1.0960580110549927
Epoch 410, training loss: 13.64024543762207 = 0.8928475379943848 + 2.0 * 6.373698711395264
Epoch 410, val loss: 1.0700455904006958
Epoch 420, training loss: 13.598288536071777 = 0.8572667837142944 + 2.0 * 6.370511054992676
Epoch 420, val loss: 1.0453447103500366
Epoch 430, training loss: 13.555298805236816 = 0.8229633569717407 + 2.0 * 6.3661675453186035
Epoch 430, val loss: 1.0218919515609741
Epoch 440, training loss: 13.523133277893066 = 0.7899208068847656 + 2.0 * 6.36660623550415
Epoch 440, val loss: 0.9997007250785828
Epoch 450, training loss: 13.493446350097656 = 0.7582324743270874 + 2.0 * 6.367607116699219
Epoch 450, val loss: 0.9787960648536682
Epoch 460, training loss: 13.44607925415039 = 0.728272020816803 + 2.0 * 6.358903408050537
Epoch 460, val loss: 0.959373414516449
Epoch 470, training loss: 13.41108226776123 = 0.6996180415153503 + 2.0 * 6.355731964111328
Epoch 470, val loss: 0.9412640333175659
Epoch 480, training loss: 13.377774238586426 = 0.6721280813217163 + 2.0 * 6.352823257446289
Epoch 480, val loss: 0.9243595004081726
Epoch 490, training loss: 13.347859382629395 = 0.645677387714386 + 2.0 * 6.351090908050537
Epoch 490, val loss: 0.9085028171539307
Epoch 500, training loss: 13.321019172668457 = 0.6202340722084045 + 2.0 * 6.3503923416137695
Epoch 500, val loss: 0.8936547636985779
Epoch 510, training loss: 13.295021057128906 = 0.5959035754203796 + 2.0 * 6.3495588302612305
Epoch 510, val loss: 0.8799005746841431
Epoch 520, training loss: 13.26525592803955 = 0.5725001692771912 + 2.0 * 6.346377849578857
Epoch 520, val loss: 0.8671250939369202
Epoch 530, training loss: 13.23784065246582 = 0.5500724911689758 + 2.0 * 6.343883991241455
Epoch 530, val loss: 0.8552998900413513
Epoch 540, training loss: 13.210454940795898 = 0.5284085273742676 + 2.0 * 6.3410234451293945
Epoch 540, val loss: 0.8443986177444458
Epoch 550, training loss: 13.18320083618164 = 0.5074782371520996 + 2.0 * 6.33786153793335
Epoch 550, val loss: 0.8342635631561279
Epoch 560, training loss: 13.17647933959961 = 0.48712992668151855 + 2.0 * 6.344674587249756
Epoch 560, val loss: 0.824887752532959
Epoch 570, training loss: 13.14062786102295 = 0.46750062704086304 + 2.0 * 6.336563587188721
Epoch 570, val loss: 0.8162351846694946
Epoch 580, training loss: 13.116053581237793 = 0.44856369495391846 + 2.0 * 6.333745002746582
Epoch 580, val loss: 0.808298647403717
Epoch 590, training loss: 13.093186378479004 = 0.4301670789718628 + 2.0 * 6.331509590148926
Epoch 590, val loss: 0.8010081648826599
Epoch 600, training loss: 13.100337982177734 = 0.41228151321411133 + 2.0 * 6.344028472900391
Epoch 600, val loss: 0.794334352016449
Epoch 610, training loss: 13.050992965698242 = 0.3949624001979828 + 2.0 * 6.328015327453613
Epoch 610, val loss: 0.7881690859794617
Epoch 620, training loss: 13.032079696655273 = 0.3781982660293579 + 2.0 * 6.326940536499023
Epoch 620, val loss: 0.7825769186019897
Epoch 630, training loss: 13.012989044189453 = 0.36189156770706177 + 2.0 * 6.3255486488342285
Epoch 630, val loss: 0.7775169014930725
Epoch 640, training loss: 12.992228507995605 = 0.3459964394569397 + 2.0 * 6.323115825653076
Epoch 640, val loss: 0.7728391289710999
Epoch 650, training loss: 12.977753639221191 = 0.3306024372577667 + 2.0 * 6.323575496673584
Epoch 650, val loss: 0.768596351146698
Epoch 660, training loss: 12.955495834350586 = 0.3156101405620575 + 2.0 * 6.319942951202393
Epoch 660, val loss: 0.7648063898086548
Epoch 670, training loss: 12.954899787902832 = 0.30106696486473083 + 2.0 * 6.326916217803955
Epoch 670, val loss: 0.7613545656204224
Epoch 680, training loss: 12.93033218383789 = 0.2869422435760498 + 2.0 * 6.321694850921631
Epoch 680, val loss: 0.7584925293922424
Epoch 690, training loss: 12.910194396972656 = 0.2734100818634033 + 2.0 * 6.318392276763916
Epoch 690, val loss: 0.755927324295044
Epoch 700, training loss: 12.89505386352539 = 0.260377436876297 + 2.0 * 6.317337989807129
Epoch 700, val loss: 0.7537575960159302
Epoch 710, training loss: 12.876303672790527 = 0.24783647060394287 + 2.0 * 6.314233779907227
Epoch 710, val loss: 0.7520636320114136
Epoch 720, training loss: 12.861478805541992 = 0.23583000898361206 + 2.0 * 6.312824249267578
Epoch 720, val loss: 0.7507978677749634
Epoch 730, training loss: 12.865509033203125 = 0.2243722826242447 + 2.0 * 6.320568561553955
Epoch 730, val loss: 0.7500206232070923
Epoch 740, training loss: 12.84097957611084 = 0.2134542316198349 + 2.0 * 6.313762664794922
Epoch 740, val loss: 0.7495895028114319
Epoch 750, training loss: 12.821462631225586 = 0.20313744246959686 + 2.0 * 6.309162616729736
Epoch 750, val loss: 0.7496147751808167
Epoch 760, training loss: 12.809222221374512 = 0.1933303028345108 + 2.0 * 6.307945728302002
Epoch 760, val loss: 0.7501428127288818
Epoch 770, training loss: 12.806517601013184 = 0.1840382069349289 + 2.0 * 6.311239719390869
Epoch 770, val loss: 0.7511227130889893
Epoch 780, training loss: 12.788747787475586 = 0.17528903484344482 + 2.0 * 6.306729316711426
Epoch 780, val loss: 0.7524423599243164
Epoch 790, training loss: 12.790664672851562 = 0.16698968410491943 + 2.0 * 6.311837673187256
Epoch 790, val loss: 0.7541118264198303
Epoch 800, training loss: 12.77132797241211 = 0.1591612994670868 + 2.0 * 6.3060832023620605
Epoch 800, val loss: 0.7561847567558289
Epoch 810, training loss: 12.758918762207031 = 0.1517835259437561 + 2.0 * 6.303567409515381
Epoch 810, val loss: 0.7585866451263428
Epoch 820, training loss: 12.749104499816895 = 0.14477841556072235 + 2.0 * 6.302163124084473
Epoch 820, val loss: 0.7612901926040649
Epoch 830, training loss: 12.74434757232666 = 0.138149693608284 + 2.0 * 6.303099155426025
Epoch 830, val loss: 0.7642377018928528
Epoch 840, training loss: 12.732150077819824 = 0.1318809986114502 + 2.0 * 6.300134658813477
Epoch 840, val loss: 0.7674980759620667
Epoch 850, training loss: 12.72299861907959 = 0.12593282759189606 + 2.0 * 6.298532962799072
Epoch 850, val loss: 0.7709363102912903
Epoch 860, training loss: 12.71461296081543 = 0.12029217183589935 + 2.0 * 6.297160625457764
Epoch 860, val loss: 0.7745687365531921
Epoch 870, training loss: 12.72115421295166 = 0.11491719633340836 + 2.0 * 6.303118705749512
Epoch 870, val loss: 0.7784955501556396
Epoch 880, training loss: 12.70469856262207 = 0.10987086594104767 + 2.0 * 6.2974138259887695
Epoch 880, val loss: 0.7824879884719849
Epoch 890, training loss: 12.693653106689453 = 0.10504107177257538 + 2.0 * 6.294305801391602
Epoch 890, val loss: 0.7866142392158508
Epoch 900, training loss: 12.71008586883545 = 0.1004825234413147 + 2.0 * 6.3048014640808105
Epoch 900, val loss: 0.7910074591636658
Epoch 910, training loss: 12.691080093383789 = 0.09614740312099457 + 2.0 * 6.297466278076172
Epoch 910, val loss: 0.7955469489097595
Epoch 920, training loss: 12.677499771118164 = 0.0920623317360878 + 2.0 * 6.292718887329102
Epoch 920, val loss: 0.8000555038452148
Epoch 930, training loss: 12.667898178100586 = 0.08818058669567108 + 2.0 * 6.289858818054199
Epoch 930, val loss: 0.8046284914016724
Epoch 940, training loss: 12.663010597229004 = 0.08447659760713577 + 2.0 * 6.289267063140869
Epoch 940, val loss: 0.8094316124916077
Epoch 950, training loss: 12.658352851867676 = 0.08095557987689972 + 2.0 * 6.288698673248291
Epoch 950, val loss: 0.8142656087875366
Epoch 960, training loss: 12.67416763305664 = 0.07760906219482422 + 2.0 * 6.298279285430908
Epoch 960, val loss: 0.8191417455673218
Epoch 970, training loss: 12.662348747253418 = 0.07443838566541672 + 2.0 * 6.293955326080322
Epoch 970, val loss: 0.8241358995437622
Epoch 980, training loss: 12.650408744812012 = 0.07146488130092621 + 2.0 * 6.2894721031188965
Epoch 980, val loss: 0.8288844227790833
Epoch 990, training loss: 12.640229225158691 = 0.0686439499258995 + 2.0 * 6.285792827606201
Epoch 990, val loss: 0.8338305950164795
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7666666666666667
0.8033737480231946
=== training gcn model ===
Epoch 0, training loss: 19.150001525878906 = 1.9563419818878174 + 2.0 * 8.596829414367676
Epoch 0, val loss: 1.9583884477615356
Epoch 10, training loss: 19.138383865356445 = 1.9455286264419556 + 2.0 * 8.596427917480469
Epoch 10, val loss: 1.9480048418045044
Epoch 20, training loss: 19.118120193481445 = 1.9317444562911987 + 2.0 * 8.593188285827637
Epoch 20, val loss: 1.9342997074127197
Epoch 30, training loss: 19.054189682006836 = 1.9125865697860718 + 2.0 * 8.570801734924316
Epoch 30, val loss: 1.915368914604187
Epoch 40, training loss: 18.78289794921875 = 1.8900493383407593 + 2.0 * 8.44642448425293
Epoch 40, val loss: 1.894365906715393
Epoch 50, training loss: 18.095409393310547 = 1.8663530349731445 + 2.0 * 8.114527702331543
Epoch 50, val loss: 1.8726879358291626
Epoch 60, training loss: 17.203752517700195 = 1.8487144708633423 + 2.0 * 7.67751932144165
Epoch 60, val loss: 1.8566787242889404
Epoch 70, training loss: 16.228626251220703 = 1.8347740173339844 + 2.0 * 7.196926593780518
Epoch 70, val loss: 1.8435068130493164
Epoch 80, training loss: 15.843293190002441 = 1.821154236793518 + 2.0 * 7.011069297790527
Epoch 80, val loss: 1.830736517906189
Epoch 90, training loss: 15.587251663208008 = 1.8067383766174316 + 2.0 * 6.890256404876709
Epoch 90, val loss: 1.8173444271087646
Epoch 100, training loss: 15.401677131652832 = 1.792535662651062 + 2.0 * 6.80457067489624
Epoch 100, val loss: 1.8044073581695557
Epoch 110, training loss: 15.272457122802734 = 1.7792514562606812 + 2.0 * 6.746603012084961
Epoch 110, val loss: 1.7921545505523682
Epoch 120, training loss: 15.164419174194336 = 1.7664870023727417 + 2.0 * 6.698966026306152
Epoch 120, val loss: 1.7801095247268677
Epoch 130, training loss: 15.081695556640625 = 1.7534037828445435 + 2.0 * 6.6641459465026855
Epoch 130, val loss: 1.7677257061004639
Epoch 140, training loss: 15.009900093078613 = 1.7393418550491333 + 2.0 * 6.635279178619385
Epoch 140, val loss: 1.754676103591919
Epoch 150, training loss: 14.950920104980469 = 1.7240757942199707 + 2.0 * 6.613422393798828
Epoch 150, val loss: 1.7407206296920776
Epoch 160, training loss: 14.891148567199707 = 1.70735764503479 + 2.0 * 6.591895580291748
Epoch 160, val loss: 1.7256501913070679
Epoch 170, training loss: 14.831130981445312 = 1.6890934705734253 + 2.0 * 6.571018695831299
Epoch 170, val loss: 1.7092913389205933
Epoch 180, training loss: 14.775988578796387 = 1.6689937114715576 + 2.0 * 6.553497314453125
Epoch 180, val loss: 1.6912318468093872
Epoch 190, training loss: 14.722269058227539 = 1.646763801574707 + 2.0 * 6.537752628326416
Epoch 190, val loss: 1.6712312698364258
Epoch 200, training loss: 14.665925025939941 = 1.6225166320800781 + 2.0 * 6.521704196929932
Epoch 200, val loss: 1.6493644714355469
Epoch 210, training loss: 14.61326789855957 = 1.5964256525039673 + 2.0 * 6.508420944213867
Epoch 210, val loss: 1.6258262395858765
Epoch 220, training loss: 14.563643455505371 = 1.5684107542037964 + 2.0 * 6.497616291046143
Epoch 220, val loss: 1.6005258560180664
Epoch 230, training loss: 14.509806632995605 = 1.5388548374176025 + 2.0 * 6.485476016998291
Epoch 230, val loss: 1.5739362239837646
Epoch 240, training loss: 14.455896377563477 = 1.508103847503662 + 2.0 * 6.473896503448486
Epoch 240, val loss: 1.5464179515838623
Epoch 250, training loss: 14.404084205627441 = 1.476324200630188 + 2.0 * 6.4638800621032715
Epoch 250, val loss: 1.5182222127914429
Epoch 260, training loss: 14.365446090698242 = 1.4438596963882446 + 2.0 * 6.4607930183410645
Epoch 260, val loss: 1.4897236824035645
Epoch 270, training loss: 14.305255889892578 = 1.4113774299621582 + 2.0 * 6.446939468383789
Epoch 270, val loss: 1.4618051052093506
Epoch 280, training loss: 14.257412910461426 = 1.3788442611694336 + 2.0 * 6.439284324645996
Epoch 280, val loss: 1.4344148635864258
Epoch 290, training loss: 14.22355842590332 = 1.3462108373641968 + 2.0 * 6.438673973083496
Epoch 290, val loss: 1.407391905784607
Epoch 300, training loss: 14.168105125427246 = 1.3137418031692505 + 2.0 * 6.427181720733643
Epoch 300, val loss: 1.381100058555603
Epoch 310, training loss: 14.12238883972168 = 1.2812635898590088 + 2.0 * 6.420562744140625
Epoch 310, val loss: 1.355485439300537
Epoch 320, training loss: 14.077356338500977 = 1.2485679388046265 + 2.0 * 6.414394378662109
Epoch 320, val loss: 1.3300856351852417
Epoch 330, training loss: 14.034422874450684 = 1.2154864072799683 + 2.0 * 6.409468173980713
Epoch 330, val loss: 1.3047879934310913
Epoch 340, training loss: 14.000016212463379 = 1.1819846630096436 + 2.0 * 6.409015655517578
Epoch 340, val loss: 1.2794326543807983
Epoch 350, training loss: 13.95467758178711 = 1.1485480070114136 + 2.0 * 6.403064727783203
Epoch 350, val loss: 1.2544924020767212
Epoch 360, training loss: 13.908894538879395 = 1.1148245334625244 + 2.0 * 6.397035121917725
Epoch 360, val loss: 1.229508638381958
Epoch 370, training loss: 13.865152359008789 = 1.0806021690368652 + 2.0 * 6.392274856567383
Epoch 370, val loss: 1.2041620016098022
Epoch 380, training loss: 13.822568893432617 = 1.045889139175415 + 2.0 * 6.388339996337891
Epoch 380, val loss: 1.1786057949066162
Epoch 390, training loss: 13.780404090881348 = 1.0107606649398804 + 2.0 * 6.384821891784668
Epoch 390, val loss: 1.1527879238128662
Epoch 400, training loss: 13.752995491027832 = 0.9756931066513062 + 2.0 * 6.388651371002197
Epoch 400, val loss: 1.1270562410354614
Epoch 410, training loss: 13.698646545410156 = 0.9412699937820435 + 2.0 * 6.378688335418701
Epoch 410, val loss: 1.1020945310592651
Epoch 420, training loss: 13.659244537353516 = 0.9075039029121399 + 2.0 * 6.375870227813721
Epoch 420, val loss: 1.0778614282608032
Epoch 430, training loss: 13.619633674621582 = 0.874388575553894 + 2.0 * 6.372622489929199
Epoch 430, val loss: 1.054230809211731
Epoch 440, training loss: 13.580334663391113 = 0.842032790184021 + 2.0 * 6.3691511154174805
Epoch 440, val loss: 1.0315073728561401
Epoch 450, training loss: 13.568967819213867 = 0.8106998205184937 + 2.0 * 6.379134178161621
Epoch 450, val loss: 1.0098519325256348
Epoch 460, training loss: 13.515677452087402 = 0.7805625200271606 + 2.0 * 6.367557525634766
Epoch 460, val loss: 0.9896056056022644
Epoch 470, training loss: 13.47796630859375 = 0.7519869804382324 + 2.0 * 6.362989902496338
Epoch 470, val loss: 0.9709770083427429
Epoch 480, training loss: 13.442678451538086 = 0.7245697975158691 + 2.0 * 6.3590545654296875
Epoch 480, val loss: 0.9537082314491272
Epoch 490, training loss: 13.411152839660645 = 0.6982397437095642 + 2.0 * 6.356456756591797
Epoch 490, val loss: 0.9376927018165588
Epoch 500, training loss: 13.386378288269043 = 0.6728894114494324 + 2.0 * 6.356744289398193
Epoch 500, val loss: 0.9228686094284058
Epoch 510, training loss: 13.360894203186035 = 0.648574948310852 + 2.0 * 6.356159687042236
Epoch 510, val loss: 0.9095216989517212
Epoch 520, training loss: 13.326600074768066 = 0.6253424286842346 + 2.0 * 6.350628852844238
Epoch 520, val loss: 0.8974626660346985
Epoch 530, training loss: 13.299604415893555 = 0.6030023694038391 + 2.0 * 6.348300933837891
Epoch 530, val loss: 0.8865062594413757
Epoch 540, training loss: 13.274001121520996 = 0.5815017223358154 + 2.0 * 6.346249580383301
Epoch 540, val loss: 0.8766289353370667
Epoch 550, training loss: 13.247535705566406 = 0.5608466267585754 + 2.0 * 6.343344688415527
Epoch 550, val loss: 0.8678503036499023
Epoch 560, training loss: 13.235248565673828 = 0.5409864187240601 + 2.0 * 6.347131252288818
Epoch 560, val loss: 0.8600181341171265
Epoch 570, training loss: 13.20329761505127 = 0.5218993425369263 + 2.0 * 6.340699195861816
Epoch 570, val loss: 0.8531126976013184
Epoch 580, training loss: 13.177984237670898 = 0.503607451915741 + 2.0 * 6.337188243865967
Epoch 580, val loss: 0.8470250964164734
Epoch 590, training loss: 13.157028198242188 = 0.4859544038772583 + 2.0 * 6.335536956787109
Epoch 590, val loss: 0.8415830135345459
Epoch 600, training loss: 13.147223472595215 = 0.4689760208129883 + 2.0 * 6.339123725891113
Epoch 600, val loss: 0.8369010090827942
Epoch 610, training loss: 13.12407112121582 = 0.4526960551738739 + 2.0 * 6.335687637329102
Epoch 610, val loss: 0.8329213857650757
Epoch 620, training loss: 13.098480224609375 = 0.437131404876709 + 2.0 * 6.330674648284912
Epoch 620, val loss: 0.8295342922210693
Epoch 630, training loss: 13.0796537399292 = 0.42219802737236023 + 2.0 * 6.328727722167969
Epoch 630, val loss: 0.8266138434410095
Epoch 640, training loss: 13.076641082763672 = 0.407819002866745 + 2.0 * 6.334411144256592
Epoch 640, val loss: 0.8242210745811462
Epoch 650, training loss: 13.050704002380371 = 0.3941238522529602 + 2.0 * 6.328289985656738
Epoch 650, val loss: 0.8223756551742554
Epoch 660, training loss: 13.030862808227539 = 0.3808562457561493 + 2.0 * 6.325003147125244
Epoch 660, val loss: 0.8209684491157532
Epoch 670, training loss: 13.02544116973877 = 0.3681582510471344 + 2.0 * 6.328641414642334
Epoch 670, val loss: 0.8199793100357056
Epoch 680, training loss: 13.004631042480469 = 0.3558938205242157 + 2.0 * 6.324368476867676
Epoch 680, val loss: 0.8193573355674744
Epoch 690, training loss: 12.983280181884766 = 0.3441462516784668 + 2.0 * 6.3195672035217285
Epoch 690, val loss: 0.8191375136375427
Epoch 700, training loss: 12.978354454040527 = 0.3327902555465698 + 2.0 * 6.322782039642334
Epoch 700, val loss: 0.8192239999771118
Epoch 710, training loss: 12.958539009094238 = 0.3218400180339813 + 2.0 * 6.318349361419678
Epoch 710, val loss: 0.8196509480476379
Epoch 720, training loss: 12.955801963806152 = 0.31125113368034363 + 2.0 * 6.322275638580322
Epoch 720, val loss: 0.8203589916229248
Epoch 730, training loss: 12.934884071350098 = 0.3010387420654297 + 2.0 * 6.316922664642334
Epoch 730, val loss: 0.8213608264923096
Epoch 740, training loss: 12.917633056640625 = 0.2911488115787506 + 2.0 * 6.313241958618164
Epoch 740, val loss: 0.8226746916770935
Epoch 750, training loss: 12.906472206115723 = 0.2815472185611725 + 2.0 * 6.312462329864502
Epoch 750, val loss: 0.8242506980895996
Epoch 760, training loss: 12.904436111450195 = 0.27222827076911926 + 2.0 * 6.316103935241699
Epoch 760, val loss: 0.8260692954063416
Epoch 770, training loss: 12.88387680053711 = 0.2631235718727112 + 2.0 * 6.3103766441345215
Epoch 770, val loss: 0.8281789422035217
Epoch 780, training loss: 12.885302543640137 = 0.25429654121398926 + 2.0 * 6.315503120422363
Epoch 780, val loss: 0.8304656147956848
Epoch 790, training loss: 12.86337661743164 = 0.24566470086574554 + 2.0 * 6.308856010437012
Epoch 790, val loss: 0.8329662084579468
Epoch 800, training loss: 12.84906005859375 = 0.2372131049633026 + 2.0 * 6.3059234619140625
Epoch 800, val loss: 0.8357065320014954
Epoch 810, training loss: 12.838566780090332 = 0.22889848053455353 + 2.0 * 6.304834365844727
Epoch 810, val loss: 0.8385784029960632
Epoch 820, training loss: 12.841863632202148 = 0.2206854671239853 + 2.0 * 6.31058931350708
Epoch 820, val loss: 0.8416337966918945
Epoch 830, training loss: 12.817368507385254 = 0.212627574801445 + 2.0 * 6.302370548248291
Epoch 830, val loss: 0.8449510931968689
Epoch 840, training loss: 12.807748794555664 = 0.20468050241470337 + 2.0 * 6.301534175872803
Epoch 840, val loss: 0.8483828902244568
Epoch 850, training loss: 12.803306579589844 = 0.19681382179260254 + 2.0 * 6.30324649810791
Epoch 850, val loss: 0.8519698977470398
Epoch 860, training loss: 12.790409088134766 = 0.1890677958726883 + 2.0 * 6.300670623779297
Epoch 860, val loss: 0.8557237982749939
Epoch 870, training loss: 12.782941818237305 = 0.18146869540214539 + 2.0 * 6.300736427307129
Epoch 870, val loss: 0.8596950769424438
Epoch 880, training loss: 12.768378257751465 = 0.17401425540447235 + 2.0 * 6.297182083129883
Epoch 880, val loss: 0.8638172745704651
Epoch 890, training loss: 12.761868476867676 = 0.1667226254940033 + 2.0 * 6.297573089599609
Epoch 890, val loss: 0.8681151866912842
Epoch 900, training loss: 12.752110481262207 = 0.15960131585597992 + 2.0 * 6.296254634857178
Epoch 900, val loss: 0.8725073337554932
Epoch 910, training loss: 12.745957374572754 = 0.15270115435123444 + 2.0 * 6.296627998352051
Epoch 910, val loss: 0.8771660327911377
Epoch 920, training loss: 12.74102783203125 = 0.14603111147880554 + 2.0 * 6.2974982261657715
Epoch 920, val loss: 0.8818534016609192
Epoch 930, training loss: 12.72795295715332 = 0.13962675631046295 + 2.0 * 6.294163227081299
Epoch 930, val loss: 0.886777400970459
Epoch 940, training loss: 12.721273422241211 = 0.13345837593078613 + 2.0 * 6.293907642364502
Epoch 940, val loss: 0.8917432427406311
Epoch 950, training loss: 12.711202621459961 = 0.12756361067295074 + 2.0 * 6.2918195724487305
Epoch 950, val loss: 0.896901547908783
Epoch 960, training loss: 12.7046537399292 = 0.12190868705511093 + 2.0 * 6.291372299194336
Epoch 960, val loss: 0.9021411538124084
Epoch 970, training loss: 12.69867992401123 = 0.11651615798473358 + 2.0 * 6.29108190536499
Epoch 970, val loss: 0.9074803590774536
Epoch 980, training loss: 12.694758415222168 = 0.11138457804918289 + 2.0 * 6.29168701171875
Epoch 980, val loss: 0.912956953048706
Epoch 990, training loss: 12.681748390197754 = 0.10651322454214096 + 2.0 * 6.2876176834106445
Epoch 990, val loss: 0.9185243844985962
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7407407407407408
0.8102266736953084
=== training gcn model ===
Epoch 0, training loss: 19.16426658630371 = 1.9706169366836548 + 2.0 * 8.596824645996094
Epoch 0, val loss: 1.9695271253585815
Epoch 10, training loss: 19.152000427246094 = 1.959068775177002 + 2.0 * 8.596466064453125
Epoch 10, val loss: 1.9587671756744385
Epoch 20, training loss: 19.13145637512207 = 1.9440338611602783 + 2.0 * 8.593710899353027
Epoch 20, val loss: 1.9445027112960815
Epoch 30, training loss: 19.068822860717773 = 1.9224127531051636 + 2.0 * 8.57320499420166
Epoch 30, val loss: 1.9240069389343262
Epoch 40, training loss: 18.782583236694336 = 1.895058512687683 + 2.0 * 8.44376277923584
Epoch 40, val loss: 1.899108648300171
Epoch 50, training loss: 17.99712371826172 = 1.866740107536316 + 2.0 * 8.065192222595215
Epoch 50, val loss: 1.8736824989318848
Epoch 60, training loss: 17.235698699951172 = 1.8450018167495728 + 2.0 * 7.695348262786865
Epoch 60, val loss: 1.8543753623962402
Epoch 70, training loss: 16.384986877441406 = 1.8304853439331055 + 2.0 * 7.27725076675415
Epoch 70, val loss: 1.841310739517212
Epoch 80, training loss: 15.993945121765137 = 1.8187181949615479 + 2.0 * 7.087613582611084
Epoch 80, val loss: 1.829983115196228
Epoch 90, training loss: 15.616111755371094 = 1.8047597408294678 + 2.0 * 6.905675888061523
Epoch 90, val loss: 1.817233920097351
Epoch 100, training loss: 15.388833999633789 = 1.7917627096176147 + 2.0 * 6.7985358238220215
Epoch 100, val loss: 1.805362582206726
Epoch 110, training loss: 15.246807098388672 = 1.7796521186828613 + 2.0 * 6.733577251434326
Epoch 110, val loss: 1.7939809560775757
Epoch 120, training loss: 15.13953971862793 = 1.7675031423568726 + 2.0 * 6.686018466949463
Epoch 120, val loss: 1.7826287746429443
Epoch 130, training loss: 15.047054290771484 = 1.754893183708191 + 2.0 * 6.646080493927002
Epoch 130, val loss: 1.7711207866668701
Epoch 140, training loss: 14.977190017700195 = 1.7415173053741455 + 2.0 * 6.6178364753723145
Epoch 140, val loss: 1.7592276334762573
Epoch 150, training loss: 14.90776538848877 = 1.7271217107772827 + 2.0 * 6.590322017669678
Epoch 150, val loss: 1.746713399887085
Epoch 160, training loss: 14.84412956237793 = 1.7114492654800415 + 2.0 * 6.56633996963501
Epoch 160, val loss: 1.7333651781082153
Epoch 170, training loss: 14.786672592163086 = 1.6941803693771362 + 2.0 * 6.54624605178833
Epoch 170, val loss: 1.7189019918441772
Epoch 180, training loss: 14.729666709899902 = 1.6750737428665161 + 2.0 * 6.527296543121338
Epoch 180, val loss: 1.703038215637207
Epoch 190, training loss: 14.67318344116211 = 1.6540011167526245 + 2.0 * 6.509591102600098
Epoch 190, val loss: 1.6856402158737183
Epoch 200, training loss: 14.62625789642334 = 1.630738377571106 + 2.0 * 6.497759819030762
Epoch 200, val loss: 1.6665111780166626
Epoch 210, training loss: 14.569190979003906 = 1.6053268909454346 + 2.0 * 6.481932163238525
Epoch 210, val loss: 1.64574134349823
Epoch 220, training loss: 14.516044616699219 = 1.5776240825653076 + 2.0 * 6.469210147857666
Epoch 220, val loss: 1.6232813596725464
Epoch 230, training loss: 14.477314949035645 = 1.5478383302688599 + 2.0 * 6.464738368988037
Epoch 230, val loss: 1.5994205474853516
Epoch 240, training loss: 14.416728019714355 = 1.5162773132324219 + 2.0 * 6.450225353240967
Epoch 240, val loss: 1.574531078338623
Epoch 250, training loss: 14.364534378051758 = 1.4831334352493286 + 2.0 * 6.440700531005859
Epoch 250, val loss: 1.5487968921661377
Epoch 260, training loss: 14.324960708618164 = 1.4486067295074463 + 2.0 * 6.438177108764648
Epoch 260, val loss: 1.5223878622055054
Epoch 270, training loss: 14.267193794250488 = 1.413182020187378 + 2.0 * 6.427005767822266
Epoch 270, val loss: 1.4958070516586304
Epoch 280, training loss: 14.216727256774902 = 1.3771520853042603 + 2.0 * 6.419787406921387
Epoch 280, val loss: 1.4692442417144775
Epoch 290, training loss: 14.171001434326172 = 1.340601921081543 + 2.0 * 6.4151997566223145
Epoch 290, val loss: 1.442878246307373
Epoch 300, training loss: 14.12244701385498 = 1.3039604425430298 + 2.0 * 6.409243106842041
Epoch 300, val loss: 1.4170317649841309
Epoch 310, training loss: 14.07452392578125 = 1.267486810684204 + 2.0 * 6.4035186767578125
Epoch 310, val loss: 1.3916624784469604
Epoch 320, training loss: 14.029728889465332 = 1.2312405109405518 + 2.0 * 6.39924430847168
Epoch 320, val loss: 1.3668636083602905
Epoch 330, training loss: 13.982368469238281 = 1.1953598260879517 + 2.0 * 6.3935041427612305
Epoch 330, val loss: 1.3426605463027954
Epoch 340, training loss: 13.938671112060547 = 1.1597667932510376 + 2.0 * 6.38945198059082
Epoch 340, val loss: 1.3189315795898438
Epoch 350, training loss: 13.905191421508789 = 1.1244580745697021 + 2.0 * 6.390366554260254
Epoch 350, val loss: 1.2955931425094604
Epoch 360, training loss: 13.861156463623047 = 1.089818000793457 + 2.0 * 6.385669231414795
Epoch 360, val loss: 1.272832989692688
Epoch 370, training loss: 13.815560340881348 = 1.0559567213058472 + 2.0 * 6.3798017501831055
Epoch 370, val loss: 1.250633716583252
Epoch 380, training loss: 13.77367877960205 = 1.022894263267517 + 2.0 * 6.375392436981201
Epoch 380, val loss: 1.2290273904800415
Epoch 390, training loss: 13.737200736999512 = 0.9903188943862915 + 2.0 * 6.373440742492676
Epoch 390, val loss: 1.2076702117919922
Epoch 400, training loss: 13.69729995727539 = 0.9583789706230164 + 2.0 * 6.369460582733154
Epoch 400, val loss: 1.1865835189819336
Epoch 410, training loss: 13.664409637451172 = 0.9268960356712341 + 2.0 * 6.3687567710876465
Epoch 410, val loss: 1.1657989025115967
Epoch 420, training loss: 13.627450942993164 = 0.8958411812782288 + 2.0 * 6.365804672241211
Epoch 420, val loss: 1.1451736688613892
Epoch 430, training loss: 13.598451614379883 = 0.8652364015579224 + 2.0 * 6.366607666015625
Epoch 430, val loss: 1.1249849796295166
Epoch 440, training loss: 13.559459686279297 = 0.8352184295654297 + 2.0 * 6.362120628356934
Epoch 440, val loss: 1.1051474809646606
Epoch 450, training loss: 13.519454956054688 = 0.8058686256408691 + 2.0 * 6.35679292678833
Epoch 450, val loss: 1.085837483406067
Epoch 460, training loss: 13.484325408935547 = 0.7771098613739014 + 2.0 * 6.353607654571533
Epoch 460, val loss: 1.0671563148498535
Epoch 470, training loss: 13.473150253295898 = 0.7490431070327759 + 2.0 * 6.362053394317627
Epoch 470, val loss: 1.0493074655532837
Epoch 480, training loss: 13.421701431274414 = 0.7218480110168457 + 2.0 * 6.349926471710205
Epoch 480, val loss: 1.0325067043304443
Epoch 490, training loss: 13.392003059387207 = 0.6955835819244385 + 2.0 * 6.348209857940674
Epoch 490, val loss: 1.0168293714523315
Epoch 500, training loss: 13.362990379333496 = 0.6701451539993286 + 2.0 * 6.3464226722717285
Epoch 500, val loss: 1.00234055519104
Epoch 510, training loss: 13.334083557128906 = 0.6455560922622681 + 2.0 * 6.344263553619385
Epoch 510, val loss: 0.9889049530029297
Epoch 520, training loss: 13.303107261657715 = 0.6215593814849854 + 2.0 * 6.340774059295654
Epoch 520, val loss: 0.9765241742134094
Epoch 530, training loss: 13.287271499633789 = 0.5980687141418457 + 2.0 * 6.344601154327393
Epoch 530, val loss: 0.9651600122451782
Epoch 540, training loss: 13.258859634399414 = 0.5753127336502075 + 2.0 * 6.341773509979248
Epoch 540, val loss: 0.9548062682151794
Epoch 550, training loss: 13.225556373596191 = 0.5531126856803894 + 2.0 * 6.336221694946289
Epoch 550, val loss: 0.9452793598175049
Epoch 560, training loss: 13.210458755493164 = 0.5315247178077698 + 2.0 * 6.3394670486450195
Epoch 560, val loss: 0.9365931749343872
Epoch 570, training loss: 13.176721572875977 = 0.5105702877044678 + 2.0 * 6.333075523376465
Epoch 570, val loss: 0.9288179874420166
Epoch 580, training loss: 13.155652046203613 = 0.4901905357837677 + 2.0 * 6.332730770111084
Epoch 580, val loss: 0.9218348264694214
Epoch 590, training loss: 13.133970260620117 = 0.4703778326511383 + 2.0 * 6.331796169281006
Epoch 590, val loss: 0.9156107306480408
Epoch 600, training loss: 13.104872703552246 = 0.4512481987476349 + 2.0 * 6.326812267303467
Epoch 600, val loss: 0.9101040959358215
Epoch 610, training loss: 13.084455490112305 = 0.43266215920448303 + 2.0 * 6.325896739959717
Epoch 610, val loss: 0.9053549766540527
Epoch 620, training loss: 13.080780029296875 = 0.41459420323371887 + 2.0 * 6.33309268951416
Epoch 620, val loss: 0.9013568162918091
Epoch 630, training loss: 13.048702239990234 = 0.3970915973186493 + 2.0 * 6.325805187225342
Epoch 630, val loss: 0.8979716897010803
Epoch 640, training loss: 13.024767875671387 = 0.3801998198032379 + 2.0 * 6.32228422164917
Epoch 640, val loss: 0.8952167630195618
Epoch 650, training loss: 13.008496284484863 = 0.363812655210495 + 2.0 * 6.3223419189453125
Epoch 650, val loss: 0.8932070136070251
Epoch 660, training loss: 12.990718841552734 = 0.34791311621665955 + 2.0 * 6.3214030265808105
Epoch 660, val loss: 0.8917132616043091
Epoch 670, training loss: 12.969259262084961 = 0.3326071798801422 + 2.0 * 6.318325996398926
Epoch 670, val loss: 0.8908657431602478
Epoch 680, training loss: 12.950591087341309 = 0.3177556097507477 + 2.0 * 6.316417694091797
Epoch 680, val loss: 0.8905055522918701
Epoch 690, training loss: 12.936527252197266 = 0.30340707302093506 + 2.0 * 6.3165602684021
Epoch 690, val loss: 0.8906364440917969
Epoch 700, training loss: 12.924028396606445 = 0.2895578145980835 + 2.0 * 6.317235469818115
Epoch 700, val loss: 0.8911171555519104
Epoch 710, training loss: 12.903225898742676 = 0.27624425292015076 + 2.0 * 6.313490867614746
Epoch 710, val loss: 0.8921676278114319
Epoch 720, training loss: 12.897989273071289 = 0.26342010498046875 + 2.0 * 6.31728458404541
Epoch 720, val loss: 0.8936097025871277
Epoch 730, training loss: 12.87505054473877 = 0.2511686086654663 + 2.0 * 6.311941146850586
Epoch 730, val loss: 0.8954394459724426
Epoch 740, training loss: 12.857951164245605 = 0.2393898069858551 + 2.0 * 6.309280872344971
Epoch 740, val loss: 0.8976513147354126
Epoch 750, training loss: 12.841618537902832 = 0.22810113430023193 + 2.0 * 6.306758880615234
Epoch 750, val loss: 0.9003258347511292
Epoch 760, training loss: 12.828984260559082 = 0.2172502875328064 + 2.0 * 6.3058671951293945
Epoch 760, val loss: 0.9033976197242737
Epoch 770, training loss: 12.835367202758789 = 0.20685110986232758 + 2.0 * 6.314258098602295
Epoch 770, val loss: 0.9067846536636353
Epoch 780, training loss: 12.81335735321045 = 0.19697731733322144 + 2.0 * 6.308189868927002
Epoch 780, val loss: 0.9104545712471008
Epoch 790, training loss: 12.793794631958008 = 0.18755728006362915 + 2.0 * 6.303118705749512
Epoch 790, val loss: 0.91450434923172
Epoch 800, training loss: 12.780791282653809 = 0.1785784810781479 + 2.0 * 6.3011064529418945
Epoch 800, val loss: 0.9189823269844055
Epoch 810, training loss: 12.795085906982422 = 0.17000895738601685 + 2.0 * 6.3125386238098145
Epoch 810, val loss: 0.9236047267913818
Epoch 820, training loss: 12.766921043395996 = 0.16185681521892548 + 2.0 * 6.302532196044922
Epoch 820, val loss: 0.9285231232643127
Epoch 830, training loss: 12.754996299743652 = 0.15413476526737213 + 2.0 * 6.300430774688721
Epoch 830, val loss: 0.9335783123970032
Epoch 840, training loss: 12.745360374450684 = 0.14681652188301086 + 2.0 * 6.299272060394287
Epoch 840, val loss: 0.9390512108802795
Epoch 850, training loss: 12.736967086791992 = 0.13987676799297333 + 2.0 * 6.2985453605651855
Epoch 850, val loss: 0.9446623921394348
Epoch 860, training loss: 12.727716445922852 = 0.13328586518764496 + 2.0 * 6.297215461730957
Epoch 860, val loss: 0.9504949450492859
Epoch 870, training loss: 12.720147132873535 = 0.1270519196987152 + 2.0 * 6.2965474128723145
Epoch 870, val loss: 0.9566071629524231
Epoch 880, training loss: 12.711979866027832 = 0.12114982306957245 + 2.0 * 6.295414924621582
Epoch 880, val loss: 0.9627806544303894
Epoch 890, training loss: 12.703563690185547 = 0.1155719980597496 + 2.0 * 6.2939958572387695
Epoch 890, val loss: 0.969136118888855
Epoch 900, training loss: 12.696380615234375 = 0.11030438542366028 + 2.0 * 6.2930378913879395
Epoch 900, val loss: 0.9755215048789978
Epoch 910, training loss: 12.698441505432129 = 0.10533250868320465 + 2.0 * 6.2965545654296875
Epoch 910, val loss: 0.9820601940155029
Epoch 920, training loss: 12.683048248291016 = 0.10065892338752747 + 2.0 * 6.291194438934326
Epoch 920, val loss: 0.9886553883552551
Epoch 930, training loss: 12.673502922058105 = 0.09623033553361893 + 2.0 * 6.288636207580566
Epoch 930, val loss: 0.995457649230957
Epoch 940, training loss: 12.668628692626953 = 0.09204225242137909 + 2.0 * 6.288293361663818
Epoch 940, val loss: 1.0023542642593384
Epoch 950, training loss: 12.685977935791016 = 0.08808387815952301 + 2.0 * 6.298946857452393
Epoch 950, val loss: 1.0092241764068604
Epoch 960, training loss: 12.661431312561035 = 0.08435287326574326 + 2.0 * 6.288539409637451
Epoch 960, val loss: 1.0161309242248535
Epoch 970, training loss: 12.651921272277832 = 0.08082613348960876 + 2.0 * 6.285547733306885
Epoch 970, val loss: 1.0230473279953003
Epoch 980, training loss: 12.650052070617676 = 0.07749945670366287 + 2.0 * 6.286276340484619
Epoch 980, val loss: 1.0300483703613281
Epoch 990, training loss: 12.65487003326416 = 0.07435522228479385 + 2.0 * 6.290257453918457
Epoch 990, val loss: 1.037064790725708
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7407407407407408
0.8007380073800738
The final CL Acc:0.74938, 0.01222, The final GNN Acc:0.80478, 0.00400
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13206])
remove edge: torch.Size([2, 7956])
updated graph: torch.Size([2, 10606])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 19.132076263427734 = 1.938413381576538 + 2.0 * 8.596831321716309
Epoch 0, val loss: 1.941884160041809
Epoch 10, training loss: 19.12159538269043 = 1.9286075830459595 + 2.0 * 8.5964937210083
Epoch 10, val loss: 1.9315273761749268
Epoch 20, training loss: 19.103466033935547 = 1.9164081811904907 + 2.0 * 8.593528747558594
Epoch 20, val loss: 1.918412685394287
Epoch 30, training loss: 19.04384422302246 = 1.8994001150131226 + 2.0 * 8.572221755981445
Epoch 30, val loss: 1.900136113166809
Epoch 40, training loss: 18.783063888549805 = 1.87769615650177 + 2.0 * 8.452683448791504
Epoch 40, val loss: 1.878296971321106
Epoch 50, training loss: 18.073083877563477 = 1.8527215719223022 + 2.0 * 8.110180854797363
Epoch 50, val loss: 1.8544495105743408
Epoch 60, training loss: 17.572877883911133 = 1.831032633781433 + 2.0 * 7.870922565460205
Epoch 60, val loss: 1.8355424404144287
Epoch 70, training loss: 16.77042007446289 = 1.8162544965744019 + 2.0 * 7.477083206176758
Epoch 70, val loss: 1.822920322418213
Epoch 80, training loss: 15.94372272491455 = 1.8101023435592651 + 2.0 * 7.066810131072998
Epoch 80, val loss: 1.8171155452728271
Epoch 90, training loss: 15.647500991821289 = 1.797986388206482 + 2.0 * 6.924757480621338
Epoch 90, val loss: 1.8047118186950684
Epoch 100, training loss: 15.435203552246094 = 1.781590223312378 + 2.0 * 6.826806545257568
Epoch 100, val loss: 1.7897616624832153
Epoch 110, training loss: 15.28971004486084 = 1.766720175743103 + 2.0 * 6.761495113372803
Epoch 110, val loss: 1.776549220085144
Epoch 120, training loss: 15.171113967895508 = 1.7529813051223755 + 2.0 * 6.709066390991211
Epoch 120, val loss: 1.763947606086731
Epoch 130, training loss: 15.078006744384766 = 1.7382410764694214 + 2.0 * 6.669882774353027
Epoch 130, val loss: 1.7504922151565552
Epoch 140, training loss: 14.996417045593262 = 1.7213945388793945 + 2.0 * 6.637511253356934
Epoch 140, val loss: 1.7355152368545532
Epoch 150, training loss: 14.92218017578125 = 1.702515721321106 + 2.0 * 6.609832286834717
Epoch 150, val loss: 1.7190626859664917
Epoch 160, training loss: 14.85501480102539 = 1.6816716194152832 + 2.0 * 6.586671829223633
Epoch 160, val loss: 1.701023817062378
Epoch 170, training loss: 14.79349136352539 = 1.658308744430542 + 2.0 * 6.567591190338135
Epoch 170, val loss: 1.6809289455413818
Epoch 180, training loss: 14.73194408416748 = 1.632529616355896 + 2.0 * 6.549707412719727
Epoch 180, val loss: 1.6588311195373535
Epoch 190, training loss: 14.666853904724121 = 1.6044915914535522 + 2.0 * 6.531181335449219
Epoch 190, val loss: 1.6348820924758911
Epoch 200, training loss: 14.605607986450195 = 1.5739630460739136 + 2.0 * 6.515822410583496
Epoch 200, val loss: 1.608821153640747
Epoch 210, training loss: 14.547677993774414 = 1.5411677360534668 + 2.0 * 6.503255367279053
Epoch 210, val loss: 1.5809214115142822
Epoch 220, training loss: 14.486688613891602 = 1.506616234779358 + 2.0 * 6.4900360107421875
Epoch 220, val loss: 1.5515975952148438
Epoch 230, training loss: 14.425284385681152 = 1.4703996181488037 + 2.0 * 6.477442264556885
Epoch 230, val loss: 1.5210407972335815
Epoch 240, training loss: 14.367215156555176 = 1.4327824115753174 + 2.0 * 6.467216491699219
Epoch 240, val loss: 1.4896615743637085
Epoch 250, training loss: 14.308931350708008 = 1.3945225477218628 + 2.0 * 6.457204341888428
Epoch 250, val loss: 1.457910418510437
Epoch 260, training loss: 14.253477096557617 = 1.3558043241500854 + 2.0 * 6.448836326599121
Epoch 260, val loss: 1.4259916543960571
Epoch 270, training loss: 14.205676078796387 = 1.3172521591186523 + 2.0 * 6.444211959838867
Epoch 270, val loss: 1.394659399986267
Epoch 280, training loss: 14.147644996643066 = 1.279348373413086 + 2.0 * 6.43414831161499
Epoch 280, val loss: 1.364156723022461
Epoch 290, training loss: 14.09546947479248 = 1.242179274559021 + 2.0 * 6.426645278930664
Epoch 290, val loss: 1.3347687721252441
Epoch 300, training loss: 14.057071685791016 = 1.2058806419372559 + 2.0 * 6.425595760345459
Epoch 300, val loss: 1.3063138723373413
Epoch 310, training loss: 14.00295352935791 = 1.1709024906158447 + 2.0 * 6.416025638580322
Epoch 310, val loss: 1.2792500257492065
Epoch 320, training loss: 13.95732593536377 = 1.1373735666275024 + 2.0 * 6.409976005554199
Epoch 320, val loss: 1.253656268119812
Epoch 330, training loss: 13.911163330078125 = 1.1054617166519165 + 2.0 * 6.40285062789917
Epoch 330, val loss: 1.2295094728469849
Epoch 340, training loss: 13.87044620513916 = 1.0749547481536865 + 2.0 * 6.397745609283447
Epoch 340, val loss: 1.2065987586975098
Epoch 350, training loss: 13.839886665344238 = 1.0457584857940674 + 2.0 * 6.397064208984375
Epoch 350, val loss: 1.1849191188812256
Epoch 360, training loss: 13.798637390136719 = 1.0181411504745483 + 2.0 * 6.3902482986450195
Epoch 360, val loss: 1.1645954847335815
Epoch 370, training loss: 13.760720252990723 = 0.9917001128196716 + 2.0 * 6.384510040283203
Epoch 370, val loss: 1.145356297492981
Epoch 380, training loss: 13.730218887329102 = 0.9662209153175354 + 2.0 * 6.3819990158081055
Epoch 380, val loss: 1.126999855041504
Epoch 390, training loss: 13.694867134094238 = 0.9415260553359985 + 2.0 * 6.3766703605651855
Epoch 390, val loss: 1.1092280149459839
Epoch 400, training loss: 13.665602684020996 = 0.9174818396568298 + 2.0 * 6.37406063079834
Epoch 400, val loss: 1.0919506549835205
Epoch 410, training loss: 13.631278038024902 = 0.8937835693359375 + 2.0 * 6.368747234344482
Epoch 410, val loss: 1.0750714540481567
Epoch 420, training loss: 13.599356651306152 = 0.8703025579452515 + 2.0 * 6.364527225494385
Epoch 420, val loss: 1.0583856105804443
Epoch 430, training loss: 13.573951721191406 = 0.8468178510665894 + 2.0 * 6.363566875457764
Epoch 430, val loss: 1.0416350364685059
Epoch 440, training loss: 13.540643692016602 = 0.8232097625732422 + 2.0 * 6.35871696472168
Epoch 440, val loss: 1.0248773097991943
Epoch 450, training loss: 13.51142406463623 = 0.799428403377533 + 2.0 * 6.3559980392456055
Epoch 450, val loss: 1.0079683065414429
Epoch 460, training loss: 13.482495307922363 = 0.7753986120223999 + 2.0 * 6.353548526763916
Epoch 460, val loss: 0.9909188747406006
Epoch 470, training loss: 13.448689460754395 = 0.7510351538658142 + 2.0 * 6.348827362060547
Epoch 470, val loss: 0.9737378358840942
Epoch 480, training loss: 13.421055793762207 = 0.726311981678009 + 2.0 * 6.347372055053711
Epoch 480, val loss: 0.9562914967536926
Epoch 490, training loss: 13.40750789642334 = 0.7011775374412537 + 2.0 * 6.353165149688721
Epoch 490, val loss: 0.9387226104736328
Epoch 500, training loss: 13.36414909362793 = 0.6760523319244385 + 2.0 * 6.344048500061035
Epoch 500, val loss: 0.9210323095321655
Epoch 510, training loss: 13.331658363342285 = 0.6507858633995056 + 2.0 * 6.3404364585876465
Epoch 510, val loss: 0.9035958647727966
Epoch 520, training loss: 13.300326347351074 = 0.625379741191864 + 2.0 * 6.337473392486572
Epoch 520, val loss: 0.886199414730072
Epoch 530, training loss: 13.294034004211426 = 0.5999556183815002 + 2.0 * 6.347039222717285
Epoch 530, val loss: 0.8689514994621277
Epoch 540, training loss: 13.244635581970215 = 0.5748311281204224 + 2.0 * 6.334902286529541
Epoch 540, val loss: 0.8520961403846741
Epoch 550, training loss: 13.217472076416016 = 0.5501646399497986 + 2.0 * 6.333653926849365
Epoch 550, val loss: 0.8359639048576355
Epoch 560, training loss: 13.187346458435059 = 0.5259631276130676 + 2.0 * 6.330691814422607
Epoch 560, val loss: 0.8203636407852173
Epoch 570, training loss: 13.159022331237793 = 0.5022653341293335 + 2.0 * 6.328378677368164
Epoch 570, val loss: 0.8055220246315002
Epoch 580, training loss: 13.136804580688477 = 0.4791838526725769 + 2.0 * 6.328810214996338
Epoch 580, val loss: 0.7915432453155518
Epoch 590, training loss: 13.125528335571289 = 0.45675936341285706 + 2.0 * 6.334384441375732
Epoch 590, val loss: 0.7784674167633057
Epoch 600, training loss: 13.087124824523926 = 0.43534114956855774 + 2.0 * 6.325891971588135
Epoch 600, val loss: 0.7665631175041199
Epoch 610, training loss: 13.058778762817383 = 0.41472798585891724 + 2.0 * 6.322025299072266
Epoch 610, val loss: 0.7556336522102356
Epoch 620, training loss: 13.036348342895508 = 0.39482539892196655 + 2.0 * 6.320761680603027
Epoch 620, val loss: 0.7456690669059753
Epoch 630, training loss: 13.025141716003418 = 0.3756401836872101 + 2.0 * 6.324750900268555
Epoch 630, val loss: 0.7365714907646179
Epoch 640, training loss: 12.992467880249023 = 0.3571806252002716 + 2.0 * 6.317643642425537
Epoch 640, val loss: 0.7284008264541626
Epoch 650, training loss: 12.972981452941895 = 0.3394407033920288 + 2.0 * 6.316770553588867
Epoch 650, val loss: 0.7210258841514587
Epoch 660, training loss: 12.957776069641113 = 0.32232382893562317 + 2.0 * 6.317726135253906
Epoch 660, val loss: 0.7144224047660828
Epoch 670, training loss: 12.941618919372559 = 0.30587559938430786 + 2.0 * 6.317871570587158
Epoch 670, val loss: 0.7084029912948608
Epoch 680, training loss: 12.917777061462402 = 0.2900574207305908 + 2.0 * 6.313859939575195
Epoch 680, val loss: 0.7032838463783264
Epoch 690, training loss: 12.898426055908203 = 0.27488866448402405 + 2.0 * 6.311768531799316
Epoch 690, val loss: 0.6986526250839233
Epoch 700, training loss: 12.880403518676758 = 0.26032859086990356 + 2.0 * 6.310037612915039
Epoch 700, val loss: 0.6946462392807007
Epoch 710, training loss: 12.869385719299316 = 0.24640394747257233 + 2.0 * 6.311491012573242
Epoch 710, val loss: 0.6912099719047546
Epoch 720, training loss: 12.86113166809082 = 0.23305672407150269 + 2.0 * 6.314037322998047
Epoch 720, val loss: 0.6883164644241333
Epoch 730, training loss: 12.835226058959961 = 0.22041967511177063 + 2.0 * 6.307403087615967
Epoch 730, val loss: 0.6858937740325928
Epoch 740, training loss: 12.819149017333984 = 0.20843632519245148 + 2.0 * 6.305356502532959
Epoch 740, val loss: 0.6841136813163757
Epoch 750, training loss: 12.806636810302734 = 0.1970563530921936 + 2.0 * 6.304790019989014
Epoch 750, val loss: 0.6828753352165222
Epoch 760, training loss: 12.800440788269043 = 0.1862897276878357 + 2.0 * 6.307075500488281
Epoch 760, val loss: 0.6820923089981079
Epoch 770, training loss: 12.780900001525879 = 0.1761789619922638 + 2.0 * 6.302360534667969
Epoch 770, val loss: 0.6817970275878906
Epoch 780, training loss: 12.774261474609375 = 0.16667641699314117 + 2.0 * 6.303792476654053
Epoch 780, val loss: 0.681982696056366
Epoch 790, training loss: 12.759513854980469 = 0.15775927901268005 + 2.0 * 6.300877094268799
Epoch 790, val loss: 0.6825730204582214
Epoch 800, training loss: 12.746694564819336 = 0.14943832159042358 + 2.0 * 6.298628330230713
Epoch 800, val loss: 0.683681070804596
Epoch 810, training loss: 12.74159049987793 = 0.14162006974220276 + 2.0 * 6.299985408782959
Epoch 810, val loss: 0.6850637793540955
Epoch 820, training loss: 12.72714900970459 = 0.13428491353988647 + 2.0 * 6.296432018280029
Epoch 820, val loss: 0.6867347359657288
Epoch 830, training loss: 12.7206392288208 = 0.1274118423461914 + 2.0 * 6.296613693237305
Epoch 830, val loss: 0.6887910962104797
Epoch 840, training loss: 12.71680736541748 = 0.12099488079547882 + 2.0 * 6.297906398773193
Epoch 840, val loss: 0.6910242438316345
Epoch 850, training loss: 12.702771186828613 = 0.11499632894992828 + 2.0 * 6.293887615203857
Epoch 850, val loss: 0.6936755180358887
Epoch 860, training loss: 12.69730281829834 = 0.10937520116567612 + 2.0 * 6.29396390914917
Epoch 860, val loss: 0.696520984172821
Epoch 870, training loss: 12.688089370727539 = 0.10410772264003754 + 2.0 * 6.291990756988525
Epoch 870, val loss: 0.6994668841362
Epoch 880, training loss: 12.679915428161621 = 0.09916640818119049 + 2.0 * 6.290374279022217
Epoch 880, val loss: 0.7027755975723267
Epoch 890, training loss: 12.676244735717773 = 0.09454257041215897 + 2.0 * 6.29085111618042
Epoch 890, val loss: 0.7061731815338135
Epoch 900, training loss: 12.669341087341309 = 0.09018574655056 + 2.0 * 6.289577484130859
Epoch 900, val loss: 0.709721565246582
Epoch 910, training loss: 12.664130210876465 = 0.08609183877706528 + 2.0 * 6.2890191078186035
Epoch 910, val loss: 0.7134398221969604
Epoch 920, training loss: 12.657217025756836 = 0.08225705474615097 + 2.0 * 6.287479877471924
Epoch 920, val loss: 0.7173193693161011
Epoch 930, training loss: 12.654715538024902 = 0.07863923907279968 + 2.0 * 6.28803825378418
Epoch 930, val loss: 0.7211095094680786
Epoch 940, training loss: 12.649049758911133 = 0.07525230199098587 + 2.0 * 6.286898612976074
Epoch 940, val loss: 0.725028932094574
Epoch 950, training loss: 12.648030281066895 = 0.07204661518335342 + 2.0 * 6.287992000579834
Epoch 950, val loss: 0.7291287779808044
Epoch 960, training loss: 12.635784149169922 = 0.06903668493032455 + 2.0 * 6.283373832702637
Epoch 960, val loss: 0.7331358194351196
Epoch 970, training loss: 12.632079124450684 = 0.06618958711624146 + 2.0 * 6.282944679260254
Epoch 970, val loss: 0.7374212741851807
Epoch 980, training loss: 12.637725830078125 = 0.06352725625038147 + 2.0 * 6.287099361419678
Epoch 980, val loss: 0.7415221333503723
Epoch 990, training loss: 12.623642921447754 = 0.06098191812634468 + 2.0 * 6.281330585479736
Epoch 990, val loss: 0.7457358241081238
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8074074074074075
0.8344754876120191
=== training gcn model ===
Epoch 0, training loss: 19.154550552368164 = 1.9609448909759521 + 2.0 * 8.596802711486816
Epoch 0, val loss: 1.9630651473999023
Epoch 10, training loss: 19.142784118652344 = 1.9500105381011963 + 2.0 * 8.596386909484863
Epoch 10, val loss: 1.95263671875
Epoch 20, training loss: 19.1221981048584 = 1.9362810850143433 + 2.0 * 8.592958450317383
Epoch 20, val loss: 1.9390430450439453
Epoch 30, training loss: 19.053314208984375 = 1.9173128604888916 + 2.0 * 8.568000793457031
Epoch 30, val loss: 1.9199342727661133
Epoch 40, training loss: 18.744102478027344 = 1.8943406343460083 + 2.0 * 8.424880981445312
Epoch 40, val loss: 1.8981961011886597
Epoch 50, training loss: 17.888866424560547 = 1.8690582513809204 + 2.0 * 8.009903907775879
Epoch 50, val loss: 1.8748056888580322
Epoch 60, training loss: 17.2648868560791 = 1.8458999395370483 + 2.0 * 7.709493637084961
Epoch 60, val loss: 1.8539129495620728
Epoch 70, training loss: 16.482093811035156 = 1.8303571939468384 + 2.0 * 7.325868129730225
Epoch 70, val loss: 1.8405734300613403
Epoch 80, training loss: 16.028377532958984 = 1.821744441986084 + 2.0 * 7.103316307067871
Epoch 80, val loss: 1.8327746391296387
Epoch 90, training loss: 15.726311683654785 = 1.8090084791183472 + 2.0 * 6.958651542663574
Epoch 90, val loss: 1.820949912071228
Epoch 100, training loss: 15.507598876953125 = 1.7944000959396362 + 2.0 * 6.8565993309021
Epoch 100, val loss: 1.8079023361206055
Epoch 110, training loss: 15.302030563354492 = 1.7811135053634644 + 2.0 * 6.760458469390869
Epoch 110, val loss: 1.796174168586731
Epoch 120, training loss: 15.162117958068848 = 1.7687265872955322 + 2.0 * 6.696695804595947
Epoch 120, val loss: 1.7844659090042114
Epoch 130, training loss: 15.050156593322754 = 1.7548424005508423 + 2.0 * 6.6476569175720215
Epoch 130, val loss: 1.7713348865509033
Epoch 140, training loss: 14.960391998291016 = 1.7395330667495728 + 2.0 * 6.610429286956787
Epoch 140, val loss: 1.7574052810668945
Epoch 150, training loss: 14.884810447692871 = 1.7229546308517456 + 2.0 * 6.580927848815918
Epoch 150, val loss: 1.7426888942718506
Epoch 160, training loss: 14.81723403930664 = 1.7044873237609863 + 2.0 * 6.556373596191406
Epoch 160, val loss: 1.7266817092895508
Epoch 170, training loss: 14.754966735839844 = 1.6839677095413208 + 2.0 * 6.535499572753906
Epoch 170, val loss: 1.7091119289398193
Epoch 180, training loss: 14.699193954467773 = 1.6612626314163208 + 2.0 * 6.518965721130371
Epoch 180, val loss: 1.6898671388626099
Epoch 190, training loss: 14.64116382598877 = 1.6360809803009033 + 2.0 * 6.502541542053223
Epoch 190, val loss: 1.6686981916427612
Epoch 200, training loss: 14.58658218383789 = 1.6082556247711182 + 2.0 * 6.489163398742676
Epoch 200, val loss: 1.6453430652618408
Epoch 210, training loss: 14.53184700012207 = 1.57794988155365 + 2.0 * 6.4769487380981445
Epoch 210, val loss: 1.6197198629379272
Epoch 220, training loss: 14.474589347839355 = 1.5452808141708374 + 2.0 * 6.464654445648193
Epoch 220, val loss: 1.5921746492385864
Epoch 230, training loss: 14.418967247009277 = 1.5106170177459717 + 2.0 * 6.454174995422363
Epoch 230, val loss: 1.562940001487732
Epoch 240, training loss: 14.365497589111328 = 1.474061131477356 + 2.0 * 6.445718288421631
Epoch 240, val loss: 1.532251238822937
Epoch 250, training loss: 14.310760498046875 = 1.436150312423706 + 2.0 * 6.437304973602295
Epoch 250, val loss: 1.500422716140747
Epoch 260, training loss: 14.2545166015625 = 1.397571086883545 + 2.0 * 6.428472995758057
Epoch 260, val loss: 1.4682464599609375
Epoch 270, training loss: 14.198952674865723 = 1.3582640886306763 + 2.0 * 6.420344352722168
Epoch 270, val loss: 1.4356913566589355
Epoch 280, training loss: 14.149615287780762 = 1.318421721458435 + 2.0 * 6.415596961975098
Epoch 280, val loss: 1.4030537605285645
Epoch 290, training loss: 14.095070838928223 = 1.2786945104599 + 2.0 * 6.408188343048096
Epoch 290, val loss: 1.3708219528198242
Epoch 300, training loss: 14.047540664672852 = 1.2391514778137207 + 2.0 * 6.404194355010986
Epoch 300, val loss: 1.3392410278320312
Epoch 310, training loss: 13.996788024902344 = 1.2002482414245605 + 2.0 * 6.398270130157471
Epoch 310, val loss: 1.3083925247192383
Epoch 320, training loss: 13.944413185119629 = 1.1621971130371094 + 2.0 * 6.39110803604126
Epoch 320, val loss: 1.2785851955413818
Epoch 330, training loss: 13.898476600646973 = 1.125145673751831 + 2.0 * 6.386665344238281
Epoch 330, val loss: 1.2500507831573486
Epoch 340, training loss: 13.85904312133789 = 1.0893878936767578 + 2.0 * 6.384827613830566
Epoch 340, val loss: 1.222632884979248
Epoch 350, training loss: 13.811394691467285 = 1.0550178289413452 + 2.0 * 6.378188610076904
Epoch 350, val loss: 1.1966562271118164
Epoch 360, training loss: 13.76835823059082 = 1.021924376487732 + 2.0 * 6.3732171058654785
Epoch 360, val loss: 1.1718425750732422
Epoch 370, training loss: 13.733246803283691 = 0.9899055361747742 + 2.0 * 6.371670722961426
Epoch 370, val loss: 1.147867202758789
Epoch 380, training loss: 13.692610740661621 = 0.9589663147926331 + 2.0 * 6.366822242736816
Epoch 380, val loss: 1.124644160270691
Epoch 390, training loss: 13.651957511901855 = 0.9286956191062927 + 2.0 * 6.361630916595459
Epoch 390, val loss: 1.10191011428833
Epoch 400, training loss: 13.616080284118652 = 0.8986976742744446 + 2.0 * 6.358691215515137
Epoch 400, val loss: 1.0792715549468994
Epoch 410, training loss: 13.584511756896973 = 0.8688632845878601 + 2.0 * 6.357824325561523
Epoch 410, val loss: 1.056858777999878
Epoch 420, training loss: 13.54263687133789 = 0.8392744064331055 + 2.0 * 6.351681232452393
Epoch 420, val loss: 1.0344061851501465
Epoch 430, training loss: 13.507433891296387 = 0.8096030354499817 + 2.0 * 6.3489155769348145
Epoch 430, val loss: 1.0119267702102661
Epoch 440, training loss: 13.474072456359863 = 0.77982497215271 + 2.0 * 6.347123622894287
Epoch 440, val loss: 0.989547848701477
Epoch 450, training loss: 13.43766975402832 = 0.7502995133399963 + 2.0 * 6.343685150146484
Epoch 450, val loss: 0.9674240946769714
Epoch 460, training loss: 13.401351928710938 = 0.7209781408309937 + 2.0 * 6.340187072753906
Epoch 460, val loss: 0.9458242654800415
Epoch 470, training loss: 13.36836051940918 = 0.6921137571334839 + 2.0 * 6.338123321533203
Epoch 470, val loss: 0.9249802231788635
Epoch 480, training loss: 13.33508586883545 = 0.6638835668563843 + 2.0 * 6.335601329803467
Epoch 480, val loss: 0.9054136872291565
Epoch 490, training loss: 13.302871704101562 = 0.6367990970611572 + 2.0 * 6.333036422729492
Epoch 490, val loss: 0.8871824145317078
Epoch 500, training loss: 13.272795677185059 = 0.6105911135673523 + 2.0 * 6.33110237121582
Epoch 500, val loss: 0.8704463839530945
Epoch 510, training loss: 13.248441696166992 = 0.5853371620178223 + 2.0 * 6.331552028656006
Epoch 510, val loss: 0.8551809787750244
Epoch 520, training loss: 13.218420028686523 = 0.561040461063385 + 2.0 * 6.3286895751953125
Epoch 520, val loss: 0.8414880037307739
Epoch 530, training loss: 13.187737464904785 = 0.5378128290176392 + 2.0 * 6.324962139129639
Epoch 530, val loss: 0.8292944431304932
Epoch 540, training loss: 13.165428161621094 = 0.5154938697814941 + 2.0 * 6.324967384338379
Epoch 540, val loss: 0.8185034990310669
Epoch 550, training loss: 13.140844345092773 = 0.49408090114593506 + 2.0 * 6.3233819007873535
Epoch 550, val loss: 0.8090761303901672
Epoch 560, training loss: 13.111876487731934 = 0.4735676646232605 + 2.0 * 6.319154262542725
Epoch 560, val loss: 0.8008691072463989
Epoch 570, training loss: 13.087777137756348 = 0.4537994861602783 + 2.0 * 6.316988945007324
Epoch 570, val loss: 0.7938019633293152
Epoch 580, training loss: 13.070274353027344 = 0.43469226360321045 + 2.0 * 6.317790985107422
Epoch 580, val loss: 0.7876685857772827
Epoch 590, training loss: 13.053057670593262 = 0.4161461293697357 + 2.0 * 6.318455696105957
Epoch 590, val loss: 0.7824633717536926
Epoch 600, training loss: 13.023834228515625 = 0.3982144296169281 + 2.0 * 6.312809944152832
Epoch 600, val loss: 0.7781082987785339
Epoch 610, training loss: 13.000261306762695 = 0.3807941973209381 + 2.0 * 6.3097333908081055
Epoch 610, val loss: 0.7745170593261719
Epoch 620, training loss: 12.983750343322754 = 0.3637416660785675 + 2.0 * 6.310004234313965
Epoch 620, val loss: 0.7716109156608582
Epoch 630, training loss: 12.962275505065918 = 0.3470822870731354 + 2.0 * 6.307596683502197
Epoch 630, val loss: 0.7693403959274292
Epoch 640, training loss: 12.940104484558105 = 0.3308469355106354 + 2.0 * 6.304628849029541
Epoch 640, val loss: 0.7676910758018494
Epoch 650, training loss: 12.922087669372559 = 0.3149632513523102 + 2.0 * 6.303562164306641
Epoch 650, val loss: 0.7665770649909973
Epoch 660, training loss: 12.91426944732666 = 0.29945704340934753 + 2.0 * 6.307406425476074
Epoch 660, val loss: 0.7659326791763306
Epoch 670, training loss: 12.889310836791992 = 0.28431737422943115 + 2.0 * 6.302496910095215
Epoch 670, val loss: 0.7657763361930847
Epoch 680, training loss: 12.872024536132812 = 0.2697382867336273 + 2.0 * 6.301143169403076
Epoch 680, val loss: 0.766099214553833
Epoch 690, training loss: 12.854996681213379 = 0.2556152939796448 + 2.0 * 6.2996907234191895
Epoch 690, val loss: 0.7667729258537292
Epoch 700, training loss: 12.838022232055664 = 0.24213828146457672 + 2.0 * 6.297942161560059
Epoch 700, val loss: 0.7678272128105164
Epoch 710, training loss: 12.825263977050781 = 0.229250967502594 + 2.0 * 6.298006534576416
Epoch 710, val loss: 0.7692381739616394
Epoch 720, training loss: 12.805779457092285 = 0.21702133119106293 + 2.0 * 6.294379234313965
Epoch 720, val loss: 0.7709550857543945
Epoch 730, training loss: 12.796235084533691 = 0.2054113894701004 + 2.0 * 6.295412063598633
Epoch 730, val loss: 0.7729979157447815
Epoch 740, training loss: 12.785425186157227 = 0.19446615874767303 + 2.0 * 6.2954792976379395
Epoch 740, val loss: 0.7752399444580078
Epoch 750, training loss: 12.766533851623535 = 0.18415924906730652 + 2.0 * 6.291187286376953
Epoch 750, val loss: 0.7778072953224182
Epoch 760, training loss: 12.754074096679688 = 0.17445649206638336 + 2.0 * 6.289808750152588
Epoch 760, val loss: 0.780615508556366
Epoch 770, training loss: 12.75311279296875 = 0.16531062126159668 + 2.0 * 6.293900966644287
Epoch 770, val loss: 0.7836147546768188
Epoch 780, training loss: 12.742596626281738 = 0.1567346453666687 + 2.0 * 6.292931079864502
Epoch 780, val loss: 0.7868539094924927
Epoch 790, training loss: 12.723875045776367 = 0.14863066375255585 + 2.0 * 6.287621974945068
Epoch 790, val loss: 0.7903512716293335
Epoch 800, training loss: 12.711429595947266 = 0.14101792871952057 + 2.0 * 6.285205841064453
Epoch 800, val loss: 0.7940876483917236
Epoch 810, training loss: 12.705739974975586 = 0.13380956649780273 + 2.0 * 6.285965442657471
Epoch 810, val loss: 0.7980508804321289
Epoch 820, training loss: 12.695026397705078 = 0.12698481976985931 + 2.0 * 6.284020900726318
Epoch 820, val loss: 0.8021343946456909
Epoch 830, training loss: 12.687371253967285 = 0.12053942680358887 + 2.0 * 6.283415794372559
Epoch 830, val loss: 0.806576132774353
Epoch 840, training loss: 12.678845405578613 = 0.11443572491407394 + 2.0 * 6.282204627990723
Epoch 840, val loss: 0.8112407922744751
Epoch 850, training loss: 12.672003746032715 = 0.10865993052721024 + 2.0 * 6.28167200088501
Epoch 850, val loss: 0.8160184025764465
Epoch 860, training loss: 12.662074089050293 = 0.10319210588932037 + 2.0 * 6.279440879821777
Epoch 860, val loss: 0.8210569620132446
Epoch 870, training loss: 12.660367012023926 = 0.0980546697974205 + 2.0 * 6.281156063079834
Epoch 870, val loss: 0.8262889385223389
Epoch 880, training loss: 12.649115562438965 = 0.09318207949399948 + 2.0 * 6.2779669761657715
Epoch 880, val loss: 0.8315079212188721
Epoch 890, training loss: 12.643253326416016 = 0.08859553933143616 + 2.0 * 6.277328968048096
Epoch 890, val loss: 0.8371145725250244
Epoch 900, training loss: 12.644187927246094 = 0.0842621847987175 + 2.0 * 6.27996301651001
Epoch 900, val loss: 0.8427172899246216
Epoch 910, training loss: 12.629364967346191 = 0.0801931694149971 + 2.0 * 6.274585723876953
Epoch 910, val loss: 0.8484840989112854
Epoch 920, training loss: 12.622198104858398 = 0.07634942978620529 + 2.0 * 6.272924423217773
Epoch 920, val loss: 0.8543307185173035
Epoch 930, training loss: 12.627680778503418 = 0.07272852212190628 + 2.0 * 6.2774763107299805
Epoch 930, val loss: 0.860175371170044
Epoch 940, training loss: 12.616579055786133 = 0.06931550800800323 + 2.0 * 6.273631572723389
Epoch 940, val loss: 0.8661996722221375
Epoch 950, training loss: 12.609049797058105 = 0.06611354649066925 + 2.0 * 6.271468162536621
Epoch 950, val loss: 0.8722323179244995
Epoch 960, training loss: 12.611047744750977 = 0.06309127807617188 + 2.0 * 6.273978233337402
Epoch 960, val loss: 0.8783183097839355
Epoch 970, training loss: 12.599364280700684 = 0.06027020141482353 + 2.0 * 6.269546985626221
Epoch 970, val loss: 0.8843972086906433
Epoch 980, training loss: 12.594223976135254 = 0.05760335549712181 + 2.0 * 6.268310546875
Epoch 980, val loss: 0.8904536366462708
Epoch 990, training loss: 12.590633392333984 = 0.05510836839675903 + 2.0 * 6.267762660980225
Epoch 990, val loss: 0.896571695804596
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8222222222222223
0.8413284132841329
=== training gcn model ===
Epoch 0, training loss: 19.14825439453125 = 1.9545929431915283 + 2.0 * 8.596830368041992
Epoch 0, val loss: 1.9488192796707153
Epoch 10, training loss: 19.13711929321289 = 1.9440428018569946 + 2.0 * 8.596538543701172
Epoch 10, val loss: 1.938547134399414
Epoch 20, training loss: 19.119518280029297 = 1.9311740398406982 + 2.0 * 8.594172477722168
Epoch 20, val loss: 1.9254112243652344
Epoch 30, training loss: 19.06374740600586 = 1.9136227369308472 + 2.0 * 8.57506275177002
Epoch 30, val loss: 1.9072540998458862
Epoch 40, training loss: 18.786724090576172 = 1.891453742980957 + 2.0 * 8.447635650634766
Epoch 40, val loss: 1.8848822116851807
Epoch 50, training loss: 17.86640739440918 = 1.867281436920166 + 2.0 * 7.999562740325928
Epoch 50, val loss: 1.8611468076705933
Epoch 60, training loss: 16.871015548706055 = 1.8485182523727417 + 2.0 * 7.51124906539917
Epoch 60, val loss: 1.8432763814926147
Epoch 70, training loss: 16.06177520751953 = 1.835018277168274 + 2.0 * 7.113378524780273
Epoch 70, val loss: 1.8305047750473022
Epoch 80, training loss: 15.73258113861084 = 1.8214572668075562 + 2.0 * 6.955562114715576
Epoch 80, val loss: 1.8176378011703491
Epoch 90, training loss: 15.516331672668457 = 1.80544114112854 + 2.0 * 6.855445384979248
Epoch 90, val loss: 1.8026677370071411
Epoch 100, training loss: 15.363037109375 = 1.790216326713562 + 2.0 * 6.786410331726074
Epoch 100, val loss: 1.7883368730545044
Epoch 110, training loss: 15.218486785888672 = 1.7762081623077393 + 2.0 * 6.721139430999756
Epoch 110, val loss: 1.7750614881515503
Epoch 120, training loss: 15.100980758666992 = 1.7621415853500366 + 2.0 * 6.669419765472412
Epoch 120, val loss: 1.7615360021591187
Epoch 130, training loss: 15.007164001464844 = 1.7467291355133057 + 2.0 * 6.630217552185059
Epoch 130, val loss: 1.747008204460144
Epoch 140, training loss: 14.925847053527832 = 1.7296215295791626 + 2.0 * 6.5981125831604
Epoch 140, val loss: 1.7311980724334717
Epoch 150, training loss: 14.85308837890625 = 1.7108020782470703 + 2.0 * 6.57114315032959
Epoch 150, val loss: 1.7137386798858643
Epoch 160, training loss: 14.78776741027832 = 1.6898325681686401 + 2.0 * 6.548967361450195
Epoch 160, val loss: 1.6943020820617676
Epoch 170, training loss: 14.724651336669922 = 1.666043758392334 + 2.0 * 6.529304027557373
Epoch 170, val loss: 1.6724495887756348
Epoch 180, training loss: 14.665900230407715 = 1.6391757726669312 + 2.0 * 6.513362407684326
Epoch 180, val loss: 1.64774751663208
Epoch 190, training loss: 14.618303298950195 = 1.6086328029632568 + 2.0 * 6.50483512878418
Epoch 190, val loss: 1.6200132369995117
Epoch 200, training loss: 14.547261238098145 = 1.5745505094528198 + 2.0 * 6.486355304718018
Epoch 200, val loss: 1.5891300439834595
Epoch 210, training loss: 14.487680435180664 = 1.5365018844604492 + 2.0 * 6.475589275360107
Epoch 210, val loss: 1.5549136400222778
Epoch 220, training loss: 14.426956176757812 = 1.4941643476486206 + 2.0 * 6.466395854949951
Epoch 220, val loss: 1.517303705215454
Epoch 230, training loss: 14.364208221435547 = 1.4481309652328491 + 2.0 * 6.458038806915283
Epoch 230, val loss: 1.4766658544540405
Epoch 240, training loss: 14.297554969787598 = 1.398675799369812 + 2.0 * 6.449439525604248
Epoch 240, val loss: 1.4338825941085815
Epoch 250, training loss: 14.231537818908691 = 1.3463103771209717 + 2.0 * 6.44261360168457
Epoch 250, val loss: 1.3891997337341309
Epoch 260, training loss: 14.176794052124023 = 1.2921442985534668 + 2.0 * 6.442325115203857
Epoch 260, val loss: 1.3436399698257446
Epoch 270, training loss: 14.101332664489746 = 1.2378180027008057 + 2.0 * 6.43175745010376
Epoch 270, val loss: 1.2984459400177002
Epoch 280, training loss: 14.032873153686523 = 1.1839364767074585 + 2.0 * 6.424468517303467
Epoch 280, val loss: 1.2543784379959106
Epoch 290, training loss: 13.970680236816406 = 1.1312464475631714 + 2.0 * 6.419716835021973
Epoch 290, val loss: 1.2117589712142944
Epoch 300, training loss: 13.91906452178955 = 1.0808721780776978 + 2.0 * 6.419095993041992
Epoch 300, val loss: 1.1719460487365723
Epoch 310, training loss: 13.854533195495605 = 1.0340595245361328 + 2.0 * 6.410236835479736
Epoch 310, val loss: 1.1347877979278564
Epoch 320, training loss: 13.799306869506836 = 0.9899753332138062 + 2.0 * 6.404665946960449
Epoch 320, val loss: 1.1003069877624512
Epoch 330, training loss: 13.756686210632324 = 0.9484970569610596 + 2.0 * 6.404094696044922
Epoch 330, val loss: 1.0683331489562988
Epoch 340, training loss: 13.703681945800781 = 0.9100236892700195 + 2.0 * 6.396829128265381
Epoch 340, val loss: 1.0386955738067627
Epoch 350, training loss: 13.659310340881348 = 0.8739588856697083 + 2.0 * 6.392675876617432
Epoch 350, val loss: 1.0114116668701172
Epoch 360, training loss: 13.61745834350586 = 0.8399186730384827 + 2.0 * 6.388769626617432
Epoch 360, val loss: 0.9861153960227966
Epoch 370, training loss: 13.583717346191406 = 0.8077717423439026 + 2.0 * 6.387972831726074
Epoch 370, val loss: 0.9625304937362671
Epoch 380, training loss: 13.542038917541504 = 0.7774486541748047 + 2.0 * 6.38229513168335
Epoch 380, val loss: 0.9405167698860168
Epoch 390, training loss: 13.511371612548828 = 0.748502254486084 + 2.0 * 6.381434917449951
Epoch 390, val loss: 0.9197968244552612
Epoch 400, training loss: 13.47091007232666 = 0.7205989360809326 + 2.0 * 6.375155448913574
Epoch 400, val loss: 0.9003442525863647
Epoch 410, training loss: 13.438089370727539 = 0.6937029361724854 + 2.0 * 6.372193336486816
Epoch 410, val loss: 0.8818116784095764
Epoch 420, training loss: 13.404736518859863 = 0.6675348281860352 + 2.0 * 6.368600845336914
Epoch 420, val loss: 0.8643982410430908
Epoch 430, training loss: 13.373052597045898 = 0.6422374844551086 + 2.0 * 6.365407466888428
Epoch 430, val loss: 0.847870409488678
Epoch 440, training loss: 13.350130081176758 = 0.6175441145896912 + 2.0 * 6.366292953491211
Epoch 440, val loss: 0.8323952555656433
Epoch 450, training loss: 13.312817573547363 = 0.5936872959136963 + 2.0 * 6.359565258026123
Epoch 450, val loss: 0.8178488612174988
Epoch 460, training loss: 13.28847599029541 = 0.5704477429389954 + 2.0 * 6.35901403427124
Epoch 460, val loss: 0.8043155670166016
Epoch 470, training loss: 13.26156234741211 = 0.5480633974075317 + 2.0 * 6.356749534606934
Epoch 470, val loss: 0.7918044924736023
Epoch 480, training loss: 13.23068904876709 = 0.52643883228302 + 2.0 * 6.35212516784668
Epoch 480, val loss: 0.7803709506988525
Epoch 490, training loss: 13.206405639648438 = 0.5056148171424866 + 2.0 * 6.350395202636719
Epoch 490, val loss: 0.7699217200279236
Epoch 500, training loss: 13.180737495422363 = 0.4855426847934723 + 2.0 * 6.347597599029541
Epoch 500, val loss: 0.7605674862861633
Epoch 510, training loss: 13.157979011535645 = 0.46638908982276917 + 2.0 * 6.345795154571533
Epoch 510, val loss: 0.7521383762359619
Epoch 520, training loss: 13.132329940795898 = 0.44791892170906067 + 2.0 * 6.34220552444458
Epoch 520, val loss: 0.7447057366371155
Epoch 530, training loss: 13.112106323242188 = 0.4301455318927765 + 2.0 * 6.340980529785156
Epoch 530, val loss: 0.7381269931793213
Epoch 540, training loss: 13.10737133026123 = 0.4130111038684845 + 2.0 * 6.347179889678955
Epoch 540, val loss: 0.7323163151741028
Epoch 550, training loss: 13.077093124389648 = 0.39670148491859436 + 2.0 * 6.340195655822754
Epoch 550, val loss: 0.7271878123283386
Epoch 560, training loss: 13.05182933807373 = 0.3809276521205902 + 2.0 * 6.335450649261475
Epoch 560, val loss: 0.7228131890296936
Epoch 570, training loss: 13.03027629852295 = 0.3656791150569916 + 2.0 * 6.332298755645752
Epoch 570, val loss: 0.7190595865249634
Epoch 580, training loss: 13.014304161071777 = 0.3508567810058594 + 2.0 * 6.331723690032959
Epoch 580, val loss: 0.7158331274986267
Epoch 590, training loss: 12.995304107666016 = 0.33646076917648315 + 2.0 * 6.329421520233154
Epoch 590, val loss: 0.713128387928009
Epoch 600, training loss: 12.981348991394043 = 0.3225807547569275 + 2.0 * 6.3293843269348145
Epoch 600, val loss: 0.7109495401382446
Epoch 610, training loss: 12.961584091186523 = 0.3091016113758087 + 2.0 * 6.3262410163879395
Epoch 610, val loss: 0.7093377113342285
Epoch 620, training loss: 12.955379486083984 = 0.2960672974586487 + 2.0 * 6.32965612411499
Epoch 620, val loss: 0.7082481384277344
Epoch 630, training loss: 12.9341402053833 = 0.28350934386253357 + 2.0 * 6.325315475463867
Epoch 630, val loss: 0.7075701951980591
Epoch 640, training loss: 12.918046951293945 = 0.27133381366729736 + 2.0 * 6.323356628417969
Epoch 640, val loss: 0.7073582410812378
Epoch 650, training loss: 12.898978233337402 = 0.2595997750759125 + 2.0 * 6.3196892738342285
Epoch 650, val loss: 0.7076285481452942
Epoch 660, training loss: 12.884709358215332 = 0.24828346073627472 + 2.0 * 6.318212985992432
Epoch 660, val loss: 0.7083374857902527
Epoch 670, training loss: 12.874156951904297 = 0.237341970205307 + 2.0 * 6.3184075355529785
Epoch 670, val loss: 0.7094764113426208
Epoch 680, training loss: 12.858078956604004 = 0.22684776782989502 + 2.0 * 6.315615653991699
Epoch 680, val loss: 0.7109830379486084
Epoch 690, training loss: 12.850897789001465 = 0.21677635610103607 + 2.0 * 6.317060947418213
Epoch 690, val loss: 0.712799608707428
Epoch 700, training loss: 12.836508750915527 = 0.20713797211647034 + 2.0 * 6.314685344696045
Epoch 700, val loss: 0.7150223255157471
Epoch 710, training loss: 12.821434020996094 = 0.1979077160358429 + 2.0 * 6.311763286590576
Epoch 710, val loss: 0.7175211310386658
Epoch 720, training loss: 12.808582305908203 = 0.1890803724527359 + 2.0 * 6.309751033782959
Epoch 720, val loss: 0.720337450504303
Epoch 730, training loss: 12.80015754699707 = 0.18061597645282745 + 2.0 * 6.309770584106445
Epoch 730, val loss: 0.7234520316123962
Epoch 740, training loss: 12.793071746826172 = 0.17252075672149658 + 2.0 * 6.310275554656982
Epoch 740, val loss: 0.7268449664115906
Epoch 750, training loss: 12.784748077392578 = 0.16481269896030426 + 2.0 * 6.309967517852783
Epoch 750, val loss: 0.7304480075836182
Epoch 760, training loss: 12.76910400390625 = 0.15744301676750183 + 2.0 * 6.305830478668213
Epoch 760, val loss: 0.7342458367347717
Epoch 770, training loss: 12.760476112365723 = 0.15042850375175476 + 2.0 * 6.305023670196533
Epoch 770, val loss: 0.7382793426513672
Epoch 780, training loss: 12.763628005981445 = 0.1437518149614334 + 2.0 * 6.309937953948975
Epoch 780, val loss: 0.7425825595855713
Epoch 790, training loss: 12.742937088012695 = 0.13736703991889954 + 2.0 * 6.3027849197387695
Epoch 790, val loss: 0.7470016479492188
Epoch 800, training loss: 12.733553886413574 = 0.1313219964504242 + 2.0 * 6.301115989685059
Epoch 800, val loss: 0.7516234517097473
Epoch 810, training loss: 12.73770523071289 = 0.1255594789981842 + 2.0 * 6.30607271194458
Epoch 810, val loss: 0.756449282169342
Epoch 820, training loss: 12.722509384155273 = 0.12008197605609894 + 2.0 * 6.30121374130249
Epoch 820, val loss: 0.7614467740058899
Epoch 830, training loss: 12.712584495544434 = 0.11487964540719986 + 2.0 * 6.298852443695068
Epoch 830, val loss: 0.7665345072746277
Epoch 840, training loss: 12.722887992858887 = 0.10994747281074524 + 2.0 * 6.3064703941345215
Epoch 840, val loss: 0.7717874050140381
Epoch 850, training loss: 12.701475143432617 = 0.10523582249879837 + 2.0 * 6.29811954498291
Epoch 850, val loss: 0.7770304083824158
Epoch 860, training loss: 12.69237995147705 = 0.10078730434179306 + 2.0 * 6.2957963943481445
Epoch 860, val loss: 0.7824587821960449
Epoch 870, training loss: 12.700090408325195 = 0.0965452790260315 + 2.0 * 6.301772594451904
Epoch 870, val loss: 0.7880088686943054
Epoch 880, training loss: 12.684252738952637 = 0.09253488481044769 + 2.0 * 6.295858860015869
Epoch 880, val loss: 0.793540358543396
Epoch 890, training loss: 12.675335884094238 = 0.08871747553348541 + 2.0 * 6.293309211730957
Epoch 890, val loss: 0.7991745471954346
Epoch 900, training loss: 12.670679092407227 = 0.08509533107280731 + 2.0 * 6.292791843414307
Epoch 900, val loss: 0.8049500584602356
Epoch 910, training loss: 12.673260688781738 = 0.08164875954389572 + 2.0 * 6.295805931091309
Epoch 910, val loss: 0.8107283115386963
Epoch 920, training loss: 12.659090042114258 = 0.07837488502264023 + 2.0 * 6.29035758972168
Epoch 920, val loss: 0.8164113759994507
Epoch 930, training loss: 12.662508010864258 = 0.07526995241641998 + 2.0 * 6.293619155883789
Epoch 930, val loss: 0.8222641944885254
Epoch 940, training loss: 12.64947509765625 = 0.07233729958534241 + 2.0 * 6.28856897354126
Epoch 940, val loss: 0.8281944394111633
Epoch 950, training loss: 12.64719009399414 = 0.06954366713762283 + 2.0 * 6.288823127746582
Epoch 950, val loss: 0.8340079188346863
Epoch 960, training loss: 12.652382850646973 = 0.06688384711742401 + 2.0 * 6.292749404907227
Epoch 960, val loss: 0.8398849964141846
Epoch 970, training loss: 12.646154403686523 = 0.06436163932085037 + 2.0 * 6.290896415710449
Epoch 970, val loss: 0.8456728458404541
Epoch 980, training loss: 12.631908416748047 = 0.06196451187133789 + 2.0 * 6.284971714019775
Epoch 980, val loss: 0.8515259027481079
Epoch 990, training loss: 12.6300630569458 = 0.059684816747903824 + 2.0 * 6.285189151763916
Epoch 990, val loss: 0.8573483824729919
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8074074074074075
0.8397469688982605
The final CL Acc:0.81235, 0.00698, The final GNN Acc:0.83852, 0.00293
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.4 der2:0.3 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11538])
remove edge: torch.Size([2, 9540])
updated graph: torch.Size([2, 10522])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 19.142250061035156 = 1.9485995769500732 + 2.0 * 8.59682559967041
Epoch 0, val loss: 1.94963538646698
Epoch 10, training loss: 19.131227493286133 = 1.9382802248001099 + 2.0 * 8.596473693847656
Epoch 10, val loss: 1.9398218393325806
Epoch 20, training loss: 19.11299705505371 = 1.9253422021865845 + 2.0 * 8.593827247619629
Epoch 20, val loss: 1.926819086074829
Epoch 30, training loss: 19.057228088378906 = 1.9074323177337646 + 2.0 * 8.574897766113281
Epoch 30, val loss: 1.9082725048065186
Epoch 40, training loss: 18.811269760131836 = 1.8853538036346436 + 2.0 * 8.462958335876465
Epoch 40, val loss: 1.8858760595321655
Epoch 50, training loss: 17.879669189453125 = 1.8624945878982544 + 2.0 * 8.008586883544922
Epoch 50, val loss: 1.8626877069473267
Epoch 60, training loss: 16.885757446289062 = 1.8445312976837158 + 2.0 * 7.520613193511963
Epoch 60, val loss: 1.8456628322601318
Epoch 70, training loss: 16.135448455810547 = 1.8333346843719482 + 2.0 * 7.15105676651001
Epoch 70, val loss: 1.8346717357635498
Epoch 80, training loss: 15.67080307006836 = 1.8235231637954712 + 2.0 * 6.92363977432251
Epoch 80, val loss: 1.8248811960220337
Epoch 90, training loss: 15.474782943725586 = 1.8122118711471558 + 2.0 * 6.83128547668457
Epoch 90, val loss: 1.8132555484771729
Epoch 100, training loss: 15.342522621154785 = 1.7994186878204346 + 2.0 * 6.771552085876465
Epoch 100, val loss: 1.800133228302002
Epoch 110, training loss: 15.2338228225708 = 1.787541389465332 + 2.0 * 6.723140716552734
Epoch 110, val loss: 1.7884585857391357
Epoch 120, training loss: 15.144403457641602 = 1.7774691581726074 + 2.0 * 6.683466911315918
Epoch 120, val loss: 1.7786316871643066
Epoch 130, training loss: 15.068180084228516 = 1.7679396867752075 + 2.0 * 6.650120258331299
Epoch 130, val loss: 1.7692681550979614
Epoch 140, training loss: 14.99933910369873 = 1.7577608823776245 + 2.0 * 6.620789051055908
Epoch 140, val loss: 1.7594306468963623
Epoch 150, training loss: 14.936018943786621 = 1.7466237545013428 + 2.0 * 6.59469747543335
Epoch 150, val loss: 1.74906325340271
Epoch 160, training loss: 14.877923965454102 = 1.7344114780426025 + 2.0 * 6.571756362915039
Epoch 160, val loss: 1.7380986213684082
Epoch 170, training loss: 14.825021743774414 = 1.720866084098816 + 2.0 * 6.552077770233154
Epoch 170, val loss: 1.7262256145477295
Epoch 180, training loss: 14.768327713012695 = 1.7057442665100098 + 2.0 * 6.531291484832764
Epoch 180, val loss: 1.713024377822876
Epoch 190, training loss: 14.716658592224121 = 1.6886978149414062 + 2.0 * 6.513980388641357
Epoch 190, val loss: 1.6982805728912354
Epoch 200, training loss: 14.675433158874512 = 1.6692982912063599 + 2.0 * 6.503067493438721
Epoch 200, val loss: 1.6817072629928589
Epoch 210, training loss: 14.621661186218262 = 1.6476027965545654 + 2.0 * 6.487029075622559
Epoch 210, val loss: 1.663215160369873
Epoch 220, training loss: 14.573166847229004 = 1.623390793800354 + 2.0 * 6.474887847900391
Epoch 220, val loss: 1.642759919166565
Epoch 230, training loss: 14.524763107299805 = 1.596321940422058 + 2.0 * 6.4642205238342285
Epoch 230, val loss: 1.619975209236145
Epoch 240, training loss: 14.4805326461792 = 1.5663635730743408 + 2.0 * 6.457084655761719
Epoch 240, val loss: 1.5948001146316528
Epoch 250, training loss: 14.427510261535645 = 1.533695936203003 + 2.0 * 6.446907043457031
Epoch 250, val loss: 1.5676089525222778
Epoch 260, training loss: 14.373988151550293 = 1.498183012008667 + 2.0 * 6.437902450561523
Epoch 260, val loss: 1.5383555889129639
Epoch 270, training loss: 14.322000503540039 = 1.4598748683929443 + 2.0 * 6.431062698364258
Epoch 270, val loss: 1.5070905685424805
Epoch 280, training loss: 14.268270492553711 = 1.4191362857818604 + 2.0 * 6.424567222595215
Epoch 280, val loss: 1.474135398864746
Epoch 290, training loss: 14.213229179382324 = 1.3764981031417847 + 2.0 * 6.418365478515625
Epoch 290, val loss: 1.44011390209198
Epoch 300, training loss: 14.163483619689941 = 1.332655668258667 + 2.0 * 6.415413856506348
Epoch 300, val loss: 1.4054335355758667
Epoch 310, training loss: 14.102374076843262 = 1.2881791591644287 + 2.0 * 6.407097339630127
Epoch 310, val loss: 1.3706790208816528
Epoch 320, training loss: 14.046951293945312 = 1.2433491945266724 + 2.0 * 6.401801109313965
Epoch 320, val loss: 1.3360368013381958
Epoch 330, training loss: 13.999429702758789 = 1.198503851890564 + 2.0 * 6.400463104248047
Epoch 330, val loss: 1.3017044067382812
Epoch 340, training loss: 13.943760871887207 = 1.1549036502838135 + 2.0 * 6.394428730010986
Epoch 340, val loss: 1.2686395645141602
Epoch 350, training loss: 13.88974380493164 = 1.1125538349151611 + 2.0 * 6.388595104217529
Epoch 350, val loss: 1.2369964122772217
Epoch 360, training loss: 13.84031867980957 = 1.07157564163208 + 2.0 * 6.384371757507324
Epoch 360, val loss: 1.206649661064148
Epoch 370, training loss: 13.793943405151367 = 1.0319373607635498 + 2.0 * 6.381002902984619
Epoch 370, val loss: 1.177735447883606
Epoch 380, training loss: 13.759918212890625 = 0.9939346313476562 + 2.0 * 6.382991790771484
Epoch 380, val loss: 1.1503939628601074
Epoch 390, training loss: 13.707977294921875 = 0.9578977227210999 + 2.0 * 6.375039577484131
Epoch 390, val loss: 1.124904990196228
Epoch 400, training loss: 13.664451599121094 = 0.9233862161636353 + 2.0 * 6.370532512664795
Epoch 400, val loss: 1.1009994745254517
Epoch 410, training loss: 13.627104759216309 = 0.8901737332344055 + 2.0 * 6.368465423583984
Epoch 410, val loss: 1.0784063339233398
Epoch 420, training loss: 13.598773002624512 = 0.8583660125732422 + 2.0 * 6.370203495025635
Epoch 420, val loss: 1.0574806928634644
Epoch 430, training loss: 13.554649353027344 = 0.8281364440917969 + 2.0 * 6.363256454467773
Epoch 430, val loss: 1.0379048585891724
Epoch 440, training loss: 13.5169095993042 = 0.799057126045227 + 2.0 * 6.358926296234131
Epoch 440, val loss: 1.01961088180542
Epoch 450, training loss: 13.48314094543457 = 0.7709457874298096 + 2.0 * 6.35609769821167
Epoch 450, val loss: 1.002484679222107
Epoch 460, training loss: 13.456067085266113 = 0.7438570857048035 + 2.0 * 6.356104850769043
Epoch 460, val loss: 0.9865326881408691
Epoch 470, training loss: 13.422993659973145 = 0.7179266214370728 + 2.0 * 6.352533340454102
Epoch 470, val loss: 0.9718158841133118
Epoch 480, training loss: 13.390420913696289 = 0.6929574012756348 + 2.0 * 6.348731994628906
Epoch 480, val loss: 0.9582171440124512
Epoch 490, training loss: 13.37277889251709 = 0.6688911318778992 + 2.0 * 6.3519439697265625
Epoch 490, val loss: 0.9457286596298218
Epoch 500, training loss: 13.335599899291992 = 0.6458210945129395 + 2.0 * 6.3448896408081055
Epoch 500, val loss: 0.9344354271888733
Epoch 510, training loss: 13.307345390319824 = 0.6235886216163635 + 2.0 * 6.341878414154053
Epoch 510, val loss: 0.9241001605987549
Epoch 520, training loss: 13.290139198303223 = 0.6020472049713135 + 2.0 * 6.344046115875244
Epoch 520, val loss: 0.9146887063980103
Epoch 530, training loss: 13.262990951538086 = 0.5812787413597107 + 2.0 * 6.340856075286865
Epoch 530, val loss: 0.9062051773071289
Epoch 540, training loss: 13.238022804260254 = 0.5611918568611145 + 2.0 * 6.338415622711182
Epoch 540, val loss: 0.8985569477081299
Epoch 550, training loss: 13.20901870727539 = 0.5416799187660217 + 2.0 * 6.333669185638428
Epoch 550, val loss: 0.8915789127349854
Epoch 560, training loss: 13.187349319458008 = 0.5225960612297058 + 2.0 * 6.332376480102539
Epoch 560, val loss: 0.8852327466011047
Epoch 570, training loss: 13.17177963256836 = 0.5039321780204773 + 2.0 * 6.333923816680908
Epoch 570, val loss: 0.8794967532157898
Epoch 580, training loss: 13.149681091308594 = 0.48572441935539246 + 2.0 * 6.3319783210754395
Epoch 580, val loss: 0.8742194771766663
Epoch 590, training loss: 13.122661590576172 = 0.46779003739356995 + 2.0 * 6.3274359703063965
Epoch 590, val loss: 0.8694049715995789
Epoch 600, training loss: 13.099716186523438 = 0.44999074935913086 + 2.0 * 6.324862480163574
Epoch 600, val loss: 0.8649700880050659
Epoch 610, training loss: 13.104974746704102 = 0.4323180615901947 + 2.0 * 6.336328506469727
Epoch 610, val loss: 0.8607935309410095
Epoch 620, training loss: 13.058563232421875 = 0.4148692488670349 + 2.0 * 6.321846961975098
Epoch 620, val loss: 0.8568926453590393
Epoch 630, training loss: 13.038737297058105 = 0.3976536691188812 + 2.0 * 6.320541858673096
Epoch 630, val loss: 0.8533156514167786
Epoch 640, training loss: 13.01840591430664 = 0.3805416524410248 + 2.0 * 6.318932056427002
Epoch 640, val loss: 0.8500791788101196
Epoch 650, training loss: 13.001358032226562 = 0.36352190375328064 + 2.0 * 6.318918228149414
Epoch 650, val loss: 0.8471733331680298
Epoch 660, training loss: 12.997218132019043 = 0.3468046486377716 + 2.0 * 6.325206756591797
Epoch 660, val loss: 0.8444884419441223
Epoch 670, training loss: 12.966883659362793 = 0.33046212792396545 + 2.0 * 6.318210601806641
Epoch 670, val loss: 0.842326819896698
Epoch 680, training loss: 12.943645477294922 = 0.314485102891922 + 2.0 * 6.314579963684082
Epoch 680, val loss: 0.8407344222068787
Epoch 690, training loss: 12.941764831542969 = 0.2988874316215515 + 2.0 * 6.321438789367676
Epoch 690, val loss: 0.8396005630493164
Epoch 700, training loss: 12.913002014160156 = 0.283907413482666 + 2.0 * 6.314547538757324
Epoch 700, val loss: 0.8387651443481445
Epoch 710, training loss: 12.892745018005371 = 0.2694929540157318 + 2.0 * 6.311625957489014
Epoch 710, val loss: 0.8385389447212219
Epoch 720, training loss: 12.874137878417969 = 0.2556860148906708 + 2.0 * 6.309226036071777
Epoch 720, val loss: 0.8388863205909729
Epoch 730, training loss: 12.875497817993164 = 0.24253380298614502 + 2.0 * 6.316482067108154
Epoch 730, val loss: 0.8398135900497437
Epoch 740, training loss: 12.850282669067383 = 0.2301257997751236 + 2.0 * 6.3100786209106445
Epoch 740, val loss: 0.8409942388534546
Epoch 750, training loss: 12.831389427185059 = 0.21839165687561035 + 2.0 * 6.306499004364014
Epoch 750, val loss: 0.8427680134773254
Epoch 760, training loss: 12.81707763671875 = 0.20727601647377014 + 2.0 * 6.304900646209717
Epoch 760, val loss: 0.8449716567993164
Epoch 770, training loss: 12.805558204650879 = 0.19675153493881226 + 2.0 * 6.304403305053711
Epoch 770, val loss: 0.8475591540336609
Epoch 780, training loss: 12.794390678405762 = 0.18685787916183472 + 2.0 * 6.303766250610352
Epoch 780, val loss: 0.850555956363678
Epoch 790, training loss: 12.787195205688477 = 0.17758572101593018 + 2.0 * 6.304804801940918
Epoch 790, val loss: 0.8537168502807617
Epoch 800, training loss: 12.770532608032227 = 0.16886186599731445 + 2.0 * 6.300835132598877
Epoch 800, val loss: 0.8573881387710571
Epoch 810, training loss: 12.759953498840332 = 0.16061896085739136 + 2.0 * 6.2996673583984375
Epoch 810, val loss: 0.8612943291664124
Epoch 820, training loss: 12.76982593536377 = 0.1528540998697281 + 2.0 * 6.308485984802246
Epoch 820, val loss: 0.8654076457023621
Epoch 830, training loss: 12.749848365783691 = 0.14552384614944458 + 2.0 * 6.302162170410156
Epoch 830, val loss: 0.8697955012321472
Epoch 840, training loss: 12.733694076538086 = 0.13867197930812836 + 2.0 * 6.297511100769043
Epoch 840, val loss: 0.8743321299552917
Epoch 850, training loss: 12.726208686828613 = 0.13220223784446716 + 2.0 * 6.297003269195557
Epoch 850, val loss: 0.8791058659553528
Epoch 860, training loss: 12.716293334960938 = 0.1260780543088913 + 2.0 * 6.295107841491699
Epoch 860, val loss: 0.8840430974960327
Epoch 870, training loss: 12.714751243591309 = 0.12029097229242325 + 2.0 * 6.297230243682861
Epoch 870, val loss: 0.8891257047653198
Epoch 880, training loss: 12.712372779846191 = 0.11484072357416153 + 2.0 * 6.298766136169434
Epoch 880, val loss: 0.8943533301353455
Epoch 890, training loss: 12.698953628540039 = 0.10971152782440186 + 2.0 * 6.294620990753174
Epoch 890, val loss: 0.8995881080627441
Epoch 900, training loss: 12.687026023864746 = 0.10487561672925949 + 2.0 * 6.291075229644775
Epoch 900, val loss: 0.9049382209777832
Epoch 910, training loss: 12.680438041687012 = 0.10029434412717819 + 2.0 * 6.290071964263916
Epoch 910, val loss: 0.9104724526405334
Epoch 920, training loss: 12.689781188964844 = 0.09595083445310593 + 2.0 * 6.296915054321289
Epoch 920, val loss: 0.9160884022712708
Epoch 930, training loss: 12.681001663208008 = 0.09186175465583801 + 2.0 * 6.294569969177246
Epoch 930, val loss: 0.9216742515563965
Epoch 940, training loss: 12.666232109069824 = 0.08801120519638062 + 2.0 * 6.2891106605529785
Epoch 940, val loss: 0.9272755980491638
Epoch 950, training loss: 12.663333892822266 = 0.08436941355466843 + 2.0 * 6.289482116699219
Epoch 950, val loss: 0.9330052137374878
Epoch 960, training loss: 12.65466594696045 = 0.08091145008802414 + 2.0 * 6.286877155303955
Epoch 960, val loss: 0.9387551546096802
Epoch 970, training loss: 12.654860496520996 = 0.07763317972421646 + 2.0 * 6.288613796234131
Epoch 970, val loss: 0.9445938467979431
Epoch 980, training loss: 12.649621963500977 = 0.07453392446041107 + 2.0 * 6.287544250488281
Epoch 980, val loss: 0.9503399729728699
Epoch 990, training loss: 12.65036678314209 = 0.07160478830337524 + 2.0 * 6.28938102722168
Epoch 990, val loss: 0.9562928080558777
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7407407407407408
0.8044280442804429
=== training gcn model ===
Epoch 0, training loss: 19.121776580810547 = 1.928174376487732 + 2.0 * 8.596800804138184
Epoch 0, val loss: 1.9222394227981567
Epoch 10, training loss: 19.111083984375 = 1.9183225631713867 + 2.0 * 8.596381187438965
Epoch 10, val loss: 1.9116992950439453
Epoch 20, training loss: 19.092512130737305 = 1.9061481952667236 + 2.0 * 8.593181610107422
Epoch 20, val loss: 1.8985573053359985
Epoch 30, training loss: 19.03493881225586 = 1.8898669481277466 + 2.0 * 8.572535514831543
Epoch 30, val loss: 1.8808847665786743
Epoch 40, training loss: 18.802154541015625 = 1.8711328506469727 + 2.0 * 8.465511322021484
Epoch 40, val loss: 1.8614784479141235
Epoch 50, training loss: 18.13575553894043 = 1.8518987894058228 + 2.0 * 8.141928672790527
Epoch 50, val loss: 1.841845154762268
Epoch 60, training loss: 17.642641067504883 = 1.8352482318878174 + 2.0 * 7.903696060180664
Epoch 60, val loss: 1.825817346572876
Epoch 70, training loss: 16.665454864501953 = 1.8225542306900024 + 2.0 * 7.421450614929199
Epoch 70, val loss: 1.8144237995147705
Epoch 80, training loss: 15.987951278686523 = 1.812833547592163 + 2.0 * 7.087558746337891
Epoch 80, val loss: 1.8059409856796265
Epoch 90, training loss: 15.677886962890625 = 1.804434061050415 + 2.0 * 6.9367265701293945
Epoch 90, val loss: 1.7979787588119507
Epoch 100, training loss: 15.51601791381836 = 1.7943092584609985 + 2.0 * 6.860854148864746
Epoch 100, val loss: 1.7881170511245728
Epoch 110, training loss: 15.371832847595215 = 1.7836687564849854 + 2.0 * 6.794082164764404
Epoch 110, val loss: 1.778403401374817
Epoch 120, training loss: 15.243733406066895 = 1.7744419574737549 + 2.0 * 6.734645843505859
Epoch 120, val loss: 1.7700351476669312
Epoch 130, training loss: 15.130742073059082 = 1.7652795314788818 + 2.0 * 6.6827311515808105
Epoch 130, val loss: 1.7615348100662231
Epoch 140, training loss: 15.036593437194824 = 1.754922866821289 + 2.0 * 6.640835285186768
Epoch 140, val loss: 1.7520158290863037
Epoch 150, training loss: 14.961155891418457 = 1.742847204208374 + 2.0 * 6.609154224395752
Epoch 150, val loss: 1.7411119937896729
Epoch 160, training loss: 14.893009185791016 = 1.7289354801177979 + 2.0 * 6.582036972045898
Epoch 160, val loss: 1.7285702228546143
Epoch 170, training loss: 14.82870864868164 = 1.7130423784255981 + 2.0 * 6.557833194732666
Epoch 170, val loss: 1.7143222093582153
Epoch 180, training loss: 14.769052505493164 = 1.69504976272583 + 2.0 * 6.537001609802246
Epoch 180, val loss: 1.6980770826339722
Epoch 190, training loss: 14.713102340698242 = 1.6746774911880493 + 2.0 * 6.519212245941162
Epoch 190, val loss: 1.6796531677246094
Epoch 200, training loss: 14.656997680664062 = 1.6517126560211182 + 2.0 * 6.502642631530762
Epoch 200, val loss: 1.6589574813842773
Epoch 210, training loss: 14.604743957519531 = 1.625960111618042 + 2.0 * 6.489391803741455
Epoch 210, val loss: 1.6358752250671387
Epoch 220, training loss: 14.551959991455078 = 1.5974348783493042 + 2.0 * 6.477262496948242
Epoch 220, val loss: 1.6102755069732666
Epoch 230, training loss: 14.497060775756836 = 1.5663169622421265 + 2.0 * 6.465372085571289
Epoch 230, val loss: 1.5824553966522217
Epoch 240, training loss: 14.446361541748047 = 1.5328106880187988 + 2.0 * 6.456775665283203
Epoch 240, val loss: 1.5527801513671875
Epoch 250, training loss: 14.39161491394043 = 1.4974184036254883 + 2.0 * 6.447098255157471
Epoch 250, val loss: 1.5216156244277954
Epoch 260, training loss: 14.339491844177246 = 1.4604660272598267 + 2.0 * 6.439512729644775
Epoch 260, val loss: 1.4894938468933105
Epoch 270, training loss: 14.282543182373047 = 1.4227116107940674 + 2.0 * 6.429915904998779
Epoch 270, val loss: 1.4568506479263306
Epoch 280, training loss: 14.22955322265625 = 1.3841197490692139 + 2.0 * 6.4227166175842285
Epoch 280, val loss: 1.42404305934906
Epoch 290, training loss: 14.183191299438477 = 1.344996690750122 + 2.0 * 6.419097423553467
Epoch 290, val loss: 1.391424298286438
Epoch 300, training loss: 14.129559516906738 = 1.3059967756271362 + 2.0 * 6.411781311035156
Epoch 300, val loss: 1.3595330715179443
Epoch 310, training loss: 14.075281143188477 = 1.267444372177124 + 2.0 * 6.403918266296387
Epoch 310, val loss: 1.3285144567489624
Epoch 320, training loss: 14.02833080291748 = 1.229162573814392 + 2.0 * 6.3995842933654785
Epoch 320, val loss: 1.2982991933822632
Epoch 330, training loss: 13.97868537902832 = 1.1911200284957886 + 2.0 * 6.393782615661621
Epoch 330, val loss: 1.2689738273620605
Epoch 340, training loss: 13.931719779968262 = 1.1533459424972534 + 2.0 * 6.389186859130859
Epoch 340, val loss: 1.2403110265731812
Epoch 350, training loss: 13.89078426361084 = 1.1157537698745728 + 2.0 * 6.387515068054199
Epoch 350, val loss: 1.2122459411621094
Epoch 360, training loss: 13.84402847290039 = 1.0785855054855347 + 2.0 * 6.382721424102783
Epoch 360, val loss: 1.1849160194396973
Epoch 370, training loss: 13.796952247619629 = 1.0417566299438477 + 2.0 * 6.377597808837891
Epoch 370, val loss: 1.1581621170043945
Epoch 380, training loss: 13.761702537536621 = 1.0053532123565674 + 2.0 * 6.378174781799316
Epoch 380, val loss: 1.1319655179977417
Epoch 390, training loss: 13.712355613708496 = 0.969822108745575 + 2.0 * 6.371266841888428
Epoch 390, val loss: 1.1065059900283813
Epoch 400, training loss: 13.668228149414062 = 0.9349329471588135 + 2.0 * 6.366647720336914
Epoch 400, val loss: 1.0818841457366943
Epoch 410, training loss: 13.631933212280273 = 0.9007214307785034 + 2.0 * 6.36560583114624
Epoch 410, val loss: 1.058158040046692
Epoch 420, training loss: 13.601645469665527 = 0.8679112195968628 + 2.0 * 6.3668670654296875
Epoch 420, val loss: 1.035216212272644
Epoch 430, training loss: 13.553718566894531 = 0.8363643884658813 + 2.0 * 6.358676910400391
Epoch 430, val loss: 1.0139236450195312
Epoch 440, training loss: 13.516270637512207 = 0.8060458302497864 + 2.0 * 6.355112552642822
Epoch 440, val loss: 0.9937673807144165
Epoch 450, training loss: 13.48484992980957 = 0.7769171595573425 + 2.0 * 6.353966236114502
Epoch 450, val loss: 0.9748845100402832
Epoch 460, training loss: 13.450716018676758 = 0.749102771282196 + 2.0 * 6.350806713104248
Epoch 460, val loss: 0.9573096632957458
Epoch 470, training loss: 13.419013977050781 = 0.7225643396377563 + 2.0 * 6.348224639892578
Epoch 470, val loss: 0.94106525182724
Epoch 480, training loss: 13.399948120117188 = 0.697287917137146 + 2.0 * 6.351330280303955
Epoch 480, val loss: 0.9261733293533325
Epoch 490, training loss: 13.36340045928955 = 0.6733192801475525 + 2.0 * 6.345040798187256
Epoch 490, val loss: 0.912632405757904
Epoch 500, training loss: 13.336894035339355 = 0.6504599452018738 + 2.0 * 6.343216896057129
Epoch 500, val loss: 0.9004942178726196
Epoch 510, training loss: 13.30780029296875 = 0.6287403702735901 + 2.0 * 6.339529991149902
Epoch 510, val loss: 0.8894779086112976
Epoch 520, training loss: 13.283404350280762 = 0.607940673828125 + 2.0 * 6.337731838226318
Epoch 520, val loss: 0.8795922994613647
Epoch 530, training loss: 13.264782905578613 = 0.5880067348480225 + 2.0 * 6.338387966156006
Epoch 530, val loss: 0.8705684542655945
Epoch 540, training loss: 13.237836837768555 = 0.5689702033996582 + 2.0 * 6.334433078765869
Epoch 540, val loss: 0.8626105785369873
Epoch 550, training loss: 13.214824676513672 = 0.5506917238235474 + 2.0 * 6.332066535949707
Epoch 550, val loss: 0.8552021980285645
Epoch 560, training loss: 13.19941520690918 = 0.5329151749610901 + 2.0 * 6.333250045776367
Epoch 560, val loss: 0.8485900163650513
Epoch 570, training loss: 13.174325942993164 = 0.5156615376472473 + 2.0 * 6.32933235168457
Epoch 570, val loss: 0.8426499962806702
Epoch 580, training loss: 13.158045768737793 = 0.49894124269485474 + 2.0 * 6.329552173614502
Epoch 580, val loss: 0.8368831276893616
Epoch 590, training loss: 13.13353157043457 = 0.4826059341430664 + 2.0 * 6.325462818145752
Epoch 590, val loss: 0.8318043351173401
Epoch 600, training loss: 13.114291191101074 = 0.4666247069835663 + 2.0 * 6.323833465576172
Epoch 600, val loss: 0.8269650936126709
Epoch 610, training loss: 13.097980499267578 = 0.4509049952030182 + 2.0 * 6.323537826538086
Epoch 610, val loss: 0.8225338459014893
Epoch 620, training loss: 13.100751876831055 = 0.4354345500469208 + 2.0 * 6.332658767700195
Epoch 620, val loss: 0.8184300065040588
Epoch 630, training loss: 13.060211181640625 = 0.42037853598594666 + 2.0 * 6.319916248321533
Epoch 630, val loss: 0.8146583437919617
Epoch 640, training loss: 13.040950775146484 = 0.4055035710334778 + 2.0 * 6.317723751068115
Epoch 640, val loss: 0.8110528588294983
Epoch 650, training loss: 13.022459030151367 = 0.3907884955406189 + 2.0 * 6.315835475921631
Epoch 650, val loss: 0.8078160285949707
Epoch 660, training loss: 13.003519058227539 = 0.376176655292511 + 2.0 * 6.313671112060547
Epoch 660, val loss: 0.8048577904701233
Epoch 670, training loss: 13.022416114807129 = 0.36169925332069397 + 2.0 * 6.330358505249023
Epoch 670, val loss: 0.8022890090942383
Epoch 680, training loss: 12.982755661010742 = 0.34765395522117615 + 2.0 * 6.3175506591796875
Epoch 680, val loss: 0.7996870279312134
Epoch 690, training loss: 12.957856178283691 = 0.33389729261398315 + 2.0 * 6.311979293823242
Epoch 690, val loss: 0.7972886562347412
Epoch 700, training loss: 12.938648223876953 = 0.32041630148887634 + 2.0 * 6.309115886688232
Epoch 700, val loss: 0.7952201962471008
Epoch 710, training loss: 12.923005104064941 = 0.3071899712085724 + 2.0 * 6.307907581329346
Epoch 710, val loss: 0.7936453223228455
Epoch 720, training loss: 12.907153129577637 = 0.29422274231910706 + 2.0 * 6.306465148925781
Epoch 720, val loss: 0.7923225164413452
Epoch 730, training loss: 12.902459144592285 = 0.2815786302089691 + 2.0 * 6.3104400634765625
Epoch 730, val loss: 0.7913843393325806
Epoch 740, training loss: 12.898524284362793 = 0.26939600706100464 + 2.0 * 6.314564228057861
Epoch 740, val loss: 0.7907114624977112
Epoch 750, training loss: 12.869935035705566 = 0.2576966881752014 + 2.0 * 6.306118965148926
Epoch 750, val loss: 0.7902538180351257
Epoch 760, training loss: 12.852048873901367 = 0.2464018017053604 + 2.0 * 6.302823543548584
Epoch 760, val loss: 0.7902529239654541
Epoch 770, training loss: 12.837995529174805 = 0.2355070561170578 + 2.0 * 6.301244258880615
Epoch 770, val loss: 0.7906361222267151
Epoch 780, training loss: 12.827827453613281 = 0.2249886393547058 + 2.0 * 6.301419258117676
Epoch 780, val loss: 0.7914242744445801
Epoch 790, training loss: 12.815489768981934 = 0.21489550173282623 + 2.0 * 6.300297260284424
Epoch 790, val loss: 0.7924726009368896
Epoch 800, training loss: 12.805459022521973 = 0.2052493542432785 + 2.0 * 6.30010461807251
Epoch 800, val loss: 0.7939512729644775
Epoch 810, training loss: 12.794061660766602 = 0.1960332840681076 + 2.0 * 6.299014091491699
Epoch 810, val loss: 0.7955840229988098
Epoch 820, training loss: 12.782995223999023 = 0.18721731007099152 + 2.0 * 6.29788875579834
Epoch 820, val loss: 0.7976738214492798
Epoch 830, training loss: 12.771984100341797 = 0.17880170047283173 + 2.0 * 6.296591281890869
Epoch 830, val loss: 0.7999793291091919
Epoch 840, training loss: 12.759774208068848 = 0.17076772451400757 + 2.0 * 6.294503211975098
Epoch 840, val loss: 0.8024964928627014
Epoch 850, training loss: 12.757977485656738 = 0.16310492157936096 + 2.0 * 6.297436237335205
Epoch 850, val loss: 0.8052012324333191
Epoch 860, training loss: 12.740638732910156 = 0.15580657124519348 + 2.0 * 6.292416095733643
Epoch 860, val loss: 0.8081420660018921
Epoch 870, training loss: 12.731287956237793 = 0.14885734021663666 + 2.0 * 6.291215419769287
Epoch 870, val loss: 0.8111651539802551
Epoch 880, training loss: 12.746689796447754 = 0.14223098754882812 + 2.0 * 6.302229404449463
Epoch 880, val loss: 0.8143854141235352
Epoch 890, training loss: 12.726158142089844 = 0.13603007793426514 + 2.0 * 6.2950639724731445
Epoch 890, val loss: 0.8175910711288452
Epoch 900, training loss: 12.709747314453125 = 0.13011929392814636 + 2.0 * 6.289813995361328
Epoch 900, val loss: 0.8208206295967102
Epoch 910, training loss: 12.700968742370605 = 0.12452088296413422 + 2.0 * 6.288223743438721
Epoch 910, val loss: 0.8242568373680115
Epoch 920, training loss: 12.693510055541992 = 0.11920171976089478 + 2.0 * 6.287154197692871
Epoch 920, val loss: 0.8278083205223083
Epoch 930, training loss: 12.694849014282227 = 0.1141408234834671 + 2.0 * 6.290354251861572
Epoch 930, val loss: 0.8313441872596741
Epoch 940, training loss: 12.685644149780273 = 0.10934542119503021 + 2.0 * 6.288149356842041
Epoch 940, val loss: 0.8352243304252625
Epoch 950, training loss: 12.677483558654785 = 0.10479427129030228 + 2.0 * 6.286344528198242
Epoch 950, val loss: 0.8386919498443604
Epoch 960, training loss: 12.667878150939941 = 0.10048579424619675 + 2.0 * 6.283696174621582
Epoch 960, val loss: 0.8424285054206848
Epoch 970, training loss: 12.663228034973145 = 0.09638308733701706 + 2.0 * 6.283422470092773
Epoch 970, val loss: 0.8462352752685547
Epoch 980, training loss: 12.669256210327148 = 0.09249226748943329 + 2.0 * 6.288382053375244
Epoch 980, val loss: 0.8499730229377747
Epoch 990, training loss: 12.660533905029297 = 0.08881311863660812 + 2.0 * 6.285860538482666
Epoch 990, val loss: 0.8535975813865662
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7407407407407408
0.8039008961518187
=== training gcn model ===
Epoch 0, training loss: 19.140657424926758 = 1.9470776319503784 + 2.0 * 8.596790313720703
Epoch 0, val loss: 1.9465854167938232
Epoch 10, training loss: 19.129615783691406 = 1.936906337738037 + 2.0 * 8.596354484558105
Epoch 10, val loss: 1.9364194869995117
Epoch 20, training loss: 19.110544204711914 = 1.9246747493743896 + 2.0 * 8.592934608459473
Epoch 20, val loss: 1.92408287525177
Epoch 30, training loss: 19.04547691345215 = 1.9080158472061157 + 2.0 * 8.568730354309082
Epoch 30, val loss: 1.9070943593978882
Epoch 40, training loss: 18.726655960083008 = 1.8877332210540771 + 2.0 * 8.419461250305176
Epoch 40, val loss: 1.8868271112442017
Epoch 50, training loss: 17.68607521057129 = 1.8657389879226685 + 2.0 * 7.910167694091797
Epoch 50, val loss: 1.865128993988037
Epoch 60, training loss: 16.784494400024414 = 1.8477392196655273 + 2.0 * 7.468377590179443
Epoch 60, val loss: 1.8490393161773682
Epoch 70, training loss: 16.167804718017578 = 1.8359723091125488 + 2.0 * 7.1659159660339355
Epoch 70, val loss: 1.836866021156311
Epoch 80, training loss: 15.791482925415039 = 1.824105978012085 + 2.0 * 6.9836883544921875
Epoch 80, val loss: 1.8249226808547974
Epoch 90, training loss: 15.512187004089355 = 1.812421202659607 + 2.0 * 6.849883079528809
Epoch 90, val loss: 1.8134734630584717
Epoch 100, training loss: 15.359375 = 1.8014088869094849 + 2.0 * 6.778983116149902
Epoch 100, val loss: 1.8028309345245361
Epoch 110, training loss: 15.243720054626465 = 1.7902284860610962 + 2.0 * 6.72674560546875
Epoch 110, val loss: 1.79246187210083
Epoch 120, training loss: 15.144445419311523 = 1.7797307968139648 + 2.0 * 6.682357311248779
Epoch 120, val loss: 1.7823467254638672
Epoch 130, training loss: 15.061884880065918 = 1.7692922353744507 + 2.0 * 6.646296501159668
Epoch 130, val loss: 1.7722291946411133
Epoch 140, training loss: 14.990652084350586 = 1.7580864429473877 + 2.0 * 6.616282939910889
Epoch 140, val loss: 1.7617270946502686
Epoch 150, training loss: 14.925227165222168 = 1.7460567951202393 + 2.0 * 6.589585304260254
Epoch 150, val loss: 1.7506916522979736
Epoch 160, training loss: 14.86376667022705 = 1.7328357696533203 + 2.0 * 6.565465450286865
Epoch 160, val loss: 1.7387174367904663
Epoch 170, training loss: 14.811521530151367 = 1.7180774211883545 + 2.0 * 6.546721935272217
Epoch 170, val loss: 1.7256020307540894
Epoch 180, training loss: 14.752342224121094 = 1.7015434503555298 + 2.0 * 6.525399208068848
Epoch 180, val loss: 1.7111425399780273
Epoch 190, training loss: 14.699148178100586 = 1.682966709136963 + 2.0 * 6.508090972900391
Epoch 190, val loss: 1.694949746131897
Epoch 200, training loss: 14.648088455200195 = 1.661848545074463 + 2.0 * 6.493119716644287
Epoch 200, val loss: 1.6766811609268188
Epoch 210, training loss: 14.597728729248047 = 1.637694239616394 + 2.0 * 6.480017185211182
Epoch 210, val loss: 1.6558479070663452
Epoch 220, training loss: 14.55151081085205 = 1.610320806503296 + 2.0 * 6.470594882965088
Epoch 220, val loss: 1.6324516534805298
Epoch 230, training loss: 14.499468803405762 = 1.5800312757492065 + 2.0 * 6.459718704223633
Epoch 230, val loss: 1.6065837144851685
Epoch 240, training loss: 14.448906898498535 = 1.5464296340942383 + 2.0 * 6.451238632202148
Epoch 240, val loss: 1.578165888786316
Epoch 250, training loss: 14.395984649658203 = 1.509291410446167 + 2.0 * 6.4433465003967285
Epoch 250, val loss: 1.5469343662261963
Epoch 260, training loss: 14.342533111572266 = 1.4685099124908447 + 2.0 * 6.43701171875
Epoch 260, val loss: 1.5129261016845703
Epoch 270, training loss: 14.28750991821289 = 1.4254121780395508 + 2.0 * 6.43104887008667
Epoch 270, val loss: 1.4775431156158447
Epoch 280, training loss: 14.228982925415039 = 1.3807101249694824 + 2.0 * 6.424136638641357
Epoch 280, val loss: 1.4411821365356445
Epoch 290, training loss: 14.173685073852539 = 1.3346607685089111 + 2.0 * 6.4195122718811035
Epoch 290, val loss: 1.404218077659607
Epoch 300, training loss: 14.115249633789062 = 1.2879241704940796 + 2.0 * 6.413662910461426
Epoch 300, val loss: 1.3675647974014282
Epoch 310, training loss: 14.05742073059082 = 1.2414733171463013 + 2.0 * 6.407973766326904
Epoch 310, val loss: 1.331276774406433
Epoch 320, training loss: 14.006458282470703 = 1.1952972412109375 + 2.0 * 6.405580520629883
Epoch 320, val loss: 1.2961231470108032
Epoch 330, training loss: 13.949599266052246 = 1.1506348848342896 + 2.0 * 6.399482250213623
Epoch 330, val loss: 1.2625104188919067
Epoch 340, training loss: 13.896136283874512 = 1.107638955116272 + 2.0 * 6.3942484855651855
Epoch 340, val loss: 1.2305326461791992
Epoch 350, training loss: 13.844447135925293 = 1.0660642385482788 + 2.0 * 6.389191627502441
Epoch 350, val loss: 1.2000879049301147
Epoch 360, training loss: 13.814197540283203 = 1.0259721279144287 + 2.0 * 6.394112586975098
Epoch 360, val loss: 1.1709634065628052
Epoch 370, training loss: 13.756087303161621 = 0.9877882599830627 + 2.0 * 6.384149551391602
Epoch 370, val loss: 1.1438114643096924
Epoch 380, training loss: 13.706771850585938 = 0.9514755010604858 + 2.0 * 6.37764835357666
Epoch 380, val loss: 1.1181758642196655
Epoch 390, training loss: 13.6640043258667 = 0.9166053533554077 + 2.0 * 6.37369966506958
Epoch 390, val loss: 1.0940213203430176
Epoch 400, training loss: 13.623849868774414 = 0.8830047249794006 + 2.0 * 6.37042236328125
Epoch 400, val loss: 1.0711368322372437
Epoch 410, training loss: 13.591349601745605 = 0.8508557677268982 + 2.0 * 6.370246887207031
Epoch 410, val loss: 1.049608826637268
Epoch 420, training loss: 13.552844047546387 = 0.8205839395523071 + 2.0 * 6.3661298751831055
Epoch 420, val loss: 1.0296632051467896
Epoch 430, training loss: 13.513714790344238 = 0.791614294052124 + 2.0 * 6.361050128936768
Epoch 430, val loss: 1.011426568031311
Epoch 440, training loss: 13.483397483825684 = 0.7638513445854187 + 2.0 * 6.3597731590271
Epoch 440, val loss: 0.9945167899131775
Epoch 450, training loss: 13.460344314575195 = 0.7376168370246887 + 2.0 * 6.361363887786865
Epoch 450, val loss: 0.9789941906929016
Epoch 460, training loss: 13.420853614807129 = 0.7126476764678955 + 2.0 * 6.354103088378906
Epoch 460, val loss: 0.9650894999504089
Epoch 470, training loss: 13.389196395874023 = 0.6887694001197815 + 2.0 * 6.350213527679443
Epoch 470, val loss: 0.952544629573822
Epoch 480, training loss: 13.361106872558594 = 0.6657929420471191 + 2.0 * 6.347656726837158
Epoch 480, val loss: 0.941188633441925
Epoch 490, training loss: 13.34052848815918 = 0.6437937021255493 + 2.0 * 6.348367214202881
Epoch 490, val loss: 0.9311119914054871
Epoch 500, training loss: 13.313883781433105 = 0.6227923035621643 + 2.0 * 6.345545768737793
Epoch 500, val loss: 0.9220852255821228
Epoch 510, training loss: 13.284913063049316 = 0.6025829911231995 + 2.0 * 6.341165065765381
Epoch 510, val loss: 0.914122998714447
Epoch 520, training loss: 13.259571075439453 = 0.5829131007194519 + 2.0 * 6.338328838348389
Epoch 520, val loss: 0.9069823026657104
Epoch 530, training loss: 13.247802734375 = 0.5637412071228027 + 2.0 * 6.3420305252075195
Epoch 530, val loss: 0.900635302066803
Epoch 540, training loss: 13.217244148254395 = 0.5449655652046204 + 2.0 * 6.33613920211792
Epoch 540, val loss: 0.8949485421180725
Epoch 550, training loss: 13.203855514526367 = 0.5266832113265991 + 2.0 * 6.338586330413818
Epoch 550, val loss: 0.889804482460022
Epoch 560, training loss: 13.172476768493652 = 0.5088270902633667 + 2.0 * 6.331824779510498
Epoch 560, val loss: 0.8851960897445679
Epoch 570, training loss: 13.149534225463867 = 0.49120134115219116 + 2.0 * 6.329166412353516
Epoch 570, val loss: 0.8811182975769043
Epoch 580, training loss: 13.13940715789795 = 0.47372081875801086 + 2.0 * 6.33284330368042
Epoch 580, val loss: 0.8774564862251282
Epoch 590, training loss: 13.114884376525879 = 0.45655113458633423 + 2.0 * 6.329166412353516
Epoch 590, val loss: 0.8740655183792114
Epoch 600, training loss: 13.090241432189941 = 0.43961721658706665 + 2.0 * 6.32531213760376
Epoch 600, val loss: 0.8710202574729919
Epoch 610, training loss: 13.068471908569336 = 0.4228310286998749 + 2.0 * 6.322820663452148
Epoch 610, val loss: 0.868306040763855
Epoch 620, training loss: 13.052852630615234 = 0.40621304512023926 + 2.0 * 6.323319911956787
Epoch 620, val loss: 0.8659094572067261
Epoch 630, training loss: 13.04722785949707 = 0.38987311720848083 + 2.0 * 6.328677177429199
Epoch 630, val loss: 0.8636353015899658
Epoch 640, training loss: 13.010618209838867 = 0.3739170730113983 + 2.0 * 6.318350791931152
Epoch 640, val loss: 0.8617303371429443
Epoch 650, training loss: 12.992203712463379 = 0.35837799310684204 + 2.0 * 6.316912651062012
Epoch 650, val loss: 0.8600858449935913
Epoch 660, training loss: 12.973969459533691 = 0.34314513206481934 + 2.0 * 6.3154120445251465
Epoch 660, val loss: 0.8588159680366516
Epoch 670, training loss: 12.955609321594238 = 0.32826173305511475 + 2.0 * 6.313673973083496
Epoch 670, val loss: 0.8578392267227173
Epoch 680, training loss: 12.946629524230957 = 0.31377995014190674 + 2.0 * 6.31642484664917
Epoch 680, val loss: 0.8572866916656494
Epoch 690, training loss: 12.944056510925293 = 0.2998444139957428 + 2.0 * 6.322105884552002
Epoch 690, val loss: 0.8570725321769714
Epoch 700, training loss: 12.912984848022461 = 0.2865484058856964 + 2.0 * 6.313218116760254
Epoch 700, val loss: 0.8571470379829407
Epoch 710, training loss: 12.893632888793945 = 0.2737770080566406 + 2.0 * 6.309927940368652
Epoch 710, val loss: 0.8577904105186462
Epoch 720, training loss: 12.877882957458496 = 0.26147380471229553 + 2.0 * 6.308204650878906
Epoch 720, val loss: 0.8588153123855591
Epoch 730, training loss: 12.865293502807617 = 0.24970455467700958 + 2.0 * 6.307794570922852
Epoch 730, val loss: 0.8602949976921082
Epoch 740, training loss: 12.853076934814453 = 0.2385258972644806 + 2.0 * 6.307275295257568
Epoch 740, val loss: 0.8620439767837524
Epoch 750, training loss: 12.837279319763184 = 0.2278553694486618 + 2.0 * 6.304711818695068
Epoch 750, val loss: 0.8642987012863159
Epoch 760, training loss: 12.84487247467041 = 0.21765106916427612 + 2.0 * 6.313610553741455
Epoch 760, val loss: 0.8669317960739136
Epoch 770, training loss: 12.812728881835938 = 0.20802000164985657 + 2.0 * 6.302354335784912
Epoch 770, val loss: 0.8697232007980347
Epoch 780, training loss: 12.802069664001465 = 0.19884946942329407 + 2.0 * 6.301609992980957
Epoch 780, val loss: 0.8730223774909973
Epoch 790, training loss: 12.790292739868164 = 0.19010062515735626 + 2.0 * 6.300096035003662
Epoch 790, val loss: 0.8766232132911682
Epoch 800, training loss: 12.795372009277344 = 0.1817767173051834 + 2.0 * 6.306797504425049
Epoch 800, val loss: 0.8805341720581055
Epoch 810, training loss: 12.777377128601074 = 0.17386193573474884 + 2.0 * 6.3017578125
Epoch 810, val loss: 0.8844943642616272
Epoch 820, training loss: 12.763171195983887 = 0.16637562215328217 + 2.0 * 6.298398017883301
Epoch 820, val loss: 0.8887709975242615
Epoch 830, training loss: 12.753302574157715 = 0.15925844013690948 + 2.0 * 6.297021865844727
Epoch 830, val loss: 0.8933143615722656
Epoch 840, training loss: 12.743924140930176 = 0.1524822860956192 + 2.0 * 6.295721054077148
Epoch 840, val loss: 0.8980082273483276
Epoch 850, training loss: 12.744096755981445 = 0.14602884650230408 + 2.0 * 6.299034118652344
Epoch 850, val loss: 0.9028673768043518
Epoch 860, training loss: 12.730342864990234 = 0.13988514244556427 + 2.0 * 6.295228958129883
Epoch 860, val loss: 0.9078995585441589
Epoch 870, training loss: 12.720734596252441 = 0.13407176733016968 + 2.0 * 6.293331623077393
Epoch 870, val loss: 0.9130107760429382
Epoch 880, training loss: 12.728407859802246 = 0.12851886451244354 + 2.0 * 6.2999444007873535
Epoch 880, val loss: 0.9182842969894409
Epoch 890, training loss: 12.716964721679688 = 0.1232595220208168 + 2.0 * 6.2968525886535645
Epoch 890, val loss: 0.9234328866004944
Epoch 900, training loss: 12.699798583984375 = 0.1183130145072937 + 2.0 * 6.290742874145508
Epoch 900, val loss: 0.928744375705719
Epoch 910, training loss: 12.690329551696777 = 0.1135665774345398 + 2.0 * 6.288381576538086
Epoch 910, val loss: 0.9341663122177124
Epoch 920, training loss: 12.684700012207031 = 0.10903431475162506 + 2.0 * 6.287832736968994
Epoch 920, val loss: 0.9396334290504456
Epoch 930, training loss: 12.69083023071289 = 0.10470882058143616 + 2.0 * 6.293060779571533
Epoch 930, val loss: 0.9451090097427368
Epoch 940, training loss: 12.67715072631836 = 0.10061227530241013 + 2.0 * 6.28826904296875
Epoch 940, val loss: 0.9506610035896301
Epoch 950, training loss: 12.673882484436035 = 0.09669451415538788 + 2.0 * 6.288593769073486
Epoch 950, val loss: 0.9561498761177063
Epoch 960, training loss: 12.66269588470459 = 0.09297343343496323 + 2.0 * 6.284861087799072
Epoch 960, val loss: 0.9615854024887085
Epoch 970, training loss: 12.660271644592285 = 0.08944853395223618 + 2.0 * 6.285411357879639
Epoch 970, val loss: 0.9670276641845703
Epoch 980, training loss: 12.652300834655762 = 0.08607375621795654 + 2.0 * 6.283113479614258
Epoch 980, val loss: 0.9725648164749146
Epoch 990, training loss: 12.654184341430664 = 0.08283871412277222 + 2.0 * 6.285672664642334
Epoch 990, val loss: 0.97800213098526
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7555555555555555
0.8075909330521878
The final CL Acc:0.74568, 0.00698, The final GNN Acc:0.80531, 0.00163
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13124])
remove edge: torch.Size([2, 8010])
updated graph: torch.Size([2, 10578])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 19.153411865234375 = 1.9598042964935303 + 2.0 * 8.596803665161133
Epoch 0, val loss: 1.957854986190796
Epoch 10, training loss: 19.142179489135742 = 1.9493502378463745 + 2.0 * 8.596414566040039
Epoch 10, val loss: 1.9470762014389038
Epoch 20, training loss: 19.123332977294922 = 1.9365363121032715 + 2.0 * 8.593398094177246
Epoch 20, val loss: 1.9336507320404053
Epoch 30, training loss: 19.061906814575195 = 1.9190032482147217 + 2.0 * 8.571452140808105
Epoch 30, val loss: 1.9152494668960571
Epoch 40, training loss: 18.736835479736328 = 1.8979209661483765 + 2.0 * 8.41945743560791
Epoch 40, val loss: 1.8942768573760986
Epoch 50, training loss: 17.235326766967773 = 1.876413106918335 + 2.0 * 7.67945671081543
Epoch 50, val loss: 1.8732364177703857
Epoch 60, training loss: 16.675411224365234 = 1.8597058057785034 + 2.0 * 7.407852649688721
Epoch 60, val loss: 1.8580055236816406
Epoch 70, training loss: 16.22251319885254 = 1.844236969947815 + 2.0 * 7.189137935638428
Epoch 70, val loss: 1.8428879976272583
Epoch 80, training loss: 15.812213897705078 = 1.829379677772522 + 2.0 * 6.991416931152344
Epoch 80, val loss: 1.8290642499923706
Epoch 90, training loss: 15.507730484008789 = 1.816421389579773 + 2.0 * 6.845654487609863
Epoch 90, val loss: 1.8169853687286377
Epoch 100, training loss: 15.33114242553711 = 1.8018683195114136 + 2.0 * 6.764636993408203
Epoch 100, val loss: 1.802916407585144
Epoch 110, training loss: 15.177085876464844 = 1.7862154245376587 + 2.0 * 6.695435047149658
Epoch 110, val loss: 1.787924885749817
Epoch 120, training loss: 15.062209129333496 = 1.7720506191253662 + 2.0 * 6.645079135894775
Epoch 120, val loss: 1.7740998268127441
Epoch 130, training loss: 14.971543312072754 = 1.7579710483551025 + 2.0 * 6.606786251068115
Epoch 130, val loss: 1.7602325677871704
Epoch 140, training loss: 14.895889282226562 = 1.742824912071228 + 2.0 * 6.576532363891602
Epoch 140, val loss: 1.7454514503479004
Epoch 150, training loss: 14.833939552307129 = 1.7264633178710938 + 2.0 * 6.553738117218018
Epoch 150, val loss: 1.7297325134277344
Epoch 160, training loss: 14.771688461303711 = 1.7086089849472046 + 2.0 * 6.5315399169921875
Epoch 160, val loss: 1.7127854824066162
Epoch 170, training loss: 14.714200019836426 = 1.6887580156326294 + 2.0 * 6.512721061706543
Epoch 170, val loss: 1.6942274570465088
Epoch 180, training loss: 14.658895492553711 = 1.666657567024231 + 2.0 * 6.496119022369385
Epoch 180, val loss: 1.6737641096115112
Epoch 190, training loss: 14.607768058776855 = 1.641972541809082 + 2.0 * 6.482897758483887
Epoch 190, val loss: 1.6511023044586182
Epoch 200, training loss: 14.5487699508667 = 1.614601969718933 + 2.0 * 6.467083930969238
Epoch 200, val loss: 1.6262179613113403
Epoch 210, training loss: 14.500433921813965 = 1.5842043161392212 + 2.0 * 6.4581146240234375
Epoch 210, val loss: 1.5988214015960693
Epoch 220, training loss: 14.440926551818848 = 1.5510748624801636 + 2.0 * 6.444925785064697
Epoch 220, val loss: 1.569319248199463
Epoch 230, training loss: 14.384243965148926 = 1.514846682548523 + 2.0 * 6.434698581695557
Epoch 230, val loss: 1.53737473487854
Epoch 240, training loss: 14.328288078308105 = 1.4763402938842773 + 2.0 * 6.425973892211914
Epoch 240, val loss: 1.5041930675506592
Epoch 250, training loss: 14.273283958435059 = 1.436000108718872 + 2.0 * 6.418642044067383
Epoch 250, val loss: 1.4699338674545288
Epoch 260, training loss: 14.217084884643555 = 1.3937855958938599 + 2.0 * 6.411649703979492
Epoch 260, val loss: 1.4346669912338257
Epoch 270, training loss: 14.16413402557373 = 1.3505057096481323 + 2.0 * 6.406814098358154
Epoch 270, val loss: 1.3995227813720703
Epoch 280, training loss: 14.10706901550293 = 1.3073093891143799 + 2.0 * 6.3998799324035645
Epoch 280, val loss: 1.3651868104934692
Epoch 290, training loss: 14.053170204162598 = 1.264244794845581 + 2.0 * 6.394462585449219
Epoch 290, val loss: 1.3315424919128418
Epoch 300, training loss: 14.005369186401367 = 1.2214818000793457 + 2.0 * 6.39194393157959
Epoch 300, val loss: 1.2986375093460083
Epoch 310, training loss: 13.9508056640625 = 1.1793887615203857 + 2.0 * 6.385708332061768
Epoch 310, val loss: 1.2666654586791992
Epoch 320, training loss: 13.89993667602539 = 1.1380772590637207 + 2.0 * 6.380929946899414
Epoch 320, val loss: 1.2355376482009888
Epoch 330, training loss: 13.850358009338379 = 1.0976927280426025 + 2.0 * 6.376332759857178
Epoch 330, val loss: 1.2052539587020874
Epoch 340, training loss: 13.80252456665039 = 1.0582976341247559 + 2.0 * 6.372113227844238
Epoch 340, val loss: 1.1757358312606812
Epoch 350, training loss: 13.7592134475708 = 1.0200105905532837 + 2.0 * 6.369601249694824
Epoch 350, val loss: 1.1469128131866455
Epoch 360, training loss: 13.71298885345459 = 0.9828436970710754 + 2.0 * 6.365072727203369
Epoch 360, val loss: 1.1189440488815308
Epoch 370, training loss: 13.667986869812012 = 0.9469671249389648 + 2.0 * 6.360509872436523
Epoch 370, val loss: 1.0919675827026367
Epoch 380, training loss: 13.624406814575195 = 0.9122778177261353 + 2.0 * 6.356064319610596
Epoch 380, val loss: 1.0658693313598633
Epoch 390, training loss: 13.586346626281738 = 0.8786729574203491 + 2.0 * 6.353837013244629
Epoch 390, val loss: 1.0405983924865723
Epoch 400, training loss: 13.556862831115723 = 0.8463468551635742 + 2.0 * 6.355257987976074
Epoch 400, val loss: 1.0162875652313232
Epoch 410, training loss: 13.511184692382812 = 0.815312385559082 + 2.0 * 6.347936153411865
Epoch 410, val loss: 0.9931504130363464
Epoch 420, training loss: 13.488469123840332 = 0.785552442073822 + 2.0 * 6.351458549499512
Epoch 420, val loss: 0.9709575772285461
Epoch 430, training loss: 13.437711715698242 = 0.7569915652275085 + 2.0 * 6.340360164642334
Epoch 430, val loss: 0.9499062895774841
Epoch 440, training loss: 13.405267715454102 = 0.7294632792472839 + 2.0 * 6.337902069091797
Epoch 440, val loss: 0.9298911094665527
Epoch 450, training loss: 13.373455047607422 = 0.7028340697288513 + 2.0 * 6.335310459136963
Epoch 450, val loss: 0.9107140302658081
Epoch 460, training loss: 13.349710464477539 = 0.6772074699401855 + 2.0 * 6.336251735687256
Epoch 460, val loss: 0.8924925327301025
Epoch 470, training loss: 13.315324783325195 = 0.6525723338127136 + 2.0 * 6.331376075744629
Epoch 470, val loss: 0.8752423524856567
Epoch 480, training loss: 13.28792667388916 = 0.6287602186203003 + 2.0 * 6.329583168029785
Epoch 480, val loss: 0.8589422106742859
Epoch 490, training loss: 13.259246826171875 = 0.6057255864143372 + 2.0 * 6.326760768890381
Epoch 490, val loss: 0.8435811996459961
Epoch 500, training loss: 13.228797912597656 = 0.5834892392158508 + 2.0 * 6.3226542472839355
Epoch 500, val loss: 0.8291598558425903
Epoch 510, training loss: 13.205732345581055 = 0.5620105862617493 + 2.0 * 6.3218607902526855
Epoch 510, val loss: 0.8157075643539429
Epoch 520, training loss: 13.180838584899902 = 0.5414424538612366 + 2.0 * 6.319697856903076
Epoch 520, val loss: 0.8031946420669556
Epoch 530, training loss: 13.156376838684082 = 0.5215679407119751 + 2.0 * 6.317404270172119
Epoch 530, val loss: 0.791681170463562
Epoch 540, training loss: 13.13815689086914 = 0.5023646354675293 + 2.0 * 6.317896366119385
Epoch 540, val loss: 0.7810349464416504
Epoch 550, training loss: 13.119138717651367 = 0.484049916267395 + 2.0 * 6.317544460296631
Epoch 550, val loss: 0.7712865471839905
Epoch 560, training loss: 13.090814590454102 = 0.46630653738975525 + 2.0 * 6.312253952026367
Epoch 560, val loss: 0.7624528408050537
Epoch 570, training loss: 13.069236755371094 = 0.4491637647151947 + 2.0 * 6.310036659240723
Epoch 570, val loss: 0.7543892860412598
Epoch 580, training loss: 13.05160903930664 = 0.43259263038635254 + 2.0 * 6.309508323669434
Epoch 580, val loss: 0.7470411658287048
Epoch 590, training loss: 13.035750389099121 = 0.41656118631362915 + 2.0 * 6.309594631195068
Epoch 590, val loss: 0.7404618263244629
Epoch 600, training loss: 13.012469291687012 = 0.40106719732284546 + 2.0 * 6.30570125579834
Epoch 600, val loss: 0.7345395088195801
Epoch 610, training loss: 12.995001792907715 = 0.386055052280426 + 2.0 * 6.304473400115967
Epoch 610, val loss: 0.7292382717132568
Epoch 620, training loss: 12.976441383361816 = 0.37135201692581177 + 2.0 * 6.302544593811035
Epoch 620, val loss: 0.7244462370872498
Epoch 630, training loss: 12.960182189941406 = 0.35697680711746216 + 2.0 * 6.301602840423584
Epoch 630, val loss: 0.7201927900314331
Epoch 640, training loss: 12.947145462036133 = 0.3430216312408447 + 2.0 * 6.302062034606934
Epoch 640, val loss: 0.7164169549942017
Epoch 650, training loss: 12.931482315063477 = 0.3294370770454407 + 2.0 * 6.301022529602051
Epoch 650, val loss: 0.7132353782653809
Epoch 660, training loss: 12.911343574523926 = 0.31621184945106506 + 2.0 * 6.297565937042236
Epoch 660, val loss: 0.7104460000991821
Epoch 670, training loss: 12.897544860839844 = 0.3032534420490265 + 2.0 * 6.297145843505859
Epoch 670, val loss: 0.7081206440925598
Epoch 680, training loss: 12.879142761230469 = 0.2905885577201843 + 2.0 * 6.294277191162109
Epoch 680, val loss: 0.7061551213264465
Epoch 690, training loss: 12.881126403808594 = 0.27821430563926697 + 2.0 * 6.301455974578857
Epoch 690, val loss: 0.7045623064041138
Epoch 700, training loss: 12.851226806640625 = 0.266206830739975 + 2.0 * 6.292510032653809
Epoch 700, val loss: 0.7034133076667786
Epoch 710, training loss: 12.838133811950684 = 0.2545035481452942 + 2.0 * 6.291815280914307
Epoch 710, val loss: 0.7026764750480652
Epoch 720, training loss: 12.823052406311035 = 0.24311262369155884 + 2.0 * 6.2899699211120605
Epoch 720, val loss: 0.7023366689682007
Epoch 730, training loss: 12.82817554473877 = 0.23203350603580475 + 2.0 * 6.298070907592773
Epoch 730, val loss: 0.7023562788963318
Epoch 740, training loss: 12.79780101776123 = 0.22147412598133087 + 2.0 * 6.288163661956787
Epoch 740, val loss: 0.7027433514595032
Epoch 750, training loss: 12.794553756713867 = 0.21127301454544067 + 2.0 * 6.291640281677246
Epoch 750, val loss: 0.7035048007965088
Epoch 760, training loss: 12.775407791137695 = 0.20150639116764069 + 2.0 * 6.286950588226318
Epoch 760, val loss: 0.7046170234680176
Epoch 770, training loss: 12.762150764465332 = 0.19212430715560913 + 2.0 * 6.285013198852539
Epoch 770, val loss: 0.7060813903808594
Epoch 780, training loss: 12.760461807250977 = 0.1831534057855606 + 2.0 * 6.288654327392578
Epoch 780, val loss: 0.7078772187232971
Epoch 790, training loss: 12.745645523071289 = 0.17465201020240784 + 2.0 * 6.285496711730957
Epoch 790, val loss: 0.7099665999412537
Epoch 800, training loss: 12.73285961151123 = 0.1665574461221695 + 2.0 * 6.283151149749756
Epoch 800, val loss: 0.7123561501502991
Epoch 810, training loss: 12.728373527526855 = 0.1588849276304245 + 2.0 * 6.2847442626953125
Epoch 810, val loss: 0.7149731516838074
Epoch 820, training loss: 12.714558601379395 = 0.15161576867103577 + 2.0 * 6.281471252441406
Epoch 820, val loss: 0.7178993225097656
Epoch 830, training loss: 12.707128524780273 = 0.14472942054271698 + 2.0 * 6.2811994552612305
Epoch 830, val loss: 0.7209946513175964
Epoch 840, training loss: 12.698080062866211 = 0.13823170959949493 + 2.0 * 6.279924392700195
Epoch 840, val loss: 0.7242927551269531
Epoch 850, training loss: 12.689208984375 = 0.1321064680814743 + 2.0 * 6.27855110168457
Epoch 850, val loss: 0.7277448773384094
Epoch 860, training loss: 12.68053150177002 = 0.12628738582134247 + 2.0 * 6.2771220207214355
Epoch 860, val loss: 0.7314097881317139
Epoch 870, training loss: 12.674881935119629 = 0.12079135328531265 + 2.0 * 6.277045249938965
Epoch 870, val loss: 0.7351968884468079
Epoch 880, training loss: 12.675980567932129 = 0.1156189888715744 + 2.0 * 6.280180931091309
Epoch 880, val loss: 0.7391005158424377
Epoch 890, training loss: 12.667366027832031 = 0.11072298884391785 + 2.0 * 6.278321743011475
Epoch 890, val loss: 0.7431095838546753
Epoch 900, training loss: 12.653670310974121 = 0.10610459744930267 + 2.0 * 6.273782730102539
Epoch 900, val loss: 0.7471703886985779
Epoch 910, training loss: 12.649419784545898 = 0.1017254889011383 + 2.0 * 6.2738471031188965
Epoch 910, val loss: 0.7513399720191956
Epoch 920, training loss: 12.654308319091797 = 0.0975785031914711 + 2.0 * 6.278365135192871
Epoch 920, val loss: 0.7555165886878967
Epoch 930, training loss: 12.645358085632324 = 0.09365639090538025 + 2.0 * 6.275850772857666
Epoch 930, val loss: 0.75983726978302
Epoch 940, training loss: 12.634037017822266 = 0.08993460983037949 + 2.0 * 6.2720513343811035
Epoch 940, val loss: 0.7641363143920898
Epoch 950, training loss: 12.627678871154785 = 0.08641225844621658 + 2.0 * 6.270633220672607
Epoch 950, val loss: 0.7685003280639648
Epoch 960, training loss: 12.626490592956543 = 0.08306065201759338 + 2.0 * 6.27171516418457
Epoch 960, val loss: 0.7729450464248657
Epoch 970, training loss: 12.619977951049805 = 0.07987035065889359 + 2.0 * 6.270053863525391
Epoch 970, val loss: 0.7773405313491821
Epoch 980, training loss: 12.618444442749023 = 0.07684638351202011 + 2.0 * 6.270799160003662
Epoch 980, val loss: 0.7817959189414978
Epoch 990, training loss: 12.611035346984863 = 0.07397308945655823 + 2.0 * 6.268531322479248
Epoch 990, val loss: 0.7862657308578491
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8037037037037037
0.8386926726410122
=== training gcn model ===
Epoch 0, training loss: 19.1502742767334 = 1.9566386938095093 + 2.0 * 8.596817970275879
Epoch 0, val loss: 1.950181484222412
Epoch 10, training loss: 19.139474868774414 = 1.9465693235397339 + 2.0 * 8.596452713012695
Epoch 10, val loss: 1.9400372505187988
Epoch 20, training loss: 19.121170043945312 = 1.9340016841888428 + 2.0 * 8.593584060668945
Epoch 20, val loss: 1.9274402856826782
Epoch 30, training loss: 19.06209373474121 = 1.9166338443756104 + 2.0 * 8.57273006439209
Epoch 30, val loss: 1.9101897478103638
Epoch 40, training loss: 18.775545120239258 = 1.8946163654327393 + 2.0 * 8.44046401977539
Epoch 40, val loss: 1.8889925479888916
Epoch 50, training loss: 17.851308822631836 = 1.8693140745162964 + 2.0 * 7.990997314453125
Epoch 50, val loss: 1.8645943403244019
Epoch 60, training loss: 17.125757217407227 = 1.8453235626220703 + 2.0 * 7.640216827392578
Epoch 60, val loss: 1.8419501781463623
Epoch 70, training loss: 16.42444610595703 = 1.8271069526672363 + 2.0 * 7.298669815063477
Epoch 70, val loss: 1.8249003887176514
Epoch 80, training loss: 15.993834495544434 = 1.8124512434005737 + 2.0 * 7.090691566467285
Epoch 80, val loss: 1.8112499713897705
Epoch 90, training loss: 15.69807243347168 = 1.7976528406143188 + 2.0 * 6.950209617614746
Epoch 90, val loss: 1.7976315021514893
Epoch 100, training loss: 15.497481346130371 = 1.7831313610076904 + 2.0 * 6.857174873352051
Epoch 100, val loss: 1.784483790397644
Epoch 110, training loss: 15.349801063537598 = 1.7694244384765625 + 2.0 * 6.790188312530518
Epoch 110, val loss: 1.7720927000045776
Epoch 120, training loss: 15.205498695373535 = 1.756199598312378 + 2.0 * 6.724649429321289
Epoch 120, val loss: 1.760025978088379
Epoch 130, training loss: 15.096031188964844 = 1.7435221672058105 + 2.0 * 6.6762542724609375
Epoch 130, val loss: 1.7485533952713013
Epoch 140, training loss: 15.013934135437012 = 1.7296664714813232 + 2.0 * 6.642133712768555
Epoch 140, val loss: 1.7360433340072632
Epoch 150, training loss: 14.934430122375488 = 1.713865041732788 + 2.0 * 6.6102824211120605
Epoch 150, val loss: 1.7221131324768066
Epoch 160, training loss: 14.85959243774414 = 1.6964247226715088 + 2.0 * 6.5815839767456055
Epoch 160, val loss: 1.7070552110671997
Epoch 170, training loss: 14.797690391540527 = 1.6768709421157837 + 2.0 * 6.5604095458984375
Epoch 170, val loss: 1.6903353929519653
Epoch 180, training loss: 14.724936485290527 = 1.6553916931152344 + 2.0 * 6.5347723960876465
Epoch 180, val loss: 1.672142505645752
Epoch 190, training loss: 14.665214538574219 = 1.6316255331039429 + 2.0 * 6.516794681549072
Epoch 190, val loss: 1.6520947217941284
Epoch 200, training loss: 14.604738235473633 = 1.6048998832702637 + 2.0 * 6.4999189376831055
Epoch 200, val loss: 1.6296063661575317
Epoch 210, training loss: 14.548561096191406 = 1.5749808549880981 + 2.0 * 6.486790180206299
Epoch 210, val loss: 1.6044293642044067
Epoch 220, training loss: 14.49063491821289 = 1.5419565439224243 + 2.0 * 6.474339008331299
Epoch 220, val loss: 1.576852560043335
Epoch 230, training loss: 14.434181213378906 = 1.5061564445495605 + 2.0 * 6.464012622833252
Epoch 230, val loss: 1.5471127033233643
Epoch 240, training loss: 14.375199317932129 = 1.467498540878296 + 2.0 * 6.453850269317627
Epoch 240, val loss: 1.5151935815811157
Epoch 250, training loss: 14.318347930908203 = 1.4264192581176758 + 2.0 * 6.445964336395264
Epoch 250, val loss: 1.4815794229507446
Epoch 260, training loss: 14.258706092834473 = 1.3837782144546509 + 2.0 * 6.437463760375977
Epoch 260, val loss: 1.4467713832855225
Epoch 270, training loss: 14.199943542480469 = 1.3398728370666504 + 2.0 * 6.43003511428833
Epoch 270, val loss: 1.4112460613250732
Epoch 280, training loss: 14.141798973083496 = 1.2952059507369995 + 2.0 * 6.4232964515686035
Epoch 280, val loss: 1.375436544418335
Epoch 290, training loss: 14.09630298614502 = 1.2504889965057373 + 2.0 * 6.422906875610352
Epoch 290, val loss: 1.3401076793670654
Epoch 300, training loss: 14.045571327209473 = 1.2069823741912842 + 2.0 * 6.419294357299805
Epoch 300, val loss: 1.3060802221298218
Epoch 310, training loss: 13.981188774108887 = 1.1656999588012695 + 2.0 * 6.407744407653809
Epoch 310, val loss: 1.274219274520874
Epoch 320, training loss: 13.930288314819336 = 1.1265441179275513 + 2.0 * 6.401872158050537
Epoch 320, val loss: 1.2444082498550415
Epoch 330, training loss: 13.881103515625 = 1.0891315937042236 + 2.0 * 6.395986080169678
Epoch 330, val loss: 1.2162055969238281
Epoch 340, training loss: 13.835736274719238 = 1.0533703565597534 + 2.0 * 6.391182899475098
Epoch 340, val loss: 1.1895015239715576
Epoch 350, training loss: 13.791686058044434 = 1.019182801246643 + 2.0 * 6.386251449584961
Epoch 350, val loss: 1.1641498804092407
Epoch 360, training loss: 13.771036148071289 = 0.9864566326141357 + 2.0 * 6.392289638519287
Epoch 360, val loss: 1.140170931816101
Epoch 370, training loss: 13.713288307189941 = 0.9556116461753845 + 2.0 * 6.378838539123535
Epoch 370, val loss: 1.117722749710083
Epoch 380, training loss: 13.674463272094727 = 0.9262153506278992 + 2.0 * 6.374124050140381
Epoch 380, val loss: 1.0963878631591797
Epoch 390, training loss: 13.63967514038086 = 0.8977819681167603 + 2.0 * 6.370946407318115
Epoch 390, val loss: 1.075819492340088
Epoch 400, training loss: 13.604227066040039 = 0.8702960014343262 + 2.0 * 6.366965293884277
Epoch 400, val loss: 1.0561463832855225
Epoch 410, training loss: 13.569914817810059 = 0.8438947796821594 + 2.0 * 6.363009929656982
Epoch 410, val loss: 1.0373297929763794
Epoch 420, training loss: 13.537306785583496 = 0.8183227777481079 + 2.0 * 6.35949182510376
Epoch 420, val loss: 1.0193066596984863
Epoch 430, training loss: 13.506956100463867 = 0.7933670282363892 + 2.0 * 6.356794357299805
Epoch 430, val loss: 1.0019326210021973
Epoch 440, training loss: 13.478656768798828 = 0.769241452217102 + 2.0 * 6.354707717895508
Epoch 440, val loss: 0.9854775667190552
Epoch 450, training loss: 13.448328018188477 = 0.7460025548934937 + 2.0 * 6.351162910461426
Epoch 450, val loss: 0.9699597358703613
Epoch 460, training loss: 13.420666694641113 = 0.7234315872192383 + 2.0 * 6.3486175537109375
Epoch 460, val loss: 0.9554002285003662
Epoch 470, training loss: 13.39700984954834 = 0.701449453830719 + 2.0 * 6.347780227661133
Epoch 470, val loss: 0.9415690302848816
Epoch 480, training loss: 13.367972373962402 = 0.6800282001495361 + 2.0 * 6.343972206115723
Epoch 480, val loss: 0.9285581707954407
Epoch 490, training loss: 13.339677810668945 = 0.6589981317520142 + 2.0 * 6.340339660644531
Epoch 490, val loss: 0.9163151979446411
Epoch 500, training loss: 13.325843811035156 = 0.6381723880767822 + 2.0 * 6.343835830688477
Epoch 500, val loss: 0.9045577645301819
Epoch 510, training loss: 13.2924165725708 = 0.6175356507301331 + 2.0 * 6.337440490722656
Epoch 510, val loss: 0.8934630751609802
Epoch 520, training loss: 13.264880180358887 = 0.5968340039253235 + 2.0 * 6.3340229988098145
Epoch 520, val loss: 0.8826797008514404
Epoch 530, training loss: 13.240036010742188 = 0.575938880443573 + 2.0 * 6.332048416137695
Epoch 530, val loss: 0.8722407221794128
Epoch 540, training loss: 13.22038745880127 = 0.554850697517395 + 2.0 * 6.332768440246582
Epoch 540, val loss: 0.8618530035018921
Epoch 550, training loss: 13.193635940551758 = 0.5336388945579529 + 2.0 * 6.32999849319458
Epoch 550, val loss: 0.8517918586730957
Epoch 560, training loss: 13.176698684692383 = 0.5122532248497009 + 2.0 * 6.332222938537598
Epoch 560, val loss: 0.841998279094696
Epoch 570, training loss: 13.141794204711914 = 0.49072265625 + 2.0 * 6.325535774230957
Epoch 570, val loss: 0.832343578338623
Epoch 580, training loss: 13.11330795288086 = 0.46917882561683655 + 2.0 * 6.322064399719238
Epoch 580, val loss: 0.8232437372207642
Epoch 590, training loss: 13.089201927185059 = 0.4475909173488617 + 2.0 * 6.320805549621582
Epoch 590, val loss: 0.814525842666626
Epoch 600, training loss: 13.081568717956543 = 0.42617011070251465 + 2.0 * 6.327699184417725
Epoch 600, val loss: 0.8062983155250549
Epoch 610, training loss: 13.04131031036377 = 0.4051450788974762 + 2.0 * 6.318082809448242
Epoch 610, val loss: 0.798784077167511
Epoch 620, training loss: 13.016325950622559 = 0.38464441895484924 + 2.0 * 6.315840721130371
Epoch 620, val loss: 0.792123556137085
Epoch 630, training loss: 13.000213623046875 = 0.3647731840610504 + 2.0 * 6.317720413208008
Epoch 630, val loss: 0.7862585783004761
Epoch 640, training loss: 12.97263240814209 = 0.3456608057022095 + 2.0 * 6.313485622406006
Epoch 640, val loss: 0.781318724155426
Epoch 650, training loss: 12.957676887512207 = 0.3273231089115143 + 2.0 * 6.315176963806152
Epoch 650, val loss: 0.7773024439811707
Epoch 660, training loss: 12.932130813598633 = 0.30988889932632446 + 2.0 * 6.311120986938477
Epoch 660, val loss: 0.77414870262146
Epoch 670, training loss: 12.913538932800293 = 0.2932578921318054 + 2.0 * 6.310140609741211
Epoch 670, val loss: 0.7718883752822876
Epoch 680, training loss: 12.900659561157227 = 0.27751922607421875 + 2.0 * 6.311570167541504
Epoch 680, val loss: 0.7703976631164551
Epoch 690, training loss: 12.886068344116211 = 0.26266032457351685 + 2.0 * 6.311704158782959
Epoch 690, val loss: 0.7696550488471985
Epoch 700, training loss: 12.864819526672363 = 0.24873501062393188 + 2.0 * 6.308042049407959
Epoch 700, val loss: 0.7696534395217896
Epoch 710, training loss: 12.844014167785645 = 0.2356974184513092 + 2.0 * 6.3041582107543945
Epoch 710, val loss: 0.7703042030334473
Epoch 720, training loss: 12.82809066772461 = 0.22340984642505646 + 2.0 * 6.302340507507324
Epoch 720, val loss: 0.7715725898742676
Epoch 730, training loss: 12.81379222869873 = 0.21182391047477722 + 2.0 * 6.3009843826293945
Epoch 730, val loss: 0.7733566761016846
Epoch 740, training loss: 12.82176399230957 = 0.20089267194271088 + 2.0 * 6.310435771942139
Epoch 740, val loss: 0.7756306529045105
Epoch 750, training loss: 12.790125846862793 = 0.19070710241794586 + 2.0 * 6.299709320068359
Epoch 750, val loss: 0.7783737778663635
Epoch 760, training loss: 12.779014587402344 = 0.18118004500865936 + 2.0 * 6.298917293548584
Epoch 760, val loss: 0.7815357446670532
Epoch 770, training loss: 12.766170501708984 = 0.17220310866832733 + 2.0 * 6.29698371887207
Epoch 770, val loss: 0.785077691078186
Epoch 780, training loss: 12.762965202331543 = 0.16374897956848145 + 2.0 * 6.29960823059082
Epoch 780, val loss: 0.7889484763145447
Epoch 790, training loss: 12.758848190307617 = 0.15579953789710999 + 2.0 * 6.3015241622924805
Epoch 790, val loss: 0.7930538654327393
Epoch 800, training loss: 12.735494613647461 = 0.14839425683021545 + 2.0 * 6.29355001449585
Epoch 800, val loss: 0.7974533438682556
Epoch 810, training loss: 12.726977348327637 = 0.14142155647277832 + 2.0 * 6.292778015136719
Epoch 810, val loss: 0.8021509051322937
Epoch 820, training loss: 12.731053352355957 = 0.13484346866607666 + 2.0 * 6.298104763031006
Epoch 820, val loss: 0.8070553541183472
Epoch 830, training loss: 12.713848114013672 = 0.12864823639392853 + 2.0 * 6.292600154876709
Epoch 830, val loss: 0.8121207356452942
Epoch 840, training loss: 12.736586570739746 = 0.12284351885318756 + 2.0 * 6.30687141418457
Epoch 840, val loss: 0.8174035549163818
Epoch 850, training loss: 12.701187133789062 = 0.11737676709890366 + 2.0 * 6.291905403137207
Epoch 850, val loss: 0.8227043151855469
Epoch 860, training loss: 12.688186645507812 = 0.11225736886262894 + 2.0 * 6.287964820861816
Epoch 860, val loss: 0.8281657695770264
Epoch 870, training loss: 12.680949211120605 = 0.10740505158901215 + 2.0 * 6.28677225112915
Epoch 870, val loss: 0.8337817192077637
Epoch 880, training loss: 12.672768592834473 = 0.10279984027147293 + 2.0 * 6.284984588623047
Epoch 880, val loss: 0.8394835591316223
Epoch 890, training loss: 12.66707992553711 = 0.09843214601278305 + 2.0 * 6.284323692321777
Epoch 890, val loss: 0.8453023433685303
Epoch 900, training loss: 12.694964408874512 = 0.09428798407316208 + 2.0 * 6.300338268280029
Epoch 900, val loss: 0.8511521220207214
Epoch 910, training loss: 12.656355857849121 = 0.09040004760026932 + 2.0 * 6.282978057861328
Epoch 910, val loss: 0.8570224046707153
Epoch 920, training loss: 12.651511192321777 = 0.08672767132520676 + 2.0 * 6.282391548156738
Epoch 920, val loss: 0.8629201054573059
Epoch 930, training loss: 12.645737648010254 = 0.08324315398931503 + 2.0 * 6.281247138977051
Epoch 930, val loss: 0.8688629865646362
Epoch 940, training loss: 12.645326614379883 = 0.0799323096871376 + 2.0 * 6.2826972007751465
Epoch 940, val loss: 0.8748602271080017
Epoch 950, training loss: 12.640301704406738 = 0.07678531855344772 + 2.0 * 6.2817583084106445
Epoch 950, val loss: 0.8807541131973267
Epoch 960, training loss: 12.637910842895508 = 0.07382087409496307 + 2.0 * 6.282044887542725
Epoch 960, val loss: 0.8867658972740173
Epoch 970, training loss: 12.627058029174805 = 0.07100608944892883 + 2.0 * 6.278026103973389
Epoch 970, val loss: 0.8927616477012634
Epoch 980, training loss: 12.623119354248047 = 0.06833063811063766 + 2.0 * 6.2773942947387695
Epoch 980, val loss: 0.8986958861351013
Epoch 990, training loss: 12.629480361938477 = 0.06578414887189865 + 2.0 * 6.281847953796387
Epoch 990, val loss: 0.9046496748924255
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7962962962962963
0.8418555614127571
=== training gcn model ===
Epoch 0, training loss: 19.135398864746094 = 1.9417717456817627 + 2.0 * 8.596813201904297
Epoch 0, val loss: 1.9349218606948853
Epoch 10, training loss: 19.123903274536133 = 1.9311139583587646 + 2.0 * 8.596394538879395
Epoch 10, val loss: 1.9248013496398926
Epoch 20, training loss: 19.104354858398438 = 1.9177778959274292 + 2.0 * 8.59328842163086
Epoch 20, val loss: 1.911621332168579
Epoch 30, training loss: 19.042171478271484 = 1.899402141571045 + 2.0 * 8.57138442993164
Epoch 30, val loss: 1.8930457830429077
Epoch 40, training loss: 18.734722137451172 = 1.877358317375183 + 2.0 * 8.428682327270508
Epoch 40, val loss: 1.8714993000030518
Epoch 50, training loss: 17.786983489990234 = 1.8535609245300293 + 2.0 * 7.966711521148682
Epoch 50, val loss: 1.8484030961990356
Epoch 60, training loss: 17.064863204956055 = 1.8342097997665405 + 2.0 * 7.615326404571533
Epoch 60, val loss: 1.830201268196106
Epoch 70, training loss: 16.267967224121094 = 1.8223589658737183 + 2.0 * 7.222804546356201
Epoch 70, val loss: 1.8191282749176025
Epoch 80, training loss: 15.79850959777832 = 1.81173574924469 + 2.0 * 6.993386745452881
Epoch 80, val loss: 1.809208631515503
Epoch 90, training loss: 15.508081436157227 = 1.7971490621566772 + 2.0 * 6.855466365814209
Epoch 90, val loss: 1.7967616319656372
Epoch 100, training loss: 15.325693130493164 = 1.7828696966171265 + 2.0 * 6.771411895751953
Epoch 100, val loss: 1.7843066453933716
Epoch 110, training loss: 15.211729049682617 = 1.7698994874954224 + 2.0 * 6.720914840698242
Epoch 110, val loss: 1.772223711013794
Epoch 120, training loss: 15.115487098693848 = 1.757203459739685 + 2.0 * 6.679141998291016
Epoch 120, val loss: 1.7602111101150513
Epoch 130, training loss: 15.031432151794434 = 1.7441991567611694 + 2.0 * 6.643616676330566
Epoch 130, val loss: 1.7481569051742554
Epoch 140, training loss: 14.957476615905762 = 1.7303873300552368 + 2.0 * 6.613544464111328
Epoch 140, val loss: 1.7355456352233887
Epoch 150, training loss: 14.88950252532959 = 1.7152382135391235 + 2.0 * 6.587131977081299
Epoch 150, val loss: 1.7220261096954346
Epoch 160, training loss: 14.826214790344238 = 1.698453664779663 + 2.0 * 6.563880443572998
Epoch 160, val loss: 1.7071412801742554
Epoch 170, training loss: 14.77346134185791 = 1.6795300245285034 + 2.0 * 6.546965599060059
Epoch 170, val loss: 1.6904094219207764
Epoch 180, training loss: 14.713495254516602 = 1.6584296226501465 + 2.0 * 6.527532577514648
Epoch 180, val loss: 1.6717857122421265
Epoch 190, training loss: 14.656095504760742 = 1.6347533464431763 + 2.0 * 6.510671138763428
Epoch 190, val loss: 1.6510405540466309
Epoch 200, training loss: 14.599873542785645 = 1.6082319021224976 + 2.0 * 6.495820999145508
Epoch 200, val loss: 1.6279551982879639
Epoch 210, training loss: 14.545889854431152 = 1.5788761377334595 + 2.0 * 6.483506679534912
Epoch 210, val loss: 1.6025598049163818
Epoch 220, training loss: 14.497415542602539 = 1.546344518661499 + 2.0 * 6.4755353927612305
Epoch 220, val loss: 1.5745009183883667
Epoch 230, training loss: 14.435550689697266 = 1.5108370780944824 + 2.0 * 6.4623565673828125
Epoch 230, val loss: 1.5441973209381104
Epoch 240, training loss: 14.379034042358398 = 1.4725615978240967 + 2.0 * 6.453236103057861
Epoch 240, val loss: 1.511660099029541
Epoch 250, training loss: 14.321549415588379 = 1.4315096139907837 + 2.0 * 6.445019721984863
Epoch 250, val loss: 1.4770891666412354
Epoch 260, training loss: 14.263723373413086 = 1.3880139589309692 + 2.0 * 6.437854766845703
Epoch 260, val loss: 1.4408633708953857
Epoch 270, training loss: 14.208128929138184 = 1.3428455591201782 + 2.0 * 6.432641506195068
Epoch 270, val loss: 1.403862476348877
Epoch 280, training loss: 14.148565292358398 = 1.2968897819519043 + 2.0 * 6.425837516784668
Epoch 280, val loss: 1.3667068481445312
Epoch 290, training loss: 14.09016227722168 = 1.2502262592315674 + 2.0 * 6.419968128204346
Epoch 290, val loss: 1.3295687437057495
Epoch 300, training loss: 14.041831970214844 = 1.2040168046951294 + 2.0 * 6.418907642364502
Epoch 300, val loss: 1.2933757305145264
Epoch 310, training loss: 13.976818084716797 = 1.1586743593215942 + 2.0 * 6.409071922302246
Epoch 310, val loss: 1.2586252689361572
Epoch 320, training loss: 13.922804832458496 = 1.1139944791793823 + 2.0 * 6.404405117034912
Epoch 320, val loss: 1.2249679565429688
Epoch 330, training loss: 13.869710922241211 = 1.0698500871658325 + 2.0 * 6.399930477142334
Epoch 330, val loss: 1.192103624343872
Epoch 340, training loss: 13.824670791625977 = 1.026650309562683 + 2.0 * 6.399010181427002
Epoch 340, val loss: 1.1603856086730957
Epoch 350, training loss: 13.768790245056152 = 0.9849063754081726 + 2.0 * 6.391942024230957
Epoch 350, val loss: 1.13005530834198
Epoch 360, training loss: 13.718851089477539 = 0.94440096616745 + 2.0 * 6.387225151062012
Epoch 360, val loss: 1.1010782718658447
Epoch 370, training loss: 13.690646171569824 = 0.9050868153572083 + 2.0 * 6.39277982711792
Epoch 370, val loss: 1.07341730594635
Epoch 380, training loss: 13.632246971130371 = 0.8678585290908813 + 2.0 * 6.3821940422058105
Epoch 380, val loss: 1.0473185777664185
Epoch 390, training loss: 13.586069107055664 = 0.8322863578796387 + 2.0 * 6.376891136169434
Epoch 390, val loss: 1.022886037826538
Epoch 400, training loss: 13.546585083007812 = 0.7982085943222046 + 2.0 * 6.374188423156738
Epoch 400, val loss: 0.9998201727867126
Epoch 410, training loss: 13.507676124572754 = 0.7656768560409546 + 2.0 * 6.370999813079834
Epoch 410, val loss: 0.9782485365867615
Epoch 420, training loss: 13.468439102172852 = 0.7346832752227783 + 2.0 * 6.366878032684326
Epoch 420, val loss: 0.9580220580101013
Epoch 430, training loss: 13.436907768249512 = 0.7050431370735168 + 2.0 * 6.365932464599609
Epoch 430, val loss: 0.9391855001449585
Epoch 440, training loss: 13.400175094604492 = 0.6769634485244751 + 2.0 * 6.361605644226074
Epoch 440, val loss: 0.9217599630355835
Epoch 450, training loss: 13.365471839904785 = 0.6500193476676941 + 2.0 * 6.357726097106934
Epoch 450, val loss: 0.9056155681610107
Epoch 460, training loss: 13.333760261535645 = 0.6240247488021851 + 2.0 * 6.354867935180664
Epoch 460, val loss: 0.8904427886009216
Epoch 470, training loss: 13.325518608093262 = 0.5990344285964966 + 2.0 * 6.363242149353027
Epoch 470, val loss: 0.876224160194397
Epoch 480, training loss: 13.278225898742676 = 0.5750111937522888 + 2.0 * 6.351607322692871
Epoch 480, val loss: 0.8631036281585693
Epoch 490, training loss: 13.247477531433105 = 0.5518720149993896 + 2.0 * 6.347802639007568
Epoch 490, val loss: 0.8509359955787659
Epoch 500, training loss: 13.220579147338867 = 0.5293577313423157 + 2.0 * 6.345610618591309
Epoch 500, val loss: 0.839415967464447
Epoch 510, training loss: 13.199153900146484 = 0.5073638558387756 + 2.0 * 6.345894813537598
Epoch 510, val loss: 0.8284638524055481
Epoch 520, training loss: 13.177994728088379 = 0.48587363958358765 + 2.0 * 6.346060752868652
Epoch 520, val loss: 0.8182781338691711
Epoch 530, training loss: 13.146027565002441 = 0.4651142358779907 + 2.0 * 6.340456485748291
Epoch 530, val loss: 0.8085445761680603
Epoch 540, training loss: 13.120099067687988 = 0.44479653239250183 + 2.0 * 6.337651252746582
Epoch 540, val loss: 0.799397349357605
Epoch 550, training loss: 13.100628852844238 = 0.42491814494132996 + 2.0 * 6.337855339050293
Epoch 550, val loss: 0.7906743884086609
Epoch 560, training loss: 13.084809303283691 = 0.40560370683670044 + 2.0 * 6.339602947235107
Epoch 560, val loss: 0.7826135754585266
Epoch 570, training loss: 13.06104850769043 = 0.38688308000564575 + 2.0 * 6.337082862854004
Epoch 570, val loss: 0.7750865817070007
Epoch 580, training loss: 13.033194541931152 = 0.3688054382801056 + 2.0 * 6.3321943283081055
Epoch 580, val loss: 0.7680274844169617
Epoch 590, training loss: 13.009963989257812 = 0.3512028455734253 + 2.0 * 6.329380512237549
Epoch 590, val loss: 0.7614930868148804
Epoch 600, training loss: 12.994827270507812 = 0.3341621160507202 + 2.0 * 6.3303327560424805
Epoch 600, val loss: 0.7555394172668457
Epoch 610, training loss: 12.96981143951416 = 0.3178158700466156 + 2.0 * 6.325997829437256
Epoch 610, val loss: 0.7500447630882263
Epoch 620, training loss: 12.963628768920898 = 0.30207955837249756 + 2.0 * 6.330774784088135
Epoch 620, val loss: 0.745131254196167
Epoch 630, training loss: 12.936762809753418 = 0.2870953381061554 + 2.0 * 6.324833869934082
Epoch 630, val loss: 0.7408069372177124
Epoch 640, training loss: 12.915989875793457 = 0.2727474272251129 + 2.0 * 6.321621417999268
Epoch 640, val loss: 0.7370522618293762
Epoch 650, training loss: 12.905815124511719 = 0.25905245542526245 + 2.0 * 6.323381423950195
Epoch 650, val loss: 0.7338143587112427
Epoch 660, training loss: 12.891622543334961 = 0.24602241814136505 + 2.0 * 6.322800159454346
Epoch 660, val loss: 0.731006383895874
Epoch 670, training loss: 12.870271682739258 = 0.2336937040090561 + 2.0 * 6.318288803100586
Epoch 670, val loss: 0.7287245988845825
Epoch 680, training loss: 12.854561805725098 = 0.2219237983226776 + 2.0 * 6.316318988800049
Epoch 680, val loss: 0.7269108891487122
Epoch 690, training loss: 12.865108489990234 = 0.21072888374328613 + 2.0 * 6.327189922332764
Epoch 690, val loss: 0.7255465984344482
Epoch 700, training loss: 12.831151962280273 = 0.20012570917606354 + 2.0 * 6.3155131340026855
Epoch 700, val loss: 0.7247167825698853
Epoch 710, training loss: 12.81489086151123 = 0.1900961548089981 + 2.0 * 6.312397480010986
Epoch 710, val loss: 0.7242593765258789
Epoch 720, training loss: 12.802509307861328 = 0.18056820333003998 + 2.0 * 6.310970783233643
Epoch 720, val loss: 0.7241966128349304
Epoch 730, training loss: 12.806912422180176 = 0.171501025557518 + 2.0 * 6.3177056312561035
Epoch 730, val loss: 0.7246145606040955
Epoch 740, training loss: 12.78775405883789 = 0.1630517691373825 + 2.0 * 6.312351226806641
Epoch 740, val loss: 0.725279688835144
Epoch 750, training loss: 12.772581100463867 = 0.15505801141262054 + 2.0 * 6.3087615966796875
Epoch 750, val loss: 0.7263498306274414
Epoch 760, training loss: 12.761669158935547 = 0.14751461148262024 + 2.0 * 6.307077407836914
Epoch 760, val loss: 0.7277474403381348
Epoch 770, training loss: 12.771791458129883 = 0.14041046798229218 + 2.0 * 6.315690517425537
Epoch 770, val loss: 0.7294864058494568
Epoch 780, training loss: 12.749382972717285 = 0.13369277119636536 + 2.0 * 6.307845115661621
Epoch 780, val loss: 0.7314644455909729
Epoch 790, training loss: 12.736875534057617 = 0.12740971148014069 + 2.0 * 6.304732799530029
Epoch 790, val loss: 0.7336441874504089
Epoch 800, training loss: 12.725968360900879 = 0.12147568166255951 + 2.0 * 6.302246570587158
Epoch 800, val loss: 0.7360846996307373
Epoch 810, training loss: 12.73657512664795 = 0.11587724089622498 + 2.0 * 6.310348987579346
Epoch 810, val loss: 0.7387959957122803
Epoch 820, training loss: 12.7153959274292 = 0.11060477048158646 + 2.0 * 6.302395343780518
Epoch 820, val loss: 0.7415446639060974
Epoch 830, training loss: 12.704479217529297 = 0.10563796758651733 + 2.0 * 6.2994208335876465
Epoch 830, val loss: 0.7445130348205566
Epoch 840, training loss: 12.71268367767334 = 0.10095325112342834 + 2.0 * 6.305865287780762
Epoch 840, val loss: 0.747667133808136
Epoch 850, training loss: 12.695954322814941 = 0.09655553847551346 + 2.0 * 6.299699306488037
Epoch 850, val loss: 0.7509756684303284
Epoch 860, training loss: 12.68653392791748 = 0.09238968044519424 + 2.0 * 6.297071933746338
Epoch 860, val loss: 0.7543110847473145
Epoch 870, training loss: 12.680127143859863 = 0.08845368772745132 + 2.0 * 6.295836925506592
Epoch 870, val loss: 0.7578685879707336
Epoch 880, training loss: 12.681647300720215 = 0.08473712205886841 + 2.0 * 6.298455238342285
Epoch 880, val loss: 0.7615185379981995
Epoch 890, training loss: 12.672096252441406 = 0.0812583640217781 + 2.0 * 6.295418739318848
Epoch 890, val loss: 0.7651682496070862
Epoch 900, training loss: 12.664597511291504 = 0.0779678001999855 + 2.0 * 6.2933149337768555
Epoch 900, val loss: 0.7688691020011902
Epoch 910, training loss: 12.659100532531738 = 0.07485383003950119 + 2.0 * 6.292123317718506
Epoch 910, val loss: 0.7726765275001526
Epoch 920, training loss: 12.672320365905762 = 0.07191438227891922 + 2.0 * 6.3002028465271
Epoch 920, val loss: 0.7765625715255737
Epoch 930, training loss: 12.65051555633545 = 0.0691051036119461 + 2.0 * 6.29070520401001
Epoch 930, val loss: 0.7805647850036621
Epoch 940, training loss: 12.64609432220459 = 0.06647226214408875 + 2.0 * 6.289811134338379
Epoch 940, val loss: 0.7845381498336792
Epoch 950, training loss: 12.644021034240723 = 0.06396804749965668 + 2.0 * 6.290026664733887
Epoch 950, val loss: 0.7885559797286987
Epoch 960, training loss: 12.639449119567871 = 0.06160484999418259 + 2.0 * 6.288922309875488
Epoch 960, val loss: 0.792739748954773
Epoch 970, training loss: 12.637472152709961 = 0.059369515627622604 + 2.0 * 6.289051532745361
Epoch 970, val loss: 0.7966957688331604
Epoch 980, training loss: 12.629454612731934 = 0.057250287383794785 + 2.0 * 6.286102294921875
Epoch 980, val loss: 0.8007671236991882
Epoch 990, training loss: 12.625398635864258 = 0.055230624973773956 + 2.0 * 6.285083770751953
Epoch 990, val loss: 0.804847002029419
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8074074074074075
0.8328940432261466
The final CL Acc:0.80247, 0.00462, The final GNN Acc:0.83781, 0.00371
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.4 der2:0.3 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11630])
remove edge: torch.Size([2, 9532])
updated graph: torch.Size([2, 10606])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 19.129520416259766 = 1.9358665943145752 + 2.0 * 8.596826553344727
Epoch 0, val loss: 1.927250623703003
Epoch 10, training loss: 19.118818283081055 = 1.9257137775421143 + 2.0 * 8.596551895141602
Epoch 10, val loss: 1.9182052612304688
Epoch 20, training loss: 19.101892471313477 = 1.9138906002044678 + 2.0 * 8.594000816345215
Epoch 20, val loss: 1.9073690176010132
Epoch 30, training loss: 19.040477752685547 = 1.898443341255188 + 2.0 * 8.571017265319824
Epoch 30, val loss: 1.89297354221344
Epoch 40, training loss: 18.729583740234375 = 1.8803105354309082 + 2.0 * 8.424636840820312
Epoch 40, val loss: 1.8763808012008667
Epoch 50, training loss: 18.2119083404541 = 1.861153483390808 + 2.0 * 8.17537784576416
Epoch 50, val loss: 1.858855128288269
Epoch 60, training loss: 17.696285247802734 = 1.8451542854309082 + 2.0 * 7.925565242767334
Epoch 60, val loss: 1.844441294670105
Epoch 70, training loss: 16.665693283081055 = 1.8336907625198364 + 2.0 * 7.416000843048096
Epoch 70, val loss: 1.834701657295227
Epoch 80, training loss: 15.932973861694336 = 1.8264098167419434 + 2.0 * 7.053282260894775
Epoch 80, val loss: 1.8284478187561035
Epoch 90, training loss: 15.673540115356445 = 1.8152801990509033 + 2.0 * 6.9291300773620605
Epoch 90, val loss: 1.8184040784835815
Epoch 100, training loss: 15.465112686157227 = 1.802076816558838 + 2.0 * 6.831517696380615
Epoch 100, val loss: 1.807268500328064
Epoch 110, training loss: 15.312853813171387 = 1.789858102798462 + 2.0 * 6.761497974395752
Epoch 110, val loss: 1.7970046997070312
Epoch 120, training loss: 15.195038795471191 = 1.7783221006393433 + 2.0 * 6.708358287811279
Epoch 120, val loss: 1.7867628335952759
Epoch 130, training loss: 15.098187446594238 = 1.7660287618637085 + 2.0 * 6.666079521179199
Epoch 130, val loss: 1.7756271362304688
Epoch 140, training loss: 15.016003608703613 = 1.7523458003997803 + 2.0 * 6.631828784942627
Epoch 140, val loss: 1.7637826204299927
Epoch 150, training loss: 14.945317268371582 = 1.737088918685913 + 2.0 * 6.604114055633545
Epoch 150, val loss: 1.7508490085601807
Epoch 160, training loss: 14.882889747619629 = 1.7200509309768677 + 2.0 * 6.581419467926025
Epoch 160, val loss: 1.7366074323654175
Epoch 170, training loss: 14.824301719665527 = 1.7009345293045044 + 2.0 * 6.561683654785156
Epoch 170, val loss: 1.7206597328186035
Epoch 180, training loss: 14.778456687927246 = 1.6794185638427734 + 2.0 * 6.549519062042236
Epoch 180, val loss: 1.7028980255126953
Epoch 190, training loss: 14.724896430969238 = 1.6554896831512451 + 2.0 * 6.534703254699707
Epoch 190, val loss: 1.6833034753799438
Epoch 200, training loss: 14.667139053344727 = 1.629351258277893 + 2.0 * 6.518893718719482
Epoch 200, val loss: 1.6619106531143188
Epoch 210, training loss: 14.612338066101074 = 1.6007871627807617 + 2.0 * 6.505775451660156
Epoch 210, val loss: 1.638706088066101
Epoch 220, training loss: 14.56296443939209 = 1.5698919296264648 + 2.0 * 6.4965362548828125
Epoch 220, val loss: 1.6136363744735718
Epoch 230, training loss: 14.506674766540527 = 1.5371172428131104 + 2.0 * 6.484778881072998
Epoch 230, val loss: 1.5873202085494995
Epoch 240, training loss: 14.453810691833496 = 1.5028668642044067 + 2.0 * 6.4754719734191895
Epoch 240, val loss: 1.5597937107086182
Epoch 250, training loss: 14.3988676071167 = 1.4671471118927002 + 2.0 * 6.465860366821289
Epoch 250, val loss: 1.5313113927841187
Epoch 260, training loss: 14.353894233703613 = 1.430673599243164 + 2.0 * 6.461610317230225
Epoch 260, val loss: 1.5024831295013428
Epoch 270, training loss: 14.297647476196289 = 1.3939770460128784 + 2.0 * 6.4518351554870605
Epoch 270, val loss: 1.473787546157837
Epoch 280, training loss: 14.242694854736328 = 1.357051134109497 + 2.0 * 6.442821979522705
Epoch 280, val loss: 1.4452354907989502
Epoch 290, training loss: 14.192136764526367 = 1.3200860023498535 + 2.0 * 6.436025619506836
Epoch 290, val loss: 1.4168938398361206
Epoch 300, training loss: 14.151748657226562 = 1.2832231521606445 + 2.0 * 6.434262752532959
Epoch 300, val loss: 1.38889479637146
Epoch 310, training loss: 14.09764289855957 = 1.2468421459197998 + 2.0 * 6.425400257110596
Epoch 310, val loss: 1.361507534980774
Epoch 320, training loss: 14.048367500305176 = 1.2109876871109009 + 2.0 * 6.418689727783203
Epoch 320, val loss: 1.3348344564437866
Epoch 330, training loss: 14.0103178024292 = 1.175825834274292 + 2.0 * 6.417245864868164
Epoch 330, val loss: 1.3086986541748047
Epoch 340, training loss: 13.962335586547852 = 1.1415141820907593 + 2.0 * 6.4104108810424805
Epoch 340, val loss: 1.2833293676376343
Epoch 350, training loss: 13.918707847595215 = 1.1080780029296875 + 2.0 * 6.405314922332764
Epoch 350, val loss: 1.2585455179214478
Epoch 360, training loss: 13.875311851501465 = 1.075110673904419 + 2.0 * 6.4001007080078125
Epoch 360, val loss: 1.2340879440307617
Epoch 370, training loss: 13.839685440063477 = 1.042540192604065 + 2.0 * 6.3985724449157715
Epoch 370, val loss: 1.2097759246826172
Epoch 380, training loss: 13.797150611877441 = 1.010452389717102 + 2.0 * 6.3933491706848145
Epoch 380, val loss: 1.1855839490890503
Epoch 390, training loss: 13.761343002319336 = 0.9785668253898621 + 2.0 * 6.391387939453125
Epoch 390, val loss: 1.1614267826080322
Epoch 400, training loss: 13.71730899810791 = 0.9471243619918823 + 2.0 * 6.385092258453369
Epoch 400, val loss: 1.1371750831604004
Epoch 410, training loss: 13.679508209228516 = 0.9158048033714294 + 2.0 * 6.381851673126221
Epoch 410, val loss: 1.1130281686782837
Epoch 420, training loss: 13.6459321975708 = 0.8845797777175903 + 2.0 * 6.38067626953125
Epoch 420, val loss: 1.088860273361206
Epoch 430, training loss: 13.616182327270508 = 0.8538176417350769 + 2.0 * 6.3811821937561035
Epoch 430, val loss: 1.0648152828216553
Epoch 440, training loss: 13.570940971374512 = 0.8236090540885925 + 2.0 * 6.373665809631348
Epoch 440, val loss: 1.0415095090866089
Epoch 450, training loss: 13.533148765563965 = 0.7940202951431274 + 2.0 * 6.369564056396484
Epoch 450, val loss: 1.0188367366790771
Epoch 460, training loss: 13.500029563903809 = 0.7650973200798035 + 2.0 * 6.367465972900391
Epoch 460, val loss: 0.9968885779380798
Epoch 470, training loss: 13.466644287109375 = 0.7369295358657837 + 2.0 * 6.364857196807861
Epoch 470, val loss: 0.9757994413375854
Epoch 480, training loss: 13.436716079711914 = 0.7098162174224854 + 2.0 * 6.363450050354004
Epoch 480, val loss: 0.9558178782463074
Epoch 490, training loss: 13.403923988342285 = 0.6836478114128113 + 2.0 * 6.360137939453125
Epoch 490, val loss: 0.9369990825653076
Epoch 500, training loss: 13.390624046325684 = 0.658379852771759 + 2.0 * 6.366122245788574
Epoch 500, val loss: 0.9193466901779175
Epoch 510, training loss: 13.347885131835938 = 0.6340457201004028 + 2.0 * 6.356919765472412
Epoch 510, val loss: 0.902855396270752
Epoch 520, training loss: 13.31531047821045 = 0.6106036901473999 + 2.0 * 6.352353572845459
Epoch 520, val loss: 0.8875333070755005
Epoch 530, training loss: 13.28883171081543 = 0.5879606604576111 + 2.0 * 6.350435733795166
Epoch 530, val loss: 0.873325765132904
Epoch 540, training loss: 13.281340599060059 = 0.5660520195960999 + 2.0 * 6.357644081115723
Epoch 540, val loss: 0.8602685928344727
Epoch 550, training loss: 13.239623069763184 = 0.5448535084724426 + 2.0 * 6.347384929656982
Epoch 550, val loss: 0.8478367328643799
Epoch 560, training loss: 13.21342945098877 = 0.5244999527931213 + 2.0 * 6.3444647789001465
Epoch 560, val loss: 0.836624264717102
Epoch 570, training loss: 13.190107345581055 = 0.5047733187675476 + 2.0 * 6.342667102813721
Epoch 570, val loss: 0.8263402581214905
Epoch 580, training loss: 13.16650676727295 = 0.48559337854385376 + 2.0 * 6.340456485748291
Epoch 580, val loss: 0.8168883323669434
Epoch 590, training loss: 13.16250228881836 = 0.4669416844844818 + 2.0 * 6.347780227661133
Epoch 590, val loss: 0.8081139326095581
Epoch 600, training loss: 13.127222061157227 = 0.4487277567386627 + 2.0 * 6.339247226715088
Epoch 600, val loss: 0.8001022934913635
Epoch 610, training loss: 13.10319709777832 = 0.43111148476600647 + 2.0 * 6.336042881011963
Epoch 610, val loss: 0.7928062081336975
Epoch 620, training loss: 13.080976486206055 = 0.41395801305770874 + 2.0 * 6.33350944519043
Epoch 620, val loss: 0.786209762096405
Epoch 630, training loss: 13.093018531799316 = 0.3972896635532379 + 2.0 * 6.347864627838135
Epoch 630, val loss: 0.7804570198059082
Epoch 640, training loss: 13.04308032989502 = 0.3810127377510071 + 2.0 * 6.331033706665039
Epoch 640, val loss: 0.7749606966972351
Epoch 650, training loss: 13.023550987243652 = 0.3653477132320404 + 2.0 * 6.3291015625
Epoch 650, val loss: 0.770325243473053
Epoch 660, training loss: 13.004907608032227 = 0.35013750195503235 + 2.0 * 6.327384948730469
Epoch 660, val loss: 0.766460657119751
Epoch 670, training loss: 12.986562728881836 = 0.33534449338912964 + 2.0 * 6.32560920715332
Epoch 670, val loss: 0.7631044387817383
Epoch 680, training loss: 12.969710350036621 = 0.32098162174224854 + 2.0 * 6.324364185333252
Epoch 680, val loss: 0.7603791356086731
Epoch 690, training loss: 12.99428653717041 = 0.30702272057533264 + 2.0 * 6.343631744384766
Epoch 690, val loss: 0.7581977248191833
Epoch 700, training loss: 12.948062896728516 = 0.29359957575798035 + 2.0 * 6.3272318840026855
Epoch 700, val loss: 0.7563372850418091
Epoch 710, training loss: 12.925256729125977 = 0.2807023227214813 + 2.0 * 6.322277069091797
Epoch 710, val loss: 0.7550997138023376
Epoch 720, training loss: 12.907174110412598 = 0.2682656943798065 + 2.0 * 6.319454193115234
Epoch 720, val loss: 0.7544699311256409
Epoch 730, training loss: 12.892565727233887 = 0.25625428557395935 + 2.0 * 6.318155765533447
Epoch 730, val loss: 0.7542672753334045
Epoch 740, training loss: 12.884078979492188 = 0.24466636776924133 + 2.0 * 6.319706439971924
Epoch 740, val loss: 0.7543853521347046
Epoch 750, training loss: 12.87092113494873 = 0.23359350860118866 + 2.0 * 6.318663597106934
Epoch 750, val loss: 0.7549659013748169
Epoch 760, training loss: 12.851805686950684 = 0.22299131751060486 + 2.0 * 6.3144073486328125
Epoch 760, val loss: 0.7559440732002258
Epoch 770, training loss: 12.842698097229004 = 0.21281394362449646 + 2.0 * 6.314941883087158
Epoch 770, val loss: 0.7572243809700012
Epoch 780, training loss: 12.828222274780273 = 0.2030395269393921 + 2.0 * 6.312591552734375
Epoch 780, val loss: 0.7588843703269958
Epoch 790, training loss: 12.818191528320312 = 0.19368256628513336 + 2.0 * 6.312254428863525
Epoch 790, val loss: 0.7609389424324036
Epoch 800, training loss: 12.814007759094238 = 0.18476326763629913 + 2.0 * 6.314622402191162
Epoch 800, val loss: 0.7631556391716003
Epoch 810, training loss: 12.795598983764648 = 0.17621809244155884 + 2.0 * 6.309690475463867
Epoch 810, val loss: 0.7658553719520569
Epoch 820, training loss: 12.788705825805664 = 0.1680874228477478 + 2.0 * 6.310309410095215
Epoch 820, val loss: 0.7686282396316528
Epoch 830, training loss: 12.776168823242188 = 0.16031423211097717 + 2.0 * 6.307927131652832
Epoch 830, val loss: 0.7717704176902771
Epoch 840, training loss: 12.766589164733887 = 0.15293917059898376 + 2.0 * 6.306825160980225
Epoch 840, val loss: 0.7750346064567566
Epoch 850, training loss: 12.757491111755371 = 0.14590364694595337 + 2.0 * 6.305793762207031
Epoch 850, val loss: 0.7787330746650696
Epoch 860, training loss: 12.752279281616211 = 0.13920538127422333 + 2.0 * 6.30653715133667
Epoch 860, val loss: 0.7825399041175842
Epoch 870, training loss: 12.738455772399902 = 0.1328313946723938 + 2.0 * 6.302812099456787
Epoch 870, val loss: 0.7864707708358765
Epoch 880, training loss: 12.74504280090332 = 0.1267601102590561 + 2.0 * 6.309141159057617
Epoch 880, val loss: 0.7906182408332825
Epoch 890, training loss: 12.72115421295166 = 0.12101758271455765 + 2.0 * 6.300068378448486
Epoch 890, val loss: 0.7950502038002014
Epoch 900, training loss: 12.713687896728516 = 0.11556758731603622 + 2.0 * 6.299060344696045
Epoch 900, val loss: 0.7996217012405396
Epoch 910, training loss: 12.706119537353516 = 0.11039529740810394 + 2.0 * 6.2978620529174805
Epoch 910, val loss: 0.8042603731155396
Epoch 920, training loss: 12.720235824584961 = 0.10549215972423553 + 2.0 * 6.307371616363525
Epoch 920, val loss: 0.8091486096382141
Epoch 930, training loss: 12.707563400268555 = 0.10082563757896423 + 2.0 * 6.303369045257568
Epoch 930, val loss: 0.8138238787651062
Epoch 940, training loss: 12.688644409179688 = 0.0964062288403511 + 2.0 * 6.296119213104248
Epoch 940, val loss: 0.818652331829071
Epoch 950, training loss: 12.682228088378906 = 0.09224500507116318 + 2.0 * 6.294991493225098
Epoch 950, val loss: 0.8236268162727356
Epoch 960, training loss: 12.679930686950684 = 0.08830524235963821 + 2.0 * 6.295812606811523
Epoch 960, val loss: 0.8286030292510986
Epoch 970, training loss: 12.671223640441895 = 0.08456747233867645 + 2.0 * 6.293328285217285
Epoch 970, val loss: 0.8334718346595764
Epoch 980, training loss: 12.667957305908203 = 0.0810307189822197 + 2.0 * 6.293463230133057
Epoch 980, val loss: 0.8384455442428589
Epoch 990, training loss: 12.662933349609375 = 0.07767656445503235 + 2.0 * 6.292628288269043
Epoch 990, val loss: 0.843578040599823
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7518518518518519
0.801265155508698
=== training gcn model ===
Epoch 0, training loss: 19.125883102416992 = 1.9321744441986084 + 2.0 * 8.596854209899902
Epoch 0, val loss: 1.9304118156433105
Epoch 10, training loss: 19.115379333496094 = 1.9222707748413086 + 2.0 * 8.596553802490234
Epoch 10, val loss: 1.9203436374664307
Epoch 20, training loss: 19.09804344177246 = 1.9101754426956177 + 2.0 * 8.593934059143066
Epoch 20, val loss: 1.9079716205596924
Epoch 30, training loss: 19.041017532348633 = 1.8938415050506592 + 2.0 * 8.573588371276855
Epoch 30, val loss: 1.89126455783844
Epoch 40, training loss: 18.7752628326416 = 1.8741662502288818 + 2.0 * 8.45054817199707
Epoch 40, val loss: 1.8719265460968018
Epoch 50, training loss: 17.895469665527344 = 1.853545069694519 + 2.0 * 8.020962715148926
Epoch 50, val loss: 1.8520212173461914
Epoch 60, training loss: 16.91413116455078 = 1.8386021852493286 + 2.0 * 7.537764072418213
Epoch 60, val loss: 1.8386050462722778
Epoch 70, training loss: 16.265850067138672 = 1.8281898498535156 + 2.0 * 7.218830585479736
Epoch 70, val loss: 1.828437328338623
Epoch 80, training loss: 15.91081428527832 = 1.8170052766799927 + 2.0 * 7.046904563903809
Epoch 80, val loss: 1.8175427913665771
Epoch 90, training loss: 15.671724319458008 = 1.8045612573623657 + 2.0 * 6.933581352233887
Epoch 90, val loss: 1.8057364225387573
Epoch 100, training loss: 15.497123718261719 = 1.7920373678207397 + 2.0 * 6.852543354034424
Epoch 100, val loss: 1.7941125631332397
Epoch 110, training loss: 15.3785982131958 = 1.780053973197937 + 2.0 * 6.799272060394287
Epoch 110, val loss: 1.7829957008361816
Epoch 120, training loss: 15.285255432128906 = 1.7679892778396606 + 2.0 * 6.758633136749268
Epoch 120, val loss: 1.7714940309524536
Epoch 130, training loss: 15.196671485900879 = 1.7552921772003174 + 2.0 * 6.72068977355957
Epoch 130, val loss: 1.759673833847046
Epoch 140, training loss: 15.121685028076172 = 1.7418270111083984 + 2.0 * 6.689929008483887
Epoch 140, val loss: 1.7472087144851685
Epoch 150, training loss: 15.048502922058105 = 1.7269967794418335 + 2.0 * 6.66075325012207
Epoch 150, val loss: 1.733809232711792
Epoch 160, training loss: 14.978904724121094 = 1.7105653285980225 + 2.0 * 6.634169578552246
Epoch 160, val loss: 1.7190686464309692
Epoch 170, training loss: 14.9213228225708 = 1.6922556161880493 + 2.0 * 6.614533424377441
Epoch 170, val loss: 1.7028297185897827
Epoch 180, training loss: 14.847366333007812 = 1.6719088554382324 + 2.0 * 6.587728977203369
Epoch 180, val loss: 1.684907078742981
Epoch 190, training loss: 14.785133361816406 = 1.649133324623108 + 2.0 * 6.567999839782715
Epoch 190, val loss: 1.6649715900421143
Epoch 200, training loss: 14.725324630737305 = 1.6235482692718506 + 2.0 * 6.5508880615234375
Epoch 200, val loss: 1.6427041292190552
Epoch 210, training loss: 14.665989875793457 = 1.594948172569275 + 2.0 * 6.535521030426025
Epoch 210, val loss: 1.6179330348968506
Epoch 220, training loss: 14.608124732971191 = 1.5635064840316772 + 2.0 * 6.522309303283691
Epoch 220, val loss: 1.590920090675354
Epoch 230, training loss: 14.548999786376953 = 1.5289864540100098 + 2.0 * 6.510006427764893
Epoch 230, val loss: 1.56122624874115
Epoch 240, training loss: 14.487727165222168 = 1.4912185668945312 + 2.0 * 6.498254299163818
Epoch 240, val loss: 1.5289802551269531
Epoch 250, training loss: 14.425211906433105 = 1.450078010559082 + 2.0 * 6.487566947937012
Epoch 250, val loss: 1.4940134286880493
Epoch 260, training loss: 14.366971015930176 = 1.405822515487671 + 2.0 * 6.480574131011963
Epoch 260, val loss: 1.4568754434585571
Epoch 270, training loss: 14.306976318359375 = 1.3596895933151245 + 2.0 * 6.4736433029174805
Epoch 270, val loss: 1.418397068977356
Epoch 280, training loss: 14.236183166503906 = 1.3118162155151367 + 2.0 * 6.462183475494385
Epoch 280, val loss: 1.3790276050567627
Epoch 290, training loss: 14.172102928161621 = 1.2626162767410278 + 2.0 * 6.454743385314941
Epoch 290, val loss: 1.339035153388977
Epoch 300, training loss: 14.133906364440918 = 1.2127121686935425 + 2.0 * 6.460597038269043
Epoch 300, val loss: 1.2989104986190796
Epoch 310, training loss: 14.04870891571045 = 1.1632179021835327 + 2.0 * 6.442745685577393
Epoch 310, val loss: 1.2595643997192383
Epoch 320, training loss: 13.989206314086914 = 1.1146421432495117 + 2.0 * 6.437282085418701
Epoch 320, val loss: 1.2213822603225708
Epoch 330, training loss: 13.928966522216797 = 1.0671998262405396 + 2.0 * 6.430883407592773
Epoch 330, val loss: 1.1845676898956299
Epoch 340, training loss: 13.872477531433105 = 1.0211451053619385 + 2.0 * 6.425666332244873
Epoch 340, val loss: 1.1493256092071533
Epoch 350, training loss: 13.820856094360352 = 0.9768744707107544 + 2.0 * 6.421990871429443
Epoch 350, val loss: 1.1160377264022827
Epoch 360, training loss: 13.76783275604248 = 0.9348713159561157 + 2.0 * 6.416480541229248
Epoch 360, val loss: 1.0848476886749268
Epoch 370, training loss: 13.71983528137207 = 0.8950197696685791 + 2.0 * 6.412407875061035
Epoch 370, val loss: 1.0558291673660278
Epoch 380, training loss: 13.677310943603516 = 0.8572458028793335 + 2.0 * 6.410032749176025
Epoch 380, val loss: 1.0289816856384277
Epoch 390, training loss: 13.628299713134766 = 0.8216762542724609 + 2.0 * 6.403311729431152
Epoch 390, val loss: 1.0042192935943604
Epoch 400, training loss: 13.587130546569824 = 0.7880086302757263 + 2.0 * 6.399560928344727
Epoch 400, val loss: 0.9813817739486694
Epoch 410, training loss: 13.560341835021973 = 0.7562580704689026 + 2.0 * 6.402041912078857
Epoch 410, val loss: 0.9604223370552063
Epoch 420, training loss: 13.512561798095703 = 0.7263894081115723 + 2.0 * 6.3930864334106445
Epoch 420, val loss: 0.9412087798118591
Epoch 430, training loss: 13.475946426391602 = 0.6982585191726685 + 2.0 * 6.388844013214111
Epoch 430, val loss: 0.9236984252929688
Epoch 440, training loss: 13.448722839355469 = 0.6716713905334473 + 2.0 * 6.38852596282959
Epoch 440, val loss: 0.9077315926551819
Epoch 450, training loss: 13.416175842285156 = 0.6466081738471985 + 2.0 * 6.384783744812012
Epoch 450, val loss: 0.8931203484535217
Epoch 460, training loss: 13.38021469116211 = 0.6229087710380554 + 2.0 * 6.378653049468994
Epoch 460, val loss: 0.8798579573631287
Epoch 470, training loss: 13.35330867767334 = 0.6004465818405151 + 2.0 * 6.376430988311768
Epoch 470, val loss: 0.8677852749824524
Epoch 480, training loss: 13.332134246826172 = 0.5790921449661255 + 2.0 * 6.376521110534668
Epoch 480, val loss: 0.8568863272666931
Epoch 490, training loss: 13.303140640258789 = 0.5587428212165833 + 2.0 * 6.372199058532715
Epoch 490, val loss: 0.8470384478569031
Epoch 500, training loss: 13.28610897064209 = 0.5393577814102173 + 2.0 * 6.373375415802002
Epoch 500, val loss: 0.8380125164985657
Epoch 510, training loss: 13.253040313720703 = 0.520801842212677 + 2.0 * 6.366119384765625
Epoch 510, val loss: 0.8299115300178528
Epoch 520, training loss: 13.230804443359375 = 0.5029901266098022 + 2.0 * 6.363907337188721
Epoch 520, val loss: 0.8225336074829102
Epoch 530, training loss: 13.211804389953613 = 0.48581066727638245 + 2.0 * 6.362997055053711
Epoch 530, val loss: 0.8158303499221802
Epoch 540, training loss: 13.185440063476562 = 0.4692143499851227 + 2.0 * 6.358112812042236
Epoch 540, val loss: 0.8097385764122009
Epoch 550, training loss: 13.166275024414062 = 0.4531871974468231 + 2.0 * 6.356544017791748
Epoch 550, val loss: 0.8041759133338928
Epoch 560, training loss: 13.15408992767334 = 0.43759098649024963 + 2.0 * 6.358249664306641
Epoch 560, val loss: 0.7991324663162231
Epoch 570, training loss: 13.129096984863281 = 0.4224642217159271 + 2.0 * 6.353316307067871
Epoch 570, val loss: 0.7945852279663086
Epoch 580, training loss: 13.108044624328613 = 0.407749742269516 + 2.0 * 6.350147247314453
Epoch 580, val loss: 0.7904652953147888
Epoch 590, training loss: 13.08708381652832 = 0.3934076130390167 + 2.0 * 6.346837997436523
Epoch 590, val loss: 0.786701500415802
Epoch 600, training loss: 13.085675239562988 = 0.37935397028923035 + 2.0 * 6.353160858154297
Epoch 600, val loss: 0.7833573818206787
Epoch 610, training loss: 13.058049201965332 = 0.36565208435058594 + 2.0 * 6.346198558807373
Epoch 610, val loss: 0.78038489818573
Epoch 620, training loss: 13.035425186157227 = 0.3522806763648987 + 2.0 * 6.341572284698486
Epoch 620, val loss: 0.7777463793754578
Epoch 630, training loss: 13.016892433166504 = 0.3391522467136383 + 2.0 * 6.338870048522949
Epoch 630, val loss: 0.7756222486495972
Epoch 640, training loss: 13.005367279052734 = 0.32627928256988525 + 2.0 * 6.33954381942749
Epoch 640, val loss: 0.7738187313079834
Epoch 650, training loss: 12.987914085388184 = 0.3136487603187561 + 2.0 * 6.337132453918457
Epoch 650, val loss: 0.772199273109436
Epoch 660, training loss: 12.972702026367188 = 0.30136722326278687 + 2.0 * 6.335667610168457
Epoch 660, val loss: 0.7709968090057373
Epoch 670, training loss: 12.954022407531738 = 0.2893744111061096 + 2.0 * 6.332324028015137
Epoch 670, val loss: 0.7701427340507507
Epoch 680, training loss: 12.942208290100098 = 0.27763092517852783 + 2.0 * 6.33228874206543
Epoch 680, val loss: 0.7697128653526306
Epoch 690, training loss: 12.924718856811523 = 0.2661864161491394 + 2.0 * 6.32926607131958
Epoch 690, val loss: 0.7694594860076904
Epoch 700, training loss: 12.910215377807617 = 0.2550658583641052 + 2.0 * 6.327574729919434
Epoch 700, val loss: 0.7696517705917358
Epoch 710, training loss: 12.900406837463379 = 0.24426813423633575 + 2.0 * 6.32806921005249
Epoch 710, val loss: 0.7701199054718018
Epoch 720, training loss: 12.885361671447754 = 0.2338191568851471 + 2.0 * 6.325771331787109
Epoch 720, val loss: 0.7710598707199097
Epoch 730, training loss: 12.87359619140625 = 0.22375363111495972 + 2.0 * 6.324921131134033
Epoch 730, val loss: 0.772161066532135
Epoch 740, training loss: 12.85814094543457 = 0.21406826376914978 + 2.0 * 6.322036266326904
Epoch 740, val loss: 0.7737126350402832
Epoch 750, training loss: 12.861543655395508 = 0.20473390817642212 + 2.0 * 6.328404903411865
Epoch 750, val loss: 0.7757074236869812
Epoch 760, training loss: 12.842355728149414 = 0.1957578808069229 + 2.0 * 6.323298931121826
Epoch 760, val loss: 0.7777863144874573
Epoch 770, training loss: 12.830894470214844 = 0.1871822625398636 + 2.0 * 6.3218560218811035
Epoch 770, val loss: 0.780149519443512
Epoch 780, training loss: 12.813238143920898 = 0.17897877097129822 + 2.0 * 6.317129611968994
Epoch 780, val loss: 0.7828887104988098
Epoch 790, training loss: 12.803960800170898 = 0.17114292085170746 + 2.0 * 6.316409111022949
Epoch 790, val loss: 0.7858439087867737
Epoch 800, training loss: 12.796716690063477 = 0.16364161670207977 + 2.0 * 6.316537380218506
Epoch 800, val loss: 0.7890225648880005
Epoch 810, training loss: 12.792064666748047 = 0.15649375319480896 + 2.0 * 6.317785263061523
Epoch 810, val loss: 0.7923276424407959
Epoch 820, training loss: 12.778631210327148 = 0.14971305429935455 + 2.0 * 6.314458847045898
Epoch 820, val loss: 0.7957739233970642
Epoch 830, training loss: 12.765822410583496 = 0.1432812511920929 + 2.0 * 6.311270713806152
Epoch 830, val loss: 0.7995412349700928
Epoch 840, training loss: 12.757473945617676 = 0.13714298605918884 + 2.0 * 6.3101654052734375
Epoch 840, val loss: 0.8034545183181763
Epoch 850, training loss: 12.759506225585938 = 0.13128595054149628 + 2.0 * 6.314110279083252
Epoch 850, val loss: 0.8074726462364197
Epoch 860, training loss: 12.752717971801758 = 0.12568822503089905 + 2.0 * 6.313514709472656
Epoch 860, val loss: 0.8118717074394226
Epoch 870, training loss: 12.73557186126709 = 0.12040437012910843 + 2.0 * 6.307583808898926
Epoch 870, val loss: 0.8159632682800293
Epoch 880, training loss: 12.728354454040527 = 0.11537682265043259 + 2.0 * 6.306488990783691
Epoch 880, val loss: 0.8203908205032349
Epoch 890, training loss: 12.722487449645996 = 0.11058209836483002 + 2.0 * 6.305952548980713
Epoch 890, val loss: 0.8250010013580322
Epoch 900, training loss: 12.733465194702148 = 0.10600457340478897 + 2.0 * 6.313730239868164
Epoch 900, val loss: 0.8295268416404724
Epoch 910, training loss: 12.713207244873047 = 0.10168366134166718 + 2.0 * 6.305761814117432
Epoch 910, val loss: 0.8341756463050842
Epoch 920, training loss: 12.704270362854004 = 0.09758317470550537 + 2.0 * 6.303343772888184
Epoch 920, val loss: 0.8388888239860535
Epoch 930, training loss: 12.698424339294434 = 0.09368348866701126 + 2.0 * 6.302370548248291
Epoch 930, val loss: 0.8438474535942078
Epoch 940, training loss: 12.698890686035156 = 0.08995980024337769 + 2.0 * 6.304465293884277
Epoch 940, val loss: 0.8487456440925598
Epoch 950, training loss: 12.685677528381348 = 0.086419016122818 + 2.0 * 6.299629211425781
Epoch 950, val loss: 0.8536316752433777
Epoch 960, training loss: 12.680704116821289 = 0.08305802941322327 + 2.0 * 6.29882287979126
Epoch 960, val loss: 0.8585754632949829
Epoch 970, training loss: 12.675874710083008 = 0.0798456147313118 + 2.0 * 6.2980146408081055
Epoch 970, val loss: 0.8637558817863464
Epoch 980, training loss: 12.707509994506836 = 0.07679849863052368 + 2.0 * 6.3153557777404785
Epoch 980, val loss: 0.8687687516212463
Epoch 990, training loss: 12.676440238952637 = 0.0738525241613388 + 2.0 * 6.301293849945068
Epoch 990, val loss: 0.8736926317214966
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.762962962962963
0.8128624143384291
=== training gcn model ===
Epoch 0, training loss: 19.1417179107666 = 1.9479953050613403 + 2.0 * 8.596860885620117
Epoch 0, val loss: 1.9505038261413574
Epoch 10, training loss: 19.13126564025879 = 1.938058853149414 + 2.0 * 8.596603393554688
Epoch 10, val loss: 1.9401222467422485
Epoch 20, training loss: 19.114656448364258 = 1.9256991147994995 + 2.0 * 8.594478607177734
Epoch 20, val loss: 1.9272080659866333
Epoch 30, training loss: 19.064090728759766 = 1.9089057445526123 + 2.0 * 8.577592849731445
Epoch 30, val loss: 1.9098435640335083
Epoch 40, training loss: 18.852705001831055 = 1.8882263898849487 + 2.0 * 8.482239723205566
Epoch 40, val loss: 1.8894274234771729
Epoch 50, training loss: 18.220386505126953 = 1.866811990737915 + 2.0 * 8.176787376403809
Epoch 50, val loss: 1.8689372539520264
Epoch 60, training loss: 17.68912696838379 = 1.8489325046539307 + 2.0 * 7.9200968742370605
Epoch 60, val loss: 1.8523244857788086
Epoch 70, training loss: 16.78580093383789 = 1.834816575050354 + 2.0 * 7.475492000579834
Epoch 70, val loss: 1.8392921686172485
Epoch 80, training loss: 16.150663375854492 = 1.8251816034317017 + 2.0 * 7.162741184234619
Epoch 80, val loss: 1.8304245471954346
Epoch 90, training loss: 15.823816299438477 = 1.8134196996688843 + 2.0 * 7.0051984786987305
Epoch 90, val loss: 1.8200526237487793
Epoch 100, training loss: 15.615482330322266 = 1.800666332244873 + 2.0 * 6.907408237457275
Epoch 100, val loss: 1.808882236480713
Epoch 110, training loss: 15.457477569580078 = 1.788669228553772 + 2.0 * 6.834403991699219
Epoch 110, val loss: 1.7979528903961182
Epoch 120, training loss: 15.322952270507812 = 1.7773884534835815 + 2.0 * 6.772781848907471
Epoch 120, val loss: 1.78726327419281
Epoch 130, training loss: 15.220470428466797 = 1.7656482458114624 + 2.0 * 6.727411270141602
Epoch 130, val loss: 1.7760496139526367
Epoch 140, training loss: 15.123910903930664 = 1.7529412508010864 + 2.0 * 6.685484886169434
Epoch 140, val loss: 1.7642730474472046
Epoch 150, training loss: 15.044187545776367 = 1.7391116619110107 + 2.0 * 6.652537822723389
Epoch 150, val loss: 1.7516506910324097
Epoch 160, training loss: 14.97568130493164 = 1.723907470703125 + 2.0 * 6.625886917114258
Epoch 160, val loss: 1.7379697561264038
Epoch 170, training loss: 14.908954620361328 = 1.706870436668396 + 2.0 * 6.6010422706604
Epoch 170, val loss: 1.7227199077606201
Epoch 180, training loss: 14.849533081054688 = 1.6876894235610962 + 2.0 * 6.580921649932861
Epoch 180, val loss: 1.7057512998580933
Epoch 190, training loss: 14.794437408447266 = 1.666240930557251 + 2.0 * 6.564098358154297
Epoch 190, val loss: 1.6868587732315063
Epoch 200, training loss: 14.742439270019531 = 1.6426799297332764 + 2.0 * 6.549879550933838
Epoch 200, val loss: 1.666440725326538
Epoch 210, training loss: 14.687557220458984 = 1.6166998147964478 + 2.0 * 6.535428524017334
Epoch 210, val loss: 1.6438796520233154
Epoch 220, training loss: 14.633255958557129 = 1.5880695581436157 + 2.0 * 6.522593021392822
Epoch 220, val loss: 1.6192958354949951
Epoch 230, training loss: 14.59165096282959 = 1.5567691326141357 + 2.0 * 6.5174407958984375
Epoch 230, val loss: 1.5926326513290405
Epoch 240, training loss: 14.529257774353027 = 1.522962212562561 + 2.0 * 6.503147602081299
Epoch 240, val loss: 1.564077615737915
Epoch 250, training loss: 14.470191955566406 = 1.4868468046188354 + 2.0 * 6.491672515869141
Epoch 250, val loss: 1.5338021516799927
Epoch 260, training loss: 14.41388988494873 = 1.448395848274231 + 2.0 * 6.4827470779418945
Epoch 260, val loss: 1.5019739866256714
Epoch 270, training loss: 14.36672592163086 = 1.4077467918395996 + 2.0 * 6.479489326477051
Epoch 270, val loss: 1.4688341617584229
Epoch 280, training loss: 14.31252670288086 = 1.3657140731811523 + 2.0 * 6.4734063148498535
Epoch 280, val loss: 1.4353653192520142
Epoch 290, training loss: 14.246015548706055 = 1.3228408098220825 + 2.0 * 6.461587429046631
Epoch 290, val loss: 1.4016793966293335
Epoch 300, training loss: 14.188923835754395 = 1.2784712314605713 + 2.0 * 6.455226421356201
Epoch 300, val loss: 1.3676334619522095
Epoch 310, training loss: 14.130777359008789 = 1.233490228652954 + 2.0 * 6.448643684387207
Epoch 310, val loss: 1.3335559368133545
Epoch 320, training loss: 14.081233024597168 = 1.1880998611450195 + 2.0 * 6.446566581726074
Epoch 320, val loss: 1.299775242805481
Epoch 330, training loss: 14.020723342895508 = 1.142976999282837 + 2.0 * 6.438873291015625
Epoch 330, val loss: 1.266992211341858
Epoch 340, training loss: 13.969894409179688 = 1.0983188152313232 + 2.0 * 6.435787677764893
Epoch 340, val loss: 1.2354114055633545
Epoch 350, training loss: 13.916647911071777 = 1.0545369386672974 + 2.0 * 6.431055545806885
Epoch 350, val loss: 1.2052478790283203
Epoch 360, training loss: 13.859469413757324 = 1.0120773315429688 + 2.0 * 6.423696041107178
Epoch 360, val loss: 1.1768670082092285
Epoch 370, training loss: 13.810129165649414 = 0.9708026051521301 + 2.0 * 6.419663429260254
Epoch 370, val loss: 1.1502186059951782
Epoch 380, training loss: 13.762003898620605 = 0.9309828877449036 + 2.0 * 6.415510654449463
Epoch 380, val loss: 1.12534499168396
Epoch 390, training loss: 13.71326732635498 = 0.8929855823516846 + 2.0 * 6.4101409912109375
Epoch 390, val loss: 1.1024304628372192
Epoch 400, training loss: 13.672290802001953 = 0.8564786314964294 + 2.0 * 6.4079060554504395
Epoch 400, val loss: 1.0812757015228271
Epoch 410, training loss: 13.639208793640137 = 0.8218613862991333 + 2.0 * 6.4086737632751465
Epoch 410, val loss: 1.061906337738037
Epoch 420, training loss: 13.589401245117188 = 0.789227306842804 + 2.0 * 6.400086879730225
Epoch 420, val loss: 1.044366478919983
Epoch 430, training loss: 13.548768997192383 = 0.758324384689331 + 2.0 * 6.395222187042236
Epoch 430, val loss: 1.0286784172058105
Epoch 440, training loss: 13.525895118713379 = 0.7288093566894531 + 2.0 * 6.398542881011963
Epoch 440, val loss: 1.0143576860427856
Epoch 450, training loss: 13.480751037597656 = 0.7009240388870239 + 2.0 * 6.389913558959961
Epoch 450, val loss: 1.0015244483947754
Epoch 460, training loss: 13.448118209838867 = 0.6745398640632629 + 2.0 * 6.386789321899414
Epoch 460, val loss: 0.9901041388511658
Epoch 470, training loss: 13.415791511535645 = 0.6493993401527405 + 2.0 * 6.383195877075195
Epoch 470, val loss: 0.9797917008399963
Epoch 480, training loss: 13.393302917480469 = 0.6254141330718994 + 2.0 * 6.383944511413574
Epoch 480, val loss: 0.9705613851547241
Epoch 490, training loss: 13.376782417297363 = 0.6024682521820068 + 2.0 * 6.387156963348389
Epoch 490, val loss: 0.962575376033783
Epoch 500, training loss: 13.331819534301758 = 0.5809731483459473 + 2.0 * 6.375423431396484
Epoch 500, val loss: 0.9553772807121277
Epoch 510, training loss: 13.305397033691406 = 0.5605220198631287 + 2.0 * 6.372437477111816
Epoch 510, val loss: 0.9492717385292053
Epoch 520, training loss: 13.280349731445312 = 0.5409317016601562 + 2.0 * 6.369709014892578
Epoch 520, val loss: 0.9440721869468689
Epoch 530, training loss: 13.257568359375 = 0.522162139415741 + 2.0 * 6.367702960968018
Epoch 530, val loss: 0.9396237134933472
Epoch 540, training loss: 13.239523887634277 = 0.5041926503181458 + 2.0 * 6.367665767669678
Epoch 540, val loss: 0.935784101486206
Epoch 550, training loss: 13.218603134155273 = 0.4871157109737396 + 2.0 * 6.365743637084961
Epoch 550, val loss: 0.9326832890510559
Epoch 560, training loss: 13.194700241088867 = 0.4709380567073822 + 2.0 * 6.361881256103516
Epoch 560, val loss: 0.9303215146064758
Epoch 570, training loss: 13.17370891571045 = 0.4553860127925873 + 2.0 * 6.359161376953125
Epoch 570, val loss: 0.9283674359321594
Epoch 580, training loss: 13.158178329467773 = 0.44041338562965393 + 2.0 * 6.358882427215576
Epoch 580, val loss: 0.9269288778305054
Epoch 590, training loss: 13.135842323303223 = 0.42603030800819397 + 2.0 * 6.35490608215332
Epoch 590, val loss: 0.9259745478630066
Epoch 600, training loss: 13.118467330932617 = 0.41217803955078125 + 2.0 * 6.353144645690918
Epoch 600, val loss: 0.9252909421920776
Epoch 610, training loss: 13.108500480651855 = 0.39878660440444946 + 2.0 * 6.354856967926025
Epoch 610, val loss: 0.9250267148017883
Epoch 620, training loss: 13.085966110229492 = 0.3858242928981781 + 2.0 * 6.350070953369141
Epoch 620, val loss: 0.9251022934913635
Epoch 630, training loss: 13.069509506225586 = 0.373233824968338 + 2.0 * 6.348137855529785
Epoch 630, val loss: 0.9254025220870972
Epoch 640, training loss: 13.063300132751465 = 0.36096593737602234 + 2.0 * 6.35116720199585
Epoch 640, val loss: 0.9260210990905762
Epoch 650, training loss: 13.04542064666748 = 0.3490034341812134 + 2.0 * 6.348208427429199
Epoch 650, val loss: 0.926961362361908
Epoch 660, training loss: 13.025202751159668 = 0.3373410403728485 + 2.0 * 6.343930721282959
Epoch 660, val loss: 0.9280745983123779
Epoch 670, training loss: 13.01203441619873 = 0.325916051864624 + 2.0 * 6.343059062957764
Epoch 670, val loss: 0.9294414520263672
Epoch 680, training loss: 12.995403289794922 = 0.3147027790546417 + 2.0 * 6.340350151062012
Epoch 680, val loss: 0.9309367537498474
Epoch 690, training loss: 12.992756843566895 = 0.30370038747787476 + 2.0 * 6.3445281982421875
Epoch 690, val loss: 0.932508647441864
Epoch 700, training loss: 12.972293853759766 = 0.292985737323761 + 2.0 * 6.339653968811035
Epoch 700, val loss: 0.9347811341285706
Epoch 710, training loss: 12.9547758102417 = 0.2824307680130005 + 2.0 * 6.336172580718994
Epoch 710, val loss: 0.9369358420372009
Epoch 720, training loss: 12.939757347106934 = 0.27215149998664856 + 2.0 * 6.333802700042725
Epoch 720, val loss: 0.9396456480026245
Epoch 730, training loss: 12.933528900146484 = 0.262008398771286 + 2.0 * 6.335760116577148
Epoch 730, val loss: 0.9425347447395325
Epoch 740, training loss: 12.924147605895996 = 0.2521572709083557 + 2.0 * 6.335995197296143
Epoch 740, val loss: 0.9459208250045776
Epoch 750, training loss: 12.905879020690918 = 0.24249662458896637 + 2.0 * 6.331691265106201
Epoch 750, val loss: 0.9494526982307434
Epoch 760, training loss: 12.891362190246582 = 0.23308168351650238 + 2.0 * 6.3291401863098145
Epoch 760, val loss: 0.9534670114517212
Epoch 770, training loss: 12.886359214782715 = 0.22394220530986786 + 2.0 * 6.3312087059021
Epoch 770, val loss: 0.9577450156211853
Epoch 780, training loss: 12.868377685546875 = 0.21505489945411682 + 2.0 * 6.326661586761475
Epoch 780, val loss: 0.962303876876831
Epoch 790, training loss: 12.868330001831055 = 0.20643369853496552 + 2.0 * 6.330948352813721
Epoch 790, val loss: 0.9671541452407837
Epoch 800, training loss: 12.847679138183594 = 0.19809888303279877 + 2.0 * 6.324790000915527
Epoch 800, val loss: 0.9724863767623901
Epoch 810, training loss: 12.836506843566895 = 0.1900358349084854 + 2.0 * 6.323235511779785
Epoch 810, val loss: 0.9777811765670776
Epoch 820, training loss: 12.824251174926758 = 0.1822696328163147 + 2.0 * 6.320990562438965
Epoch 820, val loss: 0.9835279583930969
Epoch 830, training loss: 12.815107345581055 = 0.17474833130836487 + 2.0 * 6.320179462432861
Epoch 830, val loss: 0.9893865585327148
Epoch 840, training loss: 12.808841705322266 = 0.16752327978610992 + 2.0 * 6.320659160614014
Epoch 840, val loss: 0.9954937696456909
Epoch 850, training loss: 12.80095386505127 = 0.16058297455310822 + 2.0 * 6.320185661315918
Epoch 850, val loss: 1.0016028881072998
Epoch 860, training loss: 12.7850980758667 = 0.1539706587791443 + 2.0 * 6.315563678741455
Epoch 860, val loss: 1.007922649383545
Epoch 870, training loss: 12.781506538391113 = 0.14762087166309357 + 2.0 * 6.3169426918029785
Epoch 870, val loss: 1.0144869089126587
Epoch 880, training loss: 12.776497840881348 = 0.14154578745365143 + 2.0 * 6.31747579574585
Epoch 880, val loss: 1.0211410522460938
Epoch 890, training loss: 12.763065338134766 = 0.13575729727745056 + 2.0 * 6.313653945922852
Epoch 890, val loss: 1.0279886722564697
Epoch 900, training loss: 12.761673927307129 = 0.13022294640541077 + 2.0 * 6.315725326538086
Epoch 900, val loss: 1.0346999168395996
Epoch 910, training loss: 12.748514175415039 = 0.12494811415672302 + 2.0 * 6.3117828369140625
Epoch 910, val loss: 1.0416345596313477
Epoch 920, training loss: 12.739981651306152 = 0.1199231967329979 + 2.0 * 6.310029029846191
Epoch 920, val loss: 1.0484486818313599
Epoch 930, training loss: 12.742742538452148 = 0.11513825505971909 + 2.0 * 6.313802242279053
Epoch 930, val loss: 1.0555286407470703
Epoch 940, training loss: 12.731840133666992 = 0.1105504035949707 + 2.0 * 6.310644626617432
Epoch 940, val loss: 1.062256932258606
Epoch 950, training loss: 12.721224784851074 = 0.10621526837348938 + 2.0 * 6.307504653930664
Epoch 950, val loss: 1.0693289041519165
Epoch 960, training loss: 12.712465286254883 = 0.10207149386405945 + 2.0 * 6.305196762084961
Epoch 960, val loss: 1.0763074159622192
Epoch 970, training loss: 12.729076385498047 = 0.09811623394489288 + 2.0 * 6.3154802322387695
Epoch 970, val loss: 1.0831618309020996
Epoch 980, training loss: 12.717321395874023 = 0.09436987340450287 + 2.0 * 6.31147575378418
Epoch 980, val loss: 1.0903502702713013
Epoch 990, training loss: 12.697596549987793 = 0.0907931849360466 + 2.0 * 6.303401470184326
Epoch 990, val loss: 1.0971307754516602
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7185185185185186
0.8065366367949395
The final CL Acc:0.74444, 0.01889, The final GNN Acc:0.80689, 0.00474
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13230])
remove edge: torch.Size([2, 7828])
updated graph: torch.Size([2, 10502])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 19.13446807861328 = 1.940775990486145 + 2.0 * 8.596845626831055
Epoch 0, val loss: 1.9408624172210693
Epoch 10, training loss: 19.12428092956543 = 1.9312458038330078 + 2.0 * 8.596517562866211
Epoch 10, val loss: 1.9315927028656006
Epoch 20, training loss: 19.107046127319336 = 1.9195739030838013 + 2.0 * 8.593735694885254
Epoch 20, val loss: 1.9196921586990356
Epoch 30, training loss: 19.044301986694336 = 1.9039931297302246 + 2.0 * 8.570154190063477
Epoch 30, val loss: 1.9034984111785889
Epoch 40, training loss: 18.653400421142578 = 1.8852428197860718 + 2.0 * 8.384078979492188
Epoch 40, val loss: 1.8843250274658203
Epoch 50, training loss: 17.336618423461914 = 1.865822196006775 + 2.0 * 7.735398292541504
Epoch 50, val loss: 1.8650254011154175
Epoch 60, training loss: 16.814882278442383 = 1.8481744527816772 + 2.0 * 7.483354091644287
Epoch 60, val loss: 1.8487160205841064
Epoch 70, training loss: 16.225568771362305 = 1.8330223560333252 + 2.0 * 7.196272850036621
Epoch 70, val loss: 1.834346055984497
Epoch 80, training loss: 15.805915832519531 = 1.8186448812484741 + 2.0 * 6.993635654449463
Epoch 80, val loss: 1.8204113245010376
Epoch 90, training loss: 15.599343299865723 = 1.8048372268676758 + 2.0 * 6.897253036499023
Epoch 90, val loss: 1.806796669960022
Epoch 100, training loss: 15.465371131896973 = 1.7893255949020386 + 2.0 * 6.838022708892822
Epoch 100, val loss: 1.792233943939209
Epoch 110, training loss: 15.338512420654297 = 1.7739993333816528 + 2.0 * 6.782256603240967
Epoch 110, val loss: 1.7781699895858765
Epoch 120, training loss: 15.21705436706543 = 1.7596055269241333 + 2.0 * 6.728724479675293
Epoch 120, val loss: 1.7646868228912354
Epoch 130, training loss: 15.123779296875 = 1.7438671588897705 + 2.0 * 6.689956188201904
Epoch 130, val loss: 1.7500745058059692
Epoch 140, training loss: 15.052414894104004 = 1.72576904296875 + 2.0 * 6.663322925567627
Epoch 140, val loss: 1.733748197555542
Epoch 150, training loss: 14.981818199157715 = 1.7058266401290894 + 2.0 * 6.637995719909668
Epoch 150, val loss: 1.716024398803711
Epoch 160, training loss: 14.920284271240234 = 1.683609127998352 + 2.0 * 6.618337631225586
Epoch 160, val loss: 1.6964514255523682
Epoch 170, training loss: 14.847121238708496 = 1.6587282419204712 + 2.0 * 6.594196319580078
Epoch 170, val loss: 1.6748648881912231
Epoch 180, training loss: 14.7803373336792 = 1.6310724020004272 + 2.0 * 6.57463264465332
Epoch 180, val loss: 1.6509125232696533
Epoch 190, training loss: 14.718011856079102 = 1.5998423099517822 + 2.0 * 6.559084892272949
Epoch 190, val loss: 1.624064564704895
Epoch 200, training loss: 14.655611038208008 = 1.5652068853378296 + 2.0 * 6.545202255249023
Epoch 200, val loss: 1.5942895412445068
Epoch 210, training loss: 14.587662696838379 = 1.526879072189331 + 2.0 * 6.530391693115234
Epoch 210, val loss: 1.5615671873092651
Epoch 220, training loss: 14.520435333251953 = 1.4848321676254272 + 2.0 * 6.517801761627197
Epoch 220, val loss: 1.5258768796920776
Epoch 230, training loss: 14.45785140991211 = 1.4395748376846313 + 2.0 * 6.509138107299805
Epoch 230, val loss: 1.487541675567627
Epoch 240, training loss: 14.382084846496582 = 1.3918076753616333 + 2.0 * 6.495138645172119
Epoch 240, val loss: 1.447713851928711
Epoch 250, training loss: 14.311327934265137 = 1.3420536518096924 + 2.0 * 6.484637260437012
Epoch 250, val loss: 1.40657639503479
Epoch 260, training loss: 14.254369735717773 = 1.2906193733215332 + 2.0 * 6.481874942779541
Epoch 260, val loss: 1.3646787405014038
Epoch 270, training loss: 14.17764663696289 = 1.2390652894973755 + 2.0 * 6.469290733337402
Epoch 270, val loss: 1.3232519626617432
Epoch 280, training loss: 14.109910011291504 = 1.188138723373413 + 2.0 * 6.460885524749756
Epoch 280, val loss: 1.2828251123428345
Epoch 290, training loss: 14.05018424987793 = 1.138215184211731 + 2.0 * 6.455984592437744
Epoch 290, val loss: 1.2437576055526733
Epoch 300, training loss: 13.9854736328125 = 1.0897986888885498 + 2.0 * 6.4478373527526855
Epoch 300, val loss: 1.2064844369888306
Epoch 310, training loss: 13.928285598754883 = 1.043178915977478 + 2.0 * 6.442553520202637
Epoch 310, val loss: 1.171026587486267
Epoch 320, training loss: 13.872794151306152 = 0.9981315732002258 + 2.0 * 6.437331199645996
Epoch 320, val loss: 1.1374263763427734
Epoch 330, training loss: 13.832292556762695 = 0.955110490322113 + 2.0 * 6.438591003417969
Epoch 330, val loss: 1.1056634187698364
Epoch 340, training loss: 13.768001556396484 = 0.9138966798782349 + 2.0 * 6.4270524978637695
Epoch 340, val loss: 1.0758962631225586
Epoch 350, training loss: 13.719837188720703 = 0.8740147948265076 + 2.0 * 6.422911167144775
Epoch 350, val loss: 1.047464370727539
Epoch 360, training loss: 13.671614646911621 = 0.8351293206214905 + 2.0 * 6.418242454528809
Epoch 360, val loss: 1.0200822353363037
Epoch 370, training loss: 13.641533851623535 = 0.7971531748771667 + 2.0 * 6.422190189361572
Epoch 370, val loss: 0.9937101006507874
Epoch 380, training loss: 13.584214210510254 = 0.7605841755867004 + 2.0 * 6.411815166473389
Epoch 380, val loss: 0.9685482382774353
Epoch 390, training loss: 13.554658889770508 = 0.7254027128219604 + 2.0 * 6.414628028869629
Epoch 390, val loss: 0.9447864890098572
Epoch 400, training loss: 13.49918270111084 = 0.6917403936386108 + 2.0 * 6.403721332550049
Epoch 400, val loss: 0.9224920272827148
Epoch 410, training loss: 13.460065841674805 = 0.659331738948822 + 2.0 * 6.400367259979248
Epoch 410, val loss: 0.9016051292419434
Epoch 420, training loss: 13.423074722290039 = 0.628081202507019 + 2.0 * 6.397496700286865
Epoch 420, val loss: 0.8819657564163208
Epoch 430, training loss: 13.398910522460938 = 0.5979505777359009 + 2.0 * 6.400479793548584
Epoch 430, val loss: 0.8636716604232788
Epoch 440, training loss: 13.35654067993164 = 0.5690613985061646 + 2.0 * 6.393739700317383
Epoch 440, val loss: 0.8468183875083923
Epoch 450, training loss: 13.319149017333984 = 0.5414280891418457 + 2.0 * 6.388860702514648
Epoch 450, val loss: 0.8315188884735107
Epoch 460, training loss: 13.283544540405273 = 0.5147750377655029 + 2.0 * 6.384384632110596
Epoch 460, val loss: 0.8176122307777405
Epoch 470, training loss: 13.273393630981445 = 0.48908311128616333 + 2.0 * 6.392155170440674
Epoch 470, val loss: 0.8049280643463135
Epoch 480, training loss: 13.22757625579834 = 0.4643796384334564 + 2.0 * 6.381598472595215
Epoch 480, val loss: 0.7934499979019165
Epoch 490, training loss: 13.193045616149902 = 0.4409225285053253 + 2.0 * 6.37606143951416
Epoch 490, val loss: 0.7834989428520203
Epoch 500, training loss: 13.163768768310547 = 0.41834133863449097 + 2.0 * 6.372713565826416
Epoch 500, val loss: 0.774700939655304
Epoch 510, training loss: 13.137818336486816 = 0.396621972322464 + 2.0 * 6.370598316192627
Epoch 510, val loss: 0.7669639587402344
Epoch 520, training loss: 13.121824264526367 = 0.3758925497531891 + 2.0 * 6.3729658126831055
Epoch 520, val loss: 0.7601568102836609
Epoch 530, training loss: 13.09264087677002 = 0.35633227229118347 + 2.0 * 6.368154525756836
Epoch 530, val loss: 0.754633367061615
Epoch 540, training loss: 13.066658020019531 = 0.3377816081047058 + 2.0 * 6.364438056945801
Epoch 540, val loss: 0.7501640319824219
Epoch 550, training loss: 13.055978775024414 = 0.32012903690338135 + 2.0 * 6.367924690246582
Epoch 550, val loss: 0.7465055584907532
Epoch 560, training loss: 13.025391578674316 = 0.30332884192466736 + 2.0 * 6.361031532287598
Epoch 560, val loss: 0.7436556220054626
Epoch 570, training loss: 13.001111030578613 = 0.2874739170074463 + 2.0 * 6.356818675994873
Epoch 570, val loss: 0.7416999936103821
Epoch 580, training loss: 12.984674453735352 = 0.2723751962184906 + 2.0 * 6.356149673461914
Epoch 580, val loss: 0.7403306365013123
Epoch 590, training loss: 12.975458145141602 = 0.258066326379776 + 2.0 * 6.358695983886719
Epoch 590, val loss: 0.7394999861717224
Epoch 600, training loss: 12.946195602416992 = 0.24460309743881226 + 2.0 * 6.350796222686768
Epoch 600, val loss: 0.7393625378608704
Epoch 610, training loss: 12.92815113067627 = 0.23190008103847504 + 2.0 * 6.348125457763672
Epoch 610, val loss: 0.7398293018341064
Epoch 620, training loss: 12.912196159362793 = 0.21982668340206146 + 2.0 * 6.346184730529785
Epoch 620, val loss: 0.7408124208450317
Epoch 630, training loss: 12.911486625671387 = 0.2083757370710373 + 2.0 * 6.351555347442627
Epoch 630, val loss: 0.7420477867126465
Epoch 640, training loss: 12.8886137008667 = 0.19755318760871887 + 2.0 * 6.345530033111572
Epoch 640, val loss: 0.7437281012535095
Epoch 650, training loss: 12.867191314697266 = 0.18736819922924042 + 2.0 * 6.339911460876465
Epoch 650, val loss: 0.7458270788192749
Epoch 660, training loss: 12.856388092041016 = 0.17768798768520355 + 2.0 * 6.33935022354126
Epoch 660, val loss: 0.7481934428215027
Epoch 670, training loss: 12.851828575134277 = 0.168512225151062 + 2.0 * 6.341658115386963
Epoch 670, val loss: 0.7508423924446106
Epoch 680, training loss: 12.832304000854492 = 0.15977491438388824 + 2.0 * 6.336264610290527
Epoch 680, val loss: 0.7536354660987854
Epoch 690, training loss: 12.81989574432373 = 0.15161015093326569 + 2.0 * 6.334142684936523
Epoch 690, val loss: 0.756811261177063
Epoch 700, training loss: 12.832626342773438 = 0.14384569227695465 + 2.0 * 6.344390392303467
Epoch 700, val loss: 0.7601413130760193
Epoch 710, training loss: 12.796916961669922 = 0.13660220801830292 + 2.0 * 6.330157279968262
Epoch 710, val loss: 0.763593316078186
Epoch 720, training loss: 12.787698745727539 = 0.12979933619499207 + 2.0 * 6.328949928283691
Epoch 720, val loss: 0.767256498336792
Epoch 730, training loss: 12.778704643249512 = 0.12338031083345413 + 2.0 * 6.327661991119385
Epoch 730, val loss: 0.7710519433021545
Epoch 740, training loss: 12.796852111816406 = 0.11734063178300858 + 2.0 * 6.339755535125732
Epoch 740, val loss: 0.7748773097991943
Epoch 750, training loss: 12.761762619018555 = 0.11165311187505722 + 2.0 * 6.32505464553833
Epoch 750, val loss: 0.7786692380905151
Epoch 760, training loss: 12.75403881072998 = 0.10631878674030304 + 2.0 * 6.323860168457031
Epoch 760, val loss: 0.7826375365257263
Epoch 770, training loss: 12.743152618408203 = 0.10129739344120026 + 2.0 * 6.320927619934082
Epoch 770, val loss: 0.7866764068603516
Epoch 780, training loss: 12.759264945983887 = 0.09655243903398514 + 2.0 * 6.331356048583984
Epoch 780, val loss: 0.7908003330230713
Epoch 790, training loss: 12.735298156738281 = 0.09208886325359344 + 2.0 * 6.3216047286987305
Epoch 790, val loss: 0.7947119474411011
Epoch 800, training loss: 12.723509788513184 = 0.08788800984621048 + 2.0 * 6.317811012268066
Epoch 800, val loss: 0.7988448143005371
Epoch 810, training loss: 12.718473434448242 = 0.08392120897769928 + 2.0 * 6.3172760009765625
Epoch 810, val loss: 0.8030040860176086
Epoch 820, training loss: 12.720952987670898 = 0.08017575740814209 + 2.0 * 6.3203887939453125
Epoch 820, val loss: 0.807125985622406
Epoch 830, training loss: 12.710785865783691 = 0.07666250318288803 + 2.0 * 6.317061901092529
Epoch 830, val loss: 0.8111347556114197
Epoch 840, training loss: 12.698906898498535 = 0.07336917519569397 + 2.0 * 6.312768936157227
Epoch 840, val loss: 0.8152262568473816
Epoch 850, training loss: 12.69510269165039 = 0.07023769617080688 + 2.0 * 6.312432289123535
Epoch 850, val loss: 0.8194271922111511
Epoch 860, training loss: 12.689054489135742 = 0.0672701746225357 + 2.0 * 6.310892105102539
Epoch 860, val loss: 0.8235204815864563
Epoch 870, training loss: 12.700793266296387 = 0.0644533783197403 + 2.0 * 6.318170070648193
Epoch 870, val loss: 0.8275452852249146
Epoch 880, training loss: 12.688467025756836 = 0.061805710196495056 + 2.0 * 6.31333065032959
Epoch 880, val loss: 0.8315739035606384
Epoch 890, training loss: 12.680148124694824 = 0.05929291993379593 + 2.0 * 6.310427665710449
Epoch 890, val loss: 0.8356335163116455
Epoch 900, training loss: 12.674196243286133 = 0.05692337080836296 + 2.0 * 6.308636665344238
Epoch 900, val loss: 0.839654266834259
Epoch 910, training loss: 12.6727876663208 = 0.05466882884502411 + 2.0 * 6.3090596199035645
Epoch 910, val loss: 0.8437160849571228
Epoch 920, training loss: 12.67112922668457 = 0.052538398653268814 + 2.0 * 6.309295177459717
Epoch 920, val loss: 0.8477587699890137
Epoch 930, training loss: 12.661592483520508 = 0.05051508918404579 + 2.0 * 6.305538654327393
Epoch 930, val loss: 0.8516877293586731
Epoch 940, training loss: 12.653397560119629 = 0.04860120639204979 + 2.0 * 6.302398204803467
Epoch 940, val loss: 0.8557299971580505
Epoch 950, training loss: 12.662747383117676 = 0.04678153619170189 + 2.0 * 6.307982921600342
Epoch 950, val loss: 0.8597073554992676
Epoch 960, training loss: 12.661255836486816 = 0.045041799545288086 + 2.0 * 6.308106899261475
Epoch 960, val loss: 0.8636271953582764
Epoch 970, training loss: 12.643728256225586 = 0.04341452568769455 + 2.0 * 6.300157070159912
Epoch 970, val loss: 0.8675467371940613
Epoch 980, training loss: 12.640948295593262 = 0.04186512157320976 + 2.0 * 6.299541473388672
Epoch 980, val loss: 0.8714558482170105
Epoch 990, training loss: 12.639077186584473 = 0.04038752242922783 + 2.0 * 6.299345016479492
Epoch 990, val loss: 0.8753796815872192
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.788888888888889
0.8323668950975225
=== training gcn model ===
Epoch 0, training loss: 19.116430282592773 = 1.9227734804153442 + 2.0 * 8.59682846069336
Epoch 0, val loss: 1.9170446395874023
Epoch 10, training loss: 19.106069564819336 = 1.9132006168365479 + 2.0 * 8.596434593200684
Epoch 10, val loss: 1.9071009159088135
Epoch 20, training loss: 19.088123321533203 = 1.9010593891143799 + 2.0 * 8.593531608581543
Epoch 20, val loss: 1.894457459449768
Epoch 30, training loss: 19.02619743347168 = 1.884429931640625 + 2.0 * 8.570883750915527
Epoch 30, val loss: 1.8773977756500244
Epoch 40, training loss: 18.653099060058594 = 1.8644787073135376 + 2.0 * 8.394309997558594
Epoch 40, val loss: 1.8582921028137207
Epoch 50, training loss: 17.059635162353516 = 1.845704197883606 + 2.0 * 7.606965065002441
Epoch 50, val loss: 1.841242790222168
Epoch 60, training loss: 16.528629302978516 = 1.8307123184204102 + 2.0 * 7.3489580154418945
Epoch 60, val loss: 1.827904462814331
Epoch 70, training loss: 16.017549514770508 = 1.8182772397994995 + 2.0 * 7.099636554718018
Epoch 70, val loss: 1.8165467977523804
Epoch 80, training loss: 15.633228302001953 = 1.8068374395370483 + 2.0 * 6.913195610046387
Epoch 80, val loss: 1.805789828300476
Epoch 90, training loss: 15.418819427490234 = 1.795076608657837 + 2.0 * 6.811871528625488
Epoch 90, val loss: 1.7945817708969116
Epoch 100, training loss: 15.266395568847656 = 1.7811921834945679 + 2.0 * 6.7426018714904785
Epoch 100, val loss: 1.781758189201355
Epoch 110, training loss: 15.140731811523438 = 1.766010046005249 + 2.0 * 6.687360763549805
Epoch 110, val loss: 1.7680697441101074
Epoch 120, training loss: 15.041481971740723 = 1.7505027055740356 + 2.0 * 6.645489692687988
Epoch 120, val loss: 1.7541536092758179
Epoch 130, training loss: 14.955296516418457 = 1.7339950799942017 + 2.0 * 6.610650539398193
Epoch 130, val loss: 1.7389498949050903
Epoch 140, training loss: 14.884130477905273 = 1.7150686979293823 + 2.0 * 6.584530830383301
Epoch 140, val loss: 1.7216765880584717
Epoch 150, training loss: 14.816673278808594 = 1.693617820739746 + 2.0 * 6.561527729034424
Epoch 150, val loss: 1.7023684978485107
Epoch 160, training loss: 14.75373649597168 = 1.6694493293762207 + 2.0 * 6.54214334487915
Epoch 160, val loss: 1.6807188987731934
Epoch 170, training loss: 14.692688941955566 = 1.642047643661499 + 2.0 * 6.525320529937744
Epoch 170, val loss: 1.6563384532928467
Epoch 180, training loss: 14.636102676391602 = 1.6111139059066772 + 2.0 * 6.5124945640563965
Epoch 180, val loss: 1.6290181875228882
Epoch 190, training loss: 14.573616981506348 = 1.5767937898635864 + 2.0 * 6.498411655426025
Epoch 190, val loss: 1.598919153213501
Epoch 200, training loss: 14.512062072753906 = 1.538938283920288 + 2.0 * 6.4865617752075195
Epoch 200, val loss: 1.5658537149429321
Epoch 210, training loss: 14.449545860290527 = 1.4975087642669678 + 2.0 * 6.47601842880249
Epoch 210, val loss: 1.5299524068832397
Epoch 220, training loss: 14.395708084106445 = 1.4528837203979492 + 2.0 * 6.471412181854248
Epoch 220, val loss: 1.491489291191101
Epoch 230, training loss: 14.327033996582031 = 1.4060850143432617 + 2.0 * 6.460474491119385
Epoch 230, val loss: 1.4517549276351929
Epoch 240, training loss: 14.259994506835938 = 1.3577768802642822 + 2.0 * 6.451108932495117
Epoch 240, val loss: 1.4112145900726318
Epoch 250, training loss: 14.199413299560547 = 1.3085004091262817 + 2.0 * 6.445456504821777
Epoch 250, val loss: 1.37029230594635
Epoch 260, training loss: 14.134849548339844 = 1.2592523097991943 + 2.0 * 6.437798500061035
Epoch 260, val loss: 1.3300892114639282
Epoch 270, training loss: 14.075380325317383 = 1.2106428146362305 + 2.0 * 6.432368755340576
Epoch 270, val loss: 1.290766954421997
Epoch 280, training loss: 14.014152526855469 = 1.1623051166534424 + 2.0 * 6.425923824310303
Epoch 280, val loss: 1.25228750705719
Epoch 290, training loss: 13.963729858398438 = 1.1144882440567017 + 2.0 * 6.424620628356934
Epoch 290, val loss: 1.2146856784820557
Epoch 300, training loss: 13.900334358215332 = 1.067922830581665 + 2.0 * 6.416205883026123
Epoch 300, val loss: 1.178346872329712
Epoch 310, training loss: 13.844344139099121 = 1.0221761465072632 + 2.0 * 6.411084175109863
Epoch 310, val loss: 1.1432069540023804
Epoch 320, training loss: 13.79570198059082 = 0.9773538112640381 + 2.0 * 6.409173965454102
Epoch 320, val loss: 1.1091388463974
Epoch 330, training loss: 13.742806434631348 = 0.93407142162323 + 2.0 * 6.404367446899414
Epoch 330, val loss: 1.0764760971069336
Epoch 340, training loss: 13.69001293182373 = 0.8921972513198853 + 2.0 * 6.398907661437988
Epoch 340, val loss: 1.0454431772232056
Epoch 350, training loss: 13.650668144226074 = 0.8519859313964844 + 2.0 * 6.399341106414795
Epoch 350, val loss: 1.0161445140838623
Epoch 360, training loss: 13.597014427185059 = 0.8137663006782532 + 2.0 * 6.3916239738464355
Epoch 360, val loss: 0.988960862159729
Epoch 370, training loss: 13.552289009094238 = 0.777562141418457 + 2.0 * 6.387363433837891
Epoch 370, val loss: 0.9638845920562744
Epoch 380, training loss: 13.517477035522461 = 0.7433243989944458 + 2.0 * 6.387076377868652
Epoch 380, val loss: 0.9409722089767456
Epoch 390, training loss: 13.482348442077637 = 0.7113144993782043 + 2.0 * 6.385517120361328
Epoch 390, val loss: 0.9202660322189331
Epoch 400, training loss: 13.441539764404297 = 0.6813910007476807 + 2.0 * 6.380074501037598
Epoch 400, val loss: 0.9019032120704651
Epoch 410, training loss: 13.40323543548584 = 0.6533447504043579 + 2.0 * 6.374945163726807
Epoch 410, val loss: 0.8856231570243835
Epoch 420, training loss: 13.371777534484863 = 0.6269298791885376 + 2.0 * 6.3724236488342285
Epoch 420, val loss: 0.8711497187614441
Epoch 430, training loss: 13.345646858215332 = 0.6020656824111938 + 2.0 * 6.371790409088135
Epoch 430, val loss: 0.8584718108177185
Epoch 440, training loss: 13.312640190124512 = 0.5787261128425598 + 2.0 * 6.366957187652588
Epoch 440, val loss: 0.8474239110946655
Epoch 450, training loss: 13.284526824951172 = 0.5566492676734924 + 2.0 * 6.363938808441162
Epoch 450, val loss: 0.837744414806366
Epoch 460, training loss: 13.274015426635742 = 0.5356408357620239 + 2.0 * 6.369187355041504
Epoch 460, val loss: 0.8293176889419556
Epoch 470, training loss: 13.234892845153809 = 0.5156587958335876 + 2.0 * 6.359617233276367
Epoch 470, val loss: 0.8219835758209229
Epoch 480, training loss: 13.20898723602295 = 0.49667590856552124 + 2.0 * 6.356155872344971
Epoch 480, val loss: 0.815716564655304
Epoch 490, training loss: 13.187564849853516 = 0.47850582003593445 + 2.0 * 6.35452938079834
Epoch 490, val loss: 0.8102400898933411
Epoch 500, training loss: 13.17300796508789 = 0.4610811471939087 + 2.0 * 6.355963230133057
Epoch 500, val loss: 0.8055302500724792
Epoch 510, training loss: 13.147351264953613 = 0.444344162940979 + 2.0 * 6.351503372192383
Epoch 510, val loss: 0.801545262336731
Epoch 520, training loss: 13.125823020935059 = 0.4283030331134796 + 2.0 * 6.34876012802124
Epoch 520, val loss: 0.7980973124504089
Epoch 530, training loss: 13.103663444519043 = 0.4128367304801941 + 2.0 * 6.3454132080078125
Epoch 530, val loss: 0.7951585650444031
Epoch 540, training loss: 13.104945182800293 = 0.3978128433227539 + 2.0 * 6.3535661697387695
Epoch 540, val loss: 0.7926133871078491
Epoch 550, training loss: 13.067907333374023 = 0.3832893371582031 + 2.0 * 6.34230899810791
Epoch 550, val loss: 0.790444016456604
Epoch 560, training loss: 13.048093795776367 = 0.36915653944015503 + 2.0 * 6.339468479156494
Epoch 560, val loss: 0.7886698246002197
Epoch 570, training loss: 13.039994239807129 = 0.35534489154815674 + 2.0 * 6.342324733734131
Epoch 570, val loss: 0.7871268391609192
Epoch 580, training loss: 13.017505645751953 = 0.3418339490890503 + 2.0 * 6.337835788726807
Epoch 580, val loss: 0.7858099937438965
Epoch 590, training loss: 13.001193046569824 = 0.32860490679740906 + 2.0 * 6.336294174194336
Epoch 590, val loss: 0.7847099304199219
Epoch 600, training loss: 12.986908912658691 = 0.315663605928421 + 2.0 * 6.335622787475586
Epoch 600, val loss: 0.783767819404602
Epoch 610, training loss: 12.964122772216797 = 0.30295631289482117 + 2.0 * 6.330583095550537
Epoch 610, val loss: 0.7830301523208618
Epoch 620, training loss: 12.952617645263672 = 0.29054343700408936 + 2.0 * 6.3310370445251465
Epoch 620, val loss: 0.7825014591217041
Epoch 630, training loss: 12.93427848815918 = 0.27837443351745605 + 2.0 * 6.327951908111572
Epoch 630, val loss: 0.7821086049079895
Epoch 640, training loss: 12.92039680480957 = 0.26654744148254395 + 2.0 * 6.326924800872803
Epoch 640, val loss: 0.7819844484329224
Epoch 650, training loss: 12.903571128845215 = 0.25499197840690613 + 2.0 * 6.324289798736572
Epoch 650, val loss: 0.7821030020713806
Epoch 660, training loss: 12.89864444732666 = 0.2437494844198227 + 2.0 * 6.327447414398193
Epoch 660, val loss: 0.7824917435646057
Epoch 670, training loss: 12.888520240783691 = 0.2328939586877823 + 2.0 * 6.327813148498535
Epoch 670, val loss: 0.7830586433410645
Epoch 680, training loss: 12.863901138305664 = 0.22246479988098145 + 2.0 * 6.320718288421631
Epoch 680, val loss: 0.7839390635490417
Epoch 690, training loss: 12.859729766845703 = 0.21239742636680603 + 2.0 * 6.323666095733643
Epoch 690, val loss: 0.7850642800331116
Epoch 700, training loss: 12.842524528503418 = 0.2027377039194107 + 2.0 * 6.3198933601379395
Epoch 700, val loss: 0.7864053845405579
Epoch 710, training loss: 12.826562881469727 = 0.19347359240055084 + 2.0 * 6.316544532775879
Epoch 710, val loss: 0.7880656719207764
Epoch 720, training loss: 12.814781188964844 = 0.18459154665470123 + 2.0 * 6.315094947814941
Epoch 720, val loss: 0.7899405360221863
Epoch 730, training loss: 12.820368766784668 = 0.1760989874601364 + 2.0 * 6.322134971618652
Epoch 730, val loss: 0.7920914888381958
Epoch 740, training loss: 12.795153617858887 = 0.16796422004699707 + 2.0 * 6.313594818115234
Epoch 740, val loss: 0.7944442629814148
Epoch 750, training loss: 12.785690307617188 = 0.16024881601333618 + 2.0 * 6.312720775604248
Epoch 750, val loss: 0.7969890236854553
Epoch 760, training loss: 12.776134490966797 = 0.15287983417510986 + 2.0 * 6.311627388000488
Epoch 760, val loss: 0.7997677326202393
Epoch 770, training loss: 12.764019966125488 = 0.145880326628685 + 2.0 * 6.309069633483887
Epoch 770, val loss: 0.8027234077453613
Epoch 780, training loss: 12.754860877990723 = 0.13921010494232178 + 2.0 * 6.307825565338135
Epoch 780, val loss: 0.8058580160140991
Epoch 790, training loss: 12.762554168701172 = 0.132851704955101 + 2.0 * 6.3148512840271
Epoch 790, val loss: 0.8092067837715149
Epoch 800, training loss: 12.74764347076416 = 0.1268032193183899 + 2.0 * 6.310420036315918
Epoch 800, val loss: 0.812693178653717
Epoch 810, training loss: 12.733845710754395 = 0.12107253819704056 + 2.0 * 6.306386470794678
Epoch 810, val loss: 0.8163501024246216
Epoch 820, training loss: 12.7227144241333 = 0.11566083878278732 + 2.0 * 6.303526878356934
Epoch 820, val loss: 0.8201056718826294
Epoch 830, training loss: 12.715654373168945 = 0.11050848662853241 + 2.0 * 6.302572727203369
Epoch 830, val loss: 0.8240481615066528
Epoch 840, training loss: 12.72795581817627 = 0.10560664534568787 + 2.0 * 6.311174392700195
Epoch 840, val loss: 0.8280852437019348
Epoch 850, training loss: 12.702816009521484 = 0.10094887018203735 + 2.0 * 6.300933361053467
Epoch 850, val loss: 0.8322311043739319
Epoch 860, training loss: 12.698288917541504 = 0.09654257446527481 + 2.0 * 6.300873279571533
Epoch 860, val loss: 0.8364148139953613
Epoch 870, training loss: 12.6956787109375 = 0.09236407279968262 + 2.0 * 6.301657199859619
Epoch 870, val loss: 0.8407266736030579
Epoch 880, training loss: 12.689665794372559 = 0.08840102702379227 + 2.0 * 6.300632476806641
Epoch 880, val loss: 0.8450508117675781
Epoch 890, training loss: 12.68396282196045 = 0.08464252948760986 + 2.0 * 6.2996602058410645
Epoch 890, val loss: 0.8493790626525879
Epoch 900, training loss: 12.678186416625977 = 0.08108711242675781 + 2.0 * 6.298549652099609
Epoch 900, val loss: 0.8536704778671265
Epoch 910, training loss: 12.670011520385742 = 0.07769989222288132 + 2.0 * 6.29615592956543
Epoch 910, val loss: 0.8581284284591675
Epoch 920, training loss: 12.671338081359863 = 0.07449787110090256 + 2.0 * 6.298419952392578
Epoch 920, val loss: 0.8625528812408447
Epoch 930, training loss: 12.66203498840332 = 0.07146447896957397 + 2.0 * 6.295285224914551
Epoch 930, val loss: 0.8669092655181885
Epoch 940, training loss: 12.655274391174316 = 0.06857727468013763 + 2.0 * 6.293348789215088
Epoch 940, val loss: 0.8714599013328552
Epoch 950, training loss: 12.650306701660156 = 0.06584297120571136 + 2.0 * 6.292232036590576
Epoch 950, val loss: 0.8759335279464722
Epoch 960, training loss: 12.662834167480469 = 0.06323890388011932 + 2.0 * 6.299797534942627
Epoch 960, val loss: 0.8804394006729126
Epoch 970, training loss: 12.648990631103516 = 0.060767702758312225 + 2.0 * 6.294111251831055
Epoch 970, val loss: 0.8849654793739319
Epoch 980, training loss: 12.641831398010254 = 0.05842147395014763 + 2.0 * 6.291705131530762
Epoch 980, val loss: 0.8894376754760742
Epoch 990, training loss: 12.634515762329102 = 0.05618607625365257 + 2.0 * 6.289165019989014
Epoch 990, val loss: 0.8939708471298218
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7814814814814816
0.8386926726410122
=== training gcn model ===
Epoch 0, training loss: 19.15007781982422 = 1.9563357830047607 + 2.0 * 8.596871376037598
Epoch 0, val loss: 1.9629842042922974
Epoch 10, training loss: 19.13939666748047 = 1.9461174011230469 + 2.0 * 8.596639633178711
Epoch 10, val loss: 1.9524785280227661
Epoch 20, training loss: 19.122825622558594 = 1.9335088729858398 + 2.0 * 8.594658851623535
Epoch 20, val loss: 1.9394612312316895
Epoch 30, training loss: 19.075572967529297 = 1.915913462638855 + 2.0 * 8.579830169677734
Epoch 30, val loss: 1.9213536977767944
Epoch 40, training loss: 18.889575958251953 = 1.8924793004989624 + 2.0 * 8.49854850769043
Epoch 40, val loss: 1.897924542427063
Epoch 50, training loss: 18.245683670043945 = 1.8656858205795288 + 2.0 * 8.189998626708984
Epoch 50, val loss: 1.8720835447311401
Epoch 60, training loss: 17.768310546875 = 1.8396635055541992 + 2.0 * 7.964323043823242
Epoch 60, val loss: 1.8478622436523438
Epoch 70, training loss: 17.088016510009766 = 1.8195772171020508 + 2.0 * 7.634220123291016
Epoch 70, val loss: 1.829318642616272
Epoch 80, training loss: 16.478796005249023 = 1.8057655096054077 + 2.0 * 7.336514949798584
Epoch 80, val loss: 1.8163776397705078
Epoch 90, training loss: 16.068246841430664 = 1.7928330898284912 + 2.0 * 7.137707233428955
Epoch 90, val loss: 1.8038675785064697
Epoch 100, training loss: 15.822364807128906 = 1.7757207155227661 + 2.0 * 7.023322105407715
Epoch 100, val loss: 1.787945032119751
Epoch 110, training loss: 15.65366268157959 = 1.757602334022522 + 2.0 * 6.9480299949646
Epoch 110, val loss: 1.7714787721633911
Epoch 120, training loss: 15.486788749694824 = 1.7403534650802612 + 2.0 * 6.873217582702637
Epoch 120, val loss: 1.7557672262191772
Epoch 130, training loss: 15.342989921569824 = 1.7226295471191406 + 2.0 * 6.810180187225342
Epoch 130, val loss: 1.7394475936889648
Epoch 140, training loss: 15.208406448364258 = 1.7025821208953857 + 2.0 * 6.7529120445251465
Epoch 140, val loss: 1.7212163209915161
Epoch 150, training loss: 15.097009658813477 = 1.680431604385376 + 2.0 * 6.70828914642334
Epoch 150, val loss: 1.7014530897140503
Epoch 160, training loss: 15.00582504272461 = 1.6558865308761597 + 2.0 * 6.67496919631958
Epoch 160, val loss: 1.6799520254135132
Epoch 170, training loss: 14.917245864868164 = 1.6286770105361938 + 2.0 * 6.644284248352051
Epoch 170, val loss: 1.6563721895217896
Epoch 180, training loss: 14.838919639587402 = 1.598791480064392 + 2.0 * 6.6200642585754395
Epoch 180, val loss: 1.630365252494812
Epoch 190, training loss: 14.757287979125977 = 1.5662363767623901 + 2.0 * 6.595525741577148
Epoch 190, val loss: 1.6025692224502563
Epoch 200, training loss: 14.680219650268555 = 1.5312352180480957 + 2.0 * 6.574492454528809
Epoch 200, val loss: 1.5727274417877197
Epoch 210, training loss: 14.614017486572266 = 1.4939627647399902 + 2.0 * 6.560027599334717
Epoch 210, val loss: 1.5412966012954712
Epoch 220, training loss: 14.539346694946289 = 1.4548969268798828 + 2.0 * 6.542224884033203
Epoch 220, val loss: 1.5086623430252075
Epoch 230, training loss: 14.470192909240723 = 1.414075255393982 + 2.0 * 6.528059005737305
Epoch 230, val loss: 1.4748388528823853
Epoch 240, training loss: 14.409453392028809 = 1.371781349182129 + 2.0 * 6.51883602142334
Epoch 240, val loss: 1.4398685693740845
Epoch 250, training loss: 14.341489791870117 = 1.3290674686431885 + 2.0 * 6.506211280822754
Epoch 250, val loss: 1.4047380685806274
Epoch 260, training loss: 14.275797843933105 = 1.2860311269760132 + 2.0 * 6.4948835372924805
Epoch 260, val loss: 1.3694498538970947
Epoch 270, training loss: 14.216986656188965 = 1.2426871061325073 + 2.0 * 6.487149715423584
Epoch 270, val loss: 1.3340002298355103
Epoch 280, training loss: 14.159370422363281 = 1.1996086835861206 + 2.0 * 6.4798808097839355
Epoch 280, val loss: 1.2987858057022095
Epoch 290, training loss: 14.094642639160156 = 1.1567819118499756 + 2.0 * 6.468930244445801
Epoch 290, val loss: 1.263864517211914
Epoch 300, training loss: 14.035937309265137 = 1.1138724088668823 + 2.0 * 6.461032390594482
Epoch 300, val loss: 1.2289406061172485
Epoch 310, training loss: 13.981760025024414 = 1.070924997329712 + 2.0 * 6.455417633056641
Epoch 310, val loss: 1.1940877437591553
Epoch 320, training loss: 13.931952476501465 = 1.0286486148834229 + 2.0 * 6.4516520500183105
Epoch 320, val loss: 1.1599928140640259
Epoch 330, training loss: 13.871077537536621 = 0.9874741435050964 + 2.0 * 6.44180154800415
Epoch 330, val loss: 1.126740574836731
Epoch 340, training loss: 13.818512916564941 = 0.9471796751022339 + 2.0 * 6.435666561126709
Epoch 340, val loss: 1.0945264101028442
Epoch 350, training loss: 13.778090476989746 = 0.9079679250717163 + 2.0 * 6.435061454772949
Epoch 350, val loss: 1.0634883642196655
Epoch 360, training loss: 13.725560188293457 = 0.8704760670661926 + 2.0 * 6.427542209625244
Epoch 360, val loss: 1.03409743309021
Epoch 370, training loss: 13.675000190734863 = 0.8346052765846252 + 2.0 * 6.420197486877441
Epoch 370, val loss: 1.0065828561782837
Epoch 380, training loss: 13.63256549835205 = 0.800438642501831 + 2.0 * 6.41606330871582
Epoch 380, val loss: 0.9808381199836731
Epoch 390, training loss: 13.59129524230957 = 0.7680436968803406 + 2.0 * 6.411625862121582
Epoch 390, val loss: 0.957232654094696
Epoch 400, training loss: 13.551838874816895 = 0.7375573515892029 + 2.0 * 6.407140731811523
Epoch 400, val loss: 0.9357719421386719
Epoch 410, training loss: 13.514708518981934 = 0.7086923122406006 + 2.0 * 6.403007984161377
Epoch 410, val loss: 0.9162335991859436
Epoch 420, training loss: 13.505233764648438 = 0.6811866164207458 + 2.0 * 6.412023544311523
Epoch 420, val loss: 0.898591935634613
Epoch 430, training loss: 13.449575424194336 = 0.6552603244781494 + 2.0 * 6.397157669067383
Epoch 430, val loss: 0.8825814723968506
Epoch 440, training loss: 13.416686058044434 = 0.6305131912231445 + 2.0 * 6.3930864334106445
Epoch 440, val loss: 0.8681787848472595
Epoch 450, training loss: 13.385597229003906 = 0.6067360043525696 + 2.0 * 6.389430522918701
Epoch 450, val loss: 0.8551386594772339
Epoch 460, training loss: 13.36031436920166 = 0.5837249755859375 + 2.0 * 6.388294696807861
Epoch 460, val loss: 0.8432693481445312
Epoch 470, training loss: 13.339065551757812 = 0.5615725517272949 + 2.0 * 6.388746738433838
Epoch 470, val loss: 0.8324347734451294
Epoch 480, training loss: 13.301348686218262 = 0.5400886535644531 + 2.0 * 6.380630016326904
Epoch 480, val loss: 0.8227104544639587
Epoch 490, training loss: 13.278450012207031 = 0.5192702412605286 + 2.0 * 6.379590034484863
Epoch 490, val loss: 0.8138993382453918
Epoch 500, training loss: 13.25068187713623 = 0.4990920424461365 + 2.0 * 6.375794887542725
Epoch 500, val loss: 0.805851399898529
Epoch 510, training loss: 13.227149963378906 = 0.4795100688934326 + 2.0 * 6.373819828033447
Epoch 510, val loss: 0.7985490560531616
Epoch 520, training loss: 13.201882362365723 = 0.46041712164878845 + 2.0 * 6.37073278427124
Epoch 520, val loss: 0.7920588254928589
Epoch 530, training loss: 13.18962287902832 = 0.4418765902519226 + 2.0 * 6.373873233795166
Epoch 530, val loss: 0.7863020896911621
Epoch 540, training loss: 13.160839080810547 = 0.4240691065788269 + 2.0 * 6.368384838104248
Epoch 540, val loss: 0.7812797427177429
Epoch 550, training loss: 13.136332511901855 = 0.4068276882171631 + 2.0 * 6.364752292633057
Epoch 550, val loss: 0.7770110964775085
Epoch 560, training loss: 13.113597869873047 = 0.39020925760269165 + 2.0 * 6.3616943359375
Epoch 560, val loss: 0.7734812498092651
Epoch 570, training loss: 13.092109680175781 = 0.3742040991783142 + 2.0 * 6.35895299911499
Epoch 570, val loss: 0.7706320881843567
Epoch 580, training loss: 13.088300704956055 = 0.35875779390335083 + 2.0 * 6.364771366119385
Epoch 580, val loss: 0.7684839963912964
Epoch 590, training loss: 13.05756950378418 = 0.34401175379753113 + 2.0 * 6.356779098510742
Epoch 590, val loss: 0.7668680548667908
Epoch 600, training loss: 13.035650253295898 = 0.3297892212867737 + 2.0 * 6.352930545806885
Epoch 600, val loss: 0.7659823894500732
Epoch 610, training loss: 13.022257804870605 = 0.31614094972610474 + 2.0 * 6.353058338165283
Epoch 610, val loss: 0.7656184434890747
Epoch 620, training loss: 13.001602172851562 = 0.30303892493247986 + 2.0 * 6.3492817878723145
Epoch 620, val loss: 0.7657874822616577
Epoch 630, training loss: 12.982890129089355 = 0.29040226340293884 + 2.0 * 6.346243858337402
Epoch 630, val loss: 0.7665184736251831
Epoch 640, training loss: 12.977503776550293 = 0.2782021164894104 + 2.0 * 6.349650859832764
Epoch 640, val loss: 0.767705500125885
Epoch 650, training loss: 12.953425407409668 = 0.26651954650878906 + 2.0 * 6.3434529304504395
Epoch 650, val loss: 0.7693107724189758
Epoch 660, training loss: 12.93581485748291 = 0.25525084137916565 + 2.0 * 6.340281963348389
Epoch 660, val loss: 0.7713645696640015
Epoch 670, training loss: 12.920086860656738 = 0.24438795447349548 + 2.0 * 6.3378496170043945
Epoch 670, val loss: 0.7738426923751831
Epoch 680, training loss: 12.924725532531738 = 0.23389239609241486 + 2.0 * 6.34541654586792
Epoch 680, val loss: 0.7767748236656189
Epoch 690, training loss: 12.9015474319458 = 0.22379060089588165 + 2.0 * 6.338878631591797
Epoch 690, val loss: 0.7799966335296631
Epoch 700, training loss: 12.883957862854004 = 0.21410268545150757 + 2.0 * 6.334927558898926
Epoch 700, val loss: 0.7835907936096191
Epoch 710, training loss: 12.869974136352539 = 0.20478330552577972 + 2.0 * 6.332595348358154
Epoch 710, val loss: 0.7876039147377014
Epoch 720, training loss: 12.859079360961914 = 0.19581931829452515 + 2.0 * 6.331630229949951
Epoch 720, val loss: 0.791911244392395
Epoch 730, training loss: 12.845927238464355 = 0.1871776133775711 + 2.0 * 6.32937479019165
Epoch 730, val loss: 0.7965129017829895
Epoch 740, training loss: 12.836889266967773 = 0.17886236310005188 + 2.0 * 6.329013347625732
Epoch 740, val loss: 0.8014098405838013
Epoch 750, training loss: 12.840455055236816 = 0.17088763415813446 + 2.0 * 6.334783554077148
Epoch 750, val loss: 0.8065292835235596
Epoch 760, training loss: 12.817047119140625 = 0.16320781409740448 + 2.0 * 6.3269195556640625
Epoch 760, val loss: 0.812000572681427
Epoch 770, training loss: 12.79952335357666 = 0.1559114158153534 + 2.0 * 6.321805953979492
Epoch 770, val loss: 0.8176253437995911
Epoch 780, training loss: 12.789822578430176 = 0.14892932772636414 + 2.0 * 6.320446491241455
Epoch 780, val loss: 0.8235577940940857
Epoch 790, training loss: 12.787967681884766 = 0.14223937690258026 + 2.0 * 6.322864055633545
Epoch 790, val loss: 0.8296877145767212
Epoch 800, training loss: 12.779109001159668 = 0.13582400977611542 + 2.0 * 6.3216423988342285
Epoch 800, val loss: 0.8360448479652405
Epoch 810, training loss: 12.76319408416748 = 0.1297476887702942 + 2.0 * 6.316723346710205
Epoch 810, val loss: 0.8424710035324097
Epoch 820, training loss: 12.755850791931152 = 0.12394446134567261 + 2.0 * 6.315953254699707
Epoch 820, val loss: 0.8491598963737488
Epoch 830, training loss: 12.759394645690918 = 0.11841065436601639 + 2.0 * 6.320491790771484
Epoch 830, val loss: 0.8561145067214966
Epoch 840, training loss: 12.747642517089844 = 0.11315139383077621 + 2.0 * 6.3172454833984375
Epoch 840, val loss: 0.8629443645477295
Epoch 850, training loss: 12.731492042541504 = 0.10813542455434799 + 2.0 * 6.311678409576416
Epoch 850, val loss: 0.8700710535049438
Epoch 860, training loss: 12.727680206298828 = 0.10338086634874344 + 2.0 * 6.312149524688721
Epoch 860, val loss: 0.8773030638694763
Epoch 870, training loss: 12.729394912719727 = 0.09885746240615845 + 2.0 * 6.315268516540527
Epoch 870, val loss: 0.8846725225448608
Epoch 880, training loss: 12.711134910583496 = 0.09454064816236496 + 2.0 * 6.308297157287598
Epoch 880, val loss: 0.8921045064926147
Epoch 890, training loss: 12.706039428710938 = 0.09045619517564774 + 2.0 * 6.307791709899902
Epoch 890, val loss: 0.8997104167938232
Epoch 900, training loss: 12.70142650604248 = 0.08657392114400864 + 2.0 * 6.307426452636719
Epoch 900, val loss: 0.9073317646980286
Epoch 910, training loss: 12.70101261138916 = 0.08288216590881348 + 2.0 * 6.309065341949463
Epoch 910, val loss: 0.9150629043579102
Epoch 920, training loss: 12.696027755737305 = 0.07939339429140091 + 2.0 * 6.308317184448242
Epoch 920, val loss: 0.92280513048172
Epoch 930, training loss: 12.682717323303223 = 0.07608187198638916 + 2.0 * 6.303317546844482
Epoch 930, val loss: 0.930557370185852
Epoch 940, training loss: 12.676506996154785 = 0.07294868677854538 + 2.0 * 6.301779270172119
Epoch 940, val loss: 0.9382642507553101
Epoch 950, training loss: 12.673023223876953 = 0.0699722170829773 + 2.0 * 6.301525592803955
Epoch 950, val loss: 0.9460251927375793
Epoch 960, training loss: 12.70534896850586 = 0.06715869903564453 + 2.0 * 6.319095134735107
Epoch 960, val loss: 0.9536848068237305
Epoch 970, training loss: 12.668533325195312 = 0.064448781311512 + 2.0 * 6.302042484283447
Epoch 970, val loss: 0.9615272879600525
Epoch 980, training loss: 12.6605863571167 = 0.061919864267110825 + 2.0 * 6.299333095550537
Epoch 980, val loss: 0.9691886901855469
Epoch 990, training loss: 12.655224800109863 = 0.05952391028404236 + 2.0 * 6.297850608825684
Epoch 990, val loss: 0.9768239259719849
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7777777777777778
0.8365840801265156
The final CL Acc:0.78272, 0.00462, The final GNN Acc:0.83588, 0.00263
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.4 der2:0.4 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11612])
remove edge: torch.Size([2, 9556])
updated graph: torch.Size([2, 10612])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 19.137413024902344 = 1.9437921047210693 + 2.0 * 8.596810340881348
Epoch 0, val loss: 1.9484376907348633
Epoch 10, training loss: 19.126697540283203 = 1.9339096546173096 + 2.0 * 8.596393585205078
Epoch 10, val loss: 1.9387849569320679
Epoch 20, training loss: 19.108625411987305 = 1.9218095541000366 + 2.0 * 8.59340763092041
Epoch 20, val loss: 1.926895260810852
Epoch 30, training loss: 19.05084228515625 = 1.9053704738616943 + 2.0 * 8.572735786437988
Epoch 30, val loss: 1.910826563835144
Epoch 40, training loss: 18.775169372558594 = 1.8848907947540283 + 2.0 * 8.445138931274414
Epoch 40, val loss: 1.8915495872497559
Epoch 50, training loss: 18.006175994873047 = 1.8630276918411255 + 2.0 * 8.071574211120605
Epoch 50, val loss: 1.8712197542190552
Epoch 60, training loss: 17.139955520629883 = 1.847169041633606 + 2.0 * 7.646393299102783
Epoch 60, val loss: 1.8566505908966064
Epoch 70, training loss: 16.364845275878906 = 1.83957040309906 + 2.0 * 7.262637615203857
Epoch 70, val loss: 1.8485532999038696
Epoch 80, training loss: 15.866803169250488 = 1.8315074443817139 + 2.0 * 7.017647743225098
Epoch 80, val loss: 1.8401800394058228
Epoch 90, training loss: 15.645078659057617 = 1.8195559978485107 + 2.0 * 6.912761211395264
Epoch 90, val loss: 1.8288333415985107
Epoch 100, training loss: 15.442277908325195 = 1.807022213935852 + 2.0 * 6.817627906799316
Epoch 100, val loss: 1.8172154426574707
Epoch 110, training loss: 15.292016983032227 = 1.795979380607605 + 2.0 * 6.748018741607666
Epoch 110, val loss: 1.8064348697662354
Epoch 120, training loss: 15.170707702636719 = 1.7854013442993164 + 2.0 * 6.692653179168701
Epoch 120, val loss: 1.7959243059158325
Epoch 130, training loss: 15.075298309326172 = 1.7743256092071533 + 2.0 * 6.650486469268799
Epoch 130, val loss: 1.7851275205612183
Epoch 140, training loss: 14.999019622802734 = 1.7622071504592896 + 2.0 * 6.618406295776367
Epoch 140, val loss: 1.7736314535140991
Epoch 150, training loss: 14.931392669677734 = 1.7488259077072144 + 2.0 * 6.591283321380615
Epoch 150, val loss: 1.7613232135772705
Epoch 160, training loss: 14.878812789916992 = 1.7338899374008179 + 2.0 * 6.5724616050720215
Epoch 160, val loss: 1.7478358745574951
Epoch 170, training loss: 14.818979263305664 = 1.7172032594680786 + 2.0 * 6.5508880615234375
Epoch 170, val loss: 1.7330265045166016
Epoch 180, training loss: 14.762742042541504 = 1.6986348628997803 + 2.0 * 6.532053470611572
Epoch 180, val loss: 1.7166717052459717
Epoch 190, training loss: 14.719884872436523 = 1.6777185201644897 + 2.0 * 6.521083354949951
Epoch 190, val loss: 1.698409080505371
Epoch 200, training loss: 14.66312026977539 = 1.6543622016906738 + 2.0 * 6.5043792724609375
Epoch 200, val loss: 1.6780823469161987
Epoch 210, training loss: 14.609460830688477 = 1.6282793283462524 + 2.0 * 6.490590572357178
Epoch 210, val loss: 1.6554712057113647
Epoch 220, training loss: 14.561779022216797 = 1.5992337465286255 + 2.0 * 6.4812726974487305
Epoch 220, val loss: 1.630423903465271
Epoch 230, training loss: 14.506097793579102 = 1.5675075054168701 + 2.0 * 6.469295024871826
Epoch 230, val loss: 1.6030980348587036
Epoch 240, training loss: 14.454212188720703 = 1.5332576036453247 + 2.0 * 6.460477352142334
Epoch 240, val loss: 1.5738327503204346
Epoch 250, training loss: 14.396251678466797 = 1.4966343641281128 + 2.0 * 6.449808597564697
Epoch 250, val loss: 1.5428344011306763
Epoch 260, training loss: 14.344756126403809 = 1.4577348232269287 + 2.0 * 6.44351053237915
Epoch 260, val loss: 1.5101757049560547
Epoch 270, training loss: 14.29256534576416 = 1.4175825119018555 + 2.0 * 6.437491416931152
Epoch 270, val loss: 1.4768041372299194
Epoch 280, training loss: 14.232926368713379 = 1.3767541646957397 + 2.0 * 6.428086280822754
Epoch 280, val loss: 1.4433876276016235
Epoch 290, training loss: 14.17790412902832 = 1.3356430530548096 + 2.0 * 6.421130657196045
Epoch 290, val loss: 1.4105561971664429
Epoch 300, training loss: 14.136177062988281 = 1.2945564985275269 + 2.0 * 6.420810222625732
Epoch 300, val loss: 1.378327488899231
Epoch 310, training loss: 14.075735092163086 = 1.2541694641113281 + 2.0 * 6.410782814025879
Epoch 310, val loss: 1.347262978553772
Epoch 320, training loss: 14.023616790771484 = 1.2142069339752197 + 2.0 * 6.404705047607422
Epoch 320, val loss: 1.3173402547836304
Epoch 330, training loss: 13.972556114196777 = 1.174288034439087 + 2.0 * 6.399134159088135
Epoch 330, val loss: 1.2880581617355347
Epoch 340, training loss: 13.938926696777344 = 1.1343508958816528 + 2.0 * 6.40228796005249
Epoch 340, val loss: 1.2593241930007935
Epoch 350, training loss: 13.883768081665039 = 1.0946881771087646 + 2.0 * 6.394539833068848
Epoch 350, val loss: 1.2314268350601196
Epoch 360, training loss: 13.832226753234863 = 1.0553972721099854 + 2.0 * 6.3884148597717285
Epoch 360, val loss: 1.2043349742889404
Epoch 370, training loss: 13.782963752746582 = 1.0164350271224976 + 2.0 * 6.383264541625977
Epoch 370, val loss: 1.1779475212097168
Epoch 380, training loss: 13.736078262329102 = 0.9775493144989014 + 2.0 * 6.3792643547058105
Epoch 380, val loss: 1.1521692276000977
Epoch 390, training loss: 13.693674087524414 = 0.9389199614524841 + 2.0 * 6.377377033233643
Epoch 390, val loss: 1.127016305923462
Epoch 400, training loss: 13.652029991149902 = 0.9008530378341675 + 2.0 * 6.375588417053223
Epoch 400, val loss: 1.102645993232727
Epoch 410, training loss: 13.602543830871582 = 0.8633081316947937 + 2.0 * 6.369617938995361
Epoch 410, val loss: 1.0791112184524536
Epoch 420, training loss: 13.560317993164062 = 0.826353132724762 + 2.0 * 6.366982460021973
Epoch 420, val loss: 1.0564682483673096
Epoch 430, training loss: 13.530061721801758 = 0.7901759743690491 + 2.0 * 6.369942665100098
Epoch 430, val loss: 1.0347341299057007
Epoch 440, training loss: 13.476447105407715 = 0.7549857497215271 + 2.0 * 6.3607306480407715
Epoch 440, val loss: 1.0141727924346924
Epoch 450, training loss: 13.437295913696289 = 0.7206965088844299 + 2.0 * 6.358299732208252
Epoch 450, val loss: 0.9946841597557068
Epoch 460, training loss: 13.40475845336914 = 0.6874265670776367 + 2.0 * 6.358665943145752
Epoch 460, val loss: 0.9760985374450684
Epoch 470, training loss: 13.376374244689941 = 0.6553612947463989 + 2.0 * 6.360506534576416
Epoch 470, val loss: 0.9588465690612793
Epoch 480, training loss: 13.326515197753906 = 0.6247429847717285 + 2.0 * 6.350886344909668
Epoch 480, val loss: 0.9429920315742493
Epoch 490, training loss: 13.294838905334473 = 0.5953834056854248 + 2.0 * 6.349727630615234
Epoch 490, val loss: 0.9283778667449951
Epoch 500, training loss: 13.260858535766602 = 0.5671248435974121 + 2.0 * 6.346867084503174
Epoch 500, val loss: 0.914735734462738
Epoch 510, training loss: 13.23392391204834 = 0.5399444103240967 + 2.0 * 6.346989631652832
Epoch 510, val loss: 0.9021062850952148
Epoch 520, training loss: 13.211654663085938 = 0.5140339136123657 + 2.0 * 6.348810195922852
Epoch 520, val loss: 0.8905457258224487
Epoch 530, training loss: 13.177780151367188 = 0.48942625522613525 + 2.0 * 6.344176769256592
Epoch 530, val loss: 0.8802245259284973
Epoch 540, training loss: 13.147515296936035 = 0.46595826745033264 + 2.0 * 6.340778350830078
Epoch 540, val loss: 0.8709667921066284
Epoch 550, training loss: 13.122061729431152 = 0.44349709153175354 + 2.0 * 6.339282512664795
Epoch 550, val loss: 0.8627113699913025
Epoch 560, training loss: 13.096858024597168 = 0.4219684898853302 + 2.0 * 6.33744478225708
Epoch 560, val loss: 0.8554942011833191
Epoch 570, training loss: 13.070476531982422 = 0.4013338088989258 + 2.0 * 6.334571361541748
Epoch 570, val loss: 0.8491230607032776
Epoch 580, training loss: 13.04891300201416 = 0.3815349340438843 + 2.0 * 6.333689212799072
Epoch 580, val loss: 0.8435913324356079
Epoch 590, training loss: 13.045573234558105 = 0.3626060485839844 + 2.0 * 6.3414835929870605
Epoch 590, val loss: 0.8389711976051331
Epoch 600, training loss: 13.00290584564209 = 0.3444693684577942 + 2.0 * 6.32921838760376
Epoch 600, val loss: 0.835004448890686
Epoch 610, training loss: 12.983453750610352 = 0.3271612823009491 + 2.0 * 6.328146457672119
Epoch 610, val loss: 0.8319441676139832
Epoch 620, training loss: 12.963786125183105 = 0.31057506799697876 + 2.0 * 6.326605319976807
Epoch 620, val loss: 0.8295661807060242
Epoch 630, training loss: 12.945639610290527 = 0.2947051227092743 + 2.0 * 6.325467109680176
Epoch 630, val loss: 0.8276508450508118
Epoch 640, training loss: 12.92908763885498 = 0.2796728312969208 + 2.0 * 6.324707508087158
Epoch 640, val loss: 0.826528787612915
Epoch 650, training loss: 12.910001754760742 = 0.2653658390045166 + 2.0 * 6.322318077087402
Epoch 650, val loss: 0.8259997963905334
Epoch 660, training loss: 12.898574829101562 = 0.2517123818397522 + 2.0 * 6.323431015014648
Epoch 660, val loss: 0.8260875940322876
Epoch 670, training loss: 12.878210067749023 = 0.23876823484897614 + 2.0 * 6.31972074508667
Epoch 670, val loss: 0.8267965316772461
Epoch 680, training loss: 12.863824844360352 = 0.22646068036556244 + 2.0 * 6.3186821937561035
Epoch 680, val loss: 0.8279668688774109
Epoch 690, training loss: 12.858182907104492 = 0.21481147408485413 + 2.0 * 6.321685791015625
Epoch 690, val loss: 0.8296776413917542
Epoch 700, training loss: 12.837301254272461 = 0.20377947390079498 + 2.0 * 6.316761016845703
Epoch 700, val loss: 0.8319311738014221
Epoch 710, training loss: 12.823716163635254 = 0.19335080683231354 + 2.0 * 6.315182685852051
Epoch 710, val loss: 0.8346266150474548
Epoch 720, training loss: 12.810331344604492 = 0.18350689113140106 + 2.0 * 6.313412189483643
Epoch 720, val loss: 0.8377240896224976
Epoch 730, training loss: 12.79705810546875 = 0.17420507967472076 + 2.0 * 6.311426639556885
Epoch 730, val loss: 0.8412868976593018
Epoch 740, training loss: 12.818744659423828 = 0.16547204554080963 + 2.0 * 6.32663631439209
Epoch 740, val loss: 0.8453502655029297
Epoch 750, training loss: 12.776594161987305 = 0.1572113037109375 + 2.0 * 6.309691429138184
Epoch 750, val loss: 0.8495082259178162
Epoch 760, training loss: 12.764007568359375 = 0.1495235562324524 + 2.0 * 6.307241916656494
Epoch 760, val loss: 0.8542201519012451
Epoch 770, training loss: 12.754310607910156 = 0.1422671228647232 + 2.0 * 6.306021690368652
Epoch 770, val loss: 0.8591696619987488
Epoch 780, training loss: 12.745041847229004 = 0.13541856408119202 + 2.0 * 6.304811477661133
Epoch 780, val loss: 0.8644740581512451
Epoch 790, training loss: 12.736987113952637 = 0.12895072996616364 + 2.0 * 6.304018020629883
Epoch 790, val loss: 0.8700160980224609
Epoch 800, training loss: 12.750690460205078 = 0.1228523775935173 + 2.0 * 6.3139190673828125
Epoch 800, val loss: 0.8757034540176392
Epoch 810, training loss: 12.724502563476562 = 0.11714126914739609 + 2.0 * 6.303680419921875
Epoch 810, val loss: 0.8816010355949402
Epoch 820, training loss: 12.71427059173584 = 0.11178463697433472 + 2.0 * 6.301242828369141
Epoch 820, val loss: 0.8876762390136719
Epoch 830, training loss: 12.719576835632324 = 0.10673326998949051 + 2.0 * 6.306421756744385
Epoch 830, val loss: 0.8939118981361389
Epoch 840, training loss: 12.703592300415039 = 0.10197532922029495 + 2.0 * 6.300808429718018
Epoch 840, val loss: 0.9004207253456116
Epoch 850, training loss: 12.695545196533203 = 0.09748237580060959 + 2.0 * 6.2990312576293945
Epoch 850, val loss: 0.9070382118225098
Epoch 860, training loss: 12.692856788635254 = 0.09324602782726288 + 2.0 * 6.299805164337158
Epoch 860, val loss: 0.9137977957725525
Epoch 870, training loss: 12.690210342407227 = 0.08923668414354324 + 2.0 * 6.300487041473389
Epoch 870, val loss: 0.9206047058105469
Epoch 880, training loss: 12.680380821228027 = 0.08545950055122375 + 2.0 * 6.297460556030273
Epoch 880, val loss: 0.9276883602142334
Epoch 890, training loss: 12.67375373840332 = 0.08189109712839127 + 2.0 * 6.295931339263916
Epoch 890, val loss: 0.9346950650215149
Epoch 900, training loss: 12.664682388305664 = 0.07850860059261322 + 2.0 * 6.293087005615234
Epoch 900, val loss: 0.9418628811836243
Epoch 910, training loss: 12.664331436157227 = 0.07530996203422546 + 2.0 * 6.294510841369629
Epoch 910, val loss: 0.9490525126457214
Epoch 920, training loss: 12.66104507446289 = 0.07227770984172821 + 2.0 * 6.294383525848389
Epoch 920, val loss: 0.9561679363250732
Epoch 930, training loss: 12.65249252319336 = 0.06941118836402893 + 2.0 * 6.291540622711182
Epoch 930, val loss: 0.9633675813674927
Epoch 940, training loss: 12.646642684936523 = 0.06669862568378448 + 2.0 * 6.289971828460693
Epoch 940, val loss: 0.9706227779388428
Epoch 950, training loss: 12.650652885437012 = 0.06412108242511749 + 2.0 * 6.2932658195495605
Epoch 950, val loss: 0.9779638051986694
Epoch 960, training loss: 12.647438049316406 = 0.06168273836374283 + 2.0 * 6.292877674102783
Epoch 960, val loss: 0.9852749109268188
Epoch 970, training loss: 12.637713432312012 = 0.05936291441321373 + 2.0 * 6.289175033569336
Epoch 970, val loss: 0.9925268292427063
Epoch 980, training loss: 12.632062911987305 = 0.05715998634696007 + 2.0 * 6.287451267242432
Epoch 980, val loss: 0.9999852776527405
Epoch 990, training loss: 12.634631156921387 = 0.05506427213549614 + 2.0 * 6.289783477783203
Epoch 990, val loss: 1.0072364807128906
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8111111111111111
0.8165524512387982
=== training gcn model ===
Epoch 0, training loss: 19.146533966064453 = 1.9528687000274658 + 2.0 * 8.596832275390625
Epoch 0, val loss: 1.9473975896835327
Epoch 10, training loss: 19.13538360595703 = 1.9423376321792603 + 2.0 * 8.59652328491211
Epoch 10, val loss: 1.93753182888031
Epoch 20, training loss: 19.117977142333984 = 1.929380178451538 + 2.0 * 8.594298362731934
Epoch 20, val loss: 1.9248584508895874
Epoch 30, training loss: 19.0631103515625 = 1.9118099212646484 + 2.0 * 8.575650215148926
Epoch 30, val loss: 1.9072288274765015
Epoch 40, training loss: 18.77569580078125 = 1.8898992538452148 + 2.0 * 8.442898750305176
Epoch 40, val loss: 1.8860312700271606
Epoch 50, training loss: 17.666189193725586 = 1.8679531812667847 + 2.0 * 7.899118423461914
Epoch 50, val loss: 1.8655340671539307
Epoch 60, training loss: 16.53961753845215 = 1.8532509803771973 + 2.0 * 7.3431830406188965
Epoch 60, val loss: 1.8525943756103516
Epoch 70, training loss: 16.02512550354004 = 1.8419671058654785 + 2.0 * 7.091578960418701
Epoch 70, val loss: 1.8417222499847412
Epoch 80, training loss: 15.768068313598633 = 1.82954740524292 + 2.0 * 6.9692606925964355
Epoch 80, val loss: 1.8303152322769165
Epoch 90, training loss: 15.577498435974121 = 1.8158409595489502 + 2.0 * 6.880828857421875
Epoch 90, val loss: 1.81820547580719
Epoch 100, training loss: 15.423933029174805 = 1.8035314083099365 + 2.0 * 6.8102006912231445
Epoch 100, val loss: 1.8071705102920532
Epoch 110, training loss: 15.27916145324707 = 1.7926585674285889 + 2.0 * 6.743251323699951
Epoch 110, val loss: 1.7970730066299438
Epoch 120, training loss: 15.163491249084473 = 1.782302975654602 + 2.0 * 6.69059419631958
Epoch 120, val loss: 1.7872989177703857
Epoch 130, training loss: 15.075532913208008 = 1.7714810371398926 + 2.0 * 6.652026176452637
Epoch 130, val loss: 1.7773853540420532
Epoch 140, training loss: 15.003960609436035 = 1.7594908475875854 + 2.0 * 6.62223482131958
Epoch 140, val loss: 1.7669185400009155
Epoch 150, training loss: 14.937368392944336 = 1.7463055849075317 + 2.0 * 6.595531463623047
Epoch 150, val loss: 1.7556203603744507
Epoch 160, training loss: 14.876103401184082 = 1.7316160202026367 + 2.0 * 6.572243690490723
Epoch 160, val loss: 1.7431895732879639
Epoch 170, training loss: 14.822450637817383 = 1.7150394916534424 + 2.0 * 6.55370569229126
Epoch 170, val loss: 1.7292890548706055
Epoch 180, training loss: 14.767068862915039 = 1.6963927745819092 + 2.0 * 6.535337924957275
Epoch 180, val loss: 1.7136273384094238
Epoch 190, training loss: 14.718889236450195 = 1.6751266717910767 + 2.0 * 6.521881103515625
Epoch 190, val loss: 1.6959233283996582
Epoch 200, training loss: 14.665375709533691 = 1.6511353254318237 + 2.0 * 6.507120132446289
Epoch 200, val loss: 1.675894021987915
Epoch 210, training loss: 14.616067886352539 = 1.6241341829299927 + 2.0 * 6.495966911315918
Epoch 210, val loss: 1.653388500213623
Epoch 220, training loss: 14.57597541809082 = 1.59388267993927 + 2.0 * 6.49104642868042
Epoch 220, val loss: 1.6282116174697876
Epoch 230, training loss: 14.513239860534668 = 1.5606091022491455 + 2.0 * 6.476315498352051
Epoch 230, val loss: 1.6006267070770264
Epoch 240, training loss: 14.458744049072266 = 1.5242633819580078 + 2.0 * 6.467240333557129
Epoch 240, val loss: 1.5706113576889038
Epoch 250, training loss: 14.409345626831055 = 1.4850884675979614 + 2.0 * 6.462128639221191
Epoch 250, val loss: 1.5384840965270996
Epoch 260, training loss: 14.347429275512695 = 1.4439144134521484 + 2.0 * 6.451757431030273
Epoch 260, val loss: 1.5050568580627441
Epoch 270, training loss: 14.29208755493164 = 1.4012280702590942 + 2.0 * 6.445429801940918
Epoch 270, val loss: 1.4706846475601196
Epoch 280, training loss: 14.235016822814941 = 1.357452630996704 + 2.0 * 6.438782215118408
Epoch 280, val loss: 1.4358494281768799
Epoch 290, training loss: 14.184659957885742 = 1.3131859302520752 + 2.0 * 6.435737133026123
Epoch 290, val loss: 1.40122652053833
Epoch 300, training loss: 14.122986793518066 = 1.269118309020996 + 2.0 * 6.426934242248535
Epoch 300, val loss: 1.3674966096878052
Epoch 310, training loss: 14.069615364074707 = 1.2253082990646362 + 2.0 * 6.422153472900391
Epoch 310, val loss: 1.3345471620559692
Epoch 320, training loss: 14.012974739074707 = 1.1818323135375977 + 2.0 * 6.415571212768555
Epoch 320, val loss: 1.3025256395339966
Epoch 330, training loss: 13.96465015411377 = 1.1390743255615234 + 2.0 * 6.412787914276123
Epoch 330, val loss: 1.2716224193572998
Epoch 340, training loss: 13.915369987487793 = 1.0973163843154907 + 2.0 * 6.409026622772217
Epoch 340, val loss: 1.2420899868011475
Epoch 350, training loss: 13.862979888916016 = 1.0567266941070557 + 2.0 * 6.4031267166137695
Epoch 350, val loss: 1.21402907371521
Epoch 360, training loss: 13.813379287719727 = 1.0175896883010864 + 2.0 * 6.397894859313965
Epoch 360, val loss: 1.187610149383545
Epoch 370, training loss: 13.766805648803711 = 0.9795582890510559 + 2.0 * 6.3936238288879395
Epoch 370, val loss: 1.1625441312789917
Epoch 380, training loss: 13.728132247924805 = 0.9426709413528442 + 2.0 * 6.392730712890625
Epoch 380, val loss: 1.138825535774231
Epoch 390, training loss: 13.682043075561523 = 0.9073641896247864 + 2.0 * 6.3873395919799805
Epoch 390, val loss: 1.116644024848938
Epoch 400, training loss: 13.638551712036133 = 0.8733227849006653 + 2.0 * 6.382614612579346
Epoch 400, val loss: 1.0959484577178955
Epoch 410, training loss: 13.611335754394531 = 0.8405553698539734 + 2.0 * 6.385390281677246
Epoch 410, val loss: 1.0765831470489502
Epoch 420, training loss: 13.562426567077637 = 0.809117317199707 + 2.0 * 6.376654624938965
Epoch 420, val loss: 1.0585992336273193
Epoch 430, training loss: 13.526566505432129 = 0.7789579033851624 + 2.0 * 6.373804092407227
Epoch 430, val loss: 1.0419561862945557
Epoch 440, training loss: 13.496438026428223 = 0.7500455379486084 + 2.0 * 6.373196125030518
Epoch 440, val loss: 1.0265523195266724
Epoch 450, training loss: 13.4614896774292 = 0.7222375869750977 + 2.0 * 6.369626045227051
Epoch 450, val loss: 1.0124189853668213
Epoch 460, training loss: 13.424894332885742 = 0.695732593536377 + 2.0 * 6.3645806312561035
Epoch 460, val loss: 0.9994269609451294
Epoch 470, training loss: 13.393163681030273 = 0.6700781583786011 + 2.0 * 6.361542701721191
Epoch 470, val loss: 0.987500786781311
Epoch 480, training loss: 13.365992546081543 = 0.6453066468238831 + 2.0 * 6.360342979431152
Epoch 480, val loss: 0.9766002297401428
Epoch 490, training loss: 13.336250305175781 = 0.6215776205062866 + 2.0 * 6.357336521148682
Epoch 490, val loss: 0.9666913151741028
Epoch 500, training loss: 13.308485984802246 = 0.5986425876617432 + 2.0 * 6.354921817779541
Epoch 500, val loss: 0.957761287689209
Epoch 510, training loss: 13.290290832519531 = 0.5763332843780518 + 2.0 * 6.356978893280029
Epoch 510, val loss: 0.9496904015541077
Epoch 520, training loss: 13.261181831359863 = 0.5547974705696106 + 2.0 * 6.353192329406738
Epoch 520, val loss: 0.9424009919166565
Epoch 530, training loss: 13.233890533447266 = 0.5338813066482544 + 2.0 * 6.35000467300415
Epoch 530, val loss: 0.9359833002090454
Epoch 540, training loss: 13.204599380493164 = 0.5135051012039185 + 2.0 * 6.345547199249268
Epoch 540, val loss: 0.9303884506225586
Epoch 550, training loss: 13.180855751037598 = 0.4936157464981079 + 2.0 * 6.3436198234558105
Epoch 550, val loss: 0.925545871257782
Epoch 560, training loss: 13.170797348022461 = 0.47414928674697876 + 2.0 * 6.348323822021484
Epoch 560, val loss: 0.9214078783988953
Epoch 570, training loss: 13.137883186340332 = 0.4552883803844452 + 2.0 * 6.341297626495361
Epoch 570, val loss: 0.917913019657135
Epoch 580, training loss: 13.111739158630371 = 0.4368957579135895 + 2.0 * 6.337421894073486
Epoch 580, val loss: 0.9152030348777771
Epoch 590, training loss: 13.091988563537598 = 0.41890594363212585 + 2.0 * 6.336541175842285
Epoch 590, val loss: 0.9131888747215271
Epoch 600, training loss: 13.09093189239502 = 0.401326060295105 + 2.0 * 6.3448028564453125
Epoch 600, val loss: 0.9118160009384155
Epoch 610, training loss: 13.051012992858887 = 0.38421785831451416 + 2.0 * 6.333397388458252
Epoch 610, val loss: 0.911141574382782
Epoch 620, training loss: 13.030800819396973 = 0.36758187413215637 + 2.0 * 6.33160924911499
Epoch 620, val loss: 0.9111558198928833
Epoch 630, training loss: 13.013157844543457 = 0.3513512909412384 + 2.0 * 6.330903053283691
Epoch 630, val loss: 0.9118734002113342
Epoch 640, training loss: 12.994053840637207 = 0.3356490731239319 + 2.0 * 6.329202175140381
Epoch 640, val loss: 0.9132065176963806
Epoch 650, training loss: 12.975064277648926 = 0.3205048143863678 + 2.0 * 6.327279567718506
Epoch 650, val loss: 0.9152894020080566
Epoch 660, training loss: 12.956399917602539 = 0.3059045970439911 + 2.0 * 6.325247764587402
Epoch 660, val loss: 0.9180281162261963
Epoch 670, training loss: 12.942930221557617 = 0.2918015420436859 + 2.0 * 6.325564384460449
Epoch 670, val loss: 0.9214273691177368
Epoch 680, training loss: 12.936179161071777 = 0.2782911956310272 + 2.0 * 6.328944206237793
Epoch 680, val loss: 0.9254165887832642
Epoch 690, training loss: 12.911873817443848 = 0.26536107063293457 + 2.0 * 6.323256492614746
Epoch 690, val loss: 0.9301377534866333
Epoch 700, training loss: 12.896218299865723 = 0.25305408239364624 + 2.0 * 6.321582317352295
Epoch 700, val loss: 0.935422956943512
Epoch 710, training loss: 12.880918502807617 = 0.24131165444850922 + 2.0 * 6.319803237915039
Epoch 710, val loss: 0.94130939245224
Epoch 720, training loss: 12.864429473876953 = 0.23010791838169098 + 2.0 * 6.317160606384277
Epoch 720, val loss: 0.9477552771568298
Epoch 730, training loss: 12.850518226623535 = 0.21942460536956787 + 2.0 * 6.315546989440918
Epoch 730, val loss: 0.9547601938247681
Epoch 740, training loss: 12.853346824645996 = 0.20924308896064758 + 2.0 * 6.322052001953125
Epoch 740, val loss: 0.962206244468689
Epoch 750, training loss: 12.835556030273438 = 0.19962213933467865 + 2.0 * 6.317966938018799
Epoch 750, val loss: 0.970009446144104
Epoch 760, training loss: 12.8189697265625 = 0.19048786163330078 + 2.0 * 6.3142409324646
Epoch 760, val loss: 0.9782988429069519
Epoch 770, training loss: 12.807116508483887 = 0.18185214698314667 + 2.0 * 6.312632083892822
Epoch 770, val loss: 0.9869398474693298
Epoch 780, training loss: 12.793357849121094 = 0.17363989353179932 + 2.0 * 6.309858798980713
Epoch 780, val loss: 0.9958112835884094
Epoch 790, training loss: 12.783174514770508 = 0.16584506630897522 + 2.0 * 6.308664798736572
Epoch 790, val loss: 1.0050439834594727
Epoch 800, training loss: 12.772162437438965 = 0.15845248103141785 + 2.0 * 6.306855201721191
Epoch 800, val loss: 1.014540195465088
Epoch 810, training loss: 12.768738746643066 = 0.15142905712127686 + 2.0 * 6.30865478515625
Epoch 810, val loss: 1.02424156665802
Epoch 820, training loss: 12.770727157592773 = 0.14472706615924835 + 2.0 * 6.313000202178955
Epoch 820, val loss: 1.0339646339416504
Epoch 830, training loss: 12.750056266784668 = 0.13845179975032806 + 2.0 * 6.305802345275879
Epoch 830, val loss: 1.0440332889556885
Epoch 840, training loss: 12.738848686218262 = 0.1325034350156784 + 2.0 * 6.303172588348389
Epoch 840, val loss: 1.0542596578598022
Epoch 850, training loss: 12.729133605957031 = 0.12684407830238342 + 2.0 * 6.301144599914551
Epoch 850, val loss: 1.0645695924758911
Epoch 860, training loss: 12.722426414489746 = 0.12146750837564468 + 2.0 * 6.300479412078857
Epoch 860, val loss: 1.075011134147644
Epoch 870, training loss: 12.72799015045166 = 0.1163536012172699 + 2.0 * 6.3058180809021
Epoch 870, val loss: 1.0854085683822632
Epoch 880, training loss: 12.71318244934082 = 0.11152422428131104 + 2.0 * 6.30082893371582
Epoch 880, val loss: 1.0958688259124756
Epoch 890, training loss: 12.702062606811523 = 0.10696965456008911 + 2.0 * 6.29754638671875
Epoch 890, val loss: 1.1064180135726929
Epoch 900, training loss: 12.69688606262207 = 0.10262636840343475 + 2.0 * 6.2971296310424805
Epoch 900, val loss: 1.1169919967651367
Epoch 910, training loss: 12.710166931152344 = 0.09850044548511505 + 2.0 * 6.305833339691162
Epoch 910, val loss: 1.1274573802947998
Epoch 920, training loss: 12.686676979064941 = 0.09457818418741226 + 2.0 * 6.29604959487915
Epoch 920, val loss: 1.1379196643829346
Epoch 930, training loss: 12.678535461425781 = 0.09086096286773682 + 2.0 * 6.293837070465088
Epoch 930, val loss: 1.148477554321289
Epoch 940, training loss: 12.675077438354492 = 0.08732817322015762 + 2.0 * 6.293874740600586
Epoch 940, val loss: 1.1589299440383911
Epoch 950, training loss: 12.676289558410645 = 0.08396010845899582 + 2.0 * 6.296164512634277
Epoch 950, val loss: 1.1692476272583008
Epoch 960, training loss: 12.662679672241211 = 0.08075390011072159 + 2.0 * 6.2909626960754395
Epoch 960, val loss: 1.17966628074646
Epoch 970, training loss: 12.657218933105469 = 0.07771481573581696 + 2.0 * 6.289752006530762
Epoch 970, val loss: 1.1900639533996582
Epoch 980, training loss: 12.65605354309082 = 0.07481265068054199 + 2.0 * 6.29062032699585
Epoch 980, val loss: 1.20025634765625
Epoch 990, training loss: 12.649584770202637 = 0.07204830646514893 + 2.0 * 6.288768291473389
Epoch 990, val loss: 1.2103757858276367
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7407407407407408
0.8191881918819188
=== training gcn model ===
Epoch 0, training loss: 19.15229034423828 = 1.958644151687622 + 2.0 * 8.596822738647461
Epoch 0, val loss: 1.9406582117080688
Epoch 10, training loss: 19.141063690185547 = 1.9480880498886108 + 2.0 * 8.596487998962402
Epoch 10, val loss: 1.930979609489441
Epoch 20, training loss: 19.123075485229492 = 1.9348475933074951 + 2.0 * 8.594114303588867
Epoch 20, val loss: 1.918350100517273
Epoch 30, training loss: 19.070518493652344 = 1.9164862632751465 + 2.0 * 8.57701587677002
Epoch 30, val loss: 1.9004464149475098
Epoch 40, training loss: 18.83428192138672 = 1.8940407037734985 + 2.0 * 8.470120429992676
Epoch 40, val loss: 1.8792760372161865
Epoch 50, training loss: 17.91907501220703 = 1.8714715242385864 + 2.0 * 8.023801803588867
Epoch 50, val loss: 1.8580001592636108
Epoch 60, training loss: 16.860336303710938 = 1.8535914421081543 + 2.0 * 7.5033721923828125
Epoch 60, val loss: 1.842628836631775
Epoch 70, training loss: 16.122783660888672 = 1.8428400754928589 + 2.0 * 7.13997220993042
Epoch 70, val loss: 1.832674264907837
Epoch 80, training loss: 15.806225776672363 = 1.8311752080917358 + 2.0 * 6.987525463104248
Epoch 80, val loss: 1.8217405080795288
Epoch 90, training loss: 15.56502628326416 = 1.8174688816070557 + 2.0 * 6.873778820037842
Epoch 90, val loss: 1.809528112411499
Epoch 100, training loss: 15.386191368103027 = 1.8043453693389893 + 2.0 * 6.790923118591309
Epoch 100, val loss: 1.7984950542449951
Epoch 110, training loss: 15.253798484802246 = 1.7927783727645874 + 2.0 * 6.730510234832764
Epoch 110, val loss: 1.7889056205749512
Epoch 120, training loss: 15.149543762207031 = 1.781736969947815 + 2.0 * 6.683903217315674
Epoch 120, val loss: 1.779593825340271
Epoch 130, training loss: 15.05615520477295 = 1.7706992626190186 + 2.0 * 6.642727851867676
Epoch 130, val loss: 1.7699835300445557
Epoch 140, training loss: 14.980874061584473 = 1.7593098878860474 + 2.0 * 6.610782146453857
Epoch 140, val loss: 1.7599923610687256
Epoch 150, training loss: 14.919672012329102 = 1.747225046157837 + 2.0 * 6.586223602294922
Epoch 150, val loss: 1.749338984489441
Epoch 160, training loss: 14.862191200256348 = 1.7342056035995483 + 2.0 * 6.563992977142334
Epoch 160, val loss: 1.7379916906356812
Epoch 170, training loss: 14.808531761169434 = 1.7199361324310303 + 2.0 * 6.544297695159912
Epoch 170, val loss: 1.7255111932754517
Epoch 180, training loss: 14.763916015625 = 1.7041096687316895 + 2.0 * 6.529903411865234
Epoch 180, val loss: 1.7116631269454956
Epoch 190, training loss: 14.714068412780762 = 1.6866673231124878 + 2.0 * 6.513700485229492
Epoch 190, val loss: 1.6964079141616821
Epoch 200, training loss: 14.668527603149414 = 1.667268991470337 + 2.0 * 6.500629425048828
Epoch 200, val loss: 1.679471492767334
Epoch 210, training loss: 14.62862491607666 = 1.6457014083862305 + 2.0 * 6.491461753845215
Epoch 210, val loss: 1.660662293434143
Epoch 220, training loss: 14.579168319702148 = 1.6218633651733398 + 2.0 * 6.478652477264404
Epoch 220, val loss: 1.6398510932922363
Epoch 230, training loss: 14.536373138427734 = 1.5955934524536133 + 2.0 * 6.4703898429870605
Epoch 230, val loss: 1.6170190572738647
Epoch 240, training loss: 14.48764419555664 = 1.5668946504592896 + 2.0 * 6.46037483215332
Epoch 240, val loss: 1.5921497344970703
Epoch 250, training loss: 14.439619064331055 = 1.5355501174926758 + 2.0 * 6.4520344734191895
Epoch 250, val loss: 1.565224289894104
Epoch 260, training loss: 14.40504264831543 = 1.5015249252319336 + 2.0 * 6.451758861541748
Epoch 260, val loss: 1.5362237691879272
Epoch 270, training loss: 14.345361709594727 = 1.4653551578521729 + 2.0 * 6.440003395080566
Epoch 270, val loss: 1.5055828094482422
Epoch 280, training loss: 14.288450241088867 = 1.427088737487793 + 2.0 * 6.430680751800537
Epoch 280, val loss: 1.4734092950820923
Epoch 290, training loss: 14.23630142211914 = 1.3866848945617676 + 2.0 * 6.424808025360107
Epoch 290, val loss: 1.439769983291626
Epoch 300, training loss: 14.197699546813965 = 1.3442132472991943 + 2.0 * 6.426743030548096
Epoch 300, val loss: 1.4047973155975342
Epoch 310, training loss: 14.13227653503418 = 1.3005497455596924 + 2.0 * 6.415863513946533
Epoch 310, val loss: 1.3690848350524902
Epoch 320, training loss: 14.075189590454102 = 1.2558928728103638 + 2.0 * 6.409648418426514
Epoch 320, val loss: 1.3330562114715576
Epoch 330, training loss: 14.019896507263184 = 1.2103685140609741 + 2.0 * 6.404764175415039
Epoch 330, val loss: 1.296773910522461
Epoch 340, training loss: 13.966827392578125 = 1.1645597219467163 + 2.0 * 6.401134014129639
Epoch 340, val loss: 1.2605961561203003
Epoch 350, training loss: 13.910234451293945 = 1.1190861463546753 + 2.0 * 6.39557409286499
Epoch 350, val loss: 1.2253615856170654
Epoch 360, training loss: 13.857359886169434 = 1.0742061138153076 + 2.0 * 6.391576766967773
Epoch 360, val loss: 1.191066026687622
Epoch 370, training loss: 13.815196990966797 = 1.0301620960235596 + 2.0 * 6.392517566680908
Epoch 370, val loss: 1.1578675508499146
Epoch 380, training loss: 13.759521484375 = 0.9875550270080566 + 2.0 * 6.385983467102051
Epoch 380, val loss: 1.1261879205703735
Epoch 390, training loss: 13.7090482711792 = 0.946496307849884 + 2.0 * 6.3812761306762695
Epoch 390, val loss: 1.096367597579956
Epoch 400, training loss: 13.662943840026855 = 0.9071406126022339 + 2.0 * 6.377901554107666
Epoch 400, val loss: 1.0683839321136475
Epoch 410, training loss: 13.61961841583252 = 0.8695061206817627 + 2.0 * 6.375056266784668
Epoch 410, val loss: 1.0421996116638184
Epoch 420, training loss: 13.576876640319824 = 0.8335849046707153 + 2.0 * 6.371645927429199
Epoch 420, val loss: 1.0178117752075195
Epoch 430, training loss: 13.543110847473145 = 0.7993134260177612 + 2.0 * 6.371898651123047
Epoch 430, val loss: 0.9951366186141968
Epoch 440, training loss: 13.503527641296387 = 0.7667520046234131 + 2.0 * 6.368387699127197
Epoch 440, val loss: 0.9740793704986572
Epoch 450, training loss: 13.460577011108398 = 0.7356947064399719 + 2.0 * 6.362441062927246
Epoch 450, val loss: 0.9546980261802673
Epoch 460, training loss: 13.43761157989502 = 0.7058675289154053 + 2.0 * 6.365871906280518
Epoch 460, val loss: 0.9366096258163452
Epoch 470, training loss: 13.396320343017578 = 0.6774118542671204 + 2.0 * 6.359454154968262
Epoch 470, val loss: 0.9195744395256042
Epoch 480, training loss: 13.35996150970459 = 0.6498786807060242 + 2.0 * 6.35504150390625
Epoch 480, val loss: 0.9037418365478516
Epoch 490, training loss: 13.335139274597168 = 0.6232166886329651 + 2.0 * 6.355961322784424
Epoch 490, val loss: 0.8888081312179565
Epoch 500, training loss: 13.305583000183105 = 0.5974503755569458 + 2.0 * 6.354066371917725
Epoch 500, val loss: 0.8747808337211609
Epoch 510, training loss: 13.270009994506836 = 0.5725159645080566 + 2.0 * 6.348747253417969
Epoch 510, val loss: 0.8617710471153259
Epoch 520, training loss: 13.241094589233398 = 0.5482719540596008 + 2.0 * 6.346411228179932
Epoch 520, val loss: 0.8495461344718933
Epoch 530, training loss: 13.23017692565918 = 0.5247416496276855 + 2.0 * 6.352717876434326
Epoch 530, val loss: 0.8380178809165955
Epoch 540, training loss: 13.190821647644043 = 0.5019509196281433 + 2.0 * 6.344435214996338
Epoch 540, val loss: 0.8273563385009766
Epoch 550, training loss: 13.162856101989746 = 0.4798743724822998 + 2.0 * 6.341490745544434
Epoch 550, val loss: 0.8176323175430298
Epoch 560, training loss: 13.140419006347656 = 0.4585605561733246 + 2.0 * 6.34092903137207
Epoch 560, val loss: 0.8086121082305908
Epoch 570, training loss: 13.115095138549805 = 0.43797004222869873 + 2.0 * 6.338562488555908
Epoch 570, val loss: 0.8002720475196838
Epoch 580, training loss: 13.09266185760498 = 0.41801705956459045 + 2.0 * 6.337322235107422
Epoch 580, val loss: 0.7926536798477173
Epoch 590, training loss: 13.067634582519531 = 0.398754358291626 + 2.0 * 6.334440231323242
Epoch 590, val loss: 0.7857645750045776
Epoch 600, training loss: 13.047589302062988 = 0.3801652491092682 + 2.0 * 6.333712100982666
Epoch 600, val loss: 0.7794556021690369
Epoch 610, training loss: 13.026719093322754 = 0.36219218373298645 + 2.0 * 6.332263469696045
Epoch 610, val loss: 0.7736985087394714
Epoch 620, training loss: 13.006591796875 = 0.34493860602378845 + 2.0 * 6.330826759338379
Epoch 620, val loss: 0.7683964967727661
Epoch 630, training loss: 12.982840538024902 = 0.3283422887325287 + 2.0 * 6.327249050140381
Epoch 630, val loss: 0.7638229727745056
Epoch 640, training loss: 12.968327522277832 = 0.3123735785484314 + 2.0 * 6.327977180480957
Epoch 640, val loss: 0.7597421407699585
Epoch 650, training loss: 12.952712059020996 = 0.2969741225242615 + 2.0 * 6.327868938446045
Epoch 650, val loss: 0.7559698820114136
Epoch 660, training loss: 12.92903995513916 = 0.2822958528995514 + 2.0 * 6.323371887207031
Epoch 660, val loss: 0.7530117034912109
Epoch 670, training loss: 12.912400245666504 = 0.2681941092014313 + 2.0 * 6.322103023529053
Epoch 670, val loss: 0.7504419684410095
Epoch 680, training loss: 12.894857406616211 = 0.2546406388282776 + 2.0 * 6.320108413696289
Epoch 680, val loss: 0.7483425736427307
Epoch 690, training loss: 12.90231704711914 = 0.24165962636470795 + 2.0 * 6.330328941345215
Epoch 690, val loss: 0.746609091758728
Epoch 700, training loss: 12.868034362792969 = 0.22933664917945862 + 2.0 * 6.3193488121032715
Epoch 700, val loss: 0.7454783320426941
Epoch 710, training loss: 12.852246284484863 = 0.21760760247707367 + 2.0 * 6.317319393157959
Epoch 710, val loss: 0.7448132038116455
Epoch 720, training loss: 12.836505889892578 = 0.20648668706417084 + 2.0 * 6.315009593963623
Epoch 720, val loss: 0.7446146607398987
Epoch 730, training loss: 12.824369430541992 = 0.19594192504882812 + 2.0 * 6.314213752746582
Epoch 730, val loss: 0.7448291778564453
Epoch 740, training loss: 12.828937530517578 = 0.18591749668121338 + 2.0 * 6.321509838104248
Epoch 740, val loss: 0.7453911304473877
Epoch 750, training loss: 12.80049991607666 = 0.17652110755443573 + 2.0 * 6.3119893074035645
Epoch 750, val loss: 0.7464596629142761
Epoch 760, training loss: 12.791465759277344 = 0.16763979196548462 + 2.0 * 6.311913013458252
Epoch 760, val loss: 0.7479616403579712
Epoch 770, training loss: 12.781187057495117 = 0.15925398468971252 + 2.0 * 6.310966491699219
Epoch 770, val loss: 0.7497527003288269
Epoch 780, training loss: 12.768739700317383 = 0.15136487782001495 + 2.0 * 6.308687210083008
Epoch 780, val loss: 0.7518764734268188
Epoch 790, training loss: 12.764683723449707 = 0.14395664632320404 + 2.0 * 6.31036376953125
Epoch 790, val loss: 0.7543532252311707
Epoch 800, training loss: 12.754305839538574 = 0.13696299493312836 + 2.0 * 6.308671474456787
Epoch 800, val loss: 0.7571020126342773
Epoch 810, training loss: 12.749626159667969 = 0.13041172921657562 + 2.0 * 6.309607028961182
Epoch 810, val loss: 0.7601045370101929
Epoch 820, training loss: 12.732830047607422 = 0.1242559403181076 + 2.0 * 6.304286956787109
Epoch 820, val loss: 0.7633219957351685
Epoch 830, training loss: 12.725114822387695 = 0.11847730726003647 + 2.0 * 6.303318977355957
Epoch 830, val loss: 0.7669166922569275
Epoch 840, training loss: 12.716296195983887 = 0.11305493116378784 + 2.0 * 6.3016204833984375
Epoch 840, val loss: 0.7705528140068054
Epoch 850, training loss: 12.728167533874512 = 0.10795123130083084 + 2.0 * 6.310108184814453
Epoch 850, val loss: 0.7743075489997864
Epoch 860, training loss: 12.704903602600098 = 0.10314227640628815 + 2.0 * 6.300880432128906
Epoch 860, val loss: 0.7783210277557373
Epoch 870, training loss: 12.695343017578125 = 0.09862697869539261 + 2.0 * 6.298357963562012
Epoch 870, val loss: 0.7825174331665039
Epoch 880, training loss: 12.689712524414062 = 0.09437204897403717 + 2.0 * 6.297670364379883
Epoch 880, val loss: 0.786777675151825
Epoch 890, training loss: 12.705723762512207 = 0.09034840762615204 + 2.0 * 6.307687759399414
Epoch 890, val loss: 0.7911057472229004
Epoch 900, training loss: 12.680846214294434 = 0.08656053245067596 + 2.0 * 6.29714298248291
Epoch 900, val loss: 0.7955747842788696
Epoch 910, training loss: 12.678179740905762 = 0.08298739045858383 + 2.0 * 6.297595977783203
Epoch 910, val loss: 0.8002195358276367
Epoch 920, training loss: 12.668326377868652 = 0.079616479575634 + 2.0 * 6.2943549156188965
Epoch 920, val loss: 0.804826557636261
Epoch 930, training loss: 12.670333862304688 = 0.0764237567782402 + 2.0 * 6.296955108642578
Epoch 930, val loss: 0.8095318675041199
Epoch 940, training loss: 12.660440444946289 = 0.07341129332780838 + 2.0 * 6.293514728546143
Epoch 940, val loss: 0.8143312931060791
Epoch 950, training loss: 12.663026809692383 = 0.07054571807384491 + 2.0 * 6.296240329742432
Epoch 950, val loss: 0.8190523982048035
Epoch 960, training loss: 12.65156078338623 = 0.06783019751310349 + 2.0 * 6.291865348815918
Epoch 960, val loss: 0.8238614797592163
Epoch 970, training loss: 12.645501136779785 = 0.06525654345750809 + 2.0 * 6.2901225090026855
Epoch 970, val loss: 0.8286762833595276
Epoch 980, training loss: 12.667553901672363 = 0.06280402839183807 + 2.0 * 6.302374839782715
Epoch 980, val loss: 0.833276629447937
Epoch 990, training loss: 12.636534690856934 = 0.06050319969654083 + 2.0 * 6.288015842437744
Epoch 990, val loss: 0.8383104205131531
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.762962962962963
0.8149710068529257
The final CL Acc:0.77160, 0.02937, The final GNN Acc:0.81690, 0.00174
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13152])
remove edge: torch.Size([2, 8028])
updated graph: torch.Size([2, 10624])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 19.125354766845703 = 1.9316742420196533 + 2.0 * 8.596839904785156
Epoch 0, val loss: 1.9270037412643433
Epoch 10, training loss: 19.114776611328125 = 1.921678066253662 + 2.0 * 8.596549034118652
Epoch 10, val loss: 1.9166673421859741
Epoch 20, training loss: 19.097810745239258 = 1.9090763330459595 + 2.0 * 8.594367027282715
Epoch 20, val loss: 1.9036093950271606
Epoch 30, training loss: 19.046586990356445 = 1.891441822052002 + 2.0 * 8.5775728225708
Epoch 30, val loss: 1.8855072259902954
Epoch 40, training loss: 18.80994987487793 = 1.8686705827713013 + 2.0 * 8.4706392288208
Epoch 40, val loss: 1.863246202468872
Epoch 50, training loss: 17.949874877929688 = 1.8438829183578491 + 2.0 * 8.052995681762695
Epoch 50, val loss: 1.8396960496902466
Epoch 60, training loss: 17.156082153320312 = 1.823362112045288 + 2.0 * 7.666360378265381
Epoch 60, val loss: 1.8212939500808716
Epoch 70, training loss: 16.247142791748047 = 1.8127379417419434 + 2.0 * 7.217202186584473
Epoch 70, val loss: 1.8119932413101196
Epoch 80, training loss: 15.812439918518066 = 1.8048126697540283 + 2.0 * 7.003813743591309
Epoch 80, val loss: 1.8042618036270142
Epoch 90, training loss: 15.555001258850098 = 1.7919284105300903 + 2.0 * 6.881536483764648
Epoch 90, val loss: 1.7925840616226196
Epoch 100, training loss: 15.407550811767578 = 1.777782678604126 + 2.0 * 6.814884185791016
Epoch 100, val loss: 1.780192255973816
Epoch 110, training loss: 15.278356552124023 = 1.7649180889129639 + 2.0 * 6.75671911239624
Epoch 110, val loss: 1.7688297033309937
Epoch 120, training loss: 15.172962188720703 = 1.7528420686721802 + 2.0 * 6.710060119628906
Epoch 120, val loss: 1.7579388618469238
Epoch 130, training loss: 15.082056045532227 = 1.739814043045044 + 2.0 * 6.671121120452881
Epoch 130, val loss: 1.7464665174484253
Epoch 140, training loss: 15.003726959228516 = 1.7252342700958252 + 2.0 * 6.639246463775635
Epoch 140, val loss: 1.7340737581253052
Epoch 150, training loss: 14.93183708190918 = 1.70881986618042 + 2.0 * 6.611508369445801
Epoch 150, val loss: 1.720301628112793
Epoch 160, training loss: 14.86208438873291 = 1.6903855800628662 + 2.0 * 6.585849285125732
Epoch 160, val loss: 1.704941749572754
Epoch 170, training loss: 14.79788589477539 = 1.6694200038909912 + 2.0 * 6.56423282623291
Epoch 170, val loss: 1.6876702308654785
Epoch 180, training loss: 14.741681098937988 = 1.6455131769180298 + 2.0 * 6.548083782196045
Epoch 180, val loss: 1.6681005954742432
Epoch 190, training loss: 14.675851821899414 = 1.6183704137802124 + 2.0 * 6.528740882873535
Epoch 190, val loss: 1.645929217338562
Epoch 200, training loss: 14.618206977844238 = 1.5877304077148438 + 2.0 * 6.515238285064697
Epoch 200, val loss: 1.6209508180618286
Epoch 210, training loss: 14.551492691040039 = 1.5538307428359985 + 2.0 * 6.498830795288086
Epoch 210, val loss: 1.5932977199554443
Epoch 220, training loss: 14.487791061401367 = 1.5165308713912964 + 2.0 * 6.485630035400391
Epoch 220, val loss: 1.5629771947860718
Epoch 230, training loss: 14.435503005981445 = 1.4759273529052734 + 2.0 * 6.479787826538086
Epoch 230, val loss: 1.530163288116455
Epoch 240, training loss: 14.357791900634766 = 1.4329837560653687 + 2.0 * 6.462404251098633
Epoch 240, val loss: 1.4959646463394165
Epoch 250, training loss: 14.293411254882812 = 1.388176679611206 + 2.0 * 6.452617168426514
Epoch 250, val loss: 1.460351586341858
Epoch 260, training loss: 14.228903770446777 = 1.341921091079712 + 2.0 * 6.443491458892822
Epoch 260, val loss: 1.4240972995758057
Epoch 270, training loss: 14.171034812927246 = 1.2952500581741333 + 2.0 * 6.437892436981201
Epoch 270, val loss: 1.3882014751434326
Epoch 280, training loss: 14.109493255615234 = 1.2493802309036255 + 2.0 * 6.430056571960449
Epoch 280, val loss: 1.3533401489257812
Epoch 290, training loss: 14.048428535461426 = 1.204411268234253 + 2.0 * 6.422008514404297
Epoch 290, val loss: 1.3196966648101807
Epoch 300, training loss: 13.9923734664917 = 1.1605383157730103 + 2.0 * 6.41591739654541
Epoch 300, val loss: 1.2874799966812134
Epoch 310, training loss: 13.946136474609375 = 1.1182022094726562 + 2.0 * 6.413967132568359
Epoch 310, val loss: 1.256926417350769
Epoch 320, training loss: 13.890959739685059 = 1.0782060623168945 + 2.0 * 6.406376838684082
Epoch 320, val loss: 1.2286080121994019
Epoch 330, training loss: 13.841501235961914 = 1.0401540994644165 + 2.0 * 6.4006733894348145
Epoch 330, val loss: 1.202233910560608
Epoch 340, training loss: 13.806131362915039 = 1.0039234161376953 + 2.0 * 6.401103973388672
Epoch 340, val loss: 1.1774845123291016
Epoch 350, training loss: 13.752920150756836 = 0.9696509838104248 + 2.0 * 6.391634464263916
Epoch 350, val loss: 1.1543314456939697
Epoch 360, training loss: 13.712092399597168 = 0.9369512796401978 + 2.0 * 6.387570381164551
Epoch 360, val loss: 1.132663369178772
Epoch 370, training loss: 13.671882629394531 = 0.9055258631706238 + 2.0 * 6.383178234100342
Epoch 370, val loss: 1.1121541261672974
Epoch 380, training loss: 13.642782211303711 = 0.8754511475563049 + 2.0 * 6.383665561676025
Epoch 380, val loss: 1.0926707983016968
Epoch 390, training loss: 13.600174903869629 = 0.8468653559684753 + 2.0 * 6.376654624938965
Epoch 390, val loss: 1.0744787454605103
Epoch 400, training loss: 13.565006256103516 = 0.8193168640136719 + 2.0 * 6.372844696044922
Epoch 400, val loss: 1.0572566986083984
Epoch 410, training loss: 13.539105415344238 = 0.7925809621810913 + 2.0 * 6.373262405395508
Epoch 410, val loss: 1.0407565832138062
Epoch 420, training loss: 13.49985122680664 = 0.7668009400367737 + 2.0 * 6.366525173187256
Epoch 420, val loss: 1.0250751972198486
Epoch 430, training loss: 13.466511726379395 = 0.7417102456092834 + 2.0 * 6.362400531768799
Epoch 430, val loss: 1.0102431774139404
Epoch 440, training loss: 13.436232566833496 = 0.7171918153762817 + 2.0 * 6.359520435333252
Epoch 440, val loss: 0.9960983991622925
Epoch 450, training loss: 13.407801628112793 = 0.6933151483535767 + 2.0 * 6.357243061065674
Epoch 450, val loss: 0.9825534224510193
Epoch 460, training loss: 13.379796028137207 = 0.6700509786605835 + 2.0 * 6.354872703552246
Epoch 460, val loss: 0.9698352813720703
Epoch 470, training loss: 13.35074520111084 = 0.6473726034164429 + 2.0 * 6.351686477661133
Epoch 470, val loss: 0.9577735066413879
Epoch 480, training loss: 13.322118759155273 = 0.6252110004425049 + 2.0 * 6.348453998565674
Epoch 480, val loss: 0.9463942050933838
Epoch 490, training loss: 13.300036430358887 = 0.6034662127494812 + 2.0 * 6.34828519821167
Epoch 490, val loss: 0.9355804324150085
Epoch 500, training loss: 13.2737398147583 = 0.5821588039398193 + 2.0 * 6.345790386199951
Epoch 500, val loss: 0.9253167510032654
Epoch 510, training loss: 13.244640350341797 = 0.5612124800682068 + 2.0 * 6.341713905334473
Epoch 510, val loss: 0.9155929088592529
Epoch 520, training loss: 13.221487045288086 = 0.5406114459037781 + 2.0 * 6.340437889099121
Epoch 520, val loss: 0.9062641263008118
Epoch 530, training loss: 13.195378303527832 = 0.5204009413719177 + 2.0 * 6.337488651275635
Epoch 530, val loss: 0.8973720073699951
Epoch 540, training loss: 13.182661056518555 = 0.5005224943161011 + 2.0 * 6.341069221496582
Epoch 540, val loss: 0.8888464570045471
Epoch 550, training loss: 13.14907455444336 = 0.4811171889305115 + 2.0 * 6.333978652954102
Epoch 550, val loss: 0.8806562423706055
Epoch 560, training loss: 13.125935554504395 = 0.4619741141796112 + 2.0 * 6.3319807052612305
Epoch 560, val loss: 0.8727923631668091
Epoch 570, training loss: 13.101592063903809 = 0.44315287470817566 + 2.0 * 6.329219818115234
Epoch 570, val loss: 0.8652496337890625
Epoch 580, training loss: 13.089131355285645 = 0.42463526129722595 + 2.0 * 6.332248210906982
Epoch 580, val loss: 0.8579603433609009
Epoch 590, training loss: 13.069043159484863 = 0.40655016899108887 + 2.0 * 6.331246376037598
Epoch 590, val loss: 0.8510442972183228
Epoch 600, training loss: 13.042332649230957 = 0.38895249366760254 + 2.0 * 6.326690196990967
Epoch 600, val loss: 0.844476044178009
Epoch 610, training loss: 13.032743453979492 = 0.3717477023601532 + 2.0 * 6.330497741699219
Epoch 610, val loss: 0.838218629360199
Epoch 620, training loss: 13.004007339477539 = 0.3551861643791199 + 2.0 * 6.324410438537598
Epoch 620, val loss: 0.8323925733566284
Epoch 630, training loss: 12.979349136352539 = 0.3390498459339142 + 2.0 * 6.3201494216918945
Epoch 630, val loss: 0.8269509673118591
Epoch 640, training loss: 12.960840225219727 = 0.32339009642601013 + 2.0 * 6.318725109100342
Epoch 640, val loss: 0.8218883275985718
Epoch 650, training loss: 12.943598747253418 = 0.3082197606563568 + 2.0 * 6.317689418792725
Epoch 650, val loss: 0.817208468914032
Epoch 660, training loss: 12.931992530822754 = 0.2935529947280884 + 2.0 * 6.319219589233398
Epoch 660, val loss: 0.8129433393478394
Epoch 670, training loss: 12.91145133972168 = 0.2795335054397583 + 2.0 * 6.3159589767456055
Epoch 670, val loss: 0.8091567158699036
Epoch 680, training loss: 12.893085479736328 = 0.266040176153183 + 2.0 * 6.313522815704346
Epoch 680, val loss: 0.8058316111564636
Epoch 690, training loss: 12.876565933227539 = 0.25308698415756226 + 2.0 * 6.311739444732666
Epoch 690, val loss: 0.8029979467391968
Epoch 700, training loss: 12.861641883850098 = 0.24061799049377441 + 2.0 * 6.310512065887451
Epoch 700, val loss: 0.8005768060684204
Epoch 710, training loss: 12.865397453308105 = 0.22867023944854736 + 2.0 * 6.318363666534424
Epoch 710, val loss: 0.798602283000946
Epoch 720, training loss: 12.838284492492676 = 0.21727754175662994 + 2.0 * 6.3105034828186035
Epoch 720, val loss: 0.7970072031021118
Epoch 730, training loss: 12.823933601379395 = 0.2064255028963089 + 2.0 * 6.308753967285156
Epoch 730, val loss: 0.7958672046661377
Epoch 740, training loss: 12.809925079345703 = 0.1961365044116974 + 2.0 * 6.306894302368164
Epoch 740, val loss: 0.7951877117156982
Epoch 750, training loss: 12.799361228942871 = 0.18635553121566772 + 2.0 * 6.306502819061279
Epoch 750, val loss: 0.7949382662773132
Epoch 760, training loss: 12.78620433807373 = 0.17704296112060547 + 2.0 * 6.3045806884765625
Epoch 760, val loss: 0.7951000332832336
Epoch 770, training loss: 12.783903121948242 = 0.1682208925485611 + 2.0 * 6.3078413009643555
Epoch 770, val loss: 0.7956402897834778
Epoch 780, training loss: 12.765493392944336 = 0.159866064786911 + 2.0 * 6.302813529968262
Epoch 780, val loss: 0.7965185046195984
Epoch 790, training loss: 12.770196914672852 = 0.15201285481452942 + 2.0 * 6.309092044830322
Epoch 790, val loss: 0.7978377342224121
Epoch 800, training loss: 12.744958877563477 = 0.14456960558891296 + 2.0 * 6.30019474029541
Epoch 800, val loss: 0.7994117736816406
Epoch 810, training loss: 12.735855102539062 = 0.13759469985961914 + 2.0 * 6.299129962921143
Epoch 810, val loss: 0.8013418912887573
Epoch 820, training loss: 12.72584056854248 = 0.13101811707019806 + 2.0 * 6.2974114418029785
Epoch 820, val loss: 0.8035994172096252
Epoch 830, training loss: 12.729053497314453 = 0.12481948733329773 + 2.0 * 6.302116870880127
Epoch 830, val loss: 0.8061002492904663
Epoch 840, training loss: 12.71414852142334 = 0.11896257102489471 + 2.0 * 6.297593116760254
Epoch 840, val loss: 0.8087970018386841
Epoch 850, training loss: 12.70429801940918 = 0.11347083002328873 + 2.0 * 6.295413494110107
Epoch 850, val loss: 0.8116939067840576
Epoch 860, training loss: 12.70815372467041 = 0.10829892754554749 + 2.0 * 6.299927234649658
Epoch 860, val loss: 0.8147544860839844
Epoch 870, training loss: 12.693795204162598 = 0.10343015938997269 + 2.0 * 6.295182704925537
Epoch 870, val loss: 0.8180075883865356
Epoch 880, training loss: 12.68320083618164 = 0.09883605688810349 + 2.0 * 6.292182445526123
Epoch 880, val loss: 0.8212895393371582
Epoch 890, training loss: 12.676337242126465 = 0.09449966251850128 + 2.0 * 6.290918827056885
Epoch 890, val loss: 0.824760377407074
Epoch 900, training loss: 12.683745384216309 = 0.09040667116641998 + 2.0 * 6.2966694831848145
Epoch 900, val loss: 0.8282729387283325
Epoch 910, training loss: 12.670429229736328 = 0.08656909316778183 + 2.0 * 6.291930198669434
Epoch 910, val loss: 0.8319181203842163
Epoch 920, training loss: 12.669742584228516 = 0.08292842656373978 + 2.0 * 6.293406963348389
Epoch 920, val loss: 0.8355603218078613
Epoch 930, training loss: 12.655179977416992 = 0.07952475547790527 + 2.0 * 6.287827491760254
Epoch 930, val loss: 0.8393040299415588
Epoch 940, training loss: 12.649767875671387 = 0.07629363238811493 + 2.0 * 6.286736965179443
Epoch 940, val loss: 0.8431238532066345
Epoch 950, training loss: 12.64533519744873 = 0.07323223352432251 + 2.0 * 6.286051273345947
Epoch 950, val loss: 0.8469786643981934
Epoch 960, training loss: 12.651628494262695 = 0.07032831758260727 + 2.0 * 6.290649890899658
Epoch 960, val loss: 0.8508853912353516
Epoch 970, training loss: 12.640946388244629 = 0.06755572557449341 + 2.0 * 6.28669548034668
Epoch 970, val loss: 0.8547095656394958
Epoch 980, training loss: 12.634607315063477 = 0.06494688987731934 + 2.0 * 6.284830093383789
Epoch 980, val loss: 0.8586376905441284
Epoch 990, training loss: 12.628288269042969 = 0.06247006729245186 + 2.0 * 6.282908916473389
Epoch 990, val loss: 0.8625345230102539
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.788888888888889
0.8434370057986295
=== training gcn model ===
Epoch 0, training loss: 19.152467727661133 = 1.9587763547897339 + 2.0 * 8.596845626831055
Epoch 0, val loss: 1.958805799484253
Epoch 10, training loss: 19.141796112060547 = 1.9485831260681152 + 2.0 * 8.596606254577637
Epoch 10, val loss: 1.9486947059631348
Epoch 20, training loss: 19.125497817993164 = 1.9357894659042358 + 2.0 * 8.594854354858398
Epoch 20, val loss: 1.9354171752929688
Epoch 30, training loss: 19.07914924621582 = 1.9178603887557983 + 2.0 * 8.580644607543945
Epoch 30, val loss: 1.9164307117462158
Epoch 40, training loss: 18.85727882385254 = 1.894605040550232 + 2.0 * 8.48133659362793
Epoch 40, val loss: 1.892280101776123
Epoch 50, training loss: 17.779024124145508 = 1.8714689016342163 + 2.0 * 7.95377779006958
Epoch 50, val loss: 1.868686556816101
Epoch 60, training loss: 16.736181259155273 = 1.8532865047454834 + 2.0 * 7.441447734832764
Epoch 60, val loss: 1.851213812828064
Epoch 70, training loss: 15.990707397460938 = 1.8392921686172485 + 2.0 * 7.07570743560791
Epoch 70, val loss: 1.8371881246566772
Epoch 80, training loss: 15.647400856018066 = 1.8252112865447998 + 2.0 * 6.911094665527344
Epoch 80, val loss: 1.8230936527252197
Epoch 90, training loss: 15.432823181152344 = 1.8089419603347778 + 2.0 * 6.811940670013428
Epoch 90, val loss: 1.8069325685501099
Epoch 100, training loss: 15.260961532592773 = 1.7921406030654907 + 2.0 * 6.734410285949707
Epoch 100, val loss: 1.7909127473831177
Epoch 110, training loss: 15.138672828674316 = 1.7764499187469482 + 2.0 * 6.6811113357543945
Epoch 110, val loss: 1.7759662866592407
Epoch 120, training loss: 15.038955688476562 = 1.7612745761871338 + 2.0 * 6.638840675354004
Epoch 120, val loss: 1.7615442276000977
Epoch 130, training loss: 14.954044342041016 = 1.7456867694854736 + 2.0 * 6.6041789054870605
Epoch 130, val loss: 1.7468703985214233
Epoch 140, training loss: 14.878116607666016 = 1.7292311191558838 + 2.0 * 6.5744428634643555
Epoch 140, val loss: 1.7316561937332153
Epoch 150, training loss: 14.810110092163086 = 1.711379051208496 + 2.0 * 6.549365520477295
Epoch 150, val loss: 1.7154943943023682
Epoch 160, training loss: 14.751523971557617 = 1.691519856452942 + 2.0 * 6.530002117156982
Epoch 160, val loss: 1.6977659463882446
Epoch 170, training loss: 14.693573951721191 = 1.6695958375930786 + 2.0 * 6.511989116668701
Epoch 170, val loss: 1.6783841848373413
Epoch 180, training loss: 14.639169692993164 = 1.6451245546340942 + 2.0 * 6.49702262878418
Epoch 180, val loss: 1.6569706201553345
Epoch 190, training loss: 14.585162162780762 = 1.6179111003875732 + 2.0 * 6.483625411987305
Epoch 190, val loss: 1.6332008838653564
Epoch 200, training loss: 14.531307220458984 = 1.5876152515411377 + 2.0 * 6.471846103668213
Epoch 200, val loss: 1.6069390773773193
Epoch 210, training loss: 14.478960990905762 = 1.5540072917938232 + 2.0 * 6.46247673034668
Epoch 210, val loss: 1.578033685684204
Epoch 220, training loss: 14.423542022705078 = 1.5178289413452148 + 2.0 * 6.452856540679932
Epoch 220, val loss: 1.5472265481948853
Epoch 230, training loss: 14.36789608001709 = 1.4794120788574219 + 2.0 * 6.444242000579834
Epoch 230, val loss: 1.5148392915725708
Epoch 240, training loss: 14.31303596496582 = 1.4389915466308594 + 2.0 * 6.4370222091674805
Epoch 240, val loss: 1.4813915491104126
Epoch 250, training loss: 14.256989479064941 = 1.3969776630401611 + 2.0 * 6.43000602722168
Epoch 250, val loss: 1.447255253791809
Epoch 260, training loss: 14.201669692993164 = 1.354230284690857 + 2.0 * 6.423719882965088
Epoch 260, val loss: 1.4133529663085938
Epoch 270, training loss: 14.14626407623291 = 1.3110824823379517 + 2.0 * 6.417590618133545
Epoch 270, val loss: 1.3799406290054321
Epoch 280, training loss: 14.093304634094238 = 1.268168568611145 + 2.0 * 6.412568092346191
Epoch 280, val loss: 1.347575306892395
Epoch 290, training loss: 14.045244216918945 = 1.2261549234390259 + 2.0 * 6.409544467926025
Epoch 290, val loss: 1.3165788650512695
Epoch 300, training loss: 13.989882469177246 = 1.1850396394729614 + 2.0 * 6.402421474456787
Epoch 300, val loss: 1.2869441509246826
Epoch 310, training loss: 13.93974494934082 = 1.1449054479599 + 2.0 * 6.3974199295043945
Epoch 310, val loss: 1.2582100629806519
Epoch 320, training loss: 13.891119003295898 = 1.1056164503097534 + 2.0 * 6.392751216888428
Epoch 320, val loss: 1.230326771736145
Epoch 330, training loss: 13.845185279846191 = 1.067160725593567 + 2.0 * 6.389012336730957
Epoch 330, val loss: 1.2032201290130615
Epoch 340, training loss: 13.800066947937012 = 1.029860258102417 + 2.0 * 6.385103225708008
Epoch 340, val loss: 1.176943063735962
Epoch 350, training loss: 13.7548189163208 = 0.9939665198326111 + 2.0 * 6.380426406860352
Epoch 350, val loss: 1.1516785621643066
Epoch 360, training loss: 13.711649894714355 = 0.958958625793457 + 2.0 * 6.376345634460449
Epoch 360, val loss: 1.1268943548202515
Epoch 370, training loss: 13.685885429382324 = 0.9252315163612366 + 2.0 * 6.380326747894287
Epoch 370, val loss: 1.1030789613723755
Epoch 380, training loss: 13.636255264282227 = 0.8928511738777161 + 2.0 * 6.371702194213867
Epoch 380, val loss: 1.0801054239273071
Epoch 390, training loss: 13.594231605529785 = 0.8616043329238892 + 2.0 * 6.366313457489014
Epoch 390, val loss: 1.0579891204833984
Epoch 400, training loss: 13.57231330871582 = 0.831366240978241 + 2.0 * 6.370473384857178
Epoch 400, val loss: 1.0366698503494263
Epoch 410, training loss: 13.522554397583008 = 0.8025208115577698 + 2.0 * 6.360016822814941
Epoch 410, val loss: 1.0160170793533325
Epoch 420, training loss: 13.488931655883789 = 0.7747178673744202 + 2.0 * 6.357106685638428
Epoch 420, val loss: 0.9963656067848206
Epoch 430, training loss: 13.456332206726074 = 0.7479214072227478 + 2.0 * 6.35420560836792
Epoch 430, val loss: 0.9775451421737671
Epoch 440, training loss: 13.436600685119629 = 0.7220665812492371 + 2.0 * 6.357266902923584
Epoch 440, val loss: 0.9595792293548584
Epoch 450, training loss: 13.39589786529541 = 0.6973884701728821 + 2.0 * 6.349254608154297
Epoch 450, val loss: 0.9424977898597717
Epoch 460, training loss: 13.367347717285156 = 0.6737147569656372 + 2.0 * 6.346816539764404
Epoch 460, val loss: 0.9263113141059875
Epoch 470, training loss: 13.33957576751709 = 0.6509150266647339 + 2.0 * 6.344330310821533
Epoch 470, val loss: 0.9110465049743652
Epoch 480, training loss: 13.31404972076416 = 0.6288407444953918 + 2.0 * 6.342604637145996
Epoch 480, val loss: 0.8965132832527161
Epoch 490, training loss: 13.295356750488281 = 0.6075106859207153 + 2.0 * 6.343923091888428
Epoch 490, val loss: 0.882716953754425
Epoch 500, training loss: 13.265403747558594 = 0.5869669914245605 + 2.0 * 6.3392181396484375
Epoch 500, val loss: 0.8696708083152771
Epoch 510, training loss: 13.241161346435547 = 0.5671249628067017 + 2.0 * 6.337018013000488
Epoch 510, val loss: 0.8574998378753662
Epoch 520, training loss: 13.22089672088623 = 0.5478807687759399 + 2.0 * 6.336507797241211
Epoch 520, val loss: 0.8459632396697998
Epoch 530, training loss: 13.193559646606445 = 0.5292599201202393 + 2.0 * 6.332149982452393
Epoch 530, val loss: 0.8349822163581848
Epoch 540, training loss: 13.1724853515625 = 0.5111807584762573 + 2.0 * 6.330652236938477
Epoch 540, val loss: 0.8246761560440063
Epoch 550, training loss: 13.15320110321045 = 0.4935193955898285 + 2.0 * 6.329840660095215
Epoch 550, val loss: 0.8149023056030273
Epoch 560, training loss: 13.138155937194824 = 0.4763416051864624 + 2.0 * 6.330907344818115
Epoch 560, val loss: 0.8055135011672974
Epoch 570, training loss: 13.11677074432373 = 0.4595576226711273 + 2.0 * 6.328606605529785
Epoch 570, val loss: 0.7966170310974121
Epoch 580, training loss: 13.090848922729492 = 0.44316479563713074 + 2.0 * 6.3238420486450195
Epoch 580, val loss: 0.7881152033805847
Epoch 590, training loss: 13.073397636413574 = 0.42699912190437317 + 2.0 * 6.323199272155762
Epoch 590, val loss: 0.7800232768058777
Epoch 600, training loss: 13.055533409118652 = 0.41105204820632935 + 2.0 * 6.322240829467773
Epoch 600, val loss: 0.7721508741378784
Epoch 610, training loss: 13.038830757141113 = 0.39535465836524963 + 2.0 * 6.321738243103027
Epoch 610, val loss: 0.7645025849342346
Epoch 620, training loss: 13.01740837097168 = 0.37983155250549316 + 2.0 * 6.318788528442383
Epoch 620, val loss: 0.7571706175804138
Epoch 630, training loss: 12.998199462890625 = 0.36439841985702515 + 2.0 * 6.316900730133057
Epoch 630, val loss: 0.7500221729278564
Epoch 640, training loss: 12.984785079956055 = 0.34905603528022766 + 2.0 * 6.317864418029785
Epoch 640, val loss: 0.7430440187454224
Epoch 650, training loss: 12.970929145812988 = 0.3338269293308258 + 2.0 * 6.318551063537598
Epoch 650, val loss: 0.7360398769378662
Epoch 660, training loss: 12.951067924499512 = 0.31877222657203674 + 2.0 * 6.316147804260254
Epoch 660, val loss: 0.7292580008506775
Epoch 670, training loss: 12.932119369506836 = 0.30386215448379517 + 2.0 * 6.314128398895264
Epoch 670, val loss: 0.7226252555847168
Epoch 680, training loss: 12.909759521484375 = 0.28917431831359863 + 2.0 * 6.310292720794678
Epoch 680, val loss: 0.7161208391189575
Epoch 690, training loss: 12.894081115722656 = 0.27469781041145325 + 2.0 * 6.309691429138184
Epoch 690, val loss: 0.7100251913070679
Epoch 700, training loss: 12.883572578430176 = 0.2605282962322235 + 2.0 * 6.311522006988525
Epoch 700, val loss: 0.7042797803878784
Epoch 710, training loss: 12.862865447998047 = 0.24667978286743164 + 2.0 * 6.3080925941467285
Epoch 710, val loss: 0.6988915205001831
Epoch 720, training loss: 12.852052688598633 = 0.23332524299621582 + 2.0 * 6.309363842010498
Epoch 720, val loss: 0.6939013004302979
Epoch 730, training loss: 12.833717346191406 = 0.22044382989406586 + 2.0 * 6.306636810302734
Epoch 730, val loss: 0.6893444061279297
Epoch 740, training loss: 12.81737232208252 = 0.2081238478422165 + 2.0 * 6.304624080657959
Epoch 740, val loss: 0.6853028535842896
Epoch 750, training loss: 12.810381889343262 = 0.19640731811523438 + 2.0 * 6.306987285614014
Epoch 750, val loss: 0.6819706559181213
Epoch 760, training loss: 12.788457870483398 = 0.1852622628211975 + 2.0 * 6.301597595214844
Epoch 760, val loss: 0.6792680621147156
Epoch 770, training loss: 12.775080680847168 = 0.17473559081554413 + 2.0 * 6.300172328948975
Epoch 770, val loss: 0.6772081255912781
Epoch 780, training loss: 12.775348663330078 = 0.16483356058597565 + 2.0 * 6.305257320404053
Epoch 780, val loss: 0.6757574677467346
Epoch 790, training loss: 12.756743431091309 = 0.15550346672534943 + 2.0 * 6.300620079040527
Epoch 790, val loss: 0.6749128103256226
Epoch 800, training loss: 12.740574836730957 = 0.14683331549167633 + 2.0 * 6.296870708465576
Epoch 800, val loss: 0.6746077537536621
Epoch 810, training loss: 12.730863571166992 = 0.1387231945991516 + 2.0 * 6.296070098876953
Epoch 810, val loss: 0.6749190092086792
Epoch 820, training loss: 12.73569393157959 = 0.131172314286232 + 2.0 * 6.302260875701904
Epoch 820, val loss: 0.6757377982139587
Epoch 830, training loss: 12.712491989135742 = 0.1241222694516182 + 2.0 * 6.294184684753418
Epoch 830, val loss: 0.6770012378692627
Epoch 840, training loss: 12.704808235168457 = 0.11756116896867752 + 2.0 * 6.293623447418213
Epoch 840, val loss: 0.6787418723106384
Epoch 850, training loss: 12.697997093200684 = 0.11146426945924759 + 2.0 * 6.293266296386719
Epoch 850, val loss: 0.6809002757072449
Epoch 860, training loss: 12.689053535461426 = 0.10577215999364853 + 2.0 * 6.291640758514404
Epoch 860, val loss: 0.6833267211914062
Epoch 870, training loss: 12.687772750854492 = 0.10047931969165802 + 2.0 * 6.293646812438965
Epoch 870, val loss: 0.686052143573761
Epoch 880, training loss: 12.673439979553223 = 0.09555076062679291 + 2.0 * 6.288944721221924
Epoch 880, val loss: 0.6890673041343689
Epoch 890, training loss: 12.66810417175293 = 0.09096331149339676 + 2.0 * 6.288570404052734
Epoch 890, val loss: 0.6923421025276184
Epoch 900, training loss: 12.664958000183105 = 0.08668328076601028 + 2.0 * 6.289137363433838
Epoch 900, val loss: 0.6957364678382874
Epoch 910, training loss: 12.655245780944824 = 0.08265361934900284 + 2.0 * 6.2862958908081055
Epoch 910, val loss: 0.6993736028671265
Epoch 920, training loss: 12.651444435119629 = 0.07888767868280411 + 2.0 * 6.286278247833252
Epoch 920, val loss: 0.7031434178352356
Epoch 930, training loss: 12.649248123168945 = 0.07535840570926666 + 2.0 * 6.28694486618042
Epoch 930, val loss: 0.7070211172103882
Epoch 940, training loss: 12.646218299865723 = 0.0720730572938919 + 2.0 * 6.287072658538818
Epoch 940, val loss: 0.7109521627426147
Epoch 950, training loss: 12.637328147888184 = 0.06897880882024765 + 2.0 * 6.28417444229126
Epoch 950, val loss: 0.7151193618774414
Epoch 960, training loss: 12.63052749633789 = 0.06609035283327103 + 2.0 * 6.2822184562683105
Epoch 960, val loss: 0.7191568613052368
Epoch 970, training loss: 12.628661155700684 = 0.06337673217058182 + 2.0 * 6.282642364501953
Epoch 970, val loss: 0.7234207391738892
Epoch 980, training loss: 12.621665000915527 = 0.060831427574157715 + 2.0 * 6.280416965484619
Epoch 980, val loss: 0.7276769280433655
Epoch 990, training loss: 12.618551254272461 = 0.0584242045879364 + 2.0 * 6.280063629150391
Epoch 990, val loss: 0.7318031191825867
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.788888888888889
0.8365840801265156
=== training gcn model ===
Epoch 0, training loss: 19.16385841369629 = 1.9702712297439575 + 2.0 * 8.596793174743652
Epoch 0, val loss: 1.9687843322753906
Epoch 10, training loss: 19.15247344970703 = 1.9598174095153809 + 2.0 * 8.596327781677246
Epoch 10, val loss: 1.9586771726608276
Epoch 20, training loss: 19.13286781311035 = 1.9468111991882324 + 2.0 * 8.59302806854248
Epoch 20, val loss: 1.945489764213562
Epoch 30, training loss: 19.067367553710938 = 1.9289133548736572 + 2.0 * 8.56922721862793
Epoch 30, val loss: 1.9268710613250732
Epoch 40, training loss: 18.71437644958496 = 1.9070663452148438 + 2.0 * 8.403655052185059
Epoch 40, val loss: 1.9052928686141968
Epoch 50, training loss: 17.324031829833984 = 1.8847756385803223 + 2.0 * 7.71962833404541
Epoch 50, val loss: 1.8834588527679443
Epoch 60, training loss: 16.478830337524414 = 1.8675589561462402 + 2.0 * 7.305635929107666
Epoch 60, val loss: 1.8679711818695068
Epoch 70, training loss: 16.02051544189453 = 1.853005290031433 + 2.0 * 7.083755016326904
Epoch 70, val loss: 1.8538849353790283
Epoch 80, training loss: 15.714070320129395 = 1.8385697603225708 + 2.0 * 6.937750339508057
Epoch 80, val loss: 1.8397412300109863
Epoch 90, training loss: 15.447540283203125 = 1.824028491973877 + 2.0 * 6.811756134033203
Epoch 90, val loss: 1.8258461952209473
Epoch 100, training loss: 15.286518096923828 = 1.8093758821487427 + 2.0 * 6.7385711669921875
Epoch 100, val loss: 1.8121187686920166
Epoch 110, training loss: 15.162887573242188 = 1.7939373254776 + 2.0 * 6.684474945068359
Epoch 110, val loss: 1.7978019714355469
Epoch 120, training loss: 15.056815147399902 = 1.7787425518035889 + 2.0 * 6.639036178588867
Epoch 120, val loss: 1.7836928367614746
Epoch 130, training loss: 14.967988967895508 = 1.7640100717544556 + 2.0 * 6.601989269256592
Epoch 130, val loss: 1.7698291540145874
Epoch 140, training loss: 14.89898681640625 = 1.748581886291504 + 2.0 * 6.575202465057373
Epoch 140, val loss: 1.7554515600204468
Epoch 150, training loss: 14.827677726745605 = 1.7319157123565674 + 2.0 * 6.547881126403809
Epoch 150, val loss: 1.7402105331420898
Epoch 160, training loss: 14.768647193908691 = 1.7136390209197998 + 2.0 * 6.527503967285156
Epoch 160, val loss: 1.7236213684082031
Epoch 170, training loss: 14.71312141418457 = 1.693446159362793 + 2.0 * 6.509837627410889
Epoch 170, val loss: 1.7055054903030396
Epoch 180, training loss: 14.663768768310547 = 1.6710214614868164 + 2.0 * 6.496373653411865
Epoch 180, val loss: 1.6854498386383057
Epoch 190, training loss: 14.608858108520508 = 1.6459012031555176 + 2.0 * 6.481478214263916
Epoch 190, val loss: 1.663354516029358
Epoch 200, training loss: 14.557965278625488 = 1.6178605556488037 + 2.0 * 6.470052242279053
Epoch 200, val loss: 1.6387717723846436
Epoch 210, training loss: 14.502710342407227 = 1.5867139101028442 + 2.0 * 6.457998275756836
Epoch 210, val loss: 1.6115217208862305
Epoch 220, training loss: 14.452699661254883 = 1.5520809888839722 + 2.0 * 6.4503092765808105
Epoch 220, val loss: 1.5813671350479126
Epoch 230, training loss: 14.392816543579102 = 1.514126181602478 + 2.0 * 6.439345359802246
Epoch 230, val loss: 1.5485647916793823
Epoch 240, training loss: 14.334846496582031 = 1.4729888439178467 + 2.0 * 6.430928707122803
Epoch 240, val loss: 1.5133074522018433
Epoch 250, training loss: 14.274662971496582 = 1.4286671876907349 + 2.0 * 6.422997951507568
Epoch 250, val loss: 1.4756240844726562
Epoch 260, training loss: 14.23025894165039 = 1.3816618919372559 + 2.0 * 6.4242987632751465
Epoch 260, val loss: 1.436008095741272
Epoch 270, training loss: 14.154711723327637 = 1.333374261856079 + 2.0 * 6.410668849945068
Epoch 270, val loss: 1.395660400390625
Epoch 280, training loss: 14.092487335205078 = 1.2843265533447266 + 2.0 * 6.404080390930176
Epoch 280, val loss: 1.3551443815231323
Epoch 290, training loss: 14.032285690307617 = 1.2347006797790527 + 2.0 * 6.398792266845703
Epoch 290, val loss: 1.314532995223999
Epoch 300, training loss: 13.97171401977539 = 1.1851603984832764 + 2.0 * 6.393276691436768
Epoch 300, val loss: 1.274436354637146
Epoch 310, training loss: 13.915227890014648 = 1.1364961862564087 + 2.0 * 6.3893656730651855
Epoch 310, val loss: 1.2355738878250122
Epoch 320, training loss: 13.860002517700195 = 1.089798927307129 + 2.0 * 6.385101795196533
Epoch 320, val loss: 1.1985588073730469
Epoch 330, training loss: 13.806878089904785 = 1.0450141429901123 + 2.0 * 6.380931854248047
Epoch 330, val loss: 1.1634140014648438
Epoch 340, training loss: 13.763911247253418 = 1.0021724700927734 + 2.0 * 6.380869388580322
Epoch 340, val loss: 1.1301666498184204
Epoch 350, training loss: 13.707083702087402 = 0.9615547060966492 + 2.0 * 6.372764587402344
Epoch 350, val loss: 1.0988043546676636
Epoch 360, training loss: 13.660892486572266 = 0.9230412840843201 + 2.0 * 6.36892557144165
Epoch 360, val loss: 1.0693769454956055
Epoch 370, training loss: 13.617098808288574 = 0.8863188028335571 + 2.0 * 6.365389823913574
Epoch 370, val loss: 1.0414379835128784
Epoch 380, training loss: 13.579484939575195 = 0.8514812588691711 + 2.0 * 6.364001750946045
Epoch 380, val loss: 1.0151054859161377
Epoch 390, training loss: 13.536219596862793 = 0.8187450170516968 + 2.0 * 6.358737468719482
Epoch 390, val loss: 0.9907252788543701
Epoch 400, training loss: 13.499154090881348 = 0.7878244519233704 + 2.0 * 6.3556647300720215
Epoch 400, val loss: 0.9679830074310303
Epoch 410, training loss: 13.465188980102539 = 0.7584829926490784 + 2.0 * 6.353353023529053
Epoch 410, val loss: 0.9467470645904541
Epoch 420, training loss: 13.43515396118164 = 0.7306693196296692 + 2.0 * 6.352242469787598
Epoch 420, val loss: 0.9269739389419556
Epoch 430, training loss: 13.4033203125 = 0.7044680118560791 + 2.0 * 6.34942626953125
Epoch 430, val loss: 0.9085342288017273
Epoch 440, training loss: 13.369778633117676 = 0.6795350313186646 + 2.0 * 6.34512186050415
Epoch 440, val loss: 0.8916119337081909
Epoch 450, training loss: 13.350573539733887 = 0.6555870771408081 + 2.0 * 6.3474931716918945
Epoch 450, val loss: 0.8758590221405029
Epoch 460, training loss: 13.314387321472168 = 0.6326611638069153 + 2.0 * 6.340863227844238
Epoch 460, val loss: 0.8610787987709045
Epoch 470, training loss: 13.286336898803711 = 0.6105196475982666 + 2.0 * 6.337908744812012
Epoch 470, val loss: 0.8473496437072754
Epoch 480, training loss: 13.266672134399414 = 0.5889843702316284 + 2.0 * 6.338843822479248
Epoch 480, val loss: 0.8344904184341431
Epoch 490, training loss: 13.24485969543457 = 0.5680248141288757 + 2.0 * 6.3384175300598145
Epoch 490, val loss: 0.822395920753479
Epoch 500, training loss: 13.210756301879883 = 0.5477268695831299 + 2.0 * 6.331514835357666
Epoch 500, val loss: 0.8111807107925415
Epoch 510, training loss: 13.186853408813477 = 0.5278804898262024 + 2.0 * 6.32948637008667
Epoch 510, val loss: 0.8006991744041443
Epoch 520, training loss: 13.172179222106934 = 0.5083250403404236 + 2.0 * 6.331927299499512
Epoch 520, val loss: 0.7909653782844543
Epoch 530, training loss: 13.141468048095703 = 0.4891469180583954 + 2.0 * 6.326160430908203
Epoch 530, val loss: 0.7817731499671936
Epoch 540, training loss: 13.120651245117188 = 0.47031861543655396 + 2.0 * 6.32516622543335
Epoch 540, val loss: 0.7733510732650757
Epoch 550, training loss: 13.09659481048584 = 0.45177510380744934 + 2.0 * 6.322409629821777
Epoch 550, val loss: 0.7656331658363342
Epoch 560, training loss: 13.080307960510254 = 0.43366268277168274 + 2.0 * 6.323322772979736
Epoch 560, val loss: 0.7585177421569824
Epoch 570, training loss: 13.057398796081543 = 0.41581669449806213 + 2.0 * 6.320791244506836
Epoch 570, val loss: 0.7520932555198669
Epoch 580, training loss: 13.0309419631958 = 0.3983520269393921 + 2.0 * 6.316295146942139
Epoch 580, val loss: 0.7464048862457275
Epoch 590, training loss: 13.013980865478516 = 0.3811936378479004 + 2.0 * 6.3163933753967285
Epoch 590, val loss: 0.7413994073867798
Epoch 600, training loss: 12.993209838867188 = 0.36443498730659485 + 2.0 * 6.314387321472168
Epoch 600, val loss: 0.7369874715805054
Epoch 610, training loss: 12.976203918457031 = 0.3481431305408478 + 2.0 * 6.314030170440674
Epoch 610, val loss: 0.7333783507347107
Epoch 620, training loss: 12.955097198486328 = 0.33233675360679626 + 2.0 * 6.311380386352539
Epoch 620, val loss: 0.7304340600967407
Epoch 630, training loss: 12.93504524230957 = 0.3170353174209595 + 2.0 * 6.309004783630371
Epoch 630, val loss: 0.7281408905982971
Epoch 640, training loss: 12.920331954956055 = 0.30221232771873474 + 2.0 * 6.3090596199035645
Epoch 640, val loss: 0.7264538407325745
Epoch 650, training loss: 12.90674877166748 = 0.28797447681427 + 2.0 * 6.30938720703125
Epoch 650, val loss: 0.7251760363578796
Epoch 660, training loss: 12.885467529296875 = 0.27427467703819275 + 2.0 * 6.305596351623535
Epoch 660, val loss: 0.7245388627052307
Epoch 670, training loss: 12.869997024536133 = 0.26113593578338623 + 2.0 * 6.3044304847717285
Epoch 670, val loss: 0.7243074774742126
Epoch 680, training loss: 12.858607292175293 = 0.24851417541503906 + 2.0 * 6.305046558380127
Epoch 680, val loss: 0.7244516611099243
Epoch 690, training loss: 12.841256141662598 = 0.23642335832118988 + 2.0 * 6.3024163246154785
Epoch 690, val loss: 0.724956750869751
Epoch 700, training loss: 12.829652786254883 = 0.22486969828605652 + 2.0 * 6.302391529083252
Epoch 700, val loss: 0.7258481383323669
Epoch 710, training loss: 12.817794799804688 = 0.21383051574230194 + 2.0 * 6.3019819259643555
Epoch 710, val loss: 0.7270601987838745
Epoch 720, training loss: 12.799739837646484 = 0.20326752960681915 + 2.0 * 6.29823637008667
Epoch 720, val loss: 0.7285311818122864
Epoch 730, training loss: 12.786660194396973 = 0.19320739805698395 + 2.0 * 6.296726226806641
Epoch 730, val loss: 0.7303076386451721
Epoch 740, training loss: 12.77432632446289 = 0.18361496925354004 + 2.0 * 6.295355796813965
Epoch 740, val loss: 0.7323336601257324
Epoch 750, training loss: 12.78042984008789 = 0.17446471750736237 + 2.0 * 6.302982330322266
Epoch 750, val loss: 0.7346512675285339
Epoch 760, training loss: 12.756867408752441 = 0.1658332496881485 + 2.0 * 6.2955169677734375
Epoch 760, val loss: 0.7371286153793335
Epoch 770, training loss: 12.744571685791016 = 0.15762461721897125 + 2.0 * 6.293473720550537
Epoch 770, val loss: 0.7398593425750732
Epoch 780, training loss: 12.734841346740723 = 0.14983274042606354 + 2.0 * 6.29250431060791
Epoch 780, val loss: 0.7428644299507141
Epoch 790, training loss: 12.729978561401367 = 0.14243732392787933 + 2.0 * 6.293770790100098
Epoch 790, val loss: 0.7461057305335999
Epoch 800, training loss: 12.716382026672363 = 0.13545013964176178 + 2.0 * 6.290465831756592
Epoch 800, val loss: 0.7494916915893555
Epoch 810, training loss: 12.707014083862305 = 0.1288386583328247 + 2.0 * 6.289087772369385
Epoch 810, val loss: 0.7530656456947327
Epoch 820, training loss: 12.69729995727539 = 0.12258802354335785 + 2.0 * 6.287355899810791
Epoch 820, val loss: 0.7568495869636536
Epoch 830, training loss: 12.704118728637695 = 0.11664353311061859 + 2.0 * 6.293737411499023
Epoch 830, val loss: 0.7608765959739685
Epoch 840, training loss: 12.686866760253906 = 0.11108558624982834 + 2.0 * 6.287890434265137
Epoch 840, val loss: 0.7650407552719116
Epoch 850, training loss: 12.685721397399902 = 0.10580845177173615 + 2.0 * 6.289956569671631
Epoch 850, val loss: 0.7693578600883484
Epoch 860, training loss: 12.670415878295898 = 0.10084566473960876 + 2.0 * 6.284785270690918
Epoch 860, val loss: 0.7737797498703003
Epoch 870, training loss: 12.661883354187012 = 0.0961751937866211 + 2.0 * 6.282854080200195
Epoch 870, val loss: 0.7783403396606445
Epoch 880, training loss: 12.654769897460938 = 0.09173997491598129 + 2.0 * 6.281515121459961
Epoch 880, val loss: 0.7829920649528503
Epoch 890, training loss: 12.667078971862793 = 0.0875510722398758 + 2.0 * 6.289763927459717
Epoch 890, val loss: 0.7877799272537231
Epoch 900, training loss: 12.651893615722656 = 0.08363217860460281 + 2.0 * 6.284130573272705
Epoch 900, val loss: 0.7925653457641602
Epoch 910, training loss: 12.641595840454102 = 0.07994000613689423 + 2.0 * 6.28082799911499
Epoch 910, val loss: 0.79738849401474
Epoch 920, training loss: 12.632072448730469 = 0.0764462798833847 + 2.0 * 6.277812957763672
Epoch 920, val loss: 0.8023513555526733
Epoch 930, training loss: 12.630593299865723 = 0.07314109057188034 + 2.0 * 6.278726100921631
Epoch 930, val loss: 0.80738765001297
Epoch 940, training loss: 12.630644798278809 = 0.07003211975097656 + 2.0 * 6.280306339263916
Epoch 940, val loss: 0.8124728798866272
Epoch 950, training loss: 12.624442100524902 = 0.06711181998252869 + 2.0 * 6.278665065765381
Epoch 950, val loss: 0.8174912929534912
Epoch 960, training loss: 12.614215850830078 = 0.06435374170541763 + 2.0 * 6.274930953979492
Epoch 960, val loss: 0.8225557208061218
Epoch 970, training loss: 12.610888481140137 = 0.061739470809698105 + 2.0 * 6.274574279785156
Epoch 970, val loss: 0.827697217464447
Epoch 980, training loss: 12.628576278686523 = 0.05925783887505531 + 2.0 * 6.284659385681152
Epoch 980, val loss: 0.8328374624252319
Epoch 990, training loss: 12.605957984924316 = 0.05692204460501671 + 2.0 * 6.274518013000488
Epoch 990, val loss: 0.8379189372062683
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8148148148148149
0.8371112282551397
The final CL Acc:0.79753, 0.01222, The final GNN Acc:0.83904, 0.00311
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.4 der2:0.4 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11568])
remove edge: torch.Size([2, 9498])
updated graph: torch.Size([2, 10510])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 19.151832580566406 = 1.9582889080047607 + 2.0 * 8.596772193908691
Epoch 0, val loss: 1.9640192985534668
Epoch 10, training loss: 19.139759063720703 = 1.947232961654663 + 2.0 * 8.59626293182373
Epoch 10, val loss: 1.9522556066513062
Epoch 20, training loss: 19.11882209777832 = 1.9333146810531616 + 2.0 * 8.592753410339355
Epoch 20, val loss: 1.9372788667678833
Epoch 30, training loss: 19.05708885192871 = 1.9138646125793457 + 2.0 * 8.571612358093262
Epoch 30, val loss: 1.9163676500320435
Epoch 40, training loss: 18.798770904541016 = 1.890730619430542 + 2.0 * 8.454020500183105
Epoch 40, val loss: 1.8929699659347534
Epoch 50, training loss: 17.764986038208008 = 1.86672842502594 + 2.0 * 7.9491286277771
Epoch 50, val loss: 1.8690322637557983
Epoch 60, training loss: 16.954191207885742 = 1.84773588180542 + 2.0 * 7.553227424621582
Epoch 60, val loss: 1.8511683940887451
Epoch 70, training loss: 16.312776565551758 = 1.8352195024490356 + 2.0 * 7.238778591156006
Epoch 70, val loss: 1.8393011093139648
Epoch 80, training loss: 15.91295337677002 = 1.8222092390060425 + 2.0 * 7.045372009277344
Epoch 80, val loss: 1.8268051147460938
Epoch 90, training loss: 15.714339256286621 = 1.8072930574417114 + 2.0 * 6.9535231590271
Epoch 90, val loss: 1.8130964040756226
Epoch 100, training loss: 15.560487747192383 = 1.7932164669036865 + 2.0 * 6.883635520935059
Epoch 100, val loss: 1.800527572631836
Epoch 110, training loss: 15.432225227355957 = 1.7813388109207153 + 2.0 * 6.825443267822266
Epoch 110, val loss: 1.790030598640442
Epoch 120, training loss: 15.307433128356934 = 1.7705278396606445 + 2.0 * 6.7684526443481445
Epoch 120, val loss: 1.7805496454238892
Epoch 130, training loss: 15.200515747070312 = 1.7597472667694092 + 2.0 * 6.720384120941162
Epoch 130, val loss: 1.7710024118423462
Epoch 140, training loss: 15.104921340942383 = 1.7481485605239868 + 2.0 * 6.678386211395264
Epoch 140, val loss: 1.760802984237671
Epoch 150, training loss: 15.018753051757812 = 1.7352961301803589 + 2.0 * 6.641728401184082
Epoch 150, val loss: 1.7498531341552734
Epoch 160, training loss: 14.944791793823242 = 1.7208983898162842 + 2.0 * 6.6119465827941895
Epoch 160, val loss: 1.7378686666488647
Epoch 170, training loss: 14.869361877441406 = 1.7049474716186523 + 2.0 * 6.582207202911377
Epoch 170, val loss: 1.7247064113616943
Epoch 180, training loss: 14.805152893066406 = 1.68692946434021 + 2.0 * 6.559111595153809
Epoch 180, val loss: 1.7096792459487915
Epoch 190, training loss: 14.747604370117188 = 1.6662951707839966 + 2.0 * 6.54065465927124
Epoch 190, val loss: 1.6925601959228516
Epoch 200, training loss: 14.69307804107666 = 1.6430593729019165 + 2.0 * 6.5250091552734375
Epoch 200, val loss: 1.6733976602554321
Epoch 210, training loss: 14.636091232299805 = 1.6171231269836426 + 2.0 * 6.50948429107666
Epoch 210, val loss: 1.6521133184432983
Epoch 220, training loss: 14.580055236816406 = 1.588283896446228 + 2.0 * 6.495885848999023
Epoch 220, val loss: 1.6285221576690674
Epoch 230, training loss: 14.523962020874023 = 1.5565309524536133 + 2.0 * 6.483715534210205
Epoch 230, val loss: 1.6025809049606323
Epoch 240, training loss: 14.4724760055542 = 1.522159218788147 + 2.0 * 6.475158214569092
Epoch 240, val loss: 1.5748026371002197
Epoch 250, training loss: 14.414910316467285 = 1.4855890274047852 + 2.0 * 6.46466064453125
Epoch 250, val loss: 1.5452289581298828
Epoch 260, training loss: 14.360799789428711 = 1.4472453594207764 + 2.0 * 6.456777095794678
Epoch 260, val loss: 1.5146163702011108
Epoch 270, training loss: 14.304888725280762 = 1.407740592956543 + 2.0 * 6.448574066162109
Epoch 270, val loss: 1.483290433883667
Epoch 280, training loss: 14.255758285522461 = 1.3674488067626953 + 2.0 * 6.444154739379883
Epoch 280, val loss: 1.4517172574996948
Epoch 290, training loss: 14.200216293334961 = 1.3275058269500732 + 2.0 * 6.436355113983154
Epoch 290, val loss: 1.4208356142044067
Epoch 300, training loss: 14.149888038635254 = 1.2882254123687744 + 2.0 * 6.430831432342529
Epoch 300, val loss: 1.3908833265304565
Epoch 310, training loss: 14.101593971252441 = 1.2493867874145508 + 2.0 * 6.426103591918945
Epoch 310, val loss: 1.361552357673645
Epoch 320, training loss: 14.056734085083008 = 1.2113093137741089 + 2.0 * 6.422712326049805
Epoch 320, val loss: 1.3330720663070679
Epoch 330, training loss: 14.006548881530762 = 1.1740249395370483 + 2.0 * 6.416262149810791
Epoch 330, val loss: 1.3055466413497925
Epoch 340, training loss: 13.959278106689453 = 1.1371667385101318 + 2.0 * 6.411055564880371
Epoch 340, val loss: 1.2785528898239136
Epoch 350, training loss: 13.9268159866333 = 1.100759506225586 + 2.0 * 6.413028240203857
Epoch 350, val loss: 1.252049207687378
Epoch 360, training loss: 13.87359619140625 = 1.065086841583252 + 2.0 * 6.404254913330078
Epoch 360, val loss: 1.2265324592590332
Epoch 370, training loss: 13.828386306762695 = 1.0297919511795044 + 2.0 * 6.39929723739624
Epoch 370, val loss: 1.2017017602920532
Epoch 380, training loss: 13.792845726013184 = 0.9948920011520386 + 2.0 * 6.398976802825928
Epoch 380, val loss: 1.1772422790527344
Epoch 390, training loss: 13.747546195983887 = 0.9607944488525391 + 2.0 * 6.393375873565674
Epoch 390, val loss: 1.1537126302719116
Epoch 400, training loss: 13.705718040466309 = 0.927424967288971 + 2.0 * 6.389146327972412
Epoch 400, val loss: 1.131155252456665
Epoch 410, training loss: 13.674772262573242 = 0.8948279023170471 + 2.0 * 6.38997220993042
Epoch 410, val loss: 1.109479546546936
Epoch 420, training loss: 13.631470680236816 = 0.8631889224052429 + 2.0 * 6.384140968322754
Epoch 420, val loss: 1.0885881185531616
Epoch 430, training loss: 13.592550277709961 = 0.8326401114463806 + 2.0 * 6.379955291748047
Epoch 430, val loss: 1.0691193342208862
Epoch 440, training loss: 13.559103012084961 = 0.803115963935852 + 2.0 * 6.377993583679199
Epoch 440, val loss: 1.0506423711776733
Epoch 450, training loss: 13.529455184936523 = 0.7747039198875427 + 2.0 * 6.377375602722168
Epoch 450, val loss: 1.03329336643219
Epoch 460, training loss: 13.491408348083496 = 0.7474890351295471 + 2.0 * 6.371959686279297
Epoch 460, val loss: 1.0172380208969116
Epoch 470, training loss: 13.457484245300293 = 0.7211135625839233 + 2.0 * 6.368185520172119
Epoch 470, val loss: 1.0020195245742798
Epoch 480, training loss: 13.437318801879883 = 0.6955614686012268 + 2.0 * 6.37087869644165
Epoch 480, val loss: 0.9877926111221313
Epoch 490, training loss: 13.397903442382812 = 0.6708564758300781 + 2.0 * 6.363523483276367
Epoch 490, val loss: 0.9745745658874512
Epoch 500, training loss: 13.374815940856934 = 0.6467508673667908 + 2.0 * 6.364032745361328
Epoch 500, val loss: 0.9621507525444031
Epoch 510, training loss: 13.340729713439941 = 0.6231420636177063 + 2.0 * 6.35879373550415
Epoch 510, val loss: 0.9504389762878418
Epoch 520, training loss: 13.317089080810547 = 0.5999875068664551 + 2.0 * 6.358550548553467
Epoch 520, val loss: 0.939552366733551
Epoch 530, training loss: 13.28702163696289 = 0.5772005319595337 + 2.0 * 6.354910373687744
Epoch 530, val loss: 0.9293054342269897
Epoch 540, training loss: 13.259857177734375 = 0.5547190308570862 + 2.0 * 6.352569103240967
Epoch 540, val loss: 0.9197941422462463
Epoch 550, training loss: 13.236637115478516 = 0.5324817895889282 + 2.0 * 6.352077484130859
Epoch 550, val loss: 0.9108327627182007
Epoch 560, training loss: 13.211434364318848 = 0.5106385946273804 + 2.0 * 6.350398063659668
Epoch 560, val loss: 0.9026302695274353
Epoch 570, training loss: 13.182765007019043 = 0.4891103506088257 + 2.0 * 6.346827507019043
Epoch 570, val loss: 0.89506995677948
Epoch 580, training loss: 13.167654991149902 = 0.46795740723609924 + 2.0 * 6.349848747253418
Epoch 580, val loss: 0.888176441192627
Epoch 590, training loss: 13.139487266540527 = 0.4472026228904724 + 2.0 * 6.346142292022705
Epoch 590, val loss: 0.8821074962615967
Epoch 600, training loss: 13.111286163330078 = 0.42700621485710144 + 2.0 * 6.342140197753906
Epoch 600, val loss: 0.8767728209495544
Epoch 610, training loss: 13.087425231933594 = 0.407256156206131 + 2.0 * 6.340084552764893
Epoch 610, val loss: 0.8720666170120239
Epoch 620, training loss: 13.08475112915039 = 0.3880452513694763 + 2.0 * 6.348352909088135
Epoch 620, val loss: 0.8680257797241211
Epoch 630, training loss: 13.045984268188477 = 0.36966952681541443 + 2.0 * 6.3381571769714355
Epoch 630, val loss: 0.8648755550384521
Epoch 640, training loss: 13.023380279541016 = 0.3519558012485504 + 2.0 * 6.335712432861328
Epoch 640, val loss: 0.8625761270523071
Epoch 650, training loss: 13.00167179107666 = 0.3348558247089386 + 2.0 * 6.333407878875732
Epoch 650, val loss: 0.8607434034347534
Epoch 660, training loss: 12.985786437988281 = 0.31835752725601196 + 2.0 * 6.333714485168457
Epoch 660, val loss: 0.8596506714820862
Epoch 670, training loss: 12.971427917480469 = 0.302664577960968 + 2.0 * 6.334381580352783
Epoch 670, val loss: 0.8591488599777222
Epoch 680, training loss: 12.94661808013916 = 0.28772270679473877 + 2.0 * 6.3294477462768555
Epoch 680, val loss: 0.8596205115318298
Epoch 690, training loss: 12.929214477539062 = 0.27348193526268005 + 2.0 * 6.327866077423096
Epoch 690, val loss: 0.8604092597961426
Epoch 700, training loss: 12.939116477966309 = 0.2599715292453766 + 2.0 * 6.339572429656982
Epoch 700, val loss: 0.861919641494751
Epoch 710, training loss: 12.900008201599121 = 0.2471853643655777 + 2.0 * 6.326411247253418
Epoch 710, val loss: 0.8640602827072144
Epoch 720, training loss: 12.885502815246582 = 0.2351338267326355 + 2.0 * 6.325184345245361
Epoch 720, val loss: 0.8668721318244934
Epoch 730, training loss: 12.869613647460938 = 0.2237151563167572 + 2.0 * 6.322949409484863
Epoch 730, val loss: 0.8700701594352722
Epoch 740, training loss: 12.886120796203613 = 0.21292215585708618 + 2.0 * 6.336599349975586
Epoch 740, val loss: 0.8737413287162781
Epoch 750, training loss: 12.844943046569824 = 0.20280051231384277 + 2.0 * 6.321071147918701
Epoch 750, val loss: 0.877772331237793
Epoch 760, training loss: 12.832645416259766 = 0.19329577684402466 + 2.0 * 6.319674968719482
Epoch 760, val loss: 0.8823612332344055
Epoch 770, training loss: 12.820889472961426 = 0.1843186616897583 + 2.0 * 6.3182854652404785
Epoch 770, val loss: 0.8871152400970459
Epoch 780, training loss: 12.816254615783691 = 0.17582622170448303 + 2.0 * 6.32021427154541
Epoch 780, val loss: 0.8923302888870239
Epoch 790, training loss: 12.810678482055664 = 0.16785115003585815 + 2.0 * 6.321413516998291
Epoch 790, val loss: 0.8976503610610962
Epoch 800, training loss: 12.790432929992676 = 0.16034863889217377 + 2.0 * 6.315042018890381
Epoch 800, val loss: 0.9035104513168335
Epoch 810, training loss: 12.779911994934082 = 0.15324625372886658 + 2.0 * 6.313333034515381
Epoch 810, val loss: 0.9093360304832458
Epoch 820, training loss: 12.785801887512207 = 0.14653247594833374 + 2.0 * 6.319634914398193
Epoch 820, val loss: 0.9153406023979187
Epoch 830, training loss: 12.768505096435547 = 0.14018136262893677 + 2.0 * 6.314161777496338
Epoch 830, val loss: 0.9216769933700562
Epoch 840, training loss: 12.756296157836914 = 0.13418887555599213 + 2.0 * 6.31105375289917
Epoch 840, val loss: 0.9280931949615479
Epoch 850, training loss: 12.75557804107666 = 0.12852077186107635 + 2.0 * 6.313528537750244
Epoch 850, val loss: 0.9344199895858765
Epoch 860, training loss: 12.74164867401123 = 0.12314147502183914 + 2.0 * 6.309253692626953
Epoch 860, val loss: 0.9411367774009705
Epoch 870, training loss: 12.733012199401855 = 0.11806534230709076 + 2.0 * 6.307473659515381
Epoch 870, val loss: 0.9476679563522339
Epoch 880, training loss: 12.726150512695312 = 0.11322585493326187 + 2.0 * 6.306462287902832
Epoch 880, val loss: 0.9543875455856323
Epoch 890, training loss: 12.733072280883789 = 0.10865084081888199 + 2.0 * 6.312210559844971
Epoch 890, val loss: 0.9610743522644043
Epoch 900, training loss: 12.714588165283203 = 0.10431502759456635 + 2.0 * 6.305136680603027
Epoch 900, val loss: 0.9678103923797607
Epoch 910, training loss: 12.713374137878418 = 0.10021432489156723 + 2.0 * 6.306580066680908
Epoch 910, val loss: 0.974574089050293
Epoch 920, training loss: 12.70205307006836 = 0.09633045643568039 + 2.0 * 6.302861213684082
Epoch 920, val loss: 0.981461226940155
Epoch 930, training loss: 12.696898460388184 = 0.09262191504240036 + 2.0 * 6.302138328552246
Epoch 930, val loss: 0.9881972670555115
Epoch 940, training loss: 12.698495864868164 = 0.08909467607736588 + 2.0 * 6.3047003746032715
Epoch 940, val loss: 0.9949846267700195
Epoch 950, training loss: 12.691401481628418 = 0.08574722707271576 + 2.0 * 6.30282735824585
Epoch 950, val loss: 1.0018569231033325
Epoch 960, training loss: 12.682595252990723 = 0.08256256580352783 + 2.0 * 6.300016403198242
Epoch 960, val loss: 1.0086171627044678
Epoch 970, training loss: 12.689518928527832 = 0.07954627275466919 + 2.0 * 6.304986476898193
Epoch 970, val loss: 1.015485167503357
Epoch 980, training loss: 12.678045272827148 = 0.07665875554084778 + 2.0 * 6.300693035125732
Epoch 980, val loss: 1.0219857692718506
Epoch 990, training loss: 12.666546821594238 = 0.07392933964729309 + 2.0 * 6.296308517456055
Epoch 990, val loss: 1.0288317203521729
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7740740740740741
0.8107538218239325
=== training gcn model ===
Epoch 0, training loss: 19.158159255981445 = 1.9645084142684937 + 2.0 * 8.59682559967041
Epoch 0, val loss: 1.9630773067474365
Epoch 10, training loss: 19.147537231445312 = 1.9544975757598877 + 2.0 * 8.596519470214844
Epoch 10, val loss: 1.952691674232483
Epoch 20, training loss: 19.13137435913086 = 1.9425443410873413 + 2.0 * 8.594414710998535
Epoch 20, val loss: 1.9402564764022827
Epoch 30, training loss: 19.084890365600586 = 1.9266372919082642 + 2.0 * 8.579126358032227
Epoch 30, val loss: 1.9235600233078003
Epoch 40, training loss: 18.88842010498047 = 1.906471848487854 + 2.0 * 8.490974426269531
Epoch 40, val loss: 1.9027947187423706
Epoch 50, training loss: 18.167634963989258 = 1.8832168579101562 + 2.0 * 8.14220905303955
Epoch 50, val loss: 1.8793162107467651
Epoch 60, training loss: 17.25278091430664 = 1.8616278171539307 + 2.0 * 7.695576190948486
Epoch 60, val loss: 1.8594189882278442
Epoch 70, training loss: 16.203510284423828 = 1.8489915132522583 + 2.0 * 7.1772589683532715
Epoch 70, val loss: 1.8482086658477783
Epoch 80, training loss: 15.77314281463623 = 1.8385742902755737 + 2.0 * 6.967284202575684
Epoch 80, val loss: 1.838808536529541
Epoch 90, training loss: 15.526811599731445 = 1.82450532913208 + 2.0 * 6.8511528968811035
Epoch 90, val loss: 1.8263529539108276
Epoch 100, training loss: 15.363574028015137 = 1.8100625276565552 + 2.0 * 6.7767558097839355
Epoch 100, val loss: 1.8140517473220825
Epoch 110, training loss: 15.24368667602539 = 1.7973475456237793 + 2.0 * 6.723169803619385
Epoch 110, val loss: 1.8031549453735352
Epoch 120, training loss: 15.144150733947754 = 1.7860183715820312 + 2.0 * 6.679066181182861
Epoch 120, val loss: 1.7933229207992554
Epoch 130, training loss: 15.068368911743164 = 1.7750458717346191 + 2.0 * 6.646661758422852
Epoch 130, val loss: 1.783864140510559
Epoch 140, training loss: 14.999218940734863 = 1.7637784481048584 + 2.0 * 6.617720127105713
Epoch 140, val loss: 1.7744250297546387
Epoch 150, training loss: 14.937241554260254 = 1.7519484758377075 + 2.0 * 6.592646598815918
Epoch 150, val loss: 1.764761209487915
Epoch 160, training loss: 14.884881019592285 = 1.7392350435256958 + 2.0 * 6.5728230476379395
Epoch 160, val loss: 1.7546194791793823
Epoch 170, training loss: 14.832386016845703 = 1.725250482559204 + 2.0 * 6.553567886352539
Epoch 170, val loss: 1.7436943054199219
Epoch 180, training loss: 14.783727645874023 = 1.7096384763717651 + 2.0 * 6.537044525146484
Epoch 180, val loss: 1.7316569089889526
Epoch 190, training loss: 14.737791061401367 = 1.6920273303985596 + 2.0 * 6.522881984710693
Epoch 190, val loss: 1.7181768417358398
Epoch 200, training loss: 14.695972442626953 = 1.672160029411316 + 2.0 * 6.511906147003174
Epoch 200, val loss: 1.7030599117279053
Epoch 210, training loss: 14.648585319519043 = 1.6499789953231812 + 2.0 * 6.499303340911865
Epoch 210, val loss: 1.6862200498580933
Epoch 220, training loss: 14.600727081298828 = 1.6250988245010376 + 2.0 * 6.487813949584961
Epoch 220, val loss: 1.6673245429992676
Epoch 230, training loss: 14.554595947265625 = 1.5972211360931396 + 2.0 * 6.478687286376953
Epoch 230, val loss: 1.6461745500564575
Epoch 240, training loss: 14.505976676940918 = 1.5665498971939087 + 2.0 * 6.46971321105957
Epoch 240, val loss: 1.6228379011154175
Epoch 250, training loss: 14.45468807220459 = 1.532871127128601 + 2.0 * 6.46090841293335
Epoch 250, val loss: 1.5971125364303589
Epoch 260, training loss: 14.40263843536377 = 1.4960362911224365 + 2.0 * 6.453300952911377
Epoch 260, val loss: 1.5689496994018555
Epoch 270, training loss: 14.357182502746582 = 1.4565551280975342 + 2.0 * 6.450313568115234
Epoch 270, val loss: 1.5388246774673462
Epoch 280, training loss: 14.295385360717773 = 1.415588140487671 + 2.0 * 6.439898490905762
Epoch 280, val loss: 1.5073436498641968
Epoch 290, training loss: 14.23897647857666 = 1.373099446296692 + 2.0 * 6.432938575744629
Epoch 290, val loss: 1.474926471710205
Epoch 300, training loss: 14.18294620513916 = 1.3293564319610596 + 2.0 * 6.42679500579834
Epoch 300, val loss: 1.4414726495742798
Epoch 310, training loss: 14.141976356506348 = 1.285112977027893 + 2.0 * 6.428431510925293
Epoch 310, val loss: 1.4078261852264404
Epoch 320, training loss: 14.081206321716309 = 1.241870641708374 + 2.0 * 6.419667720794678
Epoch 320, val loss: 1.3747942447662354
Epoch 330, training loss: 14.022236824035645 = 1.1999571323394775 + 2.0 * 6.411139965057373
Epoch 330, val loss: 1.3431620597839355
Epoch 340, training loss: 13.970274925231934 = 1.1592553853988647 + 2.0 * 6.405509948730469
Epoch 340, val loss: 1.3125267028808594
Epoch 350, training loss: 13.921199798583984 = 1.1197172403335571 + 2.0 * 6.400741100311279
Epoch 350, val loss: 1.2829350233078003
Epoch 360, training loss: 13.877674102783203 = 1.0814728736877441 + 2.0 * 6.39810037612915
Epoch 360, val loss: 1.2545032501220703
Epoch 370, training loss: 13.829586029052734 = 1.0450180768966675 + 2.0 * 6.392283916473389
Epoch 370, val loss: 1.2275559902191162
Epoch 380, training loss: 13.79106616973877 = 1.0103397369384766 + 2.0 * 6.3903632164001465
Epoch 380, val loss: 1.2022743225097656
Epoch 390, training loss: 13.74632453918457 = 0.9769352674484253 + 2.0 * 6.384694576263428
Epoch 390, val loss: 1.178285002708435
Epoch 400, training loss: 13.708174705505371 = 0.944553792476654 + 2.0 * 6.381810665130615
Epoch 400, val loss: 1.1554135084152222
Epoch 410, training loss: 13.673870086669922 = 0.9134485721588135 + 2.0 * 6.380210876464844
Epoch 410, val loss: 1.13352632522583
Epoch 420, training loss: 13.632577896118164 = 0.8833383321762085 + 2.0 * 6.374619960784912
Epoch 420, val loss: 1.1129156351089478
Epoch 430, training loss: 13.600520133972168 = 0.8540886640548706 + 2.0 * 6.373215675354004
Epoch 430, val loss: 1.0933257341384888
Epoch 440, training loss: 13.566210746765137 = 0.8258935213088989 + 2.0 * 6.370158672332764
Epoch 440, val loss: 1.0745998620986938
Epoch 450, training loss: 13.528478622436523 = 0.7985185384750366 + 2.0 * 6.364980220794678
Epoch 450, val loss: 1.0570296049118042
Epoch 460, training loss: 13.502153396606445 = 0.7720110416412354 + 2.0 * 6.3650712966918945
Epoch 460, val loss: 1.0403077602386475
Epoch 470, training loss: 13.464771270751953 = 0.7464425563812256 + 2.0 * 6.359164237976074
Epoch 470, val loss: 1.0246001482009888
Epoch 480, training loss: 13.434896469116211 = 0.7216135859489441 + 2.0 * 6.3566412925720215
Epoch 480, val loss: 1.0096646547317505
Epoch 490, training loss: 13.404745101928711 = 0.6974214315414429 + 2.0 * 6.353662014007568
Epoch 490, val loss: 0.9954817891120911
Epoch 500, training loss: 13.38939380645752 = 0.673889696598053 + 2.0 * 6.357751846313477
Epoch 500, val loss: 0.9819929599761963
Epoch 510, training loss: 13.349191665649414 = 0.6510758399963379 + 2.0 * 6.349058151245117
Epoch 510, val loss: 0.9693750739097595
Epoch 520, training loss: 13.322147369384766 = 0.6290032863616943 + 2.0 * 6.346571922302246
Epoch 520, val loss: 0.957674503326416
Epoch 530, training loss: 13.304459571838379 = 0.6074942946434021 + 2.0 * 6.348482608795166
Epoch 530, val loss: 0.9465115070343018
Epoch 540, training loss: 13.275036811828613 = 0.5865424871444702 + 2.0 * 6.344247341156006
Epoch 540, val loss: 0.936018168926239
Epoch 550, training loss: 13.24840259552002 = 0.5660572648048401 + 2.0 * 6.341172695159912
Epoch 550, val loss: 0.9261201024055481
Epoch 560, training loss: 13.237983703613281 = 0.5459521412849426 + 2.0 * 6.346015930175781
Epoch 560, val loss: 0.9167900681495667
Epoch 570, training loss: 13.201130867004395 = 0.5263792872428894 + 2.0 * 6.337375640869141
Epoch 570, val loss: 0.9078692197799683
Epoch 580, training loss: 13.177303314208984 = 0.5071367025375366 + 2.0 * 6.335083484649658
Epoch 580, val loss: 0.8996710777282715
Epoch 590, training loss: 13.15441608428955 = 0.4881895184516907 + 2.0 * 6.333113193511963
Epoch 590, val loss: 0.8918536901473999
Epoch 600, training loss: 13.143760681152344 = 0.4694439768791199 + 2.0 * 6.337158203125
Epoch 600, val loss: 0.8844643235206604
Epoch 610, training loss: 13.112281799316406 = 0.4510639011859894 + 2.0 * 6.33060884475708
Epoch 610, val loss: 0.8777152895927429
Epoch 620, training loss: 13.090819358825684 = 0.4329485595226288 + 2.0 * 6.328935623168945
Epoch 620, val loss: 0.8714834451675415
Epoch 630, training loss: 13.067242622375488 = 0.41504988074302673 + 2.0 * 6.326096534729004
Epoch 630, val loss: 0.865723192691803
Epoch 640, training loss: 13.062257766723633 = 0.39740443229675293 + 2.0 * 6.33242654800415
Epoch 640, val loss: 0.860427737236023
Epoch 650, training loss: 13.034130096435547 = 0.3801191449165344 + 2.0 * 6.327005386352539
Epoch 650, val loss: 0.8557924032211304
Epoch 660, training loss: 13.006770133972168 = 0.3631896376609802 + 2.0 * 6.3217902183532715
Epoch 660, val loss: 0.8517561554908752
Epoch 670, training loss: 12.9884614944458 = 0.34660330414772034 + 2.0 * 6.320929050445557
Epoch 670, val loss: 0.8482881784439087
Epoch 680, training loss: 12.971292495727539 = 0.33035415410995483 + 2.0 * 6.320469379425049
Epoch 680, val loss: 0.8454205393791199
Epoch 690, training loss: 12.953132629394531 = 0.31449875235557556 + 2.0 * 6.319316864013672
Epoch 690, val loss: 0.8432055115699768
Epoch 700, training loss: 12.932097434997559 = 0.29907384514808655 + 2.0 * 6.316511631011963
Epoch 700, val loss: 0.8416733145713806
Epoch 710, training loss: 12.915149688720703 = 0.28410908579826355 + 2.0 * 6.315520286560059
Epoch 710, val loss: 0.8408457040786743
Epoch 720, training loss: 12.908035278320312 = 0.2696267068386078 + 2.0 * 6.319204330444336
Epoch 720, val loss: 0.8403765559196472
Epoch 730, training loss: 12.884514808654785 = 0.2556554973125458 + 2.0 * 6.314429759979248
Epoch 730, val loss: 0.8407321572303772
Epoch 740, training loss: 12.87525463104248 = 0.2422899454832077 + 2.0 * 6.3164825439453125
Epoch 740, val loss: 0.8416519165039062
Epoch 750, training loss: 12.851744651794434 = 0.22945371270179749 + 2.0 * 6.311145305633545
Epoch 750, val loss: 0.842932403087616
Epoch 760, training loss: 12.83643913269043 = 0.21727213263511658 + 2.0 * 6.30958366394043
Epoch 760, val loss: 0.8449930548667908
Epoch 770, training loss: 12.826107025146484 = 0.2056717723608017 + 2.0 * 6.31021785736084
Epoch 770, val loss: 0.8473901152610779
Epoch 780, training loss: 12.816458702087402 = 0.1947086900472641 + 2.0 * 6.310874938964844
Epoch 780, val loss: 0.8500643968582153
Epoch 790, training loss: 12.796646118164062 = 0.18440932035446167 + 2.0 * 6.306118488311768
Epoch 790, val loss: 0.8535322546958923
Epoch 800, training loss: 12.784693717956543 = 0.17471283674240112 + 2.0 * 6.304990291595459
Epoch 800, val loss: 0.8571292757987976
Epoch 810, training loss: 12.790959358215332 = 0.1655844748020172 + 2.0 * 6.312687397003174
Epoch 810, val loss: 0.8610585331916809
Epoch 820, training loss: 12.765969276428223 = 0.15704166889190674 + 2.0 * 6.304463863372803
Epoch 820, val loss: 0.865283191204071
Epoch 830, training loss: 12.751065254211426 = 0.14902640879154205 + 2.0 * 6.301019191741943
Epoch 830, val loss: 0.8699682950973511
Epoch 840, training loss: 12.741377830505371 = 0.14148874580860138 + 2.0 * 6.2999444007873535
Epoch 840, val loss: 0.874728798866272
Epoch 850, training loss: 12.75203800201416 = 0.13441845774650574 + 2.0 * 6.308809757232666
Epoch 850, val loss: 0.8795949816703796
Epoch 860, training loss: 12.729576110839844 = 0.1278003305196762 + 2.0 * 6.3008880615234375
Epoch 860, val loss: 0.8847101926803589
Epoch 870, training loss: 12.717041015625 = 0.12160682678222656 + 2.0 * 6.297717094421387
Epoch 870, val loss: 0.8901635408401489
Epoch 880, training loss: 12.707462310791016 = 0.115780770778656 + 2.0 * 6.295840740203857
Epoch 880, val loss: 0.8956373929977417
Epoch 890, training loss: 12.708735466003418 = 0.11029965430498123 + 2.0 * 6.299217700958252
Epoch 890, val loss: 0.9013307094573975
Epoch 900, training loss: 12.703153610229492 = 0.1051207110285759 + 2.0 * 6.29901647567749
Epoch 900, val loss: 0.9068137407302856
Epoch 910, training loss: 12.69099235534668 = 0.10029805451631546 + 2.0 * 6.295347213745117
Epoch 910, val loss: 0.9127263426780701
Epoch 920, training loss: 12.681781768798828 = 0.09576499462127686 + 2.0 * 6.293008327484131
Epoch 920, val loss: 0.9186459183692932
Epoch 930, training loss: 12.677326202392578 = 0.09147834777832031 + 2.0 * 6.292923927307129
Epoch 930, val loss: 0.9245373606681824
Epoch 940, training loss: 12.670964241027832 = 0.08743864297866821 + 2.0 * 6.291762828826904
Epoch 940, val loss: 0.9304741621017456
Epoch 950, training loss: 12.662413597106934 = 0.08363136649131775 + 2.0 * 6.289391040802002
Epoch 950, val loss: 0.9365843534469604
Epoch 960, training loss: 12.665590286254883 = 0.08003418892621994 + 2.0 * 6.292778015136719
Epoch 960, val loss: 0.9426073431968689
Epoch 970, training loss: 12.67586612701416 = 0.0766436755657196 + 2.0 * 6.2996110916137695
Epoch 970, val loss: 0.9484656453132629
Epoch 980, training loss: 12.651793479919434 = 0.07346218079328537 + 2.0 * 6.289165496826172
Epoch 980, val loss: 0.9546511173248291
Epoch 990, training loss: 12.643296241760254 = 0.07047037035226822 + 2.0 * 6.286412715911865
Epoch 990, val loss: 0.9606660008430481
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.737037037037037
0.8118081180811808
=== training gcn model ===
Epoch 0, training loss: 19.14202117919922 = 1.9484405517578125 + 2.0 * 8.596790313720703
Epoch 0, val loss: 1.938847541809082
Epoch 10, training loss: 19.130966186523438 = 1.9384242296218872 + 2.0 * 8.596270561218262
Epoch 10, val loss: 1.9292993545532227
Epoch 20, training loss: 19.11115264892578 = 1.9261603355407715 + 2.0 * 8.592495918273926
Epoch 20, val loss: 1.9171152114868164
Epoch 30, training loss: 19.03817367553711 = 1.9101742506027222 + 2.0 * 8.564000129699707
Epoch 30, val loss: 1.900818943977356
Epoch 40, training loss: 18.528728485107422 = 1.8918110132217407 + 2.0 * 8.318458557128906
Epoch 40, val loss: 1.8823031187057495
Epoch 50, training loss: 17.07063102722168 = 1.8736258745193481 + 2.0 * 7.5985026359558105
Epoch 50, val loss: 1.8648144006729126
Epoch 60, training loss: 16.292715072631836 = 1.8615128993988037 + 2.0 * 7.215600967407227
Epoch 60, val loss: 1.8532063961029053
Epoch 70, training loss: 15.826787948608398 = 1.850121021270752 + 2.0 * 6.988333702087402
Epoch 70, val loss: 1.8423486948013306
Epoch 80, training loss: 15.519172668457031 = 1.8394451141357422 + 2.0 * 6.8398637771606445
Epoch 80, val loss: 1.8322612047195435
Epoch 90, training loss: 15.315082550048828 = 1.8272825479507446 + 2.0 * 6.743899822235107
Epoch 90, val loss: 1.8210264444351196
Epoch 100, training loss: 15.176717758178711 = 1.8147169351577759 + 2.0 * 6.681000232696533
Epoch 100, val loss: 1.8096258640289307
Epoch 110, training loss: 15.083658218383789 = 1.8026349544525146 + 2.0 * 6.640511512756348
Epoch 110, val loss: 1.7987210750579834
Epoch 120, training loss: 15.007640838623047 = 1.7910284996032715 + 2.0 * 6.608305931091309
Epoch 120, val loss: 1.7882822751998901
Epoch 130, training loss: 14.94384765625 = 1.7795871496200562 + 2.0 * 6.582130432128906
Epoch 130, val loss: 1.7780611515045166
Epoch 140, training loss: 14.885509490966797 = 1.768025517463684 + 2.0 * 6.558742046356201
Epoch 140, val loss: 1.7679437398910522
Epoch 150, training loss: 14.833681106567383 = 1.7559351921081543 + 2.0 * 6.538872718811035
Epoch 150, val loss: 1.7574182748794556
Epoch 160, training loss: 14.783937454223633 = 1.743091106414795 + 2.0 * 6.520423412322998
Epoch 160, val loss: 1.746235966682434
Epoch 170, training loss: 14.73468017578125 = 1.7290176153182983 + 2.0 * 6.50283145904541
Epoch 170, val loss: 1.7340174913406372
Epoch 180, training loss: 14.69382095336914 = 1.7134053707122803 + 2.0 * 6.490207672119141
Epoch 180, val loss: 1.7204169034957886
Epoch 190, training loss: 14.646001815795898 = 1.6959855556488037 + 2.0 * 6.475008010864258
Epoch 190, val loss: 1.705311894416809
Epoch 200, training loss: 14.601731300354004 = 1.6765549182891846 + 2.0 * 6.462588310241699
Epoch 200, val loss: 1.688325047492981
Epoch 210, training loss: 14.557537078857422 = 1.6547431945800781 + 2.0 * 6.451396942138672
Epoch 210, val loss: 1.6692280769348145
Epoch 220, training loss: 14.515737533569336 = 1.6302711963653564 + 2.0 * 6.442733287811279
Epoch 220, val loss: 1.6478087902069092
Epoch 230, training loss: 14.468345642089844 = 1.6030380725860596 + 2.0 * 6.432653903961182
Epoch 230, val loss: 1.6239919662475586
Epoch 240, training loss: 14.423162460327148 = 1.5733603239059448 + 2.0 * 6.424901008605957
Epoch 240, val loss: 1.5980942249298096
Epoch 250, training loss: 14.382762908935547 = 1.5408241748809814 + 2.0 * 6.420969486236572
Epoch 250, val loss: 1.5697940587997437
Epoch 260, training loss: 14.33265495300293 = 1.5059444904327393 + 2.0 * 6.413355350494385
Epoch 260, val loss: 1.5398846864700317
Epoch 270, training loss: 14.279396057128906 = 1.4690260887145996 + 2.0 * 6.405185222625732
Epoch 270, val loss: 1.5083214044570923
Epoch 280, training loss: 14.228169441223145 = 1.4299768209457397 + 2.0 * 6.399096488952637
Epoch 280, val loss: 1.4754629135131836
Epoch 290, training loss: 14.188453674316406 = 1.3896403312683105 + 2.0 * 6.399406909942627
Epoch 290, val loss: 1.4421592950820923
Epoch 300, training loss: 14.13121509552002 = 1.3492459058761597 + 2.0 * 6.390984535217285
Epoch 300, val loss: 1.4095258712768555
Epoch 310, training loss: 14.080513954162598 = 1.30902099609375 + 2.0 * 6.385746479034424
Epoch 310, val loss: 1.3777917623519897
Epoch 320, training loss: 14.03921127319336 = 1.2692620754241943 + 2.0 * 6.384974479675293
Epoch 320, val loss: 1.3472012281417847
Epoch 330, training loss: 13.987693786621094 = 1.2302374839782715 + 2.0 * 6.378727912902832
Epoch 330, val loss: 1.318250060081482
Epoch 340, training loss: 13.942845344543457 = 1.1926021575927734 + 2.0 * 6.375121593475342
Epoch 340, val loss: 1.2908310890197754
Epoch 350, training loss: 13.900689125061035 = 1.1562778949737549 + 2.0 * 6.37220573425293
Epoch 350, val loss: 1.2652428150177002
Epoch 360, training loss: 13.853218078613281 = 1.121532917022705 + 2.0 * 6.365842819213867
Epoch 360, val loss: 1.2413725852966309
Epoch 370, training loss: 13.814984321594238 = 1.0880639553070068 + 2.0 * 6.363460063934326
Epoch 370, val loss: 1.2191154956817627
Epoch 380, training loss: 13.785606384277344 = 1.0561188459396362 + 2.0 * 6.364743709564209
Epoch 380, val loss: 1.1984496116638184
Epoch 390, training loss: 13.739808082580566 = 1.0257436037063599 + 2.0 * 6.357032299041748
Epoch 390, val loss: 1.1794239282608032
Epoch 400, training loss: 13.703967094421387 = 0.9968375563621521 + 2.0 * 6.353564739227295
Epoch 400, val loss: 1.1618751287460327
Epoch 410, training loss: 13.670798301696777 = 0.9691288471221924 + 2.0 * 6.350834846496582
Epoch 410, val loss: 1.1457383632659912
Epoch 420, training loss: 13.639985084533691 = 0.9428106546401978 + 2.0 * 6.3485870361328125
Epoch 420, val loss: 1.1307984590530396
Epoch 430, training loss: 13.609405517578125 = 0.9177296757698059 + 2.0 * 6.3458380699157715
Epoch 430, val loss: 1.1172326803207397
Epoch 440, training loss: 13.579413414001465 = 0.8936780691146851 + 2.0 * 6.342867851257324
Epoch 440, val loss: 1.104815125465393
Epoch 450, training loss: 13.554915428161621 = 0.8704367876052856 + 2.0 * 6.3422393798828125
Epoch 450, val loss: 1.0932691097259521
Epoch 460, training loss: 13.531195640563965 = 0.8479891419410706 + 2.0 * 6.3416032791137695
Epoch 460, val loss: 1.0826576948165894
Epoch 470, training loss: 13.496392250061035 = 0.8262029886245728 + 2.0 * 6.335094451904297
Epoch 470, val loss: 1.0727163553237915
Epoch 480, training loss: 13.472064971923828 = 0.8048842549324036 + 2.0 * 6.333590507507324
Epoch 480, val loss: 1.0633872747421265
Epoch 490, training loss: 13.467704772949219 = 0.7840285301208496 + 2.0 * 6.341838359832764
Epoch 490, val loss: 1.0545806884765625
Epoch 500, training loss: 13.422593116760254 = 0.7634928822517395 + 2.0 * 6.329550266265869
Epoch 500, val loss: 1.0463314056396484
Epoch 510, training loss: 13.39726448059082 = 0.7433751821517944 + 2.0 * 6.326944828033447
Epoch 510, val loss: 1.038503646850586
Epoch 520, training loss: 13.375847816467285 = 0.7234468460083008 + 2.0 * 6.326200485229492
Epoch 520, val loss: 1.030919075012207
Epoch 530, training loss: 13.352300643920898 = 0.703698992729187 + 2.0 * 6.324300765991211
Epoch 530, val loss: 1.0236141681671143
Epoch 540, training loss: 13.331055641174316 = 0.6842076182365417 + 2.0 * 6.323423862457275
Epoch 540, val loss: 1.0164647102355957
Epoch 550, training loss: 13.308608055114746 = 0.664873480796814 + 2.0 * 6.3218674659729
Epoch 550, val loss: 1.0095566511154175
Epoch 560, training loss: 13.289666175842285 = 0.6457258462905884 + 2.0 * 6.321969985961914
Epoch 560, val loss: 1.0028393268585205
Epoch 570, training loss: 13.26500129699707 = 0.6268181204795837 + 2.0 * 6.319091796875
Epoch 570, val loss: 0.9962411522865295
Epoch 580, training loss: 13.239680290222168 = 0.6080798506736755 + 2.0 * 6.315800189971924
Epoch 580, val loss: 0.9899355173110962
Epoch 590, training loss: 13.21780776977539 = 0.589425265789032 + 2.0 * 6.3141913414001465
Epoch 590, val loss: 0.9837115406990051
Epoch 600, training loss: 13.22968578338623 = 0.5707984566688538 + 2.0 * 6.329443454742432
Epoch 600, val loss: 0.9775179028511047
Epoch 610, training loss: 13.176734924316406 = 0.5523838996887207 + 2.0 * 6.312175273895264
Epoch 610, val loss: 0.9713114500045776
Epoch 620, training loss: 13.157062530517578 = 0.5342878699302673 + 2.0 * 6.311387538909912
Epoch 620, val loss: 0.9653975963592529
Epoch 630, training loss: 13.134764671325684 = 0.5163107514381409 + 2.0 * 6.309226989746094
Epoch 630, val loss: 0.9596388339996338
Epoch 640, training loss: 13.113842964172363 = 0.4984343945980072 + 2.0 * 6.307704448699951
Epoch 640, val loss: 0.9539477229118347
Epoch 650, training loss: 13.116179466247559 = 0.4806729555130005 + 2.0 * 6.317753314971924
Epoch 650, val loss: 0.9482079148292542
Epoch 660, training loss: 13.07784652709961 = 0.4629271924495697 + 2.0 * 6.307459831237793
Epoch 660, val loss: 0.9424260258674622
Epoch 670, training loss: 13.055999755859375 = 0.4454192519187927 + 2.0 * 6.305290222167969
Epoch 670, val loss: 0.9368349313735962
Epoch 680, training loss: 13.039135932922363 = 0.4280361235141754 + 2.0 * 6.3055500984191895
Epoch 680, val loss: 0.9313860535621643
Epoch 690, training loss: 13.016707420349121 = 0.41077977418899536 + 2.0 * 6.302963733673096
Epoch 690, val loss: 0.9260250329971313
Epoch 700, training loss: 12.999595642089844 = 0.3936767876148224 + 2.0 * 6.302959442138672
Epoch 700, val loss: 0.9207702875137329
Epoch 710, training loss: 12.98031234741211 = 0.3767678737640381 + 2.0 * 6.301772117614746
Epoch 710, val loss: 0.9156806468963623
Epoch 720, training loss: 12.960172653198242 = 0.3601147532463074 + 2.0 * 6.3000288009643555
Epoch 720, val loss: 0.9108722805976868
Epoch 730, training loss: 12.947539329528809 = 0.34371310472488403 + 2.0 * 6.301913261413574
Epoch 730, val loss: 0.9061425924301147
Epoch 740, training loss: 12.921874046325684 = 0.3275970220565796 + 2.0 * 6.297138690948486
Epoch 740, val loss: 0.9015485644340515
Epoch 750, training loss: 12.905743598937988 = 0.3118952214717865 + 2.0 * 6.296924114227295
Epoch 750, val loss: 0.8973721861839294
Epoch 760, training loss: 12.889466285705566 = 0.29666444659233093 + 2.0 * 6.296401023864746
Epoch 760, val loss: 0.8935967683792114
Epoch 770, training loss: 12.87433910369873 = 0.28196850419044495 + 2.0 * 6.296185493469238
Epoch 770, val loss: 0.8902634382247925
Epoch 780, training loss: 12.86556625366211 = 0.2679350674152374 + 2.0 * 6.298815727233887
Epoch 780, val loss: 0.8873165845870972
Epoch 790, training loss: 12.84367561340332 = 0.25452160835266113 + 2.0 * 6.294577121734619
Epoch 790, val loss: 0.8846678733825684
Epoch 800, training loss: 12.826536178588867 = 0.24177996814250946 + 2.0 * 6.292377948760986
Epoch 800, val loss: 0.8826048970222473
Epoch 810, training loss: 12.821756362915039 = 0.22962509095668793 + 2.0 * 6.296065807342529
Epoch 810, val loss: 0.8809307217597961
Epoch 820, training loss: 12.799802780151367 = 0.21812057495117188 + 2.0 * 6.290841102600098
Epoch 820, val loss: 0.8798357248306274
Epoch 830, training loss: 12.785520553588867 = 0.2072024792432785 + 2.0 * 6.289158821105957
Epoch 830, val loss: 0.8790461421012878
Epoch 840, training loss: 12.776972770690918 = 0.19684644043445587 + 2.0 * 6.290063381195068
Epoch 840, val loss: 0.8786842823028564
Epoch 850, training loss: 12.764992713928223 = 0.18704663217067719 + 2.0 * 6.288972854614258
Epoch 850, val loss: 0.8786430358886719
Epoch 860, training loss: 12.753746032714844 = 0.17781029641628265 + 2.0 * 6.287967681884766
Epoch 860, val loss: 0.8789601922035217
Epoch 870, training loss: 12.748854637145996 = 0.1690913885831833 + 2.0 * 6.289881706237793
Epoch 870, val loss: 0.8795272707939148
Epoch 880, training loss: 12.729632377624512 = 0.16084712743759155 + 2.0 * 6.284392833709717
Epoch 880, val loss: 0.8805990815162659
Epoch 890, training loss: 12.722136497497559 = 0.1530701220035553 + 2.0 * 6.2845330238342285
Epoch 890, val loss: 0.8818857669830322
Epoch 900, training loss: 12.728520393371582 = 0.14573729038238525 + 2.0 * 6.291391372680664
Epoch 900, val loss: 0.883430004119873
Epoch 910, training loss: 12.703858375549316 = 0.13880275189876556 + 2.0 * 6.282527923583984
Epoch 910, val loss: 0.8850870132446289
Epoch 920, training loss: 12.694218635559082 = 0.13228031992912292 + 2.0 * 6.280969142913818
Epoch 920, val loss: 0.8870960474014282
Epoch 930, training loss: 12.685876846313477 = 0.12611442804336548 + 2.0 * 6.279881000518799
Epoch 930, val loss: 0.8893866539001465
Epoch 940, training loss: 12.690503120422363 = 0.12025764584541321 + 2.0 * 6.285122871398926
Epoch 940, val loss: 0.8918824791908264
Epoch 950, training loss: 12.678050994873047 = 0.11472544074058533 + 2.0 * 6.281662940979004
Epoch 950, val loss: 0.8942510485649109
Epoch 960, training loss: 12.67719554901123 = 0.10949664562940598 + 2.0 * 6.283849239349365
Epoch 960, val loss: 0.8970397114753723
Epoch 970, training loss: 12.662991523742676 = 0.10459958016872406 + 2.0 * 6.279195785522461
Epoch 970, val loss: 0.8998314738273621
Epoch 980, training loss: 12.653986930847168 = 0.09997190535068512 + 2.0 * 6.277007579803467
Epoch 980, val loss: 0.9027760624885559
Epoch 990, training loss: 12.647071838378906 = 0.09559261053800583 + 2.0 * 6.275739669799805
Epoch 990, val loss: 0.9059508442878723
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7481481481481482
0.8075909330521878
The final CL Acc:0.75309, 0.01552, The final GNN Acc:0.81005, 0.00179
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13216])
remove edge: torch.Size([2, 7904])
updated graph: torch.Size([2, 10564])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 19.130714416503906 = 1.9370986223220825 + 2.0 * 8.596807479858398
Epoch 0, val loss: 1.9286291599273682
Epoch 10, training loss: 19.11960792541504 = 1.926958441734314 + 2.0 * 8.596324920654297
Epoch 10, val loss: 1.918837070465088
Epoch 20, training loss: 19.099430084228516 = 1.914227843284607 + 2.0 * 8.59260082244873
Epoch 20, val loss: 1.9061448574066162
Epoch 30, training loss: 19.022159576416016 = 1.8971242904663086 + 2.0 * 8.562518119812012
Epoch 30, val loss: 1.8892379999160767
Epoch 40, training loss: 18.555025100708008 = 1.8774350881576538 + 2.0 * 8.338794708251953
Epoch 40, val loss: 1.8706501722335815
Epoch 50, training loss: 17.417051315307617 = 1.8566588163375854 + 2.0 * 7.780196666717529
Epoch 50, val loss: 1.8511972427368164
Epoch 60, training loss: 16.528640747070312 = 1.8417110443115234 + 2.0 * 7.343464374542236
Epoch 60, val loss: 1.8380259275436401
Epoch 70, training loss: 15.967580795288086 = 1.8313709497451782 + 2.0 * 7.0681047439575195
Epoch 70, val loss: 1.8287413120269775
Epoch 80, training loss: 15.700422286987305 = 1.8194186687469482 + 2.0 * 6.940501689910889
Epoch 80, val loss: 1.818028211593628
Epoch 90, training loss: 15.478272438049316 = 1.8067466020584106 + 2.0 * 6.835762977600098
Epoch 90, val loss: 1.8070067167282104
Epoch 100, training loss: 15.265470504760742 = 1.7964352369308472 + 2.0 * 6.734517574310303
Epoch 100, val loss: 1.7980470657348633
Epoch 110, training loss: 15.119902610778809 = 1.7873002290725708 + 2.0 * 6.666301250457764
Epoch 110, val loss: 1.7897660732269287
Epoch 120, training loss: 15.014557838439941 = 1.7773916721343994 + 2.0 * 6.6185832023620605
Epoch 120, val loss: 1.7808246612548828
Epoch 130, training loss: 14.931808471679688 = 1.7665659189224243 + 2.0 * 6.582621097564697
Epoch 130, val loss: 1.7712026834487915
Epoch 140, training loss: 14.869319915771484 = 1.754807472229004 + 2.0 * 6.55725622177124
Epoch 140, val loss: 1.7608349323272705
Epoch 150, training loss: 14.808452606201172 = 1.7421104907989502 + 2.0 * 6.5331711769104
Epoch 150, val loss: 1.749565839767456
Epoch 160, training loss: 14.755414962768555 = 1.728041648864746 + 2.0 * 6.513686656951904
Epoch 160, val loss: 1.7371522188186646
Epoch 170, training loss: 14.706170082092285 = 1.7121918201446533 + 2.0 * 6.4969892501831055
Epoch 170, val loss: 1.7232691049575806
Epoch 180, training loss: 14.661162376403809 = 1.694149374961853 + 2.0 * 6.483506679534912
Epoch 180, val loss: 1.707597017288208
Epoch 190, training loss: 14.61517333984375 = 1.6736292839050293 + 2.0 * 6.4707722663879395
Epoch 190, val loss: 1.6898791790008545
Epoch 200, training loss: 14.567594528198242 = 1.6502301692962646 + 2.0 * 6.458682060241699
Epoch 200, val loss: 1.6697211265563965
Epoch 210, training loss: 14.520453453063965 = 1.6234983205795288 + 2.0 * 6.448477745056152
Epoch 210, val loss: 1.6466230154037476
Epoch 220, training loss: 14.472617149353027 = 1.5934088230133057 + 2.0 * 6.43960428237915
Epoch 220, val loss: 1.620827317237854
Epoch 230, training loss: 14.422907829284668 = 1.5598955154418945 + 2.0 * 6.431506156921387
Epoch 230, val loss: 1.5922244787216187
Epoch 240, training loss: 14.371147155761719 = 1.5229356288909912 + 2.0 * 6.424105644226074
Epoch 240, val loss: 1.560789704322815
Epoch 250, training loss: 14.316349983215332 = 1.482677698135376 + 2.0 * 6.416836261749268
Epoch 250, val loss: 1.5267210006713867
Epoch 260, training loss: 14.269637107849121 = 1.439645767211914 + 2.0 * 6.4149956703186035
Epoch 260, val loss: 1.4906153678894043
Epoch 270, training loss: 14.207316398620605 = 1.3951947689056396 + 2.0 * 6.406060695648193
Epoch 270, val loss: 1.4535763263702393
Epoch 280, training loss: 14.148146629333496 = 1.349759817123413 + 2.0 * 6.399193286895752
Epoch 280, val loss: 1.4161449670791626
Epoch 290, training loss: 14.095016479492188 = 1.3038023710250854 + 2.0 * 6.395606994628906
Epoch 290, val loss: 1.3787076473236084
Epoch 300, training loss: 14.043985366821289 = 1.2584997415542603 + 2.0 * 6.39274263381958
Epoch 300, val loss: 1.3422452211380005
Epoch 310, training loss: 13.98569393157959 = 1.2143465280532837 + 2.0 * 6.385673522949219
Epoch 310, val loss: 1.3071714639663696
Epoch 320, training loss: 13.932023048400879 = 1.1713258028030396 + 2.0 * 6.3803486824035645
Epoch 320, val loss: 1.2735366821289062
Epoch 330, training loss: 13.896634101867676 = 1.1297715902328491 + 2.0 * 6.383431434631348
Epoch 330, val loss: 1.2414054870605469
Epoch 340, training loss: 13.839091300964355 = 1.0900695323944092 + 2.0 * 6.374510765075684
Epoch 340, val loss: 1.2112457752227783
Epoch 350, training loss: 13.791967391967773 = 1.0521039962768555 + 2.0 * 6.369931697845459
Epoch 350, val loss: 1.1828593015670776
Epoch 360, training loss: 13.746115684509277 = 1.015797734260559 + 2.0 * 6.365159034729004
Epoch 360, val loss: 1.1560293436050415
Epoch 370, training loss: 13.706120491027832 = 0.9809460043907166 + 2.0 * 6.3625874519348145
Epoch 370, val loss: 1.1307008266448975
Epoch 380, training loss: 13.675056457519531 = 0.9475508332252502 + 2.0 * 6.363752841949463
Epoch 380, val loss: 1.1067986488342285
Epoch 390, training loss: 13.627068519592285 = 0.9156938791275024 + 2.0 * 6.355687141418457
Epoch 390, val loss: 1.0842463970184326
Epoch 400, training loss: 13.588725090026855 = 0.8850725293159485 + 2.0 * 6.351826190948486
Epoch 400, val loss: 1.0630125999450684
Epoch 410, training loss: 13.552865028381348 = 0.8554325699806213 + 2.0 * 6.3487162590026855
Epoch 410, val loss: 1.0426409244537354
Epoch 420, training loss: 13.53592586517334 = 0.82657790184021 + 2.0 * 6.354673862457275
Epoch 420, val loss: 1.023073434829712
Epoch 430, training loss: 13.489870071411133 = 0.7988653182983398 + 2.0 * 6.3455023765563965
Epoch 430, val loss: 1.0044548511505127
Epoch 440, training loss: 13.453472137451172 = 0.7718287706375122 + 2.0 * 6.340821743011475
Epoch 440, val loss: 0.9866400957107544
Epoch 450, training loss: 13.421255111694336 = 0.7452956438064575 + 2.0 * 6.337979793548584
Epoch 450, val loss: 0.9693377614021301
Epoch 460, training loss: 13.392532348632812 = 0.7190910577774048 + 2.0 * 6.3367204666137695
Epoch 460, val loss: 0.9524886608123779
Epoch 470, training loss: 13.375118255615234 = 0.6933590173721313 + 2.0 * 6.340879440307617
Epoch 470, val loss: 0.9360476732254028
Epoch 480, training loss: 13.33562183380127 = 0.6681344509124756 + 2.0 * 6.333743572235107
Epoch 480, val loss: 0.9201981425285339
Epoch 490, training loss: 13.302762985229492 = 0.6432432532310486 + 2.0 * 6.3297600746154785
Epoch 490, val loss: 0.9047907590866089
Epoch 500, training loss: 13.275111198425293 = 0.6185646653175354 + 2.0 * 6.328273296356201
Epoch 500, val loss: 0.889803409576416
Epoch 510, training loss: 13.246594429016113 = 0.594186007976532 + 2.0 * 6.326204299926758
Epoch 510, val loss: 0.8751661777496338
Epoch 520, training loss: 13.220154762268066 = 0.5701187252998352 + 2.0 * 6.325017929077148
Epoch 520, val loss: 0.861029863357544
Epoch 530, training loss: 13.192401885986328 = 0.5463359951972961 + 2.0 * 6.323032855987549
Epoch 530, val loss: 0.8474113941192627
Epoch 540, training loss: 13.176312446594238 = 0.5227814316749573 + 2.0 * 6.326765537261963
Epoch 540, val loss: 0.8342515826225281
Epoch 550, training loss: 13.144577026367188 = 0.49967315793037415 + 2.0 * 6.322452068328857
Epoch 550, val loss: 0.8215847611427307
Epoch 560, training loss: 13.115181922912598 = 0.4770478904247284 + 2.0 * 6.319067001342773
Epoch 560, val loss: 0.809847891330719
Epoch 570, training loss: 13.087942123413086 = 0.4548877477645874 + 2.0 * 6.316527366638184
Epoch 570, val loss: 0.7986677289009094
Epoch 580, training loss: 13.064149856567383 = 0.43318912386894226 + 2.0 * 6.3154802322387695
Epoch 580, val loss: 0.7882463335990906
Epoch 590, training loss: 13.048590660095215 = 0.4121159613132477 + 2.0 * 6.3182373046875
Epoch 590, val loss: 0.7786118984222412
Epoch 600, training loss: 13.019145011901855 = 0.39176586270332336 + 2.0 * 6.313689708709717
Epoch 600, val loss: 0.769941508769989
Epoch 610, training loss: 12.994547843933105 = 0.3722023367881775 + 2.0 * 6.311172962188721
Epoch 610, val loss: 0.762151300907135
Epoch 620, training loss: 12.973990440368652 = 0.3534049987792969 + 2.0 * 6.310292720794678
Epoch 620, val loss: 0.7552574872970581
Epoch 630, training loss: 12.957077980041504 = 0.3354184329509735 + 2.0 * 6.3108296394348145
Epoch 630, val loss: 0.7493337988853455
Epoch 640, training loss: 12.938725471496582 = 0.31836286187171936 + 2.0 * 6.310181140899658
Epoch 640, val loss: 0.7445869445800781
Epoch 650, training loss: 12.916302680969238 = 0.30216771364212036 + 2.0 * 6.307067394256592
Epoch 650, val loss: 0.7406643629074097
Epoch 660, training loss: 12.896355628967285 = 0.28678783774375916 + 2.0 * 6.304783821105957
Epoch 660, val loss: 0.7376384139060974
Epoch 670, training loss: 12.886309623718262 = 0.272117018699646 + 2.0 * 6.307096481323242
Epoch 670, val loss: 0.7353579998016357
Epoch 680, training loss: 12.865301132202148 = 0.2581964433193207 + 2.0 * 6.303552150726318
Epoch 680, val loss: 0.7339967489242554
Epoch 690, training loss: 12.849447250366211 = 0.2449711710214615 + 2.0 * 6.3022379875183105
Epoch 690, val loss: 0.7333095073699951
Epoch 700, training loss: 12.832826614379883 = 0.2323949784040451 + 2.0 * 6.300215721130371
Epoch 700, val loss: 0.7333269715309143
Epoch 710, training loss: 12.818507194519043 = 0.22040900588035583 + 2.0 * 6.299048900604248
Epoch 710, val loss: 0.7339311242103577
Epoch 720, training loss: 12.814023971557617 = 0.20902496576309204 + 2.0 * 6.302499294281006
Epoch 720, val loss: 0.7350937724113464
Epoch 730, training loss: 12.797540664672852 = 0.1983162760734558 + 2.0 * 6.299612045288086
Epoch 730, val loss: 0.7367910742759705
Epoch 740, training loss: 12.781699180603027 = 0.18819956481456757 + 2.0 * 6.296749591827393
Epoch 740, val loss: 0.738982617855072
Epoch 750, training loss: 12.769658088684082 = 0.17860592901706696 + 2.0 * 6.295526027679443
Epoch 750, val loss: 0.7416254878044128
Epoch 760, training loss: 12.761880874633789 = 0.16951008141040802 + 2.0 * 6.296185493469238
Epoch 760, val loss: 0.7445368766784668
Epoch 770, training loss: 12.745767593383789 = 0.16092449426651 + 2.0 * 6.292421340942383
Epoch 770, val loss: 0.7478663921356201
Epoch 780, training loss: 12.739992141723633 = 0.15281696617603302 + 2.0 * 6.293587684631348
Epoch 780, val loss: 0.751456618309021
Epoch 790, training loss: 12.72551441192627 = 0.14518216252326965 + 2.0 * 6.290165901184082
Epoch 790, val loss: 0.7552446722984314
Epoch 800, training loss: 12.72074031829834 = 0.13800692558288574 + 2.0 * 6.2913665771484375
Epoch 800, val loss: 0.759502112865448
Epoch 810, training loss: 12.707755088806152 = 0.1312386691570282 + 2.0 * 6.288258075714111
Epoch 810, val loss: 0.7638301253318787
Epoch 820, training loss: 12.701026916503906 = 0.124846450984478 + 2.0 * 6.288090229034424
Epoch 820, val loss: 0.7684044241905212
Epoch 830, training loss: 12.694069862365723 = 0.11882422119379044 + 2.0 * 6.287622928619385
Epoch 830, val loss: 0.7731335759162903
Epoch 840, training loss: 12.690292358398438 = 0.11318326741456985 + 2.0 * 6.288554668426514
Epoch 840, val loss: 0.7779903411865234
Epoch 850, training loss: 12.68001937866211 = 0.10787444561719894 + 2.0 * 6.286072254180908
Epoch 850, val loss: 0.7829545140266418
Epoch 860, training loss: 12.671768188476562 = 0.1028706431388855 + 2.0 * 6.284448623657227
Epoch 860, val loss: 0.7880305647850037
Epoch 870, training loss: 12.674853324890137 = 0.09815122932195663 + 2.0 * 6.288351058959961
Epoch 870, val loss: 0.7931599020957947
Epoch 880, training loss: 12.661730766296387 = 0.09373173862695694 + 2.0 * 6.283999443054199
Epoch 880, val loss: 0.7983738780021667
Epoch 890, training loss: 12.657092094421387 = 0.08954782038927078 + 2.0 * 6.283771991729736
Epoch 890, val loss: 0.8035977482795715
Epoch 900, training loss: 12.652904510498047 = 0.08562224358320236 + 2.0 * 6.283641338348389
Epoch 900, val loss: 0.8088700771331787
Epoch 910, training loss: 12.642781257629395 = 0.08191095292568207 + 2.0 * 6.280435085296631
Epoch 910, val loss: 0.8141397833824158
Epoch 920, training loss: 12.639876365661621 = 0.07842065393924713 + 2.0 * 6.280727863311768
Epoch 920, val loss: 0.819452166557312
Epoch 930, training loss: 12.634847640991211 = 0.07512476295232773 + 2.0 * 6.2798614501953125
Epoch 930, val loss: 0.8247080445289612
Epoch 940, training loss: 12.62740707397461 = 0.07201617956161499 + 2.0 * 6.277695655822754
Epoch 940, val loss: 0.8300821185112
Epoch 950, training loss: 12.62348461151123 = 0.06908577680587769 + 2.0 * 6.2771992683410645
Epoch 950, val loss: 0.8354282379150391
Epoch 960, training loss: 12.622432708740234 = 0.06629886478185654 + 2.0 * 6.278067111968994
Epoch 960, val loss: 0.840723991394043
Epoch 970, training loss: 12.617871284484863 = 0.06366273760795593 + 2.0 * 6.277104377746582
Epoch 970, val loss: 0.8459174036979675
Epoch 980, training loss: 12.612004280090332 = 0.061182651668787 + 2.0 * 6.2754106521606445
Epoch 980, val loss: 0.8512489199638367
Epoch 990, training loss: 12.608242988586426 = 0.058837808668613434 + 2.0 * 6.274702548980713
Epoch 990, val loss: 0.8564625978469849
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.788888888888889
0.8360569319978914
=== training gcn model ===
Epoch 0, training loss: 19.13365364074707 = 1.9400665760040283 + 2.0 * 8.596793174743652
Epoch 0, val loss: 1.9454693794250488
Epoch 10, training loss: 19.122615814208984 = 1.92985200881958 + 2.0 * 8.596382141113281
Epoch 10, val loss: 1.934533953666687
Epoch 20, training loss: 19.104095458984375 = 1.9171528816223145 + 2.0 * 8.59347152709961
Epoch 20, val loss: 1.9205894470214844
Epoch 30, training loss: 19.045787811279297 = 1.899775505065918 + 2.0 * 8.573006629943848
Epoch 30, val loss: 1.9015041589736938
Epoch 40, training loss: 18.768508911132812 = 1.878849983215332 + 2.0 * 8.444829940795898
Epoch 40, val loss: 1.879738450050354
Epoch 50, training loss: 17.891008377075195 = 1.8557345867156982 + 2.0 * 8.017637252807617
Epoch 50, val loss: 1.8558238744735718
Epoch 60, training loss: 17.064908981323242 = 1.8348263502120972 + 2.0 * 7.615041732788086
Epoch 60, val loss: 1.8360637426376343
Epoch 70, training loss: 16.264482498168945 = 1.8232637643814087 + 2.0 * 7.220609188079834
Epoch 70, val loss: 1.8261114358901978
Epoch 80, training loss: 15.845514297485352 = 1.8129210472106934 + 2.0 * 7.016296863555908
Epoch 80, val loss: 1.8154884576797485
Epoch 90, training loss: 15.621875762939453 = 1.798875093460083 + 2.0 * 6.911500453948975
Epoch 90, val loss: 1.8018757104873657
Epoch 100, training loss: 15.500467300415039 = 1.7837973833084106 + 2.0 * 6.858335018157959
Epoch 100, val loss: 1.788089632987976
Epoch 110, training loss: 15.37926197052002 = 1.7700320482254028 + 2.0 * 6.804615020751953
Epoch 110, val loss: 1.7755886316299438
Epoch 120, training loss: 15.258584022521973 = 1.7576720714569092 + 2.0 * 6.750455856323242
Epoch 120, val loss: 1.7638702392578125
Epoch 130, training loss: 15.132123947143555 = 1.7455209493637085 + 2.0 * 6.693301677703857
Epoch 130, val loss: 1.7524845600128174
Epoch 140, training loss: 15.022253036499023 = 1.732331395149231 + 2.0 * 6.644960880279541
Epoch 140, val loss: 1.740494728088379
Epoch 150, training loss: 14.932466506958008 = 1.716960072517395 + 2.0 * 6.607753276824951
Epoch 150, val loss: 1.727151870727539
Epoch 160, training loss: 14.85171127319336 = 1.6990694999694824 + 2.0 * 6.576321125030518
Epoch 160, val loss: 1.7117528915405273
Epoch 170, training loss: 14.782357215881348 = 1.6784723997116089 + 2.0 * 6.551942348480225
Epoch 170, val loss: 1.6941410303115845
Epoch 180, training loss: 14.716686248779297 = 1.6550967693328857 + 2.0 * 6.530794620513916
Epoch 180, val loss: 1.6742534637451172
Epoch 190, training loss: 14.655607223510742 = 1.6286959648132324 + 2.0 * 6.513455390930176
Epoch 190, val loss: 1.6518653631210327
Epoch 200, training loss: 14.593657493591309 = 1.5992060899734497 + 2.0 * 6.497225761413574
Epoch 200, val loss: 1.6270551681518555
Epoch 210, training loss: 14.537992477416992 = 1.5665650367736816 + 2.0 * 6.485713481903076
Epoch 210, val loss: 1.599615454673767
Epoch 220, training loss: 14.475327491760254 = 1.5313047170639038 + 2.0 * 6.472011566162109
Epoch 220, val loss: 1.5701968669891357
Epoch 230, training loss: 14.414705276489258 = 1.4937611818313599 + 2.0 * 6.460472106933594
Epoch 230, val loss: 1.5389306545257568
Epoch 240, training loss: 14.355731010437012 = 1.45429527759552 + 2.0 * 6.450717926025391
Epoch 240, val loss: 1.5061317682266235
Epoch 250, training loss: 14.304019927978516 = 1.4138973951339722 + 2.0 * 6.445061206817627
Epoch 250, val loss: 1.4727858304977417
Epoch 260, training loss: 14.242615699768066 = 1.3735005855560303 + 2.0 * 6.4345574378967285
Epoch 260, val loss: 1.4395434856414795
Epoch 270, training loss: 14.184751510620117 = 1.3332502841949463 + 2.0 * 6.425750732421875
Epoch 270, val loss: 1.4066411256790161
Epoch 280, training loss: 14.137596130371094 = 1.2932202816009521 + 2.0 * 6.422187805175781
Epoch 280, val loss: 1.3742616176605225
Epoch 290, training loss: 14.082772254943848 = 1.254272699356079 + 2.0 * 6.414249897003174
Epoch 290, val loss: 1.3429441452026367
Epoch 300, training loss: 14.030632019042969 = 1.2165043354034424 + 2.0 * 6.407063961029053
Epoch 300, val loss: 1.3128076791763306
Epoch 310, training loss: 13.981222152709961 = 1.179587483406067 + 2.0 * 6.400817394256592
Epoch 310, val loss: 1.2835875749588013
Epoch 320, training loss: 13.937066078186035 = 1.1434065103530884 + 2.0 * 6.396829605102539
Epoch 320, val loss: 1.2551182508468628
Epoch 330, training loss: 13.89248275756836 = 1.1080666780471802 + 2.0 * 6.392208099365234
Epoch 330, val loss: 1.227536916732788
Epoch 340, training loss: 13.848563194274902 = 1.073490023612976 + 2.0 * 6.387536525726318
Epoch 340, val loss: 1.2006268501281738
Epoch 350, training loss: 13.805435180664062 = 1.0394749641418457 + 2.0 * 6.382979869842529
Epoch 350, val loss: 1.1742786169052124
Epoch 360, training loss: 13.762839317321777 = 1.005861520767212 + 2.0 * 6.378489017486572
Epoch 360, val loss: 1.1482502222061157
Epoch 370, training loss: 13.723069190979004 = 0.9725542664527893 + 2.0 * 6.37525749206543
Epoch 370, val loss: 1.1225109100341797
Epoch 380, training loss: 13.684767723083496 = 0.9396525621414185 + 2.0 * 6.372557640075684
Epoch 380, val loss: 1.097197413444519
Epoch 390, training loss: 13.644580841064453 = 0.9070927500724792 + 2.0 * 6.368743896484375
Epoch 390, val loss: 1.0722461938858032
Epoch 400, training loss: 13.604777336120605 = 0.8748255372047424 + 2.0 * 6.364975929260254
Epoch 400, val loss: 1.0475709438323975
Epoch 410, training loss: 13.567253112792969 = 0.8425332307815552 + 2.0 * 6.362360000610352
Epoch 410, val loss: 1.022998571395874
Epoch 420, training loss: 13.531645774841309 = 0.8104463815689087 + 2.0 * 6.360599517822266
Epoch 420, val loss: 0.9986887574195862
Epoch 430, training loss: 13.491166114807129 = 0.7787534594535828 + 2.0 * 6.35620641708374
Epoch 430, val loss: 0.974922776222229
Epoch 440, training loss: 13.455194473266602 = 0.7473035454750061 + 2.0 * 6.353945255279541
Epoch 440, val loss: 0.9515611529350281
Epoch 450, training loss: 13.420039176940918 = 0.7162128686904907 + 2.0 * 6.351912975311279
Epoch 450, val loss: 0.9288250803947449
Epoch 460, training loss: 13.385619163513184 = 0.6857308149337769 + 2.0 * 6.349944114685059
Epoch 460, val loss: 0.9070008397102356
Epoch 470, training loss: 13.349257469177246 = 0.6558074355125427 + 2.0 * 6.346724987030029
Epoch 470, val loss: 0.8859432935714722
Epoch 480, training loss: 13.331865310668945 = 0.6265248656272888 + 2.0 * 6.352670192718506
Epoch 480, val loss: 0.8658389449119568
Epoch 490, training loss: 13.288171768188477 = 0.5980905890464783 + 2.0 * 6.345040798187256
Epoch 490, val loss: 0.8469836115837097
Epoch 500, training loss: 13.254284858703613 = 0.5704532861709595 + 2.0 * 6.341915607452393
Epoch 500, val loss: 0.8291967511177063
Epoch 510, training loss: 13.220870971679688 = 0.5437221527099609 + 2.0 * 6.338574409484863
Epoch 510, val loss: 0.8126612305641174
Epoch 520, training loss: 13.190301895141602 = 0.5178731083869934 + 2.0 * 6.336214542388916
Epoch 520, val loss: 0.7974041700363159
Epoch 530, training loss: 13.16114330291748 = 0.4928778409957886 + 2.0 * 6.334132671356201
Epoch 530, val loss: 0.7833211421966553
Epoch 540, training loss: 13.154398918151855 = 0.4687589704990387 + 2.0 * 6.342820167541504
Epoch 540, val loss: 0.7703937888145447
Epoch 550, training loss: 13.11660099029541 = 0.44589340686798096 + 2.0 * 6.335353851318359
Epoch 550, val loss: 0.7586762309074402
Epoch 560, training loss: 13.085823059082031 = 0.42409276962280273 + 2.0 * 6.330865383148193
Epoch 560, val loss: 0.7481048703193665
Epoch 570, training loss: 13.06651782989502 = 0.403319388628006 + 2.0 * 6.331599235534668
Epoch 570, val loss: 0.7386442422866821
Epoch 580, training loss: 13.038132667541504 = 0.38353657722473145 + 2.0 * 6.327298164367676
Epoch 580, val loss: 0.7302614450454712
Epoch 590, training loss: 13.015573501586914 = 0.3647690713405609 + 2.0 * 6.32540225982666
Epoch 590, val loss: 0.7227382063865662
Epoch 600, training loss: 12.998257637023926 = 0.3469063341617584 + 2.0 * 6.3256754875183105
Epoch 600, val loss: 0.7160061001777649
Epoch 610, training loss: 12.973602294921875 = 0.32994508743286133 + 2.0 * 6.321828365325928
Epoch 610, val loss: 0.7101498246192932
Epoch 620, training loss: 12.963104248046875 = 0.3138015866279602 + 2.0 * 6.32465124130249
Epoch 620, val loss: 0.70488440990448
Epoch 630, training loss: 12.938069343566895 = 0.2984815537929535 + 2.0 * 6.319793701171875
Epoch 630, val loss: 0.7003517150878906
Epoch 640, training loss: 12.92049789428711 = 0.2838342785835266 + 2.0 * 6.318331718444824
Epoch 640, val loss: 0.6964265704154968
Epoch 650, training loss: 12.901457786560059 = 0.2698259651660919 + 2.0 * 6.3158159255981445
Epoch 650, val loss: 0.6930224299430847
Epoch 660, training loss: 12.884509086608887 = 0.25636255741119385 + 2.0 * 6.314073085784912
Epoch 660, val loss: 0.690144419670105
Epoch 670, training loss: 12.871587753295898 = 0.24344605207443237 + 2.0 * 6.314070701599121
Epoch 670, val loss: 0.6878106594085693
Epoch 680, training loss: 12.859341621398926 = 0.23103268444538116 + 2.0 * 6.314154624938965
Epoch 680, val loss: 0.6859604716300964
Epoch 690, training loss: 12.845330238342285 = 0.21917247772216797 + 2.0 * 6.313078880310059
Epoch 690, val loss: 0.6845493316650391
Epoch 700, training loss: 12.826578140258789 = 0.20781999826431274 + 2.0 * 6.3093791007995605
Epoch 700, val loss: 0.6836237907409668
Epoch 710, training loss: 12.80966567993164 = 0.19696110486984253 + 2.0 * 6.306352138519287
Epoch 710, val loss: 0.6832938194274902
Epoch 720, training loss: 12.797539710998535 = 0.18659628927707672 + 2.0 * 6.305471897125244
Epoch 720, val loss: 0.683562159538269
Epoch 730, training loss: 12.815539360046387 = 0.17677809298038483 + 2.0 * 6.319380760192871
Epoch 730, val loss: 0.6842889189720154
Epoch 740, training loss: 12.775752067565918 = 0.1675974279642105 + 2.0 * 6.3040771484375
Epoch 740, val loss: 0.6854171752929688
Epoch 750, training loss: 12.762744903564453 = 0.15901264548301697 + 2.0 * 6.301866054534912
Epoch 750, val loss: 0.6870777010917664
Epoch 760, training loss: 12.751935005187988 = 0.15097571909427643 + 2.0 * 6.300479412078857
Epoch 760, val loss: 0.6892687082290649
Epoch 770, training loss: 12.746282577514648 = 0.1434466689825058 + 2.0 * 6.301417827606201
Epoch 770, val loss: 0.6918802857398987
Epoch 780, training loss: 12.734353065490723 = 0.1364230364561081 + 2.0 * 6.298964977264404
Epoch 780, val loss: 0.6947776079177856
Epoch 790, training loss: 12.740276336669922 = 0.12989136576652527 + 2.0 * 6.305192470550537
Epoch 790, val loss: 0.6979987025260925
Epoch 800, training loss: 12.71911907196045 = 0.12380888313055038 + 2.0 * 6.29765510559082
Epoch 800, val loss: 0.7015246152877808
Epoch 810, training loss: 12.708559036254883 = 0.11814405769109726 + 2.0 * 6.295207500457764
Epoch 810, val loss: 0.7053338289260864
Epoch 820, training loss: 12.70259952545166 = 0.1128227710723877 + 2.0 * 6.294888496398926
Epoch 820, val loss: 0.7093687653541565
Epoch 830, training loss: 12.703173637390137 = 0.10783567279577255 + 2.0 * 6.297668933868408
Epoch 830, val loss: 0.7135289907455444
Epoch 840, training loss: 12.688559532165527 = 0.10317552834749222 + 2.0 * 6.292692184448242
Epoch 840, val loss: 0.7177923917770386
Epoch 850, training loss: 12.682465553283691 = 0.09879284352064133 + 2.0 * 6.291836261749268
Epoch 850, val loss: 0.7223027348518372
Epoch 860, training loss: 12.68114185333252 = 0.09465222805738449 + 2.0 * 6.2932448387146
Epoch 860, val loss: 0.726881206035614
Epoch 870, training loss: 12.672507286071777 = 0.0907592698931694 + 2.0 * 6.290874004364014
Epoch 870, val loss: 0.7315060496330261
Epoch 880, training loss: 12.67277717590332 = 0.08707650005817413 + 2.0 * 6.292850494384766
Epoch 880, val loss: 0.7362062931060791
Epoch 890, training loss: 12.6614408493042 = 0.08361579477787018 + 2.0 * 6.288912296295166
Epoch 890, val loss: 0.7409718036651611
Epoch 900, training loss: 12.653693199157715 = 0.08033890277147293 + 2.0 * 6.286677360534668
Epoch 900, val loss: 0.7458794116973877
Epoch 910, training loss: 12.652054786682129 = 0.0772223249077797 + 2.0 * 6.287416458129883
Epoch 910, val loss: 0.7508561015129089
Epoch 920, training loss: 12.646292686462402 = 0.07425682991743088 + 2.0 * 6.286017894744873
Epoch 920, val loss: 0.7558391094207764
Epoch 930, training loss: 12.642019271850586 = 0.07144172489643097 + 2.0 * 6.2852888107299805
Epoch 930, val loss: 0.7609107494354248
Epoch 940, training loss: 12.641127586364746 = 0.06877461075782776 + 2.0 * 6.286176681518555
Epoch 940, val loss: 0.7659947872161865
Epoch 950, training loss: 12.633273124694824 = 0.06623518466949463 + 2.0 * 6.2835187911987305
Epoch 950, val loss: 0.7711293697357178
Epoch 960, training loss: 12.630711555480957 = 0.06382592022418976 + 2.0 * 6.283442974090576
Epoch 960, val loss: 0.7763001322746277
Epoch 970, training loss: 12.62603759765625 = 0.061530862003564835 + 2.0 * 6.282253265380859
Epoch 970, val loss: 0.7814251184463501
Epoch 980, training loss: 12.622098922729492 = 0.05934559181332588 + 2.0 * 6.281376838684082
Epoch 980, val loss: 0.7865989208221436
Epoch 990, training loss: 12.625626564025879 = 0.05726192519068718 + 2.0 * 6.284182548522949
Epoch 990, val loss: 0.7917887568473816
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8185185185185185
0.8360569319978914
=== training gcn model ===
Epoch 0, training loss: 19.127477645874023 = 1.9339101314544678 + 2.0 * 8.596783638000488
Epoch 0, val loss: 1.9265204668045044
Epoch 10, training loss: 19.116558074951172 = 1.9239603281021118 + 2.0 * 8.596299171447754
Epoch 10, val loss: 1.9167407751083374
Epoch 20, training loss: 19.097288131713867 = 1.911556601524353 + 2.0 * 8.592865943908691
Epoch 20, val loss: 1.9041634798049927
Epoch 30, training loss: 19.0300235748291 = 1.8944569826126099 + 2.0 * 8.56778335571289
Epoch 30, val loss: 1.8866578340530396
Epoch 40, training loss: 18.613887786865234 = 1.8737883567810059 + 2.0 * 8.370049476623535
Epoch 40, val loss: 1.8667688369750977
Epoch 50, training loss: 16.94939422607422 = 1.8538315296173096 + 2.0 * 7.547781467437744
Epoch 50, val loss: 1.8483184576034546
Epoch 60, training loss: 16.148305892944336 = 1.8387635946273804 + 2.0 * 7.154771327972412
Epoch 60, val loss: 1.83323073387146
Epoch 70, training loss: 15.777108192443848 = 1.826768159866333 + 2.0 * 6.975170135498047
Epoch 70, val loss: 1.8214097023010254
Epoch 80, training loss: 15.535297393798828 = 1.8153445720672607 + 2.0 * 6.859976291656494
Epoch 80, val loss: 1.8104116916656494
Epoch 90, training loss: 15.372523307800293 = 1.80372154712677 + 2.0 * 6.784400939941406
Epoch 90, val loss: 1.7994916439056396
Epoch 100, training loss: 15.250853538513184 = 1.7917195558547974 + 2.0 * 6.729567050933838
Epoch 100, val loss: 1.7883145809173584
Epoch 110, training loss: 15.147789001464844 = 1.7794779539108276 + 2.0 * 6.684155464172363
Epoch 110, val loss: 1.77707839012146
Epoch 120, training loss: 15.058250427246094 = 1.7672736644744873 + 2.0 * 6.645488262176514
Epoch 120, val loss: 1.7659423351287842
Epoch 130, training loss: 14.97918701171875 = 1.754655361175537 + 2.0 * 6.612265586853027
Epoch 130, val loss: 1.7542625665664673
Epoch 140, training loss: 14.904071807861328 = 1.740883469581604 + 2.0 * 6.581593990325928
Epoch 140, val loss: 1.7415567636489868
Epoch 150, training loss: 14.837021827697754 = 1.7254858016967773 + 2.0 * 6.555768013000488
Epoch 150, val loss: 1.7274878025054932
Epoch 160, training loss: 14.773287773132324 = 1.7080961465835571 + 2.0 * 6.532595634460449
Epoch 160, val loss: 1.7115401029586792
Epoch 170, training loss: 14.71447467803955 = 1.6882189512252808 + 2.0 * 6.51312780380249
Epoch 170, val loss: 1.6933770179748535
Epoch 180, training loss: 14.66357707977295 = 1.6653894186019897 + 2.0 * 6.499094009399414
Epoch 180, val loss: 1.672582983970642
Epoch 190, training loss: 14.605624198913574 = 1.6394070386886597 + 2.0 * 6.4831085205078125
Epoch 190, val loss: 1.6490906476974487
Epoch 200, training loss: 14.550532341003418 = 1.6101444959640503 + 2.0 * 6.470193862915039
Epoch 200, val loss: 1.6226228475570679
Epoch 210, training loss: 14.497309684753418 = 1.577364206314087 + 2.0 * 6.459972858428955
Epoch 210, val loss: 1.5931485891342163
Epoch 220, training loss: 14.439340591430664 = 1.5410767793655396 + 2.0 * 6.449131965637207
Epoch 220, val loss: 1.5609409809112549
Epoch 230, training loss: 14.384222984313965 = 1.5018260478973389 + 2.0 * 6.441198348999023
Epoch 230, val loss: 1.5262805223464966
Epoch 240, training loss: 14.32431411743164 = 1.4602923393249512 + 2.0 * 6.432011127471924
Epoch 240, val loss: 1.4902435541152954
Epoch 250, training loss: 14.265228271484375 = 1.4176417589187622 + 2.0 * 6.423793315887451
Epoch 250, val loss: 1.4535048007965088
Epoch 260, training loss: 14.20817756652832 = 1.3744707107543945 + 2.0 * 6.416853427886963
Epoch 260, val loss: 1.4167557954788208
Epoch 270, training loss: 14.155763626098633 = 1.331435203552246 + 2.0 * 6.412164211273193
Epoch 270, val loss: 1.3806788921356201
Epoch 280, training loss: 14.102653503417969 = 1.289483904838562 + 2.0 * 6.406584739685059
Epoch 280, val loss: 1.3461008071899414
Epoch 290, training loss: 14.049201965332031 = 1.2490592002868652 + 2.0 * 6.400071620941162
Epoch 290, val loss: 1.3134541511535645
Epoch 300, training loss: 13.998245239257812 = 1.2099415063858032 + 2.0 * 6.39415168762207
Epoch 300, val loss: 1.2824842929840088
Epoch 310, training loss: 13.967850685119629 = 1.1721113920211792 + 2.0 * 6.39786958694458
Epoch 310, val loss: 1.2530462741851807
Epoch 320, training loss: 13.912503242492676 = 1.1357412338256836 + 2.0 * 6.388381004333496
Epoch 320, val loss: 1.2256698608398438
Epoch 330, training loss: 13.863347053527832 = 1.1006261110305786 + 2.0 * 6.3813605308532715
Epoch 330, val loss: 1.1996536254882812
Epoch 340, training loss: 13.821403503417969 = 1.0663429498672485 + 2.0 * 6.377530097961426
Epoch 340, val loss: 1.1746541261672974
Epoch 350, training loss: 13.783308029174805 = 1.032863974571228 + 2.0 * 6.375222206115723
Epoch 350, val loss: 1.1506366729736328
Epoch 360, training loss: 13.743345260620117 = 1.0004209280014038 + 2.0 * 6.371462345123291
Epoch 360, val loss: 1.1278598308563232
Epoch 370, training loss: 13.70305347442627 = 0.968625009059906 + 2.0 * 6.367214202880859
Epoch 370, val loss: 1.1058727502822876
Epoch 380, training loss: 13.664586067199707 = 0.9373342394828796 + 2.0 * 6.363626003265381
Epoch 380, val loss: 1.084546446800232
Epoch 390, training loss: 13.634363174438477 = 0.9065430760383606 + 2.0 * 6.36391019821167
Epoch 390, val loss: 1.0639601945877075
Epoch 400, training loss: 13.593758583068848 = 0.8765394687652588 + 2.0 * 6.358609676361084
Epoch 400, val loss: 1.0441346168518066
Epoch 410, training loss: 13.557279586791992 = 0.847091019153595 + 2.0 * 6.3550944328308105
Epoch 410, val loss: 1.025192141532898
Epoch 420, training loss: 13.529319763183594 = 0.8179904818534851 + 2.0 * 6.3556647300720215
Epoch 420, val loss: 1.0067980289459229
Epoch 430, training loss: 13.496158599853516 = 0.7894760370254517 + 2.0 * 6.353341102600098
Epoch 430, val loss: 0.988978385925293
Epoch 440, training loss: 13.457895278930664 = 0.7613885402679443 + 2.0 * 6.34825325012207
Epoch 440, val loss: 0.971827507019043
Epoch 450, training loss: 13.422216415405273 = 0.7336294651031494 + 2.0 * 6.344293594360352
Epoch 450, val loss: 0.9550946354866028
Epoch 460, training loss: 13.392585754394531 = 0.7060640454292297 + 2.0 * 6.343260765075684
Epoch 460, val loss: 0.9388478398323059
Epoch 470, training loss: 13.367050170898438 = 0.6788204312324524 + 2.0 * 6.344114780426025
Epoch 470, val loss: 0.9232168793678284
Epoch 480, training loss: 13.329092979431152 = 0.6520814299583435 + 2.0 * 6.338505744934082
Epoch 480, val loss: 0.908187210559845
Epoch 490, training loss: 13.299789428710938 = 0.6256428956985474 + 2.0 * 6.33707332611084
Epoch 490, val loss: 0.8937547206878662
Epoch 500, training loss: 13.271203994750977 = 0.5994820594787598 + 2.0 * 6.335860729217529
Epoch 500, val loss: 0.8799543380737305
Epoch 510, training loss: 13.239749908447266 = 0.5739017128944397 + 2.0 * 6.332923889160156
Epoch 510, val loss: 0.8670480251312256
Epoch 520, training loss: 13.210174560546875 = 0.548617959022522 + 2.0 * 6.330778121948242
Epoch 520, val loss: 0.854831337928772
Epoch 530, training loss: 13.18443489074707 = 0.5237183570861816 + 2.0 * 6.330358028411865
Epoch 530, val loss: 0.8433611989021301
Epoch 540, training loss: 13.163737297058105 = 0.49936211109161377 + 2.0 * 6.332187652587891
Epoch 540, val loss: 0.8329968452453613
Epoch 550, training loss: 13.129059791564941 = 0.47595587372779846 + 2.0 * 6.326551914215088
Epoch 550, val loss: 0.8237209916114807
Epoch 560, training loss: 13.111148834228516 = 0.45324328541755676 + 2.0 * 6.328952789306641
Epoch 560, val loss: 0.8154209852218628
Epoch 570, training loss: 13.082660675048828 = 0.4316212832927704 + 2.0 * 6.325519561767578
Epoch 570, val loss: 0.8084982633590698
Epoch 580, training loss: 13.052276611328125 = 0.4109736680984497 + 2.0 * 6.320651531219482
Epoch 580, val loss: 0.8028470873832703
Epoch 590, training loss: 13.033679008483887 = 0.3912690579891205 + 2.0 * 6.321205139160156
Epoch 590, val loss: 0.7982428073883057
Epoch 600, training loss: 13.011797904968262 = 0.37256643176078796 + 2.0 * 6.319615840911865
Epoch 600, val loss: 0.7948377132415771
Epoch 610, training loss: 12.992525100708008 = 0.3550044894218445 + 2.0 * 6.318760395050049
Epoch 610, val loss: 0.7926238775253296
Epoch 620, training loss: 12.969344139099121 = 0.33843177556991577 + 2.0 * 6.315456390380859
Epoch 620, val loss: 0.7914774417877197
Epoch 630, training loss: 12.950883865356445 = 0.32276806235313416 + 2.0 * 6.31405782699585
Epoch 630, val loss: 0.7912037968635559
Epoch 640, training loss: 12.94013786315918 = 0.307964563369751 + 2.0 * 6.316086769104004
Epoch 640, val loss: 0.7916622757911682
Epoch 650, training loss: 12.916301727294922 = 0.2939721643924713 + 2.0 * 6.311164855957031
Epoch 650, val loss: 0.7929215431213379
Epoch 660, training loss: 12.900802612304688 = 0.28081128001213074 + 2.0 * 6.309995651245117
Epoch 660, val loss: 0.7948315143585205
Epoch 670, training loss: 12.897087097167969 = 0.2683218717575073 + 2.0 * 6.314382553100586
Epoch 670, val loss: 0.7972788214683533
Epoch 680, training loss: 12.873787879943848 = 0.2565459609031677 + 2.0 * 6.308620929718018
Epoch 680, val loss: 0.8003455996513367
Epoch 690, training loss: 12.862459182739258 = 0.24531517922878265 + 2.0 * 6.308571815490723
Epoch 690, val loss: 0.8038065433502197
Epoch 700, training loss: 12.844819068908691 = 0.2347141057252884 + 2.0 * 6.305052280426025
Epoch 700, val loss: 0.8075606226921082
Epoch 710, training loss: 12.835295677185059 = 0.22461818158626556 + 2.0 * 6.3053388595581055
Epoch 710, val loss: 0.8117988705635071
Epoch 720, training loss: 12.822569847106934 = 0.21505285799503326 + 2.0 * 6.30375862121582
Epoch 720, val loss: 0.8162751793861389
Epoch 730, training loss: 12.808088302612305 = 0.20591406524181366 + 2.0 * 6.301086902618408
Epoch 730, val loss: 0.8210930824279785
Epoch 740, training loss: 12.799066543579102 = 0.1972140371799469 + 2.0 * 6.300926208496094
Epoch 740, val loss: 0.8260764479637146
Epoch 750, training loss: 12.78786849975586 = 0.1889367550611496 + 2.0 * 6.299465656280518
Epoch 750, val loss: 0.8313180208206177
Epoch 760, training loss: 12.785479545593262 = 0.1811113804578781 + 2.0 * 6.302184104919434
Epoch 760, val loss: 0.8368807435035706
Epoch 770, training loss: 12.7717924118042 = 0.17362718284130096 + 2.0 * 6.2990827560424805
Epoch 770, val loss: 0.8426041603088379
Epoch 780, training loss: 12.757390975952148 = 0.16653627157211304 + 2.0 * 6.295427322387695
Epoch 780, val loss: 0.8484941720962524
Epoch 790, training loss: 12.750238418579102 = 0.15974996984004974 + 2.0 * 6.295244216918945
Epoch 790, val loss: 0.8544976115226746
Epoch 800, training loss: 12.745765686035156 = 0.15329071879386902 + 2.0 * 6.296237468719482
Epoch 800, val loss: 0.8606727123260498
Epoch 810, training loss: 12.742003440856934 = 0.14718040823936462 + 2.0 * 6.2974114418029785
Epoch 810, val loss: 0.867134690284729
Epoch 820, training loss: 12.72575569152832 = 0.14130602777004242 + 2.0 * 6.292224884033203
Epoch 820, val loss: 0.8734479546546936
Epoch 830, training loss: 12.720902442932129 = 0.13575869798660278 + 2.0 * 6.292572021484375
Epoch 830, val loss: 0.8799627423286438
Epoch 840, training loss: 12.711570739746094 = 0.1304558664560318 + 2.0 * 6.290557384490967
Epoch 840, val loss: 0.8863723278045654
Epoch 850, training loss: 12.705998420715332 = 0.12543055415153503 + 2.0 * 6.290284156799316
Epoch 850, val loss: 0.8931827545166016
Epoch 860, training loss: 12.698633193969727 = 0.12062609940767288 + 2.0 * 6.289003372192383
Epoch 860, val loss: 0.8997608423233032
Epoch 870, training loss: 12.691914558410645 = 0.11604905128479004 + 2.0 * 6.287932872772217
Epoch 870, val loss: 0.9065529704093933
Epoch 880, training loss: 12.690153121948242 = 0.11167774349451065 + 2.0 * 6.2892374992370605
Epoch 880, val loss: 0.9133480787277222
Epoch 890, training loss: 12.682112693786621 = 0.10751911997795105 + 2.0 * 6.287296772003174
Epoch 890, val loss: 0.9202417731285095
Epoch 900, training loss: 12.676876068115234 = 0.10354331135749817 + 2.0 * 6.286666393280029
Epoch 900, val loss: 0.9269878268241882
Epoch 910, training loss: 12.67050838470459 = 0.09976966679096222 + 2.0 * 6.285369396209717
Epoch 910, val loss: 0.9338576793670654
Epoch 920, training loss: 12.66215991973877 = 0.09614704549312592 + 2.0 * 6.28300666809082
Epoch 920, val loss: 0.9406772255897522
Epoch 930, training loss: 12.663589477539062 = 0.09269420057535172 + 2.0 * 6.285447597503662
Epoch 930, val loss: 0.9474321007728577
Epoch 940, training loss: 12.65273380279541 = 0.0894174873828888 + 2.0 * 6.281658172607422
Epoch 940, val loss: 0.9544113874435425
Epoch 950, training loss: 12.647329330444336 = 0.08627186715602875 + 2.0 * 6.280528545379639
Epoch 950, val loss: 0.961323618888855
Epoch 960, training loss: 12.650979995727539 = 0.08326154202222824 + 2.0 * 6.2838592529296875
Epoch 960, val loss: 0.9680693745613098
Epoch 970, training loss: 12.645648956298828 = 0.08039993792772293 + 2.0 * 6.2826247215271
Epoch 970, val loss: 0.9748557806015015
Epoch 980, training loss: 12.638426780700684 = 0.0776829645037651 + 2.0 * 6.280372142791748
Epoch 980, val loss: 0.9817763566970825
Epoch 990, training loss: 12.62865924835205 = 0.07507435977458954 + 2.0 * 6.276792526245117
Epoch 990, val loss: 0.9884660243988037
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8037037037037037
0.8408012651555088
The final CL Acc:0.80370, 0.01210, The final GNN Acc:0.83764, 0.00224
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.4 der2:0.4 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11606])
remove edge: torch.Size([2, 9466])
updated graph: torch.Size([2, 10516])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 19.140663146972656 = 1.946964144706726 + 2.0 * 8.59684944152832
Epoch 0, val loss: 1.943455696105957
Epoch 10, training loss: 19.129444122314453 = 1.9363621473312378 + 2.0 * 8.596541404724121
Epoch 10, val loss: 1.9329627752304077
Epoch 20, training loss: 19.111122131347656 = 1.9226256608963013 + 2.0 * 8.594247817993164
Epoch 20, val loss: 1.9192814826965332
Epoch 30, training loss: 19.058290481567383 = 1.9032022953033447 + 2.0 * 8.577544212341309
Epoch 30, val loss: 1.8999489545822144
Epoch 40, training loss: 18.83391761779785 = 1.878504991531372 + 2.0 * 8.477705955505371
Epoch 40, val loss: 1.8767304420471191
Epoch 50, training loss: 18.07600212097168 = 1.8516674041748047 + 2.0 * 8.112167358398438
Epoch 50, val loss: 1.8528680801391602
Epoch 60, training loss: 17.34256935119629 = 1.8315430879592896 + 2.0 * 7.7555131912231445
Epoch 60, val loss: 1.8353922367095947
Epoch 70, training loss: 16.620576858520508 = 1.8186460733413696 + 2.0 * 7.400965213775635
Epoch 70, val loss: 1.8235037326812744
Epoch 80, training loss: 16.144046783447266 = 1.8078185319900513 + 2.0 * 7.168113708496094
Epoch 80, val loss: 1.8130989074707031
Epoch 90, training loss: 15.742626190185547 = 1.7963836193084717 + 2.0 * 6.973121166229248
Epoch 90, val loss: 1.8028401136398315
Epoch 100, training loss: 15.506440162658691 = 1.784912347793579 + 2.0 * 6.860764026641846
Epoch 100, val loss: 1.7931665182113647
Epoch 110, training loss: 15.329328536987305 = 1.7733229398727417 + 2.0 * 6.778002738952637
Epoch 110, val loss: 1.7832026481628418
Epoch 120, training loss: 15.19531536102295 = 1.7619903087615967 + 2.0 * 6.716662406921387
Epoch 120, val loss: 1.772595763206482
Epoch 130, training loss: 15.099040985107422 = 1.7500249147415161 + 2.0 * 6.674508094787598
Epoch 130, val loss: 1.7610538005828857
Epoch 140, training loss: 15.011489868164062 = 1.7366819381713867 + 2.0 * 6.637403964996338
Epoch 140, val loss: 1.7487105131149292
Epoch 150, training loss: 14.939846992492676 = 1.7218897342681885 + 2.0 * 6.608978748321533
Epoch 150, val loss: 1.7355307340621948
Epoch 160, training loss: 14.87963581085205 = 1.7052971124649048 + 2.0 * 6.587169170379639
Epoch 160, val loss: 1.7211157083511353
Epoch 170, training loss: 14.821015357971191 = 1.686775803565979 + 2.0 * 6.567119598388672
Epoch 170, val loss: 1.7052040100097656
Epoch 180, training loss: 14.766307830810547 = 1.666266679763794 + 2.0 * 6.550020694732666
Epoch 180, val loss: 1.68770432472229
Epoch 190, training loss: 14.712709426879883 = 1.6434993743896484 + 2.0 * 6.534605026245117
Epoch 190, val loss: 1.6684211492538452
Epoch 200, training loss: 14.660099983215332 = 1.6182767152786255 + 2.0 * 6.520911693572998
Epoch 200, val loss: 1.6473252773284912
Epoch 210, training loss: 14.607501029968262 = 1.5906150341033936 + 2.0 * 6.5084428787231445
Epoch 210, val loss: 1.6242903470993042
Epoch 220, training loss: 14.559778213500977 = 1.5604910850524902 + 2.0 * 6.499643802642822
Epoch 220, val loss: 1.5993906259536743
Epoch 230, training loss: 14.502665519714355 = 1.5281115770339966 + 2.0 * 6.487277030944824
Epoch 230, val loss: 1.5727893114089966
Epoch 240, training loss: 14.449198722839355 = 1.4935544729232788 + 2.0 * 6.477822303771973
Epoch 240, val loss: 1.5446377992630005
Epoch 250, training loss: 14.39594841003418 = 1.4569454193115234 + 2.0 * 6.469501495361328
Epoch 250, val loss: 1.5150188207626343
Epoch 260, training loss: 14.343538284301758 = 1.4186201095581055 + 2.0 * 6.462459087371826
Epoch 260, val loss: 1.4844764471054077
Epoch 270, training loss: 14.292134284973145 = 1.3790785074234009 + 2.0 * 6.4565277099609375
Epoch 270, val loss: 1.4530123472213745
Epoch 280, training loss: 14.233217239379883 = 1.3382974863052368 + 2.0 * 6.447459697723389
Epoch 280, val loss: 1.4209578037261963
Epoch 290, training loss: 14.178326606750488 = 1.2963221073150635 + 2.0 * 6.441002368927002
Epoch 290, val loss: 1.3883426189422607
Epoch 300, training loss: 14.125072479248047 = 1.2533248662948608 + 2.0 * 6.435873985290527
Epoch 300, val loss: 1.355218529701233
Epoch 310, training loss: 14.07435417175293 = 1.2095789909362793 + 2.0 * 6.432387828826904
Epoch 310, val loss: 1.32181715965271
Epoch 320, training loss: 14.01280403137207 = 1.1653976440429688 + 2.0 * 6.423703193664551
Epoch 320, val loss: 1.2883588075637817
Epoch 330, training loss: 13.958478927612305 = 1.120678424835205 + 2.0 * 6.418900489807129
Epoch 330, val loss: 1.254878044128418
Epoch 340, training loss: 13.905471801757812 = 1.0757511854171753 + 2.0 * 6.414860248565674
Epoch 340, val loss: 1.2217864990234375
Epoch 350, training loss: 13.852899551391602 = 1.031280755996704 + 2.0 * 6.410809516906738
Epoch 350, val loss: 1.1895263195037842
Epoch 360, training loss: 13.798863410949707 = 0.9874221086502075 + 2.0 * 6.4057207107543945
Epoch 360, val loss: 1.1582108736038208
Epoch 370, training loss: 13.747598648071289 = 0.9444811940193176 + 2.0 * 6.401558876037598
Epoch 370, val loss: 1.1281734704971313
Epoch 380, training loss: 13.707937240600586 = 0.9031333923339844 + 2.0 * 6.402401924133301
Epoch 380, val loss: 1.0999139547348022
Epoch 390, training loss: 13.65541934967041 = 0.8638458847999573 + 2.0 * 6.395786762237549
Epoch 390, val loss: 1.0739123821258545
Epoch 400, training loss: 13.606807708740234 = 0.8265249133110046 + 2.0 * 6.390141487121582
Epoch 400, val loss: 1.050121784210205
Epoch 410, training loss: 13.569348335266113 = 0.7911914587020874 + 2.0 * 6.389078617095947
Epoch 410, val loss: 1.0286951065063477
Epoch 420, training loss: 13.530891418457031 = 0.758391261100769 + 2.0 * 6.386250019073486
Epoch 420, val loss: 1.0094331502914429
Epoch 430, training loss: 13.487956047058105 = 0.727710485458374 + 2.0 * 6.380122661590576
Epoch 430, val loss: 0.9926122426986694
Epoch 440, training loss: 13.45737361907959 = 0.6989416480064392 + 2.0 * 6.379216194152832
Epoch 440, val loss: 0.9776670336723328
Epoch 450, training loss: 13.426888465881348 = 0.6719719767570496 + 2.0 * 6.377458095550537
Epoch 450, val loss: 0.9646251797676086
Epoch 460, training loss: 13.389147758483887 = 0.6467204689979553 + 2.0 * 6.371213436126709
Epoch 460, val loss: 0.953315794467926
Epoch 470, training loss: 13.357450485229492 = 0.6227326393127441 + 2.0 * 6.367359161376953
Epoch 470, val loss: 0.9435046911239624
Epoch 480, training loss: 13.328398704528809 = 0.5998575687408447 + 2.0 * 6.3642706871032715
Epoch 480, val loss: 0.9349801540374756
Epoch 490, training loss: 13.323830604553223 = 0.577933669090271 + 2.0 * 6.37294864654541
Epoch 490, val loss: 0.9276679158210754
Epoch 500, training loss: 13.283706665039062 = 0.5570340752601624 + 2.0 * 6.363336086273193
Epoch 500, val loss: 0.9215318560600281
Epoch 510, training loss: 13.252537727355957 = 0.5371503829956055 + 2.0 * 6.357693672180176
Epoch 510, val loss: 0.9164259433746338
Epoch 520, training loss: 13.22713851928711 = 0.5179614424705505 + 2.0 * 6.354588508605957
Epoch 520, val loss: 0.912322461605072
Epoch 530, training loss: 13.209969520568848 = 0.4994162619113922 + 2.0 * 6.355276584625244
Epoch 530, val loss: 0.9090996980667114
Epoch 540, training loss: 13.194384574890137 = 0.4815375506877899 + 2.0 * 6.356423377990723
Epoch 540, val loss: 0.9067490100860596
Epoch 550, training loss: 13.162773132324219 = 0.4642607867717743 + 2.0 * 6.3492560386657715
Epoch 550, val loss: 0.9051843881607056
Epoch 560, training loss: 13.140273094177246 = 0.44760042428970337 + 2.0 * 6.346336364746094
Epoch 560, val loss: 0.9044054746627808
Epoch 570, training loss: 13.121082305908203 = 0.4314185678958893 + 2.0 * 6.344831943511963
Epoch 570, val loss: 0.9042754173278809
Epoch 580, training loss: 13.107584953308105 = 0.4156632125377655 + 2.0 * 6.345961093902588
Epoch 580, val loss: 0.9047476649284363
Epoch 590, training loss: 13.085421562194824 = 0.4004516899585724 + 2.0 * 6.342484951019287
Epoch 590, val loss: 0.9057954549789429
Epoch 600, training loss: 13.065316200256348 = 0.3857521414756775 + 2.0 * 6.339782238006592
Epoch 600, val loss: 0.9075208306312561
Epoch 610, training loss: 13.044901847839355 = 0.37148410081863403 + 2.0 * 6.336709022521973
Epoch 610, val loss: 0.9098150134086609
Epoch 620, training loss: 13.0274076461792 = 0.3576125204563141 + 2.0 * 6.334897518157959
Epoch 620, val loss: 0.912582516670227
Epoch 630, training loss: 13.023601531982422 = 0.34411942958831787 + 2.0 * 6.339741230010986
Epoch 630, val loss: 0.9158411622047424
Epoch 640, training loss: 13.001498222351074 = 0.3310399651527405 + 2.0 * 6.33522891998291
Epoch 640, val loss: 0.9195241928100586
Epoch 650, training loss: 12.98624038696289 = 0.31842240691185 + 2.0 * 6.333909034729004
Epoch 650, val loss: 0.923667848110199
Epoch 660, training loss: 12.966910362243652 = 0.30628976225852966 + 2.0 * 6.330310344696045
Epoch 660, val loss: 0.9282460808753967
Epoch 670, training loss: 12.950638771057129 = 0.29453983902931213 + 2.0 * 6.328049659729004
Epoch 670, val loss: 0.9332137107849121
Epoch 680, training loss: 12.936040878295898 = 0.2831314206123352 + 2.0 * 6.3264546394348145
Epoch 680, val loss: 0.9385367631912231
Epoch 690, training loss: 12.931976318359375 = 0.27210330963134766 + 2.0 * 6.329936504364014
Epoch 690, val loss: 0.9441724419593811
Epoch 700, training loss: 12.91159725189209 = 0.2614723742008209 + 2.0 * 6.325062274932861
Epoch 700, val loss: 0.9500368237495422
Epoch 710, training loss: 12.896500587463379 = 0.2512211203575134 + 2.0 * 6.3226399421691895
Epoch 710, val loss: 0.9561708569526672
Epoch 720, training loss: 12.883444786071777 = 0.2413167953491211 + 2.0 * 6.321063995361328
Epoch 720, val loss: 0.9626409411430359
Epoch 730, training loss: 12.89216136932373 = 0.23174837231636047 + 2.0 * 6.330206394195557
Epoch 730, val loss: 0.9693821668624878
Epoch 740, training loss: 12.861733436584473 = 0.22250333428382874 + 2.0 * 6.319614887237549
Epoch 740, val loss: 0.9762636423110962
Epoch 750, training loss: 12.847576141357422 = 0.21363404393196106 + 2.0 * 6.3169708251953125
Epoch 750, val loss: 0.9834792613983154
Epoch 760, training loss: 12.839165687561035 = 0.20507150888442993 + 2.0 * 6.317047119140625
Epoch 760, val loss: 0.9908627867698669
Epoch 770, training loss: 12.831918716430664 = 0.1968121975660324 + 2.0 * 6.3175530433654785
Epoch 770, val loss: 0.9983466267585754
Epoch 780, training loss: 12.81778335571289 = 0.18887127935886383 + 2.0 * 6.314455986022949
Epoch 780, val loss: 1.0059998035430908
Epoch 790, training loss: 12.805814743041992 = 0.1812509149312973 + 2.0 * 6.312282085418701
Epoch 790, val loss: 1.0137808322906494
Epoch 800, training loss: 12.808218955993652 = 0.17390073835849762 + 2.0 * 6.317159175872803
Epoch 800, val loss: 1.021682858467102
Epoch 810, training loss: 12.789488792419434 = 0.16687466204166412 + 2.0 * 6.311306953430176
Epoch 810, val loss: 1.0297002792358398
Epoch 820, training loss: 12.779242515563965 = 0.16010499000549316 + 2.0 * 6.309568881988525
Epoch 820, val loss: 1.0377825498580933
Epoch 830, training loss: 12.778667449951172 = 0.1536303609609604 + 2.0 * 6.31251859664917
Epoch 830, val loss: 1.0460551977157593
Epoch 840, training loss: 12.765983581542969 = 0.14740879833698273 + 2.0 * 6.3092875480651855
Epoch 840, val loss: 1.0542590618133545
Epoch 850, training loss: 12.756002426147461 = 0.1414734274148941 + 2.0 * 6.30726432800293
Epoch 850, val loss: 1.0625829696655273
Epoch 860, training loss: 12.764639854431152 = 0.13578914105892181 + 2.0 * 6.314425468444824
Epoch 860, val loss: 1.0709095001220703
Epoch 870, training loss: 12.744811058044434 = 0.13033364713191986 + 2.0 * 6.307238578796387
Epoch 870, val loss: 1.0791070461273193
Epoch 880, training loss: 12.730916976928711 = 0.12514039874076843 + 2.0 * 6.3028883934021
Epoch 880, val loss: 1.0874600410461426
Epoch 890, training loss: 12.724462509155273 = 0.12016738951206207 + 2.0 * 6.302147388458252
Epoch 890, val loss: 1.0958194732666016
Epoch 900, training loss: 12.72744369506836 = 0.11539425700902939 + 2.0 * 6.306024551391602
Epoch 900, val loss: 1.104155421257019
Epoch 910, training loss: 12.716650009155273 = 0.11084898561239243 + 2.0 * 6.302900314331055
Epoch 910, val loss: 1.1124824285507202
Epoch 920, training loss: 12.709400177001953 = 0.10650673508644104 + 2.0 * 6.301446914672852
Epoch 920, val loss: 1.1207815408706665
Epoch 930, training loss: 12.70154094696045 = 0.10237026959657669 + 2.0 * 6.299585342407227
Epoch 930, val loss: 1.129118800163269
Epoch 940, training loss: 12.698131561279297 = 0.09840371459722519 + 2.0 * 6.299863815307617
Epoch 940, val loss: 1.13743257522583
Epoch 950, training loss: 12.694832801818848 = 0.09461820870637894 + 2.0 * 6.300107479095459
Epoch 950, val loss: 1.1456019878387451
Epoch 960, training loss: 12.693796157836914 = 0.0909847542643547 + 2.0 * 6.301405906677246
Epoch 960, val loss: 1.1536831855773926
Epoch 970, training loss: 12.680234909057617 = 0.08754727244377136 + 2.0 * 6.296343803405762
Epoch 970, val loss: 1.1617703437805176
Epoch 980, training loss: 12.673341751098633 = 0.0842398852109909 + 2.0 * 6.294550895690918
Epoch 980, val loss: 1.169756293296814
Epoch 990, training loss: 12.673486709594727 = 0.08108974993228912 + 2.0 * 6.29619836807251
Epoch 990, val loss: 1.177703857421875
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7481481481481482
0.8033737480231946
=== training gcn model ===
Epoch 0, training loss: 19.133264541625977 = 1.9395709037780762 + 2.0 * 8.596846580505371
Epoch 0, val loss: 1.9356729984283447
Epoch 10, training loss: 19.12334632873535 = 1.9301960468292236 + 2.0 * 8.596574783325195
Epoch 10, val loss: 1.926463007926941
Epoch 20, training loss: 19.10741424560547 = 1.9185057878494263 + 2.0 * 8.594453811645508
Epoch 20, val loss: 1.9145535230636597
Epoch 30, training loss: 19.057924270629883 = 1.9026434421539307 + 2.0 * 8.577640533447266
Epoch 30, val loss: 1.8980351686477661
Epoch 40, training loss: 18.792762756347656 = 1.883094310760498 + 2.0 * 8.454833984375
Epoch 40, val loss: 1.8782483339309692
Epoch 50, training loss: 17.977689743041992 = 1.8612091541290283 + 2.0 * 8.058239936828613
Epoch 50, val loss: 1.8567686080932617
Epoch 60, training loss: 17.161771774291992 = 1.8439003229141235 + 2.0 * 7.658935546875
Epoch 60, val loss: 1.8412166833877563
Epoch 70, training loss: 16.512557983398438 = 1.8327196836471558 + 2.0 * 7.339919567108154
Epoch 70, val loss: 1.830704927444458
Epoch 80, training loss: 16.025270462036133 = 1.819893717765808 + 2.0 * 7.102688789367676
Epoch 80, val loss: 1.8182156085968018
Epoch 90, training loss: 15.723811149597168 = 1.8076096773147583 + 2.0 * 6.95810079574585
Epoch 90, val loss: 1.8071876764297485
Epoch 100, training loss: 15.493513107299805 = 1.794785976409912 + 2.0 * 6.849363803863525
Epoch 100, val loss: 1.796079158782959
Epoch 110, training loss: 15.340249061584473 = 1.7818142175674438 + 2.0 * 6.77921724319458
Epoch 110, val loss: 1.785025954246521
Epoch 120, training loss: 15.215444564819336 = 1.7683528661727905 + 2.0 * 6.723546028137207
Epoch 120, val loss: 1.7732938528060913
Epoch 130, training loss: 15.115851402282715 = 1.7541310787200928 + 2.0 * 6.6808600425720215
Epoch 130, val loss: 1.761103630065918
Epoch 140, training loss: 15.030522346496582 = 1.7386653423309326 + 2.0 * 6.645928382873535
Epoch 140, val loss: 1.7480684518814087
Epoch 150, training loss: 14.966166496276855 = 1.7215934991836548 + 2.0 * 6.622286319732666
Epoch 150, val loss: 1.733906626701355
Epoch 160, training loss: 14.890835762023926 = 1.7025601863861084 + 2.0 * 6.594137668609619
Epoch 160, val loss: 1.718396782875061
Epoch 170, training loss: 14.829729080200195 = 1.681576132774353 + 2.0 * 6.5740766525268555
Epoch 170, val loss: 1.70131254196167
Epoch 180, training loss: 14.770997047424316 = 1.6583017110824585 + 2.0 * 6.556347846984863
Epoch 180, val loss: 1.682439923286438
Epoch 190, training loss: 14.727535247802734 = 1.6324963569641113 + 2.0 * 6.547519683837891
Epoch 190, val loss: 1.6616348028182983
Epoch 200, training loss: 14.664297103881836 = 1.604236125946045 + 2.0 * 6.530030250549316
Epoch 200, val loss: 1.6389219760894775
Epoch 210, training loss: 14.607226371765137 = 1.5737076997756958 + 2.0 * 6.516759395599365
Epoch 210, val loss: 1.6144059896469116
Epoch 220, training loss: 14.553621292114258 = 1.5407530069351196 + 2.0 * 6.506433963775635
Epoch 220, val loss: 1.588151454925537
Epoch 230, training loss: 14.500215530395508 = 1.5055238008499146 + 2.0 * 6.497345924377441
Epoch 230, val loss: 1.5601861476898193
Epoch 240, training loss: 14.441288948059082 = 1.468485713005066 + 2.0 * 6.486401557922363
Epoch 240, val loss: 1.5308763980865479
Epoch 250, training loss: 14.38320255279541 = 1.4297027587890625 + 2.0 * 6.476749897003174
Epoch 250, val loss: 1.500430703163147
Epoch 260, training loss: 14.33694839477539 = 1.3897264003753662 + 2.0 * 6.473610877990723
Epoch 260, val loss: 1.4692554473876953
Epoch 270, training loss: 14.27286148071289 = 1.3490864038467407 + 2.0 * 6.461887359619141
Epoch 270, val loss: 1.437747836112976
Epoch 280, training loss: 14.214340209960938 = 1.3077023029327393 + 2.0 * 6.453319072723389
Epoch 280, val loss: 1.4058948755264282
Epoch 290, training loss: 14.16757869720459 = 1.2657051086425781 + 2.0 * 6.450936794281006
Epoch 290, val loss: 1.3735867738723755
Epoch 300, training loss: 14.10669231414795 = 1.2237873077392578 + 2.0 * 6.441452503204346
Epoch 300, val loss: 1.3414615392684937
Epoch 310, training loss: 14.052718162536621 = 1.181984782218933 + 2.0 * 6.435366630554199
Epoch 310, val loss: 1.3094244003295898
Epoch 320, training loss: 14.0116548538208 = 1.1402899026870728 + 2.0 * 6.43568229675293
Epoch 320, val loss: 1.2775410413742065
Epoch 330, training loss: 13.948076248168945 = 1.0990703105926514 + 2.0 * 6.424502849578857
Epoch 330, val loss: 1.2462483644485474
Epoch 340, training loss: 13.89808464050293 = 1.0583187341690063 + 2.0 * 6.419882774353027
Epoch 340, val loss: 1.215185523033142
Epoch 350, training loss: 13.875380516052246 = 1.01799476146698 + 2.0 * 6.428692817687988
Epoch 350, val loss: 1.1845988035202026
Epoch 360, training loss: 13.80501651763916 = 0.978695809841156 + 2.0 * 6.41316032409668
Epoch 360, val loss: 1.1548495292663574
Epoch 370, training loss: 13.754034042358398 = 0.9402129650115967 + 2.0 * 6.406910419464111
Epoch 370, val loss: 1.125863790512085
Epoch 380, training loss: 13.706480026245117 = 0.902585506439209 + 2.0 * 6.401947021484375
Epoch 380, val loss: 1.0978840589523315
Epoch 390, training loss: 13.694193840026855 = 0.8659101128578186 + 2.0 * 6.414141654968262
Epoch 390, val loss: 1.0709683895111084
Epoch 400, training loss: 13.62985610961914 = 0.8308326601982117 + 2.0 * 6.399511814117432
Epoch 400, val loss: 1.0455589294433594
Epoch 410, training loss: 13.58015251159668 = 0.7972530126571655 + 2.0 * 6.391449928283691
Epoch 410, val loss: 1.0218391418457031
Epoch 420, training loss: 13.539143562316895 = 0.7650459408760071 + 2.0 * 6.387048721313477
Epoch 420, val loss: 0.9996569752693176
Epoch 430, training loss: 13.5018949508667 = 0.7341272830963135 + 2.0 * 6.383883953094482
Epoch 430, val loss: 0.9789376854896545
Epoch 440, training loss: 13.47004508972168 = 0.704562246799469 + 2.0 * 6.382741451263428
Epoch 440, val loss: 0.9597938656806946
Epoch 450, training loss: 13.43478012084961 = 0.6765194535255432 + 2.0 * 6.3791303634643555
Epoch 450, val loss: 0.9421392679214478
Epoch 460, training loss: 13.40350341796875 = 0.6498125791549683 + 2.0 * 6.376845359802246
Epoch 460, val loss: 0.925937294960022
Epoch 470, training loss: 13.3692045211792 = 0.6241934895515442 + 2.0 * 6.3725056648254395
Epoch 470, val loss: 0.9109686613082886
Epoch 480, training loss: 13.356328010559082 = 0.5995809435844421 + 2.0 * 6.378373622894287
Epoch 480, val loss: 0.8972203731536865
Epoch 490, training loss: 13.316889762878418 = 0.5759515166282654 + 2.0 * 6.370469093322754
Epoch 490, val loss: 0.8844103813171387
Epoch 500, training loss: 13.285734176635742 = 0.5534072518348694 + 2.0 * 6.36616325378418
Epoch 500, val loss: 0.8727554678916931
Epoch 510, training loss: 13.25788688659668 = 0.5317426919937134 + 2.0 * 6.363071918487549
Epoch 510, val loss: 0.8620894551277161
Epoch 520, training loss: 13.23143196105957 = 0.5108737945556641 + 2.0 * 6.360279083251953
Epoch 520, val loss: 0.8522998094558716
Epoch 530, training loss: 13.20626449584961 = 0.49081358313560486 + 2.0 * 6.357725620269775
Epoch 530, val loss: 0.8432705402374268
Epoch 540, training loss: 13.181671142578125 = 0.4714246690273285 + 2.0 * 6.355123043060303
Epoch 540, val loss: 0.8350825905799866
Epoch 550, training loss: 13.159481048583984 = 0.45261895656585693 + 2.0 * 6.353431224822998
Epoch 550, val loss: 0.8275482654571533
Epoch 560, training loss: 13.13928508758545 = 0.4343864321708679 + 2.0 * 6.352449417114258
Epoch 560, val loss: 0.8207074403762817
Epoch 570, training loss: 13.11478042602539 = 0.4167545437812805 + 2.0 * 6.349012851715088
Epoch 570, val loss: 0.8144288063049316
Epoch 580, training loss: 13.099809646606445 = 0.39966192841529846 + 2.0 * 6.35007381439209
Epoch 580, val loss: 0.8087413907051086
Epoch 590, training loss: 13.076818466186523 = 0.383035272359848 + 2.0 * 6.346891403198242
Epoch 590, val loss: 0.8036685585975647
Epoch 600, training loss: 13.054978370666504 = 0.36683881282806396 + 2.0 * 6.344069957733154
Epoch 600, val loss: 0.7990264296531677
Epoch 610, training loss: 13.033408164978027 = 0.3510439991950989 + 2.0 * 6.341182231903076
Epoch 610, val loss: 0.7949187159538269
Epoch 620, training loss: 13.017075538635254 = 0.3356025815010071 + 2.0 * 6.340736389160156
Epoch 620, val loss: 0.7912731766700745
Epoch 630, training loss: 13.008615493774414 = 0.32046225666999817 + 2.0 * 6.344076633453369
Epoch 630, val loss: 0.7881026864051819
Epoch 640, training loss: 12.983258247375488 = 0.30571427941322327 + 2.0 * 6.338771820068359
Epoch 640, val loss: 0.785100519657135
Epoch 650, training loss: 12.96275806427002 = 0.29129844903945923 + 2.0 * 6.335729598999023
Epoch 650, val loss: 0.7828112840652466
Epoch 660, training loss: 12.942571640014648 = 0.2772270739078522 + 2.0 * 6.332672119140625
Epoch 660, val loss: 0.7809017896652222
Epoch 670, training loss: 12.951337814331055 = 0.26349496841430664 + 2.0 * 6.343921661376953
Epoch 670, val loss: 0.7794332504272461
Epoch 680, training loss: 12.91396713256836 = 0.2501624524593353 + 2.0 * 6.331902503967285
Epoch 680, val loss: 0.7784581780433655
Epoch 690, training loss: 12.89682388305664 = 0.2373625636100769 + 2.0 * 6.32973051071167
Epoch 690, val loss: 0.77797532081604
Epoch 700, training loss: 12.878060340881348 = 0.22504408657550812 + 2.0 * 6.326508045196533
Epoch 700, val loss: 0.7780840396881104
Epoch 710, training loss: 12.898335456848145 = 0.21327008306980133 + 2.0 * 6.342532634735107
Epoch 710, val loss: 0.7787601351737976
Epoch 720, training loss: 12.856101036071777 = 0.20204225182533264 + 2.0 * 6.327029228210449
Epoch 720, val loss: 0.7798926830291748
Epoch 730, training loss: 12.83881664276123 = 0.19145724177360535 + 2.0 * 6.3236799240112305
Epoch 730, val loss: 0.7817039489746094
Epoch 740, training loss: 12.824771881103516 = 0.18145273625850677 + 2.0 * 6.321659564971924
Epoch 740, val loss: 0.7841029167175293
Epoch 750, training loss: 12.812250137329102 = 0.17196831107139587 + 2.0 * 6.320140838623047
Epoch 750, val loss: 0.7869975566864014
Epoch 760, training loss: 12.828534126281738 = 0.16300465166568756 + 2.0 * 6.332764625549316
Epoch 760, val loss: 0.7904316186904907
Epoch 770, training loss: 12.797969818115234 = 0.15459522604942322 + 2.0 * 6.3216872215271
Epoch 770, val loss: 0.7942155599594116
Epoch 780, training loss: 12.780920028686523 = 0.14670534431934357 + 2.0 * 6.317107200622559
Epoch 780, val loss: 0.7984949350357056
Epoch 790, training loss: 12.769881248474121 = 0.1393001824617386 + 2.0 * 6.315290451049805
Epoch 790, val loss: 0.8032286763191223
Epoch 800, training loss: 12.760940551757812 = 0.13232584297657013 + 2.0 * 6.31430721282959
Epoch 800, val loss: 0.8082907199859619
Epoch 810, training loss: 12.771791458129883 = 0.12574617564678192 + 2.0 * 6.323022842407227
Epoch 810, val loss: 0.8135122656822205
Epoch 820, training loss: 12.751626014709473 = 0.11957862973213196 + 2.0 * 6.316023826599121
Epoch 820, val loss: 0.8188593983650208
Epoch 830, training loss: 12.735548973083496 = 0.11377716064453125 + 2.0 * 6.310885906219482
Epoch 830, val loss: 0.8245708346366882
Epoch 840, training loss: 12.727140426635742 = 0.10832589119672775 + 2.0 * 6.3094072341918945
Epoch 840, val loss: 0.8303827047348022
Epoch 850, training loss: 12.755159378051758 = 0.10320325195789337 + 2.0 * 6.3259782791137695
Epoch 850, val loss: 0.8364076018333435
Epoch 860, training loss: 12.717869758605957 = 0.09835751354694366 + 2.0 * 6.309756278991699
Epoch 860, val loss: 0.842327892780304
Epoch 870, training loss: 12.708552360534668 = 0.09385444968938828 + 2.0 * 6.307348728179932
Epoch 870, val loss: 0.8484988212585449
Epoch 880, training loss: 12.70058822631836 = 0.0896034985780716 + 2.0 * 6.305492401123047
Epoch 880, val loss: 0.8548086881637573
Epoch 890, training loss: 12.7069091796875 = 0.08559025824069977 + 2.0 * 6.310659408569336
Epoch 890, val loss: 0.8612000942230225
Epoch 900, training loss: 12.69372844696045 = 0.08180046081542969 + 2.0 * 6.30596399307251
Epoch 900, val loss: 0.8674413561820984
Epoch 910, training loss: 12.688042640686035 = 0.07823356240987778 + 2.0 * 6.304904460906982
Epoch 910, val loss: 0.8738433122634888
Epoch 920, training loss: 12.67936897277832 = 0.07486771047115326 + 2.0 * 6.302250862121582
Epoch 920, val loss: 0.8802821636199951
Epoch 930, training loss: 12.676225662231445 = 0.07168776541948318 + 2.0 * 6.302268981933594
Epoch 930, val loss: 0.8868144154548645
Epoch 940, training loss: 12.678842544555664 = 0.06867389380931854 + 2.0 * 6.305084228515625
Epoch 940, val loss: 0.8933387398719788
Epoch 950, training loss: 12.667095184326172 = 0.06583262979984283 + 2.0 * 6.300631046295166
Epoch 950, val loss: 0.899784505367279
Epoch 960, training loss: 12.660941123962402 = 0.0631469190120697 + 2.0 * 6.2988972663879395
Epoch 960, val loss: 0.9062233567237854
Epoch 970, training loss: 12.656156539916992 = 0.06060653552412987 + 2.0 * 6.297774791717529
Epoch 970, val loss: 0.9127578139305115
Epoch 980, training loss: 12.669113159179688 = 0.0581933818757534 + 2.0 * 6.305459976196289
Epoch 980, val loss: 0.9193976521492004
Epoch 990, training loss: 12.660840034484863 = 0.055891331285238266 + 2.0 * 6.302474498748779
Epoch 990, val loss: 0.9257259964942932
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7555555555555555
0.8118081180811808
=== training gcn model ===
Epoch 0, training loss: 19.126174926757812 = 1.9325822591781616 + 2.0 * 8.596796035766602
Epoch 0, val loss: 1.9298434257507324
Epoch 10, training loss: 19.115140914916992 = 1.922399640083313 + 2.0 * 8.596370697021484
Epoch 10, val loss: 1.9189523458480835
Epoch 20, training loss: 19.096057891845703 = 1.909741759300232 + 2.0 * 8.593157768249512
Epoch 20, val loss: 1.9050297737121582
Epoch 30, training loss: 19.037715911865234 = 1.8925161361694336 + 2.0 * 8.572600364685059
Epoch 30, val loss: 1.885904312133789
Epoch 40, training loss: 18.801008224487305 = 1.872291088104248 + 2.0 * 8.46435832977295
Epoch 40, val loss: 1.8646305799484253
Epoch 50, training loss: 18.158512115478516 = 1.8515859842300415 + 2.0 * 8.153463363647461
Epoch 50, val loss: 1.8432929515838623
Epoch 60, training loss: 17.418643951416016 = 1.8338804244995117 + 2.0 * 7.792381286621094
Epoch 60, val loss: 1.8263037204742432
Epoch 70, training loss: 16.394643783569336 = 1.8213688135147095 + 2.0 * 7.286637306213379
Epoch 70, val loss: 1.8148303031921387
Epoch 80, training loss: 15.929492950439453 = 1.8125659227371216 + 2.0 * 7.0584635734558105
Epoch 80, val loss: 1.8067047595977783
Epoch 90, training loss: 15.69808292388916 = 1.8006352186203003 + 2.0 * 6.948723793029785
Epoch 90, val loss: 1.7951862812042236
Epoch 100, training loss: 15.49474811553955 = 1.788028359413147 + 2.0 * 6.853359699249268
Epoch 100, val loss: 1.783857822418213
Epoch 110, training loss: 15.345952033996582 = 1.776480793952942 + 2.0 * 6.784735679626465
Epoch 110, val loss: 1.774077296257019
Epoch 120, training loss: 15.20786190032959 = 1.7653945684432983 + 2.0 * 6.72123384475708
Epoch 120, val loss: 1.765136480331421
Epoch 130, training loss: 15.110442161560059 = 1.7539012432098389 + 2.0 * 6.67827033996582
Epoch 130, val loss: 1.7556772232055664
Epoch 140, training loss: 15.019783020019531 = 1.7408097982406616 + 2.0 * 6.639486789703369
Epoch 140, val loss: 1.7448604106903076
Epoch 150, training loss: 14.940813064575195 = 1.7262781858444214 + 2.0 * 6.607267379760742
Epoch 150, val loss: 1.7328764200210571
Epoch 160, training loss: 14.873637199401855 = 1.7100192308425903 + 2.0 * 6.581809043884277
Epoch 160, val loss: 1.719496488571167
Epoch 170, training loss: 14.817510604858398 = 1.6916193962097168 + 2.0 * 6.562945365905762
Epoch 170, val loss: 1.704448938369751
Epoch 180, training loss: 14.763690948486328 = 1.6708108186721802 + 2.0 * 6.546440124511719
Epoch 180, val loss: 1.6874052286148071
Epoch 190, training loss: 14.705559730529785 = 1.6475111246109009 + 2.0 * 6.529024124145508
Epoch 190, val loss: 1.6683580875396729
Epoch 200, training loss: 14.652837753295898 = 1.6213703155517578 + 2.0 * 6.51573371887207
Epoch 200, val loss: 1.6470180749893188
Epoch 210, training loss: 14.597623825073242 = 1.5923612117767334 + 2.0 * 6.502631187438965
Epoch 210, val loss: 1.6234824657440186
Epoch 220, training loss: 14.543890953063965 = 1.5606099367141724 + 2.0 * 6.491640567779541
Epoch 220, val loss: 1.5977479219436646
Epoch 230, training loss: 14.485298156738281 = 1.5257222652435303 + 2.0 * 6.479787826538086
Epoch 230, val loss: 1.5696463584899902
Epoch 240, training loss: 14.427556991577148 = 1.487725853919983 + 2.0 * 6.469915390014648
Epoch 240, val loss: 1.5390701293945312
Epoch 250, training loss: 14.373108863830566 = 1.4467194080352783 + 2.0 * 6.463194847106934
Epoch 250, val loss: 1.5062414407730103
Epoch 260, training loss: 14.309165954589844 = 1.4032244682312012 + 2.0 * 6.452970504760742
Epoch 260, val loss: 1.471364140510559
Epoch 270, training loss: 14.24592399597168 = 1.35746169090271 + 2.0 * 6.444231033325195
Epoch 270, val loss: 1.4349355697631836
Epoch 280, training loss: 14.196626663208008 = 1.309951901435852 + 2.0 * 6.443337440490723
Epoch 280, val loss: 1.397329330444336
Epoch 290, training loss: 14.125747680664062 = 1.2618380784988403 + 2.0 * 6.431954860687256
Epoch 290, val loss: 1.3594666719436646
Epoch 300, training loss: 14.06249713897705 = 1.2135316133499146 + 2.0 * 6.424482822418213
Epoch 300, val loss: 1.3218543529510498
Epoch 310, training loss: 14.01293659210205 = 1.1656380891799927 + 2.0 * 6.423649311065674
Epoch 310, val loss: 1.2847330570220947
Epoch 320, training loss: 13.947511672973633 = 1.1189730167388916 + 2.0 * 6.41426944732666
Epoch 320, val loss: 1.248894214630127
Epoch 330, training loss: 13.891663551330566 = 1.0738441944122314 + 2.0 * 6.408909797668457
Epoch 330, val loss: 1.2145947217941284
Epoch 340, training loss: 13.841598510742188 = 1.0302761793136597 + 2.0 * 6.405661106109619
Epoch 340, val loss: 1.1818186044692993
Epoch 350, training loss: 13.792144775390625 = 0.9885931015014648 + 2.0 * 6.40177583694458
Epoch 350, val loss: 1.1506500244140625
Epoch 360, training loss: 13.740779876708984 = 0.9487285614013672 + 2.0 * 6.396025657653809
Epoch 360, val loss: 1.1213313341140747
Epoch 370, training loss: 13.698248863220215 = 0.9105927348136902 + 2.0 * 6.39382791519165
Epoch 370, val loss: 1.093794584274292
Epoch 380, training loss: 13.657516479492188 = 0.8741439580917358 + 2.0 * 6.39168643951416
Epoch 380, val loss: 1.067630648612976
Epoch 390, training loss: 13.610219955444336 = 0.8394249677658081 + 2.0 * 6.385397434234619
Epoch 390, val loss: 1.0432662963867188
Epoch 400, training loss: 13.568991661071777 = 0.8060290813446045 + 2.0 * 6.381481170654297
Epoch 400, val loss: 1.02032470703125
Epoch 410, training loss: 13.532499313354492 = 0.7738671898841858 + 2.0 * 6.3793158531188965
Epoch 410, val loss: 0.9985959529876709
Epoch 420, training loss: 13.49565315246582 = 0.7429813742637634 + 2.0 * 6.376336097717285
Epoch 420, val loss: 0.9783384203910828
Epoch 430, training loss: 13.457157135009766 = 0.7129935622215271 + 2.0 * 6.372081756591797
Epoch 430, val loss: 0.9592182040214539
Epoch 440, training loss: 13.428109169006348 = 0.683665931224823 + 2.0 * 6.37222146987915
Epoch 440, val loss: 0.9409833550453186
Epoch 450, training loss: 13.392489433288574 = 0.6550864577293396 + 2.0 * 6.368701457977295
Epoch 450, val loss: 0.9237870573997498
Epoch 460, training loss: 13.356410026550293 = 0.627254068851471 + 2.0 * 6.364577770233154
Epoch 460, val loss: 0.9076525568962097
Epoch 470, training loss: 13.323554992675781 = 0.5999413132667542 + 2.0 * 6.361806869506836
Epoch 470, val loss: 0.8924896121025085
Epoch 480, training loss: 13.306069374084473 = 0.5730961561203003 + 2.0 * 6.366486549377441
Epoch 480, val loss: 0.8780882358551025
Epoch 490, training loss: 13.263483047485352 = 0.5467387437820435 + 2.0 * 6.358372211456299
Epoch 490, val loss: 0.8646297454833984
Epoch 500, training loss: 13.228442192077637 = 0.5208621621131897 + 2.0 * 6.353789806365967
Epoch 500, val loss: 0.8521891236305237
Epoch 510, training loss: 13.198404312133789 = 0.49546489119529724 + 2.0 * 6.35146951675415
Epoch 510, val loss: 0.8406335115432739
Epoch 520, training loss: 13.168620109558105 = 0.47057002782821655 + 2.0 * 6.349025249481201
Epoch 520, val loss: 0.8299770951271057
Epoch 530, training loss: 13.163633346557617 = 0.4462338984012604 + 2.0 * 6.358699798583984
Epoch 530, val loss: 0.8202667236328125
Epoch 540, training loss: 13.120407104492188 = 0.4227330684661865 + 2.0 * 6.348836898803711
Epoch 540, val loss: 0.8115358948707581
Epoch 550, training loss: 13.087401390075684 = 0.40013691782951355 + 2.0 * 6.343632221221924
Epoch 550, val loss: 0.8039359450340271
Epoch 560, training loss: 13.059895515441895 = 0.37834402918815613 + 2.0 * 6.340775966644287
Epoch 560, val loss: 0.7973177433013916
Epoch 570, training loss: 13.038896560668945 = 0.3573942184448242 + 2.0 * 6.3407511711120605
Epoch 570, val loss: 0.7916173338890076
Epoch 580, training loss: 13.020925521850586 = 0.3373304307460785 + 2.0 * 6.341797351837158
Epoch 580, val loss: 0.7867759466171265
Epoch 590, training loss: 12.992417335510254 = 0.3183008134365082 + 2.0 * 6.337058067321777
Epoch 590, val loss: 0.7827290296554565
Epoch 600, training loss: 12.968727111816406 = 0.3001723289489746 + 2.0 * 6.334277153015137
Epoch 600, val loss: 0.7795186042785645
Epoch 610, training loss: 12.949124336242676 = 0.28294527530670166 + 2.0 * 6.333089351654053
Epoch 610, val loss: 0.7770699858665466
Epoch 620, training loss: 12.933025360107422 = 0.26658591628074646 + 2.0 * 6.333219528198242
Epoch 620, val loss: 0.7753018140792847
Epoch 630, training loss: 12.910962104797363 = 0.2511287033557892 + 2.0 * 6.329916477203369
Epoch 630, val loss: 0.7741900682449341
Epoch 640, training loss: 12.899538040161133 = 0.23650586605072021 + 2.0 * 6.331516265869141
Epoch 640, val loss: 0.7738150954246521
Epoch 650, training loss: 12.875537872314453 = 0.22271250188350677 + 2.0 * 6.326412677764893
Epoch 650, val loss: 0.77412348985672
Epoch 660, training loss: 12.857900619506836 = 0.20978084206581116 + 2.0 * 6.324059963226318
Epoch 660, val loss: 0.7751293778419495
Epoch 670, training loss: 12.845200538635254 = 0.19760720431804657 + 2.0 * 6.32379674911499
Epoch 670, val loss: 0.7768253684043884
Epoch 680, training loss: 12.835463523864746 = 0.18622076511383057 + 2.0 * 6.324621200561523
Epoch 680, val loss: 0.7790309190750122
Epoch 690, training loss: 12.818277359008789 = 0.1756422370672226 + 2.0 * 6.321317672729492
Epoch 690, val loss: 0.7818165421485901
Epoch 700, training loss: 12.803985595703125 = 0.16574211418628693 + 2.0 * 6.319121837615967
Epoch 700, val loss: 0.7851917743682861
Epoch 710, training loss: 12.790732383728027 = 0.1564781814813614 + 2.0 * 6.317127227783203
Epoch 710, val loss: 0.7890974879264832
Epoch 720, training loss: 12.793243408203125 = 0.14780491590499878 + 2.0 * 6.322719097137451
Epoch 720, val loss: 0.7934178709983826
Epoch 730, training loss: 12.771992683410645 = 0.13975393772125244 + 2.0 * 6.316119194030762
Epoch 730, val loss: 0.7981038689613342
Epoch 740, training loss: 12.763203620910645 = 0.13220296800136566 + 2.0 * 6.315500259399414
Epoch 740, val loss: 0.8031560182571411
Epoch 750, training loss: 12.749300956726074 = 0.12518124282360077 + 2.0 * 6.3120598793029785
Epoch 750, val loss: 0.8085528612136841
Epoch 760, training loss: 12.741334915161133 = 0.1186138167977333 + 2.0 * 6.3113603591918945
Epoch 760, val loss: 0.8141799569129944
Epoch 770, training loss: 12.739596366882324 = 0.11246799677610397 + 2.0 * 6.313564300537109
Epoch 770, val loss: 0.820142388343811
Epoch 780, training loss: 12.72512435913086 = 0.10674276202917099 + 2.0 * 6.30919075012207
Epoch 780, val loss: 0.8263094425201416
Epoch 790, training loss: 12.715904235839844 = 0.10137384384870529 + 2.0 * 6.307265281677246
Epoch 790, val loss: 0.8326868414878845
Epoch 800, training loss: 12.708250999450684 = 0.0963570699095726 + 2.0 * 6.3059468269348145
Epoch 800, val loss: 0.8392999768257141
Epoch 810, training loss: 12.710962295532227 = 0.0916600450873375 + 2.0 * 6.309650897979736
Epoch 810, val loss: 0.8459844589233398
Epoch 820, training loss: 12.696866035461426 = 0.08723990619182587 + 2.0 * 6.304812908172607
Epoch 820, val loss: 0.8527995347976685
Epoch 830, training loss: 12.688547134399414 = 0.08311109244823456 + 2.0 * 6.302718162536621
Epoch 830, val loss: 0.8597154021263123
Epoch 840, training loss: 12.689037322998047 = 0.07924763113260269 + 2.0 * 6.304894924163818
Epoch 840, val loss: 0.8667477965354919
Epoch 850, training loss: 12.680802345275879 = 0.0756152868270874 + 2.0 * 6.30259370803833
Epoch 850, val loss: 0.8737754821777344
Epoch 860, training loss: 12.672892570495605 = 0.07221704721450806 + 2.0 * 6.300337791442871
Epoch 860, val loss: 0.8808205723762512
Epoch 870, training loss: 12.668627738952637 = 0.0690150335431099 + 2.0 * 6.299806118011475
Epoch 870, val loss: 0.8879765272140503
Epoch 880, training loss: 12.662489891052246 = 0.06600223481655121 + 2.0 * 6.298243999481201
Epoch 880, val loss: 0.895043134689331
Epoch 890, training loss: 12.657198905944824 = 0.06317505985498428 + 2.0 * 6.297011852264404
Epoch 890, val loss: 0.9021604061126709
Epoch 900, training loss: 12.65148639678955 = 0.060494083911180496 + 2.0 * 6.295495986938477
Epoch 900, val loss: 0.909300684928894
Epoch 910, training loss: 12.657369613647461 = 0.05797567218542099 + 2.0 * 6.299696922302246
Epoch 910, val loss: 0.916368842124939
Epoch 920, training loss: 12.645485877990723 = 0.055594123899936676 + 2.0 * 6.29494571685791
Epoch 920, val loss: 0.9234186410903931
Epoch 930, training loss: 12.641711235046387 = 0.05335194990038872 + 2.0 * 6.294179439544678
Epoch 930, val loss: 0.9304229021072388
Epoch 940, training loss: 12.633861541748047 = 0.05123892053961754 + 2.0 * 6.291311264038086
Epoch 940, val loss: 0.9374572038650513
Epoch 950, training loss: 12.633801460266113 = 0.049244869500398636 + 2.0 * 6.292278289794922
Epoch 950, val loss: 0.9444912672042847
Epoch 960, training loss: 12.628987312316895 = 0.04734591394662857 + 2.0 * 6.290820598602295
Epoch 960, val loss: 0.9514486789703369
Epoch 970, training loss: 12.633204460144043 = 0.045558467507362366 + 2.0 * 6.293822765350342
Epoch 970, val loss: 0.958302915096283
Epoch 980, training loss: 12.622580528259277 = 0.043873198330402374 + 2.0 * 6.289353847503662
Epoch 980, val loss: 0.9652085900306702
Epoch 990, training loss: 12.616029739379883 = 0.04227013140916824 + 2.0 * 6.286880016326904
Epoch 990, val loss: 0.9720458984375
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7185185185185186
0.8112809699525567
The final CL Acc:0.74074, 0.01600, The final GNN Acc:0.80882, 0.00386
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13218])
remove edge: torch.Size([2, 7942])
updated graph: torch.Size([2, 10604])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 19.155113220214844 = 1.9614224433898926 + 2.0 * 8.596845626831055
Epoch 0, val loss: 1.9568567276000977
Epoch 10, training loss: 19.144166946411133 = 1.950986385345459 + 2.0 * 8.596590042114258
Epoch 10, val loss: 1.9462270736694336
Epoch 20, training loss: 19.12749481201172 = 1.9381211996078491 + 2.0 * 8.594686508178711
Epoch 20, val loss: 1.9329453706741333
Epoch 30, training loss: 19.081212997436523 = 1.9200522899627686 + 2.0 * 8.580580711364746
Epoch 30, val loss: 1.9144248962402344
Epoch 40, training loss: 18.901039123535156 = 1.8963038921356201 + 2.0 * 8.502367973327637
Epoch 40, val loss: 1.8909326791763306
Epoch 50, training loss: 18.2843074798584 = 1.8691117763519287 + 2.0 * 8.207597732543945
Epoch 50, val loss: 1.864855170249939
Epoch 60, training loss: 17.60177230834961 = 1.8443650007247925 + 2.0 * 7.878704071044922
Epoch 60, val loss: 1.8422847986221313
Epoch 70, training loss: 16.54547119140625 = 1.8256880044937134 + 2.0 * 7.359891891479492
Epoch 70, val loss: 1.8254022598266602
Epoch 80, training loss: 15.986100196838379 = 1.8114039897918701 + 2.0 * 7.087347984313965
Epoch 80, val loss: 1.8119747638702393
Epoch 90, training loss: 15.704782485961914 = 1.7952933311462402 + 2.0 * 6.954744815826416
Epoch 90, val loss: 1.7961918115615845
Epoch 100, training loss: 15.536019325256348 = 1.7761180400848389 + 2.0 * 6.879950523376465
Epoch 100, val loss: 1.7784358263015747
Epoch 110, training loss: 15.393592834472656 = 1.7574000358581543 + 2.0 * 6.818096160888672
Epoch 110, val loss: 1.7621800899505615
Epoch 120, training loss: 15.267224311828613 = 1.7398465871810913 + 2.0 * 6.763689041137695
Epoch 120, val loss: 1.7467089891433716
Epoch 130, training loss: 15.162202835083008 = 1.7211631536483765 + 2.0 * 6.72052001953125
Epoch 130, val loss: 1.729843258857727
Epoch 140, training loss: 15.061223030090332 = 1.7006032466888428 + 2.0 * 6.680309772491455
Epoch 140, val loss: 1.7112351655960083
Epoch 150, training loss: 14.976905822753906 = 1.6777256727218628 + 2.0 * 6.649590015411377
Epoch 150, val loss: 1.6908360719680786
Epoch 160, training loss: 14.897294998168945 = 1.6521068811416626 + 2.0 * 6.622593879699707
Epoch 160, val loss: 1.6683415174484253
Epoch 170, training loss: 14.819984436035156 = 1.623739242553711 + 2.0 * 6.598122596740723
Epoch 170, val loss: 1.643419623374939
Epoch 180, training loss: 14.749725341796875 = 1.591986894607544 + 2.0 * 6.578869342803955
Epoch 180, val loss: 1.6155591011047363
Epoch 190, training loss: 14.673288345336914 = 1.5566465854644775 + 2.0 * 6.558320999145508
Epoch 190, val loss: 1.5849558115005493
Epoch 200, training loss: 14.600746154785156 = 1.5177454948425293 + 2.0 * 6.541500091552734
Epoch 200, val loss: 1.5514402389526367
Epoch 210, training loss: 14.532368659973145 = 1.4755140542984009 + 2.0 * 6.5284271240234375
Epoch 210, val loss: 1.515069603919983
Epoch 220, training loss: 14.459633827209473 = 1.4305531978607178 + 2.0 * 6.514540195465088
Epoch 220, val loss: 1.476502537727356
Epoch 230, training loss: 14.389412879943848 = 1.3831712007522583 + 2.0 * 6.5031208992004395
Epoch 230, val loss: 1.4359230995178223
Epoch 240, training loss: 14.319602966308594 = 1.333972454071045 + 2.0 * 6.4928154945373535
Epoch 240, val loss: 1.3942146301269531
Epoch 250, training loss: 14.252371788024902 = 1.2840417623519897 + 2.0 * 6.484165191650391
Epoch 250, val loss: 1.352075219154358
Epoch 260, training loss: 14.1830472946167 = 1.2339580059051514 + 2.0 * 6.474544525146484
Epoch 260, val loss: 1.3100138902664185
Epoch 270, training loss: 14.116788864135742 = 1.184251308441162 + 2.0 * 6.466269016265869
Epoch 270, val loss: 1.2686996459960938
Epoch 280, training loss: 14.052789688110352 = 1.135369062423706 + 2.0 * 6.458710193634033
Epoch 280, val loss: 1.228456974029541
Epoch 290, training loss: 14.003893852233887 = 1.0876864194869995 + 2.0 * 6.458103656768799
Epoch 290, val loss: 1.1897608041763306
Epoch 300, training loss: 13.936606407165527 = 1.0422521829605103 + 2.0 * 6.447176933288574
Epoch 300, val loss: 1.1532618999481201
Epoch 310, training loss: 13.878687858581543 = 0.9986275434494019 + 2.0 * 6.440030097961426
Epoch 310, val loss: 1.1188353300094604
Epoch 320, training loss: 13.825704574584961 = 0.956633985042572 + 2.0 * 6.434535503387451
Epoch 320, val loss: 1.08632493019104
Epoch 330, training loss: 13.779463768005371 = 0.9165694713592529 + 2.0 * 6.4314470291137695
Epoch 330, val loss: 1.0558878183364868
Epoch 340, training loss: 13.731431007385254 = 0.8787956833839417 + 2.0 * 6.4263176918029785
Epoch 340, val loss: 1.0278035402297974
Epoch 350, training loss: 13.683838844299316 = 0.8428283333778381 + 2.0 * 6.420505046844482
Epoch 350, val loss: 1.00174081325531
Epoch 360, training loss: 13.640406608581543 = 0.8084191083908081 + 2.0 * 6.415993690490723
Epoch 360, val loss: 0.9774830937385559
Epoch 370, training loss: 13.6041898727417 = 0.7757248282432556 + 2.0 * 6.4142327308654785
Epoch 370, val loss: 0.9551274180412292
Epoch 380, training loss: 13.565247535705566 = 0.7451736927032471 + 2.0 * 6.410037040710449
Epoch 380, val loss: 0.9348419308662415
Epoch 390, training loss: 13.525720596313477 = 0.7161000370979309 + 2.0 * 6.404810428619385
Epoch 390, val loss: 0.9165202379226685
Epoch 400, training loss: 13.50236701965332 = 0.6884423494338989 + 2.0 * 6.4069623947143555
Epoch 400, val loss: 0.8996756076812744
Epoch 410, training loss: 13.460527420043945 = 0.6620339155197144 + 2.0 * 6.399246692657471
Epoch 410, val loss: 0.8844868540763855
Epoch 420, training loss: 13.424188613891602 = 0.6368922591209412 + 2.0 * 6.393648147583008
Epoch 420, val loss: 0.8707113862037659
Epoch 430, training loss: 13.393797874450684 = 0.612750232219696 + 2.0 * 6.390523910522461
Epoch 430, val loss: 0.8581599593162537
Epoch 440, training loss: 13.36682415008545 = 0.5895057320594788 + 2.0 * 6.3886590003967285
Epoch 440, val loss: 0.8468828201293945
Epoch 450, training loss: 13.336848258972168 = 0.567360520362854 + 2.0 * 6.384743690490723
Epoch 450, val loss: 0.8367470502853394
Epoch 460, training loss: 13.313546180725098 = 0.5460178852081299 + 2.0 * 6.383764266967773
Epoch 460, val loss: 0.8276943564414978
Epoch 470, training loss: 13.28576946258545 = 0.5254095196723938 + 2.0 * 6.3801798820495605
Epoch 470, val loss: 0.8196606040000916
Epoch 480, training loss: 13.261322975158691 = 0.5055385828018188 + 2.0 * 6.377892017364502
Epoch 480, val loss: 0.8124774098396301
Epoch 490, training loss: 13.23513412475586 = 0.4863976836204529 + 2.0 * 6.374368190765381
Epoch 490, val loss: 0.8061066269874573
Epoch 500, training loss: 13.21237564086914 = 0.4677644968032837 + 2.0 * 6.372305393218994
Epoch 500, val loss: 0.8005536198616028
Epoch 510, training loss: 13.189624786376953 = 0.44975364208221436 + 2.0 * 6.369935512542725
Epoch 510, val loss: 0.7957093715667725
Epoch 520, training loss: 13.177650451660156 = 0.4323789179325104 + 2.0 * 6.372635841369629
Epoch 520, val loss: 0.7915613055229187
Epoch 530, training loss: 13.146016120910645 = 0.4155358076095581 + 2.0 * 6.365240097045898
Epoch 530, val loss: 0.7881806492805481
Epoch 540, training loss: 13.122546195983887 = 0.3993271589279175 + 2.0 * 6.36160945892334
Epoch 540, val loss: 0.7853647470474243
Epoch 550, training loss: 13.104012489318848 = 0.38355156779289246 + 2.0 * 6.360230445861816
Epoch 550, val loss: 0.7830528020858765
Epoch 560, training loss: 13.083321571350098 = 0.3681926727294922 + 2.0 * 6.357564449310303
Epoch 560, val loss: 0.7812327146530151
Epoch 570, training loss: 13.072190284729004 = 0.3533163368701935 + 2.0 * 6.359436988830566
Epoch 570, val loss: 0.779895007610321
Epoch 580, training loss: 13.048166275024414 = 0.338931143283844 + 2.0 * 6.354617595672607
Epoch 580, val loss: 0.7790728211402893
Epoch 590, training loss: 13.026778221130371 = 0.3249751329421997 + 2.0 * 6.3509016036987305
Epoch 590, val loss: 0.7786693572998047
Epoch 600, training loss: 13.01240348815918 = 0.3113877475261688 + 2.0 * 6.350507736206055
Epoch 600, val loss: 0.7787092924118042
Epoch 610, training loss: 13.002571105957031 = 0.298168420791626 + 2.0 * 6.352201461791992
Epoch 610, val loss: 0.7792004942893982
Epoch 620, training loss: 12.977319717407227 = 0.28547176718711853 + 2.0 * 6.345923900604248
Epoch 620, val loss: 0.7800664901733398
Epoch 630, training loss: 12.962570190429688 = 0.27318447828292847 + 2.0 * 6.344692707061768
Epoch 630, val loss: 0.7813166975975037
Epoch 640, training loss: 12.95160961151123 = 0.26127713918685913 + 2.0 * 6.345166206359863
Epoch 640, val loss: 0.7829888463020325
Epoch 650, training loss: 12.931392669677734 = 0.24980485439300537 + 2.0 * 6.340794086456299
Epoch 650, val loss: 0.7850467562675476
Epoch 660, training loss: 12.925610542297363 = 0.23870675265789032 + 2.0 * 6.343451976776123
Epoch 660, val loss: 0.7874886393547058
Epoch 670, training loss: 12.906621932983398 = 0.22805705666542053 + 2.0 * 6.339282512664795
Epoch 670, val loss: 0.7903517484664917
Epoch 680, training loss: 12.89013957977295 = 0.21785011887550354 + 2.0 * 6.336144924163818
Epoch 680, val loss: 0.7934852838516235
Epoch 690, training loss: 12.899807929992676 = 0.20805197954177856 + 2.0 * 6.3458781242370605
Epoch 690, val loss: 0.7970500588417053
Epoch 700, training loss: 12.869487762451172 = 0.1987685114145279 + 2.0 * 6.335359573364258
Epoch 700, val loss: 0.8007877469062805
Epoch 710, training loss: 12.85453987121582 = 0.1898697465658188 + 2.0 * 6.332334995269775
Epoch 710, val loss: 0.804975152015686
Epoch 720, training loss: 12.841276168823242 = 0.1813805252313614 + 2.0 * 6.3299479484558105
Epoch 720, val loss: 0.8094021677970886
Epoch 730, training loss: 12.833711624145508 = 0.17325268685817719 + 2.0 * 6.33022928237915
Epoch 730, val loss: 0.8141224384307861
Epoch 740, training loss: 12.82058334350586 = 0.16551437973976135 + 2.0 * 6.3275346755981445
Epoch 740, val loss: 0.8190302848815918
Epoch 750, training loss: 12.813529014587402 = 0.15818947553634644 + 2.0 * 6.327669620513916
Epoch 750, val loss: 0.8241958618164062
Epoch 760, training loss: 12.80063247680664 = 0.15120919048786163 + 2.0 * 6.324711799621582
Epoch 760, val loss: 0.8296356797218323
Epoch 770, training loss: 12.808351516723633 = 0.14456956088542938 + 2.0 * 6.331891059875488
Epoch 770, val loss: 0.8352192640304565
Epoch 780, training loss: 12.783485412597656 = 0.13823843002319336 + 2.0 * 6.3226237297058105
Epoch 780, val loss: 0.8410356640815735
Epoch 790, training loss: 12.774520874023438 = 0.13226614892482758 + 2.0 * 6.321127414703369
Epoch 790, val loss: 0.8470116257667542
Epoch 800, training loss: 12.769678115844727 = 0.12657387554645538 + 2.0 * 6.321552276611328
Epoch 800, val loss: 0.8531008958816528
Epoch 810, training loss: 12.763382911682129 = 0.12114216387271881 + 2.0 * 6.321120262145996
Epoch 810, val loss: 0.8593772649765015
Epoch 820, training loss: 12.749691009521484 = 0.1160052940249443 + 2.0 * 6.316843032836914
Epoch 820, val loss: 0.865681529045105
Epoch 830, training loss: 12.7472562789917 = 0.11112522333860397 + 2.0 * 6.318065643310547
Epoch 830, val loss: 0.872112512588501
Epoch 840, training loss: 12.737039566040039 = 0.10645050555467606 + 2.0 * 6.3152947425842285
Epoch 840, val loss: 0.8787269592285156
Epoch 850, training loss: 12.736384391784668 = 0.102017343044281 + 2.0 * 6.317183494567871
Epoch 850, val loss: 0.8853628039360046
Epoch 860, training loss: 12.725495338439941 = 0.09778930991888046 + 2.0 * 6.313852787017822
Epoch 860, val loss: 0.8919313549995422
Epoch 870, training loss: 12.721736907958984 = 0.09380032122135162 + 2.0 * 6.313968181610107
Epoch 870, val loss: 0.8984873294830322
Epoch 880, training loss: 12.709626197814941 = 0.08999254554510117 + 2.0 * 6.309816837310791
Epoch 880, val loss: 0.905251681804657
Epoch 890, training loss: 12.706433296203613 = 0.08636020869016647 + 2.0 * 6.310036659240723
Epoch 890, val loss: 0.9120144844055176
Epoch 900, training loss: 12.70439624786377 = 0.08289897441864014 + 2.0 * 6.31074857711792
Epoch 900, val loss: 0.9187818169593811
Epoch 910, training loss: 12.6940336227417 = 0.07960974425077438 + 2.0 * 6.307211875915527
Epoch 910, val loss: 0.9255120754241943
Epoch 920, training loss: 12.69736099243164 = 0.07647262513637543 + 2.0 * 6.310444355010986
Epoch 920, val loss: 0.9322827458381653
Epoch 930, training loss: 12.683944702148438 = 0.07349144667387009 + 2.0 * 6.305226802825928
Epoch 930, val loss: 0.9390859603881836
Epoch 940, training loss: 12.680484771728516 = 0.07064549624919891 + 2.0 * 6.304919719696045
Epoch 940, val loss: 0.9458580017089844
Epoch 950, training loss: 12.682435989379883 = 0.06793508678674698 + 2.0 * 6.307250499725342
Epoch 950, val loss: 0.9526253938674927
Epoch 960, training loss: 12.670628547668457 = 0.06535939872264862 + 2.0 * 6.3026347160339355
Epoch 960, val loss: 0.9592846035957336
Epoch 970, training loss: 12.673090934753418 = 0.06290823966264725 + 2.0 * 6.305091381072998
Epoch 970, val loss: 0.9659838676452637
Epoch 980, training loss: 12.667750358581543 = 0.06056137755513191 + 2.0 * 6.303594589233398
Epoch 980, val loss: 0.9725330471992493
Epoch 990, training loss: 12.661201477050781 = 0.058332473039627075 + 2.0 * 6.301434516906738
Epoch 990, val loss: 0.9791373014450073
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.788888888888889
0.8355297838692674
=== training gcn model ===
Epoch 0, training loss: 19.136825561523438 = 1.9431331157684326 + 2.0 * 8.596846580505371
Epoch 0, val loss: 1.933358907699585
Epoch 10, training loss: 19.12665557861328 = 1.9334105253219604 + 2.0 * 8.596622467041016
Epoch 10, val loss: 1.924200415611267
Epoch 20, training loss: 19.111011505126953 = 1.9214670658111572 + 2.0 * 8.594772338867188
Epoch 20, val loss: 1.9124127626419067
Epoch 30, training loss: 19.066112518310547 = 1.9049900770187378 + 2.0 * 8.580561637878418
Epoch 30, val loss: 1.8957656621932983
Epoch 40, training loss: 18.896516799926758 = 1.8833681344985962 + 2.0 * 8.506574630737305
Epoch 40, val loss: 1.8748135566711426
Epoch 50, training loss: 18.242868423461914 = 1.8605000972747803 + 2.0 * 8.191184043884277
Epoch 50, val loss: 1.853645920753479
Epoch 60, training loss: 17.45137596130371 = 1.8410812616348267 + 2.0 * 7.805147647857666
Epoch 60, val loss: 1.8359870910644531
Epoch 70, training loss: 16.4525146484375 = 1.8266236782073975 + 2.0 * 7.312945365905762
Epoch 70, val loss: 1.8226842880249023
Epoch 80, training loss: 15.887526512145996 = 1.8150529861450195 + 2.0 * 7.036236763000488
Epoch 80, val loss: 1.8118013143539429
Epoch 90, training loss: 15.643681526184082 = 1.8027194738388062 + 2.0 * 6.920481204986572
Epoch 90, val loss: 1.7996258735656738
Epoch 100, training loss: 15.476123809814453 = 1.7873485088348389 + 2.0 * 6.844387531280518
Epoch 100, val loss: 1.7850358486175537
Epoch 110, training loss: 15.334723472595215 = 1.7721521854400635 + 2.0 * 6.781285762786865
Epoch 110, val loss: 1.7715741395950317
Epoch 120, training loss: 15.214349746704102 = 1.7581851482391357 + 2.0 * 6.728082180023193
Epoch 120, val loss: 1.7594531774520874
Epoch 130, training loss: 15.120880126953125 = 1.7440890073776245 + 2.0 * 6.6883955001831055
Epoch 130, val loss: 1.7468559741973877
Epoch 140, training loss: 15.027623176574707 = 1.728676676750183 + 2.0 * 6.649473190307617
Epoch 140, val loss: 1.7330875396728516
Epoch 150, training loss: 14.950523376464844 = 1.7116024494171143 + 2.0 * 6.619460582733154
Epoch 150, val loss: 1.7179205417633057
Epoch 160, training loss: 14.880353927612305 = 1.692639946937561 + 2.0 * 6.5938568115234375
Epoch 160, val loss: 1.7011134624481201
Epoch 170, training loss: 14.812602043151855 = 1.671565055847168 + 2.0 * 6.570518493652344
Epoch 170, val loss: 1.6826063394546509
Epoch 180, training loss: 14.75034236907959 = 1.6479575634002686 + 2.0 * 6.551192283630371
Epoch 180, val loss: 1.6618943214416504
Epoch 190, training loss: 14.691667556762695 = 1.6213092803955078 + 2.0 * 6.535179138183594
Epoch 190, val loss: 1.6384323835372925
Epoch 200, training loss: 14.630322456359863 = 1.591261625289917 + 2.0 * 6.519530296325684
Epoch 200, val loss: 1.6121768951416016
Epoch 210, training loss: 14.570185661315918 = 1.557363748550415 + 2.0 * 6.506411075592041
Epoch 210, val loss: 1.582664966583252
Epoch 220, training loss: 14.514633178710938 = 1.5193753242492676 + 2.0 * 6.497629165649414
Epoch 220, val loss: 1.5495415925979614
Epoch 230, training loss: 14.44970989227295 = 1.4775705337524414 + 2.0 * 6.486069679260254
Epoch 230, val loss: 1.513396978378296
Epoch 240, training loss: 14.38579273223877 = 1.4322583675384521 + 2.0 * 6.476767063140869
Epoch 240, val loss: 1.4742053747177124
Epoch 250, training loss: 14.320094108581543 = 1.3834073543548584 + 2.0 * 6.468343257904053
Epoch 250, val loss: 1.4322054386138916
Epoch 260, training loss: 14.253116607666016 = 1.3314177989959717 + 2.0 * 6.460849285125732
Epoch 260, val loss: 1.3878655433654785
Epoch 270, training loss: 14.188318252563477 = 1.277290940284729 + 2.0 * 6.4555134773254395
Epoch 270, val loss: 1.3420460224151611
Epoch 280, training loss: 14.116497993469238 = 1.2224448919296265 + 2.0 * 6.44702672958374
Epoch 280, val loss: 1.2960566282272339
Epoch 290, training loss: 14.048420906066895 = 1.1675561666488647 + 2.0 * 6.440432548522949
Epoch 290, val loss: 1.2504016160964966
Epoch 300, training loss: 13.985098838806152 = 1.1136705875396729 + 2.0 * 6.435714244842529
Epoch 300, val loss: 1.2060422897338867
Epoch 310, training loss: 13.9239501953125 = 1.0622389316558838 + 2.0 * 6.430855751037598
Epoch 310, val loss: 1.1641570329666138
Epoch 320, training loss: 13.861510276794434 = 1.0132967233657837 + 2.0 * 6.424106597900391
Epoch 320, val loss: 1.1244231462478638
Epoch 330, training loss: 13.804298400878906 = 0.966598629951477 + 2.0 * 6.418849945068359
Epoch 330, val loss: 1.0868300199508667
Epoch 340, training loss: 13.767228126525879 = 0.9221975207328796 + 2.0 * 6.422515392303467
Epoch 340, val loss: 1.051582932472229
Epoch 350, training loss: 13.704323768615723 = 0.8808764219284058 + 2.0 * 6.411723613739014
Epoch 350, val loss: 1.0190281867980957
Epoch 360, training loss: 13.653164863586426 = 0.8419954776763916 + 2.0 * 6.405584812164307
Epoch 360, val loss: 0.9887787699699402
Epoch 370, training loss: 13.609268188476562 = 0.8052161931991577 + 2.0 * 6.402026176452637
Epoch 370, val loss: 0.96044921875
Epoch 380, training loss: 13.574213027954102 = 0.7704423069953918 + 2.0 * 6.401885509490967
Epoch 380, val loss: 0.934224009513855
Epoch 390, training loss: 13.524807929992676 = 0.7378842234611511 + 2.0 * 6.39346170425415
Epoch 390, val loss: 0.9101729393005371
Epoch 400, training loss: 13.484320640563965 = 0.7069007158279419 + 2.0 * 6.388710021972656
Epoch 400, val loss: 0.8876830339431763
Epoch 410, training loss: 13.458375930786133 = 0.6772286891937256 + 2.0 * 6.390573501586914
Epoch 410, val loss: 0.8667666912078857
Epoch 420, training loss: 13.418157577514648 = 0.649057924747467 + 2.0 * 6.384549617767334
Epoch 420, val loss: 0.8473283052444458
Epoch 430, training loss: 13.381973266601562 = 0.6222352981567383 + 2.0 * 6.379868984222412
Epoch 430, val loss: 0.8296732902526855
Epoch 440, training loss: 13.34831428527832 = 0.5964708924293518 + 2.0 * 6.375921726226807
Epoch 440, val loss: 0.8132590055465698
Epoch 450, training loss: 13.31639289855957 = 0.5715712308883667 + 2.0 * 6.372410774230957
Epoch 450, val loss: 0.7979774475097656
Epoch 460, training loss: 13.287638664245605 = 0.5474745035171509 + 2.0 * 6.370081901550293
Epoch 460, val loss: 0.7838542461395264
Epoch 470, training loss: 13.263702392578125 = 0.5243601202964783 + 2.0 * 6.36967134475708
Epoch 470, val loss: 0.7707425951957703
Epoch 480, training loss: 13.234336853027344 = 0.5023524761199951 + 2.0 * 6.365992069244385
Epoch 480, val loss: 0.7590969204902649
Epoch 490, training loss: 13.215251922607422 = 0.4812529683113098 + 2.0 * 6.366999626159668
Epoch 490, val loss: 0.7484470009803772
Epoch 500, training loss: 13.186508178710938 = 0.46118614077568054 + 2.0 * 6.362660884857178
Epoch 500, val loss: 0.7387893199920654
Epoch 510, training loss: 13.15601921081543 = 0.4420214593410492 + 2.0 * 6.356998920440674
Epoch 510, val loss: 0.7301357388496399
Epoch 520, training loss: 13.133944511413574 = 0.42367634177207947 + 2.0 * 6.355134010314941
Epoch 520, val loss: 0.722308337688446
Epoch 530, training loss: 13.113754272460938 = 0.4061732292175293 + 2.0 * 6.353790283203125
Epoch 530, val loss: 0.7151020169258118
Epoch 540, training loss: 13.090617179870605 = 0.38951751589775085 + 2.0 * 6.350549697875977
Epoch 540, val loss: 0.7087200284004211
Epoch 550, training loss: 13.069906234741211 = 0.3735427260398865 + 2.0 * 6.34818172454834
Epoch 550, val loss: 0.7029575109481812
Epoch 560, training loss: 13.056962966918945 = 0.35816583037376404 + 2.0 * 6.349398612976074
Epoch 560, val loss: 0.6976950168609619
Epoch 570, training loss: 13.04171371459961 = 0.3432595729827881 + 2.0 * 6.349226951599121
Epoch 570, val loss: 0.6928640007972717
Epoch 580, training loss: 13.016377449035645 = 0.3289671242237091 + 2.0 * 6.343705177307129
Epoch 580, val loss: 0.6884382963180542
Epoch 590, training loss: 12.995928764343262 = 0.31498873233795166 + 2.0 * 6.340469837188721
Epoch 590, val loss: 0.6844192743301392
Epoch 600, training loss: 12.979523658752441 = 0.3013066351413727 + 2.0 * 6.339108467102051
Epoch 600, val loss: 0.6807356476783752
Epoch 610, training loss: 12.969693183898926 = 0.28791651129722595 + 2.0 * 6.340888500213623
Epoch 610, val loss: 0.6772823333740234
Epoch 620, training loss: 12.947456359863281 = 0.2748485207557678 + 2.0 * 6.3363037109375
Epoch 620, val loss: 0.6741253137588501
Epoch 630, training loss: 12.929344177246094 = 0.2620977759361267 + 2.0 * 6.33362340927124
Epoch 630, val loss: 0.6712726354598999
Epoch 640, training loss: 12.916135787963867 = 0.2495788186788559 + 2.0 * 6.333278656005859
Epoch 640, val loss: 0.668752908706665
Epoch 650, training loss: 12.899763107299805 = 0.23736657202243805 + 2.0 * 6.331198215484619
Epoch 650, val loss: 0.6665210723876953
Epoch 660, training loss: 12.88319206237793 = 0.22547216713428497 + 2.0 * 6.328859806060791
Epoch 660, val loss: 0.6646351218223572
Epoch 670, training loss: 12.877071380615234 = 0.21397101879119873 + 2.0 * 6.331550121307373
Epoch 670, val loss: 0.6631439328193665
Epoch 680, training loss: 12.865549087524414 = 0.20301024615764618 + 2.0 * 6.331269264221191
Epoch 680, val loss: 0.6620036363601685
Epoch 690, training loss: 12.841215133666992 = 0.19251267611980438 + 2.0 * 6.3243513107299805
Epoch 690, val loss: 0.6612628698348999
Epoch 700, training loss: 12.830921173095703 = 0.18255449831485748 + 2.0 * 6.324183464050293
Epoch 700, val loss: 0.6609793305397034
Epoch 710, training loss: 12.82532787322998 = 0.17314648628234863 + 2.0 * 6.3260908126831055
Epoch 710, val loss: 0.661117434501648
Epoch 720, training loss: 12.807615280151367 = 0.16433900594711304 + 2.0 * 6.321638107299805
Epoch 720, val loss: 0.6616916060447693
Epoch 730, training loss: 12.794816017150879 = 0.15602490305900574 + 2.0 * 6.319395542144775
Epoch 730, val loss: 0.6626099944114685
Epoch 740, training loss: 12.796825408935547 = 0.1482633650302887 + 2.0 * 6.324281215667725
Epoch 740, val loss: 0.6639071106910706
Epoch 750, training loss: 12.780330657958984 = 0.14100198447704315 + 2.0 * 6.319664478302002
Epoch 750, val loss: 0.665457010269165
Epoch 760, training loss: 12.7648286819458 = 0.13422009348869324 + 2.0 * 6.315304279327393
Epoch 760, val loss: 0.6673926115036011
Epoch 770, training loss: 12.760455131530762 = 0.12783946096897125 + 2.0 * 6.31630802154541
Epoch 770, val loss: 0.6696245670318604
Epoch 780, training loss: 12.747496604919434 = 0.12187381088733673 + 2.0 * 6.312811374664307
Epoch 780, val loss: 0.672132670879364
Epoch 790, training loss: 12.741414070129395 = 0.11627311259508133 + 2.0 * 6.312570571899414
Epoch 790, val loss: 0.674851655960083
Epoch 800, training loss: 12.737515449523926 = 0.11101549863815308 + 2.0 * 6.3132500648498535
Epoch 800, val loss: 0.6777853965759277
Epoch 810, training loss: 12.726513862609863 = 0.1060803011059761 + 2.0 * 6.310216903686523
Epoch 810, val loss: 0.680885910987854
Epoch 820, training loss: 12.721031188964844 = 0.10144557058811188 + 2.0 * 6.309792995452881
Epoch 820, val loss: 0.6842113137245178
Epoch 830, training loss: 12.722497940063477 = 0.09706151485443115 + 2.0 * 6.312718391418457
Epoch 830, val loss: 0.687608540058136
Epoch 840, training loss: 12.708966255187988 = 0.09295283257961273 + 2.0 * 6.308006763458252
Epoch 840, val loss: 0.6911759376525879
Epoch 850, training loss: 12.708559036254883 = 0.0890679806470871 + 2.0 * 6.3097453117370605
Epoch 850, val loss: 0.6948255896568298
Epoch 860, training loss: 12.69585132598877 = 0.08540666103363037 + 2.0 * 6.305222511291504
Epoch 860, val loss: 0.6986360549926758
Epoch 870, training loss: 12.692275047302246 = 0.08194039016962051 + 2.0 * 6.305167198181152
Epoch 870, val loss: 0.7025051116943359
Epoch 880, training loss: 12.687151908874512 = 0.07865724712610245 + 2.0 * 6.3042473793029785
Epoch 880, val loss: 0.7063854932785034
Epoch 890, training loss: 12.67789077758789 = 0.07554943114519119 + 2.0 * 6.301170825958252
Epoch 890, val loss: 0.7103701233863831
Epoch 900, training loss: 12.675047874450684 = 0.07260586321353912 + 2.0 * 6.301220893859863
Epoch 900, val loss: 0.7144370079040527
Epoch 910, training loss: 12.681453704833984 = 0.06982085108757019 + 2.0 * 6.305816650390625
Epoch 910, val loss: 0.7185645699501038
Epoch 920, training loss: 12.665294647216797 = 0.06716526299715042 + 2.0 * 6.299064636230469
Epoch 920, val loss: 0.7226654291152954
Epoch 930, training loss: 12.65959358215332 = 0.06465291976928711 + 2.0 * 6.297470569610596
Epoch 930, val loss: 0.7268701791763306
Epoch 940, training loss: 12.661128044128418 = 0.062260858714580536 + 2.0 * 6.299433708190918
Epoch 940, val loss: 0.731100857257843
Epoch 950, training loss: 12.66784954071045 = 0.05998600274324417 + 2.0 * 6.303931713104248
Epoch 950, val loss: 0.7353304028511047
Epoch 960, training loss: 12.649118423461914 = 0.057844605296850204 + 2.0 * 6.295637130737305
Epoch 960, val loss: 0.7395058870315552
Epoch 970, training loss: 12.643896102905273 = 0.05580728128552437 + 2.0 * 6.294044494628906
Epoch 970, val loss: 0.7437183856964111
Epoch 980, training loss: 12.640531539916992 = 0.05386427789926529 + 2.0 * 6.293333530426025
Epoch 980, val loss: 0.7480587363243103
Epoch 990, training loss: 12.635995864868164 = 0.05200555548071861 + 2.0 * 6.291995048522949
Epoch 990, val loss: 0.7523916959762573
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7925925925925926
0.8371112282551397
=== training gcn model ===
Epoch 0, training loss: 19.118703842163086 = 1.9249815940856934 + 2.0 * 8.596860885620117
Epoch 0, val loss: 1.9088575839996338
Epoch 10, training loss: 19.108407974243164 = 1.9151426553726196 + 2.0 * 8.596632957458496
Epoch 10, val loss: 1.8991731405258179
Epoch 20, training loss: 19.09257698059082 = 1.90277898311615 + 2.0 * 8.59489917755127
Epoch 20, val loss: 1.8871471881866455
Epoch 30, training loss: 19.049854278564453 = 1.8857316970825195 + 2.0 * 8.582061767578125
Epoch 30, val loss: 1.8710825443267822
Epoch 40, training loss: 18.894886016845703 = 1.8636711835861206 + 2.0 * 8.515607833862305
Epoch 40, val loss: 1.8512108325958252
Epoch 50, training loss: 18.285024642944336 = 1.8399536609649658 + 2.0 * 8.222535133361816
Epoch 50, val loss: 1.8303406238555908
Epoch 60, training loss: 17.535415649414062 = 1.8188763856887817 + 2.0 * 7.858269691467285
Epoch 60, val loss: 1.8122692108154297
Epoch 70, training loss: 16.512176513671875 = 1.8034776449203491 + 2.0 * 7.354349136352539
Epoch 70, val loss: 1.7981958389282227
Epoch 80, training loss: 15.956281661987305 = 1.7906098365783691 + 2.0 * 7.082836151123047
Epoch 80, val loss: 1.7866307497024536
Epoch 90, training loss: 15.679193496704102 = 1.7751682996749878 + 2.0 * 6.952012538909912
Epoch 90, val loss: 1.7737864255905151
Epoch 100, training loss: 15.4779691696167 = 1.758455514907837 + 2.0 * 6.859756946563721
Epoch 100, val loss: 1.7603660821914673
Epoch 110, training loss: 15.346053123474121 = 1.7422107458114624 + 2.0 * 6.801921367645264
Epoch 110, val loss: 1.7465078830718994
Epoch 120, training loss: 15.237436294555664 = 1.724623203277588 + 2.0 * 6.756406784057617
Epoch 120, val loss: 1.7313858270645142
Epoch 130, training loss: 15.132068634033203 = 1.7052597999572754 + 2.0 * 6.713404178619385
Epoch 130, val loss: 1.7149385213851929
Epoch 140, training loss: 15.03563404083252 = 1.684051513671875 + 2.0 * 6.675791263580322
Epoch 140, val loss: 1.6969598531723022
Epoch 150, training loss: 14.95021915435791 = 1.6602165699005127 + 2.0 * 6.645001411437988
Epoch 150, val loss: 1.6770480871200562
Epoch 160, training loss: 14.868952751159668 = 1.633344054222107 + 2.0 * 6.617804527282715
Epoch 160, val loss: 1.6545448303222656
Epoch 170, training loss: 14.792675018310547 = 1.6033399105072021 + 2.0 * 6.594667434692383
Epoch 170, val loss: 1.6295201778411865
Epoch 180, training loss: 14.720597267150879 = 1.570357084274292 + 2.0 * 6.575119972229004
Epoch 180, val loss: 1.6020301580429077
Epoch 190, training loss: 14.647614479064941 = 1.534282922744751 + 2.0 * 6.556665897369385
Epoch 190, val loss: 1.57188880443573
Epoch 200, training loss: 14.58418083190918 = 1.4952147006988525 + 2.0 * 6.544483184814453
Epoch 200, val loss: 1.539485216140747
Epoch 210, training loss: 14.509828567504883 = 1.4540362358093262 + 2.0 * 6.527895927429199
Epoch 210, val loss: 1.505186676979065
Epoch 220, training loss: 14.442872047424316 = 1.4109874963760376 + 2.0 * 6.515942096710205
Epoch 220, val loss: 1.4695569276809692
Epoch 230, training loss: 14.382345199584961 = 1.3665059804916382 + 2.0 * 6.507919788360596
Epoch 230, val loss: 1.4329842329025269
Epoch 240, training loss: 14.313630104064941 = 1.321717619895935 + 2.0 * 6.4959564208984375
Epoch 240, val loss: 1.3963911533355713
Epoch 250, training loss: 14.249295234680176 = 1.2769490480422974 + 2.0 * 6.486173152923584
Epoch 250, val loss: 1.3600736856460571
Epoch 260, training loss: 14.190930366516113 = 1.232517123222351 + 2.0 * 6.479206562042236
Epoch 260, val loss: 1.3243448734283447
Epoch 270, training loss: 14.130922317504883 = 1.1893237829208374 + 2.0 * 6.470799446105957
Epoch 270, val loss: 1.2898160219192505
Epoch 280, training loss: 14.074305534362793 = 1.1474084854125977 + 2.0 * 6.463448524475098
Epoch 280, val loss: 1.2567062377929688
Epoch 290, training loss: 14.019815444946289 = 1.1068004369735718 + 2.0 * 6.456507682800293
Epoch 290, val loss: 1.2249702215194702
Epoch 300, training loss: 13.96909236907959 = 1.0675296783447266 + 2.0 * 6.450781345367432
Epoch 300, val loss: 1.1945134401321411
Epoch 310, training loss: 13.919434547424316 = 1.029539942741394 + 2.0 * 6.444947242736816
Epoch 310, val loss: 1.1652321815490723
Epoch 320, training loss: 13.873414993286133 = 0.9927116632461548 + 2.0 * 6.440351486206055
Epoch 320, val loss: 1.1371138095855713
Epoch 330, training loss: 13.824584007263184 = 0.9571597576141357 + 2.0 * 6.433712005615234
Epoch 330, val loss: 1.110161542892456
Epoch 340, training loss: 13.780416488647461 = 0.9226619005203247 + 2.0 * 6.428877353668213
Epoch 340, val loss: 1.0842366218566895
Epoch 350, training loss: 13.74318790435791 = 0.8891909718513489 + 2.0 * 6.426998615264893
Epoch 350, val loss: 1.059267520904541
Epoch 360, training loss: 13.698890686035156 = 0.8570322394371033 + 2.0 * 6.420929431915283
Epoch 360, val loss: 1.035488486289978
Epoch 370, training loss: 13.657422065734863 = 0.8260029554367065 + 2.0 * 6.415709495544434
Epoch 370, val loss: 1.0130233764648438
Epoch 380, training loss: 13.621854782104492 = 0.7960876226425171 + 2.0 * 6.412883758544922
Epoch 380, val loss: 0.9917693138122559
Epoch 390, training loss: 13.587215423583984 = 0.767453670501709 + 2.0 * 6.409881114959717
Epoch 390, val loss: 0.9716817140579224
Epoch 400, training loss: 13.550031661987305 = 0.739960789680481 + 2.0 * 6.405035495758057
Epoch 400, val loss: 0.9530839920043945
Epoch 410, training loss: 13.517328262329102 = 0.7136117815971375 + 2.0 * 6.401858329772949
Epoch 410, val loss: 0.9357649087905884
Epoch 420, training loss: 13.490288734436035 = 0.6883159279823303 + 2.0 * 6.400986194610596
Epoch 420, val loss: 0.9196299910545349
Epoch 430, training loss: 13.45670223236084 = 0.6641069650650024 + 2.0 * 6.396297454833984
Epoch 430, val loss: 0.9048234820365906
Epoch 440, training loss: 13.425432205200195 = 0.6408042311668396 + 2.0 * 6.3923139572143555
Epoch 440, val loss: 0.8912465572357178
Epoch 450, training loss: 13.399992942810059 = 0.618269681930542 + 2.0 * 6.390861511230469
Epoch 450, val loss: 0.8787022829055786
Epoch 460, training loss: 13.379020690917969 = 0.5965508222579956 + 2.0 * 6.391234874725342
Epoch 460, val loss: 0.867193877696991
Epoch 470, training loss: 13.3448486328125 = 0.5754372477531433 + 2.0 * 6.384705543518066
Epoch 470, val loss: 0.8567245006561279
Epoch 480, training loss: 13.317464828491211 = 0.5551075339317322 + 2.0 * 6.381178855895996
Epoch 480, val loss: 0.8472071886062622
Epoch 490, training loss: 13.29988956451416 = 0.5353953242301941 + 2.0 * 6.382246971130371
Epoch 490, val loss: 0.8385697603225708
Epoch 500, training loss: 13.271465301513672 = 0.516181468963623 + 2.0 * 6.377641677856445
Epoch 500, val loss: 0.8307960033416748
Epoch 510, training loss: 13.247963905334473 = 0.4977070093154907 + 2.0 * 6.375128269195557
Epoch 510, val loss: 0.8237465023994446
Epoch 520, training loss: 13.225251197814941 = 0.47985827922821045 + 2.0 * 6.372696399688721
Epoch 520, val loss: 0.8175626993179321
Epoch 530, training loss: 13.199344635009766 = 0.46260133385658264 + 2.0 * 6.368371486663818
Epoch 530, val loss: 0.8120332956314087
Epoch 540, training loss: 13.178762435913086 = 0.4457765519618988 + 2.0 * 6.366492748260498
Epoch 540, val loss: 0.8071044087409973
Epoch 550, training loss: 13.160979270935059 = 0.42941275238990784 + 2.0 * 6.365783214569092
Epoch 550, val loss: 0.8026331067085266
Epoch 560, training loss: 13.137151718139648 = 0.41352900862693787 + 2.0 * 6.36181116104126
Epoch 560, val loss: 0.798685610294342
Epoch 570, training loss: 13.116395950317383 = 0.39799171686172485 + 2.0 * 6.359201908111572
Epoch 570, val loss: 0.7951456904411316
Epoch 580, training loss: 13.104119300842285 = 0.38268929719924927 + 2.0 * 6.360714912414551
Epoch 580, val loss: 0.7918933629989624
Epoch 590, training loss: 13.0968656539917 = 0.36781546473503113 + 2.0 * 6.364525318145752
Epoch 590, val loss: 0.7886654734611511
Epoch 600, training loss: 13.062097549438477 = 0.3531681001186371 + 2.0 * 6.354464530944824
Epoch 600, val loss: 0.7858092188835144
Epoch 610, training loss: 13.042590141296387 = 0.3387780785560608 + 2.0 * 6.351905822753906
Epoch 610, val loss: 0.78321373462677
Epoch 620, training loss: 13.023892402648926 = 0.3245672285556793 + 2.0 * 6.349662780761719
Epoch 620, val loss: 0.7809215784072876
Epoch 630, training loss: 13.005403518676758 = 0.31052350997924805 + 2.0 * 6.347440242767334
Epoch 630, val loss: 0.7789256572723389
Epoch 640, training loss: 13.012185096740723 = 0.29670196771621704 + 2.0 * 6.357741355895996
Epoch 640, val loss: 0.7772316932678223
Epoch 650, training loss: 12.973968505859375 = 0.28322625160217285 + 2.0 * 6.345371246337891
Epoch 650, val loss: 0.7758166193962097
Epoch 660, training loss: 12.964797973632812 = 0.2701864540576935 + 2.0 * 6.347305774688721
Epoch 660, val loss: 0.7747677564620972
Epoch 670, training loss: 12.940848350524902 = 0.2575380802154541 + 2.0 * 6.341655254364014
Epoch 670, val loss: 0.7741668224334717
Epoch 680, training loss: 12.924571990966797 = 0.24532626569271088 + 2.0 * 6.339622974395752
Epoch 680, val loss: 0.7740651965141296
Epoch 690, training loss: 12.915681838989258 = 0.23355118930339813 + 2.0 * 6.341065406799316
Epoch 690, val loss: 0.774518609046936
Epoch 700, training loss: 12.905957221984863 = 0.22238034009933472 + 2.0 * 6.341788291931152
Epoch 700, val loss: 0.7753377556800842
Epoch 710, training loss: 12.883125305175781 = 0.21165187656879425 + 2.0 * 6.3357367515563965
Epoch 710, val loss: 0.776684582233429
Epoch 720, training loss: 12.869065284729004 = 0.20145849883556366 + 2.0 * 6.333803176879883
Epoch 720, val loss: 0.7785585522651672
Epoch 730, training loss: 12.858621597290039 = 0.19175493717193604 + 2.0 * 6.333433151245117
Epoch 730, val loss: 0.7808737754821777
Epoch 740, training loss: 12.853621482849121 = 0.1825481802225113 + 2.0 * 6.335536479949951
Epoch 740, val loss: 0.7835220694541931
Epoch 750, training loss: 12.833945274353027 = 0.17383840680122375 + 2.0 * 6.330053329467773
Epoch 750, val loss: 0.7865889072418213
Epoch 760, training loss: 12.82199764251709 = 0.16561374068260193 + 2.0 * 6.328191757202148
Epoch 760, val loss: 0.7899911999702454
Epoch 770, training loss: 12.811158180236816 = 0.15780392289161682 + 2.0 * 6.326677322387695
Epoch 770, val loss: 0.7937694787979126
Epoch 780, training loss: 12.803040504455566 = 0.15037380158901215 + 2.0 * 6.326333522796631
Epoch 780, val loss: 0.7978093028068542
Epoch 790, training loss: 12.793106079101562 = 0.14333897829055786 + 2.0 * 6.324883460998535
Epoch 790, val loss: 0.8020111918449402
Epoch 800, training loss: 12.786972999572754 = 0.13668593764305115 + 2.0 * 6.325143337249756
Epoch 800, val loss: 0.8064155578613281
Epoch 810, training loss: 12.777755737304688 = 0.1304301619529724 + 2.0 * 6.323662757873535
Epoch 810, val loss: 0.810976505279541
Epoch 820, training loss: 12.766487121582031 = 0.12452686578035355 + 2.0 * 6.320980072021484
Epoch 820, val loss: 0.8157630562782288
Epoch 830, training loss: 12.758404731750488 = 0.11892469227313995 + 2.0 * 6.319739818572998
Epoch 830, val loss: 0.8207352757453918
Epoch 840, training loss: 12.757267951965332 = 0.1136154904961586 + 2.0 * 6.321826457977295
Epoch 840, val loss: 0.8258141875267029
Epoch 850, training loss: 12.750462532043457 = 0.10860878229141235 + 2.0 * 6.320926666259766
Epoch 850, val loss: 0.8309494256973267
Epoch 860, training loss: 12.735968589782715 = 0.10385899245738983 + 2.0 * 6.316054821014404
Epoch 860, val loss: 0.8361561298370361
Epoch 870, training loss: 12.731446266174316 = 0.09937403351068497 + 2.0 * 6.316036224365234
Epoch 870, val loss: 0.8415212631225586
Epoch 880, training loss: 12.72244930267334 = 0.09512574225664139 + 2.0 * 6.313661575317383
Epoch 880, val loss: 0.8469066619873047
Epoch 890, training loss: 12.726856231689453 = 0.09109044075012207 + 2.0 * 6.317883014678955
Epoch 890, val loss: 0.8523772358894348
Epoch 900, training loss: 12.720565795898438 = 0.08726903796195984 + 2.0 * 6.316648483276367
Epoch 900, val loss: 0.8579521179199219
Epoch 910, training loss: 12.709014892578125 = 0.08366565406322479 + 2.0 * 6.312674522399902
Epoch 910, val loss: 0.8633617162704468
Epoch 920, training loss: 12.70052433013916 = 0.08024396002292633 + 2.0 * 6.310140132904053
Epoch 920, val loss: 0.8689002990722656
Epoch 930, training loss: 12.717504501342773 = 0.07699228823184967 + 2.0 * 6.320256233215332
Epoch 930, val loss: 0.874430775642395
Epoch 940, training loss: 12.69694709777832 = 0.0739300400018692 + 2.0 * 6.311508655548096
Epoch 940, val loss: 0.8798666596412659
Epoch 950, training loss: 12.685792922973633 = 0.0710345134139061 + 2.0 * 6.307379245758057
Epoch 950, val loss: 0.885384738445282
Epoch 960, training loss: 12.680222511291504 = 0.06827415525913239 + 2.0 * 6.305974006652832
Epoch 960, val loss: 0.8909552693367004
Epoch 970, training loss: 12.680870056152344 = 0.06563998013734818 + 2.0 * 6.307614803314209
Epoch 970, val loss: 0.896588146686554
Epoch 980, training loss: 12.673563003540039 = 0.06313753128051758 + 2.0 * 6.30521297454834
Epoch 980, val loss: 0.9021384716033936
Epoch 990, training loss: 12.671485900878906 = 0.06076277419924736 + 2.0 * 6.305361747741699
Epoch 990, val loss: 0.9076941609382629
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7925925925925926
0.8365840801265156
The final CL Acc:0.79136, 0.00175, The final GNN Acc:0.83641, 0.00066
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.5 der2:0 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11588])
remove edge: torch.Size([2, 9440])
updated graph: torch.Size([2, 10472])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 19.13267707824707 = 1.9389851093292236 + 2.0 * 8.596845626831055
Epoch 0, val loss: 1.936425805091858
Epoch 10, training loss: 19.12240982055664 = 1.9292484521865845 + 2.0 * 8.596580505371094
Epoch 10, val loss: 1.9261832237243652
Epoch 20, training loss: 19.106365203857422 = 1.917130947113037 + 2.0 * 8.594616889953613
Epoch 20, val loss: 1.9133540391921997
Epoch 30, training loss: 19.0594539642334 = 1.900289535522461 + 2.0 * 8.579582214355469
Epoch 30, val loss: 1.8955639600753784
Epoch 40, training loss: 18.852571487426758 = 1.87873375415802 + 2.0 * 8.486918449401855
Epoch 40, val loss: 1.8736497163772583
Epoch 50, training loss: 17.859207153320312 = 1.8563635349273682 + 2.0 * 8.001421928405762
Epoch 50, val loss: 1.8516374826431274
Epoch 60, training loss: 16.94512176513672 = 1.8377151489257812 + 2.0 * 7.553703784942627
Epoch 60, val loss: 1.8346997499465942
Epoch 70, training loss: 16.370649337768555 = 1.8236554861068726 + 2.0 * 7.273497104644775
Epoch 70, val loss: 1.8211947679519653
Epoch 80, training loss: 15.964397430419922 = 1.8093962669372559 + 2.0 * 7.077500343322754
Epoch 80, val loss: 1.8079321384429932
Epoch 90, training loss: 15.710714340209961 = 1.7964348793029785 + 2.0 * 6.957139492034912
Epoch 90, val loss: 1.7962781190872192
Epoch 100, training loss: 15.548995971679688 = 1.7840063571929932 + 2.0 * 6.882494926452637
Epoch 100, val loss: 1.7850664854049683
Epoch 110, training loss: 15.434202194213867 = 1.771735429763794 + 2.0 * 6.831233501434326
Epoch 110, val loss: 1.774174690246582
Epoch 120, training loss: 15.316864967346191 = 1.7596044540405273 + 2.0 * 6.778630256652832
Epoch 120, val loss: 1.7636737823486328
Epoch 130, training loss: 15.201354026794434 = 1.7476625442504883 + 2.0 * 6.726845741271973
Epoch 130, val loss: 1.753282904624939
Epoch 140, training loss: 15.088447570800781 = 1.7352068424224854 + 2.0 * 6.6766204833984375
Epoch 140, val loss: 1.7424074411392212
Epoch 150, training loss: 14.99966812133789 = 1.7207140922546387 + 2.0 * 6.639477252960205
Epoch 150, val loss: 1.7300963401794434
Epoch 160, training loss: 14.922760009765625 = 1.7038146257400513 + 2.0 * 6.609472751617432
Epoch 160, val loss: 1.7159000635147095
Epoch 170, training loss: 14.850732803344727 = 1.6847622394561768 + 2.0 * 6.5829854011535645
Epoch 170, val loss: 1.7000112533569336
Epoch 180, training loss: 14.785406112670898 = 1.6634784936904907 + 2.0 * 6.5609636306762695
Epoch 180, val loss: 1.6821212768554688
Epoch 190, training loss: 14.720945358276367 = 1.6397265195846558 + 2.0 * 6.540609359741211
Epoch 190, val loss: 1.662263035774231
Epoch 200, training loss: 14.660645484924316 = 1.613341212272644 + 2.0 * 6.523652076721191
Epoch 200, val loss: 1.6401972770690918
Epoch 210, training loss: 14.608278274536133 = 1.584033489227295 + 2.0 * 6.512122631072998
Epoch 210, val loss: 1.6156573295593262
Epoch 220, training loss: 14.543192863464355 = 1.5519648790359497 + 2.0 * 6.495614051818848
Epoch 220, val loss: 1.588863730430603
Epoch 230, training loss: 14.483176231384277 = 1.51726233959198 + 2.0 * 6.482956886291504
Epoch 230, val loss: 1.5599520206451416
Epoch 240, training loss: 14.42275619506836 = 1.479896903038025 + 2.0 * 6.471429824829102
Epoch 240, val loss: 1.5289733409881592
Epoch 250, training loss: 14.364246368408203 = 1.4403412342071533 + 2.0 * 6.4619526863098145
Epoch 250, val loss: 1.4962819814682007
Epoch 260, training loss: 14.30826187133789 = 1.3995773792266846 + 2.0 * 6.454342365264893
Epoch 260, val loss: 1.4627864360809326
Epoch 270, training loss: 14.246010780334473 = 1.3577829599380493 + 2.0 * 6.444113731384277
Epoch 270, val loss: 1.4287525415420532
Epoch 280, training loss: 14.19222640991211 = 1.3154593706130981 + 2.0 * 6.43838357925415
Epoch 280, val loss: 1.3946226835250854
Epoch 290, training loss: 14.13294792175293 = 1.2736005783081055 + 2.0 * 6.429673671722412
Epoch 290, val loss: 1.3610886335372925
Epoch 300, training loss: 14.078435897827148 = 1.232178807258606 + 2.0 * 6.423128604888916
Epoch 300, val loss: 1.3284387588500977
Epoch 310, training loss: 14.037399291992188 = 1.191515326499939 + 2.0 * 6.422942161560059
Epoch 310, val loss: 1.2967216968536377
Epoch 320, training loss: 13.974864959716797 = 1.1520309448242188 + 2.0 * 6.411417007446289
Epoch 320, val loss: 1.2665492296218872
Epoch 330, training loss: 13.924859046936035 = 1.1138274669647217 + 2.0 * 6.405515670776367
Epoch 330, val loss: 1.2377853393554688
Epoch 340, training loss: 13.87831974029541 = 1.0767630338668823 + 2.0 * 6.400778293609619
Epoch 340, val loss: 1.210362434387207
Epoch 350, training loss: 13.837559700012207 = 1.0409820079803467 + 2.0 * 6.398288726806641
Epoch 350, val loss: 1.184357762336731
Epoch 360, training loss: 13.78989315032959 = 1.0065606832504272 + 2.0 * 6.391666412353516
Epoch 360, val loss: 1.1598224639892578
Epoch 370, training loss: 13.746896743774414 = 0.9732431173324585 + 2.0 * 6.386826992034912
Epoch 370, val loss: 1.1365188360214233
Epoch 380, training loss: 13.709366798400879 = 0.9408372640609741 + 2.0 * 6.384264945983887
Epoch 380, val loss: 1.114208698272705
Epoch 390, training loss: 13.66869068145752 = 0.9092888236045837 + 2.0 * 6.379701137542725
Epoch 390, val loss: 1.09299898147583
Epoch 400, training loss: 13.629944801330566 = 0.8786516785621643 + 2.0 * 6.375646591186523
Epoch 400, val loss: 1.072713851928711
Epoch 410, training loss: 13.599574089050293 = 0.8487434983253479 + 2.0 * 6.375415325164795
Epoch 410, val loss: 1.0533592700958252
Epoch 420, training loss: 13.561380386352539 = 0.8197245001792908 + 2.0 * 6.370828151702881
Epoch 420, val loss: 1.03506338596344
Epoch 430, training loss: 13.523481369018555 = 0.7917177081108093 + 2.0 * 6.36588191986084
Epoch 430, val loss: 1.0177360773086548
Epoch 440, training loss: 13.487359046936035 = 0.7645164132118225 + 2.0 * 6.36142110824585
Epoch 440, val loss: 1.0013495683670044
Epoch 450, training loss: 13.462288856506348 = 0.7381357550621033 + 2.0 * 6.362076759338379
Epoch 450, val loss: 0.9858685731887817
Epoch 460, training loss: 13.42672348022461 = 0.7128354907035828 + 2.0 * 6.3569440841674805
Epoch 460, val loss: 0.971590518951416
Epoch 470, training loss: 13.397201538085938 = 0.688698947429657 + 2.0 * 6.354251384735107
Epoch 470, val loss: 0.9584080576896667
Epoch 480, training loss: 13.368698120117188 = 0.6655390858650208 + 2.0 * 6.351579666137695
Epoch 480, val loss: 0.9463608264923096
Epoch 490, training loss: 13.353890419006348 = 0.6433107852935791 + 2.0 * 6.355289936065674
Epoch 490, val loss: 0.9352923035621643
Epoch 500, training loss: 13.320371627807617 = 0.6219205260276794 + 2.0 * 6.3492255210876465
Epoch 500, val loss: 0.9252502918243408
Epoch 510, training loss: 13.289538383483887 = 0.6014992594718933 + 2.0 * 6.344019412994385
Epoch 510, val loss: 0.9161807894706726
Epoch 520, training loss: 13.266605377197266 = 0.581820011138916 + 2.0 * 6.342392444610596
Epoch 520, val loss: 0.9079349040985107
Epoch 530, training loss: 13.252764701843262 = 0.5627546310424805 + 2.0 * 6.345005035400391
Epoch 530, val loss: 0.9004769325256348
Epoch 540, training loss: 13.225574493408203 = 0.5443524122238159 + 2.0 * 6.340610980987549
Epoch 540, val loss: 0.8937455415725708
Epoch 550, training loss: 13.201079368591309 = 0.5265672206878662 + 2.0 * 6.337255954742432
Epoch 550, val loss: 0.8876717686653137
Epoch 560, training loss: 13.178771018981934 = 0.509225606918335 + 2.0 * 6.33477258682251
Epoch 560, val loss: 0.8821772933006287
Epoch 570, training loss: 13.158217430114746 = 0.4921955168247223 + 2.0 * 6.333011150360107
Epoch 570, val loss: 0.8771171569824219
Epoch 580, training loss: 13.142874717712402 = 0.47535526752471924 + 2.0 * 6.333759784698486
Epoch 580, val loss: 0.8723874688148499
Epoch 590, training loss: 13.12315845489502 = 0.4586624801158905 + 2.0 * 6.332248210906982
Epoch 590, val loss: 0.8677514791488647
Epoch 600, training loss: 13.101512908935547 = 0.44217005372047424 + 2.0 * 6.329671382904053
Epoch 600, val loss: 0.8635504841804504
Epoch 610, training loss: 13.08084774017334 = 0.42572638392448425 + 2.0 * 6.327560901641846
Epoch 610, val loss: 0.8593653440475464
Epoch 620, training loss: 13.059412956237793 = 0.4091787338256836 + 2.0 * 6.325117111206055
Epoch 620, val loss: 0.8553129434585571
Epoch 630, training loss: 13.047235488891602 = 0.39254963397979736 + 2.0 * 6.327342987060547
Epoch 630, val loss: 0.8513487577438354
Epoch 640, training loss: 13.024736404418945 = 0.375918447971344 + 2.0 * 6.324409008026123
Epoch 640, val loss: 0.8475348949432373
Epoch 650, training loss: 13.013031959533691 = 0.3592079281806946 + 2.0 * 6.326911926269531
Epoch 650, val loss: 0.8436219096183777
Epoch 660, training loss: 12.989033699035645 = 0.34251531958580017 + 2.0 * 6.323259353637695
Epoch 660, val loss: 0.8398029804229736
Epoch 670, training loss: 12.964210510253906 = 0.32603880763053894 + 2.0 * 6.319086074829102
Epoch 670, val loss: 0.8362603783607483
Epoch 680, training loss: 12.944170951843262 = 0.3097061514854431 + 2.0 * 6.317232608795166
Epoch 680, val loss: 0.8329870104789734
Epoch 690, training loss: 12.925671577453613 = 0.29358407855033875 + 2.0 * 6.316043853759766
Epoch 690, val loss: 0.8299640417098999
Epoch 700, training loss: 12.932394981384277 = 0.27784156799316406 + 2.0 * 6.327276706695557
Epoch 700, val loss: 0.8271389007568359
Epoch 710, training loss: 12.899611473083496 = 0.26276299357414246 + 2.0 * 6.318424224853516
Epoch 710, val loss: 0.8246910572052002
Epoch 720, training loss: 12.875487327575684 = 0.24830204248428345 + 2.0 * 6.313592433929443
Epoch 720, val loss: 0.8228402733802795
Epoch 730, training loss: 12.857866287231445 = 0.23445935547351837 + 2.0 * 6.311703681945801
Epoch 730, val loss: 0.8215341567993164
Epoch 740, training loss: 12.845277786254883 = 0.22129210829734802 + 2.0 * 6.311992645263672
Epoch 740, val loss: 0.8208292126655579
Epoch 750, training loss: 12.828320503234863 = 0.20885123312473297 + 2.0 * 6.30973482131958
Epoch 750, val loss: 0.8206323385238647
Epoch 760, training loss: 12.81478214263916 = 0.1971161812543869 + 2.0 * 6.308833122253418
Epoch 760, val loss: 0.8209953308105469
Epoch 770, training loss: 12.804266929626465 = 0.18614943325519562 + 2.0 * 6.309058666229248
Epoch 770, val loss: 0.8219204545021057
Epoch 780, training loss: 12.790902137756348 = 0.17588140070438385 + 2.0 * 6.3075103759765625
Epoch 780, val loss: 0.8233446478843689
Epoch 790, training loss: 12.778697967529297 = 0.16631077229976654 + 2.0 * 6.306193828582764
Epoch 790, val loss: 0.8252938985824585
Epoch 800, training loss: 12.765761375427246 = 0.15737639367580414 + 2.0 * 6.304192543029785
Epoch 800, val loss: 0.8278090953826904
Epoch 810, training loss: 12.766718864440918 = 0.14902372658252716 + 2.0 * 6.308847427368164
Epoch 810, val loss: 0.8306983709335327
Epoch 820, training loss: 12.7488374710083 = 0.14127600193023682 + 2.0 * 6.303780555725098
Epoch 820, val loss: 0.8340439796447754
Epoch 830, training loss: 12.753175735473633 = 0.1340174823999405 + 2.0 * 6.309578895568848
Epoch 830, val loss: 0.8377249836921692
Epoch 840, training loss: 12.729129791259766 = 0.12724074721336365 + 2.0 * 6.3009443283081055
Epoch 840, val loss: 0.8416415452957153
Epoch 850, training loss: 12.718972206115723 = 0.12093515694141388 + 2.0 * 6.299018383026123
Epoch 850, val loss: 0.8459871411323547
Epoch 860, training loss: 12.71009635925293 = 0.11501432210206985 + 2.0 * 6.29754114151001
Epoch 860, val loss: 0.8505722880363464
Epoch 870, training loss: 12.717551231384277 = 0.10948377847671509 + 2.0 * 6.3040337562561035
Epoch 870, val loss: 0.8554103374481201
Epoch 880, training loss: 12.703469276428223 = 0.1042647436261177 + 2.0 * 6.299602031707764
Epoch 880, val loss: 0.8602331876754761
Epoch 890, training loss: 12.691411972045898 = 0.09942634403705597 + 2.0 * 6.295992851257324
Epoch 890, val loss: 0.8654252290725708
Epoch 900, training loss: 12.686607360839844 = 0.09488153457641602 + 2.0 * 6.295863151550293
Epoch 900, val loss: 0.8707500100135803
Epoch 910, training loss: 12.676905632019043 = 0.0906011313199997 + 2.0 * 6.293152332305908
Epoch 910, val loss: 0.8762245178222656
Epoch 920, training loss: 12.670928955078125 = 0.08657577633857727 + 2.0 * 6.292176723480225
Epoch 920, val loss: 0.8818988800048828
Epoch 930, training loss: 12.679120063781738 = 0.0827857106924057 + 2.0 * 6.2981672286987305
Epoch 930, val loss: 0.8876916766166687
Epoch 940, training loss: 12.667469024658203 = 0.07918903976678848 + 2.0 * 6.294139862060547
Epoch 940, val loss: 0.8933576345443726
Epoch 950, training loss: 12.65693473815918 = 0.07583486288785934 + 2.0 * 6.2905497550964355
Epoch 950, val loss: 0.8993528485298157
Epoch 960, training loss: 12.660980224609375 = 0.07265107333660126 + 2.0 * 6.294164657592773
Epoch 960, val loss: 0.9053089618682861
Epoch 970, training loss: 12.65215015411377 = 0.06966865807771683 + 2.0 * 6.291240692138672
Epoch 970, val loss: 0.9113965630531311
Epoch 980, training loss: 12.641443252563477 = 0.06683032959699631 + 2.0 * 6.287306308746338
Epoch 980, val loss: 0.9175199270248413
Epoch 990, training loss: 12.636811256408691 = 0.06415785849094391 + 2.0 * 6.286326885223389
Epoch 990, val loss: 0.9237005710601807
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7740740740740741
0.8118081180811808
=== training gcn model ===
Epoch 0, training loss: 19.14447784423828 = 1.9507637023925781 + 2.0 * 8.596857070922852
Epoch 0, val loss: 1.9491276741027832
Epoch 10, training loss: 19.1337947845459 = 1.940604567527771 + 2.0 * 8.59659481048584
Epoch 10, val loss: 1.9390147924423218
Epoch 20, training loss: 19.117420196533203 = 1.9279487133026123 + 2.0 * 8.594736099243164
Epoch 20, val loss: 1.9264107942581177
Epoch 30, training loss: 19.07148551940918 = 1.910563349723816 + 2.0 * 8.580461502075195
Epoch 30, val loss: 1.9090591669082642
Epoch 40, training loss: 18.862260818481445 = 1.8878973722457886 + 2.0 * 8.487181663513184
Epoch 40, val loss: 1.8871632814407349
Epoch 50, training loss: 18.00214195251465 = 1.8627749681472778 + 2.0 * 8.069683074951172
Epoch 50, val loss: 1.8633006811141968
Epoch 60, training loss: 17.10432243347168 = 1.8424813747406006 + 2.0 * 7.63092041015625
Epoch 60, val loss: 1.8442503213882446
Epoch 70, training loss: 16.22184181213379 = 1.8305408954620361 + 2.0 * 7.195650577545166
Epoch 70, val loss: 1.8326935768127441
Epoch 80, training loss: 15.831643104553223 = 1.8193644285202026 + 2.0 * 7.006139278411865
Epoch 80, val loss: 1.8220422267913818
Epoch 90, training loss: 15.584266662597656 = 1.8056637048721313 + 2.0 * 6.889301300048828
Epoch 90, val loss: 1.8097702264785767
Epoch 100, training loss: 15.391742706298828 = 1.792372703552246 + 2.0 * 6.799685001373291
Epoch 100, val loss: 1.7982505559921265
Epoch 110, training loss: 15.270179748535156 = 1.7808029651641846 + 2.0 * 6.744688510894775
Epoch 110, val loss: 1.7876414060592651
Epoch 120, training loss: 15.157018661499023 = 1.7697319984436035 + 2.0 * 6.693643569946289
Epoch 120, val loss: 1.7771499156951904
Epoch 130, training loss: 15.071338653564453 = 1.7585331201553345 + 2.0 * 6.656402587890625
Epoch 130, val loss: 1.7665715217590332
Epoch 140, training loss: 14.998217582702637 = 1.7466107606887817 + 2.0 * 6.625803470611572
Epoch 140, val loss: 1.7556272745132446
Epoch 150, training loss: 14.940223693847656 = 1.7335734367370605 + 2.0 * 6.603324890136719
Epoch 150, val loss: 1.7439727783203125
Epoch 160, training loss: 14.880046844482422 = 1.7191004753112793 + 2.0 * 6.58047342300415
Epoch 160, val loss: 1.7312321662902832
Epoch 170, training loss: 14.835382461547852 = 1.7029129266738892 + 2.0 * 6.566234588623047
Epoch 170, val loss: 1.7171273231506348
Epoch 180, training loss: 14.77892780303955 = 1.6847779750823975 + 2.0 * 6.547074794769287
Epoch 180, val loss: 1.7014179229736328
Epoch 190, training loss: 14.728500366210938 = 1.6644105911254883 + 2.0 * 6.532044887542725
Epoch 190, val loss: 1.6839313507080078
Epoch 200, training loss: 14.678971290588379 = 1.6415408849716187 + 2.0 * 6.5187153816223145
Epoch 200, val loss: 1.6643563508987427
Epoch 210, training loss: 14.626570701599121 = 1.6159381866455078 + 2.0 * 6.505316257476807
Epoch 210, val loss: 1.6423732042312622
Epoch 220, training loss: 14.579602241516113 = 1.5872825384140015 + 2.0 * 6.49616003036499
Epoch 220, val loss: 1.6178172826766968
Epoch 230, training loss: 14.52096176147461 = 1.5554273128509521 + 2.0 * 6.482767105102539
Epoch 230, val loss: 1.5906097888946533
Epoch 240, training loss: 14.466331481933594 = 1.5204212665557861 + 2.0 * 6.472955226898193
Epoch 240, val loss: 1.560767412185669
Epoch 250, training loss: 14.43073844909668 = 1.482208013534546 + 2.0 * 6.474265098571777
Epoch 250, val loss: 1.5283290147781372
Epoch 260, training loss: 14.35344409942627 = 1.4415308237075806 + 2.0 * 6.45595645904541
Epoch 260, val loss: 1.4937069416046143
Epoch 270, training loss: 14.295211791992188 = 1.3984733819961548 + 2.0 * 6.448369026184082
Epoch 270, val loss: 1.4572172164916992
Epoch 280, training loss: 14.233929634094238 = 1.3528165817260742 + 2.0 * 6.440556526184082
Epoch 280, val loss: 1.4187023639678955
Epoch 290, training loss: 14.183120727539062 = 1.305656909942627 + 2.0 * 6.438731670379639
Epoch 290, val loss: 1.3793977499008179
Epoch 300, training loss: 14.118611335754395 = 1.258431077003479 + 2.0 * 6.430089950561523
Epoch 300, val loss: 1.3402048349380493
Epoch 310, training loss: 14.05830192565918 = 1.2111659049987793 + 2.0 * 6.423568248748779
Epoch 310, val loss: 1.3012590408325195
Epoch 320, training loss: 14.000093460083008 = 1.1639899015426636 + 2.0 * 6.418051719665527
Epoch 320, val loss: 1.2627898454666138
Epoch 330, training loss: 13.96264362335205 = 1.117297887802124 + 2.0 * 6.422672748565674
Epoch 330, val loss: 1.2250304222106934
Epoch 340, training loss: 13.898802757263184 = 1.0720701217651367 + 2.0 * 6.413366317749023
Epoch 340, val loss: 1.188850998878479
Epoch 350, training loss: 13.839529037475586 = 1.0284276008605957 + 2.0 * 6.405550956726074
Epoch 350, val loss: 1.1544076204299927
Epoch 360, training loss: 13.789032936096191 = 0.9861257076263428 + 2.0 * 6.401453495025635
Epoch 360, val loss: 1.1214535236358643
Epoch 370, training loss: 13.740097999572754 = 0.9450690150260925 + 2.0 * 6.397514343261719
Epoch 370, val loss: 1.0898544788360596
Epoch 380, training loss: 13.700230598449707 = 0.9054471254348755 + 2.0 * 6.3973917961120605
Epoch 380, val loss: 1.0597764253616333
Epoch 390, training loss: 13.650931358337402 = 0.8676042556762695 + 2.0 * 6.391663551330566
Epoch 390, val loss: 1.0314666032791138
Epoch 400, training loss: 13.60666561126709 = 0.831455647945404 + 2.0 * 6.3876051902771
Epoch 400, val loss: 1.0049043893814087
Epoch 410, training loss: 13.568964958190918 = 0.7969871163368225 + 2.0 * 6.385988712310791
Epoch 410, val loss: 0.9799582958221436
Epoch 420, training loss: 13.53227710723877 = 0.7642753720283508 + 2.0 * 6.384000778198242
Epoch 420, val loss: 0.9566426873207092
Epoch 430, training loss: 13.490823745727539 = 0.7332803010940552 + 2.0 * 6.378771781921387
Epoch 430, val loss: 0.9348702430725098
Epoch 440, training loss: 13.453392028808594 = 0.7038674354553223 + 2.0 * 6.374762058258057
Epoch 440, val loss: 0.9146196246147156
Epoch 450, training loss: 13.418081283569336 = 0.6758488416671753 + 2.0 * 6.3711161613464355
Epoch 450, val loss: 0.895717978477478
Epoch 460, training loss: 13.421294212341309 = 0.649172842502594 + 2.0 * 6.38606071472168
Epoch 460, val loss: 0.8780850768089294
Epoch 470, training loss: 13.363801956176758 = 0.6241828799247742 + 2.0 * 6.369809627532959
Epoch 470, val loss: 0.8619822263717651
Epoch 480, training loss: 13.326656341552734 = 0.6005557775497437 + 2.0 * 6.36305046081543
Epoch 480, val loss: 0.8472699522972107
Epoch 490, training loss: 13.299947738647461 = 0.5780394077301025 + 2.0 * 6.360954284667969
Epoch 490, val loss: 0.8336880207061768
Epoch 500, training loss: 13.275284767150879 = 0.5562942624092102 + 2.0 * 6.359495162963867
Epoch 500, val loss: 0.8210396766662598
Epoch 510, training loss: 13.26185131072998 = 0.5355097055435181 + 2.0 * 6.363170623779297
Epoch 510, val loss: 0.809424877166748
Epoch 520, training loss: 13.22866439819336 = 0.515708327293396 + 2.0 * 6.356478214263916
Epoch 520, val loss: 0.7986282706260681
Epoch 530, training loss: 13.206220626831055 = 0.496856689453125 + 2.0 * 6.354681968688965
Epoch 530, val loss: 0.788726806640625
Epoch 540, training loss: 13.182554244995117 = 0.47880664467811584 + 2.0 * 6.351873874664307
Epoch 540, val loss: 0.779604434967041
Epoch 550, training loss: 13.158232688903809 = 0.46152642369270325 + 2.0 * 6.348352909088135
Epoch 550, val loss: 0.771213173866272
Epoch 560, training loss: 13.137750625610352 = 0.44485828280448914 + 2.0 * 6.3464460372924805
Epoch 560, val loss: 0.7635062336921692
Epoch 570, training loss: 13.122386932373047 = 0.42879945039749146 + 2.0 * 6.3467936515808105
Epoch 570, val loss: 0.7563696503639221
Epoch 580, training loss: 13.097911834716797 = 0.4132874011993408 + 2.0 * 6.342312335968018
Epoch 580, val loss: 0.7498319149017334
Epoch 590, training loss: 13.078505516052246 = 0.3982764780521393 + 2.0 * 6.340114593505859
Epoch 590, val loss: 0.7438910603523254
Epoch 600, training loss: 13.072978973388672 = 0.3837341368198395 + 2.0 * 6.344622611999512
Epoch 600, val loss: 0.7384336590766907
Epoch 610, training loss: 13.04261302947998 = 0.3696393370628357 + 2.0 * 6.33648681640625
Epoch 610, val loss: 0.7334981560707092
Epoch 620, training loss: 13.025078773498535 = 0.35598480701446533 + 2.0 * 6.33454704284668
Epoch 620, val loss: 0.729064404964447
Epoch 630, training loss: 13.008525848388672 = 0.34266912937164307 + 2.0 * 6.33292818069458
Epoch 630, val loss: 0.7250812649726868
Epoch 640, training loss: 13.005082130432129 = 0.3296385109424591 + 2.0 * 6.337721824645996
Epoch 640, val loss: 0.7215197682380676
Epoch 650, training loss: 12.980633735656738 = 0.3169442117214203 + 2.0 * 6.331844806671143
Epoch 650, val loss: 0.7183805108070374
Epoch 660, training loss: 12.961843490600586 = 0.30455225706100464 + 2.0 * 6.328645706176758
Epoch 660, val loss: 0.7155606746673584
Epoch 670, training loss: 12.945947647094727 = 0.29235658049583435 + 2.0 * 6.32679557800293
Epoch 670, val loss: 0.7131737470626831
Epoch 680, training loss: 12.948237419128418 = 0.28039422631263733 + 2.0 * 6.333921432495117
Epoch 680, val loss: 0.7111538648605347
Epoch 690, training loss: 12.915529251098633 = 0.2686426341533661 + 2.0 * 6.323443412780762
Epoch 690, val loss: 0.7094687819480896
Epoch 700, training loss: 12.901836395263672 = 0.2571004331111908 + 2.0 * 6.322368144989014
Epoch 700, val loss: 0.7081601619720459
Epoch 710, training loss: 12.889184951782227 = 0.24573932588100433 + 2.0 * 6.321722984313965
Epoch 710, val loss: 0.7071611881256104
Epoch 720, training loss: 12.877028465270996 = 0.23458276689052582 + 2.0 * 6.32122278213501
Epoch 720, val loss: 0.7065612077713013
Epoch 730, training loss: 12.859413146972656 = 0.22372320294380188 + 2.0 * 6.317844867706299
Epoch 730, val loss: 0.7061511874198914
Epoch 740, training loss: 12.846235275268555 = 0.21309827268123627 + 2.0 * 6.316568374633789
Epoch 740, val loss: 0.7061510682106018
Epoch 750, training loss: 12.832947731018066 = 0.2027149349451065 + 2.0 * 6.3151164054870605
Epoch 750, val loss: 0.7065547108650208
Epoch 760, training loss: 12.829347610473633 = 0.19260577857494354 + 2.0 * 6.318370819091797
Epoch 760, val loss: 0.7072030305862427
Epoch 770, training loss: 12.832982063293457 = 0.18286876380443573 + 2.0 * 6.325056552886963
Epoch 770, val loss: 0.7082780599594116
Epoch 780, training loss: 12.806340217590332 = 0.17356590926647186 + 2.0 * 6.316387176513672
Epoch 780, val loss: 0.7095667123794556
Epoch 790, training loss: 12.78809928894043 = 0.16467276215553284 + 2.0 * 6.311713218688965
Epoch 790, val loss: 0.7112268209457397
Epoch 800, training loss: 12.776379585266113 = 0.1561703085899353 + 2.0 * 6.310104846954346
Epoch 800, val loss: 0.7131868600845337
Epoch 810, training loss: 12.789867401123047 = 0.14808319509029388 + 2.0 * 6.320892333984375
Epoch 810, val loss: 0.7154289484024048
Epoch 820, training loss: 12.764701843261719 = 0.1404358297586441 + 2.0 * 6.312132835388184
Epoch 820, val loss: 0.7179657816886902
Epoch 830, training loss: 12.749176979064941 = 0.13326051831245422 + 2.0 * 6.307958126068115
Epoch 830, val loss: 0.7205927968025208
Epoch 840, training loss: 12.737503051757812 = 0.1264757663011551 + 2.0 * 6.305513858795166
Epoch 840, val loss: 0.723569929599762
Epoch 850, training loss: 12.729966163635254 = 0.12006627768278122 + 2.0 * 6.304949760437012
Epoch 850, val loss: 0.7266832590103149
Epoch 860, training loss: 12.732318878173828 = 0.11403127014636993 + 2.0 * 6.309144020080566
Epoch 860, val loss: 0.729956865310669
Epoch 870, training loss: 12.731066703796387 = 0.10838919878005981 + 2.0 * 6.311338901519775
Epoch 870, val loss: 0.7333784103393555
Epoch 880, training loss: 12.70986557006836 = 0.10311577469110489 + 2.0 * 6.303374767303467
Epoch 880, val loss: 0.7368673086166382
Epoch 890, training loss: 12.70134449005127 = 0.0981539860367775 + 2.0 * 6.301595211029053
Epoch 890, val loss: 0.7406207919120789
Epoch 900, training loss: 12.693337440490723 = 0.09350591897964478 + 2.0 * 6.299915790557861
Epoch 900, val loss: 0.7444818019866943
Epoch 910, training loss: 12.686931610107422 = 0.08911511301994324 + 2.0 * 6.298908233642578
Epoch 910, val loss: 0.7485992908477783
Epoch 920, training loss: 12.726479530334473 = 0.08500664681196213 + 2.0 * 6.320736408233643
Epoch 920, val loss: 0.7527291774749756
Epoch 930, training loss: 12.687396049499512 = 0.08117841929197311 + 2.0 * 6.3031086921691895
Epoch 930, val loss: 0.7567383646965027
Epoch 940, training loss: 12.673108100891113 = 0.07759050279855728 + 2.0 * 6.29775857925415
Epoch 940, val loss: 0.7608795166015625
Epoch 950, training loss: 12.665231704711914 = 0.07422248274087906 + 2.0 * 6.295504570007324
Epoch 950, val loss: 0.7652373313903809
Epoch 960, training loss: 12.659112930297852 = 0.07104171067476273 + 2.0 * 6.2940354347229
Epoch 960, val loss: 0.7696843147277832
Epoch 970, training loss: 12.656266212463379 = 0.0680355355143547 + 2.0 * 6.2941155433654785
Epoch 970, val loss: 0.774132490158081
Epoch 980, training loss: 12.660391807556152 = 0.06520591676235199 + 2.0 * 6.297593116760254
Epoch 980, val loss: 0.7785083055496216
Epoch 990, training loss: 12.654542922973633 = 0.06255430728197098 + 2.0 * 6.295994281768799
Epoch 990, val loss: 0.782873272895813
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.737037037037037
0.8118081180811808
=== training gcn model ===
Epoch 0, training loss: 19.130321502685547 = 1.9365997314453125 + 2.0 * 8.596860885620117
Epoch 0, val loss: 1.9336355924606323
Epoch 10, training loss: 19.12062644958496 = 1.9273616075515747 + 2.0 * 8.59663200378418
Epoch 10, val loss: 1.9239370822906494
Epoch 20, training loss: 19.105060577392578 = 1.915745735168457 + 2.0 * 8.594656944274902
Epoch 20, val loss: 1.9115855693817139
Epoch 30, training loss: 19.056982040405273 = 1.899786114692688 + 2.0 * 8.578598022460938
Epoch 30, val loss: 1.8946919441223145
Epoch 40, training loss: 18.83799171447754 = 1.87973952293396 + 2.0 * 8.4791259765625
Epoch 40, val loss: 1.8743155002593994
Epoch 50, training loss: 18.00137710571289 = 1.8588149547576904 + 2.0 * 8.071281433105469
Epoch 50, val loss: 1.854645848274231
Epoch 60, training loss: 17.09237289428711 = 1.841770887374878 + 2.0 * 7.625301361083984
Epoch 60, val loss: 1.839406132698059
Epoch 70, training loss: 16.246849060058594 = 1.8296860456466675 + 2.0 * 7.208581924438477
Epoch 70, val loss: 1.8283710479736328
Epoch 80, training loss: 15.826448440551758 = 1.818798542022705 + 2.0 * 7.003824710845947
Epoch 80, val loss: 1.8181827068328857
Epoch 90, training loss: 15.58922004699707 = 1.8054921627044678 + 2.0 * 6.891863822937012
Epoch 90, val loss: 1.8054107427597046
Epoch 100, training loss: 15.392291069030762 = 1.791265845298767 + 2.0 * 6.800512790679932
Epoch 100, val loss: 1.7923694849014282
Epoch 110, training loss: 15.253570556640625 = 1.7781001329421997 + 2.0 * 6.737735271453857
Epoch 110, val loss: 1.7803198099136353
Epoch 120, training loss: 15.145368576049805 = 1.7650121450424194 + 2.0 * 6.690178394317627
Epoch 120, val loss: 1.76848304271698
Epoch 130, training loss: 15.062431335449219 = 1.7510956525802612 + 2.0 * 6.655667781829834
Epoch 130, val loss: 1.7557820081710815
Epoch 140, training loss: 14.99160385131836 = 1.7356536388397217 + 2.0 * 6.627974987030029
Epoch 140, val loss: 1.7420176267623901
Epoch 150, training loss: 14.929091453552246 = 1.7185274362564087 + 2.0 * 6.605281829833984
Epoch 150, val loss: 1.7271878719329834
Epoch 160, training loss: 14.863558769226074 = 1.699404001235962 + 2.0 * 6.582077503204346
Epoch 160, val loss: 1.7109404802322388
Epoch 170, training loss: 14.804780006408691 = 1.6780402660369873 + 2.0 * 6.5633697509765625
Epoch 170, val loss: 1.6928073167800903
Epoch 180, training loss: 14.737641334533691 = 1.6542690992355347 + 2.0 * 6.541686058044434
Epoch 180, val loss: 1.6727919578552246
Epoch 190, training loss: 14.67247200012207 = 1.6277575492858887 + 2.0 * 6.522356986999512
Epoch 190, val loss: 1.6504961252212524
Epoch 200, training loss: 14.610566139221191 = 1.5983844995498657 + 2.0 * 6.5060906410217285
Epoch 200, val loss: 1.6258779764175415
Epoch 210, training loss: 14.560665130615234 = 1.5659774541854858 + 2.0 * 6.497344017028809
Epoch 210, val loss: 1.598677158355713
Epoch 220, training loss: 14.489309310913086 = 1.5307725667953491 + 2.0 * 6.479268550872803
Epoch 220, val loss: 1.5692524909973145
Epoch 230, training loss: 14.430142402648926 = 1.4931567907333374 + 2.0 * 6.4684929847717285
Epoch 230, val loss: 1.5377339124679565
Epoch 240, training loss: 14.371696472167969 = 1.4533238410949707 + 2.0 * 6.45918607711792
Epoch 240, val loss: 1.5044114589691162
Epoch 250, training loss: 14.313582420349121 = 1.4115387201309204 + 2.0 * 6.451021671295166
Epoch 250, val loss: 1.469504475593567
Epoch 260, training loss: 14.28886604309082 = 1.3683300018310547 + 2.0 * 6.460268020629883
Epoch 260, val loss: 1.4337214231491089
Epoch 270, training loss: 14.207756042480469 = 1.3253259658813477 + 2.0 * 6.4412150382995605
Epoch 270, val loss: 1.3981382846832275
Epoch 280, training loss: 14.14820671081543 = 1.2822284698486328 + 2.0 * 6.432989120483398
Epoch 280, val loss: 1.362898349761963
Epoch 290, training loss: 14.092113494873047 = 1.239147663116455 + 2.0 * 6.426482677459717
Epoch 290, val loss: 1.3279448747634888
Epoch 300, training loss: 14.037959098815918 = 1.1960556507110596 + 2.0 * 6.420951843261719
Epoch 300, val loss: 1.2934648990631104
Epoch 310, training loss: 13.988775253295898 = 1.153152585029602 + 2.0 * 6.417811393737793
Epoch 310, val loss: 1.2595726251602173
Epoch 320, training loss: 13.938472747802734 = 1.111323356628418 + 2.0 * 6.413574695587158
Epoch 320, val loss: 1.2268273830413818
Epoch 330, training loss: 13.88534927368164 = 1.0705302953720093 + 2.0 * 6.40740966796875
Epoch 330, val loss: 1.1955578327178955
Epoch 340, training loss: 13.836990356445312 = 1.0308411121368408 + 2.0 * 6.403074741363525
Epoch 340, val loss: 1.1656713485717773
Epoch 350, training loss: 13.790868759155273 = 0.9925318360328674 + 2.0 * 6.399168491363525
Epoch 350, val loss: 1.1373757123947144
Epoch 360, training loss: 13.746489524841309 = 0.9558990001678467 + 2.0 * 6.395295143127441
Epoch 360, val loss: 1.1109727621078491
Epoch 370, training loss: 13.704380989074707 = 0.9209443926811218 + 2.0 * 6.39171838760376
Epoch 370, val loss: 1.0863946676254272
Epoch 380, training loss: 13.67439079284668 = 0.8877221941947937 + 2.0 * 6.39333438873291
Epoch 380, val loss: 1.0637367963790894
Epoch 390, training loss: 13.635026931762695 = 0.8563480973243713 + 2.0 * 6.389339447021484
Epoch 390, val loss: 1.0431299209594727
Epoch 400, training loss: 13.59329605102539 = 0.8270737528800964 + 2.0 * 6.383111000061035
Epoch 400, val loss: 1.0245460271835327
Epoch 410, training loss: 13.555675506591797 = 0.7994442582130432 + 2.0 * 6.378115653991699
Epoch 410, val loss: 1.0078299045562744
Epoch 420, training loss: 13.523709297180176 = 0.7733258605003357 + 2.0 * 6.375191688537598
Epoch 420, val loss: 0.9927148818969727
Epoch 430, training loss: 13.508593559265137 = 0.7485169768333435 + 2.0 * 6.380038261413574
Epoch 430, val loss: 0.978958249092102
Epoch 440, training loss: 13.468194007873535 = 0.7248814105987549 + 2.0 * 6.37165641784668
Epoch 440, val loss: 0.9665279984474182
Epoch 450, training loss: 13.437628746032715 = 0.7023538947105408 + 2.0 * 6.367637634277344
Epoch 450, val loss: 0.955413818359375
Epoch 460, training loss: 13.409163475036621 = 0.6805890202522278 + 2.0 * 6.364287376403809
Epoch 460, val loss: 0.9451857805252075
Epoch 470, training loss: 13.394290924072266 = 0.6593983173370361 + 2.0 * 6.367446422576904
Epoch 470, val loss: 0.9356898069381714
Epoch 480, training loss: 13.362710952758789 = 0.6388640403747559 + 2.0 * 6.361923694610596
Epoch 480, val loss: 0.9268682599067688
Epoch 490, training loss: 13.3373441696167 = 0.6188189387321472 + 2.0 * 6.359262466430664
Epoch 490, val loss: 0.918701708316803
Epoch 500, training loss: 13.310399055480957 = 0.5992299914360046 + 2.0 * 6.355584621429443
Epoch 500, val loss: 0.911050021648407
Epoch 510, training loss: 13.298078536987305 = 0.5800288915634155 + 2.0 * 6.359025001525879
Epoch 510, val loss: 0.9038984179496765
Epoch 520, training loss: 13.267257690429688 = 0.5613213777542114 + 2.0 * 6.352968215942383
Epoch 520, val loss: 0.8972470760345459
Epoch 530, training loss: 13.24199104309082 = 0.5429227948188782 + 2.0 * 6.349534034729004
Epoch 530, val loss: 0.8910884857177734
Epoch 540, training loss: 13.218560218811035 = 0.5248722434043884 + 2.0 * 6.34684419631958
Epoch 540, val loss: 0.885331392288208
Epoch 550, training loss: 13.198555946350098 = 0.5070865154266357 + 2.0 * 6.345734596252441
Epoch 550, val loss: 0.8799594640731812
Epoch 560, training loss: 13.18175220489502 = 0.48957353830337524 + 2.0 * 6.3460893630981445
Epoch 560, val loss: 0.8749716281890869
Epoch 570, training loss: 13.1587553024292 = 0.472502201795578 + 2.0 * 6.3431267738342285
Epoch 570, val loss: 0.8702802658081055
Epoch 580, training loss: 13.136635780334473 = 0.45576271414756775 + 2.0 * 6.3404364585876465
Epoch 580, val loss: 0.8660680055618286
Epoch 590, training loss: 13.116875648498535 = 0.43934544920921326 + 2.0 * 6.3387651443481445
Epoch 590, val loss: 0.8622221946716309
Epoch 600, training loss: 13.095870018005371 = 0.42315900325775146 + 2.0 * 6.336355686187744
Epoch 600, val loss: 0.8586191534996033
Epoch 610, training loss: 13.078228950500488 = 0.4071435332298279 + 2.0 * 6.335542678833008
Epoch 610, val loss: 0.8552160859107971
Epoch 620, training loss: 13.06029987335205 = 0.3913922905921936 + 2.0 * 6.334453582763672
Epoch 620, val loss: 0.8520300984382629
Epoch 630, training loss: 13.043257713317871 = 0.37595266103744507 + 2.0 * 6.333652496337891
Epoch 630, val loss: 0.8491275310516357
Epoch 640, training loss: 13.023689270019531 = 0.3608071804046631 + 2.0 * 6.3314409255981445
Epoch 640, val loss: 0.8464651703834534
Epoch 650, training loss: 13.017206192016602 = 0.34589073061943054 + 2.0 * 6.335657596588135
Epoch 650, val loss: 0.8439821004867554
Epoch 660, training loss: 12.992591857910156 = 0.33133140206336975 + 2.0 * 6.330630302429199
Epoch 660, val loss: 0.8417055010795593
Epoch 670, training loss: 12.970429420471191 = 0.3170326352119446 + 2.0 * 6.326698303222656
Epoch 670, val loss: 0.8396778106689453
Epoch 680, training loss: 12.979629516601562 = 0.3031049966812134 + 2.0 * 6.33826208114624
Epoch 680, val loss: 0.8378247022628784
Epoch 690, training loss: 12.939720153808594 = 0.28952449560165405 + 2.0 * 6.325098037719727
Epoch 690, val loss: 0.8363416790962219
Epoch 700, training loss: 12.922264099121094 = 0.27640682458877563 + 2.0 * 6.322928428649902
Epoch 700, val loss: 0.8352469801902771
Epoch 710, training loss: 12.90710163116455 = 0.26364412903785706 + 2.0 * 6.321728706359863
Epoch 710, val loss: 0.8344466686248779
Epoch 720, training loss: 12.902937889099121 = 0.2513110935688019 + 2.0 * 6.325813293457031
Epoch 720, val loss: 0.8339040279388428
Epoch 730, training loss: 12.887803077697754 = 0.2394150197505951 + 2.0 * 6.324193954467773
Epoch 730, val loss: 0.8338572382926941
Epoch 740, training loss: 12.863624572753906 = 0.22803661227226257 + 2.0 * 6.317793846130371
Epoch 740, val loss: 0.8341042399406433
Epoch 750, training loss: 12.851983070373535 = 0.21712148189544678 + 2.0 * 6.3174309730529785
Epoch 750, val loss: 0.8347792625427246
Epoch 760, training loss: 12.860928535461426 = 0.20663496851921082 + 2.0 * 6.327147006988525
Epoch 760, val loss: 0.8357892036437988
Epoch 770, training loss: 12.829622268676758 = 0.19674688577651978 + 2.0 * 6.316437721252441
Epoch 770, val loss: 0.8371086716651917
Epoch 780, training loss: 12.816816329956055 = 0.18731629848480225 + 2.0 * 6.3147501945495605
Epoch 780, val loss: 0.8389198780059814
Epoch 790, training loss: 12.803653717041016 = 0.1783429980278015 + 2.0 * 6.312655448913574
Epoch 790, val loss: 0.8409571051597595
Epoch 800, training loss: 12.792797088623047 = 0.16979657113552094 + 2.0 * 6.311500072479248
Epoch 800, val loss: 0.8433427810668945
Epoch 810, training loss: 12.786767959594727 = 0.16165415942668915 + 2.0 * 6.312556743621826
Epoch 810, val loss: 0.8459341526031494
Epoch 820, training loss: 12.781347274780273 = 0.15393568575382233 + 2.0 * 6.313705921173096
Epoch 820, val loss: 0.848764955997467
Epoch 830, training loss: 12.769586563110352 = 0.14667455852031708 + 2.0 * 6.311456203460693
Epoch 830, val loss: 0.8519145250320435
Epoch 840, training loss: 12.757269859313965 = 0.13981126248836517 + 2.0 * 6.30872917175293
Epoch 840, val loss: 0.8553341031074524
Epoch 850, training loss: 12.746240615844727 = 0.13329161703586578 + 2.0 * 6.306474685668945
Epoch 850, val loss: 0.8590112924575806
Epoch 860, training loss: 12.741008758544922 = 0.12710720300674438 + 2.0 * 6.306950569152832
Epoch 860, val loss: 0.8628830313682556
Epoch 870, training loss: 12.73886775970459 = 0.1212657168507576 + 2.0 * 6.308801174163818
Epoch 870, val loss: 0.866818904876709
Epoch 880, training loss: 12.727280616760254 = 0.11575128883123398 + 2.0 * 6.305764675140381
Epoch 880, val loss: 0.8709961771965027
Epoch 890, training loss: 12.719417572021484 = 0.1105477437376976 + 2.0 * 6.304434776306152
Epoch 890, val loss: 0.8753800392150879
Epoch 900, training loss: 12.709816932678223 = 0.10563478618860245 + 2.0 * 6.302091121673584
Epoch 900, val loss: 0.8798769116401672
Epoch 910, training loss: 12.710237503051758 = 0.10097786784172058 + 2.0 * 6.304629802703857
Epoch 910, val loss: 0.8845106363296509
Epoch 920, training loss: 12.698010444641113 = 0.09657473862171173 + 2.0 * 6.300717830657959
Epoch 920, val loss: 0.8891978859901428
Epoch 930, training loss: 12.702557563781738 = 0.0924277976155281 + 2.0 * 6.305064678192139
Epoch 930, val loss: 0.8939815163612366
Epoch 940, training loss: 12.68720817565918 = 0.08850224316120148 + 2.0 * 6.299353122711182
Epoch 940, val loss: 0.8987843990325928
Epoch 950, training loss: 12.683917045593262 = 0.08480557799339294 + 2.0 * 6.299555778503418
Epoch 950, val loss: 0.9036535620689392
Epoch 960, training loss: 12.676249504089355 = 0.08130276948213577 + 2.0 * 6.297473430633545
Epoch 960, val loss: 0.908652126789093
Epoch 970, training loss: 12.673587799072266 = 0.07797642052173615 + 2.0 * 6.2978057861328125
Epoch 970, val loss: 0.9136420488357544
Epoch 980, training loss: 12.672410011291504 = 0.07482337951660156 + 2.0 * 6.298793315887451
Epoch 980, val loss: 0.9186440110206604
Epoch 990, training loss: 12.668806076049805 = 0.07185111194849014 + 2.0 * 6.298477649688721
Epoch 990, val loss: 0.9235814809799194
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.762962962962963
0.8149710068529257
The final CL Acc:0.75802, 0.01552, The final GNN Acc:0.81286, 0.00149
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13180])
remove edge: torch.Size([2, 8018])
updated graph: torch.Size([2, 10642])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 19.138761520385742 = 1.945056438446045 + 2.0 * 8.59685230255127
Epoch 0, val loss: 1.9402629137039185
Epoch 10, training loss: 19.12773895263672 = 1.9344745874404907 + 2.0 * 8.59663200378418
Epoch 10, val loss: 1.9294544458389282
Epoch 20, training loss: 19.111024856567383 = 1.9213404655456543 + 2.0 * 8.594841957092285
Epoch 20, val loss: 1.9158788919448853
Epoch 30, training loss: 19.064376831054688 = 1.903071641921997 + 2.0 * 8.580652236938477
Epoch 30, val loss: 1.8970997333526611
Epoch 40, training loss: 18.8676700592041 = 1.8789546489715576 + 2.0 * 8.49435806274414
Epoch 40, val loss: 1.873026967048645
Epoch 50, training loss: 18.0051212310791 = 1.8521686792373657 + 2.0 * 8.076476097106934
Epoch 50, val loss: 1.8475077152252197
Epoch 60, training loss: 17.041519165039062 = 1.829270839691162 + 2.0 * 7.606124401092529
Epoch 60, val loss: 1.8274356126785278
Epoch 70, training loss: 16.432064056396484 = 1.8133854866027832 + 2.0 * 7.30933952331543
Epoch 70, val loss: 1.8128999471664429
Epoch 80, training loss: 16.124649047851562 = 1.79766845703125 + 2.0 * 7.163489818572998
Epoch 80, val loss: 1.798611044883728
Epoch 90, training loss: 15.828125 = 1.782957673072815 + 2.0 * 7.022583484649658
Epoch 90, val loss: 1.7856812477111816
Epoch 100, training loss: 15.556907653808594 = 1.7685120105743408 + 2.0 * 6.894197940826416
Epoch 100, val loss: 1.7734335660934448
Epoch 110, training loss: 15.37122917175293 = 1.753537654876709 + 2.0 * 6.8088459968566895
Epoch 110, val loss: 1.7604148387908936
Epoch 120, training loss: 15.227252006530762 = 1.7379597425460815 + 2.0 * 6.744646072387695
Epoch 120, val loss: 1.7465373277664185
Epoch 130, training loss: 15.107864379882812 = 1.7214140892028809 + 2.0 * 6.693224906921387
Epoch 130, val loss: 1.7318943738937378
Epoch 140, training loss: 15.004199981689453 = 1.7032923698425293 + 2.0 * 6.650454044342041
Epoch 140, val loss: 1.7163243293762207
Epoch 150, training loss: 14.909128189086914 = 1.6833596229553223 + 2.0 * 6.612884521484375
Epoch 150, val loss: 1.6996448040008545
Epoch 160, training loss: 14.840827941894531 = 1.661143183708191 + 2.0 * 6.589842319488525
Epoch 160, val loss: 1.681191325187683
Epoch 170, training loss: 14.760580062866211 = 1.6360563039779663 + 2.0 * 6.562262058258057
Epoch 170, val loss: 1.6603140830993652
Epoch 180, training loss: 14.695892333984375 = 1.6076769828796387 + 2.0 * 6.544107437133789
Epoch 180, val loss: 1.6367106437683105
Epoch 190, training loss: 14.631330490112305 = 1.575629711151123 + 2.0 * 6.527850151062012
Epoch 190, val loss: 1.6101067066192627
Epoch 200, training loss: 14.577493667602539 = 1.5399055480957031 + 2.0 * 6.518794059753418
Epoch 200, val loss: 1.5806792974472046
Epoch 210, training loss: 14.5106840133667 = 1.5013290643692017 + 2.0 * 6.5046772956848145
Epoch 210, val loss: 1.5491297245025635
Epoch 220, training loss: 14.445369720458984 = 1.4601606130599976 + 2.0 * 6.492604732513428
Epoch 220, val loss: 1.5156676769256592
Epoch 230, training loss: 14.387344360351562 = 1.416465401649475 + 2.0 * 6.485439300537109
Epoch 230, val loss: 1.4805514812469482
Epoch 240, training loss: 14.31772232055664 = 1.3711578845977783 + 2.0 * 6.473282337188721
Epoch 240, val loss: 1.4445375204086304
Epoch 250, training loss: 14.253458976745605 = 1.3245400190353394 + 2.0 * 6.464459419250488
Epoch 250, val loss: 1.4080089330673218
Epoch 260, training loss: 14.189733505249023 = 1.2768657207489014 + 2.0 * 6.4564337730407715
Epoch 260, val loss: 1.3712749481201172
Epoch 270, training loss: 14.13050651550293 = 1.2290059328079224 + 2.0 * 6.450750350952148
Epoch 270, val loss: 1.3349707126617432
Epoch 280, training loss: 14.069475173950195 = 1.1822859048843384 + 2.0 * 6.443594455718994
Epoch 280, val loss: 1.3000147342681885
Epoch 290, training loss: 14.007347106933594 = 1.1364872455596924 + 2.0 * 6.43543004989624
Epoch 290, val loss: 1.2664002180099487
Epoch 300, training loss: 13.950565338134766 = 1.0915310382843018 + 2.0 * 6.4295172691345215
Epoch 300, val loss: 1.2339082956314087
Epoch 310, training loss: 13.900688171386719 = 1.0478672981262207 + 2.0 * 6.426410675048828
Epoch 310, val loss: 1.2028193473815918
Epoch 320, training loss: 13.840719223022461 = 1.0057555437088013 + 2.0 * 6.417481899261475
Epoch 320, val loss: 1.1733354330062866
Epoch 330, training loss: 13.793363571166992 = 0.9649247527122498 + 2.0 * 6.414219379425049
Epoch 330, val loss: 1.1452431678771973
Epoch 340, training loss: 13.73787784576416 = 0.9254474639892578 + 2.0 * 6.406215190887451
Epoch 340, val loss: 1.118548035621643
Epoch 350, training loss: 13.692532539367676 = 0.8872622847557068 + 2.0 * 6.402635097503662
Epoch 350, val loss: 1.0931223630905151
Epoch 360, training loss: 13.652225494384766 = 0.8504342436790466 + 2.0 * 6.400895595550537
Epoch 360, val loss: 1.0690375566482544
Epoch 370, training loss: 13.600969314575195 = 0.8151825070381165 + 2.0 * 6.392893314361572
Epoch 370, val loss: 1.046336054801941
Epoch 380, training loss: 13.556379318237305 = 0.7811098694801331 + 2.0 * 6.387634754180908
Epoch 380, val loss: 1.024824619293213
Epoch 390, training loss: 13.54758358001709 = 0.7481368184089661 + 2.0 * 6.399723529815674
Epoch 390, val loss: 1.0042794942855835
Epoch 400, training loss: 13.486001014709473 = 0.7168277502059937 + 2.0 * 6.384586811065674
Epoch 400, val loss: 0.9850612878799438
Epoch 410, training loss: 13.438794136047363 = 0.6869973540306091 + 2.0 * 6.375898361206055
Epoch 410, val loss: 0.9672634601593018
Epoch 420, training loss: 13.403887748718262 = 0.6582380533218384 + 2.0 * 6.372824668884277
Epoch 420, val loss: 0.9503558278083801
Epoch 430, training loss: 13.371288299560547 = 0.6304771304130554 + 2.0 * 6.370405673980713
Epoch 430, val loss: 0.9343791007995605
Epoch 440, training loss: 13.34683895111084 = 0.6038913726806641 + 2.0 * 6.371473789215088
Epoch 440, val loss: 0.9194815754890442
Epoch 450, training loss: 13.308597564697266 = 0.5785359740257263 + 2.0 * 6.365030765533447
Epoch 450, val loss: 0.905767023563385
Epoch 460, training loss: 13.27556324005127 = 0.5542327165603638 + 2.0 * 6.360665321350098
Epoch 460, val loss: 0.8930935263633728
Epoch 470, training loss: 13.256196975708008 = 0.530921220779419 + 2.0 * 6.362637996673584
Epoch 470, val loss: 0.8813275098800659
Epoch 480, training loss: 13.219103813171387 = 0.5086224675178528 + 2.0 * 6.355240821838379
Epoch 480, val loss: 0.8706183433532715
Epoch 490, training loss: 13.192638397216797 = 0.4872087240219116 + 2.0 * 6.352715015411377
Epoch 490, val loss: 0.8608490228652954
Epoch 500, training loss: 13.169037818908691 = 0.4665960967540741 + 2.0 * 6.351221084594727
Epoch 500, val loss: 0.8518929481506348
Epoch 510, training loss: 13.160754203796387 = 0.4468393921852112 + 2.0 * 6.35695743560791
Epoch 510, val loss: 0.8436603546142578
Epoch 520, training loss: 13.120766639709473 = 0.42804497480392456 + 2.0 * 6.346360683441162
Epoch 520, val loss: 0.8364328742027283
Epoch 530, training loss: 13.097941398620605 = 0.4099371135234833 + 2.0 * 6.3440022468566895
Epoch 530, val loss: 0.8299631476402283
Epoch 540, training loss: 13.074975967407227 = 0.3924458920955658 + 2.0 * 6.3412652015686035
Epoch 540, val loss: 0.8240761160850525
Epoch 550, training loss: 13.069756507873535 = 0.37550678849220276 + 2.0 * 6.347125053405762
Epoch 550, val loss: 0.8187695145606995
Epoch 560, training loss: 13.040342330932617 = 0.3591363728046417 + 2.0 * 6.340602874755859
Epoch 560, val loss: 0.8140617609024048
Epoch 570, training loss: 13.01798152923584 = 0.34337303042411804 + 2.0 * 6.33730411529541
Epoch 570, val loss: 0.809849202632904
Epoch 580, training loss: 12.996914863586426 = 0.32806479930877686 + 2.0 * 6.33442497253418
Epoch 580, val loss: 0.8061602711677551
Epoch 590, training loss: 12.979363441467285 = 0.31316763162612915 + 2.0 * 6.3330979347229
Epoch 590, val loss: 0.8028721809387207
Epoch 600, training loss: 12.963558197021484 = 0.2986845374107361 + 2.0 * 6.332437038421631
Epoch 600, val loss: 0.7998932003974915
Epoch 610, training loss: 12.948065757751465 = 0.2846684753894806 + 2.0 * 6.331698417663574
Epoch 610, val loss: 0.7974514365196228
Epoch 620, training loss: 12.93147087097168 = 0.27106937766075134 + 2.0 * 6.330200672149658
Epoch 620, val loss: 0.7953775525093079
Epoch 630, training loss: 12.915759086608887 = 0.25787314772605896 + 2.0 * 6.328942775726318
Epoch 630, val loss: 0.7937160730361938
Epoch 640, training loss: 12.897615432739258 = 0.2450997233390808 + 2.0 * 6.326257705688477
Epoch 640, val loss: 0.7924196720123291
Epoch 650, training loss: 12.883414268493652 = 0.232744961977005 + 2.0 * 6.325334548950195
Epoch 650, val loss: 0.7916548252105713
Epoch 660, training loss: 12.866178512573242 = 0.22087253630161285 + 2.0 * 6.322652816772461
Epoch 660, val loss: 0.7913022637367249
Epoch 670, training loss: 12.851814270019531 = 0.209432452917099 + 2.0 * 6.32119083404541
Epoch 670, val loss: 0.7914209961891174
Epoch 680, training loss: 12.838838577270508 = 0.19849705696105957 + 2.0 * 6.320170879364014
Epoch 680, val loss: 0.7919780611991882
Epoch 690, training loss: 12.824304580688477 = 0.18809659779071808 + 2.0 * 6.318103790283203
Epoch 690, val loss: 0.793052077293396
Epoch 700, training loss: 12.82005786895752 = 0.1782083958387375 + 2.0 * 6.320924758911133
Epoch 700, val loss: 0.7946510910987854
Epoch 710, training loss: 12.805587768554688 = 0.1688675880432129 + 2.0 * 6.318359851837158
Epoch 710, val loss: 0.7965776324272156
Epoch 720, training loss: 12.789613723754883 = 0.16006144881248474 + 2.0 * 6.3147759437561035
Epoch 720, val loss: 0.7990944981575012
Epoch 730, training loss: 12.779486656188965 = 0.15177327394485474 + 2.0 * 6.313856601715088
Epoch 730, val loss: 0.8019961714744568
Epoch 740, training loss: 12.771111488342285 = 0.14397461712360382 + 2.0 * 6.313568592071533
Epoch 740, val loss: 0.8052571415901184
Epoch 750, training loss: 12.761245727539062 = 0.1366705596446991 + 2.0 * 6.3122878074646
Epoch 750, val loss: 0.8089430928230286
Epoch 760, training loss: 12.750800132751465 = 0.12982162833213806 + 2.0 * 6.310489177703857
Epoch 760, val loss: 0.812940239906311
Epoch 770, training loss: 12.74412727355957 = 0.12340262532234192 + 2.0 * 6.310362339019775
Epoch 770, val loss: 0.8173124194145203
Epoch 780, training loss: 12.734707832336426 = 0.11738497018814087 + 2.0 * 6.308661460876465
Epoch 780, val loss: 0.8219284415245056
Epoch 790, training loss: 12.723832130432129 = 0.11175721138715744 + 2.0 * 6.306037425994873
Epoch 790, val loss: 0.8267989754676819
Epoch 800, training loss: 12.718730926513672 = 0.10648825764656067 + 2.0 * 6.306121349334717
Epoch 800, val loss: 0.8319461941719055
Epoch 810, training loss: 12.714019775390625 = 0.10153929144144058 + 2.0 * 6.306240081787109
Epoch 810, val loss: 0.8372319340705872
Epoch 820, training loss: 12.707240104675293 = 0.09690297394990921 + 2.0 * 6.305168628692627
Epoch 820, val loss: 0.8427372574806213
Epoch 830, training loss: 12.699838638305664 = 0.0925641655921936 + 2.0 * 6.3036370277404785
Epoch 830, val loss: 0.8483611345291138
Epoch 840, training loss: 12.693318367004395 = 0.08847452700138092 + 2.0 * 6.302422046661377
Epoch 840, val loss: 0.8541824221611023
Epoch 850, training loss: 12.686295509338379 = 0.08462606370449066 + 2.0 * 6.300834655761719
Epoch 850, val loss: 0.8601279854774475
Epoch 860, training loss: 12.684917449951172 = 0.08100739866495132 + 2.0 * 6.301955223083496
Epoch 860, val loss: 0.8661324977874756
Epoch 870, training loss: 12.681214332580566 = 0.07759218662977219 + 2.0 * 6.301811218261719
Epoch 870, val loss: 0.8722562193870544
Epoch 880, training loss: 12.67300796508789 = 0.07438826560974121 + 2.0 * 6.299309730529785
Epoch 880, val loss: 0.878325343132019
Epoch 890, training loss: 12.66351318359375 = 0.0713663324713707 + 2.0 * 6.2960734367370605
Epoch 890, val loss: 0.8845564723014832
Epoch 900, training loss: 12.659361839294434 = 0.06850738823413849 + 2.0 * 6.295427322387695
Epoch 900, val loss: 0.8908187747001648
Epoch 910, training loss: 12.666850090026855 = 0.06579919904470444 + 2.0 * 6.300525665283203
Epoch 910, val loss: 0.8971073031425476
Epoch 920, training loss: 12.662487030029297 = 0.06322647631168365 + 2.0 * 6.299630165100098
Epoch 920, val loss: 0.9033836126327515
Epoch 930, training loss: 12.648836135864258 = 0.060810524970293045 + 2.0 * 6.294013023376465
Epoch 930, val loss: 0.9097299575805664
Epoch 940, training loss: 12.65457820892334 = 0.05851150304079056 + 2.0 * 6.298033237457275
Epoch 940, val loss: 0.9160443544387817
Epoch 950, training loss: 12.642210960388184 = 0.056339967995882034 + 2.0 * 6.292935371398926
Epoch 950, val loss: 0.922325074672699
Epoch 960, training loss: 12.63510513305664 = 0.054277628660202026 + 2.0 * 6.290413856506348
Epoch 960, val loss: 0.9286471605300903
Epoch 970, training loss: 12.634446144104004 = 0.052316416054964066 + 2.0 * 6.291064739227295
Epoch 970, val loss: 0.9349642992019653
Epoch 980, training loss: 12.628233909606934 = 0.050451792776584625 + 2.0 * 6.288890838623047
Epoch 980, val loss: 0.941233217716217
Epoch 990, training loss: 12.62938404083252 = 0.04868040606379509 + 2.0 * 6.290351867675781
Epoch 990, val loss: 0.9475112557411194
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8185185185185185
0.8376383763837639
=== training gcn model ===
Epoch 0, training loss: 19.13916015625 = 1.9454494714736938 + 2.0 * 8.596855163574219
Epoch 0, val loss: 1.9449833631515503
Epoch 10, training loss: 19.127933502197266 = 1.9347734451293945 + 2.0 * 8.596579551696777
Epoch 10, val loss: 1.9337589740753174
Epoch 20, training loss: 19.109268188476562 = 1.9208264350891113 + 2.0 * 8.594221115112305
Epoch 20, val loss: 1.9189308881759644
Epoch 30, training loss: 19.052003860473633 = 1.9010273218154907 + 2.0 * 8.575488090515137
Epoch 30, val loss: 1.8981201648712158
Epoch 40, training loss: 18.810686111450195 = 1.8758978843688965 + 2.0 * 8.46739387512207
Epoch 40, val loss: 1.8733372688293457
Epoch 50, training loss: 18.039424896240234 = 1.8493226766586304 + 2.0 * 8.095050811767578
Epoch 50, val loss: 1.847929835319519
Epoch 60, training loss: 17.492029190063477 = 1.8261886835098267 + 2.0 * 7.832920551300049
Epoch 60, val loss: 1.8264880180358887
Epoch 70, training loss: 16.73539161682129 = 1.8108164072036743 + 2.0 * 7.462287425994873
Epoch 70, val loss: 1.8130242824554443
Epoch 80, training loss: 16.072290420532227 = 1.802489161491394 + 2.0 * 7.13490104675293
Epoch 80, val loss: 1.8060098886489868
Epoch 90, training loss: 15.690128326416016 = 1.7922652959823608 + 2.0 * 6.948931694030762
Epoch 90, val loss: 1.796193242073059
Epoch 100, training loss: 15.483054161071777 = 1.7783573865890503 + 2.0 * 6.852348327636719
Epoch 100, val loss: 1.7833898067474365
Epoch 110, training loss: 15.32839298248291 = 1.764543890953064 + 2.0 * 6.781924724578857
Epoch 110, val loss: 1.7708033323287964
Epoch 120, training loss: 15.19174861907959 = 1.7515690326690674 + 2.0 * 6.720089912414551
Epoch 120, val loss: 1.758644938468933
Epoch 130, training loss: 15.07472038269043 = 1.7377216815948486 + 2.0 * 6.66849946975708
Epoch 130, val loss: 1.7455345392227173
Epoch 140, training loss: 14.985204696655273 = 1.721926212310791 + 2.0 * 6.63163948059082
Epoch 140, val loss: 1.7308599948883057
Epoch 150, training loss: 14.910171508789062 = 1.7040752172470093 + 2.0 * 6.603048324584961
Epoch 150, val loss: 1.7144702672958374
Epoch 160, training loss: 14.847461700439453 = 1.6840318441390991 + 2.0 * 6.581715106964111
Epoch 160, val loss: 1.696153998374939
Epoch 170, training loss: 14.784614562988281 = 1.6616774797439575 + 2.0 * 6.561468601226807
Epoch 170, val loss: 1.6757539510726929
Epoch 180, training loss: 14.72525691986084 = 1.6366596221923828 + 2.0 * 6.5442986488342285
Epoch 180, val loss: 1.6530009508132935
Epoch 190, training loss: 14.675479888916016 = 1.6088281869888306 + 2.0 * 6.533325672149658
Epoch 190, val loss: 1.6278131008148193
Epoch 200, training loss: 14.607431411743164 = 1.5787535905838013 + 2.0 * 6.514338970184326
Epoch 200, val loss: 1.6007459163665771
Epoch 210, training loss: 14.548479080200195 = 1.5465257167816162 + 2.0 * 6.5009765625
Epoch 210, val loss: 1.571986198425293
Epoch 220, training loss: 14.48840618133545 = 1.512285590171814 + 2.0 * 6.488060474395752
Epoch 220, val loss: 1.541688323020935
Epoch 230, training loss: 14.433675765991211 = 1.4768294095993042 + 2.0 * 6.478423118591309
Epoch 230, val loss: 1.5107911825180054
Epoch 240, training loss: 14.37435531616211 = 1.4409127235412598 + 2.0 * 6.466721057891846
Epoch 240, val loss: 1.479992151260376
Epoch 250, training loss: 14.316268920898438 = 1.4045076370239258 + 2.0 * 6.455880641937256
Epoch 250, val loss: 1.4492028951644897
Epoch 260, training loss: 14.27049446105957 = 1.3679499626159668 + 2.0 * 6.451272487640381
Epoch 260, val loss: 1.4188512563705444
Epoch 270, training loss: 14.215513229370117 = 1.332328200340271 + 2.0 * 6.441592693328857
Epoch 270, val loss: 1.389955997467041
Epoch 280, training loss: 14.161080360412598 = 1.2974746227264404 + 2.0 * 6.431802749633789
Epoch 280, val loss: 1.3621543645858765
Epoch 290, training loss: 14.112564086914062 = 1.2631510496139526 + 2.0 * 6.42470645904541
Epoch 290, val loss: 1.3351629972457886
Epoch 300, training loss: 14.065927505493164 = 1.229336142539978 + 2.0 * 6.418295860290527
Epoch 300, val loss: 1.3089197874069214
Epoch 310, training loss: 14.025341987609863 = 1.196128010749817 + 2.0 * 6.414607048034668
Epoch 310, val loss: 1.2834789752960205
Epoch 320, training loss: 13.977806091308594 = 1.1633555889129639 + 2.0 * 6.407225131988525
Epoch 320, val loss: 1.2587255239486694
Epoch 330, training loss: 13.933801651000977 = 1.1308060884475708 + 2.0 * 6.401497840881348
Epoch 330, val loss: 1.2342826128005981
Epoch 340, training loss: 13.900564193725586 = 1.0983160734176636 + 2.0 * 6.401124000549316
Epoch 340, val loss: 1.2100813388824463
Epoch 350, training loss: 13.85110092163086 = 1.0661011934280396 + 2.0 * 6.392499923706055
Epoch 350, val loss: 1.1862045526504517
Epoch 360, training loss: 13.810016632080078 = 1.0341070890426636 + 2.0 * 6.3879547119140625
Epoch 360, val loss: 1.1626628637313843
Epoch 370, training loss: 13.781938552856445 = 1.002379298210144 + 2.0 * 6.389779567718506
Epoch 370, val loss: 1.1395103931427002
Epoch 380, training loss: 13.731303215026855 = 0.9711677432060242 + 2.0 * 6.380067825317383
Epoch 380, val loss: 1.116816759109497
Epoch 390, training loss: 13.6935453414917 = 0.9405887126922607 + 2.0 * 6.37647819519043
Epoch 390, val loss: 1.0947744846343994
Epoch 400, training loss: 13.670709609985352 = 0.9107347130775452 + 2.0 * 6.3799872398376465
Epoch 400, val loss: 1.0734333992004395
Epoch 410, training loss: 13.62317180633545 = 0.8817060589790344 + 2.0 * 6.37073278427124
Epoch 410, val loss: 1.0529541969299316
Epoch 420, training loss: 13.590487480163574 = 0.8535677194595337 + 2.0 * 6.368459701538086
Epoch 420, val loss: 1.0334033966064453
Epoch 430, training loss: 13.557141304016113 = 0.8262744545936584 + 2.0 * 6.365433216094971
Epoch 430, val loss: 1.0147106647491455
Epoch 440, training loss: 13.522904396057129 = 0.7999187707901001 + 2.0 * 6.36149263381958
Epoch 440, val loss: 0.9969448447227478
Epoch 450, training loss: 13.491976737976074 = 0.7742561101913452 + 2.0 * 6.358860492706299
Epoch 450, val loss: 0.9799575209617615
Epoch 460, training loss: 13.462564468383789 = 0.7492342591285706 + 2.0 * 6.356665134429932
Epoch 460, val loss: 0.9637231826782227
Epoch 470, training loss: 13.433003425598145 = 0.7249074578285217 + 2.0 * 6.354047775268555
Epoch 470, val loss: 0.9482033252716064
Epoch 480, training loss: 13.405942916870117 = 0.7010894417762756 + 2.0 * 6.352426528930664
Epoch 480, val loss: 0.9332780241966248
Epoch 490, training loss: 13.37296199798584 = 0.677764892578125 + 2.0 * 6.347598552703857
Epoch 490, val loss: 0.9189240336418152
Epoch 500, training loss: 13.358858108520508 = 0.6548599600791931 + 2.0 * 6.351999282836914
Epoch 500, val loss: 0.9050400853157043
Epoch 510, training loss: 13.324529647827148 = 0.6324474215507507 + 2.0 * 6.346041202545166
Epoch 510, val loss: 0.8916364312171936
Epoch 520, training loss: 13.292867660522461 = 0.6105107069015503 + 2.0 * 6.3411784172058105
Epoch 520, val loss: 0.8787091374397278
Epoch 530, training loss: 13.266362190246582 = 0.5888588428497314 + 2.0 * 6.338751792907715
Epoch 530, val loss: 0.8661128878593445
Epoch 540, training loss: 13.253820419311523 = 0.5674489140510559 + 2.0 * 6.343185901641846
Epoch 540, val loss: 0.8538603782653809
Epoch 550, training loss: 13.224864959716797 = 0.5465189814567566 + 2.0 * 6.339172840118408
Epoch 550, val loss: 0.8419722318649292
Epoch 560, training loss: 13.193742752075195 = 0.5259334444999695 + 2.0 * 6.33390474319458
Epoch 560, val loss: 0.8304964303970337
Epoch 570, training loss: 13.167671203613281 = 0.5056604743003845 + 2.0 * 6.331005573272705
Epoch 570, val loss: 0.819306492805481
Epoch 580, training loss: 13.147957801818848 = 0.48567911982536316 + 2.0 * 6.33113956451416
Epoch 580, val loss: 0.8085043430328369
Epoch 590, training loss: 13.125990867614746 = 0.465992271900177 + 2.0 * 6.3299994468688965
Epoch 590, val loss: 0.7981230020523071
Epoch 600, training loss: 13.09945011138916 = 0.44680434465408325 + 2.0 * 6.32632303237915
Epoch 600, val loss: 0.7882137298583984
Epoch 610, training loss: 13.077105522155762 = 0.4279586374759674 + 2.0 * 6.324573516845703
Epoch 610, val loss: 0.7788342237472534
Epoch 620, training loss: 13.064358711242676 = 0.40949586033821106 + 2.0 * 6.3274312019348145
Epoch 620, val loss: 0.7699869871139526
Epoch 630, training loss: 13.034788131713867 = 0.3915177583694458 + 2.0 * 6.3216352462768555
Epoch 630, val loss: 0.761773407459259
Epoch 640, training loss: 13.018696784973145 = 0.37400737404823303 + 2.0 * 6.322344779968262
Epoch 640, val loss: 0.754101574420929
Epoch 650, training loss: 12.996187210083008 = 0.35696426033973694 + 2.0 * 6.319611549377441
Epoch 650, val loss: 0.7469618320465088
Epoch 660, training loss: 12.974471092224121 = 0.340400367975235 + 2.0 * 6.31703519821167
Epoch 660, val loss: 0.7405604720115662
Epoch 670, training loss: 12.954605102539062 = 0.3242958188056946 + 2.0 * 6.315154552459717
Epoch 670, val loss: 0.7346961498260498
Epoch 680, training loss: 12.945390701293945 = 0.3086751699447632 + 2.0 * 6.318357944488525
Epoch 680, val loss: 0.7294076681137085
Epoch 690, training loss: 12.923598289489746 = 0.2935638725757599 + 2.0 * 6.315017223358154
Epoch 690, val loss: 0.7246581315994263
Epoch 700, training loss: 12.900283813476562 = 0.2790057957172394 + 2.0 * 6.310638904571533
Epoch 700, val loss: 0.7206122279167175
Epoch 710, training loss: 12.883350372314453 = 0.2649286091327667 + 2.0 * 6.309210777282715
Epoch 710, val loss: 0.7170766592025757
Epoch 720, training loss: 12.876744270324707 = 0.2513209283351898 + 2.0 * 6.312711715698242
Epoch 720, val loss: 0.7141188383102417
Epoch 730, training loss: 12.867764472961426 = 0.238333597779274 + 2.0 * 6.314715385437012
Epoch 730, val loss: 0.7116929888725281
Epoch 740, training loss: 12.839112281799316 = 0.22592926025390625 + 2.0 * 6.306591510772705
Epoch 740, val loss: 0.7098217010498047
Epoch 750, training loss: 12.823038101196289 = 0.21409323811531067 + 2.0 * 6.30447244644165
Epoch 750, val loss: 0.7083781361579895
Epoch 760, training loss: 12.815694808959961 = 0.20286095142364502 + 2.0 * 6.306416988372803
Epoch 760, val loss: 0.7074793577194214
Epoch 770, training loss: 12.803107261657715 = 0.19220131635665894 + 2.0 * 6.305452823638916
Epoch 770, val loss: 0.7069578766822815
Epoch 780, training loss: 12.785379409790039 = 0.18216989934444427 + 2.0 * 6.301604747772217
Epoch 780, val loss: 0.707018256187439
Epoch 790, training loss: 12.774994850158691 = 0.1727031022310257 + 2.0 * 6.301146030426025
Epoch 790, val loss: 0.707392692565918
Epoch 800, training loss: 12.765835762023926 = 0.16381491720676422 + 2.0 * 6.301010608673096
Epoch 800, val loss: 0.7081602215766907
Epoch 810, training loss: 12.75400447845459 = 0.15546882152557373 + 2.0 * 6.299267768859863
Epoch 810, val loss: 0.7091703414916992
Epoch 820, training loss: 12.740809440612793 = 0.14765898883342743 + 2.0 * 6.29657506942749
Epoch 820, val loss: 0.7105241417884827
Epoch 830, training loss: 12.730687141418457 = 0.1403416097164154 + 2.0 * 6.295172691345215
Epoch 830, val loss: 0.7121108174324036
Epoch 840, training loss: 12.733072280883789 = 0.13346603512763977 + 2.0 * 6.299803256988525
Epoch 840, val loss: 0.713991641998291
Epoch 850, training loss: 12.719402313232422 = 0.12701447308063507 + 2.0 * 6.296194076538086
Epoch 850, val loss: 0.7161482572555542
Epoch 860, training loss: 12.705963134765625 = 0.12098150700330734 + 2.0 * 6.2924909591674805
Epoch 860, val loss: 0.7183727025985718
Epoch 870, training loss: 12.6978759765625 = 0.11531755328178406 + 2.0 * 6.291279315948486
Epoch 870, val loss: 0.7208842635154724
Epoch 880, training loss: 12.707096099853516 = 0.11000140756368637 + 2.0 * 6.298547267913818
Epoch 880, val loss: 0.7236015796661377
Epoch 890, training loss: 12.689314842224121 = 0.10498403012752533 + 2.0 * 6.292165279388428
Epoch 890, val loss: 0.7261695861816406
Epoch 900, training loss: 12.685159683227539 = 0.10029703378677368 + 2.0 * 6.292431354522705
Epoch 900, val loss: 0.7291331887245178
Epoch 910, training loss: 12.673529624938965 = 0.09589879214763641 + 2.0 * 6.288815498352051
Epoch 910, val loss: 0.7321534752845764
Epoch 920, training loss: 12.665940284729004 = 0.09174453467130661 + 2.0 * 6.287097930908203
Epoch 920, val loss: 0.735214352607727
Epoch 930, training loss: 12.663503646850586 = 0.08782988041639328 + 2.0 * 6.287837028503418
Epoch 930, val loss: 0.7384635806083679
Epoch 940, training loss: 12.658238410949707 = 0.0841405913233757 + 2.0 * 6.287048816680908
Epoch 940, val loss: 0.7418020963668823
Epoch 950, training loss: 12.648087501525879 = 0.08064880967140198 + 2.0 * 6.283719539642334
Epoch 950, val loss: 0.7451651692390442
Epoch 960, training loss: 12.659250259399414 = 0.07734847068786621 + 2.0 * 6.290950775146484
Epoch 960, val loss: 0.7485618591308594
Epoch 970, training loss: 12.645669937133789 = 0.07425061613321304 + 2.0 * 6.285709857940674
Epoch 970, val loss: 0.7521435022354126
Epoch 980, training loss: 12.635327339172363 = 0.07130103558301926 + 2.0 * 6.282012939453125
Epoch 980, val loss: 0.755663275718689
Epoch 990, training loss: 12.629968643188477 = 0.06852183490991592 + 2.0 * 6.280723571777344
Epoch 990, val loss: 0.7592835426330566
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7814814814814816
0.8376383763837639
=== training gcn model ===
Epoch 0, training loss: 19.145082473754883 = 1.9513801336288452 + 2.0 * 8.596851348876953
Epoch 0, val loss: 1.9512137174606323
Epoch 10, training loss: 19.134262084960938 = 1.9412447214126587 + 2.0 * 8.596508979797363
Epoch 10, val loss: 1.9407023191452026
Epoch 20, training loss: 19.116104125976562 = 1.9287912845611572 + 2.0 * 8.593656539916992
Epoch 20, val loss: 1.9277011156082153
Epoch 30, training loss: 19.04849624633789 = 1.912188172340393 + 2.0 * 8.568154335021973
Epoch 30, val loss: 1.9104992151260376
Epoch 40, training loss: 18.644092559814453 = 1.8919036388397217 + 2.0 * 8.376094818115234
Epoch 40, val loss: 1.8898918628692627
Epoch 50, training loss: 17.382823944091797 = 1.8717108964920044 + 2.0 * 7.755556106567383
Epoch 50, val loss: 1.8703974485397339
Epoch 60, training loss: 16.389137268066406 = 1.857568621635437 + 2.0 * 7.26578426361084
Epoch 60, val loss: 1.8576499223709106
Epoch 70, training loss: 15.867425918579102 = 1.8444558382034302 + 2.0 * 7.0114850997924805
Epoch 70, val loss: 1.8449174165725708
Epoch 80, training loss: 15.622472763061523 = 1.8303556442260742 + 2.0 * 6.896058559417725
Epoch 80, val loss: 1.831403136253357
Epoch 90, training loss: 15.394196510314941 = 1.8159421682357788 + 2.0 * 6.789127349853516
Epoch 90, val loss: 1.8178592920303345
Epoch 100, training loss: 15.248804092407227 = 1.8027759790420532 + 2.0 * 6.723013877868652
Epoch 100, val loss: 1.8054900169372559
Epoch 110, training loss: 15.138622283935547 = 1.7900782823562622 + 2.0 * 6.674272060394287
Epoch 110, val loss: 1.7934390306472778
Epoch 120, training loss: 15.045239448547363 = 1.7773773670196533 + 2.0 * 6.6339311599731445
Epoch 120, val loss: 1.781601905822754
Epoch 130, training loss: 14.966102600097656 = 1.7642405033111572 + 2.0 * 6.600931167602539
Epoch 130, val loss: 1.7696970701217651
Epoch 140, training loss: 14.89275074005127 = 1.7503801584243774 + 2.0 * 6.571185111999512
Epoch 140, val loss: 1.7571624517440796
Epoch 150, training loss: 14.845783233642578 = 1.7353378534317017 + 2.0 * 6.555222511291504
Epoch 150, val loss: 1.7437896728515625
Epoch 160, training loss: 14.779212951660156 = 1.7187780141830444 + 2.0 * 6.53021764755249
Epoch 160, val loss: 1.729109287261963
Epoch 170, training loss: 14.726924896240234 = 1.7001256942749023 + 2.0 * 6.513399600982666
Epoch 170, val loss: 1.7127981185913086
Epoch 180, training loss: 14.675395965576172 = 1.679085612297058 + 2.0 * 6.498155117034912
Epoch 180, val loss: 1.6946241855621338
Epoch 190, training loss: 14.625311851501465 = 1.6547762155532837 + 2.0 * 6.485267639160156
Epoch 190, val loss: 1.6737689971923828
Epoch 200, training loss: 14.582048416137695 = 1.6270670890808105 + 2.0 * 6.477490425109863
Epoch 200, val loss: 1.6502991914749146
Epoch 210, training loss: 14.523094177246094 = 1.5961909294128418 + 2.0 * 6.463451385498047
Epoch 210, val loss: 1.6243634223937988
Epoch 220, training loss: 14.471193313598633 = 1.5618219375610352 + 2.0 * 6.454685688018799
Epoch 220, val loss: 1.5956428050994873
Epoch 230, training loss: 14.418739318847656 = 1.5243560075759888 + 2.0 * 6.4471917152404785
Epoch 230, val loss: 1.564383625984192
Epoch 240, training loss: 14.360038757324219 = 1.4837726354599 + 2.0 * 6.438133239746094
Epoch 240, val loss: 1.5308194160461426
Epoch 250, training loss: 14.312010765075684 = 1.4405626058578491 + 2.0 * 6.435724258422852
Epoch 250, val loss: 1.495116949081421
Epoch 260, training loss: 14.24623966217041 = 1.395540475845337 + 2.0 * 6.425349712371826
Epoch 260, val loss: 1.4581477642059326
Epoch 270, training loss: 14.187485694885254 = 1.3493870496749878 + 2.0 * 6.419049263000488
Epoch 270, val loss: 1.4205747842788696
Epoch 280, training loss: 14.12825870513916 = 1.3026092052459717 + 2.0 * 6.412824630737305
Epoch 280, val loss: 1.382480502128601
Epoch 290, training loss: 14.0730562210083 = 1.2560755014419556 + 2.0 * 6.408490180969238
Epoch 290, val loss: 1.3449538946151733
Epoch 300, training loss: 14.021587371826172 = 1.2113479375839233 + 2.0 * 6.405119895935059
Epoch 300, val loss: 1.3090964555740356
Epoch 310, training loss: 13.962823867797852 = 1.1679966449737549 + 2.0 * 6.397413730621338
Epoch 310, val loss: 1.2746734619140625
Epoch 320, training loss: 13.912073135375977 = 1.1260197162628174 + 2.0 * 6.393026828765869
Epoch 320, val loss: 1.2414556741714478
Epoch 330, training loss: 13.874492645263672 = 1.0858910083770752 + 2.0 * 6.394300937652588
Epoch 330, val loss: 1.2098703384399414
Epoch 340, training loss: 13.821805953979492 = 1.0480022430419922 + 2.0 * 6.38690185546875
Epoch 340, val loss: 1.180246114730835
Epoch 350, training loss: 13.774162292480469 = 1.011857509613037 + 2.0 * 6.381152629852295
Epoch 350, val loss: 1.1520798206329346
Epoch 360, training loss: 13.73100757598877 = 0.9769999980926514 + 2.0 * 6.3770036697387695
Epoch 360, val loss: 1.1249101161956787
Epoch 370, training loss: 13.71043872833252 = 0.9433017373085022 + 2.0 * 6.383568286895752
Epoch 370, val loss: 1.0985723733901978
Epoch 380, training loss: 13.652984619140625 = 0.9108385443687439 + 2.0 * 6.371073246002197
Epoch 380, val loss: 1.0734423398971558
Epoch 390, training loss: 13.614383697509766 = 0.8795695304870605 + 2.0 * 6.367407321929932
Epoch 390, val loss: 1.0492771863937378
Epoch 400, training loss: 13.578539848327637 = 0.849228024482727 + 2.0 * 6.3646559715271
Epoch 400, val loss: 1.0257794857025146
Epoch 410, training loss: 13.541398048400879 = 0.819793701171875 + 2.0 * 6.360802173614502
Epoch 410, val loss: 1.0031148195266724
Epoch 420, training loss: 13.508891105651855 = 0.7913247346878052 + 2.0 * 6.35878324508667
Epoch 420, val loss: 0.9813824892044067
Epoch 430, training loss: 13.48031234741211 = 0.763781726360321 + 2.0 * 6.358265399932861
Epoch 430, val loss: 0.9606553912162781
Epoch 440, training loss: 13.449063301086426 = 0.7372058033943176 + 2.0 * 6.355928897857666
Epoch 440, val loss: 0.9409323930740356
Epoch 450, training loss: 13.414115905761719 = 0.7116890549659729 + 2.0 * 6.351213455200195
Epoch 450, val loss: 0.9224602580070496
Epoch 460, training loss: 13.392521858215332 = 0.6870458722114563 + 2.0 * 6.352737903594971
Epoch 460, val loss: 0.9051045775413513
Epoch 470, training loss: 13.360197067260742 = 0.6633456945419312 + 2.0 * 6.34842586517334
Epoch 470, val loss: 0.8887022733688354
Epoch 480, training loss: 13.325433731079102 = 0.6404154896736145 + 2.0 * 6.3425092697143555
Epoch 480, val loss: 0.8735335469245911
Epoch 490, training loss: 13.300578117370605 = 0.6181586980819702 + 2.0 * 6.341209888458252
Epoch 490, val loss: 0.8593711853027344
Epoch 500, training loss: 13.277688980102539 = 0.5965796709060669 + 2.0 * 6.340554714202881
Epoch 500, val loss: 0.8460968136787415
Epoch 510, training loss: 13.248991012573242 = 0.5756826400756836 + 2.0 * 6.336654186248779
Epoch 510, val loss: 0.8340228796005249
Epoch 520, training loss: 13.229013442993164 = 0.5553864240646362 + 2.0 * 6.336813449859619
Epoch 520, val loss: 0.822855532169342
Epoch 530, training loss: 13.205230712890625 = 0.5356775522232056 + 2.0 * 6.334776401519775
Epoch 530, val loss: 0.8124946355819702
Epoch 540, training loss: 13.184840202331543 = 0.5165329575538635 + 2.0 * 6.334153652191162
Epoch 540, val loss: 0.8031343817710876
Epoch 550, training loss: 13.156390190124512 = 0.4979217052459717 + 2.0 * 6.3292341232299805
Epoch 550, val loss: 0.794507622718811
Epoch 560, training loss: 13.137260437011719 = 0.47977226972579956 + 2.0 * 6.328743934631348
Epoch 560, val loss: 0.7866966128349304
Epoch 570, training loss: 13.123605728149414 = 0.46207934617996216 + 2.0 * 6.330763339996338
Epoch 570, val loss: 0.7796199321746826
Epoch 580, training loss: 13.09659194946289 = 0.4448390603065491 + 2.0 * 6.325876235961914
Epoch 580, val loss: 0.7732590436935425
Epoch 590, training loss: 13.075142860412598 = 0.4281012713909149 + 2.0 * 6.323520660400391
Epoch 590, val loss: 0.7676223516464233
Epoch 600, training loss: 13.065635681152344 = 0.4118415117263794 + 2.0 * 6.326897144317627
Epoch 600, val loss: 0.7625212669372559
Epoch 610, training loss: 13.043068885803223 = 0.39609232544898987 + 2.0 * 6.323488235473633
Epoch 610, val loss: 0.7580730319023132
Epoch 620, training loss: 13.020273208618164 = 0.38092589378356934 + 2.0 * 6.319673538208008
Epoch 620, val loss: 0.7541831135749817
Epoch 630, training loss: 13.00065803527832 = 0.3662499785423279 + 2.0 * 6.317203998565674
Epoch 630, val loss: 0.7508974075317383
Epoch 640, training loss: 12.99622917175293 = 0.35207101702690125 + 2.0 * 6.322079181671143
Epoch 640, val loss: 0.7481580972671509
Epoch 650, training loss: 12.972729682922363 = 0.33838361501693726 + 2.0 * 6.317173004150391
Epoch 650, val loss: 0.7458112239837646
Epoch 660, training loss: 12.97210693359375 = 0.32523947954177856 + 2.0 * 6.323433876037598
Epoch 660, val loss: 0.743989109992981
Epoch 670, training loss: 12.940707206726074 = 0.3125826120376587 + 2.0 * 6.314062118530273
Epoch 670, val loss: 0.7425934672355652
Epoch 680, training loss: 12.922536849975586 = 0.3003980219364166 + 2.0 * 6.311069488525391
Epoch 680, val loss: 0.7415876984596252
Epoch 690, training loss: 12.917572021484375 = 0.2885757386684418 + 2.0 * 6.314497947692871
Epoch 690, val loss: 0.7409897446632385
Epoch 700, training loss: 12.908819198608398 = 0.2771552801132202 + 2.0 * 6.315832138061523
Epoch 700, val loss: 0.7407718300819397
Epoch 710, training loss: 12.883151054382324 = 0.26610511541366577 + 2.0 * 6.308523178100586
Epoch 710, val loss: 0.7407858371734619
Epoch 720, training loss: 12.867501258850098 = 0.25538843870162964 + 2.0 * 6.306056499481201
Epoch 720, val loss: 0.7411028742790222
Epoch 730, training loss: 12.855999946594238 = 0.24495187401771545 + 2.0 * 6.305523872375488
Epoch 730, val loss: 0.7416634559631348
Epoch 740, training loss: 12.868765830993652 = 0.23477685451507568 + 2.0 * 6.316994667053223
Epoch 740, val loss: 0.7423982620239258
Epoch 750, training loss: 12.835771560668945 = 0.22491568326950073 + 2.0 * 6.3054280281066895
Epoch 750, val loss: 0.7433565855026245
Epoch 760, training loss: 12.825488090515137 = 0.21532197296619415 + 2.0 * 6.305083274841309
Epoch 760, val loss: 0.7445247769355774
Epoch 770, training loss: 12.808523178100586 = 0.20602580904960632 + 2.0 * 6.301248550415039
Epoch 770, val loss: 0.7458550333976746
Epoch 780, training loss: 12.799089431762695 = 0.19701725244522095 + 2.0 * 6.3010358810424805
Epoch 780, val loss: 0.747413694858551
Epoch 790, training loss: 12.794160842895508 = 0.18829169869422913 + 2.0 * 6.302934646606445
Epoch 790, val loss: 0.7491999864578247
Epoch 800, training loss: 12.785388946533203 = 0.17986363172531128 + 2.0 * 6.302762508392334
Epoch 800, val loss: 0.7510691285133362
Epoch 810, training loss: 12.76982593536377 = 0.17180615663528442 + 2.0 * 6.299009799957275
Epoch 810, val loss: 0.7531470060348511
Epoch 820, training loss: 12.757956504821777 = 0.1640927493572235 + 2.0 * 6.296931743621826
Epoch 820, val loss: 0.7554076313972473
Epoch 830, training loss: 12.751131057739258 = 0.15671953558921814 + 2.0 * 6.297205924987793
Epoch 830, val loss: 0.7579450607299805
Epoch 840, training loss: 12.747676849365234 = 0.14967425167560577 + 2.0 * 6.299001216888428
Epoch 840, val loss: 0.7605843544006348
Epoch 850, training loss: 12.734729766845703 = 0.14295585453510284 + 2.0 * 6.295886993408203
Epoch 850, val loss: 0.7633809447288513
Epoch 860, training loss: 12.726079940795898 = 0.136582612991333 + 2.0 * 6.294748783111572
Epoch 860, val loss: 0.7664315700531006
Epoch 870, training loss: 12.722406387329102 = 0.13052396476268768 + 2.0 * 6.295941352844238
Epoch 870, val loss: 0.7696324586868286
Epoch 880, training loss: 12.710338592529297 = 0.12478253990411758 + 2.0 * 6.292778015136719
Epoch 880, val loss: 0.7729557156562805
Epoch 890, training loss: 12.70880126953125 = 0.11933456361293793 + 2.0 * 6.29473352432251
Epoch 890, val loss: 0.7763869762420654
Epoch 900, training loss: 12.698342323303223 = 0.11416874825954437 + 2.0 * 6.292086601257324
Epoch 900, val loss: 0.7799063324928284
Epoch 910, training loss: 12.698384284973145 = 0.10927689075469971 + 2.0 * 6.294553756713867
Epoch 910, val loss: 0.7835466265678406
Epoch 920, training loss: 12.681902885437012 = 0.10465050488710403 + 2.0 * 6.288626194000244
Epoch 920, val loss: 0.7873424887657166
Epoch 930, training loss: 12.674781799316406 = 0.10027267783880234 + 2.0 * 6.287254333496094
Epoch 930, val loss: 0.7912845611572266
Epoch 940, training loss: 12.67582893371582 = 0.0961187481880188 + 2.0 * 6.289855003356934
Epoch 940, val loss: 0.7952995896339417
Epoch 950, training loss: 12.666858673095703 = 0.09216418117284775 + 2.0 * 6.287347316741943
Epoch 950, val loss: 0.7992954254150391
Epoch 960, training loss: 12.657471656799316 = 0.08841961622238159 + 2.0 * 6.2845258712768555
Epoch 960, val loss: 0.8033566474914551
Epoch 970, training loss: 12.655644416809082 = 0.08487343788146973 + 2.0 * 6.285385608673096
Epoch 970, val loss: 0.8075857162475586
Epoch 980, training loss: 12.657127380371094 = 0.08150304853916168 + 2.0 * 6.287812232971191
Epoch 980, val loss: 0.811817467212677
Epoch 990, training loss: 12.645990371704102 = 0.07830540090799332 + 2.0 * 6.28384256362915
Epoch 990, val loss: 0.8160160779953003
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7925925925925926
0.8323668950975225
The final CL Acc:0.79753, 0.01552, The final GNN Acc:0.83588, 0.00249
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.5 der2:0 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11660])
remove edge: torch.Size([2, 9500])
updated graph: torch.Size([2, 10604])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 19.156023025512695 = 1.9623262882232666 + 2.0 * 8.596848487854004
Epoch 0, val loss: 1.9619473218917847
Epoch 10, training loss: 19.144533157348633 = 1.9513798952102661 + 2.0 * 8.596576690673828
Epoch 10, val loss: 1.9516510963439941
Epoch 20, training loss: 19.126991271972656 = 1.9379093647003174 + 2.0 * 8.5945405960083
Epoch 20, val loss: 1.9382823705673218
Epoch 30, training loss: 19.078025817871094 = 1.919320821762085 + 2.0 * 8.579352378845215
Epoch 30, val loss: 1.9193994998931885
Epoch 40, training loss: 18.850120544433594 = 1.8960809707641602 + 2.0 * 8.477019309997559
Epoch 40, val loss: 1.8969035148620605
Epoch 50, training loss: 17.64381980895996 = 1.874338150024414 + 2.0 * 7.884740829467773
Epoch 50, val loss: 1.876312017440796
Epoch 60, training loss: 16.653547286987305 = 1.858672857284546 + 2.0 * 7.397437572479248
Epoch 60, val loss: 1.8615342378616333
Epoch 70, training loss: 16.07479476928711 = 1.8451881408691406 + 2.0 * 7.114802837371826
Epoch 70, val loss: 1.848745584487915
Epoch 80, training loss: 15.742623329162598 = 1.8329284191131592 + 2.0 * 6.95484733581543
Epoch 80, val loss: 1.837007999420166
Epoch 90, training loss: 15.510786056518555 = 1.821058988571167 + 2.0 * 6.844863414764404
Epoch 90, val loss: 1.8254987001419067
Epoch 100, training loss: 15.345202445983887 = 1.8097246885299683 + 2.0 * 6.7677388191223145
Epoch 100, val loss: 1.814892053604126
Epoch 110, training loss: 15.209168434143066 = 1.7992573976516724 + 2.0 * 6.704955577850342
Epoch 110, val loss: 1.80543851852417
Epoch 120, training loss: 15.092284202575684 = 1.7894474267959595 + 2.0 * 6.651418209075928
Epoch 120, val loss: 1.7961976528167725
Epoch 130, training loss: 15.005084991455078 = 1.7797199487686157 + 2.0 * 6.612682342529297
Epoch 130, val loss: 1.787106990814209
Epoch 140, training loss: 14.934549331665039 = 1.769705057144165 + 2.0 * 6.582422256469727
Epoch 140, val loss: 1.777888536453247
Epoch 150, training loss: 14.87607479095459 = 1.759011149406433 + 2.0 * 6.558531761169434
Epoch 150, val loss: 1.7683799266815186
Epoch 160, training loss: 14.81532096862793 = 1.7473777532577515 + 2.0 * 6.533971786499023
Epoch 160, val loss: 1.7584036588668823
Epoch 170, training loss: 14.765664100646973 = 1.7346365451812744 + 2.0 * 6.515513896942139
Epoch 170, val loss: 1.7475839853286743
Epoch 180, training loss: 14.721709251403809 = 1.7203624248504639 + 2.0 * 6.500673294067383
Epoch 180, val loss: 1.7356534004211426
Epoch 190, training loss: 14.682208061218262 = 1.7043483257293701 + 2.0 * 6.488929748535156
Epoch 190, val loss: 1.7222650051116943
Epoch 200, training loss: 14.638582229614258 = 1.686267375946045 + 2.0 * 6.476157188415527
Epoch 200, val loss: 1.7074002027511597
Epoch 210, training loss: 14.595230102539062 = 1.6660103797912598 + 2.0 * 6.4646100997924805
Epoch 210, val loss: 1.6907228231430054
Epoch 220, training loss: 14.552728652954102 = 1.6431013345718384 + 2.0 * 6.454813480377197
Epoch 220, val loss: 1.6720240116119385
Epoch 230, training loss: 14.51115608215332 = 1.6174439191818237 + 2.0 * 6.4468560218811035
Epoch 230, val loss: 1.6512157917022705
Epoch 240, training loss: 14.468371391296387 = 1.5890905857086182 + 2.0 * 6.439640522003174
Epoch 240, val loss: 1.6283079385757446
Epoch 250, training loss: 14.421847343444824 = 1.5579124689102173 + 2.0 * 6.431967258453369
Epoch 250, val loss: 1.603311538696289
Epoch 260, training loss: 14.37391471862793 = 1.5243377685546875 + 2.0 * 6.424788475036621
Epoch 260, val loss: 1.5768029689788818
Epoch 270, training loss: 14.328763961791992 = 1.4888778924942017 + 2.0 * 6.419942855834961
Epoch 270, val loss: 1.5491881370544434
Epoch 280, training loss: 14.278708457946777 = 1.4517085552215576 + 2.0 * 6.41349983215332
Epoch 280, val loss: 1.5207571983337402
Epoch 290, training loss: 14.24537467956543 = 1.4133813381195068 + 2.0 * 6.415996551513672
Epoch 290, val loss: 1.492178201675415
Epoch 300, training loss: 14.187296867370605 = 1.3751786947250366 + 2.0 * 6.406059265136719
Epoch 300, val loss: 1.46420156955719
Epoch 310, training loss: 14.134211540222168 = 1.3374062776565552 + 2.0 * 6.398402690887451
Epoch 310, val loss: 1.4370261430740356
Epoch 320, training loss: 14.092638969421387 = 1.300113320350647 + 2.0 * 6.3962626457214355
Epoch 320, val loss: 1.4106663465499878
Epoch 330, training loss: 14.051996231079102 = 1.2641483545303345 + 2.0 * 6.393923759460449
Epoch 330, val loss: 1.3860102891921997
Epoch 340, training loss: 14.002618789672852 = 1.2295668125152588 + 2.0 * 6.386526107788086
Epoch 340, val loss: 1.3627537488937378
Epoch 350, training loss: 13.961883544921875 = 1.1958224773406982 + 2.0 * 6.383030414581299
Epoch 350, val loss: 1.3401747941970825
Epoch 360, training loss: 13.92045783996582 = 1.1626142263412476 + 2.0 * 6.378921985626221
Epoch 360, val loss: 1.3182215690612793
Epoch 370, training loss: 13.881633758544922 = 1.1296887397766113 + 2.0 * 6.375972270965576
Epoch 370, val loss: 1.2966334819793701
Epoch 380, training loss: 13.845186233520508 = 1.0970863103866577 + 2.0 * 6.374050140380859
Epoch 380, val loss: 1.2752232551574707
Epoch 390, training loss: 13.80656623840332 = 1.0651655197143555 + 2.0 * 6.370700359344482
Epoch 390, val loss: 1.2543967962265015
Epoch 400, training loss: 13.767495155334473 = 1.0335665941238403 + 2.0 * 6.366964340209961
Epoch 400, val loss: 1.2336417436599731
Epoch 410, training loss: 13.729793548583984 = 1.002143383026123 + 2.0 * 6.363824844360352
Epoch 410, val loss: 1.2129820585250854
Epoch 420, training loss: 13.713423728942871 = 0.970960795879364 + 2.0 * 6.371231555938721
Epoch 420, val loss: 1.1923757791519165
Epoch 430, training loss: 13.659653663635254 = 0.9402845501899719 + 2.0 * 6.359684467315674
Epoch 430, val loss: 1.1722898483276367
Epoch 440, training loss: 13.623458862304688 = 0.9102897047996521 + 2.0 * 6.356584548950195
Epoch 440, val loss: 1.15249764919281
Epoch 450, training loss: 13.588111877441406 = 0.8807539343833923 + 2.0 * 6.353679180145264
Epoch 450, val loss: 1.1329727172851562
Epoch 460, training loss: 13.577982902526855 = 0.8517236709594727 + 2.0 * 6.363129615783691
Epoch 460, val loss: 1.1137523651123047
Epoch 470, training loss: 13.529282569885254 = 0.8234642744064331 + 2.0 * 6.352909088134766
Epoch 470, val loss: 1.0953525304794312
Epoch 480, training loss: 13.491365432739258 = 0.7960646748542786 + 2.0 * 6.347650527954102
Epoch 480, val loss: 1.0775320529937744
Epoch 490, training loss: 13.45937442779541 = 0.7693266272544861 + 2.0 * 6.345024108886719
Epoch 490, val loss: 1.0603538751602173
Epoch 500, training loss: 13.446272850036621 = 0.7432879209518433 + 2.0 * 6.351492404937744
Epoch 500, val loss: 1.0438344478607178
Epoch 510, training loss: 13.398930549621582 = 0.7181311249732971 + 2.0 * 6.340399742126465
Epoch 510, val loss: 1.0283786058425903
Epoch 520, training loss: 13.373215675354004 = 0.6938890814781189 + 2.0 * 6.339663505554199
Epoch 520, val loss: 1.0137557983398438
Epoch 530, training loss: 13.353950500488281 = 0.6703798770904541 + 2.0 * 6.341785430908203
Epoch 530, val loss: 1.0000172853469849
Epoch 540, training loss: 13.321270942687988 = 0.6477940082550049 + 2.0 * 6.336738586425781
Epoch 540, val loss: 0.9870952367782593
Epoch 550, training loss: 13.293290138244629 = 0.6259734630584717 + 2.0 * 6.333658218383789
Epoch 550, val loss: 0.9754358530044556
Epoch 560, training loss: 13.266974449157715 = 0.604938805103302 + 2.0 * 6.331017971038818
Epoch 560, val loss: 0.9644489288330078
Epoch 570, training loss: 13.250657081604004 = 0.5846014618873596 + 2.0 * 6.3330278396606445
Epoch 570, val loss: 0.9544659852981567
Epoch 580, training loss: 13.222494125366211 = 0.5650985240936279 + 2.0 * 6.328697681427002
Epoch 580, val loss: 0.9455137848854065
Epoch 590, training loss: 13.206335067749023 = 0.5463141202926636 + 2.0 * 6.330010414123535
Epoch 590, val loss: 0.9374907612800598
Epoch 600, training loss: 13.184774398803711 = 0.5283050537109375 + 2.0 * 6.328234672546387
Epoch 600, val loss: 0.9301673173904419
Epoch 610, training loss: 13.158336639404297 = 0.5110276341438293 + 2.0 * 6.323654651641846
Epoch 610, val loss: 0.9239615797996521
Epoch 620, training loss: 13.155274391174316 = 0.494358092546463 + 2.0 * 6.330458164215088
Epoch 620, val loss: 0.9183962345123291
Epoch 630, training loss: 13.123462677001953 = 0.47839540243148804 + 2.0 * 6.32253360748291
Epoch 630, val loss: 0.9135993123054504
Epoch 640, training loss: 13.10026741027832 = 0.46297910809516907 + 2.0 * 6.318644046783447
Epoch 640, val loss: 0.9096006751060486
Epoch 650, training loss: 13.082353591918945 = 0.4481046497821808 + 2.0 * 6.317124366760254
Epoch 650, val loss: 0.9061886668205261
Epoch 660, training loss: 13.072492599487305 = 0.43371543288230896 + 2.0 * 6.319388389587402
Epoch 660, val loss: 0.9034414291381836
Epoch 670, training loss: 13.052796363830566 = 0.41990742087364197 + 2.0 * 6.316444396972656
Epoch 670, val loss: 0.9011257886886597
Epoch 680, training loss: 13.034061431884766 = 0.40658804774284363 + 2.0 * 6.313736915588379
Epoch 680, val loss: 0.8996946215629578
Epoch 690, training loss: 13.0169095993042 = 0.3937491774559021 + 2.0 * 6.311580181121826
Epoch 690, val loss: 0.8986223936080933
Epoch 700, training loss: 13.001131057739258 = 0.3813127875328064 + 2.0 * 6.309909343719482
Epoch 700, val loss: 0.898045539855957
Epoch 710, training loss: 13.012527465820312 = 0.3691966235637665 + 2.0 * 6.321665287017822
Epoch 710, val loss: 0.8979684114456177
Epoch 720, training loss: 12.974053382873535 = 0.35755273699760437 + 2.0 * 6.308250427246094
Epoch 720, val loss: 0.8981719017028809
Epoch 730, training loss: 12.959424018859863 = 0.3462698459625244 + 2.0 * 6.306577205657959
Epoch 730, val loss: 0.8989713788032532
Epoch 740, training loss: 12.945968627929688 = 0.33524224162101746 + 2.0 * 6.305363178253174
Epoch 740, val loss: 0.9000325798988342
Epoch 750, training loss: 12.952018737792969 = 0.32443153858184814 + 2.0 * 6.313793659210205
Epoch 750, val loss: 0.9014347195625305
Epoch 760, training loss: 12.936680793762207 = 0.313816636800766 + 2.0 * 6.311431884765625
Epoch 760, val loss: 0.9030411839485168
Epoch 770, training loss: 12.909209251403809 = 0.30346670746803284 + 2.0 * 6.302871227264404
Epoch 770, val loss: 0.9053901433944702
Epoch 780, training loss: 12.895167350769043 = 0.29322439432144165 + 2.0 * 6.300971508026123
Epoch 780, val loss: 0.9075299501419067
Epoch 790, training loss: 12.882824897766113 = 0.283062607049942 + 2.0 * 6.2998809814453125
Epoch 790, val loss: 0.9102176427841187
Epoch 800, training loss: 12.895883560180664 = 0.2729744613170624 + 2.0 * 6.311454772949219
Epoch 800, val loss: 0.9128208160400391
Epoch 810, training loss: 12.860278129577637 = 0.26293662190437317 + 2.0 * 6.298670768737793
Epoch 810, val loss: 0.9160839915275574
Epoch 820, training loss: 12.848128318786621 = 0.2530043125152588 + 2.0 * 6.297562122344971
Epoch 820, val loss: 0.9194592237472534
Epoch 830, training loss: 12.836482048034668 = 0.24312223494052887 + 2.0 * 6.296679973602295
Epoch 830, val loss: 0.9229804873466492
Epoch 840, training loss: 12.840845108032227 = 0.23331589996814728 + 2.0 * 6.303764820098877
Epoch 840, val loss: 0.9266892075538635
Epoch 850, training loss: 12.813934326171875 = 0.2236582487821579 + 2.0 * 6.295137882232666
Epoch 850, val loss: 0.9308928847312927
Epoch 860, training loss: 12.803528785705566 = 0.2141941487789154 + 2.0 * 6.2946672439575195
Epoch 860, val loss: 0.9352564215660095
Epoch 870, training loss: 12.795926094055176 = 0.20494011044502258 + 2.0 * 6.295493125915527
Epoch 870, val loss: 0.9397937059402466
Epoch 880, training loss: 12.798577308654785 = 0.19595913589000702 + 2.0 * 6.301309108734131
Epoch 880, val loss: 0.9444003105163574
Epoch 890, training loss: 12.776177406311035 = 0.1873348206281662 + 2.0 * 6.294421195983887
Epoch 890, val loss: 0.9497588276863098
Epoch 900, training loss: 12.761213302612305 = 0.17904117703437805 + 2.0 * 6.291086196899414
Epoch 900, val loss: 0.9549667835235596
Epoch 910, training loss: 12.750307083129883 = 0.17108869552612305 + 2.0 * 6.289608955383301
Epoch 910, val loss: 0.9604855179786682
Epoch 920, training loss: 12.741533279418945 = 0.16347184777259827 + 2.0 * 6.2890305519104
Epoch 920, val loss: 0.9660823345184326
Epoch 930, training loss: 12.7470064163208 = 0.15621639788150787 + 2.0 * 6.2953948974609375
Epoch 930, val loss: 0.9716917872428894
Epoch 940, training loss: 12.731579780578613 = 0.14932826161384583 + 2.0 * 6.291125774383545
Epoch 940, val loss: 0.9777910113334656
Epoch 950, training loss: 12.718591690063477 = 0.14281034469604492 + 2.0 * 6.287890434265137
Epoch 950, val loss: 0.9838123321533203
Epoch 960, training loss: 12.718440055847168 = 0.13660962879657745 + 2.0 * 6.290915012359619
Epoch 960, val loss: 0.9898922443389893
Epoch 970, training loss: 12.707019805908203 = 0.1307516098022461 + 2.0 * 6.2881340980529785
Epoch 970, val loss: 0.9960768818855286
Epoch 980, training loss: 12.699109077453613 = 0.12519100308418274 + 2.0 * 6.286959171295166
Epoch 980, val loss: 1.0025115013122559
Epoch 990, training loss: 12.692610740661621 = 0.11993308365345001 + 2.0 * 6.286338806152344
Epoch 990, val loss: 1.0087684392929077
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7296296296296296
0.8181338956246705
=== training gcn model ===
Epoch 0, training loss: 19.138671875 = 1.945038080215454 + 2.0 * 8.596817016601562
Epoch 0, val loss: 1.9553320407867432
Epoch 10, training loss: 19.128028869628906 = 1.9350786209106445 + 2.0 * 8.596474647521973
Epoch 10, val loss: 1.945007085800171
Epoch 20, training loss: 19.110403060913086 = 1.9230378866195679 + 2.0 * 8.593682289123535
Epoch 20, val loss: 1.9322272539138794
Epoch 30, training loss: 19.048599243164062 = 1.9067412614822388 + 2.0 * 8.570928573608398
Epoch 30, val loss: 1.9148792028427124
Epoch 40, training loss: 18.703184127807617 = 1.8865292072296143 + 2.0 * 8.408327102661133
Epoch 40, val loss: 1.8940376043319702
Epoch 50, training loss: 17.704750061035156 = 1.8658218383789062 + 2.0 * 7.919464588165283
Epoch 50, val loss: 1.8734074831008911
Epoch 60, training loss: 16.586946487426758 = 1.8508760929107666 + 2.0 * 7.368035316467285
Epoch 60, val loss: 1.8580702543258667
Epoch 70, training loss: 15.934996604919434 = 1.8393880128860474 + 2.0 * 7.047804355621338
Epoch 70, val loss: 1.8455973863601685
Epoch 80, training loss: 15.676065444946289 = 1.8269485235214233 + 2.0 * 6.924558639526367
Epoch 80, val loss: 1.8326871395111084
Epoch 90, training loss: 15.46451187133789 = 1.8135910034179688 + 2.0 * 6.825460433959961
Epoch 90, val loss: 1.8192427158355713
Epoch 100, training loss: 15.314291954040527 = 1.8014192581176758 + 2.0 * 6.756436347961426
Epoch 100, val loss: 1.8070988655090332
Epoch 110, training loss: 15.18203067779541 = 1.7905880212783813 + 2.0 * 6.69572114944458
Epoch 110, val loss: 1.7962734699249268
Epoch 120, training loss: 15.075769424438477 = 1.7802058458328247 + 2.0 * 6.647781848907471
Epoch 120, val loss: 1.786144733428955
Epoch 130, training loss: 14.991606712341309 = 1.7694309949874878 + 2.0 * 6.611087799072266
Epoch 130, val loss: 1.7760571241378784
Epoch 140, training loss: 14.921226501464844 = 1.757741093635559 + 2.0 * 6.581742763519287
Epoch 140, val loss: 1.7656304836273193
Epoch 150, training loss: 14.860033988952637 = 1.745027780532837 + 2.0 * 6.5575032234191895
Epoch 150, val loss: 1.7546542882919312
Epoch 160, training loss: 14.806665420532227 = 1.7309849262237549 + 2.0 * 6.537840366363525
Epoch 160, val loss: 1.742781639099121
Epoch 170, training loss: 14.755781173706055 = 1.7153816223144531 + 2.0 * 6.520199775695801
Epoch 170, val loss: 1.7297613620758057
Epoch 180, training loss: 14.708157539367676 = 1.6979446411132812 + 2.0 * 6.505106449127197
Epoch 180, val loss: 1.7153756618499756
Epoch 190, training loss: 14.66161060333252 = 1.6783782243728638 + 2.0 * 6.491616249084473
Epoch 190, val loss: 1.6993855237960815
Epoch 200, training loss: 14.618574142456055 = 1.6563878059387207 + 2.0 * 6.481093406677246
Epoch 200, val loss: 1.6815791130065918
Epoch 210, training loss: 14.569075584411621 = 1.631927728652954 + 2.0 * 6.468574047088623
Epoch 210, val loss: 1.6618927717208862
Epoch 220, training loss: 14.521045684814453 = 1.6046698093414307 + 2.0 * 6.458188056945801
Epoch 220, val loss: 1.640108346939087
Epoch 230, training loss: 14.472315788269043 = 1.5744909048080444 + 2.0 * 6.448912620544434
Epoch 230, val loss: 1.6160974502563477
Epoch 240, training loss: 14.423006057739258 = 1.5415321588516235 + 2.0 * 6.440736770629883
Epoch 240, val loss: 1.5900534391403198
Epoch 250, training loss: 14.381061553955078 = 1.5057621002197266 + 2.0 * 6.437649726867676
Epoch 250, val loss: 1.5620158910751343
Epoch 260, training loss: 14.321577072143555 = 1.467811107635498 + 2.0 * 6.426883220672607
Epoch 260, val loss: 1.5325677394866943
Epoch 270, training loss: 14.267853736877441 = 1.4278572797775269 + 2.0 * 6.4199981689453125
Epoch 270, val loss: 1.5018514394760132
Epoch 280, training loss: 14.214202880859375 = 1.3861814737319946 + 2.0 * 6.414010524749756
Epoch 280, val loss: 1.4701436758041382
Epoch 290, training loss: 14.164098739624023 = 1.3433942794799805 + 2.0 * 6.4103522300720215
Epoch 290, val loss: 1.4379781484603882
Epoch 300, training loss: 14.108871459960938 = 1.300447940826416 + 2.0 * 6.404211521148682
Epoch 300, val loss: 1.4060148000717163
Epoch 310, training loss: 14.055252075195312 = 1.2573994398117065 + 2.0 * 6.398926258087158
Epoch 310, val loss: 1.3743207454681396
Epoch 320, training loss: 14.003522872924805 = 1.2146943807601929 + 2.0 * 6.39441442489624
Epoch 320, val loss: 1.3433618545532227
Epoch 330, training loss: 13.974620819091797 = 1.1728531122207642 + 2.0 * 6.400883674621582
Epoch 330, val loss: 1.3133800029754639
Epoch 340, training loss: 13.91242790222168 = 1.1328492164611816 + 2.0 * 6.38978910446167
Epoch 340, val loss: 1.2850027084350586
Epoch 350, training loss: 13.862119674682617 = 1.0942679643630981 + 2.0 * 6.383925914764404
Epoch 350, val loss: 1.2578625679016113
Epoch 360, training loss: 13.815929412841797 = 1.0566836595535278 + 2.0 * 6.379622936248779
Epoch 360, val loss: 1.2317403554916382
Epoch 370, training loss: 13.771255493164062 = 1.0198168754577637 + 2.0 * 6.3757195472717285
Epoch 370, val loss: 1.206377387046814
Epoch 380, training loss: 13.728352546691895 = 0.9837132096290588 + 2.0 * 6.37231969833374
Epoch 380, val loss: 1.1816822290420532
Epoch 390, training loss: 13.707803726196289 = 0.9484573602676392 + 2.0 * 6.379673004150391
Epoch 390, val loss: 1.1576833724975586
Epoch 400, training loss: 13.649955749511719 = 0.9147162437438965 + 2.0 * 6.36761999130249
Epoch 400, val loss: 1.13485586643219
Epoch 410, training loss: 13.611031532287598 = 0.8821758031845093 + 2.0 * 6.3644280433654785
Epoch 410, val loss: 1.1130001544952393
Epoch 420, training loss: 13.576157569885254 = 0.8506251573562622 + 2.0 * 6.362766265869141
Epoch 420, val loss: 1.0920435190200806
Epoch 430, training loss: 13.537103652954102 = 0.8203206062316895 + 2.0 * 6.358391284942627
Epoch 430, val loss: 1.0720436573028564
Epoch 440, training loss: 13.504798889160156 = 0.7912105917930603 + 2.0 * 6.356794357299805
Epoch 440, val loss: 1.0529183149337769
Epoch 450, training loss: 13.473092079162598 = 0.7635467648506165 + 2.0 * 6.354772567749023
Epoch 450, val loss: 1.0351791381835938
Epoch 460, training loss: 13.440812110900879 = 0.7374927401542664 + 2.0 * 6.351659774780273
Epoch 460, val loss: 1.0187561511993408
Epoch 470, training loss: 13.41048526763916 = 0.7127322554588318 + 2.0 * 6.348876476287842
Epoch 470, val loss: 1.0038034915924072
Epoch 480, training loss: 13.381217956542969 = 0.6890648007392883 + 2.0 * 6.346076488494873
Epoch 480, val loss: 0.9899676442146301
Epoch 490, training loss: 13.354466438293457 = 0.6663861870765686 + 2.0 * 6.3440399169921875
Epoch 490, val loss: 0.9771116971969604
Epoch 500, training loss: 13.33345890045166 = 0.6446163654327393 + 2.0 * 6.34442138671875
Epoch 500, val loss: 0.9654157757759094
Epoch 510, training loss: 13.308698654174805 = 0.6239031553268433 + 2.0 * 6.342397689819336
Epoch 510, val loss: 0.9548535346984863
Epoch 520, training loss: 13.283943176269531 = 0.6042010188102722 + 2.0 * 6.339870929718018
Epoch 520, val loss: 0.9453615546226501
Epoch 530, training loss: 13.257318496704102 = 0.5851924419403076 + 2.0 * 6.336062908172607
Epoch 530, val loss: 0.9367987513542175
Epoch 540, training loss: 13.247803688049316 = 0.5667056441307068 + 2.0 * 6.340548992156982
Epoch 540, val loss: 0.9289664030075073
Epoch 550, training loss: 13.219217300415039 = 0.5488701462745667 + 2.0 * 6.335173606872559
Epoch 550, val loss: 0.9219055771827698
Epoch 560, training loss: 13.194178581237793 = 0.5316148400306702 + 2.0 * 6.331281661987305
Epoch 560, val loss: 0.9155935049057007
Epoch 570, training loss: 13.172646522521973 = 0.5147695541381836 + 2.0 * 6.3289384841918945
Epoch 570, val loss: 0.9099573493003845
Epoch 580, training loss: 13.153403282165527 = 0.4982297420501709 + 2.0 * 6.327586650848389
Epoch 580, val loss: 0.9048647880554199
Epoch 590, training loss: 13.133759498596191 = 0.48206812143325806 + 2.0 * 6.325845718383789
Epoch 590, val loss: 0.9001038074493408
Epoch 600, training loss: 13.115116119384766 = 0.466510146856308 + 2.0 * 6.324303150177002
Epoch 600, val loss: 0.8960909247398376
Epoch 610, training loss: 13.097840309143066 = 0.45136091113090515 + 2.0 * 6.323239803314209
Epoch 610, val loss: 0.8928271532058716
Epoch 620, training loss: 13.07879638671875 = 0.43655574321746826 + 2.0 * 6.321120262145996
Epoch 620, val loss: 0.8899574279785156
Epoch 630, training loss: 13.06041145324707 = 0.4220632314682007 + 2.0 * 6.319174289703369
Epoch 630, val loss: 0.8876215219497681
Epoch 640, training loss: 13.044914245605469 = 0.4078451097011566 + 2.0 * 6.3185343742370605
Epoch 640, val loss: 0.8858801126480103
Epoch 650, training loss: 13.033695220947266 = 0.39399072527885437 + 2.0 * 6.319852352142334
Epoch 650, val loss: 0.8846228122711182
Epoch 660, training loss: 13.015134811401367 = 0.38052108883857727 + 2.0 * 6.317306995391846
Epoch 660, val loss: 0.8838204741477966
Epoch 670, training loss: 12.99746036529541 = 0.3673100769519806 + 2.0 * 6.315074920654297
Epoch 670, val loss: 0.8835329413414001
Epoch 680, training loss: 12.981178283691406 = 0.3542333245277405 + 2.0 * 6.313472270965576
Epoch 680, val loss: 0.8835816383361816
Epoch 690, training loss: 12.971773147583008 = 0.3412734568119049 + 2.0 * 6.315249919891357
Epoch 690, val loss: 0.8839786648750305
Epoch 700, training loss: 12.949675559997559 = 0.3284718692302704 + 2.0 * 6.310601711273193
Epoch 700, val loss: 0.8848068118095398
Epoch 710, training loss: 12.936982154846191 = 0.3157382905483246 + 2.0 * 6.310621738433838
Epoch 710, val loss: 0.8859911561012268
Epoch 720, training loss: 12.92161750793457 = 0.30311188101768494 + 2.0 * 6.309252738952637
Epoch 720, val loss: 0.8874136805534363
Epoch 730, training loss: 12.908452987670898 = 0.290637344121933 + 2.0 * 6.308907985687256
Epoch 730, val loss: 0.8891328573226929
Epoch 740, training loss: 12.890231132507324 = 0.2783203125 + 2.0 * 6.305955410003662
Epoch 740, val loss: 0.8912281394004822
Epoch 750, training loss: 12.88083553314209 = 0.2661895453929901 + 2.0 * 6.307322978973389
Epoch 750, val loss: 0.8935927748680115
Epoch 760, training loss: 12.865521430969238 = 0.25437790155410767 + 2.0 * 6.305571556091309
Epoch 760, val loss: 0.8963879346847534
Epoch 770, training loss: 12.852810859680176 = 0.24294885993003845 + 2.0 * 6.304931163787842
Epoch 770, val loss: 0.8994250297546387
Epoch 780, training loss: 12.836042404174805 = 0.23194676637649536 + 2.0 * 6.3020477294921875
Epoch 780, val loss: 0.903048038482666
Epoch 790, training loss: 12.83203411102295 = 0.2213609516620636 + 2.0 * 6.3053364753723145
Epoch 790, val loss: 0.9068383574485779
Epoch 800, training loss: 12.81177043914795 = 0.21125267446041107 + 2.0 * 6.300259113311768
Epoch 800, val loss: 0.9108564853668213
Epoch 810, training loss: 12.799805641174316 = 0.20166683197021484 + 2.0 * 6.299069404602051
Epoch 810, val loss: 0.9154220223426819
Epoch 820, training loss: 12.788308143615723 = 0.19255433976650238 + 2.0 * 6.297876834869385
Epoch 820, val loss: 0.9202537536621094
Epoch 830, training loss: 12.801220893859863 = 0.18392236530780792 + 2.0 * 6.308649063110352
Epoch 830, val loss: 0.925388514995575
Epoch 840, training loss: 12.771689414978027 = 0.1758163869380951 + 2.0 * 6.29793643951416
Epoch 840, val loss: 0.930854320526123
Epoch 850, training loss: 12.766459465026855 = 0.16818943619728088 + 2.0 * 6.299135208129883
Epoch 850, val loss: 0.9366554617881775
Epoch 860, training loss: 12.753669738769531 = 0.1609824299812317 + 2.0 * 6.296343803405762
Epoch 860, val loss: 0.9425708055496216
Epoch 870, training loss: 12.743962287902832 = 0.15417204797267914 + 2.0 * 6.294895172119141
Epoch 870, val loss: 0.9487107992172241
Epoch 880, training loss: 12.742521286010742 = 0.1477413922548294 + 2.0 * 6.297389984130859
Epoch 880, val loss: 0.9551143050193787
Epoch 890, training loss: 12.733468055725098 = 0.1416991949081421 + 2.0 * 6.295884609222412
Epoch 890, val loss: 0.9617809057235718
Epoch 900, training loss: 12.722291946411133 = 0.13596953451633453 + 2.0 * 6.293161392211914
Epoch 900, val loss: 0.9684715270996094
Epoch 910, training loss: 12.71566104888916 = 0.13056114315986633 + 2.0 * 6.292550086975098
Epoch 910, val loss: 0.975405752658844
Epoch 920, training loss: 12.709036827087402 = 0.1254381686449051 + 2.0 * 6.291799545288086
Epoch 920, val loss: 0.9823312163352966
Epoch 930, training loss: 12.703892707824707 = 0.1206035390496254 + 2.0 * 6.29164457321167
Epoch 930, val loss: 0.9895597100257874
Epoch 940, training loss: 12.697863578796387 = 0.11600426584482193 + 2.0 * 6.290929794311523
Epoch 940, val loss: 0.9966475963592529
Epoch 950, training loss: 12.696053504943848 = 0.11164870858192444 + 2.0 * 6.292202472686768
Epoch 950, val loss: 1.004054307937622
Epoch 960, training loss: 12.68417739868164 = 0.10750643908977509 + 2.0 * 6.28833532333374
Epoch 960, val loss: 1.01128089427948
Epoch 970, training loss: 12.677576065063477 = 0.10357850790023804 + 2.0 * 6.286998748779297
Epoch 970, val loss: 1.0187331438064575
Epoch 980, training loss: 12.671087265014648 = 0.09982265532016754 + 2.0 * 6.285632133483887
Epoch 980, val loss: 1.0261504650115967
Epoch 990, training loss: 12.66854190826416 = 0.09624316543340683 + 2.0 * 6.286149501800537
Epoch 990, val loss: 1.0337072610855103
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7481481481481482
0.8096995255666843
=== training gcn model ===
Epoch 0, training loss: 19.133005142211914 = 1.9393362998962402 + 2.0 * 8.596834182739258
Epoch 0, val loss: 1.9229062795639038
Epoch 10, training loss: 19.122425079345703 = 1.9294246435165405 + 2.0 * 8.596500396728516
Epoch 10, val loss: 1.9140816926956177
Epoch 20, training loss: 19.105335235595703 = 1.917151927947998 + 2.0 * 8.594091415405273
Epoch 20, val loss: 1.902909755706787
Epoch 30, training loss: 19.048547744750977 = 1.9007209539413452 + 2.0 * 8.57391357421875
Epoch 30, val loss: 1.8879883289337158
Epoch 40, training loss: 18.64600944519043 = 1.8810404539108276 + 2.0 * 8.382484436035156
Epoch 40, val loss: 1.870882272720337
Epoch 50, training loss: 16.977249145507812 = 1.8610150814056396 + 2.0 * 7.558117389678955
Epoch 50, val loss: 1.8541178703308105
Epoch 60, training loss: 16.198644638061523 = 1.846935510635376 + 2.0 * 7.175854682922363
Epoch 60, val loss: 1.8418880701065063
Epoch 70, training loss: 15.756941795349121 = 1.8364254236221313 + 2.0 * 6.9602580070495605
Epoch 70, val loss: 1.832432746887207
Epoch 80, training loss: 15.56717586517334 = 1.8254833221435547 + 2.0 * 6.870846271514893
Epoch 80, val loss: 1.823351502418518
Epoch 90, training loss: 15.428997993469238 = 1.8146895170211792 + 2.0 * 6.807154178619385
Epoch 90, val loss: 1.8148643970489502
Epoch 100, training loss: 15.285353660583496 = 1.8055973052978516 + 2.0 * 6.739878177642822
Epoch 100, val loss: 1.8075448274612427
Epoch 110, training loss: 15.154887199401855 = 1.7980947494506836 + 2.0 * 6.678396224975586
Epoch 110, val loss: 1.8011574745178223
Epoch 120, training loss: 15.053426742553711 = 1.7911818027496338 + 2.0 * 6.631122589111328
Epoch 120, val loss: 1.79498291015625
Epoch 130, training loss: 14.97096061706543 = 1.78384530544281 + 2.0 * 6.593557834625244
Epoch 130, val loss: 1.788637399673462
Epoch 140, training loss: 14.904784202575684 = 1.7757623195648193 + 2.0 * 6.564510822296143
Epoch 140, val loss: 1.7819358110427856
Epoch 150, training loss: 14.852859497070312 = 1.766823172569275 + 2.0 * 6.543018341064453
Epoch 150, val loss: 1.7745968103408813
Epoch 160, training loss: 14.800386428833008 = 1.7569955587387085 + 2.0 * 6.521695613861084
Epoch 160, val loss: 1.7666599750518799
Epoch 170, training loss: 14.754465103149414 = 1.7460521459579468 + 2.0 * 6.504206657409668
Epoch 170, val loss: 1.757914662361145
Epoch 180, training loss: 14.709664344787598 = 1.733856201171875 + 2.0 * 6.487904071807861
Epoch 180, val loss: 1.7484054565429688
Epoch 190, training loss: 14.667537689208984 = 1.7201642990112305 + 2.0 * 6.473686695098877
Epoch 190, val loss: 1.7378408908843994
Epoch 200, training loss: 14.625015258789062 = 1.7046751976013184 + 2.0 * 6.460169792175293
Epoch 200, val loss: 1.7259854078292847
Epoch 210, training loss: 14.583602905273438 = 1.687026023864746 + 2.0 * 6.448288440704346
Epoch 210, val loss: 1.7124462127685547
Epoch 220, training loss: 14.543439865112305 = 1.6669193506240845 + 2.0 * 6.438260078430176
Epoch 220, val loss: 1.6970136165618896
Epoch 230, training loss: 14.500382423400879 = 1.644060492515564 + 2.0 * 6.428161144256592
Epoch 230, val loss: 1.679503321647644
Epoch 240, training loss: 14.458173751831055 = 1.6180673837661743 + 2.0 * 6.420053005218506
Epoch 240, val loss: 1.6595486402511597
Epoch 250, training loss: 14.416759490966797 = 1.5886168479919434 + 2.0 * 6.414071083068848
Epoch 250, val loss: 1.6368637084960938
Epoch 260, training loss: 14.369132041931152 = 1.5558072328567505 + 2.0 * 6.406662464141846
Epoch 260, val loss: 1.6114181280136108
Epoch 270, training loss: 14.320565223693848 = 1.5198097229003906 + 2.0 * 6.4003777503967285
Epoch 270, val loss: 1.5834797620773315
Epoch 280, training loss: 14.269983291625977 = 1.480528712272644 + 2.0 * 6.3947272300720215
Epoch 280, val loss: 1.5530555248260498
Epoch 290, training loss: 14.22663688659668 = 1.4383281469345093 + 2.0 * 6.3941545486450195
Epoch 290, val loss: 1.5203572511672974
Epoch 300, training loss: 14.165853500366211 = 1.3941748142242432 + 2.0 * 6.385839462280273
Epoch 300, val loss: 1.486274003982544
Epoch 310, training loss: 14.110093116760254 = 1.348631739616394 + 2.0 * 6.380730628967285
Epoch 310, val loss: 1.451271891593933
Epoch 320, training loss: 14.060113906860352 = 1.302274227142334 + 2.0 * 6.37891960144043
Epoch 320, val loss: 1.4158360958099365
Epoch 330, training loss: 14.010143280029297 = 1.2562788724899292 + 2.0 * 6.376932144165039
Epoch 330, val loss: 1.3810529708862305
Epoch 340, training loss: 13.951382637023926 = 1.2113358974456787 + 2.0 * 6.370023250579834
Epoch 340, val loss: 1.3474091291427612
Epoch 350, training loss: 13.898700714111328 = 1.1675134897232056 + 2.0 * 6.365593433380127
Epoch 350, val loss: 1.315032958984375
Epoch 360, training loss: 13.858463287353516 = 1.1249988079071045 + 2.0 * 6.366732120513916
Epoch 360, val loss: 1.2841278314590454
Epoch 370, training loss: 13.804591178894043 = 1.0844876766204834 + 2.0 * 6.36005163192749
Epoch 370, val loss: 1.255058765411377
Epoch 380, training loss: 13.757479667663574 = 1.0456217527389526 + 2.0 * 6.355928897857666
Epoch 380, val loss: 1.2275022268295288
Epoch 390, training loss: 13.72191047668457 = 1.008143663406372 + 2.0 * 6.356883525848389
Epoch 390, val loss: 1.2012344598770142
Epoch 400, training loss: 13.673348426818848 = 0.9721109867095947 + 2.0 * 6.350618839263916
Epoch 400, val loss: 1.176378607749939
Epoch 410, training loss: 13.633770942687988 = 0.9372028708457947 + 2.0 * 6.3482842445373535
Epoch 410, val loss: 1.1526635885238647
Epoch 420, training loss: 13.601110458374023 = 0.9032837748527527 + 2.0 * 6.348913192749023
Epoch 420, val loss: 1.1297945976257324
Epoch 430, training loss: 13.5637788772583 = 0.870373547077179 + 2.0 * 6.346702575683594
Epoch 430, val loss: 1.1078846454620361
Epoch 440, training loss: 13.5205717086792 = 0.8385334610939026 + 2.0 * 6.341019153594971
Epoch 440, val loss: 1.0870654582977295
Epoch 450, training loss: 13.484414100646973 = 0.8073551654815674 + 2.0 * 6.338529586791992
Epoch 450, val loss: 1.066923975944519
Epoch 460, training loss: 13.450508117675781 = 0.7767641544342041 + 2.0 * 6.336872100830078
Epoch 460, val loss: 1.0476655960083008
Epoch 470, training loss: 13.418229103088379 = 0.7469282746315002 + 2.0 * 6.335650444030762
Epoch 470, val loss: 1.029107928276062
Epoch 480, training loss: 13.387970924377441 = 0.7181816697120667 + 2.0 * 6.33489465713501
Epoch 480, val loss: 1.011622667312622
Epoch 490, training loss: 13.353789329528809 = 0.690475583076477 + 2.0 * 6.3316569328308105
Epoch 490, val loss: 0.9952290654182434
Epoch 500, training loss: 13.32595443725586 = 0.6636056900024414 + 2.0 * 6.331174373626709
Epoch 500, val loss: 0.9798252582550049
Epoch 510, training loss: 13.307767868041992 = 0.6377406716346741 + 2.0 * 6.335013389587402
Epoch 510, val loss: 0.9655866622924805
Epoch 520, training loss: 13.266417503356934 = 0.6130706667900085 + 2.0 * 6.32667350769043
Epoch 520, val loss: 0.9524542689323425
Epoch 530, training loss: 13.240216255187988 = 0.5894108414649963 + 2.0 * 6.325402736663818
Epoch 530, val loss: 0.9405251741409302
Epoch 540, training loss: 13.21822738647461 = 0.5665930509567261 + 2.0 * 6.325817108154297
Epoch 540, val loss: 0.9295616149902344
Epoch 550, training loss: 13.187621116638184 = 0.5446652770042419 + 2.0 * 6.321477890014648
Epoch 550, val loss: 0.9195876717567444
Epoch 560, training loss: 13.17687702178955 = 0.5236572027206421 + 2.0 * 6.326610088348389
Epoch 560, val loss: 0.9106472134590149
Epoch 570, training loss: 13.144793510437012 = 0.5036553740501404 + 2.0 * 6.320569038391113
Epoch 570, val loss: 0.9025952219963074
Epoch 580, training loss: 13.12093734741211 = 0.4845677614212036 + 2.0 * 6.318184852600098
Epoch 580, val loss: 0.8955433964729309
Epoch 590, training loss: 13.09817123413086 = 0.46623021364212036 + 2.0 * 6.315970420837402
Epoch 590, val loss: 0.8892859220504761
Epoch 600, training loss: 13.079038619995117 = 0.4486931264400482 + 2.0 * 6.3151726722717285
Epoch 600, val loss: 0.8837709426879883
Epoch 610, training loss: 13.059925079345703 = 0.4319806694984436 + 2.0 * 6.313971996307373
Epoch 610, val loss: 0.8790275454521179
Epoch 620, training loss: 13.061962127685547 = 0.4159974157810211 + 2.0 * 6.322982311248779
Epoch 620, val loss: 0.874884843826294
Epoch 630, training loss: 13.022107124328613 = 0.40065819025039673 + 2.0 * 6.310724258422852
Epoch 630, val loss: 0.8712471127510071
Epoch 640, training loss: 13.007379531860352 = 0.3859449028968811 + 2.0 * 6.3107171058654785
Epoch 640, val loss: 0.8681961894035339
Epoch 650, training loss: 12.98786735534668 = 0.37177038192749023 + 2.0 * 6.308048248291016
Epoch 650, val loss: 0.8654895424842834
Epoch 660, training loss: 12.976009368896484 = 0.35812342166900635 + 2.0 * 6.308942794799805
Epoch 660, val loss: 0.8631950616836548
Epoch 670, training loss: 12.959463119506836 = 0.3448519706726074 + 2.0 * 6.307305335998535
Epoch 670, val loss: 0.8612436652183533
Epoch 680, training loss: 12.946419715881348 = 0.33190399408340454 + 2.0 * 6.307257652282715
Epoch 680, val loss: 0.8594889044761658
Epoch 690, training loss: 12.9273681640625 = 0.31927716732025146 + 2.0 * 6.304045677185059
Epoch 690, val loss: 0.8580244183540344
Epoch 700, training loss: 12.918683052062988 = 0.30685359239578247 + 2.0 * 6.305914878845215
Epoch 700, val loss: 0.8566563725471497
Epoch 710, training loss: 12.898392677307129 = 0.29465457797050476 + 2.0 * 6.301868915557861
Epoch 710, val loss: 0.8554368615150452
Epoch 720, training loss: 12.883641242980957 = 0.2826293706893921 + 2.0 * 6.300506114959717
Epoch 720, val loss: 0.8544120192527771
Epoch 730, training loss: 12.877154350280762 = 0.2707349359989166 + 2.0 * 6.3032097816467285
Epoch 730, val loss: 0.8533728718757629
Epoch 740, training loss: 12.8589448928833 = 0.25909239053726196 + 2.0 * 6.299926280975342
Epoch 740, val loss: 0.8524262309074402
Epoch 750, training loss: 12.846681594848633 = 0.24758023023605347 + 2.0 * 6.299550533294678
Epoch 750, val loss: 0.8515803217887878
Epoch 760, training loss: 12.830700874328613 = 0.23630081117153168 + 2.0 * 6.2972002029418945
Epoch 760, val loss: 0.8508991599082947
Epoch 770, training loss: 12.816055297851562 = 0.2252649962902069 + 2.0 * 6.295395374298096
Epoch 770, val loss: 0.8504357933998108
Epoch 780, training loss: 12.815467834472656 = 0.21447990834712982 + 2.0 * 6.300494194030762
Epoch 780, val loss: 0.8501574397087097
Epoch 790, training loss: 12.806004524230957 = 0.20408765971660614 + 2.0 * 6.300958633422852
Epoch 790, val loss: 0.8499302864074707
Epoch 800, training loss: 12.782367706298828 = 0.19410794973373413 + 2.0 * 6.294129848480225
Epoch 800, val loss: 0.8500292301177979
Epoch 810, training loss: 12.7684965133667 = 0.1845312863588333 + 2.0 * 6.291982650756836
Epoch 810, val loss: 0.8503965735435486
Epoch 820, training loss: 12.770318031311035 = 0.17538031935691833 + 2.0 * 6.297468662261963
Epoch 820, val loss: 0.8510128259658813
Epoch 830, training loss: 12.755481719970703 = 0.16670028865337372 + 2.0 * 6.294390678405762
Epoch 830, val loss: 0.8517718315124512
Epoch 840, training loss: 12.741717338562012 = 0.15851078927516937 + 2.0 * 6.291603088378906
Epoch 840, val loss: 0.8529068827629089
Epoch 850, training loss: 12.73249340057373 = 0.15075986087322235 + 2.0 * 6.290866851806641
Epoch 850, val loss: 0.8541831970214844
Epoch 860, training loss: 12.721260070800781 = 0.14344707131385803 + 2.0 * 6.288906574249268
Epoch 860, val loss: 0.8557421565055847
Epoch 870, training loss: 12.716259956359863 = 0.1365698128938675 + 2.0 * 6.289844989776611
Epoch 870, val loss: 0.8575798869132996
Epoch 880, training loss: 12.705670356750488 = 0.1300998479127884 + 2.0 * 6.287785053253174
Epoch 880, val loss: 0.859503448009491
Epoch 890, training loss: 12.699350357055664 = 0.12399747222661972 + 2.0 * 6.2876763343811035
Epoch 890, val loss: 0.8616209626197815
Epoch 900, training loss: 12.691503524780273 = 0.11826594918966293 + 2.0 * 6.286618709564209
Epoch 900, val loss: 0.8639947772026062
Epoch 910, training loss: 12.680078506469727 = 0.11285862326622009 + 2.0 * 6.283609867095947
Epoch 910, val loss: 0.8664714097976685
Epoch 920, training loss: 12.698987007141113 = 0.10776428878307343 + 2.0 * 6.295611381530762
Epoch 920, val loss: 0.869140625
Epoch 930, training loss: 12.668118476867676 = 0.1030007004737854 + 2.0 * 6.282558917999268
Epoch 930, val loss: 0.8717522025108337
Epoch 940, training loss: 12.663755416870117 = 0.0985231027007103 + 2.0 * 6.282616138458252
Epoch 940, val loss: 0.8745853900909424
Epoch 950, training loss: 12.654550552368164 = 0.09428394585847855 + 2.0 * 6.280133247375488
Epoch 950, val loss: 0.8775378465652466
Epoch 960, training loss: 12.669842720031738 = 0.0902819037437439 + 2.0 * 6.289780616760254
Epoch 960, val loss: 0.8806619644165039
Epoch 970, training loss: 12.660688400268555 = 0.08649523556232452 + 2.0 * 6.2870965003967285
Epoch 970, val loss: 0.8835873007774353
Epoch 980, training loss: 12.639654159545898 = 0.08295466005802155 + 2.0 * 6.278349876403809
Epoch 980, val loss: 0.8866590857505798
Epoch 990, training loss: 12.636564254760742 = 0.07959184050559998 + 2.0 * 6.278486251831055
Epoch 990, val loss: 0.8898593187332153
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7703703703703704
0.8244596731681603
The final CL Acc:0.74938, 0.01666, The final GNN Acc:0.81743, 0.00605
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13190])
remove edge: torch.Size([2, 7872])
updated graph: torch.Size([2, 10506])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 19.149383544921875 = 1.9557008743286133 + 2.0 * 8.596840858459473
Epoch 0, val loss: 1.9578784704208374
Epoch 10, training loss: 19.138084411621094 = 1.9451452493667603 + 2.0 * 8.59646987915039
Epoch 10, val loss: 1.9471921920776367
Epoch 20, training loss: 19.118249893188477 = 1.9317556619644165 + 2.0 * 8.593247413635254
Epoch 20, val loss: 1.9334070682525635
Epoch 30, training loss: 19.047983169555664 = 1.91306471824646 + 2.0 * 8.567459106445312
Epoch 30, val loss: 1.914190649986267
Epoch 40, training loss: 18.657554626464844 = 1.8898283243179321 + 2.0 * 8.38386344909668
Epoch 40, val loss: 1.8910187482833862
Epoch 50, training loss: 17.742509841918945 = 1.862274408340454 + 2.0 * 7.940117835998535
Epoch 50, val loss: 1.8640838861465454
Epoch 60, training loss: 17.23105239868164 = 1.8395332098007202 + 2.0 * 7.695759296417236
Epoch 60, val loss: 1.8426942825317383
Epoch 70, training loss: 16.6377010345459 = 1.8248339891433716 + 2.0 * 7.406433582305908
Epoch 70, val loss: 1.8293288946151733
Epoch 80, training loss: 16.052806854248047 = 1.8153718709945679 + 2.0 * 7.118717193603516
Epoch 80, val loss: 1.8199737071990967
Epoch 90, training loss: 15.673331260681152 = 1.804240107536316 + 2.0 * 6.934545516967773
Epoch 90, val loss: 1.8089956045150757
Epoch 100, training loss: 15.385310173034668 = 1.7918825149536133 + 2.0 * 6.796713829040527
Epoch 100, val loss: 1.796984314918518
Epoch 110, training loss: 15.194755554199219 = 1.7799229621887207 + 2.0 * 6.70741605758667
Epoch 110, val loss: 1.7850892543792725
Epoch 120, training loss: 15.071675300598145 = 1.7677183151245117 + 2.0 * 6.651978492736816
Epoch 120, val loss: 1.7727707624435425
Epoch 130, training loss: 14.983528137207031 = 1.7546005249023438 + 2.0 * 6.614463806152344
Epoch 130, val loss: 1.7599880695343018
Epoch 140, training loss: 14.91668701171875 = 1.7410006523132324 + 2.0 * 6.587843418121338
Epoch 140, val loss: 1.7471153736114502
Epoch 150, training loss: 14.846543312072754 = 1.7267974615097046 + 2.0 * 6.559873104095459
Epoch 150, val loss: 1.733857274055481
Epoch 160, training loss: 14.78627872467041 = 1.7111692428588867 + 2.0 * 6.537554740905762
Epoch 160, val loss: 1.7195395231246948
Epoch 170, training loss: 14.734870910644531 = 1.6936628818511963 + 2.0 * 6.520604133605957
Epoch 170, val loss: 1.703850269317627
Epoch 180, training loss: 14.676966667175293 = 1.674165964126587 + 2.0 * 6.501400470733643
Epoch 180, val loss: 1.686363935470581
Epoch 190, training loss: 14.625797271728516 = 1.6522759199142456 + 2.0 * 6.48676061630249
Epoch 190, val loss: 1.666887879371643
Epoch 200, training loss: 14.582939147949219 = 1.6274995803833008 + 2.0 * 6.477719783782959
Epoch 200, val loss: 1.644956350326538
Epoch 210, training loss: 14.526139259338379 = 1.6000397205352783 + 2.0 * 6.46304988861084
Epoch 210, val loss: 1.6206966638565063
Epoch 220, training loss: 14.473332405090332 = 1.5697602033615112 + 2.0 * 6.451786041259766
Epoch 220, val loss: 1.5940370559692383
Epoch 230, training loss: 14.43300724029541 = 1.5365791320800781 + 2.0 * 6.448214054107666
Epoch 230, val loss: 1.5650385618209839
Epoch 240, training loss: 14.37649154663086 = 1.5012400150299072 + 2.0 * 6.437625885009766
Epoch 240, val loss: 1.5343399047851562
Epoch 250, training loss: 14.319561004638672 = 1.4641060829162598 + 2.0 * 6.427727699279785
Epoch 250, val loss: 1.5022528171539307
Epoch 260, training loss: 14.267635345458984 = 1.4254659414291382 + 2.0 * 6.421084880828857
Epoch 260, val loss: 1.469207763671875
Epoch 270, training loss: 14.21835708618164 = 1.3855232000350952 + 2.0 * 6.416417121887207
Epoch 270, val loss: 1.435458779335022
Epoch 280, training loss: 14.164214134216309 = 1.3460237979888916 + 2.0 * 6.409095287322998
Epoch 280, val loss: 1.4023839235305786
Epoch 290, training loss: 14.116893768310547 = 1.3072788715362549 + 2.0 * 6.4048075675964355
Epoch 290, val loss: 1.3701978921890259
Epoch 300, training loss: 14.067506790161133 = 1.2694159746170044 + 2.0 * 6.399045467376709
Epoch 300, val loss: 1.3389867544174194
Epoch 310, training loss: 14.025017738342285 = 1.2321659326553345 + 2.0 * 6.396425724029541
Epoch 310, val loss: 1.3086538314819336
Epoch 320, training loss: 13.975558280944824 = 1.196062684059143 + 2.0 * 6.389747619628906
Epoch 320, val loss: 1.2795007228851318
Epoch 330, training loss: 13.932040214538574 = 1.1610056161880493 + 2.0 * 6.385517120361328
Epoch 330, val loss: 1.2513374090194702
Epoch 340, training loss: 13.892190933227539 = 1.1265573501586914 + 2.0 * 6.382816791534424
Epoch 340, val loss: 1.22380530834198
Epoch 350, training loss: 13.850214958190918 = 1.0926913022994995 + 2.0 * 6.3787617683410645
Epoch 350, val loss: 1.1970210075378418
Epoch 360, training loss: 13.8056640625 = 1.0593276023864746 + 2.0 * 6.373167991638184
Epoch 360, val loss: 1.170552134513855
Epoch 370, training loss: 13.76921272277832 = 1.0262644290924072 + 2.0 * 6.371474266052246
Epoch 370, val loss: 1.1444222927093506
Epoch 380, training loss: 13.729931831359863 = 0.9937496185302734 + 2.0 * 6.368091106414795
Epoch 380, val loss: 1.118829369544983
Epoch 390, training loss: 13.692950248718262 = 0.9620577692985535 + 2.0 * 6.365446090698242
Epoch 390, val loss: 1.0938225984573364
Epoch 400, training loss: 13.651141166687012 = 0.9312008619308472 + 2.0 * 6.3599700927734375
Epoch 400, val loss: 1.0694656372070312
Epoch 410, training loss: 13.616606712341309 = 0.9011625647544861 + 2.0 * 6.357722282409668
Epoch 410, val loss: 1.0459060668945312
Epoch 420, training loss: 13.58020305633545 = 0.8720294833183289 + 2.0 * 6.354086875915527
Epoch 420, val loss: 1.0231914520263672
Epoch 430, training loss: 13.547035217285156 = 0.8440842628479004 + 2.0 * 6.351475238800049
Epoch 430, val loss: 1.0014201402664185
Epoch 440, training loss: 13.515060424804688 = 0.8170908093452454 + 2.0 * 6.348984718322754
Epoch 440, val loss: 0.980682909488678
Epoch 450, training loss: 13.497603416442871 = 0.7913253307342529 + 2.0 * 6.3531389236450195
Epoch 450, val loss: 0.9610560536384583
Epoch 460, training loss: 13.4594144821167 = 0.7669437527656555 + 2.0 * 6.346235275268555
Epoch 460, val loss: 0.9427385330200195
Epoch 470, training loss: 13.426851272583008 = 0.7436065077781677 + 2.0 * 6.341622352600098
Epoch 470, val loss: 0.925543487071991
Epoch 480, training loss: 13.401741981506348 = 0.7211228609085083 + 2.0 * 6.3403096199035645
Epoch 480, val loss: 0.9093528985977173
Epoch 490, training loss: 13.375092506408691 = 0.6996058821678162 + 2.0 * 6.337743282318115
Epoch 490, val loss: 0.8942286968231201
Epoch 500, training loss: 13.346792221069336 = 0.6788322329521179 + 2.0 * 6.333980083465576
Epoch 500, val loss: 0.8800821900367737
Epoch 510, training loss: 13.321833610534668 = 0.6587114930152893 + 2.0 * 6.331561088562012
Epoch 510, val loss: 0.8667523264884949
Epoch 520, training loss: 13.308245658874512 = 0.6391614675521851 + 2.0 * 6.334542274475098
Epoch 520, val loss: 0.8543127775192261
Epoch 530, training loss: 13.279440879821777 = 0.6203556060791016 + 2.0 * 6.329542636871338
Epoch 530, val loss: 0.8427839875221252
Epoch 540, training loss: 13.255171775817871 = 0.6020613312721252 + 2.0 * 6.326555252075195
Epoch 540, val loss: 0.831994354724884
Epoch 550, training loss: 13.236507415771484 = 0.5842013359069824 + 2.0 * 6.326152801513672
Epoch 550, val loss: 0.8219076991081238
Epoch 560, training loss: 13.222675323486328 = 0.5668240189552307 + 2.0 * 6.327925682067871
Epoch 560, val loss: 0.8126822113990784
Epoch 570, training loss: 13.19456958770752 = 0.5498860478401184 + 2.0 * 6.3223419189453125
Epoch 570, val loss: 0.8041369318962097
Epoch 580, training loss: 13.172636032104492 = 0.5333569645881653 + 2.0 * 6.319639682769775
Epoch 580, val loss: 0.7961478233337402
Epoch 590, training loss: 13.16049575805664 = 0.5170637965202332 + 2.0 * 6.321715831756592
Epoch 590, val loss: 0.7888056635856628
Epoch 600, training loss: 13.138427734375 = 0.50110924243927 + 2.0 * 6.31865930557251
Epoch 600, val loss: 0.7820058465003967
Epoch 610, training loss: 13.117278099060059 = 0.4853936433792114 + 2.0 * 6.315942287445068
Epoch 610, val loss: 0.7757025957107544
Epoch 620, training loss: 13.099888801574707 = 0.4699024558067322 + 2.0 * 6.314993381500244
Epoch 620, val loss: 0.7697991728782654
Epoch 630, training loss: 13.08352279663086 = 0.4546332061290741 + 2.0 * 6.3144450187683105
Epoch 630, val loss: 0.764257550239563
Epoch 640, training loss: 13.068172454833984 = 0.43965017795562744 + 2.0 * 6.314260959625244
Epoch 640, val loss: 0.7591802477836609
Epoch 650, training loss: 13.045074462890625 = 0.42485305666923523 + 2.0 * 6.310110569000244
Epoch 650, val loss: 0.7544419765472412
Epoch 660, training loss: 13.027914047241211 = 0.4102231562137604 + 2.0 * 6.308845520019531
Epoch 660, val loss: 0.7500089406967163
Epoch 670, training loss: 13.019844055175781 = 0.3957631289958954 + 2.0 * 6.312040328979492
Epoch 670, val loss: 0.7458726167678833
Epoch 680, training loss: 13.003843307495117 = 0.3815666139125824 + 2.0 * 6.311138153076172
Epoch 680, val loss: 0.7420094609260559
Epoch 690, training loss: 12.979032516479492 = 0.3675384521484375 + 2.0 * 6.305747032165527
Epoch 690, val loss: 0.7384876608848572
Epoch 700, training loss: 12.961956024169922 = 0.35375452041625977 + 2.0 * 6.304100513458252
Epoch 700, val loss: 0.7352876663208008
Epoch 710, training loss: 12.953253746032715 = 0.3401641547679901 + 2.0 * 6.306544780731201
Epoch 710, val loss: 0.7324041128158569
Epoch 720, training loss: 12.938995361328125 = 0.3268394470214844 + 2.0 * 6.30607795715332
Epoch 720, val loss: 0.7298942804336548
Epoch 730, training loss: 12.918903350830078 = 0.31380578875541687 + 2.0 * 6.302548885345459
Epoch 730, val loss: 0.7277565598487854
Epoch 740, training loss: 12.902923583984375 = 0.3010503649711609 + 2.0 * 6.300936698913574
Epoch 740, val loss: 0.7260468602180481
Epoch 750, training loss: 12.890135765075684 = 0.2885754108428955 + 2.0 * 6.300780296325684
Epoch 750, val loss: 0.7247012853622437
Epoch 760, training loss: 12.872617721557617 = 0.27644652128219604 + 2.0 * 6.298085689544678
Epoch 760, val loss: 0.7237805128097534
Epoch 770, training loss: 12.858427047729492 = 0.2646736204624176 + 2.0 * 6.296876907348633
Epoch 770, val loss: 0.7232787013053894
Epoch 780, training loss: 12.854978561401367 = 0.2532669007778168 + 2.0 * 6.30085563659668
Epoch 780, val loss: 0.7233003973960876
Epoch 790, training loss: 12.842283248901367 = 0.24224241077899933 + 2.0 * 6.300020217895508
Epoch 790, val loss: 0.7237271666526794
Epoch 800, training loss: 12.82015609741211 = 0.23174095153808594 + 2.0 * 6.294207572937012
Epoch 800, val loss: 0.7245588302612305
Epoch 810, training loss: 12.811447143554688 = 0.2216644138097763 + 2.0 * 6.294891357421875
Epoch 810, val loss: 0.7258594632148743
Epoch 820, training loss: 12.800655364990234 = 0.21202990412712097 + 2.0 * 6.294312953948975
Epoch 820, val loss: 0.7275444865226746
Epoch 830, training loss: 12.78896427154541 = 0.20288732647895813 + 2.0 * 6.293038368225098
Epoch 830, val loss: 0.7296226620674133
Epoch 840, training loss: 12.77570629119873 = 0.1942286640405655 + 2.0 * 6.290738582611084
Epoch 840, val loss: 0.7320913076400757
Epoch 850, training loss: 12.765658378601074 = 0.18598128855228424 + 2.0 * 6.2898383140563965
Epoch 850, val loss: 0.7349271178245544
Epoch 860, training loss: 12.779230117797852 = 0.1781650334596634 + 2.0 * 6.300532341003418
Epoch 860, val loss: 0.7380396127700806
Epoch 870, training loss: 12.7503080368042 = 0.17081589996814728 + 2.0 * 6.289746284484863
Epoch 870, val loss: 0.7413167953491211
Epoch 880, training loss: 12.7467679977417 = 0.16386647522449493 + 2.0 * 6.2914509773254395
Epoch 880, val loss: 0.7449283003807068
Epoch 890, training loss: 12.730415344238281 = 0.15728306770324707 + 2.0 * 6.286566257476807
Epoch 890, val loss: 0.7486653923988342
Epoch 900, training loss: 12.727727890014648 = 0.1510433852672577 + 2.0 * 6.288342475891113
Epoch 900, val loss: 0.7525949478149414
Epoch 910, training loss: 12.723098754882812 = 0.14515340328216553 + 2.0 * 6.288972854614258
Epoch 910, val loss: 0.7566876411437988
Epoch 920, training loss: 12.710260391235352 = 0.13955020904541016 + 2.0 * 6.285355091094971
Epoch 920, val loss: 0.7608800530433655
Epoch 930, training loss: 12.70203971862793 = 0.13424593210220337 + 2.0 * 6.2838969230651855
Epoch 930, val loss: 0.7653406262397766
Epoch 940, training loss: 12.694775581359863 = 0.1291729211807251 + 2.0 * 6.282801151275635
Epoch 940, val loss: 0.7698583006858826
Epoch 950, training loss: 12.702622413635254 = 0.12434209883213043 + 2.0 * 6.289140224456787
Epoch 950, val loss: 0.774490475654602
Epoch 960, training loss: 12.694477081298828 = 0.11973407119512558 + 2.0 * 6.287371635437012
Epoch 960, val loss: 0.7789458632469177
Epoch 970, training loss: 12.680343627929688 = 0.11539498716592789 + 2.0 * 6.282474517822266
Epoch 970, val loss: 0.7836703062057495
Epoch 980, training loss: 12.670726776123047 = 0.11123566329479218 + 2.0 * 6.279745578765869
Epoch 980, val loss: 0.7885368466377258
Epoch 990, training loss: 12.666975021362305 = 0.10726222395896912 + 2.0 * 6.279856204986572
Epoch 990, val loss: 0.7934365272521973
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.788888888888889
0.8444913020558777
=== training gcn model ===
Epoch 0, training loss: 19.122371673583984 = 1.928694725036621 + 2.0 * 8.596837997436523
Epoch 0, val loss: 1.9211206436157227
Epoch 10, training loss: 19.111650466918945 = 1.918712854385376 + 2.0 * 8.596468925476074
Epoch 10, val loss: 1.9107351303100586
Epoch 20, training loss: 19.09288787841797 = 1.9059057235717773 + 2.0 * 8.593491554260254
Epoch 20, val loss: 1.8972694873809814
Epoch 30, training loss: 19.0277099609375 = 1.8881194591522217 + 2.0 * 8.569795608520508
Epoch 30, val loss: 1.8787130117416382
Epoch 40, training loss: 18.651126861572266 = 1.8659944534301758 + 2.0 * 8.392566680908203
Epoch 40, val loss: 1.856759786605835
Epoch 50, training loss: 17.275793075561523 = 1.8438494205474854 + 2.0 * 7.71597146987915
Epoch 50, val loss: 1.8361001014709473
Epoch 60, training loss: 16.461015701293945 = 1.8291293382644653 + 2.0 * 7.315943241119385
Epoch 60, val loss: 1.8234379291534424
Epoch 70, training loss: 16.003154754638672 = 1.8177423477172852 + 2.0 * 7.092705726623535
Epoch 70, val loss: 1.8132531642913818
Epoch 80, training loss: 15.732887268066406 = 1.806404948234558 + 2.0 * 6.963241100311279
Epoch 80, val loss: 1.8032376766204834
Epoch 90, training loss: 15.544974327087402 = 1.7941865921020508 + 2.0 * 6.875393867492676
Epoch 90, val loss: 1.792121410369873
Epoch 100, training loss: 15.375990867614746 = 1.7827197313308716 + 2.0 * 6.796635627746582
Epoch 100, val loss: 1.7820097208023071
Epoch 110, training loss: 15.24197769165039 = 1.7728747129440308 + 2.0 * 6.734551429748535
Epoch 110, val loss: 1.7734603881835938
Epoch 120, training loss: 15.116926193237305 = 1.7627707719802856 + 2.0 * 6.677077770233154
Epoch 120, val loss: 1.7647113800048828
Epoch 130, training loss: 15.022137641906738 = 1.7512626647949219 + 2.0 * 6.635437488555908
Epoch 130, val loss: 1.7545912265777588
Epoch 140, training loss: 14.945268630981445 = 1.7378901243209839 + 2.0 * 6.603689193725586
Epoch 140, val loss: 1.742915391921997
Epoch 150, training loss: 14.872356414794922 = 1.7229245901107788 + 2.0 * 6.574716091156006
Epoch 150, val loss: 1.7299680709838867
Epoch 160, training loss: 14.809471130371094 = 1.7060465812683105 + 2.0 * 6.5517120361328125
Epoch 160, val loss: 1.7155905961990356
Epoch 170, training loss: 14.7576904296875 = 1.6867247819900513 + 2.0 * 6.535482883453369
Epoch 170, val loss: 1.6992228031158447
Epoch 180, training loss: 14.699426651000977 = 1.664692759513855 + 2.0 * 6.517366886138916
Epoch 180, val loss: 1.6808465719223022
Epoch 190, training loss: 14.644026756286621 = 1.6396331787109375 + 2.0 * 6.502196788787842
Epoch 190, val loss: 1.6599091291427612
Epoch 200, training loss: 14.595329284667969 = 1.6111681461334229 + 2.0 * 6.4920806884765625
Epoch 200, val loss: 1.6362251043319702
Epoch 210, training loss: 14.536338806152344 = 1.5794028043746948 + 2.0 * 6.47846794128418
Epoch 210, val loss: 1.6099252700805664
Epoch 220, training loss: 14.48203182220459 = 1.5439659357070923 + 2.0 * 6.4690327644348145
Epoch 220, val loss: 1.5806094408035278
Epoch 230, training loss: 14.427352905273438 = 1.5049448013305664 + 2.0 * 6.4612040519714355
Epoch 230, val loss: 1.5485553741455078
Epoch 240, training loss: 14.363289833068848 = 1.4630261659622192 + 2.0 * 6.450131893157959
Epoch 240, val loss: 1.5140411853790283
Epoch 250, training loss: 14.301655769348145 = 1.4179284572601318 + 2.0 * 6.441863536834717
Epoch 250, val loss: 1.4771617650985718
Epoch 260, training loss: 14.25511646270752 = 1.3700343370437622 + 2.0 * 6.442541122436523
Epoch 260, val loss: 1.438122034072876
Epoch 270, training loss: 14.176980018615723 = 1.3204925060272217 + 2.0 * 6.428243637084961
Epoch 270, val loss: 1.3980048894882202
Epoch 280, training loss: 14.110286712646484 = 1.269620418548584 + 2.0 * 6.420332908630371
Epoch 280, val loss: 1.3570412397384644
Epoch 290, training loss: 14.045937538146973 = 1.2177692651748657 + 2.0 * 6.414083957672119
Epoch 290, val loss: 1.3155484199523926
Epoch 300, training loss: 13.987393379211426 = 1.1656512022018433 + 2.0 * 6.4108710289001465
Epoch 300, val loss: 1.2743761539459229
Epoch 310, training loss: 13.92363452911377 = 1.11454176902771 + 2.0 * 6.40454626083374
Epoch 310, val loss: 1.2342606782913208
Epoch 320, training loss: 13.859428405761719 = 1.064286470413208 + 2.0 * 6.397571086883545
Epoch 320, val loss: 1.1953011751174927
Epoch 330, training loss: 13.801043510437012 = 1.0152919292449951 + 2.0 * 6.392875671386719
Epoch 330, val loss: 1.1577869653701782
Epoch 340, training loss: 13.74725341796875 = 0.9682973623275757 + 2.0 * 6.3894782066345215
Epoch 340, val loss: 1.1223722696304321
Epoch 350, training loss: 13.691338539123535 = 0.9235847592353821 + 2.0 * 6.383876800537109
Epoch 350, val loss: 1.0892192125320435
Epoch 360, training loss: 13.640823364257812 = 0.8813297748565674 + 2.0 * 6.379746913909912
Epoch 360, val loss: 1.0584309101104736
Epoch 370, training loss: 13.593941688537598 = 0.841450035572052 + 2.0 * 6.376245975494385
Epoch 370, val loss: 1.0300941467285156
Epoch 380, training loss: 13.55538272857666 = 0.8043354153633118 + 2.0 * 6.375523567199707
Epoch 380, val loss: 1.004195213317871
Epoch 390, training loss: 13.512312889099121 = 0.7700690627098083 + 2.0 * 6.371121883392334
Epoch 390, val loss: 0.9811233878135681
Epoch 400, training loss: 13.47010612487793 = 0.7383763194084167 + 2.0 * 6.3658647537231445
Epoch 400, val loss: 0.9604781270027161
Epoch 410, training loss: 13.436620712280273 = 0.7087521553039551 + 2.0 * 6.363934516906738
Epoch 410, val loss: 0.9419988393783569
Epoch 420, training loss: 13.400555610656738 = 0.6810008883476257 + 2.0 * 6.359777450561523
Epoch 420, val loss: 0.9255545139312744
Epoch 430, training loss: 13.375588417053223 = 0.6550169587135315 + 2.0 * 6.360285758972168
Epoch 430, val loss: 0.9107909798622131
Epoch 440, training loss: 13.336360931396484 = 0.6304304003715515 + 2.0 * 6.352965354919434
Epoch 440, val loss: 0.8977251648902893
Epoch 450, training loss: 13.307523727416992 = 0.6070389151573181 + 2.0 * 6.350242614746094
Epoch 450, val loss: 0.886093258857727
Epoch 460, training loss: 13.2858304977417 = 0.5846516489982605 + 2.0 * 6.350589275360107
Epoch 460, val loss: 0.8755936622619629
Epoch 470, training loss: 13.256333351135254 = 0.5632568001747131 + 2.0 * 6.346538066864014
Epoch 470, val loss: 0.8662093281745911
Epoch 480, training loss: 13.228031158447266 = 0.5427041053771973 + 2.0 * 6.342663764953613
Epoch 480, val loss: 0.8578929901123047
Epoch 490, training loss: 13.216594696044922 = 0.5229704976081848 + 2.0 * 6.3468122482299805
Epoch 490, val loss: 0.8504810333251953
Epoch 500, training loss: 13.186895370483398 = 0.5040669441223145 + 2.0 * 6.341414451599121
Epoch 500, val loss: 0.8440024852752686
Epoch 510, training loss: 13.15944766998291 = 0.4859153628349304 + 2.0 * 6.336766242980957
Epoch 510, val loss: 0.8384568095207214
Epoch 520, training loss: 13.135507583618164 = 0.46834802627563477 + 2.0 * 6.3335795402526855
Epoch 520, val loss: 0.8336873650550842
Epoch 530, training loss: 13.121289253234863 = 0.45130541920661926 + 2.0 * 6.334991931915283
Epoch 530, val loss: 0.8296749591827393
Epoch 540, training loss: 13.107219696044922 = 0.43501797318458557 + 2.0 * 6.336101055145264
Epoch 540, val loss: 0.8261166214942932
Epoch 550, training loss: 13.076828956604004 = 0.41929835081100464 + 2.0 * 6.328765392303467
Epoch 550, val loss: 0.823408305644989
Epoch 560, training loss: 13.057358741760254 = 0.40408238768577576 + 2.0 * 6.326638221740723
Epoch 560, val loss: 0.8213443756103516
Epoch 570, training loss: 13.05156135559082 = 0.3893987238407135 + 2.0 * 6.331081390380859
Epoch 570, val loss: 0.8197380900382996
Epoch 580, training loss: 13.025644302368164 = 0.37523096799850464 + 2.0 * 6.325206756591797
Epoch 580, val loss: 0.8187013864517212
Epoch 590, training loss: 13.005497932434082 = 0.3615436851978302 + 2.0 * 6.321977138519287
Epoch 590, val loss: 0.8181923627853394
Epoch 600, training loss: 13.003161430358887 = 0.3482474982738495 + 2.0 * 6.327456951141357
Epoch 600, val loss: 0.8182037472724915
Epoch 610, training loss: 12.9732666015625 = 0.33534789085388184 + 2.0 * 6.3189592361450195
Epoch 610, val loss: 0.8186088800430298
Epoch 620, training loss: 12.956295013427734 = 0.32282552123069763 + 2.0 * 6.316734790802002
Epoch 620, val loss: 0.8194688558578491
Epoch 630, training loss: 12.94324779510498 = 0.31058841943740845 + 2.0 * 6.316329479217529
Epoch 630, val loss: 0.8207512497901917
Epoch 640, training loss: 12.926812171936035 = 0.29864272475242615 + 2.0 * 6.314084529876709
Epoch 640, val loss: 0.8224008083343506
Epoch 650, training loss: 12.916024208068848 = 0.28707900643348694 + 2.0 * 6.314472675323486
Epoch 650, val loss: 0.8243563771247864
Epoch 660, training loss: 12.899832725524902 = 0.27576735615730286 + 2.0 * 6.312032699584961
Epoch 660, val loss: 0.8267250657081604
Epoch 670, training loss: 12.922025680541992 = 0.26476940512657166 + 2.0 * 6.328628063201904
Epoch 670, val loss: 0.8294116258621216
Epoch 680, training loss: 12.873722076416016 = 0.25404298305511475 + 2.0 * 6.309839725494385
Epoch 680, val loss: 0.8323490619659424
Epoch 690, training loss: 12.858901977539062 = 0.2436932623386383 + 2.0 * 6.3076043128967285
Epoch 690, val loss: 0.835648238658905
Epoch 700, training loss: 12.846869468688965 = 0.23362135887145996 + 2.0 * 6.306623935699463
Epoch 700, val loss: 0.839310884475708
Epoch 710, training loss: 12.854047775268555 = 0.2238406389951706 + 2.0 * 6.315103530883789
Epoch 710, val loss: 0.843280553817749
Epoch 720, training loss: 12.829944610595703 = 0.21438732743263245 + 2.0 * 6.307778835296631
Epoch 720, val loss: 0.8474511504173279
Epoch 730, training loss: 12.815756797790527 = 0.20528452098369598 + 2.0 * 6.305236339569092
Epoch 730, val loss: 0.8519056439399719
Epoch 740, training loss: 12.804671287536621 = 0.19649682939052582 + 2.0 * 6.304087162017822
Epoch 740, val loss: 0.8566516637802124
Epoch 750, training loss: 12.790465354919434 = 0.18805257976055145 + 2.0 * 6.301206588745117
Epoch 750, val loss: 0.8615697026252747
Epoch 760, training loss: 12.783032417297363 = 0.17994071543216705 + 2.0 * 6.3015456199646
Epoch 760, val loss: 0.8667998909950256
Epoch 770, training loss: 12.773268699645996 = 0.17216455936431885 + 2.0 * 6.300551891326904
Epoch 770, val loss: 0.8721928000450134
Epoch 780, training loss: 12.763694763183594 = 0.16470637917518616 + 2.0 * 6.29949426651001
Epoch 780, val loss: 0.8778916001319885
Epoch 790, training loss: 12.752547264099121 = 0.15759797394275665 + 2.0 * 6.2974748611450195
Epoch 790, val loss: 0.8838070034980774
Epoch 800, training loss: 12.7445707321167 = 0.15076899528503418 + 2.0 * 6.296900749206543
Epoch 800, val loss: 0.8899673223495483
Epoch 810, training loss: 12.740257263183594 = 0.14425788819789886 + 2.0 * 6.297999858856201
Epoch 810, val loss: 0.8963246941566467
Epoch 820, training loss: 12.733738899230957 = 0.13804744184017181 + 2.0 * 6.297845840454102
Epoch 820, val loss: 0.9027235507965088
Epoch 830, training loss: 12.721149444580078 = 0.13213326036930084 + 2.0 * 6.29450798034668
Epoch 830, val loss: 0.9093711972236633
Epoch 840, training loss: 12.716832160949707 = 0.1265089511871338 + 2.0 * 6.295161724090576
Epoch 840, val loss: 0.916171133518219
Epoch 850, training loss: 12.708581924438477 = 0.12115511298179626 + 2.0 * 6.293713569641113
Epoch 850, val loss: 0.9230992197990417
Epoch 860, training loss: 12.717033386230469 = 0.11604253947734833 + 2.0 * 6.300495624542236
Epoch 860, val loss: 0.9300873279571533
Epoch 870, training loss: 12.6958589553833 = 0.11121032387018204 + 2.0 * 6.292324542999268
Epoch 870, val loss: 0.9371494054794312
Epoch 880, training loss: 12.68472957611084 = 0.10660445690155029 + 2.0 * 6.2890625
Epoch 880, val loss: 0.9442721605300903
Epoch 890, training loss: 12.689305305480957 = 0.10222549736499786 + 2.0 * 6.293540000915527
Epoch 890, val loss: 0.9514819383621216
Epoch 900, training loss: 12.676244735717773 = 0.09805367887020111 + 2.0 * 6.289095401763916
Epoch 900, val loss: 0.9586955904960632
Epoch 910, training loss: 12.667991638183594 = 0.0940956249833107 + 2.0 * 6.286948204040527
Epoch 910, val loss: 0.9659554958343506
Epoch 920, training loss: 12.66214656829834 = 0.09032856673002243 + 2.0 * 6.285909175872803
Epoch 920, val loss: 0.973268985748291
Epoch 930, training loss: 12.674009323120117 = 0.08674289286136627 + 2.0 * 6.293632984161377
Epoch 930, val loss: 0.9805676341056824
Epoch 940, training loss: 12.657822608947754 = 0.08335305750370026 + 2.0 * 6.287234783172607
Epoch 940, val loss: 0.9877217411994934
Epoch 950, training loss: 12.650320053100586 = 0.080137699842453 + 2.0 * 6.285091400146484
Epoch 950, val loss: 0.994937002658844
Epoch 960, training loss: 12.64312744140625 = 0.07708775252103806 + 2.0 * 6.28302001953125
Epoch 960, val loss: 1.0022422075271606
Epoch 970, training loss: 12.650408744812012 = 0.07418105006217957 + 2.0 * 6.288114070892334
Epoch 970, val loss: 1.0095117092132568
Epoch 980, training loss: 12.640304565429688 = 0.07141242921352386 + 2.0 * 6.2844462394714355
Epoch 980, val loss: 1.0166218280792236
Epoch 990, training loss: 12.632168769836426 = 0.06879191100597382 + 2.0 * 6.281688213348389
Epoch 990, val loss: 1.0237646102905273
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7962962962962963
0.8386926726410122
=== training gcn model ===
Epoch 0, training loss: 19.127920150756836 = 1.9342796802520752 + 2.0 * 8.596819877624512
Epoch 0, val loss: 1.9312814474105835
Epoch 10, training loss: 19.117338180541992 = 1.924496054649353 + 2.0 * 8.596421241760254
Epoch 10, val loss: 1.9222570657730103
Epoch 20, training loss: 19.098430633544922 = 1.9123585224151611 + 2.0 * 8.593035697937012
Epoch 20, val loss: 1.910618782043457
Epoch 30, training loss: 19.02543830871582 = 1.8960037231445312 + 2.0 * 8.564717292785645
Epoch 30, val loss: 1.8946259021759033
Epoch 40, training loss: 18.550739288330078 = 1.8765661716461182 + 2.0 * 8.33708667755127
Epoch 40, val loss: 1.8758426904678345
Epoch 50, training loss: 17.294748306274414 = 1.8570407629013062 + 2.0 * 7.71885347366333
Epoch 50, val loss: 1.8572710752487183
Epoch 60, training loss: 16.272825241088867 = 1.8438705205917358 + 2.0 * 7.214477062225342
Epoch 60, val loss: 1.8448423147201538
Epoch 70, training loss: 15.778417587280273 = 1.8337055444717407 + 2.0 * 6.972355842590332
Epoch 70, val loss: 1.8345171213150024
Epoch 80, training loss: 15.528029441833496 = 1.8225001096725464 + 2.0 * 6.85276460647583
Epoch 80, val loss: 1.823226809501648
Epoch 90, training loss: 15.366731643676758 = 1.8103018999099731 + 2.0 * 6.778214931488037
Epoch 90, val loss: 1.8116254806518555
Epoch 100, training loss: 15.23639965057373 = 1.7991218566894531 + 2.0 * 6.718638896942139
Epoch 100, val loss: 1.8009960651397705
Epoch 110, training loss: 15.133966445922852 = 1.7892900705337524 + 2.0 * 6.672338008880615
Epoch 110, val loss: 1.791380763053894
Epoch 120, training loss: 15.055638313293457 = 1.7797918319702148 + 2.0 * 6.637923240661621
Epoch 120, val loss: 1.7819037437438965
Epoch 130, training loss: 14.983566284179688 = 1.7698618173599243 + 2.0 * 6.606852054595947
Epoch 130, val loss: 1.7722091674804688
Epoch 140, training loss: 14.914774894714355 = 1.7596131563186646 + 2.0 * 6.57758092880249
Epoch 140, val loss: 1.7623754739761353
Epoch 150, training loss: 14.855693817138672 = 1.7486761808395386 + 2.0 * 6.553508758544922
Epoch 150, val loss: 1.7523447275161743
Epoch 160, training loss: 14.801408767700195 = 1.7367631196975708 + 2.0 * 6.532322883605957
Epoch 160, val loss: 1.7416049242019653
Epoch 170, training loss: 14.753789901733398 = 1.723215103149414 + 2.0 * 6.515287399291992
Epoch 170, val loss: 1.7296491861343384
Epoch 180, training loss: 14.70889949798584 = 1.7077542543411255 + 2.0 * 6.500572681427002
Epoch 180, val loss: 1.7162437438964844
Epoch 190, training loss: 14.664701461791992 = 1.6904265880584717 + 2.0 * 6.487137317657471
Epoch 190, val loss: 1.7012721300125122
Epoch 200, training loss: 14.620166778564453 = 1.6708546876907349 + 2.0 * 6.474656105041504
Epoch 200, val loss: 1.684437870979309
Epoch 210, training loss: 14.574713706970215 = 1.6484233140945435 + 2.0 * 6.4631452560424805
Epoch 210, val loss: 1.6653097867965698
Epoch 220, training loss: 14.548526763916016 = 1.6226098537445068 + 2.0 * 6.462958335876465
Epoch 220, val loss: 1.6434576511383057
Epoch 230, training loss: 14.48630428314209 = 1.5938379764556885 + 2.0 * 6.44623327255249
Epoch 230, val loss: 1.6190268993377686
Epoch 240, training loss: 14.43416976928711 = 1.5613889694213867 + 2.0 * 6.436390399932861
Epoch 240, val loss: 1.5915324687957764
Epoch 250, training loss: 14.381270408630371 = 1.5250303745269775 + 2.0 * 6.428120136260986
Epoch 250, val loss: 1.5609058141708374
Epoch 260, training loss: 14.326013565063477 = 1.4843621253967285 + 2.0 * 6.420825958251953
Epoch 260, val loss: 1.5267881155014038
Epoch 270, training loss: 14.268682479858398 = 1.439501166343689 + 2.0 * 6.414590835571289
Epoch 270, val loss: 1.489375114440918
Epoch 280, training loss: 14.217044830322266 = 1.3913837671279907 + 2.0 * 6.412830352783203
Epoch 280, val loss: 1.4498361349105835
Epoch 290, training loss: 14.150076866149902 = 1.3413251638412476 + 2.0 * 6.404376029968262
Epoch 290, val loss: 1.4090020656585693
Epoch 300, training loss: 14.085518836975098 = 1.2899179458618164 + 2.0 * 6.397800445556641
Epoch 300, val loss: 1.3674938678741455
Epoch 310, training loss: 14.025069236755371 = 1.2376865148544312 + 2.0 * 6.393691539764404
Epoch 310, val loss: 1.325854778289795
Epoch 320, training loss: 13.973654747009277 = 1.1858502626419067 + 2.0 * 6.39390230178833
Epoch 320, val loss: 1.285487174987793
Epoch 330, training loss: 13.90632438659668 = 1.1357712745666504 + 2.0 * 6.3852763175964355
Epoch 330, val loss: 1.2466011047363281
Epoch 340, training loss: 13.850235939025879 = 1.0870000123977661 + 2.0 * 6.381618022918701
Epoch 340, val loss: 1.2092396020889282
Epoch 350, training loss: 13.800241470336914 = 1.0395498275756836 + 2.0 * 6.380345821380615
Epoch 350, val loss: 1.1732960939407349
Epoch 360, training loss: 13.740973472595215 = 0.9935007095336914 + 2.0 * 6.373736381530762
Epoch 360, val loss: 1.1388713121414185
Epoch 370, training loss: 13.689604759216309 = 0.9490184783935547 + 2.0 * 6.370293140411377
Epoch 370, val loss: 1.1056652069091797
Epoch 380, training loss: 13.647736549377441 = 0.9059590101242065 + 2.0 * 6.370888710021973
Epoch 380, val loss: 1.073675274848938
Epoch 390, training loss: 13.595447540283203 = 0.8647105693817139 + 2.0 * 6.365368366241455
Epoch 390, val loss: 1.043264389038086
Epoch 400, training loss: 13.54746150970459 = 0.8255832195281982 + 2.0 * 6.360939025878906
Epoch 400, val loss: 1.0143574476242065
Epoch 410, training loss: 13.504073143005371 = 0.7880927324295044 + 2.0 * 6.357990264892578
Epoch 410, val loss: 0.9868175387382507
Epoch 420, training loss: 13.468587875366211 = 0.7523220181465149 + 2.0 * 6.358132839202881
Epoch 420, val loss: 0.9606180787086487
Epoch 430, training loss: 13.426600456237793 = 0.7184790372848511 + 2.0 * 6.354060649871826
Epoch 430, val loss: 0.9360379576683044
Epoch 440, training loss: 13.386163711547852 = 0.6864575147628784 + 2.0 * 6.349853038787842
Epoch 440, val loss: 0.9129803776741028
Epoch 450, training loss: 13.358413696289062 = 0.6560359597206116 + 2.0 * 6.351188659667969
Epoch 450, val loss: 0.8913589715957642
Epoch 460, training loss: 13.315119743347168 = 0.627435564994812 + 2.0 * 6.343842029571533
Epoch 460, val loss: 0.8711958527565002
Epoch 470, training loss: 13.283868789672852 = 0.6004857420921326 + 2.0 * 6.341691493988037
Epoch 470, val loss: 0.8525542616844177
Epoch 480, training loss: 13.255104064941406 = 0.5749518275260925 + 2.0 * 6.340075969696045
Epoch 480, val loss: 0.8352199196815491
Epoch 490, training loss: 13.226537704467773 = 0.5507857203483582 + 2.0 * 6.337875843048096
Epoch 490, val loss: 0.8192293047904968
Epoch 500, training loss: 13.197736740112305 = 0.5279328227043152 + 2.0 * 6.334901809692383
Epoch 500, val loss: 0.8044934272766113
Epoch 510, training loss: 13.170634269714355 = 0.5062026381492615 + 2.0 * 6.332215785980225
Epoch 510, val loss: 0.790967583656311
Epoch 520, training loss: 13.156435012817383 = 0.4855622947216034 + 2.0 * 6.3354363441467285
Epoch 520, val loss: 0.7784242630004883
Epoch 530, training loss: 13.128710746765137 = 0.46588778495788574 + 2.0 * 6.331411361694336
Epoch 530, val loss: 0.7668753862380981
Epoch 540, training loss: 13.102091789245605 = 0.44729310274124146 + 2.0 * 6.327399253845215
Epoch 540, val loss: 0.7562355995178223
Epoch 550, training loss: 13.078743934631348 = 0.42952436208724976 + 2.0 * 6.324609756469727
Epoch 550, val loss: 0.7464499473571777
Epoch 560, training loss: 13.056663513183594 = 0.4124184846878052 + 2.0 * 6.322122573852539
Epoch 560, val loss: 0.7373893857002258
Epoch 570, training loss: 13.046011924743652 = 0.3959333002567291 + 2.0 * 6.325039386749268
Epoch 570, val loss: 0.7289597988128662
Epoch 580, training loss: 13.025638580322266 = 0.38020139932632446 + 2.0 * 6.322718620300293
Epoch 580, val loss: 0.7210954427719116
Epoch 590, training loss: 13.001657485961914 = 0.36504000425338745 + 2.0 * 6.3183088302612305
Epoch 590, val loss: 0.7138292789459229
Epoch 600, training loss: 12.98170280456543 = 0.35040634870529175 + 2.0 * 6.315648078918457
Epoch 600, val loss: 0.7070339322090149
Epoch 610, training loss: 12.971610069274902 = 0.3361660838127136 + 2.0 * 6.317721843719482
Epoch 610, val loss: 0.7006896138191223
Epoch 620, training loss: 12.966726303100586 = 0.32239747047424316 + 2.0 * 6.322164535522461
Epoch 620, val loss: 0.6947224736213684
Epoch 630, training loss: 12.935399055480957 = 0.30920782685279846 + 2.0 * 6.313095569610596
Epoch 630, val loss: 0.6890367269515991
Epoch 640, training loss: 12.914188385009766 = 0.29630008339881897 + 2.0 * 6.308944225311279
Epoch 640, val loss: 0.6838899850845337
Epoch 650, training loss: 12.899295806884766 = 0.2838207185268402 + 2.0 * 6.307737350463867
Epoch 650, val loss: 0.6790429353713989
Epoch 660, training loss: 12.888130187988281 = 0.2716512382030487 + 2.0 * 6.308239459991455
Epoch 660, val loss: 0.6745255589485168
Epoch 670, training loss: 12.879562377929688 = 0.2598915100097656 + 2.0 * 6.309835433959961
Epoch 670, val loss: 0.6702556014060974
Epoch 680, training loss: 12.863194465637207 = 0.24855448305606842 + 2.0 * 6.3073201179504395
Epoch 680, val loss: 0.6663289666175842
Epoch 690, training loss: 12.843096733093262 = 0.2375791221857071 + 2.0 * 6.302758693695068
Epoch 690, val loss: 0.6627811193466187
Epoch 700, training loss: 12.828548431396484 = 0.22699131071567535 + 2.0 * 6.300778388977051
Epoch 700, val loss: 0.6595392227172852
Epoch 710, training loss: 12.822633743286133 = 0.2167292982339859 + 2.0 * 6.302952289581299
Epoch 710, val loss: 0.6566347479820251
Epoch 720, training loss: 12.812904357910156 = 0.20693708956241608 + 2.0 * 6.30298376083374
Epoch 720, val loss: 0.6538835167884827
Epoch 730, training loss: 12.793590545654297 = 0.1975037306547165 + 2.0 * 6.298043251037598
Epoch 730, val loss: 0.6515604257583618
Epoch 740, training loss: 12.782066345214844 = 0.18850238621234894 + 2.0 * 6.29678201675415
Epoch 740, val loss: 0.6495581865310669
Epoch 750, training loss: 12.769966125488281 = 0.1798546463251114 + 2.0 * 6.295055866241455
Epoch 750, val loss: 0.6478999853134155
Epoch 760, training loss: 12.75927734375 = 0.17156945168972015 + 2.0 * 6.293853759765625
Epoch 760, val loss: 0.6465352773666382
Epoch 770, training loss: 12.765376091003418 = 0.1636437624692917 + 2.0 * 6.30086612701416
Epoch 770, val loss: 0.6454259753227234
Epoch 780, training loss: 12.749252319335938 = 0.15609368681907654 + 2.0 * 6.296579360961914
Epoch 780, val loss: 0.6447122693061829
Epoch 790, training loss: 12.733039855957031 = 0.14900119602680206 + 2.0 * 6.292019367218018
Epoch 790, val loss: 0.6441830396652222
Epoch 800, training loss: 12.72268295288086 = 0.14225177466869354 + 2.0 * 6.290215492248535
Epoch 800, val loss: 0.6440044045448303
Epoch 810, training loss: 12.719843864440918 = 0.13584715127944946 + 2.0 * 6.291998386383057
Epoch 810, val loss: 0.6441223621368408
Epoch 820, training loss: 12.706241607666016 = 0.12975913286209106 + 2.0 * 6.288241386413574
Epoch 820, val loss: 0.6444994807243347
Epoch 830, training loss: 12.706387519836426 = 0.12401050329208374 + 2.0 * 6.291188716888428
Epoch 830, val loss: 0.6450778245925903
Epoch 840, training loss: 12.692605018615723 = 0.1185973733663559 + 2.0 * 6.287003993988037
Epoch 840, val loss: 0.6459336280822754
Epoch 850, training loss: 12.684098243713379 = 0.11347523331642151 + 2.0 * 6.285311698913574
Epoch 850, val loss: 0.6469792127609253
Epoch 860, training loss: 12.677116394042969 = 0.10861621797084808 + 2.0 * 6.284250259399414
Epoch 860, val loss: 0.6483004093170166
Epoch 870, training loss: 12.684292793273926 = 0.10401179641485214 + 2.0 * 6.290140628814697
Epoch 870, val loss: 0.6498128175735474
Epoch 880, training loss: 12.677445411682129 = 0.09967820346355438 + 2.0 * 6.288883686065674
Epoch 880, val loss: 0.6514706611633301
Epoch 890, training loss: 12.663773536682129 = 0.0955999419093132 + 2.0 * 6.28408670425415
Epoch 890, val loss: 0.6532716155052185
Epoch 900, training loss: 12.653023719787598 = 0.09171620011329651 + 2.0 * 6.280653953552246
Epoch 900, val loss: 0.6552972793579102
Epoch 910, training loss: 12.65178108215332 = 0.08803762495517731 + 2.0 * 6.281871795654297
Epoch 910, val loss: 0.657493531703949
Epoch 920, training loss: 12.65188217163086 = 0.08455432206392288 + 2.0 * 6.283663749694824
Epoch 920, val loss: 0.6597932577133179
Epoch 930, training loss: 12.641613006591797 = 0.08127358555793762 + 2.0 * 6.280169486999512
Epoch 930, val loss: 0.6621524691581726
Epoch 940, training loss: 12.647204399108887 = 0.07812122255563736 + 2.0 * 6.284541606903076
Epoch 940, val loss: 0.664680540561676
Epoch 950, training loss: 12.634872436523438 = 0.07515570521354675 + 2.0 * 6.279858589172363
Epoch 950, val loss: 0.6673022508621216
Epoch 960, training loss: 12.625800132751465 = 0.0723334550857544 + 2.0 * 6.2767333984375
Epoch 960, val loss: 0.6699480414390564
Epoch 970, training loss: 12.620977401733398 = 0.06964446604251862 + 2.0 * 6.275666236877441
Epoch 970, val loss: 0.672737717628479
Epoch 980, training loss: 12.629780769348145 = 0.06708944588899612 + 2.0 * 6.281345844268799
Epoch 980, val loss: 0.6755766272544861
Epoch 990, training loss: 12.625861167907715 = 0.06465961784124374 + 2.0 * 6.280600547790527
Epoch 990, val loss: 0.6784535646438599
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8037037037037037
0.838165524512388
The final CL Acc:0.79630, 0.00605, The final GNN Acc:0.84045, 0.00287
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.5 der2:0 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11574])
remove edge: torch.Size([2, 9490])
updated graph: torch.Size([2, 10508])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 19.135303497314453 = 1.9415518045425415 + 2.0 * 8.59687614440918
Epoch 0, val loss: 1.9439929723739624
Epoch 10, training loss: 19.124332427978516 = 1.9310083389282227 + 2.0 * 8.596662521362305
Epoch 10, val loss: 1.934254765510559
Epoch 20, training loss: 19.107664108276367 = 1.9177275896072388 + 2.0 * 8.59496784210205
Epoch 20, val loss: 1.921617031097412
Epoch 30, training loss: 19.06122398376465 = 1.8993968963623047 + 2.0 * 8.580913543701172
Epoch 30, val loss: 1.9039092063903809
Epoch 40, training loss: 18.83414649963379 = 1.8756016492843628 + 2.0 * 8.479272842407227
Epoch 40, val loss: 1.8816561698913574
Epoch 50, training loss: 18.13243865966797 = 1.8484734296798706 + 2.0 * 8.141983032226562
Epoch 50, val loss: 1.857552409172058
Epoch 60, training loss: 17.368417739868164 = 1.829569935798645 + 2.0 * 7.769423484802246
Epoch 60, val loss: 1.8404605388641357
Epoch 70, training loss: 16.665603637695312 = 1.8172446489334106 + 2.0 * 7.4241790771484375
Epoch 70, val loss: 1.8289060592651367
Epoch 80, training loss: 16.065650939941406 = 1.8067808151245117 + 2.0 * 7.1294355392456055
Epoch 80, val loss: 1.8197661638259888
Epoch 90, training loss: 15.702943801879883 = 1.7945737838745117 + 2.0 * 6.9541850090026855
Epoch 90, val loss: 1.8089914321899414
Epoch 100, training loss: 15.500104904174805 = 1.7816576957702637 + 2.0 * 6.859223365783691
Epoch 100, val loss: 1.797111988067627
Epoch 110, training loss: 15.350293159484863 = 1.769026517868042 + 2.0 * 6.790633201599121
Epoch 110, val loss: 1.785107135772705
Epoch 120, training loss: 15.22689437866211 = 1.756468653678894 + 2.0 * 6.735212802886963
Epoch 120, val loss: 1.773496150970459
Epoch 130, training loss: 15.123634338378906 = 1.7432745695114136 + 2.0 * 6.690179824829102
Epoch 130, val loss: 1.7618736028671265
Epoch 140, training loss: 15.045878410339355 = 1.7288447618484497 + 2.0 * 6.658516883850098
Epoch 140, val loss: 1.7496520280838013
Epoch 150, training loss: 14.95817756652832 = 1.713113784790039 + 2.0 * 6.622531890869141
Epoch 150, val loss: 1.736721396446228
Epoch 160, training loss: 14.88831615447998 = 1.6957660913467407 + 2.0 * 6.5962748527526855
Epoch 160, val loss: 1.7225722074508667
Epoch 170, training loss: 14.828887939453125 = 1.6764928102493286 + 2.0 * 6.576197624206543
Epoch 170, val loss: 1.7071117162704468
Epoch 180, training loss: 14.764294624328613 = 1.655076026916504 + 2.0 * 6.554609298706055
Epoch 180, val loss: 1.6900826692581177
Epoch 190, training loss: 14.702962875366211 = 1.6316086053848267 + 2.0 * 6.535676956176758
Epoch 190, val loss: 1.671565294265747
Epoch 200, training loss: 14.644906997680664 = 1.6058950424194336 + 2.0 * 6.519505977630615
Epoch 200, val loss: 1.6513991355895996
Epoch 210, training loss: 14.612297058105469 = 1.5779681205749512 + 2.0 * 6.51716423034668
Epoch 210, val loss: 1.6295897960662842
Epoch 220, training loss: 14.544615745544434 = 1.548183798789978 + 2.0 * 6.498216152191162
Epoch 220, val loss: 1.6064987182617188
Epoch 230, training loss: 14.483291625976562 = 1.5168629884719849 + 2.0 * 6.483214378356934
Epoch 230, val loss: 1.582367181777954
Epoch 240, training loss: 14.43049430847168 = 1.4838640689849854 + 2.0 * 6.473315238952637
Epoch 240, val loss: 1.5571082830429077
Epoch 250, training loss: 14.376851081848145 = 1.4492906332015991 + 2.0 * 6.463780403137207
Epoch 250, val loss: 1.5309374332427979
Epoch 260, training loss: 14.33405876159668 = 1.4136486053466797 + 2.0 * 6.460205078125
Epoch 260, val loss: 1.5043985843658447
Epoch 270, training loss: 14.277016639709473 = 1.3777992725372314 + 2.0 * 6.44960880279541
Epoch 270, val loss: 1.4780399799346924
Epoch 280, training loss: 14.223465919494629 = 1.3419471979141235 + 2.0 * 6.440759181976318
Epoch 280, val loss: 1.4522359371185303
Epoch 290, training loss: 14.171332359313965 = 1.306014895439148 + 2.0 * 6.432658672332764
Epoch 290, val loss: 1.4268450736999512
Epoch 300, training loss: 14.131450653076172 = 1.2703139781951904 + 2.0 * 6.430568218231201
Epoch 300, val loss: 1.402069091796875
Epoch 310, training loss: 14.07786750793457 = 1.2353640794754028 + 2.0 * 6.4212517738342285
Epoch 310, val loss: 1.3782141208648682
Epoch 320, training loss: 14.030097961425781 = 1.201043725013733 + 2.0 * 6.41452693939209
Epoch 320, val loss: 1.3553138971328735
Epoch 330, training loss: 13.985028266906738 = 1.1673219203948975 + 2.0 * 6.408853054046631
Epoch 330, val loss: 1.3331485986709595
Epoch 340, training loss: 13.952996253967285 = 1.1343287229537964 + 2.0 * 6.4093337059021
Epoch 340, val loss: 1.3117355108261108
Epoch 350, training loss: 13.909021377563477 = 1.1025527715682983 + 2.0 * 6.403234481811523
Epoch 350, val loss: 1.2912753820419312
Epoch 360, training loss: 13.861144065856934 = 1.071542739868164 + 2.0 * 6.394800662994385
Epoch 360, val loss: 1.2715399265289307
Epoch 370, training loss: 13.823355674743652 = 1.0410640239715576 + 2.0 * 6.391145706176758
Epoch 370, val loss: 1.2523796558380127
Epoch 380, training loss: 13.78661823272705 = 1.011034607887268 + 2.0 * 6.387791633605957
Epoch 380, val loss: 1.2335389852523804
Epoch 390, training loss: 13.75842571258545 = 0.9813453555107117 + 2.0 * 6.388540267944336
Epoch 390, val loss: 1.2150344848632812
Epoch 400, training loss: 13.715580940246582 = 0.9521642327308655 + 2.0 * 6.381708145141602
Epoch 400, val loss: 1.1969093084335327
Epoch 410, training loss: 13.676980972290039 = 0.9231582880020142 + 2.0 * 6.376911163330078
Epoch 410, val loss: 1.179093599319458
Epoch 420, training loss: 13.662676811218262 = 0.8942108154296875 + 2.0 * 6.384232997894287
Epoch 420, val loss: 1.161350965499878
Epoch 430, training loss: 13.610881805419922 = 0.8654694557189941 + 2.0 * 6.372706413269043
Epoch 430, val loss: 1.1436970233917236
Epoch 440, training loss: 13.57354736328125 = 0.8367408514022827 + 2.0 * 6.368403434753418
Epoch 440, val loss: 1.1262985467910767
Epoch 450, training loss: 13.540558815002441 = 0.8081390857696533 + 2.0 * 6.366209983825684
Epoch 450, val loss: 1.1090667247772217
Epoch 460, training loss: 13.505036354064941 = 0.779705822467804 + 2.0 * 6.362665176391602
Epoch 460, val loss: 1.0919908285140991
Epoch 470, training loss: 13.473701477050781 = 0.7514919638633728 + 2.0 * 6.361104965209961
Epoch 470, val loss: 1.075377345085144
Epoch 480, training loss: 13.444416999816895 = 0.7235743403434753 + 2.0 * 6.360421180725098
Epoch 480, val loss: 1.0591405630111694
Epoch 490, training loss: 13.409340858459473 = 0.6961005926132202 + 2.0 * 6.3566203117370605
Epoch 490, val loss: 1.043610692024231
Epoch 500, training loss: 13.3778715133667 = 0.6691533327102661 + 2.0 * 6.354359149932861
Epoch 500, val loss: 1.028688907623291
Epoch 510, training loss: 13.356435775756836 = 0.6427319645881653 + 2.0 * 6.356852054595947
Epoch 510, val loss: 1.0146501064300537
Epoch 520, training loss: 13.319212913513184 = 0.616996705532074 + 2.0 * 6.351108074188232
Epoch 520, val loss: 1.0014102458953857
Epoch 530, training loss: 13.288321495056152 = 0.5918810963630676 + 2.0 * 6.348220348358154
Epoch 530, val loss: 0.9891684055328369
Epoch 540, training loss: 13.259543418884277 = 0.5674059987068176 + 2.0 * 6.346068859100342
Epoch 540, val loss: 0.9778798222541809
Epoch 550, training loss: 13.243321418762207 = 0.5435783267021179 + 2.0 * 6.349871635437012
Epoch 550, val loss: 0.9675784707069397
Epoch 560, training loss: 13.210779190063477 = 0.5204921960830688 + 2.0 * 6.3451433181762695
Epoch 560, val loss: 0.9581913948059082
Epoch 570, training loss: 13.181347846984863 = 0.498190313577652 + 2.0 * 6.341578960418701
Epoch 570, val loss: 0.9499102830886841
Epoch 580, training loss: 13.16071891784668 = 0.4766101539134979 + 2.0 * 6.34205436706543
Epoch 580, val loss: 0.9425877928733826
Epoch 590, training loss: 13.130165100097656 = 0.45577654242515564 + 2.0 * 6.337194442749023
Epoch 590, val loss: 0.9362378120422363
Epoch 600, training loss: 13.109304428100586 = 0.4356739819049835 + 2.0 * 6.336815357208252
Epoch 600, val loss: 0.9308386445045471
Epoch 610, training loss: 13.102259635925293 = 0.41633284091949463 + 2.0 * 6.342963218688965
Epoch 610, val loss: 0.9264005422592163
Epoch 620, training loss: 13.064188003540039 = 0.3977801203727722 + 2.0 * 6.3332037925720215
Epoch 620, val loss: 0.9228843450546265
Epoch 630, training loss: 13.041753768920898 = 0.3800121247768402 + 2.0 * 6.330870628356934
Epoch 630, val loss: 0.9201544523239136
Epoch 640, training loss: 13.020475387573242 = 0.36300304532051086 + 2.0 * 6.328736305236816
Epoch 640, val loss: 0.9184423685073853
Epoch 650, training loss: 13.04286003112793 = 0.34673357009887695 + 2.0 * 6.348062992095947
Epoch 650, val loss: 0.9174158573150635
Epoch 660, training loss: 12.987492561340332 = 0.33118489384651184 + 2.0 * 6.328153610229492
Epoch 660, val loss: 0.9171538949012756
Epoch 670, training loss: 12.970134735107422 = 0.3164029121398926 + 2.0 * 6.3268656730651855
Epoch 670, val loss: 0.9176325798034668
Epoch 680, training loss: 12.94761848449707 = 0.3022397458553314 + 2.0 * 6.322689533233643
Epoch 680, val loss: 0.918764054775238
Epoch 690, training loss: 12.931924819946289 = 0.2886655330657959 + 2.0 * 6.321629524230957
Epoch 690, val loss: 0.9205356240272522
Epoch 700, training loss: 12.949390411376953 = 0.2756408751010895 + 2.0 * 6.336874961853027
Epoch 700, val loss: 0.9228786826133728
Epoch 710, training loss: 12.905376434326172 = 0.2631969749927521 + 2.0 * 6.321089744567871
Epoch 710, val loss: 0.9257251024246216
Epoch 720, training loss: 12.887394905090332 = 0.25130802392959595 + 2.0 * 6.318043231964111
Epoch 720, val loss: 0.9291468858718872
Epoch 730, training loss: 12.881796836853027 = 0.23988847434520721 + 2.0 * 6.320954322814941
Epoch 730, val loss: 0.9330620765686035
Epoch 740, training loss: 12.85965347290039 = 0.2289348989725113 + 2.0 * 6.315359115600586
Epoch 740, val loss: 0.9373466372489929
Epoch 750, training loss: 12.84628963470459 = 0.21844232082366943 + 2.0 * 6.3139238357543945
Epoch 750, val loss: 0.9420178532600403
Epoch 760, training loss: 12.837952613830566 = 0.2083672434091568 + 2.0 * 6.314792633056641
Epoch 760, val loss: 0.9470845460891724
Epoch 770, training loss: 12.831870079040527 = 0.19870850443840027 + 2.0 * 6.316580772399902
Epoch 770, val loss: 0.9524538516998291
Epoch 780, training loss: 12.81554889678955 = 0.18946775794029236 + 2.0 * 6.313040733337402
Epoch 780, val loss: 0.9580921530723572
Epoch 790, training loss: 12.809417724609375 = 0.1806323230266571 + 2.0 * 6.314392566680908
Epoch 790, val loss: 0.9639540910720825
Epoch 800, training loss: 12.794955253601074 = 0.1721741110086441 + 2.0 * 6.311390399932861
Epoch 800, val loss: 0.9699825644493103
Epoch 810, training loss: 12.779667854309082 = 0.1641256958246231 + 2.0 * 6.3077712059021
Epoch 810, val loss: 0.97629314661026
Epoch 820, training loss: 12.76949405670166 = 0.15643718838691711 + 2.0 * 6.306528568267822
Epoch 820, val loss: 0.9828579425811768
Epoch 830, training loss: 12.789155006408691 = 0.14909511804580688 + 2.0 * 6.3200297355651855
Epoch 830, val loss: 0.9896209836006165
Epoch 840, training loss: 12.757421493530273 = 0.14212672412395477 + 2.0 * 6.307647228240967
Epoch 840, val loss: 0.9963512420654297
Epoch 850, training loss: 12.741165161132812 = 0.13551083207130432 + 2.0 * 6.30282735824585
Epoch 850, val loss: 1.0034468173980713
Epoch 860, training loss: 12.734068870544434 = 0.12922340631484985 + 2.0 * 6.302422523498535
Epoch 860, val loss: 1.0107027292251587
Epoch 870, training loss: 12.746991157531738 = 0.12324925512075424 + 2.0 * 6.31187105178833
Epoch 870, val loss: 1.0181032419204712
Epoch 880, training loss: 12.722295761108398 = 0.11757772415876389 + 2.0 * 6.302359104156494
Epoch 880, val loss: 1.0255366563796997
Epoch 890, training loss: 12.714089393615723 = 0.11221612244844437 + 2.0 * 6.300936698913574
Epoch 890, val loss: 1.03304123878479
Epoch 900, training loss: 12.704310417175293 = 0.107136070728302 + 2.0 * 6.298587322235107
Epoch 900, val loss: 1.0407770872116089
Epoch 910, training loss: 12.705394744873047 = 0.10231953114271164 + 2.0 * 6.30153751373291
Epoch 910, val loss: 1.0485568046569824
Epoch 920, training loss: 12.700997352600098 = 0.09775316715240479 + 2.0 * 6.301621913909912
Epoch 920, val loss: 1.0563364028930664
Epoch 930, training loss: 12.685215950012207 = 0.09344520419836044 + 2.0 * 6.2958855628967285
Epoch 930, val loss: 1.064168930053711
Epoch 940, training loss: 12.67837142944336 = 0.08937909454107285 + 2.0 * 6.294496059417725
Epoch 940, val loss: 1.0720093250274658
Epoch 950, training loss: 12.674116134643555 = 0.08553287386894226 + 2.0 * 6.2942914962768555
Epoch 950, val loss: 1.0799651145935059
Epoch 960, training loss: 12.686383247375488 = 0.08189073950052261 + 2.0 * 6.30224609375
Epoch 960, val loss: 1.0879206657409668
Epoch 970, training loss: 12.66821002960205 = 0.0784384086728096 + 2.0 * 6.294885635375977
Epoch 970, val loss: 1.0958880186080933
Epoch 980, training loss: 12.661972045898438 = 0.07518799602985382 + 2.0 * 6.293392181396484
Epoch 980, val loss: 1.1037694215774536
Epoch 990, training loss: 12.654541969299316 = 0.07211329787969589 + 2.0 * 6.291214466094971
Epoch 990, val loss: 1.111666202545166
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7703703703703704
0.8075909330521878
=== training gcn model ===
Epoch 0, training loss: 19.135175704956055 = 1.9414732456207275 + 2.0 * 8.596851348876953
Epoch 0, val loss: 1.9309301376342773
Epoch 10, training loss: 19.12430191040039 = 1.931262731552124 + 2.0 * 8.596519470214844
Epoch 10, val loss: 1.9203943014144897
Epoch 20, training loss: 19.105804443359375 = 1.918319582939148 + 2.0 * 8.593742370605469
Epoch 20, val loss: 1.9067238569259644
Epoch 30, training loss: 19.04730796813965 = 1.900428056716919 + 2.0 * 8.573439598083496
Epoch 30, val loss: 1.8878148794174194
Epoch 40, training loss: 18.8082218170166 = 1.8789172172546387 + 2.0 * 8.464652061462402
Epoch 40, val loss: 1.8667380809783936
Epoch 50, training loss: 18.07427406311035 = 1.857570767402649 + 2.0 * 8.108351707458496
Epoch 50, val loss: 1.8471262454986572
Epoch 60, training loss: 17.19563102722168 = 1.8411157131195068 + 2.0 * 7.677257537841797
Epoch 60, val loss: 1.8330501317977905
Epoch 70, training loss: 16.365537643432617 = 1.8306540250778198 + 2.0 * 7.267442226409912
Epoch 70, val loss: 1.8241463899612427
Epoch 80, training loss: 15.936594009399414 = 1.8208683729171753 + 2.0 * 7.057862758636475
Epoch 80, val loss: 1.8154079914093018
Epoch 90, training loss: 15.647226333618164 = 1.808287501335144 + 2.0 * 6.919469356536865
Epoch 90, val loss: 1.8049217462539673
Epoch 100, training loss: 15.449851989746094 = 1.7956128120422363 + 2.0 * 6.82711935043335
Epoch 100, val loss: 1.7948518991470337
Epoch 110, training loss: 15.292526245117188 = 1.7835593223571777 + 2.0 * 6.754483222961426
Epoch 110, val loss: 1.7852628231048584
Epoch 120, training loss: 15.174361228942871 = 1.7716645002365112 + 2.0 * 6.701348304748535
Epoch 120, val loss: 1.7756506204605103
Epoch 130, training loss: 15.067100524902344 = 1.7591849565505981 + 2.0 * 6.653957843780518
Epoch 130, val loss: 1.7653716802597046
Epoch 140, training loss: 14.987316131591797 = 1.7456259727478027 + 2.0 * 6.620845317840576
Epoch 140, val loss: 1.7542028427124023
Epoch 150, training loss: 14.907201766967773 = 1.730781078338623 + 2.0 * 6.588210582733154
Epoch 150, val loss: 1.7420134544372559
Epoch 160, training loss: 14.840584754943848 = 1.7142730951309204 + 2.0 * 6.563155651092529
Epoch 160, val loss: 1.7285314798355103
Epoch 170, training loss: 14.78105354309082 = 1.6957095861434937 + 2.0 * 6.542672157287598
Epoch 170, val loss: 1.7134822607040405
Epoch 180, training loss: 14.73862075805664 = 1.6748957633972168 + 2.0 * 6.531862258911133
Epoch 180, val loss: 1.6967477798461914
Epoch 190, training loss: 14.676777839660645 = 1.6520195007324219 + 2.0 * 6.512379169464111
Epoch 190, val loss: 1.6783453226089478
Epoch 200, training loss: 14.622236251831055 = 1.626908302307129 + 2.0 * 6.497663974761963
Epoch 200, val loss: 1.658187747001648
Epoch 210, training loss: 14.570672988891602 = 1.5994367599487305 + 2.0 * 6.4856181144714355
Epoch 210, val loss: 1.6363601684570312
Epoch 220, training loss: 14.51955509185791 = 1.569595217704773 + 2.0 * 6.474979877471924
Epoch 220, val loss: 1.6128660440444946
Epoch 230, training loss: 14.480649948120117 = 1.5376133918762207 + 2.0 * 6.471518039703369
Epoch 230, val loss: 1.5879805088043213
Epoch 240, training loss: 14.419391632080078 = 1.5042228698730469 + 2.0 * 6.457584381103516
Epoch 240, val loss: 1.5622648000717163
Epoch 250, training loss: 14.369629859924316 = 1.4695594310760498 + 2.0 * 6.450035095214844
Epoch 250, val loss: 1.5358898639678955
Epoch 260, training loss: 14.32475471496582 = 1.4337315559387207 + 2.0 * 6.445511341094971
Epoch 260, val loss: 1.5091184377670288
Epoch 270, training loss: 14.268418312072754 = 1.397143006324768 + 2.0 * 6.435637474060059
Epoch 270, val loss: 1.4821326732635498
Epoch 280, training loss: 14.218381881713867 = 1.3600975275039673 + 2.0 * 6.429141998291016
Epoch 280, val loss: 1.4550992250442505
Epoch 290, training loss: 14.174339294433594 = 1.3226730823516846 + 2.0 * 6.425833225250244
Epoch 290, val loss: 1.4281649589538574
Epoch 300, training loss: 14.124492645263672 = 1.285123586654663 + 2.0 * 6.419684410095215
Epoch 300, val loss: 1.4016063213348389
Epoch 310, training loss: 14.07397174835205 = 1.2478336095809937 + 2.0 * 6.413069248199463
Epoch 310, val loss: 1.3754130601882935
Epoch 320, training loss: 14.026479721069336 = 1.2105956077575684 + 2.0 * 6.407941818237305
Epoch 320, val loss: 1.3495442867279053
Epoch 330, training loss: 13.99416732788086 = 1.1735115051269531 + 2.0 * 6.410327911376953
Epoch 330, val loss: 1.3238532543182373
Epoch 340, training loss: 13.93699836730957 = 1.1367523670196533 + 2.0 * 6.400123119354248
Epoch 340, val loss: 1.2984923124313354
Epoch 350, training loss: 13.892410278320312 = 1.1005982160568237 + 2.0 * 6.3959059715271
Epoch 350, val loss: 1.2736492156982422
Epoch 360, training loss: 13.847599029541016 = 1.0649974346160889 + 2.0 * 6.391300678253174
Epoch 360, val loss: 1.2492597103118896
Epoch 370, training loss: 13.819175720214844 = 1.0300238132476807 + 2.0 * 6.394576072692871
Epoch 370, val loss: 1.2254235744476318
Epoch 380, training loss: 13.76740550994873 = 0.9960504174232483 + 2.0 * 6.385677337646484
Epoch 380, val loss: 1.2021669149398804
Epoch 390, training loss: 13.726816177368164 = 0.963179349899292 + 2.0 * 6.3818182945251465
Epoch 390, val loss: 1.1799805164337158
Epoch 400, training loss: 13.687314987182617 = 0.9315023422241211 + 2.0 * 6.377906322479248
Epoch 400, val loss: 1.1586856842041016
Epoch 410, training loss: 13.652320861816406 = 0.9011656641960144 + 2.0 * 6.375577449798584
Epoch 410, val loss: 1.1384942531585693
Epoch 420, training loss: 13.62028980255127 = 0.8720429539680481 + 2.0 * 6.374123573303223
Epoch 420, val loss: 1.1193958520889282
Epoch 430, training loss: 13.592334747314453 = 0.8442367315292358 + 2.0 * 6.374049186706543
Epoch 430, val loss: 1.1014149188995361
Epoch 440, training loss: 13.551371574401855 = 0.8177188634872437 + 2.0 * 6.36682653427124
Epoch 440, val loss: 1.084619402885437
Epoch 450, training loss: 13.518856048583984 = 0.7922202944755554 + 2.0 * 6.363317966461182
Epoch 450, val loss: 1.0688636302947998
Epoch 460, training loss: 13.497615814208984 = 0.7676233649253845 + 2.0 * 6.364996433258057
Epoch 460, val loss: 1.0539718866348267
Epoch 470, training loss: 13.458584785461426 = 0.7437211871147156 + 2.0 * 6.357431888580322
Epoch 470, val loss: 1.0398937463760376
Epoch 480, training loss: 13.430978775024414 = 0.720486044883728 + 2.0 * 6.355246543884277
Epoch 480, val loss: 1.0265406370162964
Epoch 490, training loss: 13.405560493469238 = 0.697711706161499 + 2.0 * 6.35392427444458
Epoch 490, val loss: 1.0136544704437256
Epoch 500, training loss: 13.37941837310791 = 0.6752808690071106 + 2.0 * 6.352068901062012
Epoch 500, val loss: 1.0012239217758179
Epoch 510, training loss: 13.351387023925781 = 0.6533181071281433 + 2.0 * 6.349034309387207
Epoch 510, val loss: 0.9893580675125122
Epoch 520, training loss: 13.3250093460083 = 0.631782591342926 + 2.0 * 6.34661340713501
Epoch 520, val loss: 0.9779855012893677
Epoch 530, training loss: 13.298140525817871 = 0.6104820370674133 + 2.0 * 6.343829154968262
Epoch 530, val loss: 0.9672086238861084
Epoch 540, training loss: 13.287137031555176 = 0.589510977268219 + 2.0 * 6.348813056945801
Epoch 540, val loss: 0.9569116830825806
Epoch 550, training loss: 13.249799728393555 = 0.5688281655311584 + 2.0 * 6.340485572814941
Epoch 550, val loss: 0.9471768736839294
Epoch 560, training loss: 13.224164962768555 = 0.5485973358154297 + 2.0 * 6.3377838134765625
Epoch 560, val loss: 0.9381676912307739
Epoch 570, training loss: 13.213290214538574 = 0.5286439061164856 + 2.0 * 6.342323303222656
Epoch 570, val loss: 0.9296867251396179
Epoch 580, training loss: 13.17843246459961 = 0.509107232093811 + 2.0 * 6.334662437438965
Epoch 580, val loss: 0.9218413829803467
Epoch 590, training loss: 13.154841423034668 = 0.4898604452610016 + 2.0 * 6.33249044418335
Epoch 590, val loss: 0.9147527813911438
Epoch 600, training loss: 13.13113784790039 = 0.47085443139076233 + 2.0 * 6.330141544342041
Epoch 600, val loss: 0.908161997795105
Epoch 610, training loss: 13.127827644348145 = 0.4520381689071655 + 2.0 * 6.337894916534424
Epoch 610, val loss: 0.9021278023719788
Epoch 620, training loss: 13.093622207641602 = 0.43368154764175415 + 2.0 * 6.329970359802246
Epoch 620, val loss: 0.8964821696281433
Epoch 630, training loss: 13.065680503845215 = 0.4155372381210327 + 2.0 * 6.325071811676025
Epoch 630, val loss: 0.8915296196937561
Epoch 640, training loss: 13.050333023071289 = 0.3976370096206665 + 2.0 * 6.326347827911377
Epoch 640, val loss: 0.8869321346282959
Epoch 650, training loss: 13.026209831237793 = 0.37997621297836304 + 2.0 * 6.323116779327393
Epoch 650, val loss: 0.8827366828918457
Epoch 660, training loss: 13.0062894821167 = 0.3625375032424927 + 2.0 * 6.321876049041748
Epoch 660, val loss: 0.8789949417114258
Epoch 670, training loss: 12.992213249206543 = 0.345376580953598 + 2.0 * 6.323418140411377
Epoch 670, val loss: 0.8757221698760986
Epoch 680, training loss: 12.969094276428223 = 0.32850781083106995 + 2.0 * 6.320293426513672
Epoch 680, val loss: 0.873116135597229
Epoch 690, training loss: 12.947754859924316 = 0.3120346665382385 + 2.0 * 6.317860126495361
Epoch 690, val loss: 0.8708745837211609
Epoch 700, training loss: 12.927492141723633 = 0.29594019055366516 + 2.0 * 6.3157758712768555
Epoch 700, val loss: 0.8693379759788513
Epoch 710, training loss: 12.920429229736328 = 0.28027141094207764 + 2.0 * 6.3200788497924805
Epoch 710, val loss: 0.8683862686157227
Epoch 720, training loss: 12.896622657775879 = 0.2651461064815521 + 2.0 * 6.315738201141357
Epoch 720, val loss: 0.8678758144378662
Epoch 730, training loss: 12.87694263458252 = 0.25057676434516907 + 2.0 * 6.313182830810547
Epoch 730, val loss: 0.8681343197822571
Epoch 740, training loss: 12.860270500183105 = 0.23661784827709198 + 2.0 * 6.311826229095459
Epoch 740, val loss: 0.8689333200454712
Epoch 750, training loss: 12.844219207763672 = 0.22324612736701965 + 2.0 * 6.310486316680908
Epoch 750, val loss: 0.8702720403671265
Epoch 760, training loss: 12.835258483886719 = 0.21052446961402893 + 2.0 * 6.312366962432861
Epoch 760, val loss: 0.8720720410346985
Epoch 770, training loss: 12.814925193786621 = 0.19844664633274078 + 2.0 * 6.308239459991455
Epoch 770, val loss: 0.8745822310447693
Epoch 780, training loss: 12.800408363342285 = 0.1871386021375656 + 2.0 * 6.306634902954102
Epoch 780, val loss: 0.8775144815444946
Epoch 790, training loss: 12.798068046569824 = 0.176472470164299 + 2.0 * 6.310797691345215
Epoch 790, val loss: 0.8809886574745178
Epoch 800, training loss: 12.780163764953613 = 0.16645139455795288 + 2.0 * 6.306856155395508
Epoch 800, val loss: 0.8848918676376343
Epoch 810, training loss: 12.763253211975098 = 0.15712697803974152 + 2.0 * 6.303062915802002
Epoch 810, val loss: 0.8891972303390503
Epoch 820, training loss: 12.749826431274414 = 0.14837592840194702 + 2.0 * 6.30072546005249
Epoch 820, val loss: 0.8939439058303833
Epoch 830, training loss: 12.748682022094727 = 0.14017395675182343 + 2.0 * 6.304254055023193
Epoch 830, val loss: 0.8991178274154663
Epoch 840, training loss: 12.735138893127441 = 0.13253174722194672 + 2.0 * 6.301303386688232
Epoch 840, val loss: 0.9045414328575134
Epoch 850, training loss: 12.72383975982666 = 0.12540094554424286 + 2.0 * 6.299219608306885
Epoch 850, val loss: 0.9102097153663635
Epoch 860, training loss: 12.711748123168945 = 0.11874266713857651 + 2.0 * 6.296502590179443
Epoch 860, val loss: 0.9162327647209167
Epoch 870, training loss: 12.703941345214844 = 0.11251351237297058 + 2.0 * 6.295713901519775
Epoch 870, val loss: 0.9225027561187744
Epoch 880, training loss: 12.719682693481445 = 0.10670314729213715 + 2.0 * 6.306489944458008
Epoch 880, val loss: 0.9289609789848328
Epoch 890, training loss: 12.690604209899902 = 0.10127468407154083 + 2.0 * 6.2946648597717285
Epoch 890, val loss: 0.9355602860450745
Epoch 900, training loss: 12.684273719787598 = 0.09623320400714874 + 2.0 * 6.294020175933838
Epoch 900, val loss: 0.9423196315765381
Epoch 910, training loss: 12.675909042358398 = 0.09153558313846588 + 2.0 * 6.292186737060547
Epoch 910, val loss: 0.949193000793457
Epoch 920, training loss: 12.669411659240723 = 0.08712603896856308 + 2.0 * 6.29114294052124
Epoch 920, val loss: 0.9561859369277954
Epoch 930, training loss: 12.680815696716309 = 0.08299616724252701 + 2.0 * 6.298909664154053
Epoch 930, val loss: 0.9632530808448792
Epoch 940, training loss: 12.659073829650879 = 0.07915190607309341 + 2.0 * 6.289960861206055
Epoch 940, val loss: 0.9702271819114685
Epoch 950, training loss: 12.65330696105957 = 0.07555003464221954 + 2.0 * 6.288878440856934
Epoch 950, val loss: 0.9772656559944153
Epoch 960, training loss: 12.647883415222168 = 0.07216700166463852 + 2.0 * 6.287858009338379
Epoch 960, val loss: 0.9844119548797607
Epoch 970, training loss: 12.659643173217773 = 0.06898216903209686 + 2.0 * 6.29533052444458
Epoch 970, val loss: 0.991510808467865
Epoch 980, training loss: 12.649410247802734 = 0.066016785800457 + 2.0 * 6.291696548461914
Epoch 980, val loss: 0.9985563158988953
Epoch 990, training loss: 12.63383674621582 = 0.06322857737541199 + 2.0 * 6.285304069519043
Epoch 990, val loss: 1.0055866241455078
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7407407407407408
0.8070637849235636
=== training gcn model ===
Epoch 0, training loss: 19.135343551635742 = 1.9416329860687256 + 2.0 * 8.596855163574219
Epoch 0, val loss: 1.9415227174758911
Epoch 10, training loss: 19.123994827270508 = 1.9309478998184204 + 2.0 * 8.59652328491211
Epoch 10, val loss: 1.9307407140731812
Epoch 20, training loss: 19.105581283569336 = 1.9173814058303833 + 2.0 * 8.594099998474121
Epoch 20, val loss: 1.917054295539856
Epoch 30, training loss: 19.04986572265625 = 1.8989012241363525 + 2.0 * 8.575482368469238
Epoch 30, val loss: 1.898594617843628
Epoch 40, training loss: 18.768333435058594 = 1.8764134645462036 + 2.0 * 8.44596004486084
Epoch 40, val loss: 1.8767735958099365
Epoch 50, training loss: 17.546178817749023 = 1.8532750606536865 + 2.0 * 7.846451759338379
Epoch 50, val loss: 1.8546364307403564
Epoch 60, training loss: 16.952747344970703 = 1.8360297679901123 + 2.0 * 7.558358669281006
Epoch 60, val loss: 1.838787317276001
Epoch 70, training loss: 16.476755142211914 = 1.823538064956665 + 2.0 * 7.326608180999756
Epoch 70, val loss: 1.8268316984176636
Epoch 80, training loss: 16.111736297607422 = 1.8096545934677124 + 2.0 * 7.151041030883789
Epoch 80, val loss: 1.8147112131118774
Epoch 90, training loss: 15.81568717956543 = 1.7984424829483032 + 2.0 * 7.008622169494629
Epoch 90, val loss: 1.8051402568817139
Epoch 100, training loss: 15.614255905151367 = 1.787139892578125 + 2.0 * 6.913558006286621
Epoch 100, val loss: 1.7948120832443237
Epoch 110, training loss: 15.456568717956543 = 1.7745503187179565 + 2.0 * 6.841009140014648
Epoch 110, val loss: 1.7838693857192993
Epoch 120, training loss: 15.325394630432129 = 1.7610361576080322 + 2.0 * 6.782179355621338
Epoch 120, val loss: 1.7726891040802002
Epoch 130, training loss: 15.212777137756348 = 1.7463791370391846 + 2.0 * 6.733199119567871
Epoch 130, val loss: 1.760461449623108
Epoch 140, training loss: 15.12834644317627 = 1.7299063205718994 + 2.0 * 6.699220180511475
Epoch 140, val loss: 1.7466099262237549
Epoch 150, training loss: 15.055770874023438 = 1.711326003074646 + 2.0 * 6.67222261428833
Epoch 150, val loss: 1.7310298681259155
Epoch 160, training loss: 14.991948127746582 = 1.6903642416000366 + 2.0 * 6.650792121887207
Epoch 160, val loss: 1.7137006521224976
Epoch 170, training loss: 14.916109085083008 = 1.6671191453933716 + 2.0 * 6.624495029449463
Epoch 170, val loss: 1.6946879625320435
Epoch 180, training loss: 14.846551895141602 = 1.6411068439483643 + 2.0 * 6.602722644805908
Epoch 180, val loss: 1.6736233234405518
Epoch 190, training loss: 14.779645919799805 = 1.6121909618377686 + 2.0 * 6.5837273597717285
Epoch 190, val loss: 1.6506931781768799
Epoch 200, training loss: 14.708020210266113 = 1.5808045864105225 + 2.0 * 6.563607692718506
Epoch 200, val loss: 1.6259676218032837
Epoch 210, training loss: 14.642257690429688 = 1.5470309257507324 + 2.0 * 6.547613620758057
Epoch 210, val loss: 1.599708080291748
Epoch 220, training loss: 14.586002349853516 = 1.5110602378845215 + 2.0 * 6.537471294403076
Epoch 220, val loss: 1.5723789930343628
Epoch 230, training loss: 14.515012741088867 = 1.473724365234375 + 2.0 * 6.520644187927246
Epoch 230, val loss: 1.5440839529037476
Epoch 240, training loss: 14.452486991882324 = 1.4352290630340576 + 2.0 * 6.508628845214844
Epoch 240, val loss: 1.5158624649047852
Epoch 250, training loss: 14.397884368896484 = 1.3962290287017822 + 2.0 * 6.500827789306641
Epoch 250, val loss: 1.4877886772155762
Epoch 260, training loss: 14.338821411132812 = 1.3576127290725708 + 2.0 * 6.490604400634766
Epoch 260, val loss: 1.460811734199524
Epoch 270, training loss: 14.279553413391113 = 1.319405436515808 + 2.0 * 6.480073928833008
Epoch 270, val loss: 1.4347795248031616
Epoch 280, training loss: 14.224406242370605 = 1.2814711332321167 + 2.0 * 6.4714674949646
Epoch 280, val loss: 1.4095726013183594
Epoch 290, training loss: 14.17702579498291 = 1.244118332862854 + 2.0 * 6.466453552246094
Epoch 290, val loss: 1.3852437734603882
Epoch 300, training loss: 14.127714157104492 = 1.2080702781677246 + 2.0 * 6.459821701049805
Epoch 300, val loss: 1.362208604812622
Epoch 310, training loss: 14.070956230163574 = 1.1728671789169312 + 2.0 * 6.449044704437256
Epoch 310, val loss: 1.340256690979004
Epoch 320, training loss: 14.021196365356445 = 1.1382454633712769 + 2.0 * 6.4414753913879395
Epoch 320, val loss: 1.318795084953308
Epoch 330, training loss: 13.975330352783203 = 1.104059100151062 + 2.0 * 6.435635566711426
Epoch 330, val loss: 1.2977153062820435
Epoch 340, training loss: 13.930767059326172 = 1.0702946186065674 + 2.0 * 6.430236339569092
Epoch 340, val loss: 1.276818037033081
Epoch 350, training loss: 13.892457962036133 = 1.0370842218399048 + 2.0 * 6.42768669128418
Epoch 350, val loss: 1.2565165758132935
Epoch 360, training loss: 13.839313507080078 = 1.0043976306915283 + 2.0 * 6.4174580574035645
Epoch 360, val loss: 1.2363109588623047
Epoch 370, training loss: 13.795330047607422 = 0.9720116853713989 + 2.0 * 6.411659240722656
Epoch 370, val loss: 1.2162318229675293
Epoch 380, training loss: 13.762734413146973 = 0.9399867653846741 + 2.0 * 6.411373615264893
Epoch 380, val loss: 1.1962734460830688
Epoch 390, training loss: 13.722524642944336 = 0.9084469079971313 + 2.0 * 6.407038688659668
Epoch 390, val loss: 1.1765763759613037
Epoch 400, training loss: 13.677103996276855 = 0.8773155808448792 + 2.0 * 6.3998942375183105
Epoch 400, val loss: 1.157180905342102
Epoch 410, training loss: 13.635408401489258 = 0.8466686010360718 + 2.0 * 6.394370079040527
Epoch 410, val loss: 1.1378647089004517
Epoch 420, training loss: 13.606380462646484 = 0.8164955377578735 + 2.0 * 6.394942283630371
Epoch 420, val loss: 1.118640661239624
Epoch 430, training loss: 13.562520027160645 = 0.7869294285774231 + 2.0 * 6.387795448303223
Epoch 430, val loss: 1.099929690361023
Epoch 440, training loss: 13.528813362121582 = 0.7581020593643188 + 2.0 * 6.385355472564697
Epoch 440, val loss: 1.0818027257919312
Epoch 450, training loss: 13.48997688293457 = 0.7301104664802551 + 2.0 * 6.3799333572387695
Epoch 450, val loss: 1.0644346475601196
Epoch 460, training loss: 13.460978507995605 = 0.7029378414154053 + 2.0 * 6.3790202140808105
Epoch 460, val loss: 1.047807216644287
Epoch 470, training loss: 13.426700592041016 = 0.6766772866249084 + 2.0 * 6.375011444091797
Epoch 470, val loss: 1.0317766666412354
Epoch 480, training loss: 13.40572452545166 = 0.6514186263084412 + 2.0 * 6.377152919769287
Epoch 480, val loss: 1.016891360282898
Epoch 490, training loss: 13.36781120300293 = 0.6272280216217041 + 2.0 * 6.370291709899902
Epoch 490, val loss: 1.00319242477417
Epoch 500, training loss: 13.337813377380371 = 0.6039982438087463 + 2.0 * 6.366907596588135
Epoch 500, val loss: 0.9903920292854309
Epoch 510, training loss: 13.320555686950684 = 0.5817397236824036 + 2.0 * 6.369408130645752
Epoch 510, val loss: 0.9788760542869568
Epoch 520, training loss: 13.291634559631348 = 0.5605086088180542 + 2.0 * 6.365562915802002
Epoch 520, val loss: 0.9685136675834656
Epoch 530, training loss: 13.259259223937988 = 0.5401418209075928 + 2.0 * 6.359558582305908
Epoch 530, val loss: 0.9592465758323669
Epoch 540, training loss: 13.236530303955078 = 0.5205965042114258 + 2.0 * 6.357966899871826
Epoch 540, val loss: 0.9510181546211243
Epoch 550, training loss: 13.222609519958496 = 0.5018022656440735 + 2.0 * 6.360403537750244
Epoch 550, val loss: 0.9436923861503601
Epoch 560, training loss: 13.19325065612793 = 0.483772873878479 + 2.0 * 6.354738712310791
Epoch 560, val loss: 0.9373299479484558
Epoch 570, training loss: 13.172106742858887 = 0.4664008617401123 + 2.0 * 6.352852821350098
Epoch 570, val loss: 0.931752622127533
Epoch 580, training loss: 13.151679039001465 = 0.4496196508407593 + 2.0 * 6.351029872894287
Epoch 580, val loss: 0.9269882440567017
Epoch 590, training loss: 13.14229679107666 = 0.43341371417045593 + 2.0 * 6.3544416427612305
Epoch 590, val loss: 0.9228306412696838
Epoch 600, training loss: 13.113371849060059 = 0.4177131652832031 + 2.0 * 6.347829341888428
Epoch 600, val loss: 0.9194139838218689
Epoch 610, training loss: 13.092988967895508 = 0.4025171995162964 + 2.0 * 6.345235824584961
Epoch 610, val loss: 0.9164487719535828
Epoch 620, training loss: 13.079839706420898 = 0.38774579763412476 + 2.0 * 6.3460469245910645
Epoch 620, val loss: 0.9139558672904968
Epoch 630, training loss: 13.064384460449219 = 0.373419851064682 + 2.0 * 6.345482349395752
Epoch 630, val loss: 0.9120437502861023
Epoch 640, training loss: 13.04277515411377 = 0.35945892333984375 + 2.0 * 6.341658115386963
Epoch 640, val loss: 0.9105491042137146
Epoch 650, training loss: 13.025832176208496 = 0.3458940386772156 + 2.0 * 6.339969158172607
Epoch 650, val loss: 0.9094700217247009
Epoch 660, training loss: 13.010980606079102 = 0.3326721489429474 + 2.0 * 6.339154243469238
Epoch 660, val loss: 0.9087099432945251
Epoch 670, training loss: 12.994211196899414 = 0.31979456543922424 + 2.0 * 6.337208271026611
Epoch 670, val loss: 0.9083805084228516
Epoch 680, training loss: 12.981398582458496 = 0.30724185705184937 + 2.0 * 6.33707857131958
Epoch 680, val loss: 0.9085710644721985
Epoch 690, training loss: 12.977005958557129 = 0.2951112389564514 + 2.0 * 6.340947151184082
Epoch 690, val loss: 0.9089345932006836
Epoch 700, training loss: 12.94892406463623 = 0.28330957889556885 + 2.0 * 6.3328070640563965
Epoch 700, val loss: 0.9096755385398865
Epoch 710, training loss: 12.931482315063477 = 0.2718629240989685 + 2.0 * 6.329809665679932
Epoch 710, val loss: 0.9107878804206848
Epoch 720, training loss: 12.917243957519531 = 0.2607426643371582 + 2.0 * 6.328250408172607
Epoch 720, val loss: 0.9123060703277588
Epoch 730, training loss: 12.907326698303223 = 0.2499273717403412 + 2.0 * 6.328699588775635
Epoch 730, val loss: 0.9141251444816589
Epoch 740, training loss: 12.89488410949707 = 0.2394610494375229 + 2.0 * 6.327711582183838
Epoch 740, val loss: 0.9161518812179565
Epoch 750, training loss: 12.885177612304688 = 0.2293204814195633 + 2.0 * 6.32792854309082
Epoch 750, val loss: 0.9183579087257385
Epoch 760, training loss: 12.871322631835938 = 0.2195727527141571 + 2.0 * 6.3258748054504395
Epoch 760, val loss: 0.9208383560180664
Epoch 770, training loss: 12.85790729522705 = 0.21018411219120026 + 2.0 * 6.323861598968506
Epoch 770, val loss: 0.9238283634185791
Epoch 780, training loss: 12.8425874710083 = 0.2011009156703949 + 2.0 * 6.320743083953857
Epoch 780, val loss: 0.9269083738327026
Epoch 790, training loss: 12.847578048706055 = 0.19235505163669586 + 2.0 * 6.327611446380615
Epoch 790, val loss: 0.9301643371582031
Epoch 800, training loss: 12.825882911682129 = 0.18395380675792694 + 2.0 * 6.320964336395264
Epoch 800, val loss: 0.9335172772407532
Epoch 810, training loss: 12.812830924987793 = 0.17589277029037476 + 2.0 * 6.318469047546387
Epoch 810, val loss: 0.9370729923248291
Epoch 820, training loss: 12.812231063842773 = 0.16816645860671997 + 2.0 * 6.322032451629639
Epoch 820, val loss: 0.9408481121063232
Epoch 830, training loss: 12.797733306884766 = 0.16075806319713593 + 2.0 * 6.318487644195557
Epoch 830, val loss: 0.9451399445533752
Epoch 840, training loss: 12.795339584350586 = 0.15367020666599274 + 2.0 * 6.320834636688232
Epoch 840, val loss: 0.9491666555404663
Epoch 850, training loss: 12.776689529418945 = 0.14689531922340393 + 2.0 * 6.314897060394287
Epoch 850, val loss: 0.9535372257232666
Epoch 860, training loss: 12.764951705932617 = 0.140412837266922 + 2.0 * 6.31226921081543
Epoch 860, val loss: 0.9580495357513428
Epoch 870, training loss: 12.763018608093262 = 0.13421499729156494 + 2.0 * 6.314401626586914
Epoch 870, val loss: 0.9627256989479065
Epoch 880, training loss: 12.753512382507324 = 0.12828753888607025 + 2.0 * 6.312612533569336
Epoch 880, val loss: 0.9672378897666931
Epoch 890, training loss: 12.745465278625488 = 0.12263784557580948 + 2.0 * 6.311413764953613
Epoch 890, val loss: 0.9721799492835999
Epoch 900, training loss: 12.737982749938965 = 0.11726279556751251 + 2.0 * 6.310359954833984
Epoch 900, val loss: 0.9771674275398254
Epoch 910, training loss: 12.729304313659668 = 0.11215285211801529 + 2.0 * 6.308575630187988
Epoch 910, val loss: 0.9824625849723816
Epoch 920, training loss: 12.72055721282959 = 0.1072814017534256 + 2.0 * 6.306637763977051
Epoch 920, val loss: 0.9876455664634705
Epoch 930, training loss: 12.728403091430664 = 0.10263655334711075 + 2.0 * 6.312883377075195
Epoch 930, val loss: 0.9928741455078125
Epoch 940, training loss: 12.72603988647461 = 0.0982305258512497 + 2.0 * 6.313904762268066
Epoch 940, val loss: 0.998336136341095
Epoch 950, training loss: 12.701674461364746 = 0.09404747188091278 + 2.0 * 6.303813457489014
Epoch 950, val loss: 1.0036685466766357
Epoch 960, training loss: 12.696718215942383 = 0.09009017795324326 + 2.0 * 6.303314208984375
Epoch 960, val loss: 1.0092744827270508
Epoch 970, training loss: 12.690613746643066 = 0.08633750677108765 + 2.0 * 6.302138328552246
Epoch 970, val loss: 1.0149383544921875
Epoch 980, training loss: 12.704073905944824 = 0.08277534693479538 + 2.0 * 6.310649394989014
Epoch 980, val loss: 1.0206235647201538
Epoch 990, training loss: 12.699991226196289 = 0.07938887178897858 + 2.0 * 6.310301303863525
Epoch 990, val loss: 1.0266329050064087
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7666666666666667
0.8107538218239325
The final CL Acc:0.75926, 0.01318, The final GNN Acc:0.80847, 0.00163
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13158])
remove edge: torch.Size([2, 7920])
updated graph: torch.Size([2, 10522])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 19.12657928466797 = 1.9328269958496094 + 2.0 * 8.59687614440918
Epoch 0, val loss: 1.9356709718704224
Epoch 10, training loss: 19.11606788635254 = 1.9227967262268066 + 2.0 * 8.596635818481445
Epoch 10, val loss: 1.9253621101379395
Epoch 20, training loss: 19.09906768798828 = 1.910119652748108 + 2.0 * 8.594473838806152
Epoch 20, val loss: 1.912185549736023
Epoch 30, training loss: 19.04604148864746 = 1.8925719261169434 + 2.0 * 8.57673454284668
Epoch 30, val loss: 1.8940669298171997
Epoch 40, training loss: 18.804973602294922 = 1.8700404167175293 + 2.0 * 8.467466354370117
Epoch 40, val loss: 1.8718465566635132
Epoch 50, training loss: 18.12786102294922 = 1.8456389904022217 + 2.0 * 8.141111373901367
Epoch 50, val loss: 1.8491181135177612
Epoch 60, training loss: 17.45941162109375 = 1.8254002332687378 + 2.0 * 7.8170061111450195
Epoch 60, val loss: 1.8308322429656982
Epoch 70, training loss: 16.48708724975586 = 1.8094544410705566 + 2.0 * 7.3388166427612305
Epoch 70, val loss: 1.815809726715088
Epoch 80, training loss: 15.92725658416748 = 1.794930100440979 + 2.0 * 7.066163063049316
Epoch 80, val loss: 1.802565574645996
Epoch 90, training loss: 15.663016319274902 = 1.777319312095642 + 2.0 * 6.9428486824035645
Epoch 90, val loss: 1.7870814800262451
Epoch 100, training loss: 15.470287322998047 = 1.7569584846496582 + 2.0 * 6.856664657592773
Epoch 100, val loss: 1.7689677476882935
Epoch 110, training loss: 15.330989837646484 = 1.7360904216766357 + 2.0 * 6.797449588775635
Epoch 110, val loss: 1.7498722076416016
Epoch 120, training loss: 15.223250389099121 = 1.7147080898284912 + 2.0 * 6.754271030426025
Epoch 120, val loss: 1.730163335800171
Epoch 130, training loss: 15.122751235961914 = 1.6917312145233154 + 2.0 * 6.71550989151001
Epoch 130, val loss: 1.7092121839523315
Epoch 140, training loss: 15.028285026550293 = 1.666190505027771 + 2.0 * 6.681047439575195
Epoch 140, val loss: 1.6861850023269653
Epoch 150, training loss: 14.941381454467773 = 1.637984275817871 + 2.0 * 6.651698589324951
Epoch 150, val loss: 1.6608232259750366
Epoch 160, training loss: 14.852413177490234 = 1.6073745489120483 + 2.0 * 6.622519493103027
Epoch 160, val loss: 1.6333266496658325
Epoch 170, training loss: 14.770484924316406 = 1.5741724967956543 + 2.0 * 6.598155975341797
Epoch 170, val loss: 1.603643536567688
Epoch 180, training loss: 14.692337989807129 = 1.5387686491012573 + 2.0 * 6.576784610748291
Epoch 180, val loss: 1.5719269514083862
Epoch 190, training loss: 14.623756408691406 = 1.5017610788345337 + 2.0 * 6.560997486114502
Epoch 190, val loss: 1.5390040874481201
Epoch 200, training loss: 14.549386978149414 = 1.4639604091644287 + 2.0 * 6.542713165283203
Epoch 200, val loss: 1.5053322315216064
Epoch 210, training loss: 14.482812881469727 = 1.425642490386963 + 2.0 * 6.528585433959961
Epoch 210, val loss: 1.4713058471679688
Epoch 220, training loss: 14.42690658569336 = 1.3873496055603027 + 2.0 * 6.519778728485107
Epoch 220, val loss: 1.4376411437988281
Epoch 230, training loss: 14.358976364135742 = 1.349793553352356 + 2.0 * 6.504591464996338
Epoch 230, val loss: 1.4047372341156006
Epoch 240, training loss: 14.303059577941895 = 1.3129066228866577 + 2.0 * 6.495076656341553
Epoch 240, val loss: 1.372739315032959
Epoch 250, training loss: 14.244731903076172 = 1.2768481969833374 + 2.0 * 6.483942031860352
Epoch 250, val loss: 1.341778039932251
Epoch 260, training loss: 14.190651893615723 = 1.241497278213501 + 2.0 * 6.4745774269104
Epoch 260, val loss: 1.3117352724075317
Epoch 270, training loss: 14.14334487915039 = 1.2066466808319092 + 2.0 * 6.468348979949951
Epoch 270, val loss: 1.2824324369430542
Epoch 280, training loss: 14.098445892333984 = 1.1723904609680176 + 2.0 * 6.463027477264404
Epoch 280, val loss: 1.2538729906082153
Epoch 290, training loss: 14.042083740234375 = 1.1385473012924194 + 2.0 * 6.451768398284912
Epoch 290, val loss: 1.226017713546753
Epoch 300, training loss: 13.993698120117188 = 1.1047186851501465 + 2.0 * 6.444489479064941
Epoch 300, val loss: 1.19829523563385
Epoch 310, training loss: 13.947190284729004 = 1.0706555843353271 + 2.0 * 6.438267230987549
Epoch 310, val loss: 1.1705297231674194
Epoch 320, training loss: 13.91368293762207 = 1.0364429950714111 + 2.0 * 6.438620090484619
Epoch 320, val loss: 1.1428349018096924
Epoch 330, training loss: 13.858378410339355 = 1.0021051168441772 + 2.0 * 6.428136825561523
Epoch 330, val loss: 1.1150758266448975
Epoch 340, training loss: 13.809971809387207 = 0.9676177501678467 + 2.0 * 6.421176910400391
Epoch 340, val loss: 1.0873208045959473
Epoch 350, training loss: 13.76471996307373 = 0.9329607486724854 + 2.0 * 6.415879726409912
Epoch 350, val loss: 1.0595228672027588
Epoch 360, training loss: 13.75300407409668 = 0.8982271552085876 + 2.0 * 6.427388668060303
Epoch 360, val loss: 1.0317014455795288
Epoch 370, training loss: 13.688993453979492 = 0.8639050722122192 + 2.0 * 6.412544250488281
Epoch 370, val loss: 1.004480242729187
Epoch 380, training loss: 13.636197090148926 = 0.8302718997001648 + 2.0 * 6.402962684631348
Epoch 380, val loss: 0.9780031442642212
Epoch 390, training loss: 13.595234870910645 = 0.7973405718803406 + 2.0 * 6.398947238922119
Epoch 390, val loss: 0.9522796869277954
Epoch 400, training loss: 13.555156707763672 = 0.765198290348053 + 2.0 * 6.394979000091553
Epoch 400, val loss: 0.9275334477424622
Epoch 410, training loss: 13.552310943603516 = 0.7340207099914551 + 2.0 * 6.409144878387451
Epoch 410, val loss: 0.9040396809577942
Epoch 420, training loss: 13.488368034362793 = 0.7042379975318909 + 2.0 * 6.392065048217773
Epoch 420, val loss: 0.8821006417274475
Epoch 430, training loss: 13.446718215942383 = 0.6759118437767029 + 2.0 * 6.385403156280518
Epoch 430, val loss: 0.8617836236953735
Epoch 440, training loss: 13.4141845703125 = 0.6489521861076355 + 2.0 * 6.38261604309082
Epoch 440, val loss: 0.8429983258247375
Epoch 450, training loss: 13.397415161132812 = 0.6233500838279724 + 2.0 * 6.387032508850098
Epoch 450, val loss: 0.8257780075073242
Epoch 460, training loss: 13.35103702545166 = 0.5991144776344299 + 2.0 * 6.3759613037109375
Epoch 460, val loss: 0.8102943301200867
Epoch 470, training loss: 13.324384689331055 = 0.576163649559021 + 2.0 * 6.374110698699951
Epoch 470, val loss: 0.7963464856147766
Epoch 480, training loss: 13.312020301818848 = 0.5543766617774963 + 2.0 * 6.378821849822998
Epoch 480, val loss: 0.7837490439414978
Epoch 490, training loss: 13.2720947265625 = 0.5337393879890442 + 2.0 * 6.36917781829834
Epoch 490, val loss: 0.7725210785865784
Epoch 500, training loss: 13.246980667114258 = 0.5140830278396606 + 2.0 * 6.366448879241943
Epoch 500, val loss: 0.7625582218170166
Epoch 510, training loss: 13.221090316772461 = 0.49527525901794434 + 2.0 * 6.362907409667969
Epoch 510, val loss: 0.7536118030548096
Epoch 520, training loss: 13.200892448425293 = 0.47714683413505554 + 2.0 * 6.361872673034668
Epoch 520, val loss: 0.7456166744232178
Epoch 530, training loss: 13.181976318359375 = 0.4596882462501526 + 2.0 * 6.361144065856934
Epoch 530, val loss: 0.7383671998977661
Epoch 540, training loss: 13.156034469604492 = 0.4428388178348541 + 2.0 * 6.356597900390625
Epoch 540, val loss: 0.7318984270095825
Epoch 550, training loss: 13.134764671325684 = 0.42652469873428345 + 2.0 * 6.354119777679443
Epoch 550, val loss: 0.7260207533836365
Epoch 560, training loss: 13.117646217346191 = 0.4106283485889435 + 2.0 * 6.353508949279785
Epoch 560, val loss: 0.7206317186355591
Epoch 570, training loss: 13.095480918884277 = 0.39506861567497253 + 2.0 * 6.35020637512207
Epoch 570, val loss: 0.7157765626907349
Epoch 580, training loss: 13.079203605651855 = 0.37987810373306274 + 2.0 * 6.349662780761719
Epoch 580, val loss: 0.7112299203872681
Epoch 590, training loss: 13.057319641113281 = 0.3650071918964386 + 2.0 * 6.346156120300293
Epoch 590, val loss: 0.7071840167045593
Epoch 600, training loss: 13.049077987670898 = 0.35046666860580444 + 2.0 * 6.349305629730225
Epoch 600, val loss: 0.7034893035888672
Epoch 610, training loss: 13.02208137512207 = 0.3362587094306946 + 2.0 * 6.342911243438721
Epoch 610, val loss: 0.7001137733459473
Epoch 620, training loss: 13.004413604736328 = 0.3223957121372223 + 2.0 * 6.341009140014648
Epoch 620, val loss: 0.6970955729484558
Epoch 630, training loss: 12.988814353942871 = 0.3088304400444031 + 2.0 * 6.339992046356201
Epoch 630, val loss: 0.69440096616745
Epoch 640, training loss: 12.9698486328125 = 0.29557567834854126 + 2.0 * 6.337136268615723
Epoch 640, val loss: 0.6920404434204102
Epoch 650, training loss: 12.954023361206055 = 0.2826993465423584 + 2.0 * 6.335661888122559
Epoch 650, val loss: 0.6900231242179871
Epoch 660, training loss: 12.94532299041748 = 0.27019116282463074 + 2.0 * 6.337565898895264
Epoch 660, val loss: 0.6884151697158813
Epoch 670, training loss: 12.927854537963867 = 0.2580975890159607 + 2.0 * 6.334878444671631
Epoch 670, val loss: 0.687160074710846
Epoch 680, training loss: 12.909841537475586 = 0.24642670154571533 + 2.0 * 6.33170747756958
Epoch 680, val loss: 0.6863111257553101
Epoch 690, training loss: 12.893858909606934 = 0.23519335687160492 + 2.0 * 6.3293328285217285
Epoch 690, val loss: 0.685836672782898
Epoch 700, training loss: 12.8837308883667 = 0.22440205514431 + 2.0 * 6.32966423034668
Epoch 700, val loss: 0.6857620477676392
Epoch 710, training loss: 12.873747825622559 = 0.21406173706054688 + 2.0 * 6.329843044281006
Epoch 710, val loss: 0.6861199140548706
Epoch 720, training loss: 12.85619068145752 = 0.2041778862476349 + 2.0 * 6.3260064125061035
Epoch 720, val loss: 0.6868994832038879
Epoch 730, training loss: 12.855518341064453 = 0.19480472803115845 + 2.0 * 6.330356597900391
Epoch 730, val loss: 0.6880540251731873
Epoch 740, training loss: 12.832895278930664 = 0.18588897585868835 + 2.0 * 6.323503017425537
Epoch 740, val loss: 0.6896103620529175
Epoch 750, training loss: 12.823528289794922 = 0.17745353281497955 + 2.0 * 6.323037147521973
Epoch 750, val loss: 0.6915103793144226
Epoch 760, training loss: 12.81717300415039 = 0.1694471836090088 + 2.0 * 6.3238630294799805
Epoch 760, val loss: 0.6938380002975464
Epoch 770, training loss: 12.8031005859375 = 0.16187235713005066 + 2.0 * 6.320614337921143
Epoch 770, val loss: 0.6964600086212158
Epoch 780, training loss: 12.79200267791748 = 0.15471258759498596 + 2.0 * 6.318645000457764
Epoch 780, val loss: 0.699337899684906
Epoch 790, training loss: 12.786206245422363 = 0.14794011414051056 + 2.0 * 6.319133281707764
Epoch 790, val loss: 0.7025638222694397
Epoch 800, training loss: 12.779745101928711 = 0.14152562618255615 + 2.0 * 6.319109916687012
Epoch 800, val loss: 0.7060315012931824
Epoch 810, training loss: 12.767818450927734 = 0.1354590505361557 + 2.0 * 6.3161797523498535
Epoch 810, val loss: 0.7097495198249817
Epoch 820, training loss: 12.759481430053711 = 0.129729762673378 + 2.0 * 6.314875602722168
Epoch 820, val loss: 0.7136580944061279
Epoch 830, training loss: 12.74966049194336 = 0.12430861592292786 + 2.0 * 6.312675952911377
Epoch 830, val loss: 0.7177363038063049
Epoch 840, training loss: 12.749273300170898 = 0.1191653460264206 + 2.0 * 6.315053939819336
Epoch 840, val loss: 0.7219842076301575
Epoch 850, training loss: 12.741771697998047 = 0.11429650336503983 + 2.0 * 6.313737392425537
Epoch 850, val loss: 0.7263405323028564
Epoch 860, training loss: 12.735196113586426 = 0.10968612134456635 + 2.0 * 6.312755107879639
Epoch 860, val loss: 0.7307840585708618
Epoch 870, training loss: 12.720258712768555 = 0.10531177371740341 + 2.0 * 6.307473659515381
Epoch 870, val loss: 0.7354048490524292
Epoch 880, training loss: 12.714800834655762 = 0.10116162151098251 + 2.0 * 6.306819438934326
Epoch 880, val loss: 0.7400689721107483
Epoch 890, training loss: 12.72347640991211 = 0.09721025079488754 + 2.0 * 6.313133239746094
Epoch 890, val loss: 0.7448850274085999
Epoch 900, training loss: 12.710721015930176 = 0.09346788376569748 + 2.0 * 6.308626651763916
Epoch 900, val loss: 0.7496501207351685
Epoch 910, training loss: 12.700401306152344 = 0.08989570289850235 + 2.0 * 6.305253028869629
Epoch 910, val loss: 0.7545996308326721
Epoch 920, training loss: 12.702863693237305 = 0.08651205897331238 + 2.0 * 6.308176040649414
Epoch 920, val loss: 0.7595881819725037
Epoch 930, training loss: 12.690267562866211 = 0.08328098058700562 + 2.0 * 6.303493499755859
Epoch 930, val loss: 0.7645513415336609
Epoch 940, training loss: 12.682398796081543 = 0.08020919561386108 + 2.0 * 6.301095008850098
Epoch 940, val loss: 0.7696418762207031
Epoch 950, training loss: 12.687975883483887 = 0.07727482914924622 + 2.0 * 6.305350303649902
Epoch 950, val loss: 0.7747103571891785
Epoch 960, training loss: 12.683769226074219 = 0.07447222620248795 + 2.0 * 6.304648399353027
Epoch 960, val loss: 0.7798846960067749
Epoch 970, training loss: 12.669108390808105 = 0.07181188464164734 + 2.0 * 6.298648357391357
Epoch 970, val loss: 0.7849302887916565
Epoch 980, training loss: 12.665084838867188 = 0.06927785277366638 + 2.0 * 6.297903537750244
Epoch 980, val loss: 0.7900905013084412
Epoch 990, training loss: 12.664909362792969 = 0.0668550655245781 + 2.0 * 6.299026966094971
Epoch 990, val loss: 0.7952598333358765
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.788888888888889
0.8371112282551397
=== training gcn model ===
Epoch 0, training loss: 19.1488094329834 = 1.9550763368606567 + 2.0 * 8.596866607666016
Epoch 0, val loss: 1.9462189674377441
Epoch 10, training loss: 19.1370906829834 = 1.9441051483154297 + 2.0 * 8.596492767333984
Epoch 10, val loss: 1.935333490371704
Epoch 20, training loss: 19.116920471191406 = 1.9300726652145386 + 2.0 * 8.593423843383789
Epoch 20, val loss: 1.921616792678833
Epoch 30, training loss: 19.05068016052246 = 1.9109387397766113 + 2.0 * 8.569870948791504
Epoch 30, val loss: 1.9031873941421509
Epoch 40, training loss: 18.72046661376953 = 1.8873369693756104 + 2.0 * 8.41656494140625
Epoch 40, val loss: 1.8811041116714478
Epoch 50, training loss: 17.876256942749023 = 1.8627275228500366 + 2.0 * 8.00676441192627
Epoch 50, val loss: 1.8588212728500366
Epoch 60, training loss: 16.82080078125 = 1.8458625078201294 + 2.0 * 7.48746919631958
Epoch 60, val loss: 1.8431342840194702
Epoch 70, training loss: 15.960800170898438 = 1.8338863849639893 + 2.0 * 7.063457012176514
Epoch 70, val loss: 1.8311125040054321
Epoch 80, training loss: 15.664385795593262 = 1.8217092752456665 + 2.0 * 6.921338081359863
Epoch 80, val loss: 1.8189432621002197
Epoch 90, training loss: 15.425066947937012 = 1.8052688837051392 + 2.0 * 6.809898853302002
Epoch 90, val loss: 1.8040615320205688
Epoch 100, training loss: 15.251137733459473 = 1.7893954515457153 + 2.0 * 6.730871200561523
Epoch 100, val loss: 1.7904281616210938
Epoch 110, training loss: 15.128975868225098 = 1.7749751806259155 + 2.0 * 6.677000522613525
Epoch 110, val loss: 1.777734398841858
Epoch 120, training loss: 15.026752471923828 = 1.7608190774917603 + 2.0 * 6.6329665184021
Epoch 120, val loss: 1.7649768590927124
Epoch 130, training loss: 14.948896408081055 = 1.7462334632873535 + 2.0 * 6.60133171081543
Epoch 130, val loss: 1.7516963481903076
Epoch 140, training loss: 14.87294864654541 = 1.7307313680648804 + 2.0 * 6.571108818054199
Epoch 140, val loss: 1.737537145614624
Epoch 150, training loss: 14.806249618530273 = 1.7141040563583374 + 2.0 * 6.546072959899902
Epoch 150, val loss: 1.7223495244979858
Epoch 160, training loss: 14.745266914367676 = 1.6958162784576416 + 2.0 * 6.524725437164307
Epoch 160, val loss: 1.7056490182876587
Epoch 170, training loss: 14.694436073303223 = 1.6753933429718018 + 2.0 * 6.509521484375
Epoch 170, val loss: 1.6870830059051514
Epoch 180, training loss: 14.641275405883789 = 1.6526843309402466 + 2.0 * 6.494295597076416
Epoch 180, val loss: 1.6666260957717896
Epoch 190, training loss: 14.589929580688477 = 1.6274428367614746 + 2.0 * 6.481243133544922
Epoch 190, val loss: 1.6440811157226562
Epoch 200, training loss: 14.541304588317871 = 1.5991640090942383 + 2.0 * 6.471070289611816
Epoch 200, val loss: 1.619072675704956
Epoch 210, training loss: 14.492259979248047 = 1.568177342414856 + 2.0 * 6.46204137802124
Epoch 210, val loss: 1.5918941497802734
Epoch 220, training loss: 14.439940452575684 = 1.534669041633606 + 2.0 * 6.452635765075684
Epoch 220, val loss: 1.562793493270874
Epoch 230, training loss: 14.388593673706055 = 1.498682975769043 + 2.0 * 6.444955348968506
Epoch 230, val loss: 1.531936526298523
Epoch 240, training loss: 14.342768669128418 = 1.4605048894882202 + 2.0 * 6.441132068634033
Epoch 240, val loss: 1.4996832609176636
Epoch 250, training loss: 14.284934997558594 = 1.420820713043213 + 2.0 * 6.4320573806762695
Epoch 250, val loss: 1.466799020767212
Epoch 260, training loss: 14.22860050201416 = 1.3800679445266724 + 2.0 * 6.424266338348389
Epoch 260, val loss: 1.4336471557617188
Epoch 270, training loss: 14.175397872924805 = 1.338536262512207 + 2.0 * 6.418430805206299
Epoch 270, val loss: 1.4004427194595337
Epoch 280, training loss: 14.127827644348145 = 1.2969046831130981 + 2.0 * 6.415461540222168
Epoch 280, val loss: 1.3678361177444458
Epoch 290, training loss: 14.071813583374023 = 1.2559475898742676 + 2.0 * 6.407933235168457
Epoch 290, val loss: 1.3363330364227295
Epoch 300, training loss: 14.017647743225098 = 1.2157491445541382 + 2.0 * 6.400949478149414
Epoch 300, val loss: 1.3059896230697632
Epoch 310, training loss: 13.970236778259277 = 1.1764298677444458 + 2.0 * 6.3969035148620605
Epoch 310, val loss: 1.2768765687942505
Epoch 320, training loss: 13.930558204650879 = 1.1384241580963135 + 2.0 * 6.396067142486572
Epoch 320, val loss: 1.2491614818572998
Epoch 330, training loss: 13.879560470581055 = 1.102091908454895 + 2.0 * 6.388734340667725
Epoch 330, val loss: 1.2231453657150269
Epoch 340, training loss: 13.83279037475586 = 1.0674042701721191 + 2.0 * 6.382693290710449
Epoch 340, val loss: 1.198772668838501
Epoch 350, training loss: 13.793241500854492 = 1.034088373184204 + 2.0 * 6.379576683044434
Epoch 350, val loss: 1.175700306892395
Epoch 360, training loss: 13.751195907592773 = 1.0020614862442017 + 2.0 * 6.374567031860352
Epoch 360, val loss: 1.1539130210876465
Epoch 370, training loss: 13.71456241607666 = 0.9713395237922668 + 2.0 * 6.371611595153809
Epoch 370, val loss: 1.1333601474761963
Epoch 380, training loss: 13.677095413208008 = 0.9416173100471497 + 2.0 * 6.367739200592041
Epoch 380, val loss: 1.1138299703598022
Epoch 390, training loss: 13.655364990234375 = 0.9127148985862732 + 2.0 * 6.3713250160217285
Epoch 390, val loss: 1.09507155418396
Epoch 400, training loss: 13.607528686523438 = 0.8846896886825562 + 2.0 * 6.361419677734375
Epoch 400, val loss: 1.0771019458770752
Epoch 410, training loss: 13.573345184326172 = 0.8574986457824707 + 2.0 * 6.3579230308532715
Epoch 410, val loss: 1.0598914623260498
Epoch 420, training loss: 13.541029930114746 = 0.8308931589126587 + 2.0 * 6.355068206787109
Epoch 420, val loss: 1.0432554483413696
Epoch 430, training loss: 13.522104263305664 = 0.8047856688499451 + 2.0 * 6.358659267425537
Epoch 430, val loss: 1.0271531343460083
Epoch 440, training loss: 13.480411529541016 = 0.7793551087379456 + 2.0 * 6.350528240203857
Epoch 440, val loss: 1.011541724205017
Epoch 450, training loss: 13.461740493774414 = 0.7545120120048523 + 2.0 * 6.353614330291748
Epoch 450, val loss: 0.9965444803237915
Epoch 460, training loss: 13.423277854919434 = 0.7302634716033936 + 2.0 * 6.3465070724487305
Epoch 460, val loss: 0.982184648513794
Epoch 470, training loss: 13.392586708068848 = 0.706626296043396 + 2.0 * 6.34298038482666
Epoch 470, val loss: 0.9683635830879211
Epoch 480, training loss: 13.375551223754883 = 0.6835114359855652 + 2.0 * 6.346019744873047
Epoch 480, val loss: 0.9551609754562378
Epoch 490, training loss: 13.341538429260254 = 0.6610431671142578 + 2.0 * 6.340247631072998
Epoch 490, val loss: 0.9425076842308044
Epoch 500, training loss: 13.31251335144043 = 0.63922518491745 + 2.0 * 6.336644172668457
Epoch 500, val loss: 0.9305786490440369
Epoch 510, training loss: 13.286174774169922 = 0.6179602146148682 + 2.0 * 6.334107398986816
Epoch 510, val loss: 0.9192617535591125
Epoch 520, training loss: 13.277840614318848 = 0.5971760153770447 + 2.0 * 6.340332508087158
Epoch 520, val loss: 0.9086256623268127
Epoch 530, training loss: 13.24537467956543 = 0.5771180391311646 + 2.0 * 6.334128379821777
Epoch 530, val loss: 0.8984535932540894
Epoch 540, training loss: 13.217268943786621 = 0.5576294660568237 + 2.0 * 6.329819679260254
Epoch 540, val loss: 0.8890029191970825
Epoch 550, training loss: 13.193231582641602 = 0.5386086106300354 + 2.0 * 6.3273115158081055
Epoch 550, val loss: 0.8801392316818237
Epoch 560, training loss: 13.171229362487793 = 0.5199719071388245 + 2.0 * 6.325628757476807
Epoch 560, val loss: 0.8718160390853882
Epoch 570, training loss: 13.156242370605469 = 0.5016975402832031 + 2.0 * 6.327272415161133
Epoch 570, val loss: 0.8639518618583679
Epoch 580, training loss: 13.132819175720215 = 0.48389437794685364 + 2.0 * 6.324462413787842
Epoch 580, val loss: 0.8564707636833191
Epoch 590, training loss: 13.107561111450195 = 0.46641987562179565 + 2.0 * 6.320570468902588
Epoch 590, val loss: 0.8494033813476562
Epoch 600, training loss: 13.088628768920898 = 0.44923412799835205 + 2.0 * 6.319697380065918
Epoch 600, val loss: 0.8427097201347351
Epoch 610, training loss: 13.06931209564209 = 0.4323280453681946 + 2.0 * 6.3184919357299805
Epoch 610, val loss: 0.8363581895828247
Epoch 620, training loss: 13.051213264465332 = 0.4157572388648987 + 2.0 * 6.317728042602539
Epoch 620, val loss: 0.8303434252738953
Epoch 630, training loss: 13.038803100585938 = 0.3994857668876648 + 2.0 * 6.3196587562561035
Epoch 630, val loss: 0.8246540427207947
Epoch 640, training loss: 13.017230987548828 = 0.383518248796463 + 2.0 * 6.316856384277344
Epoch 640, val loss: 0.8193457722663879
Epoch 650, training loss: 12.992484092712402 = 0.36787688732147217 + 2.0 * 6.31230354309082
Epoch 650, val loss: 0.8143543004989624
Epoch 660, training loss: 12.975393295288086 = 0.35252103209495544 + 2.0 * 6.311436176300049
Epoch 660, val loss: 0.8096482157707214
Epoch 670, training loss: 12.973797798156738 = 0.3374609053134918 + 2.0 * 6.318168640136719
Epoch 670, val loss: 0.8053323030471802
Epoch 680, training loss: 12.946911811828613 = 0.32275843620300293 + 2.0 * 6.312076568603516
Epoch 680, val loss: 0.8012831807136536
Epoch 690, training loss: 12.922774314880371 = 0.30842575430870056 + 2.0 * 6.307174205780029
Epoch 690, val loss: 0.7976230382919312
Epoch 700, training loss: 12.908698081970215 = 0.294450581073761 + 2.0 * 6.30712366104126
Epoch 700, val loss: 0.794421911239624
Epoch 710, training loss: 12.902156829833984 = 0.28088822960853577 + 2.0 * 6.310634136199951
Epoch 710, val loss: 0.7915533185005188
Epoch 720, training loss: 12.877161979675293 = 0.267836332321167 + 2.0 * 6.304662704467773
Epoch 720, val loss: 0.789111316204071
Epoch 730, training loss: 12.863094329833984 = 0.25526162981987 + 2.0 * 6.3039164543151855
Epoch 730, val loss: 0.7871100306510925
Epoch 740, training loss: 12.848688125610352 = 0.24313698709011078 + 2.0 * 6.3027753829956055
Epoch 740, val loss: 0.7855727076530457
Epoch 750, training loss: 12.840977668762207 = 0.23152245581150055 + 2.0 * 6.304727554321289
Epoch 750, val loss: 0.7844474911689758
Epoch 760, training loss: 12.838791847229004 = 0.22042915225028992 + 2.0 * 6.309181213378906
Epoch 760, val loss: 0.7837752103805542
Epoch 770, training loss: 12.813566207885742 = 0.20997078716754913 + 2.0 * 6.301797866821289
Epoch 770, val loss: 0.7834993004798889
Epoch 780, training loss: 12.797161102294922 = 0.2000076025724411 + 2.0 * 6.298576831817627
Epoch 780, val loss: 0.7836871147155762
Epoch 790, training loss: 12.783807754516602 = 0.1905701607465744 + 2.0 * 6.296618938446045
Epoch 790, val loss: 0.7843148708343506
Epoch 800, training loss: 12.792404174804688 = 0.1816241443157196 + 2.0 * 6.305389881134033
Epoch 800, val loss: 0.7853485345840454
Epoch 810, training loss: 12.77375602722168 = 0.17317035794258118 + 2.0 * 6.30029296875
Epoch 810, val loss: 0.786637544631958
Epoch 820, training loss: 12.75430679321289 = 0.16525031626224518 + 2.0 * 6.294528007507324
Epoch 820, val loss: 0.7882813215255737
Epoch 830, training loss: 12.747390747070312 = 0.157768115401268 + 2.0 * 6.294811248779297
Epoch 830, val loss: 0.7902870774269104
Epoch 840, training loss: 12.738191604614258 = 0.15068915486335754 + 2.0 * 6.293751239776611
Epoch 840, val loss: 0.7925688028335571
Epoch 850, training loss: 12.74641227722168 = 0.14400896430015564 + 2.0 * 6.301201820373535
Epoch 850, val loss: 0.7951019406318665
Epoch 860, training loss: 12.721480369567871 = 0.13772587478160858 + 2.0 * 6.291877269744873
Epoch 860, val loss: 0.7977650761604309
Epoch 870, training loss: 12.709969520568848 = 0.1317943036556244 + 2.0 * 6.289087772369385
Epoch 870, val loss: 0.800732433795929
Epoch 880, training loss: 12.703228950500488 = 0.1261836588382721 + 2.0 * 6.288522720336914
Epoch 880, val loss: 0.803916871547699
Epoch 890, training loss: 12.699847221374512 = 0.1208631619811058 + 2.0 * 6.289492130279541
Epoch 890, val loss: 0.807272732257843
Epoch 900, training loss: 12.69296932220459 = 0.11580806225538254 + 2.0 * 6.288580417633057
Epoch 900, val loss: 0.810669481754303
Epoch 910, training loss: 12.686470985412598 = 0.11103259027004242 + 2.0 * 6.287719249725342
Epoch 910, val loss: 0.8142498731613159
Epoch 920, training loss: 12.68166446685791 = 0.10649983584880829 + 2.0 * 6.2875823974609375
Epoch 920, val loss: 0.8179919123649597
Epoch 930, training loss: 12.674398422241211 = 0.10219751298427582 + 2.0 * 6.286100387573242
Epoch 930, val loss: 0.821864128112793
Epoch 940, training loss: 12.670804023742676 = 0.09810276329517365 + 2.0 * 6.286350727081299
Epoch 940, val loss: 0.825868546962738
Epoch 950, training loss: 12.671793937683105 = 0.09420959651470184 + 2.0 * 6.288792133331299
Epoch 950, val loss: 0.8299010396003723
Epoch 960, training loss: 12.656472206115723 = 0.09053341299295425 + 2.0 * 6.2829694747924805
Epoch 960, val loss: 0.8340886831283569
Epoch 970, training loss: 12.650010108947754 = 0.0870196595788002 + 2.0 * 6.281495094299316
Epoch 970, val loss: 0.8383445739746094
Epoch 980, training loss: 12.645133972167969 = 0.08366955816745758 + 2.0 * 6.280732154846191
Epoch 980, val loss: 0.8426908254623413
Epoch 990, training loss: 12.668288230895996 = 0.08046425133943558 + 2.0 * 6.293911933898926
Epoch 990, val loss: 0.8470102548599243
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8222222222222223
0.8434370057986295
=== training gcn model ===
Epoch 0, training loss: 19.128002166748047 = 1.934294581413269 + 2.0 * 8.596854209899902
Epoch 0, val loss: 1.935678243637085
Epoch 10, training loss: 19.118043899536133 = 1.9248026609420776 + 2.0 * 8.596620559692383
Epoch 10, val loss: 1.9266622066497803
Epoch 20, training loss: 19.10264015197754 = 1.9133669137954712 + 2.0 * 8.594636917114258
Epoch 20, val loss: 1.9153932332992554
Epoch 30, training loss: 19.0540828704834 = 1.897955060005188 + 2.0 * 8.57806396484375
Epoch 30, val loss: 1.8999954462051392
Epoch 40, training loss: 18.80280876159668 = 1.8784420490264893 + 2.0 * 8.462182998657227
Epoch 40, val loss: 1.8809171915054321
Epoch 50, training loss: 17.712505340576172 = 1.8575315475463867 + 2.0 * 7.927487373352051
Epoch 50, val loss: 1.8610411882400513
Epoch 60, training loss: 16.697494506835938 = 1.8392670154571533 + 2.0 * 7.429113388061523
Epoch 60, val loss: 1.8437659740447998
Epoch 70, training loss: 16.106998443603516 = 1.8254249095916748 + 2.0 * 7.140786647796631
Epoch 70, val loss: 1.8293217420578003
Epoch 80, training loss: 15.776426315307617 = 1.8111546039581299 + 2.0 * 6.982635974884033
Epoch 80, val loss: 1.8156540393829346
Epoch 90, training loss: 15.515576362609863 = 1.7977850437164307 + 2.0 * 6.858895778656006
Epoch 90, val loss: 1.8031858205795288
Epoch 100, training loss: 15.364032745361328 = 1.7836579084396362 + 2.0 * 6.790187358856201
Epoch 100, val loss: 1.7899715900421143
Epoch 110, training loss: 15.255577087402344 = 1.7686131000518799 + 2.0 * 6.7434821128845215
Epoch 110, val loss: 1.7758758068084717
Epoch 120, training loss: 15.160918235778809 = 1.752761721611023 + 2.0 * 6.704078197479248
Epoch 120, val loss: 1.7611345052719116
Epoch 130, training loss: 15.074641227722168 = 1.7360695600509644 + 2.0 * 6.669285774230957
Epoch 130, val loss: 1.7458267211914062
Epoch 140, training loss: 14.991583824157715 = 1.7182676792144775 + 2.0 * 6.636658191680908
Epoch 140, val loss: 1.7298167943954468
Epoch 150, training loss: 14.916410446166992 = 1.6986031532287598 + 2.0 * 6.608903884887695
Epoch 150, val loss: 1.712479591369629
Epoch 160, training loss: 14.847061157226562 = 1.6762652397155762 + 2.0 * 6.585397720336914
Epoch 160, val loss: 1.693025827407837
Epoch 170, training loss: 14.783148765563965 = 1.6507349014282227 + 2.0 * 6.566206932067871
Epoch 170, val loss: 1.6711980104446411
Epoch 180, training loss: 14.719127655029297 = 1.6221628189086914 + 2.0 * 6.548482418060303
Epoch 180, val loss: 1.646841287612915
Epoch 190, training loss: 14.657200813293457 = 1.5901771783828735 + 2.0 * 6.533511638641357
Epoch 190, val loss: 1.6197565793991089
Epoch 200, training loss: 14.592621803283691 = 1.5548696517944336 + 2.0 * 6.518876075744629
Epoch 200, val loss: 1.5901298522949219
Epoch 210, training loss: 14.528021812438965 = 1.5163586139678955 + 2.0 * 6.505831718444824
Epoch 210, val loss: 1.5580452680587769
Epoch 220, training loss: 14.465967178344727 = 1.474590539932251 + 2.0 * 6.495688438415527
Epoch 220, val loss: 1.5235395431518555
Epoch 230, training loss: 14.400251388549805 = 1.4304108619689941 + 2.0 * 6.484920024871826
Epoch 230, val loss: 1.4874041080474854
Epoch 240, training loss: 14.333624839782715 = 1.3841521739959717 + 2.0 * 6.474736213684082
Epoch 240, val loss: 1.4500154256820679
Epoch 250, training loss: 14.270822525024414 = 1.336124300956726 + 2.0 * 6.467349052429199
Epoch 250, val loss: 1.4115279912948608
Epoch 260, training loss: 14.20603084564209 = 1.2872514724731445 + 2.0 * 6.459389686584473
Epoch 260, val loss: 1.372677206993103
Epoch 270, training loss: 14.142520904541016 = 1.2382988929748535 + 2.0 * 6.452110767364502
Epoch 270, val loss: 1.3342198133468628
Epoch 280, training loss: 14.081015586853027 = 1.1895800828933716 + 2.0 * 6.445717811584473
Epoch 280, val loss: 1.2963883876800537
Epoch 290, training loss: 14.019892692565918 = 1.1416046619415283 + 2.0 * 6.439144134521484
Epoch 290, val loss: 1.2594504356384277
Epoch 300, training loss: 13.963390350341797 = 1.0950337648391724 + 2.0 * 6.434178352355957
Epoch 300, val loss: 1.2236980199813843
Epoch 310, training loss: 13.905707359313965 = 1.0496999025344849 + 2.0 * 6.428003787994385
Epoch 310, val loss: 1.189156413078308
Epoch 320, training loss: 13.853883743286133 = 1.005932092666626 + 2.0 * 6.423975944519043
Epoch 320, val loss: 1.155983567237854
Epoch 330, training loss: 13.801348686218262 = 0.9642489552497864 + 2.0 * 6.41855001449585
Epoch 330, val loss: 1.1242836713790894
Epoch 340, training loss: 13.752250671386719 = 0.9243074655532837 + 2.0 * 6.413971424102783
Epoch 340, val loss: 1.0940824747085571
Epoch 350, training loss: 13.706586837768555 = 0.8861844539642334 + 2.0 * 6.410201072692871
Epoch 350, val loss: 1.0652498006820679
Epoch 360, training loss: 13.65948486328125 = 0.8501240015029907 + 2.0 * 6.404680252075195
Epoch 360, val loss: 1.0379631519317627
Epoch 370, training loss: 13.616583824157715 = 0.8157039284706116 + 2.0 * 6.400439739227295
Epoch 370, val loss: 1.0120567083358765
Epoch 380, training loss: 13.58829116821289 = 0.7827983498573303 + 2.0 * 6.402746200561523
Epoch 380, val loss: 0.9874727725982666
Epoch 390, training loss: 13.540711402893066 = 0.7514891028404236 + 2.0 * 6.394611358642578
Epoch 390, val loss: 0.9641911387443542
Epoch 400, training loss: 13.501158714294434 = 0.7214325070381165 + 2.0 * 6.389863014221191
Epoch 400, val loss: 0.9420891404151917
Epoch 410, training loss: 13.465530395507812 = 0.6922659873962402 + 2.0 * 6.386631965637207
Epoch 410, val loss: 0.9209263920783997
Epoch 420, training loss: 13.434643745422363 = 0.6638604998588562 + 2.0 * 6.385391712188721
Epoch 420, val loss: 0.9006772041320801
Epoch 430, training loss: 13.39675235748291 = 0.6362205147743225 + 2.0 * 6.380265712738037
Epoch 430, val loss: 0.8812307119369507
Epoch 440, training loss: 13.364569664001465 = 0.6089884638786316 + 2.0 * 6.377790451049805
Epoch 440, val loss: 0.8624168634414673
Epoch 450, training loss: 13.336055755615234 = 0.5819414854049683 + 2.0 * 6.377057075500488
Epoch 450, val loss: 0.8441604375839233
Epoch 460, training loss: 13.306455612182617 = 0.5550592541694641 + 2.0 * 6.375698089599609
Epoch 460, val loss: 0.8264285922050476
Epoch 470, training loss: 13.269200325012207 = 0.5284093618392944 + 2.0 * 6.370395660400391
Epoch 470, val loss: 0.8093251585960388
Epoch 480, training loss: 13.250734329223633 = 0.5019709467887878 + 2.0 * 6.3743815422058105
Epoch 480, val loss: 0.7927970886230469
Epoch 490, training loss: 13.21252727508545 = 0.4760034382343292 + 2.0 * 6.368261814117432
Epoch 490, val loss: 0.7771898508071899
Epoch 500, training loss: 13.17895793914795 = 0.4507687985897064 + 2.0 * 6.3640947341918945
Epoch 500, val loss: 0.7624208927154541
Epoch 510, training loss: 13.146998405456543 = 0.4261431396007538 + 2.0 * 6.3604278564453125
Epoch 510, val loss: 0.7486237287521362
Epoch 520, training loss: 13.124260902404785 = 0.4023434817790985 + 2.0 * 6.360958576202393
Epoch 520, val loss: 0.7358932495117188
Epoch 530, training loss: 13.111896514892578 = 0.37975895404815674 + 2.0 * 6.3660688400268555
Epoch 530, val loss: 0.7242330312728882
Epoch 540, training loss: 13.068502426147461 = 0.35844895243644714 + 2.0 * 6.355026721954346
Epoch 540, val loss: 0.7138399481773376
Epoch 550, training loss: 13.042035102844238 = 0.33828824758529663 + 2.0 * 6.351873397827148
Epoch 550, val loss: 0.7045923471450806
Epoch 560, training loss: 13.022286415100098 = 0.3192346394062042 + 2.0 * 6.351525783538818
Epoch 560, val loss: 0.6964382529258728
Epoch 570, training loss: 12.99704647064209 = 0.30125969648361206 + 2.0 * 6.347893238067627
Epoch 570, val loss: 0.6894130706787109
Epoch 580, training loss: 12.98047924041748 = 0.28448617458343506 + 2.0 * 6.347996711730957
Epoch 580, val loss: 0.6832669377326965
Epoch 590, training loss: 12.957132339477539 = 0.2687067687511444 + 2.0 * 6.344213008880615
Epoch 590, val loss: 0.6781692504882812
Epoch 600, training loss: 12.945018768310547 = 0.2539328634738922 + 2.0 * 6.345542907714844
Epoch 600, val loss: 0.6739194989204407
Epoch 610, training loss: 12.932453155517578 = 0.2401166409254074 + 2.0 * 6.346168041229248
Epoch 610, val loss: 0.6704791188240051
Epoch 620, training loss: 12.90955924987793 = 0.22729328274726868 + 2.0 * 6.341133117675781
Epoch 620, val loss: 0.6678730845451355
Epoch 630, training loss: 12.890198707580566 = 0.21538732945919037 + 2.0 * 6.337405681610107
Epoch 630, val loss: 0.6659725904464722
Epoch 640, training loss: 12.876579284667969 = 0.20426294207572937 + 2.0 * 6.336158275604248
Epoch 640, val loss: 0.6648463606834412
Epoch 650, training loss: 12.864450454711914 = 0.19386699795722961 + 2.0 * 6.335291862487793
Epoch 650, val loss: 0.6644287109375
Epoch 660, training loss: 12.850046157836914 = 0.18419566750526428 + 2.0 * 6.332925319671631
Epoch 660, val loss: 0.6645554304122925
Epoch 670, training loss: 12.835420608520508 = 0.17518103122711182 + 2.0 * 6.330119609832764
Epoch 670, val loss: 0.6652518510818481
Epoch 680, training loss: 12.832539558410645 = 0.16675427556037903 + 2.0 * 6.332892417907715
Epoch 680, val loss: 0.6664360165596008
Epoch 690, training loss: 12.815146446228027 = 0.15884780883789062 + 2.0 * 6.328149318695068
Epoch 690, val loss: 0.6681803464889526
Epoch 700, training loss: 12.826637268066406 = 0.15143875777721405 + 2.0 * 6.337599277496338
Epoch 700, val loss: 0.670318603515625
Epoch 710, training loss: 12.797293663024902 = 0.14454950392246246 + 2.0 * 6.326372146606445
Epoch 710, val loss: 0.6726863980293274
Epoch 720, training loss: 12.780736923217773 = 0.1380660980939865 + 2.0 * 6.321335315704346
Epoch 720, val loss: 0.6755247116088867
Epoch 730, training loss: 12.772879600524902 = 0.13196353614330292 + 2.0 * 6.320457935333252
Epoch 730, val loss: 0.6787262558937073
Epoch 740, training loss: 12.783129692077637 = 0.1261928826570511 + 2.0 * 6.328468322753906
Epoch 740, val loss: 0.6822574734687805
Epoch 750, training loss: 12.765417098999023 = 0.12078190594911575 + 2.0 * 6.322317600250244
Epoch 750, val loss: 0.6859288215637207
Epoch 760, training loss: 12.747759819030762 = 0.11567288637161255 + 2.0 * 6.316043376922607
Epoch 760, val loss: 0.6898248195648193
Epoch 770, training loss: 12.744596481323242 = 0.11083061993122101 + 2.0 * 6.316883087158203
Epoch 770, val loss: 0.6939684152603149
Epoch 780, training loss: 12.734979629516602 = 0.10625068843364716 + 2.0 * 6.314364433288574
Epoch 780, val loss: 0.6982523202896118
Epoch 790, training loss: 12.730583190917969 = 0.10193025320768356 + 2.0 * 6.314326286315918
Epoch 790, val loss: 0.7026455402374268
Epoch 800, training loss: 12.720254898071289 = 0.09783788025379181 + 2.0 * 6.311208724975586
Epoch 800, val loss: 0.7072628736495972
Epoch 810, training loss: 12.737996101379395 = 0.09395608305931091 + 2.0 * 6.322020053863525
Epoch 810, val loss: 0.7119731903076172
Epoch 820, training loss: 12.71267318725586 = 0.09024717658758163 + 2.0 * 6.31121301651001
Epoch 820, val loss: 0.7168928980827332
Epoch 830, training loss: 12.702486038208008 = 0.08675874769687653 + 2.0 * 6.307863712310791
Epoch 830, val loss: 0.7218095064163208
Epoch 840, training loss: 12.696586608886719 = 0.08342647552490234 + 2.0 * 6.306580066680908
Epoch 840, val loss: 0.726868212223053
Epoch 850, training loss: 12.71847152709961 = 0.08024998009204865 + 2.0 * 6.319110870361328
Epoch 850, val loss: 0.7320119738578796
Epoch 860, training loss: 12.688990592956543 = 0.07725129276514053 + 2.0 * 6.3058695793151855
Epoch 860, val loss: 0.7370536923408508
Epoch 870, training loss: 12.68114948272705 = 0.07440028339624405 + 2.0 * 6.303374767303467
Epoch 870, val loss: 0.7422481179237366
Epoch 880, training loss: 12.67634391784668 = 0.07168295979499817 + 2.0 * 6.302330493927002
Epoch 880, val loss: 0.7475206255912781
Epoch 890, training loss: 12.671524047851562 = 0.06907923519611359 + 2.0 * 6.301222324371338
Epoch 890, val loss: 0.7528358697891235
Epoch 900, training loss: 12.686389923095703 = 0.06659197062253952 + 2.0 * 6.309898853302002
Epoch 900, val loss: 0.7581545114517212
Epoch 910, training loss: 12.66653823852539 = 0.06423179805278778 + 2.0 * 6.301153182983398
Epoch 910, val loss: 0.7633628249168396
Epoch 920, training loss: 12.66055965423584 = 0.06199461966753006 + 2.0 * 6.299282550811768
Epoch 920, val loss: 0.7686392068862915
Epoch 930, training loss: 12.654605865478516 = 0.0598611943423748 + 2.0 * 6.297372341156006
Epoch 930, val loss: 0.7739953398704529
Epoch 940, training loss: 12.652929306030273 = 0.05781193822622299 + 2.0 * 6.297558784484863
Epoch 940, val loss: 0.7793835401535034
Epoch 950, training loss: 12.653343200683594 = 0.055848781019449234 + 2.0 * 6.2987470626831055
Epoch 950, val loss: 0.7846801280975342
Epoch 960, training loss: 12.643586158752441 = 0.05399075150489807 + 2.0 * 6.294797897338867
Epoch 960, val loss: 0.7899718880653381
Epoch 970, training loss: 12.641743659973145 = 0.05221162736415863 + 2.0 * 6.294765949249268
Epoch 970, val loss: 0.7952781915664673
Epoch 980, training loss: 12.636486053466797 = 0.050499752163887024 + 2.0 * 6.292993068695068
Epoch 980, val loss: 0.8007086515426636
Epoch 990, training loss: 12.634201049804688 = 0.04886368662118912 + 2.0 * 6.29266881942749
Epoch 990, val loss: 0.8060513138771057
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8333333333333334
0.8323668950975225
The final CL Acc:0.81481, 0.01889, The final GNN Acc:0.83764, 0.00453
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.5 der2:0.2 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11628])
remove edge: torch.Size([2, 9438])
updated graph: torch.Size([2, 10510])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 19.149768829345703 = 1.956097960472107 + 2.0 * 8.596835136413574
Epoch 0, val loss: 1.9661643505096436
Epoch 10, training loss: 19.138227462768555 = 1.9452991485595703 + 2.0 * 8.596464157104492
Epoch 10, val loss: 1.9548715353012085
Epoch 20, training loss: 19.118844985961914 = 1.9316214323043823 + 2.0 * 8.593611717224121
Epoch 20, val loss: 1.9404531717300415
Epoch 30, training loss: 19.059886932373047 = 1.9125802516937256 + 2.0 * 8.573653221130371
Epoch 30, val loss: 1.920434832572937
Epoch 40, training loss: 18.78910255432129 = 1.889600396156311 + 2.0 * 8.449750900268555
Epoch 40, val loss: 1.8972814083099365
Epoch 50, training loss: 18.004661560058594 = 1.8667807579040527 + 2.0 * 8.068940162658691
Epoch 50, val loss: 1.8749892711639404
Epoch 60, training loss: 17.01628303527832 = 1.851026177406311 + 2.0 * 7.58262825012207
Epoch 60, val loss: 1.8597790002822876
Epoch 70, training loss: 16.337200164794922 = 1.838560938835144 + 2.0 * 7.249319553375244
Epoch 70, val loss: 1.8465327024459839
Epoch 80, training loss: 15.956822395324707 = 1.8261991739273071 + 2.0 * 7.065311431884766
Epoch 80, val loss: 1.8341577053070068
Epoch 90, training loss: 15.660493850708008 = 1.8134225606918335 + 2.0 * 6.9235358238220215
Epoch 90, val loss: 1.8219130039215088
Epoch 100, training loss: 15.461372375488281 = 1.8015639781951904 + 2.0 * 6.829904079437256
Epoch 100, val loss: 1.811011791229248
Epoch 110, training loss: 15.314657211303711 = 1.7907582521438599 + 2.0 * 6.76194953918457
Epoch 110, val loss: 1.8009687662124634
Epoch 120, training loss: 15.207584381103516 = 1.7801839113235474 + 2.0 * 6.713700294494629
Epoch 120, val loss: 1.7908631563186646
Epoch 130, training loss: 15.108783721923828 = 1.76926589012146 + 2.0 * 6.6697587966918945
Epoch 130, val loss: 1.7804471254348755
Epoch 140, training loss: 15.026987075805664 = 1.7578340768814087 + 2.0 * 6.634576320648193
Epoch 140, val loss: 1.769749402999878
Epoch 150, training loss: 14.953903198242188 = 1.7455438375473022 + 2.0 * 6.604179859161377
Epoch 150, val loss: 1.7584534883499146
Epoch 160, training loss: 14.889184951782227 = 1.7320033311843872 + 2.0 * 6.5785908699035645
Epoch 160, val loss: 1.7462151050567627
Epoch 170, training loss: 14.836196899414062 = 1.7170352935791016 + 2.0 * 6.5595808029174805
Epoch 170, val loss: 1.732904314994812
Epoch 180, training loss: 14.778640747070312 = 1.7004952430725098 + 2.0 * 6.539072513580322
Epoch 180, val loss: 1.7184417247772217
Epoch 190, training loss: 14.724427223205566 = 1.6821335554122925 + 2.0 * 6.521146774291992
Epoch 190, val loss: 1.7025299072265625
Epoch 200, training loss: 14.675324440002441 = 1.661514401435852 + 2.0 * 6.5069050788879395
Epoch 200, val loss: 1.684691309928894
Epoch 210, training loss: 14.629688262939453 = 1.6384905576705933 + 2.0 * 6.495598793029785
Epoch 210, val loss: 1.6648920774459839
Epoch 220, training loss: 14.57553482055664 = 1.6130543947219849 + 2.0 * 6.481240272521973
Epoch 220, val loss: 1.6431283950805664
Epoch 230, training loss: 14.526566505432129 = 1.5851986408233643 + 2.0 * 6.470684051513672
Epoch 230, val loss: 1.6193193197250366
Epoch 240, training loss: 14.476554870605469 = 1.5546514987945557 + 2.0 * 6.460951805114746
Epoch 240, val loss: 1.5934797525405884
Epoch 250, training loss: 14.428546905517578 = 1.5216126441955566 + 2.0 * 6.45346736907959
Epoch 250, val loss: 1.5656378269195557
Epoch 260, training loss: 14.378036499023438 = 1.4866359233856201 + 2.0 * 6.445700168609619
Epoch 260, val loss: 1.536434531211853
Epoch 270, training loss: 14.326242446899414 = 1.4496437311172485 + 2.0 * 6.438299179077148
Epoch 270, val loss: 1.5060497522354126
Epoch 280, training loss: 14.273516654968262 = 1.4113388061523438 + 2.0 * 6.431088924407959
Epoch 280, val loss: 1.4746967554092407
Epoch 290, training loss: 14.225653648376465 = 1.371682047843933 + 2.0 * 6.426985740661621
Epoch 290, val loss: 1.4431138038635254
Epoch 300, training loss: 14.170604705810547 = 1.331477403640747 + 2.0 * 6.4195637702941895
Epoch 300, val loss: 1.4114408493041992
Epoch 310, training loss: 14.117011070251465 = 1.2907320261001587 + 2.0 * 6.413139343261719
Epoch 310, val loss: 1.3801112174987793
Epoch 320, training loss: 14.074604034423828 = 1.2497076988220215 + 2.0 * 6.412448406219482
Epoch 320, val loss: 1.3491957187652588
Epoch 330, training loss: 14.018366813659668 = 1.2089002132415771 + 2.0 * 6.404733180999756
Epoch 330, val loss: 1.319352149963379
Epoch 340, training loss: 13.9736967086792 = 1.168502926826477 + 2.0 * 6.402596950531006
Epoch 340, val loss: 1.2903656959533691
Epoch 350, training loss: 13.917865753173828 = 1.1289411783218384 + 2.0 * 6.3944621086120605
Epoch 350, val loss: 1.2623519897460938
Epoch 360, training loss: 13.871033668518066 = 1.0900957584381104 + 2.0 * 6.390469074249268
Epoch 360, val loss: 1.2355928421020508
Epoch 370, training loss: 13.833158493041992 = 1.0522148609161377 + 2.0 * 6.390471935272217
Epoch 370, val loss: 1.2100051641464233
Epoch 380, training loss: 13.781839370727539 = 1.015653371810913 + 2.0 * 6.383092880249023
Epoch 380, val loss: 1.1858859062194824
Epoch 390, training loss: 13.75338363647461 = 0.9806501865386963 + 2.0 * 6.386366844177246
Epoch 390, val loss: 1.1630325317382812
Epoch 400, training loss: 13.701436996459961 = 0.9472969770431519 + 2.0 * 6.37706995010376
Epoch 400, val loss: 1.1418893337249756
Epoch 410, training loss: 13.661046981811523 = 0.9157372117042542 + 2.0 * 6.372654914855957
Epoch 410, val loss: 1.1222931146621704
Epoch 420, training loss: 13.63293743133545 = 0.8857624530792236 + 2.0 * 6.373587608337402
Epoch 420, val loss: 1.1041780710220337
Epoch 430, training loss: 13.595649719238281 = 0.8575447797775269 + 2.0 * 6.369052410125732
Epoch 430, val loss: 1.0876299142837524
Epoch 440, training loss: 13.56047248840332 = 0.831148624420166 + 2.0 * 6.364662170410156
Epoch 440, val loss: 1.0726205110549927
Epoch 450, training loss: 13.532539367675781 = 0.8063696622848511 + 2.0 * 6.36308479309082
Epoch 450, val loss: 1.059011459350586
Epoch 460, training loss: 13.498785018920898 = 0.7830947041511536 + 2.0 * 6.357845306396484
Epoch 460, val loss: 1.046876072883606
Epoch 470, training loss: 13.47131633758545 = 0.7612846493721008 + 2.0 * 6.355015754699707
Epoch 470, val loss: 1.0359224081039429
Epoch 480, training loss: 13.460641860961914 = 0.740748405456543 + 2.0 * 6.3599467277526855
Epoch 480, val loss: 1.0260790586471558
Epoch 490, training loss: 13.424610137939453 = 0.7213575839996338 + 2.0 * 6.351626396179199
Epoch 490, val loss: 1.017159342765808
Epoch 500, training loss: 13.40276050567627 = 0.7030196785926819 + 2.0 * 6.349870204925537
Epoch 500, val loss: 1.009225845336914
Epoch 510, training loss: 13.385644912719727 = 0.68544602394104 + 2.0 * 6.350099563598633
Epoch 510, val loss: 1.0019673109054565
Epoch 520, training loss: 13.357192993164062 = 0.6685084700584412 + 2.0 * 6.344342231750488
Epoch 520, val loss: 0.9952405095100403
Epoch 530, training loss: 13.334589958190918 = 0.6520265340805054 + 2.0 * 6.341281890869141
Epoch 530, val loss: 0.9890194535255432
Epoch 540, training loss: 13.329679489135742 = 0.6358816623687744 + 2.0 * 6.346899032592773
Epoch 540, val loss: 0.9829628467559814
Epoch 550, training loss: 13.301718711853027 = 0.6199209690093994 + 2.0 * 6.3408989906311035
Epoch 550, val loss: 0.9774997234344482
Epoch 560, training loss: 13.275151252746582 = 0.6041904091835022 + 2.0 * 6.335480213165283
Epoch 560, val loss: 0.9721008539199829
Epoch 570, training loss: 13.2557373046875 = 0.5884493589401245 + 2.0 * 6.333643913269043
Epoch 570, val loss: 0.9667301774024963
Epoch 580, training loss: 13.262741088867188 = 0.5726172924041748 + 2.0 * 6.345061779022217
Epoch 580, val loss: 0.9614691734313965
Epoch 590, training loss: 13.219326972961426 = 0.5567835569381714 + 2.0 * 6.331271648406982
Epoch 590, val loss: 0.9564477801322937
Epoch 600, training loss: 13.198596000671387 = 0.540803074836731 + 2.0 * 6.328896522521973
Epoch 600, val loss: 0.9514698386192322
Epoch 610, training loss: 13.182897567749023 = 0.5246574282646179 + 2.0 * 6.32912015914917
Epoch 610, val loss: 0.9465070962905884
Epoch 620, training loss: 13.163126945495605 = 0.5082981586456299 + 2.0 * 6.327414512634277
Epoch 620, val loss: 0.9417824745178223
Epoch 630, training loss: 13.140549659729004 = 0.4917915463447571 + 2.0 * 6.324378967285156
Epoch 630, val loss: 0.9373283386230469
Epoch 640, training loss: 13.121243476867676 = 0.4750620722770691 + 2.0 * 6.323090553283691
Epoch 640, val loss: 0.9329032897949219
Epoch 650, training loss: 13.113033294677734 = 0.45810043811798096 + 2.0 * 6.3274664878845215
Epoch 650, val loss: 0.9287742376327515
Epoch 660, training loss: 13.085095405578613 = 0.44108396768569946 + 2.0 * 6.322005748748779
Epoch 660, val loss: 0.9246501326560974
Epoch 670, training loss: 13.0637788772583 = 0.4238760769367218 + 2.0 * 6.31995153427124
Epoch 670, val loss: 0.921001136302948
Epoch 680, training loss: 13.059453010559082 = 0.40666714310646057 + 2.0 * 6.326393127441406
Epoch 680, val loss: 0.9174301624298096
Epoch 690, training loss: 13.021261215209961 = 0.38942527770996094 + 2.0 * 6.31591796875
Epoch 690, val loss: 0.914368212223053
Epoch 700, training loss: 13.00377082824707 = 0.3722662031650543 + 2.0 * 6.3157525062561035
Epoch 700, val loss: 0.9114596843719482
Epoch 710, training loss: 12.995678901672363 = 0.3552156984806061 + 2.0 * 6.3202314376831055
Epoch 710, val loss: 0.9089661240577698
Epoch 720, training loss: 12.967154502868652 = 0.33840233087539673 + 2.0 * 6.314375877380371
Epoch 720, val loss: 0.9068359136581421
Epoch 730, training loss: 12.94906234741211 = 0.3218361437320709 + 2.0 * 6.313612937927246
Epoch 730, val loss: 0.9050137996673584
Epoch 740, training loss: 12.928522109985352 = 0.3056102693080902 + 2.0 * 6.311455726623535
Epoch 740, val loss: 0.903364360332489
Epoch 750, training loss: 12.913813591003418 = 0.28976646065711975 + 2.0 * 6.312023639678955
Epoch 750, val loss: 0.9021878838539124
Epoch 760, training loss: 12.891254425048828 = 0.27442455291748047 + 2.0 * 6.308414936065674
Epoch 760, val loss: 0.9011720418930054
Epoch 770, training loss: 12.87804126739502 = 0.25959303975105286 + 2.0 * 6.3092241287231445
Epoch 770, val loss: 0.9006567597389221
Epoch 780, training loss: 12.874134063720703 = 0.2453390508890152 + 2.0 * 6.31439733505249
Epoch 780, val loss: 0.9003018736839294
Epoch 790, training loss: 12.849091529846191 = 0.23168997466564178 + 2.0 * 6.3087005615234375
Epoch 790, val loss: 0.9001778960227966
Epoch 800, training loss: 12.830138206481934 = 0.21871620416641235 + 2.0 * 6.305710792541504
Epoch 800, val loss: 0.9006870985031128
Epoch 810, training loss: 12.816954612731934 = 0.20641735196113586 + 2.0 * 6.30526876449585
Epoch 810, val loss: 0.9013088941574097
Epoch 820, training loss: 12.803967475891113 = 0.1948104053735733 + 2.0 * 6.3045783042907715
Epoch 820, val loss: 0.9022925496101379
Epoch 830, training loss: 12.797330856323242 = 0.18391695618629456 + 2.0 * 6.30670690536499
Epoch 830, val loss: 0.9035364389419556
Epoch 840, training loss: 12.777562141418457 = 0.1737079620361328 + 2.0 * 6.301927089691162
Epoch 840, val loss: 0.9055214524269104
Epoch 850, training loss: 12.767433166503906 = 0.16419565677642822 + 2.0 * 6.301618576049805
Epoch 850, val loss: 0.9073841571807861
Epoch 860, training loss: 12.758441925048828 = 0.15531417727470398 + 2.0 * 6.301563739776611
Epoch 860, val loss: 0.9099234342575073
Epoch 870, training loss: 12.742841720581055 = 0.14706730842590332 + 2.0 * 6.297887325286865
Epoch 870, val loss: 0.9126996397972107
Epoch 880, training loss: 12.746050834655762 = 0.13937127590179443 + 2.0 * 6.303339958190918
Epoch 880, val loss: 0.9156544804573059
Epoch 890, training loss: 12.730132102966309 = 0.13222776353359222 + 2.0 * 6.298952102661133
Epoch 890, val loss: 0.9187180399894714
Epoch 900, training loss: 12.715330123901367 = 0.12560461461544037 + 2.0 * 6.294862747192383
Epoch 900, val loss: 0.9224016666412354
Epoch 910, training loss: 12.70909309387207 = 0.11943134665489197 + 2.0 * 6.294830799102783
Epoch 910, val loss: 0.925980269908905
Epoch 920, training loss: 12.707246780395508 = 0.11366765201091766 + 2.0 * 6.296789646148682
Epoch 920, val loss: 0.9297839403152466
Epoch 930, training loss: 12.692453384399414 = 0.10828188061714172 + 2.0 * 6.292085647583008
Epoch 930, val loss: 0.9340369701385498
Epoch 940, training loss: 12.684715270996094 = 0.10324911028146744 + 2.0 * 6.2907328605651855
Epoch 940, val loss: 0.9382444620132446
Epoch 950, training loss: 12.682051658630371 = 0.09852340072393417 + 2.0 * 6.291764259338379
Epoch 950, val loss: 0.9426971673965454
Epoch 960, training loss: 12.676549911499023 = 0.09409301728010178 + 2.0 * 6.291228294372559
Epoch 960, val loss: 0.9469337463378906
Epoch 970, training loss: 12.678765296936035 = 0.08994176238775253 + 2.0 * 6.294411659240723
Epoch 970, val loss: 0.9517347812652588
Epoch 980, training loss: 12.6626615524292 = 0.0860467478632927 + 2.0 * 6.288307189941406
Epoch 980, val loss: 0.9563385844230652
Epoch 990, training loss: 12.658730506896973 = 0.08237813413143158 + 2.0 * 6.2881760597229
Epoch 990, val loss: 0.9610448479652405
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7444444444444445
0.8039008961518187
=== training gcn model ===
Epoch 0, training loss: 19.135528564453125 = 1.941837191581726 + 2.0 * 8.596845626831055
Epoch 0, val loss: 1.9427679777145386
Epoch 10, training loss: 19.12516975402832 = 1.9320346117019653 + 2.0 * 8.596567153930664
Epoch 10, val loss: 1.9333763122558594
Epoch 20, training loss: 19.108394622802734 = 1.919773817062378 + 2.0 * 8.594310760498047
Epoch 20, val loss: 1.9209821224212646
Epoch 30, training loss: 19.056629180908203 = 1.9027013778686523 + 2.0 * 8.576963424682617
Epoch 30, val loss: 1.9032965898513794
Epoch 40, training loss: 18.80968475341797 = 1.8818069696426392 + 2.0 * 8.46393871307373
Epoch 40, val loss: 1.8827500343322754
Epoch 50, training loss: 18.05453109741211 = 1.861045002937317 + 2.0 * 8.096742630004883
Epoch 50, val loss: 1.8631340265274048
Epoch 60, training loss: 17.212244033813477 = 1.8441802263259888 + 2.0 * 7.684031963348389
Epoch 60, val loss: 1.8476835489273071
Epoch 70, training loss: 16.262571334838867 = 1.832371473312378 + 2.0 * 7.215100288391113
Epoch 70, val loss: 1.8364744186401367
Epoch 80, training loss: 15.803287506103516 = 1.822766900062561 + 2.0 * 6.990260124206543
Epoch 80, val loss: 1.8267579078674316
Epoch 90, training loss: 15.551277160644531 = 1.8102647066116333 + 2.0 * 6.870506286621094
Epoch 90, val loss: 1.8145514726638794
Epoch 100, training loss: 15.370341300964355 = 1.7972538471221924 + 2.0 * 6.786543846130371
Epoch 100, val loss: 1.8020232915878296
Epoch 110, training loss: 15.241127967834473 = 1.785256266593933 + 2.0 * 6.727935791015625
Epoch 110, val loss: 1.790450096130371
Epoch 120, training loss: 15.144047737121582 = 1.7738367319107056 + 2.0 * 6.685105323791504
Epoch 120, val loss: 1.7796176671981812
Epoch 130, training loss: 15.059956550598145 = 1.7622308731079102 + 2.0 * 6.648862838745117
Epoch 130, val loss: 1.7688190937042236
Epoch 140, training loss: 14.990022659301758 = 1.7499529123306274 + 2.0 * 6.620034694671631
Epoch 140, val loss: 1.757628321647644
Epoch 150, training loss: 14.927806854248047 = 1.7367318868637085 + 2.0 * 6.5955376625061035
Epoch 150, val loss: 1.7459529638290405
Epoch 160, training loss: 14.871305465698242 = 1.722296953201294 + 2.0 * 6.574504375457764
Epoch 160, val loss: 1.7333790063858032
Epoch 170, training loss: 14.829609870910645 = 1.706297516822815 + 2.0 * 6.5616559982299805
Epoch 170, val loss: 1.7196515798568726
Epoch 180, training loss: 14.768550872802734 = 1.6884995698928833 + 2.0 * 6.54002571105957
Epoch 180, val loss: 1.7045245170593262
Epoch 190, training loss: 14.719551086425781 = 1.6687577962875366 + 2.0 * 6.525396823883057
Epoch 190, val loss: 1.6878756284713745
Epoch 200, training loss: 14.673837661743164 = 1.6468456983566284 + 2.0 * 6.513495922088623
Epoch 200, val loss: 1.6695317029953003
Epoch 210, training loss: 14.625693321228027 = 1.6226482391357422 + 2.0 * 6.501522541046143
Epoch 210, val loss: 1.6494430303573608
Epoch 220, training loss: 14.57630729675293 = 1.596040964126587 + 2.0 * 6.490133285522461
Epoch 220, val loss: 1.6274763345718384
Epoch 230, training loss: 14.529220581054688 = 1.5670156478881836 + 2.0 * 6.481102466583252
Epoch 230, val loss: 1.6035974025726318
Epoch 240, training loss: 14.478673934936523 = 1.5355116128921509 + 2.0 * 6.471580982208252
Epoch 240, val loss: 1.5779603719711304
Epoch 250, training loss: 14.425331115722656 = 1.501682162284851 + 2.0 * 6.461824417114258
Epoch 250, val loss: 1.5506864786148071
Epoch 260, training loss: 14.381280899047852 = 1.4655321836471558 + 2.0 * 6.457874298095703
Epoch 260, val loss: 1.521711826324463
Epoch 270, training loss: 14.320878982543945 = 1.4275773763656616 + 2.0 * 6.446650981903076
Epoch 270, val loss: 1.4915695190429688
Epoch 280, training loss: 14.266697883605957 = 1.3880013227462769 + 2.0 * 6.439348220825195
Epoch 280, val loss: 1.4604958295822144
Epoch 290, training loss: 14.211843490600586 = 1.3469215631484985 + 2.0 * 6.432460784912109
Epoch 290, val loss: 1.4284554719924927
Epoch 300, training loss: 14.175950050354004 = 1.304744005203247 + 2.0 * 6.435603141784668
Epoch 300, val loss: 1.3958653211593628
Epoch 310, training loss: 14.109959602355957 = 1.2621623277664185 + 2.0 * 6.423898696899414
Epoch 310, val loss: 1.3634121417999268
Epoch 320, training loss: 14.050604820251465 = 1.219696283340454 + 2.0 * 6.415454387664795
Epoch 320, val loss: 1.3315743207931519
Epoch 330, training loss: 13.997908592224121 = 1.1774413585662842 + 2.0 * 6.410233497619629
Epoch 330, val loss: 1.3002586364746094
Epoch 340, training loss: 13.961347579956055 = 1.1357040405273438 + 2.0 * 6.4128217697143555
Epoch 340, val loss: 1.269758939743042
Epoch 350, training loss: 13.901205062866211 = 1.0951428413391113 + 2.0 * 6.403030872344971
Epoch 350, val loss: 1.240459680557251
Epoch 360, training loss: 13.850421905517578 = 1.0557808876037598 + 2.0 * 6.397320747375488
Epoch 360, val loss: 1.2126836776733398
Epoch 370, training loss: 13.804325103759766 = 1.017541527748108 + 2.0 * 6.3933916091918945
Epoch 370, val loss: 1.1859878301620483
Epoch 380, training loss: 13.758644104003906 = 0.9804122447967529 + 2.0 * 6.389115810394287
Epoch 380, val loss: 1.1604291200637817
Epoch 390, training loss: 13.72562313079834 = 0.9446591734886169 + 2.0 * 6.390481948852539
Epoch 390, val loss: 1.1362316608428955
Epoch 400, training loss: 13.67778205871582 = 0.910484790802002 + 2.0 * 6.383648872375488
Epoch 400, val loss: 1.1135761737823486
Epoch 410, training loss: 13.634478569030762 = 0.8775796294212341 + 2.0 * 6.378449440002441
Epoch 410, val loss: 1.092265248298645
Epoch 420, training loss: 13.596573829650879 = 0.8459342122077942 + 2.0 * 6.375319957733154
Epoch 420, val loss: 1.0721039772033691
Epoch 430, training loss: 13.562944412231445 = 0.8154769539833069 + 2.0 * 6.3737335205078125
Epoch 430, val loss: 1.0529966354370117
Epoch 440, training loss: 13.52790641784668 = 0.7864196300506592 + 2.0 * 6.370743274688721
Epoch 440, val loss: 1.0354098081588745
Epoch 450, training loss: 13.498564720153809 = 0.7586049437522888 + 2.0 * 6.3699798583984375
Epoch 450, val loss: 1.018957257270813
Epoch 460, training loss: 13.461339950561523 = 0.7320132255554199 + 2.0 * 6.364663124084473
Epoch 460, val loss: 1.0036431550979614
Epoch 470, training loss: 13.428258895874023 = 0.7064891457557678 + 2.0 * 6.360884666442871
Epoch 470, val loss: 0.9897204637527466
Epoch 480, training loss: 13.399198532104492 = 0.6818826198577881 + 2.0 * 6.3586578369140625
Epoch 480, val loss: 0.9767662882804871
Epoch 490, training loss: 13.378867149353027 = 0.658071756362915 + 2.0 * 6.360397815704346
Epoch 490, val loss: 0.9648091793060303
Epoch 500, training loss: 13.351500511169434 = 0.6351410746574402 + 2.0 * 6.358179569244385
Epoch 500, val loss: 0.9536676406860352
Epoch 510, training loss: 13.319906234741211 = 0.6129958033561707 + 2.0 * 6.353455066680908
Epoch 510, val loss: 0.9438623785972595
Epoch 520, training loss: 13.2908353805542 = 0.5915769338607788 + 2.0 * 6.3496294021606445
Epoch 520, val loss: 0.9348275661468506
Epoch 530, training loss: 13.275379180908203 = 0.5707080960273743 + 2.0 * 6.352335453033447
Epoch 530, val loss: 0.9262285232543945
Epoch 540, training loss: 13.251160621643066 = 0.550491452217102 + 2.0 * 6.350334644317627
Epoch 540, val loss: 0.9185087084770203
Epoch 550, training loss: 13.218473434448242 = 0.5308066010475159 + 2.0 * 6.3438334465026855
Epoch 550, val loss: 0.9115225076675415
Epoch 560, training loss: 13.195816040039062 = 0.5115587115287781 + 2.0 * 6.342128753662109
Epoch 560, val loss: 0.9051126837730408
Epoch 570, training loss: 13.175344467163086 = 0.49267956614494324 + 2.0 * 6.34133243560791
Epoch 570, val loss: 0.8992369174957275
Epoch 580, training loss: 13.15360164642334 = 0.47405871748924255 + 2.0 * 6.339771270751953
Epoch 580, val loss: 0.8933659791946411
Epoch 590, training loss: 13.1336030960083 = 0.45579972863197327 + 2.0 * 6.338901519775391
Epoch 590, val loss: 0.8884518146514893
Epoch 600, training loss: 13.110084533691406 = 0.4377986490726471 + 2.0 * 6.3361430168151855
Epoch 600, val loss: 0.8838646411895752
Epoch 610, training loss: 13.088651657104492 = 0.419964075088501 + 2.0 * 6.334343910217285
Epoch 610, val loss: 0.8793960213661194
Epoch 620, training loss: 13.066678047180176 = 0.4022487699985504 + 2.0 * 6.332214832305908
Epoch 620, val loss: 0.8750808835029602
Epoch 630, training loss: 13.04967212677002 = 0.3847534656524658 + 2.0 * 6.332459449768066
Epoch 630, val loss: 0.8713442087173462
Epoch 640, training loss: 13.02606201171875 = 0.36744117736816406 + 2.0 * 6.329310417175293
Epoch 640, val loss: 0.8678591251373291
Epoch 650, training loss: 13.022992134094238 = 0.3503233790397644 + 2.0 * 6.336334228515625
Epoch 650, val loss: 0.8645929098129272
Epoch 660, training loss: 12.987300872802734 = 0.33348214626312256 + 2.0 * 6.32690954208374
Epoch 660, val loss: 0.8615768551826477
Epoch 670, training loss: 12.97150707244873 = 0.3169536888599396 + 2.0 * 6.327276706695557
Epoch 670, val loss: 0.8589552640914917
Epoch 680, training loss: 12.951301574707031 = 0.3007446825504303 + 2.0 * 6.325278282165527
Epoch 680, val loss: 0.8565651178359985
Epoch 690, training loss: 12.93129825592041 = 0.284917950630188 + 2.0 * 6.323190212249756
Epoch 690, val loss: 0.8546223044395447
Epoch 700, training loss: 12.914286613464355 = 0.2695489823818207 + 2.0 * 6.322368621826172
Epoch 700, val loss: 0.853057861328125
Epoch 710, training loss: 12.917316436767578 = 0.254657506942749 + 2.0 * 6.331329345703125
Epoch 710, val loss: 0.8518664836883545
Epoch 720, training loss: 12.883752822875977 = 0.2404019981622696 + 2.0 * 6.3216753005981445
Epoch 720, val loss: 0.8510120511054993
Epoch 730, training loss: 12.864106178283691 = 0.22678020596504211 + 2.0 * 6.318663120269775
Epoch 730, val loss: 0.8508437871932983
Epoch 740, training loss: 12.847493171691895 = 0.21383574604988098 + 2.0 * 6.316828727722168
Epoch 740, val loss: 0.8508986234664917
Epoch 750, training loss: 12.85744571685791 = 0.2015744000673294 + 2.0 * 6.327935695648193
Epoch 750, val loss: 0.8515833616256714
Epoch 760, training loss: 12.826509475708008 = 0.19003546237945557 + 2.0 * 6.318236827850342
Epoch 760, val loss: 0.8524639010429382
Epoch 770, training loss: 12.809747695922852 = 0.17927689850330353 + 2.0 * 6.315235614776611
Epoch 770, val loss: 0.8541289567947388
Epoch 780, training loss: 12.793798446655273 = 0.169202983379364 + 2.0 * 6.312297821044922
Epoch 780, val loss: 0.856066882610321
Epoch 790, training loss: 12.78349494934082 = 0.1597728431224823 + 2.0 * 6.311861038208008
Epoch 790, val loss: 0.8584577441215515
Epoch 800, training loss: 12.791619300842285 = 0.15096808969974518 + 2.0 * 6.3203253746032715
Epoch 800, val loss: 0.8610239028930664
Epoch 810, training loss: 12.765531539916992 = 0.14278806746006012 + 2.0 * 6.311371803283691
Epoch 810, val loss: 0.8640841245651245
Epoch 820, training loss: 12.752328872680664 = 0.13518808782100677 + 2.0 * 6.308570384979248
Epoch 820, val loss: 0.8675069808959961
Epoch 830, training loss: 12.752388000488281 = 0.12811537086963654 + 2.0 * 6.312136173248291
Epoch 830, val loss: 0.8709937334060669
Epoch 840, training loss: 12.740503311157227 = 0.12151613086462021 + 2.0 * 6.309493541717529
Epoch 840, val loss: 0.8747231364250183
Epoch 850, training loss: 12.727978706359863 = 0.11537255346775055 + 2.0 * 6.306303024291992
Epoch 850, val loss: 0.8789171576499939
Epoch 860, training loss: 12.71866226196289 = 0.10963961482048035 + 2.0 * 6.304511547088623
Epoch 860, val loss: 0.8831884860992432
Epoch 870, training loss: 12.713188171386719 = 0.10427483171224594 + 2.0 * 6.30445671081543
Epoch 870, val loss: 0.8876018524169922
Epoch 880, training loss: 12.707605361938477 = 0.09925615787506104 + 2.0 * 6.304174423217773
Epoch 880, val loss: 0.892041802406311
Epoch 890, training loss: 12.702444076538086 = 0.09457231312990189 + 2.0 * 6.303936004638672
Epoch 890, val loss: 0.8968477845191956
Epoch 900, training loss: 12.693049430847168 = 0.09019497781991959 + 2.0 * 6.301427364349365
Epoch 900, val loss: 0.9016652703285217
Epoch 910, training loss: 12.68882942199707 = 0.08608974516391754 + 2.0 * 6.301369667053223
Epoch 910, val loss: 0.9065130949020386
Epoch 920, training loss: 12.691231727600098 = 0.08224157989025116 + 2.0 * 6.304494857788086
Epoch 920, val loss: 0.9114018082618713
Epoch 930, training loss: 12.677388191223145 = 0.07862219214439392 + 2.0 * 6.299383163452148
Epoch 930, val loss: 0.9164241552352905
Epoch 940, training loss: 12.671066284179688 = 0.07524765282869339 + 2.0 * 6.297909259796143
Epoch 940, val loss: 0.9215497374534607
Epoch 950, training loss: 12.672658920288086 = 0.07205504179000854 + 2.0 * 6.300302028656006
Epoch 950, val loss: 0.9265710711479187
Epoch 960, training loss: 12.660993576049805 = 0.0690525695681572 + 2.0 * 6.295970439910889
Epoch 960, val loss: 0.9318099617958069
Epoch 970, training loss: 12.657979011535645 = 0.06623488664627075 + 2.0 * 6.295872211456299
Epoch 970, val loss: 0.9370579123497009
Epoch 980, training loss: 12.65611457824707 = 0.0635627955198288 + 2.0 * 6.296276092529297
Epoch 980, val loss: 0.9422721862792969
Epoch 990, training loss: 12.652356147766113 = 0.061056338250637054 + 2.0 * 6.295650005340576
Epoch 990, val loss: 0.9475482106208801
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7592592592592593
0.8054823405376912
=== training gcn model ===
Epoch 0, training loss: 19.14307975769043 = 1.9494808912277222 + 2.0 * 8.596799850463867
Epoch 0, val loss: 1.9502685070037842
Epoch 10, training loss: 19.131929397583008 = 1.9390485286712646 + 2.0 * 8.596440315246582
Epoch 10, val loss: 1.9394011497497559
Epoch 20, training loss: 19.11417007446289 = 1.926302194595337 + 2.0 * 8.593934059143066
Epoch 20, val loss: 1.9260090589523315
Epoch 30, training loss: 19.058738708496094 = 1.9088200330734253 + 2.0 * 8.574959754943848
Epoch 30, val loss: 1.907838225364685
Epoch 40, training loss: 18.747880935668945 = 1.8868937492370605 + 2.0 * 8.430493354797363
Epoch 40, val loss: 1.886153221130371
Epoch 50, training loss: 17.313045501708984 = 1.865352988243103 + 2.0 * 7.723845958709717
Epoch 50, val loss: 1.866243600845337
Epoch 60, training loss: 16.558372497558594 = 1.8494173288345337 + 2.0 * 7.354477405548096
Epoch 60, val loss: 1.8513953685760498
Epoch 70, training loss: 16.082565307617188 = 1.8358769416809082 + 2.0 * 7.1233439445495605
Epoch 70, val loss: 1.8382426500320435
Epoch 80, training loss: 15.762749671936035 = 1.8235418796539307 + 2.0 * 6.969604015350342
Epoch 80, val loss: 1.8262132406234741
Epoch 90, training loss: 15.547122955322266 = 1.8121622800827026 + 2.0 * 6.867480278015137
Epoch 90, val loss: 1.8153407573699951
Epoch 100, training loss: 15.395065307617188 = 1.801081895828247 + 2.0 * 6.79699182510376
Epoch 100, val loss: 1.8049628734588623
Epoch 110, training loss: 15.25659465789795 = 1.7901180982589722 + 2.0 * 6.733238220214844
Epoch 110, val loss: 1.7945678234100342
Epoch 120, training loss: 15.151972770690918 = 1.7794067859649658 + 2.0 * 6.686283111572266
Epoch 120, val loss: 1.784609079360962
Epoch 130, training loss: 15.07250690460205 = 1.7686793804168701 + 2.0 * 6.651913642883301
Epoch 130, val loss: 1.7747838497161865
Epoch 140, training loss: 14.994279861450195 = 1.757500410079956 + 2.0 * 6.61838960647583
Epoch 140, val loss: 1.7648842334747314
Epoch 150, training loss: 14.927037239074707 = 1.7454880475997925 + 2.0 * 6.5907745361328125
Epoch 150, val loss: 1.7546406984329224
Epoch 160, training loss: 14.866950035095215 = 1.7323553562164307 + 2.0 * 6.567297458648682
Epoch 160, val loss: 1.7437412738800049
Epoch 170, training loss: 14.81096363067627 = 1.7176525592803955 + 2.0 * 6.546655654907227
Epoch 170, val loss: 1.7317206859588623
Epoch 180, training loss: 14.756021499633789 = 1.701079249382019 + 2.0 * 6.52747106552124
Epoch 180, val loss: 1.7182319164276123
Epoch 190, training loss: 14.707124710083008 = 1.6822552680969238 + 2.0 * 6.512434959411621
Epoch 190, val loss: 1.7029975652694702
Epoch 200, training loss: 14.658793449401855 = 1.6609574556350708 + 2.0 * 6.498918056488037
Epoch 200, val loss: 1.6858309507369995
Epoch 210, training loss: 14.60920238494873 = 1.6368290185928345 + 2.0 * 6.486186504364014
Epoch 210, val loss: 1.6665924787521362
Epoch 220, training loss: 14.559907913208008 = 1.6096829175949097 + 2.0 * 6.475112438201904
Epoch 220, val loss: 1.6447895765304565
Epoch 230, training loss: 14.50968074798584 = 1.5791699886322021 + 2.0 * 6.465255260467529
Epoch 230, val loss: 1.6204344034194946
Epoch 240, training loss: 14.47421932220459 = 1.545167088508606 + 2.0 * 6.464526176452637
Epoch 240, val loss: 1.5933358669281006
Epoch 250, training loss: 14.405342102050781 = 1.5083695650100708 + 2.0 * 6.448486328125
Epoch 250, val loss: 1.564137578010559
Epoch 260, training loss: 14.352286338806152 = 1.4688851833343506 + 2.0 * 6.441700458526611
Epoch 260, val loss: 1.5329891443252563
Epoch 270, training loss: 14.295821189880371 = 1.4268721342086792 + 2.0 * 6.434474468231201
Epoch 270, val loss: 1.500006079673767
Epoch 280, training loss: 14.250044822692871 = 1.3825918436050415 + 2.0 * 6.4337263107299805
Epoch 280, val loss: 1.4657031297683716
Epoch 290, training loss: 14.188176155090332 = 1.3376929759979248 + 2.0 * 6.425241470336914
Epoch 290, val loss: 1.4310811758041382
Epoch 300, training loss: 14.127059936523438 = 1.2925289869308472 + 2.0 * 6.41726541519165
Epoch 300, val loss: 1.3967373371124268
Epoch 310, training loss: 14.07052230834961 = 1.2471383810043335 + 2.0 * 6.411692142486572
Epoch 310, val loss: 1.3628807067871094
Epoch 320, training loss: 14.017251968383789 = 1.2018555402755737 + 2.0 * 6.407698154449463
Epoch 320, val loss: 1.3296555280685425
Epoch 330, training loss: 13.966416358947754 = 1.157920479774475 + 2.0 * 6.404247760772705
Epoch 330, val loss: 1.2980717420578003
Epoch 340, training loss: 13.913978576660156 = 1.1156022548675537 + 2.0 * 6.399188041687012
Epoch 340, val loss: 1.2684053182601929
Epoch 350, training loss: 13.862421035766602 = 1.0745991468429565 + 2.0 * 6.393910884857178
Epoch 350, val loss: 1.2402175664901733
Epoch 360, training loss: 13.815385818481445 = 1.0351026058197021 + 2.0 * 6.390141487121582
Epoch 360, val loss: 1.2135732173919678
Epoch 370, training loss: 13.771803855895996 = 0.9972070455551147 + 2.0 * 6.387298583984375
Epoch 370, val loss: 1.1885234117507935
Epoch 380, training loss: 13.725093841552734 = 0.9606607556343079 + 2.0 * 6.382216453552246
Epoch 380, val loss: 1.164769172668457
Epoch 390, training loss: 13.695002555847168 = 0.9252180457115173 + 2.0 * 6.384892463684082
Epoch 390, val loss: 1.142063856124878
Epoch 400, training loss: 13.642049789428711 = 0.8910926580429077 + 2.0 * 6.375478744506836
Epoch 400, val loss: 1.1207175254821777
Epoch 410, training loss: 13.602365493774414 = 0.8582506775856018 + 2.0 * 6.3720574378967285
Epoch 410, val loss: 1.1006050109863281
Epoch 420, training loss: 13.565650939941406 = 0.826384961605072 + 2.0 * 6.369633197784424
Epoch 420, val loss: 1.0814110040664673
Epoch 430, training loss: 13.540691375732422 = 0.7956212162971497 + 2.0 * 6.372535228729248
Epoch 430, val loss: 1.0633212327957153
Epoch 440, training loss: 13.502734184265137 = 0.7660565972328186 + 2.0 * 6.368338584899902
Epoch 440, val loss: 1.0464228391647339
Epoch 450, training loss: 13.4617919921875 = 0.7376209497451782 + 2.0 * 6.362085342407227
Epoch 450, val loss: 1.0307637453079224
Epoch 460, training loss: 13.426816940307617 = 0.7100867033004761 + 2.0 * 6.358365058898926
Epoch 460, val loss: 1.0161993503570557
Epoch 470, training loss: 13.397507667541504 = 0.6833049654960632 + 2.0 * 6.3571014404296875
Epoch 470, val loss: 1.0024930238723755
Epoch 480, training loss: 13.37428092956543 = 0.6572791337966919 + 2.0 * 6.358500957489014
Epoch 480, val loss: 0.989743709564209
Epoch 490, training loss: 13.336338996887207 = 0.6323469877243042 + 2.0 * 6.351995944976807
Epoch 490, val loss: 0.9781281352043152
Epoch 500, training loss: 13.309074401855469 = 0.6081658601760864 + 2.0 * 6.350454330444336
Epoch 500, val loss: 0.967338502407074
Epoch 510, training loss: 13.277853965759277 = 0.5846938490867615 + 2.0 * 6.3465800285339355
Epoch 510, val loss: 0.9572476744651794
Epoch 520, training loss: 13.252342224121094 = 0.5619354248046875 + 2.0 * 6.345203399658203
Epoch 520, val loss: 0.9482965469360352
Epoch 530, training loss: 13.229371070861816 = 0.5397228598594666 + 2.0 * 6.344824314117432
Epoch 530, val loss: 0.939939558506012
Epoch 540, training loss: 13.201876640319824 = 0.5181575417518616 + 2.0 * 6.341859340667725
Epoch 540, val loss: 0.9321845173835754
Epoch 550, training loss: 13.175874710083008 = 0.497031569480896 + 2.0 * 6.33942174911499
Epoch 550, val loss: 0.9251710176467896
Epoch 560, training loss: 13.167115211486816 = 0.47644945979118347 + 2.0 * 6.345333099365234
Epoch 560, val loss: 0.9187828898429871
Epoch 570, training loss: 13.125041961669922 = 0.4563696086406708 + 2.0 * 6.334336280822754
Epoch 570, val loss: 0.9130890369415283
Epoch 580, training loss: 13.102787017822266 = 0.4367711544036865 + 2.0 * 6.3330078125
Epoch 580, val loss: 0.9079447984695435
Epoch 590, training loss: 13.086753845214844 = 0.41760721802711487 + 2.0 * 6.334573268890381
Epoch 590, val loss: 0.903351366519928
Epoch 600, training loss: 13.066341400146484 = 0.3989993929862976 + 2.0 * 6.3336710929870605
Epoch 600, val loss: 0.8993442058563232
Epoch 610, training loss: 13.038826942443848 = 0.38088172674179077 + 2.0 * 6.328972816467285
Epoch 610, val loss: 0.896088182926178
Epoch 620, training loss: 13.015218734741211 = 0.36329421401023865 + 2.0 * 6.325962066650391
Epoch 620, val loss: 0.8933444023132324
Epoch 630, training loss: 13.007450103759766 = 0.3462125062942505 + 2.0 * 6.330618858337402
Epoch 630, val loss: 0.8911128044128418
Epoch 640, training loss: 12.989127159118652 = 0.32988712191581726 + 2.0 * 6.329619884490967
Epoch 640, val loss: 0.8895497918128967
Epoch 650, training loss: 12.95846939086914 = 0.3141436278820038 + 2.0 * 6.322163105010986
Epoch 650, val loss: 0.8886953592300415
Epoch 660, training loss: 12.948436737060547 = 0.29899686574935913 + 2.0 * 6.3247199058532715
Epoch 660, val loss: 0.888256847858429
Epoch 670, training loss: 12.925271034240723 = 0.28444764018058777 + 2.0 * 6.320411682128906
Epoch 670, val loss: 0.8885051608085632
Epoch 680, training loss: 12.909512519836426 = 0.2704234719276428 + 2.0 * 6.319544315338135
Epoch 680, val loss: 0.8891839981079102
Epoch 690, training loss: 12.890737533569336 = 0.2570122182369232 + 2.0 * 6.3168625831604
Epoch 690, val loss: 0.8902190923690796
Epoch 700, training loss: 12.876782417297363 = 0.24418112635612488 + 2.0 * 6.316300868988037
Epoch 700, val loss: 0.8918572068214417
Epoch 710, training loss: 12.858757972717285 = 0.2319420576095581 + 2.0 * 6.313407897949219
Epoch 710, val loss: 0.8938407897949219
Epoch 720, training loss: 12.84475040435791 = 0.22020024061203003 + 2.0 * 6.312274932861328
Epoch 720, val loss: 0.8962434530258179
Epoch 730, training loss: 12.844826698303223 = 0.2089998573064804 + 2.0 * 6.31791353225708
Epoch 730, val loss: 0.8988437056541443
Epoch 740, training loss: 12.823762893676758 = 0.1983506679534912 + 2.0 * 6.312705993652344
Epoch 740, val loss: 0.9017399549484253
Epoch 750, training loss: 12.805856704711914 = 0.18826031684875488 + 2.0 * 6.308798313140869
Epoch 750, val loss: 0.9049720168113708
Epoch 760, training loss: 12.795616149902344 = 0.17866230010986328 + 2.0 * 6.30847692489624
Epoch 760, val loss: 0.9084230661392212
Epoch 770, training loss: 12.79018497467041 = 0.1695532351732254 + 2.0 * 6.31031608581543
Epoch 770, val loss: 0.912075400352478
Epoch 780, training loss: 12.777047157287598 = 0.16093645989894867 + 2.0 * 6.308055400848389
Epoch 780, val loss: 0.9159530997276306
Epoch 790, training loss: 12.762126922607422 = 0.15281260013580322 + 2.0 * 6.304656982421875
Epoch 790, val loss: 0.92019122838974
Epoch 800, training loss: 12.759284019470215 = 0.1451289802789688 + 2.0 * 6.307077407836914
Epoch 800, val loss: 0.9244948029518127
Epoch 810, training loss: 12.74514389038086 = 0.13790728151798248 + 2.0 * 6.303618431091309
Epoch 810, val loss: 0.9289730787277222
Epoch 820, training loss: 12.735485076904297 = 0.13111022114753723 + 2.0 * 6.302187442779541
Epoch 820, val loss: 0.9337238669395447
Epoch 830, training loss: 12.724332809448242 = 0.12469815462827682 + 2.0 * 6.2998175621032715
Epoch 830, val loss: 0.9385616183280945
Epoch 840, training loss: 12.717145919799805 = 0.1186474934220314 + 2.0 * 6.299249172210693
Epoch 840, val loss: 0.9435656070709229
Epoch 850, training loss: 12.717290878295898 = 0.11294496804475784 + 2.0 * 6.302173137664795
Epoch 850, val loss: 0.9485958814620972
Epoch 860, training loss: 12.71218490600586 = 0.10758735984563828 + 2.0 * 6.302298545837402
Epoch 860, val loss: 0.953913152217865
Epoch 870, training loss: 12.697772026062012 = 0.10253377258777618 + 2.0 * 6.297619342803955
Epoch 870, val loss: 0.9590008854866028
Epoch 880, training loss: 12.687664031982422 = 0.09778399020433426 + 2.0 * 6.294939994812012
Epoch 880, val loss: 0.9645203948020935
Epoch 890, training loss: 12.683354377746582 = 0.09330728650093079 + 2.0 * 6.295023441314697
Epoch 890, val loss: 0.9700239300727844
Epoch 900, training loss: 12.68950366973877 = 0.08910568058490753 + 2.0 * 6.300199031829834
Epoch 900, val loss: 0.9753832817077637
Epoch 910, training loss: 12.670836448669434 = 0.085138700902462 + 2.0 * 6.292849063873291
Epoch 910, val loss: 0.9808826446533203
Epoch 920, training loss: 12.665422439575195 = 0.08141559362411499 + 2.0 * 6.292003631591797
Epoch 920, val loss: 0.986541211605072
Epoch 930, training loss: 12.659618377685547 = 0.07789413630962372 + 2.0 * 6.290862083435059
Epoch 930, val loss: 0.9921634197235107
Epoch 940, training loss: 12.665487289428711 = 0.07457074522972107 + 2.0 * 6.2954583168029785
Epoch 940, val loss: 0.9976596236228943
Epoch 950, training loss: 12.651656150817871 = 0.07144864648580551 + 2.0 * 6.290103912353516
Epoch 950, val loss: 1.00349760055542
Epoch 960, training loss: 12.647852897644043 = 0.06848981231451035 + 2.0 * 6.289681434631348
Epoch 960, val loss: 1.0090973377227783
Epoch 970, training loss: 12.642267227172852 = 0.06570293009281158 + 2.0 * 6.2882819175720215
Epoch 970, val loss: 1.0146260261535645
Epoch 980, training loss: 12.634794235229492 = 0.06306380033493042 + 2.0 * 6.285865306854248
Epoch 980, val loss: 1.0203458070755005
Epoch 990, training loss: 12.631206512451172 = 0.060567550361156464 + 2.0 * 6.2853193283081055
Epoch 990, val loss: 1.0259608030319214
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.737037037037037
0.79388508170796
The final CL Acc:0.74691, 0.00924, The final GNN Acc:0.80109, 0.00513
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13236])
remove edge: torch.Size([2, 7912])
updated graph: torch.Size([2, 10592])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 19.139169692993164 = 1.9455205202102661 + 2.0 * 8.596824645996094
Epoch 0, val loss: 1.9430103302001953
Epoch 10, training loss: 19.1275577545166 = 1.9346989393234253 + 2.0 * 8.596429824829102
Epoch 10, val loss: 1.9325095415115356
Epoch 20, training loss: 19.107938766479492 = 1.921225666999817 + 2.0 * 8.593356132507324
Epoch 20, val loss: 1.918946385383606
Epoch 30, training loss: 19.042118072509766 = 1.902875304222107 + 2.0 * 8.569621086120605
Epoch 30, val loss: 1.900217890739441
Epoch 40, training loss: 18.679773330688477 = 1.8807369470596313 + 2.0 * 8.399518013000488
Epoch 40, val loss: 1.87861967086792
Epoch 50, training loss: 17.616147994995117 = 1.858125925064087 + 2.0 * 7.8790106773376465
Epoch 50, val loss: 1.857352614402771
Epoch 60, training loss: 16.69367790222168 = 1.8416332006454468 + 2.0 * 7.426022052764893
Epoch 60, val loss: 1.8418816328048706
Epoch 70, training loss: 16.06363868713379 = 1.8292121887207031 + 2.0 * 7.117213249206543
Epoch 70, val loss: 1.8299392461776733
Epoch 80, training loss: 15.758383750915527 = 1.8169599771499634 + 2.0 * 6.970711708068848
Epoch 80, val loss: 1.8181145191192627
Epoch 90, training loss: 15.537757873535156 = 1.8030110597610474 + 2.0 * 6.867373466491699
Epoch 90, val loss: 1.8048079013824463
Epoch 100, training loss: 15.369379997253418 = 1.7894479036331177 + 2.0 * 6.789966106414795
Epoch 100, val loss: 1.791630744934082
Epoch 110, training loss: 15.247016906738281 = 1.7766051292419434 + 2.0 * 6.73520565032959
Epoch 110, val loss: 1.7789421081542969
Epoch 120, training loss: 15.145126342773438 = 1.7640628814697266 + 2.0 * 6.6905317306518555
Epoch 120, val loss: 1.7666840553283691
Epoch 130, training loss: 15.04730224609375 = 1.7514740228652954 + 2.0 * 6.647913932800293
Epoch 130, val loss: 1.7543903589248657
Epoch 140, training loss: 14.964381217956543 = 1.7382456064224243 + 2.0 * 6.613067626953125
Epoch 140, val loss: 1.741729736328125
Epoch 150, training loss: 14.884523391723633 = 1.7238428592681885 + 2.0 * 6.580340385437012
Epoch 150, val loss: 1.7280811071395874
Epoch 160, training loss: 14.816741943359375 = 1.7076423168182373 + 2.0 * 6.554549694061279
Epoch 160, val loss: 1.7128196954727173
Epoch 170, training loss: 14.756546020507812 = 1.6892600059509277 + 2.0 * 6.533642768859863
Epoch 170, val loss: 1.6957366466522217
Epoch 180, training loss: 14.695235252380371 = 1.6686394214630127 + 2.0 * 6.513298034667969
Epoch 180, val loss: 1.6766127347946167
Epoch 190, training loss: 14.639464378356934 = 1.6454380750656128 + 2.0 * 6.497013092041016
Epoch 190, val loss: 1.655251383781433
Epoch 200, training loss: 14.584583282470703 = 1.6191656589508057 + 2.0 * 6.482708930969238
Epoch 200, val loss: 1.631269931793213
Epoch 210, training loss: 14.536401748657227 = 1.5896247625350952 + 2.0 * 6.473388671875
Epoch 210, val loss: 1.6046391725540161
Epoch 220, training loss: 14.478826522827148 = 1.5570690631866455 + 2.0 * 6.460878849029541
Epoch 220, val loss: 1.5755865573883057
Epoch 230, training loss: 14.421930313110352 = 1.5215197801589966 + 2.0 * 6.450205326080322
Epoch 230, val loss: 1.5443642139434814
Epoch 240, training loss: 14.365185737609863 = 1.4829498529434204 + 2.0 * 6.441117763519287
Epoch 240, val loss: 1.5110392570495605
Epoch 250, training loss: 14.315001487731934 = 1.4416160583496094 + 2.0 * 6.436692714691162
Epoch 250, val loss: 1.4757599830627441
Epoch 260, training loss: 14.251691818237305 = 1.398059606552124 + 2.0 * 6.426815986633301
Epoch 260, val loss: 1.4392645359039307
Epoch 270, training loss: 14.191149711608887 = 1.3529622554779053 + 2.0 * 6.419093608856201
Epoch 270, val loss: 1.4019420146942139
Epoch 280, training loss: 14.139835357666016 = 1.3065842390060425 + 2.0 * 6.416625499725342
Epoch 280, val loss: 1.364135503768921
Epoch 290, training loss: 14.074884414672852 = 1.2597860097885132 + 2.0 * 6.4075493812561035
Epoch 290, val loss: 1.3264334201812744
Epoch 300, training loss: 14.015298843383789 = 1.2127528190612793 + 2.0 * 6.401273250579834
Epoch 300, val loss: 1.2890312671661377
Epoch 310, training loss: 13.964529991149902 = 1.1656672954559326 + 2.0 * 6.399431228637695
Epoch 310, val loss: 1.2518244981765747
Epoch 320, training loss: 13.903526306152344 = 1.1190736293792725 + 2.0 * 6.392226219177246
Epoch 320, val loss: 1.215374231338501
Epoch 330, training loss: 13.847087860107422 = 1.073388695716858 + 2.0 * 6.386849403381348
Epoch 330, val loss: 1.1798731088638306
Epoch 340, training loss: 13.797659873962402 = 1.0285416841506958 + 2.0 * 6.384559154510498
Epoch 340, val loss: 1.1453930139541626
Epoch 350, training loss: 13.745399475097656 = 0.9850800037384033 + 2.0 * 6.380159854888916
Epoch 350, val loss: 1.1121766567230225
Epoch 360, training loss: 13.693623542785645 = 0.9432032704353333 + 2.0 * 6.375210285186768
Epoch 360, val loss: 1.0806010961532593
Epoch 370, training loss: 13.648491859436035 = 0.9029980897903442 + 2.0 * 6.37274694442749
Epoch 370, val loss: 1.050646185874939
Epoch 380, training loss: 13.60243034362793 = 0.8646656274795532 + 2.0 * 6.368882179260254
Epoch 380, val loss: 1.022681474685669
Epoch 390, training loss: 13.558335304260254 = 0.8285635709762573 + 2.0 * 6.3648858070373535
Epoch 390, val loss: 0.996640682220459
Epoch 400, training loss: 13.517118453979492 = 0.7944046854972839 + 2.0 * 6.361356735229492
Epoch 400, val loss: 0.9725916981697083
Epoch 410, training loss: 13.481562614440918 = 0.7620656490325928 + 2.0 * 6.359748363494873
Epoch 410, val loss: 0.9503712058067322
Epoch 420, training loss: 13.441166877746582 = 0.7315018773078918 + 2.0 * 6.354832649230957
Epoch 420, val loss: 0.9299845099449158
Epoch 430, training loss: 13.417455673217773 = 0.7025850415229797 + 2.0 * 6.35743522644043
Epoch 430, val loss: 0.9114441871643066
Epoch 440, training loss: 13.375975608825684 = 0.6751474738121033 + 2.0 * 6.350414276123047
Epoch 440, val loss: 0.8943907022476196
Epoch 450, training loss: 13.340429306030273 = 0.6488244533538818 + 2.0 * 6.345802307128906
Epoch 450, val loss: 0.8788472414016724
Epoch 460, training loss: 13.31517505645752 = 0.6235092878341675 + 2.0 * 6.345832824707031
Epoch 460, val loss: 0.8646690249443054
Epoch 470, training loss: 13.294571876525879 = 0.5993825793266296 + 2.0 * 6.347594738006592
Epoch 470, val loss: 0.8518170118331909
Epoch 480, training loss: 13.25790786743164 = 0.5764160752296448 + 2.0 * 6.34074592590332
Epoch 480, val loss: 0.8402321338653564
Epoch 490, training loss: 13.22831916809082 = 0.554364025592804 + 2.0 * 6.336977481842041
Epoch 490, val loss: 0.8298364877700806
Epoch 500, training loss: 13.207977294921875 = 0.5330788493156433 + 2.0 * 6.337449073791504
Epoch 500, val loss: 0.8204285502433777
Epoch 510, training loss: 13.188943862915039 = 0.5126556754112244 + 2.0 * 6.338144302368164
Epoch 510, val loss: 0.811975359916687
Epoch 520, training loss: 13.15726375579834 = 0.49313247203826904 + 2.0 * 6.332065582275391
Epoch 520, val loss: 0.804534375667572
Epoch 530, training loss: 13.131705284118652 = 0.4743059575557709 + 2.0 * 6.328699588775635
Epoch 530, val loss: 0.7979524731636047
Epoch 540, training loss: 13.127586364746094 = 0.45609861612319946 + 2.0 * 6.3357439041137695
Epoch 540, val loss: 0.7922260165214539
Epoch 550, training loss: 13.090902328491211 = 0.4388299286365509 + 2.0 * 6.326035976409912
Epoch 550, val loss: 0.7872352004051208
Epoch 560, training loss: 13.069372177124023 = 0.42213571071624756 + 2.0 * 6.323618412017822
Epoch 560, val loss: 0.7829890251159668
Epoch 570, training loss: 13.047394752502441 = 0.4059848487377167 + 2.0 * 6.320704936981201
Epoch 570, val loss: 0.7794092893600464
Epoch 580, training loss: 13.03676986694336 = 0.39033764600753784 + 2.0 * 6.323215961456299
Epoch 580, val loss: 0.776351809501648
Epoch 590, training loss: 13.020952224731445 = 0.37526383996009827 + 2.0 * 6.3228440284729
Epoch 590, val loss: 0.7737564444541931
Epoch 600, training loss: 12.99638557434082 = 0.36070001125335693 + 2.0 * 6.317842960357666
Epoch 600, val loss: 0.7716897130012512
Epoch 610, training loss: 12.976807594299316 = 0.3466005027294159 + 2.0 * 6.315103530883789
Epoch 610, val loss: 0.7700731158256531
Epoch 620, training loss: 12.970769882202148 = 0.3328818082809448 + 2.0 * 6.318943977355957
Epoch 620, val loss: 0.7688148021697998
Epoch 630, training loss: 12.945283889770508 = 0.3196617662906647 + 2.0 * 6.312810897827148
Epoch 630, val loss: 0.7679697275161743
Epoch 640, training loss: 12.92669677734375 = 0.3068305552005768 + 2.0 * 6.309933185577393
Epoch 640, val loss: 0.7675678133964539
Epoch 650, training loss: 12.913971900939941 = 0.29430702328681946 + 2.0 * 6.309832572937012
Epoch 650, val loss: 0.7675326466560364
Epoch 660, training loss: 12.90030574798584 = 0.28216320276260376 + 2.0 * 6.309071063995361
Epoch 660, val loss: 0.7676918506622314
Epoch 670, training loss: 12.884452819824219 = 0.27039745450019836 + 2.0 * 6.307027816772461
Epoch 670, val loss: 0.768254816532135
Epoch 680, training loss: 12.86926555633545 = 0.2590050995349884 + 2.0 * 6.3051300048828125
Epoch 680, val loss: 0.769254207611084
Epoch 690, training loss: 12.867319107055664 = 0.24793033301830292 + 2.0 * 6.309694290161133
Epoch 690, val loss: 0.7706125378608704
Epoch 700, training loss: 12.84581184387207 = 0.2372460514307022 + 2.0 * 6.3042826652526855
Epoch 700, val loss: 0.772266149520874
Epoch 710, training loss: 12.831076622009277 = 0.226890429854393 + 2.0 * 6.302093029022217
Epoch 710, val loss: 0.7743237614631653
Epoch 720, training loss: 12.8306245803833 = 0.21691957116127014 + 2.0 * 6.306852340698242
Epoch 720, val loss: 0.7767705321311951
Epoch 730, training loss: 12.812750816345215 = 0.20731279253959656 + 2.0 * 6.3027191162109375
Epoch 730, val loss: 0.7794129252433777
Epoch 740, training loss: 12.793707847595215 = 0.1981375515460968 + 2.0 * 6.29778528213501
Epoch 740, val loss: 0.7824944853782654
Epoch 750, training loss: 12.782870292663574 = 0.18933038413524628 + 2.0 * 6.296770095825195
Epoch 750, val loss: 0.7858990430831909
Epoch 760, training loss: 12.793899536132812 = 0.18088145554065704 + 2.0 * 6.306509017944336
Epoch 760, val loss: 0.7895329594612122
Epoch 770, training loss: 12.765427589416504 = 0.1728840470314026 + 2.0 * 6.296271800994873
Epoch 770, val loss: 0.7933464050292969
Epoch 780, training loss: 12.755830764770508 = 0.16524958610534668 + 2.0 * 6.295290470123291
Epoch 780, val loss: 0.7973819375038147
Epoch 790, training loss: 12.75271224975586 = 0.15797917544841766 + 2.0 * 6.297366619110107
Epoch 790, val loss: 0.8016971945762634
Epoch 800, training loss: 12.740845680236816 = 0.1510714739561081 + 2.0 * 6.294887065887451
Epoch 800, val loss: 0.8063598871231079
Epoch 810, training loss: 12.73209285736084 = 0.14450062811374664 + 2.0 * 6.293796062469482
Epoch 810, val loss: 0.8109864592552185
Epoch 820, training loss: 12.72450065612793 = 0.13828687369823456 + 2.0 * 6.293107032775879
Epoch 820, val loss: 0.8158596754074097
Epoch 830, training loss: 12.712600708007812 = 0.13236519694328308 + 2.0 * 6.2901177406311035
Epoch 830, val loss: 0.8208467960357666
Epoch 840, training loss: 12.70399284362793 = 0.12674200534820557 + 2.0 * 6.288625240325928
Epoch 840, val loss: 0.8260974287986755
Epoch 850, training loss: 12.705036163330078 = 0.12137991935014725 + 2.0 * 6.291828155517578
Epoch 850, val loss: 0.8313652873039246
Epoch 860, training loss: 12.696991920471191 = 0.11628072708845139 + 2.0 * 6.290355682373047
Epoch 860, val loss: 0.836704432964325
Epoch 870, training loss: 12.682762145996094 = 0.11144974827766418 + 2.0 * 6.285655975341797
Epoch 870, val loss: 0.8421233892440796
Epoch 880, training loss: 12.679949760437012 = 0.1068616434931755 + 2.0 * 6.286543846130371
Epoch 880, val loss: 0.847747802734375
Epoch 890, training loss: 12.677431106567383 = 0.10249309986829758 + 2.0 * 6.287468910217285
Epoch 890, val loss: 0.8532747626304626
Epoch 900, training loss: 12.6673002243042 = 0.09832935035228729 + 2.0 * 6.284485340118408
Epoch 900, val loss: 0.8588981628417969
Epoch 910, training loss: 12.6624116897583 = 0.09438171982765198 + 2.0 * 6.28401517868042
Epoch 910, val loss: 0.8647001385688782
Epoch 920, training loss: 12.658132553100586 = 0.09062527120113373 + 2.0 * 6.283753871917725
Epoch 920, val loss: 0.8703796863555908
Epoch 930, training loss: 12.652505874633789 = 0.0870579332113266 + 2.0 * 6.282723903656006
Epoch 930, val loss: 0.8763081431388855
Epoch 940, training loss: 12.649194717407227 = 0.08364908397197723 + 2.0 * 6.282773017883301
Epoch 940, val loss: 0.8819195628166199
Epoch 950, training loss: 12.642998695373535 = 0.0804087296128273 + 2.0 * 6.281294822692871
Epoch 950, val loss: 0.8878198862075806
Epoch 960, training loss: 12.635729789733887 = 0.07732954621315002 + 2.0 * 6.279200077056885
Epoch 960, val loss: 0.893460214138031
Epoch 970, training loss: 12.630537033081055 = 0.07439983636140823 + 2.0 * 6.278068542480469
Epoch 970, val loss: 0.8994054794311523
Epoch 980, training loss: 12.635355949401855 = 0.07160282135009766 + 2.0 * 6.281876564025879
Epoch 980, val loss: 0.9050735831260681
Epoch 990, training loss: 12.627330780029297 = 0.06893906742334366 + 2.0 * 6.279195785522461
Epoch 990, val loss: 0.9109730124473572
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7666666666666667
0.8318397469688983
=== training gcn model ===
Epoch 0, training loss: 19.148286819458008 = 1.9546514749526978 + 2.0 * 8.596817970275879
Epoch 0, val loss: 1.9583868980407715
Epoch 10, training loss: 19.137746810913086 = 1.944884181022644 + 2.0 * 8.596431732177734
Epoch 10, val loss: 1.9490941762924194
Epoch 20, training loss: 19.119369506835938 = 1.9328815937042236 + 2.0 * 8.593243598937988
Epoch 20, val loss: 1.937333345413208
Epoch 30, training loss: 19.057126998901367 = 1.9164055585861206 + 2.0 * 8.570361137390137
Epoch 30, val loss: 1.920796275138855
Epoch 40, training loss: 18.7675838470459 = 1.895569920539856 + 2.0 * 8.436006546020508
Epoch 40, val loss: 1.9005038738250732
Epoch 50, training loss: 17.924436569213867 = 1.8710474967956543 + 2.0 * 8.026694297790527
Epoch 50, val loss: 1.8766847848892212
Epoch 60, training loss: 17.128681182861328 = 1.849757194519043 + 2.0 * 7.639461994171143
Epoch 60, val loss: 1.857023000717163
Epoch 70, training loss: 16.31574249267578 = 1.8355787992477417 + 2.0 * 7.240082263946533
Epoch 70, val loss: 1.843765139579773
Epoch 80, training loss: 15.770082473754883 = 1.821439504623413 + 2.0 * 6.974321365356445
Epoch 80, val loss: 1.8298825025558472
Epoch 90, training loss: 15.522982597351074 = 1.8034714460372925 + 2.0 * 6.859755516052246
Epoch 90, val loss: 1.8123397827148438
Epoch 100, training loss: 15.356498718261719 = 1.7848294973373413 + 2.0 * 6.785834789276123
Epoch 100, val loss: 1.7947437763214111
Epoch 110, training loss: 15.214655876159668 = 1.7679270505905151 + 2.0 * 6.723364353179932
Epoch 110, val loss: 1.7788336277008057
Epoch 120, training loss: 15.105097770690918 = 1.7517441511154175 + 2.0 * 6.6766767501831055
Epoch 120, val loss: 1.7632418870925903
Epoch 130, training loss: 15.010507583618164 = 1.7346017360687256 + 2.0 * 6.63795280456543
Epoch 130, val loss: 1.7467749118804932
Epoch 140, training loss: 14.925525665283203 = 1.7160534858703613 + 2.0 * 6.604735851287842
Epoch 140, val loss: 1.7292888164520264
Epoch 150, training loss: 14.851142883300781 = 1.6958078145980835 + 2.0 * 6.577667713165283
Epoch 150, val loss: 1.7106987237930298
Epoch 160, training loss: 14.780830383300781 = 1.673588752746582 + 2.0 * 6.5536208152771
Epoch 160, val loss: 1.6906213760375977
Epoch 170, training loss: 14.715726852416992 = 1.6491941213607788 + 2.0 * 6.533266544342041
Epoch 170, val loss: 1.6687679290771484
Epoch 180, training loss: 14.651572227478027 = 1.6221210956573486 + 2.0 * 6.514725685119629
Epoch 180, val loss: 1.644747018814087
Epoch 190, training loss: 14.596999168395996 = 1.5921550989151 + 2.0 * 6.502421855926514
Epoch 190, val loss: 1.618503212928772
Epoch 200, training loss: 14.531584739685059 = 1.5596590042114258 + 2.0 * 6.485962867736816
Epoch 200, val loss: 1.5902024507522583
Epoch 210, training loss: 14.472174644470215 = 1.5244189500808716 + 2.0 * 6.473877906799316
Epoch 210, val loss: 1.5598808526992798
Epoch 220, training loss: 14.41739273071289 = 1.4865500926971436 + 2.0 * 6.465421199798584
Epoch 220, val loss: 1.5276546478271484
Epoch 230, training loss: 14.352987289428711 = 1.446579098701477 + 2.0 * 6.453204154968262
Epoch 230, val loss: 1.4940211772918701
Epoch 240, training loss: 14.295475006103516 = 1.4048819541931152 + 2.0 * 6.445296287536621
Epoch 240, val loss: 1.4593299627304077
Epoch 250, training loss: 14.23538875579834 = 1.3619182109832764 + 2.0 * 6.436735153198242
Epoch 250, val loss: 1.4239338636398315
Epoch 260, training loss: 14.176558494567871 = 1.3178671598434448 + 2.0 * 6.429345607757568
Epoch 260, val loss: 1.3880659341812134
Epoch 270, training loss: 14.121638298034668 = 1.2733880281448364 + 2.0 * 6.4241251945495605
Epoch 270, val loss: 1.3522920608520508
Epoch 280, training loss: 14.06572151184082 = 1.2292510271072388 + 2.0 * 6.4182353019714355
Epoch 280, val loss: 1.3171579837799072
Epoch 290, training loss: 14.008997917175293 = 1.1857773065567017 + 2.0 * 6.411610126495361
Epoch 290, val loss: 1.2828810214996338
Epoch 300, training loss: 13.955997467041016 = 1.1432247161865234 + 2.0 * 6.406386375427246
Epoch 300, val loss: 1.2494641542434692
Epoch 310, training loss: 13.909025192260742 = 1.1016780138015747 + 2.0 * 6.4036736488342285
Epoch 310, val loss: 1.2171140909194946
Epoch 320, training loss: 13.854039192199707 = 1.0616639852523804 + 2.0 * 6.396187782287598
Epoch 320, val loss: 1.1861258745193481
Epoch 330, training loss: 13.806675910949707 = 1.0229543447494507 + 2.0 * 6.3918609619140625
Epoch 330, val loss: 1.1563999652862549
Epoch 340, training loss: 13.760190963745117 = 0.9853285551071167 + 2.0 * 6.3874311447143555
Epoch 340, val loss: 1.1276570558547974
Epoch 350, training loss: 13.721153259277344 = 0.9489536285400391 + 2.0 * 6.386099815368652
Epoch 350, val loss: 1.099758267402649
Epoch 360, training loss: 13.675902366638184 = 0.9137362837791443 + 2.0 * 6.381083011627197
Epoch 360, val loss: 1.0730254650115967
Epoch 370, training loss: 13.63329792022705 = 0.8795214295387268 + 2.0 * 6.376888275146484
Epoch 370, val loss: 1.0472071170806885
Epoch 380, training loss: 13.591800689697266 = 0.8464125394821167 + 2.0 * 6.37269401550293
Epoch 380, val loss: 1.0222982168197632
Epoch 390, training loss: 13.556833267211914 = 0.8145163655281067 + 2.0 * 6.371158599853516
Epoch 390, val loss: 0.9985626339912415
Epoch 400, training loss: 13.523591041564941 = 0.7838432192802429 + 2.0 * 6.369874000549316
Epoch 400, val loss: 0.9759994149208069
Epoch 410, training loss: 13.483850479125977 = 0.7545555830001831 + 2.0 * 6.364647388458252
Epoch 410, val loss: 0.954612135887146
Epoch 420, training loss: 13.448572158813477 = 0.7263942956924438 + 2.0 * 6.361088752746582
Epoch 420, val loss: 0.9345178604125977
Epoch 430, training loss: 13.43411636352539 = 0.6994274854660034 + 2.0 * 6.367344379425049
Epoch 430, val loss: 0.9155986905097961
Epoch 440, training loss: 13.388457298278809 = 0.6737224459648132 + 2.0 * 6.357367515563965
Epoch 440, val loss: 0.8979207277297974
Epoch 450, training loss: 13.357522964477539 = 0.649069607257843 + 2.0 * 6.354226589202881
Epoch 450, val loss: 0.8814360499382019
Epoch 460, training loss: 13.331379890441895 = 0.6252565383911133 + 2.0 * 6.353061676025391
Epoch 460, val loss: 0.8658829927444458
Epoch 470, training loss: 13.3090238571167 = 0.6021891236305237 + 2.0 * 6.35341739654541
Epoch 470, val loss: 0.851329505443573
Epoch 480, training loss: 13.273863792419434 = 0.5798971056938171 + 2.0 * 6.346983432769775
Epoch 480, val loss: 0.8376601338386536
Epoch 490, training loss: 13.25129222869873 = 0.5581376552581787 + 2.0 * 6.346577167510986
Epoch 490, val loss: 0.8248115181922913
Epoch 500, training loss: 13.228336334228516 = 0.5369210243225098 + 2.0 * 6.345707416534424
Epoch 500, val loss: 0.8127464652061462
Epoch 510, training loss: 13.198254585266113 = 0.5162061452865601 + 2.0 * 6.341024398803711
Epoch 510, val loss: 0.8014848232269287
Epoch 520, training loss: 13.174734115600586 = 0.4959123432636261 + 2.0 * 6.339410781860352
Epoch 520, val loss: 0.7908755540847778
Epoch 530, training loss: 13.155903816223145 = 0.4760190546512604 + 2.0 * 6.339942455291748
Epoch 530, val loss: 0.7809481024742126
Epoch 540, training loss: 13.130804061889648 = 0.456569641828537 + 2.0 * 6.3371171951293945
Epoch 540, val loss: 0.7716887593269348
Epoch 550, training loss: 13.104736328125 = 0.4374963343143463 + 2.0 * 6.333620071411133
Epoch 550, val loss: 0.7631372809410095
Epoch 560, training loss: 13.084809303283691 = 0.4188637137413025 + 2.0 * 6.332973003387451
Epoch 560, val loss: 0.7551766037940979
Epoch 570, training loss: 13.061498641967773 = 0.40066367387771606 + 2.0 * 6.330417633056641
Epoch 570, val loss: 0.7477785348892212
Epoch 580, training loss: 13.049200057983398 = 0.38288235664367676 + 2.0 * 6.33315896987915
Epoch 580, val loss: 0.7409670948982239
Epoch 590, training loss: 13.01987361907959 = 0.36558863520622253 + 2.0 * 6.327142715454102
Epoch 590, val loss: 0.7346926927566528
Epoch 600, training loss: 13.00055980682373 = 0.34877219796180725 + 2.0 * 6.325893878936768
Epoch 600, val loss: 0.7289658784866333
Epoch 610, training loss: 12.985573768615723 = 0.3324422836303711 + 2.0 * 6.326565742492676
Epoch 610, val loss: 0.7236387133598328
Epoch 620, training loss: 12.961431503295898 = 0.3166103661060333 + 2.0 * 6.322410583496094
Epoch 620, val loss: 0.7188137769699097
Epoch 630, training loss: 12.94228458404541 = 0.3012515604496002 + 2.0 * 6.320516586303711
Epoch 630, val loss: 0.7143919467926025
Epoch 640, training loss: 12.930108070373535 = 0.2863333225250244 + 2.0 * 6.321887493133545
Epoch 640, val loss: 0.7103213667869568
Epoch 650, training loss: 12.912771224975586 = 0.27189576625823975 + 2.0 * 6.320437908172607
Epoch 650, val loss: 0.7066859006881714
Epoch 660, training loss: 12.89449691772461 = 0.25796380639076233 + 2.0 * 6.31826639175415
Epoch 660, val loss: 0.7035130262374878
Epoch 670, training loss: 12.875085830688477 = 0.24456651508808136 + 2.0 * 6.3152594566345215
Epoch 670, val loss: 0.7007558941841125
Epoch 680, training loss: 12.859353065490723 = 0.2316506803035736 + 2.0 * 6.313851356506348
Epoch 680, val loss: 0.6984804272651672
Epoch 690, training loss: 12.848694801330566 = 0.21920561790466309 + 2.0 * 6.314744472503662
Epoch 690, val loss: 0.6966693997383118
Epoch 700, training loss: 12.829739570617676 = 0.20724356174468994 + 2.0 * 6.311247825622559
Epoch 700, val loss: 0.6953114867210388
Epoch 710, training loss: 12.81697940826416 = 0.1958521157503128 + 2.0 * 6.310563564300537
Epoch 710, val loss: 0.6944586038589478
Epoch 720, training loss: 12.813331604003906 = 0.18499284982681274 + 2.0 * 6.314169406890869
Epoch 720, val loss: 0.6940894722938538
Epoch 730, training loss: 12.792091369628906 = 0.17471091449260712 + 2.0 * 6.308690071105957
Epoch 730, val loss: 0.6942614912986755
Epoch 740, training loss: 12.77788257598877 = 0.16503475606441498 + 2.0 * 6.306424140930176
Epoch 740, val loss: 0.6949119567871094
Epoch 750, training loss: 12.7832612991333 = 0.15591663122177124 + 2.0 * 6.3136725425720215
Epoch 750, val loss: 0.6960678696632385
Epoch 760, training loss: 12.758058547973633 = 0.14738845825195312 + 2.0 * 6.30533504486084
Epoch 760, val loss: 0.6976275444030762
Epoch 770, training loss: 12.744043350219727 = 0.1394052803516388 + 2.0 * 6.302319049835205
Epoch 770, val loss: 0.6997005939483643
Epoch 780, training loss: 12.73928165435791 = 0.13192057609558105 + 2.0 * 6.303680419921875
Epoch 780, val loss: 0.7021161317825317
Epoch 790, training loss: 12.728070259094238 = 0.12494295835494995 + 2.0 * 6.301563739776611
Epoch 790, val loss: 0.7048534154891968
Epoch 800, training loss: 12.719084739685059 = 0.11843428015708923 + 2.0 * 6.300325393676758
Epoch 800, val loss: 0.7080029249191284
Epoch 810, training loss: 12.711075782775879 = 0.11237804591655731 + 2.0 * 6.299348831176758
Epoch 810, val loss: 0.7114400267601013
Epoch 820, training loss: 12.705897331237793 = 0.10672080516815186 + 2.0 * 6.299588203430176
Epoch 820, val loss: 0.7151188850402832
Epoch 830, training loss: 12.699078559875488 = 0.10144250094890594 + 2.0 * 6.298818111419678
Epoch 830, val loss: 0.7191041707992554
Epoch 840, training loss: 12.687003135681152 = 0.09651081264019012 + 2.0 * 6.295246124267578
Epoch 840, val loss: 0.7232702374458313
Epoch 850, training loss: 12.695718765258789 = 0.09190385043621063 + 2.0 * 6.301907539367676
Epoch 850, val loss: 0.7276172041893005
Epoch 860, training loss: 12.675029754638672 = 0.08760370314121246 + 2.0 * 6.293713092803955
Epoch 860, val loss: 0.7320786118507385
Epoch 870, training loss: 12.668122291564941 = 0.08358008414506912 + 2.0 * 6.292271137237549
Epoch 870, val loss: 0.7367597222328186
Epoch 880, training loss: 12.662175178527832 = 0.07979230582714081 + 2.0 * 6.291191577911377
Epoch 880, val loss: 0.741493284702301
Epoch 890, training loss: 12.67309284210205 = 0.07623586803674698 + 2.0 * 6.298428535461426
Epoch 890, val loss: 0.7463063597679138
Epoch 900, training loss: 12.661551475524902 = 0.07289499789476395 + 2.0 * 6.294328212738037
Epoch 900, val loss: 0.7512975335121155
Epoch 910, training loss: 12.647743225097656 = 0.06976588070392609 + 2.0 * 6.2889885902404785
Epoch 910, val loss: 0.7563148140907288
Epoch 920, training loss: 12.643545150756836 = 0.0668296068906784 + 2.0 * 6.288357734680176
Epoch 920, val loss: 0.7613645792007446
Epoch 930, training loss: 12.63779354095459 = 0.06405588984489441 + 2.0 * 6.286869049072266
Epoch 930, val loss: 0.7664463520050049
Epoch 940, training loss: 12.643779754638672 = 0.061432596296072006 + 2.0 * 6.291173458099365
Epoch 940, val loss: 0.7715646028518677
Epoch 950, training loss: 12.637482643127441 = 0.05896542966365814 + 2.0 * 6.2892584800720215
Epoch 950, val loss: 0.7767736911773682
Epoch 960, training loss: 12.630297660827637 = 0.05663047730922699 + 2.0 * 6.286833763122559
Epoch 960, val loss: 0.7819470763206482
Epoch 970, training loss: 12.623655319213867 = 0.05443720892071724 + 2.0 * 6.284608840942383
Epoch 970, val loss: 0.787097156047821
Epoch 980, training loss: 12.625076293945312 = 0.052366144955158234 + 2.0 * 6.286355018615723
Epoch 980, val loss: 0.7922804951667786
Epoch 990, training loss: 12.61403751373291 = 0.05039523169398308 + 2.0 * 6.281821250915527
Epoch 990, val loss: 0.7974780201911926
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8185185185185185
0.8365840801265156
=== training gcn model ===
Epoch 0, training loss: 19.13651466369629 = 1.9428271055221558 + 2.0 * 8.596843719482422
Epoch 0, val loss: 1.9406238794326782
Epoch 10, training loss: 19.126317977905273 = 1.9333243370056152 + 2.0 * 8.59649658203125
Epoch 10, val loss: 1.9312355518341064
Epoch 20, training loss: 19.10916519165039 = 1.92139732837677 + 2.0 * 8.593883514404297
Epoch 20, val loss: 1.919015645980835
Epoch 30, training loss: 19.049972534179688 = 1.9054875373840332 + 2.0 * 8.572242736816406
Epoch 30, val loss: 1.9023823738098145
Epoch 40, training loss: 18.691186904907227 = 1.886425256729126 + 2.0 * 8.40238094329834
Epoch 40, val loss: 1.8830116987228394
Epoch 50, training loss: 17.438949584960938 = 1.8661860227584839 + 2.0 * 7.78638219833374
Epoch 50, val loss: 1.8631566762924194
Epoch 60, training loss: 16.581466674804688 = 1.8505806922912598 + 2.0 * 7.365442752838135
Epoch 60, val loss: 1.84844970703125
Epoch 70, training loss: 16.015676498413086 = 1.837504506111145 + 2.0 * 7.089085578918457
Epoch 70, val loss: 1.8363866806030273
Epoch 80, training loss: 15.668377876281738 = 1.8247880935668945 + 2.0 * 6.921794891357422
Epoch 80, val loss: 1.8247390985488892
Epoch 90, training loss: 15.456015586853027 = 1.8106235265731812 + 2.0 * 6.822696208953857
Epoch 90, val loss: 1.8111711740493774
Epoch 100, training loss: 15.291345596313477 = 1.7961220741271973 + 2.0 * 6.747611999511719
Epoch 100, val loss: 1.7970740795135498
Epoch 110, training loss: 15.17347240447998 = 1.7826745510101318 + 2.0 * 6.695398807525635
Epoch 110, val loss: 1.7840747833251953
Epoch 120, training loss: 15.08354663848877 = 1.7695568799972534 + 2.0 * 6.656994819641113
Epoch 120, val loss: 1.7718639373779297
Epoch 130, training loss: 15.004789352416992 = 1.7561564445495605 + 2.0 * 6.624316692352295
Epoch 130, val loss: 1.75957453250885
Epoch 140, training loss: 14.93574333190918 = 1.7420635223388672 + 2.0 * 6.596839904785156
Epoch 140, val loss: 1.74679696559906
Epoch 150, training loss: 14.872708320617676 = 1.7268034219741821 + 2.0 * 6.5729522705078125
Epoch 150, val loss: 1.732973575592041
Epoch 160, training loss: 14.817649841308594 = 1.709855556488037 + 2.0 * 6.553896903991699
Epoch 160, val loss: 1.7177155017852783
Epoch 170, training loss: 14.768181800842285 = 1.6908702850341797 + 2.0 * 6.538655757904053
Epoch 170, val loss: 1.7007287740707397
Epoch 180, training loss: 14.719646453857422 = 1.669540286064148 + 2.0 * 6.525053024291992
Epoch 180, val loss: 1.68180513381958
Epoch 190, training loss: 14.666800498962402 = 1.645743489265442 + 2.0 * 6.510528564453125
Epoch 190, val loss: 1.6608506441116333
Epoch 200, training loss: 14.617326736450195 = 1.6190931797027588 + 2.0 * 6.499116897583008
Epoch 200, val loss: 1.6375221014022827
Epoch 210, training loss: 14.563535690307617 = 1.5895028114318848 + 2.0 * 6.487016677856445
Epoch 210, val loss: 1.6118050813674927
Epoch 220, training loss: 14.508177757263184 = 1.5565017461776733 + 2.0 * 6.4758381843566895
Epoch 220, val loss: 1.5833467245101929
Epoch 230, training loss: 14.462679862976074 = 1.5198663473129272 + 2.0 * 6.471406936645508
Epoch 230, val loss: 1.5518386363983154
Epoch 240, training loss: 14.400672912597656 = 1.4800467491149902 + 2.0 * 6.460312843322754
Epoch 240, val loss: 1.5179551839828491
Epoch 250, training loss: 14.337644577026367 = 1.437377691268921 + 2.0 * 6.450133323669434
Epoch 250, val loss: 1.4818453788757324
Epoch 260, training loss: 14.276443481445312 = 1.391939401626587 + 2.0 * 6.442252159118652
Epoch 260, val loss: 1.4436028003692627
Epoch 270, training loss: 14.217818260192871 = 1.344024658203125 + 2.0 * 6.436896800994873
Epoch 270, val loss: 1.4036345481872559
Epoch 280, training loss: 14.158487319946289 = 1.2946193218231201 + 2.0 * 6.431933879852295
Epoch 280, val loss: 1.3626716136932373
Epoch 290, training loss: 14.0917329788208 = 1.2447116374969482 + 2.0 * 6.423510551452637
Epoch 290, val loss: 1.3217169046401978
Epoch 300, training loss: 14.031388282775879 = 1.1945180892944336 + 2.0 * 6.418435096740723
Epoch 300, val loss: 1.2809678316116333
Epoch 310, training loss: 13.977546691894531 = 1.1448373794555664 + 2.0 * 6.416354656219482
Epoch 310, val loss: 1.240692138671875
Epoch 320, training loss: 13.91374683380127 = 1.0961129665374756 + 2.0 * 6.408816814422607
Epoch 320, val loss: 1.2017366886138916
Epoch 330, training loss: 13.85510540008545 = 1.048503041267395 + 2.0 * 6.403301239013672
Epoch 330, val loss: 1.1640516519546509
Epoch 340, training loss: 13.809889793395996 = 1.002066731452942 + 2.0 * 6.403911590576172
Epoch 340, val loss: 1.1276323795318604
Epoch 350, training loss: 13.74853515625 = 0.9575009346008301 + 2.0 * 6.395516872406006
Epoch 350, val loss: 1.0928454399108887
Epoch 360, training loss: 13.700735092163086 = 0.914935827255249 + 2.0 * 6.392899513244629
Epoch 360, val loss: 1.0599570274353027
Epoch 370, training loss: 13.649140357971191 = 0.8739837408065796 + 2.0 * 6.38757848739624
Epoch 370, val loss: 1.0286288261413574
Epoch 380, training loss: 13.604686737060547 = 0.8344952464103699 + 2.0 * 6.385095596313477
Epoch 380, val loss: 0.9986407160758972
Epoch 390, training loss: 13.568787574768066 = 0.7967370748519897 + 2.0 * 6.386025428771973
Epoch 390, val loss: 0.9704428315162659
Epoch 400, training loss: 13.519179344177246 = 0.7611042857170105 + 2.0 * 6.379037380218506
Epoch 400, val loss: 0.9441114068031311
Epoch 410, training loss: 13.477550506591797 = 0.7272927761077881 + 2.0 * 6.375128746032715
Epoch 410, val loss: 0.9195274710655212
Epoch 420, training loss: 13.451803207397461 = 0.6951378583908081 + 2.0 * 6.378332614898682
Epoch 420, val loss: 0.8965158462524414
Epoch 430, training loss: 13.407429695129395 = 0.6646282076835632 + 2.0 * 6.371400833129883
Epoch 430, val loss: 0.8750801086425781
Epoch 440, training loss: 13.371745109558105 = 0.6358217597007751 + 2.0 * 6.367961883544922
Epoch 440, val loss: 0.8554173111915588
Epoch 450, training loss: 13.343777656555176 = 0.6083632707595825 + 2.0 * 6.367707252502441
Epoch 450, val loss: 0.8370493650436401
Epoch 460, training loss: 13.306662559509277 = 0.5821371078491211 + 2.0 * 6.362262725830078
Epoch 460, val loss: 0.8199876546859741
Epoch 470, training loss: 13.276972770690918 = 0.5570153594017029 + 2.0 * 6.359978675842285
Epoch 470, val loss: 0.8039413094520569
Epoch 480, training loss: 13.248449325561523 = 0.5329652428627014 + 2.0 * 6.357741832733154
Epoch 480, val loss: 0.7889705300331116
Epoch 490, training loss: 13.220012664794922 = 0.5100246071815491 + 2.0 * 6.35499382019043
Epoch 490, val loss: 0.7750630974769592
Epoch 500, training loss: 13.19276237487793 = 0.488023966550827 + 2.0 * 6.35236930847168
Epoch 500, val loss: 0.7621007561683655
Epoch 510, training loss: 13.186416625976562 = 0.4669371545314789 + 2.0 * 6.359739780426025
Epoch 510, val loss: 0.7498845458030701
Epoch 520, training loss: 13.145001411437988 = 0.44685062766075134 + 2.0 * 6.3490753173828125
Epoch 520, val loss: 0.7386757135391235
Epoch 530, training loss: 13.11947250366211 = 0.42772287130355835 + 2.0 * 6.345874786376953
Epoch 530, val loss: 0.7283970713615417
Epoch 540, training loss: 13.099059104919434 = 0.409464031457901 + 2.0 * 6.344797611236572
Epoch 540, val loss: 0.7188717126846313
Epoch 550, training loss: 13.07949447631836 = 0.39210695028305054 + 2.0 * 6.343693733215332
Epoch 550, val loss: 0.7100630402565002
Epoch 560, training loss: 13.05624008178711 = 0.3757680058479309 + 2.0 * 6.340236186981201
Epoch 560, val loss: 0.7022027969360352
Epoch 570, training loss: 13.035652160644531 = 0.36028432846069336 + 2.0 * 6.337684154510498
Epoch 570, val loss: 0.6950843334197998
Epoch 580, training loss: 13.017498970031738 = 0.3455764949321747 + 2.0 * 6.33596134185791
Epoch 580, val loss: 0.688570499420166
Epoch 590, training loss: 13.00053882598877 = 0.3316129148006439 + 2.0 * 6.334463119506836
Epoch 590, val loss: 0.6826473474502563
Epoch 600, training loss: 12.989622116088867 = 0.31845080852508545 + 2.0 * 6.335585594177246
Epoch 600, val loss: 0.6773613691329956
Epoch 610, training loss: 12.969587326049805 = 0.30613118410110474 + 2.0 * 6.331727981567383
Epoch 610, val loss: 0.6727592349052429
Epoch 620, training loss: 12.954769134521484 = 0.29456156492233276 + 2.0 * 6.330103874206543
Epoch 620, val loss: 0.6687262654304504
Epoch 630, training loss: 12.939019203186035 = 0.2836185395717621 + 2.0 * 6.327700138092041
Epoch 630, val loss: 0.6652058959007263
Epoch 640, training loss: 12.930508613586426 = 0.2732284665107727 + 2.0 * 6.328639984130859
Epoch 640, val loss: 0.6620883941650391
Epoch 650, training loss: 12.919179916381836 = 0.26347237825393677 + 2.0 * 6.327853679656982
Epoch 650, val loss: 0.6595357060432434
Epoch 660, training loss: 12.902630805969238 = 0.2541849911212921 + 2.0 * 6.324223041534424
Epoch 660, val loss: 0.6572775840759277
Epoch 670, training loss: 12.8887357711792 = 0.24540188908576965 + 2.0 * 6.321666717529297
Epoch 670, val loss: 0.655484139919281
Epoch 680, training loss: 12.888045310974121 = 0.23700422048568726 + 2.0 * 6.3255205154418945
Epoch 680, val loss: 0.6540747880935669
Epoch 690, training loss: 12.873664855957031 = 0.22893041372299194 + 2.0 * 6.322367191314697
Epoch 690, val loss: 0.6526924967765808
Epoch 700, training loss: 12.855326652526855 = 0.22119572758674622 + 2.0 * 6.317065238952637
Epoch 700, val loss: 0.6517468690872192
Epoch 710, training loss: 12.844972610473633 = 0.21370388567447662 + 2.0 * 6.315634250640869
Epoch 710, val loss: 0.6510047912597656
Epoch 720, training loss: 12.85194206237793 = 0.20644447207450867 + 2.0 * 6.32274866104126
Epoch 720, val loss: 0.6504018306732178
Epoch 730, training loss: 12.83054256439209 = 0.19934238493442535 + 2.0 * 6.3155999183654785
Epoch 730, val loss: 0.649950385093689
Epoch 740, training loss: 12.815281867980957 = 0.1924629807472229 + 2.0 * 6.3114094734191895
Epoch 740, val loss: 0.6496918201446533
Epoch 750, training loss: 12.80635929107666 = 0.18570703268051147 + 2.0 * 6.310326099395752
Epoch 750, val loss: 0.6494867205619812
Epoch 760, training loss: 12.847347259521484 = 0.1791146993637085 + 2.0 * 6.334116458892822
Epoch 760, val loss: 0.6495183110237122
Epoch 770, training loss: 12.797402381896973 = 0.17257912456989288 + 2.0 * 6.312411785125732
Epoch 770, val loss: 0.6493387818336487
Epoch 780, training loss: 12.783334732055664 = 0.1662450134754181 + 2.0 * 6.308544635772705
Epoch 780, val loss: 0.6493829488754272
Epoch 790, training loss: 12.77214527130127 = 0.1600266844034195 + 2.0 * 6.30605936050415
Epoch 790, val loss: 0.6496073603630066
Epoch 800, training loss: 12.763416290283203 = 0.153905987739563 + 2.0 * 6.304755210876465
Epoch 800, val loss: 0.6498620510101318
Epoch 810, training loss: 12.7698974609375 = 0.14790642261505127 + 2.0 * 6.310995578765869
Epoch 810, val loss: 0.6501885652542114
Epoch 820, training loss: 12.746719360351562 = 0.14206728339195251 + 2.0 * 6.302326202392578
Epoch 820, val loss: 0.6506072282791138
Epoch 830, training loss: 12.740160942077637 = 0.1363944113254547 + 2.0 * 6.301883220672607
Epoch 830, val loss: 0.6512190103530884
Epoch 840, training loss: 12.730910301208496 = 0.13086968660354614 + 2.0 * 6.300020217895508
Epoch 840, val loss: 0.6518855094909668
Epoch 850, training loss: 12.748124122619629 = 0.12550483644008636 + 2.0 * 6.311309814453125
Epoch 850, val loss: 0.6526449918746948
Epoch 860, training loss: 12.719461441040039 = 0.12039713561534882 + 2.0 * 6.299531936645508
Epoch 860, val loss: 0.6535685658454895
Epoch 870, training loss: 12.710427284240723 = 0.11545214056968689 + 2.0 * 6.297487735748291
Epoch 870, val loss: 0.6545773148536682
Epoch 880, training loss: 12.703600883483887 = 0.11070963740348816 + 2.0 * 6.296445846557617
Epoch 880, val loss: 0.6557719707489014
Epoch 890, training loss: 12.728267669677734 = 0.10616809129714966 + 2.0 * 6.311049938201904
Epoch 890, val loss: 0.6571447253227234
Epoch 900, training loss: 12.692151069641113 = 0.10185753554105759 + 2.0 * 6.295146942138672
Epoch 900, val loss: 0.658519983291626
Epoch 910, training loss: 12.686074256896973 = 0.09773953258991241 + 2.0 * 6.294167518615723
Epoch 910, val loss: 0.6600127816200256
Epoch 920, training loss: 12.679572105407715 = 0.09381639212369919 + 2.0 * 6.292877674102783
Epoch 920, val loss: 0.6617613434791565
Epoch 930, training loss: 12.693351745605469 = 0.09008441120386124 + 2.0 * 6.301633834838867
Epoch 930, val loss: 0.6636961102485657
Epoch 940, training loss: 12.676591873168945 = 0.08651477843523026 + 2.0 * 6.29503870010376
Epoch 940, val loss: 0.6653950214385986
Epoch 950, training loss: 12.663415908813477 = 0.0831453949213028 + 2.0 * 6.290135383605957
Epoch 950, val loss: 0.6673685312271118
Epoch 960, training loss: 12.670472145080566 = 0.07994890958070755 + 2.0 * 6.295261383056641
Epoch 960, val loss: 0.6694774627685547
Epoch 970, training loss: 12.655712127685547 = 0.07687202095985413 + 2.0 * 6.289420127868652
Epoch 970, val loss: 0.671582818031311
Epoch 980, training loss: 12.648833274841309 = 0.07398315519094467 + 2.0 * 6.2874250411987305
Epoch 980, val loss: 0.6736935973167419
Epoch 990, training loss: 12.644012451171875 = 0.07121694087982178 + 2.0 * 6.286397933959961
Epoch 990, val loss: 0.6759417057037354
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.837037037037037
0.8429098576700054
The final CL Acc:0.80741, 0.02978, The final GNN Acc:0.83711, 0.00453
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.5 der2:0.2 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11570])
remove edge: torch.Size([2, 9474])
updated graph: torch.Size([2, 10488])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 19.129840850830078 = 1.9362366199493408 + 2.0 * 8.5968017578125
Epoch 0, val loss: 1.9285472631454468
Epoch 10, training loss: 19.118717193603516 = 1.9260200262069702 + 2.0 * 8.596348762512207
Epoch 10, val loss: 1.9190549850463867
Epoch 20, training loss: 19.099369049072266 = 1.9129259586334229 + 2.0 * 8.593221664428711
Epoch 20, val loss: 1.9066566228866577
Epoch 30, training loss: 19.04062843322754 = 1.89453125 + 2.0 * 8.57304859161377
Epoch 30, val loss: 1.8891346454620361
Epoch 40, training loss: 18.79067611694336 = 1.8725805282592773 + 2.0 * 8.4590482711792
Epoch 40, val loss: 1.8690301179885864
Epoch 50, training loss: 17.877931594848633 = 1.8487091064453125 + 2.0 * 8.01461124420166
Epoch 50, val loss: 1.8473951816558838
Epoch 60, training loss: 16.964523315429688 = 1.831166386604309 + 2.0 * 7.566678524017334
Epoch 60, val loss: 1.8327441215515137
Epoch 70, training loss: 16.316524505615234 = 1.820637822151184 + 2.0 * 7.247942924499512
Epoch 70, val loss: 1.8231087923049927
Epoch 80, training loss: 15.873310089111328 = 1.8107120990753174 + 2.0 * 7.031299114227295
Epoch 80, val loss: 1.8143199682235718
Epoch 90, training loss: 15.655389785766602 = 1.7996934652328491 + 2.0 * 6.9278483390808105
Epoch 90, val loss: 1.8047345876693726
Epoch 100, training loss: 15.452938079833984 = 1.7874245643615723 + 2.0 * 6.832756519317627
Epoch 100, val loss: 1.7948229312896729
Epoch 110, training loss: 15.293004035949707 = 1.7766669988632202 + 2.0 * 6.758168697357178
Epoch 110, val loss: 1.7861295938491821
Epoch 120, training loss: 15.16257095336914 = 1.766381859779358 + 2.0 * 6.698094367980957
Epoch 120, val loss: 1.7771042585372925
Epoch 130, training loss: 15.070626258850098 = 1.7547603845596313 + 2.0 * 6.657932758331299
Epoch 130, val loss: 1.7668907642364502
Epoch 140, training loss: 14.98624324798584 = 1.7418063879013062 + 2.0 * 6.622218608856201
Epoch 140, val loss: 1.7556145191192627
Epoch 150, training loss: 14.921303749084473 = 1.7274938821792603 + 2.0 * 6.596904754638672
Epoch 150, val loss: 1.7433921098709106
Epoch 160, training loss: 14.853887557983398 = 1.7114547491073608 + 2.0 * 6.571216583251953
Epoch 160, val loss: 1.7301067113876343
Epoch 170, training loss: 14.795339584350586 = 1.6934775114059448 + 2.0 * 6.550930976867676
Epoch 170, val loss: 1.715524673461914
Epoch 180, training loss: 14.738920211791992 = 1.6731486320495605 + 2.0 * 6.532885551452637
Epoch 180, val loss: 1.6991301774978638
Epoch 190, training loss: 14.690389633178711 = 1.6502097845077515 + 2.0 * 6.520090103149414
Epoch 190, val loss: 1.680743932723999
Epoch 200, training loss: 14.635358810424805 = 1.6248749494552612 + 2.0 * 6.505241870880127
Epoch 200, val loss: 1.660429835319519
Epoch 210, training loss: 14.581974029541016 = 1.5973485708236694 + 2.0 * 6.492312908172607
Epoch 210, val loss: 1.6383804082870483
Epoch 220, training loss: 14.529064178466797 = 1.5674389600753784 + 2.0 * 6.4808125495910645
Epoch 220, val loss: 1.6144921779632568
Epoch 230, training loss: 14.479594230651855 = 1.5353652238845825 + 2.0 * 6.472114562988281
Epoch 230, val loss: 1.5889090299606323
Epoch 240, training loss: 14.424278259277344 = 1.5018348693847656 + 2.0 * 6.461221694946289
Epoch 240, val loss: 1.5621836185455322
Epoch 250, training loss: 14.372554779052734 = 1.467469334602356 + 2.0 * 6.452542781829834
Epoch 250, val loss: 1.5349446535110474
Epoch 260, training loss: 14.32685375213623 = 1.432616949081421 + 2.0 * 6.447118282318115
Epoch 260, val loss: 1.507587194442749
Epoch 270, training loss: 14.273728370666504 = 1.3980000019073486 + 2.0 * 6.437864303588867
Epoch 270, val loss: 1.4805375337600708
Epoch 280, training loss: 14.223345756530762 = 1.363782286643982 + 2.0 * 6.429781913757324
Epoch 280, val loss: 1.4540350437164307
Epoch 290, training loss: 14.185182571411133 = 1.3298707008361816 + 2.0 * 6.4276556968688965
Epoch 290, val loss: 1.4280908107757568
Epoch 300, training loss: 14.133461952209473 = 1.2966043949127197 + 2.0 * 6.418428897857666
Epoch 300, val loss: 1.402857780456543
Epoch 310, training loss: 14.090873718261719 = 1.2638622522354126 + 2.0 * 6.413505554199219
Epoch 310, val loss: 1.378464937210083
Epoch 320, training loss: 14.045553207397461 = 1.2316230535507202 + 2.0 * 6.406965255737305
Epoch 320, val loss: 1.3547272682189941
Epoch 330, training loss: 14.003461837768555 = 1.1994385719299316 + 2.0 * 6.402011871337891
Epoch 330, val loss: 1.3315054178237915
Epoch 340, training loss: 13.964838981628418 = 1.1672931909561157 + 2.0 * 6.398772716522217
Epoch 340, val loss: 1.3085553646087646
Epoch 350, training loss: 13.920825958251953 = 1.1351876258850098 + 2.0 * 6.392818927764893
Epoch 350, val loss: 1.2859299182891846
Epoch 360, training loss: 13.88039493560791 = 1.1026995182037354 + 2.0 * 6.388847827911377
Epoch 360, val loss: 1.2633849382400513
Epoch 370, training loss: 13.846944808959961 = 1.069775104522705 + 2.0 * 6.388584613800049
Epoch 370, val loss: 1.2408086061477661
Epoch 380, training loss: 13.801813125610352 = 1.0365610122680664 + 2.0 * 6.382626056671143
Epoch 380, val loss: 1.2182539701461792
Epoch 390, training loss: 13.759759902954102 = 1.003095269203186 + 2.0 * 6.378332138061523
Epoch 390, val loss: 1.1958317756652832
Epoch 400, training loss: 13.727592468261719 = 0.9692924618721008 + 2.0 * 6.379149913787842
Epoch 400, val loss: 1.1734189987182617
Epoch 410, training loss: 13.677928924560547 = 0.9354544878005981 + 2.0 * 6.371237277984619
Epoch 410, val loss: 1.1511571407318115
Epoch 420, training loss: 13.63823127746582 = 0.9013487696647644 + 2.0 * 6.368441104888916
Epoch 420, val loss: 1.129104495048523
Epoch 430, training loss: 13.60123062133789 = 0.8670947551727295 + 2.0 * 6.367067813873291
Epoch 430, val loss: 1.1072827577590942
Epoch 440, training loss: 13.564416885375977 = 0.8332018852233887 + 2.0 * 6.365607738494873
Epoch 440, val loss: 1.085860252380371
Epoch 450, training loss: 13.523124694824219 = 0.7997961044311523 + 2.0 * 6.361664295196533
Epoch 450, val loss: 1.0655512809753418
Epoch 460, training loss: 13.482080459594727 = 0.7672832608222961 + 2.0 * 6.357398509979248
Epoch 460, val loss: 1.0462815761566162
Epoch 470, training loss: 13.447150230407715 = 0.7355287671089172 + 2.0 * 6.355810642242432
Epoch 470, val loss: 1.0281665325164795
Epoch 480, training loss: 13.414634704589844 = 0.7048454284667969 + 2.0 * 6.354894638061523
Epoch 480, val loss: 1.011318564414978
Epoch 490, training loss: 13.386713027954102 = 0.675521969795227 + 2.0 * 6.355595588684082
Epoch 490, val loss: 0.9960542917251587
Epoch 500, training loss: 13.346231460571289 = 0.647702693939209 + 2.0 * 6.349264621734619
Epoch 500, val loss: 0.9825229048728943
Epoch 510, training loss: 13.313183784484863 = 0.6213020086288452 + 2.0 * 6.345941066741943
Epoch 510, val loss: 0.9705873727798462
Epoch 520, training loss: 13.284435272216797 = 0.5961165428161621 + 2.0 * 6.3441596031188965
Epoch 520, val loss: 0.9601941108703613
Epoch 530, training loss: 13.268180847167969 = 0.5721111297607422 + 2.0 * 6.348034858703613
Epoch 530, val loss: 0.951306164264679
Epoch 540, training loss: 13.231656074523926 = 0.5493707060813904 + 2.0 * 6.341142654418945
Epoch 540, val loss: 0.9438278675079346
Epoch 550, training loss: 13.20710563659668 = 0.5276575088500977 + 2.0 * 6.339724063873291
Epoch 550, val loss: 0.9376670122146606
Epoch 560, training loss: 13.185354232788086 = 0.5069184303283691 + 2.0 * 6.339217662811279
Epoch 560, val loss: 0.9325926899909973
Epoch 570, training loss: 13.162638664245605 = 0.4870644807815552 + 2.0 * 6.33778715133667
Epoch 570, val loss: 0.9287617206573486
Epoch 580, training loss: 13.135440826416016 = 0.46791672706604004 + 2.0 * 6.333762168884277
Epoch 580, val loss: 0.9259207248687744
Epoch 590, training loss: 13.113024711608887 = 0.4494706392288208 + 2.0 * 6.331777095794678
Epoch 590, val loss: 0.9239943027496338
Epoch 600, training loss: 13.100055694580078 = 0.4315681457519531 + 2.0 * 6.3342437744140625
Epoch 600, val loss: 0.9228112101554871
Epoch 610, training loss: 13.073492050170898 = 0.41422751545906067 + 2.0 * 6.32963228225708
Epoch 610, val loss: 0.9224326014518738
Epoch 620, training loss: 13.063526153564453 = 0.3974415063858032 + 2.0 * 6.333042144775391
Epoch 620, val loss: 0.9226387143135071
Epoch 630, training loss: 13.034612655639648 = 0.3812969923019409 + 2.0 * 6.326657772064209
Epoch 630, val loss: 0.9235749244689941
Epoch 640, training loss: 13.012207984924316 = 0.36560195684432983 + 2.0 * 6.32330322265625
Epoch 640, val loss: 0.9250105619430542
Epoch 650, training loss: 12.994827270507812 = 0.35030773282051086 + 2.0 * 6.322259902954102
Epoch 650, val loss: 0.9268656969070435
Epoch 660, training loss: 12.976325988769531 = 0.3354387581348419 + 2.0 * 6.320443630218506
Epoch 660, val loss: 0.929047703742981
Epoch 670, training loss: 12.9616060256958 = 0.3210676312446594 + 2.0 * 6.3202691078186035
Epoch 670, val loss: 0.9317257404327393
Epoch 680, training loss: 12.958022117614746 = 0.30716171860694885 + 2.0 * 6.325430393218994
Epoch 680, val loss: 0.9346393942832947
Epoch 690, training loss: 12.927257537841797 = 0.2937026023864746 + 2.0 * 6.31677770614624
Epoch 690, val loss: 0.9380160570144653
Epoch 700, training loss: 12.90866470336914 = 0.2807002067565918 + 2.0 * 6.313982009887695
Epoch 700, val loss: 0.9416770935058594
Epoch 710, training loss: 12.893659591674805 = 0.2681264281272888 + 2.0 * 6.3127665519714355
Epoch 710, val loss: 0.9456936120986938
Epoch 720, training loss: 12.91334342956543 = 0.25600430369377136 + 2.0 * 6.328669548034668
Epoch 720, val loss: 0.9498664140701294
Epoch 730, training loss: 12.877851486206055 = 0.24454288184642792 + 2.0 * 6.316654205322266
Epoch 730, val loss: 0.9543909430503845
Epoch 740, training loss: 12.85544490814209 = 0.23358233273029327 + 2.0 * 6.310931205749512
Epoch 740, val loss: 0.9594634771347046
Epoch 750, training loss: 12.839667320251465 = 0.22305378317832947 + 2.0 * 6.308306694030762
Epoch 750, val loss: 0.9646644592285156
Epoch 760, training loss: 12.826007843017578 = 0.21297027170658112 + 2.0 * 6.3065185546875
Epoch 760, val loss: 0.9702546000480652
Epoch 770, training loss: 12.813971519470215 = 0.20332476496696472 + 2.0 * 6.305323600769043
Epoch 770, val loss: 0.9761501550674438
Epoch 780, training loss: 12.817068099975586 = 0.1941455900669098 + 2.0 * 6.311461448669434
Epoch 780, val loss: 0.9820563197135925
Epoch 790, training loss: 12.801042556762695 = 0.1854880303144455 + 2.0 * 6.307777404785156
Epoch 790, val loss: 0.9884089827537537
Epoch 800, training loss: 12.788440704345703 = 0.17726020514965057 + 2.0 * 6.3055901527404785
Epoch 800, val loss: 0.9949501156806946
Epoch 810, training loss: 12.774063110351562 = 0.1694723665714264 + 2.0 * 6.302295207977295
Epoch 810, val loss: 1.0016701221466064
Epoch 820, training loss: 12.766597747802734 = 0.16206857562065125 + 2.0 * 6.30226469039917
Epoch 820, val loss: 1.0086551904678345
Epoch 830, training loss: 12.760184288024902 = 0.155018612742424 + 2.0 * 6.302582740783691
Epoch 830, val loss: 1.0156397819519043
Epoch 840, training loss: 12.751079559326172 = 0.14836253225803375 + 2.0 * 6.301358699798584
Epoch 840, val loss: 1.0229915380477905
Epoch 850, training loss: 12.743487358093262 = 0.1420222520828247 + 2.0 * 6.300732612609863
Epoch 850, val loss: 1.0303231477737427
Epoch 860, training loss: 12.734041213989258 = 0.1360243409872055 + 2.0 * 6.299008369445801
Epoch 860, val loss: 1.0379530191421509
Epoch 870, training loss: 12.721761703491211 = 0.13032527267932892 + 2.0 * 6.295718193054199
Epoch 870, val loss: 1.045432209968567
Epoch 880, training loss: 12.71351146697998 = 0.12492845952510834 + 2.0 * 6.2942914962768555
Epoch 880, val loss: 1.053242564201355
Epoch 890, training loss: 12.708891868591309 = 0.11979580670595169 + 2.0 * 6.294548034667969
Epoch 890, val loss: 1.0610387325286865
Epoch 900, training loss: 12.702061653137207 = 0.114925816655159 + 2.0 * 6.293568134307861
Epoch 900, val loss: 1.0687421560287476
Epoch 910, training loss: 12.701003074645996 = 0.11032599955797195 + 2.0 * 6.2953386306762695
Epoch 910, val loss: 1.0766273736953735
Epoch 920, training loss: 12.688694953918457 = 0.10594864189624786 + 2.0 * 6.291373252868652
Epoch 920, val loss: 1.084540605545044
Epoch 930, training loss: 12.683683395385742 = 0.10178802907466888 + 2.0 * 6.290947914123535
Epoch 930, val loss: 1.0925487279891968
Epoch 940, training loss: 12.683423042297363 = 0.09783148020505905 + 2.0 * 6.292795658111572
Epoch 940, val loss: 1.1005210876464844
Epoch 950, training loss: 12.674985885620117 = 0.09405974298715591 + 2.0 * 6.290462970733643
Epoch 950, val loss: 1.1084275245666504
Epoch 960, training loss: 12.666854858398438 = 0.09047795832157135 + 2.0 * 6.288188457489014
Epoch 960, val loss: 1.11650550365448
Epoch 970, training loss: 12.660870552062988 = 0.08706565946340561 + 2.0 * 6.28690242767334
Epoch 970, val loss: 1.1244443655014038
Epoch 980, training loss: 12.664362907409668 = 0.08381462097167969 + 2.0 * 6.290274143218994
Epoch 980, val loss: 1.1323914527893066
Epoch 990, training loss: 12.656730651855469 = 0.08072186261415482 + 2.0 * 6.288004398345947
Epoch 990, val loss: 1.1401643753051758
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6851851851851852
0.8096995255666843
=== training gcn model ===
Epoch 0, training loss: 19.132091522216797 = 1.9384889602661133 + 2.0 * 8.5968017578125
Epoch 0, val loss: 1.9419499635696411
Epoch 10, training loss: 19.121795654296875 = 1.9291143417358398 + 2.0 * 8.59634017944336
Epoch 10, val loss: 1.9322530031204224
Epoch 20, training loss: 19.103870391845703 = 1.9172924757003784 + 2.0 * 8.593289375305176
Epoch 20, val loss: 1.9198217391967773
Epoch 30, training loss: 19.045047760009766 = 1.9005564451217651 + 2.0 * 8.572245597839355
Epoch 30, val loss: 1.9020469188690186
Epoch 40, training loss: 18.70414924621582 = 1.8800697326660156 + 2.0 * 8.412039756774902
Epoch 40, val loss: 1.8814587593078613
Epoch 50, training loss: 17.17046546936035 = 1.8592045307159424 + 2.0 * 7.655630111694336
Epoch 50, val loss: 1.8607885837554932
Epoch 60, training loss: 16.34589385986328 = 1.8447771072387695 + 2.0 * 7.250557899475098
Epoch 60, val loss: 1.847985029220581
Epoch 70, training loss: 15.862630844116211 = 1.834133505821228 + 2.0 * 7.014248847961426
Epoch 70, val loss: 1.8379465341567993
Epoch 80, training loss: 15.62767505645752 = 1.8243299722671509 + 2.0 * 6.90167236328125
Epoch 80, val loss: 1.8283082246780396
Epoch 90, training loss: 15.41536808013916 = 1.8130933046340942 + 2.0 * 6.801137447357178
Epoch 90, val loss: 1.8176556825637817
Epoch 100, training loss: 15.262274742126465 = 1.8020590543746948 + 2.0 * 6.73010778427124
Epoch 100, val loss: 1.8075056076049805
Epoch 110, training loss: 15.155550003051758 = 1.7916972637176514 + 2.0 * 6.681926250457764
Epoch 110, val loss: 1.7979830503463745
Epoch 120, training loss: 15.071722030639648 = 1.781786322593689 + 2.0 * 6.644968032836914
Epoch 120, val loss: 1.788651704788208
Epoch 130, training loss: 14.997632026672363 = 1.7718428373336792 + 2.0 * 6.612894535064697
Epoch 130, val loss: 1.779430627822876
Epoch 140, training loss: 14.932112693786621 = 1.7613461017608643 + 2.0 * 6.585383415222168
Epoch 140, val loss: 1.769989013671875
Epoch 150, training loss: 14.875847816467285 = 1.7499853372573853 + 2.0 * 6.562931060791016
Epoch 150, val loss: 1.760080337524414
Epoch 160, training loss: 14.822921752929688 = 1.7374346256256104 + 2.0 * 6.542743682861328
Epoch 160, val loss: 1.7494192123413086
Epoch 170, training loss: 14.77192497253418 = 1.7233275175094604 + 2.0 * 6.524298667907715
Epoch 170, val loss: 1.7376935482025146
Epoch 180, training loss: 14.73314380645752 = 1.7072690725326538 + 2.0 * 6.512937545776367
Epoch 180, val loss: 1.7245278358459473
Epoch 190, training loss: 14.678112030029297 = 1.6891520023345947 + 2.0 * 6.494480133056641
Epoch 190, val loss: 1.7095500230789185
Epoch 200, training loss: 14.630730628967285 = 1.6687389612197876 + 2.0 * 6.4809956550598145
Epoch 200, val loss: 1.692795991897583
Epoch 210, training loss: 14.583422660827637 = 1.6454578638076782 + 2.0 * 6.468982219696045
Epoch 210, val loss: 1.67398202419281
Epoch 220, training loss: 14.535764694213867 = 1.6190770864486694 + 2.0 * 6.458343982696533
Epoch 220, val loss: 1.6525906324386597
Epoch 230, training loss: 14.489090919494629 = 1.5894100666046143 + 2.0 * 6.449840545654297
Epoch 230, val loss: 1.62861168384552
Epoch 240, training loss: 14.439745903015137 = 1.5561449527740479 + 2.0 * 6.441800594329834
Epoch 240, val loss: 1.6018732786178589
Epoch 250, training loss: 14.391640663146973 = 1.5195099115371704 + 2.0 * 6.436065196990967
Epoch 250, val loss: 1.5727170705795288
Epoch 260, training loss: 14.335258483886719 = 1.4801560640335083 + 2.0 * 6.42755126953125
Epoch 260, val loss: 1.5413979291915894
Epoch 270, training loss: 14.280257225036621 = 1.4381366968154907 + 2.0 * 6.421060085296631
Epoch 270, val loss: 1.5081928968429565
Epoch 280, training loss: 14.22748851776123 = 1.393977403640747 + 2.0 * 6.416755676269531
Epoch 280, val loss: 1.473636507987976
Epoch 290, training loss: 14.170973777770996 = 1.348895788192749 + 2.0 * 6.411038875579834
Epoch 290, val loss: 1.4386286735534668
Epoch 300, training loss: 14.114798545837402 = 1.3036658763885498 + 2.0 * 6.405566215515137
Epoch 300, val loss: 1.4039709568023682
Epoch 310, training loss: 14.064044952392578 = 1.259369134902954 + 2.0 * 6.402338027954102
Epoch 310, val loss: 1.3704626560211182
Epoch 320, training loss: 14.008811950683594 = 1.2167317867279053 + 2.0 * 6.396039962768555
Epoch 320, val loss: 1.338731288909912
Epoch 330, training loss: 13.960014343261719 = 1.1756820678710938 + 2.0 * 6.3921661376953125
Epoch 330, val loss: 1.3087095022201538
Epoch 340, training loss: 13.914271354675293 = 1.1364883184432983 + 2.0 * 6.388891696929932
Epoch 340, val loss: 1.28061842918396
Epoch 350, training loss: 13.866912841796875 = 1.0993764400482178 + 2.0 * 6.383768081665039
Epoch 350, val loss: 1.2545679807662964
Epoch 360, training loss: 13.826583862304688 = 1.064064621925354 + 2.0 * 6.381259441375732
Epoch 360, val loss: 1.2302815914154053
Epoch 370, training loss: 13.784330368041992 = 1.03043532371521 + 2.0 * 6.376947402954102
Epoch 370, val loss: 1.2076059579849243
Epoch 380, training loss: 13.74533462524414 = 0.9982227683067322 + 2.0 * 6.373556137084961
Epoch 380, val loss: 1.1863181591033936
Epoch 390, training loss: 13.707218170166016 = 0.9672706127166748 + 2.0 * 6.369973659515381
Epoch 390, val loss: 1.1661611795425415
Epoch 400, training loss: 13.680890083312988 = 0.9372667670249939 + 2.0 * 6.371811866760254
Epoch 400, val loss: 1.146998643875122
Epoch 410, training loss: 13.63707160949707 = 0.9083192944526672 + 2.0 * 6.364376068115234
Epoch 410, val loss: 1.1287331581115723
Epoch 420, training loss: 13.602681159973145 = 0.8802997469902039 + 2.0 * 6.3611907958984375
Epoch 420, val loss: 1.1113336086273193
Epoch 430, training loss: 13.569742202758789 = 0.8530460000038147 + 2.0 * 6.3583478927612305
Epoch 430, val loss: 1.0946916341781616
Epoch 440, training loss: 13.543045043945312 = 0.8265631198883057 + 2.0 * 6.358241081237793
Epoch 440, val loss: 1.0788520574569702
Epoch 450, training loss: 13.511661529541016 = 0.8008508086204529 + 2.0 * 6.355405330657959
Epoch 450, val loss: 1.0637316703796387
Epoch 460, training loss: 13.478342056274414 = 0.7760192155838013 + 2.0 * 6.351161479949951
Epoch 460, val loss: 1.0494678020477295
Epoch 470, training loss: 13.447114944458008 = 0.7519083619117737 + 2.0 * 6.3476033210754395
Epoch 470, val loss: 1.0360641479492188
Epoch 480, training loss: 13.434078216552734 = 0.7284559607505798 + 2.0 * 6.352811336517334
Epoch 480, val loss: 1.023404598236084
Epoch 490, training loss: 13.391342163085938 = 0.7058143615722656 + 2.0 * 6.342763900756836
Epoch 490, val loss: 1.0116833448410034
Epoch 500, training loss: 13.366363525390625 = 0.6838706135749817 + 2.0 * 6.341246604919434
Epoch 500, val loss: 1.000849723815918
Epoch 510, training loss: 13.360000610351562 = 0.6625450253486633 + 2.0 * 6.348727703094482
Epoch 510, val loss: 0.990817129611969
Epoch 520, training loss: 13.318482398986816 = 0.6420232653617859 + 2.0 * 6.338229656219482
Epoch 520, val loss: 0.9815540313720703
Epoch 530, training loss: 13.292169570922852 = 0.6222947239875793 + 2.0 * 6.334937572479248
Epoch 530, val loss: 0.9733295440673828
Epoch 540, training loss: 13.269598960876465 = 0.6032137274742126 + 2.0 * 6.333192825317383
Epoch 540, val loss: 0.9658142328262329
Epoch 550, training loss: 13.252424240112305 = 0.5847356915473938 + 2.0 * 6.333844184875488
Epoch 550, val loss: 0.9589467644691467
Epoch 560, training loss: 13.226173400878906 = 0.5669545531272888 + 2.0 * 6.329609394073486
Epoch 560, val loss: 0.9529832601547241
Epoch 570, training loss: 13.205413818359375 = 0.5497525930404663 + 2.0 * 6.327830791473389
Epoch 570, val loss: 0.9476460218429565
Epoch 580, training loss: 13.194425582885742 = 0.5330585241317749 + 2.0 * 6.330683708190918
Epoch 580, val loss: 0.9428803324699402
Epoch 590, training loss: 13.172892570495605 = 0.5169141292572021 + 2.0 * 6.327989101409912
Epoch 590, val loss: 0.9386981129646301
Epoch 600, training loss: 13.147976875305176 = 0.5013171434402466 + 2.0 * 6.323329925537109
Epoch 600, val loss: 0.9351577162742615
Epoch 610, training loss: 13.128825187683105 = 0.48614501953125 + 2.0 * 6.321340084075928
Epoch 610, val loss: 0.932072103023529
Epoch 620, training loss: 13.110532760620117 = 0.4713042676448822 + 2.0 * 6.319614410400391
Epoch 620, val loss: 0.9293837547302246
Epoch 630, training loss: 13.100793838500977 = 0.45671871304512024 + 2.0 * 6.322037696838379
Epoch 630, val loss: 0.9269514679908752
Epoch 640, training loss: 13.087005615234375 = 0.44239717721939087 + 2.0 * 6.3223042488098145
Epoch 640, val loss: 0.9247877597808838
Epoch 650, training loss: 13.062588691711426 = 0.4283110499382019 + 2.0 * 6.317138671875
Epoch 650, val loss: 0.9229570031166077
Epoch 660, training loss: 13.043413162231445 = 0.4143596291542053 + 2.0 * 6.314526557922363
Epoch 660, val loss: 0.9212160706520081
Epoch 670, training loss: 13.028220176696777 = 0.4004642069339752 + 2.0 * 6.313878059387207
Epoch 670, val loss: 0.9195924997329712
Epoch 680, training loss: 13.011466026306152 = 0.38654670119285583 + 2.0 * 6.312459468841553
Epoch 680, val loss: 0.9180630445480347
Epoch 690, training loss: 12.99294662475586 = 0.37260961532592773 + 2.0 * 6.310168743133545
Epoch 690, val loss: 0.9166717529296875
Epoch 700, training loss: 12.975313186645508 = 0.3586251437664032 + 2.0 * 6.308343887329102
Epoch 700, val loss: 0.9154886603355408
Epoch 710, training loss: 12.96809196472168 = 0.34451499581336975 + 2.0 * 6.311788558959961
Epoch 710, val loss: 0.9143442511558533
Epoch 720, training loss: 12.94930362701416 = 0.33038845658302307 + 2.0 * 6.309457778930664
Epoch 720, val loss: 0.9130681157112122
Epoch 730, training loss: 12.927739143371582 = 0.3162216544151306 + 2.0 * 6.305758953094482
Epoch 730, val loss: 0.9122734665870667
Epoch 740, training loss: 12.920256614685059 = 0.30204513669013977 + 2.0 * 6.30910587310791
Epoch 740, val loss: 0.9113835096359253
Epoch 750, training loss: 12.899694442749023 = 0.2879869043827057 + 2.0 * 6.305853843688965
Epoch 750, val loss: 0.9107993841171265
Epoch 760, training loss: 12.879501342773438 = 0.27406489849090576 + 2.0 * 6.302718162536621
Epoch 760, val loss: 0.9104232788085938
Epoch 770, training loss: 12.863272666931152 = 0.26033663749694824 + 2.0 * 6.3014678955078125
Epoch 770, val loss: 0.9103104472160339
Epoch 780, training loss: 12.862828254699707 = 0.24690717458724976 + 2.0 * 6.307960510253906
Epoch 780, val loss: 0.9104474186897278
Epoch 790, training loss: 12.837742805480957 = 0.23394560813903809 + 2.0 * 6.30189847946167
Epoch 790, val loss: 0.9110570549964905
Epoch 800, training loss: 12.823464393615723 = 0.22148098051548004 + 2.0 * 6.300991535186768
Epoch 800, val loss: 0.9120440483093262
Epoch 810, training loss: 12.804347038269043 = 0.20960266888141632 + 2.0 * 6.297372341156006
Epoch 810, val loss: 0.9134655594825745
Epoch 820, training loss: 12.794469833374023 = 0.19827361404895782 + 2.0 * 6.298098087310791
Epoch 820, val loss: 0.9151915907859802
Epoch 830, training loss: 12.780025482177734 = 0.18756073713302612 + 2.0 * 6.296232223510742
Epoch 830, val loss: 0.9172589182853699
Epoch 840, training loss: 12.767945289611816 = 0.17745602130889893 + 2.0 * 6.2952446937561035
Epoch 840, val loss: 0.9199027419090271
Epoch 850, training loss: 12.756009101867676 = 0.16793398559093475 + 2.0 * 6.294037342071533
Epoch 850, val loss: 0.9228582382202148
Epoch 860, training loss: 12.745888710021973 = 0.1589532494544983 + 2.0 * 6.2934675216674805
Epoch 860, val loss: 0.9261486530303955
Epoch 870, training loss: 12.735639572143555 = 0.1505596786737442 + 2.0 * 6.292540073394775
Epoch 870, val loss: 0.9297735095024109
Epoch 880, training loss: 12.729166030883789 = 0.1427178978919983 + 2.0 * 6.293223857879639
Epoch 880, val loss: 0.9337541460990906
Epoch 890, training loss: 12.71744155883789 = 0.13537023961544037 + 2.0 * 6.2910356521606445
Epoch 890, val loss: 0.9380689263343811
Epoch 900, training loss: 12.70741081237793 = 0.12847189605236053 + 2.0 * 6.289469242095947
Epoch 900, val loss: 0.9425744414329529
Epoch 910, training loss: 12.719932556152344 = 0.12200614809989929 + 2.0 * 6.2989630699157715
Epoch 910, val loss: 0.9472320079803467
Epoch 920, training loss: 12.692974090576172 = 0.11598004400730133 + 2.0 * 6.288496971130371
Epoch 920, val loss: 0.9522252678871155
Epoch 930, training loss: 12.687481880187988 = 0.1103326827287674 + 2.0 * 6.288574695587158
Epoch 930, val loss: 0.9573855996131897
Epoch 940, training loss: 12.683189392089844 = 0.10503838956356049 + 2.0 * 6.2890753746032715
Epoch 940, val loss: 0.9626151919364929
Epoch 950, training loss: 12.672670364379883 = 0.10006411373615265 + 2.0 * 6.2863030433654785
Epoch 950, val loss: 0.9681187272071838
Epoch 960, training loss: 12.663744926452637 = 0.09539367258548737 + 2.0 * 6.284175395965576
Epoch 960, val loss: 0.9736719131469727
Epoch 970, training loss: 12.663993835449219 = 0.09099595248699188 + 2.0 * 6.2864990234375
Epoch 970, val loss: 0.9793248772621155
Epoch 980, training loss: 12.664922714233398 = 0.08687011152505875 + 2.0 * 6.289026260375977
Epoch 980, val loss: 0.9849900007247925
Epoch 990, training loss: 12.652791023254395 = 0.08300723135471344 + 2.0 * 6.2848920822143555
Epoch 990, val loss: 0.9908090233802795
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7296296296296296
0.8039008961518187
=== training gcn model ===
Epoch 0, training loss: 19.141944885253906 = 1.9482758045196533 + 2.0 * 8.596834182739258
Epoch 0, val loss: 1.9417157173156738
Epoch 10, training loss: 19.1317195892334 = 1.9385470151901245 + 2.0 * 8.596586227416992
Epoch 10, val loss: 1.9322623014450073
Epoch 20, training loss: 19.116527557373047 = 1.9267444610595703 + 2.0 * 8.594891548156738
Epoch 20, val loss: 1.9203213453292847
Epoch 30, training loss: 19.074594497680664 = 1.9107487201690674 + 2.0 * 8.58192253112793
Epoch 30, val loss: 1.9037387371063232
Epoch 40, training loss: 18.852874755859375 = 1.8906694650650024 + 2.0 * 8.48110294342041
Epoch 40, val loss: 1.883358359336853
Epoch 50, training loss: 17.57027816772461 = 1.8705940246582031 + 2.0 * 7.849841594696045
Epoch 50, val loss: 1.8631057739257812
Epoch 60, training loss: 17.10068702697754 = 1.851965069770813 + 2.0 * 7.624361038208008
Epoch 60, val loss: 1.8459968566894531
Epoch 70, training loss: 16.66705894470215 = 1.838249683380127 + 2.0 * 7.414404392242432
Epoch 70, val loss: 1.8344498872756958
Epoch 80, training loss: 16.245019912719727 = 1.82555091381073 + 2.0 * 7.209734916687012
Epoch 80, val loss: 1.8229728937149048
Epoch 90, training loss: 15.834227561950684 = 1.813851237297058 + 2.0 * 7.010188102722168
Epoch 90, val loss: 1.8124525547027588
Epoch 100, training loss: 15.616767883300781 = 1.803600788116455 + 2.0 * 6.906583309173584
Epoch 100, val loss: 1.8031262159347534
Epoch 110, training loss: 15.434191703796387 = 1.793442964553833 + 2.0 * 6.820374488830566
Epoch 110, val loss: 1.7940983772277832
Epoch 120, training loss: 15.304839134216309 = 1.7846944332122803 + 2.0 * 6.760072231292725
Epoch 120, val loss: 1.7863585948944092
Epoch 130, training loss: 15.203376770019531 = 1.7766607999801636 + 2.0 * 6.713357925415039
Epoch 130, val loss: 1.779004454612732
Epoch 140, training loss: 15.107625007629395 = 1.767842173576355 + 2.0 * 6.669891357421875
Epoch 140, val loss: 1.7710288763046265
Epoch 150, training loss: 15.026927947998047 = 1.7582428455352783 + 2.0 * 6.634342670440674
Epoch 150, val loss: 1.7627501487731934
Epoch 160, training loss: 14.960959434509277 = 1.7475892305374146 + 2.0 * 6.606685161590576
Epoch 160, val loss: 1.7536909580230713
Epoch 170, training loss: 14.898345947265625 = 1.7353380918502808 + 2.0 * 6.581503868103027
Epoch 170, val loss: 1.743519902229309
Epoch 180, training loss: 14.841399192810059 = 1.7213009595870972 + 2.0 * 6.560049057006836
Epoch 180, val loss: 1.7319806814193726
Epoch 190, training loss: 14.788269996643066 = 1.7053271532058716 + 2.0 * 6.541471481323242
Epoch 190, val loss: 1.7189345359802246
Epoch 200, training loss: 14.74050521850586 = 1.6872663497924805 + 2.0 * 6.5266194343566895
Epoch 200, val loss: 1.704323649406433
Epoch 210, training loss: 14.687058448791504 = 1.6667135953903198 + 2.0 * 6.510172367095947
Epoch 210, val loss: 1.6879037618637085
Epoch 220, training loss: 14.642294883728027 = 1.6433771848678589 + 2.0 * 6.4994587898254395
Epoch 220, val loss: 1.669396996498108
Epoch 230, training loss: 14.588274002075195 = 1.6172293424606323 + 2.0 * 6.485522270202637
Epoch 230, val loss: 1.6487423181533813
Epoch 240, training loss: 14.535455703735352 = 1.5879050493240356 + 2.0 * 6.473775386810303
Epoch 240, val loss: 1.6258761882781982
Epoch 250, training loss: 14.488320350646973 = 1.5550605058670044 + 2.0 * 6.466629981994629
Epoch 250, val loss: 1.6005961894989014
Epoch 260, training loss: 14.43308162689209 = 1.5193122625350952 + 2.0 * 6.456884860992432
Epoch 260, val loss: 1.5733224153518677
Epoch 270, training loss: 14.374494552612305 = 1.4812040328979492 + 2.0 * 6.446645259857178
Epoch 270, val loss: 1.5448076725006104
Epoch 280, training loss: 14.31777286529541 = 1.4405317306518555 + 2.0 * 6.438620567321777
Epoch 280, val loss: 1.5150561332702637
Epoch 290, training loss: 14.272054672241211 = 1.3979058265686035 + 2.0 * 6.437074184417725
Epoch 290, val loss: 1.4846770763397217
Epoch 300, training loss: 14.20742416381836 = 1.354644536972046 + 2.0 * 6.426389694213867
Epoch 300, val loss: 1.4548203945159912
Epoch 310, training loss: 14.14923095703125 = 1.3109885454177856 + 2.0 * 6.419121265411377
Epoch 310, val loss: 1.4257327318191528
Epoch 320, training loss: 14.097408294677734 = 1.26723051071167 + 2.0 * 6.415088653564453
Epoch 320, val loss: 1.3975675106048584
Epoch 330, training loss: 14.038735389709473 = 1.2238543033599854 + 2.0 * 6.407440662384033
Epoch 330, val loss: 1.370667815208435
Epoch 340, training loss: 13.986612319946289 = 1.1810745000839233 + 2.0 * 6.402769088745117
Epoch 340, val loss: 1.3449019193649292
Epoch 350, training loss: 13.939674377441406 = 1.139115571975708 + 2.0 * 6.400279521942139
Epoch 350, val loss: 1.3200139999389648
Epoch 360, training loss: 13.885793685913086 = 1.0983816385269165 + 2.0 * 6.39370584487915
Epoch 360, val loss: 1.2964086532592773
Epoch 370, training loss: 13.838092803955078 = 1.0585856437683105 + 2.0 * 6.389753818511963
Epoch 370, val loss: 1.2736401557922363
Epoch 380, training loss: 13.798872947692871 = 1.0195846557617188 + 2.0 * 6.389644145965576
Epoch 380, val loss: 1.2515281438827515
Epoch 390, training loss: 13.748453140258789 = 0.9817003011703491 + 2.0 * 6.383376598358154
Epoch 390, val loss: 1.2300561666488647
Epoch 400, training loss: 13.70338249206543 = 0.9449697732925415 + 2.0 * 6.37920618057251
Epoch 400, val loss: 1.2094154357910156
Epoch 410, training loss: 13.662196159362793 = 0.9091582894325256 + 2.0 * 6.376518726348877
Epoch 410, val loss: 1.189286708831787
Epoch 420, training loss: 13.630899429321289 = 0.8744536638259888 + 2.0 * 6.378222942352295
Epoch 420, val loss: 1.169763445854187
Epoch 430, training loss: 13.582950592041016 = 0.8413029313087463 + 2.0 * 6.370823860168457
Epoch 430, val loss: 1.1511752605438232
Epoch 440, training loss: 13.543659210205078 = 0.8092634081840515 + 2.0 * 6.3671979904174805
Epoch 440, val loss: 1.1332597732543945
Epoch 450, training loss: 13.510826110839844 = 0.7782713770866394 + 2.0 * 6.36627721786499
Epoch 450, val loss: 1.115912914276123
Epoch 460, training loss: 13.47611141204834 = 0.7486540675163269 + 2.0 * 6.3637285232543945
Epoch 460, val loss: 1.0993629693984985
Epoch 470, training loss: 13.4385404586792 = 0.7201448678970337 + 2.0 * 6.359197616577148
Epoch 470, val loss: 1.0835999250411987
Epoch 480, training loss: 13.408819198608398 = 0.6927393078804016 + 2.0 * 6.358039855957031
Epoch 480, val loss: 1.068595051765442
Epoch 490, training loss: 13.379914283752441 = 0.6663691401481628 + 2.0 * 6.356772422790527
Epoch 490, val loss: 1.0544465780258179
Epoch 500, training loss: 13.346441268920898 = 0.6407938599586487 + 2.0 * 6.352823734283447
Epoch 500, val loss: 1.0408830642700195
Epoch 510, training loss: 13.327312469482422 = 0.615905225276947 + 2.0 * 6.355703830718994
Epoch 510, val loss: 1.0278576612472534
Epoch 520, training loss: 13.292439460754395 = 0.591858446598053 + 2.0 * 6.350290298461914
Epoch 520, val loss: 1.015645980834961
Epoch 530, training loss: 13.261839866638184 = 0.5683392882347107 + 2.0 * 6.346750259399414
Epoch 530, val loss: 1.0038974285125732
Epoch 540, training loss: 13.232421875 = 0.5451099276542664 + 2.0 * 6.343656063079834
Epoch 540, val loss: 0.9926741123199463
Epoch 550, training loss: 13.204641342163086 = 0.5219699740409851 + 2.0 * 6.341335773468018
Epoch 550, val loss: 0.981748104095459
Epoch 560, training loss: 13.182927131652832 = 0.4989127516746521 + 2.0 * 6.342007160186768
Epoch 560, val loss: 0.9712109565734863
Epoch 570, training loss: 13.160951614379883 = 0.4761900305747986 + 2.0 * 6.342381000518799
Epoch 570, val loss: 0.9613033533096313
Epoch 580, training loss: 13.126985549926758 = 0.4537563920021057 + 2.0 * 6.336614608764648
Epoch 580, val loss: 0.9518072009086609
Epoch 590, training loss: 13.102015495300293 = 0.43160343170166016 + 2.0 * 6.335206031799316
Epoch 590, val loss: 0.9429436326026917
Epoch 600, training loss: 13.077912330627441 = 0.4097881019115448 + 2.0 * 6.334062099456787
Epoch 600, val loss: 0.9347096085548401
Epoch 610, training loss: 13.051736831665039 = 0.3883994221687317 + 2.0 * 6.331668853759766
Epoch 610, val loss: 0.9271624088287354
Epoch 620, training loss: 13.031289100646973 = 0.3674840033054352 + 2.0 * 6.331902503967285
Epoch 620, val loss: 0.9203358292579651
Epoch 630, training loss: 13.007055282592773 = 0.34722381830215454 + 2.0 * 6.329915523529053
Epoch 630, val loss: 0.9141666293144226
Epoch 640, training loss: 12.980684280395508 = 0.3276105523109436 + 2.0 * 6.326536655426025
Epoch 640, val loss: 0.9087849259376526
Epoch 650, training loss: 12.962244987487793 = 0.3086432218551636 + 2.0 * 6.32680082321167
Epoch 650, val loss: 0.9041115045547485
Epoch 660, training loss: 12.95720386505127 = 0.29048341512680054 + 2.0 * 6.333360195159912
Epoch 660, val loss: 0.9003233313560486
Epoch 670, training loss: 12.924283027648926 = 0.27340373396873474 + 2.0 * 6.325439453125
Epoch 670, val loss: 0.8972745537757874
Epoch 680, training loss: 12.898401260375977 = 0.2572217881679535 + 2.0 * 6.320589542388916
Epoch 680, val loss: 0.8951866626739502
Epoch 690, training loss: 12.879451751708984 = 0.2419072538614273 + 2.0 * 6.318772315979004
Epoch 690, val loss: 0.8937891125679016
Epoch 700, training loss: 12.86250114440918 = 0.22744423151016235 + 2.0 * 6.317528247833252
Epoch 700, val loss: 0.893093466758728
Epoch 710, training loss: 12.886798858642578 = 0.2138650268316269 + 2.0 * 6.3364667892456055
Epoch 710, val loss: 0.8930537700653076
Epoch 720, training loss: 12.840116500854492 = 0.20141498744487762 + 2.0 * 6.319350719451904
Epoch 720, val loss: 0.8936281800270081
Epoch 730, training loss: 12.817851066589355 = 0.18985915184020996 + 2.0 * 6.313995838165283
Epoch 730, val loss: 0.8948889970779419
Epoch 740, training loss: 12.80487060546875 = 0.17907723784446716 + 2.0 * 6.312896728515625
Epoch 740, val loss: 0.8966974020004272
Epoch 750, training loss: 12.79190731048584 = 0.16900798678398132 + 2.0 * 6.3114495277404785
Epoch 750, val loss: 0.8989730477333069
Epoch 760, training loss: 12.798049926757812 = 0.15963539481163025 + 2.0 * 6.319207191467285
Epoch 760, val loss: 0.9017060399055481
Epoch 770, training loss: 12.773505210876465 = 0.1509304791688919 + 2.0 * 6.3112874031066895
Epoch 770, val loss: 0.9047020673751831
Epoch 780, training loss: 12.758339881896973 = 0.14284874498844147 + 2.0 * 6.307745456695557
Epoch 780, val loss: 0.9081559181213379
Epoch 790, training loss: 12.751294136047363 = 0.13532115519046783 + 2.0 * 6.307986259460449
Epoch 790, val loss: 0.9119380712509155
Epoch 800, training loss: 12.74714469909668 = 0.12832427024841309 + 2.0 * 6.309410095214844
Epoch 800, val loss: 0.9158582091331482
Epoch 810, training loss: 12.732422828674316 = 0.12184608727693558 + 2.0 * 6.305288314819336
Epoch 810, val loss: 0.9199711680412292
Epoch 820, training loss: 12.724419593811035 = 0.1158098429441452 + 2.0 * 6.304305076599121
Epoch 820, val loss: 0.9242873191833496
Epoch 830, training loss: 12.717062950134277 = 0.11015252768993378 + 2.0 * 6.303455352783203
Epoch 830, val loss: 0.9287532567977905
Epoch 840, training loss: 12.712089538574219 = 0.10486895591020584 + 2.0 * 6.303610324859619
Epoch 840, val loss: 0.9332840442657471
Epoch 850, training loss: 12.700925827026367 = 0.0999520868062973 + 2.0 * 6.300487041473389
Epoch 850, val loss: 0.938042402267456
Epoch 860, training loss: 12.697090148925781 = 0.09534042328596115 + 2.0 * 6.300874710083008
Epoch 860, val loss: 0.9428866505622864
Epoch 870, training loss: 12.694709777832031 = 0.09100986272096634 + 2.0 * 6.301849842071533
Epoch 870, val loss: 0.9477102160453796
Epoch 880, training loss: 12.684277534484863 = 0.08695007115602493 + 2.0 * 6.29866361618042
Epoch 880, val loss: 0.9525088667869568
Epoch 890, training loss: 12.67949390411377 = 0.0831339880824089 + 2.0 * 6.298180103302002
Epoch 890, val loss: 0.9575200080871582
Epoch 900, training loss: 12.674310684204102 = 0.07955247163772583 + 2.0 * 6.297379016876221
Epoch 900, val loss: 0.9624448418617249
Epoch 910, training loss: 12.685885429382324 = 0.07618398219347 + 2.0 * 6.3048505783081055
Epoch 910, val loss: 0.9673603177070618
Epoch 920, training loss: 12.665101051330566 = 0.07303357869386673 + 2.0 * 6.29603385925293
Epoch 920, val loss: 0.9723630547523499
Epoch 930, training loss: 12.656437873840332 = 0.07005786895751953 + 2.0 * 6.293190002441406
Epoch 930, val loss: 0.9773501753807068
Epoch 940, training loss: 12.651232719421387 = 0.06724292039871216 + 2.0 * 6.291995048522949
Epoch 940, val loss: 0.9822828769683838
Epoch 950, training loss: 12.654376983642578 = 0.0645754411816597 + 2.0 * 6.294900894165039
Epoch 950, val loss: 0.9872093200683594
Epoch 960, training loss: 12.64622974395752 = 0.06206372752785683 + 2.0 * 6.292082786560059
Epoch 960, val loss: 0.9921565055847168
Epoch 970, training loss: 12.651289939880371 = 0.059697601944208145 + 2.0 * 6.2957963943481445
Epoch 970, val loss: 0.9971451163291931
Epoch 980, training loss: 12.636861801147461 = 0.057464804500341415 + 2.0 * 6.289698600769043
Epoch 980, val loss: 1.0019354820251465
Epoch 990, training loss: 12.632905960083008 = 0.055348239839076996 + 2.0 * 6.288778781890869
Epoch 990, val loss: 1.006840467453003
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.725925925925926
0.7912493410648392
The final CL Acc:0.71358, 0.02014, The final GNN Acc:0.80162, 0.00770
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13314])
remove edge: torch.Size([2, 7932])
updated graph: torch.Size([2, 10690])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 19.142866134643555 = 1.949230670928955 + 2.0 * 8.596817970275879
Epoch 0, val loss: 1.9431378841400146
Epoch 10, training loss: 19.131877899169922 = 1.9388725757598877 + 2.0 * 8.596502304077148
Epoch 10, val loss: 1.9324085712432861
Epoch 20, training loss: 19.11435317993164 = 1.9261934757232666 + 2.0 * 8.594079971313477
Epoch 20, val loss: 1.919144868850708
Epoch 30, training loss: 19.06085968017578 = 1.908882737159729 + 2.0 * 8.57598876953125
Epoch 30, val loss: 1.901107668876648
Epoch 40, training loss: 18.823062896728516 = 1.887628436088562 + 2.0 * 8.467717170715332
Epoch 40, val loss: 1.8802471160888672
Epoch 50, training loss: 17.974233627319336 = 1.865172266960144 + 2.0 * 8.05453109741211
Epoch 50, val loss: 1.8587919473648071
Epoch 60, training loss: 17.175289154052734 = 1.8445391654968262 + 2.0 * 7.665375232696533
Epoch 60, val loss: 1.8406566381454468
Epoch 70, training loss: 16.396635055541992 = 1.8302017450332642 + 2.0 * 7.28321647644043
Epoch 70, val loss: 1.828099250793457
Epoch 80, training loss: 15.929696083068848 = 1.8172504901885986 + 2.0 * 7.056222915649414
Epoch 80, val loss: 1.8157665729522705
Epoch 90, training loss: 15.614311218261719 = 1.8043441772460938 + 2.0 * 6.9049835205078125
Epoch 90, val loss: 1.8033828735351562
Epoch 100, training loss: 15.410914421081543 = 1.7894943952560425 + 2.0 * 6.8107099533081055
Epoch 100, val loss: 1.7899374961853027
Epoch 110, training loss: 15.26138687133789 = 1.7746682167053223 + 2.0 * 6.743359565734863
Epoch 110, val loss: 1.7770590782165527
Epoch 120, training loss: 15.155529022216797 = 1.76143217086792 + 2.0 * 6.697048187255859
Epoch 120, val loss: 1.7649946212768555
Epoch 130, training loss: 15.071941375732422 = 1.747642159461975 + 2.0 * 6.662149429321289
Epoch 130, val loss: 1.752135157585144
Epoch 140, training loss: 15.001216888427734 = 1.7328953742980957 + 2.0 * 6.634160995483398
Epoch 140, val loss: 1.7383617162704468
Epoch 150, training loss: 14.935728073120117 = 1.717063546180725 + 2.0 * 6.609332084655762
Epoch 150, val loss: 1.7237557172775269
Epoch 160, training loss: 14.871118545532227 = 1.6997572183609009 + 2.0 * 6.5856804847717285
Epoch 160, val loss: 1.7081362009048462
Epoch 170, training loss: 14.808323860168457 = 1.6805299520492554 + 2.0 * 6.563897132873535
Epoch 170, val loss: 1.6910511255264282
Epoch 180, training loss: 14.748047828674316 = 1.6591302156448364 + 2.0 * 6.544458866119385
Epoch 180, val loss: 1.6722540855407715
Epoch 190, training loss: 14.690010070800781 = 1.635088562965393 + 2.0 * 6.52746057510376
Epoch 190, val loss: 1.651196837425232
Epoch 200, training loss: 14.634797096252441 = 1.6078665256500244 + 2.0 * 6.513465404510498
Epoch 200, val loss: 1.627461314201355
Epoch 210, training loss: 14.575586318969727 = 1.577544927597046 + 2.0 * 6.499020576477051
Epoch 210, val loss: 1.6011360883712769
Epoch 220, training loss: 14.515169143676758 = 1.544136881828308 + 2.0 * 6.48551607131958
Epoch 220, val loss: 1.5722583532333374
Epoch 230, training loss: 14.459224700927734 = 1.5076111555099487 + 2.0 * 6.475806713104248
Epoch 230, val loss: 1.5408718585968018
Epoch 240, training loss: 14.395537376403809 = 1.4679880142211914 + 2.0 * 6.463774681091309
Epoch 240, val loss: 1.5069369077682495
Epoch 250, training loss: 14.339347839355469 = 1.425236701965332 + 2.0 * 6.457055568695068
Epoch 250, val loss: 1.470592737197876
Epoch 260, training loss: 14.271541595458984 = 1.380083441734314 + 2.0 * 6.4457292556762695
Epoch 260, val loss: 1.4325731992721558
Epoch 270, training loss: 14.208500862121582 = 1.3330566883087158 + 2.0 * 6.437722206115723
Epoch 270, val loss: 1.3932511806488037
Epoch 280, training loss: 14.146137237548828 = 1.284359335899353 + 2.0 * 6.430889129638672
Epoch 280, val loss: 1.3528461456298828
Epoch 290, training loss: 14.083073616027832 = 1.2347447872161865 + 2.0 * 6.424164295196533
Epoch 290, val loss: 1.3119136095046997
Epoch 300, training loss: 14.020698547363281 = 1.1849913597106934 + 2.0 * 6.417853355407715
Epoch 300, val loss: 1.2711882591247559
Epoch 310, training loss: 13.959918975830078 = 1.1355504989624023 + 2.0 * 6.412184238433838
Epoch 310, val loss: 1.2311174869537354
Epoch 320, training loss: 13.899569511413574 = 1.0870487689971924 + 2.0 * 6.4062604904174805
Epoch 320, val loss: 1.1922026872634888
Epoch 330, training loss: 13.842597961425781 = 1.0397899150848389 + 2.0 * 6.401403903961182
Epoch 330, val loss: 1.1546915769577026
Epoch 340, training loss: 13.793167114257812 = 0.994652271270752 + 2.0 * 6.399257659912109
Epoch 340, val loss: 1.1191374063491821
Epoch 350, training loss: 13.739232063293457 = 0.9521924257278442 + 2.0 * 6.393519878387451
Epoch 350, val loss: 1.0862129926681519
Epoch 360, training loss: 13.688401222229004 = 0.9124354720115662 + 2.0 * 6.3879828453063965
Epoch 360, val loss: 1.0559875965118408
Epoch 370, training loss: 13.641400337219238 = 0.8752004504203796 + 2.0 * 6.3831000328063965
Epoch 370, val loss: 1.02815580368042
Epoch 380, training loss: 13.599770545959473 = 0.8404036164283752 + 2.0 * 6.379683494567871
Epoch 380, val loss: 1.0026646852493286
Epoch 390, training loss: 13.562859535217285 = 0.8081142902374268 + 2.0 * 6.377372741699219
Epoch 390, val loss: 0.9794978499412537
Epoch 400, training loss: 13.521717071533203 = 0.7783986330032349 + 2.0 * 6.371659278869629
Epoch 400, val loss: 0.9587794542312622
Epoch 410, training loss: 13.487140655517578 = 0.7508232593536377 + 2.0 * 6.36815881729126
Epoch 410, val loss: 0.9400674700737
Epoch 420, training loss: 13.467374801635742 = 0.7249788641929626 + 2.0 * 6.3711981773376465
Epoch 420, val loss: 0.9230428338050842
Epoch 430, training loss: 13.42680549621582 = 0.7007090449333191 + 2.0 * 6.363048076629639
Epoch 430, val loss: 0.9074926972389221
Epoch 440, training loss: 13.39571762084961 = 0.6776175498962402 + 2.0 * 6.359050273895264
Epoch 440, val loss: 0.8932102918624878
Epoch 450, training loss: 13.3753080368042 = 0.6553623676300049 + 2.0 * 6.359972953796387
Epoch 450, val loss: 0.8798128366470337
Epoch 460, training loss: 13.342292785644531 = 0.633897066116333 + 2.0 * 6.354197978973389
Epoch 460, val loss: 0.8670428395271301
Epoch 470, training loss: 13.314133644104004 = 0.6128638386726379 + 2.0 * 6.350635051727295
Epoch 470, val loss: 0.8549654483795166
Epoch 480, training loss: 13.292664527893066 = 0.5921677350997925 + 2.0 * 6.350248336791992
Epoch 480, val loss: 0.8433093428611755
Epoch 490, training loss: 13.272455215454102 = 0.5719301700592041 + 2.0 * 6.350262641906738
Epoch 490, val loss: 0.8319252729415894
Epoch 500, training loss: 13.238837242126465 = 0.5520349740982056 + 2.0 * 6.343400955200195
Epoch 500, val loss: 0.8212279677391052
Epoch 510, training loss: 13.214153289794922 = 0.5324263572692871 + 2.0 * 6.340863227844238
Epoch 510, val loss: 0.8109015226364136
Epoch 520, training loss: 13.19033432006836 = 0.5130219459533691 + 2.0 * 6.338656425476074
Epoch 520, val loss: 0.8008330464363098
Epoch 530, training loss: 13.187596321105957 = 0.4938911199569702 + 2.0 * 6.346852779388428
Epoch 530, val loss: 0.7911481261253357
Epoch 540, training loss: 13.1491117477417 = 0.47523829340934753 + 2.0 * 6.336936950683594
Epoch 540, val loss: 0.7822397947311401
Epoch 550, training loss: 13.124311447143555 = 0.4570762515068054 + 2.0 * 6.333617687225342
Epoch 550, val loss: 0.7739880681037903
Epoch 560, training loss: 13.102280616760254 = 0.4393989145755768 + 2.0 * 6.3314409255981445
Epoch 560, val loss: 0.7663924694061279
Epoch 570, training loss: 13.094826698303223 = 0.42220476269721985 + 2.0 * 6.336310863494873
Epoch 570, val loss: 0.7593369483947754
Epoch 580, training loss: 13.062827110290527 = 0.40546154975891113 + 2.0 * 6.328682899475098
Epoch 580, val loss: 0.7530654668807983
Epoch 590, training loss: 13.043590545654297 = 0.3893260359764099 + 2.0 * 6.327132225036621
Epoch 590, val loss: 0.747395396232605
Epoch 600, training loss: 13.025818824768066 = 0.37366265058517456 + 2.0 * 6.326077938079834
Epoch 600, val loss: 0.7423363924026489
Epoch 610, training loss: 13.007261276245117 = 0.35846781730651855 + 2.0 * 6.32439661026001
Epoch 610, val loss: 0.7377510070800781
Epoch 620, training loss: 12.996169090270996 = 0.3437994718551636 + 2.0 * 6.3261847496032715
Epoch 620, val loss: 0.7337096333503723
Epoch 630, training loss: 12.971037864685059 = 0.32958748936653137 + 2.0 * 6.320724964141846
Epoch 630, val loss: 0.7302902936935425
Epoch 640, training loss: 12.952959060668945 = 0.315814733505249 + 2.0 * 6.318572044372559
Epoch 640, val loss: 0.7272764444351196
Epoch 650, training loss: 12.938852310180664 = 0.3023872375488281 + 2.0 * 6.318232536315918
Epoch 650, val loss: 0.7247560024261475
Epoch 660, training loss: 12.925699234008789 = 0.28934088349342346 + 2.0 * 6.318179130554199
Epoch 660, val loss: 0.7225964069366455
Epoch 670, training loss: 12.906805992126465 = 0.27670881152153015 + 2.0 * 6.315048694610596
Epoch 670, val loss: 0.7209096550941467
Epoch 680, training loss: 12.89116096496582 = 0.264477401971817 + 2.0 * 6.3133416175842285
Epoch 680, val loss: 0.7196734547615051
Epoch 690, training loss: 12.879385948181152 = 0.2526548206806183 + 2.0 * 6.313365459442139
Epoch 690, val loss: 0.7188120484352112
Epoch 700, training loss: 12.868252754211426 = 0.24123956263065338 + 2.0 * 6.313506603240967
Epoch 700, val loss: 0.718316912651062
Epoch 710, training loss: 12.85064697265625 = 0.23035858571529388 + 2.0 * 6.310144424438477
Epoch 710, val loss: 0.7182589769363403
Epoch 720, training loss: 12.849148750305176 = 0.21991084516048431 + 2.0 * 6.314619064331055
Epoch 720, val loss: 0.7185817956924438
Epoch 730, training loss: 12.829012870788574 = 0.21001550555229187 + 2.0 * 6.3094987869262695
Epoch 730, val loss: 0.719261109828949
Epoch 740, training loss: 12.814382553100586 = 0.20055289566516876 + 2.0 * 6.306914806365967
Epoch 740, val loss: 0.7203660607337952
Epoch 750, training loss: 12.802191734313965 = 0.19154195487499237 + 2.0 * 6.305325031280518
Epoch 750, val loss: 0.7218459248542786
Epoch 760, training loss: 12.791647911071777 = 0.182956725358963 + 2.0 * 6.304345607757568
Epoch 760, val loss: 0.7236452698707581
Epoch 770, training loss: 12.797775268554688 = 0.17478981614112854 + 2.0 * 6.311492919921875
Epoch 770, val loss: 0.7257985472679138
Epoch 780, training loss: 12.774802207946777 = 0.16706633567810059 + 2.0 * 6.303867816925049
Epoch 780, val loss: 0.728079617023468
Epoch 790, training loss: 12.76433277130127 = 0.15977731347084045 + 2.0 * 6.302277565002441
Epoch 790, val loss: 0.7308114767074585
Epoch 800, training loss: 12.760917663574219 = 0.152870774269104 + 2.0 * 6.304023265838623
Epoch 800, val loss: 0.7337237000465393
Epoch 810, training loss: 12.746842384338379 = 0.14631181955337524 + 2.0 * 6.300265312194824
Epoch 810, val loss: 0.7368325591087341
Epoch 820, training loss: 12.736417770385742 = 0.14012554287910461 + 2.0 * 6.2981462478637695
Epoch 820, val loss: 0.7401575446128845
Epoch 830, training loss: 12.738184928894043 = 0.13424044847488403 + 2.0 * 6.301972389221191
Epoch 830, val loss: 0.743636429309845
Epoch 840, training loss: 12.732428550720215 = 0.1287037432193756 + 2.0 * 6.3018622398376465
Epoch 840, val loss: 0.7472605109214783
Epoch 850, training loss: 12.716347694396973 = 0.12343538552522659 + 2.0 * 6.296456336975098
Epoch 850, val loss: 0.7509337067604065
Epoch 860, training loss: 12.708122253417969 = 0.11846339702606201 + 2.0 * 6.294829368591309
Epoch 860, val loss: 0.7548291087150574
Epoch 870, training loss: 12.700431823730469 = 0.11372800171375275 + 2.0 * 6.293352127075195
Epoch 870, val loss: 0.7587748169898987
Epoch 880, training loss: 12.712359428405762 = 0.10920692980289459 + 2.0 * 6.301576137542725
Epoch 880, val loss: 0.7627965807914734
Epoch 890, training loss: 12.70091724395752 = 0.10497475415468216 + 2.0 * 6.297971248626709
Epoch 890, val loss: 0.7669112086296082
Epoch 900, training loss: 12.681756973266602 = 0.10091343522071838 + 2.0 * 6.290421962738037
Epoch 900, val loss: 0.7710848450660706
Epoch 910, training loss: 12.677556037902832 = 0.09705843776464462 + 2.0 * 6.290248870849609
Epoch 910, val loss: 0.7752978801727295
Epoch 920, training loss: 12.679503440856934 = 0.09338587522506714 + 2.0 * 6.2930588722229
Epoch 920, val loss: 0.7795493602752686
Epoch 930, training loss: 12.67248821258545 = 0.0898938998579979 + 2.0 * 6.29129695892334
Epoch 930, val loss: 0.7838948369026184
Epoch 940, training loss: 12.666899681091309 = 0.08656419813632965 + 2.0 * 6.290167808532715
Epoch 940, val loss: 0.7881154417991638
Epoch 950, training loss: 12.66140079498291 = 0.08339370042085648 + 2.0 * 6.289003372192383
Epoch 950, val loss: 0.7924842834472656
Epoch 960, training loss: 12.655397415161133 = 0.08037853986024857 + 2.0 * 6.287509441375732
Epoch 960, val loss: 0.7967967987060547
Epoch 970, training loss: 12.653779983520508 = 0.07748919725418091 + 2.0 * 6.288145542144775
Epoch 970, val loss: 0.8010589480400085
Epoch 980, training loss: 12.644129753112793 = 0.0747428685426712 + 2.0 * 6.284693241119385
Epoch 980, val loss: 0.8054527044296265
Epoch 990, training loss: 12.639891624450684 = 0.07211755216121674 + 2.0 * 6.283886909484863
Epoch 990, val loss: 0.8098062872886658
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8111111111111111
0.845018450184502
=== training gcn model ===
Epoch 0, training loss: 19.165254592895508 = 1.9716930389404297 + 2.0 * 8.596780776977539
Epoch 0, val loss: 1.978871464729309
Epoch 10, training loss: 19.15381622314453 = 1.9613244533538818 + 2.0 * 8.596245765686035
Epoch 10, val loss: 1.9677966833114624
Epoch 20, training loss: 19.132366180419922 = 1.9488439559936523 + 2.0 * 8.591760635375977
Epoch 20, val loss: 1.9542856216430664
Epoch 30, training loss: 19.04729652404785 = 1.9322115182876587 + 2.0 * 8.55754280090332
Epoch 30, val loss: 1.9361454248428345
Epoch 40, training loss: 18.529321670532227 = 1.9136978387832642 + 2.0 * 8.307811737060547
Epoch 40, val loss: 1.9168554544448853
Epoch 50, training loss: 17.17566680908203 = 1.8950086832046509 + 2.0 * 7.640329360961914
Epoch 50, val loss: 1.8982385396957397
Epoch 60, training loss: 16.383398056030273 = 1.8818409442901611 + 2.0 * 7.250778675079346
Epoch 60, val loss: 1.8863123655319214
Epoch 70, training loss: 15.8944091796875 = 1.868886113166809 + 2.0 * 7.01276159286499
Epoch 70, val loss: 1.8733937740325928
Epoch 80, training loss: 15.568489074707031 = 1.856325626373291 + 2.0 * 6.856081485748291
Epoch 80, val loss: 1.860764980316162
Epoch 90, training loss: 15.40781307220459 = 1.842327356338501 + 2.0 * 6.782742977142334
Epoch 90, val loss: 1.8466413021087646
Epoch 100, training loss: 15.287266731262207 = 1.8272628784179688 + 2.0 * 6.730001926422119
Epoch 100, val loss: 1.8312873840332031
Epoch 110, training loss: 15.186976432800293 = 1.8123868703842163 + 2.0 * 6.687294960021973
Epoch 110, val loss: 1.816303014755249
Epoch 120, training loss: 15.101509094238281 = 1.7985717058181763 + 2.0 * 6.651468753814697
Epoch 120, val loss: 1.80231511592865
Epoch 130, training loss: 15.018144607543945 = 1.785492181777954 + 2.0 * 6.616326332092285
Epoch 130, val loss: 1.789283037185669
Epoch 140, training loss: 14.952315330505371 = 1.772666335105896 + 2.0 * 6.589824676513672
Epoch 140, val loss: 1.7764469385147095
Epoch 150, training loss: 14.888372421264648 = 1.7593860626220703 + 2.0 * 6.564493179321289
Epoch 150, val loss: 1.7634456157684326
Epoch 160, training loss: 14.82919692993164 = 1.745347499847412 + 2.0 * 6.541924476623535
Epoch 160, val loss: 1.7499959468841553
Epoch 170, training loss: 14.78029727935791 = 1.730309247970581 + 2.0 * 6.524993896484375
Epoch 170, val loss: 1.7359018325805664
Epoch 180, training loss: 14.724225044250488 = 1.713868260383606 + 2.0 * 6.505178451538086
Epoch 180, val loss: 1.7207674980163574
Epoch 190, training loss: 14.672657012939453 = 1.6957001686096191 + 2.0 * 6.488478660583496
Epoch 190, val loss: 1.7044116258621216
Epoch 200, training loss: 14.625336647033691 = 1.6752684116363525 + 2.0 * 6.475034236907959
Epoch 200, val loss: 1.6863000392913818
Epoch 210, training loss: 14.577801704406738 = 1.652368426322937 + 2.0 * 6.462716579437256
Epoch 210, val loss: 1.66605806350708
Epoch 220, training loss: 14.530791282653809 = 1.6267024278640747 + 2.0 * 6.452044486999512
Epoch 220, val loss: 1.6436080932617188
Epoch 230, training loss: 14.480911254882812 = 1.5979294776916504 + 2.0 * 6.441490650177002
Epoch 230, val loss: 1.6184360980987549
Epoch 240, training loss: 14.431427955627441 = 1.565683364868164 + 2.0 * 6.432872295379639
Epoch 240, val loss: 1.5904308557510376
Epoch 250, training loss: 14.380964279174805 = 1.5298631191253662 + 2.0 * 6.42555046081543
Epoch 250, val loss: 1.5594909191131592
Epoch 260, training loss: 14.327208518981934 = 1.4901670217514038 + 2.0 * 6.418520927429199
Epoch 260, val loss: 1.5254237651824951
Epoch 270, training loss: 14.271456718444824 = 1.4466150999069214 + 2.0 * 6.412420749664307
Epoch 270, val loss: 1.4882692098617554
Epoch 280, training loss: 14.212194442749023 = 1.399415373802185 + 2.0 * 6.4063897132873535
Epoch 280, val loss: 1.4482192993164062
Epoch 290, training loss: 14.150577545166016 = 1.34903883934021 + 2.0 * 6.400769233703613
Epoch 290, val loss: 1.4056785106658936
Epoch 300, training loss: 14.090065002441406 = 1.2962394952774048 + 2.0 * 6.396912574768066
Epoch 300, val loss: 1.3616459369659424
Epoch 310, training loss: 14.023813247680664 = 1.2421596050262451 + 2.0 * 6.39082670211792
Epoch 310, val loss: 1.3165068626403809
Epoch 320, training loss: 13.961819648742676 = 1.1869202852249146 + 2.0 * 6.387449741363525
Epoch 320, val loss: 1.2707222700119019
Epoch 330, training loss: 13.903485298156738 = 1.1316096782684326 + 2.0 * 6.385937690734863
Epoch 330, val loss: 1.225339651107788
Epoch 340, training loss: 13.835748672485352 = 1.077724575996399 + 2.0 * 6.379012107849121
Epoch 340, val loss: 1.1813186407089233
Epoch 350, training loss: 13.774069786071777 = 1.0254677534103394 + 2.0 * 6.374300956726074
Epoch 350, val loss: 1.1391407251358032
Epoch 360, training loss: 13.720236778259277 = 0.975432813167572 + 2.0 * 6.372402191162109
Epoch 360, val loss: 1.0991374254226685
Epoch 370, training loss: 13.673418998718262 = 0.9287592768669128 + 2.0 * 6.3723297119140625
Epoch 370, val loss: 1.0621981620788574
Epoch 380, training loss: 13.615232467651367 = 0.8860098123550415 + 2.0 * 6.3646111488342285
Epoch 380, val loss: 1.0289535522460938
Epoch 390, training loss: 13.568205833435059 = 0.8466752767562866 + 2.0 * 6.36076545715332
Epoch 390, val loss: 0.9990342855453491
Epoch 400, training loss: 13.526511192321777 = 0.8103410601615906 + 2.0 * 6.3580851554870605
Epoch 400, val loss: 0.9721322655677795
Epoch 410, training loss: 13.490272521972656 = 0.776902973651886 + 2.0 * 6.356684684753418
Epoch 410, val loss: 0.9482480883598328
Epoch 420, training loss: 13.450662612915039 = 0.7463684678077698 + 2.0 * 6.352147102355957
Epoch 420, val loss: 0.9271658062934875
Epoch 430, training loss: 13.419577598571777 = 0.7179906368255615 + 2.0 * 6.350793361663818
Epoch 430, val loss: 0.9084720611572266
Epoch 440, training loss: 13.388794898986816 = 0.6914194226264954 + 2.0 * 6.348687648773193
Epoch 440, val loss: 0.8917602896690369
Epoch 450, training loss: 13.355618476867676 = 0.6664785146713257 + 2.0 * 6.344570159912109
Epoch 450, val loss: 0.8766883015632629
Epoch 460, training loss: 13.328361511230469 = 0.6426548957824707 + 2.0 * 6.342853546142578
Epoch 460, val loss: 0.8629689812660217
Epoch 470, training loss: 13.302165985107422 = 0.6199055910110474 + 2.0 * 6.341130256652832
Epoch 470, val loss: 0.8503556847572327
Epoch 480, training loss: 13.274096488952637 = 0.5980590581893921 + 2.0 * 6.338018894195557
Epoch 480, val loss: 0.8388091325759888
Epoch 490, training loss: 13.252252578735352 = 0.5768586993217468 + 2.0 * 6.3376970291137695
Epoch 490, val loss: 0.8280311822891235
Epoch 500, training loss: 13.227231979370117 = 0.5561586618423462 + 2.0 * 6.335536479949951
Epoch 500, val loss: 0.8179494142532349
Epoch 510, training loss: 13.19790267944336 = 0.535963773727417 + 2.0 * 6.330969333648682
Epoch 510, val loss: 0.8085263967514038
Epoch 520, training loss: 13.175267219543457 = 0.5160702466964722 + 2.0 * 6.329598426818848
Epoch 520, val loss: 0.7997057437896729
Epoch 530, training loss: 13.149762153625488 = 0.4964921772480011 + 2.0 * 6.326634883880615
Epoch 530, val loss: 0.7914270758628845
Epoch 540, training loss: 13.128609657287598 = 0.4773010015487671 + 2.0 * 6.32565450668335
Epoch 540, val loss: 0.7837672829627991
Epoch 550, training loss: 13.124537467956543 = 0.4584788680076599 + 2.0 * 6.333029270172119
Epoch 550, val loss: 0.7766472101211548
Epoch 560, training loss: 13.082964897155762 = 0.44002586603164673 + 2.0 * 6.321469306945801
Epoch 560, val loss: 0.7702791094779968
Epoch 570, training loss: 13.062952995300293 = 0.4220825731754303 + 2.0 * 6.320435047149658
Epoch 570, val loss: 0.7645750641822815
Epoch 580, training loss: 13.045126914978027 = 0.4045828580856323 + 2.0 * 6.320271968841553
Epoch 580, val loss: 0.7595220804214478
Epoch 590, training loss: 13.033838272094727 = 0.3875328004360199 + 2.0 * 6.323152542114258
Epoch 590, val loss: 0.7551573514938354
Epoch 600, training loss: 13.00438404083252 = 0.3712836503982544 + 2.0 * 6.316550254821777
Epoch 600, val loss: 0.7515466213226318
Epoch 610, training loss: 12.982756614685059 = 0.3555693030357361 + 2.0 * 6.313593864440918
Epoch 610, val loss: 0.7485655546188354
Epoch 620, training loss: 12.968123435974121 = 0.34041544795036316 + 2.0 * 6.313854217529297
Epoch 620, val loss: 0.7462686896324158
Epoch 630, training loss: 12.959922790527344 = 0.32584065198898315 + 2.0 * 6.317040920257568
Epoch 630, val loss: 0.7445483803749084
Epoch 640, training loss: 12.933403968811035 = 0.31199023127555847 + 2.0 * 6.310707092285156
Epoch 640, val loss: 0.7435416579246521
Epoch 650, training loss: 12.915663719177246 = 0.29870104789733887 + 2.0 * 6.308481216430664
Epoch 650, val loss: 0.743111252784729
Epoch 660, training loss: 12.899821281433105 = 0.2859293818473816 + 2.0 * 6.30694580078125
Epoch 660, val loss: 0.7432374358177185
Epoch 670, training loss: 12.898582458496094 = 0.2736603915691376 + 2.0 * 6.312460899353027
Epoch 670, val loss: 0.7438548803329468
Epoch 680, training loss: 12.877588272094727 = 0.2620440721511841 + 2.0 * 6.307772159576416
Epoch 680, val loss: 0.7449633479118347
Epoch 690, training loss: 12.858898162841797 = 0.2509888708591461 + 2.0 * 6.303954601287842
Epoch 690, val loss: 0.7464963793754578
Epoch 700, training loss: 12.844528198242188 = 0.2403794825077057 + 2.0 * 6.302074432373047
Epoch 700, val loss: 0.7484483122825623
Epoch 710, training loss: 12.830767631530762 = 0.23021085560321808 + 2.0 * 6.300278186798096
Epoch 710, val loss: 0.7507736682891846
Epoch 720, training loss: 12.843560218811035 = 0.22048434615135193 + 2.0 * 6.311537742614746
Epoch 720, val loss: 0.7534117102622986
Epoch 730, training loss: 12.809314727783203 = 0.2111685574054718 + 2.0 * 6.299073219299316
Epoch 730, val loss: 0.756428062915802
Epoch 740, training loss: 12.79832649230957 = 0.20239770412445068 + 2.0 * 6.297964572906494
Epoch 740, val loss: 0.7595962285995483
Epoch 750, training loss: 12.788002014160156 = 0.19398406147956848 + 2.0 * 6.297008991241455
Epoch 750, val loss: 0.7631126046180725
Epoch 760, training loss: 12.776493072509766 = 0.18591462075710297 + 2.0 * 6.295289039611816
Epoch 760, val loss: 0.766910195350647
Epoch 770, training loss: 12.771428108215332 = 0.17817465960979462 + 2.0 * 6.296626567840576
Epoch 770, val loss: 0.7709347009658813
Epoch 780, training loss: 12.759407997131348 = 0.17079655826091766 + 2.0 * 6.294305801391602
Epoch 780, val loss: 0.7751083374023438
Epoch 790, training loss: 12.751603126525879 = 0.1637541651725769 + 2.0 * 6.293924331665039
Epoch 790, val loss: 0.7794442176818848
Epoch 800, training loss: 12.740042686462402 = 0.15703971683979034 + 2.0 * 6.291501522064209
Epoch 800, val loss: 0.7839422821998596
Epoch 810, training loss: 12.741992950439453 = 0.15060122311115265 + 2.0 * 6.295695781707764
Epoch 810, val loss: 0.7885626554489136
Epoch 820, training loss: 12.725953102111816 = 0.14444468915462494 + 2.0 * 6.290754318237305
Epoch 820, val loss: 0.7933191657066345
Epoch 830, training loss: 12.715149879455566 = 0.13859790563583374 + 2.0 * 6.288276195526123
Epoch 830, val loss: 0.7981625199317932
Epoch 840, training loss: 12.707932472229004 = 0.1330033540725708 + 2.0 * 6.287464618682861
Epoch 840, val loss: 0.8031595945358276
Epoch 850, training loss: 12.716798782348633 = 0.12764449417591095 + 2.0 * 6.294577121734619
Epoch 850, val loss: 0.8083187341690063
Epoch 860, training loss: 12.7027006149292 = 0.12255094945430756 + 2.0 * 6.290074825286865
Epoch 860, val loss: 0.8134175539016724
Epoch 870, training loss: 12.688650131225586 = 0.11768920719623566 + 2.0 * 6.285480499267578
Epoch 870, val loss: 0.8186545372009277
Epoch 880, training loss: 12.681180000305176 = 0.11306235939264297 + 2.0 * 6.284059047698975
Epoch 880, val loss: 0.8240002393722534
Epoch 890, training loss: 12.6788911819458 = 0.10863412171602249 + 2.0 * 6.285128593444824
Epoch 890, val loss: 0.8293114900588989
Epoch 900, training loss: 12.67423152923584 = 0.10441280901432037 + 2.0 * 6.284909248352051
Epoch 900, val loss: 0.8347617387771606
Epoch 910, training loss: 12.666869163513184 = 0.10038824379444122 + 2.0 * 6.28324031829834
Epoch 910, val loss: 0.8402343988418579
Epoch 920, training loss: 12.660953521728516 = 0.09655410796403885 + 2.0 * 6.282199859619141
Epoch 920, val loss: 0.8456333875656128
Epoch 930, training loss: 12.653388023376465 = 0.09289903193712234 + 2.0 * 6.28024435043335
Epoch 930, val loss: 0.8512247800827026
Epoch 940, training loss: 12.65497875213623 = 0.08940683305263519 + 2.0 * 6.282785892486572
Epoch 940, val loss: 0.8567155003547668
Epoch 950, training loss: 12.646369934082031 = 0.08608318865299225 + 2.0 * 6.2801432609558105
Epoch 950, val loss: 0.8622609376907349
Epoch 960, training loss: 12.638554573059082 = 0.08291375637054443 + 2.0 * 6.277820587158203
Epoch 960, val loss: 0.8677983283996582
Epoch 970, training loss: 12.638818740844727 = 0.07989604771137238 + 2.0 * 6.27946138381958
Epoch 970, val loss: 0.8733808398246765
Epoch 980, training loss: 12.628275871276855 = 0.0770072266459465 + 2.0 * 6.275634288787842
Epoch 980, val loss: 0.8788964748382568
Epoch 990, training loss: 12.628462791442871 = 0.0742507204413414 + 2.0 * 6.277105808258057
Epoch 990, val loss: 0.884354293346405
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8037037037037037
0.8292040063257776
=== training gcn model ===
Epoch 0, training loss: 19.146753311157227 = 1.953157663345337 + 2.0 * 8.596797943115234
Epoch 0, val loss: 1.9462465047836304
Epoch 10, training loss: 19.13479232788086 = 1.9421970844268799 + 2.0 * 8.596297264099121
Epoch 10, val loss: 1.9346623420715332
Epoch 20, training loss: 19.113616943359375 = 1.9285482168197632 + 2.0 * 8.592534065246582
Epoch 20, val loss: 1.9202162027359009
Epoch 30, training loss: 19.04013442993164 = 1.910029411315918 + 2.0 * 8.565052032470703
Epoch 30, val loss: 1.9008454084396362
Epoch 40, training loss: 18.65487289428711 = 1.8880746364593506 + 2.0 * 8.38339900970459
Epoch 40, val loss: 1.879159688949585
Epoch 50, training loss: 17.559038162231445 = 1.8631765842437744 + 2.0 * 7.847930908203125
Epoch 50, val loss: 1.855047583580017
Epoch 60, training loss: 16.681554794311523 = 1.844783902168274 + 2.0 * 7.4183855056762695
Epoch 60, val loss: 1.8393104076385498
Epoch 70, training loss: 16.087636947631836 = 1.8321399688720703 + 2.0 * 7.127748489379883
Epoch 70, val loss: 1.8277745246887207
Epoch 80, training loss: 15.666840553283691 = 1.819029450416565 + 2.0 * 6.923905372619629
Epoch 80, val loss: 1.8161091804504395
Epoch 90, training loss: 15.481232643127441 = 1.8054925203323364 + 2.0 * 6.837870121002197
Epoch 90, val loss: 1.8041119575500488
Epoch 100, training loss: 15.332481384277344 = 1.7915321588516235 + 2.0 * 6.770474433898926
Epoch 100, val loss: 1.7922719717025757
Epoch 110, training loss: 15.221962928771973 = 1.7793830633163452 + 2.0 * 6.721290111541748
Epoch 110, val loss: 1.7822731733322144
Epoch 120, training loss: 15.123077392578125 = 1.768511414527893 + 2.0 * 6.677282810211182
Epoch 120, val loss: 1.7732293605804443
Epoch 130, training loss: 15.028311729431152 = 1.7580121755599976 + 2.0 * 6.635149955749512
Epoch 130, val loss: 1.7643182277679443
Epoch 140, training loss: 14.950202941894531 = 1.7470133304595947 + 2.0 * 6.601594924926758
Epoch 140, val loss: 1.755064845085144
Epoch 150, training loss: 14.883777618408203 = 1.734729290008545 + 2.0 * 6.574524402618408
Epoch 150, val loss: 1.7449523210525513
Epoch 160, training loss: 14.823185920715332 = 1.7207900285720825 + 2.0 * 6.5511980056762695
Epoch 160, val loss: 1.7336574792861938
Epoch 170, training loss: 14.768527030944824 = 1.7050024271011353 + 2.0 * 6.53176212310791
Epoch 170, val loss: 1.7209289073944092
Epoch 180, training loss: 14.718623161315918 = 1.6870774030685425 + 2.0 * 6.515772819519043
Epoch 180, val loss: 1.7065026760101318
Epoch 190, training loss: 14.663363456726074 = 1.6668336391448975 + 2.0 * 6.498264789581299
Epoch 190, val loss: 1.6902579069137573
Epoch 200, training loss: 14.610986709594727 = 1.6438292264938354 + 2.0 * 6.483578681945801
Epoch 200, val loss: 1.671851396560669
Epoch 210, training loss: 14.558972358703613 = 1.6175663471221924 + 2.0 * 6.470703125
Epoch 210, val loss: 1.6509079933166504
Epoch 220, training loss: 14.506590843200684 = 1.588042140007019 + 2.0 * 6.4592742919921875
Epoch 220, val loss: 1.627414345741272
Epoch 230, training loss: 14.452619552612305 = 1.5553447008132935 + 2.0 * 6.44863748550415
Epoch 230, val loss: 1.6015210151672363
Epoch 240, training loss: 14.39778995513916 = 1.5194804668426514 + 2.0 * 6.439154624938965
Epoch 240, val loss: 1.5733094215393066
Epoch 250, training loss: 14.34395980834961 = 1.4807398319244385 + 2.0 * 6.431610107421875
Epoch 250, val loss: 1.542964220046997
Epoch 260, training loss: 14.293363571166992 = 1.440027117729187 + 2.0 * 6.426668167114258
Epoch 260, val loss: 1.5113273859024048
Epoch 270, training loss: 14.234368324279785 = 1.3982956409454346 + 2.0 * 6.418036460876465
Epoch 270, val loss: 1.4792149066925049
Epoch 280, training loss: 14.179296493530273 = 1.3559842109680176 + 2.0 * 6.411656379699707
Epoch 280, val loss: 1.446982502937317
Epoch 290, training loss: 14.128863334655762 = 1.3135337829589844 + 2.0 * 6.407664775848389
Epoch 290, val loss: 1.415076732635498
Epoch 300, training loss: 14.078522682189941 = 1.2718786001205444 + 2.0 * 6.403322219848633
Epoch 300, val loss: 1.384223461151123
Epoch 310, training loss: 14.02547836303711 = 1.231455683708191 + 2.0 * 6.3970112800598145
Epoch 310, val loss: 1.3546605110168457
Epoch 320, training loss: 13.980975151062012 = 1.1920509338378906 + 2.0 * 6.3944621086120605
Epoch 320, val loss: 1.326279878616333
Epoch 330, training loss: 13.93040657043457 = 1.154128074645996 + 2.0 * 6.388139247894287
Epoch 330, val loss: 1.2994344234466553
Epoch 340, training loss: 13.884811401367188 = 1.1176496744155884 + 2.0 * 6.383580684661865
Epoch 340, val loss: 1.273951768875122
Epoch 350, training loss: 13.84031867980957 = 1.082223892211914 + 2.0 * 6.379047393798828
Epoch 350, val loss: 1.2496217489242554
Epoch 360, training loss: 13.800474166870117 = 1.047910213470459 + 2.0 * 6.376282215118408
Epoch 360, val loss: 1.2262089252471924
Epoch 370, training loss: 13.76353645324707 = 1.0149388313293457 + 2.0 * 6.374299049377441
Epoch 370, val loss: 1.2039110660552979
Epoch 380, training loss: 13.719742774963379 = 0.9828653931617737 + 2.0 * 6.368438720703125
Epoch 380, val loss: 1.182544469833374
Epoch 390, training loss: 13.6823091506958 = 0.9513218998908997 + 2.0 * 6.3654937744140625
Epoch 390, val loss: 1.1616220474243164
Epoch 400, training loss: 13.647478103637695 = 0.9203214645385742 + 2.0 * 6.3635783195495605
Epoch 400, val loss: 1.1412341594696045
Epoch 410, training loss: 13.60901927947998 = 0.8900797963142395 + 2.0 * 6.359469890594482
Epoch 410, val loss: 1.1213189363479614
Epoch 420, training loss: 13.574265480041504 = 0.8604406714439392 + 2.0 * 6.356912612915039
Epoch 420, val loss: 1.1019303798675537
Epoch 430, training loss: 13.541553497314453 = 0.8312758207321167 + 2.0 * 6.355138778686523
Epoch 430, val loss: 1.0828436613082886
Epoch 440, training loss: 13.505987167358398 = 0.8028877973556519 + 2.0 * 6.3515496253967285
Epoch 440, val loss: 1.0643466711044312
Epoch 450, training loss: 13.474164009094238 = 0.7751703858375549 + 2.0 * 6.349496841430664
Epoch 450, val loss: 1.0462889671325684
Epoch 460, training loss: 13.449356079101562 = 0.748354971408844 + 2.0 * 6.350500583648682
Epoch 460, val loss: 1.0290542840957642
Epoch 470, training loss: 13.413228988647461 = 0.7224834561347961 + 2.0 * 6.345372676849365
Epoch 470, val loss: 1.0125939846038818
Epoch 480, training loss: 13.381982803344727 = 0.6975725293159485 + 2.0 * 6.342205047607422
Epoch 480, val loss: 0.9971580505371094
Epoch 490, training loss: 13.356614112854004 = 0.6734923720359802 + 2.0 * 6.3415608406066895
Epoch 490, val loss: 0.9824792742729187
Epoch 500, training loss: 13.327210426330566 = 0.6502984166145325 + 2.0 * 6.338456153869629
Epoch 500, val loss: 0.9686880707740784
Epoch 510, training loss: 13.301973342895508 = 0.6280617713928223 + 2.0 * 6.336955547332764
Epoch 510, val loss: 0.9558277130126953
Epoch 520, training loss: 13.274524688720703 = 0.6066426038742065 + 2.0 * 6.3339409828186035
Epoch 520, val loss: 0.9438719153404236
Epoch 530, training loss: 13.258466720581055 = 0.5859427452087402 + 2.0 * 6.336261749267578
Epoch 530, val loss: 0.9327781796455383
Epoch 540, training loss: 13.234626770019531 = 0.565930962562561 + 2.0 * 6.334347724914551
Epoch 540, val loss: 0.922295868396759
Epoch 550, training loss: 13.203927040100098 = 0.5466349720954895 + 2.0 * 6.328646183013916
Epoch 550, val loss: 0.9126619100570679
Epoch 560, training loss: 13.181692123413086 = 0.52785325050354 + 2.0 * 6.3269195556640625
Epoch 560, val loss: 0.9036678075790405
Epoch 570, training loss: 13.160548210144043 = 0.5095977783203125 + 2.0 * 6.325475215911865
Epoch 570, val loss: 0.8954185843467712
Epoch 580, training loss: 13.137237548828125 = 0.49203920364379883 + 2.0 * 6.322598934173584
Epoch 580, val loss: 0.8878910541534424
Epoch 590, training loss: 13.117589950561523 = 0.4749973714351654 + 2.0 * 6.321296215057373
Epoch 590, val loss: 0.8809994459152222
Epoch 600, training loss: 13.116338729858398 = 0.4584593176841736 + 2.0 * 6.328939914703369
Epoch 600, val loss: 0.8747617602348328
Epoch 610, training loss: 13.079240798950195 = 0.4425572454929352 + 2.0 * 6.3183417320251465
Epoch 610, val loss: 0.8691692352294922
Epoch 620, training loss: 13.066323280334473 = 0.4272204637527466 + 2.0 * 6.319551467895508
Epoch 620, val loss: 0.864266574382782
Epoch 630, training loss: 13.044792175292969 = 0.412443608045578 + 2.0 * 6.316174507141113
Epoch 630, val loss: 0.859982967376709
Epoch 640, training loss: 13.027387619018555 = 0.39821144938468933 + 2.0 * 6.3145880699157715
Epoch 640, val loss: 0.8561754822731018
Epoch 650, training loss: 13.012948989868164 = 0.3844582736492157 + 2.0 * 6.314245223999023
Epoch 650, val loss: 0.8529402017593384
Epoch 660, training loss: 12.994481086730957 = 0.3711887300014496 + 2.0 * 6.311645984649658
Epoch 660, val loss: 0.8501105904579163
Epoch 670, training loss: 12.98122501373291 = 0.35847556591033936 + 2.0 * 6.311374664306641
Epoch 670, val loss: 0.8478619456291199
Epoch 680, training loss: 12.962864875793457 = 0.3461683690547943 + 2.0 * 6.308348178863525
Epoch 680, val loss: 0.8460108637809753
Epoch 690, training loss: 12.961777687072754 = 0.3343002498149872 + 2.0 * 6.313738822937012
Epoch 690, val loss: 0.8445116877555847
Epoch 700, training loss: 12.936869621276855 = 0.3229551911354065 + 2.0 * 6.306957244873047
Epoch 700, val loss: 0.8434838652610779
Epoch 710, training loss: 12.922719955444336 = 0.3119683861732483 + 2.0 * 6.305375576019287
Epoch 710, val loss: 0.842779815196991
Epoch 720, training loss: 12.919083595275879 = 0.3014061748981476 + 2.0 * 6.308838844299316
Epoch 720, val loss: 0.8422996401786804
Epoch 730, training loss: 12.898900985717773 = 0.29130351543426514 + 2.0 * 6.303798675537109
Epoch 730, val loss: 0.8421764373779297
Epoch 740, training loss: 12.885273933410645 = 0.28154754638671875 + 2.0 * 6.301863193511963
Epoch 740, val loss: 0.8423123955726624
Epoch 750, training loss: 12.873092651367188 = 0.2720816433429718 + 2.0 * 6.300505638122559
Epoch 750, val loss: 0.8426176905632019
Epoch 760, training loss: 12.862818717956543 = 0.26288896799087524 + 2.0 * 6.299964904785156
Epoch 760, val loss: 0.8431594371795654
Epoch 770, training loss: 12.852865219116211 = 0.25395911931991577 + 2.0 * 6.299453258514404
Epoch 770, val loss: 0.8438031077384949
Epoch 780, training loss: 12.845973014831543 = 0.24532541632652283 + 2.0 * 6.300323963165283
Epoch 780, val loss: 0.8446773290634155
Epoch 790, training loss: 12.830482482910156 = 0.23691222071647644 + 2.0 * 6.296785354614258
Epoch 790, val loss: 0.8456574082374573
Epoch 800, training loss: 12.821080207824707 = 0.22867321968078613 + 2.0 * 6.29620361328125
Epoch 800, val loss: 0.8466871380805969
Epoch 810, training loss: 12.811442375183105 = 0.220584437251091 + 2.0 * 6.29542875289917
Epoch 810, val loss: 0.8477261662483215
Epoch 820, training loss: 12.800668716430664 = 0.2126384675502777 + 2.0 * 6.294014930725098
Epoch 820, val loss: 0.8488999605178833
Epoch 830, training loss: 12.798389434814453 = 0.20481328666210175 + 2.0 * 6.296788215637207
Epoch 830, val loss: 0.8501306772232056
Epoch 840, training loss: 12.78364086151123 = 0.1971110850572586 + 2.0 * 6.293264865875244
Epoch 840, val loss: 0.8513075113296509
Epoch 850, training loss: 12.771172523498535 = 0.1895453929901123 + 2.0 * 6.290813446044922
Epoch 850, val loss: 0.8525830507278442
Epoch 860, training loss: 12.761276245117188 = 0.18209579586982727 + 2.0 * 6.289590358734131
Epoch 860, val loss: 0.8539554476737976
Epoch 870, training loss: 12.75422477722168 = 0.17477640509605408 + 2.0 * 6.289724349975586
Epoch 870, val loss: 0.8553799986839294
Epoch 880, training loss: 12.750598907470703 = 0.16760846972465515 + 2.0 * 6.291495323181152
Epoch 880, val loss: 0.856749951839447
Epoch 890, training loss: 12.74014663696289 = 0.16063444316387177 + 2.0 * 6.2897562980651855
Epoch 890, val loss: 0.858269214630127
Epoch 900, training loss: 12.733236312866211 = 0.15386204421520233 + 2.0 * 6.289687156677246
Epoch 900, val loss: 0.8598243594169617
Epoch 910, training loss: 12.719822883605957 = 0.14731813967227936 + 2.0 * 6.286252498626709
Epoch 910, val loss: 0.8613410592079163
Epoch 920, training loss: 12.711941719055176 = 0.14100360870361328 + 2.0 * 6.285469055175781
Epoch 920, val loss: 0.863007128238678
Epoch 930, training loss: 12.712146759033203 = 0.13493593037128448 + 2.0 * 6.288605213165283
Epoch 930, val loss: 0.8648160696029663
Epoch 940, training loss: 12.703865051269531 = 0.12916496396064758 + 2.0 * 6.287350177764893
Epoch 940, val loss: 0.8667302131652832
Epoch 950, training loss: 12.693316459655762 = 0.12365180999040604 + 2.0 * 6.28483247756958
Epoch 950, val loss: 0.8686562180519104
Epoch 960, training loss: 12.682607650756836 = 0.11843233555555344 + 2.0 * 6.282087802886963
Epoch 960, val loss: 0.8708122968673706
Epoch 970, training loss: 12.678653717041016 = 0.11345397680997849 + 2.0 * 6.282599925994873
Epoch 970, val loss: 0.8730461001396179
Epoch 980, training loss: 12.670614242553711 = 0.10876987129449844 + 2.0 * 6.2809224128723145
Epoch 980, val loss: 0.8753005862236023
Epoch 990, training loss: 12.668010711669922 = 0.10432207584381104 + 2.0 * 6.281844139099121
Epoch 990, val loss: 0.8777506351470947
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7814814814814816
0.8371112282551397
The final CL Acc:0.79877, 0.01259, The final GNN Acc:0.83711, 0.00646
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.5 der2:0.2 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11622])
remove edge: torch.Size([2, 9472])
updated graph: torch.Size([2, 10538])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 19.14854621887207 = 1.9548436403274536 + 2.0 * 8.596851348876953
Epoch 0, val loss: 1.950652003288269
Epoch 10, training loss: 19.136905670166016 = 1.943821668624878 + 2.0 * 8.596542358398438
Epoch 10, val loss: 1.9400132894515991
Epoch 20, training loss: 19.118886947631836 = 1.9301820993423462 + 2.0 * 8.594352722167969
Epoch 20, val loss: 1.926314115524292
Epoch 30, training loss: 19.06727409362793 = 1.9116075038909912 + 2.0 * 8.57783317565918
Epoch 30, val loss: 1.907194972038269
Epoch 40, training loss: 18.816740036010742 = 1.888495922088623 + 2.0 * 8.46412181854248
Epoch 40, val loss: 1.8841358423233032
Epoch 50, training loss: 17.61250114440918 = 1.865334391593933 + 2.0 * 7.8735833168029785
Epoch 50, val loss: 1.8617230653762817
Epoch 60, training loss: 16.62457275390625 = 1.8488483428955078 + 2.0 * 7.387862682342529
Epoch 60, val loss: 1.8474361896514893
Epoch 70, training loss: 16.166025161743164 = 1.8345814943313599 + 2.0 * 7.165721416473389
Epoch 70, val loss: 1.834159016609192
Epoch 80, training loss: 15.89671516418457 = 1.819063425064087 + 2.0 * 7.038825988769531
Epoch 80, val loss: 1.819843053817749
Epoch 90, training loss: 15.688423156738281 = 1.803582787513733 + 2.0 * 6.94242000579834
Epoch 90, val loss: 1.8063722848892212
Epoch 100, training loss: 15.489935874938965 = 1.7902287244796753 + 2.0 * 6.849853515625
Epoch 100, val loss: 1.7952160835266113
Epoch 110, training loss: 15.344947814941406 = 1.777817964553833 + 2.0 * 6.783565044403076
Epoch 110, val loss: 1.784616470336914
Epoch 120, training loss: 15.23798656463623 = 1.7648861408233643 + 2.0 * 6.736550331115723
Epoch 120, val loss: 1.7733906507492065
Epoch 130, training loss: 15.131979942321777 = 1.7511060237884521 + 2.0 * 6.690436840057373
Epoch 130, val loss: 1.761638879776001
Epoch 140, training loss: 15.041937828063965 = 1.7365039587020874 + 2.0 * 6.652717113494873
Epoch 140, val loss: 1.7494795322418213
Epoch 150, training loss: 14.966968536376953 = 1.7206014394760132 + 2.0 * 6.623183727264404
Epoch 150, val loss: 1.7364914417266846
Epoch 160, training loss: 14.897775650024414 = 1.702904224395752 + 2.0 * 6.597435474395752
Epoch 160, val loss: 1.7222554683685303
Epoch 170, training loss: 14.833724975585938 = 1.683021903038025 + 2.0 * 6.575351715087891
Epoch 170, val loss: 1.7064404487609863
Epoch 180, training loss: 14.77834701538086 = 1.6607892513275146 + 2.0 * 6.558778762817383
Epoch 180, val loss: 1.6889036893844604
Epoch 190, training loss: 14.722168922424316 = 1.6361404657363892 + 2.0 * 6.543014049530029
Epoch 190, val loss: 1.6696557998657227
Epoch 200, training loss: 14.665874481201172 = 1.60896635055542 + 2.0 * 6.528454303741455
Epoch 200, val loss: 1.648504376411438
Epoch 210, training loss: 14.616037368774414 = 1.5788273811340332 + 2.0 * 6.5186052322387695
Epoch 210, val loss: 1.6250947713851929
Epoch 220, training loss: 14.558595657348633 = 1.5458611249923706 + 2.0 * 6.506367206573486
Epoch 220, val loss: 1.5996589660644531
Epoch 230, training loss: 14.500877380371094 = 1.5108025074005127 + 2.0 * 6.49503755569458
Epoch 230, val loss: 1.5728223323822021
Epoch 240, training loss: 14.445635795593262 = 1.473704218864441 + 2.0 * 6.485965728759766
Epoch 240, val loss: 1.5447735786437988
Epoch 250, training loss: 14.392935752868652 = 1.4356319904327393 + 2.0 * 6.478652000427246
Epoch 250, val loss: 1.516907811164856
Epoch 260, training loss: 14.336241722106934 = 1.3973829746246338 + 2.0 * 6.4694294929504395
Epoch 260, val loss: 1.4896093606948853
Epoch 270, training loss: 14.280777931213379 = 1.3595105409622192 + 2.0 * 6.460633754730225
Epoch 270, val loss: 1.4633209705352783
Epoch 280, training loss: 14.22730541229248 = 1.3222194910049438 + 2.0 * 6.452542781829834
Epoch 280, val loss: 1.4386053085327148
Epoch 290, training loss: 14.190897941589355 = 1.2856868505477905 + 2.0 * 6.452605724334717
Epoch 290, val loss: 1.41546630859375
Epoch 300, training loss: 14.13260555267334 = 1.250529170036316 + 2.0 * 6.441038131713867
Epoch 300, val loss: 1.3939369916915894
Epoch 310, training loss: 14.082494735717773 = 1.2160687446594238 + 2.0 * 6.433212757110596
Epoch 310, val loss: 1.3734275102615356
Epoch 320, training loss: 14.038414001464844 = 1.182195782661438 + 2.0 * 6.428109169006348
Epoch 320, val loss: 1.3538951873779297
Epoch 330, training loss: 13.992044448852539 = 1.1485285758972168 + 2.0 * 6.42175817489624
Epoch 330, val loss: 1.3346948623657227
Epoch 340, training loss: 13.948946952819824 = 1.1147514581680298 + 2.0 * 6.417097568511963
Epoch 340, val loss: 1.315673589706421
Epoch 350, training loss: 13.906341552734375 = 1.080681562423706 + 2.0 * 6.412829875946045
Epoch 350, val loss: 1.296386480331421
Epoch 360, training loss: 13.861600875854492 = 1.0464094877243042 + 2.0 * 6.407595634460449
Epoch 360, val loss: 1.2770167589187622
Epoch 370, training loss: 13.819106101989746 = 1.011749267578125 + 2.0 * 6.4036784172058105
Epoch 370, val loss: 1.2574721574783325
Epoch 380, training loss: 13.781806945800781 = 0.9767851829528809 + 2.0 * 6.402511119842529
Epoch 380, val loss: 1.2376450300216675
Epoch 390, training loss: 13.734491348266602 = 0.9419094324111938 + 2.0 * 6.3962907791137695
Epoch 390, val loss: 1.217866063117981
Epoch 400, training loss: 13.69139575958252 = 0.9072539806365967 + 2.0 * 6.392070770263672
Epoch 400, val loss: 1.1983708143234253
Epoch 410, training loss: 13.650437355041504 = 0.8731260299682617 + 2.0 * 6.388655662536621
Epoch 410, val loss: 1.1792744398117065
Epoch 420, training loss: 13.612377166748047 = 0.8397620320320129 + 2.0 * 6.386307716369629
Epoch 420, val loss: 1.1609076261520386
Epoch 430, training loss: 13.571710586547852 = 0.8076291680335999 + 2.0 * 6.382040500640869
Epoch 430, val loss: 1.143567681312561
Epoch 440, training loss: 13.53494644165039 = 0.7767131328582764 + 2.0 * 6.379116535186768
Epoch 440, val loss: 1.1275053024291992
Epoch 450, training loss: 13.504122734069824 = 0.747075617313385 + 2.0 * 6.378523349761963
Epoch 450, val loss: 1.112622618675232
Epoch 460, training loss: 13.470575332641602 = 0.7187231779098511 + 2.0 * 6.3759260177612305
Epoch 460, val loss: 1.0989625453948975
Epoch 470, training loss: 13.435572624206543 = 0.6916065812110901 + 2.0 * 6.371983051300049
Epoch 470, val loss: 1.0863819122314453
Epoch 480, training loss: 13.404037475585938 = 0.6655370593070984 + 2.0 * 6.369250297546387
Epoch 480, val loss: 1.0750445127487183
Epoch 490, training loss: 13.377204895019531 = 0.6404327154159546 + 2.0 * 6.368386268615723
Epoch 490, val loss: 1.0646672248840332
Epoch 500, training loss: 13.350032806396484 = 0.616197407245636 + 2.0 * 6.366917610168457
Epoch 500, val loss: 1.0552978515625
Epoch 510, training loss: 13.317063331604004 = 0.5928061604499817 + 2.0 * 6.362128734588623
Epoch 510, val loss: 1.0467256307601929
Epoch 520, training loss: 13.288199424743652 = 0.5701180696487427 + 2.0 * 6.3590407371521
Epoch 520, val loss: 1.039141058921814
Epoch 530, training loss: 13.272855758666992 = 0.548062264919281 + 2.0 * 6.362396717071533
Epoch 530, val loss: 1.0323492288589478
Epoch 540, training loss: 13.237231254577637 = 0.5266631841659546 + 2.0 * 6.355284214019775
Epoch 540, val loss: 1.0261797904968262
Epoch 550, training loss: 13.215761184692383 = 0.5058251619338989 + 2.0 * 6.354968070983887
Epoch 550, val loss: 1.020859956741333
Epoch 560, training loss: 13.188981056213379 = 0.48563385009765625 + 2.0 * 6.351673603057861
Epoch 560, val loss: 1.016276240348816
Epoch 570, training loss: 13.166232109069824 = 0.4661286473274231 + 2.0 * 6.3500518798828125
Epoch 570, val loss: 1.0123401880264282
Epoch 580, training loss: 13.145598411560059 = 0.44725170731544495 + 2.0 * 6.349173545837402
Epoch 580, val loss: 1.0093520879745483
Epoch 590, training loss: 13.133502006530762 = 0.429008811712265 + 2.0 * 6.3522467613220215
Epoch 590, val loss: 1.0070899724960327
Epoch 600, training loss: 13.100658416748047 = 0.4115229547023773 + 2.0 * 6.344567775726318
Epoch 600, val loss: 1.005350947380066
Epoch 610, training loss: 13.082076072692871 = 0.394736647605896 + 2.0 * 6.343669891357422
Epoch 610, val loss: 1.0047087669372559
Epoch 620, training loss: 13.057945251464844 = 0.37870144844055176 + 2.0 * 6.3396220207214355
Epoch 620, val loss: 1.0046141147613525
Epoch 630, training loss: 13.040749549865723 = 0.36338362097740173 + 2.0 * 6.338683128356934
Epoch 630, val loss: 1.005456805229187
Epoch 640, training loss: 13.024192810058594 = 0.3487318754196167 + 2.0 * 6.337730407714844
Epoch 640, val loss: 1.0070066452026367
Epoch 650, training loss: 13.010294914245605 = 0.3347645699977875 + 2.0 * 6.337765216827393
Epoch 650, val loss: 1.0091739892959595
Epoch 660, training loss: 12.98806381225586 = 0.3215470314025879 + 2.0 * 6.333258628845215
Epoch 660, val loss: 1.0122365951538086
Epoch 670, training loss: 12.971208572387695 = 0.30894502997398376 + 2.0 * 6.331131935119629
Epoch 670, val loss: 1.0160348415374756
Epoch 680, training loss: 12.956621170043945 = 0.29687753319740295 + 2.0 * 6.329871654510498
Epoch 680, val loss: 1.02028226852417
Epoch 690, training loss: 12.97295093536377 = 0.285307377576828 + 2.0 * 6.343822002410889
Epoch 690, val loss: 1.0250056982040405
Epoch 700, training loss: 12.928191184997559 = 0.2742365300655365 + 2.0 * 6.326977252960205
Epoch 700, val loss: 1.030097246170044
Epoch 710, training loss: 12.915043830871582 = 0.2636306583881378 + 2.0 * 6.325706481933594
Epoch 710, val loss: 1.0359171628952026
Epoch 720, training loss: 12.909310340881348 = 0.2534145712852478 + 2.0 * 6.327948093414307
Epoch 720, val loss: 1.0419267416000366
Epoch 730, training loss: 12.890033721923828 = 0.24353958666324615 + 2.0 * 6.323246955871582
Epoch 730, val loss: 1.0484353303909302
Epoch 740, training loss: 12.876574516296387 = 0.23395393788814545 + 2.0 * 6.321310520172119
Epoch 740, val loss: 1.055242896080017
Epoch 750, training loss: 12.873834609985352 = 0.22463294863700867 + 2.0 * 6.324600696563721
Epoch 750, val loss: 1.0622650384902954
Epoch 760, training loss: 12.86562728881836 = 0.21557395160198212 + 2.0 * 6.325026512145996
Epoch 760, val loss: 1.0694829225540161
Epoch 770, training loss: 12.841988563537598 = 0.2067456692457199 + 2.0 * 6.317621231079102
Epoch 770, val loss: 1.0770846605300903
Epoch 780, training loss: 12.833247184753418 = 0.19815514981746674 + 2.0 * 6.3175458908081055
Epoch 780, val loss: 1.0847903490066528
Epoch 790, training loss: 12.823522567749023 = 0.18978580832481384 + 2.0 * 6.316868305206299
Epoch 790, val loss: 1.0927762985229492
Epoch 800, training loss: 12.81360912322998 = 0.18164107203483582 + 2.0 * 6.31598424911499
Epoch 800, val loss: 1.1009209156036377
Epoch 810, training loss: 12.801661491394043 = 0.17372260987758636 + 2.0 * 6.313969612121582
Epoch 810, val loss: 1.1093454360961914
Epoch 820, training loss: 12.790815353393555 = 0.16604208946228027 + 2.0 * 6.312386512756348
Epoch 820, val loss: 1.1179403066635132
Epoch 830, training loss: 12.787008285522461 = 0.15860778093338013 + 2.0 * 6.314200401306152
Epoch 830, val loss: 1.1266813278198242
Epoch 840, training loss: 12.771780967712402 = 0.15143729746341705 + 2.0 * 6.310171604156494
Epoch 840, val loss: 1.1356263160705566
Epoch 850, training loss: 12.773187637329102 = 0.14452819526195526 + 2.0 * 6.314329624176025
Epoch 850, val loss: 1.1447439193725586
Epoch 860, training loss: 12.756196022033691 = 0.13792459666728973 + 2.0 * 6.309135913848877
Epoch 860, val loss: 1.1538596153259277
Epoch 870, training loss: 12.745589256286621 = 0.1315811723470688 + 2.0 * 6.307003974914551
Epoch 870, val loss: 1.1633718013763428
Epoch 880, training loss: 12.740015983581543 = 0.12552356719970703 + 2.0 * 6.307246208190918
Epoch 880, val loss: 1.172789454460144
Epoch 890, training loss: 12.732993125915527 = 0.11973767727613449 + 2.0 * 6.3066277503967285
Epoch 890, val loss: 1.1824005842208862
Epoch 900, training loss: 12.725932121276855 = 0.11421800404787064 + 2.0 * 6.305857181549072
Epoch 900, val loss: 1.1921420097351074
Epoch 910, training loss: 12.718709945678711 = 0.10896600037813187 + 2.0 * 6.304872035980225
Epoch 910, val loss: 1.2018219232559204
Epoch 920, training loss: 12.717602729797363 = 0.10397004336118698 + 2.0 * 6.306816577911377
Epoch 920, val loss: 1.2115650177001953
Epoch 930, training loss: 12.703768730163574 = 0.09926676750183105 + 2.0 * 6.302250862121582
Epoch 930, val loss: 1.2213518619537354
Epoch 940, training loss: 12.698610305786133 = 0.09478083997964859 + 2.0 * 6.301914691925049
Epoch 940, val loss: 1.231192708015442
Epoch 950, training loss: 12.694767951965332 = 0.09055700153112411 + 2.0 * 6.30210542678833
Epoch 950, val loss: 1.2409451007843018
Epoch 960, training loss: 12.686797142028809 = 0.08654522895812988 + 2.0 * 6.300126075744629
Epoch 960, val loss: 1.250848412513733
Epoch 970, training loss: 12.692017555236816 = 0.08276063203811646 + 2.0 * 6.304628372192383
Epoch 970, val loss: 1.2604477405548096
Epoch 980, training loss: 12.675813674926758 = 0.07918991893529892 + 2.0 * 6.298311710357666
Epoch 980, val loss: 1.270274043083191
Epoch 990, training loss: 12.667287826538086 = 0.07581456750631332 + 2.0 * 6.295736789703369
Epoch 990, val loss: 1.2799911499023438
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7111111111111111
0.8144438587243016
=== training gcn model ===
Epoch 0, training loss: 19.12691879272461 = 1.9331848621368408 + 2.0 * 8.596866607666016
Epoch 0, val loss: 1.9364402294158936
Epoch 10, training loss: 19.1171875 = 1.9239107370376587 + 2.0 * 8.596638679504395
Epoch 10, val loss: 1.9274029731750488
Epoch 20, training loss: 19.10232162475586 = 1.912582516670227 + 2.0 * 8.594869613647461
Epoch 20, val loss: 1.9157969951629639
Epoch 30, training loss: 19.05826187133789 = 1.8972781896591187 + 2.0 * 8.58049201965332
Epoch 30, val loss: 1.8997739553451538
Epoch 40, training loss: 18.84335708618164 = 1.8780072927474976 + 2.0 * 8.482674598693848
Epoch 40, val loss: 1.8799753189086914
Epoch 50, training loss: 18.016876220703125 = 1.8573566675186157 + 2.0 * 8.07975959777832
Epoch 50, val loss: 1.8593075275421143
Epoch 60, training loss: 17.201343536376953 = 1.8417062759399414 + 2.0 * 7.679818630218506
Epoch 60, val loss: 1.844559907913208
Epoch 70, training loss: 16.643619537353516 = 1.8299474716186523 + 2.0 * 7.406835556030273
Epoch 70, val loss: 1.8335412740707397
Epoch 80, training loss: 16.050424575805664 = 1.8177884817123413 + 2.0 * 7.1163177490234375
Epoch 80, val loss: 1.8223671913146973
Epoch 90, training loss: 15.745546340942383 = 1.8053240776062012 + 2.0 * 6.970110893249512
Epoch 90, val loss: 1.8102283477783203
Epoch 100, training loss: 15.54819107055664 = 1.7920247316360474 + 2.0 * 6.878083229064941
Epoch 100, val loss: 1.7967851161956787
Epoch 110, training loss: 15.3792142868042 = 1.7788509130477905 + 2.0 * 6.800181865692139
Epoch 110, val loss: 1.7839767932891846
Epoch 120, training loss: 15.258627891540527 = 1.7656272649765015 + 2.0 * 6.746500492095947
Epoch 120, val loss: 1.7716224193572998
Epoch 130, training loss: 15.140979766845703 = 1.7514570951461792 + 2.0 * 6.694761276245117
Epoch 130, val loss: 1.7584116458892822
Epoch 140, training loss: 15.04903507232666 = 1.7359012365341187 + 2.0 * 6.656567096710205
Epoch 140, val loss: 1.7441539764404297
Epoch 150, training loss: 14.980798721313477 = 1.71866774559021 + 2.0 * 6.631065368652344
Epoch 150, val loss: 1.7285526990890503
Epoch 160, training loss: 14.903070449829102 = 1.699221134185791 + 2.0 * 6.601924419403076
Epoch 160, val loss: 1.711221694946289
Epoch 170, training loss: 14.83626651763916 = 1.677376627922058 + 2.0 * 6.579444885253906
Epoch 170, val loss: 1.691992998123169
Epoch 180, training loss: 14.777061462402344 = 1.652737021446228 + 2.0 * 6.562162399291992
Epoch 180, val loss: 1.6705307960510254
Epoch 190, training loss: 14.721330642700195 = 1.625171422958374 + 2.0 * 6.548079490661621
Epoch 190, val loss: 1.646578073501587
Epoch 200, training loss: 14.660245895385742 = 1.5944733619689941 + 2.0 * 6.532886505126953
Epoch 200, val loss: 1.6202832460403442
Epoch 210, training loss: 14.599937438964844 = 1.560640811920166 + 2.0 * 6.519648551940918
Epoch 210, val loss: 1.5913996696472168
Epoch 220, training loss: 14.541841506958008 = 1.5235481262207031 + 2.0 * 6.509146690368652
Epoch 220, val loss: 1.5599968433380127
Epoch 230, training loss: 14.489011764526367 = 1.4835126399993896 + 2.0 * 6.502749443054199
Epoch 230, val loss: 1.5265151262283325
Epoch 240, training loss: 14.418564796447754 = 1.4413801431655884 + 2.0 * 6.488592147827148
Epoch 240, val loss: 1.4915626049041748
Epoch 250, training loss: 14.35467529296875 = 1.3976101875305176 + 2.0 * 6.478532791137695
Epoch 250, val loss: 1.4556114673614502
Epoch 260, training loss: 14.29493236541748 = 1.3525851964950562 + 2.0 * 6.4711737632751465
Epoch 260, val loss: 1.4190322160720825
Epoch 270, training loss: 14.235776901245117 = 1.3072929382324219 + 2.0 * 6.464241981506348
Epoch 270, val loss: 1.382448434829712
Epoch 280, training loss: 14.172372817993164 = 1.2623845338821411 + 2.0 * 6.454994201660156
Epoch 280, val loss: 1.3466925621032715
Epoch 290, training loss: 14.113718032836914 = 1.2180581092834473 + 2.0 * 6.4478302001953125
Epoch 290, val loss: 1.3119057416915894
Epoch 300, training loss: 14.058189392089844 = 1.174684762954712 + 2.0 * 6.4417524337768555
Epoch 300, val loss: 1.2782185077667236
Epoch 310, training loss: 14.004002571105957 = 1.1325137615203857 + 2.0 * 6.435744285583496
Epoch 310, val loss: 1.2459787130355835
Epoch 320, training loss: 13.95406723022461 = 1.0914723873138428 + 2.0 * 6.431297302246094
Epoch 320, val loss: 1.215071678161621
Epoch 330, training loss: 13.909510612487793 = 1.0516879558563232 + 2.0 * 6.428911209106445
Epoch 330, val loss: 1.1856621503829956
Epoch 340, training loss: 13.854536056518555 = 1.013211965560913 + 2.0 * 6.420661926269531
Epoch 340, val loss: 1.1576297283172607
Epoch 350, training loss: 13.805519104003906 = 0.9758204817771912 + 2.0 * 6.414849281311035
Epoch 350, val loss: 1.1310070753097534
Epoch 360, training loss: 13.76659870147705 = 0.9394699335098267 + 2.0 * 6.413564205169678
Epoch 360, val loss: 1.1054707765579224
Epoch 370, training loss: 13.721467971801758 = 0.9042251110076904 + 2.0 * 6.408621311187744
Epoch 370, val loss: 1.0812870264053345
Epoch 380, training loss: 13.67442798614502 = 0.8701161742210388 + 2.0 * 6.402155876159668
Epoch 380, val loss: 1.058154821395874
Epoch 390, training loss: 13.63465690612793 = 0.8369121551513672 + 2.0 * 6.398872375488281
Epoch 390, val loss: 1.036004662513733
Epoch 400, training loss: 13.601486206054688 = 0.8044105172157288 + 2.0 * 6.398537635803223
Epoch 400, val loss: 1.0147292613983154
Epoch 410, training loss: 13.55672550201416 = 0.772761344909668 + 2.0 * 6.391982078552246
Epoch 410, val loss: 0.9944136142730713
Epoch 420, training loss: 13.518735885620117 = 0.7419061660766602 + 2.0 * 6.3884148597717285
Epoch 420, val loss: 0.9747486114501953
Epoch 430, training loss: 13.490876197814941 = 0.7119046449661255 + 2.0 * 6.389485836029053
Epoch 430, val loss: 0.9559792876243591
Epoch 440, training loss: 13.447720527648926 = 0.6827402710914612 + 2.0 * 6.382490158081055
Epoch 440, val loss: 0.9381831884384155
Epoch 450, training loss: 13.414454460144043 = 0.6545674204826355 + 2.0 * 6.379943370819092
Epoch 450, val loss: 0.9212931990623474
Epoch 460, training loss: 13.379173278808594 = 0.627225399017334 + 2.0 * 6.375974178314209
Epoch 460, val loss: 0.9052610397338867
Epoch 470, training loss: 13.350424766540527 = 0.6007421016693115 + 2.0 * 6.374841213226318
Epoch 470, val loss: 0.8901926875114441
Epoch 480, training loss: 13.318188667297363 = 0.5750882029533386 + 2.0 * 6.3715500831604
Epoch 480, val loss: 0.8759312033653259
Epoch 490, training loss: 13.292590141296387 = 0.5501543283462524 + 2.0 * 6.371217727661133
Epoch 490, val loss: 0.8624918460845947
Epoch 500, training loss: 13.259687423706055 = 0.5259760618209839 + 2.0 * 6.366855621337891
Epoch 500, val loss: 0.8498163223266602
Epoch 510, training loss: 13.23287582397461 = 0.5025752782821655 + 2.0 * 6.365150451660156
Epoch 510, val loss: 0.8378955721855164
Epoch 520, training loss: 13.203380584716797 = 0.47993865609169006 + 2.0 * 6.361721038818359
Epoch 520, val loss: 0.8268035650253296
Epoch 530, training loss: 13.174814224243164 = 0.4579043984413147 + 2.0 * 6.358454704284668
Epoch 530, val loss: 0.8164440989494324
Epoch 540, training loss: 13.157089233398438 = 0.43648186326026917 + 2.0 * 6.36030387878418
Epoch 540, val loss: 0.8066869974136353
Epoch 550, training loss: 13.137748718261719 = 0.41559332609176636 + 2.0 * 6.361077785491943
Epoch 550, val loss: 0.797667384147644
Epoch 560, training loss: 13.101602554321289 = 0.3954942226409912 + 2.0 * 6.353054046630859
Epoch 560, val loss: 0.7892250418663025
Epoch 570, training loss: 13.077463150024414 = 0.37606900930404663 + 2.0 * 6.350697040557861
Epoch 570, val loss: 0.7814772129058838
Epoch 580, training loss: 13.056441307067871 = 0.3572636544704437 + 2.0 * 6.349588871002197
Epoch 580, val loss: 0.7744267582893372
Epoch 590, training loss: 13.046363830566406 = 0.3390929400920868 + 2.0 * 6.353635311126709
Epoch 590, val loss: 0.7679576873779297
Epoch 600, training loss: 13.014646530151367 = 0.3216332495212555 + 2.0 * 6.346506595611572
Epoch 600, val loss: 0.7621644139289856
Epoch 610, training loss: 12.993951797485352 = 0.3049229681491852 + 2.0 * 6.3445143699646
Epoch 610, val loss: 0.7570732235908508
Epoch 620, training loss: 12.97351360321045 = 0.2889481484889984 + 2.0 * 6.342282772064209
Epoch 620, val loss: 0.7525908946990967
Epoch 630, training loss: 12.962556838989258 = 0.27364620566368103 + 2.0 * 6.344455242156982
Epoch 630, val loss: 0.74873286485672
Epoch 640, training loss: 12.945831298828125 = 0.2590600252151489 + 2.0 * 6.343385696411133
Epoch 640, val loss: 0.7453522682189941
Epoch 650, training loss: 12.922388076782227 = 0.24521228671073914 + 2.0 * 6.338587760925293
Epoch 650, val loss: 0.7425763607025146
Epoch 660, training loss: 12.90683364868164 = 0.23206523060798645 + 2.0 * 6.337384223937988
Epoch 660, val loss: 0.7404005527496338
Epoch 670, training loss: 12.88980484008789 = 0.21958595514297485 + 2.0 * 6.335109233856201
Epoch 670, val loss: 0.7387316226959229
Epoch 680, training loss: 12.885600090026855 = 0.2077670395374298 + 2.0 * 6.338916301727295
Epoch 680, val loss: 0.737549364566803
Epoch 690, training loss: 12.864097595214844 = 0.19659645855426788 + 2.0 * 6.3337507247924805
Epoch 690, val loss: 0.7367447018623352
Epoch 700, training loss: 12.848374366760254 = 0.1860305368900299 + 2.0 * 6.331171989440918
Epoch 700, val loss: 0.7364336252212524
Epoch 710, training loss: 12.842097282409668 = 0.17608167231082916 + 2.0 * 6.3330078125
Epoch 710, val loss: 0.7365859150886536
Epoch 720, training loss: 12.83034610748291 = 0.16671517491340637 + 2.0 * 6.331815242767334
Epoch 720, val loss: 0.7371399402618408
Epoch 730, training loss: 12.811895370483398 = 0.15798062086105347 + 2.0 * 6.3269572257995605
Epoch 730, val loss: 0.7380532622337341
Epoch 740, training loss: 12.801294326782227 = 0.149777352809906 + 2.0 * 6.325758457183838
Epoch 740, val loss: 0.7393149733543396
Epoch 750, training loss: 12.79789924621582 = 0.14204536378383636 + 2.0 * 6.327927112579346
Epoch 750, val loss: 0.7408915758132935
Epoch 760, training loss: 12.788820266723633 = 0.1348005086183548 + 2.0 * 6.327009677886963
Epoch 760, val loss: 0.7426546216011047
Epoch 770, training loss: 12.773783683776855 = 0.12799620628356934 + 2.0 * 6.3228936195373535
Epoch 770, val loss: 0.7447362542152405
Epoch 780, training loss: 12.767139434814453 = 0.12162521481513977 + 2.0 * 6.322757244110107
Epoch 780, val loss: 0.7470664978027344
Epoch 790, training loss: 12.755487442016602 = 0.11563841253519058 + 2.0 * 6.319924354553223
Epoch 790, val loss: 0.7496126294136047
Epoch 800, training loss: 12.75401782989502 = 0.11000353842973709 + 2.0 * 6.322007179260254
Epoch 800, val loss: 0.75239497423172
Epoch 810, training loss: 12.74475383758545 = 0.10472910851240158 + 2.0 * 6.32001256942749
Epoch 810, val loss: 0.7553468942642212
Epoch 820, training loss: 12.739435195922852 = 0.09975802153348923 + 2.0 * 6.319838523864746
Epoch 820, val loss: 0.7584241032600403
Epoch 830, training loss: 12.726195335388184 = 0.09512461721897125 + 2.0 * 6.315535545349121
Epoch 830, val loss: 0.7617270350456238
Epoch 840, training loss: 12.720419883728027 = 0.09075043350458145 + 2.0 * 6.3148345947265625
Epoch 840, val loss: 0.765127420425415
Epoch 850, training loss: 12.71717643737793 = 0.08664184808731079 + 2.0 * 6.315267086029053
Epoch 850, val loss: 0.7686806321144104
Epoch 860, training loss: 12.717255592346191 = 0.08275309950113297 + 2.0 * 6.317251205444336
Epoch 860, val loss: 0.7722776532173157
Epoch 870, training loss: 12.707024574279785 = 0.07911138236522675 + 2.0 * 6.3139567375183105
Epoch 870, val loss: 0.7758700847625732
Epoch 880, training loss: 12.695642471313477 = 0.07568450272083282 + 2.0 * 6.30997896194458
Epoch 880, val loss: 0.7796076536178589
Epoch 890, training loss: 12.691523551940918 = 0.07246654480695724 + 2.0 * 6.309528350830078
Epoch 890, val loss: 0.7834621667861938
Epoch 900, training loss: 12.696715354919434 = 0.0694289356470108 + 2.0 * 6.313642978668213
Epoch 900, val loss: 0.7873028516769409
Epoch 910, training loss: 12.681943893432617 = 0.06656553596258163 + 2.0 * 6.307689189910889
Epoch 910, val loss: 0.7912212014198303
Epoch 920, training loss: 12.675126075744629 = 0.06387057155370712 + 2.0 * 6.305627822875977
Epoch 920, val loss: 0.7952072620391846
Epoch 930, training loss: 12.670884132385254 = 0.061330538243055344 + 2.0 * 6.304776668548584
Epoch 930, val loss: 0.7992237210273743
Epoch 940, training loss: 12.6848783493042 = 0.058929163962602615 + 2.0 * 6.312974452972412
Epoch 940, val loss: 0.8032469153404236
Epoch 950, training loss: 12.67047119140625 = 0.05663887783885002 + 2.0 * 6.306916236877441
Epoch 950, val loss: 0.807237982749939
Epoch 960, training loss: 12.657724380493164 = 0.05448301136493683 + 2.0 * 6.3016204833984375
Epoch 960, val loss: 0.8112949132919312
Epoch 970, training loss: 12.65645980834961 = 0.05244804173707962 + 2.0 * 6.302005767822266
Epoch 970, val loss: 0.8153437376022339
Epoch 980, training loss: 12.65936279296875 = 0.050516024231910706 + 2.0 * 6.3044233322143555
Epoch 980, val loss: 0.819421112537384
Epoch 990, training loss: 12.658285140991211 = 0.04868674278259277 + 2.0 * 6.3047990798950195
Epoch 990, val loss: 0.8235077857971191
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7555555555555555
0.8149710068529257
=== training gcn model ===
Epoch 0, training loss: 19.14735984802246 = 1.953665852546692 + 2.0 * 8.596846580505371
Epoch 0, val loss: 1.9548699855804443
Epoch 10, training loss: 19.136367797851562 = 1.9432449340820312 + 2.0 * 8.596561431884766
Epoch 10, val loss: 1.9447518587112427
Epoch 20, training loss: 19.118791580200195 = 1.9301410913467407 + 2.0 * 8.594325065612793
Epoch 20, val loss: 1.931471347808838
Epoch 30, training loss: 19.06844711303711 = 1.912110686302185 + 2.0 * 8.578167915344238
Epoch 30, val loss: 1.9129127264022827
Epoch 40, training loss: 18.870698928833008 = 1.8898760080337524 + 2.0 * 8.490411758422852
Epoch 40, val loss: 1.8911676406860352
Epoch 50, training loss: 18.199148178100586 = 1.8679145574569702 + 2.0 * 8.165616989135742
Epoch 50, val loss: 1.8706610202789307
Epoch 60, training loss: 17.34436798095703 = 1.8500531911849976 + 2.0 * 7.747157573699951
Epoch 60, val loss: 1.8550941944122314
Epoch 70, training loss: 16.541452407836914 = 1.8365455865859985 + 2.0 * 7.352453708648682
Epoch 70, val loss: 1.842592477798462
Epoch 80, training loss: 16.012052536010742 = 1.8229635953903198 + 2.0 * 7.094544887542725
Epoch 80, val loss: 1.8296324014663696
Epoch 90, training loss: 15.741737365722656 = 1.8084297180175781 + 2.0 * 6.966653823852539
Epoch 90, val loss: 1.816379189491272
Epoch 100, training loss: 15.543237686157227 = 1.7929272651672363 + 2.0 * 6.875154972076416
Epoch 100, val loss: 1.803476333618164
Epoch 110, training loss: 15.393207550048828 = 1.7778900861740112 + 2.0 * 6.807658672332764
Epoch 110, val loss: 1.7912732362747192
Epoch 120, training loss: 15.277487754821777 = 1.7629821300506592 + 2.0 * 6.7572526931762695
Epoch 120, val loss: 1.7786047458648682
Epoch 130, training loss: 15.178834915161133 = 1.7471795082092285 + 2.0 * 6.715827465057373
Epoch 130, val loss: 1.7646957635879517
Epoch 140, training loss: 15.099976539611816 = 1.7295665740966797 + 2.0 * 6.685204982757568
Epoch 140, val loss: 1.7494114637374878
Epoch 150, training loss: 15.026010513305664 = 1.709791660308838 + 2.0 * 6.658109188079834
Epoch 150, val loss: 1.7325581312179565
Epoch 160, training loss: 14.959830284118652 = 1.6875320672988892 + 2.0 * 6.636148929595947
Epoch 160, val loss: 1.713830590248108
Epoch 170, training loss: 14.897689819335938 = 1.6624523401260376 + 2.0 * 6.617618560791016
Epoch 170, val loss: 1.6927578449249268
Epoch 180, training loss: 14.829834938049316 = 1.6344131231307983 + 2.0 * 6.597711086273193
Epoch 180, val loss: 1.669543743133545
Epoch 190, training loss: 14.761514663696289 = 1.603346347808838 + 2.0 * 6.5790839195251465
Epoch 190, val loss: 1.643851399421692
Epoch 200, training loss: 14.69336986541748 = 1.568821907043457 + 2.0 * 6.562273979187012
Epoch 200, val loss: 1.6156623363494873
Epoch 210, training loss: 14.62352466583252 = 1.531046986579895 + 2.0 * 6.546238899230957
Epoch 210, val loss: 1.5851017236709595
Epoch 220, training loss: 14.55606746673584 = 1.4903978109359741 + 2.0 * 6.532835006713867
Epoch 220, val loss: 1.5521397590637207
Epoch 230, training loss: 14.486737251281738 = 1.4468268156051636 + 2.0 * 6.519955158233643
Epoch 230, val loss: 1.5172836780548096
Epoch 240, training loss: 14.420257568359375 = 1.4006218910217285 + 2.0 * 6.509817600250244
Epoch 240, val loss: 1.4806512594223022
Epoch 250, training loss: 14.349963188171387 = 1.3529523611068726 + 2.0 * 6.498505592346191
Epoch 250, val loss: 1.4431952238082886
Epoch 260, training loss: 14.283637046813965 = 1.304561972618103 + 2.0 * 6.489537715911865
Epoch 260, val loss: 1.4054827690124512
Epoch 270, training loss: 14.216012954711914 = 1.2556325197219849 + 2.0 * 6.480190277099609
Epoch 270, val loss: 1.3677647113800049
Epoch 280, training loss: 14.158693313598633 = 1.206811547279358 + 2.0 * 6.475940704345703
Epoch 280, val loss: 1.330626130104065
Epoch 290, training loss: 14.089929580688477 = 1.158873438835144 + 2.0 * 6.4655280113220215
Epoch 290, val loss: 1.2946460247039795
Epoch 300, training loss: 14.026869773864746 = 1.1119165420532227 + 2.0 * 6.457476615905762
Epoch 300, val loss: 1.2598801851272583
Epoch 310, training loss: 13.976832389831543 = 1.0661462545394897 + 2.0 * 6.455343246459961
Epoch 310, val loss: 1.226466178894043
Epoch 320, training loss: 13.912343978881836 = 1.0220052003860474 + 2.0 * 6.445169448852539
Epoch 320, val loss: 1.194896936416626
Epoch 330, training loss: 13.865398406982422 = 0.9797747135162354 + 2.0 * 6.442811965942383
Epoch 330, val loss: 1.1652398109436035
Epoch 340, training loss: 13.812749862670898 = 0.939725399017334 + 2.0 * 6.436511993408203
Epoch 340, val loss: 1.1377594470977783
Epoch 350, training loss: 13.761053085327148 = 0.9019733667373657 + 2.0 * 6.429539680480957
Epoch 350, val loss: 1.11253023147583
Epoch 360, training loss: 13.719931602478027 = 0.8662351965904236 + 2.0 * 6.426848411560059
Epoch 360, val loss: 1.0895360708236694
Epoch 370, training loss: 13.675426483154297 = 0.8326325416564941 + 2.0 * 6.4213972091674805
Epoch 370, val loss: 1.0686849355697632
Epoch 380, training loss: 13.632848739624023 = 0.8010563850402832 + 2.0 * 6.415895938873291
Epoch 380, val loss: 1.0501759052276611
Epoch 390, training loss: 13.609835624694824 = 0.7714121341705322 + 2.0 * 6.4192118644714355
Epoch 390, val loss: 1.0336878299713135
Epoch 400, training loss: 13.558362007141113 = 0.743567705154419 + 2.0 * 6.407397270202637
Epoch 400, val loss: 1.0191344022750854
Epoch 410, training loss: 13.523065567016602 = 0.7173758149147034 + 2.0 * 6.4028449058532715
Epoch 410, val loss: 1.0063143968582153
Epoch 420, training loss: 13.497262954711914 = 0.692584216594696 + 2.0 * 6.402339458465576
Epoch 420, val loss: 0.9950374960899353
Epoch 430, training loss: 13.467845916748047 = 0.6690871715545654 + 2.0 * 6.399379253387451
Epoch 430, val loss: 0.984972357749939
Epoch 440, training loss: 13.43327808380127 = 0.6467673182487488 + 2.0 * 6.393255233764648
Epoch 440, val loss: 0.9762651324272156
Epoch 450, training loss: 13.4027681350708 = 0.6254187226295471 + 2.0 * 6.388674736022949
Epoch 450, val loss: 0.968708872795105
Epoch 460, training loss: 13.375892639160156 = 0.6048350930213928 + 2.0 * 6.385528564453125
Epoch 460, val loss: 0.9619874358177185
Epoch 470, training loss: 13.360261917114258 = 0.5849319696426392 + 2.0 * 6.387664794921875
Epoch 470, val loss: 0.9560718536376953
Epoch 480, training loss: 13.32867431640625 = 0.5655566453933716 + 2.0 * 6.381558895111084
Epoch 480, val loss: 0.9508869051933289
Epoch 490, training loss: 13.305298805236816 = 0.5467855930328369 + 2.0 * 6.379256725311279
Epoch 490, val loss: 0.946337103843689
Epoch 500, training loss: 13.27929973602295 = 0.5284745097160339 + 2.0 * 6.375412464141846
Epoch 500, val loss: 0.9422417879104614
Epoch 510, training loss: 13.26187801361084 = 0.5106255412101746 + 2.0 * 6.375626087188721
Epoch 510, val loss: 0.9388512969017029
Epoch 520, training loss: 13.234098434448242 = 0.49320387840270996 + 2.0 * 6.370447158813477
Epoch 520, val loss: 0.9360970854759216
Epoch 530, training loss: 13.209226608276367 = 0.4762265384197235 + 2.0 * 6.366499900817871
Epoch 530, val loss: 0.9337729215621948
Epoch 540, training loss: 13.195146560668945 = 0.45963457226753235 + 2.0 * 6.367755889892578
Epoch 540, val loss: 0.9319568872451782
Epoch 550, training loss: 13.174342155456543 = 0.44356074929237366 + 2.0 * 6.365390777587891
Epoch 550, val loss: 0.9307616353034973
Epoch 560, training loss: 13.14794635772705 = 0.42792680859565735 + 2.0 * 6.360009670257568
Epoch 560, val loss: 0.9302946329116821
Epoch 570, training loss: 13.129152297973633 = 0.4127654433250427 + 2.0 * 6.358193397521973
Epoch 570, val loss: 0.9302392601966858
Epoch 580, training loss: 13.12674331665039 = 0.3980086147785187 + 2.0 * 6.364367485046387
Epoch 580, val loss: 0.9306671619415283
Epoch 590, training loss: 13.093435287475586 = 0.38381636142730713 + 2.0 * 6.354809284210205
Epoch 590, val loss: 0.931921660900116
Epoch 600, training loss: 13.073395729064941 = 0.37002530694007874 + 2.0 * 6.351685047149658
Epoch 600, val loss: 0.9336614012718201
Epoch 610, training loss: 13.059159278869629 = 0.356625497341156 + 2.0 * 6.351266860961914
Epoch 610, val loss: 0.9358096718788147
Epoch 620, training loss: 13.046540260314941 = 0.3436003029346466 + 2.0 * 6.351469993591309
Epoch 620, val loss: 0.9381981492042542
Epoch 630, training loss: 13.024181365966797 = 0.3310052156448364 + 2.0 * 6.346588134765625
Epoch 630, val loss: 0.941411018371582
Epoch 640, training loss: 13.00877571105957 = 0.31876713037490845 + 2.0 * 6.345004081726074
Epoch 640, val loss: 0.9448350667953491
Epoch 650, training loss: 13.003323554992676 = 0.3068442642688751 + 2.0 * 6.348239421844482
Epoch 650, val loss: 0.9485344886779785
Epoch 660, training loss: 12.989785194396973 = 0.29531824588775635 + 2.0 * 6.347233295440674
Epoch 660, val loss: 0.9526076912879944
Epoch 670, training loss: 12.963521957397461 = 0.2840811610221863 + 2.0 * 6.339720249176025
Epoch 670, val loss: 0.9569202661514282
Epoch 680, training loss: 12.951671600341797 = 0.27317702770233154 + 2.0 * 6.339247226715088
Epoch 680, val loss: 0.9616294503211975
Epoch 690, training loss: 12.940617561340332 = 0.26256635785102844 + 2.0 * 6.339025497436523
Epoch 690, val loss: 0.9665961265563965
Epoch 700, training loss: 12.924786567687988 = 0.2522242069244385 + 2.0 * 6.3362812995910645
Epoch 700, val loss: 0.9718461632728577
Epoch 710, training loss: 12.908905982971191 = 0.24218101799488068 + 2.0 * 6.333362579345703
Epoch 710, val loss: 0.9771072864532471
Epoch 720, training loss: 12.900528907775879 = 0.2324674278497696 + 2.0 * 6.334030628204346
Epoch 720, val loss: 0.9829564094543457
Epoch 730, training loss: 12.884007453918457 = 0.22302602231502533 + 2.0 * 6.330490589141846
Epoch 730, val loss: 0.9888386130332947
Epoch 740, training loss: 12.906872749328613 = 0.21387110650539398 + 2.0 * 6.346500873565674
Epoch 740, val loss: 0.9948921203613281
Epoch 750, training loss: 12.863616943359375 = 0.20505036413669586 + 2.0 * 6.329283237457275
Epoch 750, val loss: 1.0009257793426514
Epoch 760, training loss: 12.852972984313965 = 0.19653326272964478 + 2.0 * 6.328219890594482
Epoch 760, val loss: 1.0074224472045898
Epoch 770, training loss: 12.841005325317383 = 0.18830719590187073 + 2.0 * 6.326349258422852
Epoch 770, val loss: 1.0138258934020996
Epoch 780, training loss: 12.840847969055176 = 0.1803482472896576 + 2.0 * 6.330249786376953
Epoch 780, val loss: 1.020309567451477
Epoch 790, training loss: 12.823807716369629 = 0.17269137501716614 + 2.0 * 6.325558185577393
Epoch 790, val loss: 1.0270280838012695
Epoch 800, training loss: 12.811944007873535 = 0.16530439257621765 + 2.0 * 6.323319911956787
Epoch 800, val loss: 1.0338690280914307
Epoch 810, training loss: 12.817464828491211 = 0.1582217514514923 + 2.0 * 6.329621315002441
Epoch 810, val loss: 1.0405715703964233
Epoch 820, training loss: 12.793392181396484 = 0.15138810873031616 + 2.0 * 6.321002006530762
Epoch 820, val loss: 1.0475565195083618
Epoch 830, training loss: 12.78223705291748 = 0.14486084878444672 + 2.0 * 6.318687915802002
Epoch 830, val loss: 1.0546436309814453
Epoch 840, training loss: 12.773714065551758 = 0.13859106600284576 + 2.0 * 6.317561626434326
Epoch 840, val loss: 1.0617129802703857
Epoch 850, training loss: 12.784945487976074 = 0.1325846016407013 + 2.0 * 6.326180458068848
Epoch 850, val loss: 1.069033145904541
Epoch 860, training loss: 12.772529602050781 = 0.1268317997455597 + 2.0 * 6.322848796844482
Epoch 860, val loss: 1.0757180452346802
Epoch 870, training loss: 12.754693984985352 = 0.12136269360780716 + 2.0 * 6.3166656494140625
Epoch 870, val loss: 1.0830459594726562
Epoch 880, training loss: 12.742990493774414 = 0.11615217477083206 + 2.0 * 6.313419342041016
Epoch 880, val loss: 1.090226650238037
Epoch 890, training loss: 12.739551544189453 = 0.1111714243888855 + 2.0 * 6.314189910888672
Epoch 890, val loss: 1.0975123643875122
Epoch 900, training loss: 12.731549263000488 = 0.10641410946846008 + 2.0 * 6.312567710876465
Epoch 900, val loss: 1.1046744585037231
Epoch 910, training loss: 12.728779792785645 = 0.10187938064336777 + 2.0 * 6.313450336456299
Epoch 910, val loss: 1.1118806600570679
Epoch 920, training loss: 12.719468116760254 = 0.09756386280059814 + 2.0 * 6.310952186584473
Epoch 920, val loss: 1.1190588474273682
Epoch 930, training loss: 12.720525741577148 = 0.09346809983253479 + 2.0 * 6.313529014587402
Epoch 930, val loss: 1.1260343790054321
Epoch 940, training loss: 12.705597877502441 = 0.08958014845848083 + 2.0 * 6.308008670806885
Epoch 940, val loss: 1.1333867311477661
Epoch 950, training loss: 12.70042610168457 = 0.08587020635604858 + 2.0 * 6.307278156280518
Epoch 950, val loss: 1.1404634714126587
Epoch 960, training loss: 12.697269439697266 = 0.08235011994838715 + 2.0 * 6.307459831237793
Epoch 960, val loss: 1.1475250720977783
Epoch 970, training loss: 12.694994926452637 = 0.07899196445941925 + 2.0 * 6.308001518249512
Epoch 970, val loss: 1.1544651985168457
Epoch 980, training loss: 12.686864852905273 = 0.07581165432929993 + 2.0 * 6.3055267333984375
Epoch 980, val loss: 1.1615456342697144
Epoch 990, training loss: 12.710681915283203 = 0.07280080020427704 + 2.0 * 6.31894063949585
Epoch 990, val loss: 1.1681321859359741
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7222222222222222
0.816025303110174
The final CL Acc:0.72963, 0.01889, The final GNN Acc:0.81515, 0.00066
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13278])
remove edge: torch.Size([2, 7924])
updated graph: torch.Size([2, 10646])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 19.155370712280273 = 1.9616557359695435 + 2.0 * 8.596857070922852
Epoch 0, val loss: 1.958749771118164
Epoch 10, training loss: 19.144515991210938 = 1.9513181447982788 + 2.0 * 8.596598625183105
Epoch 10, val loss: 1.9486061334609985
Epoch 20, training loss: 19.12725257873535 = 1.93839693069458 + 2.0 * 8.594428062438965
Epoch 20, val loss: 1.9354033470153809
Epoch 30, training loss: 19.075971603393555 = 1.9202481508255005 + 2.0 * 8.577861785888672
Epoch 30, val loss: 1.9166831970214844
Epoch 40, training loss: 18.838239669799805 = 1.896966814994812 + 2.0 * 8.470636367797852
Epoch 40, val loss: 1.8934744596481323
Epoch 50, training loss: 18.068138122558594 = 1.8719141483306885 + 2.0 * 8.098112106323242
Epoch 50, val loss: 1.8696982860565186
Epoch 60, training loss: 16.971054077148438 = 1.8540109395980835 + 2.0 * 7.558521270751953
Epoch 60, val loss: 1.8543996810913086
Epoch 70, training loss: 16.01540756225586 = 1.8402807712554932 + 2.0 * 7.0875630378723145
Epoch 70, val loss: 1.8422213792800903
Epoch 80, training loss: 15.692938804626465 = 1.8262271881103516 + 2.0 * 6.933355808258057
Epoch 80, val loss: 1.828921914100647
Epoch 90, training loss: 15.445816993713379 = 1.8101028203964233 + 2.0 * 6.817857265472412
Epoch 90, val loss: 1.8138853311538696
Epoch 100, training loss: 15.278342247009277 = 1.794729471206665 + 2.0 * 6.741806507110596
Epoch 100, val loss: 1.8001707792282104
Epoch 110, training loss: 15.154423713684082 = 1.780562400817871 + 2.0 * 6.6869306564331055
Epoch 110, val loss: 1.787657618522644
Epoch 120, training loss: 15.059853553771973 = 1.767250895500183 + 2.0 * 6.64630126953125
Epoch 120, val loss: 1.7755388021469116
Epoch 130, training loss: 14.989053726196289 = 1.753813624382019 + 2.0 * 6.61761999130249
Epoch 130, val loss: 1.7632451057434082
Epoch 140, training loss: 14.915069580078125 = 1.7396055459976196 + 2.0 * 6.587731838226318
Epoch 140, val loss: 1.750416874885559
Epoch 150, training loss: 14.853728294372559 = 1.7242034673690796 + 2.0 * 6.564762592315674
Epoch 150, val loss: 1.7368052005767822
Epoch 160, training loss: 14.799609184265137 = 1.70746910572052 + 2.0 * 6.546070098876953
Epoch 160, val loss: 1.7223424911499023
Epoch 170, training loss: 14.74631118774414 = 1.6891355514526367 + 2.0 * 6.528587818145752
Epoch 170, val loss: 1.706680178642273
Epoch 180, training loss: 14.695548057556152 = 1.6688191890716553 + 2.0 * 6.513364315032959
Epoch 180, val loss: 1.6893810033798218
Epoch 190, training loss: 14.665294647216797 = 1.646262764930725 + 2.0 * 6.509515762329102
Epoch 190, val loss: 1.6701993942260742
Epoch 200, training loss: 14.600313186645508 = 1.6215382814407349 + 2.0 * 6.489387512207031
Epoch 200, val loss: 1.6492196321487427
Epoch 210, training loss: 14.549541473388672 = 1.5945473909378052 + 2.0 * 6.477497100830078
Epoch 210, val loss: 1.6261355876922607
Epoch 220, training loss: 14.499802589416504 = 1.5650005340576172 + 2.0 * 6.467401027679443
Epoch 220, val loss: 1.600941777229309
Epoch 230, training loss: 14.459333419799805 = 1.5329079627990723 + 2.0 * 6.463212966918945
Epoch 230, val loss: 1.5735615491867065
Epoch 240, training loss: 14.400139808654785 = 1.498630166053772 + 2.0 * 6.450754642486572
Epoch 240, val loss: 1.5443370342254639
Epoch 250, training loss: 14.34821891784668 = 1.462242603302002 + 2.0 * 6.442988395690918
Epoch 250, val loss: 1.5133546590805054
Epoch 260, training loss: 14.29603385925293 = 1.423994541168213 + 2.0 * 6.4360198974609375
Epoch 260, val loss: 1.480997085571289
Epoch 270, training loss: 14.241366386413574 = 1.3841720819473267 + 2.0 * 6.4285969734191895
Epoch 270, val loss: 1.447498083114624
Epoch 280, training loss: 14.189193725585938 = 1.342746615409851 + 2.0 * 6.423223495483398
Epoch 280, val loss: 1.412947654724121
Epoch 290, training loss: 14.147841453552246 = 1.3004595041275024 + 2.0 * 6.4236907958984375
Epoch 290, val loss: 1.3782668113708496
Epoch 300, training loss: 14.081192016601562 = 1.257895588874817 + 2.0 * 6.411648273468018
Epoch 300, val loss: 1.3433043956756592
Epoch 310, training loss: 14.028688430786133 = 1.214775562286377 + 2.0 * 6.406956672668457
Epoch 310, val loss: 1.3083943128585815
Epoch 320, training loss: 13.974613189697266 = 1.1711382865905762 + 2.0 * 6.401737689971924
Epoch 320, val loss: 1.273458480834961
Epoch 330, training loss: 13.929346084594727 = 1.1274166107177734 + 2.0 * 6.400964736938477
Epoch 330, val loss: 1.238762617111206
Epoch 340, training loss: 13.872496604919434 = 1.0842448472976685 + 2.0 * 6.394125938415527
Epoch 340, val loss: 1.2048951387405396
Epoch 350, training loss: 13.819867134094238 = 1.04176926612854 + 2.0 * 6.389049053192139
Epoch 350, val loss: 1.1719239950180054
Epoch 360, training loss: 13.770252227783203 = 0.9999222755432129 + 2.0 * 6.385165214538574
Epoch 360, val loss: 1.1397795677185059
Epoch 370, training loss: 13.734580993652344 = 0.9589438438415527 + 2.0 * 6.387818336486816
Epoch 370, val loss: 1.108613133430481
Epoch 380, training loss: 13.68193244934082 = 0.9193047285079956 + 2.0 * 6.381313800811768
Epoch 380, val loss: 1.0787590742111206
Epoch 390, training loss: 13.632811546325684 = 0.8810637593269348 + 2.0 * 6.375874042510986
Epoch 390, val loss: 1.050374150276184
Epoch 400, training loss: 13.588571548461914 = 0.8440282940864563 + 2.0 * 6.372271537780762
Epoch 400, val loss: 1.0232787132263184
Epoch 410, training loss: 13.563225746154785 = 0.8083136677742004 + 2.0 * 6.377456188201904
Epoch 410, val loss: 0.9974796772003174
Epoch 420, training loss: 13.511754989624023 = 0.7741719484329224 + 2.0 * 6.368791580200195
Epoch 420, val loss: 0.9731295108795166
Epoch 430, training loss: 13.469693183898926 = 0.7415503263473511 + 2.0 * 6.364071369171143
Epoch 430, val loss: 0.9503113031387329
Epoch 440, training loss: 13.432014465332031 = 0.7101117372512817 + 2.0 * 6.3609514236450195
Epoch 440, val loss: 0.9287877678871155
Epoch 450, training loss: 13.413420677185059 = 0.6798614263534546 + 2.0 * 6.366779804229736
Epoch 450, val loss: 0.9085475206375122
Epoch 460, training loss: 13.368324279785156 = 0.6509942412376404 + 2.0 * 6.3586649894714355
Epoch 460, val loss: 0.8896182775497437
Epoch 470, training loss: 13.331595420837402 = 0.6232710480690002 + 2.0 * 6.354162216186523
Epoch 470, val loss: 0.8719089031219482
Epoch 480, training loss: 13.300252914428711 = 0.5964210629463196 + 2.0 * 6.3519158363342285
Epoch 480, val loss: 0.8552492260932922
Epoch 490, training loss: 13.271284103393555 = 0.5703662633895874 + 2.0 * 6.350459098815918
Epoch 490, val loss: 0.8395646810531616
Epoch 500, training loss: 13.243416786193848 = 0.5452108979225159 + 2.0 * 6.349102973937988
Epoch 500, val loss: 0.8249066472053528
Epoch 510, training loss: 13.216886520385742 = 0.5208415389060974 + 2.0 * 6.3480224609375
Epoch 510, val loss: 0.8112155199050903
Epoch 520, training loss: 13.18442153930664 = 0.49728816747665405 + 2.0 * 6.34356689453125
Epoch 520, val loss: 0.7985121011734009
Epoch 530, training loss: 13.157386779785156 = 0.4744403064250946 + 2.0 * 6.34147310256958
Epoch 530, val loss: 0.7867198586463928
Epoch 540, training loss: 13.13849925994873 = 0.45226573944091797 + 2.0 * 6.343116760253906
Epoch 540, val loss: 0.7758143544197083
Epoch 550, training loss: 13.11322021484375 = 0.4308154582977295 + 2.0 * 6.341202259063721
Epoch 550, val loss: 0.7657737731933594
Epoch 560, training loss: 13.085982322692871 = 0.4101026952266693 + 2.0 * 6.337939739227295
Epoch 560, val loss: 0.7566683888435364
Epoch 570, training loss: 13.067163467407227 = 0.39009547233581543 + 2.0 * 6.338533878326416
Epoch 570, val loss: 0.7483525276184082
Epoch 580, training loss: 13.037221908569336 = 0.37085697054862976 + 2.0 * 6.333182334899902
Epoch 580, val loss: 0.7408856153488159
Epoch 590, training loss: 13.0169095993042 = 0.35236799716949463 + 2.0 * 6.332270622253418
Epoch 590, val loss: 0.7342304587364197
Epoch 600, training loss: 12.997828483581543 = 0.3345564901828766 + 2.0 * 6.33163595199585
Epoch 600, val loss: 0.7283170223236084
Epoch 610, training loss: 12.974026679992676 = 0.3175150454044342 + 2.0 * 6.328255653381348
Epoch 610, val loss: 0.7231304049491882
Epoch 620, training loss: 12.954157829284668 = 0.3012373447418213 + 2.0 * 6.326460361480713
Epoch 620, val loss: 0.7186776995658875
Epoch 630, training loss: 12.941825866699219 = 0.2857109308242798 + 2.0 * 6.328057289123535
Epoch 630, val loss: 0.7149443626403809
Epoch 640, training loss: 12.926054954528809 = 0.27091658115386963 + 2.0 * 6.327569007873535
Epoch 640, val loss: 0.7117803692817688
Epoch 650, training loss: 12.901074409484863 = 0.2569592297077179 + 2.0 * 6.322057723999023
Epoch 650, val loss: 0.7093830704689026
Epoch 660, training loss: 12.886302947998047 = 0.24368791282176971 + 2.0 * 6.32130765914917
Epoch 660, val loss: 0.7075905203819275
Epoch 670, training loss: 12.885918617248535 = 0.23108334839344025 + 2.0 * 6.327417850494385
Epoch 670, val loss: 0.7063106894493103
Epoch 680, training loss: 12.857542037963867 = 0.21921582520008087 + 2.0 * 6.3191633224487305
Epoch 680, val loss: 0.7055546641349792
Epoch 690, training loss: 12.841755867004395 = 0.20800867676734924 + 2.0 * 6.316873550415039
Epoch 690, val loss: 0.7053021192550659
Epoch 700, training loss: 12.850114822387695 = 0.19740745425224304 + 2.0 * 6.326353549957275
Epoch 700, val loss: 0.7055210471153259
Epoch 710, training loss: 12.8162841796875 = 0.1874677538871765 + 2.0 * 6.314408302307129
Epoch 710, val loss: 0.7060853242874146
Epoch 720, training loss: 12.806288719177246 = 0.17812436819076538 + 2.0 * 6.314082145690918
Epoch 720, val loss: 0.7070834636688232
Epoch 730, training loss: 12.79288387298584 = 0.1692817062139511 + 2.0 * 6.311800956726074
Epoch 730, val loss: 0.7084693312644958
Epoch 740, training loss: 12.793044090270996 = 0.1609271615743637 + 2.0 * 6.31605863571167
Epoch 740, val loss: 0.7101943492889404
Epoch 750, training loss: 12.77406120300293 = 0.15307454764842987 + 2.0 * 6.310493469238281
Epoch 750, val loss: 0.7121076583862305
Epoch 760, training loss: 12.775446891784668 = 0.1456589549779892 + 2.0 * 6.314894199371338
Epoch 760, val loss: 0.7143179774284363
Epoch 770, training loss: 12.757948875427246 = 0.13871294260025024 + 2.0 * 6.30961799621582
Epoch 770, val loss: 0.7167341709136963
Epoch 780, training loss: 12.745944023132324 = 0.13213519752025604 + 2.0 * 6.306904315948486
Epoch 780, val loss: 0.7194245457649231
Epoch 790, training loss: 12.73567008972168 = 0.12593111395835876 + 2.0 * 6.304869651794434
Epoch 790, val loss: 0.7223092317581177
Epoch 800, training loss: 12.73661994934082 = 0.12005595862865448 + 2.0 * 6.308281898498535
Epoch 800, val loss: 0.7254136204719543
Epoch 810, training loss: 12.72989273071289 = 0.11449278891086578 + 2.0 * 6.307700157165527
Epoch 810, val loss: 0.7286577820777893
Epoch 820, training loss: 12.715082168579102 = 0.10928071290254593 + 2.0 * 6.302900791168213
Epoch 820, val loss: 0.7319998741149902
Epoch 830, training loss: 12.706501960754395 = 0.10435324907302856 + 2.0 * 6.301074504852295
Epoch 830, val loss: 0.7354444861412048
Epoch 840, training loss: 12.719382286071777 = 0.09969191998243332 + 2.0 * 6.309844970703125
Epoch 840, val loss: 0.7390884160995483
Epoch 850, training loss: 12.702570915222168 = 0.09529007971286774 + 2.0 * 6.303640365600586
Epoch 850, val loss: 0.7426353693008423
Epoch 860, training loss: 12.688002586364746 = 0.09114522486925125 + 2.0 * 6.298428535461426
Epoch 860, val loss: 0.7463405728340149
Epoch 870, training loss: 12.681778907775879 = 0.08720950782299042 + 2.0 * 6.2972846031188965
Epoch 870, val loss: 0.7501816153526306
Epoch 880, training loss: 12.690583229064941 = 0.08348348736763 + 2.0 * 6.303549766540527
Epoch 880, val loss: 0.7540964484214783
Epoch 890, training loss: 12.67789077758789 = 0.07994614541530609 + 2.0 * 6.298972129821777
Epoch 890, val loss: 0.7580860257148743
Epoch 900, training loss: 12.672330856323242 = 0.07661651074886322 + 2.0 * 6.297857284545898
Epoch 900, val loss: 0.7621021866798401
Epoch 910, training loss: 12.660205841064453 = 0.07344847917556763 + 2.0 * 6.293378829956055
Epoch 910, val loss: 0.7661698460578918
Epoch 920, training loss: 12.655674934387207 = 0.07045425474643707 + 2.0 * 6.292610168457031
Epoch 920, val loss: 0.7703445553779602
Epoch 930, training loss: 12.664013862609863 = 0.06760363280773163 + 2.0 * 6.2982048988342285
Epoch 930, val loss: 0.7745611667633057
Epoch 940, training loss: 12.653985977172852 = 0.064918652176857 + 2.0 * 6.294533729553223
Epoch 940, val loss: 0.77872633934021
Epoch 950, training loss: 12.643594741821289 = 0.062373753637075424 + 2.0 * 6.290610313415527
Epoch 950, val loss: 0.7829639911651611
Epoch 960, training loss: 12.638688087463379 = 0.059962790459394455 + 2.0 * 6.28936243057251
Epoch 960, val loss: 0.7872418165206909
Epoch 970, training loss: 12.636096000671387 = 0.05766860395669937 + 2.0 * 6.28921365737915
Epoch 970, val loss: 0.7916030883789062
Epoch 980, training loss: 12.639165878295898 = 0.05549310892820358 + 2.0 * 6.291836261749268
Epoch 980, val loss: 0.7958979606628418
Epoch 990, training loss: 12.626971244812012 = 0.053427424281835556 + 2.0 * 6.286771774291992
Epoch 990, val loss: 0.8001916408538818
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7851851851851852
0.8402741170268846
=== training gcn model ===
Epoch 0, training loss: 19.139907836914062 = 1.9462547302246094 + 2.0 * 8.596826553344727
Epoch 0, val loss: 1.9440829753875732
Epoch 10, training loss: 19.129915237426758 = 1.9368901252746582 + 2.0 * 8.596512794494629
Epoch 10, val loss: 1.9351352453231812
Epoch 20, training loss: 19.113574981689453 = 1.9255934953689575 + 2.0 * 8.593990325927734
Epoch 20, val loss: 1.9238263368606567
Epoch 30, training loss: 19.058021545410156 = 1.9102684259414673 + 2.0 * 8.57387638092041
Epoch 30, val loss: 1.908161997795105
Epoch 40, training loss: 18.734895706176758 = 1.8912509679794312 + 2.0 * 8.421822547912598
Epoch 40, val loss: 1.8892489671707153
Epoch 50, training loss: 17.54530143737793 = 1.871092677116394 + 2.0 * 7.837104320526123
Epoch 50, val loss: 1.869739294052124
Epoch 60, training loss: 16.613161087036133 = 1.8563519716262817 + 2.0 * 7.378404140472412
Epoch 60, val loss: 1.8563309907913208
Epoch 70, training loss: 16.080703735351562 = 1.843225121498108 + 2.0 * 7.118739604949951
Epoch 70, val loss: 1.8433549404144287
Epoch 80, training loss: 15.71934700012207 = 1.830075740814209 + 2.0 * 6.94463586807251
Epoch 80, val loss: 1.8302887678146362
Epoch 90, training loss: 15.484395027160645 = 1.8167871236801147 + 2.0 * 6.833804130554199
Epoch 90, val loss: 1.8169341087341309
Epoch 100, training loss: 15.331077575683594 = 1.802960991859436 + 2.0 * 6.7640581130981445
Epoch 100, val loss: 1.8033310174942017
Epoch 110, training loss: 15.219527244567871 = 1.7887440919876099 + 2.0 * 6.715391635894775
Epoch 110, val loss: 1.7895960807800293
Epoch 120, training loss: 15.122095108032227 = 1.7745001316070557 + 2.0 * 6.673797607421875
Epoch 120, val loss: 1.775949239730835
Epoch 130, training loss: 15.042985916137695 = 1.7602046728134155 + 2.0 * 6.641390800476074
Epoch 130, val loss: 1.762415885925293
Epoch 140, training loss: 14.979459762573242 = 1.7454380989074707 + 2.0 * 6.617010593414307
Epoch 140, val loss: 1.7487043142318726
Epoch 150, training loss: 14.917299270629883 = 1.7294234037399292 + 2.0 * 6.593937873840332
Epoch 150, val loss: 1.734239101409912
Epoch 160, training loss: 14.860363006591797 = 1.7116239070892334 + 2.0 * 6.574369430541992
Epoch 160, val loss: 1.718540072441101
Epoch 170, training loss: 14.81010913848877 = 1.6914883852005005 + 2.0 * 6.559310436248779
Epoch 170, val loss: 1.7009639739990234
Epoch 180, training loss: 14.756261825561523 = 1.668669581413269 + 2.0 * 6.543796062469482
Epoch 180, val loss: 1.6810938119888306
Epoch 190, training loss: 14.704110145568848 = 1.6427236795425415 + 2.0 * 6.530693054199219
Epoch 190, val loss: 1.6585187911987305
Epoch 200, training loss: 14.653646469116211 = 1.6134147644042969 + 2.0 * 6.520115852355957
Epoch 200, val loss: 1.6329858303070068
Epoch 210, training loss: 14.595452308654785 = 1.580546498298645 + 2.0 * 6.507452964782715
Epoch 210, val loss: 1.6044695377349854
Epoch 220, training loss: 14.539880752563477 = 1.5438461303710938 + 2.0 * 6.498017311096191
Epoch 220, val loss: 1.5727869272232056
Epoch 230, training loss: 14.480033874511719 = 1.5035380125045776 + 2.0 * 6.488247871398926
Epoch 230, val loss: 1.5382928848266602
Epoch 240, training loss: 14.417028427124023 = 1.4600642919540405 + 2.0 * 6.478482246398926
Epoch 240, val loss: 1.501430869102478
Epoch 250, training loss: 14.354432106018066 = 1.4136173725128174 + 2.0 * 6.470407485961914
Epoch 250, val loss: 1.4624840021133423
Epoch 260, training loss: 14.289131164550781 = 1.3651331663131714 + 2.0 * 6.46199893951416
Epoch 260, val loss: 1.4222654104232788
Epoch 270, training loss: 14.237412452697754 = 1.3155258893966675 + 2.0 * 6.460943222045898
Epoch 270, val loss: 1.3818284273147583
Epoch 280, training loss: 14.165969848632812 = 1.2665108442306519 + 2.0 * 6.4497294425964355
Epoch 280, val loss: 1.3424642086029053
Epoch 290, training loss: 14.101499557495117 = 1.2182458639144897 + 2.0 * 6.441627025604248
Epoch 290, val loss: 1.3041932582855225
Epoch 300, training loss: 14.041106224060059 = 1.171112060546875 + 2.0 * 6.434997081756592
Epoch 300, val loss: 1.2672548294067383
Epoch 310, training loss: 13.987775802612305 = 1.1252739429473877 + 2.0 * 6.431251049041748
Epoch 310, val loss: 1.2319585084915161
Epoch 320, training loss: 13.928879737854004 = 1.0817097425460815 + 2.0 * 6.423584938049316
Epoch 320, val loss: 1.1987968683242798
Epoch 330, training loss: 13.874791145324707 = 1.039961814880371 + 2.0 * 6.417414665222168
Epoch 330, val loss: 1.1675258874893188
Epoch 340, training loss: 13.823404312133789 = 0.9999849796295166 + 2.0 * 6.411709785461426
Epoch 340, val loss: 1.1376938819885254
Epoch 350, training loss: 13.780271530151367 = 0.9617254137992859 + 2.0 * 6.409273147583008
Epoch 350, val loss: 1.1094996929168701
Epoch 360, training loss: 13.735233306884766 = 0.9251524209976196 + 2.0 * 6.405040264129639
Epoch 360, val loss: 1.082795262336731
Epoch 370, training loss: 13.685545921325684 = 0.8902415633201599 + 2.0 * 6.3976521492004395
Epoch 370, val loss: 1.057618260383606
Epoch 380, training loss: 13.650720596313477 = 0.8565850853919983 + 2.0 * 6.397067546844482
Epoch 380, val loss: 1.0335850715637207
Epoch 390, training loss: 13.604535102844238 = 0.8242998719215393 + 2.0 * 6.390117645263672
Epoch 390, val loss: 1.0106282234191895
Epoch 400, training loss: 13.564065933227539 = 0.7931042313575745 + 2.0 * 6.385480880737305
Epoch 400, val loss: 0.9888665080070496
Epoch 410, training loss: 13.533182144165039 = 0.7627976536750793 + 2.0 * 6.385192394256592
Epoch 410, val loss: 0.9680300951004028
Epoch 420, training loss: 13.496658325195312 = 0.7335772514343262 + 2.0 * 6.381540775299072
Epoch 420, val loss: 0.9481238722801208
Epoch 430, training loss: 13.456186294555664 = 0.7053132057189941 + 2.0 * 6.375436782836914
Epoch 430, val loss: 0.9293691515922546
Epoch 440, training loss: 13.420635223388672 = 0.6777840256690979 + 2.0 * 6.371425628662109
Epoch 440, val loss: 0.9114037156105042
Epoch 450, training loss: 13.393579483032227 = 0.6509528160095215 + 2.0 * 6.371313095092773
Epoch 450, val loss: 0.8941801190376282
Epoch 460, training loss: 13.35791015625 = 0.624819278717041 + 2.0 * 6.366545677185059
Epoch 460, val loss: 0.8780294060707092
Epoch 470, training loss: 13.330577850341797 = 0.5993156433105469 + 2.0 * 6.365631103515625
Epoch 470, val loss: 0.8626025915145874
Epoch 480, training loss: 13.29468822479248 = 0.574407160282135 + 2.0 * 6.360140323638916
Epoch 480, val loss: 0.8478073477745056
Epoch 490, training loss: 13.263248443603516 = 0.549830436706543 + 2.0 * 6.356709003448486
Epoch 490, val loss: 0.8338007926940918
Epoch 500, training loss: 13.241965293884277 = 0.5256766676902771 + 2.0 * 6.358144283294678
Epoch 500, val loss: 0.8203141093254089
Epoch 510, training loss: 13.214651107788086 = 0.5018468499183655 + 2.0 * 6.3564019203186035
Epoch 510, val loss: 0.8075559735298157
Epoch 520, training loss: 13.179332733154297 = 0.47850969433784485 + 2.0 * 6.350411415100098
Epoch 520, val loss: 0.795525074005127
Epoch 530, training loss: 13.155213356018066 = 0.45559966564178467 + 2.0 * 6.349806785583496
Epoch 530, val loss: 0.7842145562171936
Epoch 540, training loss: 13.127304077148438 = 0.4332813620567322 + 2.0 * 6.347011566162109
Epoch 540, val loss: 0.7738432884216309
Epoch 550, training loss: 13.096794128417969 = 0.4115087389945984 + 2.0 * 6.342642784118652
Epoch 550, val loss: 0.7644069790840149
Epoch 560, training loss: 13.07371711730957 = 0.39025554060935974 + 2.0 * 6.34173059463501
Epoch 560, val loss: 0.7558658719062805
Epoch 570, training loss: 13.04786491394043 = 0.36968493461608887 + 2.0 * 6.339089870452881
Epoch 570, val loss: 0.7483067512512207
Epoch 580, training loss: 13.028522491455078 = 0.34988662600517273 + 2.0 * 6.339317798614502
Epoch 580, val loss: 0.7419517636299133
Epoch 590, training loss: 13.001028060913086 = 0.33089277148246765 + 2.0 * 6.3350677490234375
Epoch 590, val loss: 0.7366196513175964
Epoch 600, training loss: 12.98004150390625 = 0.31268230080604553 + 2.0 * 6.333679676055908
Epoch 600, val loss: 0.7323144674301147
Epoch 610, training loss: 12.966815948486328 = 0.29532238841056824 + 2.0 * 6.335746765136719
Epoch 610, val loss: 0.7287546992301941
Epoch 620, training loss: 12.94343376159668 = 0.2789419889450073 + 2.0 * 6.332245826721191
Epoch 620, val loss: 0.7263809442520142
Epoch 630, training loss: 12.92231559753418 = 0.26345551013946533 + 2.0 * 6.329430103302002
Epoch 630, val loss: 0.724923849105835
Epoch 640, training loss: 12.904587745666504 = 0.24879920482635498 + 2.0 * 6.32789421081543
Epoch 640, val loss: 0.7241808772087097
Epoch 650, training loss: 12.901839256286621 = 0.23502017557621002 + 2.0 * 6.333409309387207
Epoch 650, val loss: 0.7240902781486511
Epoch 660, training loss: 12.872695922851562 = 0.22208338975906372 + 2.0 * 6.325306415557861
Epoch 660, val loss: 0.7247089743614197
Epoch 670, training loss: 12.855268478393555 = 0.20999185740947723 + 2.0 * 6.322638511657715
Epoch 670, val loss: 0.725990891456604
Epoch 680, training loss: 12.840376853942871 = 0.19860389828681946 + 2.0 * 6.320886611938477
Epoch 680, val loss: 0.7277047634124756
Epoch 690, training loss: 12.82674503326416 = 0.18788322806358337 + 2.0 * 6.319430828094482
Epoch 690, val loss: 0.7297675013542175
Epoch 700, training loss: 12.822900772094727 = 0.17784199118614197 + 2.0 * 6.322529315948486
Epoch 700, val loss: 0.7322569489479065
Epoch 710, training loss: 12.808197975158691 = 0.1684677004814148 + 2.0 * 6.3198652267456055
Epoch 710, val loss: 0.735132098197937
Epoch 720, training loss: 12.791963577270508 = 0.15968194603919983 + 2.0 * 6.316140651702881
Epoch 720, val loss: 0.7384663224220276
Epoch 730, training loss: 12.778346061706543 = 0.15142937004566193 + 2.0 * 6.313458442687988
Epoch 730, val loss: 0.7419053316116333
Epoch 740, training loss: 12.767885208129883 = 0.14366619288921356 + 2.0 * 6.312109470367432
Epoch 740, val loss: 0.7456788420677185
Epoch 750, training loss: 12.800454139709473 = 0.13638608157634735 + 2.0 * 6.332034111022949
Epoch 750, val loss: 0.7495660781860352
Epoch 760, training loss: 12.76043701171875 = 0.12959516048431396 + 2.0 * 6.315421104431152
Epoch 760, val loss: 0.7537302374839783
Epoch 770, training loss: 12.744091033935547 = 0.12323015183210373 + 2.0 * 6.310430526733398
Epoch 770, val loss: 0.7581875920295715
Epoch 780, training loss: 12.732434272766113 = 0.11724983155727386 + 2.0 * 6.307592391967773
Epoch 780, val loss: 0.7626261115074158
Epoch 790, training loss: 12.72370433807373 = 0.1116068884730339 + 2.0 * 6.30604887008667
Epoch 790, val loss: 0.7671400904655457
Epoch 800, training loss: 12.737070083618164 = 0.10630901157855988 + 2.0 * 6.315380573272705
Epoch 800, val loss: 0.7718023061752319
Epoch 810, training loss: 12.719292640686035 = 0.10130828619003296 + 2.0 * 6.308992385864258
Epoch 810, val loss: 0.7764298319816589
Epoch 820, training loss: 12.702205657958984 = 0.09664350003004074 + 2.0 * 6.302781105041504
Epoch 820, val loss: 0.781305193901062
Epoch 830, training loss: 12.696293830871582 = 0.09224829822778702 + 2.0 * 6.302022933959961
Epoch 830, val loss: 0.7862393856048584
Epoch 840, training loss: 12.69286823272705 = 0.08811155706644058 + 2.0 * 6.302378177642822
Epoch 840, val loss: 0.7911674976348877
Epoch 850, training loss: 12.68221664428711 = 0.08419746905565262 + 2.0 * 6.299009799957275
Epoch 850, val loss: 0.7961052060127258
Epoch 860, training loss: 12.682394981384277 = 0.08050879836082458 + 2.0 * 6.300942897796631
Epoch 860, val loss: 0.8012469410896301
Epoch 870, training loss: 12.675155639648438 = 0.07704108953475952 + 2.0 * 6.299057483673096
Epoch 870, val loss: 0.8062840700149536
Epoch 880, training loss: 12.671630859375 = 0.07375846803188324 + 2.0 * 6.298936367034912
Epoch 880, val loss: 0.8115257620811462
Epoch 890, training loss: 12.665153503417969 = 0.07067432254552841 + 2.0 * 6.297239780426025
Epoch 890, val loss: 0.8167206048965454
Epoch 900, training loss: 12.656644821166992 = 0.06774593889713287 + 2.0 * 6.294449329376221
Epoch 900, val loss: 0.8220247030258179
Epoch 910, training loss: 12.6593017578125 = 0.06499054282903671 + 2.0 * 6.297155380249023
Epoch 910, val loss: 0.8273633122444153
Epoch 920, training loss: 12.654772758483887 = 0.06237804889678955 + 2.0 * 6.296197414398193
Epoch 920, val loss: 0.832546055316925
Epoch 930, training loss: 12.642492294311523 = 0.05990946665406227 + 2.0 * 6.291291236877441
Epoch 930, val loss: 0.8379308581352234
Epoch 940, training loss: 12.637798309326172 = 0.057576194405555725 + 2.0 * 6.290111064910889
Epoch 940, val loss: 0.8433279395103455
Epoch 950, training loss: 12.639567375183105 = 0.05537108704447746 + 2.0 * 6.292098045349121
Epoch 950, val loss: 0.8487423062324524
Epoch 960, training loss: 12.639095306396484 = 0.053281959146261215 + 2.0 * 6.292906761169434
Epoch 960, val loss: 0.8541640043258667
Epoch 970, training loss: 12.630546569824219 = 0.05129876732826233 + 2.0 * 6.289623737335205
Epoch 970, val loss: 0.8595983386039734
Epoch 980, training loss: 12.624885559082031 = 0.049419716000556946 + 2.0 * 6.28773307800293
Epoch 980, val loss: 0.8651825785636902
Epoch 990, training loss: 12.625041961669922 = 0.04764866828918457 + 2.0 * 6.288696765899658
Epoch 990, val loss: 0.8706904649734497
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8111111111111111
0.8413284132841329
=== training gcn model ===
Epoch 0, training loss: 19.129135131835938 = 1.9354090690612793 + 2.0 * 8.59686279296875
Epoch 0, val loss: 1.9343478679656982
Epoch 10, training loss: 19.118778228759766 = 1.9254686832427979 + 2.0 * 8.596654891967773
Epoch 10, val loss: 1.9240713119506836
Epoch 20, training loss: 19.103540420532227 = 1.9135189056396484 + 2.0 * 8.595010757446289
Epoch 20, val loss: 1.911500096321106
Epoch 30, training loss: 19.062551498413086 = 1.8972630500793457 + 2.0 * 8.58264446258545
Epoch 30, val loss: 1.8943854570388794
Epoch 40, training loss: 18.904590606689453 = 1.8760130405426025 + 2.0 * 8.514288902282715
Epoch 40, val loss: 1.872908115386963
Epoch 50, training loss: 18.24235725402832 = 1.8536791801452637 + 2.0 * 8.19433879852295
Epoch 50, val loss: 1.8513587713241577
Epoch 60, training loss: 17.284591674804688 = 1.8324289321899414 + 2.0 * 7.726081848144531
Epoch 60, val loss: 1.8320651054382324
Epoch 70, training loss: 16.375104904174805 = 1.8169749975204468 + 2.0 * 7.279064655303955
Epoch 70, val loss: 1.8182169198989868
Epoch 80, training loss: 15.94308853149414 = 1.803043246269226 + 2.0 * 7.0700225830078125
Epoch 80, val loss: 1.8045597076416016
Epoch 90, training loss: 15.737844467163086 = 1.787233829498291 + 2.0 * 6.975305080413818
Epoch 90, val loss: 1.789399266242981
Epoch 100, training loss: 15.533302307128906 = 1.7705973386764526 + 2.0 * 6.881352424621582
Epoch 100, val loss: 1.7739108800888062
Epoch 110, training loss: 15.375069618225098 = 1.7551438808441162 + 2.0 * 6.809962749481201
Epoch 110, val loss: 1.7596886157989502
Epoch 120, training loss: 15.2477445602417 = 1.7389262914657593 + 2.0 * 6.754409313201904
Epoch 120, val loss: 1.7450811862945557
Epoch 130, training loss: 15.148426055908203 = 1.7209552526474 + 2.0 * 6.713735580444336
Epoch 130, val loss: 1.7291836738586426
Epoch 140, training loss: 15.05876350402832 = 1.7009530067443848 + 2.0 * 6.678905487060547
Epoch 140, val loss: 1.7118351459503174
Epoch 150, training loss: 14.97663688659668 = 1.6787056922912598 + 2.0 * 6.648965358734131
Epoch 150, val loss: 1.6929174661636353
Epoch 160, training loss: 14.891042709350586 = 1.6538100242614746 + 2.0 * 6.618616104125977
Epoch 160, val loss: 1.6719386577606201
Epoch 170, training loss: 14.808205604553223 = 1.625440001487732 + 2.0 * 6.59138298034668
Epoch 170, val loss: 1.64816153049469
Epoch 180, training loss: 14.734177589416504 = 1.5928542613983154 + 2.0 * 6.570661544799805
Epoch 180, val loss: 1.621038794517517
Epoch 190, training loss: 14.65218448638916 = 1.555934190750122 + 2.0 * 6.548125267028809
Epoch 190, val loss: 1.5902652740478516
Epoch 200, training loss: 14.579751968383789 = 1.514427900314331 + 2.0 * 6.5326619148254395
Epoch 200, val loss: 1.5556343793869019
Epoch 210, training loss: 14.503997802734375 = 1.4683308601379395 + 2.0 * 6.517833232879639
Epoch 210, val loss: 1.5174087285995483
Epoch 220, training loss: 14.433125495910645 = 1.418135643005371 + 2.0 * 6.507494926452637
Epoch 220, val loss: 1.4761650562286377
Epoch 230, training loss: 14.354264259338379 = 1.365427851676941 + 2.0 * 6.494418144226074
Epoch 230, val loss: 1.4332867860794067
Epoch 240, training loss: 14.27806568145752 = 1.3109720945358276 + 2.0 * 6.483546733856201
Epoch 240, val loss: 1.3893959522247314
Epoch 250, training loss: 14.223590850830078 = 1.2554792165756226 + 2.0 * 6.484055995941162
Epoch 250, val loss: 1.34532630443573
Epoch 260, training loss: 14.133237838745117 = 1.2010810375213623 + 2.0 * 6.466078281402588
Epoch 260, val loss: 1.302941918373108
Epoch 270, training loss: 14.063664436340332 = 1.1480672359466553 + 2.0 * 6.457798480987549
Epoch 270, val loss: 1.2621561288833618
Epoch 280, training loss: 13.996780395507812 = 1.0966705083847046 + 2.0 * 6.450055122375488
Epoch 280, val loss: 1.223065972328186
Epoch 290, training loss: 13.93360424041748 = 1.0474622249603271 + 2.0 * 6.443070888519287
Epoch 290, val loss: 1.1862788200378418
Epoch 300, training loss: 13.874799728393555 = 1.0012884140014648 + 2.0 * 6.436755657196045
Epoch 300, val loss: 1.1521904468536377
Epoch 310, training loss: 13.817957878112793 = 0.9575516581535339 + 2.0 * 6.430202960968018
Epoch 310, val loss: 1.1204389333724976
Epoch 320, training loss: 13.765717506408691 = 0.916388750076294 + 2.0 * 6.424664497375488
Epoch 320, val loss: 1.0910511016845703
Epoch 330, training loss: 13.713483810424805 = 0.8778578639030457 + 2.0 * 6.417812824249268
Epoch 330, val loss: 1.0640100240707397
Epoch 340, training loss: 13.676138877868652 = 0.8416330814361572 + 2.0 * 6.417253017425537
Epoch 340, val loss: 1.0390349626541138
Epoch 350, training loss: 13.6277494430542 = 0.8077942132949829 + 2.0 * 6.409977436065674
Epoch 350, val loss: 1.0161426067352295
Epoch 360, training loss: 13.582568168640137 = 0.7759409546852112 + 2.0 * 6.403313636779785
Epoch 360, val loss: 0.9951459169387817
Epoch 370, training loss: 13.546533584594727 = 0.7459092140197754 + 2.0 * 6.400312423706055
Epoch 370, val loss: 0.9759278297424316
Epoch 380, training loss: 13.51536750793457 = 0.7176137566566467 + 2.0 * 6.398876667022705
Epoch 380, val loss: 0.9584614038467407
Epoch 390, training loss: 13.476207733154297 = 0.6911179423332214 + 2.0 * 6.392544746398926
Epoch 390, val loss: 0.9429031014442444
Epoch 400, training loss: 13.442313194274902 = 0.6660460829734802 + 2.0 * 6.388133525848389
Epoch 400, val loss: 0.9289736151695251
Epoch 410, training loss: 13.418855667114258 = 0.6422057747840881 + 2.0 * 6.388324737548828
Epoch 410, val loss: 0.9166051745414734
Epoch 420, training loss: 13.384621620178223 = 0.6197556257247925 + 2.0 * 6.38243293762207
Epoch 420, val loss: 0.9055948853492737
Epoch 430, training loss: 13.359428405761719 = 0.59844571352005 + 2.0 * 6.380491256713867
Epoch 430, val loss: 0.8961042165756226
Epoch 440, training loss: 13.328459739685059 = 0.578116238117218 + 2.0 * 6.375171661376953
Epoch 440, val loss: 0.8877209424972534
Epoch 450, training loss: 13.307902336120605 = 0.5585997700691223 + 2.0 * 6.3746514320373535
Epoch 450, val loss: 0.8804143071174622
Epoch 460, training loss: 13.279383659362793 = 0.5398332476615906 + 2.0 * 6.369775295257568
Epoch 460, val loss: 0.8741204738616943
Epoch 470, training loss: 13.260110855102539 = 0.5217077136039734 + 2.0 * 6.36920166015625
Epoch 470, val loss: 0.8686373829841614
Epoch 480, training loss: 13.238469123840332 = 0.5041838884353638 + 2.0 * 6.367142677307129
Epoch 480, val loss: 0.8637515902519226
Epoch 490, training loss: 13.215598106384277 = 0.4872790277004242 + 2.0 * 6.36415958404541
Epoch 490, val loss: 0.8597381711006165
Epoch 500, training loss: 13.191537857055664 = 0.4708518385887146 + 2.0 * 6.360342979431152
Epoch 500, val loss: 0.8561930656433105
Epoch 510, training loss: 13.18329906463623 = 0.45487314462661743 + 2.0 * 6.364212989807129
Epoch 510, val loss: 0.853065013885498
Epoch 520, training loss: 13.157928466796875 = 0.4393693804740906 + 2.0 * 6.359279632568359
Epoch 520, val loss: 0.8505772352218628
Epoch 530, training loss: 13.135106086730957 = 0.42443445324897766 + 2.0 * 6.355335712432861
Epoch 530, val loss: 0.8484377264976501
Epoch 540, training loss: 13.115493774414062 = 0.40997427701950073 + 2.0 * 6.352759838104248
Epoch 540, val loss: 0.8467448353767395
Epoch 550, training loss: 13.095940589904785 = 0.3959279954433441 + 2.0 * 6.350006103515625
Epoch 550, val loss: 0.845336377620697
Epoch 560, training loss: 13.090751647949219 = 0.3823069632053375 + 2.0 * 6.354222297668457
Epoch 560, val loss: 0.8442403674125671
Epoch 570, training loss: 13.064746856689453 = 0.36912307143211365 + 2.0 * 6.347811698913574
Epoch 570, val loss: 0.8435671925544739
Epoch 580, training loss: 13.051331520080566 = 0.3563649654388428 + 2.0 * 6.347483158111572
Epoch 580, val loss: 0.8429768681526184
Epoch 590, training loss: 13.032455444335938 = 0.34403157234191895 + 2.0 * 6.344212055206299
Epoch 590, val loss: 0.8425291180610657
Epoch 600, training loss: 13.01419734954834 = 0.33202046155929565 + 2.0 * 6.34108829498291
Epoch 600, val loss: 0.8423300981521606
Epoch 610, training loss: 12.997904777526855 = 0.32025596499443054 + 2.0 * 6.338824272155762
Epoch 610, val loss: 0.8422989845275879
Epoch 620, training loss: 12.984875679016113 = 0.30873429775238037 + 2.0 * 6.338070869445801
Epoch 620, val loss: 0.8423581123352051
Epoch 630, training loss: 12.972469329833984 = 0.2974480390548706 + 2.0 * 6.337510585784912
Epoch 630, val loss: 0.8424245119094849
Epoch 640, training loss: 12.96474838256836 = 0.2863570749759674 + 2.0 * 6.339195728302002
Epoch 640, val loss: 0.8426269888877869
Epoch 650, training loss: 12.94098949432373 = 0.2754356265068054 + 2.0 * 6.33277702331543
Epoch 650, val loss: 0.8426080346107483
Epoch 660, training loss: 12.926471710205078 = 0.26463979482650757 + 2.0 * 6.330915927886963
Epoch 660, val loss: 0.8429330587387085
Epoch 670, training loss: 12.913375854492188 = 0.25394827127456665 + 2.0 * 6.329713821411133
Epoch 670, val loss: 0.843181848526001
Epoch 680, training loss: 12.910332679748535 = 0.24336518347263336 + 2.0 * 6.333483695983887
Epoch 680, val loss: 0.8434516191482544
Epoch 690, training loss: 12.88597583770752 = 0.23296211659908295 + 2.0 * 6.326507091522217
Epoch 690, val loss: 0.843742311000824
Epoch 700, training loss: 12.872660636901855 = 0.22273766994476318 + 2.0 * 6.3249616622924805
Epoch 700, val loss: 0.8442020416259766
Epoch 710, training loss: 12.859085083007812 = 0.21271030604839325 + 2.0 * 6.323187351226807
Epoch 710, val loss: 0.8446528315544128
Epoch 720, training loss: 12.889289855957031 = 0.2029241919517517 + 2.0 * 6.3431830406188965
Epoch 720, val loss: 0.8451364040374756
Epoch 730, training loss: 12.843623161315918 = 0.19345079362392426 + 2.0 * 6.3250861167907715
Epoch 730, val loss: 0.8457490801811218
Epoch 740, training loss: 12.82503890991211 = 0.18435698747634888 + 2.0 * 6.320341110229492
Epoch 740, val loss: 0.8466289639472961
Epoch 750, training loss: 12.812057495117188 = 0.17561326920986176 + 2.0 * 6.3182220458984375
Epoch 750, val loss: 0.8478192687034607
Epoch 760, training loss: 12.80066204071045 = 0.1672159880399704 + 2.0 * 6.316722869873047
Epoch 760, val loss: 0.8491758108139038
Epoch 770, training loss: 12.79521656036377 = 0.15918034315109253 + 2.0 * 6.318017959594727
Epoch 770, val loss: 0.8506821990013123
Epoch 780, training loss: 12.784830093383789 = 0.15153072774410248 + 2.0 * 6.316649913787842
Epoch 780, val loss: 0.8522681593894958
Epoch 790, training loss: 12.772608757019043 = 0.14428192377090454 + 2.0 * 6.3141632080078125
Epoch 790, val loss: 0.8540925979614258
Epoch 800, training loss: 12.766584396362305 = 0.13742488622665405 + 2.0 * 6.314579963684082
Epoch 800, val loss: 0.856137216091156
Epoch 810, training loss: 12.754685401916504 = 0.13093197345733643 + 2.0 * 6.3118767738342285
Epoch 810, val loss: 0.8583390116691589
Epoch 820, training loss: 12.745987892150879 = 0.12480054795742035 + 2.0 * 6.310593605041504
Epoch 820, val loss: 0.8606321811676025
Epoch 830, training loss: 12.738075256347656 = 0.11900445073843002 + 2.0 * 6.309535503387451
Epoch 830, val loss: 0.8631312847137451
Epoch 840, training loss: 12.729731559753418 = 0.113517165184021 + 2.0 * 6.308107376098633
Epoch 840, val loss: 0.865770161151886
Epoch 850, training loss: 12.731220245361328 = 0.10831417888402939 + 2.0 * 6.311452865600586
Epoch 850, val loss: 0.8684735894203186
Epoch 860, training loss: 12.730424880981445 = 0.10340265184640884 + 2.0 * 6.313510894775391
Epoch 860, val loss: 0.8713496327400208
Epoch 870, training loss: 12.711197853088379 = 0.0987713485956192 + 2.0 * 6.30621337890625
Epoch 870, val loss: 0.8741896748542786
Epoch 880, training loss: 12.702704429626465 = 0.09440334886312485 + 2.0 * 6.304150581359863
Epoch 880, val loss: 0.8772183656692505
Epoch 890, training loss: 12.710286140441895 = 0.09026321768760681 + 2.0 * 6.310011386871338
Epoch 890, val loss: 0.8803858757019043
Epoch 900, training loss: 12.695110321044922 = 0.08634845912456512 + 2.0 * 6.304380893707275
Epoch 900, val loss: 0.8833835124969482
Epoch 910, training loss: 12.687261581420898 = 0.08264404535293579 + 2.0 * 6.302308559417725
Epoch 910, val loss: 0.8867526650428772
Epoch 920, training loss: 12.680678367614746 = 0.07914196699857712 + 2.0 * 6.3007683753967285
Epoch 920, val loss: 0.8901216983795166
Epoch 930, training loss: 12.694835662841797 = 0.07582493126392365 + 2.0 * 6.309505462646484
Epoch 930, val loss: 0.8935964703559875
Epoch 940, training loss: 12.6707181930542 = 0.0726756677031517 + 2.0 * 6.299021244049072
Epoch 940, val loss: 0.8970710039138794
Epoch 950, training loss: 12.666409492492676 = 0.06970730423927307 + 2.0 * 6.298351287841797
Epoch 950, val loss: 0.9005277156829834
Epoch 960, training loss: 12.65993881225586 = 0.06689576804637909 + 2.0 * 6.2965216636657715
Epoch 960, val loss: 0.9041771292686462
Epoch 970, training loss: 12.668438911437988 = 0.06423105299472809 + 2.0 * 6.3021039962768555
Epoch 970, val loss: 0.9077807664871216
Epoch 980, training loss: 12.66445541381836 = 0.061691317707300186 + 2.0 * 6.301382064819336
Epoch 980, val loss: 0.9113820791244507
Epoch 990, training loss: 12.649863243103027 = 0.059298623353242874 + 2.0 * 6.295282363891602
Epoch 990, val loss: 0.9150204062461853
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7703703703703704
0.8365840801265156
The final CL Acc:0.78889, 0.01684, The final GNN Acc:0.83940, 0.00203
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.5 der2:0.3 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11566])
remove edge: torch.Size([2, 9522])
updated graph: torch.Size([2, 10532])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 19.133146286010742 = 1.9394819736480713 + 2.0 * 8.596832275390625
Epoch 0, val loss: 1.9400254487991333
Epoch 10, training loss: 19.122779846191406 = 1.9296008348464966 + 2.0 * 8.596589088439941
Epoch 10, val loss: 1.9302504062652588
Epoch 20, training loss: 19.1076602935791 = 1.9176627397537231 + 2.0 * 8.594998359680176
Epoch 20, val loss: 1.9183738231658936
Epoch 30, training loss: 19.06920051574707 = 1.901458978652954 + 2.0 * 8.583870887756348
Epoch 30, val loss: 1.9021278619766235
Epoch 40, training loss: 18.924396514892578 = 1.8805546760559082 + 2.0 * 8.521921157836914
Epoch 40, val loss: 1.8818010091781616
Epoch 50, training loss: 18.216079711914062 = 1.8600645065307617 + 2.0 * 8.178008079528809
Epoch 50, val loss: 1.8617796897888184
Epoch 60, training loss: 16.97119140625 = 1.8418681621551514 + 2.0 * 7.564661979675293
Epoch 60, val loss: 1.8441896438598633
Epoch 70, training loss: 16.275632858276367 = 1.827545166015625 + 2.0 * 7.224043846130371
Epoch 70, val loss: 1.8312240839004517
Epoch 80, training loss: 15.895740509033203 = 1.8148404359817505 + 2.0 * 7.040450096130371
Epoch 80, val loss: 1.819007158279419
Epoch 90, training loss: 15.655765533447266 = 1.80210542678833 + 2.0 * 6.926829814910889
Epoch 90, val loss: 1.8072460889816284
Epoch 100, training loss: 15.48402214050293 = 1.789170742034912 + 2.0 * 6.847425937652588
Epoch 100, val loss: 1.7950799465179443
Epoch 110, training loss: 15.338706016540527 = 1.7765294313430786 + 2.0 * 6.781088352203369
Epoch 110, val loss: 1.7833566665649414
Epoch 120, training loss: 15.222286224365234 = 1.7644095420837402 + 2.0 * 6.728938102722168
Epoch 120, val loss: 1.7724432945251465
Epoch 130, training loss: 15.127896308898926 = 1.7523858547210693 + 2.0 * 6.687755107879639
Epoch 130, val loss: 1.7618510723114014
Epoch 140, training loss: 15.04802131652832 = 1.7393559217453003 + 2.0 * 6.654332637786865
Epoch 140, val loss: 1.7507096529006958
Epoch 150, training loss: 14.975431442260742 = 1.7247682809829712 + 2.0 * 6.625331401824951
Epoch 150, val loss: 1.7383928298950195
Epoch 160, training loss: 14.909348487854004 = 1.708440899848938 + 2.0 * 6.600453853607178
Epoch 160, val loss: 1.7249019145965576
Epoch 170, training loss: 14.844263076782227 = 1.6903060674667358 + 2.0 * 6.57697868347168
Epoch 170, val loss: 1.7099250555038452
Epoch 180, training loss: 14.78146743774414 = 1.669877052307129 + 2.0 * 6.555795192718506
Epoch 180, val loss: 1.6932846307754517
Epoch 190, training loss: 14.725258827209473 = 1.6466716527938843 + 2.0 * 6.5392937660217285
Epoch 190, val loss: 1.674559235572815
Epoch 200, training loss: 14.669313430786133 = 1.620482325553894 + 2.0 * 6.524415493011475
Epoch 200, val loss: 1.6533631086349487
Epoch 210, training loss: 14.610833168029785 = 1.5910881757736206 + 2.0 * 6.5098724365234375
Epoch 210, val loss: 1.6295840740203857
Epoch 220, training loss: 14.55386734008789 = 1.557924747467041 + 2.0 * 6.497971534729004
Epoch 220, val loss: 1.6028691530227661
Epoch 230, training loss: 14.49862289428711 = 1.521010398864746 + 2.0 * 6.488806247711182
Epoch 230, val loss: 1.5731021165847778
Epoch 240, training loss: 14.436869621276855 = 1.4803926944732666 + 2.0 * 6.478238582611084
Epoch 240, val loss: 1.5404545068740845
Epoch 250, training loss: 14.376233100891113 = 1.436138391494751 + 2.0 * 6.470047473907471
Epoch 250, val loss: 1.5050337314605713
Epoch 260, training loss: 14.31822395324707 = 1.3891773223876953 + 2.0 * 6.4645233154296875
Epoch 260, val loss: 1.4676117897033691
Epoch 270, training loss: 14.253705978393555 = 1.3404805660247803 + 2.0 * 6.456612586975098
Epoch 270, val loss: 1.4291729927062988
Epoch 280, training loss: 14.189579963684082 = 1.290522813796997 + 2.0 * 6.449528694152832
Epoch 280, val loss: 1.3902170658111572
Epoch 290, training loss: 14.127456665039062 = 1.2399678230285645 + 2.0 * 6.44374418258667
Epoch 290, val loss: 1.3512475490570068
Epoch 300, training loss: 14.067178726196289 = 1.1897697448730469 + 2.0 * 6.438704490661621
Epoch 300, val loss: 1.313227891921997
Epoch 310, training loss: 14.009489059448242 = 1.1410034894943237 + 2.0 * 6.4342427253723145
Epoch 310, val loss: 1.276843786239624
Epoch 320, training loss: 13.952224731445312 = 1.0935639142990112 + 2.0 * 6.429330348968506
Epoch 320, val loss: 1.2421340942382812
Epoch 330, training loss: 13.895952224731445 = 1.0477358102798462 + 2.0 * 6.424108028411865
Epoch 330, val loss: 1.2092658281326294
Epoch 340, training loss: 13.844428062438965 = 1.0037020444869995 + 2.0 * 6.420362949371338
Epoch 340, val loss: 1.1783417463302612
Epoch 350, training loss: 13.79236125946045 = 0.9612828493118286 + 2.0 * 6.415539264678955
Epoch 350, val loss: 1.1493116617202759
Epoch 360, training loss: 13.747732162475586 = 0.9206512570381165 + 2.0 * 6.413540363311768
Epoch 360, val loss: 1.122252106666565
Epoch 370, training loss: 13.699295043945312 = 0.8820379376411438 + 2.0 * 6.408628463745117
Epoch 370, val loss: 1.0973014831542969
Epoch 380, training loss: 13.653048515319824 = 0.8452426195144653 + 2.0 * 6.403903007507324
Epoch 380, val loss: 1.0742714405059814
Epoch 390, training loss: 13.617305755615234 = 0.8100621700286865 + 2.0 * 6.403621673583984
Epoch 390, val loss: 1.0529961585998535
Epoch 400, training loss: 13.573929786682129 = 0.7765990495681763 + 2.0 * 6.398665428161621
Epoch 400, val loss: 1.0336129665374756
Epoch 410, training loss: 13.53075122833252 = 0.7448987364768982 + 2.0 * 6.392926216125488
Epoch 410, val loss: 1.0162372589111328
Epoch 420, training loss: 13.5087308883667 = 0.714712381362915 + 2.0 * 6.397009372711182
Epoch 420, val loss: 1.0005496740341187
Epoch 430, training loss: 13.46358585357666 = 0.6862925887107849 + 2.0 * 6.388646602630615
Epoch 430, val loss: 0.9865880012512207
Epoch 440, training loss: 13.42733097076416 = 0.659263014793396 + 2.0 * 6.384034156799316
Epoch 440, val loss: 0.9742623567581177
Epoch 450, training loss: 13.411051750183105 = 0.6334592700004578 + 2.0 * 6.388796329498291
Epoch 450, val loss: 0.9634037613868713
Epoch 460, training loss: 13.36658000946045 = 0.6088557839393616 + 2.0 * 6.378861904144287
Epoch 460, val loss: 0.9538043141365051
Epoch 470, training loss: 13.336760520935059 = 0.5854687094688416 + 2.0 * 6.375646114349365
Epoch 470, val loss: 0.9455432295799255
Epoch 480, training loss: 13.309355735778809 = 0.5630199909210205 + 2.0 * 6.373167991638184
Epoch 480, val loss: 0.9384548664093018
Epoch 490, training loss: 13.285938262939453 = 0.5415379405021667 + 2.0 * 6.372200012207031
Epoch 490, val loss: 0.9323060512542725
Epoch 500, training loss: 13.263221740722656 = 0.521112322807312 + 2.0 * 6.371054649353027
Epoch 500, val loss: 0.9273163676261902
Epoch 510, training loss: 13.234375 = 0.5016369819641113 + 2.0 * 6.366369247436523
Epoch 510, val loss: 0.9232379198074341
Epoch 520, training loss: 13.208563804626465 = 0.4828496277332306 + 2.0 * 6.362856864929199
Epoch 520, val loss: 0.9199082255363464
Epoch 530, training loss: 13.18580436706543 = 0.4646757245063782 + 2.0 * 6.360564231872559
Epoch 530, val loss: 0.9172543883323669
Epoch 540, training loss: 13.168761253356934 = 0.4470862150192261 + 2.0 * 6.360837459564209
Epoch 540, val loss: 0.9152529835700989
Epoch 550, training loss: 13.143681526184082 = 0.43015989661216736 + 2.0 * 6.3567609786987305
Epoch 550, val loss: 0.9139638543128967
Epoch 560, training loss: 13.120942115783691 = 0.41369760036468506 + 2.0 * 6.3536224365234375
Epoch 560, val loss: 0.9131914377212524
Epoch 570, training loss: 13.118049621582031 = 0.39766374230384827 + 2.0 * 6.360192775726318
Epoch 570, val loss: 0.9128730893135071
Epoch 580, training loss: 13.083026885986328 = 0.38206616044044495 + 2.0 * 6.350480556488037
Epoch 580, val loss: 0.9131917953491211
Epoch 590, training loss: 13.062620162963867 = 0.36699149012565613 + 2.0 * 6.347814559936523
Epoch 590, val loss: 0.9141115546226501
Epoch 600, training loss: 13.048079490661621 = 0.35228294134140015 + 2.0 * 6.347898483276367
Epoch 600, val loss: 0.9153586626052856
Epoch 610, training loss: 13.028999328613281 = 0.33804357051849365 + 2.0 * 6.345478057861328
Epoch 610, val loss: 0.917209267616272
Epoch 620, training loss: 13.009007453918457 = 0.32421132922172546 + 2.0 * 6.342398166656494
Epoch 620, val loss: 0.91960209608078
Epoch 630, training loss: 12.989874839782715 = 0.31079649925231934 + 2.0 * 6.339539051055908
Epoch 630, val loss: 0.9224288463592529
Epoch 640, training loss: 12.991307258605957 = 0.29780614376068115 + 2.0 * 6.346750736236572
Epoch 640, val loss: 0.9257310032844543
Epoch 650, training loss: 12.961080551147461 = 0.2852165699005127 + 2.0 * 6.337932109832764
Epoch 650, val loss: 0.9294275641441345
Epoch 660, training loss: 12.941817283630371 = 0.27314493060112 + 2.0 * 6.334336280822754
Epoch 660, val loss: 0.9336763620376587
Epoch 670, training loss: 12.944944381713867 = 0.2615100145339966 + 2.0 * 6.34171724319458
Epoch 670, val loss: 0.9383420944213867
Epoch 680, training loss: 12.917052268981934 = 0.25038495659828186 + 2.0 * 6.333333492279053
Epoch 680, val loss: 0.9434853196144104
Epoch 690, training loss: 12.90075969696045 = 0.23971807956695557 + 2.0 * 6.3305206298828125
Epoch 690, val loss: 0.9488697052001953
Epoch 700, training loss: 12.895562171936035 = 0.22948133945465088 + 2.0 * 6.333040237426758
Epoch 700, val loss: 0.9546328783035278
Epoch 710, training loss: 12.87528133392334 = 0.219720721244812 + 2.0 * 6.327780246734619
Epoch 710, val loss: 0.9607999324798584
Epoch 720, training loss: 12.862119674682617 = 0.21037046611309052 + 2.0 * 6.3258748054504395
Epoch 720, val loss: 0.967300534248352
Epoch 730, training loss: 12.859485626220703 = 0.2014121562242508 + 2.0 * 6.329036712646484
Epoch 730, val loss: 0.9740265011787415
Epoch 740, training loss: 12.845274925231934 = 0.19285111129283905 + 2.0 * 6.326211929321289
Epoch 740, val loss: 0.9811193346977234
Epoch 750, training loss: 12.829729080200195 = 0.1846989095211029 + 2.0 * 6.32251501083374
Epoch 750, val loss: 0.9882549047470093
Epoch 760, training loss: 12.817865371704102 = 0.1768943816423416 + 2.0 * 6.320485591888428
Epoch 760, val loss: 0.9956932067871094
Epoch 770, training loss: 12.82912540435791 = 0.1694219559431076 + 2.0 * 6.3298516273498535
Epoch 770, val loss: 1.0032662153244019
Epoch 780, training loss: 12.799389839172363 = 0.1623612940311432 + 2.0 * 6.318514347076416
Epoch 780, val loss: 1.0111796855926514
Epoch 790, training loss: 12.792071342468262 = 0.15561428666114807 + 2.0 * 6.318228721618652
Epoch 790, val loss: 1.0190788507461548
Epoch 800, training loss: 12.786698341369629 = 0.14915722608566284 + 2.0 * 6.318770408630371
Epoch 800, val loss: 1.027026891708374
Epoch 810, training loss: 12.774420738220215 = 0.14299814403057098 + 2.0 * 6.315711498260498
Epoch 810, val loss: 1.0352859497070312
Epoch 820, training loss: 12.76472282409668 = 0.1371184140443802 + 2.0 * 6.313802242279053
Epoch 820, val loss: 1.0435341596603394
Epoch 830, training loss: 12.76498794555664 = 0.13150295615196228 + 2.0 * 6.316742420196533
Epoch 830, val loss: 1.051795482635498
Epoch 840, training loss: 12.753730773925781 = 0.1261744350194931 + 2.0 * 6.313778400421143
Epoch 840, val loss: 1.0603686571121216
Epoch 850, training loss: 12.74173641204834 = 0.12108589708805084 + 2.0 * 6.3103251457214355
Epoch 850, val loss: 1.0689195394515991
Epoch 860, training loss: 12.734867095947266 = 0.11622320860624313 + 2.0 * 6.309321880340576
Epoch 860, val loss: 1.0776194334030151
Epoch 870, training loss: 12.738553047180176 = 0.11158437281847 + 2.0 * 6.313484191894531
Epoch 870, val loss: 1.0862555503845215
Epoch 880, training loss: 12.72413444519043 = 0.1071741059422493 + 2.0 * 6.308480262756348
Epoch 880, val loss: 1.0951135158538818
Epoch 890, training loss: 12.713951110839844 = 0.1029752641916275 + 2.0 * 6.305488109588623
Epoch 890, val loss: 1.1038856506347656
Epoch 900, training loss: 12.708224296569824 = 0.09896226227283478 + 2.0 * 6.304631233215332
Epoch 900, val loss: 1.1128509044647217
Epoch 910, training loss: 12.731606483459473 = 0.0951482430100441 + 2.0 * 6.3182291984558105
Epoch 910, val loss: 1.1218656301498413
Epoch 920, training loss: 12.7008638381958 = 0.09149174392223358 + 2.0 * 6.304686069488525
Epoch 920, val loss: 1.1306642293930054
Epoch 930, training loss: 12.694814682006836 = 0.0880405455827713 + 2.0 * 6.30338716506958
Epoch 930, val loss: 1.1395087242126465
Epoch 940, training loss: 12.686258316040039 = 0.08474055677652359 + 2.0 * 6.3007588386535645
Epoch 940, val loss: 1.1484074592590332
Epoch 950, training loss: 12.696224212646484 = 0.08158960938453674 + 2.0 * 6.30731725692749
Epoch 950, val loss: 1.1573925018310547
Epoch 960, training loss: 12.681902885437012 = 0.07858073711395264 + 2.0 * 6.301661014556885
Epoch 960, val loss: 1.1662744283676147
Epoch 970, training loss: 12.674038887023926 = 0.07571856677532196 + 2.0 * 6.299160003662109
Epoch 970, val loss: 1.1751288175582886
Epoch 980, training loss: 12.676153182983398 = 0.07298272103071213 + 2.0 * 6.3015851974487305
Epoch 980, val loss: 1.183938980102539
Epoch 990, training loss: 12.66642951965332 = 0.07037705183029175 + 2.0 * 6.298026084899902
Epoch 990, val loss: 1.192825198173523
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7407407407407408
0.8128624143384291
=== training gcn model ===
Epoch 0, training loss: 19.155261993408203 = 1.961574673652649 + 2.0 * 8.596843719482422
Epoch 0, val loss: 1.95668625831604
Epoch 10, training loss: 19.143651962280273 = 1.9505800008773804 + 2.0 * 8.596535682678223
Epoch 10, val loss: 1.9459248781204224
Epoch 20, training loss: 19.125324249267578 = 1.9363586902618408 + 2.0 * 8.594482421875
Epoch 20, val loss: 1.9315065145492554
Epoch 30, training loss: 19.077768325805664 = 1.9162476062774658 + 2.0 * 8.58076000213623
Epoch 30, val loss: 1.9109752178192139
Epoch 40, training loss: 18.897899627685547 = 1.8902748823165894 + 2.0 * 8.503812789916992
Epoch 40, val loss: 1.8858511447906494
Epoch 50, training loss: 18.284921646118164 = 1.8618322610855103 + 2.0 * 8.21154499053955
Epoch 50, val loss: 1.8597439527511597
Epoch 60, training loss: 17.628828048706055 = 1.8376799821853638 + 2.0 * 7.89557409286499
Epoch 60, val loss: 1.8389836549758911
Epoch 70, training loss: 16.703723907470703 = 1.8239927291870117 + 2.0 * 7.439865589141846
Epoch 70, val loss: 1.8275425434112549
Epoch 80, training loss: 16.030004501342773 = 1.8147996664047241 + 2.0 * 7.107602119445801
Epoch 80, val loss: 1.8193608522415161
Epoch 90, training loss: 15.69635009765625 = 1.8038285970687866 + 2.0 * 6.946260929107666
Epoch 90, val loss: 1.809931755065918
Epoch 100, training loss: 15.465388298034668 = 1.7919330596923828 + 2.0 * 6.836727619171143
Epoch 100, val loss: 1.8000116348266602
Epoch 110, training loss: 15.312286376953125 = 1.7810554504394531 + 2.0 * 6.765615463256836
Epoch 110, val loss: 1.7901227474212646
Epoch 120, training loss: 15.197659492492676 = 1.77052903175354 + 2.0 * 6.713565349578857
Epoch 120, val loss: 1.7801313400268555
Epoch 130, training loss: 15.108004570007324 = 1.759531855583191 + 2.0 * 6.674236297607422
Epoch 130, val loss: 1.7699412107467651
Epoch 140, training loss: 15.03644847869873 = 1.7475963830947876 + 2.0 * 6.644425868988037
Epoch 140, val loss: 1.7594236135482788
Epoch 150, training loss: 14.972078323364258 = 1.7345483303070068 + 2.0 * 6.618764877319336
Epoch 150, val loss: 1.748201608657837
Epoch 160, training loss: 14.91425895690918 = 1.7200837135314941 + 2.0 * 6.597087860107422
Epoch 160, val loss: 1.735849142074585
Epoch 170, training loss: 14.867815017700195 = 1.7038509845733643 + 2.0 * 6.581982135772705
Epoch 170, val loss: 1.7221119403839111
Epoch 180, training loss: 14.811847686767578 = 1.6857465505599976 + 2.0 * 6.563050746917725
Epoch 180, val loss: 1.7067809104919434
Epoch 190, training loss: 14.760832786560059 = 1.6654046773910522 + 2.0 * 6.5477142333984375
Epoch 190, val loss: 1.6896638870239258
Epoch 200, training loss: 14.711396217346191 = 1.6424455642700195 + 2.0 * 6.534475326538086
Epoch 200, val loss: 1.6703928709030151
Epoch 210, training loss: 14.662818908691406 = 1.6166800260543823 + 2.0 * 6.523069381713867
Epoch 210, val loss: 1.6488524675369263
Epoch 220, training loss: 14.609879493713379 = 1.5880790948867798 + 2.0 * 6.510900020599365
Epoch 220, val loss: 1.6249053478240967
Epoch 230, training loss: 14.5621919631958 = 1.5564082860946655 + 2.0 * 6.502892017364502
Epoch 230, val loss: 1.5983985662460327
Epoch 240, training loss: 14.504912376403809 = 1.5217812061309814 + 2.0 * 6.491565704345703
Epoch 240, val loss: 1.56960928440094
Epoch 250, training loss: 14.449841499328613 = 1.4842941761016846 + 2.0 * 6.482773780822754
Epoch 250, val loss: 1.5385546684265137
Epoch 260, training loss: 14.39707088470459 = 1.444078803062439 + 2.0 * 6.47649621963501
Epoch 260, val loss: 1.5052871704101562
Epoch 270, training loss: 14.33915901184082 = 1.4018094539642334 + 2.0 * 6.468674659729004
Epoch 270, val loss: 1.4706453084945679
Epoch 280, training loss: 14.279257774353027 = 1.358071208000183 + 2.0 * 6.460593223571777
Epoch 280, val loss: 1.4350007772445679
Epoch 290, training loss: 14.225799560546875 = 1.3132542371749878 + 2.0 * 6.456272602081299
Epoch 290, val loss: 1.3987352848052979
Epoch 300, training loss: 14.172789573669434 = 1.2680186033248901 + 2.0 * 6.452385425567627
Epoch 300, val loss: 1.3626682758331299
Epoch 310, training loss: 14.112682342529297 = 1.2240188121795654 + 2.0 * 6.444331645965576
Epoch 310, val loss: 1.3277467489242554
Epoch 320, training loss: 14.055397033691406 = 1.1811288595199585 + 2.0 * 6.437134265899658
Epoch 320, val loss: 1.294233798980713
Epoch 330, training loss: 14.00131607055664 = 1.1392722129821777 + 2.0 * 6.431021690368652
Epoch 330, val loss: 1.2619352340698242
Epoch 340, training loss: 13.950233459472656 = 1.0985560417175293 + 2.0 * 6.425838947296143
Epoch 340, val loss: 1.2309976816177368
Epoch 350, training loss: 13.904973983764648 = 1.0593152046203613 + 2.0 * 6.422829627990723
Epoch 350, val loss: 1.2018508911132812
Epoch 360, training loss: 13.859920501708984 = 1.0220879316329956 + 2.0 * 6.41891622543335
Epoch 360, val loss: 1.1746973991394043
Epoch 370, training loss: 13.81070613861084 = 0.9866088032722473 + 2.0 * 6.412048816680908
Epoch 370, val loss: 1.1492822170257568
Epoch 380, training loss: 13.768234252929688 = 0.9524750709533691 + 2.0 * 6.407879829406738
Epoch 380, val loss: 1.1254019737243652
Epoch 390, training loss: 13.731344223022461 = 0.9195504784584045 + 2.0 * 6.4058966636657715
Epoch 390, val loss: 1.1028891801834106
Epoch 400, training loss: 13.687326431274414 = 0.887962818145752 + 2.0 * 6.39968204498291
Epoch 400, val loss: 1.0818750858306885
Epoch 410, training loss: 13.648209571838379 = 0.857524573802948 + 2.0 * 6.3953423500061035
Epoch 410, val loss: 1.0620628595352173
Epoch 420, training loss: 13.610480308532715 = 0.827963650226593 + 2.0 * 6.391258239746094
Epoch 420, val loss: 1.043272852897644
Epoch 430, training loss: 13.609973907470703 = 0.7992159724235535 + 2.0 * 6.405378818511963
Epoch 430, val loss: 1.0255796909332275
Epoch 440, training loss: 13.552457809448242 = 0.7716912627220154 + 2.0 * 6.390383243560791
Epoch 440, val loss: 1.0089646577835083
Epoch 450, training loss: 13.50855827331543 = 0.7451203465461731 + 2.0 * 6.38171911239624
Epoch 450, val loss: 0.993675947189331
Epoch 460, training loss: 13.475540161132812 = 0.7193002700805664 + 2.0 * 6.378119945526123
Epoch 460, val loss: 0.9793157577514648
Epoch 470, training loss: 13.446468353271484 = 0.6941144466400146 + 2.0 * 6.376176834106445
Epoch 470, val loss: 0.9658169150352478
Epoch 480, training loss: 13.419581413269043 = 0.6697237491607666 + 2.0 * 6.374928951263428
Epoch 480, val loss: 0.9532253742218018
Epoch 490, training loss: 13.385481834411621 = 0.6461250185966492 + 2.0 * 6.369678497314453
Epoch 490, val loss: 0.9417494535446167
Epoch 500, training loss: 13.359894752502441 = 0.6232712864875793 + 2.0 * 6.368311882019043
Epoch 500, val loss: 0.931188702583313
Epoch 510, training loss: 13.340839385986328 = 0.6011987924575806 + 2.0 * 6.3698201179504395
Epoch 510, val loss: 0.9214111566543579
Epoch 520, training loss: 13.308154106140137 = 0.579994797706604 + 2.0 * 6.364079475402832
Epoch 520, val loss: 0.9131235480308533
Epoch 530, training loss: 13.279967308044434 = 0.5595384240150452 + 2.0 * 6.3602142333984375
Epoch 530, val loss: 0.9057981371879578
Epoch 540, training loss: 13.254941940307617 = 0.5397118330001831 + 2.0 * 6.357614994049072
Epoch 540, val loss: 0.8993269801139832
Epoch 550, training loss: 13.232498168945312 = 0.5204658508300781 + 2.0 * 6.356016159057617
Epoch 550, val loss: 0.8938466310501099
Epoch 560, training loss: 13.216733932495117 = 0.5018726587295532 + 2.0 * 6.357430458068848
Epoch 560, val loss: 0.8892355561256409
Epoch 570, training loss: 13.190119743347168 = 0.48398059606552124 + 2.0 * 6.35306978225708
Epoch 570, val loss: 0.8856061697006226
Epoch 580, training loss: 13.16457748413086 = 0.4667465090751648 + 2.0 * 6.3489155769348145
Epoch 580, val loss: 0.8831061720848083
Epoch 590, training loss: 13.148934364318848 = 0.45007532835006714 + 2.0 * 6.349429607391357
Epoch 590, val loss: 0.8813362717628479
Epoch 600, training loss: 13.130138397216797 = 0.43399232625961304 + 2.0 * 6.3480730056762695
Epoch 600, val loss: 0.8801243305206299
Epoch 610, training loss: 13.109972953796387 = 0.4185221493244171 + 2.0 * 6.3457255363464355
Epoch 610, val loss: 0.8801188468933105
Epoch 620, training loss: 13.088852882385254 = 0.4036061465740204 + 2.0 * 6.342623233795166
Epoch 620, val loss: 0.8806918263435364
Epoch 630, training loss: 13.071561813354492 = 0.38919126987457275 + 2.0 * 6.341185092926025
Epoch 630, val loss: 0.8818684816360474
Epoch 640, training loss: 13.053372383117676 = 0.3752415180206299 + 2.0 * 6.3390655517578125
Epoch 640, val loss: 0.8836826086044312
Epoch 650, training loss: 13.064776420593262 = 0.36176323890686035 + 2.0 * 6.35150671005249
Epoch 650, val loss: 0.885739266872406
Epoch 660, training loss: 13.020502090454102 = 0.3488522469997406 + 2.0 * 6.335824966430664
Epoch 660, val loss: 0.8887847661972046
Epoch 670, training loss: 13.00368881225586 = 0.33641213178634644 + 2.0 * 6.3336381912231445
Epoch 670, val loss: 0.8924049735069275
Epoch 680, training loss: 12.987090110778809 = 0.3243553042411804 + 2.0 * 6.331367492675781
Epoch 680, val loss: 0.896462082862854
Epoch 690, training loss: 12.97887897491455 = 0.3126501739025116 + 2.0 * 6.3331146240234375
Epoch 690, val loss: 0.9007554054260254
Epoch 700, training loss: 12.9708251953125 = 0.3013259172439575 + 2.0 * 6.334749698638916
Epoch 700, val loss: 0.9053612947463989
Epoch 710, training loss: 12.94533920288086 = 0.29041939973831177 + 2.0 * 6.327459812164307
Epoch 710, val loss: 0.910811185836792
Epoch 720, training loss: 12.932658195495605 = 0.27985209226608276 + 2.0 * 6.3264031410217285
Epoch 720, val loss: 0.9163726568222046
Epoch 730, training loss: 12.920120239257812 = 0.26963359117507935 + 2.0 * 6.3252434730529785
Epoch 730, val loss: 0.9220843315124512
Epoch 740, training loss: 12.911544799804688 = 0.25975537300109863 + 2.0 * 6.325894832611084
Epoch 740, val loss: 0.9284350275993347
Epoch 750, training loss: 12.898433685302734 = 0.25020962953567505 + 2.0 * 6.3241119384765625
Epoch 750, val loss: 0.9350199103355408
Epoch 760, training loss: 12.882622718811035 = 0.24095292389392853 + 2.0 * 6.320835113525391
Epoch 760, val loss: 0.9418036937713623
Epoch 770, training loss: 12.881912231445312 = 0.2319689393043518 + 2.0 * 6.324971675872803
Epoch 770, val loss: 0.9489839673042297
Epoch 780, training loss: 12.864776611328125 = 0.22327566146850586 + 2.0 * 6.320750713348389
Epoch 780, val loss: 0.9558767676353455
Epoch 790, training loss: 12.852354049682617 = 0.21486937999725342 + 2.0 * 6.318742275238037
Epoch 790, val loss: 0.963406503200531
Epoch 800, training loss: 12.840437889099121 = 0.20672476291656494 + 2.0 * 6.316856384277344
Epoch 800, val loss: 0.9710398316383362
Epoch 810, training loss: 12.841218948364258 = 0.19883295893669128 + 2.0 * 6.321193218231201
Epoch 810, val loss: 0.9785935878753662
Epoch 820, training loss: 12.821271896362305 = 0.19123981893062592 + 2.0 * 6.315016269683838
Epoch 820, val loss: 0.9868572950363159
Epoch 830, training loss: 12.811814308166504 = 0.1838931292295456 + 2.0 * 6.313960552215576
Epoch 830, val loss: 0.9950726628303528
Epoch 840, training loss: 12.803705215454102 = 0.1768018752336502 + 2.0 * 6.313451766967773
Epoch 840, val loss: 1.003490686416626
Epoch 850, training loss: 12.809368133544922 = 0.16994944214820862 + 2.0 * 6.319709300994873
Epoch 850, val loss: 1.0115197896957397
Epoch 860, training loss: 12.79123306274414 = 0.1634049266576767 + 2.0 * 6.3139142990112305
Epoch 860, val loss: 1.0206509828567505
Epoch 870, training loss: 12.775221824645996 = 0.15709610283374786 + 2.0 * 6.309062957763672
Epoch 870, val loss: 1.0293159484863281
Epoch 880, training loss: 12.766684532165527 = 0.1510075330734253 + 2.0 * 6.307838439941406
Epoch 880, val loss: 1.0381501913070679
Epoch 890, training loss: 12.759042739868164 = 0.14512993395328522 + 2.0 * 6.3069562911987305
Epoch 890, val loss: 1.0472873449325562
Epoch 900, training loss: 12.765668869018555 = 0.1394636183977127 + 2.0 * 6.313102722167969
Epoch 900, val loss: 1.056493878364563
Epoch 910, training loss: 12.74935245513916 = 0.13401708006858826 + 2.0 * 6.3076677322387695
Epoch 910, val loss: 1.0656085014343262
Epoch 920, training loss: 12.759912490844727 = 0.12880536913871765 + 2.0 * 6.315553665161133
Epoch 920, val loss: 1.0750502347946167
Epoch 930, training loss: 12.734641075134277 = 0.12379968911409378 + 2.0 * 6.305420875549316
Epoch 930, val loss: 1.0841251611709595
Epoch 940, training loss: 12.725566864013672 = 0.11901690810918808 + 2.0 * 6.303275108337402
Epoch 940, val loss: 1.0937743186950684
Epoch 950, training loss: 12.718092918395996 = 0.11441601812839508 + 2.0 * 6.301838397979736
Epoch 950, val loss: 1.1032029390335083
Epoch 960, training loss: 12.72230052947998 = 0.11000075191259384 + 2.0 * 6.306149959564209
Epoch 960, val loss: 1.1127824783325195
Epoch 970, training loss: 12.722062110900879 = 0.10577128082513809 + 2.0 * 6.308145523071289
Epoch 970, val loss: 1.1218830347061157
Epoch 980, training loss: 12.700309753417969 = 0.10176268965005875 + 2.0 * 6.299273490905762
Epoch 980, val loss: 1.1317588090896606
Epoch 990, training loss: 12.69640827178955 = 0.09792397916316986 + 2.0 * 6.29924201965332
Epoch 990, val loss: 1.1412835121154785
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7185185185185186
0.8191881918819188
=== training gcn model ===
Epoch 0, training loss: 19.137441635131836 = 1.9438241720199585 + 2.0 * 8.596808433532715
Epoch 0, val loss: 1.9468767642974854
Epoch 10, training loss: 19.12653160095215 = 1.933817982673645 + 2.0 * 8.596356391906738
Epoch 10, val loss: 1.9361870288848877
Epoch 20, training loss: 19.107257843017578 = 1.9212818145751953 + 2.0 * 8.592988014221191
Epoch 20, val loss: 1.9225952625274658
Epoch 30, training loss: 19.042936325073242 = 1.904111385345459 + 2.0 * 8.569412231445312
Epoch 30, val loss: 1.9038888216018677
Epoch 40, training loss: 18.727914810180664 = 1.8842471837997437 + 2.0 * 8.421833992004395
Epoch 40, val loss: 1.8834863901138306
Epoch 50, training loss: 17.940689086914062 = 1.8642672300338745 + 2.0 * 8.03821086883545
Epoch 50, val loss: 1.864021897315979
Epoch 60, training loss: 16.898311614990234 = 1.8499517440795898 + 2.0 * 7.5241804122924805
Epoch 60, val loss: 1.851732850074768
Epoch 70, training loss: 15.972246170043945 = 1.841537356376648 + 2.0 * 7.065354347229004
Epoch 70, val loss: 1.8447765111923218
Epoch 80, training loss: 15.645458221435547 = 1.8329806327819824 + 2.0 * 6.906238555908203
Epoch 80, val loss: 1.8363884687423706
Epoch 90, training loss: 15.42685604095459 = 1.8210680484771729 + 2.0 * 6.802894115447998
Epoch 90, val loss: 1.8248194456100464
Epoch 100, training loss: 15.282344818115234 = 1.8089704513549805 + 2.0 * 6.736687183380127
Epoch 100, val loss: 1.8132693767547607
Epoch 110, training loss: 15.171339988708496 = 1.7981146574020386 + 2.0 * 6.686612606048584
Epoch 110, val loss: 1.8031845092773438
Epoch 120, training loss: 15.08281421661377 = 1.788019061088562 + 2.0 * 6.647397518157959
Epoch 120, val loss: 1.7937099933624268
Epoch 130, training loss: 15.003005027770996 = 1.7778189182281494 + 2.0 * 6.612593173980713
Epoch 130, val loss: 1.7840993404388428
Epoch 140, training loss: 14.934981346130371 = 1.7671109437942505 + 2.0 * 6.583935260772705
Epoch 140, val loss: 1.7741609811782837
Epoch 150, training loss: 14.876564979553223 = 1.7555301189422607 + 2.0 * 6.560517311096191
Epoch 150, val loss: 1.7636754512786865
Epoch 160, training loss: 14.82105827331543 = 1.7427221536636353 + 2.0 * 6.539167881011963
Epoch 160, val loss: 1.7523988485336304
Epoch 170, training loss: 14.776955604553223 = 1.728387475013733 + 2.0 * 6.5242838859558105
Epoch 170, val loss: 1.7399704456329346
Epoch 180, training loss: 14.732881546020508 = 1.7122163772583008 + 2.0 * 6.5103325843811035
Epoch 180, val loss: 1.726195216178894
Epoch 190, training loss: 14.681893348693848 = 1.6941945552825928 + 2.0 * 6.493849277496338
Epoch 190, val loss: 1.7109699249267578
Epoch 200, training loss: 14.636800765991211 = 1.6738818883895874 + 2.0 * 6.481459617614746
Epoch 200, val loss: 1.6940162181854248
Epoch 210, training loss: 14.592100143432617 = 1.6509413719177246 + 2.0 * 6.470579147338867
Epoch 210, val loss: 1.6749699115753174
Epoch 220, training loss: 14.553996086120605 = 1.6252080202102661 + 2.0 * 6.4643940925598145
Epoch 220, val loss: 1.6536574363708496
Epoch 230, training loss: 14.503911972045898 = 1.5966237783432007 + 2.0 * 6.453644275665283
Epoch 230, val loss: 1.6302460432052612
Epoch 240, training loss: 14.453057289123535 = 1.5652254819869995 + 2.0 * 6.443915843963623
Epoch 240, val loss: 1.6043012142181396
Epoch 250, training loss: 14.412260055541992 = 1.530860185623169 + 2.0 * 6.440700054168701
Epoch 250, val loss: 1.5761237144470215
Epoch 260, training loss: 14.353246688842773 = 1.4938578605651855 + 2.0 * 6.429694652557373
Epoch 260, val loss: 1.5458952188491821
Epoch 270, training loss: 14.305424690246582 = 1.4544031620025635 + 2.0 * 6.425510883331299
Epoch 270, val loss: 1.5136276483535767
Epoch 280, training loss: 14.249366760253906 = 1.4129934310913086 + 2.0 * 6.418186664581299
Epoch 280, val loss: 1.4801416397094727
Epoch 290, training loss: 14.19470500946045 = 1.370306134223938 + 2.0 * 6.4121994972229
Epoch 290, val loss: 1.4457404613494873
Epoch 300, training loss: 14.147107124328613 = 1.326692819595337 + 2.0 * 6.410207271575928
Epoch 300, val loss: 1.4108049869537354
Epoch 310, training loss: 14.086360931396484 = 1.2829248905181885 + 2.0 * 6.4017181396484375
Epoch 310, val loss: 1.3758260011672974
Epoch 320, training loss: 14.03284740447998 = 1.2394334077835083 + 2.0 * 6.396707057952881
Epoch 320, val loss: 1.341089129447937
Epoch 330, training loss: 13.983358383178711 = 1.196298360824585 + 2.0 * 6.393529891967773
Epoch 330, val loss: 1.3068000078201294
Epoch 340, training loss: 13.9369535446167 = 1.1538145542144775 + 2.0 * 6.3915696144104
Epoch 340, val loss: 1.273285150527954
Epoch 350, training loss: 13.882375717163086 = 1.1125035285949707 + 2.0 * 6.384936332702637
Epoch 350, val loss: 1.240540623664856
Epoch 360, training loss: 13.833414077758789 = 1.0720713138580322 + 2.0 * 6.380671501159668
Epoch 360, val loss: 1.2087821960449219
Epoch 370, training loss: 13.78582763671875 = 1.0326135158538818 + 2.0 * 6.3766069412231445
Epoch 370, val loss: 1.1778837442398071
Epoch 380, training loss: 13.774421691894531 = 0.9942853450775146 + 2.0 * 6.390068054199219
Epoch 380, val loss: 1.147937536239624
Epoch 390, training loss: 13.701444625854492 = 0.9576027989387512 + 2.0 * 6.371921062469482
Epoch 390, val loss: 1.1195979118347168
Epoch 400, training loss: 13.659530639648438 = 0.9225348234176636 + 2.0 * 6.368497848510742
Epoch 400, val loss: 1.0926991701126099
Epoch 410, training loss: 13.61750602722168 = 0.8889005184173584 + 2.0 * 6.364302635192871
Epoch 410, val loss: 1.0671190023422241
Epoch 420, training loss: 13.585165977478027 = 0.8565236926078796 + 2.0 * 6.364321231842041
Epoch 420, val loss: 1.0428715944290161
Epoch 430, training loss: 13.549854278564453 = 0.8254472017288208 + 2.0 * 6.362203598022461
Epoch 430, val loss: 1.0201176404953003
Epoch 440, training loss: 13.508223533630371 = 0.795762836933136 + 2.0 * 6.35623025894165
Epoch 440, val loss: 0.9987627863883972
Epoch 450, training loss: 13.474920272827148 = 0.7672293186187744 + 2.0 * 6.353845596313477
Epoch 450, val loss: 0.9787065386772156
Epoch 460, training loss: 13.472256660461426 = 0.7398091554641724 + 2.0 * 6.3662238121032715
Epoch 460, val loss: 0.9597821235656738
Epoch 470, training loss: 13.420085906982422 = 0.7135483622550964 + 2.0 * 6.353268623352051
Epoch 470, val loss: 0.9423611760139465
Epoch 480, training loss: 13.383149147033691 = 0.68831467628479 + 2.0 * 6.34741735458374
Epoch 480, val loss: 0.9261714220046997
Epoch 490, training loss: 13.352874755859375 = 0.6639151573181152 + 2.0 * 6.344479560852051
Epoch 490, val loss: 0.9110292196273804
Epoch 500, training loss: 13.325949668884277 = 0.6402454972267151 + 2.0 * 6.3428521156311035
Epoch 500, val loss: 0.8968032598495483
Epoch 510, training loss: 13.303216934204102 = 0.6173083186149597 + 2.0 * 6.342954158782959
Epoch 510, val loss: 0.8834613561630249
Epoch 520, training loss: 13.276949882507324 = 0.5950896143913269 + 2.0 * 6.340929985046387
Epoch 520, val loss: 0.8711915612220764
Epoch 530, training loss: 13.248648643493652 = 0.5735600590705872 + 2.0 * 6.3375444412231445
Epoch 530, val loss: 0.8597241640090942
Epoch 540, training loss: 13.232213020324707 = 0.5525384545326233 + 2.0 * 6.339837074279785
Epoch 540, val loss: 0.8490526676177979
Epoch 550, training loss: 13.204078674316406 = 0.5320191383361816 + 2.0 * 6.336029529571533
Epoch 550, val loss: 0.8392143249511719
Epoch 560, training loss: 13.177566528320312 = 0.5120065808296204 + 2.0 * 6.332779884338379
Epoch 560, val loss: 0.8300002813339233
Epoch 570, training loss: 13.158358573913574 = 0.4923819899559021 + 2.0 * 6.332988262176514
Epoch 570, val loss: 0.8214158415794373
Epoch 580, training loss: 13.135171890258789 = 0.4731135070323944 + 2.0 * 6.331029415130615
Epoch 580, val loss: 0.8134596347808838
Epoch 590, training loss: 13.112100601196289 = 0.45434489846229553 + 2.0 * 6.328877925872803
Epoch 590, val loss: 0.8061511516571045
Epoch 600, training loss: 13.089653015136719 = 0.4359419643878937 + 2.0 * 6.326855659484863
Epoch 600, val loss: 0.79964679479599
Epoch 610, training loss: 13.06739330291748 = 0.4179393947124481 + 2.0 * 6.3247270584106445
Epoch 610, val loss: 0.7937434315681458
Epoch 620, training loss: 13.046296119689941 = 0.4002929925918579 + 2.0 * 6.323001384735107
Epoch 620, val loss: 0.7883941531181335
Epoch 630, training loss: 13.041385650634766 = 0.38294336199760437 + 2.0 * 6.329221248626709
Epoch 630, val loss: 0.7836387157440186
Epoch 640, training loss: 13.016121864318848 = 0.36603987216949463 + 2.0 * 6.325040817260742
Epoch 640, val loss: 0.7795368432998657
Epoch 650, training loss: 12.991230964660645 = 0.34962257742881775 + 2.0 * 6.320804119110107
Epoch 650, val loss: 0.7761574387550354
Epoch 660, training loss: 12.96967887878418 = 0.3336831331253052 + 2.0 * 6.317997932434082
Epoch 660, val loss: 0.7734370827674866
Epoch 670, training loss: 12.966005325317383 = 0.31819722056388855 + 2.0 * 6.323904037475586
Epoch 670, val loss: 0.7713285088539124
Epoch 680, training loss: 12.941338539123535 = 0.3032597303390503 + 2.0 * 6.319039344787598
Epoch 680, val loss: 0.7695679664611816
Epoch 690, training loss: 12.916971206665039 = 0.2888180613517761 + 2.0 * 6.3140764236450195
Epoch 690, val loss: 0.7686411738395691
Epoch 700, training loss: 12.901461601257324 = 0.2749050259590149 + 2.0 * 6.3132781982421875
Epoch 700, val loss: 0.7682918310165405
Epoch 710, training loss: 12.90307331085205 = 0.2615453898906708 + 2.0 * 6.320764064788818
Epoch 710, val loss: 0.768155574798584
Epoch 720, training loss: 12.878613471984863 = 0.24881424009799957 + 2.0 * 6.314899444580078
Epoch 720, val loss: 0.7689168453216553
Epoch 730, training loss: 12.858192443847656 = 0.2366749346256256 + 2.0 * 6.310758590698242
Epoch 730, val loss: 0.7699581384658813
Epoch 740, training loss: 12.8426513671875 = 0.22508040070533752 + 2.0 * 6.308785438537598
Epoch 740, val loss: 0.7715955972671509
Epoch 750, training loss: 12.829065322875977 = 0.21398721635341644 + 2.0 * 6.307538986206055
Epoch 750, val loss: 0.7736903429031372
Epoch 760, training loss: 12.838065147399902 = 0.2033746838569641 + 2.0 * 6.317345142364502
Epoch 760, val loss: 0.7762377858161926
Epoch 770, training loss: 12.806806564331055 = 0.19337071478366852 + 2.0 * 6.306717872619629
Epoch 770, val loss: 0.7789959907531738
Epoch 780, training loss: 12.794922828674316 = 0.1838696300983429 + 2.0 * 6.3055267333984375
Epoch 780, val loss: 0.782234787940979
Epoch 790, training loss: 12.79089641571045 = 0.17486006021499634 + 2.0 * 6.308018207550049
Epoch 790, val loss: 0.7858729362487793
Epoch 800, training loss: 12.774300575256348 = 0.16635408997535706 + 2.0 * 6.303973197937012
Epoch 800, val loss: 0.7899919152259827
Epoch 810, training loss: 12.762321472167969 = 0.15827856957912445 + 2.0 * 6.302021503448486
Epoch 810, val loss: 0.7942814826965332
Epoch 820, training loss: 12.760005950927734 = 0.1506563276052475 + 2.0 * 6.3046746253967285
Epoch 820, val loss: 0.7989814877510071
Epoch 830, training loss: 12.748761177062988 = 0.1434607058763504 + 2.0 * 6.302650451660156
Epoch 830, val loss: 0.8037136793136597
Epoch 840, training loss: 12.736838340759277 = 0.13665370643138885 + 2.0 * 6.3000922203063965
Epoch 840, val loss: 0.8089318871498108
Epoch 850, training loss: 12.727761268615723 = 0.13022205233573914 + 2.0 * 6.298769474029541
Epoch 850, val loss: 0.8142062425613403
Epoch 860, training loss: 12.728163719177246 = 0.124149851500988 + 2.0 * 6.302006721496582
Epoch 860, val loss: 0.8196763396263123
Epoch 870, training loss: 12.71575927734375 = 0.11845135688781738 + 2.0 * 6.298654079437256
Epoch 870, val loss: 0.8252251148223877
Epoch 880, training loss: 12.706282615661621 = 0.1130637526512146 + 2.0 * 6.296609401702881
Epoch 880, val loss: 0.8308754563331604
Epoch 890, training loss: 12.714165687561035 = 0.10798761993646622 + 2.0 * 6.303089141845703
Epoch 890, val loss: 0.8367178440093994
Epoch 900, training loss: 12.69664478302002 = 0.10320988297462463 + 2.0 * 6.296717643737793
Epoch 900, val loss: 0.8424984216690063
Epoch 910, training loss: 12.686131477355957 = 0.09870240092277527 + 2.0 * 6.29371452331543
Epoch 910, val loss: 0.8485097885131836
Epoch 920, training loss: 12.678292274475098 = 0.09444884210824966 + 2.0 * 6.291921615600586
Epoch 920, val loss: 0.8544937968254089
Epoch 930, training loss: 12.674181938171387 = 0.09041280299425125 + 2.0 * 6.291884422302246
Epoch 930, val loss: 0.8606049418449402
Epoch 940, training loss: 12.682229042053223 = 0.08660964667797089 + 2.0 * 6.297809600830078
Epoch 940, val loss: 0.8666102886199951
Epoch 950, training loss: 12.66184139251709 = 0.08300555497407913 + 2.0 * 6.289417743682861
Epoch 950, val loss: 0.8726743459701538
Epoch 960, training loss: 12.658570289611816 = 0.07961161434650421 + 2.0 * 6.2894792556762695
Epoch 960, val loss: 0.8787672519683838
Epoch 970, training loss: 12.65456771850586 = 0.07640327513217926 + 2.0 * 6.289082050323486
Epoch 970, val loss: 0.8849984407424927
Epoch 980, training loss: 12.658268928527832 = 0.0733610987663269 + 2.0 * 6.292453765869141
Epoch 980, val loss: 0.8910407423973083
Epoch 990, training loss: 12.648905754089355 = 0.07048271596431732 + 2.0 * 6.289211750030518
Epoch 990, val loss: 0.8972690105438232
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.762962962962963
0.8149710068529257
The final CL Acc:0.74074, 0.01814, The final GNN Acc:0.81567, 0.00263
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13192])
remove edge: torch.Size([2, 8000])
updated graph: torch.Size([2, 10636])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 19.138051986694336 = 1.9443806409835815 + 2.0 * 8.59683609008789
Epoch 0, val loss: 1.9415136575698853
Epoch 10, training loss: 19.127622604370117 = 1.9345629215240479 + 2.0 * 8.596529960632324
Epoch 10, val loss: 1.9323185682296753
Epoch 20, training loss: 19.110149383544922 = 1.9224573373794556 + 2.0 * 8.593846321105957
Epoch 20, val loss: 1.920714020729065
Epoch 30, training loss: 19.049724578857422 = 1.9059133529663086 + 2.0 * 8.571906089782715
Epoch 30, val loss: 1.9048597812652588
Epoch 40, training loss: 18.761810302734375 = 1.8851687908172607 + 2.0 * 8.438321113586426
Epoch 40, val loss: 1.885828971862793
Epoch 50, training loss: 18.146587371826172 = 1.8632745742797852 + 2.0 * 8.141655921936035
Epoch 50, val loss: 1.865800380706787
Epoch 60, training loss: 17.539384841918945 = 1.8429396152496338 + 2.0 * 7.848222255706787
Epoch 60, val loss: 1.8471808433532715
Epoch 70, training loss: 16.597707748413086 = 1.8290656805038452 + 2.0 * 7.384321212768555
Epoch 70, val loss: 1.8345491886138916
Epoch 80, training loss: 15.882294654846191 = 1.8201606273651123 + 2.0 * 7.03106689453125
Epoch 80, val loss: 1.826280117034912
Epoch 90, training loss: 15.575518608093262 = 1.8057047128677368 + 2.0 * 6.884906768798828
Epoch 90, val loss: 1.8127938508987427
Epoch 100, training loss: 15.377843856811523 = 1.788663625717163 + 2.0 * 6.794589996337891
Epoch 100, val loss: 1.7972795963287354
Epoch 110, training loss: 15.225852966308594 = 1.7727421522140503 + 2.0 * 6.726555347442627
Epoch 110, val loss: 1.7825566530227661
Epoch 120, training loss: 15.116731643676758 = 1.757215976715088 + 2.0 * 6.679758071899414
Epoch 120, val loss: 1.7680108547210693
Epoch 130, training loss: 15.02283000946045 = 1.7401179075241089 + 2.0 * 6.641355991363525
Epoch 130, val loss: 1.7522516250610352
Epoch 140, training loss: 14.944911003112793 = 1.7211153507232666 + 2.0 * 6.611897945404053
Epoch 140, val loss: 1.7349127531051636
Epoch 150, training loss: 14.873902320861816 = 1.7002894878387451 + 2.0 * 6.586806297302246
Epoch 150, val loss: 1.716080904006958
Epoch 160, training loss: 14.814733505249023 = 1.6770527362823486 + 2.0 * 6.568840503692627
Epoch 160, val loss: 1.695294976234436
Epoch 170, training loss: 14.751459121704102 = 1.650905728340149 + 2.0 * 6.550276756286621
Epoch 170, val loss: 1.6721975803375244
Epoch 180, training loss: 14.692713737487793 = 1.6217149496078491 + 2.0 * 6.535499572753906
Epoch 180, val loss: 1.646538257598877
Epoch 190, training loss: 14.633026123046875 = 1.5892863273620605 + 2.0 * 6.521870136260986
Epoch 190, val loss: 1.6181784868240356
Epoch 200, training loss: 14.572364807128906 = 1.553762674331665 + 2.0 * 6.50930118560791
Epoch 200, val loss: 1.5872982740402222
Epoch 210, training loss: 14.508830070495605 = 1.5154087543487549 + 2.0 * 6.496710777282715
Epoch 210, val loss: 1.5540934801101685
Epoch 220, training loss: 14.454771041870117 = 1.474413514137268 + 2.0 * 6.49017858505249
Epoch 220, val loss: 1.518905520439148
Epoch 230, training loss: 14.384987831115723 = 1.4317189455032349 + 2.0 * 6.476634502410889
Epoch 230, val loss: 1.4825528860092163
Epoch 240, training loss: 14.319128036499023 = 1.3878363370895386 + 2.0 * 6.465645790100098
Epoch 240, val loss: 1.4455130100250244
Epoch 250, training loss: 14.26016616821289 = 1.343412160873413 + 2.0 * 6.458376884460449
Epoch 250, val loss: 1.4083516597747803
Epoch 260, training loss: 14.20105266571045 = 1.299390196800232 + 2.0 * 6.450831413269043
Epoch 260, val loss: 1.3717598915100098
Epoch 270, training loss: 14.140243530273438 = 1.2563672065734863 + 2.0 * 6.441938400268555
Epoch 270, val loss: 1.3363113403320312
Epoch 280, training loss: 14.083647727966309 = 1.2145942449569702 + 2.0 * 6.4345269203186035
Epoch 280, val loss: 1.3019992113113403
Epoch 290, training loss: 14.033041000366211 = 1.174218773841858 + 2.0 * 6.429410934448242
Epoch 290, val loss: 1.2692675590515137
Epoch 300, training loss: 13.978886604309082 = 1.1354739665985107 + 2.0 * 6.421706199645996
Epoch 300, val loss: 1.2380234003067017
Epoch 310, training loss: 13.93282413482666 = 1.098462462425232 + 2.0 * 6.417181015014648
Epoch 310, val loss: 1.2083730697631836
Epoch 320, training loss: 13.883699417114258 = 1.0632776021957397 + 2.0 * 6.410211086273193
Epoch 320, val loss: 1.1803677082061768
Epoch 330, training loss: 13.836697578430176 = 1.0296963453292847 + 2.0 * 6.403500556945801
Epoch 330, val loss: 1.1538995504379272
Epoch 340, training loss: 13.803585052490234 = 0.997585654258728 + 2.0 * 6.4029998779296875
Epoch 340, val loss: 1.1287779808044434
Epoch 350, training loss: 13.75818157196045 = 0.9669989347457886 + 2.0 * 6.3955912590026855
Epoch 350, val loss: 1.1050227880477905
Epoch 360, training loss: 13.733786582946777 = 0.9377332925796509 + 2.0 * 6.398026466369629
Epoch 360, val loss: 1.0825620889663696
Epoch 370, training loss: 13.68307876586914 = 0.9098175168037415 + 2.0 * 6.386630535125732
Epoch 370, val loss: 1.0613272190093994
Epoch 380, training loss: 13.646390914916992 = 0.883010745048523 + 2.0 * 6.38169002532959
Epoch 380, val loss: 1.0412005186080933
Epoch 390, training loss: 13.612100601196289 = 0.8571408987045288 + 2.0 * 6.3774800300598145
Epoch 390, val loss: 1.022027850151062
Epoch 400, training loss: 13.596450805664062 = 0.8321506381034851 + 2.0 * 6.382150173187256
Epoch 400, val loss: 1.0038195848464966
Epoch 410, training loss: 13.553177833557129 = 0.8082069158554077 + 2.0 * 6.372485637664795
Epoch 410, val loss: 0.9866573810577393
Epoch 420, training loss: 13.520284652709961 = 0.7851279377937317 + 2.0 * 6.367578506469727
Epoch 420, val loss: 0.9705218076705933
Epoch 430, training loss: 13.505743026733398 = 0.7627974152565002 + 2.0 * 6.3714728355407715
Epoch 430, val loss: 0.9552420377731323
Epoch 440, training loss: 13.467159271240234 = 0.7412316203117371 + 2.0 * 6.362963676452637
Epoch 440, val loss: 0.9408538341522217
Epoch 450, training loss: 13.438972473144531 = 0.7203141450881958 + 2.0 * 6.3593292236328125
Epoch 450, val loss: 0.927304744720459
Epoch 460, training loss: 13.413374900817871 = 0.6999398469924927 + 2.0 * 6.356717586517334
Epoch 460, val loss: 0.9145389795303345
Epoch 470, training loss: 13.388920783996582 = 0.6801275014877319 + 2.0 * 6.354396820068359
Epoch 470, val loss: 0.9024648666381836
Epoch 480, training loss: 13.363128662109375 = 0.6608254313468933 + 2.0 * 6.351151466369629
Epoch 480, val loss: 0.8911146521568298
Epoch 490, training loss: 13.34221076965332 = 0.6419493556022644 + 2.0 * 6.350130558013916
Epoch 490, val loss: 0.8803515434265137
Epoch 500, training loss: 13.31782054901123 = 0.6234000325202942 + 2.0 * 6.34721040725708
Epoch 500, val loss: 0.8700545430183411
Epoch 510, training loss: 13.296121597290039 = 0.6051342487335205 + 2.0 * 6.345493793487549
Epoch 510, val loss: 0.8602356314659119
Epoch 520, training loss: 13.273052215576172 = 0.5870605111122131 + 2.0 * 6.342995643615723
Epoch 520, val loss: 0.8509036898612976
Epoch 530, training loss: 13.255334854125977 = 0.5691404342651367 + 2.0 * 6.34309720993042
Epoch 530, val loss: 0.8419525623321533
Epoch 540, training loss: 13.232126235961914 = 0.5513975620269775 + 2.0 * 6.340364456176758
Epoch 540, val loss: 0.8333193063735962
Epoch 550, training loss: 13.205906867980957 = 0.5337666869163513 + 2.0 * 6.3360700607299805
Epoch 550, val loss: 0.8250720500946045
Epoch 560, training loss: 13.187333106994629 = 0.5161678791046143 + 2.0 * 6.335582733154297
Epoch 560, val loss: 0.8171117305755615
Epoch 570, training loss: 13.165143013000488 = 0.4986037611961365 + 2.0 * 6.3332695960998535
Epoch 570, val loss: 0.8093844652175903
Epoch 580, training loss: 13.145905494689941 = 0.4811309278011322 + 2.0 * 6.332387447357178
Epoch 580, val loss: 0.8019837737083435
Epoch 590, training loss: 13.126541137695312 = 0.4637306034564972 + 2.0 * 6.331405162811279
Epoch 590, val loss: 0.7948487997055054
Epoch 600, training loss: 13.102843284606934 = 0.4463682174682617 + 2.0 * 6.328237533569336
Epoch 600, val loss: 0.7880728840827942
Epoch 610, training loss: 13.081046104431152 = 0.42904287576675415 + 2.0 * 6.3260016441345215
Epoch 610, val loss: 0.7816171646118164
Epoch 620, training loss: 13.068150520324707 = 0.4117966890335083 + 2.0 * 6.328176975250244
Epoch 620, val loss: 0.7755458950996399
Epoch 630, training loss: 13.05280876159668 = 0.3946833610534668 + 2.0 * 6.329062461853027
Epoch 630, val loss: 0.7698841094970703
Epoch 640, training loss: 13.021201133728027 = 0.3778294026851654 + 2.0 * 6.321685791015625
Epoch 640, val loss: 0.7647572159767151
Epoch 650, training loss: 13.006536483764648 = 0.3612499237060547 + 2.0 * 6.322643280029297
Epoch 650, val loss: 0.7601608633995056
Epoch 660, training loss: 12.983777046203613 = 0.34498706459999084 + 2.0 * 6.319395065307617
Epoch 660, val loss: 0.7561611533164978
Epoch 670, training loss: 12.966146469116211 = 0.3291338086128235 + 2.0 * 6.318506240844727
Epoch 670, val loss: 0.7527590990066528
Epoch 680, training loss: 12.946281433105469 = 0.3137040138244629 + 2.0 * 6.316288471221924
Epoch 680, val loss: 0.7500419616699219
Epoch 690, training loss: 12.927820205688477 = 0.29871875047683716 + 2.0 * 6.314550876617432
Epoch 690, val loss: 0.7479375004768372
Epoch 700, training loss: 12.941848754882812 = 0.28421616554260254 + 2.0 * 6.3288164138793945
Epoch 700, val loss: 0.7464200854301453
Epoch 710, training loss: 12.905807495117188 = 0.27028125524520874 + 2.0 * 6.317763328552246
Epoch 710, val loss: 0.7453927397727966
Epoch 720, training loss: 12.882226943969727 = 0.25695088505744934 + 2.0 * 6.312637805938721
Epoch 720, val loss: 0.7449501156806946
Epoch 730, training loss: 12.864029884338379 = 0.2441880702972412 + 2.0 * 6.309920787811279
Epoch 730, val loss: 0.7450517416000366
Epoch 740, training loss: 12.848087310791016 = 0.23196642100811005 + 2.0 * 6.308060646057129
Epoch 740, val loss: 0.7456272840499878
Epoch 750, training loss: 12.863227844238281 = 0.22029449045658112 + 2.0 * 6.321466445922852
Epoch 750, val loss: 0.7466396689414978
Epoch 760, training loss: 12.82790756225586 = 0.2091119885444641 + 2.0 * 6.3093976974487305
Epoch 760, val loss: 0.7479739785194397
Epoch 770, training loss: 12.811317443847656 = 0.1985515058040619 + 2.0 * 6.30638313293457
Epoch 770, val loss: 0.7496862411499023
Epoch 780, training loss: 12.797314643859863 = 0.18854153156280518 + 2.0 * 6.304386615753174
Epoch 780, val loss: 0.7517513036727905
Epoch 790, training loss: 12.786591529846191 = 0.17902560532093048 + 2.0 * 6.303782939910889
Epoch 790, val loss: 0.7541770339012146
Epoch 800, training loss: 12.774386405944824 = 0.16998684406280518 + 2.0 * 6.302199840545654
Epoch 800, val loss: 0.7567873597145081
Epoch 810, training loss: 12.767356872558594 = 0.1614236831665039 + 2.0 * 6.302966594696045
Epoch 810, val loss: 0.7596064805984497
Epoch 820, training loss: 12.756280899047852 = 0.1533326953649521 + 2.0 * 6.301474094390869
Epoch 820, val loss: 0.7625126242637634
Epoch 830, training loss: 12.742867469787598 = 0.14571666717529297 + 2.0 * 6.298575401306152
Epoch 830, val loss: 0.7657597661018372
Epoch 840, training loss: 12.733244895935059 = 0.13855163753032684 + 2.0 * 6.297346591949463
Epoch 840, val loss: 0.7691296339035034
Epoch 850, training loss: 12.726716995239258 = 0.13178762793540955 + 2.0 * 6.297464847564697
Epoch 850, val loss: 0.7726981043815613
Epoch 860, training loss: 12.719832420349121 = 0.1254073530435562 + 2.0 * 6.297212600708008
Epoch 860, val loss: 0.776309609413147
Epoch 870, training loss: 12.711661338806152 = 0.11941692978143692 + 2.0 * 6.296122074127197
Epoch 870, val loss: 0.7800551056861877
Epoch 880, training loss: 12.711641311645508 = 0.11378661543130875 + 2.0 * 6.298927307128906
Epoch 880, val loss: 0.7837968468666077
Epoch 890, training loss: 12.693471908569336 = 0.10849697887897491 + 2.0 * 6.292487621307373
Epoch 890, val loss: 0.7877352237701416
Epoch 900, training loss: 12.686788558959961 = 0.10354074835777283 + 2.0 * 6.291624069213867
Epoch 900, val loss: 0.7917557954788208
Epoch 910, training loss: 12.679784774780273 = 0.0988587811589241 + 2.0 * 6.290462970733643
Epoch 910, val loss: 0.795867383480072
Epoch 920, training loss: 12.690491676330566 = 0.09444471448659897 + 2.0 * 6.298023700714111
Epoch 920, val loss: 0.8000235557556152
Epoch 930, training loss: 12.674080848693848 = 0.09026748687028885 + 2.0 * 6.291906833648682
Epoch 930, val loss: 0.8041591644287109
Epoch 940, training loss: 12.66335678100586 = 0.08634018152952194 + 2.0 * 6.288508415222168
Epoch 940, val loss: 0.8084155321121216
Epoch 950, training loss: 12.659893989562988 = 0.08263177424669266 + 2.0 * 6.288630962371826
Epoch 950, val loss: 0.8126832246780396
Epoch 960, training loss: 12.65736198425293 = 0.07911545038223267 + 2.0 * 6.289123058319092
Epoch 960, val loss: 0.8169587254524231
Epoch 970, training loss: 12.648947715759277 = 0.07579163461923599 + 2.0 * 6.286578178405762
Epoch 970, val loss: 0.8212389349937439
Epoch 980, training loss: 12.63986587524414 = 0.07266067713499069 + 2.0 * 6.283602714538574
Epoch 980, val loss: 0.8255962133407593
Epoch 990, training loss: 12.637425422668457 = 0.06970657408237457 + 2.0 * 6.2838592529296875
Epoch 990, val loss: 0.8300008773803711
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7814814814814816
0.838165524512388
=== training gcn model ===
Epoch 0, training loss: 19.16142463684082 = 1.9677860736846924 + 2.0 * 8.596818923950195
Epoch 0, val loss: 1.97127366065979
Epoch 10, training loss: 19.149517059326172 = 1.956569790840149 + 2.0 * 8.596473693847656
Epoch 10, val loss: 1.9601165056228638
Epoch 20, training loss: 19.130146026611328 = 1.9423004388809204 + 2.0 * 8.59392261505127
Epoch 20, val loss: 1.945348858833313
Epoch 30, training loss: 19.073909759521484 = 1.9218792915344238 + 2.0 * 8.57601547241211
Epoch 30, val loss: 1.9240117073059082
Epoch 40, training loss: 18.832965850830078 = 1.8963496685028076 + 2.0 * 8.468308448791504
Epoch 40, val loss: 1.898833155632019
Epoch 50, training loss: 18.09957504272461 = 1.8690848350524902 + 2.0 * 8.11524486541748
Epoch 50, val loss: 1.8728892803192139
Epoch 60, training loss: 17.373144149780273 = 1.845214605331421 + 2.0 * 7.763964653015137
Epoch 60, val loss: 1.8508638143539429
Epoch 70, training loss: 16.585979461669922 = 1.8298139572143555 + 2.0 * 7.378082275390625
Epoch 70, val loss: 1.837321400642395
Epoch 80, training loss: 15.995387077331543 = 1.8184958696365356 + 2.0 * 7.088445663452148
Epoch 80, val loss: 1.8273472785949707
Epoch 90, training loss: 15.649917602539062 = 1.8042062520980835 + 2.0 * 6.922855854034424
Epoch 90, val loss: 1.814621090888977
Epoch 100, training loss: 15.453214645385742 = 1.7880229949951172 + 2.0 * 6.8325958251953125
Epoch 100, val loss: 1.7995727062225342
Epoch 110, training loss: 15.291032791137695 = 1.7727197408676147 + 2.0 * 6.759156703948975
Epoch 110, val loss: 1.7851274013519287
Epoch 120, training loss: 15.164270401000977 = 1.7589707374572754 + 2.0 * 6.7026495933532715
Epoch 120, val loss: 1.7720212936401367
Epoch 130, training loss: 15.06102466583252 = 1.7450684309005737 + 2.0 * 6.657978057861328
Epoch 130, val loss: 1.7590683698654175
Epoch 140, training loss: 14.979801177978516 = 1.730067253112793 + 2.0 * 6.624866962432861
Epoch 140, val loss: 1.745504379272461
Epoch 150, training loss: 14.912286758422852 = 1.713697075843811 + 2.0 * 6.599294662475586
Epoch 150, val loss: 1.7310786247253418
Epoch 160, training loss: 14.846648216247559 = 1.6957423686981201 + 2.0 * 6.57545280456543
Epoch 160, val loss: 1.7155474424362183
Epoch 170, training loss: 14.788211822509766 = 1.675889253616333 + 2.0 * 6.556161403656006
Epoch 170, val loss: 1.698629379272461
Epoch 180, training loss: 14.727548599243164 = 1.653948187828064 + 2.0 * 6.536800384521484
Epoch 180, val loss: 1.6800727844238281
Epoch 190, training loss: 14.667122840881348 = 1.6298149824142456 + 2.0 * 6.518653869628906
Epoch 190, val loss: 1.659722924232483
Epoch 200, training loss: 14.609180450439453 = 1.603286623954773 + 2.0 * 6.502946853637695
Epoch 200, val loss: 1.6373941898345947
Epoch 210, training loss: 14.555635452270508 = 1.5742696523666382 + 2.0 * 6.490683078765869
Epoch 210, val loss: 1.6132075786590576
Epoch 220, training loss: 14.496789932250977 = 1.5431904792785645 + 2.0 * 6.476799488067627
Epoch 220, val loss: 1.5874404907226562
Epoch 230, training loss: 14.440093040466309 = 1.5103453397750854 + 2.0 * 6.464873790740967
Epoch 230, val loss: 1.560695767402649
Epoch 240, training loss: 14.383211135864258 = 1.4761358499526978 + 2.0 * 6.453537464141846
Epoch 240, val loss: 1.533134937286377
Epoch 250, training loss: 14.327593803405762 = 1.4407135248184204 + 2.0 * 6.443439960479736
Epoch 250, val loss: 1.5050095319747925
Epoch 260, training loss: 14.275025367736816 = 1.4045382738113403 + 2.0 * 6.435243606567383
Epoch 260, val loss: 1.47682523727417
Epoch 270, training loss: 14.221076011657715 = 1.3683987855911255 + 2.0 * 6.4263386726379395
Epoch 270, val loss: 1.449411392211914
Epoch 280, training loss: 14.169501304626465 = 1.3323928117752075 + 2.0 * 6.418554306030273
Epoch 280, val loss: 1.4225431680679321
Epoch 290, training loss: 14.137968063354492 = 1.2965511083602905 + 2.0 * 6.420708656311035
Epoch 290, val loss: 1.3963596820831299
Epoch 300, training loss: 14.074718475341797 = 1.2613763809204102 + 2.0 * 6.406671047210693
Epoch 300, val loss: 1.3709124326705933
Epoch 310, training loss: 14.027596473693848 = 1.226760983467102 + 2.0 * 6.400417804718018
Epoch 310, val loss: 1.3462764024734497
Epoch 320, training loss: 13.982961654663086 = 1.1925369501113892 + 2.0 * 6.395212173461914
Epoch 320, val loss: 1.322191596031189
Epoch 330, training loss: 13.939032554626465 = 1.1589136123657227 + 2.0 * 6.390059471130371
Epoch 330, val loss: 1.2986265420913696
Epoch 340, training loss: 13.899384498596191 = 1.1259124279022217 + 2.0 * 6.386735916137695
Epoch 340, val loss: 1.2758101224899292
Epoch 350, training loss: 13.85572338104248 = 1.0935784578323364 + 2.0 * 6.381072521209717
Epoch 350, val loss: 1.2535268068313599
Epoch 360, training loss: 13.818296432495117 = 1.0618430376052856 + 2.0 * 6.3782267570495605
Epoch 360, val loss: 1.231813669204712
Epoch 370, training loss: 13.777432441711426 = 1.0307728052139282 + 2.0 * 6.3733296394348145
Epoch 370, val loss: 1.2107951641082764
Epoch 380, training loss: 13.74856948852539 = 1.000262975692749 + 2.0 * 6.374153137207031
Epoch 380, val loss: 1.1903711557388306
Epoch 390, training loss: 13.703497886657715 = 0.9707373380661011 + 2.0 * 6.366380214691162
Epoch 390, val loss: 1.17075514793396
Epoch 400, training loss: 13.666252136230469 = 0.9418084621429443 + 2.0 * 6.362221717834473
Epoch 400, val loss: 1.1517537832260132
Epoch 410, training loss: 13.631312370300293 = 0.9133731722831726 + 2.0 * 6.358969688415527
Epoch 410, val loss: 1.1331968307495117
Epoch 420, training loss: 13.604711532592773 = 0.8853818774223328 + 2.0 * 6.3596649169921875
Epoch 420, val loss: 1.115069031715393
Epoch 430, training loss: 13.566973686218262 = 0.8579322695732117 + 2.0 * 6.354520797729492
Epoch 430, val loss: 1.0974829196929932
Epoch 440, training loss: 13.53172492980957 = 0.8309260606765747 + 2.0 * 6.350399494171143
Epoch 440, val loss: 1.0804141759872437
Epoch 450, training loss: 13.501262664794922 = 0.8042410016059875 + 2.0 * 6.3485107421875
Epoch 450, val loss: 1.0637248754501343
Epoch 460, training loss: 13.471015930175781 = 0.7779016494750977 + 2.0 * 6.346557140350342
Epoch 460, val loss: 1.047577977180481
Epoch 470, training loss: 13.441307067871094 = 0.7519989609718323 + 2.0 * 6.344654083251953
Epoch 470, val loss: 1.0319836139678955
Epoch 480, training loss: 13.408271789550781 = 0.7264524698257446 + 2.0 * 6.340909481048584
Epoch 480, val loss: 1.0169230699539185
Epoch 490, training loss: 13.38657283782959 = 0.7011945247650146 + 2.0 * 6.342689037322998
Epoch 490, val loss: 1.0024278163909912
Epoch 500, training loss: 13.355096817016602 = 0.6763463020324707 + 2.0 * 6.3393754959106445
Epoch 500, val loss: 0.9885619878768921
Epoch 510, training loss: 13.322349548339844 = 0.6517884731292725 + 2.0 * 6.335280418395996
Epoch 510, val loss: 0.9753279089927673
Epoch 520, training loss: 13.294495582580566 = 0.6275278925895691 + 2.0 * 6.333483695983887
Epoch 520, val loss: 0.962636411190033
Epoch 530, training loss: 13.266302108764648 = 0.6035264134407043 + 2.0 * 6.331387996673584
Epoch 530, val loss: 0.9504786729812622
Epoch 540, training loss: 13.241674423217773 = 0.5797125101089478 + 2.0 * 6.3309807777404785
Epoch 540, val loss: 0.9388402104377747
Epoch 550, training loss: 13.218764305114746 = 0.5560866594314575 + 2.0 * 6.331338882446289
Epoch 550, val loss: 0.9279245138168335
Epoch 560, training loss: 13.186065673828125 = 0.5327639579772949 + 2.0 * 6.326650619506836
Epoch 560, val loss: 0.9175387620925903
Epoch 570, training loss: 13.157379150390625 = 0.5096213817596436 + 2.0 * 6.323878765106201
Epoch 570, val loss: 0.9076981544494629
Epoch 580, training loss: 13.146706581115723 = 0.4867513179779053 + 2.0 * 6.329977512359619
Epoch 580, val loss: 0.8985730409622192
Epoch 590, training loss: 13.109538078308105 = 0.46427449584007263 + 2.0 * 6.3226318359375
Epoch 590, val loss: 0.8901557326316833
Epoch 600, training loss: 13.087101936340332 = 0.44238466024398804 + 2.0 * 6.32235860824585
Epoch 600, val loss: 0.8827903270721436
Epoch 610, training loss: 13.058675765991211 = 0.4211176633834839 + 2.0 * 6.318778991699219
Epoch 610, val loss: 0.8762747049331665
Epoch 620, training loss: 13.03332233428955 = 0.4004899859428406 + 2.0 * 6.316416263580322
Epoch 620, val loss: 0.8706673979759216
Epoch 630, training loss: 13.01539421081543 = 0.38056680560112 + 2.0 * 6.317413806915283
Epoch 630, val loss: 0.8658770322799683
Epoch 640, training loss: 12.994599342346191 = 0.36148008704185486 + 2.0 * 6.316559791564941
Epoch 640, val loss: 0.8620250821113586
Epoch 650, training loss: 12.974544525146484 = 0.3432091772556305 + 2.0 * 6.315667629241943
Epoch 650, val loss: 0.8589577078819275
Epoch 660, training loss: 12.949115753173828 = 0.3257519006729126 + 2.0 * 6.311681747436523
Epoch 660, val loss: 0.8565244078636169
Epoch 670, training loss: 12.927582740783691 = 0.30907395482063293 + 2.0 * 6.309254169464111
Epoch 670, val loss: 0.8548884987831116
Epoch 680, training loss: 12.91193675994873 = 0.2931051254272461 + 2.0 * 6.309415817260742
Epoch 680, val loss: 0.8538227081298828
Epoch 690, training loss: 12.893550872802734 = 0.277833491563797 + 2.0 * 6.307858467102051
Epoch 690, val loss: 0.8532865643501282
Epoch 700, training loss: 12.880345344543457 = 0.2632765471935272 + 2.0 * 6.308534622192383
Epoch 700, val loss: 0.8532831072807312
Epoch 710, training loss: 12.863590240478516 = 0.24937056005001068 + 2.0 * 6.307109832763672
Epoch 710, val loss: 0.8537732362747192
Epoch 720, training loss: 12.842737197875977 = 0.23610027134418488 + 2.0 * 6.303318500518799
Epoch 720, val loss: 0.8547435998916626
Epoch 730, training loss: 12.835006713867188 = 0.2234409749507904 + 2.0 * 6.305782794952393
Epoch 730, val loss: 0.8561177849769592
Epoch 740, training loss: 12.814085960388184 = 0.211450457572937 + 2.0 * 6.3013176918029785
Epoch 740, val loss: 0.8581067323684692
Epoch 750, training loss: 12.803147315979004 = 0.20005638897418976 + 2.0 * 6.3015456199646
Epoch 750, val loss: 0.8604139089584351
Epoch 760, training loss: 12.791276931762695 = 0.1892905980348587 + 2.0 * 6.300992965698242
Epoch 760, val loss: 0.8631924390792847
Epoch 770, training loss: 12.776168823242188 = 0.17909418046474457 + 2.0 * 6.298537254333496
Epoch 770, val loss: 0.8662636280059814
Epoch 780, training loss: 12.764341354370117 = 0.16949626803398132 + 2.0 * 6.297422409057617
Epoch 780, val loss: 0.8698535561561584
Epoch 790, training loss: 12.762843132019043 = 0.16044102609157562 + 2.0 * 6.301200866699219
Epoch 790, val loss: 0.8736972808837891
Epoch 800, training loss: 12.747065544128418 = 0.1519354283809662 + 2.0 * 6.29756498336792
Epoch 800, val loss: 0.8780351281166077
Epoch 810, training loss: 12.746764183044434 = 0.14398476481437683 + 2.0 * 6.301389694213867
Epoch 810, val loss: 0.8825861215591431
Epoch 820, training loss: 12.725696563720703 = 0.1365133374929428 + 2.0 * 6.294591426849365
Epoch 820, val loss: 0.8873769044876099
Epoch 830, training loss: 12.718427658081055 = 0.12953093647956848 + 2.0 * 6.294448375701904
Epoch 830, val loss: 0.8926104307174683
Epoch 840, training loss: 12.706684112548828 = 0.12297162413597107 + 2.0 * 6.291856288909912
Epoch 840, val loss: 0.897943377494812
Epoch 850, training loss: 12.705245018005371 = 0.11681884527206421 + 2.0 * 6.29421329498291
Epoch 850, val loss: 0.903567910194397
Epoch 860, training loss: 12.69385051727295 = 0.11107548326253891 + 2.0 * 6.291387557983398
Epoch 860, val loss: 0.9094884991645813
Epoch 870, training loss: 12.689156532287598 = 0.10567283630371094 + 2.0 * 6.291741847991943
Epoch 870, val loss: 0.9154913425445557
Epoch 880, training loss: 12.678024291992188 = 0.10062126070261002 + 2.0 * 6.28870153427124
Epoch 880, val loss: 0.9217161536216736
Epoch 890, training loss: 12.67513370513916 = 0.09587081521749496 + 2.0 * 6.289631366729736
Epoch 890, val loss: 0.9280725717544556
Epoch 900, training loss: 12.666624069213867 = 0.09142250567674637 + 2.0 * 6.287600994110107
Epoch 900, val loss: 0.9345709681510925
Epoch 910, training loss: 12.660934448242188 = 0.08723577857017517 + 2.0 * 6.286849498748779
Epoch 910, val loss: 0.9411176443099976
Epoch 920, training loss: 12.652352333068848 = 0.08331647515296936 + 2.0 * 6.284517765045166
Epoch 920, val loss: 0.9478474855422974
Epoch 930, training loss: 12.649042129516602 = 0.0796375498175621 + 2.0 * 6.284702301025391
Epoch 930, val loss: 0.9546732306480408
Epoch 940, training loss: 12.644219398498535 = 0.07616610080003738 + 2.0 * 6.284026622772217
Epoch 940, val loss: 0.9614406824111938
Epoch 950, training loss: 12.64074993133545 = 0.07289908081293106 + 2.0 * 6.283925533294678
Epoch 950, val loss: 0.968273937702179
Epoch 960, training loss: 12.636091232299805 = 0.06982552260160446 + 2.0 * 6.283133029937744
Epoch 960, val loss: 0.9752013087272644
Epoch 970, training loss: 12.641088485717773 = 0.06692744791507721 + 2.0 * 6.28708028793335
Epoch 970, val loss: 0.9820733070373535
Epoch 980, training loss: 12.627252578735352 = 0.06420446187257767 + 2.0 * 6.281524181365967
Epoch 980, val loss: 0.9889965057373047
Epoch 990, training loss: 12.619156837463379 = 0.06163966283202171 + 2.0 * 6.278758525848389
Epoch 990, val loss: 0.9959449172019958
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7962962962962963
0.8344754876120191
=== training gcn model ===
Epoch 0, training loss: 19.136249542236328 = 1.9425923824310303 + 2.0 * 8.59682846069336
Epoch 0, val loss: 1.9360800981521606
Epoch 10, training loss: 19.124773025512695 = 1.9317988157272339 + 2.0 * 8.596487045288086
Epoch 10, val loss: 1.925947666168213
Epoch 20, training loss: 19.106664657592773 = 1.9183870553970337 + 2.0 * 8.594139099121094
Epoch 20, val loss: 1.9129964113235474
Epoch 30, training loss: 19.052705764770508 = 1.9001307487487793 + 2.0 * 8.576287269592285
Epoch 30, val loss: 1.8951044082641602
Epoch 40, training loss: 18.803632736206055 = 1.8774158954620361 + 2.0 * 8.46310806274414
Epoch 40, val loss: 1.8733997344970703
Epoch 50, training loss: 17.907941818237305 = 1.852500557899475 + 2.0 * 8.02772045135498
Epoch 50, val loss: 1.84998619556427
Epoch 60, training loss: 17.027353286743164 = 1.8320807218551636 + 2.0 * 7.597636699676514
Epoch 60, val loss: 1.831526517868042
Epoch 70, training loss: 16.239242553710938 = 1.8191797733306885 + 2.0 * 7.210031509399414
Epoch 70, val loss: 1.818968653678894
Epoch 80, training loss: 15.790964126586914 = 1.8064310550689697 + 2.0 * 6.992266654968262
Epoch 80, val loss: 1.8064409494400024
Epoch 90, training loss: 15.534651756286621 = 1.7907274961471558 + 2.0 * 6.871962070465088
Epoch 90, val loss: 1.791884422302246
Epoch 100, training loss: 15.344813346862793 = 1.7735015153884888 + 2.0 * 6.785655975341797
Epoch 100, val loss: 1.7767598628997803
Epoch 110, training loss: 15.212166786193848 = 1.7569719552993774 + 2.0 * 6.727597236633301
Epoch 110, val loss: 1.7617734670639038
Epoch 120, training loss: 15.100296020507812 = 1.7404669523239136 + 2.0 * 6.679914474487305
Epoch 120, val loss: 1.7462763786315918
Epoch 130, training loss: 15.008612632751465 = 1.7227070331573486 + 2.0 * 6.642952919006348
Epoch 130, val loss: 1.7297568321228027
Epoch 140, training loss: 14.923707962036133 = 1.7030316591262817 + 2.0 * 6.61033821105957
Epoch 140, val loss: 1.7117847204208374
Epoch 150, training loss: 14.851120948791504 = 1.6811214685440063 + 2.0 * 6.5849995613098145
Epoch 150, val loss: 1.6920167207717896
Epoch 160, training loss: 14.782266616821289 = 1.6566095352172852 + 2.0 * 6.562828540802002
Epoch 160, val loss: 1.6701619625091553
Epoch 170, training loss: 14.714082717895508 = 1.6293225288391113 + 2.0 * 6.542379856109619
Epoch 170, val loss: 1.6459013223648071
Epoch 180, training loss: 14.649554252624512 = 1.598832130432129 + 2.0 * 6.525361061096191
Epoch 180, val loss: 1.61911141872406
Epoch 190, training loss: 14.589539527893066 = 1.5650862455368042 + 2.0 * 6.512226581573486
Epoch 190, val loss: 1.589794397354126
Epoch 200, training loss: 14.523982048034668 = 1.5282233953475952 + 2.0 * 6.497879505157471
Epoch 200, val loss: 1.5580379962921143
Epoch 210, training loss: 14.463427543640137 = 1.4882519245147705 + 2.0 * 6.487587928771973
Epoch 210, val loss: 1.5239499807357788
Epoch 220, training loss: 14.400300979614258 = 1.4456669092178345 + 2.0 * 6.477316856384277
Epoch 220, val loss: 1.4881211519241333
Epoch 230, training loss: 14.333396911621094 = 1.4011225700378418 + 2.0 * 6.466136932373047
Epoch 230, val loss: 1.4510136842727661
Epoch 240, training loss: 14.267416000366211 = 1.354880452156067 + 2.0 * 6.456267833709717
Epoch 240, val loss: 1.4131027460098267
Epoch 250, training loss: 14.211268424987793 = 1.307350516319275 + 2.0 * 6.451959133148193
Epoch 250, val loss: 1.3747245073318481
Epoch 260, training loss: 14.144347190856934 = 1.2597383260726929 + 2.0 * 6.442304611206055
Epoch 260, val loss: 1.3367875814437866
Epoch 270, training loss: 14.083063125610352 = 1.2125067710876465 + 2.0 * 6.435278415679932
Epoch 270, val loss: 1.2994909286499023
Epoch 280, training loss: 14.021526336669922 = 1.1656967401504517 + 2.0 * 6.427914619445801
Epoch 280, val loss: 1.2627094984054565
Epoch 290, training loss: 13.963957786560059 = 1.1193628311157227 + 2.0 * 6.422297477722168
Epoch 290, val loss: 1.2265174388885498
Epoch 300, training loss: 13.910087585449219 = 1.0739775896072388 + 2.0 * 6.418055057525635
Epoch 300, val loss: 1.1910336017608643
Epoch 310, training loss: 13.854949951171875 = 1.0300389528274536 + 2.0 * 6.4124555587768555
Epoch 310, val loss: 1.1568617820739746
Epoch 320, training loss: 13.801107406616211 = 0.9876269102096558 + 2.0 * 6.406740188598633
Epoch 320, val loss: 1.1239135265350342
Epoch 330, training loss: 13.750539779663086 = 0.946631669998169 + 2.0 * 6.401954174041748
Epoch 330, val loss: 1.0921517610549927
Epoch 340, training loss: 13.713078498840332 = 0.9072364568710327 + 2.0 * 6.402921199798584
Epoch 340, val loss: 1.0618624687194824
Epoch 350, training loss: 13.65733528137207 = 0.8698506951332092 + 2.0 * 6.393742084503174
Epoch 350, val loss: 1.033308744430542
Epoch 360, training loss: 13.61397647857666 = 0.8344030380249023 + 2.0 * 6.389786720275879
Epoch 360, val loss: 1.00662362575531
Epoch 370, training loss: 13.589176177978516 = 0.8009565472602844 + 2.0 * 6.394109725952148
Epoch 370, val loss: 0.9817779660224915
Epoch 380, training loss: 13.537614822387695 = 0.7695159912109375 + 2.0 * 6.384049415588379
Epoch 380, val loss: 0.9590203166007996
Epoch 390, training loss: 13.496872901916504 = 0.7396953701972961 + 2.0 * 6.378588676452637
Epoch 390, val loss: 0.9379981756210327
Epoch 400, training loss: 13.464421272277832 = 0.7112601399421692 + 2.0 * 6.376580715179443
Epoch 400, val loss: 0.9184257388114929
Epoch 410, training loss: 13.439382553100586 = 0.6840918064117432 + 2.0 * 6.377645492553711
Epoch 410, val loss: 0.9004549980163574
Epoch 420, training loss: 13.394621849060059 = 0.6582160592079163 + 2.0 * 6.3682026863098145
Epoch 420, val loss: 0.8838332295417786
Epoch 430, training loss: 13.36300277709961 = 0.6332035064697266 + 2.0 * 6.364899635314941
Epoch 430, val loss: 0.8684361577033997
Epoch 440, training loss: 13.332669258117676 = 0.608856201171875 + 2.0 * 6.3619065284729
Epoch 440, val loss: 0.8541237711906433
Epoch 450, training loss: 13.318052291870117 = 0.5851449370384216 + 2.0 * 6.366453647613525
Epoch 450, val loss: 0.8408000469207764
Epoch 460, training loss: 13.279539108276367 = 0.5621508955955505 + 2.0 * 6.358694076538086
Epoch 460, val loss: 0.8285220265388489
Epoch 470, training loss: 13.248087882995605 = 0.5397819876670837 + 2.0 * 6.354153156280518
Epoch 470, val loss: 0.8172110319137573
Epoch 480, training loss: 13.230595588684082 = 0.5179822444915771 + 2.0 * 6.356306552886963
Epoch 480, val loss: 0.8067817091941833
Epoch 490, training loss: 13.20115852355957 = 0.496827632188797 + 2.0 * 6.352165222167969
Epoch 490, val loss: 0.7973184585571289
Epoch 500, training loss: 13.171178817749023 = 0.4763311743736267 + 2.0 * 6.347424030303955
Epoch 500, val loss: 0.7888407111167908
Epoch 510, training loss: 13.149703979492188 = 0.45650187134742737 + 2.0 * 6.3466010093688965
Epoch 510, val loss: 0.781229555606842
Epoch 520, training loss: 13.122392654418945 = 0.4372332990169525 + 2.0 * 6.3425798416137695
Epoch 520, val loss: 0.7744670510292053
Epoch 530, training loss: 13.098628044128418 = 0.41860008239746094 + 2.0 * 6.3400139808654785
Epoch 530, val loss: 0.7685012817382812
Epoch 540, training loss: 13.07850170135498 = 0.4005046486854553 + 2.0 * 6.338998317718506
Epoch 540, val loss: 0.763367235660553
Epoch 550, training loss: 13.05970573425293 = 0.38296958804130554 + 2.0 * 6.338367938995361
Epoch 550, val loss: 0.7590600848197937
Epoch 560, training loss: 13.041759490966797 = 0.366093248128891 + 2.0 * 6.337832927703857
Epoch 560, val loss: 0.7553997039794922
Epoch 570, training loss: 13.016804695129395 = 0.3497559428215027 + 2.0 * 6.333524227142334
Epoch 570, val loss: 0.7524353265762329
Epoch 580, training loss: 12.995880126953125 = 0.3338913917541504 + 2.0 * 6.330994129180908
Epoch 580, val loss: 0.7501275539398193
Epoch 590, training loss: 12.995705604553223 = 0.31853145360946655 + 2.0 * 6.338587284088135
Epoch 590, val loss: 0.7483617067337036
Epoch 600, training loss: 12.96329116821289 = 0.303692102432251 + 2.0 * 6.329799652099609
Epoch 600, val loss: 0.7469689249992371
Epoch 610, training loss: 12.941193580627441 = 0.28939831256866455 + 2.0 * 6.325897693634033
Epoch 610, val loss: 0.7460662722587585
Epoch 620, training loss: 12.924044609069824 = 0.27556708455085754 + 2.0 * 6.3242387771606445
Epoch 620, val loss: 0.745676577091217
Epoch 630, training loss: 12.930246353149414 = 0.26221004128456116 + 2.0 * 6.334018230438232
Epoch 630, val loss: 0.7457305192947388
Epoch 640, training loss: 12.892060279846191 = 0.24947400391101837 + 2.0 * 6.321293354034424
Epoch 640, val loss: 0.7460938096046448
Epoch 650, training loss: 12.877057075500488 = 0.23726904392242432 + 2.0 * 6.319893836975098
Epoch 650, val loss: 0.7468493580818176
Epoch 660, training loss: 12.86162281036377 = 0.22557640075683594 + 2.0 * 6.318023204803467
Epoch 660, val loss: 0.7480996251106262
Epoch 670, training loss: 12.854775428771973 = 0.2144043743610382 + 2.0 * 6.320185661315918
Epoch 670, val loss: 0.7497066259384155
Epoch 680, training loss: 12.84207534790039 = 0.20377443730831146 + 2.0 * 6.319150447845459
Epoch 680, val loss: 0.7515182495117188
Epoch 690, training loss: 12.830170631408691 = 0.1936710625886917 + 2.0 * 6.318249702453613
Epoch 690, val loss: 0.7536121606826782
Epoch 700, training loss: 12.812785148620605 = 0.18412716686725616 + 2.0 * 6.314329147338867
Epoch 700, val loss: 0.7560933232307434
Epoch 710, training loss: 12.798562049865723 = 0.17504166066646576 + 2.0 * 6.311760425567627
Epoch 710, val loss: 0.7588552832603455
Epoch 720, training loss: 12.796918869018555 = 0.1664322167634964 + 2.0 * 6.315243244171143
Epoch 720, val loss: 0.7619311213493347
Epoch 730, training loss: 12.783761978149414 = 0.15826410055160522 + 2.0 * 6.312748908996582
Epoch 730, val loss: 0.7652133703231812
Epoch 740, training loss: 12.765734672546387 = 0.15056727826595306 + 2.0 * 6.307583808898926
Epoch 740, val loss: 0.7686777710914612
Epoch 750, training loss: 12.755578994750977 = 0.14327067136764526 + 2.0 * 6.306154251098633
Epoch 750, val loss: 0.7724774479866028
Epoch 760, training loss: 12.764676094055176 = 0.13636384904384613 + 2.0 * 6.3141560554504395
Epoch 760, val loss: 0.7765316963195801
Epoch 770, training loss: 12.743860244750977 = 0.1298380047082901 + 2.0 * 6.307011127471924
Epoch 770, val loss: 0.7805309891700745
Epoch 780, training loss: 12.733785629272461 = 0.12368948012590408 + 2.0 * 6.305047988891602
Epoch 780, val loss: 0.7847253680229187
Epoch 790, training loss: 12.724214553833008 = 0.11788689345121384 + 2.0 * 6.303164005279541
Epoch 790, val loss: 0.7891907095909119
Epoch 800, training loss: 12.7173433303833 = 0.11239709705114365 + 2.0 * 6.302473068237305
Epoch 800, val loss: 0.7937583923339844
Epoch 810, training loss: 12.712616920471191 = 0.10721170157194138 + 2.0 * 6.3027024269104
Epoch 810, val loss: 0.7984287738800049
Epoch 820, training loss: 12.713237762451172 = 0.10233154892921448 + 2.0 * 6.305453300476074
Epoch 820, val loss: 0.8031723499298096
Epoch 830, training loss: 12.695585250854492 = 0.09771239757537842 + 2.0 * 6.298936367034912
Epoch 830, val loss: 0.8080028891563416
Epoch 840, training loss: 12.686064720153809 = 0.09335857629776001 + 2.0 * 6.296352863311768
Epoch 840, val loss: 0.8129621744155884
Epoch 850, training loss: 12.68306827545166 = 0.08923493325710297 + 2.0 * 6.296916484832764
Epoch 850, val loss: 0.8179817795753479
Epoch 860, training loss: 12.67768669128418 = 0.0853491947054863 + 2.0 * 6.296168804168701
Epoch 860, val loss: 0.8231825232505798
Epoch 870, training loss: 12.674246788024902 = 0.081686832010746 + 2.0 * 6.2962799072265625
Epoch 870, val loss: 0.8282318115234375
Epoch 880, training loss: 12.666028022766113 = 0.07824079692363739 + 2.0 * 6.293893814086914
Epoch 880, val loss: 0.8333576321601868
Epoch 890, training loss: 12.665225982666016 = 0.07497556507587433 + 2.0 * 6.2951250076293945
Epoch 890, val loss: 0.8386743068695068
Epoch 900, training loss: 12.657124519348145 = 0.07189059257507324 + 2.0 * 6.292616844177246
Epoch 900, val loss: 0.843845784664154
Epoch 910, training loss: 12.65827465057373 = 0.06897341459989548 + 2.0 * 6.294650554656982
Epoch 910, val loss: 0.849145233631134
Epoch 920, training loss: 12.645842552185059 = 0.06621769070625305 + 2.0 * 6.2898125648498535
Epoch 920, val loss: 0.8543881773948669
Epoch 930, training loss: 12.639288902282715 = 0.06360821425914764 + 2.0 * 6.287840366363525
Epoch 930, val loss: 0.8596410155296326
Epoch 940, training loss: 12.63823127746582 = 0.06113485246896744 + 2.0 * 6.288547992706299
Epoch 940, val loss: 0.8650022745132446
Epoch 950, training loss: 12.644105911254883 = 0.05878913030028343 + 2.0 * 6.29265832901001
Epoch 950, val loss: 0.8703359365463257
Epoch 960, training loss: 12.632131576538086 = 0.05657261237502098 + 2.0 * 6.287779331207275
Epoch 960, val loss: 0.8755400776863098
Epoch 970, training loss: 12.626096725463867 = 0.0544704832136631 + 2.0 * 6.285813331604004
Epoch 970, val loss: 0.8807164430618286
Epoch 980, training loss: 12.61944580078125 = 0.05247699096798897 + 2.0 * 6.28348445892334
Epoch 980, val loss: 0.8860086798667908
Epoch 990, training loss: 12.620043754577637 = 0.050578150898218155 + 2.0 * 6.284732818603516
Epoch 990, val loss: 0.8912957310676575
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8037037037037037
0.8397469688982605
The final CL Acc:0.79383, 0.00924, The final GNN Acc:0.83746, 0.00221
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.5 der2:0.3 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11628])
remove edge: torch.Size([2, 9550])
updated graph: torch.Size([2, 10622])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 19.16197395324707 = 1.9683551788330078 + 2.0 * 8.596809387207031
Epoch 0, val loss: 1.9737306833267212
Epoch 10, training loss: 19.149181365966797 = 1.9564745426177979 + 2.0 * 8.596353530883789
Epoch 10, val loss: 1.9613460302352905
Epoch 20, training loss: 19.128002166748047 = 1.9416327476501465 + 2.0 * 8.593184471130371
Epoch 20, val loss: 1.94570791721344
Epoch 30, training loss: 19.062843322753906 = 1.9214597940444946 + 2.0 * 8.57069206237793
Epoch 30, val loss: 1.9245375394821167
Epoch 40, training loss: 18.700284957885742 = 1.8981425762176514 + 2.0 * 8.401071548461914
Epoch 40, val loss: 1.900842308998108
Epoch 50, training loss: 17.428359985351562 = 1.874697208404541 + 2.0 * 7.77683162689209
Epoch 50, val loss: 1.8779752254486084
Epoch 60, training loss: 16.57478141784668 = 1.8601480722427368 + 2.0 * 7.357316493988037
Epoch 60, val loss: 1.8640743494033813
Epoch 70, training loss: 15.990443229675293 = 1.8486279249191284 + 2.0 * 7.0709075927734375
Epoch 70, val loss: 1.8521467447280884
Epoch 80, training loss: 15.650907516479492 = 1.8365931510925293 + 2.0 * 6.9071574211120605
Epoch 80, val loss: 1.8399021625518799
Epoch 90, training loss: 15.432223320007324 = 1.8240078687667847 + 2.0 * 6.804107666015625
Epoch 90, val loss: 1.8275007009506226
Epoch 100, training loss: 15.276171684265137 = 1.8123209476470947 + 2.0 * 6.7319254875183105
Epoch 100, val loss: 1.8160150051116943
Epoch 110, training loss: 15.156723022460938 = 1.8019146919250488 + 2.0 * 6.677404403686523
Epoch 110, val loss: 1.8055344820022583
Epoch 120, training loss: 15.062582969665527 = 1.7923825979232788 + 2.0 * 6.635100364685059
Epoch 120, val loss: 1.7957234382629395
Epoch 130, training loss: 14.98516845703125 = 1.7832441329956055 + 2.0 * 6.600962162017822
Epoch 130, val loss: 1.7863634824752808
Epoch 140, training loss: 14.921935081481934 = 1.7740987539291382 + 2.0 * 6.573918342590332
Epoch 140, val loss: 1.7772594690322876
Epoch 150, training loss: 14.86829662322998 = 1.7645504474639893 + 2.0 * 6.551873207092285
Epoch 150, val loss: 1.7681411504745483
Epoch 160, training loss: 14.820759773254395 = 1.7543452978134155 + 2.0 * 6.533207416534424
Epoch 160, val loss: 1.7587023973464966
Epoch 170, training loss: 14.774941444396973 = 1.7431989908218384 + 2.0 * 6.515871047973633
Epoch 170, val loss: 1.7487082481384277
Epoch 180, training loss: 14.7338285446167 = 1.7310022115707397 + 2.0 * 6.501413345336914
Epoch 180, val loss: 1.7380049228668213
Epoch 190, training loss: 14.69098949432373 = 1.7174627780914307 + 2.0 * 6.4867634773254395
Epoch 190, val loss: 1.7262521982192993
Epoch 200, training loss: 14.656195640563965 = 1.7023893594741821 + 2.0 * 6.476902961730957
Epoch 200, val loss: 1.7132996320724487
Epoch 210, training loss: 14.615490913391113 = 1.6856986284255981 + 2.0 * 6.464896202087402
Epoch 210, val loss: 1.6990121603012085
Epoch 220, training loss: 14.576072692871094 = 1.667218804359436 + 2.0 * 6.4544267654418945
Epoch 220, val loss: 1.6832047700881958
Epoch 230, training loss: 14.537227630615234 = 1.6466116905212402 + 2.0 * 6.445308208465576
Epoch 230, val loss: 1.6656941175460815
Epoch 240, training loss: 14.499566078186035 = 1.6236542463302612 + 2.0 * 6.437955856323242
Epoch 240, val loss: 1.6461561918258667
Epoch 250, training loss: 14.45919132232666 = 1.59845769405365 + 2.0 * 6.4303669929504395
Epoch 250, val loss: 1.624804139137268
Epoch 260, training loss: 14.416540145874023 = 1.570664882659912 + 2.0 * 6.422937870025635
Epoch 260, val loss: 1.6013551950454712
Epoch 270, training loss: 14.384119987487793 = 1.540433645248413 + 2.0 * 6.4218430519104
Epoch 270, val loss: 1.5760306119918823
Epoch 280, training loss: 14.331571578979492 = 1.507973313331604 + 2.0 * 6.41179895401001
Epoch 280, val loss: 1.5489516258239746
Epoch 290, training loss: 14.282770156860352 = 1.4732027053833008 + 2.0 * 6.404783725738525
Epoch 290, val loss: 1.520125150680542
Epoch 300, training loss: 14.23485279083252 = 1.4361573457717896 + 2.0 * 6.39934778213501
Epoch 300, val loss: 1.4896857738494873
Epoch 310, training loss: 14.18612289428711 = 1.3970990180969238 + 2.0 * 6.394512176513672
Epoch 310, val loss: 1.4578408002853394
Epoch 320, training loss: 14.13686466217041 = 1.356357455253601 + 2.0 * 6.39025354385376
Epoch 320, val loss: 1.4250155687332153
Epoch 330, training loss: 14.087930679321289 = 1.314120888710022 + 2.0 * 6.386904716491699
Epoch 330, val loss: 1.3912391662597656
Epoch 340, training loss: 14.034173965454102 = 1.2706753015518188 + 2.0 * 6.381749153137207
Epoch 340, val loss: 1.3567630052566528
Epoch 350, training loss: 13.990517616271973 = 1.2261077165603638 + 2.0 * 6.382205009460449
Epoch 350, val loss: 1.3218570947647095
Epoch 360, training loss: 13.938114166259766 = 1.1812214851379395 + 2.0 * 6.378446102142334
Epoch 360, val loss: 1.2871519327163696
Epoch 370, training loss: 13.881832122802734 = 1.136554479598999 + 2.0 * 6.372638702392578
Epoch 370, val loss: 1.2530211210250854
Epoch 380, training loss: 13.82886791229248 = 1.0923445224761963 + 2.0 * 6.368261814117432
Epoch 380, val loss: 1.2196189165115356
Epoch 390, training loss: 13.77929973602295 = 1.0489352941513062 + 2.0 * 6.365182399749756
Epoch 390, val loss: 1.1872457265853882
Epoch 400, training loss: 13.742887496948242 = 1.0069996118545532 + 2.0 * 6.36794376373291
Epoch 400, val loss: 1.1566698551177979
Epoch 410, training loss: 13.693553924560547 = 0.9675453305244446 + 2.0 * 6.363004207611084
Epoch 410, val loss: 1.128387451171875
Epoch 420, training loss: 13.643657684326172 = 0.9302539825439453 + 2.0 * 6.356701850891113
Epoch 420, val loss: 1.1022988557815552
Epoch 430, training loss: 13.602834701538086 = 0.8950347900390625 + 2.0 * 6.353899955749512
Epoch 430, val loss: 1.078391671180725
Epoch 440, training loss: 13.564769744873047 = 0.8618581891059875 + 2.0 * 6.3514556884765625
Epoch 440, val loss: 1.05655837059021
Epoch 450, training loss: 13.531779289245605 = 0.8307842016220093 + 2.0 * 6.350497722625732
Epoch 450, val loss: 1.03685462474823
Epoch 460, training loss: 13.4973783493042 = 0.8020362257957458 + 2.0 * 6.347671031951904
Epoch 460, val loss: 1.019435167312622
Epoch 470, training loss: 13.464170455932617 = 0.7750897407531738 + 2.0 * 6.344540119171143
Epoch 470, val loss: 1.003861427307129
Epoch 480, training loss: 13.443400382995605 = 0.7496607303619385 + 2.0 * 6.346869945526123
Epoch 480, val loss: 0.9898796677589417
Epoch 490, training loss: 13.415092468261719 = 0.7257203459739685 + 2.0 * 6.344686031341553
Epoch 490, val loss: 0.9774649143218994
Epoch 500, training loss: 13.388021469116211 = 0.7031384706497192 + 2.0 * 6.342441558837891
Epoch 500, val loss: 0.9663286209106445
Epoch 510, training loss: 13.354412078857422 = 0.6815909743309021 + 2.0 * 6.3364105224609375
Epoch 510, val loss: 0.95644611120224
Epoch 520, training loss: 13.329896926879883 = 0.6608872413635254 + 2.0 * 6.334505081176758
Epoch 520, val loss: 0.947475016117096
Epoch 530, training loss: 13.308577537536621 = 0.6408541798591614 + 2.0 * 6.333861827850342
Epoch 530, val loss: 0.9393793940544128
Epoch 540, training loss: 13.287917137145996 = 0.6215080618858337 + 2.0 * 6.333204746246338
Epoch 540, val loss: 0.9320927262306213
Epoch 550, training loss: 13.26484203338623 = 0.6027211546897888 + 2.0 * 6.331060409545898
Epoch 550, val loss: 0.9256542921066284
Epoch 560, training loss: 13.240766525268555 = 0.5845929980278015 + 2.0 * 6.328086853027344
Epoch 560, val loss: 0.9199140667915344
Epoch 570, training loss: 13.219199180603027 = 0.566916286945343 + 2.0 * 6.326141357421875
Epoch 570, val loss: 0.9148646593093872
Epoch 580, training loss: 13.19678020477295 = 0.549578070640564 + 2.0 * 6.323601245880127
Epoch 580, val loss: 0.9104560613632202
Epoch 590, training loss: 13.194266319274902 = 0.5326738953590393 + 2.0 * 6.330796241760254
Epoch 590, val loss: 0.9066895842552185
Epoch 600, training loss: 13.163433074951172 = 0.5162651538848877 + 2.0 * 6.323584079742432
Epoch 600, val loss: 0.9035974740982056
Epoch 610, training loss: 13.138897895812988 = 0.5002626776695251 + 2.0 * 6.319317817687988
Epoch 610, val loss: 0.9011288285255432
Epoch 620, training loss: 13.137561798095703 = 0.4845905900001526 + 2.0 * 6.326485633850098
Epoch 620, val loss: 0.899284839630127
Epoch 630, training loss: 13.105996131896973 = 0.4693593382835388 + 2.0 * 6.3183183670043945
Epoch 630, val loss: 0.8979640603065491
Epoch 640, training loss: 13.0838623046875 = 0.45442724227905273 + 2.0 * 6.314717769622803
Epoch 640, val loss: 0.8972862958908081
Epoch 650, training loss: 13.078642845153809 = 0.4397849440574646 + 2.0 * 6.31942892074585
Epoch 650, val loss: 0.8971953392028809
Epoch 660, training loss: 13.065716743469238 = 0.42555490136146545 + 2.0 * 6.320080757141113
Epoch 660, val loss: 0.8977205753326416
Epoch 670, training loss: 13.039219856262207 = 0.4116925299167633 + 2.0 * 6.313763618469238
Epoch 670, val loss: 0.8987715244293213
Epoch 680, training loss: 13.018836975097656 = 0.3981366753578186 + 2.0 * 6.310349941253662
Epoch 680, val loss: 0.9003363251686096
Epoch 690, training loss: 13.001373291015625 = 0.3848510682582855 + 2.0 * 6.308260917663574
Epoch 690, val loss: 0.9024250507354736
Epoch 700, training loss: 13.018533706665039 = 0.37186500430107117 + 2.0 * 6.323334217071533
Epoch 700, val loss: 0.9049952030181885
Epoch 710, training loss: 12.973883628845215 = 0.35932981967926025 + 2.0 * 6.307276725769043
Epoch 710, val loss: 0.9080895185470581
Epoch 720, training loss: 12.959172248840332 = 0.3471372723579407 + 2.0 * 6.3060173988342285
Epoch 720, val loss: 0.9115818738937378
Epoch 730, training loss: 12.944324493408203 = 0.3352426588535309 + 2.0 * 6.304541110992432
Epoch 730, val loss: 0.9155591726303101
Epoch 740, training loss: 12.93598747253418 = 0.3236085772514343 + 2.0 * 6.30618953704834
Epoch 740, val loss: 0.9199422001838684
Epoch 750, training loss: 12.918222427368164 = 0.3123559355735779 + 2.0 * 6.302933216094971
Epoch 750, val loss: 0.9247775077819824
Epoch 760, training loss: 12.911954879760742 = 0.3013603091239929 + 2.0 * 6.305297374725342
Epoch 760, val loss: 0.9299134612083435
Epoch 770, training loss: 12.891876220703125 = 0.290745347738266 + 2.0 * 6.300565242767334
Epoch 770, val loss: 0.9354591965675354
Epoch 780, training loss: 12.879921913146973 = 0.28041326999664307 + 2.0 * 6.2997541427612305
Epoch 780, val loss: 0.9413617849349976
Epoch 790, training loss: 12.869702339172363 = 0.2703225314617157 + 2.0 * 6.299689769744873
Epoch 790, val loss: 0.9475667476654053
Epoch 800, training loss: 12.868097305297852 = 0.26056843996047974 + 2.0 * 6.303764343261719
Epoch 800, val loss: 0.9540923833847046
Epoch 810, training loss: 12.845714569091797 = 0.25113415718078613 + 2.0 * 6.297290325164795
Epoch 810, val loss: 0.9609048366546631
Epoch 820, training loss: 12.83407974243164 = 0.24194768071174622 + 2.0 * 6.296065807342529
Epoch 820, val loss: 0.9680025577545166
Epoch 830, training loss: 12.824660301208496 = 0.23306281864643097 + 2.0 * 6.2957987785339355
Epoch 830, val loss: 0.9754011631011963
Epoch 840, training loss: 12.820937156677246 = 0.22443506121635437 + 2.0 * 6.298251152038574
Epoch 840, val loss: 0.9830164313316345
Epoch 850, training loss: 12.804814338684082 = 0.21609213948249817 + 2.0 * 6.294361114501953
Epoch 850, val loss: 0.9908531904220581
Epoch 860, training loss: 12.802569389343262 = 0.20805871486663818 + 2.0 * 6.297255516052246
Epoch 860, val loss: 0.9989239573478699
Epoch 870, training loss: 12.784276008605957 = 0.2002948671579361 + 2.0 * 6.291990756988525
Epoch 870, val loss: 1.0071090459823608
Epoch 880, training loss: 12.779631614685059 = 0.1927991509437561 + 2.0 * 6.2934160232543945
Epoch 880, val loss: 1.0155061483383179
Epoch 890, training loss: 12.766006469726562 = 0.18559236824512482 + 2.0 * 6.2902069091796875
Epoch 890, val loss: 1.0240131616592407
Epoch 900, training loss: 12.75843334197998 = 0.17865543067455292 + 2.0 * 6.289888858795166
Epoch 900, val loss: 1.0327450037002563
Epoch 910, training loss: 12.77207088470459 = 0.17197415232658386 + 2.0 * 6.300048351287842
Epoch 910, val loss: 1.0416126251220703
Epoch 920, training loss: 12.745565414428711 = 0.16555775701999664 + 2.0 * 6.290003776550293
Epoch 920, val loss: 1.0504716634750366
Epoch 930, training loss: 12.732196807861328 = 0.15940184891223907 + 2.0 * 6.286397457122803
Epoch 930, val loss: 1.0595141649246216
Epoch 940, training loss: 12.724556922912598 = 0.15345120429992676 + 2.0 * 6.285552978515625
Epoch 940, val loss: 1.0687122344970703
Epoch 950, training loss: 12.73659896850586 = 0.1477431356906891 + 2.0 * 6.294427871704102
Epoch 950, val loss: 1.0779362916946411
Epoch 960, training loss: 12.72803020477295 = 0.14228370785713196 + 2.0 * 6.292873382568359
Epoch 960, val loss: 1.0871679782867432
Epoch 970, training loss: 12.70511245727539 = 0.13705383241176605 + 2.0 * 6.284029483795166
Epoch 970, val loss: 1.0965261459350586
Epoch 980, training loss: 12.698545455932617 = 0.13203571736812592 + 2.0 * 6.283255100250244
Epoch 980, val loss: 1.1058714389801025
Epoch 990, training loss: 12.697731018066406 = 0.12721988558769226 + 2.0 * 6.285255432128906
Epoch 990, val loss: 1.1152701377868652
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.725925925925926
0.804955192409067
=== training gcn model ===
Epoch 0, training loss: 19.139419555664062 = 1.9457957744598389 + 2.0 * 8.59681224822998
Epoch 0, val loss: 1.9480414390563965
Epoch 10, training loss: 19.128150939941406 = 1.9353482723236084 + 2.0 * 8.59640121459961
Epoch 10, val loss: 1.9375718832015991
Epoch 20, training loss: 19.10936164855957 = 1.9217708110809326 + 2.0 * 8.593795776367188
Epoch 20, val loss: 1.92404043674469
Epoch 30, training loss: 19.056917190551758 = 1.902639389038086 + 2.0 * 8.577138900756836
Epoch 30, val loss: 1.9049540758132935
Epoch 40, training loss: 18.802085876464844 = 1.878699779510498 + 2.0 * 8.461692810058594
Epoch 40, val loss: 1.8818130493164062
Epoch 50, training loss: 17.72736358642578 = 1.8529009819030762 + 2.0 * 7.937231540679932
Epoch 50, val loss: 1.8569856882095337
Epoch 60, training loss: 16.768047332763672 = 1.8353188037872314 + 2.0 * 7.466363906860352
Epoch 60, val loss: 1.84022057056427
Epoch 70, training loss: 16.0976505279541 = 1.8236562013626099 + 2.0 * 7.136997222900391
Epoch 70, val loss: 1.8282040357589722
Epoch 80, training loss: 15.734430313110352 = 1.8137542009353638 + 2.0 * 6.960338115692139
Epoch 80, val loss: 1.8181556463241577
Epoch 90, training loss: 15.476105690002441 = 1.8028230667114258 + 2.0 * 6.836641311645508
Epoch 90, val loss: 1.8072640895843506
Epoch 100, training loss: 15.3023099899292 = 1.7912379503250122 + 2.0 * 6.755536079406738
Epoch 100, val loss: 1.7955667972564697
Epoch 110, training loss: 15.184020042419434 = 1.7797809839248657 + 2.0 * 6.70211935043335
Epoch 110, val loss: 1.7839964628219604
Epoch 120, training loss: 15.087435722351074 = 1.7683305740356445 + 2.0 * 6.659552574157715
Epoch 120, val loss: 1.7726298570632935
Epoch 130, training loss: 15.003934860229492 = 1.7563271522521973 + 2.0 * 6.623804092407227
Epoch 130, val loss: 1.761011004447937
Epoch 140, training loss: 14.93919849395752 = 1.7432432174682617 + 2.0 * 6.597977638244629
Epoch 140, val loss: 1.7488043308258057
Epoch 150, training loss: 14.871574401855469 = 1.72886061668396 + 2.0 * 6.571356773376465
Epoch 150, val loss: 1.7358028888702393
Epoch 160, training loss: 14.812734603881836 = 1.7128973007202148 + 2.0 * 6.5499186515808105
Epoch 160, val loss: 1.7217496633529663
Epoch 170, training loss: 14.756990432739258 = 1.6950414180755615 + 2.0 * 6.530974388122559
Epoch 170, val loss: 1.706268548965454
Epoch 180, training loss: 14.707210540771484 = 1.6749885082244873 + 2.0 * 6.516110897064209
Epoch 180, val loss: 1.6890554428100586
Epoch 190, training loss: 14.653521537780762 = 1.6525112390518188 + 2.0 * 6.500504970550537
Epoch 190, val loss: 1.6698977947235107
Epoch 200, training loss: 14.601371765136719 = 1.6273971796035767 + 2.0 * 6.486987113952637
Epoch 200, val loss: 1.6486049890518188
Epoch 210, training loss: 14.55969524383545 = 1.5993932485580444 + 2.0 * 6.480151176452637
Epoch 210, val loss: 1.6249542236328125
Epoch 220, training loss: 14.500726699829102 = 1.568778157234192 + 2.0 * 6.4659743309021
Epoch 220, val loss: 1.5991272926330566
Epoch 230, training loss: 14.447122573852539 = 1.5356013774871826 + 2.0 * 6.455760478973389
Epoch 230, val loss: 1.5712018013000488
Epoch 240, training loss: 14.392621994018555 = 1.4997766017913818 + 2.0 * 6.446422576904297
Epoch 240, val loss: 1.5411818027496338
Epoch 250, training loss: 14.343276977539062 = 1.4616202116012573 + 2.0 * 6.440828323364258
Epoch 250, val loss: 1.5093244314193726
Epoch 260, training loss: 14.28491497039795 = 1.4218158721923828 + 2.0 * 6.431549549102783
Epoch 260, val loss: 1.476206660270691
Epoch 270, training loss: 14.229391098022461 = 1.3805783987045288 + 2.0 * 6.4244065284729
Epoch 270, val loss: 1.4420031309127808
Epoch 280, training loss: 14.174389839172363 = 1.3381367921829224 + 2.0 * 6.418126583099365
Epoch 280, val loss: 1.4068623781204224
Epoch 290, training loss: 14.124422073364258 = 1.2949001789093018 + 2.0 * 6.414761066436768
Epoch 290, val loss: 1.3712337017059326
Epoch 300, training loss: 14.069988250732422 = 1.2518110275268555 + 2.0 * 6.409088611602783
Epoch 300, val loss: 1.3358241319656372
Epoch 310, training loss: 14.015033721923828 = 1.2088403701782227 + 2.0 * 6.403096675872803
Epoch 310, val loss: 1.3005812168121338
Epoch 320, training loss: 13.962930679321289 = 1.1658672094345093 + 2.0 * 6.398531913757324
Epoch 320, val loss: 1.2655410766601562
Epoch 330, training loss: 13.927627563476562 = 1.1231393814086914 + 2.0 * 6.4022440910339355
Epoch 330, val loss: 1.2306792736053467
Epoch 340, training loss: 13.86495590209961 = 1.081305742263794 + 2.0 * 6.391825199127197
Epoch 340, val loss: 1.1967135667800903
Epoch 350, training loss: 13.81635570526123 = 1.0405900478363037 + 2.0 * 6.387882709503174
Epoch 350, val loss: 1.1637964248657227
Epoch 360, training loss: 13.768452644348145 = 1.0009615421295166 + 2.0 * 6.3837456703186035
Epoch 360, val loss: 1.131873607635498
Epoch 370, training loss: 13.725631713867188 = 0.9626582264900208 + 2.0 * 6.381486892700195
Epoch 370, val loss: 1.101094126701355
Epoch 380, training loss: 13.68846607208252 = 0.9260688424110413 + 2.0 * 6.381198406219482
Epoch 380, val loss: 1.0721060037612915
Epoch 390, training loss: 13.641958236694336 = 0.8916369080543518 + 2.0 * 6.3751606941223145
Epoch 390, val loss: 1.0449837446212769
Epoch 400, training loss: 13.602164268493652 = 0.8590895533561707 + 2.0 * 6.371537208557129
Epoch 400, val loss: 1.0196876525878906
Epoch 410, training loss: 13.5685453414917 = 0.8284341096878052 + 2.0 * 6.370055675506592
Epoch 410, val loss: 0.9962256550788879
Epoch 420, training loss: 13.537070274353027 = 0.7997282147407532 + 2.0 * 6.36867094039917
Epoch 420, val loss: 0.9748005270957947
Epoch 430, training loss: 13.499770164489746 = 0.7726972103118896 + 2.0 * 6.363536357879639
Epoch 430, val loss: 0.9551360011100769
Epoch 440, training loss: 13.469738960266113 = 0.7470802664756775 + 2.0 * 6.361329555511475
Epoch 440, val loss: 0.9370332956314087
Epoch 450, training loss: 13.440390586853027 = 0.7226133346557617 + 2.0 * 6.358888626098633
Epoch 450, val loss: 0.9202379584312439
Epoch 460, training loss: 13.415538787841797 = 0.699059784412384 + 2.0 * 6.358239650726318
Epoch 460, val loss: 0.9046787023544312
Epoch 470, training loss: 13.394086837768555 = 0.6764159798622131 + 2.0 * 6.358835220336914
Epoch 470, val loss: 0.8900418281555176
Epoch 480, training loss: 13.359054565429688 = 0.6543614864349365 + 2.0 * 6.352346420288086
Epoch 480, val loss: 0.8764411807060242
Epoch 490, training loss: 13.333163261413574 = 0.6326967477798462 + 2.0 * 6.35023307800293
Epoch 490, val loss: 0.8634465336799622
Epoch 500, training loss: 13.313639640808105 = 0.6112745404243469 + 2.0 * 6.351182460784912
Epoch 500, val loss: 0.8509814143180847
Epoch 510, training loss: 13.283770561218262 = 0.5901061296463013 + 2.0 * 6.346832275390625
Epoch 510, val loss: 0.8389548659324646
Epoch 520, training loss: 13.26355266571045 = 0.5690197348594666 + 2.0 * 6.347266674041748
Epoch 520, val loss: 0.8273813128471375
Epoch 530, training loss: 13.230710983276367 = 0.54814612865448 + 2.0 * 6.341282367706299
Epoch 530, val loss: 0.8163096904754639
Epoch 540, training loss: 13.207097053527832 = 0.5274478793144226 + 2.0 * 6.339824676513672
Epoch 540, val loss: 0.8056528568267822
Epoch 550, training loss: 13.189556121826172 = 0.5069074630737305 + 2.0 * 6.341324329376221
Epoch 550, val loss: 0.7954638004302979
Epoch 560, training loss: 13.169454574584961 = 0.4868040084838867 + 2.0 * 6.341325283050537
Epoch 560, val loss: 0.7857780456542969
Epoch 570, training loss: 13.137537002563477 = 0.46718430519104004 + 2.0 * 6.335176467895508
Epoch 570, val loss: 0.7768152356147766
Epoch 580, training loss: 13.114194869995117 = 0.4479683041572571 + 2.0 * 6.333113193511963
Epoch 580, val loss: 0.768450915813446
Epoch 590, training loss: 13.094819068908691 = 0.4292078912258148 + 2.0 * 6.332805633544922
Epoch 590, val loss: 0.7606965899467468
Epoch 600, training loss: 13.07275104522705 = 0.41101738810539246 + 2.0 * 6.330866813659668
Epoch 600, val loss: 0.7535995841026306
Epoch 610, training loss: 13.054512977600098 = 0.39335447549819946 + 2.0 * 6.3305792808532715
Epoch 610, val loss: 0.7471581697463989
Epoch 620, training loss: 13.033660888671875 = 0.3762825131416321 + 2.0 * 6.328689098358154
Epoch 620, val loss: 0.741358757019043
Epoch 630, training loss: 13.01767635345459 = 0.35976821184158325 + 2.0 * 6.328954219818115
Epoch 630, val loss: 0.7362392544746399
Epoch 640, training loss: 12.993639945983887 = 0.34386736154556274 + 2.0 * 6.324886322021484
Epoch 640, val loss: 0.7317095994949341
Epoch 650, training loss: 12.97360897064209 = 0.3284381330013275 + 2.0 * 6.322585582733154
Epoch 650, val loss: 0.7277892827987671
Epoch 660, training loss: 12.980419158935547 = 0.31351539492607117 + 2.0 * 6.333451747894287
Epoch 660, val loss: 0.7243624925613403
Epoch 670, training loss: 12.938907623291016 = 0.29917973279953003 + 2.0 * 6.319863796234131
Epoch 670, val loss: 0.7214069366455078
Epoch 680, training loss: 12.924559593200684 = 0.28537169098854065 + 2.0 * 6.319593906402588
Epoch 680, val loss: 0.7190291285514832
Epoch 690, training loss: 12.90514850616455 = 0.272041916847229 + 2.0 * 6.316553115844727
Epoch 690, val loss: 0.7172042727470398
Epoch 700, training loss: 12.892809867858887 = 0.259178102016449 + 2.0 * 6.3168158531188965
Epoch 700, val loss: 0.7158812880516052
Epoch 710, training loss: 12.888829231262207 = 0.24684250354766846 + 2.0 * 6.320993423461914
Epoch 710, val loss: 0.715057909488678
Epoch 720, training loss: 12.865157127380371 = 0.23514606058597565 + 2.0 * 6.315005302429199
Epoch 720, val loss: 0.7145722508430481
Epoch 730, training loss: 12.849020004272461 = 0.22394903004169464 + 2.0 * 6.312535285949707
Epoch 730, val loss: 0.7146160006523132
Epoch 740, training loss: 12.837549209594727 = 0.2132532000541687 + 2.0 * 6.312148094177246
Epoch 740, val loss: 0.715103030204773
Epoch 750, training loss: 12.823777198791504 = 0.20308779180049896 + 2.0 * 6.310344696044922
Epoch 750, val loss: 0.7159320712089539
Epoch 760, training loss: 12.813790321350098 = 0.19346404075622559 + 2.0 * 6.3101630210876465
Epoch 760, val loss: 0.7171545028686523
Epoch 770, training loss: 12.79875373840332 = 0.18433834612369537 + 2.0 * 6.3072075843811035
Epoch 770, val loss: 0.7187554240226746
Epoch 780, training loss: 12.810251235961914 = 0.17567074298858643 + 2.0 * 6.317290306091309
Epoch 780, val loss: 0.7207268476486206
Epoch 790, training loss: 12.782585144042969 = 0.1675647646188736 + 2.0 * 6.3075103759765625
Epoch 790, val loss: 0.7228920459747314
Epoch 800, training loss: 12.769316673278809 = 0.15990440547466278 + 2.0 * 6.30470609664917
Epoch 800, val loss: 0.7253829836845398
Epoch 810, training loss: 12.757874488830566 = 0.15265831351280212 + 2.0 * 6.302608013153076
Epoch 810, val loss: 0.7282624840736389
Epoch 820, training loss: 12.74882698059082 = 0.1457817107439041 + 2.0 * 6.301522731781006
Epoch 820, val loss: 0.7314424514770508
Epoch 830, training loss: 12.77121353149414 = 0.1392674446105957 + 2.0 * 6.315973281860352
Epoch 830, val loss: 0.7348268628120422
Epoch 840, training loss: 12.737887382507324 = 0.13315536081790924 + 2.0 * 6.302365779876709
Epoch 840, val loss: 0.7384579181671143
Epoch 850, training loss: 12.727867126464844 = 0.12738752365112305 + 2.0 * 6.3002400398254395
Epoch 850, val loss: 0.7422356009483337
Epoch 860, training loss: 12.716996192932129 = 0.12191726267337799 + 2.0 * 6.297539234161377
Epoch 860, val loss: 0.7462993860244751
Epoch 870, training loss: 12.709595680236816 = 0.11671856790781021 + 2.0 * 6.296438694000244
Epoch 870, val loss: 0.7505730986595154
Epoch 880, training loss: 12.705877304077148 = 0.1117786392569542 + 2.0 * 6.297049522399902
Epoch 880, val loss: 0.7550538778305054
Epoch 890, training loss: 12.707216262817383 = 0.10710605978965759 + 2.0 * 6.300055027008057
Epoch 890, val loss: 0.7594785690307617
Epoch 900, training loss: 12.693219184875488 = 0.102718785405159 + 2.0 * 6.295250415802002
Epoch 900, val loss: 0.7641934156417847
Epoch 910, training loss: 12.685789108276367 = 0.09854542464017868 + 2.0 * 6.293622016906738
Epoch 910, val loss: 0.7689353823661804
Epoch 920, training loss: 12.678609848022461 = 0.09457746893167496 + 2.0 * 6.29201602935791
Epoch 920, val loss: 0.7739182710647583
Epoch 930, training loss: 12.672088623046875 = 0.0907990038394928 + 2.0 * 6.290644645690918
Epoch 930, val loss: 0.7789266705513
Epoch 940, training loss: 12.673751831054688 = 0.08720353990793228 + 2.0 * 6.29327392578125
Epoch 940, val loss: 0.7840396761894226
Epoch 950, training loss: 12.669462203979492 = 0.08378470689058304 + 2.0 * 6.2928385734558105
Epoch 950, val loss: 0.7891422510147095
Epoch 960, training loss: 12.658853530883789 = 0.08056587725877762 + 2.0 * 6.289144039154053
Epoch 960, val loss: 0.7942857146263123
Epoch 970, training loss: 12.66103744506836 = 0.07749910652637482 + 2.0 * 6.291769027709961
Epoch 970, val loss: 0.7995642423629761
Epoch 980, training loss: 12.648181915283203 = 0.07458070665597916 + 2.0 * 6.286800384521484
Epoch 980, val loss: 0.804845929145813
Epoch 990, training loss: 12.644440650939941 = 0.0718022882938385 + 2.0 * 6.286319255828857
Epoch 990, val loss: 0.8101585507392883
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7222222222222222
0.8060094886663153
=== training gcn model ===
Epoch 0, training loss: 19.141292572021484 = 1.9477182626724243 + 2.0 * 8.596787452697754
Epoch 0, val loss: 1.949202060699463
Epoch 10, training loss: 19.12865447998047 = 1.9363528490066528 + 2.0 * 8.596150398254395
Epoch 10, val loss: 1.9380635023117065
Epoch 20, training loss: 19.105409622192383 = 1.9217180013656616 + 2.0 * 8.591845512390137
Epoch 20, val loss: 1.9230769872665405
Epoch 30, training loss: 19.026823043823242 = 1.901781439781189 + 2.0 * 8.562520980834961
Epoch 30, val loss: 1.9024988412857056
Epoch 40, training loss: 18.554611206054688 = 1.8796331882476807 + 2.0 * 8.337489128112793
Epoch 40, val loss: 1.8809034824371338
Epoch 50, training loss: 17.331525802612305 = 1.8586136102676392 + 2.0 * 7.736456394195557
Epoch 50, val loss: 1.8617134094238281
Epoch 60, training loss: 16.639833450317383 = 1.8462557792663574 + 2.0 * 7.396788597106934
Epoch 60, val loss: 1.8507846593856812
Epoch 70, training loss: 16.089336395263672 = 1.83590829372406 + 2.0 * 7.126713752746582
Epoch 70, val loss: 1.841201901435852
Epoch 80, training loss: 15.715950012207031 = 1.8264920711517334 + 2.0 * 6.944728851318359
Epoch 80, val loss: 1.8325990438461304
Epoch 90, training loss: 15.482321739196777 = 1.8165252208709717 + 2.0 * 6.832898139953613
Epoch 90, val loss: 1.823144555091858
Epoch 100, training loss: 15.313215255737305 = 1.8055881261825562 + 2.0 * 6.753813743591309
Epoch 100, val loss: 1.8128023147583008
Epoch 110, training loss: 15.178235054016113 = 1.7951184511184692 + 2.0 * 6.691558361053467
Epoch 110, val loss: 1.8029565811157227
Epoch 120, training loss: 15.078393936157227 = 1.7854888439178467 + 2.0 * 6.6464524269104
Epoch 120, val loss: 1.7937898635864258
Epoch 130, training loss: 14.993290901184082 = 1.7757675647735596 + 2.0 * 6.608761787414551
Epoch 130, val loss: 1.7843263149261475
Epoch 140, training loss: 14.926416397094727 = 1.7652592658996582 + 2.0 * 6.580578327178955
Epoch 140, val loss: 1.7746000289916992
Epoch 150, training loss: 14.868709564208984 = 1.7538914680480957 + 2.0 * 6.557409286499023
Epoch 150, val loss: 1.764374852180481
Epoch 160, training loss: 14.812736511230469 = 1.7414000034332275 + 2.0 * 6.53566837310791
Epoch 160, val loss: 1.7533941268920898
Epoch 170, training loss: 14.762943267822266 = 1.7274757623672485 + 2.0 * 6.517733573913574
Epoch 170, val loss: 1.7414103746414185
Epoch 180, training loss: 14.715486526489258 = 1.7118611335754395 + 2.0 * 6.501812934875488
Epoch 180, val loss: 1.7280299663543701
Epoch 190, training loss: 14.672468185424805 = 1.6942133903503418 + 2.0 * 6.4891276359558105
Epoch 190, val loss: 1.7128889560699463
Epoch 200, training loss: 14.622982025146484 = 1.674241542816162 + 2.0 * 6.474370002746582
Epoch 200, val loss: 1.6958696842193604
Epoch 210, training loss: 14.578189849853516 = 1.6517387628555298 + 2.0 * 6.463225364685059
Epoch 210, val loss: 1.6766555309295654
Epoch 220, training loss: 14.53882122039795 = 1.6263850927352905 + 2.0 * 6.456218242645264
Epoch 220, val loss: 1.655055046081543
Epoch 230, training loss: 14.485420227050781 = 1.5982472896575928 + 2.0 * 6.443586349487305
Epoch 230, val loss: 1.6310217380523682
Epoch 240, training loss: 14.438586235046387 = 1.5673528909683228 + 2.0 * 6.435616493225098
Epoch 240, val loss: 1.6047879457473755
Epoch 250, training loss: 14.389899253845215 = 1.5340502262115479 + 2.0 * 6.427924633026123
Epoch 250, val loss: 1.5766266584396362
Epoch 260, training loss: 14.33984088897705 = 1.4984667301177979 + 2.0 * 6.420687198638916
Epoch 260, val loss: 1.54694664478302
Epoch 270, training loss: 14.293557167053223 = 1.461142897605896 + 2.0 * 6.416207313537598
Epoch 270, val loss: 1.5162051916122437
Epoch 280, training loss: 14.24657154083252 = 1.422979712486267 + 2.0 * 6.4117960929870605
Epoch 280, val loss: 1.4852944612503052
Epoch 290, training loss: 14.193833351135254 = 1.3847887516021729 + 2.0 * 6.40452241897583
Epoch 290, val loss: 1.4547972679138184
Epoch 300, training loss: 14.142936706542969 = 1.3467681407928467 + 2.0 * 6.3980841636657715
Epoch 300, val loss: 1.4250046014785767
Epoch 310, training loss: 14.095529556274414 = 1.309134602546692 + 2.0 * 6.393197536468506
Epoch 310, val loss: 1.395787239074707
Epoch 320, training loss: 14.056964874267578 = 1.2719711065292358 + 2.0 * 6.3924970626831055
Epoch 320, val loss: 1.3674731254577637
Epoch 330, training loss: 14.005332946777344 = 1.2359318733215332 + 2.0 * 6.384700775146484
Epoch 330, val loss: 1.34019935131073
Epoch 340, training loss: 13.961749076843262 = 1.2007081508636475 + 2.0 * 6.380520343780518
Epoch 340, val loss: 1.313838243484497
Epoch 350, training loss: 13.924394607543945 = 1.166184425354004 + 2.0 * 6.379105091094971
Epoch 350, val loss: 1.2881428003311157
Epoch 360, training loss: 13.877861976623535 = 1.1323680877685547 + 2.0 * 6.37274694442749
Epoch 360, val loss: 1.2630351781845093
Epoch 370, training loss: 13.846260070800781 = 1.0991131067276 + 2.0 * 6.373573303222656
Epoch 370, val loss: 1.2384486198425293
Epoch 380, training loss: 13.803504943847656 = 1.066689372062683 + 2.0 * 6.368407726287842
Epoch 380, val loss: 1.2144781351089478
Epoch 390, training loss: 13.76219654083252 = 1.0351152420043945 + 2.0 * 6.3635406494140625
Epoch 390, val loss: 1.1912834644317627
Epoch 400, training loss: 13.724044799804688 = 1.004178762435913 + 2.0 * 6.359932899475098
Epoch 400, val loss: 1.1686829328536987
Epoch 410, training loss: 13.701723098754883 = 0.9740535020828247 + 2.0 * 6.363834857940674
Epoch 410, val loss: 1.1466699838638306
Epoch 420, training loss: 13.656001091003418 = 0.9448525309562683 + 2.0 * 6.355574131011963
Epoch 420, val loss: 1.1255353689193726
Epoch 430, training loss: 13.621432304382324 = 0.9165201783180237 + 2.0 * 6.352456092834473
Epoch 430, val loss: 1.1051137447357178
Epoch 440, training loss: 13.588749885559082 = 0.8890154957771301 + 2.0 * 6.349867343902588
Epoch 440, val loss: 1.0854066610336304
Epoch 450, training loss: 13.5565824508667 = 0.8624111413955688 + 2.0 * 6.347085475921631
Epoch 450, val loss: 1.0664846897125244
Epoch 460, training loss: 13.526975631713867 = 0.8364914655685425 + 2.0 * 6.345242023468018
Epoch 460, val loss: 1.0483019351959229
Epoch 470, training loss: 13.495115280151367 = 0.8113229274749756 + 2.0 * 6.341896057128906
Epoch 470, val loss: 1.030775785446167
Epoch 480, training loss: 13.47545051574707 = 0.786915123462677 + 2.0 * 6.344267845153809
Epoch 480, val loss: 1.0140222311019897
Epoch 490, training loss: 13.438992500305176 = 0.7631450891494751 + 2.0 * 6.337923526763916
Epoch 490, val loss: 0.9981008768081665
Epoch 500, training loss: 13.415842056274414 = 0.7400497794151306 + 2.0 * 6.337896347045898
Epoch 500, val loss: 0.9827771186828613
Epoch 510, training loss: 13.386054039001465 = 0.7175407409667969 + 2.0 * 6.334256649017334
Epoch 510, val loss: 0.9681532382965088
Epoch 520, training loss: 13.359399795532227 = 0.6955974698066711 + 2.0 * 6.3319010734558105
Epoch 520, val loss: 0.9541692733764648
Epoch 530, training loss: 13.339550018310547 = 0.674075186252594 + 2.0 * 6.332737445831299
Epoch 530, val loss: 0.9407945275306702
Epoch 540, training loss: 13.310385704040527 = 0.6530470252037048 + 2.0 * 6.328669548034668
Epoch 540, val loss: 0.9280635118484497
Epoch 550, training loss: 13.29220962524414 = 0.6323879361152649 + 2.0 * 6.329910755157471
Epoch 550, val loss: 0.9158909916877747
Epoch 560, training loss: 13.262869834899902 = 0.6122744083404541 + 2.0 * 6.325297832489014
Epoch 560, val loss: 0.9042587280273438
Epoch 570, training loss: 13.2451753616333 = 0.5924509167671204 + 2.0 * 6.326362133026123
Epoch 570, val loss: 0.8932000398635864
Epoch 580, training loss: 13.21534252166748 = 0.5730478167533875 + 2.0 * 6.321147441864014
Epoch 580, val loss: 0.8827367424964905
Epoch 590, training loss: 13.193734169006348 = 0.5539783239364624 + 2.0 * 6.319878101348877
Epoch 590, val loss: 0.8726693987846375
Epoch 600, training loss: 13.170951843261719 = 0.5351567268371582 + 2.0 * 6.317897796630859
Epoch 600, val loss: 0.8631640672683716
Epoch 610, training loss: 13.149824142456055 = 0.5167235136032104 + 2.0 * 6.316550254821777
Epoch 610, val loss: 0.8540829420089722
Epoch 620, training loss: 13.140115737915039 = 0.49853622913360596 + 2.0 * 6.320789813995361
Epoch 620, val loss: 0.8454336524009705
Epoch 630, training loss: 13.112470626831055 = 0.4806680381298065 + 2.0 * 6.315901279449463
Epoch 630, val loss: 0.8374348282814026
Epoch 640, training loss: 13.089229583740234 = 0.463148295879364 + 2.0 * 6.313040733337402
Epoch 640, val loss: 0.8296828866004944
Epoch 650, training loss: 13.069129943847656 = 0.4458986818790436 + 2.0 * 6.311615467071533
Epoch 650, val loss: 0.8224517107009888
Epoch 660, training loss: 13.049903869628906 = 0.428936243057251 + 2.0 * 6.310483932495117
Epoch 660, val loss: 0.8156747221946716
Epoch 670, training loss: 13.035088539123535 = 0.41220664978027344 + 2.0 * 6.311440944671631
Epoch 670, val loss: 0.809327244758606
Epoch 680, training loss: 13.020793914794922 = 0.3958474099636078 + 2.0 * 6.312473297119141
Epoch 680, val loss: 0.8034716844558716
Epoch 690, training loss: 12.991819381713867 = 0.37988343834877014 + 2.0 * 6.305967807769775
Epoch 690, val loss: 0.7979350090026855
Epoch 700, training loss: 12.97316837310791 = 0.36419564485549927 + 2.0 * 6.304486274719238
Epoch 700, val loss: 0.7929511666297913
Epoch 710, training loss: 12.954657554626465 = 0.34880322217941284 + 2.0 * 6.302927017211914
Epoch 710, val loss: 0.7885286211967468
Epoch 720, training loss: 12.951020240783691 = 0.33375057578086853 + 2.0 * 6.3086347579956055
Epoch 720, val loss: 0.7845744490623474
Epoch 730, training loss: 12.92503547668457 = 0.31909364461898804 + 2.0 * 6.302970886230469
Epoch 730, val loss: 0.7811832427978516
Epoch 740, training loss: 12.917332649230957 = 0.3048737347126007 + 2.0 * 6.306229591369629
Epoch 740, val loss: 0.778412401676178
Epoch 750, training loss: 12.887935638427734 = 0.29120832681655884 + 2.0 * 6.29836368560791
Epoch 750, val loss: 0.776078999042511
Epoch 760, training loss: 12.874701499938965 = 0.2779763340950012 + 2.0 * 6.298362731933594
Epoch 760, val loss: 0.7742680311203003
Epoch 770, training loss: 12.858458518981934 = 0.265110582113266 + 2.0 * 6.296673774719238
Epoch 770, val loss: 0.7730828523635864
Epoch 780, training loss: 12.85291576385498 = 0.25270095467567444 + 2.0 * 6.300107479095459
Epoch 780, val loss: 0.7724221348762512
Epoch 790, training loss: 12.82982349395752 = 0.24078072607517242 + 2.0 * 6.294521331787109
Epoch 790, val loss: 0.7720907330513
Epoch 800, training loss: 12.822607040405273 = 0.22930920124053955 + 2.0 * 6.296648979187012
Epoch 800, val loss: 0.7723011374473572
Epoch 810, training loss: 12.813693046569824 = 0.21835893392562866 + 2.0 * 6.297667026519775
Epoch 810, val loss: 0.7730168104171753
Epoch 820, training loss: 12.797298431396484 = 0.20785000920295715 + 2.0 * 6.294723987579346
Epoch 820, val loss: 0.7739219069480896
Epoch 830, training loss: 12.780875205993652 = 0.19785745441913605 + 2.0 * 6.291508674621582
Epoch 830, val loss: 0.7753567099571228
Epoch 840, training loss: 12.771716117858887 = 0.18827004730701447 + 2.0 * 6.291723251342773
Epoch 840, val loss: 0.7771753668785095
Epoch 850, training loss: 12.757543563842773 = 0.17914295196533203 + 2.0 * 6.289200305938721
Epoch 850, val loss: 0.7793384790420532
Epoch 860, training loss: 12.746947288513184 = 0.17047062516212463 + 2.0 * 6.288238525390625
Epoch 860, val loss: 0.7816959619522095
Epoch 870, training loss: 12.752331733703613 = 0.16221854090690613 + 2.0 * 6.2950568199157715
Epoch 870, val loss: 0.7844513654708862
Epoch 880, training loss: 12.734109878540039 = 0.15444236993789673 + 2.0 * 6.2898335456848145
Epoch 880, val loss: 0.7876002192497253
Epoch 890, training loss: 12.718994140625 = 0.14707648754119873 + 2.0 * 6.285958766937256
Epoch 890, val loss: 0.7906966805458069
Epoch 900, training loss: 12.717976570129395 = 0.14010508358478546 + 2.0 * 6.288935661315918
Epoch 900, val loss: 0.79414963722229
Epoch 910, training loss: 12.70170783996582 = 0.13352225720882416 + 2.0 * 6.284092903137207
Epoch 910, val loss: 0.7976992130279541
Epoch 920, training loss: 12.697874069213867 = 0.12727928161621094 + 2.0 * 6.285297393798828
Epoch 920, val loss: 0.8016451597213745
Epoch 930, training loss: 12.68991756439209 = 0.12139707803726196 + 2.0 * 6.284260272979736
Epoch 930, val loss: 0.8056319355964661
Epoch 940, training loss: 12.678483009338379 = 0.1158648356795311 + 2.0 * 6.281309127807617
Epoch 940, val loss: 0.8095978498458862
Epoch 950, training loss: 12.67734146118164 = 0.11063960939645767 + 2.0 * 6.283350944519043
Epoch 950, val loss: 0.813930332660675
Epoch 960, training loss: 12.666279792785645 = 0.10569404065608978 + 2.0 * 6.280292987823486
Epoch 960, val loss: 0.8184050917625427
Epoch 970, training loss: 12.658946990966797 = 0.10104837268590927 + 2.0 * 6.27894926071167
Epoch 970, val loss: 0.8227525949478149
Epoch 980, training loss: 12.65692138671875 = 0.09665247797966003 + 2.0 * 6.280134677886963
Epoch 980, val loss: 0.8273677229881287
Epoch 990, training loss: 12.65044116973877 = 0.09250558167695999 + 2.0 * 6.27896785736084
Epoch 990, val loss: 0.8321902751922607
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7666666666666667
0.8070637849235636
The final CL Acc:0.73827, 0.02014, The final GNN Acc:0.80601, 0.00086
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13178])
remove edge: torch.Size([2, 7900])
updated graph: torch.Size([2, 10522])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 19.149803161621094 = 1.956160545349121 + 2.0 * 8.596821784973145
Epoch 0, val loss: 1.9549006223678589
Epoch 10, training loss: 19.138669967651367 = 1.9457213878631592 + 2.0 * 8.596474647521973
Epoch 10, val loss: 1.9445199966430664
Epoch 20, training loss: 19.120208740234375 = 1.93265962600708 + 2.0 * 8.593774795532227
Epoch 20, val loss: 1.9313803911209106
Epoch 30, training loss: 19.062705993652344 = 1.914504051208496 + 2.0 * 8.574101448059082
Epoch 30, val loss: 1.913023591041565
Epoch 40, training loss: 18.80521011352539 = 1.891910195350647 + 2.0 * 8.456649780273438
Epoch 40, val loss: 1.8915612697601318
Epoch 50, training loss: 18.03121566772461 = 1.8671293258666992 + 2.0 * 8.082042694091797
Epoch 50, val loss: 1.8687993288040161
Epoch 60, training loss: 17.198457717895508 = 1.8465903997421265 + 2.0 * 7.675933361053467
Epoch 60, val loss: 1.851464867591858
Epoch 70, training loss: 16.192249298095703 = 1.8342770338058472 + 2.0 * 7.178986072540283
Epoch 70, val loss: 1.8414068222045898
Epoch 80, training loss: 15.760674476623535 = 1.8229988813400269 + 2.0 * 6.968837738037109
Epoch 80, val loss: 1.8311562538146973
Epoch 90, training loss: 15.469677925109863 = 1.8071035146713257 + 2.0 * 6.831287384033203
Epoch 90, val loss: 1.8164445161819458
Epoch 100, training loss: 15.288970947265625 = 1.7908521890640259 + 2.0 * 6.749059200286865
Epoch 100, val loss: 1.802107810974121
Epoch 110, training loss: 15.166505813598633 = 1.7758952379226685 + 2.0 * 6.695305347442627
Epoch 110, val loss: 1.7891175746917725
Epoch 120, training loss: 15.066226959228516 = 1.7613842487335205 + 2.0 * 6.652421474456787
Epoch 120, val loss: 1.7764674425125122
Epoch 130, training loss: 14.980920791625977 = 1.7468748092651367 + 2.0 * 6.61702299118042
Epoch 130, val loss: 1.7636501789093018
Epoch 140, training loss: 14.906879425048828 = 1.731899619102478 + 2.0 * 6.587490081787109
Epoch 140, val loss: 1.7502822875976562
Epoch 150, training loss: 14.839622497558594 = 1.7157624959945679 + 2.0 * 6.561930179595947
Epoch 150, val loss: 1.736003041267395
Epoch 160, training loss: 14.77660083770752 = 1.6980339288711548 + 2.0 * 6.539283275604248
Epoch 160, val loss: 1.720474362373352
Epoch 170, training loss: 14.721381187438965 = 1.678314208984375 + 2.0 * 6.521533489227295
Epoch 170, val loss: 1.7032984495162964
Epoch 180, training loss: 14.666348457336426 = 1.6564114093780518 + 2.0 * 6.504968643188477
Epoch 180, val loss: 1.6843066215515137
Epoch 190, training loss: 14.611626625061035 = 1.6322860717773438 + 2.0 * 6.489670276641846
Epoch 190, val loss: 1.6634079217910767
Epoch 200, training loss: 14.558979034423828 = 1.6055786609649658 + 2.0 * 6.476700305938721
Epoch 200, val loss: 1.6401958465576172
Epoch 210, training loss: 14.510441780090332 = 1.576167106628418 + 2.0 * 6.467137336730957
Epoch 210, val loss: 1.6145635843276978
Epoch 220, training loss: 14.453914642333984 = 1.5443177223205566 + 2.0 * 6.454798221588135
Epoch 220, val loss: 1.5867962837219238
Epoch 230, training loss: 14.399961471557617 = 1.510164499282837 + 2.0 * 6.44489860534668
Epoch 230, val loss: 1.5570396184921265
Epoch 240, training loss: 14.344757080078125 = 1.4738940000534058 + 2.0 * 6.435431480407715
Epoch 240, val loss: 1.5254147052764893
Epoch 250, training loss: 14.297958374023438 = 1.435762643814087 + 2.0 * 6.431097984313965
Epoch 250, val loss: 1.4921855926513672
Epoch 260, training loss: 14.236528396606445 = 1.3964269161224365 + 2.0 * 6.420050621032715
Epoch 260, val loss: 1.4580225944519043
Epoch 270, training loss: 14.18204116821289 = 1.3561944961547852 + 2.0 * 6.412923336029053
Epoch 270, val loss: 1.4232686758041382
Epoch 280, training loss: 14.132646560668945 = 1.3157105445861816 + 2.0 * 6.408468246459961
Epoch 280, val loss: 1.3886133432388306
Epoch 290, training loss: 14.074825286865234 = 1.2754164934158325 + 2.0 * 6.399704456329346
Epoch 290, val loss: 1.3546063899993896
Epoch 300, training loss: 14.02297592163086 = 1.235540509223938 + 2.0 * 6.3937177658081055
Epoch 300, val loss: 1.321380853652954
Epoch 310, training loss: 13.98012638092041 = 1.1962279081344604 + 2.0 * 6.39194917678833
Epoch 310, val loss: 1.289141058921814
Epoch 320, training loss: 13.92905044555664 = 1.1581759452819824 + 2.0 * 6.385437488555908
Epoch 320, val loss: 1.258563756942749
Epoch 330, training loss: 13.882044792175293 = 1.1216013431549072 + 2.0 * 6.380221843719482
Epoch 330, val loss: 1.2297654151916504
Epoch 340, training loss: 13.837272644042969 = 1.0861679315567017 + 2.0 * 6.375552177429199
Epoch 340, val loss: 1.2023476362228394
Epoch 350, training loss: 13.795822143554688 = 1.0516810417175293 + 2.0 * 6.3720703125
Epoch 350, val loss: 1.176091194152832
Epoch 360, training loss: 13.758779525756836 = 1.0184153318405151 + 2.0 * 6.370182037353516
Epoch 360, val loss: 1.1511635780334473
Epoch 370, training loss: 13.716861724853516 = 0.986243724822998 + 2.0 * 6.365309238433838
Epoch 370, val loss: 1.127572774887085
Epoch 380, training loss: 13.676740646362305 = 0.9552432894706726 + 2.0 * 6.360748767852783
Epoch 380, val loss: 1.1052652597427368
Epoch 390, training loss: 13.644622802734375 = 0.925197958946228 + 2.0 * 6.359712600708008
Epoch 390, val loss: 1.083877682685852
Epoch 400, training loss: 13.60483455657959 = 0.8960915207862854 + 2.0 * 6.354371547698975
Epoch 400, val loss: 1.0634790658950806
Epoch 410, training loss: 13.570701599121094 = 0.867767870426178 + 2.0 * 6.351466655731201
Epoch 410, val loss: 1.0439502000808716
Epoch 420, training loss: 13.53900146484375 = 0.8400520086288452 + 2.0 * 6.349474906921387
Epoch 420, val loss: 1.025167465209961
Epoch 430, training loss: 13.509379386901855 = 0.8130648136138916 + 2.0 * 6.3481574058532715
Epoch 430, val loss: 1.0072236061096191
Epoch 440, training loss: 13.471356391906738 = 0.7867234945297241 + 2.0 * 6.342316627502441
Epoch 440, val loss: 0.9901317358016968
Epoch 450, training loss: 13.455949783325195 = 0.7609978914260864 + 2.0 * 6.347476005554199
Epoch 450, val loss: 0.973838210105896
Epoch 460, training loss: 13.411774635314941 = 0.7359812259674072 + 2.0 * 6.337896823883057
Epoch 460, val loss: 0.9583418965339661
Epoch 470, training loss: 13.382427215576172 = 0.711638331413269 + 2.0 * 6.335394382476807
Epoch 470, val loss: 0.9437860250473022
Epoch 480, training loss: 13.353877067565918 = 0.6877902150154114 + 2.0 * 6.333043575286865
Epoch 480, val loss: 0.9299673438072205
Epoch 490, training loss: 13.325671195983887 = 0.6643854379653931 + 2.0 * 6.3306427001953125
Epoch 490, val loss: 0.9168543219566345
Epoch 500, training loss: 13.302396774291992 = 0.6415143013000488 + 2.0 * 6.330441474914551
Epoch 500, val loss: 0.9045038223266602
Epoch 510, training loss: 13.277176856994629 = 0.6193587779998779 + 2.0 * 6.328908920288086
Epoch 510, val loss: 0.8930978178977966
Epoch 520, training loss: 13.250560760498047 = 0.5978220105171204 + 2.0 * 6.326369285583496
Epoch 520, val loss: 0.8825499415397644
Epoch 530, training loss: 13.22371768951416 = 0.5769363045692444 + 2.0 * 6.323390483856201
Epoch 530, val loss: 0.8728579878807068
Epoch 540, training loss: 13.200102806091309 = 0.5565595626831055 + 2.0 * 6.321771621704102
Epoch 540, val loss: 0.8640083074569702
Epoch 550, training loss: 13.190797805786133 = 0.5367398262023926 + 2.0 * 6.327029228210449
Epoch 550, val loss: 0.8559038639068604
Epoch 560, training loss: 13.155441284179688 = 0.5175716280937195 + 2.0 * 6.318934917449951
Epoch 560, val loss: 0.8486724495887756
Epoch 570, training loss: 13.133035659790039 = 0.4989253282546997 + 2.0 * 6.3170552253723145
Epoch 570, val loss: 0.8421589136123657
Epoch 580, training loss: 13.11512565612793 = 0.48075366020202637 + 2.0 * 6.317185878753662
Epoch 580, val loss: 0.8362454175949097
Epoch 590, training loss: 13.094928741455078 = 0.4631882905960083 + 2.0 * 6.31587028503418
Epoch 590, val loss: 0.8310461044311523
Epoch 600, training loss: 13.074790954589844 = 0.44618749618530273 + 2.0 * 6.314301490783691
Epoch 600, val loss: 0.8265036344528198
Epoch 610, training loss: 13.053427696228027 = 0.42980048060417175 + 2.0 * 6.311813831329346
Epoch 610, val loss: 0.8226181268692017
Epoch 620, training loss: 13.033166885375977 = 0.413831502199173 + 2.0 * 6.309667587280273
Epoch 620, val loss: 0.8191913366317749
Epoch 630, training loss: 13.01757526397705 = 0.39828765392303467 + 2.0 * 6.309643745422363
Epoch 630, val loss: 0.8161893486976624
Epoch 640, training loss: 12.999114036560059 = 0.38319894671440125 + 2.0 * 6.307957649230957
Epoch 640, val loss: 0.8136531114578247
Epoch 650, training loss: 12.982922554016113 = 0.36857232451438904 + 2.0 * 6.307175159454346
Epoch 650, val loss: 0.8116316795349121
Epoch 660, training loss: 12.963980674743652 = 0.3543635308742523 + 2.0 * 6.304808616638184
Epoch 660, val loss: 0.8099992871284485
Epoch 670, training loss: 12.96410083770752 = 0.34056347608566284 + 2.0 * 6.311768531799316
Epoch 670, val loss: 0.8086336255073547
Epoch 680, training loss: 12.930843353271484 = 0.32718050479888916 + 2.0 * 6.301831245422363
Epoch 680, val loss: 0.8075910806655884
Epoch 690, training loss: 12.916717529296875 = 0.3141520321369171 + 2.0 * 6.30128288269043
Epoch 690, val loss: 0.8068950772285461
Epoch 700, training loss: 12.904025077819824 = 0.301471084356308 + 2.0 * 6.301277160644531
Epoch 700, val loss: 0.8063479661941528
Epoch 710, training loss: 12.889416694641113 = 0.28916412591934204 + 2.0 * 6.300126075744629
Epoch 710, val loss: 0.8060631155967712
Epoch 720, training loss: 12.875384330749512 = 0.27721741795539856 + 2.0 * 6.299083232879639
Epoch 720, val loss: 0.8060826063156128
Epoch 730, training loss: 12.859334945678711 = 0.26570072770118713 + 2.0 * 6.296817302703857
Epoch 730, val loss: 0.806390643119812
Epoch 740, training loss: 12.844538688659668 = 0.25449660420417786 + 2.0 * 6.295021057128906
Epoch 740, val loss: 0.8068906664848328
Epoch 750, training loss: 12.848731994628906 = 0.24365325272083282 + 2.0 * 6.302539348602295
Epoch 750, val loss: 0.8076264262199402
Epoch 760, training loss: 12.825979232788086 = 0.23321953415870667 + 2.0 * 6.296380043029785
Epoch 760, val loss: 0.8084957003593445
Epoch 770, training loss: 12.810192108154297 = 0.22321517765522003 + 2.0 * 6.293488502502441
Epoch 770, val loss: 0.809807300567627
Epoch 780, training loss: 12.805319786071777 = 0.2136039286851883 + 2.0 * 6.295857906341553
Epoch 780, val loss: 0.811252772808075
Epoch 790, training loss: 12.793179512023926 = 0.20440667867660522 + 2.0 * 6.294386386871338
Epoch 790, val loss: 0.8129340410232544
Epoch 800, training loss: 12.776825904846191 = 0.19565169513225555 + 2.0 * 6.290586948394775
Epoch 800, val loss: 0.8150231838226318
Epoch 810, training loss: 12.765161514282227 = 0.18724985420703888 + 2.0 * 6.2889556884765625
Epoch 810, val loss: 0.8172497153282166
Epoch 820, training loss: 12.77070140838623 = 0.17923647165298462 + 2.0 * 6.295732498168945
Epoch 820, val loss: 0.8197290301322937
Epoch 830, training loss: 12.750362396240234 = 0.17160621285438538 + 2.0 * 6.2893781661987305
Epoch 830, val loss: 0.8224171996116638
Epoch 840, training loss: 12.73868465423584 = 0.16433085501194 + 2.0 * 6.287177085876465
Epoch 840, val loss: 0.8254380226135254
Epoch 850, training loss: 12.73974323272705 = 0.15741631388664246 + 2.0 * 6.291163444519043
Epoch 850, val loss: 0.8285512328147888
Epoch 860, training loss: 12.72193717956543 = 0.15082406997680664 + 2.0 * 6.285556316375732
Epoch 860, val loss: 0.8319007754325867
Epoch 870, training loss: 12.713375091552734 = 0.14457426965236664 + 2.0 * 6.284400463104248
Epoch 870, val loss: 0.8355361223220825
Epoch 880, training loss: 12.718572616577148 = 0.13861246407032013 + 2.0 * 6.289979934692383
Epoch 880, val loss: 0.8391942977905273
Epoch 890, training loss: 12.699752807617188 = 0.132967010140419 + 2.0 * 6.283392906188965
Epoch 890, val loss: 0.8430880308151245
Epoch 900, training loss: 12.690568923950195 = 0.12759019434452057 + 2.0 * 6.281489372253418
Epoch 900, val loss: 0.8472154140472412
Epoch 910, training loss: 12.709198951721191 = 0.12245364487171173 + 2.0 * 6.293372631072998
Epoch 910, val loss: 0.8513497114181519
Epoch 920, training loss: 12.680107116699219 = 0.1175917237997055 + 2.0 * 6.281257629394531
Epoch 920, val loss: 0.8556101322174072
Epoch 930, training loss: 12.674220085144043 = 0.11296074837446213 + 2.0 * 6.280629634857178
Epoch 930, val loss: 0.8600208163261414
Epoch 940, training loss: 12.66626262664795 = 0.10853492468595505 + 2.0 * 6.278863906860352
Epoch 940, val loss: 0.8644803762435913
Epoch 950, training loss: 12.672222137451172 = 0.1043156087398529 + 2.0 * 6.2839531898498535
Epoch 950, val loss: 0.8688288927078247
Epoch 960, training loss: 12.66286563873291 = 0.10030542314052582 + 2.0 * 6.281280040740967
Epoch 960, val loss: 0.8735106587409973
Epoch 970, training loss: 12.649744033813477 = 0.0964680016040802 + 2.0 * 6.276638031005859
Epoch 970, val loss: 0.8780906796455383
Epoch 980, training loss: 12.644546508789062 = 0.09278910607099533 + 2.0 * 6.27587890625
Epoch 980, val loss: 0.8825984001159668
Epoch 990, training loss: 12.641989707946777 = 0.08926267176866531 + 2.0 * 6.276363372802734
Epoch 990, val loss: 0.887248694896698
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7777777777777778
0.8392198207696363
=== training gcn model ===
Epoch 0, training loss: 19.13837242126465 = 1.9447673559188843 + 2.0 * 8.596802711486816
Epoch 0, val loss: 1.947905421257019
Epoch 10, training loss: 19.12755584716797 = 1.9349814653396606 + 2.0 * 8.59628677368164
Epoch 10, val loss: 1.937719464302063
Epoch 20, training loss: 19.107467651367188 = 1.9227418899536133 + 2.0 * 8.592362403869629
Epoch 20, val loss: 1.9250361919403076
Epoch 30, training loss: 19.032581329345703 = 1.9061464071273804 + 2.0 * 8.563217163085938
Epoch 30, val loss: 1.908158779144287
Epoch 40, training loss: 18.616199493408203 = 1.8867777585983276 + 2.0 * 8.364710807800293
Epoch 40, val loss: 1.8892461061477661
Epoch 50, training loss: 17.52576446533203 = 1.865058183670044 + 2.0 * 7.830353260040283
Epoch 50, val loss: 1.8685195446014404
Epoch 60, training loss: 16.68736457824707 = 1.84919011592865 + 2.0 * 7.4190874099731445
Epoch 60, val loss: 1.8548610210418701
Epoch 70, training loss: 16.032686233520508 = 1.839477777481079 + 2.0 * 7.096604347229004
Epoch 70, val loss: 1.8460510969161987
Epoch 80, training loss: 15.707489013671875 = 1.8282724618911743 + 2.0 * 6.939608097076416
Epoch 80, val loss: 1.835401177406311
Epoch 90, training loss: 15.491868019104004 = 1.8142117261886597 + 2.0 * 6.838828086853027
Epoch 90, val loss: 1.8228613138198853
Epoch 100, training loss: 15.343695640563965 = 1.8003524541854858 + 2.0 * 6.771671772003174
Epoch 100, val loss: 1.8104952573776245
Epoch 110, training loss: 15.214065551757812 = 1.7868483066558838 + 2.0 * 6.713608741760254
Epoch 110, val loss: 1.7983933687210083
Epoch 120, training loss: 15.114501953125 = 1.7737685441970825 + 2.0 * 6.6703667640686035
Epoch 120, val loss: 1.7863966226577759
Epoch 130, training loss: 15.025257110595703 = 1.7604289054870605 + 2.0 * 6.6324143409729
Epoch 130, val loss: 1.7741121053695679
Epoch 140, training loss: 14.945204734802246 = 1.7465547323226929 + 2.0 * 6.599325180053711
Epoch 140, val loss: 1.7616651058197021
Epoch 150, training loss: 14.87153434753418 = 1.7316806316375732 + 2.0 * 6.569926738739014
Epoch 150, val loss: 1.7485731840133667
Epoch 160, training loss: 14.808863639831543 = 1.715098261833191 + 2.0 * 6.546882629394531
Epoch 160, val loss: 1.7344777584075928
Epoch 170, training loss: 14.7508544921875 = 1.6964703798294067 + 2.0 * 6.527192115783691
Epoch 170, val loss: 1.718998670578003
Epoch 180, training loss: 14.6964693069458 = 1.6754854917526245 + 2.0 * 6.510491847991943
Epoch 180, val loss: 1.7017816305160522
Epoch 190, training loss: 14.645928382873535 = 1.6517677307128906 + 2.0 * 6.497080326080322
Epoch 190, val loss: 1.6824144124984741
Epoch 200, training loss: 14.593184471130371 = 1.6249967813491821 + 2.0 * 6.48409366607666
Epoch 200, val loss: 1.660715103149414
Epoch 210, training loss: 14.54403018951416 = 1.5949831008911133 + 2.0 * 6.474523544311523
Epoch 210, val loss: 1.6363303661346436
Epoch 220, training loss: 14.487151145935059 = 1.5618584156036377 + 2.0 * 6.462646484375
Epoch 220, val loss: 1.6094086170196533
Epoch 230, training loss: 14.431097984313965 = 1.5254138708114624 + 2.0 * 6.4528422355651855
Epoch 230, val loss: 1.5798399448394775
Epoch 240, training loss: 14.380889892578125 = 1.485839605331421 + 2.0 * 6.4475250244140625
Epoch 240, val loss: 1.5477986335754395
Epoch 250, training loss: 14.318074226379395 = 1.444326639175415 + 2.0 * 6.436873912811279
Epoch 250, val loss: 1.514222502708435
Epoch 260, training loss: 14.257935523986816 = 1.4009780883789062 + 2.0 * 6.428478717803955
Epoch 260, val loss: 1.4793070554733276
Epoch 270, training loss: 14.203023910522461 = 1.3563238382339478 + 2.0 * 6.423349857330322
Epoch 270, val loss: 1.4434925317764282
Epoch 280, training loss: 14.146553039550781 = 1.3117070198059082 + 2.0 * 6.417423248291016
Epoch 280, val loss: 1.4080263376235962
Epoch 290, training loss: 14.08742618560791 = 1.2677571773529053 + 2.0 * 6.409834384918213
Epoch 290, val loss: 1.373440146446228
Epoch 300, training loss: 14.03370475769043 = 1.2245591878890991 + 2.0 * 6.4045729637146
Epoch 300, val loss: 1.33995521068573
Epoch 310, training loss: 13.988415718078613 = 1.182787537574768 + 2.0 * 6.402813911437988
Epoch 310, val loss: 1.3078515529632568
Epoch 320, training loss: 13.935099601745605 = 1.1429558992385864 + 2.0 * 6.396071910858154
Epoch 320, val loss: 1.2777262926101685
Epoch 330, training loss: 13.885517120361328 = 1.1047089099884033 + 2.0 * 6.390404224395752
Epoch 330, val loss: 1.2493221759796143
Epoch 340, training loss: 13.84028434753418 = 1.068122386932373 + 2.0 * 6.386081218719482
Epoch 340, val loss: 1.22255539894104
Epoch 350, training loss: 13.796985626220703 = 1.0332790613174438 + 2.0 * 6.381853103637695
Epoch 350, val loss: 1.1974339485168457
Epoch 360, training loss: 13.758625984191895 = 1.0000782012939453 + 2.0 * 6.379273891448975
Epoch 360, val loss: 1.1739368438720703
Epoch 370, training loss: 13.71688461303711 = 0.9685273170471191 + 2.0 * 6.374178409576416
Epoch 370, val loss: 1.1518912315368652
Epoch 380, training loss: 13.680964469909668 = 0.9383321404457092 + 2.0 * 6.371315956115723
Epoch 380, val loss: 1.1312005519866943
Epoch 390, training loss: 13.645872116088867 = 0.9093498587608337 + 2.0 * 6.368261337280273
Epoch 390, val loss: 1.1117745637893677
Epoch 400, training loss: 13.60972785949707 = 0.8815969824790955 + 2.0 * 6.364065647125244
Epoch 400, val loss: 1.0934337377548218
Epoch 410, training loss: 13.576628684997559 = 0.8547259569168091 + 2.0 * 6.3609514236450195
Epoch 410, val loss: 1.0759223699569702
Epoch 420, training loss: 13.555929183959961 = 0.8286001086235046 + 2.0 * 6.363664627075195
Epoch 420, val loss: 1.0589723587036133
Epoch 430, training loss: 13.512813568115234 = 0.8030910491943359 + 2.0 * 6.354861259460449
Epoch 430, val loss: 1.0426156520843506
Epoch 440, training loss: 13.48208236694336 = 0.7780545949935913 + 2.0 * 6.352014064788818
Epoch 440, val loss: 1.0266499519348145
Epoch 450, training loss: 13.464713096618652 = 0.7532399892807007 + 2.0 * 6.35573673248291
Epoch 450, val loss: 1.0107446908950806
Epoch 460, training loss: 13.425459861755371 = 0.728554368019104 + 2.0 * 6.348452568054199
Epoch 460, val loss: 0.9949079751968384
Epoch 470, training loss: 13.395655632019043 = 0.7040119171142578 + 2.0 * 6.345821857452393
Epoch 470, val loss: 0.9789724946022034
Epoch 480, training loss: 13.36912727355957 = 0.679452121257782 + 2.0 * 6.344837665557861
Epoch 480, val loss: 0.9628012180328369
Epoch 490, training loss: 13.34110164642334 = 0.6548290252685547 + 2.0 * 6.343136310577393
Epoch 490, val loss: 0.9465004801750183
Epoch 500, training loss: 13.306886672973633 = 0.6302573680877686 + 2.0 * 6.338314533233643
Epoch 500, val loss: 0.9299618601799011
Epoch 510, training loss: 13.278003692626953 = 0.6055852770805359 + 2.0 * 6.336209297180176
Epoch 510, val loss: 0.9132529497146606
Epoch 520, training loss: 13.254980087280273 = 0.5809761881828308 + 2.0 * 6.337001800537109
Epoch 520, val loss: 0.8963726758956909
Epoch 530, training loss: 13.223955154418945 = 0.5566760897636414 + 2.0 * 6.333639621734619
Epoch 530, val loss: 0.8796284794807434
Epoch 540, training loss: 13.1943359375 = 0.532642126083374 + 2.0 * 6.330846786499023
Epoch 540, val loss: 0.8631161451339722
Epoch 550, training loss: 13.17140007019043 = 0.5089040398597717 + 2.0 * 6.331247806549072
Epoch 550, val loss: 0.8468931317329407
Epoch 560, training loss: 13.152118682861328 = 0.48573964834213257 + 2.0 * 6.333189487457275
Epoch 560, val loss: 0.8310604095458984
Epoch 570, training loss: 13.116825103759766 = 0.4632165729999542 + 2.0 * 6.326804161071777
Epoch 570, val loss: 0.8160950541496277
Epoch 580, training loss: 13.089897155761719 = 0.4413740336894989 + 2.0 * 6.324261665344238
Epoch 580, val loss: 0.8019393086433411
Epoch 590, training loss: 13.06789779663086 = 0.4201694428920746 + 2.0 * 6.323863983154297
Epoch 590, val loss: 0.7886294722557068
Epoch 600, training loss: 13.043682098388672 = 0.399665892124176 + 2.0 * 6.32200813293457
Epoch 600, val loss: 0.7762318849563599
Epoch 610, training loss: 13.02046012878418 = 0.37997981905937195 + 2.0 * 6.320240020751953
Epoch 610, val loss: 0.7647814154624939
Epoch 620, training loss: 12.999642372131348 = 0.3610316216945648 + 2.0 * 6.319305419921875
Epoch 620, val loss: 0.7543485760688782
Epoch 630, training loss: 12.992205619812012 = 0.34284481406211853 + 2.0 * 6.324680328369141
Epoch 630, val loss: 0.7448819279670715
Epoch 640, training loss: 12.959476470947266 = 0.3254581391811371 + 2.0 * 6.317008972167969
Epoch 640, val loss: 0.7363570332527161
Epoch 650, training loss: 12.937347412109375 = 0.3088015913963318 + 2.0 * 6.314272880554199
Epoch 650, val loss: 0.7288268208503723
Epoch 660, training loss: 12.917366027832031 = 0.29277023673057556 + 2.0 * 6.312297821044922
Epoch 660, val loss: 0.7220896482467651
Epoch 670, training loss: 12.913511276245117 = 0.2773451805114746 + 2.0 * 6.318082809448242
Epoch 670, val loss: 0.7161526679992676
Epoch 680, training loss: 12.890536308288574 = 0.2627120912075043 + 2.0 * 6.3139119148254395
Epoch 680, val loss: 0.710975706577301
Epoch 690, training loss: 12.868159294128418 = 0.2488355040550232 + 2.0 * 6.309661865234375
Epoch 690, val loss: 0.7067072987556458
Epoch 700, training loss: 12.850529670715332 = 0.23563449084758759 + 2.0 * 6.30744743347168
Epoch 700, val loss: 0.7031615972518921
Epoch 710, training loss: 12.856100082397461 = 0.22313174605369568 + 2.0 * 6.316483974456787
Epoch 710, val loss: 0.7003161907196045
Epoch 720, training loss: 12.824645042419434 = 0.21134909987449646 + 2.0 * 6.306647777557373
Epoch 720, val loss: 0.6980383992195129
Epoch 730, training loss: 12.810539245605469 = 0.2002735286951065 + 2.0 * 6.305132865905762
Epoch 730, val loss: 0.6965171098709106
Epoch 740, training loss: 12.794903755187988 = 0.18984192609786987 + 2.0 * 6.302530765533447
Epoch 740, val loss: 0.6955291628837585
Epoch 750, training loss: 12.812728881835938 = 0.18002860248088837 + 2.0 * 6.316349983215332
Epoch 750, val loss: 0.6950217485427856
Epoch 760, training loss: 12.772883415222168 = 0.17087122797966003 + 2.0 * 6.301006317138672
Epoch 760, val loss: 0.6949367523193359
Epoch 770, training loss: 12.760523796081543 = 0.16230525076389313 + 2.0 * 6.29910945892334
Epoch 770, val loss: 0.6954156756401062
Epoch 780, training loss: 12.750528335571289 = 0.1542671173810959 + 2.0 * 6.298130512237549
Epoch 780, val loss: 0.6962739825248718
Epoch 790, training loss: 12.740689277648926 = 0.14670220017433167 + 2.0 * 6.296993732452393
Epoch 790, val loss: 0.6974869966506958
Epoch 800, training loss: 12.73703670501709 = 0.13960467278957367 + 2.0 * 6.298716068267822
Epoch 800, val loss: 0.6988649368286133
Epoch 810, training loss: 12.728524208068848 = 0.1330094039440155 + 2.0 * 6.297757625579834
Epoch 810, val loss: 0.7005419731140137
Epoch 820, training loss: 12.717617988586426 = 0.1268230527639389 + 2.0 * 6.2953972816467285
Epoch 820, val loss: 0.7026031017303467
Epoch 830, training loss: 12.70663070678711 = 0.12100058794021606 + 2.0 * 6.292815208435059
Epoch 830, val loss: 0.7048521637916565
Epoch 840, training loss: 12.69892692565918 = 0.11550259590148926 + 2.0 * 6.291712284088135
Epoch 840, val loss: 0.7073245644569397
Epoch 850, training loss: 12.710185050964355 = 0.11031954735517502 + 2.0 * 6.299932956695557
Epoch 850, val loss: 0.7099765539169312
Epoch 860, training loss: 12.687100410461426 = 0.10544250905513763 + 2.0 * 6.290829181671143
Epoch 860, val loss: 0.7127211689949036
Epoch 870, training loss: 12.688150405883789 = 0.10086197406053543 + 2.0 * 6.293644428253174
Epoch 870, val loss: 0.71570885181427
Epoch 880, training loss: 12.676416397094727 = 0.09654873609542847 + 2.0 * 6.289933681488037
Epoch 880, val loss: 0.7188170552253723
Epoch 890, training loss: 12.668580055236816 = 0.09247878193855286 + 2.0 * 6.288050651550293
Epoch 890, val loss: 0.7220104932785034
Epoch 900, training loss: 12.66091537475586 = 0.08863426744937897 + 2.0 * 6.286140441894531
Epoch 900, val loss: 0.725318193435669
Epoch 910, training loss: 12.659533500671387 = 0.08500418812036514 + 2.0 * 6.287264823913574
Epoch 910, val loss: 0.7287315130233765
Epoch 920, training loss: 12.65695571899414 = 0.08156272768974304 + 2.0 * 6.287696361541748
Epoch 920, val loss: 0.7321012020111084
Epoch 930, training loss: 12.646605491638184 = 0.07831831276416779 + 2.0 * 6.284143447875977
Epoch 930, val loss: 0.7356078028678894
Epoch 940, training loss: 12.647841453552246 = 0.07525692135095596 + 2.0 * 6.28629207611084
Epoch 940, val loss: 0.7391262650489807
Epoch 950, training loss: 12.639047622680664 = 0.07235097885131836 + 2.0 * 6.283348560333252
Epoch 950, val loss: 0.7427406311035156
Epoch 960, training loss: 12.640850067138672 = 0.0696026086807251 + 2.0 * 6.285623550415039
Epoch 960, val loss: 0.7463074922561646
Epoch 970, training loss: 12.63080883026123 = 0.06699658930301666 + 2.0 * 6.2819061279296875
Epoch 970, val loss: 0.7499169707298279
Epoch 980, training loss: 12.623740196228027 = 0.06452976912260056 + 2.0 * 6.279605388641357
Epoch 980, val loss: 0.7535871267318726
Epoch 990, training loss: 12.619669914245605 = 0.06218459829688072 + 2.0 * 6.278742790222168
Epoch 990, val loss: 0.7572924494743347
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7962962962962963
0.8365840801265156
=== training gcn model ===
Epoch 0, training loss: 19.149585723876953 = 1.9559592008590698 + 2.0 * 8.596813201904297
Epoch 0, val loss: 1.9490141868591309
Epoch 10, training loss: 19.137922286987305 = 1.9450193643569946 + 2.0 * 8.596451759338379
Epoch 10, val loss: 1.9389729499816895
Epoch 20, training loss: 19.11933708190918 = 1.9315258264541626 + 2.0 * 8.593905448913574
Epoch 20, val loss: 1.9262117147445679
Epoch 30, training loss: 19.066547393798828 = 1.9133553504943848 + 2.0 * 8.5765962600708
Epoch 30, val loss: 1.9088324308395386
Epoch 40, training loss: 18.849185943603516 = 1.8908411264419556 + 2.0 * 8.479172706604004
Epoch 40, val loss: 1.8879848718643188
Epoch 50, training loss: 18.031448364257812 = 1.8670412302017212 + 2.0 * 8.08220386505127
Epoch 50, val loss: 1.8657170534133911
Epoch 60, training loss: 17.17195701599121 = 1.8469716310501099 + 2.0 * 7.662492752075195
Epoch 60, val loss: 1.8471119403839111
Epoch 70, training loss: 16.40874481201172 = 1.8324918746948242 + 2.0 * 7.288125991821289
Epoch 70, val loss: 1.8333357572555542
Epoch 80, training loss: 15.987327575683594 = 1.8191924095153809 + 2.0 * 7.0840678215026855
Epoch 80, val loss: 1.8208143711090088
Epoch 90, training loss: 15.686699867248535 = 1.8044966459274292 + 2.0 * 6.941101551055908
Epoch 90, val loss: 1.8074804544448853
Epoch 100, training loss: 15.474386215209961 = 1.789096713066101 + 2.0 * 6.842644691467285
Epoch 100, val loss: 1.793519377708435
Epoch 110, training loss: 15.319634437561035 = 1.7743093967437744 + 2.0 * 6.77266263961792
Epoch 110, val loss: 1.7794923782348633
Epoch 120, training loss: 15.20936107635498 = 1.7597923278808594 + 2.0 * 6.7247843742370605
Epoch 120, val loss: 1.7659378051757812
Epoch 130, training loss: 15.10700511932373 = 1.745896339416504 + 2.0 * 6.680554389953613
Epoch 130, val loss: 1.7530043125152588
Epoch 140, training loss: 15.012955665588379 = 1.7318003177642822 + 2.0 * 6.640577793121338
Epoch 140, val loss: 1.7399674654006958
Epoch 150, training loss: 14.932819366455078 = 1.7164981365203857 + 2.0 * 6.608160495758057
Epoch 150, val loss: 1.7258871793746948
Epoch 160, training loss: 14.86223030090332 = 1.6994227170944214 + 2.0 * 6.581403732299805
Epoch 160, val loss: 1.7104685306549072
Epoch 170, training loss: 14.802083969116211 = 1.6800932884216309 + 2.0 * 6.560995578765869
Epoch 170, val loss: 1.693242073059082
Epoch 180, training loss: 14.738999366760254 = 1.6584030389785767 + 2.0 * 6.540297985076904
Epoch 180, val loss: 1.6742515563964844
Epoch 190, training loss: 14.681551933288574 = 1.6340194940567017 + 2.0 * 6.523766040802002
Epoch 190, val loss: 1.6529107093811035
Epoch 200, training loss: 14.623147964477539 = 1.6067256927490234 + 2.0 * 6.508211135864258
Epoch 200, val loss: 1.6292192935943604
Epoch 210, training loss: 14.5665864944458 = 1.576132893562317 + 2.0 * 6.495226860046387
Epoch 210, val loss: 1.6029176712036133
Epoch 220, training loss: 14.513021469116211 = 1.5423568487167358 + 2.0 * 6.485332489013672
Epoch 220, val loss: 1.5741755962371826
Epoch 230, training loss: 14.453171730041504 = 1.5060513019561768 + 2.0 * 6.473560333251953
Epoch 230, val loss: 1.5432778596878052
Epoch 240, training loss: 14.390789985656738 = 1.4673272371292114 + 2.0 * 6.461731433868408
Epoch 240, val loss: 1.5106974840164185
Epoch 250, training loss: 14.33222484588623 = 1.4262841939926147 + 2.0 * 6.452970504760742
Epoch 250, val loss: 1.4765452146530151
Epoch 260, training loss: 14.273581504821777 = 1.383334755897522 + 2.0 * 6.445123195648193
Epoch 260, val loss: 1.4411187171936035
Epoch 270, training loss: 14.219862937927246 = 1.3395805358886719 + 2.0 * 6.440141201019287
Epoch 270, val loss: 1.405398964881897
Epoch 280, training loss: 14.158281326293945 = 1.296021819114685 + 2.0 * 6.4311299324035645
Epoch 280, val loss: 1.3702044486999512
Epoch 290, training loss: 14.101663589477539 = 1.252720594406128 + 2.0 * 6.424471378326416
Epoch 290, val loss: 1.3357404470443726
Epoch 300, training loss: 14.052447319030762 = 1.2101118564605713 + 2.0 * 6.421167850494385
Epoch 300, val loss: 1.3022539615631104
Epoch 310, training loss: 13.995614051818848 = 1.1692215204238892 + 2.0 * 6.413196086883545
Epoch 310, val loss: 1.2703720331192017
Epoch 320, training loss: 13.950886726379395 = 1.130042314529419 + 2.0 * 6.410422325134277
Epoch 320, val loss: 1.240182876586914
Epoch 330, training loss: 13.900681495666504 = 1.092549204826355 + 2.0 * 6.40406608581543
Epoch 330, val loss: 1.2117178440093994
Epoch 340, training loss: 13.854207992553711 = 1.0569698810577393 + 2.0 * 6.398619174957275
Epoch 340, val loss: 1.184739112854004
Epoch 350, training loss: 13.816301345825195 = 1.0229099988937378 + 2.0 * 6.396695613861084
Epoch 350, val loss: 1.159097671508789
Epoch 360, training loss: 13.772759437561035 = 0.9906726479530334 + 2.0 * 6.391043186187744
Epoch 360, val loss: 1.1346791982650757
Epoch 370, training loss: 13.732831954956055 = 0.9602457880973816 + 2.0 * 6.386292934417725
Epoch 370, val loss: 1.1116032600402832
Epoch 380, training loss: 13.695015907287598 = 0.9312623143196106 + 2.0 * 6.3818769454956055
Epoch 380, val loss: 1.0894993543624878
Epoch 390, training loss: 13.668561935424805 = 0.9036701917648315 + 2.0 * 6.382445812225342
Epoch 390, val loss: 1.0681644678115845
Epoch 400, training loss: 13.630087852478027 = 0.8775608539581299 + 2.0 * 6.376263618469238
Epoch 400, val loss: 1.0479577779769897
Epoch 410, training loss: 13.596600532531738 = 0.8528738021850586 + 2.0 * 6.37186336517334
Epoch 410, val loss: 1.0287330150604248
Epoch 420, training loss: 13.565374374389648 = 0.8292673826217651 + 2.0 * 6.368053436279297
Epoch 420, val loss: 1.0102919340133667
Epoch 430, training loss: 13.53671646118164 = 0.8066554665565491 + 2.0 * 6.365030288696289
Epoch 430, val loss: 0.9926326274871826
Epoch 440, training loss: 13.528594970703125 = 0.7851153612136841 + 2.0 * 6.371739864349365
Epoch 440, val loss: 0.9758006930351257
Epoch 450, training loss: 13.488282203674316 = 0.7648065686225891 + 2.0 * 6.3617377281188965
Epoch 450, val loss: 0.9602012634277344
Epoch 460, training loss: 13.459389686584473 = 0.7456624507904053 + 2.0 * 6.356863498687744
Epoch 460, val loss: 0.9456574320793152
Epoch 470, training loss: 13.436253547668457 = 0.7272856831550598 + 2.0 * 6.3544840812683105
Epoch 470, val loss: 0.9319400191307068
Epoch 480, training loss: 13.412718772888184 = 0.7095339298248291 + 2.0 * 6.351592540740967
Epoch 480, val loss: 0.9189589619636536
Epoch 490, training loss: 13.411803245544434 = 0.6923074126243591 + 2.0 * 6.359747886657715
Epoch 490, val loss: 0.9065788388252258
Epoch 500, training loss: 13.37242317199707 = 0.6757643222808838 + 2.0 * 6.348329544067383
Epoch 500, val loss: 0.894930362701416
Epoch 510, training loss: 13.349324226379395 = 0.6595801115036011 + 2.0 * 6.344871997833252
Epoch 510, val loss: 0.8839244246482849
Epoch 520, training loss: 13.328963279724121 = 0.643551766872406 + 2.0 * 6.342705726623535
Epoch 520, val loss: 0.8732273578643799
Epoch 530, training loss: 13.330049514770508 = 0.6275776624679565 + 2.0 * 6.351235866546631
Epoch 530, val loss: 0.8627207279205322
Epoch 540, training loss: 13.290166854858398 = 0.6114745736122131 + 2.0 * 6.339345932006836
Epoch 540, val loss: 0.852526843547821
Epoch 550, training loss: 13.269186973571777 = 0.5952093005180359 + 2.0 * 6.336988925933838
Epoch 550, val loss: 0.8424859046936035
Epoch 560, training loss: 13.255182266235352 = 0.5785647630691528 + 2.0 * 6.338308811187744
Epoch 560, val loss: 0.8323805928230286
Epoch 570, training loss: 13.227760314941406 = 0.5615841746330261 + 2.0 * 6.333087921142578
Epoch 570, val loss: 0.8223448395729065
Epoch 580, training loss: 13.207497596740723 = 0.5440346002578735 + 2.0 * 6.33173131942749
Epoch 580, val loss: 0.8123022317886353
Epoch 590, training loss: 13.185470581054688 = 0.525968074798584 + 2.0 * 6.329751491546631
Epoch 590, val loss: 0.8022128343582153
Epoch 600, training loss: 13.165384292602539 = 0.5074907541275024 + 2.0 * 6.328946590423584
Epoch 600, val loss: 0.7923396825790405
Epoch 610, training loss: 13.142193794250488 = 0.488603413105011 + 2.0 * 6.3267951011657715
Epoch 610, val loss: 0.7826621532440186
Epoch 620, training loss: 13.118592262268066 = 0.46941983699798584 + 2.0 * 6.324586391448975
Epoch 620, val loss: 0.7732699513435364
Epoch 630, training loss: 13.107537269592285 = 0.4501716196537018 + 2.0 * 6.328682899475098
Epoch 630, val loss: 0.7643225789070129
Epoch 640, training loss: 13.084569931030273 = 0.4312135875225067 + 2.0 * 6.326678276062012
Epoch 640, val loss: 0.7558354139328003
Epoch 650, training loss: 13.054047584533691 = 0.41261374950408936 + 2.0 * 6.320716857910156
Epoch 650, val loss: 0.7482413649559021
Epoch 660, training loss: 13.034256935119629 = 0.39444226026535034 + 2.0 * 6.319907188415527
Epoch 660, val loss: 0.7413507103919983
Epoch 670, training loss: 13.015149116516113 = 0.37673065066337585 + 2.0 * 6.319209098815918
Epoch 670, val loss: 0.7351704835891724
Epoch 680, training loss: 12.99499225616455 = 0.35961493849754333 + 2.0 * 6.317688465118408
Epoch 680, val loss: 0.7296106219291687
Epoch 690, training loss: 12.978343963623047 = 0.34324610233306885 + 2.0 * 6.317548751831055
Epoch 690, val loss: 0.7247943878173828
Epoch 700, training loss: 12.95693302154541 = 0.32752490043640137 + 2.0 * 6.314703941345215
Epoch 700, val loss: 0.7206445336341858
Epoch 710, training loss: 12.944421768188477 = 0.3124372065067291 + 2.0 * 6.31599235534668
Epoch 710, val loss: 0.7171543836593628
Epoch 720, training loss: 12.93343734741211 = 0.2980373501777649 + 2.0 * 6.317699909210205
Epoch 720, val loss: 0.7142229080200195
Epoch 730, training loss: 12.906181335449219 = 0.2843523919582367 + 2.0 * 6.310914516448975
Epoch 730, val loss: 0.711851954460144
Epoch 740, training loss: 12.892165184020996 = 0.2713078558444977 + 2.0 * 6.310428619384766
Epoch 740, val loss: 0.7100650072097778
Epoch 750, training loss: 12.8843412399292 = 0.25881749391555786 + 2.0 * 6.3127617835998535
Epoch 750, val loss: 0.7087594270706177
Epoch 760, training loss: 12.867081642150879 = 0.24697808921337128 + 2.0 * 6.310051918029785
Epoch 760, val loss: 0.7078577876091003
Epoch 770, training loss: 12.856062889099121 = 0.2356550693511963 + 2.0 * 6.310204029083252
Epoch 770, val loss: 0.7072795629501343
Epoch 780, training loss: 12.835478782653809 = 0.22492387890815735 + 2.0 * 6.305277347564697
Epoch 780, val loss: 0.7070885896682739
Epoch 790, training loss: 12.8233003616333 = 0.21467679738998413 + 2.0 * 6.304311752319336
Epoch 790, val loss: 0.707277774810791
Epoch 800, training loss: 12.812882423400879 = 0.2048952728509903 + 2.0 * 6.3039937019348145
Epoch 800, val loss: 0.7078261971473694
Epoch 810, training loss: 12.80713939666748 = 0.19560053944587708 + 2.0 * 6.305769443511963
Epoch 810, val loss: 0.7086243629455566
Epoch 820, training loss: 12.79912281036377 = 0.1867421716451645 + 2.0 * 6.306190490722656
Epoch 820, val loss: 0.7095906734466553
Epoch 830, training loss: 12.77934455871582 = 0.1784057468175888 + 2.0 * 6.300469398498535
Epoch 830, val loss: 0.7109209895133972
Epoch 840, training loss: 12.769953727722168 = 0.170451357960701 + 2.0 * 6.299751281738281
Epoch 840, val loss: 0.7124888300895691
Epoch 850, training loss: 12.764355659484863 = 0.16287395358085632 + 2.0 * 6.300740718841553
Epoch 850, val loss: 0.7142987847328186
Epoch 860, training loss: 12.75317096710205 = 0.1556476354598999 + 2.0 * 6.29876184463501
Epoch 860, val loss: 0.7162127494812012
Epoch 870, training loss: 12.747910499572754 = 0.14880508184432983 + 2.0 * 6.299552917480469
Epoch 870, val loss: 0.7183331251144409
Epoch 880, training loss: 12.733941078186035 = 0.14232955873012543 + 2.0 * 6.295805931091309
Epoch 880, val loss: 0.7206498980522156
Epoch 890, training loss: 12.727745056152344 = 0.13616181910037994 + 2.0 * 6.2957916259765625
Epoch 890, val loss: 0.7231875658035278
Epoch 900, training loss: 12.718232154846191 = 0.1303379237651825 + 2.0 * 6.293947219848633
Epoch 900, val loss: 0.7259141802787781
Epoch 910, training loss: 12.713808059692383 = 0.12480755895376205 + 2.0 * 6.294500350952148
Epoch 910, val loss: 0.7286574244499207
Epoch 920, training loss: 12.70186996459961 = 0.11957679688930511 + 2.0 * 6.291146755218506
Epoch 920, val loss: 0.7316733002662659
Epoch 930, training loss: 12.701545715332031 = 0.11459290236234665 + 2.0 * 6.293476581573486
Epoch 930, val loss: 0.7348200678825378
Epoch 940, training loss: 12.692793846130371 = 0.10987747460603714 + 2.0 * 6.2914581298828125
Epoch 940, val loss: 0.7380136847496033
Epoch 950, training loss: 12.688179969787598 = 0.1054045632481575 + 2.0 * 6.291387557983398
Epoch 950, val loss: 0.7412049770355225
Epoch 960, training loss: 12.677267074584961 = 0.10118461400270462 + 2.0 * 6.288041114807129
Epoch 960, val loss: 0.7446333765983582
Epoch 970, training loss: 12.675128936767578 = 0.09715762734413147 + 2.0 * 6.288985729217529
Epoch 970, val loss: 0.7481498718261719
Epoch 980, training loss: 12.666779518127441 = 0.09332515299320221 + 2.0 * 6.286726951599121
Epoch 980, val loss: 0.7516446709632874
Epoch 990, training loss: 12.666533470153809 = 0.08968686312437057 + 2.0 * 6.288423538208008
Epoch 990, val loss: 0.7552409768104553
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8074074074074075
0.8413284132841329
The final CL Acc:0.79383, 0.01222, The final GNN Acc:0.83904, 0.00194
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.5 der2:0.3 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11610])
remove edge: torch.Size([2, 9504])
updated graph: torch.Size([2, 10558])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 19.129837036132812 = 1.936110258102417 + 2.0 * 8.596863746643066
Epoch 0, val loss: 1.929127812385559
Epoch 10, training loss: 19.119943618774414 = 1.9266051054000854 + 2.0 * 8.59666919708252
Epoch 10, val loss: 1.9191765785217285
Epoch 20, training loss: 19.104999542236328 = 1.9145392179489136 + 2.0 * 8.595230102539062
Epoch 20, val loss: 1.9064446687698364
Epoch 30, training loss: 19.064163208007812 = 1.8973720073699951 + 2.0 * 8.583395957946777
Epoch 30, val loss: 1.8884177207946777
Epoch 40, training loss: 18.879817962646484 = 1.8747295141220093 + 2.0 * 8.502544403076172
Epoch 40, val loss: 1.8654446601867676
Epoch 50, training loss: 17.98761558532715 = 1.8518939018249512 + 2.0 * 8.06786060333252
Epoch 50, val loss: 1.8430790901184082
Epoch 60, training loss: 16.959640502929688 = 1.8349018096923828 + 2.0 * 7.5623698234558105
Epoch 60, val loss: 1.8280143737792969
Epoch 70, training loss: 16.2877254486084 = 1.8218201398849487 + 2.0 * 7.23295259475708
Epoch 70, val loss: 1.816047191619873
Epoch 80, training loss: 15.957003593444824 = 1.8078711032867432 + 2.0 * 7.07456636428833
Epoch 80, val loss: 1.80295729637146
Epoch 90, training loss: 15.71054744720459 = 1.7935655117034912 + 2.0 * 6.95849084854126
Epoch 90, val loss: 1.7901079654693604
Epoch 100, training loss: 15.514656066894531 = 1.7813079357147217 + 2.0 * 6.866673946380615
Epoch 100, val loss: 1.7798914909362793
Epoch 110, training loss: 15.367795944213867 = 1.7696119546890259 + 2.0 * 6.799091815948486
Epoch 110, val loss: 1.7701947689056396
Epoch 120, training loss: 15.247186660766602 = 1.7571206092834473 + 2.0 * 6.745033264160156
Epoch 120, val loss: 1.7596369981765747
Epoch 130, training loss: 15.150104522705078 = 1.7437998056411743 + 2.0 * 6.703152179718018
Epoch 130, val loss: 1.7485175132751465
Epoch 140, training loss: 15.061119079589844 = 1.7293415069580078 + 2.0 * 6.665888786315918
Epoch 140, val loss: 1.736465573310852
Epoch 150, training loss: 14.99295711517334 = 1.7131563425064087 + 2.0 * 6.639900207519531
Epoch 150, val loss: 1.7229704856872559
Epoch 160, training loss: 14.919511795043945 = 1.6948705911636353 + 2.0 * 6.612320423126221
Epoch 160, val loss: 1.7077220678329468
Epoch 170, training loss: 14.853961944580078 = 1.6743896007537842 + 2.0 * 6.589786052703857
Epoch 170, val loss: 1.690677285194397
Epoch 180, training loss: 14.79507064819336 = 1.6514314413070679 + 2.0 * 6.57181978225708
Epoch 180, val loss: 1.6715818643569946
Epoch 190, training loss: 14.743454933166504 = 1.626008152961731 + 2.0 * 6.558723449707031
Epoch 190, val loss: 1.6505151987075806
Epoch 200, training loss: 14.682421684265137 = 1.598250389099121 + 2.0 * 6.542085647583008
Epoch 200, val loss: 1.6277401447296143
Epoch 210, training loss: 14.624011993408203 = 1.567873477935791 + 2.0 * 6.528069019317627
Epoch 210, val loss: 1.6026960611343384
Epoch 220, training loss: 14.566153526306152 = 1.534692406654358 + 2.0 * 6.515730381011963
Epoch 220, val loss: 1.5754984617233276
Epoch 230, training loss: 14.50741958618164 = 1.4985904693603516 + 2.0 * 6.5044145584106445
Epoch 230, val loss: 1.5461087226867676
Epoch 240, training loss: 14.44851016998291 = 1.4595956802368164 + 2.0 * 6.494457244873047
Epoch 240, val loss: 1.5146582126617432
Epoch 250, training loss: 14.406386375427246 = 1.4184430837631226 + 2.0 * 6.493971824645996
Epoch 250, val loss: 1.482144832611084
Epoch 260, training loss: 14.334259033203125 = 1.37629234790802 + 2.0 * 6.478983402252197
Epoch 260, val loss: 1.4490280151367188
Epoch 270, training loss: 14.272222518920898 = 1.3327116966247559 + 2.0 * 6.469755172729492
Epoch 270, val loss: 1.4155644178390503
Epoch 280, training loss: 14.21316146850586 = 1.2882407903671265 + 2.0 * 6.462460517883301
Epoch 280, val loss: 1.3818933963775635
Epoch 290, training loss: 14.15362548828125 = 1.2427022457122803 + 2.0 * 6.455461502075195
Epoch 290, val loss: 1.3480195999145508
Epoch 300, training loss: 14.10291862487793 = 1.1966367959976196 + 2.0 * 6.453140735626221
Epoch 300, val loss: 1.3145513534545898
Epoch 310, training loss: 14.043057441711426 = 1.1512813568115234 + 2.0 * 6.445888042449951
Epoch 310, val loss: 1.2823266983032227
Epoch 320, training loss: 13.983263969421387 = 1.1066278219223022 + 2.0 * 6.438318252563477
Epoch 320, val loss: 1.2512050867080688
Epoch 330, training loss: 13.92831039428711 = 1.062415599822998 + 2.0 * 6.432947158813477
Epoch 330, val loss: 1.2211267948150635
Epoch 340, training loss: 13.895914077758789 = 1.0190423727035522 + 2.0 * 6.438436031341553
Epoch 340, val loss: 1.1921615600585938
Epoch 350, training loss: 13.826300621032715 = 0.9768387675285339 + 2.0 * 6.4247307777404785
Epoch 350, val loss: 1.1648095846176147
Epoch 360, training loss: 13.773584365844727 = 0.935945987701416 + 2.0 * 6.418818950653076
Epoch 360, val loss: 1.1389647722244263
Epoch 370, training loss: 13.724288940429688 = 0.8963381052017212 + 2.0 * 6.413975238800049
Epoch 370, val loss: 1.1145919561386108
Epoch 380, training loss: 13.67943000793457 = 0.8580844402313232 + 2.0 * 6.410672664642334
Epoch 380, val loss: 1.0917736291885376
Epoch 390, training loss: 13.645272254943848 = 0.821858823299408 + 2.0 * 6.411706924438477
Epoch 390, val loss: 1.0708931684494019
Epoch 400, training loss: 13.594500541687012 = 0.7875888347625732 + 2.0 * 6.40345573425293
Epoch 400, val loss: 1.0519375801086426
Epoch 410, training loss: 13.553340911865234 = 0.7550179362297058 + 2.0 * 6.399161338806152
Epoch 410, val loss: 1.0347483158111572
Epoch 420, training loss: 13.513693809509277 = 0.7239712476730347 + 2.0 * 6.394861221313477
Epoch 420, val loss: 1.0191341638565063
Epoch 430, training loss: 13.482924461364746 = 0.6943815350532532 + 2.0 * 6.394271373748779
Epoch 430, val loss: 1.0049313306808472
Epoch 440, training loss: 13.449315071105957 = 0.6663779616355896 + 2.0 * 6.391468524932861
Epoch 440, val loss: 0.9924201369285583
Epoch 450, training loss: 13.413567543029785 = 0.6401938796043396 + 2.0 * 6.3866868019104
Epoch 450, val loss: 0.9815168380737305
Epoch 460, training loss: 13.381603240966797 = 0.6151720881462097 + 2.0 * 6.383215427398682
Epoch 460, val loss: 0.9718714952468872
Epoch 470, training loss: 13.351215362548828 = 0.5912049412727356 + 2.0 * 6.380005359649658
Epoch 470, val loss: 0.9634263515472412
Epoch 480, training loss: 13.321419715881348 = 0.5680814385414124 + 2.0 * 6.376668930053711
Epoch 480, val loss: 0.9560614824295044
Epoch 490, training loss: 13.293584823608398 = 0.5457718372344971 + 2.0 * 6.37390661239624
Epoch 490, val loss: 0.949714720249176
Epoch 500, training loss: 13.294084548950195 = 0.5243672132492065 + 2.0 * 6.38485860824585
Epoch 500, val loss: 0.9442047476768494
Epoch 510, training loss: 13.249547958374023 = 0.5037710070610046 + 2.0 * 6.372888565063477
Epoch 510, val loss: 0.9397076368331909
Epoch 520, training loss: 13.217635154724121 = 0.48400506377220154 + 2.0 * 6.366815090179443
Epoch 520, val loss: 0.9361110925674438
Epoch 530, training loss: 13.192737579345703 = 0.4648818075656891 + 2.0 * 6.363927841186523
Epoch 530, val loss: 0.9332259297370911
Epoch 540, training loss: 13.189260482788086 = 0.4463518261909485 + 2.0 * 6.371454238891602
Epoch 540, val loss: 0.9310787916183472
Epoch 550, training loss: 13.152480125427246 = 0.4284913241863251 + 2.0 * 6.36199426651001
Epoch 550, val loss: 0.9294586181640625
Epoch 560, training loss: 13.128744125366211 = 0.4111945331096649 + 2.0 * 6.358774662017822
Epoch 560, val loss: 0.9284823536872864
Epoch 570, training loss: 13.107951164245605 = 0.3944574296474457 + 2.0 * 6.356746673583984
Epoch 570, val loss: 0.9280848503112793
Epoch 580, training loss: 13.089077949523926 = 0.3782496154308319 + 2.0 * 6.355414390563965
Epoch 580, val loss: 0.9282251596450806
Epoch 590, training loss: 13.065540313720703 = 0.3625889718532562 + 2.0 * 6.351475715637207
Epoch 590, val loss: 0.9288609027862549
Epoch 600, training loss: 13.050661087036133 = 0.3473948836326599 + 2.0 * 6.351633071899414
Epoch 600, val loss: 0.929929256439209
Epoch 610, training loss: 13.03061294555664 = 0.33269286155700684 + 2.0 * 6.348959922790527
Epoch 610, val loss: 0.9314305186271667
Epoch 620, training loss: 13.01967716217041 = 0.3185191750526428 + 2.0 * 6.350578784942627
Epoch 620, val loss: 0.9332671165466309
Epoch 630, training loss: 12.99654483795166 = 0.3048587739467621 + 2.0 * 6.3458428382873535
Epoch 630, val loss: 0.9357079863548279
Epoch 640, training loss: 12.976813316345215 = 0.29174381494522095 + 2.0 * 6.34253454208374
Epoch 640, val loss: 0.9384117126464844
Epoch 650, training loss: 12.960079193115234 = 0.27904465794563293 + 2.0 * 6.340517044067383
Epoch 650, val loss: 0.9414617419242859
Epoch 660, training loss: 12.962489128112793 = 0.266792893409729 + 2.0 * 6.347847938537598
Epoch 660, val loss: 0.9448462724685669
Epoch 670, training loss: 12.933490753173828 = 0.25498059391975403 + 2.0 * 6.339254856109619
Epoch 670, val loss: 0.9486645460128784
Epoch 680, training loss: 12.917648315429688 = 0.2436474710702896 + 2.0 * 6.337000370025635
Epoch 680, val loss: 0.9527307748794556
Epoch 690, training loss: 12.903730392456055 = 0.23276424407958984 + 2.0 * 6.335483074188232
Epoch 690, val loss: 0.9570708274841309
Epoch 700, training loss: 12.904919624328613 = 0.22232121229171753 + 2.0 * 6.341299057006836
Epoch 700, val loss: 0.9616802930831909
Epoch 710, training loss: 12.881684303283691 = 0.21227911114692688 + 2.0 * 6.334702491760254
Epoch 710, val loss: 0.9666186571121216
Epoch 720, training loss: 12.864422798156738 = 0.20269839465618134 + 2.0 * 6.330862045288086
Epoch 720, val loss: 0.9717481732368469
Epoch 730, training loss: 12.852201461791992 = 0.19350016117095947 + 2.0 * 6.329350471496582
Epoch 730, val loss: 0.9770848155021667
Epoch 740, training loss: 12.856167793273926 = 0.18466816842556 + 2.0 * 6.335749626159668
Epoch 740, val loss: 0.9826738238334656
Epoch 750, training loss: 12.831708908081055 = 0.1763031780719757 + 2.0 * 6.32770299911499
Epoch 750, val loss: 0.9885465502738953
Epoch 760, training loss: 12.81830883026123 = 0.1682523488998413 + 2.0 * 6.325028419494629
Epoch 760, val loss: 0.9944973587989807
Epoch 770, training loss: 12.808849334716797 = 0.16059379279613495 + 2.0 * 6.324127674102783
Epoch 770, val loss: 1.000771403312683
Epoch 780, training loss: 12.819515228271484 = 0.15326587855815887 + 2.0 * 6.33312463760376
Epoch 780, val loss: 1.007017731666565
Epoch 790, training loss: 12.79002571105957 = 0.14629054069519043 + 2.0 * 6.3218674659729
Epoch 790, val loss: 1.0135681629180908
Epoch 800, training loss: 12.7818603515625 = 0.1396753191947937 + 2.0 * 6.32109260559082
Epoch 800, val loss: 1.0201505422592163
Epoch 810, training loss: 12.771985054016113 = 0.13335716724395752 + 2.0 * 6.319314002990723
Epoch 810, val loss: 1.0268844366073608
Epoch 820, training loss: 12.781039237976074 = 0.1273350864648819 + 2.0 * 6.326851844787598
Epoch 820, val loss: 1.033606767654419
Epoch 830, training loss: 12.757803916931152 = 0.12163317948579788 + 2.0 * 6.318085193634033
Epoch 830, val loss: 1.040568232536316
Epoch 840, training loss: 12.752108573913574 = 0.11621694266796112 + 2.0 * 6.317945957183838
Epoch 840, val loss: 1.0474456548690796
Epoch 850, training loss: 12.742263793945312 = 0.11106961220502853 + 2.0 * 6.315597057342529
Epoch 850, val loss: 1.0544697046279907
Epoch 860, training loss: 12.742244720458984 = 0.1061750054359436 + 2.0 * 6.318034648895264
Epoch 860, val loss: 1.0615217685699463
Epoch 870, training loss: 12.729076385498047 = 0.10154883563518524 + 2.0 * 6.313763618469238
Epoch 870, val loss: 1.0686925649642944
Epoch 880, training loss: 12.725447654724121 = 0.09714042395353317 + 2.0 * 6.314153671264648
Epoch 880, val loss: 1.0757508277893066
Epoch 890, training loss: 12.718057632446289 = 0.09298686683177948 + 2.0 * 6.312535285949707
Epoch 890, val loss: 1.0828657150268555
Epoch 900, training loss: 12.711527824401855 = 0.08903390914201736 + 2.0 * 6.311246871948242
Epoch 900, val loss: 1.0901154279708862
Epoch 910, training loss: 12.703779220581055 = 0.08528921008110046 + 2.0 * 6.3092451095581055
Epoch 910, val loss: 1.097243309020996
Epoch 920, training loss: 12.700687408447266 = 0.08172964304685593 + 2.0 * 6.309478759765625
Epoch 920, val loss: 1.104377269744873
Epoch 930, training loss: 12.70295524597168 = 0.07835489511489868 + 2.0 * 6.312300205230713
Epoch 930, val loss: 1.1113455295562744
Epoch 940, training loss: 12.688092231750488 = 0.07516328990459442 + 2.0 * 6.306464672088623
Epoch 940, val loss: 1.1184337139129639
Epoch 950, training loss: 12.684175491333008 = 0.07214006036520004 + 2.0 * 6.306017875671387
Epoch 950, val loss: 1.1254234313964844
Epoch 960, training loss: 12.682072639465332 = 0.0692659392952919 + 2.0 * 6.306403160095215
Epoch 960, val loss: 1.1324177980422974
Epoch 970, training loss: 12.680371284484863 = 0.06653924286365509 + 2.0 * 6.306916236877441
Epoch 970, val loss: 1.139129400253296
Epoch 980, training loss: 12.670284271240234 = 0.06393962353467941 + 2.0 * 6.3031721115112305
Epoch 980, val loss: 1.1459763050079346
Epoch 990, training loss: 12.667304992675781 = 0.06148293614387512 + 2.0 * 6.302910804748535
Epoch 990, val loss: 1.1526870727539062
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7444444444444445
0.8181338956246705
=== training gcn model ===
Epoch 0, training loss: 19.126361846923828 = 1.9326684474945068 + 2.0 * 8.596846580505371
Epoch 0, val loss: 1.927927851676941
Epoch 10, training loss: 19.115591049194336 = 1.9224966764450073 + 2.0 * 8.59654712677002
Epoch 10, val loss: 1.918269395828247
Epoch 20, training loss: 19.0980167388916 = 1.9091142416000366 + 2.0 * 8.594450950622559
Epoch 20, val loss: 1.9051642417907715
Epoch 30, training loss: 19.05006980895996 = 1.890012502670288 + 2.0 * 8.580028533935547
Epoch 30, val loss: 1.8861465454101562
Epoch 40, training loss: 18.872295379638672 = 1.8663358688354492 + 2.0 * 8.50298023223877
Epoch 40, val loss: 1.8633147478103638
Epoch 50, training loss: 18.220579147338867 = 1.8434181213378906 + 2.0 * 8.188580513000488
Epoch 50, val loss: 1.8412683010101318
Epoch 60, training loss: 17.339826583862305 = 1.8264002799987793 + 2.0 * 7.756713390350342
Epoch 60, val loss: 1.825223445892334
Epoch 70, training loss: 16.330902099609375 = 1.8150830268859863 + 2.0 * 7.257909774780273
Epoch 70, val loss: 1.814340591430664
Epoch 80, training loss: 15.886552810668945 = 1.8057334423065186 + 2.0 * 7.040409564971924
Epoch 80, val loss: 1.805299162864685
Epoch 90, training loss: 15.65322494506836 = 1.7921605110168457 + 2.0 * 6.930532455444336
Epoch 90, val loss: 1.792858600616455
Epoch 100, training loss: 15.461562156677246 = 1.7769593000411987 + 2.0 * 6.842301368713379
Epoch 100, val loss: 1.779565691947937
Epoch 110, training loss: 15.317947387695312 = 1.762565016746521 + 2.0 * 6.77769136428833
Epoch 110, val loss: 1.7667816877365112
Epoch 120, training loss: 15.209543228149414 = 1.7480316162109375 + 2.0 * 6.730755805969238
Epoch 120, val loss: 1.7533869743347168
Epoch 130, training loss: 15.114729881286621 = 1.7316950559616089 + 2.0 * 6.691517353057861
Epoch 130, val loss: 1.7383686304092407
Epoch 140, training loss: 15.028864860534668 = 1.7132337093353271 + 2.0 * 6.657815456390381
Epoch 140, val loss: 1.7216099500656128
Epoch 150, training loss: 14.954374313354492 = 1.692716121673584 + 2.0 * 6.630828857421875
Epoch 150, val loss: 1.703212022781372
Epoch 160, training loss: 14.87918758392334 = 1.6699326038360596 + 2.0 * 6.60462760925293
Epoch 160, val loss: 1.6829837560653687
Epoch 170, training loss: 14.810514450073242 = 1.64473557472229 + 2.0 * 6.582889556884766
Epoch 170, val loss: 1.660740852355957
Epoch 180, training loss: 14.747004508972168 = 1.6171351671218872 + 2.0 * 6.564934730529785
Epoch 180, val loss: 1.6365808248519897
Epoch 190, training loss: 14.677897453308105 = 1.587260127067566 + 2.0 * 6.545318603515625
Epoch 190, val loss: 1.6106847524642944
Epoch 200, training loss: 14.619345664978027 = 1.5552412271499634 + 2.0 * 6.532052040100098
Epoch 200, val loss: 1.5831331014633179
Epoch 210, training loss: 14.559109687805176 = 1.521548867225647 + 2.0 * 6.51878023147583
Epoch 210, val loss: 1.5544804334640503
Epoch 220, training loss: 14.497212409973145 = 1.4863885641098022 + 2.0 * 6.5054121017456055
Epoch 220, val loss: 1.5250238180160522
Epoch 230, training loss: 14.4385986328125 = 1.450103759765625 + 2.0 * 6.4942474365234375
Epoch 230, val loss: 1.4950886964797974
Epoch 240, training loss: 14.384800910949707 = 1.4132070541381836 + 2.0 * 6.485796928405762
Epoch 240, val loss: 1.465219497680664
Epoch 250, training loss: 14.325272560119629 = 1.3760815858840942 + 2.0 * 6.474595546722412
Epoch 250, val loss: 1.4357810020446777
Epoch 260, training loss: 14.273862838745117 = 1.3389525413513184 + 2.0 * 6.4674553871154785
Epoch 260, val loss: 1.406909704208374
Epoch 270, training loss: 14.216212272644043 = 1.3022361993789673 + 2.0 * 6.4569878578186035
Epoch 270, val loss: 1.3789870738983154
Epoch 280, training loss: 14.175802230834961 = 1.2659584283828735 + 2.0 * 6.454921722412109
Epoch 280, val loss: 1.3520323038101196
Epoch 290, training loss: 14.122664451599121 = 1.2304266691207886 + 2.0 * 6.4461188316345215
Epoch 290, val loss: 1.3261500597000122
Epoch 300, training loss: 14.070472717285156 = 1.1957086324691772 + 2.0 * 6.437382221221924
Epoch 300, val loss: 1.301296353340149
Epoch 310, training loss: 14.021682739257812 = 1.1617246866226196 + 2.0 * 6.429978847503662
Epoch 310, val loss: 1.2775630950927734
Epoch 320, training loss: 13.97514820098877 = 1.1283947229385376 + 2.0 * 6.423376560211182
Epoch 320, val loss: 1.254675030708313
Epoch 330, training loss: 13.946385383605957 = 1.0958499908447266 + 2.0 * 6.425267696380615
Epoch 330, val loss: 1.2327948808670044
Epoch 340, training loss: 13.894777297973633 = 1.0643529891967773 + 2.0 * 6.415212154388428
Epoch 340, val loss: 1.2119359970092773
Epoch 350, training loss: 13.850776672363281 = 1.0336947441101074 + 2.0 * 6.408540725708008
Epoch 350, val loss: 1.1920757293701172
Epoch 360, training loss: 13.816568374633789 = 1.003763198852539 + 2.0 * 6.406402587890625
Epoch 360, val loss: 1.1731326580047607
Epoch 370, training loss: 13.781970977783203 = 0.9748230576515198 + 2.0 * 6.403573989868164
Epoch 370, val loss: 1.154768466949463
Epoch 380, training loss: 13.741796493530273 = 0.9467384219169617 + 2.0 * 6.397529125213623
Epoch 380, val loss: 1.1373083591461182
Epoch 390, training loss: 13.703949928283691 = 0.9191135764122009 + 2.0 * 6.392418384552002
Epoch 390, val loss: 1.1203265190124512
Epoch 400, training loss: 13.668804168701172 = 0.8917463421821594 + 2.0 * 6.388528823852539
Epoch 400, val loss: 1.103577733039856
Epoch 410, training loss: 13.657032012939453 = 0.8644880056381226 + 2.0 * 6.3962721824646
Epoch 410, val loss: 1.0869323015213013
Epoch 420, training loss: 13.607475280761719 = 0.8375081419944763 + 2.0 * 6.384983539581299
Epoch 420, val loss: 1.0706785917282104
Epoch 430, training loss: 13.570404052734375 = 0.8105170130729675 + 2.0 * 6.379943370819092
Epoch 430, val loss: 1.0543946027755737
Epoch 440, training loss: 13.539874076843262 = 0.7834174036979675 + 2.0 * 6.378228187561035
Epoch 440, val loss: 1.0381739139556885
Epoch 450, training loss: 13.514816284179688 = 0.7562401294708252 + 2.0 * 6.379288196563721
Epoch 450, val loss: 1.0221898555755615
Epoch 460, training loss: 13.47430419921875 = 0.7290977835655212 + 2.0 * 6.372603416442871
Epoch 460, val loss: 1.0063201189041138
Epoch 470, training loss: 13.439202308654785 = 0.7019946575164795 + 2.0 * 6.368603706359863
Epoch 470, val loss: 0.9906842112541199
Epoch 480, training loss: 13.409887313842773 = 0.6749702095985413 + 2.0 * 6.367458343505859
Epoch 480, val loss: 0.9753854274749756
Epoch 490, training loss: 13.374907493591309 = 0.6481755375862122 + 2.0 * 6.36336612701416
Epoch 490, val loss: 0.9604380130767822
Epoch 500, training loss: 13.368173599243164 = 0.6218694448471069 + 2.0 * 6.373152256011963
Epoch 500, val loss: 0.9462683796882629
Epoch 510, training loss: 13.316099166870117 = 0.596166729927063 + 2.0 * 6.359966278076172
Epoch 510, val loss: 0.9327651262283325
Epoch 520, training loss: 13.284416198730469 = 0.571107804775238 + 2.0 * 6.356654167175293
Epoch 520, val loss: 0.9201840758323669
Epoch 530, training loss: 13.255183219909668 = 0.5466635823249817 + 2.0 * 6.354259967803955
Epoch 530, val loss: 0.9084954857826233
Epoch 540, training loss: 13.228375434875488 = 0.5228351354598999 + 2.0 * 6.3527703285217285
Epoch 540, val loss: 0.8976179361343384
Epoch 550, training loss: 13.20691204071045 = 0.49967554211616516 + 2.0 * 6.353618144989014
Epoch 550, val loss: 0.8877089619636536
Epoch 560, training loss: 13.17878532409668 = 0.4772616922855377 + 2.0 * 6.350761890411377
Epoch 560, val loss: 0.8786118030548096
Epoch 570, training loss: 13.148756980895996 = 0.4555634558200836 + 2.0 * 6.346596717834473
Epoch 570, val loss: 0.8705304861068726
Epoch 580, training loss: 13.137787818908691 = 0.4345360994338989 + 2.0 * 6.351625919342041
Epoch 580, val loss: 0.8632884621620178
Epoch 590, training loss: 13.110286712646484 = 0.41427308320999146 + 2.0 * 6.348006725311279
Epoch 590, val loss: 0.8569134473800659
Epoch 600, training loss: 13.082036018371582 = 0.39472126960754395 + 2.0 * 6.343657493591309
Epoch 600, val loss: 0.8514280915260315
Epoch 610, training loss: 13.06029987335205 = 0.3758619725704193 + 2.0 * 6.34221887588501
Epoch 610, val loss: 0.8468068838119507
Epoch 620, training loss: 13.033449172973633 = 0.35768765211105347 + 2.0 * 6.337880611419678
Epoch 620, val loss: 0.8430063724517822
Epoch 630, training loss: 13.017528533935547 = 0.3401818573474884 + 2.0 * 6.338673114776611
Epoch 630, val loss: 0.8399413228034973
Epoch 640, training loss: 13.015427589416504 = 0.32335972785949707 + 2.0 * 6.346034049987793
Epoch 640, val loss: 0.8375051617622375
Epoch 650, training loss: 12.979782104492188 = 0.3073466420173645 + 2.0 * 6.336217880249023
Epoch 650, val loss: 0.8359497785568237
Epoch 660, training loss: 12.958606719970703 = 0.2920718193054199 + 2.0 * 6.3332672119140625
Epoch 660, val loss: 0.8352454304695129
Epoch 670, training loss: 12.938279151916504 = 0.2774943709373474 + 2.0 * 6.330392360687256
Epoch 670, val loss: 0.8351932168006897
Epoch 680, training loss: 12.927372932434082 = 0.2635965347290039 + 2.0 * 6.331888198852539
Epoch 680, val loss: 0.8357716798782349
Epoch 690, training loss: 12.90959358215332 = 0.25037461519241333 + 2.0 * 6.329609394073486
Epoch 690, val loss: 0.837141752243042
Epoch 700, training loss: 12.893322944641113 = 0.2379191815853119 + 2.0 * 6.327702045440674
Epoch 700, val loss: 0.8388999700546265
Epoch 710, training loss: 12.876093864440918 = 0.22610895335674286 + 2.0 * 6.324992656707764
Epoch 710, val loss: 0.8413687944412231
Epoch 720, training loss: 12.863187789916992 = 0.21490997076034546 + 2.0 * 6.32413911819458
Epoch 720, val loss: 0.8444289565086365
Epoch 730, training loss: 12.864944458007812 = 0.20433518290519714 + 2.0 * 6.3303046226501465
Epoch 730, val loss: 0.8479412794113159
Epoch 740, training loss: 12.841346740722656 = 0.19432471692562103 + 2.0 * 6.323511123657227
Epoch 740, val loss: 0.8517619967460632
Epoch 750, training loss: 12.824986457824707 = 0.18488150835037231 + 2.0 * 6.320052623748779
Epoch 750, val loss: 0.8561974763870239
Epoch 760, training loss: 12.814776420593262 = 0.1759655475616455 + 2.0 * 6.319405555725098
Epoch 760, val loss: 0.8610211610794067
Epoch 770, training loss: 12.81788444519043 = 0.1675272285938263 + 2.0 * 6.325178623199463
Epoch 770, val loss: 0.8660475611686707
Epoch 780, training loss: 12.796566009521484 = 0.15957365930080414 + 2.0 * 6.318496227264404
Epoch 780, val loss: 0.8713580965995789
Epoch 790, training loss: 12.787968635559082 = 0.15204380452632904 + 2.0 * 6.317962646484375
Epoch 790, val loss: 0.8769480586051941
Epoch 800, training loss: 12.778522491455078 = 0.1449517160654068 + 2.0 * 6.3167853355407715
Epoch 800, val loss: 0.8828922510147095
Epoch 810, training loss: 12.766408920288086 = 0.13824589550495148 + 2.0 * 6.31408166885376
Epoch 810, val loss: 0.88872230052948
Epoch 820, training loss: 12.756933212280273 = 0.13192220032215118 + 2.0 * 6.312505722045898
Epoch 820, val loss: 0.894978940486908
Epoch 830, training loss: 12.754084587097168 = 0.12592853605747223 + 2.0 * 6.314077854156494
Epoch 830, val loss: 0.9011881947517395
Epoch 840, training loss: 12.749821662902832 = 0.12027337402105331 + 2.0 * 6.314774036407471
Epoch 840, val loss: 0.9077096581459045
Epoch 850, training loss: 12.735103607177734 = 0.11491185426712036 + 2.0 * 6.31009578704834
Epoch 850, val loss: 0.9139858484268188
Epoch 860, training loss: 12.726024627685547 = 0.10985457897186279 + 2.0 * 6.308084964752197
Epoch 860, val loss: 0.9207197427749634
Epoch 870, training loss: 12.718398094177246 = 0.10505130887031555 + 2.0 * 6.306673526763916
Epoch 870, val loss: 0.9273037314414978
Epoch 880, training loss: 12.720366477966309 = 0.1004994586110115 + 2.0 * 6.309933662414551
Epoch 880, val loss: 0.9341167211532593
Epoch 890, training loss: 12.715999603271484 = 0.09618642181158066 + 2.0 * 6.309906482696533
Epoch 890, val loss: 0.9408745765686035
Epoch 900, training loss: 12.704809188842773 = 0.09210490435361862 + 2.0 * 6.306352138519287
Epoch 900, val loss: 0.9476808905601501
Epoch 910, training loss: 12.696996688842773 = 0.08824285119771957 + 2.0 * 6.30437707901001
Epoch 910, val loss: 0.9545483589172363
Epoch 920, training loss: 12.691947937011719 = 0.08458184450864792 + 2.0 * 6.303683280944824
Epoch 920, val loss: 0.9613153338432312
Epoch 930, training loss: 12.690906524658203 = 0.08111333847045898 + 2.0 * 6.304896354675293
Epoch 930, val loss: 0.968254029750824
Epoch 940, training loss: 12.679215431213379 = 0.07781001180410385 + 2.0 * 6.3007025718688965
Epoch 940, val loss: 0.9749591946601868
Epoch 950, training loss: 12.677367210388184 = 0.07468568533658981 + 2.0 * 6.301340579986572
Epoch 950, val loss: 0.9817753434181213
Epoch 960, training loss: 12.670028686523438 = 0.07171521335840225 + 2.0 * 6.299156665802002
Epoch 960, val loss: 0.9885957837104797
Epoch 970, training loss: 12.663362503051758 = 0.06889406591653824 + 2.0 * 6.297234058380127
Epoch 970, val loss: 0.9954115152359009
Epoch 980, training loss: 12.667612075805664 = 0.06621462851762772 + 2.0 * 6.300698757171631
Epoch 980, val loss: 1.0022313594818115
Epoch 990, training loss: 12.657210350036621 = 0.06365951895713806 + 2.0 * 6.2967753410339355
Epoch 990, val loss: 1.0089561939239502
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7444444444444445
0.8249868212967845
=== training gcn model ===
Epoch 0, training loss: 19.158845901489258 = 1.965146541595459 + 2.0 * 8.59684944152832
Epoch 0, val loss: 1.974043369293213
Epoch 10, training loss: 19.148122787475586 = 1.9549272060394287 + 2.0 * 8.596597671508789
Epoch 10, val loss: 1.963837742805481
Epoch 20, training loss: 19.13135528564453 = 1.9421066045761108 + 2.0 * 8.594624519348145
Epoch 20, val loss: 1.95114004611969
Epoch 30, training loss: 19.082176208496094 = 1.924179196357727 + 2.0 * 8.578998565673828
Epoch 30, val loss: 1.9335544109344482
Epoch 40, training loss: 18.8587589263916 = 1.9013689756393433 + 2.0 * 8.478694915771484
Epoch 40, val loss: 1.9117789268493652
Epoch 50, training loss: 18.181859970092773 = 1.8777709007263184 + 2.0 * 8.152044296264648
Epoch 50, val loss: 1.888936996459961
Epoch 60, training loss: 17.736148834228516 = 1.85616135597229 + 2.0 * 7.939993381500244
Epoch 60, val loss: 1.8675462007522583
Epoch 70, training loss: 17.16746711730957 = 1.8373324871063232 + 2.0 * 7.665067672729492
Epoch 70, val loss: 1.8497748374938965
Epoch 80, training loss: 16.315645217895508 = 1.827805519104004 + 2.0 * 7.243919849395752
Epoch 80, val loss: 1.8416688442230225
Epoch 90, training loss: 15.893978118896484 = 1.81761634349823 + 2.0 * 7.038180828094482
Epoch 90, val loss: 1.8317031860351562
Epoch 100, training loss: 15.583837509155273 = 1.8007402420043945 + 2.0 * 6.8915486335754395
Epoch 100, val loss: 1.8163983821868896
Epoch 110, training loss: 15.396323204040527 = 1.7845866680145264 + 2.0 * 6.805868148803711
Epoch 110, val loss: 1.8017739057540894
Epoch 120, training loss: 15.249700546264648 = 1.7702099084854126 + 2.0 * 6.739745140075684
Epoch 120, val loss: 1.7880362272262573
Epoch 130, training loss: 15.141532897949219 = 1.7552330493927002 + 2.0 * 6.693150043487549
Epoch 130, val loss: 1.7736742496490479
Epoch 140, training loss: 15.051185607910156 = 1.7391934394836426 + 2.0 * 6.655995845794678
Epoch 140, val loss: 1.7586910724639893
Epoch 150, training loss: 14.968361854553223 = 1.722399353981018 + 2.0 * 6.622981071472168
Epoch 150, val loss: 1.7434040307998657
Epoch 160, training loss: 14.89786148071289 = 1.704323410987854 + 2.0 * 6.596768856048584
Epoch 160, val loss: 1.727362871170044
Epoch 170, training loss: 14.847997665405273 = 1.6845440864562988 + 2.0 * 6.581726551055908
Epoch 170, val loss: 1.7102235555648804
Epoch 180, training loss: 14.780824661254883 = 1.6631414890289307 + 2.0 * 6.558841705322266
Epoch 180, val loss: 1.691946268081665
Epoch 190, training loss: 14.72194766998291 = 1.6397699117660522 + 2.0 * 6.541089057922363
Epoch 190, val loss: 1.6720471382141113
Epoch 200, training loss: 14.667644500732422 = 1.6142843961715698 + 2.0 * 6.526679992675781
Epoch 200, val loss: 1.6504307985305786
Epoch 210, training loss: 14.613917350769043 = 1.5866262912750244 + 2.0 * 6.513645648956299
Epoch 210, val loss: 1.6269313097000122
Epoch 220, training loss: 14.561412811279297 = 1.5568747520446777 + 2.0 * 6.5022687911987305
Epoch 220, val loss: 1.6017050743103027
Epoch 230, training loss: 14.513276100158691 = 1.5255916118621826 + 2.0 * 6.493842124938965
Epoch 230, val loss: 1.5752376317977905
Epoch 240, training loss: 14.461075782775879 = 1.4935715198516846 + 2.0 * 6.483752250671387
Epoch 240, val loss: 1.548203706741333
Epoch 250, training loss: 14.410154342651367 = 1.4609465599060059 + 2.0 * 6.474603652954102
Epoch 250, val loss: 1.520957350730896
Epoch 260, training loss: 14.36070728302002 = 1.4282578229904175 + 2.0 * 6.466224670410156
Epoch 260, val loss: 1.493886113166809
Epoch 270, training loss: 14.309030532836914 = 1.3961106538772583 + 2.0 * 6.456459999084473
Epoch 270, val loss: 1.4677026271820068
Epoch 280, training loss: 14.26259708404541 = 1.3643896579742432 + 2.0 * 6.449103832244873
Epoch 280, val loss: 1.442305564880371
Epoch 290, training loss: 14.238654136657715 = 1.333223581314087 + 2.0 * 6.4527153968811035
Epoch 290, val loss: 1.4177660942077637
Epoch 300, training loss: 14.18057632446289 = 1.3029918670654297 + 2.0 * 6.4387922286987305
Epoch 300, val loss: 1.3944844007492065
Epoch 310, training loss: 14.134411811828613 = 1.2734651565551758 + 2.0 * 6.430473327636719
Epoch 310, val loss: 1.3721704483032227
Epoch 320, training loss: 14.093111991882324 = 1.244292974472046 + 2.0 * 6.42440938949585
Epoch 320, val loss: 1.3506115674972534
Epoch 330, training loss: 14.056451797485352 = 1.2152785062789917 + 2.0 * 6.420586585998535
Epoch 330, val loss: 1.3294950723648071
Epoch 340, training loss: 14.02873420715332 = 1.1863118410110474 + 2.0 * 6.421211242675781
Epoch 340, val loss: 1.3090128898620605
Epoch 350, training loss: 13.979926109313965 = 1.1574288606643677 + 2.0 * 6.411248683929443
Epoch 350, val loss: 1.2886465787887573
Epoch 360, training loss: 13.939018249511719 = 1.1282230615615845 + 2.0 * 6.405397415161133
Epoch 360, val loss: 1.2683889865875244
Epoch 370, training loss: 13.900665283203125 = 1.0985140800476074 + 2.0 * 6.401075839996338
Epoch 370, val loss: 1.24805748462677
Epoch 380, training loss: 13.865540504455566 = 1.0682604312896729 + 2.0 * 6.398640155792236
Epoch 380, val loss: 1.2275344133377075
Epoch 390, training loss: 13.826498031616211 = 1.0375813245773315 + 2.0 * 6.394458293914795
Epoch 390, val loss: 1.2068275213241577
Epoch 400, training loss: 13.787235260009766 = 1.0064799785614014 + 2.0 * 6.390377521514893
Epoch 400, val loss: 1.1860525608062744
Epoch 410, training loss: 13.7498140335083 = 0.9751133322715759 + 2.0 * 6.387350559234619
Epoch 410, val loss: 1.1653510332107544
Epoch 420, training loss: 13.712115287780762 = 0.9436832070350647 + 2.0 * 6.384215831756592
Epoch 420, val loss: 1.1446796655654907
Epoch 430, training loss: 13.673439979553223 = 0.9124036431312561 + 2.0 * 6.380517959594727
Epoch 430, val loss: 1.1242849826812744
Epoch 440, training loss: 13.63663387298584 = 0.8813024759292603 + 2.0 * 6.3776655197143555
Epoch 440, val loss: 1.1042345762252808
Epoch 450, training loss: 13.605284690856934 = 0.8505166172981262 + 2.0 * 6.377384185791016
Epoch 450, val loss: 1.0845578908920288
Epoch 460, training loss: 13.565074920654297 = 0.8202101588249207 + 2.0 * 6.372432231903076
Epoch 460, val loss: 1.0652462244033813
Epoch 470, training loss: 13.533340454101562 = 0.7904459834098816 + 2.0 * 6.3714470863342285
Epoch 470, val loss: 1.0465753078460693
Epoch 480, training loss: 13.498397827148438 = 0.7612929344177246 + 2.0 * 6.3685526847839355
Epoch 480, val loss: 1.0282946825027466
Epoch 490, training loss: 13.463298797607422 = 0.7327815890312195 + 2.0 * 6.365258693695068
Epoch 490, val loss: 1.0106098651885986
Epoch 500, training loss: 13.43058967590332 = 0.7048153877258301 + 2.0 * 6.362887382507324
Epoch 500, val loss: 0.9936094880104065
Epoch 510, training loss: 13.400256156921387 = 0.677428662776947 + 2.0 * 6.361413955688477
Epoch 510, val loss: 0.9771550893783569
Epoch 520, training loss: 13.394094467163086 = 0.6506730318069458 + 2.0 * 6.371710777282715
Epoch 520, val loss: 0.9615474939346313
Epoch 530, training loss: 13.34202766418457 = 0.6247232556343079 + 2.0 * 6.358652114868164
Epoch 530, val loss: 0.946282148361206
Epoch 540, training loss: 13.307559967041016 = 0.5995344519615173 + 2.0 * 6.354012966156006
Epoch 540, val loss: 0.9319692254066467
Epoch 550, training loss: 13.27771282196045 = 0.5749920010566711 + 2.0 * 6.351360321044922
Epoch 550, val loss: 0.9185189008712769
Epoch 560, training loss: 13.254653930664062 = 0.5510473251342773 + 2.0 * 6.351803302764893
Epoch 560, val loss: 0.9057675004005432
Epoch 570, training loss: 13.227495193481445 = 0.5277553200721741 + 2.0 * 6.349869728088379
Epoch 570, val loss: 0.893885612487793
Epoch 580, training loss: 13.204440116882324 = 0.5051781535148621 + 2.0 * 6.349630832672119
Epoch 580, val loss: 0.8827707767486572
Epoch 590, training loss: 13.17574405670166 = 0.4833051264286041 + 2.0 * 6.346219539642334
Epoch 590, val loss: 0.8726551532745361
Epoch 600, training loss: 13.148204803466797 = 0.4621199667453766 + 2.0 * 6.343042373657227
Epoch 600, val loss: 0.8633902072906494
Epoch 610, training loss: 13.128252983093262 = 0.441569447517395 + 2.0 * 6.343341827392578
Epoch 610, val loss: 0.8549965023994446
Epoch 620, training loss: 13.111542701721191 = 0.42163944244384766 + 2.0 * 6.344951629638672
Epoch 620, val loss: 0.8474838733673096
Epoch 630, training loss: 13.08184814453125 = 0.40237462520599365 + 2.0 * 6.3397369384765625
Epoch 630, val loss: 0.8408054113388062
Epoch 640, training loss: 13.058728218078613 = 0.38375499844551086 + 2.0 * 6.337486743927002
Epoch 640, val loss: 0.8349440693855286
Epoch 650, training loss: 13.035962104797363 = 0.365744024515152 + 2.0 * 6.335109233856201
Epoch 650, val loss: 0.829927384853363
Epoch 660, training loss: 13.02884292602539 = 0.3483186960220337 + 2.0 * 6.340261936187744
Epoch 660, val loss: 0.8255887031555176
Epoch 670, training loss: 12.998886108398438 = 0.33141106367111206 + 2.0 * 6.333737373352051
Epoch 670, val loss: 0.8220188617706299
Epoch 680, training loss: 12.976961135864258 = 0.31517431139945984 + 2.0 * 6.330893516540527
Epoch 680, val loss: 0.8191303610801697
Epoch 690, training loss: 12.977246284484863 = 0.29954683780670166 + 2.0 * 6.3388495445251465
Epoch 690, val loss: 0.8170239329338074
Epoch 700, training loss: 12.940479278564453 = 0.2845191955566406 + 2.0 * 6.327980041503906
Epoch 700, val loss: 0.8153126239776611
Epoch 710, training loss: 12.928839683532715 = 0.27012962102890015 + 2.0 * 6.329355239868164
Epoch 710, val loss: 0.8143669962882996
Epoch 720, training loss: 12.905157089233398 = 0.2563530504703522 + 2.0 * 6.32440185546875
Epoch 720, val loss: 0.8140614032745361
Epoch 730, training loss: 12.892359733581543 = 0.24319559335708618 + 2.0 * 6.324582099914551
Epoch 730, val loss: 0.8143163919448853
Epoch 740, training loss: 12.875783920288086 = 0.2306043654680252 + 2.0 * 6.322589874267578
Epoch 740, val loss: 0.8149450421333313
Epoch 750, training loss: 12.870662689208984 = 0.21857599914073944 + 2.0 * 6.326043128967285
Epoch 750, val loss: 0.8160961866378784
Epoch 760, training loss: 12.858054161071777 = 0.20716305077075958 + 2.0 * 6.325445652008057
Epoch 760, val loss: 0.8173767924308777
Epoch 770, training loss: 12.832964897155762 = 0.19630682468414307 + 2.0 * 6.318328857421875
Epoch 770, val loss: 0.8190808296203613
Epoch 780, training loss: 12.821346282958984 = 0.18602336943149567 + 2.0 * 6.317661285400391
Epoch 780, val loss: 0.8212002515792847
Epoch 790, training loss: 12.823404312133789 = 0.17626629769802094 + 2.0 * 6.323568820953369
Epoch 790, val loss: 0.8235004544258118
Epoch 800, training loss: 12.797257423400879 = 0.16699540615081787 + 2.0 * 6.315131187438965
Epoch 800, val loss: 0.8259802460670471
Epoch 810, training loss: 12.799588203430176 = 0.1582258641719818 + 2.0 * 6.320681095123291
Epoch 810, val loss: 0.8287191390991211
Epoch 820, training loss: 12.778653144836426 = 0.14992877840995789 + 2.0 * 6.314362049102783
Epoch 820, val loss: 0.8317472338676453
Epoch 830, training loss: 12.76693058013916 = 0.14210796356201172 + 2.0 * 6.312411308288574
Epoch 830, val loss: 0.8349679708480835
Epoch 840, training loss: 12.771157264709473 = 0.1347045749425888 + 2.0 * 6.318226337432861
Epoch 840, val loss: 0.8383184671401978
Epoch 850, training loss: 12.746155738830566 = 0.1277184933423996 + 2.0 * 6.309218406677246
Epoch 850, val loss: 0.8419520258903503
Epoch 860, training loss: 12.736392974853516 = 0.12113481760025024 + 2.0 * 6.307629108428955
Epoch 860, val loss: 0.8457315564155579
Epoch 870, training loss: 12.728019714355469 = 0.11491208523511887 + 2.0 * 6.306553840637207
Epoch 870, val loss: 0.8496297597885132
Epoch 880, training loss: 12.739426612854004 = 0.10902554541826248 + 2.0 * 6.315200328826904
Epoch 880, val loss: 0.8537044525146484
Epoch 890, training loss: 12.717341423034668 = 0.10349660366773605 + 2.0 * 6.306922435760498
Epoch 890, val loss: 0.8577637672424316
Epoch 900, training loss: 12.706348419189453 = 0.0982956662774086 + 2.0 * 6.3040266036987305
Epoch 900, val loss: 0.862027645111084
Epoch 910, training loss: 12.70334529876709 = 0.09341566264629364 + 2.0 * 6.304965019226074
Epoch 910, val loss: 0.8663074970245361
Epoch 920, training loss: 12.693477630615234 = 0.08884146064519882 + 2.0 * 6.302318096160889
Epoch 920, val loss: 0.8707336187362671
Epoch 930, training loss: 12.69951057434082 = 0.08455689996480942 + 2.0 * 6.307476997375488
Epoch 930, val loss: 0.8752270936965942
Epoch 940, training loss: 12.687198638916016 = 0.08054481446743011 + 2.0 * 6.3033270835876465
Epoch 940, val loss: 0.8797352910041809
Epoch 950, training loss: 12.674382209777832 = 0.07678715884685516 + 2.0 * 6.298797607421875
Epoch 950, val loss: 0.8844441175460815
Epoch 960, training loss: 12.668846130371094 = 0.07326684147119522 + 2.0 * 6.297789573669434
Epoch 960, val loss: 0.8892829418182373
Epoch 970, training loss: 12.669334411621094 = 0.06995559483766556 + 2.0 * 6.299689292907715
Epoch 970, val loss: 0.8940837383270264
Epoch 980, training loss: 12.66202163696289 = 0.0668448805809021 + 2.0 * 6.297588348388672
Epoch 980, val loss: 0.8989521861076355
Epoch 990, training loss: 12.656293869018555 = 0.06392154842615128 + 2.0 * 6.2961859703063965
Epoch 990, val loss: 0.9037708044052124
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7481481481481482
0.8244596731681603
The final CL Acc:0.74568, 0.00175, The final GNN Acc:0.82253, 0.00311
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13168])
remove edge: torch.Size([2, 7888])
updated graph: torch.Size([2, 10500])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 19.157997131347656 = 1.964328646659851 + 2.0 * 8.596834182739258
Epoch 0, val loss: 1.9661681652069092
Epoch 10, training loss: 19.14711570739746 = 1.9540454149246216 + 2.0 * 8.596534729003906
Epoch 10, val loss: 1.9562468528747559
Epoch 20, training loss: 19.129728317260742 = 1.9415146112442017 + 2.0 * 8.594106674194336
Epoch 20, val loss: 1.9436259269714355
Epoch 30, training loss: 19.07196807861328 = 1.9245102405548096 + 2.0 * 8.573728561401367
Epoch 30, val loss: 1.926146149635315
Epoch 40, training loss: 18.736406326293945 = 1.9032891988754272 + 2.0 * 8.416558265686035
Epoch 40, val loss: 1.905290126800537
Epoch 50, training loss: 17.514663696289062 = 1.8818426132202148 + 2.0 * 7.816410541534424
Epoch 50, val loss: 1.8850531578063965
Epoch 60, training loss: 16.657438278198242 = 1.8640159368515015 + 2.0 * 7.3967108726501465
Epoch 60, val loss: 1.8686290979385376
Epoch 70, training loss: 16.19937515258789 = 1.8475542068481445 + 2.0 * 7.175910949707031
Epoch 70, val loss: 1.8527331352233887
Epoch 80, training loss: 15.885916709899902 = 1.8308831453323364 + 2.0 * 7.027516841888428
Epoch 80, val loss: 1.8364721536636353
Epoch 90, training loss: 15.648149490356445 = 1.8137904405593872 + 2.0 * 6.917179584503174
Epoch 90, val loss: 1.8205363750457764
Epoch 100, training loss: 15.457364082336426 = 1.7970266342163086 + 2.0 * 6.830168724060059
Epoch 100, val loss: 1.8052245378494263
Epoch 110, training loss: 15.300691604614258 = 1.7805030345916748 + 2.0 * 6.760094165802002
Epoch 110, val loss: 1.789891004562378
Epoch 120, training loss: 15.168465614318848 = 1.763912558555603 + 2.0 * 6.702276706695557
Epoch 120, val loss: 1.7742786407470703
Epoch 130, training loss: 15.065101623535156 = 1.746366024017334 + 2.0 * 6.659367561340332
Epoch 130, val loss: 1.7578821182250977
Epoch 140, training loss: 14.97789478302002 = 1.7271400690078735 + 2.0 * 6.625377178192139
Epoch 140, val loss: 1.7403475046157837
Epoch 150, training loss: 14.90029525756836 = 1.705810546875 + 2.0 * 6.59724235534668
Epoch 150, val loss: 1.7211346626281738
Epoch 160, training loss: 14.83031940460205 = 1.682110071182251 + 2.0 * 6.5741047859191895
Epoch 160, val loss: 1.6999844312667847
Epoch 170, training loss: 14.76377010345459 = 1.655604362487793 + 2.0 * 6.554082870483398
Epoch 170, val loss: 1.6765321493148804
Epoch 180, training loss: 14.703433990478516 = 1.6259567737579346 + 2.0 * 6.53873872756958
Epoch 180, val loss: 1.650367259979248
Epoch 190, training loss: 14.634720802307129 = 1.5930509567260742 + 2.0 * 6.520834922790527
Epoch 190, val loss: 1.6214089393615723
Epoch 200, training loss: 14.57291030883789 = 1.5568255186080933 + 2.0 * 6.508042335510254
Epoch 200, val loss: 1.5896764993667603
Epoch 210, training loss: 14.51376724243164 = 1.5174423456192017 + 2.0 * 6.498162269592285
Epoch 210, val loss: 1.5555490255355835
Epoch 220, training loss: 14.445605278015137 = 1.4756863117218018 + 2.0 * 6.484959602355957
Epoch 220, val loss: 1.5194108486175537
Epoch 230, training loss: 14.379096031188965 = 1.4317960739135742 + 2.0 * 6.473649978637695
Epoch 230, val loss: 1.4818317890167236
Epoch 240, training loss: 14.315389633178711 = 1.3863110542297363 + 2.0 * 6.464539051055908
Epoch 240, val loss: 1.4432239532470703
Epoch 250, training loss: 14.252184867858887 = 1.339743733406067 + 2.0 * 6.456220626831055
Epoch 250, val loss: 1.4041153192520142
Epoch 260, training loss: 14.197874069213867 = 1.2931605577468872 + 2.0 * 6.452356815338135
Epoch 260, val loss: 1.3653737306594849
Epoch 270, training loss: 14.136518478393555 = 1.2477397918701172 + 2.0 * 6.444389343261719
Epoch 270, val loss: 1.3279473781585693
Epoch 280, training loss: 14.075628280639648 = 1.203425407409668 + 2.0 * 6.43610143661499
Epoch 280, val loss: 1.2918106317520142
Epoch 290, training loss: 14.02194595336914 = 1.1603726148605347 + 2.0 * 6.430786609649658
Epoch 290, val loss: 1.2569597959518433
Epoch 300, training loss: 13.975658416748047 = 1.1189559698104858 + 2.0 * 6.428351402282715
Epoch 300, val loss: 1.2234634160995483
Epoch 310, training loss: 13.922861099243164 = 1.079301357269287 + 2.0 * 6.421779632568359
Epoch 310, val loss: 1.191748023033142
Epoch 320, training loss: 13.871419906616211 = 1.0412601232528687 + 2.0 * 6.4150800704956055
Epoch 320, val loss: 1.1614362001419067
Epoch 330, training loss: 13.824850082397461 = 1.0046300888061523 + 2.0 * 6.410109996795654
Epoch 330, val loss: 1.132411241531372
Epoch 340, training loss: 13.795247077941895 = 0.9693824648857117 + 2.0 * 6.412932395935059
Epoch 340, val loss: 1.104544997215271
Epoch 350, training loss: 13.744529724121094 = 0.9356660842895508 + 2.0 * 6.4044318199157715
Epoch 350, val loss: 1.0780023336410522
Epoch 360, training loss: 13.69991683959961 = 0.9033846855163574 + 2.0 * 6.398265838623047
Epoch 360, val loss: 1.0528514385223389
Epoch 370, training loss: 13.660943031311035 = 0.8723921179771423 + 2.0 * 6.394275665283203
Epoch 370, val loss: 1.0287840366363525
Epoch 380, training loss: 13.626008033752441 = 0.8425136804580688 + 2.0 * 6.391746997833252
Epoch 380, val loss: 1.005735158920288
Epoch 390, training loss: 13.597376823425293 = 0.8136951327323914 + 2.0 * 6.391840934753418
Epoch 390, val loss: 0.9837741255760193
Epoch 400, training loss: 13.555643081665039 = 0.7859405875205994 + 2.0 * 6.384851455688477
Epoch 400, val loss: 0.9625943899154663
Epoch 410, training loss: 13.519946098327637 = 0.7591114640235901 + 2.0 * 6.380417346954346
Epoch 410, val loss: 0.9424891471862793
Epoch 420, training loss: 13.49400520324707 = 0.7331241369247437 + 2.0 * 6.380440711975098
Epoch 420, val loss: 0.9232289791107178
Epoch 430, training loss: 13.457152366638184 = 0.7079881429672241 + 2.0 * 6.374582290649414
Epoch 430, val loss: 0.9047921895980835
Epoch 440, training loss: 13.426026344299316 = 0.6835906505584717 + 2.0 * 6.371217727661133
Epoch 440, val loss: 0.8871250748634338
Epoch 450, training loss: 13.42394733428955 = 0.6599066853523254 + 2.0 * 6.382020473480225
Epoch 450, val loss: 0.8700615763664246
Epoch 460, training loss: 13.374284744262695 = 0.6368439197540283 + 2.0 * 6.368720531463623
Epoch 460, val loss: 0.8538332581520081
Epoch 470, training loss: 13.342849731445312 = 0.614483118057251 + 2.0 * 6.36418342590332
Epoch 470, val loss: 0.8383256196975708
Epoch 480, training loss: 13.314689636230469 = 0.5926399827003479 + 2.0 * 6.361024856567383
Epoch 480, val loss: 0.8233696818351746
Epoch 490, training loss: 13.302115440368652 = 0.5712404847145081 + 2.0 * 6.3654375076293945
Epoch 490, val loss: 0.8089757561683655
Epoch 500, training loss: 13.262809753417969 = 0.550292432308197 + 2.0 * 6.356258869171143
Epoch 500, val loss: 0.7950736880302429
Epoch 510, training loss: 13.247832298278809 = 0.5297382473945618 + 2.0 * 6.359046936035156
Epoch 510, val loss: 0.7817789912223816
Epoch 520, training loss: 13.215733528137207 = 0.5095182657241821 + 2.0 * 6.353107452392578
Epoch 520, val loss: 0.7688613533973694
Epoch 530, training loss: 13.191849708557129 = 0.4895743727684021 + 2.0 * 6.351137638092041
Epoch 530, val loss: 0.7563986778259277
Epoch 540, training loss: 13.167591094970703 = 0.4698401391506195 + 2.0 * 6.348875522613525
Epoch 540, val loss: 0.7442788481712341
Epoch 550, training loss: 13.14113712310791 = 0.4503215253353119 + 2.0 * 6.345407962799072
Epoch 550, val loss: 0.7325807213783264
Epoch 560, training loss: 13.121807098388672 = 0.43094581365585327 + 2.0 * 6.345430850982666
Epoch 560, val loss: 0.7213130593299866
Epoch 570, training loss: 13.105806350708008 = 0.41179952025413513 + 2.0 * 6.34700345993042
Epoch 570, val loss: 0.7104854583740234
Epoch 580, training loss: 13.075559616088867 = 0.3929641842842102 + 2.0 * 6.341297626495361
Epoch 580, val loss: 0.7001329064369202
Epoch 590, training loss: 13.051375389099121 = 0.37442606687545776 + 2.0 * 6.338474750518799
Epoch 590, val loss: 0.690291702747345
Epoch 600, training loss: 13.029190063476562 = 0.3562360405921936 + 2.0 * 6.336476802825928
Epoch 600, val loss: 0.6810715198516846
Epoch 610, training loss: 13.020841598510742 = 0.33842939138412476 + 2.0 * 6.341206073760986
Epoch 610, val loss: 0.6724041700363159
Epoch 620, training loss: 12.98626708984375 = 0.3211091160774231 + 2.0 * 6.332579135894775
Epoch 620, val loss: 0.6643825769424438
Epoch 630, training loss: 12.96718978881836 = 0.30437570810317993 + 2.0 * 6.331407070159912
Epoch 630, val loss: 0.657050609588623
Epoch 640, training loss: 12.959310531616211 = 0.2882732152938843 + 2.0 * 6.335518836975098
Epoch 640, val loss: 0.6503195762634277
Epoch 650, training loss: 12.940361976623535 = 0.2727835178375244 + 2.0 * 6.333789348602295
Epoch 650, val loss: 0.644204318523407
Epoch 660, training loss: 12.912922859191895 = 0.25810280442237854 + 2.0 * 6.3274102210998535
Epoch 660, val loss: 0.6388698816299438
Epoch 670, training loss: 12.892827033996582 = 0.24415838718414307 + 2.0 * 6.324334144592285
Epoch 670, val loss: 0.6341962814331055
Epoch 680, training loss: 12.899827003479004 = 0.2309175729751587 + 2.0 * 6.334454536437988
Epoch 680, val loss: 0.6300351023674011
Epoch 690, training loss: 12.863356590270996 = 0.21842770278453827 + 2.0 * 6.322464466094971
Epoch 690, val loss: 0.6266158819198608
Epoch 700, training loss: 12.848374366760254 = 0.20667095482349396 + 2.0 * 6.320851802825928
Epoch 700, val loss: 0.6237462162971497
Epoch 710, training loss: 12.835708618164062 = 0.19561883807182312 + 2.0 * 6.320044994354248
Epoch 710, val loss: 0.6214746236801147
Epoch 720, training loss: 12.827186584472656 = 0.18524393439292908 + 2.0 * 6.320971488952637
Epoch 720, val loss: 0.619731605052948
Epoch 730, training loss: 12.810877799987793 = 0.17551098763942719 + 2.0 * 6.317683219909668
Epoch 730, val loss: 0.6184405088424683
Epoch 740, training loss: 12.80612564086914 = 0.16638125479221344 + 2.0 * 6.3198723793029785
Epoch 740, val loss: 0.617663562297821
Epoch 750, training loss: 12.78744888305664 = 0.15785571932792664 + 2.0 * 6.314796447753906
Epoch 750, val loss: 0.6173425912857056
Epoch 760, training loss: 12.774977684020996 = 0.149852454662323 + 2.0 * 6.312562465667725
Epoch 760, val loss: 0.6174623966217041
Epoch 770, training loss: 12.77457046508789 = 0.1423618346452713 + 2.0 * 6.316104412078857
Epoch 770, val loss: 0.6180003881454468
Epoch 780, training loss: 12.758383750915527 = 0.135322704911232 + 2.0 * 6.311530590057373
Epoch 780, val loss: 0.6186434030532837
Epoch 790, training loss: 12.748594284057617 = 0.12872754037380219 + 2.0 * 6.309933185577393
Epoch 790, val loss: 0.6198704242706299
Epoch 800, training loss: 12.743696212768555 = 0.12254233658313751 + 2.0 * 6.310576915740967
Epoch 800, val loss: 0.6212173700332642
Epoch 810, training loss: 12.737151145935059 = 0.11671797186136246 + 2.0 * 6.310216426849365
Epoch 810, val loss: 0.6228901147842407
Epoch 820, training loss: 12.728057861328125 = 0.11125506460666656 + 2.0 * 6.308401584625244
Epoch 820, val loss: 0.6247559785842896
Epoch 830, training loss: 12.715852737426758 = 0.10611692070960999 + 2.0 * 6.304867744445801
Epoch 830, val loss: 0.6269723773002625
Epoch 840, training loss: 12.707036018371582 = 0.10127727687358856 + 2.0 * 6.302879333496094
Epoch 840, val loss: 0.6293577551841736
Epoch 850, training loss: 12.710166931152344 = 0.09670717269182205 + 2.0 * 6.306729793548584
Epoch 850, val loss: 0.632000207901001
Epoch 860, training loss: 12.710206031799316 = 0.09236213564872742 + 2.0 * 6.308921813964844
Epoch 860, val loss: 0.6346778273582458
Epoch 870, training loss: 12.691938400268555 = 0.08828778564929962 + 2.0 * 6.301825523376465
Epoch 870, val loss: 0.6376294493675232
Epoch 880, training loss: 12.681011199951172 = 0.08442679792642593 + 2.0 * 6.29829216003418
Epoch 880, val loss: 0.6407567858695984
Epoch 890, training loss: 12.676031112670898 = 0.08077254146337509 + 2.0 * 6.297629356384277
Epoch 890, val loss: 0.6440228223800659
Epoch 900, training loss: 12.677628517150879 = 0.07731132954359055 + 2.0 * 6.300158500671387
Epoch 900, val loss: 0.6473745703697205
Epoch 910, training loss: 12.67621898651123 = 0.07401914894580841 + 2.0 * 6.30109977722168
Epoch 910, val loss: 0.6508134007453918
Epoch 920, training loss: 12.672469139099121 = 0.0709090605378151 + 2.0 * 6.300779819488525
Epoch 920, val loss: 0.6543489694595337
Epoch 930, training loss: 12.657854080200195 = 0.06797324120998383 + 2.0 * 6.29494047164917
Epoch 930, val loss: 0.6581804752349854
Epoch 940, training loss: 12.651124954223633 = 0.0651843324303627 + 2.0 * 6.292970180511475
Epoch 940, val loss: 0.6620029807090759
Epoch 950, training loss: 12.648508071899414 = 0.06253740191459656 + 2.0 * 6.292985439300537
Epoch 950, val loss: 0.6658453941345215
Epoch 960, training loss: 12.647043228149414 = 0.06001532822847366 + 2.0 * 6.293513774871826
Epoch 960, val loss: 0.6697447896003723
Epoch 970, training loss: 12.6424560546875 = 0.05761854350566864 + 2.0 * 6.292418956756592
Epoch 970, val loss: 0.6737087965011597
Epoch 980, training loss: 12.638731002807617 = 0.05534381791949272 + 2.0 * 6.291693687438965
Epoch 980, val loss: 0.67781001329422
Epoch 990, training loss: 12.633172988891602 = 0.05318189412355423 + 2.0 * 6.2899956703186035
Epoch 990, val loss: 0.681976854801178
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8111111111111111
0.8402741170268846
=== training gcn model ===
Epoch 0, training loss: 19.12506675720215 = 1.93136465549469 + 2.0 * 8.596851348876953
Epoch 0, val loss: 1.919661045074463
Epoch 10, training loss: 19.11464500427246 = 1.9215737581253052 + 2.0 * 8.596535682678223
Epoch 10, val loss: 1.9106578826904297
Epoch 20, training loss: 19.097232818603516 = 1.909057378768921 + 2.0 * 8.594087600708008
Epoch 20, val loss: 1.8988823890686035
Epoch 30, training loss: 19.04230499267578 = 1.8915103673934937 + 2.0 * 8.575397491455078
Epoch 30, val loss: 1.8822413682937622
Epoch 40, training loss: 18.77910804748535 = 1.8692269325256348 + 2.0 * 8.454940795898438
Epoch 40, val loss: 1.8615143299102783
Epoch 50, training loss: 17.860445022583008 = 1.844587802886963 + 2.0 * 8.007928848266602
Epoch 50, val loss: 1.8382188081741333
Epoch 60, training loss: 16.969900131225586 = 1.8252109289169312 + 2.0 * 7.572344779968262
Epoch 60, val loss: 1.8208130598068237
Epoch 70, training loss: 16.31557846069336 = 1.8140031099319458 + 2.0 * 7.250787258148193
Epoch 70, val loss: 1.8107078075408936
Epoch 80, training loss: 15.935155868530273 = 1.8008837699890137 + 2.0 * 7.067135810852051
Epoch 80, val loss: 1.7987483739852905
Epoch 90, training loss: 15.709014892578125 = 1.7857153415679932 + 2.0 * 6.9616498947143555
Epoch 90, val loss: 1.786698341369629
Epoch 100, training loss: 15.494806289672852 = 1.7712299823760986 + 2.0 * 6.861788272857666
Epoch 100, val loss: 1.7756272554397583
Epoch 110, training loss: 15.330462455749512 = 1.7572704553604126 + 2.0 * 6.786595821380615
Epoch 110, val loss: 1.7642446756362915
Epoch 120, training loss: 15.209641456604004 = 1.7419567108154297 + 2.0 * 6.733842372894287
Epoch 120, val loss: 1.751599907875061
Epoch 130, training loss: 15.106511116027832 = 1.724936604499817 + 2.0 * 6.690787315368652
Epoch 130, val loss: 1.7376084327697754
Epoch 140, training loss: 15.022174835205078 = 1.706173062324524 + 2.0 * 6.658000946044922
Epoch 140, val loss: 1.7221401929855347
Epoch 150, training loss: 14.93837833404541 = 1.6854221820831299 + 2.0 * 6.62647819519043
Epoch 150, val loss: 1.704882025718689
Epoch 160, training loss: 14.862914085388184 = 1.662250280380249 + 2.0 * 6.600331783294678
Epoch 160, val loss: 1.6855634450912476
Epoch 170, training loss: 14.7932710647583 = 1.6361429691314697 + 2.0 * 6.578564167022705
Epoch 170, val loss: 1.6638352870941162
Epoch 180, training loss: 14.722990036010742 = 1.6070032119750977 + 2.0 * 6.557993412017822
Epoch 180, val loss: 1.6396759748458862
Epoch 190, training loss: 14.65683364868164 = 1.5747885704040527 + 2.0 * 6.541022777557373
Epoch 190, val loss: 1.612940788269043
Epoch 200, training loss: 14.594164848327637 = 1.5394086837768555 + 2.0 * 6.527378082275391
Epoch 200, val loss: 1.5836139917373657
Epoch 210, training loss: 14.527140617370605 = 1.5013192892074585 + 2.0 * 6.512910842895508
Epoch 210, val loss: 1.5521143674850464
Epoch 220, training loss: 14.462600708007812 = 1.4610832929611206 + 2.0 * 6.500758647918701
Epoch 220, val loss: 1.5188807249069214
Epoch 230, training loss: 14.397971153259277 = 1.418830156326294 + 2.0 * 6.489570617675781
Epoch 230, val loss: 1.484170913696289
Epoch 240, training loss: 14.341081619262695 = 1.3752388954162598 + 2.0 * 6.482921600341797
Epoch 240, val loss: 1.4484939575195312
Epoch 250, training loss: 14.277719497680664 = 1.3314522504806519 + 2.0 * 6.473133563995361
Epoch 250, val loss: 1.412800908088684
Epoch 260, training loss: 14.213326454162598 = 1.2877577543258667 + 2.0 * 6.462784290313721
Epoch 260, val loss: 1.3774731159210205
Epoch 270, training loss: 14.155891418457031 = 1.2444167137145996 + 2.0 * 6.455737113952637
Epoch 270, val loss: 1.3427150249481201
Epoch 280, training loss: 14.1075439453125 = 1.2022101879119873 + 2.0 * 6.452666759490967
Epoch 280, val loss: 1.3093883991241455
Epoch 290, training loss: 14.049949645996094 = 1.161536455154419 + 2.0 * 6.444206714630127
Epoch 290, val loss: 1.2776559591293335
Epoch 300, training loss: 13.995837211608887 = 1.1225193738937378 + 2.0 * 6.43665885925293
Epoch 300, val loss: 1.247727870941162
Epoch 310, training loss: 13.949650764465332 = 1.0852442979812622 + 2.0 * 6.43220329284668
Epoch 310, val loss: 1.2195017337799072
Epoch 320, training loss: 13.901281356811523 = 1.0494756698608398 + 2.0 * 6.425902843475342
Epoch 320, val loss: 1.1931191682815552
Epoch 330, training loss: 13.858118057250977 = 1.0151314735412598 + 2.0 * 6.4214935302734375
Epoch 330, val loss: 1.16824209690094
Epoch 340, training loss: 13.815595626831055 = 0.9820915460586548 + 2.0 * 6.416751861572266
Epoch 340, val loss: 1.144742488861084
Epoch 350, training loss: 13.77501106262207 = 0.9501675367355347 + 2.0 * 6.412421703338623
Epoch 350, val loss: 1.1222622394561768
Epoch 360, training loss: 13.735105514526367 = 0.9189823865890503 + 2.0 * 6.408061504364014
Epoch 360, val loss: 1.1007484197616577
Epoch 370, training loss: 13.700815200805664 = 0.8883995413780212 + 2.0 * 6.406208038330078
Epoch 370, val loss: 1.0798214673995972
Epoch 380, training loss: 13.658499717712402 = 0.8584269881248474 + 2.0 * 6.400036334991455
Epoch 380, val loss: 1.0595579147338867
Epoch 390, training loss: 13.619401931762695 = 0.8287226557731628 + 2.0 * 6.395339488983154
Epoch 390, val loss: 1.0397857427597046
Epoch 400, training loss: 13.593541145324707 = 0.7992405295372009 + 2.0 * 6.39715051651001
Epoch 400, val loss: 1.0204358100891113
Epoch 410, training loss: 13.549036026000977 = 0.7703328132629395 + 2.0 * 6.3893513679504395
Epoch 410, val loss: 1.001632571220398
Epoch 420, training loss: 13.513359069824219 = 0.7419537901878357 + 2.0 * 6.385702610015869
Epoch 420, val loss: 0.9833970665931702
Epoch 430, training loss: 13.48807430267334 = 0.7140922546386719 + 2.0 * 6.386991024017334
Epoch 430, val loss: 0.9658225774765015
Epoch 440, training loss: 13.447127342224121 = 0.6867733597755432 + 2.0 * 6.380177021026611
Epoch 440, val loss: 0.9488927721977234
Epoch 450, training loss: 13.411340713500977 = 0.6602258682250977 + 2.0 * 6.3755574226379395
Epoch 450, val loss: 0.9328584671020508
Epoch 460, training loss: 13.391621589660645 = 0.6343349814414978 + 2.0 * 6.37864351272583
Epoch 460, val loss: 0.9177752137184143
Epoch 470, training loss: 13.355122566223145 = 0.6093807816505432 + 2.0 * 6.372870922088623
Epoch 470, val loss: 0.9035795331001282
Epoch 480, training loss: 13.322447776794434 = 0.5853646397590637 + 2.0 * 6.368541717529297
Epoch 480, val loss: 0.8905554413795471
Epoch 490, training loss: 13.291499137878418 = 0.5621415376663208 + 2.0 * 6.364678859710693
Epoch 490, val loss: 0.87849360704422
Epoch 500, training loss: 13.284709930419922 = 0.5396361947059631 + 2.0 * 6.372536659240723
Epoch 500, val loss: 0.8673109412193298
Epoch 510, training loss: 13.239485740661621 = 0.5179747939109802 + 2.0 * 6.360755443572998
Epoch 510, val loss: 0.857044517993927
Epoch 520, training loss: 13.213804244995117 = 0.4970564842224121 + 2.0 * 6.358373641967773
Epoch 520, val loss: 0.8479548692703247
Epoch 530, training loss: 13.1875581741333 = 0.47678542137145996 + 2.0 * 6.355386257171631
Epoch 530, val loss: 0.8395339846611023
Epoch 540, training loss: 13.163697242736816 = 0.45702245831489563 + 2.0 * 6.353337287902832
Epoch 540, val loss: 0.8318377137184143
Epoch 550, training loss: 13.141252517700195 = 0.4377322793006897 + 2.0 * 6.351759910583496
Epoch 550, val loss: 0.8247213363647461
Epoch 560, training loss: 13.12179183959961 = 0.41897889971733093 + 2.0 * 6.351406574249268
Epoch 560, val loss: 0.8182432651519775
Epoch 570, training loss: 13.095162391662598 = 0.4007242023944855 + 2.0 * 6.347218990325928
Epoch 570, val loss: 0.8123302459716797
Epoch 580, training loss: 13.076146125793457 = 0.3829221725463867 + 2.0 * 6.346611976623535
Epoch 580, val loss: 0.8068689703941345
Epoch 590, training loss: 13.05614948272705 = 0.36555016040802 + 2.0 * 6.34529972076416
Epoch 590, val loss: 0.8019211888313293
Epoch 600, training loss: 13.033953666687012 = 0.3487618565559387 + 2.0 * 6.342596054077148
Epoch 600, val loss: 0.7974003553390503
Epoch 610, training loss: 13.013416290283203 = 0.33252662420272827 + 2.0 * 6.340445041656494
Epoch 610, val loss: 0.7933681607246399
Epoch 620, training loss: 12.999432563781738 = 0.3168601989746094 + 2.0 * 6.3412861824035645
Epoch 620, val loss: 0.7896941900253296
Epoch 630, training loss: 12.979148864746094 = 0.30180981755256653 + 2.0 * 6.338669300079346
Epoch 630, val loss: 0.7865436673164368
Epoch 640, training loss: 12.95806884765625 = 0.2874027490615845 + 2.0 * 6.335332870483398
Epoch 640, val loss: 0.7838382124900818
Epoch 650, training loss: 12.96088981628418 = 0.2735665440559387 + 2.0 * 6.343661785125732
Epoch 650, val loss: 0.7815353870391846
Epoch 660, training loss: 12.925445556640625 = 0.26041674613952637 + 2.0 * 6.33251428604126
Epoch 660, val loss: 0.7797495722770691
Epoch 670, training loss: 12.907585144042969 = 0.2478761523962021 + 2.0 * 6.329854488372803
Epoch 670, val loss: 0.7783786058425903
Epoch 680, training loss: 12.893310546875 = 0.23592613637447357 + 2.0 * 6.328692436218262
Epoch 680, val loss: 0.777385413646698
Epoch 690, training loss: 12.878440856933594 = 0.22450456023216248 + 2.0 * 6.326968193054199
Epoch 690, val loss: 0.7768109440803528
Epoch 700, training loss: 12.881401062011719 = 0.21359765529632568 + 2.0 * 6.333901882171631
Epoch 700, val loss: 0.7765709161758423
Epoch 710, training loss: 12.85287094116211 = 0.20327997207641602 + 2.0 * 6.324795722961426
Epoch 710, val loss: 0.7766867280006409
Epoch 720, training loss: 12.842665672302246 = 0.19346986711025238 + 2.0 * 6.3245978355407715
Epoch 720, val loss: 0.7772002220153809
Epoch 730, training loss: 12.834745407104492 = 0.1841270625591278 + 2.0 * 6.3253092765808105
Epoch 730, val loss: 0.778079092502594
Epoch 740, training loss: 12.817557334899902 = 0.17526957392692566 + 2.0 * 6.321144104003906
Epoch 740, val loss: 0.7793829441070557
Epoch 750, training loss: 12.81367015838623 = 0.16682277619838715 + 2.0 * 6.323423862457275
Epoch 750, val loss: 0.7809184193611145
Epoch 760, training loss: 12.79751205444336 = 0.1588161736726761 + 2.0 * 6.319347858428955
Epoch 760, val loss: 0.782798171043396
Epoch 770, training loss: 12.786291122436523 = 0.1512179970741272 + 2.0 * 6.317536354064941
Epoch 770, val loss: 0.7850577235221863
Epoch 780, training loss: 12.776830673217773 = 0.1440228521823883 + 2.0 * 6.316403865814209
Epoch 780, val loss: 0.7874981760978699
Epoch 790, training loss: 12.781715393066406 = 0.13719242811203003 + 2.0 * 6.322261333465576
Epoch 790, val loss: 0.7902398109436035
Epoch 800, training loss: 12.758427619934082 = 0.13070473074913025 + 2.0 * 6.31386137008667
Epoch 800, val loss: 0.7932352423667908
Epoch 810, training loss: 12.748867988586426 = 0.12458059191703796 + 2.0 * 6.312143802642822
Epoch 810, val loss: 0.7965375781059265
Epoch 820, training loss: 12.744847297668457 = 0.118779756128788 + 2.0 * 6.313033580780029
Epoch 820, val loss: 0.8000550270080566
Epoch 830, training loss: 12.733358383178711 = 0.11326674371957779 + 2.0 * 6.3100457191467285
Epoch 830, val loss: 0.8037394881248474
Epoch 840, training loss: 12.730043411254883 = 0.10806871205568314 + 2.0 * 6.31098747253418
Epoch 840, val loss: 0.8076582551002502
Epoch 850, training loss: 12.721375465393066 = 0.1031438335776329 + 2.0 * 6.309115886688232
Epoch 850, val loss: 0.8117589354515076
Epoch 860, training loss: 12.717242240905762 = 0.09847766906023026 + 2.0 * 6.309382438659668
Epoch 860, val loss: 0.8160327672958374
Epoch 870, training loss: 12.709272384643555 = 0.09408210217952728 + 2.0 * 6.307595252990723
Epoch 870, val loss: 0.8205022811889648
Epoch 880, training loss: 12.709545135498047 = 0.08989966660737991 + 2.0 * 6.3098225593566895
Epoch 880, val loss: 0.8250217437744141
Epoch 890, training loss: 12.69616413116455 = 0.08596032112836838 + 2.0 * 6.3051018714904785
Epoch 890, val loss: 0.8296090364456177
Epoch 900, training loss: 12.688441276550293 = 0.08221395313739777 + 2.0 * 6.3031134605407715
Epoch 900, val loss: 0.8343747854232788
Epoch 910, training loss: 12.688552856445312 = 0.07867021858692169 + 2.0 * 6.304941177368164
Epoch 910, val loss: 0.8392184972763062
Epoch 920, training loss: 12.681760787963867 = 0.07532400637865067 + 2.0 * 6.303218364715576
Epoch 920, val loss: 0.8441842794418335
Epoch 930, training loss: 12.675790786743164 = 0.07214448601007462 + 2.0 * 6.301823139190674
Epoch 930, val loss: 0.8491536378860474
Epoch 940, training loss: 12.668310165405273 = 0.06914496421813965 + 2.0 * 6.299582481384277
Epoch 940, val loss: 0.854193925857544
Epoch 950, training loss: 12.665740966796875 = 0.06629974395036697 + 2.0 * 6.299720764160156
Epoch 950, val loss: 0.8593148589134216
Epoch 960, training loss: 12.667447090148926 = 0.06359995156526566 + 2.0 * 6.301923751831055
Epoch 960, val loss: 0.8644936084747314
Epoch 970, training loss: 12.656638145446777 = 0.061041705310344696 + 2.0 * 6.297798156738281
Epoch 970, val loss: 0.8695885539054871
Epoch 980, training loss: 12.658244132995605 = 0.05862054228782654 + 2.0 * 6.299811840057373
Epoch 980, val loss: 0.8747646808624268
Epoch 990, training loss: 12.649046897888184 = 0.056335315108299255 + 2.0 * 6.296355724334717
Epoch 990, val loss: 0.8799681663513184
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.762962962962963
0.8408012651555088
=== training gcn model ===
Epoch 0, training loss: 19.15907859802246 = 1.9654008150100708 + 2.0 * 8.59683895111084
Epoch 0, val loss: 1.9620451927185059
Epoch 10, training loss: 19.14736557006836 = 1.9543946981430054 + 2.0 * 8.596485137939453
Epoch 10, val loss: 1.951422095298767
Epoch 20, training loss: 19.12850570678711 = 1.9411052465438843 + 2.0 * 8.593700408935547
Epoch 20, val loss: 1.9382225275039673
Epoch 30, training loss: 19.06903076171875 = 1.9235763549804688 + 2.0 * 8.57272720336914
Epoch 30, val loss: 1.920637607574463
Epoch 40, training loss: 18.787256240844727 = 1.9020960330963135 + 2.0 * 8.442580223083496
Epoch 40, val loss: 1.8996471166610718
Epoch 50, training loss: 18.038232803344727 = 1.8785414695739746 + 2.0 * 8.079845428466797
Epoch 50, val loss: 1.8766860961914062
Epoch 60, training loss: 17.221996307373047 = 1.8574090003967285 + 2.0 * 7.682293891906738
Epoch 60, val loss: 1.857144832611084
Epoch 70, training loss: 16.55793571472168 = 1.8420042991638184 + 2.0 * 7.357965469360352
Epoch 70, val loss: 1.8439793586730957
Epoch 80, training loss: 16.075035095214844 = 1.8264559507369995 + 2.0 * 7.124289512634277
Epoch 80, val loss: 1.8303552865982056
Epoch 90, training loss: 15.748311996459961 = 1.810117244720459 + 2.0 * 6.96909761428833
Epoch 90, val loss: 1.8166464567184448
Epoch 100, training loss: 15.545164108276367 = 1.793864130973816 + 2.0 * 6.875649929046631
Epoch 100, val loss: 1.8031150102615356
Epoch 110, training loss: 15.399727821350098 = 1.7780359983444214 + 2.0 * 6.810845851898193
Epoch 110, val loss: 1.7897427082061768
Epoch 120, training loss: 15.267662048339844 = 1.7628273963928223 + 2.0 * 6.75241756439209
Epoch 120, val loss: 1.7766168117523193
Epoch 130, training loss: 15.164312362670898 = 1.7472184896469116 + 2.0 * 6.708547115325928
Epoch 130, val loss: 1.7627865076065063
Epoch 140, training loss: 15.080696105957031 = 1.730027675628662 + 2.0 * 6.675334453582764
Epoch 140, val loss: 1.7474275827407837
Epoch 150, training loss: 15.003012657165527 = 1.7110260725021362 + 2.0 * 6.645993232727051
Epoch 150, val loss: 1.7305731773376465
Epoch 160, training loss: 14.93004035949707 = 1.6899826526641846 + 2.0 * 6.620028972625732
Epoch 160, val loss: 1.7121458053588867
Epoch 170, training loss: 14.86625862121582 = 1.6666193008422852 + 2.0 * 6.599819660186768
Epoch 170, val loss: 1.691961646080017
Epoch 180, training loss: 14.796316146850586 = 1.6410331726074219 + 2.0 * 6.577641487121582
Epoch 180, val loss: 1.670080304145813
Epoch 190, training loss: 14.726778984069824 = 1.6131772994995117 + 2.0 * 6.556800842285156
Epoch 190, val loss: 1.64638090133667
Epoch 200, training loss: 14.664010047912598 = 1.5829130411148071 + 2.0 * 6.540548324584961
Epoch 200, val loss: 1.6207765340805054
Epoch 210, training loss: 14.60267448425293 = 1.5505636930465698 + 2.0 * 6.526055335998535
Epoch 210, val loss: 1.5935814380645752
Epoch 220, training loss: 14.538548469543457 = 1.5167957544326782 + 2.0 * 6.510876178741455
Epoch 220, val loss: 1.5653867721557617
Epoch 230, training loss: 14.478585243225098 = 1.4818006753921509 + 2.0 * 6.498392105102539
Epoch 230, val loss: 1.5364245176315308
Epoch 240, training loss: 14.417988777160645 = 1.4457311630249023 + 2.0 * 6.486128807067871
Epoch 240, val loss: 1.5069204568862915
Epoch 250, training loss: 14.362491607666016 = 1.4091445207595825 + 2.0 * 6.476673603057861
Epoch 250, val loss: 1.4771863222122192
Epoch 260, training loss: 14.313169479370117 = 1.3721176385879517 + 2.0 * 6.470525741577148
Epoch 260, val loss: 1.4474350214004517
Epoch 270, training loss: 14.253705978393555 = 1.3349963426589966 + 2.0 * 6.459354877471924
Epoch 270, val loss: 1.4180454015731812
Epoch 280, training loss: 14.198471069335938 = 1.2978568077087402 + 2.0 * 6.4503068923950195
Epoch 280, val loss: 1.388814091682434
Epoch 290, training loss: 14.152538299560547 = 1.2607917785644531 + 2.0 * 6.445873260498047
Epoch 290, val loss: 1.3601845502853394
Epoch 300, training loss: 14.098435401916504 = 1.2243732213974 + 2.0 * 6.437031269073486
Epoch 300, val loss: 1.3320468664169312
Epoch 310, training loss: 14.046448707580566 = 1.1881706714630127 + 2.0 * 6.429139137268066
Epoch 310, val loss: 1.3043179512023926
Epoch 320, training loss: 14.002400398254395 = 1.1522130966186523 + 2.0 * 6.425093650817871
Epoch 320, val loss: 1.2770112752914429
Epoch 330, training loss: 13.954255104064941 = 1.1167757511138916 + 2.0 * 6.4187397956848145
Epoch 330, val loss: 1.2502020597457886
Epoch 340, training loss: 13.907827377319336 = 1.0818120241165161 + 2.0 * 6.413007736206055
Epoch 340, val loss: 1.2238267660140991
Epoch 350, training loss: 13.866409301757812 = 1.0472865104675293 + 2.0 * 6.409561634063721
Epoch 350, val loss: 1.1978065967559814
Epoch 360, training loss: 13.821953773498535 = 1.0132561922073364 + 2.0 * 6.404348850250244
Epoch 360, val loss: 1.1722816228866577
Epoch 370, training loss: 13.781913757324219 = 0.9798737168312073 + 2.0 * 6.401020050048828
Epoch 370, val loss: 1.1471402645111084
Epoch 380, training loss: 13.739150047302246 = 0.9471400380134583 + 2.0 * 6.396005153656006
Epoch 380, val loss: 1.1225603818893433
Epoch 390, training loss: 13.699542999267578 = 0.915010392665863 + 2.0 * 6.392266273498535
Epoch 390, val loss: 1.098345160484314
Epoch 400, training loss: 13.658615112304688 = 0.8835975527763367 + 2.0 * 6.387508869171143
Epoch 400, val loss: 1.0746681690216064
Epoch 410, training loss: 13.624117851257324 = 0.853009045124054 + 2.0 * 6.385554313659668
Epoch 410, val loss: 1.0514888763427734
Epoch 420, training loss: 13.58169937133789 = 0.8234314918518066 + 2.0 * 6.379134178161621
Epoch 420, val loss: 1.0291622877120972
Epoch 430, training loss: 13.546079635620117 = 0.7948181629180908 + 2.0 * 6.375630855560303
Epoch 430, val loss: 1.007582426071167
Epoch 440, training loss: 13.522719383239746 = 0.7671297192573547 + 2.0 * 6.3777947425842285
Epoch 440, val loss: 0.9866633415222168
Epoch 450, training loss: 13.485479354858398 = 0.7405428290367126 + 2.0 * 6.3724684715271
Epoch 450, val loss: 0.966618537902832
Epoch 460, training loss: 13.449224472045898 = 0.7150291204452515 + 2.0 * 6.367097854614258
Epoch 460, val loss: 0.9476392865180969
Epoch 470, training loss: 13.421470642089844 = 0.6904309988021851 + 2.0 * 6.365520000457764
Epoch 470, val loss: 0.9293977618217468
Epoch 480, training loss: 13.395475387573242 = 0.6667079329490662 + 2.0 * 6.364383697509766
Epoch 480, val loss: 0.911738932132721
Epoch 490, training loss: 13.358874320983887 = 0.6438085436820984 + 2.0 * 6.357532978057861
Epoch 490, val loss: 0.8951299786567688
Epoch 500, training loss: 13.340201377868652 = 0.6216176748275757 + 2.0 * 6.359292030334473
Epoch 500, val loss: 0.8792775273323059
Epoch 510, training loss: 13.309761047363281 = 0.5999664664268494 + 2.0 * 6.354897499084473
Epoch 510, val loss: 0.863884687423706
Epoch 520, training loss: 13.280282974243164 = 0.5789589881896973 + 2.0 * 6.350661754608154
Epoch 520, val loss: 0.8493617177009583
Epoch 530, training loss: 13.254349708557129 = 0.5584564208984375 + 2.0 * 6.347946643829346
Epoch 530, val loss: 0.8355425000190735
Epoch 540, training loss: 13.241039276123047 = 0.5384902954101562 + 2.0 * 6.351274490356445
Epoch 540, val loss: 0.8224936723709106
Epoch 550, training loss: 13.208035469055176 = 0.5190918445587158 + 2.0 * 6.3444719314575195
Epoch 550, val loss: 0.810188353061676
Epoch 560, training loss: 13.182987213134766 = 0.5002925992012024 + 2.0 * 6.3413472175598145
Epoch 560, val loss: 0.7987366914749146
Epoch 570, training loss: 13.161654472351074 = 0.48201194405555725 + 2.0 * 6.3398213386535645
Epoch 570, val loss: 0.7881360650062561
Epoch 580, training loss: 13.140625 = 0.4641864597797394 + 2.0 * 6.338219165802002
Epoch 580, val loss: 0.7782981395721436
Epoch 590, training loss: 13.121102333068848 = 0.44681739807128906 + 2.0 * 6.337142467498779
Epoch 590, val loss: 0.7691482901573181
Epoch 600, training loss: 13.09572982788086 = 0.4298083484172821 + 2.0 * 6.332960605621338
Epoch 600, val loss: 0.7606895565986633
Epoch 610, training loss: 13.077054977416992 = 0.41313332319259644 + 2.0 * 6.331960678100586
Epoch 610, val loss: 0.7529274821281433
Epoch 620, training loss: 13.0546875 = 0.39668649435043335 + 2.0 * 6.329000473022461
Epoch 620, val loss: 0.745607852935791
Epoch 630, training loss: 13.037961959838867 = 0.3804863691329956 + 2.0 * 6.328737735748291
Epoch 630, val loss: 0.7388243675231934
Epoch 640, training loss: 13.027070045471191 = 0.36446189880371094 + 2.0 * 6.33130407333374
Epoch 640, val loss: 0.732508659362793
Epoch 650, training loss: 13.001422882080078 = 0.34861135482788086 + 2.0 * 6.326406002044678
Epoch 650, val loss: 0.726774275302887
Epoch 660, training loss: 12.980030059814453 = 0.3329128324985504 + 2.0 * 6.323558807373047
Epoch 660, val loss: 0.7212868332862854
Epoch 670, training loss: 12.961471557617188 = 0.31742504239082336 + 2.0 * 6.322023391723633
Epoch 670, val loss: 0.7162862420082092
Epoch 680, training loss: 12.950798988342285 = 0.3021564185619354 + 2.0 * 6.324321269989014
Epoch 680, val loss: 0.7116394639015198
Epoch 690, training loss: 12.93535041809082 = 0.28714603185653687 + 2.0 * 6.324102401733398
Epoch 690, val loss: 0.7074701189994812
Epoch 700, training loss: 12.905147552490234 = 0.27253982424736023 + 2.0 * 6.316303730010986
Epoch 700, val loss: 0.7037088871002197
Epoch 710, training loss: 12.890645980834961 = 0.2583273947238922 + 2.0 * 6.316159248352051
Epoch 710, val loss: 0.700472891330719
Epoch 720, training loss: 12.88033390045166 = 0.2445766180753708 + 2.0 * 6.317878723144531
Epoch 720, val loss: 0.6976520419120789
Epoch 730, training loss: 12.856057167053223 = 0.23138360679149628 + 2.0 * 6.3123369216918945
Epoch 730, val loss: 0.6953555345535278
Epoch 740, training loss: 12.840139389038086 = 0.21879489719867706 + 2.0 * 6.310672283172607
Epoch 740, val loss: 0.6936247944831848
Epoch 750, training loss: 12.827132225036621 = 0.20680966973304749 + 2.0 * 6.310161113739014
Epoch 750, val loss: 0.692461371421814
Epoch 760, training loss: 12.818561553955078 = 0.19544707238674164 + 2.0 * 6.311557292938232
Epoch 760, val loss: 0.6917463541030884
Epoch 770, training loss: 12.807415962219238 = 0.18475615978240967 + 2.0 * 6.3113298416137695
Epoch 770, val loss: 0.6915858387947083
Epoch 780, training loss: 12.789278030395508 = 0.1746869832277298 + 2.0 * 6.307295322418213
Epoch 780, val loss: 0.6917650699615479
Epoch 790, training loss: 12.774767875671387 = 0.16528570652008057 + 2.0 * 6.304740905761719
Epoch 790, val loss: 0.6925017237663269
Epoch 800, training loss: 12.764869689941406 = 0.15645848214626312 + 2.0 * 6.304205417633057
Epoch 800, val loss: 0.6935641765594482
Epoch 810, training loss: 12.761482238769531 = 0.14814406633377075 + 2.0 * 6.306669235229492
Epoch 810, val loss: 0.6950151920318604
Epoch 820, training loss: 12.743203163146973 = 0.14036265015602112 + 2.0 * 6.301420211791992
Epoch 820, val loss: 0.6968492865562439
Epoch 830, training loss: 12.735288619995117 = 0.1330897957086563 + 2.0 * 6.3010993003845215
Epoch 830, val loss: 0.6990536451339722
Epoch 840, training loss: 12.728400230407715 = 0.12625765800476074 + 2.0 * 6.3010711669921875
Epoch 840, val loss: 0.7014527916908264
Epoch 850, training loss: 12.719237327575684 = 0.11984333395957947 + 2.0 * 6.299696922302246
Epoch 850, val loss: 0.7040963172912598
Epoch 860, training loss: 12.709382057189941 = 0.11383802443742752 + 2.0 * 6.29777193069458
Epoch 860, val loss: 0.7069092988967896
Epoch 870, training loss: 12.700909614562988 = 0.1081923246383667 + 2.0 * 6.296358585357666
Epoch 870, val loss: 0.7100087404251099
Epoch 880, training loss: 12.700860977172852 = 0.10289385914802551 + 2.0 * 6.298983573913574
Epoch 880, val loss: 0.7131935358047485
Epoch 890, training loss: 12.685839653015137 = 0.09790980070829391 + 2.0 * 6.293964862823486
Epoch 890, val loss: 0.7165632843971252
Epoch 900, training loss: 12.688773155212402 = 0.0932263731956482 + 2.0 * 6.297773361206055
Epoch 900, val loss: 0.7200764417648315
Epoch 910, training loss: 12.675049781799316 = 0.08882156014442444 + 2.0 * 6.293114185333252
Epoch 910, val loss: 0.7235103249549866
Epoch 920, training loss: 12.666873931884766 = 0.08467911183834076 + 2.0 * 6.291097640991211
Epoch 920, val loss: 0.7272591590881348
Epoch 930, training loss: 12.676506042480469 = 0.0807914063334465 + 2.0 * 6.297857284545898
Epoch 930, val loss: 0.7309383749961853
Epoch 940, training loss: 12.661504745483398 = 0.07708416879177094 + 2.0 * 6.292210102081299
Epoch 940, val loss: 0.7348209619522095
Epoch 950, training loss: 12.651065826416016 = 0.07362730801105499 + 2.0 * 6.288719177246094
Epoch 950, val loss: 0.7386854290962219
Epoch 960, training loss: 12.649276733398438 = 0.07035557925701141 + 2.0 * 6.2894606590271
Epoch 960, val loss: 0.742612361907959
Epoch 970, training loss: 12.638504028320312 = 0.06726264208555222 + 2.0 * 6.28562068939209
Epoch 970, val loss: 0.7465449571609497
Epoch 980, training loss: 12.64309310913086 = 0.06434914469718933 + 2.0 * 6.289371967315674
Epoch 980, val loss: 0.7505351901054382
Epoch 990, training loss: 12.641642570495605 = 0.061586201190948486 + 2.0 * 6.290028095245361
Epoch 990, val loss: 0.7546105980873108
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8037037037037037
0.8386926726410122
The final CL Acc:0.79259, 0.02117, The final GNN Acc:0.83992, 0.00090
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.5 der2:0.4 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11676])
remove edge: torch.Size([2, 9412])
updated graph: torch.Size([2, 10532])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 19.134593963623047 = 1.9409093856811523 + 2.0 * 8.596841812133789
Epoch 0, val loss: 1.9484764337539673
Epoch 10, training loss: 19.124187469482422 = 1.9309618473052979 + 2.0 * 8.596612930297852
Epoch 10, val loss: 1.9378833770751953
Epoch 20, training loss: 19.1090087890625 = 1.9188909530639648 + 2.0 * 8.59505844116211
Epoch 20, val loss: 1.924748420715332
Epoch 30, training loss: 19.069828033447266 = 1.9023715257644653 + 2.0 * 8.583727836608887
Epoch 30, val loss: 1.9065965414047241
Epoch 40, training loss: 18.929189682006836 = 1.8809846639633179 + 2.0 * 8.524102210998535
Epoch 40, val loss: 1.8840696811676025
Epoch 50, training loss: 18.293180465698242 = 1.8603718280792236 + 2.0 * 8.21640396118164
Epoch 50, val loss: 1.8629313707351685
Epoch 60, training loss: 17.481290817260742 = 1.8428884744644165 + 2.0 * 7.819201469421387
Epoch 60, val loss: 1.8466105461120605
Epoch 70, training loss: 16.52543067932129 = 1.8309046030044556 + 2.0 * 7.347262859344482
Epoch 70, val loss: 1.8354084491729736
Epoch 80, training loss: 15.998226165771484 = 1.8201426267623901 + 2.0 * 7.089041709899902
Epoch 80, val loss: 1.8251560926437378
Epoch 90, training loss: 15.715696334838867 = 1.8077541589736938 + 2.0 * 6.953970909118652
Epoch 90, val loss: 1.8136236667633057
Epoch 100, training loss: 15.504419326782227 = 1.7951242923736572 + 2.0 * 6.854647636413574
Epoch 100, val loss: 1.8021992444992065
Epoch 110, training loss: 15.350051879882812 = 1.7832913398742676 + 2.0 * 6.783380508422852
Epoch 110, val loss: 1.791324257850647
Epoch 120, training loss: 15.224167823791504 = 1.7717819213867188 + 2.0 * 6.726192951202393
Epoch 120, val loss: 1.7806886434555054
Epoch 130, training loss: 15.127093315124512 = 1.7598440647125244 + 2.0 * 6.683624744415283
Epoch 130, val loss: 1.7696284055709839
Epoch 140, training loss: 15.044962882995605 = 1.7468520402908325 + 2.0 * 6.649055480957031
Epoch 140, val loss: 1.7577457427978516
Epoch 150, training loss: 14.973370552062988 = 1.7325000762939453 + 2.0 * 6.6204352378845215
Epoch 150, val loss: 1.7447197437286377
Epoch 160, training loss: 14.911304473876953 = 1.7165887355804443 + 2.0 * 6.597357749938965
Epoch 160, val loss: 1.7303524017333984
Epoch 170, training loss: 14.851311683654785 = 1.698663353919983 + 2.0 * 6.576323986053467
Epoch 170, val loss: 1.714422583580017
Epoch 180, training loss: 14.795280456542969 = 1.6785318851470947 + 2.0 * 6.558374404907227
Epoch 180, val loss: 1.6964714527130127
Epoch 190, training loss: 14.751898765563965 = 1.655919075012207 + 2.0 * 6.547989845275879
Epoch 190, val loss: 1.676376461982727
Epoch 200, training loss: 14.691601753234863 = 1.6306946277618408 + 2.0 * 6.530453681945801
Epoch 200, val loss: 1.654066801071167
Epoch 210, training loss: 14.64113998413086 = 1.6026837825775146 + 2.0 * 6.519227981567383
Epoch 210, val loss: 1.629339575767517
Epoch 220, training loss: 14.588665008544922 = 1.5717535018920898 + 2.0 * 6.508455753326416
Epoch 220, val loss: 1.6021238565444946
Epoch 230, training loss: 14.540149688720703 = 1.5379170179367065 + 2.0 * 6.5011162757873535
Epoch 230, val loss: 1.5727320909500122
Epoch 240, training loss: 14.483097076416016 = 1.5020453929901123 + 2.0 * 6.490525722503662
Epoch 240, val loss: 1.5413767099380493
Epoch 250, training loss: 14.428926467895508 = 1.4641470909118652 + 2.0 * 6.4823899269104
Epoch 250, val loss: 1.50892972946167
Epoch 260, training loss: 14.373502731323242 = 1.4243998527526855 + 2.0 * 6.474551677703857
Epoch 260, val loss: 1.4751648902893066
Epoch 270, training loss: 14.322112083435059 = 1.383475422859192 + 2.0 * 6.469318389892578
Epoch 270, val loss: 1.4407968521118164
Epoch 280, training loss: 14.264220237731934 = 1.3422470092773438 + 2.0 * 6.460986614227295
Epoch 280, val loss: 1.406707525253296
Epoch 290, training loss: 14.209193229675293 = 1.3009241819381714 + 2.0 * 6.454134464263916
Epoch 290, val loss: 1.3730695247650146
Epoch 300, training loss: 14.170705795288086 = 1.2597943544387817 + 2.0 * 6.455455780029297
Epoch 300, val loss: 1.3400251865386963
Epoch 310, training loss: 14.107560157775879 = 1.2193776369094849 + 2.0 * 6.444091320037842
Epoch 310, val loss: 1.3080953359603882
Epoch 320, training loss: 14.053720474243164 = 1.1796727180480957 + 2.0 * 6.437023639678955
Epoch 320, val loss: 1.2772523164749146
Epoch 330, training loss: 14.00384521484375 = 1.1405292749404907 + 2.0 * 6.431657791137695
Epoch 330, val loss: 1.247146487236023
Epoch 340, training loss: 13.95997142791748 = 1.1019898653030396 + 2.0 * 6.428990840911865
Epoch 340, val loss: 1.2180157899856567
Epoch 350, training loss: 13.909122467041016 = 1.0644581317901611 + 2.0 * 6.422332286834717
Epoch 350, val loss: 1.1899245977401733
Epoch 360, training loss: 13.861774444580078 = 1.0277445316314697 + 2.0 * 6.417015075683594
Epoch 360, val loss: 1.1629265546798706
Epoch 370, training loss: 13.823384284973145 = 0.9918091893196106 + 2.0 * 6.415787696838379
Epoch 370, val loss: 1.1369484663009644
Epoch 380, training loss: 13.776276588439941 = 0.9570268392562866 + 2.0 * 6.409625053405762
Epoch 380, val loss: 1.1121209859848022
Epoch 390, training loss: 13.740767478942871 = 0.9232361912727356 + 2.0 * 6.40876579284668
Epoch 390, val loss: 1.0884439945220947
Epoch 400, training loss: 13.6933012008667 = 0.8906402587890625 + 2.0 * 6.401330471038818
Epoch 400, val loss: 1.0660048723220825
Epoch 410, training loss: 13.657090187072754 = 0.8592681884765625 + 2.0 * 6.398910999298096
Epoch 410, val loss: 1.045016884803772
Epoch 420, training loss: 13.615923881530762 = 0.8290225863456726 + 2.0 * 6.393450736999512
Epoch 420, val loss: 1.025404691696167
Epoch 430, training loss: 13.579505920410156 = 0.800021767616272 + 2.0 * 6.389741897583008
Epoch 430, val loss: 1.0070215463638306
Epoch 440, training loss: 13.549304962158203 = 0.7719464302062988 + 2.0 * 6.388679027557373
Epoch 440, val loss: 0.989832878112793
Epoch 450, training loss: 13.51910400390625 = 0.745052695274353 + 2.0 * 6.387025833129883
Epoch 450, val loss: 0.9739392399787903
Epoch 460, training loss: 13.483716011047363 = 0.7191354036331177 + 2.0 * 6.382290363311768
Epoch 460, val loss: 0.9592877626419067
Epoch 470, training loss: 13.448453903198242 = 0.6942548155784607 + 2.0 * 6.377099514007568
Epoch 470, val loss: 0.9456798434257507
Epoch 480, training loss: 13.41800308227539 = 0.6701833009719849 + 2.0 * 6.373909950256348
Epoch 480, val loss: 0.9331175684928894
Epoch 490, training loss: 13.388739585876465 = 0.6468150019645691 + 2.0 * 6.370962142944336
Epoch 490, val loss: 0.9214591383934021
Epoch 500, training loss: 13.368366241455078 = 0.6241186857223511 + 2.0 * 6.372123718261719
Epoch 500, val loss: 0.9106692671775818
Epoch 510, training loss: 13.339557647705078 = 0.602196455001831 + 2.0 * 6.368680477142334
Epoch 510, val loss: 0.900658369064331
Epoch 520, training loss: 13.308492660522461 = 0.5809816122055054 + 2.0 * 6.363755702972412
Epoch 520, val loss: 0.8915799260139465
Epoch 530, training loss: 13.284719467163086 = 0.5602812767028809 + 2.0 * 6.362218856811523
Epoch 530, val loss: 0.8831568956375122
Epoch 540, training loss: 13.264187812805176 = 0.5400892496109009 + 2.0 * 6.362049102783203
Epoch 540, val loss: 0.8752466440200806
Epoch 550, training loss: 13.2424955368042 = 0.5204495787620544 + 2.0 * 6.36102294921875
Epoch 550, val loss: 0.8680369853973389
Epoch 560, training loss: 13.212968826293945 = 0.5013795495033264 + 2.0 * 6.355794429779053
Epoch 560, val loss: 0.8613962531089783
Epoch 570, training loss: 13.187536239624023 = 0.48275572061538696 + 2.0 * 6.352390289306641
Epoch 570, val loss: 0.8553332090377808
Epoch 580, training loss: 13.165124893188477 = 0.4645130932331085 + 2.0 * 6.350306034088135
Epoch 580, val loss: 0.8497552275657654
Epoch 590, training loss: 13.154866218566895 = 0.4466637372970581 + 2.0 * 6.354101181030273
Epoch 590, val loss: 0.8446357846260071
Epoch 600, training loss: 13.123454093933105 = 0.4292292892932892 + 2.0 * 6.34711217880249
Epoch 600, val loss: 0.8400301933288574
Epoch 610, training loss: 13.106940269470215 = 0.4122700095176697 + 2.0 * 6.347335338592529
Epoch 610, val loss: 0.8359199166297913
Epoch 620, training loss: 13.084246635437012 = 0.3957640826702118 + 2.0 * 6.344241142272949
Epoch 620, val loss: 0.8321807384490967
Epoch 630, training loss: 13.064489364624023 = 0.3796989321708679 + 2.0 * 6.342395305633545
Epoch 630, val loss: 0.829086184501648
Epoch 640, training loss: 13.0438871383667 = 0.3640015125274658 + 2.0 * 6.339942932128906
Epoch 640, val loss: 0.8264424800872803
Epoch 650, training loss: 13.024508476257324 = 0.34865283966064453 + 2.0 * 6.33792781829834
Epoch 650, val loss: 0.8242679238319397
Epoch 660, training loss: 13.023543357849121 = 0.33365166187286377 + 2.0 * 6.344945907592773
Epoch 660, val loss: 0.8225133419036865
Epoch 670, training loss: 12.992600440979004 = 0.3190837800502777 + 2.0 * 6.336758136749268
Epoch 670, val loss: 0.821290135383606
Epoch 680, training loss: 12.973569869995117 = 0.304889053106308 + 2.0 * 6.334340572357178
Epoch 680, val loss: 0.820631206035614
Epoch 690, training loss: 12.968907356262207 = 0.29108673334121704 + 2.0 * 6.338910102844238
Epoch 690, val loss: 0.8204725980758667
Epoch 700, training loss: 12.940654754638672 = 0.2778289318084717 + 2.0 * 6.3314127922058105
Epoch 700, val loss: 0.820828378200531
Epoch 710, training loss: 12.924337387084961 = 0.2649836540222168 + 2.0 * 6.329677104949951
Epoch 710, val loss: 0.8216973543167114
Epoch 720, training loss: 12.91448974609375 = 0.2525463104248047 + 2.0 * 6.330971717834473
Epoch 720, val loss: 0.8230637907981873
Epoch 730, training loss: 12.895140647888184 = 0.24052710831165314 + 2.0 * 6.327306747436523
Epoch 730, val loss: 0.8247880935668945
Epoch 740, training loss: 12.88828182220459 = 0.2289753258228302 + 2.0 * 6.329653263092041
Epoch 740, val loss: 0.8269661664962769
Epoch 750, training loss: 12.869180679321289 = 0.21793963015079498 + 2.0 * 6.325620651245117
Epoch 750, val loss: 0.8294433355331421
Epoch 760, training loss: 12.853348731994629 = 0.20739834010601044 + 2.0 * 6.322975158691406
Epoch 760, val loss: 0.8324804902076721
Epoch 770, training loss: 12.841543197631836 = 0.19729799032211304 + 2.0 * 6.322122573852539
Epoch 770, val loss: 0.835814356803894
Epoch 780, training loss: 12.835844039916992 = 0.18764886260032654 + 2.0 * 6.324097633361816
Epoch 780, val loss: 0.8394527435302734
Epoch 790, training loss: 12.82144546508789 = 0.17845529317855835 + 2.0 * 6.321495056152344
Epoch 790, val loss: 0.8434615731239319
Epoch 800, training loss: 12.815598487854004 = 0.16974732279777527 + 2.0 * 6.322925567626953
Epoch 800, val loss: 0.8477405309677124
Epoch 810, training loss: 12.796045303344727 = 0.16146740317344666 + 2.0 * 6.317288875579834
Epoch 810, val loss: 0.8522818088531494
Epoch 820, training loss: 12.785953521728516 = 0.1536167412996292 + 2.0 * 6.316168308258057
Epoch 820, val loss: 0.8571047186851501
Epoch 830, training loss: 12.77958869934082 = 0.1461651623249054 + 2.0 * 6.316711902618408
Epoch 830, val loss: 0.862066924571991
Epoch 840, training loss: 12.77586555480957 = 0.13908100128173828 + 2.0 * 6.318392276763916
Epoch 840, val loss: 0.8670293688774109
Epoch 850, training loss: 12.761746406555176 = 0.132451131939888 + 2.0 * 6.314647674560547
Epoch 850, val loss: 0.8723686337471008
Epoch 860, training loss: 12.748689651489258 = 0.1261395514011383 + 2.0 * 6.311275005340576
Epoch 860, val loss: 0.8777350783348083
Epoch 870, training loss: 12.741630554199219 = 0.12016754597425461 + 2.0 * 6.310731410980225
Epoch 870, val loss: 0.8832811117172241
Epoch 880, training loss: 12.740839958190918 = 0.11450506746768951 + 2.0 * 6.313167572021484
Epoch 880, val loss: 0.8887733817100525
Epoch 890, training loss: 12.732011795043945 = 0.10915585607290268 + 2.0 * 6.311428070068359
Epoch 890, val loss: 0.8943824172019958
Epoch 900, training loss: 12.718843460083008 = 0.10411793738603592 + 2.0 * 6.3073625564575195
Epoch 900, val loss: 0.9001039862632751
Epoch 910, training loss: 12.711385726928711 = 0.0993538573384285 + 2.0 * 6.306015968322754
Epoch 910, val loss: 0.9059410095214844
Epoch 920, training loss: 12.722402572631836 = 0.09483511000871658 + 2.0 * 6.313783645629883
Epoch 920, val loss: 0.9117698669433594
Epoch 930, training loss: 12.70798110961914 = 0.09057661145925522 + 2.0 * 6.30870246887207
Epoch 930, val loss: 0.9174662232398987
Epoch 940, training loss: 12.695673942565918 = 0.0865466445684433 + 2.0 * 6.304563522338867
Epoch 940, val loss: 0.9234833717346191
Epoch 950, training loss: 12.69006633758545 = 0.0827401876449585 + 2.0 * 6.30366325378418
Epoch 950, val loss: 0.9292696118354797
Epoch 960, training loss: 12.685297966003418 = 0.0791245847940445 + 2.0 * 6.303086757659912
Epoch 960, val loss: 0.93507981300354
Epoch 970, training loss: 12.687280654907227 = 0.07572681456804276 + 2.0 * 6.305777072906494
Epoch 970, val loss: 0.941048264503479
Epoch 980, training loss: 12.673285484313965 = 0.07249921560287476 + 2.0 * 6.300393104553223
Epoch 980, val loss: 0.9468400478363037
Epoch 990, training loss: 12.66782283782959 = 0.06945128738880157 + 2.0 * 6.299185752868652
Epoch 990, val loss: 0.9527305364608765
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8148148148148149
0.8176067474960464
=== training gcn model ===
Epoch 0, training loss: 19.15680694580078 = 1.9631800651550293 + 2.0 * 8.596813201904297
Epoch 0, val loss: 1.968664526939392
Epoch 10, training loss: 19.145095825195312 = 1.9521899223327637 + 2.0 * 8.596452713012695
Epoch 10, val loss: 1.9574334621429443
Epoch 20, training loss: 19.126720428466797 = 1.9382134675979614 + 2.0 * 8.594253540039062
Epoch 20, val loss: 1.942979097366333
Epoch 30, training loss: 19.077579498291016 = 1.918487548828125 + 2.0 * 8.579545974731445
Epoch 30, val loss: 1.9226843118667603
Epoch 40, training loss: 18.869592666625977 = 1.8932064771652222 + 2.0 * 8.48819351196289
Epoch 40, val loss: 1.897505521774292
Epoch 50, training loss: 17.865398406982422 = 1.8667796850204468 + 2.0 * 7.999309062957764
Epoch 50, val loss: 1.8715111017227173
Epoch 60, training loss: 16.84674072265625 = 1.844383716583252 + 2.0 * 7.50117826461792
Epoch 60, val loss: 1.850763201713562
Epoch 70, training loss: 16.19195556640625 = 1.8314083814620972 + 2.0 * 7.180273532867432
Epoch 70, val loss: 1.8382128477096558
Epoch 80, training loss: 15.92074203491211 = 1.8163031339645386 + 2.0 * 7.052219390869141
Epoch 80, val loss: 1.8233352899551392
Epoch 90, training loss: 15.741045951843262 = 1.8001073598861694 + 2.0 * 6.9704694747924805
Epoch 90, val loss: 1.808337926864624
Epoch 100, training loss: 15.621000289916992 = 1.784428358078003 + 2.0 * 6.918285846710205
Epoch 100, val loss: 1.7945501804351807
Epoch 110, training loss: 15.506567001342773 = 1.7721041440963745 + 2.0 * 6.867231369018555
Epoch 110, val loss: 1.7835060358047485
Epoch 120, training loss: 15.38921070098877 = 1.7613779306411743 + 2.0 * 6.813916206359863
Epoch 120, val loss: 1.7731938362121582
Epoch 130, training loss: 15.280473709106445 = 1.7500510215759277 + 2.0 * 6.76521110534668
Epoch 130, val loss: 1.7621580362319946
Epoch 140, training loss: 15.184357643127441 = 1.7373850345611572 + 2.0 * 6.723486423492432
Epoch 140, val loss: 1.7500637769699097
Epoch 150, training loss: 15.105245590209961 = 1.722981333732605 + 2.0 * 6.691132068634033
Epoch 150, val loss: 1.7366777658462524
Epoch 160, training loss: 15.026952743530273 = 1.707047939300537 + 2.0 * 6.659952640533447
Epoch 160, val loss: 1.7222630977630615
Epoch 170, training loss: 14.949543952941895 = 1.689648985862732 + 2.0 * 6.629947662353516
Epoch 170, val loss: 1.7068707942962646
Epoch 180, training loss: 14.878125190734863 = 1.670348048210144 + 2.0 * 6.603888511657715
Epoch 180, val loss: 1.690076470375061
Epoch 190, training loss: 14.811084747314453 = 1.648876667022705 + 2.0 * 6.581103801727295
Epoch 190, val loss: 1.671598196029663
Epoch 200, training loss: 14.74406909942627 = 1.6250640153884888 + 2.0 * 6.559502601623535
Epoch 200, val loss: 1.6513553857803345
Epoch 210, training loss: 14.683624267578125 = 1.5988059043884277 + 2.0 * 6.5424089431762695
Epoch 210, val loss: 1.6292943954467773
Epoch 220, training loss: 14.619842529296875 = 1.5703125 + 2.0 * 6.5247650146484375
Epoch 220, val loss: 1.605582356452942
Epoch 230, training loss: 14.560663223266602 = 1.5396350622177124 + 2.0 * 6.510514259338379
Epoch 230, val loss: 1.580427646636963
Epoch 240, training loss: 14.506258010864258 = 1.5071579217910767 + 2.0 * 6.499549865722656
Epoch 240, val loss: 1.554282307624817
Epoch 250, training loss: 14.45185661315918 = 1.4738132953643799 + 2.0 * 6.4890217781066895
Epoch 250, val loss: 1.5279637575149536
Epoch 260, training loss: 14.3965482711792 = 1.439775824546814 + 2.0 * 6.478386402130127
Epoch 260, val loss: 1.5014324188232422
Epoch 270, training loss: 14.343510627746582 = 1.4050217866897583 + 2.0 * 6.469244480133057
Epoch 270, val loss: 1.4746861457824707
Epoch 280, training loss: 14.298016548156738 = 1.3697401285171509 + 2.0 * 6.464138031005859
Epoch 280, val loss: 1.448046088218689
Epoch 290, training loss: 14.244599342346191 = 1.3346081972122192 + 2.0 * 6.454995632171631
Epoch 290, val loss: 1.4221221208572388
Epoch 300, training loss: 14.19365406036377 = 1.2997270822525024 + 2.0 * 6.446963310241699
Epoch 300, val loss: 1.3967735767364502
Epoch 310, training loss: 14.144908905029297 = 1.2649691104888916 + 2.0 * 6.439970016479492
Epoch 310, val loss: 1.3717106580734253
Epoch 320, training loss: 14.105350494384766 = 1.2302175760269165 + 2.0 * 6.43756628036499
Epoch 320, val loss: 1.3468915224075317
Epoch 330, training loss: 14.053017616271973 = 1.195613145828247 + 2.0 * 6.428702354431152
Epoch 330, val loss: 1.3224248886108398
Epoch 340, training loss: 14.010464668273926 = 1.1611617803573608 + 2.0 * 6.424651622772217
Epoch 340, val loss: 1.2981442213058472
Epoch 350, training loss: 13.962027549743652 = 1.1268020868301392 + 2.0 * 6.417612552642822
Epoch 350, val loss: 1.2739120721817017
Epoch 360, training loss: 13.919572830200195 = 1.092370867729187 + 2.0 * 6.413600921630859
Epoch 360, val loss: 1.24970543384552
Epoch 370, training loss: 13.877784729003906 = 1.0580960512161255 + 2.0 * 6.409844398498535
Epoch 370, val loss: 1.2256014347076416
Epoch 380, training loss: 13.837336540222168 = 1.024008870124817 + 2.0 * 6.40666389465332
Epoch 380, val loss: 1.2015538215637207
Epoch 390, training loss: 13.791088104248047 = 0.9902827739715576 + 2.0 * 6.400402545928955
Epoch 390, val loss: 1.1777575016021729
Epoch 400, training loss: 13.74984073638916 = 0.9567596912384033 + 2.0 * 6.396540641784668
Epoch 400, val loss: 1.1540772914886475
Epoch 410, training loss: 13.709009170532227 = 0.9235422015190125 + 2.0 * 6.392733573913574
Epoch 410, val loss: 1.1305962800979614
Epoch 420, training loss: 13.674356460571289 = 0.8909305930137634 + 2.0 * 6.3917131423950195
Epoch 420, val loss: 1.1075090169906616
Epoch 430, training loss: 13.630317687988281 = 0.8590945601463318 + 2.0 * 6.385611534118652
Epoch 430, val loss: 1.0851091146469116
Epoch 440, training loss: 13.592696189880371 = 0.8281180262565613 + 2.0 * 6.382288932800293
Epoch 440, val loss: 1.0633724927902222
Epoch 450, training loss: 13.563124656677246 = 0.7979906797409058 + 2.0 * 6.382566928863525
Epoch 450, val loss: 1.04246187210083
Epoch 460, training loss: 13.524829864501953 = 0.7690714001655579 + 2.0 * 6.3778791427612305
Epoch 460, val loss: 1.0229527950286865
Epoch 470, training loss: 13.49136734008789 = 0.741461992263794 + 2.0 * 6.374952793121338
Epoch 470, val loss: 1.004929780960083
Epoch 480, training loss: 13.45559310913086 = 0.7152459025382996 + 2.0 * 6.370173454284668
Epoch 480, val loss: 0.9883481860160828
Epoch 490, training loss: 13.426725387573242 = 0.6902563571929932 + 2.0 * 6.368234634399414
Epoch 490, val loss: 0.9733520746231079
Epoch 500, training loss: 13.397832870483398 = 0.6665043830871582 + 2.0 * 6.365664005279541
Epoch 500, val loss: 0.9599392414093018
Epoch 510, training loss: 13.375621795654297 = 0.6439929008483887 + 2.0 * 6.365814208984375
Epoch 510, val loss: 0.9480770230293274
Epoch 520, training loss: 13.344206809997559 = 0.62261563539505 + 2.0 * 6.360795497894287
Epoch 520, val loss: 0.9376603960990906
Epoch 530, training loss: 13.321213722229004 = 0.6021957397460938 + 2.0 * 6.359508991241455
Epoch 530, val loss: 0.9283742308616638
Epoch 540, training loss: 13.296348571777344 = 0.5826784372329712 + 2.0 * 6.356834888458252
Epoch 540, val loss: 0.9203061461448669
Epoch 550, training loss: 13.270546913146973 = 0.5639265775680542 + 2.0 * 6.3533101081848145
Epoch 550, val loss: 0.9132683873176575
Epoch 560, training loss: 13.249147415161133 = 0.545840859413147 + 2.0 * 6.351653099060059
Epoch 560, val loss: 0.9070361852645874
Epoch 570, training loss: 13.229663848876953 = 0.5283792614936829 + 2.0 * 6.350642204284668
Epoch 570, val loss: 0.9015693068504333
Epoch 580, training loss: 13.215197563171387 = 0.5114551186561584 + 2.0 * 6.351871013641357
Epoch 580, val loss: 0.8966934680938721
Epoch 590, training loss: 13.188582420349121 = 0.495064377784729 + 2.0 * 6.346758842468262
Epoch 590, val loss: 0.8922668099403381
Epoch 600, training loss: 13.1668701171875 = 0.4790409207344055 + 2.0 * 6.34391450881958
Epoch 600, val loss: 0.8882511258125305
Epoch 610, training loss: 13.155454635620117 = 0.463290810585022 + 2.0 * 6.346081733703613
Epoch 610, val loss: 0.8844335079193115
Epoch 620, training loss: 13.127819061279297 = 0.4478379487991333 + 2.0 * 6.339990615844727
Epoch 620, val loss: 0.880925178527832
Epoch 630, training loss: 13.108134269714355 = 0.432589590549469 + 2.0 * 6.337772369384766
Epoch 630, val loss: 0.8775352835655212
Epoch 640, training loss: 13.090703964233398 = 0.41747310757637024 + 2.0 * 6.336615562438965
Epoch 640, val loss: 0.8742882609367371
Epoch 650, training loss: 13.072037696838379 = 0.402496874332428 + 2.0 * 6.334770202636719
Epoch 650, val loss: 0.8713260889053345
Epoch 660, training loss: 13.054882049560547 = 0.38774776458740234 + 2.0 * 6.333567142486572
Epoch 660, val loss: 0.8684424161911011
Epoch 670, training loss: 13.045669555664062 = 0.3731743097305298 + 2.0 * 6.336247444152832
Epoch 670, val loss: 0.8658574819564819
Epoch 680, training loss: 13.021594047546387 = 0.3588394522666931 + 2.0 * 6.3313775062561035
Epoch 680, val loss: 0.8634567260742188
Epoch 690, training loss: 13.002082824707031 = 0.3447599411010742 + 2.0 * 6.3286614418029785
Epoch 690, val loss: 0.8613729476928711
Epoch 700, training loss: 12.994007110595703 = 0.3309406042098999 + 2.0 * 6.331533432006836
Epoch 700, val loss: 0.8596013784408569
Epoch 710, training loss: 12.970730781555176 = 0.31742173433303833 + 2.0 * 6.326654434204102
Epoch 710, val loss: 0.8581341505050659
Epoch 720, training loss: 12.95162296295166 = 0.30418434739112854 + 2.0 * 6.323719501495361
Epoch 720, val loss: 0.857012927532196
Epoch 730, training loss: 12.953397750854492 = 0.2912181615829468 + 2.0 * 6.331089973449707
Epoch 730, val loss: 0.8562332987785339
Epoch 740, training loss: 12.922955513000488 = 0.2785300314426422 + 2.0 * 6.3222126960754395
Epoch 740, val loss: 0.8559272289276123
Epoch 750, training loss: 12.906903266906738 = 0.26613742113113403 + 2.0 * 6.320383071899414
Epoch 750, val loss: 0.8561283349990845
Epoch 760, training loss: 12.89089298248291 = 0.25391796231269836 + 2.0 * 6.318487644195557
Epoch 760, val loss: 0.8566092252731323
Epoch 770, training loss: 12.88154125213623 = 0.24183380603790283 + 2.0 * 6.319853782653809
Epoch 770, val loss: 0.8575843572616577
Epoch 780, training loss: 12.863933563232422 = 0.22993038594722748 + 2.0 * 6.317001819610596
Epoch 780, val loss: 0.8589101433753967
Epoch 790, training loss: 12.85155963897705 = 0.21821951866149902 + 2.0 * 6.316669940948486
Epoch 790, val loss: 0.8608195185661316
Epoch 800, training loss: 12.839289665222168 = 0.2067275196313858 + 2.0 * 6.316280841827393
Epoch 800, val loss: 0.8631566166877747
Epoch 810, training loss: 12.821209907531738 = 0.19550593197345734 + 2.0 * 6.312851905822754
Epoch 810, val loss: 0.8660706877708435
Epoch 820, training loss: 12.806803703308105 = 0.1846112310886383 + 2.0 * 6.31109619140625
Epoch 820, val loss: 0.8695510625839233
Epoch 830, training loss: 12.799976348876953 = 0.17412032186985016 + 2.0 * 6.312928199768066
Epoch 830, val loss: 0.8735525012016296
Epoch 840, training loss: 12.791868209838867 = 0.16415706276893616 + 2.0 * 6.3138556480407715
Epoch 840, val loss: 0.878053605556488
Epoch 850, training loss: 12.771133422851562 = 0.15475517511367798 + 2.0 * 6.3081889152526855
Epoch 850, val loss: 0.883270800113678
Epoch 860, training loss: 12.758550643920898 = 0.14587895572185516 + 2.0 * 6.306335926055908
Epoch 860, val loss: 0.8888067603111267
Epoch 870, training loss: 12.748446464538574 = 0.13751102983951569 + 2.0 * 6.30546760559082
Epoch 870, val loss: 0.894817054271698
Epoch 880, training loss: 12.760451316833496 = 0.129661425948143 + 2.0 * 6.315394878387451
Epoch 880, val loss: 0.901161789894104
Epoch 890, training loss: 12.73587703704834 = 0.12236226350069046 + 2.0 * 6.30675745010376
Epoch 890, val loss: 0.9076519012451172
Epoch 900, training loss: 12.725076675415039 = 0.1155688539147377 + 2.0 * 6.30475378036499
Epoch 900, val loss: 0.9144639372825623
Epoch 910, training loss: 12.71168327331543 = 0.10923611372709274 + 2.0 * 6.3012237548828125
Epoch 910, val loss: 0.9213520288467407
Epoch 920, training loss: 12.705599784851074 = 0.10330893844366074 + 2.0 * 6.301145553588867
Epoch 920, val loss: 0.9285261631011963
Epoch 930, training loss: 12.70443344116211 = 0.09776777774095535 + 2.0 * 6.303332805633545
Epoch 930, val loss: 0.9357553720474243
Epoch 940, training loss: 12.691925048828125 = 0.0926046073436737 + 2.0 * 6.2996602058410645
Epoch 940, val loss: 0.9430173635482788
Epoch 950, training loss: 12.684513092041016 = 0.0877804085612297 + 2.0 * 6.298366546630859
Epoch 950, val loss: 0.9504689574241638
Epoch 960, training loss: 12.686528205871582 = 0.08327595889568329 + 2.0 * 6.301626205444336
Epoch 960, val loss: 0.9579213261604309
Epoch 970, training loss: 12.672762870788574 = 0.07907398045063019 + 2.0 * 6.296844482421875
Epoch 970, val loss: 0.9653593897819519
Epoch 980, training loss: 12.664986610412598 = 0.07516433298587799 + 2.0 * 6.294910907745361
Epoch 980, val loss: 0.972859263420105
Epoch 990, training loss: 12.659384727478027 = 0.0715017095208168 + 2.0 * 6.293941497802734
Epoch 990, val loss: 0.9802706241607666
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7666666666666667
0.8176067474960464
=== training gcn model ===
Epoch 0, training loss: 19.141870498657227 = 1.948194146156311 + 2.0 * 8.596837997436523
Epoch 0, val loss: 1.9513726234436035
Epoch 10, training loss: 19.130762100219727 = 1.9377102851867676 + 2.0 * 8.596526145935059
Epoch 10, val loss: 1.94061279296875
Epoch 20, training loss: 19.11277198791504 = 1.924254298210144 + 2.0 * 8.594259262084961
Epoch 20, val loss: 1.9267568588256836
Epoch 30, training loss: 19.060997009277344 = 1.9050532579421997 + 2.0 * 8.577971458435059
Epoch 30, val loss: 1.907308578491211
Epoch 40, training loss: 18.828195571899414 = 1.8810749053955078 + 2.0 * 8.473560333251953
Epoch 40, val loss: 1.8845101594924927
Epoch 50, training loss: 18.1862735748291 = 1.8570513725280762 + 2.0 * 8.164610862731934
Epoch 50, val loss: 1.862111210823059
Epoch 60, training loss: 17.35169792175293 = 1.839263677597046 + 2.0 * 7.756217002868652
Epoch 60, val loss: 1.84566068649292
Epoch 70, training loss: 16.403627395629883 = 1.8279556035995483 + 2.0 * 7.287836074829102
Epoch 70, val loss: 1.8349623680114746
Epoch 80, training loss: 15.874897956848145 = 1.8193073272705078 + 2.0 * 7.027795314788818
Epoch 80, val loss: 1.8260208368301392
Epoch 90, training loss: 15.54422378540039 = 1.8078727722167969 + 2.0 * 6.868175506591797
Epoch 90, val loss: 1.8144965171813965
Epoch 100, training loss: 15.367229461669922 = 1.7947665452957153 + 2.0 * 6.786231517791748
Epoch 100, val loss: 1.8013983964920044
Epoch 110, training loss: 15.233799934387207 = 1.7820621728897095 + 2.0 * 6.7258687019348145
Epoch 110, val loss: 1.7887059450149536
Epoch 120, training loss: 15.12799072265625 = 1.7703733444213867 + 2.0 * 6.678808689117432
Epoch 120, val loss: 1.7769972085952759
Epoch 130, training loss: 15.044085502624512 = 1.7586214542388916 + 2.0 * 6.6427321434021
Epoch 130, val loss: 1.7654997110366821
Epoch 140, training loss: 14.96780014038086 = 1.745949149131775 + 2.0 * 6.610925674438477
Epoch 140, val loss: 1.753609299659729
Epoch 150, training loss: 14.907297134399414 = 1.7320846319198608 + 2.0 * 6.587606430053711
Epoch 150, val loss: 1.7408275604248047
Epoch 160, training loss: 14.845718383789062 = 1.7166905403137207 + 2.0 * 6.564513683319092
Epoch 160, val loss: 1.7269282341003418
Epoch 170, training loss: 14.792228698730469 = 1.6994543075561523 + 2.0 * 6.546387195587158
Epoch 170, val loss: 1.7115293741226196
Epoch 180, training loss: 14.741952896118164 = 1.680206298828125 + 2.0 * 6.5308732986450195
Epoch 180, val loss: 1.6943529844284058
Epoch 190, training loss: 14.688737869262695 = 1.658590316772461 + 2.0 * 6.515073776245117
Epoch 190, val loss: 1.6752599477767944
Epoch 200, training loss: 14.639702796936035 = 1.6344969272613525 + 2.0 * 6.502603054046631
Epoch 200, val loss: 1.6539604663848877
Epoch 210, training loss: 14.595562934875488 = 1.607850432395935 + 2.0 * 6.493856430053711
Epoch 210, val loss: 1.6305192708969116
Epoch 220, training loss: 14.539447784423828 = 1.5788078308105469 + 2.0 * 6.480319976806641
Epoch 220, val loss: 1.6050968170166016
Epoch 230, training loss: 14.489803314208984 = 1.5474222898483276 + 2.0 * 6.471190452575684
Epoch 230, val loss: 1.5778756141662598
Epoch 240, training loss: 14.443205833435059 = 1.513856053352356 + 2.0 * 6.464674949645996
Epoch 240, val loss: 1.5490260124206543
Epoch 250, training loss: 14.388203620910645 = 1.4786474704742432 + 2.0 * 6.45477819442749
Epoch 250, val loss: 1.519083023071289
Epoch 260, training loss: 14.335022926330566 = 1.4420582056045532 + 2.0 * 6.446482181549072
Epoch 260, val loss: 1.4884945154190063
Epoch 270, training loss: 14.291831016540527 = 1.404535174369812 + 2.0 * 6.443647861480713
Epoch 270, val loss: 1.457566738128662
Epoch 280, training loss: 14.236451148986816 = 1.3665525913238525 + 2.0 * 6.4349493980407715
Epoch 280, val loss: 1.4272218942642212
Epoch 290, training loss: 14.183788299560547 = 1.3286986351013184 + 2.0 * 6.427544593811035
Epoch 290, val loss: 1.3976246118545532
Epoch 300, training loss: 14.134937286376953 = 1.2909048795700073 + 2.0 * 6.422016143798828
Epoch 300, val loss: 1.3688220977783203
Epoch 310, training loss: 14.094054222106934 = 1.2533884048461914 + 2.0 * 6.420332908630371
Epoch 310, val loss: 1.3409656286239624
Epoch 320, training loss: 14.043841361999512 = 1.216622233390808 + 2.0 * 6.413609504699707
Epoch 320, val loss: 1.3143409490585327
Epoch 330, training loss: 13.996192932128906 = 1.1803905963897705 + 2.0 * 6.407901287078857
Epoch 330, val loss: 1.288873553276062
Epoch 340, training loss: 13.963166236877441 = 1.1447752714157104 + 2.0 * 6.409195423126221
Epoch 340, val loss: 1.264419674873352
Epoch 350, training loss: 13.91203498840332 = 1.1098933219909668 + 2.0 * 6.401070594787598
Epoch 350, val loss: 1.2409027814865112
Epoch 360, training loss: 13.867137908935547 = 1.0757558345794678 + 2.0 * 6.39569091796875
Epoch 360, val loss: 1.2185395956039429
Epoch 370, training loss: 13.83065414428711 = 1.0422805547714233 + 2.0 * 6.394186973571777
Epoch 370, val loss: 1.1970901489257812
Epoch 380, training loss: 13.791624069213867 = 1.009774088859558 + 2.0 * 6.39092493057251
Epoch 380, val loss: 1.1764899492263794
Epoch 390, training loss: 13.748278617858887 = 0.9781073331832886 + 2.0 * 6.385085582733154
Epoch 390, val loss: 1.1569836139678955
Epoch 400, training loss: 13.711177825927734 = 0.947411060333252 + 2.0 * 6.38188362121582
Epoch 400, val loss: 1.1383503675460815
Epoch 410, training loss: 13.676850318908691 = 0.9176886677742004 + 2.0 * 6.379580974578857
Epoch 410, val loss: 1.120631217956543
Epoch 420, training loss: 13.640402793884277 = 0.8890597224235535 + 2.0 * 6.37567138671875
Epoch 420, val loss: 1.1040325164794922
Epoch 430, training loss: 13.60969352722168 = 0.8616032600402832 + 2.0 * 6.374044895172119
Epoch 430, val loss: 1.0884135961532593
Epoch 440, training loss: 13.577171325683594 = 0.8353005051612854 + 2.0 * 6.370935440063477
Epoch 440, val loss: 1.0739717483520508
Epoch 450, training loss: 13.548335075378418 = 0.8102249503135681 + 2.0 * 6.369055271148682
Epoch 450, val loss: 1.0605518817901611
Epoch 460, training loss: 13.513985633850098 = 0.7862434983253479 + 2.0 * 6.363871097564697
Epoch 460, val loss: 1.0483094453811646
Epoch 470, training loss: 13.486212730407715 = 0.7632739543914795 + 2.0 * 6.361469268798828
Epoch 470, val loss: 1.0371191501617432
Epoch 480, training loss: 13.471211433410645 = 0.7413171529769897 + 2.0 * 6.364947319030762
Epoch 480, val loss: 1.0267201662063599
Epoch 490, training loss: 13.43460464477539 = 0.7204101085662842 + 2.0 * 6.357097148895264
Epoch 490, val loss: 1.0175882577896118
Epoch 500, training loss: 13.408226013183594 = 0.7003559470176697 + 2.0 * 6.353935241699219
Epoch 500, val loss: 1.0092254877090454
Epoch 510, training loss: 13.384443283081055 = 0.6808862090110779 + 2.0 * 6.351778507232666
Epoch 510, val loss: 1.0016764402389526
Epoch 520, training loss: 13.36117935180664 = 0.6619910001754761 + 2.0 * 6.3495941162109375
Epoch 520, val loss: 0.994641900062561
Epoch 530, training loss: 13.339539527893066 = 0.6436012387275696 + 2.0 * 6.347969055175781
Epoch 530, val loss: 0.9882369637489319
Epoch 540, training loss: 13.315362930297852 = 0.6256101727485657 + 2.0 * 6.344876289367676
Epoch 540, val loss: 0.982281506061554
Epoch 550, training loss: 13.305432319641113 = 0.6079239249229431 + 2.0 * 6.348754405975342
Epoch 550, val loss: 0.9766784906387329
Epoch 560, training loss: 13.271265029907227 = 0.5904438495635986 + 2.0 * 6.3404107093811035
Epoch 560, val loss: 0.9715721011161804
Epoch 570, training loss: 13.25239086151123 = 0.5731763243675232 + 2.0 * 6.339607238769531
Epoch 570, val loss: 0.9667587876319885
Epoch 580, training loss: 13.246833801269531 = 0.555993378162384 + 2.0 * 6.3454203605651855
Epoch 580, val loss: 0.9623172283172607
Epoch 590, training loss: 13.212471008300781 = 0.5390154719352722 + 2.0 * 6.336727619171143
Epoch 590, val loss: 0.9580966234207153
Epoch 600, training loss: 13.190447807312012 = 0.5220569968223572 + 2.0 * 6.334195613861084
Epoch 600, val loss: 0.9542226195335388
Epoch 610, training loss: 13.175869941711426 = 0.5051475763320923 + 2.0 * 6.335361003875732
Epoch 610, val loss: 0.95065838098526
Epoch 620, training loss: 13.150175094604492 = 0.4881870448589325 + 2.0 * 6.330994129180908
Epoch 620, val loss: 0.9472824931144714
Epoch 630, training loss: 13.132347106933594 = 0.4713186025619507 + 2.0 * 6.330514430999756
Epoch 630, val loss: 0.9443733096122742
Epoch 640, training loss: 13.109354972839355 = 0.4544861912727356 + 2.0 * 6.327434539794922
Epoch 640, val loss: 0.9416561722755432
Epoch 650, training loss: 13.091336250305176 = 0.4377022981643677 + 2.0 * 6.326817035675049
Epoch 650, val loss: 0.9391844868659973
Epoch 660, training loss: 13.078283309936523 = 0.42104989290237427 + 2.0 * 6.328616619110107
Epoch 660, val loss: 0.9371145963668823
Epoch 670, training loss: 13.051217079162598 = 0.40448734164237976 + 2.0 * 6.323364734649658
Epoch 670, val loss: 0.9353233575820923
Epoch 680, training loss: 13.06013011932373 = 0.3882697820663452 + 2.0 * 6.335930347442627
Epoch 680, val loss: 0.9337005019187927
Epoch 690, training loss: 13.017728805541992 = 0.3724394142627716 + 2.0 * 6.3226447105407715
Epoch 690, val loss: 0.9326592087745667
Epoch 700, training loss: 12.994119644165039 = 0.35694772005081177 + 2.0 * 6.3185858726501465
Epoch 700, val loss: 0.9320033192634583
Epoch 710, training loss: 12.976364135742188 = 0.3418586254119873 + 2.0 * 6.3172526359558105
Epoch 710, val loss: 0.931789755821228
Epoch 720, training loss: 12.974325180053711 = 0.3271760642528534 + 2.0 * 6.323574542999268
Epoch 720, val loss: 0.9320257306098938
Epoch 730, training loss: 12.952856063842773 = 0.31301194429397583 + 2.0 * 6.319921970367432
Epoch 730, val loss: 0.9324305653572083
Epoch 740, training loss: 12.926190376281738 = 0.2992825210094452 + 2.0 * 6.3134541511535645
Epoch 740, val loss: 0.9333618879318237
Epoch 750, training loss: 12.911977767944336 = 0.2860722243785858 + 2.0 * 6.312952995300293
Epoch 750, val loss: 0.9346584677696228
Epoch 760, training loss: 12.907318115234375 = 0.2733533978462219 + 2.0 * 6.316982269287109
Epoch 760, val loss: 0.9362993240356445
Epoch 770, training loss: 12.883540153503418 = 0.26103124022483826 + 2.0 * 6.311254501342773
Epoch 770, val loss: 0.9382121562957764
Epoch 780, training loss: 12.870759963989258 = 0.24923962354660034 + 2.0 * 6.310760021209717
Epoch 780, val loss: 0.9405368566513062
Epoch 790, training loss: 12.856907844543457 = 0.23788827657699585 + 2.0 * 6.309509754180908
Epoch 790, val loss: 0.9431546926498413
Epoch 800, training loss: 12.853045463562012 = 0.22698551416397095 + 2.0 * 6.313029766082764
Epoch 800, val loss: 0.9460369944572449
Epoch 810, training loss: 12.829084396362305 = 0.21653743088245392 + 2.0 * 6.306273460388184
Epoch 810, val loss: 0.9491302371025085
Epoch 820, training loss: 12.816387176513672 = 0.20651941001415253 + 2.0 * 6.304934024810791
Epoch 820, val loss: 0.9526757001876831
Epoch 830, training loss: 12.8080415725708 = 0.19688460230827332 + 2.0 * 6.305578708648682
Epoch 830, val loss: 0.9563060402870178
Epoch 840, training loss: 12.7971830368042 = 0.18769563734531403 + 2.0 * 6.304743766784668
Epoch 840, val loss: 0.9600133895874023
Epoch 850, training loss: 12.783209800720215 = 0.17890305817127228 + 2.0 * 6.302153587341309
Epoch 850, val loss: 0.9642765522003174
Epoch 860, training loss: 12.778254508972168 = 0.17054888606071472 + 2.0 * 6.3038530349731445
Epoch 860, val loss: 0.9684264063835144
Epoch 870, training loss: 12.762852668762207 = 0.16254793107509613 + 2.0 * 6.30015230178833
Epoch 870, val loss: 0.9726752638816833
Epoch 880, training loss: 12.751943588256836 = 0.15495961904525757 + 2.0 * 6.298491954803467
Epoch 880, val loss: 0.9772343635559082
Epoch 890, training loss: 12.74472427368164 = 0.14773139357566833 + 2.0 * 6.298496246337891
Epoch 890, val loss: 0.9818186163902283
Epoch 900, training loss: 12.746031761169434 = 0.14085297286510468 + 2.0 * 6.302589416503906
Epoch 900, val loss: 0.9864799380302429
Epoch 910, training loss: 12.728103637695312 = 0.13433369994163513 + 2.0 * 6.296885013580322
Epoch 910, val loss: 0.9913777709007263
Epoch 920, training loss: 12.719270706176758 = 0.12814602255821228 + 2.0 * 6.295562267303467
Epoch 920, val loss: 0.9962118864059448
Epoch 930, training loss: 12.729857444763184 = 0.12229431420564651 + 2.0 * 6.303781509399414
Epoch 930, val loss: 1.0010111331939697
Epoch 940, training loss: 12.705873489379883 = 0.11669919639825821 + 2.0 * 6.294587135314941
Epoch 940, val loss: 1.0061612129211426
Epoch 950, training loss: 12.696715354919434 = 0.11144697666168213 + 2.0 * 6.292634010314941
Epoch 950, val loss: 1.01108717918396
Epoch 960, training loss: 12.690359115600586 = 0.10646386444568634 + 2.0 * 6.291947841644287
Epoch 960, val loss: 1.016192078590393
Epoch 970, training loss: 12.707438468933105 = 0.10173068195581436 + 2.0 * 6.302854061126709
Epoch 970, val loss: 1.021217942237854
Epoch 980, training loss: 12.679642677307129 = 0.0972527414560318 + 2.0 * 6.291194915771484
Epoch 980, val loss: 1.0265119075775146
Epoch 990, training loss: 12.673099517822266 = 0.09302070736885071 + 2.0 * 6.290039539337158
Epoch 990, val loss: 1.0317413806915283
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.737037037037037
0.8165524512387982
The final CL Acc:0.77284, 0.03205, The final GNN Acc:0.81726, 0.00050
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13276])
remove edge: torch.Size([2, 8070])
updated graph: torch.Size([2, 10790])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 19.14506721496582 = 1.951453685760498 + 2.0 * 8.596806526184082
Epoch 0, val loss: 1.9429434537887573
Epoch 10, training loss: 19.133071899414062 = 1.9403152465820312 + 2.0 * 8.596378326416016
Epoch 10, val loss: 1.932444453239441
Epoch 20, training loss: 19.11288833618164 = 1.9265642166137695 + 2.0 * 8.593162536621094
Epoch 20, val loss: 1.9191365242004395
Epoch 30, training loss: 19.047950744628906 = 1.9080408811569214 + 2.0 * 8.569954872131348
Epoch 30, val loss: 1.9008748531341553
Epoch 40, training loss: 18.74751091003418 = 1.8859608173370361 + 2.0 * 8.430774688720703
Epoch 40, val loss: 1.879954218864441
Epoch 50, training loss: 17.926069259643555 = 1.8624475002288818 + 2.0 * 8.031810760498047
Epoch 50, val loss: 1.857728362083435
Epoch 60, training loss: 17.07952308654785 = 1.843352198600769 + 2.0 * 7.618085861206055
Epoch 60, val loss: 1.8404500484466553
Epoch 70, training loss: 16.321887969970703 = 1.8313709497451782 + 2.0 * 7.245258331298828
Epoch 70, val loss: 1.8291276693344116
Epoch 80, training loss: 15.867100715637207 = 1.818454384803772 + 2.0 * 7.024322986602783
Epoch 80, val loss: 1.8168123960494995
Epoch 90, training loss: 15.602859497070312 = 1.8037447929382324 + 2.0 * 6.899557590484619
Epoch 90, val loss: 1.8040522336959839
Epoch 100, training loss: 15.421374320983887 = 1.7899229526519775 + 2.0 * 6.815725803375244
Epoch 100, val loss: 1.7922273874282837
Epoch 110, training loss: 15.295221328735352 = 1.7768312692642212 + 2.0 * 6.759194850921631
Epoch 110, val loss: 1.7805970907211304
Epoch 120, training loss: 15.182913780212402 = 1.763756513595581 + 2.0 * 6.709578514099121
Epoch 120, val loss: 1.7686699628829956
Epoch 130, training loss: 15.086024284362793 = 1.7503758668899536 + 2.0 * 6.6678242683410645
Epoch 130, val loss: 1.7564738988876343
Epoch 140, training loss: 15.007501602172852 = 1.7360625267028809 + 2.0 * 6.6357197761535645
Epoch 140, val loss: 1.7436548471450806
Epoch 150, training loss: 14.930827140808105 = 1.720258116722107 + 2.0 * 6.605284690856934
Epoch 150, val loss: 1.7297041416168213
Epoch 160, training loss: 14.861790657043457 = 1.7027232646942139 + 2.0 * 6.579533576965332
Epoch 160, val loss: 1.7143720388412476
Epoch 170, training loss: 14.80810260772705 = 1.6829646825790405 + 2.0 * 6.5625691413879395
Epoch 170, val loss: 1.6972392797470093
Epoch 180, training loss: 14.743462562561035 = 1.6609128713607788 + 2.0 * 6.5412750244140625
Epoch 180, val loss: 1.6784018278121948
Epoch 190, training loss: 14.685037612915039 = 1.6363914012908936 + 2.0 * 6.524322986602783
Epoch 190, val loss: 1.6575703620910645
Epoch 200, training loss: 14.633784294128418 = 1.6089781522750854 + 2.0 * 6.5124030113220215
Epoch 200, val loss: 1.6344951391220093
Epoch 210, training loss: 14.573192596435547 = 1.57866370677948 + 2.0 * 6.497264385223389
Epoch 210, val loss: 1.6090540885925293
Epoch 220, training loss: 14.517759323120117 = 1.5453282594680786 + 2.0 * 6.486215591430664
Epoch 220, val loss: 1.581267237663269
Epoch 230, training loss: 14.458576202392578 = 1.50907564163208 + 2.0 * 6.474750518798828
Epoch 230, val loss: 1.5513155460357666
Epoch 240, training loss: 14.40046501159668 = 1.470118522644043 + 2.0 * 6.465173244476318
Epoch 240, val loss: 1.51958429813385
Epoch 250, training loss: 14.341300964355469 = 1.4288978576660156 + 2.0 * 6.456201553344727
Epoch 250, val loss: 1.4862735271453857
Epoch 260, training loss: 14.284370422363281 = 1.3857275247573853 + 2.0 * 6.449321269989014
Epoch 260, val loss: 1.4517877101898193
Epoch 270, training loss: 14.222003936767578 = 1.3412543535232544 + 2.0 * 6.440374851226807
Epoch 270, val loss: 1.4166868925094604
Epoch 280, training loss: 14.163422584533691 = 1.2958965301513672 + 2.0 * 6.433763027191162
Epoch 280, val loss: 1.3812904357910156
Epoch 290, training loss: 14.104633331298828 = 1.2504677772521973 + 2.0 * 6.4270830154418945
Epoch 290, val loss: 1.3460487127304077
Epoch 300, training loss: 14.045694351196289 = 1.205254077911377 + 2.0 * 6.420220375061035
Epoch 300, val loss: 1.3114300966262817
Epoch 310, training loss: 13.990898132324219 = 1.1607860326766968 + 2.0 * 6.415056228637695
Epoch 310, val loss: 1.2774866819381714
Epoch 320, training loss: 13.93704605102539 = 1.117500901222229 + 2.0 * 6.4097723960876465
Epoch 320, val loss: 1.2445416450500488
Epoch 330, training loss: 13.882022857666016 = 1.075815200805664 + 2.0 * 6.403103828430176
Epoch 330, val loss: 1.212984323501587
Epoch 340, training loss: 13.845514297485352 = 1.0357881784439087 + 2.0 * 6.404862880706787
Epoch 340, val loss: 1.1828359365463257
Epoch 350, training loss: 13.78775405883789 = 0.9975702166557312 + 2.0 * 6.395092010498047
Epoch 350, val loss: 1.1544138193130493
Epoch 360, training loss: 13.739441871643066 = 0.9612897038459778 + 2.0 * 6.389076232910156
Epoch 360, val loss: 1.127503514289856
Epoch 370, training loss: 13.69625473022461 = 0.9265626668930054 + 2.0 * 6.384846210479736
Epoch 370, val loss: 1.1020228862762451
Epoch 380, training loss: 13.65628719329834 = 0.893319308757782 + 2.0 * 6.381484031677246
Epoch 380, val loss: 1.0779563188552856
Epoch 390, training loss: 13.616183280944824 = 0.8618036508560181 + 2.0 * 6.377189636230469
Epoch 390, val loss: 1.055289387702942
Epoch 400, training loss: 13.57875919342041 = 0.8316200375556946 + 2.0 * 6.373569488525391
Epoch 400, val loss: 1.03391432762146
Epoch 410, training loss: 13.56536865234375 = 0.8026022911071777 + 2.0 * 6.381382942199707
Epoch 410, val loss: 1.0138092041015625
Epoch 420, training loss: 13.515909194946289 = 0.7752349972724915 + 2.0 * 6.370337009429932
Epoch 420, val loss: 0.9949954748153687
Epoch 430, training loss: 13.47841739654541 = 0.7490684390068054 + 2.0 * 6.3646745681762695
Epoch 430, val loss: 0.9774216413497925
Epoch 440, training loss: 13.446314811706543 = 0.7238837480545044 + 2.0 * 6.361215591430664
Epoch 440, val loss: 0.9609771966934204
Epoch 450, training loss: 13.446168899536133 = 0.6995704770088196 + 2.0 * 6.3732991218566895
Epoch 450, val loss: 0.9455780982971191
Epoch 460, training loss: 13.39428424835205 = 0.6764376759529114 + 2.0 * 6.358923435211182
Epoch 460, val loss: 0.9313549995422363
Epoch 470, training loss: 13.359912872314453 = 0.6541921496391296 + 2.0 * 6.352860450744629
Epoch 470, val loss: 0.9181889891624451
Epoch 480, training loss: 13.33409595489502 = 0.6326747536659241 + 2.0 * 6.350710391998291
Epoch 480, val loss: 0.9059475064277649
Epoch 490, training loss: 13.312297821044922 = 0.6118186712265015 + 2.0 * 6.3502397537231445
Epoch 490, val loss: 0.8945589661598206
Epoch 500, training loss: 13.294856071472168 = 0.5915785431861877 + 2.0 * 6.3516387939453125
Epoch 500, val loss: 0.8842652440071106
Epoch 510, training loss: 13.263208389282227 = 0.572089433670044 + 2.0 * 6.345559597015381
Epoch 510, val loss: 0.8746749758720398
Epoch 520, training loss: 13.241682052612305 = 0.5530862212181091 + 2.0 * 6.344297885894775
Epoch 520, val loss: 0.865868091583252
Epoch 530, training loss: 13.218822479248047 = 0.5346487760543823 + 2.0 * 6.3420867919921875
Epoch 530, val loss: 0.8577545285224915
Epoch 540, training loss: 13.193817138671875 = 0.5165778994560242 + 2.0 * 6.338619709014893
Epoch 540, val loss: 0.8503177762031555
Epoch 550, training loss: 13.174667358398438 = 0.49892178177833557 + 2.0 * 6.3378729820251465
Epoch 550, val loss: 0.8434447050094604
Epoch 560, training loss: 13.149710655212402 = 0.48163196444511414 + 2.0 * 6.334039211273193
Epoch 560, val loss: 0.8370535373687744
Epoch 570, training loss: 13.134340286254883 = 0.46463513374328613 + 2.0 * 6.334852695465088
Epoch 570, val loss: 0.8311687111854553
Epoch 580, training loss: 13.111259460449219 = 0.44796568155288696 + 2.0 * 6.331646919250488
Epoch 580, val loss: 0.8256956338882446
Epoch 590, training loss: 13.092638969421387 = 0.4316207468509674 + 2.0 * 6.330509185791016
Epoch 590, val loss: 0.8206411004066467
Epoch 600, training loss: 13.072175025939941 = 0.41550689935684204 + 2.0 * 6.328333854675293
Epoch 600, val loss: 0.8159533739089966
Epoch 610, training loss: 13.051427841186523 = 0.3996827006340027 + 2.0 * 6.325872421264648
Epoch 610, val loss: 0.8115591406822205
Epoch 620, training loss: 13.031966209411621 = 0.38401979207992554 + 2.0 * 6.323973178863525
Epoch 620, val loss: 0.8074604272842407
Epoch 630, training loss: 13.022136688232422 = 0.36852824687957764 + 2.0 * 6.326804161071777
Epoch 630, val loss: 0.8036976456642151
Epoch 640, training loss: 12.995537757873535 = 0.3532973527908325 + 2.0 * 6.321120262145996
Epoch 640, val loss: 0.8001870512962341
Epoch 650, training loss: 12.980279922485352 = 0.3383364677429199 + 2.0 * 6.320971488952637
Epoch 650, val loss: 0.7969517707824707
Epoch 660, training loss: 12.96217155456543 = 0.3236349821090698 + 2.0 * 6.319268226623535
Epoch 660, val loss: 0.7941615581512451
Epoch 670, training loss: 12.94798469543457 = 0.3092784881591797 + 2.0 * 6.319353103637695
Epoch 670, val loss: 0.7916890978813171
Epoch 680, training loss: 12.93362808227539 = 0.29535526037216187 + 2.0 * 6.319136619567871
Epoch 680, val loss: 0.7895698547363281
Epoch 690, training loss: 12.910897254943848 = 0.28189554810523987 + 2.0 * 6.31450080871582
Epoch 690, val loss: 0.7880050539970398
Epoch 700, training loss: 12.89480972290039 = 0.26893919706344604 + 2.0 * 6.3129353523254395
Epoch 700, val loss: 0.7869380712509155
Epoch 710, training loss: 12.895184516906738 = 0.2564677298069 + 2.0 * 6.3193583488464355
Epoch 710, val loss: 0.7864224314689636
Epoch 720, training loss: 12.873785018920898 = 0.244667187333107 + 2.0 * 6.314558982849121
Epoch 720, val loss: 0.7864537239074707
Epoch 730, training loss: 12.852522850036621 = 0.23337280750274658 + 2.0 * 6.309575080871582
Epoch 730, val loss: 0.7869083285331726
Epoch 740, training loss: 12.838088989257812 = 0.22263994812965393 + 2.0 * 6.307724475860596
Epoch 740, val loss: 0.7879142165184021
Epoch 750, training loss: 12.82622241973877 = 0.2124231904745102 + 2.0 * 6.306899547576904
Epoch 750, val loss: 0.7894219160079956
Epoch 760, training loss: 12.815906524658203 = 0.20275819301605225 + 2.0 * 6.30657434463501
Epoch 760, val loss: 0.7913813591003418
Epoch 770, training loss: 12.806014060974121 = 0.1936260312795639 + 2.0 * 6.306193828582764
Epoch 770, val loss: 0.7937713265419006
Epoch 780, training loss: 12.793581008911133 = 0.18498964607715607 + 2.0 * 6.304295539855957
Epoch 780, val loss: 0.7965219616889954
Epoch 790, training loss: 12.794734001159668 = 0.17680805921554565 + 2.0 * 6.308962821960449
Epoch 790, val loss: 0.799618661403656
Epoch 800, training loss: 12.777608871459961 = 0.1690327376127243 + 2.0 * 6.304287910461426
Epoch 800, val loss: 0.8031198978424072
Epoch 810, training loss: 12.76391887664795 = 0.16168935596942902 + 2.0 * 6.301114559173584
Epoch 810, val loss: 0.8068221807479858
Epoch 820, training loss: 12.757455825805664 = 0.1546960473060608 + 2.0 * 6.301379680633545
Epoch 820, val loss: 0.8108360171318054
Epoch 830, training loss: 12.746098518371582 = 0.14807622134685516 + 2.0 * 6.29901123046875
Epoch 830, val loss: 0.8151624202728271
Epoch 840, training loss: 12.738565444946289 = 0.14178220927715302 + 2.0 * 6.298391819000244
Epoch 840, val loss: 0.819675087928772
Epoch 850, training loss: 12.741270065307617 = 0.13577719032764435 + 2.0 * 6.302746295928955
Epoch 850, val loss: 0.8243326544761658
Epoch 860, training loss: 12.728955268859863 = 0.13014814257621765 + 2.0 * 6.299403667449951
Epoch 860, val loss: 0.8293348550796509
Epoch 870, training loss: 12.71340560913086 = 0.12474309653043747 + 2.0 * 6.294331073760986
Epoch 870, val loss: 0.8342968821525574
Epoch 880, training loss: 12.706552505493164 = 0.11962099373340607 + 2.0 * 6.293465614318848
Epoch 880, val loss: 0.8394187688827515
Epoch 890, training loss: 12.706586837768555 = 0.11473915725946426 + 2.0 * 6.295923709869385
Epoch 890, val loss: 0.844773530960083
Epoch 900, training loss: 12.70322322845459 = 0.11007694154977798 + 2.0 * 6.296573162078857
Epoch 900, val loss: 0.8502557873725891
Epoch 910, training loss: 12.689126014709473 = 0.10566406697034836 + 2.0 * 6.291730880737305
Epoch 910, val loss: 0.8557168245315552
Epoch 920, training loss: 12.68106460571289 = 0.10145413875579834 + 2.0 * 6.2898054122924805
Epoch 920, val loss: 0.8613320589065552
Epoch 930, training loss: 12.694618225097656 = 0.09742647409439087 + 2.0 * 6.298595905303955
Epoch 930, val loss: 0.8670467138290405
Epoch 940, training loss: 12.674551963806152 = 0.09361827373504639 + 2.0 * 6.290466785430908
Epoch 940, val loss: 0.8728352189064026
Epoch 950, training loss: 12.664336204528809 = 0.08997923880815506 + 2.0 * 6.2871785163879395
Epoch 950, val loss: 0.8786107301712036
Epoch 960, training loss: 12.659103393554688 = 0.08650290220975876 + 2.0 * 6.286300182342529
Epoch 960, val loss: 0.8845440149307251
Epoch 970, training loss: 12.662154197692871 = 0.08318421989679337 + 2.0 * 6.289484977722168
Epoch 970, val loss: 0.890479326248169
Epoch 980, training loss: 12.660764694213867 = 0.08004676550626755 + 2.0 * 6.290359020233154
Epoch 980, val loss: 0.896576464176178
Epoch 990, training loss: 12.648883819580078 = 0.0770285353064537 + 2.0 * 6.285927772521973
Epoch 990, val loss: 0.902493953704834
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8037037037037037
0.8413284132841329
=== training gcn model ===
Epoch 0, training loss: 19.149093627929688 = 1.9555065631866455 + 2.0 * 8.596793174743652
Epoch 0, val loss: 1.9660348892211914
Epoch 10, training loss: 19.137086868286133 = 1.944388747215271 + 2.0 * 8.596348762512207
Epoch 10, val loss: 1.954163908958435
Epoch 20, training loss: 19.116086959838867 = 1.9303756952285767 + 2.0 * 8.592855453491211
Epoch 20, val loss: 1.9388660192489624
Epoch 30, training loss: 19.046850204467773 = 1.9106898307800293 + 2.0 * 8.568079948425293
Epoch 30, val loss: 1.9175143241882324
Epoch 40, training loss: 18.68739128112793 = 1.8875095844268799 + 2.0 * 8.399940490722656
Epoch 40, val loss: 1.8939628601074219
Epoch 50, training loss: 17.842178344726562 = 1.8631560802459717 + 2.0 * 7.989511013031006
Epoch 50, val loss: 1.8697216510772705
Epoch 60, training loss: 17.191064834594727 = 1.8435509204864502 + 2.0 * 7.6737565994262695
Epoch 60, val loss: 1.8503682613372803
Epoch 70, training loss: 16.32740020751953 = 1.8311368227005005 + 2.0 * 7.24813175201416
Epoch 70, val loss: 1.838694453239441
Epoch 80, training loss: 15.912276268005371 = 1.8197441101074219 + 2.0 * 7.046266078948975
Epoch 80, val loss: 1.82767915725708
Epoch 90, training loss: 15.597829818725586 = 1.803807020187378 + 2.0 * 6.8970112800598145
Epoch 90, val loss: 1.8128901720046997
Epoch 100, training loss: 15.372416496276855 = 1.790114164352417 + 2.0 * 6.79115104675293
Epoch 100, val loss: 1.7998552322387695
Epoch 110, training loss: 15.231432914733887 = 1.777820348739624 + 2.0 * 6.726806163787842
Epoch 110, val loss: 1.787500262260437
Epoch 120, training loss: 15.122702598571777 = 1.7647182941436768 + 2.0 * 6.67899227142334
Epoch 120, val loss: 1.7746766805648804
Epoch 130, training loss: 15.032032012939453 = 1.7510912418365479 + 2.0 * 6.640470504760742
Epoch 130, val loss: 1.761635184288025
Epoch 140, training loss: 14.959185600280762 = 1.736549735069275 + 2.0 * 6.611318111419678
Epoch 140, val loss: 1.7480789422988892
Epoch 150, training loss: 14.895525932312012 = 1.7205122709274292 + 2.0 * 6.5875067710876465
Epoch 150, val loss: 1.7335107326507568
Epoch 160, training loss: 14.839221954345703 = 1.7026476860046387 + 2.0 * 6.568286895751953
Epoch 160, val loss: 1.7176380157470703
Epoch 170, training loss: 14.788433074951172 = 1.6826766729354858 + 2.0 * 6.552878379821777
Epoch 170, val loss: 1.7002450227737427
Epoch 180, training loss: 14.733022689819336 = 1.660264253616333 + 2.0 * 6.536379337310791
Epoch 180, val loss: 1.6809934377670288
Epoch 190, training loss: 14.682341575622559 = 1.635137915611267 + 2.0 * 6.52360200881958
Epoch 190, val loss: 1.6595574617385864
Epoch 200, training loss: 14.628958702087402 = 1.6071981191635132 + 2.0 * 6.510880470275879
Epoch 200, val loss: 1.6357533931732178
Epoch 210, training loss: 14.570908546447754 = 1.5762865543365479 + 2.0 * 6.497311115264893
Epoch 210, val loss: 1.6096630096435547
Epoch 220, training loss: 14.513757705688477 = 1.5421078205108643 + 2.0 * 6.485825061798096
Epoch 220, val loss: 1.5810456275939941
Epoch 230, training loss: 14.458898544311523 = 1.504822015762329 + 2.0 * 6.477038383483887
Epoch 230, val loss: 1.5501371622085571
Epoch 240, training loss: 14.39950942993164 = 1.4648889303207397 + 2.0 * 6.467310428619385
Epoch 240, val loss: 1.5172628164291382
Epoch 250, training loss: 14.338414192199707 = 1.4224101305007935 + 2.0 * 6.458002090454102
Epoch 250, val loss: 1.4826925992965698
Epoch 260, training loss: 14.277624130249023 = 1.377744436264038 + 2.0 * 6.449939727783203
Epoch 260, val loss: 1.446520209312439
Epoch 270, training loss: 14.21717357635498 = 1.331205129623413 + 2.0 * 6.442984104156494
Epoch 270, val loss: 1.409242033958435
Epoch 280, training loss: 14.161630630493164 = 1.283829927444458 + 2.0 * 6.438900470733643
Epoch 280, val loss: 1.3714532852172852
Epoch 290, training loss: 14.093650817871094 = 1.2364083528518677 + 2.0 * 6.428621292114258
Epoch 290, val loss: 1.3339296579360962
Epoch 300, training loss: 14.03468132019043 = 1.1890075206756592 + 2.0 * 6.422836780548096
Epoch 300, val loss: 1.2965015172958374
Epoch 310, training loss: 13.977502822875977 = 1.1423412561416626 + 2.0 * 6.417580604553223
Epoch 310, val loss: 1.2597764730453491
Epoch 320, training loss: 13.91848373413086 = 1.0971349477767944 + 2.0 * 6.410674571990967
Epoch 320, val loss: 1.2240878343582153
Epoch 330, training loss: 13.864511489868164 = 1.0533448457717896 + 2.0 * 6.405583381652832
Epoch 330, val loss: 1.1894158124923706
Epoch 340, training loss: 13.81502914428711 = 1.011165976524353 + 2.0 * 6.4019317626953125
Epoch 340, val loss: 1.1559677124023438
Epoch 350, training loss: 13.762904167175293 = 0.9712318181991577 + 2.0 * 6.395836353302002
Epoch 350, val loss: 1.1241706609725952
Epoch 360, training loss: 13.714709281921387 = 0.9333146810531616 + 2.0 * 6.390697479248047
Epoch 360, val loss: 1.0939741134643555
Epoch 370, training loss: 13.684432029724121 = 0.8972772359848022 + 2.0 * 6.393577575683594
Epoch 370, val loss: 1.0652912855148315
Epoch 380, training loss: 13.633602142333984 = 0.8634518384933472 + 2.0 * 6.385075092315674
Epoch 380, val loss: 1.0384480953216553
Epoch 390, training loss: 13.58917236328125 = 0.8317230343818665 + 2.0 * 6.378724575042725
Epoch 390, val loss: 1.013361930847168
Epoch 400, training loss: 13.549948692321777 = 0.8017232418060303 + 2.0 * 6.374112606048584
Epoch 400, val loss: 0.9897960424423218
Epoch 410, training loss: 13.534900665283203 = 0.773383617401123 + 2.0 * 6.380758285522461
Epoch 410, val loss: 0.9677852988243103
Epoch 420, training loss: 13.491866111755371 = 0.7467454671859741 + 2.0 * 6.372560501098633
Epoch 420, val loss: 0.9473602771759033
Epoch 430, training loss: 13.452545166015625 = 0.7217780947685242 + 2.0 * 6.365383625030518
Epoch 430, val loss: 0.9284448027610779
Epoch 440, training loss: 13.419123649597168 = 0.6978986263275146 + 2.0 * 6.360612392425537
Epoch 440, val loss: 0.9108214974403381
Epoch 450, training loss: 13.393649101257324 = 0.6749008893966675 + 2.0 * 6.359374046325684
Epoch 450, val loss: 0.8943222761154175
Epoch 460, training loss: 13.373139381408691 = 0.6527999043464661 + 2.0 * 6.360169887542725
Epoch 460, val loss: 0.878804624080658
Epoch 470, training loss: 13.34179401397705 = 0.6313058733940125 + 2.0 * 6.355244159698486
Epoch 470, val loss: 0.8642681837081909
Epoch 480, training loss: 13.309453010559082 = 0.6102753281593323 + 2.0 * 6.349588871002197
Epoch 480, val loss: 0.8504607081413269
Epoch 490, training loss: 13.285740852355957 = 0.589519739151001 + 2.0 * 6.348110675811768
Epoch 490, val loss: 0.8372527956962585
Epoch 500, training loss: 13.257771492004395 = 0.5690014362335205 + 2.0 * 6.344385147094727
Epoch 500, val loss: 0.824627697467804
Epoch 510, training loss: 13.236299514770508 = 0.548691987991333 + 2.0 * 6.343803882598877
Epoch 510, val loss: 0.8125253319740295
Epoch 520, training loss: 13.219173431396484 = 0.5286703109741211 + 2.0 * 6.345251560211182
Epoch 520, val loss: 0.8008893728256226
Epoch 530, training loss: 13.184457778930664 = 0.5088648200035095 + 2.0 * 6.337796688079834
Epoch 530, val loss: 0.7897709608078003
Epoch 540, training loss: 13.159056663513184 = 0.4893011450767517 + 2.0 * 6.334877967834473
Epoch 540, val loss: 0.7791474461555481
Epoch 550, training loss: 13.140315055847168 = 0.4699701964855194 + 2.0 * 6.335172653198242
Epoch 550, val loss: 0.7689985036849976
Epoch 560, training loss: 13.118429183959961 = 0.45095115900039673 + 2.0 * 6.333738803863525
Epoch 560, val loss: 0.759316086769104
Epoch 570, training loss: 13.091520309448242 = 0.432232528924942 + 2.0 * 6.329643726348877
Epoch 570, val loss: 0.7501927018165588
Epoch 580, training loss: 13.078433990478516 = 0.41380226612091064 + 2.0 * 6.332315921783447
Epoch 580, val loss: 0.7414334416389465
Epoch 590, training loss: 13.04941463470459 = 0.3956772983074188 + 2.0 * 6.326868534088135
Epoch 590, val loss: 0.7333560585975647
Epoch 600, training loss: 13.028749465942383 = 0.37789103388786316 + 2.0 * 6.325429439544678
Epoch 600, val loss: 0.7257541418075562
Epoch 610, training loss: 13.010412216186523 = 0.3604777753353119 + 2.0 * 6.324967384338379
Epoch 610, val loss: 0.718627393245697
Epoch 620, training loss: 12.985651016235352 = 0.3434392213821411 + 2.0 * 6.32110595703125
Epoch 620, val loss: 0.712081253528595
Epoch 630, training loss: 12.966666221618652 = 0.3268204629421234 + 2.0 * 6.319922924041748
Epoch 630, val loss: 0.7060214281082153
Epoch 640, training loss: 12.946703910827637 = 0.31067726016044617 + 2.0 * 6.3180131912231445
Epoch 640, val loss: 0.700470507144928
Epoch 650, training loss: 12.929797172546387 = 0.29510146379470825 + 2.0 * 6.317348003387451
Epoch 650, val loss: 0.6955004930496216
Epoch 660, training loss: 12.908601760864258 = 0.2800869345664978 + 2.0 * 6.314257621765137
Epoch 660, val loss: 0.6911433935165405
Epoch 670, training loss: 12.914886474609375 = 0.2657083570957184 + 2.0 * 6.324589252471924
Epoch 670, val loss: 0.6873876452445984
Epoch 680, training loss: 12.875504493713379 = 0.2519893944263458 + 2.0 * 6.311757564544678
Epoch 680, val loss: 0.6842896342277527
Epoch 690, training loss: 12.864509582519531 = 0.23895539343357086 + 2.0 * 6.312777042388916
Epoch 690, val loss: 0.6818479299545288
Epoch 700, training loss: 12.845478057861328 = 0.2266126126050949 + 2.0 * 6.309432506561279
Epoch 700, val loss: 0.6799858808517456
Epoch 710, training loss: 12.830883979797363 = 0.2149302065372467 + 2.0 * 6.307976722717285
Epoch 710, val loss: 0.6786717772483826
Epoch 720, training loss: 12.825349807739258 = 0.20387108623981476 + 2.0 * 6.310739517211914
Epoch 720, val loss: 0.6778655648231506
Epoch 730, training loss: 12.806556701660156 = 0.19345571100711823 + 2.0 * 6.3065505027771
Epoch 730, val loss: 0.6775929927825928
Epoch 740, training loss: 12.792284965515137 = 0.18366090953350067 + 2.0 * 6.304312229156494
Epoch 740, val loss: 0.6777791380882263
Epoch 750, training loss: 12.788426399230957 = 0.1744222640991211 + 2.0 * 6.307002067565918
Epoch 750, val loss: 0.6784278154373169
Epoch 760, training loss: 12.784340858459473 = 0.1657625436782837 + 2.0 * 6.30928897857666
Epoch 760, val loss: 0.6794097423553467
Epoch 770, training loss: 12.76131534576416 = 0.15764763951301575 + 2.0 * 6.301833629608154
Epoch 770, val loss: 0.680826723575592
Epoch 780, training loss: 12.748611450195312 = 0.15003158152103424 + 2.0 * 6.299289703369141
Epoch 780, val loss: 0.6826026439666748
Epoch 790, training loss: 12.743705749511719 = 0.14286476373672485 + 2.0 * 6.30042028427124
Epoch 790, val loss: 0.6847004890441895
Epoch 800, training loss: 12.731520652770996 = 0.1361270248889923 + 2.0 * 6.297696590423584
Epoch 800, val loss: 0.6870341897010803
Epoch 810, training loss: 12.73643684387207 = 0.1298081874847412 + 2.0 * 6.303314208984375
Epoch 810, val loss: 0.6896544098854065
Epoch 820, training loss: 12.717869758605957 = 0.12386814504861832 + 2.0 * 6.297000885009766
Epoch 820, val loss: 0.6924270987510681
Epoch 830, training loss: 12.707353591918945 = 0.11830338090658188 + 2.0 * 6.294525146484375
Epoch 830, val loss: 0.695480465888977
Epoch 840, training loss: 12.698103904724121 = 0.11304685473442078 + 2.0 * 6.2925286293029785
Epoch 840, val loss: 0.6986706852912903
Epoch 850, training loss: 12.697427749633789 = 0.1080850213766098 + 2.0 * 6.294671535491943
Epoch 850, val loss: 0.7020301222801208
Epoch 860, training loss: 12.69049072265625 = 0.10340777039527893 + 2.0 * 6.293541431427002
Epoch 860, val loss: 0.7054601311683655
Epoch 870, training loss: 12.68148136138916 = 0.09901159256696701 + 2.0 * 6.291234970092773
Epoch 870, val loss: 0.7090714573860168
Epoch 880, training loss: 12.671618461608887 = 0.09486368298530579 + 2.0 * 6.288377285003662
Epoch 880, val loss: 0.7128096222877502
Epoch 890, training loss: 12.669509887695312 = 0.09093470871448517 + 2.0 * 6.289287567138672
Epoch 890, val loss: 0.7166576385498047
Epoch 900, training loss: 12.664592742919922 = 0.08722429722547531 + 2.0 * 6.288684368133545
Epoch 900, val loss: 0.7205616235733032
Epoch 910, training loss: 12.663484573364258 = 0.08371838927268982 + 2.0 * 6.289883136749268
Epoch 910, val loss: 0.7245126962661743
Epoch 920, training loss: 12.649493217468262 = 0.08041027933359146 + 2.0 * 6.284541606903076
Epoch 920, val loss: 0.7284901142120361
Epoch 930, training loss: 12.64627742767334 = 0.07726746052503586 + 2.0 * 6.2845048904418945
Epoch 930, val loss: 0.7325294017791748
Epoch 940, training loss: 12.657455444335938 = 0.07427898049354553 + 2.0 * 6.291588306427002
Epoch 940, val loss: 0.7365636825561523
Epoch 950, training loss: 12.646196365356445 = 0.0714479610323906 + 2.0 * 6.287374019622803
Epoch 950, val loss: 0.7406359910964966
Epoch 960, training loss: 12.63354778289795 = 0.06876036524772644 + 2.0 * 6.282393932342529
Epoch 960, val loss: 0.7447555661201477
Epoch 970, training loss: 12.632461547851562 = 0.06621496379375458 + 2.0 * 6.28312349319458
Epoch 970, val loss: 0.7488597631454468
Epoch 980, training loss: 12.626001358032227 = 0.06378481537103653 + 2.0 * 6.281108379364014
Epoch 980, val loss: 0.753049373626709
Epoch 990, training loss: 12.622233390808105 = 0.06147220358252525 + 2.0 * 6.280380725860596
Epoch 990, val loss: 0.757209837436676
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.825925925925926
0.838165524512388
=== training gcn model ===
Epoch 0, training loss: 19.132856369018555 = 1.9391677379608154 + 2.0 * 8.596844673156738
Epoch 0, val loss: 1.937388300895691
Epoch 10, training loss: 19.122556686401367 = 1.9293725490570068 + 2.0 * 8.59659194946289
Epoch 10, val loss: 1.928054928779602
Epoch 20, training loss: 19.106904983520508 = 1.9172247648239136 + 2.0 * 8.594840049743652
Epoch 20, val loss: 1.916075348854065
Epoch 30, training loss: 19.063989639282227 = 1.9004497528076172 + 2.0 * 8.581769943237305
Epoch 30, val loss: 1.8992856740951538
Epoch 40, training loss: 18.881423950195312 = 1.879238247871399 + 2.0 * 8.501092910766602
Epoch 40, val loss: 1.8787614107131958
Epoch 50, training loss: 18.14577865600586 = 1.857662320137024 + 2.0 * 8.144058227539062
Epoch 50, val loss: 1.8585399389266968
Epoch 60, training loss: 17.06742286682129 = 1.8406001329421997 + 2.0 * 7.613410949707031
Epoch 60, val loss: 1.8429855108261108
Epoch 70, training loss: 16.109830856323242 = 1.8286981582641602 + 2.0 * 7.140566349029541
Epoch 70, val loss: 1.8315365314483643
Epoch 80, training loss: 15.707526206970215 = 1.8182032108306885 + 2.0 * 6.944661617279053
Epoch 80, val loss: 1.8211370706558228
Epoch 90, training loss: 15.46859359741211 = 1.8041901588439941 + 2.0 * 6.8322014808654785
Epoch 90, val loss: 1.8077970743179321
Epoch 100, training loss: 15.297999382019043 = 1.788678526878357 + 2.0 * 6.754660606384277
Epoch 100, val loss: 1.7935711145401
Epoch 110, training loss: 15.173552513122559 = 1.7736470699310303 + 2.0 * 6.699952602386475
Epoch 110, val loss: 1.779822587966919
Epoch 120, training loss: 15.069989204406738 = 1.7588564157485962 + 2.0 * 6.655566215515137
Epoch 120, val loss: 1.7662339210510254
Epoch 130, training loss: 14.983994483947754 = 1.7435086965560913 + 2.0 * 6.620243072509766
Epoch 130, val loss: 1.752286672592163
Epoch 140, training loss: 14.908648490905762 = 1.7269961833953857 + 2.0 * 6.590826034545898
Epoch 140, val loss: 1.7376750707626343
Epoch 150, training loss: 14.839922904968262 = 1.7088499069213867 + 2.0 * 6.5655364990234375
Epoch 150, val loss: 1.7217289209365845
Epoch 160, training loss: 14.780030250549316 = 1.6884244680404663 + 2.0 * 6.545803070068359
Epoch 160, val loss: 1.704102635383606
Epoch 170, training loss: 14.720807075500488 = 1.6654126644134521 + 2.0 * 6.5276970863342285
Epoch 170, val loss: 1.684517741203308
Epoch 180, training loss: 14.670159339904785 = 1.639506220817566 + 2.0 * 6.515326499938965
Epoch 180, val loss: 1.6626336574554443
Epoch 190, training loss: 14.60821533203125 = 1.6107008457183838 + 2.0 * 6.498757362365723
Epoch 190, val loss: 1.638349175453186
Epoch 200, training loss: 14.551652908325195 = 1.5784850120544434 + 2.0 * 6.486584186553955
Epoch 200, val loss: 1.6114784479141235
Epoch 210, training loss: 14.494071960449219 = 1.5426746606826782 + 2.0 * 6.475698471069336
Epoch 210, val loss: 1.581717610359192
Epoch 220, training loss: 14.440004348754883 = 1.503435730934143 + 2.0 * 6.4682841300964355
Epoch 220, val loss: 1.5491483211517334
Epoch 230, training loss: 14.378310203552246 = 1.4613913297653198 + 2.0 * 6.458459377288818
Epoch 230, val loss: 1.5145303010940552
Epoch 240, training loss: 14.316460609436035 = 1.4168291091918945 + 2.0 * 6.44981575012207
Epoch 240, val loss: 1.4780067205429077
Epoch 250, training loss: 14.258477210998535 = 1.370471715927124 + 2.0 * 6.444002628326416
Epoch 250, val loss: 1.4402509927749634
Epoch 260, training loss: 14.195367813110352 = 1.323317050933838 + 2.0 * 6.436025619506836
Epoch 260, val loss: 1.4021884202957153
Epoch 270, training loss: 14.136032104492188 = 1.2758903503417969 + 2.0 * 6.430070877075195
Epoch 270, val loss: 1.3643687963485718
Epoch 280, training loss: 14.083581924438477 = 1.229066252708435 + 2.0 * 6.427258014678955
Epoch 280, val loss: 1.3274744749069214
Epoch 290, training loss: 14.02316951751709 = 1.1842032670974731 + 2.0 * 6.419483184814453
Epoch 290, val loss: 1.2928457260131836
Epoch 300, training loss: 13.970226287841797 = 1.1416250467300415 + 2.0 * 6.414300441741943
Epoch 300, val loss: 1.2605361938476562
Epoch 310, training loss: 13.917444229125977 = 1.1007812023162842 + 2.0 * 6.408331394195557
Epoch 310, val loss: 1.230181336402893
Epoch 320, training loss: 13.879624366760254 = 1.0616648197174072 + 2.0 * 6.408979892730713
Epoch 320, val loss: 1.2016363143920898
Epoch 330, training loss: 13.834439277648926 = 1.0246633291244507 + 2.0 * 6.404888153076172
Epoch 330, val loss: 1.1752994060516357
Epoch 340, training loss: 13.781295776367188 = 0.9897481203079224 + 2.0 * 6.395773887634277
Epoch 340, val loss: 1.1508867740631104
Epoch 350, training loss: 13.738739013671875 = 0.9562111496925354 + 2.0 * 6.391263961791992
Epoch 350, val loss: 1.1278327703475952
Epoch 360, training loss: 13.698348045349121 = 0.9238005876541138 + 2.0 * 6.387273788452148
Epoch 360, val loss: 1.1058934926986694
Epoch 370, training loss: 13.663650512695312 = 0.8923678994178772 + 2.0 * 6.385641098022461
Epoch 370, val loss: 1.0850070714950562
Epoch 380, training loss: 13.627578735351562 = 0.862271249294281 + 2.0 * 6.382653713226318
Epoch 380, val loss: 1.0653928518295288
Epoch 390, training loss: 13.590727806091309 = 0.8336045145988464 + 2.0 * 6.378561496734619
Epoch 390, val loss: 1.047118902206421
Epoch 400, training loss: 13.554052352905273 = 0.8058840036392212 + 2.0 * 6.374083995819092
Epoch 400, val loss: 1.0298867225646973
Epoch 410, training loss: 13.520502090454102 = 0.7789866328239441 + 2.0 * 6.370757579803467
Epoch 410, val loss: 1.01357102394104
Epoch 420, training loss: 13.497852325439453 = 0.75279700756073 + 2.0 * 6.372527599334717
Epoch 420, val loss: 0.9981828331947327
Epoch 430, training loss: 13.459315299987793 = 0.7277101278305054 + 2.0 * 6.365802764892578
Epoch 430, val loss: 0.9836530685424805
Epoch 440, training loss: 13.432348251342773 = 0.7034967541694641 + 2.0 * 6.3644256591796875
Epoch 440, val loss: 0.9702216386795044
Epoch 450, training loss: 13.400712013244629 = 0.6800526976585388 + 2.0 * 6.360329627990723
Epoch 450, val loss: 0.9575111865997314
Epoch 460, training loss: 13.371794700622559 = 0.6571357846260071 + 2.0 * 6.357329368591309
Epoch 460, val loss: 0.9455066323280334
Epoch 470, training loss: 13.344409942626953 = 0.6346535682678223 + 2.0 * 6.354877948760986
Epoch 470, val loss: 0.9340940713882446
Epoch 480, training loss: 13.326998710632324 = 0.6126331090927124 + 2.0 * 6.35718297958374
Epoch 480, val loss: 0.9232181310653687
Epoch 490, training loss: 13.292572975158691 = 0.5911616683006287 + 2.0 * 6.350705623626709
Epoch 490, val loss: 0.9128698706626892
Epoch 500, training loss: 13.268668174743652 = 0.5701330900192261 + 2.0 * 6.349267482757568
Epoch 500, val loss: 0.9029991626739502
Epoch 510, training loss: 13.242829322814941 = 0.5493851900100708 + 2.0 * 6.34672212600708
Epoch 510, val loss: 0.8934295177459717
Epoch 520, training loss: 13.225298881530762 = 0.5288875699043274 + 2.0 * 6.34820556640625
Epoch 520, val loss: 0.8841555118560791
Epoch 530, training loss: 13.195355415344238 = 0.5087308883666992 + 2.0 * 6.3433122634887695
Epoch 530, val loss: 0.8752695918083191
Epoch 540, training loss: 13.173768997192383 = 0.4888186752796173 + 2.0 * 6.342474937438965
Epoch 540, val loss: 0.8666629195213318
Epoch 550, training loss: 13.14756965637207 = 0.4691333770751953 + 2.0 * 6.3392181396484375
Epoch 550, val loss: 0.8583197593688965
Epoch 560, training loss: 13.125007629394531 = 0.4496701955795288 + 2.0 * 6.3376688957214355
Epoch 560, val loss: 0.8502342700958252
Epoch 570, training loss: 13.118885040283203 = 0.43030357360839844 + 2.0 * 6.344290733337402
Epoch 570, val loss: 0.8424088358879089
Epoch 580, training loss: 13.080609321594238 = 0.41133248805999756 + 2.0 * 6.334638595581055
Epoch 580, val loss: 0.8348701000213623
Epoch 590, training loss: 13.056928634643555 = 0.392622709274292 + 2.0 * 6.332152843475342
Epoch 590, val loss: 0.8277236223220825
Epoch 600, training loss: 13.037201881408691 = 0.3742299973964691 + 2.0 * 6.331485748291016
Epoch 600, val loss: 0.8209840059280396
Epoch 610, training loss: 13.015169143676758 = 0.3562566339969635 + 2.0 * 6.329456329345703
Epoch 610, val loss: 0.8146394491195679
Epoch 620, training loss: 12.996624946594238 = 0.33880287408828735 + 2.0 * 6.328910827636719
Epoch 620, val loss: 0.8087716698646545
Epoch 630, training loss: 12.975752830505371 = 0.32188791036605835 + 2.0 * 6.326932430267334
Epoch 630, val loss: 0.8034374713897705
Epoch 640, training loss: 12.964067459106445 = 0.30548352003097534 + 2.0 * 6.329291820526123
Epoch 640, val loss: 0.7986049056053162
Epoch 650, training loss: 12.952530860900879 = 0.28976166248321533 + 2.0 * 6.331384658813477
Epoch 650, val loss: 0.7942807674407959
Epoch 660, training loss: 12.919760704040527 = 0.27468109130859375 + 2.0 * 6.322539806365967
Epoch 660, val loss: 0.7905140519142151
Epoch 670, training loss: 12.902076721191406 = 0.2602492570877075 + 2.0 * 6.320913791656494
Epoch 670, val loss: 0.7873885631561279
Epoch 680, training loss: 12.885777473449707 = 0.24646295607089996 + 2.0 * 6.319657325744629
Epoch 680, val loss: 0.7848777174949646
Epoch 690, training loss: 12.882782936096191 = 0.23331443965435028 + 2.0 * 6.324734210968018
Epoch 690, val loss: 0.7829400897026062
Epoch 700, training loss: 12.857998847961426 = 0.22088323533535004 + 2.0 * 6.3185577392578125
Epoch 700, val loss: 0.7815947532653809
Epoch 710, training loss: 12.840606689453125 = 0.20908279716968536 + 2.0 * 6.315762042999268
Epoch 710, val loss: 0.7807273864746094
Epoch 720, training loss: 12.85814094543457 = 0.19796739518642426 + 2.0 * 6.330086708068848
Epoch 720, val loss: 0.7804807424545288
Epoch 730, training loss: 12.816319465637207 = 0.18750618398189545 + 2.0 * 6.314406871795654
Epoch 730, val loss: 0.7804574966430664
Epoch 740, training loss: 12.801891326904297 = 0.17770452797412872 + 2.0 * 6.312093257904053
Epoch 740, val loss: 0.7809364795684814
Epoch 750, training loss: 12.789587020874023 = 0.1684959977865219 + 2.0 * 6.310545444488525
Epoch 750, val loss: 0.7819639444351196
Epoch 760, training loss: 12.787323951721191 = 0.15981636941432953 + 2.0 * 6.313753604888916
Epoch 760, val loss: 0.7833864092826843
Epoch 770, training loss: 12.783997535705566 = 0.1516900360584259 + 2.0 * 6.316153526306152
Epoch 770, val loss: 0.7851230502128601
Epoch 780, training loss: 12.760115623474121 = 0.14409656822681427 + 2.0 * 6.308009624481201
Epoch 780, val loss: 0.7870963215827942
Epoch 790, training loss: 12.74984359741211 = 0.1369614601135254 + 2.0 * 6.306441307067871
Epoch 790, val loss: 0.7894161939620972
Epoch 800, training loss: 12.743910789489746 = 0.13025765120983124 + 2.0 * 6.306826591491699
Epoch 800, val loss: 0.7920445799827576
Epoch 810, training loss: 12.741844177246094 = 0.12398076057434082 + 2.0 * 6.308931827545166
Epoch 810, val loss: 0.7949954867362976
Epoch 820, training loss: 12.73022174835205 = 0.1180831640958786 + 2.0 * 6.306069374084473
Epoch 820, val loss: 0.7979247570037842
Epoch 830, training loss: 12.718073844909668 = 0.1125936433672905 + 2.0 * 6.302740097045898
Epoch 830, val loss: 0.8011669516563416
Epoch 840, training loss: 12.70889663696289 = 0.10742121189832687 + 2.0 * 6.3007378578186035
Epoch 840, val loss: 0.8046661019325256
Epoch 850, training loss: 12.704111099243164 = 0.1025455892086029 + 2.0 * 6.300782680511475
Epoch 850, val loss: 0.8083533644676208
Epoch 860, training loss: 12.702174186706543 = 0.09796491265296936 + 2.0 * 6.302104473114014
Epoch 860, val loss: 0.812203049659729
Epoch 870, training loss: 12.693034172058105 = 0.0936514213681221 + 2.0 * 6.299691200256348
Epoch 870, val loss: 0.8159705996513367
Epoch 880, training loss: 12.685810089111328 = 0.08961417526006699 + 2.0 * 6.298098087310791
Epoch 880, val loss: 0.8200299143791199
Epoch 890, training loss: 12.681703567504883 = 0.08580522984266281 + 2.0 * 6.297949314117432
Epoch 890, val loss: 0.8241287469863892
Epoch 900, training loss: 12.684420585632324 = 0.08221209794282913 + 2.0 * 6.3011040687561035
Epoch 900, val loss: 0.8283251523971558
Epoch 910, training loss: 12.668729782104492 = 0.07881508767604828 + 2.0 * 6.294957160949707
Epoch 910, val loss: 0.832547128200531
Epoch 920, training loss: 12.663477897644043 = 0.07563158869743347 + 2.0 * 6.293923377990723
Epoch 920, val loss: 0.8368901014328003
Epoch 930, training loss: 12.657459259033203 = 0.0726097822189331 + 2.0 * 6.29242467880249
Epoch 930, val loss: 0.8413046002388
Epoch 940, training loss: 12.680953025817871 = 0.06974852085113525 + 2.0 * 6.305602073669434
Epoch 940, val loss: 0.8458032011985779
Epoch 950, training loss: 12.657768249511719 = 0.06706007570028305 + 2.0 * 6.295353889465332
Epoch 950, val loss: 0.8502558469772339
Epoch 960, training loss: 12.645671844482422 = 0.0645158588886261 + 2.0 * 6.2905778884887695
Epoch 960, val loss: 0.8547016978263855
Epoch 970, training loss: 12.641039848327637 = 0.06210573762655258 + 2.0 * 6.289466857910156
Epoch 970, val loss: 0.8592754602432251
Epoch 980, training loss: 12.636486053466797 = 0.0598163940012455 + 2.0 * 6.288334846496582
Epoch 980, val loss: 0.8639246225357056
Epoch 990, training loss: 12.651615142822266 = 0.057637058198451996 + 2.0 * 6.2969889640808105
Epoch 990, val loss: 0.8685493469238281
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.788888888888889
0.8371112282551397
The final CL Acc:0.80617, 0.01522, The final GNN Acc:0.83887, 0.00179
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.5 der2:0.4 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11610])
remove edge: torch.Size([2, 9488])
updated graph: torch.Size([2, 10542])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 19.141088485717773 = 1.9475082159042358 + 2.0 * 8.596790313720703
Epoch 0, val loss: 1.9535088539123535
Epoch 10, training loss: 19.130098342895508 = 1.9373857975006104 + 2.0 * 8.596356391906738
Epoch 10, val loss: 1.94380784034729
Epoch 20, training loss: 19.11138916015625 = 1.9247498512268066 + 2.0 * 8.5933198928833
Epoch 20, val loss: 1.9312958717346191
Epoch 30, training loss: 19.051589965820312 = 1.9073948860168457 + 2.0 * 8.572097778320312
Epoch 30, val loss: 1.9139257669448853
Epoch 40, training loss: 18.732789993286133 = 1.887529969215393 + 2.0 * 8.422630310058594
Epoch 40, val loss: 1.894924283027649
Epoch 50, training loss: 17.825153350830078 = 1.8680274486541748 + 2.0 * 7.978562831878662
Epoch 50, val loss: 1.8766095638275146
Epoch 60, training loss: 16.72319221496582 = 1.854334831237793 + 2.0 * 7.434428691864014
Epoch 60, val loss: 1.864138126373291
Epoch 70, training loss: 15.913897514343262 = 1.844476580619812 + 2.0 * 7.03471040725708
Epoch 70, val loss: 1.8543241024017334
Epoch 80, training loss: 15.65286922454834 = 1.8340893983840942 + 2.0 * 6.909389972686768
Epoch 80, val loss: 1.843693494796753
Epoch 90, training loss: 15.446552276611328 = 1.8203091621398926 + 2.0 * 6.813121795654297
Epoch 90, val loss: 1.8305021524429321
Epoch 100, training loss: 15.298606872558594 = 1.8065638542175293 + 2.0 * 6.746021270751953
Epoch 100, val loss: 1.817417860031128
Epoch 110, training loss: 15.190414428710938 = 1.7941838502883911 + 2.0 * 6.698115348815918
Epoch 110, val loss: 1.8051378726959229
Epoch 120, training loss: 15.093523979187012 = 1.7824310064315796 + 2.0 * 6.65554666519165
Epoch 120, val loss: 1.7931550741195679
Epoch 130, training loss: 15.016295433044434 = 1.7703949213027954 + 2.0 * 6.622950077056885
Epoch 130, val loss: 1.78114652633667
Epoch 140, training loss: 14.951457023620605 = 1.757765769958496 + 2.0 * 6.596845626831055
Epoch 140, val loss: 1.768964409828186
Epoch 150, training loss: 14.891032218933105 = 1.7443609237670898 + 2.0 * 6.573335647583008
Epoch 150, val loss: 1.7564623355865479
Epoch 160, training loss: 14.8392972946167 = 1.7297495603561401 + 2.0 * 6.554773807525635
Epoch 160, val loss: 1.743223786354065
Epoch 170, training loss: 14.785825729370117 = 1.7136750221252441 + 2.0 * 6.536075115203857
Epoch 170, val loss: 1.729087471961975
Epoch 180, training loss: 14.737950325012207 = 1.695867657661438 + 2.0 * 6.521041393280029
Epoch 180, val loss: 1.713715672492981
Epoch 190, training loss: 14.68978214263916 = 1.6758928298950195 + 2.0 * 6.50694465637207
Epoch 190, val loss: 1.696751356124878
Epoch 200, training loss: 14.642196655273438 = 1.6535354852676392 + 2.0 * 6.494330406188965
Epoch 200, val loss: 1.6780598163604736
Epoch 210, training loss: 14.596043586730957 = 1.6286581754684448 + 2.0 * 6.483692646026611
Epoch 210, val loss: 1.6573988199234009
Epoch 220, training loss: 14.54855728149414 = 1.6010968685150146 + 2.0 * 6.473730087280273
Epoch 220, val loss: 1.6346913576126099
Epoch 230, training loss: 14.49767780303955 = 1.5707805156707764 + 2.0 * 6.463448524475098
Epoch 230, val loss: 1.6099292039871216
Epoch 240, training loss: 14.446159362792969 = 1.5375924110412598 + 2.0 * 6.454283237457275
Epoch 240, val loss: 1.5830081701278687
Epoch 250, training loss: 14.398218154907227 = 1.5016688108444214 + 2.0 * 6.448274612426758
Epoch 250, val loss: 1.5540564060211182
Epoch 260, training loss: 14.341421127319336 = 1.4635330438613892 + 2.0 * 6.438943862915039
Epoch 260, val loss: 1.5235683917999268
Epoch 270, training loss: 14.288105964660645 = 1.4234397411346436 + 2.0 * 6.432332992553711
Epoch 270, val loss: 1.4918217658996582
Epoch 280, training loss: 14.23660659790039 = 1.381598711013794 + 2.0 * 6.427504062652588
Epoch 280, val loss: 1.459067940711975
Epoch 290, training loss: 14.181434631347656 = 1.3388447761535645 + 2.0 * 6.421295166015625
Epoch 290, val loss: 1.4259309768676758
Epoch 300, training loss: 14.123625755310059 = 1.2957113981246948 + 2.0 * 6.413957118988037
Epoch 300, val loss: 1.392818570137024
Epoch 310, training loss: 14.07066535949707 = 1.2524504661560059 + 2.0 * 6.409107208251953
Epoch 310, val loss: 1.359976053237915
Epoch 320, training loss: 14.017614364624023 = 1.2096498012542725 + 2.0 * 6.403982162475586
Epoch 320, val loss: 1.3276431560516357
Epoch 330, training loss: 13.968225479125977 = 1.1677846908569336 + 2.0 * 6.4002203941345215
Epoch 330, val loss: 1.296593189239502
Epoch 340, training loss: 13.91427230834961 = 1.1271717548370361 + 2.0 * 6.393550395965576
Epoch 340, val loss: 1.2665815353393555
Epoch 350, training loss: 13.867738723754883 = 1.0878442525863647 + 2.0 * 6.389947414398193
Epoch 350, val loss: 1.2376881837844849
Epoch 360, training loss: 13.822041511535645 = 1.0499213933944702 + 2.0 * 6.3860602378845215
Epoch 360, val loss: 1.2100763320922852
Epoch 370, training loss: 13.778915405273438 = 1.0136542320251465 + 2.0 * 6.382630825042725
Epoch 370, val loss: 1.1836493015289307
Epoch 380, training loss: 13.73344612121582 = 0.9788158535957336 + 2.0 * 6.377315044403076
Epoch 380, val loss: 1.1584328413009644
Epoch 390, training loss: 13.701360702514648 = 0.9453994631767273 + 2.0 * 6.377980709075928
Epoch 390, val loss: 1.1342811584472656
Epoch 400, training loss: 13.658252716064453 = 0.9135907888412476 + 2.0 * 6.372331142425537
Epoch 400, val loss: 1.1113965511322021
Epoch 410, training loss: 13.62159252166748 = 0.883231520652771 + 2.0 * 6.369180679321289
Epoch 410, val loss: 1.0897061824798584
Epoch 420, training loss: 13.584230422973633 = 0.8543300628662109 + 2.0 * 6.364950180053711
Epoch 420, val loss: 1.0690957307815552
Epoch 430, training loss: 13.550139427185059 = 0.8267449736595154 + 2.0 * 6.361697196960449
Epoch 430, val loss: 1.0498052835464478
Epoch 440, training loss: 13.519416809082031 = 0.8005033731460571 + 2.0 * 6.359456539154053
Epoch 440, val loss: 1.031640648841858
Epoch 450, training loss: 13.490649223327637 = 0.775546133518219 + 2.0 * 6.357551574707031
Epoch 450, val loss: 1.0147002935409546
Epoch 460, training loss: 13.463709831237793 = 0.751801073551178 + 2.0 * 6.355954170227051
Epoch 460, val loss: 0.999015212059021
Epoch 470, training loss: 13.430353164672852 = 0.7292729616165161 + 2.0 * 6.3505401611328125
Epoch 470, val loss: 0.9846438765525818
Epoch 480, training loss: 13.404212951660156 = 0.707662045955658 + 2.0 * 6.348275661468506
Epoch 480, val loss: 0.9713647961616516
Epoch 490, training loss: 13.394636154174805 = 0.6867702603340149 + 2.0 * 6.353932857513428
Epoch 490, val loss: 0.9590975046157837
Epoch 500, training loss: 13.355499267578125 = 0.6666616201400757 + 2.0 * 6.344419002532959
Epoch 500, val loss: 0.9476088285446167
Epoch 510, training loss: 13.332358360290527 = 0.6470590829849243 + 2.0 * 6.342649459838867
Epoch 510, val loss: 0.9371286630630493
Epoch 520, training loss: 13.32007122039795 = 0.6278485059738159 + 2.0 * 6.346111297607422
Epoch 520, val loss: 0.927376925945282
Epoch 530, training loss: 13.28441047668457 = 0.6089213490486145 + 2.0 * 6.33774471282959
Epoch 530, val loss: 0.918353796005249
Epoch 540, training loss: 13.262031555175781 = 0.5902372598648071 + 2.0 * 6.335896968841553
Epoch 540, val loss: 0.909850537776947
Epoch 550, training loss: 13.252222061157227 = 0.5716577768325806 + 2.0 * 6.340281963348389
Epoch 550, val loss: 0.9018759727478027
Epoch 560, training loss: 13.22042465209961 = 0.553305447101593 + 2.0 * 6.333559513092041
Epoch 560, val loss: 0.8944042921066284
Epoch 570, training loss: 13.198073387145996 = 0.5350736379623413 + 2.0 * 6.331500053405762
Epoch 570, val loss: 0.8874767422676086
Epoch 580, training loss: 13.179720878601074 = 0.5169801115989685 + 2.0 * 6.3313703536987305
Epoch 580, val loss: 0.8810712695121765
Epoch 590, training loss: 13.154250144958496 = 0.49905630946159363 + 2.0 * 6.327597141265869
Epoch 590, val loss: 0.8751592636108398
Epoch 600, training loss: 13.141010284423828 = 0.48136815428733826 + 2.0 * 6.3298211097717285
Epoch 600, val loss: 0.86985182762146
Epoch 610, training loss: 13.114234924316406 = 0.4640234708786011 + 2.0 * 6.325105667114258
Epoch 610, val loss: 0.865090012550354
Epoch 620, training loss: 13.092721939086914 = 0.4469124376773834 + 2.0 * 6.322904586791992
Epoch 620, val loss: 0.8609629273414612
Epoch 630, training loss: 13.078721046447754 = 0.4301624000072479 + 2.0 * 6.324279308319092
Epoch 630, val loss: 0.8573998212814331
Epoch 640, training loss: 13.055252075195312 = 0.4138643145561218 + 2.0 * 6.3206939697265625
Epoch 640, val loss: 0.8544830083847046
Epoch 650, training loss: 13.036434173583984 = 0.39796608686447144 + 2.0 * 6.3192338943481445
Epoch 650, val loss: 0.8522270917892456
Epoch 660, training loss: 13.020604133605957 = 0.3824586272239685 + 2.0 * 6.319072723388672
Epoch 660, val loss: 0.8504488468170166
Epoch 670, training loss: 12.99951457977295 = 0.3673689663410187 + 2.0 * 6.316072940826416
Epoch 670, val loss: 0.8491724729537964
Epoch 680, training loss: 12.982711791992188 = 0.3526797294616699 + 2.0 * 6.315016269683838
Epoch 680, val loss: 0.8484324216842651
Epoch 690, training loss: 12.973381042480469 = 0.3383641242980957 + 2.0 * 6.317508220672607
Epoch 690, val loss: 0.8481382727622986
Epoch 700, training loss: 12.954789161682129 = 0.3245096802711487 + 2.0 * 6.3151397705078125
Epoch 700, val loss: 0.8482869863510132
Epoch 710, training loss: 12.933730125427246 = 0.3110601603984833 + 2.0 * 6.31133508682251
Epoch 710, val loss: 0.848922610282898
Epoch 720, training loss: 12.92573070526123 = 0.2979673147201538 + 2.0 * 6.313881874084473
Epoch 720, val loss: 0.8499251008033752
Epoch 730, training loss: 12.904815673828125 = 0.28526008129119873 + 2.0 * 6.309777736663818
Epoch 730, val loss: 0.8512940406799316
Epoch 740, training loss: 12.900911331176758 = 0.27293333411216736 + 2.0 * 6.313989162445068
Epoch 740, val loss: 0.8530272245407104
Epoch 750, training loss: 12.879478454589844 = 0.26095151901245117 + 2.0 * 6.309263706207275
Epoch 750, val loss: 0.8549546003341675
Epoch 760, training loss: 12.860910415649414 = 0.2493312954902649 + 2.0 * 6.305789470672607
Epoch 760, val loss: 0.8573455214500427
Epoch 770, training loss: 12.846417427062988 = 0.23801840841770172 + 2.0 * 6.304199695587158
Epoch 770, val loss: 0.8599530458450317
Epoch 780, training loss: 12.845685005187988 = 0.2270222306251526 + 2.0 * 6.30933141708374
Epoch 780, val loss: 0.8628687262535095
Epoch 790, training loss: 12.824399948120117 = 0.2164096236228943 + 2.0 * 6.303995132446289
Epoch 790, val loss: 0.8661382794380188
Epoch 800, training loss: 12.81024169921875 = 0.20611737668514252 + 2.0 * 6.302062034606934
Epoch 800, val loss: 0.8696224689483643
Epoch 810, training loss: 12.805397033691406 = 0.19621802866458893 + 2.0 * 6.30458927154541
Epoch 810, val loss: 0.8733730912208557
Epoch 820, training loss: 12.787032127380371 = 0.1866958737373352 + 2.0 * 6.300168037414551
Epoch 820, val loss: 0.8773595094680786
Epoch 830, training loss: 12.778822898864746 = 0.17752160131931305 + 2.0 * 6.300650596618652
Epoch 830, val loss: 0.8815513849258423
Epoch 840, training loss: 12.763096809387207 = 0.16873319447040558 + 2.0 * 6.297181606292725
Epoch 840, val loss: 0.8860066533088684
Epoch 850, training loss: 12.754120826721191 = 0.160305917263031 + 2.0 * 6.296907424926758
Epoch 850, val loss: 0.8906758427619934
Epoch 860, training loss: 12.751420021057129 = 0.15224891901016235 + 2.0 * 6.299585342407227
Epoch 860, val loss: 0.895512580871582
Epoch 870, training loss: 12.741902351379395 = 0.14455612003803253 + 2.0 * 6.298673152923584
Epoch 870, val loss: 0.9004946351051331
Epoch 880, training loss: 12.73036003112793 = 0.1372523158788681 + 2.0 * 6.296554088592529
Epoch 880, val loss: 0.9056991934776306
Epoch 890, training loss: 12.71751880645752 = 0.13033781945705414 + 2.0 * 6.293590545654297
Epoch 890, val loss: 0.9110958576202393
Epoch 900, training loss: 12.708113670349121 = 0.12377753108739853 + 2.0 * 6.292168140411377
Epoch 900, val loss: 0.9166575074195862
Epoch 910, training loss: 12.702969551086426 = 0.11755216866731644 + 2.0 * 6.292708873748779
Epoch 910, val loss: 0.9223021864891052
Epoch 920, training loss: 12.694451332092285 = 0.11167609691619873 + 2.0 * 6.291387557983398
Epoch 920, val loss: 0.9280219674110413
Epoch 930, training loss: 12.694089889526367 = 0.1061534509062767 + 2.0 * 6.293968200683594
Epoch 930, val loss: 0.9338890314102173
Epoch 940, training loss: 12.679039001464844 = 0.10096944123506546 + 2.0 * 6.289034843444824
Epoch 940, val loss: 0.9399790167808533
Epoch 950, training loss: 12.672151565551758 = 0.096089206635952 + 2.0 * 6.288031101226807
Epoch 950, val loss: 0.9460698366165161
Epoch 960, training loss: 12.679901123046875 = 0.09150108695030212 + 2.0 * 6.2941999435424805
Epoch 960, val loss: 0.9521997570991516
Epoch 970, training loss: 12.663055419921875 = 0.08719553798437119 + 2.0 * 6.287930011749268
Epoch 970, val loss: 0.9584482908248901
Epoch 980, training loss: 12.654800415039062 = 0.08315087109804153 + 2.0 * 6.285824775695801
Epoch 980, val loss: 0.9647021293640137
Epoch 990, training loss: 12.648944854736328 = 0.07935542613267899 + 2.0 * 6.284794807434082
Epoch 990, val loss: 0.9710817933082581
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7666666666666667
0.8091723774380601
=== training gcn model ===
Epoch 0, training loss: 19.133502960205078 = 1.9398579597473145 + 2.0 * 8.596822738647461
Epoch 0, val loss: 1.9359630346298218
Epoch 10, training loss: 19.123092651367188 = 1.9300428628921509 + 2.0 * 8.596525192260742
Epoch 10, val loss: 1.925979495048523
Epoch 20, training loss: 19.106821060180664 = 1.9178277254104614 + 2.0 * 8.594496726989746
Epoch 20, val loss: 1.9134554862976074
Epoch 30, training loss: 19.062044143676758 = 1.9007755517959595 + 2.0 * 8.580634117126465
Epoch 30, val loss: 1.896142840385437
Epoch 40, training loss: 18.891300201416016 = 1.8796813488006592 + 2.0 * 8.505809783935547
Epoch 40, val loss: 1.875805139541626
Epoch 50, training loss: 18.241973876953125 = 1.8574782609939575 + 2.0 * 8.19224739074707
Epoch 50, val loss: 1.8548356294631958
Epoch 60, training loss: 17.59730339050293 = 1.8374998569488525 + 2.0 * 7.87990140914917
Epoch 60, val loss: 1.8365923166275024
Epoch 70, training loss: 16.59003257751465 = 1.8251577615737915 + 2.0 * 7.382437229156494
Epoch 70, val loss: 1.8251107931137085
Epoch 80, training loss: 16.05152130126953 = 1.816941499710083 + 2.0 * 7.1172895431518555
Epoch 80, val loss: 1.8168416023254395
Epoch 90, training loss: 15.70788860321045 = 1.8078750371932983 + 2.0 * 6.95000696182251
Epoch 90, val loss: 1.8087269067764282
Epoch 100, training loss: 15.505189895629883 = 1.7975590229034424 + 2.0 * 6.85381555557251
Epoch 100, val loss: 1.7995187044143677
Epoch 110, training loss: 15.34972095489502 = 1.7871869802474976 + 2.0 * 6.781267166137695
Epoch 110, val loss: 1.7897638082504272
Epoch 120, training loss: 15.23664665222168 = 1.7773774862289429 + 2.0 * 6.729634761810303
Epoch 120, val loss: 1.7801365852355957
Epoch 130, training loss: 15.14421558380127 = 1.7674357891082764 + 2.0 * 6.688389778137207
Epoch 130, val loss: 1.7705414295196533
Epoch 140, training loss: 15.070038795471191 = 1.7568790912628174 + 2.0 * 6.656579971313477
Epoch 140, val loss: 1.7606080770492554
Epoch 150, training loss: 15.004790306091309 = 1.7454204559326172 + 2.0 * 6.629684925079346
Epoch 150, val loss: 1.7502241134643555
Epoch 160, training loss: 14.944836616516113 = 1.732863426208496 + 2.0 * 6.605986595153809
Epoch 160, val loss: 1.7389814853668213
Epoch 170, training loss: 14.887226104736328 = 1.718839406967163 + 2.0 * 6.584193229675293
Epoch 170, val loss: 1.7266424894332886
Epoch 180, training loss: 14.835981369018555 = 1.7030507326126099 + 2.0 * 6.566465377807617
Epoch 180, val loss: 1.712877869606018
Epoch 190, training loss: 14.779812812805176 = 1.6854184865951538 + 2.0 * 6.547197341918945
Epoch 190, val loss: 1.69776451587677
Epoch 200, training loss: 14.727828025817871 = 1.6656370162963867 + 2.0 * 6.531095504760742
Epoch 200, val loss: 1.6807785034179688
Epoch 210, training loss: 14.682998657226562 = 1.6432229280471802 + 2.0 * 6.519887924194336
Epoch 210, val loss: 1.6617419719696045
Epoch 220, training loss: 14.624786376953125 = 1.618047833442688 + 2.0 * 6.503369331359863
Epoch 220, val loss: 1.6403751373291016
Epoch 230, training loss: 14.573446273803711 = 1.5898849964141846 + 2.0 * 6.491780757904053
Epoch 230, val loss: 1.6165403127670288
Epoch 240, training loss: 14.527454376220703 = 1.5584988594055176 + 2.0 * 6.484477519989014
Epoch 240, val loss: 1.5901126861572266
Epoch 250, training loss: 14.47152328491211 = 1.5242010354995728 + 2.0 * 6.473660945892334
Epoch 250, val loss: 1.561414122581482
Epoch 260, training loss: 14.413047790527344 = 1.4875023365020752 + 2.0 * 6.462772846221924
Epoch 260, val loss: 1.530774474143982
Epoch 270, training loss: 14.35739517211914 = 1.4483603239059448 + 2.0 * 6.454517364501953
Epoch 270, val loss: 1.49832022190094
Epoch 280, training loss: 14.29952621459961 = 1.406959891319275 + 2.0 * 6.446283340454102
Epoch 280, val loss: 1.4644423723220825
Epoch 290, training loss: 14.259407043457031 = 1.3637123107910156 + 2.0 * 6.447847366333008
Epoch 290, val loss: 1.4295765161514282
Epoch 300, training loss: 14.193808555603027 = 1.3201359510421753 + 2.0 * 6.436836242675781
Epoch 300, val loss: 1.3948124647140503
Epoch 310, training loss: 14.129541397094727 = 1.2766666412353516 + 2.0 * 6.4264373779296875
Epoch 310, val loss: 1.3606524467468262
Epoch 320, training loss: 14.073463439941406 = 1.2331311702728271 + 2.0 * 6.420166015625
Epoch 320, val loss: 1.3272784948349
Epoch 330, training loss: 14.030525207519531 = 1.1898198127746582 + 2.0 * 6.420352935791016
Epoch 330, val loss: 1.294759750366211
Epoch 340, training loss: 13.967150688171387 = 1.1476541757583618 + 2.0 * 6.409748077392578
Epoch 340, val loss: 1.263692855834961
Epoch 350, training loss: 13.915221214294434 = 1.1066505908966064 + 2.0 * 6.404285430908203
Epoch 350, val loss: 1.2343472242355347
Epoch 360, training loss: 13.870555877685547 = 1.0667058229446411 + 2.0 * 6.401925086975098
Epoch 360, val loss: 1.2065237760543823
Epoch 370, training loss: 13.820687294006348 = 1.0281975269317627 + 2.0 * 6.396245002746582
Epoch 370, val loss: 1.1801888942718506
Epoch 380, training loss: 13.774576187133789 = 0.991059422492981 + 2.0 * 6.391758441925049
Epoch 380, val loss: 1.1555629968643188
Epoch 390, training loss: 13.732868194580078 = 0.955348789691925 + 2.0 * 6.388759613037109
Epoch 390, val loss: 1.1324336528778076
Epoch 400, training loss: 13.689449310302734 = 0.921170711517334 + 2.0 * 6.384139537811279
Epoch 400, val loss: 1.1108720302581787
Epoch 410, training loss: 13.653402328491211 = 0.8886802792549133 + 2.0 * 6.382360935211182
Epoch 410, val loss: 1.090887188911438
Epoch 420, training loss: 13.612380981445312 = 0.8580459356307983 + 2.0 * 6.377167701721191
Epoch 420, val loss: 1.072659969329834
Epoch 430, training loss: 13.574715614318848 = 0.8288614749908447 + 2.0 * 6.372927188873291
Epoch 430, val loss: 1.055931568145752
Epoch 440, training loss: 13.565789222717285 = 0.8010340929031372 + 2.0 * 6.382377624511719
Epoch 440, val loss: 1.0405489206314087
Epoch 450, training loss: 13.513875007629395 = 0.7746902704238892 + 2.0 * 6.369592189788818
Epoch 450, val loss: 1.0266088247299194
Epoch 460, training loss: 13.479007720947266 = 0.7496956586837769 + 2.0 * 6.3646559715271
Epoch 460, val loss: 1.0141549110412598
Epoch 470, training loss: 13.449027061462402 = 0.725757360458374 + 2.0 * 6.361634731292725
Epoch 470, val loss: 1.0027936697006226
Epoch 480, training loss: 13.418950080871582 = 0.7028091549873352 + 2.0 * 6.358070373535156
Epoch 480, val loss: 0.9926044344902039
Epoch 490, training loss: 13.394262313842773 = 0.6809075474739075 + 2.0 * 6.356677532196045
Epoch 490, val loss: 0.9834678173065186
Epoch 500, training loss: 13.367837905883789 = 0.6597731113433838 + 2.0 * 6.354032516479492
Epoch 500, val loss: 0.9753371477127075
Epoch 510, training loss: 13.34033489227295 = 0.6392333507537842 + 2.0 * 6.350550651550293
Epoch 510, val loss: 0.967997133731842
Epoch 520, training loss: 13.348993301391602 = 0.6192216277122498 + 2.0 * 6.3648858070373535
Epoch 520, val loss: 0.9614772200584412
Epoch 530, training loss: 13.302033424377441 = 0.5999789834022522 + 2.0 * 6.351027011871338
Epoch 530, val loss: 0.955642580986023
Epoch 540, training loss: 13.270466804504395 = 0.5813514590263367 + 2.0 * 6.344557762145996
Epoch 540, val loss: 0.9506019949913025
Epoch 550, training loss: 13.247126579284668 = 0.5631242394447327 + 2.0 * 6.342000961303711
Epoch 550, val loss: 0.9462100267410278
Epoch 560, training loss: 13.24262523651123 = 0.5452433228492737 + 2.0 * 6.348690986633301
Epoch 560, val loss: 0.9423994421958923
Epoch 570, training loss: 13.20738697052002 = 0.5278870463371277 + 2.0 * 6.339749813079834
Epoch 570, val loss: 0.9392791986465454
Epoch 580, training loss: 13.185580253601074 = 0.5109490752220154 + 2.0 * 6.337315559387207
Epoch 580, val loss: 0.9366966485977173
Epoch 590, training loss: 13.16469669342041 = 0.4943661391735077 + 2.0 * 6.335165500640869
Epoch 590, val loss: 0.9345887899398804
Epoch 600, training loss: 13.151792526245117 = 0.47813600301742554 + 2.0 * 6.336828231811523
Epoch 600, val loss: 0.9329296946525574
Epoch 610, training loss: 13.139266014099121 = 0.4623785614967346 + 2.0 * 6.338443756103516
Epoch 610, val loss: 0.9316989779472351
Epoch 620, training loss: 13.10955810546875 = 0.4470250606536865 + 2.0 * 6.331266403198242
Epoch 620, val loss: 0.9308701157569885
Epoch 630, training loss: 13.087981224060059 = 0.43197956681251526 + 2.0 * 6.328001022338867
Epoch 630, val loss: 0.9304603934288025
Epoch 640, training loss: 13.069683074951172 = 0.4172184467315674 + 2.0 * 6.326232433319092
Epoch 640, val loss: 0.9304013848304749
Epoch 650, training loss: 13.054150581359863 = 0.40270715951919556 + 2.0 * 6.325721740722656
Epoch 650, val loss: 0.9306434988975525
Epoch 660, training loss: 13.042083740234375 = 0.38845109939575195 + 2.0 * 6.326816558837891
Epoch 660, val loss: 0.9310913681983948
Epoch 670, training loss: 13.022212028503418 = 0.37462982535362244 + 2.0 * 6.323791027069092
Epoch 670, val loss: 0.931862473487854
Epoch 680, training loss: 13.004383087158203 = 0.3610842823982239 + 2.0 * 6.321649551391602
Epoch 680, val loss: 0.9329374432563782
Epoch 690, training loss: 12.988163948059082 = 0.3477247357368469 + 2.0 * 6.32021951675415
Epoch 690, val loss: 0.9342954754829407
Epoch 700, training loss: 12.983363151550293 = 0.3345683515071869 + 2.0 * 6.324397563934326
Epoch 700, val loss: 0.9358639717102051
Epoch 710, training loss: 12.958450317382812 = 0.3216148912906647 + 2.0 * 6.318417549133301
Epoch 710, val loss: 0.9377058744430542
Epoch 720, training loss: 12.942017555236816 = 0.30888840556144714 + 2.0 * 6.316564559936523
Epoch 720, val loss: 0.939726710319519
Epoch 730, training loss: 12.938390731811523 = 0.2963363826274872 + 2.0 * 6.3210272789001465
Epoch 730, val loss: 0.9420246481895447
Epoch 740, training loss: 12.914905548095703 = 0.2840811014175415 + 2.0 * 6.3154120445251465
Epoch 740, val loss: 0.944546103477478
Epoch 750, training loss: 12.897306442260742 = 0.27202656865119934 + 2.0 * 6.3126397132873535
Epoch 750, val loss: 0.9474263787269592
Epoch 760, training loss: 12.891145706176758 = 0.2602745592594147 + 2.0 * 6.315435409545898
Epoch 760, val loss: 0.950584352016449
Epoch 770, training loss: 12.890414237976074 = 0.24886023998260498 + 2.0 * 6.32077693939209
Epoch 770, val loss: 0.9539368748664856
Epoch 780, training loss: 12.862283706665039 = 0.23791873455047607 + 2.0 * 6.312182426452637
Epoch 780, val loss: 0.9575623869895935
Epoch 790, training loss: 12.844486236572266 = 0.22735081613063812 + 2.0 * 6.308567523956299
Epoch 790, val loss: 0.9616727232933044
Epoch 800, training loss: 12.830974578857422 = 0.21714366972446442 + 2.0 * 6.306915283203125
Epoch 800, val loss: 0.9661556482315063
Epoch 810, training loss: 12.84514045715332 = 0.20734903216362 + 2.0 * 6.3188958168029785
Epoch 810, val loss: 0.9709984660148621
Epoch 820, training loss: 12.807672500610352 = 0.19799037277698517 + 2.0 * 6.304841041564941
Epoch 820, val loss: 0.9758937358856201
Epoch 830, training loss: 12.799254417419434 = 0.18909373879432678 + 2.0 * 6.305080413818359
Epoch 830, val loss: 0.9811875224113464
Epoch 840, training loss: 12.801275253295898 = 0.18061473965644836 + 2.0 * 6.310330390930176
Epoch 840, val loss: 0.9867916703224182
Epoch 850, training loss: 12.77622127532959 = 0.1725892424583435 + 2.0 * 6.301815986633301
Epoch 850, val loss: 0.9925852417945862
Epoch 860, training loss: 12.767741203308105 = 0.1649509221315384 + 2.0 * 6.301394939422607
Epoch 860, val loss: 0.9987316131591797
Epoch 870, training loss: 12.760903358459473 = 0.15767665207386017 + 2.0 * 6.3016133308410645
Epoch 870, val loss: 1.0051089525222778
Epoch 880, training loss: 12.750710487365723 = 0.15078720450401306 + 2.0 * 6.299961566925049
Epoch 880, val loss: 1.0116266012191772
Epoch 890, training loss: 12.744112014770508 = 0.14426018297672272 + 2.0 * 6.299925804138184
Epoch 890, val loss: 1.018377423286438
Epoch 900, training loss: 12.73348331451416 = 0.13807158172130585 + 2.0 * 6.29770565032959
Epoch 900, val loss: 1.0253961086273193
Epoch 910, training loss: 12.736929893493652 = 0.13220055401325226 + 2.0 * 6.302364826202393
Epoch 910, val loss: 1.032564640045166
Epoch 920, training loss: 12.721344947814941 = 0.12662027776241302 + 2.0 * 6.297362327575684
Epoch 920, val loss: 1.0399333238601685
Epoch 930, training loss: 12.711731910705566 = 0.12134392559528351 + 2.0 * 6.295194149017334
Epoch 930, val loss: 1.047370195388794
Epoch 940, training loss: 12.703625679016113 = 0.11630628257989883 + 2.0 * 6.293659687042236
Epoch 940, val loss: 1.0549393892288208
Epoch 950, training loss: 12.698990821838379 = 0.1115126758813858 + 2.0 * 6.293738842010498
Epoch 950, val loss: 1.0626583099365234
Epoch 960, training loss: 12.711433410644531 = 0.10696060210466385 + 2.0 * 6.302236557006836
Epoch 960, val loss: 1.0703423023223877
Epoch 970, training loss: 12.69370174407959 = 0.10267336666584015 + 2.0 * 6.295514106750488
Epoch 970, val loss: 1.0780836343765259
Epoch 980, training loss: 12.68118953704834 = 0.09861450642347336 + 2.0 * 6.291287422180176
Epoch 980, val loss: 1.0859259366989136
Epoch 990, training loss: 12.674882888793945 = 0.09475566446781158 + 2.0 * 6.290063381195068
Epoch 990, val loss: 1.0938544273376465
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7407407407407408
0.8054823405376912
=== training gcn model ===
Epoch 0, training loss: 19.152931213378906 = 1.9592722654342651 + 2.0 * 8.596829414367676
Epoch 0, val loss: 1.957759141921997
Epoch 10, training loss: 19.1422061920166 = 1.9492058753967285 + 2.0 * 8.596500396728516
Epoch 10, val loss: 1.9481384754180908
Epoch 20, training loss: 19.124757766723633 = 1.9362282752990723 + 2.0 * 8.59426498413086
Epoch 20, val loss: 1.9354408979415894
Epoch 30, training loss: 19.073957443237305 = 1.9178805351257324 + 2.0 * 8.578038215637207
Epoch 30, val loss: 1.9173964262008667
Epoch 40, training loss: 18.84153938293457 = 1.8951237201690674 + 2.0 * 8.473207473754883
Epoch 40, val loss: 1.8958755731582642
Epoch 50, training loss: 17.97955322265625 = 1.8712407350540161 + 2.0 * 8.054156303405762
Epoch 50, val loss: 1.8734749555587769
Epoch 60, training loss: 17.042415618896484 = 1.8529574871063232 + 2.0 * 7.594729423522949
Epoch 60, val loss: 1.858005166053772
Epoch 70, training loss: 16.251840591430664 = 1.8411332368850708 + 2.0 * 7.205353260040283
Epoch 70, val loss: 1.8467963933944702
Epoch 80, training loss: 15.843446731567383 = 1.8302884101867676 + 2.0 * 7.0065789222717285
Epoch 80, val loss: 1.836313009262085
Epoch 90, training loss: 15.506782531738281 = 1.8183794021606445 + 2.0 * 6.844201564788818
Epoch 90, val loss: 1.8256548643112183
Epoch 100, training loss: 15.3215970993042 = 1.807105302810669 + 2.0 * 6.757246017456055
Epoch 100, val loss: 1.8151246309280396
Epoch 110, training loss: 15.1915922164917 = 1.7964409589767456 + 2.0 * 6.697575569152832
Epoch 110, val loss: 1.804779052734375
Epoch 120, training loss: 15.088054656982422 = 1.7863640785217285 + 2.0 * 6.650845527648926
Epoch 120, val loss: 1.7947354316711426
Epoch 130, training loss: 15.003846168518066 = 1.7765966653823853 + 2.0 * 6.613624572753906
Epoch 130, val loss: 1.7849771976470947
Epoch 140, training loss: 14.935346603393555 = 1.7665876150131226 + 2.0 * 6.58437967300415
Epoch 140, val loss: 1.7751977443695068
Epoch 150, training loss: 14.874455451965332 = 1.7559298276901245 + 2.0 * 6.559262752532959
Epoch 150, val loss: 1.765063762664795
Epoch 160, training loss: 14.819497108459473 = 1.7443695068359375 + 2.0 * 6.537563800811768
Epoch 160, val loss: 1.7543532848358154
Epoch 170, training loss: 14.769044876098633 = 1.7315998077392578 + 2.0 * 6.5187225341796875
Epoch 170, val loss: 1.7428351640701294
Epoch 180, training loss: 14.726520538330078 = 1.71736741065979 + 2.0 * 6.504576683044434
Epoch 180, val loss: 1.7302428483963013
Epoch 190, training loss: 14.678382873535156 = 1.7014801502227783 + 2.0 * 6.4884514808654785
Epoch 190, val loss: 1.7163474559783936
Epoch 200, training loss: 14.641685485839844 = 1.6836998462677002 + 2.0 * 6.478992938995361
Epoch 200, val loss: 1.7008956670761108
Epoch 210, training loss: 14.593841552734375 = 1.6638550758361816 + 2.0 * 6.464993476867676
Epoch 210, val loss: 1.6838281154632568
Epoch 220, training loss: 14.550569534301758 = 1.6417948007583618 + 2.0 * 6.454387187957764
Epoch 220, val loss: 1.6649245023727417
Epoch 230, training loss: 14.506970405578613 = 1.6172597408294678 + 2.0 * 6.444855213165283
Epoch 230, val loss: 1.6440140008926392
Epoch 240, training loss: 14.472375869750977 = 1.590100646018982 + 2.0 * 6.441137790679932
Epoch 240, val loss: 1.620911717414856
Epoch 250, training loss: 14.418825149536133 = 1.5604629516601562 + 2.0 * 6.429181098937988
Epoch 250, val loss: 1.59579336643219
Epoch 260, training loss: 14.375067710876465 = 1.5284496545791626 + 2.0 * 6.423308849334717
Epoch 260, val loss: 1.5688318014144897
Epoch 270, training loss: 14.335489273071289 = 1.4943218231201172 + 2.0 * 6.420583724975586
Epoch 270, val loss: 1.5402153730392456
Epoch 280, training loss: 14.279848098754883 = 1.4584280252456665 + 2.0 * 6.410709857940674
Epoch 280, val loss: 1.5103529691696167
Epoch 290, training loss: 14.230475425720215 = 1.4210681915283203 + 2.0 * 6.404703617095947
Epoch 290, val loss: 1.4795165061950684
Epoch 300, training loss: 14.180407524108887 = 1.3823926448822021 + 2.0 * 6.399007320404053
Epoch 300, val loss: 1.447889804840088
Epoch 310, training loss: 14.137866973876953 = 1.3426553010940552 + 2.0 * 6.397605895996094
Epoch 310, val loss: 1.4157370328903198
Epoch 320, training loss: 14.083157539367676 = 1.3027297258377075 + 2.0 * 6.390213966369629
Epoch 320, val loss: 1.3837889432907104
Epoch 330, training loss: 14.034756660461426 = 1.2625454664230347 + 2.0 * 6.386105537414551
Epoch 330, val loss: 1.3521876335144043
Epoch 340, training loss: 13.987835884094238 = 1.2221431732177734 + 2.0 * 6.382846355438232
Epoch 340, val loss: 1.3209267854690552
Epoch 350, training loss: 13.944454193115234 = 1.1817140579223633 + 2.0 * 6.3813700675964355
Epoch 350, val loss: 1.2900696992874146
Epoch 360, training loss: 13.890556335449219 = 1.1415486335754395 + 2.0 * 6.3745036125183105
Epoch 360, val loss: 1.2599390745162964
Epoch 370, training loss: 13.844493865966797 = 1.1016583442687988 + 2.0 * 6.371417999267578
Epoch 370, val loss: 1.2303869724273682
Epoch 380, training loss: 13.801432609558105 = 1.062196969985962 + 2.0 * 6.369617938995361
Epoch 380, val loss: 1.2013905048370361
Epoch 390, training loss: 13.755077362060547 = 1.0235044956207275 + 2.0 * 6.365786552429199
Epoch 390, val loss: 1.1734358072280884
Epoch 400, training loss: 13.709630012512207 = 0.9856647253036499 + 2.0 * 6.361982822418213
Epoch 400, val loss: 1.146405816078186
Epoch 410, training loss: 13.670124053955078 = 0.9488184452056885 + 2.0 * 6.360652923583984
Epoch 410, val loss: 1.1202890872955322
Epoch 420, training loss: 13.627151489257812 = 0.9132974147796631 + 2.0 * 6.356926918029785
Epoch 420, val loss: 1.095435380935669
Epoch 430, training loss: 13.5858793258667 = 0.8791167140007019 + 2.0 * 6.353381156921387
Epoch 430, val loss: 1.0719006061553955
Epoch 440, training loss: 13.563549041748047 = 0.8464113473892212 + 2.0 * 6.3585686683654785
Epoch 440, val loss: 1.049661636352539
Epoch 450, training loss: 13.517136573791504 = 0.8153073191642761 + 2.0 * 6.350914478302002
Epoch 450, val loss: 1.0289967060089111
Epoch 460, training loss: 13.479190826416016 = 0.785768449306488 + 2.0 * 6.346711158752441
Epoch 460, val loss: 1.0099313259124756
Epoch 470, training loss: 13.445206642150879 = 0.7575168609619141 + 2.0 * 6.343844890594482
Epoch 470, val loss: 0.992175281047821
Epoch 480, training loss: 13.422504425048828 = 0.7304283976554871 + 2.0 * 6.346037864685059
Epoch 480, val loss: 0.9756442308425903
Epoch 490, training loss: 13.39083480834961 = 0.7045638561248779 + 2.0 * 6.343135356903076
Epoch 490, val loss: 0.9605729579925537
Epoch 500, training loss: 13.357488632202148 = 0.6797056794166565 + 2.0 * 6.338891506195068
Epoch 500, val loss: 0.9467743039131165
Epoch 510, training loss: 13.329879760742188 = 0.655540943145752 + 2.0 * 6.337169170379639
Epoch 510, val loss: 0.933901846408844
Epoch 520, training loss: 13.305943489074707 = 0.6319917440414429 + 2.0 * 6.336976051330566
Epoch 520, val loss: 0.9219701290130615
Epoch 530, training loss: 13.275259971618652 = 0.6091476082801819 + 2.0 * 6.3330559730529785
Epoch 530, val loss: 0.911058783531189
Epoch 540, training loss: 13.249421119689941 = 0.5868273973464966 + 2.0 * 6.331296920776367
Epoch 540, val loss: 0.9010730385780334
Epoch 550, training loss: 13.227547645568848 = 0.5650579929351807 + 2.0 * 6.331244945526123
Epoch 550, val loss: 0.8918651938438416
Epoch 560, training loss: 13.200176239013672 = 0.5438903570175171 + 2.0 * 6.328143119812012
Epoch 560, val loss: 0.883699893951416
Epoch 570, training loss: 13.177308082580566 = 0.5233538746833801 + 2.0 * 6.326977252960205
Epoch 570, val loss: 0.8764615654945374
Epoch 580, training loss: 13.151305198669434 = 0.5034669041633606 + 2.0 * 6.323919296264648
Epoch 580, val loss: 0.8700217604637146
Epoch 590, training loss: 13.133944511413574 = 0.4842815399169922 + 2.0 * 6.324831485748291
Epoch 590, val loss: 0.8646063208580017
Epoch 600, training loss: 13.11213493347168 = 0.46582216024398804 + 2.0 * 6.323156356811523
Epoch 600, val loss: 0.8600013852119446
Epoch 610, training loss: 13.08631420135498 = 0.4481140375137329 + 2.0 * 6.3190999031066895
Epoch 610, val loss: 0.856267511844635
Epoch 620, training loss: 13.069808959960938 = 0.43107742071151733 + 2.0 * 6.319365978240967
Epoch 620, val loss: 0.8533127307891846
Epoch 630, training loss: 13.051275253295898 = 0.41472822427749634 + 2.0 * 6.318273544311523
Epoch 630, val loss: 0.8509864807128906
Epoch 640, training loss: 13.029489517211914 = 0.3990379273891449 + 2.0 * 6.315225601196289
Epoch 640, val loss: 0.8494747877120972
Epoch 650, training loss: 13.011550903320312 = 0.3839608430862427 + 2.0 * 6.31379508972168
Epoch 650, val loss: 0.8484635353088379
Epoch 660, training loss: 13.005379676818848 = 0.36942583322525024 + 2.0 * 6.317976951599121
Epoch 660, val loss: 0.8479945659637451
Epoch 670, training loss: 12.99071979522705 = 0.35555732250213623 + 2.0 * 6.3175811767578125
Epoch 670, val loss: 0.8480720520019531
Epoch 680, training loss: 12.965046882629395 = 0.3422081470489502 + 2.0 * 6.311419486999512
Epoch 680, val loss: 0.8487745523452759
Epoch 690, training loss: 12.945969581604004 = 0.3293505907058716 + 2.0 * 6.308309555053711
Epoch 690, val loss: 0.8498756885528564
Epoch 700, training loss: 12.937520027160645 = 0.3168965280056 + 2.0 * 6.310311794281006
Epoch 700, val loss: 0.8513584733009338
Epoch 710, training loss: 12.91933822631836 = 0.30492061376571655 + 2.0 * 6.307209014892578
Epoch 710, val loss: 0.85318523645401
Epoch 720, training loss: 12.90917682647705 = 0.29335689544677734 + 2.0 * 6.307909965515137
Epoch 720, val loss: 0.8553968071937561
Epoch 730, training loss: 12.892840385437012 = 0.2822021245956421 + 2.0 * 6.305319309234619
Epoch 730, val loss: 0.8578348755836487
Epoch 740, training loss: 12.87729263305664 = 0.2714354395866394 + 2.0 * 6.302928447723389
Epoch 740, val loss: 0.8607410788536072
Epoch 750, training loss: 12.86540412902832 = 0.2610030770301819 + 2.0 * 6.3022003173828125
Epoch 750, val loss: 0.863847017288208
Epoch 760, training loss: 12.869192123413086 = 0.2509034276008606 + 2.0 * 6.309144496917725
Epoch 760, val loss: 0.8671596646308899
Epoch 770, training loss: 12.840997695922852 = 0.24112927913665771 + 2.0 * 6.299934387207031
Epoch 770, val loss: 0.8708269000053406
Epoch 780, training loss: 12.831843376159668 = 0.2316758632659912 + 2.0 * 6.300083637237549
Epoch 780, val loss: 0.8747042417526245
Epoch 790, training loss: 12.82170581817627 = 0.2225089818239212 + 2.0 * 6.299598217010498
Epoch 790, val loss: 0.8788459300994873
Epoch 800, training loss: 12.808849334716797 = 0.21365410089492798 + 2.0 * 6.297597408294678
Epoch 800, val loss: 0.8834210634231567
Epoch 810, training loss: 12.808989524841309 = 0.20509035885334015 + 2.0 * 6.301949501037598
Epoch 810, val loss: 0.8881344199180603
Epoch 820, training loss: 12.787489891052246 = 0.1968357264995575 + 2.0 * 6.295327186584473
Epoch 820, val loss: 0.8932418823242188
Epoch 830, training loss: 12.776259422302246 = 0.1888464093208313 + 2.0 * 6.29370641708374
Epoch 830, val loss: 0.8986192941665649
Epoch 840, training loss: 12.77322769165039 = 0.1811174601316452 + 2.0 * 6.296055316925049
Epoch 840, val loss: 0.9042311906814575
Epoch 850, training loss: 12.758539199829102 = 0.17369911074638367 + 2.0 * 6.292419910430908
Epoch 850, val loss: 0.9101557731628418
Epoch 860, training loss: 12.750988960266113 = 0.16653770208358765 + 2.0 * 6.2922258377075195
Epoch 860, val loss: 0.916396975517273
Epoch 870, training loss: 12.756194114685059 = 0.15966583788394928 + 2.0 * 6.298264026641846
Epoch 870, val loss: 0.9227393865585327
Epoch 880, training loss: 12.738837242126465 = 0.15312732756137848 + 2.0 * 6.2928547859191895
Epoch 880, val loss: 0.92957603931427
Epoch 890, training loss: 12.724325180053711 = 0.14684931933879852 + 2.0 * 6.288737773895264
Epoch 890, val loss: 0.9365464448928833
Epoch 900, training loss: 12.717392921447754 = 0.1408115178346634 + 2.0 * 6.288290500640869
Epoch 900, val loss: 0.9437790513038635
Epoch 910, training loss: 12.717801094055176 = 0.135032057762146 + 2.0 * 6.291384696960449
Epoch 910, val loss: 0.951158881187439
Epoch 920, training loss: 12.702765464782715 = 0.12951162457466125 + 2.0 * 6.286626815795898
Epoch 920, val loss: 0.9588931798934937
Epoch 930, training loss: 12.697723388671875 = 0.12424203753471375 + 2.0 * 6.286740779876709
Epoch 930, val loss: 0.9667267799377441
Epoch 940, training loss: 12.696186065673828 = 0.11921198666095734 + 2.0 * 6.288486957550049
Epoch 940, val loss: 0.9747440814971924
Epoch 950, training loss: 12.691198348999023 = 0.11440503597259521 + 2.0 * 6.288396835327148
Epoch 950, val loss: 0.9828148484230042
Epoch 960, training loss: 12.678105354309082 = 0.10983869433403015 + 2.0 * 6.284133434295654
Epoch 960, val loss: 0.9911191463470459
Epoch 970, training loss: 12.675668716430664 = 0.10547158122062683 + 2.0 * 6.285098552703857
Epoch 970, val loss: 0.9994710087776184
Epoch 980, training loss: 12.667071342468262 = 0.10131143778562546 + 2.0 * 6.282879829406738
Epoch 980, val loss: 1.0078518390655518
Epoch 990, training loss: 12.661650657653809 = 0.09733772277832031 + 2.0 * 6.282156467437744
Epoch 990, val loss: 1.0163931846618652
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.737037037037037
0.8060094886663153
The final CL Acc:0.74815, 0.01318, The final GNN Acc:0.80689, 0.00163
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13216])
remove edge: torch.Size([2, 7888])
updated graph: torch.Size([2, 10548])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 19.146099090576172 = 1.952472448348999 + 2.0 * 8.596813201904297
Epoch 0, val loss: 1.9455758333206177
Epoch 10, training loss: 19.13589859008789 = 1.9429051876068115 + 2.0 * 8.59649658203125
Epoch 10, val loss: 1.9358750581741333
Epoch 20, training loss: 19.11930274963379 = 1.9310108423233032 + 2.0 * 8.594145774841309
Epoch 20, val loss: 1.9235789775848389
Epoch 30, training loss: 19.065048217773438 = 1.9143476486206055 + 2.0 * 8.575349807739258
Epoch 30, val loss: 1.906184196472168
Epoch 40, training loss: 18.75995445251465 = 1.8936949968338013 + 2.0 * 8.43312931060791
Epoch 40, val loss: 1.8858152627944946
Epoch 50, training loss: 17.63469886779785 = 1.87258780002594 + 2.0 * 7.8810553550720215
Epoch 50, val loss: 1.8650703430175781
Epoch 60, training loss: 16.553781509399414 = 1.8566515445709229 + 2.0 * 7.348565101623535
Epoch 60, val loss: 1.8504213094711304
Epoch 70, training loss: 15.891767501831055 = 1.8451263904571533 + 2.0 * 7.02332067489624
Epoch 70, val loss: 1.8398834466934204
Epoch 80, training loss: 15.609639167785645 = 1.8341715335845947 + 2.0 * 6.8877339363098145
Epoch 80, val loss: 1.8295756578445435
Epoch 90, training loss: 15.381659507751465 = 1.8210790157318115 + 2.0 * 6.780290126800537
Epoch 90, val loss: 1.817323923110962
Epoch 100, training loss: 15.228034019470215 = 1.808182716369629 + 2.0 * 6.709925651550293
Epoch 100, val loss: 1.8055763244628906
Epoch 110, training loss: 15.109728813171387 = 1.7960835695266724 + 2.0 * 6.656822681427002
Epoch 110, val loss: 1.7948938608169556
Epoch 120, training loss: 15.016236305236816 = 1.7848728895187378 + 2.0 * 6.6156816482543945
Epoch 120, val loss: 1.7850443124771118
Epoch 130, training loss: 14.939773559570312 = 1.7739368677139282 + 2.0 * 6.582918167114258
Epoch 130, val loss: 1.7753853797912598
Epoch 140, training loss: 14.873357772827148 = 1.7627636194229126 + 2.0 * 6.555296897888184
Epoch 140, val loss: 1.7655590772628784
Epoch 150, training loss: 14.814751625061035 = 1.7510493993759155 + 2.0 * 6.531851291656494
Epoch 150, val loss: 1.7552926540374756
Epoch 160, training loss: 14.763890266418457 = 1.7383100986480713 + 2.0 * 6.512790203094482
Epoch 160, val loss: 1.7442374229431152
Epoch 170, training loss: 14.71695613861084 = 1.7242149114608765 + 2.0 * 6.496370792388916
Epoch 170, val loss: 1.7321155071258545
Epoch 180, training loss: 14.672571182250977 = 1.7083582878112793 + 2.0 * 6.482106685638428
Epoch 180, val loss: 1.7185708284378052
Epoch 190, training loss: 14.631942749023438 = 1.6905629634857178 + 2.0 * 6.47068977355957
Epoch 190, val loss: 1.7033287286758423
Epoch 200, training loss: 14.586140632629395 = 1.670456051826477 + 2.0 * 6.4578423500061035
Epoch 200, val loss: 1.686272144317627
Epoch 210, training loss: 14.542778015136719 = 1.6477795839309692 + 2.0 * 6.4474992752075195
Epoch 210, val loss: 1.667015552520752
Epoch 220, training loss: 14.50027847290039 = 1.622168779373169 + 2.0 * 6.4390549659729
Epoch 220, val loss: 1.645208477973938
Epoch 230, training loss: 14.455968856811523 = 1.5936434268951416 + 2.0 * 6.4311628341674805
Epoch 230, val loss: 1.6209090948104858
Epoch 240, training loss: 14.406634330749512 = 1.5622271299362183 + 2.0 * 6.422203540802002
Epoch 240, val loss: 1.5941849946975708
Epoch 250, training loss: 14.35820484161377 = 1.5279314517974854 + 2.0 * 6.415136814117432
Epoch 250, val loss: 1.5651116371154785
Epoch 260, training loss: 14.315911293029785 = 1.4912606477737427 + 2.0 * 6.412325382232666
Epoch 260, val loss: 1.5342798233032227
Epoch 270, training loss: 14.259284019470215 = 1.4531018733978271 + 2.0 * 6.403090953826904
Epoch 270, val loss: 1.502241849899292
Epoch 280, training loss: 14.207571983337402 = 1.413685917854309 + 2.0 * 6.396943092346191
Epoch 280, val loss: 1.469361424446106
Epoch 290, training loss: 14.158531188964844 = 1.373740792274475 + 2.0 * 6.39239501953125
Epoch 290, val loss: 1.4363185167312622
Epoch 300, training loss: 14.106117248535156 = 1.3339351415634155 + 2.0 * 6.386091232299805
Epoch 300, val loss: 1.4038728475570679
Epoch 310, training loss: 14.05716323852539 = 1.2943288087844849 + 2.0 * 6.381417274475098
Epoch 310, val loss: 1.3720039129257202
Epoch 320, training loss: 14.013236999511719 = 1.2552974224090576 + 2.0 * 6.378969669342041
Epoch 320, val loss: 1.341030478477478
Epoch 330, training loss: 13.96402359008789 = 1.2172263860702515 + 2.0 * 6.373398780822754
Epoch 330, val loss: 1.3112788200378418
Epoch 340, training loss: 13.917084693908691 = 1.179555058479309 + 2.0 * 6.368764877319336
Epoch 340, val loss: 1.282333493232727
Epoch 350, training loss: 13.871582984924316 = 1.142230749130249 + 2.0 * 6.364675998687744
Epoch 350, val loss: 1.2540793418884277
Epoch 360, training loss: 13.83776569366455 = 1.105019211769104 + 2.0 * 6.366373062133789
Epoch 360, val loss: 1.2262340784072876
Epoch 370, training loss: 13.784642219543457 = 1.0685899257659912 + 2.0 * 6.358026027679443
Epoch 370, val loss: 1.1993600130081177
Epoch 380, training loss: 13.74309253692627 = 1.0326780080795288 + 2.0 * 6.355207443237305
Epoch 380, val loss: 1.1731475591659546
Epoch 390, training loss: 13.699756622314453 = 0.9970475435256958 + 2.0 * 6.351354598999023
Epoch 390, val loss: 1.1472866535186768
Epoch 400, training loss: 13.666963577270508 = 0.9618307948112488 + 2.0 * 6.352566242218018
Epoch 400, val loss: 1.1218247413635254
Epoch 410, training loss: 13.619363784790039 = 0.9275107383728027 + 2.0 * 6.345926761627197
Epoch 410, val loss: 1.097312569618225
Epoch 420, training loss: 13.579323768615723 = 0.8939866423606873 + 2.0 * 6.342668533325195
Epoch 420, val loss: 1.0734158754348755
Epoch 430, training loss: 13.540919303894043 = 0.8612118363380432 + 2.0 * 6.339853763580322
Epoch 430, val loss: 1.0499815940856934
Epoch 440, training loss: 13.524118423461914 = 0.8292762637138367 + 2.0 * 6.347421169281006
Epoch 440, val loss: 1.0272562503814697
Epoch 450, training loss: 13.473443984985352 = 0.798596203327179 + 2.0 * 6.337423801422119
Epoch 450, val loss: 1.0055354833602905
Epoch 460, training loss: 13.43591594696045 = 0.7691506147384644 + 2.0 * 6.333382606506348
Epoch 460, val loss: 0.984796941280365
Epoch 470, training loss: 13.406198501586914 = 0.7408041954040527 + 2.0 * 6.33269739151001
Epoch 470, val loss: 0.964858889579773
Epoch 480, training loss: 13.378727912902832 = 0.7136009335517883 + 2.0 * 6.332563400268555
Epoch 480, val loss: 0.9460136294364929
Epoch 490, training loss: 13.341211318969727 = 0.687800943851471 + 2.0 * 6.326704978942871
Epoch 490, val loss: 0.9283639788627625
Epoch 500, training loss: 13.311783790588379 = 0.6631301045417786 + 2.0 * 6.324326992034912
Epoch 500, val loss: 0.9116482734680176
Epoch 510, training loss: 13.286178588867188 = 0.6394719481468201 + 2.0 * 6.323353290557861
Epoch 510, val loss: 0.8958963751792908
Epoch 520, training loss: 13.260027885437012 = 0.6168755292892456 + 2.0 * 6.321576118469238
Epoch 520, val loss: 0.8810513615608215
Epoch 530, training loss: 13.23544979095459 = 0.5954192876815796 + 2.0 * 6.3200154304504395
Epoch 530, val loss: 0.8674035668373108
Epoch 540, training loss: 13.212807655334473 = 0.57491135597229 + 2.0 * 6.318948268890381
Epoch 540, val loss: 0.8546375036239624
Epoch 550, training loss: 13.187241554260254 = 0.5552839636802673 + 2.0 * 6.31597900390625
Epoch 550, val loss: 0.8427402377128601
Epoch 560, training loss: 13.169488906860352 = 0.5364811420440674 + 2.0 * 6.316504001617432
Epoch 560, val loss: 0.831566333770752
Epoch 570, training loss: 13.146137237548828 = 0.5184212327003479 + 2.0 * 6.3138580322265625
Epoch 570, val loss: 0.8212123513221741
Epoch 580, training loss: 13.124489784240723 = 0.5010194778442383 + 2.0 * 6.311735153198242
Epoch 580, val loss: 0.8114728331565857
Epoch 590, training loss: 13.120195388793945 = 0.48410564661026 + 2.0 * 6.318044662475586
Epoch 590, val loss: 0.8022907376289368
Epoch 600, training loss: 13.086137771606445 = 0.4677594006061554 + 2.0 * 6.309189319610596
Epoch 600, val loss: 0.7937332391738892
Epoch 610, training loss: 13.067416191101074 = 0.45173904299736023 + 2.0 * 6.307838439941406
Epoch 610, val loss: 0.78556227684021
Epoch 620, training loss: 13.051443099975586 = 0.4359012246131897 + 2.0 * 6.307770729064941
Epoch 620, val loss: 0.7775936722755432
Epoch 630, training loss: 13.02885913848877 = 0.4201790988445282 + 2.0 * 6.30433988571167
Epoch 630, val loss: 0.7699508666992188
Epoch 640, training loss: 13.009539604187012 = 0.40440496802330017 + 2.0 * 6.302567481994629
Epoch 640, val loss: 0.7625642418861389
Epoch 650, training loss: 13.01358413696289 = 0.38853222131729126 + 2.0 * 6.312525749206543
Epoch 650, val loss: 0.7552072405815125
Epoch 660, training loss: 12.9767427444458 = 0.37265336513519287 + 2.0 * 6.302044868469238
Epoch 660, val loss: 0.7479748725891113
Epoch 670, training loss: 12.955880165100098 = 0.35667070746421814 + 2.0 * 6.299604892730713
Epoch 670, val loss: 0.7410098910331726
Epoch 680, training loss: 12.939388275146484 = 0.34060290455818176 + 2.0 * 6.2993927001953125
Epoch 680, val loss: 0.7341668605804443
Epoch 690, training loss: 12.926983833312988 = 0.3245339095592499 + 2.0 * 6.301225185394287
Epoch 690, val loss: 0.7275391221046448
Epoch 700, training loss: 12.903617858886719 = 0.3084835112094879 + 2.0 * 6.297567367553711
Epoch 700, val loss: 0.7212337851524353
Epoch 710, training loss: 12.8843412399292 = 0.2927570044994354 + 2.0 * 6.295792102813721
Epoch 710, val loss: 0.7152780890464783
Epoch 720, training loss: 12.865914344787598 = 0.27730584144592285 + 2.0 * 6.294304370880127
Epoch 720, val loss: 0.7098352313041687
Epoch 730, training loss: 12.861846923828125 = 0.2623719871044159 + 2.0 * 6.299737453460693
Epoch 730, val loss: 0.704801082611084
Epoch 740, training loss: 12.835034370422363 = 0.24813368916511536 + 2.0 * 6.293450355529785
Epoch 740, val loss: 0.7003902196884155
Epoch 750, training loss: 12.818812370300293 = 0.23463958501815796 + 2.0 * 6.292086601257324
Epoch 750, val loss: 0.696564257144928
Epoch 760, training loss: 12.803606986999512 = 0.22189867496490479 + 2.0 * 6.290853977203369
Epoch 760, val loss: 0.6933345794677734
Epoch 770, training loss: 12.788983345031738 = 0.20990945398807526 + 2.0 * 6.289536952972412
Epoch 770, val loss: 0.6906905770301819
Epoch 780, training loss: 12.777338027954102 = 0.19868037104606628 + 2.0 * 6.2893290519714355
Epoch 780, val loss: 0.688517689704895
Epoch 790, training loss: 12.767231941223145 = 0.18823939561843872 + 2.0 * 6.289496421813965
Epoch 790, val loss: 0.6867564916610718
Epoch 800, training loss: 12.753463745117188 = 0.17851638793945312 + 2.0 * 6.287473678588867
Epoch 800, val loss: 0.6855260729789734
Epoch 810, training loss: 12.740804672241211 = 0.16948190331459045 + 2.0 * 6.285661220550537
Epoch 810, val loss: 0.6847777962684631
Epoch 820, training loss: 12.729265213012695 = 0.16102924942970276 + 2.0 * 6.284118175506592
Epoch 820, val loss: 0.6843875646591187
Epoch 830, training loss: 12.730730056762695 = 0.15312030911445618 + 2.0 * 6.28880500793457
Epoch 830, val loss: 0.6843565106391907
Epoch 840, training loss: 12.72643756866455 = 0.14584334194660187 + 2.0 * 6.290297031402588
Epoch 840, val loss: 0.6845294237136841
Epoch 850, training loss: 12.703338623046875 = 0.1390063613653183 + 2.0 * 6.282166004180908
Epoch 850, val loss: 0.6850472092628479
Epoch 860, training loss: 12.695045471191406 = 0.13263478875160217 + 2.0 * 6.281205177307129
Epoch 860, val loss: 0.6858547925949097
Epoch 870, training loss: 12.698005676269531 = 0.1266603320837021 + 2.0 * 6.285672664642334
Epoch 870, val loss: 0.6868319511413574
Epoch 880, training loss: 12.68355941772461 = 0.12106983363628387 + 2.0 * 6.28124475479126
Epoch 880, val loss: 0.6880086660385132
Epoch 890, training loss: 12.674280166625977 = 0.11581287533044815 + 2.0 * 6.279233455657959
Epoch 890, val loss: 0.6893754005432129
Epoch 900, training loss: 12.675895690917969 = 0.11085663735866547 + 2.0 * 6.282519340515137
Epoch 900, val loss: 0.690878689289093
Epoch 910, training loss: 12.663741111755371 = 0.10619986057281494 + 2.0 * 6.278770446777344
Epoch 910, val loss: 0.6925052404403687
Epoch 920, training loss: 12.656152725219727 = 0.10180545598268509 + 2.0 * 6.2771735191345215
Epoch 920, val loss: 0.6942636966705322
Epoch 930, training loss: 12.659674644470215 = 0.09765642881393433 + 2.0 * 6.281009197235107
Epoch 930, val loss: 0.6961163282394409
Epoch 940, training loss: 12.645845413208008 = 0.09374941885471344 + 2.0 * 6.276048183441162
Epoch 940, val loss: 0.6980995535850525
Epoch 950, training loss: 12.637979507446289 = 0.09003404527902603 + 2.0 * 6.273972511291504
Epoch 950, val loss: 0.7001723647117615
Epoch 960, training loss: 12.631184577941895 = 0.086513452231884 + 2.0 * 6.272335529327393
Epoch 960, val loss: 0.702345073223114
Epoch 970, training loss: 12.634150505065918 = 0.08316454291343689 + 2.0 * 6.275493144989014
Epoch 970, val loss: 0.7045921087265015
Epoch 980, training loss: 12.632421493530273 = 0.07998445630073547 + 2.0 * 6.276218414306641
Epoch 980, val loss: 0.7069550156593323
Epoch 990, training loss: 12.624190330505371 = 0.07699170708656311 + 2.0 * 6.273599147796631
Epoch 990, val loss: 0.7093609571456909
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7962962962962963
0.8365840801265156
=== training gcn model ===
Epoch 0, training loss: 19.151256561279297 = 1.9576948881149292 + 2.0 * 8.596780776977539
Epoch 0, val loss: 1.960661768913269
Epoch 10, training loss: 19.140111923217773 = 1.9472590684890747 + 2.0 * 8.596426010131836
Epoch 10, val loss: 1.9503449201583862
Epoch 20, training loss: 19.122913360595703 = 1.93485689163208 + 2.0 * 8.59402847290039
Epoch 20, val loss: 1.9375178813934326
Epoch 30, training loss: 19.074058532714844 = 1.9182602167129517 + 2.0 * 8.577898979187012
Epoch 30, val loss: 1.919911503791809
Epoch 40, training loss: 18.869304656982422 = 1.8973884582519531 + 2.0 * 8.485958099365234
Epoch 40, val loss: 1.8985832929611206
Epoch 50, training loss: 17.918058395385742 = 1.8756279945373535 + 2.0 * 8.021215438842773
Epoch 50, val loss: 1.8765559196472168
Epoch 60, training loss: 16.83492088317871 = 1.856053113937378 + 2.0 * 7.489433765411377
Epoch 60, val loss: 1.8577609062194824
Epoch 70, training loss: 16.04690933227539 = 1.841027021408081 + 2.0 * 7.102941513061523
Epoch 70, val loss: 1.8429309129714966
Epoch 80, training loss: 15.709300994873047 = 1.8264546394348145 + 2.0 * 6.941423416137695
Epoch 80, val loss: 1.8285748958587646
Epoch 90, training loss: 15.507264137268066 = 1.8109509944915771 + 2.0 * 6.848156452178955
Epoch 90, val loss: 1.8132143020629883
Epoch 100, training loss: 15.355674743652344 = 1.7957501411437988 + 2.0 * 6.779962539672852
Epoch 100, val loss: 1.7984064817428589
Epoch 110, training loss: 15.255943298339844 = 1.7811052799224854 + 2.0 * 6.737419128417969
Epoch 110, val loss: 1.7841166257858276
Epoch 120, training loss: 15.170609474182129 = 1.7668601274490356 + 2.0 * 6.701874732971191
Epoch 120, val loss: 1.7701785564422607
Epoch 130, training loss: 15.093642234802246 = 1.7526443004608154 + 2.0 * 6.670498847961426
Epoch 130, val loss: 1.7563837766647339
Epoch 140, training loss: 15.016550064086914 = 1.738004207611084 + 2.0 * 6.639272689819336
Epoch 140, val loss: 1.74259614944458
Epoch 150, training loss: 14.946194648742676 = 1.7224855422973633 + 2.0 * 6.611854553222656
Epoch 150, val loss: 1.7283573150634766
Epoch 160, training loss: 14.873762130737305 = 1.7053380012512207 + 2.0 * 6.584212303161621
Epoch 160, val loss: 1.7130846977233887
Epoch 170, training loss: 14.811507225036621 = 1.6860471963882446 + 2.0 * 6.562729835510254
Epoch 170, val loss: 1.6958810091018677
Epoch 180, training loss: 14.755046844482422 = 1.6637283563613892 + 2.0 * 6.545659065246582
Epoch 180, val loss: 1.6761529445648193
Epoch 190, training loss: 14.702588081359863 = 1.6381031274795532 + 2.0 * 6.532242298126221
Epoch 190, val loss: 1.6537015438079834
Epoch 200, training loss: 14.645705223083496 = 1.6090748310089111 + 2.0 * 6.518315315246582
Epoch 200, val loss: 1.6286425590515137
Epoch 210, training loss: 14.58732795715332 = 1.5763044357299805 + 2.0 * 6.50551176071167
Epoch 210, val loss: 1.6006635427474976
Epoch 220, training loss: 14.527618408203125 = 1.539516806602478 + 2.0 * 6.494050979614258
Epoch 220, val loss: 1.5693954229354858
Epoch 230, training loss: 14.470661163330078 = 1.4983742237091064 + 2.0 * 6.486143589019775
Epoch 230, val loss: 1.5347689390182495
Epoch 240, training loss: 14.404862403869629 = 1.4536879062652588 + 2.0 * 6.475587368011475
Epoch 240, val loss: 1.4973698854446411
Epoch 250, training loss: 14.339916229248047 = 1.4055509567260742 + 2.0 * 6.467182636260986
Epoch 250, val loss: 1.4574006795883179
Epoch 260, training loss: 14.273031234741211 = 1.3547924757003784 + 2.0 * 6.4591193199157715
Epoch 260, val loss: 1.4157990217208862
Epoch 270, training loss: 14.20388412475586 = 1.3022881746292114 + 2.0 * 6.450798034667969
Epoch 270, val loss: 1.373133659362793
Epoch 280, training loss: 14.140583038330078 = 1.248823642730713 + 2.0 * 6.4458794593811035
Epoch 280, val loss: 1.3301554918289185
Epoch 290, training loss: 14.072417259216309 = 1.1959638595581055 + 2.0 * 6.438226699829102
Epoch 290, val loss: 1.2885633707046509
Epoch 300, training loss: 14.009895324707031 = 1.1448092460632324 + 2.0 * 6.43254280090332
Epoch 300, val loss: 1.248795986175537
Epoch 310, training loss: 13.948380470275879 = 1.095400094985962 + 2.0 * 6.426490306854248
Epoch 310, val loss: 1.211033582687378
Epoch 320, training loss: 13.898115158081055 = 1.0487786531448364 + 2.0 * 6.424668312072754
Epoch 320, val loss: 1.1754144430160522
Epoch 330, training loss: 13.836807250976562 = 1.004819393157959 + 2.0 * 6.415993690490723
Epoch 330, val loss: 1.142475962638855
Epoch 340, training loss: 13.78511905670166 = 0.9630929231643677 + 2.0 * 6.411013126373291
Epoch 340, val loss: 1.111271619796753
Epoch 350, training loss: 13.76086139678955 = 0.9234034419059753 + 2.0 * 6.418728828430176
Epoch 350, val loss: 1.0816389322280884
Epoch 360, training loss: 13.691047668457031 = 0.8860551118850708 + 2.0 * 6.402496337890625
Epoch 360, val loss: 1.0539681911468506
Epoch 370, training loss: 13.64768123626709 = 0.8506148457527161 + 2.0 * 6.398533344268799
Epoch 370, val loss: 1.0278652906417847
Epoch 380, training loss: 13.603979110717773 = 0.8166639804840088 + 2.0 * 6.393657684326172
Epoch 380, val loss: 1.0029054880142212
Epoch 390, training loss: 13.587111473083496 = 0.7839291095733643 + 2.0 * 6.4015913009643555
Epoch 390, val loss: 0.9793140292167664
Epoch 400, training loss: 13.533246994018555 = 0.7533390522003174 + 2.0 * 6.389954090118408
Epoch 400, val loss: 0.9572061896324158
Epoch 410, training loss: 13.491107940673828 = 0.724337100982666 + 2.0 * 6.383385181427002
Epoch 410, val loss: 0.93673175573349
Epoch 420, training loss: 13.456517219543457 = 0.6966826915740967 + 2.0 * 6.379917144775391
Epoch 420, val loss: 0.9178199768066406
Epoch 430, training loss: 13.425235748291016 = 0.6702815890312195 + 2.0 * 6.377477169036865
Epoch 430, val loss: 0.9003542065620422
Epoch 440, training loss: 13.403780937194824 = 0.6454525589942932 + 2.0 * 6.379164218902588
Epoch 440, val loss: 0.8843221068382263
Epoch 450, training loss: 13.365541458129883 = 0.6220529675483704 + 2.0 * 6.371744155883789
Epoch 450, val loss: 0.8700739741325378
Epoch 460, training loss: 13.335538864135742 = 0.599871814250946 + 2.0 * 6.367833614349365
Epoch 460, val loss: 0.857216477394104
Epoch 470, training loss: 13.323569297790527 = 0.5788043141365051 + 2.0 * 6.372382640838623
Epoch 470, val loss: 0.8455051779747009
Epoch 480, training loss: 13.287394523620605 = 0.5587030649185181 + 2.0 * 6.364345550537109
Epoch 480, val loss: 0.8350434899330139
Epoch 490, training loss: 13.260189056396484 = 0.5395331978797913 + 2.0 * 6.36032772064209
Epoch 490, val loss: 0.8256567716598511
Epoch 500, training loss: 13.239640235900879 = 0.5210352540016174 + 2.0 * 6.359302520751953
Epoch 500, val loss: 0.8170814514160156
Epoch 510, training loss: 13.217828750610352 = 0.5031701326370239 + 2.0 * 6.357329368591309
Epoch 510, val loss: 0.809196412563324
Epoch 520, training loss: 13.192183494567871 = 0.48584988713264465 + 2.0 * 6.353166580200195
Epoch 520, val loss: 0.8019912242889404
Epoch 530, training loss: 13.172894477844238 = 0.46893012523651123 + 2.0 * 6.351982116699219
Epoch 530, val loss: 0.7952804565429688
Epoch 540, training loss: 13.149337768554688 = 0.45232048630714417 + 2.0 * 6.348508834838867
Epoch 540, val loss: 0.7889986634254456
Epoch 550, training loss: 13.129233360290527 = 0.43588465452194214 + 2.0 * 6.34667444229126
Epoch 550, val loss: 0.7830188274383545
Epoch 560, training loss: 13.108642578125 = 0.41966763138771057 + 2.0 * 6.34448766708374
Epoch 560, val loss: 0.7771618366241455
Epoch 570, training loss: 13.089910507202148 = 0.40353628993034363 + 2.0 * 6.34318733215332
Epoch 570, val loss: 0.7716004848480225
Epoch 580, training loss: 13.074018478393555 = 0.3875513970851898 + 2.0 * 6.343233585357666
Epoch 580, val loss: 0.7662190794944763
Epoch 590, training loss: 13.051565170288086 = 0.37170833349227905 + 2.0 * 6.33992862701416
Epoch 590, val loss: 0.7610309720039368
Epoch 600, training loss: 13.030048370361328 = 0.35591551661491394 + 2.0 * 6.337066650390625
Epoch 600, val loss: 0.7561953663825989
Epoch 610, training loss: 13.027244567871094 = 0.3403334617614746 + 2.0 * 6.343455791473389
Epoch 610, val loss: 0.7516539692878723
Epoch 620, training loss: 12.992783546447754 = 0.3249130845069885 + 2.0 * 6.333935260772705
Epoch 620, val loss: 0.7474397420883179
Epoch 630, training loss: 12.973701477050781 = 0.30983561277389526 + 2.0 * 6.33193302154541
Epoch 630, val loss: 0.7435892224311829
Epoch 640, training loss: 12.9577054977417 = 0.29503798484802246 + 2.0 * 6.331333637237549
Epoch 640, val loss: 0.7401435375213623
Epoch 650, training loss: 12.944856643676758 = 0.2806468605995178 + 2.0 * 6.332104682922363
Epoch 650, val loss: 0.7371587753295898
Epoch 660, training loss: 12.920888900756836 = 0.2666618227958679 + 2.0 * 6.327113628387451
Epoch 660, val loss: 0.7345762252807617
Epoch 670, training loss: 12.90450668334961 = 0.2531279921531677 + 2.0 * 6.325689315795898
Epoch 670, val loss: 0.7325277328491211
Epoch 680, training loss: 12.89990234375 = 0.24003879725933075 + 2.0 * 6.329931735992432
Epoch 680, val loss: 0.730953574180603
Epoch 690, training loss: 12.878923416137695 = 0.22760073840618134 + 2.0 * 6.3256611824035645
Epoch 690, val loss: 0.729665994644165
Epoch 700, training loss: 12.859845161437988 = 0.21564039587974548 + 2.0 * 6.3221025466918945
Epoch 700, val loss: 0.728968620300293
Epoch 710, training loss: 12.845595359802246 = 0.20424753427505493 + 2.0 * 6.320673942565918
Epoch 710, val loss: 0.7287678718566895
Epoch 720, training loss: 12.831128120422363 = 0.19342657923698425 + 2.0 * 6.318850994110107
Epoch 720, val loss: 0.7290167212486267
Epoch 730, training loss: 12.841130256652832 = 0.18317268788814545 + 2.0 * 6.328979015350342
Epoch 730, val loss: 0.7296410799026489
Epoch 740, training loss: 12.806975364685059 = 0.17346477508544922 + 2.0 * 6.316755294799805
Epoch 740, val loss: 0.7307144999504089
Epoch 750, training loss: 12.794855117797852 = 0.1643889993429184 + 2.0 * 6.31523323059082
Epoch 750, val loss: 0.7321491241455078
Epoch 760, training loss: 12.783096313476562 = 0.15582971274852753 + 2.0 * 6.313633441925049
Epoch 760, val loss: 0.7340645790100098
Epoch 770, training loss: 12.773683547973633 = 0.14777976274490356 + 2.0 * 6.312952041625977
Epoch 770, val loss: 0.7363398671150208
Epoch 780, training loss: 12.764727592468262 = 0.1402035653591156 + 2.0 * 6.312262058258057
Epoch 780, val loss: 0.7389249801635742
Epoch 790, training loss: 12.766311645507812 = 0.1331467479467392 + 2.0 * 6.316582679748535
Epoch 790, val loss: 0.7417066693305969
Epoch 800, training loss: 12.748542785644531 = 0.12658990919589996 + 2.0 * 6.310976505279541
Epoch 800, val loss: 0.7447579503059387
Epoch 810, training loss: 12.737802505493164 = 0.12039895355701447 + 2.0 * 6.308701992034912
Epoch 810, val loss: 0.7481640577316284
Epoch 820, training loss: 12.728303909301758 = 0.1146019846200943 + 2.0 * 6.306850910186768
Epoch 820, val loss: 0.7518090009689331
Epoch 830, training loss: 12.733854293823242 = 0.10913995653390884 + 2.0 * 6.312356948852539
Epoch 830, val loss: 0.7556803226470947
Epoch 840, training loss: 12.715163230895996 = 0.10406763851642609 + 2.0 * 6.305547714233398
Epoch 840, val loss: 0.7595975399017334
Epoch 850, training loss: 12.709857940673828 = 0.09929750859737396 + 2.0 * 6.3052802085876465
Epoch 850, val loss: 0.7637273669242859
Epoch 860, training loss: 12.708678245544434 = 0.09480568766593933 + 2.0 * 6.306936264038086
Epoch 860, val loss: 0.7680556178092957
Epoch 870, training loss: 12.710195541381836 = 0.09061770886182785 + 2.0 * 6.309788703918457
Epoch 870, val loss: 0.7723778486251831
Epoch 880, training loss: 12.69457721710205 = 0.08665388822555542 + 2.0 * 6.303961753845215
Epoch 880, val loss: 0.7767936587333679
Epoch 890, training loss: 12.683874130249023 = 0.08296642452478409 + 2.0 * 6.3004536628723145
Epoch 890, val loss: 0.7812841534614563
Epoch 900, training loss: 12.6771821975708 = 0.07945770770311356 + 2.0 * 6.298862457275391
Epoch 900, val loss: 0.7859545946121216
Epoch 910, training loss: 12.672404289245605 = 0.07614587247371674 + 2.0 * 6.298129081726074
Epoch 910, val loss: 0.790668249130249
Epoch 920, training loss: 12.688970565795898 = 0.07300413399934769 + 2.0 * 6.3079833984375
Epoch 920, val loss: 0.7953917384147644
Epoch 930, training loss: 12.665728569030762 = 0.07007461786270142 + 2.0 * 6.297826766967773
Epoch 930, val loss: 0.8000802993774414
Epoch 940, training loss: 12.658278465270996 = 0.06730052083730698 + 2.0 * 6.2954888343811035
Epoch 940, val loss: 0.8047581315040588
Epoch 950, training loss: 12.656242370605469 = 0.06466799974441528 + 2.0 * 6.295787334442139
Epoch 950, val loss: 0.8095521926879883
Epoch 960, training loss: 12.656414985656738 = 0.06218647584319115 + 2.0 * 6.297114372253418
Epoch 960, val loss: 0.8144003748893738
Epoch 970, training loss: 12.659921646118164 = 0.05982828140258789 + 2.0 * 6.300046920776367
Epoch 970, val loss: 0.8191331028938293
Epoch 980, training loss: 12.64619255065918 = 0.05759203061461449 + 2.0 * 6.294300079345703
Epoch 980, val loss: 0.8237954378128052
Epoch 990, training loss: 12.637803077697754 = 0.05548989400267601 + 2.0 * 6.291156768798828
Epoch 990, val loss: 0.8285564184188843
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.788888888888889
0.838165524512388
=== training gcn model ===
Epoch 0, training loss: 19.13030433654785 = 1.9367249011993408 + 2.0 * 8.596789360046387
Epoch 0, val loss: 1.93648362159729
Epoch 10, training loss: 19.1190128326416 = 1.9263859987258911 + 2.0 * 8.5963134765625
Epoch 10, val loss: 1.9253971576690674
Epoch 20, training loss: 19.099411010742188 = 1.9138635396957397 + 2.0 * 8.5927734375
Epoch 20, val loss: 1.9117834568023682
Epoch 30, training loss: 19.032896041870117 = 1.8970527648925781 + 2.0 * 8.56792163848877
Epoch 30, val loss: 1.8934725522994995
Epoch 40, training loss: 18.67906951904297 = 1.8782641887664795 + 2.0 * 8.400403022766113
Epoch 40, val loss: 1.874239444732666
Epoch 50, training loss: 17.528318405151367 = 1.8591299057006836 + 2.0 * 7.834594249725342
Epoch 50, val loss: 1.8553210496902466
Epoch 60, training loss: 16.76628303527832 = 1.8461604118347168 + 2.0 * 7.460061073303223
Epoch 60, val loss: 1.843458652496338
Epoch 70, training loss: 16.188167572021484 = 1.8364722728729248 + 2.0 * 7.17584753036499
Epoch 70, val loss: 1.8340625762939453
Epoch 80, training loss: 15.699454307556152 = 1.8264656066894531 + 2.0 * 6.93649435043335
Epoch 80, val loss: 1.8244655132293701
Epoch 90, training loss: 15.475793838500977 = 1.816171646118164 + 2.0 * 6.829811096191406
Epoch 90, val loss: 1.8147320747375488
Epoch 100, training loss: 15.31811809539795 = 1.8046931028366089 + 2.0 * 6.756712436676025
Epoch 100, val loss: 1.804338812828064
Epoch 110, training loss: 15.198476791381836 = 1.7930908203125 + 2.0 * 6.702692985534668
Epoch 110, val loss: 1.7942620515823364
Epoch 120, training loss: 15.10498046875 = 1.7823803424835205 + 2.0 * 6.661300182342529
Epoch 120, val loss: 1.7849922180175781
Epoch 130, training loss: 15.026727676391602 = 1.77182137966156 + 2.0 * 6.627453327178955
Epoch 130, val loss: 1.7758362293243408
Epoch 140, training loss: 14.956586837768555 = 1.7607078552246094 + 2.0 * 6.597939491271973
Epoch 140, val loss: 1.7663766145706177
Epoch 150, training loss: 14.89584732055664 = 1.7486300468444824 + 2.0 * 6.5736083984375
Epoch 150, val loss: 1.7562787532806396
Epoch 160, training loss: 14.840728759765625 = 1.7353606224060059 + 2.0 * 6.5526838302612305
Epoch 160, val loss: 1.7452998161315918
Epoch 170, training loss: 14.78696346282959 = 1.7207454442977905 + 2.0 * 6.533109188079834
Epoch 170, val loss: 1.7333199977874756
Epoch 180, training loss: 14.735970497131348 = 1.7044955492019653 + 2.0 * 6.515737533569336
Epoch 180, val loss: 1.7202180624008179
Epoch 190, training loss: 14.687448501586914 = 1.686224102973938 + 2.0 * 6.500612258911133
Epoch 190, val loss: 1.7055591344833374
Epoch 200, training loss: 14.642433166503906 = 1.66557776927948 + 2.0 * 6.488427639007568
Epoch 200, val loss: 1.6890156269073486
Epoch 210, training loss: 14.59294605255127 = 1.6426143646240234 + 2.0 * 6.475165843963623
Epoch 210, val loss: 1.6705670356750488
Epoch 220, training loss: 14.5471830368042 = 1.6170837879180908 + 2.0 * 6.465049743652344
Epoch 220, val loss: 1.650092601776123
Epoch 230, training loss: 14.498760223388672 = 1.5888878107070923 + 2.0 * 6.4549360275268555
Epoch 230, val loss: 1.6274982690811157
Epoch 240, training loss: 14.45120620727539 = 1.5580925941467285 + 2.0 * 6.44655704498291
Epoch 240, val loss: 1.6027151346206665
Epoch 250, training loss: 14.40284252166748 = 1.5249804258346558 + 2.0 * 6.438930988311768
Epoch 250, val loss: 1.576015591621399
Epoch 260, training loss: 14.352325439453125 = 1.4898966550827026 + 2.0 * 6.431214332580566
Epoch 260, val loss: 1.5476332902908325
Epoch 270, training loss: 14.302562713623047 = 1.453302264213562 + 2.0 * 6.424630165100098
Epoch 270, val loss: 1.5179979801177979
Epoch 280, training loss: 14.250067710876465 = 1.4153484106063843 + 2.0 * 6.417359828948975
Epoch 280, val loss: 1.4872413873672485
Epoch 290, training loss: 14.199888229370117 = 1.3760428428649902 + 2.0 * 6.411922931671143
Epoch 290, val loss: 1.4553943872451782
Epoch 300, training loss: 14.15540885925293 = 1.3360501527786255 + 2.0 * 6.409679412841797
Epoch 300, val loss: 1.4226388931274414
Epoch 310, training loss: 14.09910774230957 = 1.295756459236145 + 2.0 * 6.401675701141357
Epoch 310, val loss: 1.3899435997009277
Epoch 320, training loss: 14.046045303344727 = 1.255087971687317 + 2.0 * 6.39547872543335
Epoch 320, val loss: 1.357003927230835
Epoch 330, training loss: 13.996944427490234 = 1.213844895362854 + 2.0 * 6.391549587249756
Epoch 330, val loss: 1.3235989809036255
Epoch 340, training loss: 13.949430465698242 = 1.1721307039260864 + 2.0 * 6.388649940490723
Epoch 340, val loss: 1.2898354530334473
Epoch 350, training loss: 13.895368576049805 = 1.1301393508911133 + 2.0 * 6.382614612579346
Epoch 350, val loss: 1.255872130393982
Epoch 360, training loss: 13.84499740600586 = 1.0875540971755981 + 2.0 * 6.378721714019775
Epoch 360, val loss: 1.2215367555618286
Epoch 370, training loss: 13.802087783813477 = 1.0444327592849731 + 2.0 * 6.3788275718688965
Epoch 370, val loss: 1.1866832971572876
Epoch 380, training loss: 13.747941970825195 = 1.0013666152954102 + 2.0 * 6.373287677764893
Epoch 380, val loss: 1.1521050930023193
Epoch 390, training loss: 13.700554847717285 = 0.9589712619781494 + 2.0 * 6.370791912078857
Epoch 390, val loss: 1.1179758310317993
Epoch 400, training loss: 13.648963928222656 = 0.9169579148292542 + 2.0 * 6.366003036499023
Epoch 400, val loss: 1.0843136310577393
Epoch 410, training loss: 13.601943969726562 = 0.8755725026130676 + 2.0 * 6.363185882568359
Epoch 410, val loss: 1.051027536392212
Epoch 420, training loss: 13.555459022521973 = 0.8350340127944946 + 2.0 * 6.360212326049805
Epoch 420, val loss: 1.0185362100601196
Epoch 430, training loss: 13.512990951538086 = 0.795652449131012 + 2.0 * 6.358669281005859
Epoch 430, val loss: 0.987038254737854
Epoch 440, training loss: 13.48127555847168 = 0.7581957578659058 + 2.0 * 6.361539840698242
Epoch 440, val loss: 0.956925094127655
Epoch 450, training loss: 13.433029174804688 = 0.722960889339447 + 2.0 * 6.355034351348877
Epoch 450, val loss: 0.9288819432258606
Epoch 460, training loss: 13.391362190246582 = 0.6898068785667419 + 2.0 * 6.350777626037598
Epoch 460, val loss: 0.9027829766273499
Epoch 470, training loss: 13.355558395385742 = 0.6585535407066345 + 2.0 * 6.3485026359558105
Epoch 470, val loss: 0.8783938884735107
Epoch 480, training loss: 13.327546119689941 = 0.6291095614433289 + 2.0 * 6.349218368530273
Epoch 480, val loss: 0.8556873798370361
Epoch 490, training loss: 13.292842864990234 = 0.6015231013298035 + 2.0 * 6.3456597328186035
Epoch 490, val loss: 0.8347727060317993
Epoch 500, training loss: 13.260590553283691 = 0.5757013559341431 + 2.0 * 6.34244441986084
Epoch 500, val loss: 0.8153945207595825
Epoch 510, training loss: 13.232583045959473 = 0.5513337254524231 + 2.0 * 6.340624809265137
Epoch 510, val loss: 0.7977015972137451
Epoch 520, training loss: 13.205009460449219 = 0.5284191966056824 + 2.0 * 6.338294982910156
Epoch 520, val loss: 0.7813441157341003
Epoch 530, training loss: 13.181751251220703 = 0.5067594051361084 + 2.0 * 6.337495803833008
Epoch 530, val loss: 0.7663822770118713
Epoch 540, training loss: 13.157596588134766 = 0.4862305223941803 + 2.0 * 6.3356828689575195
Epoch 540, val loss: 0.752578616142273
Epoch 550, training loss: 13.131702423095703 = 0.46660667657852173 + 2.0 * 6.332547664642334
Epoch 550, val loss: 0.7397022247314453
Epoch 560, training loss: 13.113673210144043 = 0.447687566280365 + 2.0 * 6.332993030548096
Epoch 560, val loss: 0.7277438640594482
Epoch 570, training loss: 13.088061332702637 = 0.4294273555278778 + 2.0 * 6.329317092895508
Epoch 570, val loss: 0.7165182828903198
Epoch 580, training loss: 13.065675735473633 = 0.4116209149360657 + 2.0 * 6.327027320861816
Epoch 580, val loss: 0.7058786749839783
Epoch 590, training loss: 13.044920921325684 = 0.3941808342933655 + 2.0 * 6.325369834899902
Epoch 590, val loss: 0.6957504749298096
Epoch 600, training loss: 13.031355857849121 = 0.3769756555557251 + 2.0 * 6.327189922332764
Epoch 600, val loss: 0.6859638690948486
Epoch 610, training loss: 13.011159896850586 = 0.35997772216796875 + 2.0 * 6.325591087341309
Epoch 610, val loss: 0.6765468120574951
Epoch 620, training loss: 12.994767189025879 = 0.3432821035385132 + 2.0 * 6.325742721557617
Epoch 620, val loss: 0.6675080060958862
Epoch 630, training loss: 12.967913627624512 = 0.3269873559474945 + 2.0 * 6.320463180541992
Epoch 630, val loss: 0.6586927175521851
Epoch 640, training loss: 12.946643829345703 = 0.31095457077026367 + 2.0 * 6.317844867706299
Epoch 640, val loss: 0.6502646207809448
Epoch 650, training loss: 12.928096771240234 = 0.2952272593975067 + 2.0 * 6.316434860229492
Epoch 650, val loss: 0.6420579552650452
Epoch 660, training loss: 12.918726921081543 = 0.27985939383506775 + 2.0 * 6.319433689117432
Epoch 660, val loss: 0.6341373324394226
Epoch 670, training loss: 12.897211074829102 = 0.2649845778942108 + 2.0 * 6.316113471984863
Epoch 670, val loss: 0.6264424920082092
Epoch 680, training loss: 12.877021789550781 = 0.2506505250930786 + 2.0 * 6.313185691833496
Epoch 680, val loss: 0.619185209274292
Epoch 690, training loss: 12.858838081359863 = 0.23688793182373047 + 2.0 * 6.310975074768066
Epoch 690, val loss: 0.6124383807182312
Epoch 700, training loss: 12.85776424407959 = 0.2237185537815094 + 2.0 * 6.317022800445557
Epoch 700, val loss: 0.606224000453949
Epoch 710, training loss: 12.838462829589844 = 0.2113018035888672 + 2.0 * 6.313580513000488
Epoch 710, val loss: 0.6001800298690796
Epoch 720, training loss: 12.816812515258789 = 0.19954606890678406 + 2.0 * 6.308633327484131
Epoch 720, val loss: 0.5948368906974792
Epoch 730, training loss: 12.800999641418457 = 0.18844358623027802 + 2.0 * 6.306278228759766
Epoch 730, val loss: 0.5900397896766663
Epoch 740, training loss: 12.788041114807129 = 0.17795613408088684 + 2.0 * 6.305042266845703
Epoch 740, val loss: 0.5857983231544495
Epoch 750, training loss: 12.777938842773438 = 0.1680881232023239 + 2.0 * 6.304925441741943
Epoch 750, val loss: 0.5820273756980896
Epoch 760, training loss: 12.768506050109863 = 0.1588575392961502 + 2.0 * 6.304824352264404
Epoch 760, val loss: 0.5787774324417114
Epoch 770, training loss: 12.755552291870117 = 0.1502385139465332 + 2.0 * 6.302657127380371
Epoch 770, val loss: 0.5759661793708801
Epoch 780, training loss: 12.744423866271973 = 0.1422245055437088 + 2.0 * 6.30109977722168
Epoch 780, val loss: 0.5736100673675537
Epoch 790, training loss: 12.734612464904785 = 0.1347036063671112 + 2.0 * 6.299954414367676
Epoch 790, val loss: 0.5717549920082092
Epoch 800, training loss: 12.733562469482422 = 0.12765511870384216 + 2.0 * 6.302953720092773
Epoch 800, val loss: 0.570270836353302
Epoch 810, training loss: 12.722652435302734 = 0.12107034027576447 + 2.0 * 6.300791263580322
Epoch 810, val loss: 0.5692839026451111
Epoch 820, training loss: 12.709089279174805 = 0.11490993201732635 + 2.0 * 6.297089576721191
Epoch 820, val loss: 0.5685261487960815
Epoch 830, training loss: 12.708600044250488 = 0.10915565490722656 + 2.0 * 6.299722194671631
Epoch 830, val loss: 0.5682069063186646
Epoch 840, training loss: 12.693557739257812 = 0.10375460237264633 + 2.0 * 6.294901371002197
Epoch 840, val loss: 0.5681399703025818
Epoch 850, training loss: 12.685900688171387 = 0.09869588911533356 + 2.0 * 6.293602466583252
Epoch 850, val loss: 0.5683408379554749
Epoch 860, training loss: 12.698444366455078 = 0.09395689517259598 + 2.0 * 6.302243709564209
Epoch 860, val loss: 0.5689241886138916
Epoch 870, training loss: 12.678732872009277 = 0.08953282237052917 + 2.0 * 6.294600009918213
Epoch 870, val loss: 0.5694131851196289
Epoch 880, training loss: 12.664984703063965 = 0.08537087589502335 + 2.0 * 6.289806842803955
Epoch 880, val loss: 0.570311963558197
Epoch 890, training loss: 12.659194946289062 = 0.08146035671234131 + 2.0 * 6.288867473602295
Epoch 890, val loss: 0.5714355111122131
Epoch 900, training loss: 12.656783103942871 = 0.07777857035398483 + 2.0 * 6.289502143859863
Epoch 900, val loss: 0.5726621150970459
Epoch 910, training loss: 12.654472351074219 = 0.07432273030281067 + 2.0 * 6.290074825286865
Epoch 910, val loss: 0.5742446184158325
Epoch 920, training loss: 12.643418312072754 = 0.07109008729457855 + 2.0 * 6.286164283752441
Epoch 920, val loss: 0.5756970643997192
Epoch 930, training loss: 12.638873100280762 = 0.06806789338588715 + 2.0 * 6.285402774810791
Epoch 930, val loss: 0.5773292779922485
Epoch 940, training loss: 12.63442325592041 = 0.0652056485414505 + 2.0 * 6.284608840942383
Epoch 940, val loss: 0.5791662931442261
Epoch 950, training loss: 12.628743171691895 = 0.06250464916229248 + 2.0 * 6.283119201660156
Epoch 950, val loss: 0.5811091661453247
Epoch 960, training loss: 12.626402854919434 = 0.059954095631837845 + 2.0 * 6.283224582672119
Epoch 960, val loss: 0.5831893086433411
Epoch 970, training loss: 12.630722045898438 = 0.05754415690898895 + 2.0 * 6.2865891456604
Epoch 970, val loss: 0.5853050947189331
Epoch 980, training loss: 12.620270729064941 = 0.05529899150133133 + 2.0 * 6.2824859619140625
Epoch 980, val loss: 0.5873088836669922
Epoch 990, training loss: 12.613556861877441 = 0.05318098142743111 + 2.0 * 6.280188083648682
Epoch 990, val loss: 0.589501142501831
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.788888888888889
0.8392198207696363
The final CL Acc:0.79136, 0.00349, The final GNN Acc:0.83799, 0.00108
Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.5 der2:0.4 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11628])
remove edge: torch.Size([2, 9516])
updated graph: torch.Size([2, 10588])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 19.139080047607422 = 1.94536554813385 + 2.0 * 8.596857070922852
Epoch 0, val loss: 1.936793327331543
Epoch 10, training loss: 19.12896728515625 = 1.935759425163269 + 2.0 * 8.596604347229004
Epoch 10, val loss: 1.928217887878418
Epoch 20, training loss: 19.113832473754883 = 1.9240573644638062 + 2.0 * 8.594887733459473
Epoch 20, val loss: 1.9176923036575317
Epoch 30, training loss: 19.073883056640625 = 1.9086418151855469 + 2.0 * 8.582620620727539
Epoch 30, val loss: 1.9039193391799927
Epoch 40, training loss: 18.902870178222656 = 1.8894104957580566 + 2.0 * 8.506730079650879
Epoch 40, val loss: 1.8871372938156128
Epoch 50, training loss: 18.108118057250977 = 1.8685564994812012 + 2.0 * 8.119780540466309
Epoch 50, val loss: 1.8683395385742188
Epoch 60, training loss: 17.295513153076172 = 1.8466706275939941 + 2.0 * 7.72442102432251
Epoch 60, val loss: 1.848832607269287
Epoch 70, training loss: 16.81496238708496 = 1.8281584978103638 + 2.0 * 7.493402004241943
Epoch 70, val loss: 1.832856297492981
Epoch 80, training loss: 16.331876754760742 = 1.8113526105880737 + 2.0 * 7.260262489318848
Epoch 80, val loss: 1.8185673952102661
Epoch 90, training loss: 16.05327606201172 = 1.7940278053283691 + 2.0 * 7.129624366760254
Epoch 90, val loss: 1.8050973415374756
Epoch 100, training loss: 15.852444648742676 = 1.776249885559082 + 2.0 * 7.038097381591797
Epoch 100, val loss: 1.7900981903076172
Epoch 110, training loss: 15.649614334106445 = 1.7592394351959229 + 2.0 * 6.945187568664551
Epoch 110, val loss: 1.7756038904190063
Epoch 120, training loss: 15.456701278686523 = 1.743438482284546 + 2.0 * 6.856631278991699
Epoch 120, val loss: 1.7613664865493774
Epoch 130, training loss: 15.320629119873047 = 1.7268749475479126 + 2.0 * 6.796876907348633
Epoch 130, val loss: 1.7465647459030151
Epoch 140, training loss: 15.212179183959961 = 1.7080122232437134 + 2.0 * 6.7520833015441895
Epoch 140, val loss: 1.7305761575698853
Epoch 150, training loss: 15.118428230285645 = 1.6870648860931396 + 2.0 * 6.715681552886963
Epoch 150, val loss: 1.713248610496521
Epoch 160, training loss: 15.034215927124023 = 1.6639609336853027 + 2.0 * 6.6851277351379395
Epoch 160, val loss: 1.6937499046325684
Epoch 170, training loss: 14.952903747558594 = 1.6383320093154907 + 2.0 * 6.657285690307617
Epoch 170, val loss: 1.6723483800888062
Epoch 180, training loss: 14.875022888183594 = 1.609878659248352 + 2.0 * 6.632572174072266
Epoch 180, val loss: 1.648738980293274
Epoch 190, training loss: 14.805627822875977 = 1.5783302783966064 + 2.0 * 6.613648891448975
Epoch 190, val loss: 1.6226662397384644
Epoch 200, training loss: 14.73443603515625 = 1.544028401374817 + 2.0 * 6.595203876495361
Epoch 200, val loss: 1.5943148136138916
Epoch 210, training loss: 14.663668632507324 = 1.507314682006836 + 2.0 * 6.578176975250244
Epoch 210, val loss: 1.5638954639434814
Epoch 220, training loss: 14.59701156616211 = 1.468191146850586 + 2.0 * 6.564410209655762
Epoch 220, val loss: 1.5315009355545044
Epoch 230, training loss: 14.539463996887207 = 1.4270554780960083 + 2.0 * 6.556204319000244
Epoch 230, val loss: 1.4975039958953857
Epoch 240, training loss: 14.467679977416992 = 1.38516104221344 + 2.0 * 6.541259288787842
Epoch 240, val loss: 1.4630939960479736
Epoch 250, training loss: 14.403553009033203 = 1.34291410446167 + 2.0 * 6.530319690704346
Epoch 250, val loss: 1.428646206855774
Epoch 260, training loss: 14.3453369140625 = 1.3008642196655273 + 2.0 * 6.522236347198486
Epoch 260, val loss: 1.3949753046035767
Epoch 270, training loss: 14.284709930419922 = 1.2598286867141724 + 2.0 * 6.5124406814575195
Epoch 270, val loss: 1.3625879287719727
Epoch 280, training loss: 14.227392196655273 = 1.21991765499115 + 2.0 * 6.503737449645996
Epoch 280, val loss: 1.3315820693969727
Epoch 290, training loss: 14.173234939575195 = 1.1813879013061523 + 2.0 * 6.4959235191345215
Epoch 290, val loss: 1.3023719787597656
Epoch 300, training loss: 14.121073722839355 = 1.1444908380508423 + 2.0 * 6.488291263580322
Epoch 300, val loss: 1.274980902671814
Epoch 310, training loss: 14.074913024902344 = 1.1090846061706543 + 2.0 * 6.482914447784424
Epoch 310, val loss: 1.2493168115615845
Epoch 320, training loss: 14.026522636413574 = 1.075424313545227 + 2.0 * 6.475549221038818
Epoch 320, val loss: 1.2253886461257935
Epoch 330, training loss: 13.978833198547363 = 1.0433948040008545 + 2.0 * 6.467719078063965
Epoch 330, val loss: 1.2031644582748413
Epoch 340, training loss: 13.945204734802246 = 1.012722373008728 + 2.0 * 6.466241359710693
Epoch 340, val loss: 1.1822052001953125
Epoch 350, training loss: 13.895191192626953 = 0.9831739068031311 + 2.0 * 6.456008434295654
Epoch 350, val loss: 1.1624581813812256
Epoch 360, training loss: 13.853998184204102 = 0.9546073079109192 + 2.0 * 6.449695587158203
Epoch 360, val loss: 1.1436398029327393
Epoch 370, training loss: 13.82602596282959 = 0.9267407059669495 + 2.0 * 6.449642658233643
Epoch 370, val loss: 1.125580072402954
Epoch 380, training loss: 13.778139114379883 = 0.8994125127792358 + 2.0 * 6.439363479614258
Epoch 380, val loss: 1.107948899269104
Epoch 390, training loss: 13.741522789001465 = 0.8725724816322327 + 2.0 * 6.434474945068359
Epoch 390, val loss: 1.090898036956787
Epoch 400, training loss: 13.706622123718262 = 0.8458467721939087 + 2.0 * 6.430387496948242
Epoch 400, val loss: 1.0740877389907837
Epoch 410, training loss: 13.667951583862305 = 0.8191182017326355 + 2.0 * 6.424416542053223
Epoch 410, val loss: 1.057502269744873
Epoch 420, training loss: 13.637939453125 = 0.7924132943153381 + 2.0 * 6.422762870788574
Epoch 420, val loss: 1.0410118103027344
Epoch 430, training loss: 13.599858283996582 = 0.765464723110199 + 2.0 * 6.417196750640869
Epoch 430, val loss: 1.0246293544769287
Epoch 440, training loss: 13.561776161193848 = 0.7383764386177063 + 2.0 * 6.4116997718811035
Epoch 440, val loss: 1.0082778930664062
Epoch 450, training loss: 13.536108016967773 = 0.7109835743904114 + 2.0 * 6.412562370300293
Epoch 450, val loss: 0.9920241832733154
Epoch 460, training loss: 13.497306823730469 = 0.683423638343811 + 2.0 * 6.4069414138793945
Epoch 460, val loss: 0.9755973815917969
Epoch 470, training loss: 13.460844039916992 = 0.655741274356842 + 2.0 * 6.402551174163818
Epoch 470, val loss: 0.9594864249229431
Epoch 480, training loss: 13.425701141357422 = 0.6280031204223633 + 2.0 * 6.398849010467529
Epoch 480, val loss: 0.9434493184089661
Epoch 490, training loss: 13.405179977416992 = 0.6003068089485168 + 2.0 * 6.40243673324585
Epoch 490, val loss: 0.9276701211929321
Epoch 500, training loss: 13.360488891601562 = 0.5728106498718262 + 2.0 * 6.393838882446289
Epoch 500, val loss: 0.9123266339302063
Epoch 510, training loss: 13.326404571533203 = 0.545711874961853 + 2.0 * 6.390346527099609
Epoch 510, val loss: 0.8974711298942566
Epoch 520, training loss: 13.303369522094727 = 0.5190707445144653 + 2.0 * 6.392149448394775
Epoch 520, val loss: 0.8831979632377625
Epoch 530, training loss: 13.273250579833984 = 0.4931313395500183 + 2.0 * 6.390059471130371
Epoch 530, val loss: 0.8698422908782959
Epoch 540, training loss: 13.236567497253418 = 0.4680767059326172 + 2.0 * 6.3842453956604
Epoch 540, val loss: 0.8572045564651489
Epoch 550, training loss: 13.205429077148438 = 0.4437888562679291 + 2.0 * 6.380820274353027
Epoch 550, val loss: 0.8455926179885864
Epoch 560, training loss: 13.178848266601562 = 0.420381098985672 + 2.0 * 6.379233360290527
Epoch 560, val loss: 0.8348113298416138
Epoch 570, training loss: 13.155407905578613 = 0.3978897035121918 + 2.0 * 6.378758907318115
Epoch 570, val loss: 0.8249062299728394
Epoch 580, training loss: 13.129024505615234 = 0.3764550983905792 + 2.0 * 6.376284599304199
Epoch 580, val loss: 0.8160009980201721
Epoch 590, training loss: 13.108475685119629 = 0.35612231492996216 + 2.0 * 6.376176834106445
Epoch 590, val loss: 0.8080461621284485
Epoch 600, training loss: 13.080175399780273 = 0.3367893397808075 + 2.0 * 6.371693134307861
Epoch 600, val loss: 0.8009634613990784
Epoch 610, training loss: 13.061415672302246 = 0.31861191987991333 + 2.0 * 6.371401786804199
Epoch 610, val loss: 0.7948048710823059
Epoch 620, training loss: 13.037481307983398 = 0.3014390170574188 + 2.0 * 6.368021011352539
Epoch 620, val loss: 0.7894963026046753
Epoch 630, training loss: 13.014355659484863 = 0.28530678153038025 + 2.0 * 6.3645243644714355
Epoch 630, val loss: 0.7848925590515137
Epoch 640, training loss: 12.996785163879395 = 0.27011290192604065 + 2.0 * 6.363336086273193
Epoch 640, val loss: 0.7809559106826782
Epoch 650, training loss: 12.987878799438477 = 0.25581440329551697 + 2.0 * 6.366032123565674
Epoch 650, val loss: 0.777720034122467
Epoch 660, training loss: 12.965786933898926 = 0.2424527108669281 + 2.0 * 6.361667156219482
Epoch 660, val loss: 0.7750378251075745
Epoch 670, training loss: 12.946267127990723 = 0.2299087941646576 + 2.0 * 6.358179092407227
Epoch 670, val loss: 0.7730018496513367
Epoch 680, training loss: 12.93131160736084 = 0.21813270449638367 + 2.0 * 6.356589317321777
Epoch 680, val loss: 0.7714675068855286
Epoch 690, training loss: 12.914798736572266 = 0.20708289742469788 + 2.0 * 6.35385799407959
Epoch 690, val loss: 0.770423412322998
Epoch 700, training loss: 12.903471946716309 = 0.19670364260673523 + 2.0 * 6.353384017944336
Epoch 700, val loss: 0.7697682976722717
Epoch 710, training loss: 12.891827583312988 = 0.1869128793478012 + 2.0 * 6.352457523345947
Epoch 710, val loss: 0.7693452835083008
Epoch 720, training loss: 12.8753023147583 = 0.17773742973804474 + 2.0 * 6.348782539367676
Epoch 720, val loss: 0.7695002555847168
Epoch 730, training loss: 12.864316940307617 = 0.16910412907600403 + 2.0 * 6.347606182098389
Epoch 730, val loss: 0.7699617147445679
Epoch 740, training loss: 12.851961135864258 = 0.16094815731048584 + 2.0 * 6.34550666809082
Epoch 740, val loss: 0.7707144021987915
Epoch 750, training loss: 12.870614051818848 = 0.15322913229465485 + 2.0 * 6.358692646026611
Epoch 750, val loss: 0.7717083692550659
Epoch 760, training loss: 12.835851669311523 = 0.1460077464580536 + 2.0 * 6.344922065734863
Epoch 760, val loss: 0.7728669047355652
Epoch 770, training loss: 12.823525428771973 = 0.13923045992851257 + 2.0 * 6.342147350311279
Epoch 770, val loss: 0.7744604349136353
Epoch 780, training loss: 12.81295108795166 = 0.13284315168857574 + 2.0 * 6.340054035186768
Epoch 780, val loss: 0.7762944102287292
Epoch 790, training loss: 12.803584098815918 = 0.12679173052310944 + 2.0 * 6.338396072387695
Epoch 790, val loss: 0.7784174084663391
Epoch 800, training loss: 12.810453414916992 = 0.12105241417884827 + 2.0 * 6.344700336456299
Epoch 800, val loss: 0.7807449102401733
Epoch 810, training loss: 12.78933048248291 = 0.11563479155302048 + 2.0 * 6.33684778213501
Epoch 810, val loss: 0.7831764817237854
Epoch 820, training loss: 12.778928756713867 = 0.11051467061042786 + 2.0 * 6.334207057952881
Epoch 820, val loss: 0.7858752012252808
Epoch 830, training loss: 12.7788667678833 = 0.10567378252744675 + 2.0 * 6.336596488952637
Epoch 830, val loss: 0.7887453436851501
Epoch 840, training loss: 12.77053165435791 = 0.10107401013374329 + 2.0 * 6.334728717803955
Epoch 840, val loss: 0.7918550372123718
Epoch 850, training loss: 12.759021759033203 = 0.09675095975399017 + 2.0 * 6.331135272979736
Epoch 850, val loss: 0.7950952053070068
Epoch 860, training loss: 12.752283096313477 = 0.09265757352113724 + 2.0 * 6.329812526702881
Epoch 860, val loss: 0.798545777797699
Epoch 870, training loss: 12.76374340057373 = 0.08878742903470993 + 2.0 * 6.337478160858154
Epoch 870, val loss: 0.8020735383033752
Epoch 880, training loss: 12.74083423614502 = 0.0851220116019249 + 2.0 * 6.327856063842773
Epoch 880, val loss: 0.8057125806808472
Epoch 890, training loss: 12.733768463134766 = 0.08166947215795517 + 2.0 * 6.326049327850342
Epoch 890, val loss: 0.809341549873352
Epoch 900, training loss: 12.72800064086914 = 0.07839682698249817 + 2.0 * 6.324801921844482
Epoch 900, val loss: 0.8132109045982361
Epoch 910, training loss: 12.73904037475586 = 0.07528705894947052 + 2.0 * 6.331876754760742
Epoch 910, val loss: 0.8170762062072754
Epoch 920, training loss: 12.724958419799805 = 0.07234717905521393 + 2.0 * 6.326305389404297
Epoch 920, val loss: 0.8209249973297119
Epoch 930, training loss: 12.71468448638916 = 0.06956493854522705 + 2.0 * 6.322559833526611
Epoch 930, val loss: 0.8248677849769592
Epoch 940, training loss: 12.708544731140137 = 0.06693343818187714 + 2.0 * 6.320805549621582
Epoch 940, val loss: 0.8287442326545715
Epoch 950, training loss: 12.711282730102539 = 0.06443477421998978 + 2.0 * 6.323423862457275
Epoch 950, val loss: 0.8326937556266785
Epoch 960, training loss: 12.703775405883789 = 0.062051571905612946 + 2.0 * 6.32086181640625
Epoch 960, val loss: 0.8366134166717529
Epoch 970, training loss: 12.697916030883789 = 0.05979827791452408 + 2.0 * 6.319058895111084
Epoch 970, val loss: 0.8405383825302124
Epoch 980, training loss: 12.697455406188965 = 0.05765527859330177 + 2.0 * 6.319900035858154
Epoch 980, val loss: 0.8443761467933655
Epoch 990, training loss: 12.68932819366455 = 0.0556250624358654 + 2.0 * 6.316851615905762
Epoch 990, val loss: 0.8482818007469177
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7925925925925926
0.8139167105956774
=== training gcn model ===
Epoch 0, training loss: 19.140762329101562 = 1.9470609426498413 + 2.0 * 8.596850395202637
Epoch 0, val loss: 1.9469027519226074
Epoch 10, training loss: 19.130470275878906 = 1.9373114109039307 + 2.0 * 8.596579551696777
Epoch 10, val loss: 1.93683922290802
Epoch 20, training loss: 19.114337921142578 = 1.9252735376358032 + 2.0 * 8.594532012939453
Epoch 20, val loss: 1.9243593215942383
Epoch 30, training loss: 19.066232681274414 = 1.9087674617767334 + 2.0 * 8.57873249053955
Epoch 30, val loss: 1.9074383974075317
Epoch 40, training loss: 18.8350830078125 = 1.8884540796279907 + 2.0 * 8.47331428527832
Epoch 40, val loss: 1.8877441883087158
Epoch 50, training loss: 18.20476531982422 = 1.8666082620620728 + 2.0 * 8.169078826904297
Epoch 50, val loss: 1.8670598268508911
Epoch 60, training loss: 17.787851333618164 = 1.8468775749206543 + 2.0 * 7.970486640930176
Epoch 60, val loss: 1.8488802909851074
Epoch 70, training loss: 17.245025634765625 = 1.830104112625122 + 2.0 * 7.707460403442383
Epoch 70, val loss: 1.8334455490112305
Epoch 80, training loss: 16.4003963470459 = 1.8188862800598145 + 2.0 * 7.290755271911621
Epoch 80, val loss: 1.8238569498062134
Epoch 90, training loss: 15.906719207763672 = 1.8098680973052979 + 2.0 * 7.048425674438477
Epoch 90, val loss: 1.8150336742401123
Epoch 100, training loss: 15.656881332397461 = 1.7973612546920776 + 2.0 * 6.929759979248047
Epoch 100, val loss: 1.8030564785003662
Epoch 110, training loss: 15.469232559204102 = 1.7840684652328491 + 2.0 * 6.8425822257995605
Epoch 110, val loss: 1.7909612655639648
Epoch 120, training loss: 15.330484390258789 = 1.7714362144470215 + 2.0 * 6.779523849487305
Epoch 120, val loss: 1.7797001600265503
Epoch 130, training loss: 15.208063125610352 = 1.7586885690689087 + 2.0 * 6.724687099456787
Epoch 130, val loss: 1.7683637142181396
Epoch 140, training loss: 15.119833946228027 = 1.7447296380996704 + 2.0 * 6.687551975250244
Epoch 140, val loss: 1.7561570405960083
Epoch 150, training loss: 15.039826393127441 = 1.7290688753128052 + 2.0 * 6.655378818511963
Epoch 150, val loss: 1.7429028749465942
Epoch 160, training loss: 14.969779968261719 = 1.7117077112197876 + 2.0 * 6.629035949707031
Epoch 160, val loss: 1.7284640073776245
Epoch 170, training loss: 14.917184829711914 = 1.6921664476394653 + 2.0 * 6.612509250640869
Epoch 170, val loss: 1.7124884128570557
Epoch 180, training loss: 14.844879150390625 = 1.6703944206237793 + 2.0 * 6.587242603302002
Epoch 180, val loss: 1.6947226524353027
Epoch 190, training loss: 14.784414291381836 = 1.646083116531372 + 2.0 * 6.5691657066345215
Epoch 190, val loss: 1.6749919652938843
Epoch 200, training loss: 14.723931312561035 = 1.6187903881072998 + 2.0 * 6.552570343017578
Epoch 200, val loss: 1.652980923652649
Epoch 210, training loss: 14.67236328125 = 1.5886235237121582 + 2.0 * 6.541869640350342
Epoch 210, val loss: 1.6288460493087769
Epoch 220, training loss: 14.61051082611084 = 1.5564757585525513 + 2.0 * 6.527017593383789
Epoch 220, val loss: 1.6031147241592407
Epoch 230, training loss: 14.55196762084961 = 1.522172451019287 + 2.0 * 6.51489782333374
Epoch 230, val loss: 1.575838327407837
Epoch 240, training loss: 14.493979454040527 = 1.4860305786132812 + 2.0 * 6.503974437713623
Epoch 240, val loss: 1.5471514463424683
Epoch 250, training loss: 14.438709259033203 = 1.4481611251831055 + 2.0 * 6.495274066925049
Epoch 250, val loss: 1.517365574836731
Epoch 260, training loss: 14.387201309204102 = 1.4092401266098022 + 2.0 * 6.488980770111084
Epoch 260, val loss: 1.4869858026504517
Epoch 270, training loss: 14.327812194824219 = 1.369893193244934 + 2.0 * 6.478959560394287
Epoch 270, val loss: 1.4567066431045532
Epoch 280, training loss: 14.284863471984863 = 1.3305333852767944 + 2.0 * 6.477165222167969
Epoch 280, val loss: 1.426891803741455
Epoch 290, training loss: 14.220893859863281 = 1.2918328046798706 + 2.0 * 6.4645304679870605
Epoch 290, val loss: 1.3981608152389526
Epoch 300, training loss: 14.170711517333984 = 1.2539429664611816 + 2.0 * 6.458384037017822
Epoch 300, val loss: 1.3704546689987183
Epoch 310, training loss: 14.120661735534668 = 1.2167633771896362 + 2.0 * 6.451949119567871
Epoch 310, val loss: 1.343815803527832
Epoch 320, training loss: 14.076000213623047 = 1.1805881261825562 + 2.0 * 6.44770622253418
Epoch 320, val loss: 1.3184303045272827
Epoch 330, training loss: 14.029515266418457 = 1.1455987691879272 + 2.0 * 6.441958427429199
Epoch 330, val loss: 1.294244408607483
Epoch 340, training loss: 13.985427856445312 = 1.1115447282791138 + 2.0 * 6.436941623687744
Epoch 340, val loss: 1.2711279392242432
Epoch 350, training loss: 13.954168319702148 = 1.0782923698425293 + 2.0 * 6.437938213348389
Epoch 350, val loss: 1.2488900423049927
Epoch 360, training loss: 13.90489673614502 = 1.0458110570907593 + 2.0 * 6.4295430183410645
Epoch 360, val loss: 1.2275782823562622
Epoch 370, training loss: 13.86117172241211 = 1.0140405893325806 + 2.0 * 6.42356538772583
Epoch 370, val loss: 1.2069473266601562
Epoch 380, training loss: 13.826190948486328 = 0.9826933145523071 + 2.0 * 6.421748638153076
Epoch 380, val loss: 1.1868818998336792
Epoch 390, training loss: 13.783317565917969 = 0.9516443610191345 + 2.0 * 6.415836811065674
Epoch 390, val loss: 1.1671833992004395
Epoch 400, training loss: 13.759108543395996 = 0.9208400249481201 + 2.0 * 6.419134140014648
Epoch 400, val loss: 1.1477810144424438
Epoch 410, training loss: 13.708931922912598 = 0.8904742002487183 + 2.0 * 6.409228801727295
Epoch 410, val loss: 1.128668189048767
Epoch 420, training loss: 13.671611785888672 = 0.8603987097740173 + 2.0 * 6.405606746673584
Epoch 420, val loss: 1.1099908351898193
Epoch 430, training loss: 13.634328842163086 = 0.8304539322853088 + 2.0 * 6.401937484741211
Epoch 430, val loss: 1.091552495956421
Epoch 440, training loss: 13.5977144241333 = 0.8006532788276672 + 2.0 * 6.39853048324585
Epoch 440, val loss: 1.0733397006988525
Epoch 450, training loss: 13.56655502319336 = 0.7711052894592285 + 2.0 * 6.397724628448486
Epoch 450, val loss: 1.0555436611175537
Epoch 460, training loss: 13.532798767089844 = 0.7421314716339111 + 2.0 * 6.395333766937256
Epoch 460, val loss: 1.0384188890457153
Epoch 470, training loss: 13.499238014221191 = 0.7138051986694336 + 2.0 * 6.392716407775879
Epoch 470, val loss: 1.0221221446990967
Epoch 480, training loss: 13.464653968811035 = 0.6862313747406006 + 2.0 * 6.389211177825928
Epoch 480, val loss: 1.006754755973816
Epoch 490, training loss: 13.429433822631836 = 0.6592488288879395 + 2.0 * 6.385092735290527
Epoch 490, val loss: 0.9923281073570251
Epoch 500, training loss: 13.397865295410156 = 0.6330035328865051 + 2.0 * 6.3824310302734375
Epoch 500, val loss: 0.9788897633552551
Epoch 510, training loss: 13.370863914489746 = 0.6073998212814331 + 2.0 * 6.381731986999512
Epoch 510, val loss: 0.9664900302886963
Epoch 520, training loss: 13.337088584899902 = 0.5826817750930786 + 2.0 * 6.377203464508057
Epoch 520, val loss: 0.9550951719284058
Epoch 530, training loss: 13.309334754943848 = 0.5587063431739807 + 2.0 * 6.375314235687256
Epoch 530, val loss: 0.9448320269584656
Epoch 540, training loss: 13.294532775878906 = 0.5354387760162354 + 2.0 * 6.379547119140625
Epoch 540, val loss: 0.935471773147583
Epoch 550, training loss: 13.255072593688965 = 0.5128620862960815 + 2.0 * 6.371105194091797
Epoch 550, val loss: 0.9270322322845459
Epoch 560, training loss: 13.227272033691406 = 0.4910893142223358 + 2.0 * 6.368091583251953
Epoch 560, val loss: 0.9194304347038269
Epoch 570, training loss: 13.2018404006958 = 0.4699558913707733 + 2.0 * 6.365942478179932
Epoch 570, val loss: 0.9125836491584778
Epoch 580, training loss: 13.188972473144531 = 0.44947704672813416 + 2.0 * 6.369747638702393
Epoch 580, val loss: 0.9064352512359619
Epoch 590, training loss: 13.155729293823242 = 0.42960986495018005 + 2.0 * 6.3630595207214355
Epoch 590, val loss: 0.901019811630249
Epoch 600, training loss: 13.13132381439209 = 0.4105164110660553 + 2.0 * 6.360403537750244
Epoch 600, val loss: 0.8964079022407532
Epoch 610, training loss: 13.110794067382812 = 0.3920471668243408 + 2.0 * 6.359373569488525
Epoch 610, val loss: 0.892470121383667
Epoch 620, training loss: 13.09040641784668 = 0.3742033839225769 + 2.0 * 6.3581013679504395
Epoch 620, val loss: 0.8890709280967712
Epoch 630, training loss: 13.065375328063965 = 0.35688045620918274 + 2.0 * 6.354247570037842
Epoch 630, val loss: 0.886326789855957
Epoch 640, training loss: 13.04793930053711 = 0.34011954069137573 + 2.0 * 6.353909969329834
Epoch 640, val loss: 0.8841356039047241
Epoch 650, training loss: 13.030009269714355 = 0.32390859723091125 + 2.0 * 6.353050231933594
Epoch 650, val loss: 0.8823843598365784
Epoch 660, training loss: 13.009982109069824 = 0.30822983384132385 + 2.0 * 6.350876331329346
Epoch 660, val loss: 0.8812298774719238
Epoch 670, training loss: 12.988324165344238 = 0.2931298613548279 + 2.0 * 6.347597122192383
Epoch 670, val loss: 0.8806560635566711
Epoch 680, training loss: 12.977189064025879 = 0.2786010205745697 + 2.0 * 6.349294185638428
Epoch 680, val loss: 0.880553126335144
Epoch 690, training loss: 12.952640533447266 = 0.2645805776119232 + 2.0 * 6.344029903411865
Epoch 690, val loss: 0.880993664264679
Epoch 700, training loss: 12.935670852661133 = 0.25115928053855896 + 2.0 * 6.342255592346191
Epoch 700, val loss: 0.8819038271903992
Epoch 710, training loss: 12.9376859664917 = 0.23834291100502014 + 2.0 * 6.349671363830566
Epoch 710, val loss: 0.8832184672355652
Epoch 720, training loss: 12.911964416503906 = 0.22602824866771698 + 2.0 * 6.342967987060547
Epoch 720, val loss: 0.8850788474082947
Epoch 730, training loss: 12.888701438903809 = 0.21443505585193634 + 2.0 * 6.337133407592773
Epoch 730, val loss: 0.8873276710510254
Epoch 740, training loss: 12.875907897949219 = 0.20340454578399658 + 2.0 * 6.336251735687256
Epoch 740, val loss: 0.8899794816970825
Epoch 750, training loss: 12.877559661865234 = 0.19292408227920532 + 2.0 * 6.342317581176758
Epoch 750, val loss: 0.8929288387298584
Epoch 760, training loss: 12.852888107299805 = 0.18300117552280426 + 2.0 * 6.3349432945251465
Epoch 760, val loss: 0.8961355686187744
Epoch 770, training loss: 12.854126930236816 = 0.17367058992385864 + 2.0 * 6.340228080749512
Epoch 770, val loss: 0.8997580409049988
Epoch 780, training loss: 12.830018997192383 = 0.16481618583202362 + 2.0 * 6.332601547241211
Epoch 780, val loss: 0.9035120010375977
Epoch 790, training loss: 12.81619930267334 = 0.156533882021904 + 2.0 * 6.329832553863525
Epoch 790, val loss: 0.9075603485107422
Epoch 800, training loss: 12.805983543395996 = 0.1487065702676773 + 2.0 * 6.328638553619385
Epoch 800, val loss: 0.911907970905304
Epoch 810, training loss: 12.804105758666992 = 0.1413295716047287 + 2.0 * 6.331387996673584
Epoch 810, val loss: 0.9163113832473755
Epoch 820, training loss: 12.798123359680176 = 0.13433219492435455 + 2.0 * 6.331895351409912
Epoch 820, val loss: 0.921082615852356
Epoch 830, training loss: 12.775787353515625 = 0.12779653072357178 + 2.0 * 6.323995590209961
Epoch 830, val loss: 0.9261278510093689
Epoch 840, training loss: 12.76722240447998 = 0.12162911891937256 + 2.0 * 6.322796821594238
Epoch 840, val loss: 0.931236207485199
Epoch 850, training loss: 12.759431838989258 = 0.11580969393253326 + 2.0 * 6.321811199188232
Epoch 850, val loss: 0.9365230202674866
Epoch 860, training loss: 12.764692306518555 = 0.11033214628696442 + 2.0 * 6.327179908752441
Epoch 860, val loss: 0.9418383836746216
Epoch 870, training loss: 12.744458198547363 = 0.105129174888134 + 2.0 * 6.319664478302002
Epoch 870, val loss: 0.9474790096282959
Epoch 880, training loss: 12.738465309143066 = 0.10025792568922043 + 2.0 * 6.319103717803955
Epoch 880, val loss: 0.9531915783882141
Epoch 890, training loss: 12.745728492736816 = 0.0956694632768631 + 2.0 * 6.325029373168945
Epoch 890, val loss: 0.9588519334793091
Epoch 900, training loss: 12.725407600402832 = 0.09134472161531448 + 2.0 * 6.317031383514404
Epoch 900, val loss: 0.9646122455596924
Epoch 910, training loss: 12.716951370239258 = 0.08726213872432709 + 2.0 * 6.314844608306885
Epoch 910, val loss: 0.970586359500885
Epoch 920, training loss: 12.710311889648438 = 0.08341087400913239 + 2.0 * 6.313450336456299
Epoch 920, val loss: 0.9765788912773132
Epoch 930, training loss: 12.726749420166016 = 0.07977686822414398 + 2.0 * 6.323486328125
Epoch 930, val loss: 0.9824865460395813
Epoch 940, training loss: 12.705389976501465 = 0.07632897794246674 + 2.0 * 6.314530372619629
Epoch 940, val loss: 0.9886386394500732
Epoch 950, training loss: 12.693283081054688 = 0.07307839393615723 + 2.0 * 6.310102462768555
Epoch 950, val loss: 0.9948314428329468
Epoch 960, training loss: 12.688720703125 = 0.0700048878788948 + 2.0 * 6.3093581199646
Epoch 960, val loss: 1.0010497570037842
Epoch 970, training loss: 12.70583724975586 = 0.06709384173154831 + 2.0 * 6.319371700286865
Epoch 970, val loss: 1.0072768926620483
Epoch 980, training loss: 12.686375617980957 = 0.06435910612344742 + 2.0 * 6.311008453369141
Epoch 980, val loss: 1.013393521308899
Epoch 990, training loss: 12.67518424987793 = 0.06175073981285095 + 2.0 * 6.3067169189453125
Epoch 990, val loss: 1.01973295211792
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.762962962962963
0.808645229309436
=== training gcn model ===
Epoch 0, training loss: 19.134965896606445 = 1.9412811994552612 + 2.0 * 8.596842765808105
Epoch 0, val loss: 1.9317896366119385
Epoch 10, training loss: 19.124507904052734 = 1.9313939809799194 + 2.0 * 8.596556663513184
Epoch 10, val loss: 1.922760248184204
Epoch 20, training loss: 19.108436584472656 = 1.9192347526550293 + 2.0 * 8.594600677490234
Epoch 20, val loss: 1.9113086462020874
Epoch 30, training loss: 19.063236236572266 = 1.902783751487732 + 2.0 * 8.580225944519043
Epoch 30, val loss: 1.8955633640289307
Epoch 40, training loss: 18.843202590942383 = 1.8824503421783447 + 2.0 * 8.480376243591309
Epoch 40, val loss: 1.8767261505126953
Epoch 50, training loss: 17.743371963500977 = 1.8617969751358032 + 2.0 * 7.9407877922058105
Epoch 50, val loss: 1.8579007387161255
Epoch 60, training loss: 16.964094161987305 = 1.8448129892349243 + 2.0 * 7.559640884399414
Epoch 60, val loss: 1.8429304361343384
Epoch 70, training loss: 16.42118263244629 = 1.8306496143341064 + 2.0 * 7.295266628265381
Epoch 70, val loss: 1.8298619985580444
Epoch 80, training loss: 16.050086975097656 = 1.815394401550293 + 2.0 * 7.117345809936523
Epoch 80, val loss: 1.816233515739441
Epoch 90, training loss: 15.816237449645996 = 1.8007408380508423 + 2.0 * 7.007748126983643
Epoch 90, val loss: 1.8035337924957275
Epoch 100, training loss: 15.592642784118652 = 1.786407232284546 + 2.0 * 6.903117656707764
Epoch 100, val loss: 1.7913076877593994
Epoch 110, training loss: 15.403999328613281 = 1.7730234861373901 + 2.0 * 6.815487861633301
Epoch 110, val loss: 1.7797050476074219
Epoch 120, training loss: 15.261524200439453 = 1.759649395942688 + 2.0 * 6.750937461853027
Epoch 120, val loss: 1.7677229642868042
Epoch 130, training loss: 15.1527738571167 = 1.7453019618988037 + 2.0 * 6.703735828399658
Epoch 130, val loss: 1.754822850227356
Epoch 140, training loss: 15.064763069152832 = 1.7292929887771606 + 2.0 * 6.6677350997924805
Epoch 140, val loss: 1.7407935857772827
Epoch 150, training loss: 14.987548828125 = 1.7113159894943237 + 2.0 * 6.638116359710693
Epoch 150, val loss: 1.7253574132919312
Epoch 160, training loss: 14.9164457321167 = 1.6910200119018555 + 2.0 * 6.612712860107422
Epoch 160, val loss: 1.7082452774047852
Epoch 170, training loss: 14.859514236450195 = 1.6680309772491455 + 2.0 * 6.5957417488098145
Epoch 170, val loss: 1.688839077949524
Epoch 180, training loss: 14.7911376953125 = 1.6426483392715454 + 2.0 * 6.574244499206543
Epoch 180, val loss: 1.6678136587142944
Epoch 190, training loss: 14.730428695678711 = 1.6148892641067505 + 2.0 * 6.557769775390625
Epoch 190, val loss: 1.6449953317642212
Epoch 200, training loss: 14.669925689697266 = 1.584705114364624 + 2.0 * 6.542610168457031
Epoch 200, val loss: 1.6202863454818726
Epoch 210, training loss: 14.621203422546387 = 1.5522844791412354 + 2.0 * 6.534459590911865
Epoch 210, val loss: 1.5941064357757568
Epoch 220, training loss: 14.561561584472656 = 1.5184605121612549 + 2.0 * 6.52155065536499
Epoch 220, val loss: 1.5672839879989624
Epoch 230, training loss: 14.500081062316895 = 1.4836653470993042 + 2.0 * 6.50820779800415
Epoch 230, val loss: 1.5401601791381836
Epoch 240, training loss: 14.442719459533691 = 1.4482094049453735 + 2.0 * 6.497254848480225
Epoch 240, val loss: 1.5131689310073853
Epoch 250, training loss: 14.388373374938965 = 1.4125279188156128 + 2.0 * 6.487922668457031
Epoch 250, val loss: 1.48677659034729
Epoch 260, training loss: 14.359787940979004 = 1.3770722150802612 + 2.0 * 6.491357803344727
Epoch 260, val loss: 1.4613137245178223
Epoch 270, training loss: 14.292142868041992 = 1.34260892868042 + 2.0 * 6.474767208099365
Epoch 270, val loss: 1.4374254941940308
Epoch 280, training loss: 14.24039363861084 = 1.3090068101882935 + 2.0 * 6.465693473815918
Epoch 280, val loss: 1.4148145914077759
Epoch 290, training loss: 14.1907958984375 = 1.276073932647705 + 2.0 * 6.457361221313477
Epoch 290, val loss: 1.3931236267089844
Epoch 300, training loss: 14.145326614379883 = 1.2435356378555298 + 2.0 * 6.450895309448242
Epoch 300, val loss: 1.3720999956130981
Epoch 310, training loss: 14.106882095336914 = 1.2112584114074707 + 2.0 * 6.447812080383301
Epoch 310, val loss: 1.3515312671661377
Epoch 320, training loss: 14.061767578125 = 1.1793073415756226 + 2.0 * 6.441230297088623
Epoch 320, val loss: 1.3310891389846802
Epoch 330, training loss: 14.016273498535156 = 1.1474393606185913 + 2.0 * 6.434417247772217
Epoch 330, val loss: 1.3105353116989136
Epoch 340, training loss: 13.976219177246094 = 1.1155296564102173 + 2.0 * 6.430344581604004
Epoch 340, val loss: 1.2897703647613525
Epoch 350, training loss: 13.940006256103516 = 1.083747386932373 + 2.0 * 6.42812967300415
Epoch 350, val loss: 1.2689599990844727
Epoch 360, training loss: 13.894791603088379 = 1.0522940158843994 + 2.0 * 6.421248912811279
Epoch 360, val loss: 1.2480021715164185
Epoch 370, training loss: 13.853590965270996 = 1.0210334062576294 + 2.0 * 6.416278839111328
Epoch 370, val loss: 1.2268952131271362
Epoch 380, training loss: 13.814353942871094 = 0.9899859428405762 + 2.0 * 6.412184238433838
Epoch 380, val loss: 1.2056828737258911
Epoch 390, training loss: 13.786149978637695 = 0.9592081904411316 + 2.0 * 6.41347074508667
Epoch 390, val loss: 1.1844322681427002
Epoch 400, training loss: 13.747702598571777 = 0.9289427995681763 + 2.0 * 6.409379959106445
Epoch 400, val loss: 1.1633838415145874
Epoch 410, training loss: 13.704538345336914 = 0.8992552757263184 + 2.0 * 6.402641296386719
Epoch 410, val loss: 1.1426584720611572
Epoch 420, training loss: 13.666523933410645 = 0.8700761795043945 + 2.0 * 6.398223876953125
Epoch 420, val loss: 1.1222138404846191
Epoch 430, training loss: 13.631739616394043 = 0.8412864804267883 + 2.0 * 6.39522647857666
Epoch 430, val loss: 1.1019806861877441
Epoch 440, training loss: 13.602653503417969 = 0.8129567503929138 + 2.0 * 6.394848346710205
Epoch 440, val loss: 1.0819272994995117
Epoch 450, training loss: 13.5635347366333 = 0.7852020859718323 + 2.0 * 6.389166355133057
Epoch 450, val loss: 1.062566876411438
Epoch 460, training loss: 13.529755592346191 = 0.7579743266105652 + 2.0 * 6.385890483856201
Epoch 460, val loss: 1.0436115264892578
Epoch 470, training loss: 13.498312950134277 = 0.7311388850212097 + 2.0 * 6.383586883544922
Epoch 470, val loss: 1.0251306295394897
Epoch 480, training loss: 13.465956687927246 = 0.7047529220581055 + 2.0 * 6.38060188293457
Epoch 480, val loss: 1.0069526433944702
Epoch 490, training loss: 13.438582420349121 = 0.6788761019706726 + 2.0 * 6.379853248596191
Epoch 490, val loss: 0.9897252917289734
Epoch 500, training loss: 13.405525207519531 = 0.6534862518310547 + 2.0 * 6.376019477844238
Epoch 500, val loss: 0.9730711579322815
Epoch 510, training loss: 13.377853393554688 = 0.6285027265548706 + 2.0 * 6.374675273895264
Epoch 510, val loss: 0.9571113586425781
Epoch 520, training loss: 13.34780502319336 = 0.6039222478866577 + 2.0 * 6.371941566467285
Epoch 520, val loss: 0.9417451024055481
Epoch 530, training loss: 13.31910228729248 = 0.5798410177230835 + 2.0 * 6.369630813598633
Epoch 530, val loss: 0.9275323748588562
Epoch 540, training loss: 13.295836448669434 = 0.5562487840652466 + 2.0 * 6.369793891906738
Epoch 540, val loss: 0.9142300486564636
Epoch 550, training loss: 13.263136863708496 = 0.5331732630729675 + 2.0 * 6.364981651306152
Epoch 550, val loss: 0.9017788171768188
Epoch 560, training loss: 13.235700607299805 = 0.5105847120285034 + 2.0 * 6.362557888031006
Epoch 560, val loss: 0.8905192613601685
Epoch 570, training loss: 13.21091079711914 = 0.4884882867336273 + 2.0 * 6.36121129989624
Epoch 570, val loss: 0.8802648186683655
Epoch 580, training loss: 13.197027206420898 = 0.4669322371482849 + 2.0 * 6.365047454833984
Epoch 580, val loss: 0.8709393739700317
Epoch 590, training loss: 13.16504955291748 = 0.44593939185142517 + 2.0 * 6.359555244445801
Epoch 590, val loss: 0.8626664876937866
Epoch 600, training loss: 13.136269569396973 = 0.4256078004837036 + 2.0 * 6.355330944061279
Epoch 600, val loss: 0.855469286441803
Epoch 610, training loss: 13.112919807434082 = 0.4058515727519989 + 2.0 * 6.35353422164917
Epoch 610, val loss: 0.8491507768630981
Epoch 620, training loss: 13.09167194366455 = 0.38664135336875916 + 2.0 * 6.35251522064209
Epoch 620, val loss: 0.8436664938926697
Epoch 630, training loss: 13.068968772888184 = 0.36797401309013367 + 2.0 * 6.350497245788574
Epoch 630, val loss: 0.8389878273010254
Epoch 640, training loss: 13.04597282409668 = 0.34990519285202026 + 2.0 * 6.348033905029297
Epoch 640, val loss: 0.8351737260818481
Epoch 650, training loss: 13.065336227416992 = 0.3324500620365143 + 2.0 * 6.366443157196045
Epoch 650, val loss: 0.8321719169616699
Epoch 660, training loss: 13.012774467468262 = 0.3157450258731842 + 2.0 * 6.348514556884766
Epoch 660, val loss: 0.8297484517097473
Epoch 670, training loss: 12.989757537841797 = 0.29977527260780334 + 2.0 * 6.344991207122803
Epoch 670, val loss: 0.8281703591346741
Epoch 680, training loss: 12.969104766845703 = 0.2844838500022888 + 2.0 * 6.342310428619385
Epoch 680, val loss: 0.827271044254303
Epoch 690, training loss: 12.950055122375488 = 0.26981857419013977 + 2.0 * 6.340118408203125
Epoch 690, val loss: 0.8270571827888489
Epoch 700, training loss: 12.945966720581055 = 0.2557923495769501 + 2.0 * 6.345087051391602
Epoch 700, val loss: 0.8274646401405334
Epoch 710, training loss: 12.928757667541504 = 0.24243101477622986 + 2.0 * 6.34316349029541
Epoch 710, val loss: 0.8282060027122498
Epoch 720, training loss: 12.904680252075195 = 0.2298024445772171 + 2.0 * 6.337439060211182
Epoch 720, val loss: 0.8297560811042786
Epoch 730, training loss: 12.887195587158203 = 0.2178535908460617 + 2.0 * 6.3346710205078125
Epoch 730, val loss: 0.8318175077438354
Epoch 740, training loss: 12.891584396362305 = 0.2065596729516983 + 2.0 * 6.342512130737305
Epoch 740, val loss: 0.8343335390090942
Epoch 750, training loss: 12.86083698272705 = 0.1958618015050888 + 2.0 * 6.3324875831604
Epoch 750, val loss: 0.8372963666915894
Epoch 760, training loss: 12.84827709197998 = 0.18580707907676697 + 2.0 * 6.331234931945801
Epoch 760, val loss: 0.8407266736030579
Epoch 770, training loss: 12.836531639099121 = 0.1763266921043396 + 2.0 * 6.330102443695068
Epoch 770, val loss: 0.8444830775260925
Epoch 780, training loss: 12.831803321838379 = 0.16736866533756256 + 2.0 * 6.332217216491699
Epoch 780, val loss: 0.8485629558563232
Epoch 790, training loss: 12.817599296569824 = 0.1589430421590805 + 2.0 * 6.3293280601501465
Epoch 790, val loss: 0.8530123829841614
Epoch 800, training loss: 12.806951522827148 = 0.15099436044692993 + 2.0 * 6.327978610992432
Epoch 800, val loss: 0.857717752456665
Epoch 810, training loss: 12.796045303344727 = 0.14351795613765717 + 2.0 * 6.326263904571533
Epoch 810, val loss: 0.8627259731292725
Epoch 820, training loss: 12.788064956665039 = 0.13648006319999695 + 2.0 * 6.32579231262207
Epoch 820, val loss: 0.8679143786430359
Epoch 830, training loss: 12.775328636169434 = 0.12984241545200348 + 2.0 * 6.322742938995361
Epoch 830, val loss: 0.8732970356941223
Epoch 840, training loss: 12.770452499389648 = 0.12359344214200974 + 2.0 * 6.323429584503174
Epoch 840, val loss: 0.8788447380065918
Epoch 850, training loss: 12.773433685302734 = 0.11770088225603104 + 2.0 * 6.327866554260254
Epoch 850, val loss: 0.8845162987709045
Epoch 860, training loss: 12.753829002380371 = 0.11215125769376755 + 2.0 * 6.320838928222656
Epoch 860, val loss: 0.890264630317688
Epoch 870, training loss: 12.742533683776855 = 0.10696624964475632 + 2.0 * 6.317783832550049
Epoch 870, val loss: 0.8963195085525513
Epoch 880, training loss: 12.734125137329102 = 0.1020699292421341 + 2.0 * 6.316027641296387
Epoch 880, val loss: 0.9024171829223633
Epoch 890, training loss: 12.742207527160645 = 0.09745346009731293 + 2.0 * 6.3223772048950195
Epoch 890, val loss: 0.9086326956748962
Epoch 900, training loss: 12.73044204711914 = 0.09308432787656784 + 2.0 * 6.318678855895996
Epoch 900, val loss: 0.9148410558700562
Epoch 910, training loss: 12.71630573272705 = 0.08897489309310913 + 2.0 * 6.313665390014648
Epoch 910, val loss: 0.9211747050285339
Epoch 920, training loss: 12.709368705749512 = 0.0851074829697609 + 2.0 * 6.312130451202393
Epoch 920, val loss: 0.9275298714637756
Epoch 930, training loss: 12.718351364135742 = 0.08145760744810104 + 2.0 * 6.318447113037109
Epoch 930, val loss: 0.9339406490325928
Epoch 940, training loss: 12.703125 = 0.07801277190446854 + 2.0 * 6.312556266784668
Epoch 940, val loss: 0.9403812289237976
Epoch 950, training loss: 12.692361831665039 = 0.07476010173559189 + 2.0 * 6.30880069732666
Epoch 950, val loss: 0.9468639492988586
Epoch 960, training loss: 12.690671920776367 = 0.07169126719236374 + 2.0 * 6.309490203857422
Epoch 960, val loss: 0.9533988833427429
Epoch 970, training loss: 12.683710098266602 = 0.0687820091843605 + 2.0 * 6.307464122772217
Epoch 970, val loss: 0.959807813167572
Epoch 980, training loss: 12.677342414855957 = 0.06604010611772537 + 2.0 * 6.3056511878967285
Epoch 980, val loss: 0.9663896560668945
Epoch 990, training loss: 12.673409461975098 = 0.06344872713088989 + 2.0 * 6.304980278015137
Epoch 990, val loss: 0.9729297757148743
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7962962962962963
0.8075909330521878
The final CL Acc:0.78395, 0.01492, The final GNN Acc:0.81005, 0.00277
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13204])
remove edge: torch.Size([2, 7826])
updated graph: torch.Size([2, 10474])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 19.13739585876465 = 1.9437185525894165 + 2.0 * 8.59683895111084
Epoch 0, val loss: 1.9429773092269897
Epoch 10, training loss: 19.12725257873535 = 1.9341578483581543 + 2.0 * 8.59654712677002
Epoch 10, val loss: 1.9332035779953003
Epoch 20, training loss: 19.1112117767334 = 1.9225685596466064 + 2.0 * 8.594321250915527
Epoch 20, val loss: 1.9210820198059082
Epoch 30, training loss: 19.061363220214844 = 1.9068341255187988 + 2.0 * 8.577264785766602
Epoch 30, val loss: 1.904789924621582
Epoch 40, training loss: 18.816665649414062 = 1.887146234512329 + 2.0 * 8.464759826660156
Epoch 40, val loss: 1.8854221105575562
Epoch 50, training loss: 18.029354095458984 = 1.866255760192871 + 2.0 * 8.081549644470215
Epoch 50, val loss: 1.865804672241211
Epoch 60, training loss: 17.038124084472656 = 1.8504199981689453 + 2.0 * 7.593851566314697
Epoch 60, val loss: 1.851309061050415
Epoch 70, training loss: 16.13698959350586 = 1.8394209146499634 + 2.0 * 7.148784160614014
Epoch 70, val loss: 1.840758204460144
Epoch 80, training loss: 15.721121788024902 = 1.8283462524414062 + 2.0 * 6.946387767791748
Epoch 80, val loss: 1.8297876119613647
Epoch 90, training loss: 15.477567672729492 = 1.8141083717346191 + 2.0 * 6.831729412078857
Epoch 90, val loss: 1.8163363933563232
Epoch 100, training loss: 15.330986976623535 = 1.7994493246078491 + 2.0 * 6.765769004821777
Epoch 100, val loss: 1.8024694919586182
Epoch 110, training loss: 15.219864845275879 = 1.785310983657837 + 2.0 * 6.7172770500183105
Epoch 110, val loss: 1.7887738943099976
Epoch 120, training loss: 15.122136116027832 = 1.7712286710739136 + 2.0 * 6.6754536628723145
Epoch 120, val loss: 1.7749388217926025
Epoch 130, training loss: 15.040929794311523 = 1.7564316987991333 + 2.0 * 6.64224910736084
Epoch 130, val loss: 1.7605957984924316
Epoch 140, training loss: 14.969095230102539 = 1.7405668497085571 + 2.0 * 6.614264011383057
Epoch 140, val loss: 1.7456258535385132
Epoch 150, training loss: 14.902555465698242 = 1.7233593463897705 + 2.0 * 6.589598178863525
Epoch 150, val loss: 1.729668140411377
Epoch 160, training loss: 14.842109680175781 = 1.7043771743774414 + 2.0 * 6.56886625289917
Epoch 160, val loss: 1.7124313116073608
Epoch 170, training loss: 14.784928321838379 = 1.6832830905914307 + 2.0 * 6.550822734832764
Epoch 170, val loss: 1.6934672594070435
Epoch 180, training loss: 14.734609603881836 = 1.6596323251724243 + 2.0 * 6.5374884605407715
Epoch 180, val loss: 1.6723847389221191
Epoch 190, training loss: 14.67620849609375 = 1.6331936120986938 + 2.0 * 6.521507263183594
Epoch 190, val loss: 1.6489300727844238
Epoch 200, training loss: 14.618623733520508 = 1.6036535501480103 + 2.0 * 6.5074849128723145
Epoch 200, val loss: 1.6228861808776855
Epoch 210, training loss: 14.561830520629883 = 1.5707366466522217 + 2.0 * 6.495546817779541
Epoch 210, val loss: 1.5940234661102295
Epoch 220, training loss: 14.51092529296875 = 1.5344038009643555 + 2.0 * 6.488260746002197
Epoch 220, val loss: 1.5623608827590942
Epoch 230, training loss: 14.447914123535156 = 1.4950617551803589 + 2.0 * 6.476426124572754
Epoch 230, val loss: 1.528227686882019
Epoch 240, training loss: 14.386665344238281 = 1.452765941619873 + 2.0 * 6.466949462890625
Epoch 240, val loss: 1.4919408559799194
Epoch 250, training loss: 14.332756042480469 = 1.4076725244522095 + 2.0 * 6.462541580200195
Epoch 250, val loss: 1.4536545276641846
Epoch 260, training loss: 14.264150619506836 = 1.3602252006530762 + 2.0 * 6.451962947845459
Epoch 260, val loss: 1.4140957593917847
Epoch 270, training loss: 14.200827598571777 = 1.3111612796783447 + 2.0 * 6.444833278656006
Epoch 270, val loss: 1.37367844581604
Epoch 280, training loss: 14.148971557617188 = 1.2610926628112793 + 2.0 * 6.443939208984375
Epoch 280, val loss: 1.3330305814743042
Epoch 290, training loss: 14.079889297485352 = 1.2108097076416016 + 2.0 * 6.434539794921875
Epoch 290, val loss: 1.2927945852279663
Epoch 300, training loss: 14.016469955444336 = 1.1606965065002441 + 2.0 * 6.427886962890625
Epoch 300, val loss: 1.2532755136489868
Epoch 310, training loss: 13.956672668457031 = 1.110916018486023 + 2.0 * 6.422878265380859
Epoch 310, val loss: 1.2146896123886108
Epoch 320, training loss: 13.903351783752441 = 1.0620803833007812 + 2.0 * 6.42063570022583
Epoch 320, val loss: 1.1774109601974487
Epoch 330, training loss: 13.845727920532227 = 1.0153727531433105 + 2.0 * 6.415177345275879
Epoch 330, val loss: 1.1420140266418457
Epoch 340, training loss: 13.79141616821289 = 0.9704926013946533 + 2.0 * 6.410461902618408
Epoch 340, val loss: 1.108465552330017
Epoch 350, training loss: 13.739296913146973 = 0.9274116158485413 + 2.0 * 6.405942440032959
Epoch 350, val loss: 1.076589822769165
Epoch 360, training loss: 13.691607475280762 = 0.8863280415534973 + 2.0 * 6.402639865875244
Epoch 360, val loss: 1.0464198589324951
Epoch 370, training loss: 13.651976585388184 = 0.8477525115013123 + 2.0 * 6.402112007141113
Epoch 370, val loss: 1.0184855461120605
Epoch 380, training loss: 13.604910850524902 = 0.8120092153549194 + 2.0 * 6.396450996398926
Epoch 380, val loss: 0.9926387071609497
Epoch 390, training loss: 13.563070297241211 = 0.778780460357666 + 2.0 * 6.392145156860352
Epoch 390, val loss: 0.9689127802848816
Epoch 400, training loss: 13.54104232788086 = 0.7478083372116089 + 2.0 * 6.3966169357299805
Epoch 400, val loss: 0.9470164179801941
Epoch 410, training loss: 13.492908477783203 = 0.7189000248908997 + 2.0 * 6.387004375457764
Epoch 410, val loss: 0.9268612265586853
Epoch 420, training loss: 13.45799732208252 = 0.6917413473129272 + 2.0 * 6.3831281661987305
Epoch 420, val loss: 0.9082334637641907
Epoch 430, training loss: 13.433843612670898 = 0.6659391522407532 + 2.0 * 6.3839521408081055
Epoch 430, val loss: 0.8907814025878906
Epoch 440, training loss: 13.39708137512207 = 0.6413362622261047 + 2.0 * 6.377872467041016
Epoch 440, val loss: 0.8743693232536316
Epoch 450, training loss: 13.366649627685547 = 0.6175010204315186 + 2.0 * 6.374574184417725
Epoch 450, val loss: 0.8587862849235535
Epoch 460, training loss: 13.356512069702148 = 0.5943573117256165 + 2.0 * 6.381077289581299
Epoch 460, val loss: 0.8439328074455261
Epoch 470, training loss: 13.312789916992188 = 0.5718567967414856 + 2.0 * 6.370466709136963
Epoch 470, val loss: 0.8299242854118347
Epoch 480, training loss: 13.282780647277832 = 0.5499927997589111 + 2.0 * 6.36639404296875
Epoch 480, val loss: 0.8168085217475891
Epoch 490, training loss: 13.256054878234863 = 0.5286362767219543 + 2.0 * 6.363709449768066
Epoch 490, val loss: 0.8044499158859253
Epoch 500, training loss: 13.239239692687988 = 0.5078224539756775 + 2.0 * 6.365708827972412
Epoch 500, val loss: 0.7929983139038086
Epoch 510, training loss: 13.213884353637695 = 0.48756664991378784 + 2.0 * 6.363158702850342
Epoch 510, val loss: 0.7824265360832214
Epoch 520, training loss: 13.184771537780762 = 0.46816328167915344 + 2.0 * 6.358304023742676
Epoch 520, val loss: 0.7729375958442688
Epoch 530, training loss: 13.159544944763184 = 0.4494533836841583 + 2.0 * 6.355045795440674
Epoch 530, val loss: 0.7644029259681702
Epoch 540, training loss: 13.13592529296875 = 0.4314359426498413 + 2.0 * 6.352244853973389
Epoch 540, val loss: 0.7568323612213135
Epoch 550, training loss: 13.133944511413574 = 0.41413310170173645 + 2.0 * 6.35990571975708
Epoch 550, val loss: 0.750177264213562
Epoch 560, training loss: 13.096293449401855 = 0.3976489305496216 + 2.0 * 6.349322319030762
Epoch 560, val loss: 0.7445652484893799
Epoch 570, training loss: 13.082757949829102 = 0.3819180428981781 + 2.0 * 6.350419998168945
Epoch 570, val loss: 0.7398943901062012
Epoch 580, training loss: 13.056827545166016 = 0.3669864535331726 + 2.0 * 6.344920635223389
Epoch 580, val loss: 0.7361442446708679
Epoch 590, training loss: 13.038603782653809 = 0.35273998975753784 + 2.0 * 6.342931747436523
Epoch 590, val loss: 0.7332716584205627
Epoch 600, training loss: 13.023337364196777 = 0.3391326367855072 + 2.0 * 6.342102527618408
Epoch 600, val loss: 0.731198787689209
Epoch 610, training loss: 13.006006240844727 = 0.3262234926223755 + 2.0 * 6.33989143371582
Epoch 610, val loss: 0.7297422885894775
Epoch 620, training loss: 12.993834495544434 = 0.3138858675956726 + 2.0 * 6.339974403381348
Epoch 620, val loss: 0.7290735840797424
Epoch 630, training loss: 12.971208572387695 = 0.30218705534935 + 2.0 * 6.334510803222656
Epoch 630, val loss: 0.7289843559265137
Epoch 640, training loss: 12.966954231262207 = 0.29101797938346863 + 2.0 * 6.337968349456787
Epoch 640, val loss: 0.7294506430625916
Epoch 650, training loss: 12.946444511413574 = 0.280319482088089 + 2.0 * 6.333062648773193
Epoch 650, val loss: 0.7304728627204895
Epoch 660, training loss: 12.931479454040527 = 0.2701238989830017 + 2.0 * 6.3306779861450195
Epoch 660, val loss: 0.7319932579994202
Epoch 670, training loss: 12.916495323181152 = 0.26033252477645874 + 2.0 * 6.3280816078186035
Epoch 670, val loss: 0.7339773178100586
Epoch 680, training loss: 12.920441627502441 = 0.2509213984012604 + 2.0 * 6.3347601890563965
Epoch 680, val loss: 0.7363362908363342
Epoch 690, training loss: 12.89486026763916 = 0.24187208712100983 + 2.0 * 6.326494216918945
Epoch 690, val loss: 0.7389692068099976
Epoch 700, training loss: 12.886519432067871 = 0.23314563930034637 + 2.0 * 6.326686859130859
Epoch 700, val loss: 0.7419729828834534
Epoch 710, training loss: 12.867904663085938 = 0.22468337416648865 + 2.0 * 6.321610450744629
Epoch 710, val loss: 0.7451384663581848
Epoch 720, training loss: 12.858026504516602 = 0.21644671261310577 + 2.0 * 6.320789813995361
Epoch 720, val loss: 0.748535692691803
Epoch 730, training loss: 12.849809646606445 = 0.20837973058223724 + 2.0 * 6.320714950561523
Epoch 730, val loss: 0.7521250247955322
Epoch 740, training loss: 12.843124389648438 = 0.20048442482948303 + 2.0 * 6.321320056915283
Epoch 740, val loss: 0.7557734847068787
Epoch 750, training loss: 12.829191207885742 = 0.19274693727493286 + 2.0 * 6.3182220458984375
Epoch 750, val loss: 0.7594075798988342
Epoch 760, training loss: 12.816058158874512 = 0.1851566880941391 + 2.0 * 6.315450668334961
Epoch 760, val loss: 0.7631924748420715
Epoch 770, training loss: 12.804655075073242 = 0.17766791582107544 + 2.0 * 6.313493728637695
Epoch 770, val loss: 0.7669999599456787
Epoch 780, training loss: 12.796178817749023 = 0.17026928067207336 + 2.0 * 6.312954902648926
Epoch 780, val loss: 0.7708178162574768
Epoch 790, training loss: 12.794856071472168 = 0.16294707357883453 + 2.0 * 6.315954685211182
Epoch 790, val loss: 0.7746619582176208
Epoch 800, training loss: 12.782883644104004 = 0.15574891865253448 + 2.0 * 6.313567161560059
Epoch 800, val loss: 0.7785071134567261
Epoch 810, training loss: 12.767349243164062 = 0.14866706728935242 + 2.0 * 6.309340953826904
Epoch 810, val loss: 0.7822696566581726
Epoch 820, training loss: 12.756391525268555 = 0.14172770082950592 + 2.0 * 6.3073320388793945
Epoch 820, val loss: 0.7861459851264954
Epoch 830, training loss: 12.75092601776123 = 0.13493084907531738 + 2.0 * 6.307997703552246
Epoch 830, val loss: 0.7900446653366089
Epoch 840, training loss: 12.750057220458984 = 0.1283145546913147 + 2.0 * 6.310871124267578
Epoch 840, val loss: 0.7939947247505188
Epoch 850, training loss: 12.734474182128906 = 0.12189134210348129 + 2.0 * 6.306291580200195
Epoch 850, val loss: 0.7979897856712341
Epoch 860, training loss: 12.723149299621582 = 0.1157306581735611 + 2.0 * 6.303709506988525
Epoch 860, val loss: 0.8020809888839722
Epoch 870, training loss: 12.713879585266113 = 0.10982652008533478 + 2.0 * 6.302026748657227
Epoch 870, val loss: 0.8062566518783569
Epoch 880, training loss: 12.722115516662598 = 0.10417884588241577 + 2.0 * 6.308968544006348
Epoch 880, val loss: 0.810587465763092
Epoch 890, training loss: 12.701715469360352 = 0.09882970154285431 + 2.0 * 6.301443099975586
Epoch 890, val loss: 0.8149790167808533
Epoch 900, training loss: 12.691570281982422 = 0.09375830739736557 + 2.0 * 6.298905849456787
Epoch 900, val loss: 0.8195396661758423
Epoch 910, training loss: 12.689541816711426 = 0.08896920084953308 + 2.0 * 6.300286293029785
Epoch 910, val loss: 0.8242748379707336
Epoch 920, training loss: 12.681368827819824 = 0.08446101099252701 + 2.0 * 6.2984538078308105
Epoch 920, val loss: 0.8290618658065796
Epoch 930, training loss: 12.674695014953613 = 0.08022663742303848 + 2.0 * 6.297234058380127
Epoch 930, val loss: 0.8340559601783752
Epoch 940, training loss: 12.666011810302734 = 0.07626083493232727 + 2.0 * 6.294875621795654
Epoch 940, val loss: 0.8390253186225891
Epoch 950, training loss: 12.66151237487793 = 0.07254068553447723 + 2.0 * 6.294486045837402
Epoch 950, val loss: 0.844132125377655
Epoch 960, training loss: 12.667150497436523 = 0.06904394924640656 + 2.0 * 6.299053192138672
Epoch 960, val loss: 0.8492794632911682
Epoch 970, training loss: 12.655681610107422 = 0.06576327234506607 + 2.0 * 6.29495906829834
Epoch 970, val loss: 0.8545367121696472
Epoch 980, training loss: 12.665390014648438 = 0.06268472224473953 + 2.0 * 6.301352500915527
Epoch 980, val loss: 0.8596614003181458
Epoch 990, training loss: 12.6436767578125 = 0.05980111286044121 + 2.0 * 6.291937828063965
Epoch 990, val loss: 0.8649286031723022
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7925925925925926
0.8386926726410122
=== training gcn model ===
Epoch 0, training loss: 19.144912719726562 = 1.9512262344360352 + 2.0 * 8.596842765808105
Epoch 0, val loss: 1.9520763158798218
Epoch 10, training loss: 19.134212493896484 = 1.941152811050415 + 2.0 * 8.596529960632324
Epoch 10, val loss: 1.941288709640503
Epoch 20, training loss: 19.11640739440918 = 1.9286404848098755 + 2.0 * 8.593883514404297
Epoch 20, val loss: 1.927683711051941
Epoch 30, training loss: 19.058910369873047 = 1.9116705656051636 + 2.0 * 8.573619842529297
Epoch 30, val loss: 1.9091930389404297
Epoch 40, training loss: 18.803266525268555 = 1.8908772468566895 + 2.0 * 8.456194877624512
Epoch 40, val loss: 1.8875118494033813
Epoch 50, training loss: 18.07175064086914 = 1.8683263063430786 + 2.0 * 8.101712226867676
Epoch 50, val loss: 1.8651171922683716
Epoch 60, training loss: 16.958675384521484 = 1.8508158922195435 + 2.0 * 7.553929805755615
Epoch 60, val loss: 1.8498010635375977
Epoch 70, training loss: 16.043174743652344 = 1.8378545045852661 + 2.0 * 7.102660179138184
Epoch 70, val loss: 1.8377689123153687
Epoch 80, training loss: 15.680710792541504 = 1.824769377708435 + 2.0 * 6.927970886230469
Epoch 80, val loss: 1.824952483177185
Epoch 90, training loss: 15.481218338012695 = 1.8090382814407349 + 2.0 * 6.836090087890625
Epoch 90, val loss: 1.8099960088729858
Epoch 100, training loss: 15.33482551574707 = 1.7921137809753418 + 2.0 * 6.771355628967285
Epoch 100, val loss: 1.7948046922683716
Epoch 110, training loss: 15.230182647705078 = 1.7753790616989136 + 2.0 * 6.7274017333984375
Epoch 110, val loss: 1.7797977924346924
Epoch 120, training loss: 15.14777660369873 = 1.758579969406128 + 2.0 * 6.694598197937012
Epoch 120, val loss: 1.7646292448043823
Epoch 130, training loss: 15.06704330444336 = 1.7410401105880737 + 2.0 * 6.663001537322998
Epoch 130, val loss: 1.7486889362335205
Epoch 140, training loss: 14.996026992797852 = 1.7219417095184326 + 2.0 * 6.63704252243042
Epoch 140, val loss: 1.7315071821212769
Epoch 150, training loss: 14.927910804748535 = 1.7007970809936523 + 2.0 * 6.613556861877441
Epoch 150, val loss: 1.712669849395752
Epoch 160, training loss: 14.861559867858887 = 1.677225112915039 + 2.0 * 6.592167377471924
Epoch 160, val loss: 1.6918824911117554
Epoch 170, training loss: 14.79403018951416 = 1.6509475708007812 + 2.0 * 6.5715413093566895
Epoch 170, val loss: 1.668934941291809
Epoch 180, training loss: 14.727952003479004 = 1.62143075466156 + 2.0 * 6.553260803222656
Epoch 180, val loss: 1.6433825492858887
Epoch 190, training loss: 14.668607711791992 = 1.5883996486663818 + 2.0 * 6.540103912353516
Epoch 190, val loss: 1.6149343252182007
Epoch 200, training loss: 14.60095500946045 = 1.551997423171997 + 2.0 * 6.524478912353516
Epoch 200, val loss: 1.5837382078170776
Epoch 210, training loss: 14.537697792053223 = 1.5120422840118408 + 2.0 * 6.5128278732299805
Epoch 210, val loss: 1.5497688055038452
Epoch 220, training loss: 14.47091007232666 = 1.4687756299972534 + 2.0 * 6.501067161560059
Epoch 220, val loss: 1.5133438110351562
Epoch 230, training loss: 14.405599594116211 = 1.4229153394699097 + 2.0 * 6.491342067718506
Epoch 230, val loss: 1.4749486446380615
Epoch 240, training loss: 14.33858871459961 = 1.3749430179595947 + 2.0 * 6.481822967529297
Epoch 240, val loss: 1.4350751638412476
Epoch 250, training loss: 14.272045135498047 = 1.3251357078552246 + 2.0 * 6.473454475402832
Epoch 250, val loss: 1.3939623832702637
Epoch 260, training loss: 14.212530136108398 = 1.2746411561965942 + 2.0 * 6.468944549560547
Epoch 260, val loss: 1.3525997400283813
Epoch 270, training loss: 14.140362739562988 = 1.2242956161499023 + 2.0 * 6.458033561706543
Epoch 270, val loss: 1.3114962577819824
Epoch 280, training loss: 14.074813842773438 = 1.1742424964904785 + 2.0 * 6.450285911560059
Epoch 280, val loss: 1.2708338499069214
Epoch 290, training loss: 14.014124870300293 = 1.1248142719268799 + 2.0 * 6.444655418395996
Epoch 290, val loss: 1.2308109998703003
Epoch 300, training loss: 13.961906433105469 = 1.0768474340438843 + 2.0 * 6.442529678344727
Epoch 300, val loss: 1.1922677755355835
Epoch 310, training loss: 13.896904945373535 = 1.0309381484985352 + 2.0 * 6.4329833984375
Epoch 310, val loss: 1.1554722785949707
Epoch 320, training loss: 13.841076850891113 = 0.9870630502700806 + 2.0 * 6.427006721496582
Epoch 320, val loss: 1.1206177473068237
Epoch 330, training loss: 13.794589042663574 = 0.9450981616973877 + 2.0 * 6.424745559692383
Epoch 330, val loss: 1.0876307487487793
Epoch 340, training loss: 13.742679595947266 = 0.9058131575584412 + 2.0 * 6.41843318939209
Epoch 340, val loss: 1.0573220252990723
Epoch 350, training loss: 13.699146270751953 = 0.8692985773086548 + 2.0 * 6.414923667907715
Epoch 350, val loss: 1.0295491218566895
Epoch 360, training loss: 13.652853012084961 = 0.8349062204360962 + 2.0 * 6.408973217010498
Epoch 360, val loss: 1.0041097402572632
Epoch 370, training loss: 13.613826751708984 = 0.8026033043861389 + 2.0 * 6.405611515045166
Epoch 370, val loss: 0.9808715581893921
Epoch 380, training loss: 13.573745727539062 = 0.7722804546356201 + 2.0 * 6.400732517242432
Epoch 380, val loss: 0.9597242474555969
Epoch 390, training loss: 13.538817405700684 = 0.7436422109603882 + 2.0 * 6.397587776184082
Epoch 390, val loss: 0.9405367374420166
Epoch 400, training loss: 13.504453659057617 = 0.7165192365646362 + 2.0 * 6.393967151641846
Epoch 400, val loss: 0.9231305718421936
Epoch 410, training loss: 13.47199821472168 = 0.6907685995101929 + 2.0 * 6.390614986419678
Epoch 410, val loss: 0.907349169254303
Epoch 420, training loss: 13.44247817993164 = 0.666135847568512 + 2.0 * 6.388171195983887
Epoch 420, val loss: 0.892937183380127
Epoch 430, training loss: 13.41134262084961 = 0.6424612998962402 + 2.0 * 6.384440898895264
Epoch 430, val loss: 0.8797205090522766
Epoch 440, training loss: 13.382668495178223 = 0.6195636987686157 + 2.0 * 6.381552219390869
Epoch 440, val loss: 0.8675762414932251
Epoch 450, training loss: 13.360289573669434 = 0.5974100828170776 + 2.0 * 6.381439685821533
Epoch 450, val loss: 0.8563574552536011
Epoch 460, training loss: 13.327737808227539 = 0.5759002566337585 + 2.0 * 6.375918865203857
Epoch 460, val loss: 0.8460513949394226
Epoch 470, training loss: 13.299086570739746 = 0.5549814701080322 + 2.0 * 6.3720526695251465
Epoch 470, val loss: 0.8365621566772461
Epoch 480, training loss: 13.282389640808105 = 0.5345795750617981 + 2.0 * 6.373905181884766
Epoch 480, val loss: 0.8278709650039673
Epoch 490, training loss: 13.25411319732666 = 0.5147186517715454 + 2.0 * 6.369697093963623
Epoch 490, val loss: 0.8199010491371155
Epoch 500, training loss: 13.223626136779785 = 0.4954194724559784 + 2.0 * 6.364103317260742
Epoch 500, val loss: 0.8127771615982056
Epoch 510, training loss: 13.201416969299316 = 0.47658810019493103 + 2.0 * 6.362414360046387
Epoch 510, val loss: 0.8064340949058533
Epoch 520, training loss: 13.188291549682617 = 0.4582667946815491 + 2.0 * 6.365012168884277
Epoch 520, val loss: 0.8007996678352356
Epoch 530, training loss: 13.155862808227539 = 0.4404506981372833 + 2.0 * 6.357706069946289
Epoch 530, val loss: 0.7959728240966797
Epoch 540, training loss: 13.139654159545898 = 0.4231247901916504 + 2.0 * 6.358264923095703
Epoch 540, val loss: 0.7918720841407776
Epoch 550, training loss: 13.113871574401855 = 0.40627408027648926 + 2.0 * 6.353798866271973
Epoch 550, val loss: 0.7884876728057861
Epoch 560, training loss: 13.094610214233398 = 0.38986337184906006 + 2.0 * 6.3523736000061035
Epoch 560, val loss: 0.7858205437660217
Epoch 570, training loss: 13.080543518066406 = 0.37385237216949463 + 2.0 * 6.3533453941345215
Epoch 570, val loss: 0.7837973237037659
Epoch 580, training loss: 13.059209823608398 = 0.35835012793540955 + 2.0 * 6.350430011749268
Epoch 580, val loss: 0.782406210899353
Epoch 590, training loss: 13.036857604980469 = 0.343290776014328 + 2.0 * 6.346783638000488
Epoch 590, val loss: 0.7817097306251526
Epoch 600, training loss: 13.017464637756348 = 0.3286151885986328 + 2.0 * 6.344424724578857
Epoch 600, val loss: 0.781566858291626
Epoch 610, training loss: 13.009570121765137 = 0.31432777643203735 + 2.0 * 6.347620964050293
Epoch 610, val loss: 0.7819392681121826
Epoch 620, training loss: 12.988853454589844 = 0.3004798889160156 + 2.0 * 6.344186782836914
Epoch 620, val loss: 0.7828776836395264
Epoch 630, training loss: 12.966689109802246 = 0.28707143664360046 + 2.0 * 6.339808940887451
Epoch 630, val loss: 0.784271776676178
Epoch 640, training loss: 12.949071884155273 = 0.2741398513317108 + 2.0 * 6.337466239929199
Epoch 640, val loss: 0.7862785458564758
Epoch 650, training loss: 12.937129974365234 = 0.26164305210113525 + 2.0 * 6.337743282318115
Epoch 650, val loss: 0.788757860660553
Epoch 660, training loss: 12.919922828674316 = 0.249594584107399 + 2.0 * 6.3351640701293945
Epoch 660, val loss: 0.7917224764823914
Epoch 670, training loss: 12.904812812805176 = 0.23806801438331604 + 2.0 * 6.333372592926025
Epoch 670, val loss: 0.7952384948730469
Epoch 680, training loss: 12.889753341674805 = 0.2270045131444931 + 2.0 * 6.331374645233154
Epoch 680, val loss: 0.7991880774497986
Epoch 690, training loss: 12.900981903076172 = 0.21642501652240753 + 2.0 * 6.342278480529785
Epoch 690, val loss: 0.803555965423584
Epoch 700, training loss: 12.863598823547363 = 0.20632146298885345 + 2.0 * 6.328638553619385
Epoch 700, val loss: 0.808285653591156
Epoch 710, training loss: 12.851155281066895 = 0.19672761857509613 + 2.0 * 6.327213764190674
Epoch 710, val loss: 0.8134585022926331
Epoch 720, training loss: 12.83948040008545 = 0.18759261071681976 + 2.0 * 6.325943946838379
Epoch 720, val loss: 0.8189800977706909
Epoch 730, training loss: 12.836930274963379 = 0.17887525260448456 + 2.0 * 6.3290276527404785
Epoch 730, val loss: 0.824786901473999
Epoch 740, training loss: 12.8206205368042 = 0.17057862877845764 + 2.0 * 6.325020790100098
Epoch 740, val loss: 0.8309839963912964
Epoch 750, training loss: 12.81098461151123 = 0.16268423199653625 + 2.0 * 6.324150085449219
Epoch 750, val loss: 0.8374028205871582
Epoch 760, training loss: 12.798080444335938 = 0.15517191588878632 + 2.0 * 6.321454048156738
Epoch 760, val loss: 0.8440789580345154
Epoch 770, training loss: 12.786892890930176 = 0.14803725481033325 + 2.0 * 6.319427967071533
Epoch 770, val loss: 0.8509846925735474
Epoch 780, training loss: 12.776324272155762 = 0.14125601947307587 + 2.0 * 6.31753396987915
Epoch 780, val loss: 0.8581635355949402
Epoch 790, training loss: 12.782662391662598 = 0.13480167090892792 + 2.0 * 6.323930263519287
Epoch 790, val loss: 0.865507960319519
Epoch 800, training loss: 12.762842178344727 = 0.12867377698421478 + 2.0 * 6.317084312438965
Epoch 800, val loss: 0.8729454874992371
Epoch 810, training loss: 12.753381729125977 = 0.12287011742591858 + 2.0 * 6.315255641937256
Epoch 810, val loss: 0.8805271983146667
Epoch 820, training loss: 12.741584777832031 = 0.1173805370926857 + 2.0 * 6.312102317810059
Epoch 820, val loss: 0.8883160948753357
Epoch 830, training loss: 12.748819351196289 = 0.11216434091329575 + 2.0 * 6.3183274269104
Epoch 830, val loss: 0.8962002396583557
Epoch 840, training loss: 12.736772537231445 = 0.10719440877437592 + 2.0 * 6.314789295196533
Epoch 840, val loss: 0.9040091037750244
Epoch 850, training loss: 12.72047233581543 = 0.10250739753246307 + 2.0 * 6.3089823722839355
Epoch 850, val loss: 0.9120203256607056
Epoch 860, training loss: 12.714754104614258 = 0.09806064516305923 + 2.0 * 6.308346748352051
Epoch 860, val loss: 0.9200300574302673
Epoch 870, training loss: 12.722373962402344 = 0.09383858740329742 + 2.0 * 6.314267635345459
Epoch 870, val loss: 0.927899181842804
Epoch 880, training loss: 12.702698707580566 = 0.08984261006116867 + 2.0 * 6.306427955627441
Epoch 880, val loss: 0.9360128045082092
Epoch 890, training loss: 12.70080852508545 = 0.08605838567018509 + 2.0 * 6.307374954223633
Epoch 890, val loss: 0.9440780878067017
Epoch 900, training loss: 12.690407752990723 = 0.08246725797653198 + 2.0 * 6.3039703369140625
Epoch 900, val loss: 0.9520017504692078
Epoch 910, training loss: 12.685517311096191 = 0.07907417416572571 + 2.0 * 6.303221702575684
Epoch 910, val loss: 0.9600847959518433
Epoch 920, training loss: 12.67927074432373 = 0.0758536085486412 + 2.0 * 6.301708698272705
Epoch 920, val loss: 0.9681088924407959
Epoch 930, training loss: 12.688153266906738 = 0.07279251515865326 + 2.0 * 6.307680606842041
Epoch 930, val loss: 0.9760748744010925
Epoch 940, training loss: 12.684147834777832 = 0.06989731639623642 + 2.0 * 6.307125091552734
Epoch 940, val loss: 0.98387610912323
Epoch 950, training loss: 12.66843032836914 = 0.06715992838144302 + 2.0 * 6.30063533782959
Epoch 950, val loss: 0.9917445182800293
Epoch 960, training loss: 12.660138130187988 = 0.06456456333398819 + 2.0 * 6.297786712646484
Epoch 960, val loss: 0.9996092915534973
Epoch 970, training loss: 12.663022994995117 = 0.06209820881485939 + 2.0 * 6.300462245941162
Epoch 970, val loss: 1.0073277950286865
Epoch 980, training loss: 12.652993202209473 = 0.05974937230348587 + 2.0 * 6.296621799468994
Epoch 980, val loss: 1.014918565750122
Epoch 990, training loss: 12.649502754211426 = 0.05752559378743172 + 2.0 * 6.2959885597229
Epoch 990, val loss: 1.022542953491211
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7740740740740741
0.8413284132841329
=== training gcn model ===
Epoch 0, training loss: 19.1484375 = 1.9547414779663086 + 2.0 * 8.596848487854004
Epoch 0, val loss: 1.9556939601898193
Epoch 10, training loss: 19.13673973083496 = 1.9435913562774658 + 2.0 * 8.596573829650879
Epoch 10, val loss: 1.9447317123413086
Epoch 20, training loss: 19.118486404418945 = 1.9295960664749146 + 2.0 * 8.59444522857666
Epoch 20, val loss: 1.9304277896881104
Epoch 30, training loss: 19.06597137451172 = 1.91029953956604 + 2.0 * 8.577836036682129
Epoch 30, val loss: 1.9104361534118652
Epoch 40, training loss: 18.817703247070312 = 1.8865290880203247 + 2.0 * 8.46558666229248
Epoch 40, val loss: 1.8872520923614502
Epoch 50, training loss: 18.144723892211914 = 1.8639590740203857 + 2.0 * 8.140382766723633
Epoch 50, val loss: 1.8658716678619385
Epoch 60, training loss: 17.135263442993164 = 1.8463914394378662 + 2.0 * 7.644435882568359
Epoch 60, val loss: 1.848490595817566
Epoch 70, training loss: 16.297874450683594 = 1.8351407051086426 + 2.0 * 7.2313666343688965
Epoch 70, val loss: 1.8373708724975586
Epoch 80, training loss: 15.88629150390625 = 1.8227912187576294 + 2.0 * 7.031750202178955
Epoch 80, val loss: 1.8252042531967163
Epoch 90, training loss: 15.605992317199707 = 1.8074849843978882 + 2.0 * 6.899253845214844
Epoch 90, val loss: 1.811118721961975
Epoch 100, training loss: 15.40768051147461 = 1.7929813861846924 + 2.0 * 6.807349681854248
Epoch 100, val loss: 1.7981617450714111
Epoch 110, training loss: 15.254733085632324 = 1.7793241739273071 + 2.0 * 6.737704277038574
Epoch 110, val loss: 1.7857365608215332
Epoch 120, training loss: 15.134262084960938 = 1.7660043239593506 + 2.0 * 6.684128761291504
Epoch 120, val loss: 1.7733385562896729
Epoch 130, training loss: 15.039189338684082 = 1.7519973516464233 + 2.0 * 6.643596172332764
Epoch 130, val loss: 1.760466456413269
Epoch 140, training loss: 14.957097053527832 = 1.736775517463684 + 2.0 * 6.610160827636719
Epoch 140, val loss: 1.7468403577804565
Epoch 150, training loss: 14.887612342834473 = 1.7201130390167236 + 2.0 * 6.583749771118164
Epoch 150, val loss: 1.7322354316711426
Epoch 160, training loss: 14.826493263244629 = 1.7017101049423218 + 2.0 * 6.562391757965088
Epoch 160, val loss: 1.7162868976593018
Epoch 170, training loss: 14.768128395080566 = 1.6814295053482056 + 2.0 * 6.543349266052246
Epoch 170, val loss: 1.6989316940307617
Epoch 180, training loss: 14.712879180908203 = 1.6588573455810547 + 2.0 * 6.527010917663574
Epoch 180, val loss: 1.6798171997070312
Epoch 190, training loss: 14.65800952911377 = 1.6337556838989258 + 2.0 * 6.512126922607422
Epoch 190, val loss: 1.6586744785308838
Epoch 200, training loss: 14.600817680358887 = 1.6058839559555054 + 2.0 * 6.497467041015625
Epoch 200, val loss: 1.6353625059127808
Epoch 210, training loss: 14.548831939697266 = 1.5751782655715942 + 2.0 * 6.4868268966674805
Epoch 210, val loss: 1.6098649501800537
Epoch 220, training loss: 14.492996215820312 = 1.541795253753662 + 2.0 * 6.475600242614746
Epoch 220, val loss: 1.582319736480713
Epoch 230, training loss: 14.436388969421387 = 1.505772352218628 + 2.0 * 6.46530818939209
Epoch 230, val loss: 1.552807331085205
Epoch 240, training loss: 14.383049964904785 = 1.4672958850860596 + 2.0 * 6.457877159118652
Epoch 240, val loss: 1.5214951038360596
Epoch 250, training loss: 14.327271461486816 = 1.4268264770507812 + 2.0 * 6.450222492218018
Epoch 250, val loss: 1.4888216257095337
Epoch 260, training loss: 14.276084899902344 = 1.3848624229431152 + 2.0 * 6.445611000061035
Epoch 260, val loss: 1.4553776979446411
Epoch 270, training loss: 14.211820602416992 = 1.342004656791687 + 2.0 * 6.434907913208008
Epoch 270, val loss: 1.4213348627090454
Epoch 280, training loss: 14.15444278717041 = 1.2983577251434326 + 2.0 * 6.428042411804199
Epoch 280, val loss: 1.3869355916976929
Epoch 290, training loss: 14.105955123901367 = 1.2542834281921387 + 2.0 * 6.425836086273193
Epoch 290, val loss: 1.3525526523590088
Epoch 300, training loss: 14.047806739807129 = 1.210723876953125 + 2.0 * 6.418541431427002
Epoch 300, val loss: 1.3191243410110474
Epoch 310, training loss: 13.992144584655762 = 1.1679017543792725 + 2.0 * 6.412121295928955
Epoch 310, val loss: 1.2865546941757202
Epoch 320, training loss: 13.941364288330078 = 1.1259312629699707 + 2.0 * 6.407716751098633
Epoch 320, val loss: 1.2551214694976807
Epoch 330, training loss: 13.891790390014648 = 1.0851294994354248 + 2.0 * 6.403330326080322
Epoch 330, val loss: 1.2249518632888794
Epoch 340, training loss: 13.84272575378418 = 1.0456949472427368 + 2.0 * 6.398515224456787
Epoch 340, val loss: 1.1962063312530518
Epoch 350, training loss: 13.800760269165039 = 1.007462978363037 + 2.0 * 6.39664888381958
Epoch 350, val loss: 1.1686997413635254
Epoch 360, training loss: 13.756709098815918 = 0.9704973697662354 + 2.0 * 6.393105983734131
Epoch 360, val loss: 1.1425203084945679
Epoch 370, training loss: 13.708137512207031 = 0.9346966743469238 + 2.0 * 6.386720180511475
Epoch 370, val loss: 1.1174534559249878
Epoch 380, training loss: 13.66476821899414 = 0.8998836278915405 + 2.0 * 6.382442474365234
Epoch 380, val loss: 1.0933846235275269
Epoch 390, training loss: 13.638114929199219 = 0.8660051822662354 + 2.0 * 6.386054992675781
Epoch 390, val loss: 1.0700230598449707
Epoch 400, training loss: 13.589457511901855 = 0.8335593938827515 + 2.0 * 6.377949237823486
Epoch 400, val loss: 1.0478456020355225
Epoch 410, training loss: 13.549399375915527 = 0.8022332787513733 + 2.0 * 6.37358283996582
Epoch 410, val loss: 1.0267388820648193
Epoch 420, training loss: 13.511613845825195 = 0.7719835638999939 + 2.0 * 6.369815349578857
Epoch 420, val loss: 1.0065406560897827
Epoch 430, training loss: 13.49677562713623 = 0.7426973581314087 + 2.0 * 6.377038955688477
Epoch 430, val loss: 0.9873085021972656
Epoch 440, training loss: 13.444408416748047 = 0.7147616147994995 + 2.0 * 6.364823341369629
Epoch 440, val loss: 0.9689925312995911
Epoch 450, training loss: 13.41185474395752 = 0.6879221200942993 + 2.0 * 6.361966133117676
Epoch 450, val loss: 0.9519802331924438
Epoch 460, training loss: 13.381664276123047 = 0.6620436310768127 + 2.0 * 6.3598103523254395
Epoch 460, val loss: 0.9358874559402466
Epoch 470, training loss: 13.353443145751953 = 0.6370764374732971 + 2.0 * 6.35818338394165
Epoch 470, val loss: 0.9207938313484192
Epoch 480, training loss: 13.325528144836426 = 0.6130601763725281 + 2.0 * 6.356234073638916
Epoch 480, val loss: 0.9066690802574158
Epoch 490, training loss: 13.295228004455566 = 0.5898963809013367 + 2.0 * 6.352665901184082
Epoch 490, val loss: 0.8935536742210388
Epoch 500, training loss: 13.271960258483887 = 0.5673990249633789 + 2.0 * 6.352280616760254
Epoch 500, val loss: 0.8812942504882812
Epoch 510, training loss: 13.242257118225098 = 0.5455571413040161 + 2.0 * 6.3483500480651855
Epoch 510, val loss: 0.8698169589042664
Epoch 520, training loss: 13.2338285446167 = 0.5243760347366333 + 2.0 * 6.354726314544678
Epoch 520, val loss: 0.8593100309371948
Epoch 530, training loss: 13.199256896972656 = 0.5040926337242126 + 2.0 * 6.3475823402404785
Epoch 530, val loss: 0.8496299386024475
Epoch 540, training loss: 13.170945167541504 = 0.4844515025615692 + 2.0 * 6.343246936798096
Epoch 540, val loss: 0.8409876227378845
Epoch 550, training loss: 13.14606761932373 = 0.4654144048690796 + 2.0 * 6.34032678604126
Epoch 550, val loss: 0.833145022392273
Epoch 560, training loss: 13.141671180725098 = 0.44695013761520386 + 2.0 * 6.347360610961914
Epoch 560, val loss: 0.8261427283287048
Epoch 570, training loss: 13.110554695129395 = 0.4292435050010681 + 2.0 * 6.34065580368042
Epoch 570, val loss: 0.8197829723358154
Epoch 580, training loss: 13.084789276123047 = 0.4121125638484955 + 2.0 * 6.336338520050049
Epoch 580, val loss: 0.814320981502533
Epoch 590, training loss: 13.06457233428955 = 0.39561495184898376 + 2.0 * 6.334478855133057
Epoch 590, val loss: 0.8096599578857422
Epoch 600, training loss: 13.04517650604248 = 0.3796846270561218 + 2.0 * 6.3327460289001465
Epoch 600, val loss: 0.8055447936058044
Epoch 610, training loss: 13.033611297607422 = 0.3643934726715088 + 2.0 * 6.334609031677246
Epoch 610, val loss: 0.802283525466919
Epoch 620, training loss: 13.008872985839844 = 0.34961429238319397 + 2.0 * 6.329629421234131
Epoch 620, val loss: 0.7994754314422607
Epoch 630, training loss: 12.991393089294434 = 0.3353908658027649 + 2.0 * 6.328001022338867
Epoch 630, val loss: 0.797446608543396
Epoch 640, training loss: 12.974245071411133 = 0.3216317892074585 + 2.0 * 6.3263068199157715
Epoch 640, val loss: 0.7960291504859924
Epoch 650, training loss: 12.975652694702148 = 0.30832725763320923 + 2.0 * 6.333662509918213
Epoch 650, val loss: 0.7951871156692505
Epoch 660, training loss: 12.946571350097656 = 0.2954109311103821 + 2.0 * 6.32558012008667
Epoch 660, val loss: 0.7947108745574951
Epoch 670, training loss: 12.927228927612305 = 0.2829555571079254 + 2.0 * 6.322136878967285
Epoch 670, val loss: 0.7948229908943176
Epoch 680, training loss: 12.913412094116211 = 0.2708589732646942 + 2.0 * 6.321276664733887
Epoch 680, val loss: 0.7954094409942627
Epoch 690, training loss: 12.915369987487793 = 0.25909367203712463 + 2.0 * 6.32813835144043
Epoch 690, val loss: 0.79637211561203
Epoch 700, training loss: 12.88668155670166 = 0.24778759479522705 + 2.0 * 6.319447040557861
Epoch 700, val loss: 0.7979153394699097
Epoch 710, training loss: 12.871254920959473 = 0.2368468940258026 + 2.0 * 6.317203998565674
Epoch 710, val loss: 0.7998414039611816
Epoch 720, training loss: 12.862812995910645 = 0.22625674307346344 + 2.0 * 6.3182783126831055
Epoch 720, val loss: 0.8022026419639587
Epoch 730, training loss: 12.845904350280762 = 0.21606606245040894 + 2.0 * 6.3149189949035645
Epoch 730, val loss: 0.804871678352356
Epoch 740, training loss: 12.843426704406738 = 0.20626890659332275 + 2.0 * 6.318578720092773
Epoch 740, val loss: 0.8080267906188965
Epoch 750, training loss: 12.829904556274414 = 0.19686652719974518 + 2.0 * 6.316518783569336
Epoch 750, val loss: 0.8113960027694702
Epoch 760, training loss: 12.811436653137207 = 0.18787090480327606 + 2.0 * 6.3117828369140625
Epoch 760, val loss: 0.815100908279419
Epoch 770, training loss: 12.800890922546387 = 0.17924028635025024 + 2.0 * 6.310825347900391
Epoch 770, val loss: 0.8190089464187622
Epoch 780, training loss: 12.795899391174316 = 0.17099465429782867 + 2.0 * 6.31245231628418
Epoch 780, val loss: 0.8232935070991516
Epoch 790, training loss: 12.783992767333984 = 0.16315217316150665 + 2.0 * 6.310420513153076
Epoch 790, val loss: 0.8279768824577332
Epoch 800, training loss: 12.769025802612305 = 0.15563401579856873 + 2.0 * 6.306695938110352
Epoch 800, val loss: 0.8326109647750854
Epoch 810, training loss: 12.759664535522461 = 0.14850099384784698 + 2.0 * 6.305581569671631
Epoch 810, val loss: 0.8377967476844788
Epoch 820, training loss: 12.763832092285156 = 0.14168427884578705 + 2.0 * 6.3110737800598145
Epoch 820, val loss: 0.8430261015892029
Epoch 830, training loss: 12.748189926147461 = 0.13521508872509003 + 2.0 * 6.306487560272217
Epoch 830, val loss: 0.8484095335006714
Epoch 840, training loss: 12.735875129699707 = 0.12904156744480133 + 2.0 * 6.303416728973389
Epoch 840, val loss: 0.8541397452354431
Epoch 850, training loss: 12.74105453491211 = 0.12319397926330566 + 2.0 * 6.308930397033691
Epoch 850, val loss: 0.8600059747695923
Epoch 860, training loss: 12.720826148986816 = 0.11759401112794876 + 2.0 * 6.301616191864014
Epoch 860, val loss: 0.8657736778259277
Epoch 870, training loss: 12.711780548095703 = 0.11229760199785233 + 2.0 * 6.299741268157959
Epoch 870, val loss: 0.8717713356018066
Epoch 880, training loss: 12.70409870147705 = 0.10725303739309311 + 2.0 * 6.298422813415527
Epoch 880, val loss: 0.8779351115226746
Epoch 890, training loss: 12.717268943786621 = 0.10244163870811462 + 2.0 * 6.307413578033447
Epoch 890, val loss: 0.8840480446815491
Epoch 900, training loss: 12.697577476501465 = 0.09790518879890442 + 2.0 * 6.299836158752441
Epoch 900, val loss: 0.8903106451034546
Epoch 910, training loss: 12.687383651733398 = 0.09356824308633804 + 2.0 * 6.296907901763916
Epoch 910, val loss: 0.8965855836868286
Epoch 920, training loss: 12.6950044631958 = 0.08946716040372849 + 2.0 * 6.302768707275391
Epoch 920, val loss: 0.902837336063385
Epoch 930, training loss: 12.678983688354492 = 0.08559134602546692 + 2.0 * 6.296696186065674
Epoch 930, val loss: 0.9093577861785889
Epoch 940, training loss: 12.670665740966797 = 0.08189569413661957 + 2.0 * 6.294384956359863
Epoch 940, val loss: 0.9157333970069885
Epoch 950, training loss: 12.666121482849121 = 0.07840682566165924 + 2.0 * 6.293857097625732
Epoch 950, val loss: 0.9222888946533203
Epoch 960, training loss: 12.674040794372559 = 0.0750863328576088 + 2.0 * 6.2994771003723145
Epoch 960, val loss: 0.9288709163665771
Epoch 970, training loss: 12.655153274536133 = 0.07192061841487885 + 2.0 * 6.291616439819336
Epoch 970, val loss: 0.935167670249939
Epoch 980, training loss: 12.650066375732422 = 0.06893427670001984 + 2.0 * 6.2905659675598145
Epoch 980, val loss: 0.9418049454689026
Epoch 990, training loss: 12.643771171569824 = 0.06609649211168289 + 2.0 * 6.288837432861328
Epoch 990, val loss: 0.9481905102729797
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7851851851851852
0.8371112282551397
The final CL Acc:0.78395, 0.00761, The final GNN Acc:0.83904, 0.00174
Begin epxeriment: cont_weight: 2 epoch:2000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11608])
remove edge: torch.Size([2, 9506])
updated graph: torch.Size([2, 10558])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 19.14646339416504 = 1.952781081199646 + 2.0 * 8.596840858459473
Epoch 0, val loss: 1.9462817907333374
Epoch 10, training loss: 19.134239196777344 = 1.941316843032837 + 2.0 * 8.596461296081543
Epoch 10, val loss: 1.9354338645935059
Epoch 20, training loss: 19.11343765258789 = 1.927161455154419 + 2.0 * 8.593137741088867
Epoch 20, val loss: 1.9215867519378662
Epoch 30, training loss: 19.043140411376953 = 1.9083436727523804 + 2.0 * 8.567398071289062
Epoch 30, val loss: 1.9031274318695068
Epoch 40, training loss: 18.719392776489258 = 1.8860234022140503 + 2.0 * 8.416685104370117
Epoch 40, val loss: 1.882533311843872
Epoch 50, training loss: 17.807811737060547 = 1.8636380434036255 + 2.0 * 7.972086429595947
Epoch 50, val loss: 1.8627722263336182
Epoch 60, training loss: 17.092384338378906 = 1.845062017440796 + 2.0 * 7.623661041259766
Epoch 60, val loss: 1.847099781036377
Epoch 70, training loss: 16.26122283935547 = 1.8317595720291138 + 2.0 * 7.214731693267822
Epoch 70, val loss: 1.835108995437622
Epoch 80, training loss: 15.77685260772705 = 1.8205772638320923 + 2.0 * 6.978137493133545
Epoch 80, val loss: 1.824521541595459
Epoch 90, training loss: 15.45712947845459 = 1.8092070817947388 + 2.0 * 6.82396125793457
Epoch 90, val loss: 1.8140157461166382
Epoch 100, training loss: 15.266633033752441 = 1.7974101305007935 + 2.0 * 6.734611511230469
Epoch 100, val loss: 1.8038851022720337
Epoch 110, training loss: 15.133697509765625 = 1.7862811088562012 + 2.0 * 6.673708438873291
Epoch 110, val loss: 1.794443130493164
Epoch 120, training loss: 15.043549537658691 = 1.7754193544387817 + 2.0 * 6.6340651512146
Epoch 120, val loss: 1.7851855754852295
Epoch 130, training loss: 14.946746826171875 = 1.7644494771957397 + 2.0 * 6.591148853302002
Epoch 130, val loss: 1.7757949829101562
Epoch 140, training loss: 14.866096496582031 = 1.7531075477600098 + 2.0 * 6.55649471282959
Epoch 140, val loss: 1.7663710117340088
Epoch 150, training loss: 14.8079195022583 = 1.7410027980804443 + 2.0 * 6.533458232879639
Epoch 150, val loss: 1.7565966844558716
Epoch 160, training loss: 14.741632461547852 = 1.7274386882781982 + 2.0 * 6.507096767425537
Epoch 160, val loss: 1.7460541725158691
Epoch 170, training loss: 14.688201904296875 = 1.7122553586959839 + 2.0 * 6.487973213195801
Epoch 170, val loss: 1.7344030141830444
Epoch 180, training loss: 14.636953353881836 = 1.6951866149902344 + 2.0 * 6.470883369445801
Epoch 180, val loss: 1.7214730978012085
Epoch 190, training loss: 14.587610244750977 = 1.676051139831543 + 2.0 * 6.455779552459717
Epoch 190, val loss: 1.707118034362793
Epoch 200, training loss: 14.5391845703125 = 1.6544684171676636 + 2.0 * 6.442358016967773
Epoch 200, val loss: 1.691044569015503
Epoch 210, training loss: 14.491266250610352 = 1.6301517486572266 + 2.0 * 6.4305572509765625
Epoch 210, val loss: 1.6729432344436646
Epoch 220, training loss: 14.46452808380127 = 1.6027662754058838 + 2.0 * 6.430881023406982
Epoch 220, val loss: 1.6526802778244019
Epoch 230, training loss: 14.400213241577148 = 1.5725960731506348 + 2.0 * 6.413808345794678
Epoch 230, val loss: 1.6304535865783691
Epoch 240, training loss: 14.352834701538086 = 1.539762020111084 + 2.0 * 6.406536102294922
Epoch 240, val loss: 1.6064311265945435
Epoch 250, training loss: 14.302770614624023 = 1.5043586492538452 + 2.0 * 6.399206161499023
Epoch 250, val loss: 1.580687403678894
Epoch 260, training loss: 14.253144264221191 = 1.4665544033050537 + 2.0 * 6.393294811248779
Epoch 260, val loss: 1.5534002780914307
Epoch 270, training loss: 14.2132568359375 = 1.4267768859863281 + 2.0 * 6.393239974975586
Epoch 270, val loss: 1.5250962972640991
Epoch 280, training loss: 14.15837574005127 = 1.3860186338424683 + 2.0 * 6.386178493499756
Epoch 280, val loss: 1.4967238903045654
Epoch 290, training loss: 14.10389232635498 = 1.3450127840042114 + 2.0 * 6.379439830780029
Epoch 290, val loss: 1.468482255935669
Epoch 300, training loss: 14.053027153015137 = 1.3037155866622925 + 2.0 * 6.374655723571777
Epoch 300, val loss: 1.4406168460845947
Epoch 310, training loss: 14.003861427307129 = 1.2623860836029053 + 2.0 * 6.370737552642822
Epoch 310, val loss: 1.4130789041519165
Epoch 320, training loss: 13.960697174072266 = 1.2213717699050903 + 2.0 * 6.369662761688232
Epoch 320, val loss: 1.386108636856079
Epoch 330, training loss: 13.91151237487793 = 1.1811389923095703 + 2.0 * 6.36518669128418
Epoch 330, val loss: 1.3600869178771973
Epoch 340, training loss: 13.860822677612305 = 1.1415156126022339 + 2.0 * 6.359653472900391
Epoch 340, val loss: 1.3345311880111694
Epoch 350, training loss: 13.813093185424805 = 1.1024169921875 + 2.0 * 6.355338096618652
Epoch 350, val loss: 1.3094652891159058
Epoch 360, training loss: 13.780197143554688 = 1.0637969970703125 + 2.0 * 6.3582000732421875
Epoch 360, val loss: 1.2849218845367432
Epoch 370, training loss: 13.729146003723145 = 1.0260649919509888 + 2.0 * 6.351540565490723
Epoch 370, val loss: 1.260694980621338
Epoch 380, training loss: 13.683354377746582 = 0.9893665909767151 + 2.0 * 6.346993923187256
Epoch 380, val loss: 1.2371599674224854
Epoch 390, training loss: 13.640498161315918 = 0.953324019908905 + 2.0 * 6.3435869216918945
Epoch 390, val loss: 1.2141982316970825
Epoch 400, training loss: 13.603219985961914 = 0.9182441830635071 + 2.0 * 6.342487812042236
Epoch 400, val loss: 1.1920380592346191
Epoch 410, training loss: 13.562352180480957 = 0.8846767544746399 + 2.0 * 6.338837623596191
Epoch 410, val loss: 1.1708805561065674
Epoch 420, training loss: 13.525628089904785 = 0.8525010347366333 + 2.0 * 6.336563587188721
Epoch 420, val loss: 1.1510778665542603
Epoch 430, training loss: 13.490180969238281 = 0.8220798969268799 + 2.0 * 6.33405065536499
Epoch 430, val loss: 1.132620096206665
Epoch 440, training loss: 13.454283714294434 = 0.793426513671875 + 2.0 * 6.330428600311279
Epoch 440, val loss: 1.115707516670227
Epoch 450, training loss: 13.43255615234375 = 0.7664450407028198 + 2.0 * 6.33305549621582
Epoch 450, val loss: 1.1002082824707031
Epoch 460, training loss: 13.394648551940918 = 0.7411327958106995 + 2.0 * 6.326757907867432
Epoch 460, val loss: 1.0862973928451538
Epoch 470, training loss: 13.364686012268066 = 0.7172338366508484 + 2.0 * 6.323726177215576
Epoch 470, val loss: 1.0737565755844116
Epoch 480, training loss: 13.342928886413574 = 0.6946114897727966 + 2.0 * 6.324158668518066
Epoch 480, val loss: 1.062419056892395
Epoch 490, training loss: 13.315130233764648 = 0.6731451749801636 + 2.0 * 6.320992469787598
Epoch 490, val loss: 1.0519649982452393
Epoch 500, training loss: 13.291461944580078 = 0.6525979042053223 + 2.0 * 6.319432258605957
Epoch 500, val loss: 1.0425492525100708
Epoch 510, training loss: 13.264480590820312 = 0.6328021287918091 + 2.0 * 6.3158392906188965
Epoch 510, val loss: 1.033814787864685
Epoch 520, training loss: 13.241024017333984 = 0.6135225296020508 + 2.0 * 6.313750743865967
Epoch 520, val loss: 1.0256552696228027
Epoch 530, training loss: 13.218735694885254 = 0.5945491194725037 + 2.0 * 6.312093257904053
Epoch 530, val loss: 1.0177903175354004
Epoch 540, training loss: 13.200909614562988 = 0.5757680535316467 + 2.0 * 6.312570571899414
Epoch 540, val loss: 1.0101276636123657
Epoch 550, training loss: 13.186017990112305 = 0.5571306347846985 + 2.0 * 6.314443588256836
Epoch 550, val loss: 1.0026605129241943
Epoch 560, training loss: 13.15618896484375 = 0.5385839939117432 + 2.0 * 6.308802604675293
Epoch 560, val loss: 0.9954165816307068
Epoch 570, training loss: 13.130952835083008 = 0.5199260115623474 + 2.0 * 6.305513381958008
Epoch 570, val loss: 0.9882485866546631
Epoch 580, training loss: 13.107620239257812 = 0.5010555386543274 + 2.0 * 6.303282260894775
Epoch 580, val loss: 0.9809009432792664
Epoch 590, training loss: 13.090482711791992 = 0.481914758682251 + 2.0 * 6.30428409576416
Epoch 590, val loss: 0.9734809994697571
Epoch 600, training loss: 13.072735786437988 = 0.4626716375350952 + 2.0 * 6.305032253265381
Epoch 600, val loss: 0.9662268757820129
Epoch 610, training loss: 13.041440963745117 = 0.4433708190917969 + 2.0 * 6.29903507232666
Epoch 610, val loss: 0.958763837814331
Epoch 620, training loss: 13.020049095153809 = 0.4240250885486603 + 2.0 * 6.298011779785156
Epoch 620, val loss: 0.9514911770820618
Epoch 630, training loss: 13.000237464904785 = 0.40473806858062744 + 2.0 * 6.2977495193481445
Epoch 630, val loss: 0.9443376660346985
Epoch 640, training loss: 12.983505249023438 = 0.38565343618392944 + 2.0 * 6.298925876617432
Epoch 640, val loss: 0.9373893737792969
Epoch 650, training loss: 12.956761360168457 = 0.36690041422843933 + 2.0 * 6.294930458068848
Epoch 650, val loss: 0.9306964874267578
Epoch 660, training loss: 12.937198638916016 = 0.3485608398914337 + 2.0 * 6.294318675994873
Epoch 660, val loss: 0.9245800375938416
Epoch 670, training loss: 12.913995742797852 = 0.33065927028656006 + 2.0 * 6.29166841506958
Epoch 670, val loss: 0.9188052415847778
Epoch 680, training loss: 12.900659561157227 = 0.3132508397102356 + 2.0 * 6.293704509735107
Epoch 680, val loss: 0.9135343432426453
Epoch 690, training loss: 12.886122703552246 = 0.2964131832122803 + 2.0 * 6.294854640960693
Epoch 690, val loss: 0.9087433218955994
Epoch 700, training loss: 12.85983943939209 = 0.2802639603614807 + 2.0 * 6.289787769317627
Epoch 700, val loss: 0.9043706655502319
Epoch 710, training loss: 12.839488983154297 = 0.2647235691547394 + 2.0 * 6.28738260269165
Epoch 710, val loss: 0.9006482362747192
Epoch 720, training loss: 12.824098587036133 = 0.24982276558876038 + 2.0 * 6.287137985229492
Epoch 720, val loss: 0.8973628282546997
Epoch 730, training loss: 12.806807518005371 = 0.235639750957489 + 2.0 * 6.285583972930908
Epoch 730, val loss: 0.8945449590682983
Epoch 740, training loss: 12.794404983520508 = 0.2222265750169754 + 2.0 * 6.2860894203186035
Epoch 740, val loss: 0.8924759030342102
Epoch 750, training loss: 12.777579307556152 = 0.20950131118297577 + 2.0 * 6.28403902053833
Epoch 750, val loss: 0.8908991813659668
Epoch 760, training loss: 12.763375282287598 = 0.19744637608528137 + 2.0 * 6.28296422958374
Epoch 760, val loss: 0.8897997140884399
Epoch 770, training loss: 12.7576904296875 = 0.1861318200826645 + 2.0 * 6.2857794761657715
Epoch 770, val loss: 0.8891212940216064
Epoch 780, training loss: 12.743037223815918 = 0.17556364834308624 + 2.0 * 6.283736705780029
Epoch 780, val loss: 0.8890686631202698
Epoch 790, training loss: 12.727276802062988 = 0.1656654179096222 + 2.0 * 6.280805587768555
Epoch 790, val loss: 0.8896713852882385
Epoch 800, training loss: 12.71408748626709 = 0.15636494755744934 + 2.0 * 6.278861045837402
Epoch 800, val loss: 0.8905631899833679
Epoch 810, training loss: 12.710491180419922 = 0.1476164609193802 + 2.0 * 6.281437397003174
Epoch 810, val loss: 0.891942024230957
Epoch 820, training loss: 12.697303771972656 = 0.13950513303279877 + 2.0 * 6.278899192810059
Epoch 820, val loss: 0.8937680721282959
Epoch 830, training loss: 12.687807083129883 = 0.1318882554769516 + 2.0 * 6.27795934677124
Epoch 830, val loss: 0.8960245251655579
Epoch 840, training loss: 12.674339294433594 = 0.12479593604803085 + 2.0 * 6.274771690368652
Epoch 840, val loss: 0.8986048102378845
Epoch 850, training loss: 12.668204307556152 = 0.11814204603433609 + 2.0 * 6.275031089782715
Epoch 850, val loss: 0.901526927947998
Epoch 860, training loss: 12.663358688354492 = 0.11190154403448105 + 2.0 * 6.275728702545166
Epoch 860, val loss: 0.9045745730400085
Epoch 870, training loss: 12.652678489685059 = 0.10609614104032516 + 2.0 * 6.273291110992432
Epoch 870, val loss: 0.9080509543418884
Epoch 880, training loss: 12.650223731994629 = 0.10065769404172897 + 2.0 * 6.274783134460449
Epoch 880, val loss: 0.9116396903991699
Epoch 890, training loss: 12.637358665466309 = 0.095582015812397 + 2.0 * 6.270888328552246
Epoch 890, val loss: 0.9155126214027405
Epoch 900, training loss: 12.633193969726562 = 0.09081654995679855 + 2.0 * 6.271188735961914
Epoch 900, val loss: 0.9195534586906433
Epoch 910, training loss: 12.637263298034668 = 0.08635824918746948 + 2.0 * 6.275452613830566
Epoch 910, val loss: 0.9236357808113098
Epoch 920, training loss: 12.623064041137695 = 0.0821961760520935 + 2.0 * 6.2704339027404785
Epoch 920, val loss: 0.9281250834465027
Epoch 930, training loss: 12.615208625793457 = 0.0783042311668396 + 2.0 * 6.268452167510986
Epoch 930, val loss: 0.9326058030128479
Epoch 940, training loss: 12.61264419555664 = 0.07465546578168869 + 2.0 * 6.268994331359863
Epoch 940, val loss: 0.9372298717498779
Epoch 950, training loss: 12.612390518188477 = 0.07124261558055878 + 2.0 * 6.27057409286499
Epoch 950, val loss: 0.9418003559112549
Epoch 960, training loss: 12.60428524017334 = 0.06806167215108871 + 2.0 * 6.268111705780029
Epoch 960, val loss: 0.946563184261322
Epoch 970, training loss: 12.595588684082031 = 0.06507448107004166 + 2.0 * 6.265256881713867
Epoch 970, val loss: 0.9514094591140747
Epoch 980, training loss: 12.59040641784668 = 0.062271859496831894 + 2.0 * 6.26406717300415
Epoch 980, val loss: 0.9562254548072815
Epoch 990, training loss: 12.588730812072754 = 0.0596347451210022 + 2.0 * 6.264547824859619
Epoch 990, val loss: 0.9610700607299805
Epoch 1000, training loss: 12.591373443603516 = 0.05715490132570267 + 2.0 * 6.267109394073486
Epoch 1000, val loss: 0.9659440517425537
Epoch 1010, training loss: 12.579996109008789 = 0.054838623851537704 + 2.0 * 6.262578964233398
Epoch 1010, val loss: 0.9710273742675781
Epoch 1020, training loss: 12.575316429138184 = 0.05264163762331009 + 2.0 * 6.2613372802734375
Epoch 1020, val loss: 0.9760092496871948
Epoch 1030, training loss: 12.589990615844727 = 0.05057809129357338 + 2.0 * 6.2697062492370605
Epoch 1030, val loss: 0.9809868931770325
Epoch 1040, training loss: 12.576308250427246 = 0.04863588511943817 + 2.0 * 6.26383638381958
Epoch 1040, val loss: 0.985819399356842
Epoch 1050, training loss: 12.565550804138184 = 0.046806760132312775 + 2.0 * 6.259372234344482
Epoch 1050, val loss: 0.9908603429794312
Epoch 1060, training loss: 12.567638397216797 = 0.04507806524634361 + 2.0 * 6.261280059814453
Epoch 1060, val loss: 0.9957786202430725
Epoch 1070, training loss: 12.562643051147461 = 0.043443065136671066 + 2.0 * 6.2596001625061035
Epoch 1070, val loss: 1.0006307363510132
Epoch 1080, training loss: 12.559011459350586 = 0.04189212992787361 + 2.0 * 6.258559703826904
Epoch 1080, val loss: 1.0055094957351685
Epoch 1090, training loss: 12.55825424194336 = 0.04042155295610428 + 2.0 * 6.25891637802124
Epoch 1090, val loss: 1.0103651285171509
Epoch 1100, training loss: 12.558292388916016 = 0.0390324592590332 + 2.0 * 6.259629726409912
Epoch 1100, val loss: 1.015160083770752
Epoch 1110, training loss: 12.55231761932373 = 0.03771726042032242 + 2.0 * 6.25730037689209
Epoch 1110, val loss: 1.0200227499008179
Epoch 1120, training loss: 12.552568435668945 = 0.03646194189786911 + 2.0 * 6.258053302764893
Epoch 1120, val loss: 1.0247821807861328
Epoch 1130, training loss: 12.547372817993164 = 0.03527471423149109 + 2.0 * 6.256049156188965
Epoch 1130, val loss: 1.0294547080993652
Epoch 1140, training loss: 12.54342269897461 = 0.034141905605793 + 2.0 * 6.254640579223633
Epoch 1140, val loss: 1.034234881401062
Epoch 1150, training loss: 12.541019439697266 = 0.03306582570075989 + 2.0 * 6.253976821899414
Epoch 1150, val loss: 1.038866639137268
Epoch 1160, training loss: 12.55038070678711 = 0.03203824535012245 + 2.0 * 6.259171009063721
Epoch 1160, val loss: 1.043514609336853
Epoch 1170, training loss: 12.54237174987793 = 0.03105997107923031 + 2.0 * 6.255655765533447
Epoch 1170, val loss: 1.047951340675354
Epoch 1180, training loss: 12.535998344421387 = 0.030124470591545105 + 2.0 * 6.252936840057373
Epoch 1180, val loss: 1.0525555610656738
Epoch 1190, training loss: 12.537781715393066 = 0.0292337778955698 + 2.0 * 6.254273891448975
Epoch 1190, val loss: 1.0569801330566406
Epoch 1200, training loss: 12.532155990600586 = 0.02838774211704731 + 2.0 * 6.2518839836120605
Epoch 1200, val loss: 1.0613445043563843
Epoch 1210, training loss: 12.529500007629395 = 0.027576450258493423 + 2.0 * 6.250961780548096
Epoch 1210, val loss: 1.0657954216003418
Epoch 1220, training loss: 12.5299072265625 = 0.026802657172083855 + 2.0 * 6.251552104949951
Epoch 1220, val loss: 1.0701552629470825
Epoch 1230, training loss: 12.529114723205566 = 0.026058457791805267 + 2.0 * 6.251528263092041
Epoch 1230, val loss: 1.0744167566299438
Epoch 1240, training loss: 12.522722244262695 = 0.02534773014485836 + 2.0 * 6.248687267303467
Epoch 1240, val loss: 1.0786986351013184
Epoch 1250, training loss: 12.532926559448242 = 0.024665948003530502 + 2.0 * 6.2541303634643555
Epoch 1250, val loss: 1.0828863382339478
Epoch 1260, training loss: 12.525004386901855 = 0.024009674787521362 + 2.0 * 6.250497341156006
Epoch 1260, val loss: 1.0870521068572998
Epoch 1270, training loss: 12.523158073425293 = 0.02338607795536518 + 2.0 * 6.2498860359191895
Epoch 1270, val loss: 1.0912346839904785
Epoch 1280, training loss: 12.52497673034668 = 0.02278667315840721 + 2.0 * 6.251094818115234
Epoch 1280, val loss: 1.0952595472335815
Epoch 1290, training loss: 12.52116870880127 = 0.02220735140144825 + 2.0 * 6.249480724334717
Epoch 1290, val loss: 1.0993448495864868
Epoch 1300, training loss: 12.517034530639648 = 0.021656030789017677 + 2.0 * 6.247689247131348
Epoch 1300, val loss: 1.1033062934875488
Epoch 1310, training loss: 12.515359878540039 = 0.021120581775903702 + 2.0 * 6.247119426727295
Epoch 1310, val loss: 1.107254981994629
Epoch 1320, training loss: 12.518357276916504 = 0.020608382299542427 + 2.0 * 6.248874664306641
Epoch 1320, val loss: 1.111167550086975
Epoch 1330, training loss: 12.514180183410645 = 0.020112475380301476 + 2.0 * 6.247034072875977
Epoch 1330, val loss: 1.1149359941482544
Epoch 1340, training loss: 12.510255813598633 = 0.019634980708360672 + 2.0 * 6.245310306549072
Epoch 1340, val loss: 1.1187329292297363
Epoch 1350, training loss: 12.508394241333008 = 0.019177043810486794 + 2.0 * 6.244608402252197
Epoch 1350, val loss: 1.1225621700286865
Epoch 1360, training loss: 12.507027626037598 = 0.01873369887471199 + 2.0 * 6.244146823883057
Epoch 1360, val loss: 1.126258134841919
Epoch 1370, training loss: 12.527359962463379 = 0.018307041376829147 + 2.0 * 6.254526615142822
Epoch 1370, val loss: 1.1298675537109375
Epoch 1380, training loss: 12.514780044555664 = 0.017898984253406525 + 2.0 * 6.248440742492676
Epoch 1380, val loss: 1.1335011720657349
Epoch 1390, training loss: 12.503129959106445 = 0.017499620094895363 + 2.0 * 6.242815017700195
Epoch 1390, val loss: 1.1371703147888184
Epoch 1400, training loss: 12.503556251525879 = 0.01711837761104107 + 2.0 * 6.243218898773193
Epoch 1400, val loss: 1.1407777070999146
Epoch 1410, training loss: 12.517995834350586 = 0.016751259565353394 + 2.0 * 6.250622272491455
Epoch 1410, val loss: 1.1442534923553467
Epoch 1420, training loss: 12.505101203918457 = 0.016391171142458916 + 2.0 * 6.244355201721191
Epoch 1420, val loss: 1.1476095914840698
Epoch 1430, training loss: 12.499690055847168 = 0.016045838594436646 + 2.0 * 6.241822242736816
Epoch 1430, val loss: 1.1511874198913574
Epoch 1440, training loss: 12.497550964355469 = 0.015711097046732903 + 2.0 * 6.240920066833496
Epoch 1440, val loss: 1.1545857191085815
Epoch 1450, training loss: 12.516772270202637 = 0.01538727805018425 + 2.0 * 6.250692367553711
Epoch 1450, val loss: 1.157841682434082
Epoch 1460, training loss: 12.50007152557373 = 0.015074583701789379 + 2.0 * 6.242498397827148
Epoch 1460, val loss: 1.1612439155578613
Epoch 1470, training loss: 12.497889518737793 = 0.014772636815905571 + 2.0 * 6.24155855178833
Epoch 1470, val loss: 1.1646085977554321
Epoch 1480, training loss: 12.495161056518555 = 0.014479149132966995 + 2.0 * 6.2403411865234375
Epoch 1480, val loss: 1.1678379774093628
Epoch 1490, training loss: 12.492049217224121 = 0.014193614013493061 + 2.0 * 6.238927841186523
Epoch 1490, val loss: 1.1710978746414185
Epoch 1500, training loss: 12.49341106414795 = 0.013916385360062122 + 2.0 * 6.239747524261475
Epoch 1500, val loss: 1.1742749214172363
Epoch 1510, training loss: 12.502965927124023 = 0.01364764291793108 + 2.0 * 6.244658946990967
Epoch 1510, val loss: 1.1773372888565063
Epoch 1520, training loss: 12.500136375427246 = 0.013391325250267982 + 2.0 * 6.243372440338135
Epoch 1520, val loss: 1.1805812120437622
Epoch 1530, training loss: 12.489727973937988 = 0.013138734735548496 + 2.0 * 6.23829460144043
Epoch 1530, val loss: 1.1836820840835571
Epoch 1540, training loss: 12.488713264465332 = 0.012895628809928894 + 2.0 * 6.237908840179443
Epoch 1540, val loss: 1.1867954730987549
Epoch 1550, training loss: 12.49025821685791 = 0.012659963220357895 + 2.0 * 6.238799095153809
Epoch 1550, val loss: 1.189868450164795
Epoch 1560, training loss: 12.493972778320312 = 0.01243074331432581 + 2.0 * 6.2407708168029785
Epoch 1560, val loss: 1.1927484273910522
Epoch 1570, training loss: 12.486795425415039 = 0.012205680832266808 + 2.0 * 6.237294673919678
Epoch 1570, val loss: 1.1957321166992188
Epoch 1580, training loss: 12.484745979309082 = 0.011990519240498543 + 2.0 * 6.236377716064453
Epoch 1580, val loss: 1.1987473964691162
Epoch 1590, training loss: 12.484384536743164 = 0.011778822168707848 + 2.0 * 6.236302852630615
Epoch 1590, val loss: 1.2016592025756836
Epoch 1600, training loss: 12.504133224487305 = 0.011573540978133678 + 2.0 * 6.246279716491699
Epoch 1600, val loss: 1.2044315338134766
Epoch 1610, training loss: 12.498868942260742 = 0.011375287547707558 + 2.0 * 6.243746757507324
Epoch 1610, val loss: 1.2073240280151367
Epoch 1620, training loss: 12.483399391174316 = 0.011181844398379326 + 2.0 * 6.236108779907227
Epoch 1620, val loss: 1.2102330923080444
Epoch 1630, training loss: 12.482748031616211 = 0.010994719341397285 + 2.0 * 6.235876560211182
Epoch 1630, val loss: 1.2130470275878906
Epoch 1640, training loss: 12.480133056640625 = 0.010811845771968365 + 2.0 * 6.234660625457764
Epoch 1640, val loss: 1.2158414125442505
Epoch 1650, training loss: 12.481531143188477 = 0.010633355006575584 + 2.0 * 6.235448837280273
Epoch 1650, val loss: 1.218596339225769
Epoch 1660, training loss: 12.489749908447266 = 0.010459098033607006 + 2.0 * 6.239645481109619
Epoch 1660, val loss: 1.2212369441986084
Epoch 1670, training loss: 12.484040260314941 = 0.01029079407453537 + 2.0 * 6.236874580383301
Epoch 1670, val loss: 1.2239785194396973
Epoch 1680, training loss: 12.482024192810059 = 0.010125621221959591 + 2.0 * 6.235949516296387
Epoch 1680, val loss: 1.226685643196106
Epoch 1690, training loss: 12.490647315979004 = 0.009964815340936184 + 2.0 * 6.2403411865234375
Epoch 1690, val loss: 1.2292207479476929
Epoch 1700, training loss: 12.481807708740234 = 0.00980930496007204 + 2.0 * 6.23599910736084
Epoch 1700, val loss: 1.2317967414855957
Epoch 1710, training loss: 12.477782249450684 = 0.00965863186866045 + 2.0 * 6.2340617179870605
Epoch 1710, val loss: 1.2345105409622192
Epoch 1720, training loss: 12.47459888458252 = 0.009510238654911518 + 2.0 * 6.232544422149658
Epoch 1720, val loss: 1.2370249032974243
Epoch 1730, training loss: 12.474116325378418 = 0.009364771656692028 + 2.0 * 6.232375621795654
Epoch 1730, val loss: 1.2395955324172974
Epoch 1740, training loss: 12.47657299041748 = 0.009222449734807014 + 2.0 * 6.233675479888916
Epoch 1740, val loss: 1.2420952320098877
Epoch 1750, training loss: 12.481379508972168 = 0.009083746932446957 + 2.0 * 6.236147880554199
Epoch 1750, val loss: 1.2445088624954224
Epoch 1760, training loss: 12.487025260925293 = 0.008950021117925644 + 2.0 * 6.23903751373291
Epoch 1760, val loss: 1.247021198272705
Epoch 1770, training loss: 12.477198600769043 = 0.008818035945296288 + 2.0 * 6.234190464019775
Epoch 1770, val loss: 1.2494242191314697
Epoch 1780, training loss: 12.472649574279785 = 0.008691689930856228 + 2.0 * 6.231978893280029
Epoch 1780, val loss: 1.2519036531448364
Epoch 1790, training loss: 12.473176956176758 = 0.00856617372483015 + 2.0 * 6.232305526733398
Epoch 1790, val loss: 1.2542967796325684
Epoch 1800, training loss: 12.475188255310059 = 0.00844405498355627 + 2.0 * 6.233372211456299
Epoch 1800, val loss: 1.2566362619400024
Epoch 1810, training loss: 12.474781036376953 = 0.008324895054101944 + 2.0 * 6.2332282066345215
Epoch 1810, val loss: 1.2590092420578003
Epoch 1820, training loss: 12.47187614440918 = 0.008207949809730053 + 2.0 * 6.2318339347839355
Epoch 1820, val loss: 1.2613154649734497
Epoch 1830, training loss: 12.473800659179688 = 0.008094645105302334 + 2.0 * 6.232852935791016
Epoch 1830, val loss: 1.2636326551437378
Epoch 1840, training loss: 12.4696683883667 = 0.007983941584825516 + 2.0 * 6.230842113494873
Epoch 1840, val loss: 1.2658532857894897
Epoch 1850, training loss: 12.469983100891113 = 0.007874267175793648 + 2.0 * 6.231054306030273
Epoch 1850, val loss: 1.2681485414505005
Epoch 1860, training loss: 12.480464935302734 = 0.007769497577100992 + 2.0 * 6.236347675323486
Epoch 1860, val loss: 1.2704365253448486
Epoch 1870, training loss: 12.469461441040039 = 0.007664247881621122 + 2.0 * 6.230898380279541
Epoch 1870, val loss: 1.272376298904419
Epoch 1880, training loss: 12.467228889465332 = 0.007563862483948469 + 2.0 * 6.229832649230957
Epoch 1880, val loss: 1.2746853828430176
Epoch 1890, training loss: 12.465225219726562 = 0.0074644070118665695 + 2.0 * 6.228880405426025
Epoch 1890, val loss: 1.2768654823303223
Epoch 1900, training loss: 12.464624404907227 = 0.007367422804236412 + 2.0 * 6.228628635406494
Epoch 1900, val loss: 1.2789981365203857
Epoch 1910, training loss: 12.48255729675293 = 0.007271470036357641 + 2.0 * 6.237642765045166
Epoch 1910, val loss: 1.2810304164886475
Epoch 1920, training loss: 12.469922065734863 = 0.007178886793553829 + 2.0 * 6.2313714027404785
Epoch 1920, val loss: 1.2830734252929688
Epoch 1930, training loss: 12.465993881225586 = 0.00708747049793601 + 2.0 * 6.229453086853027
Epoch 1930, val loss: 1.2851953506469727
Epoch 1940, training loss: 12.465537071228027 = 0.006998879369348288 + 2.0 * 6.229269027709961
Epoch 1940, val loss: 1.2872782945632935
Epoch 1950, training loss: 12.468856811523438 = 0.006911309435963631 + 2.0 * 6.230972766876221
Epoch 1950, val loss: 1.2891913652420044
Epoch 1960, training loss: 12.46318531036377 = 0.00682586245238781 + 2.0 * 6.228179931640625
Epoch 1960, val loss: 1.2912237644195557
Epoch 1970, training loss: 12.469842910766602 = 0.006741802208125591 + 2.0 * 6.231550693511963
Epoch 1970, val loss: 1.2931534051895142
Epoch 1980, training loss: 12.46478271484375 = 0.006659788079559803 + 2.0 * 6.229061603546143
Epoch 1980, val loss: 1.2951370477676392
Epoch 1990, training loss: 12.465768814086914 = 0.006579686887562275 + 2.0 * 6.229594707489014
Epoch 1990, val loss: 1.297066569328308
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7592592592592593
0.8144438587243016
=== training gcn model ===
Epoch 0, training loss: 19.12383270263672 = 1.9300990104675293 + 2.0 * 8.596866607666016
Epoch 0, val loss: 1.9281752109527588
Epoch 10, training loss: 19.113971710205078 = 1.9208124876022339 + 2.0 * 8.596579551696777
Epoch 10, val loss: 1.9182476997375488
Epoch 20, training loss: 19.096712112426758 = 1.9090980291366577 + 2.0 * 8.593807220458984
Epoch 20, val loss: 1.9055453538894653
Epoch 30, training loss: 19.03368377685547 = 1.8932827711105347 + 2.0 * 8.57020092010498
Epoch 30, val loss: 1.8881001472473145
Epoch 40, training loss: 18.711322784423828 = 1.874113917350769 + 2.0 * 8.418604850769043
Epoch 40, val loss: 1.8677629232406616
Epoch 50, training loss: 17.663557052612305 = 1.8543721437454224 + 2.0 * 7.904592514038086
Epoch 50, val loss: 1.8471157550811768
Epoch 60, training loss: 16.8651123046875 = 1.8377203941345215 + 2.0 * 7.513696193695068
Epoch 60, val loss: 1.8320058584213257
Epoch 70, training loss: 16.202226638793945 = 1.8236414194107056 + 2.0 * 7.189292907714844
Epoch 70, val loss: 1.8190648555755615
Epoch 80, training loss: 15.78498363494873 = 1.8113985061645508 + 2.0 * 6.98679256439209
Epoch 80, val loss: 1.8078705072402954
Epoch 90, training loss: 15.516510009765625 = 1.7988333702087402 + 2.0 * 6.858838081359863
Epoch 90, val loss: 1.7961785793304443
Epoch 100, training loss: 15.307114601135254 = 1.7859464883804321 + 2.0 * 6.760583877563477
Epoch 100, val loss: 1.7845077514648438
Epoch 110, training loss: 15.124544143676758 = 1.773819923400879 + 2.0 * 6.6753621101379395
Epoch 110, val loss: 1.7740099430084229
Epoch 120, training loss: 14.99528694152832 = 1.762084722518921 + 2.0 * 6.61660099029541
Epoch 120, val loss: 1.7636533975601196
Epoch 130, training loss: 14.903480529785156 = 1.7494523525238037 + 2.0 * 6.577013969421387
Epoch 130, val loss: 1.7526742219924927
Epoch 140, training loss: 14.829754829406738 = 1.735329270362854 + 2.0 * 6.547212600708008
Epoch 140, val loss: 1.740716576576233
Epoch 150, training loss: 14.76695442199707 = 1.7195837497711182 + 2.0 * 6.523685455322266
Epoch 150, val loss: 1.727550745010376
Epoch 160, training loss: 14.711341857910156 = 1.7022626399993896 + 2.0 * 6.504539489746094
Epoch 160, val loss: 1.7132537364959717
Epoch 170, training loss: 14.650299072265625 = 1.6831048727035522 + 2.0 * 6.483597278594971
Epoch 170, val loss: 1.6976070404052734
Epoch 180, training loss: 14.610074996948242 = 1.6615686416625977 + 2.0 * 6.474253177642822
Epoch 180, val loss: 1.6801375150680542
Epoch 190, training loss: 14.546476364135742 = 1.6376960277557373 + 2.0 * 6.454390048980713
Epoch 190, val loss: 1.6607789993286133
Epoch 200, training loss: 14.492244720458984 = 1.611157774925232 + 2.0 * 6.4405436515808105
Epoch 200, val loss: 1.639303207397461
Epoch 210, training loss: 14.43929672241211 = 1.5814944505691528 + 2.0 * 6.428901195526123
Epoch 210, val loss: 1.6153799295425415
Epoch 220, training loss: 14.390961647033691 = 1.5487502813339233 + 2.0 * 6.421105861663818
Epoch 220, val loss: 1.5892339944839478
Epoch 230, training loss: 14.336687088012695 = 1.5131009817123413 + 2.0 * 6.411793231964111
Epoch 230, val loss: 1.5609849691390991
Epoch 240, training loss: 14.281112670898438 = 1.4746146202087402 + 2.0 * 6.4032487869262695
Epoch 240, val loss: 1.5305323600769043
Epoch 250, training loss: 14.233343124389648 = 1.43330717086792 + 2.0 * 6.400017738342285
Epoch 250, val loss: 1.4980822801589966
Epoch 260, training loss: 14.1795015335083 = 1.3895765542984009 + 2.0 * 6.394962310791016
Epoch 260, val loss: 1.4642802476882935
Epoch 270, training loss: 14.118074417114258 = 1.344228744506836 + 2.0 * 6.386922836303711
Epoch 270, val loss: 1.4293971061706543
Epoch 280, training loss: 14.059083938598633 = 1.2973650693893433 + 2.0 * 6.380859375
Epoch 280, val loss: 1.3937993049621582
Epoch 290, training loss: 14.001571655273438 = 1.2494800090789795 + 2.0 * 6.3760457038879395
Epoch 290, val loss: 1.3579614162445068
Epoch 300, training loss: 13.948004722595215 = 1.2011919021606445 + 2.0 * 6.373406410217285
Epoch 300, val loss: 1.3224549293518066
Epoch 310, training loss: 13.895849227905273 = 1.1536362171173096 + 2.0 * 6.3711066246032715
Epoch 310, val loss: 1.2882614135742188
Epoch 320, training loss: 13.836004257202148 = 1.1077746152877808 + 2.0 * 6.364114761352539
Epoch 320, val loss: 1.2557460069656372
Epoch 330, training loss: 13.785911560058594 = 1.0636544227600098 + 2.0 * 6.361128807067871
Epoch 330, val loss: 1.2252264022827148
Epoch 340, training loss: 13.759586334228516 = 1.0217125415802002 + 2.0 * 6.368937015533447
Epoch 340, val loss: 1.1969496011734009
Epoch 350, training loss: 13.694120407104492 = 0.982583224773407 + 2.0 * 6.35576868057251
Epoch 350, val loss: 1.1713767051696777
Epoch 360, training loss: 13.64840316772461 = 0.9462913274765015 + 2.0 * 6.351056098937988
Epoch 360, val loss: 1.1483420133590698
Epoch 370, training loss: 13.608217239379883 = 0.9124982357025146 + 2.0 * 6.3478593826293945
Epoch 370, val loss: 1.1276755332946777
Epoch 380, training loss: 13.574200630187988 = 0.8810771107673645 + 2.0 * 6.346561908721924
Epoch 380, val loss: 1.1091930866241455
Epoch 390, training loss: 13.544979095458984 = 0.8520309925079346 + 2.0 * 6.3464741706848145
Epoch 390, val loss: 1.0928996801376343
Epoch 400, training loss: 13.505894660949707 = 0.8251772522926331 + 2.0 * 6.340358734130859
Epoch 400, val loss: 1.0784350633621216
Epoch 410, training loss: 13.474810600280762 = 0.8000683188438416 + 2.0 * 6.337371349334717
Epoch 410, val loss: 1.065582036972046
Epoch 420, training loss: 13.46862506866455 = 0.7763821482658386 + 2.0 * 6.346121311187744
Epoch 420, val loss: 1.054034948348999
Epoch 430, training loss: 13.420564651489258 = 0.7542170286178589 + 2.0 * 6.333173751831055
Epoch 430, val loss: 1.0435477495193481
Epoch 440, training loss: 13.395102500915527 = 0.7331613898277283 + 2.0 * 6.330970764160156
Epoch 440, val loss: 1.0341414213180542
Epoch 450, training loss: 13.370182991027832 = 0.7128542065620422 + 2.0 * 6.328664302825928
Epoch 450, val loss: 1.0253407955169678
Epoch 460, training loss: 13.34529972076416 = 0.6929387450218201 + 2.0 * 6.326180458068848
Epoch 460, val loss: 1.017017126083374
Epoch 470, training loss: 13.324336051940918 = 0.6731972098350525 + 2.0 * 6.3255696296691895
Epoch 470, val loss: 1.0089908838272095
Epoch 480, training loss: 13.303791999816895 = 0.6536524891853333 + 2.0 * 6.325069904327393
Epoch 480, val loss: 1.0012195110321045
Epoch 490, training loss: 13.277950286865234 = 0.6342005729675293 + 2.0 * 6.321874618530273
Epoch 490, val loss: 0.993644654750824
Epoch 500, training loss: 13.253249168395996 = 0.6147903203964233 + 2.0 * 6.319229602813721
Epoch 500, val loss: 0.9863645434379578
Epoch 510, training loss: 13.239394187927246 = 0.5953065156936646 + 2.0 * 6.3220438957214355
Epoch 510, val loss: 0.9793703556060791
Epoch 520, training loss: 13.215128898620605 = 0.575996994972229 + 2.0 * 6.319565773010254
Epoch 520, val loss: 0.9726282358169556
Epoch 530, training loss: 13.18777847290039 = 0.556769073009491 + 2.0 * 6.315504550933838
Epoch 530, val loss: 0.9663134813308716
Epoch 540, training loss: 13.163833618164062 = 0.5375573039054871 + 2.0 * 6.313138008117676
Epoch 540, val loss: 0.960426926612854
Epoch 550, training loss: 13.14031982421875 = 0.5183072090148926 + 2.0 * 6.31100606918335
Epoch 550, val loss: 0.9550032019615173
Epoch 560, training loss: 13.11883544921875 = 0.4990220367908478 + 2.0 * 6.309906482696533
Epoch 560, val loss: 0.9500898718833923
Epoch 570, training loss: 13.100652694702148 = 0.47991400957107544 + 2.0 * 6.310369491577148
Epoch 570, val loss: 0.9456801414489746
Epoch 580, training loss: 13.075740814208984 = 0.46117833256721497 + 2.0 * 6.307281017303467
Epoch 580, val loss: 0.941910445690155
Epoch 590, training loss: 13.053874969482422 = 0.4426797926425934 + 2.0 * 6.30559778213501
Epoch 590, val loss: 0.9388821125030518
Epoch 600, training loss: 13.0325345993042 = 0.42445656657218933 + 2.0 * 6.304039001464844
Epoch 600, val loss: 0.9364367723464966
Epoch 610, training loss: 13.027194023132324 = 0.40650755167007446 + 2.0 * 6.310343265533447
Epoch 610, val loss: 0.9346409440040588
Epoch 620, training loss: 13.005410194396973 = 0.3891724944114685 + 2.0 * 6.30811882019043
Epoch 620, val loss: 0.9332659244537354
Epoch 630, training loss: 12.97693920135498 = 0.37225842475891113 + 2.0 * 6.302340507507324
Epoch 630, val loss: 0.9326871633529663
Epoch 640, training loss: 12.95442008972168 = 0.35583093762397766 + 2.0 * 6.299294471740723
Epoch 640, val loss: 0.9327111840248108
Epoch 650, training loss: 12.938679695129395 = 0.33986392617225647 + 2.0 * 6.299407958984375
Epoch 650, val loss: 0.9332600235939026
Epoch 660, training loss: 12.920321464538574 = 0.32441800832748413 + 2.0 * 6.297951698303223
Epoch 660, val loss: 0.9343440532684326
Epoch 670, training loss: 12.914525032043457 = 0.30955058336257935 + 2.0 * 6.302487373352051
Epoch 670, val loss: 0.9358416199684143
Epoch 680, training loss: 12.889533042907715 = 0.29525309801101685 + 2.0 * 6.297140121459961
Epoch 680, val loss: 0.937886118888855
Epoch 690, training loss: 12.869366645812988 = 0.28145894408226013 + 2.0 * 6.293953895568848
Epoch 690, val loss: 0.9403467178344727
Epoch 700, training loss: 12.85389518737793 = 0.2681652903556824 + 2.0 * 6.292864799499512
Epoch 700, val loss: 0.9432835578918457
Epoch 710, training loss: 12.860976219177246 = 0.25538891553878784 + 2.0 * 6.302793502807617
Epoch 710, val loss: 0.9464758038520813
Epoch 720, training loss: 12.835853576660156 = 0.2432873398065567 + 2.0 * 6.29628324508667
Epoch 720, val loss: 0.9499539136886597
Epoch 730, training loss: 12.81505298614502 = 0.23171240091323853 + 2.0 * 6.291670322418213
Epoch 730, val loss: 0.9539299607276917
Epoch 740, training loss: 12.798657417297363 = 0.22061093151569366 + 2.0 * 6.289023399353027
Epoch 740, val loss: 0.9583579897880554
Epoch 750, training loss: 12.784564971923828 = 0.20998473465442657 + 2.0 * 6.287290096282959
Epoch 750, val loss: 0.9630371928215027
Epoch 760, training loss: 12.773934364318848 = 0.19983088970184326 + 2.0 * 6.287051677703857
Epoch 760, val loss: 0.9681074619293213
Epoch 770, training loss: 12.774494171142578 = 0.19018657505512238 + 2.0 * 6.292153835296631
Epoch 770, val loss: 0.9733876585960388
Epoch 780, training loss: 12.757636070251465 = 0.18112242221832275 + 2.0 * 6.288256645202637
Epoch 780, val loss: 0.97871333360672
Epoch 790, training loss: 12.741778373718262 = 0.1725011020898819 + 2.0 * 6.284638404846191
Epoch 790, val loss: 0.9845583438873291
Epoch 800, training loss: 12.730411529541016 = 0.1643228530883789 + 2.0 * 6.283044338226318
Epoch 800, val loss: 0.9906140565872192
Epoch 810, training loss: 12.720708847045898 = 0.1565430462360382 + 2.0 * 6.282083034515381
Epoch 810, val loss: 0.9968502521514893
Epoch 820, training loss: 12.729586601257324 = 0.14915810525417328 + 2.0 * 6.2902140617370605
Epoch 820, val loss: 1.0032204389572144
Epoch 830, training loss: 12.719088554382324 = 0.1421712338924408 + 2.0 * 6.288458824157715
Epoch 830, val loss: 1.0096356868743896
Epoch 840, training loss: 12.696415901184082 = 0.13561508059501648 + 2.0 * 6.280400276184082
Epoch 840, val loss: 1.0162172317504883
Epoch 850, training loss: 12.687278747558594 = 0.12939152121543884 + 2.0 * 6.2789435386657715
Epoch 850, val loss: 1.0229690074920654
Epoch 860, training loss: 12.679941177368164 = 0.12348811328411102 + 2.0 * 6.278226375579834
Epoch 860, val loss: 1.0297168493270874
Epoch 870, training loss: 12.696097373962402 = 0.11789364367723465 + 2.0 * 6.289102077484131
Epoch 870, val loss: 1.0366387367248535
Epoch 880, training loss: 12.672821044921875 = 0.11264682561159134 + 2.0 * 6.280086994171143
Epoch 880, val loss: 1.0433858633041382
Epoch 890, training loss: 12.667203903198242 = 0.10767987370491028 + 2.0 * 6.279761791229248
Epoch 890, val loss: 1.0504127740859985
Epoch 900, training loss: 12.653322219848633 = 0.10297948122024536 + 2.0 * 6.275171279907227
Epoch 900, val loss: 1.057403802871704
Epoch 910, training loss: 12.647102355957031 = 0.0985439270734787 + 2.0 * 6.2742791175842285
Epoch 910, val loss: 1.0644665956497192
Epoch 920, training loss: 12.644103050231934 = 0.09434575587511063 + 2.0 * 6.27487850189209
Epoch 920, val loss: 1.0716891288757324
Epoch 930, training loss: 12.649665832519531 = 0.09037601202726364 + 2.0 * 6.279644966125488
Epoch 930, val loss: 1.0787134170532227
Epoch 940, training loss: 12.632929801940918 = 0.08660636842250824 + 2.0 * 6.273161888122559
Epoch 940, val loss: 1.085792899131775
Epoch 950, training loss: 12.626751899719238 = 0.08305763453245163 + 2.0 * 6.271847248077393
Epoch 950, val loss: 1.0929826498031616
Epoch 960, training loss: 12.622389793395996 = 0.07968159019947052 + 2.0 * 6.2713541984558105
Epoch 960, val loss: 1.100176453590393
Epoch 970, training loss: 12.629396438598633 = 0.07647913694381714 + 2.0 * 6.276458740234375
Epoch 970, val loss: 1.1072999238967896
Epoch 980, training loss: 12.613102912902832 = 0.07346087694168091 + 2.0 * 6.2698211669921875
Epoch 980, val loss: 1.114261507987976
Epoch 990, training loss: 12.607670783996582 = 0.07060053199529648 + 2.0 * 6.268535137176514
Epoch 990, val loss: 1.1214466094970703
Epoch 1000, training loss: 12.608773231506348 = 0.06788313388824463 + 2.0 * 6.270444869995117
Epoch 1000, val loss: 1.1286065578460693
Epoch 1010, training loss: 12.614594459533691 = 0.06530509144067764 + 2.0 * 6.27464485168457
Epoch 1010, val loss: 1.135527491569519
Epoch 1020, training loss: 12.599032402038574 = 0.06285959482192993 + 2.0 * 6.2680864334106445
Epoch 1020, val loss: 1.1423780918121338
Epoch 1030, training loss: 12.59355354309082 = 0.06054669991135597 + 2.0 * 6.26650333404541
Epoch 1030, val loss: 1.1494982242584229
Epoch 1040, training loss: 12.591789245605469 = 0.05834398791193962 + 2.0 * 6.266722679138184
Epoch 1040, val loss: 1.1564449071884155
Epoch 1050, training loss: 12.59526538848877 = 0.056243930011987686 + 2.0 * 6.269510746002197
Epoch 1050, val loss: 1.1632888317108154
Epoch 1060, training loss: 12.588377952575684 = 0.054257676005363464 + 2.0 * 6.267060279846191
Epoch 1060, val loss: 1.1699082851409912
Epoch 1070, training loss: 12.585150718688965 = 0.05236075073480606 + 2.0 * 6.266395092010498
Epoch 1070, val loss: 1.176674723625183
Epoch 1080, training loss: 12.576906204223633 = 0.05056381598114967 + 2.0 * 6.263171195983887
Epoch 1080, val loss: 1.1833552122116089
Epoch 1090, training loss: 12.574710845947266 = 0.04884377494454384 + 2.0 * 6.262933731079102
Epoch 1090, val loss: 1.1900241374969482
Epoch 1100, training loss: 12.587316513061523 = 0.04719974100589752 + 2.0 * 6.2700581550598145
Epoch 1100, val loss: 1.1965887546539307
Epoch 1110, training loss: 12.571337699890137 = 0.04565088450908661 + 2.0 * 6.262843608856201
Epoch 1110, val loss: 1.2028892040252686
Epoch 1120, training loss: 12.566049575805664 = 0.04416468366980553 + 2.0 * 6.260942459106445
Epoch 1120, val loss: 1.2093015909194946
Epoch 1130, training loss: 12.56164264678955 = 0.04274672269821167 + 2.0 * 6.259448051452637
Epoch 1130, val loss: 1.2158130407333374
Epoch 1140, training loss: 12.564994812011719 = 0.041390348225831985 + 2.0 * 6.2618021965026855
Epoch 1140, val loss: 1.2221734523773193
Epoch 1150, training loss: 12.557765007019043 = 0.04008860886096954 + 2.0 * 6.258838176727295
Epoch 1150, val loss: 1.2281968593597412
Epoch 1160, training loss: 12.559470176696777 = 0.038858119398355484 + 2.0 * 6.260305881500244
Epoch 1160, val loss: 1.2341541051864624
Epoch 1170, training loss: 12.554250717163086 = 0.03767438605427742 + 2.0 * 6.258288383483887
Epoch 1170, val loss: 1.2403476238250732
Epoch 1180, training loss: 12.563199043273926 = 0.036542363464832306 + 2.0 * 6.263328552246094
Epoch 1180, val loss: 1.2463911771774292
Epoch 1190, training loss: 12.550546646118164 = 0.03546136990189552 + 2.0 * 6.257542610168457
Epoch 1190, val loss: 1.2522729635238647
Epoch 1200, training loss: 12.546957015991211 = 0.03442138805985451 + 2.0 * 6.25626802444458
Epoch 1200, val loss: 1.2582054138183594
Epoch 1210, training loss: 12.568070411682129 = 0.033423881977796555 + 2.0 * 6.2673234939575195
Epoch 1210, val loss: 1.2640752792358398
Epoch 1220, training loss: 12.545968055725098 = 0.032486964017152786 + 2.0 * 6.256740570068359
Epoch 1220, val loss: 1.2693814039230347
Epoch 1230, training loss: 12.547016143798828 = 0.0315815731883049 + 2.0 * 6.257717132568359
Epoch 1230, val loss: 1.2750236988067627
Epoch 1240, training loss: 12.540292739868164 = 0.030715476721525192 + 2.0 * 6.254788398742676
Epoch 1240, val loss: 1.2808384895324707
Epoch 1250, training loss: 12.536599159240723 = 0.029878200963139534 + 2.0 * 6.253360271453857
Epoch 1250, val loss: 1.2864488363265991
Epoch 1260, training loss: 12.534883499145508 = 0.02907094918191433 + 2.0 * 6.252906322479248
Epoch 1260, val loss: 1.2920303344726562
Epoch 1270, training loss: 12.565569877624512 = 0.02829337678849697 + 2.0 * 6.2686381340026855
Epoch 1270, val loss: 1.2974478006362915
Epoch 1280, training loss: 12.548601150512695 = 0.02755538560450077 + 2.0 * 6.260522842407227
Epoch 1280, val loss: 1.3025349378585815
Epoch 1290, training loss: 12.533721923828125 = 0.026846706867218018 + 2.0 * 6.253437519073486
Epoch 1290, val loss: 1.3077352046966553
Epoch 1300, training loss: 12.528594017028809 = 0.026161883026361465 + 2.0 * 6.251215934753418
Epoch 1300, val loss: 1.313181757926941
Epoch 1310, training loss: 12.5277738571167 = 0.025502000004053116 + 2.0 * 6.25113582611084
Epoch 1310, val loss: 1.3184627294540405
Epoch 1320, training loss: 12.528520584106445 = 0.024861911311745644 + 2.0 * 6.251829147338867
Epoch 1320, val loss: 1.3236229419708252
Epoch 1330, training loss: 12.540425300598145 = 0.024246012791991234 + 2.0 * 6.258089542388916
Epoch 1330, val loss: 1.3284938335418701
Epoch 1340, training loss: 12.523533821105957 = 0.023660995066165924 + 2.0 * 6.249936580657959
Epoch 1340, val loss: 1.3333898782730103
Epoch 1350, training loss: 12.52257251739502 = 0.023094983771443367 + 2.0 * 6.249738693237305
Epoch 1350, val loss: 1.338577151298523
Epoch 1360, training loss: 12.521007537841797 = 0.022545836865901947 + 2.0 * 6.249230861663818
Epoch 1360, val loss: 1.343554973602295
Epoch 1370, training loss: 12.527047157287598 = 0.02201864682137966 + 2.0 * 6.252514362335205
Epoch 1370, val loss: 1.3485206365585327
Epoch 1380, training loss: 12.525178909301758 = 0.021505296230316162 + 2.0 * 6.251836776733398
Epoch 1380, val loss: 1.353067398071289
Epoch 1390, training loss: 12.518336296081543 = 0.021014370024204254 + 2.0 * 6.248661041259766
Epoch 1390, val loss: 1.3577502965927124
Epoch 1400, training loss: 12.517471313476562 = 0.020538968965411186 + 2.0 * 6.2484660148620605
Epoch 1400, val loss: 1.3625538349151611
Epoch 1410, training loss: 12.514646530151367 = 0.02008267678320408 + 2.0 * 6.247282028198242
Epoch 1410, val loss: 1.3672910928726196
Epoch 1420, training loss: 12.513760566711426 = 0.019636770710349083 + 2.0 * 6.247061729431152
Epoch 1420, val loss: 1.3719459772109985
Epoch 1430, training loss: 12.530951499938965 = 0.01920907385647297 + 2.0 * 6.255871295928955
Epoch 1430, val loss: 1.3764415979385376
Epoch 1440, training loss: 12.514084815979004 = 0.01879330351948738 + 2.0 * 6.247645854949951
Epoch 1440, val loss: 1.3807493448257446
Epoch 1450, training loss: 12.52357006072998 = 0.01839466392993927 + 2.0 * 6.252587795257568
Epoch 1450, val loss: 1.3852794170379639
Epoch 1460, training loss: 12.510766983032227 = 0.0180073082447052 + 2.0 * 6.246379852294922
Epoch 1460, val loss: 1.3895299434661865
Epoch 1470, training loss: 12.509178161621094 = 0.017631489783525467 + 2.0 * 6.2457733154296875
Epoch 1470, val loss: 1.3938874006271362
Epoch 1480, training loss: 12.506291389465332 = 0.017268773168325424 + 2.0 * 6.244511127471924
Epoch 1480, val loss: 1.3982601165771484
Epoch 1490, training loss: 12.513216972351074 = 0.016915613785386086 + 2.0 * 6.248150825500488
Epoch 1490, val loss: 1.4025378227233887
Epoch 1500, training loss: 12.508733749389648 = 0.016574379056692123 + 2.0 * 6.246079921722412
Epoch 1500, val loss: 1.4065728187561035
Epoch 1510, training loss: 12.506586074829102 = 0.016243694350123405 + 2.0 * 6.245171070098877
Epoch 1510, val loss: 1.4106099605560303
Epoch 1520, training loss: 12.505030632019043 = 0.01592669077217579 + 2.0 * 6.244552135467529
Epoch 1520, val loss: 1.414771318435669
Epoch 1530, training loss: 12.51032543182373 = 0.015616039745509624 + 2.0 * 6.247354507446289
Epoch 1530, val loss: 1.4189099073410034
Epoch 1540, training loss: 12.50454044342041 = 0.01531493104994297 + 2.0 * 6.244612693786621
Epoch 1540, val loss: 1.4228460788726807
Epoch 1550, training loss: 12.502568244934082 = 0.015022755600512028 + 2.0 * 6.243772506713867
Epoch 1550, val loss: 1.426745891571045
Epoch 1560, training loss: 12.518006324768066 = 0.014740724116563797 + 2.0 * 6.2516326904296875
Epoch 1560, val loss: 1.430612325668335
Epoch 1570, training loss: 12.501847267150879 = 0.014468704350292683 + 2.0 * 6.243689060211182
Epoch 1570, val loss: 1.4344102144241333
Epoch 1580, training loss: 12.497340202331543 = 0.014202636666595936 + 2.0 * 6.241568565368652
Epoch 1580, val loss: 1.4383304119110107
Epoch 1590, training loss: 12.496742248535156 = 0.013943532481789589 + 2.0 * 6.24139928817749
Epoch 1590, val loss: 1.4422402381896973
Epoch 1600, training loss: 12.508803367614746 = 0.013693721033632755 + 2.0 * 6.247554779052734
Epoch 1600, val loss: 1.4459666013717651
Epoch 1610, training loss: 12.498125076293945 = 0.013445676304399967 + 2.0 * 6.242339611053467
Epoch 1610, val loss: 1.449434757232666
Epoch 1620, training loss: 12.496293067932129 = 0.013207580894231796 + 2.0 * 6.241542816162109
Epoch 1620, val loss: 1.4531495571136475
Epoch 1630, training loss: 12.498058319091797 = 0.012976403348147869 + 2.0 * 6.2425408363342285
Epoch 1630, val loss: 1.4567939043045044
Epoch 1640, training loss: 12.491265296936035 = 0.012752785347402096 + 2.0 * 6.239256381988525
Epoch 1640, val loss: 1.460354208946228
Epoch 1650, training loss: 12.492725372314453 = 0.012534207664430141 + 2.0 * 6.240095615386963
Epoch 1650, val loss: 1.4640333652496338
Epoch 1660, training loss: 12.498674392700195 = 0.012320770882070065 + 2.0 * 6.2431769371032715
Epoch 1660, val loss: 1.4674897193908691
Epoch 1670, training loss: 12.49588680267334 = 0.012111525982618332 + 2.0 * 6.24188756942749
Epoch 1670, val loss: 1.4708210229873657
Epoch 1680, training loss: 12.49312686920166 = 0.011909198947250843 + 2.0 * 6.2406086921691895
Epoch 1680, val loss: 1.4742255210876465
Epoch 1690, training loss: 12.500415802001953 = 0.011714178137481213 + 2.0 * 6.244350910186768
Epoch 1690, val loss: 1.4776142835617065
Epoch 1700, training loss: 12.48940372467041 = 0.011524577625095844 + 2.0 * 6.2389397621154785
Epoch 1700, val loss: 1.48077392578125
Epoch 1710, training loss: 12.488286972045898 = 0.01134102139621973 + 2.0 * 6.238472938537598
Epoch 1710, val loss: 1.4841396808624268
Epoch 1720, training loss: 12.486123085021973 = 0.011158585548400879 + 2.0 * 6.237482070922852
Epoch 1720, val loss: 1.4875710010528564
Epoch 1730, training loss: 12.485769271850586 = 0.010981490835547447 + 2.0 * 6.237393856048584
Epoch 1730, val loss: 1.4908571243286133
Epoch 1740, training loss: 12.502201080322266 = 0.010808957740664482 + 2.0 * 6.245696067810059
Epoch 1740, val loss: 1.4940108060836792
Epoch 1750, training loss: 12.490660667419434 = 0.010641245171427727 + 2.0 * 6.240009784698486
Epoch 1750, val loss: 1.4970910549163818
Epoch 1760, training loss: 12.491849899291992 = 0.010477928444743156 + 2.0 * 6.240685939788818
Epoch 1760, val loss: 1.5002186298370361
Epoch 1770, training loss: 12.486257553100586 = 0.010319198481738567 + 2.0 * 6.237969398498535
Epoch 1770, val loss: 1.5032767057418823
Epoch 1780, training loss: 12.483738899230957 = 0.010162213817238808 + 2.0 * 6.236788272857666
Epoch 1780, val loss: 1.5064407587051392
Epoch 1790, training loss: 12.482912063598633 = 0.010009659454226494 + 2.0 * 6.236451148986816
Epoch 1790, val loss: 1.5095432996749878
Epoch 1800, training loss: 12.482702255249023 = 0.009860055521130562 + 2.0 * 6.23642110824585
Epoch 1800, val loss: 1.512609839439392
Epoch 1810, training loss: 12.488478660583496 = 0.00971459224820137 + 2.0 * 6.239382266998291
Epoch 1810, val loss: 1.5155832767486572
Epoch 1820, training loss: 12.490564346313477 = 0.00957372784614563 + 2.0 * 6.240495204925537
Epoch 1820, val loss: 1.5184438228607178
Epoch 1830, training loss: 12.483153343200684 = 0.009435195475816727 + 2.0 * 6.23685884475708
Epoch 1830, val loss: 1.5213801860809326
Epoch 1840, training loss: 12.497393608093262 = 0.009303283877670765 + 2.0 * 6.244045257568359
Epoch 1840, val loss: 1.5241888761520386
Epoch 1850, training loss: 12.47930908203125 = 0.009168637916445732 + 2.0 * 6.23507022857666
Epoch 1850, val loss: 1.5269020795822144
Epoch 1860, training loss: 12.476726531982422 = 0.009041554294526577 + 2.0 * 6.233842372894287
Epoch 1860, val loss: 1.5297958850860596
Epoch 1870, training loss: 12.476066589355469 = 0.008916476741433144 + 2.0 * 6.233574867248535
Epoch 1870, val loss: 1.532758116722107
Epoch 1880, training loss: 12.47390079498291 = 0.00879216380417347 + 2.0 * 6.2325544357299805
Epoch 1880, val loss: 1.5355875492095947
Epoch 1890, training loss: 12.475948333740234 = 0.008670140989124775 + 2.0 * 6.233639240264893
Epoch 1890, val loss: 1.5383613109588623
Epoch 1900, training loss: 12.489535331726074 = 0.008551076985895634 + 2.0 * 6.240492343902588
Epoch 1900, val loss: 1.5409259796142578
Epoch 1910, training loss: 12.47894287109375 = 0.008437306620180607 + 2.0 * 6.235252857208252
Epoch 1910, val loss: 1.5435540676116943
Epoch 1920, training loss: 12.47423267364502 = 0.008324156515300274 + 2.0 * 6.232954025268555
Epoch 1920, val loss: 1.5462020635604858
Epoch 1930, training loss: 12.493107795715332 = 0.008214648813009262 + 2.0 * 6.242446422576904
Epoch 1930, val loss: 1.5489273071289062
Epoch 1940, training loss: 12.478630065917969 = 0.008106548339128494 + 2.0 * 6.235261917114258
Epoch 1940, val loss: 1.5512059926986694
Epoch 1950, training loss: 12.4864501953125 = 0.008001637645065784 + 2.0 * 6.239224433898926
Epoch 1950, val loss: 1.5537461042404175
Epoch 1960, training loss: 12.471128463745117 = 0.007899552583694458 + 2.0 * 6.231614589691162
Epoch 1960, val loss: 1.5562275648117065
Epoch 1970, training loss: 12.472357749938965 = 0.0078001972287893295 + 2.0 * 6.232278823852539
Epoch 1970, val loss: 1.5589306354522705
Epoch 1980, training loss: 12.471888542175293 = 0.0077012525871396065 + 2.0 * 6.232093811035156
Epoch 1980, val loss: 1.561434268951416
Epoch 1990, training loss: 12.473430633544922 = 0.0076040965504944324 + 2.0 * 6.232913494110107
Epoch 1990, val loss: 1.5638995170593262
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7703703703703704
0.8091723774380601
=== training gcn model ===
Epoch 0, training loss: 19.150949478149414 = 1.9572157859802246 + 2.0 * 8.596866607666016
Epoch 0, val loss: 1.958619475364685
Epoch 10, training loss: 19.13925552368164 = 1.9460821151733398 + 2.0 * 8.596587181091309
Epoch 10, val loss: 1.9471440315246582
Epoch 20, training loss: 19.11996841430664 = 1.9323315620422363 + 2.0 * 8.593818664550781
Epoch 20, val loss: 1.9329473972320557
Epoch 30, training loss: 19.04950523376465 = 1.9137332439422607 + 2.0 * 8.567886352539062
Epoch 30, val loss: 1.91377854347229
Epoch 40, training loss: 18.597265243530273 = 1.8908579349517822 + 2.0 * 8.353203773498535
Epoch 40, val loss: 1.8913229703903198
Epoch 50, training loss: 17.331188201904297 = 1.8670560121536255 + 2.0 * 7.7320661544799805
Epoch 50, val loss: 1.8690396547317505
Epoch 60, training loss: 16.533885955810547 = 1.851584553718567 + 2.0 * 7.341150283813477
Epoch 60, val loss: 1.855873465538025
Epoch 70, training loss: 16.02166748046875 = 1.8384480476379395 + 2.0 * 7.091609954833984
Epoch 70, val loss: 1.8438825607299805
Epoch 80, training loss: 15.705814361572266 = 1.8259329795837402 + 2.0 * 6.939940452575684
Epoch 80, val loss: 1.8322025537490845
Epoch 90, training loss: 15.524277687072754 = 1.812711477279663 + 2.0 * 6.855782985687256
Epoch 90, val loss: 1.819640040397644
Epoch 100, training loss: 15.383063316345215 = 1.8002535104751587 + 2.0 * 6.791404724121094
Epoch 100, val loss: 1.8080456256866455
Epoch 110, training loss: 15.268193244934082 = 1.7887952327728271 + 2.0 * 6.739698886871338
Epoch 110, val loss: 1.7971702814102173
Epoch 120, training loss: 15.16096019744873 = 1.7779111862182617 + 2.0 * 6.691524505615234
Epoch 120, val loss: 1.7867376804351807
Epoch 130, training loss: 15.056952476501465 = 1.7672253847122192 + 2.0 * 6.644863605499268
Epoch 130, val loss: 1.7766473293304443
Epoch 140, training loss: 14.964343070983887 = 1.7564023733139038 + 2.0 * 6.603970527648926
Epoch 140, val loss: 1.766735315322876
Epoch 150, training loss: 14.887764930725098 = 1.7449170351028442 + 2.0 * 6.5714240074157715
Epoch 150, val loss: 1.7566641569137573
Epoch 160, training loss: 14.81454086303711 = 1.7322779893875122 + 2.0 * 6.541131496429443
Epoch 160, val loss: 1.7459112405776978
Epoch 170, training loss: 14.75792407989502 = 1.7181626558303833 + 2.0 * 6.519880771636963
Epoch 170, val loss: 1.7341779470443726
Epoch 180, training loss: 14.693016052246094 = 1.7023520469665527 + 2.0 * 6.495331764221191
Epoch 180, val loss: 1.7212034463882446
Epoch 190, training loss: 14.63744831085205 = 1.6845834255218506 + 2.0 * 6.4764323234558105
Epoch 190, val loss: 1.7067511081695557
Epoch 200, training loss: 14.599677085876465 = 1.6645833253860474 + 2.0 * 6.4675469398498535
Epoch 200, val loss: 1.6906362771987915
Epoch 210, training loss: 14.541865348815918 = 1.642351746559143 + 2.0 * 6.449756622314453
Epoch 210, val loss: 1.6727749109268188
Epoch 220, training loss: 14.49480152130127 = 1.6176620721817017 + 2.0 * 6.43856954574585
Epoch 220, val loss: 1.6529982089996338
Epoch 230, training loss: 14.446475982666016 = 1.5903233289718628 + 2.0 * 6.428076267242432
Epoch 230, val loss: 1.6312352418899536
Epoch 240, training loss: 14.4107027053833 = 1.5604335069656372 + 2.0 * 6.425134658813477
Epoch 240, val loss: 1.607619285583496
Epoch 250, training loss: 14.351951599121094 = 1.5284922122955322 + 2.0 * 6.41172981262207
Epoch 250, val loss: 1.5826091766357422
Epoch 260, training loss: 14.304905891418457 = 1.4944677352905273 + 2.0 * 6.405219078063965
Epoch 260, val loss: 1.5562664270401
Epoch 270, training loss: 14.258342742919922 = 1.458819031715393 + 2.0 * 6.39976167678833
Epoch 270, val loss: 1.5291131734848022
Epoch 280, training loss: 14.208601951599121 = 1.4223524332046509 + 2.0 * 6.393124580383301
Epoch 280, val loss: 1.5018337965011597
Epoch 290, training loss: 14.157096862792969 = 1.3850473165512085 + 2.0 * 6.3860249519348145
Epoch 290, val loss: 1.4744311571121216
Epoch 300, training loss: 14.117234230041504 = 1.3470431566238403 + 2.0 * 6.385095596313477
Epoch 300, val loss: 1.4470192193984985
Epoch 310, training loss: 14.06446647644043 = 1.3090453147888184 + 2.0 * 6.377710819244385
Epoch 310, val loss: 1.4202104806900024
Epoch 320, training loss: 14.014121055603027 = 1.2710435390472412 + 2.0 * 6.3715386390686035
Epoch 320, val loss: 1.3939154148101807
Epoch 330, training loss: 13.968955993652344 = 1.232743740081787 + 2.0 * 6.368106365203857
Epoch 330, val loss: 1.3678655624389648
Epoch 340, training loss: 13.925131797790527 = 1.1944912672042847 + 2.0 * 6.365320205688477
Epoch 340, val loss: 1.341935396194458
Epoch 350, training loss: 13.878056526184082 = 1.1563054323196411 + 2.0 * 6.360875606536865
Epoch 350, val loss: 1.3164231777191162
Epoch 360, training loss: 13.830770492553711 = 1.1180768013000488 + 2.0 * 6.35634708404541
Epoch 360, val loss: 1.2908189296722412
Epoch 370, training loss: 13.79145622253418 = 1.0797255039215088 + 2.0 * 6.355865478515625
Epoch 370, val loss: 1.2651481628417969
Epoch 380, training loss: 13.757566452026367 = 1.0414847135543823 + 2.0 * 6.358040809631348
Epoch 380, val loss: 1.2397311925888062
Epoch 390, training loss: 13.702661514282227 = 1.0041123628616333 + 2.0 * 6.349274635314941
Epoch 390, val loss: 1.2147207260131836
Epoch 400, training loss: 13.656237602233887 = 0.9672938585281372 + 2.0 * 6.3444719314575195
Epoch 400, val loss: 1.1902539730072021
Epoch 410, training loss: 13.613679885864258 = 0.9311897158622742 + 2.0 * 6.341245174407959
Epoch 410, val loss: 1.1662753820419312
Epoch 420, training loss: 13.573153495788574 = 0.8959112167358398 + 2.0 * 6.338621139526367
Epoch 420, val loss: 1.143080711364746
Epoch 430, training loss: 13.547431945800781 = 0.8615854382514954 + 2.0 * 6.342923164367676
Epoch 430, val loss: 1.1206541061401367
Epoch 440, training loss: 13.502612113952637 = 0.8285750150680542 + 2.0 * 6.3370184898376465
Epoch 440, val loss: 1.0991623401641846
Epoch 450, training loss: 13.462522506713867 = 0.7970181107521057 + 2.0 * 6.332752227783203
Epoch 450, val loss: 1.0789116621017456
Epoch 460, training loss: 13.433956146240234 = 0.7667266726493835 + 2.0 * 6.333614826202393
Epoch 460, val loss: 1.0598373413085938
Epoch 470, training loss: 13.397933006286621 = 0.7377424240112305 + 2.0 * 6.330095291137695
Epoch 470, val loss: 1.0415717363357544
Epoch 480, training loss: 13.361743927001953 = 0.7099859118461609 + 2.0 * 6.325879096984863
Epoch 480, val loss: 1.0248562097549438
Epoch 490, training loss: 13.330471992492676 = 0.6831490993499756 + 2.0 * 6.3236613273620605
Epoch 490, val loss: 1.0090365409851074
Epoch 500, training loss: 13.310196876525879 = 0.6571190357208252 + 2.0 * 6.326539039611816
Epoch 500, val loss: 0.9938769340515137
Epoch 510, training loss: 13.28297233581543 = 0.6320263743400574 + 2.0 * 6.325472831726074
Epoch 510, val loss: 0.9795883893966675
Epoch 520, training loss: 13.24832534790039 = 0.6077446341514587 + 2.0 * 6.320290565490723
Epoch 520, val loss: 0.9665499925613403
Epoch 530, training loss: 13.217500686645508 = 0.5841919779777527 + 2.0 * 6.316654205322266
Epoch 530, val loss: 0.9542189836502075
Epoch 540, training loss: 13.215720176696777 = 0.5612595081329346 + 2.0 * 6.327230453491211
Epoch 540, val loss: 0.9425948858261108
Epoch 550, training loss: 13.165384292602539 = 0.5392203330993652 + 2.0 * 6.313081741333008
Epoch 550, val loss: 0.9319430589675903
Epoch 560, training loss: 13.148404121398926 = 0.5178021192550659 + 2.0 * 6.315300941467285
Epoch 560, val loss: 0.9223241209983826
Epoch 570, training loss: 13.122243881225586 = 0.49699854850769043 + 2.0 * 6.312622547149658
Epoch 570, val loss: 0.9134019613265991
Epoch 580, training loss: 13.093610763549805 = 0.47687217593193054 + 2.0 * 6.308369159698486
Epoch 580, val loss: 0.9053195714950562
Epoch 590, training loss: 13.07193660736084 = 0.45734164118766785 + 2.0 * 6.307297706604004
Epoch 590, val loss: 0.8980858325958252
Epoch 600, training loss: 13.066476821899414 = 0.4383842349052429 + 2.0 * 6.314046382904053
Epoch 600, val loss: 0.8914421200752258
Epoch 610, training loss: 13.040128707885742 = 0.4200839102268219 + 2.0 * 6.310022354125977
Epoch 610, val loss: 0.8855788111686707
Epoch 620, training loss: 13.011443138122559 = 0.40248921513557434 + 2.0 * 6.304476737976074
Epoch 620, val loss: 0.8806709051132202
Epoch 630, training loss: 12.987164497375488 = 0.38545340299606323 + 2.0 * 6.30085563659668
Epoch 630, val loss: 0.8762508034706116
Epoch 640, training loss: 12.968293190002441 = 0.36890286207199097 + 2.0 * 6.299695014953613
Epoch 640, val loss: 0.8723945617675781
Epoch 650, training loss: 12.980158805847168 = 0.3528759479522705 + 2.0 * 6.313641548156738
Epoch 650, val loss: 0.8690842390060425
Epoch 660, training loss: 12.934643745422363 = 0.33728930354118347 + 2.0 * 6.298677444458008
Epoch 660, val loss: 0.8662596940994263
Epoch 670, training loss: 12.915157318115234 = 0.32224732637405396 + 2.0 * 6.296454906463623
Epoch 670, val loss: 0.8641669154167175
Epoch 680, training loss: 12.904982566833496 = 0.3076039254665375 + 2.0 * 6.298689365386963
Epoch 680, val loss: 0.8621863722801208
Epoch 690, training loss: 12.883648872375488 = 0.29339322447776794 + 2.0 * 6.295127868652344
Epoch 690, val loss: 0.8607212901115417
Epoch 700, training loss: 12.869074821472168 = 0.27957966923713684 + 2.0 * 6.294747352600098
Epoch 700, val loss: 0.8597484230995178
Epoch 710, training loss: 12.859664916992188 = 0.2661789059638977 + 2.0 * 6.296742916107178
Epoch 710, val loss: 0.8590720295906067
Epoch 720, training loss: 12.841132164001465 = 0.25320398807525635 + 2.0 * 6.29396390914917
Epoch 720, val loss: 0.8588264584541321
Epoch 730, training loss: 12.822929382324219 = 0.24059979617595673 + 2.0 * 6.291164875030518
Epoch 730, val loss: 0.8590644001960754
Epoch 740, training loss: 12.812650680541992 = 0.22842878103256226 + 2.0 * 6.292110919952393
Epoch 740, val loss: 0.8596756458282471
Epoch 750, training loss: 12.793108940124512 = 0.216680645942688 + 2.0 * 6.288214206695557
Epoch 750, val loss: 0.8606602549552917
Epoch 760, training loss: 12.796554565429688 = 0.20542976260185242 + 2.0 * 6.295562267303467
Epoch 760, val loss: 0.8619305491447449
Epoch 770, training loss: 12.765491485595703 = 0.194734126329422 + 2.0 * 6.285378456115723
Epoch 770, val loss: 0.8638371229171753
Epoch 780, training loss: 12.75399398803711 = 0.18452781438827515 + 2.0 * 6.284733295440674
Epoch 780, val loss: 0.8662631511688232
Epoch 790, training loss: 12.755328178405762 = 0.17482507228851318 + 2.0 * 6.290251731872559
Epoch 790, val loss: 0.8688375949859619
Epoch 800, training loss: 12.735553741455078 = 0.16563135385513306 + 2.0 * 6.284961223602295
Epoch 800, val loss: 0.8717098832130432
Epoch 810, training loss: 12.724105834960938 = 0.1569984406232834 + 2.0 * 6.283553600311279
Epoch 810, val loss: 0.8751062154769897
Epoch 820, training loss: 12.71947193145752 = 0.1488463431596756 + 2.0 * 6.285312652587891
Epoch 820, val loss: 0.8786652684211731
Epoch 830, training loss: 12.704703330993652 = 0.14117330312728882 + 2.0 * 6.281764984130859
Epoch 830, val loss: 0.882597804069519
Epoch 840, training loss: 12.69544792175293 = 0.13397377729415894 + 2.0 * 6.280736923217773
Epoch 840, val loss: 0.8867635726928711
Epoch 850, training loss: 12.685114860534668 = 0.12721313536167145 + 2.0 * 6.2789506912231445
Epoch 850, val loss: 0.8912506699562073
Epoch 860, training loss: 12.675686836242676 = 0.12086581438779831 + 2.0 * 6.277410507202148
Epoch 860, val loss: 0.8958651423454285
Epoch 870, training loss: 12.693812370300293 = 0.11492887139320374 + 2.0 * 6.2894415855407715
Epoch 870, val loss: 0.9007027745246887
Epoch 880, training loss: 12.664468765258789 = 0.10937758535146713 + 2.0 * 6.27754545211792
Epoch 880, val loss: 0.9054025411605835
Epoch 890, training loss: 12.656543731689453 = 0.104204922914505 + 2.0 * 6.276169300079346
Epoch 890, val loss: 0.9106730222702026
Epoch 900, training loss: 12.647014617919922 = 0.09936251491308212 + 2.0 * 6.2738261222839355
Epoch 900, val loss: 0.9159175753593445
Epoch 910, training loss: 12.642777442932129 = 0.0948164314031601 + 2.0 * 6.273980617523193
Epoch 910, val loss: 0.9212309122085571
Epoch 920, training loss: 12.653592109680176 = 0.09055151045322418 + 2.0 * 6.281520366668701
Epoch 920, val loss: 0.9266953468322754
Epoch 930, training loss: 12.637604713439941 = 0.08653927594423294 + 2.0 * 6.2755327224731445
Epoch 930, val loss: 0.9322340488433838
Epoch 940, training loss: 12.634692192077637 = 0.08278832584619522 + 2.0 * 6.275951862335205
Epoch 940, val loss: 0.9378800988197327
Epoch 950, training loss: 12.623173713684082 = 0.07926136255264282 + 2.0 * 6.271955966949463
Epoch 950, val loss: 0.9435089826583862
Epoch 960, training loss: 12.617700576782227 = 0.0759352520108223 + 2.0 * 6.270882606506348
Epoch 960, val loss: 0.9492162466049194
Epoch 970, training loss: 12.623053550720215 = 0.07279787212610245 + 2.0 * 6.27512788772583
Epoch 970, val loss: 0.954907238483429
Epoch 980, training loss: 12.609785079956055 = 0.06984145939350128 + 2.0 * 6.26997184753418
Epoch 980, val loss: 0.9606677293777466
Epoch 990, training loss: 12.611564636230469 = 0.06704537570476532 + 2.0 * 6.272259712219238
Epoch 990, val loss: 0.96638023853302
Epoch 1000, training loss: 12.604493141174316 = 0.06441757082939148 + 2.0 * 6.270037651062012
Epoch 1000, val loss: 0.9721581339836121
Epoch 1010, training loss: 12.595399856567383 = 0.061926279217004776 + 2.0 * 6.26673698425293
Epoch 1010, val loss: 0.9780198931694031
Epoch 1020, training loss: 12.595928192138672 = 0.059569429606199265 + 2.0 * 6.268179416656494
Epoch 1020, val loss: 0.9838342666625977
Epoch 1030, training loss: 12.591261863708496 = 0.05733345448970795 + 2.0 * 6.266964435577393
Epoch 1030, val loss: 0.989388108253479
Epoch 1040, training loss: 12.585777282714844 = 0.05521717667579651 + 2.0 * 6.265280246734619
Epoch 1040, val loss: 0.9951861500740051
Epoch 1050, training loss: 12.586461067199707 = 0.05320960283279419 + 2.0 * 6.266625881195068
Epoch 1050, val loss: 1.0008741617202759
Epoch 1060, training loss: 12.585972785949707 = 0.051310084760189056 + 2.0 * 6.267331123352051
Epoch 1060, val loss: 1.0065187215805054
Epoch 1070, training loss: 12.576990127563477 = 0.04949755221605301 + 2.0 * 6.26374626159668
Epoch 1070, val loss: 1.0122394561767578
Epoch 1080, training loss: 12.583327293395996 = 0.04777583107352257 + 2.0 * 6.267775535583496
Epoch 1080, val loss: 1.0178638696670532
Epoch 1090, training loss: 12.577878952026367 = 0.046144548803567886 + 2.0 * 6.265867233276367
Epoch 1090, val loss: 1.0235275030136108
Epoch 1100, training loss: 12.578246116638184 = 0.04458708316087723 + 2.0 * 6.266829490661621
Epoch 1100, val loss: 1.029069423675537
Epoch 1110, training loss: 12.569857597351074 = 0.04311341419816017 + 2.0 * 6.26337194442749
Epoch 1110, val loss: 1.0345736742019653
Epoch 1120, training loss: 12.56702995300293 = 0.04170295223593712 + 2.0 * 6.2626633644104
Epoch 1120, val loss: 1.0400853157043457
Epoch 1130, training loss: 12.567974090576172 = 0.04036477208137512 + 2.0 * 6.2638044357299805
Epoch 1130, val loss: 1.0455290079116821
Epoch 1140, training loss: 12.56000804901123 = 0.039080217480659485 + 2.0 * 6.260463714599609
Epoch 1140, val loss: 1.0508825778961182
Epoch 1150, training loss: 12.557992935180664 = 0.03785374015569687 + 2.0 * 6.260069370269775
Epoch 1150, val loss: 1.056291103363037
Epoch 1160, training loss: 12.563891410827637 = 0.03668459132313728 + 2.0 * 6.263603210449219
Epoch 1160, val loss: 1.0615702867507935
Epoch 1170, training loss: 12.55544662475586 = 0.03556674346327782 + 2.0 * 6.259940147399902
Epoch 1170, val loss: 1.066870927810669
Epoch 1180, training loss: 12.554591178894043 = 0.034497007727622986 + 2.0 * 6.26004695892334
Epoch 1180, val loss: 1.0720429420471191
Epoch 1190, training loss: 12.555691719055176 = 0.03347744420170784 + 2.0 * 6.261106967926025
Epoch 1190, val loss: 1.0771255493164062
Epoch 1200, training loss: 12.547646522521973 = 0.03250632807612419 + 2.0 * 6.257570266723633
Epoch 1200, val loss: 1.0824322700500488
Epoch 1210, training loss: 12.546202659606934 = 0.03157119080424309 + 2.0 * 6.257315635681152
Epoch 1210, val loss: 1.0875369310379028
Epoch 1220, training loss: 12.547834396362305 = 0.030677519738674164 + 2.0 * 6.258578300476074
Epoch 1220, val loss: 1.092558741569519
Epoch 1230, training loss: 12.545358657836914 = 0.029815077781677246 + 2.0 * 6.257771968841553
Epoch 1230, val loss: 1.0974185466766357
Epoch 1240, training loss: 12.546027183532715 = 0.028991300612688065 + 2.0 * 6.258517742156982
Epoch 1240, val loss: 1.1023287773132324
Epoch 1250, training loss: 12.542426109313965 = 0.028202997520565987 + 2.0 * 6.257111549377441
Epoch 1250, val loss: 1.1071656942367554
Epoch 1260, training loss: 12.54335880279541 = 0.02744884230196476 + 2.0 * 6.257955074310303
Epoch 1260, val loss: 1.1120628118515015
Epoch 1270, training loss: 12.543739318847656 = 0.026726311072707176 + 2.0 * 6.2585062980651855
Epoch 1270, val loss: 1.116796612739563
Epoch 1280, training loss: 12.531700134277344 = 0.026028694584965706 + 2.0 * 6.252835750579834
Epoch 1280, val loss: 1.1213377714157104
Epoch 1290, training loss: 12.532689094543457 = 0.025358520448207855 + 2.0 * 6.253665447235107
Epoch 1290, val loss: 1.1260344982147217
Epoch 1300, training loss: 12.532153129577637 = 0.024713929742574692 + 2.0 * 6.253719806671143
Epoch 1300, val loss: 1.1306076049804688
Epoch 1310, training loss: 12.535158157348633 = 0.024091504514217377 + 2.0 * 6.255533218383789
Epoch 1310, val loss: 1.1351133584976196
Epoch 1320, training loss: 12.534283638000488 = 0.023494180291891098 + 2.0 * 6.25539493560791
Epoch 1320, val loss: 1.1396958827972412
Epoch 1330, training loss: 12.530174255371094 = 0.02291872911155224 + 2.0 * 6.253627777099609
Epoch 1330, val loss: 1.1441138982772827
Epoch 1340, training loss: 12.531473159790039 = 0.0223652645945549 + 2.0 * 6.25455379486084
Epoch 1340, val loss: 1.1485260725021362
Epoch 1350, training loss: 12.525071144104004 = 0.021834542974829674 + 2.0 * 6.251618385314941
Epoch 1350, val loss: 1.1529196500778198
Epoch 1360, training loss: 12.53400707244873 = 0.021323634311556816 + 2.0 * 6.256341934204102
Epoch 1360, val loss: 1.1572840213775635
Epoch 1370, training loss: 12.522367477416992 = 0.020824909210205078 + 2.0 * 6.250771522521973
Epoch 1370, val loss: 1.1613351106643677
Epoch 1380, training loss: 12.528654098510742 = 0.020348722115159035 + 2.0 * 6.254152774810791
Epoch 1380, val loss: 1.1656743288040161
Epoch 1390, training loss: 12.519044876098633 = 0.019888391718268394 + 2.0 * 6.249578475952148
Epoch 1390, val loss: 1.1697051525115967
Epoch 1400, training loss: 12.516898155212402 = 0.019442547112703323 + 2.0 * 6.248727798461914
Epoch 1400, val loss: 1.1738510131835938
Epoch 1410, training loss: 12.52285385131836 = 0.019014671444892883 + 2.0 * 6.251919746398926
Epoch 1410, val loss: 1.1780401468276978
Epoch 1420, training loss: 12.521748542785645 = 0.01859862171113491 + 2.0 * 6.251574993133545
Epoch 1420, val loss: 1.1818151473999023
Epoch 1430, training loss: 12.512563705444336 = 0.018197085708379745 + 2.0 * 6.247183322906494
Epoch 1430, val loss: 1.185774326324463
Epoch 1440, training loss: 12.511772155761719 = 0.017811594530940056 + 2.0 * 6.2469801902771
Epoch 1440, val loss: 1.1898008584976196
Epoch 1450, training loss: 12.510271072387695 = 0.017437506467103958 + 2.0 * 6.2464165687561035
Epoch 1450, val loss: 1.193696141242981
Epoch 1460, training loss: 12.520606994628906 = 0.017073653638362885 + 2.0 * 6.251766681671143
Epoch 1460, val loss: 1.1974459886550903
Epoch 1470, training loss: 12.510627746582031 = 0.016719678416848183 + 2.0 * 6.246953964233398
Epoch 1470, val loss: 1.2011767625808716
Epoch 1480, training loss: 12.515669822692871 = 0.016379740089178085 + 2.0 * 6.249645233154297
Epoch 1480, val loss: 1.2049247026443481
Epoch 1490, training loss: 12.513077735900879 = 0.01605239324271679 + 2.0 * 6.2485127449035645
Epoch 1490, val loss: 1.2087897062301636
Epoch 1500, training loss: 12.506535530090332 = 0.015735356137156487 + 2.0 * 6.2453999519348145
Epoch 1500, val loss: 1.212368369102478
Epoch 1510, training loss: 12.50625991821289 = 0.015425944700837135 + 2.0 * 6.24541711807251
Epoch 1510, val loss: 1.2160955667495728
Epoch 1520, training loss: 12.50400447845459 = 0.015127809718251228 + 2.0 * 6.244438171386719
Epoch 1520, val loss: 1.219743251800537
Epoch 1530, training loss: 12.523000717163086 = 0.014838832430541515 + 2.0 * 6.254080772399902
Epoch 1530, val loss: 1.2234163284301758
Epoch 1540, training loss: 12.508353233337402 = 0.01455328706651926 + 2.0 * 6.2469000816345215
Epoch 1540, val loss: 1.2266243696212769
Epoch 1550, training loss: 12.502840995788574 = 0.014280254021286964 + 2.0 * 6.2442803382873535
Epoch 1550, val loss: 1.2302418947219849
Epoch 1560, training loss: 12.500597953796387 = 0.014016254805028439 + 2.0 * 6.243290901184082
Epoch 1560, val loss: 1.2338107824325562
Epoch 1570, training loss: 12.513087272644043 = 0.013759840279817581 + 2.0 * 6.249663829803467
Epoch 1570, val loss: 1.2372102737426758
Epoch 1580, training loss: 12.50082778930664 = 0.013508854433894157 + 2.0 * 6.243659496307373
Epoch 1580, val loss: 1.240377426147461
Epoch 1590, training loss: 12.497163772583008 = 0.013265064917504787 + 2.0 * 6.241949558258057
Epoch 1590, val loss: 1.2438583374023438
Epoch 1600, training loss: 12.504045486450195 = 0.013029225170612335 + 2.0 * 6.245508193969727
Epoch 1600, val loss: 1.247090458869934
Epoch 1610, training loss: 12.502578735351562 = 0.012799205258488655 + 2.0 * 6.244889736175537
Epoch 1610, val loss: 1.2502245903015137
Epoch 1620, training loss: 12.499451637268066 = 0.012576946057379246 + 2.0 * 6.24343729019165
Epoch 1620, val loss: 1.253542184829712
Epoch 1630, training loss: 12.49919319152832 = 0.012362971901893616 + 2.0 * 6.243414878845215
Epoch 1630, val loss: 1.2568645477294922
Epoch 1640, training loss: 12.496362686157227 = 0.012152506969869137 + 2.0 * 6.242105007171631
Epoch 1640, val loss: 1.2600407600402832
Epoch 1650, training loss: 12.5011625289917 = 0.011947103776037693 + 2.0 * 6.244607925415039
Epoch 1650, val loss: 1.2630631923675537
Epoch 1660, training loss: 12.495569229125977 = 0.01174759678542614 + 2.0 * 6.241910934448242
Epoch 1660, val loss: 1.2661547660827637
Epoch 1670, training loss: 12.50078010559082 = 0.011553254909813404 + 2.0 * 6.2446136474609375
Epoch 1670, val loss: 1.2691377401351929
Epoch 1680, training loss: 12.492047309875488 = 0.011364663019776344 + 2.0 * 6.2403411865234375
Epoch 1680, val loss: 1.2723801136016846
Epoch 1690, training loss: 12.488536834716797 = 0.011180606670677662 + 2.0 * 6.238677978515625
Epoch 1690, val loss: 1.2754874229431152
Epoch 1700, training loss: 12.501449584960938 = 0.011000876314938068 + 2.0 * 6.245224475860596
Epoch 1700, val loss: 1.2785325050354004
Epoch 1710, training loss: 12.490729331970215 = 0.010824842378497124 + 2.0 * 6.239952087402344
Epoch 1710, val loss: 1.2812680006027222
Epoch 1720, training loss: 12.493265151977539 = 0.010653780773282051 + 2.0 * 6.241305828094482
Epoch 1720, val loss: 1.2842309474945068
Epoch 1730, training loss: 12.491329193115234 = 0.010488683357834816 + 2.0 * 6.240420341491699
Epoch 1730, val loss: 1.2871935367584229
Epoch 1740, training loss: 12.484966278076172 = 0.010327620431780815 + 2.0 * 6.237319469451904
Epoch 1740, val loss: 1.290282130241394
Epoch 1750, training loss: 12.48809814453125 = 0.010170556604862213 + 2.0 * 6.238963603973389
Epoch 1750, val loss: 1.2932689189910889
Epoch 1760, training loss: 12.492989540100098 = 0.010015957057476044 + 2.0 * 6.2414870262146
Epoch 1760, val loss: 1.2961018085479736
Epoch 1770, training loss: 12.487942695617676 = 0.00986259151250124 + 2.0 * 6.239039897918701
Epoch 1770, val loss: 1.2987754344940186
Epoch 1780, training loss: 12.493952751159668 = 0.009716170839965343 + 2.0 * 6.2421183586120605
Epoch 1780, val loss: 1.301495909690857
Epoch 1790, training loss: 12.491446495056152 = 0.009573370218276978 + 2.0 * 6.240936756134033
Epoch 1790, val loss: 1.3043651580810547
Epoch 1800, training loss: 12.48449420928955 = 0.009434415027499199 + 2.0 * 6.237529754638672
Epoch 1800, val loss: 1.3071486949920654
Epoch 1810, training loss: 12.487933158874512 = 0.00929900724440813 + 2.0 * 6.239316940307617
Epoch 1810, val loss: 1.3099466562271118
Epoch 1820, training loss: 12.483100891113281 = 0.009165731258690357 + 2.0 * 6.23696756362915
Epoch 1820, val loss: 1.3125534057617188
Epoch 1830, training loss: 12.481024742126465 = 0.009035401046276093 + 2.0 * 6.235994815826416
Epoch 1830, val loss: 1.3153067827224731
Epoch 1840, training loss: 12.495559692382812 = 0.008907848969101906 + 2.0 * 6.243325710296631
Epoch 1840, val loss: 1.3178082704544067
Epoch 1850, training loss: 12.481586456298828 = 0.008783599361777306 + 2.0 * 6.236401557922363
Epoch 1850, val loss: 1.320518136024475
Epoch 1860, training loss: 12.47696304321289 = 0.008662406355142593 + 2.0 * 6.234150409698486
Epoch 1860, val loss: 1.3232121467590332
Epoch 1870, training loss: 12.479504585266113 = 0.008543633855879307 + 2.0 * 6.235480308532715
Epoch 1870, val loss: 1.3258744478225708
Epoch 1880, training loss: 12.49283218383789 = 0.008428261615335941 + 2.0 * 6.242201805114746
Epoch 1880, val loss: 1.32840096950531
Epoch 1890, training loss: 12.48123550415039 = 0.008313868194818497 + 2.0 * 6.2364606857299805
Epoch 1890, val loss: 1.3308486938476562
Epoch 1900, training loss: 12.480724334716797 = 0.008203011006116867 + 2.0 * 6.236260890960693
Epoch 1900, val loss: 1.3335237503051758
Epoch 1910, training loss: 12.489184379577637 = 0.008095425553619862 + 2.0 * 6.240544319152832
Epoch 1910, val loss: 1.3360042572021484
Epoch 1920, training loss: 12.477324485778809 = 0.007988481782376766 + 2.0 * 6.234667778015137
Epoch 1920, val loss: 1.338291883468628
Epoch 1930, training loss: 12.476048469543457 = 0.007885793223977089 + 2.0 * 6.234081268310547
Epoch 1930, val loss: 1.3409806489944458
Epoch 1940, training loss: 12.47364330291748 = 0.007783870212733746 + 2.0 * 6.232929706573486
Epoch 1940, val loss: 1.3434090614318848
Epoch 1950, training loss: 12.482132911682129 = 0.007684077601879835 + 2.0 * 6.237224578857422
Epoch 1950, val loss: 1.345787763595581
Epoch 1960, training loss: 12.47238826751709 = 0.0075856470502913 + 2.0 * 6.232401371002197
Epoch 1960, val loss: 1.3481643199920654
Epoch 1970, training loss: 12.475983619689941 = 0.007490626070648432 + 2.0 * 6.234246730804443
Epoch 1970, val loss: 1.3507633209228516
Epoch 1980, training loss: 12.491241455078125 = 0.007397218141704798 + 2.0 * 6.241921901702881
Epoch 1980, val loss: 1.3531330823898315
Epoch 1990, training loss: 12.481197357177734 = 0.007304694037884474 + 2.0 * 6.236946105957031
Epoch 1990, val loss: 1.355217456817627
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7222222222222222
0.8112809699525567
The final CL Acc:0.75062, 0.02058, The final GNN Acc:0.81163, 0.00217
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13290])
remove edge: torch.Size([2, 7936])
updated graph: torch.Size([2, 10670])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 19.13045310974121 = 1.9368654489517212 + 2.0 * 8.596794128417969
Epoch 0, val loss: 1.9420355558395386
Epoch 10, training loss: 19.119384765625 = 1.927024006843567 + 2.0 * 8.596179962158203
Epoch 10, val loss: 1.931640625
Epoch 20, training loss: 19.0965633392334 = 1.9148389101028442 + 2.0 * 8.590862274169922
Epoch 20, val loss: 1.918516993522644
Epoch 30, training loss: 19.00673484802246 = 1.8987256288528442 + 2.0 * 8.554004669189453
Epoch 30, val loss: 1.9011197090148926
Epoch 40, training loss: 18.59107208251953 = 1.8791003227233887 + 2.0 * 8.355985641479492
Epoch 40, val loss: 1.8809657096862793
Epoch 50, training loss: 17.381086349487305 = 1.8571445941925049 + 2.0 * 7.761970520019531
Epoch 50, val loss: 1.859344720840454
Epoch 60, training loss: 16.729412078857422 = 1.8384792804718018 + 2.0 * 7.4454665184021
Epoch 60, val loss: 1.8421504497528076
Epoch 70, training loss: 16.193449020385742 = 1.8246012926101685 + 2.0 * 7.184423923492432
Epoch 70, val loss: 1.8290029764175415
Epoch 80, training loss: 15.709968566894531 = 1.8088843822479248 + 2.0 * 6.950541973114014
Epoch 80, val loss: 1.8133999109268188
Epoch 90, training loss: 15.438989639282227 = 1.7930350303649902 + 2.0 * 6.822977542877197
Epoch 90, val loss: 1.7975150346755981
Epoch 100, training loss: 15.308204650878906 = 1.7754981517791748 + 2.0 * 6.766353130340576
Epoch 100, val loss: 1.7803521156311035
Epoch 110, training loss: 15.194869995117188 = 1.7585242986679077 + 2.0 * 6.718173027038574
Epoch 110, val loss: 1.7643810510635376
Epoch 120, training loss: 15.086048126220703 = 1.7428464889526367 + 2.0 * 6.671600818634033
Epoch 120, val loss: 1.7499897480010986
Epoch 130, training loss: 14.999378204345703 = 1.72671639919281 + 2.0 * 6.636331081390381
Epoch 130, val loss: 1.734985589981079
Epoch 140, training loss: 14.924899101257324 = 1.708388090133667 + 2.0 * 6.608255386352539
Epoch 140, val loss: 1.7179938554763794
Epoch 150, training loss: 14.869837760925293 = 1.6872535943984985 + 2.0 * 6.591291904449463
Epoch 150, val loss: 1.6988365650177002
Epoch 160, training loss: 14.800369262695312 = 1.6637566089630127 + 2.0 * 6.5683064460754395
Epoch 160, val loss: 1.6776341199874878
Epoch 170, training loss: 14.73412799835205 = 1.6376140117645264 + 2.0 * 6.548256874084473
Epoch 170, val loss: 1.6546287536621094
Epoch 180, training loss: 14.666324615478516 = 1.6085271835327148 + 2.0 * 6.5288987159729
Epoch 180, val loss: 1.629250407218933
Epoch 190, training loss: 14.600255012512207 = 1.5756151676177979 + 2.0 * 6.512320041656494
Epoch 190, val loss: 1.6007840633392334
Epoch 200, training loss: 14.530288696289062 = 1.5390331745147705 + 2.0 * 6.4956278800964355
Epoch 200, val loss: 1.5687648057937622
Epoch 210, training loss: 14.458113670349121 = 1.4986140727996826 + 2.0 * 6.47974967956543
Epoch 210, val loss: 1.5335108041763306
Epoch 220, training loss: 14.385987281799316 = 1.4542425870895386 + 2.0 * 6.465872287750244
Epoch 220, val loss: 1.495017170906067
Epoch 230, training loss: 14.313901901245117 = 1.4061312675476074 + 2.0 * 6.453885078430176
Epoch 230, val loss: 1.4536222219467163
Epoch 240, training loss: 14.240981101989746 = 1.3556528091430664 + 2.0 * 6.44266414642334
Epoch 240, val loss: 1.4103330373764038
Epoch 250, training loss: 14.163818359375 = 1.3037468194961548 + 2.0 * 6.430035591125488
Epoch 250, val loss: 1.3663880825042725
Epoch 260, training loss: 14.089827537536621 = 1.2509452104568481 + 2.0 * 6.419441223144531
Epoch 260, val loss: 1.3221794366836548
Epoch 270, training loss: 14.044926643371582 = 1.197967767715454 + 2.0 * 6.4234795570373535
Epoch 270, val loss: 1.278637170791626
Epoch 280, training loss: 13.956686019897461 = 1.1472076177597046 + 2.0 * 6.4047393798828125
Epoch 280, val loss: 1.2371963262557983
Epoch 290, training loss: 13.890504837036133 = 1.098142385482788 + 2.0 * 6.396181106567383
Epoch 290, val loss: 1.1982191801071167
Epoch 300, training loss: 13.828764915466309 = 1.0507738590240479 + 2.0 * 6.38899564743042
Epoch 300, val loss: 1.1613510847091675
Epoch 310, training loss: 13.771345138549805 = 1.0051908493041992 + 2.0 * 6.383077144622803
Epoch 310, val loss: 1.1266554594039917
Epoch 320, training loss: 13.72536849975586 = 0.9615465998649597 + 2.0 * 6.381910800933838
Epoch 320, val loss: 1.0941927433013916
Epoch 330, training loss: 13.669599533081055 = 0.9206616878509521 + 2.0 * 6.374468803405762
Epoch 330, val loss: 1.0643473863601685
Epoch 340, training loss: 13.622164726257324 = 0.8821161985397339 + 2.0 * 6.37002420425415
Epoch 340, val loss: 1.036988377571106
Epoch 350, training loss: 13.57690715789795 = 0.8455707430839539 + 2.0 * 6.365668296813965
Epoch 350, val loss: 1.0116004943847656
Epoch 360, training loss: 13.544401168823242 = 0.8108536601066589 + 2.0 * 6.36677360534668
Epoch 360, val loss: 0.9880560040473938
Epoch 370, training loss: 13.498272895812988 = 0.7782163023948669 + 2.0 * 6.360028266906738
Epoch 370, val loss: 0.9665855169296265
Epoch 380, training loss: 13.459555625915527 = 0.747455894947052 + 2.0 * 6.35605001449585
Epoch 380, val loss: 0.9468763470649719
Epoch 390, training loss: 13.424091339111328 = 0.7182285189628601 + 2.0 * 6.352931499481201
Epoch 390, val loss: 0.9287909269332886
Epoch 400, training loss: 13.390740394592285 = 0.6906269788742065 + 2.0 * 6.3500566482543945
Epoch 400, val loss: 0.9122413396835327
Epoch 410, training loss: 13.358505249023438 = 0.6646303534507751 + 2.0 * 6.346937656402588
Epoch 410, val loss: 0.8973456621170044
Epoch 420, training loss: 13.331233024597168 = 0.6399362683296204 + 2.0 * 6.345648288726807
Epoch 420, val loss: 0.8838791251182556
Epoch 430, training loss: 13.301450729370117 = 0.6165280342102051 + 2.0 * 6.342461585998535
Epoch 430, val loss: 0.8718906044960022
Epoch 440, training loss: 13.273449897766113 = 0.5943483114242554 + 2.0 * 6.339550971984863
Epoch 440, val loss: 0.861283540725708
Epoch 450, training loss: 13.246316909790039 = 0.5731608867645264 + 2.0 * 6.336577892303467
Epoch 450, val loss: 0.8518881797790527
Epoch 460, training loss: 13.225810050964355 = 0.5529634952545166 + 2.0 * 6.336423397064209
Epoch 460, val loss: 0.8436644077301025
Epoch 470, training loss: 13.200907707214355 = 0.5338439345359802 + 2.0 * 6.333531856536865
Epoch 470, val loss: 0.8366438746452332
Epoch 480, training loss: 13.17607593536377 = 0.5155466794967651 + 2.0 * 6.330264568328857
Epoch 480, val loss: 0.83060622215271
Epoch 490, training loss: 13.153626441955566 = 0.4978958070278168 + 2.0 * 6.327865123748779
Epoch 490, val loss: 0.8254256248474121
Epoch 500, training loss: 13.140609741210938 = 0.4808778762817383 + 2.0 * 6.3298659324646
Epoch 500, val loss: 0.8210495710372925
Epoch 510, training loss: 13.120665550231934 = 0.4646376073360443 + 2.0 * 6.328013896942139
Epoch 510, val loss: 0.817492663860321
Epoch 520, training loss: 13.094905853271484 = 0.4489758014678955 + 2.0 * 6.322965145111084
Epoch 520, val loss: 0.8147836923599243
Epoch 530, training loss: 13.074134826660156 = 0.4338306188583374 + 2.0 * 6.320152282714844
Epoch 530, val loss: 0.8126066327095032
Epoch 540, training loss: 13.056901931762695 = 0.41908568143844604 + 2.0 * 6.318908214569092
Epoch 540, val loss: 0.8109515309333801
Epoch 550, training loss: 13.0457124710083 = 0.40475448966026306 + 2.0 * 6.320478916168213
Epoch 550, val loss: 0.8097673058509827
Epoch 560, training loss: 13.025096893310547 = 0.3908820152282715 + 2.0 * 6.317107200622559
Epoch 560, val loss: 0.809148907661438
Epoch 570, training loss: 13.00476360321045 = 0.37736353278160095 + 2.0 * 6.313700199127197
Epoch 570, val loss: 0.8089462518692017
Epoch 580, training loss: 12.98597526550293 = 0.36405399441719055 + 2.0 * 6.31096076965332
Epoch 580, val loss: 0.8091204762458801
Epoch 590, training loss: 12.974422454833984 = 0.35091686248779297 + 2.0 * 6.311752796173096
Epoch 590, val loss: 0.8096793293952942
Epoch 600, training loss: 12.962203025817871 = 0.33806294202804565 + 2.0 * 6.312069892883301
Epoch 600, val loss: 0.8103780150413513
Epoch 610, training loss: 12.94141960144043 = 0.3254241645336151 + 2.0 * 6.307997703552246
Epoch 610, val loss: 0.8115513920783997
Epoch 620, training loss: 12.925116539001465 = 0.3130132853984833 + 2.0 * 6.306051731109619
Epoch 620, val loss: 0.8130176663398743
Epoch 630, training loss: 12.907760620117188 = 0.30074605345726013 + 2.0 * 6.303507328033447
Epoch 630, val loss: 0.8148565888404846
Epoch 640, training loss: 12.892621040344238 = 0.28861522674560547 + 2.0 * 6.302002906799316
Epoch 640, val loss: 0.8170690536499023
Epoch 650, training loss: 12.88943099975586 = 0.27665868401527405 + 2.0 * 6.3063859939575195
Epoch 650, val loss: 0.8195900321006775
Epoch 660, training loss: 12.870409965515137 = 0.264970600605011 + 2.0 * 6.302719593048096
Epoch 660, val loss: 0.8226184844970703
Epoch 670, training loss: 12.859764099121094 = 0.2536812722682953 + 2.0 * 6.303041458129883
Epoch 670, val loss: 0.8258880972862244
Epoch 680, training loss: 12.83869457244873 = 0.24269171059131622 + 2.0 * 6.298001289367676
Epoch 680, val loss: 0.8297827839851379
Epoch 690, training loss: 12.82414436340332 = 0.23205123841762543 + 2.0 * 6.296046733856201
Epoch 690, val loss: 0.8340728282928467
Epoch 700, training loss: 12.81431770324707 = 0.22173844277858734 + 2.0 * 6.296289443969727
Epoch 700, val loss: 0.8387584686279297
Epoch 710, training loss: 12.801412582397461 = 0.21183286607265472 + 2.0 * 6.294789791107178
Epoch 710, val loss: 0.8437039256095886
Epoch 720, training loss: 12.78831958770752 = 0.2023477405309677 + 2.0 * 6.292985916137695
Epoch 720, val loss: 0.8491095900535583
Epoch 730, training loss: 12.775208473205566 = 0.19326657056808472 + 2.0 * 6.290970802307129
Epoch 730, val loss: 0.8548727631568909
Epoch 740, training loss: 12.770652770996094 = 0.18454819917678833 + 2.0 * 6.2930521965026855
Epoch 740, val loss: 0.8609037399291992
Epoch 750, training loss: 12.7634859085083 = 0.17628447711467743 + 2.0 * 6.293600559234619
Epoch 750, val loss: 0.8670027852058411
Epoch 760, training loss: 12.744958877563477 = 0.1684381663799286 + 2.0 * 6.288260459899902
Epoch 760, val loss: 0.8735051155090332
Epoch 770, training loss: 12.735967636108398 = 0.16096563637256622 + 2.0 * 6.287500858306885
Epoch 770, val loss: 0.8802940249443054
Epoch 780, training loss: 12.727288246154785 = 0.1538427323102951 + 2.0 * 6.286722660064697
Epoch 780, val loss: 0.8872503638267517
Epoch 790, training loss: 12.72889232635498 = 0.14706629514694214 + 2.0 * 6.290913105010986
Epoch 790, val loss: 0.8942590951919556
Epoch 800, training loss: 12.721407890319824 = 0.1406797617673874 + 2.0 * 6.2903642654418945
Epoch 800, val loss: 0.9015028476715088
Epoch 810, training loss: 12.704678535461426 = 0.13462196290493011 + 2.0 * 6.285028457641602
Epoch 810, val loss: 0.9088795185089111
Epoch 820, training loss: 12.695151329040527 = 0.12887516617774963 + 2.0 * 6.283138275146484
Epoch 820, val loss: 0.916436493396759
Epoch 830, training loss: 12.68891429901123 = 0.12340008467435837 + 2.0 * 6.28275728225708
Epoch 830, val loss: 0.9240078330039978
Epoch 840, training loss: 12.683490753173828 = 0.11820412427186966 + 2.0 * 6.2826433181762695
Epoch 840, val loss: 0.931530237197876
Epoch 850, training loss: 12.677302360534668 = 0.11328175663948059 + 2.0 * 6.282010078430176
Epoch 850, val loss: 0.9392423033714294
Epoch 860, training loss: 12.670649528503418 = 0.10861905664205551 + 2.0 * 6.281015396118164
Epoch 860, val loss: 0.9468908309936523
Epoch 870, training loss: 12.663997650146484 = 0.10418429970741272 + 2.0 * 6.279906749725342
Epoch 870, val loss: 0.9546522498130798
Epoch 880, training loss: 12.671270370483398 = 0.09997160732746124 + 2.0 * 6.285649299621582
Epoch 880, val loss: 0.9623426795005798
Epoch 890, training loss: 12.654648780822754 = 0.09600397199392319 + 2.0 * 6.279322624206543
Epoch 890, val loss: 0.9700106978416443
Epoch 900, training loss: 12.646007537841797 = 0.09221453219652176 + 2.0 * 6.2768964767456055
Epoch 900, val loss: 0.9778077006340027
Epoch 910, training loss: 12.640156745910645 = 0.08861708641052246 + 2.0 * 6.2757697105407715
Epoch 910, val loss: 0.9855553507804871
Epoch 920, training loss: 12.643746376037598 = 0.08518471568822861 + 2.0 * 6.279280662536621
Epoch 920, val loss: 0.9932940602302551
Epoch 930, training loss: 12.638736724853516 = 0.0819239392876625 + 2.0 * 6.278406620025635
Epoch 930, val loss: 1.0008124113082886
Epoch 940, training loss: 12.630526542663574 = 0.07884689420461655 + 2.0 * 6.275839805603027
Epoch 940, val loss: 1.0084574222564697
Epoch 950, training loss: 12.623135566711426 = 0.07591448724269867 + 2.0 * 6.273610591888428
Epoch 950, val loss: 1.0160725116729736
Epoch 960, training loss: 12.61633014678955 = 0.07311826199293137 + 2.0 * 6.271605968475342
Epoch 960, val loss: 1.023669719696045
Epoch 970, training loss: 12.62691593170166 = 0.07044648379087448 + 2.0 * 6.278234958648682
Epoch 970, val loss: 1.0311335325241089
Epoch 980, training loss: 12.619651794433594 = 0.06790422648191452 + 2.0 * 6.27587366104126
Epoch 980, val loss: 1.0384092330932617
Epoch 990, training loss: 12.608895301818848 = 0.06549107283353806 + 2.0 * 6.271702289581299
Epoch 990, val loss: 1.0458736419677734
Epoch 1000, training loss: 12.602654457092285 = 0.06318685412406921 + 2.0 * 6.269733905792236
Epoch 1000, val loss: 1.0532052516937256
Epoch 1010, training loss: 12.60332202911377 = 0.06098514050245285 + 2.0 * 6.271168231964111
Epoch 1010, val loss: 1.0604743957519531
Epoch 1020, training loss: 12.596783638000488 = 0.05888090655207634 + 2.0 * 6.268951416015625
Epoch 1020, val loss: 1.067584753036499
Epoch 1030, training loss: 12.5988187789917 = 0.05687399581074715 + 2.0 * 6.27097225189209
Epoch 1030, val loss: 1.0746937990188599
Epoch 1040, training loss: 12.591870307922363 = 0.05496341362595558 + 2.0 * 6.268453598022461
Epoch 1040, val loss: 1.0816574096679688
Epoch 1050, training loss: 12.586766242980957 = 0.05313799902796745 + 2.0 * 6.266814231872559
Epoch 1050, val loss: 1.088706612586975
Epoch 1060, training loss: 12.581660270690918 = 0.05138558894395828 + 2.0 * 6.265137195587158
Epoch 1060, val loss: 1.0956594944000244
Epoch 1070, training loss: 12.578693389892578 = 0.04970264062285423 + 2.0 * 6.264495372772217
Epoch 1070, val loss: 1.1025617122650146
Epoch 1080, training loss: 12.609341621398926 = 0.048085469752550125 + 2.0 * 6.280628204345703
Epoch 1080, val loss: 1.1092272996902466
Epoch 1090, training loss: 12.582633018493652 = 0.04655974730849266 + 2.0 * 6.268036842346191
Epoch 1090, val loss: 1.1158424615859985
Epoch 1100, training loss: 12.571599006652832 = 0.04509245231747627 + 2.0 * 6.263253211975098
Epoch 1100, val loss: 1.1226228475570679
Epoch 1110, training loss: 12.569091796875 = 0.043684039264917374 + 2.0 * 6.262703895568848
Epoch 1110, val loss: 1.1292426586151123
Epoch 1120, training loss: 12.567656517028809 = 0.042333733290433884 + 2.0 * 6.262661457061768
Epoch 1120, val loss: 1.1358470916748047
Epoch 1130, training loss: 12.567140579223633 = 0.041037097573280334 + 2.0 * 6.263051509857178
Epoch 1130, val loss: 1.1422468423843384
Epoch 1140, training loss: 12.56310749053955 = 0.03979708254337311 + 2.0 * 6.261655330657959
Epoch 1140, val loss: 1.1486955881118774
Epoch 1150, training loss: 12.56943130493164 = 0.03861425444483757 + 2.0 * 6.265408515930176
Epoch 1150, val loss: 1.155015230178833
Epoch 1160, training loss: 12.557355880737305 = 0.03747792914509773 + 2.0 * 6.259939193725586
Epoch 1160, val loss: 1.161260724067688
Epoch 1170, training loss: 12.554573059082031 = 0.03638671711087227 + 2.0 * 6.259093284606934
Epoch 1170, val loss: 1.1675156354904175
Epoch 1180, training loss: 12.551941871643066 = 0.03533662483096123 + 2.0 * 6.258302688598633
Epoch 1180, val loss: 1.1737085580825806
Epoch 1190, training loss: 12.556146621704102 = 0.0343242771923542 + 2.0 * 6.260910987854004
Epoch 1190, val loss: 1.1797832250595093
Epoch 1200, training loss: 12.552968978881836 = 0.033352080732584 + 2.0 * 6.259808540344238
Epoch 1200, val loss: 1.1855905055999756
Epoch 1210, training loss: 12.548617362976074 = 0.03242408484220505 + 2.0 * 6.258096694946289
Epoch 1210, val loss: 1.191611409187317
Epoch 1220, training loss: 12.545228004455566 = 0.031531814485788345 + 2.0 * 6.256847858428955
Epoch 1220, val loss: 1.1975512504577637
Epoch 1230, training loss: 12.546645164489746 = 0.03066857159137726 + 2.0 * 6.257988452911377
Epoch 1230, val loss: 1.2033748626708984
Epoch 1240, training loss: 12.541749954223633 = 0.029838798567652702 + 2.0 * 6.255955696105957
Epoch 1240, val loss: 1.208919644355774
Epoch 1250, training loss: 12.540215492248535 = 0.029045190662145615 + 2.0 * 6.255585193634033
Epoch 1250, val loss: 1.2147009372711182
Epoch 1260, training loss: 12.538222312927246 = 0.02828107587993145 + 2.0 * 6.254970550537109
Epoch 1260, val loss: 1.2203539609909058
Epoch 1270, training loss: 12.561930656433105 = 0.027546066790819168 + 2.0 * 6.267192363739014
Epoch 1270, val loss: 1.2258548736572266
Epoch 1280, training loss: 12.541828155517578 = 0.026831122115254402 + 2.0 * 6.257498741149902
Epoch 1280, val loss: 1.2310386896133423
Epoch 1290, training loss: 12.53356647491455 = 0.026152437552809715 + 2.0 * 6.253706932067871
Epoch 1290, val loss: 1.236586332321167
Epoch 1300, training loss: 12.531996726989746 = 0.025494305416941643 + 2.0 * 6.253251075744629
Epoch 1300, val loss: 1.2419441938400269
Epoch 1310, training loss: 12.54455280303955 = 0.02485797554254532 + 2.0 * 6.259847640991211
Epoch 1310, val loss: 1.2470732927322388
Epoch 1320, training loss: 12.532164573669434 = 0.0242481529712677 + 2.0 * 6.253958225250244
Epoch 1320, val loss: 1.2522977590560913
Epoch 1330, training loss: 12.52720832824707 = 0.02365720458328724 + 2.0 * 6.251775741577148
Epoch 1330, val loss: 1.2575340270996094
Epoch 1340, training loss: 12.526111602783203 = 0.023086082190275192 + 2.0 * 6.25151252746582
Epoch 1340, val loss: 1.2626092433929443
Epoch 1350, training loss: 12.539794921875 = 0.02253439836204052 + 2.0 * 6.258630275726318
Epoch 1350, val loss: 1.2674903869628906
Epoch 1360, training loss: 12.52306079864502 = 0.02200428396463394 + 2.0 * 6.250528335571289
Epoch 1360, val loss: 1.2724273204803467
Epoch 1370, training loss: 12.522552490234375 = 0.021494310349225998 + 2.0 * 6.2505292892456055
Epoch 1370, val loss: 1.2774677276611328
Epoch 1380, training loss: 12.51997184753418 = 0.020999135449528694 + 2.0 * 6.249486446380615
Epoch 1380, val loss: 1.2823255062103271
Epoch 1390, training loss: 12.524730682373047 = 0.0205190721899271 + 2.0 * 6.252105712890625
Epoch 1390, val loss: 1.2870807647705078
Epoch 1400, training loss: 12.521469116210938 = 0.020055370405316353 + 2.0 * 6.250706672668457
Epoch 1400, val loss: 1.2916922569274902
Epoch 1410, training loss: 12.524497032165527 = 0.01961008459329605 + 2.0 * 6.252443313598633
Epoch 1410, val loss: 1.2962578535079956
Epoch 1420, training loss: 12.516289710998535 = 0.019183296710252762 + 2.0 * 6.248553276062012
Epoch 1420, val loss: 1.3010919094085693
Epoch 1430, training loss: 12.51378059387207 = 0.018766580149531364 + 2.0 * 6.247507095336914
Epoch 1430, val loss: 1.305692434310913
Epoch 1440, training loss: 12.512823104858398 = 0.01836167648434639 + 2.0 * 6.247230529785156
Epoch 1440, val loss: 1.310200810432434
Epoch 1450, training loss: 12.518765449523926 = 0.017968734726309776 + 2.0 * 6.2503981590271
Epoch 1450, val loss: 1.3146337270736694
Epoch 1460, training loss: 12.513543128967285 = 0.01759023219347 + 2.0 * 6.247976303100586
Epoch 1460, val loss: 1.3190110921859741
Epoch 1470, training loss: 12.512392044067383 = 0.017224453389644623 + 2.0 * 6.247583866119385
Epoch 1470, val loss: 1.323380947113037
Epoch 1480, training loss: 12.510377883911133 = 0.01686972565948963 + 2.0 * 6.246754169464111
Epoch 1480, val loss: 1.3277448415756226
Epoch 1490, training loss: 12.507822036743164 = 0.016526106745004654 + 2.0 * 6.24564790725708
Epoch 1490, val loss: 1.3319538831710815
Epoch 1500, training loss: 12.50700569152832 = 0.016191810369491577 + 2.0 * 6.2454071044921875
Epoch 1500, val loss: 1.3362189531326294
Epoch 1510, training loss: 12.512717247009277 = 0.015868233516812325 + 2.0 * 6.248424530029297
Epoch 1510, val loss: 1.3403879404067993
Epoch 1520, training loss: 12.506074905395508 = 0.015552869997918606 + 2.0 * 6.245261192321777
Epoch 1520, val loss: 1.3444206714630127
Epoch 1530, training loss: 12.5211820602417 = 0.01524939201772213 + 2.0 * 6.252966403961182
Epoch 1530, val loss: 1.348394513130188
Epoch 1540, training loss: 12.507722854614258 = 0.014954590238630772 + 2.0 * 6.246384143829346
Epoch 1540, val loss: 1.3522753715515137
Epoch 1550, training loss: 12.501504898071289 = 0.01466982252895832 + 2.0 * 6.243417739868164
Epoch 1550, val loss: 1.3563591241836548
Epoch 1560, training loss: 12.49891471862793 = 0.014392674900591373 + 2.0 * 6.242260932922363
Epoch 1560, val loss: 1.360289454460144
Epoch 1570, training loss: 12.500475883483887 = 0.014121945016086102 + 2.0 * 6.2431769371032715
Epoch 1570, val loss: 1.364149808883667
Epoch 1580, training loss: 12.506794929504395 = 0.013858129270374775 + 2.0 * 6.246468544006348
Epoch 1580, val loss: 1.3677576780319214
Epoch 1590, training loss: 12.49697208404541 = 0.013604369014501572 + 2.0 * 6.2416839599609375
Epoch 1590, val loss: 1.371593952178955
Epoch 1600, training loss: 12.496378898620605 = 0.013357408344745636 + 2.0 * 6.24151086807251
Epoch 1600, val loss: 1.3753917217254639
Epoch 1610, training loss: 12.514892578125 = 0.013118301518261433 + 2.0 * 6.250886917114258
Epoch 1610, val loss: 1.379076361656189
Epoch 1620, training loss: 12.500773429870605 = 0.01288190484046936 + 2.0 * 6.243945598602295
Epoch 1620, val loss: 1.3823961019515991
Epoch 1630, training loss: 12.495076179504395 = 0.01265709102153778 + 2.0 * 6.241209506988525
Epoch 1630, val loss: 1.3862031698226929
Epoch 1640, training loss: 12.492319107055664 = 0.012435931712388992 + 2.0 * 6.239941596984863
Epoch 1640, val loss: 1.3897976875305176
Epoch 1650, training loss: 12.492345809936523 = 0.012220319360494614 + 2.0 * 6.240062713623047
Epoch 1650, val loss: 1.3932700157165527
Epoch 1660, training loss: 12.50749397277832 = 0.012010786682367325 + 2.0 * 6.24774169921875
Epoch 1660, val loss: 1.396624207496643
Epoch 1670, training loss: 12.498641014099121 = 0.011805655434727669 + 2.0 * 6.243417739868164
Epoch 1670, val loss: 1.3998829126358032
Epoch 1680, training loss: 12.488911628723145 = 0.011607944965362549 + 2.0 * 6.238651752471924
Epoch 1680, val loss: 1.4034110307693481
Epoch 1690, training loss: 12.489831924438477 = 0.011416148394346237 + 2.0 * 6.239207744598389
Epoch 1690, val loss: 1.4068113565444946
Epoch 1700, training loss: 12.489859580993652 = 0.01122735720127821 + 2.0 * 6.239315986633301
Epoch 1700, val loss: 1.4100910425186157
Epoch 1710, training loss: 12.495939254760742 = 0.011043553240597248 + 2.0 * 6.242447853088379
Epoch 1710, val loss: 1.413223385810852
Epoch 1720, training loss: 12.490874290466309 = 0.010863943956792355 + 2.0 * 6.240005016326904
Epoch 1720, val loss: 1.416458249092102
Epoch 1730, training loss: 12.486897468566895 = 0.01068966370075941 + 2.0 * 6.238103866577148
Epoch 1730, val loss: 1.419683814048767
Epoch 1740, training loss: 12.499459266662598 = 0.010519010946154594 + 2.0 * 6.244470119476318
Epoch 1740, val loss: 1.4227170944213867
Epoch 1750, training loss: 12.491026878356934 = 0.01035521924495697 + 2.0 * 6.240335941314697
Epoch 1750, val loss: 1.4257310628890991
Epoch 1760, training loss: 12.486063957214355 = 0.010194141417741776 + 2.0 * 6.2379350662231445
Epoch 1760, val loss: 1.4287186861038208
Epoch 1770, training loss: 12.48180103302002 = 0.010038328357040882 + 2.0 * 6.235881328582764
Epoch 1770, val loss: 1.4319343566894531
Epoch 1780, training loss: 12.480587005615234 = 0.00988430343568325 + 2.0 * 6.2353515625
Epoch 1780, val loss: 1.4349151849746704
Epoch 1790, training loss: 12.480507850646973 = 0.00973369088023901 + 2.0 * 6.235386848449707
Epoch 1790, val loss: 1.4378957748413086
Epoch 1800, training loss: 12.497201919555664 = 0.00958672259002924 + 2.0 * 6.243807792663574
Epoch 1800, val loss: 1.440740704536438
Epoch 1810, training loss: 12.492831230163574 = 0.009443622082471848 + 2.0 * 6.24169397354126
Epoch 1810, val loss: 1.4434828758239746
Epoch 1820, training loss: 12.478780746459961 = 0.009304258972406387 + 2.0 * 6.234738349914551
Epoch 1820, val loss: 1.4463602304458618
Epoch 1830, training loss: 12.479076385498047 = 0.0091686537489295 + 2.0 * 6.234953880310059
Epoch 1830, val loss: 1.4492323398590088
Epoch 1840, training loss: 12.478534698486328 = 0.009036142379045486 + 2.0 * 6.2347493171691895
Epoch 1840, val loss: 1.4520403146743774
Epoch 1850, training loss: 12.496716499328613 = 0.008906850591301918 + 2.0 * 6.2439045906066895
Epoch 1850, val loss: 1.4546176195144653
Epoch 1860, training loss: 12.476707458496094 = 0.008779443800449371 + 2.0 * 6.233963966369629
Epoch 1860, val loss: 1.4571382999420166
Epoch 1870, training loss: 12.476304054260254 = 0.008657285943627357 + 2.0 * 6.233823299407959
Epoch 1870, val loss: 1.4600285291671753
Epoch 1880, training loss: 12.474623680114746 = 0.008536520414054394 + 2.0 * 6.233043670654297
Epoch 1880, val loss: 1.4626703262329102
Epoch 1890, training loss: 12.477667808532715 = 0.008417367935180664 + 2.0 * 6.234625339508057
Epoch 1890, val loss: 1.465283989906311
Epoch 1900, training loss: 12.479134559631348 = 0.008301259018480778 + 2.0 * 6.235416412353516
Epoch 1900, val loss: 1.4675575494766235
Epoch 1910, training loss: 12.473970413208008 = 0.008188994601368904 + 2.0 * 6.232890605926514
Epoch 1910, val loss: 1.4702057838439941
Epoch 1920, training loss: 12.474241256713867 = 0.008079136721789837 + 2.0 * 6.233080863952637
Epoch 1920, val loss: 1.4728139638900757
Epoch 1930, training loss: 12.478925704956055 = 0.007970910519361496 + 2.0 * 6.235477447509766
Epoch 1930, val loss: 1.475339412689209
Epoch 1940, training loss: 12.47182559967041 = 0.007865025661885738 + 2.0 * 6.231980323791504
Epoch 1940, val loss: 1.4776545763015747
Epoch 1950, training loss: 12.471841812133789 = 0.007761477492749691 + 2.0 * 6.2320404052734375
Epoch 1950, val loss: 1.480157732963562
Epoch 1960, training loss: 12.47074031829834 = 0.007659965194761753 + 2.0 * 6.231540203094482
Epoch 1960, val loss: 1.482627511024475
Epoch 1970, training loss: 12.48086166381836 = 0.007559942081570625 + 2.0 * 6.2366509437561035
Epoch 1970, val loss: 1.4848445653915405
Epoch 1980, training loss: 12.469889640808105 = 0.007463333662599325 + 2.0 * 6.231213092803955
Epoch 1980, val loss: 1.4871819019317627
Epoch 1990, training loss: 12.470357894897461 = 0.00736828101798892 + 2.0 * 6.231494903564453
Epoch 1990, val loss: 1.4895490407943726
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7740740740740741
0.8386926726410122
=== training gcn model ===
Epoch 0, training loss: 19.150344848632812 = 1.9566056728363037 + 2.0 * 8.596869468688965
Epoch 0, val loss: 1.9553166627883911
Epoch 10, training loss: 19.139902114868164 = 1.9465478658676147 + 2.0 * 8.59667682647705
Epoch 10, val loss: 1.9449386596679688
Epoch 20, training loss: 19.124691009521484 = 1.9346091747283936 + 2.0 * 8.595041275024414
Epoch 20, val loss: 1.9322376251220703
Epoch 30, training loss: 19.079919815063477 = 1.9188563823699951 + 2.0 * 8.58053207397461
Epoch 30, val loss: 1.9152750968933105
Epoch 40, training loss: 18.865028381347656 = 1.8986538648605347 + 2.0 * 8.483187675476074
Epoch 40, val loss: 1.8940632343292236
Epoch 50, training loss: 17.94775390625 = 1.8768272399902344 + 2.0 * 8.035463333129883
Epoch 50, val loss: 1.8715921640396118
Epoch 60, training loss: 17.052993774414062 = 1.8561779260635376 + 2.0 * 7.598407745361328
Epoch 60, val loss: 1.8517606258392334
Epoch 70, training loss: 16.350866317749023 = 1.8394992351531982 + 2.0 * 7.255683422088623
Epoch 70, val loss: 1.8361761569976807
Epoch 80, training loss: 15.942863464355469 = 1.8217499256134033 + 2.0 * 7.060556888580322
Epoch 80, val loss: 1.8190077543258667
Epoch 90, training loss: 15.672892570495605 = 1.8054444789886475 + 2.0 * 6.9337239265441895
Epoch 90, val loss: 1.803938627243042
Epoch 100, training loss: 15.390800476074219 = 1.789656639099121 + 2.0 * 6.800571918487549
Epoch 100, val loss: 1.789623498916626
Epoch 110, training loss: 15.20809555053711 = 1.775364637374878 + 2.0 * 6.716365337371826
Epoch 110, val loss: 1.7767119407653809
Epoch 120, training loss: 15.087027549743652 = 1.7611960172653198 + 2.0 * 6.6629157066345215
Epoch 120, val loss: 1.7642168998718262
Epoch 130, training loss: 14.994904518127441 = 1.7455800771713257 + 2.0 * 6.624662399291992
Epoch 130, val loss: 1.7507013082504272
Epoch 140, training loss: 14.918096542358398 = 1.7285979986190796 + 2.0 * 6.594749450683594
Epoch 140, val loss: 1.7363390922546387
Epoch 150, training loss: 14.856383323669434 = 1.7107064723968506 + 2.0 * 6.572838306427002
Epoch 150, val loss: 1.7215019464492798
Epoch 160, training loss: 14.781167030334473 = 1.6917550563812256 + 2.0 * 6.544705867767334
Epoch 160, val loss: 1.7057600021362305
Epoch 170, training loss: 14.712224006652832 = 1.6711188554763794 + 2.0 * 6.520552635192871
Epoch 170, val loss: 1.6887696981430054
Epoch 180, training loss: 14.648760795593262 = 1.6482845544815063 + 2.0 * 6.500237941741943
Epoch 180, val loss: 1.6701629161834717
Epoch 190, training loss: 14.597379684448242 = 1.6228915452957153 + 2.0 * 6.487244129180908
Epoch 190, val loss: 1.6496038436889648
Epoch 200, training loss: 14.53969955444336 = 1.5948600769042969 + 2.0 * 6.472419738769531
Epoch 200, val loss: 1.6270151138305664
Epoch 210, training loss: 14.48663330078125 = 1.5643315315246582 + 2.0 * 6.461150646209717
Epoch 210, val loss: 1.6023889780044556
Epoch 220, training loss: 14.432101249694824 = 1.5310848951339722 + 2.0 * 6.450508117675781
Epoch 220, val loss: 1.5755364894866943
Epoch 230, training loss: 14.382946014404297 = 1.4950904846191406 + 2.0 * 6.443927764892578
Epoch 230, val loss: 1.546521544456482
Epoch 240, training loss: 14.326215744018555 = 1.4567711353302002 + 2.0 * 6.434722423553467
Epoch 240, val loss: 1.515839695930481
Epoch 250, training loss: 14.267560958862305 = 1.416566014289856 + 2.0 * 6.425497531890869
Epoch 250, val loss: 1.4835498332977295
Epoch 260, training loss: 14.209634780883789 = 1.3743046522140503 + 2.0 * 6.417665004730225
Epoch 260, val loss: 1.4495853185653687
Epoch 270, training loss: 14.153449058532715 = 1.330139398574829 + 2.0 * 6.411654949188232
Epoch 270, val loss: 1.414065957069397
Epoch 280, training loss: 14.09776496887207 = 1.284629225730896 + 2.0 * 6.4065680503845215
Epoch 280, val loss: 1.3776719570159912
Epoch 290, training loss: 14.033687591552734 = 1.2388464212417603 + 2.0 * 6.397420406341553
Epoch 290, val loss: 1.3411012887954712
Epoch 300, training loss: 13.974283218383789 = 1.1926932334899902 + 2.0 * 6.3907952308654785
Epoch 300, val loss: 1.3044054508209229
Epoch 310, training loss: 13.927824020385742 = 1.146517276763916 + 2.0 * 6.390653610229492
Epoch 310, val loss: 1.2678519487380981
Epoch 320, training loss: 13.86526870727539 = 1.101179838180542 + 2.0 * 6.382044315338135
Epoch 320, val loss: 1.2321807146072388
Epoch 330, training loss: 13.810105323791504 = 1.0566914081573486 + 2.0 * 6.376707077026367
Epoch 330, val loss: 1.197442889213562
Epoch 340, training loss: 13.759111404418945 = 1.013372778892517 + 2.0 * 6.372869491577148
Epoch 340, val loss: 1.1638373136520386
Epoch 350, training loss: 13.71000862121582 = 0.9716868996620178 + 2.0 * 6.3691606521606445
Epoch 350, val loss: 1.131661295890808
Epoch 360, training loss: 13.661337852478027 = 0.9314319491386414 + 2.0 * 6.36495304107666
Epoch 360, val loss: 1.100821614265442
Epoch 370, training loss: 13.613859176635742 = 0.8924726247787476 + 2.0 * 6.360693454742432
Epoch 370, val loss: 1.0710561275482178
Epoch 380, training loss: 13.569342613220215 = 0.8548566102981567 + 2.0 * 6.357243061065674
Epoch 380, val loss: 1.042492389678955
Epoch 390, training loss: 13.537720680236816 = 0.8190155029296875 + 2.0 * 6.3593525886535645
Epoch 390, val loss: 1.015344500541687
Epoch 400, training loss: 13.486747741699219 = 0.7851960062980652 + 2.0 * 6.350775718688965
Epoch 400, val loss: 0.9900745749473572
Epoch 410, training loss: 13.44885540008545 = 0.7531070113182068 + 2.0 * 6.347874164581299
Epoch 410, val loss: 0.9663822650909424
Epoch 420, training loss: 13.417909622192383 = 0.7226725816726685 + 2.0 * 6.347618579864502
Epoch 420, val loss: 0.9440522789955139
Epoch 430, training loss: 13.377008438110352 = 0.693783700466156 + 2.0 * 6.341612339019775
Epoch 430, val loss: 0.9232164621353149
Epoch 440, training loss: 13.342345237731934 = 0.6661519408226013 + 2.0 * 6.338096618652344
Epoch 440, val loss: 0.9035815596580505
Epoch 450, training loss: 13.309640884399414 = 0.6396146416664124 + 2.0 * 6.335012912750244
Epoch 450, val loss: 0.8848732113838196
Epoch 460, training loss: 13.281546592712402 = 0.6139414310455322 + 2.0 * 6.333802700042725
Epoch 460, val loss: 0.8670417666435242
Epoch 470, training loss: 13.250998497009277 = 0.5890899300575256 + 2.0 * 6.330954074859619
Epoch 470, val loss: 0.8501059412956238
Epoch 480, training loss: 13.2196626663208 = 0.5649296045303345 + 2.0 * 6.327366352081299
Epoch 480, val loss: 0.833813488483429
Epoch 490, training loss: 13.192066192626953 = 0.5413179397583008 + 2.0 * 6.325374126434326
Epoch 490, val loss: 0.8182640075683594
Epoch 500, training loss: 13.171806335449219 = 0.518173098564148 + 2.0 * 6.326816558837891
Epoch 500, val loss: 0.8033304214477539
Epoch 510, training loss: 13.13900375366211 = 0.49543455243110657 + 2.0 * 6.321784496307373
Epoch 510, val loss: 0.7890391945838928
Epoch 520, training loss: 13.110796928405762 = 0.47329944372177124 + 2.0 * 6.318748950958252
Epoch 520, val loss: 0.7755370736122131
Epoch 530, training loss: 13.082876205444336 = 0.45164045691490173 + 2.0 * 6.31561803817749
Epoch 530, val loss: 0.7626753449440002
Epoch 540, training loss: 13.068825721740723 = 0.43042856454849243 + 2.0 * 6.3191986083984375
Epoch 540, val loss: 0.7505632042884827
Epoch 550, training loss: 13.045575141906738 = 0.41003236174583435 + 2.0 * 6.3177714347839355
Epoch 550, val loss: 0.7389358282089233
Epoch 560, training loss: 13.012737274169922 = 0.3902443051338196 + 2.0 * 6.311246395111084
Epoch 560, val loss: 0.7283688187599182
Epoch 570, training loss: 12.988369941711426 = 0.3710772693157196 + 2.0 * 6.308646202087402
Epoch 570, val loss: 0.7184661030769348
Epoch 580, training loss: 12.966225624084473 = 0.35248881578445435 + 2.0 * 6.306868553161621
Epoch 580, val loss: 0.709179699420929
Epoch 590, training loss: 12.952179908752441 = 0.3344504237174988 + 2.0 * 6.308864593505859
Epoch 590, val loss: 0.7004258632659912
Epoch 600, training loss: 12.933939933776855 = 0.31709617376327515 + 2.0 * 6.308422088623047
Epoch 600, val loss: 0.6922755241394043
Epoch 610, training loss: 12.905950546264648 = 0.3004143536090851 + 2.0 * 6.302768230438232
Epoch 610, val loss: 0.684848964214325
Epoch 620, training loss: 12.887080192565918 = 0.2844341993331909 + 2.0 * 6.301322937011719
Epoch 620, val loss: 0.677983283996582
Epoch 630, training loss: 12.883106231689453 = 0.2691290080547333 + 2.0 * 6.306988716125488
Epoch 630, val loss: 0.67171710729599
Epoch 640, training loss: 12.855152130126953 = 0.25466611981391907 + 2.0 * 6.300242900848389
Epoch 640, val loss: 0.6659966707229614
Epoch 650, training loss: 12.835182189941406 = 0.24087856709957123 + 2.0 * 6.297152042388916
Epoch 650, val loss: 0.6609765887260437
Epoch 660, training loss: 12.83315658569336 = 0.22781473398208618 + 2.0 * 6.302670955657959
Epoch 660, val loss: 0.6564385294914246
Epoch 670, training loss: 12.805261611938477 = 0.21538624167442322 + 2.0 * 6.294937610626221
Epoch 670, val loss: 0.6525593996047974
Epoch 680, training loss: 12.79032039642334 = 0.2036965787410736 + 2.0 * 6.293312072753906
Epoch 680, val loss: 0.649198591709137
Epoch 690, training loss: 12.77798843383789 = 0.1926303207874298 + 2.0 * 6.2926788330078125
Epoch 690, val loss: 0.6463227868080139
Epoch 700, training loss: 12.78079891204834 = 0.18216891586780548 + 2.0 * 6.299314975738525
Epoch 700, val loss: 0.6439622044563293
Epoch 710, training loss: 12.76055908203125 = 0.17222270369529724 + 2.0 * 6.294167995452881
Epoch 710, val loss: 0.6421783566474915
Epoch 720, training loss: 12.740233421325684 = 0.16295479238033295 + 2.0 * 6.288639545440674
Epoch 720, val loss: 0.6408370137214661
Epoch 730, training loss: 12.730864524841309 = 0.15420036017894745 + 2.0 * 6.288331985473633
Epoch 730, val loss: 0.6399598717689514
Epoch 740, training loss: 12.721860885620117 = 0.1459665298461914 + 2.0 * 6.287947177886963
Epoch 740, val loss: 0.6395060420036316
Epoch 750, training loss: 12.713356018066406 = 0.1382560282945633 + 2.0 * 6.28754997253418
Epoch 750, val loss: 0.6393852233886719
Epoch 760, training loss: 12.70154857635498 = 0.1310270130634308 + 2.0 * 6.2852606773376465
Epoch 760, val loss: 0.6396481394767761
Epoch 770, training loss: 12.692183494567871 = 0.12424235045909882 + 2.0 * 6.283970355987549
Epoch 770, val loss: 0.6403093338012695
Epoch 780, training loss: 12.706891059875488 = 0.11784853786230087 + 2.0 * 6.294521331787109
Epoch 780, val loss: 0.6413212418556213
Epoch 790, training loss: 12.680074691772461 = 0.11193977296352386 + 2.0 * 6.284067630767822
Epoch 790, val loss: 0.6425238251686096
Epoch 800, training loss: 12.67054271697998 = 0.106382817029953 + 2.0 * 6.282080173492432
Epoch 800, val loss: 0.6440915465354919
Epoch 810, training loss: 12.659116744995117 = 0.1011609360575676 + 2.0 * 6.278977870941162
Epoch 810, val loss: 0.6459843516349792
Epoch 820, training loss: 12.654121398925781 = 0.09623119980096817 + 2.0 * 6.278944969177246
Epoch 820, val loss: 0.6481476426124573
Epoch 830, training loss: 12.658689498901367 = 0.09159284085035324 + 2.0 * 6.283548355102539
Epoch 830, val loss: 0.6504572033882141
Epoch 840, training loss: 12.641731262207031 = 0.08726613223552704 + 2.0 * 6.277232646942139
Epoch 840, val loss: 0.6529090404510498
Epoch 850, training loss: 12.635767936706543 = 0.08320112526416779 + 2.0 * 6.276283264160156
Epoch 850, val loss: 0.6555532217025757
Epoch 860, training loss: 12.633406639099121 = 0.07936684787273407 + 2.0 * 6.27701997756958
Epoch 860, val loss: 0.6583981513977051
Epoch 870, training loss: 12.631165504455566 = 0.07576876878738403 + 2.0 * 6.277698516845703
Epoch 870, val loss: 0.6613264679908752
Epoch 880, training loss: 12.622031211853027 = 0.07241998612880707 + 2.0 * 6.274805545806885
Epoch 880, val loss: 0.6642839312553406
Epoch 890, training loss: 12.614276885986328 = 0.0692649632692337 + 2.0 * 6.272505760192871
Epoch 890, val loss: 0.6673778295516968
Epoch 900, training loss: 12.609345436096191 = 0.06629777699708939 + 2.0 * 6.271523952484131
Epoch 900, val loss: 0.6705796122550964
Epoch 910, training loss: 12.604752540588379 = 0.06349659711122513 + 2.0 * 6.270627975463867
Epoch 910, val loss: 0.673897385597229
Epoch 920, training loss: 12.606124877929688 = 0.060851190239191055 + 2.0 * 6.272636890411377
Epoch 920, val loss: 0.6772636771202087
Epoch 930, training loss: 12.59892749786377 = 0.05835266783833504 + 2.0 * 6.27028751373291
Epoch 930, val loss: 0.6806555986404419
Epoch 940, training loss: 12.598221778869629 = 0.05600916966795921 + 2.0 * 6.271106243133545
Epoch 940, val loss: 0.6840211153030396
Epoch 950, training loss: 12.601499557495117 = 0.053804364055395126 + 2.0 * 6.273847579956055
Epoch 950, val loss: 0.6874344348907471
Epoch 960, training loss: 12.588321685791016 = 0.05169845372438431 + 2.0 * 6.268311500549316
Epoch 960, val loss: 0.6909359097480774
Epoch 970, training loss: 12.582841873168945 = 0.04971962049603462 + 2.0 * 6.266561031341553
Epoch 970, val loss: 0.6944655179977417
Epoch 980, training loss: 12.581022262573242 = 0.04783664643764496 + 2.0 * 6.266592979431152
Epoch 980, val loss: 0.6980352997779846
Epoch 990, training loss: 12.583466529846191 = 0.04605230689048767 + 2.0 * 6.268707275390625
Epoch 990, val loss: 0.701586902141571
Epoch 1000, training loss: 12.572794914245605 = 0.04436105117201805 + 2.0 * 6.264216899871826
Epoch 1000, val loss: 0.7051600217819214
Epoch 1010, training loss: 12.569844245910645 = 0.04275761544704437 + 2.0 * 6.263543128967285
Epoch 1010, val loss: 0.7087236642837524
Epoch 1020, training loss: 12.572155952453613 = 0.04122242331504822 + 2.0 * 6.265466690063477
Epoch 1020, val loss: 0.712364137172699
Epoch 1030, training loss: 12.573431015014648 = 0.039768707007169724 + 2.0 * 6.266830921173096
Epoch 1030, val loss: 0.7159435153007507
Epoch 1040, training loss: 12.564388275146484 = 0.038391757756471634 + 2.0 * 6.262998104095459
Epoch 1040, val loss: 0.7194861173629761
Epoch 1050, training loss: 12.56069564819336 = 0.03708824887871742 + 2.0 * 6.26180362701416
Epoch 1050, val loss: 0.7230662107467651
Epoch 1060, training loss: 12.557543754577637 = 0.03584238141775131 + 2.0 * 6.26085090637207
Epoch 1060, val loss: 0.7266688942909241
Epoch 1070, training loss: 12.564556121826172 = 0.03465540334582329 + 2.0 * 6.264950275421143
Epoch 1070, val loss: 0.7302877306938171
Epoch 1080, training loss: 12.558816909790039 = 0.03351547196507454 + 2.0 * 6.262650489807129
Epoch 1080, val loss: 0.7338311672210693
Epoch 1090, training loss: 12.55126667022705 = 0.03244137018918991 + 2.0 * 6.25941276550293
Epoch 1090, val loss: 0.7373558878898621
Epoch 1100, training loss: 12.547995567321777 = 0.03141307830810547 + 2.0 * 6.258291244506836
Epoch 1100, val loss: 0.7409268021583557
Epoch 1110, training loss: 12.545113563537598 = 0.030431412160396576 + 2.0 * 6.257340908050537
Epoch 1110, val loss: 0.7445237040519714
Epoch 1120, training loss: 12.548657417297363 = 0.02949230931699276 + 2.0 * 6.25958251953125
Epoch 1120, val loss: 0.7480905055999756
Epoch 1130, training loss: 12.545241355895996 = 0.028595132753252983 + 2.0 * 6.2583231925964355
Epoch 1130, val loss: 0.7515906691551208
Epoch 1140, training loss: 12.541754722595215 = 0.027739539742469788 + 2.0 * 6.257007598876953
Epoch 1140, val loss: 0.755074143409729
Epoch 1150, training loss: 12.543886184692383 = 0.026920640841126442 + 2.0 * 6.258482933044434
Epoch 1150, val loss: 0.7585646510124207
Epoch 1160, training loss: 12.538558006286621 = 0.0261353962123394 + 2.0 * 6.256211280822754
Epoch 1160, val loss: 0.7620373964309692
Epoch 1170, training loss: 12.535797119140625 = 0.025382881984114647 + 2.0 * 6.255207061767578
Epoch 1170, val loss: 0.7654603719711304
Epoch 1180, training loss: 12.539505958557129 = 0.02466588094830513 + 2.0 * 6.257420063018799
Epoch 1180, val loss: 0.7688735127449036
Epoch 1190, training loss: 12.53087043762207 = 0.02398129366338253 + 2.0 * 6.253444671630859
Epoch 1190, val loss: 0.7722788453102112
Epoch 1200, training loss: 12.52919864654541 = 0.023324022069573402 + 2.0 * 6.252937316894531
Epoch 1200, val loss: 0.775667130947113
Epoch 1210, training loss: 12.540336608886719 = 0.022692829370498657 + 2.0 * 6.258821964263916
Epoch 1210, val loss: 0.7790665030479431
Epoch 1220, training loss: 12.535589218139648 = 0.02208351530134678 + 2.0 * 6.256752967834473
Epoch 1220, val loss: 0.7823311686515808
Epoch 1230, training loss: 12.527420043945312 = 0.02150317095220089 + 2.0 * 6.252958297729492
Epoch 1230, val loss: 0.7856064438819885
Epoch 1240, training loss: 12.524311065673828 = 0.02095072716474533 + 2.0 * 6.251680374145508
Epoch 1240, val loss: 0.7888777852058411
Epoch 1250, training loss: 12.522339820861816 = 0.02041652239859104 + 2.0 * 6.250961780548096
Epoch 1250, val loss: 0.7921652793884277
Epoch 1260, training loss: 12.525345802307129 = 0.019902464002370834 + 2.0 * 6.252721786499023
Epoch 1260, val loss: 0.7954291701316833
Epoch 1270, training loss: 12.519740104675293 = 0.019402969628572464 + 2.0 * 6.250168800354004
Epoch 1270, val loss: 0.7986621856689453
Epoch 1280, training loss: 12.526754379272461 = 0.018923427909612656 + 2.0 * 6.253915309906006
Epoch 1280, val loss: 0.8018085360527039
Epoch 1290, training loss: 12.518310546875 = 0.018469346687197685 + 2.0 * 6.24992036819458
Epoch 1290, val loss: 0.8050234913825989
Epoch 1300, training loss: 12.519275665283203 = 0.018028318881988525 + 2.0 * 6.25062370300293
Epoch 1300, val loss: 0.8081502914428711
Epoch 1310, training loss: 12.514155387878418 = 0.01760437898337841 + 2.0 * 6.248275279998779
Epoch 1310, val loss: 0.8112603425979614
Epoch 1320, training loss: 12.51839542388916 = 0.017195938155055046 + 2.0 * 6.2505998611450195
Epoch 1320, val loss: 0.8143674731254578
Epoch 1330, training loss: 12.512787818908691 = 0.01680091768503189 + 2.0 * 6.247993469238281
Epoch 1330, val loss: 0.8174670338630676
Epoch 1340, training loss: 12.509639739990234 = 0.016422104090452194 + 2.0 * 6.246608734130859
Epoch 1340, val loss: 0.8204696178436279
Epoch 1350, training loss: 12.508749961853027 = 0.016055438667535782 + 2.0 * 6.246347427368164
Epoch 1350, val loss: 0.8235092163085938
Epoch 1360, training loss: 12.531237602233887 = 0.01570473611354828 + 2.0 * 6.257766246795654
Epoch 1360, val loss: 0.8265256881713867
Epoch 1370, training loss: 12.514304161071777 = 0.015357068739831448 + 2.0 * 6.249473571777344
Epoch 1370, val loss: 0.8294050693511963
Epoch 1380, training loss: 12.506487846374512 = 0.01503090187907219 + 2.0 * 6.245728492736816
Epoch 1380, val loss: 0.8323030471801758
Epoch 1390, training loss: 12.503894805908203 = 0.014712098054587841 + 2.0 * 6.244591236114502
Epoch 1390, val loss: 0.8352247476577759
Epoch 1400, training loss: 12.511916160583496 = 0.014405970461666584 + 2.0 * 6.248754978179932
Epoch 1400, val loss: 0.8381451368331909
Epoch 1410, training loss: 12.503329277038574 = 0.014105218462646008 + 2.0 * 6.244612216949463
Epoch 1410, val loss: 0.840965986251831
Epoch 1420, training loss: 12.501580238342285 = 0.01381697691977024 + 2.0 * 6.243881702423096
Epoch 1420, val loss: 0.8437806963920593
Epoch 1430, training loss: 12.501680374145508 = 0.013537080027163029 + 2.0 * 6.2440714836120605
Epoch 1430, val loss: 0.846551775932312
Epoch 1440, training loss: 12.509923934936523 = 0.013267363421618938 + 2.0 * 6.24832820892334
Epoch 1440, val loss: 0.8493279814720154
Epoch 1450, training loss: 12.498892784118652 = 0.013006565161049366 + 2.0 * 6.242943286895752
Epoch 1450, val loss: 0.8520402312278748
Epoch 1460, training loss: 12.512714385986328 = 0.012755708768963814 + 2.0 * 6.249979496002197
Epoch 1460, val loss: 0.8547292947769165
Epoch 1470, training loss: 12.501319885253906 = 0.012509684078395367 + 2.0 * 6.244405269622803
Epoch 1470, val loss: 0.8573738932609558
Epoch 1480, training loss: 12.495861053466797 = 0.012274016626179218 + 2.0 * 6.241793632507324
Epoch 1480, val loss: 0.8600136637687683
Epoch 1490, training loss: 12.500794410705566 = 0.012044661678373814 + 2.0 * 6.244374752044678
Epoch 1490, val loss: 0.8626914620399475
Epoch 1500, training loss: 12.494686126708984 = 0.011820263229310513 + 2.0 * 6.241433143615723
Epoch 1500, val loss: 0.8652734756469727
Epoch 1510, training loss: 12.4940824508667 = 0.011600744910538197 + 2.0 * 6.241240978240967
Epoch 1510, val loss: 0.8678584098815918
Epoch 1520, training loss: 12.490453720092773 = 0.011391513049602509 + 2.0 * 6.23953104019165
Epoch 1520, val loss: 0.8704591989517212
Epoch 1530, training loss: 12.49494457244873 = 0.011187665164470673 + 2.0 * 6.241878509521484
Epoch 1530, val loss: 0.8730378150939941
Epoch 1540, training loss: 12.498393058776855 = 0.010987604968249798 + 2.0 * 6.2437028884887695
Epoch 1540, val loss: 0.8755062818527222
Epoch 1550, training loss: 12.490659713745117 = 0.010794487781822681 + 2.0 * 6.239932537078857
Epoch 1550, val loss: 0.8779696226119995
Epoch 1560, training loss: 12.488516807556152 = 0.010608131997287273 + 2.0 * 6.238954544067383
Epoch 1560, val loss: 0.8803663849830627
Epoch 1570, training loss: 12.48717975616455 = 0.01042706985026598 + 2.0 * 6.238376140594482
Epoch 1570, val loss: 0.8828673362731934
Epoch 1580, training loss: 12.498753547668457 = 0.010249371640384197 + 2.0 * 6.2442522048950195
Epoch 1580, val loss: 0.8853066563606262
Epoch 1590, training loss: 12.494303703308105 = 0.010076150298118591 + 2.0 * 6.2421135902404785
Epoch 1590, val loss: 0.8876952528953552
Epoch 1600, training loss: 12.48733901977539 = 0.009909899905323982 + 2.0 * 6.238714694976807
Epoch 1600, val loss: 0.8899828791618347
Epoch 1610, training loss: 12.507257461547852 = 0.009750758297741413 + 2.0 * 6.248753547668457
Epoch 1610, val loss: 0.8923443555831909
Epoch 1620, training loss: 12.488153457641602 = 0.00958743691444397 + 2.0 * 6.239283084869385
Epoch 1620, val loss: 0.8946122527122498
Epoch 1630, training loss: 12.483904838562012 = 0.009436475113034248 + 2.0 * 6.237234115600586
Epoch 1630, val loss: 0.8968930244445801
Epoch 1640, training loss: 12.481301307678223 = 0.009285645559430122 + 2.0 * 6.2360076904296875
Epoch 1640, val loss: 0.8992131948471069
Epoch 1650, training loss: 12.479949951171875 = 0.009139927104115486 + 2.0 * 6.235404968261719
Epoch 1650, val loss: 0.901498019695282
Epoch 1660, training loss: 12.485302925109863 = 0.008996165357530117 + 2.0 * 6.238153457641602
Epoch 1660, val loss: 0.903800368309021
Epoch 1670, training loss: 12.48336410522461 = 0.008856058120727539 + 2.0 * 6.2372541427612305
Epoch 1670, val loss: 0.9060754179954529
Epoch 1680, training loss: 12.477605819702148 = 0.008720073848962784 + 2.0 * 6.234442710876465
Epoch 1680, val loss: 0.9081912636756897
Epoch 1690, training loss: 12.476489067077637 = 0.008588232100009918 + 2.0 * 6.233950614929199
Epoch 1690, val loss: 0.9103797674179077
Epoch 1700, training loss: 12.479310989379883 = 0.00845886766910553 + 2.0 * 6.23542594909668
Epoch 1700, val loss: 0.9125633239746094
Epoch 1710, training loss: 12.487424850463867 = 0.008331981487572193 + 2.0 * 6.239546298980713
Epoch 1710, val loss: 0.9147303700447083
Epoch 1720, training loss: 12.482612609863281 = 0.008211168460547924 + 2.0 * 6.237200736999512
Epoch 1720, val loss: 0.9168499708175659
Epoch 1730, training loss: 12.477710723876953 = 0.008089997805655003 + 2.0 * 6.2348103523254395
Epoch 1730, val loss: 0.9189320802688599
Epoch 1740, training loss: 12.493940353393555 = 0.00797578226774931 + 2.0 * 6.242982387542725
Epoch 1740, val loss: 0.9210463166236877
Epoch 1750, training loss: 12.479315757751465 = 0.007859991863369942 + 2.0 * 6.235727787017822
Epoch 1750, val loss: 0.9230327606201172
Epoch 1760, training loss: 12.475513458251953 = 0.007750469725579023 + 2.0 * 6.23388147354126
Epoch 1760, val loss: 0.9250543117523193
Epoch 1770, training loss: 12.472137451171875 = 0.007642320357263088 + 2.0 * 6.232247352600098
Epoch 1770, val loss: 0.9271029233932495
Epoch 1780, training loss: 12.470617294311523 = 0.007536596152931452 + 2.0 * 6.231540203094482
Epoch 1780, val loss: 0.9291550517082214
Epoch 1790, training loss: 12.47145938873291 = 0.007432193029671907 + 2.0 * 6.232013702392578
Epoch 1790, val loss: 0.9312049746513367
Epoch 1800, training loss: 12.483705520629883 = 0.007330555003136396 + 2.0 * 6.238187313079834
Epoch 1800, val loss: 0.9332488775253296
Epoch 1810, training loss: 12.472273826599121 = 0.0072313714772462845 + 2.0 * 6.232521057128906
Epoch 1810, val loss: 0.9351977109909058
Epoch 1820, training loss: 12.473101615905762 = 0.007134959567338228 + 2.0 * 6.232983112335205
Epoch 1820, val loss: 0.937161386013031
Epoch 1830, training loss: 12.475600242614746 = 0.007040561176836491 + 2.0 * 6.234279632568359
Epoch 1830, val loss: 0.9391134977340698
Epoch 1840, training loss: 12.471104621887207 = 0.006947153247892857 + 2.0 * 6.232078552246094
Epoch 1840, val loss: 0.9410353899002075
Epoch 1850, training loss: 12.467551231384277 = 0.0068575008772313595 + 2.0 * 6.2303466796875
Epoch 1850, val loss: 0.942925214767456
Epoch 1860, training loss: 12.472199440002441 = 0.006770373322069645 + 2.0 * 6.232714653015137
Epoch 1860, val loss: 0.9448452591896057
Epoch 1870, training loss: 12.48065185546875 = 0.006683379411697388 + 2.0 * 6.2369842529296875
Epoch 1870, val loss: 0.946764349937439
Epoch 1880, training loss: 12.469538688659668 = 0.006597626488655806 + 2.0 * 6.231470584869385
Epoch 1880, val loss: 0.9485282897949219
Epoch 1890, training loss: 12.466461181640625 = 0.006516009569168091 + 2.0 * 6.2299723625183105
Epoch 1890, val loss: 0.9503510594367981
Epoch 1900, training loss: 12.464386940002441 = 0.006435557268559933 + 2.0 * 6.228975772857666
Epoch 1900, val loss: 0.9522108435630798
Epoch 1910, training loss: 12.464131355285645 = 0.006356187630444765 + 2.0 * 6.228887557983398
Epoch 1910, val loss: 0.9540628790855408
Epoch 1920, training loss: 12.485301971435547 = 0.006278672721236944 + 2.0 * 6.239511489868164
Epoch 1920, val loss: 0.9559346437454224
Epoch 1930, training loss: 12.467812538146973 = 0.00620007049292326 + 2.0 * 6.230806350708008
Epoch 1930, val loss: 0.9575875997543335
Epoch 1940, training loss: 12.466350555419922 = 0.006125644315034151 + 2.0 * 6.230112552642822
Epoch 1940, val loss: 0.9593789577484131
Epoch 1950, training loss: 12.472452163696289 = 0.00605227192863822 + 2.0 * 6.2332000732421875
Epoch 1950, val loss: 0.9611291885375977
Epoch 1960, training loss: 12.464247703552246 = 0.005982696078717709 + 2.0 * 6.229132652282715
Epoch 1960, val loss: 0.9628428220748901
Epoch 1970, training loss: 12.460304260253906 = 0.005911793559789658 + 2.0 * 6.227196216583252
Epoch 1970, val loss: 0.964571475982666
Epoch 1980, training loss: 12.460348129272461 = 0.005843123886734247 + 2.0 * 6.22725248336792
Epoch 1980, val loss: 0.9663373231887817
Epoch 1990, training loss: 12.477751731872559 = 0.005776326637715101 + 2.0 * 6.235987663269043
Epoch 1990, val loss: 0.9680722951889038
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.825925925925926
0.8413284132841329
=== training gcn model ===
Epoch 0, training loss: 19.142627716064453 = 1.9489635229110718 + 2.0 * 8.596832275390625
Epoch 0, val loss: 1.9461218118667603
Epoch 10, training loss: 19.131893157958984 = 1.9391781091690063 + 2.0 * 8.596357345581055
Epoch 10, val loss: 1.935990571975708
Epoch 20, training loss: 19.110942840576172 = 1.9269089698791504 + 2.0 * 8.59201717376709
Epoch 20, val loss: 1.9232263565063477
Epoch 30, training loss: 19.032764434814453 = 1.910081386566162 + 2.0 * 8.561341285705566
Epoch 30, val loss: 1.9059524536132812
Epoch 40, training loss: 18.69940948486328 = 1.8889086246490479 + 2.0 * 8.405250549316406
Epoch 40, val loss: 1.885383129119873
Epoch 50, training loss: 17.756860733032227 = 1.8661024570465088 + 2.0 * 7.94537878036499
Epoch 50, val loss: 1.8635011911392212
Epoch 60, training loss: 17.009929656982422 = 1.8462157249450684 + 2.0 * 7.581856727600098
Epoch 60, val loss: 1.8445991277694702
Epoch 70, training loss: 16.395870208740234 = 1.830043077468872 + 2.0 * 7.282913684844971
Epoch 70, val loss: 1.8285937309265137
Epoch 80, training loss: 15.965070724487305 = 1.814182162284851 + 2.0 * 7.075444221496582
Epoch 80, val loss: 1.813043236732483
Epoch 90, training loss: 15.550553321838379 = 1.8004037141799927 + 2.0 * 6.875074863433838
Epoch 90, val loss: 1.8003854751586914
Epoch 100, training loss: 15.362445831298828 = 1.7875734567642212 + 2.0 * 6.787436008453369
Epoch 100, val loss: 1.7885535955429077
Epoch 110, training loss: 15.236754417419434 = 1.7728102207183838 + 2.0 * 6.7319722175598145
Epoch 110, val loss: 1.774678349494934
Epoch 120, training loss: 15.128861427307129 = 1.7570720911026 + 2.0 * 6.68589448928833
Epoch 120, val loss: 1.7599397897720337
Epoch 130, training loss: 15.042461395263672 = 1.7414089441299438 + 2.0 * 6.65052604675293
Epoch 130, val loss: 1.7456265687942505
Epoch 140, training loss: 14.967523574829102 = 1.7252620458602905 + 2.0 * 6.62113094329834
Epoch 140, val loss: 1.731088638305664
Epoch 150, training loss: 14.894274711608887 = 1.7072162628173828 + 2.0 * 6.593529224395752
Epoch 150, val loss: 1.71502685546875
Epoch 160, training loss: 14.830357551574707 = 1.6863348484039307 + 2.0 * 6.572011470794678
Epoch 160, val loss: 1.6965880393981934
Epoch 170, training loss: 14.771034240722656 = 1.6625021696090698 + 2.0 * 6.554265975952148
Epoch 170, val loss: 1.6756714582443237
Epoch 180, training loss: 14.699167251586914 = 1.6360872983932495 + 2.0 * 6.5315399169921875
Epoch 180, val loss: 1.6526216268539429
Epoch 190, training loss: 14.631210327148438 = 1.6065442562103271 + 2.0 * 6.512332916259766
Epoch 190, val loss: 1.6269503831863403
Epoch 200, training loss: 14.573150634765625 = 1.5731219053268433 + 2.0 * 6.500014305114746
Epoch 200, val loss: 1.5978385210037231
Epoch 210, training loss: 14.503506660461426 = 1.5357781648635864 + 2.0 * 6.4838643074035645
Epoch 210, val loss: 1.5656908750534058
Epoch 220, training loss: 14.436332702636719 = 1.494573712348938 + 2.0 * 6.470879554748535
Epoch 220, val loss: 1.530470609664917
Epoch 230, training loss: 14.367415428161621 = 1.4496147632598877 + 2.0 * 6.458900451660156
Epoch 230, val loss: 1.4922304153442383
Epoch 240, training loss: 14.303213119506836 = 1.4010487794876099 + 2.0 * 6.451082229614258
Epoch 240, val loss: 1.4515410661697388
Epoch 250, training loss: 14.225857734680176 = 1.3515878915786743 + 2.0 * 6.437134742736816
Epoch 250, val loss: 1.4105114936828613
Epoch 260, training loss: 14.15656852722168 = 1.3017916679382324 + 2.0 * 6.4273881912231445
Epoch 260, val loss: 1.3696331977844238
Epoch 270, training loss: 14.08730411529541 = 1.2524162530899048 + 2.0 * 6.417443752288818
Epoch 270, val loss: 1.3295334577560425
Epoch 280, training loss: 14.024116516113281 = 1.20457923412323 + 2.0 * 6.409768581390381
Epoch 280, val loss: 1.2913382053375244
Epoch 290, training loss: 13.959615707397461 = 1.1588088274002075 + 2.0 * 6.4004034996032715
Epoch 290, val loss: 1.255401611328125
Epoch 300, training loss: 13.900395393371582 = 1.1150315999984741 + 2.0 * 6.392682075500488
Epoch 300, val loss: 1.2215790748596191
Epoch 310, training loss: 13.86264419555664 = 1.0732380151748657 + 2.0 * 6.394702911376953
Epoch 310, val loss: 1.1896507740020752
Epoch 320, training loss: 13.801064491271973 = 1.0341302156448364 + 2.0 * 6.383467197418213
Epoch 320, val loss: 1.160390853881836
Epoch 330, training loss: 13.75233268737793 = 0.9974374175071716 + 2.0 * 6.377447605133057
Epoch 330, val loss: 1.133402943611145
Epoch 340, training loss: 13.704488754272461 = 0.9626743793487549 + 2.0 * 6.370907306671143
Epoch 340, val loss: 1.1080875396728516
Epoch 350, training loss: 13.661962509155273 = 0.9293150305747986 + 2.0 * 6.366323947906494
Epoch 350, val loss: 1.084208369255066
Epoch 360, training loss: 13.623035430908203 = 0.8971383571624756 + 2.0 * 6.362948417663574
Epoch 360, val loss: 1.0614571571350098
Epoch 370, training loss: 13.582512855529785 = 0.8661359548568726 + 2.0 * 6.358188629150391
Epoch 370, val loss: 1.0399202108383179
Epoch 380, training loss: 13.548571586608887 = 0.836245596408844 + 2.0 * 6.356163024902344
Epoch 380, val loss: 1.019342303276062
Epoch 390, training loss: 13.513274192810059 = 0.8070529103279114 + 2.0 * 6.3531107902526855
Epoch 390, val loss: 0.9995102286338806
Epoch 400, training loss: 13.480737686157227 = 0.7786756753921509 + 2.0 * 6.3510308265686035
Epoch 400, val loss: 0.9803323149681091
Epoch 410, training loss: 13.44383430480957 = 0.7508436441421509 + 2.0 * 6.346495151519775
Epoch 410, val loss: 0.9618990421295166
Epoch 420, training loss: 13.412537574768066 = 0.7235374450683594 + 2.0 * 6.3445000648498535
Epoch 420, val loss: 0.944021463394165
Epoch 430, training loss: 13.379341125488281 = 0.6968110203742981 + 2.0 * 6.3412652015686035
Epoch 430, val loss: 0.9267275929450989
Epoch 440, training loss: 13.349800109863281 = 0.6707039475440979 + 2.0 * 6.339548110961914
Epoch 440, val loss: 0.910269021987915
Epoch 450, training loss: 13.316988945007324 = 0.6450074315071106 + 2.0 * 6.335990905761719
Epoch 450, val loss: 0.8945159316062927
Epoch 460, training loss: 13.290096282958984 = 0.6196280717849731 + 2.0 * 6.33523416519165
Epoch 460, val loss: 0.8794333934783936
Epoch 470, training loss: 13.272716522216797 = 0.594879686832428 + 2.0 * 6.338918209075928
Epoch 470, val loss: 0.865155816078186
Epoch 480, training loss: 13.232071876525879 = 0.5707586407661438 + 2.0 * 6.3306565284729
Epoch 480, val loss: 0.8519672751426697
Epoch 490, training loss: 13.203622817993164 = 0.5470995306968689 + 2.0 * 6.328261852264404
Epoch 490, val loss: 0.8396514058113098
Epoch 500, training loss: 13.175024032592773 = 0.5237853527069092 + 2.0 * 6.325619220733643
Epoch 500, val loss: 0.8281559944152832
Epoch 510, training loss: 13.155542373657227 = 0.5008139610290527 + 2.0 * 6.327364444732666
Epoch 510, val loss: 0.8175749778747559
Epoch 520, training loss: 13.130867958068848 = 0.478498250246048 + 2.0 * 6.3261847496032715
Epoch 520, val loss: 0.8080654740333557
Epoch 530, training loss: 13.097169876098633 = 0.45686766505241394 + 2.0 * 6.320151329040527
Epoch 530, val loss: 0.7997196316719055
Epoch 540, training loss: 13.07282543182373 = 0.4357960522174835 + 2.0 * 6.318514823913574
Epoch 540, val loss: 0.7923679351806641
Epoch 550, training loss: 13.048272132873535 = 0.41529521346092224 + 2.0 * 6.316488265991211
Epoch 550, val loss: 0.7860431671142578
Epoch 560, training loss: 13.070223808288574 = 0.395401269197464 + 2.0 * 6.337411403656006
Epoch 560, val loss: 0.7807764410972595
Epoch 570, training loss: 13.011679649353027 = 0.3766119182109833 + 2.0 * 6.31753396987915
Epoch 570, val loss: 0.7766275405883789
Epoch 580, training loss: 12.982856750488281 = 0.3586157262325287 + 2.0 * 6.31212043762207
Epoch 580, val loss: 0.7735077738761902
Epoch 590, training loss: 12.962215423583984 = 0.34128621220588684 + 2.0 * 6.310464382171631
Epoch 590, val loss: 0.7711809873580933
Epoch 600, training loss: 12.940553665161133 = 0.32459503412246704 + 2.0 * 6.307979106903076
Epoch 600, val loss: 0.7696549892425537
Epoch 610, training loss: 12.921135902404785 = 0.30855539441108704 + 2.0 * 6.306290149688721
Epoch 610, val loss: 0.7688742876052856
Epoch 620, training loss: 12.920095443725586 = 0.2931755781173706 + 2.0 * 6.313459873199463
Epoch 620, val loss: 0.7687361836433411
Epoch 630, training loss: 12.8901948928833 = 0.27863016724586487 + 2.0 * 6.305782318115234
Epoch 630, val loss: 0.7691061496734619
Epoch 640, training loss: 12.875804901123047 = 0.26488128304481506 + 2.0 * 6.305461883544922
Epoch 640, val loss: 0.7702485918998718
Epoch 650, training loss: 12.853940963745117 = 0.2517443597316742 + 2.0 * 6.301098346710205
Epoch 650, val loss: 0.7716813683509827
Epoch 660, training loss: 12.838371276855469 = 0.23918047547340393 + 2.0 * 6.299595355987549
Epoch 660, val loss: 0.7735499739646912
Epoch 670, training loss: 12.823599815368652 = 0.22716382145881653 + 2.0 * 6.2982177734375
Epoch 670, val loss: 0.7759097218513489
Epoch 680, training loss: 12.820556640625 = 0.21567553281784058 + 2.0 * 6.302440643310547
Epoch 680, val loss: 0.7785728573799133
Epoch 690, training loss: 12.813392639160156 = 0.20491176843643188 + 2.0 * 6.3042402267456055
Epoch 690, val loss: 0.7816579341888428
Epoch 700, training loss: 12.788853645324707 = 0.19469021260738373 + 2.0 * 6.29708194732666
Epoch 700, val loss: 0.7851024270057678
Epoch 710, training loss: 12.773335456848145 = 0.18499472737312317 + 2.0 * 6.294170379638672
Epoch 710, val loss: 0.788723886013031
Epoch 720, training loss: 12.76069450378418 = 0.17578427493572235 + 2.0 * 6.292455196380615
Epoch 720, val loss: 0.7925835251808167
Epoch 730, training loss: 12.751517295837402 = 0.1670445054769516 + 2.0 * 6.292236328125
Epoch 730, val loss: 0.7967513203620911
Epoch 740, training loss: 12.743375778198242 = 0.15880338847637177 + 2.0 * 6.292286396026611
Epoch 740, val loss: 0.8011187314987183
Epoch 750, training loss: 12.734028816223145 = 0.15105894207954407 + 2.0 * 6.291484832763672
Epoch 750, val loss: 0.8057517409324646
Epoch 760, training loss: 12.722675323486328 = 0.14376579225063324 + 2.0 * 6.289454936981201
Epoch 760, val loss: 0.8106143474578857
Epoch 770, training loss: 12.721464157104492 = 0.1368696540594101 + 2.0 * 6.29229736328125
Epoch 770, val loss: 0.8155496716499329
Epoch 780, training loss: 12.706046104431152 = 0.1303856521844864 + 2.0 * 6.287830352783203
Epoch 780, val loss: 0.8207031488418579
Epoch 790, training loss: 12.699000358581543 = 0.12428024411201477 + 2.0 * 6.287360191345215
Epoch 790, val loss: 0.8260233402252197
Epoch 800, training loss: 12.69084358215332 = 0.118520587682724 + 2.0 * 6.286161422729492
Epoch 800, val loss: 0.8315021991729736
Epoch 810, training loss: 12.689643859863281 = 0.11308649182319641 + 2.0 * 6.288278579711914
Epoch 810, val loss: 0.836979329586029
Epoch 820, training loss: 12.67662525177002 = 0.10799480229616165 + 2.0 * 6.28431510925293
Epoch 820, val loss: 0.8427480459213257
Epoch 830, training loss: 12.674845695495605 = 0.10317011177539825 + 2.0 * 6.285837650299072
Epoch 830, val loss: 0.84843510389328
Epoch 840, training loss: 12.661965370178223 = 0.09864358603954315 + 2.0 * 6.281661033630371
Epoch 840, val loss: 0.8541192412376404
Epoch 850, training loss: 12.656203269958496 = 0.09438343346118927 + 2.0 * 6.280910015106201
Epoch 850, val loss: 0.860059916973114
Epoch 860, training loss: 12.648903846740723 = 0.09035639464855194 + 2.0 * 6.279273509979248
Epoch 860, val loss: 0.8658844828605652
Epoch 870, training loss: 12.67229175567627 = 0.08653662353754044 + 2.0 * 6.292877674102783
Epoch 870, val loss: 0.8717032074928284
Epoch 880, training loss: 12.647089958190918 = 0.08298761397600174 + 2.0 * 6.282051086425781
Epoch 880, val loss: 0.8776572346687317
Epoch 890, training loss: 12.63231086730957 = 0.07961463183164597 + 2.0 * 6.276348114013672
Epoch 890, val loss: 0.8836575150489807
Epoch 900, training loss: 12.627677917480469 = 0.07641828805208206 + 2.0 * 6.275629997253418
Epoch 900, val loss: 0.8895556330680847
Epoch 910, training loss: 12.623026847839355 = 0.07339140772819519 + 2.0 * 6.274817943572998
Epoch 910, val loss: 0.8954855799674988
Epoch 920, training loss: 12.635875701904297 = 0.07051955163478851 + 2.0 * 6.282678127288818
Epoch 920, val loss: 0.9012735486030579
Epoch 930, training loss: 12.619545936584473 = 0.06781906634569168 + 2.0 * 6.2758636474609375
Epoch 930, val loss: 0.9072349071502686
Epoch 940, training loss: 12.609562873840332 = 0.06525740027427673 + 2.0 * 6.272152900695801
Epoch 940, val loss: 0.913133978843689
Epoch 950, training loss: 12.605555534362793 = 0.06282337009906769 + 2.0 * 6.271366119384766
Epoch 950, val loss: 0.918959379196167
Epoch 960, training loss: 12.605395317077637 = 0.06050610542297363 + 2.0 * 6.272444725036621
Epoch 960, val loss: 0.9247449040412903
Epoch 970, training loss: 12.599936485290527 = 0.05830409377813339 + 2.0 * 6.270816326141357
Epoch 970, val loss: 0.9304660558700562
Epoch 980, training loss: 12.609087944030762 = 0.05621805414557457 + 2.0 * 6.276434898376465
Epoch 980, val loss: 0.9361348748207092
Epoch 990, training loss: 12.592207908630371 = 0.05424759164452553 + 2.0 * 6.268980026245117
Epoch 990, val loss: 0.9418637156486511
Epoch 1000, training loss: 12.588518142700195 = 0.05237840116024017 + 2.0 * 6.268069744110107
Epoch 1000, val loss: 0.9476308226585388
Epoch 1010, training loss: 12.589314460754395 = 0.05058944970369339 + 2.0 * 6.269362449645996
Epoch 1010, val loss: 0.9531784653663635
Epoch 1020, training loss: 12.58834171295166 = 0.048884857445955276 + 2.0 * 6.269728660583496
Epoch 1020, val loss: 0.9586399793624878
Epoch 1030, training loss: 12.578360557556152 = 0.04725867137312889 + 2.0 * 6.2655510902404785
Epoch 1030, val loss: 0.9641278386116028
Epoch 1040, training loss: 12.57574462890625 = 0.04571259766817093 + 2.0 * 6.265016078948975
Epoch 1040, val loss: 0.9696778655052185
Epoch 1050, training loss: 12.581863403320312 = 0.044227663427591324 + 2.0 * 6.268817901611328
Epoch 1050, val loss: 0.9750096201896667
Epoch 1060, training loss: 12.574183464050293 = 0.04281963035464287 + 2.0 * 6.265681743621826
Epoch 1060, val loss: 0.9803379774093628
Epoch 1070, training loss: 12.56738567352295 = 0.041476041078567505 + 2.0 * 6.2629547119140625
Epoch 1070, val loss: 0.9856844544410706
Epoch 1080, training loss: 12.565571784973145 = 0.04018997400999069 + 2.0 * 6.262691020965576
Epoch 1080, val loss: 0.9909902215003967
Epoch 1090, training loss: 12.563495635986328 = 0.03895706310868263 + 2.0 * 6.262269496917725
Epoch 1090, val loss: 0.9962254166603088
Epoch 1100, training loss: 12.568729400634766 = 0.037776730954647064 + 2.0 * 6.265476226806641
Epoch 1100, val loss: 1.0013525485992432
Epoch 1110, training loss: 12.561676025390625 = 0.03665124624967575 + 2.0 * 6.26251220703125
Epoch 1110, val loss: 1.0064969062805176
Epoch 1120, training loss: 12.565573692321777 = 0.03557492420077324 + 2.0 * 6.2649993896484375
Epoch 1120, val loss: 1.0115493535995483
Epoch 1130, training loss: 12.554508209228516 = 0.034542687237262726 + 2.0 * 6.259982585906982
Epoch 1130, val loss: 1.0165108442306519
Epoch 1140, training loss: 12.553008079528809 = 0.033555690199136734 + 2.0 * 6.259726047515869
Epoch 1140, val loss: 1.021546483039856
Epoch 1150, training loss: 12.564335823059082 = 0.03261026740074158 + 2.0 * 6.265862941741943
Epoch 1150, val loss: 1.0265123844146729
Epoch 1160, training loss: 12.551922798156738 = 0.03169887885451317 + 2.0 * 6.2601118087768555
Epoch 1160, val loss: 1.0313076972961426
Epoch 1170, training loss: 12.550459861755371 = 0.03083042986690998 + 2.0 * 6.259814739227295
Epoch 1170, val loss: 1.0362796783447266
Epoch 1180, training loss: 12.548221588134766 = 0.029993100091814995 + 2.0 * 6.2591142654418945
Epoch 1180, val loss: 1.040984869003296
Epoch 1190, training loss: 12.54706859588623 = 0.029187729582190514 + 2.0 * 6.25894021987915
Epoch 1190, val loss: 1.0456691980361938
Epoch 1200, training loss: 12.540315628051758 = 0.028415247797966003 + 2.0 * 6.255949974060059
Epoch 1200, val loss: 1.0504878759384155
Epoch 1210, training loss: 12.540618896484375 = 0.027669593691825867 + 2.0 * 6.256474494934082
Epoch 1210, val loss: 1.0552064180374146
Epoch 1220, training loss: 12.556675910949707 = 0.02695322409272194 + 2.0 * 6.264861106872559
Epoch 1220, val loss: 1.0597225427627563
Epoch 1230, training loss: 12.539186477661133 = 0.02626468800008297 + 2.0 * 6.256460666656494
Epoch 1230, val loss: 1.064244031906128
Epoch 1240, training loss: 12.533951759338379 = 0.0256056971848011 + 2.0 * 6.2541728019714355
Epoch 1240, val loss: 1.0689164400100708
Epoch 1250, training loss: 12.543030738830566 = 0.024968178942799568 + 2.0 * 6.259031295776367
Epoch 1250, val loss: 1.073379635810852
Epoch 1260, training loss: 12.532291412353516 = 0.02435114048421383 + 2.0 * 6.253970146179199
Epoch 1260, val loss: 1.0777314901351929
Epoch 1270, training loss: 12.530036926269531 = 0.023762376978993416 + 2.0 * 6.253137111663818
Epoch 1270, val loss: 1.0822367668151855
Epoch 1280, training loss: 12.52779483795166 = 0.023189863190054893 + 2.0 * 6.252302646636963
Epoch 1280, val loss: 1.086629033088684
Epoch 1290, training loss: 12.541582107543945 = 0.022636111825704575 + 2.0 * 6.259472846984863
Epoch 1290, val loss: 1.0907478332519531
Epoch 1300, training loss: 12.530184745788574 = 0.02210746705532074 + 2.0 * 6.2540388107299805
Epoch 1300, val loss: 1.0951945781707764
Epoch 1310, training loss: 12.526020050048828 = 0.02159568853676319 + 2.0 * 6.252212047576904
Epoch 1310, val loss: 1.099414348602295
Epoch 1320, training loss: 12.522439002990723 = 0.021101705729961395 + 2.0 * 6.250668525695801
Epoch 1320, val loss: 1.103711724281311
Epoch 1330, training loss: 12.527130126953125 = 0.0206223726272583 + 2.0 * 6.253253936767578
Epoch 1330, val loss: 1.1077606678009033
Epoch 1340, training loss: 12.525640487670898 = 0.020159345120191574 + 2.0 * 6.252740383148193
Epoch 1340, val loss: 1.1118942499160767
Epoch 1350, training loss: 12.522821426391602 = 0.019714197143912315 + 2.0 * 6.251553535461426
Epoch 1350, val loss: 1.1158937215805054
Epoch 1360, training loss: 12.516687393188477 = 0.01928468979895115 + 2.0 * 6.248701572418213
Epoch 1360, val loss: 1.120013952255249
Epoch 1370, training loss: 12.516929626464844 = 0.018867503851652145 + 2.0 * 6.249031066894531
Epoch 1370, val loss: 1.1240360736846924
Epoch 1380, training loss: 12.529850006103516 = 0.018462365493178368 + 2.0 * 6.2556939125061035
Epoch 1380, val loss: 1.1278622150421143
Epoch 1390, training loss: 12.518668174743652 = 0.01807604357600212 + 2.0 * 6.250296115875244
Epoch 1390, val loss: 1.1318552494049072
Epoch 1400, training loss: 12.512925148010254 = 0.01769736409187317 + 2.0 * 6.247613906860352
Epoch 1400, val loss: 1.1357694864273071
Epoch 1410, training loss: 12.510868072509766 = 0.017331348732113838 + 2.0 * 6.246768474578857
Epoch 1410, val loss: 1.1396852731704712
Epoch 1420, training loss: 12.527932167053223 = 0.01697584055364132 + 2.0 * 6.255478382110596
Epoch 1420, val loss: 1.1434226036071777
Epoch 1430, training loss: 12.51212215423584 = 0.01663178950548172 + 2.0 * 6.247745037078857
Epoch 1430, val loss: 1.1471657752990723
Epoch 1440, training loss: 12.508458137512207 = 0.016301387920975685 + 2.0 * 6.2460784912109375
Epoch 1440, val loss: 1.1509935855865479
Epoch 1450, training loss: 12.508012771606445 = 0.015978453680872917 + 2.0 * 6.246016979217529
Epoch 1450, val loss: 1.1548148393630981
Epoch 1460, training loss: 12.521231651306152 = 0.01566663570702076 + 2.0 * 6.252782344818115
Epoch 1460, val loss: 1.1584813594818115
Epoch 1470, training loss: 12.508177757263184 = 0.015360184945166111 + 2.0 * 6.246408939361572
Epoch 1470, val loss: 1.1618963479995728
Epoch 1480, training loss: 12.505644798278809 = 0.01506682951003313 + 2.0 * 6.245288848876953
Epoch 1480, val loss: 1.165672779083252
Epoch 1490, training loss: 12.513545989990234 = 0.014780109748244286 + 2.0 * 6.249382972717285
Epoch 1490, val loss: 1.1692146062850952
Epoch 1500, training loss: 12.504128456115723 = 0.014501303434371948 + 2.0 * 6.244813442230225
Epoch 1500, val loss: 1.1727169752120972
Epoch 1510, training loss: 12.50087833404541 = 0.014231900684535503 + 2.0 * 6.24332332611084
Epoch 1510, val loss: 1.176302433013916
Epoch 1520, training loss: 12.508808135986328 = 0.01396924164146185 + 2.0 * 6.247419357299805
Epoch 1520, val loss: 1.1797784566879272
Epoch 1530, training loss: 12.505374908447266 = 0.013713217340409756 + 2.0 * 6.24583101272583
Epoch 1530, val loss: 1.1830594539642334
Epoch 1540, training loss: 12.510268211364746 = 0.013467682525515556 + 2.0 * 6.2484002113342285
Epoch 1540, val loss: 1.1866710186004639
Epoch 1550, training loss: 12.498300552368164 = 0.013226530514657497 + 2.0 * 6.242537021636963
Epoch 1550, val loss: 1.1897637844085693
Epoch 1560, training loss: 12.496559143066406 = 0.01299523189663887 + 2.0 * 6.241782188415527
Epoch 1560, val loss: 1.1932003498077393
Epoch 1570, training loss: 12.494630813598633 = 0.012768140994012356 + 2.0 * 6.240931510925293
Epoch 1570, val loss: 1.1965361833572388
Epoch 1580, training loss: 12.49343490600586 = 0.012545762583613396 + 2.0 * 6.240444660186768
Epoch 1580, val loss: 1.1997557878494263
Epoch 1590, training loss: 12.514200210571289 = 0.012329572811722755 + 2.0 * 6.2509355545043945
Epoch 1590, val loss: 1.2029787302017212
Epoch 1600, training loss: 12.502474784851074 = 0.012119459919631481 + 2.0 * 6.245177745819092
Epoch 1600, val loss: 1.2060500383377075
Epoch 1610, training loss: 12.496098518371582 = 0.011917289346456528 + 2.0 * 6.242090702056885
Epoch 1610, val loss: 1.2091660499572754
Epoch 1620, training loss: 12.490601539611816 = 0.011719806119799614 + 2.0 * 6.23944091796875
Epoch 1620, val loss: 1.2123829126358032
Epoch 1630, training loss: 12.48991584777832 = 0.011526892893016338 + 2.0 * 6.239194393157959
Epoch 1630, val loss: 1.2155636548995972
Epoch 1640, training loss: 12.494389533996582 = 0.011336862109601498 + 2.0 * 6.241526126861572
Epoch 1640, val loss: 1.2185380458831787
Epoch 1650, training loss: 12.489958763122559 = 0.011151882819831371 + 2.0 * 6.239403247833252
Epoch 1650, val loss: 1.2214165925979614
Epoch 1660, training loss: 12.491499900817871 = 0.010973721742630005 + 2.0 * 6.240262985229492
Epoch 1660, val loss: 1.22446870803833
Epoch 1670, training loss: 12.491902351379395 = 0.010799014940857887 + 2.0 * 6.240551471710205
Epoch 1670, val loss: 1.2274580001831055
Epoch 1680, training loss: 12.484932899475098 = 0.010629898868501186 + 2.0 * 6.237151622772217
Epoch 1680, val loss: 1.230574369430542
Epoch 1690, training loss: 12.491599082946777 = 0.010463896207511425 + 2.0 * 6.240567684173584
Epoch 1690, val loss: 1.2335166931152344
Epoch 1700, training loss: 12.488468170166016 = 0.010300710797309875 + 2.0 * 6.239083766937256
Epoch 1700, val loss: 1.2360608577728271
Epoch 1710, training loss: 12.489541053771973 = 0.010145798325538635 + 2.0 * 6.239697456359863
Epoch 1710, val loss: 1.239134669303894
Epoch 1720, training loss: 12.482680320739746 = 0.009992874227464199 + 2.0 * 6.236343860626221
Epoch 1720, val loss: 1.2420692443847656
Epoch 1730, training loss: 12.482597351074219 = 0.009842927567660809 + 2.0 * 6.236377239227295
Epoch 1730, val loss: 1.244903564453125
Epoch 1740, training loss: 12.491988182067871 = 0.009695935994386673 + 2.0 * 6.241146087646484
Epoch 1740, val loss: 1.2476998567581177
Epoch 1750, training loss: 12.480899810791016 = 0.00955083966255188 + 2.0 * 6.2356743812561035
Epoch 1750, val loss: 1.2502379417419434
Epoch 1760, training loss: 12.481403350830078 = 0.009410970844328403 + 2.0 * 6.235996246337891
Epoch 1760, val loss: 1.2530455589294434
Epoch 1770, training loss: 12.480572700500488 = 0.009274199604988098 + 2.0 * 6.235649108886719
Epoch 1770, val loss: 1.255861759185791
Epoch 1780, training loss: 12.48776626586914 = 0.009139617905020714 + 2.0 * 6.239313125610352
Epoch 1780, val loss: 1.2585124969482422
Epoch 1790, training loss: 12.485398292541504 = 0.009007208980619907 + 2.0 * 6.238195419311523
Epoch 1790, val loss: 1.2609344720840454
Epoch 1800, training loss: 12.48155403137207 = 0.008881047368049622 + 2.0 * 6.236336708068848
Epoch 1800, val loss: 1.2637258768081665
Epoch 1810, training loss: 12.476736068725586 = 0.008756767027080059 + 2.0 * 6.233989715576172
Epoch 1810, val loss: 1.2664213180541992
Epoch 1820, training loss: 12.477813720703125 = 0.008634533733129501 + 2.0 * 6.234589576721191
Epoch 1820, val loss: 1.2690531015396118
Epoch 1830, training loss: 12.488202095031738 = 0.008515482768416405 + 2.0 * 6.239843368530273
Epoch 1830, val loss: 1.2715356349945068
Epoch 1840, training loss: 12.484502792358398 = 0.008397992700338364 + 2.0 * 6.2380523681640625
Epoch 1840, val loss: 1.2739536762237549
Epoch 1850, training loss: 12.475776672363281 = 0.008284255862236023 + 2.0 * 6.23374605178833
Epoch 1850, val loss: 1.2764531373977661
Epoch 1860, training loss: 12.475343704223633 = 0.008174000307917595 + 2.0 * 6.233584880828857
Epoch 1860, val loss: 1.2790066003799438
Epoch 1870, training loss: 12.473672866821289 = 0.008064758032560349 + 2.0 * 6.232803821563721
Epoch 1870, val loss: 1.2814574241638184
Epoch 1880, training loss: 12.482566833496094 = 0.007957045920193195 + 2.0 * 6.2373046875
Epoch 1880, val loss: 1.2838232517242432
Epoch 1890, training loss: 12.47916316986084 = 0.0078523438423872 + 2.0 * 6.235655307769775
Epoch 1890, val loss: 1.2863255739212036
Epoch 1900, training loss: 12.473560333251953 = 0.0077508012764155865 + 2.0 * 6.23290491104126
Epoch 1900, val loss: 1.2886205911636353
Epoch 1910, training loss: 12.471193313598633 = 0.007650991436094046 + 2.0 * 6.231770992279053
Epoch 1910, val loss: 1.2910661697387695
Epoch 1920, training loss: 12.472709655761719 = 0.0075528924353420734 + 2.0 * 6.232578277587891
Epoch 1920, val loss: 1.2934881448745728
Epoch 1930, training loss: 12.479122161865234 = 0.007456202991306782 + 2.0 * 6.235833168029785
Epoch 1930, val loss: 1.295737624168396
Epoch 1940, training loss: 12.476484298706055 = 0.00736283790320158 + 2.0 * 6.234560966491699
Epoch 1940, val loss: 1.2981289625167847
Epoch 1950, training loss: 12.468217849731445 = 0.007270352449268103 + 2.0 * 6.230473518371582
Epoch 1950, val loss: 1.3003482818603516
Epoch 1960, training loss: 12.470163345336914 = 0.007179918233305216 + 2.0 * 6.231491565704346
Epoch 1960, val loss: 1.3026686906814575
Epoch 1970, training loss: 12.47708797454834 = 0.007091635372489691 + 2.0 * 6.2349982261657715
Epoch 1970, val loss: 1.3049396276474
Epoch 1980, training loss: 12.4760160446167 = 0.0070040542632341385 + 2.0 * 6.234506130218506
Epoch 1980, val loss: 1.3070791959762573
Epoch 1990, training loss: 12.468114852905273 = 0.006919459905475378 + 2.0 * 6.230597496032715
Epoch 1990, val loss: 1.3092355728149414
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7962962962962963
0.838165524512388
The final CL Acc:0.79877, 0.02124, The final GNN Acc:0.83940, 0.00138
Begin epxeriment: cont_weight: 2 epoch:2000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11642])
remove edge: torch.Size([2, 9496])
updated graph: torch.Size([2, 10582])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 19.15723419189453 = 1.9635545015335083 + 2.0 * 8.596839904785156
Epoch 0, val loss: 1.9654619693756104
Epoch 10, training loss: 19.146196365356445 = 1.9532923698425293 + 2.0 * 8.596451759338379
Epoch 10, val loss: 1.9547958374023438
Epoch 20, training loss: 19.1264705657959 = 1.940877079963684 + 2.0 * 8.592796325683594
Epoch 20, val loss: 1.9416230916976929
Epoch 30, training loss: 19.053701400756836 = 1.9244673252105713 + 2.0 * 8.564617156982422
Epoch 30, val loss: 1.9242208003997803
Epoch 40, training loss: 18.701820373535156 = 1.9047932624816895 + 2.0 * 8.398513793945312
Epoch 40, val loss: 1.9045268297195435
Epoch 50, training loss: 17.674423217773438 = 1.8831861019134521 + 2.0 * 7.895618915557861
Epoch 50, val loss: 1.8834346532821655
Epoch 60, training loss: 16.87552833557129 = 1.8652501106262207 + 2.0 * 7.505139350891113
Epoch 60, val loss: 1.867343544960022
Epoch 70, training loss: 16.144712448120117 = 1.8523848056793213 + 2.0 * 7.1461639404296875
Epoch 70, val loss: 1.8557894229888916
Epoch 80, training loss: 15.721942901611328 = 1.8419435024261475 + 2.0 * 6.939999580383301
Epoch 80, val loss: 1.8460599184036255
Epoch 90, training loss: 15.52999496459961 = 1.8295207023620605 + 2.0 * 6.850236892700195
Epoch 90, val loss: 1.8343452215194702
Epoch 100, training loss: 15.340995788574219 = 1.8162086009979248 + 2.0 * 6.762393474578857
Epoch 100, val loss: 1.8222720623016357
Epoch 110, training loss: 15.191593170166016 = 1.8047821521759033 + 2.0 * 6.693405628204346
Epoch 110, val loss: 1.811657428741455
Epoch 120, training loss: 15.075641632080078 = 1.7944834232330322 + 2.0 * 6.6405792236328125
Epoch 120, val loss: 1.8018929958343506
Epoch 130, training loss: 14.972101211547852 = 1.7845379114151 + 2.0 * 6.593781471252441
Epoch 130, val loss: 1.7922964096069336
Epoch 140, training loss: 14.884912490844727 = 1.7744730710983276 + 2.0 * 6.555219650268555
Epoch 140, val loss: 1.7827527523040771
Epoch 150, training loss: 14.813114166259766 = 1.7639248371124268 + 2.0 * 6.524594783782959
Epoch 150, val loss: 1.7728846073150635
Epoch 160, training loss: 14.783970832824707 = 1.752432942390442 + 2.0 * 6.515769004821777
Epoch 160, val loss: 1.7624220848083496
Epoch 170, training loss: 14.706588745117188 = 1.7400310039520264 + 2.0 * 6.483278751373291
Epoch 170, val loss: 1.751156210899353
Epoch 180, training loss: 14.661580085754395 = 1.7262991666793823 + 2.0 * 6.467640399932861
Epoch 180, val loss: 1.7391456365585327
Epoch 190, training loss: 14.616852760314941 = 1.7109546661376953 + 2.0 * 6.452949047088623
Epoch 190, val loss: 1.7260253429412842
Epoch 200, training loss: 14.57595443725586 = 1.6936644315719604 + 2.0 * 6.441144943237305
Epoch 200, val loss: 1.7114421129226685
Epoch 210, training loss: 14.53567123413086 = 1.6740607023239136 + 2.0 * 6.430805206298828
Epoch 210, val loss: 1.6950128078460693
Epoch 220, training loss: 14.494723320007324 = 1.6517924070358276 + 2.0 * 6.4214653968811035
Epoch 220, val loss: 1.676517128944397
Epoch 230, training loss: 14.453789710998535 = 1.6266182661056519 + 2.0 * 6.413585662841797
Epoch 230, val loss: 1.6557260751724243
Epoch 240, training loss: 14.41018295288086 = 1.598623275756836 + 2.0 * 6.405779838562012
Epoch 240, val loss: 1.6327306032180786
Epoch 250, training loss: 14.3665132522583 = 1.5679298639297485 + 2.0 * 6.399291515350342
Epoch 250, val loss: 1.6077197790145874
Epoch 260, training loss: 14.321479797363281 = 1.534131407737732 + 2.0 * 6.393674373626709
Epoch 260, val loss: 1.580376148223877
Epoch 270, training loss: 14.279814720153809 = 1.4979286193847656 + 2.0 * 6.3909430503845215
Epoch 270, val loss: 1.5513361692428589
Epoch 280, training loss: 14.22816276550293 = 1.4596436023712158 + 2.0 * 6.3842597007751465
Epoch 280, val loss: 1.5209558010101318
Epoch 290, training loss: 14.175503730773926 = 1.4195812940597534 + 2.0 * 6.377961158752441
Epoch 290, val loss: 1.489475131034851
Epoch 300, training loss: 14.124396324157715 = 1.3780858516693115 + 2.0 * 6.373155117034912
Epoch 300, val loss: 1.4572535753250122
Epoch 310, training loss: 14.076265335083008 = 1.3357045650482178 + 2.0 * 6.3702802658081055
Epoch 310, val loss: 1.424666166305542
Epoch 320, training loss: 14.025469779968262 = 1.2935845851898193 + 2.0 * 6.365942478179932
Epoch 320, val loss: 1.3926959037780762
Epoch 330, training loss: 13.979959487915039 = 1.2525181770324707 + 2.0 * 6.363720417022705
Epoch 330, val loss: 1.3618240356445312
Epoch 340, training loss: 13.928333282470703 = 1.212151288986206 + 2.0 * 6.358090877532959
Epoch 340, val loss: 1.3317598104476929
Epoch 350, training loss: 13.881200790405273 = 1.172423005104065 + 2.0 * 6.35438871383667
Epoch 350, val loss: 1.302374005317688
Epoch 360, training loss: 13.834883689880371 = 1.1332998275756836 + 2.0 * 6.350791931152344
Epoch 360, val loss: 1.273658275604248
Epoch 370, training loss: 13.799015998840332 = 1.09481942653656 + 2.0 * 6.35209846496582
Epoch 370, val loss: 1.2456697225570679
Epoch 380, training loss: 13.756080627441406 = 1.0576872825622559 + 2.0 * 6.349196910858154
Epoch 380, val loss: 1.218636155128479
Epoch 390, training loss: 13.706486701965332 = 1.0217562913894653 + 2.0 * 6.342365264892578
Epoch 390, val loss: 1.192772626876831
Epoch 400, training loss: 13.66777229309082 = 0.986786961555481 + 2.0 * 6.3404927253723145
Epoch 400, val loss: 1.1676380634307861
Epoch 410, training loss: 13.627833366394043 = 0.9525775909423828 + 2.0 * 6.33762788772583
Epoch 410, val loss: 1.1430991888046265
Epoch 420, training loss: 13.606866836547852 = 0.9191298484802246 + 2.0 * 6.343868255615234
Epoch 420, val loss: 1.1192866563796997
Epoch 430, training loss: 13.553451538085938 = 0.886718213558197 + 2.0 * 6.333366870880127
Epoch 430, val loss: 1.0963786840438843
Epoch 440, training loss: 13.51675033569336 = 0.855279266834259 + 2.0 * 6.330735683441162
Epoch 440, val loss: 1.074386715888977
Epoch 450, training loss: 13.484674453735352 = 0.8246919512748718 + 2.0 * 6.329991340637207
Epoch 450, val loss: 1.0532737970352173
Epoch 460, training loss: 13.454533576965332 = 0.795103132724762 + 2.0 * 6.329715251922607
Epoch 460, val loss: 1.0328519344329834
Epoch 470, training loss: 13.416364669799805 = 0.7665254473686218 + 2.0 * 6.324919700622559
Epoch 470, val loss: 1.0137051343917847
Epoch 480, training loss: 13.384418487548828 = 0.7387971878051758 + 2.0 * 6.322810649871826
Epoch 480, val loss: 0.9955140352249146
Epoch 490, training loss: 13.36658000946045 = 0.7118564248085022 + 2.0 * 6.327361583709717
Epoch 490, val loss: 0.9781771898269653
Epoch 500, training loss: 13.325658798217773 = 0.6858709454536438 + 2.0 * 6.319893836975098
Epoch 500, val loss: 0.9618847966194153
Epoch 510, training loss: 13.294747352600098 = 0.6605560779571533 + 2.0 * 6.317095756530762
Epoch 510, val loss: 0.9464461207389832
Epoch 520, training loss: 13.266654014587402 = 0.6357802152633667 + 2.0 * 6.315436840057373
Epoch 520, val loss: 0.9318839907646179
Epoch 530, training loss: 13.243608474731445 = 0.6115467548370361 + 2.0 * 6.316030979156494
Epoch 530, val loss: 0.9182335734367371
Epoch 540, training loss: 13.217925071716309 = 0.5879910588264465 + 2.0 * 6.314967155456543
Epoch 540, val loss: 0.9054955840110779
Epoch 550, training loss: 13.188423156738281 = 0.5649345517158508 + 2.0 * 6.311744213104248
Epoch 550, val loss: 0.89359050989151
Epoch 560, training loss: 13.160133361816406 = 0.5423387885093689 + 2.0 * 6.308897495269775
Epoch 560, val loss: 0.8825103640556335
Epoch 570, training loss: 13.138408660888672 = 0.5200958251953125 + 2.0 * 6.30915641784668
Epoch 570, val loss: 0.8722549080848694
Epoch 580, training loss: 13.134486198425293 = 0.4983852803707123 + 2.0 * 6.318050384521484
Epoch 580, val loss: 0.8627164363861084
Epoch 590, training loss: 13.091625213623047 = 0.47729113698005676 + 2.0 * 6.307167053222656
Epoch 590, val loss: 0.8543487191200256
Epoch 600, training loss: 13.065492630004883 = 0.45680955052375793 + 2.0 * 6.3043413162231445
Epoch 600, val loss: 0.8468466997146606
Epoch 610, training loss: 13.041363716125488 = 0.4367634654045105 + 2.0 * 6.302299976348877
Epoch 610, val loss: 0.8401197195053101
Epoch 620, training loss: 13.021533012390137 = 0.4171869158744812 + 2.0 * 6.302173137664795
Epoch 620, val loss: 0.8343170881271362
Epoch 630, training loss: 13.000922203063965 = 0.3982203006744385 + 2.0 * 6.301351070404053
Epoch 630, val loss: 0.8293249011039734
Epoch 640, training loss: 12.977205276489258 = 0.37992796301841736 + 2.0 * 6.298638820648193
Epoch 640, val loss: 0.8251811861991882
Epoch 650, training loss: 12.95671558380127 = 0.3622070550918579 + 2.0 * 6.2972540855407715
Epoch 650, val loss: 0.8220896124839783
Epoch 660, training loss: 12.944063186645508 = 0.3450397551059723 + 2.0 * 6.299511909484863
Epoch 660, val loss: 0.8198384642601013
Epoch 670, training loss: 12.930343627929688 = 0.32868894934654236 + 2.0 * 6.300827503204346
Epoch 670, val loss: 0.8182880878448486
Epoch 680, training loss: 12.903735160827637 = 0.3130750060081482 + 2.0 * 6.295330047607422
Epoch 680, val loss: 0.8178167343139648
Epoch 690, training loss: 12.882464408874512 = 0.298106849193573 + 2.0 * 6.292178630828857
Epoch 690, val loss: 0.8182402849197388
Epoch 700, training loss: 12.865941047668457 = 0.2837426960468292 + 2.0 * 6.2910990715026855
Epoch 700, val loss: 0.8194708228111267
Epoch 710, training loss: 12.879415512084961 = 0.2699783146381378 + 2.0 * 6.304718494415283
Epoch 710, val loss: 0.8213755488395691
Epoch 720, training loss: 12.84728717803955 = 0.25706541538238525 + 2.0 * 6.295110702514648
Epoch 720, val loss: 0.8239692449569702
Epoch 730, training loss: 12.82226848602295 = 0.24473164975643158 + 2.0 * 6.288768291473389
Epoch 730, val loss: 0.8271618485450745
Epoch 740, training loss: 12.806575775146484 = 0.23297250270843506 + 2.0 * 6.286801815032959
Epoch 740, val loss: 0.8309587836265564
Epoch 750, training loss: 12.792689323425293 = 0.22169910371303558 + 2.0 * 6.285495281219482
Epoch 750, val loss: 0.8352517485618591
Epoch 760, training loss: 12.782818794250488 = 0.21091346442699432 + 2.0 * 6.285952568054199
Epoch 760, val loss: 0.8400641083717346
Epoch 770, training loss: 12.779184341430664 = 0.2006714940071106 + 2.0 * 6.289256572723389
Epoch 770, val loss: 0.8453418016433716
Epoch 780, training loss: 12.765331268310547 = 0.19097121059894562 + 2.0 * 6.287179946899414
Epoch 780, val loss: 0.8508943915367126
Epoch 790, training loss: 12.74968433380127 = 0.18175305426120758 + 2.0 * 6.283965587615967
Epoch 790, val loss: 0.8566925525665283
Epoch 800, training loss: 12.737292289733887 = 0.1730077564716339 + 2.0 * 6.282142162322998
Epoch 800, val loss: 0.8628727793693542
Epoch 810, training loss: 12.727185249328613 = 0.16464641690254211 + 2.0 * 6.281269550323486
Epoch 810, val loss: 0.869393527507782
Epoch 820, training loss: 12.719620704650879 = 0.15671177208423615 + 2.0 * 6.281454563140869
Epoch 820, val loss: 0.8762267231941223
Epoch 830, training loss: 12.712924003601074 = 0.1492013782262802 + 2.0 * 6.281861305236816
Epoch 830, val loss: 0.8832778334617615
Epoch 840, training loss: 12.697813034057617 = 0.1420772224664688 + 2.0 * 6.277867794036865
Epoch 840, val loss: 0.8904764652252197
Epoch 850, training loss: 12.69290828704834 = 0.13532453775405884 + 2.0 * 6.278791904449463
Epoch 850, val loss: 0.8979173898696899
Epoch 860, training loss: 12.688828468322754 = 0.1289282739162445 + 2.0 * 6.279950141906738
Epoch 860, val loss: 0.9055261015892029
Epoch 870, training loss: 12.67689037322998 = 0.12290368229150772 + 2.0 * 6.276993274688721
Epoch 870, val loss: 0.9132704734802246
Epoch 880, training loss: 12.667093276977539 = 0.117164246737957 + 2.0 * 6.274964332580566
Epoch 880, val loss: 0.9211874008178711
Epoch 890, training loss: 12.66370677947998 = 0.11175068467855453 + 2.0 * 6.275978088378906
Epoch 890, val loss: 0.9292904734611511
Epoch 900, training loss: 12.652835845947266 = 0.106656514108181 + 2.0 * 6.27308988571167
Epoch 900, val loss: 0.9373011589050293
Epoch 910, training loss: 12.64693546295166 = 0.10184179246425629 + 2.0 * 6.272546768188477
Epoch 910, val loss: 0.9455061554908752
Epoch 920, training loss: 12.659811019897461 = 0.0973048135638237 + 2.0 * 6.281253337860107
Epoch 920, val loss: 0.9537053108215332
Epoch 930, training loss: 12.64852523803711 = 0.09300986677408218 + 2.0 * 6.27775764465332
Epoch 930, val loss: 0.9618896245956421
Epoch 940, training loss: 12.628377914428711 = 0.08898603916168213 + 2.0 * 6.26969575881958
Epoch 940, val loss: 0.9700936675071716
Epoch 950, training loss: 12.623418807983398 = 0.08518721163272858 + 2.0 * 6.269115924835205
Epoch 950, val loss: 0.9783466458320618
Epoch 960, training loss: 12.618006706237793 = 0.08157853037118912 + 2.0 * 6.268214225769043
Epoch 960, val loss: 0.98667973279953
Epoch 970, training loss: 12.636691093444824 = 0.07817632704973221 + 2.0 * 6.279257297515869
Epoch 970, val loss: 0.9948694109916687
Epoch 980, training loss: 12.616357803344727 = 0.07493589073419571 + 2.0 * 6.2707109451293945
Epoch 980, val loss: 1.0032073259353638
Epoch 990, training loss: 12.605180740356445 = 0.07190750539302826 + 2.0 * 6.266636848449707
Epoch 990, val loss: 1.0114015340805054
Epoch 1000, training loss: 12.600946426391602 = 0.06902161240577698 + 2.0 * 6.265962600708008
Epoch 1000, val loss: 1.0196524858474731
Epoch 1010, training loss: 12.619234085083008 = 0.06630843132734299 + 2.0 * 6.276463031768799
Epoch 1010, val loss: 1.0278105735778809
Epoch 1020, training loss: 12.595097541809082 = 0.06372389942407608 + 2.0 * 6.265686988830566
Epoch 1020, val loss: 1.0358794927597046
Epoch 1030, training loss: 12.587660789489746 = 0.06129962578415871 + 2.0 * 6.263180732727051
Epoch 1030, val loss: 1.043928861618042
Epoch 1040, training loss: 12.58652400970459 = 0.05899028107523918 + 2.0 * 6.263766765594482
Epoch 1040, val loss: 1.0518181324005127
Epoch 1050, training loss: 12.590097427368164 = 0.05679813027381897 + 2.0 * 6.2666497230529785
Epoch 1050, val loss: 1.0598615407943726
Epoch 1060, training loss: 12.580011367797852 = 0.05473070591688156 + 2.0 * 6.262640476226807
Epoch 1060, val loss: 1.0677183866500854
Epoch 1070, training loss: 12.577917098999023 = 0.052769068628549576 + 2.0 * 6.262574195861816
Epoch 1070, val loss: 1.0754716396331787
Epoch 1080, training loss: 12.577881813049316 = 0.05090571567416191 + 2.0 * 6.263487815856934
Epoch 1080, val loss: 1.083170771598816
Epoch 1090, training loss: 12.580662727355957 = 0.04913908615708351 + 2.0 * 6.265761852264404
Epoch 1090, val loss: 1.0908634662628174
Epoch 1100, training loss: 12.567317008972168 = 0.047448091208934784 + 2.0 * 6.259934425354004
Epoch 1100, val loss: 1.0984143018722534
Epoch 1110, training loss: 12.566194534301758 = 0.04584730416536331 + 2.0 * 6.260173797607422
Epoch 1110, val loss: 1.1058913469314575
Epoch 1120, training loss: 12.568073272705078 = 0.04432625323534012 + 2.0 * 6.261873722076416
Epoch 1120, val loss: 1.113389492034912
Epoch 1130, training loss: 12.566131591796875 = 0.042877230793237686 + 2.0 * 6.261627197265625
Epoch 1130, val loss: 1.1207332611083984
Epoch 1140, training loss: 12.55620002746582 = 0.041491955518722534 + 2.0 * 6.257354259490967
Epoch 1140, val loss: 1.1279979944229126
Epoch 1150, training loss: 12.553223609924316 = 0.040169697254896164 + 2.0 * 6.256526947021484
Epoch 1150, val loss: 1.1352640390396118
Epoch 1160, training loss: 12.566269874572754 = 0.03890308365225792 + 2.0 * 6.263683319091797
Epoch 1160, val loss: 1.1424801349639893
Epoch 1170, training loss: 12.553878784179688 = 0.03770941123366356 + 2.0 * 6.258084774017334
Epoch 1170, val loss: 1.1494438648223877
Epoch 1180, training loss: 12.552525520324707 = 0.03656219691038132 + 2.0 * 6.257981777191162
Epoch 1180, val loss: 1.1564183235168457
Epoch 1190, training loss: 12.548972129821777 = 0.03547058254480362 + 2.0 * 6.256750583648682
Epoch 1190, val loss: 1.1632732152938843
Epoch 1200, training loss: 12.545943260192871 = 0.03442415967583656 + 2.0 * 6.2557597160339355
Epoch 1200, val loss: 1.1700890064239502
Epoch 1210, training loss: 12.546034812927246 = 0.03342157229781151 + 2.0 * 6.2563066482543945
Epoch 1210, val loss: 1.1767886877059937
Epoch 1220, training loss: 12.543830871582031 = 0.03246148303151131 + 2.0 * 6.255684852600098
Epoch 1220, val loss: 1.1834355592727661
Epoch 1230, training loss: 12.542421340942383 = 0.03154297545552254 + 2.0 * 6.255439281463623
Epoch 1230, val loss: 1.1898741722106934
Epoch 1240, training loss: 12.548225402832031 = 0.03066098503768444 + 2.0 * 6.258782386779785
Epoch 1240, val loss: 1.1965131759643555
Epoch 1250, training loss: 12.534503936767578 = 0.029827741906046867 + 2.0 * 6.25233793258667
Epoch 1250, val loss: 1.2027146816253662
Epoch 1260, training loss: 12.533483505249023 = 0.029026063159108162 + 2.0 * 6.252228736877441
Epoch 1260, val loss: 1.2090094089508057
Epoch 1270, training loss: 12.52977466583252 = 0.02825149893760681 + 2.0 * 6.25076150894165
Epoch 1270, val loss: 1.215150237083435
Epoch 1280, training loss: 12.528972625732422 = 0.02750406414270401 + 2.0 * 6.250734329223633
Epoch 1280, val loss: 1.2213054895401
Epoch 1290, training loss: 12.546402931213379 = 0.026786237955093384 + 2.0 * 6.259808540344238
Epoch 1290, val loss: 1.2273457050323486
Epoch 1300, training loss: 12.533239364624023 = 0.026096265763044357 + 2.0 * 6.253571510314941
Epoch 1300, val loss: 1.233506679534912
Epoch 1310, training loss: 12.53720474243164 = 0.02543696016073227 + 2.0 * 6.255883693695068
Epoch 1310, val loss: 1.2393851280212402
Epoch 1320, training loss: 12.527023315429688 = 0.024813290685415268 + 2.0 * 6.251104831695557
Epoch 1320, val loss: 1.2451001405715942
Epoch 1330, training loss: 12.520813941955566 = 0.02420234866440296 + 2.0 * 6.248305797576904
Epoch 1330, val loss: 1.2507914304733276
Epoch 1340, training loss: 12.521467208862305 = 0.023612482473254204 + 2.0 * 6.248927593231201
Epoch 1340, val loss: 1.2565367221832275
Epoch 1350, training loss: 12.524280548095703 = 0.023042555898427963 + 2.0 * 6.250618934631348
Epoch 1350, val loss: 1.2621848583221436
Epoch 1360, training loss: 12.526585578918457 = 0.022494899109005928 + 2.0 * 6.252045154571533
Epoch 1360, val loss: 1.2678570747375488
Epoch 1370, training loss: 12.520269393920898 = 0.02197166346013546 + 2.0 * 6.249148845672607
Epoch 1370, val loss: 1.2732558250427246
Epoch 1380, training loss: 12.516600608825684 = 0.021461987867951393 + 2.0 * 6.2475690841674805
Epoch 1380, val loss: 1.2786576747894287
Epoch 1390, training loss: 12.533398628234863 = 0.020970726385712624 + 2.0 * 6.256214141845703
Epoch 1390, val loss: 1.2840752601623535
Epoch 1400, training loss: 12.51793098449707 = 0.020504716783761978 + 2.0 * 6.24871301651001
Epoch 1400, val loss: 1.2894067764282227
Epoch 1410, training loss: 12.512954711914062 = 0.020049015060067177 + 2.0 * 6.246452808380127
Epoch 1410, val loss: 1.2944899797439575
Epoch 1420, training loss: 12.510136604309082 = 0.019608616828918457 + 2.0 * 6.245264053344727
Epoch 1420, val loss: 1.2996727228164673
Epoch 1430, training loss: 12.514065742492676 = 0.019181881099939346 + 2.0 * 6.24744176864624
Epoch 1430, val loss: 1.3047478199005127
Epoch 1440, training loss: 12.515698432922363 = 0.01877089962363243 + 2.0 * 6.2484636306762695
Epoch 1440, val loss: 1.3098336458206177
Epoch 1450, training loss: 12.513673782348633 = 0.018373465165495872 + 2.0 * 6.247650146484375
Epoch 1450, val loss: 1.3149476051330566
Epoch 1460, training loss: 12.5107421875 = 0.017986690625548363 + 2.0 * 6.246377944946289
Epoch 1460, val loss: 1.3198519945144653
Epoch 1470, training loss: 12.510333061218262 = 0.017617827281355858 + 2.0 * 6.246357440948486
Epoch 1470, val loss: 1.324641466140747
Epoch 1480, training loss: 12.505329132080078 = 0.017258429899811745 + 2.0 * 6.244035243988037
Epoch 1480, val loss: 1.3294217586517334
Epoch 1490, training loss: 12.509812355041504 = 0.01690918765962124 + 2.0 * 6.246451377868652
Epoch 1490, val loss: 1.3342328071594238
Epoch 1500, training loss: 12.506363868713379 = 0.016570938751101494 + 2.0 * 6.244896411895752
Epoch 1500, val loss: 1.3389278650283813
Epoch 1510, training loss: 12.504960060119629 = 0.016242176294326782 + 2.0 * 6.244359016418457
Epoch 1510, val loss: 1.3435242176055908
Epoch 1520, training loss: 12.502159118652344 = 0.015923958271741867 + 2.0 * 6.243117809295654
Epoch 1520, val loss: 1.3481652736663818
Epoch 1530, training loss: 12.512880325317383 = 0.015617976896464825 + 2.0 * 6.248631000518799
Epoch 1530, val loss: 1.3526206016540527
Epoch 1540, training loss: 12.50334644317627 = 0.015316111966967583 + 2.0 * 6.244015216827393
Epoch 1540, val loss: 1.3573421239852905
Epoch 1550, training loss: 12.499171257019043 = 0.01502723153680563 + 2.0 * 6.242072105407715
Epoch 1550, val loss: 1.361688494682312
Epoch 1560, training loss: 12.49923324584961 = 0.014745848253369331 + 2.0 * 6.242243766784668
Epoch 1560, val loss: 1.3660541772842407
Epoch 1570, training loss: 12.507411003112793 = 0.01447365153580904 + 2.0 * 6.246468544006348
Epoch 1570, val loss: 1.370469093322754
Epoch 1580, training loss: 12.497093200683594 = 0.014206655323505402 + 2.0 * 6.241443157196045
Epoch 1580, val loss: 1.3747634887695312
Epoch 1590, training loss: 12.49830436706543 = 0.013949040323495865 + 2.0 * 6.242177486419678
Epoch 1590, val loss: 1.3790409564971924
Epoch 1600, training loss: 12.5055570602417 = 0.01370143610984087 + 2.0 * 6.245927810668945
Epoch 1600, val loss: 1.3831868171691895
Epoch 1610, training loss: 12.49451732635498 = 0.01345427893102169 + 2.0 * 6.2405314445495605
Epoch 1610, val loss: 1.387411117553711
Epoch 1620, training loss: 12.491532325744629 = 0.013219027779996395 + 2.0 * 6.239156723022461
Epoch 1620, val loss: 1.3915040493011475
Epoch 1630, training loss: 12.491800308227539 = 0.012987914495170116 + 2.0 * 6.239406108856201
Epoch 1630, val loss: 1.395490050315857
Epoch 1640, training loss: 12.503300666809082 = 0.012761814519762993 + 2.0 * 6.245269298553467
Epoch 1640, val loss: 1.399519920349121
Epoch 1650, training loss: 12.4994535446167 = 0.012542922049760818 + 2.0 * 6.243455410003662
Epoch 1650, val loss: 1.403739333152771
Epoch 1660, training loss: 12.48983383178711 = 0.012333082966506481 + 2.0 * 6.238750457763672
Epoch 1660, val loss: 1.407492995262146
Epoch 1670, training loss: 12.495849609375 = 0.012126709334552288 + 2.0 * 6.241861343383789
Epoch 1670, val loss: 1.411430835723877
Epoch 1680, training loss: 12.48990535736084 = 0.01192445121705532 + 2.0 * 6.238990306854248
Epoch 1680, val loss: 1.415378451347351
Epoch 1690, training loss: 12.4893217086792 = 0.011728289537131786 + 2.0 * 6.238796710968018
Epoch 1690, val loss: 1.4190400838851929
Epoch 1700, training loss: 12.489383697509766 = 0.0115380072966218 + 2.0 * 6.238923072814941
Epoch 1700, val loss: 1.4228438138961792
Epoch 1710, training loss: 12.492219924926758 = 0.011351269669830799 + 2.0 * 6.240434169769287
Epoch 1710, val loss: 1.4264740943908691
Epoch 1720, training loss: 12.49354076385498 = 0.011171810328960419 + 2.0 * 6.241184711456299
Epoch 1720, val loss: 1.4302617311477661
Epoch 1730, training loss: 12.487616539001465 = 0.010997316800057888 + 2.0 * 6.238309383392334
Epoch 1730, val loss: 1.4339096546173096
Epoch 1740, training loss: 12.485493659973145 = 0.01082421001046896 + 2.0 * 6.237334728240967
Epoch 1740, val loss: 1.4374126195907593
Epoch 1750, training loss: 12.486235618591309 = 0.010657157748937607 + 2.0 * 6.237789154052734
Epoch 1750, val loss: 1.4410209655761719
Epoch 1760, training loss: 12.489542007446289 = 0.01049330085515976 + 2.0 * 6.2395243644714355
Epoch 1760, val loss: 1.4446285963058472
Epoch 1770, training loss: 12.48505687713623 = 0.010331660509109497 + 2.0 * 6.237362384796143
Epoch 1770, val loss: 1.4482802152633667
Epoch 1780, training loss: 12.48338508605957 = 0.010176334530115128 + 2.0 * 6.2366042137146
Epoch 1780, val loss: 1.4516348838806152
Epoch 1790, training loss: 12.494180679321289 = 0.010022453032433987 + 2.0 * 6.242079257965088
Epoch 1790, val loss: 1.4550503492355347
Epoch 1800, training loss: 12.48597240447998 = 0.009876782074570656 + 2.0 * 6.2380475997924805
Epoch 1800, val loss: 1.4584980010986328
Epoch 1810, training loss: 12.47951602935791 = 0.009732836857438087 + 2.0 * 6.234891414642334
Epoch 1810, val loss: 1.4618921279907227
Epoch 1820, training loss: 12.477005004882812 = 0.009591557085514069 + 2.0 * 6.233706951141357
Epoch 1820, val loss: 1.465139389038086
Epoch 1830, training loss: 12.478260040283203 = 0.009453109465539455 + 2.0 * 6.234403610229492
Epoch 1830, val loss: 1.4683529138565063
Epoch 1840, training loss: 12.488706588745117 = 0.009317755699157715 + 2.0 * 6.239694595336914
Epoch 1840, val loss: 1.4716111421585083
Epoch 1850, training loss: 12.487305641174316 = 0.009185729548335075 + 2.0 * 6.239059925079346
Epoch 1850, val loss: 1.47518789768219
Epoch 1860, training loss: 12.47883415222168 = 0.009056707844138145 + 2.0 * 6.234888553619385
Epoch 1860, val loss: 1.4781360626220703
Epoch 1870, training loss: 12.477933883666992 = 0.008929806761443615 + 2.0 * 6.234501838684082
Epoch 1870, val loss: 1.4814014434814453
Epoch 1880, training loss: 12.477240562438965 = 0.008807040750980377 + 2.0 * 6.234216690063477
Epoch 1880, val loss: 1.4844931364059448
Epoch 1890, training loss: 12.481027603149414 = 0.008686129935085773 + 2.0 * 6.236170768737793
Epoch 1890, val loss: 1.4877468347549438
Epoch 1900, training loss: 12.474653244018555 = 0.008568160235881805 + 2.0 * 6.2330427169799805
Epoch 1900, val loss: 1.4907476902008057
Epoch 1910, training loss: 12.476109504699707 = 0.008452901616692543 + 2.0 * 6.233828067779541
Epoch 1910, val loss: 1.493722915649414
Epoch 1920, training loss: 12.495087623596191 = 0.00834182184189558 + 2.0 * 6.243372917175293
Epoch 1920, val loss: 1.4967938661575317
Epoch 1930, training loss: 12.477434158325195 = 0.008228418417274952 + 2.0 * 6.234602928161621
Epoch 1930, val loss: 1.4998979568481445
Epoch 1940, training loss: 12.471952438354492 = 0.008123570121824741 + 2.0 * 6.231914520263672
Epoch 1940, val loss: 1.5027390718460083
Epoch 1950, training loss: 12.470499038696289 = 0.00801683496683836 + 2.0 * 6.231241226196289
Epoch 1950, val loss: 1.5057315826416016
Epoch 1960, training loss: 12.473220825195312 = 0.007912776432931423 + 2.0 * 6.232654094696045
Epoch 1960, val loss: 1.5086549520492554
Epoch 1970, training loss: 12.478673934936523 = 0.007810865994542837 + 2.0 * 6.235431671142578
Epoch 1970, val loss: 1.511500597000122
Epoch 1980, training loss: 12.473125457763672 = 0.007711931597441435 + 2.0 * 6.232706546783447
Epoch 1980, val loss: 1.5143914222717285
Epoch 1990, training loss: 12.468375205993652 = 0.007614179980009794 + 2.0 * 6.230380535125732
Epoch 1990, val loss: 1.5171458721160889
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7518518518518519
0.8081180811808119
=== training gcn model ===
Epoch 0, training loss: 19.150606155395508 = 1.9569132328033447 + 2.0 * 8.596846580505371
Epoch 0, val loss: 1.9543592929840088
Epoch 10, training loss: 19.13932991027832 = 1.9461489915847778 + 2.0 * 8.596590042114258
Epoch 10, val loss: 1.9442706108093262
Epoch 20, training loss: 19.121999740600586 = 1.9329001903533936 + 2.0 * 8.594550132751465
Epoch 20, val loss: 1.9313937425613403
Epoch 30, training loss: 19.06978416442871 = 1.9147145748138428 + 2.0 * 8.577534675598145
Epoch 30, val loss: 1.9131711721420288
Epoch 40, training loss: 18.85291862487793 = 1.8913263082504272 + 2.0 * 8.480795860290527
Epoch 40, val loss: 1.890296220779419
Epoch 50, training loss: 17.80184555053711 = 1.8683067560195923 + 2.0 * 7.966769218444824
Epoch 50, val loss: 1.8682442903518677
Epoch 60, training loss: 16.88042640686035 = 1.8482191562652588 + 2.0 * 7.516103267669678
Epoch 60, val loss: 1.8499870300292969
Epoch 70, training loss: 16.254804611206055 = 1.8320664167404175 + 2.0 * 7.211369514465332
Epoch 70, val loss: 1.835404872894287
Epoch 80, training loss: 15.832645416259766 = 1.8167985677719116 + 2.0 * 7.007923603057861
Epoch 80, val loss: 1.8215105533599854
Epoch 90, training loss: 15.61940860748291 = 1.8033140897750854 + 2.0 * 6.908047199249268
Epoch 90, val loss: 1.8091613054275513
Epoch 100, training loss: 15.480459213256836 = 1.7890287637710571 + 2.0 * 6.845715045928955
Epoch 100, val loss: 1.7959575653076172
Epoch 110, training loss: 15.335091590881348 = 1.7753795385360718 + 2.0 * 6.779856204986572
Epoch 110, val loss: 1.7835088968276978
Epoch 120, training loss: 15.209842681884766 = 1.7625967264175415 + 2.0 * 6.723622798919678
Epoch 120, val loss: 1.7721110582351685
Epoch 130, training loss: 15.094693183898926 = 1.7498142719268799 + 2.0 * 6.6724395751953125
Epoch 130, val loss: 1.7607531547546387
Epoch 140, training loss: 14.989424705505371 = 1.7359063625335693 + 2.0 * 6.626759052276611
Epoch 140, val loss: 1.7486763000488281
Epoch 150, training loss: 14.888452529907227 = 1.7205220460891724 + 2.0 * 6.583965301513672
Epoch 150, val loss: 1.7351634502410889
Epoch 160, training loss: 14.804343223571777 = 1.7031680345535278 + 2.0 * 6.5505876541137695
Epoch 160, val loss: 1.7202401161193848
Epoch 170, training loss: 14.737334251403809 = 1.6834098100662231 + 2.0 * 6.5269622802734375
Epoch 170, val loss: 1.7032891511917114
Epoch 180, training loss: 14.671364784240723 = 1.6609805822372437 + 2.0 * 6.505192279815674
Epoch 180, val loss: 1.6839722394943237
Epoch 190, training loss: 14.613106727600098 = 1.6353899240493774 + 2.0 * 6.488858222961426
Epoch 190, val loss: 1.662069320678711
Epoch 200, training loss: 14.558749198913574 = 1.6063588857650757 + 2.0 * 6.476195335388184
Epoch 200, val loss: 1.637511968612671
Epoch 210, training loss: 14.506117820739746 = 1.5739755630493164 + 2.0 * 6.466071128845215
Epoch 210, val loss: 1.6104282140731812
Epoch 220, training loss: 14.4501953125 = 1.5385470390319824 + 2.0 * 6.45582389831543
Epoch 220, val loss: 1.5809968709945679
Epoch 230, training loss: 14.392741203308105 = 1.4999178647994995 + 2.0 * 6.446411609649658
Epoch 230, val loss: 1.5492262840270996
Epoch 240, training loss: 14.335647583007812 = 1.4584369659423828 + 2.0 * 6.438605308532715
Epoch 240, val loss: 1.5155014991760254
Epoch 250, training loss: 14.278424263000488 = 1.4149619340896606 + 2.0 * 6.431731224060059
Epoch 250, val loss: 1.480645775794983
Epoch 260, training loss: 14.219122886657715 = 1.3707940578460693 + 2.0 * 6.424164295196533
Epoch 260, val loss: 1.4455403089523315
Epoch 270, training loss: 14.158937454223633 = 1.3262382745742798 + 2.0 * 6.416349411010742
Epoch 270, val loss: 1.410954475402832
Epoch 280, training loss: 14.100433349609375 = 1.2818061113357544 + 2.0 * 6.409313678741455
Epoch 280, val loss: 1.3769028186798096
Epoch 290, training loss: 14.046744346618652 = 1.2379748821258545 + 2.0 * 6.404384613037109
Epoch 290, val loss: 1.3438307046890259
Epoch 300, training loss: 13.989720344543457 = 1.1954675912857056 + 2.0 * 6.397126197814941
Epoch 300, val loss: 1.312354326248169
Epoch 310, training loss: 13.935957908630371 = 1.1541930437088013 + 2.0 * 6.39088249206543
Epoch 310, val loss: 1.2823002338409424
Epoch 320, training loss: 13.896830558776855 = 1.1140269041061401 + 2.0 * 6.391401767730713
Epoch 320, val loss: 1.25347900390625
Epoch 330, training loss: 13.844377517700195 = 1.0756255388259888 + 2.0 * 6.384376049041748
Epoch 330, val loss: 1.2262295484542847
Epoch 340, training loss: 13.79049015045166 = 1.0388820171356201 + 2.0 * 6.3758039474487305
Epoch 340, val loss: 1.2002944946289062
Epoch 350, training loss: 13.74500846862793 = 1.0033406019210815 + 2.0 * 6.370833873748779
Epoch 350, val loss: 1.1755344867706299
Epoch 360, training loss: 13.705804824829102 = 0.968935489654541 + 2.0 * 6.368434906005859
Epoch 360, val loss: 1.1516844034194946
Epoch 370, training loss: 13.664953231811523 = 0.9358752369880676 + 2.0 * 6.36453914642334
Epoch 370, val loss: 1.1289503574371338
Epoch 380, training loss: 13.621600151062012 = 0.9042453765869141 + 2.0 * 6.358677387237549
Epoch 380, val loss: 1.1072531938552856
Epoch 390, training loss: 13.584139823913574 = 0.8737228512763977 + 2.0 * 6.355208396911621
Epoch 390, val loss: 1.086410403251648
Epoch 400, training loss: 13.548097610473633 = 0.8441166281700134 + 2.0 * 6.351990699768066
Epoch 400, val loss: 1.0663408041000366
Epoch 410, training loss: 13.528295516967773 = 0.8153702020645142 + 2.0 * 6.356462478637695
Epoch 410, val loss: 1.0471137762069702
Epoch 420, training loss: 13.48591423034668 = 0.7878669500350952 + 2.0 * 6.349023818969727
Epoch 420, val loss: 1.0287816524505615
Epoch 430, training loss: 13.449758529663086 = 0.7612343430519104 + 2.0 * 6.34426212310791
Epoch 430, val loss: 1.0114079713821411
Epoch 440, training loss: 13.422347068786621 = 0.7353536486625671 + 2.0 * 6.343496799468994
Epoch 440, val loss: 0.9948541522026062
Epoch 450, training loss: 13.393893241882324 = 0.7104483246803284 + 2.0 * 6.34172248840332
Epoch 450, val loss: 0.979215145111084
Epoch 460, training loss: 13.3624906539917 = 0.6862695217132568 + 2.0 * 6.338110446929932
Epoch 460, val loss: 0.9645848870277405
Epoch 470, training loss: 13.333418846130371 = 0.662911057472229 + 2.0 * 6.335253715515137
Epoch 470, val loss: 0.9509210586547852
Epoch 480, training loss: 13.305418968200684 = 0.6402848362922668 + 2.0 * 6.33256721496582
Epoch 480, val loss: 0.9381654858589172
Epoch 490, training loss: 13.279292106628418 = 0.618270754814148 + 2.0 * 6.33051061630249
Epoch 490, val loss: 0.9262950420379639
Epoch 500, training loss: 13.26308822631836 = 0.5967497825622559 + 2.0 * 6.333169460296631
Epoch 500, val loss: 0.9152547121047974
Epoch 510, training loss: 13.231958389282227 = 0.5759899020195007 + 2.0 * 6.32798433303833
Epoch 510, val loss: 0.9050217866897583
Epoch 520, training loss: 13.205967903137207 = 0.5558376312255859 + 2.0 * 6.3250651359558105
Epoch 520, val loss: 0.8955828547477722
Epoch 530, training loss: 13.187882423400879 = 0.5361423492431641 + 2.0 * 6.325870037078857
Epoch 530, val loss: 0.8868367075920105
Epoch 540, training loss: 13.161099433898926 = 0.5169791579246521 + 2.0 * 6.3220601081848145
Epoch 540, val loss: 0.8787267208099365
Epoch 550, training loss: 13.141708374023438 = 0.498162180185318 + 2.0 * 6.321773052215576
Epoch 550, val loss: 0.8712974786758423
Epoch 560, training loss: 13.133338928222656 = 0.47995078563690186 + 2.0 * 6.326694011688232
Epoch 560, val loss: 0.8641640543937683
Epoch 570, training loss: 13.101667404174805 = 0.46217289566993713 + 2.0 * 6.319747447967529
Epoch 570, val loss: 0.8577859997749329
Epoch 580, training loss: 13.075557708740234 = 0.444742351770401 + 2.0 * 6.315407752990723
Epoch 580, val loss: 0.8517575860023499
Epoch 590, training loss: 13.05517864227295 = 0.4276113510131836 + 2.0 * 6.313783645629883
Epoch 590, val loss: 0.8462413549423218
Epoch 600, training loss: 13.052470207214355 = 0.4107663631439209 + 2.0 * 6.320851802825928
Epoch 600, val loss: 0.8412137627601624
Epoch 610, training loss: 13.017631530761719 = 0.3943771421909332 + 2.0 * 6.311627388000488
Epoch 610, val loss: 0.8365870714187622
Epoch 620, training loss: 12.99847412109375 = 0.3783343434333801 + 2.0 * 6.310070037841797
Epoch 620, val loss: 0.8324477076530457
Epoch 630, training loss: 12.98116683959961 = 0.36259710788726807 + 2.0 * 6.309284687042236
Epoch 630, val loss: 0.8287559747695923
Epoch 640, training loss: 12.961870193481445 = 0.34722888469696045 + 2.0 * 6.307320594787598
Epoch 640, val loss: 0.8255846500396729
Epoch 650, training loss: 12.94694995880127 = 0.3323124647140503 + 2.0 * 6.307318687438965
Epoch 650, val loss: 0.8228034377098083
Epoch 660, training loss: 12.927643775939941 = 0.31779175996780396 + 2.0 * 6.304925918579102
Epoch 660, val loss: 0.8205409049987793
Epoch 670, training loss: 12.911585807800293 = 0.30365970730781555 + 2.0 * 6.3039631843566895
Epoch 670, val loss: 0.8188647627830505
Epoch 680, training loss: 12.895957946777344 = 0.2899508476257324 + 2.0 * 6.303003787994385
Epoch 680, val loss: 0.8176918029785156
Epoch 690, training loss: 12.881624221801758 = 0.2767380475997925 + 2.0 * 6.302443027496338
Epoch 690, val loss: 0.8169534206390381
Epoch 700, training loss: 12.865652084350586 = 0.26400965452194214 + 2.0 * 6.300821304321289
Epoch 700, val loss: 0.8166795372962952
Epoch 710, training loss: 12.868136405944824 = 0.25171926617622375 + 2.0 * 6.308208465576172
Epoch 710, val loss: 0.816927433013916
Epoch 720, training loss: 12.840185165405273 = 0.2399117648601532 + 2.0 * 6.300136566162109
Epoch 720, val loss: 0.8176425695419312
Epoch 730, training loss: 12.82393741607666 = 0.22862426936626434 + 2.0 * 6.297656536102295
Epoch 730, val loss: 0.8187564611434937
Epoch 740, training loss: 12.809809684753418 = 0.21779051423072815 + 2.0 * 6.296009540557861
Epoch 740, val loss: 0.8203811049461365
Epoch 750, training loss: 12.810369491577148 = 0.2073947936296463 + 2.0 * 6.301487445831299
Epoch 750, val loss: 0.8225414156913757
Epoch 760, training loss: 12.786516189575195 = 0.19753247499465942 + 2.0 * 6.294491767883301
Epoch 760, val loss: 0.824983537197113
Epoch 770, training loss: 12.774595260620117 = 0.18815456330776215 + 2.0 * 6.293220520019531
Epoch 770, val loss: 0.8278297185897827
Epoch 780, training loss: 12.762890815734863 = 0.17918674647808075 + 2.0 * 6.291851997375488
Epoch 780, val loss: 0.8310952186584473
Epoch 790, training loss: 12.760367393493652 = 0.17065364122390747 + 2.0 * 6.294857025146484
Epoch 790, val loss: 0.8347577452659607
Epoch 800, training loss: 12.74686336517334 = 0.1625322848558426 + 2.0 * 6.292165756225586
Epoch 800, val loss: 0.8387665152549744
Epoch 810, training loss: 12.73433780670166 = 0.1548336148262024 + 2.0 * 6.289752006530762
Epoch 810, val loss: 0.8429243564605713
Epoch 820, training loss: 12.731710433959961 = 0.14753296971321106 + 2.0 * 6.292088508605957
Epoch 820, val loss: 0.8474081754684448
Epoch 830, training loss: 12.720393180847168 = 0.1406269669532776 + 2.0 * 6.289883136749268
Epoch 830, val loss: 0.8521357178688049
Epoch 840, training loss: 12.708206176757812 = 0.1340828537940979 + 2.0 * 6.28706169128418
Epoch 840, val loss: 0.8569250702857971
Epoch 850, training loss: 12.696666717529297 = 0.12790493667125702 + 2.0 * 6.284380912780762
Epoch 850, val loss: 0.8620004653930664
Epoch 860, training loss: 12.691306114196777 = 0.12204115092754364 + 2.0 * 6.284632682800293
Epoch 860, val loss: 0.867279589176178
Epoch 870, training loss: 12.69067668914795 = 0.11648780107498169 + 2.0 * 6.287094593048096
Epoch 870, val loss: 0.8726154565811157
Epoch 880, training loss: 12.677332878112793 = 0.11121586710214615 + 2.0 * 6.2830586433410645
Epoch 880, val loss: 0.8780233860015869
Epoch 890, training loss: 12.668625831604004 = 0.10625535994768143 + 2.0 * 6.281185150146484
Epoch 890, val loss: 0.8835192918777466
Epoch 900, training loss: 12.671528816223145 = 0.10156227648258209 + 2.0 * 6.284983158111572
Epoch 900, val loss: 0.889127790927887
Epoch 910, training loss: 12.659832000732422 = 0.09710098803043365 + 2.0 * 6.281365394592285
Epoch 910, val loss: 0.8948153257369995
Epoch 920, training loss: 12.65311050415039 = 0.0929177775979042 + 2.0 * 6.280096530914307
Epoch 920, val loss: 0.9004972577095032
Epoch 930, training loss: 12.665715217590332 = 0.08893503993749619 + 2.0 * 6.288390159606934
Epoch 930, val loss: 0.9062173962593079
Epoch 940, training loss: 12.645556449890137 = 0.08519493788480759 + 2.0 * 6.280180931091309
Epoch 940, val loss: 0.9118777513504028
Epoch 950, training loss: 12.633722305297852 = 0.08165165036916733 + 2.0 * 6.276035308837891
Epoch 950, val loss: 0.917600154876709
Epoch 960, training loss: 12.63010311126709 = 0.07829497754573822 + 2.0 * 6.275904178619385
Epoch 960, val loss: 0.9233534932136536
Epoch 970, training loss: 12.626646995544434 = 0.0751049742102623 + 2.0 * 6.275771141052246
Epoch 970, val loss: 0.9291634559631348
Epoch 980, training loss: 12.62182331085205 = 0.07208301872015 + 2.0 * 6.274869918823242
Epoch 980, val loss: 0.9349679946899414
Epoch 990, training loss: 12.621833801269531 = 0.0692293718457222 + 2.0 * 6.276302337646484
Epoch 990, val loss: 0.9405815601348877
Epoch 1000, training loss: 12.61361312866211 = 0.06652969121932983 + 2.0 * 6.2735419273376465
Epoch 1000, val loss: 0.9462264180183411
Epoch 1010, training loss: 12.607701301574707 = 0.06396684050559998 + 2.0 * 6.271867275238037
Epoch 1010, val loss: 0.9519035220146179
Epoch 1020, training loss: 12.627999305725098 = 0.061530087143182755 + 2.0 * 6.283234596252441
Epoch 1020, val loss: 0.9575770497322083
Epoch 1030, training loss: 12.602131843566895 = 0.05922112241387367 + 2.0 * 6.27145528793335
Epoch 1030, val loss: 0.9631941914558411
Epoch 1040, training loss: 12.60069465637207 = 0.05703071132302284 + 2.0 * 6.27183198928833
Epoch 1040, val loss: 0.9687603116035461
Epoch 1050, training loss: 12.600335121154785 = 0.054945074021816254 + 2.0 * 6.272695064544678
Epoch 1050, val loss: 0.9742138981819153
Epoch 1060, training loss: 12.59536075592041 = 0.05295773968100548 + 2.0 * 6.2712016105651855
Epoch 1060, val loss: 0.9795701503753662
Epoch 1070, training loss: 12.594887733459473 = 0.051074277609586716 + 2.0 * 6.271906852722168
Epoch 1070, val loss: 0.9850667119026184
Epoch 1080, training loss: 12.584162712097168 = 0.04928181320428848 + 2.0 * 6.267440319061279
Epoch 1080, val loss: 0.9904277920722961
Epoch 1090, training loss: 12.581441879272461 = 0.04757429286837578 + 2.0 * 6.266933917999268
Epoch 1090, val loss: 0.9958370923995972
Epoch 1100, training loss: 12.586177825927734 = 0.04594459757208824 + 2.0 * 6.270116806030273
Epoch 1100, val loss: 1.0011570453643799
Epoch 1110, training loss: 12.586677551269531 = 0.04437826946377754 + 2.0 * 6.271149635314941
Epoch 1110, val loss: 1.00627863407135
Epoch 1120, training loss: 12.584104537963867 = 0.04289638623595238 + 2.0 * 6.270604133605957
Epoch 1120, val loss: 1.0114195346832275
Epoch 1130, training loss: 12.572745323181152 = 0.04148349165916443 + 2.0 * 6.265630722045898
Epoch 1130, val loss: 1.0164721012115479
Epoch 1140, training loss: 12.566995620727539 = 0.040138307958841324 + 2.0 * 6.263428688049316
Epoch 1140, val loss: 1.0214145183563232
Epoch 1150, training loss: 12.564987182617188 = 0.038853708654642105 + 2.0 * 6.26306676864624
Epoch 1150, val loss: 1.0264006853103638
Epoch 1160, training loss: 12.565719604492188 = 0.0376233272254467 + 2.0 * 6.264048099517822
Epoch 1160, val loss: 1.0314407348632812
Epoch 1170, training loss: 12.565937995910645 = 0.03644314408302307 + 2.0 * 6.264747619628906
Epoch 1170, val loss: 1.0363121032714844
Epoch 1180, training loss: 12.560407638549805 = 0.035308871418237686 + 2.0 * 6.26254940032959
Epoch 1180, val loss: 1.0411182641983032
Epoch 1190, training loss: 12.566174507141113 = 0.03423456475138664 + 2.0 * 6.265969753265381
Epoch 1190, val loss: 1.0458614826202393
Epoch 1200, training loss: 12.555755615234375 = 0.03319823369383812 + 2.0 * 6.2612786293029785
Epoch 1200, val loss: 1.0504182577133179
Epoch 1210, training loss: 12.559402465820312 = 0.03221258521080017 + 2.0 * 6.263595104217529
Epoch 1210, val loss: 1.0550554990768433
Epoch 1220, training loss: 12.549856185913086 = 0.03126246854662895 + 2.0 * 6.259296894073486
Epoch 1220, val loss: 1.0595130920410156
Epoch 1230, training loss: 12.549022674560547 = 0.030357444658875465 + 2.0 * 6.259332656860352
Epoch 1230, val loss: 1.0640355348587036
Epoch 1240, training loss: 12.551508903503418 = 0.02948663756251335 + 2.0 * 6.261011123657227
Epoch 1240, val loss: 1.0684551000595093
Epoch 1250, training loss: 12.542821884155273 = 0.02865483984351158 + 2.0 * 6.257083415985107
Epoch 1250, val loss: 1.0729060173034668
Epoch 1260, training loss: 12.542980194091797 = 0.027855709195137024 + 2.0 * 6.257562160491943
Epoch 1260, val loss: 1.0772550106048584
Epoch 1270, training loss: 12.55286979675293 = 0.027085477486252785 + 2.0 * 6.262892246246338
Epoch 1270, val loss: 1.0815023183822632
Epoch 1280, training loss: 12.546110153198242 = 0.026342276483774185 + 2.0 * 6.259883880615234
Epoch 1280, val loss: 1.0857031345367432
Epoch 1290, training loss: 12.537761688232422 = 0.025635013356804848 + 2.0 * 6.256063461303711
Epoch 1290, val loss: 1.0898547172546387
Epoch 1300, training loss: 12.53614616394043 = 0.02495533414185047 + 2.0 * 6.2555952072143555
Epoch 1300, val loss: 1.0939257144927979
Epoch 1310, training loss: 12.545685768127441 = 0.024299541488289833 + 2.0 * 6.260693073272705
Epoch 1310, val loss: 1.0979660749435425
Epoch 1320, training loss: 12.533294677734375 = 0.02367194928228855 + 2.0 * 6.2548112869262695
Epoch 1320, val loss: 1.1020528078079224
Epoch 1330, training loss: 12.530397415161133 = 0.023064110428094864 + 2.0 * 6.253666877746582
Epoch 1330, val loss: 1.105993390083313
Epoch 1340, training loss: 12.547115325927734 = 0.02247783914208412 + 2.0 * 6.2623186111450195
Epoch 1340, val loss: 1.1098992824554443
Epoch 1350, training loss: 12.529679298400879 = 0.021919235587120056 + 2.0 * 6.253880023956299
Epoch 1350, val loss: 1.113720178604126
Epoch 1360, training loss: 12.525181770324707 = 0.021379748359322548 + 2.0 * 6.251901149749756
Epoch 1360, val loss: 1.1175719499588013
Epoch 1370, training loss: 12.524107933044434 = 0.02085943892598152 + 2.0 * 6.25162410736084
Epoch 1370, val loss: 1.121288776397705
Epoch 1380, training loss: 12.52631950378418 = 0.020358137786388397 + 2.0 * 6.252980709075928
Epoch 1380, val loss: 1.1250801086425781
Epoch 1390, training loss: 12.525312423706055 = 0.019872162491083145 + 2.0 * 6.252720355987549
Epoch 1390, val loss: 1.128741979598999
Epoch 1400, training loss: 12.529346466064453 = 0.019405711442232132 + 2.0 * 6.254970550537109
Epoch 1400, val loss: 1.1324694156646729
Epoch 1410, training loss: 12.520559310913086 = 0.018951835110783577 + 2.0 * 6.2508039474487305
Epoch 1410, val loss: 1.135900855064392
Epoch 1420, training loss: 12.519145011901855 = 0.018516773357987404 + 2.0 * 6.250314235687256
Epoch 1420, val loss: 1.139402985572815
Epoch 1430, training loss: 12.516122817993164 = 0.018099091947078705 + 2.0 * 6.249011993408203
Epoch 1430, val loss: 1.1428916454315186
Epoch 1440, training loss: 12.521839141845703 = 0.017693817615509033 + 2.0 * 6.252072811126709
Epoch 1440, val loss: 1.1463122367858887
Epoch 1450, training loss: 12.513843536376953 = 0.01730278693139553 + 2.0 * 6.248270511627197
Epoch 1450, val loss: 1.1497632265090942
Epoch 1460, training loss: 12.513429641723633 = 0.01692422293126583 + 2.0 * 6.248252868652344
Epoch 1460, val loss: 1.1530702114105225
Epoch 1470, training loss: 12.522372245788574 = 0.01656031422317028 + 2.0 * 6.25290584564209
Epoch 1470, val loss: 1.1563630104064941
Epoch 1480, training loss: 12.511688232421875 = 0.01620364934206009 + 2.0 * 6.247742176055908
Epoch 1480, val loss: 1.1595896482467651
Epoch 1490, training loss: 12.509058952331543 = 0.01586148887872696 + 2.0 * 6.246598720550537
Epoch 1490, val loss: 1.1628057956695557
Epoch 1500, training loss: 12.511796951293945 = 0.015531344339251518 + 2.0 * 6.248132705688477
Epoch 1500, val loss: 1.1660019159317017
Epoch 1510, training loss: 12.50908088684082 = 0.015209573321044445 + 2.0 * 6.246935844421387
Epoch 1510, val loss: 1.169188380241394
Epoch 1520, training loss: 12.519363403320312 = 0.014899765141308308 + 2.0 * 6.252231597900391
Epoch 1520, val loss: 1.1723421812057495
Epoch 1530, training loss: 12.507291793823242 = 0.014597226865589619 + 2.0 * 6.246347427368164
Epoch 1530, val loss: 1.1752715110778809
Epoch 1540, training loss: 12.504260063171387 = 0.014305990189313889 + 2.0 * 6.244976997375488
Epoch 1540, val loss: 1.1783697605133057
Epoch 1550, training loss: 12.511821746826172 = 0.014025560580193996 + 2.0 * 6.248898029327393
Epoch 1550, val loss: 1.1813633441925049
Epoch 1560, training loss: 12.503114700317383 = 0.013749202713370323 + 2.0 * 6.244682788848877
Epoch 1560, val loss: 1.1842554807662964
Epoch 1570, training loss: 12.503273963928223 = 0.013483719900250435 + 2.0 * 6.244894981384277
Epoch 1570, val loss: 1.1871827840805054
Epoch 1580, training loss: 12.505208969116211 = 0.0132265230640769 + 2.0 * 6.245991230010986
Epoch 1580, val loss: 1.1900622844696045
Epoch 1590, training loss: 12.499832153320312 = 0.012976892292499542 + 2.0 * 6.243427753448486
Epoch 1590, val loss: 1.19292151927948
Epoch 1600, training loss: 12.507314682006836 = 0.012734069488942623 + 2.0 * 6.247290134429932
Epoch 1600, val loss: 1.195755958557129
Epoch 1610, training loss: 12.50011920928955 = 0.012498106807470322 + 2.0 * 6.243810653686523
Epoch 1610, val loss: 1.1985118389129639
Epoch 1620, training loss: 12.49591064453125 = 0.012269786559045315 + 2.0 * 6.241820335388184
Epoch 1620, val loss: 1.2012829780578613
Epoch 1630, training loss: 12.499683380126953 = 0.012048937380313873 + 2.0 * 6.243817329406738
Epoch 1630, val loss: 1.2040163278579712
Epoch 1640, training loss: 12.501919746398926 = 0.011833002790808678 + 2.0 * 6.2450432777404785
Epoch 1640, val loss: 1.2067146301269531
Epoch 1650, training loss: 12.497045516967773 = 0.011623149737715721 + 2.0 * 6.242711067199707
Epoch 1650, val loss: 1.2093920707702637
Epoch 1660, training loss: 12.493478775024414 = 0.011419367976486683 + 2.0 * 6.241029739379883
Epoch 1660, val loss: 1.2119837999343872
Epoch 1670, training loss: 12.499283790588379 = 0.011221989057958126 + 2.0 * 6.244030952453613
Epoch 1670, val loss: 1.214647889137268
Epoch 1680, training loss: 12.495116233825684 = 0.011028108187019825 + 2.0 * 6.242043972015381
Epoch 1680, val loss: 1.21718168258667
Epoch 1690, training loss: 12.49538803100586 = 0.010840637609362602 + 2.0 * 6.242273807525635
Epoch 1690, val loss: 1.2197177410125732
Epoch 1700, training loss: 12.492106437683105 = 0.010657647624611855 + 2.0 * 6.240724563598633
Epoch 1700, val loss: 1.2221561670303345
Epoch 1710, training loss: 12.489227294921875 = 0.01048019528388977 + 2.0 * 6.239373683929443
Epoch 1710, val loss: 1.2247073650360107
Epoch 1720, training loss: 12.494328498840332 = 0.010307924821972847 + 2.0 * 6.242010116577148
Epoch 1720, val loss: 1.2272032499313354
Epoch 1730, training loss: 12.495574951171875 = 0.01013830304145813 + 2.0 * 6.24271821975708
Epoch 1730, val loss: 1.2296123504638672
Epoch 1740, training loss: 12.487028121948242 = 0.009972906671464443 + 2.0 * 6.238527774810791
Epoch 1740, val loss: 1.2319258451461792
Epoch 1750, training loss: 12.485352516174316 = 0.009814118035137653 + 2.0 * 6.23776912689209
Epoch 1750, val loss: 1.2343443632125854
Epoch 1760, training loss: 12.494812965393066 = 0.009658935479819775 + 2.0 * 6.242577075958252
Epoch 1760, val loss: 1.2367689609527588
Epoch 1770, training loss: 12.483561515808105 = 0.00950659066438675 + 2.0 * 6.237027645111084
Epoch 1770, val loss: 1.2390050888061523
Epoch 1780, training loss: 12.48442554473877 = 0.0093596912920475 + 2.0 * 6.237533092498779
Epoch 1780, val loss: 1.241355538368225
Epoch 1790, training loss: 12.495392799377441 = 0.009216638281941414 + 2.0 * 6.243088245391846
Epoch 1790, val loss: 1.2436126470565796
Epoch 1800, training loss: 12.486202239990234 = 0.009074091911315918 + 2.0 * 6.2385640144348145
Epoch 1800, val loss: 1.2457743883132935
Epoch 1810, training loss: 12.480144500732422 = 0.008937892504036427 + 2.0 * 6.235603332519531
Epoch 1810, val loss: 1.2480729818344116
Epoch 1820, training loss: 12.479087829589844 = 0.008804183453321457 + 2.0 * 6.235141754150391
Epoch 1820, val loss: 1.2502762079238892
Epoch 1830, training loss: 12.481598854064941 = 0.008672588504850864 + 2.0 * 6.2364630699157715
Epoch 1830, val loss: 1.2524839639663696
Epoch 1840, training loss: 12.490606307983398 = 0.008544259704649448 + 2.0 * 6.241031169891357
Epoch 1840, val loss: 1.2546230554580688
Epoch 1850, training loss: 12.47856616973877 = 0.008420142345130444 + 2.0 * 6.235073089599609
Epoch 1850, val loss: 1.2568470239639282
Epoch 1860, training loss: 12.482037544250488 = 0.008298169821500778 + 2.0 * 6.236869812011719
Epoch 1860, val loss: 1.2589828968048096
Epoch 1870, training loss: 12.478336334228516 = 0.00817873701453209 + 2.0 * 6.235078811645508
Epoch 1870, val loss: 1.26090407371521
Epoch 1880, training loss: 12.476043701171875 = 0.008063866756856441 + 2.0 * 6.233989715576172
Epoch 1880, val loss: 1.2630095481872559
Epoch 1890, training loss: 12.47506046295166 = 0.007951111532747746 + 2.0 * 6.233554840087891
Epoch 1890, val loss: 1.2649654150009155
Epoch 1900, training loss: 12.473999977111816 = 0.007840686477720737 + 2.0 * 6.233079433441162
Epoch 1900, val loss: 1.2670692205429077
Epoch 1910, training loss: 12.474471092224121 = 0.007732019294053316 + 2.0 * 6.23336935043335
Epoch 1910, val loss: 1.2691529989242554
Epoch 1920, training loss: 12.49123477935791 = 0.00762559287250042 + 2.0 * 6.241804599761963
Epoch 1920, val loss: 1.2712020874023438
Epoch 1930, training loss: 12.487030982971191 = 0.007520738989114761 + 2.0 * 6.239755153656006
Epoch 1930, val loss: 1.2731070518493652
Epoch 1940, training loss: 12.475927352905273 = 0.007419147994369268 + 2.0 * 6.234253883361816
Epoch 1940, val loss: 1.274901270866394
Epoch 1950, training loss: 12.472039222717285 = 0.007321690209209919 + 2.0 * 6.232358932495117
Epoch 1950, val loss: 1.2768744230270386
Epoch 1960, training loss: 12.471214294433594 = 0.0072252461686730385 + 2.0 * 6.23199462890625
Epoch 1960, val loss: 1.2787891626358032
Epoch 1970, training loss: 12.477453231811523 = 0.00713114719837904 + 2.0 * 6.235160827636719
Epoch 1970, val loss: 1.2807568311691284
Epoch 1980, training loss: 12.473335266113281 = 0.00703797023743391 + 2.0 * 6.233148574829102
Epoch 1980, val loss: 1.2825278043746948
Epoch 1990, training loss: 12.47012710571289 = 0.006946203764528036 + 2.0 * 6.231590270996094
Epoch 1990, val loss: 1.2843501567840576
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7555555555555555
0.8128624143384291
=== training gcn model ===
Epoch 0, training loss: 19.137414932250977 = 1.9437371492385864 + 2.0 * 8.59683895111084
Epoch 0, val loss: 1.9510540962219238
Epoch 10, training loss: 19.127334594726562 = 1.9342172145843506 + 2.0 * 8.596558570861816
Epoch 10, val loss: 1.9419928789138794
Epoch 20, training loss: 19.111011505126953 = 1.9222229719161987 + 2.0 * 8.59439468383789
Epoch 20, val loss: 1.9298973083496094
Epoch 30, training loss: 19.062097549438477 = 1.9054359197616577 + 2.0 * 8.578330993652344
Epoch 30, val loss: 1.9124070405960083
Epoch 40, training loss: 18.867727279663086 = 1.8838136196136475 + 2.0 * 8.49195671081543
Epoch 40, val loss: 1.8904459476470947
Epoch 50, training loss: 17.87400245666504 = 1.8624241352081299 + 2.0 * 8.005788803100586
Epoch 50, val loss: 1.8687353134155273
Epoch 60, training loss: 17.057809829711914 = 1.8436979055404663 + 2.0 * 7.607056140899658
Epoch 60, val loss: 1.8508490324020386
Epoch 70, training loss: 16.466760635375977 = 1.8285578489303589 + 2.0 * 7.319101333618164
Epoch 70, val loss: 1.8356965780258179
Epoch 80, training loss: 16.03675079345703 = 1.8133726119995117 + 2.0 * 7.111688613891602
Epoch 80, val loss: 1.8210375308990479
Epoch 90, training loss: 15.738463401794434 = 1.8007065057754517 + 2.0 * 6.968878269195557
Epoch 90, val loss: 1.808833360671997
Epoch 100, training loss: 15.509692192077637 = 1.789840579032898 + 2.0 * 6.859925746917725
Epoch 100, val loss: 1.7982611656188965
Epoch 110, training loss: 15.33443546295166 = 1.7785570621490479 + 2.0 * 6.777939319610596
Epoch 110, val loss: 1.7876664400100708
Epoch 120, training loss: 15.211685180664062 = 1.7655389308929443 + 2.0 * 6.7230730056762695
Epoch 120, val loss: 1.776206612586975
Epoch 130, training loss: 15.126058578491211 = 1.7514687776565552 + 2.0 * 6.687294960021973
Epoch 130, val loss: 1.7643240690231323
Epoch 140, training loss: 15.053829193115234 = 1.7361505031585693 + 2.0 * 6.658839225769043
Epoch 140, val loss: 1.7517902851104736
Epoch 150, training loss: 14.981740951538086 = 1.7197399139404297 + 2.0 * 6.631000518798828
Epoch 150, val loss: 1.7388250827789307
Epoch 160, training loss: 14.908775329589844 = 1.7021338939666748 + 2.0 * 6.603320598602295
Epoch 160, val loss: 1.7250847816467285
Epoch 170, training loss: 14.838134765625 = 1.6823320388793945 + 2.0 * 6.577901363372803
Epoch 170, val loss: 1.709838628768921
Epoch 180, training loss: 14.770380973815918 = 1.659628987312317 + 2.0 * 6.555376052856445
Epoch 180, val loss: 1.6923753023147583
Epoch 190, training loss: 14.71278190612793 = 1.6335721015930176 + 2.0 * 6.539604663848877
Epoch 190, val loss: 1.6725331544876099
Epoch 200, training loss: 14.64150619506836 = 1.6045169830322266 + 2.0 * 6.518494606018066
Epoch 200, val loss: 1.650498867034912
Epoch 210, training loss: 14.576679229736328 = 1.572008728981018 + 2.0 * 6.502335071563721
Epoch 210, val loss: 1.6259324550628662
Epoch 220, training loss: 14.508899688720703 = 1.5359536409378052 + 2.0 * 6.486473083496094
Epoch 220, val loss: 1.5986320972442627
Epoch 230, training loss: 14.442190170288086 = 1.4965864419937134 + 2.0 * 6.472801685333252
Epoch 230, val loss: 1.5691503286361694
Epoch 240, training loss: 14.375322341918945 = 1.4549072980880737 + 2.0 * 6.460207462310791
Epoch 240, val loss: 1.5380914211273193
Epoch 250, training loss: 14.307333946228027 = 1.4113407135009766 + 2.0 * 6.447996616363525
Epoch 250, val loss: 1.5059092044830322
Epoch 260, training loss: 14.246919631958008 = 1.3668190240859985 + 2.0 * 6.44005012512207
Epoch 260, val loss: 1.4733400344848633
Epoch 270, training loss: 14.180163383483887 = 1.3231827020645142 + 2.0 * 6.428490161895752
Epoch 270, val loss: 1.4415088891983032
Epoch 280, training loss: 14.115674018859863 = 1.2805966138839722 + 2.0 * 6.417538642883301
Epoch 280, val loss: 1.4107919931411743
Epoch 290, training loss: 14.061361312866211 = 1.2392756938934326 + 2.0 * 6.4110426902771
Epoch 290, val loss: 1.381158709526062
Epoch 300, training loss: 14.006145477294922 = 1.2000105381011963 + 2.0 * 6.403067588806152
Epoch 300, val loss: 1.3536145687103271
Epoch 310, training loss: 13.960332870483398 = 1.1632705926895142 + 2.0 * 6.398530960083008
Epoch 310, val loss: 1.3282073736190796
Epoch 320, training loss: 13.909721374511719 = 1.1283116340637207 + 2.0 * 6.390705108642578
Epoch 320, val loss: 1.304473876953125
Epoch 330, training loss: 13.866151809692383 = 1.0950496196746826 + 2.0 * 6.3855509757995605
Epoch 330, val loss: 1.2822701930999756
Epoch 340, training loss: 13.826827049255371 = 1.0633013248443604 + 2.0 * 6.381762981414795
Epoch 340, val loss: 1.261635184288025
Epoch 350, training loss: 13.78955364227295 = 1.0333164930343628 + 2.0 * 6.378118515014648
Epoch 350, val loss: 1.2425377368927002
Epoch 360, training loss: 13.748327255249023 = 1.0047744512557983 + 2.0 * 6.371776580810547
Epoch 360, val loss: 1.2249764204025269
Epoch 370, training loss: 13.713969230651855 = 0.9773229956626892 + 2.0 * 6.36832332611084
Epoch 370, val loss: 1.2084627151489258
Epoch 380, training loss: 13.681316375732422 = 0.9507461190223694 + 2.0 * 6.3652849197387695
Epoch 380, val loss: 1.1927838325500488
Epoch 390, training loss: 13.645254135131836 = 0.9249870181083679 + 2.0 * 6.360133647918701
Epoch 390, val loss: 1.1779571771621704
Epoch 400, training loss: 13.616209030151367 = 0.8996170163154602 + 2.0 * 6.358295917510986
Epoch 400, val loss: 1.1636388301849365
Epoch 410, training loss: 13.579817771911621 = 0.8744022250175476 + 2.0 * 6.352707862854004
Epoch 410, val loss: 1.1496771574020386
Epoch 420, training loss: 13.548391342163086 = 0.8491656184196472 + 2.0 * 6.349612712860107
Epoch 420, val loss: 1.1358463764190674
Epoch 430, training loss: 13.535662651062012 = 0.8238131999969482 + 2.0 * 6.355924606323242
Epoch 430, val loss: 1.1220961809158325
Epoch 440, training loss: 13.490551948547363 = 0.798849880695343 + 2.0 * 6.345850944519043
Epoch 440, val loss: 1.1084179878234863
Epoch 450, training loss: 13.455609321594238 = 0.773689329624176 + 2.0 * 6.3409600257873535
Epoch 450, val loss: 1.094944715499878
Epoch 460, training loss: 13.422845840454102 = 0.7482624053955078 + 2.0 * 6.337291717529297
Epoch 460, val loss: 1.081318974494934
Epoch 470, training loss: 13.39365005493164 = 0.7225227355957031 + 2.0 * 6.335563659667969
Epoch 470, val loss: 1.0676435232162476
Epoch 480, training loss: 13.361915588378906 = 0.6966574788093567 + 2.0 * 6.332629203796387
Epoch 480, val loss: 1.0538901090621948
Epoch 490, training loss: 13.33639144897461 = 0.670748770236969 + 2.0 * 6.332821369171143
Epoch 490, val loss: 1.0403311252593994
Epoch 500, training loss: 13.302263259887695 = 0.6447697877883911 + 2.0 * 6.328746795654297
Epoch 500, val loss: 1.0268514156341553
Epoch 510, training loss: 13.273344039916992 = 0.6186899542808533 + 2.0 * 6.327327251434326
Epoch 510, val loss: 1.0133920907974243
Epoch 520, training loss: 13.240375518798828 = 0.5925266742706299 + 2.0 * 6.323924541473389
Epoch 520, val loss: 1.0001543760299683
Epoch 530, training loss: 13.213275909423828 = 0.5663995146751404 + 2.0 * 6.3234381675720215
Epoch 530, val loss: 0.987131655216217
Epoch 540, training loss: 13.181017875671387 = 0.5403926968574524 + 2.0 * 6.3203125
Epoch 540, val loss: 0.9743139147758484
Epoch 550, training loss: 13.15527057647705 = 0.5147778391838074 + 2.0 * 6.32024621963501
Epoch 550, val loss: 0.9619672894477844
Epoch 560, training loss: 13.129117012023926 = 0.4895336627960205 + 2.0 * 6.319791793823242
Epoch 560, val loss: 0.9501302242279053
Epoch 570, training loss: 13.094671249389648 = 0.46500152349472046 + 2.0 * 6.314835071563721
Epoch 570, val loss: 0.9389567375183105
Epoch 580, training loss: 13.067496299743652 = 0.44109997153282166 + 2.0 * 6.313198089599609
Epoch 580, val loss: 0.9285402297973633
Epoch 590, training loss: 13.044973373413086 = 0.4178675413131714 + 2.0 * 6.3135528564453125
Epoch 590, val loss: 0.9188575148582458
Epoch 600, training loss: 13.033825874328613 = 0.3956155776977539 + 2.0 * 6.31910514831543
Epoch 600, val loss: 0.9100039005279541
Epoch 610, training loss: 12.99505615234375 = 0.37431415915489197 + 2.0 * 6.310370922088623
Epoch 610, val loss: 0.9018953442573547
Epoch 620, training loss: 12.970775604248047 = 0.35408875346183777 + 2.0 * 6.308343410491943
Epoch 620, val loss: 0.8947343230247498
Epoch 630, training loss: 12.947269439697266 = 0.33478033542633057 + 2.0 * 6.306244373321533
Epoch 630, val loss: 0.888353705406189
Epoch 640, training loss: 12.944147109985352 = 0.3163171112537384 + 2.0 * 6.313914775848389
Epoch 640, val loss: 0.8826470971107483
Epoch 650, training loss: 12.916342735290527 = 0.29904308915138245 + 2.0 * 6.308650016784668
Epoch 650, val loss: 0.8775730729103088
Epoch 660, training loss: 12.889449119567871 = 0.28260117769241333 + 2.0 * 6.303423881530762
Epoch 660, val loss: 0.873245358467102
Epoch 670, training loss: 12.869328498840332 = 0.2670144736766815 + 2.0 * 6.301156997680664
Epoch 670, val loss: 0.8696141839027405
Epoch 680, training loss: 12.851465225219727 = 0.2522285282611847 + 2.0 * 6.299618244171143
Epoch 680, val loss: 0.8666321039199829
Epoch 690, training loss: 12.847138404846191 = 0.23819352686405182 + 2.0 * 6.30447244644165
Epoch 690, val loss: 0.8641831874847412
Epoch 700, training loss: 12.827445030212402 = 0.2250319868326187 + 2.0 * 6.301206588745117
Epoch 700, val loss: 0.8619691729545593
Epoch 710, training loss: 12.80624008178711 = 0.2126416116952896 + 2.0 * 6.296799182891846
Epoch 710, val loss: 0.860353410243988
Epoch 720, training loss: 12.791327476501465 = 0.20095384120941162 + 2.0 * 6.295186996459961
Epoch 720, val loss: 0.8591872453689575
Epoch 730, training loss: 12.778319358825684 = 0.18990156054496765 + 2.0 * 6.294209003448486
Epoch 730, val loss: 0.858401894569397
Epoch 740, training loss: 12.77627182006836 = 0.17945370078086853 + 2.0 * 6.2984089851379395
Epoch 740, val loss: 0.8579259514808655
Epoch 750, training loss: 12.780461311340332 = 0.1696111410856247 + 2.0 * 6.30542516708374
Epoch 750, val loss: 0.8576645851135254
Epoch 760, training loss: 12.754546165466309 = 0.16049353778362274 + 2.0 * 6.29702615737915
Epoch 760, val loss: 0.8577336668968201
Epoch 770, training loss: 12.735519409179688 = 0.15188007056713104 + 2.0 * 6.2918195724487305
Epoch 770, val loss: 0.8580635786056519
Epoch 780, training loss: 12.723224639892578 = 0.14380641281604767 + 2.0 * 6.289709091186523
Epoch 780, val loss: 0.8587118983268738
Epoch 790, training loss: 12.713273048400879 = 0.1361861377954483 + 2.0 * 6.288543224334717
Epoch 790, val loss: 0.859577476978302
Epoch 800, training loss: 12.704093933105469 = 0.12900696694850922 + 2.0 * 6.287543296813965
Epoch 800, val loss: 0.8606765270233154
Epoch 810, training loss: 12.722124099731445 = 0.1222664937376976 + 2.0 * 6.299928665161133
Epoch 810, val loss: 0.8619962930679321
Epoch 820, training loss: 12.688437461853027 = 0.11592643707990646 + 2.0 * 6.286255359649658
Epoch 820, val loss: 0.8632287383079529
Epoch 830, training loss: 12.680763244628906 = 0.11002160608768463 + 2.0 * 6.285370826721191
Epoch 830, val loss: 0.8647554516792297
Epoch 840, training loss: 12.673606872558594 = 0.10447517037391663 + 2.0 * 6.2845659255981445
Epoch 840, val loss: 0.8664886951446533
Epoch 850, training loss: 12.680887222290039 = 0.09924609959125519 + 2.0 * 6.290820598602295
Epoch 850, val loss: 0.8683410286903381
Epoch 860, training loss: 12.664355278015137 = 0.09437376260757446 + 2.0 * 6.2849907875061035
Epoch 860, val loss: 0.8703850507736206
Epoch 870, training loss: 12.656145095825195 = 0.08976983278989792 + 2.0 * 6.2831878662109375
Epoch 870, val loss: 0.8724722862243652
Epoch 880, training loss: 12.658243179321289 = 0.08545949310064316 + 2.0 * 6.286391735076904
Epoch 880, val loss: 0.8747694492340088
Epoch 890, training loss: 12.644436836242676 = 0.08140919357538223 + 2.0 * 6.281513690948486
Epoch 890, val loss: 0.8770249485969543
Epoch 900, training loss: 12.638415336608887 = 0.07760421186685562 + 2.0 * 6.280405521392822
Epoch 900, val loss: 0.8794518709182739
Epoch 910, training loss: 12.643179893493652 = 0.07400936633348465 + 2.0 * 6.284585475921631
Epoch 910, val loss: 0.8819237351417542
Epoch 920, training loss: 12.629205703735352 = 0.07065825909376144 + 2.0 * 6.279273509979248
Epoch 920, val loss: 0.8845046758651733
Epoch 930, training loss: 12.621133804321289 = 0.0674852579832077 + 2.0 * 6.276824474334717
Epoch 930, val loss: 0.8871713280677795
Epoch 940, training loss: 12.620829582214355 = 0.0645006000995636 + 2.0 * 6.278164386749268
Epoch 940, val loss: 0.88994961977005
Epoch 950, training loss: 12.622145652770996 = 0.06169869378209114 + 2.0 * 6.280223369598389
Epoch 950, val loss: 0.8927415013313293
Epoch 960, training loss: 12.612086296081543 = 0.05904438719153404 + 2.0 * 6.276520729064941
Epoch 960, val loss: 0.8954359292984009
Epoch 970, training loss: 12.605182647705078 = 0.056562479585409164 + 2.0 * 6.274310111999512
Epoch 970, val loss: 0.898417592048645
Epoch 980, training loss: 12.601883888244629 = 0.054219916462898254 + 2.0 * 6.273831844329834
Epoch 980, val loss: 0.9014096260070801
Epoch 990, training loss: 12.606467247009277 = 0.05200440064072609 + 2.0 * 6.277231216430664
Epoch 990, val loss: 0.9043975472450256
Epoch 1000, training loss: 12.603352546691895 = 0.04990490525960922 + 2.0 * 6.276723861694336
Epoch 1000, val loss: 0.9073212146759033
Epoch 1010, training loss: 12.590712547302246 = 0.04794551804661751 + 2.0 * 6.271383285522461
Epoch 1010, val loss: 0.9103062748908997
Epoch 1020, training loss: 12.587895393371582 = 0.04609213024377823 + 2.0 * 6.270901679992676
Epoch 1020, val loss: 0.9133405685424805
Epoch 1030, training loss: 12.585064888000488 = 0.044332049787044525 + 2.0 * 6.270366191864014
Epoch 1030, val loss: 0.916419267654419
Epoch 1040, training loss: 12.587597846984863 = 0.04267256706953049 + 2.0 * 6.272462844848633
Epoch 1040, val loss: 0.9195338487625122
Epoch 1050, training loss: 12.586828231811523 = 0.04109584540128708 + 2.0 * 6.272866249084473
Epoch 1050, val loss: 0.922465980052948
Epoch 1060, training loss: 12.578804969787598 = 0.03960346058011055 + 2.0 * 6.269600868225098
Epoch 1060, val loss: 0.9254323840141296
Epoch 1070, training loss: 12.573028564453125 = 0.03820359334349632 + 2.0 * 6.2674126625061035
Epoch 1070, val loss: 0.9284775853157043
Epoch 1080, training loss: 12.571650505065918 = 0.03686537221074104 + 2.0 * 6.267392635345459
Epoch 1080, val loss: 0.9315546751022339
Epoch 1090, training loss: 12.58061408996582 = 0.03560013696551323 + 2.0 * 6.272507190704346
Epoch 1090, val loss: 0.9346323609352112
Epoch 1100, training loss: 12.5698881149292 = 0.034388307482004166 + 2.0 * 6.267749786376953
Epoch 1100, val loss: 0.9375750422477722
Epoch 1110, training loss: 12.566656112670898 = 0.033245448023080826 + 2.0 * 6.266705513000488
Epoch 1110, val loss: 0.9405795335769653
Epoch 1120, training loss: 12.562853813171387 = 0.032160643488168716 + 2.0 * 6.265346527099609
Epoch 1120, val loss: 0.9436126351356506
Epoch 1130, training loss: 12.55956745147705 = 0.031123636290431023 + 2.0 * 6.264222145080566
Epoch 1130, val loss: 0.9466099739074707
Epoch 1140, training loss: 12.572539329528809 = 0.030132737010717392 + 2.0 * 6.271203517913818
Epoch 1140, val loss: 0.9495671987533569
Epoch 1150, training loss: 12.558781623840332 = 0.02919958345592022 + 2.0 * 6.264791011810303
Epoch 1150, val loss: 0.9525359869003296
Epoch 1160, training loss: 12.557868957519531 = 0.028300881385803223 + 2.0 * 6.26478385925293
Epoch 1160, val loss: 0.9554299712181091
Epoch 1170, training loss: 12.554591178894043 = 0.02745244652032852 + 2.0 * 6.263569355010986
Epoch 1170, val loss: 0.9583935737609863
Epoch 1180, training loss: 12.550922393798828 = 0.026636822149157524 + 2.0 * 6.262142658233643
Epoch 1180, val loss: 0.9612856507301331
Epoch 1190, training loss: 12.552596092224121 = 0.025861354544758797 + 2.0 * 6.263367176055908
Epoch 1190, val loss: 0.9642229080200195
Epoch 1200, training loss: 12.5447359085083 = 0.025117307901382446 + 2.0 * 6.259809494018555
Epoch 1200, val loss: 0.9670842885971069
Epoch 1210, training loss: 12.543071746826172 = 0.024406442418694496 + 2.0 * 6.259332656860352
Epoch 1210, val loss: 0.9699414372444153
Epoch 1220, training loss: 12.544632911682129 = 0.023725805804133415 + 2.0 * 6.260453701019287
Epoch 1220, val loss: 0.9728106260299683
Epoch 1230, training loss: 12.552355766296387 = 0.023070601746439934 + 2.0 * 6.264642715454102
Epoch 1230, val loss: 0.9755815267562866
Epoch 1240, training loss: 12.539666175842285 = 0.022453216835856438 + 2.0 * 6.258606433868408
Epoch 1240, val loss: 0.9783117175102234
Epoch 1250, training loss: 12.538236618041992 = 0.02185721881687641 + 2.0 * 6.258189678192139
Epoch 1250, val loss: 0.9811137914657593
Epoch 1260, training loss: 12.54853630065918 = 0.021284308284521103 + 2.0 * 6.2636260986328125
Epoch 1260, val loss: 0.9838332533836365
Epoch 1270, training loss: 12.537930488586426 = 0.02073793113231659 + 2.0 * 6.258596420288086
Epoch 1270, val loss: 0.9864713549613953
Epoch 1280, training loss: 12.534894943237305 = 0.020212022587656975 + 2.0 * 6.257341384887695
Epoch 1280, val loss: 0.9891646504402161
Epoch 1290, training loss: 12.532960891723633 = 0.019707268103957176 + 2.0 * 6.256626605987549
Epoch 1290, val loss: 0.9918704628944397
Epoch 1300, training loss: 12.544967651367188 = 0.019222572445869446 + 2.0 * 6.262872695922852
Epoch 1300, val loss: 0.9945536255836487
Epoch 1310, training loss: 12.532573699951172 = 0.018749121576547623 + 2.0 * 6.2569122314453125
Epoch 1310, val loss: 0.9970221519470215
Epoch 1320, training loss: 12.53145980834961 = 0.01829998567700386 + 2.0 * 6.256579875946045
Epoch 1320, val loss: 0.9996518492698669
Epoch 1330, training loss: 12.525348663330078 = 0.017867160961031914 + 2.0 * 6.2537407875061035
Epoch 1330, val loss: 1.0021787881851196
Epoch 1340, training loss: 12.525091171264648 = 0.017450135201215744 + 2.0 * 6.253820419311523
Epoch 1340, val loss: 1.0047539472579956
Epoch 1350, training loss: 12.530004501342773 = 0.017049439251422882 + 2.0 * 6.256477355957031
Epoch 1350, val loss: 1.0073249340057373
Epoch 1360, training loss: 12.521803855895996 = 0.016659293323755264 + 2.0 * 6.252572059631348
Epoch 1360, val loss: 1.009797215461731
Epoch 1370, training loss: 12.534090995788574 = 0.016285330057144165 + 2.0 * 6.2589030265808105
Epoch 1370, val loss: 1.012267827987671
Epoch 1380, training loss: 12.523528099060059 = 0.01592377945780754 + 2.0 * 6.253802299499512
Epoch 1380, val loss: 1.014672875404358
Epoch 1390, training loss: 12.521309852600098 = 0.015576764941215515 + 2.0 * 6.252866744995117
Epoch 1390, val loss: 1.0170806646347046
Epoch 1400, training loss: 12.537557601928711 = 0.015239539556205273 + 2.0 * 6.2611589431762695
Epoch 1400, val loss: 1.019469141960144
Epoch 1410, training loss: 12.522110939025879 = 0.014922305941581726 + 2.0 * 6.253594398498535
Epoch 1410, val loss: 1.0218510627746582
Epoch 1420, training loss: 12.516023635864258 = 0.014607219025492668 + 2.0 * 6.250708103179932
Epoch 1420, val loss: 1.024229884147644
Epoch 1430, training loss: 12.513069152832031 = 0.014306671917438507 + 2.0 * 6.249381065368652
Epoch 1430, val loss: 1.0266224145889282
Epoch 1440, training loss: 12.512568473815918 = 0.014012585394084454 + 2.0 * 6.2492780685424805
Epoch 1440, val loss: 1.0290048122406006
Epoch 1450, training loss: 12.533639907836914 = 0.013728002086281776 + 2.0 * 6.259955883026123
Epoch 1450, val loss: 1.0313571691513062
Epoch 1460, training loss: 12.520310401916504 = 0.013453877530992031 + 2.0 * 6.2534284591674805
Epoch 1460, val loss: 1.0335079431533813
Epoch 1470, training loss: 12.51183032989502 = 0.013189481571316719 + 2.0 * 6.2493205070495605
Epoch 1470, val loss: 1.0357149839401245
Epoch 1480, training loss: 12.508893966674805 = 0.012933215126395226 + 2.0 * 6.24798059463501
Epoch 1480, val loss: 1.037996530532837
Epoch 1490, training loss: 12.509941101074219 = 0.012684493325650692 + 2.0 * 6.24862813949585
Epoch 1490, val loss: 1.04025399684906
Epoch 1500, training loss: 12.511293411254883 = 0.012443398125469685 + 2.0 * 6.249424934387207
Epoch 1500, val loss: 1.0424532890319824
Epoch 1510, training loss: 12.516582489013672 = 0.012211090885102749 + 2.0 * 6.252185821533203
Epoch 1510, val loss: 1.044662594795227
Epoch 1520, training loss: 12.505377769470215 = 0.011982251890003681 + 2.0 * 6.246697902679443
Epoch 1520, val loss: 1.046644926071167
Epoch 1530, training loss: 12.504420280456543 = 0.011765274219214916 + 2.0 * 6.2463274002075195
Epoch 1530, val loss: 1.04876708984375
Epoch 1540, training loss: 12.503193855285645 = 0.01155161764472723 + 2.0 * 6.245820999145508
Epoch 1540, val loss: 1.0508629083633423
Epoch 1550, training loss: 12.501105308532715 = 0.011346074752509594 + 2.0 * 6.244879722595215
Epoch 1550, val loss: 1.053041934967041
Epoch 1560, training loss: 12.50120735168457 = 0.011143778450787067 + 2.0 * 6.245031833648682
Epoch 1560, val loss: 1.0551319122314453
Epoch 1570, training loss: 12.509056091308594 = 0.010948694311082363 + 2.0 * 6.249053478240967
Epoch 1570, val loss: 1.057220697402954
Epoch 1580, training loss: 12.499688148498535 = 0.010756402276456356 + 2.0 * 6.2444658279418945
Epoch 1580, val loss: 1.0591849088668823
Epoch 1590, training loss: 12.500986099243164 = 0.01057190541177988 + 2.0 * 6.2452073097229
Epoch 1590, val loss: 1.0612651109695435
Epoch 1600, training loss: 12.507189750671387 = 0.010391874238848686 + 2.0 * 6.248398780822754
Epoch 1600, val loss: 1.063238263130188
Epoch 1610, training loss: 12.499905586242676 = 0.01021707896143198 + 2.0 * 6.244844436645508
Epoch 1610, val loss: 1.0652008056640625
Epoch 1620, training loss: 12.498307228088379 = 0.0100480355322361 + 2.0 * 6.244129657745361
Epoch 1620, val loss: 1.0672029256820679
Epoch 1630, training loss: 12.501212120056152 = 0.00988185964524746 + 2.0 * 6.245665073394775
Epoch 1630, val loss: 1.0691368579864502
Epoch 1640, training loss: 12.495658874511719 = 0.009721656329929829 + 2.0 * 6.242968559265137
Epoch 1640, val loss: 1.0711007118225098
Epoch 1650, training loss: 12.495235443115234 = 0.009565425105392933 + 2.0 * 6.24283504486084
Epoch 1650, val loss: 1.0730361938476562
Epoch 1660, training loss: 12.494611740112305 = 0.009412032552063465 + 2.0 * 6.242599964141846
Epoch 1660, val loss: 1.0749423503875732
Epoch 1670, training loss: 12.50011920928955 = 0.009263221174478531 + 2.0 * 6.245428085327148
Epoch 1670, val loss: 1.0768224000930786
Epoch 1680, training loss: 12.493511199951172 = 0.00911786686629057 + 2.0 * 6.242196559906006
Epoch 1680, val loss: 1.0786808729171753
Epoch 1690, training loss: 12.498250961303711 = 0.008976451121270657 + 2.0 * 6.244637489318848
Epoch 1690, val loss: 1.080506443977356
Epoch 1700, training loss: 12.492337226867676 = 0.008839834481477737 + 2.0 * 6.241748809814453
Epoch 1700, val loss: 1.0823161602020264
Epoch 1710, training loss: 12.488856315612793 = 0.008707674220204353 + 2.0 * 6.240074157714844
Epoch 1710, val loss: 1.0841869115829468
Epoch 1720, training loss: 12.48851490020752 = 0.008577199652791023 + 2.0 * 6.239968776702881
Epoch 1720, val loss: 1.086020588874817
Epoch 1730, training loss: 12.490321159362793 = 0.008449709042906761 + 2.0 * 6.240935802459717
Epoch 1730, val loss: 1.087855577468872
Epoch 1740, training loss: 12.489604949951172 = 0.00832497887313366 + 2.0 * 6.240640163421631
Epoch 1740, val loss: 1.0896254777908325
Epoch 1750, training loss: 12.496506690979004 = 0.008202632889151573 + 2.0 * 6.244152069091797
Epoch 1750, val loss: 1.0914075374603271
Epoch 1760, training loss: 12.486454010009766 = 0.008083692751824856 + 2.0 * 6.239185333251953
Epoch 1760, val loss: 1.0930920839309692
Epoch 1770, training loss: 12.488139152526855 = 0.007967581041157246 + 2.0 * 6.240085601806641
Epoch 1770, val loss: 1.094829797744751
Epoch 1780, training loss: 12.490443229675293 = 0.007855329662561417 + 2.0 * 6.241293907165527
Epoch 1780, val loss: 1.0965280532836914
Epoch 1790, training loss: 12.484979629516602 = 0.007744248956441879 + 2.0 * 6.238617897033691
Epoch 1790, val loss: 1.0981874465942383
Epoch 1800, training loss: 12.485852241516113 = 0.007636604364961386 + 2.0 * 6.239107608795166
Epoch 1800, val loss: 1.099906086921692
Epoch 1810, training loss: 12.489984512329102 = 0.0075309183448553085 + 2.0 * 6.241226673126221
Epoch 1810, val loss: 1.1015368700027466
Epoch 1820, training loss: 12.485344886779785 = 0.007429353892803192 + 2.0 * 6.23895788192749
Epoch 1820, val loss: 1.1032497882843018
Epoch 1830, training loss: 12.480199813842773 = 0.007328649517148733 + 2.0 * 6.236435413360596
Epoch 1830, val loss: 1.10487961769104
Epoch 1840, training loss: 12.48205280303955 = 0.007229905109852552 + 2.0 * 6.2374114990234375
Epoch 1840, val loss: 1.1065303087234497
Epoch 1850, training loss: 12.483316421508789 = 0.007133648730814457 + 2.0 * 6.238091468811035
Epoch 1850, val loss: 1.1081629991531372
Epoch 1860, training loss: 12.48236083984375 = 0.0070392838679254055 + 2.0 * 6.237660884857178
Epoch 1860, val loss: 1.1097322702407837
Epoch 1870, training loss: 12.48265266418457 = 0.006947655230760574 + 2.0 * 6.237852573394775
Epoch 1870, val loss: 1.1113371849060059
Epoch 1880, training loss: 12.476593017578125 = 0.006858090870082378 + 2.0 * 6.234867572784424
Epoch 1880, val loss: 1.1129684448242188
Epoch 1890, training loss: 12.47717571258545 = 0.006770088337361813 + 2.0 * 6.235202789306641
Epoch 1890, val loss: 1.114577293395996
Epoch 1900, training loss: 12.48547077178955 = 0.006684600841253996 + 2.0 * 6.23939323425293
Epoch 1900, val loss: 1.1161797046661377
Epoch 1910, training loss: 12.484700202941895 = 0.0066000921651721 + 2.0 * 6.239049911499023
Epoch 1910, val loss: 1.1176395416259766
Epoch 1920, training loss: 12.479596138000488 = 0.006516274530440569 + 2.0 * 6.236539840698242
Epoch 1920, val loss: 1.119130253791809
Epoch 1930, training loss: 12.478893280029297 = 0.006436114199459553 + 2.0 * 6.2362284660339355
Epoch 1930, val loss: 1.1206778287887573
Epoch 1940, training loss: 12.478254318237305 = 0.006356922443956137 + 2.0 * 6.23594856262207
Epoch 1940, val loss: 1.1221518516540527
Epoch 1950, training loss: 12.475547790527344 = 0.006279840599745512 + 2.0 * 6.234633922576904
Epoch 1950, val loss: 1.1236530542373657
Epoch 1960, training loss: 12.472559928894043 = 0.006204977165907621 + 2.0 * 6.233177661895752
Epoch 1960, val loss: 1.12515127658844
Epoch 1970, training loss: 12.471354484558105 = 0.006130739115178585 + 2.0 * 6.232611656188965
Epoch 1970, val loss: 1.1266604661941528
Epoch 1980, training loss: 12.478540420532227 = 0.006058151833713055 + 2.0 * 6.236241340637207
Epoch 1980, val loss: 1.1281561851501465
Epoch 1990, training loss: 12.480584144592285 = 0.0059865727089345455 + 2.0 * 6.237298965454102
Epoch 1990, val loss: 1.1295536756515503
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8
0.8044280442804429
The final CL Acc:0.76914, 0.02188, The final GNN Acc:0.80847, 0.00345
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13188])
remove edge: torch.Size([2, 7924])
updated graph: torch.Size([2, 10556])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 19.14282989501953 = 1.9491565227508545 + 2.0 * 8.596837043762207
Epoch 0, val loss: 1.9506707191467285
Epoch 10, training loss: 19.131860733032227 = 1.938820481300354 + 2.0 * 8.59652042388916
Epoch 10, val loss: 1.940612554550171
Epoch 20, training loss: 19.113540649414062 = 1.926037073135376 + 2.0 * 8.593751907348633
Epoch 20, val loss: 1.927770972251892
Epoch 30, training loss: 19.0477352142334 = 1.9084867238998413 + 2.0 * 8.569623947143555
Epoch 30, val loss: 1.9097870588302612
Epoch 40, training loss: 18.691543579101562 = 1.885877013206482 + 2.0 * 8.402832984924316
Epoch 40, val loss: 1.8874229192733765
Epoch 50, training loss: 17.07709503173828 = 1.8624329566955566 + 2.0 * 7.607330799102783
Epoch 50, val loss: 1.8650398254394531
Epoch 60, training loss: 16.403200149536133 = 1.8443044424057007 + 2.0 * 7.279447555541992
Epoch 60, val loss: 1.8489209413528442
Epoch 70, training loss: 16.030105590820312 = 1.829119324684143 + 2.0 * 7.100493431091309
Epoch 70, val loss: 1.8353238105773926
Epoch 80, training loss: 15.80620002746582 = 1.8147010803222656 + 2.0 * 6.995749473571777
Epoch 80, val loss: 1.8216545581817627
Epoch 90, training loss: 15.557976722717285 = 1.8005781173706055 + 2.0 * 6.87869930267334
Epoch 90, val loss: 1.8084741830825806
Epoch 100, training loss: 15.341066360473633 = 1.7868789434432983 + 2.0 * 6.777093887329102
Epoch 100, val loss: 1.7952603101730347
Epoch 110, training loss: 15.16508960723877 = 1.7740591764450073 + 2.0 * 6.695515155792236
Epoch 110, val loss: 1.7829222679138184
Epoch 120, training loss: 15.038949966430664 = 1.7618783712387085 + 2.0 * 6.638535976409912
Epoch 120, val loss: 1.7712347507476807
Epoch 130, training loss: 14.940723419189453 = 1.7477691173553467 + 2.0 * 6.596477031707764
Epoch 130, val loss: 1.7582182884216309
Epoch 140, training loss: 14.853793144226074 = 1.7312278747558594 + 2.0 * 6.561282634735107
Epoch 140, val loss: 1.7435557842254639
Epoch 150, training loss: 14.781290054321289 = 1.7130513191223145 + 2.0 * 6.534119129180908
Epoch 150, val loss: 1.7274702787399292
Epoch 160, training loss: 14.71811294555664 = 1.6923285722732544 + 2.0 * 6.512892246246338
Epoch 160, val loss: 1.7089931964874268
Epoch 170, training loss: 14.658697128295898 = 1.6684657335281372 + 2.0 * 6.495115756988525
Epoch 170, val loss: 1.6879538297653198
Epoch 180, training loss: 14.599007606506348 = 1.6413347721099854 + 2.0 * 6.478836536407471
Epoch 180, val loss: 1.6640151739120483
Epoch 190, training loss: 14.538344383239746 = 1.6104834079742432 + 2.0 * 6.463930606842041
Epoch 190, val loss: 1.636713981628418
Epoch 200, training loss: 14.48674201965332 = 1.5759797096252441 + 2.0 * 6.455380916595459
Epoch 200, val loss: 1.6063932180404663
Epoch 210, training loss: 14.417620658874512 = 1.5388402938842773 + 2.0 * 6.439390182495117
Epoch 210, val loss: 1.5739238262176514
Epoch 220, training loss: 14.354608535766602 = 1.499006986618042 + 2.0 * 6.42780065536499
Epoch 220, val loss: 1.5389184951782227
Epoch 230, training loss: 14.291901588439941 = 1.4562896490097046 + 2.0 * 6.417806148529053
Epoch 230, val loss: 1.5013668537139893
Epoch 240, training loss: 14.2357816696167 = 1.41124427318573 + 2.0 * 6.41226863861084
Epoch 240, val loss: 1.4619098901748657
Epoch 250, training loss: 14.16776180267334 = 1.3649873733520508 + 2.0 * 6.4013872146606445
Epoch 250, val loss: 1.4218827486038208
Epoch 260, training loss: 14.10729694366455 = 1.3183014392852783 + 2.0 * 6.394497871398926
Epoch 260, val loss: 1.3815093040466309
Epoch 270, training loss: 14.046451568603516 = 1.2715872526168823 + 2.0 * 6.387432098388672
Epoch 270, val loss: 1.3414562940597534
Epoch 280, training loss: 13.98777961730957 = 1.2250453233718872 + 2.0 * 6.381367206573486
Epoch 280, val loss: 1.3018537759780884
Epoch 290, training loss: 13.950033187866211 = 1.180352807044983 + 2.0 * 6.38484001159668
Epoch 290, val loss: 1.26497483253479
Epoch 300, training loss: 13.887533187866211 = 1.1390480995178223 + 2.0 * 6.374242782592773
Epoch 300, val loss: 1.2307040691375732
Epoch 310, training loss: 13.836572647094727 = 1.0993093252182007 + 2.0 * 6.368631839752197
Epoch 310, val loss: 1.1984992027282715
Epoch 320, training loss: 13.789142608642578 = 1.0608320236206055 + 2.0 * 6.364155292510986
Epoch 320, val loss: 1.1677770614624023
Epoch 330, training loss: 13.742935180664062 = 1.0233185291290283 + 2.0 * 6.359808444976807
Epoch 330, val loss: 1.1383765935897827
Epoch 340, training loss: 13.698625564575195 = 0.9864583015441895 + 2.0 * 6.356083869934082
Epoch 340, val loss: 1.1100267171859741
Epoch 350, training loss: 13.656943321228027 = 0.9502643346786499 + 2.0 * 6.353339672088623
Epoch 350, val loss: 1.0825051069259644
Epoch 360, training loss: 13.616747856140137 = 0.9149729609489441 + 2.0 * 6.350887298583984
Epoch 360, val loss: 1.0557971000671387
Epoch 370, training loss: 13.573433876037598 = 0.8805709481239319 + 2.0 * 6.346431255340576
Epoch 370, val loss: 1.0302790403366089
Epoch 380, training loss: 13.533371925354004 = 0.8468621373176575 + 2.0 * 6.343255043029785
Epoch 380, val loss: 1.0055487155914307
Epoch 390, training loss: 13.495528221130371 = 0.8137454390525818 + 2.0 * 6.340891361236572
Epoch 390, val loss: 0.981378972530365
Epoch 400, training loss: 13.461111068725586 = 0.781479001045227 + 2.0 * 6.339816093444824
Epoch 400, val loss: 0.9582917094230652
Epoch 410, training loss: 13.421818733215332 = 0.750335156917572 + 2.0 * 6.335741996765137
Epoch 410, val loss: 0.9361794590950012
Epoch 420, training loss: 13.400501251220703 = 0.7204115390777588 + 2.0 * 6.340044975280762
Epoch 420, val loss: 0.9155851602554321
Epoch 430, training loss: 13.353336334228516 = 0.692179262638092 + 2.0 * 6.330578327178955
Epoch 430, val loss: 0.8962886333465576
Epoch 440, training loss: 13.318376541137695 = 0.6651541590690613 + 2.0 * 6.326611042022705
Epoch 440, val loss: 0.878292441368103
Epoch 450, training loss: 13.286545753479004 = 0.6392220258712769 + 2.0 * 6.323661804199219
Epoch 450, val loss: 0.8616158962249756
Epoch 460, training loss: 13.259452819824219 = 0.6143798232078552 + 2.0 * 6.322536468505859
Epoch 460, val loss: 0.8462578058242798
Epoch 470, training loss: 13.2396821975708 = 0.5907316207885742 + 2.0 * 6.324475288391113
Epoch 470, val loss: 0.8324743509292603
Epoch 480, training loss: 13.205757141113281 = 0.5683336853981018 + 2.0 * 6.318711757659912
Epoch 480, val loss: 0.8198480606079102
Epoch 490, training loss: 13.177713394165039 = 0.5468659400939941 + 2.0 * 6.315423965454102
Epoch 490, val loss: 0.8083788156509399
Epoch 500, training loss: 13.15811538696289 = 0.526229202747345 + 2.0 * 6.315943241119385
Epoch 500, val loss: 0.7979928851127625
Epoch 510, training loss: 13.139135360717773 = 0.5064879059791565 + 2.0 * 6.316323757171631
Epoch 510, val loss: 0.78884357213974
Epoch 520, training loss: 13.107869148254395 = 0.48765385150909424 + 2.0 * 6.310107707977295
Epoch 520, val loss: 0.7805155515670776
Epoch 530, training loss: 13.086027145385742 = 0.4694051444530487 + 2.0 * 6.3083109855651855
Epoch 530, val loss: 0.7730097770690918
Epoch 540, training loss: 13.06282901763916 = 0.4516443908214569 + 2.0 * 6.3055925369262695
Epoch 540, val loss: 0.7662375569343567
Epoch 550, training loss: 13.044302940368652 = 0.43429329991340637 + 2.0 * 6.305004596710205
Epoch 550, val loss: 0.7600633502006531
Epoch 560, training loss: 13.036369323730469 = 0.41742026805877686 + 2.0 * 6.309474468231201
Epoch 560, val loss: 0.7545791268348694
Epoch 570, training loss: 13.007326126098633 = 0.4011337161064148 + 2.0 * 6.303096294403076
Epoch 570, val loss: 0.7497051358222961
Epoch 580, training loss: 12.986564636230469 = 0.3853115141391754 + 2.0 * 6.300626754760742
Epoch 580, val loss: 0.7452306151390076
Epoch 590, training loss: 12.966803550720215 = 0.36980029940605164 + 2.0 * 6.298501491546631
Epoch 590, val loss: 0.7413616180419922
Epoch 600, training loss: 12.954662322998047 = 0.3546241521835327 + 2.0 * 6.300019264221191
Epoch 600, val loss: 0.7380412220954895
Epoch 610, training loss: 12.936256408691406 = 0.33997005224227905 + 2.0 * 6.29814338684082
Epoch 610, val loss: 0.7351580858230591
Epoch 620, training loss: 12.91834831237793 = 0.32575395703315735 + 2.0 * 6.296297073364258
Epoch 620, val loss: 0.7329295873641968
Epoch 630, training loss: 12.898097038269043 = 0.3120615482330322 + 2.0 * 6.293017864227295
Epoch 630, val loss: 0.7310649752616882
Epoch 640, training loss: 12.882280349731445 = 0.29878994822502136 + 2.0 * 6.291745185852051
Epoch 640, val loss: 0.7297009229660034
Epoch 650, training loss: 12.868086814880371 = 0.2859116494655609 + 2.0 * 6.291087627410889
Epoch 650, val loss: 0.7288619875907898
Epoch 660, training loss: 12.856410026550293 = 0.2735307514667511 + 2.0 * 6.291439533233643
Epoch 660, val loss: 0.7284640669822693
Epoch 670, training loss: 12.843277931213379 = 0.26167383790016174 + 2.0 * 6.290802001953125
Epoch 670, val loss: 0.7285071015357971
Epoch 680, training loss: 12.827982902526855 = 0.2503555715084076 + 2.0 * 6.288813591003418
Epoch 680, val loss: 0.7288636565208435
Epoch 690, training loss: 12.819253921508789 = 0.23947013914585114 + 2.0 * 6.289891719818115
Epoch 690, val loss: 0.7296026945114136
Epoch 700, training loss: 12.802373886108398 = 0.22910486161708832 + 2.0 * 6.28663444519043
Epoch 700, val loss: 0.7305936813354492
Epoch 710, training loss: 12.787032127380371 = 0.21921318769454956 + 2.0 * 6.283909320831299
Epoch 710, val loss: 0.7318797707557678
Epoch 720, training loss: 12.782098770141602 = 0.20973530411720276 + 2.0 * 6.286181926727295
Epoch 720, val loss: 0.7334544658660889
Epoch 730, training loss: 12.767436027526855 = 0.20067983865737915 + 2.0 * 6.2833781242370605
Epoch 730, val loss: 0.7353909611701965
Epoch 740, training loss: 12.755388259887695 = 0.19206289947032928 + 2.0 * 6.281662464141846
Epoch 740, val loss: 0.7374621629714966
Epoch 750, training loss: 12.749771118164062 = 0.18381744623184204 + 2.0 * 6.2829766273498535
Epoch 750, val loss: 0.7398189902305603
Epoch 760, training loss: 12.738641738891602 = 0.17598654329776764 + 2.0 * 6.281327724456787
Epoch 760, val loss: 0.7423193454742432
Epoch 770, training loss: 12.730560302734375 = 0.16856002807617188 + 2.0 * 6.281000137329102
Epoch 770, val loss: 0.7449421286582947
Epoch 780, training loss: 12.717618942260742 = 0.16147945821285248 + 2.0 * 6.278069972991943
Epoch 780, val loss: 0.747771143913269
Epoch 790, training loss: 12.712143898010254 = 0.15473143756389618 + 2.0 * 6.278706073760986
Epoch 790, val loss: 0.7507518529891968
Epoch 800, training loss: 12.702506065368652 = 0.14830946922302246 + 2.0 * 6.277098178863525
Epoch 800, val loss: 0.7538175582885742
Epoch 810, training loss: 12.692545890808105 = 0.14220808446407318 + 2.0 * 6.2751688957214355
Epoch 810, val loss: 0.7569459080696106
Epoch 820, training loss: 12.685462951660156 = 0.1363774687051773 + 2.0 * 6.274542808532715
Epoch 820, val loss: 0.7602136731147766
Epoch 830, training loss: 12.688461303710938 = 0.13081030547618866 + 2.0 * 6.278825283050537
Epoch 830, val loss: 0.763546884059906
Epoch 840, training loss: 12.675440788269043 = 0.1255263239145279 + 2.0 * 6.274957180023193
Epoch 840, val loss: 0.7669536471366882
Epoch 850, training loss: 12.665021896362305 = 0.12052392959594727 + 2.0 * 6.2722487449646
Epoch 850, val loss: 0.7703167796134949
Epoch 860, training loss: 12.6568603515625 = 0.11572917550802231 + 2.0 * 6.270565509796143
Epoch 860, val loss: 0.7738503813743591
Epoch 870, training loss: 12.652436256408691 = 0.11115586012601852 + 2.0 * 6.2706403732299805
Epoch 870, val loss: 0.7774486541748047
Epoch 880, training loss: 12.659602165222168 = 0.106793113052845 + 2.0 * 6.27640438079834
Epoch 880, val loss: 0.7810969352722168
Epoch 890, training loss: 12.640785217285156 = 0.10262733697891235 + 2.0 * 6.269078731536865
Epoch 890, val loss: 0.7847199440002441
Epoch 900, training loss: 12.635534286499023 = 0.09866153448820114 + 2.0 * 6.268436431884766
Epoch 900, val loss: 0.7883992791175842
Epoch 910, training loss: 12.634964942932129 = 0.09488385170698166 + 2.0 * 6.270040512084961
Epoch 910, val loss: 0.7921357750892639
Epoch 920, training loss: 12.62589168548584 = 0.09125766158103943 + 2.0 * 6.267316818237305
Epoch 920, val loss: 0.7958710193634033
Epoch 930, training loss: 12.620052337646484 = 0.08780732750892639 + 2.0 * 6.266122341156006
Epoch 930, val loss: 0.7996103167533875
Epoch 940, training loss: 12.617732048034668 = 0.0844973549246788 + 2.0 * 6.266617298126221
Epoch 940, val loss: 0.8034477233886719
Epoch 950, training loss: 12.616286277770996 = 0.08134109526872635 + 2.0 * 6.267472743988037
Epoch 950, val loss: 0.8072872161865234
Epoch 960, training loss: 12.609715461730957 = 0.07832717895507812 + 2.0 * 6.2656941413879395
Epoch 960, val loss: 0.8111317753791809
Epoch 970, training loss: 12.607542991638184 = 0.07544060051441193 + 2.0 * 6.266051292419434
Epoch 970, val loss: 0.8150304555892944
Epoch 980, training loss: 12.598137855529785 = 0.07267644256353378 + 2.0 * 6.262730598449707
Epoch 980, val loss: 0.818953812122345
Epoch 990, training loss: 12.600546836853027 = 0.07003404200077057 + 2.0 * 6.265256404876709
Epoch 990, val loss: 0.8229212760925293
Epoch 1000, training loss: 12.591778755187988 = 0.06750816106796265 + 2.0 * 6.2621355056762695
Epoch 1000, val loss: 0.8268739581108093
Epoch 1010, training loss: 12.604764938354492 = 0.06510006636381149 + 2.0 * 6.269832611083984
Epoch 1010, val loss: 0.8308050036430359
Epoch 1020, training loss: 12.586453437805176 = 0.06281092017889023 + 2.0 * 6.261821269989014
Epoch 1020, val loss: 0.8347628116607666
Epoch 1030, training loss: 12.580465316772461 = 0.060621533542871475 + 2.0 * 6.259922027587891
Epoch 1030, val loss: 0.8386759161949158
Epoch 1040, training loss: 12.577203750610352 = 0.058528125286102295 + 2.0 * 6.259337902069092
Epoch 1040, val loss: 0.8426808714866638
Epoch 1050, training loss: 12.585713386535645 = 0.05652603507041931 + 2.0 * 6.264593601226807
Epoch 1050, val loss: 0.8466976881027222
Epoch 1060, training loss: 12.576011657714844 = 0.05459160730242729 + 2.0 * 6.2607102394104
Epoch 1060, val loss: 0.8507241606712341
Epoch 1070, training loss: 12.568082809448242 = 0.052763912826776505 + 2.0 * 6.257659435272217
Epoch 1070, val loss: 0.8546993136405945
Epoch 1080, training loss: 12.567270278930664 = 0.05099925398826599 + 2.0 * 6.2581353187561035
Epoch 1080, val loss: 0.8587937951087952
Epoch 1090, training loss: 12.571497917175293 = 0.04931747168302536 + 2.0 * 6.261090278625488
Epoch 1090, val loss: 0.8627839684486389
Epoch 1100, training loss: 12.563983917236328 = 0.04771152883768082 + 2.0 * 6.25813627243042
Epoch 1100, val loss: 0.8667773604393005
Epoch 1110, training loss: 12.556260108947754 = 0.046183180063962936 + 2.0 * 6.255038261413574
Epoch 1110, val loss: 0.8707386255264282
Epoch 1120, training loss: 12.554144859313965 = 0.04471338540315628 + 2.0 * 6.254715919494629
Epoch 1120, val loss: 0.8748024702072144
Epoch 1130, training loss: 12.557150840759277 = 0.043304380029439926 + 2.0 * 6.256923198699951
Epoch 1130, val loss: 0.8788435459136963
Epoch 1140, training loss: 12.550387382507324 = 0.04195191338658333 + 2.0 * 6.254217624664307
Epoch 1140, val loss: 0.8828323483467102
Epoch 1150, training loss: 12.561624526977539 = 0.04065248742699623 + 2.0 * 6.260486125946045
Epoch 1150, val loss: 0.8868101239204407
Epoch 1160, training loss: 12.547903060913086 = 0.039426907896995544 + 2.0 * 6.254238128662109
Epoch 1160, val loss: 0.8907026052474976
Epoch 1170, training loss: 12.543052673339844 = 0.03824286535382271 + 2.0 * 6.252404689788818
Epoch 1170, val loss: 0.8946410417556763
Epoch 1180, training loss: 12.541180610656738 = 0.037107110023498535 + 2.0 * 6.2520365715026855
Epoch 1180, val loss: 0.8986150622367859
Epoch 1190, training loss: 12.551867485046387 = 0.03601443022489548 + 2.0 * 6.2579264640808105
Epoch 1190, val loss: 0.9025936126708984
Epoch 1200, training loss: 12.538176536560059 = 0.03496694937348366 + 2.0 * 6.2516045570373535
Epoch 1200, val loss: 0.9064388275146484
Epoch 1210, training loss: 12.534660339355469 = 0.033959418535232544 + 2.0 * 6.250350475311279
Epoch 1210, val loss: 0.9103828072547913
Epoch 1220, training loss: 12.546799659729004 = 0.03299124538898468 + 2.0 * 6.256904125213623
Epoch 1220, val loss: 0.9143310189247131
Epoch 1230, training loss: 12.543133735656738 = 0.032061781734228134 + 2.0 * 6.255536079406738
Epoch 1230, val loss: 0.9181603789329529
Epoch 1240, training loss: 12.530986785888672 = 0.031174544245004654 + 2.0 * 6.249906063079834
Epoch 1240, val loss: 0.9219790101051331
Epoch 1250, training loss: 12.528432846069336 = 0.030320575460791588 + 2.0 * 6.249056339263916
Epoch 1250, val loss: 0.9258193373680115
Epoch 1260, training loss: 12.531950950622559 = 0.029497478157281876 + 2.0 * 6.251226902008057
Epoch 1260, val loss: 0.9296954870223999
Epoch 1270, training loss: 12.527043342590332 = 0.028699815273284912 + 2.0 * 6.249171733856201
Epoch 1270, val loss: 0.9335376024246216
Epoch 1280, training loss: 12.528602600097656 = 0.027933182194828987 + 2.0 * 6.250334739685059
Epoch 1280, val loss: 0.9373125433921814
Epoch 1290, training loss: 12.523895263671875 = 0.027202030643820763 + 2.0 * 6.24834680557251
Epoch 1290, val loss: 0.9410326480865479
Epoch 1300, training loss: 12.518095970153809 = 0.026494093239307404 + 2.0 * 6.245800971984863
Epoch 1300, val loss: 0.9447639584541321
Epoch 1310, training loss: 12.524477005004883 = 0.025813303887844086 + 2.0 * 6.249331951141357
Epoch 1310, val loss: 0.9484869837760925
Epoch 1320, training loss: 12.530218124389648 = 0.025153670459985733 + 2.0 * 6.252532005310059
Epoch 1320, val loss: 0.9522040486335754
Epoch 1330, training loss: 12.51501750946045 = 0.024525657296180725 + 2.0 * 6.245245933532715
Epoch 1330, val loss: 0.9557004570960999
Epoch 1340, training loss: 12.515080451965332 = 0.02392169088125229 + 2.0 * 6.245579242706299
Epoch 1340, val loss: 0.9592678546905518
Epoch 1350, training loss: 12.513891220092773 = 0.023336300626397133 + 2.0 * 6.245277404785156
Epoch 1350, val loss: 0.9629405736923218
Epoch 1360, training loss: 12.522541999816895 = 0.022769849747419357 + 2.0 * 6.2498860359191895
Epoch 1360, val loss: 0.9665615558624268
Epoch 1370, training loss: 12.515074729919434 = 0.022227201610803604 + 2.0 * 6.246423721313477
Epoch 1370, val loss: 0.9700637459754944
Epoch 1380, training loss: 12.513376235961914 = 0.021702755242586136 + 2.0 * 6.2458367347717285
Epoch 1380, val loss: 0.9735525846481323
Epoch 1390, training loss: 12.509021759033203 = 0.021196629852056503 + 2.0 * 6.243912696838379
Epoch 1390, val loss: 0.9770466089248657
Epoch 1400, training loss: 12.506305694580078 = 0.02070820890367031 + 2.0 * 6.242798805236816
Epoch 1400, val loss: 0.9805198311805725
Epoch 1410, training loss: 12.51689338684082 = 0.020239034667611122 + 2.0 * 6.248327255249023
Epoch 1410, val loss: 0.983932375907898
Epoch 1420, training loss: 12.512750625610352 = 0.019781265407800674 + 2.0 * 6.246484756469727
Epoch 1420, val loss: 0.9873494505882263
Epoch 1430, training loss: 12.50342082977295 = 0.019340449944138527 + 2.0 * 6.242040157318115
Epoch 1430, val loss: 0.9906520247459412
Epoch 1440, training loss: 12.500648498535156 = 0.018918273970484734 + 2.0 * 6.240865230560303
Epoch 1440, val loss: 0.9940157532691956
Epoch 1450, training loss: 12.500221252441406 = 0.018506094813346863 + 2.0 * 6.2408576011657715
Epoch 1450, val loss: 0.9974032044410706
Epoch 1460, training loss: 12.519430160522461 = 0.018106523901224136 + 2.0 * 6.250661849975586
Epoch 1460, val loss: 1.0007681846618652
Epoch 1470, training loss: 12.505552291870117 = 0.017721232026815414 + 2.0 * 6.243915557861328
Epoch 1470, val loss: 1.003976821899414
Epoch 1480, training loss: 12.51501750946045 = 0.01734953559935093 + 2.0 * 6.248834133148193
Epoch 1480, val loss: 1.007165789604187
Epoch 1490, training loss: 12.497945785522461 = 0.016991302371025085 + 2.0 * 6.240477085113525
Epoch 1490, val loss: 1.0102735757827759
Epoch 1500, training loss: 12.49587345123291 = 0.016645178198814392 + 2.0 * 6.239614009857178
Epoch 1500, val loss: 1.013417363166809
Epoch 1510, training loss: 12.493770599365234 = 0.016307570040225983 + 2.0 * 6.238731384277344
Epoch 1510, val loss: 1.01664137840271
Epoch 1520, training loss: 12.496020317077637 = 0.015978947281837463 + 2.0 * 6.240020751953125
Epoch 1520, val loss: 1.0198705196380615
Epoch 1530, training loss: 12.498661994934082 = 0.015660075470805168 + 2.0 * 6.2415008544921875
Epoch 1530, val loss: 1.0229792594909668
Epoch 1540, training loss: 12.493988990783691 = 0.015352638438344002 + 2.0 * 6.239318370819092
Epoch 1540, val loss: 1.0260231494903564
Epoch 1550, training loss: 12.50771713256836 = 0.015053668059408665 + 2.0 * 6.246331691741943
Epoch 1550, val loss: 1.029093623161316
Epoch 1560, training loss: 12.4934720993042 = 0.014766927808523178 + 2.0 * 6.239352703094482
Epoch 1560, val loss: 1.0319297313690186
Epoch 1570, training loss: 12.489363670349121 = 0.01448585744947195 + 2.0 * 6.237438678741455
Epoch 1570, val loss: 1.0349582433700562
Epoch 1580, training loss: 12.489082336425781 = 0.01421340275555849 + 2.0 * 6.237434387207031
Epoch 1580, val loss: 1.0379558801651
Epoch 1590, training loss: 12.512031555175781 = 0.013948461972177029 + 2.0 * 6.249041557312012
Epoch 1590, val loss: 1.0409324169158936
Epoch 1600, training loss: 12.492864608764648 = 0.01368962973356247 + 2.0 * 6.239587306976318
Epoch 1600, val loss: 1.0437849760055542
Epoch 1610, training loss: 12.486075401306152 = 0.013441629707813263 + 2.0 * 6.236316680908203
Epoch 1610, val loss: 1.046601414680481
Epoch 1620, training loss: 12.48546028137207 = 0.013199699111282825 + 2.0 * 6.236130237579346
Epoch 1620, val loss: 1.0495001077651978
Epoch 1630, training loss: 12.500571250915527 = 0.012963823974132538 + 2.0 * 6.24380350112915
Epoch 1630, val loss: 1.0523823499679565
Epoch 1640, training loss: 12.487529754638672 = 0.012734596617519855 + 2.0 * 6.23739767074585
Epoch 1640, val loss: 1.0551408529281616
Epoch 1650, training loss: 12.483556747436523 = 0.012511888518929482 + 2.0 * 6.235522270202637
Epoch 1650, val loss: 1.0579321384429932
Epoch 1660, training loss: 12.487092018127441 = 0.012295565567910671 + 2.0 * 6.237398147583008
Epoch 1660, val loss: 1.0607177019119263
Epoch 1670, training loss: 12.482089042663574 = 0.012083221226930618 + 2.0 * 6.2350029945373535
Epoch 1670, val loss: 1.0634839534759521
Epoch 1680, training loss: 12.481919288635254 = 0.011877007782459259 + 2.0 * 6.235021114349365
Epoch 1680, val loss: 1.0662040710449219
Epoch 1690, training loss: 12.48705005645752 = 0.01167715061455965 + 2.0 * 6.237686634063721
Epoch 1690, val loss: 1.0689308643341064
Epoch 1700, training loss: 12.482741355895996 = 0.011480949819087982 + 2.0 * 6.235630035400391
Epoch 1700, val loss: 1.071624994277954
Epoch 1710, training loss: 12.4784517288208 = 0.011291079223155975 + 2.0 * 6.233580112457275
Epoch 1710, val loss: 1.0742822885513306
Epoch 1720, training loss: 12.488531112670898 = 0.01110714953392744 + 2.0 * 6.238711833953857
Epoch 1720, val loss: 1.0769023895263672
Epoch 1730, training loss: 12.47661304473877 = 0.010924342088401318 + 2.0 * 6.232844352722168
Epoch 1730, val loss: 1.0795623064041138
Epoch 1740, training loss: 12.475247383117676 = 0.010749634355306625 + 2.0 * 6.232248783111572
Epoch 1740, val loss: 1.08208167552948
Epoch 1750, training loss: 12.475240707397461 = 0.010578940622508526 + 2.0 * 6.232330799102783
Epoch 1750, val loss: 1.0846774578094482
Epoch 1760, training loss: 12.4946928024292 = 0.010410772636532784 + 2.0 * 6.242141246795654
Epoch 1760, val loss: 1.0872708559036255
Epoch 1770, training loss: 12.478859901428223 = 0.010250050574541092 + 2.0 * 6.234304904937744
Epoch 1770, val loss: 1.0897451639175415
Epoch 1780, training loss: 12.474349021911621 = 0.010090699419379234 + 2.0 * 6.232129096984863
Epoch 1780, val loss: 1.0922245979309082
Epoch 1790, training loss: 12.475259780883789 = 0.009937007911503315 + 2.0 * 6.232661247253418
Epoch 1790, val loss: 1.094761610031128
Epoch 1800, training loss: 12.480356216430664 = 0.009785151109099388 + 2.0 * 6.235285758972168
Epoch 1800, val loss: 1.0972590446472168
Epoch 1810, training loss: 12.475651741027832 = 0.00963838305324316 + 2.0 * 6.233006477355957
Epoch 1810, val loss: 1.0996665954589844
Epoch 1820, training loss: 12.4766263961792 = 0.009494435973465443 + 2.0 * 6.233565807342529
Epoch 1820, val loss: 1.1021100282669067
Epoch 1830, training loss: 12.478110313415527 = 0.00935461837798357 + 2.0 * 6.234377861022949
Epoch 1830, val loss: 1.1044684648513794
Epoch 1840, training loss: 12.480118751525879 = 0.009218296967446804 + 2.0 * 6.235450267791748
Epoch 1840, val loss: 1.106877088546753
Epoch 1850, training loss: 12.471148490905762 = 0.009083090350031853 + 2.0 * 6.231032848358154
Epoch 1850, val loss: 1.1092239618301392
Epoch 1860, training loss: 12.467918395996094 = 0.00895358994603157 + 2.0 * 6.229482173919678
Epoch 1860, val loss: 1.1115977764129639
Epoch 1870, training loss: 12.467000961303711 = 0.008826008066534996 + 2.0 * 6.2290873527526855
Epoch 1870, val loss: 1.1139668226242065
Epoch 1880, training loss: 12.467655181884766 = 0.008700691163539886 + 2.0 * 6.229477405548096
Epoch 1880, val loss: 1.1163603067398071
Epoch 1890, training loss: 12.491842269897461 = 0.00857795774936676 + 2.0 * 6.241631984710693
Epoch 1890, val loss: 1.118735909461975
Epoch 1900, training loss: 12.46854305267334 = 0.008457503281533718 + 2.0 * 6.230042934417725
Epoch 1900, val loss: 1.1208242177963257
Epoch 1910, training loss: 12.468013763427734 = 0.008341540582478046 + 2.0 * 6.2298359870910645
Epoch 1910, val loss: 1.1229524612426758
Epoch 1920, training loss: 12.464625358581543 = 0.008229662664234638 + 2.0 * 6.228198051452637
Epoch 1920, val loss: 1.12515127658844
Epoch 1930, training loss: 12.463515281677246 = 0.008118593133985996 + 2.0 * 6.22769832611084
Epoch 1930, val loss: 1.1274254322052002
Epoch 1940, training loss: 12.46706771850586 = 0.008008691482245922 + 2.0 * 6.22952938079834
Epoch 1940, val loss: 1.1297184228897095
Epoch 1950, training loss: 12.468782424926758 = 0.00790075771510601 + 2.0 * 6.230440616607666
Epoch 1950, val loss: 1.1319429874420166
Epoch 1960, training loss: 12.475682258605957 = 0.007796306163072586 + 2.0 * 6.233942985534668
Epoch 1960, val loss: 1.134002923965454
Epoch 1970, training loss: 12.466253280639648 = 0.0076966434717178345 + 2.0 * 6.229278087615967
Epoch 1970, val loss: 1.1359487771987915
Epoch 1980, training loss: 12.460897445678711 = 0.007597044575959444 + 2.0 * 6.226650238037109
Epoch 1980, val loss: 1.1380716562271118
Epoch 1990, training loss: 12.46108627319336 = 0.007499745115637779 + 2.0 * 6.22679328918457
Epoch 1990, val loss: 1.1402795314788818
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.788888888888889
0.8328940432261466
=== training gcn model ===
Epoch 0, training loss: 19.150928497314453 = 1.957237958908081 + 2.0 * 8.596845626831055
Epoch 0, val loss: 1.9582176208496094
Epoch 10, training loss: 19.140439987182617 = 1.9472706317901611 + 2.0 * 8.59658432006836
Epoch 10, val loss: 1.9486273527145386
Epoch 20, training loss: 19.123403549194336 = 1.9348713159561157 + 2.0 * 8.594265937805176
Epoch 20, val loss: 1.9362752437591553
Epoch 30, training loss: 19.068008422851562 = 1.9176445007324219 + 2.0 * 8.57518196105957
Epoch 30, val loss: 1.9187244176864624
Epoch 40, training loss: 18.83460235595703 = 1.8946641683578491 + 2.0 * 8.469968795776367
Epoch 40, val loss: 1.8953124284744263
Epoch 50, training loss: 17.931238174438477 = 1.8677631616592407 + 2.0 * 8.031737327575684
Epoch 50, val loss: 1.868356704711914
Epoch 60, training loss: 17.19683074951172 = 1.8414548635482788 + 2.0 * 7.677688121795654
Epoch 60, val loss: 1.8442153930664062
Epoch 70, training loss: 16.541471481323242 = 1.8241528272628784 + 2.0 * 7.358659267425537
Epoch 70, val loss: 1.8287272453308105
Epoch 80, training loss: 16.096965789794922 = 1.8092378377914429 + 2.0 * 7.143864154815674
Epoch 80, val loss: 1.8142540454864502
Epoch 90, training loss: 15.809748649597168 = 1.7920347452163696 + 2.0 * 7.008856773376465
Epoch 90, val loss: 1.7976957559585571
Epoch 100, training loss: 15.544471740722656 = 1.7757214307785034 + 2.0 * 6.884375095367432
Epoch 100, val loss: 1.7828700542449951
Epoch 110, training loss: 15.303376197814941 = 1.7634375095367432 + 2.0 * 6.769969463348389
Epoch 110, val loss: 1.7720838785171509
Epoch 120, training loss: 15.17953872680664 = 1.751129388809204 + 2.0 * 6.714204788208008
Epoch 120, val loss: 1.7612732648849487
Epoch 130, training loss: 15.070404052734375 = 1.7369087934494019 + 2.0 * 6.666747570037842
Epoch 130, val loss: 1.7488479614257812
Epoch 140, training loss: 14.972084999084473 = 1.7215163707733154 + 2.0 * 6.625284194946289
Epoch 140, val loss: 1.7354170083999634
Epoch 150, training loss: 14.900810241699219 = 1.7047196626663208 + 2.0 * 6.598045349121094
Epoch 150, val loss: 1.7207341194152832
Epoch 160, training loss: 14.809548377990723 = 1.686020016670227 + 2.0 * 6.561764240264893
Epoch 160, val loss: 1.7043484449386597
Epoch 170, training loss: 14.74005126953125 = 1.6647580862045288 + 2.0 * 6.537646770477295
Epoch 170, val loss: 1.6858577728271484
Epoch 180, training loss: 14.67413330078125 = 1.6404919624328613 + 2.0 * 6.516820907592773
Epoch 180, val loss: 1.6648484468460083
Epoch 190, training loss: 14.607693672180176 = 1.6129472255706787 + 2.0 * 6.497373104095459
Epoch 190, val loss: 1.6409815549850464
Epoch 200, training loss: 14.547144889831543 = 1.5821598768234253 + 2.0 * 6.482492446899414
Epoch 200, val loss: 1.6143842935562134
Epoch 210, training loss: 14.479236602783203 = 1.548493504524231 + 2.0 * 6.465371608734131
Epoch 210, val loss: 1.585211992263794
Epoch 220, training loss: 14.416167259216309 = 1.5113483667373657 + 2.0 * 6.452409267425537
Epoch 220, val loss: 1.5530831813812256
Epoch 230, training loss: 14.351717948913574 = 1.4705345630645752 + 2.0 * 6.440591812133789
Epoch 230, val loss: 1.5179499387741089
Epoch 240, training loss: 14.287162780761719 = 1.4261999130249023 + 2.0 * 6.430481433868408
Epoch 240, val loss: 1.479862093925476
Epoch 250, training loss: 14.229516983032227 = 1.3792644739151 + 2.0 * 6.425126075744629
Epoch 250, val loss: 1.4397140741348267
Epoch 260, training loss: 14.158112525939941 = 1.330628752708435 + 2.0 * 6.4137420654296875
Epoch 260, val loss: 1.3986185789108276
Epoch 270, training loss: 14.092029571533203 = 1.2811309099197388 + 2.0 * 6.405449390411377
Epoch 270, val loss: 1.357061743736267
Epoch 280, training loss: 14.028849601745605 = 1.231894850730896 + 2.0 * 6.398477554321289
Epoch 280, val loss: 1.315965175628662
Epoch 290, training loss: 13.966363906860352 = 1.1829743385314941 + 2.0 * 6.39169454574585
Epoch 290, val loss: 1.2755250930786133
Epoch 300, training loss: 13.906243324279785 = 1.1345188617706299 + 2.0 * 6.385862350463867
Epoch 300, val loss: 1.2358911037445068
Epoch 310, training loss: 13.851363182067871 = 1.0869652032852173 + 2.0 * 6.382198810577393
Epoch 310, val loss: 1.1975016593933105
Epoch 320, training loss: 13.796289443969727 = 1.041501522064209 + 2.0 * 6.37739372253418
Epoch 320, val loss: 1.1609714031219482
Epoch 330, training loss: 13.741908073425293 = 0.9982227683067322 + 2.0 * 6.371842861175537
Epoch 330, val loss: 1.1267445087432861
Epoch 340, training loss: 13.687761306762695 = 0.9567508101463318 + 2.0 * 6.365505218505859
Epoch 340, val loss: 1.094467043876648
Epoch 350, training loss: 13.647956848144531 = 0.9171042442321777 + 2.0 * 6.365426063537598
Epoch 350, val loss: 1.0638666152954102
Epoch 360, training loss: 13.60000228881836 = 0.8801539540290833 + 2.0 * 6.35992431640625
Epoch 360, val loss: 1.0356807708740234
Epoch 370, training loss: 13.553696632385254 = 0.8451055288314819 + 2.0 * 6.35429573059082
Epoch 370, val loss: 1.0096124410629272
Epoch 380, training loss: 13.512088775634766 = 0.811583936214447 + 2.0 * 6.350252628326416
Epoch 380, val loss: 0.9851274490356445
Epoch 390, training loss: 13.472496032714844 = 0.7794889211654663 + 2.0 * 6.346503734588623
Epoch 390, val loss: 0.96214759349823
Epoch 400, training loss: 13.43659782409668 = 0.7487340569496155 + 2.0 * 6.343931674957275
Epoch 400, val loss: 0.9406514763832092
Epoch 410, training loss: 13.410274505615234 = 0.7194652557373047 + 2.0 * 6.345404624938965
Epoch 410, val loss: 0.9206856489181519
Epoch 420, training loss: 13.370562553405762 = 0.6919308304786682 + 2.0 * 6.339315891265869
Epoch 420, val loss: 0.9025729298591614
Epoch 430, training loss: 13.334501266479492 = 0.665621280670166 + 2.0 * 6.334440231323242
Epoch 430, val loss: 0.8859500885009766
Epoch 440, training loss: 13.307592391967773 = 0.6403319239616394 + 2.0 * 6.333630084991455
Epoch 440, val loss: 0.8705479502677917
Epoch 450, training loss: 13.283592224121094 = 0.6162357330322266 + 2.0 * 6.333678245544434
Epoch 450, val loss: 0.856452226638794
Epoch 460, training loss: 13.247987747192383 = 0.5933802127838135 + 2.0 * 6.327303886413574
Epoch 460, val loss: 0.843778669834137
Epoch 470, training loss: 13.221381187438965 = 0.5713453888893127 + 2.0 * 6.325017929077148
Epoch 470, val loss: 0.8321825861930847
Epoch 480, training loss: 13.212315559387207 = 0.5500537157058716 + 2.0 * 6.3311309814453125
Epoch 480, val loss: 0.821472704410553
Epoch 490, training loss: 13.169934272766113 = 0.5295762419700623 + 2.0 * 6.320178985595703
Epoch 490, val loss: 0.8118029832839966
Epoch 500, training loss: 13.14810848236084 = 0.5097850561141968 + 2.0 * 6.319161891937256
Epoch 500, val loss: 0.8030934929847717
Epoch 510, training loss: 13.123208999633789 = 0.49059003591537476 + 2.0 * 6.316309452056885
Epoch 510, val loss: 0.7951680421829224
Epoch 520, training loss: 13.124470710754395 = 0.4719516336917877 + 2.0 * 6.326259613037109
Epoch 520, val loss: 0.7879624962806702
Epoch 530, training loss: 13.084717750549316 = 0.4540291130542755 + 2.0 * 6.315344333648682
Epoch 530, val loss: 0.7815973162651062
Epoch 540, training loss: 13.05945873260498 = 0.4367046654224396 + 2.0 * 6.311377048492432
Epoch 540, val loss: 0.7759807109832764
Epoch 550, training loss: 13.066286087036133 = 0.4198874533176422 + 2.0 * 6.323199272155762
Epoch 550, val loss: 0.7708253264427185
Epoch 560, training loss: 13.019879341125488 = 0.4038204252719879 + 2.0 * 6.308029651641846
Epoch 560, val loss: 0.766314685344696
Epoch 570, training loss: 13.0004301071167 = 0.38822516798973083 + 2.0 * 6.306102275848389
Epoch 570, val loss: 0.7624174952507019
Epoch 580, training loss: 12.980993270874023 = 0.3730188012123108 + 2.0 * 6.3039870262146
Epoch 580, val loss: 0.7588367462158203
Epoch 590, training loss: 12.98332691192627 = 0.3582228720188141 + 2.0 * 6.312551975250244
Epoch 590, val loss: 0.7556684017181396
Epoch 600, training loss: 12.946650505065918 = 0.3439141809940338 + 2.0 * 6.301368236541748
Epoch 600, val loss: 0.7528873085975647
Epoch 610, training loss: 12.92829704284668 = 0.3299539387226105 + 2.0 * 6.299171447753906
Epoch 610, val loss: 0.7504331469535828
Epoch 620, training loss: 12.924546241760254 = 0.31631600856781006 + 2.0 * 6.304115295410156
Epoch 620, val loss: 0.748195469379425
Epoch 630, training loss: 12.902325630187988 = 0.30305320024490356 + 2.0 * 6.299636363983154
Epoch 630, val loss: 0.7461746335029602
Epoch 640, training loss: 12.883724212646484 = 0.29019293189048767 + 2.0 * 6.2967658042907715
Epoch 640, val loss: 0.7445427179336548
Epoch 650, training loss: 12.868762969970703 = 0.27767813205718994 + 2.0 * 6.295542240142822
Epoch 650, val loss: 0.743045449256897
Epoch 660, training loss: 12.850818634033203 = 0.2655346095561981 + 2.0 * 6.292642116546631
Epoch 660, val loss: 0.7417771220207214
Epoch 670, training loss: 12.838399887084961 = 0.2537786066532135 + 2.0 * 6.29231071472168
Epoch 670, val loss: 0.7408397197723389
Epoch 680, training loss: 12.826020240783691 = 0.2424631416797638 + 2.0 * 6.291778564453125
Epoch 680, val loss: 0.7400913238525391
Epoch 690, training loss: 12.812657356262207 = 0.23166082799434662 + 2.0 * 6.29049825668335
Epoch 690, val loss: 0.7397245168685913
Epoch 700, training loss: 12.795404434204102 = 0.22129245102405548 + 2.0 * 6.287055969238281
Epoch 700, val loss: 0.7396957874298096
Epoch 710, training loss: 12.790876388549805 = 0.2113635241985321 + 2.0 * 6.2897562980651855
Epoch 710, val loss: 0.7398726940155029
Epoch 720, training loss: 12.778815269470215 = 0.20194260776042938 + 2.0 * 6.288436412811279
Epoch 720, val loss: 0.7403177618980408
Epoch 730, training loss: 12.760711669921875 = 0.1930222511291504 + 2.0 * 6.283844947814941
Epoch 730, val loss: 0.7413270473480225
Epoch 740, training loss: 12.750414848327637 = 0.1845282018184662 + 2.0 * 6.282943248748779
Epoch 740, val loss: 0.7424909472465515
Epoch 750, training loss: 12.75632381439209 = 0.17647862434387207 + 2.0 * 6.289922714233398
Epoch 750, val loss: 0.7438034415245056
Epoch 760, training loss: 12.736496925354004 = 0.1688683182001114 + 2.0 * 6.283814430236816
Epoch 760, val loss: 0.7454243302345276
Epoch 770, training loss: 12.72414779663086 = 0.16169017553329468 + 2.0 * 6.281229019165039
Epoch 770, val loss: 0.7473180294036865
Epoch 780, training loss: 12.714365005493164 = 0.15488916635513306 + 2.0 * 6.279737949371338
Epoch 780, val loss: 0.7494937181472778
Epoch 790, training loss: 12.70626163482666 = 0.14843317866325378 + 2.0 * 6.278914451599121
Epoch 790, val loss: 0.7518485188484192
Epoch 800, training loss: 12.698553085327148 = 0.14231684803962708 + 2.0 * 6.278118133544922
Epoch 800, val loss: 0.7544440627098083
Epoch 810, training loss: 12.699117660522461 = 0.13651685416698456 + 2.0 * 6.2813005447387695
Epoch 810, val loss: 0.7572062611579895
Epoch 820, training loss: 12.682496070861816 = 0.13103704154491425 + 2.0 * 6.275729656219482
Epoch 820, val loss: 0.7601878046989441
Epoch 830, training loss: 12.674654006958008 = 0.12583480775356293 + 2.0 * 6.274409770965576
Epoch 830, val loss: 0.7633281350135803
Epoch 840, training loss: 12.689618110656738 = 0.12089788168668747 + 2.0 * 6.284359931945801
Epoch 840, val loss: 0.76654052734375
Epoch 850, training loss: 12.663738250732422 = 0.11622017621994019 + 2.0 * 6.273758888244629
Epoch 850, val loss: 0.7698546648025513
Epoch 860, training loss: 12.65473747253418 = 0.11179088801145554 + 2.0 * 6.271473407745361
Epoch 860, val loss: 0.7735036611557007
Epoch 870, training loss: 12.648205757141113 = 0.1075582280755043 + 2.0 * 6.270323753356934
Epoch 870, val loss: 0.777083694934845
Epoch 880, training loss: 12.677659034729004 = 0.10352955758571625 + 2.0 * 6.287064552307129
Epoch 880, val loss: 0.780781626701355
Epoch 890, training loss: 12.638673782348633 = 0.09972024708986282 + 2.0 * 6.269476890563965
Epoch 890, val loss: 0.784557580947876
Epoch 900, training loss: 12.6359224319458 = 0.09609618782997131 + 2.0 * 6.269913196563721
Epoch 900, val loss: 0.7885831594467163
Epoch 910, training loss: 12.635808944702148 = 0.09263986349105835 + 2.0 * 6.271584510803223
Epoch 910, val loss: 0.7924307584762573
Epoch 920, training loss: 12.622841835021973 = 0.08934254944324493 + 2.0 * 6.266749858856201
Epoch 920, val loss: 0.7964003086090088
Epoch 930, training loss: 12.620047569274902 = 0.08619565516710281 + 2.0 * 6.266925811767578
Epoch 930, val loss: 0.8005841374397278
Epoch 940, training loss: 12.629918098449707 = 0.08318387717008591 + 2.0 * 6.273366928100586
Epoch 940, val loss: 0.8045973777770996
Epoch 950, training loss: 12.613456726074219 = 0.08032279461622238 + 2.0 * 6.266566753387451
Epoch 950, val loss: 0.808897852897644
Epoch 960, training loss: 12.605876922607422 = 0.07757574319839478 + 2.0 * 6.264150619506836
Epoch 960, val loss: 0.813150942325592
Epoch 970, training loss: 12.607026100158691 = 0.07495152950286865 + 2.0 * 6.266037464141846
Epoch 970, val loss: 0.8173772096633911
Epoch 980, training loss: 12.596688270568848 = 0.0724371001124382 + 2.0 * 6.262125492095947
Epoch 980, val loss: 0.8216086030006409
Epoch 990, training loss: 12.597859382629395 = 0.07002854347229004 + 2.0 * 6.263915538787842
Epoch 990, val loss: 0.8259871602058411
Epoch 1000, training loss: 12.600193977355957 = 0.0677255392074585 + 2.0 * 6.266234397888184
Epoch 1000, val loss: 0.8303245306015015
Epoch 1010, training loss: 12.586195945739746 = 0.06552234292030334 + 2.0 * 6.260336875915527
Epoch 1010, val loss: 0.8346477746963501
Epoch 1020, training loss: 12.582812309265137 = 0.06341273337602615 + 2.0 * 6.259699821472168
Epoch 1020, val loss: 0.8391178250312805
Epoch 1030, training loss: 12.59725570678711 = 0.06138676404953003 + 2.0 * 6.267934322357178
Epoch 1030, val loss: 0.8433813452720642
Epoch 1040, training loss: 12.581080436706543 = 0.059456534683704376 + 2.0 * 6.260811805725098
Epoch 1040, val loss: 0.8477494120597839
Epoch 1050, training loss: 12.574474334716797 = 0.05759719759225845 + 2.0 * 6.258438587188721
Epoch 1050, val loss: 0.8522723913192749
Epoch 1060, training loss: 12.572416305541992 = 0.05581416189670563 + 2.0 * 6.258301258087158
Epoch 1060, val loss: 0.8566265106201172
Epoch 1070, training loss: 12.585917472839355 = 0.054098840802907944 + 2.0 * 6.265909194946289
Epoch 1070, val loss: 0.8609457612037659
Epoch 1080, training loss: 12.56942367553711 = 0.05245264619588852 + 2.0 * 6.258485317230225
Epoch 1080, val loss: 0.8652271628379822
Epoch 1090, training loss: 12.563673973083496 = 0.050874605774879456 + 2.0 * 6.256399631500244
Epoch 1090, val loss: 0.8697848320007324
Epoch 1100, training loss: 12.567838668823242 = 0.049354154616594315 + 2.0 * 6.259242057800293
Epoch 1100, val loss: 0.8739655613899231
Epoch 1110, training loss: 12.556310653686523 = 0.047897543758153915 + 2.0 * 6.254206657409668
Epoch 1110, val loss: 0.8782652020454407
Epoch 1120, training loss: 12.554140090942383 = 0.04649800807237625 + 2.0 * 6.253820896148682
Epoch 1120, val loss: 0.8827683329582214
Epoch 1130, training loss: 12.566743850708008 = 0.04515277221798897 + 2.0 * 6.260795593261719
Epoch 1130, val loss: 0.8870381116867065
Epoch 1140, training loss: 12.55243968963623 = 0.04385427385568619 + 2.0 * 6.2542924880981445
Epoch 1140, val loss: 0.8911203145980835
Epoch 1150, training loss: 12.54948902130127 = 0.04261312633752823 + 2.0 * 6.2534379959106445
Epoch 1150, val loss: 0.8955952525138855
Epoch 1160, training loss: 12.550140380859375 = 0.04141485318541527 + 2.0 * 6.2543625831604
Epoch 1160, val loss: 0.8998470902442932
Epoch 1170, training loss: 12.54830265045166 = 0.04026094079017639 + 2.0 * 6.254020690917969
Epoch 1170, val loss: 0.9038392305374146
Epoch 1180, training loss: 12.54240894317627 = 0.03914995118975639 + 2.0 * 6.25162935256958
Epoch 1180, val loss: 0.9081323742866516
Epoch 1190, training loss: 12.539005279541016 = 0.03808086737990379 + 2.0 * 6.250462055206299
Epoch 1190, val loss: 0.9123350381851196
Epoch 1200, training loss: 12.545167922973633 = 0.03704714775085449 + 2.0 * 6.2540602684021
Epoch 1200, val loss: 0.9163656830787659
Epoch 1210, training loss: 12.539167404174805 = 0.03605426475405693 + 2.0 * 6.251556396484375
Epoch 1210, val loss: 0.9204216599464417
Epoch 1220, training loss: 12.534774780273438 = 0.03510017320513725 + 2.0 * 6.249837398529053
Epoch 1220, val loss: 0.9246029853820801
Epoch 1230, training loss: 12.532672882080078 = 0.0341816321015358 + 2.0 * 6.249245643615723
Epoch 1230, val loss: 0.928747832775116
Epoch 1240, training loss: 12.534096717834473 = 0.03329228237271309 + 2.0 * 6.250402450561523
Epoch 1240, val loss: 0.9326447248458862
Epoch 1250, training loss: 12.53406810760498 = 0.032432932406663895 + 2.0 * 6.250817775726318
Epoch 1250, val loss: 0.9365623593330383
Epoch 1260, training loss: 12.527978897094727 = 0.031605903059244156 + 2.0 * 6.2481865882873535
Epoch 1260, val loss: 0.9406052827835083
Epoch 1270, training loss: 12.537135124206543 = 0.03081108070909977 + 2.0 * 6.253161907196045
Epoch 1270, val loss: 0.9447115063667297
Epoch 1280, training loss: 12.527597427368164 = 0.030042391270399094 + 2.0 * 6.248777389526367
Epoch 1280, val loss: 0.9481346011161804
Epoch 1290, training loss: 12.526004791259766 = 0.02930787391960621 + 2.0 * 6.248348236083984
Epoch 1290, val loss: 0.9522296190261841
Epoch 1300, training loss: 12.521904945373535 = 0.02859683521091938 + 2.0 * 6.246654033660889
Epoch 1300, val loss: 0.9560719728469849
Epoch 1310, training loss: 12.520263671875 = 0.02790921740233898 + 2.0 * 6.2461771965026855
Epoch 1310, val loss: 0.9597178101539612
Epoch 1320, training loss: 12.535177230834961 = 0.0272431168705225 + 2.0 * 6.25396728515625
Epoch 1320, val loss: 0.9633423089981079
Epoch 1330, training loss: 12.521051406860352 = 0.02659679763019085 + 2.0 * 6.247227191925049
Epoch 1330, val loss: 0.9671571850776672
Epoch 1340, training loss: 12.515694618225098 = 0.025975126773118973 + 2.0 * 6.24485969543457
Epoch 1340, val loss: 0.9709373116493225
Epoch 1350, training loss: 12.513193130493164 = 0.02537083812057972 + 2.0 * 6.243911266326904
Epoch 1350, val loss: 0.9745676517486572
Epoch 1360, training loss: 12.51778507232666 = 0.024784376844763756 + 2.0 * 6.246500492095947
Epoch 1360, val loss: 0.9780833125114441
Epoch 1370, training loss: 12.52701187133789 = 0.024216406047344208 + 2.0 * 6.251397609710693
Epoch 1370, val loss: 0.9815148115158081
Epoch 1380, training loss: 12.517769813537598 = 0.02367374300956726 + 2.0 * 6.2470479011535645
Epoch 1380, val loss: 0.9852369427680969
Epoch 1390, training loss: 12.507646560668945 = 0.0231494028121233 + 2.0 * 6.24224853515625
Epoch 1390, val loss: 0.9889612197875977
Epoch 1400, training loss: 12.506686210632324 = 0.022640591487288475 + 2.0 * 6.24202299118042
Epoch 1400, val loss: 0.9923086762428284
Epoch 1410, training loss: 12.505617141723633 = 0.022145196795463562 + 2.0 * 6.241735935211182
Epoch 1410, val loss: 0.995671272277832
Epoch 1420, training loss: 12.530131340026855 = 0.021664785221219063 + 2.0 * 6.254233360290527
Epoch 1420, val loss: 0.9990296363830566
Epoch 1430, training loss: 12.503270149230957 = 0.021200338378548622 + 2.0 * 6.241034984588623
Epoch 1430, val loss: 1.0023833513259888
Epoch 1440, training loss: 12.503739356994629 = 0.02075241692364216 + 2.0 * 6.2414937019348145
Epoch 1440, val loss: 1.005924105644226
Epoch 1450, training loss: 12.51567554473877 = 0.020318653434515 + 2.0 * 6.247678279876709
Epoch 1450, val loss: 1.0092432498931885
Epoch 1460, training loss: 12.499555587768555 = 0.019894134253263474 + 2.0 * 6.239830493927002
Epoch 1460, val loss: 1.0120738744735718
Epoch 1470, training loss: 12.498198509216309 = 0.019487032666802406 + 2.0 * 6.239355564117432
Epoch 1470, val loss: 1.0156813859939575
Epoch 1480, training loss: 12.496499061584473 = 0.01909167878329754 + 2.0 * 6.238703727722168
Epoch 1480, val loss: 1.018912672996521
Epoch 1490, training loss: 12.496193885803223 = 0.01870572566986084 + 2.0 * 6.238744258880615
Epoch 1490, val loss: 1.0219565629959106
Epoch 1500, training loss: 12.494745254516602 = 0.018330419436097145 + 2.0 * 6.2382073402404785
Epoch 1500, val loss: 1.02516770362854
Epoch 1510, training loss: 12.52393913269043 = 0.017964811995625496 + 2.0 * 6.252987384796143
Epoch 1510, val loss: 1.0283483266830444
Epoch 1520, training loss: 12.505731582641602 = 0.017610015347599983 + 2.0 * 6.24406099319458
Epoch 1520, val loss: 1.0311031341552734
Epoch 1530, training loss: 12.493143081665039 = 0.017270082607865334 + 2.0 * 6.237936496734619
Epoch 1530, val loss: 1.03448486328125
Epoch 1540, training loss: 12.492539405822754 = 0.01693924516439438 + 2.0 * 6.237800121307373
Epoch 1540, val loss: 1.0375747680664062
Epoch 1550, training loss: 12.489873886108398 = 0.016616573557257652 + 2.0 * 6.236628532409668
Epoch 1550, val loss: 1.0403833389282227
Epoch 1560, training loss: 12.518620491027832 = 0.01630149409174919 + 2.0 * 6.25115966796875
Epoch 1560, val loss: 1.0431269407272339
Epoch 1570, training loss: 12.500870704650879 = 0.015996797010302544 + 2.0 * 6.24243688583374
Epoch 1570, val loss: 1.046042561531067
Epoch 1580, training loss: 12.492071151733398 = 0.01570252515375614 + 2.0 * 6.238184452056885
Epoch 1580, val loss: 1.0491763353347778
Epoch 1590, training loss: 12.487793922424316 = 0.015416108071804047 + 2.0 * 6.236188888549805
Epoch 1590, val loss: 1.0521405935287476
Epoch 1600, training loss: 12.492232322692871 = 0.015136685222387314 + 2.0 * 6.2385478019714355
Epoch 1600, val loss: 1.054805040359497
Epoch 1610, training loss: 12.489684104919434 = 0.014863373711705208 + 2.0 * 6.237410545349121
Epoch 1610, val loss: 1.0574357509613037
Epoch 1620, training loss: 12.484594345092773 = 0.014597903937101364 + 2.0 * 6.2349982261657715
Epoch 1620, val loss: 1.0603256225585938
Epoch 1630, training loss: 12.484930038452148 = 0.01433964166790247 + 2.0 * 6.235295295715332
Epoch 1630, val loss: 1.0632160902023315
Epoch 1640, training loss: 12.489580154418945 = 0.014087166637182236 + 2.0 * 6.237746715545654
Epoch 1640, val loss: 1.065792202949524
Epoch 1650, training loss: 12.48469066619873 = 0.013841121457517147 + 2.0 * 6.235424995422363
Epoch 1650, val loss: 1.0683809518814087
Epoch 1660, training loss: 12.491374969482422 = 0.013604221865534782 + 2.0 * 6.238885402679443
Epoch 1660, val loss: 1.0712764263153076
Epoch 1670, training loss: 12.486837387084961 = 0.013369944877922535 + 2.0 * 6.236733913421631
Epoch 1670, val loss: 1.073824405670166
Epoch 1680, training loss: 12.480989456176758 = 0.013145173899829388 + 2.0 * 6.233922004699707
Epoch 1680, val loss: 1.0764802694320679
Epoch 1690, training loss: 12.479464530944824 = 0.012924554757773876 + 2.0 * 6.233270168304443
Epoch 1690, val loss: 1.079100251197815
Epoch 1700, training loss: 12.500662803649902 = 0.01271043624728918 + 2.0 * 6.24397611618042
Epoch 1700, val loss: 1.0816032886505127
Epoch 1710, training loss: 12.480781555175781 = 0.012499322183430195 + 2.0 * 6.2341413497924805
Epoch 1710, val loss: 1.0838464498519897
Epoch 1720, training loss: 12.47719669342041 = 0.012295765802264214 + 2.0 * 6.232450485229492
Epoch 1720, val loss: 1.0866361856460571
Epoch 1730, training loss: 12.476040840148926 = 0.012097087688744068 + 2.0 * 6.231971740722656
Epoch 1730, val loss: 1.0891131162643433
Epoch 1740, training loss: 12.490972518920898 = 0.011902466416358948 + 2.0 * 6.239534854888916
Epoch 1740, val loss: 1.091353178024292
Epoch 1750, training loss: 12.479815483093262 = 0.011711862869560719 + 2.0 * 6.234051704406738
Epoch 1750, val loss: 1.0936341285705566
Epoch 1760, training loss: 12.476753234863281 = 0.011528671719133854 + 2.0 * 6.232612133026123
Epoch 1760, val loss: 1.0963239669799805
Epoch 1770, training loss: 12.482935905456543 = 0.01134888269007206 + 2.0 * 6.235793590545654
Epoch 1770, val loss: 1.0986080169677734
Epoch 1780, training loss: 12.475991249084473 = 0.011173141188919544 + 2.0 * 6.2324090003967285
Epoch 1780, val loss: 1.1008543968200684
Epoch 1790, training loss: 12.472102165222168 = 0.011001615785062313 + 2.0 * 6.230550289154053
Epoch 1790, val loss: 1.1032330989837646
Epoch 1800, training loss: 12.471245765686035 = 0.010833991691470146 + 2.0 * 6.23020601272583
Epoch 1800, val loss: 1.105566143989563
Epoch 1810, training loss: 12.470382690429688 = 0.010669905692338943 + 2.0 * 6.229856491088867
Epoch 1810, val loss: 1.1078375577926636
Epoch 1820, training loss: 12.488386154174805 = 0.01050983089953661 + 2.0 * 6.238938331604004
Epoch 1820, val loss: 1.1099638938903809
Epoch 1830, training loss: 12.483708381652832 = 0.01035287231206894 + 2.0 * 6.236677646636963
Epoch 1830, val loss: 1.1123076677322388
Epoch 1840, training loss: 12.470335006713867 = 0.010199908167123795 + 2.0 * 6.230067729949951
Epoch 1840, val loss: 1.1142219305038452
Epoch 1850, training loss: 12.469109535217285 = 0.010053451173007488 + 2.0 * 6.229527950286865
Epoch 1850, val loss: 1.1165913343429565
Epoch 1860, training loss: 12.46769905090332 = 0.009909326210618019 + 2.0 * 6.2288947105407715
Epoch 1860, val loss: 1.1188044548034668
Epoch 1870, training loss: 12.465058326721191 = 0.009768028743565083 + 2.0 * 6.227644920349121
Epoch 1870, val loss: 1.120845079421997
Epoch 1880, training loss: 12.46500015258789 = 0.009628742933273315 + 2.0 * 6.227685928344727
Epoch 1880, val loss: 1.1228474378585815
Epoch 1890, training loss: 12.475259780883789 = 0.009491817094385624 + 2.0 * 6.232883930206299
Epoch 1890, val loss: 1.1249362230300903
Epoch 1900, training loss: 12.472660064697266 = 0.009357698261737823 + 2.0 * 6.231651306152344
Epoch 1900, val loss: 1.1268510818481445
Epoch 1910, training loss: 12.469254493713379 = 0.009228386916220188 + 2.0 * 6.230012893676758
Epoch 1910, val loss: 1.1290514469146729
Epoch 1920, training loss: 12.464715003967285 = 0.009101279079914093 + 2.0 * 6.22780704498291
Epoch 1920, val loss: 1.1311646699905396
Epoch 1930, training loss: 12.46536922454834 = 0.008977328427135944 + 2.0 * 6.228196144104004
Epoch 1930, val loss: 1.1331452131271362
Epoch 1940, training loss: 12.468307495117188 = 0.008854986168444157 + 2.0 * 6.229726314544678
Epoch 1940, val loss: 1.134952425956726
Epoch 1950, training loss: 12.462777137756348 = 0.00873501691967249 + 2.0 * 6.227021217346191
Epoch 1950, val loss: 1.1369225978851318
Epoch 1960, training loss: 12.464722633361816 = 0.008617850951850414 + 2.0 * 6.228052616119385
Epoch 1960, val loss: 1.1389899253845215
Epoch 1970, training loss: 12.46776008605957 = 0.008502796292304993 + 2.0 * 6.229628562927246
Epoch 1970, val loss: 1.140781044960022
Epoch 1980, training loss: 12.46387004852295 = 0.008391009643673897 + 2.0 * 6.227739334106445
Epoch 1980, val loss: 1.142827033996582
Epoch 1990, training loss: 12.464812278747559 = 0.008281758055090904 + 2.0 * 6.228265285491943
Epoch 1990, val loss: 1.1448105573654175
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8037037037037037
0.838165524512388
=== training gcn model ===
Epoch 0, training loss: 19.158523559570312 = 1.964865803718567 + 2.0 * 8.59682846069336
Epoch 0, val loss: 1.9677202701568604
Epoch 10, training loss: 19.146272659301758 = 1.9534320831298828 + 2.0 * 8.596420288085938
Epoch 10, val loss: 1.95587158203125
Epoch 20, training loss: 19.124902725219727 = 1.9389891624450684 + 2.0 * 8.59295654296875
Epoch 20, val loss: 1.9408448934555054
Epoch 30, training loss: 19.04814338684082 = 1.9190572500228882 + 2.0 * 8.564542770385742
Epoch 30, val loss: 1.9203203916549683
Epoch 40, training loss: 18.637615203857422 = 1.894971489906311 + 2.0 * 8.371321678161621
Epoch 40, val loss: 1.8964660167694092
Epoch 50, training loss: 17.413318634033203 = 1.86991286277771 + 2.0 * 7.771702766418457
Epoch 50, val loss: 1.872268795967102
Epoch 60, training loss: 16.70357894897461 = 1.849589467048645 + 2.0 * 7.426994323730469
Epoch 60, val loss: 1.8539363145828247
Epoch 70, training loss: 16.05621337890625 = 1.8364715576171875 + 2.0 * 7.109870433807373
Epoch 70, val loss: 1.8418515920639038
Epoch 80, training loss: 15.72514820098877 = 1.825074315071106 + 2.0 * 6.950037002563477
Epoch 80, val loss: 1.8309967517852783
Epoch 90, training loss: 15.466320037841797 = 1.8121979236602783 + 2.0 * 6.827061176300049
Epoch 90, val loss: 1.8188502788543701
Epoch 100, training loss: 15.266748428344727 = 1.799363613128662 + 2.0 * 6.733692646026611
Epoch 100, val loss: 1.8070734739303589
Epoch 110, training loss: 15.111961364746094 = 1.7874433994293213 + 2.0 * 6.662259101867676
Epoch 110, val loss: 1.7960784435272217
Epoch 120, training loss: 14.983983039855957 = 1.7763515710830688 + 2.0 * 6.60381555557251
Epoch 120, val loss: 1.7858716249465942
Epoch 130, training loss: 14.874876976013184 = 1.7656198740005493 + 2.0 * 6.554628372192383
Epoch 130, val loss: 1.7760283946990967
Epoch 140, training loss: 14.79307746887207 = 1.7543671131134033 + 2.0 * 6.519355297088623
Epoch 140, val loss: 1.766000747680664
Epoch 150, training loss: 14.724837303161621 = 1.7419729232788086 + 2.0 * 6.491432189941406
Epoch 150, val loss: 1.7551463842391968
Epoch 160, training loss: 14.66572380065918 = 1.7282501459121704 + 2.0 * 6.46873664855957
Epoch 160, val loss: 1.74322509765625
Epoch 170, training loss: 14.61341381072998 = 1.7130120992660522 + 2.0 * 6.450201034545898
Epoch 170, val loss: 1.730126142501831
Epoch 180, training loss: 14.566804885864258 = 1.6959614753723145 + 2.0 * 6.435421943664551
Epoch 180, val loss: 1.7155355215072632
Epoch 190, training loss: 14.521998405456543 = 1.6768522262573242 + 2.0 * 6.422573089599609
Epoch 190, val loss: 1.6992908716201782
Epoch 200, training loss: 14.477643013000488 = 1.655334234237671 + 2.0 * 6.411154270172119
Epoch 200, val loss: 1.6810797452926636
Epoch 210, training loss: 14.43620491027832 = 1.6314278841018677 + 2.0 * 6.402388572692871
Epoch 210, val loss: 1.660833477973938
Epoch 220, training loss: 14.389968872070312 = 1.6050726175308228 + 2.0 * 6.3924479484558105
Epoch 220, val loss: 1.638621211051941
Epoch 230, training loss: 14.342456817626953 = 1.5761628150939941 + 2.0 * 6.383147239685059
Epoch 230, val loss: 1.6141787767410278
Epoch 240, training loss: 14.296319961547852 = 1.5445971488952637 + 2.0 * 6.375861167907715
Epoch 240, val loss: 1.5875648260116577
Epoch 250, training loss: 14.265978813171387 = 1.5105377435684204 + 2.0 * 6.377720355987549
Epoch 250, val loss: 1.5589874982833862
Epoch 260, training loss: 14.20710563659668 = 1.474987268447876 + 2.0 * 6.366059303283691
Epoch 260, val loss: 1.5291461944580078
Epoch 270, training loss: 14.155418395996094 = 1.4384232759475708 + 2.0 * 6.358497619628906
Epoch 270, val loss: 1.4986765384674072
Epoch 280, training loss: 14.106067657470703 = 1.400854229927063 + 2.0 * 6.352606773376465
Epoch 280, val loss: 1.4677773714065552
Epoch 290, training loss: 14.057528495788574 = 1.3625246286392212 + 2.0 * 6.347501754760742
Epoch 290, val loss: 1.4363981485366821
Epoch 300, training loss: 14.021296501159668 = 1.3238728046417236 + 2.0 * 6.348711967468262
Epoch 300, val loss: 1.4050415754318237
Epoch 310, training loss: 13.966043472290039 = 1.2855135202407837 + 2.0 * 6.340264797210693
Epoch 310, val loss: 1.3743075132369995
Epoch 320, training loss: 13.919429779052734 = 1.247544765472412 + 2.0 * 6.335942268371582
Epoch 320, val loss: 1.344213604927063
Epoch 330, training loss: 13.880654335021973 = 1.210110068321228 + 2.0 * 6.335272312164307
Epoch 330, val loss: 1.3149127960205078
Epoch 340, training loss: 13.831019401550293 = 1.1735812425613403 + 2.0 * 6.328719139099121
Epoch 340, val loss: 1.286557912826538
Epoch 350, training loss: 13.78848934173584 = 1.1377356052398682 + 2.0 * 6.325376987457275
Epoch 350, val loss: 1.2590386867523193
Epoch 360, training loss: 13.749448776245117 = 1.102615237236023 + 2.0 * 6.323416709899902
Epoch 360, val loss: 1.2322529554367065
Epoch 370, training loss: 13.719389915466309 = 1.0684551000595093 + 2.0 * 6.325467586517334
Epoch 370, val loss: 1.2063775062561035
Epoch 380, training loss: 13.67142391204834 = 1.035467267036438 + 2.0 * 6.317978382110596
Epoch 380, val loss: 1.1816432476043701
Epoch 390, training loss: 13.632688522338867 = 1.003336787223816 + 2.0 * 6.314675807952881
Epoch 390, val loss: 1.1576969623565674
Epoch 400, training loss: 13.59794807434082 = 0.9718834161758423 + 2.0 * 6.313032150268555
Epoch 400, val loss: 1.1343598365783691
Epoch 410, training loss: 13.57787036895752 = 0.9413083791732788 + 2.0 * 6.318281173706055
Epoch 410, val loss: 1.1116341352462769
Epoch 420, training loss: 13.532851219177246 = 0.9117659330368042 + 2.0 * 6.310542583465576
Epoch 420, val loss: 1.0900384187698364
Epoch 430, training loss: 13.497116088867188 = 0.8829509019851685 + 2.0 * 6.307082653045654
Epoch 430, val loss: 1.069032073020935
Epoch 440, training loss: 13.465227127075195 = 0.8546223044395447 + 2.0 * 6.305302619934082
Epoch 440, val loss: 1.0484049320220947
Epoch 450, training loss: 13.434704780578613 = 0.8267980217933655 + 2.0 * 6.303953170776367
Epoch 450, val loss: 1.02828848361969
Epoch 460, training loss: 13.410553932189941 = 0.799514651298523 + 2.0 * 6.3055195808410645
Epoch 460, val loss: 1.0086438655853271
Epoch 470, training loss: 13.377021789550781 = 0.7727968692779541 + 2.0 * 6.302112579345703
Epoch 470, val loss: 0.9896950125694275
Epoch 480, training loss: 13.343847274780273 = 0.7467894554138184 + 2.0 * 6.298528671264648
Epoch 480, val loss: 0.9713994860649109
Epoch 490, training loss: 13.316295623779297 = 0.7212639451026917 + 2.0 * 6.297515869140625
Epoch 490, val loss: 0.9535192251205444
Epoch 500, training loss: 13.298503875732422 = 0.6962988972663879 + 2.0 * 6.301102638244629
Epoch 500, val loss: 0.9362345337867737
Epoch 510, training loss: 13.263965606689453 = 0.671935498714447 + 2.0 * 6.29601526260376
Epoch 510, val loss: 0.919743537902832
Epoch 520, training loss: 13.233925819396973 = 0.648298442363739 + 2.0 * 6.292813777923584
Epoch 520, val loss: 0.9039996266365051
Epoch 530, training loss: 13.20810317993164 = 0.6251985430717468 + 2.0 * 6.291452407836914
Epoch 530, val loss: 0.888960063457489
Epoch 540, training loss: 13.19294548034668 = 0.6025581955909729 + 2.0 * 6.295193672180176
Epoch 540, val loss: 0.8745089769363403
Epoch 550, training loss: 13.16583251953125 = 0.5804575085639954 + 2.0 * 6.29268741607666
Epoch 550, val loss: 0.8608579039573669
Epoch 560, training loss: 13.13793659210205 = 0.5589869022369385 + 2.0 * 6.289474964141846
Epoch 560, val loss: 0.8479434251785278
Epoch 570, training loss: 13.11233901977539 = 0.537800133228302 + 2.0 * 6.287269592285156
Epoch 570, val loss: 0.8357299566268921
Epoch 580, training loss: 13.092236518859863 = 0.5168785452842712 + 2.0 * 6.287679195404053
Epoch 580, val loss: 0.8239660859107971
Epoch 590, training loss: 13.072151184082031 = 0.49625924229621887 + 2.0 * 6.287945747375488
Epoch 590, val loss: 0.8128736615180969
Epoch 600, training loss: 13.043966293334961 = 0.47587716579437256 + 2.0 * 6.2840447425842285
Epoch 600, val loss: 0.8024136424064636
Epoch 610, training loss: 13.020694732666016 = 0.45557156205177307 + 2.0 * 6.282561779022217
Epoch 610, val loss: 0.7924781441688538
Epoch 620, training loss: 13.007333755493164 = 0.4353368580341339 + 2.0 * 6.285998344421387
Epoch 620, val loss: 0.7830448746681213
Epoch 630, training loss: 12.988097190856934 = 0.4151693880558014 + 2.0 * 6.286463737487793
Epoch 630, val loss: 0.7741654515266418
Epoch 640, training loss: 12.95649528503418 = 0.395345538854599 + 2.0 * 6.280574798583984
Epoch 640, val loss: 0.7662172913551331
Epoch 650, training loss: 12.932851791381836 = 0.37561914324760437 + 2.0 * 6.278616428375244
Epoch 650, val loss: 0.7587776780128479
Epoch 660, training loss: 12.911988258361816 = 0.3560445308685303 + 2.0 * 6.2779717445373535
Epoch 660, val loss: 0.7520176768302917
Epoch 670, training loss: 12.89496898651123 = 0.3367917239665985 + 2.0 * 6.279088497161865
Epoch 670, val loss: 0.7459537982940674
Epoch 680, training loss: 12.874868392944336 = 0.318008154630661 + 2.0 * 6.278429985046387
Epoch 680, val loss: 0.7409525513648987
Epoch 690, training loss: 12.858616828918457 = 0.29981112480163574 + 2.0 * 6.279402732849121
Epoch 690, val loss: 0.7367702126502991
Epoch 700, training loss: 12.832390785217285 = 0.2822507917881012 + 2.0 * 6.2750701904296875
Epoch 700, val loss: 0.733405590057373
Epoch 710, training loss: 12.812162399291992 = 0.2654353082180023 + 2.0 * 6.2733635902404785
Epoch 710, val loss: 0.7310015559196472
Epoch 720, training loss: 12.817182540893555 = 0.24938873946666718 + 2.0 * 6.2838969230651855
Epoch 720, val loss: 0.7293612957000732
Epoch 730, training loss: 12.783782005310059 = 0.23448336124420166 + 2.0 * 6.274649143218994
Epoch 730, val loss: 0.7287471294403076
Epoch 740, training loss: 12.765584945678711 = 0.2204318791627884 + 2.0 * 6.272576332092285
Epoch 740, val loss: 0.7289274334907532
Epoch 750, training loss: 12.74760913848877 = 0.2073470503091812 + 2.0 * 6.2701311111450195
Epoch 750, val loss: 0.729697048664093
Epoch 760, training loss: 12.737648963928223 = 0.19511638581752777 + 2.0 * 6.271266460418701
Epoch 760, val loss: 0.7311786413192749
Epoch 770, training loss: 12.725850105285645 = 0.1838456392288208 + 2.0 * 6.271002292633057
Epoch 770, val loss: 0.7332744598388672
Epoch 780, training loss: 12.710441589355469 = 0.17344142496585846 + 2.0 * 6.268499851226807
Epoch 780, val loss: 0.7360122799873352
Epoch 790, training loss: 12.70132827758789 = 0.16380444169044495 + 2.0 * 6.268762111663818
Epoch 790, val loss: 0.7391204833984375
Epoch 800, training loss: 12.696616172790527 = 0.15488487482070923 + 2.0 * 6.270865440368652
Epoch 800, val loss: 0.7425196766853333
Epoch 810, training loss: 12.67872428894043 = 0.14667750895023346 + 2.0 * 6.2660231590271
Epoch 810, val loss: 0.7465158700942993
Epoch 820, training loss: 12.668261528015137 = 0.1390232890844345 + 2.0 * 6.26461935043335
Epoch 820, val loss: 0.7507757544517517
Epoch 830, training loss: 12.661138534545898 = 0.13189099729061127 + 2.0 * 6.264623641967773
Epoch 830, val loss: 0.755177915096283
Epoch 840, training loss: 12.664390563964844 = 0.12525106966495514 + 2.0 * 6.2695698738098145
Epoch 840, val loss: 0.759783923625946
Epoch 850, training loss: 12.6500825881958 = 0.11913897842168808 + 2.0 * 6.265471935272217
Epoch 850, val loss: 0.7647644877433777
Epoch 860, training loss: 12.637638092041016 = 0.11343106627464294 + 2.0 * 6.26210355758667
Epoch 860, val loss: 0.769906759262085
Epoch 870, training loss: 12.630300521850586 = 0.1080751046538353 + 2.0 * 6.261112689971924
Epoch 870, val loss: 0.7750653028488159
Epoch 880, training loss: 12.623658180236816 = 0.10303433984518051 + 2.0 * 6.260312080383301
Epoch 880, val loss: 0.7804465889930725
Epoch 890, training loss: 12.642663955688477 = 0.09829860925674438 + 2.0 * 6.272182464599609
Epoch 890, val loss: 0.7859881520271301
Epoch 900, training loss: 12.618087768554688 = 0.09389463067054749 + 2.0 * 6.262096405029297
Epoch 900, val loss: 0.7915441989898682
Epoch 910, training loss: 12.609545707702637 = 0.08976247161626816 + 2.0 * 6.259891510009766
Epoch 910, val loss: 0.7973008155822754
Epoch 920, training loss: 12.60281753540039 = 0.08588016778230667 + 2.0 * 6.2584686279296875
Epoch 920, val loss: 0.8030417561531067
Epoch 930, training loss: 12.610239028930664 = 0.08220449835062027 + 2.0 * 6.264017105102539
Epoch 930, val loss: 0.8087870478630066
Epoch 940, training loss: 12.599618911743164 = 0.07876655459403992 + 2.0 * 6.260426044464111
Epoch 940, val loss: 0.8145803213119507
Epoch 950, training loss: 12.589680671691895 = 0.0755300298333168 + 2.0 * 6.257075309753418
Epoch 950, val loss: 0.8206349611282349
Epoch 960, training loss: 12.582393646240234 = 0.07246442139148712 + 2.0 * 6.254964828491211
Epoch 960, val loss: 0.8265427350997925
Epoch 970, training loss: 12.578983306884766 = 0.06955791264772415 + 2.0 * 6.2547125816345215
Epoch 970, val loss: 0.8324481248855591
Epoch 980, training loss: 12.60598373413086 = 0.06679735332727432 + 2.0 * 6.269593238830566
Epoch 980, val loss: 0.8383099436759949
Epoch 990, training loss: 12.576685905456543 = 0.06423715502023697 + 2.0 * 6.256224155426025
Epoch 990, val loss: 0.8443628549575806
Epoch 1000, training loss: 12.568523406982422 = 0.06179629638791084 + 2.0 * 6.253363609313965
Epoch 1000, val loss: 0.850449800491333
Epoch 1010, training loss: 12.56515884399414 = 0.05948132649064064 + 2.0 * 6.252838611602783
Epoch 1010, val loss: 0.8562883138656616
Epoch 1020, training loss: 12.560884475708008 = 0.057281725108623505 + 2.0 * 6.251801490783691
Epoch 1020, val loss: 0.8622228503227234
Epoch 1030, training loss: 12.573587417602539 = 0.055184464901685715 + 2.0 * 6.259201526641846
Epoch 1030, val loss: 0.8681354522705078
Epoch 1040, training loss: 12.56621265411377 = 0.05320711433887482 + 2.0 * 6.256502628326416
Epoch 1040, val loss: 0.8740410208702087
Epoch 1050, training loss: 12.555158615112305 = 0.05133827030658722 + 2.0 * 6.251910209655762
Epoch 1050, val loss: 0.8800573348999023
Epoch 1060, training loss: 12.549881935119629 = 0.04955597594380379 + 2.0 * 6.2501630783081055
Epoch 1060, val loss: 0.8858161568641663
Epoch 1070, training loss: 12.566373825073242 = 0.047851789742708206 + 2.0 * 6.259261131286621
Epoch 1070, val loss: 0.8914569020271301
Epoch 1080, training loss: 12.547262191772461 = 0.046256013214588165 + 2.0 * 6.250503063201904
Epoch 1080, val loss: 0.8973060250282288
Epoch 1090, training loss: 12.542218208312988 = 0.04472523182630539 + 2.0 * 6.248746395111084
Epoch 1090, val loss: 0.9031050205230713
Epoch 1100, training loss: 12.538540840148926 = 0.04326285421848297 + 2.0 * 6.247639179229736
Epoch 1100, val loss: 0.9087358117103577
Epoch 1110, training loss: 12.537384986877441 = 0.04186270758509636 + 2.0 * 6.247761249542236
Epoch 1110, val loss: 0.9143572449684143
Epoch 1120, training loss: 12.552804946899414 = 0.04052332788705826 + 2.0 * 6.25614070892334
Epoch 1120, val loss: 0.9198760390281677
Epoch 1130, training loss: 12.53342056274414 = 0.03925752639770508 + 2.0 * 6.247081756591797
Epoch 1130, val loss: 0.9255008697509766
Epoch 1140, training loss: 12.532687187194824 = 0.03805306926369667 + 2.0 * 6.247316837310791
Epoch 1140, val loss: 0.9311548471450806
Epoch 1150, training loss: 12.542869567871094 = 0.03689192607998848 + 2.0 * 6.252988815307617
Epoch 1150, val loss: 0.9364844560623169
Epoch 1160, training loss: 12.533219337463379 = 0.03579089045524597 + 2.0 * 6.248714447021484
Epoch 1160, val loss: 0.9417684078216553
Epoch 1170, training loss: 12.52538776397705 = 0.034742310643196106 + 2.0 * 6.2453227043151855
Epoch 1170, val loss: 0.9473305940628052
Epoch 1180, training loss: 12.52234935760498 = 0.03373095020651817 + 2.0 * 6.244309425354004
Epoch 1180, val loss: 0.9526063203811646
Epoch 1190, training loss: 12.521661758422852 = 0.03275822475552559 + 2.0 * 6.244451999664307
Epoch 1190, val loss: 0.957842230796814
Epoch 1200, training loss: 12.525201797485352 = 0.03182720020413399 + 2.0 * 6.246687412261963
Epoch 1200, val loss: 0.963140070438385
Epoch 1210, training loss: 12.534356117248535 = 0.030936356633901596 + 2.0 * 6.251709938049316
Epoch 1210, val loss: 0.9683887362480164
Epoch 1220, training loss: 12.519491195678711 = 0.030076412484049797 + 2.0 * 6.2447075843811035
Epoch 1220, val loss: 0.9735243320465088
Epoch 1230, training loss: 12.514063835144043 = 0.029262466356158257 + 2.0 * 6.242400646209717
Epoch 1230, val loss: 0.9787461757659912
Epoch 1240, training loss: 12.511231422424316 = 0.02847215346992016 + 2.0 * 6.241379737854004
Epoch 1240, val loss: 0.9837465882301331
Epoch 1250, training loss: 12.511595726013184 = 0.027711663395166397 + 2.0 * 6.241941928863525
Epoch 1250, val loss: 0.9888203740119934
Epoch 1260, training loss: 12.516409873962402 = 0.026980778202414513 + 2.0 * 6.244714736938477
Epoch 1260, val loss: 0.993766725063324
Epoch 1270, training loss: 12.51644229888916 = 0.026280732825398445 + 2.0 * 6.245080947875977
Epoch 1270, val loss: 0.9988124370574951
Epoch 1280, training loss: 12.508432388305664 = 0.025607025250792503 + 2.0 * 6.24141263961792
Epoch 1280, val loss: 1.0036979913711548
Epoch 1290, training loss: 12.505941390991211 = 0.02496051974594593 + 2.0 * 6.240490436553955
Epoch 1290, val loss: 1.0085614919662476
Epoch 1300, training loss: 12.503999710083008 = 0.02433907240629196 + 2.0 * 6.239830493927002
Epoch 1300, val loss: 1.0133622884750366
Epoch 1310, training loss: 12.52439022064209 = 0.02373640611767769 + 2.0 * 6.250327110290527
Epoch 1310, val loss: 1.0179446935653687
Epoch 1320, training loss: 12.503293991088867 = 0.023160420358181 + 2.0 * 6.240067005157471
Epoch 1320, val loss: 1.022686243057251
Epoch 1330, training loss: 12.50197982788086 = 0.022610269486904144 + 2.0 * 6.239684581756592
Epoch 1330, val loss: 1.0275071859359741
Epoch 1340, training loss: 12.49787425994873 = 0.022074786946177483 + 2.0 * 6.2378997802734375
Epoch 1340, val loss: 1.0320227146148682
Epoch 1350, training loss: 12.497610092163086 = 0.02155596762895584 + 2.0 * 6.238027095794678
Epoch 1350, val loss: 1.036486029624939
Epoch 1360, training loss: 12.512286186218262 = 0.021051602438092232 + 2.0 * 6.245617389678955
Epoch 1360, val loss: 1.0409033298492432
Epoch 1370, training loss: 12.499506950378418 = 0.02057155780494213 + 2.0 * 6.239467620849609
Epoch 1370, val loss: 1.045538306236267
Epoch 1380, training loss: 12.494778633117676 = 0.020106133073568344 + 2.0 * 6.237336158752441
Epoch 1380, val loss: 1.05009126663208
Epoch 1390, training loss: 12.493695259094238 = 0.019656432792544365 + 2.0 * 6.2370195388793945
Epoch 1390, val loss: 1.05447518825531
Epoch 1400, training loss: 12.496710777282715 = 0.019219743087887764 + 2.0 * 6.23874568939209
Epoch 1400, val loss: 1.0587496757507324
Epoch 1410, training loss: 12.495707511901855 = 0.018795255571603775 + 2.0 * 6.2384562492370605
Epoch 1410, val loss: 1.0630414485931396
Epoch 1420, training loss: 12.49360466003418 = 0.018389254808425903 + 2.0 * 6.237607479095459
Epoch 1420, val loss: 1.0672656297683716
Epoch 1430, training loss: 12.488828659057617 = 0.017998693510890007 + 2.0 * 6.235414981842041
Epoch 1430, val loss: 1.0716583728790283
Epoch 1440, training loss: 12.487398147583008 = 0.01761816255748272 + 2.0 * 6.234889984130859
Epoch 1440, val loss: 1.0758895874023438
Epoch 1450, training loss: 12.500258445739746 = 0.017250431701540947 + 2.0 * 6.241504192352295
Epoch 1450, val loss: 1.0800065994262695
Epoch 1460, training loss: 12.490574836730957 = 0.0168912336230278 + 2.0 * 6.236841678619385
Epoch 1460, val loss: 1.0839570760726929
Epoch 1470, training loss: 12.485896110534668 = 0.016546372324228287 + 2.0 * 6.23467493057251
Epoch 1470, val loss: 1.0882415771484375
Epoch 1480, training loss: 12.485307693481445 = 0.016210777685046196 + 2.0 * 6.234548568725586
Epoch 1480, val loss: 1.0923081636428833
Epoch 1490, training loss: 12.49748706817627 = 0.015887608751654625 + 2.0 * 6.240799903869629
Epoch 1490, val loss: 1.0961889028549194
Epoch 1500, training loss: 12.49010944366455 = 0.015570460818707943 + 2.0 * 6.237269401550293
Epoch 1500, val loss: 1.1000545024871826
Epoch 1510, training loss: 12.48266887664795 = 0.015266379341483116 + 2.0 * 6.233701229095459
Epoch 1510, val loss: 1.1041380167007446
Epoch 1520, training loss: 12.480254173278809 = 0.014971371740102768 + 2.0 * 6.232641220092773
Epoch 1520, val loss: 1.1079961061477661
Epoch 1530, training loss: 12.482519149780273 = 0.014683270826935768 + 2.0 * 6.233917713165283
Epoch 1530, val loss: 1.1117826700210571
Epoch 1540, training loss: 12.482762336730957 = 0.014402616769075394 + 2.0 * 6.234179973602295
Epoch 1540, val loss: 1.115535855293274
Epoch 1550, training loss: 12.484855651855469 = 0.014131996780633926 + 2.0 * 6.2353620529174805
Epoch 1550, val loss: 1.1194181442260742
Epoch 1560, training loss: 12.4786958694458 = 0.013869614340364933 + 2.0 * 6.232413291931152
Epoch 1560, val loss: 1.1232268810272217
Epoch 1570, training loss: 12.49422836303711 = 0.013617612421512604 + 2.0 * 6.240305423736572
Epoch 1570, val loss: 1.127030849456787
Epoch 1580, training loss: 12.479554176330566 = 0.013366696424782276 + 2.0 * 6.233093738555908
Epoch 1580, val loss: 1.1303590536117554
Epoch 1590, training loss: 12.477075576782227 = 0.013131195679306984 + 2.0 * 6.2319722175598145
Epoch 1590, val loss: 1.1342594623565674
Epoch 1600, training loss: 12.473267555236816 = 0.012898687273263931 + 2.0 * 6.230184555053711
Epoch 1600, val loss: 1.137892723083496
Epoch 1610, training loss: 12.471110343933105 = 0.012671606615185738 + 2.0 * 6.229219436645508
Epoch 1610, val loss: 1.1413657665252686
Epoch 1620, training loss: 12.470454216003418 = 0.012448293156921864 + 2.0 * 6.229002952575684
Epoch 1620, val loss: 1.144877314567566
Epoch 1630, training loss: 12.484830856323242 = 0.012230194173753262 + 2.0 * 6.236300468444824
Epoch 1630, val loss: 1.1483534574508667
Epoch 1640, training loss: 12.479745864868164 = 0.012019910849630833 + 2.0 * 6.23386287689209
Epoch 1640, val loss: 1.1518210172653198
Epoch 1650, training loss: 12.473138809204102 = 0.011817585676908493 + 2.0 * 6.230660438537598
Epoch 1650, val loss: 1.1554797887802124
Epoch 1660, training loss: 12.4690523147583 = 0.011620423756539822 + 2.0 * 6.228715896606445
Epoch 1660, val loss: 1.1589466333389282
Epoch 1670, training loss: 12.477167129516602 = 0.011427919380366802 + 2.0 * 6.232869625091553
Epoch 1670, val loss: 1.1622462272644043
Epoch 1680, training loss: 12.46975040435791 = 0.011238669976592064 + 2.0 * 6.229255676269531
Epoch 1680, val loss: 1.1654796600341797
Epoch 1690, training loss: 12.46706485748291 = 0.0110546899959445 + 2.0 * 6.2280049324035645
Epoch 1690, val loss: 1.1689221858978271
Epoch 1700, training loss: 12.46933650970459 = 0.010875649750232697 + 2.0 * 6.2292304039001465
Epoch 1700, val loss: 1.1722437143325806
Epoch 1710, training loss: 12.468823432922363 = 0.010700513608753681 + 2.0 * 6.229061603546143
Epoch 1710, val loss: 1.1754058599472046
Epoch 1720, training loss: 12.47605037689209 = 0.010528882034122944 + 2.0 * 6.232760906219482
Epoch 1720, val loss: 1.178611397743225
Epoch 1730, training loss: 12.469025611877441 = 0.010364212095737457 + 2.0 * 6.229330539703369
Epoch 1730, val loss: 1.181758999824524
Epoch 1740, training loss: 12.46304702758789 = 0.010205608792603016 + 2.0 * 6.226420879364014
Epoch 1740, val loss: 1.1850605010986328
Epoch 1750, training loss: 12.461472511291504 = 0.010049437172710896 + 2.0 * 6.225711345672607
Epoch 1750, val loss: 1.1882705688476562
Epoch 1760, training loss: 12.461725234985352 = 0.009895923547446728 + 2.0 * 6.225914478302002
Epoch 1760, val loss: 1.191303014755249
Epoch 1770, training loss: 12.475255012512207 = 0.009745080024003983 + 2.0 * 6.232755184173584
Epoch 1770, val loss: 1.1943217515945435
Epoch 1780, training loss: 12.468528747558594 = 0.009597486816346645 + 2.0 * 6.229465484619141
Epoch 1780, val loss: 1.1973695755004883
Epoch 1790, training loss: 12.463400840759277 = 0.009455434046685696 + 2.0 * 6.226972579956055
Epoch 1790, val loss: 1.2004849910736084
Epoch 1800, training loss: 12.463699340820312 = 0.009316298179328442 + 2.0 * 6.22719144821167
Epoch 1800, val loss: 1.2035117149353027
Epoch 1810, training loss: 12.462503433227539 = 0.009180442430078983 + 2.0 * 6.226661682128906
Epoch 1810, val loss: 1.2063504457473755
Epoch 1820, training loss: 12.457984924316406 = 0.0090491883456707 + 2.0 * 6.224467754364014
Epoch 1820, val loss: 1.2094131708145142
Epoch 1830, training loss: 12.462306022644043 = 0.008919584564864635 + 2.0 * 6.226693153381348
Epoch 1830, val loss: 1.2124029397964478
Epoch 1840, training loss: 12.46303939819336 = 0.008791538886725903 + 2.0 * 6.227123737335205
Epoch 1840, val loss: 1.215201497077942
Epoch 1850, training loss: 12.45768928527832 = 0.00866719987243414 + 2.0 * 6.22451114654541
Epoch 1850, val loss: 1.218164086341858
Epoch 1860, training loss: 12.463180541992188 = 0.00854714959859848 + 2.0 * 6.227316856384277
Epoch 1860, val loss: 1.2211816310882568
Epoch 1870, training loss: 12.462431907653809 = 0.008428778499364853 + 2.0 * 6.227001667022705
Epoch 1870, val loss: 1.2236945629119873
Epoch 1880, training loss: 12.455785751342773 = 0.00831229891628027 + 2.0 * 6.223736763000488
Epoch 1880, val loss: 1.2265548706054688
Epoch 1890, training loss: 12.453323364257812 = 0.008201553486287594 + 2.0 * 6.222560882568359
Epoch 1890, val loss: 1.2294813394546509
Epoch 1900, training loss: 12.45315170288086 = 0.008090359158813953 + 2.0 * 6.222530841827393
Epoch 1900, val loss: 1.232107400894165
Epoch 1910, training loss: 12.468537330627441 = 0.007981263101100922 + 2.0 * 6.230278015136719
Epoch 1910, val loss: 1.2347081899642944
Epoch 1920, training loss: 12.460529327392578 = 0.007874692790210247 + 2.0 * 6.226327419281006
Epoch 1920, val loss: 1.2374063730239868
Epoch 1930, training loss: 12.453679084777832 = 0.0077714258804917336 + 2.0 * 6.222953796386719
Epoch 1930, val loss: 1.2402559518814087
Epoch 1940, training loss: 12.45128059387207 = 0.0076706730760633945 + 2.0 * 6.221805095672607
Epoch 1940, val loss: 1.242993950843811
Epoch 1950, training loss: 12.456171035766602 = 0.007570866961032152 + 2.0 * 6.224299907684326
Epoch 1950, val loss: 1.2455283403396606
Epoch 1960, training loss: 12.454608917236328 = 0.007473048288375139 + 2.0 * 6.223567962646484
Epoch 1960, val loss: 1.2480196952819824
Epoch 1970, training loss: 12.462885856628418 = 0.007378149777650833 + 2.0 * 6.227753639221191
Epoch 1970, val loss: 1.2506883144378662
Epoch 1980, training loss: 12.449806213378906 = 0.007283923681825399 + 2.0 * 6.221261024475098
Epoch 1980, val loss: 1.2532079219818115
Epoch 1990, training loss: 12.450238227844238 = 0.007193869445472956 + 2.0 * 6.221522331237793
Epoch 1990, val loss: 1.2558408975601196
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8037037037037037
0.8423827095413812
The final CL Acc:0.79877, 0.00698, The final GNN Acc:0.83781, 0.00388
Begin epxeriment: cont_weight: 2 epoch:2000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11638])
remove edge: torch.Size([2, 9544])
updated graph: torch.Size([2, 10626])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 19.13015365600586 = 1.9364320039749146 + 2.0 * 8.596860885620117
Epoch 0, val loss: 1.9363703727722168
Epoch 10, training loss: 19.119672775268555 = 1.9264721870422363 + 2.0 * 8.596600532531738
Epoch 10, val loss: 1.9260364770889282
Epoch 20, training loss: 19.102434158325195 = 1.9145541191101074 + 2.0 * 8.593939781188965
Epoch 20, val loss: 1.9133296012878418
Epoch 30, training loss: 19.043174743652344 = 1.8989379405975342 + 2.0 * 8.572118759155273
Epoch 30, val loss: 1.8964649438858032
Epoch 40, training loss: 18.768463134765625 = 1.8797332048416138 + 2.0 * 8.444364547729492
Epoch 40, val loss: 1.8759632110595703
Epoch 50, training loss: 17.92759132385254 = 1.8590677976608276 + 2.0 * 8.034261703491211
Epoch 50, val loss: 1.8541361093521118
Epoch 60, training loss: 17.424163818359375 = 1.8409751653671265 + 2.0 * 7.791594505310059
Epoch 60, val loss: 1.8363878726959229
Epoch 70, training loss: 16.494718551635742 = 1.8274856805801392 + 2.0 * 7.333616256713867
Epoch 70, val loss: 1.823856234550476
Epoch 80, training loss: 15.947803497314453 = 1.8150181770324707 + 2.0 * 7.06639289855957
Epoch 80, val loss: 1.8118107318878174
Epoch 90, training loss: 15.682969093322754 = 1.801939845085144 + 2.0 * 6.94051456451416
Epoch 90, val loss: 1.799071192741394
Epoch 100, training loss: 15.442756652832031 = 1.788088321685791 + 2.0 * 6.827334403991699
Epoch 100, val loss: 1.787024736404419
Epoch 110, training loss: 15.273829460144043 = 1.7753746509552002 + 2.0 * 6.749227523803711
Epoch 110, val loss: 1.7763736248016357
Epoch 120, training loss: 15.155442237854004 = 1.762405514717102 + 2.0 * 6.696518421173096
Epoch 120, val loss: 1.7651573419570923
Epoch 130, training loss: 15.070083618164062 = 1.7479883432388306 + 2.0 * 6.661047458648682
Epoch 130, val loss: 1.7524703741073608
Epoch 140, training loss: 14.99205207824707 = 1.7321510314941406 + 2.0 * 6.629950523376465
Epoch 140, val loss: 1.738752841949463
Epoch 150, training loss: 14.922162055969238 = 1.7145975828170776 + 2.0 * 6.6037821769714355
Epoch 150, val loss: 1.7240407466888428
Epoch 160, training loss: 14.854168891906738 = 1.6949633359909058 + 2.0 * 6.5796027183532715
Epoch 160, val loss: 1.7077852487564087
Epoch 170, training loss: 14.783659934997559 = 1.6726922988891602 + 2.0 * 6.555483818054199
Epoch 170, val loss: 1.689449429512024
Epoch 180, training loss: 14.735006332397461 = 1.6474215984344482 + 2.0 * 6.543792247772217
Epoch 180, val loss: 1.668848991394043
Epoch 190, training loss: 14.642234802246094 = 1.6194840669631958 + 2.0 * 6.511375427246094
Epoch 190, val loss: 1.6461315155029297
Epoch 200, training loss: 14.577797889709473 = 1.5882673263549805 + 2.0 * 6.494765281677246
Epoch 200, val loss: 1.6207982301712036
Epoch 210, training loss: 14.510047912597656 = 1.5532974004745483 + 2.0 * 6.478375434875488
Epoch 210, val loss: 1.5925829410552979
Epoch 220, training loss: 14.445674896240234 = 1.5144034624099731 + 2.0 * 6.465635776519775
Epoch 220, val loss: 1.5614105463027954
Epoch 230, training loss: 14.382349967956543 = 1.4720557928085327 + 2.0 * 6.4551472663879395
Epoch 230, val loss: 1.5278966426849365
Epoch 240, training loss: 14.321061134338379 = 1.4273293018341064 + 2.0 * 6.446866035461426
Epoch 240, val loss: 1.4931071996688843
Epoch 250, training loss: 14.257071495056152 = 1.3809540271759033 + 2.0 * 6.438058853149414
Epoch 250, val loss: 1.4577679634094238
Epoch 260, training loss: 14.194887161254883 = 1.3341656923294067 + 2.0 * 6.430360794067383
Epoch 260, val loss: 1.4226521253585815
Epoch 270, training loss: 14.132744789123535 = 1.2873668670654297 + 2.0 * 6.422688961029053
Epoch 270, val loss: 1.3881133794784546
Epoch 280, training loss: 14.072380065917969 = 1.2409594058990479 + 2.0 * 6.41571044921875
Epoch 280, val loss: 1.3544206619262695
Epoch 290, training loss: 14.01372241973877 = 1.1952768564224243 + 2.0 * 6.409222602844238
Epoch 290, val loss: 1.3218199014663696
Epoch 300, training loss: 13.98154067993164 = 1.1508570909500122 + 2.0 * 6.415341854095459
Epoch 300, val loss: 1.2906712293624878
Epoch 310, training loss: 13.908924102783203 = 1.108275055885315 + 2.0 * 6.40032434463501
Epoch 310, val loss: 1.261360764503479
Epoch 320, training loss: 13.856925964355469 = 1.067413330078125 + 2.0 * 6.394756317138672
Epoch 320, val loss: 1.2336333990097046
Epoch 330, training loss: 13.807104110717773 = 1.0279550552368164 + 2.0 * 6.3895745277404785
Epoch 330, val loss: 1.207312822341919
Epoch 340, training loss: 13.76086139678955 = 0.9896436333656311 + 2.0 * 6.385608673095703
Epoch 340, val loss: 1.182119607925415
Epoch 350, training loss: 13.71958065032959 = 0.952418327331543 + 2.0 * 6.383581161499023
Epoch 350, val loss: 1.1581298112869263
Epoch 360, training loss: 13.673341751098633 = 0.916674017906189 + 2.0 * 6.378334045410156
Epoch 360, val loss: 1.1355732679367065
Epoch 370, training loss: 13.635356903076172 = 0.882357120513916 + 2.0 * 6.376500129699707
Epoch 370, val loss: 1.1143335103988647
Epoch 380, training loss: 13.59351634979248 = 0.8492658138275146 + 2.0 * 6.372125148773193
Epoch 380, val loss: 1.0944486856460571
Epoch 390, training loss: 13.554314613342285 = 0.8172169923782349 + 2.0 * 6.36854887008667
Epoch 390, val loss: 1.0757969617843628
Epoch 400, training loss: 13.518439292907715 = 0.7861177921295166 + 2.0 * 6.366160869598389
Epoch 400, val loss: 1.0582787990570068
Epoch 410, training loss: 13.491848945617676 = 0.7561728954315186 + 2.0 * 6.367837905883789
Epoch 410, val loss: 1.0420876741409302
Epoch 420, training loss: 13.45008659362793 = 0.7274594306945801 + 2.0 * 6.361313343048096
Epoch 420, val loss: 1.0271333456039429
Epoch 430, training loss: 13.415132522583008 = 0.7000178098678589 + 2.0 * 6.35755729675293
Epoch 430, val loss: 1.013756275177002
Epoch 440, training loss: 13.389629364013672 = 0.6736202836036682 + 2.0 * 6.358004570007324
Epoch 440, val loss: 1.0016881227493286
Epoch 450, training loss: 13.36614990234375 = 0.6482964754104614 + 2.0 * 6.358926773071289
Epoch 450, val loss: 0.9907861948013306
Epoch 460, training loss: 13.323814392089844 = 0.6240915656089783 + 2.0 * 6.3498616218566895
Epoch 460, val loss: 0.9812531471252441
Epoch 470, training loss: 13.295918464660645 = 0.6008819341659546 + 2.0 * 6.347518444061279
Epoch 470, val loss: 0.9729037880897522
Epoch 480, training loss: 13.28316593170166 = 0.5784872174263 + 2.0 * 6.352339267730713
Epoch 480, val loss: 0.9655389785766602
Epoch 490, training loss: 13.256665229797363 = 0.5571736097335815 + 2.0 * 6.349745750427246
Epoch 490, val loss: 0.9594822525978088
Epoch 500, training loss: 13.219720840454102 = 0.5366555452346802 + 2.0 * 6.3415327072143555
Epoch 500, val loss: 0.9541383981704712
Epoch 510, training loss: 13.195334434509277 = 0.516897976398468 + 2.0 * 6.3392181396484375
Epoch 510, val loss: 0.9496262073516846
Epoch 520, training loss: 13.172935485839844 = 0.4978383183479309 + 2.0 * 6.337548732757568
Epoch 520, val loss: 0.945821225643158
Epoch 530, training loss: 13.154150009155273 = 0.47945648431777954 + 2.0 * 6.33734655380249
Epoch 530, val loss: 0.9427487254142761
Epoch 540, training loss: 13.128166198730469 = 0.4618014693260193 + 2.0 * 6.333182334899902
Epoch 540, val loss: 0.9402680397033691
Epoch 550, training loss: 13.106562614440918 = 0.44473356008529663 + 2.0 * 6.330914497375488
Epoch 550, val loss: 0.9382877945899963
Epoch 560, training loss: 13.096684455871582 = 0.42824244499206543 + 2.0 * 6.334220886230469
Epoch 560, val loss: 0.9369414448738098
Epoch 570, training loss: 13.094050407409668 = 0.4123816192150116 + 2.0 * 6.340834617614746
Epoch 570, val loss: 0.9360178709030151
Epoch 580, training loss: 13.051033973693848 = 0.3970263600349426 + 2.0 * 6.3270039558410645
Epoch 580, val loss: 0.9352545738220215
Epoch 590, training loss: 13.030730247497559 = 0.38218504190444946 + 2.0 * 6.324272632598877
Epoch 590, val loss: 0.9351261854171753
Epoch 600, training loss: 13.012224197387695 = 0.367700457572937 + 2.0 * 6.322261810302734
Epoch 600, val loss: 0.9353772401809692
Epoch 610, training loss: 12.995854377746582 = 0.35348689556121826 + 2.0 * 6.321183681488037
Epoch 610, val loss: 0.9357879161834717
Epoch 620, training loss: 12.992146492004395 = 0.33953529596328735 + 2.0 * 6.326305389404297
Epoch 620, val loss: 0.9362668395042419
Epoch 630, training loss: 12.968311309814453 = 0.3258535861968994 + 2.0 * 6.321228981018066
Epoch 630, val loss: 0.9367878437042236
Epoch 640, training loss: 12.952391624450684 = 0.3123941421508789 + 2.0 * 6.319998741149902
Epoch 640, val loss: 0.9375810623168945
Epoch 650, training loss: 12.931325912475586 = 0.29914602637290955 + 2.0 * 6.316090106964111
Epoch 650, val loss: 0.9384157657623291
Epoch 660, training loss: 12.916851997375488 = 0.2860936224460602 + 2.0 * 6.3153791427612305
Epoch 660, val loss: 0.939440131187439
Epoch 670, training loss: 12.897743225097656 = 0.27315863966941833 + 2.0 * 6.312292098999023
Epoch 670, val loss: 0.9403790831565857
Epoch 680, training loss: 12.899701118469238 = 0.2603781819343567 + 2.0 * 6.319661617279053
Epoch 680, val loss: 0.9413492679595947
Epoch 690, training loss: 12.87600326538086 = 0.24790458381175995 + 2.0 * 6.314049243927002
Epoch 690, val loss: 0.9428582191467285
Epoch 700, training loss: 12.856467247009277 = 0.23566934466362 + 2.0 * 6.310399055480957
Epoch 700, val loss: 0.9440890550613403
Epoch 710, training loss: 12.839786529541016 = 0.22379782795906067 + 2.0 * 6.307994365692139
Epoch 710, val loss: 0.9458492994308472
Epoch 720, training loss: 12.845745086669922 = 0.21227562427520752 + 2.0 * 6.316734790802002
Epoch 720, val loss: 0.9475702047348022
Epoch 730, training loss: 12.81396484375 = 0.20128557085990906 + 2.0 * 6.306339740753174
Epoch 730, val loss: 0.949625551700592
Epoch 740, training loss: 12.802713394165039 = 0.19078828394412994 + 2.0 * 6.305962562561035
Epoch 740, val loss: 0.9522243738174438
Epoch 750, training loss: 12.799415588378906 = 0.18078354001045227 + 2.0 * 6.309316158294678
Epoch 750, val loss: 0.9550210237503052
Epoch 760, training loss: 12.777336120605469 = 0.17134103178977966 + 2.0 * 6.302997589111328
Epoch 760, val loss: 0.9585186839103699
Epoch 770, training loss: 12.765120506286621 = 0.16237834095954895 + 2.0 * 6.301371097564697
Epoch 770, val loss: 0.9621669054031372
Epoch 780, training loss: 12.753100395202637 = 0.1539067029953003 + 2.0 * 6.299596786499023
Epoch 780, val loss: 0.9661213159561157
Epoch 790, training loss: 12.76402473449707 = 0.1459025740623474 + 2.0 * 6.309061050415039
Epoch 790, val loss: 0.9705292582511902
Epoch 800, training loss: 12.744479179382324 = 0.13841041922569275 + 2.0 * 6.30303430557251
Epoch 800, val loss: 0.9750440716743469
Epoch 810, training loss: 12.727821350097656 = 0.13141097128391266 + 2.0 * 6.298205375671387
Epoch 810, val loss: 0.9802687168121338
Epoch 820, training loss: 12.718949317932129 = 0.12483611702919006 + 2.0 * 6.297056674957275
Epoch 820, val loss: 0.9857596158981323
Epoch 830, training loss: 12.71062183380127 = 0.11863771826028824 + 2.0 * 6.295991897583008
Epoch 830, val loss: 0.9914080500602722
Epoch 840, training loss: 12.707386016845703 = 0.11280441284179688 + 2.0 * 6.297290802001953
Epoch 840, val loss: 0.9972964525222778
Epoch 850, training loss: 12.693109512329102 = 0.10729917138814926 + 2.0 * 6.292905330657959
Epoch 850, val loss: 1.0031852722167969
Epoch 860, training loss: 12.690162658691406 = 0.10213088989257812 + 2.0 * 6.294015884399414
Epoch 860, val loss: 1.0093261003494263
Epoch 870, training loss: 12.681696891784668 = 0.09728352725505829 + 2.0 * 6.292206764221191
Epoch 870, val loss: 1.0156365633010864
Epoch 880, training loss: 12.69604206085205 = 0.09271208941936493 + 2.0 * 6.30166482925415
Epoch 880, val loss: 1.0218102931976318
Epoch 890, training loss: 12.67460823059082 = 0.08845386654138565 + 2.0 * 6.293076992034912
Epoch 890, val loss: 1.028627634048462
Epoch 900, training loss: 12.661916732788086 = 0.08442418277263641 + 2.0 * 6.288746356964111
Epoch 900, val loss: 1.0351327657699585
Epoch 910, training loss: 12.654879570007324 = 0.08063248544931412 + 2.0 * 6.287123680114746
Epoch 910, val loss: 1.0418996810913086
Epoch 920, training loss: 12.652569770812988 = 0.07704488188028336 + 2.0 * 6.287762641906738
Epoch 920, val loss: 1.0485142469406128
Epoch 930, training loss: 12.65001392364502 = 0.07366974651813507 + 2.0 * 6.288172245025635
Epoch 930, val loss: 1.0551161766052246
Epoch 940, training loss: 12.644662857055664 = 0.07050830125808716 + 2.0 * 6.2870774269104
Epoch 940, val loss: 1.0617294311523438
Epoch 950, training loss: 12.63641357421875 = 0.06754083931446075 + 2.0 * 6.284436225891113
Epoch 950, val loss: 1.0686233043670654
Epoch 960, training loss: 12.638062477111816 = 0.06473469734191895 + 2.0 * 6.286664009094238
Epoch 960, val loss: 1.0754231214523315
Epoch 970, training loss: 12.629229545593262 = 0.062077660113573074 + 2.0 * 6.283576011657715
Epoch 970, val loss: 1.0818328857421875
Epoch 980, training loss: 12.625565528869629 = 0.059564098715782166 + 2.0 * 6.283000946044922
Epoch 980, val loss: 1.0883350372314453
Epoch 990, training loss: 12.627382278442383 = 0.0571964792907238 + 2.0 * 6.285092830657959
Epoch 990, val loss: 1.0949839353561401
Epoch 1000, training loss: 12.624370574951172 = 0.05497163534164429 + 2.0 * 6.284699440002441
Epoch 1000, val loss: 1.1016596555709839
Epoch 1010, training loss: 12.616477012634277 = 0.05285213887691498 + 2.0 * 6.28181266784668
Epoch 1010, val loss: 1.107973337173462
Epoch 1020, training loss: 12.608746528625488 = 0.05085289478302002 + 2.0 * 6.278946876525879
Epoch 1020, val loss: 1.1143863201141357
Epoch 1030, training loss: 12.606917381286621 = 0.04895912483334541 + 2.0 * 6.278979301452637
Epoch 1030, val loss: 1.1207969188690186
Epoch 1040, training loss: 12.623208045959473 = 0.04716016724705696 + 2.0 * 6.288023948669434
Epoch 1040, val loss: 1.1269243955612183
Epoch 1050, training loss: 12.606616973876953 = 0.04547016695141792 + 2.0 * 6.28057336807251
Epoch 1050, val loss: 1.1335186958312988
Epoch 1060, training loss: 12.59908390045166 = 0.04385794699192047 + 2.0 * 6.277613162994385
Epoch 1060, val loss: 1.1397167444229126
Epoch 1070, training loss: 12.59416675567627 = 0.042332977056503296 + 2.0 * 6.275917053222656
Epoch 1070, val loss: 1.146073579788208
Epoch 1080, training loss: 12.594718933105469 = 0.04087773337960243 + 2.0 * 6.276920795440674
Epoch 1080, val loss: 1.1522071361541748
Epoch 1090, training loss: 12.59408950805664 = 0.03949720785021782 + 2.0 * 6.27729606628418
Epoch 1090, val loss: 1.1583635807037354
Epoch 1100, training loss: 12.588740348815918 = 0.03818053379654884 + 2.0 * 6.275279998779297
Epoch 1100, val loss: 1.1642729043960571
Epoch 1110, training loss: 12.585514068603516 = 0.036931268870830536 + 2.0 * 6.274291515350342
Epoch 1110, val loss: 1.1702207326889038
Epoch 1120, training loss: 12.583795547485352 = 0.03574245423078537 + 2.0 * 6.274026393890381
Epoch 1120, val loss: 1.1761046648025513
Epoch 1130, training loss: 12.589680671691895 = 0.034604575484991074 + 2.0 * 6.277537822723389
Epoch 1130, val loss: 1.181876540184021
Epoch 1140, training loss: 12.577781677246094 = 0.03352668508887291 + 2.0 * 6.272127628326416
Epoch 1140, val loss: 1.187880516052246
Epoch 1150, training loss: 12.589849472045898 = 0.03249938413500786 + 2.0 * 6.278675079345703
Epoch 1150, val loss: 1.1937159299850464
Epoch 1160, training loss: 12.577820777893066 = 0.03150798752903938 + 2.0 * 6.27315616607666
Epoch 1160, val loss: 1.1990745067596436
Epoch 1170, training loss: 12.571253776550293 = 0.030572528019547462 + 2.0 * 6.270340442657471
Epoch 1170, val loss: 1.204919457435608
Epoch 1180, training loss: 12.568445205688477 = 0.029671261087059975 + 2.0 * 6.2693867683410645
Epoch 1180, val loss: 1.2105281352996826
Epoch 1190, training loss: 12.583040237426758 = 0.02881106175482273 + 2.0 * 6.277114391326904
Epoch 1190, val loss: 1.216174602508545
Epoch 1200, training loss: 12.58093547821045 = 0.02798621729016304 + 2.0 * 6.276474475860596
Epoch 1200, val loss: 1.2213687896728516
Epoch 1210, training loss: 12.563251495361328 = 0.027198119089007378 + 2.0 * 6.268026828765869
Epoch 1210, val loss: 1.2265985012054443
Epoch 1220, training loss: 12.563076972961426 = 0.026446674019098282 + 2.0 * 6.268315315246582
Epoch 1220, val loss: 1.2319676876068115
Epoch 1230, training loss: 12.57312297821045 = 0.025723906233906746 + 2.0 * 6.273699760437012
Epoch 1230, val loss: 1.2372132539749146
Epoch 1240, training loss: 12.574580192565918 = 0.025038421154022217 + 2.0 * 6.274770736694336
Epoch 1240, val loss: 1.242400884628296
Epoch 1250, training loss: 12.55821704864502 = 0.02436866983771324 + 2.0 * 6.2669243812561035
Epoch 1250, val loss: 1.2472913265228271
Epoch 1260, training loss: 12.556609153747559 = 0.02373499609529972 + 2.0 * 6.26643705368042
Epoch 1260, val loss: 1.252435326576233
Epoch 1270, training loss: 12.555200576782227 = 0.02312260866165161 + 2.0 * 6.26603889465332
Epoch 1270, val loss: 1.2574950456619263
Epoch 1280, training loss: 12.560280799865723 = 0.02253253571689129 + 2.0 * 6.268874168395996
Epoch 1280, val loss: 1.2623975276947021
Epoch 1290, training loss: 12.555225372314453 = 0.021965445950627327 + 2.0 * 6.266630172729492
Epoch 1290, val loss: 1.2672375440597534
Epoch 1300, training loss: 12.559937477111816 = 0.021420292556285858 + 2.0 * 6.269258499145508
Epoch 1300, val loss: 1.2721004486083984
Epoch 1310, training loss: 12.548526763916016 = 0.020900847390294075 + 2.0 * 6.263813018798828
Epoch 1310, val loss: 1.2769114971160889
Epoch 1320, training loss: 12.548155784606934 = 0.020398149266839027 + 2.0 * 6.26387882232666
Epoch 1320, val loss: 1.2818489074707031
Epoch 1330, training loss: 12.551811218261719 = 0.019913237541913986 + 2.0 * 6.26594877243042
Epoch 1330, val loss: 1.2865537405014038
Epoch 1340, training loss: 12.54793643951416 = 0.01944442093372345 + 2.0 * 6.264245986938477
Epoch 1340, val loss: 1.2910758256912231
Epoch 1350, training loss: 12.543838500976562 = 0.018991824239492416 + 2.0 * 6.262423515319824
Epoch 1350, val loss: 1.2955743074417114
Epoch 1360, training loss: 12.543193817138672 = 0.01855653151869774 + 2.0 * 6.2623186111450195
Epoch 1360, val loss: 1.3000696897506714
Epoch 1370, training loss: 12.567268371582031 = 0.01813500188291073 + 2.0 * 6.274566650390625
Epoch 1370, val loss: 1.3043060302734375
Epoch 1380, training loss: 12.546636581420898 = 0.017733504995703697 + 2.0 * 6.264451503753662
Epoch 1380, val loss: 1.3087482452392578
Epoch 1390, training loss: 12.539225578308105 = 0.017343679443001747 + 2.0 * 6.260941028594971
Epoch 1390, val loss: 1.3131576776504517
Epoch 1400, training loss: 12.537812232971191 = 0.016969207674264908 + 2.0 * 6.260421276092529
Epoch 1400, val loss: 1.3175941705703735
Epoch 1410, training loss: 12.552038192749023 = 0.016606399789452553 + 2.0 * 6.267715930938721
Epoch 1410, val loss: 1.321907877922058
Epoch 1420, training loss: 12.539965629577637 = 0.016249073669314384 + 2.0 * 6.2618584632873535
Epoch 1420, val loss: 1.3258112668991089
Epoch 1430, training loss: 12.534089088439941 = 0.015910953283309937 + 2.0 * 6.25908899307251
Epoch 1430, val loss: 1.3300732374191284
Epoch 1440, training loss: 12.53238296508789 = 0.015580926090478897 + 2.0 * 6.258400917053223
Epoch 1440, val loss: 1.3342190980911255
Epoch 1450, training loss: 12.545502662658691 = 0.015262485481798649 + 2.0 * 6.265120029449463
Epoch 1450, val loss: 1.3384071588516235
Epoch 1460, training loss: 12.541767120361328 = 0.014954361133277416 + 2.0 * 6.263406276702881
Epoch 1460, val loss: 1.3421231508255005
Epoch 1470, training loss: 12.530754089355469 = 0.014654753729701042 + 2.0 * 6.258049488067627
Epoch 1470, val loss: 1.3459811210632324
Epoch 1480, training loss: 12.528247833251953 = 0.014366640709340572 + 2.0 * 6.2569403648376465
Epoch 1480, val loss: 1.349897027015686
Epoch 1490, training loss: 12.528257369995117 = 0.014086976647377014 + 2.0 * 6.25708532333374
Epoch 1490, val loss: 1.3537826538085938
Epoch 1500, training loss: 12.538006782531738 = 0.013814655132591724 + 2.0 * 6.262095928192139
Epoch 1500, val loss: 1.3575176000595093
Epoch 1510, training loss: 12.541159629821777 = 0.01355340238660574 + 2.0 * 6.263803005218506
Epoch 1510, val loss: 1.3614994287490845
Epoch 1520, training loss: 12.526909828186035 = 0.013295847922563553 + 2.0 * 6.25680685043335
Epoch 1520, val loss: 1.364778995513916
Epoch 1530, training loss: 12.523306846618652 = 0.013049555942416191 + 2.0 * 6.255128860473633
Epoch 1530, val loss: 1.368499755859375
Epoch 1540, training loss: 12.521671295166016 = 0.012811197899281979 + 2.0 * 6.254429817199707
Epoch 1540, val loss: 1.372234582901001
Epoch 1550, training loss: 12.521200180053711 = 0.012576893903315067 + 2.0 * 6.254311561584473
Epoch 1550, val loss: 1.375893473625183
Epoch 1560, training loss: 12.529746055603027 = 0.012350143864750862 + 2.0 * 6.258697986602783
Epoch 1560, val loss: 1.3795839548110962
Epoch 1570, training loss: 12.542346000671387 = 0.012131287716329098 + 2.0 * 6.265107154846191
Epoch 1570, val loss: 1.382882833480835
Epoch 1580, training loss: 12.526731491088867 = 0.011914229951798916 + 2.0 * 6.257408618927002
Epoch 1580, val loss: 1.385888934135437
Epoch 1590, training loss: 12.518983840942383 = 0.01171113271266222 + 2.0 * 6.253636360168457
Epoch 1590, val loss: 1.3896996974945068
Epoch 1600, training loss: 12.517066955566406 = 0.011509252712130547 + 2.0 * 6.252779006958008
Epoch 1600, val loss: 1.3930397033691406
Epoch 1610, training loss: 12.518051147460938 = 0.011312962509691715 + 2.0 * 6.253368854522705
Epoch 1610, val loss: 1.3964929580688477
Epoch 1620, training loss: 12.52703857421875 = 0.011122298426926136 + 2.0 * 6.257957935333252
Epoch 1620, val loss: 1.3998091220855713
Epoch 1630, training loss: 12.521985054016113 = 0.01093575544655323 + 2.0 * 6.255524635314941
Epoch 1630, val loss: 1.4029114246368408
Epoch 1640, training loss: 12.518838882446289 = 0.010755221359431744 + 2.0 * 6.25404167175293
Epoch 1640, val loss: 1.4062061309814453
Epoch 1650, training loss: 12.515341758728027 = 0.010579383000731468 + 2.0 * 6.252381324768066
Epoch 1650, val loss: 1.409384846687317
Epoch 1660, training loss: 12.517833709716797 = 0.010408525355160236 + 2.0 * 6.2537126541137695
Epoch 1660, val loss: 1.4125821590423584
Epoch 1670, training loss: 12.51754093170166 = 0.010242278687655926 + 2.0 * 6.2536492347717285
Epoch 1670, val loss: 1.4156209230422974
Epoch 1680, training loss: 12.524666786193848 = 0.010080752894282341 + 2.0 * 6.257293224334717
Epoch 1680, val loss: 1.418647289276123
Epoch 1690, training loss: 12.512484550476074 = 0.009920183569192886 + 2.0 * 6.251282215118408
Epoch 1690, val loss: 1.421377420425415
Epoch 1700, training loss: 12.510488510131836 = 0.009767981246113777 + 2.0 * 6.250360488891602
Epoch 1700, val loss: 1.424452781677246
Epoch 1710, training loss: 12.512560844421387 = 0.00961664505302906 + 2.0 * 6.251471996307373
Epoch 1710, val loss: 1.4273607730865479
Epoch 1720, training loss: 12.516711235046387 = 0.009470055811107159 + 2.0 * 6.253620624542236
Epoch 1720, val loss: 1.430242657661438
Epoch 1730, training loss: 12.510420799255371 = 0.009328694082796574 + 2.0 * 6.250545978546143
Epoch 1730, val loss: 1.4333363771438599
Epoch 1740, training loss: 12.516189575195312 = 0.009189626201987267 + 2.0 * 6.253499984741211
Epoch 1740, val loss: 1.4361697435379028
Epoch 1750, training loss: 12.508151054382324 = 0.009052985347807407 + 2.0 * 6.24954891204834
Epoch 1750, val loss: 1.438746452331543
Epoch 1760, training loss: 12.504942893981934 = 0.008921405300498009 + 2.0 * 6.248010635375977
Epoch 1760, val loss: 1.4416776895523071
Epoch 1770, training loss: 12.524796485900879 = 0.008792202919721603 + 2.0 * 6.258002281188965
Epoch 1770, val loss: 1.4443475008010864
Epoch 1780, training loss: 12.51216983795166 = 0.008665109053254128 + 2.0 * 6.2517523765563965
Epoch 1780, val loss: 1.4467836618423462
Epoch 1790, training loss: 12.504901885986328 = 0.008543049916625023 + 2.0 * 6.2481794357299805
Epoch 1790, val loss: 1.4496220350265503
Epoch 1800, training loss: 12.504243850708008 = 0.008423112332820892 + 2.0 * 6.247910499572754
Epoch 1800, val loss: 1.4522141218185425
Epoch 1810, training loss: 12.514690399169922 = 0.00830505695194006 + 2.0 * 6.253192901611328
Epoch 1810, val loss: 1.4547159671783447
Epoch 1820, training loss: 12.502096176147461 = 0.008191836066544056 + 2.0 * 6.246952056884766
Epoch 1820, val loss: 1.4573849439620972
Epoch 1830, training loss: 12.502114295959473 = 0.008080334402620792 + 2.0 * 6.247016906738281
Epoch 1830, val loss: 1.4600504636764526
Epoch 1840, training loss: 12.503824234008789 = 0.00797017477452755 + 2.0 * 6.247927188873291
Epoch 1840, val loss: 1.4625017642974854
Epoch 1850, training loss: 12.50473403930664 = 0.00786280445754528 + 2.0 * 6.2484354972839355
Epoch 1850, val loss: 1.4648715257644653
Epoch 1860, training loss: 12.500872611999512 = 0.007758026942610741 + 2.0 * 6.246557235717773
Epoch 1860, val loss: 1.4673166275024414
Epoch 1870, training loss: 12.49782943725586 = 0.007656175177544355 + 2.0 * 6.245086669921875
Epoch 1870, val loss: 1.4699076414108276
Epoch 1880, training loss: 12.505706787109375 = 0.007556284312158823 + 2.0 * 6.249075412750244
Epoch 1880, val loss: 1.4723402261734009
Epoch 1890, training loss: 12.504412651062012 = 0.00745713897049427 + 2.0 * 6.248477935791016
Epoch 1890, val loss: 1.474454402923584
Epoch 1900, training loss: 12.500983238220215 = 0.007362029515206814 + 2.0 * 6.246810436248779
Epoch 1900, val loss: 1.4767061471939087
Epoch 1910, training loss: 12.494892120361328 = 0.007269180379807949 + 2.0 * 6.24381160736084
Epoch 1910, val loss: 1.479203701019287
Epoch 1920, training loss: 12.497820854187012 = 0.007177701685577631 + 2.0 * 6.245321750640869
Epoch 1920, val loss: 1.4813709259033203
Epoch 1930, training loss: 12.505932807922363 = 0.007087127771228552 + 2.0 * 6.249423027038574
Epoch 1930, val loss: 1.4834880828857422
Epoch 1940, training loss: 12.495925903320312 = 0.0069997236132621765 + 2.0 * 6.244462966918945
Epoch 1940, val loss: 1.485789179801941
Epoch 1950, training loss: 12.492987632751465 = 0.006915156729519367 + 2.0 * 6.243036270141602
Epoch 1950, val loss: 1.4881558418273926
Epoch 1960, training loss: 12.493657112121582 = 0.006831095553934574 + 2.0 * 6.243412971496582
Epoch 1960, val loss: 1.4903746843338013
Epoch 1970, training loss: 12.497915267944336 = 0.006748736836016178 + 2.0 * 6.2455830574035645
Epoch 1970, val loss: 1.4925620555877686
Epoch 1980, training loss: 12.494264602661133 = 0.0066671110689640045 + 2.0 * 6.243798732757568
Epoch 1980, val loss: 1.4945372343063354
Epoch 1990, training loss: 12.491859436035156 = 0.006586731411516666 + 2.0 * 6.242636203765869
Epoch 1990, val loss: 1.4964827299118042
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.737037037037037
0.8181338956246705
=== training gcn model ===
Epoch 0, training loss: 19.16671371459961 = 1.972983479499817 + 2.0 * 8.596864700317383
Epoch 0, val loss: 1.9698612689971924
Epoch 10, training loss: 19.155563354492188 = 1.9623044729232788 + 2.0 * 8.59662914276123
Epoch 10, val loss: 1.9596327543258667
Epoch 20, training loss: 19.137941360473633 = 1.949313759803772 + 2.0 * 8.594313621520996
Epoch 20, val loss: 1.9465899467468262
Epoch 30, training loss: 19.082273483276367 = 1.9317243099212646 + 2.0 * 8.575274467468262
Epoch 30, val loss: 1.928404688835144
Epoch 40, training loss: 18.82991600036621 = 1.9091147184371948 + 2.0 * 8.460400581359863
Epoch 40, val loss: 1.9055659770965576
Epoch 50, training loss: 17.972192764282227 = 1.8844081163406372 + 2.0 * 8.043891906738281
Epoch 50, val loss: 1.8815829753875732
Epoch 60, training loss: 17.56301498413086 = 1.863767147064209 + 2.0 * 7.849623680114746
Epoch 60, val loss: 1.8631446361541748
Epoch 70, training loss: 16.664003372192383 = 1.8478220701217651 + 2.0 * 7.408091068267822
Epoch 70, val loss: 1.8485181331634521
Epoch 80, training loss: 15.95580768585205 = 1.8335591554641724 + 2.0 * 7.061124324798584
Epoch 80, val loss: 1.8356484174728394
Epoch 90, training loss: 15.58835220336914 = 1.819456696510315 + 2.0 * 6.8844475746154785
Epoch 90, val loss: 1.823136329650879
Epoch 100, training loss: 15.376459121704102 = 1.8051998615264893 + 2.0 * 6.785629749298096
Epoch 100, val loss: 1.8108856678009033
Epoch 110, training loss: 15.233607292175293 = 1.79148268699646 + 2.0 * 6.721062183380127
Epoch 110, val loss: 1.7993998527526855
Epoch 120, training loss: 15.1105318069458 = 1.7787220478057861 + 2.0 * 6.665904998779297
Epoch 120, val loss: 1.7886135578155518
Epoch 130, training loss: 15.02375602722168 = 1.766221284866333 + 2.0 * 6.628767490386963
Epoch 130, val loss: 1.777848720550537
Epoch 140, training loss: 14.936856269836426 = 1.7533009052276611 + 2.0 * 6.591777801513672
Epoch 140, val loss: 1.7667944431304932
Epoch 150, training loss: 14.868391036987305 = 1.7395662069320679 + 2.0 * 6.564412593841553
Epoch 150, val loss: 1.7552850246429443
Epoch 160, training loss: 14.807455062866211 = 1.7245991230010986 + 2.0 * 6.541428089141846
Epoch 160, val loss: 1.7429687976837158
Epoch 170, training loss: 14.741960525512695 = 1.7082003355026245 + 2.0 * 6.516880035400391
Epoch 170, val loss: 1.7295348644256592
Epoch 180, training loss: 14.689679145812988 = 1.690179467201233 + 2.0 * 6.499749660491943
Epoch 180, val loss: 1.7148317098617554
Epoch 190, training loss: 14.635187149047852 = 1.670190453529358 + 2.0 * 6.4824981689453125
Epoch 190, val loss: 1.698496699333191
Epoch 200, training loss: 14.584025382995605 = 1.6479313373565674 + 2.0 * 6.468047142028809
Epoch 200, val loss: 1.6802597045898438
Epoch 210, training loss: 14.535282135009766 = 1.623083233833313 + 2.0 * 6.456099510192871
Epoch 210, val loss: 1.6599032878875732
Epoch 220, training loss: 14.485010147094727 = 1.595695972442627 + 2.0 * 6.444657325744629
Epoch 220, val loss: 1.6375830173492432
Epoch 230, training loss: 14.438581466674805 = 1.5660128593444824 + 2.0 * 6.436284065246582
Epoch 230, val loss: 1.6133068799972534
Epoch 240, training loss: 14.386195182800293 = 1.533990502357483 + 2.0 * 6.426102161407471
Epoch 240, val loss: 1.5872972011566162
Epoch 250, training loss: 14.3343505859375 = 1.4995843172073364 + 2.0 * 6.417383193969727
Epoch 250, val loss: 1.5596095323562622
Epoch 260, training loss: 14.283407211303711 = 1.4628664255142212 + 2.0 * 6.4102702140808105
Epoch 260, val loss: 1.530182957649231
Epoch 270, training loss: 14.234345436096191 = 1.4243525266647339 + 2.0 * 6.404996395111084
Epoch 270, val loss: 1.499876618385315
Epoch 280, training loss: 14.184289932250977 = 1.3847293853759766 + 2.0 * 6.3997802734375
Epoch 280, val loss: 1.4688458442687988
Epoch 290, training loss: 14.129337310791016 = 1.3444582223892212 + 2.0 * 6.392439365386963
Epoch 290, val loss: 1.4379302263259888
Epoch 300, training loss: 14.0757474899292 = 1.3035222291946411 + 2.0 * 6.386112689971924
Epoch 300, val loss: 1.4067832231521606
Epoch 310, training loss: 14.024253845214844 = 1.2619930505752563 + 2.0 * 6.381130218505859
Epoch 310, val loss: 1.3754206895828247
Epoch 320, training loss: 13.979175567626953 = 1.2200509309768677 + 2.0 * 6.3795623779296875
Epoch 320, val loss: 1.343948245048523
Epoch 330, training loss: 13.926132202148438 = 1.1783274412155151 + 2.0 * 6.373902320861816
Epoch 330, val loss: 1.3130615949630737
Epoch 340, training loss: 13.877747535705566 = 1.1369179487228394 + 2.0 * 6.370414733886719
Epoch 340, val loss: 1.2823745012283325
Epoch 350, training loss: 13.827432632446289 = 1.096011757850647 + 2.0 * 6.365710258483887
Epoch 350, val loss: 1.2523202896118164
Epoch 360, training loss: 13.779216766357422 = 1.0556190013885498 + 2.0 * 6.3617987632751465
Epoch 360, val loss: 1.222553014755249
Epoch 370, training loss: 13.739934921264648 = 1.0161945819854736 + 2.0 * 6.361870288848877
Epoch 370, val loss: 1.1937251091003418
Epoch 380, training loss: 13.690007209777832 = 0.9783039689064026 + 2.0 * 6.355851650238037
Epoch 380, val loss: 1.166273593902588
Epoch 390, training loss: 13.662103652954102 = 0.9419455528259277 + 2.0 * 6.360078811645508
Epoch 390, val loss: 1.1400823593139648
Epoch 400, training loss: 13.61104965209961 = 0.9074690937995911 + 2.0 * 6.351790428161621
Epoch 400, val loss: 1.1156002283096313
Epoch 410, training loss: 13.570069313049316 = 0.8746582269668579 + 2.0 * 6.347705364227295
Epoch 410, val loss: 1.0925326347351074
Epoch 420, training loss: 13.53217887878418 = 0.8436320424079895 + 2.0 * 6.344273567199707
Epoch 420, val loss: 1.071244716644287
Epoch 430, training loss: 13.50264835357666 = 0.8143854141235352 + 2.0 * 6.3441314697265625
Epoch 430, val loss: 1.0515820980072021
Epoch 440, training loss: 13.471583366394043 = 0.7869086861610413 + 2.0 * 6.342337131500244
Epoch 440, val loss: 1.0333359241485596
Epoch 450, training loss: 13.44002628326416 = 0.7613379955291748 + 2.0 * 6.339344024658203
Epoch 450, val loss: 1.0171324014663696
Epoch 460, training loss: 13.408418655395508 = 0.7372421026229858 + 2.0 * 6.335588455200195
Epoch 460, val loss: 1.002386450767517
Epoch 470, training loss: 13.381097793579102 = 0.7143688201904297 + 2.0 * 6.333364486694336
Epoch 470, val loss: 0.9887990355491638
Epoch 480, training loss: 13.354670524597168 = 0.6924875378608704 + 2.0 * 6.331091403961182
Epoch 480, val loss: 0.976269543170929
Epoch 490, training loss: 13.35499382019043 = 0.671420156955719 + 2.0 * 6.341786861419678
Epoch 490, val loss: 0.9646741151809692
Epoch 500, training loss: 13.307536125183105 = 0.6511005759239197 + 2.0 * 6.32821798324585
Epoch 500, val loss: 0.9538313150405884
Epoch 510, training loss: 13.286480903625488 = 0.6313136219978333 + 2.0 * 6.3275837898254395
Epoch 510, val loss: 0.9437773823738098
Epoch 520, training loss: 13.260900497436523 = 0.6118285655975342 + 2.0 * 6.324535846710205
Epoch 520, val loss: 0.934145450592041
Epoch 530, training loss: 13.237455368041992 = 0.5925664901733398 + 2.0 * 6.322444438934326
Epoch 530, val loss: 0.9250724911689758
Epoch 540, training loss: 13.213968276977539 = 0.5733436942100525 + 2.0 * 6.3203125
Epoch 540, val loss: 0.9164680242538452
Epoch 550, training loss: 13.192376136779785 = 0.5540158152580261 + 2.0 * 6.319180011749268
Epoch 550, val loss: 0.9080955386161804
Epoch 560, training loss: 13.178701400756836 = 0.5345513820648193 + 2.0 * 6.322074890136719
Epoch 560, val loss: 0.9000253081321716
Epoch 570, training loss: 13.147878646850586 = 0.5150629281997681 + 2.0 * 6.316407680511475
Epoch 570, val loss: 0.8924626708030701
Epoch 580, training loss: 13.12450885772705 = 0.49543631076812744 + 2.0 * 6.314536094665527
Epoch 580, val loss: 0.8851531744003296
Epoch 590, training loss: 13.1030855178833 = 0.4757446050643921 + 2.0 * 6.313670635223389
Epoch 590, val loss: 0.8782435059547424
Epoch 600, training loss: 13.087882995605469 = 0.4560716152191162 + 2.0 * 6.315905570983887
Epoch 600, val loss: 0.8717596530914307
Epoch 610, training loss: 13.060232162475586 = 0.4364358186721802 + 2.0 * 6.311898231506348
Epoch 610, val loss: 0.8656793236732483
Epoch 620, training loss: 13.035082817077637 = 0.41710174083709717 + 2.0 * 6.308990478515625
Epoch 620, val loss: 0.8604231476783752
Epoch 630, training loss: 13.013286590576172 = 0.398000568151474 + 2.0 * 6.307642936706543
Epoch 630, val loss: 0.8556062579154968
Epoch 640, training loss: 13.011786460876465 = 0.3792855739593506 + 2.0 * 6.316250324249268
Epoch 640, val loss: 0.8512596487998962
Epoch 650, training loss: 12.978432655334473 = 0.3612363636493683 + 2.0 * 6.308598041534424
Epoch 650, val loss: 0.8476862907409668
Epoch 660, training loss: 12.950121879577637 = 0.34368985891342163 + 2.0 * 6.303215980529785
Epoch 660, val loss: 0.8446946740150452
Epoch 670, training loss: 12.930015563964844 = 0.3267446756362915 + 2.0 * 6.301635265350342
Epoch 670, val loss: 0.842256486415863
Epoch 680, training loss: 12.913747787475586 = 0.3103674352169037 + 2.0 * 6.301690101623535
Epoch 680, val loss: 0.8402542471885681
Epoch 690, training loss: 12.899681091308594 = 0.2946424186229706 + 2.0 * 6.30251932144165
Epoch 690, val loss: 0.8388664126396179
Epoch 700, training loss: 12.882538795471191 = 0.2796480059623718 + 2.0 * 6.301445484161377
Epoch 700, val loss: 0.8378704786300659
Epoch 710, training loss: 12.859221458435059 = 0.2653104364871979 + 2.0 * 6.296955585479736
Epoch 710, val loss: 0.8377087116241455
Epoch 720, training loss: 12.845789909362793 = 0.2515700161457062 + 2.0 * 6.297110080718994
Epoch 720, val loss: 0.8379438519477844
Epoch 730, training loss: 12.842862129211426 = 0.23844319581985474 + 2.0 * 6.302209377288818
Epoch 730, val loss: 0.8384636044502258
Epoch 740, training loss: 12.824979782104492 = 0.2259882688522339 + 2.0 * 6.299495697021484
Epoch 740, val loss: 0.8393847942352295
Epoch 750, training loss: 12.801630973815918 = 0.21422137320041656 + 2.0 * 6.293704986572266
Epoch 750, val loss: 0.8408940434455872
Epoch 760, training loss: 12.789690017700195 = 0.20304760336875916 + 2.0 * 6.293321132659912
Epoch 760, val loss: 0.8429341912269592
Epoch 770, training loss: 12.776195526123047 = 0.19244134426116943 + 2.0 * 6.291877269744873
Epoch 770, val loss: 0.8451502919197083
Epoch 780, training loss: 12.772920608520508 = 0.1824226826429367 + 2.0 * 6.295248985290527
Epoch 780, val loss: 0.8477357625961304
Epoch 790, training loss: 12.750767707824707 = 0.1729343980550766 + 2.0 * 6.28891658782959
Epoch 790, val loss: 0.850577175617218
Epoch 800, training loss: 12.743931770324707 = 0.16399909555912018 + 2.0 * 6.289966106414795
Epoch 800, val loss: 0.8537964820861816
Epoch 810, training loss: 12.734136581420898 = 0.15560224652290344 + 2.0 * 6.289267063140869
Epoch 810, val loss: 0.8572461009025574
Epoch 820, training loss: 12.729986190795898 = 0.1477142721414566 + 2.0 * 6.291135787963867
Epoch 820, val loss: 0.860953688621521
Epoch 830, training loss: 12.710869789123535 = 0.14030811190605164 + 2.0 * 6.285280704498291
Epoch 830, val loss: 0.8647858500480652
Epoch 840, training loss: 12.702638626098633 = 0.13335618376731873 + 2.0 * 6.284641265869141
Epoch 840, val loss: 0.8690038323402405
Epoch 850, training loss: 12.697178840637207 = 0.12682148814201355 + 2.0 * 6.2851786613464355
Epoch 850, val loss: 0.8731873631477356
Epoch 860, training loss: 12.698460578918457 = 0.1206841915845871 + 2.0 * 6.288887977600098
Epoch 860, val loss: 0.8775268793106079
Epoch 870, training loss: 12.684134483337402 = 0.11492794752120972 + 2.0 * 6.284603118896484
Epoch 870, val loss: 0.8818209767341614
Epoch 880, training loss: 12.67300796508789 = 0.10953330248594284 + 2.0 * 6.281737327575684
Epoch 880, val loss: 0.8864495754241943
Epoch 890, training loss: 12.665900230407715 = 0.10447531193494797 + 2.0 * 6.280712604522705
Epoch 890, val loss: 0.8911511898040771
Epoch 900, training loss: 12.657756805419922 = 0.09969127178192139 + 2.0 * 6.2790327072143555
Epoch 900, val loss: 0.8958467841148376
Epoch 910, training loss: 12.652021408081055 = 0.09517201036214828 + 2.0 * 6.2784247398376465
Epoch 910, val loss: 0.9005853533744812
Epoch 920, training loss: 12.685528755187988 = 0.09089983999729156 + 2.0 * 6.297314643859863
Epoch 920, val loss: 0.9052004218101501
Epoch 930, training loss: 12.6461181640625 = 0.08692687749862671 + 2.0 * 6.279595851898193
Epoch 930, val loss: 0.9101111888885498
Epoch 940, training loss: 12.635237693786621 = 0.08314839750528336 + 2.0 * 6.276044845581055
Epoch 940, val loss: 0.915217936038971
Epoch 950, training loss: 12.630876541137695 = 0.07957351207733154 + 2.0 * 6.275651454925537
Epoch 950, val loss: 0.9200865030288696
Epoch 960, training loss: 12.626019477844238 = 0.07619644701480865 + 2.0 * 6.274911403656006
Epoch 960, val loss: 0.9250745177268982
Epoch 970, training loss: 12.643943786621094 = 0.07298314571380615 + 2.0 * 6.285480499267578
Epoch 970, val loss: 0.9298056364059448
Epoch 980, training loss: 12.61934757232666 = 0.06997019797563553 + 2.0 * 6.274688720703125
Epoch 980, val loss: 0.9349244236946106
Epoch 990, training loss: 12.612663269042969 = 0.06710240989923477 + 2.0 * 6.272780418395996
Epoch 990, val loss: 0.9398899078369141
Epoch 1000, training loss: 12.608872413635254 = 0.06438945233821869 + 2.0 * 6.272241592407227
Epoch 1000, val loss: 0.944844663143158
Epoch 1010, training loss: 12.625195503234863 = 0.06181642785668373 + 2.0 * 6.281689643859863
Epoch 1010, val loss: 0.9496477842330933
Epoch 1020, training loss: 12.601651191711426 = 0.05938665568828583 + 2.0 * 6.271132469177246
Epoch 1020, val loss: 0.9544501304626465
Epoch 1030, training loss: 12.596983909606934 = 0.05708480253815651 + 2.0 * 6.269949436187744
Epoch 1030, val loss: 0.9594581723213196
Epoch 1040, training loss: 12.596161842346191 = 0.05490092560648918 + 2.0 * 6.270630359649658
Epoch 1040, val loss: 0.9644995927810669
Epoch 1050, training loss: 12.60208797454834 = 0.0528331883251667 + 2.0 * 6.274627208709717
Epoch 1050, val loss: 0.9693308472633362
Epoch 1060, training loss: 12.585789680480957 = 0.05085518956184387 + 2.0 * 6.267467021942139
Epoch 1060, val loss: 0.9739031791687012
Epoch 1070, training loss: 12.585116386413574 = 0.048986032605171204 + 2.0 * 6.268064975738525
Epoch 1070, val loss: 0.9789292812347412
Epoch 1080, training loss: 12.579915046691895 = 0.04721272736787796 + 2.0 * 6.266351222991943
Epoch 1080, val loss: 0.9837728142738342
Epoch 1090, training loss: 12.578987121582031 = 0.04552312195301056 + 2.0 * 6.266732215881348
Epoch 1090, val loss: 0.9886074662208557
Epoch 1100, training loss: 12.60061264038086 = 0.04391178488731384 + 2.0 * 6.278350353240967
Epoch 1100, val loss: 0.993202805519104
Epoch 1110, training loss: 12.578662872314453 = 0.042402204126119614 + 2.0 * 6.268130302429199
Epoch 1110, val loss: 0.9981685876846313
Epoch 1120, training loss: 12.569992065429688 = 0.04094872251152992 + 2.0 * 6.264521598815918
Epoch 1120, val loss: 1.0029178857803345
Epoch 1130, training loss: 12.565978050231934 = 0.03957413509488106 + 2.0 * 6.26320219039917
Epoch 1130, val loss: 1.00778067111969
Epoch 1140, training loss: 12.564262390136719 = 0.03826060891151428 + 2.0 * 6.263000965118408
Epoch 1140, val loss: 1.0124897956848145
Epoch 1150, training loss: 12.59633731842041 = 0.03701220825314522 + 2.0 * 6.279662609100342
Epoch 1150, val loss: 1.0170713663101196
Epoch 1160, training loss: 12.560968399047852 = 0.03581422567367554 + 2.0 * 6.262577056884766
Epoch 1160, val loss: 1.0214956998825073
Epoch 1170, training loss: 12.558014869689941 = 0.03468725085258484 + 2.0 * 6.261663913726807
Epoch 1170, val loss: 1.026310920715332
Epoch 1180, training loss: 12.55566692352295 = 0.03360878676176071 + 2.0 * 6.261029243469238
Epoch 1180, val loss: 1.0309324264526367
Epoch 1190, training loss: 12.5683012008667 = 0.03258480131626129 + 2.0 * 6.267858028411865
Epoch 1190, val loss: 1.0355839729309082
Epoch 1200, training loss: 12.554909706115723 = 0.031590625643730164 + 2.0 * 6.261659622192383
Epoch 1200, val loss: 1.039608120918274
Epoch 1210, training loss: 12.552224159240723 = 0.030657008290290833 + 2.0 * 6.260783672332764
Epoch 1210, val loss: 1.0442867279052734
Epoch 1220, training loss: 12.546957969665527 = 0.029758086428046227 + 2.0 * 6.258599758148193
Epoch 1220, val loss: 1.0487788915634155
Epoch 1230, training loss: 12.548307418823242 = 0.0289000291377306 + 2.0 * 6.259703636169434
Epoch 1230, val loss: 1.0531517267227173
Epoch 1240, training loss: 12.547497749328613 = 0.028078380972146988 + 2.0 * 6.25970983505249
Epoch 1240, val loss: 1.0574145317077637
Epoch 1250, training loss: 12.541935920715332 = 0.02729201130568981 + 2.0 * 6.257321834564209
Epoch 1250, val loss: 1.0618274211883545
Epoch 1260, training loss: 12.539571762084961 = 0.02653764933347702 + 2.0 * 6.256516933441162
Epoch 1260, val loss: 1.0661953687667847
Epoch 1270, training loss: 12.541584014892578 = 0.025814903900027275 + 2.0 * 6.257884502410889
Epoch 1270, val loss: 1.0705103874206543
Epoch 1280, training loss: 12.552811622619629 = 0.025124112144112587 + 2.0 * 6.263843536376953
Epoch 1280, val loss: 1.0745961666107178
Epoch 1290, training loss: 12.538433074951172 = 0.024449961259961128 + 2.0 * 6.256991386413574
Epoch 1290, val loss: 1.0784939527511597
Epoch 1300, training loss: 12.534113883972168 = 0.02381214313209057 + 2.0 * 6.25515079498291
Epoch 1300, val loss: 1.0828278064727783
Epoch 1310, training loss: 12.531930923461914 = 0.02319966070353985 + 2.0 * 6.25436544418335
Epoch 1310, val loss: 1.0869183540344238
Epoch 1320, training loss: 12.534828186035156 = 0.022610798478126526 + 2.0 * 6.25610876083374
Epoch 1320, val loss: 1.0910595655441284
Epoch 1330, training loss: 12.537571907043457 = 0.022045480087399483 + 2.0 * 6.257763385772705
Epoch 1330, val loss: 1.0948913097381592
Epoch 1340, training loss: 12.535618782043457 = 0.021496614441275597 + 2.0 * 6.257061004638672
Epoch 1340, val loss: 1.0988101959228516
Epoch 1350, training loss: 12.52734088897705 = 0.020971130579710007 + 2.0 * 6.253184795379639
Epoch 1350, val loss: 1.1026010513305664
Epoch 1360, training loss: 12.527623176574707 = 0.020468678325414658 + 2.0 * 6.25357723236084
Epoch 1360, val loss: 1.1066694259643555
Epoch 1370, training loss: 12.527771949768066 = 0.01998186856508255 + 2.0 * 6.253894805908203
Epoch 1370, val loss: 1.1103971004486084
Epoch 1380, training loss: 12.53223705291748 = 0.019513005390763283 + 2.0 * 6.256361961364746
Epoch 1380, val loss: 1.114091396331787
Epoch 1390, training loss: 12.527174949645996 = 0.019063478335738182 + 2.0 * 6.254055500030518
Epoch 1390, val loss: 1.1179018020629883
Epoch 1400, training loss: 12.522940635681152 = 0.018629007041454315 + 2.0 * 6.252155780792236
Epoch 1400, val loss: 1.121709942817688
Epoch 1410, training loss: 12.520674705505371 = 0.018209243193268776 + 2.0 * 6.251232624053955
Epoch 1410, val loss: 1.1254181861877441
Epoch 1420, training loss: 12.530251502990723 = 0.017806323245167732 + 2.0 * 6.256222724914551
Epoch 1420, val loss: 1.1291240453720093
Epoch 1430, training loss: 12.524246215820312 = 0.017414813861250877 + 2.0 * 6.253415584564209
Epoch 1430, val loss: 1.1325759887695312
Epoch 1440, training loss: 12.518274307250977 = 0.01703607104718685 + 2.0 * 6.250618934631348
Epoch 1440, val loss: 1.1361583471298218
Epoch 1450, training loss: 12.522029876708984 = 0.016674304381012917 + 2.0 * 6.252677917480469
Epoch 1450, val loss: 1.139826774597168
Epoch 1460, training loss: 12.523399353027344 = 0.016323046758770943 + 2.0 * 6.253538131713867
Epoch 1460, val loss: 1.1432863473892212
Epoch 1470, training loss: 12.515068054199219 = 0.015979034826159477 + 2.0 * 6.249544620513916
Epoch 1470, val loss: 1.1465835571289062
Epoch 1480, training loss: 12.51251220703125 = 0.015649743378162384 + 2.0 * 6.248431205749512
Epoch 1480, val loss: 1.1500898599624634
Epoch 1490, training loss: 12.511199951171875 = 0.01533088181167841 + 2.0 * 6.247934341430664
Epoch 1490, val loss: 1.1535123586654663
Epoch 1500, training loss: 12.517440795898438 = 0.015021851286292076 + 2.0 * 6.251209259033203
Epoch 1500, val loss: 1.1567931175231934
Epoch 1510, training loss: 12.511547088623047 = 0.014723584055900574 + 2.0 * 6.248411655426025
Epoch 1510, val loss: 1.1602308750152588
Epoch 1520, training loss: 12.508708000183105 = 0.014433881267905235 + 2.0 * 6.247137069702148
Epoch 1520, val loss: 1.1636165380477905
Epoch 1530, training loss: 12.54244613647461 = 0.014159120619297028 + 2.0 * 6.264143466949463
Epoch 1530, val loss: 1.1669738292694092
Epoch 1540, training loss: 12.51696491241455 = 0.013880598358809948 + 2.0 * 6.251542091369629
Epoch 1540, val loss: 1.1696346998214722
Epoch 1550, training loss: 12.508718490600586 = 0.013621771708130836 + 2.0 * 6.247548580169678
Epoch 1550, val loss: 1.173103928565979
Epoch 1560, training loss: 12.503744125366211 = 0.01336606778204441 + 2.0 * 6.245189189910889
Epoch 1560, val loss: 1.1762175559997559
Epoch 1570, training loss: 12.5017671585083 = 0.013119781389832497 + 2.0 * 6.24432373046875
Epoch 1570, val loss: 1.1794395446777344
Epoch 1580, training loss: 12.501593589782715 = 0.012878473848104477 + 2.0 * 6.244357585906982
Epoch 1580, val loss: 1.182573914527893
Epoch 1590, training loss: 12.523622512817383 = 0.012643981724977493 + 2.0 * 6.255489349365234
Epoch 1590, val loss: 1.1855392456054688
Epoch 1600, training loss: 12.508337020874023 = 0.012414404191076756 + 2.0 * 6.247961521148682
Epoch 1600, val loss: 1.188500165939331
Epoch 1610, training loss: 12.501999855041504 = 0.012192866764962673 + 2.0 * 6.244903564453125
Epoch 1610, val loss: 1.1916582584381104
Epoch 1620, training loss: 12.50869369506836 = 0.011978481896221638 + 2.0 * 6.248357772827148
Epoch 1620, val loss: 1.1946046352386475
Epoch 1630, training loss: 12.50465202331543 = 0.011768653988838196 + 2.0 * 6.246441841125488
Epoch 1630, val loss: 1.19741690158844
Epoch 1640, training loss: 12.503049850463867 = 0.011565824039280415 + 2.0 * 6.245741844177246
Epoch 1640, val loss: 1.2003874778747559
Epoch 1650, training loss: 12.498537063598633 = 0.01136995479464531 + 2.0 * 6.243583679199219
Epoch 1650, val loss: 1.2034127712249756
Epoch 1660, training loss: 12.496159553527832 = 0.011178744025528431 + 2.0 * 6.242490291595459
Epoch 1660, val loss: 1.2063370943069458
Epoch 1670, training loss: 12.496321678161621 = 0.01099190954118967 + 2.0 * 6.242664813995361
Epoch 1670, val loss: 1.209226369857788
Epoch 1680, training loss: 12.51903247833252 = 0.010813749395310879 + 2.0 * 6.2541093826293945
Epoch 1680, val loss: 1.2120895385742188
Epoch 1690, training loss: 12.497686386108398 = 0.010631609708070755 + 2.0 * 6.243527412414551
Epoch 1690, val loss: 1.2145365476608276
Epoch 1700, training loss: 12.492962837219238 = 0.010460923425853252 + 2.0 * 6.241250991821289
Epoch 1700, val loss: 1.2175229787826538
Epoch 1710, training loss: 12.491057395935059 = 0.010293480008840561 + 2.0 * 6.240382194519043
Epoch 1710, val loss: 1.2202547788619995
Epoch 1720, training loss: 12.496577262878418 = 0.010129809379577637 + 2.0 * 6.243223667144775
Epoch 1720, val loss: 1.2229053974151611
Epoch 1730, training loss: 12.491849899291992 = 0.009970508515834808 + 2.0 * 6.240939617156982
Epoch 1730, val loss: 1.2256048917770386
Epoch 1740, training loss: 12.49109172821045 = 0.009815280325710773 + 2.0 * 6.240638256072998
Epoch 1740, val loss: 1.2283703088760376
Epoch 1750, training loss: 12.496929168701172 = 0.009663617238402367 + 2.0 * 6.243632793426514
Epoch 1750, val loss: 1.2310549020767212
Epoch 1760, training loss: 12.49406909942627 = 0.009514258243143559 + 2.0 * 6.2422776222229
Epoch 1760, val loss: 1.2334377765655518
Epoch 1770, training loss: 12.489630699157715 = 0.009370279498398304 + 2.0 * 6.240130424499512
Epoch 1770, val loss: 1.2361525297164917
Epoch 1780, training loss: 12.488289833068848 = 0.009230279363691807 + 2.0 * 6.239529609680176
Epoch 1780, val loss: 1.2387155294418335
Epoch 1790, training loss: 12.490435600280762 = 0.009093336760997772 + 2.0 * 6.240671157836914
Epoch 1790, val loss: 1.2412395477294922
Epoch 1800, training loss: 12.486686706542969 = 0.00895888451486826 + 2.0 * 6.238863945007324
Epoch 1800, val loss: 1.2437506914138794
Epoch 1810, training loss: 12.491374969482422 = 0.008829537779092789 + 2.0 * 6.241272926330566
Epoch 1810, val loss: 1.2463663816452026
Epoch 1820, training loss: 12.50826644897461 = 0.008704127743840218 + 2.0 * 6.249781131744385
Epoch 1820, val loss: 1.2486371994018555
Epoch 1830, training loss: 12.491156578063965 = 0.008576680906116962 + 2.0 * 6.241290092468262
Epoch 1830, val loss: 1.2507742643356323
Epoch 1840, training loss: 12.484594345092773 = 0.008457480929791927 + 2.0 * 6.238068580627441
Epoch 1840, val loss: 1.2533804178237915
Epoch 1850, training loss: 12.482163429260254 = 0.008339212276041508 + 2.0 * 6.236912250518799
Epoch 1850, val loss: 1.2557339668273926
Epoch 1860, training loss: 12.481287002563477 = 0.008224486373364925 + 2.0 * 6.2365312576293945
Epoch 1860, val loss: 1.258137822151184
Epoch 1870, training loss: 12.482722282409668 = 0.00811148714274168 + 2.0 * 6.237305164337158
Epoch 1870, val loss: 1.2604776620864868
Epoch 1880, training loss: 12.497021675109863 = 0.00800201017409563 + 2.0 * 6.244509696960449
Epoch 1880, val loss: 1.262717366218567
Epoch 1890, training loss: 12.484954833984375 = 0.007893303409218788 + 2.0 * 6.23853063583374
Epoch 1890, val loss: 1.2649129629135132
Epoch 1900, training loss: 12.482318878173828 = 0.0077874804846942425 + 2.0 * 6.237265586853027
Epoch 1900, val loss: 1.2671525478363037
Epoch 1910, training loss: 12.478812217712402 = 0.007685585413128138 + 2.0 * 6.235563278198242
Epoch 1910, val loss: 1.2694051265716553
Epoch 1920, training loss: 12.482437133789062 = 0.007585449609905481 + 2.0 * 6.237425804138184
Epoch 1920, val loss: 1.2715585231781006
Epoch 1930, training loss: 12.493706703186035 = 0.007487520109862089 + 2.0 * 6.243109703063965
Epoch 1930, val loss: 1.273704171180725
Epoch 1940, training loss: 12.483245849609375 = 0.007392607629299164 + 2.0 * 6.237926483154297
Epoch 1940, val loss: 1.2758338451385498
Epoch 1950, training loss: 12.478992462158203 = 0.007297906093299389 + 2.0 * 6.235847473144531
Epoch 1950, val loss: 1.2780075073242188
Epoch 1960, training loss: 12.476644515991211 = 0.007207400165498257 + 2.0 * 6.234718322753906
Epoch 1960, val loss: 1.2802553176879883
Epoch 1970, training loss: 12.476788520812988 = 0.007117698900401592 + 2.0 * 6.234835624694824
Epoch 1970, val loss: 1.2823742628097534
Epoch 1980, training loss: 12.490335464477539 = 0.007031135726720095 + 2.0 * 6.241652011871338
Epoch 1980, val loss: 1.2845040559768677
Epoch 1990, training loss: 12.48197078704834 = 0.006944247987121344 + 2.0 * 6.237513065338135
Epoch 1990, val loss: 1.2863671779632568
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7555555555555555
0.8218239325250396
=== training gcn model ===
Epoch 0, training loss: 19.143884658813477 = 1.950121283531189 + 2.0 * 8.596881866455078
Epoch 0, val loss: 1.9513976573944092
Epoch 10, training loss: 19.133792877197266 = 1.9402967691421509 + 2.0 * 8.596748352050781
Epoch 10, val loss: 1.9413025379180908
Epoch 20, training loss: 19.11940574645996 = 1.9283126592636108 + 2.0 * 8.59554672241211
Epoch 20, val loss: 1.9290118217468262
Epoch 30, training loss: 19.080533981323242 = 1.9118598699569702 + 2.0 * 8.58433723449707
Epoch 30, val loss: 1.912217617034912
Epoch 40, training loss: 18.917116165161133 = 1.8894703388214111 + 2.0 * 8.513822555541992
Epoch 40, val loss: 1.8902069330215454
Epoch 50, training loss: 18.14365005493164 = 1.8648756742477417 + 2.0 * 8.139387130737305
Epoch 50, val loss: 1.8669461011886597
Epoch 60, training loss: 17.641054153442383 = 1.8406718969345093 + 2.0 * 7.900191307067871
Epoch 60, val loss: 1.845295786857605
Epoch 70, training loss: 16.841304779052734 = 1.822133183479309 + 2.0 * 7.509585380554199
Epoch 70, val loss: 1.8286677598953247
Epoch 80, training loss: 16.095897674560547 = 1.8080058097839355 + 2.0 * 7.143945693969727
Epoch 80, val loss: 1.8152638673782349
Epoch 90, training loss: 15.666563987731934 = 1.795331597328186 + 2.0 * 6.9356160163879395
Epoch 90, val loss: 1.8026385307312012
Epoch 100, training loss: 15.425347328186035 = 1.7800862789154053 + 2.0 * 6.822630405426025
Epoch 100, val loss: 1.7885667085647583
Epoch 110, training loss: 15.276259422302246 = 1.7628413438796997 + 2.0 * 6.756709098815918
Epoch 110, val loss: 1.773662805557251
Epoch 120, training loss: 15.144285202026367 = 1.7451605796813965 + 2.0 * 6.699562072753906
Epoch 120, val loss: 1.7585904598236084
Epoch 130, training loss: 15.031810760498047 = 1.7271528244018555 + 2.0 * 6.652328968048096
Epoch 130, val loss: 1.7429673671722412
Epoch 140, training loss: 14.938743591308594 = 1.7077516317367554 + 2.0 * 6.6154961585998535
Epoch 140, val loss: 1.7261583805084229
Epoch 150, training loss: 14.84946346282959 = 1.6861684322357178 + 2.0 * 6.5816473960876465
Epoch 150, val loss: 1.7079520225524902
Epoch 160, training loss: 14.77114486694336 = 1.6622698307037354 + 2.0 * 6.554437637329102
Epoch 160, val loss: 1.6880284547805786
Epoch 170, training loss: 14.70020866394043 = 1.6359381675720215 + 2.0 * 6.532135486602783
Epoch 170, val loss: 1.666161060333252
Epoch 180, training loss: 14.635360717773438 = 1.6071609258651733 + 2.0 * 6.514100074768066
Epoch 180, val loss: 1.6424074172973633
Epoch 190, training loss: 14.574260711669922 = 1.5757580995559692 + 2.0 * 6.499251365661621
Epoch 190, val loss: 1.616392970085144
Epoch 200, training loss: 14.517891883850098 = 1.5417406558990479 + 2.0 * 6.4880757331848145
Epoch 200, val loss: 1.588356852531433
Epoch 210, training loss: 14.459267616271973 = 1.5056153535842896 + 2.0 * 6.476826190948486
Epoch 210, val loss: 1.5586724281311035
Epoch 220, training loss: 14.402741432189941 = 1.4678064584732056 + 2.0 * 6.467467308044434
Epoch 220, val loss: 1.527835726737976
Epoch 230, training loss: 14.349270820617676 = 1.4286538362503052 + 2.0 * 6.46030855178833
Epoch 230, val loss: 1.4963234663009644
Epoch 240, training loss: 14.294280052185059 = 1.3888580799102783 + 2.0 * 6.45271110534668
Epoch 240, val loss: 1.4647685289382935
Epoch 250, training loss: 14.238594055175781 = 1.3488210439682007 + 2.0 * 6.444886684417725
Epoch 250, val loss: 1.4332184791564941
Epoch 260, training loss: 14.18465805053711 = 1.308617115020752 + 2.0 * 6.438020706176758
Epoch 260, val loss: 1.4021495580673218
Epoch 270, training loss: 14.134004592895508 = 1.2686961889266968 + 2.0 * 6.43265438079834
Epoch 270, val loss: 1.3715757131576538
Epoch 280, training loss: 14.0869140625 = 1.229308009147644 + 2.0 * 6.428802967071533
Epoch 280, val loss: 1.3420714139938354
Epoch 290, training loss: 14.02933120727539 = 1.1904189586639404 + 2.0 * 6.4194560050964355
Epoch 290, val loss: 1.3134608268737793
Epoch 300, training loss: 13.978574752807617 = 1.1517515182495117 + 2.0 * 6.413411617279053
Epoch 300, val loss: 1.2853658199310303
Epoch 310, training loss: 13.928141593933105 = 1.1130844354629517 + 2.0 * 6.407528400421143
Epoch 310, val loss: 1.2576099634170532
Epoch 320, training loss: 13.885235786437988 = 1.0745418071746826 + 2.0 * 6.405346870422363
Epoch 320, val loss: 1.2299309968948364
Epoch 330, training loss: 13.832232475280762 = 1.0364543199539185 + 2.0 * 6.397889137268066
Epoch 330, val loss: 1.2029874324798584
Epoch 340, training loss: 13.785775184631348 = 0.9987927079200745 + 2.0 * 6.393491268157959
Epoch 340, val loss: 1.176641583442688
Epoch 350, training loss: 13.738639831542969 = 0.9614111185073853 + 2.0 * 6.388614177703857
Epoch 350, val loss: 1.1508445739746094
Epoch 360, training loss: 13.703598022460938 = 0.9245116114616394 + 2.0 * 6.389543056488037
Epoch 360, val loss: 1.1256135702133179
Epoch 370, training loss: 13.649022102355957 = 0.8885437250137329 + 2.0 * 6.380239009857178
Epoch 370, val loss: 1.10135018825531
Epoch 380, training loss: 13.605117797851562 = 0.8534711599349976 + 2.0 * 6.375823497772217
Epoch 380, val loss: 1.0782698392868042
Epoch 390, training loss: 13.563483238220215 = 0.8192238211631775 + 2.0 * 6.372129917144775
Epoch 390, val loss: 1.0562684535980225
Epoch 400, training loss: 13.531989097595215 = 0.7860399484634399 + 2.0 * 6.372974395751953
Epoch 400, val loss: 1.0354021787643433
Epoch 410, training loss: 13.488008499145508 = 0.7542107105255127 + 2.0 * 6.366899013519287
Epoch 410, val loss: 1.016154170036316
Epoch 420, training loss: 13.447187423706055 = 0.7236689329147339 + 2.0 * 6.361759185791016
Epoch 420, val loss: 0.998367190361023
Epoch 430, training loss: 13.426547050476074 = 0.6944684982299805 + 2.0 * 6.366039276123047
Epoch 430, val loss: 0.981895387172699
Epoch 440, training loss: 13.378862380981445 = 0.6667472124099731 + 2.0 * 6.356057643890381
Epoch 440, val loss: 0.9671927094459534
Epoch 450, training loss: 13.352974891662598 = 0.6403859853744507 + 2.0 * 6.356294631958008
Epoch 450, val loss: 0.9540807604789734
Epoch 460, training loss: 13.318000793457031 = 0.6153576970100403 + 2.0 * 6.351321697235107
Epoch 460, val loss: 0.9423100352287292
Epoch 470, training loss: 13.29124927520752 = 0.5914823412895203 + 2.0 * 6.349883556365967
Epoch 470, val loss: 0.9318679571151733
Epoch 480, training loss: 13.265227317810059 = 0.5687353610992432 + 2.0 * 6.348246097564697
Epoch 480, val loss: 0.9227780699729919
Epoch 490, training loss: 13.234797477722168 = 0.5469328761100769 + 2.0 * 6.343932151794434
Epoch 490, val loss: 0.9147108793258667
Epoch 500, training loss: 13.212118148803711 = 0.5260534286499023 + 2.0 * 6.343032360076904
Epoch 500, val loss: 0.9077134132385254
Epoch 510, training loss: 13.18342113494873 = 0.5059705972671509 + 2.0 * 6.3387250900268555
Epoch 510, val loss: 0.9015057682991028
Epoch 520, training loss: 13.161964416503906 = 0.4865323305130005 + 2.0 * 6.337716102600098
Epoch 520, val loss: 0.896228015422821
Epoch 530, training loss: 13.14638614654541 = 0.46768930554389954 + 2.0 * 6.339348316192627
Epoch 530, val loss: 0.8914781212806702
Epoch 540, training loss: 13.117855072021484 = 0.44935160875320435 + 2.0 * 6.334251880645752
Epoch 540, val loss: 0.8874075412750244
Epoch 550, training loss: 13.095712661743164 = 0.4314693212509155 + 2.0 * 6.332121849060059
Epoch 550, val loss: 0.8838397264480591
Epoch 560, training loss: 13.073521614074707 = 0.41391050815582275 + 2.0 * 6.329805374145508
Epoch 560, val loss: 0.8806681632995605
Epoch 570, training loss: 13.060558319091797 = 0.39661136269569397 + 2.0 * 6.331973552703857
Epoch 570, val loss: 0.8779519200325012
Epoch 580, training loss: 13.03250789642334 = 0.37953320145606995 + 2.0 * 6.3264875411987305
Epoch 580, val loss: 0.8754430413246155
Epoch 590, training loss: 13.010793685913086 = 0.3626375198364258 + 2.0 * 6.32407808303833
Epoch 590, val loss: 0.8733105659484863
Epoch 600, training loss: 12.99072551727295 = 0.3458736538887024 + 2.0 * 6.322425842285156
Epoch 600, val loss: 0.8715563416481018
Epoch 610, training loss: 12.986061096191406 = 0.32923394441604614 + 2.0 * 6.328413486480713
Epoch 610, val loss: 0.8700591325759888
Epoch 620, training loss: 12.952936172485352 = 0.3128449022769928 + 2.0 * 6.320045471191406
Epoch 620, val loss: 0.8689588904380798
Epoch 630, training loss: 12.937091827392578 = 0.2967860698699951 + 2.0 * 6.320152759552002
Epoch 630, val loss: 0.8684236407279968
Epoch 640, training loss: 12.917283058166504 = 0.2811550199985504 + 2.0 * 6.318064212799072
Epoch 640, val loss: 0.8683302998542786
Epoch 650, training loss: 12.904372215270996 = 0.266042560338974 + 2.0 * 6.319164752960205
Epoch 650, val loss: 0.8689380288124084
Epoch 660, training loss: 12.882710456848145 = 0.25153449177742004 + 2.0 * 6.315587997436523
Epoch 660, val loss: 0.8703105449676514
Epoch 670, training loss: 12.868593215942383 = 0.23771101236343384 + 2.0 * 6.315441131591797
Epoch 670, val loss: 0.8721247911453247
Epoch 680, training loss: 12.85805606842041 = 0.22462937235832214 + 2.0 * 6.316713333129883
Epoch 680, val loss: 0.8742654323577881
Epoch 690, training loss: 12.834830284118652 = 0.21234850585460663 + 2.0 * 6.3112406730651855
Epoch 690, val loss: 0.8772736191749573
Epoch 700, training loss: 12.820229530334473 = 0.2008465826511383 + 2.0 * 6.309691429138184
Epoch 700, val loss: 0.8807145953178406
Epoch 710, training loss: 12.819570541381836 = 0.19007624685764313 + 2.0 * 6.314747333526611
Epoch 710, val loss: 0.8844327330589294
Epoch 720, training loss: 12.800798416137695 = 0.18000808358192444 + 2.0 * 6.310395240783691
Epoch 720, val loss: 0.888577938079834
Epoch 730, training loss: 12.790977478027344 = 0.1706201285123825 + 2.0 * 6.310178756713867
Epoch 730, val loss: 0.8933212161064148
Epoch 740, training loss: 12.770855903625488 = 0.16182754933834076 + 2.0 * 6.304514408111572
Epoch 740, val loss: 0.8981143832206726
Epoch 750, training loss: 12.759904861450195 = 0.15359678864479065 + 2.0 * 6.303153991699219
Epoch 750, val loss: 0.9034138321876526
Epoch 760, training loss: 12.780847549438477 = 0.14588026702404022 + 2.0 * 6.317483425140381
Epoch 760, val loss: 0.9087651968002319
Epoch 770, training loss: 12.743317604064941 = 0.13871215283870697 + 2.0 * 6.302302837371826
Epoch 770, val loss: 0.9145509004592896
Epoch 780, training loss: 12.732587814331055 = 0.13198967278003693 + 2.0 * 6.300299167633057
Epoch 780, val loss: 0.9206199645996094
Epoch 790, training loss: 12.723885536193848 = 0.12567824125289917 + 2.0 * 6.299103736877441
Epoch 790, val loss: 0.926669180393219
Epoch 800, training loss: 12.745442390441895 = 0.11974713951349258 + 2.0 * 6.31284761428833
Epoch 800, val loss: 0.9328616857528687
Epoch 810, training loss: 12.710782051086426 = 0.11418856680393219 + 2.0 * 6.298296928405762
Epoch 810, val loss: 0.9394590258598328
Epoch 820, training loss: 12.70231819152832 = 0.10896420478820801 + 2.0 * 6.296677112579346
Epoch 820, val loss: 0.9461449980735779
Epoch 830, training loss: 12.693382263183594 = 0.1040443629026413 + 2.0 * 6.294669151306152
Epoch 830, val loss: 0.9527915716171265
Epoch 840, training loss: 12.689573287963867 = 0.09939146786928177 + 2.0 * 6.295090675354004
Epoch 840, val loss: 0.9596079587936401
Epoch 850, training loss: 12.683772087097168 = 0.09501634538173676 + 2.0 * 6.29437780380249
Epoch 850, val loss: 0.9663827419281006
Epoch 860, training loss: 12.67855453491211 = 0.0908970907330513 + 2.0 * 6.29382848739624
Epoch 860, val loss: 0.9733549952507019
Epoch 870, training loss: 12.67044734954834 = 0.0870172455906868 + 2.0 * 6.291715145111084
Epoch 870, val loss: 0.9803716540336609
Epoch 880, training loss: 12.682973861694336 = 0.08334758877754211 + 2.0 * 6.299813270568848
Epoch 880, val loss: 0.9872089624404907
Epoch 890, training loss: 12.658171653747559 = 0.07989711314439774 + 2.0 * 6.289137363433838
Epoch 890, val loss: 0.9943928122520447
Epoch 900, training loss: 12.654145240783691 = 0.07663071155548096 + 2.0 * 6.28875732421875
Epoch 900, val loss: 1.001511573791504
Epoch 910, training loss: 12.656436920166016 = 0.07353240251541138 + 2.0 * 6.291452407836914
Epoch 910, val loss: 1.0086026191711426
Epoch 920, training loss: 12.645197868347168 = 0.07059969753026962 + 2.0 * 6.287299156188965
Epoch 920, val loss: 1.0153768062591553
Epoch 930, training loss: 12.643472671508789 = 0.06782077252864838 + 2.0 * 6.287826061248779
Epoch 930, val loss: 1.0227155685424805
Epoch 940, training loss: 12.636395454406738 = 0.06518547981977463 + 2.0 * 6.285604953765869
Epoch 940, val loss: 1.029711127281189
Epoch 950, training loss: 12.64175033569336 = 0.06268172711133957 + 2.0 * 6.289534091949463
Epoch 950, val loss: 1.036589503288269
Epoch 960, training loss: 12.629048347473145 = 0.06030267849564552 + 2.0 * 6.284372806549072
Epoch 960, val loss: 1.0438919067382812
Epoch 970, training loss: 12.624062538146973 = 0.058046139776706696 + 2.0 * 6.283008098602295
Epoch 970, val loss: 1.0509672164916992
Epoch 980, training loss: 12.619357109069824 = 0.05589642748236656 + 2.0 * 6.2817301750183105
Epoch 980, val loss: 1.0580354928970337
Epoch 990, training loss: 12.645242691040039 = 0.0538557730615139 + 2.0 * 6.295693397521973
Epoch 990, val loss: 1.064923644065857
Epoch 1000, training loss: 12.61418342590332 = 0.05190235376358032 + 2.0 * 6.281140327453613
Epoch 1000, val loss: 1.0718828439712524
Epoch 1010, training loss: 12.611425399780273 = 0.050058528780937195 + 2.0 * 6.280683517456055
Epoch 1010, val loss: 1.0788233280181885
Epoch 1020, training loss: 12.607401847839355 = 0.048304442316293716 + 2.0 * 6.279548645019531
Epoch 1020, val loss: 1.085809588432312
Epoch 1030, training loss: 12.61707878112793 = 0.046631332486867905 + 2.0 * 6.285223960876465
Epoch 1030, val loss: 1.0925318002700806
Epoch 1040, training loss: 12.599930763244629 = 0.04503505676984787 + 2.0 * 6.277447700500488
Epoch 1040, val loss: 1.0994197130203247
Epoch 1050, training loss: 12.59675121307373 = 0.04351801797747612 + 2.0 * 6.27661657333374
Epoch 1050, val loss: 1.1061077117919922
Epoch 1060, training loss: 12.59682559967041 = 0.042069047689437866 + 2.0 * 6.277378082275391
Epoch 1060, val loss: 1.1127533912658691
Epoch 1070, training loss: 12.60167121887207 = 0.04068593308329582 + 2.0 * 6.280492782592773
Epoch 1070, val loss: 1.1192641258239746
Epoch 1080, training loss: 12.597527503967285 = 0.039364252239465714 + 2.0 * 6.27908182144165
Epoch 1080, val loss: 1.1260744333267212
Epoch 1090, training loss: 12.591235160827637 = 0.03810591995716095 + 2.0 * 6.276564598083496
Epoch 1090, val loss: 1.1323193311691284
Epoch 1100, training loss: 12.589485168457031 = 0.03690472990274429 + 2.0 * 6.276290416717529
Epoch 1100, val loss: 1.1388282775878906
Epoch 1110, training loss: 12.584539413452148 = 0.0357578806579113 + 2.0 * 6.274390697479248
Epoch 1110, val loss: 1.1451693773269653
Epoch 1120, training loss: 12.58487606048584 = 0.034659966826438904 + 2.0 * 6.2751078605651855
Epoch 1120, val loss: 1.1515496969223022
Epoch 1130, training loss: 12.58096981048584 = 0.033607859164476395 + 2.0 * 6.273681163787842
Epoch 1130, val loss: 1.1577523946762085
Epoch 1140, training loss: 12.581293106079102 = 0.03260326012969017 + 2.0 * 6.2743449211120605
Epoch 1140, val loss: 1.1638015508651733
Epoch 1150, training loss: 12.575129508972168 = 0.03164017200469971 + 2.0 * 6.271744728088379
Epoch 1150, val loss: 1.1700102090835571
Epoch 1160, training loss: 12.575814247131348 = 0.030720768496394157 + 2.0 * 6.272546768188477
Epoch 1160, val loss: 1.1761200428009033
Epoch 1170, training loss: 12.575896263122559 = 0.029836615547537804 + 2.0 * 6.273029804229736
Epoch 1170, val loss: 1.181852102279663
Epoch 1180, training loss: 12.566089630126953 = 0.028991881757974625 + 2.0 * 6.268548965454102
Epoch 1180, val loss: 1.1878973245620728
Epoch 1190, training loss: 12.565744400024414 = 0.028182294219732285 + 2.0 * 6.2687811851501465
Epoch 1190, val loss: 1.1938596963882446
Epoch 1200, training loss: 12.58244800567627 = 0.02740565687417984 + 2.0 * 6.277521133422852
Epoch 1200, val loss: 1.199568271636963
Epoch 1210, training loss: 12.568259239196777 = 0.026661112904548645 + 2.0 * 6.270799160003662
Epoch 1210, val loss: 1.2050379514694214
Epoch 1220, training loss: 12.561247825622559 = 0.025945650413632393 + 2.0 * 6.267651081085205
Epoch 1220, val loss: 1.210925817489624
Epoch 1230, training loss: 12.560798645019531 = 0.02526158280670643 + 2.0 * 6.267768383026123
Epoch 1230, val loss: 1.2163779735565186
Epoch 1240, training loss: 12.5642728805542 = 0.024602020159363747 + 2.0 * 6.269835472106934
Epoch 1240, val loss: 1.2217081785202026
Epoch 1250, training loss: 12.559109687805176 = 0.02396474964916706 + 2.0 * 6.267572402954102
Epoch 1250, val loss: 1.2272089719772339
Epoch 1260, training loss: 12.556649208068848 = 0.02335684560239315 + 2.0 * 6.266646385192871
Epoch 1260, val loss: 1.232649564743042
Epoch 1270, training loss: 12.549911499023438 = 0.022771652787923813 + 2.0 * 6.2635698318481445
Epoch 1270, val loss: 1.2379741668701172
Epoch 1280, training loss: 12.550477027893066 = 0.022207653149962425 + 2.0 * 6.264134883880615
Epoch 1280, val loss: 1.2432081699371338
Epoch 1290, training loss: 12.560117721557617 = 0.021662451326847076 + 2.0 * 6.269227504730225
Epoch 1290, val loss: 1.248246669769287
Epoch 1300, training loss: 12.557262420654297 = 0.021137291565537453 + 2.0 * 6.268062591552734
Epoch 1300, val loss: 1.2536470890045166
Epoch 1310, training loss: 12.551398277282715 = 0.020632658153772354 + 2.0 * 6.265382766723633
Epoch 1310, val loss: 1.2585326433181763
Epoch 1320, training loss: 12.544827461242676 = 0.020148448646068573 + 2.0 * 6.2623395919799805
Epoch 1320, val loss: 1.2636228799819946
Epoch 1330, training loss: 12.546462059020996 = 0.019680587574839592 + 2.0 * 6.26339054107666
Epoch 1330, val loss: 1.2686195373535156
Epoch 1340, training loss: 12.543871879577637 = 0.01922888122498989 + 2.0 * 6.262321472167969
Epoch 1340, val loss: 1.2731844186782837
Epoch 1350, training loss: 12.541043281555176 = 0.018794585019350052 + 2.0 * 6.261124134063721
Epoch 1350, val loss: 1.278199553489685
Epoch 1360, training loss: 12.538909912109375 = 0.018375204876065254 + 2.0 * 6.26026725769043
Epoch 1360, val loss: 1.283145546913147
Epoch 1370, training loss: 12.53647518157959 = 0.017970824614167213 + 2.0 * 6.259252071380615
Epoch 1370, val loss: 1.2878323793411255
Epoch 1380, training loss: 12.535247802734375 = 0.017577311024069786 + 2.0 * 6.258835315704346
Epoch 1380, val loss: 1.292630910873413
Epoch 1390, training loss: 12.559009552001953 = 0.01719624362885952 + 2.0 * 6.270906448364258
Epoch 1390, val loss: 1.2973145246505737
Epoch 1400, training loss: 12.54529094696045 = 0.016826223582029343 + 2.0 * 6.264232158660889
Epoch 1400, val loss: 1.3016124963760376
Epoch 1410, training loss: 12.538227081298828 = 0.016471531242132187 + 2.0 * 6.26087760925293
Epoch 1410, val loss: 1.3063583374023438
Epoch 1420, training loss: 12.533243179321289 = 0.016129156574606895 + 2.0 * 6.258556842803955
Epoch 1420, val loss: 1.3109278678894043
Epoch 1430, training loss: 12.533249855041504 = 0.01579761505126953 + 2.0 * 6.258726119995117
Epoch 1430, val loss: 1.3154516220092773
Epoch 1440, training loss: 12.533848762512207 = 0.015475758351385593 + 2.0 * 6.259186267852783
Epoch 1440, val loss: 1.3197780847549438
Epoch 1450, training loss: 12.528142929077148 = 0.015163379721343517 + 2.0 * 6.2564897537231445
Epoch 1450, val loss: 1.324216365814209
Epoch 1460, training loss: 12.541818618774414 = 0.014862315729260445 + 2.0 * 6.2634782791137695
Epoch 1460, val loss: 1.3284510374069214
Epoch 1470, training loss: 12.53170108795166 = 0.014567377045750618 + 2.0 * 6.258566856384277
Epoch 1470, val loss: 1.3327605724334717
Epoch 1480, training loss: 12.528280258178711 = 0.014284155331552029 + 2.0 * 6.256998062133789
Epoch 1480, val loss: 1.3371378183364868
Epoch 1490, training loss: 12.541500091552734 = 0.014009966515004635 + 2.0 * 6.263744831085205
Epoch 1490, val loss: 1.341085433959961
Epoch 1500, training loss: 12.525691986083984 = 0.013742122799158096 + 2.0 * 6.255974769592285
Epoch 1500, val loss: 1.3455138206481934
Epoch 1510, training loss: 12.521475791931152 = 0.013484563678503036 + 2.0 * 6.253995418548584
Epoch 1510, val loss: 1.3496519327163696
Epoch 1520, training loss: 12.521158218383789 = 0.01323343999683857 + 2.0 * 6.253962516784668
Epoch 1520, val loss: 1.3537096977233887
Epoch 1530, training loss: 12.5316801071167 = 0.012989222072064877 + 2.0 * 6.259345531463623
Epoch 1530, val loss: 1.357718825340271
Epoch 1540, training loss: 12.527071952819824 = 0.012752028182148933 + 2.0 * 6.257160186767578
Epoch 1540, val loss: 1.3617255687713623
Epoch 1550, training loss: 12.52441120147705 = 0.012521212920546532 + 2.0 * 6.255945205688477
Epoch 1550, val loss: 1.3656166791915894
Epoch 1560, training loss: 12.519954681396484 = 0.012299416586756706 + 2.0 * 6.2538275718688965
Epoch 1560, val loss: 1.369654893875122
Epoch 1570, training loss: 12.516456604003906 = 0.01208233181387186 + 2.0 * 6.252187252044678
Epoch 1570, val loss: 1.3734629154205322
Epoch 1580, training loss: 12.52601146697998 = 0.011872177012264729 + 2.0 * 6.2570695877075195
Epoch 1580, val loss: 1.3771499395370483
Epoch 1590, training loss: 12.516613960266113 = 0.011666175909340382 + 2.0 * 6.252473831176758
Epoch 1590, val loss: 1.3810197114944458
Epoch 1600, training loss: 12.51321029663086 = 0.01146692130714655 + 2.0 * 6.250871658325195
Epoch 1600, val loss: 1.3848479986190796
Epoch 1610, training loss: 12.518301010131836 = 0.011273102834820747 + 2.0 * 6.253513813018799
Epoch 1610, val loss: 1.3885588645935059
Epoch 1620, training loss: 12.516304969787598 = 0.011083540506660938 + 2.0 * 6.252610683441162
Epoch 1620, val loss: 1.3922181129455566
Epoch 1630, training loss: 12.519783020019531 = 0.010898356325924397 + 2.0 * 6.25444221496582
Epoch 1630, val loss: 1.3957035541534424
Epoch 1640, training loss: 12.512739181518555 = 0.01072013285011053 + 2.0 * 6.251009464263916
Epoch 1640, val loss: 1.3994475603103638
Epoch 1650, training loss: 12.51588249206543 = 0.010545693337917328 + 2.0 * 6.252668380737305
Epoch 1650, val loss: 1.4030557870864868
Epoch 1660, training loss: 12.513163566589355 = 0.010375207290053368 + 2.0 * 6.251394271850586
Epoch 1660, val loss: 1.4065830707550049
Epoch 1670, training loss: 12.509662628173828 = 0.010210886597633362 + 2.0 * 6.249725818634033
Epoch 1670, val loss: 1.4099829196929932
Epoch 1680, training loss: 12.506702423095703 = 0.010050363838672638 + 2.0 * 6.248325824737549
Epoch 1680, val loss: 1.4135524034500122
Epoch 1690, training loss: 12.505819320678711 = 0.009893901646137238 + 2.0 * 6.247962474822998
Epoch 1690, val loss: 1.417008399963379
Epoch 1700, training loss: 12.508552551269531 = 0.009741361252963543 + 2.0 * 6.249405384063721
Epoch 1700, val loss: 1.4204076528549194
Epoch 1710, training loss: 12.513015747070312 = 0.00959152914583683 + 2.0 * 6.251712322235107
Epoch 1710, val loss: 1.423699975013733
Epoch 1720, training loss: 12.510050773620605 = 0.009445243515074253 + 2.0 * 6.250302791595459
Epoch 1720, val loss: 1.4272698163986206
Epoch 1730, training loss: 12.515666007995605 = 0.009303510189056396 + 2.0 * 6.253181457519531
Epoch 1730, val loss: 1.4305051565170288
Epoch 1740, training loss: 12.50615119934082 = 0.009165006689727306 + 2.0 * 6.248493194580078
Epoch 1740, val loss: 1.4338722229003906
Epoch 1750, training loss: 12.512336730957031 = 0.009030373767018318 + 2.0 * 6.25165319442749
Epoch 1750, val loss: 1.4371291399002075
Epoch 1760, training loss: 12.501360893249512 = 0.008899361826479435 + 2.0 * 6.246230602264404
Epoch 1760, val loss: 1.44034743309021
Epoch 1770, training loss: 12.502364158630371 = 0.008771531283855438 + 2.0 * 6.246796131134033
Epoch 1770, val loss: 1.4436841011047363
Epoch 1780, training loss: 12.504261016845703 = 0.00864605512470007 + 2.0 * 6.247807502746582
Epoch 1780, val loss: 1.4468742609024048
Epoch 1790, training loss: 12.501420021057129 = 0.008523202501237392 + 2.0 * 6.246448516845703
Epoch 1790, val loss: 1.4500908851623535
Epoch 1800, training loss: 12.505438804626465 = 0.008403629995882511 + 2.0 * 6.2485175132751465
Epoch 1800, val loss: 1.4534058570861816
Epoch 1810, training loss: 12.504504203796387 = 0.008285940624773502 + 2.0 * 6.248109340667725
Epoch 1810, val loss: 1.4563852548599243
Epoch 1820, training loss: 12.499422073364258 = 0.008171595633029938 + 2.0 * 6.245625019073486
Epoch 1820, val loss: 1.4594262838363647
Epoch 1830, training loss: 12.49820613861084 = 0.00806062575429678 + 2.0 * 6.245072841644287
Epoch 1830, val loss: 1.462613582611084
Epoch 1840, training loss: 12.504783630371094 = 0.007951466366648674 + 2.0 * 6.248415946960449
Epoch 1840, val loss: 1.4655364751815796
Epoch 1850, training loss: 12.50718879699707 = 0.007843941450119019 + 2.0 * 6.2496724128723145
Epoch 1850, val loss: 1.468481421470642
Epoch 1860, training loss: 12.49535846710205 = 0.007740602362900972 + 2.0 * 6.243808746337891
Epoch 1860, val loss: 1.4716525077819824
Epoch 1870, training loss: 12.496129035949707 = 0.007639160379767418 + 2.0 * 6.2442450523376465
Epoch 1870, val loss: 1.4746723175048828
Epoch 1880, training loss: 12.508947372436523 = 0.007539916783571243 + 2.0 * 6.250703811645508
Epoch 1880, val loss: 1.4775073528289795
Epoch 1890, training loss: 12.49659538269043 = 0.0074432166293263435 + 2.0 * 6.2445759773254395
Epoch 1890, val loss: 1.4804857969284058
Epoch 1900, training loss: 12.49330997467041 = 0.007347662001848221 + 2.0 * 6.24298095703125
Epoch 1900, val loss: 1.4834409952163696
Epoch 1910, training loss: 12.493645668029785 = 0.007254964672029018 + 2.0 * 6.243195533752441
Epoch 1910, val loss: 1.4863314628601074
Epoch 1920, training loss: 12.50760555267334 = 0.0071631125174462795 + 2.0 * 6.250221252441406
Epoch 1920, val loss: 1.4891526699066162
Epoch 1930, training loss: 12.49770450592041 = 0.00707347271963954 + 2.0 * 6.2453155517578125
Epoch 1930, val loss: 1.491961121559143
Epoch 1940, training loss: 12.49099349975586 = 0.00698663666844368 + 2.0 * 6.242003440856934
Epoch 1940, val loss: 1.4948781728744507
Epoch 1950, training loss: 12.492715835571289 = 0.006901765242218971 + 2.0 * 6.2429070472717285
Epoch 1950, val loss: 1.4977003335952759
Epoch 1960, training loss: 12.498736381530762 = 0.006818278692662716 + 2.0 * 6.245959281921387
Epoch 1960, val loss: 1.500477910041809
Epoch 1970, training loss: 12.495644569396973 = 0.006735167931765318 + 2.0 * 6.244454860687256
Epoch 1970, val loss: 1.503124475479126
Epoch 1980, training loss: 12.48806381225586 = 0.00665500620380044 + 2.0 * 6.240704536437988
Epoch 1980, val loss: 1.5059449672698975
Epoch 1990, training loss: 12.489090919494629 = 0.006576765328645706 + 2.0 * 6.241257190704346
Epoch 1990, val loss: 1.5085386037826538
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.788888888888889
0.8186610437532947
The final CL Acc:0.76049, 0.02145, The final GNN Acc:0.81954, 0.00163
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13234])
remove edge: torch.Size([2, 7872])
updated graph: torch.Size([2, 10550])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 19.146589279174805 = 1.95285964012146 + 2.0 * 8.596864700317383
Epoch 0, val loss: 1.9572597742080688
Epoch 10, training loss: 19.135982513427734 = 1.9427416324615479 + 2.0 * 8.596620559692383
Epoch 10, val loss: 1.9472774267196655
Epoch 20, training loss: 19.118669509887695 = 1.9302315711975098 + 2.0 * 8.594219207763672
Epoch 20, val loss: 1.9343677759170532
Epoch 30, training loss: 19.060949325561523 = 1.9129313230514526 + 2.0 * 8.57400894165039
Epoch 30, val loss: 1.9162287712097168
Epoch 40, training loss: 18.781465530395508 = 1.8912310600280762 + 2.0 * 8.445116996765137
Epoch 40, val loss: 1.894256353378296
Epoch 50, training loss: 17.910606384277344 = 1.8687554597854614 + 2.0 * 8.020925521850586
Epoch 50, val loss: 1.8720380067825317
Epoch 60, training loss: 17.423879623413086 = 1.8479175567626953 + 2.0 * 7.787981033325195
Epoch 60, val loss: 1.8528836965560913
Epoch 70, training loss: 16.513919830322266 = 1.8304497003555298 + 2.0 * 7.341735363006592
Epoch 70, val loss: 1.836633324623108
Epoch 80, training loss: 15.967842102050781 = 1.8147127628326416 + 2.0 * 7.076564788818359
Epoch 80, val loss: 1.8214476108551025
Epoch 90, training loss: 15.673417091369629 = 1.799196481704712 + 2.0 * 6.937110424041748
Epoch 90, val loss: 1.8067289590835571
Epoch 100, training loss: 15.399310111999512 = 1.783957839012146 + 2.0 * 6.807676315307617
Epoch 100, val loss: 1.7929673194885254
Epoch 110, training loss: 15.243400573730469 = 1.768639087677002 + 2.0 * 6.7373809814453125
Epoch 110, val loss: 1.7789121866226196
Epoch 120, training loss: 15.12306022644043 = 1.7511916160583496 + 2.0 * 6.685934066772461
Epoch 120, val loss: 1.763152837753296
Epoch 130, training loss: 15.013961791992188 = 1.7322781085968018 + 2.0 * 6.640841960906982
Epoch 130, val loss: 1.7461328506469727
Epoch 140, training loss: 14.930145263671875 = 1.7120287418365479 + 2.0 * 6.609058380126953
Epoch 140, val loss: 1.72802734375
Epoch 150, training loss: 14.843880653381348 = 1.6898177862167358 + 2.0 * 6.57703161239624
Epoch 150, val loss: 1.7083423137664795
Epoch 160, training loss: 14.772748947143555 = 1.6649725437164307 + 2.0 * 6.553888320922852
Epoch 160, val loss: 1.6864327192306519
Epoch 170, training loss: 14.713541030883789 = 1.637107253074646 + 2.0 * 6.538217067718506
Epoch 170, val loss: 1.6619818210601807
Epoch 180, training loss: 14.64044189453125 = 1.606320858001709 + 2.0 * 6.517060279846191
Epoch 180, val loss: 1.6354213953018188
Epoch 190, training loss: 14.56991195678711 = 1.5724425315856934 + 2.0 * 6.498734951019287
Epoch 190, val loss: 1.6063593626022339
Epoch 200, training loss: 14.503387451171875 = 1.5352721214294434 + 2.0 * 6.484057426452637
Epoch 200, val loss: 1.574607253074646
Epoch 210, training loss: 14.451774597167969 = 1.4949002265930176 + 2.0 * 6.478437423706055
Epoch 210, val loss: 1.5404150485992432
Epoch 220, training loss: 14.370561599731445 = 1.4522353410720825 + 2.0 * 6.459163188934326
Epoch 220, val loss: 1.5046436786651611
Epoch 230, training loss: 14.301190376281738 = 1.4077118635177612 + 2.0 * 6.446739196777344
Epoch 230, val loss: 1.4675939083099365
Epoch 240, training loss: 14.244750022888184 = 1.36168372631073 + 2.0 * 6.441533088684082
Epoch 240, val loss: 1.4298263788223267
Epoch 250, training loss: 14.171747207641602 = 1.3148589134216309 + 2.0 * 6.428443908691406
Epoch 250, val loss: 1.3921301364898682
Epoch 260, training loss: 14.105224609375 = 1.2680364847183228 + 2.0 * 6.418593883514404
Epoch 260, val loss: 1.354722499847412
Epoch 270, training loss: 14.05210018157959 = 1.2213467359542847 + 2.0 * 6.415376663208008
Epoch 270, val loss: 1.31794011592865
Epoch 280, training loss: 13.983660697937012 = 1.1756212711334229 + 2.0 * 6.404019832611084
Epoch 280, val loss: 1.2820881605148315
Epoch 290, training loss: 13.925117492675781 = 1.13096022605896 + 2.0 * 6.397078514099121
Epoch 290, val loss: 1.2474439144134521
Epoch 300, training loss: 13.870174407958984 = 1.0874239206314087 + 2.0 * 6.3913750648498535
Epoch 300, val loss: 1.2139519453048706
Epoch 310, training loss: 13.83531379699707 = 1.045454740524292 + 2.0 * 6.3949294090271
Epoch 310, val loss: 1.1817468404769897
Epoch 320, training loss: 13.775968551635742 = 1.0052714347839355 + 2.0 * 6.385348796844482
Epoch 320, val loss: 1.1511708498001099
Epoch 330, training loss: 13.723575592041016 = 0.9669021368026733 + 2.0 * 6.3783369064331055
Epoch 330, val loss: 1.1219451427459717
Epoch 340, training loss: 13.676045417785645 = 0.929706335067749 + 2.0 * 6.373169422149658
Epoch 340, val loss: 1.0938279628753662
Epoch 350, training loss: 13.632039070129395 = 0.8935652375221252 + 2.0 * 6.369236946105957
Epoch 350, val loss: 1.0666197538375854
Epoch 360, training loss: 13.589834213256836 = 0.8583821654319763 + 2.0 * 6.365725994110107
Epoch 360, val loss: 1.0402525663375854
Epoch 370, training loss: 13.567981719970703 = 0.8241787552833557 + 2.0 * 6.371901512145996
Epoch 370, val loss: 1.0149577856063843
Epoch 380, training loss: 13.510059356689453 = 0.7914121747016907 + 2.0 * 6.359323501586914
Epoch 380, val loss: 0.9906980395317078
Epoch 390, training loss: 13.471036911010742 = 0.7596346139907837 + 2.0 * 6.355700969696045
Epoch 390, val loss: 0.9676521420478821
Epoch 400, training loss: 13.433682441711426 = 0.728474497795105 + 2.0 * 6.352603912353516
Epoch 400, val loss: 0.945357084274292
Epoch 410, training loss: 13.396851539611816 = 0.697909951210022 + 2.0 * 6.349470615386963
Epoch 410, val loss: 0.9236913323402405
Epoch 420, training loss: 13.361156463623047 = 0.6678409576416016 + 2.0 * 6.346657752990723
Epoch 420, val loss: 0.9026943445205688
Epoch 430, training loss: 13.326626777648926 = 0.6383388042449951 + 2.0 * 6.344143867492676
Epoch 430, val loss: 0.8824261426925659
Epoch 440, training loss: 13.292937278747559 = 0.6096006035804749 + 2.0 * 6.341668128967285
Epoch 440, val loss: 0.8629283308982849
Epoch 450, training loss: 13.272459983825684 = 0.5815240740776062 + 2.0 * 6.345468044281006
Epoch 450, val loss: 0.844322919845581
Epoch 460, training loss: 13.2318115234375 = 0.5543313026428223 + 2.0 * 6.338740348815918
Epoch 460, val loss: 0.8266888856887817
Epoch 470, training loss: 13.197379112243652 = 0.5276959538459778 + 2.0 * 6.334841728210449
Epoch 470, val loss: 0.8100847601890564
Epoch 480, training loss: 13.165828704833984 = 0.5017514228820801 + 2.0 * 6.332038402557373
Epoch 480, val loss: 0.7943964600563049
Epoch 490, training loss: 13.15853500366211 = 0.47646501660346985 + 2.0 * 6.341034889221191
Epoch 490, val loss: 0.779660165309906
Epoch 500, training loss: 13.113948822021484 = 0.45197808742523193 + 2.0 * 6.3309855461120605
Epoch 500, val loss: 0.7658891677856445
Epoch 510, training loss: 13.083085060119629 = 0.4284295439720154 + 2.0 * 6.327327728271484
Epoch 510, val loss: 0.7533465623855591
Epoch 520, training loss: 13.054929733276367 = 0.4056190550327301 + 2.0 * 6.324655532836914
Epoch 520, val loss: 0.7417913675308228
Epoch 530, training loss: 13.056986808776855 = 0.38361185789108276 + 2.0 * 6.3366875648498535
Epoch 530, val loss: 0.7311637997627258
Epoch 540, training loss: 13.012297630310059 = 0.36243563890457153 + 2.0 * 6.3249311447143555
Epoch 540, val loss: 0.7215753197669983
Epoch 550, training loss: 12.982233047485352 = 0.3421507179737091 + 2.0 * 6.320041179656982
Epoch 550, val loss: 0.7130588293075562
Epoch 560, training loss: 12.956972122192383 = 0.3226335942745209 + 2.0 * 6.317169189453125
Epoch 560, val loss: 0.7055144309997559
Epoch 570, training loss: 12.933755874633789 = 0.30387306213378906 + 2.0 * 6.31494140625
Epoch 570, val loss: 0.6989259719848633
Epoch 580, training loss: 12.917947769165039 = 0.28588607907295227 + 2.0 * 6.316030979156494
Epoch 580, val loss: 0.6931871771812439
Epoch 590, training loss: 12.90397834777832 = 0.26880091428756714 + 2.0 * 6.317588806152344
Epoch 590, val loss: 0.6882938146591187
Epoch 600, training loss: 12.876261711120605 = 0.2526381313800812 + 2.0 * 6.311811923980713
Epoch 600, val loss: 0.6843330264091492
Epoch 610, training loss: 12.857282638549805 = 0.2373574823141098 + 2.0 * 6.309962749481201
Epoch 610, val loss: 0.6812910437583923
Epoch 620, training loss: 12.837504386901855 = 0.22295626997947693 + 2.0 * 6.307273864746094
Epoch 620, val loss: 0.6790961027145386
Epoch 630, training loss: 12.823137283325195 = 0.20946674048900604 + 2.0 * 6.306835174560547
Epoch 630, val loss: 0.6777011156082153
Epoch 640, training loss: 12.806405067443848 = 0.1968105584383011 + 2.0 * 6.304797172546387
Epoch 640, val loss: 0.6771140098571777
Epoch 650, training loss: 12.80959415435791 = 0.18495488166809082 + 2.0 * 6.312319755554199
Epoch 650, val loss: 0.6772345900535583
Epoch 660, training loss: 12.784420013427734 = 0.17398683726787567 + 2.0 * 6.3052167892456055
Epoch 660, val loss: 0.6778503656387329
Epoch 670, training loss: 12.765356063842773 = 0.16374444961547852 + 2.0 * 6.300805568695068
Epoch 670, val loss: 0.6792712211608887
Epoch 680, training loss: 12.752699851989746 = 0.15422987937927246 + 2.0 * 6.299234867095947
Epoch 680, val loss: 0.6811960339546204
Epoch 690, training loss: 12.740265846252441 = 0.14536164700984955 + 2.0 * 6.297451972961426
Epoch 690, val loss: 0.6836724877357483
Epoch 700, training loss: 12.751288414001465 = 0.13709205389022827 + 2.0 * 6.307098388671875
Epoch 700, val loss: 0.6866251826286316
Epoch 710, training loss: 12.731225967407227 = 0.1294550895690918 + 2.0 * 6.300885200500488
Epoch 710, val loss: 0.6898558139801025
Epoch 720, training loss: 12.712298393249512 = 0.12236866354942322 + 2.0 * 6.294964790344238
Epoch 720, val loss: 0.6935204863548279
Epoch 730, training loss: 12.702014923095703 = 0.1158151850104332 + 2.0 * 6.293099880218506
Epoch 730, val loss: 0.697566568851471
Epoch 740, training loss: 12.692524909973145 = 0.10969021916389465 + 2.0 * 6.291417121887207
Epoch 740, val loss: 0.7019153833389282
Epoch 750, training loss: 12.687577247619629 = 0.10396180301904678 + 2.0 * 6.291807651519775
Epoch 750, val loss: 0.7064956426620483
Epoch 760, training loss: 12.685155868530273 = 0.09861762076616287 + 2.0 * 6.293269157409668
Epoch 760, val loss: 0.7112597823143005
Epoch 770, training loss: 12.6762056350708 = 0.09363508969545364 + 2.0 * 6.291285037994385
Epoch 770, val loss: 0.7161999344825745
Epoch 780, training loss: 12.663681030273438 = 0.08901119232177734 + 2.0 * 6.28733491897583
Epoch 780, val loss: 0.7213358283042908
Epoch 790, training loss: 12.658483505249023 = 0.08468922972679138 + 2.0 * 6.2868971824646
Epoch 790, val loss: 0.7266340255737305
Epoch 800, training loss: 12.665864944458008 = 0.08063370734453201 + 2.0 * 6.2926154136657715
Epoch 800, val loss: 0.7319974899291992
Epoch 810, training loss: 12.654979705810547 = 0.07681504637002945 + 2.0 * 6.2890825271606445
Epoch 810, val loss: 0.7374995946884155
Epoch 820, training loss: 12.643669128417969 = 0.07327334582805634 + 2.0 * 6.285197734832764
Epoch 820, val loss: 0.7430073618888855
Epoch 830, training loss: 12.635218620300293 = 0.06994672119617462 + 2.0 * 6.2826361656188965
Epoch 830, val loss: 0.7486330270767212
Epoch 840, training loss: 12.636629104614258 = 0.06682543456554413 + 2.0 * 6.2849016189575195
Epoch 840, val loss: 0.754326581954956
Epoch 850, training loss: 12.625025749206543 = 0.06387628614902496 + 2.0 * 6.280574798583984
Epoch 850, val loss: 0.7599936127662659
Epoch 860, training loss: 12.621892929077148 = 0.06110939756035805 + 2.0 * 6.280391693115234
Epoch 860, val loss: 0.7657118439674377
Epoch 870, training loss: 12.627882957458496 = 0.058517102152109146 + 2.0 * 6.284682750701904
Epoch 870, val loss: 0.7714104652404785
Epoch 880, training loss: 12.620473861694336 = 0.05606177821755409 + 2.0 * 6.282206058502197
Epoch 880, val loss: 0.7771225571632385
Epoch 890, training loss: 12.611827850341797 = 0.053759895265102386 + 2.0 * 6.27903413772583
Epoch 890, val loss: 0.7828311920166016
Epoch 900, training loss: 12.60930061340332 = 0.05158708617091179 + 2.0 * 6.2788567543029785
Epoch 900, val loss: 0.7884629368782043
Epoch 910, training loss: 12.603610038757324 = 0.049541447311639786 + 2.0 * 6.277034282684326
Epoch 910, val loss: 0.7941387891769409
Epoch 920, training loss: 12.601314544677734 = 0.047597579658031464 + 2.0 * 6.276858329772949
Epoch 920, val loss: 0.799796462059021
Epoch 930, training loss: 12.60084342956543 = 0.04576217010617256 + 2.0 * 6.277540683746338
Epoch 930, val loss: 0.8054232597351074
Epoch 940, training loss: 12.592646598815918 = 0.04403219744563103 + 2.0 * 6.2743072509765625
Epoch 940, val loss: 0.810936450958252
Epoch 950, training loss: 12.594170570373535 = 0.042389288544654846 + 2.0 * 6.275890827178955
Epoch 950, val loss: 0.8164441585540771
Epoch 960, training loss: 12.586872100830078 = 0.04083416610956192 + 2.0 * 6.273018836975098
Epoch 960, val loss: 0.8219555020332336
Epoch 970, training loss: 12.586461067199707 = 0.03936045989394188 + 2.0 * 6.273550510406494
Epoch 970, val loss: 0.827402651309967
Epoch 980, training loss: 12.58284854888916 = 0.037961602210998535 + 2.0 * 6.2724432945251465
Epoch 980, val loss: 0.8328337073326111
Epoch 990, training loss: 12.57615852355957 = 0.03663696348667145 + 2.0 * 6.269760608673096
Epoch 990, val loss: 0.838219404220581
Epoch 1000, training loss: 12.58112907409668 = 0.035373225808143616 + 2.0 * 6.2728776931762695
Epoch 1000, val loss: 0.843600869178772
Epoch 1010, training loss: 12.579258918762207 = 0.03417116776108742 + 2.0 * 6.272543907165527
Epoch 1010, val loss: 0.8488120436668396
Epoch 1020, training loss: 12.570207595825195 = 0.03303565829992294 + 2.0 * 6.268586158752441
Epoch 1020, val loss: 0.854058027267456
Epoch 1030, training loss: 12.566635131835938 = 0.03195430338382721 + 2.0 * 6.267340183258057
Epoch 1030, val loss: 0.8592349290847778
Epoch 1040, training loss: 12.567159652709961 = 0.0309231486171484 + 2.0 * 6.268118381500244
Epoch 1040, val loss: 0.8643523454666138
Epoch 1050, training loss: 12.570633888244629 = 0.0299331396818161 + 2.0 * 6.270350456237793
Epoch 1050, val loss: 0.8694117069244385
Epoch 1060, training loss: 12.563403129577637 = 0.028992116451263428 + 2.0 * 6.267205715179443
Epoch 1060, val loss: 0.8744003176689148
Epoch 1070, training loss: 12.558111190795898 = 0.028102247044444084 + 2.0 * 6.265004634857178
Epoch 1070, val loss: 0.8793470859527588
Epoch 1080, training loss: 12.555495262145996 = 0.02725326269865036 + 2.0 * 6.264121055603027
Epoch 1080, val loss: 0.8842569589614868
Epoch 1090, training loss: 12.552725791931152 = 0.02643761597573757 + 2.0 * 6.263144016265869
Epoch 1090, val loss: 0.8891314268112183
Epoch 1100, training loss: 12.568092346191406 = 0.02565891295671463 + 2.0 * 6.271216869354248
Epoch 1100, val loss: 0.893977165222168
Epoch 1110, training loss: 12.554132461547852 = 0.024907667189836502 + 2.0 * 6.264612197875977
Epoch 1110, val loss: 0.8986817598342896
Epoch 1120, training loss: 12.548073768615723 = 0.024197151884436607 + 2.0 * 6.261938095092773
Epoch 1120, val loss: 0.9033764004707336
Epoch 1130, training loss: 12.555168151855469 = 0.023518729954957962 + 2.0 * 6.265824794769287
Epoch 1130, val loss: 0.9080368876457214
Epoch 1140, training loss: 12.550681114196777 = 0.022861259058117867 + 2.0 * 6.263909816741943
Epoch 1140, val loss: 0.912599503993988
Epoch 1150, training loss: 12.5443754196167 = 0.022240517660975456 + 2.0 * 6.2610673904418945
Epoch 1150, val loss: 0.9170935153961182
Epoch 1160, training loss: 12.542296409606934 = 0.02164474129676819 + 2.0 * 6.260325908660889
Epoch 1160, val loss: 0.9215657114982605
Epoch 1170, training loss: 12.546161651611328 = 0.021071499213576317 + 2.0 * 6.262545108795166
Epoch 1170, val loss: 0.9259657263755798
Epoch 1180, training loss: 12.53683090209961 = 0.020516036078333855 + 2.0 * 6.258157253265381
Epoch 1180, val loss: 0.9303848743438721
Epoch 1190, training loss: 12.545937538146973 = 0.019987329840660095 + 2.0 * 6.262975215911865
Epoch 1190, val loss: 0.9347060322761536
Epoch 1200, training loss: 12.536901473999023 = 0.019479764625430107 + 2.0 * 6.258710861206055
Epoch 1200, val loss: 0.9389040470123291
Epoch 1210, training loss: 12.533547401428223 = 0.0189885925501585 + 2.0 * 6.257279396057129
Epoch 1210, val loss: 0.943138599395752
Epoch 1220, training loss: 12.534841537475586 = 0.018521595746278763 + 2.0 * 6.25816011428833
Epoch 1220, val loss: 0.9472769498825073
Epoch 1230, training loss: 12.52979564666748 = 0.018065162003040314 + 2.0 * 6.255865097045898
Epoch 1230, val loss: 0.9513373374938965
Epoch 1240, training loss: 12.527554512023926 = 0.017631087452173233 + 2.0 * 6.2549614906311035
Epoch 1240, val loss: 0.9553920030593872
Epoch 1250, training loss: 12.526203155517578 = 0.01721288450062275 + 2.0 * 6.254495143890381
Epoch 1250, val loss: 0.9594402313232422
Epoch 1260, training loss: 12.529947280883789 = 0.016812127083539963 + 2.0 * 6.256567478179932
Epoch 1260, val loss: 0.9633905291557312
Epoch 1270, training loss: 12.525306701660156 = 0.016418319195508957 + 2.0 * 6.254444122314453
Epoch 1270, val loss: 0.9671874046325684
Epoch 1280, training loss: 12.526610374450684 = 0.016046015545725822 + 2.0 * 6.255282402038574
Epoch 1280, val loss: 0.9710863828659058
Epoch 1290, training loss: 12.519989013671875 = 0.015685437247157097 + 2.0 * 6.252151966094971
Epoch 1290, val loss: 0.974853515625
Epoch 1300, training loss: 12.518381118774414 = 0.015338550321757793 + 2.0 * 6.251521110534668
Epoch 1300, val loss: 0.9786404967308044
Epoch 1310, training loss: 12.527848243713379 = 0.015004013665020466 + 2.0 * 6.25642204284668
Epoch 1310, val loss: 0.9824255108833313
Epoch 1320, training loss: 12.522226333618164 = 0.014674214646220207 + 2.0 * 6.2537760734558105
Epoch 1320, val loss: 0.9860342741012573
Epoch 1330, training loss: 12.519330978393555 = 0.014361916109919548 + 2.0 * 6.252484321594238
Epoch 1330, val loss: 0.9897171854972839
Epoch 1340, training loss: 12.515707969665527 = 0.014057954773306847 + 2.0 * 6.250824928283691
Epoch 1340, val loss: 0.9932652115821838
Epoch 1350, training loss: 12.516518592834473 = 0.013766210526227951 + 2.0 * 6.251376152038574
Epoch 1350, val loss: 0.9968641400337219
Epoch 1360, training loss: 12.520123481750488 = 0.013482355512678623 + 2.0 * 6.253320693969727
Epoch 1360, val loss: 1.0002931356430054
Epoch 1370, training loss: 12.516493797302246 = 0.013209131546318531 + 2.0 * 6.251642227172852
Epoch 1370, val loss: 1.0038082599639893
Epoch 1380, training loss: 12.511314392089844 = 0.012941461987793446 + 2.0 * 6.2491865158081055
Epoch 1380, val loss: 1.0072672367095947
Epoch 1390, training loss: 12.50973129272461 = 0.012686239555478096 + 2.0 * 6.248522758483887
Epoch 1390, val loss: 1.0106852054595947
Epoch 1400, training loss: 12.516510009765625 = 0.012436910532414913 + 2.0 * 6.2520365715026855
Epoch 1400, val loss: 1.014082431793213
Epoch 1410, training loss: 12.509048461914062 = 0.012194221839308739 + 2.0 * 6.248426914215088
Epoch 1410, val loss: 1.0173606872558594
Epoch 1420, training loss: 12.510354042053223 = 0.01196112297475338 + 2.0 * 6.249196529388428
Epoch 1420, val loss: 1.0206738710403442
Epoch 1430, training loss: 12.505020141601562 = 0.011735424399375916 + 2.0 * 6.246642589569092
Epoch 1430, val loss: 1.0238876342773438
Epoch 1440, training loss: 12.507966041564941 = 0.01151701994240284 + 2.0 * 6.24822473526001
Epoch 1440, val loss: 1.0271224975585938
Epoch 1450, training loss: 12.510108947753906 = 0.01130232959985733 + 2.0 * 6.249403476715088
Epoch 1450, val loss: 1.0302661657333374
Epoch 1460, training loss: 12.502832412719727 = 0.011095063760876656 + 2.0 * 6.245868682861328
Epoch 1460, val loss: 1.0334876775741577
Epoch 1470, training loss: 12.501761436462402 = 0.010895492509007454 + 2.0 * 6.2454328536987305
Epoch 1470, val loss: 1.0366201400756836
Epoch 1480, training loss: 12.504110336303711 = 0.010701055638492107 + 2.0 * 6.246704578399658
Epoch 1480, val loss: 1.0397406816482544
Epoch 1490, training loss: 12.504827499389648 = 0.010511414147913456 + 2.0 * 6.247158050537109
Epoch 1490, val loss: 1.0428045988082886
Epoch 1500, training loss: 12.505456924438477 = 0.010329877957701683 + 2.0 * 6.247563362121582
Epoch 1500, val loss: 1.0458297729492188
Epoch 1510, training loss: 12.507277488708496 = 0.010150114074349403 + 2.0 * 6.248563766479492
Epoch 1510, val loss: 1.0488349199295044
Epoch 1520, training loss: 12.49968433380127 = 0.009978530928492546 + 2.0 * 6.2448530197143555
Epoch 1520, val loss: 1.0517165660858154
Epoch 1530, training loss: 12.496928215026855 = 0.009809567593038082 + 2.0 * 6.24355936050415
Epoch 1530, val loss: 1.0546751022338867
Epoch 1540, training loss: 12.499557495117188 = 0.009646487422287464 + 2.0 * 6.244955539703369
Epoch 1540, val loss: 1.0576528310775757
Epoch 1550, training loss: 12.497139930725098 = 0.009485631249845028 + 2.0 * 6.2438273429870605
Epoch 1550, val loss: 1.060437560081482
Epoch 1560, training loss: 12.495099067687988 = 0.00933113507926464 + 2.0 * 6.242884159088135
Epoch 1560, val loss: 1.0632885694503784
Epoch 1570, training loss: 12.497833251953125 = 0.009180239401757717 + 2.0 * 6.244326591491699
Epoch 1570, val loss: 1.0661777257919312
Epoch 1580, training loss: 12.499829292297363 = 0.009031681343913078 + 2.0 * 6.245398998260498
Epoch 1580, val loss: 1.0688377618789673
Epoch 1590, training loss: 12.49093246459961 = 0.008889644406735897 + 2.0 * 6.241021633148193
Epoch 1590, val loss: 1.071632742881775
Epoch 1600, training loss: 12.49353313446045 = 0.00875146221369505 + 2.0 * 6.2423906326293945
Epoch 1600, val loss: 1.0743519067764282
Epoch 1610, training loss: 12.4977445602417 = 0.00861551333218813 + 2.0 * 6.244564533233643
Epoch 1610, val loss: 1.077049970626831
Epoch 1620, training loss: 12.487945556640625 = 0.008483783341944218 + 2.0 * 6.2397308349609375
Epoch 1620, val loss: 1.0797723531723022
Epoch 1630, training loss: 12.489603996276855 = 0.008355762809515 + 2.0 * 6.240623950958252
Epoch 1630, val loss: 1.0824867486953735
Epoch 1640, training loss: 12.4940824508667 = 0.008230230771005154 + 2.0 * 6.242926120758057
Epoch 1640, val loss: 1.0851095914840698
Epoch 1650, training loss: 12.493879318237305 = 0.008105752989649773 + 2.0 * 6.242887020111084
Epoch 1650, val loss: 1.0876883268356323
Epoch 1660, training loss: 12.486688613891602 = 0.007986649870872498 + 2.0 * 6.23935079574585
Epoch 1660, val loss: 1.0902622938156128
Epoch 1670, training loss: 12.486713409423828 = 0.007871304638683796 + 2.0 * 6.2394208908081055
Epoch 1670, val loss: 1.0928318500518799
Epoch 1680, training loss: 12.48871898651123 = 0.007757395040243864 + 2.0 * 6.240480899810791
Epoch 1680, val loss: 1.0953816175460815
Epoch 1690, training loss: 12.487387657165527 = 0.007646952755749226 + 2.0 * 6.239870548248291
Epoch 1690, val loss: 1.0978624820709229
Epoch 1700, training loss: 12.486913681030273 = 0.007538958452641964 + 2.0 * 6.239687442779541
Epoch 1700, val loss: 1.1003241539001465
Epoch 1710, training loss: 12.496536254882812 = 0.007433198392391205 + 2.0 * 6.244551658630371
Epoch 1710, val loss: 1.102751612663269
Epoch 1720, training loss: 12.485118865966797 = 0.007328238803893328 + 2.0 * 6.238895416259766
Epoch 1720, val loss: 1.1052931547164917
Epoch 1730, training loss: 12.481480598449707 = 0.007228388916701078 + 2.0 * 6.237125873565674
Epoch 1730, val loss: 1.1076865196228027
Epoch 1740, training loss: 12.482872009277344 = 0.007130168378353119 + 2.0 * 6.237870693206787
Epoch 1740, val loss: 1.110098958015442
Epoch 1750, training loss: 12.484570503234863 = 0.007033453322947025 + 2.0 * 6.238768577575684
Epoch 1750, val loss: 1.1124546527862549
Epoch 1760, training loss: 12.481298446655273 = 0.006938694976270199 + 2.0 * 6.237179756164551
Epoch 1760, val loss: 1.114775538444519
Epoch 1770, training loss: 12.480085372924805 = 0.006846628151834011 + 2.0 * 6.236619472503662
Epoch 1770, val loss: 1.117158055305481
Epoch 1780, training loss: 12.484451293945312 = 0.006756737362593412 + 2.0 * 6.238847255706787
Epoch 1780, val loss: 1.1194572448730469
Epoch 1790, training loss: 12.478683471679688 = 0.006668579764664173 + 2.0 * 6.236007213592529
Epoch 1790, val loss: 1.121744155883789
Epoch 1800, training loss: 12.483664512634277 = 0.0065827686339616776 + 2.0 * 6.2385406494140625
Epoch 1800, val loss: 1.1240713596343994
Epoch 1810, training loss: 12.480484008789062 = 0.006497987546026707 + 2.0 * 6.236992835998535
Epoch 1810, val loss: 1.1262855529785156
Epoch 1820, training loss: 12.476107597351074 = 0.0064167445525527 + 2.0 * 6.2348456382751465
Epoch 1820, val loss: 1.1285780668258667
Epoch 1830, training loss: 12.481354713439941 = 0.006336385849863291 + 2.0 * 6.237509250640869
Epoch 1830, val loss: 1.1308454275131226
Epoch 1840, training loss: 12.48361873626709 = 0.006255792919546366 + 2.0 * 6.238681316375732
Epoch 1840, val loss: 1.1329748630523682
Epoch 1850, training loss: 12.475900650024414 = 0.0061793336644768715 + 2.0 * 6.234860420227051
Epoch 1850, val loss: 1.1351475715637207
Epoch 1860, training loss: 12.47671127319336 = 0.00610369723290205 + 2.0 * 6.23530387878418
Epoch 1860, val loss: 1.137337327003479
Epoch 1870, training loss: 12.472660064697266 = 0.0060295541770756245 + 2.0 * 6.233315467834473
Epoch 1870, val loss: 1.1394835710525513
Epoch 1880, training loss: 12.475723266601562 = 0.005957442335784435 + 2.0 * 6.234882831573486
Epoch 1880, val loss: 1.1416599750518799
Epoch 1890, training loss: 12.47563362121582 = 0.005885703023523092 + 2.0 * 6.2348737716674805
Epoch 1890, val loss: 1.1437848806381226
Epoch 1900, training loss: 12.472858428955078 = 0.005816856399178505 + 2.0 * 6.233520984649658
Epoch 1900, val loss: 1.1458969116210938
Epoch 1910, training loss: 12.484271049499512 = 0.0057494863867759705 + 2.0 * 6.239260673522949
Epoch 1910, val loss: 1.147865653038025
Epoch 1920, training loss: 12.472012519836426 = 0.00568041717633605 + 2.0 * 6.233166217803955
Epoch 1920, val loss: 1.149933099746704
Epoch 1930, training loss: 12.46955394744873 = 0.005616228561848402 + 2.0 * 6.231968879699707
Epoch 1930, val loss: 1.1520296335220337
Epoch 1940, training loss: 12.479022026062012 = 0.005552672315388918 + 2.0 * 6.236734867095947
Epoch 1940, val loss: 1.15395987033844
Epoch 1950, training loss: 12.467964172363281 = 0.005489126779139042 + 2.0 * 6.231237411499023
Epoch 1950, val loss: 1.155943751335144
Epoch 1960, training loss: 12.466829299926758 = 0.005427767522633076 + 2.0 * 6.230700969696045
Epoch 1960, val loss: 1.1579723358154297
Epoch 1970, training loss: 12.466941833496094 = 0.005367588251829147 + 2.0 * 6.23078727722168
Epoch 1970, val loss: 1.15999436378479
Epoch 1980, training loss: 12.469864845275879 = 0.005308120045810938 + 2.0 * 6.232278347015381
Epoch 1980, val loss: 1.1619893312454224
Epoch 1990, training loss: 12.473541259765625 = 0.005248887464404106 + 2.0 * 6.2341461181640625
Epoch 1990, val loss: 1.1639444828033447
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8111111111111111
0.8360569319978914
=== training gcn model ===
Epoch 0, training loss: 19.146875381469727 = 1.9531596899032593 + 2.0 * 8.596858024597168
Epoch 0, val loss: 1.960577368736267
Epoch 10, training loss: 19.13546371459961 = 1.9423620700836182 + 2.0 * 8.596550941467285
Epoch 10, val loss: 1.9492961168289185
Epoch 20, training loss: 19.11631965637207 = 1.9286987781524658 + 2.0 * 8.593810081481934
Epoch 20, val loss: 1.9350920915603638
Epoch 30, training loss: 19.048141479492188 = 1.909860610961914 + 2.0 * 8.569140434265137
Epoch 30, val loss: 1.9157522916793823
Epoch 40, training loss: 18.695415496826172 = 1.8861640691757202 + 2.0 * 8.40462589263916
Epoch 40, val loss: 1.8925387859344482
Epoch 50, training loss: 17.442035675048828 = 1.8604483604431152 + 2.0 * 7.7907938957214355
Epoch 50, val loss: 1.867995023727417
Epoch 60, training loss: 16.759225845336914 = 1.840406060218811 + 2.0 * 7.459409713745117
Epoch 60, val loss: 1.8500767946243286
Epoch 70, training loss: 16.1762752532959 = 1.8263630867004395 + 2.0 * 7.17495584487915
Epoch 70, val loss: 1.8370671272277832
Epoch 80, training loss: 15.707494735717773 = 1.8150898218154907 + 2.0 * 6.946202278137207
Epoch 80, val loss: 1.8258557319641113
Epoch 90, training loss: 15.517053604125977 = 1.8019356727600098 + 2.0 * 6.857558727264404
Epoch 90, val loss: 1.8124951124191284
Epoch 100, training loss: 15.353812217712402 = 1.7855178117752075 + 2.0 * 6.784147262573242
Epoch 100, val loss: 1.7967476844787598
Epoch 110, training loss: 15.222426414489746 = 1.76955246925354 + 2.0 * 6.726437091827393
Epoch 110, val loss: 1.7816293239593506
Epoch 120, training loss: 15.109458923339844 = 1.7543138265609741 + 2.0 * 6.677572727203369
Epoch 120, val loss: 1.7671993970870972
Epoch 130, training loss: 14.999764442443848 = 1.7384759187698364 + 2.0 * 6.63064432144165
Epoch 130, val loss: 1.7522659301757812
Epoch 140, training loss: 14.900062561035156 = 1.7214428186416626 + 2.0 * 6.5893096923828125
Epoch 140, val loss: 1.7365413904190063
Epoch 150, training loss: 14.811317443847656 = 1.7028387784957886 + 2.0 * 6.554239273071289
Epoch 150, val loss: 1.7196186780929565
Epoch 160, training loss: 14.736987113952637 = 1.6820143461227417 + 2.0 * 6.527486324310303
Epoch 160, val loss: 1.7008475065231323
Epoch 170, training loss: 14.677751541137695 = 1.6583415269851685 + 2.0 * 6.509705066680908
Epoch 170, val loss: 1.679685115814209
Epoch 180, training loss: 14.618433952331543 = 1.631531834602356 + 2.0 * 6.493451118469238
Epoch 180, val loss: 1.6559735536575317
Epoch 190, training loss: 14.560050964355469 = 1.6014833450317383 + 2.0 * 6.479283809661865
Epoch 190, val loss: 1.6295030117034912
Epoch 200, training loss: 14.517888069152832 = 1.568334698677063 + 2.0 * 6.474776744842529
Epoch 200, val loss: 1.6004846096038818
Epoch 210, training loss: 14.449195861816406 = 1.5326225757598877 + 2.0 * 6.458286762237549
Epoch 210, val loss: 1.5697860717773438
Epoch 220, training loss: 14.387016296386719 = 1.4944238662719727 + 2.0 * 6.446296215057373
Epoch 220, val loss: 1.5369328260421753
Epoch 230, training loss: 14.327186584472656 = 1.453647494316101 + 2.0 * 6.436769485473633
Epoch 230, val loss: 1.502158761024475
Epoch 240, training loss: 14.272445678710938 = 1.4115134477615356 + 2.0 * 6.430466175079346
Epoch 240, val loss: 1.4665393829345703
Epoch 250, training loss: 14.210538864135742 = 1.3685978651046753 + 2.0 * 6.420970439910889
Epoch 250, val loss: 1.4307568073272705
Epoch 260, training loss: 14.150683403015137 = 1.3250688314437866 + 2.0 * 6.412807464599609
Epoch 260, val loss: 1.3947645425796509
Epoch 270, training loss: 14.093124389648438 = 1.2810912132263184 + 2.0 * 6.4060163497924805
Epoch 270, val loss: 1.3588848114013672
Epoch 280, training loss: 14.036046981811523 = 1.2367868423461914 + 2.0 * 6.399630069732666
Epoch 280, val loss: 1.3232331275939941
Epoch 290, training loss: 13.982466697692871 = 1.1923773288726807 + 2.0 * 6.395044803619385
Epoch 290, val loss: 1.287951111793518
Epoch 300, training loss: 13.92818546295166 = 1.1488498449325562 + 2.0 * 6.389667987823486
Epoch 300, val loss: 1.253624677658081
Epoch 310, training loss: 13.874678611755371 = 1.106431245803833 + 2.0 * 6.384123802185059
Epoch 310, val loss: 1.220351219177246
Epoch 320, training loss: 13.822870254516602 = 1.064578652381897 + 2.0 * 6.379145622253418
Epoch 320, val loss: 1.1876972913742065
Epoch 330, training loss: 13.771978378295898 = 1.0230810642242432 + 2.0 * 6.374448776245117
Epoch 330, val loss: 1.155465006828308
Epoch 340, training loss: 13.733787536621094 = 0.9818727970123291 + 2.0 * 6.375957489013672
Epoch 340, val loss: 1.1237026453018188
Epoch 350, training loss: 13.676346778869629 = 0.9419060945510864 + 2.0 * 6.367220401763916
Epoch 350, val loss: 1.0928120613098145
Epoch 360, training loss: 13.630414962768555 = 0.9030886292457581 + 2.0 * 6.363663196563721
Epoch 360, val loss: 1.062787413597107
Epoch 370, training loss: 13.581587791442871 = 0.8654203414916992 + 2.0 * 6.358083724975586
Epoch 370, val loss: 1.033683180809021
Epoch 380, training loss: 13.539704322814941 = 0.8288449048995972 + 2.0 * 6.355429649353027
Epoch 380, val loss: 1.0056512355804443
Epoch 390, training loss: 13.509201049804688 = 0.7939848899841309 + 2.0 * 6.357607841491699
Epoch 390, val loss: 0.979052722454071
Epoch 400, training loss: 13.457632064819336 = 0.7608661651611328 + 2.0 * 6.348382949829102
Epoch 400, val loss: 0.9540874361991882
Epoch 410, training loss: 13.420517921447754 = 0.7292653918266296 + 2.0 * 6.345626354217529
Epoch 410, val loss: 0.9306433796882629
Epoch 420, training loss: 13.391474723815918 = 0.6990214586257935 + 2.0 * 6.346226692199707
Epoch 420, val loss: 0.9086046814918518
Epoch 430, training loss: 13.353225708007812 = 0.670171320438385 + 2.0 * 6.341526985168457
Epoch 430, val loss: 0.8881372213363647
Epoch 440, training loss: 13.316554069519043 = 0.6427487730979919 + 2.0 * 6.336902618408203
Epoch 440, val loss: 0.8692156672477722
Epoch 450, training loss: 13.285226821899414 = 0.6163814663887024 + 2.0 * 6.334422588348389
Epoch 450, val loss: 0.8517283797264099
Epoch 460, training loss: 13.261333465576172 = 0.5910053849220276 + 2.0 * 6.3351640701293945
Epoch 460, val loss: 0.8356163501739502
Epoch 470, training loss: 13.22717571258545 = 0.5668359398841858 + 2.0 * 6.330169677734375
Epoch 470, val loss: 0.8208584785461426
Epoch 480, training loss: 13.198647499084473 = 0.5434362888336182 + 2.0 * 6.327605724334717
Epoch 480, val loss: 0.8073521852493286
Epoch 490, training loss: 13.171539306640625 = 0.520732045173645 + 2.0 * 6.325403690338135
Epoch 490, val loss: 0.7948693037033081
Epoch 500, training loss: 13.165141105651855 = 0.4986889958381653 + 2.0 * 6.333226203918457
Epoch 500, val loss: 0.7833766937255859
Epoch 510, training loss: 13.124470710754395 = 0.4773194491863251 + 2.0 * 6.323575496673584
Epoch 510, val loss: 0.7729092836380005
Epoch 520, training loss: 13.099367141723633 = 0.45672738552093506 + 2.0 * 6.321320056915283
Epoch 520, val loss: 0.7635500431060791
Epoch 530, training loss: 13.0743989944458 = 0.43670907616615295 + 2.0 * 6.318844795227051
Epoch 530, val loss: 0.7550449967384338
Epoch 540, training loss: 13.0523681640625 = 0.41729870438575745 + 2.0 * 6.317534923553467
Epoch 540, val loss: 0.7473868131637573
Epoch 550, training loss: 13.04208755493164 = 0.39855048060417175 + 2.0 * 6.321768760681152
Epoch 550, val loss: 0.7405794262886047
Epoch 560, training loss: 13.010162353515625 = 0.38050928711891174 + 2.0 * 6.314826488494873
Epoch 560, val loss: 0.7346977591514587
Epoch 570, training loss: 12.987556457519531 = 0.3630404472351074 + 2.0 * 6.312258243560791
Epoch 570, val loss: 0.7296130061149597
Epoch 580, training loss: 12.98007583618164 = 0.34617379307746887 + 2.0 * 6.316950798034668
Epoch 580, val loss: 0.7252278923988342
Epoch 590, training loss: 12.954705238342285 = 0.32991349697113037 + 2.0 * 6.312396049499512
Epoch 590, val loss: 0.7215049862861633
Epoch 600, training loss: 12.930695533752441 = 0.3143245577812195 + 2.0 * 6.308185577392578
Epoch 600, val loss: 0.7185751795768738
Epoch 610, training loss: 12.919099807739258 = 0.2992956340312958 + 2.0 * 6.309902191162109
Epoch 610, val loss: 0.7161862850189209
Epoch 620, training loss: 12.906682014465332 = 0.28490468859672546 + 2.0 * 6.310888767242432
Epoch 620, val loss: 0.7142654061317444
Epoch 630, training loss: 12.881982803344727 = 0.27118629217147827 + 2.0 * 6.305398464202881
Epoch 630, val loss: 0.7129439115524292
Epoch 640, training loss: 12.874198913574219 = 0.2581033706665039 + 2.0 * 6.308047771453857
Epoch 640, val loss: 0.7121758460998535
Epoch 650, training loss: 12.852066040039062 = 0.2455757111310959 + 2.0 * 6.3032450675964355
Epoch 650, val loss: 0.7116913795471191
Epoch 660, training loss: 12.835198402404785 = 0.2336980253458023 + 2.0 * 6.300750255584717
Epoch 660, val loss: 0.7118352651596069
Epoch 670, training loss: 12.823244094848633 = 0.22233381867408752 + 2.0 * 6.300455093383789
Epoch 670, val loss: 0.712325394153595
Epoch 680, training loss: 12.810160636901855 = 0.21156346797943115 + 2.0 * 6.2992987632751465
Epoch 680, val loss: 0.713239312171936
Epoch 690, training loss: 12.796631813049316 = 0.20136044919490814 + 2.0 * 6.297635555267334
Epoch 690, val loss: 0.7145549058914185
Epoch 700, training loss: 12.788323402404785 = 0.19168534874916077 + 2.0 * 6.298318862915039
Epoch 700, val loss: 0.7162770628929138
Epoch 710, training loss: 12.77349853515625 = 0.18254739046096802 + 2.0 * 6.295475482940674
Epoch 710, val loss: 0.7182809710502625
Epoch 720, training loss: 12.76207447052002 = 0.1739126443862915 + 2.0 * 6.29408073425293
Epoch 720, val loss: 0.720695972442627
Epoch 730, training loss: 12.761807441711426 = 0.16573566198349 + 2.0 * 6.298036098480225
Epoch 730, val loss: 0.7233741283416748
Epoch 740, training loss: 12.75252914428711 = 0.1580067276954651 + 2.0 * 6.2972612380981445
Epoch 740, val loss: 0.7262060642242432
Epoch 750, training loss: 12.733174324035645 = 0.15077097713947296 + 2.0 * 6.291201591491699
Epoch 750, val loss: 0.7294601798057556
Epoch 760, training loss: 12.727693557739258 = 0.14392229914665222 + 2.0 * 6.291885852813721
Epoch 760, val loss: 0.7328857779502869
Epoch 770, training loss: 12.71759033203125 = 0.13745208084583282 + 2.0 * 6.290069103240967
Epoch 770, val loss: 0.7365540862083435
Epoch 780, training loss: 12.707697868347168 = 0.13132528960704803 + 2.0 * 6.288186073303223
Epoch 780, val loss: 0.7403910756111145
Epoch 790, training loss: 12.69914722442627 = 0.1255420744419098 + 2.0 * 6.286802768707275
Epoch 790, val loss: 0.7444122433662415
Epoch 800, training loss: 12.721248626708984 = 0.12005668878555298 + 2.0 * 6.300595760345459
Epoch 800, val loss: 0.7485321760177612
Epoch 810, training loss: 12.685744285583496 = 0.11492441594600677 + 2.0 * 6.285409927368164
Epoch 810, val loss: 0.7528280019760132
Epoch 820, training loss: 12.679759979248047 = 0.11006323993206024 + 2.0 * 6.284848213195801
Epoch 820, val loss: 0.7571930885314941
Epoch 830, training loss: 12.672199249267578 = 0.10544724017381668 + 2.0 * 6.283376216888428
Epoch 830, val loss: 0.7616506218910217
Epoch 840, training loss: 12.685664176940918 = 0.10108207911252975 + 2.0 * 6.292291164398193
Epoch 840, val loss: 0.7661868333816528
Epoch 850, training loss: 12.662282943725586 = 0.09697648882865906 + 2.0 * 6.282653331756592
Epoch 850, val loss: 0.7708407044410706
Epoch 860, training loss: 12.653905868530273 = 0.09307067841291428 + 2.0 * 6.280417442321777
Epoch 860, val loss: 0.7755265831947327
Epoch 870, training loss: 12.654898643493652 = 0.08936936408281326 + 2.0 * 6.282764434814453
Epoch 870, val loss: 0.780232310295105
Epoch 880, training loss: 12.646316528320312 = 0.0858561173081398 + 2.0 * 6.2802300453186035
Epoch 880, val loss: 0.7850528955459595
Epoch 890, training loss: 12.641783714294434 = 0.08253413438796997 + 2.0 * 6.279624938964844
Epoch 890, val loss: 0.7898513078689575
Epoch 900, training loss: 12.633489608764648 = 0.07937590777873993 + 2.0 * 6.277056694030762
Epoch 900, val loss: 0.7947049140930176
Epoch 910, training loss: 12.632660865783691 = 0.07637246698141098 + 2.0 * 6.278144359588623
Epoch 910, val loss: 0.799594521522522
Epoch 920, training loss: 12.627867698669434 = 0.07352067530155182 + 2.0 * 6.2771735191345215
Epoch 920, val loss: 0.8044412732124329
Epoch 930, training loss: 12.631709098815918 = 0.07082182914018631 + 2.0 * 6.2804436683654785
Epoch 930, val loss: 0.8093099594116211
Epoch 940, training loss: 12.61888599395752 = 0.06825011968612671 + 2.0 * 6.275318145751953
Epoch 940, val loss: 0.8140859007835388
Epoch 950, training loss: 12.613606452941895 = 0.06581424921751022 + 2.0 * 6.273896217346191
Epoch 950, val loss: 0.8188939690589905
Epoch 960, training loss: 12.614924430847168 = 0.06349215656518936 + 2.0 * 6.275716304779053
Epoch 960, val loss: 0.8237361311912537
Epoch 970, training loss: 12.607610702514648 = 0.06127312779426575 + 2.0 * 6.273168563842773
Epoch 970, val loss: 0.8285205364227295
Epoch 980, training loss: 12.603734016418457 = 0.059169042855501175 + 2.0 * 6.272282600402832
Epoch 980, val loss: 0.8333005309104919
Epoch 990, training loss: 12.60088062286377 = 0.05716226249933243 + 2.0 * 6.271859169006348
Epoch 990, val loss: 0.8380610346794128
Epoch 1000, training loss: 12.603827476501465 = 0.05524459853768349 + 2.0 * 6.274291515350342
Epoch 1000, val loss: 0.8427969813346863
Epoch 1010, training loss: 12.595088958740234 = 0.05340944230556488 + 2.0 * 6.270839691162109
Epoch 1010, val loss: 0.8474884033203125
Epoch 1020, training loss: 12.591144561767578 = 0.05166688561439514 + 2.0 * 6.269738674163818
Epoch 1020, val loss: 0.8521737456321716
Epoch 1030, training loss: 12.589658737182617 = 0.049998193979263306 + 2.0 * 6.269830226898193
Epoch 1030, val loss: 0.8568350076675415
Epoch 1040, training loss: 12.594388008117676 = 0.048401813954114914 + 2.0 * 6.272993087768555
Epoch 1040, val loss: 0.8614516258239746
Epoch 1050, training loss: 12.589841842651367 = 0.04688451066613197 + 2.0 * 6.271478652954102
Epoch 1050, val loss: 0.8660705089569092
Epoch 1060, training loss: 12.582887649536133 = 0.04542379826307297 + 2.0 * 6.268732070922852
Epoch 1060, val loss: 0.8706337213516235
Epoch 1070, training loss: 12.578608512878418 = 0.04403117299079895 + 2.0 * 6.267288684844971
Epoch 1070, val loss: 0.8752036690711975
Epoch 1080, training loss: 12.574958801269531 = 0.04269805923104286 + 2.0 * 6.266130447387695
Epoch 1080, val loss: 0.8796873092651367
Epoch 1090, training loss: 12.593006134033203 = 0.04141981899738312 + 2.0 * 6.275793075561523
Epoch 1090, val loss: 0.8841660618782043
Epoch 1100, training loss: 12.576834678649902 = 0.04018961638212204 + 2.0 * 6.268322467803955
Epoch 1100, val loss: 0.8885809183120728
Epoch 1110, training loss: 12.56751537322998 = 0.03901936858892441 + 2.0 * 6.264247894287109
Epoch 1110, val loss: 0.8929470777511597
Epoch 1120, training loss: 12.564970016479492 = 0.037895288318395615 + 2.0 * 6.263537406921387
Epoch 1120, val loss: 0.8972849249839783
Epoch 1130, training loss: 12.578608512878418 = 0.03681051731109619 + 2.0 * 6.270898818969727
Epoch 1130, val loss: 0.9016444683074951
Epoch 1140, training loss: 12.569931030273438 = 0.03577587381005287 + 2.0 * 6.267077445983887
Epoch 1140, val loss: 0.9058506488800049
Epoch 1150, training loss: 12.57009220123291 = 0.03477875143289566 + 2.0 * 6.2676568031311035
Epoch 1150, val loss: 0.9100784659385681
Epoch 1160, training loss: 12.557653427124023 = 0.03382986783981323 + 2.0 * 6.261911869049072
Epoch 1160, val loss: 0.9142241477966309
Epoch 1170, training loss: 12.556717872619629 = 0.03291431814432144 + 2.0 * 6.26190185546875
Epoch 1170, val loss: 0.9183434247970581
Epoch 1180, training loss: 12.553082466125488 = 0.03202975168824196 + 2.0 * 6.260526180267334
Epoch 1180, val loss: 0.9224647283554077
Epoch 1190, training loss: 12.59351634979248 = 0.031182002276182175 + 2.0 * 6.281167030334473
Epoch 1190, val loss: 0.9265769124031067
Epoch 1200, training loss: 12.550816535949707 = 0.03036247380077839 + 2.0 * 6.260227203369141
Epoch 1200, val loss: 0.9305253028869629
Epoch 1210, training loss: 12.550936698913574 = 0.029582155868411064 + 2.0 * 6.260677337646484
Epoch 1210, val loss: 0.9344426989555359
Epoch 1220, training loss: 12.546807289123535 = 0.02882920578122139 + 2.0 * 6.258988857269287
Epoch 1220, val loss: 0.9383378624916077
Epoch 1230, training loss: 12.544408798217773 = 0.028100287541747093 + 2.0 * 6.258154392242432
Epoch 1230, val loss: 0.9422177076339722
Epoch 1240, training loss: 12.549263954162598 = 0.027396587654948235 + 2.0 * 6.260933876037598
Epoch 1240, val loss: 0.9461121559143066
Epoch 1250, training loss: 12.545907974243164 = 0.026714308187365532 + 2.0 * 6.259596824645996
Epoch 1250, val loss: 0.9499234557151794
Epoch 1260, training loss: 12.550758361816406 = 0.026062650606036186 + 2.0 * 6.26234769821167
Epoch 1260, val loss: 0.9537026882171631
Epoch 1270, training loss: 12.539793968200684 = 0.025438256561756134 + 2.0 * 6.257177829742432
Epoch 1270, val loss: 0.9573622941970825
Epoch 1280, training loss: 12.537251472473145 = 0.02483450248837471 + 2.0 * 6.256208419799805
Epoch 1280, val loss: 0.960995614528656
Epoch 1290, training loss: 12.535305976867676 = 0.024249568581581116 + 2.0 * 6.255527973175049
Epoch 1290, val loss: 0.9646642804145813
Epoch 1300, training loss: 12.540578842163086 = 0.02368156798183918 + 2.0 * 6.258448600769043
Epoch 1300, val loss: 0.9682786464691162
Epoch 1310, training loss: 12.534789085388184 = 0.023134008049964905 + 2.0 * 6.2558274269104
Epoch 1310, val loss: 0.971829354763031
Epoch 1320, training loss: 12.530595779418945 = 0.022607874125242233 + 2.0 * 6.253993988037109
Epoch 1320, val loss: 0.9753583073616028
Epoch 1330, training loss: 12.529478073120117 = 0.022098584100604057 + 2.0 * 6.253689765930176
Epoch 1330, val loss: 0.9788388013839722
Epoch 1340, training loss: 12.546005249023438 = 0.02160511165857315 + 2.0 * 6.262199878692627
Epoch 1340, val loss: 0.9823126792907715
Epoch 1350, training loss: 12.537358283996582 = 0.02112768031656742 + 2.0 * 6.258115291595459
Epoch 1350, val loss: 0.9856084585189819
Epoch 1360, training loss: 12.528368949890137 = 0.020668916404247284 + 2.0 * 6.253849983215332
Epoch 1360, val loss: 0.9890069961547852
Epoch 1370, training loss: 12.523896217346191 = 0.020225070416927338 + 2.0 * 6.251835346221924
Epoch 1370, val loss: 0.9923055171966553
Epoch 1380, training loss: 12.52470588684082 = 0.01979357749223709 + 2.0 * 6.252456188201904
Epoch 1380, val loss: 0.995607852935791
Epoch 1390, training loss: 12.536765098571777 = 0.019376251846551895 + 2.0 * 6.258694648742676
Epoch 1390, val loss: 0.9988781809806824
Epoch 1400, training loss: 12.52690601348877 = 0.018972521647810936 + 2.0 * 6.253966808319092
Epoch 1400, val loss: 1.0020725727081299
Epoch 1410, training loss: 12.520185470581055 = 0.01858185976743698 + 2.0 * 6.250802040100098
Epoch 1410, val loss: 1.005241870880127
Epoch 1420, training loss: 12.519855499267578 = 0.018202995881438255 + 2.0 * 6.250826358795166
Epoch 1420, val loss: 1.0083577632904053
Epoch 1430, training loss: 12.523569107055664 = 0.017835162580013275 + 2.0 * 6.252866744995117
Epoch 1430, val loss: 1.0114727020263672
Epoch 1440, training loss: 12.524441719055176 = 0.017480425536632538 + 2.0 * 6.253480434417725
Epoch 1440, val loss: 1.014557957649231
Epoch 1450, training loss: 12.522128105163574 = 0.017135530710220337 + 2.0 * 6.252496242523193
Epoch 1450, val loss: 1.017573356628418
Epoch 1460, training loss: 12.515254020690918 = 0.01680092141032219 + 2.0 * 6.2492265701293945
Epoch 1460, val loss: 1.0206012725830078
Epoch 1470, training loss: 12.511630058288574 = 0.016477003693580627 + 2.0 * 6.247576713562012
Epoch 1470, val loss: 1.023545503616333
Epoch 1480, training loss: 12.511418342590332 = 0.016161756590008736 + 2.0 * 6.247628211975098
Epoch 1480, val loss: 1.0264859199523926
Epoch 1490, training loss: 12.528119087219238 = 0.01585559733211994 + 2.0 * 6.256131649017334
Epoch 1490, val loss: 1.0294033288955688
Epoch 1500, training loss: 12.521707534790039 = 0.01555468700826168 + 2.0 * 6.253076553344727
Epoch 1500, val loss: 1.032391905784607
Epoch 1510, training loss: 12.50981330871582 = 0.015268930234014988 + 2.0 * 6.24727201461792
Epoch 1510, val loss: 1.0351881980895996
Epoch 1520, training loss: 12.507278442382812 = 0.01499001681804657 + 2.0 * 6.2461442947387695
Epoch 1520, val loss: 1.0379691123962402
Epoch 1530, training loss: 12.509297370910645 = 0.014718233607709408 + 2.0 * 6.247289657592773
Epoch 1530, val loss: 1.0407575368881226
Epoch 1540, training loss: 12.514019966125488 = 0.01445303950458765 + 2.0 * 6.249783515930176
Epoch 1540, val loss: 1.0435113906860352
Epoch 1550, training loss: 12.508705139160156 = 0.014193055219948292 + 2.0 * 6.247256278991699
Epoch 1550, val loss: 1.046281337738037
Epoch 1560, training loss: 12.513813018798828 = 0.013942491263151169 + 2.0 * 6.249935150146484
Epoch 1560, val loss: 1.0489970445632935
Epoch 1570, training loss: 12.503764152526855 = 0.0137000298127532 + 2.0 * 6.245031833648682
Epoch 1570, val loss: 1.051626443862915
Epoch 1580, training loss: 12.501286506652832 = 0.013463937677443027 + 2.0 * 6.243911266326904
Epoch 1580, val loss: 1.054227352142334
Epoch 1590, training loss: 12.50156021118164 = 0.013234162703156471 + 2.0 * 6.2441630363464355
Epoch 1590, val loss: 1.0568394660949707
Epoch 1600, training loss: 12.509912490844727 = 0.013009591028094292 + 2.0 * 6.248451232910156
Epoch 1600, val loss: 1.0594450235366821
Epoch 1610, training loss: 12.502191543579102 = 0.012789982371032238 + 2.0 * 6.244700908660889
Epoch 1610, val loss: 1.0620554685592651
Epoch 1620, training loss: 12.50498104095459 = 0.012577375397086143 + 2.0 * 6.246201992034912
Epoch 1620, val loss: 1.064637541770935
Epoch 1630, training loss: 12.502120971679688 = 0.012370648793876171 + 2.0 * 6.244874954223633
Epoch 1630, val loss: 1.0671058893203735
Epoch 1640, training loss: 12.510531425476074 = 0.012168750166893005 + 2.0 * 6.249181270599365
Epoch 1640, val loss: 1.0695843696594238
Epoch 1650, training loss: 12.498995780944824 = 0.011971951462328434 + 2.0 * 6.24351167678833
Epoch 1650, val loss: 1.0720436573028564
Epoch 1660, training loss: 12.494771003723145 = 0.011780643835663795 + 2.0 * 6.241495132446289
Epoch 1660, val loss: 1.0744872093200684
Epoch 1670, training loss: 12.493878364562988 = 0.011593842878937721 + 2.0 * 6.241142272949219
Epoch 1670, val loss: 1.0768846273422241
Epoch 1680, training loss: 12.498514175415039 = 0.011410852894186974 + 2.0 * 6.243551731109619
Epoch 1680, val loss: 1.07931387424469
Epoch 1690, training loss: 12.494574546813965 = 0.011232218705117702 + 2.0 * 6.241671085357666
Epoch 1690, val loss: 1.0817021131515503
Epoch 1700, training loss: 12.501888275146484 = 0.011058736592531204 + 2.0 * 6.245414733886719
Epoch 1700, val loss: 1.0840356349945068
Epoch 1710, training loss: 12.495222091674805 = 0.010890988633036613 + 2.0 * 6.242165565490723
Epoch 1710, val loss: 1.086344599723816
Epoch 1720, training loss: 12.490344047546387 = 0.01072700135409832 + 2.0 * 6.239808559417725
Epoch 1720, val loss: 1.0886075496673584
Epoch 1730, training loss: 12.49122428894043 = 0.010566063225269318 + 2.0 * 6.240329265594482
Epoch 1730, val loss: 1.0908615589141846
Epoch 1740, training loss: 12.51246452331543 = 0.01040967833250761 + 2.0 * 6.251027584075928
Epoch 1740, val loss: 1.093105435371399
Epoch 1750, training loss: 12.492725372314453 = 0.01025509275496006 + 2.0 * 6.241235256195068
Epoch 1750, val loss: 1.0953325033187866
Epoch 1760, training loss: 12.487010955810547 = 0.010106834582984447 + 2.0 * 6.238451957702637
Epoch 1760, val loss: 1.0974721908569336
Epoch 1770, training loss: 12.486437797546387 = 0.009961606003344059 + 2.0 * 6.2382378578186035
Epoch 1770, val loss: 1.0996179580688477
Epoch 1780, training loss: 12.486888885498047 = 0.00981876254081726 + 2.0 * 6.238534927368164
Epoch 1780, val loss: 1.1017422676086426
Epoch 1790, training loss: 12.497456550598145 = 0.0096788564696908 + 2.0 * 6.243888854980469
Epoch 1790, val loss: 1.1038824319839478
Epoch 1800, training loss: 12.495427131652832 = 0.00954090803861618 + 2.0 * 6.242943286895752
Epoch 1800, val loss: 1.1060397624969482
Epoch 1810, training loss: 12.487569808959961 = 0.009409140795469284 + 2.0 * 6.239080429077148
Epoch 1810, val loss: 1.1081417798995972
Epoch 1820, training loss: 12.483877182006836 = 0.009278704412281513 + 2.0 * 6.23729944229126
Epoch 1820, val loss: 1.110181450843811
Epoch 1830, training loss: 12.481698036193848 = 0.009151531383395195 + 2.0 * 6.236273288726807
Epoch 1830, val loss: 1.1122287511825562
Epoch 1840, training loss: 12.483102798461914 = 0.009027144871652126 + 2.0 * 6.237037658691406
Epoch 1840, val loss: 1.1142557859420776
Epoch 1850, training loss: 12.502266883850098 = 0.008905723690986633 + 2.0 * 6.246680736541748
Epoch 1850, val loss: 1.1162769794464111
Epoch 1860, training loss: 12.487069129943848 = 0.008784433826804161 + 2.0 * 6.239142417907715
Epoch 1860, val loss: 1.1183278560638428
Epoch 1870, training loss: 12.481722831726074 = 0.008668770082294941 + 2.0 * 6.236526966094971
Epoch 1870, val loss: 1.1202434301376343
Epoch 1880, training loss: 12.488930702209473 = 0.008554201573133469 + 2.0 * 6.240188121795654
Epoch 1880, val loss: 1.1222258806228638
Epoch 1890, training loss: 12.479791641235352 = 0.008442599326372147 + 2.0 * 6.2356743812561035
Epoch 1890, val loss: 1.1241282224655151
Epoch 1900, training loss: 12.479509353637695 = 0.008333575911819935 + 2.0 * 6.235588073730469
Epoch 1900, val loss: 1.126037836074829
Epoch 1910, training loss: 12.483596801757812 = 0.008226561360061169 + 2.0 * 6.237685203552246
Epoch 1910, val loss: 1.1279244422912598
Epoch 1920, training loss: 12.47805404663086 = 0.008121524937450886 + 2.0 * 6.234966278076172
Epoch 1920, val loss: 1.1297667026519775
Epoch 1930, training loss: 12.481019020080566 = 0.00801839865744114 + 2.0 * 6.236500263214111
Epoch 1930, val loss: 1.1316157579421997
Epoch 1940, training loss: 12.485305786132812 = 0.007917978800833225 + 2.0 * 6.238693714141846
Epoch 1940, val loss: 1.1334766149520874
Epoch 1950, training loss: 12.480180740356445 = 0.007819408550858498 + 2.0 * 6.236180782318115
Epoch 1950, val loss: 1.1353402137756348
Epoch 1960, training loss: 12.475064277648926 = 0.007722916081547737 + 2.0 * 6.233670711517334
Epoch 1960, val loss: 1.137096643447876
Epoch 1970, training loss: 12.475825309753418 = 0.007628560531884432 + 2.0 * 6.234098434448242
Epoch 1970, val loss: 1.1388912200927734
Epoch 1980, training loss: 12.486186981201172 = 0.0075361900962889194 + 2.0 * 6.239325523376465
Epoch 1980, val loss: 1.140669584274292
Epoch 1990, training loss: 12.477706909179688 = 0.007444382645189762 + 2.0 * 6.23513126373291
Epoch 1990, val loss: 1.142393708229065
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.8185185185185185
0.8371112282551397
=== training gcn model ===
Epoch 0, training loss: 19.138303756713867 = 1.9445279836654663 + 2.0 * 8.596887588500977
Epoch 0, val loss: 1.942850112915039
Epoch 10, training loss: 19.128761291503906 = 1.9353950023651123 + 2.0 * 8.596683502197266
Epoch 10, val loss: 1.9337682723999023
Epoch 20, training loss: 19.113384246826172 = 1.9239472150802612 + 2.0 * 8.594718933105469
Epoch 20, val loss: 1.9222726821899414
Epoch 30, training loss: 19.064172744750977 = 1.90809166431427 + 2.0 * 8.57804012298584
Epoch 30, val loss: 1.9064167737960815
Epoch 40, training loss: 18.87250518798828 = 1.8866901397705078 + 2.0 * 8.492907524108887
Epoch 40, val loss: 1.885684609413147
Epoch 50, training loss: 18.193592071533203 = 1.8633334636688232 + 2.0 * 8.165129661560059
Epoch 50, val loss: 1.8633663654327393
Epoch 60, training loss: 17.5538330078125 = 1.8382729291915894 + 2.0 * 7.8577799797058105
Epoch 60, val loss: 1.840692162513733
Epoch 70, training loss: 16.85218048095703 = 1.8166295289993286 + 2.0 * 7.517775535583496
Epoch 70, val loss: 1.8211759328842163
Epoch 80, training loss: 16.34339141845703 = 1.7995803356170654 + 2.0 * 7.271905422210693
Epoch 80, val loss: 1.8051528930664062
Epoch 90, training loss: 15.946471214294434 = 1.7833175659179688 + 2.0 * 7.081576824188232
Epoch 90, val loss: 1.7899333238601685
Epoch 100, training loss: 15.706567764282227 = 1.7649047374725342 + 2.0 * 6.970831394195557
Epoch 100, val loss: 1.7734025716781616
Epoch 110, training loss: 15.574535369873047 = 1.7448731660842896 + 2.0 * 6.914831161499023
Epoch 110, val loss: 1.755833387374878
Epoch 120, training loss: 15.456490516662598 = 1.7243865728378296 + 2.0 * 6.866052150726318
Epoch 120, val loss: 1.7375974655151367
Epoch 130, training loss: 15.329999923706055 = 1.703660011291504 + 2.0 * 6.813169956207275
Epoch 130, val loss: 1.719349980354309
Epoch 140, training loss: 15.19974422454834 = 1.6824519634246826 + 2.0 * 6.758646011352539
Epoch 140, val loss: 1.701206088066101
Epoch 150, training loss: 15.087634086608887 = 1.659369707107544 + 2.0 * 6.714132308959961
Epoch 150, val loss: 1.68155837059021
Epoch 160, training loss: 14.988479614257812 = 1.6328163146972656 + 2.0 * 6.677831649780273
Epoch 160, val loss: 1.6587841510772705
Epoch 170, training loss: 14.899791717529297 = 1.6024832725524902 + 2.0 * 6.648654460906982
Epoch 170, val loss: 1.632872223854065
Epoch 180, training loss: 14.813385963439941 = 1.5687235593795776 + 2.0 * 6.622331142425537
Epoch 180, val loss: 1.6041369438171387
Epoch 190, training loss: 14.726231575012207 = 1.5319713354110718 + 2.0 * 6.597130298614502
Epoch 190, val loss: 1.5733243227005005
Epoch 200, training loss: 14.642778396606445 = 1.491658091545105 + 2.0 * 6.575560092926025
Epoch 200, val loss: 1.5397871732711792
Epoch 210, training loss: 14.567668914794922 = 1.4481227397918701 + 2.0 * 6.559772968292236
Epoch 210, val loss: 1.5036909580230713
Epoch 220, training loss: 14.486344337463379 = 1.4024626016616821 + 2.0 * 6.541940689086914
Epoch 220, val loss: 1.4659314155578613
Epoch 230, training loss: 14.40678596496582 = 1.355286717414856 + 2.0 * 6.525749683380127
Epoch 230, val loss: 1.427467942237854
Epoch 240, training loss: 14.336316108703613 = 1.3072071075439453 + 2.0 * 6.514554500579834
Epoch 240, val loss: 1.3888696432113647
Epoch 250, training loss: 14.25952434539795 = 1.259149432182312 + 2.0 * 6.500187397003174
Epoch 250, val loss: 1.3506369590759277
Epoch 260, training loss: 14.185829162597656 = 1.2110122442245483 + 2.0 * 6.487408638000488
Epoch 260, val loss: 1.3129469156265259
Epoch 270, training loss: 14.126453399658203 = 1.1636956930160522 + 2.0 * 6.48137903213501
Epoch 270, val loss: 1.2761045694351196
Epoch 280, training loss: 14.055071830749512 = 1.118047833442688 + 2.0 * 6.468512058258057
Epoch 280, val loss: 1.241276502609253
Epoch 290, training loss: 13.99257755279541 = 1.0742456912994385 + 2.0 * 6.459166049957275
Epoch 290, val loss: 1.2081704139709473
Epoch 300, training loss: 13.93272590637207 = 1.0317729711532593 + 2.0 * 6.45047664642334
Epoch 300, val loss: 1.176366925239563
Epoch 310, training loss: 13.880395889282227 = 0.9905816316604614 + 2.0 * 6.444907188415527
Epoch 310, val loss: 1.1458638906478882
Epoch 320, training loss: 13.828673362731934 = 0.9509589672088623 + 2.0 * 6.438857078552246
Epoch 320, val loss: 1.1171256303787231
Epoch 330, training loss: 13.771702766418457 = 0.9132168292999268 + 2.0 * 6.429243087768555
Epoch 330, val loss: 1.0899362564086914
Epoch 340, training loss: 13.722617149353027 = 0.8769280910491943 + 2.0 * 6.422844409942627
Epoch 340, val loss: 1.0640556812286377
Epoch 350, training loss: 13.678345680236816 = 0.8419819474220276 + 2.0 * 6.418181896209717
Epoch 350, val loss: 1.0396959781646729
Epoch 360, training loss: 13.634040832519531 = 0.8089497685432434 + 2.0 * 6.412545680999756
Epoch 360, val loss: 1.0169419050216675
Epoch 370, training loss: 13.587696075439453 = 0.7773876190185547 + 2.0 * 6.405154228210449
Epoch 370, val loss: 0.9957136511802673
Epoch 380, training loss: 13.551616668701172 = 0.7471616268157959 + 2.0 * 6.402227401733398
Epoch 380, val loss: 0.9759457111358643
Epoch 390, training loss: 13.513466835021973 = 0.7183777689933777 + 2.0 * 6.3975443840026855
Epoch 390, val loss: 0.9578126072883606
Epoch 400, training loss: 13.476461410522461 = 0.6911899447441101 + 2.0 * 6.392635822296143
Epoch 400, val loss: 0.9411771893501282
Epoch 410, training loss: 13.440797805786133 = 0.6652661561965942 + 2.0 * 6.387765884399414
Epoch 410, val loss: 0.9259814620018005
Epoch 420, training loss: 13.409477233886719 = 0.640545666217804 + 2.0 * 6.38446569442749
Epoch 420, val loss: 0.9122837781906128
Epoch 430, training loss: 13.381141662597656 = 0.6171202659606934 + 2.0 * 6.382010459899902
Epoch 430, val loss: 0.8999223709106445
Epoch 440, training loss: 13.346151351928711 = 0.5946931838989258 + 2.0 * 6.375729084014893
Epoch 440, val loss: 0.8888331651687622
Epoch 450, training loss: 13.321418762207031 = 0.5731412768363953 + 2.0 * 6.374138832092285
Epoch 450, val loss: 0.8788948059082031
Epoch 460, training loss: 13.299399375915527 = 0.5526130795478821 + 2.0 * 6.3733930587768555
Epoch 460, val loss: 0.8700240254402161
Epoch 470, training loss: 13.265852928161621 = 0.5329037308692932 + 2.0 * 6.366474628448486
Epoch 470, val loss: 0.8620925545692444
Epoch 480, training loss: 13.240025520324707 = 0.5138846635818481 + 2.0 * 6.363070487976074
Epoch 480, val loss: 0.8549851179122925
Epoch 490, training loss: 13.224260330200195 = 0.49543049931526184 + 2.0 * 6.364414691925049
Epoch 490, val loss: 0.8486319184303284
Epoch 500, training loss: 13.196306228637695 = 0.47760888934135437 + 2.0 * 6.359348773956299
Epoch 500, val loss: 0.8428926467895508
Epoch 510, training loss: 13.174617767333984 = 0.46020588278770447 + 2.0 * 6.357205867767334
Epoch 510, val loss: 0.8377220630645752
Epoch 520, training loss: 13.148594856262207 = 0.44322994351387024 + 2.0 * 6.352682590484619
Epoch 520, val loss: 0.8329662680625916
Epoch 530, training loss: 13.13031005859375 = 0.42648619413375854 + 2.0 * 6.351912021636963
Epoch 530, val loss: 0.8286149501800537
Epoch 540, training loss: 13.10934066772461 = 0.40995922684669495 + 2.0 * 6.349690914154053
Epoch 540, val loss: 0.824705183506012
Epoch 550, training loss: 13.08578872680664 = 0.3936949372291565 + 2.0 * 6.3460469245910645
Epoch 550, val loss: 0.8210298418998718
Epoch 560, training loss: 13.06567096710205 = 0.37758180499076843 + 2.0 * 6.3440446853637695
Epoch 560, val loss: 0.8175933957099915
Epoch 570, training loss: 13.054064750671387 = 0.3616134524345398 + 2.0 * 6.346225738525391
Epoch 570, val loss: 0.8143138289451599
Epoch 580, training loss: 13.037911415100098 = 0.3457419276237488 + 2.0 * 6.3460845947265625
Epoch 580, val loss: 0.8113296031951904
Epoch 590, training loss: 13.00893783569336 = 0.33020612597465515 + 2.0 * 6.3393659591674805
Epoch 590, val loss: 0.8083763122558594
Epoch 600, training loss: 12.987638473510742 = 0.31481674313545227 + 2.0 * 6.336410999298096
Epoch 600, val loss: 0.805655300617218
Epoch 610, training loss: 12.968989372253418 = 0.2995585799217224 + 2.0 * 6.334715366363525
Epoch 610, val loss: 0.8031587600708008
Epoch 620, training loss: 12.983662605285645 = 0.28455880284309387 + 2.0 * 6.349551677703857
Epoch 620, val loss: 0.8008152842521667
Epoch 630, training loss: 12.938031196594238 = 0.26986178755760193 + 2.0 * 6.334084510803223
Epoch 630, val loss: 0.7988068461418152
Epoch 640, training loss: 12.916303634643555 = 0.2556196451187134 + 2.0 * 6.330341815948486
Epoch 640, val loss: 0.7970033288002014
Epoch 650, training loss: 12.89864444732666 = 0.24176375567913055 + 2.0 * 6.328440189361572
Epoch 650, val loss: 0.7957103252410889
Epoch 660, training loss: 12.88420295715332 = 0.22836284339427948 + 2.0 * 6.327919960021973
Epoch 660, val loss: 0.7948907613754272
Epoch 670, training loss: 12.873062133789062 = 0.2155255824327469 + 2.0 * 6.328768253326416
Epoch 670, val loss: 0.7945089936256409
Epoch 680, training loss: 12.853327751159668 = 0.20335568487644196 + 2.0 * 6.324985980987549
Epoch 680, val loss: 0.7945839762687683
Epoch 690, training loss: 12.836739540100098 = 0.19178766012191772 + 2.0 * 6.322475910186768
Epoch 690, val loss: 0.7952736020088196
Epoch 700, training loss: 12.828147888183594 = 0.18081709742546082 + 2.0 * 6.323665618896484
Epoch 700, val loss: 0.7966326475143433
Epoch 710, training loss: 12.818434715270996 = 0.17054568231105804 + 2.0 * 6.323944568634033
Epoch 710, val loss: 0.7982699275016785
Epoch 720, training loss: 12.797674179077148 = 0.16089209914207458 + 2.0 * 6.318390846252441
Epoch 720, val loss: 0.8004869222640991
Epoch 730, training loss: 12.787101745605469 = 0.15185125172138214 + 2.0 * 6.317625045776367
Epoch 730, val loss: 0.8033077716827393
Epoch 740, training loss: 12.774209976196289 = 0.1433623731136322 + 2.0 * 6.315423965454102
Epoch 740, val loss: 0.8066639304161072
Epoch 750, training loss: 12.772919654846191 = 0.13539563119411469 + 2.0 * 6.318761825561523
Epoch 750, val loss: 0.8104382753372192
Epoch 760, training loss: 12.768571853637695 = 0.12800602614879608 + 2.0 * 6.320282936096191
Epoch 760, val loss: 0.8146324157714844
Epoch 770, training loss: 12.74882984161377 = 0.1211218535900116 + 2.0 * 6.313854217529297
Epoch 770, val loss: 0.8190114498138428
Epoch 780, training loss: 12.737242698669434 = 0.11469253152608871 + 2.0 * 6.311275005340576
Epoch 780, val loss: 0.8237946629524231
Epoch 790, training loss: 12.744425773620605 = 0.10868818312883377 + 2.0 * 6.317868709564209
Epoch 790, val loss: 0.8289170265197754
Epoch 800, training loss: 12.724577903747559 = 0.10307325422763824 + 2.0 * 6.3107523918151855
Epoch 800, val loss: 0.8344025015830994
Epoch 810, training loss: 12.711909294128418 = 0.09786992520093918 + 2.0 * 6.3070197105407715
Epoch 810, val loss: 0.83985435962677
Epoch 820, training loss: 12.705434799194336 = 0.09299338608980179 + 2.0 * 6.306220531463623
Epoch 820, val loss: 0.8457092642784119
Epoch 830, training loss: 12.710426330566406 = 0.0884476900100708 + 2.0 * 6.3109893798828125
Epoch 830, val loss: 0.8517518639564514
Epoch 840, training loss: 12.696982383728027 = 0.08425247669219971 + 2.0 * 6.306365013122559
Epoch 840, val loss: 0.8575605154037476
Epoch 850, training loss: 12.684266090393066 = 0.08031820505857468 + 2.0 * 6.301973819732666
Epoch 850, val loss: 0.8636558055877686
Epoch 860, training loss: 12.678654670715332 = 0.07663507014513016 + 2.0 * 6.301009654998779
Epoch 860, val loss: 0.8699201345443726
Epoch 870, training loss: 12.704264640808105 = 0.0731884092092514 + 2.0 * 6.315537929534912
Epoch 870, val loss: 0.876198410987854
Epoch 880, training loss: 12.66702651977539 = 0.06996341794729233 + 2.0 * 6.298531532287598
Epoch 880, val loss: 0.8825684189796448
Epoch 890, training loss: 12.664055824279785 = 0.06695906072854996 + 2.0 * 6.298548221588135
Epoch 890, val loss: 0.8888278007507324
Epoch 900, training loss: 12.659177780151367 = 0.06412945687770844 + 2.0 * 6.2975239753723145
Epoch 900, val loss: 0.8952223658561707
Epoch 910, training loss: 12.655956268310547 = 0.0614689402282238 + 2.0 * 6.297243595123291
Epoch 910, val loss: 0.9017274975776672
Epoch 920, training loss: 12.653319358825684 = 0.05898641422390938 + 2.0 * 6.297166347503662
Epoch 920, val loss: 0.9078726172447205
Epoch 930, training loss: 12.643229484558105 = 0.05665206536650658 + 2.0 * 6.293288707733154
Epoch 930, val loss: 0.9140618443489075
Epoch 940, training loss: 12.65079116821289 = 0.05444802716374397 + 2.0 * 6.298171520233154
Epoch 940, val loss: 0.920352578163147
Epoch 950, training loss: 12.63681697845459 = 0.05235857143998146 + 2.0 * 6.292229175567627
Epoch 950, val loss: 0.9268043041229248
Epoch 960, training loss: 12.631470680236816 = 0.05040091648697853 + 2.0 * 6.290534973144531
Epoch 960, val loss: 0.9328246712684631
Epoch 970, training loss: 12.631098747253418 = 0.04854333773255348 + 2.0 * 6.291277885437012
Epoch 970, val loss: 0.9390940070152283
Epoch 980, training loss: 12.627459526062012 = 0.04678691178560257 + 2.0 * 6.2903361320495605
Epoch 980, val loss: 0.945294201374054
Epoch 990, training loss: 12.627751350402832 = 0.04512570798397064 + 2.0 * 6.2913126945495605
Epoch 990, val loss: 0.9514402151107788
Epoch 1000, training loss: 12.616263389587402 = 0.04355365037918091 + 2.0 * 6.286355018615723
Epoch 1000, val loss: 0.9573732614517212
Epoch 1010, training loss: 12.615823745727539 = 0.042057909071445465 + 2.0 * 6.2868828773498535
Epoch 1010, val loss: 0.9633821845054626
Epoch 1020, training loss: 12.618180274963379 = 0.040629055351018906 + 2.0 * 6.288775444030762
Epoch 1020, val loss: 0.9694697856903076
Epoch 1030, training loss: 12.614330291748047 = 0.03927603363990784 + 2.0 * 6.287527084350586
Epoch 1030, val loss: 0.975410521030426
Epoch 1040, training loss: 12.611050605773926 = 0.03799432888627052 + 2.0 * 6.28652811050415
Epoch 1040, val loss: 0.9809839129447937
Epoch 1050, training loss: 12.601178169250488 = 0.036774661391973495 + 2.0 * 6.282201766967773
Epoch 1050, val loss: 0.9866600036621094
Epoch 1060, training loss: 12.598299980163574 = 0.035599227994680405 + 2.0 * 6.281350612640381
Epoch 1060, val loss: 0.9924978613853455
Epoch 1070, training loss: 12.601107597351074 = 0.034469299018383026 + 2.0 * 6.283318996429443
Epoch 1070, val loss: 0.9981716871261597
Epoch 1080, training loss: 12.595471382141113 = 0.03338830545544624 + 2.0 * 6.281041622161865
Epoch 1080, val loss: 1.004073977470398
Epoch 1090, training loss: 12.595053672790527 = 0.03236135467886925 + 2.0 * 6.281346321105957
Epoch 1090, val loss: 1.0094157457351685
Epoch 1100, training loss: 12.588473320007324 = 0.03137637674808502 + 2.0 * 6.278548240661621
Epoch 1100, val loss: 1.0148963928222656
Epoch 1110, training loss: 12.587172508239746 = 0.030430961400270462 + 2.0 * 6.2783708572387695
Epoch 1110, val loss: 1.0205228328704834
Epoch 1120, training loss: 12.605774879455566 = 0.02952025644481182 + 2.0 * 6.288127422332764
Epoch 1120, val loss: 1.0261157751083374
Epoch 1130, training loss: 12.586400032043457 = 0.028666295111179352 + 2.0 * 6.278866767883301
Epoch 1130, val loss: 1.0313268899917603
Epoch 1140, training loss: 12.57927417755127 = 0.02784734033048153 + 2.0 * 6.2757134437561035
Epoch 1140, val loss: 1.0363727807998657
Epoch 1150, training loss: 12.577728271484375 = 0.027058249339461327 + 2.0 * 6.27533483505249
Epoch 1150, val loss: 1.0416709184646606
Epoch 1160, training loss: 12.577733039855957 = 0.02629769966006279 + 2.0 * 6.275717735290527
Epoch 1160, val loss: 1.0470153093338013
Epoch 1170, training loss: 12.582114219665527 = 0.025568189099431038 + 2.0 * 6.278273105621338
Epoch 1170, val loss: 1.0521936416625977
Epoch 1180, training loss: 12.57447624206543 = 0.02487419918179512 + 2.0 * 6.274801254272461
Epoch 1180, val loss: 1.0570710897445679
Epoch 1190, training loss: 12.574471473693848 = 0.024204226210713387 + 2.0 * 6.2751336097717285
Epoch 1190, val loss: 1.0621944665908813
Epoch 1200, training loss: 12.568140983581543 = 0.02356533519923687 + 2.0 * 6.272287845611572
Epoch 1200, val loss: 1.0670530796051025
Epoch 1210, training loss: 12.565455436706543 = 0.022951416671276093 + 2.0 * 6.271252155303955
Epoch 1210, val loss: 1.0718681812286377
Epoch 1220, training loss: 12.57025146484375 = 0.02236058935523033 + 2.0 * 6.273945331573486
Epoch 1220, val loss: 1.0767532587051392
Epoch 1230, training loss: 12.5684175491333 = 0.021792210638523102 + 2.0 * 6.273312568664551
Epoch 1230, val loss: 1.081595778465271
Epoch 1240, training loss: 12.562482833862305 = 0.021246934309601784 + 2.0 * 6.270617961883545
Epoch 1240, val loss: 1.086287260055542
Epoch 1250, training loss: 12.559276580810547 = 0.0207234937697649 + 2.0 * 6.2692766189575195
Epoch 1250, val loss: 1.0908229351043701
Epoch 1260, training loss: 12.558706283569336 = 0.020219359546899796 + 2.0 * 6.269243240356445
Epoch 1260, val loss: 1.0955308675765991
Epoch 1270, training loss: 12.5643949508667 = 0.019730407744646072 + 2.0 * 6.272332191467285
Epoch 1270, val loss: 1.1002702713012695
Epoch 1280, training loss: 12.557156562805176 = 0.019264686852693558 + 2.0 * 6.268946170806885
Epoch 1280, val loss: 1.104663610458374
Epoch 1290, training loss: 12.561409950256348 = 0.018815290182828903 + 2.0 * 6.271297454833984
Epoch 1290, val loss: 1.1090432405471802
Epoch 1300, training loss: 12.550498962402344 = 0.0183811467140913 + 2.0 * 6.266058921813965
Epoch 1300, val loss: 1.1135289669036865
Epoch 1310, training loss: 12.54859733581543 = 0.01796528697013855 + 2.0 * 6.265316009521484
Epoch 1310, val loss: 1.1177594661712646
Epoch 1320, training loss: 12.547072410583496 = 0.01756204664707184 + 2.0 * 6.2647552490234375
Epoch 1320, val loss: 1.1221253871917725
Epoch 1330, training loss: 12.546113967895508 = 0.017170926555991173 + 2.0 * 6.264471530914307
Epoch 1330, val loss: 1.1264735460281372
Epoch 1340, training loss: 12.562695503234863 = 0.01679050922393799 + 2.0 * 6.272952556610107
Epoch 1340, val loss: 1.1309454441070557
Epoch 1350, training loss: 12.561511993408203 = 0.01642712764441967 + 2.0 * 6.272542476654053
Epoch 1350, val loss: 1.1350915431976318
Epoch 1360, training loss: 12.544028282165527 = 0.0160821620374918 + 2.0 * 6.263973236083984
Epoch 1360, val loss: 1.1389116048812866
Epoch 1370, training loss: 12.542040824890137 = 0.015745921060442924 + 2.0 * 6.263147354125977
Epoch 1370, val loss: 1.1428673267364502
Epoch 1380, training loss: 12.538704872131348 = 0.015419263392686844 + 2.0 * 6.261642932891846
Epoch 1380, val loss: 1.1470062732696533
Epoch 1390, training loss: 12.539369583129883 = 0.015101835131645203 + 2.0 * 6.262134075164795
Epoch 1390, val loss: 1.1510955095291138
Epoch 1400, training loss: 12.549066543579102 = 0.014793210662901402 + 2.0 * 6.267136573791504
Epoch 1400, val loss: 1.155211329460144
Epoch 1410, training loss: 12.549246788024902 = 0.01449520606547594 + 2.0 * 6.267375946044922
Epoch 1410, val loss: 1.1590955257415771
Epoch 1420, training loss: 12.537932395935059 = 0.01421366073191166 + 2.0 * 6.26185941696167
Epoch 1420, val loss: 1.1624877452850342
Epoch 1430, training loss: 12.535316467285156 = 0.013936429284512997 + 2.0 * 6.260690212249756
Epoch 1430, val loss: 1.1663331985473633
Epoch 1440, training loss: 12.534785270690918 = 0.01366727240383625 + 2.0 * 6.26055908203125
Epoch 1440, val loss: 1.1701685190200806
Epoch 1450, training loss: 12.538865089416504 = 0.013405869714915752 + 2.0 * 6.262729644775391
Epoch 1450, val loss: 1.1740286350250244
Epoch 1460, training loss: 12.531333923339844 = 0.01315225288271904 + 2.0 * 6.259090900421143
Epoch 1460, val loss: 1.1775802373886108
Epoch 1470, training loss: 12.528962135314941 = 0.012907508760690689 + 2.0 * 6.258027076721191
Epoch 1470, val loss: 1.1810204982757568
Epoch 1480, training loss: 12.53172779083252 = 0.012668551877140999 + 2.0 * 6.2595295906066895
Epoch 1480, val loss: 1.1847974061965942
Epoch 1490, training loss: 12.52828598022461 = 0.01243606023490429 + 2.0 * 6.257925033569336
Epoch 1490, val loss: 1.1884106397628784
Epoch 1500, training loss: 12.537795066833496 = 0.012211770750582218 + 2.0 * 6.262791633605957
Epoch 1500, val loss: 1.1918597221374512
Epoch 1510, training loss: 12.535018920898438 = 0.01199400145560503 + 2.0 * 6.261512279510498
Epoch 1510, val loss: 1.1952545642852783
Epoch 1520, training loss: 12.524879455566406 = 0.011781714856624603 + 2.0 * 6.256548881530762
Epoch 1520, val loss: 1.19874107837677
Epoch 1530, training loss: 12.5233736038208 = 0.011574951931834221 + 2.0 * 6.255899429321289
Epoch 1530, val loss: 1.2022285461425781
Epoch 1540, training loss: 12.532917022705078 = 0.011372985318303108 + 2.0 * 6.260772228240967
Epoch 1540, val loss: 1.2058169841766357
Epoch 1550, training loss: 12.520533561706543 = 0.011178400367498398 + 2.0 * 6.254677772521973
Epoch 1550, val loss: 1.2089987993240356
Epoch 1560, training loss: 12.521707534790039 = 0.010989864356815815 + 2.0 * 6.255358695983887
Epoch 1560, val loss: 1.2121708393096924
Epoch 1570, training loss: 12.531728744506836 = 0.010804331861436367 + 2.0 * 6.260462284088135
Epoch 1570, val loss: 1.2156611680984497
Epoch 1580, training loss: 12.52013874053955 = 0.010623829439282417 + 2.0 * 6.254757404327393
Epoch 1580, val loss: 1.2188868522644043
Epoch 1590, training loss: 12.518527030944824 = 0.010449418798089027 + 2.0 * 6.2540388107299805
Epoch 1590, val loss: 1.2219572067260742
Epoch 1600, training loss: 12.526045799255371 = 0.010278332978487015 + 2.0 * 6.257883548736572
Epoch 1600, val loss: 1.2253057956695557
Epoch 1610, training loss: 12.516473770141602 = 0.010112868621945381 + 2.0 * 6.253180503845215
Epoch 1610, val loss: 1.228439450263977
Epoch 1620, training loss: 12.51484489440918 = 0.00995212234556675 + 2.0 * 6.252446174621582
Epoch 1620, val loss: 1.2313939332962036
Epoch 1630, training loss: 12.515869140625 = 0.009793644770979881 + 2.0 * 6.253037929534912
Epoch 1630, val loss: 1.2346081733703613
Epoch 1640, training loss: 12.528803825378418 = 0.009638450108468533 + 2.0 * 6.25958251953125
Epoch 1640, val loss: 1.2378276586532593
Epoch 1650, training loss: 12.516435623168945 = 0.009491882286965847 + 2.0 * 6.253471851348877
Epoch 1650, val loss: 1.2405591011047363
Epoch 1660, training loss: 12.512613296508789 = 0.009345382452011108 + 2.0 * 6.251634120941162
Epoch 1660, val loss: 1.2434961795806885
Epoch 1670, training loss: 12.512120246887207 = 0.009203841909766197 + 2.0 * 6.251458168029785
Epoch 1670, val loss: 1.2464790344238281
Epoch 1680, training loss: 12.526966094970703 = 0.00906505435705185 + 2.0 * 6.258950710296631
Epoch 1680, val loss: 1.2495688199996948
Epoch 1690, training loss: 12.520040512084961 = 0.008927823975682259 + 2.0 * 6.255556106567383
Epoch 1690, val loss: 1.2525840997695923
Epoch 1700, training loss: 12.512502670288086 = 0.008798285387456417 + 2.0 * 6.251852035522461
Epoch 1700, val loss: 1.2551738023757935
Epoch 1710, training loss: 12.512907981872559 = 0.008669249713420868 + 2.0 * 6.252119541168213
Epoch 1710, val loss: 1.25812566280365
Epoch 1720, training loss: 12.509509086608887 = 0.008543918840587139 + 2.0 * 6.250482559204102
Epoch 1720, val loss: 1.2609983682632446
Epoch 1730, training loss: 12.50648021697998 = 0.00842191744595766 + 2.0 * 6.249029159545898
Epoch 1730, val loss: 1.2635635137557983
Epoch 1740, training loss: 12.50531005859375 = 0.008302026428282261 + 2.0 * 6.248504161834717
Epoch 1740, val loss: 1.2663345336914062
Epoch 1750, training loss: 12.52902889251709 = 0.008185326121747494 + 2.0 * 6.2604217529296875
Epoch 1750, val loss: 1.269183874130249
Epoch 1760, training loss: 12.510418891906738 = 0.008069304749369621 + 2.0 * 6.2511749267578125
Epoch 1760, val loss: 1.272047519683838
Epoch 1770, training loss: 12.503129959106445 = 0.007959574460983276 + 2.0 * 6.247585296630859
Epoch 1770, val loss: 1.274354338645935
Epoch 1780, training loss: 12.503605842590332 = 0.007849936373531818 + 2.0 * 6.247878074645996
Epoch 1780, val loss: 1.277191162109375
Epoch 1790, training loss: 12.516242980957031 = 0.007742526475340128 + 2.0 * 6.2542500495910645
Epoch 1790, val loss: 1.2801201343536377
Epoch 1800, training loss: 12.507572174072266 = 0.00763890752568841 + 2.0 * 6.249966621398926
Epoch 1800, val loss: 1.2823914289474487
Epoch 1810, training loss: 12.502823829650879 = 0.00753764808177948 + 2.0 * 6.247642993927002
Epoch 1810, val loss: 1.2849040031433105
Epoch 1820, training loss: 12.501261711120605 = 0.007437834516167641 + 2.0 * 6.246912002563477
Epoch 1820, val loss: 1.287472128868103
Epoch 1830, training loss: 12.518616676330566 = 0.007339003495872021 + 2.0 * 6.255638599395752
Epoch 1830, val loss: 1.290313482284546
Epoch 1840, training loss: 12.50572395324707 = 0.007245913147926331 + 2.0 * 6.249238967895508
Epoch 1840, val loss: 1.2924864292144775
Epoch 1850, training loss: 12.500387191772461 = 0.007152342703193426 + 2.0 * 6.246617317199707
Epoch 1850, val loss: 1.2948691844940186
Epoch 1860, training loss: 12.498886108398438 = 0.007061756681650877 + 2.0 * 6.245912075042725
Epoch 1860, val loss: 1.297315001487732
Epoch 1870, training loss: 12.502197265625 = 0.00697197113186121 + 2.0 * 6.247612476348877
Epoch 1870, val loss: 1.2999204397201538
Epoch 1880, training loss: 12.496916770935059 = 0.006883692927658558 + 2.0 * 6.245016574859619
Epoch 1880, val loss: 1.302338719367981
Epoch 1890, training loss: 12.503164291381836 = 0.006798140704631805 + 2.0 * 6.248183250427246
Epoch 1890, val loss: 1.304690957069397
Epoch 1900, training loss: 12.508151054382324 = 0.00671484787017107 + 2.0 * 6.250718116760254
Epoch 1900, val loss: 1.3069239854812622
Epoch 1910, training loss: 12.496627807617188 = 0.0066325487568974495 + 2.0 * 6.244997501373291
Epoch 1910, val loss: 1.3092600107192993
Epoch 1920, training loss: 12.49386978149414 = 0.006552462000399828 + 2.0 * 6.243658542633057
Epoch 1920, val loss: 1.311438798904419
Epoch 1930, training loss: 12.494169235229492 = 0.0064734797924757 + 2.0 * 6.243847846984863
Epoch 1930, val loss: 1.3138819932937622
Epoch 1940, training loss: 12.50060749053955 = 0.006395383737981319 + 2.0 * 6.247106075286865
Epoch 1940, val loss: 1.3163527250289917
Epoch 1950, training loss: 12.49761962890625 = 0.0063194334506988525 + 2.0 * 6.245650291442871
Epoch 1950, val loss: 1.3185633420944214
Epoch 1960, training loss: 12.495634078979492 = 0.006245580967515707 + 2.0 * 6.244694232940674
Epoch 1960, val loss: 1.320665955543518
Epoch 1970, training loss: 12.491401672363281 = 0.006173020228743553 + 2.0 * 6.242614269256592
Epoch 1970, val loss: 1.322761058807373
Epoch 1980, training loss: 12.498619079589844 = 0.0061014327220618725 + 2.0 * 6.246258735656738
Epoch 1980, val loss: 1.324960470199585
Epoch 1990, training loss: 12.491530418395996 = 0.006031625438481569 + 2.0 * 6.242749214172363
Epoch 1990, val loss: 1.3273086547851562
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7851851851851852
0.8376383763837639
The final CL Acc:0.80494, 0.01429, The final GNN Acc:0.83694, 0.00066
Begin epxeriment: cont_weight: 2 epoch:2000 der1:0.3 der2:0.2 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11654])
remove edge: torch.Size([2, 9530])
updated graph: torch.Size([2, 10628])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 19.144878387451172 = 1.9511754512786865 + 2.0 * 8.596851348876953
Epoch 0, val loss: 1.9538720846176147
Epoch 10, training loss: 19.134685516357422 = 1.941427230834961 + 2.0 * 8.59662914276123
Epoch 10, val loss: 1.9442083835601807
Epoch 20, training loss: 19.119428634643555 = 1.9293770790100098 + 2.0 * 8.595026016235352
Epoch 20, val loss: 1.9318076372146606
Epoch 30, training loss: 19.077375411987305 = 1.9127027988433838 + 2.0 * 8.58233642578125
Epoch 30, val loss: 1.914245367050171
Epoch 40, training loss: 18.926103591918945 = 1.8901058435440063 + 2.0 * 8.517998695373535
Epoch 40, val loss: 1.8909169435501099
Epoch 50, training loss: 18.258769989013672 = 1.866438865661621 + 2.0 * 8.196165084838867
Epoch 50, val loss: 1.867318868637085
Epoch 60, training loss: 17.40113067626953 = 1.8428456783294678 + 2.0 * 7.779142379760742
Epoch 60, val loss: 1.8461800813674927
Epoch 70, training loss: 16.46935272216797 = 1.8268216848373413 + 2.0 * 7.321265697479248
Epoch 70, val loss: 1.8319445848464966
Epoch 80, training loss: 15.97780990600586 = 1.8133387565612793 + 2.0 * 7.082235813140869
Epoch 80, val loss: 1.819114327430725
Epoch 90, training loss: 15.670321464538574 = 1.8013098239898682 + 2.0 * 6.934505939483643
Epoch 90, val loss: 1.8077791929244995
Epoch 100, training loss: 15.412727355957031 = 1.7896028757095337 + 2.0 * 6.8115620613098145
Epoch 100, val loss: 1.797300100326538
Epoch 110, training loss: 15.265477180480957 = 1.778100609779358 + 2.0 * 6.743688106536865
Epoch 110, val loss: 1.7871891260147095
Epoch 120, training loss: 15.161498069763184 = 1.7662684917449951 + 2.0 * 6.697614669799805
Epoch 120, val loss: 1.776693344116211
Epoch 130, training loss: 15.072269439697266 = 1.7540613412857056 + 2.0 * 6.659103870391846
Epoch 130, val loss: 1.7658978700637817
Epoch 140, training loss: 14.997941017150879 = 1.7411067485809326 + 2.0 * 6.628417015075684
Epoch 140, val loss: 1.7545855045318604
Epoch 150, training loss: 14.937726020812988 = 1.7266277074813843 + 2.0 * 6.605549335479736
Epoch 150, val loss: 1.7422300577163696
Epoch 160, training loss: 14.874869346618652 = 1.7103445529937744 + 2.0 * 6.5822625160217285
Epoch 160, val loss: 1.7285430431365967
Epoch 170, training loss: 14.816337585449219 = 1.6919264793395996 + 2.0 * 6.562205791473389
Epoch 170, val loss: 1.713115930557251
Epoch 180, training loss: 14.761096954345703 = 1.6710925102233887 + 2.0 * 6.545002460479736
Epoch 180, val loss: 1.695701241493225
Epoch 190, training loss: 14.699140548706055 = 1.6476812362670898 + 2.0 * 6.525729656219482
Epoch 190, val loss: 1.6762661933898926
Epoch 200, training loss: 14.63734245300293 = 1.621290683746338 + 2.0 * 6.508025646209717
Epoch 200, val loss: 1.6544482707977295
Epoch 210, training loss: 14.577009201049805 = 1.5916075706481934 + 2.0 * 6.492701053619385
Epoch 210, val loss: 1.6299738883972168
Epoch 220, training loss: 14.519889831542969 = 1.5585485696792603 + 2.0 * 6.48067045211792
Epoch 220, val loss: 1.6027721166610718
Epoch 230, training loss: 14.45583724975586 = 1.522042155265808 + 2.0 * 6.466897487640381
Epoch 230, val loss: 1.572834849357605
Epoch 240, training loss: 14.393636703491211 = 1.4820300340652466 + 2.0 * 6.455803394317627
Epoch 240, val loss: 1.5402287244796753
Epoch 250, training loss: 14.338726997375488 = 1.438746690750122 + 2.0 * 6.449990272521973
Epoch 250, val loss: 1.5051411390304565
Epoch 260, training loss: 14.274441719055176 = 1.3931828737258911 + 2.0 * 6.440629482269287
Epoch 260, val loss: 1.4685686826705933
Epoch 270, training loss: 14.208867073059082 = 1.346070408821106 + 2.0 * 6.431398391723633
Epoch 270, val loss: 1.431241750717163
Epoch 280, training loss: 14.147089004516602 = 1.2976698875427246 + 2.0 * 6.424709320068359
Epoch 280, val loss: 1.3932446241378784
Epoch 290, training loss: 14.085408210754395 = 1.2484490871429443 + 2.0 * 6.4184794425964355
Epoch 290, val loss: 1.3551182746887207
Epoch 300, training loss: 14.035093307495117 = 1.198968529701233 + 2.0 * 6.418062210083008
Epoch 300, val loss: 1.3172783851623535
Epoch 310, training loss: 13.972536087036133 = 1.1503347158432007 + 2.0 * 6.4111008644104
Epoch 310, val loss: 1.280741810798645
Epoch 320, training loss: 13.911252975463867 = 1.1030497550964355 + 2.0 * 6.404101848602295
Epoch 320, val loss: 1.2457737922668457
Epoch 330, training loss: 13.854862213134766 = 1.057044267654419 + 2.0 * 6.398909091949463
Epoch 330, val loss: 1.2122639417648315
Epoch 340, training loss: 13.804424285888672 = 1.0124622583389282 + 2.0 * 6.3959808349609375
Epoch 340, val loss: 1.1802539825439453
Epoch 350, training loss: 13.759519577026367 = 0.9695808291435242 + 2.0 * 6.394969463348389
Epoch 350, val loss: 1.149954915046692
Epoch 360, training loss: 13.702147483825684 = 0.9287405014038086 + 2.0 * 6.3867034912109375
Epoch 360, val loss: 1.1214654445648193
Epoch 370, training loss: 13.65994644165039 = 0.8896710872650146 + 2.0 * 6.385137557983398
Epoch 370, val loss: 1.0947858095169067
Epoch 380, training loss: 13.621713638305664 = 0.8529384136199951 + 2.0 * 6.384387493133545
Epoch 380, val loss: 1.0699536800384521
Epoch 390, training loss: 13.572425842285156 = 0.8180583119392395 + 2.0 * 6.37718391418457
Epoch 390, val loss: 1.0471231937408447
Epoch 400, training loss: 13.531416893005371 = 0.7847660183906555 + 2.0 * 6.373325347900391
Epoch 400, val loss: 1.0258867740631104
Epoch 410, training loss: 13.502203941345215 = 0.7528665065765381 + 2.0 * 6.374668598175049
Epoch 410, val loss: 1.005990743637085
Epoch 420, training loss: 13.459010124206543 = 0.7224148511886597 + 2.0 * 6.368297576904297
Epoch 420, val loss: 0.9875403642654419
Epoch 430, training loss: 13.423233985900879 = 0.6932662725448608 + 2.0 * 6.364984035491943
Epoch 430, val loss: 0.9705482721328735
Epoch 440, training loss: 13.394144058227539 = 0.665066123008728 + 2.0 * 6.36453914642334
Epoch 440, val loss: 0.9547192454338074
Epoch 450, training loss: 13.360045433044434 = 0.6378056406974792 + 2.0 * 6.361119747161865
Epoch 450, val loss: 0.9400328993797302
Epoch 460, training loss: 13.327292442321777 = 0.6112964153289795 + 2.0 * 6.357997894287109
Epoch 460, val loss: 0.9264619946479797
Epoch 470, training loss: 13.31390380859375 = 0.5854401588439941 + 2.0 * 6.364231586456299
Epoch 470, val loss: 0.9139266014099121
Epoch 480, training loss: 13.27223014831543 = 0.5605605840682983 + 2.0 * 6.3558349609375
Epoch 480, val loss: 0.9023354649543762
Epoch 490, training loss: 13.238821983337402 = 0.5363664627075195 + 2.0 * 6.351227760314941
Epoch 490, val loss: 0.8917245268821716
Epoch 500, training loss: 13.213040351867676 = 0.5128062963485718 + 2.0 * 6.350117206573486
Epoch 500, val loss: 0.8820028901100159
Epoch 510, training loss: 13.183202743530273 = 0.4899125099182129 + 2.0 * 6.346644878387451
Epoch 510, val loss: 0.873256504535675
Epoch 520, training loss: 13.156405448913574 = 0.4677473306655884 + 2.0 * 6.344328880310059
Epoch 520, val loss: 0.8652302622795105
Epoch 530, training loss: 13.133512496948242 = 0.44629570841789246 + 2.0 * 6.343608379364014
Epoch 530, val loss: 0.8581599593162537
Epoch 540, training loss: 13.111725807189941 = 0.4256248474121094 + 2.0 * 6.343050479888916
Epoch 540, val loss: 0.8517563343048096
Epoch 550, training loss: 13.084218978881836 = 0.40574637055397034 + 2.0 * 6.339236259460449
Epoch 550, val loss: 0.8461195826530457
Epoch 560, training loss: 13.060007095336914 = 0.3866405785083771 + 2.0 * 6.33668327331543
Epoch 560, val loss: 0.8413069248199463
Epoch 570, training loss: 13.045440673828125 = 0.36829477548599243 + 2.0 * 6.338572978973389
Epoch 570, val loss: 0.8370850086212158
Epoch 580, training loss: 13.01826286315918 = 0.3508167862892151 + 2.0 * 6.333723068237305
Epoch 580, val loss: 0.8336290717124939
Epoch 590, training loss: 12.999629020690918 = 0.33409130573272705 + 2.0 * 6.33276891708374
Epoch 590, val loss: 0.8309098482131958
Epoch 600, training loss: 12.985699653625488 = 0.31809374690055847 + 2.0 * 6.333803176879883
Epoch 600, val loss: 0.8288447856903076
Epoch 610, training loss: 12.96027946472168 = 0.30291399359703064 + 2.0 * 6.328682899475098
Epoch 610, val loss: 0.8273593783378601
Epoch 620, training loss: 12.940912246704102 = 0.2884354293346405 + 2.0 * 6.326238632202148
Epoch 620, val loss: 0.8265632390975952
Epoch 630, training loss: 12.929169654846191 = 0.27464497089385986 + 2.0 * 6.3272624015808105
Epoch 630, val loss: 0.8264104723930359
Epoch 640, training loss: 12.915575981140137 = 0.2615537643432617 + 2.0 * 6.3270111083984375
Epoch 640, val loss: 0.8265631198883057
Epoch 650, training loss: 12.896854400634766 = 0.2492017149925232 + 2.0 * 6.323826313018799
Epoch 650, val loss: 0.8273440599441528
Epoch 660, training loss: 12.880290985107422 = 0.23755000531673431 + 2.0 * 6.321370601654053
Epoch 660, val loss: 0.8286792039871216
Epoch 670, training loss: 12.864275932312012 = 0.22645354270935059 + 2.0 * 6.318911075592041
Epoch 670, val loss: 0.8305253982543945
Epoch 680, training loss: 12.84989070892334 = 0.21591022610664368 + 2.0 * 6.316990375518799
Epoch 680, val loss: 0.8327781558036804
Epoch 690, training loss: 12.859118461608887 = 0.20587609708309174 + 2.0 * 6.326621055603027
Epoch 690, val loss: 0.8355010151863098
Epoch 700, training loss: 12.825797080993652 = 0.19642625749111176 + 2.0 * 6.314685344696045
Epoch 700, val loss: 0.8384449481964111
Epoch 710, training loss: 12.816755294799805 = 0.18746858835220337 + 2.0 * 6.314643383026123
Epoch 710, val loss: 0.8417734503746033
Epoch 720, training loss: 12.817948341369629 = 0.17895786464214325 + 2.0 * 6.31949520111084
Epoch 720, val loss: 0.8454955816268921
Epoch 730, training loss: 12.800055503845215 = 0.17090879380702972 + 2.0 * 6.314573287963867
Epoch 730, val loss: 0.8495240807533264
Epoch 740, training loss: 12.783349990844727 = 0.16329066455364227 + 2.0 * 6.31002950668335
Epoch 740, val loss: 0.8538302779197693
Epoch 750, training loss: 12.779589653015137 = 0.15605242550373077 + 2.0 * 6.311768531799316
Epoch 750, val loss: 0.858476996421814
Epoch 760, training loss: 12.765909194946289 = 0.14917123317718506 + 2.0 * 6.308369159698486
Epoch 760, val loss: 0.8632469773292542
Epoch 770, training loss: 12.773042678833008 = 0.14264453947544098 + 2.0 * 6.31519889831543
Epoch 770, val loss: 0.868334949016571
Epoch 780, training loss: 12.750162124633789 = 0.13650882244110107 + 2.0 * 6.306826591491699
Epoch 780, val loss: 0.87359619140625
Epoch 790, training loss: 12.740697860717773 = 0.13065789639949799 + 2.0 * 6.305019855499268
Epoch 790, val loss: 0.8790951371192932
Epoch 800, training loss: 12.731685638427734 = 0.12509046494960785 + 2.0 * 6.303297519683838
Epoch 800, val loss: 0.8848487138748169
Epoch 810, training loss: 12.726142883300781 = 0.11978654563426971 + 2.0 * 6.303178310394287
Epoch 810, val loss: 0.8907977938652039
Epoch 820, training loss: 12.718400001525879 = 0.11474031955003738 + 2.0 * 6.301829814910889
Epoch 820, val loss: 0.8967345356941223
Epoch 830, training loss: 12.714773178100586 = 0.10997064411640167 + 2.0 * 6.302401065826416
Epoch 830, val loss: 0.9028403759002686
Epoch 840, training loss: 12.705172538757324 = 0.10543061792850494 + 2.0 * 6.29987096786499
Epoch 840, val loss: 0.9091280102729797
Epoch 850, training loss: 12.69774055480957 = 0.10109983384609222 + 2.0 * 6.298320293426514
Epoch 850, val loss: 0.915500283241272
Epoch 860, training loss: 12.704410552978516 = 0.09696758538484573 + 2.0 * 6.3037214279174805
Epoch 860, val loss: 0.9219540357589722
Epoch 870, training loss: 12.703712463378906 = 0.09307079017162323 + 2.0 * 6.305320739746094
Epoch 870, val loss: 0.9285020232200623
Epoch 880, training loss: 12.684625625610352 = 0.08937839418649673 + 2.0 * 6.297623634338379
Epoch 880, val loss: 0.9350072145462036
Epoch 890, training loss: 12.676591873168945 = 0.08585315197706223 + 2.0 * 6.2953691482543945
Epoch 890, val loss: 0.9416727423667908
Epoch 900, training loss: 12.675317764282227 = 0.08249949663877487 + 2.0 * 6.2964091300964355
Epoch 900, val loss: 0.948415219783783
Epoch 910, training loss: 12.66916561126709 = 0.07929860800504684 + 2.0 * 6.294933319091797
Epoch 910, val loss: 0.9549798965454102
Epoch 920, training loss: 12.665136337280273 = 0.07628922909498215 + 2.0 * 6.294423580169678
Epoch 920, val loss: 0.9617303013801575
Epoch 930, training loss: 12.657622337341309 = 0.07340972870588303 + 2.0 * 6.2921061515808105
Epoch 930, val loss: 0.9684960246086121
Epoch 940, training loss: 12.658086776733398 = 0.07066051661968231 + 2.0 * 6.293713092803955
Epoch 940, val loss: 0.9752879738807678
Epoch 950, training loss: 12.64896011352539 = 0.06804080307483673 + 2.0 * 6.290459632873535
Epoch 950, val loss: 0.9820729494094849
Epoch 960, training loss: 12.652501106262207 = 0.0655481368303299 + 2.0 * 6.293476581573486
Epoch 960, val loss: 0.9888958930969238
Epoch 970, training loss: 12.642727851867676 = 0.06317538022994995 + 2.0 * 6.28977632522583
Epoch 970, val loss: 0.9956942796707153
Epoch 980, training loss: 12.641763687133789 = 0.06091047823429108 + 2.0 * 6.290426731109619
Epoch 980, val loss: 1.0025228261947632
Epoch 990, training loss: 12.638826370239258 = 0.05875970795750618 + 2.0 * 6.290033340454102
Epoch 990, val loss: 1.0092840194702148
Epoch 1000, training loss: 12.6314697265625 = 0.056704845279455185 + 2.0 * 6.28738260269165
Epoch 1000, val loss: 1.0160698890686035
Epoch 1010, training loss: 12.627761840820312 = 0.05474371835589409 + 2.0 * 6.286509037017822
Epoch 1010, val loss: 1.0228254795074463
Epoch 1020, training loss: 12.62781047821045 = 0.05286749079823494 + 2.0 * 6.287471294403076
Epoch 1020, val loss: 1.0295993089675903
Epoch 1030, training loss: 12.624918937683105 = 0.05108446255326271 + 2.0 * 6.286917209625244
Epoch 1030, val loss: 1.0362790822982788
Epoch 1040, training loss: 12.627888679504395 = 0.049384333193302155 + 2.0 * 6.289252281188965
Epoch 1040, val loss: 1.0429296493530273
Epoch 1050, training loss: 12.620488166809082 = 0.047756802290678024 + 2.0 * 6.286365509033203
Epoch 1050, val loss: 1.049407958984375
Epoch 1060, training loss: 12.61264419555664 = 0.046211566776037216 + 2.0 * 6.28321647644043
Epoch 1060, val loss: 1.05605947971344
Epoch 1070, training loss: 12.610179901123047 = 0.04472668841481209 + 2.0 * 6.282726764678955
Epoch 1070, val loss: 1.062623143196106
Epoch 1080, training loss: 12.622300148010254 = 0.043304622173309326 + 2.0 * 6.2894978523254395
Epoch 1080, val loss: 1.0690799951553345
Epoch 1090, training loss: 12.605809211730957 = 0.041954804211854935 + 2.0 * 6.281927108764648
Epoch 1090, val loss: 1.0754902362823486
Epoch 1100, training loss: 12.601301193237305 = 0.040660444647073746 + 2.0 * 6.280320167541504
Epoch 1100, val loss: 1.0819311141967773
Epoch 1110, training loss: 12.60999870300293 = 0.03941941633820534 + 2.0 * 6.285289764404297
Epoch 1110, val loss: 1.0883452892303467
Epoch 1120, training loss: 12.604713439941406 = 0.038235973566770554 + 2.0 * 6.283238887786865
Epoch 1120, val loss: 1.0944703817367554
Epoch 1130, training loss: 12.595470428466797 = 0.03710421919822693 + 2.0 * 6.2791829109191895
Epoch 1130, val loss: 1.1007126569747925
Epoch 1140, training loss: 12.59278678894043 = 0.03601891174912453 + 2.0 * 6.278383731842041
Epoch 1140, val loss: 1.1068828105926514
Epoch 1150, training loss: 12.596752166748047 = 0.034977223724126816 + 2.0 * 6.280887603759766
Epoch 1150, val loss: 1.113006591796875
Epoch 1160, training loss: 12.592196464538574 = 0.03397808596491814 + 2.0 * 6.279109001159668
Epoch 1160, val loss: 1.1189191341400146
Epoch 1170, training loss: 12.595457077026367 = 0.033026568591594696 + 2.0 * 6.281215190887451
Epoch 1170, val loss: 1.1248220205307007
Epoch 1180, training loss: 12.5859375 = 0.03211899474263191 + 2.0 * 6.276909351348877
Epoch 1180, val loss: 1.1308071613311768
Epoch 1190, training loss: 12.583165168762207 = 0.03124183416366577 + 2.0 * 6.275961875915527
Epoch 1190, val loss: 1.1367290019989014
Epoch 1200, training loss: 12.581122398376465 = 0.030396491289138794 + 2.0 * 6.275362968444824
Epoch 1200, val loss: 1.1425820589065552
Epoch 1210, training loss: 12.588887214660645 = 0.029585761949419975 + 2.0 * 6.279650688171387
Epoch 1210, val loss: 1.1484112739562988
Epoch 1220, training loss: 12.580127716064453 = 0.02880173735320568 + 2.0 * 6.275662899017334
Epoch 1220, val loss: 1.1540762186050415
Epoch 1230, training loss: 12.583041191101074 = 0.02805190719664097 + 2.0 * 6.277494430541992
Epoch 1230, val loss: 1.1597589254379272
Epoch 1240, training loss: 12.575752258300781 = 0.027328921481966972 + 2.0 * 6.274211883544922
Epoch 1240, val loss: 1.1652897596359253
Epoch 1250, training loss: 12.5746431350708 = 0.026636967435479164 + 2.0 * 6.274003028869629
Epoch 1250, val loss: 1.170857310295105
Epoch 1260, training loss: 12.5716552734375 = 0.025970550253987312 + 2.0 * 6.2728424072265625
Epoch 1260, val loss: 1.1763801574707031
Epoch 1270, training loss: 12.569289207458496 = 0.025328533723950386 + 2.0 * 6.271980285644531
Epoch 1270, val loss: 1.1818265914916992
Epoch 1280, training loss: 12.578167915344238 = 0.02471262216567993 + 2.0 * 6.276727676391602
Epoch 1280, val loss: 1.187254548072815
Epoch 1290, training loss: 12.57372760772705 = 0.024108773097395897 + 2.0 * 6.27480936050415
Epoch 1290, val loss: 1.19240140914917
Epoch 1300, training loss: 12.566315650939941 = 0.02353765442967415 + 2.0 * 6.271389007568359
Epoch 1300, val loss: 1.1977261304855347
Epoch 1310, training loss: 12.565078735351562 = 0.022981951013207436 + 2.0 * 6.271048545837402
Epoch 1310, val loss: 1.202894926071167
Epoch 1320, training loss: 12.572602272033691 = 0.022450894117355347 + 2.0 * 6.275075912475586
Epoch 1320, val loss: 1.208066701889038
Epoch 1330, training loss: 12.564451217651367 = 0.021933069452643394 + 2.0 * 6.271259307861328
Epoch 1330, val loss: 1.213085651397705
Epoch 1340, training loss: 12.55919361114502 = 0.021436449140310287 + 2.0 * 6.26887845993042
Epoch 1340, val loss: 1.2181483507156372
Epoch 1350, training loss: 12.56458568572998 = 0.020955057814717293 + 2.0 * 6.271815299987793
Epoch 1350, val loss: 1.223157525062561
Epoch 1360, training loss: 12.557775497436523 = 0.020491814240813255 + 2.0 * 6.268641948699951
Epoch 1360, val loss: 1.227920651435852
Epoch 1370, training loss: 12.555448532104492 = 0.020043963566422462 + 2.0 * 6.267702102661133
Epoch 1370, val loss: 1.2327957153320312
Epoch 1380, training loss: 12.553979873657227 = 0.019611388444900513 + 2.0 * 6.267184257507324
Epoch 1380, val loss: 1.2375797033309937
Epoch 1390, training loss: 12.554448127746582 = 0.019190777093172073 + 2.0 * 6.2676286697387695
Epoch 1390, val loss: 1.2423661947250366
Epoch 1400, training loss: 12.557086944580078 = 0.018783217296004295 + 2.0 * 6.26915168762207
Epoch 1400, val loss: 1.24700129032135
Epoch 1410, training loss: 12.55335807800293 = 0.018392091616988182 + 2.0 * 6.267482757568359
Epoch 1410, val loss: 1.251672387123108
Epoch 1420, training loss: 12.57366943359375 = 0.018015487119555473 + 2.0 * 6.27782678604126
Epoch 1420, val loss: 1.2561880350112915
Epoch 1430, training loss: 12.547304153442383 = 0.017643723636865616 + 2.0 * 6.264830112457275
Epoch 1430, val loss: 1.260521411895752
Epoch 1440, training loss: 12.547405242919922 = 0.01729174330830574 + 2.0 * 6.265056610107422
Epoch 1440, val loss: 1.2649896144866943
Epoch 1450, training loss: 12.54358196258545 = 0.01694793812930584 + 2.0 * 6.263317108154297
Epoch 1450, val loss: 1.2694685459136963
Epoch 1460, training loss: 12.552099227905273 = 0.01661275513470173 + 2.0 * 6.267743110656738
Epoch 1460, val loss: 1.2738415002822876
Epoch 1470, training loss: 12.542985916137695 = 0.016288351267576218 + 2.0 * 6.263348579406738
Epoch 1470, val loss: 1.2781227827072144
Epoch 1480, training loss: 12.541923522949219 = 0.01597597636282444 + 2.0 * 6.262973785400391
Epoch 1480, val loss: 1.2823954820632935
Epoch 1490, training loss: 12.544625282287598 = 0.015671391040086746 + 2.0 * 6.264476776123047
Epoch 1490, val loss: 1.286633849143982
Epoch 1500, training loss: 12.538143157958984 = 0.015375403687357903 + 2.0 * 6.261384010314941
Epoch 1500, val loss: 1.2908384799957275
Epoch 1510, training loss: 12.543122291564941 = 0.01508939266204834 + 2.0 * 6.264016628265381
Epoch 1510, val loss: 1.2950628995895386
Epoch 1520, training loss: 12.538105010986328 = 0.014808769337832928 + 2.0 * 6.261648178100586
Epoch 1520, val loss: 1.2990093231201172
Epoch 1530, training loss: 12.534823417663574 = 0.014536972157657146 + 2.0 * 6.260143280029297
Epoch 1530, val loss: 1.3031138181686401
Epoch 1540, training loss: 12.534558296203613 = 0.014274436049163342 + 2.0 * 6.260141849517822
Epoch 1540, val loss: 1.3071975708007812
Epoch 1550, training loss: 12.562809944152832 = 0.014020491391420364 + 2.0 * 6.274394512176514
Epoch 1550, val loss: 1.3111525774002075
Epoch 1560, training loss: 12.543542861938477 = 0.013767510652542114 + 2.0 * 6.264887809753418
Epoch 1560, val loss: 1.3148505687713623
Epoch 1570, training loss: 12.534165382385254 = 0.013529440388083458 + 2.0 * 6.260317802429199
Epoch 1570, val loss: 1.3187752962112427
Epoch 1580, training loss: 12.529443740844727 = 0.013296746648848057 + 2.0 * 6.258073329925537
Epoch 1580, val loss: 1.3226873874664307
Epoch 1590, training loss: 12.52855396270752 = 0.013067678548395634 + 2.0 * 6.2577433586120605
Epoch 1590, val loss: 1.3265331983566284
Epoch 1600, training loss: 12.530397415161133 = 0.012843656353652477 + 2.0 * 6.258776664733887
Epoch 1600, val loss: 1.3303451538085938
Epoch 1610, training loss: 12.53449535369873 = 0.01262743305414915 + 2.0 * 6.260933876037598
Epoch 1610, val loss: 1.3340139389038086
Epoch 1620, training loss: 12.526765823364258 = 0.012416230514645576 + 2.0 * 6.257174968719482
Epoch 1620, val loss: 1.3376824855804443
Epoch 1630, training loss: 12.530157089233398 = 0.01221244316548109 + 2.0 * 6.25897216796875
Epoch 1630, val loss: 1.341329574584961
Epoch 1640, training loss: 12.528886795043945 = 0.012012463063001633 + 2.0 * 6.258437156677246
Epoch 1640, val loss: 1.3449323177337646
Epoch 1650, training loss: 12.536386489868164 = 0.011820456013083458 + 2.0 * 6.262282848358154
Epoch 1650, val loss: 1.3485521078109741
Epoch 1660, training loss: 12.526057243347168 = 0.011630467139184475 + 2.0 * 6.257213592529297
Epoch 1660, val loss: 1.3520628213882446
Epoch 1670, training loss: 12.522007942199707 = 0.011445964686572552 + 2.0 * 6.2552809715271
Epoch 1670, val loss: 1.355588674545288
Epoch 1680, training loss: 12.525470733642578 = 0.011265871115028858 + 2.0 * 6.2571024894714355
Epoch 1680, val loss: 1.3590627908706665
Epoch 1690, training loss: 12.521984100341797 = 0.011089968495070934 + 2.0 * 6.255446910858154
Epoch 1690, val loss: 1.3624060153961182
Epoch 1700, training loss: 12.519574165344238 = 0.010920228436589241 + 2.0 * 6.254326820373535
Epoch 1700, val loss: 1.365889310836792
Epoch 1710, training loss: 12.525707244873047 = 0.010754375718533993 + 2.0 * 6.257476329803467
Epoch 1710, val loss: 1.3693302869796753
Epoch 1720, training loss: 12.517742156982422 = 0.010590373538434505 + 2.0 * 6.253575801849365
Epoch 1720, val loss: 1.3725206851959229
Epoch 1730, training loss: 12.51751708984375 = 0.01043207198381424 + 2.0 * 6.253542423248291
Epoch 1730, val loss: 1.3758512735366821
Epoch 1740, training loss: 12.530680656433105 = 0.010278228670358658 + 2.0 * 6.2602009773254395
Epoch 1740, val loss: 1.379167079925537
Epoch 1750, training loss: 12.518218040466309 = 0.010127156972885132 + 2.0 * 6.254045486450195
Epoch 1750, val loss: 1.3822977542877197
Epoch 1760, training loss: 12.515310287475586 = 0.009980631060898304 + 2.0 * 6.252665042877197
Epoch 1760, val loss: 1.385624885559082
Epoch 1770, training loss: 12.517168045043945 = 0.009836331009864807 + 2.0 * 6.253665924072266
Epoch 1770, val loss: 1.3888229131698608
Epoch 1780, training loss: 12.519161224365234 = 0.009696442633867264 + 2.0 * 6.254732608795166
Epoch 1780, val loss: 1.3919185400009155
Epoch 1790, training loss: 12.513717651367188 = 0.009557471610605717 + 2.0 * 6.252079963684082
Epoch 1790, val loss: 1.395024061203003
Epoch 1800, training loss: 12.512041091918945 = 0.009423741139471531 + 2.0 * 6.251308441162109
Epoch 1800, val loss: 1.3981622457504272
Epoch 1810, training loss: 12.511239051818848 = 0.009291429072618484 + 2.0 * 6.250973701477051
Epoch 1810, val loss: 1.4011952877044678
Epoch 1820, training loss: 12.525677680969238 = 0.009161541238427162 + 2.0 * 6.258257865905762
Epoch 1820, val loss: 1.404184341430664
Epoch 1830, training loss: 12.529153823852539 = 0.009037280455231667 + 2.0 * 6.260058403015137
Epoch 1830, val loss: 1.4070903062820435
Epoch 1840, training loss: 12.512494087219238 = 0.008916369639337063 + 2.0 * 6.251789093017578
Epoch 1840, val loss: 1.4100178480148315
Epoch 1850, training loss: 12.508031845092773 = 0.008797019720077515 + 2.0 * 6.249617576599121
Epoch 1850, val loss: 1.4129606485366821
Epoch 1860, training loss: 12.507201194763184 = 0.00868014246225357 + 2.0 * 6.249260425567627
Epoch 1860, val loss: 1.415916085243225
Epoch 1870, training loss: 12.50747299194336 = 0.008564463816583157 + 2.0 * 6.249454498291016
Epoch 1870, val loss: 1.4188601970672607
Epoch 1880, training loss: 12.527655601501465 = 0.008452904410660267 + 2.0 * 6.25960111618042
Epoch 1880, val loss: 1.4217228889465332
Epoch 1890, training loss: 12.508621215820312 = 0.008343003690242767 + 2.0 * 6.250139236450195
Epoch 1890, val loss: 1.4243900775909424
Epoch 1900, training loss: 12.506365776062012 = 0.008235662244260311 + 2.0 * 6.249064922332764
Epoch 1900, val loss: 1.4271578788757324
Epoch 1910, training loss: 12.517624855041504 = 0.008131759241223335 + 2.0 * 6.254746437072754
Epoch 1910, val loss: 1.4298770427703857
Epoch 1920, training loss: 12.506211280822754 = 0.008028690703213215 + 2.0 * 6.249091148376465
Epoch 1920, val loss: 1.432648777961731
Epoch 1930, training loss: 12.501973152160645 = 0.00792768970131874 + 2.0 * 6.24702262878418
Epoch 1930, val loss: 1.4353631734848022
Epoch 1940, training loss: 12.502263069152832 = 0.007828401401638985 + 2.0 * 6.247217178344727
Epoch 1940, val loss: 1.4380791187286377
Epoch 1950, training loss: 12.51225757598877 = 0.007730990648269653 + 2.0 * 6.252263069152832
Epoch 1950, val loss: 1.4407135248184204
Epoch 1960, training loss: 12.500697135925293 = 0.007636529393494129 + 2.0 * 6.246530532836914
Epoch 1960, val loss: 1.4434199333190918
Epoch 1970, training loss: 12.507801055908203 = 0.007544263731688261 + 2.0 * 6.250128269195557
Epoch 1970, val loss: 1.4460593461990356
Epoch 1980, training loss: 12.498394966125488 = 0.0074526164680719376 + 2.0 * 6.245471000671387
Epoch 1980, val loss: 1.448530912399292
Epoch 1990, training loss: 12.49880313873291 = 0.00736293476074934 + 2.0 * 6.245719909667969
Epoch 1990, val loss: 1.451101541519165
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7407407407407408
0.7965208223510807
=== training gcn model ===
Epoch 0, training loss: 19.134593963623047 = 1.9409247636795044 + 2.0 * 8.596834182739258
Epoch 0, val loss: 1.941391110420227
Epoch 10, training loss: 19.12467384338379 = 1.9316521883010864 + 2.0 * 8.596510887145996
Epoch 10, val loss: 1.9317258596420288
Epoch 20, training loss: 19.107688903808594 = 1.9201865196228027 + 2.0 * 8.593750953674316
Epoch 20, val loss: 1.9197425842285156
Epoch 30, training loss: 19.04613494873047 = 1.9044500589370728 + 2.0 * 8.570842742919922
Epoch 30, val loss: 1.9030052423477173
Epoch 40, training loss: 18.686689376831055 = 1.8850852251052856 + 2.0 * 8.400801658630371
Epoch 40, val loss: 1.8830909729003906
Epoch 50, training loss: 17.396442413330078 = 1.8665834665298462 + 2.0 * 7.76492977142334
Epoch 50, val loss: 1.8650327920913696
Epoch 60, training loss: 16.48246955871582 = 1.852287769317627 + 2.0 * 7.315090656280518
Epoch 60, val loss: 1.852346420288086
Epoch 70, training loss: 16.03409767150879 = 1.8405354022979736 + 2.0 * 7.096781253814697
Epoch 70, val loss: 1.8416481018066406
Epoch 80, training loss: 15.763291358947754 = 1.827581524848938 + 2.0 * 6.967854976654053
Epoch 80, val loss: 1.8299723863601685
Epoch 90, training loss: 15.477645874023438 = 1.8137696981430054 + 2.0 * 6.83193826675415
Epoch 90, val loss: 1.8183377981185913
Epoch 100, training loss: 15.298457145690918 = 1.8021763563156128 + 2.0 * 6.748140335083008
Epoch 100, val loss: 1.8086079359054565
Epoch 110, training loss: 15.165167808532715 = 1.7906256914138794 + 2.0 * 6.6872711181640625
Epoch 110, val loss: 1.7987412214279175
Epoch 120, training loss: 15.060381889343262 = 1.778725266456604 + 2.0 * 6.6408281326293945
Epoch 120, val loss: 1.788744568824768
Epoch 130, training loss: 14.975846290588379 = 1.7665131092071533 + 2.0 * 6.604666709899902
Epoch 130, val loss: 1.778679609298706
Epoch 140, training loss: 14.915184020996094 = 1.7534565925598145 + 2.0 * 6.580863952636719
Epoch 140, val loss: 1.7679448127746582
Epoch 150, training loss: 14.850114822387695 = 1.7390475273132324 + 2.0 * 6.555533409118652
Epoch 150, val loss: 1.7561277151107788
Epoch 160, training loss: 14.797279357910156 = 1.7230520248413086 + 2.0 * 6.537113666534424
Epoch 160, val loss: 1.7430671453475952
Epoch 170, training loss: 14.747434616088867 = 1.7051329612731934 + 2.0 * 6.521151065826416
Epoch 170, val loss: 1.7285077571868896
Epoch 180, training loss: 14.701472282409668 = 1.6848442554473877 + 2.0 * 6.50831413269043
Epoch 180, val loss: 1.7119743824005127
Epoch 190, training loss: 14.651139259338379 = 1.6620608568191528 + 2.0 * 6.494539260864258
Epoch 190, val loss: 1.6936067342758179
Epoch 200, training loss: 14.597553253173828 = 1.63668954372406 + 2.0 * 6.480432033538818
Epoch 200, val loss: 1.673056960105896
Epoch 210, training loss: 14.543607711791992 = 1.6081372499465942 + 2.0 * 6.467735290527344
Epoch 210, val loss: 1.6500405073165894
Epoch 220, training loss: 14.503479957580566 = 1.5762969255447388 + 2.0 * 6.463591575622559
Epoch 220, val loss: 1.6244224309921265
Epoch 230, training loss: 14.437454223632812 = 1.5416024923324585 + 2.0 * 6.447926044464111
Epoch 230, val loss: 1.5966589450836182
Epoch 240, training loss: 14.378305435180664 = 1.5040899515151978 + 2.0 * 6.437107563018799
Epoch 240, val loss: 1.5667026042938232
Epoch 250, training loss: 14.32439136505127 = 1.4638679027557373 + 2.0 * 6.430261611938477
Epoch 250, val loss: 1.5348172187805176
Epoch 260, training loss: 14.266571044921875 = 1.4214832782745361 + 2.0 * 6.422544002532959
Epoch 260, val loss: 1.501531958580017
Epoch 270, training loss: 14.20473575592041 = 1.377666711807251 + 2.0 * 6.413534641265869
Epoch 270, val loss: 1.4672820568084717
Epoch 280, training loss: 14.148977279663086 = 1.332764744758606 + 2.0 * 6.408106327056885
Epoch 280, val loss: 1.4325746297836304
Epoch 290, training loss: 14.094264030456543 = 1.2878475189208984 + 2.0 * 6.403208255767822
Epoch 290, val loss: 1.398208737373352
Epoch 300, training loss: 14.03581428527832 = 1.2435959577560425 + 2.0 * 6.396109104156494
Epoch 300, val loss: 1.3649439811706543
Epoch 310, training loss: 13.978920936584473 = 1.2003049850463867 + 2.0 * 6.389307975769043
Epoch 310, val loss: 1.332687258720398
Epoch 320, training loss: 13.929935455322266 = 1.1580008268356323 + 2.0 * 6.385967254638672
Epoch 320, val loss: 1.3017038106918335
Epoch 330, training loss: 13.87769603729248 = 1.1171751022338867 + 2.0 * 6.380260467529297
Epoch 330, val loss: 1.2722387313842773
Epoch 340, training loss: 13.82986068725586 = 1.0778270959854126 + 2.0 * 6.376016616821289
Epoch 340, val loss: 1.2443528175354004
Epoch 350, training loss: 13.782756805419922 = 1.0397759675979614 + 2.0 * 6.371490478515625
Epoch 350, val loss: 1.217897891998291
Epoch 360, training loss: 13.739704132080078 = 1.00290846824646 + 2.0 * 6.3683977127075195
Epoch 360, val loss: 1.192827820777893
Epoch 370, training loss: 13.697587013244629 = 0.9672018885612488 + 2.0 * 6.365192413330078
Epoch 370, val loss: 1.16910982131958
Epoch 380, training loss: 13.653894424438477 = 0.9326473474502563 + 2.0 * 6.360623359680176
Epoch 380, val loss: 1.1466981172561646
Epoch 390, training loss: 13.613557815551758 = 0.8991096019744873 + 2.0 * 6.357223987579346
Epoch 390, val loss: 1.1255371570587158
Epoch 400, training loss: 13.58742904663086 = 0.8664297461509705 + 2.0 * 6.360499858856201
Epoch 400, val loss: 1.105458378791809
Epoch 410, training loss: 13.536958694458008 = 0.8350182771682739 + 2.0 * 6.350970268249512
Epoch 410, val loss: 1.0866588354110718
Epoch 420, training loss: 13.500661849975586 = 0.8044712543487549 + 2.0 * 6.348095417022705
Epoch 420, val loss: 1.0690869092941284
Epoch 430, training loss: 13.475854873657227 = 0.7748533487319946 + 2.0 * 6.350500583648682
Epoch 430, val loss: 1.052655577659607
Epoch 440, training loss: 13.436193466186523 = 0.7464523315429688 + 2.0 * 6.344870567321777
Epoch 440, val loss: 1.0375422239303589
Epoch 450, training loss: 13.401057243347168 = 0.7191115617752075 + 2.0 * 6.340972900390625
Epoch 450, val loss: 1.0237221717834473
Epoch 460, training loss: 13.370977401733398 = 0.6927586197853088 + 2.0 * 6.339109420776367
Epoch 460, val loss: 1.0111531019210815
Epoch 470, training loss: 13.338682174682617 = 0.6674565672874451 + 2.0 * 6.335612773895264
Epoch 470, val loss: 0.9997963309288025
Epoch 480, training loss: 13.312803268432617 = 0.6432551145553589 + 2.0 * 6.334774017333984
Epoch 480, val loss: 0.9897346496582031
Epoch 490, training loss: 13.283143043518066 = 0.6200385689735413 + 2.0 * 6.331552028656006
Epoch 490, val loss: 0.9807944893836975
Epoch 500, training loss: 13.276721000671387 = 0.5977019667625427 + 2.0 * 6.3395094871521
Epoch 500, val loss: 0.9729650616645813
Epoch 510, training loss: 13.24782943725586 = 0.5764520764350891 + 2.0 * 6.335688591003418
Epoch 510, val loss: 0.9661694765090942
Epoch 520, training loss: 13.210213661193848 = 0.5561617612838745 + 2.0 * 6.327025890350342
Epoch 520, val loss: 0.9604503512382507
Epoch 530, training loss: 13.184198379516602 = 0.5366528034210205 + 2.0 * 6.32377290725708
Epoch 530, val loss: 0.9555698037147522
Epoch 540, training loss: 13.161340713500977 = 0.5177517533302307 + 2.0 * 6.321794509887695
Epoch 540, val loss: 0.9514371752738953
Epoch 550, training loss: 13.139810562133789 = 0.49936753511428833 + 2.0 * 6.320221424102783
Epoch 550, val loss: 0.947960615158081
Epoch 560, training loss: 13.13895034790039 = 0.4814748167991638 + 2.0 * 6.328737735748291
Epoch 560, val loss: 0.9451332092285156
Epoch 570, training loss: 13.106379508972168 = 0.46397826075553894 + 2.0 * 6.321200847625732
Epoch 570, val loss: 0.9425732493400574
Epoch 580, training loss: 13.078810691833496 = 0.4469878077507019 + 2.0 * 6.315911293029785
Epoch 580, val loss: 0.9406090378761292
Epoch 590, training loss: 13.0589017868042 = 0.43030494451522827 + 2.0 * 6.314298629760742
Epoch 590, val loss: 0.9391273856163025
Epoch 600, training loss: 13.041577339172363 = 0.4138128459453583 + 2.0 * 6.313882350921631
Epoch 600, val loss: 0.9379667043685913
Epoch 610, training loss: 13.023650169372559 = 0.39753174781799316 + 2.0 * 6.313059329986572
Epoch 610, val loss: 0.9372308850288391
Epoch 620, training loss: 13.001418113708496 = 0.38146665692329407 + 2.0 * 6.309975624084473
Epoch 620, val loss: 0.9368284344673157
Epoch 630, training loss: 12.988912582397461 = 0.3655686676502228 + 2.0 * 6.311671733856201
Epoch 630, val loss: 0.9368741512298584
Epoch 640, training loss: 12.968788146972656 = 0.34985196590423584 + 2.0 * 6.3094682693481445
Epoch 640, val loss: 0.9371572732925415
Epoch 650, training loss: 12.958006858825684 = 0.33441177010536194 + 2.0 * 6.311797618865967
Epoch 650, val loss: 0.9379838705062866
Epoch 660, training loss: 12.931309700012207 = 0.3192267119884491 + 2.0 * 6.306041717529297
Epoch 660, val loss: 0.9390363097190857
Epoch 670, training loss: 12.914881706237793 = 0.3043971359729767 + 2.0 * 6.30524206161499
Epoch 670, val loss: 0.940552294254303
Epoch 680, training loss: 12.897048950195312 = 0.28990212082862854 + 2.0 * 6.3035736083984375
Epoch 680, val loss: 0.942598283290863
Epoch 690, training loss: 12.892767906188965 = 0.2757795751094818 + 2.0 * 6.3084940910339355
Epoch 690, val loss: 0.9450568556785583
Epoch 700, training loss: 12.873224258422852 = 0.2621710002422333 + 2.0 * 6.3055267333984375
Epoch 700, val loss: 0.9481039643287659
Epoch 710, training loss: 12.854211807250977 = 0.24902459979057312 + 2.0 * 6.30259370803833
Epoch 710, val loss: 0.951393187046051
Epoch 720, training loss: 12.835354804992676 = 0.23640593886375427 + 2.0 * 6.299474239349365
Epoch 720, val loss: 0.9551402926445007
Epoch 730, training loss: 12.82031536102295 = 0.22433693706989288 + 2.0 * 6.297989368438721
Epoch 730, val loss: 0.9594508409500122
Epoch 740, training loss: 12.807829856872559 = 0.21275334060192108 + 2.0 * 6.2975382804870605
Epoch 740, val loss: 0.9640660285949707
Epoch 750, training loss: 12.806949615478516 = 0.201711505651474 + 2.0 * 6.302618980407715
Epoch 750, val loss: 0.9690101742744446
Epoch 760, training loss: 12.786209106445312 = 0.19127105176448822 + 2.0 * 6.297469139099121
Epoch 760, val loss: 0.974243700504303
Epoch 770, training loss: 12.774337768554688 = 0.1813976913690567 + 2.0 * 6.2964701652526855
Epoch 770, val loss: 0.9796027541160583
Epoch 780, training loss: 12.760807991027832 = 0.17206522822380066 + 2.0 * 6.294371604919434
Epoch 780, val loss: 0.9851852059364319
Epoch 790, training loss: 12.747295379638672 = 0.1632869839668274 + 2.0 * 6.292004108428955
Epoch 790, val loss: 0.9910564422607422
Epoch 800, training loss: 12.74071979522705 = 0.15499064326286316 + 2.0 * 6.292864799499512
Epoch 800, val loss: 0.9970791339874268
Epoch 810, training loss: 12.733145713806152 = 0.14717409014701843 + 2.0 * 6.292985916137695
Epoch 810, val loss: 1.0030802488327026
Epoch 820, training loss: 12.72933292388916 = 0.139835387468338 + 2.0 * 6.294748783111572
Epoch 820, val loss: 1.0093003511428833
Epoch 830, training loss: 12.715152740478516 = 0.13299164175987244 + 2.0 * 6.291080474853516
Epoch 830, val loss: 1.0156095027923584
Epoch 840, training loss: 12.702289581298828 = 0.12655600905418396 + 2.0 * 6.287866592407227
Epoch 840, val loss: 1.0220327377319336
Epoch 850, training loss: 12.693391799926758 = 0.12049516290426254 + 2.0 * 6.2864484786987305
Epoch 850, val loss: 1.0285406112670898
Epoch 860, training loss: 12.690784454345703 = 0.11477817595005035 + 2.0 * 6.288002967834473
Epoch 860, val loss: 1.0350605249404907
Epoch 870, training loss: 12.681809425354004 = 0.10941392928361893 + 2.0 * 6.286197662353516
Epoch 870, val loss: 1.0417762994766235
Epoch 880, training loss: 12.68327808380127 = 0.10436553508043289 + 2.0 * 6.289456367492676
Epoch 880, val loss: 1.0483102798461914
Epoch 890, training loss: 12.667874336242676 = 0.0996628999710083 + 2.0 * 6.2841057777404785
Epoch 890, val loss: 1.0551680326461792
Epoch 900, training loss: 12.66077995300293 = 0.09521396458148956 + 2.0 * 6.282783031463623
Epoch 900, val loss: 1.061924695968628
Epoch 910, training loss: 12.698110580444336 = 0.09101780503988266 + 2.0 * 6.30354642868042
Epoch 910, val loss: 1.0685958862304688
Epoch 920, training loss: 12.649150848388672 = 0.08709827810525894 + 2.0 * 6.281026363372803
Epoch 920, val loss: 1.0753458738327026
Epoch 930, training loss: 12.647740364074707 = 0.083414226770401 + 2.0 * 6.282163143157959
Epoch 930, val loss: 1.0822287797927856
Epoch 940, training loss: 12.638428688049316 = 0.07992438226938248 + 2.0 * 6.279252052307129
Epoch 940, val loss: 1.0889643430709839
Epoch 950, training loss: 12.63398265838623 = 0.07662065327167511 + 2.0 * 6.278680801391602
Epoch 950, val loss: 1.095706820487976
Epoch 960, training loss: 12.641556739807129 = 0.07349612563848495 + 2.0 * 6.284030437469482
Epoch 960, val loss: 1.1024888753890991
Epoch 970, training loss: 12.634051322937012 = 0.0705476701259613 + 2.0 * 6.28175163269043
Epoch 970, val loss: 1.1091572046279907
Epoch 980, training loss: 12.627878189086914 = 0.06776395440101624 + 2.0 * 6.280056953430176
Epoch 980, val loss: 1.115869164466858
Epoch 990, training loss: 12.616745948791504 = 0.06512797623872757 + 2.0 * 6.275808811187744
Epoch 990, val loss: 1.122430443763733
Epoch 1000, training loss: 12.613943099975586 = 0.06263140588998795 + 2.0 * 6.275655746459961
Epoch 1000, val loss: 1.128983974456787
Epoch 1010, training loss: 12.621651649475098 = 0.060257501900196075 + 2.0 * 6.280696868896484
Epoch 1010, val loss: 1.1354713439941406
Epoch 1020, training loss: 12.618449211120605 = 0.05801817402243614 + 2.0 * 6.280215740203857
Epoch 1020, val loss: 1.1420841217041016
Epoch 1030, training loss: 12.60664176940918 = 0.05588548630475998 + 2.0 * 6.275378227233887
Epoch 1030, val loss: 1.1485888957977295
Epoch 1040, training loss: 12.599151611328125 = 0.05385290086269379 + 2.0 * 6.27264928817749
Epoch 1040, val loss: 1.1551010608673096
Epoch 1050, training loss: 12.596903800964355 = 0.05190890282392502 + 2.0 * 6.272497653961182
Epoch 1050, val loss: 1.161497712135315
Epoch 1060, training loss: 12.611209869384766 = 0.05006837844848633 + 2.0 * 6.2805705070495605
Epoch 1060, val loss: 1.1678144931793213
Epoch 1070, training loss: 12.592183113098145 = 0.04829994961619377 + 2.0 * 6.271941661834717
Epoch 1070, val loss: 1.1738215684890747
Epoch 1080, training loss: 12.588367462158203 = 0.04664519056677818 + 2.0 * 6.2708611488342285
Epoch 1080, val loss: 1.180046558380127
Epoch 1090, training loss: 12.582797050476074 = 0.04505769535899162 + 2.0 * 6.268869876861572
Epoch 1090, val loss: 1.186128854751587
Epoch 1100, training loss: 12.586029052734375 = 0.04353926703333855 + 2.0 * 6.271245002746582
Epoch 1100, val loss: 1.192135214805603
Epoch 1110, training loss: 12.578936576843262 = 0.042107243090867996 + 2.0 * 6.268414497375488
Epoch 1110, val loss: 1.1980884075164795
Epoch 1120, training loss: 12.580087661743164 = 0.04072315990924835 + 2.0 * 6.26968240737915
Epoch 1120, val loss: 1.2040727138519287
Epoch 1130, training loss: 12.57367992401123 = 0.03942089155316353 + 2.0 * 6.267129421234131
Epoch 1130, val loss: 1.2099146842956543
Epoch 1140, training loss: 12.569944381713867 = 0.038165461272001266 + 2.0 * 6.265889644622803
Epoch 1140, val loss: 1.2157096862792969
Epoch 1150, training loss: 12.574382781982422 = 0.03696271404623985 + 2.0 * 6.268710136413574
Epoch 1150, val loss: 1.2213847637176514
Epoch 1160, training loss: 12.5697603225708 = 0.0358247347176075 + 2.0 * 6.2669677734375
Epoch 1160, val loss: 1.2269419431686401
Epoch 1170, training loss: 12.570306777954102 = 0.03472708910703659 + 2.0 * 6.267789840698242
Epoch 1170, val loss: 1.2326005697250366
Epoch 1180, training loss: 12.562248229980469 = 0.033693280071020126 + 2.0 * 6.264277458190918
Epoch 1180, val loss: 1.2380820512771606
Epoch 1190, training loss: 12.560216903686523 = 0.03269485756754875 + 2.0 * 6.263761043548584
Epoch 1190, val loss: 1.243574857711792
Epoch 1200, training loss: 12.569714546203613 = 0.03174032270908356 + 2.0 * 6.26898717880249
Epoch 1200, val loss: 1.2488980293273926
Epoch 1210, training loss: 12.563871383666992 = 0.030821336433291435 + 2.0 * 6.266524791717529
Epoch 1210, val loss: 1.2541835308074951
Epoch 1220, training loss: 12.559475898742676 = 0.029948081821203232 + 2.0 * 6.264763832092285
Epoch 1220, val loss: 1.259530782699585
Epoch 1230, training loss: 12.554157257080078 = 0.029109720140695572 + 2.0 * 6.262523651123047
Epoch 1230, val loss: 1.2646563053131104
Epoch 1240, training loss: 12.5510835647583 = 0.0283069908618927 + 2.0 * 6.261388301849365
Epoch 1240, val loss: 1.269837498664856
Epoch 1250, training loss: 12.552264213562012 = 0.02753361500799656 + 2.0 * 6.262365341186523
Epoch 1250, val loss: 1.2749353647232056
Epoch 1260, training loss: 12.554408073425293 = 0.026791715994477272 + 2.0 * 6.263808250427246
Epoch 1260, val loss: 1.2799906730651855
Epoch 1270, training loss: 12.555480003356934 = 0.02607610635459423 + 2.0 * 6.264701843261719
Epoch 1270, val loss: 1.2848933935165405
Epoch 1280, training loss: 12.550085067749023 = 0.02539234608411789 + 2.0 * 6.262346267700195
Epoch 1280, val loss: 1.2897616624832153
Epoch 1290, training loss: 12.544251441955566 = 0.024734249338507652 + 2.0 * 6.259758472442627
Epoch 1290, val loss: 1.2946027517318726
Epoch 1300, training loss: 12.541749954223633 = 0.02410321868956089 + 2.0 * 6.258823394775391
Epoch 1300, val loss: 1.2994155883789062
Epoch 1310, training loss: 12.552759170532227 = 0.023497534915804863 + 2.0 * 6.2646307945251465
Epoch 1310, val loss: 1.3040846586227417
Epoch 1320, training loss: 12.539332389831543 = 0.022905027493834496 + 2.0 * 6.258213520050049
Epoch 1320, val loss: 1.308704137802124
Epoch 1330, training loss: 12.537675857543945 = 0.022342601791024208 + 2.0 * 6.25766658782959
Epoch 1330, val loss: 1.3133467435836792
Epoch 1340, training loss: 12.55410099029541 = 0.02180376835167408 + 2.0 * 6.266148567199707
Epoch 1340, val loss: 1.317780613899231
Epoch 1350, training loss: 12.533360481262207 = 0.021279947832226753 + 2.0 * 6.256040096282959
Epoch 1350, val loss: 1.322260856628418
Epoch 1360, training loss: 12.532665252685547 = 0.020779235288500786 + 2.0 * 6.2559428215026855
Epoch 1360, val loss: 1.3266764879226685
Epoch 1370, training loss: 12.53088665008545 = 0.020296113565564156 + 2.0 * 6.255295276641846
Epoch 1370, val loss: 1.3310805559158325
Epoch 1380, training loss: 12.541833877563477 = 0.019828248769044876 + 2.0 * 6.261003017425537
Epoch 1380, val loss: 1.3354136943817139
Epoch 1390, training loss: 12.535178184509277 = 0.019375817850232124 + 2.0 * 6.257901191711426
Epoch 1390, val loss: 1.3395191431045532
Epoch 1400, training loss: 12.531571388244629 = 0.018940972164273262 + 2.0 * 6.256315231323242
Epoch 1400, val loss: 1.3437085151672363
Epoch 1410, training loss: 12.529664039611816 = 0.018521875143051147 + 2.0 * 6.255570888519287
Epoch 1410, val loss: 1.3478318452835083
Epoch 1420, training loss: 12.525699615478516 = 0.018115274608135223 + 2.0 * 6.2537922859191895
Epoch 1420, val loss: 1.351933240890503
Epoch 1430, training loss: 12.528225898742676 = 0.017722535878419876 + 2.0 * 6.255251884460449
Epoch 1430, val loss: 1.3559753894805908
Epoch 1440, training loss: 12.523500442504883 = 0.017341863363981247 + 2.0 * 6.253079414367676
Epoch 1440, val loss: 1.359875202178955
Epoch 1450, training loss: 12.525022506713867 = 0.01697215624153614 + 2.0 * 6.254024982452393
Epoch 1450, val loss: 1.3638032674789429
Epoch 1460, training loss: 12.534311294555664 = 0.016617311164736748 + 2.0 * 6.258846759796143
Epoch 1460, val loss: 1.3676190376281738
Epoch 1470, training loss: 12.52210521697998 = 0.01627112552523613 + 2.0 * 6.2529168128967285
Epoch 1470, val loss: 1.3714429140090942
Epoch 1480, training loss: 12.519399642944336 = 0.015936434268951416 + 2.0 * 6.2517313957214355
Epoch 1480, val loss: 1.3752586841583252
Epoch 1490, training loss: 12.538623809814453 = 0.01561395451426506 + 2.0 * 6.261505126953125
Epoch 1490, val loss: 1.3787037134170532
Epoch 1500, training loss: 12.51917839050293 = 0.015304205939173698 + 2.0 * 6.251936912536621
Epoch 1500, val loss: 1.3824583292007446
Epoch 1510, training loss: 12.51481819152832 = 0.01500015426427126 + 2.0 * 6.249908924102783
Epoch 1510, val loss: 1.3861467838287354
Epoch 1520, training loss: 12.512877464294434 = 0.014708729460835457 + 2.0 * 6.24908447265625
Epoch 1520, val loss: 1.3896974325180054
Epoch 1530, training loss: 12.51187801361084 = 0.01442271750420332 + 2.0 * 6.248727798461914
Epoch 1530, val loss: 1.3931827545166016
Epoch 1540, training loss: 12.555051803588867 = 0.014145800843834877 + 2.0 * 6.270452976226807
Epoch 1540, val loss: 1.3965072631835938
Epoch 1550, training loss: 12.51309585571289 = 0.013879808597266674 + 2.0 * 6.249608039855957
Epoch 1550, val loss: 1.3998969793319702
Epoch 1560, training loss: 12.51295280456543 = 0.013618772849440575 + 2.0 * 6.249667167663574
Epoch 1560, val loss: 1.403275728225708
Epoch 1570, training loss: 12.50770378112793 = 0.013368844985961914 + 2.0 * 6.247167587280273
Epoch 1570, val loss: 1.4066239595413208
Epoch 1580, training loss: 12.506811141967773 = 0.013123508542776108 + 2.0 * 6.2468438148498535
Epoch 1580, val loss: 1.4099611043930054
Epoch 1590, training loss: 12.512319564819336 = 0.01288507878780365 + 2.0 * 6.2497172355651855
Epoch 1590, val loss: 1.4132018089294434
Epoch 1600, training loss: 12.507180213928223 = 0.012653516605496407 + 2.0 * 6.247263431549072
Epoch 1600, val loss: 1.4162017107009888
Epoch 1610, training loss: 12.508498191833496 = 0.012427159585058689 + 2.0 * 6.248035430908203
Epoch 1610, val loss: 1.4194144010543823
Epoch 1620, training loss: 12.505600929260254 = 0.012211209163069725 + 2.0 * 6.246695041656494
Epoch 1620, val loss: 1.4224787950515747
Epoch 1630, training loss: 12.506584167480469 = 0.011998062022030354 + 2.0 * 6.247292995452881
Epoch 1630, val loss: 1.4256240129470825
Epoch 1640, training loss: 12.51382064819336 = 0.011791704222559929 + 2.0 * 6.251014709472656
Epoch 1640, val loss: 1.428493618965149
Epoch 1650, training loss: 12.503525733947754 = 0.0115923210978508 + 2.0 * 6.245966911315918
Epoch 1650, val loss: 1.4315805435180664
Epoch 1660, training loss: 12.499791145324707 = 0.011397333815693855 + 2.0 * 6.244196891784668
Epoch 1660, val loss: 1.4344819784164429
Epoch 1670, training loss: 12.499334335327148 = 0.011207060888409615 + 2.0 * 6.244063854217529
Epoch 1670, val loss: 1.4374969005584717
Epoch 1680, training loss: 12.510015487670898 = 0.011022482067346573 + 2.0 * 6.2494964599609375
Epoch 1680, val loss: 1.440331220626831
Epoch 1690, training loss: 12.501810073852539 = 0.010843009687960148 + 2.0 * 6.2454833984375
Epoch 1690, val loss: 1.4432452917099
Epoch 1700, training loss: 12.49893856048584 = 0.01066620647907257 + 2.0 * 6.244136333465576
Epoch 1700, val loss: 1.4459534883499146
Epoch 1710, training loss: 12.504364013671875 = 0.010495923459529877 + 2.0 * 6.246933937072754
Epoch 1710, val loss: 1.4487251043319702
Epoch 1720, training loss: 12.4966402053833 = 0.01032943744212389 + 2.0 * 6.243155479431152
Epoch 1720, val loss: 1.4515502452850342
Epoch 1730, training loss: 12.496282577514648 = 0.010167870670557022 + 2.0 * 6.2430572509765625
Epoch 1730, val loss: 1.4542474746704102
Epoch 1740, training loss: 12.499375343322754 = 0.01000946294516325 + 2.0 * 6.244682788848877
Epoch 1740, val loss: 1.4569792747497559
Epoch 1750, training loss: 12.504040718078613 = 0.009854689240455627 + 2.0 * 6.247093200683594
Epoch 1750, val loss: 1.4595853090286255
Epoch 1760, training loss: 12.49630355834961 = 0.00970414374023676 + 2.0 * 6.24329948425293
Epoch 1760, val loss: 1.4620755910873413
Epoch 1770, training loss: 12.492594718933105 = 0.009558431804180145 + 2.0 * 6.241518020629883
Epoch 1770, val loss: 1.4648280143737793
Epoch 1780, training loss: 12.491872787475586 = 0.009416521526873112 + 2.0 * 6.241228103637695
Epoch 1780, val loss: 1.4674235582351685
Epoch 1790, training loss: 12.49569034576416 = 0.009277231059968472 + 2.0 * 6.24320650100708
Epoch 1790, val loss: 1.46999192237854
Epoch 1800, training loss: 12.496048927307129 = 0.009140605106949806 + 2.0 * 6.2434539794921875
Epoch 1800, val loss: 1.4724103212356567
Epoch 1810, training loss: 12.493613243103027 = 0.009004526771605015 + 2.0 * 6.24230432510376
Epoch 1810, val loss: 1.4749577045440674
Epoch 1820, training loss: 12.500855445861816 = 0.008875575847923756 + 2.0 * 6.245989799499512
Epoch 1820, val loss: 1.4773728847503662
Epoch 1830, training loss: 12.497057914733887 = 0.008748156018555164 + 2.0 * 6.244154930114746
Epoch 1830, val loss: 1.4797625541687012
Epoch 1840, training loss: 12.489501953125 = 0.008624870330095291 + 2.0 * 6.240438461303711
Epoch 1840, val loss: 1.4820612668991089
Epoch 1850, training loss: 12.487048149108887 = 0.008503659628331661 + 2.0 * 6.239272117614746
Epoch 1850, val loss: 1.484558343887329
Epoch 1860, training loss: 12.486224174499512 = 0.00838522333651781 + 2.0 * 6.238919258117676
Epoch 1860, val loss: 1.4869868755340576
Epoch 1870, training loss: 12.494377136230469 = 0.008268792182207108 + 2.0 * 6.243054389953613
Epoch 1870, val loss: 1.4893046617507935
Epoch 1880, training loss: 12.497648239135742 = 0.00815586932003498 + 2.0 * 6.244746208190918
Epoch 1880, val loss: 1.491345763206482
Epoch 1890, training loss: 12.489750862121582 = 0.008044491522014141 + 2.0 * 6.240853309631348
Epoch 1890, val loss: 1.4937429428100586
Epoch 1900, training loss: 12.48625659942627 = 0.007937007583677769 + 2.0 * 6.23915958404541
Epoch 1900, val loss: 1.4959098100662231
Epoch 1910, training loss: 12.494447708129883 = 0.007832362316548824 + 2.0 * 6.243307590484619
Epoch 1910, val loss: 1.498173475265503
Epoch 1920, training loss: 12.482789039611816 = 0.007728276774287224 + 2.0 * 6.23753023147583
Epoch 1920, val loss: 1.5002562999725342
Epoch 1930, training loss: 12.482000350952148 = 0.007627084385603666 + 2.0 * 6.237186431884766
Epoch 1930, val loss: 1.5024007558822632
Epoch 1940, training loss: 12.482547760009766 = 0.007528990041464567 + 2.0 * 6.237509250640869
Epoch 1940, val loss: 1.5046017169952393
Epoch 1950, training loss: 12.496820449829102 = 0.007432388607412577 + 2.0 * 6.244694232940674
Epoch 1950, val loss: 1.5067217350006104
Epoch 1960, training loss: 12.48273754119873 = 0.007337027695029974 + 2.0 * 6.237700462341309
Epoch 1960, val loss: 1.508653163909912
Epoch 1970, training loss: 12.48198413848877 = 0.007242618128657341 + 2.0 * 6.23737096786499
Epoch 1970, val loss: 1.5106775760650635
Epoch 1980, training loss: 12.494317054748535 = 0.00715290242806077 + 2.0 * 6.243582248687744
Epoch 1980, val loss: 1.5127229690551758
Epoch 1990, training loss: 12.480018615722656 = 0.00706349266692996 + 2.0 * 6.236477375030518
Epoch 1990, val loss: 1.5146715641021729
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7148148148148148
0.7954665260938324
=== training gcn model ===
Epoch 0, training loss: 19.141263961791992 = 1.9476372003555298 + 2.0 * 8.596813201904297
Epoch 0, val loss: 1.9415093660354614
Epoch 10, training loss: 19.131135940551758 = 1.9382586479187012 + 2.0 * 8.59643840789795
Epoch 10, val loss: 1.9325730800628662
Epoch 20, training loss: 19.113208770751953 = 1.927044153213501 + 2.0 * 8.593082427978516
Epoch 20, val loss: 1.921407699584961
Epoch 30, training loss: 19.043617248535156 = 1.912110686302185 + 2.0 * 8.565752983093262
Epoch 30, val loss: 1.9064104557037354
Epoch 40, training loss: 18.619491577148438 = 1.8945093154907227 + 2.0 * 8.362491607666016
Epoch 40, val loss: 1.8896712064743042
Epoch 50, training loss: 17.414318084716797 = 1.876975178718567 + 2.0 * 7.76867151260376
Epoch 50, val loss: 1.8731950521469116
Epoch 60, training loss: 16.483150482177734 = 1.8626277446746826 + 2.0 * 7.310261249542236
Epoch 60, val loss: 1.860386610031128
Epoch 70, training loss: 15.864526748657227 = 1.8490506410598755 + 2.0 * 7.00773811340332
Epoch 70, val loss: 1.8481053113937378
Epoch 80, training loss: 15.613030433654785 = 1.8364989757537842 + 2.0 * 6.888265609741211
Epoch 80, val loss: 1.8364455699920654
Epoch 90, training loss: 15.400298118591309 = 1.8231070041656494 + 2.0 * 6.788595676422119
Epoch 90, val loss: 1.8241827487945557
Epoch 100, training loss: 15.235496520996094 = 1.8104838132858276 + 2.0 * 6.712506294250488
Epoch 100, val loss: 1.8130896091461182
Epoch 110, training loss: 15.107137680053711 = 1.7983994483947754 + 2.0 * 6.654368877410889
Epoch 110, val loss: 1.8027770519256592
Epoch 120, training loss: 15.011959075927734 = 1.7864230871200562 + 2.0 * 6.612768173217773
Epoch 120, val loss: 1.792641520500183
Epoch 130, training loss: 14.917739868164062 = 1.7741702795028687 + 2.0 * 6.571784973144531
Epoch 130, val loss: 1.7826787233352661
Epoch 140, training loss: 14.849258422851562 = 1.7614271640777588 + 2.0 * 6.543915748596191
Epoch 140, val loss: 1.7721318006515503
Epoch 150, training loss: 14.79313850402832 = 1.747656226158142 + 2.0 * 6.522741317749023
Epoch 150, val loss: 1.7608592510223389
Epoch 160, training loss: 14.737869262695312 = 1.7324585914611816 + 2.0 * 6.502705097198486
Epoch 160, val loss: 1.7484796047210693
Epoch 170, training loss: 14.693422317504883 = 1.7155064344406128 + 2.0 * 6.48895788192749
Epoch 170, val loss: 1.7347030639648438
Epoch 180, training loss: 14.642409324645996 = 1.6963930130004883 + 2.0 * 6.473008155822754
Epoch 180, val loss: 1.7192665338516235
Epoch 190, training loss: 14.594688415527344 = 1.6748331785202026 + 2.0 * 6.459927558898926
Epoch 190, val loss: 1.7018388509750366
Epoch 200, training loss: 14.548026084899902 = 1.6507567167282104 + 2.0 * 6.448634624481201
Epoch 200, val loss: 1.6823967695236206
Epoch 210, training loss: 14.49878215789795 = 1.6238629817962646 + 2.0 * 6.437459468841553
Epoch 210, val loss: 1.6605952978134155
Epoch 220, training loss: 14.448698043823242 = 1.5937485694885254 + 2.0 * 6.427474498748779
Epoch 220, val loss: 1.6361538171768188
Epoch 230, training loss: 14.405369758605957 = 1.560439944267273 + 2.0 * 6.422464847564697
Epoch 230, val loss: 1.6091513633728027
Epoch 240, training loss: 14.347965240478516 = 1.524678111076355 + 2.0 * 6.4116435050964355
Epoch 240, val loss: 1.58014976978302
Epoch 250, training loss: 14.295034408569336 = 1.4862043857574463 + 2.0 * 6.404415130615234
Epoch 250, val loss: 1.5491540431976318
Epoch 260, training loss: 14.239971160888672 = 1.445208191871643 + 2.0 * 6.39738130569458
Epoch 260, val loss: 1.516276240348816
Epoch 270, training loss: 14.198935508728027 = 1.4023723602294922 + 2.0 * 6.398281574249268
Epoch 270, val loss: 1.4820995330810547
Epoch 280, training loss: 14.131913185119629 = 1.358666181564331 + 2.0 * 6.386623382568359
Epoch 280, val loss: 1.4478559494018555
Epoch 290, training loss: 14.07615852355957 = 1.3147934675216675 + 2.0 * 6.380682468414307
Epoch 290, val loss: 1.4137333631515503
Epoch 300, training loss: 14.023005485534668 = 1.2709957361221313 + 2.0 * 6.376004695892334
Epoch 300, val loss: 1.3800889253616333
Epoch 310, training loss: 13.975448608398438 = 1.2281017303466797 + 2.0 * 6.373673439025879
Epoch 310, val loss: 1.3475689888000488
Epoch 320, training loss: 13.925016403198242 = 1.1868507862091064 + 2.0 * 6.369082927703857
Epoch 320, val loss: 1.3168894052505493
Epoch 330, training loss: 13.874882698059082 = 1.1470822095870972 + 2.0 * 6.363900184631348
Epoch 330, val loss: 1.2876927852630615
Epoch 340, training loss: 13.827845573425293 = 1.1085997819900513 + 2.0 * 6.359622955322266
Epoch 340, val loss: 1.2598768472671509
Epoch 350, training loss: 13.792409896850586 = 1.0714499950408936 + 2.0 * 6.360479831695557
Epoch 350, val loss: 1.2334895133972168
Epoch 360, training loss: 13.753609657287598 = 1.035820722579956 + 2.0 * 6.358894348144531
Epoch 360, val loss: 1.2087146043777466
Epoch 370, training loss: 13.70501708984375 = 1.0014945268630981 + 2.0 * 6.351761341094971
Epoch 370, val loss: 1.1855807304382324
Epoch 380, training loss: 13.662848472595215 = 0.9684074521064758 + 2.0 * 6.347220420837402
Epoch 380, val loss: 1.1635570526123047
Epoch 390, training loss: 13.623183250427246 = 0.9361423254013062 + 2.0 * 6.343520641326904
Epoch 390, val loss: 1.1427180767059326
Epoch 400, training loss: 13.610655784606934 = 0.9045756459236145 + 2.0 * 6.3530402183532715
Epoch 400, val loss: 1.1229530572891235
Epoch 410, training loss: 13.555749893188477 = 0.8742611408233643 + 2.0 * 6.340744495391846
Epoch 410, val loss: 1.104146957397461
Epoch 420, training loss: 13.51865291595459 = 0.8447704315185547 + 2.0 * 6.336941242218018
Epoch 420, val loss: 1.0865012407302856
Epoch 430, training loss: 13.483841896057129 = 0.8159202933311462 + 2.0 * 6.333961009979248
Epoch 430, val loss: 1.0696443319320679
Epoch 440, training loss: 13.462167739868164 = 0.7876669764518738 + 2.0 * 6.337250232696533
Epoch 440, val loss: 1.0538147687911987
Epoch 450, training loss: 13.422341346740723 = 0.7602436542510986 + 2.0 * 6.331048965454102
Epoch 450, val loss: 1.0389069318771362
Epoch 460, training loss: 13.386832237243652 = 0.7335123419761658 + 2.0 * 6.32666015625
Epoch 460, val loss: 1.0249656438827515
Epoch 470, training loss: 13.361099243164062 = 0.7074553370475769 + 2.0 * 6.326821804046631
Epoch 470, val loss: 1.0119589567184448
Epoch 480, training loss: 13.34090805053711 = 0.6821662783622742 + 2.0 * 6.329370975494385
Epoch 480, val loss: 0.999903678894043
Epoch 490, training loss: 13.302605628967285 = 0.6577499508857727 + 2.0 * 6.322427749633789
Epoch 490, val loss: 0.9890941381454468
Epoch 500, training loss: 13.272624969482422 = 0.6340327262878418 + 2.0 * 6.319296360015869
Epoch 500, val loss: 0.9790186285972595
Epoch 510, training loss: 13.245627403259277 = 0.610937774181366 + 2.0 * 6.317344665527344
Epoch 510, val loss: 0.9698628783226013
Epoch 520, training loss: 13.254395484924316 = 0.5885028839111328 + 2.0 * 6.332946300506592
Epoch 520, val loss: 0.9614295959472656
Epoch 530, training loss: 13.205081939697266 = 0.5667224526405334 + 2.0 * 6.319179534912109
Epoch 530, val loss: 0.9541499614715576
Epoch 540, training loss: 13.173375129699707 = 0.5457156896591187 + 2.0 * 6.3138298988342285
Epoch 540, val loss: 0.947655975818634
Epoch 550, training loss: 13.147744178771973 = 0.5251882672309875 + 2.0 * 6.311277866363525
Epoch 550, val loss: 0.941772997379303
Epoch 560, training loss: 13.124722480773926 = 0.5050511956214905 + 2.0 * 6.309835433959961
Epoch 560, val loss: 0.9365043640136719
Epoch 570, training loss: 13.128559112548828 = 0.4852766990661621 + 2.0 * 6.321640968322754
Epoch 570, val loss: 0.9317523837089539
Epoch 580, training loss: 13.08028793334961 = 0.4659305810928345 + 2.0 * 6.307178497314453
Epoch 580, val loss: 0.9276297688484192
Epoch 590, training loss: 13.059011459350586 = 0.44697123765945435 + 2.0 * 6.306020259857178
Epoch 590, val loss: 0.9241417050361633
Epoch 600, training loss: 13.038143157958984 = 0.42832645773887634 + 2.0 * 6.304908275604248
Epoch 600, val loss: 0.921035647392273
Epoch 610, training loss: 13.034323692321777 = 0.40991467237472534 + 2.0 * 6.312204360961914
Epoch 610, val loss: 0.9185333847999573
Epoch 620, training loss: 13.002792358398438 = 0.39206036925315857 + 2.0 * 6.305366039276123
Epoch 620, val loss: 0.9161307215690613
Epoch 630, training loss: 12.979683876037598 = 0.3745079040527344 + 2.0 * 6.302587985992432
Epoch 630, val loss: 0.9145007729530334
Epoch 640, training loss: 12.958160400390625 = 0.35728445649147034 + 2.0 * 6.300437927246094
Epoch 640, val loss: 0.9132973551750183
Epoch 650, training loss: 12.938485145568848 = 0.3404088616371155 + 2.0 * 6.299037933349609
Epoch 650, val loss: 0.9126760363578796
Epoch 660, training loss: 12.931462287902832 = 0.3239222466945648 + 2.0 * 6.303770065307617
Epoch 660, val loss: 0.912395179271698
Epoch 670, training loss: 12.901956558227539 = 0.30802011489868164 + 2.0 * 6.296968460083008
Epoch 670, val loss: 0.9128633737564087
Epoch 680, training loss: 12.885077476501465 = 0.2926134765148163 + 2.0 * 6.296232223510742
Epoch 680, val loss: 0.9136403203010559
Epoch 690, training loss: 12.867362022399902 = 0.2776917815208435 + 2.0 * 6.294835090637207
Epoch 690, val loss: 0.915028989315033
Epoch 700, training loss: 12.859339714050293 = 0.2633601427078247 + 2.0 * 6.297989845275879
Epoch 700, val loss: 0.9169057011604309
Epoch 710, training loss: 12.842534065246582 = 0.24958299100399017 + 2.0 * 6.296475410461426
Epoch 710, val loss: 0.9192593693733215
Epoch 720, training loss: 12.821144104003906 = 0.23647457361221313 + 2.0 * 6.29233455657959
Epoch 720, val loss: 0.922052800655365
Epoch 730, training loss: 12.807985305786133 = 0.22400563955307007 + 2.0 * 6.291989803314209
Epoch 730, val loss: 0.9253982305526733
Epoch 740, training loss: 12.793351173400879 = 0.212171733379364 + 2.0 * 6.290589809417725
Epoch 740, val loss: 0.9289955496788025
Epoch 750, training loss: 12.784278869628906 = 0.20095263421535492 + 2.0 * 6.29166316986084
Epoch 750, val loss: 0.9330872893333435
Epoch 760, training loss: 12.7719144821167 = 0.1903415024280548 + 2.0 * 6.290786266326904
Epoch 760, val loss: 0.9376770853996277
Epoch 770, training loss: 12.75831127166748 = 0.1803656369447708 + 2.0 * 6.288972854614258
Epoch 770, val loss: 0.9424227476119995
Epoch 780, training loss: 12.74319076538086 = 0.17095781862735748 + 2.0 * 6.286116600036621
Epoch 780, val loss: 0.9475479125976562
Epoch 790, training loss: 12.734000205993652 = 0.1621008962392807 + 2.0 * 6.28594970703125
Epoch 790, val loss: 0.952964723110199
Epoch 800, training loss: 12.735608100891113 = 0.15379339456558228 + 2.0 * 6.290907382965088
Epoch 800, val loss: 0.9584166407585144
Epoch 810, training loss: 12.718060493469238 = 0.1459685117006302 + 2.0 * 6.286046028137207
Epoch 810, val loss: 0.9643474221229553
Epoch 820, training loss: 12.703783988952637 = 0.13866710662841797 + 2.0 * 6.282558441162109
Epoch 820, val loss: 0.9703385233879089
Epoch 830, training loss: 12.695724487304688 = 0.1318160593509674 + 2.0 * 6.281954288482666
Epoch 830, val loss: 0.9764403104782104
Epoch 840, training loss: 12.688080787658691 = 0.12536999583244324 + 2.0 * 6.281355381011963
Epoch 840, val loss: 0.9827507734298706
Epoch 850, training loss: 12.68836498260498 = 0.11932510882616043 + 2.0 * 6.284520149230957
Epoch 850, val loss: 0.9889598488807678
Epoch 860, training loss: 12.679580688476562 = 0.11363635212182999 + 2.0 * 6.28297233581543
Epoch 860, val loss: 0.9958162903785706
Epoch 870, training loss: 12.666152954101562 = 0.10831845551729202 + 2.0 * 6.27891731262207
Epoch 870, val loss: 1.002177119255066
Epoch 880, training loss: 12.658957481384277 = 0.10331862419843674 + 2.0 * 6.277819633483887
Epoch 880, val loss: 1.0086828470230103
Epoch 890, training loss: 12.671219825744629 = 0.09861291199922562 + 2.0 * 6.286303520202637
Epoch 890, val loss: 1.0152170658111572
Epoch 900, training loss: 12.655195236206055 = 0.0941699668765068 + 2.0 * 6.280512809753418
Epoch 900, val loss: 1.0219123363494873
Epoch 910, training loss: 12.641853332519531 = 0.09001243859529495 + 2.0 * 6.275920391082764
Epoch 910, val loss: 1.0285319089889526
Epoch 920, training loss: 12.63524341583252 = 0.08608894050121307 + 2.0 * 6.2745771408081055
Epoch 920, val loss: 1.035177230834961
Epoch 930, training loss: 12.644625663757324 = 0.08239878714084625 + 2.0 * 6.281113624572754
Epoch 930, val loss: 1.0417566299438477
Epoch 940, training loss: 12.633452415466309 = 0.07888301461935043 + 2.0 * 6.277284622192383
Epoch 940, val loss: 1.0484848022460938
Epoch 950, training loss: 12.622854232788086 = 0.07560981810092926 + 2.0 * 6.273622035980225
Epoch 950, val loss: 1.055216670036316
Epoch 960, training loss: 12.616109848022461 = 0.07250438630580902 + 2.0 * 6.27180290222168
Epoch 960, val loss: 1.061714768409729
Epoch 970, training loss: 12.621726989746094 = 0.06958078593015671 + 2.0 * 6.276072978973389
Epoch 970, val loss: 1.068314552307129
Epoch 980, training loss: 12.61313247680664 = 0.06679993122816086 + 2.0 * 6.273166179656982
Epoch 980, val loss: 1.0749152898788452
Epoch 990, training loss: 12.60594367980957 = 0.06418406963348389 + 2.0 * 6.270879745483398
Epoch 990, val loss: 1.0815309286117554
Epoch 1000, training loss: 12.600799560546875 = 0.06170378625392914 + 2.0 * 6.269547939300537
Epoch 1000, val loss: 1.0880427360534668
Epoch 1010, training loss: 12.606203079223633 = 0.05935352295637131 + 2.0 * 6.2734246253967285
Epoch 1010, val loss: 1.0945535898208618
Epoch 1020, training loss: 12.595251083374023 = 0.05713880434632301 + 2.0 * 6.26905632019043
Epoch 1020, val loss: 1.1009087562561035
Epoch 1030, training loss: 12.591846466064453 = 0.05502588301897049 + 2.0 * 6.2684102058410645
Epoch 1030, val loss: 1.1074159145355225
Epoch 1040, training loss: 12.593025207519531 = 0.0530313178896904 + 2.0 * 6.2699971199035645
Epoch 1040, val loss: 1.1137250661849976
Epoch 1050, training loss: 12.588464736938477 = 0.051133424043655396 + 2.0 * 6.268665790557861
Epoch 1050, val loss: 1.12002432346344
Epoch 1060, training loss: 12.58137321472168 = 0.049338046461343765 + 2.0 * 6.266017436981201
Epoch 1060, val loss: 1.126304268836975
Epoch 1070, training loss: 12.57823657989502 = 0.04763595387339592 + 2.0 * 6.265300273895264
Epoch 1070, val loss: 1.1325819492340088
Epoch 1080, training loss: 12.576735496520996 = 0.04601629450917244 + 2.0 * 6.265359401702881
Epoch 1080, val loss: 1.1387310028076172
Epoch 1090, training loss: 12.585607528686523 = 0.04447831213474274 + 2.0 * 6.270564556121826
Epoch 1090, val loss: 1.1447795629501343
Epoch 1100, training loss: 12.574870109558105 = 0.04298977181315422 + 2.0 * 6.265940189361572
Epoch 1100, val loss: 1.1510871648788452
Epoch 1110, training loss: 12.570067405700684 = 0.041595928370952606 + 2.0 * 6.264235973358154
Epoch 1110, val loss: 1.1570947170257568
Epoch 1120, training loss: 12.58149528503418 = 0.04025719687342644 + 2.0 * 6.270618915557861
Epoch 1120, val loss: 1.1631863117218018
Epoch 1130, training loss: 12.566629409790039 = 0.03899536654353142 + 2.0 * 6.263816833496094
Epoch 1130, val loss: 1.1690616607666016
Epoch 1140, training loss: 12.560724258422852 = 0.037784479558467865 + 2.0 * 6.261469841003418
Epoch 1140, val loss: 1.1750380992889404
Epoch 1150, training loss: 12.560882568359375 = 0.03663329407572746 + 2.0 * 6.262124538421631
Epoch 1150, val loss: 1.1809682846069336
Epoch 1160, training loss: 12.563121795654297 = 0.03553203120827675 + 2.0 * 6.263794898986816
Epoch 1160, val loss: 1.1866295337677002
Epoch 1170, training loss: 12.554049491882324 = 0.03446946293115616 + 2.0 * 6.259789943695068
Epoch 1170, val loss: 1.192660927772522
Epoch 1180, training loss: 12.552639961242676 = 0.03346532583236694 + 2.0 * 6.259587287902832
Epoch 1180, val loss: 1.1983683109283447
Epoch 1190, training loss: 12.559676170349121 = 0.03249985724687576 + 2.0 * 6.263587951660156
Epoch 1190, val loss: 1.2040932178497314
Epoch 1200, training loss: 12.550912857055664 = 0.03158137574791908 + 2.0 * 6.2596659660339355
Epoch 1200, val loss: 1.2096449136734009
Epoch 1210, training loss: 12.551206588745117 = 0.030700592324137688 + 2.0 * 6.260252952575684
Epoch 1210, val loss: 1.2154980897903442
Epoch 1220, training loss: 12.547563552856445 = 0.029855811968445778 + 2.0 * 6.258853912353516
Epoch 1220, val loss: 1.2208611965179443
Epoch 1230, training loss: 12.544524192810059 = 0.029049720615148544 + 2.0 * 6.257737159729004
Epoch 1230, val loss: 1.2265126705169678
Epoch 1240, training loss: 12.544848442077637 = 0.028274552896618843 + 2.0 * 6.258286952972412
Epoch 1240, val loss: 1.231909155845642
Epoch 1250, training loss: 12.54395866394043 = 0.02752901054918766 + 2.0 * 6.258214950561523
Epoch 1250, val loss: 1.2373056411743164
Epoch 1260, training loss: 12.54366683959961 = 0.026816118508577347 + 2.0 * 6.258425235748291
Epoch 1260, val loss: 1.242666482925415
Epoch 1270, training loss: 12.542720794677734 = 0.026130910962820053 + 2.0 * 6.258295059204102
Epoch 1270, val loss: 1.2479463815689087
Epoch 1280, training loss: 12.535686492919922 = 0.025468263775110245 + 2.0 * 6.2551093101501465
Epoch 1280, val loss: 1.2532832622528076
Epoch 1290, training loss: 12.534266471862793 = 0.024834228679537773 + 2.0 * 6.254715919494629
Epoch 1290, val loss: 1.2585248947143555
Epoch 1300, training loss: 12.534103393554688 = 0.024225661531090736 + 2.0 * 6.254939079284668
Epoch 1300, val loss: 1.2636393308639526
Epoch 1310, training loss: 12.54028034210205 = 0.023639433085918427 + 2.0 * 6.258320331573486
Epoch 1310, val loss: 1.2686543464660645
Epoch 1320, training loss: 12.534453392028809 = 0.023078221827745438 + 2.0 * 6.255687713623047
Epoch 1320, val loss: 1.2738468647003174
Epoch 1330, training loss: 12.529847145080566 = 0.022532708942890167 + 2.0 * 6.253657341003418
Epoch 1330, val loss: 1.2788432836532593
Epoch 1340, training loss: 12.540518760681152 = 0.02201773039996624 + 2.0 * 6.259250640869141
Epoch 1340, val loss: 1.2837332487106323
Epoch 1350, training loss: 12.526443481445312 = 0.021503962576389313 + 2.0 * 6.252469539642334
Epoch 1350, val loss: 1.288759469985962
Epoch 1360, training loss: 12.523139953613281 = 0.021020352840423584 + 2.0 * 6.2510600090026855
Epoch 1360, val loss: 1.2935727834701538
Epoch 1370, training loss: 12.521961212158203 = 0.020552072674036026 + 2.0 * 6.250704765319824
Epoch 1370, val loss: 1.2984706163406372
Epoch 1380, training loss: 12.531820297241211 = 0.020097937434911728 + 2.0 * 6.255861282348633
Epoch 1380, val loss: 1.3032037019729614
Epoch 1390, training loss: 12.523378372192383 = 0.01966055855154991 + 2.0 * 6.251858711242676
Epoch 1390, val loss: 1.3078739643096924
Epoch 1400, training loss: 12.521744728088379 = 0.01923508755862713 + 2.0 * 6.251255035400391
Epoch 1400, val loss: 1.3127182722091675
Epoch 1410, training loss: 12.518474578857422 = 0.01882980205118656 + 2.0 * 6.249822616577148
Epoch 1410, val loss: 1.3172928094863892
Epoch 1420, training loss: 12.52104377746582 = 0.01843666099011898 + 2.0 * 6.251303672790527
Epoch 1420, val loss: 1.321960210800171
Epoch 1430, training loss: 12.518501281738281 = 0.018054049462080002 + 2.0 * 6.250223636627197
Epoch 1430, val loss: 1.3262687921524048
Epoch 1440, training loss: 12.515686988830566 = 0.01767858862876892 + 2.0 * 6.249004364013672
Epoch 1440, val loss: 1.3309773206710815
Epoch 1450, training loss: 12.521383285522461 = 0.017321143299341202 + 2.0 * 6.252030849456787
Epoch 1450, val loss: 1.335400938987732
Epoch 1460, training loss: 12.51210880279541 = 0.01697651296854019 + 2.0 * 6.247566223144531
Epoch 1460, val loss: 1.3396687507629395
Epoch 1470, training loss: 12.512787818908691 = 0.01664273627102375 + 2.0 * 6.248072624206543
Epoch 1470, val loss: 1.3441251516342163
Epoch 1480, training loss: 12.513541221618652 = 0.016315285116434097 + 2.0 * 6.248612880706787
Epoch 1480, val loss: 1.3484196662902832
Epoch 1490, training loss: 12.514744758605957 = 0.016000093892216682 + 2.0 * 6.249372482299805
Epoch 1490, val loss: 1.3525965213775635
Epoch 1500, training loss: 12.512007713317871 = 0.01569439098238945 + 2.0 * 6.248156547546387
Epoch 1500, val loss: 1.3568131923675537
Epoch 1510, training loss: 12.509660720825195 = 0.015399386174976826 + 2.0 * 6.247130870819092
Epoch 1510, val loss: 1.361081838607788
Epoch 1520, training loss: 12.509775161743164 = 0.015112987719476223 + 2.0 * 6.247331142425537
Epoch 1520, val loss: 1.3651641607284546
Epoch 1530, training loss: 12.508868217468262 = 0.014833938330411911 + 2.0 * 6.247016906738281
Epoch 1530, val loss: 1.3692585229873657
Epoch 1540, training loss: 12.507709503173828 = 0.014561455696821213 + 2.0 * 6.2465739250183105
Epoch 1540, val loss: 1.3734357357025146
Epoch 1550, training loss: 12.507079124450684 = 0.014296992681920528 + 2.0 * 6.246391296386719
Epoch 1550, val loss: 1.3773554563522339
Epoch 1560, training loss: 12.51162338256836 = 0.014040880836546421 + 2.0 * 6.248791217803955
Epoch 1560, val loss: 1.3812897205352783
Epoch 1570, training loss: 12.505738258361816 = 0.013793176971375942 + 2.0 * 6.245972633361816
Epoch 1570, val loss: 1.3853466510772705
Epoch 1580, training loss: 12.505311965942383 = 0.013550017960369587 + 2.0 * 6.245881080627441
Epoch 1580, val loss: 1.389253854751587
Epoch 1590, training loss: 12.502531051635742 = 0.013316759839653969 + 2.0 * 6.244606971740723
Epoch 1590, val loss: 1.3931459188461304
Epoch 1600, training loss: 12.512154579162598 = 0.01308936346322298 + 2.0 * 6.249532699584961
Epoch 1600, val loss: 1.3968981504440308
Epoch 1610, training loss: 12.500944137573242 = 0.012870745733380318 + 2.0 * 6.244036674499512
Epoch 1610, val loss: 1.4005998373031616
Epoch 1620, training loss: 12.49716567993164 = 0.012652715668082237 + 2.0 * 6.2422566413879395
Epoch 1620, val loss: 1.4045699834823608
Epoch 1630, training loss: 12.49615478515625 = 0.012443180195987225 + 2.0 * 6.241855621337891
Epoch 1630, val loss: 1.4082506895065308
Epoch 1640, training loss: 12.501349449157715 = 0.01224005501717329 + 2.0 * 6.24455451965332
Epoch 1640, val loss: 1.4119383096694946
Epoch 1650, training loss: 12.506492614746094 = 0.012043722905218601 + 2.0 * 6.2472243309021
Epoch 1650, val loss: 1.41543447971344
Epoch 1660, training loss: 12.496175765991211 = 0.01184489019215107 + 2.0 * 6.242165565490723
Epoch 1660, val loss: 1.419165849685669
Epoch 1670, training loss: 12.494050979614258 = 0.011656370013952255 + 2.0 * 6.241197109222412
Epoch 1670, val loss: 1.422852635383606
Epoch 1680, training loss: 12.492314338684082 = 0.011473400518298149 + 2.0 * 6.240420341491699
Epoch 1680, val loss: 1.426399827003479
Epoch 1690, training loss: 12.49862003326416 = 0.011294861324131489 + 2.0 * 6.243662357330322
Epoch 1690, val loss: 1.4298973083496094
Epoch 1700, training loss: 12.494498252868652 = 0.011118652299046516 + 2.0 * 6.241689682006836
Epoch 1700, val loss: 1.4334778785705566
Epoch 1710, training loss: 12.492008209228516 = 0.010947449132800102 + 2.0 * 6.240530490875244
Epoch 1710, val loss: 1.4369909763336182
Epoch 1720, training loss: 12.49153995513916 = 0.010782739147543907 + 2.0 * 6.240378379821777
Epoch 1720, val loss: 1.440492868423462
Epoch 1730, training loss: 12.499323844909668 = 0.010622460395097733 + 2.0 * 6.244350910186768
Epoch 1730, val loss: 1.4438507556915283
Epoch 1740, training loss: 12.493088722229004 = 0.010463353246450424 + 2.0 * 6.241312503814697
Epoch 1740, val loss: 1.447270154953003
Epoch 1750, training loss: 12.49047565460205 = 0.010310034267604351 + 2.0 * 6.240082740783691
Epoch 1750, val loss: 1.4506655931472778
Epoch 1760, training loss: 12.488330841064453 = 0.010158265940845013 + 2.0 * 6.239086151123047
Epoch 1760, val loss: 1.4539992809295654
Epoch 1770, training loss: 12.491316795349121 = 0.010011252947151661 + 2.0 * 6.240652561187744
Epoch 1770, val loss: 1.4572433233261108
Epoch 1780, training loss: 12.4901704788208 = 0.009866504929959774 + 2.0 * 6.240151882171631
Epoch 1780, val loss: 1.4605571031570435
Epoch 1790, training loss: 12.491722106933594 = 0.009725067764520645 + 2.0 * 6.2409987449646
Epoch 1790, val loss: 1.4638984203338623
Epoch 1800, training loss: 12.488283157348633 = 0.009588873013854027 + 2.0 * 6.239346981048584
Epoch 1800, val loss: 1.466971516609192
Epoch 1810, training loss: 12.488677978515625 = 0.009455435909330845 + 2.0 * 6.2396111488342285
Epoch 1810, val loss: 1.4700889587402344
Epoch 1820, training loss: 12.483611106872559 = 0.009325356222689152 + 2.0 * 6.237143039703369
Epoch 1820, val loss: 1.4734419584274292
Epoch 1830, training loss: 12.484143257141113 = 0.009198772720992565 + 2.0 * 6.237472057342529
Epoch 1830, val loss: 1.47649085521698
Epoch 1840, training loss: 12.489230155944824 = 0.009073581546545029 + 2.0 * 6.240078449249268
Epoch 1840, val loss: 1.4796531200408936
Epoch 1850, training loss: 12.484170913696289 = 0.008949784561991692 + 2.0 * 6.237610340118408
Epoch 1850, val loss: 1.4825791120529175
Epoch 1860, training loss: 12.487638473510742 = 0.008829846046864986 + 2.0 * 6.239404201507568
Epoch 1860, val loss: 1.485610842704773
Epoch 1870, training loss: 12.48072338104248 = 0.00871469359844923 + 2.0 * 6.23600435256958
Epoch 1870, val loss: 1.488669991493225
Epoch 1880, training loss: 12.479982376098633 = 0.008601072244346142 + 2.0 * 6.235690593719482
Epoch 1880, val loss: 1.4915657043457031
Epoch 1890, training loss: 12.48079776763916 = 0.008489386178553104 + 2.0 * 6.236154079437256
Epoch 1890, val loss: 1.4944829940795898
Epoch 1900, training loss: 12.484663009643555 = 0.008380873128771782 + 2.0 * 6.238141059875488
Epoch 1900, val loss: 1.4973350763320923
Epoch 1910, training loss: 12.483721733093262 = 0.00827313307672739 + 2.0 * 6.237724304199219
Epoch 1910, val loss: 1.500359296798706
Epoch 1920, training loss: 12.489361763000488 = 0.008169758133590221 + 2.0 * 6.240595817565918
Epoch 1920, val loss: 1.5029919147491455
Epoch 1930, training loss: 12.48266887664795 = 0.008065231144428253 + 2.0 * 6.237301826477051
Epoch 1930, val loss: 1.5059576034545898
Epoch 1940, training loss: 12.482156753540039 = 0.007965770550072193 + 2.0 * 6.237095355987549
Epoch 1940, val loss: 1.5087412595748901
Epoch 1950, training loss: 12.475834846496582 = 0.007867328822612762 + 2.0 * 6.233983993530273
Epoch 1950, val loss: 1.5114986896514893
Epoch 1960, training loss: 12.474319458007812 = 0.007770559284836054 + 2.0 * 6.233274459838867
Epoch 1960, val loss: 1.5142689943313599
Epoch 1970, training loss: 12.480022430419922 = 0.007675532251596451 + 2.0 * 6.236173629760742
Epoch 1970, val loss: 1.5169718265533447
Epoch 1980, training loss: 12.484338760375977 = 0.007583437487483025 + 2.0 * 6.238377571105957
Epoch 1980, val loss: 1.519452452659607
Epoch 1990, training loss: 12.47558879852295 = 0.007493508048355579 + 2.0 * 6.2340474128723145
Epoch 1990, val loss: 1.5222585201263428
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7333333333333334
0.7970479704797049
The final CL Acc:0.72963, 0.01090, The final GNN Acc:0.79635, 0.00066
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13254])
remove edge: torch.Size([2, 7870])
updated graph: torch.Size([2, 10568])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 19.14422035217285 = 1.9505784511566162 + 2.0 * 8.596820831298828
Epoch 0, val loss: 1.9460878372192383
Epoch 10, training loss: 19.133684158325195 = 1.9409500360488892 + 2.0 * 8.596366882324219
Epoch 10, val loss: 1.9361090660095215
Epoch 20, training loss: 19.113853454589844 = 1.9289110898971558 + 2.0 * 8.5924711227417
Epoch 20, val loss: 1.923542857170105
Epoch 30, training loss: 19.035505294799805 = 1.912644624710083 + 2.0 * 8.561429977416992
Epoch 30, val loss: 1.906464695930481
Epoch 40, training loss: 18.636032104492188 = 1.892009973526001 + 2.0 * 8.372011184692383
Epoch 40, val loss: 1.8856855630874634
Epoch 50, training loss: 17.630599975585938 = 1.8688838481903076 + 2.0 * 7.880857944488525
Epoch 50, val loss: 1.8633233308792114
Epoch 60, training loss: 16.718914031982422 = 1.8499506711959839 + 2.0 * 7.434481620788574
Epoch 60, val loss: 1.8460270166397095
Epoch 70, training loss: 16.046916961669922 = 1.8374673128128052 + 2.0 * 7.104724884033203
Epoch 70, val loss: 1.8341259956359863
Epoch 80, training loss: 15.703835487365723 = 1.825321912765503 + 2.0 * 6.93925666809082
Epoch 80, val loss: 1.8227050304412842
Epoch 90, training loss: 15.399994850158691 = 1.8120615482330322 + 2.0 * 6.793966770172119
Epoch 90, val loss: 1.8109737634658813
Epoch 100, training loss: 15.251497268676758 = 1.7993453741073608 + 2.0 * 6.726076126098633
Epoch 100, val loss: 1.7998270988464355
Epoch 110, training loss: 15.136664390563965 = 1.7863283157348633 + 2.0 * 6.675168037414551
Epoch 110, val loss: 1.7883135080337524
Epoch 120, training loss: 15.041604995727539 = 1.7734791040420532 + 2.0 * 6.634062767028809
Epoch 120, val loss: 1.7769795656204224
Epoch 130, training loss: 14.946316719055176 = 1.7610933780670166 + 2.0 * 6.592611789703369
Epoch 130, val loss: 1.7660356760025024
Epoch 140, training loss: 14.867496490478516 = 1.7480847835540771 + 2.0 * 6.55970573425293
Epoch 140, val loss: 1.7547532320022583
Epoch 150, training loss: 14.800665855407715 = 1.733392357826233 + 2.0 * 6.533636569976807
Epoch 150, val loss: 1.7422395944595337
Epoch 160, training loss: 14.740731239318848 = 1.7168079614639282 + 2.0 * 6.511961460113525
Epoch 160, val loss: 1.728297472000122
Epoch 170, training loss: 14.68386459350586 = 1.697960376739502 + 2.0 * 6.4929518699646
Epoch 170, val loss: 1.7123547792434692
Epoch 180, training loss: 14.637639045715332 = 1.6764503717422485 + 2.0 * 6.480594158172607
Epoch 180, val loss: 1.6942816972732544
Epoch 190, training loss: 14.579265594482422 = 1.6521775722503662 + 2.0 * 6.463543891906738
Epoch 190, val loss: 1.6737951040267944
Epoch 200, training loss: 14.526564598083496 = 1.6248096227645874 + 2.0 * 6.450877666473389
Epoch 200, val loss: 1.6508822441101074
Epoch 210, training loss: 14.483244895935059 = 1.5941412448883057 + 2.0 * 6.444551944732666
Epoch 210, val loss: 1.6254793405532837
Epoch 220, training loss: 14.423458099365234 = 1.5601507425308228 + 2.0 * 6.4316534996032715
Epoch 220, val loss: 1.5976531505584717
Epoch 230, training loss: 14.367514610290527 = 1.5228856801986694 + 2.0 * 6.422314643859863
Epoch 230, val loss: 1.5672880411148071
Epoch 240, training loss: 14.31114673614502 = 1.4820979833602905 + 2.0 * 6.414524555206299
Epoch 240, val loss: 1.5343437194824219
Epoch 250, training loss: 14.25559139251709 = 1.4382476806640625 + 2.0 * 6.408671855926514
Epoch 250, val loss: 1.4992702007293701
Epoch 260, training loss: 14.198904991149902 = 1.3924894332885742 + 2.0 * 6.403207778930664
Epoch 260, val loss: 1.4632238149642944
Epoch 270, training loss: 14.138043403625488 = 1.3450119495391846 + 2.0 * 6.396515846252441
Epoch 270, val loss: 1.4264806509017944
Epoch 280, training loss: 14.078908920288086 = 1.2962989807128906 + 2.0 * 6.391304969787598
Epoch 280, val loss: 1.3892791271209717
Epoch 290, training loss: 14.03157901763916 = 1.2468900680541992 + 2.0 * 6.3923444747924805
Epoch 290, val loss: 1.3520283699035645
Epoch 300, training loss: 13.966278076171875 = 1.197999358177185 + 2.0 * 6.384139537811279
Epoch 300, val loss: 1.3155418634414673
Epoch 310, training loss: 13.908863067626953 = 1.1500263214111328 + 2.0 * 6.37941837310791
Epoch 310, val loss: 1.2799324989318848
Epoch 320, training loss: 13.8552885055542 = 1.1028276681900024 + 2.0 * 6.376230239868164
Epoch 320, val loss: 1.245233178138733
Epoch 330, training loss: 13.803047180175781 = 1.0565929412841797 + 2.0 * 6.373227119445801
Epoch 330, val loss: 1.2115705013275146
Epoch 340, training loss: 13.752907752990723 = 1.0119192600250244 + 2.0 * 6.370494365692139
Epoch 340, val loss: 1.1789048910140991
Epoch 350, training loss: 13.699654579162598 = 0.9685944318771362 + 2.0 * 6.365530014038086
Epoch 350, val loss: 1.147496223449707
Epoch 360, training loss: 13.658308982849121 = 0.9266679286956787 + 2.0 * 6.365820407867432
Epoch 360, val loss: 1.1173086166381836
Epoch 370, training loss: 13.606426239013672 = 0.8864733576774597 + 2.0 * 6.359976291656494
Epoch 370, val loss: 1.0887458324432373
Epoch 380, training loss: 13.561287879943848 = 0.8482478260993958 + 2.0 * 6.356520175933838
Epoch 380, val loss: 1.0618503093719482
Epoch 390, training loss: 13.519607543945312 = 0.8116536140441895 + 2.0 * 6.353977203369141
Epoch 390, val loss: 1.0366793870925903
Epoch 400, training loss: 13.484179496765137 = 0.7769092321395874 + 2.0 * 6.353635311126709
Epoch 400, val loss: 1.0132348537445068
Epoch 410, training loss: 13.442419052124023 = 0.744096577167511 + 2.0 * 6.349161148071289
Epoch 410, val loss: 0.9918760061264038
Epoch 420, training loss: 13.405466079711914 = 0.7130445241928101 + 2.0 * 6.346210956573486
Epoch 420, val loss: 0.9722430109977722
Epoch 430, training loss: 13.37291431427002 = 0.6833510994911194 + 2.0 * 6.344781398773193
Epoch 430, val loss: 0.9541995525360107
Epoch 440, training loss: 13.345746994018555 = 0.6551951169967651 + 2.0 * 6.34527587890625
Epoch 440, val loss: 0.9378083348274231
Epoch 450, training loss: 13.305962562561035 = 0.6284725666046143 + 2.0 * 6.3387451171875
Epoch 450, val loss: 0.922970712184906
Epoch 460, training loss: 13.276118278503418 = 0.6028833985328674 + 2.0 * 6.336617469787598
Epoch 460, val loss: 0.9095438718795776
Epoch 470, training loss: 13.250484466552734 = 0.578535258769989 + 2.0 * 6.33597469329834
Epoch 470, val loss: 0.8973579406738281
Epoch 480, training loss: 13.218035697937012 = 0.5553554892539978 + 2.0 * 6.331340312957764
Epoch 480, val loss: 0.8865880966186523
Epoch 490, training loss: 13.191333770751953 = 0.5332659482955933 + 2.0 * 6.329033851623535
Epoch 490, val loss: 0.8770044445991516
Epoch 500, training loss: 13.166576385498047 = 0.5121580958366394 + 2.0 * 6.327208995819092
Epoch 500, val loss: 0.8685100674629211
Epoch 510, training loss: 13.142072677612305 = 0.49209827184677124 + 2.0 * 6.324987411499023
Epoch 510, val loss: 0.8611178398132324
Epoch 520, training loss: 13.120476722717285 = 0.4731387794017792 + 2.0 * 6.323668956756592
Epoch 520, val loss: 0.8548492789268494
Epoch 530, training loss: 13.097151756286621 = 0.4551706910133362 + 2.0 * 6.320990562438965
Epoch 530, val loss: 0.8496188521385193
Epoch 540, training loss: 13.076388359069824 = 0.4381270110607147 + 2.0 * 6.319130897521973
Epoch 540, val loss: 0.8452983498573303
Epoch 550, training loss: 13.056522369384766 = 0.42184963822364807 + 2.0 * 6.317336559295654
Epoch 550, val loss: 0.8417829275131226
Epoch 560, training loss: 13.040872573852539 = 0.40635740756988525 + 2.0 * 6.317257404327393
Epoch 560, val loss: 0.8390072584152222
Epoch 570, training loss: 13.022461891174316 = 0.39160752296447754 + 2.0 * 6.315427303314209
Epoch 570, val loss: 0.836933970451355
Epoch 580, training loss: 13.00090503692627 = 0.3775734305381775 + 2.0 * 6.311666011810303
Epoch 580, val loss: 0.8354763388633728
Epoch 590, training loss: 12.990345001220703 = 0.3640815317630768 + 2.0 * 6.313131809234619
Epoch 590, val loss: 0.8346509337425232
Epoch 600, training loss: 12.97368049621582 = 0.35113757848739624 + 2.0 * 6.311271667480469
Epoch 600, val loss: 0.8344163298606873
Epoch 610, training loss: 12.953269004821777 = 0.3386761248111725 + 2.0 * 6.307296276092529
Epoch 610, val loss: 0.8346018195152283
Epoch 620, training loss: 12.937116622924805 = 0.32663437724113464 + 2.0 * 6.305241107940674
Epoch 620, val loss: 0.835341215133667
Epoch 630, training loss: 12.922667503356934 = 0.3149169683456421 + 2.0 * 6.30387544631958
Epoch 630, val loss: 0.836482048034668
Epoch 640, training loss: 12.920538902282715 = 0.30350232124328613 + 2.0 * 6.308518409729004
Epoch 640, val loss: 0.8380917310714722
Epoch 650, training loss: 12.894153594970703 = 0.2923769950866699 + 2.0 * 6.3008880615234375
Epoch 650, val loss: 0.8400197625160217
Epoch 660, training loss: 12.879292488098145 = 0.2814839482307434 + 2.0 * 6.2989044189453125
Epoch 660, val loss: 0.8423463106155396
Epoch 670, training loss: 12.873303413391113 = 0.2707803547382355 + 2.0 * 6.3012614250183105
Epoch 670, val loss: 0.8449968099594116
Epoch 680, training loss: 12.861170768737793 = 0.26022642850875854 + 2.0 * 6.300472259521484
Epoch 680, val loss: 0.8480513095855713
Epoch 690, training loss: 12.838508605957031 = 0.24989119172096252 + 2.0 * 6.294308662414551
Epoch 690, val loss: 0.8513951897621155
Epoch 700, training loss: 12.828058242797852 = 0.23969323933124542 + 2.0 * 6.294182300567627
Epoch 700, val loss: 0.8550845384597778
Epoch 710, training loss: 12.821578979492188 = 0.2296692579984665 + 2.0 * 6.295954704284668
Epoch 710, val loss: 0.8591364026069641
Epoch 720, training loss: 12.810689926147461 = 0.21987496316432953 + 2.0 * 6.295407295227051
Epoch 720, val loss: 0.8634188175201416
Epoch 730, training loss: 12.791646003723145 = 0.2103038728237152 + 2.0 * 6.290670871734619
Epoch 730, val loss: 0.8680664896965027
Epoch 740, training loss: 12.778608322143555 = 0.2010086178779602 + 2.0 * 6.28879976272583
Epoch 740, val loss: 0.872967004776001
Epoch 750, training loss: 12.767645835876465 = 0.1919582337141037 + 2.0 * 6.287843704223633
Epoch 750, val loss: 0.8782426714897156
Epoch 760, training loss: 12.766948699951172 = 0.18319180607795715 + 2.0 * 6.2918782234191895
Epoch 760, val loss: 0.8837537169456482
Epoch 770, training loss: 12.751043319702148 = 0.17480289936065674 + 2.0 * 6.288120269775391
Epoch 770, val loss: 0.8895597457885742
Epoch 780, training loss: 12.734515190124512 = 0.16676265001296997 + 2.0 * 6.283876419067383
Epoch 780, val loss: 0.8956565856933594
Epoch 790, training loss: 12.725332260131836 = 0.15904459357261658 + 2.0 * 6.283143997192383
Epoch 790, val loss: 0.9020060300827026
Epoch 800, training loss: 12.725605964660645 = 0.15165412425994873 + 2.0 * 6.286975860595703
Epoch 800, val loss: 0.9085342288017273
Epoch 810, training loss: 12.712432861328125 = 0.1445576697587967 + 2.0 * 6.283937454223633
Epoch 810, val loss: 0.9152906537055969
Epoch 820, training loss: 12.706991195678711 = 0.13784393668174744 + 2.0 * 6.284573554992676
Epoch 820, val loss: 0.922051191329956
Epoch 830, training loss: 12.692127227783203 = 0.1314365267753601 + 2.0 * 6.280345439910889
Epoch 830, val loss: 0.9292090535163879
Epoch 840, training loss: 12.683838844299316 = 0.1253477782011032 + 2.0 * 6.279245376586914
Epoch 840, val loss: 0.9363923072814941
Epoch 850, training loss: 12.675813674926758 = 0.11955450475215912 + 2.0 * 6.278129577636719
Epoch 850, val loss: 0.9436642527580261
Epoch 860, training loss: 12.676952362060547 = 0.11404396593570709 + 2.0 * 6.281454086303711
Epoch 860, val loss: 0.951075553894043
Epoch 870, training loss: 12.663476943969727 = 0.10881202667951584 + 2.0 * 6.277332305908203
Epoch 870, val loss: 0.9585496783256531
Epoch 880, training loss: 12.661482810974121 = 0.10386201739311218 + 2.0 * 6.278810501098633
Epoch 880, val loss: 0.9660358428955078
Epoch 890, training loss: 12.65235424041748 = 0.09918403625488281 + 2.0 * 6.276585102081299
Epoch 890, val loss: 0.973727285861969
Epoch 900, training loss: 12.645026206970215 = 0.09475124627351761 + 2.0 * 6.275137424468994
Epoch 900, val loss: 0.9813527464866638
Epoch 910, training loss: 12.635543823242188 = 0.09056568890810013 + 2.0 * 6.272489070892334
Epoch 910, val loss: 0.9891670346260071
Epoch 920, training loss: 12.636517524719238 = 0.08660353720188141 + 2.0 * 6.274957180023193
Epoch 920, val loss: 0.996927797794342
Epoch 930, training loss: 12.627748489379883 = 0.08286204189062119 + 2.0 * 6.2724432945251465
Epoch 930, val loss: 1.0045493841171265
Epoch 940, training loss: 12.62088394165039 = 0.07931403070688248 + 2.0 * 6.270784854888916
Epoch 940, val loss: 1.0123648643493652
Epoch 950, training loss: 12.61783218383789 = 0.07596570253372192 + 2.0 * 6.270933151245117
Epoch 950, val loss: 1.0200048685073853
Epoch 960, training loss: 12.618124008178711 = 0.07280171662569046 + 2.0 * 6.272661209106445
Epoch 960, val loss: 1.0277411937713623
Epoch 970, training loss: 12.607847213745117 = 0.06981080770492554 + 2.0 * 6.269018173217773
Epoch 970, val loss: 1.0353776216506958
Epoch 980, training loss: 12.602249145507812 = 0.06697992980480194 + 2.0 * 6.267634391784668
Epoch 980, val loss: 1.0429441928863525
Epoch 990, training loss: 12.600732803344727 = 0.06430027633905411 + 2.0 * 6.268216133117676
Epoch 990, val loss: 1.0505141019821167
Epoch 1000, training loss: 12.59713363647461 = 0.06177229434251785 + 2.0 * 6.267680644989014
Epoch 1000, val loss: 1.0579246282577515
Epoch 1010, training loss: 12.590641021728516 = 0.0593755729496479 + 2.0 * 6.265632629394531
Epoch 1010, val loss: 1.065425992012024
Epoch 1020, training loss: 12.587560653686523 = 0.05710165947675705 + 2.0 * 6.26522970199585
Epoch 1020, val loss: 1.0728404521942139
Epoch 1030, training loss: 12.589513778686523 = 0.05494922026991844 + 2.0 * 6.267282485961914
Epoch 1030, val loss: 1.0801085233688354
Epoch 1040, training loss: 12.586204528808594 = 0.05290934816002846 + 2.0 * 6.266647815704346
Epoch 1040, val loss: 1.0872629880905151
Epoch 1050, training loss: 12.576332092285156 = 0.050973787903785706 + 2.0 * 6.262679100036621
Epoch 1050, val loss: 1.0944921970367432
Epoch 1060, training loss: 12.57370662689209 = 0.04914272576570511 + 2.0 * 6.262281894683838
Epoch 1060, val loss: 1.1015207767486572
Epoch 1070, training loss: 12.580062866210938 = 0.0473971962928772 + 2.0 * 6.266332626342773
Epoch 1070, val loss: 1.1085292100906372
Epoch 1080, training loss: 12.567419052124023 = 0.04575047269463539 + 2.0 * 6.260834217071533
Epoch 1080, val loss: 1.1154208183288574
Epoch 1090, training loss: 12.562983512878418 = 0.04417424276471138 + 2.0 * 6.25940465927124
Epoch 1090, val loss: 1.1222318410873413
Epoch 1100, training loss: 12.568443298339844 = 0.04267820715904236 + 2.0 * 6.262882709503174
Epoch 1100, val loss: 1.128995656967163
Epoch 1110, training loss: 12.561187744140625 = 0.04125305265188217 + 2.0 * 6.25996732711792
Epoch 1110, val loss: 1.1356480121612549
Epoch 1120, training loss: 12.558048248291016 = 0.03989618271589279 + 2.0 * 6.259076118469238
Epoch 1120, val loss: 1.142158031463623
Epoch 1130, training loss: 12.553444862365723 = 0.038607507944107056 + 2.0 * 6.257418632507324
Epoch 1130, val loss: 1.148646354675293
Epoch 1140, training loss: 12.55616283416748 = 0.03737344220280647 + 2.0 * 6.259394645690918
Epoch 1140, val loss: 1.155164122581482
Epoch 1150, training loss: 12.548757553100586 = 0.03619339317083359 + 2.0 * 6.256281852722168
Epoch 1150, val loss: 1.1614155769348145
Epoch 1160, training loss: 12.553705215454102 = 0.03506871312856674 + 2.0 * 6.2593183517456055
Epoch 1160, val loss: 1.1677318811416626
Epoch 1170, training loss: 12.54908275604248 = 0.033998891711235046 + 2.0 * 6.257542133331299
Epoch 1170, val loss: 1.1736319065093994
Epoch 1180, training loss: 12.542181968688965 = 0.03298071771860123 + 2.0 * 6.254600524902344
Epoch 1180, val loss: 1.1797091960906982
Epoch 1190, training loss: 12.540818214416504 = 0.03200236335396767 + 2.0 * 6.25440788269043
Epoch 1190, val loss: 1.1857458353042603
Epoch 1200, training loss: 12.54995346069336 = 0.031065626069903374 + 2.0 * 6.259443759918213
Epoch 1200, val loss: 1.1915740966796875
Epoch 1210, training loss: 12.539166450500488 = 0.030165258795022964 + 2.0 * 6.254500389099121
Epoch 1210, val loss: 1.1973564624786377
Epoch 1220, training loss: 12.536238670349121 = 0.02930951677262783 + 2.0 * 6.253464698791504
Epoch 1220, val loss: 1.202982783317566
Epoch 1230, training loss: 12.53709602355957 = 0.028486456722021103 + 2.0 * 6.254304885864258
Epoch 1230, val loss: 1.2086715698242188
Epoch 1240, training loss: 12.53374195098877 = 0.02769806794822216 + 2.0 * 6.253021717071533
Epoch 1240, val loss: 1.2141680717468262
Epoch 1250, training loss: 12.531944274902344 = 0.02694542333483696 + 2.0 * 6.252499580383301
Epoch 1250, val loss: 1.2195837497711182
Epoch 1260, training loss: 12.537125587463379 = 0.026216577738523483 + 2.0 * 6.2554545402526855
Epoch 1260, val loss: 1.225073218345642
Epoch 1270, training loss: 12.529241561889648 = 0.025523055344820023 + 2.0 * 6.251859188079834
Epoch 1270, val loss: 1.2303165197372437
Epoch 1280, training loss: 12.531414031982422 = 0.02485284022986889 + 2.0 * 6.2532806396484375
Epoch 1280, val loss: 1.2356537580490112
Epoch 1290, training loss: 12.524551391601562 = 0.024211406707763672 + 2.0 * 6.2501702308654785
Epoch 1290, val loss: 1.2406339645385742
Epoch 1300, training loss: 12.522758483886719 = 0.02359289675951004 + 2.0 * 6.249582767486572
Epoch 1300, val loss: 1.2458332777023315
Epoch 1310, training loss: 12.52902889251709 = 0.023000186309218407 + 2.0 * 6.25301456451416
Epoch 1310, val loss: 1.2508779764175415
Epoch 1320, training loss: 12.523943901062012 = 0.022423293441534042 + 2.0 * 6.250760078430176
Epoch 1320, val loss: 1.2558425664901733
Epoch 1330, training loss: 12.518728256225586 = 0.0218735933303833 + 2.0 * 6.248427391052246
Epoch 1330, val loss: 1.2605795860290527
Epoch 1340, training loss: 12.5233793258667 = 0.021343544125556946 + 2.0 * 6.251018047332764
Epoch 1340, val loss: 1.2653728723526
Epoch 1350, training loss: 12.515847206115723 = 0.02083352394402027 + 2.0 * 6.247506618499756
Epoch 1350, val loss: 1.2701616287231445
Epoch 1360, training loss: 12.51352310180664 = 0.020338216796517372 + 2.0 * 6.2465925216674805
Epoch 1360, val loss: 1.274825930595398
Epoch 1370, training loss: 12.512430191040039 = 0.01986534893512726 + 2.0 * 6.246282577514648
Epoch 1370, val loss: 1.2793903350830078
Epoch 1380, training loss: 12.513764381408691 = 0.019404558464884758 + 2.0 * 6.247179985046387
Epoch 1380, val loss: 1.2839791774749756
Epoch 1390, training loss: 12.520797729492188 = 0.018959680572152138 + 2.0 * 6.250918865203857
Epoch 1390, val loss: 1.2884507179260254
Epoch 1400, training loss: 12.514627456665039 = 0.018533267080783844 + 2.0 * 6.248046875
Epoch 1400, val loss: 1.2929002046585083
Epoch 1410, training loss: 12.508362770080566 = 0.018122199922800064 + 2.0 * 6.245120048522949
Epoch 1410, val loss: 1.2970983982086182
Epoch 1420, training loss: 12.508337020874023 = 0.017726004123687744 + 2.0 * 6.24530553817749
Epoch 1420, val loss: 1.3014391660690308
Epoch 1430, training loss: 12.513084411621094 = 0.017341235652565956 + 2.0 * 6.247871398925781
Epoch 1430, val loss: 1.3056731224060059
Epoch 1440, training loss: 12.504888534545898 = 0.01696769893169403 + 2.0 * 6.243960380554199
Epoch 1440, val loss: 1.30986487865448
Epoch 1450, training loss: 12.503861427307129 = 0.016609426587820053 + 2.0 * 6.243626117706299
Epoch 1450, val loss: 1.3140466213226318
Epoch 1460, training loss: 12.50581169128418 = 0.016261041164398193 + 2.0 * 6.244775295257568
Epoch 1460, val loss: 1.3181055784225464
Epoch 1470, training loss: 12.507506370544434 = 0.015922535210847855 + 2.0 * 6.245791912078857
Epoch 1470, val loss: 1.3220683336257935
Epoch 1480, training loss: 12.503228187561035 = 0.015596689656376839 + 2.0 * 6.243815898895264
Epoch 1480, val loss: 1.3261624574661255
Epoch 1490, training loss: 12.502985000610352 = 0.01528066024184227 + 2.0 * 6.243852138519287
Epoch 1490, val loss: 1.3300411701202393
Epoch 1500, training loss: 12.499187469482422 = 0.014975002966821194 + 2.0 * 6.2421064376831055
Epoch 1500, val loss: 1.333922266960144
Epoch 1510, training loss: 12.496828079223633 = 0.014680791646242142 + 2.0 * 6.2410736083984375
Epoch 1510, val loss: 1.3377467393875122
Epoch 1520, training loss: 12.501158714294434 = 0.014393778517842293 + 2.0 * 6.243382453918457
Epoch 1520, val loss: 1.3414887189865112
Epoch 1530, training loss: 12.498583793640137 = 0.014113085344433784 + 2.0 * 6.24223518371582
Epoch 1530, val loss: 1.3453716039657593
Epoch 1540, training loss: 12.496687889099121 = 0.013843037188053131 + 2.0 * 6.241422653198242
Epoch 1540, val loss: 1.348830223083496
Epoch 1550, training loss: 12.500195503234863 = 0.013582580722868443 + 2.0 * 6.243306636810303
Epoch 1550, val loss: 1.3525041341781616
Epoch 1560, training loss: 12.49596118927002 = 0.0133281284943223 + 2.0 * 6.241316318511963
Epoch 1560, val loss: 1.356105089187622
Epoch 1570, training loss: 12.491663932800293 = 0.01308266818523407 + 2.0 * 6.239290714263916
Epoch 1570, val loss: 1.3596199750900269
Epoch 1580, training loss: 12.490518569946289 = 0.012844298966228962 + 2.0 * 6.238837242126465
Epoch 1580, val loss: 1.363071322441101
Epoch 1590, training loss: 12.492939949035645 = 0.012612578459084034 + 2.0 * 6.240163803100586
Epoch 1590, val loss: 1.3665465116500854
Epoch 1600, training loss: 12.49509048461914 = 0.01238622423261404 + 2.0 * 6.241352081298828
Epoch 1600, val loss: 1.3699672222137451
Epoch 1610, training loss: 12.49079418182373 = 0.012163413688540459 + 2.0 * 6.239315509796143
Epoch 1610, val loss: 1.3733177185058594
Epoch 1620, training loss: 12.491536140441895 = 0.011951403692364693 + 2.0 * 6.239792346954346
Epoch 1620, val loss: 1.3765923976898193
Epoch 1630, training loss: 12.486804962158203 = 0.011744328774511814 + 2.0 * 6.23753023147583
Epoch 1630, val loss: 1.379858374595642
Epoch 1640, training loss: 12.48642349243164 = 0.011543291620910168 + 2.0 * 6.23744010925293
Epoch 1640, val loss: 1.3831558227539062
Epoch 1650, training loss: 12.49215030670166 = 0.011347043327987194 + 2.0 * 6.240401744842529
Epoch 1650, val loss: 1.3863221406936646
Epoch 1660, training loss: 12.489789009094238 = 0.011155309155583382 + 2.0 * 6.239316940307617
Epoch 1660, val loss: 1.3894970417022705
Epoch 1670, training loss: 12.487783432006836 = 0.010969692841172218 + 2.0 * 6.238406658172607
Epoch 1670, val loss: 1.3926129341125488
Epoch 1680, training loss: 12.492079734802246 = 0.010789250023663044 + 2.0 * 6.240645408630371
Epoch 1680, val loss: 1.3955776691436768
Epoch 1690, training loss: 12.483956336975098 = 0.01061228010803461 + 2.0 * 6.2366719245910645
Epoch 1690, val loss: 1.3985252380371094
Epoch 1700, training loss: 12.481053352355957 = 0.010442422702908516 + 2.0 * 6.235305309295654
Epoch 1700, val loss: 1.4015306234359741
Epoch 1710, training loss: 12.479040145874023 = 0.010275355540215969 + 2.0 * 6.234382629394531
Epoch 1710, val loss: 1.404473900794983
Epoch 1720, training loss: 12.479063034057617 = 0.010112115181982517 + 2.0 * 6.234475612640381
Epoch 1720, val loss: 1.4074245691299438
Epoch 1730, training loss: 12.505573272705078 = 0.009952128864824772 + 2.0 * 6.247810363769531
Epoch 1730, val loss: 1.4103927612304688
Epoch 1740, training loss: 12.479809761047363 = 0.00979839637875557 + 2.0 * 6.235005855560303
Epoch 1740, val loss: 1.4130381345748901
Epoch 1750, training loss: 12.476602554321289 = 0.009647810831665993 + 2.0 * 6.233477592468262
Epoch 1750, val loss: 1.4157782793045044
Epoch 1760, training loss: 12.476339340209961 = 0.009500352665781975 + 2.0 * 6.233419418334961
Epoch 1760, val loss: 1.4186846017837524
Epoch 1770, training loss: 12.480752944946289 = 0.009357224218547344 + 2.0 * 6.2356977462768555
Epoch 1770, val loss: 1.4213484525680542
Epoch 1780, training loss: 12.477642059326172 = 0.009215670637786388 + 2.0 * 6.234213352203369
Epoch 1780, val loss: 1.4241507053375244
Epoch 1790, training loss: 12.477190971374512 = 0.00907909031957388 + 2.0 * 6.234055995941162
Epoch 1790, val loss: 1.4266561269760132
Epoch 1800, training loss: 12.477033615112305 = 0.008946973830461502 + 2.0 * 6.234043121337891
Epoch 1800, val loss: 1.4294027090072632
Epoch 1810, training loss: 12.47488784790039 = 0.008816869929432869 + 2.0 * 6.233035564422607
Epoch 1810, val loss: 1.4319381713867188
Epoch 1820, training loss: 12.473060607910156 = 0.008689835667610168 + 2.0 * 6.232185363769531
Epoch 1820, val loss: 1.434552788734436
Epoch 1830, training loss: 12.48190689086914 = 0.008566469885408878 + 2.0 * 6.236670017242432
Epoch 1830, val loss: 1.437066674232483
Epoch 1840, training loss: 12.476530075073242 = 0.00844300165772438 + 2.0 * 6.234043598175049
Epoch 1840, val loss: 1.4396483898162842
Epoch 1850, training loss: 12.47088623046875 = 0.00832459144294262 + 2.0 * 6.23128080368042
Epoch 1850, val loss: 1.442016839981079
Epoch 1860, training loss: 12.47376823425293 = 0.008209398947656155 + 2.0 * 6.232779502868652
Epoch 1860, val loss: 1.4445724487304688
Epoch 1870, training loss: 12.474677085876465 = 0.008095946162939072 + 2.0 * 6.233290672302246
Epoch 1870, val loss: 1.4469884634017944
Epoch 1880, training loss: 12.475996971130371 = 0.007984047755599022 + 2.0 * 6.234006404876709
Epoch 1880, val loss: 1.4494504928588867
Epoch 1890, training loss: 12.468398094177246 = 0.00787592027336359 + 2.0 * 6.230260848999023
Epoch 1890, val loss: 1.4516735076904297
Epoch 1900, training loss: 12.468329429626465 = 0.00777135556563735 + 2.0 * 6.230278968811035
Epoch 1900, val loss: 1.4541040658950806
Epoch 1910, training loss: 12.468134880065918 = 0.007667507044970989 + 2.0 * 6.230233669281006
Epoch 1910, val loss: 1.456442952156067
Epoch 1920, training loss: 12.480039596557617 = 0.007565663196146488 + 2.0 * 6.236237049102783
Epoch 1920, val loss: 1.4587655067443848
Epoch 1930, training loss: 12.46893310546875 = 0.007465421687811613 + 2.0 * 6.230733871459961
Epoch 1930, val loss: 1.4610507488250732
Epoch 1940, training loss: 12.466185569763184 = 0.0073694405145943165 + 2.0 * 6.229408264160156
Epoch 1940, val loss: 1.4631919860839844
Epoch 1950, training loss: 12.472063064575195 = 0.007274590898305178 + 2.0 * 6.232394218444824
Epoch 1950, val loss: 1.4655095338821411
Epoch 1960, training loss: 12.464936256408691 = 0.007180970162153244 + 2.0 * 6.228877544403076
Epoch 1960, val loss: 1.4676907062530518
Epoch 1970, training loss: 12.464576721191406 = 0.007090041413903236 + 2.0 * 6.228743553161621
Epoch 1970, val loss: 1.46976637840271
Epoch 1980, training loss: 12.466231346130371 = 0.007001086138188839 + 2.0 * 6.229615211486816
Epoch 1980, val loss: 1.4720139503479004
Epoch 1990, training loss: 12.470309257507324 = 0.006913813762366772 + 2.0 * 6.2316975593566895
Epoch 1990, val loss: 1.4741806983947754
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7703703703703704
0.8408012651555088
=== training gcn model ===
Epoch 0, training loss: 19.163267135620117 = 1.969533920288086 + 2.0 * 8.596866607666016
Epoch 0, val loss: 1.964802861213684
Epoch 10, training loss: 19.151792526245117 = 1.9584505558013916 + 2.0 * 8.596671104431152
Epoch 10, val loss: 1.9532461166381836
Epoch 20, training loss: 19.135093688964844 = 1.945014476776123 + 2.0 * 8.595039367675781
Epoch 20, val loss: 1.9388654232025146
Epoch 30, training loss: 19.08908462524414 = 1.926737904548645 + 2.0 * 8.581172943115234
Epoch 30, val loss: 1.9190759658813477
Epoch 40, training loss: 18.892900466918945 = 1.9024498462677002 + 2.0 * 8.495224952697754
Epoch 40, val loss: 1.8935153484344482
Epoch 50, training loss: 18.05487632751465 = 1.8744454383850098 + 2.0 * 8.090215682983398
Epoch 50, val loss: 1.8649356365203857
Epoch 60, training loss: 17.2247257232666 = 1.8491281270980835 + 2.0 * 7.687798976898193
Epoch 60, val loss: 1.8407338857650757
Epoch 70, training loss: 16.358932495117188 = 1.8323676586151123 + 2.0 * 7.263282775878906
Epoch 70, val loss: 1.8241320848464966
Epoch 80, training loss: 15.8676118850708 = 1.8177891969680786 + 2.0 * 7.024911403656006
Epoch 80, val loss: 1.810199499130249
Epoch 90, training loss: 15.571073532104492 = 1.8023056983947754 + 2.0 * 6.8843841552734375
Epoch 90, val loss: 1.7956684827804565
Epoch 100, training loss: 15.379528999328613 = 1.785953164100647 + 2.0 * 6.796787738800049
Epoch 100, val loss: 1.7803765535354614
Epoch 110, training loss: 15.232353210449219 = 1.769095778465271 + 2.0 * 6.731628894805908
Epoch 110, val loss: 1.7642229795455933
Epoch 120, training loss: 15.106038093566895 = 1.7535260915756226 + 2.0 * 6.67625617980957
Epoch 120, val loss: 1.749893069267273
Epoch 130, training loss: 14.994721412658691 = 1.7387932538986206 + 2.0 * 6.627964019775391
Epoch 130, val loss: 1.736511468887329
Epoch 140, training loss: 14.903522491455078 = 1.7229728698730469 + 2.0 * 6.590274810791016
Epoch 140, val loss: 1.7221218347549438
Epoch 150, training loss: 14.828343391418457 = 1.7052348852157593 + 2.0 * 6.561554431915283
Epoch 150, val loss: 1.7059404850006104
Epoch 160, training loss: 14.762003898620605 = 1.6854817867279053 + 2.0 * 6.5382609367370605
Epoch 160, val loss: 1.6881165504455566
Epoch 170, training loss: 14.703471183776855 = 1.6632623672485352 + 2.0 * 6.52010440826416
Epoch 170, val loss: 1.6682634353637695
Epoch 180, training loss: 14.649444580078125 = 1.6380717754364014 + 2.0 * 6.505686283111572
Epoch 180, val loss: 1.6457884311676025
Epoch 190, training loss: 14.59635066986084 = 1.6101772785186768 + 2.0 * 6.493086814880371
Epoch 190, val loss: 1.6209691762924194
Epoch 200, training loss: 14.54164981842041 = 1.579758644104004 + 2.0 * 6.480945587158203
Epoch 200, val loss: 1.5941026210784912
Epoch 210, training loss: 14.485267639160156 = 1.5464752912521362 + 2.0 * 6.469396114349365
Epoch 210, val loss: 1.5647403001785278
Epoch 220, training loss: 14.427582740783691 = 1.5101337432861328 + 2.0 * 6.458724498748779
Epoch 220, val loss: 1.5330086946487427
Epoch 230, training loss: 14.374059677124023 = 1.4710029363632202 + 2.0 * 6.451528549194336
Epoch 230, val loss: 1.499215006828308
Epoch 240, training loss: 14.311429977416992 = 1.4298357963562012 + 2.0 * 6.440797328948975
Epoch 240, val loss: 1.4641392230987549
Epoch 250, training loss: 14.251799583435059 = 1.3868744373321533 + 2.0 * 6.432462692260742
Epoch 250, val loss: 1.4280287027359009
Epoch 260, training loss: 14.200214385986328 = 1.3424869775772095 + 2.0 * 6.428863525390625
Epoch 260, val loss: 1.391194224357605
Epoch 270, training loss: 14.135671615600586 = 1.297650933265686 + 2.0 * 6.419010162353516
Epoch 270, val loss: 1.354657769203186
Epoch 280, training loss: 14.077573776245117 = 1.252860188484192 + 2.0 * 6.412356853485107
Epoch 280, val loss: 1.3186780214309692
Epoch 290, training loss: 14.020959854125977 = 1.2083185911178589 + 2.0 * 6.406320571899414
Epoch 290, val loss: 1.2833653688430786
Epoch 300, training loss: 13.969290733337402 = 1.1644543409347534 + 2.0 * 6.40241813659668
Epoch 300, val loss: 1.2490448951721191
Epoch 310, training loss: 13.917595863342285 = 1.122096061706543 + 2.0 * 6.397749900817871
Epoch 310, val loss: 1.216257095336914
Epoch 320, training loss: 13.86885929107666 = 1.0815130472183228 + 2.0 * 6.393672943115234
Epoch 320, val loss: 1.1851954460144043
Epoch 330, training loss: 13.819855690002441 = 1.0423568487167358 + 2.0 * 6.388749599456787
Epoch 330, val loss: 1.1553993225097656
Epoch 340, training loss: 13.77462100982666 = 1.0046873092651367 + 2.0 * 6.384966850280762
Epoch 340, val loss: 1.126967430114746
Epoch 350, training loss: 13.729215621948242 = 0.9685384631156921 + 2.0 * 6.380338668823242
Epoch 350, val loss: 1.0997650623321533
Epoch 360, training loss: 13.68593978881836 = 0.9335305690765381 + 2.0 * 6.376204490661621
Epoch 360, val loss: 1.0736020803451538
Epoch 370, training loss: 13.652552604675293 = 0.8996303081512451 + 2.0 * 6.376461029052734
Epoch 370, val loss: 1.0483624935150146
Epoch 380, training loss: 13.607311248779297 = 0.8671355247497559 + 2.0 * 6.37008810043335
Epoch 380, val loss: 1.0242695808410645
Epoch 390, training loss: 13.5701904296875 = 0.835705578327179 + 2.0 * 6.367242336273193
Epoch 390, val loss: 1.001146674156189
Epoch 400, training loss: 13.534246444702148 = 0.8051657676696777 + 2.0 * 6.3645405769348145
Epoch 400, val loss: 0.9788928031921387
Epoch 410, training loss: 13.497904777526855 = 0.7755469083786011 + 2.0 * 6.361178874969482
Epoch 410, val loss: 0.9575247168540955
Epoch 420, training loss: 13.464388847351074 = 0.7468599081039429 + 2.0 * 6.3587646484375
Epoch 420, val loss: 0.9372120499610901
Epoch 430, training loss: 13.436995506286621 = 0.7191488742828369 + 2.0 * 6.358923435211182
Epoch 430, val loss: 0.9179345965385437
Epoch 440, training loss: 13.398660659790039 = 0.6924611926078796 + 2.0 * 6.353099822998047
Epoch 440, val loss: 0.8998433351516724
Epoch 450, training loss: 13.366774559020996 = 0.6666180491447449 + 2.0 * 6.350078105926514
Epoch 450, val loss: 0.8828860521316528
Epoch 460, training loss: 13.33680534362793 = 0.6414787769317627 + 2.0 * 6.347663402557373
Epoch 460, val loss: 0.8669973015785217
Epoch 470, training loss: 13.320365905761719 = 0.6170247197151184 + 2.0 * 6.351670742034912
Epoch 470, val loss: 0.8521693348884583
Epoch 480, training loss: 13.284809112548828 = 0.5934203267097473 + 2.0 * 6.345694541931152
Epoch 480, val loss: 0.8384040594100952
Epoch 490, training loss: 13.253448486328125 = 0.5705638527870178 + 2.0 * 6.341442108154297
Epoch 490, val loss: 0.8257494568824768
Epoch 500, training loss: 13.224677085876465 = 0.5481661558151245 + 2.0 * 6.338255405426025
Epoch 500, val loss: 0.8139532208442688
Epoch 510, training loss: 13.213667869567871 = 0.5261587500572205 + 2.0 * 6.343754768371582
Epoch 510, val loss: 0.8030192852020264
Epoch 520, training loss: 13.179153442382812 = 0.5047545433044434 + 2.0 * 6.337199687957764
Epoch 520, val loss: 0.7927458882331848
Epoch 530, training loss: 13.152393341064453 = 0.48372408747673035 + 2.0 * 6.334334850311279
Epoch 530, val loss: 0.7832783460617065
Epoch 540, training loss: 13.123644828796387 = 0.463068425655365 + 2.0 * 6.330288410186768
Epoch 540, val loss: 0.774447500705719
Epoch 550, training loss: 13.10126781463623 = 0.4428165555000305 + 2.0 * 6.329225540161133
Epoch 550, val loss: 0.7661988139152527
Epoch 560, training loss: 13.07776927947998 = 0.4229148030281067 + 2.0 * 6.327427387237549
Epoch 560, val loss: 0.7583995461463928
Epoch 570, training loss: 13.057761192321777 = 0.40345579385757446 + 2.0 * 6.327152729034424
Epoch 570, val loss: 0.7511515617370605
Epoch 580, training loss: 13.03231430053711 = 0.38440269231796265 + 2.0 * 6.32395601272583
Epoch 580, val loss: 0.7443810701370239
Epoch 590, training loss: 13.016043663024902 = 0.3658069968223572 + 2.0 * 6.325118541717529
Epoch 590, val loss: 0.7380849719047546
Epoch 600, training loss: 12.997673988342285 = 0.3476988673210144 + 2.0 * 6.324987411499023
Epoch 600, val loss: 0.7321954965591431
Epoch 610, training loss: 12.97023868560791 = 0.33023518323898315 + 2.0 * 6.320001602172852
Epoch 610, val loss: 0.7267640829086304
Epoch 620, training loss: 12.94997787475586 = 0.31330692768096924 + 2.0 * 6.31833553314209
Epoch 620, val loss: 0.7218297123908997
Epoch 630, training loss: 12.928526878356934 = 0.2969949543476105 + 2.0 * 6.315765857696533
Epoch 630, val loss: 0.7173709869384766
Epoch 640, training loss: 12.924116134643555 = 0.28125882148742676 + 2.0 * 6.3214287757873535
Epoch 640, val loss: 0.7134107947349548
Epoch 650, training loss: 12.895528793334961 = 0.26625341176986694 + 2.0 * 6.314637660980225
Epoch 650, val loss: 0.7099494934082031
Epoch 660, training loss: 12.881375312805176 = 0.2518981397151947 + 2.0 * 6.314738750457764
Epoch 660, val loss: 0.7070366144180298
Epoch 670, training loss: 12.861817359924316 = 0.23828302323818207 + 2.0 * 6.311767101287842
Epoch 670, val loss: 0.7046209573745728
Epoch 680, training loss: 12.845780372619629 = 0.2253335565328598 + 2.0 * 6.310223579406738
Epoch 680, val loss: 0.702759325504303
Epoch 690, training loss: 12.828428268432617 = 0.21307870745658875 + 2.0 * 6.307674884796143
Epoch 690, val loss: 0.7014312744140625
Epoch 700, training loss: 12.828447341918945 = 0.2014969438314438 + 2.0 * 6.313475131988525
Epoch 700, val loss: 0.7005496621131897
Epoch 710, training loss: 12.80189323425293 = 0.19056659936904907 + 2.0 * 6.305663108825684
Epoch 710, val loss: 0.7002272009849548
Epoch 720, training loss: 12.78980827331543 = 0.18030670285224915 + 2.0 * 6.304750919342041
Epoch 720, val loss: 0.7004103064537048
Epoch 730, training loss: 12.778059005737305 = 0.17067545652389526 + 2.0 * 6.303691864013672
Epoch 730, val loss: 0.7010197639465332
Epoch 740, training loss: 12.768285751342773 = 0.1616283804178238 + 2.0 * 6.303328514099121
Epoch 740, val loss: 0.7020052671432495
Epoch 750, training loss: 12.763139724731445 = 0.15315306186676025 + 2.0 * 6.304993152618408
Epoch 750, val loss: 0.7034002542495728
Epoch 760, training loss: 12.745298385620117 = 0.14520467817783356 + 2.0 * 6.300046920776367
Epoch 760, val loss: 0.7051445245742798
Epoch 770, training loss: 12.734033584594727 = 0.1377985030412674 + 2.0 * 6.298117637634277
Epoch 770, val loss: 0.7072383165359497
Epoch 780, training loss: 12.72793197631836 = 0.13083845376968384 + 2.0 * 6.29854679107666
Epoch 780, val loss: 0.7096303701400757
Epoch 790, training loss: 12.725764274597168 = 0.12431397289037704 + 2.0 * 6.300724983215332
Epoch 790, val loss: 0.7122575640678406
Epoch 800, training loss: 12.718705177307129 = 0.11823637038469315 + 2.0 * 6.300234317779541
Epoch 800, val loss: 0.7151133418083191
Epoch 810, training loss: 12.703381538391113 = 0.11254304647445679 + 2.0 * 6.295419216156006
Epoch 810, val loss: 0.7181165814399719
Epoch 820, training loss: 12.692140579223633 = 0.1072298213839531 + 2.0 * 6.292455196380615
Epoch 820, val loss: 0.7213422656059265
Epoch 830, training loss: 12.684929847717285 = 0.1022266373038292 + 2.0 * 6.291351795196533
Epoch 830, val loss: 0.72472083568573
Epoch 840, training loss: 12.684781074523926 = 0.09751704335212708 + 2.0 * 6.2936320304870605
Epoch 840, val loss: 0.7282460331916809
Epoch 850, training loss: 12.67265796661377 = 0.09311678260564804 + 2.0 * 6.289770603179932
Epoch 850, val loss: 0.7317996025085449
Epoch 860, training loss: 12.666699409484863 = 0.08896426111459732 + 2.0 * 6.288867473602295
Epoch 860, val loss: 0.7355284094810486
Epoch 870, training loss: 12.678474426269531 = 0.08508937805891037 + 2.0 * 6.296692371368408
Epoch 870, val loss: 0.7393327355384827
Epoch 880, training loss: 12.659612655639648 = 0.08140136301517487 + 2.0 * 6.289105415344238
Epoch 880, val loss: 0.7431806921958923
Epoch 890, training loss: 12.649094581604004 = 0.07795616239309311 + 2.0 * 6.285569190979004
Epoch 890, val loss: 0.7471217513084412
Epoch 900, training loss: 12.662302017211914 = 0.07471155375242233 + 2.0 * 6.293795108795166
Epoch 900, val loss: 0.7511045336723328
Epoch 910, training loss: 12.64660358428955 = 0.07163767516613007 + 2.0 * 6.287482738494873
Epoch 910, val loss: 0.7551263570785522
Epoch 920, training loss: 12.635351181030273 = 0.06873615831136703 + 2.0 * 6.2833075523376465
Epoch 920, val loss: 0.7592176795005798
Epoch 930, training loss: 12.636198997497559 = 0.06599827855825424 + 2.0 * 6.28510046005249
Epoch 930, val loss: 0.7633450031280518
Epoch 940, training loss: 12.626578330993652 = 0.06340630352497101 + 2.0 * 6.281586170196533
Epoch 940, val loss: 0.7674983143806458
Epoch 950, training loss: 12.629538536071777 = 0.06095103546977043 + 2.0 * 6.2842936515808105
Epoch 950, val loss: 0.771670401096344
Epoch 960, training loss: 12.621782302856445 = 0.058618802577257156 + 2.0 * 6.281581878662109
Epoch 960, val loss: 0.7758479714393616
Epoch 970, training loss: 12.621360778808594 = 0.05641308054327965 + 2.0 * 6.282474040985107
Epoch 970, val loss: 0.7800688743591309
Epoch 980, training loss: 12.61202335357666 = 0.05431342124938965 + 2.0 * 6.278854846954346
Epoch 980, val loss: 0.7842722535133362
Epoch 990, training loss: 12.60875129699707 = 0.05232144519686699 + 2.0 * 6.278214931488037
Epoch 990, val loss: 0.788536787033081
Epoch 1000, training loss: 12.61215877532959 = 0.050431765615940094 + 2.0 * 6.280863285064697
Epoch 1000, val loss: 0.7927831411361694
Epoch 1010, training loss: 12.603723526000977 = 0.04862517863512039 + 2.0 * 6.2775492668151855
Epoch 1010, val loss: 0.7970540523529053
Epoch 1020, training loss: 12.598498344421387 = 0.04691160097718239 + 2.0 * 6.275793552398682
Epoch 1020, val loss: 0.8012878894805908
Epoch 1030, training loss: 12.599466323852539 = 0.04528556019067764 + 2.0 * 6.277090549468994
Epoch 1030, val loss: 0.8055487871170044
Epoch 1040, training loss: 12.592740058898926 = 0.04373317211866379 + 2.0 * 6.274503231048584
Epoch 1040, val loss: 0.8097783923149109
Epoch 1050, training loss: 12.58923625946045 = 0.042258959263563156 + 2.0 * 6.273488521575928
Epoch 1050, val loss: 0.8139848709106445
Epoch 1060, training loss: 12.588818550109863 = 0.04084613174200058 + 2.0 * 6.273986339569092
Epoch 1060, val loss: 0.8181909918785095
Epoch 1070, training loss: 12.591289520263672 = 0.03950229287147522 + 2.0 * 6.275893688201904
Epoch 1070, val loss: 0.8223097920417786
Epoch 1080, training loss: 12.581904411315918 = 0.03822358697652817 + 2.0 * 6.271840572357178
Epoch 1080, val loss: 0.8265027403831482
Epoch 1090, training loss: 12.57811164855957 = 0.03700341656804085 + 2.0 * 6.270554065704346
Epoch 1090, val loss: 0.8306112885475159
Epoch 1100, training loss: 12.57808780670166 = 0.03583045303821564 + 2.0 * 6.2711286544799805
Epoch 1100, val loss: 0.8347752094268799
Epoch 1110, training loss: 12.579282760620117 = 0.03471475839614868 + 2.0 * 6.272284030914307
Epoch 1110, val loss: 0.8388194441795349
Epoch 1120, training loss: 12.571038246154785 = 0.03364960849285126 + 2.0 * 6.2686944007873535
Epoch 1120, val loss: 0.842868447303772
Epoch 1130, training loss: 12.570088386535645 = 0.032635852694511414 + 2.0 * 6.268726348876953
Epoch 1130, val loss: 0.8469163179397583
Epoch 1140, training loss: 12.570390701293945 = 0.03165733069181442 + 2.0 * 6.26936674118042
Epoch 1140, val loss: 0.8509483337402344
Epoch 1150, training loss: 12.565764427185059 = 0.0307238157838583 + 2.0 * 6.267520427703857
Epoch 1150, val loss: 0.8549361228942871
Epoch 1160, training loss: 12.562941551208496 = 0.02982681803405285 + 2.0 * 6.266557216644287
Epoch 1160, val loss: 0.858917772769928
Epoch 1170, training loss: 12.574270248413086 = 0.02897346019744873 + 2.0 * 6.272648334503174
Epoch 1170, val loss: 0.8628737926483154
Epoch 1180, training loss: 12.567044258117676 = 0.02814631536602974 + 2.0 * 6.269448757171631
Epoch 1180, val loss: 0.8666907548904419
Epoch 1190, training loss: 12.557289123535156 = 0.027360763400793076 + 2.0 * 6.2649641036987305
Epoch 1190, val loss: 0.8705984950065613
Epoch 1200, training loss: 12.554237365722656 = 0.026606274768710136 + 2.0 * 6.263815402984619
Epoch 1200, val loss: 0.8744530081748962
Epoch 1210, training loss: 12.556280136108398 = 0.025879455730319023 + 2.0 * 6.265200138092041
Epoch 1210, val loss: 0.8782978653907776
Epoch 1220, training loss: 12.556204795837402 = 0.025179924443364143 + 2.0 * 6.265512466430664
Epoch 1220, val loss: 0.8820773363113403
Epoch 1230, training loss: 12.549860954284668 = 0.024508122354745865 + 2.0 * 6.262676239013672
Epoch 1230, val loss: 0.8858274221420288
Epoch 1240, training loss: 12.556366920471191 = 0.023864369839429855 + 2.0 * 6.266251087188721
Epoch 1240, val loss: 0.8895300030708313
Epoch 1250, training loss: 12.547052383422852 = 0.023248404264450073 + 2.0 * 6.26190185546875
Epoch 1250, val loss: 0.8931724429130554
Epoch 1260, training loss: 12.546256065368652 = 0.022654861211776733 + 2.0 * 6.261800765991211
Epoch 1260, val loss: 0.8968310952186584
Epoch 1270, training loss: 12.544221878051758 = 0.022080544382333755 + 2.0 * 6.261070728302002
Epoch 1270, val loss: 0.9004716277122498
Epoch 1280, training loss: 12.558884620666504 = 0.021527092903852463 + 2.0 * 6.268678665161133
Epoch 1280, val loss: 0.9041044116020203
Epoch 1290, training loss: 12.545022964477539 = 0.020996762439608574 + 2.0 * 6.262012958526611
Epoch 1290, val loss: 0.9076393246650696
Epoch 1300, training loss: 12.538454055786133 = 0.020486732944846153 + 2.0 * 6.258983612060547
Epoch 1300, val loss: 0.9111503958702087
Epoch 1310, training loss: 12.545949935913086 = 0.019993700087070465 + 2.0 * 6.2629780769348145
Epoch 1310, val loss: 0.914647102355957
Epoch 1320, training loss: 12.540553092956543 = 0.019520530477166176 + 2.0 * 6.260516166687012
Epoch 1320, val loss: 0.918133556842804
Epoch 1330, training loss: 12.535515785217285 = 0.019062388688325882 + 2.0 * 6.2582268714904785
Epoch 1330, val loss: 0.921488344669342
Epoch 1340, training loss: 12.535280227661133 = 0.018622515723109245 + 2.0 * 6.258328914642334
Epoch 1340, val loss: 0.9248704314231873
Epoch 1350, training loss: 12.53868579864502 = 0.018194368109107018 + 2.0 * 6.2602458000183105
Epoch 1350, val loss: 0.9281830191612244
Epoch 1360, training loss: 12.53354549407959 = 0.01778545044362545 + 2.0 * 6.257880210876465
Epoch 1360, val loss: 0.9314465522766113
Epoch 1370, training loss: 12.530104637145996 = 0.01738910563290119 + 2.0 * 6.256357669830322
Epoch 1370, val loss: 0.9347191452980042
Epoch 1380, training loss: 12.52889347076416 = 0.017005445435643196 + 2.0 * 6.255943775177002
Epoch 1380, val loss: 0.9379971623420715
Epoch 1390, training loss: 12.545129776000977 = 0.01663193292915821 + 2.0 * 6.264248847961426
Epoch 1390, val loss: 0.9411807060241699
Epoch 1400, training loss: 12.535775184631348 = 0.0162760391831398 + 2.0 * 6.259749412536621
Epoch 1400, val loss: 0.9442698359489441
Epoch 1410, training loss: 12.526846885681152 = 0.015929395332932472 + 2.0 * 6.255458831787109
Epoch 1410, val loss: 0.9474629163742065
Epoch 1420, training loss: 12.524100303649902 = 0.015595550648868084 + 2.0 * 6.2542524337768555
Epoch 1420, val loss: 0.9505264759063721
Epoch 1430, training loss: 12.527058601379395 = 0.01527175959199667 + 2.0 * 6.255893230438232
Epoch 1430, val loss: 0.9536120295524597
Epoch 1440, training loss: 12.524858474731445 = 0.014957384206354618 + 2.0 * 6.254950523376465
Epoch 1440, val loss: 0.9565904140472412
Epoch 1450, training loss: 12.522492408752441 = 0.014650148339569569 + 2.0 * 6.253921031951904
Epoch 1450, val loss: 0.9596264958381653
Epoch 1460, training loss: 12.522500038146973 = 0.014355960302054882 + 2.0 * 6.254072189331055
Epoch 1460, val loss: 0.9625808000564575
Epoch 1470, training loss: 12.527315139770508 = 0.014068372547626495 + 2.0 * 6.256623268127441
Epoch 1470, val loss: 0.9655255079269409
Epoch 1480, training loss: 12.517884254455566 = 0.013794265687465668 + 2.0 * 6.252045154571533
Epoch 1480, val loss: 0.9684433341026306
Epoch 1490, training loss: 12.517549514770508 = 0.013527918606996536 + 2.0 * 6.252010822296143
Epoch 1490, val loss: 0.9712955951690674
Epoch 1500, training loss: 12.529411315917969 = 0.013267920352518559 + 2.0 * 6.2580718994140625
Epoch 1500, val loss: 0.974166989326477
Epoch 1510, training loss: 12.52043628692627 = 0.01301397942006588 + 2.0 * 6.253711223602295
Epoch 1510, val loss: 0.9768964052200317
Epoch 1520, training loss: 12.515923500061035 = 0.012771038338541985 + 2.0 * 6.2515764236450195
Epoch 1520, val loss: 0.9796844720840454
Epoch 1530, training loss: 12.51335620880127 = 0.012534555047750473 + 2.0 * 6.250411033630371
Epoch 1530, val loss: 0.982426643371582
Epoch 1540, training loss: 12.5148286819458 = 0.01230432465672493 + 2.0 * 6.251262187957764
Epoch 1540, val loss: 0.9851927757263184
Epoch 1550, training loss: 12.520471572875977 = 0.012079460546374321 + 2.0 * 6.2541961669921875
Epoch 1550, val loss: 0.9878498315811157
Epoch 1560, training loss: 12.513559341430664 = 0.011864285916090012 + 2.0 * 6.250847339630127
Epoch 1560, val loss: 0.9905101656913757
Epoch 1570, training loss: 12.511597633361816 = 0.011653142049908638 + 2.0 * 6.249972343444824
Epoch 1570, val loss: 0.9931914210319519
Epoch 1580, training loss: 12.520822525024414 = 0.011449363082647324 + 2.0 * 6.25468635559082
Epoch 1580, val loss: 0.9957898259162903
Epoch 1590, training loss: 12.5105562210083 = 0.011248378083109856 + 2.0 * 6.2496538162231445
Epoch 1590, val loss: 0.9983556866645813
Epoch 1600, training loss: 12.511805534362793 = 0.011056759394705296 + 2.0 * 6.2503743171691895
Epoch 1600, val loss: 1.0009220838546753
Epoch 1610, training loss: 12.511730194091797 = 0.010868321172893047 + 2.0 * 6.250431060791016
Epoch 1610, val loss: 1.0034749507904053
Epoch 1620, training loss: 12.509129524230957 = 0.010684685781598091 + 2.0 * 6.249222278594971
Epoch 1620, val loss: 1.0059869289398193
Epoch 1630, training loss: 12.504581451416016 = 0.01050493586808443 + 2.0 * 6.2470383644104
Epoch 1630, val loss: 1.0084748268127441
Epoch 1640, training loss: 12.505040168762207 = 0.010331215336918831 + 2.0 * 6.247354507446289
Epoch 1640, val loss: 1.010972499847412
Epoch 1650, training loss: 12.514673233032227 = 0.01016157865524292 + 2.0 * 6.252255916595459
Epoch 1650, val loss: 1.01341712474823
Epoch 1660, training loss: 12.504652976989746 = 0.009998802095651627 + 2.0 * 6.247326850891113
Epoch 1660, val loss: 1.0158098936080933
Epoch 1670, training loss: 12.50637149810791 = 0.009838802739977837 + 2.0 * 6.248266220092773
Epoch 1670, val loss: 1.0182186365127563
Epoch 1680, training loss: 12.505877494812012 = 0.009681466035544872 + 2.0 * 6.248097896575928
Epoch 1680, val loss: 1.0205702781677246
Epoch 1690, training loss: 12.51119327545166 = 0.009529130533337593 + 2.0 * 6.2508320808410645
Epoch 1690, val loss: 1.0228792428970337
Epoch 1700, training loss: 12.505353927612305 = 0.009383839555084705 + 2.0 * 6.247984886169434
Epoch 1700, val loss: 1.0252207517623901
Epoch 1710, training loss: 12.502213478088379 = 0.009238364174962044 + 2.0 * 6.246487617492676
Epoch 1710, val loss: 1.0275243520736694
Epoch 1720, training loss: 12.498320579528809 = 0.009098888374865055 + 2.0 * 6.244610786437988
Epoch 1720, val loss: 1.0297837257385254
Epoch 1730, training loss: 12.50411319732666 = 0.008961928077042103 + 2.0 * 6.247575759887695
Epoch 1730, val loss: 1.032021164894104
Epoch 1740, training loss: 12.496350288391113 = 0.008828059770166874 + 2.0 * 6.24376106262207
Epoch 1740, val loss: 1.0342881679534912
Epoch 1750, training loss: 12.49866008758545 = 0.008697042241692543 + 2.0 * 6.244981288909912
Epoch 1750, val loss: 1.0365692377090454
Epoch 1760, training loss: 12.505398750305176 = 0.008570326492190361 + 2.0 * 6.248414039611816
Epoch 1760, val loss: 1.0387300252914429
Epoch 1770, training loss: 12.497352600097656 = 0.008446014486253262 + 2.0 * 6.244453430175781
Epoch 1770, val loss: 1.0409413576126099
Epoch 1780, training loss: 12.496033668518066 = 0.008323676884174347 + 2.0 * 6.243854999542236
Epoch 1780, val loss: 1.043129324913025
Epoch 1790, training loss: 12.499761581420898 = 0.008206019178032875 + 2.0 * 6.245777606964111
Epoch 1790, val loss: 1.0453150272369385
Epoch 1800, training loss: 12.49500846862793 = 0.008089076727628708 + 2.0 * 6.243459701538086
Epoch 1800, val loss: 1.047385811805725
Epoch 1810, training loss: 12.50778865814209 = 0.007974307984113693 + 2.0 * 6.24990701675415
Epoch 1810, val loss: 1.0495527982711792
Epoch 1820, training loss: 12.49465274810791 = 0.007866043597459793 + 2.0 * 6.243393421173096
Epoch 1820, val loss: 1.0514963865280151
Epoch 1830, training loss: 12.491195678710938 = 0.007758135441690683 + 2.0 * 6.241718769073486
Epoch 1830, val loss: 1.0536435842514038
Epoch 1840, training loss: 12.489357948303223 = 0.007652848493307829 + 2.0 * 6.240852355957031
Epoch 1840, val loss: 1.0556772947311401
Epoch 1850, training loss: 12.490194320678711 = 0.007550008594989777 + 2.0 * 6.241322040557861
Epoch 1850, val loss: 1.0577372312545776
Epoch 1860, training loss: 12.510066986083984 = 0.007448897231370211 + 2.0 * 6.251308917999268
Epoch 1860, val loss: 1.0597259998321533
Epoch 1870, training loss: 12.501280784606934 = 0.0073479581624269485 + 2.0 * 6.246966361999512
Epoch 1870, val loss: 1.06166672706604
Epoch 1880, training loss: 12.48913288116455 = 0.007253190036863089 + 2.0 * 6.240939617156982
Epoch 1880, val loss: 1.0636346340179443
Epoch 1890, training loss: 12.486627578735352 = 0.007158536463975906 + 2.0 * 6.239734649658203
Epoch 1890, val loss: 1.0656193494796753
Epoch 1900, training loss: 12.489938735961914 = 0.007066650781780481 + 2.0 * 6.241436004638672
Epoch 1900, val loss: 1.0675588846206665
Epoch 1910, training loss: 12.491522789001465 = 0.006976551376283169 + 2.0 * 6.242273330688477
Epoch 1910, val loss: 1.0694319009780884
Epoch 1920, training loss: 12.495738983154297 = 0.006886494345963001 + 2.0 * 6.244426250457764
Epoch 1920, val loss: 1.0712965726852417
Epoch 1930, training loss: 12.486526489257812 = 0.0068001220934093 + 2.0 * 6.239863395690918
Epoch 1930, val loss: 1.0732018947601318
Epoch 1940, training loss: 12.485180854797363 = 0.006715293042361736 + 2.0 * 6.239233016967773
Epoch 1940, val loss: 1.0750683546066284
Epoch 1950, training loss: 12.48477840423584 = 0.006631832104176283 + 2.0 * 6.239073276519775
Epoch 1950, val loss: 1.0769237279891968
Epoch 1960, training loss: 12.496941566467285 = 0.0065498570911586285 + 2.0 * 6.2451958656311035
Epoch 1960, val loss: 1.0787441730499268
Epoch 1970, training loss: 12.487685203552246 = 0.006469881162047386 + 2.0 * 6.240607738494873
Epoch 1970, val loss: 1.080538272857666
Epoch 1980, training loss: 12.483678817749023 = 0.006392228417098522 + 2.0 * 6.238643169403076
Epoch 1980, val loss: 1.0823348760604858
Epoch 1990, training loss: 12.484418869018555 = 0.006316115614026785 + 2.0 * 6.239051342010498
Epoch 1990, val loss: 1.0841163396835327
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7814814814814816
0.8397469688982605
=== training gcn model ===
Epoch 0, training loss: 19.13784408569336 = 1.9441094398498535 + 2.0 * 8.596867561340332
Epoch 0, val loss: 1.947424054145813
Epoch 10, training loss: 19.127431869506836 = 1.9341356754302979 + 2.0 * 8.596648216247559
Epoch 10, val loss: 1.9377888441085815
Epoch 20, training loss: 19.110824584960938 = 1.9216904640197754 + 2.0 * 8.59456729888916
Epoch 20, val loss: 1.9252221584320068
Epoch 30, training loss: 19.058834075927734 = 1.9043056964874268 + 2.0 * 8.577263832092285
Epoch 30, val loss: 1.9073126316070557
Epoch 40, training loss: 18.845151901245117 = 1.8812872171401978 + 2.0 * 8.481932640075684
Epoch 40, val loss: 1.8841599225997925
Epoch 50, training loss: 18.11880111694336 = 1.855685830116272 + 2.0 * 8.13155746459961
Epoch 50, val loss: 1.859198808670044
Epoch 60, training loss: 17.61116600036621 = 1.832123875617981 + 2.0 * 7.88952112197876
Epoch 60, val loss: 1.8374453783035278
Epoch 70, training loss: 16.743349075317383 = 1.8144574165344238 + 2.0 * 7.464446067810059
Epoch 70, val loss: 1.8209941387176514
Epoch 80, training loss: 16.169065475463867 = 1.8013794422149658 + 2.0 * 7.18384313583374
Epoch 80, val loss: 1.8080357313156128
Epoch 90, training loss: 15.732264518737793 = 1.7882264852523804 + 2.0 * 6.972019195556641
Epoch 90, val loss: 1.7949273586273193
Epoch 100, training loss: 15.4876070022583 = 1.7734180688858032 + 2.0 * 6.8570942878723145
Epoch 100, val loss: 1.7802002429962158
Epoch 110, training loss: 15.340575218200684 = 1.7580219507217407 + 2.0 * 6.791276454925537
Epoch 110, val loss: 1.7649857997894287
Epoch 120, training loss: 15.222558975219727 = 1.7422388792037964 + 2.0 * 6.74015998840332
Epoch 120, val loss: 1.7497001886367798
Epoch 130, training loss: 15.11320972442627 = 1.7265056371688843 + 2.0 * 6.693352222442627
Epoch 130, val loss: 1.7347967624664307
Epoch 140, training loss: 15.016667366027832 = 1.7103444337844849 + 2.0 * 6.653161525726318
Epoch 140, val loss: 1.7198033332824707
Epoch 150, training loss: 14.93459701538086 = 1.6918976306915283 + 2.0 * 6.621349811553955
Epoch 150, val loss: 1.7031797170639038
Epoch 160, training loss: 14.858370780944824 = 1.6706255674362183 + 2.0 * 6.593872547149658
Epoch 160, val loss: 1.6843551397323608
Epoch 170, training loss: 14.786409378051758 = 1.646552562713623 + 2.0 * 6.5699286460876465
Epoch 170, val loss: 1.6632795333862305
Epoch 180, training loss: 14.714934349060059 = 1.619913935661316 + 2.0 * 6.547510147094727
Epoch 180, val loss: 1.6402751207351685
Epoch 190, training loss: 14.649900436401367 = 1.5905035734176636 + 2.0 * 6.529698371887207
Epoch 190, val loss: 1.6150290966033936
Epoch 200, training loss: 14.578351020812988 = 1.5583184957504272 + 2.0 * 6.510016441345215
Epoch 200, val loss: 1.5873574018478394
Epoch 210, training loss: 14.509031295776367 = 1.523300051689148 + 2.0 * 6.492865562438965
Epoch 210, val loss: 1.5574891567230225
Epoch 220, training loss: 14.451478958129883 = 1.4855984449386597 + 2.0 * 6.482940196990967
Epoch 220, val loss: 1.5254383087158203
Epoch 230, training loss: 14.379104614257812 = 1.4456557035446167 + 2.0 * 6.466724395751953
Epoch 230, val loss: 1.491933822631836
Epoch 240, training loss: 14.31422233581543 = 1.4040932655334473 + 2.0 * 6.45506477355957
Epoch 240, val loss: 1.4573681354522705
Epoch 250, training loss: 14.252725601196289 = 1.3609600067138672 + 2.0 * 6.445882797241211
Epoch 250, val loss: 1.4220044612884521
Epoch 260, training loss: 14.200830459594727 = 1.3166563510894775 + 2.0 * 6.442087173461914
Epoch 260, val loss: 1.3861366510391235
Epoch 270, training loss: 14.140151977539062 = 1.2724074125289917 + 2.0 * 6.433872222900391
Epoch 270, val loss: 1.350475788116455
Epoch 280, training loss: 14.081430435180664 = 1.228373408317566 + 2.0 * 6.426528453826904
Epoch 280, val loss: 1.3155447244644165
Epoch 290, training loss: 14.025985717773438 = 1.1846504211425781 + 2.0 * 6.42066764831543
Epoch 290, val loss: 1.28128981590271
Epoch 300, training loss: 13.973258972167969 = 1.141339659690857 + 2.0 * 6.41595983505249
Epoch 300, val loss: 1.2476449012756348
Epoch 310, training loss: 13.925201416015625 = 1.0987627506256104 + 2.0 * 6.413219451904297
Epoch 310, val loss: 1.2147935628890991
Epoch 320, training loss: 13.872459411621094 = 1.0576025247573853 + 2.0 * 6.40742826461792
Epoch 320, val loss: 1.1832385063171387
Epoch 330, training loss: 13.824043273925781 = 1.0175166130065918 + 2.0 * 6.403263092041016
Epoch 330, val loss: 1.1528608798980713
Epoch 340, training loss: 13.775915145874023 = 0.9782315492630005 + 2.0 * 6.398841857910156
Epoch 340, val loss: 1.1232881546020508
Epoch 350, training loss: 13.734477996826172 = 0.9397841095924377 + 2.0 * 6.3973469734191895
Epoch 350, val loss: 1.0945345163345337
Epoch 360, training loss: 13.68681526184082 = 0.902677059173584 + 2.0 * 6.392068862915039
Epoch 360, val loss: 1.066666603088379
Epoch 370, training loss: 13.640807151794434 = 0.8665347695350647 + 2.0 * 6.387135982513428
Epoch 370, val loss: 1.039841890335083
Epoch 380, training loss: 13.59787654876709 = 0.8312742710113525 + 2.0 * 6.383301258087158
Epoch 380, val loss: 1.0136042833328247
Epoch 390, training loss: 13.557313919067383 = 0.796951413154602 + 2.0 * 6.380181312561035
Epoch 390, val loss: 0.9882389903068542
Epoch 400, training loss: 13.51694107055664 = 0.7641141414642334 + 2.0 * 6.376413345336914
Epoch 400, val loss: 0.9640058875083923
Epoch 410, training loss: 13.477967262268066 = 0.7323496341705322 + 2.0 * 6.372808933258057
Epoch 410, val loss: 0.9407578110694885
Epoch 420, training loss: 13.440332412719727 = 0.7014312744140625 + 2.0 * 6.369450569152832
Epoch 420, val loss: 0.918361485004425
Epoch 430, training loss: 13.412200927734375 = 0.671482503414154 + 2.0 * 6.370359420776367
Epoch 430, val loss: 0.8969795107841492
Epoch 440, training loss: 13.373071670532227 = 0.6429154872894287 + 2.0 * 6.365077972412109
Epoch 440, val loss: 0.8771166205406189
Epoch 450, training loss: 13.339005470275879 = 0.615372359752655 + 2.0 * 6.36181640625
Epoch 450, val loss: 0.8585417866706848
Epoch 460, training loss: 13.30527114868164 = 0.5887579917907715 + 2.0 * 6.358256816864014
Epoch 460, val loss: 0.8410241603851318
Epoch 470, training loss: 13.277170181274414 = 0.5630684494972229 + 2.0 * 6.357050895690918
Epoch 470, val loss: 0.8248320817947388
Epoch 480, training loss: 13.24578857421875 = 0.5385479927062988 + 2.0 * 6.3536200523376465
Epoch 480, val loss: 0.8100049495697021
Epoch 490, training loss: 13.216673851013184 = 0.5149837732315063 + 2.0 * 6.350844860076904
Epoch 490, val loss: 0.796504020690918
Epoch 500, training loss: 13.189454078674316 = 0.49232587218284607 + 2.0 * 6.348564147949219
Epoch 500, val loss: 0.7842094302177429
Epoch 510, training loss: 13.185576438903809 = 0.4706730246543884 + 2.0 * 6.357451915740967
Epoch 510, val loss: 0.7730441093444824
Epoch 520, training loss: 13.14045524597168 = 0.450137197971344 + 2.0 * 6.34515905380249
Epoch 520, val loss: 0.7633525729179382
Epoch 530, training loss: 13.117910385131836 = 0.43071454763412476 + 2.0 * 6.343597888946533
Epoch 530, val loss: 0.7549736499786377
Epoch 540, training loss: 13.092686653137207 = 0.41217708587646484 + 2.0 * 6.340254783630371
Epoch 540, val loss: 0.7475329041481018
Epoch 550, training loss: 13.071029663085938 = 0.3944043219089508 + 2.0 * 6.33831262588501
Epoch 550, val loss: 0.74106764793396
Epoch 560, training loss: 13.050846099853516 = 0.3773934543132782 + 2.0 * 6.336726188659668
Epoch 560, val loss: 0.7355475425720215
Epoch 570, training loss: 13.038253784179688 = 0.36117473244667053 + 2.0 * 6.3385396003723145
Epoch 570, val loss: 0.7307261824607849
Epoch 580, training loss: 13.015083312988281 = 0.3457679748535156 + 2.0 * 6.334657669067383
Epoch 580, val loss: 0.7268837690353394
Epoch 590, training loss: 12.9962158203125 = 0.33116692304611206 + 2.0 * 6.332524299621582
Epoch 590, val loss: 0.7237686514854431
Epoch 600, training loss: 12.976990699768066 = 0.3171682357788086 + 2.0 * 6.329911231994629
Epoch 600, val loss: 0.7214070558547974
Epoch 610, training loss: 12.964425086975098 = 0.3037143647670746 + 2.0 * 6.330355167388916
Epoch 610, val loss: 0.7196592092514038
Epoch 620, training loss: 12.946426391601562 = 0.29080939292907715 + 2.0 * 6.327808380126953
Epoch 620, val loss: 0.7183789610862732
Epoch 630, training loss: 12.948878288269043 = 0.2784520387649536 + 2.0 * 6.3352131843566895
Epoch 630, val loss: 0.7175708413124084
Epoch 640, training loss: 12.92109489440918 = 0.2665325999259949 + 2.0 * 6.3272809982299805
Epoch 640, val loss: 0.7174456119537354
Epoch 650, training loss: 12.901515007019043 = 0.25517672300338745 + 2.0 * 6.323169231414795
Epoch 650, val loss: 0.7176268696784973
Epoch 660, training loss: 12.88554573059082 = 0.24420146644115448 + 2.0 * 6.320672035217285
Epoch 660, val loss: 0.7183201313018799
Epoch 670, training loss: 12.871810913085938 = 0.23355479538440704 + 2.0 * 6.319128036499023
Epoch 670, val loss: 0.719432532787323
Epoch 680, training loss: 12.858732223510742 = 0.22322851419448853 + 2.0 * 6.317751884460449
Epoch 680, val loss: 0.7208854556083679
Epoch 690, training loss: 12.85964584350586 = 0.21324506402015686 + 2.0 * 6.323200225830078
Epoch 690, val loss: 0.722609281539917
Epoch 700, training loss: 12.839144706726074 = 0.20365530252456665 + 2.0 * 6.317744731903076
Epoch 700, val loss: 0.7247064113616943
Epoch 710, training loss: 12.823958396911621 = 0.19443640112876892 + 2.0 * 6.314761161804199
Epoch 710, val loss: 0.7270865440368652
Epoch 720, training loss: 12.812322616577148 = 0.18556317687034607 + 2.0 * 6.313379764556885
Epoch 720, val loss: 0.7298043966293335
Epoch 730, training loss: 12.807044982910156 = 0.17703725397586823 + 2.0 * 6.315003871917725
Epoch 730, val loss: 0.7326355576515198
Epoch 740, training loss: 12.795409202575684 = 0.168914794921875 + 2.0 * 6.313247203826904
Epoch 740, val loss: 0.7356065511703491
Epoch 750, training loss: 12.779629707336426 = 0.16115252673625946 + 2.0 * 6.309238433837891
Epoch 750, val loss: 0.7387943863868713
Epoch 760, training loss: 12.769516944885254 = 0.1537022888660431 + 2.0 * 6.3079071044921875
Epoch 760, val loss: 0.7422097325325012
Epoch 770, training loss: 12.779191970825195 = 0.1465592235326767 + 2.0 * 6.316316604614258
Epoch 770, val loss: 0.7457730770111084
Epoch 780, training loss: 12.755898475646973 = 0.13980607688426971 + 2.0 * 6.308046340942383
Epoch 780, val loss: 0.7492188811302185
Epoch 790, training loss: 12.756523132324219 = 0.13337787985801697 + 2.0 * 6.311572551727295
Epoch 790, val loss: 0.7528949975967407
Epoch 800, training loss: 12.735859870910645 = 0.12729012966156006 + 2.0 * 6.304285049438477
Epoch 800, val loss: 0.756668210029602
Epoch 810, training loss: 12.726524353027344 = 0.12152315676212311 + 2.0 * 6.3025007247924805
Epoch 810, val loss: 0.760446310043335
Epoch 820, training loss: 12.723665237426758 = 0.11603162437677383 + 2.0 * 6.303816795349121
Epoch 820, val loss: 0.7644615769386292
Epoch 830, training loss: 12.711091041564941 = 0.11083373427391052 + 2.0 * 6.30012845993042
Epoch 830, val loss: 0.768342137336731
Epoch 840, training loss: 12.704238891601562 = 0.10591579228639603 + 2.0 * 6.299161434173584
Epoch 840, val loss: 0.7722099423408508
Epoch 850, training loss: 12.697714805603027 = 0.1012456938624382 + 2.0 * 6.298234462738037
Epoch 850, val loss: 0.776306688785553
Epoch 860, training loss: 12.697551727294922 = 0.09681646525859833 + 2.0 * 6.300367832183838
Epoch 860, val loss: 0.7804564237594604
Epoch 870, training loss: 12.693860054016113 = 0.09262131154537201 + 2.0 * 6.300619602203369
Epoch 870, val loss: 0.7845615148544312
Epoch 880, training loss: 12.679954528808594 = 0.08867454528808594 + 2.0 * 6.295639991760254
Epoch 880, val loss: 0.7885701060295105
Epoch 890, training loss: 12.677967071533203 = 0.08492662757635117 + 2.0 * 6.296520233154297
Epoch 890, val loss: 0.7928406596183777
Epoch 900, training loss: 12.670047760009766 = 0.0813772901892662 + 2.0 * 6.29433536529541
Epoch 900, val loss: 0.7970163226127625
Epoch 910, training loss: 12.664231300354004 = 0.07802269607782364 + 2.0 * 6.29310417175293
Epoch 910, val loss: 0.8012167811393738
Epoch 920, training loss: 12.658416748046875 = 0.07485354691743851 + 2.0 * 6.291781425476074
Epoch 920, val loss: 0.8054783940315247
Epoch 930, training loss: 12.65969467163086 = 0.07184188067913055 + 2.0 * 6.293926239013672
Epoch 930, val loss: 0.8097847700119019
Epoch 940, training loss: 12.65713119506836 = 0.06898107379674911 + 2.0 * 6.294075012207031
Epoch 940, val loss: 0.8142397403717041
Epoch 950, training loss: 12.64731216430664 = 0.06628061085939407 + 2.0 * 6.290515899658203
Epoch 950, val loss: 0.8182609677314758
Epoch 960, training loss: 12.638794898986816 = 0.06372803449630737 + 2.0 * 6.287533283233643
Epoch 960, val loss: 0.8226860761642456
Epoch 970, training loss: 12.635305404663086 = 0.061302416026592255 + 2.0 * 6.287001609802246
Epoch 970, val loss: 0.8270938396453857
Epoch 980, training loss: 12.64279842376709 = 0.0589938648045063 + 2.0 * 6.2919020652771
Epoch 980, val loss: 0.8314930200576782
Epoch 990, training loss: 12.63023567199707 = 0.0567903071641922 + 2.0 * 6.286722660064697
Epoch 990, val loss: 0.8358877301216125
Epoch 1000, training loss: 12.630073547363281 = 0.054718017578125 + 2.0 * 6.287677764892578
Epoch 1000, val loss: 0.8402121663093567
Epoch 1010, training loss: 12.622445106506348 = 0.05273989215493202 + 2.0 * 6.284852504730225
Epoch 1010, val loss: 0.8446580767631531
Epoch 1020, training loss: 12.618239402770996 = 0.05086444318294525 + 2.0 * 6.283687591552734
Epoch 1020, val loss: 0.8491050601005554
Epoch 1030, training loss: 12.627276420593262 = 0.04907239228487015 + 2.0 * 6.289102077484131
Epoch 1030, val loss: 0.853634238243103
Epoch 1040, training loss: 12.612327575683594 = 0.047375746071338654 + 2.0 * 6.28247594833374
Epoch 1040, val loss: 0.8579224944114685
Epoch 1050, training loss: 12.609058380126953 = 0.04575897753238678 + 2.0 * 6.281649589538574
Epoch 1050, val loss: 0.8622775077819824
Epoch 1060, training loss: 12.61027717590332 = 0.04421591758728027 + 2.0 * 6.2830305099487305
Epoch 1060, val loss: 0.866750955581665
Epoch 1070, training loss: 12.603888511657715 = 0.04275258257985115 + 2.0 * 6.2805681228637695
Epoch 1070, val loss: 0.8709585666656494
Epoch 1080, training loss: 12.602188110351562 = 0.04135066270828247 + 2.0 * 6.280418872833252
Epoch 1080, val loss: 0.875329315662384
Epoch 1090, training loss: 12.59757137298584 = 0.040010757744312286 + 2.0 * 6.278780460357666
Epoch 1090, val loss: 0.8797193765640259
Epoch 1100, training loss: 12.597634315490723 = 0.03873492777347565 + 2.0 * 6.279449462890625
Epoch 1100, val loss: 0.883949875831604
Epoch 1110, training loss: 12.598387718200684 = 0.037514328956604004 + 2.0 * 6.2804365158081055
Epoch 1110, val loss: 0.8883445262908936
Epoch 1120, training loss: 12.595602035522461 = 0.03635135665535927 + 2.0 * 6.279625415802002
Epoch 1120, val loss: 0.8925721049308777
Epoch 1130, training loss: 12.587746620178223 = 0.035240013152360916 + 2.0 * 6.2762532234191895
Epoch 1130, val loss: 0.8966231942176819
Epoch 1140, training loss: 12.583805084228516 = 0.034177567809820175 + 2.0 * 6.274813652038574
Epoch 1140, val loss: 0.9008244872093201
Epoch 1150, training loss: 12.59051513671875 = 0.03316090255975723 + 2.0 * 6.278676986694336
Epoch 1150, val loss: 0.9049915075302124
Epoch 1160, training loss: 12.579804420471191 = 0.0321766659617424 + 2.0 * 6.273813724517822
Epoch 1160, val loss: 0.9092922210693359
Epoch 1170, training loss: 12.57819652557373 = 0.031242651864886284 + 2.0 * 6.273477077484131
Epoch 1170, val loss: 0.9132351875305176
Epoch 1180, training loss: 12.579292297363281 = 0.030348431318998337 + 2.0 * 6.274471759796143
Epoch 1180, val loss: 0.9173784255981445
Epoch 1190, training loss: 12.57780647277832 = 0.02948676608502865 + 2.0 * 6.274159908294678
Epoch 1190, val loss: 0.9214415550231934
Epoch 1200, training loss: 12.570327758789062 = 0.028663810342550278 + 2.0 * 6.270832061767578
Epoch 1200, val loss: 0.925366997718811
Epoch 1210, training loss: 12.56851863861084 = 0.027875768020749092 + 2.0 * 6.270321369171143
Epoch 1210, val loss: 0.9293510317802429
Epoch 1220, training loss: 12.56910514831543 = 0.02711549587547779 + 2.0 * 6.270994663238525
Epoch 1220, val loss: 0.933451235294342
Epoch 1230, training loss: 12.573238372802734 = 0.02638326957821846 + 2.0 * 6.273427486419678
Epoch 1230, val loss: 0.9373693466186523
Epoch 1240, training loss: 12.565595626831055 = 0.025680048391222954 + 2.0 * 6.269958019256592
Epoch 1240, val loss: 0.941134512424469
Epoch 1250, training loss: 12.561717987060547 = 0.02500862255692482 + 2.0 * 6.268354892730713
Epoch 1250, val loss: 0.9448433518409729
Epoch 1260, training loss: 12.563283920288086 = 0.02436274290084839 + 2.0 * 6.269460678100586
Epoch 1260, val loss: 0.9487887620925903
Epoch 1270, training loss: 12.56407356262207 = 0.023738475516438484 + 2.0 * 6.270167350769043
Epoch 1270, val loss: 0.9525368213653564
Epoch 1280, training loss: 12.559600830078125 = 0.023138392716646194 + 2.0 * 6.268231391906738
Epoch 1280, val loss: 0.9561861157417297
Epoch 1290, training loss: 12.556159019470215 = 0.02256159484386444 + 2.0 * 6.266798496246338
Epoch 1290, val loss: 0.9599226117134094
Epoch 1300, training loss: 12.554397583007812 = 0.02200501412153244 + 2.0 * 6.266196250915527
Epoch 1300, val loss: 0.9635563492774963
Epoch 1310, training loss: 12.557332038879395 = 0.021468738093972206 + 2.0 * 6.2679314613342285
Epoch 1310, val loss: 0.9672536253929138
Epoch 1320, training loss: 12.554588317871094 = 0.02094918303191662 + 2.0 * 6.266819477081299
Epoch 1320, val loss: 0.9708250761032104
Epoch 1330, training loss: 12.550337791442871 = 0.020448952913284302 + 2.0 * 6.264944553375244
Epoch 1330, val loss: 0.9743946194648743
Epoch 1340, training loss: 12.54698371887207 = 0.019968854263424873 + 2.0 * 6.26350736618042
Epoch 1340, val loss: 0.9777641892433167
Epoch 1350, training loss: 12.546202659606934 = 0.019505945965647697 + 2.0 * 6.263348579406738
Epoch 1350, val loss: 0.981320858001709
Epoch 1360, training loss: 12.561013221740723 = 0.019058840349316597 + 2.0 * 6.270977020263672
Epoch 1360, val loss: 0.9848200082778931
Epoch 1370, training loss: 12.552399635314941 = 0.018625378608703613 + 2.0 * 6.266887187957764
Epoch 1370, val loss: 0.9881663918495178
Epoch 1380, training loss: 12.544561386108398 = 0.018208160996437073 + 2.0 * 6.263176441192627
Epoch 1380, val loss: 0.991419792175293
Epoch 1390, training loss: 12.540349006652832 = 0.017805634066462517 + 2.0 * 6.2612714767456055
Epoch 1390, val loss: 0.9948200583457947
Epoch 1400, training loss: 12.540372848510742 = 0.017416376620531082 + 2.0 * 6.261478424072266
Epoch 1400, val loss: 0.9982357621192932
Epoch 1410, training loss: 12.556717872619629 = 0.017038920894265175 + 2.0 * 6.269839286804199
Epoch 1410, val loss: 1.0015206336975098
Epoch 1420, training loss: 12.538227081298828 = 0.016670718789100647 + 2.0 * 6.260777950286865
Epoch 1420, val loss: 1.0045989751815796
Epoch 1430, training loss: 12.535656929016113 = 0.016319356858730316 + 2.0 * 6.259668827056885
Epoch 1430, val loss: 1.0077801942825317
Epoch 1440, training loss: 12.53349494934082 = 0.01598026603460312 + 2.0 * 6.2587571144104
Epoch 1440, val loss: 1.011037826538086
Epoch 1450, training loss: 12.533556938171387 = 0.01564883626997471 + 2.0 * 6.258954048156738
Epoch 1450, val loss: 1.0142712593078613
Epoch 1460, training loss: 12.544811248779297 = 0.015327589586377144 + 2.0 * 6.264741897583008
Epoch 1460, val loss: 1.0174137353897095
Epoch 1470, training loss: 12.531596183776855 = 0.01501445472240448 + 2.0 * 6.258290767669678
Epoch 1470, val loss: 1.0204471349716187
Epoch 1480, training loss: 12.532144546508789 = 0.014713888056576252 + 2.0 * 6.2587151527404785
Epoch 1480, val loss: 1.023418664932251
Epoch 1490, training loss: 12.535688400268555 = 0.01442257221788168 + 2.0 * 6.2606329917907715
Epoch 1490, val loss: 1.0265175104141235
Epoch 1500, training loss: 12.52694034576416 = 0.014140442945063114 + 2.0 * 6.256400108337402
Epoch 1500, val loss: 1.0294054746627808
Epoch 1510, training loss: 12.527399063110352 = 0.013867419213056564 + 2.0 * 6.256765842437744
Epoch 1510, val loss: 1.0323364734649658
Epoch 1520, training loss: 12.536124229431152 = 0.01360114011913538 + 2.0 * 6.261261463165283
Epoch 1520, val loss: 1.0353788137435913
Epoch 1530, training loss: 12.53371810913086 = 0.013339251279830933 + 2.0 * 6.260189533233643
Epoch 1530, val loss: 1.0383154153823853
Epoch 1540, training loss: 12.524979591369629 = 0.01309121772646904 + 2.0 * 6.25594425201416
Epoch 1540, val loss: 1.0409483909606934
Epoch 1550, training loss: 12.522411346435547 = 0.012848288752138615 + 2.0 * 6.254781723022461
Epoch 1550, val loss: 1.0438557863235474
Epoch 1560, training loss: 12.523863792419434 = 0.012612349353730679 + 2.0 * 6.2556257247924805
Epoch 1560, val loss: 1.0468024015426636
Epoch 1570, training loss: 12.533860206604004 = 0.01238175854086876 + 2.0 * 6.260739326477051
Epoch 1570, val loss: 1.04965341091156
Epoch 1580, training loss: 12.525607109069824 = 0.01215615775436163 + 2.0 * 6.256725311279297
Epoch 1580, val loss: 1.052367925643921
Epoch 1590, training loss: 12.518939971923828 = 0.011941127479076385 + 2.0 * 6.253499507904053
Epoch 1590, val loss: 1.0548735857009888
Epoch 1600, training loss: 12.518001556396484 = 0.011730893515050411 + 2.0 * 6.2531352043151855
Epoch 1600, val loss: 1.0577079057693481
Epoch 1610, training loss: 12.523483276367188 = 0.011525098234415054 + 2.0 * 6.255979061126709
Epoch 1610, val loss: 1.0604898929595947
Epoch 1620, training loss: 12.517969131469727 = 0.011324713937938213 + 2.0 * 6.253322124481201
Epoch 1620, val loss: 1.0632200241088867
Epoch 1630, training loss: 12.516888618469238 = 0.01112992875277996 + 2.0 * 6.2528791427612305
Epoch 1630, val loss: 1.0657141208648682
Epoch 1640, training loss: 12.51913070678711 = 0.01094115525484085 + 2.0 * 6.25409460067749
Epoch 1640, val loss: 1.0683526992797852
Epoch 1650, training loss: 12.517653465270996 = 0.01075813639909029 + 2.0 * 6.253447532653809
Epoch 1650, val loss: 1.070902943611145
Epoch 1660, training loss: 12.517115592956543 = 0.010578468441963196 + 2.0 * 6.253268718719482
Epoch 1660, val loss: 1.073512077331543
Epoch 1670, training loss: 12.513614654541016 = 0.01040366105735302 + 2.0 * 6.25160551071167
Epoch 1670, val loss: 1.0759949684143066
Epoch 1680, training loss: 12.512228965759277 = 0.010233074426651001 + 2.0 * 6.250998020172119
Epoch 1680, val loss: 1.078629970550537
Epoch 1690, training loss: 12.517370223999023 = 0.01006636954843998 + 2.0 * 6.253652095794678
Epoch 1690, val loss: 1.0811793804168701
Epoch 1700, training loss: 12.515283584594727 = 0.00990340206772089 + 2.0 * 6.252690315246582
Epoch 1700, val loss: 1.0835211277008057
Epoch 1710, training loss: 12.508110046386719 = 0.009746174328029156 + 2.0 * 6.249181747436523
Epoch 1710, val loss: 1.0859500169754028
Epoch 1720, training loss: 12.509452819824219 = 0.009593794122338295 + 2.0 * 6.249929428100586
Epoch 1720, val loss: 1.0882549285888672
Epoch 1730, training loss: 12.512580871582031 = 0.009443534538149834 + 2.0 * 6.251568794250488
Epoch 1730, val loss: 1.0908222198486328
Epoch 1740, training loss: 12.510271072387695 = 0.009297053329646587 + 2.0 * 6.250486850738525
Epoch 1740, val loss: 1.0930860042572021
Epoch 1750, training loss: 12.50511360168457 = 0.009155165404081345 + 2.0 * 6.247979164123535
Epoch 1750, val loss: 1.095378041267395
Epoch 1760, training loss: 12.51420783996582 = 0.009016793221235275 + 2.0 * 6.2525954246521
Epoch 1760, val loss: 1.097717523574829
Epoch 1770, training loss: 12.505474090576172 = 0.008879904635250568 + 2.0 * 6.248297214508057
Epoch 1770, val loss: 1.1000429391860962
Epoch 1780, training loss: 12.50792407989502 = 0.00874816719442606 + 2.0 * 6.2495880126953125
Epoch 1780, val loss: 1.10224449634552
Epoch 1790, training loss: 12.5088472366333 = 0.008618890307843685 + 2.0 * 6.2501139640808105
Epoch 1790, val loss: 1.1044222116470337
Epoch 1800, training loss: 12.502389907836914 = 0.008491667918860912 + 2.0 * 6.246949195861816
Epoch 1800, val loss: 1.1066557168960571
Epoch 1810, training loss: 12.500852584838867 = 0.008368821814656258 + 2.0 * 6.246242046356201
Epoch 1810, val loss: 1.1089463233947754
Epoch 1820, training loss: 12.503168106079102 = 0.008248157799243927 + 2.0 * 6.247459888458252
Epoch 1820, val loss: 1.111174464225769
Epoch 1830, training loss: 12.503490447998047 = 0.008129717782139778 + 2.0 * 6.247680187225342
Epoch 1830, val loss: 1.113318920135498
Epoch 1840, training loss: 12.506135940551758 = 0.008015068247914314 + 2.0 * 6.24906063079834
Epoch 1840, val loss: 1.1153590679168701
Epoch 1850, training loss: 12.500836372375488 = 0.007901700213551521 + 2.0 * 6.246467113494873
Epoch 1850, val loss: 1.1174941062927246
Epoch 1860, training loss: 12.49728775024414 = 0.007792693097144365 + 2.0 * 6.244747638702393
Epoch 1860, val loss: 1.1194913387298584
Epoch 1870, training loss: 12.502345085144043 = 0.007685998920351267 + 2.0 * 6.2473297119140625
Epoch 1870, val loss: 1.121674656867981
Epoch 1880, training loss: 12.50484561920166 = 0.007580493576824665 + 2.0 * 6.248632431030273
Epoch 1880, val loss: 1.1237751245498657
Epoch 1890, training loss: 12.496628761291504 = 0.007476998958736658 + 2.0 * 6.2445759773254395
Epoch 1890, val loss: 1.1257613897323608
Epoch 1900, training loss: 12.494246482849121 = 0.007376811932772398 + 2.0 * 6.243434906005859
Epoch 1900, val loss: 1.1276448965072632
Epoch 1910, training loss: 12.493489265441895 = 0.007279381155967712 + 2.0 * 6.243104934692383
Epoch 1910, val loss: 1.1297653913497925
Epoch 1920, training loss: 12.503631591796875 = 0.007182839326560497 + 2.0 * 6.248224258422852
Epoch 1920, val loss: 1.131912112236023
Epoch 1930, training loss: 12.494817733764648 = 0.007087751291692257 + 2.0 * 6.243865013122559
Epoch 1930, val loss: 1.1339383125305176
Epoch 1940, training loss: 12.494647979736328 = 0.006996580399572849 + 2.0 * 6.243825912475586
Epoch 1940, val loss: 1.1356003284454346
Epoch 1950, training loss: 12.493837356567383 = 0.006906714756041765 + 2.0 * 6.243465423583984
Epoch 1950, val loss: 1.1376498937606812
Epoch 1960, training loss: 12.490217208862305 = 0.006818388123065233 + 2.0 * 6.24169921875
Epoch 1960, val loss: 1.1396147012710571
Epoch 1970, training loss: 12.494766235351562 = 0.006731734145432711 + 2.0 * 6.244017124176025
Epoch 1970, val loss: 1.141577959060669
Epoch 1980, training loss: 12.490560531616211 = 0.006646431516855955 + 2.0 * 6.241957187652588
Epoch 1980, val loss: 1.1434828042984009
Epoch 1990, training loss: 12.49671745300293 = 0.006563954520970583 + 2.0 * 6.245076656341553
Epoch 1990, val loss: 1.145171880722046
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7925925925925926
0.8286768581971534
The final CL Acc:0.78148, 0.00907, The final GNN Acc:0.83641, 0.00548
Begin epxeriment: cont_weight: 2 epoch:2000 der1:0.3 der2:0.2 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11598])
remove edge: torch.Size([2, 9470])
updated graph: torch.Size([2, 10512])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 19.14875030517578 = 1.9550411701202393 + 2.0 * 8.596854209899902
Epoch 0, val loss: 1.9602267742156982
Epoch 10, training loss: 19.13751983642578 = 1.9442987442016602 + 2.0 * 8.596610069274902
Epoch 10, val loss: 1.9486805200576782
Epoch 20, training loss: 19.120229721069336 = 1.9310014247894287 + 2.0 * 8.594614028930664
Epoch 20, val loss: 1.9340684413909912
Epoch 30, training loss: 19.071645736694336 = 1.9126883745193481 + 2.0 * 8.57947826385498
Epoch 30, val loss: 1.913641333580017
Epoch 40, training loss: 18.916973114013672 = 1.8887940645217896 + 2.0 * 8.514089584350586
Epoch 40, val loss: 1.887389063835144
Epoch 50, training loss: 18.325286865234375 = 1.8649156093597412 + 2.0 * 8.230185508728027
Epoch 50, val loss: 1.8612585067749023
Epoch 60, training loss: 17.82634735107422 = 1.839297890663147 + 2.0 * 7.993524551391602
Epoch 60, val loss: 1.835795521736145
Epoch 70, training loss: 17.3151912689209 = 1.8196337223052979 + 2.0 * 7.747778415679932
Epoch 70, val loss: 1.8174971342086792
Epoch 80, training loss: 16.5559024810791 = 1.8062278032302856 + 2.0 * 7.3748369216918945
Epoch 80, val loss: 1.806016206741333
Epoch 90, training loss: 15.954277992248535 = 1.797873616218567 + 2.0 * 7.078202247619629
Epoch 90, val loss: 1.798636794090271
Epoch 100, training loss: 15.601987838745117 = 1.7886372804641724 + 2.0 * 6.906675338745117
Epoch 100, val loss: 1.7900060415267944
Epoch 110, training loss: 15.395378112792969 = 1.7781022787094116 + 2.0 * 6.808638095855713
Epoch 110, val loss: 1.7795554399490356
Epoch 120, training loss: 15.254573822021484 = 1.7670550346374512 + 2.0 * 6.743759632110596
Epoch 120, val loss: 1.7684991359710693
Epoch 130, training loss: 15.154186248779297 = 1.7559936046600342 + 2.0 * 6.699096202850342
Epoch 130, val loss: 1.7579104900360107
Epoch 140, training loss: 15.07015609741211 = 1.7439827919006348 + 2.0 * 6.663086414337158
Epoch 140, val loss: 1.7467881441116333
Epoch 150, training loss: 14.98957633972168 = 1.7309218645095825 + 2.0 * 6.629327297210693
Epoch 150, val loss: 1.7355213165283203
Epoch 160, training loss: 14.90927505493164 = 1.717012882232666 + 2.0 * 6.596130847930908
Epoch 160, val loss: 1.7238047122955322
Epoch 170, training loss: 14.837562561035156 = 1.7018712759017944 + 2.0 * 6.567845821380615
Epoch 170, val loss: 1.7111313343048096
Epoch 180, training loss: 14.764701843261719 = 1.6847947835922241 + 2.0 * 6.539953708648682
Epoch 180, val loss: 1.6968621015548706
Epoch 190, training loss: 14.701480865478516 = 1.6654103994369507 + 2.0 * 6.518035411834717
Epoch 190, val loss: 1.6807481050491333
Epoch 200, training loss: 14.650103569030762 = 1.6433199644088745 + 2.0 * 6.503391742706299
Epoch 200, val loss: 1.662351131439209
Epoch 210, training loss: 14.59580135345459 = 1.618323564529419 + 2.0 * 6.488739013671875
Epoch 210, val loss: 1.6416441202163696
Epoch 220, training loss: 14.545950889587402 = 1.5902732610702515 + 2.0 * 6.47783899307251
Epoch 220, val loss: 1.6185400485992432
Epoch 230, training loss: 14.496499061584473 = 1.5592210292816162 + 2.0 * 6.468638896942139
Epoch 230, val loss: 1.5932600498199463
Epoch 240, training loss: 14.443197250366211 = 1.5253735780715942 + 2.0 * 6.458911895751953
Epoch 240, val loss: 1.5659006834030151
Epoch 250, training loss: 14.390751838684082 = 1.4890124797821045 + 2.0 * 6.450869560241699
Epoch 250, val loss: 1.5368140935897827
Epoch 260, training loss: 14.334660530090332 = 1.4505926370620728 + 2.0 * 6.442033767700195
Epoch 260, val loss: 1.5064455270767212
Epoch 270, training loss: 14.277933120727539 = 1.410258173942566 + 2.0 * 6.433837413787842
Epoch 270, val loss: 1.4749373197555542
Epoch 280, training loss: 14.220725059509277 = 1.3681288957595825 + 2.0 * 6.426298141479492
Epoch 280, val loss: 1.4424397945404053
Epoch 290, training loss: 14.167234420776367 = 1.3246039152145386 + 2.0 * 6.4213151931762695
Epoch 290, val loss: 1.409232258796692
Epoch 300, training loss: 14.110036849975586 = 1.2809712886810303 + 2.0 * 6.414532661437988
Epoch 300, val loss: 1.3765300512313843
Epoch 310, training loss: 14.055635452270508 = 1.2377877235412598 + 2.0 * 6.408923625946045
Epoch 310, val loss: 1.3443994522094727
Epoch 320, training loss: 13.999069213867188 = 1.19461190700531 + 2.0 * 6.402228832244873
Epoch 320, val loss: 1.3126941919326782
Epoch 330, training loss: 13.949495315551758 = 1.1515535116195679 + 2.0 * 6.398971080780029
Epoch 330, val loss: 1.2813538312911987
Epoch 340, training loss: 13.897936820983887 = 1.1095291376113892 + 2.0 * 6.3942036628723145
Epoch 340, val loss: 1.25069260597229
Epoch 350, training loss: 13.845590591430664 = 1.0683083534240723 + 2.0 * 6.388641357421875
Epoch 350, val loss: 1.2210819721221924
Epoch 360, training loss: 13.795262336730957 = 1.0281168222427368 + 2.0 * 6.383572578430176
Epoch 360, val loss: 1.1923027038574219
Epoch 370, training loss: 13.749156951904297 = 0.9889995455741882 + 2.0 * 6.3800787925720215
Epoch 370, val loss: 1.1643128395080566
Epoch 380, training loss: 13.7098388671875 = 0.9508925080299377 + 2.0 * 6.3794732093811035
Epoch 380, val loss: 1.1373223066329956
Epoch 390, training loss: 13.6593599319458 = 0.9139907956123352 + 2.0 * 6.372684478759766
Epoch 390, val loss: 1.111457109451294
Epoch 400, training loss: 13.61824893951416 = 0.8783929347991943 + 2.0 * 6.369927883148193
Epoch 400, val loss: 1.0867657661437988
Epoch 410, training loss: 13.579380989074707 = 0.844103217124939 + 2.0 * 6.367639064788818
Epoch 410, val loss: 1.0632476806640625
Epoch 420, training loss: 13.53736400604248 = 0.8112456202507019 + 2.0 * 6.363059043884277
Epoch 420, val loss: 1.0410362482070923
Epoch 430, training loss: 13.49917221069336 = 0.7797272801399231 + 2.0 * 6.35972261428833
Epoch 430, val loss: 1.0203087329864502
Epoch 440, training loss: 13.464950561523438 = 0.7495293617248535 + 2.0 * 6.357710361480713
Epoch 440, val loss: 1.000993013381958
Epoch 450, training loss: 13.432915687561035 = 0.7208212018013 + 2.0 * 6.3560471534729
Epoch 450, val loss: 0.9831423163414001
Epoch 460, training loss: 13.395681381225586 = 0.6937119364738464 + 2.0 * 6.350984573364258
Epoch 460, val loss: 0.9668140411376953
Epoch 470, training loss: 13.366488456726074 = 0.6678556799888611 + 2.0 * 6.349316596984863
Epoch 470, val loss: 0.9520787000656128
Epoch 480, training loss: 13.33672046661377 = 0.643283486366272 + 2.0 * 6.3467183113098145
Epoch 480, val loss: 0.9387806057929993
Epoch 490, training loss: 13.3111572265625 = 0.6200113892555237 + 2.0 * 6.3455729484558105
Epoch 490, val loss: 0.9268497824668884
Epoch 500, training loss: 13.280967712402344 = 0.5979226231575012 + 2.0 * 6.341522693634033
Epoch 500, val loss: 0.9162505269050598
Epoch 510, training loss: 13.253615379333496 = 0.5768706798553467 + 2.0 * 6.338372230529785
Epoch 510, val loss: 0.9067586660385132
Epoch 520, training loss: 13.235150337219238 = 0.55668044090271 + 2.0 * 6.339234828948975
Epoch 520, val loss: 0.8984116911888123
Epoch 530, training loss: 13.216119766235352 = 0.5374231338500977 + 2.0 * 6.339348316192627
Epoch 530, val loss: 0.891044020652771
Epoch 540, training loss: 13.186727523803711 = 0.5190364122390747 + 2.0 * 6.333845615386963
Epoch 540, val loss: 0.8845813870429993
Epoch 550, training loss: 13.162734985351562 = 0.5012199878692627 + 2.0 * 6.3307576179504395
Epoch 550, val loss: 0.8788093328475952
Epoch 560, training loss: 13.141721725463867 = 0.4837692081928253 + 2.0 * 6.328976154327393
Epoch 560, val loss: 0.8735801577568054
Epoch 570, training loss: 13.12115478515625 = 0.4666764736175537 + 2.0 * 6.327239036560059
Epoch 570, val loss: 0.8688273429870605
Epoch 580, training loss: 13.10311222076416 = 0.4499352276325226 + 2.0 * 6.3265886306762695
Epoch 580, val loss: 0.8643753528594971
Epoch 590, training loss: 13.081576347351074 = 0.43335768580436707 + 2.0 * 6.3241095542907715
Epoch 590, val loss: 0.8602651953697205
Epoch 600, training loss: 13.062743186950684 = 0.4168262779712677 + 2.0 * 6.322958469390869
Epoch 600, val loss: 0.8564832806587219
Epoch 610, training loss: 13.045392990112305 = 0.40052366256713867 + 2.0 * 6.322434902191162
Epoch 610, val loss: 0.8529551029205322
Epoch 620, training loss: 13.022777557373047 = 0.384397953748703 + 2.0 * 6.31919002532959
Epoch 620, val loss: 0.8497571349143982
Epoch 630, training loss: 13.003385543823242 = 0.3684152066707611 + 2.0 * 6.317485332489014
Epoch 630, val loss: 0.8469918966293335
Epoch 640, training loss: 12.992656707763672 = 0.3525770604610443 + 2.0 * 6.320039749145508
Epoch 640, val loss: 0.8446576595306396
Epoch 650, training loss: 12.974390983581543 = 0.3369717001914978 + 2.0 * 6.318709850311279
Epoch 650, val loss: 0.84278804063797
Epoch 660, training loss: 12.950146675109863 = 0.321734756231308 + 2.0 * 6.314206123352051
Epoch 660, val loss: 0.8413025736808777
Epoch 670, training loss: 12.930313110351562 = 0.30679821968078613 + 2.0 * 6.311757564544678
Epoch 670, val loss: 0.8403597474098206
Epoch 680, training loss: 12.91408920288086 = 0.29223453998565674 + 2.0 * 6.310927391052246
Epoch 680, val loss: 0.8399368524551392
Epoch 690, training loss: 12.918673515319824 = 0.2781127393245697 + 2.0 * 6.3202805519104
Epoch 690, val loss: 0.8400802612304688
Epoch 700, training loss: 12.885689735412598 = 0.2646656632423401 + 2.0 * 6.310512065887451
Epoch 700, val loss: 0.8406543731689453
Epoch 710, training loss: 12.867399215698242 = 0.25175535678863525 + 2.0 * 6.307821750640869
Epoch 710, val loss: 0.841645359992981
Epoch 720, training loss: 12.8511323928833 = 0.23938219249248505 + 2.0 * 6.305875301361084
Epoch 720, val loss: 0.8431905508041382
Epoch 730, training loss: 12.83803653717041 = 0.22754313051700592 + 2.0 * 6.305246829986572
Epoch 730, val loss: 0.8451884388923645
Epoch 740, training loss: 12.827986717224121 = 0.21627528965473175 + 2.0 * 6.305855751037598
Epoch 740, val loss: 0.8476721048355103
Epoch 750, training loss: 12.818949699401855 = 0.20561760663986206 + 2.0 * 6.306665897369385
Epoch 750, val loss: 0.8504279851913452
Epoch 760, training loss: 12.801552772521973 = 0.19561074674129486 + 2.0 * 6.302970886230469
Epoch 760, val loss: 0.8537367582321167
Epoch 770, training loss: 12.788858413696289 = 0.18613313138484955 + 2.0 * 6.30136251449585
Epoch 770, val loss: 0.8572551608085632
Epoch 780, training loss: 12.775510787963867 = 0.1771683245897293 + 2.0 * 6.299171447753906
Epoch 780, val loss: 0.8612801432609558
Epoch 790, training loss: 12.765556335449219 = 0.16864870488643646 + 2.0 * 6.2984538078308105
Epoch 790, val loss: 0.86556077003479
Epoch 800, training loss: 12.75898551940918 = 0.16061624884605408 + 2.0 * 6.299184799194336
Epoch 800, val loss: 0.8702328205108643
Epoch 810, training loss: 12.750602722167969 = 0.15308742225170135 + 2.0 * 6.298757553100586
Epoch 810, val loss: 0.8748819231987
Epoch 820, training loss: 12.737942695617676 = 0.1459907740354538 + 2.0 * 6.295976161956787
Epoch 820, val loss: 0.8799087405204773
Epoch 830, training loss: 12.731221199035645 = 0.13924969732761383 + 2.0 * 6.295985698699951
Epoch 830, val loss: 0.8852198123931885
Epoch 840, training loss: 12.718384742736816 = 0.13287201523780823 + 2.0 * 6.2927565574646
Epoch 840, val loss: 0.8906933069229126
Epoch 850, training loss: 12.710919380187988 = 0.1268395483493805 + 2.0 * 6.29203987121582
Epoch 850, val loss: 0.8962803483009338
Epoch 860, training loss: 12.707074165344238 = 0.1211220994591713 + 2.0 * 6.292975902557373
Epoch 860, val loss: 0.902006983757019
Epoch 870, training loss: 12.698248863220215 = 0.11572142690420151 + 2.0 * 6.291263580322266
Epoch 870, val loss: 0.9079770445823669
Epoch 880, training loss: 12.69205093383789 = 0.11062940210103989 + 2.0 * 6.290710926055908
Epoch 880, val loss: 0.9137676954269409
Epoch 890, training loss: 12.68232536315918 = 0.10579821467399597 + 2.0 * 6.28826379776001
Epoch 890, val loss: 0.919863224029541
Epoch 900, training loss: 12.679298400878906 = 0.10120260715484619 + 2.0 * 6.289047718048096
Epoch 900, val loss: 0.926032543182373
Epoch 910, training loss: 12.672029495239258 = 0.09683957695960999 + 2.0 * 6.287594795227051
Epoch 910, val loss: 0.9322906732559204
Epoch 920, training loss: 12.667891502380371 = 0.09272194653749466 + 2.0 * 6.2875847816467285
Epoch 920, val loss: 0.9384245276451111
Epoch 930, training loss: 12.65992546081543 = 0.08881683647632599 + 2.0 * 6.2855544090271
Epoch 930, val loss: 0.9446344971656799
Epoch 940, training loss: 12.660304069519043 = 0.08510515838861465 + 2.0 * 6.287599563598633
Epoch 940, val loss: 0.9510285258293152
Epoch 950, training loss: 12.648246765136719 = 0.08157853782176971 + 2.0 * 6.283334255218506
Epoch 950, val loss: 0.9572972059249878
Epoch 960, training loss: 12.642879486083984 = 0.07823946326971054 + 2.0 * 6.282320022583008
Epoch 960, val loss: 0.9635342955589294
Epoch 970, training loss: 12.63917064666748 = 0.07506066560745239 + 2.0 * 6.282054901123047
Epoch 970, val loss: 0.9698538184165955
Epoch 980, training loss: 12.652849197387695 = 0.07203330099582672 + 2.0 * 6.290408134460449
Epoch 980, val loss: 0.9761207699775696
Epoch 990, training loss: 12.634267807006836 = 0.06919155269861221 + 2.0 * 6.282537937164307
Epoch 990, val loss: 0.9824331998825073
Epoch 1000, training loss: 12.628060340881348 = 0.06648925691843033 + 2.0 * 6.28078556060791
Epoch 1000, val loss: 0.9887481927871704
Epoch 1010, training loss: 12.622262954711914 = 0.06392008811235428 + 2.0 * 6.279171466827393
Epoch 1010, val loss: 0.995000422000885
Epoch 1020, training loss: 12.618499755859375 = 0.06147528439760208 + 2.0 * 6.278512001037598
Epoch 1020, val loss: 1.001313328742981
Epoch 1030, training loss: 12.630111694335938 = 0.0591537207365036 + 2.0 * 6.2854790687561035
Epoch 1030, val loss: 1.0076630115509033
Epoch 1040, training loss: 12.613985061645508 = 0.056943487375974655 + 2.0 * 6.278520584106445
Epoch 1040, val loss: 1.0137830972671509
Epoch 1050, training loss: 12.607799530029297 = 0.05486171692609787 + 2.0 * 6.276468753814697
Epoch 1050, val loss: 1.01993727684021
Epoch 1060, training loss: 12.604118347167969 = 0.0528741292655468 + 2.0 * 6.2756218910217285
Epoch 1060, val loss: 1.0261328220367432
Epoch 1070, training loss: 12.61035442352295 = 0.050981294363737106 + 2.0 * 6.279686450958252
Epoch 1070, val loss: 1.0323388576507568
Epoch 1080, training loss: 12.601741790771484 = 0.049176182597875595 + 2.0 * 6.276282787322998
Epoch 1080, val loss: 1.0382903814315796
Epoch 1090, training loss: 12.59490966796875 = 0.04746809974312782 + 2.0 * 6.273720741271973
Epoch 1090, val loss: 1.0444257259368896
Epoch 1100, training loss: 12.59107780456543 = 0.04583597183227539 + 2.0 * 6.272621154785156
Epoch 1100, val loss: 1.0504376888275146
Epoch 1110, training loss: 12.600968360900879 = 0.04427589103579521 + 2.0 * 6.278346061706543
Epoch 1110, val loss: 1.0564724206924438
Epoch 1120, training loss: 12.588960647583008 = 0.042810115963220596 + 2.0 * 6.273075103759766
Epoch 1120, val loss: 1.0624849796295166
Epoch 1130, training loss: 12.588045120239258 = 0.041401203721761703 + 2.0 * 6.273322105407715
Epoch 1130, val loss: 1.068350076675415
Epoch 1140, training loss: 12.580249786376953 = 0.04005832225084305 + 2.0 * 6.2700958251953125
Epoch 1140, val loss: 1.074256181716919
Epoch 1150, training loss: 12.579975128173828 = 0.038776010274887085 + 2.0 * 6.270599365234375
Epoch 1150, val loss: 1.0800225734710693
Epoch 1160, training loss: 12.577442169189453 = 0.03754859045147896 + 2.0 * 6.269946575164795
Epoch 1160, val loss: 1.085850715637207
Epoch 1170, training loss: 12.57688045501709 = 0.036381881684064865 + 2.0 * 6.270249366760254
Epoch 1170, val loss: 1.091599702835083
Epoch 1180, training loss: 12.571922302246094 = 0.035265643149614334 + 2.0 * 6.2683281898498535
Epoch 1180, val loss: 1.0973384380340576
Epoch 1190, training loss: 12.57324504852295 = 0.03420158103108406 + 2.0 * 6.269521713256836
Epoch 1190, val loss: 1.1028605699539185
Epoch 1200, training loss: 12.566865921020508 = 0.03318427503108978 + 2.0 * 6.266840934753418
Epoch 1200, val loss: 1.1084271669387817
Epoch 1210, training loss: 12.563033103942871 = 0.032211653888225555 + 2.0 * 6.265410900115967
Epoch 1210, val loss: 1.113958716392517
Epoch 1220, training loss: 12.569376945495605 = 0.03127533197402954 + 2.0 * 6.269050598144531
Epoch 1220, val loss: 1.1194473505020142
Epoch 1230, training loss: 12.561136245727539 = 0.030377639457583427 + 2.0 * 6.265379428863525
Epoch 1230, val loss: 1.1247743368148804
Epoch 1240, training loss: 12.566632270812988 = 0.02952008694410324 + 2.0 * 6.268556118011475
Epoch 1240, val loss: 1.1300445795059204
Epoch 1250, training loss: 12.555746078491211 = 0.028705747798085213 + 2.0 * 6.263520240783691
Epoch 1250, val loss: 1.1354342699050903
Epoch 1260, training loss: 12.553269386291504 = 0.02792220190167427 + 2.0 * 6.262673377990723
Epoch 1260, val loss: 1.140589714050293
Epoch 1270, training loss: 12.55112361907959 = 0.02716725692152977 + 2.0 * 6.2619781494140625
Epoch 1270, val loss: 1.1457855701446533
Epoch 1280, training loss: 12.555100440979004 = 0.026439255103468895 + 2.0 * 6.2643303871154785
Epoch 1280, val loss: 1.1509126424789429
Epoch 1290, training loss: 12.552403450012207 = 0.02574852854013443 + 2.0 * 6.263327598571777
Epoch 1290, val loss: 1.156269907951355
Epoch 1300, training loss: 12.549436569213867 = 0.02507838048040867 + 2.0 * 6.262178897857666
Epoch 1300, val loss: 1.1610482931137085
Epoch 1310, training loss: 12.543481826782227 = 0.024441437795758247 + 2.0 * 6.259520053863525
Epoch 1310, val loss: 1.1661542654037476
Epoch 1320, training loss: 12.54714584350586 = 0.023823702707886696 + 2.0 * 6.261661052703857
Epoch 1320, val loss: 1.171109914779663
Epoch 1330, training loss: 12.541040420532227 = 0.02322828769683838 + 2.0 * 6.25890588760376
Epoch 1330, val loss: 1.1760263442993164
Epoch 1340, training loss: 12.540353775024414 = 0.022660400718450546 + 2.0 * 6.258846759796143
Epoch 1340, val loss: 1.1808874607086182
Epoch 1350, training loss: 12.545167922973633 = 0.02211182191967964 + 2.0 * 6.261528015136719
Epoch 1350, val loss: 1.1857105493545532
Epoch 1360, training loss: 12.535572052001953 = 0.021579697728157043 + 2.0 * 6.256996154785156
Epoch 1360, val loss: 1.190416693687439
Epoch 1370, training loss: 12.535688400268555 = 0.021067718043923378 + 2.0 * 6.257310390472412
Epoch 1370, val loss: 1.1950985193252563
Epoch 1380, training loss: 12.534754753112793 = 0.020573746412992477 + 2.0 * 6.2570905685424805
Epoch 1380, val loss: 1.1998145580291748
Epoch 1390, training loss: 12.540752410888672 = 0.020098282024264336 + 2.0 * 6.260326862335205
Epoch 1390, val loss: 1.2045036554336548
Epoch 1400, training loss: 12.538625717163086 = 0.019642746075987816 + 2.0 * 6.259491443634033
Epoch 1400, val loss: 1.2092313766479492
Epoch 1410, training loss: 12.531335830688477 = 0.01920349895954132 + 2.0 * 6.25606632232666
Epoch 1410, val loss: 1.21354079246521
Epoch 1420, training loss: 12.528030395507812 = 0.018781647086143494 + 2.0 * 6.254624366760254
Epoch 1420, val loss: 1.2180149555206299
Epoch 1430, training loss: 12.525734901428223 = 0.01836964674293995 + 2.0 * 6.253682613372803
Epoch 1430, val loss: 1.2224671840667725
Epoch 1440, training loss: 12.530354499816895 = 0.017969578504562378 + 2.0 * 6.256192684173584
Epoch 1440, val loss: 1.226814866065979
Epoch 1450, training loss: 12.523338317871094 = 0.01758444868028164 + 2.0 * 6.2528767585754395
Epoch 1450, val loss: 1.2313766479492188
Epoch 1460, training loss: 12.522796630859375 = 0.017211653292179108 + 2.0 * 6.2527923583984375
Epoch 1460, val loss: 1.2355639934539795
Epoch 1470, training loss: 12.531591415405273 = 0.016850458458065987 + 2.0 * 6.257370471954346
Epoch 1470, val loss: 1.239914059638977
Epoch 1480, training loss: 12.520573616027832 = 0.01650444231927395 + 2.0 * 6.252034664154053
Epoch 1480, val loss: 1.2441447973251343
Epoch 1490, training loss: 12.518471717834473 = 0.0161681417375803 + 2.0 * 6.2511515617370605
Epoch 1490, val loss: 1.2484033107757568
Epoch 1500, training loss: 12.51806640625 = 0.015840085223317146 + 2.0 * 6.251112937927246
Epoch 1500, val loss: 1.2525323629379272
Epoch 1510, training loss: 12.531177520751953 = 0.015523979440331459 + 2.0 * 6.257826805114746
Epoch 1510, val loss: 1.2567416429519653
Epoch 1520, training loss: 12.524924278259277 = 0.01521484088152647 + 2.0 * 6.254854679107666
Epoch 1520, val loss: 1.260692834854126
Epoch 1530, training loss: 12.516013145446777 = 0.01492058765143156 + 2.0 * 6.250546455383301
Epoch 1530, val loss: 1.264735460281372
Epoch 1540, training loss: 12.512779235839844 = 0.014634354040026665 + 2.0 * 6.249072551727295
Epoch 1540, val loss: 1.268660068511963
Epoch 1550, training loss: 12.513206481933594 = 0.014355136081576347 + 2.0 * 6.249425888061523
Epoch 1550, val loss: 1.272646427154541
Epoch 1560, training loss: 12.529098510742188 = 0.014085463248193264 + 2.0 * 6.257506370544434
Epoch 1560, val loss: 1.2766544818878174
Epoch 1570, training loss: 12.515207290649414 = 0.013819881714880466 + 2.0 * 6.2506937980651855
Epoch 1570, val loss: 1.2803385257720947
Epoch 1580, training loss: 12.510794639587402 = 0.013566900976002216 + 2.0 * 6.2486138343811035
Epoch 1580, val loss: 1.2841635942459106
Epoch 1590, training loss: 12.515511512756348 = 0.013319006189703941 + 2.0 * 6.251096248626709
Epoch 1590, val loss: 1.287902593612671
Epoch 1600, training loss: 12.507433891296387 = 0.013078730553388596 + 2.0 * 6.247177600860596
Epoch 1600, val loss: 1.2917476892471313
Epoch 1610, training loss: 12.510629653930664 = 0.01284567266702652 + 2.0 * 6.248891830444336
Epoch 1610, val loss: 1.2954511642456055
Epoch 1620, training loss: 12.510895729064941 = 0.012617340311408043 + 2.0 * 6.249139308929443
Epoch 1620, val loss: 1.2990089654922485
Epoch 1630, training loss: 12.507523536682129 = 0.012397554703056812 + 2.0 * 6.247562885284424
Epoch 1630, val loss: 1.3026831150054932
Epoch 1640, training loss: 12.503615379333496 = 0.012184989638626575 + 2.0 * 6.245715141296387
Epoch 1640, val loss: 1.3063583374023438
Epoch 1650, training loss: 12.50442886352539 = 0.011976330541074276 + 2.0 * 6.2462263107299805
Epoch 1650, val loss: 1.310004472732544
Epoch 1660, training loss: 12.509716987609863 = 0.011773458682000637 + 2.0 * 6.248971939086914
Epoch 1660, val loss: 1.3136112689971924
Epoch 1670, training loss: 12.515337944030762 = 0.01157284900546074 + 2.0 * 6.251882553100586
Epoch 1670, val loss: 1.3168308734893799
Epoch 1680, training loss: 12.505179405212402 = 0.011381948366761208 + 2.0 * 6.246898651123047
Epoch 1680, val loss: 1.3206090927124023
Epoch 1690, training loss: 12.500752449035645 = 0.01119618397206068 + 2.0 * 6.244778156280518
Epoch 1690, val loss: 1.323801040649414
Epoch 1700, training loss: 12.49935245513916 = 0.011013886891305447 + 2.0 * 6.244169235229492
Epoch 1700, val loss: 1.327330470085144
Epoch 1710, training loss: 12.51527214050293 = 0.010836925357580185 + 2.0 * 6.252217769622803
Epoch 1710, val loss: 1.3306488990783691
Epoch 1720, training loss: 12.501205444335938 = 0.010663132183253765 + 2.0 * 6.2452712059021
Epoch 1720, val loss: 1.3340229988098145
Epoch 1730, training loss: 12.496491432189941 = 0.010495132766664028 + 2.0 * 6.242998123168945
Epoch 1730, val loss: 1.3373080492019653
Epoch 1740, training loss: 12.494990348815918 = 0.010330598801374435 + 2.0 * 6.242330074310303
Epoch 1740, val loss: 1.3406243324279785
Epoch 1750, training loss: 12.510627746582031 = 0.010169481858611107 + 2.0 * 6.250229358673096
Epoch 1750, val loss: 1.3437448740005493
Epoch 1760, training loss: 12.498979568481445 = 0.010011774487793446 + 2.0 * 6.244483947753906
Epoch 1760, val loss: 1.3472551107406616
Epoch 1770, training loss: 12.500762939453125 = 0.0098593570291996 + 2.0 * 6.245451927185059
Epoch 1770, val loss: 1.3502991199493408
Epoch 1780, training loss: 12.491602897644043 = 0.009712589904665947 + 2.0 * 6.240945339202881
Epoch 1780, val loss: 1.3535386323928833
Epoch 1790, training loss: 12.491929054260254 = 0.009568719193339348 + 2.0 * 6.241179943084717
Epoch 1790, val loss: 1.356772541999817
Epoch 1800, training loss: 12.493202209472656 = 0.009427113458514214 + 2.0 * 6.24188756942749
Epoch 1800, val loss: 1.3599282503128052
Epoch 1810, training loss: 12.507915496826172 = 0.009288850240409374 + 2.0 * 6.2493133544921875
Epoch 1810, val loss: 1.3631359338760376
Epoch 1820, training loss: 12.491015434265137 = 0.009152497164905071 + 2.0 * 6.240931510925293
Epoch 1820, val loss: 1.3659871816635132
Epoch 1830, training loss: 12.488577842712402 = 0.00902322307229042 + 2.0 * 6.239777088165283
Epoch 1830, val loss: 1.3691189289093018
Epoch 1840, training loss: 12.487003326416016 = 0.008894645608961582 + 2.0 * 6.239054203033447
Epoch 1840, val loss: 1.3720921277999878
Epoch 1850, training loss: 12.486662864685059 = 0.008767811581492424 + 2.0 * 6.23894739151001
Epoch 1850, val loss: 1.3750920295715332
Epoch 1860, training loss: 12.51600456237793 = 0.008644957095384598 + 2.0 * 6.2536797523498535
Epoch 1860, val loss: 1.3779562711715698
Epoch 1870, training loss: 12.490490913391113 = 0.008525100536644459 + 2.0 * 6.240983009338379
Epoch 1870, val loss: 1.381138801574707
Epoch 1880, training loss: 12.48667049407959 = 0.008408567868173122 + 2.0 * 6.239130973815918
Epoch 1880, val loss: 1.3839731216430664
Epoch 1890, training loss: 12.483670234680176 = 0.008294652216136456 + 2.0 * 6.237687587738037
Epoch 1890, val loss: 1.386827826499939
Epoch 1900, training loss: 12.482855796813965 = 0.008181976154446602 + 2.0 * 6.237337112426758
Epoch 1900, val loss: 1.3897467851638794
Epoch 1910, training loss: 12.490394592285156 = 0.008071301504969597 + 2.0 * 6.241161823272705
Epoch 1910, val loss: 1.3926466703414917
Epoch 1920, training loss: 12.493514060974121 = 0.007962457835674286 + 2.0 * 6.242775917053223
Epoch 1920, val loss: 1.3954566717147827
Epoch 1930, training loss: 12.488430976867676 = 0.007860749028623104 + 2.0 * 6.2402849197387695
Epoch 1930, val loss: 1.3982545137405396
Epoch 1940, training loss: 12.480685234069824 = 0.007759542670100927 + 2.0 * 6.2364630699157715
Epoch 1940, val loss: 1.4009995460510254
Epoch 1950, training loss: 12.480396270751953 = 0.007659537717700005 + 2.0 * 6.236368179321289
Epoch 1950, val loss: 1.4037426710128784
Epoch 1960, training loss: 12.479373931884766 = 0.00756053626537323 + 2.0 * 6.235906600952148
Epoch 1960, val loss: 1.406516671180725
Epoch 1970, training loss: 12.484946250915527 = 0.0074631026946008205 + 2.0 * 6.238741397857666
Epoch 1970, val loss: 1.4091589450836182
Epoch 1980, training loss: 12.483803749084473 = 0.0073676579631865025 + 2.0 * 6.238217830657959
Epoch 1980, val loss: 1.4119781255722046
Epoch 1990, training loss: 12.479764938354492 = 0.0072765774093568325 + 2.0 * 6.236244201660156
Epoch 1990, val loss: 1.4146020412445068
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7740740740740741
0.8075909330521878
=== training gcn model ===
Epoch 0, training loss: 19.14423942565918 = 1.9506127834320068 + 2.0 * 8.596813201904297
Epoch 0, val loss: 1.9595314264297485
Epoch 10, training loss: 19.132972717285156 = 1.9401607513427734 + 2.0 * 8.596405982971191
Epoch 10, val loss: 1.948827862739563
Epoch 20, training loss: 19.11336326599121 = 1.9270694255828857 + 2.0 * 8.593147277832031
Epoch 20, val loss: 1.9353011846542358
Epoch 30, training loss: 19.045541763305664 = 1.9090027809143066 + 2.0 * 8.568269729614258
Epoch 30, val loss: 1.9166728258132935
Epoch 40, training loss: 18.7022647857666 = 1.8876030445098877 + 2.0 * 8.407330513000488
Epoch 40, val loss: 1.8954890966415405
Epoch 50, training loss: 17.719942092895508 = 1.8654521703720093 + 2.0 * 7.927245140075684
Epoch 50, val loss: 1.873593807220459
Epoch 60, training loss: 16.73688316345215 = 1.8500570058822632 + 2.0 * 7.443412780761719
Epoch 60, val loss: 1.8592827320098877
Epoch 70, training loss: 15.871902465820312 = 1.84089994430542 + 2.0 * 7.015501499176025
Epoch 70, val loss: 1.8505464792251587
Epoch 80, training loss: 15.487195014953613 = 1.8336960077285767 + 2.0 * 6.826749324798584
Epoch 80, val loss: 1.8431172370910645
Epoch 90, training loss: 15.282682418823242 = 1.8233872652053833 + 2.0 * 6.729647636413574
Epoch 90, val loss: 1.8332231044769287
Epoch 100, training loss: 15.129972457885742 = 1.8121846914291382 + 2.0 * 6.658894062042236
Epoch 100, val loss: 1.822874665260315
Epoch 110, training loss: 15.025872230529785 = 1.8019963502883911 + 2.0 * 6.611937999725342
Epoch 110, val loss: 1.8132489919662476
Epoch 120, training loss: 14.94310474395752 = 1.7926257848739624 + 2.0 * 6.575239658355713
Epoch 120, val loss: 1.8041741847991943
Epoch 130, training loss: 14.874359130859375 = 1.7832603454589844 + 2.0 * 6.545549392700195
Epoch 130, val loss: 1.795194387435913
Epoch 140, training loss: 14.818889617919922 = 1.7733403444290161 + 2.0 * 6.522774696350098
Epoch 140, val loss: 1.786060094833374
Epoch 150, training loss: 14.768195152282715 = 1.7626135349273682 + 2.0 * 6.502790927886963
Epoch 150, val loss: 1.776658058166504
Epoch 160, training loss: 14.722501754760742 = 1.7506978511810303 + 2.0 * 6.485901832580566
Epoch 160, val loss: 1.7666339874267578
Epoch 170, training loss: 14.679459571838379 = 1.7373809814453125 + 2.0 * 6.471039295196533
Epoch 170, val loss: 1.755697250366211
Epoch 180, training loss: 14.639549255371094 = 1.722358226776123 + 2.0 * 6.458595275878906
Epoch 180, val loss: 1.7435126304626465
Epoch 190, training loss: 14.598685264587402 = 1.705296277999878 + 2.0 * 6.446694374084473
Epoch 190, val loss: 1.7297834157943726
Epoch 200, training loss: 14.558589935302734 = 1.6857510805130005 + 2.0 * 6.436419486999512
Epoch 200, val loss: 1.7140949964523315
Epoch 210, training loss: 14.517330169677734 = 1.6633551120758057 + 2.0 * 6.426987648010254
Epoch 210, val loss: 1.6963121891021729
Epoch 220, training loss: 14.476423263549805 = 1.6383105516433716 + 2.0 * 6.419056415557861
Epoch 220, val loss: 1.6764682531356812
Epoch 230, training loss: 14.430532455444336 = 1.6098480224609375 + 2.0 * 6.410342216491699
Epoch 230, val loss: 1.6541194915771484
Epoch 240, training loss: 14.395281791687012 = 1.578242301940918 + 2.0 * 6.408519744873047
Epoch 240, val loss: 1.62930166721344
Epoch 250, training loss: 14.341829299926758 = 1.5443596839904785 + 2.0 * 6.3987345695495605
Epoch 250, val loss: 1.6026723384857178
Epoch 260, training loss: 14.290314674377441 = 1.5081441402435303 + 2.0 * 6.391085147857666
Epoch 260, val loss: 1.5745798349380493
Epoch 270, training loss: 14.240201950073242 = 1.4698001146316528 + 2.0 * 6.3852009773254395
Epoch 270, val loss: 1.5450853109359741
Epoch 280, training loss: 14.195060729980469 = 1.4297109842300415 + 2.0 * 6.382674694061279
Epoch 280, val loss: 1.5143179893493652
Epoch 290, training loss: 14.140885353088379 = 1.3892544507980347 + 2.0 * 6.375815391540527
Epoch 290, val loss: 1.4838581085205078
Epoch 300, training loss: 14.089479446411133 = 1.3491462469100952 + 2.0 * 6.370166778564453
Epoch 300, val loss: 1.4545305967330933
Epoch 310, training loss: 14.039809226989746 = 1.3095338344573975 + 2.0 * 6.365137577056885
Epoch 310, val loss: 1.426142930984497
Epoch 320, training loss: 13.999924659729004 = 1.2705190181732178 + 2.0 * 6.3647027015686035
Epoch 320, val loss: 1.3986939191818237
Epoch 330, training loss: 13.95174503326416 = 1.23272705078125 + 2.0 * 6.359508991241455
Epoch 330, val loss: 1.3729463815689087
Epoch 340, training loss: 13.906578063964844 = 1.196338176727295 + 2.0 * 6.355119705200195
Epoch 340, val loss: 1.3490536212921143
Epoch 350, training loss: 13.862162590026855 = 1.160581350326538 + 2.0 * 6.350790500640869
Epoch 350, val loss: 1.326069951057434
Epoch 360, training loss: 13.818899154663086 = 1.1252436637878418 + 2.0 * 6.346827507019043
Epoch 360, val loss: 1.3037739992141724
Epoch 370, training loss: 13.77910327911377 = 1.0901696681976318 + 2.0 * 6.344466686248779
Epoch 370, val loss: 1.2823145389556885
Epoch 380, training loss: 13.750797271728516 = 1.0557478666305542 + 2.0 * 6.347524642944336
Epoch 380, val loss: 1.2616486549377441
Epoch 390, training loss: 13.699491500854492 = 1.0219457149505615 + 2.0 * 6.338772773742676
Epoch 390, val loss: 1.241698145866394
Epoch 400, training loss: 13.66026496887207 = 0.9883902072906494 + 2.0 * 6.3359375
Epoch 400, val loss: 1.2220603227615356
Epoch 410, training loss: 13.639824867248535 = 0.9550773501396179 + 2.0 * 6.342373847961426
Epoch 410, val loss: 1.202749490737915
Epoch 420, training loss: 13.584346771240234 = 0.9221813678741455 + 2.0 * 6.331082820892334
Epoch 420, val loss: 1.1841413974761963
Epoch 430, training loss: 13.55224609375 = 0.8896954655647278 + 2.0 * 6.331275463104248
Epoch 430, val loss: 1.1658302545547485
Epoch 440, training loss: 13.51396656036377 = 0.8575349450111389 + 2.0 * 6.328215599060059
Epoch 440, val loss: 1.1477808952331543
Epoch 450, training loss: 13.476181983947754 = 0.8258074522018433 + 2.0 * 6.3251872062683105
Epoch 450, val loss: 1.1301239728927612
Epoch 460, training loss: 13.445845603942871 = 0.7943394184112549 + 2.0 * 6.325753211975098
Epoch 460, val loss: 1.112677812576294
Epoch 470, training loss: 13.40841293334961 = 0.7632986903190613 + 2.0 * 6.322556972503662
Epoch 470, val loss: 1.0956846475601196
Epoch 480, training loss: 13.373372077941895 = 0.7327700853347778 + 2.0 * 6.320301055908203
Epoch 480, val loss: 1.0790678262710571
Epoch 490, training loss: 13.341595649719238 = 0.7026389241218567 + 2.0 * 6.319478511810303
Epoch 490, val loss: 1.0627944469451904
Epoch 500, training loss: 13.307929992675781 = 0.673149585723877 + 2.0 * 6.317389965057373
Epoch 500, val loss: 1.0470398664474487
Epoch 510, training loss: 13.281495094299316 = 0.6442457437515259 + 2.0 * 6.318624496459961
Epoch 510, val loss: 1.0318330526351929
Epoch 520, training loss: 13.243293762207031 = 0.6161212921142578 + 2.0 * 6.313586235046387
Epoch 520, val loss: 1.017220377922058
Epoch 530, training loss: 13.210379600524902 = 0.5887370705604553 + 2.0 * 6.310821056365967
Epoch 530, val loss: 1.0032927989959717
Epoch 540, training loss: 13.199474334716797 = 0.562050998210907 + 2.0 * 6.318711757659912
Epoch 540, val loss: 0.9901263117790222
Epoch 550, training loss: 13.155277252197266 = 0.5363442897796631 + 2.0 * 6.309466361999512
Epoch 550, val loss: 0.9778155088424683
Epoch 560, training loss: 13.129481315612793 = 0.5116509199142456 + 2.0 * 6.308915138244629
Epoch 560, val loss: 0.9665293097496033
Epoch 570, training loss: 13.099743843078613 = 0.4879271686077118 + 2.0 * 6.305908203125
Epoch 570, val loss: 0.9563429355621338
Epoch 580, training loss: 13.086803436279297 = 0.4651794135570526 + 2.0 * 6.310811996459961
Epoch 580, val loss: 0.9471819996833801
Epoch 590, training loss: 13.048511505126953 = 0.4434526264667511 + 2.0 * 6.302529335021973
Epoch 590, val loss: 0.9391900897026062
Epoch 600, training loss: 13.02626895904541 = 0.42280614376068115 + 2.0 * 6.301731586456299
Epoch 600, val loss: 0.932325005531311
Epoch 610, training loss: 13.005660057067871 = 0.40308505296707153 + 2.0 * 6.301287651062012
Epoch 610, val loss: 0.9264299869537354
Epoch 620, training loss: 12.983975410461426 = 0.3843116760253906 + 2.0 * 6.299831867218018
Epoch 620, val loss: 0.921504557132721
Epoch 630, training loss: 12.962980270385742 = 0.36665138602256775 + 2.0 * 6.298164367675781
Epoch 630, val loss: 0.9176408052444458
Epoch 640, training loss: 12.944640159606934 = 0.34981757402420044 + 2.0 * 6.2974114418029785
Epoch 640, val loss: 0.9146756529808044
Epoch 650, training loss: 12.937973976135254 = 0.33377206325531006 + 2.0 * 6.302101135253906
Epoch 650, val loss: 0.9124862551689148
Epoch 660, training loss: 12.910451889038086 = 0.31847405433654785 + 2.0 * 6.295989036560059
Epoch 660, val loss: 0.9110986590385437
Epoch 670, training loss: 12.892786026000977 = 0.303983211517334 + 2.0 * 6.2944016456604
Epoch 670, val loss: 0.9104627966880798
Epoch 680, training loss: 12.88419246673584 = 0.2901889681816101 + 2.0 * 6.297001838684082
Epoch 680, val loss: 0.9103662371635437
Epoch 690, training loss: 12.863478660583496 = 0.277164101600647 + 2.0 * 6.29315710067749
Epoch 690, val loss: 0.9109976291656494
Epoch 700, training loss: 12.844697952270508 = 0.26477327942848206 + 2.0 * 6.289962291717529
Epoch 700, val loss: 0.9122030735015869
Epoch 710, training loss: 12.833620071411133 = 0.2529538869857788 + 2.0 * 6.290333271026611
Epoch 710, val loss: 0.9139342904090881
Epoch 720, training loss: 12.819768905639648 = 0.24172192811965942 + 2.0 * 6.289023399353027
Epoch 720, val loss: 0.916094958782196
Epoch 730, training loss: 12.817574501037598 = 0.23109494149684906 + 2.0 * 6.293239593505859
Epoch 730, val loss: 0.9187250137329102
Epoch 740, training loss: 12.79511833190918 = 0.22098320722579956 + 2.0 * 6.287067413330078
Epoch 740, val loss: 0.9217404127120972
Epoch 750, training loss: 12.780652046203613 = 0.211378276348114 + 2.0 * 6.284636974334717
Epoch 750, val loss: 0.9250912666320801
Epoch 760, training loss: 12.773380279541016 = 0.20220820605754852 + 2.0 * 6.285585880279541
Epoch 760, val loss: 0.9287043809890747
Epoch 770, training loss: 12.763397216796875 = 0.19346904754638672 + 2.0 * 6.284964084625244
Epoch 770, val loss: 0.9325445294380188
Epoch 780, training loss: 12.756771087646484 = 0.1851995587348938 + 2.0 * 6.285785675048828
Epoch 780, val loss: 0.9367188215255737
Epoch 790, training loss: 12.739870071411133 = 0.17735040187835693 + 2.0 * 6.281260013580322
Epoch 790, val loss: 0.9411777853965759
Epoch 800, training loss: 12.731979370117188 = 0.16985198855400085 + 2.0 * 6.281063556671143
Epoch 800, val loss: 0.9458532929420471
Epoch 810, training loss: 12.741044998168945 = 0.16271854937076569 + 2.0 * 6.289163112640381
Epoch 810, val loss: 0.9505955576896667
Epoch 820, training loss: 12.71633243560791 = 0.1559305191040039 + 2.0 * 6.280200958251953
Epoch 820, val loss: 0.9554567933082581
Epoch 830, training loss: 12.705183029174805 = 0.1494959443807602 + 2.0 * 6.277843475341797
Epoch 830, val loss: 0.9605363607406616
Epoch 840, training loss: 12.713330268859863 = 0.14334677159786224 + 2.0 * 6.28499174118042
Epoch 840, val loss: 0.9657122492790222
Epoch 850, training loss: 12.696159362792969 = 0.13753101229667664 + 2.0 * 6.279314041137695
Epoch 850, val loss: 0.9707887768745422
Epoch 860, training loss: 12.684907913208008 = 0.13197053968906403 + 2.0 * 6.276468753814697
Epoch 860, val loss: 0.976199209690094
Epoch 870, training loss: 12.689838409423828 = 0.12667079269886017 + 2.0 * 6.281583786010742
Epoch 870, val loss: 0.9815239906311035
Epoch 880, training loss: 12.67583179473877 = 0.12164663523435593 + 2.0 * 6.277092456817627
Epoch 880, val loss: 0.9869061708450317
Epoch 890, training loss: 12.668889999389648 = 0.11684708297252655 + 2.0 * 6.276021480560303
Epoch 890, val loss: 0.9923645257949829
Epoch 900, training loss: 12.658928871154785 = 0.1122819185256958 + 2.0 * 6.2733235359191895
Epoch 900, val loss: 0.9977584481239319
Epoch 910, training loss: 12.653911590576172 = 0.10793321579694748 + 2.0 * 6.272989273071289
Epoch 910, val loss: 1.0032610893249512
Epoch 920, training loss: 12.653279304504395 = 0.10377299785614014 + 2.0 * 6.274753093719482
Epoch 920, val loss: 1.0086982250213623
Epoch 930, training loss: 12.642645835876465 = 0.09982816129922867 + 2.0 * 6.271409034729004
Epoch 930, val loss: 1.0141817331314087
Epoch 940, training loss: 12.636451721191406 = 0.09607457369565964 + 2.0 * 6.270188808441162
Epoch 940, val loss: 1.0197607278823853
Epoch 950, training loss: 12.640172004699707 = 0.09248632937669754 + 2.0 * 6.273842811584473
Epoch 950, val loss: 1.0252355337142944
Epoch 960, training loss: 12.6317777633667 = 0.08907480537891388 + 2.0 * 6.271351337432861
Epoch 960, val loss: 1.0307488441467285
Epoch 970, training loss: 12.624444007873535 = 0.0858423039317131 + 2.0 * 6.269300937652588
Epoch 970, val loss: 1.0363858938217163
Epoch 980, training loss: 12.617392539978027 = 0.08274833858013153 + 2.0 * 6.267322063446045
Epoch 980, val loss: 1.0419498682022095
Epoch 990, training loss: 12.612845420837402 = 0.07979040592908859 + 2.0 * 6.2665276527404785
Epoch 990, val loss: 1.0475627183914185
Epoch 1000, training loss: 12.642619132995605 = 0.07695735991001129 + 2.0 * 6.282830715179443
Epoch 1000, val loss: 1.0530532598495483
Epoch 1010, training loss: 12.605842590332031 = 0.07428466528654099 + 2.0 * 6.2657790184021
Epoch 1010, val loss: 1.058483362197876
Epoch 1020, training loss: 12.605354309082031 = 0.07174203544855118 + 2.0 * 6.266806125640869
Epoch 1020, val loss: 1.0640466213226318
Epoch 1030, training loss: 12.602439880371094 = 0.0693034753203392 + 2.0 * 6.266568183898926
Epoch 1030, val loss: 1.0695017576217651
Epoch 1040, training loss: 12.596177101135254 = 0.06697878241539001 + 2.0 * 6.264599323272705
Epoch 1040, val loss: 1.0750575065612793
Epoch 1050, training loss: 12.592694282531738 = 0.06476470828056335 + 2.0 * 6.263964653015137
Epoch 1050, val loss: 1.0805925130844116
Epoch 1060, training loss: 12.59852123260498 = 0.0626441091299057 + 2.0 * 6.267938613891602
Epoch 1060, val loss: 1.085981011390686
Epoch 1070, training loss: 12.588191032409668 = 0.060625042766332626 + 2.0 * 6.263782978057861
Epoch 1070, val loss: 1.0914945602416992
Epoch 1080, training loss: 12.58210277557373 = 0.058690838515758514 + 2.0 * 6.2617058753967285
Epoch 1080, val loss: 1.0969618558883667
Epoch 1090, training loss: 12.579622268676758 = 0.056841153651475906 + 2.0 * 6.261390686035156
Epoch 1090, val loss: 1.1024397611618042
Epoch 1100, training loss: 12.585453987121582 = 0.05507054179906845 + 2.0 * 6.265191555023193
Epoch 1100, val loss: 1.1078463792800903
Epoch 1110, training loss: 12.578763008117676 = 0.053369343280792236 + 2.0 * 6.262696743011475
Epoch 1110, val loss: 1.1131105422973633
Epoch 1120, training loss: 12.57277774810791 = 0.05174636095762253 + 2.0 * 6.2605156898498535
Epoch 1120, val loss: 1.1184436082839966
Epoch 1130, training loss: 12.568177223205566 = 0.05019544064998627 + 2.0 * 6.25899076461792
Epoch 1130, val loss: 1.1238718032836914
Epoch 1140, training loss: 12.566834449768066 = 0.04870196059346199 + 2.0 * 6.259066104888916
Epoch 1140, val loss: 1.1292107105255127
Epoch 1150, training loss: 12.578182220458984 = 0.047267258167266846 + 2.0 * 6.265457630157471
Epoch 1150, val loss: 1.1345181465148926
Epoch 1160, training loss: 12.564858436584473 = 0.04589793086051941 + 2.0 * 6.2594804763793945
Epoch 1160, val loss: 1.1396440267562866
Epoch 1170, training loss: 12.570691108703613 = 0.04458104074001312 + 2.0 * 6.263054847717285
Epoch 1170, val loss: 1.1449353694915771
Epoch 1180, training loss: 12.557530403137207 = 0.04332580789923668 + 2.0 * 6.2571024894714355
Epoch 1180, val loss: 1.1500300168991089
Epoch 1190, training loss: 12.55604076385498 = 0.04211674630641937 + 2.0 * 6.256961822509766
Epoch 1190, val loss: 1.1553549766540527
Epoch 1200, training loss: 12.561358451843262 = 0.040956802666187286 + 2.0 * 6.2602009773254395
Epoch 1200, val loss: 1.1602851152420044
Epoch 1210, training loss: 12.553853988647461 = 0.039841532707214355 + 2.0 * 6.2570061683654785
Epoch 1210, val loss: 1.165420651435852
Epoch 1220, training loss: 12.54791259765625 = 0.038777146488428116 + 2.0 * 6.254567623138428
Epoch 1220, val loss: 1.1704521179199219
Epoch 1230, training loss: 12.546357154846191 = 0.03774580359458923 + 2.0 * 6.254305839538574
Epoch 1230, val loss: 1.1754963397979736
Epoch 1240, training loss: 12.557693481445312 = 0.036752134561538696 + 2.0 * 6.260470867156982
Epoch 1240, val loss: 1.18045175075531
Epoch 1250, training loss: 12.54857063293457 = 0.03579842299222946 + 2.0 * 6.2563862800598145
Epoch 1250, val loss: 1.1854923963546753
Epoch 1260, training loss: 12.547608375549316 = 0.03487681970000267 + 2.0 * 6.256365776062012
Epoch 1260, val loss: 1.1903845071792603
Epoch 1270, training loss: 12.539168357849121 = 0.033995985984802246 + 2.0 * 6.252586364746094
Epoch 1270, val loss: 1.1953740119934082
Epoch 1280, training loss: 12.536881446838379 = 0.0331430658698082 + 2.0 * 6.251869201660156
Epoch 1280, val loss: 1.2003140449523926
Epoch 1290, training loss: 12.553807258605957 = 0.03232220560312271 + 2.0 * 6.260742664337158
Epoch 1290, val loss: 1.2051807641983032
Epoch 1300, training loss: 12.539987564086914 = 0.0315239243209362 + 2.0 * 6.254231929779053
Epoch 1300, val loss: 1.2098028659820557
Epoch 1310, training loss: 12.541110038757324 = 0.030763696879148483 + 2.0 * 6.255173206329346
Epoch 1310, val loss: 1.2146074771881104
Epoch 1320, training loss: 12.531645774841309 = 0.030026758089661598 + 2.0 * 6.250809669494629
Epoch 1320, val loss: 1.2193565368652344
Epoch 1330, training loss: 12.542472839355469 = 0.029315806925296783 + 2.0 * 6.25657844543457
Epoch 1330, val loss: 1.2241427898406982
Epoch 1340, training loss: 12.530755043029785 = 0.02863207645714283 + 2.0 * 6.25106143951416
Epoch 1340, val loss: 1.2285070419311523
Epoch 1350, training loss: 12.528996467590332 = 0.027972644194960594 + 2.0 * 6.25051212310791
Epoch 1350, val loss: 1.2333922386169434
Epoch 1360, training loss: 12.531413078308105 = 0.027335524559020996 + 2.0 * 6.252038955688477
Epoch 1360, val loss: 1.237978458404541
Epoch 1370, training loss: 12.52580451965332 = 0.026716643944382668 + 2.0 * 6.249544143676758
Epoch 1370, val loss: 1.2424508333206177
Epoch 1380, training loss: 12.523682594299316 = 0.026120299473404884 + 2.0 * 6.248781204223633
Epoch 1380, val loss: 1.2470191717147827
Epoch 1390, training loss: 12.520857810974121 = 0.02554011531174183 + 2.0 * 6.247658729553223
Epoch 1390, val loss: 1.2516052722930908
Epoch 1400, training loss: 12.523652076721191 = 0.02497696876525879 + 2.0 * 6.249337673187256
Epoch 1400, val loss: 1.256064772605896
Epoch 1410, training loss: 12.529549598693848 = 0.024432430043816566 + 2.0 * 6.252558708190918
Epoch 1410, val loss: 1.2603023052215576
Epoch 1420, training loss: 12.520788192749023 = 0.02391185238957405 + 2.0 * 6.248438358306885
Epoch 1420, val loss: 1.264803409576416
Epoch 1430, training loss: 12.518701553344727 = 0.02340896613895893 + 2.0 * 6.247646331787109
Epoch 1430, val loss: 1.2691731452941895
Epoch 1440, training loss: 12.514814376831055 = 0.022918876260519028 + 2.0 * 6.24594783782959
Epoch 1440, val loss: 1.2735179662704468
Epoch 1450, training loss: 12.51315689086914 = 0.022443071007728577 + 2.0 * 6.245357036590576
Epoch 1450, val loss: 1.2778985500335693
Epoch 1460, training loss: 12.523063659667969 = 0.02197995036840439 + 2.0 * 6.250541687011719
Epoch 1460, val loss: 1.2821983098983765
Epoch 1470, training loss: 12.524141311645508 = 0.02153361774981022 + 2.0 * 6.251303672790527
Epoch 1470, val loss: 1.2862753868103027
Epoch 1480, training loss: 12.514960289001465 = 0.02109813131392002 + 2.0 * 6.246931076049805
Epoch 1480, val loss: 1.290558099746704
Epoch 1490, training loss: 12.508703231811523 = 0.020681969821453094 + 2.0 * 6.2440104484558105
Epoch 1490, val loss: 1.2947275638580322
Epoch 1500, training loss: 12.508933067321777 = 0.02027423307299614 + 2.0 * 6.244329452514648
Epoch 1500, val loss: 1.2988604307174683
Epoch 1510, training loss: 12.52574348449707 = 0.019877662882208824 + 2.0 * 6.252933025360107
Epoch 1510, val loss: 1.302885890007019
Epoch 1520, training loss: 12.51327896118164 = 0.01949484460055828 + 2.0 * 6.246891975402832
Epoch 1520, val loss: 1.3070733547210693
Epoch 1530, training loss: 12.51132869720459 = 0.01912434585392475 + 2.0 * 6.246102333068848
Epoch 1530, val loss: 1.3110921382904053
Epoch 1540, training loss: 12.505701065063477 = 0.01876179873943329 + 2.0 * 6.243469715118408
Epoch 1540, val loss: 1.3149700164794922
Epoch 1550, training loss: 12.508090019226074 = 0.01841125264763832 + 2.0 * 6.244839191436768
Epoch 1550, val loss: 1.3189858198165894
Epoch 1560, training loss: 12.502456665039062 = 0.01807059533894062 + 2.0 * 6.242193222045898
Epoch 1560, val loss: 1.323046088218689
Epoch 1570, training loss: 12.507842063903809 = 0.017737379297614098 + 2.0 * 6.245052337646484
Epoch 1570, val loss: 1.3270368576049805
Epoch 1580, training loss: 12.513702392578125 = 0.01741143688559532 + 2.0 * 6.248145580291748
Epoch 1580, val loss: 1.3306185007095337
Epoch 1590, training loss: 12.50456714630127 = 0.017102349549531937 + 2.0 * 6.243732452392578
Epoch 1590, val loss: 1.3346060514450073
Epoch 1600, training loss: 12.500195503234863 = 0.016798749566078186 + 2.0 * 6.241698265075684
Epoch 1600, val loss: 1.338471531867981
Epoch 1610, training loss: 12.499953269958496 = 0.01650339365005493 + 2.0 * 6.241724967956543
Epoch 1610, val loss: 1.342268705368042
Epoch 1620, training loss: 12.504261016845703 = 0.01621354930102825 + 2.0 * 6.24402379989624
Epoch 1620, val loss: 1.3460485935211182
Epoch 1630, training loss: 12.505642890930176 = 0.01593100279569626 + 2.0 * 6.244855880737305
Epoch 1630, val loss: 1.3497138023376465
Epoch 1640, training loss: 12.495927810668945 = 0.01565881259739399 + 2.0 * 6.2401347160339355
Epoch 1640, val loss: 1.353545904159546
Epoch 1650, training loss: 12.494790077209473 = 0.015393165871500969 + 2.0 * 6.23969841003418
Epoch 1650, val loss: 1.3572700023651123
Epoch 1660, training loss: 12.49612045288086 = 0.015132761560380459 + 2.0 * 6.2404937744140625
Epoch 1660, val loss: 1.360944151878357
Epoch 1670, training loss: 12.50621223449707 = 0.014879313297569752 + 2.0 * 6.24566650390625
Epoch 1670, val loss: 1.36459219455719
Epoch 1680, training loss: 12.497961044311523 = 0.014630401507019997 + 2.0 * 6.241665363311768
Epoch 1680, val loss: 1.368111491203308
Epoch 1690, training loss: 12.493374824523926 = 0.014391927048563957 + 2.0 * 6.2394914627075195
Epoch 1690, val loss: 1.371695876121521
Epoch 1700, training loss: 12.500415802001953 = 0.014157569035887718 + 2.0 * 6.243129253387451
Epoch 1700, val loss: 1.3752292394638062
Epoch 1710, training loss: 12.491536140441895 = 0.013928891159594059 + 2.0 * 6.238803386688232
Epoch 1710, val loss: 1.3786680698394775
Epoch 1720, training loss: 12.500772476196289 = 0.013707447797060013 + 2.0 * 6.243532657623291
Epoch 1720, val loss: 1.382178544998169
Epoch 1730, training loss: 12.488236427307129 = 0.013490783050656319 + 2.0 * 6.237372875213623
Epoch 1730, val loss: 1.385666847229004
Epoch 1740, training loss: 12.487923622131348 = 0.013280331157147884 + 2.0 * 6.237321853637695
Epoch 1740, val loss: 1.3892085552215576
Epoch 1750, training loss: 12.488899230957031 = 0.013072825036942959 + 2.0 * 6.237913131713867
Epoch 1750, val loss: 1.3926485776901245
Epoch 1760, training loss: 12.510339736938477 = 0.012868483550846577 + 2.0 * 6.248735427856445
Epoch 1760, val loss: 1.3958418369293213
Epoch 1770, training loss: 12.489591598510742 = 0.0126743633300066 + 2.0 * 6.238458633422852
Epoch 1770, val loss: 1.3991233110427856
Epoch 1780, training loss: 12.484189987182617 = 0.01248419564217329 + 2.0 * 6.2358527183532715
Epoch 1780, val loss: 1.4026139974594116
Epoch 1790, training loss: 12.48401927947998 = 0.012297761626541615 + 2.0 * 6.235860824584961
Epoch 1790, val loss: 1.405989408493042
Epoch 1800, training loss: 12.4887113571167 = 0.012114587239921093 + 2.0 * 6.238298416137695
Epoch 1800, val loss: 1.4093557596206665
Epoch 1810, training loss: 12.490469932556152 = 0.011934686452150345 + 2.0 * 6.239267826080322
Epoch 1810, val loss: 1.4124104976654053
Epoch 1820, training loss: 12.48636531829834 = 0.011759212240576744 + 2.0 * 6.237303256988525
Epoch 1820, val loss: 1.415619969367981
Epoch 1830, training loss: 12.486879348754883 = 0.011589487083256245 + 2.0 * 6.237645149230957
Epoch 1830, val loss: 1.4188255071640015
Epoch 1840, training loss: 12.480660438537598 = 0.011422851122915745 + 2.0 * 6.234618663787842
Epoch 1840, val loss: 1.4220561981201172
Epoch 1850, training loss: 12.488853454589844 = 0.011259875260293484 + 2.0 * 6.238796710968018
Epoch 1850, val loss: 1.4252638816833496
Epoch 1860, training loss: 12.482357025146484 = 0.011099464260041714 + 2.0 * 6.235628604888916
Epoch 1860, val loss: 1.4282450675964355
Epoch 1870, training loss: 12.479058265686035 = 0.010943672619760036 + 2.0 * 6.234057426452637
Epoch 1870, val loss: 1.431395411491394
Epoch 1880, training loss: 12.483898162841797 = 0.010790908709168434 + 2.0 * 6.23655366897583
Epoch 1880, val loss: 1.4345417022705078
Epoch 1890, training loss: 12.485197067260742 = 0.010640567168593407 + 2.0 * 6.237278461456299
Epoch 1890, val loss: 1.4373475313186646
Epoch 1900, training loss: 12.48033332824707 = 0.010496714152395725 + 2.0 * 6.234918117523193
Epoch 1900, val loss: 1.440576195716858
Epoch 1910, training loss: 12.477686882019043 = 0.010353652760386467 + 2.0 * 6.23366641998291
Epoch 1910, val loss: 1.4435604810714722
Epoch 1920, training loss: 12.478322982788086 = 0.010213393718004227 + 2.0 * 6.2340545654296875
Epoch 1920, val loss: 1.4465912580490112
Epoch 1930, training loss: 12.486818313598633 = 0.010074918158352375 + 2.0 * 6.238371849060059
Epoch 1930, val loss: 1.4493515491485596
Epoch 1940, training loss: 12.478130340576172 = 0.009943231008946896 + 2.0 * 6.23409366607666
Epoch 1940, val loss: 1.4523594379425049
Epoch 1950, training loss: 12.4816255569458 = 0.009812576696276665 + 2.0 * 6.235906600952148
Epoch 1950, val loss: 1.4553719758987427
Epoch 1960, training loss: 12.47772216796875 = 0.009684407152235508 + 2.0 * 6.234018802642822
Epoch 1960, val loss: 1.4579648971557617
Epoch 1970, training loss: 12.473968505859375 = 0.009559902362525463 + 2.0 * 6.232204437255859
Epoch 1970, val loss: 1.4608477354049683
Epoch 1980, training loss: 12.473516464233398 = 0.009438104927539825 + 2.0 * 6.232038974761963
Epoch 1980, val loss: 1.4638128280639648
Epoch 1990, training loss: 12.4730806350708 = 0.00931744184345007 + 2.0 * 6.231881618499756
Epoch 1990, val loss: 1.4666097164154053
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7703703703703704
0.8065366367949395
=== training gcn model ===
Epoch 0, training loss: 19.15616798400879 = 1.9625049829483032 + 2.0 * 8.596831321716309
Epoch 0, val loss: 1.967560052871704
Epoch 10, training loss: 19.14533233642578 = 1.9522591829299927 + 2.0 * 8.596536636352539
Epoch 10, val loss: 1.9568138122558594
Epoch 20, training loss: 19.12860107421875 = 1.9397677183151245 + 2.0 * 8.594416618347168
Epoch 20, val loss: 1.9434138536453247
Epoch 30, training loss: 19.079809188842773 = 1.9226744174957275 + 2.0 * 8.578567504882812
Epoch 30, val loss: 1.9249125719070435
Epoch 40, training loss: 18.87932014465332 = 1.9002845287322998 + 2.0 * 8.489518165588379
Epoch 40, val loss: 1.9013793468475342
Epoch 50, training loss: 17.8116397857666 = 1.8768112659454346 + 2.0 * 7.967413902282715
Epoch 50, val loss: 1.8771798610687256
Epoch 60, training loss: 16.76836395263672 = 1.8571767807006836 + 2.0 * 7.455593109130859
Epoch 60, val loss: 1.8582019805908203
Epoch 70, training loss: 16.166318893432617 = 1.8420605659484863 + 2.0 * 7.162128925323486
Epoch 70, val loss: 1.8433432579040527
Epoch 80, training loss: 15.843046188354492 = 1.8269755840301514 + 2.0 * 7.008035182952881
Epoch 80, val loss: 1.8284907341003418
Epoch 90, training loss: 15.615320205688477 = 1.8131113052368164 + 2.0 * 6.90110445022583
Epoch 90, val loss: 1.8151665925979614
Epoch 100, training loss: 15.464722633361816 = 1.8003164529800415 + 2.0 * 6.832202911376953
Epoch 100, val loss: 1.8029232025146484
Epoch 110, training loss: 15.340536117553711 = 1.7889580726623535 + 2.0 * 6.7757887840271
Epoch 110, val loss: 1.7919268608093262
Epoch 120, training loss: 15.230203628540039 = 1.7788960933685303 + 2.0 * 6.725653648376465
Epoch 120, val loss: 1.782200574874878
Epoch 130, training loss: 15.138673782348633 = 1.7688814401626587 + 2.0 * 6.684895992279053
Epoch 130, val loss: 1.772989273071289
Epoch 140, training loss: 15.053641319274902 = 1.7583646774291992 + 2.0 * 6.647638320922852
Epoch 140, val loss: 1.7639360427856445
Epoch 150, training loss: 14.96965217590332 = 1.7472000122070312 + 2.0 * 6.6112260818481445
Epoch 150, val loss: 1.7544915676116943
Epoch 160, training loss: 14.89816951751709 = 1.7347729206085205 + 2.0 * 6.581698417663574
Epoch 160, val loss: 1.7442280054092407
Epoch 170, training loss: 14.822225570678711 = 1.720864176750183 + 2.0 * 6.550680637359619
Epoch 170, val loss: 1.732472538948059
Epoch 180, training loss: 14.75965690612793 = 1.704954981803894 + 2.0 * 6.527350902557373
Epoch 180, val loss: 1.7191256284713745
Epoch 190, training loss: 14.703619956970215 = 1.6866159439086914 + 2.0 * 6.508502006530762
Epoch 190, val loss: 1.7040241956710815
Epoch 200, training loss: 14.650675773620605 = 1.6657875776290894 + 2.0 * 6.492444038391113
Epoch 200, val loss: 1.6867443323135376
Epoch 210, training loss: 14.598374366760254 = 1.6419811248779297 + 2.0 * 6.478196620941162
Epoch 210, val loss: 1.6670902967453003
Epoch 220, training loss: 14.552387237548828 = 1.6147772073745728 + 2.0 * 6.468804836273193
Epoch 220, val loss: 1.644804835319519
Epoch 230, training loss: 14.498007774353027 = 1.5845136642456055 + 2.0 * 6.456747055053711
Epoch 230, val loss: 1.620184302330017
Epoch 240, training loss: 14.445175170898438 = 1.5510352849960327 + 2.0 * 6.447070121765137
Epoch 240, val loss: 1.5930699110031128
Epoch 250, training loss: 14.391258239746094 = 1.5142590999603271 + 2.0 * 6.438499450683594
Epoch 250, val loss: 1.5636943578720093
Epoch 260, training loss: 14.343282699584961 = 1.4746308326721191 + 2.0 * 6.434326171875
Epoch 260, val loss: 1.5325340032577515
Epoch 270, training loss: 14.282304763793945 = 1.4332103729248047 + 2.0 * 6.42454719543457
Epoch 270, val loss: 1.500680923461914
Epoch 280, training loss: 14.223946571350098 = 1.3903381824493408 + 2.0 * 6.416804313659668
Epoch 280, val loss: 1.4683082103729248
Epoch 290, training loss: 14.168144226074219 = 1.3467060327529907 + 2.0 * 6.41071891784668
Epoch 290, val loss: 1.4361165761947632
Epoch 300, training loss: 14.112936019897461 = 1.303734302520752 + 2.0 * 6.404601097106934
Epoch 300, val loss: 1.405305027961731
Epoch 310, training loss: 14.057376861572266 = 1.2612086534500122 + 2.0 * 6.3980841636657715
Epoch 310, val loss: 1.3754924535751343
Epoch 320, training loss: 14.00471019744873 = 1.2189257144927979 + 2.0 * 6.392892360687256
Epoch 320, val loss: 1.3464829921722412
Epoch 330, training loss: 13.960912704467773 = 1.1776922941207886 + 2.0 * 6.391610145568848
Epoch 330, val loss: 1.3185765743255615
Epoch 340, training loss: 13.903106689453125 = 1.137291669845581 + 2.0 * 6.382907390594482
Epoch 340, val loss: 1.2920513153076172
Epoch 350, training loss: 13.854071617126465 = 1.0976886749267578 + 2.0 * 6.3781914710998535
Epoch 350, val loss: 1.2662001848220825
Epoch 360, training loss: 13.808890342712402 = 1.0586398839950562 + 2.0 * 6.375125408172607
Epoch 360, val loss: 1.2409861087799072
Epoch 370, training loss: 13.765755653381348 = 1.0205554962158203 + 2.0 * 6.372600078582764
Epoch 370, val loss: 1.2168437242507935
Epoch 380, training loss: 13.716209411621094 = 0.983597457408905 + 2.0 * 6.366305828094482
Epoch 380, val loss: 1.1936014890670776
Epoch 390, training loss: 13.67560863494873 = 0.9475603103637695 + 2.0 * 6.3640241622924805
Epoch 390, val loss: 1.1711220741271973
Epoch 400, training loss: 13.640280723571777 = 0.9126595258712769 + 2.0 * 6.3638105392456055
Epoch 400, val loss: 1.1497957706451416
Epoch 410, training loss: 13.593815803527832 = 0.8790866136550903 + 2.0 * 6.357364654541016
Epoch 410, val loss: 1.129363775253296
Epoch 420, training loss: 13.553533554077148 = 0.846519410610199 + 2.0 * 6.353507041931152
Epoch 420, val loss: 1.1096709966659546
Epoch 430, training loss: 13.52496337890625 = 0.8148510456085205 + 2.0 * 6.355056285858154
Epoch 430, val loss: 1.090697169303894
Epoch 440, training loss: 13.484025001525879 = 0.784116804599762 + 2.0 * 6.349954128265381
Epoch 440, val loss: 1.0727267265319824
Epoch 450, training loss: 13.445184707641602 = 0.754524290561676 + 2.0 * 6.345330238342285
Epoch 450, val loss: 1.0554229021072388
Epoch 460, training loss: 13.411272048950195 = 0.7256353497505188 + 2.0 * 6.342818260192871
Epoch 460, val loss: 1.0388009548187256
Epoch 470, training loss: 13.382550239562988 = 0.6976757645606995 + 2.0 * 6.342437267303467
Epoch 470, val loss: 1.0229541063308716
Epoch 480, training loss: 13.348200798034668 = 0.6707437038421631 + 2.0 * 6.338728427886963
Epoch 480, val loss: 1.008059024810791
Epoch 490, training loss: 13.316644668579102 = 0.6446313261985779 + 2.0 * 6.3360066413879395
Epoch 490, val loss: 0.9938265085220337
Epoch 500, training loss: 13.299674987792969 = 0.6193090677261353 + 2.0 * 6.340182781219482
Epoch 500, val loss: 0.9803318977355957
Epoch 510, training loss: 13.258614540100098 = 0.5948914885520935 + 2.0 * 6.33186149597168
Epoch 510, val loss: 0.967819094657898
Epoch 520, training loss: 13.23117446899414 = 0.5715927481651306 + 2.0 * 6.329791069030762
Epoch 520, val loss: 0.9562228918075562
Epoch 530, training loss: 13.204422950744629 = 0.5489299893379211 + 2.0 * 6.327746391296387
Epoch 530, val loss: 0.945381760597229
Epoch 540, training loss: 13.177655220031738 = 0.5268641114234924 + 2.0 * 6.325395584106445
Epoch 540, val loss: 0.9352419376373291
Epoch 550, training loss: 13.157357215881348 = 0.5053085684776306 + 2.0 * 6.326024532318115
Epoch 550, val loss: 0.9257963299751282
Epoch 560, training loss: 13.139838218688965 = 0.4844910800457001 + 2.0 * 6.327673435211182
Epoch 560, val loss: 0.9169616103172302
Epoch 570, training loss: 13.107114791870117 = 0.46434205770492554 + 2.0 * 6.321386337280273
Epoch 570, val loss: 0.9091446995735168
Epoch 580, training loss: 13.082415580749512 = 0.4447060227394104 + 2.0 * 6.318854808807373
Epoch 580, val loss: 0.9020059108734131
Epoch 590, training loss: 13.059259414672852 = 0.42550623416900635 + 2.0 * 6.316876411437988
Epoch 590, val loss: 0.8956103324890137
Epoch 600, training loss: 13.043095588684082 = 0.40668830275535583 + 2.0 * 6.318203449249268
Epoch 600, val loss: 0.8899474740028381
Epoch 610, training loss: 13.032129287719727 = 0.38858333230018616 + 2.0 * 6.321773052215576
Epoch 610, val loss: 0.8850186467170715
Epoch 620, training loss: 12.99899673461914 = 0.3709738850593567 + 2.0 * 6.314011573791504
Epoch 620, val loss: 0.8810983896255493
Epoch 630, training loss: 12.975897789001465 = 0.35389137268066406 + 2.0 * 6.3110032081604
Epoch 630, val loss: 0.8779217004776001
Epoch 640, training loss: 12.956350326538086 = 0.33726587891578674 + 2.0 * 6.309542179107666
Epoch 640, val loss: 0.8754867315292358
Epoch 650, training loss: 12.953956604003906 = 0.3211444318294525 + 2.0 * 6.31640625
Epoch 650, val loss: 0.8737607598304749
Epoch 660, training loss: 12.919811248779297 = 0.3055727779865265 + 2.0 * 6.307119369506836
Epoch 660, val loss: 0.8728496432304382
Epoch 670, training loss: 12.903067588806152 = 0.2905997037887573 + 2.0 * 6.306233882904053
Epoch 670, val loss: 0.8726436495780945
Epoch 680, training loss: 12.890978813171387 = 0.2762368619441986 + 2.0 * 6.307371139526367
Epoch 680, val loss: 0.8728923797607422
Epoch 690, training loss: 12.873477935791016 = 0.2626141309738159 + 2.0 * 6.305431842803955
Epoch 690, val loss: 0.8739708662033081
Epoch 700, training loss: 12.854963302612305 = 0.24962177872657776 + 2.0 * 6.302670955657959
Epoch 700, val loss: 0.8756491541862488
Epoch 710, training loss: 12.838951110839844 = 0.23721911013126373 + 2.0 * 6.30086612701416
Epoch 710, val loss: 0.8778320550918579
Epoch 720, training loss: 12.825556755065918 = 0.22540400922298431 + 2.0 * 6.300076484680176
Epoch 720, val loss: 0.8805689215660095
Epoch 730, training loss: 12.81428337097168 = 0.21421542763710022 + 2.0 * 6.300034046173096
Epoch 730, val loss: 0.8836184144020081
Epoch 740, training loss: 12.802156448364258 = 0.20365093648433685 + 2.0 * 6.299252986907959
Epoch 740, val loss: 0.887267529964447
Epoch 750, training loss: 12.7899808883667 = 0.19371987879276276 + 2.0 * 6.298130512237549
Epoch 750, val loss: 0.8913151025772095
Epoch 760, training loss: 12.776169776916504 = 0.18433620035648346 + 2.0 * 6.295916557312012
Epoch 760, val loss: 0.8955379128456116
Epoch 770, training loss: 12.765155792236328 = 0.17553728818893433 + 2.0 * 6.294809341430664
Epoch 770, val loss: 0.9003166556358337
Epoch 780, training loss: 12.753640174865723 = 0.1672237068414688 + 2.0 * 6.293208122253418
Epoch 780, val loss: 0.9053559899330139
Epoch 790, training loss: 12.7568359375 = 0.15939578413963318 + 2.0 * 6.298719882965088
Epoch 790, val loss: 0.9105796813964844
Epoch 800, training loss: 12.740251541137695 = 0.1520448625087738 + 2.0 * 6.294103145599365
Epoch 800, val loss: 0.9162405133247375
Epoch 810, training loss: 12.725894927978516 = 0.14511625468730927 + 2.0 * 6.290389537811279
Epoch 810, val loss: 0.9220401644706726
Epoch 820, training loss: 12.725446701049805 = 0.13858601450920105 + 2.0 * 6.293430328369141
Epoch 820, val loss: 0.9279375672340393
Epoch 830, training loss: 12.70930290222168 = 0.13245822489261627 + 2.0 * 6.288422107696533
Epoch 830, val loss: 0.9342621564865112
Epoch 840, training loss: 12.70177173614502 = 0.12665702402591705 + 2.0 * 6.287557125091553
Epoch 840, val loss: 0.9406467080116272
Epoch 850, training loss: 12.710866928100586 = 0.12117467075586319 + 2.0 * 6.294846057891846
Epoch 850, val loss: 0.9469914436340332
Epoch 860, training loss: 12.687796592712402 = 0.11601468175649643 + 2.0 * 6.285891056060791
Epoch 860, val loss: 0.9536400437355042
Epoch 870, training loss: 12.681403160095215 = 0.11115019768476486 + 2.0 * 6.285126686096191
Epoch 870, val loss: 0.9603725671768188
Epoch 880, training loss: 12.6737060546875 = 0.106547050178051 + 2.0 * 6.283579349517822
Epoch 880, val loss: 0.967221200466156
Epoch 890, training loss: 12.685843467712402 = 0.10219931602478027 + 2.0 * 6.2918219566345215
Epoch 890, val loss: 0.974075973033905
Epoch 900, training loss: 12.666136741638184 = 0.09805498272180557 + 2.0 * 6.284040927886963
Epoch 900, val loss: 0.9809030294418335
Epoch 910, training loss: 12.657852172851562 = 0.09415573626756668 + 2.0 * 6.281848430633545
Epoch 910, val loss: 0.9878833889961243
Epoch 920, training loss: 12.652131080627441 = 0.09046538919210434 + 2.0 * 6.280832767486572
Epoch 920, val loss: 0.9948828816413879
Epoch 930, training loss: 12.652461051940918 = 0.08695487678050995 + 2.0 * 6.282752990722656
Epoch 930, val loss: 1.0018796920776367
Epoch 940, training loss: 12.641047477722168 = 0.08363140374422073 + 2.0 * 6.278707981109619
Epoch 940, val loss: 1.0090200901031494
Epoch 950, training loss: 12.643157958984375 = 0.08047197014093399 + 2.0 * 6.28134298324585
Epoch 950, val loss: 1.0161192417144775
Epoch 960, training loss: 12.64279556274414 = 0.07745759934186935 + 2.0 * 6.2826690673828125
Epoch 960, val loss: 1.022993803024292
Epoch 970, training loss: 12.63684368133545 = 0.07462697476148605 + 2.0 * 6.281108379364014
Epoch 970, val loss: 1.0301942825317383
Epoch 980, training loss: 12.625088691711426 = 0.07191818207502365 + 2.0 * 6.276585102081299
Epoch 980, val loss: 1.03713858127594
Epoch 990, training loss: 12.619282722473145 = 0.06935159862041473 + 2.0 * 6.274965763092041
Epoch 990, val loss: 1.0442088842391968
Epoch 1000, training loss: 12.61652946472168 = 0.0669025182723999 + 2.0 * 6.274813652038574
Epoch 1000, val loss: 1.051253080368042
Epoch 1010, training loss: 12.621103286743164 = 0.0645638108253479 + 2.0 * 6.2782697677612305
Epoch 1010, val loss: 1.0580918788909912
Epoch 1020, training loss: 12.616474151611328 = 0.06233156844973564 + 2.0 * 6.277071475982666
Epoch 1020, val loss: 1.0650156736373901
Epoch 1030, training loss: 12.607852935791016 = 0.060225680470466614 + 2.0 * 6.273813724517822
Epoch 1030, val loss: 1.072037935256958
Epoch 1040, training loss: 12.600990295410156 = 0.05821207910776138 + 2.0 * 6.271389007568359
Epoch 1040, val loss: 1.0789101123809814
Epoch 1050, training loss: 12.598736763000488 = 0.056285593658685684 + 2.0 * 6.271225452423096
Epoch 1050, val loss: 1.085671067237854
Epoch 1060, training loss: 12.611933708190918 = 0.05444201081991196 + 2.0 * 6.278745651245117
Epoch 1060, val loss: 1.0923538208007812
Epoch 1070, training loss: 12.59110164642334 = 0.05269473418593407 + 2.0 * 6.2692036628723145
Epoch 1070, val loss: 1.0991438627243042
Epoch 1080, training loss: 12.590601921081543 = 0.05102686956524849 + 2.0 * 6.269787311553955
Epoch 1080, val loss: 1.1059871912002563
Epoch 1090, training loss: 12.587769508361816 = 0.049426428973674774 + 2.0 * 6.269171714782715
Epoch 1090, val loss: 1.1125651597976685
Epoch 1100, training loss: 12.599448204040527 = 0.047899749130010605 + 2.0 * 6.275774002075195
Epoch 1100, val loss: 1.1190539598464966
Epoch 1110, training loss: 12.580038070678711 = 0.04642864316701889 + 2.0 * 6.2668046951293945
Epoch 1110, val loss: 1.1254608631134033
Epoch 1120, training loss: 12.579032897949219 = 0.0450332909822464 + 2.0 * 6.2669997215271
Epoch 1120, val loss: 1.1319682598114014
Epoch 1130, training loss: 12.575675964355469 = 0.04369897395372391 + 2.0 * 6.265988349914551
Epoch 1130, val loss: 1.1384522914886475
Epoch 1140, training loss: 12.575981140136719 = 0.04241190850734711 + 2.0 * 6.26678466796875
Epoch 1140, val loss: 1.1448572874069214
Epoch 1150, training loss: 12.577240943908691 = 0.04118107259273529 + 2.0 * 6.268030166625977
Epoch 1150, val loss: 1.151111364364624
Epoch 1160, training loss: 12.57796859741211 = 0.03999156877398491 + 2.0 * 6.268988609313965
Epoch 1160, val loss: 1.1572673320770264
Epoch 1170, training loss: 12.567127227783203 = 0.03886264190077782 + 2.0 * 6.264132499694824
Epoch 1170, val loss: 1.1636269092559814
Epoch 1180, training loss: 12.563288688659668 = 0.03777438774704933 + 2.0 * 6.262757301330566
Epoch 1180, val loss: 1.169806957244873
Epoch 1190, training loss: 12.56155776977539 = 0.03672853112220764 + 2.0 * 6.262414455413818
Epoch 1190, val loss: 1.1759402751922607
Epoch 1200, training loss: 12.57978630065918 = 0.035722095519304276 + 2.0 * 6.272032260894775
Epoch 1200, val loss: 1.1820224523544312
Epoch 1210, training loss: 12.56389331817627 = 0.03474995493888855 + 2.0 * 6.264571666717529
Epoch 1210, val loss: 1.187748670578003
Epoch 1220, training loss: 12.5563383102417 = 0.033827267587184906 + 2.0 * 6.261255741119385
Epoch 1220, val loss: 1.1938811540603638
Epoch 1230, training loss: 12.56113338470459 = 0.03293566405773163 + 2.0 * 6.264098644256592
Epoch 1230, val loss: 1.1996855735778809
Epoch 1240, training loss: 12.552203178405762 = 0.03208354115486145 + 2.0 * 6.260059833526611
Epoch 1240, val loss: 1.2054312229156494
Epoch 1250, training loss: 12.550583839416504 = 0.03126038610935211 + 2.0 * 6.259661674499512
Epoch 1250, val loss: 1.211269736289978
Epoch 1260, training loss: 12.549544334411621 = 0.030469384044408798 + 2.0 * 6.259537696838379
Epoch 1260, val loss: 1.2170193195343018
Epoch 1270, training loss: 12.5512113571167 = 0.029703550040721893 + 2.0 * 6.260754108428955
Epoch 1270, val loss: 1.2225421667099
Epoch 1280, training loss: 12.55161190032959 = 0.02896430715918541 + 2.0 * 6.261323928833008
Epoch 1280, val loss: 1.2281802892684937
Epoch 1290, training loss: 12.545491218566895 = 0.0282587893307209 + 2.0 * 6.2586164474487305
Epoch 1290, val loss: 1.2336751222610474
Epoch 1300, training loss: 12.54172134399414 = 0.02757851593196392 + 2.0 * 6.257071495056152
Epoch 1300, val loss: 1.2393094301223755
Epoch 1310, training loss: 12.539581298828125 = 0.026919057592749596 + 2.0 * 6.256330966949463
Epoch 1310, val loss: 1.2447404861450195
Epoch 1320, training loss: 12.539661407470703 = 0.02628251351416111 + 2.0 * 6.256689548492432
Epoch 1320, val loss: 1.2501059770584106
Epoch 1330, training loss: 12.543169021606445 = 0.02566927671432495 + 2.0 * 6.258749961853027
Epoch 1330, val loss: 1.2552458047866821
Epoch 1340, training loss: 12.54214859008789 = 0.02507266402244568 + 2.0 * 6.258537769317627
Epoch 1340, val loss: 1.260452151298523
Epoch 1350, training loss: 12.535797119140625 = 0.024503666907548904 + 2.0 * 6.255646705627441
Epoch 1350, val loss: 1.265787124633789
Epoch 1360, training loss: 12.538856506347656 = 0.023956065997481346 + 2.0 * 6.257450103759766
Epoch 1360, val loss: 1.2710299491882324
Epoch 1370, training loss: 12.532334327697754 = 0.023422811180353165 + 2.0 * 6.25445556640625
Epoch 1370, val loss: 1.2758102416992188
Epoch 1380, training loss: 12.528897285461426 = 0.02290693297982216 + 2.0 * 6.252995014190674
Epoch 1380, val loss: 1.2808935642242432
Epoch 1390, training loss: 12.527336120605469 = 0.022410864010453224 + 2.0 * 6.252462863922119
Epoch 1390, val loss: 1.2859643697738647
Epoch 1400, training loss: 12.532365798950195 = 0.021930092945694923 + 2.0 * 6.255218029022217
Epoch 1400, val loss: 1.2909314632415771
Epoch 1410, training loss: 12.52608871459961 = 0.021459223702549934 + 2.0 * 6.252314567565918
Epoch 1410, val loss: 1.2954282760620117
Epoch 1420, training loss: 12.526359558105469 = 0.021008778363466263 + 2.0 * 6.252675533294678
Epoch 1420, val loss: 1.3003877401351929
Epoch 1430, training loss: 12.525057792663574 = 0.02057166025042534 + 2.0 * 6.2522430419921875
Epoch 1430, val loss: 1.3052098751068115
Epoch 1440, training loss: 12.529521942138672 = 0.0201493538916111 + 2.0 * 6.25468635559082
Epoch 1440, val loss: 1.3097293376922607
Epoch 1450, training loss: 12.526155471801758 = 0.019740581512451172 + 2.0 * 6.253207206726074
Epoch 1450, val loss: 1.3144571781158447
Epoch 1460, training loss: 12.522848129272461 = 0.01934315823018551 + 2.0 * 6.2517523765563965
Epoch 1460, val loss: 1.3190479278564453
Epoch 1470, training loss: 12.518488883972168 = 0.018956206738948822 + 2.0 * 6.2497663497924805
Epoch 1470, val loss: 1.3235679864883423
Epoch 1480, training loss: 12.51589584350586 = 0.018582409247756004 + 2.0 * 6.248656749725342
Epoch 1480, val loss: 1.3281044960021973
Epoch 1490, training loss: 12.516894340515137 = 0.018219735473394394 + 2.0 * 6.249337196350098
Epoch 1490, val loss: 1.3326126337051392
Epoch 1500, training loss: 12.524802207946777 = 0.017867563292384148 + 2.0 * 6.253467559814453
Epoch 1500, val loss: 1.3368895053863525
Epoch 1510, training loss: 12.51448917388916 = 0.017524968832731247 + 2.0 * 6.2484822273254395
Epoch 1510, val loss: 1.3412134647369385
Epoch 1520, training loss: 12.513100624084473 = 0.01719309762120247 + 2.0 * 6.24795389175415
Epoch 1520, val loss: 1.3456934690475464
Epoch 1530, training loss: 12.521141052246094 = 0.01687110774219036 + 2.0 * 6.252134799957275
Epoch 1530, val loss: 1.3498306274414062
Epoch 1540, training loss: 12.520452499389648 = 0.016563042998313904 + 2.0 * 6.251944541931152
Epoch 1540, val loss: 1.3541526794433594
Epoch 1550, training loss: 12.51301097869873 = 0.01625753939151764 + 2.0 * 6.248376846313477
Epoch 1550, val loss: 1.358102798461914
Epoch 1560, training loss: 12.508706092834473 = 0.015964915975928307 + 2.0 * 6.246370792388916
Epoch 1560, val loss: 1.3623604774475098
Epoch 1570, training loss: 12.506550788879395 = 0.01567978411912918 + 2.0 * 6.24543571472168
Epoch 1570, val loss: 1.3665426969528198
Epoch 1580, training loss: 12.504822731018066 = 0.01540107186883688 + 2.0 * 6.244710922241211
Epoch 1580, val loss: 1.3705360889434814
Epoch 1590, training loss: 12.504524230957031 = 0.015127924270927906 + 2.0 * 6.2446980476379395
Epoch 1590, val loss: 1.3745667934417725
Epoch 1600, training loss: 12.522170066833496 = 0.014862633310258389 + 2.0 * 6.253653526306152
Epoch 1600, val loss: 1.3784339427947998
Epoch 1610, training loss: 12.51353645324707 = 0.014607814140617847 + 2.0 * 6.249464511871338
Epoch 1610, val loss: 1.3823689222335815
Epoch 1620, training loss: 12.504339218139648 = 0.014357226900756359 + 2.0 * 6.244990825653076
Epoch 1620, val loss: 1.386486530303955
Epoch 1630, training loss: 12.502998352050781 = 0.014114724472165108 + 2.0 * 6.244441986083984
Epoch 1630, val loss: 1.3902283906936646
Epoch 1640, training loss: 12.50265121459961 = 0.01387811079621315 + 2.0 * 6.244386672973633
Epoch 1640, val loss: 1.3939250707626343
Epoch 1650, training loss: 12.501729011535645 = 0.013646867126226425 + 2.0 * 6.2440409660339355
Epoch 1650, val loss: 1.397679090499878
Epoch 1660, training loss: 12.503511428833008 = 0.013422451913356781 + 2.0 * 6.245044708251953
Epoch 1660, val loss: 1.4013348817825317
Epoch 1670, training loss: 12.50499153137207 = 0.013205801136791706 + 2.0 * 6.2458930015563965
Epoch 1670, val loss: 1.4053640365600586
Epoch 1680, training loss: 12.504524230957031 = 0.012992160394787788 + 2.0 * 6.2457661628723145
Epoch 1680, val loss: 1.4085755348205566
Epoch 1690, training loss: 12.497093200683594 = 0.012783090583980083 + 2.0 * 6.242155075073242
Epoch 1690, val loss: 1.4122880697250366
Epoch 1700, training loss: 12.495129585266113 = 0.012582363560795784 + 2.0 * 6.241273403167725
Epoch 1700, val loss: 1.4160387516021729
Epoch 1710, training loss: 12.494670867919922 = 0.01238432340323925 + 2.0 * 6.241143226623535
Epoch 1710, val loss: 1.4194620847702026
Epoch 1720, training loss: 12.502148628234863 = 0.01219166349619627 + 2.0 * 6.244978427886963
Epoch 1720, val loss: 1.4228830337524414
Epoch 1730, training loss: 12.501395225524902 = 0.012004098854959011 + 2.0 * 6.244695663452148
Epoch 1730, val loss: 1.4264123439788818
Epoch 1740, training loss: 12.493185043334961 = 0.011819257400929928 + 2.0 * 6.240683078765869
Epoch 1740, val loss: 1.429754376411438
Epoch 1750, training loss: 12.492307662963867 = 0.01164177618920803 + 2.0 * 6.240333080291748
Epoch 1750, val loss: 1.433401107788086
Epoch 1760, training loss: 12.493631362915039 = 0.011465991847217083 + 2.0 * 6.241082668304443
Epoch 1760, val loss: 1.4366105794906616
Epoch 1770, training loss: 12.497162818908691 = 0.01129372138530016 + 2.0 * 6.242934703826904
Epoch 1770, val loss: 1.4396259784698486
Epoch 1780, training loss: 12.490398406982422 = 0.011129926890134811 + 2.0 * 6.239634037017822
Epoch 1780, val loss: 1.443168044090271
Epoch 1790, training loss: 12.487988471984863 = 0.01096663810312748 + 2.0 * 6.238511085510254
Epoch 1790, val loss: 1.4466605186462402
Epoch 1800, training loss: 12.488691329956055 = 0.01080766785889864 + 2.0 * 6.238941669464111
Epoch 1800, val loss: 1.4498575925827026
Epoch 1810, training loss: 12.49404239654541 = 0.010651282966136932 + 2.0 * 6.241695404052734
Epoch 1810, val loss: 1.452884316444397
Epoch 1820, training loss: 12.495412826538086 = 0.010499386116862297 + 2.0 * 6.242456912994385
Epoch 1820, val loss: 1.4560034275054932
Epoch 1830, training loss: 12.488250732421875 = 0.010350426658987999 + 2.0 * 6.238950252532959
Epoch 1830, val loss: 1.4591017961502075
Epoch 1840, training loss: 12.48583698272705 = 0.010207891464233398 + 2.0 * 6.237814426422119
Epoch 1840, val loss: 1.46254563331604
Epoch 1850, training loss: 12.487321853637695 = 0.010066591203212738 + 2.0 * 6.2386274337768555
Epoch 1850, val loss: 1.4656198024749756
Epoch 1860, training loss: 12.486127853393555 = 0.00992758758366108 + 2.0 * 6.238100051879883
Epoch 1860, val loss: 1.4685111045837402
Epoch 1870, training loss: 12.482410430908203 = 0.009793008677661419 + 2.0 * 6.236308574676514
Epoch 1870, val loss: 1.4717329740524292
Epoch 1880, training loss: 12.483909606933594 = 0.009660459123551846 + 2.0 * 6.237124443054199
Epoch 1880, val loss: 1.4748016595840454
Epoch 1890, training loss: 12.484332084655762 = 0.00953052006661892 + 2.0 * 6.237401008605957
Epoch 1890, val loss: 1.4777313470840454
Epoch 1900, training loss: 12.48853588104248 = 0.009403390809893608 + 2.0 * 6.239566326141357
Epoch 1900, val loss: 1.480751872062683
Epoch 1910, training loss: 12.485616683959961 = 0.0092764375731349 + 2.0 * 6.238170146942139
Epoch 1910, val loss: 1.4833823442459106
Epoch 1920, training loss: 12.483999252319336 = 0.00915590487420559 + 2.0 * 6.23742151260376
Epoch 1920, val loss: 1.4864113330841064
Epoch 1930, training loss: 12.480658531188965 = 0.009037897922098637 + 2.0 * 6.235810279846191
Epoch 1930, val loss: 1.4895464181900024
Epoch 1940, training loss: 12.478111267089844 = 0.008921918459236622 + 2.0 * 6.234594821929932
Epoch 1940, val loss: 1.4923630952835083
Epoch 1950, training loss: 12.479004859924316 = 0.00880722887814045 + 2.0 * 6.235098838806152
Epoch 1950, val loss: 1.4952040910720825
Epoch 1960, training loss: 12.492417335510254 = 0.00869597215205431 + 2.0 * 6.241860866546631
Epoch 1960, val loss: 1.4978833198547363
Epoch 1970, training loss: 12.483182907104492 = 0.008582377806305885 + 2.0 * 6.237300395965576
Epoch 1970, val loss: 1.5004706382751465
Epoch 1980, training loss: 12.479435920715332 = 0.008476680144667625 + 2.0 * 6.235479831695557
Epoch 1980, val loss: 1.503537654876709
Epoch 1990, training loss: 12.478748321533203 = 0.008371198549866676 + 2.0 * 6.2351884841918945
Epoch 1990, val loss: 1.506126880645752
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7814814814814816
0.808645229309436
The final CL Acc:0.77531, 0.00462, The final GNN Acc:0.80759, 0.00086
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13214])
remove edge: torch.Size([2, 7864])
updated graph: torch.Size([2, 10522])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 19.142221450805664 = 1.9485585689544678 + 2.0 * 8.596831321716309
Epoch 0, val loss: 1.9358855485916138
Epoch 10, training loss: 19.130447387695312 = 1.9375758171081543 + 2.0 * 8.596435546875
Epoch 10, val loss: 1.9252384901046753
Epoch 20, training loss: 19.10931396484375 = 1.9235345125198364 + 2.0 * 8.592889785766602
Epoch 20, val loss: 1.911016821861267
Epoch 30, training loss: 19.036935806274414 = 1.9039579629898071 + 2.0 * 8.566489219665527
Epoch 30, val loss: 1.8909945487976074
Epoch 40, training loss: 18.717985153198242 = 1.8794795274734497 + 2.0 * 8.419252395629883
Epoch 40, val loss: 1.8666768074035645
Epoch 50, training loss: 17.85323715209961 = 1.8517266511917114 + 2.0 * 8.000755310058594
Epoch 50, val loss: 1.8400768041610718
Epoch 60, training loss: 17.135578155517578 = 1.829985499382019 + 2.0 * 7.652796745300293
Epoch 60, val loss: 1.82096529006958
Epoch 70, training loss: 16.261943817138672 = 1.8172612190246582 + 2.0 * 7.222341537475586
Epoch 70, val loss: 1.80938720703125
Epoch 80, training loss: 15.772252082824707 = 1.8070353269577026 + 2.0 * 6.982608318328857
Epoch 80, val loss: 1.799344778060913
Epoch 90, training loss: 15.512356758117676 = 1.7935584783554077 + 2.0 * 6.859399318695068
Epoch 90, val loss: 1.7866179943084717
Epoch 100, training loss: 15.342982292175293 = 1.7798508405685425 + 2.0 * 6.7815656661987305
Epoch 100, val loss: 1.7746570110321045
Epoch 110, training loss: 15.199986457824707 = 1.767500877380371 + 2.0 * 6.716242790222168
Epoch 110, val loss: 1.764009714126587
Epoch 120, training loss: 15.066657066345215 = 1.7555161714553833 + 2.0 * 6.6555705070495605
Epoch 120, val loss: 1.7535320520401
Epoch 130, training loss: 14.949246406555176 = 1.7432224750518799 + 2.0 * 6.6030120849609375
Epoch 130, val loss: 1.7426817417144775
Epoch 140, training loss: 14.860466003417969 = 1.7297065258026123 + 2.0 * 6.565379619598389
Epoch 140, val loss: 1.730784296989441
Epoch 150, training loss: 14.783601760864258 = 1.7142243385314941 + 2.0 * 6.534688949584961
Epoch 150, val loss: 1.7170432806015015
Epoch 160, training loss: 14.715766906738281 = 1.696558952331543 + 2.0 * 6.509603977203369
Epoch 160, val loss: 1.7013108730316162
Epoch 170, training loss: 14.658171653747559 = 1.676673173904419 + 2.0 * 6.490749359130859
Epoch 170, val loss: 1.6835702657699585
Epoch 180, training loss: 14.59872817993164 = 1.65458083152771 + 2.0 * 6.472073554992676
Epoch 180, val loss: 1.6638652086257935
Epoch 190, training loss: 14.544551849365234 = 1.6299729347229004 + 2.0 * 6.457289218902588
Epoch 190, val loss: 1.641998291015625
Epoch 200, training loss: 14.492215156555176 = 1.6026699542999268 + 2.0 * 6.444772720336914
Epoch 200, val loss: 1.6179907321929932
Epoch 210, training loss: 14.441822052001953 = 1.5731043815612793 + 2.0 * 6.434359073638916
Epoch 210, val loss: 1.5922448635101318
Epoch 220, training loss: 14.39085578918457 = 1.5417277812957764 + 2.0 * 6.424563884735107
Epoch 220, val loss: 1.565400242805481
Epoch 230, training loss: 14.338650703430176 = 1.5087636709213257 + 2.0 * 6.414943695068359
Epoch 230, val loss: 1.5376811027526855
Epoch 240, training loss: 14.291823387145996 = 1.4745804071426392 + 2.0 * 6.408621311187744
Epoch 240, val loss: 1.5097399950027466
Epoch 250, training loss: 14.241762161254883 = 1.4399056434631348 + 2.0 * 6.400928497314453
Epoch 250, val loss: 1.4823027849197388
Epoch 260, training loss: 14.193737030029297 = 1.4051061868667603 + 2.0 * 6.394315242767334
Epoch 260, val loss: 1.455647587776184
Epoch 270, training loss: 14.149402618408203 = 1.370368480682373 + 2.0 * 6.389516830444336
Epoch 270, val loss: 1.4300453662872314
Epoch 280, training loss: 14.10246753692627 = 1.3361127376556396 + 2.0 * 6.383177280426025
Epoch 280, val loss: 1.4056833982467651
Epoch 290, training loss: 14.060918807983398 = 1.3023722171783447 + 2.0 * 6.379273414611816
Epoch 290, val loss: 1.3825291395187378
Epoch 300, training loss: 14.019477844238281 = 1.2693980932235718 + 2.0 * 6.375040054321289
Epoch 300, val loss: 1.360444188117981
Epoch 310, training loss: 13.975879669189453 = 1.236933946609497 + 2.0 * 6.369472980499268
Epoch 310, val loss: 1.339162826538086
Epoch 320, training loss: 13.943514823913574 = 1.204977035522461 + 2.0 * 6.369268894195557
Epoch 320, val loss: 1.3183916807174683
Epoch 330, training loss: 13.901421546936035 = 1.173707127571106 + 2.0 * 6.363857269287109
Epoch 330, val loss: 1.2982178926467896
Epoch 340, training loss: 13.860153198242188 = 1.1429874897003174 + 2.0 * 6.358582973480225
Epoch 340, val loss: 1.2785437107086182
Epoch 350, training loss: 13.822197914123535 = 1.1125739812850952 + 2.0 * 6.354812145233154
Epoch 350, val loss: 1.2590525150299072
Epoch 360, training loss: 13.795611381530762 = 1.0823657512664795 + 2.0 * 6.356622695922852
Epoch 360, val loss: 1.2395684719085693
Epoch 370, training loss: 13.753860473632812 = 1.052550196647644 + 2.0 * 6.3506550788879395
Epoch 370, val loss: 1.2201855182647705
Epoch 380, training loss: 13.715450286865234 = 1.0231239795684814 + 2.0 * 6.346163272857666
Epoch 380, val loss: 1.2010542154312134
Epoch 390, training loss: 13.682867050170898 = 0.9939032793045044 + 2.0 * 6.344481945037842
Epoch 390, val loss: 1.1819145679473877
Epoch 400, training loss: 13.649109840393066 = 0.9648786187171936 + 2.0 * 6.34211540222168
Epoch 400, val loss: 1.162750244140625
Epoch 410, training loss: 13.614548683166504 = 0.9361217617988586 + 2.0 * 6.3392133712768555
Epoch 410, val loss: 1.1436647176742554
Epoch 420, training loss: 13.578994750976562 = 0.90752112865448 + 2.0 * 6.3357367515563965
Epoch 420, val loss: 1.1246217489242554
Epoch 430, training loss: 13.54940414428711 = 0.8790109753608704 + 2.0 * 6.335196495056152
Epoch 430, val loss: 1.1053847074508667
Epoch 440, training loss: 13.528092384338379 = 0.8506980538368225 + 2.0 * 6.3386969566345215
Epoch 440, val loss: 1.0860722064971924
Epoch 450, training loss: 13.484861373901367 = 0.8227753639221191 + 2.0 * 6.331043243408203
Epoch 450, val loss: 1.0668119192123413
Epoch 460, training loss: 13.44989013671875 = 0.7952113151550293 + 2.0 * 6.3273396492004395
Epoch 460, val loss: 1.0476220846176147
Epoch 470, training loss: 13.420400619506836 = 0.7679129838943481 + 2.0 * 6.326243877410889
Epoch 470, val loss: 1.0283914804458618
Epoch 480, training loss: 13.391850471496582 = 0.7409642934799194 + 2.0 * 6.325443267822266
Epoch 480, val loss: 1.0092514753341675
Epoch 490, training loss: 13.361169815063477 = 0.7144899368286133 + 2.0 * 6.323339939117432
Epoch 490, val loss: 0.9904353618621826
Epoch 500, training loss: 13.32823657989502 = 0.688650369644165 + 2.0 * 6.319793224334717
Epoch 500, val loss: 0.9719840884208679
Epoch 510, training loss: 13.300154685974121 = 0.6633846163749695 + 2.0 * 6.318385124206543
Epoch 510, val loss: 0.9541150331497192
Epoch 520, training loss: 13.281715393066406 = 0.6386899948120117 + 2.0 * 6.321512699127197
Epoch 520, val loss: 0.936747133731842
Epoch 530, training loss: 13.245575904846191 = 0.6146824359893799 + 2.0 * 6.315446853637695
Epoch 530, val loss: 0.9201645851135254
Epoch 540, training loss: 13.220602035522461 = 0.5913692712783813 + 2.0 * 6.3146162033081055
Epoch 540, val loss: 0.9045498967170715
Epoch 550, training loss: 13.191839218139648 = 0.5686705112457275 + 2.0 * 6.31158447265625
Epoch 550, val loss: 0.8898040056228638
Epoch 560, training loss: 13.17950439453125 = 0.5465469360351562 + 2.0 * 6.316478729248047
Epoch 560, val loss: 0.8758769035339355
Epoch 570, training loss: 13.1452054977417 = 0.5250565409660339 + 2.0 * 6.310074329376221
Epoch 570, val loss: 0.8628258109092712
Epoch 580, training loss: 13.118480682373047 = 0.5043714046478271 + 2.0 * 6.30705451965332
Epoch 580, val loss: 0.8510838747024536
Epoch 590, training loss: 13.095053672790527 = 0.48428791761398315 + 2.0 * 6.30538272857666
Epoch 590, val loss: 0.8403686285018921
Epoch 600, training loss: 13.07308292388916 = 0.4647553265094757 + 2.0 * 6.304163932800293
Epoch 600, val loss: 0.8305175304412842
Epoch 610, training loss: 13.063825607299805 = 0.4457990825176239 + 2.0 * 6.309013366699219
Epoch 610, val loss: 0.8215915560722351
Epoch 620, training loss: 13.034467697143555 = 0.4274837076663971 + 2.0 * 6.303492069244385
Epoch 620, val loss: 0.8134356737136841
Epoch 630, training loss: 13.010831832885742 = 0.4098232388496399 + 2.0 * 6.300504207611084
Epoch 630, val loss: 0.8063014149665833
Epoch 640, training loss: 12.9910249710083 = 0.39276474714279175 + 2.0 * 6.299129962921143
Epoch 640, val loss: 0.8000592589378357
Epoch 650, training loss: 12.987133026123047 = 0.3762032389640808 + 2.0 * 6.305464744567871
Epoch 650, val loss: 0.7944410443305969
Epoch 660, training loss: 12.954565048217773 = 0.3603692054748535 + 2.0 * 6.297097682952881
Epoch 660, val loss: 0.7894746661186218
Epoch 670, training loss: 12.937390327453613 = 0.34508609771728516 + 2.0 * 6.296152114868164
Epoch 670, val loss: 0.7853342890739441
Epoch 680, training loss: 12.917551040649414 = 0.3303464949131012 + 2.0 * 6.293602466583252
Epoch 680, val loss: 0.7817915678024292
Epoch 690, training loss: 12.902668952941895 = 0.3161000907421112 + 2.0 * 6.2932844161987305
Epoch 690, val loss: 0.7787617444992065
Epoch 700, training loss: 12.895344734191895 = 0.3023599088191986 + 2.0 * 6.296492576599121
Epoch 700, val loss: 0.7760895490646362
Epoch 710, training loss: 12.87313461303711 = 0.2892230749130249 + 2.0 * 6.291955947875977
Epoch 710, val loss: 0.7740172147750854
Epoch 720, training loss: 12.855243682861328 = 0.276633620262146 + 2.0 * 6.289305210113525
Epoch 720, val loss: 0.7724904417991638
Epoch 730, training loss: 12.842055320739746 = 0.26453009247779846 + 2.0 * 6.28876256942749
Epoch 730, val loss: 0.7713422775268555
Epoch 740, training loss: 12.827066421508789 = 0.2529003918170929 + 2.0 * 6.287083148956299
Epoch 740, val loss: 0.7706316709518433
Epoch 750, training loss: 12.831574440002441 = 0.24170979857444763 + 2.0 * 6.2949323654174805
Epoch 750, val loss: 0.7701234221458435
Epoch 760, training loss: 12.808577537536621 = 0.23105593025684357 + 2.0 * 6.288760662078857
Epoch 760, val loss: 0.7700631022453308
Epoch 770, training loss: 12.78996467590332 = 0.22087889909744263 + 2.0 * 6.284543037414551
Epoch 770, val loss: 0.7704632878303528
Epoch 780, training loss: 12.778966903686523 = 0.21112416684627533 + 2.0 * 6.283921241760254
Epoch 780, val loss: 0.771109938621521
Epoch 790, training loss: 12.771498680114746 = 0.20180681347846985 + 2.0 * 6.28484582901001
Epoch 790, val loss: 0.7719435691833496
Epoch 800, training loss: 12.758269309997559 = 0.1929580122232437 + 2.0 * 6.282655715942383
Epoch 800, val loss: 0.7731645107269287
Epoch 810, training loss: 12.74666690826416 = 0.18452496826648712 + 2.0 * 6.281071186065674
Epoch 810, val loss: 0.7747446894645691
Epoch 820, training loss: 12.73755168914795 = 0.17644236981868744 + 2.0 * 6.28055477142334
Epoch 820, val loss: 0.7764779329299927
Epoch 830, training loss: 12.728395462036133 = 0.16875667870044708 + 2.0 * 6.279819488525391
Epoch 830, val loss: 0.7782106399536133
Epoch 840, training loss: 12.720212936401367 = 0.161459282040596 + 2.0 * 6.279376983642578
Epoch 840, val loss: 0.7804533839225769
Epoch 850, training loss: 12.708392143249512 = 0.15451228618621826 + 2.0 * 6.276939868927002
Epoch 850, val loss: 0.7828347086906433
Epoch 860, training loss: 12.700786590576172 = 0.14787930250167847 + 2.0 * 6.276453495025635
Epoch 860, val loss: 0.7853342294692993
Epoch 870, training loss: 12.711013793945312 = 0.14156439900398254 + 2.0 * 6.284724712371826
Epoch 870, val loss: 0.7878604531288147
Epoch 880, training loss: 12.690704345703125 = 0.13562040030956268 + 2.0 * 6.2775421142578125
Epoch 880, val loss: 0.7906769514083862
Epoch 890, training loss: 12.67785930633545 = 0.12998762726783752 + 2.0 * 6.273935794830322
Epoch 890, val loss: 0.7937911152839661
Epoch 900, training loss: 12.669910430908203 = 0.12462227046489716 + 2.0 * 6.27264404296875
Epoch 900, val loss: 0.796935498714447
Epoch 910, training loss: 12.66418170928955 = 0.11949927359819412 + 2.0 * 6.272341251373291
Epoch 910, val loss: 0.8001988530158997
Epoch 920, training loss: 12.66818904876709 = 0.11462387442588806 + 2.0 * 6.276782512664795
Epoch 920, val loss: 0.8035091757774353
Epoch 930, training loss: 12.654458045959473 = 0.10999467968940735 + 2.0 * 6.272231578826904
Epoch 930, val loss: 0.8068869113922119
Epoch 940, training loss: 12.654122352600098 = 0.10560292750597 + 2.0 * 6.274259567260742
Epoch 940, val loss: 0.810438334941864
Epoch 950, training loss: 12.643884658813477 = 0.10143877565860748 + 2.0 * 6.271223068237305
Epoch 950, val loss: 0.8140267133712769
Epoch 960, training loss: 12.635581016540527 = 0.09748014807701111 + 2.0 * 6.269050598144531
Epoch 960, val loss: 0.8177558779716492
Epoch 970, training loss: 12.630666732788086 = 0.09370259940624237 + 2.0 * 6.268482208251953
Epoch 970, val loss: 0.821561336517334
Epoch 980, training loss: 12.636930465698242 = 0.09010480344295502 + 2.0 * 6.273412704467773
Epoch 980, val loss: 0.825319230556488
Epoch 990, training loss: 12.624642372131348 = 0.08666270971298218 + 2.0 * 6.2689900398254395
Epoch 990, val loss: 0.8290647864341736
Epoch 1000, training loss: 12.617837905883789 = 0.08340650796890259 + 2.0 * 6.267215728759766
Epoch 1000, val loss: 0.8330698609352112
Epoch 1010, training loss: 12.614485740661621 = 0.08029128611087799 + 2.0 * 6.267096996307373
Epoch 1010, val loss: 0.8369954824447632
Epoch 1020, training loss: 12.60848617553711 = 0.07731686532497406 + 2.0 * 6.265584468841553
Epoch 1020, val loss: 0.840945303440094
Epoch 1030, training loss: 12.608179092407227 = 0.07447537779808044 + 2.0 * 6.266851902008057
Epoch 1030, val loss: 0.8449319005012512
Epoch 1040, training loss: 12.605365753173828 = 0.07177634537220001 + 2.0 * 6.266794681549072
Epoch 1040, val loss: 0.8488448262214661
Epoch 1050, training loss: 12.596405029296875 = 0.06920254230499268 + 2.0 * 6.263601303100586
Epoch 1050, val loss: 0.8529638648033142
Epoch 1060, training loss: 12.597311973571777 = 0.06674780696630478 + 2.0 * 6.265282154083252
Epoch 1060, val loss: 0.8570117950439453
Epoch 1070, training loss: 12.590453147888184 = 0.06440653651952744 + 2.0 * 6.263023376464844
Epoch 1070, val loss: 0.8609907627105713
Epoch 1080, training loss: 12.584736824035645 = 0.06216823309659958 + 2.0 * 6.261284351348877
Epoch 1080, val loss: 0.8650875687599182
Epoch 1090, training loss: 12.582402229309082 = 0.06002700328826904 + 2.0 * 6.261187553405762
Epoch 1090, val loss: 0.8692034482955933
Epoch 1100, training loss: 12.586874008178711 = 0.05797545611858368 + 2.0 * 6.264449119567871
Epoch 1100, val loss: 0.873202919960022
Epoch 1110, training loss: 12.587594032287598 = 0.05602807179093361 + 2.0 * 6.265782833099365
Epoch 1110, val loss: 0.8771047592163086
Epoch 1120, training loss: 12.576933860778809 = 0.05416344478726387 + 2.0 * 6.261385440826416
Epoch 1120, val loss: 0.8812463283538818
Epoch 1130, training loss: 12.569746971130371 = 0.05239073932170868 + 2.0 * 6.258677959442139
Epoch 1130, val loss: 0.8853505253791809
Epoch 1140, training loss: 12.565926551818848 = 0.0506826713681221 + 2.0 * 6.257621765136719
Epoch 1140, val loss: 0.8893734812736511
Epoch 1150, training loss: 12.569032669067383 = 0.04904729127883911 + 2.0 * 6.259992599487305
Epoch 1150, val loss: 0.8934458494186401
Epoch 1160, training loss: 12.56737995147705 = 0.04747980460524559 + 2.0 * 6.259950160980225
Epoch 1160, val loss: 0.8972296714782715
Epoch 1170, training loss: 12.562767028808594 = 0.0459841713309288 + 2.0 * 6.258391380310059
Epoch 1170, val loss: 0.9012455344200134
Epoch 1180, training loss: 12.557103157043457 = 0.04455357417464256 + 2.0 * 6.256274700164795
Epoch 1180, val loss: 0.9053056240081787
Epoch 1190, training loss: 12.563515663146973 = 0.04317578673362732 + 2.0 * 6.260169982910156
Epoch 1190, val loss: 0.9091023206710815
Epoch 1200, training loss: 12.555675506591797 = 0.04185980185866356 + 2.0 * 6.256907939910889
Epoch 1200, val loss: 0.9129549860954285
Epoch 1210, training loss: 12.551592826843262 = 0.040596239268779755 + 2.0 * 6.25549840927124
Epoch 1210, val loss: 0.9169138073921204
Epoch 1220, training loss: 12.54860782623291 = 0.039383240044116974 + 2.0 * 6.254612445831299
Epoch 1220, val loss: 0.9208475351333618
Epoch 1230, training loss: 12.551178932189941 = 0.03821581229567528 + 2.0 * 6.256481647491455
Epoch 1230, val loss: 0.9246697425842285
Epoch 1240, training loss: 12.551583290100098 = 0.037093743681907654 + 2.0 * 6.25724458694458
Epoch 1240, val loss: 0.9283941984176636
Epoch 1250, training loss: 12.543761253356934 = 0.036015599966049194 + 2.0 * 6.253872871398926
Epoch 1250, val loss: 0.9320849776268005
Epoch 1260, training loss: 12.540782928466797 = 0.03498777747154236 + 2.0 * 6.2528977394104
Epoch 1260, val loss: 0.9360256195068359
Epoch 1270, training loss: 12.538119316101074 = 0.03399670496582985 + 2.0 * 6.252061367034912
Epoch 1270, val loss: 0.9397403597831726
Epoch 1280, training loss: 12.535730361938477 = 0.033039480447769165 + 2.0 * 6.251345634460449
Epoch 1280, val loss: 0.94345623254776
Epoch 1290, training loss: 12.545245170593262 = 0.03211873024702072 + 2.0 * 6.256563186645508
Epoch 1290, val loss: 0.9471336007118225
Epoch 1300, training loss: 12.547805786132812 = 0.03123675473034382 + 2.0 * 6.258284568786621
Epoch 1300, val loss: 0.9506065249443054
Epoch 1310, training loss: 12.532575607299805 = 0.030386365950107574 + 2.0 * 6.251094818115234
Epoch 1310, val loss: 0.9542986750602722
Epoch 1320, training loss: 12.529020309448242 = 0.02957378514111042 + 2.0 * 6.249723434448242
Epoch 1320, val loss: 0.9579952955245972
Epoch 1330, training loss: 12.528021812438965 = 0.02878953330218792 + 2.0 * 6.2496161460876465
Epoch 1330, val loss: 0.9615505933761597
Epoch 1340, training loss: 12.559258460998535 = 0.02803816832602024 + 2.0 * 6.265610218048096
Epoch 1340, val loss: 0.96512371301651
Epoch 1350, training loss: 12.533327102661133 = 0.02730405703186989 + 2.0 * 6.253011703491211
Epoch 1350, val loss: 0.968271791934967
Epoch 1360, training loss: 12.522648811340332 = 0.02660878375172615 + 2.0 * 6.248020172119141
Epoch 1360, val loss: 0.9719581604003906
Epoch 1370, training loss: 12.521408081054688 = 0.025935707613825798 + 2.0 * 6.247735977172852
Epoch 1370, val loss: 0.9754999279975891
Epoch 1380, training loss: 12.520140647888184 = 0.02528342977166176 + 2.0 * 6.2474284172058105
Epoch 1380, val loss: 0.9788506627082825
Epoch 1390, training loss: 12.54341983795166 = 0.02465372160077095 + 2.0 * 6.259383201599121
Epoch 1390, val loss: 0.982172429561615
Epoch 1400, training loss: 12.525744438171387 = 0.024049146100878716 + 2.0 * 6.250847816467285
Epoch 1400, val loss: 0.9855139851570129
Epoch 1410, training loss: 12.515641212463379 = 0.023464778438210487 + 2.0 * 6.246088027954102
Epoch 1410, val loss: 0.9889335632324219
Epoch 1420, training loss: 12.51543140411377 = 0.022901011630892754 + 2.0 * 6.246265411376953
Epoch 1420, val loss: 0.9922243356704712
Epoch 1430, training loss: 12.52367877960205 = 0.022357655689120293 + 2.0 * 6.250660419464111
Epoch 1430, val loss: 0.9954230189323425
Epoch 1440, training loss: 12.518745422363281 = 0.021827194839715958 + 2.0 * 6.248459339141846
Epoch 1440, val loss: 0.998496413230896
Epoch 1450, training loss: 12.515498161315918 = 0.021320728585124016 + 2.0 * 6.24708890914917
Epoch 1450, val loss: 1.0017167329788208
Epoch 1460, training loss: 12.513978958129883 = 0.020829278975725174 + 2.0 * 6.246574878692627
Epoch 1460, val loss: 1.0049175024032593
Epoch 1470, training loss: 12.513175010681152 = 0.020356332883238792 + 2.0 * 6.2464094161987305
Epoch 1470, val loss: 1.0079537630081177
Epoch 1480, training loss: 12.508837699890137 = 0.01989729516208172 + 2.0 * 6.244470119476318
Epoch 1480, val loss: 1.0111191272735596
Epoch 1490, training loss: 12.518641471862793 = 0.01945420168340206 + 2.0 * 6.249593734741211
Epoch 1490, val loss: 1.014215350151062
Epoch 1500, training loss: 12.509390830993652 = 0.01902308687567711 + 2.0 * 6.245183944702148
Epoch 1500, val loss: 1.017037034034729
Epoch 1510, training loss: 12.505230903625488 = 0.01860594004392624 + 2.0 * 6.243312358856201
Epoch 1510, val loss: 1.0201339721679688
Epoch 1520, training loss: 12.505111694335938 = 0.018203550949692726 + 2.0 * 6.2434539794921875
Epoch 1520, val loss: 1.0231220722198486
Epoch 1530, training loss: 12.508855819702148 = 0.017812324687838554 + 2.0 * 6.245521545410156
Epoch 1530, val loss: 1.0260202884674072
Epoch 1540, training loss: 12.511336326599121 = 0.017434027045965195 + 2.0 * 6.246951103210449
Epoch 1540, val loss: 1.028856635093689
Epoch 1550, training loss: 12.503777503967285 = 0.017069481313228607 + 2.0 * 6.243353843688965
Epoch 1550, val loss: 1.0317209959030151
Epoch 1560, training loss: 12.502963066101074 = 0.016716398298740387 + 2.0 * 6.243123531341553
Epoch 1560, val loss: 1.034590482711792
Epoch 1570, training loss: 12.507097244262695 = 0.01637428067624569 + 2.0 * 6.245361328125
Epoch 1570, val loss: 1.037383794784546
Epoch 1580, training loss: 12.498422622680664 = 0.016039889305830002 + 2.0 * 6.241191387176514
Epoch 1580, val loss: 1.040027141571045
Epoch 1590, training loss: 12.499225616455078 = 0.015717023983597755 + 2.0 * 6.241754531860352
Epoch 1590, val loss: 1.0428154468536377
Epoch 1600, training loss: 12.50145149230957 = 0.015403486788272858 + 2.0 * 6.243023872375488
Epoch 1600, val loss: 1.0454868078231812
Epoch 1610, training loss: 12.505208015441895 = 0.015100390650331974 + 2.0 * 6.245053768157959
Epoch 1610, val loss: 1.0481282472610474
Epoch 1620, training loss: 12.496472358703613 = 0.014804186299443245 + 2.0 * 6.2408342361450195
Epoch 1620, val loss: 1.0508232116699219
Epoch 1630, training loss: 12.502812385559082 = 0.014518413692712784 + 2.0 * 6.244146823883057
Epoch 1630, val loss: 1.0534210205078125
Epoch 1640, training loss: 12.49325942993164 = 0.014242541044950485 + 2.0 * 6.239508628845215
Epoch 1640, val loss: 1.0559366941452026
Epoch 1650, training loss: 12.492352485656738 = 0.013974690809845924 + 2.0 * 6.2391886711120605
Epoch 1650, val loss: 1.0585837364196777
Epoch 1660, training loss: 12.494535446166992 = 0.013712250627577305 + 2.0 * 6.240411758422852
Epoch 1660, val loss: 1.061115026473999
Epoch 1670, training loss: 12.496395111083984 = 0.013458131812512875 + 2.0 * 6.24146842956543
Epoch 1670, val loss: 1.0635063648223877
Epoch 1680, training loss: 12.491327285766602 = 0.013210306875407696 + 2.0 * 6.239058494567871
Epoch 1680, val loss: 1.0660938024520874
Epoch 1690, training loss: 12.490020751953125 = 0.01297034602612257 + 2.0 * 6.238525390625
Epoch 1690, val loss: 1.0686089992523193
Epoch 1700, training loss: 12.495591163635254 = 0.012736142612993717 + 2.0 * 6.241427421569824
Epoch 1700, val loss: 1.0709809064865112
Epoch 1710, training loss: 12.487659454345703 = 0.012508708983659744 + 2.0 * 6.237575531005859
Epoch 1710, val loss: 1.0733494758605957
Epoch 1720, training loss: 12.488651275634766 = 0.012288527563214302 + 2.0 * 6.2381815910339355
Epoch 1720, val loss: 1.0758723020553589
Epoch 1730, training loss: 12.49532699584961 = 0.012073846533894539 + 2.0 * 6.241626739501953
Epoch 1730, val loss: 1.0781692266464233
Epoch 1740, training loss: 12.487807273864746 = 0.011863122694194317 + 2.0 * 6.237972259521484
Epoch 1740, val loss: 1.0803881883621216
Epoch 1750, training loss: 12.485811233520508 = 0.011660362593829632 + 2.0 * 6.237075328826904
Epoch 1750, val loss: 1.0828306674957275
Epoch 1760, training loss: 12.495022773742676 = 0.011463276110589504 + 2.0 * 6.241779804229736
Epoch 1760, val loss: 1.085127592086792
Epoch 1770, training loss: 12.484745025634766 = 0.011267012916505337 + 2.0 * 6.236739158630371
Epoch 1770, val loss: 1.0872257947921753
Epoch 1780, training loss: 12.48635196685791 = 0.011079208925366402 + 2.0 * 6.237636566162109
Epoch 1780, val loss: 1.0895408391952515
Epoch 1790, training loss: 12.485669136047363 = 0.01089702919125557 + 2.0 * 6.237386226654053
Epoch 1790, val loss: 1.091762900352478
Epoch 1800, training loss: 12.482568740844727 = 0.010717376135289669 + 2.0 * 6.235925674438477
Epoch 1800, val loss: 1.0939561128616333
Epoch 1810, training loss: 12.484183311462402 = 0.010543559677898884 + 2.0 * 6.236819744110107
Epoch 1810, val loss: 1.0962045192718506
Epoch 1820, training loss: 12.484230041503906 = 0.010373963974416256 + 2.0 * 6.2369279861450195
Epoch 1820, val loss: 1.0982840061187744
Epoch 1830, training loss: 12.485844612121582 = 0.01020788960158825 + 2.0 * 6.237818241119385
Epoch 1830, val loss: 1.1003165245056152
Epoch 1840, training loss: 12.479073524475098 = 0.010046510025858879 + 2.0 * 6.234513282775879
Epoch 1840, val loss: 1.1025054454803467
Epoch 1850, training loss: 12.481696128845215 = 0.009889043867588043 + 2.0 * 6.235903739929199
Epoch 1850, val loss: 1.1045629978179932
Epoch 1860, training loss: 12.482240676879883 = 0.009735066443681717 + 2.0 * 6.236252784729004
Epoch 1860, val loss: 1.1065064668655396
Epoch 1870, training loss: 12.479104995727539 = 0.009585043415427208 + 2.0 * 6.23475980758667
Epoch 1870, val loss: 1.108660101890564
Epoch 1880, training loss: 12.480010032653809 = 0.009438760578632355 + 2.0 * 6.235285758972168
Epoch 1880, val loss: 1.1106736660003662
Epoch 1890, training loss: 12.487319946289062 = 0.009295173920691013 + 2.0 * 6.239012241363525
Epoch 1890, val loss: 1.1124838590621948
Epoch 1900, training loss: 12.478020668029785 = 0.009158062748610973 + 2.0 * 6.234431266784668
Epoch 1900, val loss: 1.1144158840179443
Epoch 1910, training loss: 12.474313735961914 = 0.009022885002195835 + 2.0 * 6.232645511627197
Epoch 1910, val loss: 1.1164910793304443
Epoch 1920, training loss: 12.472831726074219 = 0.008891456760466099 + 2.0 * 6.231970310211182
Epoch 1920, val loss: 1.1184747219085693
Epoch 1930, training loss: 12.473362922668457 = 0.008761436678469181 + 2.0 * 6.232300758361816
Epoch 1930, val loss: 1.1203526258468628
Epoch 1940, training loss: 12.483673095703125 = 0.008634323254227638 + 2.0 * 6.237519264221191
Epoch 1940, val loss: 1.1222093105316162
Epoch 1950, training loss: 12.477015495300293 = 0.008510655723512173 + 2.0 * 6.234252452850342
Epoch 1950, val loss: 1.1240556240081787
Epoch 1960, training loss: 12.478706359863281 = 0.008390279486775398 + 2.0 * 6.2351579666137695
Epoch 1960, val loss: 1.1259444952011108
Epoch 1970, training loss: 12.474754333496094 = 0.008271781727671623 + 2.0 * 6.233241081237793
Epoch 1970, val loss: 1.1276615858078003
Epoch 1980, training loss: 12.4739351272583 = 0.00815640203654766 + 2.0 * 6.232889175415039
Epoch 1980, val loss: 1.129455327987671
Epoch 1990, training loss: 12.470569610595703 = 0.00804403517395258 + 2.0 * 6.231262683868408
Epoch 1990, val loss: 1.1313966512680054
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.825925925925926
0.845018450184502
=== training gcn model ===
Epoch 0, training loss: 19.158796310424805 = 1.9651864767074585 + 2.0 * 8.59680461883545
Epoch 0, val loss: 1.9628676176071167
Epoch 10, training loss: 19.14704704284668 = 1.954403042793274 + 2.0 * 8.596322059631348
Epoch 10, val loss: 1.9521671533584595
Epoch 20, training loss: 19.125526428222656 = 1.9408769607543945 + 2.0 * 8.592325210571289
Epoch 20, val loss: 1.9380027055740356
Epoch 30, training loss: 19.048999786376953 = 1.9222664833068848 + 2.0 * 8.563366889953613
Epoch 30, val loss: 1.9180657863616943
Epoch 40, training loss: 18.708938598632812 = 1.90015709400177 + 2.0 * 8.404390335083008
Epoch 40, val loss: 1.8953883647918701
Epoch 50, training loss: 17.485462188720703 = 1.8764877319335938 + 2.0 * 7.8044867515563965
Epoch 50, val loss: 1.8715770244598389
Epoch 60, training loss: 16.5352725982666 = 1.8560073375701904 + 2.0 * 7.339632987976074
Epoch 60, val loss: 1.85275399684906
Epoch 70, training loss: 15.958839416503906 = 1.8400355577468872 + 2.0 * 7.059401988983154
Epoch 70, val loss: 1.8371270895004272
Epoch 80, training loss: 15.667597770690918 = 1.8234269618988037 + 2.0 * 6.922085285186768
Epoch 80, val loss: 1.8212026357650757
Epoch 90, training loss: 15.459210395812988 = 1.8081996440887451 + 2.0 * 6.825505256652832
Epoch 90, val loss: 1.8067387342453003
Epoch 100, training loss: 15.30593490600586 = 1.7938916683197021 + 2.0 * 6.756021499633789
Epoch 100, val loss: 1.7938858270645142
Epoch 110, training loss: 15.184181213378906 = 1.7810962200164795 + 2.0 * 6.701542377471924
Epoch 110, val loss: 1.7827256917953491
Epoch 120, training loss: 15.078311920166016 = 1.7693597078323364 + 2.0 * 6.654476165771484
Epoch 120, val loss: 1.7724947929382324
Epoch 130, training loss: 14.98250675201416 = 1.7579845190048218 + 2.0 * 6.6122612953186035
Epoch 130, val loss: 1.7625528573989868
Epoch 140, training loss: 14.897115707397461 = 1.7459224462509155 + 2.0 * 6.575596809387207
Epoch 140, val loss: 1.7520980834960938
Epoch 150, training loss: 14.822628021240234 = 1.7329578399658203 + 2.0 * 6.544835090637207
Epoch 150, val loss: 1.74091374874115
Epoch 160, training loss: 14.756258010864258 = 1.7188022136688232 + 2.0 * 6.518727779388428
Epoch 160, val loss: 1.7286884784698486
Epoch 170, training loss: 14.699074745178223 = 1.7028197050094604 + 2.0 * 6.498127460479736
Epoch 170, val loss: 1.7149330377578735
Epoch 180, training loss: 14.653212547302246 = 1.684687852859497 + 2.0 * 6.484262466430664
Epoch 180, val loss: 1.6993824243545532
Epoch 190, training loss: 14.599833488464355 = 1.6645342111587524 + 2.0 * 6.467649459838867
Epoch 190, val loss: 1.6821452379226685
Epoch 200, training loss: 14.550915718078613 = 1.6419919729232788 + 2.0 * 6.454462051391602
Epoch 200, val loss: 1.6629940271377563
Epoch 210, training loss: 14.504236221313477 = 1.616593599319458 + 2.0 * 6.443821430206299
Epoch 210, val loss: 1.6415022611618042
Epoch 220, training loss: 14.468355178833008 = 1.588006854057312 + 2.0 * 6.440174102783203
Epoch 220, val loss: 1.617544174194336
Epoch 230, training loss: 14.414520263671875 = 1.5570647716522217 + 2.0 * 6.428727626800537
Epoch 230, val loss: 1.5915558338165283
Epoch 240, training loss: 14.362041473388672 = 1.5233172178268433 + 2.0 * 6.4193620681762695
Epoch 240, val loss: 1.5634831190109253
Epoch 250, training loss: 14.310178756713867 = 1.4863582849502563 + 2.0 * 6.411910057067871
Epoch 250, val loss: 1.53277587890625
Epoch 260, training loss: 14.256770133972168 = 1.4461702108383179 + 2.0 * 6.405300140380859
Epoch 260, val loss: 1.4995843172073364
Epoch 270, training loss: 14.200800895690918 = 1.402929663658142 + 2.0 * 6.398935794830322
Epoch 270, val loss: 1.4642138481140137
Epoch 280, training loss: 14.148351669311523 = 1.3571666479110718 + 2.0 * 6.39559268951416
Epoch 280, val loss: 1.427250862121582
Epoch 290, training loss: 14.087790489196777 = 1.3103514909744263 + 2.0 * 6.38871955871582
Epoch 290, val loss: 1.389668583869934
Epoch 300, training loss: 14.029500007629395 = 1.2624131441116333 + 2.0 * 6.383543491363525
Epoch 300, val loss: 1.3515814542770386
Epoch 310, training loss: 13.969639778137207 = 1.213537573814392 + 2.0 * 6.378051280975342
Epoch 310, val loss: 1.3129559755325317
Epoch 320, training loss: 13.912848472595215 = 1.1643427610397339 + 2.0 * 6.374252796173096
Epoch 320, val loss: 1.2742156982421875
Epoch 330, training loss: 13.859247207641602 = 1.1157785654067993 + 2.0 * 6.371734142303467
Epoch 330, val loss: 1.2362421751022339
Epoch 340, training loss: 13.799270629882812 = 1.0686120986938477 + 2.0 * 6.365329265594482
Epoch 340, val loss: 1.1993482112884521
Epoch 350, training loss: 13.745959281921387 = 1.0226365327835083 + 2.0 * 6.361661434173584
Epoch 350, val loss: 1.1635375022888184
Epoch 360, training loss: 13.701671600341797 = 0.9783897399902344 + 2.0 * 6.361640930175781
Epoch 360, val loss: 1.1291691064834595
Epoch 370, training loss: 13.647300720214844 = 0.936517059803009 + 2.0 * 6.355391979217529
Epoch 370, val loss: 1.0967272520065308
Epoch 380, training loss: 13.598125457763672 = 0.8964828848838806 + 2.0 * 6.350821495056152
Epoch 380, val loss: 1.0659871101379395
Epoch 390, training loss: 13.552938461303711 = 0.858123242855072 + 2.0 * 6.347407817840576
Epoch 390, val loss: 1.0367552042007446
Epoch 400, training loss: 13.51440715789795 = 0.8215206265449524 + 2.0 * 6.346443176269531
Epoch 400, val loss: 1.009206771850586
Epoch 410, training loss: 13.47183609008789 = 0.7870295643806458 + 2.0 * 6.342403411865234
Epoch 410, val loss: 0.9835660457611084
Epoch 420, training loss: 13.431483268737793 = 0.7541729211807251 + 2.0 * 6.3386549949646
Epoch 420, val loss: 0.9596617817878723
Epoch 430, training loss: 13.39441204071045 = 0.7227495312690735 + 2.0 * 6.335831165313721
Epoch 430, val loss: 0.9372414946556091
Epoch 440, training loss: 13.370589256286621 = 0.6929023861885071 + 2.0 * 6.33884334564209
Epoch 440, val loss: 0.9162836074829102
Epoch 450, training loss: 13.330613136291504 = 0.664794385433197 + 2.0 * 6.33290958404541
Epoch 450, val loss: 0.8972103595733643
Epoch 460, training loss: 13.293230056762695 = 0.6379798054695129 + 2.0 * 6.327625274658203
Epoch 460, val loss: 0.8796856999397278
Epoch 470, training loss: 13.26319408416748 = 0.6121750473976135 + 2.0 * 6.325509548187256
Epoch 470, val loss: 0.863276481628418
Epoch 480, training loss: 13.252303123474121 = 0.5873669385910034 + 2.0 * 6.332468032836914
Epoch 480, val loss: 0.8478923439979553
Epoch 490, training loss: 13.209239959716797 = 0.5635428428649902 + 2.0 * 6.322848320007324
Epoch 490, val loss: 0.833786129951477
Epoch 500, training loss: 13.178621292114258 = 0.5405142307281494 + 2.0 * 6.319053649902344
Epoch 500, val loss: 0.8207648396492004
Epoch 510, training loss: 13.162922859191895 = 0.518096387386322 + 2.0 * 6.322413444519043
Epoch 510, val loss: 0.8084645867347717
Epoch 520, training loss: 13.132452964782715 = 0.4962604343891144 + 2.0 * 6.318096160888672
Epoch 520, val loss: 0.7970650792121887
Epoch 530, training loss: 13.104190826416016 = 0.47513189911842346 + 2.0 * 6.3145294189453125
Epoch 530, val loss: 0.7865921854972839
Epoch 540, training loss: 13.07767105102539 = 0.4543936252593994 + 2.0 * 6.311638832092285
Epoch 540, val loss: 0.7767431139945984
Epoch 550, training loss: 13.054693222045898 = 0.43401849269866943 + 2.0 * 6.310337543487549
Epoch 550, val loss: 0.7675492167472839
Epoch 560, training loss: 13.03343677520752 = 0.41409116983413696 + 2.0 * 6.309672832489014
Epoch 560, val loss: 0.7591409087181091
Epoch 570, training loss: 13.009635925292969 = 0.3947521150112152 + 2.0 * 6.307441711425781
Epoch 570, val loss: 0.7515250444412231
Epoch 580, training loss: 12.985142707824707 = 0.3758094906806946 + 2.0 * 6.304666519165039
Epoch 580, val loss: 0.7445809841156006
Epoch 590, training loss: 12.964431762695312 = 0.35727131366729736 + 2.0 * 6.303580284118652
Epoch 590, val loss: 0.7382691502571106
Epoch 600, training loss: 12.949710845947266 = 0.3391818106174469 + 2.0 * 6.305264472961426
Epoch 600, val loss: 0.7326672673225403
Epoch 610, training loss: 12.92664909362793 = 0.32163020968437195 + 2.0 * 6.302509307861328
Epoch 610, val loss: 0.7279330492019653
Epoch 620, training loss: 12.90344524383545 = 0.3046126067638397 + 2.0 * 6.299416542053223
Epoch 620, val loss: 0.7239429950714111
Epoch 630, training loss: 12.889921188354492 = 0.2880142629146576 + 2.0 * 6.300953388214111
Epoch 630, val loss: 0.7205426096916199
Epoch 640, training loss: 12.873026847839355 = 0.2719615697860718 + 2.0 * 6.300532817840576
Epoch 640, val loss: 0.717776358127594
Epoch 650, training loss: 12.847859382629395 = 0.256621778011322 + 2.0 * 6.295619010925293
Epoch 650, val loss: 0.7159023880958557
Epoch 660, training loss: 12.83057689666748 = 0.24185451865196228 + 2.0 * 6.294361114501953
Epoch 660, val loss: 0.7146850228309631
Epoch 670, training loss: 12.815518379211426 = 0.22770549356937408 + 2.0 * 6.293906211853027
Epoch 670, val loss: 0.7141088843345642
Epoch 680, training loss: 12.812488555908203 = 0.2144373059272766 + 2.0 * 6.299025535583496
Epoch 680, val loss: 0.7141697406768799
Epoch 690, training loss: 12.789271354675293 = 0.20203837752342224 + 2.0 * 6.29361629486084
Epoch 690, val loss: 0.7151236534118652
Epoch 700, training loss: 12.770716667175293 = 0.1904100626707077 + 2.0 * 6.290153503417969
Epoch 700, val loss: 0.7167168855667114
Epoch 710, training loss: 12.756107330322266 = 0.179484561085701 + 2.0 * 6.28831148147583
Epoch 710, val loss: 0.718761146068573
Epoch 720, training loss: 12.74519157409668 = 0.16925576329231262 + 2.0 * 6.287967681884766
Epoch 720, val loss: 0.7213819026947021
Epoch 730, training loss: 12.734213829040527 = 0.15974093973636627 + 2.0 * 6.287236213684082
Epoch 730, val loss: 0.724449872970581
Epoch 740, training loss: 12.724562644958496 = 0.15094046294689178 + 2.0 * 6.286810874938965
Epoch 740, val loss: 0.7280780673027039
Epoch 750, training loss: 12.711955070495605 = 0.14275404810905457 + 2.0 * 6.284600734710693
Epoch 750, val loss: 0.7320982217788696
Epoch 760, training loss: 12.703228950500488 = 0.13510891795158386 + 2.0 * 6.284060001373291
Epoch 760, val loss: 0.7364166378974915
Epoch 770, training loss: 12.69501781463623 = 0.12799230217933655 + 2.0 * 6.283512592315674
Epoch 770, val loss: 0.741062581539154
Epoch 780, training loss: 12.687556266784668 = 0.12140075862407684 + 2.0 * 6.283077716827393
Epoch 780, val loss: 0.7460713982582092
Epoch 790, training loss: 12.678006172180176 = 0.11526084691286087 + 2.0 * 6.281372547149658
Epoch 790, val loss: 0.751319944858551
Epoch 800, training loss: 12.667616844177246 = 0.10951687395572662 + 2.0 * 6.279049873352051
Epoch 800, val loss: 0.7567667961120605
Epoch 810, training loss: 12.671005249023438 = 0.10413199663162231 + 2.0 * 6.2834367752075195
Epoch 810, val loss: 0.7623574137687683
Epoch 820, training loss: 12.6646089553833 = 0.09913432598114014 + 2.0 * 6.2827372550964355
Epoch 820, val loss: 0.7681382298469543
Epoch 830, training loss: 12.649284362792969 = 0.09446974843740463 + 2.0 * 6.277407169342041
Epoch 830, val loss: 0.774070143699646
Epoch 840, training loss: 12.647870063781738 = 0.09010422229766846 + 2.0 * 6.27888298034668
Epoch 840, val loss: 0.7800315022468567
Epoch 850, training loss: 12.637746810913086 = 0.08601096272468567 + 2.0 * 6.275867938995361
Epoch 850, val loss: 0.7860760688781738
Epoch 860, training loss: 12.63234806060791 = 0.08218155056238174 + 2.0 * 6.275083065032959
Epoch 860, val loss: 0.7922634482383728
Epoch 870, training loss: 12.62545108795166 = 0.0785757452249527 + 2.0 * 6.2734375
Epoch 870, val loss: 0.798517644405365
Epoch 880, training loss: 12.633756637573242 = 0.0751887634396553 + 2.0 * 6.2792840003967285
Epoch 880, val loss: 0.8048422932624817
Epoch 890, training loss: 12.619498252868652 = 0.07201977074146271 + 2.0 * 6.273739337921143
Epoch 890, val loss: 0.8110998272895813
Epoch 900, training loss: 12.614736557006836 = 0.0690450370311737 + 2.0 * 6.27284574508667
Epoch 900, val loss: 0.817388117313385
Epoch 910, training loss: 12.606298446655273 = 0.06624665856361389 + 2.0 * 6.270025730133057
Epoch 910, val loss: 0.8238013982772827
Epoch 920, training loss: 12.62258529663086 = 0.063602514564991 + 2.0 * 6.279491424560547
Epoch 920, val loss: 0.8301247954368591
Epoch 930, training loss: 12.60543155670166 = 0.06110626086592674 + 2.0 * 6.272162437438965
Epoch 930, val loss: 0.8363835215568542
Epoch 940, training loss: 12.594499588012695 = 0.0587775781750679 + 2.0 * 6.2678608894348145
Epoch 940, val loss: 0.8427585363388062
Epoch 950, training loss: 12.591339111328125 = 0.05655867978930473 + 2.0 * 6.267390251159668
Epoch 950, val loss: 0.8491045236587524
Epoch 960, training loss: 12.591187477111816 = 0.054447270929813385 + 2.0 * 6.268370151519775
Epoch 960, val loss: 0.8554039597511292
Epoch 970, training loss: 12.587226867675781 = 0.0524582713842392 + 2.0 * 6.2673845291137695
Epoch 970, val loss: 0.8616623282432556
Epoch 980, training loss: 12.586540222167969 = 0.05057778209447861 + 2.0 * 6.267981052398682
Epoch 980, val loss: 0.8678942918777466
Epoch 990, training loss: 12.579270362854004 = 0.048796847462654114 + 2.0 * 6.265236854553223
Epoch 990, val loss: 0.8740618824958801
Epoch 1000, training loss: 12.5748872756958 = 0.04710797965526581 + 2.0 * 6.263889789581299
Epoch 1000, val loss: 0.8802384734153748
Epoch 1010, training loss: 12.574337005615234 = 0.045493241399526596 + 2.0 * 6.2644219398498535
Epoch 1010, val loss: 0.88637375831604
Epoch 1020, training loss: 12.572534561157227 = 0.04396214336156845 + 2.0 * 6.264286041259766
Epoch 1020, val loss: 0.8924739956855774
Epoch 1030, training loss: 12.569661140441895 = 0.04251345619559288 + 2.0 * 6.26357364654541
Epoch 1030, val loss: 0.8984565734863281
Epoch 1040, training loss: 12.564380645751953 = 0.04113667830824852 + 2.0 * 6.261621952056885
Epoch 1040, val loss: 0.9045010209083557
Epoch 1050, training loss: 12.564886093139648 = 0.039822034537792206 + 2.0 * 6.2625322341918945
Epoch 1050, val loss: 0.9104940891265869
Epoch 1060, training loss: 12.565354347229004 = 0.03856968507170677 + 2.0 * 6.263392448425293
Epoch 1060, val loss: 0.9162499308586121
Epoch 1070, training loss: 12.563106536865234 = 0.037382952868938446 + 2.0 * 6.262861728668213
Epoch 1070, val loss: 0.9220344424247742
Epoch 1080, training loss: 12.554000854492188 = 0.036253176629543304 + 2.0 * 6.25887393951416
Epoch 1080, val loss: 0.92784583568573
Epoch 1090, training loss: 12.55172348022461 = 0.035166531801223755 + 2.0 * 6.2582783699035645
Epoch 1090, val loss: 0.9335231781005859
Epoch 1100, training loss: 12.550095558166504 = 0.03412117809057236 + 2.0 * 6.257987022399902
Epoch 1100, val loss: 0.9392346739768982
Epoch 1110, training loss: 12.562267303466797 = 0.03312103822827339 + 2.0 * 6.264573097229004
Epoch 1110, val loss: 0.944908618927002
Epoch 1120, training loss: 12.549116134643555 = 0.03216737508773804 + 2.0 * 6.258474349975586
Epoch 1120, val loss: 0.9504925608634949
Epoch 1130, training loss: 12.550944328308105 = 0.031258728355169296 + 2.0 * 6.259842872619629
Epoch 1130, val loss: 0.9561028480529785
Epoch 1140, training loss: 12.542692184448242 = 0.03038192354142666 + 2.0 * 6.256155014038086
Epoch 1140, val loss: 0.9615642428398132
Epoch 1150, training loss: 12.54110050201416 = 0.029544221237301826 + 2.0 * 6.2557783126831055
Epoch 1150, val loss: 0.9669667482376099
Epoch 1160, training loss: 12.542121887207031 = 0.028739530593156815 + 2.0 * 6.256690979003906
Epoch 1160, val loss: 0.9724234342575073
Epoch 1170, training loss: 12.536860466003418 = 0.02796371467411518 + 2.0 * 6.254448413848877
Epoch 1170, val loss: 0.9777662754058838
Epoch 1180, training loss: 12.536208152770996 = 0.027221880853176117 + 2.0 * 6.254493236541748
Epoch 1180, val loss: 0.9830555319786072
Epoch 1190, training loss: 12.548108100891113 = 0.026508476585149765 + 2.0 * 6.260799884796143
Epoch 1190, val loss: 0.988189160823822
Epoch 1200, training loss: 12.534331321716309 = 0.02583225630223751 + 2.0 * 6.254249572753906
Epoch 1200, val loss: 0.9934974908828735
Epoch 1210, training loss: 12.52996826171875 = 0.02517676167190075 + 2.0 * 6.2523956298828125
Epoch 1210, val loss: 0.9986269474029541
Epoch 1220, training loss: 12.528388023376465 = 0.02454555034637451 + 2.0 * 6.2519211769104
Epoch 1220, val loss: 1.0037301778793335
Epoch 1230, training loss: 12.537861824035645 = 0.023935189470648766 + 2.0 * 6.25696325302124
Epoch 1230, val loss: 1.0087769031524658
Epoch 1240, training loss: 12.536759376525879 = 0.023348521441221237 + 2.0 * 6.256705284118652
Epoch 1240, val loss: 1.01374351978302
Epoch 1250, training loss: 12.52507209777832 = 0.02278611809015274 + 2.0 * 6.251142978668213
Epoch 1250, val loss: 1.0186461210250854
Epoch 1260, training loss: 12.521171569824219 = 0.022245420143008232 + 2.0 * 6.249463081359863
Epoch 1260, val loss: 1.023593783378601
Epoch 1270, training loss: 12.519702911376953 = 0.021719828248023987 + 2.0 * 6.2489914894104
Epoch 1270, val loss: 1.0284039974212646
Epoch 1280, training loss: 12.525168418884277 = 0.02121034450829029 + 2.0 * 6.251978874206543
Epoch 1280, val loss: 1.033126711845398
Epoch 1290, training loss: 12.53099250793457 = 0.020717253908514977 + 2.0 * 6.2551374435424805
Epoch 1290, val loss: 1.0377931594848633
Epoch 1300, training loss: 12.521267890930176 = 0.020254559814929962 + 2.0 * 6.25050687789917
Epoch 1300, val loss: 1.0425918102264404
Epoch 1310, training loss: 12.514982223510742 = 0.019801750779151917 + 2.0 * 6.247590065002441
Epoch 1310, val loss: 1.047240972518921
Epoch 1320, training loss: 12.515191078186035 = 0.019363341853022575 + 2.0 * 6.247913837432861
Epoch 1320, val loss: 1.051803469657898
Epoch 1330, training loss: 12.531587600708008 = 0.01893923617899418 + 2.0 * 6.256324291229248
Epoch 1330, val loss: 1.0562260150909424
Epoch 1340, training loss: 12.51356029510498 = 0.018528977409005165 + 2.0 * 6.247515678405762
Epoch 1340, val loss: 1.060623049736023
Epoch 1350, training loss: 12.509844779968262 = 0.018135637044906616 + 2.0 * 6.245854377746582
Epoch 1350, val loss: 1.0651038885116577
Epoch 1360, training loss: 12.508174896240234 = 0.017753414809703827 + 2.0 * 6.245210647583008
Epoch 1360, val loss: 1.0695197582244873
Epoch 1370, training loss: 12.507762908935547 = 0.017379239201545715 + 2.0 * 6.245192050933838
Epoch 1370, val loss: 1.0738056898117065
Epoch 1380, training loss: 12.519309997558594 = 0.01701618731021881 + 2.0 * 6.2511467933654785
Epoch 1380, val loss: 1.0780606269836426
Epoch 1390, training loss: 12.513612747192383 = 0.01666686125099659 + 2.0 * 6.248473167419434
Epoch 1390, val loss: 1.0822261571884155
Epoch 1400, training loss: 12.510644912719727 = 0.016329573467373848 + 2.0 * 6.247157573699951
Epoch 1400, val loss: 1.0865250825881958
Epoch 1410, training loss: 12.504528999328613 = 0.016005760058760643 + 2.0 * 6.244261741638184
Epoch 1410, val loss: 1.09073805809021
Epoch 1420, training loss: 12.502561569213867 = 0.015688853338360786 + 2.0 * 6.243436336517334
Epoch 1420, val loss: 1.0947962999343872
Epoch 1430, training loss: 12.50385570526123 = 0.015379276126623154 + 2.0 * 6.244238376617432
Epoch 1430, val loss: 1.0988467931747437
Epoch 1440, training loss: 12.504505157470703 = 0.015078914351761341 + 2.0 * 6.244713306427002
Epoch 1440, val loss: 1.1027945280075073
Epoch 1450, training loss: 12.505901336669922 = 0.014790222980082035 + 2.0 * 6.245555400848389
Epoch 1450, val loss: 1.1067618131637573
Epoch 1460, training loss: 12.501240730285645 = 0.014509844593703747 + 2.0 * 6.243365287780762
Epoch 1460, val loss: 1.1108174324035645
Epoch 1470, training loss: 12.509744644165039 = 0.014238767325878143 + 2.0 * 6.247753143310547
Epoch 1470, val loss: 1.1147193908691406
Epoch 1480, training loss: 12.50069808959961 = 0.013973378576338291 + 2.0 * 6.2433624267578125
Epoch 1480, val loss: 1.118312120437622
Epoch 1490, training loss: 12.497761726379395 = 0.01371786929666996 + 2.0 * 6.2420220375061035
Epoch 1490, val loss: 1.12227463722229
Epoch 1500, training loss: 12.496313095092773 = 0.013467812910676003 + 2.0 * 6.241422653198242
Epoch 1500, val loss: 1.1259807348251343
Epoch 1510, training loss: 12.494880676269531 = 0.013223683461546898 + 2.0 * 6.240828514099121
Epoch 1510, val loss: 1.1297111511230469
Epoch 1520, training loss: 12.504388809204102 = 0.012985944747924805 + 2.0 * 6.245701313018799
Epoch 1520, val loss: 1.1333684921264648
Epoch 1530, training loss: 12.493330955505371 = 0.012757939286530018 + 2.0 * 6.240286350250244
Epoch 1530, val loss: 1.1370348930358887
Epoch 1540, training loss: 12.491039276123047 = 0.012534833513200283 + 2.0 * 6.239252090454102
Epoch 1540, val loss: 1.1406371593475342
Epoch 1550, training loss: 12.493566513061523 = 0.012317227199673653 + 2.0 * 6.24062442779541
Epoch 1550, val loss: 1.1441707611083984
Epoch 1560, training loss: 12.493854522705078 = 0.012104140594601631 + 2.0 * 6.240875244140625
Epoch 1560, val loss: 1.147615671157837
Epoch 1570, training loss: 12.48977279663086 = 0.011901184916496277 + 2.0 * 6.238935947418213
Epoch 1570, val loss: 1.1511462926864624
Epoch 1580, training loss: 12.48720645904541 = 0.01170263346284628 + 2.0 * 6.2377519607543945
Epoch 1580, val loss: 1.1547605991363525
Epoch 1590, training loss: 12.487546920776367 = 0.011507700197398663 + 2.0 * 6.2380194664001465
Epoch 1590, val loss: 1.1581802368164062
Epoch 1600, training loss: 12.493345260620117 = 0.011316644959151745 + 2.0 * 6.24101448059082
Epoch 1600, val loss: 1.1615757942199707
Epoch 1610, training loss: 12.485193252563477 = 0.011129507794976234 + 2.0 * 6.237031936645508
Epoch 1610, val loss: 1.1648694276809692
Epoch 1620, training loss: 12.50015640258789 = 0.010949144139885902 + 2.0 * 6.244603633880615
Epoch 1620, val loss: 1.1682161092758179
Epoch 1630, training loss: 12.488893508911133 = 0.010773226618766785 + 2.0 * 6.239059925079346
Epoch 1630, val loss: 1.171386957168579
Epoch 1640, training loss: 12.483488082885742 = 0.010602953843772411 + 2.0 * 6.236442565917969
Epoch 1640, val loss: 1.1747065782546997
Epoch 1650, training loss: 12.484651565551758 = 0.01043610367923975 + 2.0 * 6.237107753753662
Epoch 1650, val loss: 1.1779080629348755
Epoch 1660, training loss: 12.487212181091309 = 0.010272164829075336 + 2.0 * 6.238470077514648
Epoch 1660, val loss: 1.1809483766555786
Epoch 1670, training loss: 12.48393726348877 = 0.010114242322742939 + 2.0 * 6.236911296844482
Epoch 1670, val loss: 1.1842842102050781
Epoch 1680, training loss: 12.48424243927002 = 0.009959076531231403 + 2.0 * 6.2371416091918945
Epoch 1680, val loss: 1.18740975856781
Epoch 1690, training loss: 12.47944164276123 = 0.009807532653212547 + 2.0 * 6.234817028045654
Epoch 1690, val loss: 1.1904414892196655
Epoch 1700, training loss: 12.48514461517334 = 0.009659240022301674 + 2.0 * 6.237742900848389
Epoch 1700, val loss: 1.1935467720031738
Epoch 1710, training loss: 12.481239318847656 = 0.00951502937823534 + 2.0 * 6.2358622550964355
Epoch 1710, val loss: 1.1965234279632568
Epoch 1720, training loss: 12.477324485778809 = 0.009373333305120468 + 2.0 * 6.233975410461426
Epoch 1720, val loss: 1.199456810951233
Epoch 1730, training loss: 12.485254287719727 = 0.009235434234142303 + 2.0 * 6.238009452819824
Epoch 1730, val loss: 1.2024427652359009
Epoch 1740, training loss: 12.475834846496582 = 0.00910203717648983 + 2.0 * 6.2333664894104
Epoch 1740, val loss: 1.2054742574691772
Epoch 1750, training loss: 12.47424030303955 = 0.008970234543085098 + 2.0 * 6.232635021209717
Epoch 1750, val loss: 1.2084153890609741
Epoch 1760, training loss: 12.49084186553955 = 0.008841113187372684 + 2.0 * 6.241000175476074
Epoch 1760, val loss: 1.211252212524414
Epoch 1770, training loss: 12.47671890258789 = 0.008716673590242863 + 2.0 * 6.234001159667969
Epoch 1770, val loss: 1.2140610218048096
Epoch 1780, training loss: 12.473187446594238 = 0.008594144135713577 + 2.0 * 6.232296466827393
Epoch 1780, val loss: 1.2169755697250366
Epoch 1790, training loss: 12.472746849060059 = 0.008474753238260746 + 2.0 * 6.232136249542236
Epoch 1790, val loss: 1.2198206186294556
Epoch 1800, training loss: 12.49808120727539 = 0.008356314152479172 + 2.0 * 6.2448625564575195
Epoch 1800, val loss: 1.222460150718689
Epoch 1810, training loss: 12.472572326660156 = 0.008242259733378887 + 2.0 * 6.2321648597717285
Epoch 1810, val loss: 1.2252724170684814
Epoch 1820, training loss: 12.470296859741211 = 0.008131742477416992 + 2.0 * 6.231082439422607
Epoch 1820, val loss: 1.2281591892242432
Epoch 1830, training loss: 12.46965503692627 = 0.00802238192409277 + 2.0 * 6.23081636428833
Epoch 1830, val loss: 1.2307828664779663
Epoch 1840, training loss: 12.474231719970703 = 0.007913591340184212 + 2.0 * 6.233159065246582
Epoch 1840, val loss: 1.2334398031234741
Epoch 1850, training loss: 12.469157218933105 = 0.00780767947435379 + 2.0 * 6.230674743652344
Epoch 1850, val loss: 1.2360271215438843
Epoch 1860, training loss: 12.467891693115234 = 0.0077053033746778965 + 2.0 * 6.230093002319336
Epoch 1860, val loss: 1.2386934757232666
Epoch 1870, training loss: 12.481451988220215 = 0.007605104241520166 + 2.0 * 6.2369232177734375
Epoch 1870, val loss: 1.2412810325622559
Epoch 1880, training loss: 12.470090866088867 = 0.007507575210183859 + 2.0 * 6.231291770935059
Epoch 1880, val loss: 1.2439591884613037
Epoch 1890, training loss: 12.466840744018555 = 0.007411542348563671 + 2.0 * 6.229714393615723
Epoch 1890, val loss: 1.2465089559555054
Epoch 1900, training loss: 12.467829704284668 = 0.007316924631595612 + 2.0 * 6.2302565574646
Epoch 1900, val loss: 1.2490867376327515
Epoch 1910, training loss: 12.470638275146484 = 0.007224172353744507 + 2.0 * 6.2317070960998535
Epoch 1910, val loss: 1.2515512704849243
Epoch 1920, training loss: 12.468656539916992 = 0.007133185397833586 + 2.0 * 6.230761528015137
Epoch 1920, val loss: 1.2541263103485107
Epoch 1930, training loss: 12.467227935791016 = 0.007044308818876743 + 2.0 * 6.2300920486450195
Epoch 1930, val loss: 1.2565888166427612
Epoch 1940, training loss: 12.463200569152832 = 0.006956863682717085 + 2.0 * 6.228121757507324
Epoch 1940, val loss: 1.259064793586731
Epoch 1950, training loss: 12.465073585510254 = 0.006870799697935581 + 2.0 * 6.229101181030273
Epoch 1950, val loss: 1.2615095376968384
Epoch 1960, training loss: 12.471369743347168 = 0.006786545272916555 + 2.0 * 6.2322916984558105
Epoch 1960, val loss: 1.2638769149780273
Epoch 1970, training loss: 12.467082023620605 = 0.006704575382173061 + 2.0 * 6.230188846588135
Epoch 1970, val loss: 1.2664257287979126
Epoch 1980, training loss: 12.466754913330078 = 0.006624190136790276 + 2.0 * 6.23006534576416
Epoch 1980, val loss: 1.2688043117523193
Epoch 1990, training loss: 12.466019630432129 = 0.006545304786413908 + 2.0 * 6.229737281799316
Epoch 1990, val loss: 1.2710572481155396
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.788888888888889
0.8376383763837639
=== training gcn model ===
Epoch 0, training loss: 19.14446449279785 = 1.9507725238800049 + 2.0 * 8.596845626831055
Epoch 0, val loss: 1.9541571140289307
Epoch 10, training loss: 19.13420295715332 = 1.9412496089935303 + 2.0 * 8.596476554870605
Epoch 10, val loss: 1.9450299739837646
Epoch 20, training loss: 19.116132736206055 = 1.929343819618225 + 2.0 * 8.59339427947998
Epoch 20, val loss: 1.9331954717636108
Epoch 30, training loss: 19.053098678588867 = 1.9132294654846191 + 2.0 * 8.569934844970703
Epoch 30, val loss: 1.9169538021087646
Epoch 40, training loss: 18.71784782409668 = 1.892789363861084 + 2.0 * 8.412528991699219
Epoch 40, val loss: 1.8966152667999268
Epoch 50, training loss: 17.44624137878418 = 1.8696467876434326 + 2.0 * 7.788297653198242
Epoch 50, val loss: 1.8743261098861694
Epoch 60, training loss: 16.857948303222656 = 1.850537896156311 + 2.0 * 7.5037055015563965
Epoch 60, val loss: 1.8564345836639404
Epoch 70, training loss: 16.362913131713867 = 1.8357080221176147 + 2.0 * 7.263602256774902
Epoch 70, val loss: 1.842289924621582
Epoch 80, training loss: 15.851875305175781 = 1.822031855583191 + 2.0 * 7.01492166519165
Epoch 80, val loss: 1.8299487829208374
Epoch 90, training loss: 15.574193000793457 = 1.8095378875732422 + 2.0 * 6.882327556610107
Epoch 90, val loss: 1.8186771869659424
Epoch 100, training loss: 15.388893127441406 = 1.7964850664138794 + 2.0 * 6.796204090118408
Epoch 100, val loss: 1.8053947687149048
Epoch 110, training loss: 15.234750747680664 = 1.783130168914795 + 2.0 * 6.7258100509643555
Epoch 110, val loss: 1.7915027141571045
Epoch 120, training loss: 15.110217094421387 = 1.7700530290603638 + 2.0 * 6.670082092285156
Epoch 120, val loss: 1.7784651517868042
Epoch 130, training loss: 15.00979995727539 = 1.757033109664917 + 2.0 * 6.626383304595947
Epoch 130, val loss: 1.7660644054412842
Epoch 140, training loss: 14.923309326171875 = 1.743595838546753 + 2.0 * 6.5898566246032715
Epoch 140, val loss: 1.7533011436462402
Epoch 150, training loss: 14.854701042175293 = 1.72894287109375 + 2.0 * 6.5628790855407715
Epoch 150, val loss: 1.739737868309021
Epoch 160, training loss: 14.781243324279785 = 1.7128294706344604 + 2.0 * 6.534206867218018
Epoch 160, val loss: 1.725142240524292
Epoch 170, training loss: 14.717942237854004 = 1.6948758363723755 + 2.0 * 6.511533260345459
Epoch 170, val loss: 1.7094112634658813
Epoch 180, training loss: 14.659003257751465 = 1.6745386123657227 + 2.0 * 6.492232322692871
Epoch 180, val loss: 1.6917459964752197
Epoch 190, training loss: 14.603384971618652 = 1.651165246963501 + 2.0 * 6.476109981536865
Epoch 190, val loss: 1.6715998649597168
Epoch 200, training loss: 14.561359405517578 = 1.6244162321090698 + 2.0 * 6.468471527099609
Epoch 200, val loss: 1.6485501527786255
Epoch 210, training loss: 14.498788833618164 = 1.5942696332931519 + 2.0 * 6.452259540557861
Epoch 210, val loss: 1.6228216886520386
Epoch 220, training loss: 14.444253921508789 = 1.5605882406234741 + 2.0 * 6.441833019256592
Epoch 220, val loss: 1.5939781665802002
Epoch 230, training loss: 14.389223098754883 = 1.5231881141662598 + 2.0 * 6.433017253875732
Epoch 230, val loss: 1.5621041059494019
Epoch 240, training loss: 14.333666801452637 = 1.482390284538269 + 2.0 * 6.425638198852539
Epoch 240, val loss: 1.527516484260559
Epoch 250, training loss: 14.275581359863281 = 1.439294695854187 + 2.0 * 6.418143272399902
Epoch 250, val loss: 1.4911634922027588
Epoch 260, training loss: 14.21523666381836 = 1.3942760229110718 + 2.0 * 6.410480499267578
Epoch 260, val loss: 1.4533768892288208
Epoch 270, training loss: 14.154519081115723 = 1.3477517366409302 + 2.0 * 6.403383731842041
Epoch 270, val loss: 1.4146919250488281
Epoch 280, training loss: 14.098071098327637 = 1.3006352186203003 + 2.0 * 6.398717880249023
Epoch 280, val loss: 1.3758763074874878
Epoch 290, training loss: 14.041812896728516 = 1.2543635368347168 + 2.0 * 6.3937249183654785
Epoch 290, val loss: 1.3379864692687988
Epoch 300, training loss: 13.983555793762207 = 1.2092478275299072 + 2.0 * 6.3871541023254395
Epoch 300, val loss: 1.3015226125717163
Epoch 310, training loss: 13.927502632141113 = 1.1653521060943604 + 2.0 * 6.381075382232666
Epoch 310, val loss: 1.2666536569595337
Epoch 320, training loss: 13.892489433288574 = 1.1229356527328491 + 2.0 * 6.384777069091797
Epoch 320, val loss: 1.2333829402923584
Epoch 330, training loss: 13.832883834838867 = 1.0826971530914307 + 2.0 * 6.375093460083008
Epoch 330, val loss: 1.2024391889572144
Epoch 340, training loss: 13.781158447265625 = 1.044504165649414 + 2.0 * 6.3683271408081055
Epoch 340, val loss: 1.1735445261001587
Epoch 350, training loss: 13.738012313842773 = 1.0079419612884521 + 2.0 * 6.365035057067871
Epoch 350, val loss: 1.1461524963378906
Epoch 360, training loss: 13.695618629455566 = 0.9729394316673279 + 2.0 * 6.361339569091797
Epoch 360, val loss: 1.1203601360321045
Epoch 370, training loss: 13.658415794372559 = 0.9396032691001892 + 2.0 * 6.359406471252441
Epoch 370, val loss: 1.0959628820419312
Epoch 380, training loss: 13.617205619812012 = 0.9076730608940125 + 2.0 * 6.354766368865967
Epoch 380, val loss: 1.0728960037231445
Epoch 390, training loss: 13.579237937927246 = 0.877096951007843 + 2.0 * 6.351070404052734
Epoch 390, val loss: 1.0509421825408936
Epoch 400, training loss: 13.561530113220215 = 0.8475582003593445 + 2.0 * 6.356986045837402
Epoch 400, val loss: 1.0299397706985474
Epoch 410, training loss: 13.511883735656738 = 0.8194015622138977 + 2.0 * 6.346240997314453
Epoch 410, val loss: 1.0099788904190063
Epoch 420, training loss: 13.478940963745117 = 0.7923725247383118 + 2.0 * 6.3432841300964355
Epoch 420, val loss: 0.9910600185394287
Epoch 430, training loss: 13.447067260742188 = 0.76622474193573 + 2.0 * 6.340421199798584
Epoch 430, val loss: 0.9729292392730713
Epoch 440, training loss: 13.434053421020508 = 0.7408948540687561 + 2.0 * 6.346579074859619
Epoch 440, val loss: 0.955657422542572
Epoch 450, training loss: 13.387350082397461 = 0.7166532278060913 + 2.0 * 6.335348606109619
Epoch 450, val loss: 0.939323365688324
Epoch 460, training loss: 13.361862182617188 = 0.6932681798934937 + 2.0 * 6.334297180175781
Epoch 460, val loss: 0.9239555597305298
Epoch 470, training loss: 13.334471702575684 = 0.6706513166427612 + 2.0 * 6.331910133361816
Epoch 470, val loss: 0.9094014763832092
Epoch 480, training loss: 13.308797836303711 = 0.6488404870033264 + 2.0 * 6.3299784660339355
Epoch 480, val loss: 0.8958286643028259
Epoch 490, training loss: 13.281886100769043 = 0.6276528239250183 + 2.0 * 6.3271164894104
Epoch 490, val loss: 0.8829743266105652
Epoch 500, training loss: 13.268720626831055 = 0.6070997714996338 + 2.0 * 6.330810546875
Epoch 500, val loss: 0.8709592819213867
Epoch 510, training loss: 13.238848686218262 = 0.5872587561607361 + 2.0 * 6.3257951736450195
Epoch 510, val loss: 0.8598228096961975
Epoch 520, training loss: 13.213116645812988 = 0.568040668964386 + 2.0 * 6.322537899017334
Epoch 520, val loss: 0.8494693040847778
Epoch 530, training loss: 13.198406219482422 = 0.5493907332420349 + 2.0 * 6.324507713317871
Epoch 530, val loss: 0.839837908744812
Epoch 540, training loss: 13.170513153076172 = 0.5313224196434021 + 2.0 * 6.3195953369140625
Epoch 540, val loss: 0.8309815526008606
Epoch 550, training loss: 13.148673057556152 = 0.5138006806373596 + 2.0 * 6.317436218261719
Epoch 550, val loss: 0.8227232694625854
Epoch 560, training loss: 13.13859748840332 = 0.49671098589897156 + 2.0 * 6.320943355560303
Epoch 560, val loss: 0.8150478601455688
Epoch 570, training loss: 13.114380836486816 = 0.480182409286499 + 2.0 * 6.317099094390869
Epoch 570, val loss: 0.8078662753105164
Epoch 580, training loss: 13.093134880065918 = 0.4641461968421936 + 2.0 * 6.3144941329956055
Epoch 580, val loss: 0.8012853860855103
Epoch 590, training loss: 13.072492599487305 = 0.4485200047492981 + 2.0 * 6.311986446380615
Epoch 590, val loss: 0.7950716614723206
Epoch 600, training loss: 13.058629989624023 = 0.43325498700141907 + 2.0 * 6.312687397003174
Epoch 600, val loss: 0.7893067598342896
Epoch 610, training loss: 13.039467811584473 = 0.4184117615222931 + 2.0 * 6.310527801513672
Epoch 610, val loss: 0.7840301394462585
Epoch 620, training loss: 13.018674850463867 = 0.4038926362991333 + 2.0 * 6.307391166687012
Epoch 620, val loss: 0.7789778113365173
Epoch 630, training loss: 13.014958381652832 = 0.3897194564342499 + 2.0 * 6.312619686126709
Epoch 630, val loss: 0.7743547558784485
Epoch 640, training loss: 12.996553421020508 = 0.3759070038795471 + 2.0 * 6.310323238372803
Epoch 640, val loss: 0.7700071930885315
Epoch 650, training loss: 12.972615242004395 = 0.3624703884124756 + 2.0 * 6.30507230758667
Epoch 650, val loss: 0.7659088373184204
Epoch 660, training loss: 12.954832077026367 = 0.34929707646369934 + 2.0 * 6.302767276763916
Epoch 660, val loss: 0.7620614767074585
Epoch 670, training loss: 12.945460319519043 = 0.3363429307937622 + 2.0 * 6.304558753967285
Epoch 670, val loss: 0.7585299015045166
Epoch 680, training loss: 12.925540924072266 = 0.3236342966556549 + 2.0 * 6.300953388214111
Epoch 680, val loss: 0.7552796006202698
Epoch 690, training loss: 12.919520378112793 = 0.31121599674224854 + 2.0 * 6.304152011871338
Epoch 690, val loss: 0.7520773410797119
Epoch 700, training loss: 12.90060806274414 = 0.29906806349754333 + 2.0 * 6.300769805908203
Epoch 700, val loss: 0.749360978603363
Epoch 710, training loss: 12.881691932678223 = 0.28721827268600464 + 2.0 * 6.297236919403076
Epoch 710, val loss: 0.7466276288032532
Epoch 720, training loss: 12.868660926818848 = 0.2755633294582367 + 2.0 * 6.296548843383789
Epoch 720, val loss: 0.7441743612289429
Epoch 730, training loss: 12.861814498901367 = 0.264157772064209 + 2.0 * 6.298828125
Epoch 730, val loss: 0.741991400718689
Epoch 740, training loss: 12.843183517456055 = 0.25304290652275085 + 2.0 * 6.295070171356201
Epoch 740, val loss: 0.7399467825889587
Epoch 750, training loss: 12.831585884094238 = 0.24218101799488068 + 2.0 * 6.294702529907227
Epoch 750, val loss: 0.7379684448242188
Epoch 760, training loss: 12.822041511535645 = 0.23158381879329681 + 2.0 * 6.295228958129883
Epoch 760, val loss: 0.7362954020500183
Epoch 770, training loss: 12.812089920043945 = 0.22128070890903473 + 2.0 * 6.295404434204102
Epoch 770, val loss: 0.7347809672355652
Epoch 780, training loss: 12.795476913452148 = 0.21128153800964355 + 2.0 * 6.292097568511963
Epoch 780, val loss: 0.7334405779838562
Epoch 790, training loss: 12.783580780029297 = 0.2015654444694519 + 2.0 * 6.2910075187683105
Epoch 790, val loss: 0.7322229146957397
Epoch 800, training loss: 12.775205612182617 = 0.19216634333133698 + 2.0 * 6.291519641876221
Epoch 800, val loss: 0.7312518954277039
Epoch 810, training loss: 12.770639419555664 = 0.1830907166004181 + 2.0 * 6.293774127960205
Epoch 810, val loss: 0.7304303050041199
Epoch 820, training loss: 12.752915382385254 = 0.1743616759777069 + 2.0 * 6.289277076721191
Epoch 820, val loss: 0.7296081781387329
Epoch 830, training loss: 12.739056587219238 = 0.16598355770111084 + 2.0 * 6.286536693572998
Epoch 830, val loss: 0.7290066480636597
Epoch 840, training loss: 12.736775398254395 = 0.15792149305343628 + 2.0 * 6.289426803588867
Epoch 840, val loss: 0.7286651134490967
Epoch 850, training loss: 12.721287727355957 = 0.15021724998950958 + 2.0 * 6.2855353355407715
Epoch 850, val loss: 0.7284033298492432
Epoch 860, training loss: 12.714914321899414 = 0.14287523925304413 + 2.0 * 6.286019325256348
Epoch 860, val loss: 0.7283710241317749
Epoch 870, training loss: 12.713433265686035 = 0.135856032371521 + 2.0 * 6.288788795471191
Epoch 870, val loss: 0.728498637676239
Epoch 880, training loss: 12.692903518676758 = 0.12921813130378723 + 2.0 * 6.2818427085876465
Epoch 880, val loss: 0.7287402153015137
Epoch 890, training loss: 12.685662269592285 = 0.12290270626544952 + 2.0 * 6.281379699707031
Epoch 890, val loss: 0.7291106581687927
Epoch 900, training loss: 12.679226875305176 = 0.11688878387212753 + 2.0 * 6.2811689376831055
Epoch 900, val loss: 0.7297298312187195
Epoch 910, training loss: 12.680346488952637 = 0.11119348555803299 + 2.0 * 6.284576416015625
Epoch 910, val loss: 0.7306749820709229
Epoch 920, training loss: 12.677427291870117 = 0.10580850392580032 + 2.0 * 6.285809516906738
Epoch 920, val loss: 0.7316855788230896
Epoch 930, training loss: 12.666923522949219 = 0.10074781626462936 + 2.0 * 6.283087730407715
Epoch 930, val loss: 0.7327665090560913
Epoch 940, training loss: 12.663599014282227 = 0.09597043693065643 + 2.0 * 6.283814430236816
Epoch 940, val loss: 0.7340653538703918
Epoch 950, training loss: 12.647150993347168 = 0.09148439019918442 + 2.0 * 6.277833461761475
Epoch 950, val loss: 0.7354510426521301
Epoch 960, training loss: 12.639464378356934 = 0.08725674450397491 + 2.0 * 6.276103973388672
Epoch 960, val loss: 0.7370555996894836
Epoch 970, training loss: 12.635446548461914 = 0.08325548470020294 + 2.0 * 6.276095390319824
Epoch 970, val loss: 0.7388505935668945
Epoch 980, training loss: 12.629583358764648 = 0.07948196679353714 + 2.0 * 6.275050640106201
Epoch 980, val loss: 0.740851640701294
Epoch 990, training loss: 12.646194458007812 = 0.07592299580574036 + 2.0 * 6.285135746002197
Epoch 990, val loss: 0.7429662346839905
Epoch 1000, training loss: 12.624920845031738 = 0.07259098440408707 + 2.0 * 6.276165008544922
Epoch 1000, val loss: 0.7452527284622192
Epoch 1010, training loss: 12.620460510253906 = 0.06946998089551926 + 2.0 * 6.2754950523376465
Epoch 1010, val loss: 0.7472331523895264
Epoch 1020, training loss: 12.610918045043945 = 0.06652123481035233 + 2.0 * 6.27219820022583
Epoch 1020, val loss: 0.7496525645256042
Epoch 1030, training loss: 12.609142303466797 = 0.06372857093811035 + 2.0 * 6.272706985473633
Epoch 1030, val loss: 0.7522717714309692
Epoch 1040, training loss: 12.607666015625 = 0.06109129637479782 + 2.0 * 6.273287296295166
Epoch 1040, val loss: 0.7550256848335266
Epoch 1050, training loss: 12.601337432861328 = 0.05861619487404823 + 2.0 * 6.271360397338867
Epoch 1050, val loss: 0.7575754523277283
Epoch 1060, training loss: 12.605781555175781 = 0.05627979338169098 + 2.0 * 6.274750709533691
Epoch 1060, val loss: 0.7601740956306458
Epoch 1070, training loss: 12.591856956481934 = 0.0540657676756382 + 2.0 * 6.268895626068115
Epoch 1070, val loss: 0.7631638050079346
Epoch 1080, training loss: 12.59089469909668 = 0.051974859088659286 + 2.0 * 6.2694597244262695
Epoch 1080, val loss: 0.7660499811172485
Epoch 1090, training loss: 12.598320007324219 = 0.04999436438083649 + 2.0 * 6.274162769317627
Epoch 1090, val loss: 0.7689967751502991
Epoch 1100, training loss: 12.58502197265625 = 0.04812363535165787 + 2.0 * 6.268449306488037
Epoch 1100, val loss: 0.7720547318458557
Epoch 1110, training loss: 12.581289291381836 = 0.04635246843099594 + 2.0 * 6.267468452453613
Epoch 1110, val loss: 0.774957001209259
Epoch 1120, training loss: 12.584227561950684 = 0.04466990754008293 + 2.0 * 6.269778728485107
Epoch 1120, val loss: 0.7780247926712036
Epoch 1130, training loss: 12.57863998413086 = 0.04307509586215019 + 2.0 * 6.267782211303711
Epoch 1130, val loss: 0.7813540101051331
Epoch 1140, training loss: 12.5745210647583 = 0.04156909137964249 + 2.0 * 6.266476154327393
Epoch 1140, val loss: 0.784292459487915
Epoch 1150, training loss: 12.570388793945312 = 0.04013809561729431 + 2.0 * 6.265125274658203
Epoch 1150, val loss: 0.7874778509140015
Epoch 1160, training loss: 12.579216003417969 = 0.038777392357587814 + 2.0 * 6.270219326019287
Epoch 1160, val loss: 0.7906298041343689
Epoch 1170, training loss: 12.57241439819336 = 0.03747750446200371 + 2.0 * 6.267468452453613
Epoch 1170, val loss: 0.7939385771751404
Epoch 1180, training loss: 12.568798065185547 = 0.0362556129693985 + 2.0 * 6.266271114349365
Epoch 1180, val loss: 0.7970592379570007
Epoch 1190, training loss: 12.565196990966797 = 0.03508385270833969 + 2.0 * 6.265056610107422
Epoch 1190, val loss: 0.8001954555511475
Epoch 1200, training loss: 12.562899589538574 = 0.03397177532315254 + 2.0 * 6.264463901519775
Epoch 1200, val loss: 0.8033125996589661
Epoch 1210, training loss: 12.556780815124512 = 0.03290969133377075 + 2.0 * 6.261935710906982
Epoch 1210, val loss: 0.8065431118011475
Epoch 1220, training loss: 12.570256233215332 = 0.03189790993928909 + 2.0 * 6.269179344177246
Epoch 1220, val loss: 0.8098328709602356
Epoch 1230, training loss: 12.557313919067383 = 0.03093465231359005 + 2.0 * 6.263189792633057
Epoch 1230, val loss: 0.8128450512886047
Epoch 1240, training loss: 12.552797317504883 = 0.030015109106898308 + 2.0 * 6.2613911628723145
Epoch 1240, val loss: 0.8159882426261902
Epoch 1250, training loss: 12.581913948059082 = 0.029137086123228073 + 2.0 * 6.276388645172119
Epoch 1250, val loss: 0.819180428981781
Epoch 1260, training loss: 12.555542945861816 = 0.028294194489717484 + 2.0 * 6.26362419128418
Epoch 1260, val loss: 0.8224313259124756
Epoch 1270, training loss: 12.545201301574707 = 0.02749665267765522 + 2.0 * 6.258852481842041
Epoch 1270, val loss: 0.8253173828125
Epoch 1280, training loss: 12.54439926147461 = 0.026728147640824318 + 2.0 * 6.258835792541504
Epoch 1280, val loss: 0.8284593820571899
Epoch 1290, training loss: 12.567837715148926 = 0.025991693139076233 + 2.0 * 6.270923137664795
Epoch 1290, val loss: 0.8318471312522888
Epoch 1300, training loss: 12.547016143798828 = 0.025286607444286346 + 2.0 * 6.260864734649658
Epoch 1300, val loss: 0.8347560167312622
Epoch 1310, training loss: 12.54330062866211 = 0.024613747373223305 + 2.0 * 6.25934362411499
Epoch 1310, val loss: 0.8377528786659241
Epoch 1320, training loss: 12.5376558303833 = 0.023966651409864426 + 2.0 * 6.256844520568848
Epoch 1320, val loss: 0.8408564925193787
Epoch 1330, training loss: 12.536710739135742 = 0.023344576358795166 + 2.0 * 6.256682872772217
Epoch 1330, val loss: 0.8438436985015869
Epoch 1340, training loss: 12.539358139038086 = 0.02274482138454914 + 2.0 * 6.258306503295898
Epoch 1340, val loss: 0.8469621539115906
Epoch 1350, training loss: 12.534968376159668 = 0.02216866798698902 + 2.0 * 6.256399631500244
Epoch 1350, val loss: 0.8499924540519714
Epoch 1360, training loss: 12.541827201843262 = 0.021614965051412582 + 2.0 * 6.260106086730957
Epoch 1360, val loss: 0.8530599474906921
Epoch 1370, training loss: 12.531534194946289 = 0.0210880134254694 + 2.0 * 6.255223274230957
Epoch 1370, val loss: 0.8559716939926147
Epoch 1380, training loss: 12.532020568847656 = 0.020580045878887177 + 2.0 * 6.255720138549805
Epoch 1380, val loss: 0.8587571978569031
Epoch 1390, training loss: 12.544987678527832 = 0.02008925750851631 + 2.0 * 6.262449264526367
Epoch 1390, val loss: 0.8617786169052124
Epoch 1400, training loss: 12.531447410583496 = 0.01962052471935749 + 2.0 * 6.255913257598877
Epoch 1400, val loss: 0.864738404750824
Epoch 1410, training loss: 12.526718139648438 = 0.019166341051459312 + 2.0 * 6.2537760734558105
Epoch 1410, val loss: 0.8674485683441162
Epoch 1420, training loss: 12.524933815002441 = 0.01872851885855198 + 2.0 * 6.253102779388428
Epoch 1420, val loss: 0.870383083820343
Epoch 1430, training loss: 12.53598690032959 = 0.01830516755580902 + 2.0 * 6.258841037750244
Epoch 1430, val loss: 0.873241126537323
Epoch 1440, training loss: 12.526835441589355 = 0.017894625663757324 + 2.0 * 6.254470348358154
Epoch 1440, val loss: 0.8761533498764038
Epoch 1450, training loss: 12.52983283996582 = 0.01750325784087181 + 2.0 * 6.256165027618408
Epoch 1450, val loss: 0.8788532614707947
Epoch 1460, training loss: 12.523305892944336 = 0.017123671248555183 + 2.0 * 6.253091335296631
Epoch 1460, val loss: 0.8817139863967896
Epoch 1470, training loss: 12.520487785339355 = 0.01675826869904995 + 2.0 * 6.251864910125732
Epoch 1470, val loss: 0.8844050168991089
Epoch 1480, training loss: 12.528035163879395 = 0.01640339568257332 + 2.0 * 6.2558159828186035
Epoch 1480, val loss: 0.8871195316314697
Epoch 1490, training loss: 12.517964363098145 = 0.016060948371887207 + 2.0 * 6.250951766967773
Epoch 1490, val loss: 0.8899240493774414
Epoch 1500, training loss: 12.51584529876709 = 0.015729963779449463 + 2.0 * 6.250057697296143
Epoch 1500, val loss: 0.8925982713699341
Epoch 1510, training loss: 12.520158767700195 = 0.015410587191581726 + 2.0 * 6.252374172210693
Epoch 1510, val loss: 0.8952887654304504
Epoch 1520, training loss: 12.5145845413208 = 0.015099343843758106 + 2.0 * 6.24974250793457
Epoch 1520, val loss: 0.8980093598365784
Epoch 1530, training loss: 12.517374038696289 = 0.014799260534346104 + 2.0 * 6.251287460327148
Epoch 1530, val loss: 0.9006043672561646
Epoch 1540, training loss: 12.52484130859375 = 0.014509948901832104 + 2.0 * 6.2551655769348145
Epoch 1540, val loss: 0.9032938480377197
Epoch 1550, training loss: 12.514901161193848 = 0.014228557236492634 + 2.0 * 6.250336170196533
Epoch 1550, val loss: 0.9057645797729492
Epoch 1560, training loss: 12.51318645477295 = 0.013957127928733826 + 2.0 * 6.249614715576172
Epoch 1560, val loss: 0.9083828926086426
Epoch 1570, training loss: 12.532476425170898 = 0.01369259413331747 + 2.0 * 6.259391784667969
Epoch 1570, val loss: 0.9110725522041321
Epoch 1580, training loss: 12.512913703918457 = 0.013437720015645027 + 2.0 * 6.2497382164001465
Epoch 1580, val loss: 0.9134692549705505
Epoch 1590, training loss: 12.506400108337402 = 0.013191231526434422 + 2.0 * 6.2466044425964355
Epoch 1590, val loss: 0.9158690571784973
Epoch 1600, training loss: 12.504706382751465 = 0.012951118871569633 + 2.0 * 6.245877742767334
Epoch 1600, val loss: 0.9183686375617981
Epoch 1610, training loss: 12.503482818603516 = 0.012715686112642288 + 2.0 * 6.2453837394714355
Epoch 1610, val loss: 0.9209805130958557
Epoch 1620, training loss: 12.513348579406738 = 0.012486898340284824 + 2.0 * 6.250431060791016
Epoch 1620, val loss: 0.923550546169281
Epoch 1630, training loss: 12.513455390930176 = 0.012264234945178032 + 2.0 * 6.250595569610596
Epoch 1630, val loss: 0.9260802865028381
Epoch 1640, training loss: 12.509178161621094 = 0.012051730416715145 + 2.0 * 6.248563289642334
Epoch 1640, val loss: 0.9283410310745239
Epoch 1650, training loss: 12.500654220581055 = 0.011845151893794537 + 2.0 * 6.244404315948486
Epoch 1650, val loss: 0.930598795413971
Epoch 1660, training loss: 12.500262260437012 = 0.011643190868198872 + 2.0 * 6.244309425354004
Epoch 1660, val loss: 0.9330083727836609
Epoch 1670, training loss: 12.513038635253906 = 0.011445277370512486 + 2.0 * 6.250796794891357
Epoch 1670, val loss: 0.9354255795478821
Epoch 1680, training loss: 12.505135536193848 = 0.011252624914050102 + 2.0 * 6.246941566467285
Epoch 1680, val loss: 0.9381187558174133
Epoch 1690, training loss: 12.504925727844238 = 0.011068723164498806 + 2.0 * 6.246928691864014
Epoch 1690, val loss: 0.9401158094406128
Epoch 1700, training loss: 12.501238822937012 = 0.010888942517340183 + 2.0 * 6.245174884796143
Epoch 1700, val loss: 0.9424341917037964
Epoch 1710, training loss: 12.498879432678223 = 0.010712794959545135 + 2.0 * 6.244083404541016
Epoch 1710, val loss: 0.9447261691093445
Epoch 1720, training loss: 12.508453369140625 = 0.010540970601141453 + 2.0 * 6.248956203460693
Epoch 1720, val loss: 0.9470347762107849
Epoch 1730, training loss: 12.497050285339355 = 0.010375533252954483 + 2.0 * 6.243337154388428
Epoch 1730, val loss: 0.9492734670639038
Epoch 1740, training loss: 12.494780540466309 = 0.010213660076260567 + 2.0 * 6.242283344268799
Epoch 1740, val loss: 0.9514530897140503
Epoch 1750, training loss: 12.502593040466309 = 0.010055780410766602 + 2.0 * 6.2462687492370605
Epoch 1750, val loss: 0.9537449479103088
Epoch 1760, training loss: 12.49325942993164 = 0.009900376200675964 + 2.0 * 6.241679668426514
Epoch 1760, val loss: 0.9559593796730042
Epoch 1770, training loss: 12.495564460754395 = 0.009750179015100002 + 2.0 * 6.2429070472717285
Epoch 1770, val loss: 0.9581149816513062
Epoch 1780, training loss: 12.500435829162598 = 0.009603708051145077 + 2.0 * 6.245416164398193
Epoch 1780, val loss: 0.9603222012519836
Epoch 1790, training loss: 12.494363784790039 = 0.00945956539362669 + 2.0 * 6.242452144622803
Epoch 1790, val loss: 0.9625283479690552
Epoch 1800, training loss: 12.493000030517578 = 0.00932048074901104 + 2.0 * 6.24183988571167
Epoch 1800, val loss: 0.9645834565162659
Epoch 1810, training loss: 12.49224853515625 = 0.009184157475829124 + 2.0 * 6.241532325744629
Epoch 1810, val loss: 0.9666913151741028
Epoch 1820, training loss: 12.493707656860352 = 0.009050678461790085 + 2.0 * 6.242328643798828
Epoch 1820, val loss: 0.9688605666160583
Epoch 1830, training loss: 12.489791870117188 = 0.00892005767673254 + 2.0 * 6.24043607711792
Epoch 1830, val loss: 0.9709674119949341
Epoch 1840, training loss: 12.49760627746582 = 0.008793139830231667 + 2.0 * 6.244406700134277
Epoch 1840, val loss: 0.973099410533905
Epoch 1850, training loss: 12.491337776184082 = 0.008669525384902954 + 2.0 * 6.241333961486816
Epoch 1850, val loss: 0.9750388860702515
Epoch 1860, training loss: 12.49894905090332 = 0.008548526093363762 + 2.0 * 6.245200157165527
Epoch 1860, val loss: 0.9770269393920898
Epoch 1870, training loss: 12.48736572265625 = 0.008429862558841705 + 2.0 * 6.239468097686768
Epoch 1870, val loss: 0.9791075587272644
Epoch 1880, training loss: 12.484992027282715 = 0.008315595798194408 + 2.0 * 6.238337993621826
Epoch 1880, val loss: 0.980988085269928
Epoch 1890, training loss: 12.484090805053711 = 0.00820352416485548 + 2.0 * 6.237943649291992
Epoch 1890, val loss: 0.9828965067863464
Epoch 1900, training loss: 12.483631134033203 = 0.00809280015528202 + 2.0 * 6.23776912689209
Epoch 1900, val loss: 0.9850103259086609
Epoch 1910, training loss: 12.507049560546875 = 0.0079845255240798 + 2.0 * 6.249532699584961
Epoch 1910, val loss: 0.9870537519454956
Epoch 1920, training loss: 12.491581916809082 = 0.00787785928696394 + 2.0 * 6.241851806640625
Epoch 1920, val loss: 0.9890556931495667
Epoch 1930, training loss: 12.488200187683105 = 0.007775451987981796 + 2.0 * 6.240212440490723
Epoch 1930, val loss: 0.990765392780304
Epoch 1940, training loss: 12.486824989318848 = 0.007674936670809984 + 2.0 * 6.239574909210205
Epoch 1940, val loss: 0.9926495552062988
Epoch 1950, training loss: 12.481261253356934 = 0.007576615549623966 + 2.0 * 6.236842155456543
Epoch 1950, val loss: 0.9946082234382629
Epoch 1960, training loss: 12.483651161193848 = 0.00747974356636405 + 2.0 * 6.238085746765137
Epoch 1960, val loss: 0.9965062141418457
Epoch 1970, training loss: 12.491253852844238 = 0.00738399475812912 + 2.0 * 6.241934776306152
Epoch 1970, val loss: 0.9984626770019531
Epoch 1980, training loss: 12.49201774597168 = 0.007292306516319513 + 2.0 * 6.2423624992370605
Epoch 1980, val loss: 1.0004420280456543
Epoch 1990, training loss: 12.479981422424316 = 0.007201497443020344 + 2.0 * 6.236390113830566
Epoch 1990, val loss: 1.0020657777786255
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7740740740740741
0.838165524512388
The final CL Acc:0.79630, 0.02181, The final GNN Acc:0.84027, 0.00336
Begin epxeriment: cont_weight: 2 epoch:2000 der1:0.3 der2:0.2 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11604])
remove edge: torch.Size([2, 9502])
updated graph: torch.Size([2, 10550])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 19.134140014648438 = 1.9404698610305786 + 2.0 * 8.596835136413574
Epoch 0, val loss: 1.9368270635604858
Epoch 10, training loss: 19.123369216918945 = 1.9304583072662354 + 2.0 * 8.596455574035645
Epoch 10, val loss: 1.926447868347168
Epoch 20, training loss: 19.10382080078125 = 1.917496681213379 + 2.0 * 8.593161582946777
Epoch 20, val loss: 1.9128201007843018
Epoch 30, training loss: 19.033344268798828 = 1.8993632793426514 + 2.0 * 8.566990852355957
Epoch 30, val loss: 1.894012212753296
Epoch 40, training loss: 18.717391967773438 = 1.8768795728683472 + 2.0 * 8.420256614685059
Epoch 40, val loss: 1.8721404075622559
Epoch 50, training loss: 17.90252113342285 = 1.8528637886047363 + 2.0 * 8.024828910827637
Epoch 50, val loss: 1.849674105644226
Epoch 60, training loss: 17.13759994506836 = 1.833640217781067 + 2.0 * 7.651979446411133
Epoch 60, val loss: 1.832785725593567
Epoch 70, training loss: 16.297618865966797 = 1.8202853202819824 + 2.0 * 7.238667011260986
Epoch 70, val loss: 1.820562481880188
Epoch 80, training loss: 15.906895637512207 = 1.8086535930633545 + 2.0 * 7.049120903015137
Epoch 80, val loss: 1.8095282316207886
Epoch 90, training loss: 15.59980583190918 = 1.7956188917160034 + 2.0 * 6.902093410491943
Epoch 90, val loss: 1.7976385354995728
Epoch 100, training loss: 15.42075252532959 = 1.782530665397644 + 2.0 * 6.819110870361328
Epoch 100, val loss: 1.7858076095581055
Epoch 110, training loss: 15.288346290588379 = 1.7697551250457764 + 2.0 * 6.759295463562012
Epoch 110, val loss: 1.7741400003433228
Epoch 120, training loss: 15.17190170288086 = 1.7571858167648315 + 2.0 * 6.707357883453369
Epoch 120, val loss: 1.762793779373169
Epoch 130, training loss: 15.069415092468262 = 1.7439998388290405 + 2.0 * 6.662707805633545
Epoch 130, val loss: 1.7511621713638306
Epoch 140, training loss: 14.982967376708984 = 1.7292665243148804 + 2.0 * 6.626850605010986
Epoch 140, val loss: 1.738608956336975
Epoch 150, training loss: 14.90582275390625 = 1.7126538753509521 + 2.0 * 6.596584320068359
Epoch 150, val loss: 1.7244716882705688
Epoch 160, training loss: 14.837324142456055 = 1.693710207939148 + 2.0 * 6.571806907653809
Epoch 160, val loss: 1.7084776163101196
Epoch 170, training loss: 14.801399230957031 = 1.6721633672714233 + 2.0 * 6.564618110656738
Epoch 170, val loss: 1.690294623374939
Epoch 180, training loss: 14.724406242370605 = 1.6479556560516357 + 2.0 * 6.538225173950195
Epoch 180, val loss: 1.6700217723846436
Epoch 190, training loss: 14.664512634277344 = 1.6210942268371582 + 2.0 * 6.521709442138672
Epoch 190, val loss: 1.6475951671600342
Epoch 200, training loss: 14.607721328735352 = 1.591169834136963 + 2.0 * 6.508275508880615
Epoch 200, val loss: 1.6227613687515259
Epoch 210, training loss: 14.5473051071167 = 1.557793378829956 + 2.0 * 6.494755744934082
Epoch 210, val loss: 1.5952733755111694
Epoch 220, training loss: 14.487557411193848 = 1.5209264755249023 + 2.0 * 6.483315467834473
Epoch 220, val loss: 1.5651915073394775
Epoch 230, training loss: 14.425704956054688 = 1.4810280799865723 + 2.0 * 6.4723381996154785
Epoch 230, val loss: 1.532907247543335
Epoch 240, training loss: 14.358654022216797 = 1.4385154247283936 + 2.0 * 6.460069179534912
Epoch 240, val loss: 1.4986863136291504
Epoch 250, training loss: 14.293190956115723 = 1.393599510192871 + 2.0 * 6.449795722961426
Epoch 250, val loss: 1.4629639387130737
Epoch 260, training loss: 14.230894088745117 = 1.3466131687164307 + 2.0 * 6.442140579223633
Epoch 260, val loss: 1.4260629415512085
Epoch 270, training loss: 14.166248321533203 = 1.2989981174468994 + 2.0 * 6.433625221252441
Epoch 270, val loss: 1.389297604560852
Epoch 280, training loss: 14.103986740112305 = 1.2516154050827026 + 2.0 * 6.426185607910156
Epoch 280, val loss: 1.3531675338745117
Epoch 290, training loss: 14.03913402557373 = 1.2044862508773804 + 2.0 * 6.417324066162109
Epoch 290, val loss: 1.3178167343139648
Epoch 300, training loss: 13.978629112243652 = 1.1576601266860962 + 2.0 * 6.410484313964844
Epoch 300, val loss: 1.2835092544555664
Epoch 310, training loss: 13.924373626708984 = 1.1112849712371826 + 2.0 * 6.406544208526611
Epoch 310, val loss: 1.2502846717834473
Epoch 320, training loss: 13.875919342041016 = 1.0658128261566162 + 2.0 * 6.40505313873291
Epoch 320, val loss: 1.2184289693832397
Epoch 330, training loss: 13.81473159790039 = 1.0219333171844482 + 2.0 * 6.396399021148682
Epoch 330, val loss: 1.1887701749801636
Epoch 340, training loss: 13.760448455810547 = 0.9795527458190918 + 2.0 * 6.390447616577148
Epoch 340, val loss: 1.1611055135726929
Epoch 350, training loss: 13.71023941040039 = 0.9386171698570251 + 2.0 * 6.3858113288879395
Epoch 350, val loss: 1.1353148221969604
Epoch 360, training loss: 13.662763595581055 = 0.8991470336914062 + 2.0 * 6.381808280944824
Epoch 360, val loss: 1.1114957332611084
Epoch 370, training loss: 13.61902141571045 = 0.8612387180328369 + 2.0 * 6.378891468048096
Epoch 370, val loss: 1.0896705389022827
Epoch 380, training loss: 13.579854011535645 = 0.825204610824585 + 2.0 * 6.37732458114624
Epoch 380, val loss: 1.0701755285263062
Epoch 390, training loss: 13.537999153137207 = 0.7913327813148499 + 2.0 * 6.373332977294922
Epoch 390, val loss: 1.0529377460479736
Epoch 400, training loss: 13.497917175292969 = 0.7593689560890198 + 2.0 * 6.369274139404297
Epoch 400, val loss: 1.0378531217575073
Epoch 410, training loss: 13.467107772827148 = 0.7291237711906433 + 2.0 * 6.368991851806641
Epoch 410, val loss: 1.024627685546875
Epoch 420, training loss: 13.427164077758789 = 0.7005342245101929 + 2.0 * 6.363315105438232
Epoch 420, val loss: 1.013360619544983
Epoch 430, training loss: 13.397198677062988 = 0.6734319925308228 + 2.0 * 6.361883163452148
Epoch 430, val loss: 1.0036836862564087
Epoch 440, training loss: 13.37104606628418 = 0.6478012204170227 + 2.0 * 6.361622333526611
Epoch 440, val loss: 0.9955124258995056
Epoch 450, training loss: 13.333317756652832 = 0.6235196590423584 + 2.0 * 6.354898929595947
Epoch 450, val loss: 0.9889894723892212
Epoch 460, training loss: 13.304804801940918 = 0.6003280878067017 + 2.0 * 6.352238178253174
Epoch 460, val loss: 0.9838352799415588
Epoch 470, training loss: 13.293013572692871 = 0.5781728625297546 + 2.0 * 6.357420444488525
Epoch 470, val loss: 0.9798494577407837
Epoch 480, training loss: 13.255951881408691 = 0.5570400953292847 + 2.0 * 6.349455833435059
Epoch 480, val loss: 0.976966142654419
Epoch 490, training loss: 13.225849151611328 = 0.5368562936782837 + 2.0 * 6.344496250152588
Epoch 490, val loss: 0.9753579497337341
Epoch 500, training loss: 13.206873893737793 = 0.5174502730369568 + 2.0 * 6.344711780548096
Epoch 500, val loss: 0.9746817350387573
Epoch 510, training loss: 13.192001342773438 = 0.498884379863739 + 2.0 * 6.346558570861816
Epoch 510, val loss: 0.9748102426528931
Epoch 520, training loss: 13.159992218017578 = 0.4811207950115204 + 2.0 * 6.339435577392578
Epoch 520, val loss: 0.9760488867759705
Epoch 530, training loss: 13.137596130371094 = 0.4640180468559265 + 2.0 * 6.336789131164551
Epoch 530, val loss: 0.9779415726661682
Epoch 540, training loss: 13.118966102600098 = 0.44751864671707153 + 2.0 * 6.335723876953125
Epoch 540, val loss: 0.980476438999176
Epoch 550, training loss: 13.0971040725708 = 0.43157336115837097 + 2.0 * 6.332765579223633
Epoch 550, val loss: 0.983794629573822
Epoch 560, training loss: 13.095732688903809 = 0.4161706268787384 + 2.0 * 6.339780807495117
Epoch 560, val loss: 0.9875293970108032
Epoch 570, training loss: 13.059489250183105 = 0.40129387378692627 + 2.0 * 6.329097747802734
Epoch 570, val loss: 0.9918432235717773
Epoch 580, training loss: 13.041484832763672 = 0.3868583142757416 + 2.0 * 6.327313423156738
Epoch 580, val loss: 0.9965878129005432
Epoch 590, training loss: 13.03519344329834 = 0.37277963757514954 + 2.0 * 6.331206798553467
Epoch 590, val loss: 1.001844048500061
Epoch 600, training loss: 13.009259223937988 = 0.35914328694343567 + 2.0 * 6.3250579833984375
Epoch 600, val loss: 1.0073316097259521
Epoch 610, training loss: 12.99045181274414 = 0.3457903265953064 + 2.0 * 6.322330951690674
Epoch 610, val loss: 1.0133323669433594
Epoch 620, training loss: 12.990689277648926 = 0.3327467441558838 + 2.0 * 6.3289713859558105
Epoch 620, val loss: 1.0195753574371338
Epoch 630, training loss: 12.96410083770752 = 0.32011815905570984 + 2.0 * 6.321991443634033
Epoch 630, val loss: 1.0263594388961792
Epoch 640, training loss: 12.944428443908691 = 0.3077961504459381 + 2.0 * 6.3183159828186035
Epoch 640, val loss: 1.033409595489502
Epoch 650, training loss: 12.93200397491455 = 0.2957555651664734 + 2.0 * 6.318124294281006
Epoch 650, val loss: 1.04079008102417
Epoch 660, training loss: 12.916032791137695 = 0.28402239084243774 + 2.0 * 6.316005229949951
Epoch 660, val loss: 1.0483222007751465
Epoch 670, training loss: 12.902538299560547 = 0.27264904975891113 + 2.0 * 6.314944744110107
Epoch 670, val loss: 1.0563791990280151
Epoch 680, training loss: 12.886062622070312 = 0.26155465841293335 + 2.0 * 6.312253952026367
Epoch 680, val loss: 1.064511775970459
Epoch 690, training loss: 12.899968147277832 = 0.25076138973236084 + 2.0 * 6.32460355758667
Epoch 690, val loss: 1.0730748176574707
Epoch 700, training loss: 12.863757133483887 = 0.24035269021987915 + 2.0 * 6.311702251434326
Epoch 700, val loss: 1.0816450119018555
Epoch 710, training loss: 12.8494873046875 = 0.23033763468265533 + 2.0 * 6.309574604034424
Epoch 710, val loss: 1.0908674001693726
Epoch 720, training loss: 12.836389541625977 = 0.22062253952026367 + 2.0 * 6.3078837394714355
Epoch 720, val loss: 1.1000781059265137
Epoch 730, training loss: 12.831562995910645 = 0.21122856438159943 + 2.0 * 6.31016731262207
Epoch 730, val loss: 1.109648585319519
Epoch 740, training loss: 12.818829536437988 = 0.20218800008296967 + 2.0 * 6.308320999145508
Epoch 740, val loss: 1.1193770170211792
Epoch 750, training loss: 12.803210258483887 = 0.19352805614471436 + 2.0 * 6.304841041564941
Epoch 750, val loss: 1.1294922828674316
Epoch 760, training loss: 12.790491104125977 = 0.18517573177814484 + 2.0 * 6.302657604217529
Epoch 760, val loss: 1.139691948890686
Epoch 770, training loss: 12.785901069641113 = 0.1771327406167984 + 2.0 * 6.304384231567383
Epoch 770, val loss: 1.150098204612732
Epoch 780, training loss: 12.80014705657959 = 0.16947880387306213 + 2.0 * 6.315334320068359
Epoch 780, val loss: 1.1605238914489746
Epoch 790, training loss: 12.770511627197266 = 0.16222473978996277 + 2.0 * 6.30414342880249
Epoch 790, val loss: 1.171454668045044
Epoch 800, training loss: 12.754074096679688 = 0.15529237687587738 + 2.0 * 6.29939079284668
Epoch 800, val loss: 1.1822357177734375
Epoch 810, training loss: 12.74407958984375 = 0.14863702654838562 + 2.0 * 6.2977213859558105
Epoch 810, val loss: 1.1932240724563599
Epoch 820, training loss: 12.73600959777832 = 0.14225636422634125 + 2.0 * 6.296876430511475
Epoch 820, val loss: 1.204498529434204
Epoch 830, training loss: 12.734015464782715 = 0.13614556193351746 + 2.0 * 6.2989349365234375
Epoch 830, val loss: 1.215795874595642
Epoch 840, training loss: 12.720465660095215 = 0.13034211099147797 + 2.0 * 6.2950615882873535
Epoch 840, val loss: 1.227117896080017
Epoch 850, training loss: 12.715420722961426 = 0.1248217299580574 + 2.0 * 6.295299530029297
Epoch 850, val loss: 1.2387109994888306
Epoch 860, training loss: 12.708057403564453 = 0.11956960707902908 + 2.0 * 6.294243812561035
Epoch 860, val loss: 1.2502167224884033
Epoch 870, training loss: 12.701069831848145 = 0.11453782021999359 + 2.0 * 6.2932658195495605
Epoch 870, val loss: 1.2617367506027222
Epoch 880, training loss: 12.703160285949707 = 0.10975565016269684 + 2.0 * 6.2967023849487305
Epoch 880, val loss: 1.2733430862426758
Epoch 890, training loss: 12.6932373046875 = 0.10523562133312225 + 2.0 * 6.294000625610352
Epoch 890, val loss: 1.2848999500274658
Epoch 900, training loss: 12.681744575500488 = 0.10095135122537613 + 2.0 * 6.290396690368652
Epoch 900, val loss: 1.2965664863586426
Epoch 910, training loss: 12.679452896118164 = 0.0968589335680008 + 2.0 * 6.29129695892334
Epoch 910, val loss: 1.3081175088882446
Epoch 920, training loss: 12.676664352416992 = 0.09296634793281555 + 2.0 * 6.291849136352539
Epoch 920, val loss: 1.3196160793304443
Epoch 930, training loss: 12.667789459228516 = 0.08926710486412048 + 2.0 * 6.289261341094971
Epoch 930, val loss: 1.3311833143234253
Epoch 940, training loss: 12.668217658996582 = 0.08575066179037094 + 2.0 * 6.291233539581299
Epoch 940, val loss: 1.3425955772399902
Epoch 950, training loss: 12.655765533447266 = 0.08240368962287903 + 2.0 * 6.286680698394775
Epoch 950, val loss: 1.3539972305297852
Epoch 960, training loss: 12.655900001525879 = 0.07922116667032242 + 2.0 * 6.288339614868164
Epoch 960, val loss: 1.3653662204742432
Epoch 970, training loss: 12.648006439208984 = 0.07618814706802368 + 2.0 * 6.285909175872803
Epoch 970, val loss: 1.3766602277755737
Epoch 980, training loss: 12.658323287963867 = 0.0733116865158081 + 2.0 * 6.292505741119385
Epoch 980, val loss: 1.388015627861023
Epoch 990, training loss: 12.641769409179688 = 0.07056865841150284 + 2.0 * 6.285600185394287
Epoch 990, val loss: 1.3989719152450562
Epoch 1000, training loss: 12.635578155517578 = 0.06796680390834808 + 2.0 * 6.283805847167969
Epoch 1000, val loss: 1.4102041721343994
Epoch 1010, training loss: 12.631617546081543 = 0.06548061221837997 + 2.0 * 6.283068656921387
Epoch 1010, val loss: 1.4211598634719849
Epoch 1020, training loss: 12.631744384765625 = 0.0631069764494896 + 2.0 * 6.284318923950195
Epoch 1020, val loss: 1.4320895671844482
Epoch 1030, training loss: 12.628689765930176 = 0.06085013225674629 + 2.0 * 6.283919811248779
Epoch 1030, val loss: 1.4427282810211182
Epoch 1040, training loss: 12.624262809753418 = 0.058708544820547104 + 2.0 * 6.282777309417725
Epoch 1040, val loss: 1.4535590410232544
Epoch 1050, training loss: 12.620848655700684 = 0.05666716396808624 + 2.0 * 6.282090663909912
Epoch 1050, val loss: 1.464053988456726
Epoch 1060, training loss: 12.618827819824219 = 0.05471840500831604 + 2.0 * 6.282054901123047
Epoch 1060, val loss: 1.4744482040405273
Epoch 1070, training loss: 12.616691589355469 = 0.05285679176449776 + 2.0 * 6.281917572021484
Epoch 1070, val loss: 1.4848886728286743
Epoch 1080, training loss: 12.614477157592773 = 0.051081668585538864 + 2.0 * 6.281697750091553
Epoch 1080, val loss: 1.4950883388519287
Epoch 1090, training loss: 12.610077857971191 = 0.04938375949859619 + 2.0 * 6.280346870422363
Epoch 1090, val loss: 1.505159854888916
Epoch 1100, training loss: 12.608152389526367 = 0.047764699906110764 + 2.0 * 6.28019380569458
Epoch 1100, val loss: 1.5153361558914185
Epoch 1110, training loss: 12.599346160888672 = 0.04622025415301323 + 2.0 * 6.2765631675720215
Epoch 1110, val loss: 1.525138258934021
Epoch 1120, training loss: 12.601471900939941 = 0.04474036768078804 + 2.0 * 6.278365612030029
Epoch 1120, val loss: 1.5350196361541748
Epoch 1130, training loss: 12.604222297668457 = 0.043329477310180664 + 2.0 * 6.280446529388428
Epoch 1130, val loss: 1.5445542335510254
Epoch 1140, training loss: 12.59463119506836 = 0.04198096692562103 + 2.0 * 6.276325225830078
Epoch 1140, val loss: 1.5541167259216309
Epoch 1150, training loss: 12.590502738952637 = 0.04069153219461441 + 2.0 * 6.274905681610107
Epoch 1150, val loss: 1.5634337663650513
Epoch 1160, training loss: 12.588787078857422 = 0.0394500270485878 + 2.0 * 6.2746686935424805
Epoch 1160, val loss: 1.5726112127304077
Epoch 1170, training loss: 12.596453666687012 = 0.03826190531253815 + 2.0 * 6.279095649719238
Epoch 1170, val loss: 1.5816028118133545
Epoch 1180, training loss: 12.596213340759277 = 0.03713513910770416 + 2.0 * 6.279539108276367
Epoch 1180, val loss: 1.5906904935836792
Epoch 1190, training loss: 12.584648132324219 = 0.03604495897889137 + 2.0 * 6.274301528930664
Epoch 1190, val loss: 1.5996276140213013
Epoch 1200, training loss: 12.582842826843262 = 0.035008832812309265 + 2.0 * 6.273917198181152
Epoch 1200, val loss: 1.6084541082382202
Epoch 1210, training loss: 12.579063415527344 = 0.03400575369596481 + 2.0 * 6.272528648376465
Epoch 1210, val loss: 1.616930365562439
Epoch 1220, training loss: 12.590357780456543 = 0.033046428114175797 + 2.0 * 6.278655529022217
Epoch 1220, val loss: 1.625353455543518
Epoch 1230, training loss: 12.57611083984375 = 0.03212868049740791 + 2.0 * 6.27199125289917
Epoch 1230, val loss: 1.6340681314468384
Epoch 1240, training loss: 12.573902130126953 = 0.03124549798667431 + 2.0 * 6.271328449249268
Epoch 1240, val loss: 1.642210602760315
Epoch 1250, training loss: 12.574575424194336 = 0.03039606846868992 + 2.0 * 6.27208948135376
Epoch 1250, val loss: 1.6503955125808716
Epoch 1260, training loss: 12.571209907531738 = 0.029579980298876762 + 2.0 * 6.270814895629883
Epoch 1260, val loss: 1.658501148223877
Epoch 1270, training loss: 12.578155517578125 = 0.028794586658477783 + 2.0 * 6.2746806144714355
Epoch 1270, val loss: 1.6665133237838745
Epoch 1280, training loss: 12.567296028137207 = 0.02804192155599594 + 2.0 * 6.269627094268799
Epoch 1280, val loss: 1.674398422241211
Epoch 1290, training loss: 12.565573692321777 = 0.027315398678183556 + 2.0 * 6.269129276275635
Epoch 1290, val loss: 1.6821502447128296
Epoch 1300, training loss: 12.579487800598145 = 0.02661764621734619 + 2.0 * 6.276434898376465
Epoch 1300, val loss: 1.6895042657852173
Epoch 1310, training loss: 12.564817428588867 = 0.02594882808625698 + 2.0 * 6.269434452056885
Epoch 1310, val loss: 1.6974716186523438
Epoch 1320, training loss: 12.559399604797363 = 0.025303956121206284 + 2.0 * 6.267047882080078
Epoch 1320, val loss: 1.7048712968826294
Epoch 1330, training loss: 12.557642936706543 = 0.02467857114970684 + 2.0 * 6.266482353210449
Epoch 1330, val loss: 1.7121812105178833
Epoch 1340, training loss: 12.572331428527832 = 0.024077583104372025 + 2.0 * 6.274127006530762
Epoch 1340, val loss: 1.7194231748580933
Epoch 1350, training loss: 12.559514045715332 = 0.023498743772506714 + 2.0 * 6.268007755279541
Epoch 1350, val loss: 1.7265299558639526
Epoch 1360, training loss: 12.55273151397705 = 0.022941118106245995 + 2.0 * 6.264894962310791
Epoch 1360, val loss: 1.7337796688079834
Epoch 1370, training loss: 12.556329727172852 = 0.022403638809919357 + 2.0 * 6.266963005065918
Epoch 1370, val loss: 1.7406538724899292
Epoch 1380, training loss: 12.556302070617676 = 0.021884463727474213 + 2.0 * 6.267208576202393
Epoch 1380, val loss: 1.747259259223938
Epoch 1390, training loss: 12.553373336791992 = 0.021385548636317253 + 2.0 * 6.265994071960449
Epoch 1390, val loss: 1.7543319463729858
Epoch 1400, training loss: 12.547632217407227 = 0.020902013406157494 + 2.0 * 6.263365268707275
Epoch 1400, val loss: 1.760914921760559
Epoch 1410, training loss: 12.545711517333984 = 0.020433861762285233 + 2.0 * 6.262639045715332
Epoch 1410, val loss: 1.7674518823623657
Epoch 1420, training loss: 12.557089805603027 = 0.019979510456323624 + 2.0 * 6.268555164337158
Epoch 1420, val loss: 1.7739089727401733
Epoch 1430, training loss: 12.549480438232422 = 0.019543414935469627 + 2.0 * 6.264968395233154
Epoch 1430, val loss: 1.7802608013153076
Epoch 1440, training loss: 12.54473876953125 = 0.019125686958432198 + 2.0 * 6.262806415557861
Epoch 1440, val loss: 1.7869341373443604
Epoch 1450, training loss: 12.54157829284668 = 0.01871742121875286 + 2.0 * 6.261430263519287
Epoch 1450, val loss: 1.7930675745010376
Epoch 1460, training loss: 12.54161548614502 = 0.018319284543395042 + 2.0 * 6.261648178100586
Epoch 1460, val loss: 1.7991769313812256
Epoch 1470, training loss: 12.549277305603027 = 0.017936795949935913 + 2.0 * 6.265670299530029
Epoch 1470, val loss: 1.8052425384521484
Epoch 1480, training loss: 12.540325164794922 = 0.01756778359413147 + 2.0 * 6.261378765106201
Epoch 1480, val loss: 1.8114049434661865
Epoch 1490, training loss: 12.537490844726562 = 0.017210235819220543 + 2.0 * 6.260140419006348
Epoch 1490, val loss: 1.8174306154251099
Epoch 1500, training loss: 12.539539337158203 = 0.016861261799931526 + 2.0 * 6.26133918762207
Epoch 1500, val loss: 1.823301076889038
Epoch 1510, training loss: 12.541877746582031 = 0.016525913029909134 + 2.0 * 6.262675762176514
Epoch 1510, val loss: 1.828718662261963
Epoch 1520, training loss: 12.535560607910156 = 0.01620296761393547 + 2.0 * 6.259678840637207
Epoch 1520, val loss: 1.8348329067230225
Epoch 1530, training loss: 12.534568786621094 = 0.015887292101979256 + 2.0 * 6.259340763092041
Epoch 1530, val loss: 1.8404947519302368
Epoch 1540, training loss: 12.531378746032715 = 0.015580744482576847 + 2.0 * 6.257898807525635
Epoch 1540, val loss: 1.8459038734436035
Epoch 1550, training loss: 12.5314302444458 = 0.015280522406101227 + 2.0 * 6.258074760437012
Epoch 1550, val loss: 1.8515948057174683
Epoch 1560, training loss: 12.554595947265625 = 0.01499252486974001 + 2.0 * 6.269801616668701
Epoch 1560, val loss: 1.8569873571395874
Epoch 1570, training loss: 12.530170440673828 = 0.014711814932525158 + 2.0 * 6.257729530334473
Epoch 1570, val loss: 1.8622491359710693
Epoch 1580, training loss: 12.531270027160645 = 0.01444227248430252 + 2.0 * 6.258413791656494
Epoch 1580, val loss: 1.867711067199707
Epoch 1590, training loss: 12.531099319458008 = 0.014179464429616928 + 2.0 * 6.25846004486084
Epoch 1590, val loss: 1.8727787733078003
Epoch 1600, training loss: 12.526872634887695 = 0.013921246863901615 + 2.0 * 6.256475925445557
Epoch 1600, val loss: 1.877989649772644
Epoch 1610, training loss: 12.53449821472168 = 0.013669961132109165 + 2.0 * 6.260414123535156
Epoch 1610, val loss: 1.8830336332321167
Epoch 1620, training loss: 12.527290344238281 = 0.013428340665996075 + 2.0 * 6.256930828094482
Epoch 1620, val loss: 1.8880540132522583
Epoch 1630, training loss: 12.5277099609375 = 0.013194196857511997 + 2.0 * 6.25725793838501
Epoch 1630, val loss: 1.8932521343231201
Epoch 1640, training loss: 12.524893760681152 = 0.012963615357875824 + 2.0 * 6.255965232849121
Epoch 1640, val loss: 1.8980473279953003
Epoch 1650, training loss: 12.524785041809082 = 0.012739746831357479 + 2.0 * 6.2560224533081055
Epoch 1650, val loss: 1.902913212776184
Epoch 1660, training loss: 12.527359962463379 = 0.012522605247795582 + 2.0 * 6.257418632507324
Epoch 1660, val loss: 1.907594084739685
Epoch 1670, training loss: 12.527144432067871 = 0.012313147075474262 + 2.0 * 6.257415771484375
Epoch 1670, val loss: 1.9124488830566406
Epoch 1680, training loss: 12.522475242614746 = 0.012106294743716717 + 2.0 * 6.255184650421143
Epoch 1680, val loss: 1.917074203491211
Epoch 1690, training loss: 12.52142333984375 = 0.011907247826457024 + 2.0 * 6.254757881164551
Epoch 1690, val loss: 1.9217710494995117
Epoch 1700, training loss: 12.534132957458496 = 0.011711391620337963 + 2.0 * 6.261210918426514
Epoch 1700, val loss: 1.9260601997375488
Epoch 1710, training loss: 12.522178649902344 = 0.011522702872753143 + 2.0 * 6.255328178405762
Epoch 1710, val loss: 1.9308290481567383
Epoch 1720, training loss: 12.524676322937012 = 0.01133880764245987 + 2.0 * 6.256668567657471
Epoch 1720, val loss: 1.9352210760116577
Epoch 1730, training loss: 12.515894889831543 = 0.011159691028296947 + 2.0 * 6.2523674964904785
Epoch 1730, val loss: 1.9394309520721436
Epoch 1740, training loss: 12.515003204345703 = 0.010984351858496666 + 2.0 * 6.252009391784668
Epoch 1740, val loss: 1.9440617561340332
Epoch 1750, training loss: 12.519548416137695 = 0.010813124477863312 + 2.0 * 6.254367828369141
Epoch 1750, val loss: 1.9482078552246094
Epoch 1760, training loss: 12.517520904541016 = 0.010644679889082909 + 2.0 * 6.2534379959106445
Epoch 1760, val loss: 1.9524472951889038
Epoch 1770, training loss: 12.519506454467773 = 0.010482707992196083 + 2.0 * 6.254511833190918
Epoch 1770, val loss: 1.956541657447815
Epoch 1780, training loss: 12.525541305541992 = 0.010323877446353436 + 2.0 * 6.257608890533447
Epoch 1780, val loss: 1.960626244544983
Epoch 1790, training loss: 12.512321472167969 = 0.010170849971473217 + 2.0 * 6.251075267791748
Epoch 1790, val loss: 1.9648492336273193
Epoch 1800, training loss: 12.510916709899902 = 0.010021665133535862 + 2.0 * 6.250447750091553
Epoch 1800, val loss: 1.9689583778381348
Epoch 1810, training loss: 12.509088516235352 = 0.009873277507722378 + 2.0 * 6.249607563018799
Epoch 1810, val loss: 1.9728953838348389
Epoch 1820, training loss: 12.515949249267578 = 0.009729119949042797 + 2.0 * 6.253109931945801
Epoch 1820, val loss: 1.9768093824386597
Epoch 1830, training loss: 12.518765449523926 = 0.00958822388201952 + 2.0 * 6.254588603973389
Epoch 1830, val loss: 1.9808496236801147
Epoch 1840, training loss: 12.511669158935547 = 0.009450996294617653 + 2.0 * 6.2511091232299805
Epoch 1840, val loss: 1.9845200777053833
Epoch 1850, training loss: 12.507205963134766 = 0.009318006224930286 + 2.0 * 6.24894380569458
Epoch 1850, val loss: 1.9884668588638306
Epoch 1860, training loss: 12.517090797424316 = 0.009188340045511723 + 2.0 * 6.253951072692871
Epoch 1860, val loss: 1.9920011758804321
Epoch 1870, training loss: 12.508204460144043 = 0.009058942086994648 + 2.0 * 6.24957275390625
Epoch 1870, val loss: 1.9956750869750977
Epoch 1880, training loss: 12.504663467407227 = 0.008936125785112381 + 2.0 * 6.24786376953125
Epoch 1880, val loss: 1.9996240139007568
Epoch 1890, training loss: 12.50479507446289 = 0.008813537657260895 + 2.0 * 6.247990608215332
Epoch 1890, val loss: 2.003199338912964
Epoch 1900, training loss: 12.526412963867188 = 0.00869416818022728 + 2.0 * 6.258859634399414
Epoch 1900, val loss: 2.0066475868225098
Epoch 1910, training loss: 12.509125709533691 = 0.00857696682214737 + 2.0 * 6.250274181365967
Epoch 1910, val loss: 2.009977102279663
Epoch 1920, training loss: 12.503981590270996 = 0.00846460647881031 + 2.0 * 6.247758388519287
Epoch 1920, val loss: 2.013856887817383
Epoch 1930, training loss: 12.500985145568848 = 0.008352833800017834 + 2.0 * 6.246315956115723
Epoch 1930, val loss: 2.0171151161193848
Epoch 1940, training loss: 12.505132675170898 = 0.008243068121373653 + 2.0 * 6.2484450340271
Epoch 1940, val loss: 2.0205979347229004
Epoch 1950, training loss: 12.504179954528809 = 0.008135571144521236 + 2.0 * 6.248022079467773
Epoch 1950, val loss: 2.023446559906006
Epoch 1960, training loss: 12.504307746887207 = 0.00803252961486578 + 2.0 * 6.248137474060059
Epoch 1960, val loss: 2.0273704528808594
Epoch 1970, training loss: 12.498918533325195 = 0.007931328378617764 + 2.0 * 6.245493412017822
Epoch 1970, val loss: 2.030682325363159
Epoch 1980, training loss: 12.497791290283203 = 0.007831276394426823 + 2.0 * 6.2449798583984375
Epoch 1980, val loss: 2.0337274074554443
Epoch 1990, training loss: 12.510622024536133 = 0.007733408361673355 + 2.0 * 6.251444339752197
Epoch 1990, val loss: 2.0369253158569336
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6888888888888889
0.8207696362677913
=== training gcn model ===
Epoch 0, training loss: 19.156373977661133 = 1.9626622200012207 + 2.0 * 8.596856117248535
Epoch 0, val loss: 1.9577820301055908
Epoch 10, training loss: 19.144193649291992 = 1.9510480165481567 + 2.0 * 8.596572875976562
Epoch 10, val loss: 1.9461082220077515
Epoch 20, training loss: 19.124778747558594 = 1.936772108078003 + 2.0 * 8.594003677368164
Epoch 20, val loss: 1.9316530227661133
Epoch 30, training loss: 19.068437576293945 = 1.9171172380447388 + 2.0 * 8.57565975189209
Epoch 30, val loss: 1.9120407104492188
Epoch 40, training loss: 18.852813720703125 = 1.8920267820358276 + 2.0 * 8.480393409729004
Epoch 40, val loss: 1.8883157968521118
Epoch 50, training loss: 18.220504760742188 = 1.865248441696167 + 2.0 * 8.177628517150879
Epoch 50, val loss: 1.8640607595443726
Epoch 60, training loss: 17.862245559692383 = 1.8426611423492432 + 2.0 * 8.00979232788086
Epoch 60, val loss: 1.844563603401184
Epoch 70, training loss: 17.044246673583984 = 1.8273820877075195 + 2.0 * 7.608432292938232
Epoch 70, val loss: 1.8305546045303345
Epoch 80, training loss: 16.287429809570312 = 1.8159159421920776 + 2.0 * 7.235756874084473
Epoch 80, val loss: 1.8196223974227905
Epoch 90, training loss: 15.757075309753418 = 1.8041900396347046 + 2.0 * 6.976442813873291
Epoch 90, val loss: 1.8089356422424316
Epoch 100, training loss: 15.491775512695312 = 1.7903382778167725 + 2.0 * 6.8507184982299805
Epoch 100, val loss: 1.7966889142990112
Epoch 110, training loss: 15.341233253479004 = 1.7751929759979248 + 2.0 * 6.78302001953125
Epoch 110, val loss: 1.7831445932388306
Epoch 120, training loss: 15.221177101135254 = 1.7595608234405518 + 2.0 * 6.730808258056641
Epoch 120, val loss: 1.7691452503204346
Epoch 130, training loss: 15.125743865966797 = 1.7431873083114624 + 2.0 * 6.691278457641602
Epoch 130, val loss: 1.754790186882019
Epoch 140, training loss: 15.03518009185791 = 1.725619912147522 + 2.0 * 6.65477991104126
Epoch 140, val loss: 1.7396442890167236
Epoch 150, training loss: 14.942108154296875 = 1.7064883708953857 + 2.0 * 6.617809772491455
Epoch 150, val loss: 1.7233564853668213
Epoch 160, training loss: 14.86197280883789 = 1.6852765083312988 + 2.0 * 6.588348388671875
Epoch 160, val loss: 1.7051942348480225
Epoch 170, training loss: 14.79193115234375 = 1.66139817237854 + 2.0 * 6.5652666091918945
Epoch 170, val loss: 1.6846235990524292
Epoch 180, training loss: 14.723185539245605 = 1.6346027851104736 + 2.0 * 6.5442914962768555
Epoch 180, val loss: 1.6617079973220825
Epoch 190, training loss: 14.659212112426758 = 1.604870319366455 + 2.0 * 6.527170658111572
Epoch 190, val loss: 1.6362123489379883
Epoch 200, training loss: 14.597786903381348 = 1.5721181631088257 + 2.0 * 6.512834548950195
Epoch 200, val loss: 1.6083266735076904
Epoch 210, training loss: 14.535101890563965 = 1.536651372909546 + 2.0 * 6.49922513961792
Epoch 210, val loss: 1.5783249139785767
Epoch 220, training loss: 14.474623680114746 = 1.4991753101348877 + 2.0 * 6.487724304199219
Epoch 220, val loss: 1.5469706058502197
Epoch 230, training loss: 14.419026374816895 = 1.4595972299575806 + 2.0 * 6.479714393615723
Epoch 230, val loss: 1.514185905456543
Epoch 240, training loss: 14.358762741088867 = 1.4188282489776611 + 2.0 * 6.469967365264893
Epoch 240, val loss: 1.4810173511505127
Epoch 250, training loss: 14.295536041259766 = 1.3774114847183228 + 2.0 * 6.459062099456787
Epoch 250, val loss: 1.447654128074646
Epoch 260, training loss: 14.237759590148926 = 1.3354564905166626 + 2.0 * 6.451151371002197
Epoch 260, val loss: 1.4144248962402344
Epoch 270, training loss: 14.184280395507812 = 1.2938027381896973 + 2.0 * 6.4452385902404785
Epoch 270, val loss: 1.3819304704666138
Epoch 280, training loss: 14.130937576293945 = 1.2530381679534912 + 2.0 * 6.4389495849609375
Epoch 280, val loss: 1.3504167795181274
Epoch 290, training loss: 14.076157569885254 = 1.2130440473556519 + 2.0 * 6.431556701660156
Epoch 290, val loss: 1.3199779987335205
Epoch 300, training loss: 14.029544830322266 = 1.1739575862884521 + 2.0 * 6.427793502807617
Epoch 300, val loss: 1.2904531955718994
Epoch 310, training loss: 13.976247787475586 = 1.1360093355178833 + 2.0 * 6.420119285583496
Epoch 310, val loss: 1.2618461847305298
Epoch 320, training loss: 13.92834186553955 = 1.09884512424469 + 2.0 * 6.414748191833496
Epoch 320, val loss: 1.2341307401657104
Epoch 330, training loss: 13.892129898071289 = 1.0624147653579712 + 2.0 * 6.414857387542725
Epoch 330, val loss: 1.2070386409759521
Epoch 340, training loss: 13.839090347290039 = 1.027005910873413 + 2.0 * 6.406042098999023
Epoch 340, val loss: 1.1808922290802002
Epoch 350, training loss: 13.796065330505371 = 0.9923846125602722 + 2.0 * 6.4018402099609375
Epoch 350, val loss: 1.1554536819458008
Epoch 360, training loss: 13.75754165649414 = 0.9584197402000427 + 2.0 * 6.399560928344727
Epoch 360, val loss: 1.1307601928710938
Epoch 370, training loss: 13.717958450317383 = 0.9252434372901917 + 2.0 * 6.396357536315918
Epoch 370, val loss: 1.1069445610046387
Epoch 380, training loss: 13.671027183532715 = 0.8928442001342773 + 2.0 * 6.389091491699219
Epoch 380, val loss: 1.084221363067627
Epoch 390, training loss: 13.633499145507812 = 0.8611586093902588 + 2.0 * 6.386170387268066
Epoch 390, val loss: 1.0624487400054932
Epoch 400, training loss: 13.602316856384277 = 0.8303307890892029 + 2.0 * 6.385993003845215
Epoch 400, val loss: 1.0416535139083862
Epoch 410, training loss: 13.559677124023438 = 0.8004423975944519 + 2.0 * 6.379617214202881
Epoch 410, val loss: 1.022151231765747
Epoch 420, training loss: 13.522589683532715 = 0.7713956832885742 + 2.0 * 6.37559700012207
Epoch 420, val loss: 1.003909945487976
Epoch 430, training loss: 13.496780395507812 = 0.7430989742279053 + 2.0 * 6.376840591430664
Epoch 430, val loss: 0.9867499470710754
Epoch 440, training loss: 13.458454132080078 = 0.715819776058197 + 2.0 * 6.371317386627197
Epoch 440, val loss: 0.9708312153816223
Epoch 450, training loss: 13.424548149108887 = 0.6893824934959412 + 2.0 * 6.36758279800415
Epoch 450, val loss: 0.9560644030570984
Epoch 460, training loss: 13.390268325805664 = 0.6638736128807068 + 2.0 * 6.363197326660156
Epoch 460, val loss: 0.9425525665283203
Epoch 470, training loss: 13.360462188720703 = 0.6390891075134277 + 2.0 * 6.360686779022217
Epoch 470, val loss: 0.9301337003707886
Epoch 480, training loss: 13.348405838012695 = 0.6150996088981628 + 2.0 * 6.366652965545654
Epoch 480, val loss: 0.9188503623008728
Epoch 490, training loss: 13.30880069732666 = 0.5919081568717957 + 2.0 * 6.35844612121582
Epoch 490, val loss: 0.9084616303443909
Epoch 500, training loss: 13.27702808380127 = 0.5695405602455139 + 2.0 * 6.353743553161621
Epoch 500, val loss: 0.8994312286376953
Epoch 510, training loss: 13.249703407287598 = 0.547795832157135 + 2.0 * 6.350953578948975
Epoch 510, val loss: 0.8911808133125305
Epoch 520, training loss: 13.2363920211792 = 0.526658833026886 + 2.0 * 6.3548665046691895
Epoch 520, val loss: 0.8838271498680115
Epoch 530, training loss: 13.201735496520996 = 0.5061639547348022 + 2.0 * 6.347785949707031
Epoch 530, val loss: 0.8773030042648315
Epoch 540, training loss: 13.177288055419922 = 0.48621052503585815 + 2.0 * 6.34553861618042
Epoch 540, val loss: 0.8715965151786804
Epoch 550, training loss: 13.155339241027832 = 0.4668448865413666 + 2.0 * 6.344247341156006
Epoch 550, val loss: 0.8666051030158997
Epoch 560, training loss: 13.133468627929688 = 0.44809263944625854 + 2.0 * 6.342688083648682
Epoch 560, val loss: 0.8623747229576111
Epoch 570, training loss: 13.107324600219727 = 0.4298880994319916 + 2.0 * 6.338718414306641
Epoch 570, val loss: 0.8589122891426086
Epoch 580, training loss: 13.08629322052002 = 0.4121866822242737 + 2.0 * 6.337053298950195
Epoch 580, val loss: 0.856069803237915
Epoch 590, training loss: 13.08358097076416 = 0.39495155215263367 + 2.0 * 6.3443145751953125
Epoch 590, val loss: 0.8537545204162598
Epoch 600, training loss: 13.053934097290039 = 0.3783189356327057 + 2.0 * 6.337807655334473
Epoch 600, val loss: 0.852086067199707
Epoch 610, training loss: 13.028258323669434 = 0.3623214066028595 + 2.0 * 6.332968235015869
Epoch 610, val loss: 0.8510420918464661
Epoch 620, training loss: 13.009834289550781 = 0.3468569219112396 + 2.0 * 6.331488609313965
Epoch 620, val loss: 0.8505852222442627
Epoch 630, training loss: 12.994194030761719 = 0.33192178606987 + 2.0 * 6.331136226654053
Epoch 630, val loss: 0.850663959980011
Epoch 640, training loss: 12.973593711853027 = 0.31754037737846375 + 2.0 * 6.32802677154541
Epoch 640, val loss: 0.8511556386947632
Epoch 650, training loss: 12.961054801940918 = 0.3037523031234741 + 2.0 * 6.328651428222656
Epoch 650, val loss: 0.8522705435752869
Epoch 660, training loss: 12.942462921142578 = 0.29050329327583313 + 2.0 * 6.325979709625244
Epoch 660, val loss: 0.8537999391555786
Epoch 670, training loss: 12.930888175964355 = 0.27780091762542725 + 2.0 * 6.326543807983398
Epoch 670, val loss: 0.8558084964752197
Epoch 680, training loss: 12.910294532775879 = 0.26564210653305054 + 2.0 * 6.322326183319092
Epoch 680, val loss: 0.8583983182907104
Epoch 690, training loss: 12.901126861572266 = 0.2539732754230499 + 2.0 * 6.323576927185059
Epoch 690, val loss: 0.8613877892494202
Epoch 700, training loss: 12.889299392700195 = 0.24279190599918365 + 2.0 * 6.323253631591797
Epoch 700, val loss: 0.8646305203437805
Epoch 710, training loss: 12.876740455627441 = 0.2321130782365799 + 2.0 * 6.3223137855529785
Epoch 710, val loss: 0.8682714104652405
Epoch 720, training loss: 12.858067512512207 = 0.22196999192237854 + 2.0 * 6.31804895401001
Epoch 720, val loss: 0.8722774386405945
Epoch 730, training loss: 12.844744682312012 = 0.212304949760437 + 2.0 * 6.316219806671143
Epoch 730, val loss: 0.876750111579895
Epoch 740, training loss: 12.833222389221191 = 0.20305009186267853 + 2.0 * 6.315086364746094
Epoch 740, val loss: 0.8813900947570801
Epoch 750, training loss: 12.830384254455566 = 0.19421938061714172 + 2.0 * 6.318082332611084
Epoch 750, val loss: 0.8862696290016174
Epoch 760, training loss: 12.811568260192871 = 0.18581584095954895 + 2.0 * 6.312876224517822
Epoch 760, val loss: 0.8913569450378418
Epoch 770, training loss: 12.807586669921875 = 0.1778075248003006 + 2.0 * 6.314889430999756
Epoch 770, val loss: 0.8967323303222656
Epoch 780, training loss: 12.790475845336914 = 0.1701899766921997 + 2.0 * 6.310142993927002
Epoch 780, val loss: 0.9021866917610168
Epoch 790, training loss: 12.781225204467773 = 0.16294163465499878 + 2.0 * 6.309141635894775
Epoch 790, val loss: 0.9079387187957764
Epoch 800, training loss: 12.772162437438965 = 0.1560220569372177 + 2.0 * 6.308070182800293
Epoch 800, val loss: 0.9137694239616394
Epoch 810, training loss: 12.779837608337402 = 0.14943942427635193 + 2.0 * 6.31519889831543
Epoch 810, val loss: 0.9196165800094604
Epoch 820, training loss: 12.762296676635742 = 0.14316706359386444 + 2.0 * 6.309564590454102
Epoch 820, val loss: 0.9254673719406128
Epoch 830, training loss: 12.7461519241333 = 0.13722044229507446 + 2.0 * 6.3044657707214355
Epoch 830, val loss: 0.931723952293396
Epoch 840, training loss: 12.738780975341797 = 0.13155055046081543 + 2.0 * 6.303615093231201
Epoch 840, val loss: 0.9377987384796143
Epoch 850, training loss: 12.741718292236328 = 0.1261405646800995 + 2.0 * 6.307788848876953
Epoch 850, val loss: 0.9440881609916687
Epoch 860, training loss: 12.7297945022583 = 0.12099014222621918 + 2.0 * 6.3044023513793945
Epoch 860, val loss: 0.9501781463623047
Epoch 870, training loss: 12.721803665161133 = 0.11608679592609406 + 2.0 * 6.302858352661133
Epoch 870, val loss: 0.9565359354019165
Epoch 880, training loss: 12.718567848205566 = 0.11142392456531525 + 2.0 * 6.303572177886963
Epoch 880, val loss: 0.9627413153648376
Epoch 890, training loss: 12.703927040100098 = 0.10697042942047119 + 2.0 * 6.298478126525879
Epoch 890, val loss: 0.9689015746116638
Epoch 900, training loss: 12.705358505249023 = 0.10273203998804092 + 2.0 * 6.301313400268555
Epoch 900, val loss: 0.9752182364463806
Epoch 910, training loss: 12.697541236877441 = 0.09869775921106339 + 2.0 * 6.299421787261963
Epoch 910, val loss: 0.9815070033073425
